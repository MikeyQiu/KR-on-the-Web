8
1
0
2
 
b
e
F
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
3
9
2
4
0
.
5
0
7
1
:
v
i
X
r
a

Bayesian Approaches to Distribution Regression

Ho Chung Leon Law∗
University of Oxford
ho.law@spc.ox.ac.uk

Dougal J. Sutherland∗
University College London
dougal@gmail.com

Dino Sejdinovic
University of Oxford
dino.sejdinovic@stats.ox.ac.uk

Seth Flaxman
Imperial College London
s.ﬂaxman@imperial.ac.uk

Abstract

Distribution regression has recently attracted
much interest as a generic solution to the prob-
lem of supervised learning where labels are avail-
able at the group level, rather than at the individ-
ual level. Current approaches, however, do not
propagate the uncertainty in observations due to
sampling variability in the groups. This effec-
tively assumes that small and large groups are
estimated equally well, and should have equal
weight in the ﬁnal regression. We account for
this uncertainty with a Bayesian distribution re-
improving the robustness
gression formalism,
and performance of the model when group sizes
vary. We frame our models in a neural network
style, allowing for simple MAP inference using
backpropagation to learn the parameters, as well
as MCMC-based inference which can fully prop-
agate uncertainty. We demonstrate our approach
on illustrative toy datasets, as well as on a chal-
lenging problem of predicting age from images.

1

INTRODUCTION

Distribution regression is the problem of learning a regres-
sion function from samples of a distribution to a single set-
level label. For example, we might attempt to infer the
sentiment of texts based on word-level features, to predict
the label of an image based on small patches, or even per-
form traditional parametric statistical inference by learning
a function from sets of samples to the parameter values.

Recent years have seen wide-ranging applications of this
framework, including inferring summary statistics in Ap-
proximate Bayesian Computation (Mitrovic et al., 2016),
estimating Expectation Propagation messages (Jitkrittum
et al., 2015), predicting the voting behaviour of demo-

Proceedings of the 21st International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS) 2018, Lanzarote, Spain.
PMLR: Volume 84. Copyright 2018 by the author(s).

graphic groups (Flaxman et al., 2015, 2016), and learning
the total mass of dark matter halos from observable galaxy
velocities (Ntampaka et al., 2015, 2016). Closely related
distribution classiﬁcation problems also include identify-
ing the direction of causal relationships from data (Lopez-
Paz et al., 2015) and classifying text based on bags of word
vectors (Yoshikawa et al., 2014; Kusner et al., 2015).

One particularly appealing approach to the distribution re-
gression problem is to represent the input set of samples
by their kernel mean embedding (described in Section 2.1),
where distributions are represented as single points in a re-
producing kernel Hilbert space. Standard kernel methods
can then be applied for distribution regression, classiﬁca-
tion, anomaly detection, and so on. This approach was per-
haps ﬁrst popularized by Muandet et al. (2012); Sz´abo et al.
(2016) provided a recent learning-theoretic analysis.

In this framework, however, each distribution is simply rep-
resented by the empirical mean embedding, ignoring the
fact that large sample sets are much more precisely under-
stood than small ones. Most studies also use point esti-
mates for their regressions, such as kernel ridge regression
or support vector machines, thus ignoring uncertainty both
in the distribution embeddings and in the regression model.

Our Contributions We propose a set of Bayesian ap-
proaches to distribution regression. The simplest method,
similar to that of Flaxman et al. (2015), is to use point
estimates of the input embeddings but account for uncer-
tainty in the regression model with simple Bayesian linear
regression. Alternatively, we can treat uncertainty in the in-
put embeddings but ignore model uncertainty with the pro-
posed Bayesian mean shrinkage model, which builds on a
recently proposed Bayesian nonparametric model of uncer-
tainty in kernel mean embeddings (Flaxman et al., 2016),
and then use a sparse representation of the desired function
in the RKHS for prediction in the regression model. This
model allows for a full account of uncertainty in the mean
embedding, but requires a point estimate of the regression
function for conjugacy; we thus use backpropagation to ob-
tain a MAP estimate for it as well as various hyperparam-
eters. We then combine the treatment of the two sources

∗These authors contributed equally.

Bayesian Approaches to Distribution Regression

of uncertainty into a fully Bayesian model and use Hamil-
tonian Monte Carlo for efﬁcient inference. Depending on
the inferential goals, each model can be useful. We demon-
strate our approaches on an illustrative toy problem as well
as a challenging real-world age estimation task.

2 BACKGROUND

2.1 Problem Overview

Distribution regression is the task of learning a classiﬁer or
a regression function that maps probability distributions to
labels. The challenge of distribution regression goes be-
yond the standard supervised learning setting: we do not
have access to exact input-output pairs since the true in-
puts, probability distributions, are observed only through
samples from that distribution:

,

(cid:17)

(cid:16)

(cid:17)

(cid:16)

(1)

, . . . ,

N1
j=1, y1

Nn
j=1, yn

xi
j}
{

xn
j }
{

x1
j }
{
Ni
xi
j=1 has a label yi along with Ni indi-
so that each bag
j}
{
vidual observations xi
. We assume that the observa-
j ∈ X
Ni
tions
j=1 are i.i.d. samples from some unobserved dis-
tribution Pi, and that the true label yi depends only on Pi.
We wish to avoid making any strong parametric assump-
tions on the Pi. For the present work, we will assume the
labels yi are real-valued; Appendix B shows an extension
to binary classiﬁcation. We typically take the observation
to be a subset of Rp, but it could easily be a struc-
space
tured domain such as text or images, since we access it only
through a kernel (for examples, see e.g. G¨artner, 2008).

X

X × X →

We consider the standard approach to distribution regres-
sion, which relies on kernel mean embeddings and ker-
nel ridge regression. For any positive deﬁnite kernel func-
tion k :
R, there exists a unique reproduc-
k, a possibly inﬁnite-
ing kernel Hilbert space (RKHS)
H
dimensional space of functions f :
R where eval-
uation can be written as an inner product, and in particu-
lar f (x) =
k, x
Hk for all f
. Here
(cid:104)
k is a function of one argument, y
, x)
k(
·

∈ X
(cid:55)→
, let us deﬁne the kernel

, x)
(cid:105)
·

k(y, x).

X →

∈ H

Given a probability measure P on
mean embedding into

f, k(

∈ H

k as

X

H

k (
·

(cid:90)

µP =

, x) P(dx)

k.

∈ H

(2)

H

k(x, x)P(dx) <

Notice that µP serves as a high- or inﬁnite-dimensional
vector representation of P. For the kernel mean embed-
ding of P into
it sufﬁces that
k to be well-deﬁned,
, which is trivially satisﬁed for all
P if k is bounded. Analogously to the reproducing prop-
(cid:82) (cid:112)
erty of RKHS, µP represents the expectation function on
Hk . For so-called characteris-
H
(cid:105)
tic kernels (Sriperumbudur et al., 2010), every probability
measure has a unique embedding, and thus µP completely
determines the corresponding probability measure.

h(x)P(dx) =

h, µP
(cid:104)

∞

k:

(cid:82)

2.2 Estimating Mean Embeddings

For a set of samples
ical estimator of µP,

xj
{
µP

n
j=1 drawn iid from P, the empir-
}
∈ H

k, is given by

µP = µ(cid:98)P =

(cid:90)

(cid:99)

(cid:99)
, x) ˆP(dx) =
k (
·

1
n

k(

, xj).
·

(3)

n

j=1
(cid:88)

This is the standard estimator used by previous distribution
regression approaches, which the reproducing property of

k shows us corresponds to the kernel

H

µPi,
(cid:104)

µPj(cid:105)

Hk =

1
NiNj

Ni

Nj

(cid:96)=1
(cid:88)

r=1
(cid:88)

k(xi

(cid:96), xj

r).

(4)

(cid:100)

(cid:100)

But (3) is an empirical mean estimator in a high- or inﬁnite-
dimensional space, and is thus subject to the well-known
Stein phenomenon, so that its performance is dominated
by the James-Stein shrinkage estimators. Indeed, Muandet
et al. (2014) studied shrinkage estimators for mean embed-
dings, which can result in substantially improved perfor-
mance for some tasks (Ramdas and Wehbe, 2015). Flax-
man et al. (2016) proposed a Bayesian analogue of shrink-
age estimators, which we now review.

·

H

∼ N

)) on

µP(x)

∈ H
|

This approach consists of (1) a Gaussian Process prior
µP
k, where r is selected to en-
,
(m0, r(
·
∼ GP
sure that µP
k almost surely and (2) a normal likeli-
(µP(x), Σ). Here, conjugacy
µP(x)
hood
of the prior and the likelihood leads to a Gaussian process
posterior on the true embedding µP, given that we have ob-
(cid:99)
µP at some set of locations x. The posterior mean
served
is then essentially identical to a particular shrinkage esti-
mator of Muandet et al. (2014), but the method described
here has the extra advantage of a closed form uncertainty
estimate, which we utilise in our distributional approach.
For the choice of r, we use a Gaussian RBF kernel k, and
choose either r = k or, following Flaxman et al. (2016),
r(x, x(cid:48)) =
k(x, z) k(z, x(cid:48)) ν(dz) where ν is proportional
to a Gaussian measure. For details of our choices, and why
they are sufﬁcient for our purposes, see Appendix A.

(cid:99)

(cid:82)

This model accounts for the uncertainty based on the num-
ber of samples Ni, shrinking the embeddings for small
sample sizes more. As we will see, this is essential in
the context of distribution regression, particularly when bag
sizes are imbalanced.

2.3 Standard Approaches to Distribution Regression

Following Sz´abo et al. (2016), assume that the probability
distributions Pi are each drawn randomly from some un-
known meta-distribution over probability distributions, and
take a two-stage approach, illustrated as in Figure 1. De-
noting the feature map k(
k by φ(x), one uses the
, x)
·
empirical kernel mean estimator (3) to separately estimate

∈ H

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

Figure 1: Each bag is summarised by a kernel mean embed-
k; a regression function f :
ding µi
R predicts
labels yi
R. We propose a Bayesian approach to propa-
gate uncertainty due to the number of samples in each bag,
obtaining posterior credible intervals illustrated in grey.

∈ H
∈

→

H

k

the mean of each group:

1
N1

N1

j=1
(cid:88)

µ1 =

(cid:99)

φ(x1

j ),

. . . ,

µn =

φ(xn

j ).

(5)

1
Nn

Nn

i=1
(cid:88)

Next, one uses kernel ridge regression (Saunders et al.,
1998) to learn a function f :
R, by minimizing
the squared loss with an RKHS complexity penalty:

k
H

→

(cid:99)

ˆf = argmin
f ∈HK

(yi

f (

µi))2 + λ
(cid:107)

f

2
HK .
(cid:107)

−

i
(cid:88)
k
× H

(cid:98)

k

H

→

Here K :
R is a “second-level” kernel on
mean embeddings. If K is a linear kernel on the RKHS
k, then the resulting method can be interpreted as a linear
H
(ridge) regression on mean embeddings, which are them-
selves nonlinear transformations of the inputs. A nonlin-
ear second-level kernel on
k sometimes improves perfor-
mance (Muandet et al., 2012; Sz´abo et al., 2016).

H

Distribution regression as described is not scalable for
even modestly-sized datasets, as computing each of the
(n2) entries of the relevant kernel matrix requires time
O
(NiNj). Many applications have thus used variants of
O
random Fourier features (Rahimi and Recht, 2007). In this
paper we instead expand in terms of landmark points drawn
randomly from the observations, yielding radial basis net-
works (Broomhead and Lowe, 1988) with mean pooling.

3 MODELS

Figure 2: Our baseline model, a RBF network for distri-
bution regression. Xi represents the matrix of samples for
bag i, while k(Xi, u(cid:96)) represents the element wise opera-
tion on each row of Xi, with b representing the batch size
for stochastic gradient descent.

begin with a non-Bayesian RBF network formulation of
the standard approach to distribution regression as a base-
line, before reﬁning this approach to better propagate un-
certainty in bag size, as well as model parameters.

3.1 Baseline Model

The baseline RBF network formulation we employ here
is a variation of the approaches of Broomhead and Lowe
(1988), Que and Belkin (2016), Law et al. (2017), and Za-
heer et al. (2017). As shown in Figure 2, the initial input
is a minibatch consisting of several bags Xi, each contain-
ing Ni points. Each point is then converted to an explicit
featurisation, taking the role of φ in (5), by a radial basis
layer: xi

Rp is mapped to

j ∈

φ(xi

j) = [k(xi

j, u1), . . . , k(xi

j, ud)](cid:62)

d

R

∈

u(cid:96)
{

Ni
j=1 φ(xi

d
where u =
(cid:96)=1 are landmark points. A mean pool-
}
ing layer yields the estimated mean embedding ˆµi corre-
sponding to each of the bags j represented in the minibatch,
where ˆµi = 1
j).1 Finally, a fully connected
Ni
output layer gives real-valued labels ˆyi = βT ˆµi + b. As a
loss function we use the mean square error 1
yi)2.
n
For learning, we use backpropagation with the Adam opti-
mizer (Kingma and Ba, 2015). To regularise the network,
we use early stopping on a validation set, as well as an L2
penalty corresponding to a normal prior on β.

i(ˆyi

(cid:80)

(cid:80)

−

We consider here three different Bayesian models, with
each model encoding different types of uncertainty. We

1In the implementation, we stack all of the bags Xi into a
j Nj × d for the ﬁrst layer, then perform

single matrix of size (cid:80)
pooling via sparse matrix multiplication.

Bayesian Approaches to Distribution Regression

variance, in order to propagate this information regarding
uncertainty from the bag size through the model. Bayesian
tools provide a natural framework for this problem.

We can use the Bayesian nonparametric prior over kernel
mean embeddings (Flaxman et al., 2016) described in Sec-
tion 2.2, and observe the empirical embeddings at the land-
mark points ui. For ui, we take a ﬁxed set of landmarks,
which we can choose via k-means clustering or sample
without replacement (Que and Belkin, 2016). Using the
conjugacy of the model to the Gaussian process prior µi

∼
(m0, ηr(., .)), we obtain a closed-form posterior Gaus-

GP
sian process whose evaluation at points h =

hs
{

nh
s=1 is:
}

µi(h)

xi

|

∼ N

Rh (R + Σi/Ni)−1 (ˆµi

m0) + m0,

−

(cid:16)

Rhh

Rh (R + Σi/Ni)−1 R(cid:62)
h

{

(cid:17)

xi
j}

−
where Rst = ηr(us, ut), (Rhh)st = ηr(hs, ht), (Rh)st =
Ni
ηr(hs, ut), and xi denotes the set
j=1. We take the
prior mean m0 to be the average of the ˆµi; under a lin-
ear kernel K, this means we shrink predictions towards the
mean prediction. Note η essentially controls the strength of
the shrinkage: a smaller η means we shrink more strongly
towards m0. We take Σi to be the average of the empirical
Ni
covariance of
j=1 across all bags, to avoid poor es-
timation of Σi for smaller bags. More intuition about the
behaviour of this estimator can be found in Appendix C.
Now, supposing we have normal observation error σ2, and
use a linear kernel as our second level kernel K, we have:

ϕ(xi
j)
}

{

|

yi

(6)

∼ N

µi, f

∈ H

f, µi
(cid:104)

Hk , σ2
(cid:105)
where f
k. Clearly, this is difﬁcult to work with;
(cid:1)
s
hence we parameterise f as f =
, z(cid:96)), where
(cid:96)=1 α(cid:96)k(
·
s
z =
(cid:96)=1 is a set of landmark points for f , which we
}
can learn or ﬁx. (Appendix D gives a motivation for this
approximation using the representer theorem.) Using the
reproducing property, our likelihood model becomes:

z(cid:96)
{

(cid:80)

(cid:0)

yi

µi, α

|

∼ N

αTµi(z), σ2

(7)

where µi(z) = [µi(z1), . . . , µi(zs)](cid:62). For ﬁxed α and z
(cid:0)
we can analytically integrate out the dependence on µi, and
the predictive distribution of a bag label becomes

(cid:1)

(ξα

i , να
i )

yi

xi, α

|

∼ N
i = α(cid:62)Rz
ξα

R +

(cid:18)

Σi
Ni (cid:19)
Rz

(cid:18)

R +

−

−1

Σi
Ni (cid:19)

i = αT
να

Rzz

(cid:32)

−

−1(ˆµi

m0) + αTm0

RT
z

α + σ2.

(cid:33)

(0, ρ2K −1

∼ N

The prior α
trix on z, gives the standard regularisation on f of
The log-likelihood objective becomes
i )2
ξα

z ), where Kz is the kernel ma-
2
Hk .
(cid:107)

f
(cid:107)

(yi

n

−
ξα
i

αTKzα
2ρ2

.

+

(cid:41)

log να

i +

1
2

i=1 (cid:40)
(cid:88)

Figure 3: Our Bayesian mean shrinkage pooling model.
This diagram takes m0 = 0, η = 1 and u = z, so that
R = Rz = Rzz, and Kz = K.

3.2 Bayesian Linear Regression Model

The most obvious approach to adding uncertainty to the
model of Section 3.1 is to encode uncertainty over regres-
sion parameters β only, as follows:

β

∼ N

(0, ρ2)

yi

xi, β

|

∼ N

(βT ˆµi, σ2).

This is essentially Bayesian linear regression on the empiri-
cal mean embeddings, and is closely related to the model of
Flaxman et al. (2015). Here, we are working directly with
the ﬁnite-dimensional ˆµi, unlike the inﬁnite-dimensional
µi before. Due to the conjugacy of the model, we can eas-
xi, integrating out
ily obtain the predictive distribution yi
the uncertainty over β. This provides us with uncertainty
intervals for the predictions yi.

|

For model tuning, we can maximise the model evidence,
i.e. the marginal log-likelihood (see Bishop (2006) for de-
tails), and use backpropagation through the network to
learn σ and ρ and any kernel parameters of interest.2

3.3 Bayesian Mean Shrinkage Model

A shortcoming of the prior models, and of the standard ap-
proach in Sz´abo et al. (2016), is that they ignore uncertainty
in the ﬁrst level of estimation due to varying number of
samples in each bag. Ideally we would estimate not just the
mean embedding per bag, but also a measure of the sample

2Note that unlike the other models considered in this paper,
we cannot easily do minibatch stochastic gradient descent, as the
marginal log-likelihood does not decompose for each individual
data point.

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

We can use backpropagation to learn the parameters α, σ,
and if we wish η, z, and any kernel parameters. The full
model is illustrated in Figure 3. This approach allows us to
directly encode uncertainty based on bag size in the objec-
tive function, and gives probabilistic predictions.

3.4 Bayesian Distribution Regression

It is natural to combine the two Bayesian models above,
fully propagating uncertainty in estimation of the mean
embedding and of the regression coefﬁcients α. Unfortu-
nately, conjugate Bayesian inference is no longer available.
Thus, we consider a Markov Chain Monte Carlo (MCMC)
sampling based approach, and here use Hamiltonian Monte
Carlo (HMC) for efﬁcient inference, though any MCMC-
type scheme would work. Whereas inference above used
gradient descent to maximise the marginal likelihood, with
the gradient calculated using automatic differentiation, here
we use automatic differentiation to calculate the gradient of
the joint log-likelihood and follow this gradient as we per-
form sampling over the parameters we wish to infer.

We can still exploit the conjugacy of the mean shrinkage
layer, obtaining an analytic posterior over the mean em-
beddings. Conditional on the mean embeddings, we have
a Bayesian linear regression model with parameters α. We
sample this model with the NUTS HMC sampler (Hoffman
and Gelman, 2014; Stan Development Team, 2014).

4 RELATED WORK

As previously mentioned, Sz´abo et al. (2016) provides
a thorough learning-theoretic analysis of the regression
model discussed in Section 2.3. This formalism consid-
ering a kernel method on distributions using their embed-
ding representations, or various scalable approximations
to it, has been widely applied (e.g. Muandet et al., 2012;
Yoshikawa et al., 2014; Flaxman et al., 2015; Jitkrittum
et al., 2015; Lopez-Paz et al., 2015; Mitrovic et al., 2016).
There are also several other notions of similarities on distri-
butions in use (not necessarily falling within the framework
of kernel methods and RKHSs), as well as local smoothing
approaches, mostly based on estimates of various probabil-
ity metrics (Moreno et al., 2003; Jebara et al., 2004; P´oczos
et al., 2011; Oliva et al., 2013; Poczos et al., 2013; Kusner
et al., 2015). For a partial overview, see Sutherland (2016).

Other related problems of learning on instances with
group-level labels include learning with label proportions
(Quadrianto et al., 2009; Patrini et al., 2014), ecological
inference (King, 1997; Gelman et al., 2001), pointillistic
pattern search (Ma et al., 2015), multiple instance learning
(Dietterich et al., 1997; K¨uck and de Freitas, 2005; Zhou
et al., 2009; Krummenacher et al., 2013) and learning with
sets (Zaheer et al., 2017).3

3For more, also see giorgiopatrini.org/nips15workshop.

There have also been some Bayesian approaches in related
contexts, though most do not follow our setting where the
label is a function of the underlying distribution rather than
the observed sample set. K¨uck and de Freitas (2005) con-
sider an MCMC method with group-level labels but focus
on individual-level classiﬁers, while Jackson et al. (2006)
use hierarchical Bayesian models on both individual-level
and aggregate data for ecological inference.

Jitkrittum et al. (2015) and Flaxman et al. (2015) quantify
the uncertainty of distribution regression models by inter-
preting the kernel ridge regression on embeddings as Gaus-
sian process regression. However, the former’s setting has
no uncertainty in the mean embeddings, while the latter’s
treats empirical embeddings as ﬁxed inputs to the learning
problem (as in Section 3.2).

There has also been generic work on input uncertainty
in Gaussian process regression (Girard, 2004; Damianou
et al., 2016). These methods could provide a framework
towards allowing for second-level kernels in our models.
One could also, though, consider regression with uncertain
inputs as a special case of distribution regression, where the
label is a function of the distribution’s mean and Ni = 1.

5 EXPERIMENTS

(cid:82)

We will now demonstrate our various Bayesian approaches:
the mean-shrinkage pooling method with r = k (shrink-
k(x, z)k(z, x(cid:48))ν(dz) for ν pro-
age) and with r(x, x(cid:48)) =
portional to a Gaussian measure (shrinkageC), Bayesian
linear regression (BLR), and the full Bayesian distribu-
tion regression model with r = k (BDR). We also com-
pare the non-Bayesian baselines RBF network (Section 3.1)
and freq-shrinkage, which uses the shrinkage estimator of
Muandet et al. (2014) to estimate mean embeddings. Code
for our methods and to reproduce the experiments is avail-
able at https://github.com/hcllaw/bdr.

We ﬁrst demonstrate the characteristics of our models on a
synthetic dataset, and then evaluate them on a real life age
prediction problem. Throughout, for simplicity, we take
u = z, i.e. R = Rz = Rzz, and Kz = K – although
u and z could be different, with z learnt. Here k is the
standard RBF kernel. We tune the learning rate, number
of landmarks, bandwidth of the kernel and regularisation
parameters on a validation set. For BDR, we use weakly
informative normal priors (possibly truncated at zero); for
other models, we learn the remaining parameters.

5.1 Gamma Synthetic Data

We create a synthetic dataset by repeatedly sampling from
the following hierarchical model, where yi is the label for
the ith bag, each xi
R5 has entries i.i.d. according to
the given distribution, and ε is an added noise term which

j ∈

Bayesian Approaches to Distribution Regression

ated dataset, 25% of the bags have Ni = 20, and 25% have
Ni = 100. Among the other half of the data, we vary the
ratio of Ni = 5 and Ni = 1 000 bags to demonstrate the
methods’ efﬁcacy at dealing with varied bag sizes: we let
s5 be the overall percentage of bags with Ni = 5, ranging
from s5 = 0 (in which case no bags have size Ni = 5) to
s5 = 50 (in which case 50% of the overall bags have size
Ni = 5). Here we do not add additional noise: ε = 0.

Results are shown in Figure 4. BDR and shrinkage meth-
ods, which take into account bag size uncertainty, per-
form well here compared to the other methods. The full
BDR model very slightly outperforms the Bayesian shrink-
age models in both likelihood and in mean-squared error;
frequentist shrinkage slightly outperforms the Bayesian
shrinkage models in MSE, likely because it is tuned for that
metric. We also see that the choice of r affects the results;
r = k does somewhat better.

Figure 5 demonstrates in more detail the difference be-
tween these models. It shows test set predictions of each
model on the bags of different sizes. Here, we can see
explicitly that the shrinkage and BDR models are able to
take into account the bag size, with decreasing variance
for larger bag sizes, while the BLR model gives the same
variance for all outputs. Furthermore, the shrinkage and
BDR models can shrink their predictions towards the mean
more for smaller bags than larger ones: this improves per-
formance on the small bags while still allowing for good
predictions on large bags, contrary to the BLR model.

Fixed bag size: Uncertainty in the regression model.
The previous experiment showed the efﬁcacy of the shrink-
age estimator in our models, but demonstrated little gain
from posterior inference for regression weights β over their
MAP estimates, i.e. there is no discernible improvement of
BLR over RBF network. To isolate the effect of quantify-
ing uncertainty in the regression model, we now consider
the case where there is no variation in bag size at all and
normal noise is added onto the observations. In particular
we take Ni = 1000 and ε
(0, 1), and sample land-
marks randomly from the training set.

∼ N

Results are shown in Table 1. Here, BLR or BDR outper-
form all other methods on all runs, highlighting that uncer-
tainty in the regression model is also important for predic-
tive performance. Importantly, the BDR method performs
well in this regime as well as in the previous one.

5.2

IMDb-WIKI: Age Estimation

We now demonstrate our methods on a celebrity age es-
timation problem, using the IMDb-WIKI database (Rothe
et al., 2016) which consists of 397 949 images of 19 545
celebrities4, with corresponding age labels. This database

4We used only the IMDb images, and removed some implau-
sible images, including one of a cat and several of people with

Figure 4: Top: negative log-likelihood. Bottom: mean-
squared error. For context, performance of the Bayes-
optimal predictor is also shown, and for NLL ‘uniform’
shows the performance of a uniform prediction on the pos-
sible labels. For MSE, the constant overall mean label pre-
dictor achieves about 1.3.

differs for the two experiments below:

yi

yi

∼
iid
∼

Uniform(4, 8)
1
1
Γ
2
yi (cid:20)

yi
2

(cid:18)

,

(cid:19)(cid:21)

xi
j

(cid:2)

(cid:96) |
(cid:3)

+ ε for j

[Ni], (cid:96)

[5].

∈

∈

In these experiments, we generate 1 000 bags for training,
500 bags for a validation set for parameter tuning, 500
bags to use for early-stopping of the models, and 1 000
bags for testing. Tuning is performed to maximize log-
likelihoods for Bayesian models, MSE for non-Bayesian
models. Landmark points u are chosen via k-means (ﬁxed
across all models). We also show results of the Bayes-
optimal model, which gives true posteriors according to
the data-generating process; this is the best performance
any model could hope to achieve. Our learning models,
which treat the inputs as ﬁve-dimensional, fully nonpara-
metric distributions, are at a substantial disadvantage even
in how they view the data compared to this true model.

Varying bag size: Uncertainty in the inputs.
In order
to study the behaviour of our models with varying bag size,
we ﬁx four sizes Ni
. For each gener-

5, 20, 100, 1 000
}

∈ {

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

Figure 5: Predictions for the varying bag size experiment of Section 5.1. Each column corresponds to a single prediction
method. Each point in an image represents a single bag, with its horizontal position the true label yi, and its vertical
position the predicted label. The black lines show theoretical perfect predictions. The rows represent different subsets of
the data: the ﬁrst row shows all bags, the second only bags with Ni = 5, and so on. Colours represent the predictive
standard deviation of each point.

Bayesian Approaches to Distribution Regression

Table 1: Results on the ﬁxed bag size dataset, over
10 dataset draws (standard deviations in parentheses).
BLR/BDR perform best on all runs in both metrics.

METHOD

MSE

NLL

Optimal
RBF network
freq-shrinkage
shrinkage
shrinkageC
BLR
BDR

0.170 (0.009)
0.235 (0.014)
0.232 (0.012)
0.237 (0.014)
0.236 (0.013)
0.228 (0.012)
0.227 (0.012)

0.401 (0.018)
–
–
0.703 (0.027)
0.700 (0.029)
0.681 (0.025)
0.683 (0.025)

Table 2: Results on the grouped IMDb-WIKI dataset over
ten runs (standard deviations in parentheses). Here shrink-
age methods perform the best across all 10 runs.

METHOD

RMSE

NLL

CNN
RBF network
freq-shrinkage
shrinkage
BLR

10.25 (0.22)
9.51 (0.20)
9.22 (0.19)
9.28 (0.20)
9.55 (0.19)

3.80 (0.034)
–
–
3.54 (0.021)
3.68 (0.021)

was constructed by crawling IMDb for images of its most
popular actors and directors, with potentially many images
for each celebrity over time. Rothe et al. (2016) use a con-
volutional neural network (CNN) with a VGG-16 architec-
ture to perform 101-way classiﬁcation, with one class cor-
responding to each age in

0, . . . , 100

{

.
}

We take a different approach, and assume that we are given
several images of a single individual (i.e. samples from
the distribution of celebrity images), and are asked to pre-
dict their mean age based on several pictures. For example,
we have 757 images of Brad Pitt from age 27 up to 51,
while we have only 13 images of Chelsea Peretti at ages 35
and 37. Note that 22.5% of bags have only a single image.
We obtain 19 545 bags, with each bag containing between
1 and 796 images of a particular celebrity, and the corre-
sponding bag label calculated from the average of the age
labels of the images inside each bag.

In particular, we use the representation ϕ(x) learnt by the
CNN in Rothe et al. (2016), where ϕ(x) : R256×256
→
R4096 maps from the pixel space of images to the CNN’s
last hidden layer. With these new representations, we can
now treat them as inputs to our radial basis network, shrink-
age (taking r = k here) and BLR models. Although we
could also use the full BDR model here, due to the compu-
tational time and memory required to perform proper pa-

supposedly negative age, or ages of several hundred years.

rameter tuning, we relegate this to a later study.

We use 9 820 bags for training, 2 948 bags for early stop-
ping, 2 946 for validation and 3 928 for testing. Landmarks
are sampled without replacement from the training set.

We repeat the experiment on 10 different splits of the data,
and report the results in Table 2. The baseline CNN results
give performance by averaging the predictive distribution
from the model of Rothe et al. (2016) for each image of a
bag; note that this model was trained on all of the images
used here. From Table 2, we can see that the shrinkage
methods have the best performance; they outperforms all
other methods in all 10 splits of the dataset, in both met-
rics. Non-Bayesian shrinkage again yields slightly better
RMSEs, likely because it is tuned for that metric. This
demonstrates that modelling bag size uncertainty is vital.

6 CONCLUSION

Supervised learning on groups of observations using ker-
nel mean embeddings typically disregards sampling vari-
ability within groups. To handle this problem, we con-
struct Bayesian approaches to modelling kernel mean em-
beddings within a regression model, and investigate advan-
tages of uncertainty propagation within different compo-
nents of the resulting distribution regression. The ability
to take into account the uncertainty in mean embedding es-
timates is demonstrated to be key for constructing mod-
els with good predictive performance when group sizes are
highly imbalanced. We also demonstrate that the results of
a complex neural network model for age estimation can be
improved by shrinkage.

Our models employ a neural network formulation to pro-
vide more expressive feature representations and learn dis-
criminative embeddings. Doing so makes our model easy
to extend to more complicated featurisations than the sim-
ple RBF network used here. By training with backpropa-
gation, or via approximate Bayesian methods such as vari-
ational inference, we can easily ‘learn the kernel’ within
our framework, for example ﬁne-tuning the deep network
of Section 5.2 rather than using a pre-trained model. We
can also apply our networks to structured settings, learning
regression functions on sets of images, audio, or text. Such
models naturally ﬁt into the empirical Bayes framework.

On the other hand, we might extend our model to more
Bayesian feature learning by placing priors over the kernel
hyperparameters, building on classic work on variational
approaches (Barber and Schottky, 1998) and fully Bayesian
inference (Andrieu et al., 2001) in RBF networks. Such
approaches are also possible using other featurisations, e.g.
random Fourier features (as in Oliva et al., 2015).

Future distribution regression approaches will need to ac-
count for uncertainty in observation of the distribution. Our
methods provide a strong, generic building block to do so.

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

References

Christophe Andrieu, Nando De Freitas, and Arnaud
Doucet. Robust full bayesian learning for radial ba-
sis networks. Neural Computation, 13(10):2359–2407,
2001.

David Barber and Bernhard Schottky. Radial basis func-
tions: a bayesian treatment. NIPS, pages 402–408, 1998.
C.M. Bishop. Pattern recognition and machine learning.

Springer New York, 2006.

David S Broomhead and David Lowe. Radial basis func-
tions, multi-variable functional interpolation and adap-
tive networks. Technical report, DTIC Document, 1988.

Andreas C. Damianou, Michalis K. Titsias, and Neil D.
Lawrence. Variational inference for latent variables and
uncertain inputs in Gaussian processes. JMLR, 17(42):
1–62, 2016.

Thomas G Dietterich, Richard H Lathrop, and Tom´as
Lozano-P´erez. Solving the multiple instance problem
with axis-parallel rectangles. Artiﬁcial intelligence, 89
(1):31–71, 1997.

Seth Flaxman, Yu-Xiang Wang, and Alexander J Smola.
Who supported Obama in 2012?: Ecological inference
through distribution regression. In KDD, pages 289–298.
ACM, 2015.

Seth Flaxman, Dino Sejdinovic, John P. Cunningham, and
Sarah Filippi. Bayesian learning of kernel embeddings.
In UAI, 2016.

Seth Flaxman, Dougal J. Sutherland, Yu-Xiang Wang,
and Yee-Whye Teh. Understanding the 2016 US pres-
inference and dis-
idential election using ecological
tribution regression with census microdata.
2016.
arXiv:1611.03787.

Thomas G¨artner. Kernels for Structured Data, volume 72.
World Scientiﬁc, Series in Machine Perception and Arti-
ﬁcial Intelligence, 2008.

Andrew Gelman, David K Park, Stephen Ansolabehere,
Phillip N Price, and Lorraine C Minnite. Models, as-
sumptions and model checking in ecological regressions.
Journal of the Royal Statistical Society: Series A (Statis-
tics in Society), 164(1):101–118, 2001.

Agathe Girard. Approximate methods for propagation of
uncertainty with Gaussian process models. PhD thesis,
University of Glasgow, 2004.

Matthew D. Hoffman and Andrew Gelman. The no-U-turn
sampler: Adaptively setting path lengths in Hamiltonian
Monte Carlo. JMLR, pages 1593–1623, 2014.

Christopher Jackson, Nicky Best, and Sylvia Richardson.
Improving ecological inference using individual-level
data. Statistics in medicine, 25(12):2136–2159, 2006.

Tony Jebara, Risi Imre Kondor, and Andrew Howard. Prob-

ability product kernels. JMLR, 5:819–844, 2004.

Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess,
S. M. Ali Eslami, Balaji Lakshminarayanan, Dino Se-
jdinovic, and Zolt´an Szab´o. Kernel-Based Just-In-Time
Learning for Passing Expectation Propagation Mes-
sages. In UAI, 2015.

Gary King. A Solution to the Ecological Inference Problem.
Princeton University Press, 1997. ISBN 0691012407.

Diederik Kingma and Jimmy Ba. Adam: A method
In ICLR, 2015.

stochastic optimization.

for
arXiv:1412.6980.

Gabriel Krummenacher, Cheng Soon Ong, and Joachim M
Buhmann. Ellipsoidal multiple instance learning.
In
ICML (2), pages 73–81, 2013.

Hendrik K¨uck and Nando de Freitas. Learning about in-
dividuals from group statistics. In UAI, pages 332–339,
2005.

Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Wein-
berger. From word embeddings to document distances.
In ICML, pages 957–966, 2015.

H. C. L. Law, C. Yau, and D. Sejdinovic. Testing and learn-
ing on distributions with symmetric noise invariance. In
NIPS, 2017. arXiv:1703.07596.

Bernhard
Towards a learn-

David Lopez-Paz, Krikamol Muandet,

Sch¨olkopf, and Ilya Tolstikhin.
ing theory of cause-effect inference. In ICML, 2015.
Milan Luki´c and Jay Beder. Stochastic processes with sam-
ple paths in reproducing kernel hilbert spaces. Trans-
actions of the American Mathematical Society, 353(10):
3945–3969, 2001.

Yifei Ma, Dougal J. Sutherland, Roman Garnett, and Jeff
In AIS-

Schneider. Active pointillistic pattern search.
TATS, 2015.

J. Mitrovic, D. Sejdinovic, and Y.W. Teh. DR-ABC:
Approximate Bayesian Computation with Kernel-Based
In ICML, pages 1482–1491,
Distribution Regression.
2016.

Pedro J Moreno, Purdy P Ho, and Nuno Vasconcelos. A
Kullback-Leibler divergence based kernel for SVM clas-
siﬁcation in multimedia applications. In NIPS, 2003.
Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo,
Learning from distribu-
In NIPS, 2012.

and Bernhard Sch¨olkopf.
tions via support measure machines.
arXiv:1202.6504.

Krikamol Muandet, Kenji Fukumizu, Bharath Sriperum-
budur, Arthur Gretton, and Bernhard Schoelkopf. Kernel
mean estimation and stein effect. In ICML, 2014.

Michelle Ntampaka, Hy Trac, Dougal J. Sutherland,
Nicholas Battaglia, Barnab´as P´oczos, and Jeff Schnei-
A machine learning approach for dynamical
der.
The Astro-
mass measurements of galaxy clusters.
physical Journal, 803(2):50, 2015.
ISSN 1538-4357.
arXiv:1410.0686.

Bayesian Approaches to Distribution Regression

Ingo Steinwart. Convergence types and rates in generic
Karhunen-Lo´eve expansions with applications to sam-
ple path properties. arXiv preprint arXiv:1403.1040v3,
March 2017.

Dougal J. Sutherland. Scalable, Flexible, and Active Learn-
ing on Distributions. PhD thesis, Carnegie Mellon Uni-
versity, 2016.

Zolt´an Sz´abo, Bharath K. Sriperumbudur, Barnab´as
Leraning theory for
JMLR, 17(152):1–40, 2016.

P´oczos, and Arthur Gretton.
distribution regression.
arXiv:1411.2066.

Grace Wahba. Spline models for observational data, vol-

ume 59. Siam, 1990.

Yuya Yoshikawa, Tomoharu Iwata, and Hiroshi Sawada.
Latent support measure machines for bag-of-words data
classiﬁcation. In NIPS, pages 1961–1969, 2014.

Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barn-
abas Poczos, Ruslan Salakhutdinov, and Alexander
Smola. Deep sets. In NIPS, 2017.

Zhi-Hua Zhou, Yu-Yin Sun, and Yu-Feng Li. Multi-
instance learning by treating instances as non-iid sam-
ples. In ICML, 2009.

Michelle Ntampaka, Hy Trac, Dougal J. Sutherland, S. Fro-
menteau, B. Poczos, and Jeff Schneider. Dynamical
mass measurements of contaminated galaxy clusters us-
ing machine learning. The Astrophysical Journal, 831
(2):135, 2016. arXiv:1509.05409.

Junier B Oliva, Barnab´as P´oczos, and Jeff Schneider. Dis-
tribution to distribution regression. In ICML, 2013.

Junier B Oliva, Avinava Dubey, Barnab´as P´oczos, Jeff
Schneider, and Eric P Xing. Bayesian nonparametric
kernel-learning. In AISTATS, 2015. arXiv:1506.08776.

Giorgio Patrini, Richard Nock, Tiberio Caetano, and Paul

Rivera. (Almost) no label no cry. In NIPS. 2014.

Natesh S Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee,
and Robert L Wolpert. Characterizing the function space
for bayesian kernel models. JMLR, 8(Aug):1769–1797,
2007.

Barnab´as P´oczos, Liang Xiong, and Jeff Schneider. Non-
parametric divergence estimation with applications to
machine learning on distributions. In UAI, 2011.

Barnabas Poczos, Aarti Singh, Alessandro Rinaldo,
Distribution-free distribu-
In AISTATS, pages 507–515, 2013.

and Larry Wasserman.
tion regression.
arXiv:1302.0082.

Novi Quadrianto, Alex J Smola, Tiberio S Caetano, and
Quoc V Le. Estimating labels from label proportions.
JMLR, 10:2349–2374, 2009.

Qichao Que and Mikhail Belkin. Back to the future: Radial
basis function networks revisited. In AISTATS, 2016.

Ali Rahimi and Benjamin Recht. Random features for
large-scale kernel machines. In NIPS, pages 1177–1184,
2007.

Aaditya Ramdas and Leila Wehbe. Nonparametric inde-
pendence testing for small sample sizes. In IJCAI, 2015.
arXiv:1406.1922.

Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep
expectation of real and apparent age from a single im-
age without facial landmarks. International Journal of
Computer Vision (IJCV), July 2016.

Craig Saunders, Alexander Gammerman, and Volodya
Vovk. Ridge regression learning algorithm in dual vari-
ables. In ICML, 1998.

Bernhard Sch¨olkopf, Ralf Herbrich, and Alex J. Smola. A

generalized representer theorem. In COLT, 2001.

Bharath K Sriperumbudur, Arthur Gretton, Kenji Fuku-
mizu, Bernhard Sch¨olkopf, and Gert RG Lanckriet.
Hilbert space embeddings and metrics on probability
measures. JMLR, 99:1517–1561, 2010.

Stan Development Team. Stan: A c++ library for proba-
bility and sampling, version 2.5.0, 2014. URL http:
//mc-stan.org/.

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

A Choice of r(

,
·

) to ensure µP
·

k
∈ H

We need to choose an appropriate covariance function r, such that µP
,
for inﬁnite-dimensional RKHSs not sufﬁcient to deﬁne r(
·

k (Wahba, 1990) (but see below). However, we can construct

) = k(
·

,
·

k, where µP

)). In particular, it is
,
(0, r(
∈ H
·
), as draws from this particular prior are no longer in
·

∼ GP

·

H

r(x, y) =

k(x, z)k(z, y)ν(dz)

(cid:90)

(8)

k with probability 1 by the nuclear dominance (Luki´c and
where ν is any ﬁnite measure on
∈ H
Beder, 2001; Pillai et al., 2007) for any stationary kernel k. In particular, Flaxman et al. (2016) provides details when k is
a squared exponential kernel deﬁned by

. This then ensures µP

X

k(x, y) = exp(

(x

1
2

−

y)(cid:62)Σ−1

k (x

y))

−

−

x, y

p

R

∈

) with a non-
and ν(dz) = exp
·
stationary component. In this paper, we take Σk = σ2Ip, where σ2 and (cid:96) are tuning parameters, or parameters that we
learn.

it is proportional to a Gaussian measure on Rd, which provides r(
·

dz, i.e.

−

(cid:16)

(cid:17)

,

||z||2
2
2(cid:96)2

Here, the above holds for a general set of stationary kernels, but note that by taking a convolution of a kernel with itself,
it might make the space of functions that we consider overly smooth (i.e. concentrated on a small part of
k). In this
work, however, we consider only the Gaussian RBF kernel k. In fact, recent work (Steinwart, 2017, Theorem 4.2) actually
shows that in this case, the sample paths almost surely belong to (interpolation) spaces which are inﬁnitesimally larger
than the RKHS of the Gaussian RBF kernel. This suggests that we can choose r to be an RBF kernel with a length scale
that is inﬁnitesimally bigger than that of k; thus, in practice, taking r = k would sufﬁce and we do observe that it actually
performs better (Fig. 4).

H

B Framework for Binary Classiﬁcation

Suppose that our labels yi
for uncertainty in the regression parameters is to use bayesian logistic regression, putting priors on β, i.e.

0, 1
, i.e. we are in a binary classiﬁcation framework. Then a simple approach to accounting
}

∈ {

(0, ρ2)

∼ N

β

yi

∼

Ber(πi), where log

= β(cid:62) ˆµi

πi

1

(cid:18)

−

πi (cid:19)
µi, α, we would not be able to obtain an analytical

however for the mean shrinkage pooling model, if we use the above yi
solution for p(yi

xi, α). Instead we use the probit link function, as given by:

|

|

where Φ denotes the Cumulative Distribution Function (CDF) of a standard normal distribution, with µi(z) =
[µi(z1), . . . , µi(zs)](cid:62). Then as before we have

µi, α) = Φ
P r(yi = 1
|

α(cid:62)µi(z)

(cid:0)

(cid:1)

µi(z)

xi

|

∼ N

(Mi, Ci)

with Mi and Ci as deﬁned in section 3.3. Hence, as before

P r(yi = 1
|

xi, α) =

P r(yi = 1
|

xi)dµi(z)
µi, α)p(µi(z)
|

Φ(α(cid:62)µi(z)) exp

(µi(z)

Mi)(cid:62)C −1

i

(µi(z)

1
2

{−

−

Mi)
}

−

dµi(z)

(with li = µi(z)

Mi) = c

Φ(α(cid:62)(li + Mi)) exp

−

1
2

{−

(li)(cid:62)C −1

i

(li)

dli
}

= P r(Y

α(cid:62)(li + Mi))

≤

(cid:90)
= c

(cid:90)

(cid:90)

Bayesian Approaches to Distribution Regression

Note here Y

(0, 1) and li

(0, Σi) Then expanding and rearranging

∼ N

∼ N

Note that since Y and li independent normal r.v., Y
have:

−

α(cid:62)li

∼ N

xi, α) = P r(Y
P r(yi = 1
|

α(cid:62)li

α(cid:62)Mi)

≤

−
(0, 1 + α(cid:62)Ciα(cid:62)). Let T be standard normal, then we

xi, α) = P r(
P r(yi = 1
|

α(cid:62)Mi)

1 + α(cid:62)Ciα T
α(cid:62)Mi
1 + α(cid:62)Ciα

≤

≤

)

α(cid:62)Mi
(cid:112)
1 + α(cid:62)Ciα (cid:33)

(cid:112)
= P r(T

= Φ

(cid:32)

(cid:112)

Hence, we also have:

xi, α) = 1
P r(yi = 0
|

−

Φ

α(cid:62)Mi
1 + α(cid:62)Ciα (cid:33)

(cid:32)

(cid:112)

Now placing the prior α

(0, ρ2K −1

z ), we have the following MAP objective:

∼ N

J(α) = log

p(α)

n

i=1
(cid:89)

p(yi

xi, α)
|

(cid:35)

=

(1

yi) log(1

Φ

(cid:34)

n

−

i=1
(cid:88)

+yi log(Φ

α(cid:62)Mi
)
1 + α(cid:62)Ciα (cid:33)

−

(cid:32)

(cid:112)

α(cid:62)Mi
1 + α(cid:62)Ciα (cid:33)

) +

1
ρ2 α(cid:62)Kzα

(cid:32)

(cid:112)

xi, α), we can also use this in HMC for BDR.
Since we have an analytical solution for P r(yi = 0
|

C Some more intuition on the shrinkage estimator

In this section, we provide some intuition behind the shrinkage estimator in section 3.3. Here, for simplicity, we choose
Σi = τ 2I for all bag i, and m0 = 0, and consider the case where z = u, i.e. R = Rz = Rzz. We can then see that if R
has eigendecomposition U ΛU T , with Λ = diag(λk), the posterior mean is

so that large eigenvalues, λk
towards 0. Likewise, the posterior variance is

(cid:29)

τ 2/Ni, are essentially unchanged, while small eigenvalues, λk

τ 2/Ni, are shrunk

(cid:28)

U diag

λk
λk + τ 2/Ni (cid:19)

(cid:18)

U T (ˆµi),

U diag

λk

(cid:32)

−

λ2
k
λk + τ 2

Ni (cid:33)

U T = U diag

1
Ni
τ 2 + 1

λk (cid:33)

(cid:32)

U T ;

its eigenvalues also decrease as Ni/τ 2 increases.

D Alternative Motivation for choice of f

Here we provide an alternative motivation for the choice of f =
model with a linear kernel K on µi, where f :

k

R:

k
, zs). First, consider the following Bayesian
s=1 αsk(
·

H

→

yi

µi, f

|

(cid:80)
f (µi), σ2

.

(cid:1)

∼ N

(cid:0)

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

Now considering the log-likelihood of

µ, Y
{

}

=

µi, yi
{

n
i=1 (supposing we have these exact embeddings), we obtain:
}

log p(Y

µ, f ) =
|

n

1
2σ2 (yi

−

−

f (µi))2

i=1
(cid:88)
To avoid over-ﬁtting, we place a Gaussian prior on f , i.e.
likelihood over f

k, we have:

−

∈ H

f ∗ = argminf ∈Hk

1
2σ2 (yi

−

f (µi))2 + λ

f
||

||

Hk

log p(f ) = λ

Hk + c. Minimizing the negative log-

f
||

||

Now this is in the form of an empirical risk minimisation problem. Hence using the representer theorem (Sch¨olkopf et al.,
2001), we have that:

i.e. we have a ﬁnite-dimensional problem to solve. Thus since K is a linear kernel:

f =

γjK(., µj)

yi

µi,

|

µj
{

n
j=1, γ
}

∼ N 

γj

µi, µj

(cid:104)

Hk , σ2
(cid:105)

n

j=1
(cid:88)



.





where

µi, µj

Hk can be thought of as the similarity between distributions.

(cid:104)

(cid:105)

xi, γ). This suggests we need to
Now we have the same
|
integrate out µ1, . . . µn. But it is unclear how to perform this integration, since the µi follow Gaussian process distributions.
Hence we can take an approximation to f , i.e. f =
, zs), which would essentially give us a dual method with
·
a sparse approximation to f .

posterior as in Section 3.3, and we would like to compute p(yi

k
s=1 αsk(

GP

n

i=1
(cid:88)

n

j=1
(cid:88)

(cid:80)

8
1
0
2
 
b
e
F
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
3
9
2
4
0
.
5
0
7
1
:
v
i
X
r
a

Bayesian Approaches to Distribution Regression

Ho Chung Leon Law∗
University of Oxford
ho.law@spc.ox.ac.uk

Dougal J. Sutherland∗
University College London
dougal@gmail.com

Dino Sejdinovic
University of Oxford
dino.sejdinovic@stats.ox.ac.uk

Seth Flaxman
Imperial College London
s.ﬂaxman@imperial.ac.uk

Abstract

Distribution regression has recently attracted
much interest as a generic solution to the prob-
lem of supervised learning where labels are avail-
able at the group level, rather than at the individ-
ual level. Current approaches, however, do not
propagate the uncertainty in observations due to
sampling variability in the groups. This effec-
tively assumes that small and large groups are
estimated equally well, and should have equal
weight in the ﬁnal regression. We account for
this uncertainty with a Bayesian distribution re-
improving the robustness
gression formalism,
and performance of the model when group sizes
vary. We frame our models in a neural network
style, allowing for simple MAP inference using
backpropagation to learn the parameters, as well
as MCMC-based inference which can fully prop-
agate uncertainty. We demonstrate our approach
on illustrative toy datasets, as well as on a chal-
lenging problem of predicting age from images.

1

INTRODUCTION

Distribution regression is the problem of learning a regres-
sion function from samples of a distribution to a single set-
level label. For example, we might attempt to infer the
sentiment of texts based on word-level features, to predict
the label of an image based on small patches, or even per-
form traditional parametric statistical inference by learning
a function from sets of samples to the parameter values.

Recent years have seen wide-ranging applications of this
framework, including inferring summary statistics in Ap-
proximate Bayesian Computation (Mitrovic et al., 2016),
estimating Expectation Propagation messages (Jitkrittum
et al., 2015), predicting the voting behaviour of demo-

Proceedings of the 21st International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS) 2018, Lanzarote, Spain.
PMLR: Volume 84. Copyright 2018 by the author(s).

graphic groups (Flaxman et al., 2015, 2016), and learning
the total mass of dark matter halos from observable galaxy
velocities (Ntampaka et al., 2015, 2016). Closely related
distribution classiﬁcation problems also include identify-
ing the direction of causal relationships from data (Lopez-
Paz et al., 2015) and classifying text based on bags of word
vectors (Yoshikawa et al., 2014; Kusner et al., 2015).

One particularly appealing approach to the distribution re-
gression problem is to represent the input set of samples
by their kernel mean embedding (described in Section 2.1),
where distributions are represented as single points in a re-
producing kernel Hilbert space. Standard kernel methods
can then be applied for distribution regression, classiﬁca-
tion, anomaly detection, and so on. This approach was per-
haps ﬁrst popularized by Muandet et al. (2012); Sz´abo et al.
(2016) provided a recent learning-theoretic analysis.

In this framework, however, each distribution is simply rep-
resented by the empirical mean embedding, ignoring the
fact that large sample sets are much more precisely under-
stood than small ones. Most studies also use point esti-
mates for their regressions, such as kernel ridge regression
or support vector machines, thus ignoring uncertainty both
in the distribution embeddings and in the regression model.

Our Contributions We propose a set of Bayesian ap-
proaches to distribution regression. The simplest method,
similar to that of Flaxman et al. (2015), is to use point
estimates of the input embeddings but account for uncer-
tainty in the regression model with simple Bayesian linear
regression. Alternatively, we can treat uncertainty in the in-
put embeddings but ignore model uncertainty with the pro-
posed Bayesian mean shrinkage model, which builds on a
recently proposed Bayesian nonparametric model of uncer-
tainty in kernel mean embeddings (Flaxman et al., 2016),
and then use a sparse representation of the desired function
in the RKHS for prediction in the regression model. This
model allows for a full account of uncertainty in the mean
embedding, but requires a point estimate of the regression
function for conjugacy; we thus use backpropagation to ob-
tain a MAP estimate for it as well as various hyperparam-
eters. We then combine the treatment of the two sources

∗These authors contributed equally.

Bayesian Approaches to Distribution Regression

of uncertainty into a fully Bayesian model and use Hamil-
tonian Monte Carlo for efﬁcient inference. Depending on
the inferential goals, each model can be useful. We demon-
strate our approaches on an illustrative toy problem as well
as a challenging real-world age estimation task.

2 BACKGROUND

2.1 Problem Overview

Distribution regression is the task of learning a classiﬁer or
a regression function that maps probability distributions to
labels. The challenge of distribution regression goes be-
yond the standard supervised learning setting: we do not
have access to exact input-output pairs since the true in-
puts, probability distributions, are observed only through
samples from that distribution:

,

(cid:17)

(cid:16)

(cid:17)

(cid:16)

(1)

, . . . ,

N1
j=1, y1

Nn
j=1, yn

xi
j}
{

xn
j }
{

x1
j }
{
Ni
xi
j=1 has a label yi along with Ni indi-
so that each bag
j}
{
vidual observations xi
. We assume that the observa-
j ∈ X
Ni
tions
j=1 are i.i.d. samples from some unobserved dis-
tribution Pi, and that the true label yi depends only on Pi.
We wish to avoid making any strong parametric assump-
tions on the Pi. For the present work, we will assume the
labels yi are real-valued; Appendix B shows an extension
to binary classiﬁcation. We typically take the observation
to be a subset of Rp, but it could easily be a struc-
space
tured domain such as text or images, since we access it only
through a kernel (for examples, see e.g. G¨artner, 2008).

X

X × X →

We consider the standard approach to distribution regres-
sion, which relies on kernel mean embeddings and ker-
nel ridge regression. For any positive deﬁnite kernel func-
tion k :
R, there exists a unique reproduc-
k, a possibly inﬁnite-
ing kernel Hilbert space (RKHS)
H
dimensional space of functions f :
R where eval-
uation can be written as an inner product, and in particu-
lar f (x) =
k, x
Hk for all f
. Here
(cid:104)
k is a function of one argument, y
, x)
k(
·

∈ X
(cid:55)→
, let us deﬁne the kernel

, x)
(cid:105)
·

k(y, x).

X →

∈ H

Given a probability measure P on
mean embedding into

f, k(

∈ H

k as

X

H

k (
·

(cid:90)

µP =

, x) P(dx)

k.

∈ H

(2)

H

k(x, x)P(dx) <

Notice that µP serves as a high- or inﬁnite-dimensional
vector representation of P. For the kernel mean embed-
ding of P into
it sufﬁces that
k to be well-deﬁned,
, which is trivially satisﬁed for all
P if k is bounded. Analogously to the reproducing prop-
(cid:82) (cid:112)
erty of RKHS, µP represents the expectation function on
Hk . For so-called characteris-
H
(cid:105)
tic kernels (Sriperumbudur et al., 2010), every probability
measure has a unique embedding, and thus µP completely
determines the corresponding probability measure.

h(x)P(dx) =

h, µP
(cid:104)

∞

k:

(cid:82)

2.2 Estimating Mean Embeddings

For a set of samples
ical estimator of µP,

xj
{
µP

n
j=1 drawn iid from P, the empir-
}
∈ H

k, is given by

µP = µ(cid:98)P =

(cid:90)

(cid:99)

(cid:99)
, x) ˆP(dx) =
k (
·

1
n

k(

, xj).
·

(3)

n

j=1
(cid:88)

This is the standard estimator used by previous distribution
regression approaches, which the reproducing property of

k shows us corresponds to the kernel

H

µPi,
(cid:104)

µPj(cid:105)

Hk =

1
NiNj

Ni

Nj

(cid:96)=1
(cid:88)

r=1
(cid:88)

k(xi

(cid:96), xj

r).

(4)

(cid:100)

(cid:100)

But (3) is an empirical mean estimator in a high- or inﬁnite-
dimensional space, and is thus subject to the well-known
Stein phenomenon, so that its performance is dominated
by the James-Stein shrinkage estimators. Indeed, Muandet
et al. (2014) studied shrinkage estimators for mean embed-
dings, which can result in substantially improved perfor-
mance for some tasks (Ramdas and Wehbe, 2015). Flax-
man et al. (2016) proposed a Bayesian analogue of shrink-
age estimators, which we now review.

·

H

∼ N

)) on

µP(x)

∈ H
|

This approach consists of (1) a Gaussian Process prior
µP
k, where r is selected to en-
,
(m0, r(
·
∼ GP
sure that µP
k almost surely and (2) a normal likeli-
(µP(x), Σ). Here, conjugacy
µP(x)
hood
of the prior and the likelihood leads to a Gaussian process
posterior on the true embedding µP, given that we have ob-
(cid:99)
µP at some set of locations x. The posterior mean
served
is then essentially identical to a particular shrinkage esti-
mator of Muandet et al. (2014), but the method described
here has the extra advantage of a closed form uncertainty
estimate, which we utilise in our distributional approach.
For the choice of r, we use a Gaussian RBF kernel k, and
choose either r = k or, following Flaxman et al. (2016),
r(x, x(cid:48)) =
k(x, z) k(z, x(cid:48)) ν(dz) where ν is proportional
to a Gaussian measure. For details of our choices, and why
they are sufﬁcient for our purposes, see Appendix A.

(cid:99)

(cid:82)

This model accounts for the uncertainty based on the num-
ber of samples Ni, shrinking the embeddings for small
sample sizes more. As we will see, this is essential in
the context of distribution regression, particularly when bag
sizes are imbalanced.

2.3 Standard Approaches to Distribution Regression

Following Sz´abo et al. (2016), assume that the probability
distributions Pi are each drawn randomly from some un-
known meta-distribution over probability distributions, and
take a two-stage approach, illustrated as in Figure 1. De-
noting the feature map k(
k by φ(x), one uses the
, x)
·
empirical kernel mean estimator (3) to separately estimate

∈ H

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

Figure 1: Each bag is summarised by a kernel mean embed-
k; a regression function f :
ding µi
R predicts
labels yi
R. We propose a Bayesian approach to propa-
gate uncertainty due to the number of samples in each bag,
obtaining posterior credible intervals illustrated in grey.

∈ H
∈

→

H

k

the mean of each group:

1
N1

N1

j=1
(cid:88)

µ1 =

(cid:99)

φ(x1

j ),

. . . ,

µn =

φ(xn

j ).

(5)

1
Nn

Nn

i=1
(cid:88)

Next, one uses kernel ridge regression (Saunders et al.,
1998) to learn a function f :
R, by minimizing
the squared loss with an RKHS complexity penalty:

k
H

→

(cid:99)

ˆf = argmin
f ∈HK

(yi

f (

µi))2 + λ
(cid:107)

f

2
HK .
(cid:107)

−

i
(cid:88)
k
× H

(cid:98)

k

H

→

Here K :
R is a “second-level” kernel on
mean embeddings. If K is a linear kernel on the RKHS
k, then the resulting method can be interpreted as a linear
H
(ridge) regression on mean embeddings, which are them-
selves nonlinear transformations of the inputs. A nonlin-
ear second-level kernel on
k sometimes improves perfor-
mance (Muandet et al., 2012; Sz´abo et al., 2016).

H

Distribution regression as described is not scalable for
even modestly-sized datasets, as computing each of the
(n2) entries of the relevant kernel matrix requires time
O
(NiNj). Many applications have thus used variants of
O
random Fourier features (Rahimi and Recht, 2007). In this
paper we instead expand in terms of landmark points drawn
randomly from the observations, yielding radial basis net-
works (Broomhead and Lowe, 1988) with mean pooling.

3 MODELS

Figure 2: Our baseline model, a RBF network for distri-
bution regression. Xi represents the matrix of samples for
bag i, while k(Xi, u(cid:96)) represents the element wise opera-
tion on each row of Xi, with b representing the batch size
for stochastic gradient descent.

begin with a non-Bayesian RBF network formulation of
the standard approach to distribution regression as a base-
line, before reﬁning this approach to better propagate un-
certainty in bag size, as well as model parameters.

3.1 Baseline Model

The baseline RBF network formulation we employ here
is a variation of the approaches of Broomhead and Lowe
(1988), Que and Belkin (2016), Law et al. (2017), and Za-
heer et al. (2017). As shown in Figure 2, the initial input
is a minibatch consisting of several bags Xi, each contain-
ing Ni points. Each point is then converted to an explicit
featurisation, taking the role of φ in (5), by a radial basis
layer: xi

Rp is mapped to

j ∈

φ(xi

j) = [k(xi

j, u1), . . . , k(xi

j, ud)](cid:62)

d

R

∈

u(cid:96)
{

Ni
j=1 φ(xi

d
where u =
(cid:96)=1 are landmark points. A mean pool-
}
ing layer yields the estimated mean embedding ˆµi corre-
sponding to each of the bags j represented in the minibatch,
where ˆµi = 1
j).1 Finally, a fully connected
Ni
output layer gives real-valued labels ˆyi = βT ˆµi + b. As a
loss function we use the mean square error 1
yi)2.
n
For learning, we use backpropagation with the Adam opti-
mizer (Kingma and Ba, 2015). To regularise the network,
we use early stopping on a validation set, as well as an L2
penalty corresponding to a normal prior on β.

i(ˆyi

(cid:80)

(cid:80)

−

We consider here three different Bayesian models, with
each model encoding different types of uncertainty. We

1In the implementation, we stack all of the bags Xi into a
j Nj × d for the ﬁrst layer, then perform

single matrix of size (cid:80)
pooling via sparse matrix multiplication.

Bayesian Approaches to Distribution Regression

variance, in order to propagate this information regarding
uncertainty from the bag size through the model. Bayesian
tools provide a natural framework for this problem.

We can use the Bayesian nonparametric prior over kernel
mean embeddings (Flaxman et al., 2016) described in Sec-
tion 2.2, and observe the empirical embeddings at the land-
mark points ui. For ui, we take a ﬁxed set of landmarks,
which we can choose via k-means clustering or sample
without replacement (Que and Belkin, 2016). Using the
conjugacy of the model to the Gaussian process prior µi

∼
(m0, ηr(., .)), we obtain a closed-form posterior Gaus-

GP
sian process whose evaluation at points h =

hs
{

nh
s=1 is:
}

µi(h)

xi

|

∼ N

Rh (R + Σi/Ni)−1 (ˆµi

m0) + m0,

−

(cid:16)

Rhh

Rh (R + Σi/Ni)−1 R(cid:62)
h

{

(cid:17)

xi
j}

−
where Rst = ηr(us, ut), (Rhh)st = ηr(hs, ht), (Rh)st =
Ni
ηr(hs, ut), and xi denotes the set
j=1. We take the
prior mean m0 to be the average of the ˆµi; under a lin-
ear kernel K, this means we shrink predictions towards the
mean prediction. Note η essentially controls the strength of
the shrinkage: a smaller η means we shrink more strongly
towards m0. We take Σi to be the average of the empirical
Ni
covariance of
j=1 across all bags, to avoid poor es-
timation of Σi for smaller bags. More intuition about the
behaviour of this estimator can be found in Appendix C.
Now, supposing we have normal observation error σ2, and
use a linear kernel as our second level kernel K, we have:

ϕ(xi
j)
}

{

|

yi

(6)

∼ N

µi, f

∈ H

f, µi
(cid:104)

Hk , σ2
(cid:105)
where f
k. Clearly, this is difﬁcult to work with;
(cid:1)
s
hence we parameterise f as f =
, z(cid:96)), where
(cid:96)=1 α(cid:96)k(
·
s
z =
(cid:96)=1 is a set of landmark points for f , which we
}
can learn or ﬁx. (Appendix D gives a motivation for this
approximation using the representer theorem.) Using the
reproducing property, our likelihood model becomes:

z(cid:96)
{

(cid:80)

(cid:0)

yi

µi, α

|

∼ N

αTµi(z), σ2

(7)

where µi(z) = [µi(z1), . . . , µi(zs)](cid:62). For ﬁxed α and z
(cid:0)
we can analytically integrate out the dependence on µi, and
the predictive distribution of a bag label becomes

(cid:1)

(ξα

i , να
i )

yi

xi, α

|

∼ N
i = α(cid:62)Rz
ξα

R +

(cid:18)

Σi
Ni (cid:19)
Rz

(cid:18)

R +

−

−1

Σi
Ni (cid:19)

i = αT
να

Rzz

(cid:32)

−

−1(ˆµi

m0) + αTm0

RT
z

α + σ2.

(cid:33)

(0, ρ2K −1

∼ N

The prior α
trix on z, gives the standard regularisation on f of
The log-likelihood objective becomes
i )2
ξα

z ), where Kz is the kernel ma-
2
Hk .
(cid:107)

f
(cid:107)

(yi

n

−
ξα
i

αTKzα
2ρ2

.

+

(cid:41)

log να

i +

1
2

i=1 (cid:40)
(cid:88)

Figure 3: Our Bayesian mean shrinkage pooling model.
This diagram takes m0 = 0, η = 1 and u = z, so that
R = Rz = Rzz, and Kz = K.

3.2 Bayesian Linear Regression Model

The most obvious approach to adding uncertainty to the
model of Section 3.1 is to encode uncertainty over regres-
sion parameters β only, as follows:

β

∼ N

(0, ρ2)

yi

xi, β

|

∼ N

(βT ˆµi, σ2).

This is essentially Bayesian linear regression on the empiri-
cal mean embeddings, and is closely related to the model of
Flaxman et al. (2015). Here, we are working directly with
the ﬁnite-dimensional ˆµi, unlike the inﬁnite-dimensional
µi before. Due to the conjugacy of the model, we can eas-
xi, integrating out
ily obtain the predictive distribution yi
the uncertainty over β. This provides us with uncertainty
intervals for the predictions yi.

|

For model tuning, we can maximise the model evidence,
i.e. the marginal log-likelihood (see Bishop (2006) for de-
tails), and use backpropagation through the network to
learn σ and ρ and any kernel parameters of interest.2

3.3 Bayesian Mean Shrinkage Model

A shortcoming of the prior models, and of the standard ap-
proach in Sz´abo et al. (2016), is that they ignore uncertainty
in the ﬁrst level of estimation due to varying number of
samples in each bag. Ideally we would estimate not just the
mean embedding per bag, but also a measure of the sample

2Note that unlike the other models considered in this paper,
we cannot easily do minibatch stochastic gradient descent, as the
marginal log-likelihood does not decompose for each individual
data point.

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

We can use backpropagation to learn the parameters α, σ,
and if we wish η, z, and any kernel parameters. The full
model is illustrated in Figure 3. This approach allows us to
directly encode uncertainty based on bag size in the objec-
tive function, and gives probabilistic predictions.

3.4 Bayesian Distribution Regression

It is natural to combine the two Bayesian models above,
fully propagating uncertainty in estimation of the mean
embedding and of the regression coefﬁcients α. Unfortu-
nately, conjugate Bayesian inference is no longer available.
Thus, we consider a Markov Chain Monte Carlo (MCMC)
sampling based approach, and here use Hamiltonian Monte
Carlo (HMC) for efﬁcient inference, though any MCMC-
type scheme would work. Whereas inference above used
gradient descent to maximise the marginal likelihood, with
the gradient calculated using automatic differentiation, here
we use automatic differentiation to calculate the gradient of
the joint log-likelihood and follow this gradient as we per-
form sampling over the parameters we wish to infer.

We can still exploit the conjugacy of the mean shrinkage
layer, obtaining an analytic posterior over the mean em-
beddings. Conditional on the mean embeddings, we have
a Bayesian linear regression model with parameters α. We
sample this model with the NUTS HMC sampler (Hoffman
and Gelman, 2014; Stan Development Team, 2014).

4 RELATED WORK

As previously mentioned, Sz´abo et al. (2016) provides
a thorough learning-theoretic analysis of the regression
model discussed in Section 2.3. This formalism consid-
ering a kernel method on distributions using their embed-
ding representations, or various scalable approximations
to it, has been widely applied (e.g. Muandet et al., 2012;
Yoshikawa et al., 2014; Flaxman et al., 2015; Jitkrittum
et al., 2015; Lopez-Paz et al., 2015; Mitrovic et al., 2016).
There are also several other notions of similarities on distri-
butions in use (not necessarily falling within the framework
of kernel methods and RKHSs), as well as local smoothing
approaches, mostly based on estimates of various probabil-
ity metrics (Moreno et al., 2003; Jebara et al., 2004; P´oczos
et al., 2011; Oliva et al., 2013; Poczos et al., 2013; Kusner
et al., 2015). For a partial overview, see Sutherland (2016).

Other related problems of learning on instances with
group-level labels include learning with label proportions
(Quadrianto et al., 2009; Patrini et al., 2014), ecological
inference (King, 1997; Gelman et al., 2001), pointillistic
pattern search (Ma et al., 2015), multiple instance learning
(Dietterich et al., 1997; K¨uck and de Freitas, 2005; Zhou
et al., 2009; Krummenacher et al., 2013) and learning with
sets (Zaheer et al., 2017).3

3For more, also see giorgiopatrini.org/nips15workshop.

There have also been some Bayesian approaches in related
contexts, though most do not follow our setting where the
label is a function of the underlying distribution rather than
the observed sample set. K¨uck and de Freitas (2005) con-
sider an MCMC method with group-level labels but focus
on individual-level classiﬁers, while Jackson et al. (2006)
use hierarchical Bayesian models on both individual-level
and aggregate data for ecological inference.

Jitkrittum et al. (2015) and Flaxman et al. (2015) quantify
the uncertainty of distribution regression models by inter-
preting the kernel ridge regression on embeddings as Gaus-
sian process regression. However, the former’s setting has
no uncertainty in the mean embeddings, while the latter’s
treats empirical embeddings as ﬁxed inputs to the learning
problem (as in Section 3.2).

There has also been generic work on input uncertainty
in Gaussian process regression (Girard, 2004; Damianou
et al., 2016). These methods could provide a framework
towards allowing for second-level kernels in our models.
One could also, though, consider regression with uncertain
inputs as a special case of distribution regression, where the
label is a function of the distribution’s mean and Ni = 1.

5 EXPERIMENTS

(cid:82)

We will now demonstrate our various Bayesian approaches:
the mean-shrinkage pooling method with r = k (shrink-
k(x, z)k(z, x(cid:48))ν(dz) for ν pro-
age) and with r(x, x(cid:48)) =
portional to a Gaussian measure (shrinkageC), Bayesian
linear regression (BLR), and the full Bayesian distribu-
tion regression model with r = k (BDR). We also com-
pare the non-Bayesian baselines RBF network (Section 3.1)
and freq-shrinkage, which uses the shrinkage estimator of
Muandet et al. (2014) to estimate mean embeddings. Code
for our methods and to reproduce the experiments is avail-
able at https://github.com/hcllaw/bdr.

We ﬁrst demonstrate the characteristics of our models on a
synthetic dataset, and then evaluate them on a real life age
prediction problem. Throughout, for simplicity, we take
u = z, i.e. R = Rz = Rzz, and Kz = K – although
u and z could be different, with z learnt. Here k is the
standard RBF kernel. We tune the learning rate, number
of landmarks, bandwidth of the kernel and regularisation
parameters on a validation set. For BDR, we use weakly
informative normal priors (possibly truncated at zero); for
other models, we learn the remaining parameters.

5.1 Gamma Synthetic Data

We create a synthetic dataset by repeatedly sampling from
the following hierarchical model, where yi is the label for
the ith bag, each xi
R5 has entries i.i.d. according to
the given distribution, and ε is an added noise term which

j ∈

Bayesian Approaches to Distribution Regression

ated dataset, 25% of the bags have Ni = 20, and 25% have
Ni = 100. Among the other half of the data, we vary the
ratio of Ni = 5 and Ni = 1 000 bags to demonstrate the
methods’ efﬁcacy at dealing with varied bag sizes: we let
s5 be the overall percentage of bags with Ni = 5, ranging
from s5 = 0 (in which case no bags have size Ni = 5) to
s5 = 50 (in which case 50% of the overall bags have size
Ni = 5). Here we do not add additional noise: ε = 0.

Results are shown in Figure 4. BDR and shrinkage meth-
ods, which take into account bag size uncertainty, per-
form well here compared to the other methods. The full
BDR model very slightly outperforms the Bayesian shrink-
age models in both likelihood and in mean-squared error;
frequentist shrinkage slightly outperforms the Bayesian
shrinkage models in MSE, likely because it is tuned for that
metric. We also see that the choice of r affects the results;
r = k does somewhat better.

Figure 5 demonstrates in more detail the difference be-
tween these models. It shows test set predictions of each
model on the bags of different sizes. Here, we can see
explicitly that the shrinkage and BDR models are able to
take into account the bag size, with decreasing variance
for larger bag sizes, while the BLR model gives the same
variance for all outputs. Furthermore, the shrinkage and
BDR models can shrink their predictions towards the mean
more for smaller bags than larger ones: this improves per-
formance on the small bags while still allowing for good
predictions on large bags, contrary to the BLR model.

Fixed bag size: Uncertainty in the regression model.
The previous experiment showed the efﬁcacy of the shrink-
age estimator in our models, but demonstrated little gain
from posterior inference for regression weights β over their
MAP estimates, i.e. there is no discernible improvement of
BLR over RBF network. To isolate the effect of quantify-
ing uncertainty in the regression model, we now consider
the case where there is no variation in bag size at all and
normal noise is added onto the observations. In particular
we take Ni = 1000 and ε
(0, 1), and sample land-
marks randomly from the training set.

∼ N

Results are shown in Table 1. Here, BLR or BDR outper-
form all other methods on all runs, highlighting that uncer-
tainty in the regression model is also important for predic-
tive performance. Importantly, the BDR method performs
well in this regime as well as in the previous one.

5.2

IMDb-WIKI: Age Estimation

We now demonstrate our methods on a celebrity age es-
timation problem, using the IMDb-WIKI database (Rothe
et al., 2016) which consists of 397 949 images of 19 545
celebrities4, with corresponding age labels. This database

4We used only the IMDb images, and removed some implau-
sible images, including one of a cat and several of people with

Figure 4: Top: negative log-likelihood. Bottom: mean-
squared error. For context, performance of the Bayes-
optimal predictor is also shown, and for NLL ‘uniform’
shows the performance of a uniform prediction on the pos-
sible labels. For MSE, the constant overall mean label pre-
dictor achieves about 1.3.

differs for the two experiments below:

yi

yi

∼
iid
∼

Uniform(4, 8)
1
1
Γ
2
yi (cid:20)

yi
2

(cid:18)

,

(cid:19)(cid:21)

xi
j

(cid:2)

(cid:96) |
(cid:3)

+ ε for j

[Ni], (cid:96)

[5].

∈

∈

In these experiments, we generate 1 000 bags for training,
500 bags for a validation set for parameter tuning, 500
bags to use for early-stopping of the models, and 1 000
bags for testing. Tuning is performed to maximize log-
likelihoods for Bayesian models, MSE for non-Bayesian
models. Landmark points u are chosen via k-means (ﬁxed
across all models). We also show results of the Bayes-
optimal model, which gives true posteriors according to
the data-generating process; this is the best performance
any model could hope to achieve. Our learning models,
which treat the inputs as ﬁve-dimensional, fully nonpara-
metric distributions, are at a substantial disadvantage even
in how they view the data compared to this true model.

Varying bag size: Uncertainty in the inputs.
In order
to study the behaviour of our models with varying bag size,
we ﬁx four sizes Ni
. For each gener-

5, 20, 100, 1 000
}

∈ {

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

Figure 5: Predictions for the varying bag size experiment of Section 5.1. Each column corresponds to a single prediction
method. Each point in an image represents a single bag, with its horizontal position the true label yi, and its vertical
position the predicted label. The black lines show theoretical perfect predictions. The rows represent different subsets of
the data: the ﬁrst row shows all bags, the second only bags with Ni = 5, and so on. Colours represent the predictive
standard deviation of each point.

Bayesian Approaches to Distribution Regression

Table 1: Results on the ﬁxed bag size dataset, over
10 dataset draws (standard deviations in parentheses).
BLR/BDR perform best on all runs in both metrics.

METHOD

MSE

NLL

Optimal
RBF network
freq-shrinkage
shrinkage
shrinkageC
BLR
BDR

0.170 (0.009)
0.235 (0.014)
0.232 (0.012)
0.237 (0.014)
0.236 (0.013)
0.228 (0.012)
0.227 (0.012)

0.401 (0.018)
–
–
0.703 (0.027)
0.700 (0.029)
0.681 (0.025)
0.683 (0.025)

Table 2: Results on the grouped IMDb-WIKI dataset over
ten runs (standard deviations in parentheses). Here shrink-
age methods perform the best across all 10 runs.

METHOD

RMSE

NLL

CNN
RBF network
freq-shrinkage
shrinkage
BLR

10.25 (0.22)
9.51 (0.20)
9.22 (0.19)
9.28 (0.20)
9.55 (0.19)

3.80 (0.034)
–
–
3.54 (0.021)
3.68 (0.021)

was constructed by crawling IMDb for images of its most
popular actors and directors, with potentially many images
for each celebrity over time. Rothe et al. (2016) use a con-
volutional neural network (CNN) with a VGG-16 architec-
ture to perform 101-way classiﬁcation, with one class cor-
responding to each age in

0, . . . , 100

{

.
}

We take a different approach, and assume that we are given
several images of a single individual (i.e. samples from
the distribution of celebrity images), and are asked to pre-
dict their mean age based on several pictures. For example,
we have 757 images of Brad Pitt from age 27 up to 51,
while we have only 13 images of Chelsea Peretti at ages 35
and 37. Note that 22.5% of bags have only a single image.
We obtain 19 545 bags, with each bag containing between
1 and 796 images of a particular celebrity, and the corre-
sponding bag label calculated from the average of the age
labels of the images inside each bag.

In particular, we use the representation ϕ(x) learnt by the
CNN in Rothe et al. (2016), where ϕ(x) : R256×256
→
R4096 maps from the pixel space of images to the CNN’s
last hidden layer. With these new representations, we can
now treat them as inputs to our radial basis network, shrink-
age (taking r = k here) and BLR models. Although we
could also use the full BDR model here, due to the compu-
tational time and memory required to perform proper pa-

supposedly negative age, or ages of several hundred years.

rameter tuning, we relegate this to a later study.

We use 9 820 bags for training, 2 948 bags for early stop-
ping, 2 946 for validation and 3 928 for testing. Landmarks
are sampled without replacement from the training set.

We repeat the experiment on 10 different splits of the data,
and report the results in Table 2. The baseline CNN results
give performance by averaging the predictive distribution
from the model of Rothe et al. (2016) for each image of a
bag; note that this model was trained on all of the images
used here. From Table 2, we can see that the shrinkage
methods have the best performance; they outperforms all
other methods in all 10 splits of the dataset, in both met-
rics. Non-Bayesian shrinkage again yields slightly better
RMSEs, likely because it is tuned for that metric. This
demonstrates that modelling bag size uncertainty is vital.

6 CONCLUSION

Supervised learning on groups of observations using ker-
nel mean embeddings typically disregards sampling vari-
ability within groups. To handle this problem, we con-
struct Bayesian approaches to modelling kernel mean em-
beddings within a regression model, and investigate advan-
tages of uncertainty propagation within different compo-
nents of the resulting distribution regression. The ability
to take into account the uncertainty in mean embedding es-
timates is demonstrated to be key for constructing mod-
els with good predictive performance when group sizes are
highly imbalanced. We also demonstrate that the results of
a complex neural network model for age estimation can be
improved by shrinkage.

Our models employ a neural network formulation to pro-
vide more expressive feature representations and learn dis-
criminative embeddings. Doing so makes our model easy
to extend to more complicated featurisations than the sim-
ple RBF network used here. By training with backpropa-
gation, or via approximate Bayesian methods such as vari-
ational inference, we can easily ‘learn the kernel’ within
our framework, for example ﬁne-tuning the deep network
of Section 5.2 rather than using a pre-trained model. We
can also apply our networks to structured settings, learning
regression functions on sets of images, audio, or text. Such
models naturally ﬁt into the empirical Bayes framework.

On the other hand, we might extend our model to more
Bayesian feature learning by placing priors over the kernel
hyperparameters, building on classic work on variational
approaches (Barber and Schottky, 1998) and fully Bayesian
inference (Andrieu et al., 2001) in RBF networks. Such
approaches are also possible using other featurisations, e.g.
random Fourier features (as in Oliva et al., 2015).

Future distribution regression approaches will need to ac-
count for uncertainty in observation of the distribution. Our
methods provide a strong, generic building block to do so.

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

References

Christophe Andrieu, Nando De Freitas, and Arnaud
Doucet. Robust full bayesian learning for radial ba-
sis networks. Neural Computation, 13(10):2359–2407,
2001.

David Barber and Bernhard Schottky. Radial basis func-
tions: a bayesian treatment. NIPS, pages 402–408, 1998.
C.M. Bishop. Pattern recognition and machine learning.

Springer New York, 2006.

David S Broomhead and David Lowe. Radial basis func-
tions, multi-variable functional interpolation and adap-
tive networks. Technical report, DTIC Document, 1988.

Andreas C. Damianou, Michalis K. Titsias, and Neil D.
Lawrence. Variational inference for latent variables and
uncertain inputs in Gaussian processes. JMLR, 17(42):
1–62, 2016.

Thomas G Dietterich, Richard H Lathrop, and Tom´as
Lozano-P´erez. Solving the multiple instance problem
with axis-parallel rectangles. Artiﬁcial intelligence, 89
(1):31–71, 1997.

Seth Flaxman, Yu-Xiang Wang, and Alexander J Smola.
Who supported Obama in 2012?: Ecological inference
through distribution regression. In KDD, pages 289–298.
ACM, 2015.

Seth Flaxman, Dino Sejdinovic, John P. Cunningham, and
Sarah Filippi. Bayesian learning of kernel embeddings.
In UAI, 2016.

Seth Flaxman, Dougal J. Sutherland, Yu-Xiang Wang,
and Yee-Whye Teh. Understanding the 2016 US pres-
inference and dis-
idential election using ecological
tribution regression with census microdata.
2016.
arXiv:1611.03787.

Thomas G¨artner. Kernels for Structured Data, volume 72.
World Scientiﬁc, Series in Machine Perception and Arti-
ﬁcial Intelligence, 2008.

Andrew Gelman, David K Park, Stephen Ansolabehere,
Phillip N Price, and Lorraine C Minnite. Models, as-
sumptions and model checking in ecological regressions.
Journal of the Royal Statistical Society: Series A (Statis-
tics in Society), 164(1):101–118, 2001.

Agathe Girard. Approximate methods for propagation of
uncertainty with Gaussian process models. PhD thesis,
University of Glasgow, 2004.

Matthew D. Hoffman and Andrew Gelman. The no-U-turn
sampler: Adaptively setting path lengths in Hamiltonian
Monte Carlo. JMLR, pages 1593–1623, 2014.

Christopher Jackson, Nicky Best, and Sylvia Richardson.
Improving ecological inference using individual-level
data. Statistics in medicine, 25(12):2136–2159, 2006.

Tony Jebara, Risi Imre Kondor, and Andrew Howard. Prob-

ability product kernels. JMLR, 5:819–844, 2004.

Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess,
S. M. Ali Eslami, Balaji Lakshminarayanan, Dino Se-
jdinovic, and Zolt´an Szab´o. Kernel-Based Just-In-Time
Learning for Passing Expectation Propagation Mes-
sages. In UAI, 2015.

Gary King. A Solution to the Ecological Inference Problem.
Princeton University Press, 1997. ISBN 0691012407.

Diederik Kingma and Jimmy Ba. Adam: A method
In ICLR, 2015.

stochastic optimization.

for
arXiv:1412.6980.

Gabriel Krummenacher, Cheng Soon Ong, and Joachim M
Buhmann. Ellipsoidal multiple instance learning.
In
ICML (2), pages 73–81, 2013.

Hendrik K¨uck and Nando de Freitas. Learning about in-
dividuals from group statistics. In UAI, pages 332–339,
2005.

Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Wein-
berger. From word embeddings to document distances.
In ICML, pages 957–966, 2015.

H. C. L. Law, C. Yau, and D. Sejdinovic. Testing and learn-
ing on distributions with symmetric noise invariance. In
NIPS, 2017. arXiv:1703.07596.

Bernhard
Towards a learn-

David Lopez-Paz, Krikamol Muandet,

Sch¨olkopf, and Ilya Tolstikhin.
ing theory of cause-effect inference. In ICML, 2015.
Milan Luki´c and Jay Beder. Stochastic processes with sam-
ple paths in reproducing kernel hilbert spaces. Trans-
actions of the American Mathematical Society, 353(10):
3945–3969, 2001.

Yifei Ma, Dougal J. Sutherland, Roman Garnett, and Jeff
In AIS-

Schneider. Active pointillistic pattern search.
TATS, 2015.

J. Mitrovic, D. Sejdinovic, and Y.W. Teh. DR-ABC:
Approximate Bayesian Computation with Kernel-Based
In ICML, pages 1482–1491,
Distribution Regression.
2016.

Pedro J Moreno, Purdy P Ho, and Nuno Vasconcelos. A
Kullback-Leibler divergence based kernel for SVM clas-
siﬁcation in multimedia applications. In NIPS, 2003.
Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo,
Learning from distribu-
In NIPS, 2012.

and Bernhard Sch¨olkopf.
tions via support measure machines.
arXiv:1202.6504.

Krikamol Muandet, Kenji Fukumizu, Bharath Sriperum-
budur, Arthur Gretton, and Bernhard Schoelkopf. Kernel
mean estimation and stein effect. In ICML, 2014.

Michelle Ntampaka, Hy Trac, Dougal J. Sutherland,
Nicholas Battaglia, Barnab´as P´oczos, and Jeff Schnei-
A machine learning approach for dynamical
der.
The Astro-
mass measurements of galaxy clusters.
physical Journal, 803(2):50, 2015.
ISSN 1538-4357.
arXiv:1410.0686.

Bayesian Approaches to Distribution Regression

Ingo Steinwart. Convergence types and rates in generic
Karhunen-Lo´eve expansions with applications to sam-
ple path properties. arXiv preprint arXiv:1403.1040v3,
March 2017.

Dougal J. Sutherland. Scalable, Flexible, and Active Learn-
ing on Distributions. PhD thesis, Carnegie Mellon Uni-
versity, 2016.

Zolt´an Sz´abo, Bharath K. Sriperumbudur, Barnab´as
Leraning theory for
JMLR, 17(152):1–40, 2016.

P´oczos, and Arthur Gretton.
distribution regression.
arXiv:1411.2066.

Grace Wahba. Spline models for observational data, vol-

ume 59. Siam, 1990.

Yuya Yoshikawa, Tomoharu Iwata, and Hiroshi Sawada.
Latent support measure machines for bag-of-words data
classiﬁcation. In NIPS, pages 1961–1969, 2014.

Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barn-
abas Poczos, Ruslan Salakhutdinov, and Alexander
Smola. Deep sets. In NIPS, 2017.

Zhi-Hua Zhou, Yu-Yin Sun, and Yu-Feng Li. Multi-
instance learning by treating instances as non-iid sam-
ples. In ICML, 2009.

Michelle Ntampaka, Hy Trac, Dougal J. Sutherland, S. Fro-
menteau, B. Poczos, and Jeff Schneider. Dynamical
mass measurements of contaminated galaxy clusters us-
ing machine learning. The Astrophysical Journal, 831
(2):135, 2016. arXiv:1509.05409.

Junier B Oliva, Barnab´as P´oczos, and Jeff Schneider. Dis-
tribution to distribution regression. In ICML, 2013.

Junier B Oliva, Avinava Dubey, Barnab´as P´oczos, Jeff
Schneider, and Eric P Xing. Bayesian nonparametric
kernel-learning. In AISTATS, 2015. arXiv:1506.08776.

Giorgio Patrini, Richard Nock, Tiberio Caetano, and Paul

Rivera. (Almost) no label no cry. In NIPS. 2014.

Natesh S Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee,
and Robert L Wolpert. Characterizing the function space
for bayesian kernel models. JMLR, 8(Aug):1769–1797,
2007.

Barnab´as P´oczos, Liang Xiong, and Jeff Schneider. Non-
parametric divergence estimation with applications to
machine learning on distributions. In UAI, 2011.

Barnabas Poczos, Aarti Singh, Alessandro Rinaldo,
Distribution-free distribu-
In AISTATS, pages 507–515, 2013.

and Larry Wasserman.
tion regression.
arXiv:1302.0082.

Novi Quadrianto, Alex J Smola, Tiberio S Caetano, and
Quoc V Le. Estimating labels from label proportions.
JMLR, 10:2349–2374, 2009.

Qichao Que and Mikhail Belkin. Back to the future: Radial
basis function networks revisited. In AISTATS, 2016.

Ali Rahimi and Benjamin Recht. Random features for
large-scale kernel machines. In NIPS, pages 1177–1184,
2007.

Aaditya Ramdas and Leila Wehbe. Nonparametric inde-
pendence testing for small sample sizes. In IJCAI, 2015.
arXiv:1406.1922.

Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep
expectation of real and apparent age from a single im-
age without facial landmarks. International Journal of
Computer Vision (IJCV), July 2016.

Craig Saunders, Alexander Gammerman, and Volodya
Vovk. Ridge regression learning algorithm in dual vari-
ables. In ICML, 1998.

Bernhard Sch¨olkopf, Ralf Herbrich, and Alex J. Smola. A

generalized representer theorem. In COLT, 2001.

Bharath K Sriperumbudur, Arthur Gretton, Kenji Fuku-
mizu, Bernhard Sch¨olkopf, and Gert RG Lanckriet.
Hilbert space embeddings and metrics on probability
measures. JMLR, 99:1517–1561, 2010.

Stan Development Team. Stan: A c++ library for proba-
bility and sampling, version 2.5.0, 2014. URL http:
//mc-stan.org/.

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

A Choice of r(

,
·

) to ensure µP
·

k
∈ H

We need to choose an appropriate covariance function r, such that µP
,
for inﬁnite-dimensional RKHSs not sufﬁcient to deﬁne r(
·

k (Wahba, 1990) (but see below). However, we can construct

) = k(
·

,
·

k, where µP

)). In particular, it is
,
(0, r(
∈ H
·
), as draws from this particular prior are no longer in
·

∼ GP

·

H

r(x, y) =

k(x, z)k(z, y)ν(dz)

(cid:90)

(8)

k with probability 1 by the nuclear dominance (Luki´c and
where ν is any ﬁnite measure on
∈ H
Beder, 2001; Pillai et al., 2007) for any stationary kernel k. In particular, Flaxman et al. (2016) provides details when k is
a squared exponential kernel deﬁned by

. This then ensures µP

X

k(x, y) = exp(

(x

1
2

−

y)(cid:62)Σ−1

k (x

y))

−

−

x, y

p

R

∈

) with a non-
and ν(dz) = exp
·
stationary component. In this paper, we take Σk = σ2Ip, where σ2 and (cid:96) are tuning parameters, or parameters that we
learn.

it is proportional to a Gaussian measure on Rd, which provides r(
·

dz, i.e.

−

(cid:17)

(cid:16)

,

||z||2
2
2(cid:96)2

Here, the above holds for a general set of stationary kernels, but note that by taking a convolution of a kernel with itself,
it might make the space of functions that we consider overly smooth (i.e. concentrated on a small part of
k). In this
work, however, we consider only the Gaussian RBF kernel k. In fact, recent work (Steinwart, 2017, Theorem 4.2) actually
shows that in this case, the sample paths almost surely belong to (interpolation) spaces which are inﬁnitesimally larger
than the RKHS of the Gaussian RBF kernel. This suggests that we can choose r to be an RBF kernel with a length scale
that is inﬁnitesimally bigger than that of k; thus, in practice, taking r = k would sufﬁce and we do observe that it actually
performs better (Fig. 4).

H

B Framework for Binary Classiﬁcation

Suppose that our labels yi
for uncertainty in the regression parameters is to use bayesian logistic regression, putting priors on β, i.e.

0, 1
, i.e. we are in a binary classiﬁcation framework. Then a simple approach to accounting
}

∈ {

(0, ρ2)

∼ N

β

yi

∼

Ber(πi), where log

= β(cid:62) ˆµi

πi

1

(cid:18)

−

πi (cid:19)
µi, α, we would not be able to obtain an analytical

however for the mean shrinkage pooling model, if we use the above yi
solution for p(yi

xi, α). Instead we use the probit link function, as given by:

|

|

where Φ denotes the Cumulative Distribution Function (CDF) of a standard normal distribution, with µi(z) =
[µi(z1), . . . , µi(zs)](cid:62). Then as before we have

µi, α) = Φ
P r(yi = 1
|

α(cid:62)µi(z)

(cid:0)

(cid:1)

µi(z)

xi

|

∼ N

(Mi, Ci)

with Mi and Ci as deﬁned in section 3.3. Hence, as before

P r(yi = 1
|

xi, α) =

P r(yi = 1
|

xi)dµi(z)
µi, α)p(µi(z)
|

Φ(α(cid:62)µi(z)) exp

(µi(z)

Mi)(cid:62)C −1

i

(µi(z)

1
2

{−

−

Mi)
}

−

dµi(z)

(with li = µi(z)

Mi) = c

Φ(α(cid:62)(li + Mi)) exp

−

1
2

{−

(li)(cid:62)C −1

i

(li)

dli
}

= P r(Y

α(cid:62)(li + Mi))

≤

(cid:90)
= c

(cid:90)

(cid:90)

Bayesian Approaches to Distribution Regression

Note here Y

(0, 1) and li

(0, Σi) Then expanding and rearranging

∼ N

∼ N

Note that since Y and li independent normal r.v., Y
have:

−

α(cid:62)li

∼ N

xi, α) = P r(Y
P r(yi = 1
|

α(cid:62)li

α(cid:62)Mi)

≤

−
(0, 1 + α(cid:62)Ciα(cid:62)). Let T be standard normal, then we

xi, α) = P r(
P r(yi = 1
|

α(cid:62)Mi)

1 + α(cid:62)Ciα T
α(cid:62)Mi
1 + α(cid:62)Ciα

≤

≤

)

α(cid:62)Mi
(cid:112)
1 + α(cid:62)Ciα (cid:33)

(cid:112)
= P r(T

= Φ

(cid:32)

(cid:112)

Hence, we also have:

xi, α) = 1
P r(yi = 0
|

−

Φ

α(cid:62)Mi
1 + α(cid:62)Ciα (cid:33)

(cid:32)

(cid:112)

Now placing the prior α

(0, ρ2K −1

z ), we have the following MAP objective:

∼ N

J(α) = log

p(α)

n

i=1
(cid:89)

p(yi

xi, α)
|

(cid:35)

=

(1

yi) log(1

Φ

(cid:34)

n

−

i=1
(cid:88)

+yi log(Φ

α(cid:62)Mi
)
1 + α(cid:62)Ciα (cid:33)

−

(cid:32)

(cid:112)

α(cid:62)Mi
1 + α(cid:62)Ciα (cid:33)

) +

1
ρ2 α(cid:62)Kzα

(cid:32)

(cid:112)

xi, α), we can also use this in HMC for BDR.
Since we have an analytical solution for P r(yi = 0
|

C Some more intuition on the shrinkage estimator

In this section, we provide some intuition behind the shrinkage estimator in section 3.3. Here, for simplicity, we choose
Σi = τ 2I for all bag i, and m0 = 0, and consider the case where z = u, i.e. R = Rz = Rzz. We can then see that if R
has eigendecomposition U ΛU T , with Λ = diag(λk), the posterior mean is

so that large eigenvalues, λk
towards 0. Likewise, the posterior variance is

(cid:29)

τ 2/Ni, are essentially unchanged, while small eigenvalues, λk

τ 2/Ni, are shrunk

(cid:28)

U diag

λk
λk + τ 2/Ni (cid:19)

(cid:18)

U T (ˆµi),

U diag

λk

(cid:32)

−

λ2
k
λk + τ 2

Ni (cid:33)

U T = U diag

1
Ni
τ 2 + 1

λk (cid:33)

(cid:32)

U T ;

its eigenvalues also decrease as Ni/τ 2 increases.

D Alternative Motivation for choice of f

Here we provide an alternative motivation for the choice of f =
model with a linear kernel K on µi, where f :

k

R:

k
, zs). First, consider the following Bayesian
s=1 αsk(
·

H

→

yi

µi, f

|

(cid:80)
f (µi), σ2

.

(cid:1)

∼ N

(cid:0)

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

Now considering the log-likelihood of

µ, Y
{

}

=

µi, yi
{

n
i=1 (supposing we have these exact embeddings), we obtain:
}

log p(Y

µ, f ) =
|

n

1
2σ2 (yi

−

−

f (µi))2

i=1
(cid:88)
To avoid over-ﬁtting, we place a Gaussian prior on f , i.e.
likelihood over f

k, we have:

−

∈ H

f ∗ = argminf ∈Hk

1
2σ2 (yi

−

f (µi))2 + λ

f
||

||

Hk

log p(f ) = λ

Hk + c. Minimizing the negative log-

f
||

||

Now this is in the form of an empirical risk minimisation problem. Hence using the representer theorem (Sch¨olkopf et al.,
2001), we have that:

i.e. we have a ﬁnite-dimensional problem to solve. Thus since K is a linear kernel:

f =

γjK(., µj)

yi

µi,

|

µj
{

n
j=1, γ
}

∼ N 

γj

µi, µj

(cid:104)

Hk , σ2
(cid:105)

n

j=1
(cid:88)



.





where

µi, µj

Hk can be thought of as the similarity between distributions.

(cid:104)

(cid:105)

xi, γ). This suggests we need to
Now we have the same
|
integrate out µ1, . . . µn. But it is unclear how to perform this integration, since the µi follow Gaussian process distributions.
Hence we can take an approximation to f , i.e. f =
, zs), which would essentially give us a dual method with
·
a sparse approximation to f .

posterior as in Section 3.3, and we would like to compute p(yi

k
s=1 αsk(

GP

n

i=1
(cid:88)

n

j=1
(cid:88)

(cid:80)

8
1
0
2
 
b
e
F
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
3
9
2
4
0
.
5
0
7
1
:
v
i
X
r
a

Bayesian Approaches to Distribution Regression

Ho Chung Leon Law∗
University of Oxford
ho.law@spc.ox.ac.uk

Dougal J. Sutherland∗
University College London
dougal@gmail.com

Dino Sejdinovic
University of Oxford
dino.sejdinovic@stats.ox.ac.uk

Seth Flaxman
Imperial College London
s.ﬂaxman@imperial.ac.uk

Abstract

Distribution regression has recently attracted
much interest as a generic solution to the prob-
lem of supervised learning where labels are avail-
able at the group level, rather than at the individ-
ual level. Current approaches, however, do not
propagate the uncertainty in observations due to
sampling variability in the groups. This effec-
tively assumes that small and large groups are
estimated equally well, and should have equal
weight in the ﬁnal regression. We account for
this uncertainty with a Bayesian distribution re-
improving the robustness
gression formalism,
and performance of the model when group sizes
vary. We frame our models in a neural network
style, allowing for simple MAP inference using
backpropagation to learn the parameters, as well
as MCMC-based inference which can fully prop-
agate uncertainty. We demonstrate our approach
on illustrative toy datasets, as well as on a chal-
lenging problem of predicting age from images.

1

INTRODUCTION

Distribution regression is the problem of learning a regres-
sion function from samples of a distribution to a single set-
level label. For example, we might attempt to infer the
sentiment of texts based on word-level features, to predict
the label of an image based on small patches, or even per-
form traditional parametric statistical inference by learning
a function from sets of samples to the parameter values.

Recent years have seen wide-ranging applications of this
framework, including inferring summary statistics in Ap-
proximate Bayesian Computation (Mitrovic et al., 2016),
estimating Expectation Propagation messages (Jitkrittum
et al., 2015), predicting the voting behaviour of demo-

Proceedings of the 21st International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS) 2018, Lanzarote, Spain.
PMLR: Volume 84. Copyright 2018 by the author(s).

graphic groups (Flaxman et al., 2015, 2016), and learning
the total mass of dark matter halos from observable galaxy
velocities (Ntampaka et al., 2015, 2016). Closely related
distribution classiﬁcation problems also include identify-
ing the direction of causal relationships from data (Lopez-
Paz et al., 2015) and classifying text based on bags of word
vectors (Yoshikawa et al., 2014; Kusner et al., 2015).

One particularly appealing approach to the distribution re-
gression problem is to represent the input set of samples
by their kernel mean embedding (described in Section 2.1),
where distributions are represented as single points in a re-
producing kernel Hilbert space. Standard kernel methods
can then be applied for distribution regression, classiﬁca-
tion, anomaly detection, and so on. This approach was per-
haps ﬁrst popularized by Muandet et al. (2012); Sz´abo et al.
(2016) provided a recent learning-theoretic analysis.

In this framework, however, each distribution is simply rep-
resented by the empirical mean embedding, ignoring the
fact that large sample sets are much more precisely under-
stood than small ones. Most studies also use point esti-
mates for their regressions, such as kernel ridge regression
or support vector machines, thus ignoring uncertainty both
in the distribution embeddings and in the regression model.

Our Contributions We propose a set of Bayesian ap-
proaches to distribution regression. The simplest method,
similar to that of Flaxman et al. (2015), is to use point
estimates of the input embeddings but account for uncer-
tainty in the regression model with simple Bayesian linear
regression. Alternatively, we can treat uncertainty in the in-
put embeddings but ignore model uncertainty with the pro-
posed Bayesian mean shrinkage model, which builds on a
recently proposed Bayesian nonparametric model of uncer-
tainty in kernel mean embeddings (Flaxman et al., 2016),
and then use a sparse representation of the desired function
in the RKHS for prediction in the regression model. This
model allows for a full account of uncertainty in the mean
embedding, but requires a point estimate of the regression
function for conjugacy; we thus use backpropagation to ob-
tain a MAP estimate for it as well as various hyperparam-
eters. We then combine the treatment of the two sources

∗These authors contributed equally.

Bayesian Approaches to Distribution Regression

of uncertainty into a fully Bayesian model and use Hamil-
tonian Monte Carlo for efﬁcient inference. Depending on
the inferential goals, each model can be useful. We demon-
strate our approaches on an illustrative toy problem as well
as a challenging real-world age estimation task.

2 BACKGROUND

2.1 Problem Overview

Distribution regression is the task of learning a classiﬁer or
a regression function that maps probability distributions to
labels. The challenge of distribution regression goes be-
yond the standard supervised learning setting: we do not
have access to exact input-output pairs since the true in-
puts, probability distributions, are observed only through
samples from that distribution:

,

(cid:17)

(cid:16)

(cid:17)

(cid:16)

(1)

, . . . ,

N1
j=1, y1

Nn
j=1, yn

xi
j}
{

xn
j }
{

x1
j }
{
Ni
xi
j=1 has a label yi along with Ni indi-
so that each bag
j}
{
vidual observations xi
. We assume that the observa-
j ∈ X
Ni
tions
j=1 are i.i.d. samples from some unobserved dis-
tribution Pi, and that the true label yi depends only on Pi.
We wish to avoid making any strong parametric assump-
tions on the Pi. For the present work, we will assume the
labels yi are real-valued; Appendix B shows an extension
to binary classiﬁcation. We typically take the observation
to be a subset of Rp, but it could easily be a struc-
space
tured domain such as text or images, since we access it only
through a kernel (for examples, see e.g. G¨artner, 2008).

X

X × X →

We consider the standard approach to distribution regres-
sion, which relies on kernel mean embeddings and ker-
nel ridge regression. For any positive deﬁnite kernel func-
tion k :
R, there exists a unique reproduc-
k, a possibly inﬁnite-
ing kernel Hilbert space (RKHS)
H
dimensional space of functions f :
R where eval-
uation can be written as an inner product, and in particu-
lar f (x) =
k, x
Hk for all f
. Here
(cid:104)
k is a function of one argument, y
, x)
k(
·

∈ X
(cid:55)→
, let us deﬁne the kernel

, x)
(cid:105)
·

k(y, x).

X →

∈ H

Given a probability measure P on
mean embedding into

f, k(

∈ H

k as

X

H

k (
·

(cid:90)

µP =

, x) P(dx)

k.

∈ H

(2)

H

k(x, x)P(dx) <

Notice that µP serves as a high- or inﬁnite-dimensional
vector representation of P. For the kernel mean embed-
ding of P into
it sufﬁces that
k to be well-deﬁned,
, which is trivially satisﬁed for all
P if k is bounded. Analogously to the reproducing prop-
(cid:82) (cid:112)
erty of RKHS, µP represents the expectation function on
Hk . For so-called characteris-
H
(cid:105)
tic kernels (Sriperumbudur et al., 2010), every probability
measure has a unique embedding, and thus µP completely
determines the corresponding probability measure.

h(x)P(dx) =

h, µP
(cid:104)

∞

k:

(cid:82)

2.2 Estimating Mean Embeddings

For a set of samples
ical estimator of µP,

xj
{
µP

n
j=1 drawn iid from P, the empir-
}
∈ H

k, is given by

µP = µ(cid:98)P =

(cid:90)

(cid:99)

(cid:99)
, x) ˆP(dx) =
k (
·

1
n

k(

, xj).
·

(3)

n

j=1
(cid:88)

This is the standard estimator used by previous distribution
regression approaches, which the reproducing property of

k shows us corresponds to the kernel

H

µPi,
(cid:104)

µPj(cid:105)

Hk =

1
NiNj

Ni

Nj

(cid:96)=1
(cid:88)

r=1
(cid:88)

k(xi

(cid:96), xj

r).

(4)

(cid:100)

(cid:100)

But (3) is an empirical mean estimator in a high- or inﬁnite-
dimensional space, and is thus subject to the well-known
Stein phenomenon, so that its performance is dominated
by the James-Stein shrinkage estimators. Indeed, Muandet
et al. (2014) studied shrinkage estimators for mean embed-
dings, which can result in substantially improved perfor-
mance for some tasks (Ramdas and Wehbe, 2015). Flax-
man et al. (2016) proposed a Bayesian analogue of shrink-
age estimators, which we now review.

·

H

∼ N

)) on

µP(x)

∈ H
|

This approach consists of (1) a Gaussian Process prior
µP
k, where r is selected to en-
,
(m0, r(
·
∼ GP
sure that µP
k almost surely and (2) a normal likeli-
(µP(x), Σ). Here, conjugacy
µP(x)
hood
of the prior and the likelihood leads to a Gaussian process
posterior on the true embedding µP, given that we have ob-
(cid:99)
µP at some set of locations x. The posterior mean
served
is then essentially identical to a particular shrinkage esti-
mator of Muandet et al. (2014), but the method described
here has the extra advantage of a closed form uncertainty
estimate, which we utilise in our distributional approach.
For the choice of r, we use a Gaussian RBF kernel k, and
choose either r = k or, following Flaxman et al. (2016),
r(x, x(cid:48)) =
k(x, z) k(z, x(cid:48)) ν(dz) where ν is proportional
to a Gaussian measure. For details of our choices, and why
they are sufﬁcient for our purposes, see Appendix A.

(cid:99)

(cid:82)

This model accounts for the uncertainty based on the num-
ber of samples Ni, shrinking the embeddings for small
sample sizes more. As we will see, this is essential in
the context of distribution regression, particularly when bag
sizes are imbalanced.

2.3 Standard Approaches to Distribution Regression

Following Sz´abo et al. (2016), assume that the probability
distributions Pi are each drawn randomly from some un-
known meta-distribution over probability distributions, and
take a two-stage approach, illustrated as in Figure 1. De-
noting the feature map k(
k by φ(x), one uses the
, x)
·
empirical kernel mean estimator (3) to separately estimate

∈ H

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

Figure 1: Each bag is summarised by a kernel mean embed-
k; a regression function f :
ding µi
R predicts
labels yi
R. We propose a Bayesian approach to propa-
gate uncertainty due to the number of samples in each bag,
obtaining posterior credible intervals illustrated in grey.

∈ H
∈

→

H

k

the mean of each group:

1
N1

N1

j=1
(cid:88)

µ1 =

(cid:99)

φ(x1

j ),

. . . ,

µn =

φ(xn

j ).

(5)

1
Nn

Nn

i=1
(cid:88)

Next, one uses kernel ridge regression (Saunders et al.,
1998) to learn a function f :
R, by minimizing
the squared loss with an RKHS complexity penalty:

k
H

→

(cid:99)

ˆf = argmin
f ∈HK

(yi

f (

µi))2 + λ
(cid:107)

f

2
HK .
(cid:107)

−

i
(cid:88)
k
× H

(cid:98)

k

H

→

Here K :
R is a “second-level” kernel on
mean embeddings. If K is a linear kernel on the RKHS
k, then the resulting method can be interpreted as a linear
H
(ridge) regression on mean embeddings, which are them-
selves nonlinear transformations of the inputs. A nonlin-
ear second-level kernel on
k sometimes improves perfor-
mance (Muandet et al., 2012; Sz´abo et al., 2016).

H

Distribution regression as described is not scalable for
even modestly-sized datasets, as computing each of the
(n2) entries of the relevant kernel matrix requires time
O
(NiNj). Many applications have thus used variants of
O
random Fourier features (Rahimi and Recht, 2007). In this
paper we instead expand in terms of landmark points drawn
randomly from the observations, yielding radial basis net-
works (Broomhead and Lowe, 1988) with mean pooling.

3 MODELS

Figure 2: Our baseline model, a RBF network for distri-
bution regression. Xi represents the matrix of samples for
bag i, while k(Xi, u(cid:96)) represents the element wise opera-
tion on each row of Xi, with b representing the batch size
for stochastic gradient descent.

begin with a non-Bayesian RBF network formulation of
the standard approach to distribution regression as a base-
line, before reﬁning this approach to better propagate un-
certainty in bag size, as well as model parameters.

3.1 Baseline Model

The baseline RBF network formulation we employ here
is a variation of the approaches of Broomhead and Lowe
(1988), Que and Belkin (2016), Law et al. (2017), and Za-
heer et al. (2017). As shown in Figure 2, the initial input
is a minibatch consisting of several bags Xi, each contain-
ing Ni points. Each point is then converted to an explicit
featurisation, taking the role of φ in (5), by a radial basis
layer: xi

Rp is mapped to

j ∈

φ(xi

j) = [k(xi

j, u1), . . . , k(xi

j, ud)](cid:62)

d

R

∈

u(cid:96)
{

Ni
j=1 φ(xi

d
where u =
(cid:96)=1 are landmark points. A mean pool-
}
ing layer yields the estimated mean embedding ˆµi corre-
sponding to each of the bags j represented in the minibatch,
where ˆµi = 1
j).1 Finally, a fully connected
Ni
output layer gives real-valued labels ˆyi = βT ˆµi + b. As a
loss function we use the mean square error 1
yi)2.
n
For learning, we use backpropagation with the Adam opti-
mizer (Kingma and Ba, 2015). To regularise the network,
we use early stopping on a validation set, as well as an L2
penalty corresponding to a normal prior on β.

i(ˆyi

(cid:80)

(cid:80)

−

We consider here three different Bayesian models, with
each model encoding different types of uncertainty. We

1In the implementation, we stack all of the bags Xi into a
j Nj × d for the ﬁrst layer, then perform

single matrix of size (cid:80)
pooling via sparse matrix multiplication.

Bayesian Approaches to Distribution Regression

variance, in order to propagate this information regarding
uncertainty from the bag size through the model. Bayesian
tools provide a natural framework for this problem.

We can use the Bayesian nonparametric prior over kernel
mean embeddings (Flaxman et al., 2016) described in Sec-
tion 2.2, and observe the empirical embeddings at the land-
mark points ui. For ui, we take a ﬁxed set of landmarks,
which we can choose via k-means clustering or sample
without replacement (Que and Belkin, 2016). Using the
conjugacy of the model to the Gaussian process prior µi

∼
(m0, ηr(., .)), we obtain a closed-form posterior Gaus-

GP
sian process whose evaluation at points h =

hs
{

nh
s=1 is:
}

µi(h)

xi

|

∼ N

Rh (R + Σi/Ni)−1 (ˆµi

m0) + m0,

−

(cid:16)

Rhh

Rh (R + Σi/Ni)−1 R(cid:62)
h

{

(cid:17)

xi
j}

−
where Rst = ηr(us, ut), (Rhh)st = ηr(hs, ht), (Rh)st =
Ni
ηr(hs, ut), and xi denotes the set
j=1. We take the
prior mean m0 to be the average of the ˆµi; under a lin-
ear kernel K, this means we shrink predictions towards the
mean prediction. Note η essentially controls the strength of
the shrinkage: a smaller η means we shrink more strongly
towards m0. We take Σi to be the average of the empirical
Ni
covariance of
j=1 across all bags, to avoid poor es-
timation of Σi for smaller bags. More intuition about the
behaviour of this estimator can be found in Appendix C.
Now, supposing we have normal observation error σ2, and
use a linear kernel as our second level kernel K, we have:

ϕ(xi
j)
}

{

|

yi

(6)

∼ N

µi, f

∈ H

f, µi
(cid:104)

Hk , σ2
(cid:105)
where f
k. Clearly, this is difﬁcult to work with;
(cid:1)
s
hence we parameterise f as f =
, z(cid:96)), where
(cid:96)=1 α(cid:96)k(
·
s
z =
(cid:96)=1 is a set of landmark points for f , which we
}
can learn or ﬁx. (Appendix D gives a motivation for this
approximation using the representer theorem.) Using the
reproducing property, our likelihood model becomes:

z(cid:96)
{

(cid:80)

(cid:0)

yi

µi, α

|

∼ N

αTµi(z), σ2

(7)

where µi(z) = [µi(z1), . . . , µi(zs)](cid:62). For ﬁxed α and z
(cid:0)
we can analytically integrate out the dependence on µi, and
the predictive distribution of a bag label becomes

(cid:1)

(ξα

i , να
i )

yi

xi, α

|

∼ N
i = α(cid:62)Rz
ξα

R +

(cid:18)

Σi
Ni (cid:19)
Rz

(cid:18)

R +

−

−1

Σi
Ni (cid:19)

i = αT
να

Rzz

(cid:32)

−

−1(ˆµi

m0) + αTm0

RT
z

α + σ2.

(cid:33)

(0, ρ2K −1

∼ N

The prior α
trix on z, gives the standard regularisation on f of
The log-likelihood objective becomes
i )2
ξα

z ), where Kz is the kernel ma-
2
Hk .
(cid:107)

f
(cid:107)

(yi

n

−
ξα
i

αTKzα
2ρ2

.

+

(cid:41)

log να

i +

1
2

i=1 (cid:40)
(cid:88)

Figure 3: Our Bayesian mean shrinkage pooling model.
This diagram takes m0 = 0, η = 1 and u = z, so that
R = Rz = Rzz, and Kz = K.

3.2 Bayesian Linear Regression Model

The most obvious approach to adding uncertainty to the
model of Section 3.1 is to encode uncertainty over regres-
sion parameters β only, as follows:

β

∼ N

(0, ρ2)

yi

xi, β

|

∼ N

(βT ˆµi, σ2).

This is essentially Bayesian linear regression on the empiri-
cal mean embeddings, and is closely related to the model of
Flaxman et al. (2015). Here, we are working directly with
the ﬁnite-dimensional ˆµi, unlike the inﬁnite-dimensional
µi before. Due to the conjugacy of the model, we can eas-
xi, integrating out
ily obtain the predictive distribution yi
the uncertainty over β. This provides us with uncertainty
intervals for the predictions yi.

|

For model tuning, we can maximise the model evidence,
i.e. the marginal log-likelihood (see Bishop (2006) for de-
tails), and use backpropagation through the network to
learn σ and ρ and any kernel parameters of interest.2

3.3 Bayesian Mean Shrinkage Model

A shortcoming of the prior models, and of the standard ap-
proach in Sz´abo et al. (2016), is that they ignore uncertainty
in the ﬁrst level of estimation due to varying number of
samples in each bag. Ideally we would estimate not just the
mean embedding per bag, but also a measure of the sample

2Note that unlike the other models considered in this paper,
we cannot easily do minibatch stochastic gradient descent, as the
marginal log-likelihood does not decompose for each individual
data point.

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

We can use backpropagation to learn the parameters α, σ,
and if we wish η, z, and any kernel parameters. The full
model is illustrated in Figure 3. This approach allows us to
directly encode uncertainty based on bag size in the objec-
tive function, and gives probabilistic predictions.

3.4 Bayesian Distribution Regression

It is natural to combine the two Bayesian models above,
fully propagating uncertainty in estimation of the mean
embedding and of the regression coefﬁcients α. Unfortu-
nately, conjugate Bayesian inference is no longer available.
Thus, we consider a Markov Chain Monte Carlo (MCMC)
sampling based approach, and here use Hamiltonian Monte
Carlo (HMC) for efﬁcient inference, though any MCMC-
type scheme would work. Whereas inference above used
gradient descent to maximise the marginal likelihood, with
the gradient calculated using automatic differentiation, here
we use automatic differentiation to calculate the gradient of
the joint log-likelihood and follow this gradient as we per-
form sampling over the parameters we wish to infer.

We can still exploit the conjugacy of the mean shrinkage
layer, obtaining an analytic posterior over the mean em-
beddings. Conditional on the mean embeddings, we have
a Bayesian linear regression model with parameters α. We
sample this model with the NUTS HMC sampler (Hoffman
and Gelman, 2014; Stan Development Team, 2014).

4 RELATED WORK

As previously mentioned, Sz´abo et al. (2016) provides
a thorough learning-theoretic analysis of the regression
model discussed in Section 2.3. This formalism consid-
ering a kernel method on distributions using their embed-
ding representations, or various scalable approximations
to it, has been widely applied (e.g. Muandet et al., 2012;
Yoshikawa et al., 2014; Flaxman et al., 2015; Jitkrittum
et al., 2015; Lopez-Paz et al., 2015; Mitrovic et al., 2016).
There are also several other notions of similarities on distri-
butions in use (not necessarily falling within the framework
of kernel methods and RKHSs), as well as local smoothing
approaches, mostly based on estimates of various probabil-
ity metrics (Moreno et al., 2003; Jebara et al., 2004; P´oczos
et al., 2011; Oliva et al., 2013; Poczos et al., 2013; Kusner
et al., 2015). For a partial overview, see Sutherland (2016).

Other related problems of learning on instances with
group-level labels include learning with label proportions
(Quadrianto et al., 2009; Patrini et al., 2014), ecological
inference (King, 1997; Gelman et al., 2001), pointillistic
pattern search (Ma et al., 2015), multiple instance learning
(Dietterich et al., 1997; K¨uck and de Freitas, 2005; Zhou
et al., 2009; Krummenacher et al., 2013) and learning with
sets (Zaheer et al., 2017).3

3For more, also see giorgiopatrini.org/nips15workshop.

There have also been some Bayesian approaches in related
contexts, though most do not follow our setting where the
label is a function of the underlying distribution rather than
the observed sample set. K¨uck and de Freitas (2005) con-
sider an MCMC method with group-level labels but focus
on individual-level classiﬁers, while Jackson et al. (2006)
use hierarchical Bayesian models on both individual-level
and aggregate data for ecological inference.

Jitkrittum et al. (2015) and Flaxman et al. (2015) quantify
the uncertainty of distribution regression models by inter-
preting the kernel ridge regression on embeddings as Gaus-
sian process regression. However, the former’s setting has
no uncertainty in the mean embeddings, while the latter’s
treats empirical embeddings as ﬁxed inputs to the learning
problem (as in Section 3.2).

There has also been generic work on input uncertainty
in Gaussian process regression (Girard, 2004; Damianou
et al., 2016). These methods could provide a framework
towards allowing for second-level kernels in our models.
One could also, though, consider regression with uncertain
inputs as a special case of distribution regression, where the
label is a function of the distribution’s mean and Ni = 1.

5 EXPERIMENTS

(cid:82)

We will now demonstrate our various Bayesian approaches:
the mean-shrinkage pooling method with r = k (shrink-
k(x, z)k(z, x(cid:48))ν(dz) for ν pro-
age) and with r(x, x(cid:48)) =
portional to a Gaussian measure (shrinkageC), Bayesian
linear regression (BLR), and the full Bayesian distribu-
tion regression model with r = k (BDR). We also com-
pare the non-Bayesian baselines RBF network (Section 3.1)
and freq-shrinkage, which uses the shrinkage estimator of
Muandet et al. (2014) to estimate mean embeddings. Code
for our methods and to reproduce the experiments is avail-
able at https://github.com/hcllaw/bdr.

We ﬁrst demonstrate the characteristics of our models on a
synthetic dataset, and then evaluate them on a real life age
prediction problem. Throughout, for simplicity, we take
u = z, i.e. R = Rz = Rzz, and Kz = K – although
u and z could be different, with z learnt. Here k is the
standard RBF kernel. We tune the learning rate, number
of landmarks, bandwidth of the kernel and regularisation
parameters on a validation set. For BDR, we use weakly
informative normal priors (possibly truncated at zero); for
other models, we learn the remaining parameters.

5.1 Gamma Synthetic Data

We create a synthetic dataset by repeatedly sampling from
the following hierarchical model, where yi is the label for
the ith bag, each xi
R5 has entries i.i.d. according to
the given distribution, and ε is an added noise term which

j ∈

Bayesian Approaches to Distribution Regression

ated dataset, 25% of the bags have Ni = 20, and 25% have
Ni = 100. Among the other half of the data, we vary the
ratio of Ni = 5 and Ni = 1 000 bags to demonstrate the
methods’ efﬁcacy at dealing with varied bag sizes: we let
s5 be the overall percentage of bags with Ni = 5, ranging
from s5 = 0 (in which case no bags have size Ni = 5) to
s5 = 50 (in which case 50% of the overall bags have size
Ni = 5). Here we do not add additional noise: ε = 0.

Results are shown in Figure 4. BDR and shrinkage meth-
ods, which take into account bag size uncertainty, per-
form well here compared to the other methods. The full
BDR model very slightly outperforms the Bayesian shrink-
age models in both likelihood and in mean-squared error;
frequentist shrinkage slightly outperforms the Bayesian
shrinkage models in MSE, likely because it is tuned for that
metric. We also see that the choice of r affects the results;
r = k does somewhat better.

Figure 5 demonstrates in more detail the difference be-
tween these models. It shows test set predictions of each
model on the bags of different sizes. Here, we can see
explicitly that the shrinkage and BDR models are able to
take into account the bag size, with decreasing variance
for larger bag sizes, while the BLR model gives the same
variance for all outputs. Furthermore, the shrinkage and
BDR models can shrink their predictions towards the mean
more for smaller bags than larger ones: this improves per-
formance on the small bags while still allowing for good
predictions on large bags, contrary to the BLR model.

Fixed bag size: Uncertainty in the regression model.
The previous experiment showed the efﬁcacy of the shrink-
age estimator in our models, but demonstrated little gain
from posterior inference for regression weights β over their
MAP estimates, i.e. there is no discernible improvement of
BLR over RBF network. To isolate the effect of quantify-
ing uncertainty in the regression model, we now consider
the case where there is no variation in bag size at all and
normal noise is added onto the observations. In particular
we take Ni = 1000 and ε
(0, 1), and sample land-
marks randomly from the training set.

∼ N

Results are shown in Table 1. Here, BLR or BDR outper-
form all other methods on all runs, highlighting that uncer-
tainty in the regression model is also important for predic-
tive performance. Importantly, the BDR method performs
well in this regime as well as in the previous one.

5.2

IMDb-WIKI: Age Estimation

We now demonstrate our methods on a celebrity age es-
timation problem, using the IMDb-WIKI database (Rothe
et al., 2016) which consists of 397 949 images of 19 545
celebrities4, with corresponding age labels. This database

4We used only the IMDb images, and removed some implau-
sible images, including one of a cat and several of people with

Figure 4: Top: negative log-likelihood. Bottom: mean-
squared error. For context, performance of the Bayes-
optimal predictor is also shown, and for NLL ‘uniform’
shows the performance of a uniform prediction on the pos-
sible labels. For MSE, the constant overall mean label pre-
dictor achieves about 1.3.

differs for the two experiments below:

yi

yi

∼
iid
∼

Uniform(4, 8)
1
1
Γ
2
yi (cid:20)

yi
2

(cid:18)

,

(cid:19)(cid:21)

xi
j

(cid:2)

(cid:96) |
(cid:3)

+ ε for j

[Ni], (cid:96)

[5].

∈

∈

In these experiments, we generate 1 000 bags for training,
500 bags for a validation set for parameter tuning, 500
bags to use for early-stopping of the models, and 1 000
bags for testing. Tuning is performed to maximize log-
likelihoods for Bayesian models, MSE for non-Bayesian
models. Landmark points u are chosen via k-means (ﬁxed
across all models). We also show results of the Bayes-
optimal model, which gives true posteriors according to
the data-generating process; this is the best performance
any model could hope to achieve. Our learning models,
which treat the inputs as ﬁve-dimensional, fully nonpara-
metric distributions, are at a substantial disadvantage even
in how they view the data compared to this true model.

Varying bag size: Uncertainty in the inputs.
In order
to study the behaviour of our models with varying bag size,
we ﬁx four sizes Ni
. For each gener-

5, 20, 100, 1 000
}

∈ {

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

Figure 5: Predictions for the varying bag size experiment of Section 5.1. Each column corresponds to a single prediction
method. Each point in an image represents a single bag, with its horizontal position the true label yi, and its vertical
position the predicted label. The black lines show theoretical perfect predictions. The rows represent different subsets of
the data: the ﬁrst row shows all bags, the second only bags with Ni = 5, and so on. Colours represent the predictive
standard deviation of each point.

Bayesian Approaches to Distribution Regression

Table 1: Results on the ﬁxed bag size dataset, over
10 dataset draws (standard deviations in parentheses).
BLR/BDR perform best on all runs in both metrics.

METHOD

MSE

NLL

Optimal
RBF network
freq-shrinkage
shrinkage
shrinkageC
BLR
BDR

0.170 (0.009)
0.235 (0.014)
0.232 (0.012)
0.237 (0.014)
0.236 (0.013)
0.228 (0.012)
0.227 (0.012)

0.401 (0.018)
–
–
0.703 (0.027)
0.700 (0.029)
0.681 (0.025)
0.683 (0.025)

Table 2: Results on the grouped IMDb-WIKI dataset over
ten runs (standard deviations in parentheses). Here shrink-
age methods perform the best across all 10 runs.

METHOD

RMSE

NLL

CNN
RBF network
freq-shrinkage
shrinkage
BLR

10.25 (0.22)
9.51 (0.20)
9.22 (0.19)
9.28 (0.20)
9.55 (0.19)

3.80 (0.034)
–
–
3.54 (0.021)
3.68 (0.021)

was constructed by crawling IMDb for images of its most
popular actors and directors, with potentially many images
for each celebrity over time. Rothe et al. (2016) use a con-
volutional neural network (CNN) with a VGG-16 architec-
ture to perform 101-way classiﬁcation, with one class cor-
responding to each age in

0, . . . , 100

{

.
}

We take a different approach, and assume that we are given
several images of a single individual (i.e. samples from
the distribution of celebrity images), and are asked to pre-
dict their mean age based on several pictures. For example,
we have 757 images of Brad Pitt from age 27 up to 51,
while we have only 13 images of Chelsea Peretti at ages 35
and 37. Note that 22.5% of bags have only a single image.
We obtain 19 545 bags, with each bag containing between
1 and 796 images of a particular celebrity, and the corre-
sponding bag label calculated from the average of the age
labels of the images inside each bag.

In particular, we use the representation ϕ(x) learnt by the
CNN in Rothe et al. (2016), where ϕ(x) : R256×256
→
R4096 maps from the pixel space of images to the CNN’s
last hidden layer. With these new representations, we can
now treat them as inputs to our radial basis network, shrink-
age (taking r = k here) and BLR models. Although we
could also use the full BDR model here, due to the compu-
tational time and memory required to perform proper pa-

supposedly negative age, or ages of several hundred years.

rameter tuning, we relegate this to a later study.

We use 9 820 bags for training, 2 948 bags for early stop-
ping, 2 946 for validation and 3 928 for testing. Landmarks
are sampled without replacement from the training set.

We repeat the experiment on 10 different splits of the data,
and report the results in Table 2. The baseline CNN results
give performance by averaging the predictive distribution
from the model of Rothe et al. (2016) for each image of a
bag; note that this model was trained on all of the images
used here. From Table 2, we can see that the shrinkage
methods have the best performance; they outperforms all
other methods in all 10 splits of the dataset, in both met-
rics. Non-Bayesian shrinkage again yields slightly better
RMSEs, likely because it is tuned for that metric. This
demonstrates that modelling bag size uncertainty is vital.

6 CONCLUSION

Supervised learning on groups of observations using ker-
nel mean embeddings typically disregards sampling vari-
ability within groups. To handle this problem, we con-
struct Bayesian approaches to modelling kernel mean em-
beddings within a regression model, and investigate advan-
tages of uncertainty propagation within different compo-
nents of the resulting distribution regression. The ability
to take into account the uncertainty in mean embedding es-
timates is demonstrated to be key for constructing mod-
els with good predictive performance when group sizes are
highly imbalanced. We also demonstrate that the results of
a complex neural network model for age estimation can be
improved by shrinkage.

Our models employ a neural network formulation to pro-
vide more expressive feature representations and learn dis-
criminative embeddings. Doing so makes our model easy
to extend to more complicated featurisations than the sim-
ple RBF network used here. By training with backpropa-
gation, or via approximate Bayesian methods such as vari-
ational inference, we can easily ‘learn the kernel’ within
our framework, for example ﬁne-tuning the deep network
of Section 5.2 rather than using a pre-trained model. We
can also apply our networks to structured settings, learning
regression functions on sets of images, audio, or text. Such
models naturally ﬁt into the empirical Bayes framework.

On the other hand, we might extend our model to more
Bayesian feature learning by placing priors over the kernel
hyperparameters, building on classic work on variational
approaches (Barber and Schottky, 1998) and fully Bayesian
inference (Andrieu et al., 2001) in RBF networks. Such
approaches are also possible using other featurisations, e.g.
random Fourier features (as in Oliva et al., 2015).

Future distribution regression approaches will need to ac-
count for uncertainty in observation of the distribution. Our
methods provide a strong, generic building block to do so.

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

References

Christophe Andrieu, Nando De Freitas, and Arnaud
Doucet. Robust full bayesian learning for radial ba-
sis networks. Neural Computation, 13(10):2359–2407,
2001.

David Barber and Bernhard Schottky. Radial basis func-
tions: a bayesian treatment. NIPS, pages 402–408, 1998.
C.M. Bishop. Pattern recognition and machine learning.

Springer New York, 2006.

David S Broomhead and David Lowe. Radial basis func-
tions, multi-variable functional interpolation and adap-
tive networks. Technical report, DTIC Document, 1988.

Andreas C. Damianou, Michalis K. Titsias, and Neil D.
Lawrence. Variational inference for latent variables and
uncertain inputs in Gaussian processes. JMLR, 17(42):
1–62, 2016.

Thomas G Dietterich, Richard H Lathrop, and Tom´as
Lozano-P´erez. Solving the multiple instance problem
with axis-parallel rectangles. Artiﬁcial intelligence, 89
(1):31–71, 1997.

Seth Flaxman, Yu-Xiang Wang, and Alexander J Smola.
Who supported Obama in 2012?: Ecological inference
through distribution regression. In KDD, pages 289–298.
ACM, 2015.

Seth Flaxman, Dino Sejdinovic, John P. Cunningham, and
Sarah Filippi. Bayesian learning of kernel embeddings.
In UAI, 2016.

Seth Flaxman, Dougal J. Sutherland, Yu-Xiang Wang,
and Yee-Whye Teh. Understanding the 2016 US pres-
inference and dis-
idential election using ecological
tribution regression with census microdata.
2016.
arXiv:1611.03787.

Thomas G¨artner. Kernels for Structured Data, volume 72.
World Scientiﬁc, Series in Machine Perception and Arti-
ﬁcial Intelligence, 2008.

Andrew Gelman, David K Park, Stephen Ansolabehere,
Phillip N Price, and Lorraine C Minnite. Models, as-
sumptions and model checking in ecological regressions.
Journal of the Royal Statistical Society: Series A (Statis-
tics in Society), 164(1):101–118, 2001.

Agathe Girard. Approximate methods for propagation of
uncertainty with Gaussian process models. PhD thesis,
University of Glasgow, 2004.

Matthew D. Hoffman and Andrew Gelman. The no-U-turn
sampler: Adaptively setting path lengths in Hamiltonian
Monte Carlo. JMLR, pages 1593–1623, 2014.

Christopher Jackson, Nicky Best, and Sylvia Richardson.
Improving ecological inference using individual-level
data. Statistics in medicine, 25(12):2136–2159, 2006.

Tony Jebara, Risi Imre Kondor, and Andrew Howard. Prob-

ability product kernels. JMLR, 5:819–844, 2004.

Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess,
S. M. Ali Eslami, Balaji Lakshminarayanan, Dino Se-
jdinovic, and Zolt´an Szab´o. Kernel-Based Just-In-Time
Learning for Passing Expectation Propagation Mes-
sages. In UAI, 2015.

Gary King. A Solution to the Ecological Inference Problem.
Princeton University Press, 1997. ISBN 0691012407.

Diederik Kingma and Jimmy Ba. Adam: A method
In ICLR, 2015.

stochastic optimization.

for
arXiv:1412.6980.

Gabriel Krummenacher, Cheng Soon Ong, and Joachim M
Buhmann. Ellipsoidal multiple instance learning.
In
ICML (2), pages 73–81, 2013.

Hendrik K¨uck and Nando de Freitas. Learning about in-
dividuals from group statistics. In UAI, pages 332–339,
2005.

Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Wein-
berger. From word embeddings to document distances.
In ICML, pages 957–966, 2015.

H. C. L. Law, C. Yau, and D. Sejdinovic. Testing and learn-
ing on distributions with symmetric noise invariance. In
NIPS, 2017. arXiv:1703.07596.

Bernhard
Towards a learn-

David Lopez-Paz, Krikamol Muandet,

Sch¨olkopf, and Ilya Tolstikhin.
ing theory of cause-effect inference. In ICML, 2015.
Milan Luki´c and Jay Beder. Stochastic processes with sam-
ple paths in reproducing kernel hilbert spaces. Trans-
actions of the American Mathematical Society, 353(10):
3945–3969, 2001.

Yifei Ma, Dougal J. Sutherland, Roman Garnett, and Jeff
In AIS-

Schneider. Active pointillistic pattern search.
TATS, 2015.

J. Mitrovic, D. Sejdinovic, and Y.W. Teh. DR-ABC:
Approximate Bayesian Computation with Kernel-Based
In ICML, pages 1482–1491,
Distribution Regression.
2016.

Pedro J Moreno, Purdy P Ho, and Nuno Vasconcelos. A
Kullback-Leibler divergence based kernel for SVM clas-
siﬁcation in multimedia applications. In NIPS, 2003.
Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo,
Learning from distribu-
In NIPS, 2012.

and Bernhard Sch¨olkopf.
tions via support measure machines.
arXiv:1202.6504.

Krikamol Muandet, Kenji Fukumizu, Bharath Sriperum-
budur, Arthur Gretton, and Bernhard Schoelkopf. Kernel
mean estimation and stein effect. In ICML, 2014.

Michelle Ntampaka, Hy Trac, Dougal J. Sutherland,
Nicholas Battaglia, Barnab´as P´oczos, and Jeff Schnei-
A machine learning approach for dynamical
der.
The Astro-
mass measurements of galaxy clusters.
physical Journal, 803(2):50, 2015.
ISSN 1538-4357.
arXiv:1410.0686.

Bayesian Approaches to Distribution Regression

Ingo Steinwart. Convergence types and rates in generic
Karhunen-Lo´eve expansions with applications to sam-
ple path properties. arXiv preprint arXiv:1403.1040v3,
March 2017.

Dougal J. Sutherland. Scalable, Flexible, and Active Learn-
ing on Distributions. PhD thesis, Carnegie Mellon Uni-
versity, 2016.

Zolt´an Sz´abo, Bharath K. Sriperumbudur, Barnab´as
Leraning theory for
JMLR, 17(152):1–40, 2016.

P´oczos, and Arthur Gretton.
distribution regression.
arXiv:1411.2066.

Grace Wahba. Spline models for observational data, vol-

ume 59. Siam, 1990.

Yuya Yoshikawa, Tomoharu Iwata, and Hiroshi Sawada.
Latent support measure machines for bag-of-words data
classiﬁcation. In NIPS, pages 1961–1969, 2014.

Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barn-
abas Poczos, Ruslan Salakhutdinov, and Alexander
Smola. Deep sets. In NIPS, 2017.

Zhi-Hua Zhou, Yu-Yin Sun, and Yu-Feng Li. Multi-
instance learning by treating instances as non-iid sam-
ples. In ICML, 2009.

Michelle Ntampaka, Hy Trac, Dougal J. Sutherland, S. Fro-
menteau, B. Poczos, and Jeff Schneider. Dynamical
mass measurements of contaminated galaxy clusters us-
ing machine learning. The Astrophysical Journal, 831
(2):135, 2016. arXiv:1509.05409.

Junier B Oliva, Barnab´as P´oczos, and Jeff Schneider. Dis-
tribution to distribution regression. In ICML, 2013.

Junier B Oliva, Avinava Dubey, Barnab´as P´oczos, Jeff
Schneider, and Eric P Xing. Bayesian nonparametric
kernel-learning. In AISTATS, 2015. arXiv:1506.08776.

Giorgio Patrini, Richard Nock, Tiberio Caetano, and Paul

Rivera. (Almost) no label no cry. In NIPS. 2014.

Natesh S Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee,
and Robert L Wolpert. Characterizing the function space
for bayesian kernel models. JMLR, 8(Aug):1769–1797,
2007.

Barnab´as P´oczos, Liang Xiong, and Jeff Schneider. Non-
parametric divergence estimation with applications to
machine learning on distributions. In UAI, 2011.

Barnabas Poczos, Aarti Singh, Alessandro Rinaldo,
Distribution-free distribu-
In AISTATS, pages 507–515, 2013.

and Larry Wasserman.
tion regression.
arXiv:1302.0082.

Novi Quadrianto, Alex J Smola, Tiberio S Caetano, and
Quoc V Le. Estimating labels from label proportions.
JMLR, 10:2349–2374, 2009.

Qichao Que and Mikhail Belkin. Back to the future: Radial
basis function networks revisited. In AISTATS, 2016.

Ali Rahimi and Benjamin Recht. Random features for
large-scale kernel machines. In NIPS, pages 1177–1184,
2007.

Aaditya Ramdas and Leila Wehbe. Nonparametric inde-
pendence testing for small sample sizes. In IJCAI, 2015.
arXiv:1406.1922.

Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep
expectation of real and apparent age from a single im-
age without facial landmarks. International Journal of
Computer Vision (IJCV), July 2016.

Craig Saunders, Alexander Gammerman, and Volodya
Vovk. Ridge regression learning algorithm in dual vari-
ables. In ICML, 1998.

Bernhard Sch¨olkopf, Ralf Herbrich, and Alex J. Smola. A

generalized representer theorem. In COLT, 2001.

Bharath K Sriperumbudur, Arthur Gretton, Kenji Fuku-
mizu, Bernhard Sch¨olkopf, and Gert RG Lanckriet.
Hilbert space embeddings and metrics on probability
measures. JMLR, 99:1517–1561, 2010.

Stan Development Team. Stan: A c++ library for proba-
bility and sampling, version 2.5.0, 2014. URL http:
//mc-stan.org/.

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

A Choice of r(

,
·

) to ensure µP
·

k
∈ H

We need to choose an appropriate covariance function r, such that µP
,
for inﬁnite-dimensional RKHSs not sufﬁcient to deﬁne r(
·

k (Wahba, 1990) (but see below). However, we can construct

) = k(
·

,
·

k, where µP

)). In particular, it is
,
(0, r(
∈ H
·
), as draws from this particular prior are no longer in
·

∼ GP

·

H

r(x, y) =

k(x, z)k(z, y)ν(dz)

(cid:90)

(8)

k with probability 1 by the nuclear dominance (Luki´c and
where ν is any ﬁnite measure on
∈ H
Beder, 2001; Pillai et al., 2007) for any stationary kernel k. In particular, Flaxman et al. (2016) provides details when k is
a squared exponential kernel deﬁned by

. This then ensures µP

X

k(x, y) = exp(

(x

1
2

−

y)(cid:62)Σ−1

k (x

y))

−

−

x, y

p

R

∈

) with a non-
and ν(dz) = exp
·
stationary component. In this paper, we take Σk = σ2Ip, where σ2 and (cid:96) are tuning parameters, or parameters that we
learn.

it is proportional to a Gaussian measure on Rd, which provides r(
·

dz, i.e.

−

(cid:17)

(cid:16)

,

||z||2
2
2(cid:96)2

Here, the above holds for a general set of stationary kernels, but note that by taking a convolution of a kernel with itself,
it might make the space of functions that we consider overly smooth (i.e. concentrated on a small part of
k). In this
work, however, we consider only the Gaussian RBF kernel k. In fact, recent work (Steinwart, 2017, Theorem 4.2) actually
shows that in this case, the sample paths almost surely belong to (interpolation) spaces which are inﬁnitesimally larger
than the RKHS of the Gaussian RBF kernel. This suggests that we can choose r to be an RBF kernel with a length scale
that is inﬁnitesimally bigger than that of k; thus, in practice, taking r = k would sufﬁce and we do observe that it actually
performs better (Fig. 4).

H

B Framework for Binary Classiﬁcation

Suppose that our labels yi
for uncertainty in the regression parameters is to use bayesian logistic regression, putting priors on β, i.e.

0, 1
, i.e. we are in a binary classiﬁcation framework. Then a simple approach to accounting
}

∈ {

(0, ρ2)

∼ N

β

yi

∼

Ber(πi), where log

= β(cid:62) ˆµi

πi

1

(cid:18)

−

πi (cid:19)
µi, α, we would not be able to obtain an analytical

however for the mean shrinkage pooling model, if we use the above yi
solution for p(yi

xi, α). Instead we use the probit link function, as given by:

|

|

where Φ denotes the Cumulative Distribution Function (CDF) of a standard normal distribution, with µi(z) =
[µi(z1), . . . , µi(zs)](cid:62). Then as before we have

µi, α) = Φ
P r(yi = 1
|

α(cid:62)µi(z)

(cid:0)

(cid:1)

µi(z)

xi

|

∼ N

(Mi, Ci)

with Mi and Ci as deﬁned in section 3.3. Hence, as before

P r(yi = 1
|

xi, α) =

P r(yi = 1
|

xi)dµi(z)
µi, α)p(µi(z)
|

Φ(α(cid:62)µi(z)) exp

(µi(z)

Mi)(cid:62)C −1

i

(µi(z)

1
2

{−

−

Mi)
}

−

dµi(z)

(with li = µi(z)

Mi) = c

Φ(α(cid:62)(li + Mi)) exp

−

1
2

{−

(li)(cid:62)C −1

i

(li)

dli
}

= P r(Y

α(cid:62)(li + Mi))

≤

(cid:90)
= c

(cid:90)

(cid:90)

Bayesian Approaches to Distribution Regression

Note here Y

(0, 1) and li

(0, Σi) Then expanding and rearranging

∼ N

∼ N

Note that since Y and li independent normal r.v., Y
have:

−

α(cid:62)li

∼ N

xi, α) = P r(Y
P r(yi = 1
|

α(cid:62)li

α(cid:62)Mi)

≤

−
(0, 1 + α(cid:62)Ciα(cid:62)). Let T be standard normal, then we

xi, α) = P r(
P r(yi = 1
|

α(cid:62)Mi)

1 + α(cid:62)Ciα T
α(cid:62)Mi
1 + α(cid:62)Ciα

≤

≤

)

α(cid:62)Mi
(cid:112)
1 + α(cid:62)Ciα (cid:33)

(cid:112)
= P r(T

= Φ

(cid:32)

(cid:112)

Hence, we also have:

xi, α) = 1
P r(yi = 0
|

−

Φ

α(cid:62)Mi
1 + α(cid:62)Ciα (cid:33)

(cid:32)

(cid:112)

Now placing the prior α

(0, ρ2K −1

z ), we have the following MAP objective:

∼ N

J(α) = log

p(α)

n

i=1
(cid:89)

p(yi

xi, α)
|

(cid:35)

=

(1

yi) log(1

Φ

(cid:34)

n

−

i=1
(cid:88)

+yi log(Φ

α(cid:62)Mi
)
1 + α(cid:62)Ciα (cid:33)

−

(cid:32)

(cid:112)

α(cid:62)Mi
1 + α(cid:62)Ciα (cid:33)

) +

1
ρ2 α(cid:62)Kzα

(cid:32)

(cid:112)

xi, α), we can also use this in HMC for BDR.
Since we have an analytical solution for P r(yi = 0
|

C Some more intuition on the shrinkage estimator

In this section, we provide some intuition behind the shrinkage estimator in section 3.3. Here, for simplicity, we choose
Σi = τ 2I for all bag i, and m0 = 0, and consider the case where z = u, i.e. R = Rz = Rzz. We can then see that if R
has eigendecomposition U ΛU T , with Λ = diag(λk), the posterior mean is

so that large eigenvalues, λk
towards 0. Likewise, the posterior variance is

(cid:29)

τ 2/Ni, are essentially unchanged, while small eigenvalues, λk

τ 2/Ni, are shrunk

(cid:28)

U diag

λk
λk + τ 2/Ni (cid:19)

(cid:18)

U T (ˆµi),

U diag

λk

(cid:32)

−

λ2
k
λk + τ 2

Ni (cid:33)

U T = U diag

1
Ni
τ 2 + 1

λk (cid:33)

(cid:32)

U T ;

its eigenvalues also decrease as Ni/τ 2 increases.

D Alternative Motivation for choice of f

Here we provide an alternative motivation for the choice of f =
model with a linear kernel K on µi, where f :

k

R:

k
, zs). First, consider the following Bayesian
s=1 αsk(
·

H

→

yi

µi, f

|

(cid:80)
f (µi), σ2

.

(cid:1)

∼ N

(cid:0)

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

Now considering the log-likelihood of

µ, Y
{

}

=

µi, yi
{

n
i=1 (supposing we have these exact embeddings), we obtain:
}

log p(Y

µ, f ) =
|

n

1
2σ2 (yi

−

−

f (µi))2

i=1
(cid:88)
To avoid over-ﬁtting, we place a Gaussian prior on f , i.e.
likelihood over f

k, we have:

−

∈ H

f ∗ = argminf ∈Hk

1
2σ2 (yi

−

f (µi))2 + λ

f
||

||

Hk

log p(f ) = λ

Hk + c. Minimizing the negative log-

f
||

||

Now this is in the form of an empirical risk minimisation problem. Hence using the representer theorem (Sch¨olkopf et al.,
2001), we have that:

i.e. we have a ﬁnite-dimensional problem to solve. Thus since K is a linear kernel:

f =

γjK(., µj)

yi

µi,

|

µj
{

n
j=1, γ
}

∼ N 

γj

µi, µj

(cid:104)

Hk , σ2
(cid:105)

n

j=1
(cid:88)



.





where

µi, µj

Hk can be thought of as the similarity between distributions.

(cid:104)

(cid:105)

xi, γ). This suggests we need to
Now we have the same
|
integrate out µ1, . . . µn. But it is unclear how to perform this integration, since the µi follow Gaussian process distributions.
Hence we can take an approximation to f , i.e. f =
, zs), which would essentially give us a dual method with
·
a sparse approximation to f .

posterior as in Section 3.3, and we would like to compute p(yi

k
s=1 αsk(

GP

n

i=1
(cid:88)

n

j=1
(cid:88)

(cid:80)

8
1
0
2
 
b
e
F
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
3
9
2
4
0
.
5
0
7
1
:
v
i
X
r
a

Bayesian Approaches to Distribution Regression

Ho Chung Leon Law∗
University of Oxford
ho.law@spc.ox.ac.uk

Dougal J. Sutherland∗
University College London
dougal@gmail.com

Dino Sejdinovic
University of Oxford
dino.sejdinovic@stats.ox.ac.uk

Seth Flaxman
Imperial College London
s.ﬂaxman@imperial.ac.uk

Abstract

Distribution regression has recently attracted
much interest as a generic solution to the prob-
lem of supervised learning where labels are avail-
able at the group level, rather than at the individ-
ual level. Current approaches, however, do not
propagate the uncertainty in observations due to
sampling variability in the groups. This effec-
tively assumes that small and large groups are
estimated equally well, and should have equal
weight in the ﬁnal regression. We account for
this uncertainty with a Bayesian distribution re-
improving the robustness
gression formalism,
and performance of the model when group sizes
vary. We frame our models in a neural network
style, allowing for simple MAP inference using
backpropagation to learn the parameters, as well
as MCMC-based inference which can fully prop-
agate uncertainty. We demonstrate our approach
on illustrative toy datasets, as well as on a chal-
lenging problem of predicting age from images.

1

INTRODUCTION

Distribution regression is the problem of learning a regres-
sion function from samples of a distribution to a single set-
level label. For example, we might attempt to infer the
sentiment of texts based on word-level features, to predict
the label of an image based on small patches, or even per-
form traditional parametric statistical inference by learning
a function from sets of samples to the parameter values.

Recent years have seen wide-ranging applications of this
framework, including inferring summary statistics in Ap-
proximate Bayesian Computation (Mitrovic et al., 2016),
estimating Expectation Propagation messages (Jitkrittum
et al., 2015), predicting the voting behaviour of demo-

Proceedings of the 21st International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS) 2018, Lanzarote, Spain.
PMLR: Volume 84. Copyright 2018 by the author(s).

graphic groups (Flaxman et al., 2015, 2016), and learning
the total mass of dark matter halos from observable galaxy
velocities (Ntampaka et al., 2015, 2016). Closely related
distribution classiﬁcation problems also include identify-
ing the direction of causal relationships from data (Lopez-
Paz et al., 2015) and classifying text based on bags of word
vectors (Yoshikawa et al., 2014; Kusner et al., 2015).

One particularly appealing approach to the distribution re-
gression problem is to represent the input set of samples
by their kernel mean embedding (described in Section 2.1),
where distributions are represented as single points in a re-
producing kernel Hilbert space. Standard kernel methods
can then be applied for distribution regression, classiﬁca-
tion, anomaly detection, and so on. This approach was per-
haps ﬁrst popularized by Muandet et al. (2012); Sz´abo et al.
(2016) provided a recent learning-theoretic analysis.

In this framework, however, each distribution is simply rep-
resented by the empirical mean embedding, ignoring the
fact that large sample sets are much more precisely under-
stood than small ones. Most studies also use point esti-
mates for their regressions, such as kernel ridge regression
or support vector machines, thus ignoring uncertainty both
in the distribution embeddings and in the regression model.

Our Contributions We propose a set of Bayesian ap-
proaches to distribution regression. The simplest method,
similar to that of Flaxman et al. (2015), is to use point
estimates of the input embeddings but account for uncer-
tainty in the regression model with simple Bayesian linear
regression. Alternatively, we can treat uncertainty in the in-
put embeddings but ignore model uncertainty with the pro-
posed Bayesian mean shrinkage model, which builds on a
recently proposed Bayesian nonparametric model of uncer-
tainty in kernel mean embeddings (Flaxman et al., 2016),
and then use a sparse representation of the desired function
in the RKHS for prediction in the regression model. This
model allows for a full account of uncertainty in the mean
embedding, but requires a point estimate of the regression
function for conjugacy; we thus use backpropagation to ob-
tain a MAP estimate for it as well as various hyperparam-
eters. We then combine the treatment of the two sources

∗These authors contributed equally.

Bayesian Approaches to Distribution Regression

of uncertainty into a fully Bayesian model and use Hamil-
tonian Monte Carlo for efﬁcient inference. Depending on
the inferential goals, each model can be useful. We demon-
strate our approaches on an illustrative toy problem as well
as a challenging real-world age estimation task.

2 BACKGROUND

2.1 Problem Overview

Distribution regression is the task of learning a classiﬁer or
a regression function that maps probability distributions to
labels. The challenge of distribution regression goes be-
yond the standard supervised learning setting: we do not
have access to exact input-output pairs since the true in-
puts, probability distributions, are observed only through
samples from that distribution:

,

(cid:17)

(cid:16)

(cid:17)

(cid:16)

(1)

, . . . ,

N1
j=1, y1

Nn
j=1, yn

xi
j}
{

xn
j }
{

x1
j }
{
Ni
xi
j=1 has a label yi along with Ni indi-
so that each bag
j}
{
vidual observations xi
. We assume that the observa-
j ∈ X
Ni
tions
j=1 are i.i.d. samples from some unobserved dis-
tribution Pi, and that the true label yi depends only on Pi.
We wish to avoid making any strong parametric assump-
tions on the Pi. For the present work, we will assume the
labels yi are real-valued; Appendix B shows an extension
to binary classiﬁcation. We typically take the observation
to be a subset of Rp, but it could easily be a struc-
space
tured domain such as text or images, since we access it only
through a kernel (for examples, see e.g. G¨artner, 2008).

X

X × X →

We consider the standard approach to distribution regres-
sion, which relies on kernel mean embeddings and ker-
nel ridge regression. For any positive deﬁnite kernel func-
tion k :
R, there exists a unique reproduc-
k, a possibly inﬁnite-
ing kernel Hilbert space (RKHS)
H
dimensional space of functions f :
R where eval-
uation can be written as an inner product, and in particu-
lar f (x) =
k, x
Hk for all f
. Here
(cid:104)
k is a function of one argument, y
, x)
k(
·

∈ X
(cid:55)→
, let us deﬁne the kernel

, x)
(cid:105)
·

k(y, x).

X →

∈ H

Given a probability measure P on
mean embedding into

f, k(

∈ H

k as

X

H

k (
·

(cid:90)

µP =

, x) P(dx)

k.

∈ H

(2)

H

k(x, x)P(dx) <

Notice that µP serves as a high- or inﬁnite-dimensional
vector representation of P. For the kernel mean embed-
ding of P into
it sufﬁces that
k to be well-deﬁned,
, which is trivially satisﬁed for all
P if k is bounded. Analogously to the reproducing prop-
(cid:82) (cid:112)
erty of RKHS, µP represents the expectation function on
Hk . For so-called characteris-
H
(cid:105)
tic kernels (Sriperumbudur et al., 2010), every probability
measure has a unique embedding, and thus µP completely
determines the corresponding probability measure.

h(x)P(dx) =

h, µP
(cid:104)

∞

k:

(cid:82)

2.2 Estimating Mean Embeddings

For a set of samples
ical estimator of µP,

xj
{
µP

n
j=1 drawn iid from P, the empir-
}
∈ H

k, is given by

µP = µ(cid:98)P =

(cid:90)

(cid:99)

(cid:99)
, x) ˆP(dx) =
k (
·

1
n

k(

, xj).
·

(3)

n

j=1
(cid:88)

This is the standard estimator used by previous distribution
regression approaches, which the reproducing property of

k shows us corresponds to the kernel

H

µPi,
(cid:104)

µPj(cid:105)

Hk =

1
NiNj

Ni

Nj

(cid:96)=1
(cid:88)

r=1
(cid:88)

k(xi

(cid:96), xj

r).

(4)

(cid:100)

(cid:100)

But (3) is an empirical mean estimator in a high- or inﬁnite-
dimensional space, and is thus subject to the well-known
Stein phenomenon, so that its performance is dominated
by the James-Stein shrinkage estimators. Indeed, Muandet
et al. (2014) studied shrinkage estimators for mean embed-
dings, which can result in substantially improved perfor-
mance for some tasks (Ramdas and Wehbe, 2015). Flax-
man et al. (2016) proposed a Bayesian analogue of shrink-
age estimators, which we now review.

·

H

∼ N

)) on

µP(x)

∈ H
|

This approach consists of (1) a Gaussian Process prior
µP
k, where r is selected to en-
,
(m0, r(
·
∼ GP
sure that µP
k almost surely and (2) a normal likeli-
(µP(x), Σ). Here, conjugacy
µP(x)
hood
of the prior and the likelihood leads to a Gaussian process
posterior on the true embedding µP, given that we have ob-
(cid:99)
µP at some set of locations x. The posterior mean
served
is then essentially identical to a particular shrinkage esti-
mator of Muandet et al. (2014), but the method described
here has the extra advantage of a closed form uncertainty
estimate, which we utilise in our distributional approach.
For the choice of r, we use a Gaussian RBF kernel k, and
choose either r = k or, following Flaxman et al. (2016),
r(x, x(cid:48)) =
k(x, z) k(z, x(cid:48)) ν(dz) where ν is proportional
to a Gaussian measure. For details of our choices, and why
they are sufﬁcient for our purposes, see Appendix A.

(cid:99)

(cid:82)

This model accounts for the uncertainty based on the num-
ber of samples Ni, shrinking the embeddings for small
sample sizes more. As we will see, this is essential in
the context of distribution regression, particularly when bag
sizes are imbalanced.

2.3 Standard Approaches to Distribution Regression

Following Sz´abo et al. (2016), assume that the probability
distributions Pi are each drawn randomly from some un-
known meta-distribution over probability distributions, and
take a two-stage approach, illustrated as in Figure 1. De-
noting the feature map k(
k by φ(x), one uses the
, x)
·
empirical kernel mean estimator (3) to separately estimate

∈ H

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

Figure 1: Each bag is summarised by a kernel mean embed-
k; a regression function f :
ding µi
R predicts
labels yi
R. We propose a Bayesian approach to propa-
gate uncertainty due to the number of samples in each bag,
obtaining posterior credible intervals illustrated in grey.

∈ H
∈

→

H

k

the mean of each group:

1
N1

N1

j=1
(cid:88)

µ1 =

(cid:99)

φ(x1

j ),

. . . ,

µn =

φ(xn

j ).

(5)

1
Nn

Nn

i=1
(cid:88)

Next, one uses kernel ridge regression (Saunders et al.,
1998) to learn a function f :
R, by minimizing
the squared loss with an RKHS complexity penalty:

k
H

→

(cid:99)

ˆf = argmin
f ∈HK

(yi

f (

µi))2 + λ
(cid:107)

f

2
HK .
(cid:107)

−

i
(cid:88)
k
× H

(cid:98)

k

H

→

Here K :
R is a “second-level” kernel on
mean embeddings. If K is a linear kernel on the RKHS
k, then the resulting method can be interpreted as a linear
H
(ridge) regression on mean embeddings, which are them-
selves nonlinear transformations of the inputs. A nonlin-
ear second-level kernel on
k sometimes improves perfor-
mance (Muandet et al., 2012; Sz´abo et al., 2016).

H

Distribution regression as described is not scalable for
even modestly-sized datasets, as computing each of the
(n2) entries of the relevant kernel matrix requires time
O
(NiNj). Many applications have thus used variants of
O
random Fourier features (Rahimi and Recht, 2007). In this
paper we instead expand in terms of landmark points drawn
randomly from the observations, yielding radial basis net-
works (Broomhead and Lowe, 1988) with mean pooling.

3 MODELS

Figure 2: Our baseline model, a RBF network for distri-
bution regression. Xi represents the matrix of samples for
bag i, while k(Xi, u(cid:96)) represents the element wise opera-
tion on each row of Xi, with b representing the batch size
for stochastic gradient descent.

begin with a non-Bayesian RBF network formulation of
the standard approach to distribution regression as a base-
line, before reﬁning this approach to better propagate un-
certainty in bag size, as well as model parameters.

3.1 Baseline Model

The baseline RBF network formulation we employ here
is a variation of the approaches of Broomhead and Lowe
(1988), Que and Belkin (2016), Law et al. (2017), and Za-
heer et al. (2017). As shown in Figure 2, the initial input
is a minibatch consisting of several bags Xi, each contain-
ing Ni points. Each point is then converted to an explicit
featurisation, taking the role of φ in (5), by a radial basis
layer: xi

Rp is mapped to

j ∈

φ(xi

j) = [k(xi

j, u1), . . . , k(xi

j, ud)](cid:62)

d

R

∈

u(cid:96)
{

Ni
j=1 φ(xi

d
where u =
(cid:96)=1 are landmark points. A mean pool-
}
ing layer yields the estimated mean embedding ˆµi corre-
sponding to each of the bags j represented in the minibatch,
where ˆµi = 1
j).1 Finally, a fully connected
Ni
output layer gives real-valued labels ˆyi = βT ˆµi + b. As a
loss function we use the mean square error 1
yi)2.
n
For learning, we use backpropagation with the Adam opti-
mizer (Kingma and Ba, 2015). To regularise the network,
we use early stopping on a validation set, as well as an L2
penalty corresponding to a normal prior on β.

i(ˆyi

(cid:80)

(cid:80)

−

We consider here three different Bayesian models, with
each model encoding different types of uncertainty. We

1In the implementation, we stack all of the bags Xi into a
j Nj × d for the ﬁrst layer, then perform

single matrix of size (cid:80)
pooling via sparse matrix multiplication.

Bayesian Approaches to Distribution Regression

variance, in order to propagate this information regarding
uncertainty from the bag size through the model. Bayesian
tools provide a natural framework for this problem.

We can use the Bayesian nonparametric prior over kernel
mean embeddings (Flaxman et al., 2016) described in Sec-
tion 2.2, and observe the empirical embeddings at the land-
mark points ui. For ui, we take a ﬁxed set of landmarks,
which we can choose via k-means clustering or sample
without replacement (Que and Belkin, 2016). Using the
conjugacy of the model to the Gaussian process prior µi

∼
(m0, ηr(., .)), we obtain a closed-form posterior Gaus-

GP
sian process whose evaluation at points h =

hs
{

nh
s=1 is:
}

µi(h)

xi

|

∼ N

Rh (R + Σi/Ni)−1 (ˆµi

m0) + m0,

−

(cid:16)

Rhh

Rh (R + Σi/Ni)−1 R(cid:62)
h

{

(cid:17)

xi
j}

−
where Rst = ηr(us, ut), (Rhh)st = ηr(hs, ht), (Rh)st =
Ni
ηr(hs, ut), and xi denotes the set
j=1. We take the
prior mean m0 to be the average of the ˆµi; under a lin-
ear kernel K, this means we shrink predictions towards the
mean prediction. Note η essentially controls the strength of
the shrinkage: a smaller η means we shrink more strongly
towards m0. We take Σi to be the average of the empirical
Ni
covariance of
j=1 across all bags, to avoid poor es-
timation of Σi for smaller bags. More intuition about the
behaviour of this estimator can be found in Appendix C.
Now, supposing we have normal observation error σ2, and
use a linear kernel as our second level kernel K, we have:

ϕ(xi
j)
}

{

|

yi

(6)

∼ N

µi, f

∈ H

f, µi
(cid:104)

Hk , σ2
(cid:105)
where f
k. Clearly, this is difﬁcult to work with;
(cid:1)
s
hence we parameterise f as f =
, z(cid:96)), where
(cid:96)=1 α(cid:96)k(
·
s
z =
(cid:96)=1 is a set of landmark points for f , which we
}
can learn or ﬁx. (Appendix D gives a motivation for this
approximation using the representer theorem.) Using the
reproducing property, our likelihood model becomes:

z(cid:96)
{

(cid:80)

(cid:0)

yi

µi, α

|

∼ N

αTµi(z), σ2

(7)

where µi(z) = [µi(z1), . . . , µi(zs)](cid:62). For ﬁxed α and z
(cid:0)
we can analytically integrate out the dependence on µi, and
the predictive distribution of a bag label becomes

(cid:1)

(ξα

i , να
i )

yi

xi, α

|

∼ N
i = α(cid:62)Rz
ξα

R +

(cid:18)

Σi
Ni (cid:19)
Rz

(cid:18)

R +

−

−1

Σi
Ni (cid:19)

i = αT
να

Rzz

(cid:32)

−

−1(ˆµi

m0) + αTm0

RT
z

α + σ2.

(cid:33)

(0, ρ2K −1

∼ N

The prior α
trix on z, gives the standard regularisation on f of
The log-likelihood objective becomes
i )2
ξα

z ), where Kz is the kernel ma-
2
Hk .
(cid:107)

f
(cid:107)

(yi

n

−
ξα
i

αTKzα
2ρ2

.

+

(cid:41)

log να

i +

1
2

i=1 (cid:40)
(cid:88)

Figure 3: Our Bayesian mean shrinkage pooling model.
This diagram takes m0 = 0, η = 1 and u = z, so that
R = Rz = Rzz, and Kz = K.

3.2 Bayesian Linear Regression Model

The most obvious approach to adding uncertainty to the
model of Section 3.1 is to encode uncertainty over regres-
sion parameters β only, as follows:

β

∼ N

(0, ρ2)

yi

xi, β

|

∼ N

(βT ˆµi, σ2).

This is essentially Bayesian linear regression on the empiri-
cal mean embeddings, and is closely related to the model of
Flaxman et al. (2015). Here, we are working directly with
the ﬁnite-dimensional ˆµi, unlike the inﬁnite-dimensional
µi before. Due to the conjugacy of the model, we can eas-
xi, integrating out
ily obtain the predictive distribution yi
the uncertainty over β. This provides us with uncertainty
intervals for the predictions yi.

|

For model tuning, we can maximise the model evidence,
i.e. the marginal log-likelihood (see Bishop (2006) for de-
tails), and use backpropagation through the network to
learn σ and ρ and any kernel parameters of interest.2

3.3 Bayesian Mean Shrinkage Model

A shortcoming of the prior models, and of the standard ap-
proach in Sz´abo et al. (2016), is that they ignore uncertainty
in the ﬁrst level of estimation due to varying number of
samples in each bag. Ideally we would estimate not just the
mean embedding per bag, but also a measure of the sample

2Note that unlike the other models considered in this paper,
we cannot easily do minibatch stochastic gradient descent, as the
marginal log-likelihood does not decompose for each individual
data point.

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

We can use backpropagation to learn the parameters α, σ,
and if we wish η, z, and any kernel parameters. The full
model is illustrated in Figure 3. This approach allows us to
directly encode uncertainty based on bag size in the objec-
tive function, and gives probabilistic predictions.

3.4 Bayesian Distribution Regression

It is natural to combine the two Bayesian models above,
fully propagating uncertainty in estimation of the mean
embedding and of the regression coefﬁcients α. Unfortu-
nately, conjugate Bayesian inference is no longer available.
Thus, we consider a Markov Chain Monte Carlo (MCMC)
sampling based approach, and here use Hamiltonian Monte
Carlo (HMC) for efﬁcient inference, though any MCMC-
type scheme would work. Whereas inference above used
gradient descent to maximise the marginal likelihood, with
the gradient calculated using automatic differentiation, here
we use automatic differentiation to calculate the gradient of
the joint log-likelihood and follow this gradient as we per-
form sampling over the parameters we wish to infer.

We can still exploit the conjugacy of the mean shrinkage
layer, obtaining an analytic posterior over the mean em-
beddings. Conditional on the mean embeddings, we have
a Bayesian linear regression model with parameters α. We
sample this model with the NUTS HMC sampler (Hoffman
and Gelman, 2014; Stan Development Team, 2014).

4 RELATED WORK

As previously mentioned, Sz´abo et al. (2016) provides
a thorough learning-theoretic analysis of the regression
model discussed in Section 2.3. This formalism consid-
ering a kernel method on distributions using their embed-
ding representations, or various scalable approximations
to it, has been widely applied (e.g. Muandet et al., 2012;
Yoshikawa et al., 2014; Flaxman et al., 2015; Jitkrittum
et al., 2015; Lopez-Paz et al., 2015; Mitrovic et al., 2016).
There are also several other notions of similarities on distri-
butions in use (not necessarily falling within the framework
of kernel methods and RKHSs), as well as local smoothing
approaches, mostly based on estimates of various probabil-
ity metrics (Moreno et al., 2003; Jebara et al., 2004; P´oczos
et al., 2011; Oliva et al., 2013; Poczos et al., 2013; Kusner
et al., 2015). For a partial overview, see Sutherland (2016).

Other related problems of learning on instances with
group-level labels include learning with label proportions
(Quadrianto et al., 2009; Patrini et al., 2014), ecological
inference (King, 1997; Gelman et al., 2001), pointillistic
pattern search (Ma et al., 2015), multiple instance learning
(Dietterich et al., 1997; K¨uck and de Freitas, 2005; Zhou
et al., 2009; Krummenacher et al., 2013) and learning with
sets (Zaheer et al., 2017).3

3For more, also see giorgiopatrini.org/nips15workshop.

There have also been some Bayesian approaches in related
contexts, though most do not follow our setting where the
label is a function of the underlying distribution rather than
the observed sample set. K¨uck and de Freitas (2005) con-
sider an MCMC method with group-level labels but focus
on individual-level classiﬁers, while Jackson et al. (2006)
use hierarchical Bayesian models on both individual-level
and aggregate data for ecological inference.

Jitkrittum et al. (2015) and Flaxman et al. (2015) quantify
the uncertainty of distribution regression models by inter-
preting the kernel ridge regression on embeddings as Gaus-
sian process regression. However, the former’s setting has
no uncertainty in the mean embeddings, while the latter’s
treats empirical embeddings as ﬁxed inputs to the learning
problem (as in Section 3.2).

There has also been generic work on input uncertainty
in Gaussian process regression (Girard, 2004; Damianou
et al., 2016). These methods could provide a framework
towards allowing for second-level kernels in our models.
One could also, though, consider regression with uncertain
inputs as a special case of distribution regression, where the
label is a function of the distribution’s mean and Ni = 1.

5 EXPERIMENTS

(cid:82)

We will now demonstrate our various Bayesian approaches:
the mean-shrinkage pooling method with r = k (shrink-
k(x, z)k(z, x(cid:48))ν(dz) for ν pro-
age) and with r(x, x(cid:48)) =
portional to a Gaussian measure (shrinkageC), Bayesian
linear regression (BLR), and the full Bayesian distribu-
tion regression model with r = k (BDR). We also com-
pare the non-Bayesian baselines RBF network (Section 3.1)
and freq-shrinkage, which uses the shrinkage estimator of
Muandet et al. (2014) to estimate mean embeddings. Code
for our methods and to reproduce the experiments is avail-
able at https://github.com/hcllaw/bdr.

We ﬁrst demonstrate the characteristics of our models on a
synthetic dataset, and then evaluate them on a real life age
prediction problem. Throughout, for simplicity, we take
u = z, i.e. R = Rz = Rzz, and Kz = K – although
u and z could be different, with z learnt. Here k is the
standard RBF kernel. We tune the learning rate, number
of landmarks, bandwidth of the kernel and regularisation
parameters on a validation set. For BDR, we use weakly
informative normal priors (possibly truncated at zero); for
other models, we learn the remaining parameters.

5.1 Gamma Synthetic Data

We create a synthetic dataset by repeatedly sampling from
the following hierarchical model, where yi is the label for
the ith bag, each xi
R5 has entries i.i.d. according to
the given distribution, and ε is an added noise term which

j ∈

Bayesian Approaches to Distribution Regression

ated dataset, 25% of the bags have Ni = 20, and 25% have
Ni = 100. Among the other half of the data, we vary the
ratio of Ni = 5 and Ni = 1 000 bags to demonstrate the
methods’ efﬁcacy at dealing with varied bag sizes: we let
s5 be the overall percentage of bags with Ni = 5, ranging
from s5 = 0 (in which case no bags have size Ni = 5) to
s5 = 50 (in which case 50% of the overall bags have size
Ni = 5). Here we do not add additional noise: ε = 0.

Results are shown in Figure 4. BDR and shrinkage meth-
ods, which take into account bag size uncertainty, per-
form well here compared to the other methods. The full
BDR model very slightly outperforms the Bayesian shrink-
age models in both likelihood and in mean-squared error;
frequentist shrinkage slightly outperforms the Bayesian
shrinkage models in MSE, likely because it is tuned for that
metric. We also see that the choice of r affects the results;
r = k does somewhat better.

Figure 5 demonstrates in more detail the difference be-
tween these models. It shows test set predictions of each
model on the bags of different sizes. Here, we can see
explicitly that the shrinkage and BDR models are able to
take into account the bag size, with decreasing variance
for larger bag sizes, while the BLR model gives the same
variance for all outputs. Furthermore, the shrinkage and
BDR models can shrink their predictions towards the mean
more for smaller bags than larger ones: this improves per-
formance on the small bags while still allowing for good
predictions on large bags, contrary to the BLR model.

Fixed bag size: Uncertainty in the regression model.
The previous experiment showed the efﬁcacy of the shrink-
age estimator in our models, but demonstrated little gain
from posterior inference for regression weights β over their
MAP estimates, i.e. there is no discernible improvement of
BLR over RBF network. To isolate the effect of quantify-
ing uncertainty in the regression model, we now consider
the case where there is no variation in bag size at all and
normal noise is added onto the observations. In particular
we take Ni = 1000 and ε
(0, 1), and sample land-
marks randomly from the training set.

∼ N

Results are shown in Table 1. Here, BLR or BDR outper-
form all other methods on all runs, highlighting that uncer-
tainty in the regression model is also important for predic-
tive performance. Importantly, the BDR method performs
well in this regime as well as in the previous one.

5.2

IMDb-WIKI: Age Estimation

We now demonstrate our methods on a celebrity age es-
timation problem, using the IMDb-WIKI database (Rothe
et al., 2016) which consists of 397 949 images of 19 545
celebrities4, with corresponding age labels. This database

4We used only the IMDb images, and removed some implau-
sible images, including one of a cat and several of people with

Figure 4: Top: negative log-likelihood. Bottom: mean-
squared error. For context, performance of the Bayes-
optimal predictor is also shown, and for NLL ‘uniform’
shows the performance of a uniform prediction on the pos-
sible labels. For MSE, the constant overall mean label pre-
dictor achieves about 1.3.

differs for the two experiments below:

yi

yi

∼
iid
∼

Uniform(4, 8)
1
1
Γ
2
yi (cid:20)

yi
2

(cid:18)

,

(cid:19)(cid:21)

xi
j

(cid:2)

(cid:96) |
(cid:3)

+ ε for j

[Ni], (cid:96)

[5].

∈

∈

In these experiments, we generate 1 000 bags for training,
500 bags for a validation set for parameter tuning, 500
bags to use for early-stopping of the models, and 1 000
bags for testing. Tuning is performed to maximize log-
likelihoods for Bayesian models, MSE for non-Bayesian
models. Landmark points u are chosen via k-means (ﬁxed
across all models). We also show results of the Bayes-
optimal model, which gives true posteriors according to
the data-generating process; this is the best performance
any model could hope to achieve. Our learning models,
which treat the inputs as ﬁve-dimensional, fully nonpara-
metric distributions, are at a substantial disadvantage even
in how they view the data compared to this true model.

Varying bag size: Uncertainty in the inputs.
In order
to study the behaviour of our models with varying bag size,
we ﬁx four sizes Ni
. For each gener-

5, 20, 100, 1 000
}

∈ {

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

Figure 5: Predictions for the varying bag size experiment of Section 5.1. Each column corresponds to a single prediction
method. Each point in an image represents a single bag, with its horizontal position the true label yi, and its vertical
position the predicted label. The black lines show theoretical perfect predictions. The rows represent different subsets of
the data: the ﬁrst row shows all bags, the second only bags with Ni = 5, and so on. Colours represent the predictive
standard deviation of each point.

Bayesian Approaches to Distribution Regression

Table 1: Results on the ﬁxed bag size dataset, over
10 dataset draws (standard deviations in parentheses).
BLR/BDR perform best on all runs in both metrics.

METHOD

MSE

NLL

Optimal
RBF network
freq-shrinkage
shrinkage
shrinkageC
BLR
BDR

0.170 (0.009)
0.235 (0.014)
0.232 (0.012)
0.237 (0.014)
0.236 (0.013)
0.228 (0.012)
0.227 (0.012)

0.401 (0.018)
–
–
0.703 (0.027)
0.700 (0.029)
0.681 (0.025)
0.683 (0.025)

Table 2: Results on the grouped IMDb-WIKI dataset over
ten runs (standard deviations in parentheses). Here shrink-
age methods perform the best across all 10 runs.

METHOD

RMSE

NLL

CNN
RBF network
freq-shrinkage
shrinkage
BLR

10.25 (0.22)
9.51 (0.20)
9.22 (0.19)
9.28 (0.20)
9.55 (0.19)

3.80 (0.034)
–
–
3.54 (0.021)
3.68 (0.021)

was constructed by crawling IMDb for images of its most
popular actors and directors, with potentially many images
for each celebrity over time. Rothe et al. (2016) use a con-
volutional neural network (CNN) with a VGG-16 architec-
ture to perform 101-way classiﬁcation, with one class cor-
responding to each age in

0, . . . , 100

{

.
}

We take a different approach, and assume that we are given
several images of a single individual (i.e. samples from
the distribution of celebrity images), and are asked to pre-
dict their mean age based on several pictures. For example,
we have 757 images of Brad Pitt from age 27 up to 51,
while we have only 13 images of Chelsea Peretti at ages 35
and 37. Note that 22.5% of bags have only a single image.
We obtain 19 545 bags, with each bag containing between
1 and 796 images of a particular celebrity, and the corre-
sponding bag label calculated from the average of the age
labels of the images inside each bag.

In particular, we use the representation ϕ(x) learnt by the
CNN in Rothe et al. (2016), where ϕ(x) : R256×256
→
R4096 maps from the pixel space of images to the CNN’s
last hidden layer. With these new representations, we can
now treat them as inputs to our radial basis network, shrink-
age (taking r = k here) and BLR models. Although we
could also use the full BDR model here, due to the compu-
tational time and memory required to perform proper pa-

supposedly negative age, or ages of several hundred years.

rameter tuning, we relegate this to a later study.

We use 9 820 bags for training, 2 948 bags for early stop-
ping, 2 946 for validation and 3 928 for testing. Landmarks
are sampled without replacement from the training set.

We repeat the experiment on 10 different splits of the data,
and report the results in Table 2. The baseline CNN results
give performance by averaging the predictive distribution
from the model of Rothe et al. (2016) for each image of a
bag; note that this model was trained on all of the images
used here. From Table 2, we can see that the shrinkage
methods have the best performance; they outperforms all
other methods in all 10 splits of the dataset, in both met-
rics. Non-Bayesian shrinkage again yields slightly better
RMSEs, likely because it is tuned for that metric. This
demonstrates that modelling bag size uncertainty is vital.

6 CONCLUSION

Supervised learning on groups of observations using ker-
nel mean embeddings typically disregards sampling vari-
ability within groups. To handle this problem, we con-
struct Bayesian approaches to modelling kernel mean em-
beddings within a regression model, and investigate advan-
tages of uncertainty propagation within different compo-
nents of the resulting distribution regression. The ability
to take into account the uncertainty in mean embedding es-
timates is demonstrated to be key for constructing mod-
els with good predictive performance when group sizes are
highly imbalanced. We also demonstrate that the results of
a complex neural network model for age estimation can be
improved by shrinkage.

Our models employ a neural network formulation to pro-
vide more expressive feature representations and learn dis-
criminative embeddings. Doing so makes our model easy
to extend to more complicated featurisations than the sim-
ple RBF network used here. By training with backpropa-
gation, or via approximate Bayesian methods such as vari-
ational inference, we can easily ‘learn the kernel’ within
our framework, for example ﬁne-tuning the deep network
of Section 5.2 rather than using a pre-trained model. We
can also apply our networks to structured settings, learning
regression functions on sets of images, audio, or text. Such
models naturally ﬁt into the empirical Bayes framework.

On the other hand, we might extend our model to more
Bayesian feature learning by placing priors over the kernel
hyperparameters, building on classic work on variational
approaches (Barber and Schottky, 1998) and fully Bayesian
inference (Andrieu et al., 2001) in RBF networks. Such
approaches are also possible using other featurisations, e.g.
random Fourier features (as in Oliva et al., 2015).

Future distribution regression approaches will need to ac-
count for uncertainty in observation of the distribution. Our
methods provide a strong, generic building block to do so.

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

References

Christophe Andrieu, Nando De Freitas, and Arnaud
Doucet. Robust full bayesian learning for radial ba-
sis networks. Neural Computation, 13(10):2359–2407,
2001.

David Barber and Bernhard Schottky. Radial basis func-
tions: a bayesian treatment. NIPS, pages 402–408, 1998.
C.M. Bishop. Pattern recognition and machine learning.

Springer New York, 2006.

David S Broomhead and David Lowe. Radial basis func-
tions, multi-variable functional interpolation and adap-
tive networks. Technical report, DTIC Document, 1988.

Andreas C. Damianou, Michalis K. Titsias, and Neil D.
Lawrence. Variational inference for latent variables and
uncertain inputs in Gaussian processes. JMLR, 17(42):
1–62, 2016.

Thomas G Dietterich, Richard H Lathrop, and Tom´as
Lozano-P´erez. Solving the multiple instance problem
with axis-parallel rectangles. Artiﬁcial intelligence, 89
(1):31–71, 1997.

Seth Flaxman, Yu-Xiang Wang, and Alexander J Smola.
Who supported Obama in 2012?: Ecological inference
through distribution regression. In KDD, pages 289–298.
ACM, 2015.

Seth Flaxman, Dino Sejdinovic, John P. Cunningham, and
Sarah Filippi. Bayesian learning of kernel embeddings.
In UAI, 2016.

Seth Flaxman, Dougal J. Sutherland, Yu-Xiang Wang,
and Yee-Whye Teh. Understanding the 2016 US pres-
inference and dis-
idential election using ecological
tribution regression with census microdata.
2016.
arXiv:1611.03787.

Thomas G¨artner. Kernels for Structured Data, volume 72.
World Scientiﬁc, Series in Machine Perception and Arti-
ﬁcial Intelligence, 2008.

Andrew Gelman, David K Park, Stephen Ansolabehere,
Phillip N Price, and Lorraine C Minnite. Models, as-
sumptions and model checking in ecological regressions.
Journal of the Royal Statistical Society: Series A (Statis-
tics in Society), 164(1):101–118, 2001.

Agathe Girard. Approximate methods for propagation of
uncertainty with Gaussian process models. PhD thesis,
University of Glasgow, 2004.

Matthew D. Hoffman and Andrew Gelman. The no-U-turn
sampler: Adaptively setting path lengths in Hamiltonian
Monte Carlo. JMLR, pages 1593–1623, 2014.

Christopher Jackson, Nicky Best, and Sylvia Richardson.
Improving ecological inference using individual-level
data. Statistics in medicine, 25(12):2136–2159, 2006.

Tony Jebara, Risi Imre Kondor, and Andrew Howard. Prob-

ability product kernels. JMLR, 5:819–844, 2004.

Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess,
S. M. Ali Eslami, Balaji Lakshminarayanan, Dino Se-
jdinovic, and Zolt´an Szab´o. Kernel-Based Just-In-Time
Learning for Passing Expectation Propagation Mes-
sages. In UAI, 2015.

Gary King. A Solution to the Ecological Inference Problem.
Princeton University Press, 1997. ISBN 0691012407.

Diederik Kingma and Jimmy Ba. Adam: A method
In ICLR, 2015.

stochastic optimization.

for
arXiv:1412.6980.

Gabriel Krummenacher, Cheng Soon Ong, and Joachim M
Buhmann. Ellipsoidal multiple instance learning.
In
ICML (2), pages 73–81, 2013.

Hendrik K¨uck and Nando de Freitas. Learning about in-
dividuals from group statistics. In UAI, pages 332–339,
2005.

Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Wein-
berger. From word embeddings to document distances.
In ICML, pages 957–966, 2015.

H. C. L. Law, C. Yau, and D. Sejdinovic. Testing and learn-
ing on distributions with symmetric noise invariance. In
NIPS, 2017. arXiv:1703.07596.

Bernhard
Towards a learn-

David Lopez-Paz, Krikamol Muandet,

Sch¨olkopf, and Ilya Tolstikhin.
ing theory of cause-effect inference. In ICML, 2015.
Milan Luki´c and Jay Beder. Stochastic processes with sam-
ple paths in reproducing kernel hilbert spaces. Trans-
actions of the American Mathematical Society, 353(10):
3945–3969, 2001.

Yifei Ma, Dougal J. Sutherland, Roman Garnett, and Jeff
In AIS-

Schneider. Active pointillistic pattern search.
TATS, 2015.

J. Mitrovic, D. Sejdinovic, and Y.W. Teh. DR-ABC:
Approximate Bayesian Computation with Kernel-Based
In ICML, pages 1482–1491,
Distribution Regression.
2016.

Pedro J Moreno, Purdy P Ho, and Nuno Vasconcelos. A
Kullback-Leibler divergence based kernel for SVM clas-
siﬁcation in multimedia applications. In NIPS, 2003.
Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo,
Learning from distribu-
In NIPS, 2012.

and Bernhard Sch¨olkopf.
tions via support measure machines.
arXiv:1202.6504.

Krikamol Muandet, Kenji Fukumizu, Bharath Sriperum-
budur, Arthur Gretton, and Bernhard Schoelkopf. Kernel
mean estimation and stein effect. In ICML, 2014.

Michelle Ntampaka, Hy Trac, Dougal J. Sutherland,
Nicholas Battaglia, Barnab´as P´oczos, and Jeff Schnei-
A machine learning approach for dynamical
der.
The Astro-
mass measurements of galaxy clusters.
physical Journal, 803(2):50, 2015.
ISSN 1538-4357.
arXiv:1410.0686.

Bayesian Approaches to Distribution Regression

Ingo Steinwart. Convergence types and rates in generic
Karhunen-Lo´eve expansions with applications to sam-
ple path properties. arXiv preprint arXiv:1403.1040v3,
March 2017.

Dougal J. Sutherland. Scalable, Flexible, and Active Learn-
ing on Distributions. PhD thesis, Carnegie Mellon Uni-
versity, 2016.

Zolt´an Sz´abo, Bharath K. Sriperumbudur, Barnab´as
Leraning theory for
JMLR, 17(152):1–40, 2016.

P´oczos, and Arthur Gretton.
distribution regression.
arXiv:1411.2066.

Grace Wahba. Spline models for observational data, vol-

ume 59. Siam, 1990.

Yuya Yoshikawa, Tomoharu Iwata, and Hiroshi Sawada.
Latent support measure machines for bag-of-words data
classiﬁcation. In NIPS, pages 1961–1969, 2014.

Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barn-
abas Poczos, Ruslan Salakhutdinov, and Alexander
Smola. Deep sets. In NIPS, 2017.

Zhi-Hua Zhou, Yu-Yin Sun, and Yu-Feng Li. Multi-
instance learning by treating instances as non-iid sam-
ples. In ICML, 2009.

Michelle Ntampaka, Hy Trac, Dougal J. Sutherland, S. Fro-
menteau, B. Poczos, and Jeff Schneider. Dynamical
mass measurements of contaminated galaxy clusters us-
ing machine learning. The Astrophysical Journal, 831
(2):135, 2016. arXiv:1509.05409.

Junier B Oliva, Barnab´as P´oczos, and Jeff Schneider. Dis-
tribution to distribution regression. In ICML, 2013.

Junier B Oliva, Avinava Dubey, Barnab´as P´oczos, Jeff
Schneider, and Eric P Xing. Bayesian nonparametric
kernel-learning. In AISTATS, 2015. arXiv:1506.08776.

Giorgio Patrini, Richard Nock, Tiberio Caetano, and Paul

Rivera. (Almost) no label no cry. In NIPS. 2014.

Natesh S Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee,
and Robert L Wolpert. Characterizing the function space
for bayesian kernel models. JMLR, 8(Aug):1769–1797,
2007.

Barnab´as P´oczos, Liang Xiong, and Jeff Schneider. Non-
parametric divergence estimation with applications to
machine learning on distributions. In UAI, 2011.

Barnabas Poczos, Aarti Singh, Alessandro Rinaldo,
Distribution-free distribu-
In AISTATS, pages 507–515, 2013.

and Larry Wasserman.
tion regression.
arXiv:1302.0082.

Novi Quadrianto, Alex J Smola, Tiberio S Caetano, and
Quoc V Le. Estimating labels from label proportions.
JMLR, 10:2349–2374, 2009.

Qichao Que and Mikhail Belkin. Back to the future: Radial
basis function networks revisited. In AISTATS, 2016.

Ali Rahimi and Benjamin Recht. Random features for
large-scale kernel machines. In NIPS, pages 1177–1184,
2007.

Aaditya Ramdas and Leila Wehbe. Nonparametric inde-
pendence testing for small sample sizes. In IJCAI, 2015.
arXiv:1406.1922.

Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep
expectation of real and apparent age from a single im-
age without facial landmarks. International Journal of
Computer Vision (IJCV), July 2016.

Craig Saunders, Alexander Gammerman, and Volodya
Vovk. Ridge regression learning algorithm in dual vari-
ables. In ICML, 1998.

Bernhard Sch¨olkopf, Ralf Herbrich, and Alex J. Smola. A

generalized representer theorem. In COLT, 2001.

Bharath K Sriperumbudur, Arthur Gretton, Kenji Fuku-
mizu, Bernhard Sch¨olkopf, and Gert RG Lanckriet.
Hilbert space embeddings and metrics on probability
measures. JMLR, 99:1517–1561, 2010.

Stan Development Team. Stan: A c++ library for proba-
bility and sampling, version 2.5.0, 2014. URL http:
//mc-stan.org/.

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

A Choice of r(

,
·

) to ensure µP
·

k
∈ H

We need to choose an appropriate covariance function r, such that µP
,
for inﬁnite-dimensional RKHSs not sufﬁcient to deﬁne r(
·

k (Wahba, 1990) (but see below). However, we can construct

) = k(
·

,
·

k, where µP

)). In particular, it is
,
(0, r(
∈ H
·
), as draws from this particular prior are no longer in
·

∼ GP

·

H

r(x, y) =

k(x, z)k(z, y)ν(dz)

(cid:90)

(8)

k with probability 1 by the nuclear dominance (Luki´c and
where ν is any ﬁnite measure on
∈ H
Beder, 2001; Pillai et al., 2007) for any stationary kernel k. In particular, Flaxman et al. (2016) provides details when k is
a squared exponential kernel deﬁned by

. This then ensures µP

X

k(x, y) = exp(

(x

1
2

−

y)(cid:62)Σ−1

k (x

y))

−

−

x, y

p

R

∈

) with a non-
and ν(dz) = exp
·
stationary component. In this paper, we take Σk = σ2Ip, where σ2 and (cid:96) are tuning parameters, or parameters that we
learn.

it is proportional to a Gaussian measure on Rd, which provides r(
·

dz, i.e.

−

(cid:17)

(cid:16)

,

||z||2
2
2(cid:96)2

Here, the above holds for a general set of stationary kernels, but note that by taking a convolution of a kernel with itself,
it might make the space of functions that we consider overly smooth (i.e. concentrated on a small part of
k). In this
work, however, we consider only the Gaussian RBF kernel k. In fact, recent work (Steinwart, 2017, Theorem 4.2) actually
shows that in this case, the sample paths almost surely belong to (interpolation) spaces which are inﬁnitesimally larger
than the RKHS of the Gaussian RBF kernel. This suggests that we can choose r to be an RBF kernel with a length scale
that is inﬁnitesimally bigger than that of k; thus, in practice, taking r = k would sufﬁce and we do observe that it actually
performs better (Fig. 4).

H

B Framework for Binary Classiﬁcation

Suppose that our labels yi
for uncertainty in the regression parameters is to use bayesian logistic regression, putting priors on β, i.e.

0, 1
, i.e. we are in a binary classiﬁcation framework. Then a simple approach to accounting
}

∈ {

(0, ρ2)

∼ N

β

yi

∼

Ber(πi), where log

= β(cid:62) ˆµi

πi

1

(cid:18)

−

πi (cid:19)
µi, α, we would not be able to obtain an analytical

however for the mean shrinkage pooling model, if we use the above yi
solution for p(yi

xi, α). Instead we use the probit link function, as given by:

|

|

where Φ denotes the Cumulative Distribution Function (CDF) of a standard normal distribution, with µi(z) =
[µi(z1), . . . , µi(zs)](cid:62). Then as before we have

µi, α) = Φ
P r(yi = 1
|

α(cid:62)µi(z)

(cid:0)

(cid:1)

µi(z)

xi

|

∼ N

(Mi, Ci)

with Mi and Ci as deﬁned in section 3.3. Hence, as before

P r(yi = 1
|

xi, α) =

P r(yi = 1
|

xi)dµi(z)
µi, α)p(µi(z)
|

Φ(α(cid:62)µi(z)) exp

(µi(z)

Mi)(cid:62)C −1

i

(µi(z)

1
2

{−

−

Mi)
}

−

dµi(z)

(with li = µi(z)

Mi) = c

Φ(α(cid:62)(li + Mi)) exp

−

1
2

{−

(li)(cid:62)C −1

i

(li)

dli
}

= P r(Y

α(cid:62)(li + Mi))

≤

(cid:90)
= c

(cid:90)

(cid:90)

Bayesian Approaches to Distribution Regression

Note here Y

(0, 1) and li

(0, Σi) Then expanding and rearranging

∼ N

∼ N

Note that since Y and li independent normal r.v., Y
have:

−

α(cid:62)li

∼ N

xi, α) = P r(Y
P r(yi = 1
|

α(cid:62)li

α(cid:62)Mi)

≤

−
(0, 1 + α(cid:62)Ciα(cid:62)). Let T be standard normal, then we

xi, α) = P r(
P r(yi = 1
|

α(cid:62)Mi)

1 + α(cid:62)Ciα T
α(cid:62)Mi
1 + α(cid:62)Ciα

≤

≤

)

α(cid:62)Mi
(cid:112)
1 + α(cid:62)Ciα (cid:33)

(cid:112)
= P r(T

= Φ

(cid:32)

(cid:112)

Hence, we also have:

xi, α) = 1
P r(yi = 0
|

−

Φ

α(cid:62)Mi
1 + α(cid:62)Ciα (cid:33)

(cid:32)

(cid:112)

Now placing the prior α

(0, ρ2K −1

z ), we have the following MAP objective:

∼ N

J(α) = log

p(α)

n

i=1
(cid:89)

p(yi

xi, α)
|

(cid:35)

=

(1

yi) log(1

Φ

(cid:34)

n

−

i=1
(cid:88)

+yi log(Φ

α(cid:62)Mi
)
1 + α(cid:62)Ciα (cid:33)

−

(cid:32)

(cid:112)

α(cid:62)Mi
1 + α(cid:62)Ciα (cid:33)

) +

1
ρ2 α(cid:62)Kzα

(cid:32)

(cid:112)

xi, α), we can also use this in HMC for BDR.
Since we have an analytical solution for P r(yi = 0
|

C Some more intuition on the shrinkage estimator

In this section, we provide some intuition behind the shrinkage estimator in section 3.3. Here, for simplicity, we choose
Σi = τ 2I for all bag i, and m0 = 0, and consider the case where z = u, i.e. R = Rz = Rzz. We can then see that if R
has eigendecomposition U ΛU T , with Λ = diag(λk), the posterior mean is

so that large eigenvalues, λk
towards 0. Likewise, the posterior variance is

(cid:29)

τ 2/Ni, are essentially unchanged, while small eigenvalues, λk

τ 2/Ni, are shrunk

(cid:28)

U diag

λk
λk + τ 2/Ni (cid:19)

(cid:18)

U T (ˆµi),

U diag

λk

(cid:32)

−

λ2
k
λk + τ 2

Ni (cid:33)

U T = U diag

1
Ni
τ 2 + 1

λk (cid:33)

(cid:32)

U T ;

its eigenvalues also decrease as Ni/τ 2 increases.

D Alternative Motivation for choice of f

Here we provide an alternative motivation for the choice of f =
model with a linear kernel K on µi, where f :

k

R:

k
, zs). First, consider the following Bayesian
s=1 αsk(
·

H

→

yi

µi, f

|

(cid:80)
f (µi), σ2

.

(cid:1)

∼ N

(cid:0)

Ho Chung Leon Law∗, Dougal J. Sutherland∗, Dino Sejdinovic, Seth Flaxman

Now considering the log-likelihood of

µ, Y
{

}

=

µi, yi
{

n
i=1 (supposing we have these exact embeddings), we obtain:
}

log p(Y

µ, f ) =
|

n

1
2σ2 (yi

−

−

f (µi))2

i=1
(cid:88)
To avoid over-ﬁtting, we place a Gaussian prior on f , i.e.
likelihood over f

k, we have:

−

∈ H

f ∗ = argminf ∈Hk

1
2σ2 (yi

−

f (µi))2 + λ

f
||

||

Hk

log p(f ) = λ

Hk + c. Minimizing the negative log-

f
||

||

Now this is in the form of an empirical risk minimisation problem. Hence using the representer theorem (Sch¨olkopf et al.,
2001), we have that:

i.e. we have a ﬁnite-dimensional problem to solve. Thus since K is a linear kernel:

f =

γjK(., µj)

yi

µi,

|

µj
{

n
j=1, γ
}

∼ N 

γj

µi, µj

(cid:104)

Hk , σ2
(cid:105)

n

j=1
(cid:88)



.





where

µi, µj

Hk can be thought of as the similarity between distributions.

(cid:104)

(cid:105)

xi, γ). This suggests we need to
Now we have the same
|
integrate out µ1, . . . µn. But it is unclear how to perform this integration, since the µi follow Gaussian process distributions.
Hence we can take an approximation to f , i.e. f =
, zs), which would essentially give us a dual method with
·
a sparse approximation to f .

posterior as in Section 3.3, and we would like to compute p(yi

k
s=1 αsk(

GP

n

i=1
(cid:88)

n

j=1
(cid:88)

(cid:80)

