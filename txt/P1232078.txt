Morse Code Datasets for Machine Learning

Sourya Dey, Keith M. Chugg and Peter A. Beerel
Ming Hsieh Department of Electrical Engineering
University of Southern California
Los Angeles, California 90089, USA
{souryade, chugg, pabeerel}@usc.edu

8
1
0
2
 
c
e
D
 
1
 
 
]

G
L
.
s
c
[
 
 
2
v
9
3
2
4
0
.
7
0
8
1
:
v
i
X
r
a

Abstract—We present an algorithm to generate synthetic
datasets of tunable difﬁculty on classiﬁcation of Morse code
symbols for supervised machine learning problems, in particular,
neural networks. The datasets are spatially one-dimensional and
have a small number of input features, leading to high density
of input information content. This makes them particularly
challenging when implementing network complexity reduction
methods. We explore how network performance is affected by
deliberately adding various forms of noise and expanding the
feature set and dataset size. Finally, we establish several metrics
to indicate the difﬁculty of a dataset, and evaluate their merits.
The algorithm and datasets are open-source.

Index Terms—Machine learning, Artiﬁcial neural networks,

Data science, Information theory, Classiﬁcation

I. INTRODUCTION AND PRIOR WORK

Neural networks in machine learning systems are commonly
employed to tackle classiﬁcation problems involving charac-
ters or images. In such problems, the neural network (NN)
processes an input sample and predicts which class it belongs
to. The inputs and their classes are drawn from a dataset, such
as the MNIST [1] dataset containing images of handwritten
digits, or the CIFAR [2] and ImageNet [3] datasets containing
images of common objects such as birds and houses. A NN is
ﬁrst trained using numerous examples where the input sample
and its class label are both available, then used for inference
(i.e. prediction) of the classes of input samples whose class
labels are not available. The training stage is data-hungry
and typically requires thousands of labeled examples. It is
therefore often a challenge to obtain adequate amounts of high
quality and accurate data required to sufﬁciently train a NN.
A possible solution is to obtain data by synthetic instead of
natural means. Synthetic data are generated using computer al-
gorithms instead of being collected from real-world scenarios.
The advantages are that a) computer algorithms can be tuned
to mimic real-world settings to desired levels of accuracy, and
b) a theoretically unlimited amount of data can be generated
by running the algorithm long enough. The effects of dataset
size on network performance has been explored in [4], in
particular, more inputs are beneﬁcial in reducing overﬁtting
and improving robustness and generalization capabilities of
NNs [5], [6]. Synthetic data has been successfully used in

©2018 IEEE

Original IEEE Publication Citation:
S. Dey, K. M. Chugg and P. A. Beerel, ”Morse Code Datasets for Machine
Learning,” 2018 9th International Conference on Computing, Communication
and Networking Technologies (ICCCNT), 2018, pp. 1-7. doi: 10.1109/ICC-
CNT.2018.8494011

problems such as 3D imaging [7], point tracking [8], breaking
Captchas on popular websites [9], and augmenting real world
datasets [10].

This present work introduces a family of synthetic datasets
on classifying Morse codewords. Morse code is a system of
communication where each letter, number or symbol in a
language is represented using a sequence of dots and dashes,
separated by spaces. It is widely used to communicate in
situations where voice is not possible, such as helping people
with disabilities talk [11]–[14], or where message transmission
needs to be achieved using only 2 states [15], or in rehabilita-
tion and education [16]. Morse code is a useful skill to learn
and there exist cellphone apps designed to train people in its
usage [17], [18].

Our work uses feed-forward multi-layer perceptron neu-
ral networks to directly classify Morse codewords into 64
character classes comprising letters, numbers and symbols.
This is different from previous works such as [13]–[15],
[19] which only had 2 classes corresponding to dots and
dashes. In particular, [15] used fuzzy logic on inputs from
a microcontroller used in security systems, while [14] used
least mean squares approximation, both to classify dots and
dashes. There has also been previous work using time series
and recurrent networks to decode English words in Morse code
[20], [21], while [22] used radial basis function networks to
classify characters with 84% accuracy. Accuracy is a common
metric for describing the performance of a classiﬁcation NN
and is measured as the percentage of class labels correctly
predicted by the NN during inference.

The key contributions of the present work are as follows:
• An algorithm (described in Section II) to generate ma-
chine learning datasets of varying difﬁculty. To the best
of our knowledge, we are the ﬁrst to develop an algo-
rithm which can scale the difﬁculty of machine learning
datasets. The difﬁculty of a dataset can be observed from
the accuracy of a NN training on it – harder datasets lead
to lower accuracy, and vice-versa. We discuss techniques
to make datasets harder and show corresponding accuracy
results in Section III. Encountering harder datasets leads
to aggressive exploration of network hyperparameters
and learning algorithms, which ultimately increases the
robustness of NNs training on them. The algorithm and
datasets are open source and available on Github [23].
• In Section IV, we introduce metrics to quantify the
difﬁculty of a dataset. While some of these arise from

information theory, we also come up with a new metric
which achieves a high correlation coefﬁcient with the
accuracy achieved by NNs on a dataset. Our metrics are
a useful way to characterize how hard a dataset is without
having a NN train on them.

• This work is one of few to introduce a spatially 1-
dimensional dataset. This is in contrast to the wide array
of image and character recognition datasets which are
usually 2-dimensional such as MNIST, where each image
has width and height, or 3-dimensional such as CIFAR
and ImageNet, where each image has width, height and
a number of features. The number of spatial dimensions
in the input data is important when dealing with low-
complexity sparse NNs. Previous works [24]–[30] have
focused on making NNs sparse, while keeping the re-
sulting accuracy reduction to a minimum. The family of
Morse code datasets described in the present work was
designed to test the limits of sparse NNs, as described in
Section III-C.

II. GENERATING ALGORITHM

We picked 64 class labels for our dataset – the 26 English
letters, the 10 Arabic numerals, and 28 other symbols such
as (, +, :, etc. Each of these is represented by a sequence of
dots and dashes in Morse code, for example, + is represented
as • — • — •. So as to mimic a real-world scenario in our
algorithm, we imagined a human or a Morse code machine
writing out this sequence within a frame of ﬁxed size. Wher-
ever the pen or electronic instrument touches is darkened and
has a high intensity, indicating the presence of dots and dashes,
while the other parts are left blank (i.e. spaces).

1) Step 1 – Frame Partitioning: For our algorithm, each
Morse codeword lies in a frame which is a vector of 64 values.
Within the frame, the length of a sequence having consecutive
similar values is used to differentiate between a dot and a dash.
In the baseline dataset, a dot can be 1-3 values wide and a
dash 4-9. This is in accordance with international Morse code
regulations [31] where the size or duration of a dash is around
3 times that of a dot. The space between a dot and a dash can
have a length of 1-3 values. The exact length of a dot, dash
or space is chosen from these ranges according to a uniform
probability distribution. This is to mimic the human writer who
is not expected to make each symbol have a consistent length,
but can be expected to make dots and spaces around the same
size, and dashes longer than them. The baseline dataset has
no leading spaces before the 1st dot or dash, i.e. the codeword
starts from the left edge of the frame. There are trailing spaces
to ﬁll up the right side of the frame after all the dots and dashes
are complete.

2) Step 2 – Assigning Values for Intensity Levels: All values
in the frame are initially real numbers in the range [0, 16]
and indicate the intensity of that point in the frame. For dots
and dashes, the values are drawn from a normal distribution
with mean µ = 12 and standard deviation σ = 4/3. The idea
is to have the six-sigma range from (12 − 3 × 4/3) = 8 to
(12 + 3 × 4/3) = 16. This ensures that any value making up a

Fig. 1. Generating the Morse codeword • — • — • corresponding to the
+ symbol. The ﬁrst 3 steps, prior to normalizing, are shown. Only integer
values are shown for convenience, however, the values can and generally will
be fractional. Normal(µ, σ) denotes a normal distribution with mean = µ,
standard deviation = σ. For this ﬁgure, σ = 1.

dot or a dash will lie in the upper half of possible values, i.e.
in the range [8, 16]. The value of a space is exactly 0. Once
again, these conditions mimic the human or machine writer
who is not expected to have consistent intensity for every dot
and dash, but can be expected to not let the writing instrument
touch portions of the frame which are spaces.

3) Step 3 – Noising: Noise in input samples is often
deliberately injected as a means of avoiding overﬁtting in NNs
[5], and has been shown to be superior to other methods of
avoiding overﬁtting [32]. This was, however, the secondary
reason behind our experimenting with noise. The primary
reason was to deliberately make the data hard to classify
and test the limits of different NNs processing it. Noise can
be thought of as a human accidentally varying the intensity
of writing the Morse codeword, or a Morse communication
channel having noise. The baseline dataset has no noise,
while others have additive noise from a mean-zero normal
distribution applied to them. Fig. 1 shows the 3 steps up to
this point. Finally, all the values are normalized to lie within
the range [0, 1] with precision of 3 decimal places.

4) Step 4 – Mass Generation: Steps 1-3 describe the
generation of 1 input sample corresponding to some particular
class label. This can be repeated as many times as required for
each of the 64 class labels. This demonstrates a key advantage
of synthetic over real-world data – the ability to generate an
arbitrary amount of data having an arbitrary prior probability
distribution over its classes. The baseline dataset has 7,000
examples for each class, for a total of 448,000 examples.

A. Variations and Difﬁculty Scaling

The baseline dataset is as described so far, except that
σ = 0, i.e. it has no additive noise. We experimented with

Fig. 2. Percentage accuracies obtained by the NN described in III-A on the test subset of the datasets described in II-A. The rightmost set of bars corresponds
to Morse 4.σ with L2 regularization.

the following variations in datasets:

1) Baseline with additive noise = Normal(0, σ), σ ∈
{0, 1, 2, 3, 4}. These are called Morse 1.σ, i.e. 1.0 to
1.4, where 1.0 is the baseline.

2) Instead of having the codeword start from the left edge
of the frame, we introduced a random number of leading
spaces. For example, in Fig. 1, the codeword occupies
a length of 26 values. The remaining 38 space values
can be randomly divided between leading and trailing
spaces. This increases the difﬁculty of the dataset since
no particular set of neurons are expected to be learning
dots and dashes as the actual codeword could be any-
where in the frame. Just like variation 1, we added noise
and call these datasets Morse 2.σ, σ ∈ {0, 1, 2, 3, 4}.
3) There is no overlap between the lengths of dots and
dashes in the datasets described so far. The difﬁculty
can be increased by making dash length = 3-9 values,
which is exactly according to the convention of having
dash length thrice of dot length. This means that dashes
can masquerade as dots and spaces, and vice-versa. This
is done on top of introducing leading spaces. These
datasets are called Morse 3.σ, σ being as before.

4) The Morse datasets only have 64 inputs, which is quite
small compared to others such as MNIST (784 inputs),
CIFAR (3072 inputs), or ImageNet (150,528 inputs).
This makes the Morse datasets hard to classify since
there is less redundancy in inputs, so a given amount of
noise will lead to greater reductions in signal-to-noise
ratio (SNR) compared to other datasets. To make the
Morse datasets easier, we introduced dilation by a factor
of 4. This is done by scaling all lengths in variation 3 by
a factor of 4, i.e. frame length is 256, dot sizes and space
sizes are 4-12, and dash size is 12-36. These datasets are
called Morse 4.σ, σ being as before.

5) Increasing the number of training examples, i.e. the size
of the dataset, makes it easier to classify since a NN has
more labeled training examples to learn from. Accord-

ingly we chose Morse 3.1 and scaled the number of ex-
amples to obtain Morse Size x, x ∈ {1/8, 1/4, 1/2, 2, 4, 8}.
For example, Morse Size 1/2 has 3,500 examples for each
class, for a total of 224,000 examples.

III. NEURAL NETWORK RESULTS AND ANALYSIS

A. Network Setup

Our NN needs to have 64 output neurons to match the
number of classes. The number of input neurons always
matches the frame length, i.e. 256 for the Morse 4.σ datasets,
and 64 for all others. We used a single hidden layer with 1024
neurons. The performance, i.e. accuracy, generally increases
on adding more hidden neurons, however, we stuck with
1024 since values above that yielded diminishing returns. The
network is purely multi-layer perceptron, i.e. there are only
fully connected layers. The hidden layer has ReLU activations,
while the output is a softmax probability distribution. We used
the Adam optimizer with default parameters [33], He normal
initialization for the weights [34], and trained for 30 epochs
using a minibatch size of 128. We used 6/7th of the total
examples for training the NN and the remaining 1/7th for
testing at the end of training. All reported accuracies are those
obtained on the test samples.

No constraints were imposed on the weights for the NNs
training on Morse 1.σ, 2.σ and 3.σ, since our experimental
results indicated that this led to optimum performance. How-
ever, the NNs for Morse 4.σ are more prone to overﬁtting
due to having more input neurons, leading to more weight
parameters. Accordingly we regularized the weights using
an L2 coefﬁcient λ = 10−5, which was the best value as
determined experimentally.

B. Results

Note that the entirety of this work – generation of various
datasets, implementing NNs to process them, and evaluation
of metrics – uses the Python programming language. Test
accuracy results after training the NN on the different Morse
datasets are shown in Fig. 2. As expected, increasing the

Fig. 4. Effects of increasing the size of Morse 3.1 by a factor of x on test
accuracy after 30 epochs (blue), and (Training Accuracy - Test Accuracy)
after 30 epochs (orange).

to training on a smaller dataset for more epochs, with the
important added advantage that overﬁtting is reduced. This
is shown in Fig. 4, which shows improving test accuracy as
the dataset is made larger. At the same time, the difference
between ﬁnal training accuracy and test accuracy reduces,
the network is generalizing better and
which implies that
not overﬁtting. Note that Morse Size 8 has 3 million labeled
training examples – a beneﬁcial consequence of being able to
cheaply generate large quantities of synthetic data.

C. Results for Sparse Networks

Our previous work [24]–[26] has focused on network com-
plexity reduction in the form of pre-deﬁned sparsity. In a pre-
deﬁned sparse network, as opposed to a fully connected one, a
fraction of the weights are chosen to be deleted before starting
training. These weights never appear during the workﬂow of
the NN. Consider our (64,1024,64) NN as an example. When
fully connected, it has 64 × 1024 + 1024 × 64 = 131, 072
weights, which gives a fractional density = 1. If we choose to
delete 75% of the weights at the beginning, then we are left
with a NN which has 32,768 weights, i.e. fractional density =
1/4. This leads to reduced storage and operational complexity,
which is particularly important for hardware realizations of
NNs, but possibly at the cost of performance degradation.

Fig. 5 shows the performance degradation for 4 different
Morse datasets. Note how the baseline dataset is reasonably
accurately classiﬁed by a NN with only a quarter of the
weights, while performance drops off much more rapidly when
dataset variations are introduced. These variations lead to
increased information content per neuron per training example.
As a result, the reduction in information learning capability as
a result of deleting weights is much more severe. Also note that
as density is reduced, Morse 4.2 has the best performance out
of the non-baseline models tested in Fig. 5. This is because it
has more weights to begin with, due to the increased number of
input neurons. Finally, note that regularization was not applied
to any of the sparse models since reducing the number of NN

Fig. 3. Effects of noise leading to spaces (orange) getting confused (brown)
with dots and dashes (blue). Higher values of noise σ lead to increased
probability of the brown region, making it harder for the NN to discern
between ‘dots and dashes’ and spaces. The x-axis in each plot shows values
in the range [0, 16], i.e. before normalizing to [0, 1].

standard deviation of noise results in drop in performance.
This effect is not felt strongly when σ = 1 since the 3σ range
can take spaces to a value of 3 (on a scale of [0, 16], i.e.
before normalizing to [0, 1]), while dots and dashes can drop
to 8−3 = 5, so the probability of a space being confused with
a dot or dash is basically 0. Confusion can occur for σ ≥ 2,
and gets worse for higher values, as shown in Fig. 3.

Since the codeword lengths do not often stretch beyond 32,
the ﬁrst half of neurons usually encounter high input intensity
values corresponding to dots and dashes during training. This
means that the latter half of neurons mostly encounter lower
input values corresponding to spaces. This aspect changes
when introducing leading spaces, which become inputs to
some neurons in the ﬁrst half. The result is an increase in the
variance of the input to each neuron. As a result, accuracy
drops. The degradation is worse when dashes can have a
length of 3-9. Since the lengths are drawn from a uniform
distribution, 1/7th of dashes can now be confused with 1/3rd
of dots and 1/3rd of intermediate spaces. As an example, for
the + codeword which has 2 dashes, 3 dots and 4 intermediate
spaces, there is a (2/9 × 1/7 + 3/9 × 1/3 + 4/9 × 1/3) = 29%
chance of this confusion occurring. Dilating by 4, however,
reduces this chance to (2/9 × 1/25 + 3/9 × 1/9 + 4/9 × 1/9) =
9.5%. Accuracy is better as a result, and is further improved
by properly regularizing the NN so that it doesn’t overﬁt.

Increasing dataset size has a beneﬁcial effect on perfor-
mance. Giving the NN more examples to train from is akin

particular class will be more prone to errors if it is close to
other classes. This can be quantiﬁed by looking at dmin(m)
,
where the numerator is given as:

σm

dmin(m) =

min
j∈{1,2,··· ,M }
j(cid:54)=m

d(m, j)

With the Gaussian assumption, eq. (1) simpliﬁes to L ≤
P (E) ≤ U , where:

L =

P (m)Q



M
(cid:88)

m=1

M
(cid:88)

m=1



(cid:115)

dmin(m)2
4σm

2







(cid:115)

Q







d(m, j)2
4σm

2

M
(cid:88)

j=1
j(cid:54)=m

U =

P (m)

where Q(.) is the tail function for a standard Gaussian
distribution.

σm

L and U can thus be used as metrics for dataset difﬁculty,
higher values for them imply higher probabilities of error,
i.e. lower accuracy. A simpler metric can be obtained by just
considering
dmin(m) . Higher values for this indicate that a)
class m is close to some other class and the NN will have
a hard time differentiating between them, and b) Variance of
class m is high, so it’s harder to form a decision boundary to
separate inputs having labels m from those with other labels.
Since
dmin(m) is different for every class, we experimented
with ways to reduce it to a single measure such as taking the
minimum, the average and the median. The average worked
best, which gives our 3rd metric D:

σm

D =

(cid:80)M

m=1

σm
dmin(m)
M

Therefore high values of D lead to low accuracy.

The 4th and ﬁnal metric is T , to obtain which, we ﬁrst
compute the class centroids just as before. Then we compute
the L1-norm between every pair of centroids and average over
N , i.e:

d1(m, j) =

(cid:107)cm − cj(cid:107)1
N

(2)

(3a)

(3b)

(4)

(5)

Fig. 5. Effects of imposing pre-deﬁned sparsity by reducing the density of
NNs on performance, for different Morse datasets.

parameters reduces the chances of overﬁtting, so is in itself a
form of regularization.

IV. METRICS

This section discusses possible metrics for quantifying how
difﬁcult a dataset is to classify. Each sample in a dataset is
a point in an N -dimensional space, N being the number of
features. For the Morse datasets (not considering dilation),
N = 64. There are M classes of points, which is also 64 in
our case. The classiﬁcation problem is essentially ﬁnding the
class of any new point. Any machine learning classiﬁer will
attempt to learn and construct decision boundaries between the
classes by partitioning the whole space into M regions. The
samples of a particular class m are clustered in the mth region.
Suppose a particular input sample actually belongs to class
m. The classiﬁer commits an error if it ranks some class j,
j (cid:54)= m, higher than m when deciding where that input sample
belongs. The probability of this happening is PP W (j|m),
where subscript P W stands for pairwise and indicates that the
quantity is speciﬁc to classes j and m. The overall probability
of error P (E) would also depend on the prior probability
P (m) of the mth class occurring. Considering all classes in
the dataset, P (E) is given according to [35] as:

M
(cid:88)

m=1

P (m)

 max
j∈{1,2,··· ,M }
j(cid:54)=m



PP W (j|m)

 ≤ P (E)



M
(cid:88)

m=1

M
(cid:88)

j=1
j(cid:54)=m

≤

P (m)

PP W (j|m)

(1)

Since all N features in each input sample are normalized to
[0, 1], all the elements in all the centroid vectors also lie in
the range [0, 1]. So the d1 number for every pair of classes
is always between 0 and 1, in fact, it is proportional to the
absolute distance between the 2 classes. Then we simply count
how many of the d1 numbers are less than a threshold, which
we empirically set to 0.05. This gives T , i.e.:

The pairwise probabilities can be approximately computed
by assuming that the locations of samples of a particular class
m are from a Gaussian distribution with mean located at the
centroid cm, which is the average of all samples for the class.
To simplify the math, we take the average variance across all
N dimensions within a class – this gives the variance σ2
m for
class m. The distance between 2 classes m and j is the L2-
norm between their centroids, i.e. d(m, j) = (cid:107)cm − cj(cid:107)2. A

T =

I (d1(m, j) < 0.05)

(6)

M
(cid:88)

M
(cid:88)

m=1

j=1
j(cid:54)=m

where I is the indicator function, which is 1 if the condition
in its argument is true, otherwise 0. The higher the value of
T , the lower the accuracy. Note that the total number of d1
(cid:1), so the count for T will typically be higher
values will be (cid:0)M

2

be to compute the N × N covariance matrix Km for each
class.

It is worthwhile noting that all these metrics are a function
of the dataset only and are independent of the machine
learning algorithm or training setup used. On the other hand,
percentage accuracy depends on the learning algorithm and
training conditions. As shown in Fig. 4, increasing dataset
size leads to accuracy improvement, i.e. the dataset becoming
easier, since the NN has more training examples to learn
from. However, increasing dataset size drives all the metric
values towards indicating higher difﬁculty. This is because
the occurrence of more examples in each class increases its
standard deviation σm and also makes samples of a particular
class more scattered, leading to reduced values for d and d1.
We hypothesize that these shortcomings of the metrics are due
to the fact that most variations of the Morse datasets have a
low SNR, while the metrics (the error bounds in particular)
are designed for high SNR problems.

V. CONCLUSION
This paper presents an algorithm to generate datasets of
varying difﬁculty on classifying Morse code symbols. While
the results have been shown for neural networks, any machine
learning algorithm can be tried and the challenge arising
from more difﬁcult datasets used to ﬁne tune it. The datasets
are synthetic and consequently may not completely represent
reality unless statistically veriﬁed with real-world tests. How-
ever, the different aspects of the generating algorithm help to
mimic real-world scenarios which can suffer from noise or
other inconsistencies. This work highlights one of the biggest
advantages of synthetic data – the ability to easily produce
large amounts of it and thereby improve the performance
of learning algorithms. The given Morse datasets are also
useful for testing the limits of various learning algorithms and
identifying when they fail or possibly overﬁt/underﬁt.

The metrics discussed, while not perfect, can be used to
understand the inherent difﬁculty of the classiﬁcation problem
on any dataset before applying learning algorithms to it. Future
work will involve improving the metrics to achieve higher
magnitudes of correlation with accuracy, and extension to
other types of neural networks and algorithms.

REFERENCES

[1] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, Nov 1998.

[2] A. Krizhevsky, “Learning multiple layers of features from tiny images,”

Master’s thesis, University of Toronto, 2009.

[3] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp.
211–252, 2015.

[4] G. M. Weiss and F. Provost, “Learning when training data are costly:
The effect of class distribution on tree induction,” Journal of Artiﬁcial
Intelligence Research, vol. 19, pp. 315–354, 2003.

[5] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press,

2016, http://www.deeplearningbook.org.

[6] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” in Proceedings of the International Conference
on Learning Representations (ICLR), 2015.

Fig. 6. Plotting each metric vs. percentage accuracy obtained for datasets
Morse 1.σ (blue), 2.σ (red), 3.σ (green) and 4.σ (black). The accuracy results
are using the fully connected network, as reported in Section III-B. Color
coding is just for clarity, the ρ values in Table I take into account all the
points regardless of color.

TABLE I
CORRELATION COEFFICIENTS BETWEEN METRICS AND ACCURACY

Metric

ρ

L

U

D

T

-0.59

-0.64

-0.63

-0.64

for datasets that have more classes. This is a desired property
since more number of classes usually makes a dataset harder
to classify. Note that the maximum value of T for the Morse
datasets is (cid:0)64

(cid:1) = 2016.

2

A. Goodness of the Metrics

We computed L, U , D and T values for all the Morse
datasets and plotted these with the classiﬁcation accuracy
results obtained from Section III-B. The results are shown in
Fig. 6, while the correlation coefﬁcient ρ of each metric with
the accuracy is given in Table I. Note that the metrics are an
indicator of dataset difﬁculty, so they are negatively correlated
with accuracy. It is apparent that the U and T metrics are the
best since their ρ values have the highest magnitude.

B. Limitations of the Metrics

As mentioned, each class has a single variance value which
is the average variance across dimensions. This is a reasonable
simpliﬁcation to make because our experiments indicate that
the variance of the variance values for different dimensions is
small. However, this simpliﬁcation possibly leads to the error
bounds L and U not being sufﬁciently tight. A possible im-
provement, involving signiﬁcantly more computation, would

[29] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen,
“Compressing neural networks with the hashing trick,” in Proceedings of
the International Conference on Machine Learning (ICML). JMLR.org,
2015, pp. 2285–2294.

[30] X. Zhou, S. Li, K. Qin, K. Li, F. Tang, S. Hu, S. Liu, and Z. Lin, “Deep
adaptive network: An efﬁcient deep neural network with sparse binary
connections,” in arXiv:1604.06154, 2016.

[31] International Morse Code, Radiocommunication Sector of International
Telecommunication Union, Oct 2009, available at http://www.itu.int/rec/
R-REC-M.1677-1-200910-I/.

[32] R. M. Zur, Y. Jiang, L. L. Pesce, and K. Drukker, “Noise injection for
training artiﬁcial neural networks: A comparison with weight decay and
early stopping,” Medical Physics, vol. 36, no. 10, pp. 4810–4818, Oct
2009.

[33] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
tion,” in Proceedings of the 3rd International Conference on Learning
Representations (ICLR), 2015.

[34] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation,” in
Proceedings of the IEEE International Conference on Computer Vision
(ICCV), Dec 2015, pp. 1026–1034.

[35] K. Chugg, A. Anastasopoulos, and X. Chen, Iterative Detection: Adap-
Springer Science &

tivity, Complexity Reduction, and Applications.
Business Media, 2012, vol. 602.

[7] X. Peng, B. Sun, K. Ali, and K. Saenko, “Learning deep object detectors
from 3d models,” in Proceedings of the IEEE International Conference
on Computer Vision (ICCV).
IEEE Computer Society, 2015, pp. 1278–
1286.

[8] D. DeTone, T. Malisiewicz, and A. Rabinovich, “Toward geometric deep

SLAM,” in arXiv:1707.07410, 2017.

[9] T. Anh Le, A. G. Baydin, R. Zinkov, and F. Wood, “Using syn-
thetic data to train neural networks is model-based reasoning,” in
arXiv:1703.00868, 2017.

[10] N. Patki, R. Wedge, and K. Veeramachaneni, “The synthetic data vault,”
in Proceedings of the IEEE International Conference on Data Science
and Advanced Analytics (DSAA), 2016, pp. 399–410.

[11] N. S. Bakde and A. P. Thakare, “Morse code decoder - using a PIC
microcontroller,” International Journal of Science, Engineering and
Technology Research (IJSETR), vol. 1, no. 5, 2012.

[12] C.-H. Yang, L.-Y. Chuang, C.-H. Yang, and C.-H. Luo, “Morse code
application for wireless environmental control systems for severely
disabled individuals,” IEEE Transactions on Neural Systems and Re-
habilitation Engineering, vol. 11, no. 4, pp. 463–469, Dec 2003.
[13] C.-H. Yang, C.-H. Yang, L.-Y. Chuang, and T.-K. Truong, “The ap-
plication of the neural network on morse code recognition for users
with physical impairments,” Proceedings of the Institution of Mechanical
Engineers, Part H: Journal of Engineering in Medicine, vol. 215, no. 3,
pp. 325–331, 2001.

[14] C.-H. Luo and C.-H. Shih, “Adaptive morse-coded single-switch com-
munication system for the disabled,” International Journal of Bio-
Medical Computing, vol. 41, no. 2, pp. 99–106, 1996.

[15] C. P. Ravikumar and M. Dathi, “A fuzzy-logic based morse code entry
system with a touch-pad interface for physically disabled persons,” in
Proceedings of the IEEE Annual India Conference (INDICON), Dec
2016.

[16] T. W. King, Modern Morse Code in Rehabilitation and Education: New
Applications in Assistive Technology, 1st ed. Allyn and Bacon, 1999.
(2017, Aug) Morse code - apps on google play.
[Online]. Available: https://play.google.com/store/apps/details?id=com.
dev.morsecode&hl=en

[17] R. Sheinker.

[18] F. Bonnin. (2018, Mar) Morse-it on the app store. [Online]. Available:

https://itunes.apple.com/us/app/morse-it/id284942940?mt=8

[19] C.-H. Luo and D.-T. Fuh, “Online morse code automatic recognition
with neural network system,” in Proceedings of the 23rd Annual Inter-
national Conference of the IEEE Engineering in Medicine and Biology
Society, vol. 1, 2001, pp. 684–686.

[20] D. Hill, “Temporally processing neural networks for morse code recogni-
tion,” in Theory and Applications of Neural Networks. Springer London,
1992, pp. 180–197.

[21] G. N. Aly and A. M. Sameh, “Evolution of recurrent cascade correlation
networks with distributed collaborative species,” in Proceedings of the
First IEEE Symposium on Combinations of Evolutionary Computation
and Neural Networks, 2000, pp. 240–249.

[22] R. Li, M. Nguyen, and W. Q. Yan, “Morse codes enter using ﬁnger
gesture recognition,” in Proceedings of the International Conference on
Digital Image Computing: Techniques and Applications (DICTA), Nov
2017.

[23] S. Dey. Github repository: souryadey/morse-dataset. [Online]. Available:

https://github.com/souryadey/morse-dataset

[24] S. Dey, Y. Shao, K. M. Chugg, and P. A. Beerel, “Accelerating training of
deep neural networks via sparse edge processing,” in Proceedings of the
26th International Conference on Artiﬁcial Neural Networks (ICANN).
Springer, 2017, pp. 273–280.

[25] S. Dey, P. A. Beerel, and K. M. Chugg, “Interleaver design for deep
neural networks,” in Proceedings of the 51st Asilomar Conference on
Signals, Systems, and Computers, Oct 2017, pp. 1979–1983.

[26] S. Dey, K.-W. Huang, P. A. Beerel, and K. M. Chugg, “Characterizing
sparse connectivity patterns in neural networks,” in Proceedings of the
Information Theory and Applications Workshop, 2018.

[27] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural networks with pruning, trained quantization and huffman
coding,” in Proceedings of the International Conference on Learning
Representations (ICLR), 2016.

[28] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and
connections for efﬁcient neural network,” in Proceedings of the Con-
ference on Neural Information Processing Systems (NIPS), 2015, pp.
1135–1143.

Morse Code Datasets for Machine Learning

Sourya Dey, Keith M. Chugg and Peter A. Beerel
Ming Hsieh Department of Electrical Engineering
University of Southern California
Los Angeles, California 90089, USA
{souryade, chugg, pabeerel}@usc.edu

8
1
0
2
 
c
e
D
 
1
 
 
]

G
L
.
s
c
[
 
 
2
v
9
3
2
4
0
.
7
0
8
1
:
v
i
X
r
a

Abstract—We present an algorithm to generate synthetic
datasets of tunable difﬁculty on classiﬁcation of Morse code
symbols for supervised machine learning problems, in particular,
neural networks. The datasets are spatially one-dimensional and
have a small number of input features, leading to high density
of input information content. This makes them particularly
challenging when implementing network complexity reduction
methods. We explore how network performance is affected by
deliberately adding various forms of noise and expanding the
feature set and dataset size. Finally, we establish several metrics
to indicate the difﬁculty of a dataset, and evaluate their merits.
The algorithm and datasets are open-source.

Index Terms—Machine learning, Artiﬁcial neural networks,

Data science, Information theory, Classiﬁcation

I. INTRODUCTION AND PRIOR WORK

Neural networks in machine learning systems are commonly
employed to tackle classiﬁcation problems involving charac-
ters or images. In such problems, the neural network (NN)
processes an input sample and predicts which class it belongs
to. The inputs and their classes are drawn from a dataset, such
as the MNIST [1] dataset containing images of handwritten
digits, or the CIFAR [2] and ImageNet [3] datasets containing
images of common objects such as birds and houses. A NN is
ﬁrst trained using numerous examples where the input sample
and its class label are both available, then used for inference
(i.e. prediction) of the classes of input samples whose class
labels are not available. The training stage is data-hungry
and typically requires thousands of labeled examples. It is
therefore often a challenge to obtain adequate amounts of high
quality and accurate data required to sufﬁciently train a NN.
A possible solution is to obtain data by synthetic instead of
natural means. Synthetic data are generated using computer al-
gorithms instead of being collected from real-world scenarios.
The advantages are that a) computer algorithms can be tuned
to mimic real-world settings to desired levels of accuracy, and
b) a theoretically unlimited amount of data can be generated
by running the algorithm long enough. The effects of dataset
size on network performance has been explored in [4], in
particular, more inputs are beneﬁcial in reducing overﬁtting
and improving robustness and generalization capabilities of
NNs [5], [6]. Synthetic data has been successfully used in

©2018 IEEE

Original IEEE Publication Citation:
S. Dey, K. M. Chugg and P. A. Beerel, ”Morse Code Datasets for Machine
Learning,” 2018 9th International Conference on Computing, Communication
and Networking Technologies (ICCCNT), 2018, pp. 1-7. doi: 10.1109/ICC-
CNT.2018.8494011

problems such as 3D imaging [7], point tracking [8], breaking
Captchas on popular websites [9], and augmenting real world
datasets [10].

This present work introduces a family of synthetic datasets
on classifying Morse codewords. Morse code is a system of
communication where each letter, number or symbol in a
language is represented using a sequence of dots and dashes,
separated by spaces. It is widely used to communicate in
situations where voice is not possible, such as helping people
with disabilities talk [11]–[14], or where message transmission
needs to be achieved using only 2 states [15], or in rehabilita-
tion and education [16]. Morse code is a useful skill to learn
and there exist cellphone apps designed to train people in its
usage [17], [18].

Our work uses feed-forward multi-layer perceptron neu-
ral networks to directly classify Morse codewords into 64
character classes comprising letters, numbers and symbols.
This is different from previous works such as [13]–[15],
[19] which only had 2 classes corresponding to dots and
dashes. In particular, [15] used fuzzy logic on inputs from
a microcontroller used in security systems, while [14] used
least mean squares approximation, both to classify dots and
dashes. There has also been previous work using time series
and recurrent networks to decode English words in Morse code
[20], [21], while [22] used radial basis function networks to
classify characters with 84% accuracy. Accuracy is a common
metric for describing the performance of a classiﬁcation NN
and is measured as the percentage of class labels correctly
predicted by the NN during inference.

The key contributions of the present work are as follows:
• An algorithm (described in Section II) to generate ma-
chine learning datasets of varying difﬁculty. To the best
of our knowledge, we are the ﬁrst to develop an algo-
rithm which can scale the difﬁculty of machine learning
datasets. The difﬁculty of a dataset can be observed from
the accuracy of a NN training on it – harder datasets lead
to lower accuracy, and vice-versa. We discuss techniques
to make datasets harder and show corresponding accuracy
results in Section III. Encountering harder datasets leads
to aggressive exploration of network hyperparameters
and learning algorithms, which ultimately increases the
robustness of NNs training on them. The algorithm and
datasets are open source and available on Github [23].
• In Section IV, we introduce metrics to quantify the
difﬁculty of a dataset. While some of these arise from

information theory, we also come up with a new metric
which achieves a high correlation coefﬁcient with the
accuracy achieved by NNs on a dataset. Our metrics are
a useful way to characterize how hard a dataset is without
having a NN train on them.

• This work is one of few to introduce a spatially 1-
dimensional dataset. This is in contrast to the wide array
of image and character recognition datasets which are
usually 2-dimensional such as MNIST, where each image
has width and height, or 3-dimensional such as CIFAR
and ImageNet, where each image has width, height and
a number of features. The number of spatial dimensions
in the input data is important when dealing with low-
complexity sparse NNs. Previous works [24]–[30] have
focused on making NNs sparse, while keeping the re-
sulting accuracy reduction to a minimum. The family of
Morse code datasets described in the present work was
designed to test the limits of sparse NNs, as described in
Section III-C.

II. GENERATING ALGORITHM

We picked 64 class labels for our dataset – the 26 English
letters, the 10 Arabic numerals, and 28 other symbols such
as (, +, :, etc. Each of these is represented by a sequence of
dots and dashes in Morse code, for example, + is represented
as • — • — •. So as to mimic a real-world scenario in our
algorithm, we imagined a human or a Morse code machine
writing out this sequence within a frame of ﬁxed size. Wher-
ever the pen or electronic instrument touches is darkened and
has a high intensity, indicating the presence of dots and dashes,
while the other parts are left blank (i.e. spaces).

1) Step 1 – Frame Partitioning: For our algorithm, each
Morse codeword lies in a frame which is a vector of 64 values.
Within the frame, the length of a sequence having consecutive
similar values is used to differentiate between a dot and a dash.
In the baseline dataset, a dot can be 1-3 values wide and a
dash 4-9. This is in accordance with international Morse code
regulations [31] where the size or duration of a dash is around
3 times that of a dot. The space between a dot and a dash can
have a length of 1-3 values. The exact length of a dot, dash
or space is chosen from these ranges according to a uniform
probability distribution. This is to mimic the human writer who
is not expected to make each symbol have a consistent length,
but can be expected to make dots and spaces around the same
size, and dashes longer than them. The baseline dataset has
no leading spaces before the 1st dot or dash, i.e. the codeword
starts from the left edge of the frame. There are trailing spaces
to ﬁll up the right side of the frame after all the dots and dashes
are complete.

2) Step 2 – Assigning Values for Intensity Levels: All values
in the frame are initially real numbers in the range [0, 16]
and indicate the intensity of that point in the frame. For dots
and dashes, the values are drawn from a normal distribution
with mean µ = 12 and standard deviation σ = 4/3. The idea
is to have the six-sigma range from (12 − 3 × 4/3) = 8 to
(12 + 3 × 4/3) = 16. This ensures that any value making up a

Fig. 1. Generating the Morse codeword • — • — • corresponding to the
+ symbol. The ﬁrst 3 steps, prior to normalizing, are shown. Only integer
values are shown for convenience, however, the values can and generally will
be fractional. Normal(µ, σ) denotes a normal distribution with mean = µ,
standard deviation = σ. For this ﬁgure, σ = 1.

dot or a dash will lie in the upper half of possible values, i.e.
in the range [8, 16]. The value of a space is exactly 0. Once
again, these conditions mimic the human or machine writer
who is not expected to have consistent intensity for every dot
and dash, but can be expected to not let the writing instrument
touch portions of the frame which are spaces.

3) Step 3 – Noising: Noise in input samples is often
deliberately injected as a means of avoiding overﬁtting in NNs
[5], and has been shown to be superior to other methods of
avoiding overﬁtting [32]. This was, however, the secondary
reason behind our experimenting with noise. The primary
reason was to deliberately make the data hard to classify
and test the limits of different NNs processing it. Noise can
be thought of as a human accidentally varying the intensity
of writing the Morse codeword, or a Morse communication
channel having noise. The baseline dataset has no noise,
while others have additive noise from a mean-zero normal
distribution applied to them. Fig. 1 shows the 3 steps up to
this point. Finally, all the values are normalized to lie within
the range [0, 1] with precision of 3 decimal places.

4) Step 4 – Mass Generation: Steps 1-3 describe the
generation of 1 input sample corresponding to some particular
class label. This can be repeated as many times as required for
each of the 64 class labels. This demonstrates a key advantage
of synthetic over real-world data – the ability to generate an
arbitrary amount of data having an arbitrary prior probability
distribution over its classes. The baseline dataset has 7,000
examples for each class, for a total of 448,000 examples.

A. Variations and Difﬁculty Scaling

The baseline dataset is as described so far, except that
σ = 0, i.e. it has no additive noise. We experimented with

Fig. 2. Percentage accuracies obtained by the NN described in III-A on the test subset of the datasets described in II-A. The rightmost set of bars corresponds
to Morse 4.σ with L2 regularization.

the following variations in datasets:

1) Baseline with additive noise = Normal(0, σ), σ ∈
{0, 1, 2, 3, 4}. These are called Morse 1.σ, i.e. 1.0 to
1.4, where 1.0 is the baseline.

2) Instead of having the codeword start from the left edge
of the frame, we introduced a random number of leading
spaces. For example, in Fig. 1, the codeword occupies
a length of 26 values. The remaining 38 space values
can be randomly divided between leading and trailing
spaces. This increases the difﬁculty of the dataset since
no particular set of neurons are expected to be learning
dots and dashes as the actual codeword could be any-
where in the frame. Just like variation 1, we added noise
and call these datasets Morse 2.σ, σ ∈ {0, 1, 2, 3, 4}.
3) There is no overlap between the lengths of dots and
dashes in the datasets described so far. The difﬁculty
can be increased by making dash length = 3-9 values,
which is exactly according to the convention of having
dash length thrice of dot length. This means that dashes
can masquerade as dots and spaces, and vice-versa. This
is done on top of introducing leading spaces. These
datasets are called Morse 3.σ, σ being as before.

4) The Morse datasets only have 64 inputs, which is quite
small compared to others such as MNIST (784 inputs),
CIFAR (3072 inputs), or ImageNet (150,528 inputs).
This makes the Morse datasets hard to classify since
there is less redundancy in inputs, so a given amount of
noise will lead to greater reductions in signal-to-noise
ratio (SNR) compared to other datasets. To make the
Morse datasets easier, we introduced dilation by a factor
of 4. This is done by scaling all lengths in variation 3 by
a factor of 4, i.e. frame length is 256, dot sizes and space
sizes are 4-12, and dash size is 12-36. These datasets are
called Morse 4.σ, σ being as before.

5) Increasing the number of training examples, i.e. the size
of the dataset, makes it easier to classify since a NN has
more labeled training examples to learn from. Accord-

ingly we chose Morse 3.1 and scaled the number of ex-
amples to obtain Morse Size x, x ∈ {1/8, 1/4, 1/2, 2, 4, 8}.
For example, Morse Size 1/2 has 3,500 examples for each
class, for a total of 224,000 examples.

III. NEURAL NETWORK RESULTS AND ANALYSIS

A. Network Setup

Our NN needs to have 64 output neurons to match the
number of classes. The number of input neurons always
matches the frame length, i.e. 256 for the Morse 4.σ datasets,
and 64 for all others. We used a single hidden layer with 1024
neurons. The performance, i.e. accuracy, generally increases
on adding more hidden neurons, however, we stuck with
1024 since values above that yielded diminishing returns. The
network is purely multi-layer perceptron, i.e. there are only
fully connected layers. The hidden layer has ReLU activations,
while the output is a softmax probability distribution. We used
the Adam optimizer with default parameters [33], He normal
initialization for the weights [34], and trained for 30 epochs
using a minibatch size of 128. We used 6/7th of the total
examples for training the NN and the remaining 1/7th for
testing at the end of training. All reported accuracies are those
obtained on the test samples.

No constraints were imposed on the weights for the NNs
training on Morse 1.σ, 2.σ and 3.σ, since our experimental
results indicated that this led to optimum performance. How-
ever, the NNs for Morse 4.σ are more prone to overﬁtting
due to having more input neurons, leading to more weight
parameters. Accordingly we regularized the weights using
an L2 coefﬁcient λ = 10−5, which was the best value as
determined experimentally.

B. Results

Note that the entirety of this work – generation of various
datasets, implementing NNs to process them, and evaluation
of metrics – uses the Python programming language. Test
accuracy results after training the NN on the different Morse
datasets are shown in Fig. 2. As expected, increasing the

Fig. 4. Effects of increasing the size of Morse 3.1 by a factor of x on test
accuracy after 30 epochs (blue), and (Training Accuracy - Test Accuracy)
after 30 epochs (orange).

to training on a smaller dataset for more epochs, with the
important added advantage that overﬁtting is reduced. This
is shown in Fig. 4, which shows improving test accuracy as
the dataset is made larger. At the same time, the difference
between ﬁnal training accuracy and test accuracy reduces,
the network is generalizing better and
which implies that
not overﬁtting. Note that Morse Size 8 has 3 million labeled
training examples – a beneﬁcial consequence of being able to
cheaply generate large quantities of synthetic data.

C. Results for Sparse Networks

Our previous work [24]–[26] has focused on network com-
plexity reduction in the form of pre-deﬁned sparsity. In a pre-
deﬁned sparse network, as opposed to a fully connected one, a
fraction of the weights are chosen to be deleted before starting
training. These weights never appear during the workﬂow of
the NN. Consider our (64,1024,64) NN as an example. When
fully connected, it has 64 × 1024 + 1024 × 64 = 131, 072
weights, which gives a fractional density = 1. If we choose to
delete 75% of the weights at the beginning, then we are left
with a NN which has 32,768 weights, i.e. fractional density =
1/4. This leads to reduced storage and operational complexity,
which is particularly important for hardware realizations of
NNs, but possibly at the cost of performance degradation.

Fig. 5 shows the performance degradation for 4 different
Morse datasets. Note how the baseline dataset is reasonably
accurately classiﬁed by a NN with only a quarter of the
weights, while performance drops off much more rapidly when
dataset variations are introduced. These variations lead to
increased information content per neuron per training example.
As a result, the reduction in information learning capability as
a result of deleting weights is much more severe. Also note that
as density is reduced, Morse 4.2 has the best performance out
of the non-baseline models tested in Fig. 5. This is because it
has more weights to begin with, due to the increased number of
input neurons. Finally, note that regularization was not applied
to any of the sparse models since reducing the number of NN

Fig. 3. Effects of noise leading to spaces (orange) getting confused (brown)
with dots and dashes (blue). Higher values of noise σ lead to increased
probability of the brown region, making it harder for the NN to discern
between ‘dots and dashes’ and spaces. The x-axis in each plot shows values
in the range [0, 16], i.e. before normalizing to [0, 1].

standard deviation of noise results in drop in performance.
This effect is not felt strongly when σ = 1 since the 3σ range
can take spaces to a value of 3 (on a scale of [0, 16], i.e.
before normalizing to [0, 1]), while dots and dashes can drop
to 8−3 = 5, so the probability of a space being confused with
a dot or dash is basically 0. Confusion can occur for σ ≥ 2,
and gets worse for higher values, as shown in Fig. 3.

Since the codeword lengths do not often stretch beyond 32,
the ﬁrst half of neurons usually encounter high input intensity
values corresponding to dots and dashes during training. This
means that the latter half of neurons mostly encounter lower
input values corresponding to spaces. This aspect changes
when introducing leading spaces, which become inputs to
some neurons in the ﬁrst half. The result is an increase in the
variance of the input to each neuron. As a result, accuracy
drops. The degradation is worse when dashes can have a
length of 3-9. Since the lengths are drawn from a uniform
distribution, 1/7th of dashes can now be confused with 1/3rd
of dots and 1/3rd of intermediate spaces. As an example, for
the + codeword which has 2 dashes, 3 dots and 4 intermediate
spaces, there is a (2/9 × 1/7 + 3/9 × 1/3 + 4/9 × 1/3) = 29%
chance of this confusion occurring. Dilating by 4, however,
reduces this chance to (2/9 × 1/25 + 3/9 × 1/9 + 4/9 × 1/9) =
9.5%. Accuracy is better as a result, and is further improved
by properly regularizing the NN so that it doesn’t overﬁt.

Increasing dataset size has a beneﬁcial effect on perfor-
mance. Giving the NN more examples to train from is akin

particular class will be more prone to errors if it is close to
other classes. This can be quantiﬁed by looking at dmin(m)
,
where the numerator is given as:

σm

dmin(m) =

min
j∈{1,2,··· ,M }
j(cid:54)=m

d(m, j)

With the Gaussian assumption, eq. (1) simpliﬁes to L ≤
P (E) ≤ U , where:

L =

P (m)Q



M
(cid:88)

m=1

M
(cid:88)

m=1



(cid:115)

dmin(m)2
4σm

2







(cid:115)

Q







d(m, j)2
4σm

2

M
(cid:88)

j=1
j(cid:54)=m

U =

P (m)

where Q(.) is the tail function for a standard Gaussian
distribution.

σm

L and U can thus be used as metrics for dataset difﬁculty,
higher values for them imply higher probabilities of error,
i.e. lower accuracy. A simpler metric can be obtained by just
considering
dmin(m) . Higher values for this indicate that a)
class m is close to some other class and the NN will have
a hard time differentiating between them, and b) Variance of
class m is high, so it’s harder to form a decision boundary to
separate inputs having labels m from those with other labels.
Since
dmin(m) is different for every class, we experimented
with ways to reduce it to a single measure such as taking the
minimum, the average and the median. The average worked
best, which gives our 3rd metric D:

σm

D =

(cid:80)M

m=1

σm
dmin(m)
M

Therefore high values of D lead to low accuracy.

The 4th and ﬁnal metric is T , to obtain which, we ﬁrst
compute the class centroids just as before. Then we compute
the L1-norm between every pair of centroids and average over
N , i.e:

d1(m, j) =

(cid:107)cm − cj(cid:107)1
N

(2)

(3a)

(3b)

(4)

(5)

Fig. 5. Effects of imposing pre-deﬁned sparsity by reducing the density of
NNs on performance, for different Morse datasets.

parameters reduces the chances of overﬁtting, so is in itself a
form of regularization.

IV. METRICS

This section discusses possible metrics for quantifying how
difﬁcult a dataset is to classify. Each sample in a dataset is
a point in an N -dimensional space, N being the number of
features. For the Morse datasets (not considering dilation),
N = 64. There are M classes of points, which is also 64 in
our case. The classiﬁcation problem is essentially ﬁnding the
class of any new point. Any machine learning classiﬁer will
attempt to learn and construct decision boundaries between the
classes by partitioning the whole space into M regions. The
samples of a particular class m are clustered in the mth region.
Suppose a particular input sample actually belongs to class
m. The classiﬁer commits an error if it ranks some class j,
j (cid:54)= m, higher than m when deciding where that input sample
belongs. The probability of this happening is PP W (j|m),
where subscript P W stands for pairwise and indicates that the
quantity is speciﬁc to classes j and m. The overall probability
of error P (E) would also depend on the prior probability
P (m) of the mth class occurring. Considering all classes in
the dataset, P (E) is given according to [35] as:

M
(cid:88)

m=1

P (m)

 max
j∈{1,2,··· ,M }
j(cid:54)=m



PP W (j|m)

 ≤ P (E)



M
(cid:88)

m=1

M
(cid:88)

j=1
j(cid:54)=m

≤

P (m)

PP W (j|m)

(1)

Since all N features in each input sample are normalized to
[0, 1], all the elements in all the centroid vectors also lie in
the range [0, 1]. So the d1 number for every pair of classes
is always between 0 and 1, in fact, it is proportional to the
absolute distance between the 2 classes. Then we simply count
how many of the d1 numbers are less than a threshold, which
we empirically set to 0.05. This gives T , i.e.:

The pairwise probabilities can be approximately computed
by assuming that the locations of samples of a particular class
m are from a Gaussian distribution with mean located at the
centroid cm, which is the average of all samples for the class.
To simplify the math, we take the average variance across all
N dimensions within a class – this gives the variance σ2
m for
class m. The distance between 2 classes m and j is the L2-
norm between their centroids, i.e. d(m, j) = (cid:107)cm − cj(cid:107)2. A

T =

I (d1(m, j) < 0.05)

(6)

M
(cid:88)

M
(cid:88)

m=1

j=1
j(cid:54)=m

where I is the indicator function, which is 1 if the condition
in its argument is true, otherwise 0. The higher the value of
T , the lower the accuracy. Note that the total number of d1
(cid:1), so the count for T will typically be higher
values will be (cid:0)M

2

be to compute the N × N covariance matrix Km for each
class.

It is worthwhile noting that all these metrics are a function
of the dataset only and are independent of the machine
learning algorithm or training setup used. On the other hand,
percentage accuracy depends on the learning algorithm and
training conditions. As shown in Fig. 4, increasing dataset
size leads to accuracy improvement, i.e. the dataset becoming
easier, since the NN has more training examples to learn
from. However, increasing dataset size drives all the metric
values towards indicating higher difﬁculty. This is because
the occurrence of more examples in each class increases its
standard deviation σm and also makes samples of a particular
class more scattered, leading to reduced values for d and d1.
We hypothesize that these shortcomings of the metrics are due
to the fact that most variations of the Morse datasets have a
low SNR, while the metrics (the error bounds in particular)
are designed for high SNR problems.

V. CONCLUSION
This paper presents an algorithm to generate datasets of
varying difﬁculty on classifying Morse code symbols. While
the results have been shown for neural networks, any machine
learning algorithm can be tried and the challenge arising
from more difﬁcult datasets used to ﬁne tune it. The datasets
are synthetic and consequently may not completely represent
reality unless statistically veriﬁed with real-world tests. How-
ever, the different aspects of the generating algorithm help to
mimic real-world scenarios which can suffer from noise or
other inconsistencies. This work highlights one of the biggest
advantages of synthetic data – the ability to easily produce
large amounts of it and thereby improve the performance
of learning algorithms. The given Morse datasets are also
useful for testing the limits of various learning algorithms and
identifying when they fail or possibly overﬁt/underﬁt.

The metrics discussed, while not perfect, can be used to
understand the inherent difﬁculty of the classiﬁcation problem
on any dataset before applying learning algorithms to it. Future
work will involve improving the metrics to achieve higher
magnitudes of correlation with accuracy, and extension to
other types of neural networks and algorithms.

REFERENCES

[1] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, Nov 1998.

[2] A. Krizhevsky, “Learning multiple layers of features from tiny images,”

Master’s thesis, University of Toronto, 2009.

[3] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp.
211–252, 2015.

[4] G. M. Weiss and F. Provost, “Learning when training data are costly:
The effect of class distribution on tree induction,” Journal of Artiﬁcial
Intelligence Research, vol. 19, pp. 315–354, 2003.

[5] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press,

2016, http://www.deeplearningbook.org.

[6] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” in Proceedings of the International Conference
on Learning Representations (ICLR), 2015.

Fig. 6. Plotting each metric vs. percentage accuracy obtained for datasets
Morse 1.σ (blue), 2.σ (red), 3.σ (green) and 4.σ (black). The accuracy results
are using the fully connected network, as reported in Section III-B. Color
coding is just for clarity, the ρ values in Table I take into account all the
points regardless of color.

TABLE I
CORRELATION COEFFICIENTS BETWEEN METRICS AND ACCURACY

Metric

ρ

L

U

D

T

-0.59

-0.64

-0.63

-0.64

for datasets that have more classes. This is a desired property
since more number of classes usually makes a dataset harder
to classify. Note that the maximum value of T for the Morse
datasets is (cid:0)64

(cid:1) = 2016.

2

A. Goodness of the Metrics

We computed L, U , D and T values for all the Morse
datasets and plotted these with the classiﬁcation accuracy
results obtained from Section III-B. The results are shown in
Fig. 6, while the correlation coefﬁcient ρ of each metric with
the accuracy is given in Table I. Note that the metrics are an
indicator of dataset difﬁculty, so they are negatively correlated
with accuracy. It is apparent that the U and T metrics are the
best since their ρ values have the highest magnitude.

B. Limitations of the Metrics

As mentioned, each class has a single variance value which
is the average variance across dimensions. This is a reasonable
simpliﬁcation to make because our experiments indicate that
the variance of the variance values for different dimensions is
small. However, this simpliﬁcation possibly leads to the error
bounds L and U not being sufﬁciently tight. A possible im-
provement, involving signiﬁcantly more computation, would

[29] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen,
“Compressing neural networks with the hashing trick,” in Proceedings of
the International Conference on Machine Learning (ICML). JMLR.org,
2015, pp. 2285–2294.

[30] X. Zhou, S. Li, K. Qin, K. Li, F. Tang, S. Hu, S. Liu, and Z. Lin, “Deep
adaptive network: An efﬁcient deep neural network with sparse binary
connections,” in arXiv:1604.06154, 2016.

[31] International Morse Code, Radiocommunication Sector of International
Telecommunication Union, Oct 2009, available at http://www.itu.int/rec/
R-REC-M.1677-1-200910-I/.

[32] R. M. Zur, Y. Jiang, L. L. Pesce, and K. Drukker, “Noise injection for
training artiﬁcial neural networks: A comparison with weight decay and
early stopping,” Medical Physics, vol. 36, no. 10, pp. 4810–4818, Oct
2009.

[33] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
tion,” in Proceedings of the 3rd International Conference on Learning
Representations (ICLR), 2015.

[34] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation,” in
Proceedings of the IEEE International Conference on Computer Vision
(ICCV), Dec 2015, pp. 1026–1034.

[35] K. Chugg, A. Anastasopoulos, and X. Chen, Iterative Detection: Adap-
Springer Science &

tivity, Complexity Reduction, and Applications.
Business Media, 2012, vol. 602.

[7] X. Peng, B. Sun, K. Ali, and K. Saenko, “Learning deep object detectors
from 3d models,” in Proceedings of the IEEE International Conference
on Computer Vision (ICCV).
IEEE Computer Society, 2015, pp. 1278–
1286.

[8] D. DeTone, T. Malisiewicz, and A. Rabinovich, “Toward geometric deep

SLAM,” in arXiv:1707.07410, 2017.

[9] T. Anh Le, A. G. Baydin, R. Zinkov, and F. Wood, “Using syn-
thetic data to train neural networks is model-based reasoning,” in
arXiv:1703.00868, 2017.

[10] N. Patki, R. Wedge, and K. Veeramachaneni, “The synthetic data vault,”
in Proceedings of the IEEE International Conference on Data Science
and Advanced Analytics (DSAA), 2016, pp. 399–410.

[11] N. S. Bakde and A. P. Thakare, “Morse code decoder - using a PIC
microcontroller,” International Journal of Science, Engineering and
Technology Research (IJSETR), vol. 1, no. 5, 2012.

[12] C.-H. Yang, L.-Y. Chuang, C.-H. Yang, and C.-H. Luo, “Morse code
application for wireless environmental control systems for severely
disabled individuals,” IEEE Transactions on Neural Systems and Re-
habilitation Engineering, vol. 11, no. 4, pp. 463–469, Dec 2003.
[13] C.-H. Yang, C.-H. Yang, L.-Y. Chuang, and T.-K. Truong, “The ap-
plication of the neural network on morse code recognition for users
with physical impairments,” Proceedings of the Institution of Mechanical
Engineers, Part H: Journal of Engineering in Medicine, vol. 215, no. 3,
pp. 325–331, 2001.

[14] C.-H. Luo and C.-H. Shih, “Adaptive morse-coded single-switch com-
munication system for the disabled,” International Journal of Bio-
Medical Computing, vol. 41, no. 2, pp. 99–106, 1996.

[15] C. P. Ravikumar and M. Dathi, “A fuzzy-logic based morse code entry
system with a touch-pad interface for physically disabled persons,” in
Proceedings of the IEEE Annual India Conference (INDICON), Dec
2016.

[16] T. W. King, Modern Morse Code in Rehabilitation and Education: New
Applications in Assistive Technology, 1st ed. Allyn and Bacon, 1999.
(2017, Aug) Morse code - apps on google play.
[Online]. Available: https://play.google.com/store/apps/details?id=com.
dev.morsecode&hl=en

[17] R. Sheinker.

[18] F. Bonnin. (2018, Mar) Morse-it on the app store. [Online]. Available:

https://itunes.apple.com/us/app/morse-it/id284942940?mt=8

[19] C.-H. Luo and D.-T. Fuh, “Online morse code automatic recognition
with neural network system,” in Proceedings of the 23rd Annual Inter-
national Conference of the IEEE Engineering in Medicine and Biology
Society, vol. 1, 2001, pp. 684–686.

[20] D. Hill, “Temporally processing neural networks for morse code recogni-
tion,” in Theory and Applications of Neural Networks. Springer London,
1992, pp. 180–197.

[21] G. N. Aly and A. M. Sameh, “Evolution of recurrent cascade correlation
networks with distributed collaborative species,” in Proceedings of the
First IEEE Symposium on Combinations of Evolutionary Computation
and Neural Networks, 2000, pp. 240–249.

[22] R. Li, M. Nguyen, and W. Q. Yan, “Morse codes enter using ﬁnger
gesture recognition,” in Proceedings of the International Conference on
Digital Image Computing: Techniques and Applications (DICTA), Nov
2017.

[23] S. Dey. Github repository: souryadey/morse-dataset. [Online]. Available:

https://github.com/souryadey/morse-dataset

[24] S. Dey, Y. Shao, K. M. Chugg, and P. A. Beerel, “Accelerating training of
deep neural networks via sparse edge processing,” in Proceedings of the
26th International Conference on Artiﬁcial Neural Networks (ICANN).
Springer, 2017, pp. 273–280.

[25] S. Dey, P. A. Beerel, and K. M. Chugg, “Interleaver design for deep
neural networks,” in Proceedings of the 51st Asilomar Conference on
Signals, Systems, and Computers, Oct 2017, pp. 1979–1983.

[26] S. Dey, K.-W. Huang, P. A. Beerel, and K. M. Chugg, “Characterizing
sparse connectivity patterns in neural networks,” in Proceedings of the
Information Theory and Applications Workshop, 2018.

[27] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural networks with pruning, trained quantization and huffman
coding,” in Proceedings of the International Conference on Learning
Representations (ICLR), 2016.

[28] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and
connections for efﬁcient neural network,” in Proceedings of the Con-
ference on Neural Information Processing Systems (NIPS), 2015, pp.
1135–1143.

