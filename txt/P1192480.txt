5
1
0
2
 
r
p
A
 
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
3
9
5
0
0
.
4
0
5
1
:
v
i
X
r
a

The Approximation of the Dissimilarity
Projection

Emanuele Olivetti, Thien Bao Nguyen and Paolo Avesani

NeuroInformatics Laboratory (NILab), Fondazione Bruno Kessler,
Trento, Italy
Centro Interdipartimentale Mente e Cervello (CIMeC), Universit`a
di Trento, Italy

March 2012

Abstract

Diﬀusion magnetic resonance imaging (dMRI) data allow to recon-
struct the 3D pathways of axons within the white matter of the brain
as a tractography. The analysis of tractographies has drawn attention
from the machine learning and pattern recognition communities provid-
ing novel challenges such as ﬁnding an appropriate representation space
for the data. Many of the current learning algorithms require the input to
be from a vectorial space. This requirement contrasts with the intrinsic
nature of the tractography because its basic elements, called streamlines
or tracks, have diﬀerent lengths and diﬀerent number of points and for this
reason they cannot be directly represented in a common vectorial space.
In this work we propose the adoption of the dissimilarity representation
which is an Euclidean embedding technique deﬁned by selecting a set of
streamlines called prototypes and then mapping any new streamline to
the vector of distances from prototypes. We investigate the degree of ap-
proximation of this projection under diﬀerent prototype selection policies
and prototype set sizes in order to characterise its use on tractography
data. Additionally we propose the use of a scalable approximation of the
most eﬀective prototype selection policy that provides fast and accurate
dissimilarity approximations of complete tractographies.

1

Introduction

Deterministic tractography algorithms [8] can reconstruct white matter ﬁber
tracts as a set of streamlines, also known as tracks, from diﬀusion Magnetic Res-
onance Imaging (dMRI) [2] data. A streamline is a mathematical approximation
of thousands of neuronal axons expressing anatomical connectivity between dif-
ferent areas of the brain, see Figure 1. Recently there has been an increase of

1

Figure 1: A set of 100 streamlines, i.e. an example of prototypes, from a full
tractography

attention in analysing dMRI/tractography data by means of machine learning
and pattern recognition methods, e.g. [14, 13]. These methods often require the
data to lie in a vectorial space, which is not the case for streamlines. Streamlines
are polylines in 3D space and have diﬀerent lengths and numbers of points. The
goal of this work is to investigate the features and limits of a speciﬁc Euclidean
embedding, i.e. the dissimilarity representation, that was recently applied to
the analysis of tractography data [9].

The dissimilarity representation is an Euclidean embedding technique de-
ﬁned by selecting a set of objects (e.g. a set of streamlines) called prototypes,
and then by mapping any new object (e.g. any new streamline) to the vec-
tor of distances from the prototypes. This representation [11, 1, 3] is usually
presented in the context of classiﬁcation and clustering problems. It is a lossy
transformation in the sense that some information is lost when projecting the
data into the dissimilarity space. To the best of our knowledge this loss, i.e. the
degree of approximation, has received little attention in the literature. In [10]
the approximation was studied to decide among competing prototype selection
policies only for classiﬁcation tasks. In this work we are interested in assessing
and controlling this loss without restriction to the classiﬁcation scenario.

This work is motivated by practical applications about executing common
algorithms, like spatial queries, clustering or classiﬁcation, on large collections
of objects that do not have a natural vectorial space representation. The lack
of the vectorial representation avoids the use of some of those algorithms and
of computationally eﬃcient implementations. The dissimilarity space represen-
tation could be the way to provide such a vectorial representation and for this
reason it is crucial to assess the degree of approximation introduced. Besides
this characterisation we propose the use of a stochastic approximation of an op-
timal algorithm for prototype selection that scales well on large datasets. This
scalability issue is of primary importance for tractographies given that a full
brain tractography is a large collection of streamlines, usually ≈ 3 × 105, a size
for which algorithms may become impractical. We provide practical examples
both from simulated data and human brain tractographies.

2

2 Methods

In the following we present a concise formal description of the dissimilarity
projection together with a notion of approximation to quantify how accurate
this representation is. Additionally we introduce three strategies for prototype
selection that will be compared in Section 3.

2.1 The Dissimilarity Projection

Let X be the space of the objects of interest, e.g. streamlines, and let X ∈ X .
Let PX be a probability distribution over X . Let d : X × X (cid:55)→ R+ be a distance
function between objects in X . Note that d is not assumed to be necessarily
metric. Let Π = { ˜X1, . . . , ˜Xp}, where ∀i ˜Xi ∈ X and p is ﬁnite. We call each ˜Xi
as prototype or landmark. The dissimilarity representation/projection is deﬁned
as φd

Π(X) : X (cid:55)→ Rp s.t.

Π(X) = [d(X, ˜X1), . . . , d(X, ˜Xp)]
φd

(1)

and maps an object X from its original space X to a vector of Rp.

Note that this representation is a lossy one in the sense that in general it is
Π(X) because some information is

not possible to exactly reconstruct X from φd
lost during the projection.

We deﬁne the distance between projected objects as the Euclidean distance
Π : X × X (cid:55)→ R+. It
Π(X, X (cid:48)) = ||φd
Π and d should be strongly related. In the following sections

between them: ∆d
is intuitive that ∆d
we will present more details and explanations about this relation.

Π(X (cid:48))||2, i.e. ∆d

Π(X) − φd

2.2 A Measure of Approximation

We investigate the relationship between the distribution of distances among
objects in X through d and the corresponding distances in the dissimilarity
representation space through ∆d
Π. We claim that a good dissimilarity represen-
tation must be able to accurately preserve the partial order of the distances, i.e.
if d(X, X (cid:48)) ≤ d(X, X (cid:48)(cid:48)) then ∆d
Π(X, X (cid:48)(cid:48)) for each X, X (cid:48), X (cid:48)(cid:48) ∈ X
Π(X, X (cid:48)) ≤ ∆d
almost always. As a measure of the degree of approximation of the dissimilarity
representation we deﬁne the Pearson correlation coeﬃcient ρ between the two
distances over all possible pairs of objects in X :

ρ =

Cov(d(X, X (cid:48)), ∆d
σd(X,X (cid:48))σ∆d

Π(X, X (cid:48)))
Π(X,X (cid:48))

(2)

where X, X (cid:48) ∼ PX . In practical cases PX is unknown and only a ﬁnite sample S
is available. We can approximate ρ as the sample correlation r where X, X (cid:48) ∈ S.
An accurate approximation of the relative distances between objects in X results
in values of ρ far from zero and close to 11.

1Note that negative correlation is not considered as accurate approximation. Moreover it

never occurred during experiments

3

In the literature of the Euclidean embeddings of metric spaces, the term of
distortion is used for representing the relation between the distances in the orig-
inal space and the corresponding ones in the projected space. The embedding
is said to have distortion≤ c if for every x, x(cid:48) ∈ X :

d(x, x(cid:48)) ≥ ∆d

Π(x, x(cid:48)) ≥

d(x, x(cid:48)).

(3)

1
c

An interesting embedding of metric spaces is described in [7]. It is based on
ideas similar to the dissimilarity representation and has the advantage of pro-
viding a theoretical bound on the distortion. Unfortunately this embedding is
computationally too expensive to be used in practice.

We claim that correlation and distortion target slightly diﬀerent aspects of
the embedding quality, the ﬁrst focussing on the averaged diﬀerences between
the original and projected space and the second on the worst case scenario.
For this reason we claim that, in the context of machine learning and pattern
recognition applications, correlation is a more appropriate measure.

2.3 Strategies for Prototype Selection

The deﬁnition of the set of prototypes with the goal of minimising the loss of the
dissimilarity projection is an open issue in the dissimilarity space representation
literature. In the context of classiﬁcation problems the policy of random selec-
tion of the prototypes was proved to be useful under certain assumptions [1].
In the following we address the issue of choosing the prototypes in order to
achieve the desired degree of approximation but we do not restrict to the clas-
siﬁcation case only. We deﬁne and discuss the following policies for prototype
selection: random selection, farthest ﬁrst traversal (FFT) and subset farthest
ﬁrst (SFF). All these policies are parametric with respect to p, i.e. the number
of prototypes.

2.3.1 Random Selection

In practical cases we have a sample of objects S = {X1, . . . , XN } ⊂ X . This
selection policy draws uniformly at random from S, i.e. Π ⊆ S and |Π| = p.
Note that sampling is without replacement because identical prototypes provide
redundant, i.e. useless, information. This policy was ﬁrst proposed in [4] for
seeding clustering algorithms. This policy has the lowest computational com-
plexity O(1).

2.3.2 Farthest First Traversal (FFT)

This policy selects an initial prototype at random from S and then each new one
is deﬁned as the farthest element of S from all previously chosen prototypes. The
FFT policy is related to the k-center problem [6]: given a set S and an integer

4

k, what is the smallest (cid:15) for which you can ﬁnd an (cid:15)-cover2 of S of size k? 3.
The k-center problem is known to be an NP-hard [6], i.e. no eﬃcient algorithm
can be devised that always returns the optimal answer. Nevertheless FFT is
known to be close to the optimal solution, in the following sense: If T is the
solution returned by FFT and T ∗ is the optimal solution, then maxx∈S d(x, T ) ≤
2 maxx∈S d(x, T ∗). Moreover, in metric spaces, any algorithm having a better
ratio must be NP-hard [6]. FFT has O(p|S|) complexity. Unfortunately when
|S| becomes very large this prototype selection policy becomes impractical.

2.3.3 Subset Farthest First (SFF)

In the context of radial basis function networks initialisation, a scalable approx-
imation of the FFT algorithm, called subset farthest ﬁrst (SFF), was proposed
in [12]. This approximation is also claimed to reduce the chances to select out-
liers that can lead to a poor representation of large datasets. The SFF policy
samples m = (cid:100)cp log p(cid:101) points from S uniformly at random and then applies
FFT on this sample in order to select the p prototypes. In [12] it was proved
that under the hypothesis of p clusters in S, the probability of not having a
representative of some clusters in the sample is at most pe−m/p. The computa-
tional complexity of SFF is O(p2 log p). Note that for large datasets and small p
this prototype selection policy has a much lower computational cost than FFT.

3 Experiments

In the following we describe the assessment of the degree of approximation of
the dissimilarity representation across diﬀerent prototype selection policies and
diﬀerent numbers of prototypes. The aim is to investigate the trade-oﬀ between
accuracy and computational cost. The experiments are carried out on 2D sim-
ulated data and on real tractographies reconstructed from dMRI recordings of
the human brain.

3.1 Simulated Data
Let X = R2, PX = N (µ, Σ), µ = [0, 0], Σ = I, d(X, X (cid:48)) = ||X −X (cid:48)||2, p = 3 and
˜X1, ˜X2, ˜X3 ∼ PX . Then φd
∈ R3.
Figure 2 shows a sample of 50 points drawn from PX together with the 3 pro-
totypes ˜X1, ˜X2, ˜X3. Figure 3 shows the sample projected into the dissimilarity
space together with the prototypes.

(cid:104)
||X − ˜X1||2, ||X − ˜X2||2, ||X − ˜X3||2

Π(X) =

(cid:105)

The selection of the prototypes according to diﬀerent policies is explained in
Section 2.3. For SFF we chose c = 3 in order to have high probability (> 0.95)
of accurately representing S through the subset. Each dataset was projected

2Given a metric space (X , d), for any (cid:15) > 0, an (cid:15)-cover of a set S ⊂ X is deﬁned to be any
set T ⊂ X such that d(x, T ) ≤ (cid:15), ∀x ∈ S. Here d(x, T ) is the distance from point x to the
closest point in set T .

3Note that in our problem k is called p.

5

Figure 2: A 2-dimensional example of 50 points (black circles) drawn from
N (0, I) and 3 prototypes (red stars) drawn from the same pdf.

Figure 3: The dissimilarity projection of the dataset and prototypes of Figure 2.

6

Figure 4: Average correlation between d and ∆d
selection policies and diﬀerent numbers of prototypes.

Π across diﬀerent prototype

in the dissimilarity space. The correlation ρ between distances in the original
space and the corresponding distances in the projected space was estimated
by computing 50 repetitions of the simulated dataset. The average correlation
and one standard deviation for each prototype selection strategy are shown in
Figure 4.

In this simulated dataset both SFF and FFT performed signiﬁcantly better
than the random selection, on average. FFT showed a small advantage over
SFF when p < 10.

3.2 Tractography data

We estimated the dissimilarity representation over tractography data from dMRI
recordings of the MRI facility at the MRC Cognition and Brain Sciences Unit,
Cambridge UK. The dataset consisted of 12 healthy subjects; 101 (+1, i.e.
b = 0) gradients; b-values from 0 to 4000; voxel size: 2.5 × 2.5 × 2.5mm3. In or-
der to get the tractography we computed the single tensor reconstruction (DTI)
and created the streamlines using EuDX, a deterministic tracking algorithm [5]
from the DiPy library 4. We obtained two tractographies using 104 and 3 × 106
random seed respectively. The ﬁrst tractography consisted of approximately 103
streamlines and the second one of 3 × 105 streamlines. An example of a set of
prototypes from the largest tractography is shown in Figure 1.

As the distance between streamlines we chose one of the most common,
i.e. the symmetric minimum average distance from [14] deﬁned as d(Xa, Xb) =
1
2 (δ(Xa, Xb) + δ(Xb, Xa)) where

δ(Xa, Xb) =

1
|Xa|

(cid:88)

xi∈Xa

min
y∈Xb

||xi − y||2.

(4)

4http://www.dipy.org

7

Figure 5: The correlation between of d and ∆d
raphy for diﬀerent prototype selection policies.

Π over a 103 streamlines tractog-

As it is shown in Figure 5 for the case of a tractography of 103 streamlines
both FFT and SFF (c = 3) had signiﬁcantly higher correlation than the random
sampling for all numbers of prototypes considered. We conﬁrmed that the SFF
selection policy is an accurate approximation of the FFT policy for tractogra-
phies. Moreover we noted that after 15 − 20 prototypes the correlation reaches
approximately 0.95 on average (50 repetitions) and then slightly decreases in-
dicating that a little number of prototypes is suﬃcient to reach a very accurate
dissimilarity representation.

Figure 6 shows the correlation for SFF and the random policy when the
tractography has 3 × 105 streamlines, i.e. the standard size of a tractography
from current dMRI recording techniques. In this case FFT is impractical to be
computed because it requires approximately 15 minutes on a standard desktop
computer for a single repetition when p = 50. The cost of computing SFF
is instead the same of the case of 103 streamlines, as its computational cost
depends only on the number of prototypes. It took ≈ 2 seconds on standard
desktop computer when p = 50 to compute one repetition. We observed that
for 3 × 105 streamlines SFF signiﬁcantly outperformed the random policy and
reached the highest correlation of 0.96 on average (50 repetitions) for 15 − 25
prototypes.

Note that the ﬁgures presented in this section refers to data from subject
1 of the dMRI dataset. We conducted the same experiments on other sub-
jects obtaining equivalent results. The code to reproduce all the experiments is
available at https://github.com/emanuele/prni2012_dissimilarity under
an open source license.

8

Figure 6: The correlation between of d and ∆d
streamlines for the random and SFF prototype selection policies.

Π for a full tractography of 3 × 105

4 Discussion

In this document we investigated the degree of approximation of the dissimilarity
representation for the goal of preserving the relative distances between stream-
lines within tractographies. Empirical assessment has been conducted on two
diﬀerent datasets and through various prototype selection methods. All of the
results from both simulated data and real tractography data reached correlation
≥ 0.95 with respect to the distances in the original space. This fact proved that
the dissimilarity representation works well for preserving the relative distances.
Moreover on tractography data the maximum correlation was reached with just
approximately 20 − 25 prototypes proving that the dissimilarity representation
can produce compact feature spaces for this kind of data.

When comparing the diﬀerent prototype selection policies we found that
FFT had a small advantage over SFF but only when the number of prototypes
was very low (p < 10). Both FFT and SFF always outperformed the random
policy. Moreover, since the computational cost of SFF does not increase with
the size of the dataset but only with the number of prototypes, we observed
that the SFF policy can be easily computed on a standard computer even in
the case of a tractography of 3 × 105 streamlines. This is diﬀerent from FFT
which is several orders of magnitude slower than SFF, thus computationally less
practical.

We advocate the use of the dissimilarity approximation for the Euclidean
embedding of tractography data in machine learning and pattern recognition
applications. Moreover we strongly suggest the use of the SFF policy to obtain
an eﬃcient and eﬀective selection of the prototypes.

9

References

[1] Maria-Florina Balcan, Avrim Blum, and Nathan Srebro. A theory of learn-
ing with similarity functions. Machine Learning, 72(1):89–112, August
2008.

[2] P. J. Basser, J. Mattiello, and D. LeBihan. MR diﬀusion tensor spec-

troscopy and imaging. Biophysical journal, 66(1):259–267, January 1994.

[3] Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi, and Luca Caz-
zanti. Similarity-based Classiﬁcation: Concepts and Algorithms. Journal
of Machine Learning Research, 10:747–776, March 2009.

[4] E. W. Forgy. Cluster analysis of multivariate data: eﬃciency vs inter-

pretability of classiﬁcations. Biometrics, 21:768–769, 1965.

[5] E. Garyfallidis. Towards an accurate brain tractography. PhD thesis, Uni-

versity of Cambridge, 2012.

[6] Dorit S. Hochbaum and David B. Shmoys. A Best Possible Heuristic for
the k-Center Problem. Mathematics of Operations Research, 10(2):180–184,
May 1985.

[7] Nathan Linial, Eran London, and Yuri Rabinovich. The geometry of graphs
and some of its algorithmic applications. Combinatorica, 15(2):215–245,
June 1995.

[8] Susumu Mori and Peter C. M. van Zijl. Fiber tracking: principles and
strategies a technical review. NMR Biomed., 15(7-8):468–480, 2002.

[9] Emanuele Olivetti and Paolo Avesani. Supervised segmentation of ﬁber
tracts. In Proceedings of the First international conference on Similarity-
based pattern recognition, SIMBAD’11, pages 261–274, Berlin, Heidelberg,
2011. Springer-Verlag.

[10] E. Pekalska, R. Duin, and P. Paclik. Prototype selection for dissimilarity-
based classiﬁers. Pattern Recognition, 39(2):189–208, February 2006.

[11] Elzbieta Pekalska, Pavel Paclik, and Robert P. W. Duin. A generalized
kernel approach to dissimilarity-based classiﬁcation. J. Mach. Learn. Res.,
2:175–211, 2002.

[12] D. Turnbull and C. Elkan. Fast recognition of musical genres using
RBF networks. Knowledge and Data Engineering, IEEE Transactions on,
17(4):580–584, April 2005.

[13] Xiaogang Wang, Grimson, and Carl-Fredrik Westin. Tractography segmen-
tation using a hierarchical Dirichlet processes mixture model. NeuroImage,
54(1):290–302, January 2011.

10

[14] Song Zhang, S. Correia, and D. H. Laidlaw.

Identifying White-Matter
Fiber Bundles in DTI Data Using an Automated Proximity-Based Fiber-
Clustering Method. Visualization and Computer Graphics, IEEE Transac-
tions on, 14(5):1044–1053, 2008.

11

5
1
0
2
 
r
p
A
 
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
3
9
5
0
0
.
4
0
5
1
:
v
i
X
r
a

The Approximation of the Dissimilarity
Projection

Emanuele Olivetti, Thien Bao Nguyen and Paolo Avesani

NeuroInformatics Laboratory (NILab), Fondazione Bruno Kessler,
Trento, Italy
Centro Interdipartimentale Mente e Cervello (CIMeC), Universit`a
di Trento, Italy

March 2012

Abstract

Diﬀusion magnetic resonance imaging (dMRI) data allow to recon-
struct the 3D pathways of axons within the white matter of the brain
as a tractography. The analysis of tractographies has drawn attention
from the machine learning and pattern recognition communities provid-
ing novel challenges such as ﬁnding an appropriate representation space
for the data. Many of the current learning algorithms require the input to
be from a vectorial space. This requirement contrasts with the intrinsic
nature of the tractography because its basic elements, called streamlines
or tracks, have diﬀerent lengths and diﬀerent number of points and for this
reason they cannot be directly represented in a common vectorial space.
In this work we propose the adoption of the dissimilarity representation
which is an Euclidean embedding technique deﬁned by selecting a set of
streamlines called prototypes and then mapping any new streamline to
the vector of distances from prototypes. We investigate the degree of ap-
proximation of this projection under diﬀerent prototype selection policies
and prototype set sizes in order to characterise its use on tractography
data. Additionally we propose the use of a scalable approximation of the
most eﬀective prototype selection policy that provides fast and accurate
dissimilarity approximations of complete tractographies.

1

Introduction

Deterministic tractography algorithms [8] can reconstruct white matter ﬁber
tracts as a set of streamlines, also known as tracks, from diﬀusion Magnetic Res-
onance Imaging (dMRI) [2] data. A streamline is a mathematical approximation
of thousands of neuronal axons expressing anatomical connectivity between dif-
ferent areas of the brain, see Figure 1. Recently there has been an increase of

1

Figure 1: A set of 100 streamlines, i.e. an example of prototypes, from a full
tractography

attention in analysing dMRI/tractography data by means of machine learning
and pattern recognition methods, e.g. [14, 13]. These methods often require the
data to lie in a vectorial space, which is not the case for streamlines. Streamlines
are polylines in 3D space and have diﬀerent lengths and numbers of points. The
goal of this work is to investigate the features and limits of a speciﬁc Euclidean
embedding, i.e. the dissimilarity representation, that was recently applied to
the analysis of tractography data [9].

The dissimilarity representation is an Euclidean embedding technique de-
ﬁned by selecting a set of objects (e.g. a set of streamlines) called prototypes,
and then by mapping any new object (e.g. any new streamline) to the vec-
tor of distances from the prototypes. This representation [11, 1, 3] is usually
presented in the context of classiﬁcation and clustering problems. It is a lossy
transformation in the sense that some information is lost when projecting the
data into the dissimilarity space. To the best of our knowledge this loss, i.e. the
degree of approximation, has received little attention in the literature. In [10]
the approximation was studied to decide among competing prototype selection
policies only for classiﬁcation tasks. In this work we are interested in assessing
and controlling this loss without restriction to the classiﬁcation scenario.

This work is motivated by practical applications about executing common
algorithms, like spatial queries, clustering or classiﬁcation, on large collections
of objects that do not have a natural vectorial space representation. The lack
of the vectorial representation avoids the use of some of those algorithms and
of computationally eﬃcient implementations. The dissimilarity space represen-
tation could be the way to provide such a vectorial representation and for this
reason it is crucial to assess the degree of approximation introduced. Besides
this characterisation we propose the use of a stochastic approximation of an op-
timal algorithm for prototype selection that scales well on large datasets. This
scalability issue is of primary importance for tractographies given that a full
brain tractography is a large collection of streamlines, usually ≈ 3 × 105, a size
for which algorithms may become impractical. We provide practical examples
both from simulated data and human brain tractographies.

2

2 Methods

In the following we present a concise formal description of the dissimilarity
projection together with a notion of approximation to quantify how accurate
this representation is. Additionally we introduce three strategies for prototype
selection that will be compared in Section 3.

2.1 The Dissimilarity Projection

Let X be the space of the objects of interest, e.g. streamlines, and let X ∈ X .
Let PX be a probability distribution over X . Let d : X × X (cid:55)→ R+ be a distance
function between objects in X . Note that d is not assumed to be necessarily
metric. Let Π = { ˜X1, . . . , ˜Xp}, where ∀i ˜Xi ∈ X and p is ﬁnite. We call each ˜Xi
as prototype or landmark. The dissimilarity representation/projection is deﬁned
as φd

Π(X) : X (cid:55)→ Rp s.t.

Π(X) = [d(X, ˜X1), . . . , d(X, ˜Xp)]
φd

(1)

and maps an object X from its original space X to a vector of Rp.

Note that this representation is a lossy one in the sense that in general it is
Π(X) because some information is

not possible to exactly reconstruct X from φd
lost during the projection.

We deﬁne the distance between projected objects as the Euclidean distance
Π : X × X (cid:55)→ R+. It
Π(X, X (cid:48)) = ||φd
Π and d should be strongly related. In the following sections

between them: ∆d
is intuitive that ∆d
we will present more details and explanations about this relation.

Π(X (cid:48))||2, i.e. ∆d

Π(X) − φd

2.2 A Measure of Approximation

We investigate the relationship between the distribution of distances among
objects in X through d and the corresponding distances in the dissimilarity
representation space through ∆d
Π. We claim that a good dissimilarity represen-
tation must be able to accurately preserve the partial order of the distances, i.e.
if d(X, X (cid:48)) ≤ d(X, X (cid:48)(cid:48)) then ∆d
Π(X, X (cid:48)(cid:48)) for each X, X (cid:48), X (cid:48)(cid:48) ∈ X
Π(X, X (cid:48)) ≤ ∆d
almost always. As a measure of the degree of approximation of the dissimilarity
representation we deﬁne the Pearson correlation coeﬃcient ρ between the two
distances over all possible pairs of objects in X :

ρ =

Cov(d(X, X (cid:48)), ∆d
σd(X,X (cid:48))σ∆d

Π(X, X (cid:48)))
Π(X,X (cid:48))

(2)

where X, X (cid:48) ∼ PX . In practical cases PX is unknown and only a ﬁnite sample S
is available. We can approximate ρ as the sample correlation r where X, X (cid:48) ∈ S.
An accurate approximation of the relative distances between objects in X results
in values of ρ far from zero and close to 11.

1Note that negative correlation is not considered as accurate approximation. Moreover it

never occurred during experiments

3

In the literature of the Euclidean embeddings of metric spaces, the term of
distortion is used for representing the relation between the distances in the orig-
inal space and the corresponding ones in the projected space. The embedding
is said to have distortion≤ c if for every x, x(cid:48) ∈ X :

d(x, x(cid:48)) ≥ ∆d

Π(x, x(cid:48)) ≥

d(x, x(cid:48)).

(3)

1
c

An interesting embedding of metric spaces is described in [7]. It is based on
ideas similar to the dissimilarity representation and has the advantage of pro-
viding a theoretical bound on the distortion. Unfortunately this embedding is
computationally too expensive to be used in practice.

We claim that correlation and distortion target slightly diﬀerent aspects of
the embedding quality, the ﬁrst focussing on the averaged diﬀerences between
the original and projected space and the second on the worst case scenario.
For this reason we claim that, in the context of machine learning and pattern
recognition applications, correlation is a more appropriate measure.

2.3 Strategies for Prototype Selection

The deﬁnition of the set of prototypes with the goal of minimising the loss of the
dissimilarity projection is an open issue in the dissimilarity space representation
literature. In the context of classiﬁcation problems the policy of random selec-
tion of the prototypes was proved to be useful under certain assumptions [1].
In the following we address the issue of choosing the prototypes in order to
achieve the desired degree of approximation but we do not restrict to the clas-
siﬁcation case only. We deﬁne and discuss the following policies for prototype
selection: random selection, farthest ﬁrst traversal (FFT) and subset farthest
ﬁrst (SFF). All these policies are parametric with respect to p, i.e. the number
of prototypes.

2.3.1 Random Selection

In practical cases we have a sample of objects S = {X1, . . . , XN } ⊂ X . This
selection policy draws uniformly at random from S, i.e. Π ⊆ S and |Π| = p.
Note that sampling is without replacement because identical prototypes provide
redundant, i.e. useless, information. This policy was ﬁrst proposed in [4] for
seeding clustering algorithms. This policy has the lowest computational com-
plexity O(1).

2.3.2 Farthest First Traversal (FFT)

This policy selects an initial prototype at random from S and then each new one
is deﬁned as the farthest element of S from all previously chosen prototypes. The
FFT policy is related to the k-center problem [6]: given a set S and an integer

4

k, what is the smallest (cid:15) for which you can ﬁnd an (cid:15)-cover2 of S of size k? 3.
The k-center problem is known to be an NP-hard [6], i.e. no eﬃcient algorithm
can be devised that always returns the optimal answer. Nevertheless FFT is
known to be close to the optimal solution, in the following sense: If T is the
solution returned by FFT and T ∗ is the optimal solution, then maxx∈S d(x, T ) ≤
2 maxx∈S d(x, T ∗). Moreover, in metric spaces, any algorithm having a better
ratio must be NP-hard [6]. FFT has O(p|S|) complexity. Unfortunately when
|S| becomes very large this prototype selection policy becomes impractical.

2.3.3 Subset Farthest First (SFF)

In the context of radial basis function networks initialisation, a scalable approx-
imation of the FFT algorithm, called subset farthest ﬁrst (SFF), was proposed
in [12]. This approximation is also claimed to reduce the chances to select out-
liers that can lead to a poor representation of large datasets. The SFF policy
samples m = (cid:100)cp log p(cid:101) points from S uniformly at random and then applies
FFT on this sample in order to select the p prototypes. In [12] it was proved
that under the hypothesis of p clusters in S, the probability of not having a
representative of some clusters in the sample is at most pe−m/p. The computa-
tional complexity of SFF is O(p2 log p). Note that for large datasets and small p
this prototype selection policy has a much lower computational cost than FFT.

3 Experiments

In the following we describe the assessment of the degree of approximation of
the dissimilarity representation across diﬀerent prototype selection policies and
diﬀerent numbers of prototypes. The aim is to investigate the trade-oﬀ between
accuracy and computational cost. The experiments are carried out on 2D sim-
ulated data and on real tractographies reconstructed from dMRI recordings of
the human brain.

3.1 Simulated Data
Let X = R2, PX = N (µ, Σ), µ = [0, 0], Σ = I, d(X, X (cid:48)) = ||X −X (cid:48)||2, p = 3 and
˜X1, ˜X2, ˜X3 ∼ PX . Then φd
∈ R3.
Figure 2 shows a sample of 50 points drawn from PX together with the 3 pro-
totypes ˜X1, ˜X2, ˜X3. Figure 3 shows the sample projected into the dissimilarity
space together with the prototypes.

(cid:104)
||X − ˜X1||2, ||X − ˜X2||2, ||X − ˜X3||2

Π(X) =

(cid:105)

The selection of the prototypes according to diﬀerent policies is explained in
Section 2.3. For SFF we chose c = 3 in order to have high probability (> 0.95)
of accurately representing S through the subset. Each dataset was projected

2Given a metric space (X , d), for any (cid:15) > 0, an (cid:15)-cover of a set S ⊂ X is deﬁned to be any
set T ⊂ X such that d(x, T ) ≤ (cid:15), ∀x ∈ S. Here d(x, T ) is the distance from point x to the
closest point in set T .

3Note that in our problem k is called p.

5

Figure 2: A 2-dimensional example of 50 points (black circles) drawn from
N (0, I) and 3 prototypes (red stars) drawn from the same pdf.

Figure 3: The dissimilarity projection of the dataset and prototypes of Figure 2.

6

Figure 4: Average correlation between d and ∆d
selection policies and diﬀerent numbers of prototypes.

Π across diﬀerent prototype

in the dissimilarity space. The correlation ρ between distances in the original
space and the corresponding distances in the projected space was estimated
by computing 50 repetitions of the simulated dataset. The average correlation
and one standard deviation for each prototype selection strategy are shown in
Figure 4.

In this simulated dataset both SFF and FFT performed signiﬁcantly better
than the random selection, on average. FFT showed a small advantage over
SFF when p < 10.

3.2 Tractography data

We estimated the dissimilarity representation over tractography data from dMRI
recordings of the MRI facility at the MRC Cognition and Brain Sciences Unit,
Cambridge UK. The dataset consisted of 12 healthy subjects; 101 (+1, i.e.
b = 0) gradients; b-values from 0 to 4000; voxel size: 2.5 × 2.5 × 2.5mm3. In or-
der to get the tractography we computed the single tensor reconstruction (DTI)
and created the streamlines using EuDX, a deterministic tracking algorithm [5]
from the DiPy library 4. We obtained two tractographies using 104 and 3 × 106
random seed respectively. The ﬁrst tractography consisted of approximately 103
streamlines and the second one of 3 × 105 streamlines. An example of a set of
prototypes from the largest tractography is shown in Figure 1.

As the distance between streamlines we chose one of the most common,
i.e. the symmetric minimum average distance from [14] deﬁned as d(Xa, Xb) =
1
2 (δ(Xa, Xb) + δ(Xb, Xa)) where

δ(Xa, Xb) =

1
|Xa|

(cid:88)

xi∈Xa

min
y∈Xb

||xi − y||2.

(4)

4http://www.dipy.org

7

Figure 5: The correlation between of d and ∆d
raphy for diﬀerent prototype selection policies.

Π over a 103 streamlines tractog-

As it is shown in Figure 5 for the case of a tractography of 103 streamlines
both FFT and SFF (c = 3) had signiﬁcantly higher correlation than the random
sampling for all numbers of prototypes considered. We conﬁrmed that the SFF
selection policy is an accurate approximation of the FFT policy for tractogra-
phies. Moreover we noted that after 15 − 20 prototypes the correlation reaches
approximately 0.95 on average (50 repetitions) and then slightly decreases in-
dicating that a little number of prototypes is suﬃcient to reach a very accurate
dissimilarity representation.

Figure 6 shows the correlation for SFF and the random policy when the
tractography has 3 × 105 streamlines, i.e. the standard size of a tractography
from current dMRI recording techniques. In this case FFT is impractical to be
computed because it requires approximately 15 minutes on a standard desktop
computer for a single repetition when p = 50. The cost of computing SFF
is instead the same of the case of 103 streamlines, as its computational cost
depends only on the number of prototypes. It took ≈ 2 seconds on standard
desktop computer when p = 50 to compute one repetition. We observed that
for 3 × 105 streamlines SFF signiﬁcantly outperformed the random policy and
reached the highest correlation of 0.96 on average (50 repetitions) for 15 − 25
prototypes.

Note that the ﬁgures presented in this section refers to data from subject
1 of the dMRI dataset. We conducted the same experiments on other sub-
jects obtaining equivalent results. The code to reproduce all the experiments is
available at https://github.com/emanuele/prni2012_dissimilarity under
an open source license.

8

Figure 6: The correlation between of d and ∆d
streamlines for the random and SFF prototype selection policies.

Π for a full tractography of 3 × 105

4 Discussion

In this document we investigated the degree of approximation of the dissimilarity
representation for the goal of preserving the relative distances between stream-
lines within tractographies. Empirical assessment has been conducted on two
diﬀerent datasets and through various prototype selection methods. All of the
results from both simulated data and real tractography data reached correlation
≥ 0.95 with respect to the distances in the original space. This fact proved that
the dissimilarity representation works well for preserving the relative distances.
Moreover on tractography data the maximum correlation was reached with just
approximately 20 − 25 prototypes proving that the dissimilarity representation
can produce compact feature spaces for this kind of data.

When comparing the diﬀerent prototype selection policies we found that
FFT had a small advantage over SFF but only when the number of prototypes
was very low (p < 10). Both FFT and SFF always outperformed the random
policy. Moreover, since the computational cost of SFF does not increase with
the size of the dataset but only with the number of prototypes, we observed
that the SFF policy can be easily computed on a standard computer even in
the case of a tractography of 3 × 105 streamlines. This is diﬀerent from FFT
which is several orders of magnitude slower than SFF, thus computationally less
practical.

We advocate the use of the dissimilarity approximation for the Euclidean
embedding of tractography data in machine learning and pattern recognition
applications. Moreover we strongly suggest the use of the SFF policy to obtain
an eﬃcient and eﬀective selection of the prototypes.

9

References

[1] Maria-Florina Balcan, Avrim Blum, and Nathan Srebro. A theory of learn-
ing with similarity functions. Machine Learning, 72(1):89–112, August
2008.

[2] P. J. Basser, J. Mattiello, and D. LeBihan. MR diﬀusion tensor spec-

troscopy and imaging. Biophysical journal, 66(1):259–267, January 1994.

[3] Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi, and Luca Caz-
zanti. Similarity-based Classiﬁcation: Concepts and Algorithms. Journal
of Machine Learning Research, 10:747–776, March 2009.

[4] E. W. Forgy. Cluster analysis of multivariate data: eﬃciency vs inter-

pretability of classiﬁcations. Biometrics, 21:768–769, 1965.

[5] E. Garyfallidis. Towards an accurate brain tractography. PhD thesis, Uni-

versity of Cambridge, 2012.

[6] Dorit S. Hochbaum and David B. Shmoys. A Best Possible Heuristic for
the k-Center Problem. Mathematics of Operations Research, 10(2):180–184,
May 1985.

[7] Nathan Linial, Eran London, and Yuri Rabinovich. The geometry of graphs
and some of its algorithmic applications. Combinatorica, 15(2):215–245,
June 1995.

[8] Susumu Mori and Peter C. M. van Zijl. Fiber tracking: principles and
strategies a technical review. NMR Biomed., 15(7-8):468–480, 2002.

[9] Emanuele Olivetti and Paolo Avesani. Supervised segmentation of ﬁber
tracts. In Proceedings of the First international conference on Similarity-
based pattern recognition, SIMBAD’11, pages 261–274, Berlin, Heidelberg,
2011. Springer-Verlag.

[10] E. Pekalska, R. Duin, and P. Paclik. Prototype selection for dissimilarity-
based classiﬁers. Pattern Recognition, 39(2):189–208, February 2006.

[11] Elzbieta Pekalska, Pavel Paclik, and Robert P. W. Duin. A generalized
kernel approach to dissimilarity-based classiﬁcation. J. Mach. Learn. Res.,
2:175–211, 2002.

[12] D. Turnbull and C. Elkan. Fast recognition of musical genres using
RBF networks. Knowledge and Data Engineering, IEEE Transactions on,
17(4):580–584, April 2005.

[13] Xiaogang Wang, Grimson, and Carl-Fredrik Westin. Tractography segmen-
tation using a hierarchical Dirichlet processes mixture model. NeuroImage,
54(1):290–302, January 2011.

10

[14] Song Zhang, S. Correia, and D. H. Laidlaw.

Identifying White-Matter
Fiber Bundles in DTI Data Using an Automated Proximity-Based Fiber-
Clustering Method. Visualization and Computer Graphics, IEEE Transac-
tions on, 14(5):1044–1053, 2008.

11

5
1
0
2
 
r
p
A
 
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
3
9
5
0
0
.
4
0
5
1
:
v
i
X
r
a

The Approximation of the Dissimilarity
Projection

Emanuele Olivetti, Thien Bao Nguyen and Paolo Avesani

NeuroInformatics Laboratory (NILab), Fondazione Bruno Kessler,
Trento, Italy
Centro Interdipartimentale Mente e Cervello (CIMeC), Universit`a
di Trento, Italy

March 2012

Abstract

Diﬀusion magnetic resonance imaging (dMRI) data allow to recon-
struct the 3D pathways of axons within the white matter of the brain
as a tractography. The analysis of tractographies has drawn attention
from the machine learning and pattern recognition communities provid-
ing novel challenges such as ﬁnding an appropriate representation space
for the data. Many of the current learning algorithms require the input to
be from a vectorial space. This requirement contrasts with the intrinsic
nature of the tractography because its basic elements, called streamlines
or tracks, have diﬀerent lengths and diﬀerent number of points and for this
reason they cannot be directly represented in a common vectorial space.
In this work we propose the adoption of the dissimilarity representation
which is an Euclidean embedding technique deﬁned by selecting a set of
streamlines called prototypes and then mapping any new streamline to
the vector of distances from prototypes. We investigate the degree of ap-
proximation of this projection under diﬀerent prototype selection policies
and prototype set sizes in order to characterise its use on tractography
data. Additionally we propose the use of a scalable approximation of the
most eﬀective prototype selection policy that provides fast and accurate
dissimilarity approximations of complete tractographies.

1

Introduction

Deterministic tractography algorithms [8] can reconstruct white matter ﬁber
tracts as a set of streamlines, also known as tracks, from diﬀusion Magnetic Res-
onance Imaging (dMRI) [2] data. A streamline is a mathematical approximation
of thousands of neuronal axons expressing anatomical connectivity between dif-
ferent areas of the brain, see Figure 1. Recently there has been an increase of

1

Figure 1: A set of 100 streamlines, i.e. an example of prototypes, from a full
tractography

attention in analysing dMRI/tractography data by means of machine learning
and pattern recognition methods, e.g. [14, 13]. These methods often require the
data to lie in a vectorial space, which is not the case for streamlines. Streamlines
are polylines in 3D space and have diﬀerent lengths and numbers of points. The
goal of this work is to investigate the features and limits of a speciﬁc Euclidean
embedding, i.e. the dissimilarity representation, that was recently applied to
the analysis of tractography data [9].

The dissimilarity representation is an Euclidean embedding technique de-
ﬁned by selecting a set of objects (e.g. a set of streamlines) called prototypes,
and then by mapping any new object (e.g. any new streamline) to the vec-
tor of distances from the prototypes. This representation [11, 1, 3] is usually
presented in the context of classiﬁcation and clustering problems. It is a lossy
transformation in the sense that some information is lost when projecting the
data into the dissimilarity space. To the best of our knowledge this loss, i.e. the
degree of approximation, has received little attention in the literature. In [10]
the approximation was studied to decide among competing prototype selection
policies only for classiﬁcation tasks. In this work we are interested in assessing
and controlling this loss without restriction to the classiﬁcation scenario.

This work is motivated by practical applications about executing common
algorithms, like spatial queries, clustering or classiﬁcation, on large collections
of objects that do not have a natural vectorial space representation. The lack
of the vectorial representation avoids the use of some of those algorithms and
of computationally eﬃcient implementations. The dissimilarity space represen-
tation could be the way to provide such a vectorial representation and for this
reason it is crucial to assess the degree of approximation introduced. Besides
this characterisation we propose the use of a stochastic approximation of an op-
timal algorithm for prototype selection that scales well on large datasets. This
scalability issue is of primary importance for tractographies given that a full
brain tractography is a large collection of streamlines, usually ≈ 3 × 105, a size
for which algorithms may become impractical. We provide practical examples
both from simulated data and human brain tractographies.

2

2 Methods

In the following we present a concise formal description of the dissimilarity
projection together with a notion of approximation to quantify how accurate
this representation is. Additionally we introduce three strategies for prototype
selection that will be compared in Section 3.

2.1 The Dissimilarity Projection

Let X be the space of the objects of interest, e.g. streamlines, and let X ∈ X .
Let PX be a probability distribution over X . Let d : X × X (cid:55)→ R+ be a distance
function between objects in X . Note that d is not assumed to be necessarily
metric. Let Π = { ˜X1, . . . , ˜Xp}, where ∀i ˜Xi ∈ X and p is ﬁnite. We call each ˜Xi
as prototype or landmark. The dissimilarity representation/projection is deﬁned
as φd

Π(X) : X (cid:55)→ Rp s.t.

Π(X) = [d(X, ˜X1), . . . , d(X, ˜Xp)]
φd

(1)

and maps an object X from its original space X to a vector of Rp.

Note that this representation is a lossy one in the sense that in general it is
Π(X) because some information is

not possible to exactly reconstruct X from φd
lost during the projection.

We deﬁne the distance between projected objects as the Euclidean distance
Π : X × X (cid:55)→ R+. It
Π(X, X (cid:48)) = ||φd
Π and d should be strongly related. In the following sections

between them: ∆d
is intuitive that ∆d
we will present more details and explanations about this relation.

Π(X (cid:48))||2, i.e. ∆d

Π(X) − φd

2.2 A Measure of Approximation

We investigate the relationship between the distribution of distances among
objects in X through d and the corresponding distances in the dissimilarity
representation space through ∆d
Π. We claim that a good dissimilarity represen-
tation must be able to accurately preserve the partial order of the distances, i.e.
if d(X, X (cid:48)) ≤ d(X, X (cid:48)(cid:48)) then ∆d
Π(X, X (cid:48)(cid:48)) for each X, X (cid:48), X (cid:48)(cid:48) ∈ X
Π(X, X (cid:48)) ≤ ∆d
almost always. As a measure of the degree of approximation of the dissimilarity
representation we deﬁne the Pearson correlation coeﬃcient ρ between the two
distances over all possible pairs of objects in X :

ρ =

Cov(d(X, X (cid:48)), ∆d
σd(X,X (cid:48))σ∆d

Π(X, X (cid:48)))
Π(X,X (cid:48))

(2)

where X, X (cid:48) ∼ PX . In practical cases PX is unknown and only a ﬁnite sample S
is available. We can approximate ρ as the sample correlation r where X, X (cid:48) ∈ S.
An accurate approximation of the relative distances between objects in X results
in values of ρ far from zero and close to 11.

1Note that negative correlation is not considered as accurate approximation. Moreover it

never occurred during experiments

3

In the literature of the Euclidean embeddings of metric spaces, the term of
distortion is used for representing the relation between the distances in the orig-
inal space and the corresponding ones in the projected space. The embedding
is said to have distortion≤ c if for every x, x(cid:48) ∈ X :

d(x, x(cid:48)) ≥ ∆d

Π(x, x(cid:48)) ≥

d(x, x(cid:48)).

(3)

1
c

An interesting embedding of metric spaces is described in [7]. It is based on
ideas similar to the dissimilarity representation and has the advantage of pro-
viding a theoretical bound on the distortion. Unfortunately this embedding is
computationally too expensive to be used in practice.

We claim that correlation and distortion target slightly diﬀerent aspects of
the embedding quality, the ﬁrst focussing on the averaged diﬀerences between
the original and projected space and the second on the worst case scenario.
For this reason we claim that, in the context of machine learning and pattern
recognition applications, correlation is a more appropriate measure.

2.3 Strategies for Prototype Selection

The deﬁnition of the set of prototypes with the goal of minimising the loss of the
dissimilarity projection is an open issue in the dissimilarity space representation
literature. In the context of classiﬁcation problems the policy of random selec-
tion of the prototypes was proved to be useful under certain assumptions [1].
In the following we address the issue of choosing the prototypes in order to
achieve the desired degree of approximation but we do not restrict to the clas-
siﬁcation case only. We deﬁne and discuss the following policies for prototype
selection: random selection, farthest ﬁrst traversal (FFT) and subset farthest
ﬁrst (SFF). All these policies are parametric with respect to p, i.e. the number
of prototypes.

2.3.1 Random Selection

In practical cases we have a sample of objects S = {X1, . . . , XN } ⊂ X . This
selection policy draws uniformly at random from S, i.e. Π ⊆ S and |Π| = p.
Note that sampling is without replacement because identical prototypes provide
redundant, i.e. useless, information. This policy was ﬁrst proposed in [4] for
seeding clustering algorithms. This policy has the lowest computational com-
plexity O(1).

2.3.2 Farthest First Traversal (FFT)

This policy selects an initial prototype at random from S and then each new one
is deﬁned as the farthest element of S from all previously chosen prototypes. The
FFT policy is related to the k-center problem [6]: given a set S and an integer

4

k, what is the smallest (cid:15) for which you can ﬁnd an (cid:15)-cover2 of S of size k? 3.
The k-center problem is known to be an NP-hard [6], i.e. no eﬃcient algorithm
can be devised that always returns the optimal answer. Nevertheless FFT is
known to be close to the optimal solution, in the following sense: If T is the
solution returned by FFT and T ∗ is the optimal solution, then maxx∈S d(x, T ) ≤
2 maxx∈S d(x, T ∗). Moreover, in metric spaces, any algorithm having a better
ratio must be NP-hard [6]. FFT has O(p|S|) complexity. Unfortunately when
|S| becomes very large this prototype selection policy becomes impractical.

2.3.3 Subset Farthest First (SFF)

In the context of radial basis function networks initialisation, a scalable approx-
imation of the FFT algorithm, called subset farthest ﬁrst (SFF), was proposed
in [12]. This approximation is also claimed to reduce the chances to select out-
liers that can lead to a poor representation of large datasets. The SFF policy
samples m = (cid:100)cp log p(cid:101) points from S uniformly at random and then applies
FFT on this sample in order to select the p prototypes. In [12] it was proved
that under the hypothesis of p clusters in S, the probability of not having a
representative of some clusters in the sample is at most pe−m/p. The computa-
tional complexity of SFF is O(p2 log p). Note that for large datasets and small p
this prototype selection policy has a much lower computational cost than FFT.

3 Experiments

In the following we describe the assessment of the degree of approximation of
the dissimilarity representation across diﬀerent prototype selection policies and
diﬀerent numbers of prototypes. The aim is to investigate the trade-oﬀ between
accuracy and computational cost. The experiments are carried out on 2D sim-
ulated data and on real tractographies reconstructed from dMRI recordings of
the human brain.

3.1 Simulated Data
Let X = R2, PX = N (µ, Σ), µ = [0, 0], Σ = I, d(X, X (cid:48)) = ||X −X (cid:48)||2, p = 3 and
˜X1, ˜X2, ˜X3 ∼ PX . Then φd
∈ R3.
Figure 2 shows a sample of 50 points drawn from PX together with the 3 pro-
totypes ˜X1, ˜X2, ˜X3. Figure 3 shows the sample projected into the dissimilarity
space together with the prototypes.

(cid:104)
||X − ˜X1||2, ||X − ˜X2||2, ||X − ˜X3||2

Π(X) =

(cid:105)

The selection of the prototypes according to diﬀerent policies is explained in
Section 2.3. For SFF we chose c = 3 in order to have high probability (> 0.95)
of accurately representing S through the subset. Each dataset was projected

2Given a metric space (X , d), for any (cid:15) > 0, an (cid:15)-cover of a set S ⊂ X is deﬁned to be any
set T ⊂ X such that d(x, T ) ≤ (cid:15), ∀x ∈ S. Here d(x, T ) is the distance from point x to the
closest point in set T .

3Note that in our problem k is called p.

5

Figure 2: A 2-dimensional example of 50 points (black circles) drawn from
N (0, I) and 3 prototypes (red stars) drawn from the same pdf.

Figure 3: The dissimilarity projection of the dataset and prototypes of Figure 2.

6

Figure 4: Average correlation between d and ∆d
selection policies and diﬀerent numbers of prototypes.

Π across diﬀerent prototype

in the dissimilarity space. The correlation ρ between distances in the original
space and the corresponding distances in the projected space was estimated
by computing 50 repetitions of the simulated dataset. The average correlation
and one standard deviation for each prototype selection strategy are shown in
Figure 4.

In this simulated dataset both SFF and FFT performed signiﬁcantly better
than the random selection, on average. FFT showed a small advantage over
SFF when p < 10.

3.2 Tractography data

We estimated the dissimilarity representation over tractography data from dMRI
recordings of the MRI facility at the MRC Cognition and Brain Sciences Unit,
Cambridge UK. The dataset consisted of 12 healthy subjects; 101 (+1, i.e.
b = 0) gradients; b-values from 0 to 4000; voxel size: 2.5 × 2.5 × 2.5mm3. In or-
der to get the tractography we computed the single tensor reconstruction (DTI)
and created the streamlines using EuDX, a deterministic tracking algorithm [5]
from the DiPy library 4. We obtained two tractographies using 104 and 3 × 106
random seed respectively. The ﬁrst tractography consisted of approximately 103
streamlines and the second one of 3 × 105 streamlines. An example of a set of
prototypes from the largest tractography is shown in Figure 1.

As the distance between streamlines we chose one of the most common,
i.e. the symmetric minimum average distance from [14] deﬁned as d(Xa, Xb) =
1
2 (δ(Xa, Xb) + δ(Xb, Xa)) where

δ(Xa, Xb) =

1
|Xa|

(cid:88)

xi∈Xa

min
y∈Xb

||xi − y||2.

(4)

4http://www.dipy.org

7

Figure 5: The correlation between of d and ∆d
raphy for diﬀerent prototype selection policies.

Π over a 103 streamlines tractog-

As it is shown in Figure 5 for the case of a tractography of 103 streamlines
both FFT and SFF (c = 3) had signiﬁcantly higher correlation than the random
sampling for all numbers of prototypes considered. We conﬁrmed that the SFF
selection policy is an accurate approximation of the FFT policy for tractogra-
phies. Moreover we noted that after 15 − 20 prototypes the correlation reaches
approximately 0.95 on average (50 repetitions) and then slightly decreases in-
dicating that a little number of prototypes is suﬃcient to reach a very accurate
dissimilarity representation.

Figure 6 shows the correlation for SFF and the random policy when the
tractography has 3 × 105 streamlines, i.e. the standard size of a tractography
from current dMRI recording techniques. In this case FFT is impractical to be
computed because it requires approximately 15 minutes on a standard desktop
computer for a single repetition when p = 50. The cost of computing SFF
is instead the same of the case of 103 streamlines, as its computational cost
depends only on the number of prototypes. It took ≈ 2 seconds on standard
desktop computer when p = 50 to compute one repetition. We observed that
for 3 × 105 streamlines SFF signiﬁcantly outperformed the random policy and
reached the highest correlation of 0.96 on average (50 repetitions) for 15 − 25
prototypes.

Note that the ﬁgures presented in this section refers to data from subject
1 of the dMRI dataset. We conducted the same experiments on other sub-
jects obtaining equivalent results. The code to reproduce all the experiments is
available at https://github.com/emanuele/prni2012_dissimilarity under
an open source license.

8

Figure 6: The correlation between of d and ∆d
streamlines for the random and SFF prototype selection policies.

Π for a full tractography of 3 × 105

4 Discussion

In this document we investigated the degree of approximation of the dissimilarity
representation for the goal of preserving the relative distances between stream-
lines within tractographies. Empirical assessment has been conducted on two
diﬀerent datasets and through various prototype selection methods. All of the
results from both simulated data and real tractography data reached correlation
≥ 0.95 with respect to the distances in the original space. This fact proved that
the dissimilarity representation works well for preserving the relative distances.
Moreover on tractography data the maximum correlation was reached with just
approximately 20 − 25 prototypes proving that the dissimilarity representation
can produce compact feature spaces for this kind of data.

When comparing the diﬀerent prototype selection policies we found that
FFT had a small advantage over SFF but only when the number of prototypes
was very low (p < 10). Both FFT and SFF always outperformed the random
policy. Moreover, since the computational cost of SFF does not increase with
the size of the dataset but only with the number of prototypes, we observed
that the SFF policy can be easily computed on a standard computer even in
the case of a tractography of 3 × 105 streamlines. This is diﬀerent from FFT
which is several orders of magnitude slower than SFF, thus computationally less
practical.

We advocate the use of the dissimilarity approximation for the Euclidean
embedding of tractography data in machine learning and pattern recognition
applications. Moreover we strongly suggest the use of the SFF policy to obtain
an eﬃcient and eﬀective selection of the prototypes.

9

References

[1] Maria-Florina Balcan, Avrim Blum, and Nathan Srebro. A theory of learn-
ing with similarity functions. Machine Learning, 72(1):89–112, August
2008.

[2] P. J. Basser, J. Mattiello, and D. LeBihan. MR diﬀusion tensor spec-

troscopy and imaging. Biophysical journal, 66(1):259–267, January 1994.

[3] Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi, and Luca Caz-
zanti. Similarity-based Classiﬁcation: Concepts and Algorithms. Journal
of Machine Learning Research, 10:747–776, March 2009.

[4] E. W. Forgy. Cluster analysis of multivariate data: eﬃciency vs inter-

pretability of classiﬁcations. Biometrics, 21:768–769, 1965.

[5] E. Garyfallidis. Towards an accurate brain tractography. PhD thesis, Uni-

versity of Cambridge, 2012.

[6] Dorit S. Hochbaum and David B. Shmoys. A Best Possible Heuristic for
the k-Center Problem. Mathematics of Operations Research, 10(2):180–184,
May 1985.

[7] Nathan Linial, Eran London, and Yuri Rabinovich. The geometry of graphs
and some of its algorithmic applications. Combinatorica, 15(2):215–245,
June 1995.

[8] Susumu Mori and Peter C. M. van Zijl. Fiber tracking: principles and
strategies a technical review. NMR Biomed., 15(7-8):468–480, 2002.

[9] Emanuele Olivetti and Paolo Avesani. Supervised segmentation of ﬁber
tracts. In Proceedings of the First international conference on Similarity-
based pattern recognition, SIMBAD’11, pages 261–274, Berlin, Heidelberg,
2011. Springer-Verlag.

[10] E. Pekalska, R. Duin, and P. Paclik. Prototype selection for dissimilarity-
based classiﬁers. Pattern Recognition, 39(2):189–208, February 2006.

[11] Elzbieta Pekalska, Pavel Paclik, and Robert P. W. Duin. A generalized
kernel approach to dissimilarity-based classiﬁcation. J. Mach. Learn. Res.,
2:175–211, 2002.

[12] D. Turnbull and C. Elkan. Fast recognition of musical genres using
RBF networks. Knowledge and Data Engineering, IEEE Transactions on,
17(4):580–584, April 2005.

[13] Xiaogang Wang, Grimson, and Carl-Fredrik Westin. Tractography segmen-
tation using a hierarchical Dirichlet processes mixture model. NeuroImage,
54(1):290–302, January 2011.

10

[14] Song Zhang, S. Correia, and D. H. Laidlaw.

Identifying White-Matter
Fiber Bundles in DTI Data Using an Automated Proximity-Based Fiber-
Clustering Method. Visualization and Computer Graphics, IEEE Transac-
tions on, 14(5):1044–1053, 2008.

11

