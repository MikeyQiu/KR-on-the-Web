9
1
0
2
 
y
a
M
 
8
2
 
 
]

G
L
.
s
c
[
 
 
1
v
7
1
8
1
1
.
5
0
9
1
:
v
i
X
r
a

Connections Between Mirror Descent, Thompson
Sampling and the Information Ratio

Julian Zimmert
DeepMind, London/
University of Copenhagen
zimmert@di.ku.dk

Tor Lattimore
DeepMind, London
lattimore@google.com

Abstract

The information-theoretic analysis by Russo and Van Roy [25] in combination
with minimax duality has proved a powerful tool for the analysis of online learning
algorithms in full and partial information settings. In most applications there
is a tantalising similarity to the classical analysis based on mirror descent. We
make a formal connection, showing that the information-theoretic bounds in most
applications can be derived from existing techniques for online convex optimisation.
Besides this, for k-armed adversarial bandits we provide an efﬁcient algorithm
with regret that matches the best information-theoretic upper bound and improve
best known regret guarantees for online linear optimisation on (cid:96)p-balls and bandits
with graph feedback.

1

Introduction

The combination of minimax duality and the information-theoretic machinery by Russo and Van Roy
[25] has proved a powerful tool in the analysis of online learning algorithms. This has led to short
and insightful analysis for k-armed bandits, linear bandits, convex bandits and partial monitoring, all
improving on prior best known results. The downside is that the approach is non-constructive. The
application of minimax duality demonstrates the existence of an algorithm with a given bound in the
adversarial setting, but provides no way of constructing that algorithm.

The fundamental quantity in the information-theoretic analysis is the ‘information ratio’ in round t,
which informally is

information ratiot =

(expected regret in round t)2
expected information gain in round t

,

where the information gain is either measured using the mutual information [25] or a generalisation
based on a Bregman divergence [21]. Proving the information ratio is small corresponds to showing
that either the learner is suffering small regret in round t or gaining information, which ultimately
leads to a bound on the cumulative regret. The aforementioned generalisation by Lattimore and
Szepesvári [21] lead to a short analysis for k-armed adversarial bandits that is minimax optimal
except for small constant factors. The authors speculated that the new idea should lead to improved
bounds for a range of online learning problems and suggested a number of applications, including
bandits with graph feedback [3] and linear bandits on (cid:96)p-balls [11].

We started to follow this plan, successfully improving existing minimax bounds for bandits with
graph feedback and online linear optimisation for (cid:96)p-balls with full information (the bandit setting
remains a mystery). Along the way, however, we noticed a striking connection between the analysis
techniques for bounding the information ratio and controlling the stability of online stochastic mirror
descent (OSMD), which is a classical algorithm for online convex optimisation. A connection was
already hypothesised by Lattimore and Szepesvári [21], who noticed a similarity between the bounds

obtained. Notably, why does using the negentropy potential in the information-theoretic analysis lead
to almost identical bounds for k-armed bandits as Exp3? Why does this continue to hold with the
Tsallis entropy and the INF strategy [6]?

Contribution Our main contribution is a formal connection between the information-theoretic
analysis and OSMD. Speciﬁcally, we show how tools for analysing OSMD can be applied to a
modiﬁed version of Thompson sampling that uses the same sampling strategy as OSMD, but replaces
the mirror descent update with a Bayesian update. This contribution is valuable for several reasons:
(a) it explains the similarity between the information-theoretic and OSMD style analysis, (b) it allows
for the transfer of techniques for OSMD to Bayesian regret analysis and (c) it opens the possibility of
a constructive transfer of ideas from Bayesian regret analysis to the adversarial framework, as we
illustrate in the next contribution.

A curiosity in the Bayesian analysis of adversarial k-armed bandits is that the resulting bound was
always a factor of 2 smaller than the corresponding bound for OSMD. This was true in the original
analysis [25] and its generalisation [21]. Our new theorem entirely explains the difference, and indeed,
allows us to improve the bounds for OSMD. This leads to an efﬁcient algorithm for adversarial
k-armed bandits with regret Rn ≤
2kn + O(k), matching the information-theoretic upper bound
except for small lower-order terms.

√

Finally, we improve the regret guarantees for two online learning problems. First, for bandits with
graph feedback we improve the minimax regret in the ‘easy’ setting by a log(n) factor, matching the
lower bound up to a factor of log3/2(k). Second, for online linear optimisation over the (cid:96)p-balls we
improve existing bounds by arbitrarily large constant factors. At ﬁrst we had proved these results
using the information-theoretic tools and minimax duality, but here we present the uniﬁed view and
consequentially the analysis also applies to OSMD for which we have efﬁcient algorithms.

Related work The information-theoretic Bayesian regret analysis was introduced by [24, 25, 26].
The focus in these papers is on the analysis of Bayesian algorithms in the stochastic setting, a line
of work continued recently by [15]. [10] noticed that the stochastic assumption is not required and
that the results continued to hold in a Bayesian adversarial setting where the prior is over arbitrary
sequences of losses, rather than over (parametric) distributions as is usual in Bayesian statistics. The
idea to use minimax duality to derive minimax regret bounds is due to [1] and has been applied and
generalised by a number of authors [10, 17, 21, 9]. Mirror descent was developed by [22] and [23]
for optimization. As far as we know its ﬁrst application to bandits was by [2], which precipitated a
ﬂood of papers as summarised in the books by [8, 20]. We work in the partial monitoring framework,
which goes back to [27]. Most of the focus since then has been on classifying the growth of the regret
on the horizon for ﬁnite partial monitoring games [13, 16, 5, 7, 19]. Bandits with graph feedback are
a special kind of partial monitoring problem and have been studied extensively [3, 14, 4, and others],
with a monograph on the subject by [28]. Online linear optimisation is an enormous subject by itself.
We refer the reader to the books by [12, 18].

Notation The reader will ﬁnd omitted proofs in the supplementary material. Let [n] = {1, 2, . . . , n}
p = {x ∈ Rd : (cid:107)x(cid:107)p ≤ 1} be the standard (cid:96)p-ball. For positive deﬁnite A we write (cid:107)x(cid:107)2
and Bd
A =
x(cid:62)Ax. Given a topological space X, let int(X) be its interior and ∆(X) be the space of probability
measures on X with the Borel σ-algebra. We write X ◦ = {y ∈ Rd : supx∈X |(cid:104)x, y(cid:105)| ≤ 1} for the
functional analysts polar and co(X) for the convex hull of X. The domain of a convex function
F : Rd → R ∪ {∞} is dom(F ) = {x : F (x) < ∞}. For x, y ∈ dom(F ) the Bregman divergence
between x and y with respect to F is DF (x, y) = F (x) − F (y) − ∇Fx−y(y) where ∇vF (x) is
the directional derivative of F at x in the direction v. The diameter of X with respect to F is
diamF (X) = supx,y∈X F (x) − F (y). We abuse notation by writing ∇−2F (x) = (∇2F (x))−1.
For x, y ∈ Rd we let [x, y] = co({x, y}) be the convex hull of x and y, which is the set of points on
the chord between x and y.

Linear partial monitoring Our results are most easily expressed in a linear version of the partial
monitoring framework, which is the same as the standard adversarial linear bandit framework, but
with a different feedback structure. Let A be the action space and L the loss space, which are subsets
of Rd with A compact. The convex hull of A is X = co(A). When A is ﬁnite we let k = |A|. The
signal function is a known function Φ : A × L → Σ for some observation space Σ. An adversary

2

and learner interact over n rounds. First the adversary secretly chooses ((cid:96)t)n
t=1 with (cid:96)t ∈ L for all t.
In each round t the learner samples an action At ∈ A from a distribution depending on observations
A1, Φ1, . . . , At−1, Φt−1 where Φs = Φ(As, (cid:96)s) is the observation in round s. The regret of policy π
in environment ((cid:96)t)n

t=1 is

where the expectation is with respect to the randomness in the actions. The regret depends on a policy
and the losses. The minimax regret is

Rn(π, ((cid:96)t)n

t=1) = max
a∈A

E

(cid:35)

(cid:104)At − a, (cid:96)t(cid:105)

,

(cid:34) n
(cid:88)

t=1

R∗

n = inf
π

sup
((cid:96)t)n

t=1

Rn(π, ((cid:96)t)n

t=1) ,

where the inﬁmum is over all policies and the supremum over all loss sequences in Ln. From here on
the dependence of Rn on the policy and loss sequence is omitted.

Examples The standard k-armed bandit is recovered when A = {e1, . . . , ek}, L = [0, 1]k and
Φ(a, (cid:96)) = (cid:104)a, (cid:96)(cid:105) ∈ Σ = [0, 1]. For linear bandits the set A is an arbitrary compact set and L is
typically A◦. Bandits with graph feedback have a richer signal function as we explain in Section 4.

In the Bayesian setting the sequence of losses ((cid:96)t)n

Bayesian setting
t=1 are sampled from a known
prior probability measure ν on Ln and subsequently the learner interacts with the sampled losses
as normal. The optimal action is now a random variable A∗ = arg mina∈A
t=1(cid:104)a, (cid:96)t(cid:105) and the
Bayesian regret is

(cid:80)n

BRn = E

(cid:34) n
(cid:88)

(cid:35)
(cid:104)At − A∗, (cid:96)t(cid:105)

.

t=1

Finally, deﬁne Pt(·) = P(· | Ft) and Et[·] = E[ · | Ft] with Ft = σ(A1, Φ1, . . . , At, Φt), ∆t =
(cid:104)At − A∗, (cid:96)t(cid:105). A crucial piece of notation is Xt = Et−1[At] ∈ X , which is the conditional expected
action played in round t.

2 Mirror descent, Thompson sampling and the information ratio

We now develop the connection between OSMD and the
information-theoretic Bayesian regret analysis. Speciﬁ-
cally we show that instances of OSMD can be transformed
into an algorithm similar to Thompson sampling (TS) for
which the Bayesian regret can be bounded in the same way
as the regret of the original algorithm. The similarity to
TS is important. Any instance of OSMD with a uniform
bound on the adversarial regret enjoys the same bound on
the Bayesian regret for any prior without modiﬁcation. Our
result has a different ﬂavour because we prove a bound for a variant of OSMD that replaces the mirror
descent update with a Bayesian update.

Algorithm 1: OSMD
Input: A = (P, E, F ) and η
Initialize X1 = arg mina∈X F (a)
for t = 1, . . . , n do

Sample At ∼ PXt and observe Φt
Construct: ˆ(cid:96)t = E(Xt, At, Φt)
Update: Xt+1 = ft(Xt, At)

OSMD is a modular algorithm that depends on deﬁning three components: (1) A sampling scheme
that determines how the algorithm explores, (2) a method for estimating the unobserved loss vectors,
and (3) a convex ‘potential’ and learning rate that determines how the algorithm updates its iterates.
The following deﬁnition makes this more precise.
Deﬁnition 1. An instance of OSMD is determined by a tuple A = (P, F, E) and learning rate η > 0
such that

(a) The sampling scheme is a collection P = {Px : x ∈ X } of probability measures in ∆(A)

such that EA∼Px [A] = x for all x ∈ X .

(b) The potential is a Legendre function F : Rd → R ∪ {∞} with dom(F ) ∩ X (cid:54)= ∅ and η > 0

is the learning rate.

3

(c) The estimation function is E : X × A × Σ → Rd, which we assume satisﬁes

EA∼Px [E(x, A, Φ(A, (cid:96)))] = (cid:96) for all (cid:96) ∈ L and x ∈ X .

The assumptions on the mean of Px and that E is unbiased are often relaxed in minor ways, but for
simplicity we maintain the strict deﬁnition. For the remainder we ﬁx A = (P, F, E) and η > 0 and
abbreviate

Et(x, a) = E(x, a, Φ(a, (cid:96)t))

and

ˆ(cid:96)t = E(Xt, At, Φt) .

You should think of Et(x, a) as the estimated loss vector when the learner plays action a while
sampling from Px and ˆ(cid:96)t as the realisation of this estimate in round t. OSMD starts by initialising X1
as the minimiser of F constrained to X . Subsequently it samples At ∼ PXt and updates

Xt+1 = arg min

η(cid:104)y, ˆ(cid:96)t(cid:105) + DF (y, x) .

y∈X

A useful notation is to let (ft)n

t=1 and (gt)n

ft(x, a) = arg min

y∈X

t=1 be sequences of functions from X × A to Rd with
(η(cid:104)y, Et(x, a)(cid:105) + DF (y, x))

and

gt(x, a) = arg min

(η(cid:104)y, Et(x, a)(cid:105) + DF (y, x)) ,

y∈int(dom(F ))

which means that Xt+1 = ft(Xt, At), while gt is the same as ft, but without the constraint to X .
The complete algorithm is summarised in Algorithm 1. The next theorem is well known [20, §28].
Theorem 2 (OSMD REGRET BOUND). The regret of OSMD satisﬁes

Rn ≤

diamF (X )
η

+

E

η
2

(cid:35)

stabt(Xt; η)

,

(cid:34) n
(cid:88)

t=1

where stabt(x; η) =

2
η

EA∼Px

(cid:20)
(cid:104)x − ft(x, A), Et(x, A)(cid:105) −

DF (ft(x, A), x)
η

(cid:21)

.

The random variable stabt(Xt; η) measures the stability of the algorithm relative to the learning rate
and is usually almost surely bounded. The diameter term depends on how fast the algorithm can
move from the starting point to optimal, which is large when the learning rate is small. In this sense
the learning rate is tuned to balance the stability of the algorithm and the requirement that (Xt) can
tend towards an optimal point. Note that stabt(x) depends on P , E, F , η and the loss vector (cid:96)t,
which means that in the Bayesian setting the stability function is random. The next lemma is also
known and is often useful for bounding the stability function.
Lemma 3. Suppose that F is twice differentiable on int(dom(F )), then

(cid:34)

stabt(x; η) ≤ EA∼Px

sup
z∈[x,ft(x,A)]

(cid:107)Et(x, A)(cid:107)2

∇−2F (z)

Furthermore, provided that gt(x, a) exists for all a in the support of Px, then
(cid:35)

(cid:34)

stabt(x; η) ≤ EA∼Px

sup
z∈[x,gt(x,A)]

(cid:107)Et(x, A)(cid:107)2

∇−2F (z)

(cid:35)

.

.

Bayesian analysis Modiﬁed Thompson sampling (MTS)
is a variant of TS summarised in Algorithm 2 that depends
on a prior distribution ν and a sampling scheme P . The
algorithm differs from Algorithm 1 in the computation
of Xt. Rather than using the mirror descent update, it
uses the Bayesian expected optimal action conditioned on
the observations. Expectations in this subsection are with
respect to both the prior and the actions, which means that
((cid:96)t)n
random. Our main theorem is the following bound on the Bayesian regret of MTS.

Algorithm 2: MTS
Input: Prior ν and P
Initialize X1 = E[A∗]
for t = 1, . . . , n do

t=1 are randomly distributed according to ν and consequently the functions ft, gt and stabt are

Sample At ∼ PXt and observe Φt
Update: Xt+1 = Et−1[A∗]

4

Theorem 4. MTS satisﬁes BRn ≤

diamF (X )
η

+

E

η
2

(cid:35)

stabt(Xt; η)

.

(cid:34) n
(cid:88)

t=1

Remark 5. The stability function depends on A = (P, F, E) and η while Algorithm 2 only uses P .
In this sense Theorem 4 shows that MTS satisﬁes the given bound for all E, F and η. MTS is the
same as TS when sampling from the posterior is the same as sampling from PXt. A fundamental
case where this always holds is when A = {e1, . . . , ed} because each x ∈ X is uniquely represented
as a linear combination of elements in A and hence Px is unique.

Proof of Theorem 4. Beginning with the deﬁnition of the per-step regret,

Et−1 [∆t] = (cid:104)Xt, Et−1[(cid:96)t](cid:105) − Et−1 [(cid:104)A∗, (cid:96)t(cid:105)]

= (cid:104)Xt, Et−1[ˆ(cid:96)t](cid:105) − Et−1

(cid:104)

(cid:105)
(cid:104)A∗, ˆ(cid:96)t(cid:105)

= (cid:104)Xt, Et−1[ˆ(cid:96)t](cid:105) − Et−1

(cid:104)

(cid:105)
(cid:104)Et−1[A∗ | At, Φt], ˆ(cid:96)t(cid:105)
(cid:105)
(cid:104)
(cid:104)Xt − Xt+1, ˆ(cid:96)t(cid:105)
(cid:20)
(cid:104)Xt − ft(Xt, At), ˆ(cid:96)t(cid:105) −

1
η

stabt(Xt; η) +

DF (Xt+1, Xt)

.

(cid:21)

1
η

= Et−1

≤ Et−1

≤ Et−1

(cid:20) η
2

DF (ft(Xt, At), Xt) +

DF (Xt+1, Xt)

(cid:21)

1
η

(1)

(2)

(3)

(4)

(5)

Eq. (1) uses that the loss estimators are unbiased. Eq. (2) follows using the tower rule for conditional
expectations and the fact that ˆ(cid:96)t is a measurable function of Xt, At and Φt so that
Et−1[(cid:104)A∗, ˆ(cid:96)t(cid:105)] = Et−1[Et−1[(cid:104)A∗, ˆ(cid:96)t(cid:105) | At, Φt]] = Et−1[(cid:104)Et−1[A∗ | At, Φt], ˆ(cid:96)t(cid:105)] = Et−1[(cid:104)Xt+1, ˆ(cid:96)t(cid:105)] .
Eq. (3) uses the deﬁnitions of Xt+1. Eq. (4) follows from the deﬁnition of ft, which implies that

(cid:104)ft(Xt, At), ˆ(cid:96)t(cid:105) +

DF (ft(Xt, At), Xt) ≤ (cid:104)Xt+1, ˆ(cid:96)t(cid:105) +

DF (Xt+1, Xt) .

1
η

1
η

Finally, Eq. (5) follows from the deﬁnition of stabt. The proof is completed by summing over the
per-step regret, noting that (Xt)n

t=1 is a (Ft)t-adapted martingale and by [21, Theorem 3],

(cid:34) n
(cid:88)

E

t=1

(cid:35)

DF (Xt+1, Xt)

≤ E[F (Xn+1)] − F (X1) ≤ diamF (X ) .

The stability coefﬁcient The only difference between Theorems 2 and 4 is the trajectory of (Xt)n
t=1
and the randomness of the stability function. In most analyses of OSMD the ﬁnal bound is obtained
via a uniform bound on stabt(x; η) that holds regardless of the losses and in this case the trajectory
Xt is irrelevant. This is formalised in the following deﬁnition and corollary. Deﬁne the stability
coefﬁcients by

stab(A ; η) = sup
x∈X

max
t∈[n]

stabt(x; η)

and

stab(A ) = sup
η>0

stab(A ; η) .

Corollary 6. The regret of Algorithm 1 for an appropriately tuned learning rate is bounded by

Rn ≤ (cid:112)2 diamF (X ) stab(A )n .

The Bayesian regret of Algorithm 2 is bounded by BRn ≤ (cid:112)2 diamF (X ) ess sup(stab(A ))n.
The essential supremum is needed because the stability coefﬁcient depends on the losses ((cid:96)t)n
t=1,
which are random in the Bayesian setting. Generally speaking, however, bounds on the stability
coefﬁcient are proven in a manner that is independent of the losses.
Remark 7. Often stab(A ; η) ≤ a + bη for constants a, b ≥ 0 and stab(A ) = ∞. Nevertheless,
the same argument shows that the regret of Algorithm 1 is bounded by

Rn ≤ (cid:112)2a diamF (X )n +

b diamF (X )
a

,

and similarly for the Bayesian regret of Algorithm 2.

5

Stability and the information ratio The generalised information-theoretic analysis by [21] starts
by assuming there exists a constant α > 0 such that the following bound on the information ratio
holds almost surely:

information ratiot = Et−1[∆t]2(cid:46)

Et−1[DF (Xt+1, Xt)] ≤ α .

(6)

Then [21, Theorem 3] shows that

BRn ≤ (cid:112)αn diamF (X ) .
The proof of Theorem 4 directly provides a bound on the information ratio in terms of the stability
coefﬁcient. To see this, notice that Eq. (4) holds for all measurable η and let
η = (cid:112)2Et−1[DF (Xt+1, Xt)]/ ess sup(stab(A )) .

(8)

(7)

Then by Eq. (4) and the deﬁnition of stab(A ) it follows that

Et−1[∆t]2(cid:46)

Et−1[DF (Xt+1, Xt)] ≤ 2 ess sup(stab(A )) a.s. .

In other words, the usual methods for bounding the stability coefﬁcient in the analysis of OSMD can
be used to bound the information ratio in the information-theoretic analysis.
Example 8. To make the abstraction more concrete, consider the k-armed bandit problem where
L = [0, 1]k and A = {e1, . . . , ek}. In this case there is a unique sampling scheme deﬁned by
Px(a) = (cid:104)x, a(cid:105). The standard loss estimation function is to use importance-weighting, which leads to
Et(x, a)i = (cid:96)ti1(a = ei)(cid:14)xi .

(9)

A commonly used potential is the unnormalised negentropy F (x) = (cid:80)k
i=1 xi log(xi) − xi that
satisﬁes ∇−2F (x) = diag(x). The instance of OSMD resulting from these choices is called Exp3
for which an explicit form for Xt is well known:

Xti = exp

(cid:16)

−η (cid:80)t−1
s=1

ˆ(cid:96)si

(cid:17) (cid:46) (cid:16)(cid:80)k

j=1 exp

−η (cid:80)t−1
s=1

ˆ(cid:96)sj

(cid:16)

(cid:17)(cid:17)

.

A short calculation shows that gt(x, a)i = xi exp(−η ˆ(cid:96)ti) ≤ xi. The stability function is bounded
using the second part of Lemma 3 by

(cid:34)

(cid:35)

stabt(x; η) ≤ EA∼Px

sup
z∈[x,gt(x,A)]

(cid:107)Et(x, A)(cid:107)2

∇−2F (z)

(cid:34)

= EA∼Px

sup
z∈[x,gt(x,A)]

k
(cid:88)

i=1

zti

1(A = ei)(cid:96)2
ti
x2
ti

(cid:35)

= EA∼Px

(cid:20) 1(A = ei)(cid:96)2
xti

ti

(cid:21)

≤

k
(cid:88)

i=1

(cid:96)2
ti ≤ k .

Finally, the diameter of the probability simplex X with respect to the unnormalised negentropy is
diamF (X ) = log(k). Applying Theorem 2 shows that the regret of OSMD and Bayesian regret of
MTS satisfy

Rn ≤ (cid:112)2nk log(k)

(OSMD)

and
Remark 9. Theorems 2 and 4 are vacuous when diamF (X ) = ∞. The most straightforward
resolution is to restrict Xt to a subset of X on which the diameter is bounded and then control the
additive error. This idea also works in the Bayesian setting as described by [21]. We omit a detailed
discussion to avoid technicalities.

(MTS) .

BRn ≤ (cid:112)2nk log(k)

3 Bandits

The best known bound on the minimax regret for k-armed bandits is Rn ≤
F (x) = −2 (cid:80)k

xi be the 1/2-Tsallis entropy and prove that

√

i=1

Et−1[∆t]2(cid:46)

Et−1[DF (Xt+1, Xt)] ≤
√

√

k .

√

2nk
By Cauchy-Schwarz diamF (X ) ≤ 2
for all priors ν. Minimax duality is used to conclude that R∗
2kn. Meanwhile,
using the importance-weighted estimator in Eq. (9) leads to a bound on the stability co-
efﬁcient of stab(A ) ≤ 2
8nk.

k and then Theorem 2 yields a bound of Rn ≤

k and then Eq. (7) shows that BRn ≤

n ≤

√

√

√

√

2kn by [21]. They let

6

INF
INF+shift

800

600

400

200

0

0

The discrepancy between these methods is entirely ex-
plained by the naive choice of importance-weighted
estimator. The approach based on bounding the infor-
mation ratio is effectively shifting the losses, which
can be achieved in the OSMD framework by shifting
the importance-weighted estimators (see Fig. 1). This
idea reduces the worst-case variance of the importance
weighted estimators by a factor of 4.
Lemma 10. If the loss estimator in Example 8 with
F (s) = −2 (cid:80)k

√

i=1

xi is replaced by
((cid:96)ti − cti)1(a = ei)
xi
(1 − 1(Xti < η2)) ,

1
2

+ cti ,

Et(x, a)i =

where cti =

then the stability coefﬁcient for any η ≤ 1/2 is bounded
by stab(A ; η) ≤ k1/2/2 + 12kη.
Theorem 11. The regret of OSMD with the loss estima-
tor of Lemma 10 and appropriate learning rate satisﬁes:
Rn ≤

2kn + 48k.

√

4 Bandits with graph feedback

25000

50000

75000

100000

INF with
Figure 1: Comparison of
η
and without shifted loss estimators.
is tuned to the horizon and all experi-
ments use Bernoulli losses with E[(cid:96)t] =
(0.45, 0.55, 0.55, 0.55, 0.55)T (k = 5). We
repeat the experiment 100 times with error
bars indicating three standard deviations.
The empirical result matches our theoreti-
cal improvement of a factor 2.

In bandits with graph feedback the action set is A = {e1, . . . , ek} and L = [0, 1]k. Let E ⊆ [k] × [k]
be a set of directed edges over vertex set [k] so that G = ([k], E) is a directed graph. The signal
function is Φ(ei, (cid:96)) = {(j, (cid:96)j) : j ∈ N (i)}. The standard bandit framework is recovered when
E = {(i, i) : i ∈ [k]} while the full information setup corresponds to E = [k] × [k]. Of course there
are settings between and beyond these extremes. The difﬁculty of the graph feedback problem is
determined by the connectivity of the graph. For example, when E = ∅, the learner has no way to
estimate the losses and the regret is linear in the worst case. Like ﬁnite partial monitoring, graph
feedback problems can be classiﬁed into one of four regimes for which:
(cid:110)

(cid:111)

R∗

n ∈

O(1), ˜Θ(n1/2), Θ(n2/3), Ω(n)

.

Our focus is on graph feedback problems that ﬁt in the second category, which is the most challenging
to analyse.
Deﬁnition 12. G is called strongly observable if for every vertex i ∈ [k] at least one of the following
holds: (a) a ∈ N (b) for all b (cid:54)= a or (b) a ∈ N (a).

Alon et al. [3] prove the minimax regret for bandits with graph feedback is ˜Θ(n1/2) if and only if
k > 1 and G is strongly observable. They also prove the following theorem upper and lower bounding
the dependence of the minimax regret on the horizon, the number of actions and a graph functional
called the independence number.
Theorem 13 ([3]). Let Gind be the independence number of G, which is the cardinality of the largest
subset of vertices such that no tow distinct vertices are connected by an edge. Suppose k > 1 and G
is strongly observable. Then R∗

Gindn log(kn)) and R∗

Gindn).

√

√

n = O(

n = Ω(

The logarithmic dependence on n in the proof of Theorem 13 appears quite naturally, which raises the
question of whether or not the upper or lower bound is tight. In fact, as n tends to inﬁnity the upper
bound in Theorem 13 could be improved to O(
nk) by using a ﬁnite-armed algorithm that ignores
the feedback except for the played action. Perhaps the independence number is not as fundamental as
ﬁrst thought? The following theorem shows the upper bound can be improved.
Theorem 14. Let A = (P, E, F ) be a triple deﬁning OSMD with Px(a) = (cid:104)a, x(cid:105),

√

F (x) =

1
α(1 − α)

k
(cid:88)

i=1

xα
i

where α = 1 − 1/ log(k) .

7

Finally, deﬁne the unbiased loss estimation function E by

Et(x, a)i =

for i (cid:54)∈ It, and Et(x, a)i =

+ 1 otherwise ,

(cid:96)ti1(a ∈ N (i))
b∈N (i) xb

(cid:80)

((cid:96)ti − 1)1(a (cid:54)= i)
1 − xi

where It = {i ∈ [k] : i (cid:54)∈ N (i) and Xti > 1/2}. Then for any k ≥ 8 and an appropriately tuned
learning rate the regret of OSMD with A satisﬁes Rn = O((cid:112)Gindn log(k)3).

5 Online linear optimisation over (cid:96)p-balls

Regret

p
p = 1 (cid:112)n log(d)
p > 1 (cid:112)n/(1 − p)
√
p ≥ 1

Algorithm

Hedge

p and L = Bd

We now consider full information online linear optimization
on the (cid:96)p balls with p ∈ [1, 2], which is modelled in our
framework by choosing A = Bd
q with 1/p +
Table 1: Known results for (cid:96)p-balls
1/q = 1 and Φ(a, (cid:96)) = (cid:96). Table 1 summarises the known
results. When p = 1 the situation is unambiguous, with matching upper and lower bounds. For
p ∈ (1, 2] there exist algorithms for which the regret is dimension free, but with constants that become
arbitrarily large as p tends to 1. Known results for online gradient descent (OGD) prove the blowup
in terms of p is avoidable, but with a price that is polynomial in the dimension.
Theorem 15. For any p ∈ [1, 2], let h be the following convex and twice continuously differentiable
function:

[12, §11.5]

d2/p−1n

OGD [18]

h(x) =

(cid:40) d

2 x2
p−2
p−1 d

p−1

p−2 |x| + |x|p

p(p−1) + 2−p
2p d

p
p−2

1
p−2

if |x| ≤ d
otherwise .

Then for OSMD using potential F (x) = (cid:80)d
exploration scheme and appropriately tuned learning rate,

i=1 h(xi), loss estimator E(x, a, σ) = σ, an arbitrary

Rn = O

(cid:17)
(cid:16)(cid:112)min {1/(p − 1), log(d)} n

.

Furthermore, the Bayesian regret of TS is bounded by the same quantity.
Remark 16. In the full information setting the loss estimation is independent of the action, which
explains the arbitrariness of the exploration scheme. The intuitive justiﬁcation for the slightly cryptic
potential function is provided in the appendix.

6 Discussion

We demonstrated a connection between the information-theoretic analysis and OSMD. For k-armed
bandits, we explained the factor of two difference between the regret analysis using information-
theoretic and convex-analytic machinery and improved the bound for the latter. For graph bandits
we improved the regret by a factor of log(n). Finally, we designed a new potential for which the
regret for online linear optimisation over the (cid:96)p-balls improves the previously best known bound by
arbitrarily large constant factors.

Open problems The main open problem is whether or not we can ‘close the circle’ and use the
information-theoretic analysis to directly construct OSMD algorithms. Another direction is to try
and relax the assumption that the loss is linear. The leading constant in the new bandit analysis now
matches the best known information-theoretic bound [21]. There is still a constant lower-order term,
which presently seems challenging to eliminate. In bandits with graph feedback one can ask whether
the log(k) dependency can be improved. Lower bounds are still needed for (cid:96)p-balls and extending
the idea to the bandit setting is an obvious followup. Finally, the best known algorithms for ﬁnite
partial monitoring also use the information-theoretic machinery. Understanding how to borrow the
ideas for OSMD remains a challenge.

References

[1] J. Abernethy, A. Agarwal, P. L. Bartlett, and A. Rakhlin. A stochastic view of optimal regret
through minimax duality. In Proceedings of the 22nd Annual Conference on Learning Theory,
2009.

8

[2] J. D. Abernethy, E. Hazan, and A. Rakhlin. Competing in the dark: An efﬁcient algorithm for
bandit linear optimization. In Proceedings of the 21st Annual Conference on Learning Theory,
pages 263–274. Omnipress, 2008.

[3] N. Alon, N. Cesa-Bianchi, O. Dekel, and T. Koren. Online learning with feedback graphs:
Beyond bandits. In Peter Grünwald, Elad Hazan, and Satyen Kale, editors, Proceedings of The
28th Conference on Learning Theory, volume 40 of Proceedings of Machine Learning Research,
pages 23–35, Paris, France, 03–06 Jul 2015. PMLR.

[4] N. Alon, N. Cesa-Bianchi, C. Gentile, S. Mannor, Y. Mansour, and O. Shamir. Nonstochastic
multi-armed bandits with graph-structured feedback. SIAM Journal on Computing, 46(6):
1785–1826, 2017.

[5] A. Antos, G. Bartók, D. Pál, and Cs. Szepesvári. Toward a classiﬁcation of ﬁnite partial-

monitoring games. Theoretical Computer Science, 473:77–99, 2013.

[6] J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In

Proceedings of Conference on Learning Theory (COLT), pages 217–226, 2009.

[7] G. Bartók, D. P. Foster, D. Pál, A. Rakhlin, and Cs. Szepesvári. Partial monitoring—
classiﬁcation, regret bounds, and algorithms. Mathematics of Operations Research, 39(4):
967–997, 2014.

[8] S. Bubeck and N. Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-armed
Bandit Problems. Foundations and Trends in Machine Learning. Now Publishers Incorporated,
2012.

[9] S. Bubeck and M. Sellke. First-order regret analysis of thompson sampling. arXiv preprint

arXiv:1902.00681, 2019.

√

[10] S. Bubeck, O. Dekel, T. Koren, and Y. Peres. Bandit convex optimization:

T regret in one
dimension. In P. Grünwald, E. Hazan, and S. Kale, editors, Proceedings of The 28th Conference
on Learning Theory, volume 40 of Proceedings of Machine Learning Research, pages 266–278,
Paris, France, 03–06 Jul 2015. PMLR.

[11] S. Bubeck, M. Cohen, and Y. Li. Sparsity, variance and curvature in multi-armed bandits. In
F. Janoos, M. Mohri, and K. Sridharan, editors, Proceedings of Algorithmic Learning Theory,
volume 83 of Proceedings of Machine Learning Research, pages 111–127. PMLR, 07–09 Apr
2018.

[12] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge university press,

2006.

[13] N. Cesa-Bianchi, G. Lugosi, and G. Stoltz. Regret minimization under partial monitoring.

Mathematics of Operations Research, 31:562–580, 2006.

[14] A. Cohen, T. Hazan, and T. Koren. Online learning with feedback graphs without the graphs. In

International Conference on Machine Learning, pages 811–819, 2016.

[15] S. Dong and B. Van Roy. An information-theoretic analysis for Thompson sampling with many

actions. arXiv preprint arXiv:1805.11845, 2018.

[16] D. Foster and A. Rakhlin. No internal regret via neighborhood watch. In N. D. Lawrence and
M. Girolami, editors, Proceedings of the 15th International Conference on Artiﬁcial Intelligence
and Statistics, volume 22 of Proceedings of Machine Learning Research, pages 382–390, La
Palma, Canary Islands, 21–23 Apr 2012. PMLR.

[17] N. Gravin, Y. Peres, and B. Sivan. Towards optimal algorithms for prediction with expert advice.
In Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms,
pages 528–547. SIAM, 2016.

[18] E. Hazan. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Optimiza-

tion, 2(3-4):157–325, 2016.

[19] T. Lattimore and Cs. Szepesvári. Cleaning up the neighbourhood: A full classiﬁcation for
adversarial partial monitoring. In International Conference on Algorithmic Learning Theory,
2019.

[20] T. Lattimore and Cs. Szepesvári. Bandit Algorithms. Cambridge University Press (preprint),

2019.

9

[21] T. Lattimore and Cs. Szepesvári. An information-theoretic approach to minimax regret in partial

[22] A. S. Nemirovsky. Efﬁcient methods for large-scale convex optimization problems. Ekonomika

i Matematicheskie Metody, 15, 1979.

[23] A. S. Nemirovsky and D. B. Yudin. Problem Complexity and Method Efﬁciency in Optimization.

monitoring. 2019.

Wiley, 1983.

[24] D. Russo and B. Van Roy. Learning to optimize via information-directed sampling.

In
Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, NIPS, pages 1583–1591. Curran Associates, Inc.,
2014.

[25] D. Russo and B. Van Roy. Learning to optimize via posterior sampling. Mathematics of

Operations Research, 39(4):1221–1243, 2014.

[26] D. Russo and B. Van Roy. An information-theoretic analysis of Thompson sampling. Journal

of Machine Learning Research, 17(1):2442–2471, 2016. ISSN 1532-4435.

[27] A. Rustichini. Minimizing regret: The general case. Games and Economic Behavior, 29(1):

224–243, 1999.

[28] M. Valko. Bandits on graphs and structures, 2016.

A Proof of Lemma 3

The proof is rather standard. In fact, the ﬁrst part is [20, Theorem 26.13]. For the second part, ﬁx
x ∈ X and a ∈ A and deﬁne

Ψ(y) = η(cid:104)y, Et(x, a)(cid:105) + DF (y, x) .

By the assumption that gt(x, a) ∈ int(dom(F )) = int(dom(Ψ)) and the deﬁnition of gt(x, a) as
the minimizer of Ψ it follows that

0 = ∇Ψ(gt(x, a)) = ηEt(x, a) + ∇F (gt(x, a)) − ∇F (x) .

Hence

stabt(x) =

EA∼Px

(cid:20)
(cid:104)x − ft(x, A), Et(x, A)(cid:105) −

DF (ft(x, A), x)
η

(cid:21)

2
η
2
η
2
η
2
η

=

=

≤

EA∼Px

EA∼Px

EA∼Px

(cid:20) 1
η
(cid:20) 1
η
(cid:20) DF (x, gt(x, A))
η

DF (x, gt(x, A)) −

(cid:21)

.

(cid:104)x − ft(x, A), ∇F (x) − ∇F (gt(x, a))(cid:105) −

DF (ft(x, A), x)
η

(cid:21)

(cid:21)
DF (ft(x, a), gt(x, A))

1
η

(10)

Let F ∗ be the Legendre dual of F . Since F is Legendre and twice differentiable on int(dom(F ))
it follows from Taylor’s theorem and duality that there exists a z∗ ∈ [∇F (x), ∇F (x) − ηEt(x, a)]
such that

DF (x, gt(x, a)) = DF ∗ (∇F (gt(x, a)), ∇F (x))

= DF ∗ (∇F (x) − ηEt(x, a), ∇F (x))

=

=

η2
2
η2
2

(cid:107)Et(x, a)(cid:107)2

∇2F ∗(z∗)

(cid:107)Et(x, a)(cid:107)2

∇−2F (∇F ∗(z∗))

≤

sup
z∈[x,gt(x,a)]

η2
2

(cid:107)Et(x, a)(cid:107)2

∇−2F (z) .

Substituting into Eq. (10) completes the result.

10

Reﬁned bound for the probability simplex For the proofs in the next sections, we require a
reﬁned version of Lemma 3. Let 1k denote the vector with all ones.
Lemma 17. Assume that A = {e1, . . . , ek} and for c ∈ R deﬁne

ftc(x, a) = arg min

(η(cid:104)y, Et(x, a) + c1k(cid:105) + DF (y, x)) ,

gtc(x, a) = arg min

(η(cid:104)y, Et(x, a) + c1k(cid:105) + DF (y, x)) .

y∈X

y∈int(dom(F ))

Provided that gtc(x, a) exists for all a in the support of Px,

stabt(x; η) ≤

EA∼Px [DF (x, gtc(x, A))] ≤ EA∼Px

(cid:107)Et(x, A) + c1k(cid:107)2

∇−2F (z)

2
η2

(cid:34)

sup
z∈[x,gtc(x,A)]

(cid:35)

.

Proof. Since X is the probability simplex (cid:104)y, c1k(cid:105) = c for all y ∈ X . Therefore ftc(x, a) = ft(x, a)
and (cid:104)x − ft(x, a), c1k(cid:105) = 0. Hence

stabt(x) =

EA∼Px

(cid:20)
(cid:104)x − ft(x, A), Et(x, A)(cid:105) −

=

EA∼Px

(cid:20)
(cid:104)x − ftc(x, A), Et(x, A) + c1k(cid:105) −

(cid:21)

DF (ft(x, A), x)
η
DF (ftc(x, A), x)
η

(cid:21)

.

2
η
2
η

The remaining proof is analogous to the proof of Lemma 3 substituting ft, gt by ftc, gtc and the loss
Et(x, a) by Et(x, a) + c1k.

B Proof of Corollary 6

Starting with the adversarial regret bound. By Theorem 2,

Rn ≤

diamF (X )
η

+

E

η
2

(cid:35)

stabt(Xt)

≤

(cid:34) n
(cid:88)

t=1

diamF (X )
η

+

ηn stab(A )
2

.

The ﬁrst part follows by choosing

(cid:115)

η =

2 diamF (X )
n stab(A )

.

The Bayesian case follows from an identical argument and Theorem 4 and the fact that
(cid:34) n
(cid:88)

(cid:34) n
(cid:88)

(cid:35)

(cid:35)

stab(A )

≤ n ess sup(stab(A )) .

≤ E

stabt(Xt)

E

t=1

t=1

The result claimed in Remark 7 follows similarly with the same choice of learning rate.

C Proof of Theorem 11

Proof of Lemma 10. We use Lemma 17 with c = − 1

2 . As a reminder, we have

Et(x, a)i + c =

((cid:96)ti − cti)1(a = ei)
xi

+ cti + c , where cti =

(1 − 1(Xti < η2).

1
2

Let ˜(cid:96)t = Et(Xt, At) + c1k. We start by calculating the Hessian of F . Since F (a) = − (cid:80)k

√

i=1 2

ai,

∇F (a) = −1/

a

and

∇2F (a) = diag(a−3/2/2) .

The next step is to bound gtc(Xt, At)

i . By deﬁnition

gtc(Xt, At) = arg min

η(cid:104)y, ˜(cid:96)t(cid:105) + F (y) − F (Xt) − (cid:104)y − Xt, ∇F (Xt)(cid:105) ,

y∈int(dom(F ))

√

3
2

11

which implies that η ˜(cid:96)t + ∇F (gtc(Xt, At)) − ∇F (Xt) = 0. Substituting the gradient of the potential
shows that

η ˜(cid:96)ti −

1
(cid:112)gtc(Xt, At)i

+

√

= 0 .

1
Xti

Solving for gtc(Xt, At)i yields

gtc(Xt, At)

3
2

i =

3
2
ti

X
(1 + ˜(cid:96)tηX

.

1
2

ti )3

(11)

For ˜(cid:96)ti ≥ 0, Eq. (11) directly implies gtc(Xt, At)
lower bound by deﬁnition of ˜(cid:96)t:

3
2

3
2

i ≤ X

ti . Let ˜(cid:96)ti < 0, then we get the following

Xti ≥ η2 : ˜(cid:96)ti = −

Xti < η2 : ˜(cid:96)ti =

((cid:96)ti − 1)1(At = ei)
2Xti
(cid:96)ti1(At = ei)
Xti

1
2

−

≥ −

≥ −

≥ −

1
2Xti

1
2ηX 1/2
ti

,

1
2ηX 1/2
ti

≥ −

1
2Xti

.

This directly implies −˜(cid:96)tiηX 1/2
2 . Going back to Eq. (11), the
following bound on f (x) = x−3 holds due to convexity for all x > −1: f (1+x) ≤ f (1)+xf (cid:48)(1+x).
Using all three inequalities provides the bound

and 1 + ˜ηX 1/2

2 ηX −1/2

ti ≥ 1

ti ≤ 1

ti

3
2

X

ti (1 + ˜(cid:96)tiηX

1
2

3
ti )−3 ≤ X
2
ti

(cid:16)

1 − 3(1 + ˜(cid:96)tiηX

1
2

ti )−4 ˜(cid:96)tiηX

1
2
ti

(cid:17)

3
2

≤ X

ti + 24ηXti .

Hence for any z ∈ [Xt, gtc(Xt, At)] we have

∇−2F (z) (cid:22) diag(2X

3
2

t + 48ηXt ◦ 1(˜(cid:96)t < 0)) ,

where 1(˜(cid:96)t > 0) is vector of element wise applied indicator function. Finally we are ready to bound
the stability:

(cid:107)Et(Xt, A) + c1k(cid:107)2

∇−2F (z)

(cid:35)

sup
z∈[Xt,gtc(Xt,A)]
2 )2

((cid:96)ti − 1
X 2
ti

Xti

(cid:34)

EA∼PXt

i:Xti≥η2

(cid:88)

(cid:88)

≤

≤

1
X
2
ti
2

+ 12η +

(cid:88)

25η3
2

i:Xti≥η2

i:Xti<η2

i:Xti<η2
√

k
2

3
2

(2X

ti + 48ηXti) +

(cid:88)

1
22 (2X

3
2

ti + 48ηXti) + Xti

(cid:96)2
ti
X 2
ti

2X

3
2
ti

(12)

+ 2η ≤

+ 12ηk .

(13)

Eq. (12) follows because for Xti ≥ η2 the term Et(Xt, A)i + c is non zero with probability Xti,
while for Xti < η2, Et(Xt, A)i + c is either non positive and bounded by − 1
2 , or it is positive with
probability lower or equal to Xti. Eq. (13) uses the condition Xti ≤ η in the second sum and the
upper bound η ≤ 1/2.

Proof of Theorem 11. Combine Lemma 10 with Theorem 2, Corollary 6, and Remark 7.

D Proof of Theorem 14

We make use of the following lemma.
Lemma 18 (Alon et al. 3). Let p ∈ ∆([k]). Then

k
(cid:88)

i=1

pi
j∈N (i) pj

(cid:80)

≤ 4Gind log

(cid:18)

4k
Gind mini pi

(cid:19)

.

12

Proof of Theorem 14. Starting from Corollary 6 we need to bound the diameter and stability.

diamF (X ) ≤

k1−α
α(1 − α)

=

1

k

log(k) log(k)
1 − 1

log(k)

=

e log(k)
1 − 1

log(k)

≤ 2e log(k) ,

where in the last inequality we used the assumption that k ≥ 8 > e2. Moving to the stability term.
As a reminder we have

Et(Xt, At)i =

for i ∈ It and Et(Xt, At)i =

+ 1 otherwise

((cid:96)ti − 1)1(At (cid:54)= i)
1 − Xti

(cid:96)ti1(At ∈ N (i))
b∈N (i) Xtb

(cid:80)

where It = {i ∈ [k] : i (cid:54)∈ N (i) and Xti > 1/2}. The set It is either empty or contains exactly one
element, since the action set it the probability simplex. As a slight abuse of notation, It denotes either
the (possible empty) set or the unique element within. We use Lemma 17 with

c = 1(It (cid:54)= ∅)

(1 − (cid:96)tIt)1(a ∈ N (It))
1 − XtIt

≥ 0 .

The Hessian of F is ∇F 2(x) = diag(xα−2). The non-negativity of Et(Xt, At) + c1k ensures that
gt(Xt, At)i ≤ Xti almost surely and hence by the deﬁnition of the potential ∇−2F (z) (cid:22) ∇−2F (Xt)
for all z ∈ [Xt, gt(Xt, At)],
(cid:34)

(cid:35)

EA∼PXt

sup
z∈[Xt,gtc(Xt,A)]

(cid:107)Et(Xt, A) + c1k(cid:107)2

∇−2F (z)

(cid:104)
(cid:107)Et(Xt, A) + c1k(cid:107)2

(cid:105)

∇−2F (Xt)

= EA∼PXt
(cid:88)

=

EA∼PXt

i(cid:54)∈It

(cid:88)

≤ 2

i(cid:54)∈It
We ﬁrst bound the c term

(cid:2)(Et(Xt, A)i + c)2∇−2F (Xt)ii

(cid:3) + 1(It (cid:54)= ∅)EA∼PXt

[∇−2F (Xt)ItIt]

EA∼PXt

(cid:2)Et(Xt, A)2

i X 2−α
ti

(cid:3) + 2EA∼PXt

[c2]

X 2−α

ti + 1 .

(cid:88)

i(cid:54)∈It

2EA∼PXt

[c2]

X 2−α

ti = 21(It (cid:54)= ∅)

(cid:88)

i(cid:54)∈It

Xti

(cid:88)

i(cid:54)∈It

(cid:32)

(cid:33)2

1 − (cid:96)tIt
(cid:80)

i(cid:54)∈It

Xti

(cid:88)

i(cid:54)∈It

X 2−α

ti ≤ 2.

Then we bound the contribution of arms i with i (cid:54)∈ N (i) and i (cid:54)∈ It, which implies Xti ≤ 1/2





2EA∼Px

(cid:88)

i:i(cid:54)∈N (i)∪It



Et(Xt, A)2

i X 2−α
ti

 = 2

(cid:88)

tiX 2−α
(cid:96)2
ti
1 − Xti

≤ 4 .

i:i(cid:54)∈N (i)∪It

Finally we bound the remaining term



2EA∼Px

Et(Xt, A)2

i X 2−α
ti

 ≤ 2





(cid:88)

i:i∈N (i)

We bound the max using Lemma 18:
a2−α
i
j∈N (i) aj

max
a∈∆([k])

= max

a∈∆([k])

k
(cid:88)

(cid:80)

i=1

(cid:88)

(cid:88)

i:i∈N (i)

tiX 2−α
(cid:96)2
ti
j∈N (i) Xtj

(cid:80)

≤ 2 max

a∈∆([k])

k
(cid:88)

i=1

a2−α
i
j∈N (i) aj

.

(cid:80)

(cid:80)

a2−α
ti
j∈N (i) aj
(cid:19)

+

(cid:88)

a2−α
i
j∈N (i) aj

(cid:80)

i:ai≤exp(− log(k)2)

+ k exp(− log(k)−1 log(k)2)

i:ai>exp(− log(k)2)
(cid:18) 4k exp(log(k)2)
Gind
(cid:19)

(cid:18)

(cid:18) 4k
Gind

≤ 4Gind log

= 4Gind

log

(cid:19)

+ log(k)2

+ 1 ,

where in the ﬁnal inequality we used Lemma 18 on the sub-graph {a : Xta > exp(− log(k)2) and
noted the fact the independence number of a sub-graph of G cannot be larger than the independence
number of G. Combining everything, we have shown that
(cid:19)

(cid:18)

(cid:19)

stab(A ) ≤ 8Gind

log

+ log(k)2

+ 9.

The proof is completed by tuning the learning rate according to Corollary 6.

(cid:18) 4k
Gind

13

E Proof of Theorem 15

Remember that the potential is F (x) = (cid:80)d

i=1 h(xi) where

h(x) =

(cid:40) d

2 x2
p−2
p−1 d

p−1

p−2 |x| + |x|p

p(p−1) + 2−p
2p d

p
p−2

1
p−2

if |x| ≤ d
otherwise .

Before the proof we provide some intuition for this choice of the potential. By the problem setting
for q = p
1−p , it holds that (cid:107)(cid:96)t(cid:107)q, (cid:107)Xt(cid:107)p ≤ 1. Assuming we have a ‘separable’ potential F (x) =
(cid:80)d

˜h(xi), we can write the stability term as

i=1

(cid:107)(cid:96)t(cid:107)2

∇−2F (z) = (cid:104)(cid:96)t ◦ (cid:96)t, (˜h(cid:48)(cid:48)(zi)−1)i=1,...,d(cid:105) ≤ (cid:107)(cid:96)t ◦ (cid:96)t(cid:107)q(cid:48)(cid:107)(˜h(cid:48)(cid:48)(zi)−1)i=1,...,d(cid:107)p(cid:48).

1

2 , p(cid:48) = q(cid:48)

q(cid:48)−1 = p

2−p , the ﬁrst factor is bounded by 1 and setting ˜h(cid:48)(cid:48)(zi) = |zi|p−2
Choosing q(cid:48) = q
ensures the second factor is bounded by 1. Unfortunately, this leads to the potential ˜h(x) =
p(p−1) |x|p, whose diameter can be arbitrarily large. To prevent the potential from exploding, we clip
h(cid:48)(cid:48)(x) at d, as shown in Fig. 2. Any upper bound on the second derivative will serve the purpose of
decreasing the diameter, however the threshold must be chosen such that the stability doesn’t suffer
too much. The value d happens to be the lowest value that keeps the stability dimension independent.

˜h(cid:48)(cid:48)(x)

˜h(1) − ˜h(x)

d

log(d)

Figure 2: p = 1: ˜h(cid:48)(cid:48)(x) and ˜h(1) − ˜h(x) for p = 1. Red lines indicate h(cid:48)(cid:48) and h respectively.

Proof of Theorem 15. By the deﬁnition of the loss estimator ˆ(cid:96)t = (cid:96)t. As usual, our plan is to bound
the stability and diameter and then apply Corollary 6.

Bounding the stability By deﬁnition h(cid:48)(cid:48)(x) = min{|x|p−2, d}. Then by Lemma 3 and the as-
sumption that Et(x, a) = (cid:96)t for all x and a,

stabt(x; η) ≤ max
z∈X

∇F −2(z)

||(cid:96)t||2


≤ max
z∈X




(cid:88)

ti|zi|2−p +
(cid:96)2

(cid:88)

i:|zi|≥d

1
p−2

i:|zi|<d

1
p−2

≤ max
z∈X

ti|zi|2−p + 1
(cid:96)2

(cid:33)






1
d

(cid:32) d

(cid:88)

i=1
(cid:32) d

(cid:88)





i=1

≤ max
z∈X

(cid:18)

=

max
z∈X

(cid:33) 2p−2

p (cid:32) d

(cid:33) 2−p

p



p
2p−2

((cid:96)2

ti)

(|zi|2−p)

p
2−p

(cid:88)

i=1

+ 1



(14)

(cid:107)(cid:96)t(cid:107)2

q(cid:107)z(cid:107)2−p

p + 1

≤ 2 ,

(cid:19)

where Eq. (14) follows from Cauchy-Schwarz.

Bounding the diameter First notice that F (x) ≥ 0 for all x ∈ X and F (0) = 0. Hence

diamF (X ) = max
x∈X

F (x) .

14

For arbitrary x ∈ X deﬁne J = {i ∈ [d]|xi ≥ d
vector xS as the |S|-dimensional vector consisting of entries (xi)i∈S. Then it holds

p−2 }, I = [d] \ J and for any S ⊂ [d] deﬁne the

1

F (x) =

(cid:107)xI (cid:107)2

2 −

d
2

2 − p
p − 1

p−1

d

p−2 (cid:107)xJ (cid:107)1 +

(cid:107)xJ (cid:107)p
p
p(p − 1)

+

2 − p
2p

p
p−2 |J|.

d

Maximizing this expression over xJ under the constraints of keeping both the set J and (cid:107)xJ (cid:107)p
1
constant is setting all but 1 coordinate in xJ to d
p−2 and shifting all other weight towards a single
entry. This follows directly from the fact that (cid:107)x(cid:107)p is convex, so the minimum of (cid:107)x(cid:107)1 under constant
(cid:107)x(cid:107)p is on the boundary. The optimal y ∈ arg maxx∈X F (x) can therefore only have a single
coordinate i such that |yi| > d

p−2 , which we assume without loss of generality is i = 1.

1

It follows that

F (y) = h(y1) +

y2
i ≤ h(y1) +

2
p−2 ≤ h(1) +

d

d2
2

1
2

.

d
2

d
(cid:88)

i=2

diamF (X ) ≤ h(1) +

=

1
2

p − 2
p − 1

p−1
p−2 +

d

1
p(p − 1)

+

2 − p
2p

p
p−2 +

d

1
2

=

1 − d

p−1
p−2

p − 1

+ d

p−1
p−2 −

1
p

+

2 − p
2p

p
p−2 +

d

≤

1
2

1 − d

p−1
p−2

p − 1

+ 1.

We immediately get the bound diamT (X ) ≤ 2

2 , we substitute z = p−1

2−p and get

diamF (X ) ≤

1 − d−z
(2 − p)z

p−1 . Let p ≤ 3
1 − d−z
z

+ 1 ≤ 2

+ 1 ≤ 2 log(d) + 1,

where we use the fact that for z ≥ 0 the term 1−d−z
log(d) for z → 0.
We have shown that diamF (X ) ≤ O(min{ 1
completed by tuning the learning rate according to Corollary 6.

z

is monotonically decreasing in z with limit

p−1 , log(d)}) and stab(A ) ≤ O(1). The proof is

15

