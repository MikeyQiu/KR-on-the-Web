8
1
0
2
 
n
a
J
 
3
 
 
]

V
C
.
s
c
[
 
 
3
v
7
9
8
6
0
.
1
1
7
1
:
v
i
X
r
a

Single-Shot Reﬁnement Neural Network for Object Detection

Shifeng Zhang1,2, Longyin Wen3, Xiao Bian3, Zhen Lei1,2, Stan Z. Li1,2
1 CBSR & NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China.
2 University of Chinese Academy of Sciences, Beijing, China.
3 GE Global Research, Niskayuna, NY.
{shifeng.zhang,zlei,szli}@nlpr.ia.ac.cn, {longyin.wen,xiao.bian}@ge.com

Abstract

For object detection,

the two-stage approach (e.g.,
Faster R-CNN) has been achieving the highest accuracy,
whereas the one-stage approach (e.g., SSD) has the ad-
vantage of high efﬁciency. To inherit the merits of both
while overcoming their disadvantages, in this paper, we pro-
pose a novel single-shot based detector, called ReﬁneDet,
that achieves better accuracy than two-stage methods and
maintains comparable efﬁciency of one-stage methods. Re-
ﬁneDet consists of two inter-connected modules, namely,
the anchor reﬁnement module and the object detection mod-
ule. Speciﬁcally, the former aims to (1) ﬁlter out nega-
tive anchors to reduce search space for the classiﬁer, and
(2) coarsely adjust the locations and sizes of anchors to
provide better initialization for the subsequent regressor.
The latter module takes the reﬁned anchors as the input
from the former to further improve the regression and pre-
dict multi-class label. Meanwhile, we design a transfer
connection block to transfer the features in the anchor re-
ﬁnement module to predict locations, sizes and class la-
bels of objects in the object detection module. The multi-
task loss function enables us to train the whole network
in an end-to-end way. Extensive experiments on PASCAL
VOC 2007, PASCAL VOC 2012, and MS COCO demon-
strate that ReﬁneDet achieves state-of-the-art detection ac-
curacy with high efﬁciency. Code is available at https:
//github.com/sfzhang15/RefineDet.

1. Introduction

Object detection has achieved signiﬁcant advances in re-
cent years, with the framework of deep neural networks
(DNN). The current DNN detectors of state-of-the-art can
be divided into two categories: (1) the two-stage approach,
including [3, 15, 36, 41], and (2) the one-stage approach,
including [30, 35]. In the two-stage approach, a sparse set
of candidate object boxes is ﬁrst generated, and then they
are further classiﬁed and regressed. The two-stage meth-

ods have been achieving top performances on several chal-
lenging benchmarks, including PASCAL VOC [8] and MS
COCO [29].

The one-stage approach detects objects by regular and
dense sampling over locations, scales and aspect ratios. The
main advantage of this is its high computational efﬁciency.
However, its detection accuracy is usually behind that of
the two-stage approach, one of the main reasons being due
to the class imbalance problem [28].

Some recent methods in the one-stage approach aim to
address the class imbalance problem, to improve the detec-
tion accuracy. Kong et al. [24] use the objectness prior con-
straint on convolutional feature maps to signiﬁcantly reduce
the search space of objects. Lin et al. [28] address the class
imbalance issue by reshaping the standard cross entropy
loss to focus training on a sparse set of hard examples and
down-weights the loss assigned to well-classiﬁed examples.
Zhang et al. [53] design a max-out labeling mechanism to
reduce false positives resulting from class imbalance.

In our opinion,

the current state-of-the-art two-stage
methods, e.g., Faster R-CNN [36], R-FCN [5], and FPN
[27], have three advantages over the one-stage methods as
follows: (1) using two-stage structure with sampling heuris-
tics to handle class imbalance; (2) using two-step cascade to
regress the object box parameters; (3) using two-stage fea-
tures to describe the objects1.
In this work, we design a
novel object detection framework, called ReﬁneDet, to in-
herit the merits of the two approaches (i.e., one-stage and
two-stage approaches) and overcome their shortcomings. It
improves the architecture of the one-stage approach, by us-
ing two inter-connected modules (see Figure 1), namely, the
anchor 2 reﬁnement module (ARM) and the object detection

1In case of Faster R-CNN, the features (excluding shared features) in
the ﬁrst stage (i.e., RPN) are trained for the binary classiﬁcation (being an
object or not), while the features (excluding shared features) in the sec-
ond stage(i.e., Fast R-CNN) are trained for the multi-class classiﬁcation
(background or object classes).

2We denote the reference bounding box as “anchor box”, which is also
called “anchor” for simplicity, as in [36]. However, in [30], it is called
“default box”.

1

Figure 1: Architecture of ReﬁneDet. For better visualization, we only display the layers used for detection. The celadon
parallelograms denote the reﬁned anchors associated with different feature layers. The stars represent the centers of the
reﬁned anchor boxes, which are not regularly paved on the image.

module (ODM). Speciﬁcally, the ARM is designed to (1)
identify and remove negative anchors to reduce search space
for the classiﬁer, and (2) coarsely adjust the locations and
sizes of anchors to provide better initialization for the subse-
quent regressor. The ODM takes the reﬁned anchors as the
input from the former to further improve the regression and
predict multi-class labels. As shown in Figure 1, these two
inter-connected modules imitate the two-stage structure and
thus inherit the three aforementioned advantages to produce
accurate detection results with high efﬁciency. In addition,
we design a transfer connection block (TCB) to transfer the
features3 in the ARM to predict locations, sizes, and class
labels of objects in the ODM. The multi-task loss function
enables us to train the whole network in an end-to-end way.
Extensive experiments on PASCAL VOC 2007, PAS-
CAL VOC 2012, and MS COCO benchmarks demonstrate
that ReﬁneDet outperforms the state-of-the-art methods.
Speciﬁcally, it achieves 85.8% and 86.8% mAPs on VOC
2007 and 2012, with VGG-16 network. Meanwhile, it out-
performs the previously best published results from both
one-stage and two-stage approaches by achieving 41.8%
AP4 on MS COCO test-dev with ResNet-101. In ad-

3The features in the ARM focus on distinguishing positive anchors
from background. We design the TCB to transfer the features in the ARM
to handle the more challenging tasks in the ODM, i.e., predict accurate
object locations, sizes and multi-class labels.

4Based on the evaluation protocol in MS COCO [29], AP is the sin-

dition, ReﬁneDet is time efﬁcient, i.e., it runs at 40.2 FPS
and 24.1 FPS on a NVIDIA Titan X GPU with the input
sizes 320 × 320 and 512 × 512 in inference.

The main contributions of this work are summarized as
follows.
(1) We introduce a novel one-stage framework
for object detection, composed of two inter-connected mod-
ules, i.e., the ARM and the ODM. This leads to performance
better than the two-stage approach while maintaining high
efﬁciency of the one-stage approach. (2) To ensure the ef-
fectiveness, we design the TCB to transfer the features in
the ARM to handle more challenging tasks, i.e., predict ac-
curate object locations, sizes and class labels, in the ODM.
(3) ReﬁneDet achieves the latest state-of-the-art results on
generic object detection (i.e., PASCAL VOC 2007 [10],
PASCAL VOC 2012 [11] and MS COCO [29]).

2. Related Work

Classical Object Detectors. Early object detection meth-
ods are based on the sliding-window paradigm, which ap-
ply the hand-crafted features and classiﬁers on dense image
grids to ﬁnd objects. As one of the most successful meth-
ods, Viola and Jones [47] use Haar feature and AdaBoost
to train a series of cascaded classiﬁers for face detection,

gle most important metric, which is computed by averaging over all 10
intersection over union (IoU) thresholds (i.e., in the range [0.5:0.95] with
uniform step size 0.05) of 80 categories.

achieving satisfactory accuracy with high efﬁciency. DPM
[12] is another popular method using mixtures of multi-
scale deformable part models to represent highly variable
object classes, maintaining top results on PASCAL VOC [8]
for many years. However, with the arrival of deep convolu-
tional network, the object detection task is quickly dom-
inated by the CNN-based detectors, which can be roughly
divided into two categories, i.e., the two-stage approach and
one-stage approach.
Two-Stage Approach. The two-stage approach consists of
two parts, where the ﬁrst one (e.g., Selective Search [46],
EdgeBoxes [55], DeepMask [32, 33], RPN [36]) generates a
sparse set of candidate object proposals, and the second one
determines the accurate object regions and the correspond-
ing class labels using convolutional networks. Notably, the
two-stage approach (e.g., R-CNN [16], SPPnet [18], Fast R-
CNN [15] to Faster R-CNN [36]) achieves dominated per-
formance on several challenging datasets (e.g., PASCAL
VOC 2012 [11] and MS COCO [29]). After that, numer-
ous effective techniques are proposed to further improve the
performance, such as architecture diagram [5, 26, 54], train-
ing strategy [41, 48], contextual reasoning [1, 14, 40, 50]
and multiple layers exploiting [3, 25, 27, 42].
One-Stage Approach. Considering the high efﬁciency, the
one-stage approach attracts much more attention recently.
Sermanet et al. [38] present the OverFeat method for clas-
siﬁcation, localization and detection based on deep Con-
vNets, which is trained end-to-end, from raw pixels to ul-
timate categories. Redmon et al. [34] use a single feed-
forward convolutional network to directly predict object
classes and locations, called YOLO, which is extremely
fast. After that, YOLOv2 [35] is proposed to improve
YOLO in several aspects, i.e., add batch normalization on
all convolution layers, use high resolution classiﬁer, use
convolution layers with anchor boxes to predict bounding
boxes instead of the fully connected layers, etc. Liu et al.
[30] propose the SSD method, which spreads out anchors
of different scales to multiple layers within a ConvNet and
enforces each layer to focus on predicting objects of a cer-
tain scale. DSSD [13] introduces additional context into
SSD via deconvolution to improve the accuracy. DSOD
[39] designs an efﬁcient framework and a set of principles to
learn object detectors from scratch, following the network
structure of SSD. To improve the accuracy, some one-stage
methods [24, 28, 53] aim to address the extreme class im-
balance problem by re-designing the loss function or clas-
siﬁcation strategies. Although the one-stage detectors have
made good progress, their accuracy still trails that of two-
stage methods.

3. Network Architecture

Refer to the overall network architecture shown in Fig-
ure 1. Similar to SSD [30], ReﬁneDet is based on a feed-

forward convolutional network that produces a ﬁxed num-
ber of bounding boxes and the scores indicating the pres-
ence of different classes of objects in those boxes, followed
by the non-maximum suppression to produce the ﬁnal re-
sult. ReﬁneDet is formed by two inter-connected modules,
i.e., the ARM and the ODM. The ARM aims to remove neg-
ative anchors so as to reduce search space for the classiﬁer
and also coarsely adjust the locations and sizes of anchors
to provide better initialization for the subsequent regressor,
whereas ODM aims to regress accurate object locations and
predict multi-class labels based on the reﬁned anchors. The
ARM is constructed by removing the classiﬁcation layers
and adding some auxiliary structures of two base networks
(i.e., VGG-16 [43] and ResNet-101 [19] pretrained on Im-
ageNet [37]) to meet our needs. The ODM is composed of
the outputs of TCBs followed by the prediction layers (i.e.,
the convolution layers with 3 × 3 kernel size), which gener-
ates the scores for object classes and shape offsets relative to
the reﬁned anchor box coordinates. The following explain
three core components in ReﬁneDet, i.e., (1) transfer con-
nection block (TCB), converting the features from the ARM
to the ODM for detection; (2) two-step cascaded regression,
accurately regressing the locations and sizes of objects; (3)
negative anchor ﬁltering, early rejecting well-classiﬁed neg-
ative anchors and mitigate the imbalance issue.

Transfer Connection Block. To link between the ARM
and ODM, we introduce the TCBs to convert features of dif-
ferent layers from the ARM, into the form required by the
ODM, so that the ODM can share features from the ARM.
Notably, from the ARM, we only use the TCBs on the fea-
ture maps associated with anchors. Another function of the
TCBs is to integrate large-scale context [13, 27] by adding
the high-level features to the transferred features to improve
detection accuracy. To match the dimensions between them,
we use the deconvolution operation to enlarge the high-level
feature maps and sum them in the element-wise way. Then,
we add a convolution layer after the summation to ensure
the discriminability of features for detection. The architec-
ture of the TCB is shown in Figure 2.

Two-Step Cascaded Regression. Current one-stage meth-
ods [13, 24, 30] rely on one-step regression based on various
feature layers with different scales to predict the locations
and sizes of objects, which is rather inaccurate in some chal-
lenging scenarios, especially for the small objects. To that
end, we present a two-step cascaded regression strategy to
regress the locations and sizes of objects. That is, we use
the ARM to ﬁrst adjust the locations and sizes of anchors to
provide better initialization for the regression in the ODM.
Speciﬁcally, we associate n anchor boxes with each regu-
larly divided cell on the feature map. The initial position of
each anchor box relative to its corresponding cell is ﬁxed.
At each feature map cell, we predict four offsets of the re-
ﬁned anchor boxes relative to the original tiled anchors and

adapt to variations of objects. That is, we randomly ex-
pand and crop the original training images with additional
random photometric distortion [20] and ﬂipping to generate
the training samples. Please refer to [30] for more details.
Backbone Network. We use VGG-16 [43] and ResNet-101
[19] as the backbone networks in our ReﬁneDet, which are
pretrained on the ILSVRC CLS-LOC dataset [37]. Notably,
ReﬁneDet can also work on other pretrained networks, such
as Inception V2 [22], Inception ResNet [44], and ResNeXt-
101 [49]. Similar to DeepLab-LargeFOV [4], we convert
fc6 and fc7 of VGG-16 to convolution layers conv fc6 and
conv fc7 via subsampling parameters. Since conv4 3 and
conv5 3 have different feature scales compared to other lay-
ers, we use L2 normalization [31] to scale the feature norms
in conv4 3 and conv5 3 to 10 and 8, then learn the scales
during back propagation. Meanwhile, to capture high-level
information and drive object detection at multiple scales,
we also add two extra convolution layers (i.e., conv6 1 and
conv6 2) to the end of the truncated VGG-16 and one extra
residual block (i.e., res6) to the end of the truncated ResNet-
101, respectively.
Anchors Design and Matching. To handle different scales
of objects, we select four feature layers with the total stride
sizes 8, 16, 32, and 64 pixels for both VGG-16 and ResNet-
1015, associated with several different scales of anchors for
prediction. Each feature layer is associated with one spe-
ciﬁc scale of anchors (i.e., the scale is 4 times of the to-
tal stride size of the corresponding layer) and three aspect
ratios (i.e., 0.5, 1.0, and 2.0). We follow the design of
anchor scales over different layers in [53], which ensures
that different scales of anchors have the same tiling den-
sity [51, 52] on the image. Meanwhile, during the train-
ing phase, we determine the correspondence between the
anchors and ground truth boxes based on the jaccard over-
lap [7], and train the whole network end-to-end accordingly.
Speciﬁcally, we ﬁrst match each ground truth to the anchor
box with the best overlap score, and then match the anchor
boxes to any ground truth with overlap higher than 0.5.
Hard Negative Mining. After matching step, most of the
anchor boxes are negatives, even for the ODM, where some
easy negative anchors are rejected by the ARM. Similar
to SSD [30], we use hard negative mining to mitigate the
extreme foreground-background class imbalance, i.e., we
select some negative anchor boxes with top loss values to
make the ratio between the negatives and positives below
3 : 1, instead of using all negative anchors or randomly se-
lecting the negative anchors in training.
Loss Function. The loss function for ReﬁneDet consists of
two parts, i.e., the loss in the ARM and the loss in the ODM.

5For the VGG-16 base network, the conv4 3, conv5 3, conv fc7, and
conv6 2 feature layers are used to predict the locations, sizes and con-
ﬁdences of objects. While for the ResNet-101 base network, res3b3,
res4b22, res5c, and res6 are used for prediction.

Figure 2: The overview of the transfer connection block.

two conﬁdence scores indicating the presence of foreground
objects in those boxes. Thus, we can yield n reﬁned anchor
boxes at each feature map cell.

After obtaining the reﬁned anchor boxes, we pass them
to the corresponding feature maps in the ODM to further
generate object categories and accurate object locations and
sizes, as shown in Figure 1. The corresponding feature
maps in the ARM and the ODM have the same dimension.
We calculate c class scores and the four accurate offsets of
objects relative to the reﬁned anchor boxes, yielding c + 4
outputs for each reﬁned anchor boxes to complete the de-
tection task. This process is similar to the default boxes
used in SSD [30]. However, in contrast to SSD [30] di-
rectly uses the regularly tiled default boxes for detection,
ReﬁneDet uses two-step strategy, i.e., the ARM generates
the reﬁned anchor boxes, and the ODM takes the reﬁned
anchor boxes as input for further detection, leading to more
accurate detection results, especially for the small objects.
Negative Anchor Filtering. To early reject well-classiﬁed
negative anchors and mitigate the imbalance issue, we de-
sign a negative anchor ﬁltering mechanism. Speciﬁcally, in
training phase, for a reﬁned anchor box, if its negative con-
ﬁdence is larger than a preset threshold θ (i.e., set θ = 0.99
empirically), we will discard it in training the ODM. That is,
we only pass the reﬁned hard negative anchor boxes and re-
ﬁned positive anchor boxes to train the ODM. Meanwhile,
in the inference phase, if a reﬁned anchor box is assigned
with a negative conﬁdence larger than θ, it will be discarded
in the ODM for detection.

4. Training and Inference

Data Augmentation. We use several data augmentation
strategies presented in [30] to construct a robust model to

Nodm

For the ARM, we assign a binary class label (of being an
object or not) to each anchor and regress its location and
size simultaneously to get the reﬁned anchor. After that, we
pass the reﬁned anchors with the negative conﬁdence less
than the threshold to the ODM to further predict object cat-
egories and accurate object locations and sizes. With these
deﬁnitions, we deﬁne the loss function as:
(cid:0) (cid:80)
(cid:0) (cid:80)

L({pi}, {xi}, {ci}, {ti}) = 1
Narm
i )(cid:1) + 1
+ (cid:80)
i ≥ 1]Lr(xi, g∗
i )(cid:1)
+ (cid:80)
i ≥ 1]Lr(ti, g∗

i Lb(pi, [l∗
i ≥ 1])
i Lm(ci, l∗
i )

i[l∗
i[l∗

i ≥ 1]) = 0 and Lr(xi, g∗
i ) = 0 and Lr(ti, g∗

(1)
where i is the index of anchor in a mini-batch, l∗
is the
i
ground truth class label of anchor i, g∗
i is the ground truth
location and size of anchor i. pi and xi are the predicted
conﬁdence of the anchor i being an object and reﬁned co-
ordinates of the anchor i in the ARM. ci and ti are the
predicted object class and coordinates of the bounding box
in the ODM. Narm and Nodm are the numbers of positive
anchors in the ARM and ODM, respectively. The binary
classiﬁcation loss Lb is the cross-entropy/log loss over two
classes (object vs. not object), and the multi-class classiﬁ-
cation loss Lm is the softmax loss over multiple classes con-
ﬁdences. Similar to Fast R-CNN [15], we use the smooth
L1 loss as the regression loss Lr. The Iverson bracket indi-
cator function [l∗
i ≥ 1] outputs 1 when the condition is true,
i.e., l∗
i ≥ 1 (the anchor is not the negative), and 0 other-
wise. Hence [l∗
i ≥ 1]Lr indicates that the regression loss is
ignored for negative anchors. Notably, if Narm = 0, we set
Lb(pi, [l∗
i ) = 0; and if Nodm = 0,
we set Lm(ci, l∗
i ) = 0 accordingly.
Optimization. As mentioned above, the backbone network
(e.g., VGG-16 and ResNet-101) in our ReﬁneDet method is
pretrained on the ILSVRC CLS-LOC dataset [37]. We use
the “xavier” method [17] to randomly initialize the parame-
ters in the two extra added convolution layers (i.e., conv6 1
and conv6 2) of VGG-16 based ReﬁneDet, and draw the pa-
rameters from a zero-mean Gaussian distribution with stan-
dard deviation 0.01 for the extra residual block (i.e., res6) of
ResNet-101 based ReﬁneDet. We set the default batch size
to 32 in training. Then, the whole network is ﬁne-tuned us-
ing SGD with 0.9 momentum and 0.0005 weight decay. We
set the initial learning rate to 10−3, and use slightly differ-
ent learning rate decay policy for different dataset, which
will be described in details later.
Inference. At inference phase, the ARM ﬁrst ﬁlters out the
regularly tiled anchors with the negative conﬁdence scores
larger than the threshold θ, and then reﬁnes the locations
and sizes of remaining anchors. After that, the ODM takes
over these reﬁned anchors, and outputs top 400 high con-
ﬁdent detections per image. Finally, we apply the non-
maximum suppression with jaccard overlap of 0.45 per
class and retain the top 200 high conﬁdent detections per
image to produce the ﬁnal detection results.

5. Experiments

Experiments are conducted on three datasets: PASCAL
VOC 2007, PASCAL VOC 2012 and MS COCO. The PAS-
CAL VOC and MS COCO datasets include 20 and 80 ob-
ject classes, respectively. The classes in PASCAL VOC are
the subset of that in MS COCO. We implement ReﬁneDet
in Caffe [23]. All the training and testing codes and the
trained models are available at https://github.com/
sfzhang15/RefineDet.

5.1. PASCAL VOC 2007

All models are trained on the VOC 2007 and VOC 2012
trainval sets, and tested on the VOC 2007 test set. We
set the learning rate to 10−3 for the ﬁrst 80k iterations, and
decay it to 10−4 and 10−5 for training another 20k and 20k
iterations, respectively. We use the default batch size 32 in
training, and only use VGG-16 as the backbone network for
all the experiments on the PASCAL VOC dataset, including
VOC 2007 and VOC 2012.

We compare ReﬁneDet6 with the state-of-the-art detec-
tors in Table 1. With low dimension input (i.e., 320 × 320),
ReﬁneDet produces 80.0% mAP without bells and whis-
tles, which is the ﬁrst method achieving above 80% mAP
with such small input images, much better than several
modern objectors. By using larger input size 512 × 512,
ReﬁneDet achieves 81.8% mAP, surpassing all one-stage
methods, e.g., RON384 [24], SSD513 [13], DSSD513 [13],
etc. Comparing to the two-stage methods, ReﬁneDet512
performs better than most of them except CoupleNet [54],
which is based on ResNet-101 and uses larger input size
(i.e., ∼ 1000 × 600) than our ReﬁneDet512. As pointed
out in [21], the input size signiﬁcantly inﬂuences detection
accuracy. The reason is that high resolution inputs make
the detectors “seeing” small objects clearly to increase suc-
cessful detections. To reduce the impact of input size for a
fair comparison, we use the multi-scale testing strategy to
evaluate ReﬁneDet, achieving 83.1% (ReﬁneDet320+) and
83.8% (ReﬁneDet512+) mAPs, which are much better than
the state-of-the-art methods.

5.1.1 Run Time Performance

We present the inference speed of ReﬁneDet and the state-
of-the-art methods in the ﬁfth column of Table 1. The speed
is evaluated with batch size 1 on a machine with NVIDIA
Titan X, CUDA 8.0 and cuDNN v6. As shown in Table 1,
we ﬁnd that ReﬁneDet processes an image in 24.8ms (40.3
FPS) and 41.5ms (24.1 FPS) with input sizes 320 × 320
and 512 × 512, respectively. To the best of our knowledge,

6Due to the shortage of computational resources, we only train Re-
ﬁneDet with two kinds of input size, i.e., 320 × 320 and 512 × 512. We
believe the accuracy of ReﬁneDet can be further improved using larger
input images.

Table 1: Detection results on PASCAL VOC dataset. For VOC 2007, all methods are trained on VOC 2007 and VOC 2012
trainval sets and tested on VOC 2007 test set. For VOC 2012, all methods are trained on VOC 2007 and VOC 2012
trainval sets plus VOC 2007 test set, and tested on VOC 2012 test set. Bold fonts indicate the best mAP.

Method

Backbone

Input size

#Boxes

FPS

mAP (%)

VOC 2007

VOC 2012

two-stage:
Fast R-CNN[15]
Faster R-CNN[36]
OHEM[41]
HyperNet[25]
Faster R-CNN[36]
ION[1]
MR-CNN[14]
R-FCN[5]
CoupleNet[54]

one-stage:
YOLO[34]
RON384[24]
SSD321[13]
SSD300∗[30]
DSOD300[39]
YOLOv2[35]
DSSD321[13]
SSD512∗[30]
SSD513[13]
DSSD513[13]
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+

VGG-16
VGG-16
VGG-16
VGG-16
ResNet-101
VGG-16
VGG-16
ResNet-101
ResNet-101

GoogleNet [45]
VGG-16
ResNet-101
VGG-16
DS/64-192-48-1
Darknet-19
ResNet-101
VGG-16
ResNet-101
ResNet-101
VGG-16
VGG-16
VGG-16
VGG-16

∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600

448 × 448
384 × 384
321 × 321
300 × 300
300 × 300
544 × 544
321 × 321
512 × 512
513 × 513
513 × 513
320 × 320
512 × 512
-
-

∼ 2000
300
300
100
300
4000
250
300
300

98
30600
17080
8732
8732
845
17080
24564
43688
43688
6375
16320
-
-

0.5
7
7
0.88
2.4
1.25
0.03
9
8.2

45
15
11.2
46
17.4
40
9.5
19
6.8
5.5
40.3
24.1
-
-

70.0
73.2
74.6
76.3
76.4
76.5
78.2
80.5
82.7

63.4
75.4
77.1
77.2
77.7
78.6
78.6
79.8
80.6
81.5
80.0
81.8
83.1
83.8

68.4
70.4
71.9
71.4
73.8
76.4
73.9
77.6
80.4

57.9
73.0
75.4
75.8
76.3
73.4
76.3
78.5
79.4
80.0
78.1
80.1
82.7
83.5

ReﬁneDet is the ﬁrst real-time method to achieve detection
accuracy above 80% mAP on PASCAL VOC 2007. Com-
paring to SSD, RON, DSSD and DSOD, ReﬁneDet asso-
ciates fewer anchor boxes on the feature maps (e.g., 24564
anchor boxes in SSD512∗[30] vs. 16320 anchor boxes in
ReﬁneDet512). However, ReﬁneDet still achieves top accu-
racy with high efﬁciency, mainly thanks to the design of two
inter-connected modules, (e.g., two-step regression), which
enables ReﬁneDet to adapt to different scales and aspect ra-
tios of objects. Meanwhile, only YOLO and SSD300∗ are
slightly faster than our ReﬁneDet320, but their accuracy are
16.6% and 2.5% worse than ours. In summary, ReﬁneDet
achieves the best trade-off between accuracy and speed.

5.1.2 Ablation Study

To demonstrate the effectiveness of different components
in ReﬁneDet, we construct four variants and evaluate them
on VOC 2007, shown in Table 3. Speciﬁcally, for a fair
comparison, we use the same parameter settings and input
size (320 × 320) in evaluation. All models are trained on
VOC 2007 and VOC 2012 trainval sets, and tested on
VOC 2007 test set.

Negative Anchor Filtering. To demonstrate the effective-
ness of the negative anchor ﬁltering, we set the conﬁdence

threshold θ of the anchors to be negative to 1.0 in both train-
ing and testing.
In this case, all reﬁned anchors will be
sent to the ODM for detection. Other parts of ReﬁneDet re-
main unchanged. Removing negative anchor ﬁltering leads
to 0.5% drop in mAP (i.e., 80.0% vs. 79.5%). The reason
is that most of these well-classiﬁed negative anchors will be
ﬁltered out during training, which solves the class imbal-
ance issue to some extent.

Two-Step Cascaded Regression. To validate the effective-
ness of the two-step cascaded regression, we redesign the
network structure by directly using the regularly paved an-
chors instead of the reﬁned ones from the ARM (see the
fourth column in Table 3). As shown in Table 3, we ﬁnd that
mAP is reduced from 79.5% to 77.3%. This sharp decline
(i.e., 2.2%) demonstrates that the two-step anchor cascaded
regression signiﬁcantly help promote the performance.

Transfer Connection Block. We construct a network by
cutting the TCBs in ReﬁneDet and redeﬁning the loss func-
tion in the ARM to directly detect multi-class of objects,
just like SSD, to demonstrate the effect of the TCB. The
detection accuracy of the model is presented in the ﬁfth col-
umn in Table 3. We compare the results in the fourth and
ﬁfth columns in Table 3 (77.3% vs. 76.2%) and ﬁnd that
the TCB improves the mAP by 1.1%. The main reason is

Table 2: Detection results on MS COCO test-dev set. Bold fonts indicate the best performance.

Method
two-stage:
Fast R-CNN [15]
Faster R-CNN [36]
OHEM [41]
ION [1]
OHEM++ [41]
R-FCN [5]
CoupleNet [54]
Faster R-CNN by G-RMI [21]
Faster R-CNN+++ [19]
Faster R-CNN w FPN [27]
Faster R-CNN w TDM [42]
Deformable R-FCN [6]
umd det [2]
G-RMI [21]

one-stage:
YOLOv2 [35]
SSD300∗ [30]
RON384++ [24]
SSD321 [13]
DSSD321 [13]
SSD512∗ [30]
SSD513 [13]
DSSD513 [13]
RetinaNet500 [28]
RetinaNet800 [28]∗
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+
ReﬁneDet320+
ReﬁneDet512+

Data

Backbone

AP

AP50

AP75

APS

APM

APL

train
trainval
trainval
train
trainval
trainval
trainval
-
trainval
trainval35k
trainval
trainval
trainval
trainval32k

trainval35k
trainval35k
trainval
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k

VGG-16
VGG-16
VGG-16
VGG-16
VGG-16
ResNet-101
ResNet-101
Inception-ResNet-v2[44]
ResNet-101-C4
ResNet-101-FPN
Inception-ResNet-v2-TDM
Aligned-Inception-ResNet
ResNet-101
Ensemble of Five Models

DarkNet-19[35]
VGG-16
VGG-16
ResNet-101
ResNet-101
VGG-16
ResNet-101
ResNet-101
ResNet-101
ResNet-101-FPN
VGG-16
VGG-16
ResNet-101
ResNet-101
VGG-16
VGG-16
ResNet-101
ResNet-101

19.7
21.9
22.6
23.6
25.5
29.9
34.4
34.7
34.9
36.2
36.8
37.5
40.8
41.6

21.6
25.1
27.4
28.0
28.0
28.8
31.2
33.2
34.4
39.1
29.4
33.0
32.0
36.4
35.2
37.6
38.6
41.8

35.9
42.7
42.5
43.2
45.9
51.9
54.8
55.5
55.7
59.1
57.7
58.0
62.4
61.9

44.0
43.1
49.5
45.4
46.1
48.5
50.4
53.3
53.1
59.1
49.2
54.5
51.4
57.5
56.1
58.7
59.9
62.9

-
-
22.2
23.6
26.1
-
37.2
36.7
37.4
39.0
39.2
40.8
44.9
45.4

19.2
25.8
27.1
29.3
29.2
30.3
33.3
35.2
36.8
42.3
31.3
35.5
34.2
39.5
37.7
40.8
41.7
45.7

-
-
5.0
6.4
7.4
10.8
13.4
13.5
15.6
18.2
16.2
19.4
23.0
23.9

5.0
6.6
-
6.2
7.4
10.9
10.2
13.0
14.7
21.8
10.0
16.3
10.5
16.6
19.5
22.7
21.1
25.6

-
-
23.7
24.1
27.7
32.8
38.1
38.1
38.7
39.0
39.8
40.1
43.4
43.5

22.4
25.9
-
28.3
28.1
31.8
34.5
35.4
38.5
42.7
32.0
36.3
34.7
39.9
37.2
40.3
41.7
45.1

-
-
37.9
38.3
40.3
45.0
50.8
52.0
50.9
48.2
52.1
52.5
53.2
54.9

35.5
41.4
-
49.3
47.6
43.5
49.8
51.1
49.1
50.2
44.4
44.3
50.4
51.4
47.0
48.3
52.3
54.1

∗ This entry reports the single model accuracy of RetinaNet method, trained with scale jitter and for 1.5× longer than RetinaNet500.

Table 3: Effectiveness of various designs. All models are
trained on VOC 2007 and VOC 2012 trainval set and
tested on VOC 2007 test set.

Component
negative anchor ﬁltering?
two-step cascaded regression?
transfer connection block?
mAP (%)

!
!
!
80.0

!
!
79.5

ReﬁneDet320

!
77.3

76.2

that the model can inherit the discriminative features from
the ARM, and integrate large-scale context information to
improve the detection accuracy by using the TCB.

5.2. PASCAL VOC 2012

Following the protocol of VOC 2012, we submit the de-
tection results of ReﬁneDet to the public testing server for
evaluation. We use VOC 2007 trainval set and test
set plus VOC 2012 trainval set (21, 503 images) for

training, and test on VOC 2012 test set (10, 991 images).
We use the default batch size 32 in training. Meanwhile, we
set the learning rate to 10−3 in the ﬁrst 160k iterations, and
decay it to 10−4 and 10−5 for another 40k and 40k itera-
tions.

Table 1 shows the accuracy of the proposed ReﬁneDet al-
gorithm, as well as the state-of-the-art methods. Among the
methods fed with input size 320 × 320, ReﬁneDet320 ob-
tains the top 78.1% mAP, which is even better than most of
those two-stage methods using about 1000 × 600 input size
(e.g., 70.4% mAP of Faster R-CNN [36] and 77.6% mAP
of R-FCN [5]). Using the input size 512 × 512, ReﬁneDet
improves mAP to 80.1%, which is surpassing all one-stage
methods and only slightly lower than CoupleNet [54] (i.e.,
80.4%). CoupleNet uses ResNet-101 as base network with
1000 × 600 input size. To reduce the impact of input size
for a fair comparison, we also use multi-scale testing to
evaluate ReﬁneDet and obtain the state-of-the-art mAPs of
82.7% (ReﬁneDet320+) and 83.5% (ReﬁneDet512+).

Table 4: Detection results on PASCAL VOC dataset. All
models are pre-trained on MS COCO, and ﬁne-tuned on
PASCAL VOC. Bold fonts indicate the best mAP.

mAP (%)
VOC 2007 test VOC 2012 test

Method

Backbone

two-stage:
Faster R-CNN[36]
OHEM++[41]
R-FCN[5]

VGG-16
VGG-16
ResNet-101

VGG-16
VGG-16
VGG-16

one-stage:
SSD300[30]
SSD512[30]
RON384++[24]
DSOD300[39] DS/64-192-48-1
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+

VGG-16
VGG-16
VGG-16
VGG-16

5.3. MS COCO

78.8
-
83.6

81.2
83.2
81.3
81.7
84.0
85.2
85.6
85.8

75.9
80.1
82.0

79.3
82.2
80.7
79.3
82.7
85.0
86.0
86.8

In addition to PASCAL VOC, we also evaluate Re-
ﬁneDet on MS COCO [29]. Unlike PASCAL VOC, the
detection methods using ResNet-101 always achieve bet-
ter performance than those using VGG-16 on MS COCO.
Thus, we also report the results of ResNet-101 based Re-
ﬁneDet. Following the protocol in MS COCO, we use the
trainval35k set [1] for training and evaluate the results
from test-dev evaluation server. We set the batch size to
32 in training7, and train the model with 10−3 learning rate
for the ﬁrst 280k iterations, then 10−4 and 10−5 for another
80k and 40k iterations, respectively.

Table 7 shows the results on MS COCO test-dev set.
ReﬁneDet320 with VGG-16 produces 29.4% AP that is bet-
ter than all other methods based on VGG-16 (e.g., SSD512∗
[30] and OHEM++ [41]). The accuracy of ReﬁneDet can
be improved to 33.0% by using larger input size (i.e.,
512 × 512), which is much better than several modern ob-
ject detectors, e.g., Faster R-CNN [36] and SSD512∗ [30].
Meanwhile, using ResNet-101 can further improve the per-
formance of ReﬁneDet, i.e., ReﬁneDet320 with ResNet-101
achieves 32.0% AP and ReﬁneDet512 achieves 36.4% AP,
exceeding most of the detection methods except Faster R-
CNN w TDM [42], Deformable R-FCN [6], RetinaNet800
[28], umd det [2], and G-RMI [21]. All these methods use a
much bigger input images for both training and testing (i.e.,
1000×600 or 800×800) than our ReﬁneDet (i.e., 320×320
and 512 × 512). Similar to PASCAL VOC, we also report
the multi-scale testing AP results of ReﬁneDet for fair com-
parison in Table 7, i.e., 35.2% (ReﬁneDet320+ with VGG-
16), 37.6% (ReﬁneDet512+ with VGG-16), 38.6% (Re-

7Due to the memory issue, we reduce the batch size to 20 (which is the
largest batch size we can use for training on a machine with 4 NVIDIA
M40 GPUs) to train the ResNet-101 based ReﬁneDet with the input size
512 × 512, and train the model with 10−3 learning rate for the ﬁrst 400k
iterations, then 10−4 and 10−5 for another 80k and 60k iterations.

ﬁneDet320+ with ResNet-101) and 41.8% (ReﬁneDet512+
with ResNet-101). The best performance of ReﬁneDet is
41.8%, which is the state-of-the-art, surpassing all pub-
lished two-stage and one-stage approaches. Although the
second best detector G-RMI [21] ensembles ﬁve Faster R-
CNN models, it still produces 0.2% lower AP than Re-
ﬁneDet using a single model. Comparing to the third and
fourth best detectors, i.e., umd det [2] and RetinaNet800
[28], ReﬁneDet produces 1.0% and 2.7% higher APs. In
addition, the main contribution: focal loss in RetinaNet800,
is complementary to our method. We believe that it can be
used in ReﬁneNet to further improve the performance.

5.4. From MS COCO to PASCAL VOC

We study how the MS COCO dataset help the detec-
tion accuracy on PASCAL VOC. Since the object classes
in PASCAL VOC are the subset of MS COCO, we directly
ﬁne-tune the detection models pretrained on MS COCO via
subsampling the parameters, which achieves 84.0% mAP
(ReﬁneDet320) and 85.2% mAP (ReﬁneDet512) on VOC
2007 test set, and 82.7% mAP (ReﬁneDet320) and 85.0%
mAP (ReﬁneDet512) on VOC 2012 test set, shown in Ta-
ble 4. After using the multi-scale testing, the detection ac-
curacy are promoted to 85.6%, 85.8%, 86.0% and 86.8%,
respectively. As shown in Table 4, using the training data in
MS COCO and PASCAL VOC, our ReﬁneDet obtains the
top mAP scores on both VOC 2007 and VOC 2012. Most
important, our single model ReﬁneNet512+ based on VGG-
16 ranks as the top 5 on the VOC 2012 Leaderboard (see
[9]), which is the best accuracy among all one-stage meth-
ods. Other two-stage methods achieving better results are
based on much deeper networks (e.g., ResNet-101 [19] and
ResNeXt-101 [49]) or using ensemble mechanism.

6. Conclusions

In this paper, we present a single-shot reﬁnement neu-
ral network based detector, which consists of two inter-
connected modules, i.e., the ARM and the ODM. The ARM
aims to ﬁlter out the negative anchors to reduce search space
for the classiﬁer and also coarsely adjust the locations and
sizes of anchors to provide better initialization for the subse-
quent regressor, while the ODM takes the reﬁned anchors as
the input from the former ARM to regress the accurate ob-
ject locations and sizes and predict the corresponding multi-
class labels. The whole network is trained in an end-to-end
fashion with the multi-task loss. We carry out several exper-
iments on PASCAL VOC 2007, PASCAL VOC 2012, and
MS COCO datasets to demonstrate that ReﬁneDet achieves
the state-of-the-art detection accuracy with high efﬁciency.
In the future, we plan to employ ReﬁneDet to detect some
other speciﬁc kinds of objects, e.g., pedestrian, vehicle, and
face, and introduce the attention mechanism in ReﬁneDet to
further improve the performance.

References

[1] S. Bell, C. L. Zitnick, K. Bala, and R. B. Girshick. Inside-
outside net: Detecting objects in context with skip pooling
and recurrent neural networks. In CVPR, pages 2874–2883,
2016. 3, 6, 7, 8

[2] N. Bodla, B. Singh, R. Chellappa, and L. S. Davis. Improv-
ing object detection with one line of code. In ICCV, 2017. 7,
8

[3] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos. A uniﬁed
multi-scale deep convolutional neural network for fast object
detection. In ECCV, pages 354–370, 2016. 1, 3

[4] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille. Semantic image segmentation with deep convolu-
tional nets and fully connected crfs. In ICLR, 2015. 4
[5] J. Dai, Y. Li, K. He, and J. Sun. R-FCN: object detection via
region-based fully convolutional networks. In NIPS, pages
379–387, 2016. 1, 3, 6, 7, 8

[6] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei.
Deformable convolutional networks. In ICCV, 2017. 7, 8
[7] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable
object detection using deep neural networks. In CVPR, pages
2155–2162, 2014. 4

[8] M. Everingham, L. J. V. Gool, C. K. I. Williams, J. M. Winn,
and A. Zisserman. The pascal visual object classes (VOC)
challenge. IJCV, 88(2):303–338, 2010. 1, 3

[9] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman.
The Leaderboard of the PASCAL
Visual Object Classes Challenge 2012 (VOC2012). http:
//host.robots.ox.ac.uk:8080/leaderboard/
displaylb.php?challengeid=11&compid=4.
Online; accessed 1 October 2017. 8

[10] M. Everingham, L. Van Gool, C. K.

I. Williams,
J. Winn, and A. Zisserman. The PASCAL Visual Ob-
ject Classes Challenge 2007 (VOC2007) Results. http:
//www.pascal-network.org/challenges/VOC/
voc2007/workshop/index.html. Online; accessed
1 October 2017. 2

[11] M. Everingham, L. Van Gool, C. K.

I. Williams,
J. Winn, and A. Zisserman. The PASCAL Visual Ob-
ject Classes Challenge 2012 (VOC2012) Results. http:
//www.pascal-network.org/challenges/VOC/
voc2012/workshop/index.html. Online; accessed
1 October 2017. 2, 3

[12] P. F. Felzenszwalb, R. B. Girshick, D. A. McAllester, and
D. Ramanan. Object detection with discriminatively trained
part-based models. TPAMI, 32(9):1627–1645, 2010. 3
[13] C. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg.
CoRR,

DSSD : Deconvolutional single shot detector.
abs/1701.06659, 2017. 3, 5, 6, 7

[14] S. Gidaris and N. Komodakis. Object detection via a multi-
region and semantic segmentation-aware CNN model.
In
ICCV, pages 1134–1142, 2015. 3, 6

[17] X. Glorot and Y. Bengio. Understanding the difﬁculty of
In AISTATS,

training deep feedforward neural networks.
pages 249–256, 2010. 5

[18] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling
in deep convolutional networks for visual recognition.
In
ECCV, pages 346–361, 2014. 3

[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, pages 770–778, 2016. 3, 4,
7, 8

[20] A. G. Howard.

Some improvements on deep convolu-
tional neural network based image classiﬁcation. CoRR,
abs/1312.5402, 2013. 4

[21] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,
A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, and
K. Murphy. Speed/accuracy trade-offs for modern convolu-
tional object detectors. In CVPR, 2017. 5, 7, 8

[22] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, pages 448–456, 2015. 4

[23] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. B.
Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. In ACMMM,
pages 675–678, 2014. 5

[24] T. Kong, F. Sun, A. Yao, H. Liu, M. Lu, and Y. Chen. RON:
reverse connection with objectness prior networks for object
detection. In CVPR, 2017. 1, 3, 5, 6, 7, 8

[25] T. Kong, A. Yao, Y. Chen, and F. Sun. Hypernet: Towards ac-
curate region proposal generation and joint object detection.
In CVPR, pages 845–853, 2016. 3, 6

[26] H. Lee, S. Eum, and H. Kwon. ME R-CNN: multi-expert

region-based CNN for object detection. In ICCV, 2017. 3

[27] T. Lin, P. Doll´ar, R. B. Girshick, K. He, B. Hariharan, and
S. J. Belongie. Feature pyramid networks for object detec-
tion. In CVPR, 2017. 1, 3, 7

[28] T. Lin, P. Goyal, R. B. Girshick, K. He, and P. Doll´ar. Focal
loss for dense object detection. In ICCV, 2017. 1, 3, 7, 8
[29] T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft COCO: com-
mon objects in context. In ECCV, pages 740–755, 2014. 1,
2, 3, 8

[30] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed,
C. Fu, and A. C. Berg. SSD: single shot multibox detector.
In ECCV, pages 21–37, 2016. 1, 3, 4, 6, 7, 8

[31] W. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking

wider to see better. In ICLR workshop, 2016. 4

[32] P. H. O. Pinheiro, R. Collobert, and P. Doll´ar. Learning to
segment object candidates. In NIPS, pages 1990–1998, 2015.
3

[33] P. O. Pinheiro, T. Lin, R. Collobert, and P. Doll´ar. Learning

to reﬁne object segments. In ECCV, pages 75–91, 2016. 3

[34] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi.
You only look once: Uniﬁed, real-time object detection. In
CVPR, pages 779–788, 2016. 3, 6

[15] R. B. Girshick. Fast R-CNN. In ICCV, pages 1440–1448,

[35] J. Redmon and A. Farhadi. YOLO9000: better, faster,

2015. 1, 3, 5, 6, 7

stronger. CoRR, abs/1612.08242, 2016. 1, 3, 6, 7

[16] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich
feature hierarchies for accurate object detection and semantic
segmentation. In CVPR, pages 580–587, 2014. 3

[36] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN:
towards real-time object detection with region proposal net-
works. TPAMI, 39(6):1137–1149, 2017. 1, 3, 6, 7, 8

[55] C. L. Zitnick and P. Doll´ar. Edge boxes: Locating object

proposals from edges. In ECCV, pages 391–405, 2014. 3

[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein,
A. C. Berg, and F. Li. Imagenet large scale visual recognition
challenge. IJCV, 115(3):211–252, 2015. 3, 4, 5

[38] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,
and Y. LeCun. Overfeat: Integrated recognition, localization
and detection using convolutional networks. In ICLR, 2014.
3

[39] Z. Shen, Z. Liu, J. Li, Y. Jiang, Y. Chen, and X. Xue. DSOD:
learning deeply supervised object detectors from scratch. In
ICCV, 2017. 3, 6, 8

[40] A. Shrivastava and A. Gupta. Contextual priming and feed-

back for faster R-CNN. In ECCV, pages 330–348, 2016. 3

[41] A. Shrivastava, A. Gupta, and R. B. Girshick. Training
region-based object detectors with online hard example min-
ing. In CVPR, pages 761–769, 2016. 1, 3, 6, 7, 8

[42] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Be-
yond skip connections: Top-down modulation for object de-
tection. CoRR, abs/1612.06851, 2016. 3, 7, 8

[43] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 3, 4

[44] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI, pages 4278–4284, 2017.
4, 7

[45] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, pages 1–9, 2015.
6

[46] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and
A. W. M. Smeulders. Selective search for object recognition.
IJCV, 104(2):154–171, 2013. 3

[47] P. A. Viola and M. J. Jones. Rapid object detection using a
In CVPR, pages 511–

boosted cascade of simple features.
518, 2001. 3

[48] X. Wang, A. Shrivastava, and A. Gupta. A-fast-rcnn: Hard
positive generation via adversary for object detection.
In
CVPR, 2017. 3

[49] S. Xie, R. B. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggre-
gated residual transformations for deep neural networks. In
CVPR, 2017. 4, 8

[50] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang. Gated
In ECCV, pages

bi-directional CNN for object detection.
354–369, 2016. 3

[51] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li. De-
tecting face with densely connected face proposal network.
In CCBR, pages 3–12, 2017. 4

[52] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li.
Faceboxes: A CPU real-time face detector with high accu-
racy. In IJCB, 2017. 4

[53] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li.
In ICCV,

S3FD: Single shot scale-invariant face detector.
2017. 1, 3, 4

[54] Y. Zhu, C. Zhao, J. Wang, X. Zhao, Y. Wu, and H. Lu. Cou-
plenet: Coupling global structure with local parts for object
detection. In ICCV, 2017. 3, 5, 6, 7

of ReﬁneDet for small objects, especially for the chairs and
tables. Increasing the input size (e.g., from 320 × 320 to
512 × 512) can improve the performance for small objects
, but it is only a temporary solution. Large input will be a
burden on running speed in inference. Therefore, detecting
small objects is still a challenge task and needs to be further
studied.

7. Complete Object Detection Results

We show the complete object detection results of the
proposed ReﬁneDet method on the PASCAL VOC 2007
test set, PASCAL VOC 2012 test set and MS COCO
test-dev set in Table 5, Table 6 and Table 7, respec-
tively. Among the results of all published methods, our Re-
ﬁneDet achieves the best performance on these three detec-
tion datasets, i.e., 85.8% mAP on the PASCAL VOC 2007
test set, 86.8% mAP on the PASCAL VOC 2012 test
set and 41.8% AP on the MS COCO test-dev set.

8. Qualitative Results

We show some qualitative results on the PASCAL VOC
2007 test set, the PASCAL VOC 2012 test set and the
MS COCO test-dev in Figure 3, Figure 4, and Figure 5,
respectively. We only display the detected bounding boxes
with the score larger than 0.6. Different colors of the bound-
ing boxes indicate different object categories. Our method
works well with the occlusions, truncations, inter-class in-
terference and clustered background.

9. Detection Analysis on PASCAL VOC 2007

We use the detection analysis tool8 to understand the
performance of two ReﬁneDet models (i.e., ReﬁneDet320
and ReﬁneDet512) clearly. Figure 6 shows that ReﬁneDet
can detect various object categories with high quality (large
white area). The majority of its conﬁdent detections are
correct. The recall is around 95%-98%, and is much higher
with “weak” (0.1 jaccard overlap) criteria. Compared to
SSD, ReﬁneDet reduces the false positive errors at all as-
pects: (1) ReﬁneDet has less localization error (Loc), indi-
cating that ReﬁneDet can localize objects better because it
uses two-step cascade to regress the objects. (2) ReﬁneDet
has less confusion with background (BG), due to the neg-
ative anchor ﬁltering mechanism in the anchor reﬁnement
module (ARM). (3) ReﬁneDet has less confusion with sim-
ilar categories (Sim), beneﬁting from using two-stage fea-
tures to describe the objects, i.e., the features in the ARM
focus on the binary classiﬁcation (being an object or not),
while the features in the object detection module (ODM) fo-
cus on the multi-class classiﬁcation (background or object
classes).

Figure 7 demonstrates that ReﬁneDet is robust to dif-
ferent object sizes and aspect ratios. This is not surprising
because the object bounding boxes are obtained by the two-
step cascade regression, i.e., the ARM diversiﬁes the default
scales and aspect ratios of anchor boxes so that the ODM
is able to regress tougher objects (e.g., extra-small, extra-
large, extra-wide and extra-tall). However, as shown in Fig-
ure 7, there is still much room to improve the performance

8http://web.engr.illinois.edu/˜dhoiem/projects/

detectionAnalysis/

Table 5: Object detection results on the PASCAL VOC 2007 test set. All models use VGG-16 as the backbone network.

Data
07+12
07+12
07+12
07+12

Method
mAP aero bike bird boat bottle bus car
ReﬁneDet320
80.0 83.9 85.4 81.4 75.5 60.2 86.4 88.1 89.1 62.7 83.9 77.0 85.4 87.1 86.7
81.8 88.7 87.0 83.2 76.5 68.0 88.5 88.7 89.2 66.5 87.9 75.0 86.8 89.2 87.8
ReﬁneDet512
ReﬁneDet320+
83.1 89.5 87.9 84.9 79.7 70.0 87.5 89.1 89.8 69.8 87.1 76.4 86.6 88.6 88.4
83.8 88.5 89.1 85.5 79.8 72.4 89.5 89.5 89.9 69.9 88.9 75.9 87.4 89.6 89.0
ReﬁneDet512+
ReﬁneDet320 COCO+07+12 84.0 88.9 88.4 86.2 81.5 71.7 88.4 89.4 89.0 71.0 87.0 80.1 88.5 90.2 88.4
ReﬁneDet512 COCO+07+12 85.2 90.0 89.2 87.9 83.1 78.5 90.0 89.9 89.7 74.7 89.8 79.5 88.7 89.9 89.2
ReﬁneDet320+ COCO+07+12 85.6 90.2 89.0 87.6 84.6 78.0 89.4 89.7 89.9 74.7 89.8 80.5 89.0 89.7 89.6
ReﬁneDet512+ COCO+07+12 85.8 90.4 89.6 88.2 84.9 78.3 89.8 89.9 90.0 75.9 90.0 80.0 89.8 90.3 89.6

cat chair cow table dog horse mbike person plant sheep sofa train tv
82.6 55.3 82.7 78.5 88.1 79.4
84.7 56.2 83.2 78.7 88.1 82.3
85.3 62.4 83.7 82.3 89.0 83.1
86.2 63.9 86.2 81.0 88.6 84.4
86.7 61.2 85.2 83.8 89.1 85.5
87.8 63.1 86.4 82.3 89.5 84.7
87.8 65.5 87.9 84.2 88.6 86.3
88.3 66.2 87.8 83.5 89.3 85.2

Table 6: Object detection results on the PASCAL VOC 2012 test set. All models use VGG-16 as the backbone network.

Data
07++12
07++12
07++12
07++12

mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv
Method
85.7 53.3 84.3 73.1 87.4 73.9
78.1 90.4 84.1 79.8 66.8 56.1 83.1 82.7 90.7 61.7 82.4 63.8 89.4 86.9 85.9
ReﬁneDet320
87.8 58.0 86.3 72.5 88.7 76.6
80.1 90.2 86.8 81.8 68.0 65.6 84.9 85.0 92.2 62.0 84.4 64.9 90.6 88.3 87.2
ReﬁneDet512
89.4 62.0 88.5 75.9 90.0 80.0
82.7 92.0 88.4 84.9 74.0 69.5 86.0 88.0 93.3 67.0 86.2 68.3 92.1 89.7 88.9
ReﬁneDet320+
90.2 64.1 89.8 75.2 90.7 81.1
ReﬁneDet512+
83.5 92.2 89.4 85.0 74.1 70.8 87.0 88.7 94.0 68.6 87.1 68.2 92.5 90.8 89.4
89.4 59.6 87.9 78.1 91.1 80.0
ReﬁneDet320 COCO+07++12 82.7 93.1 88.2 83.6 74.4 65.1 87.1 87.1 93.7 67.4 86.1 69.4 91.5 90.6 91.4
91.4 66.0 91.2 75.4 91.8 83.0
ReﬁneDet512 COCO+07++12 85.0 94.0 90.0 86.9 76.9 74.1 89.7 89.8 94.2 69.7 90.0 68.5 92.6 92.8 91.5
ReﬁneDet320+ COCO+07++12 86.0 94.2 90.2 87.7 80.4 74.9 90.0 91.7 94.9 71.9 89.8 71.7 93.5 91.9 92.4
91.9 66.5 91.5 79.1 92.8 83.9
92.6 68.8 92.4 78.5 93.6 85.2
ReﬁneDet512+ COCO+07++12 86.8 94.7 91.5 88.8 80.4 77.6 90.4 92.3 95.6 72.5 91.6 69.9 93.9 93.5 92.4

Table 7: Object detection results on the MS COCO test-dev set.

Method
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+
ReﬁneDet320+
ReﬁneDet512+

Net
VGG-16
VGG-16
ResNet-101
ResNet-101
VGG-16
VGG-16
ResNet-101
ResNet-101

AP
29.4
33.0
32.0
36.4
35.2
37.6
38.6
41.8

AP50
49.2
54.5
51.4
57.5
56.1
58.7
59.9
62.9

AP75
31.3
35.5
34.2
39.5
37.7
40.8
41.7
45.7

APS
10.0
16.3
10.5
16.6
19.5
22.7
21.1
25.6

APM
32.0
36.3
34.7
39.9
37.2
40.3
41.7
45.1

APL
44.4
44.3
50.4
51.4
47.0
48.3
52.3
54.1

AR1
26.2
28.3
28.0
30.6
30.1
31.4
32.2
34.0

AR10
42.2
46.4
44.0
49.0
49.6
52.4
52.9
56.3

AR100
45.8
50.6
47.6
53.0
57.4
61.3
61.1
65.5

ARS
18.7
29.3
20.2
30.0
36.2
41.6
40.2
46.2

ARM
52.1
55.5
53.0
58.2
62.3
65.8
66.2
70.2

ARL
66.0
66.0
69.8
70.3
72.6
75.4
77.1
79.8

Figure 3: Qualitative results of ReﬁneDet512 on the PASCAL VOC 2007 test set (corresponding to 85.2% mAP). VGG-16
is used as the backbone network. The training data is 07+12+COCO.

Figure 4: Qualitative results of ReﬁneDet512 on the PASCAL VOC 2012 test set (corresponding to 85.0% mAP). VGG-16
is used as the backbone network. The training data is 07++12+COCO.

Figure 5: Qualitative results of ReﬁneDet512 on the MS COCO test-dev set (corresponding to 36.4% mAP). ResNet-101
is used as the backbone network. The training data is COCO trainval35k.

Figure 6: Visualization of the performance of ReﬁneDet512 on animals, vehicles, and furniture classes in the VOC 2007
test set. The top row shows the cumulative fraction of detections that are correct (Cor) or false positive due to poor
localization (Loc), confusion with similar categories (Sim), with others (Oth), or with background (BG). The solid red line
reﬂects the change of recall with strong criteria (0.5 jaccard overlap) as the number of detections increases. The dashed red
line is using the “weak” criteria (0.1 jaccard overlap). The bottom row shows the distribution of the top-ranked false positive
types.

Figure 7: Sensitivity and impact of different object characteristics on the VOC 2007 test set. The plot on the left shows the
effects of BBox Area per category, and the right plot shows the effect of Aspect Ratio. Key: BBox Area: XS=extra-small;
S=small; M=medium; L=large; XL =extra-large. Aspect Ratio: XT=extra-tall/narrow; T=tall; M=medium; W=wide; XW
=extra-wide.

8
1
0
2
 
n
a
J
 
3
 
 
]

V
C
.
s
c
[
 
 
3
v
7
9
8
6
0
.
1
1
7
1
:
v
i
X
r
a

Single-Shot Reﬁnement Neural Network for Object Detection

Shifeng Zhang1,2, Longyin Wen3, Xiao Bian3, Zhen Lei1,2, Stan Z. Li1,2
1 CBSR & NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China.
2 University of Chinese Academy of Sciences, Beijing, China.
3 GE Global Research, Niskayuna, NY.
{shifeng.zhang,zlei,szli}@nlpr.ia.ac.cn, {longyin.wen,xiao.bian}@ge.com

Abstract

For object detection,

the two-stage approach (e.g.,
Faster R-CNN) has been achieving the highest accuracy,
whereas the one-stage approach (e.g., SSD) has the ad-
vantage of high efﬁciency. To inherit the merits of both
while overcoming their disadvantages, in this paper, we pro-
pose a novel single-shot based detector, called ReﬁneDet,
that achieves better accuracy than two-stage methods and
maintains comparable efﬁciency of one-stage methods. Re-
ﬁneDet consists of two inter-connected modules, namely,
the anchor reﬁnement module and the object detection mod-
ule. Speciﬁcally, the former aims to (1) ﬁlter out nega-
tive anchors to reduce search space for the classiﬁer, and
(2) coarsely adjust the locations and sizes of anchors to
provide better initialization for the subsequent regressor.
The latter module takes the reﬁned anchors as the input
from the former to further improve the regression and pre-
dict multi-class label. Meanwhile, we design a transfer
connection block to transfer the features in the anchor re-
ﬁnement module to predict locations, sizes and class la-
bels of objects in the object detection module. The multi-
task loss function enables us to train the whole network
in an end-to-end way. Extensive experiments on PASCAL
VOC 2007, PASCAL VOC 2012, and MS COCO demon-
strate that ReﬁneDet achieves state-of-the-art detection ac-
curacy with high efﬁciency. Code is available at https:
//github.com/sfzhang15/RefineDet.

1. Introduction

Object detection has achieved signiﬁcant advances in re-
cent years, with the framework of deep neural networks
(DNN). The current DNN detectors of state-of-the-art can
be divided into two categories: (1) the two-stage approach,
including [3, 15, 36, 41], and (2) the one-stage approach,
including [30, 35]. In the two-stage approach, a sparse set
of candidate object boxes is ﬁrst generated, and then they
are further classiﬁed and regressed. The two-stage meth-

ods have been achieving top performances on several chal-
lenging benchmarks, including PASCAL VOC [8] and MS
COCO [29].

The one-stage approach detects objects by regular and
dense sampling over locations, scales and aspect ratios. The
main advantage of this is its high computational efﬁciency.
However, its detection accuracy is usually behind that of
the two-stage approach, one of the main reasons being due
to the class imbalance problem [28].

Some recent methods in the one-stage approach aim to
address the class imbalance problem, to improve the detec-
tion accuracy. Kong et al. [24] use the objectness prior con-
straint on convolutional feature maps to signiﬁcantly reduce
the search space of objects. Lin et al. [28] address the class
imbalance issue by reshaping the standard cross entropy
loss to focus training on a sparse set of hard examples and
down-weights the loss assigned to well-classiﬁed examples.
Zhang et al. [53] design a max-out labeling mechanism to
reduce false positives resulting from class imbalance.

In our opinion,

the current state-of-the-art two-stage
methods, e.g., Faster R-CNN [36], R-FCN [5], and FPN
[27], have three advantages over the one-stage methods as
follows: (1) using two-stage structure with sampling heuris-
tics to handle class imbalance; (2) using two-step cascade to
regress the object box parameters; (3) using two-stage fea-
tures to describe the objects1.
In this work, we design a
novel object detection framework, called ReﬁneDet, to in-
herit the merits of the two approaches (i.e., one-stage and
two-stage approaches) and overcome their shortcomings. It
improves the architecture of the one-stage approach, by us-
ing two inter-connected modules (see Figure 1), namely, the
anchor 2 reﬁnement module (ARM) and the object detection

1In case of Faster R-CNN, the features (excluding shared features) in
the ﬁrst stage (i.e., RPN) are trained for the binary classiﬁcation (being an
object or not), while the features (excluding shared features) in the sec-
ond stage(i.e., Fast R-CNN) are trained for the multi-class classiﬁcation
(background or object classes).

2We denote the reference bounding box as “anchor box”, which is also
called “anchor” for simplicity, as in [36]. However, in [30], it is called
“default box”.

1

Figure 1: Architecture of ReﬁneDet. For better visualization, we only display the layers used for detection. The celadon
parallelograms denote the reﬁned anchors associated with different feature layers. The stars represent the centers of the
reﬁned anchor boxes, which are not regularly paved on the image.

module (ODM). Speciﬁcally, the ARM is designed to (1)
identify and remove negative anchors to reduce search space
for the classiﬁer, and (2) coarsely adjust the locations and
sizes of anchors to provide better initialization for the subse-
quent regressor. The ODM takes the reﬁned anchors as the
input from the former to further improve the regression and
predict multi-class labels. As shown in Figure 1, these two
inter-connected modules imitate the two-stage structure and
thus inherit the three aforementioned advantages to produce
accurate detection results with high efﬁciency. In addition,
we design a transfer connection block (TCB) to transfer the
features3 in the ARM to predict locations, sizes, and class
labels of objects in the ODM. The multi-task loss function
enables us to train the whole network in an end-to-end way.
Extensive experiments on PASCAL VOC 2007, PAS-
CAL VOC 2012, and MS COCO benchmarks demonstrate
that ReﬁneDet outperforms the state-of-the-art methods.
Speciﬁcally, it achieves 85.8% and 86.8% mAPs on VOC
2007 and 2012, with VGG-16 network. Meanwhile, it out-
performs the previously best published results from both
one-stage and two-stage approaches by achieving 41.8%
AP4 on MS COCO test-dev with ResNet-101. In ad-

3The features in the ARM focus on distinguishing positive anchors
from background. We design the TCB to transfer the features in the ARM
to handle the more challenging tasks in the ODM, i.e., predict accurate
object locations, sizes and multi-class labels.

4Based on the evaluation protocol in MS COCO [29], AP is the sin-

dition, ReﬁneDet is time efﬁcient, i.e., it runs at 40.2 FPS
and 24.1 FPS on a NVIDIA Titan X GPU with the input
sizes 320 × 320 and 512 × 512 in inference.

The main contributions of this work are summarized as
follows.
(1) We introduce a novel one-stage framework
for object detection, composed of two inter-connected mod-
ules, i.e., the ARM and the ODM. This leads to performance
better than the two-stage approach while maintaining high
efﬁciency of the one-stage approach. (2) To ensure the ef-
fectiveness, we design the TCB to transfer the features in
the ARM to handle more challenging tasks, i.e., predict ac-
curate object locations, sizes and class labels, in the ODM.
(3) ReﬁneDet achieves the latest state-of-the-art results on
generic object detection (i.e., PASCAL VOC 2007 [10],
PASCAL VOC 2012 [11] and MS COCO [29]).

2. Related Work

Classical Object Detectors. Early object detection meth-
ods are based on the sliding-window paradigm, which ap-
ply the hand-crafted features and classiﬁers on dense image
grids to ﬁnd objects. As one of the most successful meth-
ods, Viola and Jones [47] use Haar feature and AdaBoost
to train a series of cascaded classiﬁers for face detection,

gle most important metric, which is computed by averaging over all 10
intersection over union (IoU) thresholds (i.e., in the range [0.5:0.95] with
uniform step size 0.05) of 80 categories.

achieving satisfactory accuracy with high efﬁciency. DPM
[12] is another popular method using mixtures of multi-
scale deformable part models to represent highly variable
object classes, maintaining top results on PASCAL VOC [8]
for many years. However, with the arrival of deep convolu-
tional network, the object detection task is quickly dom-
inated by the CNN-based detectors, which can be roughly
divided into two categories, i.e., the two-stage approach and
one-stage approach.
Two-Stage Approach. The two-stage approach consists of
two parts, where the ﬁrst one (e.g., Selective Search [46],
EdgeBoxes [55], DeepMask [32, 33], RPN [36]) generates a
sparse set of candidate object proposals, and the second one
determines the accurate object regions and the correspond-
ing class labels using convolutional networks. Notably, the
two-stage approach (e.g., R-CNN [16], SPPnet [18], Fast R-
CNN [15] to Faster R-CNN [36]) achieves dominated per-
formance on several challenging datasets (e.g., PASCAL
VOC 2012 [11] and MS COCO [29]). After that, numer-
ous effective techniques are proposed to further improve the
performance, such as architecture diagram [5, 26, 54], train-
ing strategy [41, 48], contextual reasoning [1, 14, 40, 50]
and multiple layers exploiting [3, 25, 27, 42].
One-Stage Approach. Considering the high efﬁciency, the
one-stage approach attracts much more attention recently.
Sermanet et al. [38] present the OverFeat method for clas-
siﬁcation, localization and detection based on deep Con-
vNets, which is trained end-to-end, from raw pixels to ul-
timate categories. Redmon et al. [34] use a single feed-
forward convolutional network to directly predict object
classes and locations, called YOLO, which is extremely
fast. After that, YOLOv2 [35] is proposed to improve
YOLO in several aspects, i.e., add batch normalization on
all convolution layers, use high resolution classiﬁer, use
convolution layers with anchor boxes to predict bounding
boxes instead of the fully connected layers, etc. Liu et al.
[30] propose the SSD method, which spreads out anchors
of different scales to multiple layers within a ConvNet and
enforces each layer to focus on predicting objects of a cer-
tain scale. DSSD [13] introduces additional context into
SSD via deconvolution to improve the accuracy. DSOD
[39] designs an efﬁcient framework and a set of principles to
learn object detectors from scratch, following the network
structure of SSD. To improve the accuracy, some one-stage
methods [24, 28, 53] aim to address the extreme class im-
balance problem by re-designing the loss function or clas-
siﬁcation strategies. Although the one-stage detectors have
made good progress, their accuracy still trails that of two-
stage methods.

3. Network Architecture

Refer to the overall network architecture shown in Fig-
ure 1. Similar to SSD [30], ReﬁneDet is based on a feed-

forward convolutional network that produces a ﬁxed num-
ber of bounding boxes and the scores indicating the pres-
ence of different classes of objects in those boxes, followed
by the non-maximum suppression to produce the ﬁnal re-
sult. ReﬁneDet is formed by two inter-connected modules,
i.e., the ARM and the ODM. The ARM aims to remove neg-
ative anchors so as to reduce search space for the classiﬁer
and also coarsely adjust the locations and sizes of anchors
to provide better initialization for the subsequent regressor,
whereas ODM aims to regress accurate object locations and
predict multi-class labels based on the reﬁned anchors. The
ARM is constructed by removing the classiﬁcation layers
and adding some auxiliary structures of two base networks
(i.e., VGG-16 [43] and ResNet-101 [19] pretrained on Im-
ageNet [37]) to meet our needs. The ODM is composed of
the outputs of TCBs followed by the prediction layers (i.e.,
the convolution layers with 3 × 3 kernel size), which gener-
ates the scores for object classes and shape offsets relative to
the reﬁned anchor box coordinates. The following explain
three core components in ReﬁneDet, i.e., (1) transfer con-
nection block (TCB), converting the features from the ARM
to the ODM for detection; (2) two-step cascaded regression,
accurately regressing the locations and sizes of objects; (3)
negative anchor ﬁltering, early rejecting well-classiﬁed neg-
ative anchors and mitigate the imbalance issue.

Transfer Connection Block. To link between the ARM
and ODM, we introduce the TCBs to convert features of dif-
ferent layers from the ARM, into the form required by the
ODM, so that the ODM can share features from the ARM.
Notably, from the ARM, we only use the TCBs on the fea-
ture maps associated with anchors. Another function of the
TCBs is to integrate large-scale context [13, 27] by adding
the high-level features to the transferred features to improve
detection accuracy. To match the dimensions between them,
we use the deconvolution operation to enlarge the high-level
feature maps and sum them in the element-wise way. Then,
we add a convolution layer after the summation to ensure
the discriminability of features for detection. The architec-
ture of the TCB is shown in Figure 2.

Two-Step Cascaded Regression. Current one-stage meth-
ods [13, 24, 30] rely on one-step regression based on various
feature layers with different scales to predict the locations
and sizes of objects, which is rather inaccurate in some chal-
lenging scenarios, especially for the small objects. To that
end, we present a two-step cascaded regression strategy to
regress the locations and sizes of objects. That is, we use
the ARM to ﬁrst adjust the locations and sizes of anchors to
provide better initialization for the regression in the ODM.
Speciﬁcally, we associate n anchor boxes with each regu-
larly divided cell on the feature map. The initial position of
each anchor box relative to its corresponding cell is ﬁxed.
At each feature map cell, we predict four offsets of the re-
ﬁned anchor boxes relative to the original tiled anchors and

adapt to variations of objects. That is, we randomly ex-
pand and crop the original training images with additional
random photometric distortion [20] and ﬂipping to generate
the training samples. Please refer to [30] for more details.
Backbone Network. We use VGG-16 [43] and ResNet-101
[19] as the backbone networks in our ReﬁneDet, which are
pretrained on the ILSVRC CLS-LOC dataset [37]. Notably,
ReﬁneDet can also work on other pretrained networks, such
as Inception V2 [22], Inception ResNet [44], and ResNeXt-
101 [49]. Similar to DeepLab-LargeFOV [4], we convert
fc6 and fc7 of VGG-16 to convolution layers conv fc6 and
conv fc7 via subsampling parameters. Since conv4 3 and
conv5 3 have different feature scales compared to other lay-
ers, we use L2 normalization [31] to scale the feature norms
in conv4 3 and conv5 3 to 10 and 8, then learn the scales
during back propagation. Meanwhile, to capture high-level
information and drive object detection at multiple scales,
we also add two extra convolution layers (i.e., conv6 1 and
conv6 2) to the end of the truncated VGG-16 and one extra
residual block (i.e., res6) to the end of the truncated ResNet-
101, respectively.
Anchors Design and Matching. To handle different scales
of objects, we select four feature layers with the total stride
sizes 8, 16, 32, and 64 pixels for both VGG-16 and ResNet-
1015, associated with several different scales of anchors for
prediction. Each feature layer is associated with one spe-
ciﬁc scale of anchors (i.e., the scale is 4 times of the to-
tal stride size of the corresponding layer) and three aspect
ratios (i.e., 0.5, 1.0, and 2.0). We follow the design of
anchor scales over different layers in [53], which ensures
that different scales of anchors have the same tiling den-
sity [51, 52] on the image. Meanwhile, during the train-
ing phase, we determine the correspondence between the
anchors and ground truth boxes based on the jaccard over-
lap [7], and train the whole network end-to-end accordingly.
Speciﬁcally, we ﬁrst match each ground truth to the anchor
box with the best overlap score, and then match the anchor
boxes to any ground truth with overlap higher than 0.5.
Hard Negative Mining. After matching step, most of the
anchor boxes are negatives, even for the ODM, where some
easy negative anchors are rejected by the ARM. Similar
to SSD [30], we use hard negative mining to mitigate the
extreme foreground-background class imbalance, i.e., we
select some negative anchor boxes with top loss values to
make the ratio between the negatives and positives below
3 : 1, instead of using all negative anchors or randomly se-
lecting the negative anchors in training.
Loss Function. The loss function for ReﬁneDet consists of
two parts, i.e., the loss in the ARM and the loss in the ODM.

5For the VGG-16 base network, the conv4 3, conv5 3, conv fc7, and
conv6 2 feature layers are used to predict the locations, sizes and con-
ﬁdences of objects. While for the ResNet-101 base network, res3b3,
res4b22, res5c, and res6 are used for prediction.

Figure 2: The overview of the transfer connection block.

two conﬁdence scores indicating the presence of foreground
objects in those boxes. Thus, we can yield n reﬁned anchor
boxes at each feature map cell.

After obtaining the reﬁned anchor boxes, we pass them
to the corresponding feature maps in the ODM to further
generate object categories and accurate object locations and
sizes, as shown in Figure 1. The corresponding feature
maps in the ARM and the ODM have the same dimension.
We calculate c class scores and the four accurate offsets of
objects relative to the reﬁned anchor boxes, yielding c + 4
outputs for each reﬁned anchor boxes to complete the de-
tection task. This process is similar to the default boxes
used in SSD [30]. However, in contrast to SSD [30] di-
rectly uses the regularly tiled default boxes for detection,
ReﬁneDet uses two-step strategy, i.e., the ARM generates
the reﬁned anchor boxes, and the ODM takes the reﬁned
anchor boxes as input for further detection, leading to more
accurate detection results, especially for the small objects.
Negative Anchor Filtering. To early reject well-classiﬁed
negative anchors and mitigate the imbalance issue, we de-
sign a negative anchor ﬁltering mechanism. Speciﬁcally, in
training phase, for a reﬁned anchor box, if its negative con-
ﬁdence is larger than a preset threshold θ (i.e., set θ = 0.99
empirically), we will discard it in training the ODM. That is,
we only pass the reﬁned hard negative anchor boxes and re-
ﬁned positive anchor boxes to train the ODM. Meanwhile,
in the inference phase, if a reﬁned anchor box is assigned
with a negative conﬁdence larger than θ, it will be discarded
in the ODM for detection.

4. Training and Inference

Data Augmentation. We use several data augmentation
strategies presented in [30] to construct a robust model to

Nodm

For the ARM, we assign a binary class label (of being an
object or not) to each anchor and regress its location and
size simultaneously to get the reﬁned anchor. After that, we
pass the reﬁned anchors with the negative conﬁdence less
than the threshold to the ODM to further predict object cat-
egories and accurate object locations and sizes. With these
deﬁnitions, we deﬁne the loss function as:
(cid:0) (cid:80)
(cid:0) (cid:80)

L({pi}, {xi}, {ci}, {ti}) = 1
Narm
i )(cid:1) + 1
+ (cid:80)
i ≥ 1]Lr(xi, g∗
i )(cid:1)
+ (cid:80)
i ≥ 1]Lr(ti, g∗

i Lb(pi, [l∗
i ≥ 1])
i Lm(ci, l∗
i )

i[l∗
i[l∗

i ≥ 1]) = 0 and Lr(xi, g∗
i ) = 0 and Lr(ti, g∗

(1)
where i is the index of anchor in a mini-batch, l∗
is the
i
ground truth class label of anchor i, g∗
i is the ground truth
location and size of anchor i. pi and xi are the predicted
conﬁdence of the anchor i being an object and reﬁned co-
ordinates of the anchor i in the ARM. ci and ti are the
predicted object class and coordinates of the bounding box
in the ODM. Narm and Nodm are the numbers of positive
anchors in the ARM and ODM, respectively. The binary
classiﬁcation loss Lb is the cross-entropy/log loss over two
classes (object vs. not object), and the multi-class classiﬁ-
cation loss Lm is the softmax loss over multiple classes con-
ﬁdences. Similar to Fast R-CNN [15], we use the smooth
L1 loss as the regression loss Lr. The Iverson bracket indi-
cator function [l∗
i ≥ 1] outputs 1 when the condition is true,
i.e., l∗
i ≥ 1 (the anchor is not the negative), and 0 other-
wise. Hence [l∗
i ≥ 1]Lr indicates that the regression loss is
ignored for negative anchors. Notably, if Narm = 0, we set
Lb(pi, [l∗
i ) = 0; and if Nodm = 0,
we set Lm(ci, l∗
i ) = 0 accordingly.
Optimization. As mentioned above, the backbone network
(e.g., VGG-16 and ResNet-101) in our ReﬁneDet method is
pretrained on the ILSVRC CLS-LOC dataset [37]. We use
the “xavier” method [17] to randomly initialize the parame-
ters in the two extra added convolution layers (i.e., conv6 1
and conv6 2) of VGG-16 based ReﬁneDet, and draw the pa-
rameters from a zero-mean Gaussian distribution with stan-
dard deviation 0.01 for the extra residual block (i.e., res6) of
ResNet-101 based ReﬁneDet. We set the default batch size
to 32 in training. Then, the whole network is ﬁne-tuned us-
ing SGD with 0.9 momentum and 0.0005 weight decay. We
set the initial learning rate to 10−3, and use slightly differ-
ent learning rate decay policy for different dataset, which
will be described in details later.
Inference. At inference phase, the ARM ﬁrst ﬁlters out the
regularly tiled anchors with the negative conﬁdence scores
larger than the threshold θ, and then reﬁnes the locations
and sizes of remaining anchors. After that, the ODM takes
over these reﬁned anchors, and outputs top 400 high con-
ﬁdent detections per image. Finally, we apply the non-
maximum suppression with jaccard overlap of 0.45 per
class and retain the top 200 high conﬁdent detections per
image to produce the ﬁnal detection results.

5. Experiments

Experiments are conducted on three datasets: PASCAL
VOC 2007, PASCAL VOC 2012 and MS COCO. The PAS-
CAL VOC and MS COCO datasets include 20 and 80 ob-
ject classes, respectively. The classes in PASCAL VOC are
the subset of that in MS COCO. We implement ReﬁneDet
in Caffe [23]. All the training and testing codes and the
trained models are available at https://github.com/
sfzhang15/RefineDet.

5.1. PASCAL VOC 2007

All models are trained on the VOC 2007 and VOC 2012
trainval sets, and tested on the VOC 2007 test set. We
set the learning rate to 10−3 for the ﬁrst 80k iterations, and
decay it to 10−4 and 10−5 for training another 20k and 20k
iterations, respectively. We use the default batch size 32 in
training, and only use VGG-16 as the backbone network for
all the experiments on the PASCAL VOC dataset, including
VOC 2007 and VOC 2012.

We compare ReﬁneDet6 with the state-of-the-art detec-
tors in Table 1. With low dimension input (i.e., 320 × 320),
ReﬁneDet produces 80.0% mAP without bells and whis-
tles, which is the ﬁrst method achieving above 80% mAP
with such small input images, much better than several
modern objectors. By using larger input size 512 × 512,
ReﬁneDet achieves 81.8% mAP, surpassing all one-stage
methods, e.g., RON384 [24], SSD513 [13], DSSD513 [13],
etc. Comparing to the two-stage methods, ReﬁneDet512
performs better than most of them except CoupleNet [54],
which is based on ResNet-101 and uses larger input size
(i.e., ∼ 1000 × 600) than our ReﬁneDet512. As pointed
out in [21], the input size signiﬁcantly inﬂuences detection
accuracy. The reason is that high resolution inputs make
the detectors “seeing” small objects clearly to increase suc-
cessful detections. To reduce the impact of input size for a
fair comparison, we use the multi-scale testing strategy to
evaluate ReﬁneDet, achieving 83.1% (ReﬁneDet320+) and
83.8% (ReﬁneDet512+) mAPs, which are much better than
the state-of-the-art methods.

5.1.1 Run Time Performance

We present the inference speed of ReﬁneDet and the state-
of-the-art methods in the ﬁfth column of Table 1. The speed
is evaluated with batch size 1 on a machine with NVIDIA
Titan X, CUDA 8.0 and cuDNN v6. As shown in Table 1,
we ﬁnd that ReﬁneDet processes an image in 24.8ms (40.3
FPS) and 41.5ms (24.1 FPS) with input sizes 320 × 320
and 512 × 512, respectively. To the best of our knowledge,

6Due to the shortage of computational resources, we only train Re-
ﬁneDet with two kinds of input size, i.e., 320 × 320 and 512 × 512. We
believe the accuracy of ReﬁneDet can be further improved using larger
input images.

Table 1: Detection results on PASCAL VOC dataset. For VOC 2007, all methods are trained on VOC 2007 and VOC 2012
trainval sets and tested on VOC 2007 test set. For VOC 2012, all methods are trained on VOC 2007 and VOC 2012
trainval sets plus VOC 2007 test set, and tested on VOC 2012 test set. Bold fonts indicate the best mAP.

Method

Backbone

Input size

#Boxes

FPS

mAP (%)

VOC 2007

VOC 2012

two-stage:
Fast R-CNN[15]
Faster R-CNN[36]
OHEM[41]
HyperNet[25]
Faster R-CNN[36]
ION[1]
MR-CNN[14]
R-FCN[5]
CoupleNet[54]

one-stage:
YOLO[34]
RON384[24]
SSD321[13]
SSD300∗[30]
DSOD300[39]
YOLOv2[35]
DSSD321[13]
SSD512∗[30]
SSD513[13]
DSSD513[13]
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+

VGG-16
VGG-16
VGG-16
VGG-16
ResNet-101
VGG-16
VGG-16
ResNet-101
ResNet-101

GoogleNet [45]
VGG-16
ResNet-101
VGG-16
DS/64-192-48-1
Darknet-19
ResNet-101
VGG-16
ResNet-101
ResNet-101
VGG-16
VGG-16
VGG-16
VGG-16

∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600

448 × 448
384 × 384
321 × 321
300 × 300
300 × 300
544 × 544
321 × 321
512 × 512
513 × 513
513 × 513
320 × 320
512 × 512
-
-

∼ 2000
300
300
100
300
4000
250
300
300

98
30600
17080
8732
8732
845
17080
24564
43688
43688
6375
16320
-
-

0.5
7
7
0.88
2.4
1.25
0.03
9
8.2

45
15
11.2
46
17.4
40
9.5
19
6.8
5.5
40.3
24.1
-
-

70.0
73.2
74.6
76.3
76.4
76.5
78.2
80.5
82.7

63.4
75.4
77.1
77.2
77.7
78.6
78.6
79.8
80.6
81.5
80.0
81.8
83.1
83.8

68.4
70.4
71.9
71.4
73.8
76.4
73.9
77.6
80.4

57.9
73.0
75.4
75.8
76.3
73.4
76.3
78.5
79.4
80.0
78.1
80.1
82.7
83.5

ReﬁneDet is the ﬁrst real-time method to achieve detection
accuracy above 80% mAP on PASCAL VOC 2007. Com-
paring to SSD, RON, DSSD and DSOD, ReﬁneDet asso-
ciates fewer anchor boxes on the feature maps (e.g., 24564
anchor boxes in SSD512∗[30] vs. 16320 anchor boxes in
ReﬁneDet512). However, ReﬁneDet still achieves top accu-
racy with high efﬁciency, mainly thanks to the design of two
inter-connected modules, (e.g., two-step regression), which
enables ReﬁneDet to adapt to different scales and aspect ra-
tios of objects. Meanwhile, only YOLO and SSD300∗ are
slightly faster than our ReﬁneDet320, but their accuracy are
16.6% and 2.5% worse than ours. In summary, ReﬁneDet
achieves the best trade-off between accuracy and speed.

5.1.2 Ablation Study

To demonstrate the effectiveness of different components
in ReﬁneDet, we construct four variants and evaluate them
on VOC 2007, shown in Table 3. Speciﬁcally, for a fair
comparison, we use the same parameter settings and input
size (320 × 320) in evaluation. All models are trained on
VOC 2007 and VOC 2012 trainval sets, and tested on
VOC 2007 test set.

Negative Anchor Filtering. To demonstrate the effective-
ness of the negative anchor ﬁltering, we set the conﬁdence

threshold θ of the anchors to be negative to 1.0 in both train-
ing and testing.
In this case, all reﬁned anchors will be
sent to the ODM for detection. Other parts of ReﬁneDet re-
main unchanged. Removing negative anchor ﬁltering leads
to 0.5% drop in mAP (i.e., 80.0% vs. 79.5%). The reason
is that most of these well-classiﬁed negative anchors will be
ﬁltered out during training, which solves the class imbal-
ance issue to some extent.

Two-Step Cascaded Regression. To validate the effective-
ness of the two-step cascaded regression, we redesign the
network structure by directly using the regularly paved an-
chors instead of the reﬁned ones from the ARM (see the
fourth column in Table 3). As shown in Table 3, we ﬁnd that
mAP is reduced from 79.5% to 77.3%. This sharp decline
(i.e., 2.2%) demonstrates that the two-step anchor cascaded
regression signiﬁcantly help promote the performance.

Transfer Connection Block. We construct a network by
cutting the TCBs in ReﬁneDet and redeﬁning the loss func-
tion in the ARM to directly detect multi-class of objects,
just like SSD, to demonstrate the effect of the TCB. The
detection accuracy of the model is presented in the ﬁfth col-
umn in Table 3. We compare the results in the fourth and
ﬁfth columns in Table 3 (77.3% vs. 76.2%) and ﬁnd that
the TCB improves the mAP by 1.1%. The main reason is

Table 2: Detection results on MS COCO test-dev set. Bold fonts indicate the best performance.

Method
two-stage:
Fast R-CNN [15]
Faster R-CNN [36]
OHEM [41]
ION [1]
OHEM++ [41]
R-FCN [5]
CoupleNet [54]
Faster R-CNN by G-RMI [21]
Faster R-CNN+++ [19]
Faster R-CNN w FPN [27]
Faster R-CNN w TDM [42]
Deformable R-FCN [6]
umd det [2]
G-RMI [21]

one-stage:
YOLOv2 [35]
SSD300∗ [30]
RON384++ [24]
SSD321 [13]
DSSD321 [13]
SSD512∗ [30]
SSD513 [13]
DSSD513 [13]
RetinaNet500 [28]
RetinaNet800 [28]∗
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+
ReﬁneDet320+
ReﬁneDet512+

Data

Backbone

AP

AP50

AP75

APS

APM

APL

train
trainval
trainval
train
trainval
trainval
trainval
-
trainval
trainval35k
trainval
trainval
trainval
trainval32k

trainval35k
trainval35k
trainval
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k

VGG-16
VGG-16
VGG-16
VGG-16
VGG-16
ResNet-101
ResNet-101
Inception-ResNet-v2[44]
ResNet-101-C4
ResNet-101-FPN
Inception-ResNet-v2-TDM
Aligned-Inception-ResNet
ResNet-101
Ensemble of Five Models

DarkNet-19[35]
VGG-16
VGG-16
ResNet-101
ResNet-101
VGG-16
ResNet-101
ResNet-101
ResNet-101
ResNet-101-FPN
VGG-16
VGG-16
ResNet-101
ResNet-101
VGG-16
VGG-16
ResNet-101
ResNet-101

19.7
21.9
22.6
23.6
25.5
29.9
34.4
34.7
34.9
36.2
36.8
37.5
40.8
41.6

21.6
25.1
27.4
28.0
28.0
28.8
31.2
33.2
34.4
39.1
29.4
33.0
32.0
36.4
35.2
37.6
38.6
41.8

35.9
42.7
42.5
43.2
45.9
51.9
54.8
55.5
55.7
59.1
57.7
58.0
62.4
61.9

44.0
43.1
49.5
45.4
46.1
48.5
50.4
53.3
53.1
59.1
49.2
54.5
51.4
57.5
56.1
58.7
59.9
62.9

-
-
22.2
23.6
26.1
-
37.2
36.7
37.4
39.0
39.2
40.8
44.9
45.4

19.2
25.8
27.1
29.3
29.2
30.3
33.3
35.2
36.8
42.3
31.3
35.5
34.2
39.5
37.7
40.8
41.7
45.7

-
-
5.0
6.4
7.4
10.8
13.4
13.5
15.6
18.2
16.2
19.4
23.0
23.9

5.0
6.6
-
6.2
7.4
10.9
10.2
13.0
14.7
21.8
10.0
16.3
10.5
16.6
19.5
22.7
21.1
25.6

-
-
23.7
24.1
27.7
32.8
38.1
38.1
38.7
39.0
39.8
40.1
43.4
43.5

22.4
25.9
-
28.3
28.1
31.8
34.5
35.4
38.5
42.7
32.0
36.3
34.7
39.9
37.2
40.3
41.7
45.1

-
-
37.9
38.3
40.3
45.0
50.8
52.0
50.9
48.2
52.1
52.5
53.2
54.9

35.5
41.4
-
49.3
47.6
43.5
49.8
51.1
49.1
50.2
44.4
44.3
50.4
51.4
47.0
48.3
52.3
54.1

∗ This entry reports the single model accuracy of RetinaNet method, trained with scale jitter and for 1.5× longer than RetinaNet500.

Table 3: Effectiveness of various designs. All models are
trained on VOC 2007 and VOC 2012 trainval set and
tested on VOC 2007 test set.

Component
negative anchor ﬁltering?
two-step cascaded regression?
transfer connection block?
mAP (%)

!
!
!
80.0

!
!
79.5

ReﬁneDet320

!
77.3

76.2

that the model can inherit the discriminative features from
the ARM, and integrate large-scale context information to
improve the detection accuracy by using the TCB.

5.2. PASCAL VOC 2012

Following the protocol of VOC 2012, we submit the de-
tection results of ReﬁneDet to the public testing server for
evaluation. We use VOC 2007 trainval set and test
set plus VOC 2012 trainval set (21, 503 images) for

training, and test on VOC 2012 test set (10, 991 images).
We use the default batch size 32 in training. Meanwhile, we
set the learning rate to 10−3 in the ﬁrst 160k iterations, and
decay it to 10−4 and 10−5 for another 40k and 40k itera-
tions.

Table 1 shows the accuracy of the proposed ReﬁneDet al-
gorithm, as well as the state-of-the-art methods. Among the
methods fed with input size 320 × 320, ReﬁneDet320 ob-
tains the top 78.1% mAP, which is even better than most of
those two-stage methods using about 1000 × 600 input size
(e.g., 70.4% mAP of Faster R-CNN [36] and 77.6% mAP
of R-FCN [5]). Using the input size 512 × 512, ReﬁneDet
improves mAP to 80.1%, which is surpassing all one-stage
methods and only slightly lower than CoupleNet [54] (i.e.,
80.4%). CoupleNet uses ResNet-101 as base network with
1000 × 600 input size. To reduce the impact of input size
for a fair comparison, we also use multi-scale testing to
evaluate ReﬁneDet and obtain the state-of-the-art mAPs of
82.7% (ReﬁneDet320+) and 83.5% (ReﬁneDet512+).

Table 4: Detection results on PASCAL VOC dataset. All
models are pre-trained on MS COCO, and ﬁne-tuned on
PASCAL VOC. Bold fonts indicate the best mAP.

mAP (%)
VOC 2007 test VOC 2012 test

Method

Backbone

two-stage:
Faster R-CNN[36]
OHEM++[41]
R-FCN[5]

VGG-16
VGG-16
ResNet-101

VGG-16
VGG-16
VGG-16

one-stage:
SSD300[30]
SSD512[30]
RON384++[24]
DSOD300[39] DS/64-192-48-1
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+

VGG-16
VGG-16
VGG-16
VGG-16

5.3. MS COCO

78.8
-
83.6

81.2
83.2
81.3
81.7
84.0
85.2
85.6
85.8

75.9
80.1
82.0

79.3
82.2
80.7
79.3
82.7
85.0
86.0
86.8

In addition to PASCAL VOC, we also evaluate Re-
ﬁneDet on MS COCO [29]. Unlike PASCAL VOC, the
detection methods using ResNet-101 always achieve bet-
ter performance than those using VGG-16 on MS COCO.
Thus, we also report the results of ResNet-101 based Re-
ﬁneDet. Following the protocol in MS COCO, we use the
trainval35k set [1] for training and evaluate the results
from test-dev evaluation server. We set the batch size to
32 in training7, and train the model with 10−3 learning rate
for the ﬁrst 280k iterations, then 10−4 and 10−5 for another
80k and 40k iterations, respectively.

Table 7 shows the results on MS COCO test-dev set.
ReﬁneDet320 with VGG-16 produces 29.4% AP that is bet-
ter than all other methods based on VGG-16 (e.g., SSD512∗
[30] and OHEM++ [41]). The accuracy of ReﬁneDet can
be improved to 33.0% by using larger input size (i.e.,
512 × 512), which is much better than several modern ob-
ject detectors, e.g., Faster R-CNN [36] and SSD512∗ [30].
Meanwhile, using ResNet-101 can further improve the per-
formance of ReﬁneDet, i.e., ReﬁneDet320 with ResNet-101
achieves 32.0% AP and ReﬁneDet512 achieves 36.4% AP,
exceeding most of the detection methods except Faster R-
CNN w TDM [42], Deformable R-FCN [6], RetinaNet800
[28], umd det [2], and G-RMI [21]. All these methods use a
much bigger input images for both training and testing (i.e.,
1000×600 or 800×800) than our ReﬁneDet (i.e., 320×320
and 512 × 512). Similar to PASCAL VOC, we also report
the multi-scale testing AP results of ReﬁneDet for fair com-
parison in Table 7, i.e., 35.2% (ReﬁneDet320+ with VGG-
16), 37.6% (ReﬁneDet512+ with VGG-16), 38.6% (Re-

7Due to the memory issue, we reduce the batch size to 20 (which is the
largest batch size we can use for training on a machine with 4 NVIDIA
M40 GPUs) to train the ResNet-101 based ReﬁneDet with the input size
512 × 512, and train the model with 10−3 learning rate for the ﬁrst 400k
iterations, then 10−4 and 10−5 for another 80k and 60k iterations.

ﬁneDet320+ with ResNet-101) and 41.8% (ReﬁneDet512+
with ResNet-101). The best performance of ReﬁneDet is
41.8%, which is the state-of-the-art, surpassing all pub-
lished two-stage and one-stage approaches. Although the
second best detector G-RMI [21] ensembles ﬁve Faster R-
CNN models, it still produces 0.2% lower AP than Re-
ﬁneDet using a single model. Comparing to the third and
fourth best detectors, i.e., umd det [2] and RetinaNet800
[28], ReﬁneDet produces 1.0% and 2.7% higher APs. In
addition, the main contribution: focal loss in RetinaNet800,
is complementary to our method. We believe that it can be
used in ReﬁneNet to further improve the performance.

5.4. From MS COCO to PASCAL VOC

We study how the MS COCO dataset help the detec-
tion accuracy on PASCAL VOC. Since the object classes
in PASCAL VOC are the subset of MS COCO, we directly
ﬁne-tune the detection models pretrained on MS COCO via
subsampling the parameters, which achieves 84.0% mAP
(ReﬁneDet320) and 85.2% mAP (ReﬁneDet512) on VOC
2007 test set, and 82.7% mAP (ReﬁneDet320) and 85.0%
mAP (ReﬁneDet512) on VOC 2012 test set, shown in Ta-
ble 4. After using the multi-scale testing, the detection ac-
curacy are promoted to 85.6%, 85.8%, 86.0% and 86.8%,
respectively. As shown in Table 4, using the training data in
MS COCO and PASCAL VOC, our ReﬁneDet obtains the
top mAP scores on both VOC 2007 and VOC 2012. Most
important, our single model ReﬁneNet512+ based on VGG-
16 ranks as the top 5 on the VOC 2012 Leaderboard (see
[9]), which is the best accuracy among all one-stage meth-
ods. Other two-stage methods achieving better results are
based on much deeper networks (e.g., ResNet-101 [19] and
ResNeXt-101 [49]) or using ensemble mechanism.

6. Conclusions

In this paper, we present a single-shot reﬁnement neu-
ral network based detector, which consists of two inter-
connected modules, i.e., the ARM and the ODM. The ARM
aims to ﬁlter out the negative anchors to reduce search space
for the classiﬁer and also coarsely adjust the locations and
sizes of anchors to provide better initialization for the subse-
quent regressor, while the ODM takes the reﬁned anchors as
the input from the former ARM to regress the accurate ob-
ject locations and sizes and predict the corresponding multi-
class labels. The whole network is trained in an end-to-end
fashion with the multi-task loss. We carry out several exper-
iments on PASCAL VOC 2007, PASCAL VOC 2012, and
MS COCO datasets to demonstrate that ReﬁneDet achieves
the state-of-the-art detection accuracy with high efﬁciency.
In the future, we plan to employ ReﬁneDet to detect some
other speciﬁc kinds of objects, e.g., pedestrian, vehicle, and
face, and introduce the attention mechanism in ReﬁneDet to
further improve the performance.

References

[1] S. Bell, C. L. Zitnick, K. Bala, and R. B. Girshick. Inside-
outside net: Detecting objects in context with skip pooling
and recurrent neural networks. In CVPR, pages 2874–2883,
2016. 3, 6, 7, 8

[2] N. Bodla, B. Singh, R. Chellappa, and L. S. Davis. Improv-
ing object detection with one line of code. In ICCV, 2017. 7,
8

[3] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos. A uniﬁed
multi-scale deep convolutional neural network for fast object
detection. In ECCV, pages 354–370, 2016. 1, 3

[4] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille. Semantic image segmentation with deep convolu-
tional nets and fully connected crfs. In ICLR, 2015. 4
[5] J. Dai, Y. Li, K. He, and J. Sun. R-FCN: object detection via
region-based fully convolutional networks. In NIPS, pages
379–387, 2016. 1, 3, 6, 7, 8

[6] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei.
Deformable convolutional networks. In ICCV, 2017. 7, 8
[7] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable
object detection using deep neural networks. In CVPR, pages
2155–2162, 2014. 4

[8] M. Everingham, L. J. V. Gool, C. K. I. Williams, J. M. Winn,
and A. Zisserman. The pascal visual object classes (VOC)
challenge. IJCV, 88(2):303–338, 2010. 1, 3

[9] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman.
The Leaderboard of the PASCAL
Visual Object Classes Challenge 2012 (VOC2012). http:
//host.robots.ox.ac.uk:8080/leaderboard/
displaylb.php?challengeid=11&compid=4.
Online; accessed 1 October 2017. 8

[10] M. Everingham, L. Van Gool, C. K.

I. Williams,
J. Winn, and A. Zisserman. The PASCAL Visual Ob-
ject Classes Challenge 2007 (VOC2007) Results. http:
//www.pascal-network.org/challenges/VOC/
voc2007/workshop/index.html. Online; accessed
1 October 2017. 2

[11] M. Everingham, L. Van Gool, C. K.

I. Williams,
J. Winn, and A. Zisserman. The PASCAL Visual Ob-
ject Classes Challenge 2012 (VOC2012) Results. http:
//www.pascal-network.org/challenges/VOC/
voc2012/workshop/index.html. Online; accessed
1 October 2017. 2, 3

[12] P. F. Felzenszwalb, R. B. Girshick, D. A. McAllester, and
D. Ramanan. Object detection with discriminatively trained
part-based models. TPAMI, 32(9):1627–1645, 2010. 3
[13] C. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg.
CoRR,

DSSD : Deconvolutional single shot detector.
abs/1701.06659, 2017. 3, 5, 6, 7

[14] S. Gidaris and N. Komodakis. Object detection via a multi-
region and semantic segmentation-aware CNN model.
In
ICCV, pages 1134–1142, 2015. 3, 6

[17] X. Glorot and Y. Bengio. Understanding the difﬁculty of
In AISTATS,

training deep feedforward neural networks.
pages 249–256, 2010. 5

[18] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling
in deep convolutional networks for visual recognition.
In
ECCV, pages 346–361, 2014. 3

[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, pages 770–778, 2016. 3, 4,
7, 8

[20] A. G. Howard.

Some improvements on deep convolu-
tional neural network based image classiﬁcation. CoRR,
abs/1312.5402, 2013. 4

[21] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,
A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, and
K. Murphy. Speed/accuracy trade-offs for modern convolu-
tional object detectors. In CVPR, 2017. 5, 7, 8

[22] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, pages 448–456, 2015. 4

[23] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. B.
Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. In ACMMM,
pages 675–678, 2014. 5

[24] T. Kong, F. Sun, A. Yao, H. Liu, M. Lu, and Y. Chen. RON:
reverse connection with objectness prior networks for object
detection. In CVPR, 2017. 1, 3, 5, 6, 7, 8

[25] T. Kong, A. Yao, Y. Chen, and F. Sun. Hypernet: Towards ac-
curate region proposal generation and joint object detection.
In CVPR, pages 845–853, 2016. 3, 6

[26] H. Lee, S. Eum, and H. Kwon. ME R-CNN: multi-expert

region-based CNN for object detection. In ICCV, 2017. 3

[27] T. Lin, P. Doll´ar, R. B. Girshick, K. He, B. Hariharan, and
S. J. Belongie. Feature pyramid networks for object detec-
tion. In CVPR, 2017. 1, 3, 7

[28] T. Lin, P. Goyal, R. B. Girshick, K. He, and P. Doll´ar. Focal
loss for dense object detection. In ICCV, 2017. 1, 3, 7, 8
[29] T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft COCO: com-
mon objects in context. In ECCV, pages 740–755, 2014. 1,
2, 3, 8

[30] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed,
C. Fu, and A. C. Berg. SSD: single shot multibox detector.
In ECCV, pages 21–37, 2016. 1, 3, 4, 6, 7, 8

[31] W. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking

wider to see better. In ICLR workshop, 2016. 4

[32] P. H. O. Pinheiro, R. Collobert, and P. Doll´ar. Learning to
segment object candidates. In NIPS, pages 1990–1998, 2015.
3

[33] P. O. Pinheiro, T. Lin, R. Collobert, and P. Doll´ar. Learning

to reﬁne object segments. In ECCV, pages 75–91, 2016. 3

[34] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi.
You only look once: Uniﬁed, real-time object detection. In
CVPR, pages 779–788, 2016. 3, 6

[15] R. B. Girshick. Fast R-CNN. In ICCV, pages 1440–1448,

[35] J. Redmon and A. Farhadi. YOLO9000: better, faster,

2015. 1, 3, 5, 6, 7

stronger. CoRR, abs/1612.08242, 2016. 1, 3, 6, 7

[16] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich
feature hierarchies for accurate object detection and semantic
segmentation. In CVPR, pages 580–587, 2014. 3

[36] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN:
towards real-time object detection with region proposal net-
works. TPAMI, 39(6):1137–1149, 2017. 1, 3, 6, 7, 8

[55] C. L. Zitnick and P. Doll´ar. Edge boxes: Locating object

proposals from edges. In ECCV, pages 391–405, 2014. 3

[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein,
A. C. Berg, and F. Li. Imagenet large scale visual recognition
challenge. IJCV, 115(3):211–252, 2015. 3, 4, 5

[38] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,
and Y. LeCun. Overfeat: Integrated recognition, localization
and detection using convolutional networks. In ICLR, 2014.
3

[39] Z. Shen, Z. Liu, J. Li, Y. Jiang, Y. Chen, and X. Xue. DSOD:
learning deeply supervised object detectors from scratch. In
ICCV, 2017. 3, 6, 8

[40] A. Shrivastava and A. Gupta. Contextual priming and feed-

back for faster R-CNN. In ECCV, pages 330–348, 2016. 3

[41] A. Shrivastava, A. Gupta, and R. B. Girshick. Training
region-based object detectors with online hard example min-
ing. In CVPR, pages 761–769, 2016. 1, 3, 6, 7, 8

[42] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Be-
yond skip connections: Top-down modulation for object de-
tection. CoRR, abs/1612.06851, 2016. 3, 7, 8

[43] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 3, 4

[44] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI, pages 4278–4284, 2017.
4, 7

[45] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, pages 1–9, 2015.
6

[46] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and
A. W. M. Smeulders. Selective search for object recognition.
IJCV, 104(2):154–171, 2013. 3

[47] P. A. Viola and M. J. Jones. Rapid object detection using a
In CVPR, pages 511–

boosted cascade of simple features.
518, 2001. 3

[48] X. Wang, A. Shrivastava, and A. Gupta. A-fast-rcnn: Hard
positive generation via adversary for object detection.
In
CVPR, 2017. 3

[49] S. Xie, R. B. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggre-
gated residual transformations for deep neural networks. In
CVPR, 2017. 4, 8

[50] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang. Gated
In ECCV, pages

bi-directional CNN for object detection.
354–369, 2016. 3

[51] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li. De-
tecting face with densely connected face proposal network.
In CCBR, pages 3–12, 2017. 4

[52] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li.
Faceboxes: A CPU real-time face detector with high accu-
racy. In IJCB, 2017. 4

[53] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li.
In ICCV,

S3FD: Single shot scale-invariant face detector.
2017. 1, 3, 4

[54] Y. Zhu, C. Zhao, J. Wang, X. Zhao, Y. Wu, and H. Lu. Cou-
plenet: Coupling global structure with local parts for object
detection. In ICCV, 2017. 3, 5, 6, 7

of ReﬁneDet for small objects, especially for the chairs and
tables. Increasing the input size (e.g., from 320 × 320 to
512 × 512) can improve the performance for small objects
, but it is only a temporary solution. Large input will be a
burden on running speed in inference. Therefore, detecting
small objects is still a challenge task and needs to be further
studied.

7. Complete Object Detection Results

We show the complete object detection results of the
proposed ReﬁneDet method on the PASCAL VOC 2007
test set, PASCAL VOC 2012 test set and MS COCO
test-dev set in Table 5, Table 6 and Table 7, respec-
tively. Among the results of all published methods, our Re-
ﬁneDet achieves the best performance on these three detec-
tion datasets, i.e., 85.8% mAP on the PASCAL VOC 2007
test set, 86.8% mAP on the PASCAL VOC 2012 test
set and 41.8% AP on the MS COCO test-dev set.

8. Qualitative Results

We show some qualitative results on the PASCAL VOC
2007 test set, the PASCAL VOC 2012 test set and the
MS COCO test-dev in Figure 3, Figure 4, and Figure 5,
respectively. We only display the detected bounding boxes
with the score larger than 0.6. Different colors of the bound-
ing boxes indicate different object categories. Our method
works well with the occlusions, truncations, inter-class in-
terference and clustered background.

9. Detection Analysis on PASCAL VOC 2007

We use the detection analysis tool8 to understand the
performance of two ReﬁneDet models (i.e., ReﬁneDet320
and ReﬁneDet512) clearly. Figure 6 shows that ReﬁneDet
can detect various object categories with high quality (large
white area). The majority of its conﬁdent detections are
correct. The recall is around 95%-98%, and is much higher
with “weak” (0.1 jaccard overlap) criteria. Compared to
SSD, ReﬁneDet reduces the false positive errors at all as-
pects: (1) ReﬁneDet has less localization error (Loc), indi-
cating that ReﬁneDet can localize objects better because it
uses two-step cascade to regress the objects. (2) ReﬁneDet
has less confusion with background (BG), due to the neg-
ative anchor ﬁltering mechanism in the anchor reﬁnement
module (ARM). (3) ReﬁneDet has less confusion with sim-
ilar categories (Sim), beneﬁting from using two-stage fea-
tures to describe the objects, i.e., the features in the ARM
focus on the binary classiﬁcation (being an object or not),
while the features in the object detection module (ODM) fo-
cus on the multi-class classiﬁcation (background or object
classes).

Figure 7 demonstrates that ReﬁneDet is robust to dif-
ferent object sizes and aspect ratios. This is not surprising
because the object bounding boxes are obtained by the two-
step cascade regression, i.e., the ARM diversiﬁes the default
scales and aspect ratios of anchor boxes so that the ODM
is able to regress tougher objects (e.g., extra-small, extra-
large, extra-wide and extra-tall). However, as shown in Fig-
ure 7, there is still much room to improve the performance

8http://web.engr.illinois.edu/˜dhoiem/projects/

detectionAnalysis/

Table 5: Object detection results on the PASCAL VOC 2007 test set. All models use VGG-16 as the backbone network.

Data
07+12
07+12
07+12
07+12

Method
mAP aero bike bird boat bottle bus car
ReﬁneDet320
80.0 83.9 85.4 81.4 75.5 60.2 86.4 88.1 89.1 62.7 83.9 77.0 85.4 87.1 86.7
81.8 88.7 87.0 83.2 76.5 68.0 88.5 88.7 89.2 66.5 87.9 75.0 86.8 89.2 87.8
ReﬁneDet512
ReﬁneDet320+
83.1 89.5 87.9 84.9 79.7 70.0 87.5 89.1 89.8 69.8 87.1 76.4 86.6 88.6 88.4
83.8 88.5 89.1 85.5 79.8 72.4 89.5 89.5 89.9 69.9 88.9 75.9 87.4 89.6 89.0
ReﬁneDet512+
ReﬁneDet320 COCO+07+12 84.0 88.9 88.4 86.2 81.5 71.7 88.4 89.4 89.0 71.0 87.0 80.1 88.5 90.2 88.4
ReﬁneDet512 COCO+07+12 85.2 90.0 89.2 87.9 83.1 78.5 90.0 89.9 89.7 74.7 89.8 79.5 88.7 89.9 89.2
ReﬁneDet320+ COCO+07+12 85.6 90.2 89.0 87.6 84.6 78.0 89.4 89.7 89.9 74.7 89.8 80.5 89.0 89.7 89.6
ReﬁneDet512+ COCO+07+12 85.8 90.4 89.6 88.2 84.9 78.3 89.8 89.9 90.0 75.9 90.0 80.0 89.8 90.3 89.6

cat chair cow table dog horse mbike person plant sheep sofa train tv
82.6 55.3 82.7 78.5 88.1 79.4
84.7 56.2 83.2 78.7 88.1 82.3
85.3 62.4 83.7 82.3 89.0 83.1
86.2 63.9 86.2 81.0 88.6 84.4
86.7 61.2 85.2 83.8 89.1 85.5
87.8 63.1 86.4 82.3 89.5 84.7
87.8 65.5 87.9 84.2 88.6 86.3
88.3 66.2 87.8 83.5 89.3 85.2

Table 6: Object detection results on the PASCAL VOC 2012 test set. All models use VGG-16 as the backbone network.

Data
07++12
07++12
07++12
07++12

mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv
Method
85.7 53.3 84.3 73.1 87.4 73.9
78.1 90.4 84.1 79.8 66.8 56.1 83.1 82.7 90.7 61.7 82.4 63.8 89.4 86.9 85.9
ReﬁneDet320
87.8 58.0 86.3 72.5 88.7 76.6
80.1 90.2 86.8 81.8 68.0 65.6 84.9 85.0 92.2 62.0 84.4 64.9 90.6 88.3 87.2
ReﬁneDet512
89.4 62.0 88.5 75.9 90.0 80.0
82.7 92.0 88.4 84.9 74.0 69.5 86.0 88.0 93.3 67.0 86.2 68.3 92.1 89.7 88.9
ReﬁneDet320+
90.2 64.1 89.8 75.2 90.7 81.1
ReﬁneDet512+
83.5 92.2 89.4 85.0 74.1 70.8 87.0 88.7 94.0 68.6 87.1 68.2 92.5 90.8 89.4
89.4 59.6 87.9 78.1 91.1 80.0
ReﬁneDet320 COCO+07++12 82.7 93.1 88.2 83.6 74.4 65.1 87.1 87.1 93.7 67.4 86.1 69.4 91.5 90.6 91.4
91.4 66.0 91.2 75.4 91.8 83.0
ReﬁneDet512 COCO+07++12 85.0 94.0 90.0 86.9 76.9 74.1 89.7 89.8 94.2 69.7 90.0 68.5 92.6 92.8 91.5
ReﬁneDet320+ COCO+07++12 86.0 94.2 90.2 87.7 80.4 74.9 90.0 91.7 94.9 71.9 89.8 71.7 93.5 91.9 92.4
91.9 66.5 91.5 79.1 92.8 83.9
92.6 68.8 92.4 78.5 93.6 85.2
ReﬁneDet512+ COCO+07++12 86.8 94.7 91.5 88.8 80.4 77.6 90.4 92.3 95.6 72.5 91.6 69.9 93.9 93.5 92.4

Table 7: Object detection results on the MS COCO test-dev set.

Method
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+
ReﬁneDet320+
ReﬁneDet512+

Net
VGG-16
VGG-16
ResNet-101
ResNet-101
VGG-16
VGG-16
ResNet-101
ResNet-101

AP
29.4
33.0
32.0
36.4
35.2
37.6
38.6
41.8

AP50
49.2
54.5
51.4
57.5
56.1
58.7
59.9
62.9

AP75
31.3
35.5
34.2
39.5
37.7
40.8
41.7
45.7

APS
10.0
16.3
10.5
16.6
19.5
22.7
21.1
25.6

APM
32.0
36.3
34.7
39.9
37.2
40.3
41.7
45.1

APL
44.4
44.3
50.4
51.4
47.0
48.3
52.3
54.1

AR1
26.2
28.3
28.0
30.6
30.1
31.4
32.2
34.0

AR10
42.2
46.4
44.0
49.0
49.6
52.4
52.9
56.3

AR100
45.8
50.6
47.6
53.0
57.4
61.3
61.1
65.5

ARS
18.7
29.3
20.2
30.0
36.2
41.6
40.2
46.2

ARM
52.1
55.5
53.0
58.2
62.3
65.8
66.2
70.2

ARL
66.0
66.0
69.8
70.3
72.6
75.4
77.1
79.8

Figure 3: Qualitative results of ReﬁneDet512 on the PASCAL VOC 2007 test set (corresponding to 85.2% mAP). VGG-16
is used as the backbone network. The training data is 07+12+COCO.

Figure 4: Qualitative results of ReﬁneDet512 on the PASCAL VOC 2012 test set (corresponding to 85.0% mAP). VGG-16
is used as the backbone network. The training data is 07++12+COCO.

Figure 5: Qualitative results of ReﬁneDet512 on the MS COCO test-dev set (corresponding to 36.4% mAP). ResNet-101
is used as the backbone network. The training data is COCO trainval35k.

Figure 6: Visualization of the performance of ReﬁneDet512 on animals, vehicles, and furniture classes in the VOC 2007
test set. The top row shows the cumulative fraction of detections that are correct (Cor) or false positive due to poor
localization (Loc), confusion with similar categories (Sim), with others (Oth), or with background (BG). The solid red line
reﬂects the change of recall with strong criteria (0.5 jaccard overlap) as the number of detections increases. The dashed red
line is using the “weak” criteria (0.1 jaccard overlap). The bottom row shows the distribution of the top-ranked false positive
types.

Figure 7: Sensitivity and impact of different object characteristics on the VOC 2007 test set. The plot on the left shows the
effects of BBox Area per category, and the right plot shows the effect of Aspect Ratio. Key: BBox Area: XS=extra-small;
S=small; M=medium; L=large; XL =extra-large. Aspect Ratio: XT=extra-tall/narrow; T=tall; M=medium; W=wide; XW
=extra-wide.

8
1
0
2
 
n
a
J
 
3
 
 
]

V
C
.
s
c
[
 
 
3
v
7
9
8
6
0
.
1
1
7
1
:
v
i
X
r
a

Single-Shot Reﬁnement Neural Network for Object Detection

Shifeng Zhang1,2, Longyin Wen3, Xiao Bian3, Zhen Lei1,2, Stan Z. Li1,2
1 CBSR & NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China.
2 University of Chinese Academy of Sciences, Beijing, China.
3 GE Global Research, Niskayuna, NY.
{shifeng.zhang,zlei,szli}@nlpr.ia.ac.cn, {longyin.wen,xiao.bian}@ge.com

Abstract

For object detection,

the two-stage approach (e.g.,
Faster R-CNN) has been achieving the highest accuracy,
whereas the one-stage approach (e.g., SSD) has the ad-
vantage of high efﬁciency. To inherit the merits of both
while overcoming their disadvantages, in this paper, we pro-
pose a novel single-shot based detector, called ReﬁneDet,
that achieves better accuracy than two-stage methods and
maintains comparable efﬁciency of one-stage methods. Re-
ﬁneDet consists of two inter-connected modules, namely,
the anchor reﬁnement module and the object detection mod-
ule. Speciﬁcally, the former aims to (1) ﬁlter out nega-
tive anchors to reduce search space for the classiﬁer, and
(2) coarsely adjust the locations and sizes of anchors to
provide better initialization for the subsequent regressor.
The latter module takes the reﬁned anchors as the input
from the former to further improve the regression and pre-
dict multi-class label. Meanwhile, we design a transfer
connection block to transfer the features in the anchor re-
ﬁnement module to predict locations, sizes and class la-
bels of objects in the object detection module. The multi-
task loss function enables us to train the whole network
in an end-to-end way. Extensive experiments on PASCAL
VOC 2007, PASCAL VOC 2012, and MS COCO demon-
strate that ReﬁneDet achieves state-of-the-art detection ac-
curacy with high efﬁciency. Code is available at https:
//github.com/sfzhang15/RefineDet.

1. Introduction

Object detection has achieved signiﬁcant advances in re-
cent years, with the framework of deep neural networks
(DNN). The current DNN detectors of state-of-the-art can
be divided into two categories: (1) the two-stage approach,
including [3, 15, 36, 41], and (2) the one-stage approach,
including [30, 35]. In the two-stage approach, a sparse set
of candidate object boxes is ﬁrst generated, and then they
are further classiﬁed and regressed. The two-stage meth-

ods have been achieving top performances on several chal-
lenging benchmarks, including PASCAL VOC [8] and MS
COCO [29].

The one-stage approach detects objects by regular and
dense sampling over locations, scales and aspect ratios. The
main advantage of this is its high computational efﬁciency.
However, its detection accuracy is usually behind that of
the two-stage approach, one of the main reasons being due
to the class imbalance problem [28].

Some recent methods in the one-stage approach aim to
address the class imbalance problem, to improve the detec-
tion accuracy. Kong et al. [24] use the objectness prior con-
straint on convolutional feature maps to signiﬁcantly reduce
the search space of objects. Lin et al. [28] address the class
imbalance issue by reshaping the standard cross entropy
loss to focus training on a sparse set of hard examples and
down-weights the loss assigned to well-classiﬁed examples.
Zhang et al. [53] design a max-out labeling mechanism to
reduce false positives resulting from class imbalance.

In our opinion,

the current state-of-the-art two-stage
methods, e.g., Faster R-CNN [36], R-FCN [5], and FPN
[27], have three advantages over the one-stage methods as
follows: (1) using two-stage structure with sampling heuris-
tics to handle class imbalance; (2) using two-step cascade to
regress the object box parameters; (3) using two-stage fea-
tures to describe the objects1.
In this work, we design a
novel object detection framework, called ReﬁneDet, to in-
herit the merits of the two approaches (i.e., one-stage and
two-stage approaches) and overcome their shortcomings. It
improves the architecture of the one-stage approach, by us-
ing two inter-connected modules (see Figure 1), namely, the
anchor 2 reﬁnement module (ARM) and the object detection

1In case of Faster R-CNN, the features (excluding shared features) in
the ﬁrst stage (i.e., RPN) are trained for the binary classiﬁcation (being an
object or not), while the features (excluding shared features) in the sec-
ond stage(i.e., Fast R-CNN) are trained for the multi-class classiﬁcation
(background or object classes).

2We denote the reference bounding box as “anchor box”, which is also
called “anchor” for simplicity, as in [36]. However, in [30], it is called
“default box”.

1

Figure 1: Architecture of ReﬁneDet. For better visualization, we only display the layers used for detection. The celadon
parallelograms denote the reﬁned anchors associated with different feature layers. The stars represent the centers of the
reﬁned anchor boxes, which are not regularly paved on the image.

module (ODM). Speciﬁcally, the ARM is designed to (1)
identify and remove negative anchors to reduce search space
for the classiﬁer, and (2) coarsely adjust the locations and
sizes of anchors to provide better initialization for the subse-
quent regressor. The ODM takes the reﬁned anchors as the
input from the former to further improve the regression and
predict multi-class labels. As shown in Figure 1, these two
inter-connected modules imitate the two-stage structure and
thus inherit the three aforementioned advantages to produce
accurate detection results with high efﬁciency. In addition,
we design a transfer connection block (TCB) to transfer the
features3 in the ARM to predict locations, sizes, and class
labels of objects in the ODM. The multi-task loss function
enables us to train the whole network in an end-to-end way.
Extensive experiments on PASCAL VOC 2007, PAS-
CAL VOC 2012, and MS COCO benchmarks demonstrate
that ReﬁneDet outperforms the state-of-the-art methods.
Speciﬁcally, it achieves 85.8% and 86.8% mAPs on VOC
2007 and 2012, with VGG-16 network. Meanwhile, it out-
performs the previously best published results from both
one-stage and two-stage approaches by achieving 41.8%
AP4 on MS COCO test-dev with ResNet-101. In ad-

3The features in the ARM focus on distinguishing positive anchors
from background. We design the TCB to transfer the features in the ARM
to handle the more challenging tasks in the ODM, i.e., predict accurate
object locations, sizes and multi-class labels.

4Based on the evaluation protocol in MS COCO [29], AP is the sin-

dition, ReﬁneDet is time efﬁcient, i.e., it runs at 40.2 FPS
and 24.1 FPS on a NVIDIA Titan X GPU with the input
sizes 320 × 320 and 512 × 512 in inference.

The main contributions of this work are summarized as
follows.
(1) We introduce a novel one-stage framework
for object detection, composed of two inter-connected mod-
ules, i.e., the ARM and the ODM. This leads to performance
better than the two-stage approach while maintaining high
efﬁciency of the one-stage approach. (2) To ensure the ef-
fectiveness, we design the TCB to transfer the features in
the ARM to handle more challenging tasks, i.e., predict ac-
curate object locations, sizes and class labels, in the ODM.
(3) ReﬁneDet achieves the latest state-of-the-art results on
generic object detection (i.e., PASCAL VOC 2007 [10],
PASCAL VOC 2012 [11] and MS COCO [29]).

2. Related Work

Classical Object Detectors. Early object detection meth-
ods are based on the sliding-window paradigm, which ap-
ply the hand-crafted features and classiﬁers on dense image
grids to ﬁnd objects. As one of the most successful meth-
ods, Viola and Jones [47] use Haar feature and AdaBoost
to train a series of cascaded classiﬁers for face detection,

gle most important metric, which is computed by averaging over all 10
intersection over union (IoU) thresholds (i.e., in the range [0.5:0.95] with
uniform step size 0.05) of 80 categories.

achieving satisfactory accuracy with high efﬁciency. DPM
[12] is another popular method using mixtures of multi-
scale deformable part models to represent highly variable
object classes, maintaining top results on PASCAL VOC [8]
for many years. However, with the arrival of deep convolu-
tional network, the object detection task is quickly dom-
inated by the CNN-based detectors, which can be roughly
divided into two categories, i.e., the two-stage approach and
one-stage approach.
Two-Stage Approach. The two-stage approach consists of
two parts, where the ﬁrst one (e.g., Selective Search [46],
EdgeBoxes [55], DeepMask [32, 33], RPN [36]) generates a
sparse set of candidate object proposals, and the second one
determines the accurate object regions and the correspond-
ing class labels using convolutional networks. Notably, the
two-stage approach (e.g., R-CNN [16], SPPnet [18], Fast R-
CNN [15] to Faster R-CNN [36]) achieves dominated per-
formance on several challenging datasets (e.g., PASCAL
VOC 2012 [11] and MS COCO [29]). After that, numer-
ous effective techniques are proposed to further improve the
performance, such as architecture diagram [5, 26, 54], train-
ing strategy [41, 48], contextual reasoning [1, 14, 40, 50]
and multiple layers exploiting [3, 25, 27, 42].
One-Stage Approach. Considering the high efﬁciency, the
one-stage approach attracts much more attention recently.
Sermanet et al. [38] present the OverFeat method for clas-
siﬁcation, localization and detection based on deep Con-
vNets, which is trained end-to-end, from raw pixels to ul-
timate categories. Redmon et al. [34] use a single feed-
forward convolutional network to directly predict object
classes and locations, called YOLO, which is extremely
fast. After that, YOLOv2 [35] is proposed to improve
YOLO in several aspects, i.e., add batch normalization on
all convolution layers, use high resolution classiﬁer, use
convolution layers with anchor boxes to predict bounding
boxes instead of the fully connected layers, etc. Liu et al.
[30] propose the SSD method, which spreads out anchors
of different scales to multiple layers within a ConvNet and
enforces each layer to focus on predicting objects of a cer-
tain scale. DSSD [13] introduces additional context into
SSD via deconvolution to improve the accuracy. DSOD
[39] designs an efﬁcient framework and a set of principles to
learn object detectors from scratch, following the network
structure of SSD. To improve the accuracy, some one-stage
methods [24, 28, 53] aim to address the extreme class im-
balance problem by re-designing the loss function or clas-
siﬁcation strategies. Although the one-stage detectors have
made good progress, their accuracy still trails that of two-
stage methods.

3. Network Architecture

Refer to the overall network architecture shown in Fig-
ure 1. Similar to SSD [30], ReﬁneDet is based on a feed-

forward convolutional network that produces a ﬁxed num-
ber of bounding boxes and the scores indicating the pres-
ence of different classes of objects in those boxes, followed
by the non-maximum suppression to produce the ﬁnal re-
sult. ReﬁneDet is formed by two inter-connected modules,
i.e., the ARM and the ODM. The ARM aims to remove neg-
ative anchors so as to reduce search space for the classiﬁer
and also coarsely adjust the locations and sizes of anchors
to provide better initialization for the subsequent regressor,
whereas ODM aims to regress accurate object locations and
predict multi-class labels based on the reﬁned anchors. The
ARM is constructed by removing the classiﬁcation layers
and adding some auxiliary structures of two base networks
(i.e., VGG-16 [43] and ResNet-101 [19] pretrained on Im-
ageNet [37]) to meet our needs. The ODM is composed of
the outputs of TCBs followed by the prediction layers (i.e.,
the convolution layers with 3 × 3 kernel size), which gener-
ates the scores for object classes and shape offsets relative to
the reﬁned anchor box coordinates. The following explain
three core components in ReﬁneDet, i.e., (1) transfer con-
nection block (TCB), converting the features from the ARM
to the ODM for detection; (2) two-step cascaded regression,
accurately regressing the locations and sizes of objects; (3)
negative anchor ﬁltering, early rejecting well-classiﬁed neg-
ative anchors and mitigate the imbalance issue.

Transfer Connection Block. To link between the ARM
and ODM, we introduce the TCBs to convert features of dif-
ferent layers from the ARM, into the form required by the
ODM, so that the ODM can share features from the ARM.
Notably, from the ARM, we only use the TCBs on the fea-
ture maps associated with anchors. Another function of the
TCBs is to integrate large-scale context [13, 27] by adding
the high-level features to the transferred features to improve
detection accuracy. To match the dimensions between them,
we use the deconvolution operation to enlarge the high-level
feature maps and sum them in the element-wise way. Then,
we add a convolution layer after the summation to ensure
the discriminability of features for detection. The architec-
ture of the TCB is shown in Figure 2.

Two-Step Cascaded Regression. Current one-stage meth-
ods [13, 24, 30] rely on one-step regression based on various
feature layers with different scales to predict the locations
and sizes of objects, which is rather inaccurate in some chal-
lenging scenarios, especially for the small objects. To that
end, we present a two-step cascaded regression strategy to
regress the locations and sizes of objects. That is, we use
the ARM to ﬁrst adjust the locations and sizes of anchors to
provide better initialization for the regression in the ODM.
Speciﬁcally, we associate n anchor boxes with each regu-
larly divided cell on the feature map. The initial position of
each anchor box relative to its corresponding cell is ﬁxed.
At each feature map cell, we predict four offsets of the re-
ﬁned anchor boxes relative to the original tiled anchors and

adapt to variations of objects. That is, we randomly ex-
pand and crop the original training images with additional
random photometric distortion [20] and ﬂipping to generate
the training samples. Please refer to [30] for more details.
Backbone Network. We use VGG-16 [43] and ResNet-101
[19] as the backbone networks in our ReﬁneDet, which are
pretrained on the ILSVRC CLS-LOC dataset [37]. Notably,
ReﬁneDet can also work on other pretrained networks, such
as Inception V2 [22], Inception ResNet [44], and ResNeXt-
101 [49]. Similar to DeepLab-LargeFOV [4], we convert
fc6 and fc7 of VGG-16 to convolution layers conv fc6 and
conv fc7 via subsampling parameters. Since conv4 3 and
conv5 3 have different feature scales compared to other lay-
ers, we use L2 normalization [31] to scale the feature norms
in conv4 3 and conv5 3 to 10 and 8, then learn the scales
during back propagation. Meanwhile, to capture high-level
information and drive object detection at multiple scales,
we also add two extra convolution layers (i.e., conv6 1 and
conv6 2) to the end of the truncated VGG-16 and one extra
residual block (i.e., res6) to the end of the truncated ResNet-
101, respectively.
Anchors Design and Matching. To handle different scales
of objects, we select four feature layers with the total stride
sizes 8, 16, 32, and 64 pixels for both VGG-16 and ResNet-
1015, associated with several different scales of anchors for
prediction. Each feature layer is associated with one spe-
ciﬁc scale of anchors (i.e., the scale is 4 times of the to-
tal stride size of the corresponding layer) and three aspect
ratios (i.e., 0.5, 1.0, and 2.0). We follow the design of
anchor scales over different layers in [53], which ensures
that different scales of anchors have the same tiling den-
sity [51, 52] on the image. Meanwhile, during the train-
ing phase, we determine the correspondence between the
anchors and ground truth boxes based on the jaccard over-
lap [7], and train the whole network end-to-end accordingly.
Speciﬁcally, we ﬁrst match each ground truth to the anchor
box with the best overlap score, and then match the anchor
boxes to any ground truth with overlap higher than 0.5.
Hard Negative Mining. After matching step, most of the
anchor boxes are negatives, even for the ODM, where some
easy negative anchors are rejected by the ARM. Similar
to SSD [30], we use hard negative mining to mitigate the
extreme foreground-background class imbalance, i.e., we
select some negative anchor boxes with top loss values to
make the ratio between the negatives and positives below
3 : 1, instead of using all negative anchors or randomly se-
lecting the negative anchors in training.
Loss Function. The loss function for ReﬁneDet consists of
two parts, i.e., the loss in the ARM and the loss in the ODM.

5For the VGG-16 base network, the conv4 3, conv5 3, conv fc7, and
conv6 2 feature layers are used to predict the locations, sizes and con-
ﬁdences of objects. While for the ResNet-101 base network, res3b3,
res4b22, res5c, and res6 are used for prediction.

Figure 2: The overview of the transfer connection block.

two conﬁdence scores indicating the presence of foreground
objects in those boxes. Thus, we can yield n reﬁned anchor
boxes at each feature map cell.

After obtaining the reﬁned anchor boxes, we pass them
to the corresponding feature maps in the ODM to further
generate object categories and accurate object locations and
sizes, as shown in Figure 1. The corresponding feature
maps in the ARM and the ODM have the same dimension.
We calculate c class scores and the four accurate offsets of
objects relative to the reﬁned anchor boxes, yielding c + 4
outputs for each reﬁned anchor boxes to complete the de-
tection task. This process is similar to the default boxes
used in SSD [30]. However, in contrast to SSD [30] di-
rectly uses the regularly tiled default boxes for detection,
ReﬁneDet uses two-step strategy, i.e., the ARM generates
the reﬁned anchor boxes, and the ODM takes the reﬁned
anchor boxes as input for further detection, leading to more
accurate detection results, especially for the small objects.
Negative Anchor Filtering. To early reject well-classiﬁed
negative anchors and mitigate the imbalance issue, we de-
sign a negative anchor ﬁltering mechanism. Speciﬁcally, in
training phase, for a reﬁned anchor box, if its negative con-
ﬁdence is larger than a preset threshold θ (i.e., set θ = 0.99
empirically), we will discard it in training the ODM. That is,
we only pass the reﬁned hard negative anchor boxes and re-
ﬁned positive anchor boxes to train the ODM. Meanwhile,
in the inference phase, if a reﬁned anchor box is assigned
with a negative conﬁdence larger than θ, it will be discarded
in the ODM for detection.

4. Training and Inference

Data Augmentation. We use several data augmentation
strategies presented in [30] to construct a robust model to

Nodm

For the ARM, we assign a binary class label (of being an
object or not) to each anchor and regress its location and
size simultaneously to get the reﬁned anchor. After that, we
pass the reﬁned anchors with the negative conﬁdence less
than the threshold to the ODM to further predict object cat-
egories and accurate object locations and sizes. With these
deﬁnitions, we deﬁne the loss function as:
(cid:0) (cid:80)
(cid:0) (cid:80)

L({pi}, {xi}, {ci}, {ti}) = 1
Narm
i )(cid:1) + 1
+ (cid:80)
i ≥ 1]Lr(xi, g∗
i )(cid:1)
+ (cid:80)
i ≥ 1]Lr(ti, g∗

i Lb(pi, [l∗
i ≥ 1])
i Lm(ci, l∗
i )

i[l∗
i[l∗

i ≥ 1]) = 0 and Lr(xi, g∗
i ) = 0 and Lr(ti, g∗

(1)
where i is the index of anchor in a mini-batch, l∗
is the
i
ground truth class label of anchor i, g∗
i is the ground truth
location and size of anchor i. pi and xi are the predicted
conﬁdence of the anchor i being an object and reﬁned co-
ordinates of the anchor i in the ARM. ci and ti are the
predicted object class and coordinates of the bounding box
in the ODM. Narm and Nodm are the numbers of positive
anchors in the ARM and ODM, respectively. The binary
classiﬁcation loss Lb is the cross-entropy/log loss over two
classes (object vs. not object), and the multi-class classiﬁ-
cation loss Lm is the softmax loss over multiple classes con-
ﬁdences. Similar to Fast R-CNN [15], we use the smooth
L1 loss as the regression loss Lr. The Iverson bracket indi-
cator function [l∗
i ≥ 1] outputs 1 when the condition is true,
i.e., l∗
i ≥ 1 (the anchor is not the negative), and 0 other-
wise. Hence [l∗
i ≥ 1]Lr indicates that the regression loss is
ignored for negative anchors. Notably, if Narm = 0, we set
Lb(pi, [l∗
i ) = 0; and if Nodm = 0,
we set Lm(ci, l∗
i ) = 0 accordingly.
Optimization. As mentioned above, the backbone network
(e.g., VGG-16 and ResNet-101) in our ReﬁneDet method is
pretrained on the ILSVRC CLS-LOC dataset [37]. We use
the “xavier” method [17] to randomly initialize the parame-
ters in the two extra added convolution layers (i.e., conv6 1
and conv6 2) of VGG-16 based ReﬁneDet, and draw the pa-
rameters from a zero-mean Gaussian distribution with stan-
dard deviation 0.01 for the extra residual block (i.e., res6) of
ResNet-101 based ReﬁneDet. We set the default batch size
to 32 in training. Then, the whole network is ﬁne-tuned us-
ing SGD with 0.9 momentum and 0.0005 weight decay. We
set the initial learning rate to 10−3, and use slightly differ-
ent learning rate decay policy for different dataset, which
will be described in details later.
Inference. At inference phase, the ARM ﬁrst ﬁlters out the
regularly tiled anchors with the negative conﬁdence scores
larger than the threshold θ, and then reﬁnes the locations
and sizes of remaining anchors. After that, the ODM takes
over these reﬁned anchors, and outputs top 400 high con-
ﬁdent detections per image. Finally, we apply the non-
maximum suppression with jaccard overlap of 0.45 per
class and retain the top 200 high conﬁdent detections per
image to produce the ﬁnal detection results.

5. Experiments

Experiments are conducted on three datasets: PASCAL
VOC 2007, PASCAL VOC 2012 and MS COCO. The PAS-
CAL VOC and MS COCO datasets include 20 and 80 ob-
ject classes, respectively. The classes in PASCAL VOC are
the subset of that in MS COCO. We implement ReﬁneDet
in Caffe [23]. All the training and testing codes and the
trained models are available at https://github.com/
sfzhang15/RefineDet.

5.1. PASCAL VOC 2007

All models are trained on the VOC 2007 and VOC 2012
trainval sets, and tested on the VOC 2007 test set. We
set the learning rate to 10−3 for the ﬁrst 80k iterations, and
decay it to 10−4 and 10−5 for training another 20k and 20k
iterations, respectively. We use the default batch size 32 in
training, and only use VGG-16 as the backbone network for
all the experiments on the PASCAL VOC dataset, including
VOC 2007 and VOC 2012.

We compare ReﬁneDet6 with the state-of-the-art detec-
tors in Table 1. With low dimension input (i.e., 320 × 320),
ReﬁneDet produces 80.0% mAP without bells and whis-
tles, which is the ﬁrst method achieving above 80% mAP
with such small input images, much better than several
modern objectors. By using larger input size 512 × 512,
ReﬁneDet achieves 81.8% mAP, surpassing all one-stage
methods, e.g., RON384 [24], SSD513 [13], DSSD513 [13],
etc. Comparing to the two-stage methods, ReﬁneDet512
performs better than most of them except CoupleNet [54],
which is based on ResNet-101 and uses larger input size
(i.e., ∼ 1000 × 600) than our ReﬁneDet512. As pointed
out in [21], the input size signiﬁcantly inﬂuences detection
accuracy. The reason is that high resolution inputs make
the detectors “seeing” small objects clearly to increase suc-
cessful detections. To reduce the impact of input size for a
fair comparison, we use the multi-scale testing strategy to
evaluate ReﬁneDet, achieving 83.1% (ReﬁneDet320+) and
83.8% (ReﬁneDet512+) mAPs, which are much better than
the state-of-the-art methods.

5.1.1 Run Time Performance

We present the inference speed of ReﬁneDet and the state-
of-the-art methods in the ﬁfth column of Table 1. The speed
is evaluated with batch size 1 on a machine with NVIDIA
Titan X, CUDA 8.0 and cuDNN v6. As shown in Table 1,
we ﬁnd that ReﬁneDet processes an image in 24.8ms (40.3
FPS) and 41.5ms (24.1 FPS) with input sizes 320 × 320
and 512 × 512, respectively. To the best of our knowledge,

6Due to the shortage of computational resources, we only train Re-
ﬁneDet with two kinds of input size, i.e., 320 × 320 and 512 × 512. We
believe the accuracy of ReﬁneDet can be further improved using larger
input images.

Table 1: Detection results on PASCAL VOC dataset. For VOC 2007, all methods are trained on VOC 2007 and VOC 2012
trainval sets and tested on VOC 2007 test set. For VOC 2012, all methods are trained on VOC 2007 and VOC 2012
trainval sets plus VOC 2007 test set, and tested on VOC 2012 test set. Bold fonts indicate the best mAP.

Method

Backbone

Input size

#Boxes

FPS

mAP (%)

VOC 2007

VOC 2012

two-stage:
Fast R-CNN[15]
Faster R-CNN[36]
OHEM[41]
HyperNet[25]
Faster R-CNN[36]
ION[1]
MR-CNN[14]
R-FCN[5]
CoupleNet[54]

one-stage:
YOLO[34]
RON384[24]
SSD321[13]
SSD300∗[30]
DSOD300[39]
YOLOv2[35]
DSSD321[13]
SSD512∗[30]
SSD513[13]
DSSD513[13]
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+

VGG-16
VGG-16
VGG-16
VGG-16
ResNet-101
VGG-16
VGG-16
ResNet-101
ResNet-101

GoogleNet [45]
VGG-16
ResNet-101
VGG-16
DS/64-192-48-1
Darknet-19
ResNet-101
VGG-16
ResNet-101
ResNet-101
VGG-16
VGG-16
VGG-16
VGG-16

∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600

448 × 448
384 × 384
321 × 321
300 × 300
300 × 300
544 × 544
321 × 321
512 × 512
513 × 513
513 × 513
320 × 320
512 × 512
-
-

∼ 2000
300
300
100
300
4000
250
300
300

98
30600
17080
8732
8732
845
17080
24564
43688
43688
6375
16320
-
-

0.5
7
7
0.88
2.4
1.25
0.03
9
8.2

45
15
11.2
46
17.4
40
9.5
19
6.8
5.5
40.3
24.1
-
-

70.0
73.2
74.6
76.3
76.4
76.5
78.2
80.5
82.7

63.4
75.4
77.1
77.2
77.7
78.6
78.6
79.8
80.6
81.5
80.0
81.8
83.1
83.8

68.4
70.4
71.9
71.4
73.8
76.4
73.9
77.6
80.4

57.9
73.0
75.4
75.8
76.3
73.4
76.3
78.5
79.4
80.0
78.1
80.1
82.7
83.5

ReﬁneDet is the ﬁrst real-time method to achieve detection
accuracy above 80% mAP on PASCAL VOC 2007. Com-
paring to SSD, RON, DSSD and DSOD, ReﬁneDet asso-
ciates fewer anchor boxes on the feature maps (e.g., 24564
anchor boxes in SSD512∗[30] vs. 16320 anchor boxes in
ReﬁneDet512). However, ReﬁneDet still achieves top accu-
racy with high efﬁciency, mainly thanks to the design of two
inter-connected modules, (e.g., two-step regression), which
enables ReﬁneDet to adapt to different scales and aspect ra-
tios of objects. Meanwhile, only YOLO and SSD300∗ are
slightly faster than our ReﬁneDet320, but their accuracy are
16.6% and 2.5% worse than ours. In summary, ReﬁneDet
achieves the best trade-off between accuracy and speed.

5.1.2 Ablation Study

To demonstrate the effectiveness of different components
in ReﬁneDet, we construct four variants and evaluate them
on VOC 2007, shown in Table 3. Speciﬁcally, for a fair
comparison, we use the same parameter settings and input
size (320 × 320) in evaluation. All models are trained on
VOC 2007 and VOC 2012 trainval sets, and tested on
VOC 2007 test set.

Negative Anchor Filtering. To demonstrate the effective-
ness of the negative anchor ﬁltering, we set the conﬁdence

threshold θ of the anchors to be negative to 1.0 in both train-
ing and testing.
In this case, all reﬁned anchors will be
sent to the ODM for detection. Other parts of ReﬁneDet re-
main unchanged. Removing negative anchor ﬁltering leads
to 0.5% drop in mAP (i.e., 80.0% vs. 79.5%). The reason
is that most of these well-classiﬁed negative anchors will be
ﬁltered out during training, which solves the class imbal-
ance issue to some extent.

Two-Step Cascaded Regression. To validate the effective-
ness of the two-step cascaded regression, we redesign the
network structure by directly using the regularly paved an-
chors instead of the reﬁned ones from the ARM (see the
fourth column in Table 3). As shown in Table 3, we ﬁnd that
mAP is reduced from 79.5% to 77.3%. This sharp decline
(i.e., 2.2%) demonstrates that the two-step anchor cascaded
regression signiﬁcantly help promote the performance.

Transfer Connection Block. We construct a network by
cutting the TCBs in ReﬁneDet and redeﬁning the loss func-
tion in the ARM to directly detect multi-class of objects,
just like SSD, to demonstrate the effect of the TCB. The
detection accuracy of the model is presented in the ﬁfth col-
umn in Table 3. We compare the results in the fourth and
ﬁfth columns in Table 3 (77.3% vs. 76.2%) and ﬁnd that
the TCB improves the mAP by 1.1%. The main reason is

Table 2: Detection results on MS COCO test-dev set. Bold fonts indicate the best performance.

Method
two-stage:
Fast R-CNN [15]
Faster R-CNN [36]
OHEM [41]
ION [1]
OHEM++ [41]
R-FCN [5]
CoupleNet [54]
Faster R-CNN by G-RMI [21]
Faster R-CNN+++ [19]
Faster R-CNN w FPN [27]
Faster R-CNN w TDM [42]
Deformable R-FCN [6]
umd det [2]
G-RMI [21]

one-stage:
YOLOv2 [35]
SSD300∗ [30]
RON384++ [24]
SSD321 [13]
DSSD321 [13]
SSD512∗ [30]
SSD513 [13]
DSSD513 [13]
RetinaNet500 [28]
RetinaNet800 [28]∗
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+
ReﬁneDet320+
ReﬁneDet512+

Data

Backbone

AP

AP50

AP75

APS

APM

APL

train
trainval
trainval
train
trainval
trainval
trainval
-
trainval
trainval35k
trainval
trainval
trainval
trainval32k

trainval35k
trainval35k
trainval
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k

VGG-16
VGG-16
VGG-16
VGG-16
VGG-16
ResNet-101
ResNet-101
Inception-ResNet-v2[44]
ResNet-101-C4
ResNet-101-FPN
Inception-ResNet-v2-TDM
Aligned-Inception-ResNet
ResNet-101
Ensemble of Five Models

DarkNet-19[35]
VGG-16
VGG-16
ResNet-101
ResNet-101
VGG-16
ResNet-101
ResNet-101
ResNet-101
ResNet-101-FPN
VGG-16
VGG-16
ResNet-101
ResNet-101
VGG-16
VGG-16
ResNet-101
ResNet-101

19.7
21.9
22.6
23.6
25.5
29.9
34.4
34.7
34.9
36.2
36.8
37.5
40.8
41.6

21.6
25.1
27.4
28.0
28.0
28.8
31.2
33.2
34.4
39.1
29.4
33.0
32.0
36.4
35.2
37.6
38.6
41.8

35.9
42.7
42.5
43.2
45.9
51.9
54.8
55.5
55.7
59.1
57.7
58.0
62.4
61.9

44.0
43.1
49.5
45.4
46.1
48.5
50.4
53.3
53.1
59.1
49.2
54.5
51.4
57.5
56.1
58.7
59.9
62.9

-
-
22.2
23.6
26.1
-
37.2
36.7
37.4
39.0
39.2
40.8
44.9
45.4

19.2
25.8
27.1
29.3
29.2
30.3
33.3
35.2
36.8
42.3
31.3
35.5
34.2
39.5
37.7
40.8
41.7
45.7

-
-
5.0
6.4
7.4
10.8
13.4
13.5
15.6
18.2
16.2
19.4
23.0
23.9

5.0
6.6
-
6.2
7.4
10.9
10.2
13.0
14.7
21.8
10.0
16.3
10.5
16.6
19.5
22.7
21.1
25.6

-
-
23.7
24.1
27.7
32.8
38.1
38.1
38.7
39.0
39.8
40.1
43.4
43.5

22.4
25.9
-
28.3
28.1
31.8
34.5
35.4
38.5
42.7
32.0
36.3
34.7
39.9
37.2
40.3
41.7
45.1

-
-
37.9
38.3
40.3
45.0
50.8
52.0
50.9
48.2
52.1
52.5
53.2
54.9

35.5
41.4
-
49.3
47.6
43.5
49.8
51.1
49.1
50.2
44.4
44.3
50.4
51.4
47.0
48.3
52.3
54.1

∗ This entry reports the single model accuracy of RetinaNet method, trained with scale jitter and for 1.5× longer than RetinaNet500.

Table 3: Effectiveness of various designs. All models are
trained on VOC 2007 and VOC 2012 trainval set and
tested on VOC 2007 test set.

Component
negative anchor ﬁltering?
two-step cascaded regression?
transfer connection block?
mAP (%)

!
!
!
80.0

!
!
79.5

ReﬁneDet320

!
77.3

76.2

that the model can inherit the discriminative features from
the ARM, and integrate large-scale context information to
improve the detection accuracy by using the TCB.

5.2. PASCAL VOC 2012

Following the protocol of VOC 2012, we submit the de-
tection results of ReﬁneDet to the public testing server for
evaluation. We use VOC 2007 trainval set and test
set plus VOC 2012 trainval set (21, 503 images) for

training, and test on VOC 2012 test set (10, 991 images).
We use the default batch size 32 in training. Meanwhile, we
set the learning rate to 10−3 in the ﬁrst 160k iterations, and
decay it to 10−4 and 10−5 for another 40k and 40k itera-
tions.

Table 1 shows the accuracy of the proposed ReﬁneDet al-
gorithm, as well as the state-of-the-art methods. Among the
methods fed with input size 320 × 320, ReﬁneDet320 ob-
tains the top 78.1% mAP, which is even better than most of
those two-stage methods using about 1000 × 600 input size
(e.g., 70.4% mAP of Faster R-CNN [36] and 77.6% mAP
of R-FCN [5]). Using the input size 512 × 512, ReﬁneDet
improves mAP to 80.1%, which is surpassing all one-stage
methods and only slightly lower than CoupleNet [54] (i.e.,
80.4%). CoupleNet uses ResNet-101 as base network with
1000 × 600 input size. To reduce the impact of input size
for a fair comparison, we also use multi-scale testing to
evaluate ReﬁneDet and obtain the state-of-the-art mAPs of
82.7% (ReﬁneDet320+) and 83.5% (ReﬁneDet512+).

Table 4: Detection results on PASCAL VOC dataset. All
models are pre-trained on MS COCO, and ﬁne-tuned on
PASCAL VOC. Bold fonts indicate the best mAP.

mAP (%)
VOC 2007 test VOC 2012 test

Method

Backbone

two-stage:
Faster R-CNN[36]
OHEM++[41]
R-FCN[5]

VGG-16
VGG-16
ResNet-101

VGG-16
VGG-16
VGG-16

one-stage:
SSD300[30]
SSD512[30]
RON384++[24]
DSOD300[39] DS/64-192-48-1
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+

VGG-16
VGG-16
VGG-16
VGG-16

5.3. MS COCO

78.8
-
83.6

81.2
83.2
81.3
81.7
84.0
85.2
85.6
85.8

75.9
80.1
82.0

79.3
82.2
80.7
79.3
82.7
85.0
86.0
86.8

In addition to PASCAL VOC, we also evaluate Re-
ﬁneDet on MS COCO [29]. Unlike PASCAL VOC, the
detection methods using ResNet-101 always achieve bet-
ter performance than those using VGG-16 on MS COCO.
Thus, we also report the results of ResNet-101 based Re-
ﬁneDet. Following the protocol in MS COCO, we use the
trainval35k set [1] for training and evaluate the results
from test-dev evaluation server. We set the batch size to
32 in training7, and train the model with 10−3 learning rate
for the ﬁrst 280k iterations, then 10−4 and 10−5 for another
80k and 40k iterations, respectively.

Table 7 shows the results on MS COCO test-dev set.
ReﬁneDet320 with VGG-16 produces 29.4% AP that is bet-
ter than all other methods based on VGG-16 (e.g., SSD512∗
[30] and OHEM++ [41]). The accuracy of ReﬁneDet can
be improved to 33.0% by using larger input size (i.e.,
512 × 512), which is much better than several modern ob-
ject detectors, e.g., Faster R-CNN [36] and SSD512∗ [30].
Meanwhile, using ResNet-101 can further improve the per-
formance of ReﬁneDet, i.e., ReﬁneDet320 with ResNet-101
achieves 32.0% AP and ReﬁneDet512 achieves 36.4% AP,
exceeding most of the detection methods except Faster R-
CNN w TDM [42], Deformable R-FCN [6], RetinaNet800
[28], umd det [2], and G-RMI [21]. All these methods use a
much bigger input images for both training and testing (i.e.,
1000×600 or 800×800) than our ReﬁneDet (i.e., 320×320
and 512 × 512). Similar to PASCAL VOC, we also report
the multi-scale testing AP results of ReﬁneDet for fair com-
parison in Table 7, i.e., 35.2% (ReﬁneDet320+ with VGG-
16), 37.6% (ReﬁneDet512+ with VGG-16), 38.6% (Re-

7Due to the memory issue, we reduce the batch size to 20 (which is the
largest batch size we can use for training on a machine with 4 NVIDIA
M40 GPUs) to train the ResNet-101 based ReﬁneDet with the input size
512 × 512, and train the model with 10−3 learning rate for the ﬁrst 400k
iterations, then 10−4 and 10−5 for another 80k and 60k iterations.

ﬁneDet320+ with ResNet-101) and 41.8% (ReﬁneDet512+
with ResNet-101). The best performance of ReﬁneDet is
41.8%, which is the state-of-the-art, surpassing all pub-
lished two-stage and one-stage approaches. Although the
second best detector G-RMI [21] ensembles ﬁve Faster R-
CNN models, it still produces 0.2% lower AP than Re-
ﬁneDet using a single model. Comparing to the third and
fourth best detectors, i.e., umd det [2] and RetinaNet800
[28], ReﬁneDet produces 1.0% and 2.7% higher APs. In
addition, the main contribution: focal loss in RetinaNet800,
is complementary to our method. We believe that it can be
used in ReﬁneNet to further improve the performance.

5.4. From MS COCO to PASCAL VOC

We study how the MS COCO dataset help the detec-
tion accuracy on PASCAL VOC. Since the object classes
in PASCAL VOC are the subset of MS COCO, we directly
ﬁne-tune the detection models pretrained on MS COCO via
subsampling the parameters, which achieves 84.0% mAP
(ReﬁneDet320) and 85.2% mAP (ReﬁneDet512) on VOC
2007 test set, and 82.7% mAP (ReﬁneDet320) and 85.0%
mAP (ReﬁneDet512) on VOC 2012 test set, shown in Ta-
ble 4. After using the multi-scale testing, the detection ac-
curacy are promoted to 85.6%, 85.8%, 86.0% and 86.8%,
respectively. As shown in Table 4, using the training data in
MS COCO and PASCAL VOC, our ReﬁneDet obtains the
top mAP scores on both VOC 2007 and VOC 2012. Most
important, our single model ReﬁneNet512+ based on VGG-
16 ranks as the top 5 on the VOC 2012 Leaderboard (see
[9]), which is the best accuracy among all one-stage meth-
ods. Other two-stage methods achieving better results are
based on much deeper networks (e.g., ResNet-101 [19] and
ResNeXt-101 [49]) or using ensemble mechanism.

6. Conclusions

In this paper, we present a single-shot reﬁnement neu-
ral network based detector, which consists of two inter-
connected modules, i.e., the ARM and the ODM. The ARM
aims to ﬁlter out the negative anchors to reduce search space
for the classiﬁer and also coarsely adjust the locations and
sizes of anchors to provide better initialization for the subse-
quent regressor, while the ODM takes the reﬁned anchors as
the input from the former ARM to regress the accurate ob-
ject locations and sizes and predict the corresponding multi-
class labels. The whole network is trained in an end-to-end
fashion with the multi-task loss. We carry out several exper-
iments on PASCAL VOC 2007, PASCAL VOC 2012, and
MS COCO datasets to demonstrate that ReﬁneDet achieves
the state-of-the-art detection accuracy with high efﬁciency.
In the future, we plan to employ ReﬁneDet to detect some
other speciﬁc kinds of objects, e.g., pedestrian, vehicle, and
face, and introduce the attention mechanism in ReﬁneDet to
further improve the performance.

References

[1] S. Bell, C. L. Zitnick, K. Bala, and R. B. Girshick. Inside-
outside net: Detecting objects in context with skip pooling
and recurrent neural networks. In CVPR, pages 2874–2883,
2016. 3, 6, 7, 8

[2] N. Bodla, B. Singh, R. Chellappa, and L. S. Davis. Improv-
ing object detection with one line of code. In ICCV, 2017. 7,
8

[3] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos. A uniﬁed
multi-scale deep convolutional neural network for fast object
detection. In ECCV, pages 354–370, 2016. 1, 3

[4] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille. Semantic image segmentation with deep convolu-
tional nets and fully connected crfs. In ICLR, 2015. 4
[5] J. Dai, Y. Li, K. He, and J. Sun. R-FCN: object detection via
region-based fully convolutional networks. In NIPS, pages
379–387, 2016. 1, 3, 6, 7, 8

[6] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei.
Deformable convolutional networks. In ICCV, 2017. 7, 8
[7] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable
object detection using deep neural networks. In CVPR, pages
2155–2162, 2014. 4

[8] M. Everingham, L. J. V. Gool, C. K. I. Williams, J. M. Winn,
and A. Zisserman. The pascal visual object classes (VOC)
challenge. IJCV, 88(2):303–338, 2010. 1, 3

[9] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman.
The Leaderboard of the PASCAL
Visual Object Classes Challenge 2012 (VOC2012). http:
//host.robots.ox.ac.uk:8080/leaderboard/
displaylb.php?challengeid=11&compid=4.
Online; accessed 1 October 2017. 8

[10] M. Everingham, L. Van Gool, C. K.

I. Williams,
J. Winn, and A. Zisserman. The PASCAL Visual Ob-
ject Classes Challenge 2007 (VOC2007) Results. http:
//www.pascal-network.org/challenges/VOC/
voc2007/workshop/index.html. Online; accessed
1 October 2017. 2

[11] M. Everingham, L. Van Gool, C. K.

I. Williams,
J. Winn, and A. Zisserman. The PASCAL Visual Ob-
ject Classes Challenge 2012 (VOC2012) Results. http:
//www.pascal-network.org/challenges/VOC/
voc2012/workshop/index.html. Online; accessed
1 October 2017. 2, 3

[12] P. F. Felzenszwalb, R. B. Girshick, D. A. McAllester, and
D. Ramanan. Object detection with discriminatively trained
part-based models. TPAMI, 32(9):1627–1645, 2010. 3
[13] C. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg.
CoRR,

DSSD : Deconvolutional single shot detector.
abs/1701.06659, 2017. 3, 5, 6, 7

[14] S. Gidaris and N. Komodakis. Object detection via a multi-
region and semantic segmentation-aware CNN model.
In
ICCV, pages 1134–1142, 2015. 3, 6

[17] X. Glorot and Y. Bengio. Understanding the difﬁculty of
In AISTATS,

training deep feedforward neural networks.
pages 249–256, 2010. 5

[18] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling
in deep convolutional networks for visual recognition.
In
ECCV, pages 346–361, 2014. 3

[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, pages 770–778, 2016. 3, 4,
7, 8

[20] A. G. Howard.

Some improvements on deep convolu-
tional neural network based image classiﬁcation. CoRR,
abs/1312.5402, 2013. 4

[21] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,
A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, and
K. Murphy. Speed/accuracy trade-offs for modern convolu-
tional object detectors. In CVPR, 2017. 5, 7, 8

[22] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, pages 448–456, 2015. 4

[23] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. B.
Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. In ACMMM,
pages 675–678, 2014. 5

[24] T. Kong, F. Sun, A. Yao, H. Liu, M. Lu, and Y. Chen. RON:
reverse connection with objectness prior networks for object
detection. In CVPR, 2017. 1, 3, 5, 6, 7, 8

[25] T. Kong, A. Yao, Y. Chen, and F. Sun. Hypernet: Towards ac-
curate region proposal generation and joint object detection.
In CVPR, pages 845–853, 2016. 3, 6

[26] H. Lee, S. Eum, and H. Kwon. ME R-CNN: multi-expert

region-based CNN for object detection. In ICCV, 2017. 3

[27] T. Lin, P. Doll´ar, R. B. Girshick, K. He, B. Hariharan, and
S. J. Belongie. Feature pyramid networks for object detec-
tion. In CVPR, 2017. 1, 3, 7

[28] T. Lin, P. Goyal, R. B. Girshick, K. He, and P. Doll´ar. Focal
loss for dense object detection. In ICCV, 2017. 1, 3, 7, 8
[29] T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft COCO: com-
mon objects in context. In ECCV, pages 740–755, 2014. 1,
2, 3, 8

[30] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed,
C. Fu, and A. C. Berg. SSD: single shot multibox detector.
In ECCV, pages 21–37, 2016. 1, 3, 4, 6, 7, 8

[31] W. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking

wider to see better. In ICLR workshop, 2016. 4

[32] P. H. O. Pinheiro, R. Collobert, and P. Doll´ar. Learning to
segment object candidates. In NIPS, pages 1990–1998, 2015.
3

[33] P. O. Pinheiro, T. Lin, R. Collobert, and P. Doll´ar. Learning

to reﬁne object segments. In ECCV, pages 75–91, 2016. 3

[34] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi.
You only look once: Uniﬁed, real-time object detection. In
CVPR, pages 779–788, 2016. 3, 6

[15] R. B. Girshick. Fast R-CNN. In ICCV, pages 1440–1448,

[35] J. Redmon and A. Farhadi. YOLO9000: better, faster,

2015. 1, 3, 5, 6, 7

stronger. CoRR, abs/1612.08242, 2016. 1, 3, 6, 7

[16] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich
feature hierarchies for accurate object detection and semantic
segmentation. In CVPR, pages 580–587, 2014. 3

[36] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN:
towards real-time object detection with region proposal net-
works. TPAMI, 39(6):1137–1149, 2017. 1, 3, 6, 7, 8

[55] C. L. Zitnick and P. Doll´ar. Edge boxes: Locating object

proposals from edges. In ECCV, pages 391–405, 2014. 3

[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein,
A. C. Berg, and F. Li. Imagenet large scale visual recognition
challenge. IJCV, 115(3):211–252, 2015. 3, 4, 5

[38] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,
and Y. LeCun. Overfeat: Integrated recognition, localization
and detection using convolutional networks. In ICLR, 2014.
3

[39] Z. Shen, Z. Liu, J. Li, Y. Jiang, Y. Chen, and X. Xue. DSOD:
learning deeply supervised object detectors from scratch. In
ICCV, 2017. 3, 6, 8

[40] A. Shrivastava and A. Gupta. Contextual priming and feed-

back for faster R-CNN. In ECCV, pages 330–348, 2016. 3

[41] A. Shrivastava, A. Gupta, and R. B. Girshick. Training
region-based object detectors with online hard example min-
ing. In CVPR, pages 761–769, 2016. 1, 3, 6, 7, 8

[42] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Be-
yond skip connections: Top-down modulation for object de-
tection. CoRR, abs/1612.06851, 2016. 3, 7, 8

[43] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 3, 4

[44] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI, pages 4278–4284, 2017.
4, 7

[45] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, pages 1–9, 2015.
6

[46] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and
A. W. M. Smeulders. Selective search for object recognition.
IJCV, 104(2):154–171, 2013. 3

[47] P. A. Viola and M. J. Jones. Rapid object detection using a
In CVPR, pages 511–

boosted cascade of simple features.
518, 2001. 3

[48] X. Wang, A. Shrivastava, and A. Gupta. A-fast-rcnn: Hard
positive generation via adversary for object detection.
In
CVPR, 2017. 3

[49] S. Xie, R. B. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggre-
gated residual transformations for deep neural networks. In
CVPR, 2017. 4, 8

[50] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang. Gated
In ECCV, pages

bi-directional CNN for object detection.
354–369, 2016. 3

[51] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li. De-
tecting face with densely connected face proposal network.
In CCBR, pages 3–12, 2017. 4

[52] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li.
Faceboxes: A CPU real-time face detector with high accu-
racy. In IJCB, 2017. 4

[53] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li.
In ICCV,

S3FD: Single shot scale-invariant face detector.
2017. 1, 3, 4

[54] Y. Zhu, C. Zhao, J. Wang, X. Zhao, Y. Wu, and H. Lu. Cou-
plenet: Coupling global structure with local parts for object
detection. In ICCV, 2017. 3, 5, 6, 7

of ReﬁneDet for small objects, especially for the chairs and
tables. Increasing the input size (e.g., from 320 × 320 to
512 × 512) can improve the performance for small objects
, but it is only a temporary solution. Large input will be a
burden on running speed in inference. Therefore, detecting
small objects is still a challenge task and needs to be further
studied.

7. Complete Object Detection Results

We show the complete object detection results of the
proposed ReﬁneDet method on the PASCAL VOC 2007
test set, PASCAL VOC 2012 test set and MS COCO
test-dev set in Table 5, Table 6 and Table 7, respec-
tively. Among the results of all published methods, our Re-
ﬁneDet achieves the best performance on these three detec-
tion datasets, i.e., 85.8% mAP on the PASCAL VOC 2007
test set, 86.8% mAP on the PASCAL VOC 2012 test
set and 41.8% AP on the MS COCO test-dev set.

8. Qualitative Results

We show some qualitative results on the PASCAL VOC
2007 test set, the PASCAL VOC 2012 test set and the
MS COCO test-dev in Figure 3, Figure 4, and Figure 5,
respectively. We only display the detected bounding boxes
with the score larger than 0.6. Different colors of the bound-
ing boxes indicate different object categories. Our method
works well with the occlusions, truncations, inter-class in-
terference and clustered background.

9. Detection Analysis on PASCAL VOC 2007

We use the detection analysis tool8 to understand the
performance of two ReﬁneDet models (i.e., ReﬁneDet320
and ReﬁneDet512) clearly. Figure 6 shows that ReﬁneDet
can detect various object categories with high quality (large
white area). The majority of its conﬁdent detections are
correct. The recall is around 95%-98%, and is much higher
with “weak” (0.1 jaccard overlap) criteria. Compared to
SSD, ReﬁneDet reduces the false positive errors at all as-
pects: (1) ReﬁneDet has less localization error (Loc), indi-
cating that ReﬁneDet can localize objects better because it
uses two-step cascade to regress the objects. (2) ReﬁneDet
has less confusion with background (BG), due to the neg-
ative anchor ﬁltering mechanism in the anchor reﬁnement
module (ARM). (3) ReﬁneDet has less confusion with sim-
ilar categories (Sim), beneﬁting from using two-stage fea-
tures to describe the objects, i.e., the features in the ARM
focus on the binary classiﬁcation (being an object or not),
while the features in the object detection module (ODM) fo-
cus on the multi-class classiﬁcation (background or object
classes).

Figure 7 demonstrates that ReﬁneDet is robust to dif-
ferent object sizes and aspect ratios. This is not surprising
because the object bounding boxes are obtained by the two-
step cascade regression, i.e., the ARM diversiﬁes the default
scales and aspect ratios of anchor boxes so that the ODM
is able to regress tougher objects (e.g., extra-small, extra-
large, extra-wide and extra-tall). However, as shown in Fig-
ure 7, there is still much room to improve the performance

8http://web.engr.illinois.edu/˜dhoiem/projects/

detectionAnalysis/

Table 5: Object detection results on the PASCAL VOC 2007 test set. All models use VGG-16 as the backbone network.

Data
07+12
07+12
07+12
07+12

Method
mAP aero bike bird boat bottle bus car
ReﬁneDet320
80.0 83.9 85.4 81.4 75.5 60.2 86.4 88.1 89.1 62.7 83.9 77.0 85.4 87.1 86.7
81.8 88.7 87.0 83.2 76.5 68.0 88.5 88.7 89.2 66.5 87.9 75.0 86.8 89.2 87.8
ReﬁneDet512
ReﬁneDet320+
83.1 89.5 87.9 84.9 79.7 70.0 87.5 89.1 89.8 69.8 87.1 76.4 86.6 88.6 88.4
83.8 88.5 89.1 85.5 79.8 72.4 89.5 89.5 89.9 69.9 88.9 75.9 87.4 89.6 89.0
ReﬁneDet512+
ReﬁneDet320 COCO+07+12 84.0 88.9 88.4 86.2 81.5 71.7 88.4 89.4 89.0 71.0 87.0 80.1 88.5 90.2 88.4
ReﬁneDet512 COCO+07+12 85.2 90.0 89.2 87.9 83.1 78.5 90.0 89.9 89.7 74.7 89.8 79.5 88.7 89.9 89.2
ReﬁneDet320+ COCO+07+12 85.6 90.2 89.0 87.6 84.6 78.0 89.4 89.7 89.9 74.7 89.8 80.5 89.0 89.7 89.6
ReﬁneDet512+ COCO+07+12 85.8 90.4 89.6 88.2 84.9 78.3 89.8 89.9 90.0 75.9 90.0 80.0 89.8 90.3 89.6

cat chair cow table dog horse mbike person plant sheep sofa train tv
82.6 55.3 82.7 78.5 88.1 79.4
84.7 56.2 83.2 78.7 88.1 82.3
85.3 62.4 83.7 82.3 89.0 83.1
86.2 63.9 86.2 81.0 88.6 84.4
86.7 61.2 85.2 83.8 89.1 85.5
87.8 63.1 86.4 82.3 89.5 84.7
87.8 65.5 87.9 84.2 88.6 86.3
88.3 66.2 87.8 83.5 89.3 85.2

Table 6: Object detection results on the PASCAL VOC 2012 test set. All models use VGG-16 as the backbone network.

Data
07++12
07++12
07++12
07++12

mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv
Method
85.7 53.3 84.3 73.1 87.4 73.9
78.1 90.4 84.1 79.8 66.8 56.1 83.1 82.7 90.7 61.7 82.4 63.8 89.4 86.9 85.9
ReﬁneDet320
87.8 58.0 86.3 72.5 88.7 76.6
80.1 90.2 86.8 81.8 68.0 65.6 84.9 85.0 92.2 62.0 84.4 64.9 90.6 88.3 87.2
ReﬁneDet512
89.4 62.0 88.5 75.9 90.0 80.0
82.7 92.0 88.4 84.9 74.0 69.5 86.0 88.0 93.3 67.0 86.2 68.3 92.1 89.7 88.9
ReﬁneDet320+
90.2 64.1 89.8 75.2 90.7 81.1
ReﬁneDet512+
83.5 92.2 89.4 85.0 74.1 70.8 87.0 88.7 94.0 68.6 87.1 68.2 92.5 90.8 89.4
89.4 59.6 87.9 78.1 91.1 80.0
ReﬁneDet320 COCO+07++12 82.7 93.1 88.2 83.6 74.4 65.1 87.1 87.1 93.7 67.4 86.1 69.4 91.5 90.6 91.4
91.4 66.0 91.2 75.4 91.8 83.0
ReﬁneDet512 COCO+07++12 85.0 94.0 90.0 86.9 76.9 74.1 89.7 89.8 94.2 69.7 90.0 68.5 92.6 92.8 91.5
ReﬁneDet320+ COCO+07++12 86.0 94.2 90.2 87.7 80.4 74.9 90.0 91.7 94.9 71.9 89.8 71.7 93.5 91.9 92.4
91.9 66.5 91.5 79.1 92.8 83.9
92.6 68.8 92.4 78.5 93.6 85.2
ReﬁneDet512+ COCO+07++12 86.8 94.7 91.5 88.8 80.4 77.6 90.4 92.3 95.6 72.5 91.6 69.9 93.9 93.5 92.4

Table 7: Object detection results on the MS COCO test-dev set.

Method
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+
ReﬁneDet320+
ReﬁneDet512+

Net
VGG-16
VGG-16
ResNet-101
ResNet-101
VGG-16
VGG-16
ResNet-101
ResNet-101

AP
29.4
33.0
32.0
36.4
35.2
37.6
38.6
41.8

AP50
49.2
54.5
51.4
57.5
56.1
58.7
59.9
62.9

AP75
31.3
35.5
34.2
39.5
37.7
40.8
41.7
45.7

APS
10.0
16.3
10.5
16.6
19.5
22.7
21.1
25.6

APM
32.0
36.3
34.7
39.9
37.2
40.3
41.7
45.1

APL
44.4
44.3
50.4
51.4
47.0
48.3
52.3
54.1

AR1
26.2
28.3
28.0
30.6
30.1
31.4
32.2
34.0

AR10
42.2
46.4
44.0
49.0
49.6
52.4
52.9
56.3

AR100
45.8
50.6
47.6
53.0
57.4
61.3
61.1
65.5

ARS
18.7
29.3
20.2
30.0
36.2
41.6
40.2
46.2

ARM
52.1
55.5
53.0
58.2
62.3
65.8
66.2
70.2

ARL
66.0
66.0
69.8
70.3
72.6
75.4
77.1
79.8

Figure 3: Qualitative results of ReﬁneDet512 on the PASCAL VOC 2007 test set (corresponding to 85.2% mAP). VGG-16
is used as the backbone network. The training data is 07+12+COCO.

Figure 4: Qualitative results of ReﬁneDet512 on the PASCAL VOC 2012 test set (corresponding to 85.0% mAP). VGG-16
is used as the backbone network. The training data is 07++12+COCO.

Figure 5: Qualitative results of ReﬁneDet512 on the MS COCO test-dev set (corresponding to 36.4% mAP). ResNet-101
is used as the backbone network. The training data is COCO trainval35k.

Figure 6: Visualization of the performance of ReﬁneDet512 on animals, vehicles, and furniture classes in the VOC 2007
test set. The top row shows the cumulative fraction of detections that are correct (Cor) or false positive due to poor
localization (Loc), confusion with similar categories (Sim), with others (Oth), or with background (BG). The solid red line
reﬂects the change of recall with strong criteria (0.5 jaccard overlap) as the number of detections increases. The dashed red
line is using the “weak” criteria (0.1 jaccard overlap). The bottom row shows the distribution of the top-ranked false positive
types.

Figure 7: Sensitivity and impact of different object characteristics on the VOC 2007 test set. The plot on the left shows the
effects of BBox Area per category, and the right plot shows the effect of Aspect Ratio. Key: BBox Area: XS=extra-small;
S=small; M=medium; L=large; XL =extra-large. Aspect Ratio: XT=extra-tall/narrow; T=tall; M=medium; W=wide; XW
=extra-wide.

8
1
0
2
 
n
a
J
 
3
 
 
]

V
C
.
s
c
[
 
 
3
v
7
9
8
6
0
.
1
1
7
1
:
v
i
X
r
a

Single-Shot Reﬁnement Neural Network for Object Detection

Shifeng Zhang1,2, Longyin Wen3, Xiao Bian3, Zhen Lei1,2, Stan Z. Li1,2
1 CBSR & NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China.
2 University of Chinese Academy of Sciences, Beijing, China.
3 GE Global Research, Niskayuna, NY.
{shifeng.zhang,zlei,szli}@nlpr.ia.ac.cn, {longyin.wen,xiao.bian}@ge.com

Abstract

For object detection,

the two-stage approach (e.g.,
Faster R-CNN) has been achieving the highest accuracy,
whereas the one-stage approach (e.g., SSD) has the ad-
vantage of high efﬁciency. To inherit the merits of both
while overcoming their disadvantages, in this paper, we pro-
pose a novel single-shot based detector, called ReﬁneDet,
that achieves better accuracy than two-stage methods and
maintains comparable efﬁciency of one-stage methods. Re-
ﬁneDet consists of two inter-connected modules, namely,
the anchor reﬁnement module and the object detection mod-
ule. Speciﬁcally, the former aims to (1) ﬁlter out nega-
tive anchors to reduce search space for the classiﬁer, and
(2) coarsely adjust the locations and sizes of anchors to
provide better initialization for the subsequent regressor.
The latter module takes the reﬁned anchors as the input
from the former to further improve the regression and pre-
dict multi-class label. Meanwhile, we design a transfer
connection block to transfer the features in the anchor re-
ﬁnement module to predict locations, sizes and class la-
bels of objects in the object detection module. The multi-
task loss function enables us to train the whole network
in an end-to-end way. Extensive experiments on PASCAL
VOC 2007, PASCAL VOC 2012, and MS COCO demon-
strate that ReﬁneDet achieves state-of-the-art detection ac-
curacy with high efﬁciency. Code is available at https:
//github.com/sfzhang15/RefineDet.

1. Introduction

Object detection has achieved signiﬁcant advances in re-
cent years, with the framework of deep neural networks
(DNN). The current DNN detectors of state-of-the-art can
be divided into two categories: (1) the two-stage approach,
including [3, 15, 36, 41], and (2) the one-stage approach,
including [30, 35]. In the two-stage approach, a sparse set
of candidate object boxes is ﬁrst generated, and then they
are further classiﬁed and regressed. The two-stage meth-

ods have been achieving top performances on several chal-
lenging benchmarks, including PASCAL VOC [8] and MS
COCO [29].

The one-stage approach detects objects by regular and
dense sampling over locations, scales and aspect ratios. The
main advantage of this is its high computational efﬁciency.
However, its detection accuracy is usually behind that of
the two-stage approach, one of the main reasons being due
to the class imbalance problem [28].

Some recent methods in the one-stage approach aim to
address the class imbalance problem, to improve the detec-
tion accuracy. Kong et al. [24] use the objectness prior con-
straint on convolutional feature maps to signiﬁcantly reduce
the search space of objects. Lin et al. [28] address the class
imbalance issue by reshaping the standard cross entropy
loss to focus training on a sparse set of hard examples and
down-weights the loss assigned to well-classiﬁed examples.
Zhang et al. [53] design a max-out labeling mechanism to
reduce false positives resulting from class imbalance.

In our opinion,

the current state-of-the-art two-stage
methods, e.g., Faster R-CNN [36], R-FCN [5], and FPN
[27], have three advantages over the one-stage methods as
follows: (1) using two-stage structure with sampling heuris-
tics to handle class imbalance; (2) using two-step cascade to
regress the object box parameters; (3) using two-stage fea-
tures to describe the objects1.
In this work, we design a
novel object detection framework, called ReﬁneDet, to in-
herit the merits of the two approaches (i.e., one-stage and
two-stage approaches) and overcome their shortcomings. It
improves the architecture of the one-stage approach, by us-
ing two inter-connected modules (see Figure 1), namely, the
anchor 2 reﬁnement module (ARM) and the object detection

1In case of Faster R-CNN, the features (excluding shared features) in
the ﬁrst stage (i.e., RPN) are trained for the binary classiﬁcation (being an
object or not), while the features (excluding shared features) in the sec-
ond stage(i.e., Fast R-CNN) are trained for the multi-class classiﬁcation
(background or object classes).

2We denote the reference bounding box as “anchor box”, which is also
called “anchor” for simplicity, as in [36]. However, in [30], it is called
“default box”.

1

Figure 1: Architecture of ReﬁneDet. For better visualization, we only display the layers used for detection. The celadon
parallelograms denote the reﬁned anchors associated with different feature layers. The stars represent the centers of the
reﬁned anchor boxes, which are not regularly paved on the image.

module (ODM). Speciﬁcally, the ARM is designed to (1)
identify and remove negative anchors to reduce search space
for the classiﬁer, and (2) coarsely adjust the locations and
sizes of anchors to provide better initialization for the subse-
quent regressor. The ODM takes the reﬁned anchors as the
input from the former to further improve the regression and
predict multi-class labels. As shown in Figure 1, these two
inter-connected modules imitate the two-stage structure and
thus inherit the three aforementioned advantages to produce
accurate detection results with high efﬁciency. In addition,
we design a transfer connection block (TCB) to transfer the
features3 in the ARM to predict locations, sizes, and class
labels of objects in the ODM. The multi-task loss function
enables us to train the whole network in an end-to-end way.
Extensive experiments on PASCAL VOC 2007, PAS-
CAL VOC 2012, and MS COCO benchmarks demonstrate
that ReﬁneDet outperforms the state-of-the-art methods.
Speciﬁcally, it achieves 85.8% and 86.8% mAPs on VOC
2007 and 2012, with VGG-16 network. Meanwhile, it out-
performs the previously best published results from both
one-stage and two-stage approaches by achieving 41.8%
AP4 on MS COCO test-dev with ResNet-101. In ad-

3The features in the ARM focus on distinguishing positive anchors
from background. We design the TCB to transfer the features in the ARM
to handle the more challenging tasks in the ODM, i.e., predict accurate
object locations, sizes and multi-class labels.

4Based on the evaluation protocol in MS COCO [29], AP is the sin-

dition, ReﬁneDet is time efﬁcient, i.e., it runs at 40.2 FPS
and 24.1 FPS on a NVIDIA Titan X GPU with the input
sizes 320 × 320 and 512 × 512 in inference.

The main contributions of this work are summarized as
follows.
(1) We introduce a novel one-stage framework
for object detection, composed of two inter-connected mod-
ules, i.e., the ARM and the ODM. This leads to performance
better than the two-stage approach while maintaining high
efﬁciency of the one-stage approach. (2) To ensure the ef-
fectiveness, we design the TCB to transfer the features in
the ARM to handle more challenging tasks, i.e., predict ac-
curate object locations, sizes and class labels, in the ODM.
(3) ReﬁneDet achieves the latest state-of-the-art results on
generic object detection (i.e., PASCAL VOC 2007 [10],
PASCAL VOC 2012 [11] and MS COCO [29]).

2. Related Work

Classical Object Detectors. Early object detection meth-
ods are based on the sliding-window paradigm, which ap-
ply the hand-crafted features and classiﬁers on dense image
grids to ﬁnd objects. As one of the most successful meth-
ods, Viola and Jones [47] use Haar feature and AdaBoost
to train a series of cascaded classiﬁers for face detection,

gle most important metric, which is computed by averaging over all 10
intersection over union (IoU) thresholds (i.e., in the range [0.5:0.95] with
uniform step size 0.05) of 80 categories.

achieving satisfactory accuracy with high efﬁciency. DPM
[12] is another popular method using mixtures of multi-
scale deformable part models to represent highly variable
object classes, maintaining top results on PASCAL VOC [8]
for many years. However, with the arrival of deep convolu-
tional network, the object detection task is quickly dom-
inated by the CNN-based detectors, which can be roughly
divided into two categories, i.e., the two-stage approach and
one-stage approach.
Two-Stage Approach. The two-stage approach consists of
two parts, where the ﬁrst one (e.g., Selective Search [46],
EdgeBoxes [55], DeepMask [32, 33], RPN [36]) generates a
sparse set of candidate object proposals, and the second one
determines the accurate object regions and the correspond-
ing class labels using convolutional networks. Notably, the
two-stage approach (e.g., R-CNN [16], SPPnet [18], Fast R-
CNN [15] to Faster R-CNN [36]) achieves dominated per-
formance on several challenging datasets (e.g., PASCAL
VOC 2012 [11] and MS COCO [29]). After that, numer-
ous effective techniques are proposed to further improve the
performance, such as architecture diagram [5, 26, 54], train-
ing strategy [41, 48], contextual reasoning [1, 14, 40, 50]
and multiple layers exploiting [3, 25, 27, 42].
One-Stage Approach. Considering the high efﬁciency, the
one-stage approach attracts much more attention recently.
Sermanet et al. [38] present the OverFeat method for clas-
siﬁcation, localization and detection based on deep Con-
vNets, which is trained end-to-end, from raw pixels to ul-
timate categories. Redmon et al. [34] use a single feed-
forward convolutional network to directly predict object
classes and locations, called YOLO, which is extremely
fast. After that, YOLOv2 [35] is proposed to improve
YOLO in several aspects, i.e., add batch normalization on
all convolution layers, use high resolution classiﬁer, use
convolution layers with anchor boxes to predict bounding
boxes instead of the fully connected layers, etc. Liu et al.
[30] propose the SSD method, which spreads out anchors
of different scales to multiple layers within a ConvNet and
enforces each layer to focus on predicting objects of a cer-
tain scale. DSSD [13] introduces additional context into
SSD via deconvolution to improve the accuracy. DSOD
[39] designs an efﬁcient framework and a set of principles to
learn object detectors from scratch, following the network
structure of SSD. To improve the accuracy, some one-stage
methods [24, 28, 53] aim to address the extreme class im-
balance problem by re-designing the loss function or clas-
siﬁcation strategies. Although the one-stage detectors have
made good progress, their accuracy still trails that of two-
stage methods.

3. Network Architecture

Refer to the overall network architecture shown in Fig-
ure 1. Similar to SSD [30], ReﬁneDet is based on a feed-

forward convolutional network that produces a ﬁxed num-
ber of bounding boxes and the scores indicating the pres-
ence of different classes of objects in those boxes, followed
by the non-maximum suppression to produce the ﬁnal re-
sult. ReﬁneDet is formed by two inter-connected modules,
i.e., the ARM and the ODM. The ARM aims to remove neg-
ative anchors so as to reduce search space for the classiﬁer
and also coarsely adjust the locations and sizes of anchors
to provide better initialization for the subsequent regressor,
whereas ODM aims to regress accurate object locations and
predict multi-class labels based on the reﬁned anchors. The
ARM is constructed by removing the classiﬁcation layers
and adding some auxiliary structures of two base networks
(i.e., VGG-16 [43] and ResNet-101 [19] pretrained on Im-
ageNet [37]) to meet our needs. The ODM is composed of
the outputs of TCBs followed by the prediction layers (i.e.,
the convolution layers with 3 × 3 kernel size), which gener-
ates the scores for object classes and shape offsets relative to
the reﬁned anchor box coordinates. The following explain
three core components in ReﬁneDet, i.e., (1) transfer con-
nection block (TCB), converting the features from the ARM
to the ODM for detection; (2) two-step cascaded regression,
accurately regressing the locations and sizes of objects; (3)
negative anchor ﬁltering, early rejecting well-classiﬁed neg-
ative anchors and mitigate the imbalance issue.

Transfer Connection Block. To link between the ARM
and ODM, we introduce the TCBs to convert features of dif-
ferent layers from the ARM, into the form required by the
ODM, so that the ODM can share features from the ARM.
Notably, from the ARM, we only use the TCBs on the fea-
ture maps associated with anchors. Another function of the
TCBs is to integrate large-scale context [13, 27] by adding
the high-level features to the transferred features to improve
detection accuracy. To match the dimensions between them,
we use the deconvolution operation to enlarge the high-level
feature maps and sum them in the element-wise way. Then,
we add a convolution layer after the summation to ensure
the discriminability of features for detection. The architec-
ture of the TCB is shown in Figure 2.

Two-Step Cascaded Regression. Current one-stage meth-
ods [13, 24, 30] rely on one-step regression based on various
feature layers with different scales to predict the locations
and sizes of objects, which is rather inaccurate in some chal-
lenging scenarios, especially for the small objects. To that
end, we present a two-step cascaded regression strategy to
regress the locations and sizes of objects. That is, we use
the ARM to ﬁrst adjust the locations and sizes of anchors to
provide better initialization for the regression in the ODM.
Speciﬁcally, we associate n anchor boxes with each regu-
larly divided cell on the feature map. The initial position of
each anchor box relative to its corresponding cell is ﬁxed.
At each feature map cell, we predict four offsets of the re-
ﬁned anchor boxes relative to the original tiled anchors and

adapt to variations of objects. That is, we randomly ex-
pand and crop the original training images with additional
random photometric distortion [20] and ﬂipping to generate
the training samples. Please refer to [30] for more details.
Backbone Network. We use VGG-16 [43] and ResNet-101
[19] as the backbone networks in our ReﬁneDet, which are
pretrained on the ILSVRC CLS-LOC dataset [37]. Notably,
ReﬁneDet can also work on other pretrained networks, such
as Inception V2 [22], Inception ResNet [44], and ResNeXt-
101 [49]. Similar to DeepLab-LargeFOV [4], we convert
fc6 and fc7 of VGG-16 to convolution layers conv fc6 and
conv fc7 via subsampling parameters. Since conv4 3 and
conv5 3 have different feature scales compared to other lay-
ers, we use L2 normalization [31] to scale the feature norms
in conv4 3 and conv5 3 to 10 and 8, then learn the scales
during back propagation. Meanwhile, to capture high-level
information and drive object detection at multiple scales,
we also add two extra convolution layers (i.e., conv6 1 and
conv6 2) to the end of the truncated VGG-16 and one extra
residual block (i.e., res6) to the end of the truncated ResNet-
101, respectively.
Anchors Design and Matching. To handle different scales
of objects, we select four feature layers with the total stride
sizes 8, 16, 32, and 64 pixels for both VGG-16 and ResNet-
1015, associated with several different scales of anchors for
prediction. Each feature layer is associated with one spe-
ciﬁc scale of anchors (i.e., the scale is 4 times of the to-
tal stride size of the corresponding layer) and three aspect
ratios (i.e., 0.5, 1.0, and 2.0). We follow the design of
anchor scales over different layers in [53], which ensures
that different scales of anchors have the same tiling den-
sity [51, 52] on the image. Meanwhile, during the train-
ing phase, we determine the correspondence between the
anchors and ground truth boxes based on the jaccard over-
lap [7], and train the whole network end-to-end accordingly.
Speciﬁcally, we ﬁrst match each ground truth to the anchor
box with the best overlap score, and then match the anchor
boxes to any ground truth with overlap higher than 0.5.
Hard Negative Mining. After matching step, most of the
anchor boxes are negatives, even for the ODM, where some
easy negative anchors are rejected by the ARM. Similar
to SSD [30], we use hard negative mining to mitigate the
extreme foreground-background class imbalance, i.e., we
select some negative anchor boxes with top loss values to
make the ratio between the negatives and positives below
3 : 1, instead of using all negative anchors or randomly se-
lecting the negative anchors in training.
Loss Function. The loss function for ReﬁneDet consists of
two parts, i.e., the loss in the ARM and the loss in the ODM.

5For the VGG-16 base network, the conv4 3, conv5 3, conv fc7, and
conv6 2 feature layers are used to predict the locations, sizes and con-
ﬁdences of objects. While for the ResNet-101 base network, res3b3,
res4b22, res5c, and res6 are used for prediction.

Figure 2: The overview of the transfer connection block.

two conﬁdence scores indicating the presence of foreground
objects in those boxes. Thus, we can yield n reﬁned anchor
boxes at each feature map cell.

After obtaining the reﬁned anchor boxes, we pass them
to the corresponding feature maps in the ODM to further
generate object categories and accurate object locations and
sizes, as shown in Figure 1. The corresponding feature
maps in the ARM and the ODM have the same dimension.
We calculate c class scores and the four accurate offsets of
objects relative to the reﬁned anchor boxes, yielding c + 4
outputs for each reﬁned anchor boxes to complete the de-
tection task. This process is similar to the default boxes
used in SSD [30]. However, in contrast to SSD [30] di-
rectly uses the regularly tiled default boxes for detection,
ReﬁneDet uses two-step strategy, i.e., the ARM generates
the reﬁned anchor boxes, and the ODM takes the reﬁned
anchor boxes as input for further detection, leading to more
accurate detection results, especially for the small objects.
Negative Anchor Filtering. To early reject well-classiﬁed
negative anchors and mitigate the imbalance issue, we de-
sign a negative anchor ﬁltering mechanism. Speciﬁcally, in
training phase, for a reﬁned anchor box, if its negative con-
ﬁdence is larger than a preset threshold θ (i.e., set θ = 0.99
empirically), we will discard it in training the ODM. That is,
we only pass the reﬁned hard negative anchor boxes and re-
ﬁned positive anchor boxes to train the ODM. Meanwhile,
in the inference phase, if a reﬁned anchor box is assigned
with a negative conﬁdence larger than θ, it will be discarded
in the ODM for detection.

4. Training and Inference

Data Augmentation. We use several data augmentation
strategies presented in [30] to construct a robust model to

Nodm

For the ARM, we assign a binary class label (of being an
object or not) to each anchor and regress its location and
size simultaneously to get the reﬁned anchor. After that, we
pass the reﬁned anchors with the negative conﬁdence less
than the threshold to the ODM to further predict object cat-
egories and accurate object locations and sizes. With these
deﬁnitions, we deﬁne the loss function as:
(cid:0) (cid:80)
(cid:0) (cid:80)

L({pi}, {xi}, {ci}, {ti}) = 1
Narm
i )(cid:1) + 1
+ (cid:80)
i ≥ 1]Lr(xi, g∗
i )(cid:1)
+ (cid:80)
i ≥ 1]Lr(ti, g∗

i Lb(pi, [l∗
i ≥ 1])
i Lm(ci, l∗
i )

i[l∗
i[l∗

i ≥ 1]) = 0 and Lr(xi, g∗
i ) = 0 and Lr(ti, g∗

(1)
where i is the index of anchor in a mini-batch, l∗
is the
i
ground truth class label of anchor i, g∗
i is the ground truth
location and size of anchor i. pi and xi are the predicted
conﬁdence of the anchor i being an object and reﬁned co-
ordinates of the anchor i in the ARM. ci and ti are the
predicted object class and coordinates of the bounding box
in the ODM. Narm and Nodm are the numbers of positive
anchors in the ARM and ODM, respectively. The binary
classiﬁcation loss Lb is the cross-entropy/log loss over two
classes (object vs. not object), and the multi-class classiﬁ-
cation loss Lm is the softmax loss over multiple classes con-
ﬁdences. Similar to Fast R-CNN [15], we use the smooth
L1 loss as the regression loss Lr. The Iverson bracket indi-
cator function [l∗
i ≥ 1] outputs 1 when the condition is true,
i.e., l∗
i ≥ 1 (the anchor is not the negative), and 0 other-
wise. Hence [l∗
i ≥ 1]Lr indicates that the regression loss is
ignored for negative anchors. Notably, if Narm = 0, we set
Lb(pi, [l∗
i ) = 0; and if Nodm = 0,
we set Lm(ci, l∗
i ) = 0 accordingly.
Optimization. As mentioned above, the backbone network
(e.g., VGG-16 and ResNet-101) in our ReﬁneDet method is
pretrained on the ILSVRC CLS-LOC dataset [37]. We use
the “xavier” method [17] to randomly initialize the parame-
ters in the two extra added convolution layers (i.e., conv6 1
and conv6 2) of VGG-16 based ReﬁneDet, and draw the pa-
rameters from a zero-mean Gaussian distribution with stan-
dard deviation 0.01 for the extra residual block (i.e., res6) of
ResNet-101 based ReﬁneDet. We set the default batch size
to 32 in training. Then, the whole network is ﬁne-tuned us-
ing SGD with 0.9 momentum and 0.0005 weight decay. We
set the initial learning rate to 10−3, and use slightly differ-
ent learning rate decay policy for different dataset, which
will be described in details later.
Inference. At inference phase, the ARM ﬁrst ﬁlters out the
regularly tiled anchors with the negative conﬁdence scores
larger than the threshold θ, and then reﬁnes the locations
and sizes of remaining anchors. After that, the ODM takes
over these reﬁned anchors, and outputs top 400 high con-
ﬁdent detections per image. Finally, we apply the non-
maximum suppression with jaccard overlap of 0.45 per
class and retain the top 200 high conﬁdent detections per
image to produce the ﬁnal detection results.

5. Experiments

Experiments are conducted on three datasets: PASCAL
VOC 2007, PASCAL VOC 2012 and MS COCO. The PAS-
CAL VOC and MS COCO datasets include 20 and 80 ob-
ject classes, respectively. The classes in PASCAL VOC are
the subset of that in MS COCO. We implement ReﬁneDet
in Caffe [23]. All the training and testing codes and the
trained models are available at https://github.com/
sfzhang15/RefineDet.

5.1. PASCAL VOC 2007

All models are trained on the VOC 2007 and VOC 2012
trainval sets, and tested on the VOC 2007 test set. We
set the learning rate to 10−3 for the ﬁrst 80k iterations, and
decay it to 10−4 and 10−5 for training another 20k and 20k
iterations, respectively. We use the default batch size 32 in
training, and only use VGG-16 as the backbone network for
all the experiments on the PASCAL VOC dataset, including
VOC 2007 and VOC 2012.

We compare ReﬁneDet6 with the state-of-the-art detec-
tors in Table 1. With low dimension input (i.e., 320 × 320),
ReﬁneDet produces 80.0% mAP without bells and whis-
tles, which is the ﬁrst method achieving above 80% mAP
with such small input images, much better than several
modern objectors. By using larger input size 512 × 512,
ReﬁneDet achieves 81.8% mAP, surpassing all one-stage
methods, e.g., RON384 [24], SSD513 [13], DSSD513 [13],
etc. Comparing to the two-stage methods, ReﬁneDet512
performs better than most of them except CoupleNet [54],
which is based on ResNet-101 and uses larger input size
(i.e., ∼ 1000 × 600) than our ReﬁneDet512. As pointed
out in [21], the input size signiﬁcantly inﬂuences detection
accuracy. The reason is that high resolution inputs make
the detectors “seeing” small objects clearly to increase suc-
cessful detections. To reduce the impact of input size for a
fair comparison, we use the multi-scale testing strategy to
evaluate ReﬁneDet, achieving 83.1% (ReﬁneDet320+) and
83.8% (ReﬁneDet512+) mAPs, which are much better than
the state-of-the-art methods.

5.1.1 Run Time Performance

We present the inference speed of ReﬁneDet and the state-
of-the-art methods in the ﬁfth column of Table 1. The speed
is evaluated with batch size 1 on a machine with NVIDIA
Titan X, CUDA 8.0 and cuDNN v6. As shown in Table 1,
we ﬁnd that ReﬁneDet processes an image in 24.8ms (40.3
FPS) and 41.5ms (24.1 FPS) with input sizes 320 × 320
and 512 × 512, respectively. To the best of our knowledge,

6Due to the shortage of computational resources, we only train Re-
ﬁneDet with two kinds of input size, i.e., 320 × 320 and 512 × 512. We
believe the accuracy of ReﬁneDet can be further improved using larger
input images.

Table 1: Detection results on PASCAL VOC dataset. For VOC 2007, all methods are trained on VOC 2007 and VOC 2012
trainval sets and tested on VOC 2007 test set. For VOC 2012, all methods are trained on VOC 2007 and VOC 2012
trainval sets plus VOC 2007 test set, and tested on VOC 2012 test set. Bold fonts indicate the best mAP.

Method

Backbone

Input size

#Boxes

FPS

mAP (%)

VOC 2007

VOC 2012

two-stage:
Fast R-CNN[15]
Faster R-CNN[36]
OHEM[41]
HyperNet[25]
Faster R-CNN[36]
ION[1]
MR-CNN[14]
R-FCN[5]
CoupleNet[54]

one-stage:
YOLO[34]
RON384[24]
SSD321[13]
SSD300∗[30]
DSOD300[39]
YOLOv2[35]
DSSD321[13]
SSD512∗[30]
SSD513[13]
DSSD513[13]
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+

VGG-16
VGG-16
VGG-16
VGG-16
ResNet-101
VGG-16
VGG-16
ResNet-101
ResNet-101

GoogleNet [45]
VGG-16
ResNet-101
VGG-16
DS/64-192-48-1
Darknet-19
ResNet-101
VGG-16
ResNet-101
ResNet-101
VGG-16
VGG-16
VGG-16
VGG-16

∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600
∼ 1000 × 600

448 × 448
384 × 384
321 × 321
300 × 300
300 × 300
544 × 544
321 × 321
512 × 512
513 × 513
513 × 513
320 × 320
512 × 512
-
-

∼ 2000
300
300
100
300
4000
250
300
300

98
30600
17080
8732
8732
845
17080
24564
43688
43688
6375
16320
-
-

0.5
7
7
0.88
2.4
1.25
0.03
9
8.2

45
15
11.2
46
17.4
40
9.5
19
6.8
5.5
40.3
24.1
-
-

70.0
73.2
74.6
76.3
76.4
76.5
78.2
80.5
82.7

63.4
75.4
77.1
77.2
77.7
78.6
78.6
79.8
80.6
81.5
80.0
81.8
83.1
83.8

68.4
70.4
71.9
71.4
73.8
76.4
73.9
77.6
80.4

57.9
73.0
75.4
75.8
76.3
73.4
76.3
78.5
79.4
80.0
78.1
80.1
82.7
83.5

ReﬁneDet is the ﬁrst real-time method to achieve detection
accuracy above 80% mAP on PASCAL VOC 2007. Com-
paring to SSD, RON, DSSD and DSOD, ReﬁneDet asso-
ciates fewer anchor boxes on the feature maps (e.g., 24564
anchor boxes in SSD512∗[30] vs. 16320 anchor boxes in
ReﬁneDet512). However, ReﬁneDet still achieves top accu-
racy with high efﬁciency, mainly thanks to the design of two
inter-connected modules, (e.g., two-step regression), which
enables ReﬁneDet to adapt to different scales and aspect ra-
tios of objects. Meanwhile, only YOLO and SSD300∗ are
slightly faster than our ReﬁneDet320, but their accuracy are
16.6% and 2.5% worse than ours. In summary, ReﬁneDet
achieves the best trade-off between accuracy and speed.

5.1.2 Ablation Study

To demonstrate the effectiveness of different components
in ReﬁneDet, we construct four variants and evaluate them
on VOC 2007, shown in Table 3. Speciﬁcally, for a fair
comparison, we use the same parameter settings and input
size (320 × 320) in evaluation. All models are trained on
VOC 2007 and VOC 2012 trainval sets, and tested on
VOC 2007 test set.

Negative Anchor Filtering. To demonstrate the effective-
ness of the negative anchor ﬁltering, we set the conﬁdence

threshold θ of the anchors to be negative to 1.0 in both train-
ing and testing.
In this case, all reﬁned anchors will be
sent to the ODM for detection. Other parts of ReﬁneDet re-
main unchanged. Removing negative anchor ﬁltering leads
to 0.5% drop in mAP (i.e., 80.0% vs. 79.5%). The reason
is that most of these well-classiﬁed negative anchors will be
ﬁltered out during training, which solves the class imbal-
ance issue to some extent.

Two-Step Cascaded Regression. To validate the effective-
ness of the two-step cascaded regression, we redesign the
network structure by directly using the regularly paved an-
chors instead of the reﬁned ones from the ARM (see the
fourth column in Table 3). As shown in Table 3, we ﬁnd that
mAP is reduced from 79.5% to 77.3%. This sharp decline
(i.e., 2.2%) demonstrates that the two-step anchor cascaded
regression signiﬁcantly help promote the performance.

Transfer Connection Block. We construct a network by
cutting the TCBs in ReﬁneDet and redeﬁning the loss func-
tion in the ARM to directly detect multi-class of objects,
just like SSD, to demonstrate the effect of the TCB. The
detection accuracy of the model is presented in the ﬁfth col-
umn in Table 3. We compare the results in the fourth and
ﬁfth columns in Table 3 (77.3% vs. 76.2%) and ﬁnd that
the TCB improves the mAP by 1.1%. The main reason is

Table 2: Detection results on MS COCO test-dev set. Bold fonts indicate the best performance.

Method
two-stage:
Fast R-CNN [15]
Faster R-CNN [36]
OHEM [41]
ION [1]
OHEM++ [41]
R-FCN [5]
CoupleNet [54]
Faster R-CNN by G-RMI [21]
Faster R-CNN+++ [19]
Faster R-CNN w FPN [27]
Faster R-CNN w TDM [42]
Deformable R-FCN [6]
umd det [2]
G-RMI [21]

one-stage:
YOLOv2 [35]
SSD300∗ [30]
RON384++ [24]
SSD321 [13]
DSSD321 [13]
SSD512∗ [30]
SSD513 [13]
DSSD513 [13]
RetinaNet500 [28]
RetinaNet800 [28]∗
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+
ReﬁneDet320+
ReﬁneDet512+

Data

Backbone

AP

AP50

AP75

APS

APM

APL

train
trainval
trainval
train
trainval
trainval
trainval
-
trainval
trainval35k
trainval
trainval
trainval
trainval32k

trainval35k
trainval35k
trainval
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k
trainval35k

VGG-16
VGG-16
VGG-16
VGG-16
VGG-16
ResNet-101
ResNet-101
Inception-ResNet-v2[44]
ResNet-101-C4
ResNet-101-FPN
Inception-ResNet-v2-TDM
Aligned-Inception-ResNet
ResNet-101
Ensemble of Five Models

DarkNet-19[35]
VGG-16
VGG-16
ResNet-101
ResNet-101
VGG-16
ResNet-101
ResNet-101
ResNet-101
ResNet-101-FPN
VGG-16
VGG-16
ResNet-101
ResNet-101
VGG-16
VGG-16
ResNet-101
ResNet-101

19.7
21.9
22.6
23.6
25.5
29.9
34.4
34.7
34.9
36.2
36.8
37.5
40.8
41.6

21.6
25.1
27.4
28.0
28.0
28.8
31.2
33.2
34.4
39.1
29.4
33.0
32.0
36.4
35.2
37.6
38.6
41.8

35.9
42.7
42.5
43.2
45.9
51.9
54.8
55.5
55.7
59.1
57.7
58.0
62.4
61.9

44.0
43.1
49.5
45.4
46.1
48.5
50.4
53.3
53.1
59.1
49.2
54.5
51.4
57.5
56.1
58.7
59.9
62.9

-
-
22.2
23.6
26.1
-
37.2
36.7
37.4
39.0
39.2
40.8
44.9
45.4

19.2
25.8
27.1
29.3
29.2
30.3
33.3
35.2
36.8
42.3
31.3
35.5
34.2
39.5
37.7
40.8
41.7
45.7

-
-
5.0
6.4
7.4
10.8
13.4
13.5
15.6
18.2
16.2
19.4
23.0
23.9

5.0
6.6
-
6.2
7.4
10.9
10.2
13.0
14.7
21.8
10.0
16.3
10.5
16.6
19.5
22.7
21.1
25.6

-
-
23.7
24.1
27.7
32.8
38.1
38.1
38.7
39.0
39.8
40.1
43.4
43.5

22.4
25.9
-
28.3
28.1
31.8
34.5
35.4
38.5
42.7
32.0
36.3
34.7
39.9
37.2
40.3
41.7
45.1

-
-
37.9
38.3
40.3
45.0
50.8
52.0
50.9
48.2
52.1
52.5
53.2
54.9

35.5
41.4
-
49.3
47.6
43.5
49.8
51.1
49.1
50.2
44.4
44.3
50.4
51.4
47.0
48.3
52.3
54.1

∗ This entry reports the single model accuracy of RetinaNet method, trained with scale jitter and for 1.5× longer than RetinaNet500.

Table 3: Effectiveness of various designs. All models are
trained on VOC 2007 and VOC 2012 trainval set and
tested on VOC 2007 test set.

Component
negative anchor ﬁltering?
two-step cascaded regression?
transfer connection block?
mAP (%)

!
!
!
80.0

!
!
79.5

ReﬁneDet320

!
77.3

76.2

that the model can inherit the discriminative features from
the ARM, and integrate large-scale context information to
improve the detection accuracy by using the TCB.

5.2. PASCAL VOC 2012

Following the protocol of VOC 2012, we submit the de-
tection results of ReﬁneDet to the public testing server for
evaluation. We use VOC 2007 trainval set and test
set plus VOC 2012 trainval set (21, 503 images) for

training, and test on VOC 2012 test set (10, 991 images).
We use the default batch size 32 in training. Meanwhile, we
set the learning rate to 10−3 in the ﬁrst 160k iterations, and
decay it to 10−4 and 10−5 for another 40k and 40k itera-
tions.

Table 1 shows the accuracy of the proposed ReﬁneDet al-
gorithm, as well as the state-of-the-art methods. Among the
methods fed with input size 320 × 320, ReﬁneDet320 ob-
tains the top 78.1% mAP, which is even better than most of
those two-stage methods using about 1000 × 600 input size
(e.g., 70.4% mAP of Faster R-CNN [36] and 77.6% mAP
of R-FCN [5]). Using the input size 512 × 512, ReﬁneDet
improves mAP to 80.1%, which is surpassing all one-stage
methods and only slightly lower than CoupleNet [54] (i.e.,
80.4%). CoupleNet uses ResNet-101 as base network with
1000 × 600 input size. To reduce the impact of input size
for a fair comparison, we also use multi-scale testing to
evaluate ReﬁneDet and obtain the state-of-the-art mAPs of
82.7% (ReﬁneDet320+) and 83.5% (ReﬁneDet512+).

Table 4: Detection results on PASCAL VOC dataset. All
models are pre-trained on MS COCO, and ﬁne-tuned on
PASCAL VOC. Bold fonts indicate the best mAP.

mAP (%)
VOC 2007 test VOC 2012 test

Method

Backbone

two-stage:
Faster R-CNN[36]
OHEM++[41]
R-FCN[5]

VGG-16
VGG-16
ResNet-101

VGG-16
VGG-16
VGG-16

one-stage:
SSD300[30]
SSD512[30]
RON384++[24]
DSOD300[39] DS/64-192-48-1
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+

VGG-16
VGG-16
VGG-16
VGG-16

5.3. MS COCO

78.8
-
83.6

81.2
83.2
81.3
81.7
84.0
85.2
85.6
85.8

75.9
80.1
82.0

79.3
82.2
80.7
79.3
82.7
85.0
86.0
86.8

In addition to PASCAL VOC, we also evaluate Re-
ﬁneDet on MS COCO [29]. Unlike PASCAL VOC, the
detection methods using ResNet-101 always achieve bet-
ter performance than those using VGG-16 on MS COCO.
Thus, we also report the results of ResNet-101 based Re-
ﬁneDet. Following the protocol in MS COCO, we use the
trainval35k set [1] for training and evaluate the results
from test-dev evaluation server. We set the batch size to
32 in training7, and train the model with 10−3 learning rate
for the ﬁrst 280k iterations, then 10−4 and 10−5 for another
80k and 40k iterations, respectively.

Table 7 shows the results on MS COCO test-dev set.
ReﬁneDet320 with VGG-16 produces 29.4% AP that is bet-
ter than all other methods based on VGG-16 (e.g., SSD512∗
[30] and OHEM++ [41]). The accuracy of ReﬁneDet can
be improved to 33.0% by using larger input size (i.e.,
512 × 512), which is much better than several modern ob-
ject detectors, e.g., Faster R-CNN [36] and SSD512∗ [30].
Meanwhile, using ResNet-101 can further improve the per-
formance of ReﬁneDet, i.e., ReﬁneDet320 with ResNet-101
achieves 32.0% AP and ReﬁneDet512 achieves 36.4% AP,
exceeding most of the detection methods except Faster R-
CNN w TDM [42], Deformable R-FCN [6], RetinaNet800
[28], umd det [2], and G-RMI [21]. All these methods use a
much bigger input images for both training and testing (i.e.,
1000×600 or 800×800) than our ReﬁneDet (i.e., 320×320
and 512 × 512). Similar to PASCAL VOC, we also report
the multi-scale testing AP results of ReﬁneDet for fair com-
parison in Table 7, i.e., 35.2% (ReﬁneDet320+ with VGG-
16), 37.6% (ReﬁneDet512+ with VGG-16), 38.6% (Re-

7Due to the memory issue, we reduce the batch size to 20 (which is the
largest batch size we can use for training on a machine with 4 NVIDIA
M40 GPUs) to train the ResNet-101 based ReﬁneDet with the input size
512 × 512, and train the model with 10−3 learning rate for the ﬁrst 400k
iterations, then 10−4 and 10−5 for another 80k and 60k iterations.

ﬁneDet320+ with ResNet-101) and 41.8% (ReﬁneDet512+
with ResNet-101). The best performance of ReﬁneDet is
41.8%, which is the state-of-the-art, surpassing all pub-
lished two-stage and one-stage approaches. Although the
second best detector G-RMI [21] ensembles ﬁve Faster R-
CNN models, it still produces 0.2% lower AP than Re-
ﬁneDet using a single model. Comparing to the third and
fourth best detectors, i.e., umd det [2] and RetinaNet800
[28], ReﬁneDet produces 1.0% and 2.7% higher APs. In
addition, the main contribution: focal loss in RetinaNet800,
is complementary to our method. We believe that it can be
used in ReﬁneNet to further improve the performance.

5.4. From MS COCO to PASCAL VOC

We study how the MS COCO dataset help the detec-
tion accuracy on PASCAL VOC. Since the object classes
in PASCAL VOC are the subset of MS COCO, we directly
ﬁne-tune the detection models pretrained on MS COCO via
subsampling the parameters, which achieves 84.0% mAP
(ReﬁneDet320) and 85.2% mAP (ReﬁneDet512) on VOC
2007 test set, and 82.7% mAP (ReﬁneDet320) and 85.0%
mAP (ReﬁneDet512) on VOC 2012 test set, shown in Ta-
ble 4. After using the multi-scale testing, the detection ac-
curacy are promoted to 85.6%, 85.8%, 86.0% and 86.8%,
respectively. As shown in Table 4, using the training data in
MS COCO and PASCAL VOC, our ReﬁneDet obtains the
top mAP scores on both VOC 2007 and VOC 2012. Most
important, our single model ReﬁneNet512+ based on VGG-
16 ranks as the top 5 on the VOC 2012 Leaderboard (see
[9]), which is the best accuracy among all one-stage meth-
ods. Other two-stage methods achieving better results are
based on much deeper networks (e.g., ResNet-101 [19] and
ResNeXt-101 [49]) or using ensemble mechanism.

6. Conclusions

In this paper, we present a single-shot reﬁnement neu-
ral network based detector, which consists of two inter-
connected modules, i.e., the ARM and the ODM. The ARM
aims to ﬁlter out the negative anchors to reduce search space
for the classiﬁer and also coarsely adjust the locations and
sizes of anchors to provide better initialization for the subse-
quent regressor, while the ODM takes the reﬁned anchors as
the input from the former ARM to regress the accurate ob-
ject locations and sizes and predict the corresponding multi-
class labels. The whole network is trained in an end-to-end
fashion with the multi-task loss. We carry out several exper-
iments on PASCAL VOC 2007, PASCAL VOC 2012, and
MS COCO datasets to demonstrate that ReﬁneDet achieves
the state-of-the-art detection accuracy with high efﬁciency.
In the future, we plan to employ ReﬁneDet to detect some
other speciﬁc kinds of objects, e.g., pedestrian, vehicle, and
face, and introduce the attention mechanism in ReﬁneDet to
further improve the performance.

References

[1] S. Bell, C. L. Zitnick, K. Bala, and R. B. Girshick. Inside-
outside net: Detecting objects in context with skip pooling
and recurrent neural networks. In CVPR, pages 2874–2883,
2016. 3, 6, 7, 8

[2] N. Bodla, B. Singh, R. Chellappa, and L. S. Davis. Improv-
ing object detection with one line of code. In ICCV, 2017. 7,
8

[3] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos. A uniﬁed
multi-scale deep convolutional neural network for fast object
detection. In ECCV, pages 354–370, 2016. 1, 3

[4] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille. Semantic image segmentation with deep convolu-
tional nets and fully connected crfs. In ICLR, 2015. 4
[5] J. Dai, Y. Li, K. He, and J. Sun. R-FCN: object detection via
region-based fully convolutional networks. In NIPS, pages
379–387, 2016. 1, 3, 6, 7, 8

[6] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei.
Deformable convolutional networks. In ICCV, 2017. 7, 8
[7] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable
object detection using deep neural networks. In CVPR, pages
2155–2162, 2014. 4

[8] M. Everingham, L. J. V. Gool, C. K. I. Williams, J. M. Winn,
and A. Zisserman. The pascal visual object classes (VOC)
challenge. IJCV, 88(2):303–338, 2010. 1, 3

[9] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman.
The Leaderboard of the PASCAL
Visual Object Classes Challenge 2012 (VOC2012). http:
//host.robots.ox.ac.uk:8080/leaderboard/
displaylb.php?challengeid=11&compid=4.
Online; accessed 1 October 2017. 8

[10] M. Everingham, L. Van Gool, C. K.

I. Williams,
J. Winn, and A. Zisserman. The PASCAL Visual Ob-
ject Classes Challenge 2007 (VOC2007) Results. http:
//www.pascal-network.org/challenges/VOC/
voc2007/workshop/index.html. Online; accessed
1 October 2017. 2

[11] M. Everingham, L. Van Gool, C. K.

I. Williams,
J. Winn, and A. Zisserman. The PASCAL Visual Ob-
ject Classes Challenge 2012 (VOC2012) Results. http:
//www.pascal-network.org/challenges/VOC/
voc2012/workshop/index.html. Online; accessed
1 October 2017. 2, 3

[12] P. F. Felzenszwalb, R. B. Girshick, D. A. McAllester, and
D. Ramanan. Object detection with discriminatively trained
part-based models. TPAMI, 32(9):1627–1645, 2010. 3
[13] C. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg.
CoRR,

DSSD : Deconvolutional single shot detector.
abs/1701.06659, 2017. 3, 5, 6, 7

[14] S. Gidaris and N. Komodakis. Object detection via a multi-
region and semantic segmentation-aware CNN model.
In
ICCV, pages 1134–1142, 2015. 3, 6

[17] X. Glorot and Y. Bengio. Understanding the difﬁculty of
In AISTATS,

training deep feedforward neural networks.
pages 249–256, 2010. 5

[18] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling
in deep convolutional networks for visual recognition.
In
ECCV, pages 346–361, 2014. 3

[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, pages 770–778, 2016. 3, 4,
7, 8

[20] A. G. Howard.

Some improvements on deep convolu-
tional neural network based image classiﬁcation. CoRR,
abs/1312.5402, 2013. 4

[21] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,
A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, and
K. Murphy. Speed/accuracy trade-offs for modern convolu-
tional object detectors. In CVPR, 2017. 5, 7, 8

[22] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, pages 448–456, 2015. 4

[23] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. B.
Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. In ACMMM,
pages 675–678, 2014. 5

[24] T. Kong, F. Sun, A. Yao, H. Liu, M. Lu, and Y. Chen. RON:
reverse connection with objectness prior networks for object
detection. In CVPR, 2017. 1, 3, 5, 6, 7, 8

[25] T. Kong, A. Yao, Y. Chen, and F. Sun. Hypernet: Towards ac-
curate region proposal generation and joint object detection.
In CVPR, pages 845–853, 2016. 3, 6

[26] H. Lee, S. Eum, and H. Kwon. ME R-CNN: multi-expert

region-based CNN for object detection. In ICCV, 2017. 3

[27] T. Lin, P. Doll´ar, R. B. Girshick, K. He, B. Hariharan, and
S. J. Belongie. Feature pyramid networks for object detec-
tion. In CVPR, 2017. 1, 3, 7

[28] T. Lin, P. Goyal, R. B. Girshick, K. He, and P. Doll´ar. Focal
loss for dense object detection. In ICCV, 2017. 1, 3, 7, 8
[29] T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft COCO: com-
mon objects in context. In ECCV, pages 740–755, 2014. 1,
2, 3, 8

[30] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed,
C. Fu, and A. C. Berg. SSD: single shot multibox detector.
In ECCV, pages 21–37, 2016. 1, 3, 4, 6, 7, 8

[31] W. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking

wider to see better. In ICLR workshop, 2016. 4

[32] P. H. O. Pinheiro, R. Collobert, and P. Doll´ar. Learning to
segment object candidates. In NIPS, pages 1990–1998, 2015.
3

[33] P. O. Pinheiro, T. Lin, R. Collobert, and P. Doll´ar. Learning

to reﬁne object segments. In ECCV, pages 75–91, 2016. 3

[34] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi.
You only look once: Uniﬁed, real-time object detection. In
CVPR, pages 779–788, 2016. 3, 6

[15] R. B. Girshick. Fast R-CNN. In ICCV, pages 1440–1448,

[35] J. Redmon and A. Farhadi. YOLO9000: better, faster,

2015. 1, 3, 5, 6, 7

stronger. CoRR, abs/1612.08242, 2016. 1, 3, 6, 7

[16] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich
feature hierarchies for accurate object detection and semantic
segmentation. In CVPR, pages 580–587, 2014. 3

[36] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN:
towards real-time object detection with region proposal net-
works. TPAMI, 39(6):1137–1149, 2017. 1, 3, 6, 7, 8

[55] C. L. Zitnick and P. Doll´ar. Edge boxes: Locating object

proposals from edges. In ECCV, pages 391–405, 2014. 3

[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein,
A. C. Berg, and F. Li. Imagenet large scale visual recognition
challenge. IJCV, 115(3):211–252, 2015. 3, 4, 5

[38] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,
and Y. LeCun. Overfeat: Integrated recognition, localization
and detection using convolutional networks. In ICLR, 2014.
3

[39] Z. Shen, Z. Liu, J. Li, Y. Jiang, Y. Chen, and X. Xue. DSOD:
learning deeply supervised object detectors from scratch. In
ICCV, 2017. 3, 6, 8

[40] A. Shrivastava and A. Gupta. Contextual priming and feed-

back for faster R-CNN. In ECCV, pages 330–348, 2016. 3

[41] A. Shrivastava, A. Gupta, and R. B. Girshick. Training
region-based object detectors with online hard example min-
ing. In CVPR, pages 761–769, 2016. 1, 3, 6, 7, 8

[42] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Be-
yond skip connections: Top-down modulation for object de-
tection. CoRR, abs/1612.06851, 2016. 3, 7, 8

[43] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 3, 4

[44] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI, pages 4278–4284, 2017.
4, 7

[45] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, pages 1–9, 2015.
6

[46] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and
A. W. M. Smeulders. Selective search for object recognition.
IJCV, 104(2):154–171, 2013. 3

[47] P. A. Viola and M. J. Jones. Rapid object detection using a
In CVPR, pages 511–

boosted cascade of simple features.
518, 2001. 3

[48] X. Wang, A. Shrivastava, and A. Gupta. A-fast-rcnn: Hard
positive generation via adversary for object detection.
In
CVPR, 2017. 3

[49] S. Xie, R. B. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggre-
gated residual transformations for deep neural networks. In
CVPR, 2017. 4, 8

[50] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang. Gated
In ECCV, pages

bi-directional CNN for object detection.
354–369, 2016. 3

[51] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li. De-
tecting face with densely connected face proposal network.
In CCBR, pages 3–12, 2017. 4

[52] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li.
Faceboxes: A CPU real-time face detector with high accu-
racy. In IJCB, 2017. 4

[53] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li.
In ICCV,

S3FD: Single shot scale-invariant face detector.
2017. 1, 3, 4

[54] Y. Zhu, C. Zhao, J. Wang, X. Zhao, Y. Wu, and H. Lu. Cou-
plenet: Coupling global structure with local parts for object
detection. In ICCV, 2017. 3, 5, 6, 7

of ReﬁneDet for small objects, especially for the chairs and
tables. Increasing the input size (e.g., from 320 × 320 to
512 × 512) can improve the performance for small objects
, but it is only a temporary solution. Large input will be a
burden on running speed in inference. Therefore, detecting
small objects is still a challenge task and needs to be further
studied.

7. Complete Object Detection Results

We show the complete object detection results of the
proposed ReﬁneDet method on the PASCAL VOC 2007
test set, PASCAL VOC 2012 test set and MS COCO
test-dev set in Table 5, Table 6 and Table 7, respec-
tively. Among the results of all published methods, our Re-
ﬁneDet achieves the best performance on these three detec-
tion datasets, i.e., 85.8% mAP on the PASCAL VOC 2007
test set, 86.8% mAP on the PASCAL VOC 2012 test
set and 41.8% AP on the MS COCO test-dev set.

8. Qualitative Results

We show some qualitative results on the PASCAL VOC
2007 test set, the PASCAL VOC 2012 test set and the
MS COCO test-dev in Figure 3, Figure 4, and Figure 5,
respectively. We only display the detected bounding boxes
with the score larger than 0.6. Different colors of the bound-
ing boxes indicate different object categories. Our method
works well with the occlusions, truncations, inter-class in-
terference and clustered background.

9. Detection Analysis on PASCAL VOC 2007

We use the detection analysis tool8 to understand the
performance of two ReﬁneDet models (i.e., ReﬁneDet320
and ReﬁneDet512) clearly. Figure 6 shows that ReﬁneDet
can detect various object categories with high quality (large
white area). The majority of its conﬁdent detections are
correct. The recall is around 95%-98%, and is much higher
with “weak” (0.1 jaccard overlap) criteria. Compared to
SSD, ReﬁneDet reduces the false positive errors at all as-
pects: (1) ReﬁneDet has less localization error (Loc), indi-
cating that ReﬁneDet can localize objects better because it
uses two-step cascade to regress the objects. (2) ReﬁneDet
has less confusion with background (BG), due to the neg-
ative anchor ﬁltering mechanism in the anchor reﬁnement
module (ARM). (3) ReﬁneDet has less confusion with sim-
ilar categories (Sim), beneﬁting from using two-stage fea-
tures to describe the objects, i.e., the features in the ARM
focus on the binary classiﬁcation (being an object or not),
while the features in the object detection module (ODM) fo-
cus on the multi-class classiﬁcation (background or object
classes).

Figure 7 demonstrates that ReﬁneDet is robust to dif-
ferent object sizes and aspect ratios. This is not surprising
because the object bounding boxes are obtained by the two-
step cascade regression, i.e., the ARM diversiﬁes the default
scales and aspect ratios of anchor boxes so that the ODM
is able to regress tougher objects (e.g., extra-small, extra-
large, extra-wide and extra-tall). However, as shown in Fig-
ure 7, there is still much room to improve the performance

8http://web.engr.illinois.edu/˜dhoiem/projects/

detectionAnalysis/

Table 5: Object detection results on the PASCAL VOC 2007 test set. All models use VGG-16 as the backbone network.

Data
07+12
07+12
07+12
07+12

Method
mAP aero bike bird boat bottle bus car
ReﬁneDet320
80.0 83.9 85.4 81.4 75.5 60.2 86.4 88.1 89.1 62.7 83.9 77.0 85.4 87.1 86.7
81.8 88.7 87.0 83.2 76.5 68.0 88.5 88.7 89.2 66.5 87.9 75.0 86.8 89.2 87.8
ReﬁneDet512
ReﬁneDet320+
83.1 89.5 87.9 84.9 79.7 70.0 87.5 89.1 89.8 69.8 87.1 76.4 86.6 88.6 88.4
83.8 88.5 89.1 85.5 79.8 72.4 89.5 89.5 89.9 69.9 88.9 75.9 87.4 89.6 89.0
ReﬁneDet512+
ReﬁneDet320 COCO+07+12 84.0 88.9 88.4 86.2 81.5 71.7 88.4 89.4 89.0 71.0 87.0 80.1 88.5 90.2 88.4
ReﬁneDet512 COCO+07+12 85.2 90.0 89.2 87.9 83.1 78.5 90.0 89.9 89.7 74.7 89.8 79.5 88.7 89.9 89.2
ReﬁneDet320+ COCO+07+12 85.6 90.2 89.0 87.6 84.6 78.0 89.4 89.7 89.9 74.7 89.8 80.5 89.0 89.7 89.6
ReﬁneDet512+ COCO+07+12 85.8 90.4 89.6 88.2 84.9 78.3 89.8 89.9 90.0 75.9 90.0 80.0 89.8 90.3 89.6

cat chair cow table dog horse mbike person plant sheep sofa train tv
82.6 55.3 82.7 78.5 88.1 79.4
84.7 56.2 83.2 78.7 88.1 82.3
85.3 62.4 83.7 82.3 89.0 83.1
86.2 63.9 86.2 81.0 88.6 84.4
86.7 61.2 85.2 83.8 89.1 85.5
87.8 63.1 86.4 82.3 89.5 84.7
87.8 65.5 87.9 84.2 88.6 86.3
88.3 66.2 87.8 83.5 89.3 85.2

Table 6: Object detection results on the PASCAL VOC 2012 test set. All models use VGG-16 as the backbone network.

Data
07++12
07++12
07++12
07++12

mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv
Method
85.7 53.3 84.3 73.1 87.4 73.9
78.1 90.4 84.1 79.8 66.8 56.1 83.1 82.7 90.7 61.7 82.4 63.8 89.4 86.9 85.9
ReﬁneDet320
87.8 58.0 86.3 72.5 88.7 76.6
80.1 90.2 86.8 81.8 68.0 65.6 84.9 85.0 92.2 62.0 84.4 64.9 90.6 88.3 87.2
ReﬁneDet512
89.4 62.0 88.5 75.9 90.0 80.0
82.7 92.0 88.4 84.9 74.0 69.5 86.0 88.0 93.3 67.0 86.2 68.3 92.1 89.7 88.9
ReﬁneDet320+
90.2 64.1 89.8 75.2 90.7 81.1
ReﬁneDet512+
83.5 92.2 89.4 85.0 74.1 70.8 87.0 88.7 94.0 68.6 87.1 68.2 92.5 90.8 89.4
89.4 59.6 87.9 78.1 91.1 80.0
ReﬁneDet320 COCO+07++12 82.7 93.1 88.2 83.6 74.4 65.1 87.1 87.1 93.7 67.4 86.1 69.4 91.5 90.6 91.4
91.4 66.0 91.2 75.4 91.8 83.0
ReﬁneDet512 COCO+07++12 85.0 94.0 90.0 86.9 76.9 74.1 89.7 89.8 94.2 69.7 90.0 68.5 92.6 92.8 91.5
ReﬁneDet320+ COCO+07++12 86.0 94.2 90.2 87.7 80.4 74.9 90.0 91.7 94.9 71.9 89.8 71.7 93.5 91.9 92.4
91.9 66.5 91.5 79.1 92.8 83.9
92.6 68.8 92.4 78.5 93.6 85.2
ReﬁneDet512+ COCO+07++12 86.8 94.7 91.5 88.8 80.4 77.6 90.4 92.3 95.6 72.5 91.6 69.9 93.9 93.5 92.4

Table 7: Object detection results on the MS COCO test-dev set.

Method
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320
ReﬁneDet512
ReﬁneDet320+
ReﬁneDet512+
ReﬁneDet320+
ReﬁneDet512+

Net
VGG-16
VGG-16
ResNet-101
ResNet-101
VGG-16
VGG-16
ResNet-101
ResNet-101

AP
29.4
33.0
32.0
36.4
35.2
37.6
38.6
41.8

AP50
49.2
54.5
51.4
57.5
56.1
58.7
59.9
62.9

AP75
31.3
35.5
34.2
39.5
37.7
40.8
41.7
45.7

APS
10.0
16.3
10.5
16.6
19.5
22.7
21.1
25.6

APM
32.0
36.3
34.7
39.9
37.2
40.3
41.7
45.1

APL
44.4
44.3
50.4
51.4
47.0
48.3
52.3
54.1

AR1
26.2
28.3
28.0
30.6
30.1
31.4
32.2
34.0

AR10
42.2
46.4
44.0
49.0
49.6
52.4
52.9
56.3

AR100
45.8
50.6
47.6
53.0
57.4
61.3
61.1
65.5

ARS
18.7
29.3
20.2
30.0
36.2
41.6
40.2
46.2

ARM
52.1
55.5
53.0
58.2
62.3
65.8
66.2
70.2

ARL
66.0
66.0
69.8
70.3
72.6
75.4
77.1
79.8

Figure 3: Qualitative results of ReﬁneDet512 on the PASCAL VOC 2007 test set (corresponding to 85.2% mAP). VGG-16
is used as the backbone network. The training data is 07+12+COCO.

Figure 4: Qualitative results of ReﬁneDet512 on the PASCAL VOC 2012 test set (corresponding to 85.0% mAP). VGG-16
is used as the backbone network. The training data is 07++12+COCO.

Figure 5: Qualitative results of ReﬁneDet512 on the MS COCO test-dev set (corresponding to 36.4% mAP). ResNet-101
is used as the backbone network. The training data is COCO trainval35k.

Figure 6: Visualization of the performance of ReﬁneDet512 on animals, vehicles, and furniture classes in the VOC 2007
test set. The top row shows the cumulative fraction of detections that are correct (Cor) or false positive due to poor
localization (Loc), confusion with similar categories (Sim), with others (Oth), or with background (BG). The solid red line
reﬂects the change of recall with strong criteria (0.5 jaccard overlap) as the number of detections increases. The dashed red
line is using the “weak” criteria (0.1 jaccard overlap). The bottom row shows the distribution of the top-ranked false positive
types.

Figure 7: Sensitivity and impact of different object characteristics on the VOC 2007 test set. The plot on the left shows the
effects of BBox Area per category, and the right plot shows the effect of Aspect Ratio. Key: BBox Area: XS=extra-small;
S=small; M=medium; L=large; XL =extra-large. Aspect Ratio: XT=extra-tall/narrow; T=tall; M=medium; W=wide; XW
=extra-wide.

