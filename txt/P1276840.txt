9
1
0
2
 
n
u
J
 
3
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
5
9
0
1
0
.
6
0
9
1
:
v
i
X
r
a

Robust Gaussian Process Regression for
Real-Time High Precision GPS Signal
Enhancement

Ming Lin ∗1, Xiaomin Song †1, Qi Qian ‡1, Hao Li §2, Liang Sun ¶1,
Shenghuo Zhu (cid:107)1, and Rong Jin ∗∗1

1Alibaba Group, Bellevue, Washington, USA.
2Alibaba Group, Hangzhou, Zhejiang, China.

June 5, 2019

Abstract

Satellite-based positioning system such as GPS often suﬀers from large
amount of noise that degrades the positioning accuracy dramatically es-
pecially in real-time applications. In this work, we consider a data-mining
approach to enhance the GPS signal. We build a large-scale high preci-
sion GPS receiver grid system to collect real-time GPS signals for training.
The Gaussian Process (GP) regression is chosen to model the vertical To-
tal Electron Content (vTEC) distribution of the ionosphere of the Earth.
Our experiments show that the noise in the real-time GPS signals often
exceeds the breakdown point of the conventional robust regression meth-
ods resulting in sub-optimal system performance. We propose a three-step
approach to address this challenge. In the ﬁrst step we perform a set of
signal validity tests to separate the signals into clean and dirty groups. In
the second step, we train an initial model on the clean signals and then
reweigting the dirty signals based on the residual error. A ﬁnal model is
retrained on both the clean signals and the reweighted dirty signals. In
the theoretical analysis, we prove that the proposed three-step approach is
able to tolerate much higher noise level than the vanilla robust regression
methods if two reweighting rules are followed. We validate the superiority
of the proposed method in our real-time high precision positioning system

∗ming.l@alibaba-inc.com
†xiaomin.song@alibaba-inc.com
‡qi.qian@alibaba-inc.com
§lihao.lh@alibaba-inc.com
¶liang.sun@alibaba-inc.com
(cid:107)shenghuo.zhu@alibaba-inc.com
∗∗jinrong.jr@alibaba-inc.com

1

against several popular state-of-the-art robust regression methods. Our
method achieves centimeter positioning accuracy in the benchmark region
with probability 78.4% , outperforming the second best baseline method
by a margin of 8.3%. The benchmark takes 6 hours on 20,000 CPU cores
or 14 years on a single CPU.

1

Introduction

Real-time high precision positioning service is a critical core component in mod-
ern AI-driven industries, such as self-driving cars, autonomous drones and so
on. The Global Positioning System (GPS) is inarguably the most popular, if
not the only for now, satellite-based positioning system accessible to public all
around the world. The major performance index of GPS based system is the
positioning accuracy. When using civilian level smart phones with GPS en-
abled, the horizontal positioning accuracy is around 5 meters. Even equipped
with high quality, single frequency GPS receiver, the horizontal accuracy is typ-
ically within 1.891 meters with 95% probability1. While it might be suﬃcient
for daily usage, the meter level accuracy is insuﬃcient for many modern au-
tonomous applications. Previously a popular method to achieve the centimeter
accuracy is to ﬁx the GPS receiver at one point for several hours. This method
is clearly infeasible in mobile applications. How to achieve centimeter accuracy
in real-time is therefore an open challenge in this domain.

In this work, we develop a data mining approach to address the above chal-
lenge. The majority of the noise in GPS signal is the signal delay caused by
the ionosphere which can be described by the vertical Total Electron Content
(vTEC). To eliminate this delay, we build a grid system where each node in the
grid is a ground station equipped with high precision multiple frequency GPS
receiver. A Gaussian Process model (GP) is learned to predict the vTEC value
for any given geographic coordinate. When a mobile client requests positioning
service, its GPS signal is calibrated by our GP model every second.

A critical problem in the model training step is the large amount of noisy
data. As validated in our experiment, there are 20%∼40% outliers in the GPS
signal received by our grid system. Directly applying oﬀ-the-shelf robust regres-
sion methods cannot achieve the required performance because existing methods
are either non-consistent or are not suitable to tolerate high corruption rate. To
overcome this diﬃculty, we develop a screening algorithm to detect outliers and
split the received signal into clean and noisy subsets. The GP model is then
trained on the clean dataset.

However, only training on the clean dataset has an obvious drawback. The
screening algorithm is not always reliable. It often over-kills clean data. If the
dirty dataset is completely discarded after screening, we cannot collect suﬃcient
clean data for robust prediction. On the other hand, each data point is collected
from an expensive high precision GPS receiver therefore simply discarding the
noisy dataset is a great waste. In the worst case nearly 40% data points will

1Data source https://www.gps.gov/systems/gps/performance/accuracy/

2

be marked as noisy in one day. This number is even higher (> 60%) at noon in
summer which results in a low positioning accuracy or even interruption of our
service.

In order to address the above problem, we formulate the problem as a robust
regression problem. We are considering a scenario where we are provided a
clean dataset and a noisy dataset by an unreliable screening algorithm. The
noise in the clean dataset is sub-gaussian while the noise in the noisy dataset
is heavy-tailed. The volume of the noisy dataset might be inﬁnite. We aim
to design an eﬃcient and robust algorithm to boost the model performance by
learning on both clean and noisy datasets. Under this setting, we ﬁnd that
existing robust regression methods cannot be applied directly due to their small
breakdown points or due to their inconsistency. To this end, we propose a three-
step algorithm. Our algorithm ﬁrst learns an initial estimator only using the
clean dataset. Then we apply the estimator on the noisy dataset to ﬁlter out
those of large residual error. The remaining noisy data points are reweighted
according to their residual error and ﬁnally a robust estimator is retrained on
the reweighted noisy data points in addition to the clean dataset together. We
call this approach as Filter-Reweight-Retrain (FRR). Our theoretical analysis
shows that the three steps in FRR is not only suﬃcient but actually necessary.
The ﬁltering step truncates the noise level in the noisy dataset. The reweighting
step reduces the variance of the noise. When the volume of the noisy dataset
is inﬁnite, FRR is consistent which means that it achieves zero recovery error.
When the volume of the noisy dataset is ﬁnite, a reweighting scheme is proposed
to improve the asymptotic recovery error bound. While many previous robust
regression methods involve iterative retraining and reweighting, we show that
In fact, our analysis suggests that
FRR does not need multiple iterations.
designing an eﬀective reweighting-retraining scheme is non-trivial due to the risk
of over-ﬁtting. We give two general rules to avoid over-ﬁtting in the reweighting
step.

The remainder of this work is organized as following. In Section 2 we brieﬂy
survey related works. Section 3 introduces some background knowledge of our
GPS positioning system and our main algorithm, robust Gaussian process re-
gression with FRR. Theoretical analysis is given in Section 4. We demonstrate
the performance of our method in real-time GPS positioning system in Section
5. Section 6 encloses this work.

2 Related Work

The vTEC contributes the majority of noise in the GPS signal received by
ground stations. Conventional ionosphere scientiﬁc researches focus on the static
estimation of vTEC. For example, Sardon et al. [1994], Mannucci et al. [1998]
used Kalman ﬁlter to vTEC based on GPS signals received by multiple ground
stations.Arikan et al. [2003] proposed a high pass penalty regularizer to smooth
the estimated vTEC values. Mukesh [2019] described the vTEC distribution by
an ordinary kriging (OK)-based surrogate model. The real-time vTEC estima-

3

tion was considered in Huang and Reinisch [2001]. In Renga et al. [2018], the
authors proposed a Linear Thin Shell model to better describe the horizontal
variation of the vTEC distribution in real-time. Several researches introduce
multiple receivers to jointly estimate the vTEC. In Otsuka et al. [2002], the au-
thors used over 1000 dual frequency receivers to construct a large-scale vTEC
map over Japan. Zhang et al. [2018] used low-cost receivers jointly to improve
the vTEC estimation quality. Comparing to previous researches, we use robust
GP regression to model the vTEC distribution. The GPS signal is collected
from a few hundreds of ground stations in a given region. With modern hard-
ware and our new algorithm, we are able to report the centimeter positioning
accuracy in real-time.

In regard to the robust regression, we brieﬂy survey recent works closely
related to ours. As there is a vast amount of robust regression literature, we
refer to Morgenthaler [2007] for a more comprehensive review.

When comparing robust regression methods, we usually consider the perfor-
mance of an algorithm from three aspects: breakdown point, consistency and
corruption model. The breakdown point is a real number which measures the
maximum ratio of corruption data the algorithm can tolerance. A more robust
method should have a larger breakdown point. The consistency tells whether
the algorithm is unbiased. An unbiased algorithm should achieve zero recovery
error when provided with inﬁnite training data. The corruption model makes as-
sumptions about the noise. The conventional setting assumes that the training
data is randomly corrupted by arbitrary large noise. A more stronger corruption
model is the oblivious adversarial model which assumes that the attacker can
add noise at arbitrary location without looking at the data. The most diﬃcult
model is the adaptive adversarial model. In this model, the attacker is able to
review the data and then adaptively corrupts the data.

Ideally, we wish to ﬁnd a robust algorithm with breakdown point asymptot-
ically convergent to 1 while being consistent under adaptive adversarial corrup-
tion. However, it is impossible to satisfy all the three requirements Diakonikolas
et al. [2018]. A popular approach is to use (cid:96)1-norm in the loss function. The
(cid:96)1-norm based methods usually have breakdown point asymptotically conver-
gent to 1 but inconsistent Wright and Ma [2010], Nguyen and Tran [2013].
Dalalyan and Chen [2012] proposed a second order cone programming method
(SOCP). The SOCP has breakdown point asymptotically convergent to 1 but
is inconsistent. McWilliams et al. [2014] proposed to use weighted subsampling
d) and
and then iteratively retrain the model. Its breakdown point is O(1/
the algorithm is known to be inconsistent. Recently, a branch of hard iterative
thresholding (HIT) based methods are proven to be more eﬃcient. Chen et al.
d). Bhatia et al.
[2013] shows that HIT has breakdown point on order of O(1/
[2015] proposed a variant of HIT with breakdown point convergent to 1 but
inconsistent. The ﬁrst consistent HIT algorithm is proposed by Bhatia et al.
[2017] with breakdown point O(10−4). The corruption models in the above
works are either oblivious or adaptive.

√

√

Comparing to the above works, our setting is new where we are given two
datasets to train our model rather than one mixed with clean and corrupted

4

(a)

(b)

(c)

Figure 1: GNSS Background

data. In our setting, the volume of the noisy dataset could be inﬁnite which
means the breakdown point of our algorithm is asymptotically convergent to 1.
Our algorithm is consistent when the volume of the noisy dataset is inﬁnite.

3 Our System and Algorithm

This section consists of two subsections.
In subsection 3.1, we describe our
Global Navigation Satellite System (GNSS) behind the scene. We give our
FRR algorithm in subsection 3.2.

3.1 Global Navigation Satellite System

A global navigation satellite system (GNSS) is a satellite navigation system
that provides positioning service with global coverage, including GPS from the
United States, GLONASS from Russia, BeiDou from China and Galileo from
European Union. In GNSS, the satellite positions are known. With high preci-
sion distance measurement between user and more than 4 satellites, the user’s
location can be determined Hofmann-Wellenhof et al. [2012]. The distance is
calculated from the time interval between signal emitting from the satellite and
signal receiving by the user, times the speed of microwave transmission through
the path from the satellite to the user.

In a high precision positioning system, the major error terms along the
path of microwave transmission are shown in Figure 1(a). The pseudorange, a
measure of distance between the satellite and the receiver, is obtained through
the correlation of the modulated code in the received signal from the satellite
with the same code generated in the receiver. The pseudorange is aﬀected by
the following factors:

• The Euclidean distance between satellite position at signal emission and

the receiver at the signal reception.

• Oﬀsets of receiver clock and satellite clock. They are the clock synchro-
nism errors of the receiver and satellite referring to GNSS time. They are
associated with the speciﬁc receiver or satellite.

5

• Relativistic correction which can be modeled very well.

• Tropospheric delay which is frequency independent.

• Ionospheric delay which is frequency dependent.

• Instrumental delay which is frequency dependent and can be model very

well.

• Multi-path eﬀect. This error can be minimized by high quality antenna.

Most of errors like clock error and instrumental delay can be mitigated by double
diﬀerencing and multiple frequencies Pajares et al. [2005]. The computation of
double diﬀerence is shown in Figure 1(b). Biases due to orbit and atmosphere are
signiﬁcantly reduced due to the similar path of transmissions. As the ionospheric
delays are spatially correlated, the between-receiver diﬀerenced ionospheric bias
is much less than its absolute values.

After double diﬀerence, the residual tropospheric delays are rather small
since the tropospheric delays can be at least 90% corrected using the empiri-
cal model Collins and Langley [1997]. The remaining error is dominated by the
modeling of the ionosphere. The ionosphere is in the Earth’s upper atmosphere.
It can be modeled as a thin layer of ionized electrons at 350 kilometers above sea
level Klobuchar [1987]. The vertical Total Electron Content (vTEC) is an im-
portant descriptive parameter of the ionosphere. It integrates the total number
of electrons along a perpendicular path through the ionosphere. The additional
phase and group delay of microwave signal accumulated during transmission
through ionosphere are proportional to the vTEC. The vTEC is impacted by
the nearly unpredictable solar ﬂare which brings inherently high temporal and
spatial ﬂuctuations Meza et al. [2009].

To estimate the vTEC, we use machine learning to learn the vTEC dis-
tribution from double diﬀerences received in real-time. However due to the
diﬃculties in signal resolving, the quality of the received data varies a lot. To
understand this, we must understand how the signal is resolved. First we solve
the double diﬀerential equations with both code and phase using wide-lane com-
bination Teunissen [1998]. The equations are solved with Kalman ﬁlter to get
ﬂoating ambiguity Teunissen [2003]. Then the narrow-lane ﬂoat ambiguity is
derived from ionosphere-free and wide-lane combination. The integer phase
ambiguity can be found by Least-squares AMBiguity Decorrelation Adjustment
(LAMBDA) method Teunissen [1995]. After ﬁnding the integer ambiguity, solve
the double diﬀerential phase equations and get the base-line Ionospheric delays
which are frequency dependent and the Tropospheric delay which are frequency
independent.

However, the LAMBDA method does not always ﬁnd the optimal integer
solutions. We use Closed Loop and n-sigma outlier tests to check if the solved
base-lines are self-consistent as shown in Figure 1(c). We mark the data point
as clean if it passes the tests otherwise we mark it as noisy/corrupted.
The two tests work well when the ionosphere is quiet. In summer or in equatorial
region, ionosphere ﬂuctuates violently and the LAMBDA method does not work

6

Algorithm 1 Filter-Reweight-Retrain Meta Algorithm
Require: Clean set S, noisy set ˆS, truncation threshold τ and reweighting func-

tion h(·) in Eq. (1).

w

Ensure: Robust estimation ¯w.
1: winit = argmin
(cid:12)
winit, ˆx(i)(cid:69)
(cid:68)
(cid:12)
(cid:12)

(cid:96)(S|w).
(cid:12)
(cid:12), ∀(ˆx(i), ˆyi) ∈ ˆS.
(cid:12)
2: ri =
3: ˆS (cid:48) = {(ˆx(i), ˆyi)| ri ≤ τ, ∀(ˆx(i), ˆyi) ∈ ˆS.
4: αi = h(ri) if ∀ˆx(i) ∈ ˆS (cid:48) otherwise αi = 1.
5: Retrain weighted least square

− ˆyi

¯w = argmin

w

(x(i),yi)∈S∪ ˆS (cid:48)

(cid:88)

(cid:16)(cid:68)

w, x(i)(cid:69)

αi

− yi

(cid:17)2

.

6: Output: ¯w.

well. The low quality of integer solutions found by LAMBDA drives the Closed
Loop and n-sigma test to over-kill and under-kill data, resulting in large noise
and missing data.

3.2 Filter-Reweight-Retrain Algorithm For GPS Signal En-

hancement

The FRR is applicable to any base estimator that can generate decision
value. For sake of simplicity, we restrict ourselves in d dimensional linear space.
Following the standard settings, suppose that the data point x ∈ Rd are sampled
from sub-gaussian distribution such that

Ex = 0, (cid:107)x(cid:107)2 ≤ c(cid:112)d log(2d/η), Exx(cid:62) = I

where c is a universal constant, 0 < η < 1. The label y ∈ R is a real number.
We are given two datasets consisting of (x, y) pairs, the clean set S and the
noisy set ˆS. Both sets are independently and identically (i.i.d.) sampled. In
the clean set S, we assume that

y = (cid:104)w∗, x(cid:105) + ξ ∀(x, y) ∈ S .

The noise term ξ is i.i.d sampled from sub-gaussian distribution with mean zero,
variance proxy σ and boundary |ξ| ≤ ξmax. In the noisy set ˆS, the observed
label is corrupted by the noise model

ˆy = (cid:104)w∗, ˆx(cid:105) + ˆξ ∀(ˆx, ˆy) ∈ ˆS .

The noise term ˆξ is i.i.d. sampled from heavy-tailed distribution. It is important
to emphasize that even the mean value of ˆξ may not exist therefore directly

7

learning on ˆS is impossible. Without loss of generality, we assume that the
distribution of ˆξ is symmetric. Otherwise we add a bias term in the linear
regression model to capture the oﬀset. The volume of the clean set S is denoted
as n = |S| and the volume of the noisy set m = | ˆS| . | ˆS|/(|S| + | ˆS|) is the noise
ratio of the full dataset. We choose linear least square regression as our base
estimator deﬁned by

(cid:98)w = argmin

w

(cid:96)(S|w) (cid:44) (cid:88)

((cid:104)w, x(cid:105) − y)2 .

(x,y)∈S

The recovery error (cid:107) (cid:98)w − w∗(cid:107)2 is used as performance index in our theoretical
analysis.

Our FRR algorithm is detailed in Algorithm 1. First we train an initial
estimator with the clean dataset S. The residual error ri is computed for every
data point in the noisy dataset ˆS and then any ˆx ∈ ˆS with residual error larger
than the threshold τ is ﬁltered out. The noisy dataset after ﬁltering is denoted
as ˆS (cid:48). We assign instance weight αi for each ˆx(i) ∈ ˆS (cid:48). Finally we retrain the
weighted least square to get the robust estimation ¯w.

The reader might note that we have not specify how to choose τ and αi in
Algorithm 1. It is possible to adaptively choose these parameters such that the
recovery error bound given in Theorem 3 (Section 4) is minimized. . When the
volume of the noisy set is inﬁnite, we can simply choose τ suﬃciently large and
αi = 1. Theorem 3 guarantees that the recovery error is asymptotically zero
in this case. However when the volume of the noisy set is ﬁnite, ﬁnding the
optimal parameters requires more eﬀorts. In practice we suggest to choose

τ = c1

ri/m, αi = c2 exp(−ri/c3)

(1)

(cid:88)

i

where {c1, c2, c3} are tuned by cross-validation. The reader is free to design any
ﬁltering and reweighting scheme if he/she has more prior knowledge about the
noise distribution. However, several rules must be followed when designing the
ﬁltering and reweighting scheme. Please check the discussion under Theorem 3
to see why those rules are important to the success of FRR.

Remark 1 Although Algorithm 1 only concerns about linear estimator and
least square loss function, the FRR is essentially a meta algorithm applicable
to non-linear kernel machines and general convex loss function, for example
Gaussian process regression. The feature vector x could be replaced by ψ(x)
where ψ(·) is the feature map function induced by the kernel function and the
inner product (cid:104)w, x(cid:105) could be replaced by the corresponding inner product
deﬁned on the Reproducing Hilbert Kernel Space (RHKS). However in this
generalized formulation, our proof should be modiﬁed respectively. For example
we should replace matrix concentration with Talagrand’s concentration. To keep
things intuitive without loss of generality, we will focus on the linear model in
our theoretical analysis.

8

4 Theoretical Analysis

In this section, we give the statistical learning guarantees of FRR. Our main
result is given in Theorem 3 which claims the recovery error bound. Two non-
trivial rules for eﬀective ﬁltering and reweighting are derived from Theorem
3.

First we show that the initial estimator trained on clean dataset is not far
away from the ground-truth w∗. This is a well-known result in linear regression.
The proof is given in Appendix A.

Lemma 1. In Algorithm 1, with probability at least 1 − η,

(cid:107)winit − w∗(cid:107)2 ≤ c2σ

√

d[log(2d/η)]3/2/(cid:112)|S| (cid:44) ∆

provided that |S| ≥ max{(ξmax/σ)2, 4d log2(2d/η)/c6}.

Lemma 1 shows that the initial estimator will not be far away from w∗ as
long as the clean dataset is suﬃciently large. The FRR only requires the volume
of the clean dataset |S| no less than O(d log d). The major challenge is how to
retrain on the noisy dataset. Recall that ˆξ is heavy-tailed so we cannot bound
any concentration directly on ˆS. Before we can do any learning on the noisy
dataset, we must bound the variance of ˆξ. To this end, the FRR uses winit to
truncate ˆξ via instance ﬁltering. After the ﬁltering step on line 3 in Algorithm 1,
for any ˆx(i) ∈ ˆS (cid:48) we have the following lemma. The proof is given in Appendix
B.

Lemma 2. In Algorithm 1, with probability at least 1 − 2η, we have

| ˆξi| ≤ ri + c∆(cid:112)log(2/η) ∀ˆx(i) ∈ ˆS (cid:48) .
Lemma 2 shows that when we take τ ≥ c∆(cid:112)log(2/η), we can truncate the
noise level on ˆS (cid:48) below 2τ . It is important to note that in the proof of Lemma 2,
we use the independence of winit and ˆx(i) ∈ ˆS (cid:48) . This indicates that one cannot
retrain on ˆS twice. Actually in the second round of retraining, since previous w
depends on x(i), the bound |w(cid:62)x(i)| will degrade to O(d/(cid:112)|S|) which requires
|S| ≥ O(d2). However, if the clean dataset is larger than O(d2), the initial
estimator is already good enough and no retraining is necessary at all. In the
experiment, we ﬁnd that iteratively retraining indeed hurts the performance.

After ﬁltering, we get a bounded noise distribution on ˆS (cid:48) making learning
possible. Suppose after ﬁltering, we have | ˆS (cid:48)| = m. The sample weights α =
[α1, α2, · · · , αm] are computed via h(ri). The following main theorem bounds
the recovery error after retraining.
Theorem 3. In Algorithm 3, denote |S| = n, | ˆS (cid:48)| = m. {α1, α2, . . . , αm} are
weights of elements in ˆS (cid:48), αmin ≤ αi ≤ αmax. x(i), ˆxi) are sub-gaussian random
vectors with bounded norm (cid:107)x(i)(cid:107)2, (cid:107)ˆx(i)(cid:107)2 ≤ c(cid:112)d log(2d/η) where c > 0 is a
universal constant, 0 < η < 1. Deﬁne
(cid:44) (cid:107)E ˆξ2

i α2

i ˆx(i) ˆx(i)(cid:62)(cid:107)2
¯σ2
1
¯α (cid:44) E{(cid:107)αi ˆx(i) ˆx(i)(cid:62)(cid:107)2} .

9

Choose τ such that {¯σ1, ¯α} are bounded and m ≥ 1. Then with probability at
least 1 − 3η,

(cid:107) ¯w − w∗(cid:107)2 ≤ [

1
2

n + mαmin − c3 ¯α
√

√

md log(2d/η)]−1

{c2σ

nd[log(2d/η)]3/2 + ¯σ1

(cid:112)m log(2d/η)}

(2)

provided

n ≥ max{(ξmax/σ)2, 4d log2(2d/η)/c6}
m ≥d log2(2d/η) max{[αmax(τ + c∆(cid:112)log(2/η))/¯σ1]2

c6(¯α/αmin)2, (αmax/¯α)2} .

Proof. Denote X = [x(1), x(2), · · · , x(n)], ˆX = [ˆx(1), ˆx(2), · · · , ˆx(n)], y = [y1, y2, · · · , yn],
ˆy = [ˆy1, ˆy2, · · · , ˆym], α = [α1, α2, · · · , αm]. D(·) is the diagonal function.

According to Algorithm 1,

¯w =argmin

L(w)

w

The derivation is

(cid:44)(cid:107)X (cid:62)w − y(cid:107)2 + tr{( ˆX (cid:62)w − ˆy)(cid:62)D(α)( ˆX (cid:62)w − ˆy)} .

∇wL(w) = XX (cid:62)w − Xy + ˆXD(α) ˆX (cid:62)w − ˆXD(α)ˆy

= (XX (cid:62) + ˆXD(α) ˆX (cid:62))w − (Xy + ˆXD(α)ˆy) .

Similar to Lemma 1,

¯w =(XX (cid:62) + ˆXD(α) ˆX (cid:62))−1(Xy + ˆXD(α)ˆy)

=(XX (cid:62) + ˆXD(α) ˆX (cid:62))−1

(XX (cid:62)w∗ + Xξ + ˆXD(α)X (cid:62)w∗ + ˆXD(α)ξ)

=(XX (cid:62) + ˆXD(α) ˆX (cid:62))−1

[(XX (cid:62) + ˆXD(α)X (cid:62))w∗ + Xξ + ˆXD(α)ˆξ]
=w∗ + (XX (cid:62) + ˆXD(α) ˆX (cid:62))−1(Xξ + ˆXD(α)ˆξ) .

To ensure the matrix inversion exists, we need to ensure that XX (cid:62) is in-
vertible as ˆXD(α) ˆX (cid:62) is a symmetric positive semi-deﬁnite matrix. According
to Lemma 1, when

we have with probability at least 1 − η,

where λmin{·} is the smallest eigenvalue.

n ≥ 4d log2(2d/η)/c6 ,

λmin{

XX (cid:62)} ≥ 1/2

1
n

10

To bound λmin{ ˆXD(α) ˆX (cid:62)},

ˆXD(α) ˆX (cid:62) =

αi ˆx(i) ˆx(i)(cid:62) ,

E{ ˆXD(α) ˆX (cid:62)} =

E{αi ˆx(i) ˆx(i)(cid:62)} .

Please note that αi and ˆx(i) are not independent so we cannot simply write
E{αi ˆx(i) ˆx(i)(cid:62)} = αiI. As αi ˆx(i) ˆx(i)(cid:62) are symmetric PSD matrices,

λmin{E[ ˆXD(α) ˆX (cid:62)]} ≥mαmin .

Next we bound the concentration of ˆXD(α) ˆX (cid:62). Applying matrix Bernstein’s
inequality, with probability at least 1 − η,

Suppose

Then

(cid:107) ˆXD(α) ˆX (cid:62) − E{ ˆXD(α) ˆX (cid:62)}(cid:107)2
√

≤c max{c2αmaxd log2(2d/η), c2 ¯α

md log(2d/η)}.

√

c2 ¯α

md log(2d/η) ≥ c2αmaxd log2(2d/η)

⇐m ≥ (αmax/¯α)2d log2(2d/η)

(cid:107) ˆXD(α) ˆX (cid:62) − E{ ˆXD(α) ˆX (cid:62)}(cid:107)2

√

≤c3 ¯α

md log(2d/η) .

Therefore with probability at least 1 − η :

λmin{ ˆXD(α) ˆX (cid:62)} ≥mαmin − c3 ¯α

md log(2d/η)

√

provided m ≥ (αmax/¯α)2d log2(2d/η) . To ensure the lower bound is meaningful,
we must constrain

mαmin − c3 ¯α

md log(2d/η) ≥ 0
⇐m ≥ c6(¯α/αmin)2d log2(2d/η) .

Combining all above together, we have with probability at least 1 − η,

λmin{XX (cid:62) + ˆXD(α) ˆX (cid:62)} ≥

n + mαmin − c3 ¯α

md log(2d/η)

√

provided

m ≥ max{c6(¯α/αmin)2d log2(2d/η), (αmax/¯α)2d log(2d/η)} .

m
(cid:88)

i=1

m
(cid:88)

i=1

√

1
2

11

To bound (cid:107)Xξ(cid:107)2 , according to Lemma 1, with probability at least 1 − η,

(cid:107)Xξ(cid:107)2 ≤ c2σ

nd[log(2d/η)]3/2

√

provided n ≥ (ξmax/σ)2.

To bound (cid:107) ˆXD(α)ˆξ(cid:107)2,

According to the assumption, the noise distribution is assumed to be symmetric,

ˆXD(α)ˆξ =

ˆξiαi ˆx(i) .

m
(cid:88)

i=1

E ˆXD(α)ˆξ =0 .

In order to apply matrix Bernstein’s inequality again,

(cid:107) ˆξiαi ˆx(i)(cid:107)2 ≤[ri + c∆(cid:112)log(2/η)]αmax

(cid:112)d log(2d/η)
≤αmax[τ + c∆(cid:112)log(2/η)](cid:112)d log(2d/η) .

Therefore with probability at least 1 − η,

(cid:107) ˆXD(α)ˆξ(cid:107)2 ≤c max{αmax[τ + c∆(cid:112)log(2/η)]

d[log(2d/η)]3/2

(cid:112)m log(2d/η)} .

, ¯σ1

Suppose

¯σ1

(cid:112)m log(2d/η) ≥ αmax[τ + c∆(cid:112)log(2/η)]
⇐m ≥ {αmax[τ + c∆(cid:112)log(2/η)]/¯σ1}2d log2(2d/η) .

d[log(2d/η)]3/2

We then get

(cid:107) ˆXD(α)ˆξ(cid:107)2 ≤ ¯σ1
Combining all together, with probability at least 1 − 3η,

(cid:112)m log(2d/η) .

(cid:107) ¯w − w∗(cid:107)2 ≤(cid:107)(XX (cid:62) + ˆXD(α) ˆX (cid:62))−1(Xξ + ˆXD(α)ˆξ)(cid:107)2

√

√

min{XX (cid:62) + ˆXD(α) ˆX (cid:62)}[(cid:107)Xξ(cid:107)2 + (cid:107) ˆXD(α)ˆξ(cid:107)2]
≤λ−1
1
n + mαmin − c3 ¯α
2
{c2σ

(cid:112)m log(2d/η)}

nd[log(2d/η)]3/2 + ¯σ1

md log(2d/η)]−1

≤[

√

√

provide

n ≥ max{(ξmax/σ)2, 4d log2(2d/η)/c6}
m ≥d log2(2d/η) max{[αmax(τ + c∆(cid:112)log(2/η))/¯σ1]2

c6(¯α/αmin)2, (αmax/¯α)2} .

12

√

√

n + ¯σ1

Theorem 3 is our main result. It delivers several important messages. Roughly
m)/(n +
speaking, the recovery error of FRR is bounded by O[(σ
mαmin)]. The choice of αi = h(ri) is critical in our analysis.
If αi depends
on ri, we cannot simply take Eαi ˆx(i) ˆx(i) = αiI in the proof therefore there is
no closed-form tight upper bound as in Theorem 3. The constants {¯σ2
1, ¯α} are
deﬁned to capture such dependency.
In the simplest scenario αi = α where
α is a ﬁxed constant independent from ˆS, the recovery error of FRR is then
σ2n + α2 ˆσ2m/(n + αm)] where ˆσ2 is the variance of truncated
bounded by O[
noise in ˆS. Therefore for a ﬁxed reweighting function h(ri) = α, the recovery
error will always decrease when m gets larger and ˆσ controlled by τ increases
at a lower rate. If we have prior knowledge about the distribution of noise term
ˆξ, we could choose τ to minimize the above error bound.

√

i r2

i α2

i /(n + (cid:80)

An obvious problem of the constant reweighting scheme is that it weights in-
stances of large noise and small noise equally. Observing that the noise variance
is multiplied by α2
i , it seems reasonable to assign small weights for instances
with large residual error ri, leading to a better (but actually incorrect) recovery
error bound O((cid:112)σ2n + (cid:80)
i αi)). We argue that the analysis of
adaptive reweighting scheme is more complex than the constant case. When αi
depends on ˆS, one cannot simply apply the expectation equality and concen-
tration inequality in the same way. A counter example is to select one instance
with ri ≈ 0 and then set αi → ∞, αj = 0 for j (cid:54)= i. This is equal to retrain the
model using the i -th instance along. Clearly the recovery error bound is not
bounded by ri ≈ 0. This counter example shows that there are some rules we
must follow when designing adaptive reweighting scheme. Based on the proof
of Theorem 3, we summarize the following two rules:

• The reweighting function h(·) should depend on ri only. That is, αi =
h(ri) must not refer to any other {rj|j (cid:54)= i}. This rule ensures that {αi}m
i=1
are independent to each other so that (cid:80)
i αi ˆx(i) ˆx(i) is concentrated around
its mean value. Particularly, this rule prohibits jointly and iteratively
optimizing {αi}m

i=1.

• The maximum of h(·) should be bounded. This rule prevents the reweight-

ing step assigns large weight for one single instance.

Please be advised that the above two rules are derived from the concentration
inequalities we used in our proof. They are imposed by the nature of our
learning problem rather than the weakness of FRR. Informally speaking, it is
discouraged to tune αi too aggressively. In practice, we suggest to choose a base
weight c2 for all instances in ˆS (cid:48) and then decay the weight a bit proportional to
ri. We ﬁnd that αi = c2 exp(−ri/c3) with τ = c1
i ri/m is a good choice for
our problem. It is easy to verify that this choice satisﬁes the above two rules.

(cid:80)

5 Experiment

We are using a Network-based Real-time Kinematic (RTK) carrier-phase ground-
based augmentation system to achieve centimeter-level positioning service. A

13

Dataset
TrainDay1
TrainDay2
TrainDay3
TrainDay4
TrainDay5
TestDay1
TestDay2
TestDay3
TestDay4
TestDay5

Table 1: Dataset Statistics
# clean data # noisy data
17751091
17443691
16198561
17091968
15807494
16292229
15048558
14616549
15191055
13616039

4288037
4619126
5851525
5042280
6305344
5605534
6983814
7327788
6849075
8374171

noisy ratio
19.5%
20.9%
26.5%
22.8%
28.5%
25.6%
31.7%
33.4%
31.1%
38.1%

(a) TestDay1

(b) TestDay2

(c) TestDay3

(d) TestDay4

(e) TestDay5

(f) Average

Figure 2: RTK Ratio Comparison

14

triangular mesh grid of 20 satellite stations spaced by 75 kilometers are dis-
tributed over 100 000 square kilometers area in an anonymous region. They
can receive and process signals of the satellites from all 4 major GNSS systems.
All stations are equipped with dual-frequency receivers. When satellites’ orbit
above horizon with elevation angle larger than 15 degrees, each receiver-satellite
pair forms a Pierce Point on the ionosphere. Their data will be sent to a central
processing facility hosted on the cloud to ﬁt the model for vTEC pseudorange
compensation.

We collected a dataset consisting of 10 consecutive days for experiment. We
use the ﬁrst 5 days as training set and the last 5 days as testing set. Due
to data anonymity, we mark the training dates from TrainDay1 to TrainDay5
and the testing dates from TestDay1 to TestDay5. The dataset statistics are
summarized in Table 1. The second and the third columns are the number of
clean and noisy data points respectively (See Subsection 3.1 for our algorithm
to detect the noisy data points). The last column reports the ratio of noisy
data points with respect to the total number of data points collected in one day.
As shown in Table 1, around 30% data points will be ﬁltered out by our noisy
data screening algorithm. This is a great waste if no robust method is used to
recycle the noisy data points. On the other hand, simply using all data points
is harmful as we will show shortly.

It is important to note that the deﬁnition of training/testing in our problem
does not follow the convention. We choose one ground station as the testing
station and use its measured signal as ground-truth. Around this testing station,
we choose 16 ground stations as training stations. The experiment consists of
two stages: the oﬄine parameter tuning stage and the online prediction stage.
In the oﬄine parameter tuning stage, we use the training set to tune parameters
of our model and the best model parameter is selected. In the online prediction
stage, we apply the best model parameter to the testing set. In both oﬄine and
online stages, the performance of the model is evaluated as following. In every
second, we retrieve new data points from our grid system and train the model on
the training stations. Then the learned model is applied to the testing station
to prediction the double diﬀerence for any quadruplet pierce point related to the
testing station. To avoid over-ﬁtting, the quadruplet pierce points containing
the testing station are excluded in the oﬄine stage.

To evaluate performance, we use RTK ratio as index, the higher the better.
The RTK ratio is the probability of successful positioning while the positioning
error is less than one centimeter. The RTK ratio largely depends on the double
diﬀerences we predicted for the testing station.

As our system is a real-time online system, we require that the total time
cost of model training and prediction cannot exceed 300 milliseconds per second
frame. This excludes many time consuming models such as deep neural network.
After benchmarking, we adopt Gaussian Process (GP) as our base estimator as
it achieves the best RTK ratio among other popular models. In our GP model,
the vTEC for each pierce point is denoted as f i where i = 1, 2, · · · n. The matrix
A encodes the double-diﬀerence quadruplet: each row of A has exact two “1”
entries and two “-1” entries where the corresponding ionosphere pierce point

15

is involved in the double-diﬀerence calculation. Each ionosphere pierce point is
presented as a four dimensional vector x = [x1, x2, x3, x4] ∈ R4. {x1, x2} encode
the latitude and the longitude of the pierce point. x3 is the zenith angle and x4
is the azimuth angle. Deﬁne kernel function

κ(x, x(cid:48)) (cid:44) exp{−(x − x(cid:48))(cid:62)Ω−1(x − x(cid:48))}

where Ω = diag(ω1, ω2, ω3, ω4) is a diagonal kernel parameter matrix. The
kernel matrix induced by κ is K ∈ Rn×n where Ki,j (cid:44) κ(x(i), x(j)). Denote y
the double-diﬀerence value. Our GP model aims to minimize the loss function

(cid:107)Af − y(cid:107)2 + λf (cid:62)K −1f .

min
f

(3)

In every second, Eq. (3) is solved using the data points collected on the training
stations. To predict on the testing station, we predict the vTEC for each pierce
point fpred on the testing station by

fpred =

κ(x(i), xpred)f i .

n
(cid:88)

i=1

The double-diﬀerence value is followed immediately by Apredfpred where Apred
is the quadruplet matrix of the testing station.

In the oﬄine stage, we tune model parameters on the 5 day training data.
The kernel parameter ω1 and ω2 are tuned in range [0.001, 1000] . ω3 and ω4
are tuned in range [0.01, π/2] . λ is tuned in range [10−6, 106] . To apply our
algorithm, we ﬁrst train Eq. (3) on clean dataset. Then we apply the learned
model on noisy dataset to predict ypred and compute the absolute residual
r = |ypred − y|. For c1 tuned in range [0.5, 8], if ri ≤ c1
i=1 ri/n, the i-th
noisy data point is reserved in the retraining step otherwise is discarded. We
tune the reweighting parameter c2 in range [0.01, 1] and c3 in range [0.001, 0.1].
The model Eq. (3) is retrained using the ﬁltered noisy dataset and the clean
dataset together.

(cid:80)n

For comparison, we considered the following four baseline methods due to
their popularity in robust regression literature: Hard Iterative Thresholding
(HIT) Bhatia et al. [2017], Huber Regressor (Huber) Huber [1992], Iteratively
Reweighted Least Squares (IRLS) Holland and Welsch [1977], Random Sample
Consensus (RANSAC) Fischler and Bolles [1981]. Due to the computational
time constraint, for algorithms requiring iterative retraining, the retraining is
limited to 5 trials. For HIT, in each iteration we keep ρ percentage data points.
That is, we always select the ρ percentage data points with the smallest residual
error for next iteration. ρ is tuned in range [0.1, 0.9]. The Huber uses (cid:96)2 loss for
small residual error and (cid:96)1 loss if the residual error is larger than a threshold.
We tune this threshold in range [1, 5]. For IRLS, we replace the (cid:96)2 loss in Eq.
(3) with (cid:96)1 norm. For RANSAC, in each iteration we keep ρ percentage data
points similarly as in HIT.

In oﬄine tuning stage, for all methods we randomly select model parameters
uniformly in the tuning range and evaluate the RTK ratio. The procedure is

16

repeated 1000 times and the model with the highest RTK ratio on training
set is selected. The parameter tuning is carried on a cluster with 20,000 CPU
cores. Each method takes about 6 wall-clock hours (13.9 CPU years) to ﬁnd
the best parameters.
In Figure 2, we report the RTK ratio of each method
with the selected parameters on 5 testing days. For each day we report the
averaged RTK ratio over 24 × 3600 = 86, 400 seconds. The last ﬁgure reports
the averaged RTK ratio over all 5 days. The 95% conﬁdence intervals of all
methods are within 10−5 .

The numerical results show that directly training on the full dataset could
be harmful as the dataset contains lots of outliers — even a robust estimator
is used. We see that all four baseline methods will decrease the RTK ratio
dramatically. This is not surprising as most consistent robust methods require
the noisy ratio small enough. In our dataset, the noisy data could take up as
many as 30% of the total data points, a number too large for most conventional
robust regression methods. The HIT achieves the best results among the four
baseline methods. This is interesting because the best provable breakdown point
of HIT is far below 1%. Our experiment shows that HIT is able to tolerate much
higher breakdown point in real-world problems. The fact that Huber does not
work well might be due to the (cid:96)1 loss used in the Huber regression. Although (cid:96)1
loss is more robust than (cid:96)2 loss, it is still aﬀected by outliers. Therefore, when
the proportion of the outliers is as large as 30%, (cid:96)1 loss cannot survive. The
RTK ratio of RANSAC is barely above 50%. RANSAC requires lots of trials
of random sampling in order to work. However in our online system, we are
only able to perform the random sampling no more than 5 trials which makes
RANSAC vulnerable. IRLS performs poorly with RTK ratio less than half of
FRR. This indicates that IRLS is sensitive to outliers, as we showed in the
discussion of Theorem 4.

6 Conclusion

We propose to estimate the vTEC distribution in real-time by Gaussian process
regression with a three-step Filter-Reweight-Retrain algorithm to enhance its
robustness. We prove that the FRR is consistent and has breakdown point
asymptotically convergent to 1. We apply this new method in our real-time
high precision satellite-based positioning system to improve the RTK ratio of
our baseline Gaussian process model. By large-scale numerical experiments, we
demonstrate the superiority of the proposed FRR against several state-of-the-
art methods. The FRR is more robust and accurate than conventional methods
especially in our system where conventional methods are fragile due to the high
corruption rate in GPS signal.

17

Acknowledgement

We acknowledge Qianxun Spatial Intelligence Inc. China for providing its high
precision GPS devices and the grid computing system used in this work.

References

F Arikan, CB Erol, and O Arikan. Regularized estimation of vertical total
electron content from global positioning system data. Journal of Geophysical
Research: Space Physics, 108(A12), 2003.

Kush Bhatia, Prateek Jain, and Purushottam Kar. Robust regression via hard
thresholding. In Advances in Neural Information Processing Systems, pages
721–729, 2015.

Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, and Purushottam
Kar. Consistent robust regression. In Advances in Neural Information Pro-
cessing Systems, pages 2110–2119. 2017.

Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust sparse regres-
sion under adversarial corruption. In Proceedings of the 30th International
Conference on Machine Learning, volume 28, pages 774–782, 2013.

J Paul Collins and Richard Brian Langley. A tropospheric delay model for the
user of the wide area augmentation system. Department of Geodesy and
Geomatics Engineering, University of New Brunswick Fredericton, 1997.

Arnak Dalalyan and Yin Chen. Fused sparsity and robust estimation for linear
models with unknown variance. In Advances in Neural Information Processing
Systems, pages 1259–1267, 2012.

Ilias Diakonikolas, Weihao Kong, and Alistair Stewart. Eﬃcient algorithms and
lower bounds for robust linear regression. arXiv preprint arXiv:1806.00040,
2018.

Martin A. Fischler and Robert C. Bolles. Random sample consensus: A
paradigm for model ﬁtting with applications to image analysis and automated
cartography. Commun. ACM, 24(6):381–395, 1981.

Bernhard Hofmann-Wellenhof, Herbert Lichtenegger, and James Collins. Global
positioning system: theory and practice. Springer Science & Business Media,
2012.

Paul W Holland and Roy E Welsch. Robust regression using iteratively
reweighted least-squares. Communications in Statistics-theory and Methods,
6(9):813–827, 1977.

Xueqin Huang and Bodo W Reinisch. Vertical electron content from ionograms

in real time. Radio Science, 36(2):335–342, 2001.

18

Peter J. Huber. Robust Estimation of a Location Parameter, pages 492–518.

Springer New York, New York, NY, 1992.

J. A. Klobuchar.

Ionospheric time-delay algorithm for single-frequency gps
users. IEEE Transactions on Aerospace and Electronic Systems, AES-23(3):
325–331, 1987.

AJ Mannucci, BD Wilson, DN Yuan, CH Ho, UJ Lindqwister, and TF Runge.
A global mapping technique for gps-derived ionospheric total electron content
measurements. Radio science, 33(3):565–582, 1998.

Brian McWilliams, Gabriel Krummenacher, Mario Lucic, and Joachim M Buh-
mann. Fast and robust least squares estimation in corrupted linear models.
In Advances in Neural Information Processing Systems, pages 415–423, 2014.

A. Meza, M.A. Van Zele, and M. Rovira. Solar ﬂare eﬀect on the geomagnetic
ﬁeld and ionosphere. Journal of Atmospheric and Solar-Terrestrial Physics,
71(12):1322 – 1332, 2009.

Stephan Morgenthaler. A survey of robust statistics. Statistical Methods and

Applications, 15(3):271–293, 2007.

P.and Karthikeyan V.and Sindhu P. Mukesh, R.and Soma. Prediction of iono-
spheric vertical total electron content from gps data using ordinary kriging-
based surrogate model. Astrophysics and Space Science, 364(1):15, Jan 2019.

N. H. Nguyen and T. D. Tran. Exact recoverability from dense corrupted obser-
vations via(cid:96)1-minimization. IEEE Transactions on Information Theory, 59
(4):2017–2035, 2013.

Y. Otsuka, T. Ogawa, A. Saito, T. Tsugawa, S. Fukao, and S. Miyazaki. A new
technique for mapping of total electron content using gps network in japan.
Earth, Planets and Space, 54(1):63–70, 2002.

M.H. Pajares, J.M.J. Zornoza, and J.S. Subirana. GPS Data Processing: Code
and Phase : Algorithms, Techniques and Recipes. Centre de Publicacions del
Campus Nord, UPC, 2005.

Alfredo Renga, Flavia Causa, Urbano Tancredi, and Michele Grassi. Accurate
ionospheric delay model for real-time gps-based positioning of leo satellites
using horizontal vtec gradient estimation. GPS Solutions, 22(2):46, Feb 2018.

Esther Sardon, A Rius, and N Zarraoa. Estimation of the transmitter and
receiver diﬀerential biases and the ionospheric total electron content from
global positioning system observations. Radio science, 29(3):577–586, 1994.

P. J. G. Teunissen. The least-squares ambiguity decorrelation adjustment: a
method for fast gps integer ambiguity estimation. Journal of Geodesy, 70(1):
65–82, 1995.

19

Peter Teunissen. Towards a uniﬁed theory of gnss ambiguity resolution. Journal

of Global Positioning Systems, 2(1):1–12, 2003.

Peter J. G. Teunissen. GPS Carrier Phase Ambiguity Fixing Concepts, pages

319–388. Springer Berlin Heidelberg, Berlin, Heidelberg, 1998.

J. Wright and Y. Ma. Dense error correction via(cid:96)1-minimization. IEEE Trans-

actions on Information Theory, 56(7):3540–3560, 2010.

Baocheng Zhang, Peter J. G. Teunissen, Yunbin Yuan, Hongxing Zhang, and
Min Li. Joint estimation of vertical total electron content (vtec) and satellite
diﬀerential code biases (sdcbs) using low-cost receivers. Journal of Geodesy,
92(4), Apr 2018.

A Proof of Lemma 1

Proof. Denote n = |S| and X ∈ Rd×n where the i-th column of X is x(i) ∈ S.
Similarly denote y ∈ Rd to be the label vector. The least square regression has
solution

winit = (XX (cid:62))−1(Xy)
1
1
n
n
n XX (cid:62))−1(cid:107)2, since E{ 1

XX (cid:62))−1(

= (

XX (cid:62)w∗ −

Xξ) .

1
n

√

(cid:107)XX (cid:62) − nI(cid:107)2 ≤c max{c2d log2(2d/η), c2

nd log(2d/η)} .

To bound (cid:107)( 1

n XX (cid:62)} = I, according to matrix Bern-

stein’s inequality, with probability at least 1 − η,

√

nd log(2d/η) ≥ d log2(2d/η)

⇐n ≥ d(cid:112)log(2d/η) .

We get with probability at least 1 − η,

(cid:107)XX (cid:62) − nI(cid:107)2 ≤c3

nd log(2d/η) .

√

Suppose

When

symmetric,

n ≥ 4d log2(2d/η)/c6 ,
(cid:107)XX (cid:62) − nI(cid:107)2 ≤ n/2 .

Eξix(i) = 0

max (cid:107)ξix(i)(cid:107)2 ≤ξmaxc(cid:112)d log(2d/η) .

20

To bound (cid:107)Xξ(cid:107)2, according to the assumption, the noise distribution is

max{(cid:107)E[ξ2

i x(i)x(i)(cid:62)](cid:107)2, (cid:107)E[ξ2

i x(i)(cid:62)x(i)](cid:107)2}

≤c2σ2d log(2d/η) .

Therefore, with probability at least 1 − η,

(cid:107)Xξ(cid:107)2 ≤c max{cξmax

d[log(2d/η)]3/2, cσ

nd[log(2d/η)]3/2} .

√

√

√

cσ
√

nd[log(2d/η)]3/2 ≥ cξmax
⇐σ
n ≥ ξmax
⇐n ≥ (ξmax/σ)2 ,

√

d[log(2d/η)]3/2

(cid:107)Xξ(cid:107)2 ≤ c2σ

nd[log(2d/η)]3/2 .

√

In summary, with probability at least 1 − η,

(cid:107)winit − w∗(cid:107)2 ≤(cid:107)(XX (cid:62))−1(cid:107)(cid:107)Xξ(cid:107)2
≤c2σ

d[log(2d/η)]3/2/

√

√

n

Suppose

we get

provided

n ≥ max{(ξmax/σ)2, 4d log2(2d/η)/c6} .

B Proof of Lemma 2

Proof. From ﬁltering step in FRR (line 3 in Algorithm 1), ∀x(i) ∈ ˆS (cid:48),

τ ≥ ri =

(cid:68)

(cid:12)
winit, x(i)(cid:69)(cid:12)
(cid:12)
(cid:12)
(cid:12)yi −
(cid:12)
(cid:12)w∗(cid:62)x(i) + ˆξi − winit(cid:62)x(i)(cid:12)
(cid:12)
(cid:12)
≥ | ˆξi| − |(w∗ − winit)(cid:62)x(i)| .

=

(cid:12)
(cid:12)

Since x(i) is sub-gaussian random vector independent to (w∗ − winit), apply
Bernstein’s concentration, we have with probability at least 1 − 2η,

|(w∗ − winit)(cid:62)x(i)| ≤ c∆(cid:112)log(2/η) .

Therefore we get

| ˆξi| ≤ ri + c∆(cid:112)log(2/η) .

21

