Formulating Camera-Adaptive Color Constancy
as a Few-shot Meta-Learning Problem

Steven McDonagh*, Sarah Parisot*, Fengwei Zhou, Xing Zhang,
Ales Leonardis, Zhenguo Li, Gregory Slabaugh
Huawei Noah’s Ark Lab
{steven.mcdonagh, sarah.parisot, zhoufengwei, zhang.xing1
ales.leonardis, li.zhenguo, greg.slabaugh}
@huawei.com

9
1
0
2
 
r
p
A
 
3
 
 
]

V
C
.
s
c
[
 
 
2
v
8
8
7
1
1
.
1
1
8
1
:
v
i
X
r
a

Abstract

Digital camera pipelines employ color constancy meth-
ods to estimate an unknown scene illuminant, in order to re-
illuminate images as if they were acquired under an achro-
matic light source. Fully-supervised learning approaches
exhibit state-of-the-art estimation accuracy with camera-
speciﬁc labelled training imagery. Resulting models typi-
cally suffer from domain gaps and fail to generalise across
imaging devices. In this work, we propose a new approach
that affords fast adaptation to previously unseen cameras,
and robustness to changes in capture device by leveraging
annotated samples across different cameras and datasets.
We present a general approach that utilizes the concept of
color temperature to frame color constancy as a set of dis-
tinct, homogeneous few-shot regression tasks, each associ-
ated with an intuitive physical meaning. We integrate this
novel formulation within a meta-learning framework, en-
abling fast generalisation to previously unseen cameras us-
ing only handfuls of camera speciﬁc training samples. Con-
sequently, the time spent for data collection and annotation
substantially diminishes in practice whenever a new sen-
sor is used. To quantify this gain, we evaluate our pipeline
on three publicly available datasets comprising 12 different
cameras and diverse scene content. Our approach deliv-
ers competitive results both qualitatively and quantitatively
while requiring a small fraction of the camera-speciﬁc sam-
ples compared to standard approaches.

1. Introduction

The colors of an image captured by a digital camera are
always affected by the prevailing light source color in the
scene. Accounting for the effect of scene illuminant to pro-
duce images of canonical appearance (as if captured under
an achromatic light source) is an essential component of
digital photography pipelines, and is of great importance
for many practical high-level computer vision applications
including image classiﬁcation, semantic segmentation and

* Authors contributed equally

1

(a) Input image

(b) Ground-truth

(c) Standard ﬁne tuning of a pre-
trained model with access to only
10 images from a test camera.

(d) Ours, k-shot learning (10 im-
ages from a test camera)

Figure 1: An example image before and after color con-
stancy correction. Our approach can quickly adapt to new
unseen camera sensors using few samples where a pre-
trained model, ﬁne-tuned naively, fails to adapt well.

machine vision quality control [34, 44, 19]. Such applica-
tions commonly require that input images are device inde-
pendent and illuminant color-unbiased. Extraction of the
intrinsic color information from scene surfaces by compen-
sating for scene illuminant color is commonly referred to
as “Color Constancy” (CC) or “Automatic White Balance”
(AWB). The process of computational CC can be deﬁned
as the transformation of the source image, captured under
an unknown illuminant, to a target image representing the
same scene under a canonical illuminant. CC algorithms
typically consist of two stages; ﬁrst, estimation of the scene
illuminant color and second, transformation of the source
image, accounting for the illuminant, such that the resulting
image illumination appears achromatic.

The perceived color of surfaces are determined by the in-
trinsic surface reﬂectance properties of objects in the scene,
the spectral power distribution of the light(s) illuminating

Figure 2: Overview of the proposed strategy deﬁning task distribution Ti ∼ p(T ). Considering a set of cameras and camera
speciﬁc images, we separate images into subtasks based on illuminant color. This is done by computing color temperature
for each image, and building a CCT histogram for each camera. Images in the same task are deﬁned as images captured using
the same camera and belonging to the same CCT histogram bin.

them and physical capture device properties encompassing
image sensor, camera spectral sensitivity (CSS) and lens ef-
fects. This combination of properties makes the problem lo-
cally underdetermined. In practice, if no device calibration
prior is available, we can only observe a product of these
factors, as measured in the digital image. More formally,
we model a tri-chromatic photosensor response in the stan-
dard way such that:

(cid:90)

Ω

ρk(X) =

E(λ)S(λ, X)Rk(λ)dλ k ∈ {R, G, B}.

(1)
where ρk(X) is the intensity of color channel k at pixel
location X, the wavelength of light λ such that E(λ) rep-
resents the spectrum of the illuminant, S(λ, X) the surface
reﬂectance at pixel location X and Rk(λ) the CSS for chan-
nel k, considered over the spectrum of visible wavelengths
Ω. The goal of computational CC then becomes estimation
of the global illumination color ρE

k where:

ρE
k =

(cid:90)

Ω

E(λ)Rk(λ)dλ

k ∈ {R, G, B}.

(2)

Given that there exist inﬁnitely many combinations of
illuminant color and surface reﬂectance that result in identi-
cal (pixel value) observations ρk(X), the problem of ﬁnding
ρE
k is (locally) ill-posed.

Modern supervised learning techniques can be used to
infer this global image illuminant color and currently pro-
vide state-of-the-art estimation accuracy [9]. However,
such approaches are typically CSS speciﬁc (i.e. consistent
Rk(λ)) and therefore require, for each camera considered,
acquisition of large sets of manually labelled images com-
prising a variety of scenes and illumination colors. This

poses a barrier preventing such tools from providing highly
accurate and robust illuminant estimation for new, previ-
ously unseen cameras in a manner that can be regarded as
both quick and cheap.

In this paper we propose a new approach that removes
the expensive, yet necessary for standard approaches, re-
quirement of large amounts of labelled, sensor-speciﬁc im-
age acquisition by decomposing the illuminant estimation
problem such that it is robust with respect to variation in
capture device. Using the concept of color temperature to
infer the nature of scene light source, we frame the CC prob-
lem as a set of related yet distinct few-shot regression tasks,
where each task is camera and illuminant speciﬁc. This
enables us to exploit small image datasets, captured from
disparate sources, and construct models with a capacity to
learn camera-speciﬁc color biases quickly and cheaply us-
ing only a handful of target-device labelled images. Inte-
grating our task deﬁnition approach within a meta-learning
framework [17], we are able to train a joint model capable of
quickly adapting to new unseen capture devices and report
performance competitive with the fully-supervised, state of
the art using a handful of camera-speciﬁc training samples.
An overview of the proposed approach is depicted in Fig. 2.

The main contributions of this work are:

1. Our work constitutes the ﬁrst few-shot learning ap-
proach for color constancy and enables the use of or-
der(s) of magnitude fewer device-speciﬁc training im-
ages in comparison to contemporary work.

2. We introduce color temperature to the CC problem,
demonstrate how it allows to estimate the type of light
source from a photograph, and use it to frame CC as a
set of simpler physically intuitive problems.

3. We provide extensive experiments on three public
datasets and provide a comparative analysis of three
meta-learning algorithm variants on a real-world im-
age regression problem.

2. Related Work

Our contributions are closely related to previous learn-
ing based color constancy work, inter-camera considera-
tions and few-shot learning techniques. We now provide
brief review of these topics.
Fully supervised methods. Prior work can broadly be di-
vided into statistics-based and learning-based methods [25].
Classical methods utilise low-level statistics that are fast
and typically contain few free parameters. However, perfor-
mance is highly dependent on strong scene content assump-
tions and these methods falter in cases where assumptions
fail to hold. Early learning-based work [20, 46, 45, 36, 22]
comprised of combinational and direct approaches, typi-
cally relying on hand-crafted image features which limited
their overall performance.

Recent fully supervised convolutional CC work now of-
fers state-of-the-art estimation accuracy. Both local patch-
based [10, 40, 11, 27] and full image [8, 32, 9] input
have been considered, investigating different model archi-
tectures [10, 11, 40] and the use of semantic informa-
tion [27, 32]. Barron [8, 9] alternatively frames computa-
tional CC as a 2D spatial localisation problem. He repre-
sents image data using log-chroma histograms for which
a single convolutional layer learns to evaluate illuminant
color solutions in the chroma plane. Despite strong perfor-
mance, fully supervised deep-learning techniques require
large amounts of calibrated and hand-labelled sensor spe-
ciﬁc data to learn robust models for each target device [3].
This makes collection and calibration of imagery for data
driven color constancy both restrictive and costly, com-
monly requiring placement of physical calibration objects
in a large variety of scenes and illuminants, and subsequent
manual segmentation to measure ground-truth illuminants.
Image augmentation, eg. synthetic relighting [11], and
transfer-learning [10] using models pre-trained for alterna-
tive tasks, eg. ImageNet classiﬁcation [30], have been pre-
viously employed to mitigate lack of available data. The
former strategy commonly struggles with synthetic-data do-
main gap issues and may not generalise well to real-world
image manifolds at inference time, while the misalignment
between object classiﬁcation and computational CC likely
results in learning features invariant to appearance attributes
of critical importance for CC, limiting the performance of
the latter strategy.
Inter-camera and unsupervised approaches. Few color
constancy works have attempted to mitigate the costs of
sensor-speciﬁc data collection, calibration and image la-
belling. The work of [21] learns a transformation between

pairs of camera CSS, but requires a priori knowledge of the
cameras’ CSS and is limited to pairs of sensors. Early un-
supervised work [42] introduces a linear statistical model
learned on a single sensor from video frame observations.
Banic et al. [5] use classical statistical approaches to learn
parameters that approximate the unknown ground-truth il-
lumination of the training images, avoiding calibration and
image labelling. Despite promising inter-camera perfor-
mance [5], unsupervised techniques still require the collec-
tion of a large amount of unlabelled images under varying
light sources, and yield subpar performance when compared
to fully-supervised methods.

Our approach proposes to strongly restrict the data re-
quirements via a few-shot formulation that is robust to vari-
ations in camera sensor, and bridges the gap between fully-
supervised and unsupervised performances.
Few-shot Learning. Few-shot learning problems consist
of learning a new task or concept using only a handful of
data points (typically 1-10 samples per task) and have re-
cently received considerable attention [43, 35, 41, 17]. This
promises a number of advantages with regard to efﬁcient
model building for new tasks; reducing the need for data
acquisition and labelling by order(s) of magnitude, and de-
creasing effort spent on ﬁne-tuning and adaptation of ex-
isting models to novel problems. A popular meta-learning
strategy consists of ﬁnding model initialisations that allow
fast adaptation to new, previously unseen tasks [17]. The
strategy has since been widely adopted for classiﬁcation
tasks [29] and several recently proposed extensions report
increases to efﬁciency [33] and performance [31, 2]. While
a natural separation of few-shot tasks exists for image clas-
siﬁcation problems, in contrast, problems framed as a re-
gression (e.g. image illuminant estimation), require a care-
ful task deﬁnition process so as to provide distinct yet ho-
mogeneous tasks that aid fast and accurate model adaption
to new problem instances using limited training data.

To take advantage of such tools for color constancy, an
important research question emerges, namely, how to de-
compose our problem in a set of few-shot tasks? This is
crucially important when camera speciﬁc data is sparse.

3. Camera-Adaptive Color Constancy

An overview of our proposed Meta-AWB method is
shown in Fig. 2. We consider a set of datasets ∆ = {Dj},
where each dataset Dj = {Cj, {Ii}j} comprises images
acquired using a single camera Cj, representing various
scenes under varying illumination conditions. Our objec-
tive is to provide a color constancy framework that is ro-
bust to variations in capture device, so as to leverage all the
data available in ∆ in order to adapt to previously unseen
cameras with limited new training samples. We propose
a physically intuitive model that casts CC as a set of sim-
ple illuminant and camera speciﬁc regression tasks. We use

(a)

(b)

(c)

Figure 3: (a) Chromaticity space with Planckian locus. (b)
Color temperature chart and types of light source associated
with speciﬁc temperatures (c) Examples of images and cor-
responding color temperature K.

the concept of color temperature to approximate the type of
light source illuminating each image in our datasets, allow-
ing us to separate images based on light source in an un-
supervised way. As tasks are illuminant speciﬁc, estimated
illuminant corrections have limited variability and can be
evaluated accurately with limited training samples. This al-
lows us to frame each task as a few-shot regression prob-
lem, which can be addressed using recent few-shot learning
strategies, such as meta-learning. Section 3.1 reviews the
concept of color temperature and its computation from im-
ages before introducing our few-shot task deﬁnition strategy
in Section 3.2. Section 3.3 details how our novel formula-
tion is integrated in a meta-learning framework.

3.1. Correlated Color Temperature

Color Temperature (CT) is a common measurement in
photography, often used in high-end camera software to
describe the color of the illuminant for setting white bal-
ance [28]. By deﬁnition, CT measures, in degrees Kelvin,
the temperature that is required to heat a Planckian (or black
body) radiator to produce light of a particular color. A
Planckian radiator is deﬁned as a theoretical object that is
a perfect radiator of visible light [38]. The Planckian locus,
illustrated in Fig. 3(a), is the path that the color of a black
body radiator would take in chromaticity space as the tem-
perature increases, effectively illustrating all possible CTs.
In practice, the chromaticity of most light sources is off
the Planckian locus, so the Correlated Color Temperature
(CCT) is computed. CCT is the point on the Planckian lo-
cus closest to the non-Planckian light source [37, 38]. In-
tuitively, CCT describes the color of the light source and
can be approximated from photos taken under this light. As
shown in Fig. 3(b), different temperatures can be associ-
ated with different types of light [28]. For each image, we

balance

individual

images
(a) Pre-white
marking
(Canon1D camera),
ground-truth
their
g , b
gain corrections in [ r
g ] space.
(red,
Image frame border color
blue)
corresponding
image temperature histogram bin
membership.

indicates

(b) Ground-truth gain corrections
for
images observing identical
scenes under similar illumination
yet captured with distinct cameras
(Canon1D, Sony; NUS-9 [14]).

Figure 4: Our meta-task deﬁnition is conditioned on both
image temperature and camera, motivated by the expected
image light source separability and CSS distribution shifts.

can compute CCT using standard approaches that map CIE
1931 chromaticities x and y to CCT [26]. Chromaticities x,
y are coordinates in the chromaticity space which can easily
be estimated from the image’s RGB values [37].

3.2. Illuminant and camera-speciﬁc tasks

Device-speciﬁc Camera Spectral Sensitivities (CSS)
(Rk(λ) in Eq. 2) affect the color domain of captured images
and the recording of scene illumination. Images captured
by different cameras can therefore exhibit ground-truth il-
luminant distributions that occupy differing regions of the
chromaticity space [21], as can be observed in Fig. 4(b). In-
tuitively, this means that two images of the same scene and
illuminant will have different illuminant corrections if taken
by different cameras. In this context, a standard approach
is to treat each camera dataset as an independent regression
task. However, we expect to observe large variability in il-
luminant correction within one camera dataset, due to both
scene and light source diversity. Achieving good perfor-
mance and efﬁcient generalisation to unseen cameras using
tasks that contain too wide intra-task diversity may be difﬁ-
cult in a setting where camera speciﬁc data is sparse. Gamut
based color constancy methods [18, 16, 6] assume that the
color of the illuminant is constrained by the colors observed
in the image. We make a similar hypothesis and aim to re-
group images with similar dominant colors in the same task.
As a result, we decompose the inter-camera color con-
stancy problem in a set of regression problems that com-
prise images acquired with the same camera and with simi-
lar CCT (i.e. similar dominant color). Our intuition is that,
for each of these problems, illuminant corrections are clus-

tered such that good performance can be obtained quickly
with only a handful of training samples.

We propose two strategies to separate images based on
color temperature values. Our ﬁrst approach is to compute
a histogram Hs for camera s containing M bins of CCT
values and deﬁne each task as containing the set of im-
ages in each histogram bin. As a result, we deﬁne a task
T (Ds, m) ∈ T as: T (Ds, m) = { I | am
s ≤ CCT (I) ≤
bm
s , Cam(I) = Cs} where Cam(I) is the camera used
s , bm
to acquire image I, and am
s are the edges of bin m in
histogram Hs. Intuitively, images within the same temper-
ature bin will have a similar dominant color, and therefore
one could expect them to have similar illuminant correc-
tions. Figure 3(b) highlights that a large variety of light
sources are deﬁned by relatively low temperatures. Ac-
counting for this non-uniform distribution, we deﬁne bin
edges of Hs as a partition of temperature values on a loga-
rithmic scale. In particular, when setting M = 2, we expect
to separate images under a warm light source from images
under a cold light source (eg.
indoor images vs. outdoor
images). This effect is illustrated in Fig. 4(a) where im-
ages are plotted marking their respective ground-truth gain
correction in [ r
g ] space and image frame border colors
indicate temperature bin membership. This low granularity
decomposition yields a few-shot scenario such that only 10
to 20 training images will be required to adapt to a previ-
ously unseen camera. A second, more granular approach
consists of sampling K-nearest neighbour images in terms
of temperatures, where K is the number of images compris-
ing the regression task. Such a setting allows to separate the
types of illuminants more precisely, but conversely requires
more illuminant speciﬁc training images at test time.

g , b

3.3. Meta-learning formulation

Using the task formulation of Section 3.2, we can
frame camera-adaptive illuminant estimation as a few-shot
learning problem where tasks are used to deﬁne learning
episodes. One way of approaching this type of problem is
to use meta-learning techniques such as the popular MAML
algorithm [17], where the strategy is to learn an optimal
neural network initialisation capable of achieving strong
performance on a new unseen task in only a few gradient
updates, using only a small number of training samples.

Each regression task instance T : ˆρθ = fθ(I) aims to
estimate a global illuminant correction vector ρ = [r, g, b]
for an input image I, where fθ is a nonlinear function de-
scribed by a neural network model. The model’s parameters
θ are learned by minimising the angular error loss:

LT (ˆθ) = arccos(

ˆρθ
(cid:107) ˆρθ (cid:107)

·

ρ
(cid:107) ρ (cid:107)

).

(3)

ground-truth ρ yet agnostic to its magnitude, providing in-
dependence to the brightness of the illuminant.

MAML is an iterative algorithm that learns a global set
of parameters θ∗ across tasks by optimising ﬁne-tuning per-
formance on each training task. Each iteration comprises
an inner update which consists of ﬁne-tuning global param-
eters θ to be task speciﬁc on a set of training images via
n gradient descent steps with learning rate α. The second
step, the outer update, updates θ as:

θ∗ = θ − β∇θ

LTi(fθi),

(4)

(cid:88)

i

where β is the meta-learning rate parameter and LTi(fθ)
is the regression loss function as described in Eq. 3, com-
puted using task speciﬁc ﬁne-tuned parameters on a new set
of (previously unseen) meta-test images. At test time, pa-
rameters are ﬁne-tuned for a new unseen task for n gradient
updates and K training samples. We ﬁnally compute the
illuminant correction for each test image I as ρθi = fθi(I).

4. Results

Datasets and preprocessing. Three public color constancy
datasets: Gehler-Shi [39, 22], NUS-9 [14], and Cube [5]
are combined to investigate the capabilities of the proposed
methodology. We use a total of 4128 images captured by 12
different cameras. For each dataset, we make use of the pro-
vided ‘almost-raw’ PNG images for all experimental work
that follows in Section 4. Ground-truth illumination is mea-
sured by Macbeth Color Checker (MCC) in each dataset
except for the ‘Cube’ database that alternatively uses a Spy-
derCube [1] calibration object. Illuminant ground-truth in-
formation (respective calibration objects) are masked in all
images (using provided MCC coordinates and Cube+ us-
ing the ﬁxed SpyderCube image location with mask value
RGB = [0, 0, 0]), during both learning and inference. The
Gehler-Shi dataset [39, 22] contains 568 images of indoor
and outdoor scenes. Images were captured using Canon 1D
and Canon 5D cameras. The NUS-9-Camera dataset [14]
consists of 9 subsets of ∼210 images per camera providing
a total of (1736 + 1171) = 1853 images. All subsets com-
prise images representing the same scene, highlighting the
inﬂuence of the camera sensor. The Cube dataset [5] con-
tains 1365 images and consists of predominantly outdoor
imagery (Canon EOS 550D camera). It was recently up-
dated to include an additional 342 indoor images (renamed
Cube+ dataset). We use all Cube+ data at train time. At
inference time, we limit evaluation to the Cube dataset, in
order to be directly comparable to previous work.

Camera speciﬁc black-level corrections are applied in
keeping with offsets speciﬁed in the dataset descriptions.

Angular error provides a standard metric sensitive to the
inferred orientation of the ˆρθ vector, with respect to the

1The NUS dataset has recently been updated to include 117 additional

images from a ninth camera. During training we use all nine cameras.

We apply a standard gamma correction (γ = 2.2) and nor-
malize network input to [0, 1]. Input images are converted
to 8-bit where required, providing bit depth consistency
across all datasets. Impoverished input (low bit-depth) has
been shown to make the color constancy problem more dif-
ﬁcult [9] yet also provides a good real-world test bed as
specialized camera hardware typically performs illuminant
estimation using small, low bit-depth images.
Implementation. For each camera, we train a model using
random image crops of variable size (128 × 128 to orig-
inal image size) from the remaining 11 cameras, spatially
resized to 128 × 128. Due to the discussed limits on typ-
ical dataset size per camera and assumed simplicity of our
regression tasks, we adopt a simple architecture compris-
ing four convolutional layers (all sized 3 × 3 × 64), an av-
erage pooling layer and two fully connected layers (sizes
64 × 64, 64 × 3), with ReLU activations on all hidden lay-
ers. Due to the makeup of our proof-of-concept amalga-
mated dataset, the majority of cameras capture similar scene
content (NUS-9 imagery). Since we aim to optimise gener-
alisation between cameras without overﬁtting to particular
scenes, we choose the Gehler-Shi (Canon 5D) images as a
validation set. This allows for optimisation of model hyper-
parameters using imagery containing unique scene content.
We evaluate three variants of MAML, characterised by
different deﬁnitions of the learning rate α. MAML [17], uses
a constant α; metaSGD [31] learns an α value per parame-
ter in the network, allowing the direction and magnitude of
the gradient descent to be learned; LSLR [2] substantially
reduces the number of trainable parameters, learning a α
single parameter per layer in the network for each inner gra-
dient update. We train our CNN model for 25k iterations,
using a meta-batch size of 10, number of training images
per batch Ktrain = 10, learning rate β = 0.001 (with ex-
ponential decay). The inner-update learning rate α is set
(or initialised for metaSGD and LSLR) to 0.001. We use
layer normalisation on all convolutional layers. At infer-
ence time, for each test image, we randomly select Ktest
training samples (from the test image task) and ﬁne-tune
the model for 10 iterations. To evaluate the statistical ro-
bustness of our method to variation in the selection of the
Ktest images, we independently repeat 10 draws for each
test image. We report the median angular error over all im-
ages, averaged over all draws. As a baseline, we train a
model with standard back-prop and leave-one-out cross val-
idation on camera using network architecture matching our
introduced base-learner. At test time we report both with
(Baseline - ﬁne tune) and without (Baseline - no ﬁne tune)
K-shot ﬁne tuning. Baselines are trained for 25k iterations
using the same parameters as our Meta-AWB model.

Histogram-based task deﬁnition. To explore the va-
lidity of our learning task formulation, we plot CCT his-
tograms per camera and ground-truth [r, g, b] gain correc-

(a) Cube

(b) Canon600D
(NUS)

(c) Canon 600D (NUS) ground-truth il-
luminants in RGB space

Figure 5: Task deﬁnition: Temperature histograms and cor-
responding separation in RGB space.

tions per image in RGB space, with CCT bin assignment
indicated. Figure 5(b) and 5(c) provide examples of these
respectively for the NUS Canon 600D image set (M = 2
bins). Correlating bin edges (as deﬁned in Section 3.2) to
the temperature chart found in Figure 3(b), we see that im-
ages can be separated based on light type, and more specif-
ically, indoor vs outdoor light sources. This observation is
conﬁrmed in Fig. 5(a) where the CCT histogram, obtained
using the original subset of Cube images (all images con-
tain outdoor scenes), essentially contains all images in one
bin. Figure 5 also illustrates that our CCT-histogram strat-
egy generates homogeneous learning tasks since ground-
truth illuminant corrections belonging to the same bin are
well clustered in RGB space. This setup assigns images to
a task, conditioned on both camera sensor and CCT bin, re-
sulting in M ·|{Cj ∈ ∆}| valid learning tasks assuming that
each CCT bin is non-empty. In the remaining experiments,
we set M = 2 as discussed in Sec. 3.2.
Parameter and method analysis. Using our valida-
tion camera we evaluate the inﬂuence of key parameters
and meta-learning strategy. For each method considered
(MAML, metaSGD and LSLR),we train models for vari-
able numbers of inner gradient updates ntrain ∈ {1, 5, 10}.
While Ktrain = 10 images is ﬁxed during meta-model
training, we evaluate the inﬂuence of available k-shot im-
age count at test time, computing performance for Ktest ∈
{5, 10, 20}. We also report results for inner updates ntest ∈
{1, 5, 10}. Since LSLR learns a different learning rate per
inner update, we set αi = αn, ∀i ≥ n when ntest is set to a
value larger than that used during training.

We report results in Fig. 6(a) and Table 1, with the best
results for each Ktest reported in bold. We observe a sub-
stantial improvement in performance when increasing the
number of inner updates from 1 to 5, but note that 10 up-
dates do not improve performance. LSLR appears to offers a
compromise between simplicity and ﬂexibility, yielding the
best results when ntrain = 5, 10. However, interestingly,
LSLR and metaSGD perform poorly compared to MAML

MAML

Training updates u = ntrain
metaSGD

LSLR

u=1 u=5 u=10 u=1 u=5 u=10 u=1 u=5 u=10

Ktest = 5

s
r
e
t
e
m
a
r
a
p
g
n
i
t
s
e
T

2.18 3.08 3.11 2.12 2.80 3.10 2.42 9.07 4.68
1 update
2.06 2.07 2.08 2.27 2.11 2.07 2.31 2.00 2.76
5 updates
10 updates 2.06 2.07 2.08 2.28 2.11 2.06 2.31 2.00 2.05

Ktest = 10
2.13 3.05 3.12 2.00 2.71 3.09 2.37 9.05 4.52
1 update
2.02 2.01 2.00 2.09 1.98 1.94 2.23 1.87 2.50
5 updates
10 updates 2.02 2.00 2.00 2.09 1.97 1.93 2.23 1.87 1.92

Ktest = 20
2.10 3.05 3.09 1.99 2.70 3.05 2.31 9.01 4.40
1 update
1.98 1.94 2.00 2.06 1.89 1.91 2.18 1.81 2.41
5 updates
10 updates 1.98 1.94 1.95 2.06 1.89 1.91 2.18 1.81 1.84

Table 1: Our meta-learning hyper-parameter investigation
and method analysis. Median angular-error. Best results for
each K-shot conﬁguration are reported in bold.

(e) Gehler-Shi (Canon 5D)

(f) NUS-9, one random draw

Figure 6: Median angular-error with respect to the number
of ﬁne-tuning updates. (a,e) Parameter study results, (b-d)
Dataset speciﬁc results compared to the baselines, (f) Per
camera angular-error after 10 updates, all NUS-9 cameras.
Error bars in (a-e) report inter-draws variance.

when training with only 1 gradient update. Predictably, we
observe an increase in performance as we increase Ktest
reaching our overall best performance with a median angu-
lar error of 1.81 degrees and Ktest = 20 samples. Finally,
we can see that all methods beneﬁt from ntest > 1 updates,
but tend to plateau when ntest > 5. Considering our ex-
perimental observations, we use ntrain = 5 and ntest = 10
with our LSLR variant for the remainder of our experimental
work. We set Ktest = 10 unless otherwise speciﬁed.
Inﬂuence of task deﬁnition strategy. Using our valida-
tion camera, we further evaluate the inﬂuence of different
task deﬁnition approaches. As shown in Fig. 6(e), we com-
pare our M = 2 bins histogram based approach (Ours -
LSLR) to 1) M = 1, corresponding to the naive approach
of setting one camera dataset Ds as a task (Ours - LSLR -
one task per camera), 2) deﬁning tasks as temperature near-
est neighbours at test time (Ours - LSLR - NN tasks (test))
and both and train and test time (Ours - LSLR - NN tasks
(train)). Results are compared to the baseline method for
context. We observe that results improve as task deﬁnition
methods get more granular (i.e. provide better illuminant
separation), while our two experiments on nearest neigh-
bour tasks show that testing on more granular tasks (with
respect to train tasks) can equally improve performance and
allows to make use of potential larger datasets at test time.
Comparisons to the state of the art Figure 6 shows the

evolution of the median angular error, with respect to the
number of gradient updates, for all datasets under both base-
lines and our approach (with and without nearest neighbour
tasks at test time). We observe a signiﬁcant gap in perfor-
mance for all datasets. Fig. 6(f) shows per camera perfor-
mance and highlights that, unlike the baseline which par-
ticularly struggles for some cameras (eg. Olympus), our
method provides a consistent and better performance on
each NUS-9 test camera individually, in addition to average
performance. We provide a visual example in Fig. 7.

Finally, we compare our results on all datasets with re-
cent state of the art approaches.We report results on NUS-8
(without the recently added Nikon D40 camera) to provide
a fair and accurate comparison. Quantitative results are
shown in Tables 2 (NUS-8), 3 (Gehler-Shi) and 4 (Cube)
where we report standard angular-error statistics (Tri. is
trimean and G.M. geometric mean). We obtain results that
are competitive with the state of the art and fully supervised
methods, despite using an order of magnitude less camera
speciﬁc training data. We achieve good generalisation on
all datasets, in particular with the NUS-8 and Cube datasets,
where we outperform most state of the art methods. The su-
perior performance on NUS-8 can be linked to the fact that
the NUS scene content is repeatedly seen during training.

5. Conclusion

In this paper, we propose a novel formulation of the com-
putational color constancy problem that adapts and gener-
alises quickly to a large variety of camera sensors. We ex-
ploit the concept of color temperature to approximate the
type of light source from images, so as to decompose the
problem into a set of simpler regression tasks, each associ-
ated with a camera sensor and type of light source. The
simpliﬁed nature of the obtained regression tasks allows

Mean Median Tri.

Best 25% Worst 25% G.M.

Algorithm
Low-level statistics-based methods
White-Patch [12]
Gray-world [13]
Edge-based Gamut [24]
Natural Image Statistics [23]
Fully-supervised learning
Bayesian [22]
Cheng et al. 2014 [14]
SqueezeNet-FC4 [27]
CCC [8]
Cheng et al. 2015 [15]
Shi et al. 2016 [40]
FFCC [9] (thumb, 8bit input)
FFCC [9]
{Unsupervised, Few-shot} learning
Color Tiger [5]
Meta-AWB K = 10
Meta-AWB K = 20

9.91
4.59
4.40
3.45

3.50
2.93
2.23
2.38
2.18
2.24
2.06
1.99

2.96
1.93
1.89

7.44
3.46
3.30
2.88

2.36
2.33
1.57
1.48
1.48
1.46
1.39
1.31

1.70
1.38
1.34

8.78
3.81
3.45
2.95

2.57
2.42
1.72
1.69
1.64
1.68
1.53
1.43

1.97
1.49
1.44

1.44
1.16
0.99
0.83

0.78
0.78
0.47
0.45
0.46
0.48
0.39
0.35

0.53
0.47
0.45

21.27
9.85
9.83
7.18

8.02
6.13
5.15
5.85
5.03
6.08
4.80
4.75

7.50
4.37
4.28

7.24
3.70
3.45
2.81

2.66
2.40
1.71
1.74
1.65
1.74
1.53
1.44

2.09
1.52
1.47

Table 2: Performance on the NUS-8 dataset [14]. We fol-
low the same format as [9], reporting average performance
(geometric mean) over the 8 original NUS cameras. Results
outperformed by ours are marked in gray.

Mean Median Tri.

Best 25% Worst 25% G.M.

Algorithm
Low-level statistics-based methods
Gray-world [13]
White-Patch [12]
Edge-based Gamut [24]
Fully-supervised learning
Bayesian [22]
Cheng et al. 2014 [14]
Bianco CNN [10]
Cheng et al. 2015 [15]
CCC [8]
SqueezeNet-FC4 [27]
DS-Net [40]
FFCC [9] (thumb, 8bit input)
FFCC [9]
Few-shot learning
Meta-AWB K = 10
Meta-AWB K = 20
Meta-AWB NN K = 10
Meta-AWB NN K = 20

6.36
7.55
6.52

4.82
3.52
2.63
2.42
1.95
1.65
1.90
2.01
1.61

3.07
2.99
2.66
2.57

6.28
5.86
5.04

3.46
2.14
1.98
1.65
1.22
1.18
1.12
1.13
0.86

2.08
2.02
1.91
1.84

6.28
6.35
5.43

3.88
2.47
2.10
1.75
1.38
1.27
1.33
1.38
1.02

2.28
2.18
1.99
1.94

2.33
1.45
1.90

1.26
0.50
0.72
0.38
0.35
0.38
0.31
0.30
0.23

0.56
0.55
0.49
0.47

10.58
16.12
13.58

10.49
8.74
3.90
5.87
4.76
3.78
4.84
5.14
4.27

7.31
7.19
6.20
6.11

5.73
5.76
5.40

3.86
2.41
2.04
1.73
1.40
1.22
1.34
1.37
1.07

2.26
2.20
1.98
1.92

Table 3: Performance on the Gehler-Shi dataset [39, 22].
Previous methods as reported by [9]. Results outperformed
by our best method are marked in gray.

Algorithm
Low-level statistics-based methods
White-Patch [12]
Gray-World [13]
General Gray-World [7]
Smart Color Cat [4]
{Unsupervised, Few-shot} learning
Color Tiger [5]
Meta-AWB K = 10
Meta-AWB K = 20

Mean Median Tri.

Best 25% Worst 25% G.M.

6.58
3.75
2.50
1.49

2.94
1.63
1.59

4.48
2.91
1.61
0.88

2.59
1.08
1.02

5.27
3.15
1.79
1.06

2.66
1.20
1.15

1.18
0.69
0.37
0.24

0.61
0.31
0.30

15.23
8.18
6.23
3.75

5.88
3.89
3.85

4.88
2.87
1.76
1.04

2.35
1.17
1.16

Table 4: Performance on the Cube dataset. Previous meth-
ods as reported by [5]. Results outperformed by ours are
marked in gray.

us to cast color constancy as a few-shot learning problem
that we address using meta-learning. Extensive experiments

(a) input image

(b) Ground-truth solution

(c) Meta-AWB, angular er-
ror: 5.42◦

(d) Baseline ﬁne-tuned, an-
gular error: 18.76◦

Figure 7: A challenging Gehler-Shi [22, 39] test image
from our worst 25%. Images are shown in sRGB space and
clipped at the 97.5 percentile. See supplementary material
for additional qualitative results.

across three benchmark datasets and 12 different camera
sensors result in performance competitive with the fully-
supervised state-of-the-art, using only a small fraction of
camera speciﬁc data at test time. We presented and studied
the inﬂuence of several variants of our technique, includ-
ing task deﬁnition approaches. We show improved learn-
ing ability over standard ﬁne-tuning, resulting in efﬁcient
use of only few training samples. Meta-AWB has the abil-
ity to generalise quickly and learns to solve the computa-
tional color constancy problem in a camera agnostic fash-
ion. This provides the potential for high accuracy perfor-
mance as new sensors become available yet mitigates ar-
duous and time-consuming calibration of training imagery,
required for fully-supervised approaches. One would ex-
pect to reach better generalisation performance with more
imaging content variability per camera. Future work will
investigate different base learner components, alternatives
to meta-learning to address the few-shot learning problem
and more diverse task deﬁnition approaches (e.g. scene con-
tent).

References

[1] Datacolor SpyderCube.

http://www.datacolor.
com/photography-design/product-overview/
spydercube/. Accessed: 2018-11-05. 5

[2] A. Antoniou, H. Edwards, and A. Storkey. How to train your
MAML. arXiv preprint arXiv:1810.09502, 2018. 3, 6
[3] C¸ . Aytekin, J. Nikkanen, and M. Gabbouj. A data set for
camera-independent color constancy. IEEE Transactions on
Image Processing, 27(2):530–544, 2018. 3

[4] N. Bani´c and S. Lonˇcari´c. Using the red chromaticity for
In Image and Signal Processing
illumination estimation.
and Analysis (ISPA), 2015 9th International Symposium on,
pages 131–136. IEEE, 2015. 8

[5] N. Banic and S. Loncaric. Unsupervised learning for color

constancy. CoRR, abs/1712.00436, 2017. 3, 5, 8

[6] K. Barnard.

Improvements to gamut mapping colour con-
stancy algorithms. In European conference on computer vi-
sion, pages 390–403. Springer, 2000. 4

[7] K. Barnard, V. Cardei, and B. Funt. A comparison of com-
putational color constancy algorithms. i: Methodology and
IEEE transactions on
experiments with synthesized data.
Image Processing, 11(9):972–984, 2002. 8

[8] J. T. Barron. Convolutional color constancy. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 379–387, 2015. 3, 8, 11

[9] J. T. Barron and Y.-T. Tsai. Fast fourier color constancy. In
Proc. IEEE Conf. on Computer Vision and Pattern Recogni-
tion (CVPR), Honolulu, HI, 21–26 July 2017, 2017. 2, 3, 6,
8

[10] S. Bianco, C. Cusano, and R. Schettini. Color constancy
using cnns. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition Workshops, pages 81–
89, 2015. 3, 8

[11] S. Bianco, C. Cusano, and R. Schettini. Single and multiple
illuminant estimation using convolutional neural networks.
IEEE Transactions on Image Processing, 26(9):4347–4362,
2017. 3

[12] D. H. Brainard and B. A. Wandell. Analysis of the retinex

theory of color vision. JOSA A, 3(10):1651–1661, 1986. 8

[13] G. Buchsbaum. A spatial processor model for object colour
perception. Journal of the Franklin institute, 310(1):1–26,
1980. 8

[14] D. Cheng, D. K. Prasad, and M. S. Brown. Illuminant estima-
tion for color constancy: why spatial-domain methods work
and the role of the color distribution. JOSA A, 31(5):1049–
1058, 2014. 4, 5, 8, 11

[15] D. Cheng, B. Price, S. Cohen, and M. S. Brown. Effective
learning-based illuminant estimation using simple features.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 1000–1008, 2015. 8

[16] G. D. Finlayson. Color in perspective.

IEEE transactions
on Pattern analysis and Machine Intelligence, 18(10):1034–
1038, 1996. 4

[17] C. Finn, P. Abbeel, and S. Levine. Model-Agnostic Meta-
Learning for fast adaptation of deep networks. arXiv preprint
arXiv:1703.03400, 2017. 2, 3, 5, 6

[18] D. A. Forsyth. A novel algorithm for color constancy. Inter-
national Journal of Computer Vision, 5(1):5–35, 1990. 4
[19] B. Funt, K. Barnard, and L. Martin. Is machine colour con-
stancy good enough? In European Conference on Computer
Vision, pages 445–459. Springer, 1998. 1

[20] B. Funt and W. Xiong. Estimating illumination chromaticity
via support vector regression. In Color and Imaging Confer-
ence, volume 2004, pages 47–52. Society for Imaging Sci-
ence and Technology, 2004. 3

[21] S.-B. Gao, M. Zhang, C.-Y. Li, and Y.-J. Li. Improving color
constancy by discounting the variation of camera spectral
sensitivity. JOSA A, 34(8):1448–1462, 2017. 3, 4

[22] P. V. Gehler, C. Rother, A. Blake, T. Minka, and T. Sharp.
Bayesian color constancy revisited. In Computer Vision and
Pattern Recognition, 2008. CVPR 2008. IEEE Conference
on, pages 1–8. IEEE, 2008. 3, 5, 8, 11

[23] A. Gijsenij and T. Gevers. Color constancy using natural
image statistics and scene semantics. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 33(4):687–698,
2011. 8

[24] A. Gijsenij, T. Gevers, and J. Van De Weijer. Generalized
gamut mapping using image derivative structures for color
constancy. International Journal of Computer Vision, 86(2-
3):127–139, 2010. 8

[25] A. Gijsenij, T. Gevers, J. Van De Weijer, et al. Computational
IEEE Transac-

color constancy: Survey and experiments.
tions on Image Processing, 20(9):2475–2489, 2011. 3
[26] J. Hernandez-Andres, R. L. Lee, and J. Romero. Calcu-
lating correlated color temperatures across the entire gamut
of daylight and skylight chromaticities. Applied optics,
38(27):5703–5709, 1999. 4

[27] Y. Hu, B. Wang, and S. Lin. Fc4: Fully convolutional color
In Proceed-
constancy with conﬁdence-weighted pooling.
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR17), pages 4085–4094, 2017. 3, 8
[28] R. Jacobson, S. Ray, G. G. Attridge, and N. Axford. Manual

of Photography. Taylor & Francis, 2000. 4
[29] M. A. Jamal, G.-J. Qi, and M. Shah.

meta-learning for
arXiv:1805.07722, 2018. 3

few-shot

learning.

Task-agnostic
arXiv preprint

[30] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012. 3

[31] Z. Li, F. Zhou, F. Chen, and H. Li. Meta-SGD: Learn-
ing to learn quickly for few shot learning. arXiv preprint
arXiv:1707.09835, 2017. 3, 6

[32] Z. Lou, T. Gevers, N. Hu, M. P. Lucassen, et al. Color con-
stancy by deep learning. In BMVC, pages 76–1, 2015. 3
[33] A. Nichol, J. Achiam, and J. Schulman. On ﬁrst-order meta-
learning algorithms. CoRR, abs/1803.02999, 2018. 3
[34] R. Ramanath, W. E. Snyder, Y. Yoo, and M. S. Drew. Color
image processing pipeline. IEEE Signal Processing Maga-
zine, 22(1):34–43, 2005. 1

[35] S. Ravi and H. Larochelle. Optimization as a model for few-

shot learning. 2017. 3

[36] C. Rosenberg, A. Ladsariya, and T. Minka. Bayesian color
constancy with non-gaussian models. In Advances in neural
information processing systems, pages 1595–1602, 2004. 3
[37] J. Schanda. Colorimetry: understanding the CIE system.

John Wiley & Sons, 2007. 4

[38] E. F. Schubert. Light-emitting diodes. E. Fred Schubert,

2018. 4

[39] L. Shi and B. Funt. Re-processed version of the gehler color
constancy dataset of 568 images. http://www. cs. sfu. ca/˜
color/data/, 2000. 5, 8, 11

[40] W. Shi, C. C. Loy, and X. Tang. Deep specialized network for
illuminant estimation. In European Conference on Computer
Vision, pages 371–387. Springer, 2016. 3, 8

[41] J. Snell, K. Swersky, and R. Zemel. Prototypical networks
In Advances in Neural Information

for few-shot learning.
Processing Systems, pages 4077–4087, 2017. 3

[42] K. Tieu and E. G. Miller. Unsupervised color constancy. In
Advances in neural information processing systems, pages
1327–1334, 2003. 3

[43] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al.
In Advances in
Matching networks for one shot learning.
Neural Information Processing Systems, pages 3630–3638,
2016. 3

[44] M. Vrhel, E. Saber, and H. J. Trussell. Color image gen-
IEEE Signal Processing

eration and display technologies.
Magazine, 22(1):23–33, 2005. 1

[45] N. Wang, D. Xu, and B. Li. Edge-based color constancy via
support vector regression. IEICE transactions on informa-
tion and systems, 92(11):2279–2282, 2009. 3

[46] W. Xiong and B. Funt. Estimating illumination chromaticity
via support vector regression. Journal of Imaging Science
and Technology, 50(4):341–348, 2006. 3

Appendix A. Additional qualitative results

Appendix C. Interpretation of camera speciﬁc

results

We visualise the distributions of ground truth RGB cor-
rections per camera in Fig.11 where datasets with larger me-
dian angular errors are shown with warmer colormaps, as
well as their respective median value in Fig.11(a). Ground-
truth distributions are linked to differences in scene con-
tent and camera (CSS, lens and sensor effects). In partic-
ular, camera latent effects can be considered dominant for
the NUS dataset cameras due to the matching inter-camera
scene content of images. Distribution difference for other
cameras can be linked to both scene content and the consid-
ered hardware effects and is therefore less comparable.

Linking distributions in Fig. 11 to the results obtained,
we observe that best results are obtained for cameras with
compact distributions (ie.
similar scene content, as ob-
served for the Cube+ dataset). Furthermore, the very dif-
ferent nature of the Gehler-Shi (Canon 1D) and Nikon D40
distributions suggest that these cameras are more difﬁcult to
adapt to.

In particular, comparative results on the NUS-9 dataset
between the ﬁne-tuned baseline (blue) and our approach
(green) are shown in Fig. 11(b). Cameras are ordered from
left to right from lower to larger gap between baseline and
our method. This shows that our method adapts well to
cameras with distributions that are shifted with respect to
the majority of NUS-9 cameras (e.g. Fujiﬁlm, Olympus).
These cameras exhibit a larger increase in performance with
respect to our baseline. Similarly, we observe one of the
larger gaps in performance for the Nikon D40 camera, with
images that were acquired later and comprises different
scenes and illuminations.

In Figure 8 we provide additional qualitative results in
the form of test images from the Gehler-Shi dataset [22,
39]. For each image we show the input image produced by
the camera and a white-balanced image corrected using the
ground-truth illumination. We also show the output of our
model (“Meta-AWB”), and that of the baseline ﬁne-tuning
approach reported in the paper. Color checker boards are
visible in the images, however the relevant areas are masked
prior to inference. Images are shown in sRGB space and
clipped at the 97.5 percentile.

In similar fashion to [8], we adopt the strategy of sorting
the test images by the combined mean angular-error of the
two evaluated methods. We present images of increasing
average difﬁculty however images to report were selected
by instead ordering from “hard” to “easy” and sampling
with a logarithmic spacing, providing a greater number of
samples that proved challenging, on average.

Appendix B. Color temperature distributions

for learning task formulation

In our paper, we decompose the inter-camera color con-
stancy problem into a set of regression tasks such that each
task comprises images acquired from the same camera and
under similar illumination settings. One approach we pro-
pose is to compute Correlated Color Temperature (CCT)
histograms and deﬁne the images of each task as those that
belong to a given histogram bin.

In the main paper, we limit the number of bins to M = 2,
as this allows the broad separation of images into indoor
and outdoor illuminants. Here we provide additional ex-
amples of CCT histograms and their corresponding clus-
tering of ground-truth (GT) illuminant corrections in RGB
space. Results for the M = 3 bins case, for two cameras
(Canon 600D and Nikon D40 from the NUS-9 dataset [14])
are shown in Fig. 9.

We ﬁrst observe that for smaller sized datasets (e.g.
Nikon D40 data), there are not enough images (< 10 = K)
available in the ﬁrst bin as shown in 9(c). Furthermore,
while ground-truth illuminants retain reasonable cluster
separability, bin edges between different cameras may be
misaligned. Finally, increasing the number of histogram
bins also increases training data requirements as each bin
requires 10 to 20 training images. These results motivate
the choice of M = 2, as shown in Figure 4 of the main pa-
per, thus providing distinctive tasks with sufﬁcient images
in each bin of the CCT histogram.

(a) Input image

(b) Ground-truth solution

(c) Baseline ﬁne-tuning, (0.073◦)

(d) Meta-AWB, (0.308◦)

(e) Input image

(f) Ground-truth solution

(g) Baseline ﬁne-tuning, (7.956◦)

(h) Meta-AWB, (1.855◦)

(i) Input image

(j) Ground-truth solution

(k) Baseline ﬁne-tuning, (17.23◦)

(l) Meta-AWB, (3.798◦)

(m) Input image

(n) Ground-truth solution

(o) Baseline ﬁne-tuning, (18.55◦)

(p) Meta-AWB, (6.931◦)

(q) Input image

(r) Ground-truth solution

(s) Baseline ﬁne-tuning, (17.67◦)

(t) Meta-AWB, (16.14◦)

(u) Input image

(v) Ground-truth solution

(w) Baseline ﬁne-tuning, (22.08◦)

(x) Meta-AWB, (15.20◦)

Figure 8: For each scene we present the input image produced by the camera alongside the ground-truth white-balanced image. Images
are shown in sRGB space and normalized to the 97.5th percentile. For both our “Meta-AWB“ approach and the baseline algorithm we
show the white-balanced image, as well as the angular-error of the estimated illumination in degrees, with respect to the ground-truth.

(a) Canon 600D
histogram

(b) Canon 600D GT illuminants in RGB space

(a) Median ground-truth RGB correction for all cam-
eras.

(c) Nikon D40
histogram

(d) Nikon D40 GT illuminants in RGB space

Figure 9: CCT histograms Hs, 3 bin example.

(b) Ground-truth RGB distributions for all cameras. Shifts in distributions
are linked to camera effects and scene content. The color of each dataset
distribution is deﬁned by a colormap linked to the angular errors obtained
over all images and cameras. Warmer colors correspond to larger median
dataset errors.

Figure 11: Visualisation of variability between distributions
of ground-truth RGB illuminant corrections for all cameras.

Figure 10: Median angular error for each NUS-9 camera,
our method (green) and ﬁne-tuned baseline (blue). Cam-
eras are ordered from left to right with increasing gap in
performance between baseline and ours.

