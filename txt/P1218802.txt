Distribution-Aware Binarization of Neural Networks for Sketch Recognition

Ameya Prabhu Vishal Batchu Sri Aurobindo Munagala Rohit Gajawada Anoop Namboodiri
Center for Visual Information Technology, Kohli Center on Intelligent Systems
IIIT-Hyderabad, India
{ameya.prabhu@research., vishal.batchu@students., s.munagala@research.,
rohit.gajawada@students., anoop@}iiit.ac.in

8
1
0
2
 
r
p
A
 
9
 
 
]

V
C
.
s
c
[
 
 
1
v
1
4
9
2
0
.
4
0
8
1
:
v
i
X
r
a

Abstract

Deep neural networks are highly effective at a range
of computational tasks. However, they tend to be com-
putationally expensive, especially in vision-related prob-
lems, and also have large memory requirements. One of
the most effective methods to achieve signiﬁcant improve-
ments in computational/spatial efﬁciency is to binarize the
weights and activations in a network. However, naive bina-
rization results in accuracy drops when applied to networks
for most tasks. In this work, we present a highly general-
ized, distribution-aware approach to binarizing deep net-
works that allows us to retain the advantages of a binarized
network, while reducing accuracy drops. We also develop
efﬁcient implementations for our proposed approach across
different architectures. We present a theoretical analysis of
the technique to show the effective representational power
of the resulting layers, and explore the forms of data they
model best. Experiments on popular datasets show that our
technique offers better accuracies than naive binarization,
while retaining the same beneﬁts that binarization provides
- with respect to run-time compression, reduction of compu-
tational costs, and power consumption.

1. Introduction

Deep learning models are pushing the state-of-the-art in
various problems across domains, but are computationally
intensive to train and run, especially Convolutional Neural
Networks (CNNs) used for vision applications. They also
occupy a large amount of memory, and the amount of com-
putation required to train a network leads to high power con-
sumption as well.

There have been many developments in the area of model
compression in the last few years, with the aim of bring-
ing down network runtimes and storage requirements to
mobile-friendly levels. Compression strategies for Convo-
lutional Neural Networks included architectural improve-
ments [16, 20] and re-parametrization [27, 34] to pruning

Figure 1: Weight distribution of a layer with corresponding
α/β values, and the scaling factor α in the XNOR-Net im-
plementation for comparison. α and β in our method have
differing magnitudes, unlike in XNOR-Net.

techniques [14, 25] and quantization [19, 40]. Among these
approaches, quantization - especially, binarization - pro-
vided the most compact models as shown in Table 1.

Quantized networks - where weights/activations were
quantized into low-precision representations - were found to
achieve great model compression. Quantization has proven
to be a powerful compression strategy, especially the most
extreme form of quantization - Binarization. Binarization
has enabled the use of XNOR-Popcount operations for vec-
tor dot products, which take much less time compared to
full-precision Multiply-Accumulates (MACs), contributing
to a huge speedup in convolutional layers [28, 19] on a
general-purpose CPU. Moreover, as each binary weight re-
quires only a single bit to represent, one can achieve drastic
reductions in run-time memory requirements. Previous re-
search [28, 19] shows that it is possible to perform weight
and activation binarization on large networks with up to 58x
speedups and approximately 32x compression ratios, albeit
with signiﬁcant drops in accuracy.

Later works have tended to move away from binary rep-
resentations of weights/inputs to multi-bit representations.
The reason for this was mainly the large accuracy degrada-

Method
Finetuned SVD 2 [34]
Circulant CNN 2 [7]
Adaptive Fastfood-16 [34]
Collins et al. [8]
Zhou et al. [39]
ACDC [27]
Network Pruning [14]
Deep Compression [14]
GreBdec [38]
Srinivas et al. [30]
Guo et al. [13]
Binarization

Compression
2.6x
3.6x
3.7x
4x
4.3x
6.3x
9.1x
9.1x
10.2x
10.3x
17.9x
≈32x

Table 1: Comparison of Binarization and other methods in
terms of compression.

with interesting areas to explore, such as fast classiﬁcation
and sketch-based image retrieval.

Reproducibility: Our implementation can be found on

GitHub 1

2. Related Work

We ask the question: Do CNNs need the representa-
tional power of 32-bit ﬂoating point operations, especially
for binary-valued data such as sketches? Is it possible to
cut down memory costs and make output computations sig-
niﬁcantly less expensive? In recent years, several different
approaches were proposed to achieve network compression
and speedups, and special-purpose networks were proposed
for sketch classiﬁcation/retrieval tasks. These are summa-
rized below:

Sketch Recognition: Many deep-network based works
in the past did not lead to fruitful results before, primarily
due to these networks being better suited for images rather
than sketches. Sketches have signiﬁcantly different char-
acteristics as compared to images, and require specialized,
ﬁne-tuned networks to work with. Sketch-a-Net from Yu
et al. [37] took these factors into account, and proposed a
carefully designed network structure that suited sketch rep-
resentations. Their single-model showed tremendous incre-
ments over the then state-of-the-art, and managed to beat
the average human performance using a Bayesian Fusion
ensemble. Being a signiﬁcant achievement in this problem
- since beating human accuracy in recognition problems is
difﬁcult - this model has been adopted by a number of later
works Bui et al. [4], Yu et al. [35], Wang et al. [33].

Pruning Networks for Compression: Optimal Brain
Damage [10] and Optimal Brain Surgeon [15] introduced
a network pruning technique based on the Hessian of the

1https://github.com/erilyth/DistributionAwareBinarizedNetworks-

WACV18

Figure 2: An example sketch passing through a convolu-
tional layer ﬁlter, with the real-valued ﬁlter shown alongside
corresponding α-β and XNOR-Net ﬁlters. Orange signiﬁes
the highest response areas. We can see that DAB-Net has
signiﬁcantly better responses when compared to XNOR-
Net

tion observed in binary networks. While some works [32]
have proposed methods to recover some of the lost accu-
racy, this leads to the natural question of whether, in the-
ory, binary-representations of neural networks can be used
at all to effectively approximate a full-precision network. If
shown to be sufﬁcient, the search for an optimally accurate
binarization technique is worthwhile, due to the large gains
in speedups (due to binary operations rather than full-prec
MACs) and compression compared to multi-bit representa-
tions.

In our paper, we make the following contributions:
1. We show that binary representations are as expressive
as full precision neural networks for polynomial func-
tions, and offer theoretical insights into the same.
2. We present a generalized, distribution-aware represen-
tation for binary networks, and proceed to calculate the
generalized parameter-values for any binary network.
3. We offer an intuitive analysis and comparison of our
representation vis-a-vis previous representations, as il-
lustrated in Figure 1.

4. We provide a provably efﬁcient implementation of net-

works trained using this representation.

5. We demonstrate the effectiveness of our method by ex-
tensive experiments applying it to popular model ar-
chitectures on large-scale sketch datasets and improv-
ing upon existing binarization approaches.

We also offer intuitions about how this technique might
be effective in problems involving data that is inherently
binary, such as sketches, as shown in Figure 2. Sketches
are a universal form of communication and are easy to draw
through mobile devices - thus emerging as a new paradigm

loss function. Deep Compression [14] also used pruning to
achieve compression by an order of magnitude in various
standard neural networks. It further reduced non-runtime
memory by employing trained quantization and Huffman
coding. Network Slimming [25] introduced a new learn-
ing scheme for CNNs that leverages channel-level sparsity
in networks, and showed compression and speedup with-
out accuracy degradation, with decreased run-time memory
footprint as well. We train our binary models from scratch,
as opposed to using pre-trained networks as in the above
approaches.

Higher Bit Quantization: HashedNets [6] hashed net-
work weights to bin them. Zhou et al. [2] quantized net-
works to 4-bit weights, achieving 8x memory compres-
sion by using 4 bits to represent 16 different values and
1 bit to represent zeros. Trained Ternary Quantization
[41] uses 2-bit weights and scaling factors to bring down
model size to 16x compression, with little accuracy degra-
dation. Quantized Neural Networks[19] use low-precision
quantized weights and inputs and replaces arithmetic op-
erations with bit-wise ones, reducing power consumption.
DoReFa-Net [40] used low bit-width gradients during back-
propagation, and obtained train-time speedups. Ternary
Weight Networks [22] optimize a threshold-based ternary
function for approximation, with stronger expressive abili-
ties than binary networks. The above works cannot leverage
the speedups gained by XNOR/Pop-count operations which
could be performed on dedicated hardware, unlike in our
work. This is our primary motivation for attempting to im-
prove binary algorithms.

Binarization: We provide an optimal method for cal-
culating binary weights, and we show that all of the above
binarization techniques were special cases of our method,
with less accurate approximations. Previous binarization
papers performed binarization independent of the distribu-
tion weights, for example [28]. The method we introduce is
distribution-aware, i.e. looks at the distribution of weights
to calculate an optimal binarization.

BinaryConnect [9] was one of the ﬁrst works to use bi-
nary (+1, -1) values for network parameters, achieving sig-
niﬁcant compression. XNOR-Nets [28] followed the work
of BNNs [18], binarizing both layer weights and inputs and
multiplying them with scaling constants - bringing signiﬁ-
cant speedups by using faster XNOR-Popcount operations
to calculate convolutional outputs. Recent research pro-
posed a variety of additional methods - including novel ac-
tivation functions [5], alternative layers [32], approxima-
tion algorithms [17], ﬁxed point bit-width allocations [23].
Merolla et al. [?] and Anderson et al. [1] offer a few the-
oretical insights and analysis into binary networks. Further
works have extended this in various directions, including
using local binary patterns [21] and lookup-based compres-
sion methods [3].

3. Representational Power of Binary Networks

Many recent works in network compression involve
higher bit weight quantization using two or more bits
[2, 41, 22] instead of binarization, arguing that binary repre-
sentations would not be able to approximate full-precision
networks. In light of this, we explore whether the repre-
sentational power that binary networks can offer is theo-
retically sufﬁcient to get similar representational power as
full-precision networks.

Rolnick et al. [24, 29] have done extensive work in char-
acterizing the expressiveness of neural networks. They
claim that due to the nature of functions - that they de-
pend on real-world physics, in addition to mathematics -
the seemingly huge set of possible functions could be ap-
proximated by deep learning models. From the Universal
Approximation Theorem [11], it is seen that any arbitrary
function can be well-approximated by an Artiﬁcial Neural
Network; but cheap learning, or models with far fewer pa-
rameters than generic ones, are often sufﬁcient to approx-
imate multivariate monomials - which are a class of func-
tions with practical interest, occurring in most real-world
problems.

We can deﬁne a binary neural network having k lay-
ers with activation function σ(x) and consider how many
neurons are required to compute a multivariate monomial
p(x) of degree d. The network takes an n dimensional in-
put x, producing a one dimensional output p(x). We deﬁne
Bk(p, σ) to be the minimum number of binary neurons (ex-
cluding input and output) required to approximate p, where
the error of approximation is of degree at least d + 1 in the
input variables. For instance, B1(p, σ) is the minimal inte-
ger m such that:

m
(cid:88)

j=1

wjσ

(cid:32) n
(cid:88)

i=1

(cid:33)

aijxi

= p(x) + O(xd+1

1 + . . . + xd+1

n ).

Any polynomial can be approximated to high precision
as long as input variables are small enough [24]. Let
B(p, σ) = mink≥0 Bk(p, σ).

Theorem 1. For p(x) equal to the product x1x2 · · · xn, and
for any σ with all nonzero Taylor coefﬁcients, we have one
construction of a binary neural network which meets the
condition

Bk(p, σ) = O

(cid:16)

n(k−1)/k · 2n1/k (cid:17)

.

(1)

Proof of the above can be found in the supplementary

material.

Conjecture III.2. of Rolnick et al. [29] says that this
bound is approximately optimal. If this conjecture proves
to be true, weight-binarized networks would have the same
representational power as full-precision networks, since the

network that was essentially used to prove that the above
theorem - that a network exists that can satisfy that bound -
was a binary network.

The above theorem shows that any neural network that
can be represented as a multivariate polynomial function
is considered as a simpliﬁed model with ELU-like activa-
tions, using continuously differentiable layers - so pool-
ing layers are excluded as well. While there can exist a
deep binary-weight network that can possibly approximate
polynomials similar to full precision networks, it does say
that such a representation would be efﬁciently obtainable
through Stochastic Gradient Descent. Also, this theorem
assumes only weights are binarized, not the activations. Ac-
tivation binarization typically loses a lot of information and
might not be a good thing to do frequently. However, this
insight motivates the fact that more investigation is needed
into approximating networks through binary network struc-
tures.

4. Distribution-Aware Binarization

We have so far established that binary representations are
possibly sufﬁcient to approximate a polynomial with simi-
lar numbers of neurons as a full-precision neural network.
We now investigate the question - What is the most gen-
eral form of binary representation possible? In this section,
we derive a generalized distribution-aware formulation of
binary weights, and provide an efﬁcient implementation of
the same. We consider models binarized with our approach
as DAB-Nets (Distribution Aware Binarized Networks).

We model the loss function layer-wise for the network.
We assume that inputs to the convolutional layers are bi-
nary - i.e. belong to {+1, −1}, and ﬁnd constants α and
β (elaborated below) as a general binary form for layer
weights. These constants are calculated from the distribu-
tion of real-valued weights in a layer - thus making our ap-
proach distribution-aware.

4.1. Derivation

Without loss of generality, we assume that W is a vector
in Rn , where n = c · w · h. We attempt to binarize the
weight vector W to (cid:102)W which takes a form similar to this
example - [αα...βαβ]. Simply put, (cid:102)W is a vector consisting
of scalars α and β, the two values forming the binary vector.
We represent this as (cid:102)W = αe + β(1 − e) where e is a
vector such that e ∈ {0, 1}n (cid:51) e (cid:54)= 0 and e (cid:54)= 1. We
deﬁne K as eT e which represents the number of ones in
the e vector. Our objective is to ﬁnd the best possible binary
approximation for W. We set up the optimization problem
as:

(cid:102)W∗ = argmin

|| W − (cid:102)W ||2

(cid:102)W

We formally state this as the following:

The optimal binary weight vector (cid:102)W∗ for any weight
vector W which minimizes the approximate-error function
J =|| W − (cid:102)W ||2 can be represented as:

(cid:102)W∗ = αe + β(1 − e) where

α =

, β =

WT e
K

WT (1 − e)
n − K

for a given K. That is, given a K, the optimal selection of
e would correspond to either the K smallest weights of W
or the K largest weights of W.

The best suited K, we calculate the value of the follow-
ing expression for every value of K, giving us an e, and
maximize the expression:

e∗ = argmax
(

e

|| WT e ||2
K

+

|| WT (1 − e) ||2
n − K

)

A detailed proof of the above can be found in the supple-
mentary material.

The above representation shows the values obtained
for e, α and β are the optimal approximate representations
of the weight vector W. The vector e, which controls the
number and distribution of occurrences of α and β, acts as
a mask of the top/bottom K values of W. We assign α to
capture the greater of the two values in magnitude. Note
that the scaling values derived in the XNOR formulation,
α and −α, are a special case of the above, and hence our
approximation error is at most that of the XNOR error. We
explore what this function represents and how this relates
to previous binarization techniques in the next subsection.

4.2. Intuitions about DAB-Net

In this section, we investigate intuitions about the de-
rived representation. We can visualize that e and (1 − e)
are orthogonal vectors. Hence, if normalized, e and (1 − e)
form a basis for a subspace R2. Theorem 2 says the best α
and β can be found by essentially projecting the weight ma-
trix W into this subspace, ﬁnding the vector in the subspace
which is closest to e and (1 − e) respectively.

α =

(cid:104)W, e(cid:105)
(cid:104)e, e(cid:105)

· e , β =

(cid:104)W, (1 − e)(cid:105)
(cid:104)(1 − e), (1 − e)(cid:105)

· (1 − e)

We also show that our derived representation is different
from the previous binary representations since we cannot
derive them by assuming a special case of our formulation.
XNOR-Net [28] or BNN [18]-like representations cannot
be obtained from our formulation. However, in practice, we
are able to simulate XNOR-Net by constraining W to be
mean-centered and K = n
2 , since roughly half the weights
are above 0, the other half below, as seen in Figure 5 in
Section 5.3.2.

Algorithm 1 Finding an optimal K value.

// Empty array of same size as W

1: Initialization
2: W = 1D weight vector
3: T = Sum of all the elements of W
4: Sort(W)
5: D = [00...0]
6: optK1 = 0 // Optimal value for K
7: maxD1 = 0 // Value of D for optimal K value
8:
9: for I= 1 to D.size do
Pi = Pi−1 + Wi
10:
Di = P 2
i + (T −Pi)2
if Di ≥ maxD1 then
maxD1 = Di
optK1 = i

11:
12:
13:
14:

n−i

i

15:
16: Sort(W, reverse=true) and Repeat steps 4-13 with

optK2 and maxD2

17:
18: optKf inal = optK1
19: if maxD2 > maxD1 then
20:

optKf inal = optK2

21:
22: return optKf inal

4.3. Implementation

The representation that we earlier derived requires
to be efﬁciently computable, in order to ensure that our
algorithm runs fast enough to be able to train binary
networks.
In this section, we investigate the implemen-
tation, by breaking it into two parts: 1) Computing the
parameter K efﬁciently for every iteration. 2) Training
the entire network using that value of K for a given
iteration. We show that it is possible to get an efﬁciently
trainable network at minimal extra cost. We provide an
efﬁcient algorithm using Dynamic Programming which
computes the optimal value for K quickly at every iteration.

4.3.1 Parallel Preﬁx-Sums to Obtain K

Theorem 2. The optimal K ∗ which minimizes the value e
can be computed in O(n · logn) complexity.

Considering one weight ﬁlter at a time for each convo-
lution layer, we ﬂatten the weights into a 1-dimensional
weight vector W. We then sort the vector in ascending or-
der and then compute the preﬁx-sum array P of W. For
a selected value of K, the term to be maximized would be
i + (T −Pi)2
( ||WT e||2
)
since the top K values in W sum up to Pi where T is the

K + ||WT (1−e)||2

), which is equal to ( P 2

n−K

n−i

i

sum of all weights in W. We also perform the same compu-
tation with a descending order of W’s weights since K can
correspond to either the smallest K weights or the largest K
weights as we mentioned earlier. In order to speed this up,
we perform these operations on all the weight ﬁlters at the
same time considering them as a 2D weight vector instead.
Our algorithm runs in O(n · logn) time complexity, and is
speciﬁed in Algorithm 1. This algorithm is integrated into
our code, and will be provided alongside.

4.3.2 Forward and Backward Pass

Now that we know how to calculate K, e, α, and β for each
ﬁlter in each layer optimally, we can compute (cid:102)W which
approximates W well. Here, topk(W, K) represents the
top K values of W which remain as is whereas the rest are
converted to zeros. Let Tk = topk(W, K).

Corollary 1 (Weight Binarization). The optimal binary
weight (cid:102)W can be represented as,

(cid:102)W = α.sgn(Tk) + β.(1 − sgn(Tk))

where,

α =

and β =

Tk
K

(W − Tk)
n − K

Once we have (cid:102)W, we can perform convolution as I (cid:126)
(cid:102)W during the forward pass of the network. Similarly, the
optimal gradient (cid:101)G can be computed as follows, which is
back-propagated throughout the network in order to update
the weights:

Theorem 3 (Backward Pass). The optimal gradient value
(cid:101)G can be represented as,

(cid:101)G = (cid:102)G1 + (cid:102)G2

(2)

where,

(cid:102)G1 =

sgn(Tk)
K

◦ sgn(Tk) +

.ST E(Tk)

(3)

||Tk||l1
K

(cid:102)G2 =

◦ (1 − sgn(Tk))

.ST E(W − Tk)

sgn(W − Tk)
n − K

+

||W − Tk||l1
n − K
(cid:40)

ST E(Tk)i =

i, where |W|i<= 1

Tk
0, elsewhere

(4)

(5)

The gradient vector, as seen above, can be intuitively
understood if seen as the sum of two independent gradi-
ents (cid:102)G1 and (cid:102)G2, each corresponding to the vectors e and
(1 − e) respectively. Further details regarding the deriva-
tion of this gradient would be provided in the supplemen-
tary material.

Algorithm 2 Training an L-layers CNN with binary
weights:

1: A minibatch of inputs and targets (I, Y), cost function
C(Y, ˆY), current weight Wt and current learning rate
ηt.

2: updated weight Wt+1 and updated learning rate ηt+1.
3: Binarizing weight ﬁlters:
4: Wt = MeanCenter(Wt)
5: Wt = Clamp(Wt, -1, 1)
6: Wreal = Wt
7: for l = 1 to L do
8:
9:

for jth ﬁlter in lth layer do

10:

11:

12:
13:

Find Klj using Algorithm 1
αlj = topk(Wlj ,Klj )
Klj
βlj = − (Wlj −topk(Wlj ,Klj ))
(cid:102)Wlj = α.sgn(topk(Wlj, Klj))

n−Klj

+ β.(1 − sgn(topk(Wlj, Klj)))

14:
15: ˆY = BinaryForward(I, (cid:102)W)
16:
17: ∂C
= BinaryBackward( ∂C
// Standard back-
ˆY
∂ (cid:102)W
ward propagation except that gradients are computed
using (cid:102)W instead of Wt as mentioned in Theorem. 3

, (cid:102)W)

18:
19: We then copy back the real weights in order to apply

the gradients computed. Wt = Wreal

20:
21: Wt+1 = UpdateParameters(Wt, ∂C
∂ (cid:102)W
22: ηt+1 = UpdateLearningrate(ηt, t)

, ηt)

4.4. Training Procedure

Putting all the components mentioned above together,
we have outlined our training procedure in Algorithm 2.
During the forward pass of the network, we ﬁrst mean cen-
ter and clamp the current weights of the network. We then
store a copy of these weights as Wreal. We compute the bi-
nary forward pass of the network, and then apply the back-
ward pass using the weights (cid:102)W, computing gradients for
each of the weights. We then apply these gradients on the
original set of weights Wt in order to obtain Wt+1.
In
essence, binarized weights are used to compute the gra-
dients, but they are applied to the original stored weights
to perform the update. This requires us to store the full
precision weights during training, but once the network is
trained, we store only the binarized weights for inference.

5. Experiments

We empirically demonstrate the effectiveness of our op-
timal distribution-aware binarization algorithm (DAB-Net)

on the TU-Berlin and Sketchy datasets. We compare DAB-
Net with BNN and XNOR-Net [28] on various architec-
tures, on two popular large-scale sketch recognition datasets
as sketches are sparse and binary. Also, they are easier to
train with than standard images, for which we believe the
algorithm needs to be stabilized - in essence, the K value
must be restricted to change by only slight amounts. We
show that our approach is superior to existing binarization
algorithms, and can generalize to different kinds of CNN
architectures on sketches.

5.1. Experimental Setup

In our experiments, we deﬁne the network having only
the convolutional layer weights binarized as WBin, the net-
work having both inputs and weights binarized as FBin
and the original full-precision network as FPrec. Binary
Networks have achieved accuracies comparable to full-
precision networks on limited domain/simpliﬁed datasets
like CIFAR-10, MNIST, SVHN, but show considerable
losses on larger datasets. Binary networks are well suited
for sketch data due to its binary and sparse nature of the
data.

TU-Berlin: The TU-Berlin [12] dataset is the most
popular
large-scale free-hand sketch dataset contain-
ing sketches of 250 categories, with a human sketch-
recognition accuracy of 73.1% on an average.

Sketchy: A recent large-scale free-hand sketch dataset
containing 75,471 hand-drawn sketches spanning 125 cat-
egories. This dataset was primarily used to cross-validate
results obtained on the TU-Berlin dataset, to ensure the ro-
bustness of our approach with respect to the method of data
collection.

For all the datasets, we ﬁrst resized the input images to
256 x 256. A 224 x 224 (225 x 225 for Sketch-A-Net) sized
crop was then randomly taken from an image with standard
augmentations such as rotation and horizontal ﬂipping, for
TU-Berlin and Sketchy. In the TU-Berlin dataset, we use
three-fold cross validation which gives us a 2:1 train-test
split ensuring that our results are comparable with all pre-
vious methods. For Sketchy, we use the training images
for retrieval as the training images for classiﬁcation, and
validation images for retrieval as the validation images for
classiﬁcation. We report ten-crop accuracies on both the
datasets.

We used the PyTorch framework to train our net-
works. We used the Sketch-A-Net[37], ResNet-18[16] and
GoogleNet[31] architectures. Weights of all layers except
the ﬁrst were binarized throughout our experiments, ex-
cept in Sketch-A-Net for which all layers except ﬁrst and
last layers were binarized. All networks were trained from
scratch. We used the Adam optimizer for all experiments.
Note that we do not use a bias term or weight decay for bi-
narized Conv layers. We used a batch size of 256 for all

Models

Method

Accuracies

Improvement XNOR-Net vs DAB-Net

Sketch-A-Net

ResNet-18

GoogleNet

FPrec
WBin (BWN)
FBin (XNOR-Net)
WBin DAB-Net
FBin DAB-Net

FPrec
WBin (BWN)
FBin (XNOR-Net)
WBin DAB-Net
FBin DAB-Net

FPrec
WBin (BWN)
FBin (XNOR-Net)
WBin DAB-Net
FBin DAB-Net

TU-Berlin
72.9%
73.0%
59.6%
72.4%
60.4%
+0.8%
74.1%
73.4%
68.8%
73.5%
71.3%
+2.5%
75.0%
74.8%
72.2%
75.7%
73.7%
+1.5%

Sketchy
85.9%
85.6%
68.6%
84.0%
70.6%
+2.0%
88.7%
89.3%
82.8%
88.8%
84.2%
+1.4%
90.0%
89.8%
86.8%
90.1%
87.4%
+0.6%

Improvement XNOR-Net vs DAB-Net

Models
AlexNet-SVM
AlexNet-Sketch
Sketch-A-Net SC
Humans
Sketch-A-Net-22[36]
Sketch-A-Net WBin DAB-Net
ResNet-18 WBin DAB-Net
GoogleNet WBin DAB-Net
Sketch-A-Net FBin DAB-Net
ResNet-18 FBin DAB-Net
GoogleNet FBin DAB-Net

Accuracy
67.1%
68.6%
72.2%
73.1%
77.0%
72.4%
73.5%
75.7%
60.4%
71.3%
73.7%

Table 3: A comparison between state-of-the-art single
model accuracies of recognition systems on the TU-Berlin
dataset.

Improvement XNOR-Net vs DAB-Net

5.3. XNOR-Net vs DAB-Net

Table 2: Our DAB-Net models compared to FBin, WBin
and FPrec models on TU-Berlin and Sketchy in terms of
accuracy.

Sketch-A-Net models and a batch size of 128 for ResNet-
18 and GoogleNet models, the maximum size that ﬁts in a
1080Ti GPU. Additional experimental details are available
in the supplementary material.

We measure how K, α, and β vary across various lay-
ers over time during training, and these variations are ob-
served to be quite different from their corresponding val-
ues in XNOR-Net. These observations show that bina-
rization can approximate a network much better when it is
distribution-aware (like in our technique) versus when it is
distribution-agnostic (like XNOR-Nets).

5.2. Results

We compare the accuracies of our distribution aware bi-
narization algorithm for WBin and FBin models on the TU-
Berlin and Sketchy datasets. Note that higher accuracies
are an improvement, hence stated in green in Table 2. On
the TU-Berlin and Sketchy datasets in Table 2, we observe
that FBin DAB-Net models consistently perform better over
their XNOR-Net counterparts. They improve upon XNOR-
Net accuracies by 0.8%, 2.5%, and 1.5% in Sketch-A-Net,
ResNet-18, and GoogleNet respectively on the TU-Berlin
dataset. Similarly, they improve by 2.0%, 1.4%, and 0.6%
respectively on the Sketchy dataset. We also compare them
with state-of-the-art sketch classiﬁcation models in Table 3.
We ﬁnd that our compressed models perform signiﬁcantly
better than the original sketch models and offer compres-
sion, runtime and energy savings additionally.

Our DAB-Net WBin models attain accuracies similar to
BWN WBin models and do not offer major improvements
mainly because WBin models achieve FPrec accuracies al-
ready, hence do not have much scope for improvement un-
like FBin models. Thus, we conclude that our DAB-Net
FBin models are able to attain signiﬁcant accuracy improve-
ments over their XNOR-Net counterparts when everything
apart from the binarization method is kept constant.

2It is the sketch-a-net SC model trained with additional imagenet
data, additional data augmentation strategies and considering an ensem-
ble, hence would not be a direct comparison

5.3.1 Variation of α and β across Time

We plot the distribution of weights of a randomly selected
ﬁlter belonging to a layer and observe that α and β of DAB-
Net start out to be similar to α and −α of XNOR-Nets,
since the distributions are randomly initialized. However, as
training progresses, we observe as we go from Subﬁgure (1)
to (4) in Figure 3, the distribution eventually becomes non-
symmetric and complex, hence our values signiﬁcantly di-
verge from their XNOR-Net counterparts. This divergence
signiﬁes a better approximation of the underlying distribu-
tion of weights in our method, giving additional evidence
to our claim that the proposed DAB-Net technique gives a
better representation of layer weights, signiﬁcantly different
from that of XNOR-Nets.

5.3.2 Variation of K across Time and Layers

We deﬁne normalized K as the K
n for a layer ﬁlter. For
XNOR-Nets, K would be the number of values below zero
in a given weight ﬁlter - which has minimal variation, and
does not take into consideration the distribution of weights
in the ﬁlter - as K in this case is simply the number of
weights below a certain ﬁxed global threshold, zero. How-
ever, we observe that the K computed in DAB-Net varies
signiﬁcantly across epochs initially, but slowly converges
to an optimal value for the speciﬁc layer as shown in Figure
4.

(1)

(3)

(2)

(4)

Figure 3: Sub-ﬁgures (1) to (4) show the train-time variation of α and β for a layer ﬁlter. Initially, α and β have nearly
equal magnitudes, similar to the XNOR-Net formulation, but as we progress to (4), we see that α and β have widely different
magnitudes.Having just one scaling constant (XNOR-Net) would be a comparatively poor approximator.

Figure 4: The variation of the normalized K-value
over time during training.
It falls initially but con-
verges eventually to 0.35. The normalized K-value
for XNOR-Net remains almost at 0.5 till the end.

Figure 5: The variation of normalized K values on random
ﬁlters across layers. The K-value corresponding to DAB-
Net varies across layers based on the distribution of weights
of the speciﬁc layer, which is not captured by XNOR-Net.

We also plot the variation of normalized K values for a
few randomly chosen ﬁlters indexes across layers and ob-
serve that it varies across layers, trying to match the distri-
bution of weights at each layer. Each ﬁlter has its own set
of weights, accounting for the differences in variation of K
in each case, as shown in Figure 5.

6. Conclusion

We have proposed an optimal binary representation for
network layer-weights that takes into account the distri-
bution of weights, unlike previous distribution-agnostic

approaches. We showed how this representation could
be computed efﬁciently in n.logn time using dynamic
programming,
thus enabling efﬁcient training on larger
datasets. We applied our technique on various datasets and
noted signiﬁcant accuracy improvements over other full-
binarization approaches. We believe that this work provides
a new perspective on network binarization, and that future
work can gain signiﬁcantly from distribution-aware explo-
rations.

References

[1] A. G. Anderson and C. P. Berg. The high-dimensional
arXiv preprint

geometry of binary neural networks.
arXiv:1705.07199, 2017.

[2] Z. Aojun, Y. Anbang, G. Yiwen, X. Lin, and C. Yurong. In-
cremental network quantization: Towards lossless cnns with
low-precision weights. ICLR, 2017.

[3] H. Bagherinezhad, M. Rastegari, and A. Farhadi. Lcnn:

Lookup-based convolutional neural network. CVPR, 2017.

[4] T. Bui, L. Ribeiro, M. Ponti, and J. P. Collomosse. Generali-
sation and sharing in triplet convnets for sketch based visual
search. CoRR, abs/1611.05301, 2016.

[5] Z. Cai, X. He, J. Sun, and N. Vasconcelos. Deep learn-
ing with low precision by half-wave gaussian quantization.
CVPR, 2017.

[6] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and
Y. Chen. Compressing neural networks with the hashing
trick. ICML, 2015.

[7] Y. Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary,
and S.-F. Chang. An exploration of parameter redundancy
in deep networks with circulant projections. In CVPR, pages
2857–2865, 2015.

[8] M. D. Collins and P. Kohli. Memory bounded deep convolu-
tional networks. arXiv preprint arXiv:1412.1442, 2014.
[9] M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect:
Training deep neural networks with binary weights during
propagations. In NIPS, pages 3123–3131, 2015.

[10] Y. L. Cunn, J. S. Denker, and S. A. Solla. Nips. chapter

Optimal Brain Damage. 1990.

[11] G. Cybenko. Approximation by superpositions of a sig-

moidal function. (MCSS), 2(4):303–314, 1989.

[12] M. Eitz, J. Hays, and M. Alexa. How do humans sketch ob-
jects? ACM Trans. Graph. (Proc. SIGGRAPH), 31(4):44:1–
44:10, 2012.

[13] Y. Guo, A. Yao, and Y. Chen. Dynamic network surgery for

efﬁcient dnns. In NIPS, pages 1379–1387, 2016.

[14] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. ICLR, 2016.

[15] B. Hassibi, D. G. Stork, G. Wolff, and T. Watanabe. Op-
timal brain surgeon: Extensions and performance compar-
isons. NIPS, 1993.

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, pages 770–778, 2016.
[17] L. Hou, Q. Yao, and J. T. Kwok. Loss-aware binarization of

deep networks. ICLR, 2017.

[18] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and

Y. Bengio. Binarized neural networks. 2016.

[19] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Y. Bengio. Quantized neural networks: Training neural net-
works with low precision weights and activations. arXiv
preprint arXiv:1609.07061, 2016.

[20] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy
with 50x fewer parameters and¡ 0.5 mb model size. ICLR,
2017.

[21] F. Juefei-Xu, V. N. Boddeti, and M. Savvides. Local binary

convolutional neural networks. CVPR, 2017.

[22] F. Li, B. Zhang, and B. Liu. Ternary weight networks. arXiv

preprint arXiv:1605.04711, 2016.

[23] D. Lin, S. Talathi, and S. Annapureddy. Fixed point quantiza-
tion of deep convolutional networks. In Proceedings of The
33rd International Conference on Machine Learning, 2016.
[24] H. W. Lin, M. Tegmark, and D. Rolnick. Why does deep and
cheap learning work so well? Journal of Statistical Physics,
168(6):1223–1247, 2017.

[25] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang.
Learning efﬁcient convolutional networks through network
slimming. ICCV, 2017.

[26] P. Merolla, R. Appuswamy, J. V. Arthur, S. K. Esser, and
D. S. Modha. Deep neural networks are robust to weight
binarization and other non-linear distortions. CoRR, 2016.

[27] M. Moczulski, M. Denil, J. Appleyard, and N. de Freitas.
Acdc: A structured efﬁcient linear layer. ICLR, 2016.
[28] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. In ECCV, pages 525–542, 2016.

[29] D. Rolnick and M. Tegmark. The power of deeper net-
arXiv preprint

works for expressing natural functions.
arXiv:1705.05502, 2017.

[30] S. Srinivas, A. Subramanya, and R. V. Babu. Training sparse

neural networks. In CVPRW, pages 455–462, 2017.

[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015.

[32] W. Tang, G. Hua, and L. Wang. How to train a compact
binary neural network with high accuracy? In AAAI, pages
2625–2631, 2017.

[33] X. Wang, X. Duan, and X. Bai. Deep sketch feature for cross-

domain image retrieval. Neurocomputing, 2016.

[34] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. Smola,
L. Song, and Z. Wang. Deep fried convnets. In ICCV, pages
1476–1483, 2015.

[35] Q. Yu, F. Liu, Y.-Z. Song, T. Xiang, T. M. Hospedales, and

C.-C. Loy. Sketch me that shoe. In CVPR, 2016.

[36] Q. Yu, Y. Yang, F. Liu, Y.-Z. Song, T. Xiang, and T. M.
Sketch-a-net: A deep neural network that
International Journal of Computer Vision,

Hospedales.
beats humans.
122(3):411–425, 2017.

[37] Q. Yu, Y. Yang, Y.-Z. Song, T. Xiang, and T. Hospedales.

Sketch-a-net that beats humans. BMVC, 2015.

[38] X. Yu, T. Liu, X. Wang, and D. Tao. On compressing deep
In CVPR,

models by low rank and sparse decomposition.
pages 7370–7379, 2017.

[39] H. Zhou, J. M. Alvarez, and F. Porikli. Less is more: Towards

compact cnns. In ECCV, pages 662–677, 2016.

[40] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou. Dorefa-
net: Training low bitwidth convolutional neural networks
with low bitwidth gradients. ICLR, 2016.

[41] C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained ternary

quantization. ICLR, 2017.

7. Appendix

A. Introduction

∂E
∂α

= 0,

= 0

∂E
∂β

(9)

The supplementary material consists of the following:

Solving the above, we get the equations:

1. Proof for Theorem 2 (Optimal representation of (cid:102)W)
which provides us with the optimal values of α, β and
e to represent W.

2. Proof for Theorem 4 (Gradient derivation) which al-
lows us to perform back-propagation through the net-
work.

3. Proof for Theorem 1 (Expressibility proof)

4. Experimental details

B. Optimal representation of (cid:102)W

∂E
∂α

= 2αK − 2 · WT e = 0

∂E
∂β

= 2β(n − K) − 2 · WT (1 − e) = 0

We can get the values of α and β from the above equations.

α =

, β =

WT e
K

WT (1 − e)
(n − K)

Then substituting the values of α and β in equation 8, we
get

Theorem 2. 1. The optimal binary weight (cid:102)W which mini-
mizes the error function J is

E =

(cid:102)W = αe + β(1 − e)

where α = WT e
e∗ = argmax

K , β = WT (1−e)
( (cid:107)WT e(cid:107)2
K

n−K
+ (cid:107)WT (1−e)(cid:107)2
n−K

)

e,K

and the optimal e∗ is

Proof. The
[αα . . . βα . . . ββ] can be decomposed as:

approximated weight

vector (cid:102)W =

[αα . . . βα . . . ββ] = α·[11 . . . 01 . . . 00]+β·[00 . . . 10 . . . 11]

where without loss of generality, e ∈ {0, 1}n, eT e > 0, e ∈
{0, 1}n, (1 − e)T (1 − e) > 0 and α, β ∈ R. This is be-
cause the trivial case where e = 0 or e = 1 is covered by
substituting α = β instead and the equation is independent
of e. We have to ﬁnd the values of α and β which would be
the best approximation of this vector.
Let us deﬁne the error function J = W−(α·e+β·(1 − e)).
We have to minimize (cid:107) J (cid:107)2= E, where:

E = (W − (α · e + β · (1 − e)))T (W

− (α · e + β · (1 − e)))

E = WT W + α2 · eT e + β2(1 − e)T (1 − e)

− 2α ·WT e − 2β ·WT (1 − e) + 2αβeT (1 − e)

(cid:107) W||2 +

(cid:107) WT e (cid:107)2
K
(cid:107) WT e (cid:107)2
K

+

− 2

(cid:107) WT (1 − e) (cid:107)2
n − K
(cid:107) WT (1 − e) (cid:107)2
n − K

− 2

(10)

E =

(cid:107) W||2 − (

(cid:107) WT e (cid:107)2
K

+

(cid:107) WT (1 − e) (cid:107)2
n − K

)

(11)

In the above equation, we want to minimize E. Since W is
a given value, we need maximize the second term to mini-
mize the expression. For a given K, eK = sgn(Tk) where
Tk = topk(W, K). Here, topk(W, K) represents the top
K values of W corresponding to either the largest positive
K values or the largest negative K values, which remain as
is whereas the rest are converted to zeros.

e(cid:63) = argmax
(

e,K

(cid:107) WT e (cid:107)2
K

+

(cid:107) WT (1 − e) (cid:107)2
n − K

)

(6)

(7)

Selecting the topk(W, K) would be optimal since
||WT e|| and ||WT (1 − e)|| are both maximized on select-
ing either the largest K positive values or the largest K neg-
ative values. Hence, this allows us to select the optimal e
given a K.

With this, we obtain the optimal e.

where eT e = K, then (1 − e)T (1 − e) = n − K and
eT (1 − e) = 0. Substituting these in, we get

C. Gradient derivation

E = WT W+α2K+β2(n−K)−2α·WT e−2β·WT (1 − e)

(8)
We minimize this equation with respect to α and β giving
us:

W ≈ (cid:102)W = αe + β(1 − e)

where α =

and β =

WT e
K

WT (1 − e)
n − K

Let Tk = topk(W, K), and (cid:103)W1 = αe, and
(cid:103)W2 = β(1 − e).
Considering α, on substituting e = sgn(Tk).

Considering the second term (cid:103)W2, we have,

d(cid:103)W2
dW

= (1 − e)

+ β

dβ
dW

d(1 − e)
dW

α =

WT e
K

∴ α =

WT sgn(Tk)
K

Hence, we have α = WT sgn(Tk)
β = WT (1−sgn(Tk))
have,

and similarly
Putting these back in (cid:102)W, we

n−K

K

.

∴ (cid:102)W =

WT sgn(Tk)
K

◦ sgn(Tk)

+

WT (1 − sgn(Tk))
n − K

◦ (1 − sgn(Tk))

(12)

Now, we compute the derivatives of α and β with respect to
W,

dα
dW

dα
dW

=

=

d(WT sgn(Tk))
dW

.

1
K

d(Tk

T sgn(Tk))
dW

.

1
K

dα
dW

.

1
K

=

=

d(||Tk||l1)
dW
sgn(Tk)
K

Similarly,

dβ
dW

=

=

d(||W − Tk||l1)
dW
sgn(W − Tk)
n − K

.

1
n − K

Now, (cid:103)W1 = αe therefore,

d(cid:103)W1
dW

= e

+ α

dα
dW

de
dW

(13)

(14)

∴ d(cid:103)W1
dW

=

sgn(Tk)
K

◦ sgn(Tk) + α.ST E(Tk)

With this, we end up at the ﬁnal equation for (cid:102)G1 = d(cid:103)W1
mentioned in the paper,

dW as

∴ (cid:102)G1 =

sgn(Tk)
K

◦ sgn(Tk) +

ST E(Tk)

(15)

||Tk||l1
K

∴ d(cid:103)W2
dW

=

sgn(W − Tk)
n − K

◦(1−sgn(Tk))+β.ST E(W−Tk)

This provides us (cid:102)G2 = d(cid:103)W2

dW as mentioned in the paper,

(cid:102)G2 =

◦ (1 − sgn(Tk))

sgn(W − Tk)
n − K

+

||W − Tk||l1
n − K

(16)

.ST E(W − Tk)

Together, we arrive at our ﬁnal gradient (cid:101)G = d(cid:102)W
dW ,

(cid:101)G = (cid:102)G1 + (cid:102)G2

(17)

D. Binary Networks as Approximators

We deﬁne mk as the number of neurons required to ap-
proximate a polynomial of n terms, given the network has a
depth of k. We show that this number is bounded in terms
of n and k.

Theorem 4. For p(x) equal to the product x1x2 · · · xn, and
for any σ with all nonzero Taylor coefﬁcients, we have:

mk(p, σ) = O

(cid:16)

n(k−1)/k · 2n1/k (cid:17)

.

(18)

Proof. We construct a binary network in which groups of
the n inputs are recursively multiplied. The n inputs are
ﬁrst divided into groups of size b1, and each group is mul-
tiplied in the ﬁrst hidden layer using 2b1 binary neurons (as
described in [24]). Thus, the ﬁrst hidden layer includes a
total of 2b1n/b1 binary neurons. This gives us n/b1 values
to multiply, which are in turn divided into groups of size b2.
Each group is multiplied in the second hidden layer using
2b2 neurons. Thus, the second hidden layer includes a total
of 2b2 n/(b1b2) binary neurons.

We continue in this fashion for b1, b2, . . . , bk such that
b1b2 · · · bk = n, giving us one neuron which is the product
of all of our inputs. By considering the total number of
binary neurons used, we conclude

mk(p, σ) ≤

k
(cid:88)

i=1

n
j=1 bj

(cid:81)i

2bi =

k
(cid:88)

k
(cid:89)






 2bi .

bj

i=1

j=i+1

(19)
Setting bi = n1/k, for each i, gives us the desired bound
(18).

E. Expressibility of Binary Networks

A binary neural network (a network with weights having
only two possible values, such as +1 and −1) with a sin-
gle hidden layer of m binary-valued neurons that approxi-
mates a product gate for n inputs can be formally written as
a choice of constants aij and wj satisfying

m
(cid:88)

j=1

(cid:32) n
(cid:88)

i=1

(cid:33)

n
(cid:89)

i=1

wjσ

aijxi

≈

xi.

(20)

[24] shows that 2n neurons are sufﬁcient to approximate
a product gate with n inputs - each of these weights are as-
signed, in the proof, a value of +1 or −1 before normaliza-
tion, and all coefﬁcients aij also have +1/−1 values. This
essentially makes it a binary network. Weight normaliza-
, which
tion introduces a scaling constant of sorts,
would translate to α in our representation, with its negative
denoting β.
The above shows how binary networks are expressive
enough to approximate real-valued networks, without the
need for higher bit quantization.

1
2nn!σn

F. Experimental details

We used the Adam optimizer for all the models with a
maximum learning rate of 0.002 and a minimum learning
rate of 0.00005 with a decay factor of 2. All networks are
trained from scratch. Weights of all layers except the ﬁrst
were binarized throughout our experiments. Our FBin layer
is structured the same as the XNOR-Net. We performed our
experiments using a cluster of GeForce GTX 1080 Tis using
PyTorch v0.2.

Note: The above proofs for expressibility power have

been borrowed from [24].

Distribution-Aware Binarization of Neural Networks for Sketch Recognition

Ameya Prabhu Vishal Batchu Sri Aurobindo Munagala Rohit Gajawada Anoop Namboodiri
Center for Visual Information Technology, Kohli Center on Intelligent Systems
IIIT-Hyderabad, India
{ameya.prabhu@research., vishal.batchu@students., s.munagala@research.,
rohit.gajawada@students., anoop@}iiit.ac.in

8
1
0
2
 
r
p
A
 
9
 
 
]

V
C
.
s
c
[
 
 
1
v
1
4
9
2
0
.
4
0
8
1
:
v
i
X
r
a

Abstract

Deep neural networks are highly effective at a range
of computational tasks. However, they tend to be com-
putationally expensive, especially in vision-related prob-
lems, and also have large memory requirements. One of
the most effective methods to achieve signiﬁcant improve-
ments in computational/spatial efﬁciency is to binarize the
weights and activations in a network. However, naive bina-
rization results in accuracy drops when applied to networks
for most tasks. In this work, we present a highly general-
ized, distribution-aware approach to binarizing deep net-
works that allows us to retain the advantages of a binarized
network, while reducing accuracy drops. We also develop
efﬁcient implementations for our proposed approach across
different architectures. We present a theoretical analysis of
the technique to show the effective representational power
of the resulting layers, and explore the forms of data they
model best. Experiments on popular datasets show that our
technique offers better accuracies than naive binarization,
while retaining the same beneﬁts that binarization provides
- with respect to run-time compression, reduction of compu-
tational costs, and power consumption.

1. Introduction

Deep learning models are pushing the state-of-the-art in
various problems across domains, but are computationally
intensive to train and run, especially Convolutional Neural
Networks (CNNs) used for vision applications. They also
occupy a large amount of memory, and the amount of com-
putation required to train a network leads to high power con-
sumption as well.

There have been many developments in the area of model
compression in the last few years, with the aim of bring-
ing down network runtimes and storage requirements to
mobile-friendly levels. Compression strategies for Convo-
lutional Neural Networks included architectural improve-
ments [16, 20] and re-parametrization [27, 34] to pruning

Figure 1: Weight distribution of a layer with corresponding
α/β values, and the scaling factor α in the XNOR-Net im-
plementation for comparison. α and β in our method have
differing magnitudes, unlike in XNOR-Net.

techniques [14, 25] and quantization [19, 40]. Among these
approaches, quantization - especially, binarization - pro-
vided the most compact models as shown in Table 1.

Quantized networks - where weights/activations were
quantized into low-precision representations - were found to
achieve great model compression. Quantization has proven
to be a powerful compression strategy, especially the most
extreme form of quantization - Binarization. Binarization
has enabled the use of XNOR-Popcount operations for vec-
tor dot products, which take much less time compared to
full-precision Multiply-Accumulates (MACs), contributing
to a huge speedup in convolutional layers [28, 19] on a
general-purpose CPU. Moreover, as each binary weight re-
quires only a single bit to represent, one can achieve drastic
reductions in run-time memory requirements. Previous re-
search [28, 19] shows that it is possible to perform weight
and activation binarization on large networks with up to 58x
speedups and approximately 32x compression ratios, albeit
with signiﬁcant drops in accuracy.

Later works have tended to move away from binary rep-
resentations of weights/inputs to multi-bit representations.
The reason for this was mainly the large accuracy degrada-

Method
Finetuned SVD 2 [34]
Circulant CNN 2 [7]
Adaptive Fastfood-16 [34]
Collins et al. [8]
Zhou et al. [39]
ACDC [27]
Network Pruning [14]
Deep Compression [14]
GreBdec [38]
Srinivas et al. [30]
Guo et al. [13]
Binarization

Compression
2.6x
3.6x
3.7x
4x
4.3x
6.3x
9.1x
9.1x
10.2x
10.3x
17.9x
≈32x

Table 1: Comparison of Binarization and other methods in
terms of compression.

with interesting areas to explore, such as fast classiﬁcation
and sketch-based image retrieval.

Reproducibility: Our implementation can be found on

GitHub 1

2. Related Work

We ask the question: Do CNNs need the representa-
tional power of 32-bit ﬂoating point operations, especially
for binary-valued data such as sketches? Is it possible to
cut down memory costs and make output computations sig-
niﬁcantly less expensive? In recent years, several different
approaches were proposed to achieve network compression
and speedups, and special-purpose networks were proposed
for sketch classiﬁcation/retrieval tasks. These are summa-
rized below:

Sketch Recognition: Many deep-network based works
in the past did not lead to fruitful results before, primarily
due to these networks being better suited for images rather
than sketches. Sketches have signiﬁcantly different char-
acteristics as compared to images, and require specialized,
ﬁne-tuned networks to work with. Sketch-a-Net from Yu
et al. [37] took these factors into account, and proposed a
carefully designed network structure that suited sketch rep-
resentations. Their single-model showed tremendous incre-
ments over the then state-of-the-art, and managed to beat
the average human performance using a Bayesian Fusion
ensemble. Being a signiﬁcant achievement in this problem
- since beating human accuracy in recognition problems is
difﬁcult - this model has been adopted by a number of later
works Bui et al. [4], Yu et al. [35], Wang et al. [33].

Pruning Networks for Compression: Optimal Brain
Damage [10] and Optimal Brain Surgeon [15] introduced
a network pruning technique based on the Hessian of the

1https://github.com/erilyth/DistributionAwareBinarizedNetworks-

WACV18

Figure 2: An example sketch passing through a convolu-
tional layer ﬁlter, with the real-valued ﬁlter shown alongside
corresponding α-β and XNOR-Net ﬁlters. Orange signiﬁes
the highest response areas. We can see that DAB-Net has
signiﬁcantly better responses when compared to XNOR-
Net

tion observed in binary networks. While some works [32]
have proposed methods to recover some of the lost accu-
racy, this leads to the natural question of whether, in the-
ory, binary-representations of neural networks can be used
at all to effectively approximate a full-precision network. If
shown to be sufﬁcient, the search for an optimally accurate
binarization technique is worthwhile, due to the large gains
in speedups (due to binary operations rather than full-prec
MACs) and compression compared to multi-bit representa-
tions.

In our paper, we make the following contributions:
1. We show that binary representations are as expressive
as full precision neural networks for polynomial func-
tions, and offer theoretical insights into the same.
2. We present a generalized, distribution-aware represen-
tation for binary networks, and proceed to calculate the
generalized parameter-values for any binary network.
3. We offer an intuitive analysis and comparison of our
representation vis-a-vis previous representations, as il-
lustrated in Figure 1.

4. We provide a provably efﬁcient implementation of net-

works trained using this representation.

5. We demonstrate the effectiveness of our method by ex-
tensive experiments applying it to popular model ar-
chitectures on large-scale sketch datasets and improv-
ing upon existing binarization approaches.

We also offer intuitions about how this technique might
be effective in problems involving data that is inherently
binary, such as sketches, as shown in Figure 2. Sketches
are a universal form of communication and are easy to draw
through mobile devices - thus emerging as a new paradigm

loss function. Deep Compression [14] also used pruning to
achieve compression by an order of magnitude in various
standard neural networks. It further reduced non-runtime
memory by employing trained quantization and Huffman
coding. Network Slimming [25] introduced a new learn-
ing scheme for CNNs that leverages channel-level sparsity
in networks, and showed compression and speedup with-
out accuracy degradation, with decreased run-time memory
footprint as well. We train our binary models from scratch,
as opposed to using pre-trained networks as in the above
approaches.

Higher Bit Quantization: HashedNets [6] hashed net-
work weights to bin them. Zhou et al. [2] quantized net-
works to 4-bit weights, achieving 8x memory compres-
sion by using 4 bits to represent 16 different values and
1 bit to represent zeros. Trained Ternary Quantization
[41] uses 2-bit weights and scaling factors to bring down
model size to 16x compression, with little accuracy degra-
dation. Quantized Neural Networks[19] use low-precision
quantized weights and inputs and replaces arithmetic op-
erations with bit-wise ones, reducing power consumption.
DoReFa-Net [40] used low bit-width gradients during back-
propagation, and obtained train-time speedups. Ternary
Weight Networks [22] optimize a threshold-based ternary
function for approximation, with stronger expressive abili-
ties than binary networks. The above works cannot leverage
the speedups gained by XNOR/Pop-count operations which
could be performed on dedicated hardware, unlike in our
work. This is our primary motivation for attempting to im-
prove binary algorithms.

Binarization: We provide an optimal method for cal-
culating binary weights, and we show that all of the above
binarization techniques were special cases of our method,
with less accurate approximations. Previous binarization
papers performed binarization independent of the distribu-
tion weights, for example [28]. The method we introduce is
distribution-aware, i.e. looks at the distribution of weights
to calculate an optimal binarization.

BinaryConnect [9] was one of the ﬁrst works to use bi-
nary (+1, -1) values for network parameters, achieving sig-
niﬁcant compression. XNOR-Nets [28] followed the work
of BNNs [18], binarizing both layer weights and inputs and
multiplying them with scaling constants - bringing signiﬁ-
cant speedups by using faster XNOR-Popcount operations
to calculate convolutional outputs. Recent research pro-
posed a variety of additional methods - including novel ac-
tivation functions [5], alternative layers [32], approxima-
tion algorithms [17], ﬁxed point bit-width allocations [23].
Merolla et al. [?] and Anderson et al. [1] offer a few the-
oretical insights and analysis into binary networks. Further
works have extended this in various directions, including
using local binary patterns [21] and lookup-based compres-
sion methods [3].

3. Representational Power of Binary Networks

Many recent works in network compression involve
higher bit weight quantization using two or more bits
[2, 41, 22] instead of binarization, arguing that binary repre-
sentations would not be able to approximate full-precision
networks. In light of this, we explore whether the repre-
sentational power that binary networks can offer is theo-
retically sufﬁcient to get similar representational power as
full-precision networks.

Rolnick et al. [24, 29] have done extensive work in char-
acterizing the expressiveness of neural networks. They
claim that due to the nature of functions - that they de-
pend on real-world physics, in addition to mathematics -
the seemingly huge set of possible functions could be ap-
proximated by deep learning models. From the Universal
Approximation Theorem [11], it is seen that any arbitrary
function can be well-approximated by an Artiﬁcial Neural
Network; but cheap learning, or models with far fewer pa-
rameters than generic ones, are often sufﬁcient to approx-
imate multivariate monomials - which are a class of func-
tions with practical interest, occurring in most real-world
problems.

We can deﬁne a binary neural network having k lay-
ers with activation function σ(x) and consider how many
neurons are required to compute a multivariate monomial
p(x) of degree d. The network takes an n dimensional in-
put x, producing a one dimensional output p(x). We deﬁne
Bk(p, σ) to be the minimum number of binary neurons (ex-
cluding input and output) required to approximate p, where
the error of approximation is of degree at least d + 1 in the
input variables. For instance, B1(p, σ) is the minimal inte-
ger m such that:

m
(cid:88)

j=1

wjσ

(cid:32) n
(cid:88)

i=1

(cid:33)

aijxi

= p(x) + O(xd+1

1 + . . . + xd+1

n ).

Any polynomial can be approximated to high precision
as long as input variables are small enough [24]. Let
B(p, σ) = mink≥0 Bk(p, σ).

Theorem 1. For p(x) equal to the product x1x2 · · · xn, and
for any σ with all nonzero Taylor coefﬁcients, we have one
construction of a binary neural network which meets the
condition

Bk(p, σ) = O

(cid:16)

n(k−1)/k · 2n1/k (cid:17)

.

(1)

Proof of the above can be found in the supplementary

material.

Conjecture III.2. of Rolnick et al. [29] says that this
bound is approximately optimal. If this conjecture proves
to be true, weight-binarized networks would have the same
representational power as full-precision networks, since the

network that was essentially used to prove that the above
theorem - that a network exists that can satisfy that bound -
was a binary network.

The above theorem shows that any neural network that
can be represented as a multivariate polynomial function
is considered as a simpliﬁed model with ELU-like activa-
tions, using continuously differentiable layers - so pool-
ing layers are excluded as well. While there can exist a
deep binary-weight network that can possibly approximate
polynomials similar to full precision networks, it does say
that such a representation would be efﬁciently obtainable
through Stochastic Gradient Descent. Also, this theorem
assumes only weights are binarized, not the activations. Ac-
tivation binarization typically loses a lot of information and
might not be a good thing to do frequently. However, this
insight motivates the fact that more investigation is needed
into approximating networks through binary network struc-
tures.

4. Distribution-Aware Binarization

We have so far established that binary representations are
possibly sufﬁcient to approximate a polynomial with simi-
lar numbers of neurons as a full-precision neural network.
We now investigate the question - What is the most gen-
eral form of binary representation possible? In this section,
we derive a generalized distribution-aware formulation of
binary weights, and provide an efﬁcient implementation of
the same. We consider models binarized with our approach
as DAB-Nets (Distribution Aware Binarized Networks).

We model the loss function layer-wise for the network.
We assume that inputs to the convolutional layers are bi-
nary - i.e. belong to {+1, −1}, and ﬁnd constants α and
β (elaborated below) as a general binary form for layer
weights. These constants are calculated from the distribu-
tion of real-valued weights in a layer - thus making our ap-
proach distribution-aware.

4.1. Derivation

Without loss of generality, we assume that W is a vector
in Rn , where n = c · w · h. We attempt to binarize the
weight vector W to (cid:102)W which takes a form similar to this
example - [αα...βαβ]. Simply put, (cid:102)W is a vector consisting
of scalars α and β, the two values forming the binary vector.
We represent this as (cid:102)W = αe + β(1 − e) where e is a
vector such that e ∈ {0, 1}n (cid:51) e (cid:54)= 0 and e (cid:54)= 1. We
deﬁne K as eT e which represents the number of ones in
the e vector. Our objective is to ﬁnd the best possible binary
approximation for W. We set up the optimization problem
as:

(cid:102)W∗ = argmin

|| W − (cid:102)W ||2

(cid:102)W

We formally state this as the following:

The optimal binary weight vector (cid:102)W∗ for any weight
vector W which minimizes the approximate-error function
J =|| W − (cid:102)W ||2 can be represented as:

(cid:102)W∗ = αe + β(1 − e) where

α =

, β =

WT e
K

WT (1 − e)
n − K

for a given K. That is, given a K, the optimal selection of
e would correspond to either the K smallest weights of W
or the K largest weights of W.

The best suited K, we calculate the value of the follow-
ing expression for every value of K, giving us an e, and
maximize the expression:

e∗ = argmax
(

e

|| WT e ||2
K

+

|| WT (1 − e) ||2
n − K

)

A detailed proof of the above can be found in the supple-
mentary material.

The above representation shows the values obtained
for e, α and β are the optimal approximate representations
of the weight vector W. The vector e, which controls the
number and distribution of occurrences of α and β, acts as
a mask of the top/bottom K values of W. We assign α to
capture the greater of the two values in magnitude. Note
that the scaling values derived in the XNOR formulation,
α and −α, are a special case of the above, and hence our
approximation error is at most that of the XNOR error. We
explore what this function represents and how this relates
to previous binarization techniques in the next subsection.

4.2. Intuitions about DAB-Net

In this section, we investigate intuitions about the de-
rived representation. We can visualize that e and (1 − e)
are orthogonal vectors. Hence, if normalized, e and (1 − e)
form a basis for a subspace R2. Theorem 2 says the best α
and β can be found by essentially projecting the weight ma-
trix W into this subspace, ﬁnding the vector in the subspace
which is closest to e and (1 − e) respectively.

α =

(cid:104)W, e(cid:105)
(cid:104)e, e(cid:105)

· e , β =

(cid:104)W, (1 − e)(cid:105)
(cid:104)(1 − e), (1 − e)(cid:105)

· (1 − e)

We also show that our derived representation is different
from the previous binary representations since we cannot
derive them by assuming a special case of our formulation.
XNOR-Net [28] or BNN [18]-like representations cannot
be obtained from our formulation. However, in practice, we
are able to simulate XNOR-Net by constraining W to be
mean-centered and K = n
2 , since roughly half the weights
are above 0, the other half below, as seen in Figure 5 in
Section 5.3.2.

Algorithm 1 Finding an optimal K value.

// Empty array of same size as W

1: Initialization
2: W = 1D weight vector
3: T = Sum of all the elements of W
4: Sort(W)
5: D = [00...0]
6: optK1 = 0 // Optimal value for K
7: maxD1 = 0 // Value of D for optimal K value
8:
9: for I= 1 to D.size do
Pi = Pi−1 + Wi
10:
Di = P 2
i + (T −Pi)2
if Di ≥ maxD1 then
maxD1 = Di
optK1 = i

11:
12:
13:
14:

n−i

i

15:
16: Sort(W, reverse=true) and Repeat steps 4-13 with

optK2 and maxD2

17:
18: optKf inal = optK1
19: if maxD2 > maxD1 then
20:

optKf inal = optK2

21:
22: return optKf inal

4.3. Implementation

The representation that we earlier derived requires
to be efﬁciently computable, in order to ensure that our
algorithm runs fast enough to be able to train binary
networks.
In this section, we investigate the implemen-
tation, by breaking it into two parts: 1) Computing the
parameter K efﬁciently for every iteration. 2) Training
the entire network using that value of K for a given
iteration. We show that it is possible to get an efﬁciently
trainable network at minimal extra cost. We provide an
efﬁcient algorithm using Dynamic Programming which
computes the optimal value for K quickly at every iteration.

4.3.1 Parallel Preﬁx-Sums to Obtain K

Theorem 2. The optimal K ∗ which minimizes the value e
can be computed in O(n · logn) complexity.

Considering one weight ﬁlter at a time for each convo-
lution layer, we ﬂatten the weights into a 1-dimensional
weight vector W. We then sort the vector in ascending or-
der and then compute the preﬁx-sum array P of W. For
a selected value of K, the term to be maximized would be
i + (T −Pi)2
( ||WT e||2
)
since the top K values in W sum up to Pi where T is the

K + ||WT (1−e)||2

), which is equal to ( P 2

n−K

n−i

i

sum of all weights in W. We also perform the same compu-
tation with a descending order of W’s weights since K can
correspond to either the smallest K weights or the largest K
weights as we mentioned earlier. In order to speed this up,
we perform these operations on all the weight ﬁlters at the
same time considering them as a 2D weight vector instead.
Our algorithm runs in O(n · logn) time complexity, and is
speciﬁed in Algorithm 1. This algorithm is integrated into
our code, and will be provided alongside.

4.3.2 Forward and Backward Pass

Now that we know how to calculate K, e, α, and β for each
ﬁlter in each layer optimally, we can compute (cid:102)W which
approximates W well. Here, topk(W, K) represents the
top K values of W which remain as is whereas the rest are
converted to zeros. Let Tk = topk(W, K).

Corollary 1 (Weight Binarization). The optimal binary
weight (cid:102)W can be represented as,

(cid:102)W = α.sgn(Tk) + β.(1 − sgn(Tk))

where,

α =

and β =

Tk
K

(W − Tk)
n − K

Once we have (cid:102)W, we can perform convolution as I (cid:126)
(cid:102)W during the forward pass of the network. Similarly, the
optimal gradient (cid:101)G can be computed as follows, which is
back-propagated throughout the network in order to update
the weights:

Theorem 3 (Backward Pass). The optimal gradient value
(cid:101)G can be represented as,

(cid:101)G = (cid:102)G1 + (cid:102)G2

(2)

where,

(cid:102)G1 =

sgn(Tk)
K

◦ sgn(Tk) +

.ST E(Tk)

(3)

||Tk||l1
K

(cid:102)G2 =

◦ (1 − sgn(Tk))

.ST E(W − Tk)

sgn(W − Tk)
n − K

+

||W − Tk||l1
n − K
(cid:40)

ST E(Tk)i =

i, where |W|i<= 1

Tk
0, elsewhere

(4)

(5)

The gradient vector, as seen above, can be intuitively
understood if seen as the sum of two independent gradi-
ents (cid:102)G1 and (cid:102)G2, each corresponding to the vectors e and
(1 − e) respectively. Further details regarding the deriva-
tion of this gradient would be provided in the supplemen-
tary material.

Algorithm 2 Training an L-layers CNN with binary
weights:

1: A minibatch of inputs and targets (I, Y), cost function
C(Y, ˆY), current weight Wt and current learning rate
ηt.

2: updated weight Wt+1 and updated learning rate ηt+1.
3: Binarizing weight ﬁlters:
4: Wt = MeanCenter(Wt)
5: Wt = Clamp(Wt, -1, 1)
6: Wreal = Wt
7: for l = 1 to L do
8:
9:

for jth ﬁlter in lth layer do

10:

11:

12:
13:

Find Klj using Algorithm 1
αlj = topk(Wlj ,Klj )
Klj
βlj = − (Wlj −topk(Wlj ,Klj ))
(cid:102)Wlj = α.sgn(topk(Wlj, Klj))

n−Klj

+ β.(1 − sgn(topk(Wlj, Klj)))

14:
15: ˆY = BinaryForward(I, (cid:102)W)
16:
17: ∂C
= BinaryBackward( ∂C
// Standard back-
ˆY
∂ (cid:102)W
ward propagation except that gradients are computed
using (cid:102)W instead of Wt as mentioned in Theorem. 3

, (cid:102)W)

18:
19: We then copy back the real weights in order to apply

the gradients computed. Wt = Wreal

20:
21: Wt+1 = UpdateParameters(Wt, ∂C
∂ (cid:102)W
22: ηt+1 = UpdateLearningrate(ηt, t)

, ηt)

4.4. Training Procedure

Putting all the components mentioned above together,
we have outlined our training procedure in Algorithm 2.
During the forward pass of the network, we ﬁrst mean cen-
ter and clamp the current weights of the network. We then
store a copy of these weights as Wreal. We compute the bi-
nary forward pass of the network, and then apply the back-
ward pass using the weights (cid:102)W, computing gradients for
each of the weights. We then apply these gradients on the
original set of weights Wt in order to obtain Wt+1.
In
essence, binarized weights are used to compute the gra-
dients, but they are applied to the original stored weights
to perform the update. This requires us to store the full
precision weights during training, but once the network is
trained, we store only the binarized weights for inference.

5. Experiments

We empirically demonstrate the effectiveness of our op-
timal distribution-aware binarization algorithm (DAB-Net)

on the TU-Berlin and Sketchy datasets. We compare DAB-
Net with BNN and XNOR-Net [28] on various architec-
tures, on two popular large-scale sketch recognition datasets
as sketches are sparse and binary. Also, they are easier to
train with than standard images, for which we believe the
algorithm needs to be stabilized - in essence, the K value
must be restricted to change by only slight amounts. We
show that our approach is superior to existing binarization
algorithms, and can generalize to different kinds of CNN
architectures on sketches.

5.1. Experimental Setup

In our experiments, we deﬁne the network having only
the convolutional layer weights binarized as WBin, the net-
work having both inputs and weights binarized as FBin
and the original full-precision network as FPrec. Binary
Networks have achieved accuracies comparable to full-
precision networks on limited domain/simpliﬁed datasets
like CIFAR-10, MNIST, SVHN, but show considerable
losses on larger datasets. Binary networks are well suited
for sketch data due to its binary and sparse nature of the
data.

TU-Berlin: The TU-Berlin [12] dataset is the most
popular
large-scale free-hand sketch dataset contain-
ing sketches of 250 categories, with a human sketch-
recognition accuracy of 73.1% on an average.

Sketchy: A recent large-scale free-hand sketch dataset
containing 75,471 hand-drawn sketches spanning 125 cat-
egories. This dataset was primarily used to cross-validate
results obtained on the TU-Berlin dataset, to ensure the ro-
bustness of our approach with respect to the method of data
collection.

For all the datasets, we ﬁrst resized the input images to
256 x 256. A 224 x 224 (225 x 225 for Sketch-A-Net) sized
crop was then randomly taken from an image with standard
augmentations such as rotation and horizontal ﬂipping, for
TU-Berlin and Sketchy. In the TU-Berlin dataset, we use
three-fold cross validation which gives us a 2:1 train-test
split ensuring that our results are comparable with all pre-
vious methods. For Sketchy, we use the training images
for retrieval as the training images for classiﬁcation, and
validation images for retrieval as the validation images for
classiﬁcation. We report ten-crop accuracies on both the
datasets.

We used the PyTorch framework to train our net-
works. We used the Sketch-A-Net[37], ResNet-18[16] and
GoogleNet[31] architectures. Weights of all layers except
the ﬁrst were binarized throughout our experiments, ex-
cept in Sketch-A-Net for which all layers except ﬁrst and
last layers were binarized. All networks were trained from
scratch. We used the Adam optimizer for all experiments.
Note that we do not use a bias term or weight decay for bi-
narized Conv layers. We used a batch size of 256 for all

Models

Method

Accuracies

Improvement XNOR-Net vs DAB-Net

Sketch-A-Net

ResNet-18

GoogleNet

FPrec
WBin (BWN)
FBin (XNOR-Net)
WBin DAB-Net
FBin DAB-Net

FPrec
WBin (BWN)
FBin (XNOR-Net)
WBin DAB-Net
FBin DAB-Net

FPrec
WBin (BWN)
FBin (XNOR-Net)
WBin DAB-Net
FBin DAB-Net

TU-Berlin
72.9%
73.0%
59.6%
72.4%
60.4%
+0.8%
74.1%
73.4%
68.8%
73.5%
71.3%
+2.5%
75.0%
74.8%
72.2%
75.7%
73.7%
+1.5%

Sketchy
85.9%
85.6%
68.6%
84.0%
70.6%
+2.0%
88.7%
89.3%
82.8%
88.8%
84.2%
+1.4%
90.0%
89.8%
86.8%
90.1%
87.4%
+0.6%

Improvement XNOR-Net vs DAB-Net

Models
AlexNet-SVM
AlexNet-Sketch
Sketch-A-Net SC
Humans
Sketch-A-Net-22[36]
Sketch-A-Net WBin DAB-Net
ResNet-18 WBin DAB-Net
GoogleNet WBin DAB-Net
Sketch-A-Net FBin DAB-Net
ResNet-18 FBin DAB-Net
GoogleNet FBin DAB-Net

Accuracy
67.1%
68.6%
72.2%
73.1%
77.0%
72.4%
73.5%
75.7%
60.4%
71.3%
73.7%

Table 3: A comparison between state-of-the-art single
model accuracies of recognition systems on the TU-Berlin
dataset.

Improvement XNOR-Net vs DAB-Net

5.3. XNOR-Net vs DAB-Net

Table 2: Our DAB-Net models compared to FBin, WBin
and FPrec models on TU-Berlin and Sketchy in terms of
accuracy.

Sketch-A-Net models and a batch size of 128 for ResNet-
18 and GoogleNet models, the maximum size that ﬁts in a
1080Ti GPU. Additional experimental details are available
in the supplementary material.

We measure how K, α, and β vary across various lay-
ers over time during training, and these variations are ob-
served to be quite different from their corresponding val-
ues in XNOR-Net. These observations show that bina-
rization can approximate a network much better when it is
distribution-aware (like in our technique) versus when it is
distribution-agnostic (like XNOR-Nets).

5.2. Results

We compare the accuracies of our distribution aware bi-
narization algorithm for WBin and FBin models on the TU-
Berlin and Sketchy datasets. Note that higher accuracies
are an improvement, hence stated in green in Table 2. On
the TU-Berlin and Sketchy datasets in Table 2, we observe
that FBin DAB-Net models consistently perform better over
their XNOR-Net counterparts. They improve upon XNOR-
Net accuracies by 0.8%, 2.5%, and 1.5% in Sketch-A-Net,
ResNet-18, and GoogleNet respectively on the TU-Berlin
dataset. Similarly, they improve by 2.0%, 1.4%, and 0.6%
respectively on the Sketchy dataset. We also compare them
with state-of-the-art sketch classiﬁcation models in Table 3.
We ﬁnd that our compressed models perform signiﬁcantly
better than the original sketch models and offer compres-
sion, runtime and energy savings additionally.

Our DAB-Net WBin models attain accuracies similar to
BWN WBin models and do not offer major improvements
mainly because WBin models achieve FPrec accuracies al-
ready, hence do not have much scope for improvement un-
like FBin models. Thus, we conclude that our DAB-Net
FBin models are able to attain signiﬁcant accuracy improve-
ments over their XNOR-Net counterparts when everything
apart from the binarization method is kept constant.

2It is the sketch-a-net SC model trained with additional imagenet
data, additional data augmentation strategies and considering an ensem-
ble, hence would not be a direct comparison

5.3.1 Variation of α and β across Time

We plot the distribution of weights of a randomly selected
ﬁlter belonging to a layer and observe that α and β of DAB-
Net start out to be similar to α and −α of XNOR-Nets,
since the distributions are randomly initialized. However, as
training progresses, we observe as we go from Subﬁgure (1)
to (4) in Figure 3, the distribution eventually becomes non-
symmetric and complex, hence our values signiﬁcantly di-
verge from their XNOR-Net counterparts. This divergence
signiﬁes a better approximation of the underlying distribu-
tion of weights in our method, giving additional evidence
to our claim that the proposed DAB-Net technique gives a
better representation of layer weights, signiﬁcantly different
from that of XNOR-Nets.

5.3.2 Variation of K across Time and Layers

We deﬁne normalized K as the K
n for a layer ﬁlter. For
XNOR-Nets, K would be the number of values below zero
in a given weight ﬁlter - which has minimal variation, and
does not take into consideration the distribution of weights
in the ﬁlter - as K in this case is simply the number of
weights below a certain ﬁxed global threshold, zero. How-
ever, we observe that the K computed in DAB-Net varies
signiﬁcantly across epochs initially, but slowly converges
to an optimal value for the speciﬁc layer as shown in Figure
4.

(1)

(3)

(2)

(4)

Figure 3: Sub-ﬁgures (1) to (4) show the train-time variation of α and β for a layer ﬁlter. Initially, α and β have nearly
equal magnitudes, similar to the XNOR-Net formulation, but as we progress to (4), we see that α and β have widely different
magnitudes.Having just one scaling constant (XNOR-Net) would be a comparatively poor approximator.

Figure 4: The variation of the normalized K-value
over time during training.
It falls initially but con-
verges eventually to 0.35. The normalized K-value
for XNOR-Net remains almost at 0.5 till the end.

Figure 5: The variation of normalized K values on random
ﬁlters across layers. The K-value corresponding to DAB-
Net varies across layers based on the distribution of weights
of the speciﬁc layer, which is not captured by XNOR-Net.

We also plot the variation of normalized K values for a
few randomly chosen ﬁlters indexes across layers and ob-
serve that it varies across layers, trying to match the distri-
bution of weights at each layer. Each ﬁlter has its own set
of weights, accounting for the differences in variation of K
in each case, as shown in Figure 5.

6. Conclusion

We have proposed an optimal binary representation for
network layer-weights that takes into account the distri-
bution of weights, unlike previous distribution-agnostic

approaches. We showed how this representation could
be computed efﬁciently in n.logn time using dynamic
programming,
thus enabling efﬁcient training on larger
datasets. We applied our technique on various datasets and
noted signiﬁcant accuracy improvements over other full-
binarization approaches. We believe that this work provides
a new perspective on network binarization, and that future
work can gain signiﬁcantly from distribution-aware explo-
rations.

References

[1] A. G. Anderson and C. P. Berg. The high-dimensional
arXiv preprint

geometry of binary neural networks.
arXiv:1705.07199, 2017.

[2] Z. Aojun, Y. Anbang, G. Yiwen, X. Lin, and C. Yurong. In-
cremental network quantization: Towards lossless cnns with
low-precision weights. ICLR, 2017.

[3] H. Bagherinezhad, M. Rastegari, and A. Farhadi. Lcnn:

Lookup-based convolutional neural network. CVPR, 2017.

[4] T. Bui, L. Ribeiro, M. Ponti, and J. P. Collomosse. Generali-
sation and sharing in triplet convnets for sketch based visual
search. CoRR, abs/1611.05301, 2016.

[5] Z. Cai, X. He, J. Sun, and N. Vasconcelos. Deep learn-
ing with low precision by half-wave gaussian quantization.
CVPR, 2017.

[6] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and
Y. Chen. Compressing neural networks with the hashing
trick. ICML, 2015.

[7] Y. Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary,
and S.-F. Chang. An exploration of parameter redundancy
in deep networks with circulant projections. In CVPR, pages
2857–2865, 2015.

[8] M. D. Collins and P. Kohli. Memory bounded deep convolu-
tional networks. arXiv preprint arXiv:1412.1442, 2014.
[9] M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect:
Training deep neural networks with binary weights during
propagations. In NIPS, pages 3123–3131, 2015.

[10] Y. L. Cunn, J. S. Denker, and S. A. Solla. Nips. chapter

Optimal Brain Damage. 1990.

[11] G. Cybenko. Approximation by superpositions of a sig-

moidal function. (MCSS), 2(4):303–314, 1989.

[12] M. Eitz, J. Hays, and M. Alexa. How do humans sketch ob-
jects? ACM Trans. Graph. (Proc. SIGGRAPH), 31(4):44:1–
44:10, 2012.

[13] Y. Guo, A. Yao, and Y. Chen. Dynamic network surgery for

efﬁcient dnns. In NIPS, pages 1379–1387, 2016.

[14] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. ICLR, 2016.

[15] B. Hassibi, D. G. Stork, G. Wolff, and T. Watanabe. Op-
timal brain surgeon: Extensions and performance compar-
isons. NIPS, 1993.

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, pages 770–778, 2016.
[17] L. Hou, Q. Yao, and J. T. Kwok. Loss-aware binarization of

deep networks. ICLR, 2017.

[18] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and

Y. Bengio. Binarized neural networks. 2016.

[19] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Y. Bengio. Quantized neural networks: Training neural net-
works with low precision weights and activations. arXiv
preprint arXiv:1609.07061, 2016.

[20] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy
with 50x fewer parameters and¡ 0.5 mb model size. ICLR,
2017.

[21] F. Juefei-Xu, V. N. Boddeti, and M. Savvides. Local binary

convolutional neural networks. CVPR, 2017.

[22] F. Li, B. Zhang, and B. Liu. Ternary weight networks. arXiv

preprint arXiv:1605.04711, 2016.

[23] D. Lin, S. Talathi, and S. Annapureddy. Fixed point quantiza-
tion of deep convolutional networks. In Proceedings of The
33rd International Conference on Machine Learning, 2016.
[24] H. W. Lin, M. Tegmark, and D. Rolnick. Why does deep and
cheap learning work so well? Journal of Statistical Physics,
168(6):1223–1247, 2017.

[25] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang.
Learning efﬁcient convolutional networks through network
slimming. ICCV, 2017.

[26] P. Merolla, R. Appuswamy, J. V. Arthur, S. K. Esser, and
D. S. Modha. Deep neural networks are robust to weight
binarization and other non-linear distortions. CoRR, 2016.

[27] M. Moczulski, M. Denil, J. Appleyard, and N. de Freitas.
Acdc: A structured efﬁcient linear layer. ICLR, 2016.
[28] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. In ECCV, pages 525–542, 2016.

[29] D. Rolnick and M. Tegmark. The power of deeper net-
arXiv preprint

works for expressing natural functions.
arXiv:1705.05502, 2017.

[30] S. Srinivas, A. Subramanya, and R. V. Babu. Training sparse

neural networks. In CVPRW, pages 455–462, 2017.

[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015.

[32] W. Tang, G. Hua, and L. Wang. How to train a compact
binary neural network with high accuracy? In AAAI, pages
2625–2631, 2017.

[33] X. Wang, X. Duan, and X. Bai. Deep sketch feature for cross-

domain image retrieval. Neurocomputing, 2016.

[34] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. Smola,
L. Song, and Z. Wang. Deep fried convnets. In ICCV, pages
1476–1483, 2015.

[35] Q. Yu, F. Liu, Y.-Z. Song, T. Xiang, T. M. Hospedales, and

C.-C. Loy. Sketch me that shoe. In CVPR, 2016.

[36] Q. Yu, Y. Yang, F. Liu, Y.-Z. Song, T. Xiang, and T. M.
Sketch-a-net: A deep neural network that
International Journal of Computer Vision,

Hospedales.
beats humans.
122(3):411–425, 2017.

[37] Q. Yu, Y. Yang, Y.-Z. Song, T. Xiang, and T. Hospedales.

Sketch-a-net that beats humans. BMVC, 2015.

[38] X. Yu, T. Liu, X. Wang, and D. Tao. On compressing deep
In CVPR,

models by low rank and sparse decomposition.
pages 7370–7379, 2017.

[39] H. Zhou, J. M. Alvarez, and F. Porikli. Less is more: Towards

compact cnns. In ECCV, pages 662–677, 2016.

[40] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou. Dorefa-
net: Training low bitwidth convolutional neural networks
with low bitwidth gradients. ICLR, 2016.

[41] C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained ternary

quantization. ICLR, 2017.

7. Appendix

A. Introduction

∂E
∂α

= 0,

= 0

∂E
∂β

(9)

The supplementary material consists of the following:

Solving the above, we get the equations:

1. Proof for Theorem 2 (Optimal representation of (cid:102)W)
which provides us with the optimal values of α, β and
e to represent W.

2. Proof for Theorem 4 (Gradient derivation) which al-
lows us to perform back-propagation through the net-
work.

3. Proof for Theorem 1 (Expressibility proof)

4. Experimental details

B. Optimal representation of (cid:102)W

∂E
∂α

= 2αK − 2 · WT e = 0

∂E
∂β

= 2β(n − K) − 2 · WT (1 − e) = 0

We can get the values of α and β from the above equations.

α =

, β =

WT e
K

WT (1 − e)
(n − K)

Then substituting the values of α and β in equation 8, we
get

Theorem 2. 1. The optimal binary weight (cid:102)W which mini-
mizes the error function J is

E =

(cid:102)W = αe + β(1 − e)

where α = WT e
e∗ = argmax

K , β = WT (1−e)
( (cid:107)WT e(cid:107)2
K

n−K
+ (cid:107)WT (1−e)(cid:107)2
n−K

)

e,K

and the optimal e∗ is

Proof. The
[αα . . . βα . . . ββ] can be decomposed as:

approximated weight

vector (cid:102)W =

[αα . . . βα . . . ββ] = α·[11 . . . 01 . . . 00]+β·[00 . . . 10 . . . 11]

where without loss of generality, e ∈ {0, 1}n, eT e > 0, e ∈
{0, 1}n, (1 − e)T (1 − e) > 0 and α, β ∈ R. This is be-
cause the trivial case where e = 0 or e = 1 is covered by
substituting α = β instead and the equation is independent
of e. We have to ﬁnd the values of α and β which would be
the best approximation of this vector.
Let us deﬁne the error function J = W−(α·e+β·(1 − e)).
We have to minimize (cid:107) J (cid:107)2= E, where:

E = (W − (α · e + β · (1 − e)))T (W

− (α · e + β · (1 − e)))

E = WT W + α2 · eT e + β2(1 − e)T (1 − e)

− 2α ·WT e − 2β ·WT (1 − e) + 2αβeT (1 − e)

(cid:107) W||2 +

(cid:107) WT e (cid:107)2
K
(cid:107) WT e (cid:107)2
K

+

− 2

(cid:107) WT (1 − e) (cid:107)2
n − K
(cid:107) WT (1 − e) (cid:107)2
n − K

− 2

(10)

E =

(cid:107) W||2 − (

(cid:107) WT e (cid:107)2
K

+

(cid:107) WT (1 − e) (cid:107)2
n − K

)

(11)

In the above equation, we want to minimize E. Since W is
a given value, we need maximize the second term to mini-
mize the expression. For a given K, eK = sgn(Tk) where
Tk = topk(W, K). Here, topk(W, K) represents the top
K values of W corresponding to either the largest positive
K values or the largest negative K values, which remain as
is whereas the rest are converted to zeros.

e(cid:63) = argmax
(

e,K

(cid:107) WT e (cid:107)2
K

+

(cid:107) WT (1 − e) (cid:107)2
n − K

)

(6)

(7)

Selecting the topk(W, K) would be optimal since
||WT e|| and ||WT (1 − e)|| are both maximized on select-
ing either the largest K positive values or the largest K neg-
ative values. Hence, this allows us to select the optimal e
given a K.

With this, we obtain the optimal e.

where eT e = K, then (1 − e)T (1 − e) = n − K and
eT (1 − e) = 0. Substituting these in, we get

C. Gradient derivation

E = WT W+α2K+β2(n−K)−2α·WT e−2β·WT (1 − e)

(8)
We minimize this equation with respect to α and β giving
us:

W ≈ (cid:102)W = αe + β(1 − e)

where α =

and β =

WT e
K

WT (1 − e)
n − K

Let Tk = topk(W, K), and (cid:103)W1 = αe, and
(cid:103)W2 = β(1 − e).
Considering α, on substituting e = sgn(Tk).

Considering the second term (cid:103)W2, we have,

d(cid:103)W2
dW

= (1 − e)

+ β

dβ
dW

d(1 − e)
dW

α =

WT e
K

∴ α =

WT sgn(Tk)
K

Hence, we have α = WT sgn(Tk)
β = WT (1−sgn(Tk))
have,

and similarly
Putting these back in (cid:102)W, we

n−K

K

.

∴ (cid:102)W =

WT sgn(Tk)
K

◦ sgn(Tk)

+

WT (1 − sgn(Tk))
n − K

◦ (1 − sgn(Tk))

(12)

Now, we compute the derivatives of α and β with respect to
W,

dα
dW

dα
dW

=

=

d(WT sgn(Tk))
dW

.

1
K

d(Tk

T sgn(Tk))
dW

.

1
K

dα
dW

.

1
K

=

=

d(||Tk||l1)
dW
sgn(Tk)
K

Similarly,

dβ
dW

=

=

d(||W − Tk||l1)
dW
sgn(W − Tk)
n − K

.

1
n − K

Now, (cid:103)W1 = αe therefore,

d(cid:103)W1
dW

= e

+ α

dα
dW

de
dW

(13)

(14)

∴ d(cid:103)W1
dW

=

sgn(Tk)
K

◦ sgn(Tk) + α.ST E(Tk)

With this, we end up at the ﬁnal equation for (cid:102)G1 = d(cid:103)W1
mentioned in the paper,

dW as

∴ (cid:102)G1 =

sgn(Tk)
K

◦ sgn(Tk) +

ST E(Tk)

(15)

||Tk||l1
K

∴ d(cid:103)W2
dW

=

sgn(W − Tk)
n − K

◦(1−sgn(Tk))+β.ST E(W−Tk)

This provides us (cid:102)G2 = d(cid:103)W2

dW as mentioned in the paper,

(cid:102)G2 =

◦ (1 − sgn(Tk))

sgn(W − Tk)
n − K

+

||W − Tk||l1
n − K

(16)

.ST E(W − Tk)

Together, we arrive at our ﬁnal gradient (cid:101)G = d(cid:102)W
dW ,

(cid:101)G = (cid:102)G1 + (cid:102)G2

(17)

D. Binary Networks as Approximators

We deﬁne mk as the number of neurons required to ap-
proximate a polynomial of n terms, given the network has a
depth of k. We show that this number is bounded in terms
of n and k.

Theorem 4. For p(x) equal to the product x1x2 · · · xn, and
for any σ with all nonzero Taylor coefﬁcients, we have:

mk(p, σ) = O

(cid:16)

n(k−1)/k · 2n1/k (cid:17)

.

(18)

Proof. We construct a binary network in which groups of
the n inputs are recursively multiplied. The n inputs are
ﬁrst divided into groups of size b1, and each group is mul-
tiplied in the ﬁrst hidden layer using 2b1 binary neurons (as
described in [24]). Thus, the ﬁrst hidden layer includes a
total of 2b1n/b1 binary neurons. This gives us n/b1 values
to multiply, which are in turn divided into groups of size b2.
Each group is multiplied in the second hidden layer using
2b2 neurons. Thus, the second hidden layer includes a total
of 2b2 n/(b1b2) binary neurons.

We continue in this fashion for b1, b2, . . . , bk such that
b1b2 · · · bk = n, giving us one neuron which is the product
of all of our inputs. By considering the total number of
binary neurons used, we conclude

mk(p, σ) ≤

k
(cid:88)

i=1

n
j=1 bj

(cid:81)i

2bi =

k
(cid:88)

k
(cid:89)






 2bi .

bj

i=1

j=i+1

(19)
Setting bi = n1/k, for each i, gives us the desired bound
(18).

E. Expressibility of Binary Networks

A binary neural network (a network with weights having
only two possible values, such as +1 and −1) with a sin-
gle hidden layer of m binary-valued neurons that approxi-
mates a product gate for n inputs can be formally written as
a choice of constants aij and wj satisfying

m
(cid:88)

j=1

(cid:32) n
(cid:88)

i=1

(cid:33)

n
(cid:89)

i=1

wjσ

aijxi

≈

xi.

(20)

[24] shows that 2n neurons are sufﬁcient to approximate
a product gate with n inputs - each of these weights are as-
signed, in the proof, a value of +1 or −1 before normaliza-
tion, and all coefﬁcients aij also have +1/−1 values. This
essentially makes it a binary network. Weight normaliza-
, which
tion introduces a scaling constant of sorts,
would translate to α in our representation, with its negative
denoting β.
The above shows how binary networks are expressive
enough to approximate real-valued networks, without the
need for higher bit quantization.

1
2nn!σn

F. Experimental details

We used the Adam optimizer for all the models with a
maximum learning rate of 0.002 and a minimum learning
rate of 0.00005 with a decay factor of 2. All networks are
trained from scratch. Weights of all layers except the ﬁrst
were binarized throughout our experiments. Our FBin layer
is structured the same as the XNOR-Net. We performed our
experiments using a cluster of GeForce GTX 1080 Tis using
PyTorch v0.2.

Note: The above proofs for expressibility power have

been borrowed from [24].

Distribution-Aware Binarization of Neural Networks for Sketch Recognition

Ameya Prabhu Vishal Batchu Sri Aurobindo Munagala Rohit Gajawada Anoop Namboodiri
Center for Visual Information Technology, Kohli Center on Intelligent Systems
IIIT-Hyderabad, India
{ameya.prabhu@research., vishal.batchu@students., s.munagala@research.,
rohit.gajawada@students., anoop@}iiit.ac.in

8
1
0
2
 
r
p
A
 
9
 
 
]

V
C
.
s
c
[
 
 
1
v
1
4
9
2
0
.
4
0
8
1
:
v
i
X
r
a

Abstract

Deep neural networks are highly effective at a range
of computational tasks. However, they tend to be com-
putationally expensive, especially in vision-related prob-
lems, and also have large memory requirements. One of
the most effective methods to achieve signiﬁcant improve-
ments in computational/spatial efﬁciency is to binarize the
weights and activations in a network. However, naive bina-
rization results in accuracy drops when applied to networks
for most tasks. In this work, we present a highly general-
ized, distribution-aware approach to binarizing deep net-
works that allows us to retain the advantages of a binarized
network, while reducing accuracy drops. We also develop
efﬁcient implementations for our proposed approach across
different architectures. We present a theoretical analysis of
the technique to show the effective representational power
of the resulting layers, and explore the forms of data they
model best. Experiments on popular datasets show that our
technique offers better accuracies than naive binarization,
while retaining the same beneﬁts that binarization provides
- with respect to run-time compression, reduction of compu-
tational costs, and power consumption.

1. Introduction

Deep learning models are pushing the state-of-the-art in
various problems across domains, but are computationally
intensive to train and run, especially Convolutional Neural
Networks (CNNs) used for vision applications. They also
occupy a large amount of memory, and the amount of com-
putation required to train a network leads to high power con-
sumption as well.

There have been many developments in the area of model
compression in the last few years, with the aim of bring-
ing down network runtimes and storage requirements to
mobile-friendly levels. Compression strategies for Convo-
lutional Neural Networks included architectural improve-
ments [16, 20] and re-parametrization [27, 34] to pruning

Figure 1: Weight distribution of a layer with corresponding
α/β values, and the scaling factor α in the XNOR-Net im-
plementation for comparison. α and β in our method have
differing magnitudes, unlike in XNOR-Net.

techniques [14, 25] and quantization [19, 40]. Among these
approaches, quantization - especially, binarization - pro-
vided the most compact models as shown in Table 1.

Quantized networks - where weights/activations were
quantized into low-precision representations - were found to
achieve great model compression. Quantization has proven
to be a powerful compression strategy, especially the most
extreme form of quantization - Binarization. Binarization
has enabled the use of XNOR-Popcount operations for vec-
tor dot products, which take much less time compared to
full-precision Multiply-Accumulates (MACs), contributing
to a huge speedup in convolutional layers [28, 19] on a
general-purpose CPU. Moreover, as each binary weight re-
quires only a single bit to represent, one can achieve drastic
reductions in run-time memory requirements. Previous re-
search [28, 19] shows that it is possible to perform weight
and activation binarization on large networks with up to 58x
speedups and approximately 32x compression ratios, albeit
with signiﬁcant drops in accuracy.

Later works have tended to move away from binary rep-
resentations of weights/inputs to multi-bit representations.
The reason for this was mainly the large accuracy degrada-

Method
Finetuned SVD 2 [34]
Circulant CNN 2 [7]
Adaptive Fastfood-16 [34]
Collins et al. [8]
Zhou et al. [39]
ACDC [27]
Network Pruning [14]
Deep Compression [14]
GreBdec [38]
Srinivas et al. [30]
Guo et al. [13]
Binarization

Compression
2.6x
3.6x
3.7x
4x
4.3x
6.3x
9.1x
9.1x
10.2x
10.3x
17.9x
≈32x

Table 1: Comparison of Binarization and other methods in
terms of compression.

with interesting areas to explore, such as fast classiﬁcation
and sketch-based image retrieval.

Reproducibility: Our implementation can be found on

GitHub 1

2. Related Work

We ask the question: Do CNNs need the representa-
tional power of 32-bit ﬂoating point operations, especially
for binary-valued data such as sketches? Is it possible to
cut down memory costs and make output computations sig-
niﬁcantly less expensive? In recent years, several different
approaches were proposed to achieve network compression
and speedups, and special-purpose networks were proposed
for sketch classiﬁcation/retrieval tasks. These are summa-
rized below:

Sketch Recognition: Many deep-network based works
in the past did not lead to fruitful results before, primarily
due to these networks being better suited for images rather
than sketches. Sketches have signiﬁcantly different char-
acteristics as compared to images, and require specialized,
ﬁne-tuned networks to work with. Sketch-a-Net from Yu
et al. [37] took these factors into account, and proposed a
carefully designed network structure that suited sketch rep-
resentations. Their single-model showed tremendous incre-
ments over the then state-of-the-art, and managed to beat
the average human performance using a Bayesian Fusion
ensemble. Being a signiﬁcant achievement in this problem
- since beating human accuracy in recognition problems is
difﬁcult - this model has been adopted by a number of later
works Bui et al. [4], Yu et al. [35], Wang et al. [33].

Pruning Networks for Compression: Optimal Brain
Damage [10] and Optimal Brain Surgeon [15] introduced
a network pruning technique based on the Hessian of the

1https://github.com/erilyth/DistributionAwareBinarizedNetworks-

WACV18

Figure 2: An example sketch passing through a convolu-
tional layer ﬁlter, with the real-valued ﬁlter shown alongside
corresponding α-β and XNOR-Net ﬁlters. Orange signiﬁes
the highest response areas. We can see that DAB-Net has
signiﬁcantly better responses when compared to XNOR-
Net

tion observed in binary networks. While some works [32]
have proposed methods to recover some of the lost accu-
racy, this leads to the natural question of whether, in the-
ory, binary-representations of neural networks can be used
at all to effectively approximate a full-precision network. If
shown to be sufﬁcient, the search for an optimally accurate
binarization technique is worthwhile, due to the large gains
in speedups (due to binary operations rather than full-prec
MACs) and compression compared to multi-bit representa-
tions.

In our paper, we make the following contributions:
1. We show that binary representations are as expressive
as full precision neural networks for polynomial func-
tions, and offer theoretical insights into the same.
2. We present a generalized, distribution-aware represen-
tation for binary networks, and proceed to calculate the
generalized parameter-values for any binary network.
3. We offer an intuitive analysis and comparison of our
representation vis-a-vis previous representations, as il-
lustrated in Figure 1.

4. We provide a provably efﬁcient implementation of net-

works trained using this representation.

5. We demonstrate the effectiveness of our method by ex-
tensive experiments applying it to popular model ar-
chitectures on large-scale sketch datasets and improv-
ing upon existing binarization approaches.

We also offer intuitions about how this technique might
be effective in problems involving data that is inherently
binary, such as sketches, as shown in Figure 2. Sketches
are a universal form of communication and are easy to draw
through mobile devices - thus emerging as a new paradigm

loss function. Deep Compression [14] also used pruning to
achieve compression by an order of magnitude in various
standard neural networks. It further reduced non-runtime
memory by employing trained quantization and Huffman
coding. Network Slimming [25] introduced a new learn-
ing scheme for CNNs that leverages channel-level sparsity
in networks, and showed compression and speedup with-
out accuracy degradation, with decreased run-time memory
footprint as well. We train our binary models from scratch,
as opposed to using pre-trained networks as in the above
approaches.

Higher Bit Quantization: HashedNets [6] hashed net-
work weights to bin them. Zhou et al. [2] quantized net-
works to 4-bit weights, achieving 8x memory compres-
sion by using 4 bits to represent 16 different values and
1 bit to represent zeros. Trained Ternary Quantization
[41] uses 2-bit weights and scaling factors to bring down
model size to 16x compression, with little accuracy degra-
dation. Quantized Neural Networks[19] use low-precision
quantized weights and inputs and replaces arithmetic op-
erations with bit-wise ones, reducing power consumption.
DoReFa-Net [40] used low bit-width gradients during back-
propagation, and obtained train-time speedups. Ternary
Weight Networks [22] optimize a threshold-based ternary
function for approximation, with stronger expressive abili-
ties than binary networks. The above works cannot leverage
the speedups gained by XNOR/Pop-count operations which
could be performed on dedicated hardware, unlike in our
work. This is our primary motivation for attempting to im-
prove binary algorithms.

Binarization: We provide an optimal method for cal-
culating binary weights, and we show that all of the above
binarization techniques were special cases of our method,
with less accurate approximations. Previous binarization
papers performed binarization independent of the distribu-
tion weights, for example [28]. The method we introduce is
distribution-aware, i.e. looks at the distribution of weights
to calculate an optimal binarization.

BinaryConnect [9] was one of the ﬁrst works to use bi-
nary (+1, -1) values for network parameters, achieving sig-
niﬁcant compression. XNOR-Nets [28] followed the work
of BNNs [18], binarizing both layer weights and inputs and
multiplying them with scaling constants - bringing signiﬁ-
cant speedups by using faster XNOR-Popcount operations
to calculate convolutional outputs. Recent research pro-
posed a variety of additional methods - including novel ac-
tivation functions [5], alternative layers [32], approxima-
tion algorithms [17], ﬁxed point bit-width allocations [23].
Merolla et al. [?] and Anderson et al. [1] offer a few the-
oretical insights and analysis into binary networks. Further
works have extended this in various directions, including
using local binary patterns [21] and lookup-based compres-
sion methods [3].

3. Representational Power of Binary Networks

Many recent works in network compression involve
higher bit weight quantization using two or more bits
[2, 41, 22] instead of binarization, arguing that binary repre-
sentations would not be able to approximate full-precision
networks. In light of this, we explore whether the repre-
sentational power that binary networks can offer is theo-
retically sufﬁcient to get similar representational power as
full-precision networks.

Rolnick et al. [24, 29] have done extensive work in char-
acterizing the expressiveness of neural networks. They
claim that due to the nature of functions - that they de-
pend on real-world physics, in addition to mathematics -
the seemingly huge set of possible functions could be ap-
proximated by deep learning models. From the Universal
Approximation Theorem [11], it is seen that any arbitrary
function can be well-approximated by an Artiﬁcial Neural
Network; but cheap learning, or models with far fewer pa-
rameters than generic ones, are often sufﬁcient to approx-
imate multivariate monomials - which are a class of func-
tions with practical interest, occurring in most real-world
problems.

We can deﬁne a binary neural network having k lay-
ers with activation function σ(x) and consider how many
neurons are required to compute a multivariate monomial
p(x) of degree d. The network takes an n dimensional in-
put x, producing a one dimensional output p(x). We deﬁne
Bk(p, σ) to be the minimum number of binary neurons (ex-
cluding input and output) required to approximate p, where
the error of approximation is of degree at least d + 1 in the
input variables. For instance, B1(p, σ) is the minimal inte-
ger m such that:

m
(cid:88)

j=1

wjσ

(cid:32) n
(cid:88)

i=1

(cid:33)

aijxi

= p(x) + O(xd+1

1 + . . . + xd+1

n ).

Any polynomial can be approximated to high precision
as long as input variables are small enough [24]. Let
B(p, σ) = mink≥0 Bk(p, σ).

Theorem 1. For p(x) equal to the product x1x2 · · · xn, and
for any σ with all nonzero Taylor coefﬁcients, we have one
construction of a binary neural network which meets the
condition

Bk(p, σ) = O

(cid:16)

n(k−1)/k · 2n1/k (cid:17)

.

(1)

Proof of the above can be found in the supplementary

material.

Conjecture III.2. of Rolnick et al. [29] says that this
bound is approximately optimal. If this conjecture proves
to be true, weight-binarized networks would have the same
representational power as full-precision networks, since the

network that was essentially used to prove that the above
theorem - that a network exists that can satisfy that bound -
was a binary network.

The above theorem shows that any neural network that
can be represented as a multivariate polynomial function
is considered as a simpliﬁed model with ELU-like activa-
tions, using continuously differentiable layers - so pool-
ing layers are excluded as well. While there can exist a
deep binary-weight network that can possibly approximate
polynomials similar to full precision networks, it does say
that such a representation would be efﬁciently obtainable
through Stochastic Gradient Descent. Also, this theorem
assumes only weights are binarized, not the activations. Ac-
tivation binarization typically loses a lot of information and
might not be a good thing to do frequently. However, this
insight motivates the fact that more investigation is needed
into approximating networks through binary network struc-
tures.

4. Distribution-Aware Binarization

We have so far established that binary representations are
possibly sufﬁcient to approximate a polynomial with simi-
lar numbers of neurons as a full-precision neural network.
We now investigate the question - What is the most gen-
eral form of binary representation possible? In this section,
we derive a generalized distribution-aware formulation of
binary weights, and provide an efﬁcient implementation of
the same. We consider models binarized with our approach
as DAB-Nets (Distribution Aware Binarized Networks).

We model the loss function layer-wise for the network.
We assume that inputs to the convolutional layers are bi-
nary - i.e. belong to {+1, −1}, and ﬁnd constants α and
β (elaborated below) as a general binary form for layer
weights. These constants are calculated from the distribu-
tion of real-valued weights in a layer - thus making our ap-
proach distribution-aware.

4.1. Derivation

Without loss of generality, we assume that W is a vector
in Rn , where n = c · w · h. We attempt to binarize the
weight vector W to (cid:102)W which takes a form similar to this
example - [αα...βαβ]. Simply put, (cid:102)W is a vector consisting
of scalars α and β, the two values forming the binary vector.
We represent this as (cid:102)W = αe + β(1 − e) where e is a
vector such that e ∈ {0, 1}n (cid:51) e (cid:54)= 0 and e (cid:54)= 1. We
deﬁne K as eT e which represents the number of ones in
the e vector. Our objective is to ﬁnd the best possible binary
approximation for W. We set up the optimization problem
as:

(cid:102)W∗ = argmin

|| W − (cid:102)W ||2

(cid:102)W

We formally state this as the following:

The optimal binary weight vector (cid:102)W∗ for any weight
vector W which minimizes the approximate-error function
J =|| W − (cid:102)W ||2 can be represented as:

(cid:102)W∗ = αe + β(1 − e) where

α =

, β =

WT e
K

WT (1 − e)
n − K

for a given K. That is, given a K, the optimal selection of
e would correspond to either the K smallest weights of W
or the K largest weights of W.

The best suited K, we calculate the value of the follow-
ing expression for every value of K, giving us an e, and
maximize the expression:

e∗ = argmax
(

e

|| WT e ||2
K

+

|| WT (1 − e) ||2
n − K

)

A detailed proof of the above can be found in the supple-
mentary material.

The above representation shows the values obtained
for e, α and β are the optimal approximate representations
of the weight vector W. The vector e, which controls the
number and distribution of occurrences of α and β, acts as
a mask of the top/bottom K values of W. We assign α to
capture the greater of the two values in magnitude. Note
that the scaling values derived in the XNOR formulation,
α and −α, are a special case of the above, and hence our
approximation error is at most that of the XNOR error. We
explore what this function represents and how this relates
to previous binarization techniques in the next subsection.

4.2. Intuitions about DAB-Net

In this section, we investigate intuitions about the de-
rived representation. We can visualize that e and (1 − e)
are orthogonal vectors. Hence, if normalized, e and (1 − e)
form a basis for a subspace R2. Theorem 2 says the best α
and β can be found by essentially projecting the weight ma-
trix W into this subspace, ﬁnding the vector in the subspace
which is closest to e and (1 − e) respectively.

α =

(cid:104)W, e(cid:105)
(cid:104)e, e(cid:105)

· e , β =

(cid:104)W, (1 − e)(cid:105)
(cid:104)(1 − e), (1 − e)(cid:105)

· (1 − e)

We also show that our derived representation is different
from the previous binary representations since we cannot
derive them by assuming a special case of our formulation.
XNOR-Net [28] or BNN [18]-like representations cannot
be obtained from our formulation. However, in practice, we
are able to simulate XNOR-Net by constraining W to be
mean-centered and K = n
2 , since roughly half the weights
are above 0, the other half below, as seen in Figure 5 in
Section 5.3.2.

Algorithm 1 Finding an optimal K value.

// Empty array of same size as W

1: Initialization
2: W = 1D weight vector
3: T = Sum of all the elements of W
4: Sort(W)
5: D = [00...0]
6: optK1 = 0 // Optimal value for K
7: maxD1 = 0 // Value of D for optimal K value
8:
9: for I= 1 to D.size do
Pi = Pi−1 + Wi
10:
Di = P 2
i + (T −Pi)2
if Di ≥ maxD1 then
maxD1 = Di
optK1 = i

11:
12:
13:
14:

n−i

i

15:
16: Sort(W, reverse=true) and Repeat steps 4-13 with

optK2 and maxD2

17:
18: optKf inal = optK1
19: if maxD2 > maxD1 then
20:

optKf inal = optK2

21:
22: return optKf inal

4.3. Implementation

The representation that we earlier derived requires
to be efﬁciently computable, in order to ensure that our
algorithm runs fast enough to be able to train binary
networks.
In this section, we investigate the implemen-
tation, by breaking it into two parts: 1) Computing the
parameter K efﬁciently for every iteration. 2) Training
the entire network using that value of K for a given
iteration. We show that it is possible to get an efﬁciently
trainable network at minimal extra cost. We provide an
efﬁcient algorithm using Dynamic Programming which
computes the optimal value for K quickly at every iteration.

4.3.1 Parallel Preﬁx-Sums to Obtain K

Theorem 2. The optimal K ∗ which minimizes the value e
can be computed in O(n · logn) complexity.

Considering one weight ﬁlter at a time for each convo-
lution layer, we ﬂatten the weights into a 1-dimensional
weight vector W. We then sort the vector in ascending or-
der and then compute the preﬁx-sum array P of W. For
a selected value of K, the term to be maximized would be
i + (T −Pi)2
( ||WT e||2
)
since the top K values in W sum up to Pi where T is the

K + ||WT (1−e)||2

), which is equal to ( P 2

n−K

n−i

i

sum of all weights in W. We also perform the same compu-
tation with a descending order of W’s weights since K can
correspond to either the smallest K weights or the largest K
weights as we mentioned earlier. In order to speed this up,
we perform these operations on all the weight ﬁlters at the
same time considering them as a 2D weight vector instead.
Our algorithm runs in O(n · logn) time complexity, and is
speciﬁed in Algorithm 1. This algorithm is integrated into
our code, and will be provided alongside.

4.3.2 Forward and Backward Pass

Now that we know how to calculate K, e, α, and β for each
ﬁlter in each layer optimally, we can compute (cid:102)W which
approximates W well. Here, topk(W, K) represents the
top K values of W which remain as is whereas the rest are
converted to zeros. Let Tk = topk(W, K).

Corollary 1 (Weight Binarization). The optimal binary
weight (cid:102)W can be represented as,

(cid:102)W = α.sgn(Tk) + β.(1 − sgn(Tk))

where,

α =

and β =

Tk
K

(W − Tk)
n − K

Once we have (cid:102)W, we can perform convolution as I (cid:126)
(cid:102)W during the forward pass of the network. Similarly, the
optimal gradient (cid:101)G can be computed as follows, which is
back-propagated throughout the network in order to update
the weights:

Theorem 3 (Backward Pass). The optimal gradient value
(cid:101)G can be represented as,

(cid:101)G = (cid:102)G1 + (cid:102)G2

(2)

where,

(cid:102)G1 =

sgn(Tk)
K

◦ sgn(Tk) +

.ST E(Tk)

(3)

||Tk||l1
K

(cid:102)G2 =

◦ (1 − sgn(Tk))

.ST E(W − Tk)

sgn(W − Tk)
n − K

+

||W − Tk||l1
n − K
(cid:40)

ST E(Tk)i =

i, where |W|i<= 1

Tk
0, elsewhere

(4)

(5)

The gradient vector, as seen above, can be intuitively
understood if seen as the sum of two independent gradi-
ents (cid:102)G1 and (cid:102)G2, each corresponding to the vectors e and
(1 − e) respectively. Further details regarding the deriva-
tion of this gradient would be provided in the supplemen-
tary material.

Algorithm 2 Training an L-layers CNN with binary
weights:

1: A minibatch of inputs and targets (I, Y), cost function
C(Y, ˆY), current weight Wt and current learning rate
ηt.

2: updated weight Wt+1 and updated learning rate ηt+1.
3: Binarizing weight ﬁlters:
4: Wt = MeanCenter(Wt)
5: Wt = Clamp(Wt, -1, 1)
6: Wreal = Wt
7: for l = 1 to L do
8:
9:

for jth ﬁlter in lth layer do

10:

11:

12:
13:

Find Klj using Algorithm 1
αlj = topk(Wlj ,Klj )
Klj
βlj = − (Wlj −topk(Wlj ,Klj ))
(cid:102)Wlj = α.sgn(topk(Wlj, Klj))

n−Klj

+ β.(1 − sgn(topk(Wlj, Klj)))

14:
15: ˆY = BinaryForward(I, (cid:102)W)
16:
17: ∂C
= BinaryBackward( ∂C
// Standard back-
ˆY
∂ (cid:102)W
ward propagation except that gradients are computed
using (cid:102)W instead of Wt as mentioned in Theorem. 3

, (cid:102)W)

18:
19: We then copy back the real weights in order to apply

the gradients computed. Wt = Wreal

20:
21: Wt+1 = UpdateParameters(Wt, ∂C
∂ (cid:102)W
22: ηt+1 = UpdateLearningrate(ηt, t)

, ηt)

4.4. Training Procedure

Putting all the components mentioned above together,
we have outlined our training procedure in Algorithm 2.
During the forward pass of the network, we ﬁrst mean cen-
ter and clamp the current weights of the network. We then
store a copy of these weights as Wreal. We compute the bi-
nary forward pass of the network, and then apply the back-
ward pass using the weights (cid:102)W, computing gradients for
each of the weights. We then apply these gradients on the
original set of weights Wt in order to obtain Wt+1.
In
essence, binarized weights are used to compute the gra-
dients, but they are applied to the original stored weights
to perform the update. This requires us to store the full
precision weights during training, but once the network is
trained, we store only the binarized weights for inference.

5. Experiments

We empirically demonstrate the effectiveness of our op-
timal distribution-aware binarization algorithm (DAB-Net)

on the TU-Berlin and Sketchy datasets. We compare DAB-
Net with BNN and XNOR-Net [28] on various architec-
tures, on two popular large-scale sketch recognition datasets
as sketches are sparse and binary. Also, they are easier to
train with than standard images, for which we believe the
algorithm needs to be stabilized - in essence, the K value
must be restricted to change by only slight amounts. We
show that our approach is superior to existing binarization
algorithms, and can generalize to different kinds of CNN
architectures on sketches.

5.1. Experimental Setup

In our experiments, we deﬁne the network having only
the convolutional layer weights binarized as WBin, the net-
work having both inputs and weights binarized as FBin
and the original full-precision network as FPrec. Binary
Networks have achieved accuracies comparable to full-
precision networks on limited domain/simpliﬁed datasets
like CIFAR-10, MNIST, SVHN, but show considerable
losses on larger datasets. Binary networks are well suited
for sketch data due to its binary and sparse nature of the
data.

TU-Berlin: The TU-Berlin [12] dataset is the most
popular
large-scale free-hand sketch dataset contain-
ing sketches of 250 categories, with a human sketch-
recognition accuracy of 73.1% on an average.

Sketchy: A recent large-scale free-hand sketch dataset
containing 75,471 hand-drawn sketches spanning 125 cat-
egories. This dataset was primarily used to cross-validate
results obtained on the TU-Berlin dataset, to ensure the ro-
bustness of our approach with respect to the method of data
collection.

For all the datasets, we ﬁrst resized the input images to
256 x 256. A 224 x 224 (225 x 225 for Sketch-A-Net) sized
crop was then randomly taken from an image with standard
augmentations such as rotation and horizontal ﬂipping, for
TU-Berlin and Sketchy. In the TU-Berlin dataset, we use
three-fold cross validation which gives us a 2:1 train-test
split ensuring that our results are comparable with all pre-
vious methods. For Sketchy, we use the training images
for retrieval as the training images for classiﬁcation, and
validation images for retrieval as the validation images for
classiﬁcation. We report ten-crop accuracies on both the
datasets.

We used the PyTorch framework to train our net-
works. We used the Sketch-A-Net[37], ResNet-18[16] and
GoogleNet[31] architectures. Weights of all layers except
the ﬁrst were binarized throughout our experiments, ex-
cept in Sketch-A-Net for which all layers except ﬁrst and
last layers were binarized. All networks were trained from
scratch. We used the Adam optimizer for all experiments.
Note that we do not use a bias term or weight decay for bi-
narized Conv layers. We used a batch size of 256 for all

Models

Method

Accuracies

Improvement XNOR-Net vs DAB-Net

Sketch-A-Net

ResNet-18

GoogleNet

FPrec
WBin (BWN)
FBin (XNOR-Net)
WBin DAB-Net
FBin DAB-Net

FPrec
WBin (BWN)
FBin (XNOR-Net)
WBin DAB-Net
FBin DAB-Net

FPrec
WBin (BWN)
FBin (XNOR-Net)
WBin DAB-Net
FBin DAB-Net

TU-Berlin
72.9%
73.0%
59.6%
72.4%
60.4%
+0.8%
74.1%
73.4%
68.8%
73.5%
71.3%
+2.5%
75.0%
74.8%
72.2%
75.7%
73.7%
+1.5%

Sketchy
85.9%
85.6%
68.6%
84.0%
70.6%
+2.0%
88.7%
89.3%
82.8%
88.8%
84.2%
+1.4%
90.0%
89.8%
86.8%
90.1%
87.4%
+0.6%

Improvement XNOR-Net vs DAB-Net

Models
AlexNet-SVM
AlexNet-Sketch
Sketch-A-Net SC
Humans
Sketch-A-Net-22[36]
Sketch-A-Net WBin DAB-Net
ResNet-18 WBin DAB-Net
GoogleNet WBin DAB-Net
Sketch-A-Net FBin DAB-Net
ResNet-18 FBin DAB-Net
GoogleNet FBin DAB-Net

Accuracy
67.1%
68.6%
72.2%
73.1%
77.0%
72.4%
73.5%
75.7%
60.4%
71.3%
73.7%

Table 3: A comparison between state-of-the-art single
model accuracies of recognition systems on the TU-Berlin
dataset.

Improvement XNOR-Net vs DAB-Net

5.3. XNOR-Net vs DAB-Net

Table 2: Our DAB-Net models compared to FBin, WBin
and FPrec models on TU-Berlin and Sketchy in terms of
accuracy.

Sketch-A-Net models and a batch size of 128 for ResNet-
18 and GoogleNet models, the maximum size that ﬁts in a
1080Ti GPU. Additional experimental details are available
in the supplementary material.

We measure how K, α, and β vary across various lay-
ers over time during training, and these variations are ob-
served to be quite different from their corresponding val-
ues in XNOR-Net. These observations show that bina-
rization can approximate a network much better when it is
distribution-aware (like in our technique) versus when it is
distribution-agnostic (like XNOR-Nets).

5.2. Results

We compare the accuracies of our distribution aware bi-
narization algorithm for WBin and FBin models on the TU-
Berlin and Sketchy datasets. Note that higher accuracies
are an improvement, hence stated in green in Table 2. On
the TU-Berlin and Sketchy datasets in Table 2, we observe
that FBin DAB-Net models consistently perform better over
their XNOR-Net counterparts. They improve upon XNOR-
Net accuracies by 0.8%, 2.5%, and 1.5% in Sketch-A-Net,
ResNet-18, and GoogleNet respectively on the TU-Berlin
dataset. Similarly, they improve by 2.0%, 1.4%, and 0.6%
respectively on the Sketchy dataset. We also compare them
with state-of-the-art sketch classiﬁcation models in Table 3.
We ﬁnd that our compressed models perform signiﬁcantly
better than the original sketch models and offer compres-
sion, runtime and energy savings additionally.

Our DAB-Net WBin models attain accuracies similar to
BWN WBin models and do not offer major improvements
mainly because WBin models achieve FPrec accuracies al-
ready, hence do not have much scope for improvement un-
like FBin models. Thus, we conclude that our DAB-Net
FBin models are able to attain signiﬁcant accuracy improve-
ments over their XNOR-Net counterparts when everything
apart from the binarization method is kept constant.

2It is the sketch-a-net SC model trained with additional imagenet
data, additional data augmentation strategies and considering an ensem-
ble, hence would not be a direct comparison

5.3.1 Variation of α and β across Time

We plot the distribution of weights of a randomly selected
ﬁlter belonging to a layer and observe that α and β of DAB-
Net start out to be similar to α and −α of XNOR-Nets,
since the distributions are randomly initialized. However, as
training progresses, we observe as we go from Subﬁgure (1)
to (4) in Figure 3, the distribution eventually becomes non-
symmetric and complex, hence our values signiﬁcantly di-
verge from their XNOR-Net counterparts. This divergence
signiﬁes a better approximation of the underlying distribu-
tion of weights in our method, giving additional evidence
to our claim that the proposed DAB-Net technique gives a
better representation of layer weights, signiﬁcantly different
from that of XNOR-Nets.

5.3.2 Variation of K across Time and Layers

We deﬁne normalized K as the K
n for a layer ﬁlter. For
XNOR-Nets, K would be the number of values below zero
in a given weight ﬁlter - which has minimal variation, and
does not take into consideration the distribution of weights
in the ﬁlter - as K in this case is simply the number of
weights below a certain ﬁxed global threshold, zero. How-
ever, we observe that the K computed in DAB-Net varies
signiﬁcantly across epochs initially, but slowly converges
to an optimal value for the speciﬁc layer as shown in Figure
4.

(1)

(3)

(2)

(4)

Figure 3: Sub-ﬁgures (1) to (4) show the train-time variation of α and β for a layer ﬁlter. Initially, α and β have nearly
equal magnitudes, similar to the XNOR-Net formulation, but as we progress to (4), we see that α and β have widely different
magnitudes.Having just one scaling constant (XNOR-Net) would be a comparatively poor approximator.

Figure 4: The variation of the normalized K-value
over time during training.
It falls initially but con-
verges eventually to 0.35. The normalized K-value
for XNOR-Net remains almost at 0.5 till the end.

Figure 5: The variation of normalized K values on random
ﬁlters across layers. The K-value corresponding to DAB-
Net varies across layers based on the distribution of weights
of the speciﬁc layer, which is not captured by XNOR-Net.

We also plot the variation of normalized K values for a
few randomly chosen ﬁlters indexes across layers and ob-
serve that it varies across layers, trying to match the distri-
bution of weights at each layer. Each ﬁlter has its own set
of weights, accounting for the differences in variation of K
in each case, as shown in Figure 5.

6. Conclusion

We have proposed an optimal binary representation for
network layer-weights that takes into account the distri-
bution of weights, unlike previous distribution-agnostic

approaches. We showed how this representation could
be computed efﬁciently in n.logn time using dynamic
programming,
thus enabling efﬁcient training on larger
datasets. We applied our technique on various datasets and
noted signiﬁcant accuracy improvements over other full-
binarization approaches. We believe that this work provides
a new perspective on network binarization, and that future
work can gain signiﬁcantly from distribution-aware explo-
rations.

References

[1] A. G. Anderson and C. P. Berg. The high-dimensional
arXiv preprint

geometry of binary neural networks.
arXiv:1705.07199, 2017.

[2] Z. Aojun, Y. Anbang, G. Yiwen, X. Lin, and C. Yurong. In-
cremental network quantization: Towards lossless cnns with
low-precision weights. ICLR, 2017.

[3] H. Bagherinezhad, M. Rastegari, and A. Farhadi. Lcnn:

Lookup-based convolutional neural network. CVPR, 2017.

[4] T. Bui, L. Ribeiro, M. Ponti, and J. P. Collomosse. Generali-
sation and sharing in triplet convnets for sketch based visual
search. CoRR, abs/1611.05301, 2016.

[5] Z. Cai, X. He, J. Sun, and N. Vasconcelos. Deep learn-
ing with low precision by half-wave gaussian quantization.
CVPR, 2017.

[6] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and
Y. Chen. Compressing neural networks with the hashing
trick. ICML, 2015.

[7] Y. Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary,
and S.-F. Chang. An exploration of parameter redundancy
in deep networks with circulant projections. In CVPR, pages
2857–2865, 2015.

[8] M. D. Collins and P. Kohli. Memory bounded deep convolu-
tional networks. arXiv preprint arXiv:1412.1442, 2014.
[9] M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect:
Training deep neural networks with binary weights during
propagations. In NIPS, pages 3123–3131, 2015.

[10] Y. L. Cunn, J. S. Denker, and S. A. Solla. Nips. chapter

Optimal Brain Damage. 1990.

[11] G. Cybenko. Approximation by superpositions of a sig-

moidal function. (MCSS), 2(4):303–314, 1989.

[12] M. Eitz, J. Hays, and M. Alexa. How do humans sketch ob-
jects? ACM Trans. Graph. (Proc. SIGGRAPH), 31(4):44:1–
44:10, 2012.

[13] Y. Guo, A. Yao, and Y. Chen. Dynamic network surgery for

efﬁcient dnns. In NIPS, pages 1379–1387, 2016.

[14] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. ICLR, 2016.

[15] B. Hassibi, D. G. Stork, G. Wolff, and T. Watanabe. Op-
timal brain surgeon: Extensions and performance compar-
isons. NIPS, 1993.

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, pages 770–778, 2016.
[17] L. Hou, Q. Yao, and J. T. Kwok. Loss-aware binarization of

deep networks. ICLR, 2017.

[18] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and

Y. Bengio. Binarized neural networks. 2016.

[19] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Y. Bengio. Quantized neural networks: Training neural net-
works with low precision weights and activations. arXiv
preprint arXiv:1609.07061, 2016.

[20] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy
with 50x fewer parameters and¡ 0.5 mb model size. ICLR,
2017.

[21] F. Juefei-Xu, V. N. Boddeti, and M. Savvides. Local binary

convolutional neural networks. CVPR, 2017.

[22] F. Li, B. Zhang, and B. Liu. Ternary weight networks. arXiv

preprint arXiv:1605.04711, 2016.

[23] D. Lin, S. Talathi, and S. Annapureddy. Fixed point quantiza-
tion of deep convolutional networks. In Proceedings of The
33rd International Conference on Machine Learning, 2016.
[24] H. W. Lin, M. Tegmark, and D. Rolnick. Why does deep and
cheap learning work so well? Journal of Statistical Physics,
168(6):1223–1247, 2017.

[25] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang.
Learning efﬁcient convolutional networks through network
slimming. ICCV, 2017.

[26] P. Merolla, R. Appuswamy, J. V. Arthur, S. K. Esser, and
D. S. Modha. Deep neural networks are robust to weight
binarization and other non-linear distortions. CoRR, 2016.

[27] M. Moczulski, M. Denil, J. Appleyard, and N. de Freitas.
Acdc: A structured efﬁcient linear layer. ICLR, 2016.
[28] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. In ECCV, pages 525–542, 2016.

[29] D. Rolnick and M. Tegmark. The power of deeper net-
arXiv preprint

works for expressing natural functions.
arXiv:1705.05502, 2017.

[30] S. Srinivas, A. Subramanya, and R. V. Babu. Training sparse

neural networks. In CVPRW, pages 455–462, 2017.

[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015.

[32] W. Tang, G. Hua, and L. Wang. How to train a compact
binary neural network with high accuracy? In AAAI, pages
2625–2631, 2017.

[33] X. Wang, X. Duan, and X. Bai. Deep sketch feature for cross-

domain image retrieval. Neurocomputing, 2016.

[34] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. Smola,
L. Song, and Z. Wang. Deep fried convnets. In ICCV, pages
1476–1483, 2015.

[35] Q. Yu, F. Liu, Y.-Z. Song, T. Xiang, T. M. Hospedales, and

C.-C. Loy. Sketch me that shoe. In CVPR, 2016.

[36] Q. Yu, Y. Yang, F. Liu, Y.-Z. Song, T. Xiang, and T. M.
Sketch-a-net: A deep neural network that
International Journal of Computer Vision,

Hospedales.
beats humans.
122(3):411–425, 2017.

[37] Q. Yu, Y. Yang, Y.-Z. Song, T. Xiang, and T. Hospedales.

Sketch-a-net that beats humans. BMVC, 2015.

[38] X. Yu, T. Liu, X. Wang, and D. Tao. On compressing deep
In CVPR,

models by low rank and sparse decomposition.
pages 7370–7379, 2017.

[39] H. Zhou, J. M. Alvarez, and F. Porikli. Less is more: Towards

compact cnns. In ECCV, pages 662–677, 2016.

[40] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou. Dorefa-
net: Training low bitwidth convolutional neural networks
with low bitwidth gradients. ICLR, 2016.

[41] C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained ternary

quantization. ICLR, 2017.

7. Appendix

A. Introduction

∂E
∂α

= 0,

= 0

∂E
∂β

(9)

The supplementary material consists of the following:

Solving the above, we get the equations:

1. Proof for Theorem 2 (Optimal representation of (cid:102)W)
which provides us with the optimal values of α, β and
e to represent W.

2. Proof for Theorem 4 (Gradient derivation) which al-
lows us to perform back-propagation through the net-
work.

3. Proof for Theorem 1 (Expressibility proof)

4. Experimental details

B. Optimal representation of (cid:102)W

∂E
∂α

= 2αK − 2 · WT e = 0

∂E
∂β

= 2β(n − K) − 2 · WT (1 − e) = 0

We can get the values of α and β from the above equations.

α =

, β =

WT e
K

WT (1 − e)
(n − K)

Then substituting the values of α and β in equation 8, we
get

Theorem 2. 1. The optimal binary weight (cid:102)W which mini-
mizes the error function J is

E =

(cid:102)W = αe + β(1 − e)

where α = WT e
e∗ = argmax

K , β = WT (1−e)
( (cid:107)WT e(cid:107)2
K

n−K
+ (cid:107)WT (1−e)(cid:107)2
n−K

)

e,K

and the optimal e∗ is

Proof. The
[αα . . . βα . . . ββ] can be decomposed as:

approximated weight

vector (cid:102)W =

[αα . . . βα . . . ββ] = α·[11 . . . 01 . . . 00]+β·[00 . . . 10 . . . 11]

where without loss of generality, e ∈ {0, 1}n, eT e > 0, e ∈
{0, 1}n, (1 − e)T (1 − e) > 0 and α, β ∈ R. This is be-
cause the trivial case where e = 0 or e = 1 is covered by
substituting α = β instead and the equation is independent
of e. We have to ﬁnd the values of α and β which would be
the best approximation of this vector.
Let us deﬁne the error function J = W−(α·e+β·(1 − e)).
We have to minimize (cid:107) J (cid:107)2= E, where:

E = (W − (α · e + β · (1 − e)))T (W

− (α · e + β · (1 − e)))

E = WT W + α2 · eT e + β2(1 − e)T (1 − e)

− 2α ·WT e − 2β ·WT (1 − e) + 2αβeT (1 − e)

(cid:107) W||2 +

(cid:107) WT e (cid:107)2
K
(cid:107) WT e (cid:107)2
K

+

− 2

(cid:107) WT (1 − e) (cid:107)2
n − K
(cid:107) WT (1 − e) (cid:107)2
n − K

− 2

(10)

E =

(cid:107) W||2 − (

(cid:107) WT e (cid:107)2
K

+

(cid:107) WT (1 − e) (cid:107)2
n − K

)

(11)

In the above equation, we want to minimize E. Since W is
a given value, we need maximize the second term to mini-
mize the expression. For a given K, eK = sgn(Tk) where
Tk = topk(W, K). Here, topk(W, K) represents the top
K values of W corresponding to either the largest positive
K values or the largest negative K values, which remain as
is whereas the rest are converted to zeros.

e(cid:63) = argmax
(

e,K

(cid:107) WT e (cid:107)2
K

+

(cid:107) WT (1 − e) (cid:107)2
n − K

)

(6)

(7)

Selecting the topk(W, K) would be optimal since
||WT e|| and ||WT (1 − e)|| are both maximized on select-
ing either the largest K positive values or the largest K neg-
ative values. Hence, this allows us to select the optimal e
given a K.

With this, we obtain the optimal e.

where eT e = K, then (1 − e)T (1 − e) = n − K and
eT (1 − e) = 0. Substituting these in, we get

C. Gradient derivation

E = WT W+α2K+β2(n−K)−2α·WT e−2β·WT (1 − e)

(8)
We minimize this equation with respect to α and β giving
us:

W ≈ (cid:102)W = αe + β(1 − e)

where α =

and β =

WT e
K

WT (1 − e)
n − K

Let Tk = topk(W, K), and (cid:103)W1 = αe, and
(cid:103)W2 = β(1 − e).
Considering α, on substituting e = sgn(Tk).

Considering the second term (cid:103)W2, we have,

d(cid:103)W2
dW

= (1 − e)

+ β

dβ
dW

d(1 − e)
dW

α =

WT e
K

∴ α =

WT sgn(Tk)
K

Hence, we have α = WT sgn(Tk)
β = WT (1−sgn(Tk))
have,

and similarly
Putting these back in (cid:102)W, we

n−K

K

.

∴ (cid:102)W =

WT sgn(Tk)
K

◦ sgn(Tk)

+

WT (1 − sgn(Tk))
n − K

◦ (1 − sgn(Tk))

(12)

Now, we compute the derivatives of α and β with respect to
W,

dα
dW

dα
dW

=

=

d(WT sgn(Tk))
dW

.

1
K

d(Tk

T sgn(Tk))
dW

.

1
K

dα
dW

.

1
K

=

=

d(||Tk||l1)
dW
sgn(Tk)
K

Similarly,

dβ
dW

=

=

d(||W − Tk||l1)
dW
sgn(W − Tk)
n − K

.

1
n − K

Now, (cid:103)W1 = αe therefore,

d(cid:103)W1
dW

= e

+ α

dα
dW

de
dW

(13)

(14)

∴ d(cid:103)W1
dW

=

sgn(Tk)
K

◦ sgn(Tk) + α.ST E(Tk)

With this, we end up at the ﬁnal equation for (cid:102)G1 = d(cid:103)W1
mentioned in the paper,

dW as

∴ (cid:102)G1 =

sgn(Tk)
K

◦ sgn(Tk) +

ST E(Tk)

(15)

||Tk||l1
K

∴ d(cid:103)W2
dW

=

sgn(W − Tk)
n − K

◦(1−sgn(Tk))+β.ST E(W−Tk)

This provides us (cid:102)G2 = d(cid:103)W2

dW as mentioned in the paper,

(cid:102)G2 =

◦ (1 − sgn(Tk))

sgn(W − Tk)
n − K

+

||W − Tk||l1
n − K

(16)

.ST E(W − Tk)

Together, we arrive at our ﬁnal gradient (cid:101)G = d(cid:102)W
dW ,

(cid:101)G = (cid:102)G1 + (cid:102)G2

(17)

D. Binary Networks as Approximators

We deﬁne mk as the number of neurons required to ap-
proximate a polynomial of n terms, given the network has a
depth of k. We show that this number is bounded in terms
of n and k.

Theorem 4. For p(x) equal to the product x1x2 · · · xn, and
for any σ with all nonzero Taylor coefﬁcients, we have:

mk(p, σ) = O

(cid:16)

n(k−1)/k · 2n1/k (cid:17)

.

(18)

Proof. We construct a binary network in which groups of
the n inputs are recursively multiplied. The n inputs are
ﬁrst divided into groups of size b1, and each group is mul-
tiplied in the ﬁrst hidden layer using 2b1 binary neurons (as
described in [24]). Thus, the ﬁrst hidden layer includes a
total of 2b1n/b1 binary neurons. This gives us n/b1 values
to multiply, which are in turn divided into groups of size b2.
Each group is multiplied in the second hidden layer using
2b2 neurons. Thus, the second hidden layer includes a total
of 2b2 n/(b1b2) binary neurons.

We continue in this fashion for b1, b2, . . . , bk such that
b1b2 · · · bk = n, giving us one neuron which is the product
of all of our inputs. By considering the total number of
binary neurons used, we conclude

mk(p, σ) ≤

k
(cid:88)

i=1

n
j=1 bj

(cid:81)i

2bi =

k
(cid:88)

k
(cid:89)






 2bi .

bj

i=1

j=i+1

(19)
Setting bi = n1/k, for each i, gives us the desired bound
(18).

E. Expressibility of Binary Networks

A binary neural network (a network with weights having
only two possible values, such as +1 and −1) with a sin-
gle hidden layer of m binary-valued neurons that approxi-
mates a product gate for n inputs can be formally written as
a choice of constants aij and wj satisfying

m
(cid:88)

j=1

(cid:32) n
(cid:88)

i=1

(cid:33)

n
(cid:89)

i=1

wjσ

aijxi

≈

xi.

(20)

[24] shows that 2n neurons are sufﬁcient to approximate
a product gate with n inputs - each of these weights are as-
signed, in the proof, a value of +1 or −1 before normaliza-
tion, and all coefﬁcients aij also have +1/−1 values. This
essentially makes it a binary network. Weight normaliza-
, which
tion introduces a scaling constant of sorts,
would translate to α in our representation, with its negative
denoting β.
The above shows how binary networks are expressive
enough to approximate real-valued networks, without the
need for higher bit quantization.

1
2nn!σn

F. Experimental details

We used the Adam optimizer for all the models with a
maximum learning rate of 0.002 and a minimum learning
rate of 0.00005 with a decay factor of 2. All networks are
trained from scratch. Weights of all layers except the ﬁrst
were binarized throughout our experiments. Our FBin layer
is structured the same as the XNOR-Net. We performed our
experiments using a cluster of GeForce GTX 1080 Tis using
PyTorch v0.2.

Note: The above proofs for expressibility power have

been borrowed from [24].

Distribution-Aware Binarization of Neural Networks for Sketch Recognition

Ameya Prabhu Vishal Batchu Sri Aurobindo Munagala Rohit Gajawada Anoop Namboodiri
Center for Visual Information Technology, Kohli Center on Intelligent Systems
IIIT-Hyderabad, India
{ameya.prabhu@research., vishal.batchu@students., s.munagala@research.,
rohit.gajawada@students., anoop@}iiit.ac.in

8
1
0
2
 
r
p
A
 
9
 
 
]

V
C
.
s
c
[
 
 
1
v
1
4
9
2
0
.
4
0
8
1
:
v
i
X
r
a

Abstract

Deep neural networks are highly effective at a range
of computational tasks. However, they tend to be com-
putationally expensive, especially in vision-related prob-
lems, and also have large memory requirements. One of
the most effective methods to achieve signiﬁcant improve-
ments in computational/spatial efﬁciency is to binarize the
weights and activations in a network. However, naive bina-
rization results in accuracy drops when applied to networks
for most tasks. In this work, we present a highly general-
ized, distribution-aware approach to binarizing deep net-
works that allows us to retain the advantages of a binarized
network, while reducing accuracy drops. We also develop
efﬁcient implementations for our proposed approach across
different architectures. We present a theoretical analysis of
the technique to show the effective representational power
of the resulting layers, and explore the forms of data they
model best. Experiments on popular datasets show that our
technique offers better accuracies than naive binarization,
while retaining the same beneﬁts that binarization provides
- with respect to run-time compression, reduction of compu-
tational costs, and power consumption.

1. Introduction

Deep learning models are pushing the state-of-the-art in
various problems across domains, but are computationally
intensive to train and run, especially Convolutional Neural
Networks (CNNs) used for vision applications. They also
occupy a large amount of memory, and the amount of com-
putation required to train a network leads to high power con-
sumption as well.

There have been many developments in the area of model
compression in the last few years, with the aim of bring-
ing down network runtimes and storage requirements to
mobile-friendly levels. Compression strategies for Convo-
lutional Neural Networks included architectural improve-
ments [16, 20] and re-parametrization [27, 34] to pruning

Figure 1: Weight distribution of a layer with corresponding
α/β values, and the scaling factor α in the XNOR-Net im-
plementation for comparison. α and β in our method have
differing magnitudes, unlike in XNOR-Net.

techniques [14, 25] and quantization [19, 40]. Among these
approaches, quantization - especially, binarization - pro-
vided the most compact models as shown in Table 1.

Quantized networks - where weights/activations were
quantized into low-precision representations - were found to
achieve great model compression. Quantization has proven
to be a powerful compression strategy, especially the most
extreme form of quantization - Binarization. Binarization
has enabled the use of XNOR-Popcount operations for vec-
tor dot products, which take much less time compared to
full-precision Multiply-Accumulates (MACs), contributing
to a huge speedup in convolutional layers [28, 19] on a
general-purpose CPU. Moreover, as each binary weight re-
quires only a single bit to represent, one can achieve drastic
reductions in run-time memory requirements. Previous re-
search [28, 19] shows that it is possible to perform weight
and activation binarization on large networks with up to 58x
speedups and approximately 32x compression ratios, albeit
with signiﬁcant drops in accuracy.

Later works have tended to move away from binary rep-
resentations of weights/inputs to multi-bit representations.
The reason for this was mainly the large accuracy degrada-

Method
Finetuned SVD 2 [34]
Circulant CNN 2 [7]
Adaptive Fastfood-16 [34]
Collins et al. [8]
Zhou et al. [39]
ACDC [27]
Network Pruning [14]
Deep Compression [14]
GreBdec [38]
Srinivas et al. [30]
Guo et al. [13]
Binarization

Compression
2.6x
3.6x
3.7x
4x
4.3x
6.3x
9.1x
9.1x
10.2x
10.3x
17.9x
≈32x

Table 1: Comparison of Binarization and other methods in
terms of compression.

with interesting areas to explore, such as fast classiﬁcation
and sketch-based image retrieval.

Reproducibility: Our implementation can be found on

GitHub 1

2. Related Work

We ask the question: Do CNNs need the representa-
tional power of 32-bit ﬂoating point operations, especially
for binary-valued data such as sketches? Is it possible to
cut down memory costs and make output computations sig-
niﬁcantly less expensive? In recent years, several different
approaches were proposed to achieve network compression
and speedups, and special-purpose networks were proposed
for sketch classiﬁcation/retrieval tasks. These are summa-
rized below:

Sketch Recognition: Many deep-network based works
in the past did not lead to fruitful results before, primarily
due to these networks being better suited for images rather
than sketches. Sketches have signiﬁcantly different char-
acteristics as compared to images, and require specialized,
ﬁne-tuned networks to work with. Sketch-a-Net from Yu
et al. [37] took these factors into account, and proposed a
carefully designed network structure that suited sketch rep-
resentations. Their single-model showed tremendous incre-
ments over the then state-of-the-art, and managed to beat
the average human performance using a Bayesian Fusion
ensemble. Being a signiﬁcant achievement in this problem
- since beating human accuracy in recognition problems is
difﬁcult - this model has been adopted by a number of later
works Bui et al. [4], Yu et al. [35], Wang et al. [33].

Pruning Networks for Compression: Optimal Brain
Damage [10] and Optimal Brain Surgeon [15] introduced
a network pruning technique based on the Hessian of the

1https://github.com/erilyth/DistributionAwareBinarizedNetworks-

WACV18

Figure 2: An example sketch passing through a convolu-
tional layer ﬁlter, with the real-valued ﬁlter shown alongside
corresponding α-β and XNOR-Net ﬁlters. Orange signiﬁes
the highest response areas. We can see that DAB-Net has
signiﬁcantly better responses when compared to XNOR-
Net

tion observed in binary networks. While some works [32]
have proposed methods to recover some of the lost accu-
racy, this leads to the natural question of whether, in the-
ory, binary-representations of neural networks can be used
at all to effectively approximate a full-precision network. If
shown to be sufﬁcient, the search for an optimally accurate
binarization technique is worthwhile, due to the large gains
in speedups (due to binary operations rather than full-prec
MACs) and compression compared to multi-bit representa-
tions.

In our paper, we make the following contributions:
1. We show that binary representations are as expressive
as full precision neural networks for polynomial func-
tions, and offer theoretical insights into the same.
2. We present a generalized, distribution-aware represen-
tation for binary networks, and proceed to calculate the
generalized parameter-values for any binary network.
3. We offer an intuitive analysis and comparison of our
representation vis-a-vis previous representations, as il-
lustrated in Figure 1.

4. We provide a provably efﬁcient implementation of net-

works trained using this representation.

5. We demonstrate the effectiveness of our method by ex-
tensive experiments applying it to popular model ar-
chitectures on large-scale sketch datasets and improv-
ing upon existing binarization approaches.

We also offer intuitions about how this technique might
be effective in problems involving data that is inherently
binary, such as sketches, as shown in Figure 2. Sketches
are a universal form of communication and are easy to draw
through mobile devices - thus emerging as a new paradigm

loss function. Deep Compression [14] also used pruning to
achieve compression by an order of magnitude in various
standard neural networks. It further reduced non-runtime
memory by employing trained quantization and Huffman
coding. Network Slimming [25] introduced a new learn-
ing scheme for CNNs that leverages channel-level sparsity
in networks, and showed compression and speedup with-
out accuracy degradation, with decreased run-time memory
footprint as well. We train our binary models from scratch,
as opposed to using pre-trained networks as in the above
approaches.

Higher Bit Quantization: HashedNets [6] hashed net-
work weights to bin them. Zhou et al. [2] quantized net-
works to 4-bit weights, achieving 8x memory compres-
sion by using 4 bits to represent 16 different values and
1 bit to represent zeros. Trained Ternary Quantization
[41] uses 2-bit weights and scaling factors to bring down
model size to 16x compression, with little accuracy degra-
dation. Quantized Neural Networks[19] use low-precision
quantized weights and inputs and replaces arithmetic op-
erations with bit-wise ones, reducing power consumption.
DoReFa-Net [40] used low bit-width gradients during back-
propagation, and obtained train-time speedups. Ternary
Weight Networks [22] optimize a threshold-based ternary
function for approximation, with stronger expressive abili-
ties than binary networks. The above works cannot leverage
the speedups gained by XNOR/Pop-count operations which
could be performed on dedicated hardware, unlike in our
work. This is our primary motivation for attempting to im-
prove binary algorithms.

Binarization: We provide an optimal method for cal-
culating binary weights, and we show that all of the above
binarization techniques were special cases of our method,
with less accurate approximations. Previous binarization
papers performed binarization independent of the distribu-
tion weights, for example [28]. The method we introduce is
distribution-aware, i.e. looks at the distribution of weights
to calculate an optimal binarization.

BinaryConnect [9] was one of the ﬁrst works to use bi-
nary (+1, -1) values for network parameters, achieving sig-
niﬁcant compression. XNOR-Nets [28] followed the work
of BNNs [18], binarizing both layer weights and inputs and
multiplying them with scaling constants - bringing signiﬁ-
cant speedups by using faster XNOR-Popcount operations
to calculate convolutional outputs. Recent research pro-
posed a variety of additional methods - including novel ac-
tivation functions [5], alternative layers [32], approxima-
tion algorithms [17], ﬁxed point bit-width allocations [23].
Merolla et al. [?] and Anderson et al. [1] offer a few the-
oretical insights and analysis into binary networks. Further
works have extended this in various directions, including
using local binary patterns [21] and lookup-based compres-
sion methods [3].

3. Representational Power of Binary Networks

Many recent works in network compression involve
higher bit weight quantization using two or more bits
[2, 41, 22] instead of binarization, arguing that binary repre-
sentations would not be able to approximate full-precision
networks. In light of this, we explore whether the repre-
sentational power that binary networks can offer is theo-
retically sufﬁcient to get similar representational power as
full-precision networks.

Rolnick et al. [24, 29] have done extensive work in char-
acterizing the expressiveness of neural networks. They
claim that due to the nature of functions - that they de-
pend on real-world physics, in addition to mathematics -
the seemingly huge set of possible functions could be ap-
proximated by deep learning models. From the Universal
Approximation Theorem [11], it is seen that any arbitrary
function can be well-approximated by an Artiﬁcial Neural
Network; but cheap learning, or models with far fewer pa-
rameters than generic ones, are often sufﬁcient to approx-
imate multivariate monomials - which are a class of func-
tions with practical interest, occurring in most real-world
problems.

We can deﬁne a binary neural network having k lay-
ers with activation function σ(x) and consider how many
neurons are required to compute a multivariate monomial
p(x) of degree d. The network takes an n dimensional in-
put x, producing a one dimensional output p(x). We deﬁne
Bk(p, σ) to be the minimum number of binary neurons (ex-
cluding input and output) required to approximate p, where
the error of approximation is of degree at least d + 1 in the
input variables. For instance, B1(p, σ) is the minimal inte-
ger m such that:

m
(cid:88)

j=1

wjσ

(cid:32) n
(cid:88)

i=1

(cid:33)

aijxi

= p(x) + O(xd+1

1 + . . . + xd+1

n ).

Any polynomial can be approximated to high precision
as long as input variables are small enough [24]. Let
B(p, σ) = mink≥0 Bk(p, σ).

Theorem 1. For p(x) equal to the product x1x2 · · · xn, and
for any σ with all nonzero Taylor coefﬁcients, we have one
construction of a binary neural network which meets the
condition

Bk(p, σ) = O

(cid:16)

n(k−1)/k · 2n1/k (cid:17)

.

(1)

Proof of the above can be found in the supplementary

material.

Conjecture III.2. of Rolnick et al. [29] says that this
bound is approximately optimal. If this conjecture proves
to be true, weight-binarized networks would have the same
representational power as full-precision networks, since the

network that was essentially used to prove that the above
theorem - that a network exists that can satisfy that bound -
was a binary network.

The above theorem shows that any neural network that
can be represented as a multivariate polynomial function
is considered as a simpliﬁed model with ELU-like activa-
tions, using continuously differentiable layers - so pool-
ing layers are excluded as well. While there can exist a
deep binary-weight network that can possibly approximate
polynomials similar to full precision networks, it does say
that such a representation would be efﬁciently obtainable
through Stochastic Gradient Descent. Also, this theorem
assumes only weights are binarized, not the activations. Ac-
tivation binarization typically loses a lot of information and
might not be a good thing to do frequently. However, this
insight motivates the fact that more investigation is needed
into approximating networks through binary network struc-
tures.

4. Distribution-Aware Binarization

We have so far established that binary representations are
possibly sufﬁcient to approximate a polynomial with simi-
lar numbers of neurons as a full-precision neural network.
We now investigate the question - What is the most gen-
eral form of binary representation possible? In this section,
we derive a generalized distribution-aware formulation of
binary weights, and provide an efﬁcient implementation of
the same. We consider models binarized with our approach
as DAB-Nets (Distribution Aware Binarized Networks).

We model the loss function layer-wise for the network.
We assume that inputs to the convolutional layers are bi-
nary - i.e. belong to {+1, −1}, and ﬁnd constants α and
β (elaborated below) as a general binary form for layer
weights. These constants are calculated from the distribu-
tion of real-valued weights in a layer - thus making our ap-
proach distribution-aware.

4.1. Derivation

Without loss of generality, we assume that W is a vector
in Rn , where n = c · w · h. We attempt to binarize the
weight vector W to (cid:102)W which takes a form similar to this
example - [αα...βαβ]. Simply put, (cid:102)W is a vector consisting
of scalars α and β, the two values forming the binary vector.
We represent this as (cid:102)W = αe + β(1 − e) where e is a
vector such that e ∈ {0, 1}n (cid:51) e (cid:54)= 0 and e (cid:54)= 1. We
deﬁne K as eT e which represents the number of ones in
the e vector. Our objective is to ﬁnd the best possible binary
approximation for W. We set up the optimization problem
as:

(cid:102)W∗ = argmin

|| W − (cid:102)W ||2

(cid:102)W

We formally state this as the following:

The optimal binary weight vector (cid:102)W∗ for any weight
vector W which minimizes the approximate-error function
J =|| W − (cid:102)W ||2 can be represented as:

(cid:102)W∗ = αe + β(1 − e) where

α =

, β =

WT e
K

WT (1 − e)
n − K

for a given K. That is, given a K, the optimal selection of
e would correspond to either the K smallest weights of W
or the K largest weights of W.

The best suited K, we calculate the value of the follow-
ing expression for every value of K, giving us an e, and
maximize the expression:

e∗ = argmax
(

e

|| WT e ||2
K

+

|| WT (1 − e) ||2
n − K

)

A detailed proof of the above can be found in the supple-
mentary material.

The above representation shows the values obtained
for e, α and β are the optimal approximate representations
of the weight vector W. The vector e, which controls the
number and distribution of occurrences of α and β, acts as
a mask of the top/bottom K values of W. We assign α to
capture the greater of the two values in magnitude. Note
that the scaling values derived in the XNOR formulation,
α and −α, are a special case of the above, and hence our
approximation error is at most that of the XNOR error. We
explore what this function represents and how this relates
to previous binarization techniques in the next subsection.

4.2. Intuitions about DAB-Net

In this section, we investigate intuitions about the de-
rived representation. We can visualize that e and (1 − e)
are orthogonal vectors. Hence, if normalized, e and (1 − e)
form a basis for a subspace R2. Theorem 2 says the best α
and β can be found by essentially projecting the weight ma-
trix W into this subspace, ﬁnding the vector in the subspace
which is closest to e and (1 − e) respectively.

α =

(cid:104)W, e(cid:105)
(cid:104)e, e(cid:105)

· e , β =

(cid:104)W, (1 − e)(cid:105)
(cid:104)(1 − e), (1 − e)(cid:105)

· (1 − e)

We also show that our derived representation is different
from the previous binary representations since we cannot
derive them by assuming a special case of our formulation.
XNOR-Net [28] or BNN [18]-like representations cannot
be obtained from our formulation. However, in practice, we
are able to simulate XNOR-Net by constraining W to be
mean-centered and K = n
2 , since roughly half the weights
are above 0, the other half below, as seen in Figure 5 in
Section 5.3.2.

Algorithm 1 Finding an optimal K value.

// Empty array of same size as W

1: Initialization
2: W = 1D weight vector
3: T = Sum of all the elements of W
4: Sort(W)
5: D = [00...0]
6: optK1 = 0 // Optimal value for K
7: maxD1 = 0 // Value of D for optimal K value
8:
9: for I= 1 to D.size do
Pi = Pi−1 + Wi
10:
Di = P 2
i + (T −Pi)2
if Di ≥ maxD1 then
maxD1 = Di
optK1 = i

11:
12:
13:
14:

n−i

i

15:
16: Sort(W, reverse=true) and Repeat steps 4-13 with

optK2 and maxD2

17:
18: optKf inal = optK1
19: if maxD2 > maxD1 then
20:

optKf inal = optK2

21:
22: return optKf inal

4.3. Implementation

The representation that we earlier derived requires
to be efﬁciently computable, in order to ensure that our
algorithm runs fast enough to be able to train binary
networks.
In this section, we investigate the implemen-
tation, by breaking it into two parts: 1) Computing the
parameter K efﬁciently for every iteration. 2) Training
the entire network using that value of K for a given
iteration. We show that it is possible to get an efﬁciently
trainable network at minimal extra cost. We provide an
efﬁcient algorithm using Dynamic Programming which
computes the optimal value for K quickly at every iteration.

4.3.1 Parallel Preﬁx-Sums to Obtain K

Theorem 2. The optimal K ∗ which minimizes the value e
can be computed in O(n · logn) complexity.

Considering one weight ﬁlter at a time for each convo-
lution layer, we ﬂatten the weights into a 1-dimensional
weight vector W. We then sort the vector in ascending or-
der and then compute the preﬁx-sum array P of W. For
a selected value of K, the term to be maximized would be
i + (T −Pi)2
( ||WT e||2
)
since the top K values in W sum up to Pi where T is the

K + ||WT (1−e)||2

), which is equal to ( P 2

n−K

n−i

i

sum of all weights in W. We also perform the same compu-
tation with a descending order of W’s weights since K can
correspond to either the smallest K weights or the largest K
weights as we mentioned earlier. In order to speed this up,
we perform these operations on all the weight ﬁlters at the
same time considering them as a 2D weight vector instead.
Our algorithm runs in O(n · logn) time complexity, and is
speciﬁed in Algorithm 1. This algorithm is integrated into
our code, and will be provided alongside.

4.3.2 Forward and Backward Pass

Now that we know how to calculate K, e, α, and β for each
ﬁlter in each layer optimally, we can compute (cid:102)W which
approximates W well. Here, topk(W, K) represents the
top K values of W which remain as is whereas the rest are
converted to zeros. Let Tk = topk(W, K).

Corollary 1 (Weight Binarization). The optimal binary
weight (cid:102)W can be represented as,

(cid:102)W = α.sgn(Tk) + β.(1 − sgn(Tk))

where,

α =

and β =

Tk
K

(W − Tk)
n − K

Once we have (cid:102)W, we can perform convolution as I (cid:126)
(cid:102)W during the forward pass of the network. Similarly, the
optimal gradient (cid:101)G can be computed as follows, which is
back-propagated throughout the network in order to update
the weights:

Theorem 3 (Backward Pass). The optimal gradient value
(cid:101)G can be represented as,

(cid:101)G = (cid:102)G1 + (cid:102)G2

(2)

where,

(cid:102)G1 =

sgn(Tk)
K

◦ sgn(Tk) +

.ST E(Tk)

(3)

||Tk||l1
K

(cid:102)G2 =

◦ (1 − sgn(Tk))

.ST E(W − Tk)

sgn(W − Tk)
n − K

+

||W − Tk||l1
n − K
(cid:40)

ST E(Tk)i =

i, where |W|i<= 1

Tk
0, elsewhere

(4)

(5)

The gradient vector, as seen above, can be intuitively
understood if seen as the sum of two independent gradi-
ents (cid:102)G1 and (cid:102)G2, each corresponding to the vectors e and
(1 − e) respectively. Further details regarding the deriva-
tion of this gradient would be provided in the supplemen-
tary material.

Algorithm 2 Training an L-layers CNN with binary
weights:

1: A minibatch of inputs and targets (I, Y), cost function
C(Y, ˆY), current weight Wt and current learning rate
ηt.

2: updated weight Wt+1 and updated learning rate ηt+1.
3: Binarizing weight ﬁlters:
4: Wt = MeanCenter(Wt)
5: Wt = Clamp(Wt, -1, 1)
6: Wreal = Wt
7: for l = 1 to L do
8:
9:

for jth ﬁlter in lth layer do

10:

11:

12:
13:

Find Klj using Algorithm 1
αlj = topk(Wlj ,Klj )
Klj
βlj = − (Wlj −topk(Wlj ,Klj ))
(cid:102)Wlj = α.sgn(topk(Wlj, Klj))

n−Klj

+ β.(1 − sgn(topk(Wlj, Klj)))

14:
15: ˆY = BinaryForward(I, (cid:102)W)
16:
17: ∂C
= BinaryBackward( ∂C
// Standard back-
ˆY
∂ (cid:102)W
ward propagation except that gradients are computed
using (cid:102)W instead of Wt as mentioned in Theorem. 3

, (cid:102)W)

18:
19: We then copy back the real weights in order to apply

the gradients computed. Wt = Wreal

20:
21: Wt+1 = UpdateParameters(Wt, ∂C
∂ (cid:102)W
22: ηt+1 = UpdateLearningrate(ηt, t)

, ηt)

4.4. Training Procedure

Putting all the components mentioned above together,
we have outlined our training procedure in Algorithm 2.
During the forward pass of the network, we ﬁrst mean cen-
ter and clamp the current weights of the network. We then
store a copy of these weights as Wreal. We compute the bi-
nary forward pass of the network, and then apply the back-
ward pass using the weights (cid:102)W, computing gradients for
each of the weights. We then apply these gradients on the
original set of weights Wt in order to obtain Wt+1.
In
essence, binarized weights are used to compute the gra-
dients, but they are applied to the original stored weights
to perform the update. This requires us to store the full
precision weights during training, but once the network is
trained, we store only the binarized weights for inference.

5. Experiments

We empirically demonstrate the effectiveness of our op-
timal distribution-aware binarization algorithm (DAB-Net)

on the TU-Berlin and Sketchy datasets. We compare DAB-
Net with BNN and XNOR-Net [28] on various architec-
tures, on two popular large-scale sketch recognition datasets
as sketches are sparse and binary. Also, they are easier to
train with than standard images, for which we believe the
algorithm needs to be stabilized - in essence, the K value
must be restricted to change by only slight amounts. We
show that our approach is superior to existing binarization
algorithms, and can generalize to different kinds of CNN
architectures on sketches.

5.1. Experimental Setup

In our experiments, we deﬁne the network having only
the convolutional layer weights binarized as WBin, the net-
work having both inputs and weights binarized as FBin
and the original full-precision network as FPrec. Binary
Networks have achieved accuracies comparable to full-
precision networks on limited domain/simpliﬁed datasets
like CIFAR-10, MNIST, SVHN, but show considerable
losses on larger datasets. Binary networks are well suited
for sketch data due to its binary and sparse nature of the
data.

TU-Berlin: The TU-Berlin [12] dataset is the most
popular
large-scale free-hand sketch dataset contain-
ing sketches of 250 categories, with a human sketch-
recognition accuracy of 73.1% on an average.

Sketchy: A recent large-scale free-hand sketch dataset
containing 75,471 hand-drawn sketches spanning 125 cat-
egories. This dataset was primarily used to cross-validate
results obtained on the TU-Berlin dataset, to ensure the ro-
bustness of our approach with respect to the method of data
collection.

For all the datasets, we ﬁrst resized the input images to
256 x 256. A 224 x 224 (225 x 225 for Sketch-A-Net) sized
crop was then randomly taken from an image with standard
augmentations such as rotation and horizontal ﬂipping, for
TU-Berlin and Sketchy. In the TU-Berlin dataset, we use
three-fold cross validation which gives us a 2:1 train-test
split ensuring that our results are comparable with all pre-
vious methods. For Sketchy, we use the training images
for retrieval as the training images for classiﬁcation, and
validation images for retrieval as the validation images for
classiﬁcation. We report ten-crop accuracies on both the
datasets.

We used the PyTorch framework to train our net-
works. We used the Sketch-A-Net[37], ResNet-18[16] and
GoogleNet[31] architectures. Weights of all layers except
the ﬁrst were binarized throughout our experiments, ex-
cept in Sketch-A-Net for which all layers except ﬁrst and
last layers were binarized. All networks were trained from
scratch. We used the Adam optimizer for all experiments.
Note that we do not use a bias term or weight decay for bi-
narized Conv layers. We used a batch size of 256 for all

Models

Method

Accuracies

Improvement XNOR-Net vs DAB-Net

Sketch-A-Net

ResNet-18

GoogleNet

FPrec
WBin (BWN)
FBin (XNOR-Net)
WBin DAB-Net
FBin DAB-Net

FPrec
WBin (BWN)
FBin (XNOR-Net)
WBin DAB-Net
FBin DAB-Net

FPrec
WBin (BWN)
FBin (XNOR-Net)
WBin DAB-Net
FBin DAB-Net

TU-Berlin
72.9%
73.0%
59.6%
72.4%
60.4%
+0.8%
74.1%
73.4%
68.8%
73.5%
71.3%
+2.5%
75.0%
74.8%
72.2%
75.7%
73.7%
+1.5%

Sketchy
85.9%
85.6%
68.6%
84.0%
70.6%
+2.0%
88.7%
89.3%
82.8%
88.8%
84.2%
+1.4%
90.0%
89.8%
86.8%
90.1%
87.4%
+0.6%

Improvement XNOR-Net vs DAB-Net

Models
AlexNet-SVM
AlexNet-Sketch
Sketch-A-Net SC
Humans
Sketch-A-Net-22[36]
Sketch-A-Net WBin DAB-Net
ResNet-18 WBin DAB-Net
GoogleNet WBin DAB-Net
Sketch-A-Net FBin DAB-Net
ResNet-18 FBin DAB-Net
GoogleNet FBin DAB-Net

Accuracy
67.1%
68.6%
72.2%
73.1%
77.0%
72.4%
73.5%
75.7%
60.4%
71.3%
73.7%

Table 3: A comparison between state-of-the-art single
model accuracies of recognition systems on the TU-Berlin
dataset.

Improvement XNOR-Net vs DAB-Net

5.3. XNOR-Net vs DAB-Net

Table 2: Our DAB-Net models compared to FBin, WBin
and FPrec models on TU-Berlin and Sketchy in terms of
accuracy.

Sketch-A-Net models and a batch size of 128 for ResNet-
18 and GoogleNet models, the maximum size that ﬁts in a
1080Ti GPU. Additional experimental details are available
in the supplementary material.

We measure how K, α, and β vary across various lay-
ers over time during training, and these variations are ob-
served to be quite different from their corresponding val-
ues in XNOR-Net. These observations show that bina-
rization can approximate a network much better when it is
distribution-aware (like in our technique) versus when it is
distribution-agnostic (like XNOR-Nets).

5.2. Results

We compare the accuracies of our distribution aware bi-
narization algorithm for WBin and FBin models on the TU-
Berlin and Sketchy datasets. Note that higher accuracies
are an improvement, hence stated in green in Table 2. On
the TU-Berlin and Sketchy datasets in Table 2, we observe
that FBin DAB-Net models consistently perform better over
their XNOR-Net counterparts. They improve upon XNOR-
Net accuracies by 0.8%, 2.5%, and 1.5% in Sketch-A-Net,
ResNet-18, and GoogleNet respectively on the TU-Berlin
dataset. Similarly, they improve by 2.0%, 1.4%, and 0.6%
respectively on the Sketchy dataset. We also compare them
with state-of-the-art sketch classiﬁcation models in Table 3.
We ﬁnd that our compressed models perform signiﬁcantly
better than the original sketch models and offer compres-
sion, runtime and energy savings additionally.

Our DAB-Net WBin models attain accuracies similar to
BWN WBin models and do not offer major improvements
mainly because WBin models achieve FPrec accuracies al-
ready, hence do not have much scope for improvement un-
like FBin models. Thus, we conclude that our DAB-Net
FBin models are able to attain signiﬁcant accuracy improve-
ments over their XNOR-Net counterparts when everything
apart from the binarization method is kept constant.

2It is the sketch-a-net SC model trained with additional imagenet
data, additional data augmentation strategies and considering an ensem-
ble, hence would not be a direct comparison

5.3.1 Variation of α and β across Time

We plot the distribution of weights of a randomly selected
ﬁlter belonging to a layer and observe that α and β of DAB-
Net start out to be similar to α and −α of XNOR-Nets,
since the distributions are randomly initialized. However, as
training progresses, we observe as we go from Subﬁgure (1)
to (4) in Figure 3, the distribution eventually becomes non-
symmetric and complex, hence our values signiﬁcantly di-
verge from their XNOR-Net counterparts. This divergence
signiﬁes a better approximation of the underlying distribu-
tion of weights in our method, giving additional evidence
to our claim that the proposed DAB-Net technique gives a
better representation of layer weights, signiﬁcantly different
from that of XNOR-Nets.

5.3.2 Variation of K across Time and Layers

We deﬁne normalized K as the K
n for a layer ﬁlter. For
XNOR-Nets, K would be the number of values below zero
in a given weight ﬁlter - which has minimal variation, and
does not take into consideration the distribution of weights
in the ﬁlter - as K in this case is simply the number of
weights below a certain ﬁxed global threshold, zero. How-
ever, we observe that the K computed in DAB-Net varies
signiﬁcantly across epochs initially, but slowly converges
to an optimal value for the speciﬁc layer as shown in Figure
4.

(1)

(3)

(2)

(4)

Figure 3: Sub-ﬁgures (1) to (4) show the train-time variation of α and β for a layer ﬁlter. Initially, α and β have nearly
equal magnitudes, similar to the XNOR-Net formulation, but as we progress to (4), we see that α and β have widely different
magnitudes.Having just one scaling constant (XNOR-Net) would be a comparatively poor approximator.

Figure 4: The variation of the normalized K-value
over time during training.
It falls initially but con-
verges eventually to 0.35. The normalized K-value
for XNOR-Net remains almost at 0.5 till the end.

Figure 5: The variation of normalized K values on random
ﬁlters across layers. The K-value corresponding to DAB-
Net varies across layers based on the distribution of weights
of the speciﬁc layer, which is not captured by XNOR-Net.

We also plot the variation of normalized K values for a
few randomly chosen ﬁlters indexes across layers and ob-
serve that it varies across layers, trying to match the distri-
bution of weights at each layer. Each ﬁlter has its own set
of weights, accounting for the differences in variation of K
in each case, as shown in Figure 5.

6. Conclusion

We have proposed an optimal binary representation for
network layer-weights that takes into account the distri-
bution of weights, unlike previous distribution-agnostic

approaches. We showed how this representation could
be computed efﬁciently in n.logn time using dynamic
programming,
thus enabling efﬁcient training on larger
datasets. We applied our technique on various datasets and
noted signiﬁcant accuracy improvements over other full-
binarization approaches. We believe that this work provides
a new perspective on network binarization, and that future
work can gain signiﬁcantly from distribution-aware explo-
rations.

References

[1] A. G. Anderson and C. P. Berg. The high-dimensional
arXiv preprint

geometry of binary neural networks.
arXiv:1705.07199, 2017.

[2] Z. Aojun, Y. Anbang, G. Yiwen, X. Lin, and C. Yurong. In-
cremental network quantization: Towards lossless cnns with
low-precision weights. ICLR, 2017.

[3] H. Bagherinezhad, M. Rastegari, and A. Farhadi. Lcnn:

Lookup-based convolutional neural network. CVPR, 2017.

[4] T. Bui, L. Ribeiro, M. Ponti, and J. P. Collomosse. Generali-
sation and sharing in triplet convnets for sketch based visual
search. CoRR, abs/1611.05301, 2016.

[5] Z. Cai, X. He, J. Sun, and N. Vasconcelos. Deep learn-
ing with low precision by half-wave gaussian quantization.
CVPR, 2017.

[6] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and
Y. Chen. Compressing neural networks with the hashing
trick. ICML, 2015.

[7] Y. Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary,
and S.-F. Chang. An exploration of parameter redundancy
in deep networks with circulant projections. In CVPR, pages
2857–2865, 2015.

[8] M. D. Collins and P. Kohli. Memory bounded deep convolu-
tional networks. arXiv preprint arXiv:1412.1442, 2014.
[9] M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect:
Training deep neural networks with binary weights during
propagations. In NIPS, pages 3123–3131, 2015.

[10] Y. L. Cunn, J. S. Denker, and S. A. Solla. Nips. chapter

Optimal Brain Damage. 1990.

[11] G. Cybenko. Approximation by superpositions of a sig-

moidal function. (MCSS), 2(4):303–314, 1989.

[12] M. Eitz, J. Hays, and M. Alexa. How do humans sketch ob-
jects? ACM Trans. Graph. (Proc. SIGGRAPH), 31(4):44:1–
44:10, 2012.

[13] Y. Guo, A. Yao, and Y. Chen. Dynamic network surgery for

efﬁcient dnns. In NIPS, pages 1379–1387, 2016.

[14] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. ICLR, 2016.

[15] B. Hassibi, D. G. Stork, G. Wolff, and T. Watanabe. Op-
timal brain surgeon: Extensions and performance compar-
isons. NIPS, 1993.

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, pages 770–778, 2016.
[17] L. Hou, Q. Yao, and J. T. Kwok. Loss-aware binarization of

deep networks. ICLR, 2017.

[18] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and

Y. Bengio. Binarized neural networks. 2016.

[19] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Y. Bengio. Quantized neural networks: Training neural net-
works with low precision weights and activations. arXiv
preprint arXiv:1609.07061, 2016.

[20] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy
with 50x fewer parameters and¡ 0.5 mb model size. ICLR,
2017.

[21] F. Juefei-Xu, V. N. Boddeti, and M. Savvides. Local binary

convolutional neural networks. CVPR, 2017.

[22] F. Li, B. Zhang, and B. Liu. Ternary weight networks. arXiv

preprint arXiv:1605.04711, 2016.

[23] D. Lin, S. Talathi, and S. Annapureddy. Fixed point quantiza-
tion of deep convolutional networks. In Proceedings of The
33rd International Conference on Machine Learning, 2016.
[24] H. W. Lin, M. Tegmark, and D. Rolnick. Why does deep and
cheap learning work so well? Journal of Statistical Physics,
168(6):1223–1247, 2017.

[25] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang.
Learning efﬁcient convolutional networks through network
slimming. ICCV, 2017.

[26] P. Merolla, R. Appuswamy, J. V. Arthur, S. K. Esser, and
D. S. Modha. Deep neural networks are robust to weight
binarization and other non-linear distortions. CoRR, 2016.

[27] M. Moczulski, M. Denil, J. Appleyard, and N. de Freitas.
Acdc: A structured efﬁcient linear layer. ICLR, 2016.
[28] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. In ECCV, pages 525–542, 2016.

[29] D. Rolnick and M. Tegmark. The power of deeper net-
arXiv preprint

works for expressing natural functions.
arXiv:1705.05502, 2017.

[30] S. Srinivas, A. Subramanya, and R. V. Babu. Training sparse

neural networks. In CVPRW, pages 455–462, 2017.

[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015.

[32] W. Tang, G. Hua, and L. Wang. How to train a compact
binary neural network with high accuracy? In AAAI, pages
2625–2631, 2017.

[33] X. Wang, X. Duan, and X. Bai. Deep sketch feature for cross-

domain image retrieval. Neurocomputing, 2016.

[34] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. Smola,
L. Song, and Z. Wang. Deep fried convnets. In ICCV, pages
1476–1483, 2015.

[35] Q. Yu, F. Liu, Y.-Z. Song, T. Xiang, T. M. Hospedales, and

C.-C. Loy. Sketch me that shoe. In CVPR, 2016.

[36] Q. Yu, Y. Yang, F. Liu, Y.-Z. Song, T. Xiang, and T. M.
Sketch-a-net: A deep neural network that
International Journal of Computer Vision,

Hospedales.
beats humans.
122(3):411–425, 2017.

[37] Q. Yu, Y. Yang, Y.-Z. Song, T. Xiang, and T. Hospedales.

Sketch-a-net that beats humans. BMVC, 2015.

[38] X. Yu, T. Liu, X. Wang, and D. Tao. On compressing deep
In CVPR,

models by low rank and sparse decomposition.
pages 7370–7379, 2017.

[39] H. Zhou, J. M. Alvarez, and F. Porikli. Less is more: Towards

compact cnns. In ECCV, pages 662–677, 2016.

[40] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou. Dorefa-
net: Training low bitwidth convolutional neural networks
with low bitwidth gradients. ICLR, 2016.

[41] C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained ternary

quantization. ICLR, 2017.

7. Appendix

A. Introduction

∂E
∂α

= 0,

= 0

∂E
∂β

(9)

The supplementary material consists of the following:

Solving the above, we get the equations:

1. Proof for Theorem 2 (Optimal representation of (cid:102)W)
which provides us with the optimal values of α, β and
e to represent W.

2. Proof for Theorem 4 (Gradient derivation) which al-
lows us to perform back-propagation through the net-
work.

3. Proof for Theorem 1 (Expressibility proof)

4. Experimental details

B. Optimal representation of (cid:102)W

∂E
∂α

= 2αK − 2 · WT e = 0

∂E
∂β

= 2β(n − K) − 2 · WT (1 − e) = 0

We can get the values of α and β from the above equations.

α =

, β =

WT e
K

WT (1 − e)
(n − K)

Then substituting the values of α and β in equation 8, we
get

Theorem 2. 1. The optimal binary weight (cid:102)W which mini-
mizes the error function J is

E =

(cid:102)W = αe + β(1 − e)

where α = WT e
e∗ = argmax

K , β = WT (1−e)
( (cid:107)WT e(cid:107)2
K

n−K
+ (cid:107)WT (1−e)(cid:107)2
n−K

)

e,K

and the optimal e∗ is

Proof. The
[αα . . . βα . . . ββ] can be decomposed as:

approximated weight

vector (cid:102)W =

[αα . . . βα . . . ββ] = α·[11 . . . 01 . . . 00]+β·[00 . . . 10 . . . 11]

where without loss of generality, e ∈ {0, 1}n, eT e > 0, e ∈
{0, 1}n, (1 − e)T (1 − e) > 0 and α, β ∈ R. This is be-
cause the trivial case where e = 0 or e = 1 is covered by
substituting α = β instead and the equation is independent
of e. We have to ﬁnd the values of α and β which would be
the best approximation of this vector.
Let us deﬁne the error function J = W−(α·e+β·(1 − e)).
We have to minimize (cid:107) J (cid:107)2= E, where:

E = (W − (α · e + β · (1 − e)))T (W

− (α · e + β · (1 − e)))

E = WT W + α2 · eT e + β2(1 − e)T (1 − e)

− 2α ·WT e − 2β ·WT (1 − e) + 2αβeT (1 − e)

(cid:107) W||2 +

(cid:107) WT e (cid:107)2
K
(cid:107) WT e (cid:107)2
K

+

− 2

(cid:107) WT (1 − e) (cid:107)2
n − K
(cid:107) WT (1 − e) (cid:107)2
n − K

− 2

(10)

E =

(cid:107) W||2 − (

(cid:107) WT e (cid:107)2
K

+

(cid:107) WT (1 − e) (cid:107)2
n − K

)

(11)

In the above equation, we want to minimize E. Since W is
a given value, we need maximize the second term to mini-
mize the expression. For a given K, eK = sgn(Tk) where
Tk = topk(W, K). Here, topk(W, K) represents the top
K values of W corresponding to either the largest positive
K values or the largest negative K values, which remain as
is whereas the rest are converted to zeros.

e(cid:63) = argmax
(

e,K

(cid:107) WT e (cid:107)2
K

+

(cid:107) WT (1 − e) (cid:107)2
n − K

)

(6)

(7)

Selecting the topk(W, K) would be optimal since
||WT e|| and ||WT (1 − e)|| are both maximized on select-
ing either the largest K positive values or the largest K neg-
ative values. Hence, this allows us to select the optimal e
given a K.

With this, we obtain the optimal e.

where eT e = K, then (1 − e)T (1 − e) = n − K and
eT (1 − e) = 0. Substituting these in, we get

C. Gradient derivation

E = WT W+α2K+β2(n−K)−2α·WT e−2β·WT (1 − e)

(8)
We minimize this equation with respect to α and β giving
us:

W ≈ (cid:102)W = αe + β(1 − e)

where α =

and β =

WT e
K

WT (1 − e)
n − K

Let Tk = topk(W, K), and (cid:103)W1 = αe, and
(cid:103)W2 = β(1 − e).
Considering α, on substituting e = sgn(Tk).

Considering the second term (cid:103)W2, we have,

d(cid:103)W2
dW

= (1 − e)

+ β

dβ
dW

d(1 − e)
dW

α =

WT e
K

∴ α =

WT sgn(Tk)
K

Hence, we have α = WT sgn(Tk)
β = WT (1−sgn(Tk))
have,

and similarly
Putting these back in (cid:102)W, we

n−K

K

.

∴ (cid:102)W =

WT sgn(Tk)
K

◦ sgn(Tk)

+

WT (1 − sgn(Tk))
n − K

◦ (1 − sgn(Tk))

(12)

Now, we compute the derivatives of α and β with respect to
W,

dα
dW

dα
dW

=

=

d(WT sgn(Tk))
dW

.

1
K

d(Tk

T sgn(Tk))
dW

.

1
K

dα
dW

.

1
K

=

=

d(||Tk||l1)
dW
sgn(Tk)
K

Similarly,

dβ
dW

=

=

d(||W − Tk||l1)
dW
sgn(W − Tk)
n − K

.

1
n − K

Now, (cid:103)W1 = αe therefore,

d(cid:103)W1
dW

= e

+ α

dα
dW

de
dW

(13)

(14)

∴ d(cid:103)W1
dW

=

sgn(Tk)
K

◦ sgn(Tk) + α.ST E(Tk)

With this, we end up at the ﬁnal equation for (cid:102)G1 = d(cid:103)W1
mentioned in the paper,

dW as

∴ (cid:102)G1 =

sgn(Tk)
K

◦ sgn(Tk) +

ST E(Tk)

(15)

||Tk||l1
K

∴ d(cid:103)W2
dW

=

sgn(W − Tk)
n − K

◦(1−sgn(Tk))+β.ST E(W−Tk)

This provides us (cid:102)G2 = d(cid:103)W2

dW as mentioned in the paper,

(cid:102)G2 =

◦ (1 − sgn(Tk))

sgn(W − Tk)
n − K

+

||W − Tk||l1
n − K

(16)

.ST E(W − Tk)

Together, we arrive at our ﬁnal gradient (cid:101)G = d(cid:102)W
dW ,

(cid:101)G = (cid:102)G1 + (cid:102)G2

(17)

D. Binary Networks as Approximators

We deﬁne mk as the number of neurons required to ap-
proximate a polynomial of n terms, given the network has a
depth of k. We show that this number is bounded in terms
of n and k.

Theorem 4. For p(x) equal to the product x1x2 · · · xn, and
for any σ with all nonzero Taylor coefﬁcients, we have:

mk(p, σ) = O

(cid:16)

n(k−1)/k · 2n1/k (cid:17)

.

(18)

Proof. We construct a binary network in which groups of
the n inputs are recursively multiplied. The n inputs are
ﬁrst divided into groups of size b1, and each group is mul-
tiplied in the ﬁrst hidden layer using 2b1 binary neurons (as
described in [24]). Thus, the ﬁrst hidden layer includes a
total of 2b1n/b1 binary neurons. This gives us n/b1 values
to multiply, which are in turn divided into groups of size b2.
Each group is multiplied in the second hidden layer using
2b2 neurons. Thus, the second hidden layer includes a total
of 2b2 n/(b1b2) binary neurons.

We continue in this fashion for b1, b2, . . . , bk such that
b1b2 · · · bk = n, giving us one neuron which is the product
of all of our inputs. By considering the total number of
binary neurons used, we conclude

mk(p, σ) ≤

k
(cid:88)

i=1

n
j=1 bj

(cid:81)i

2bi =

k
(cid:88)

k
(cid:89)






 2bi .

bj

i=1

j=i+1

(19)
Setting bi = n1/k, for each i, gives us the desired bound
(18).

E. Expressibility of Binary Networks

A binary neural network (a network with weights having
only two possible values, such as +1 and −1) with a sin-
gle hidden layer of m binary-valued neurons that approxi-
mates a product gate for n inputs can be formally written as
a choice of constants aij and wj satisfying

m
(cid:88)

j=1

(cid:32) n
(cid:88)

i=1

(cid:33)

n
(cid:89)

i=1

wjσ

aijxi

≈

xi.

(20)

[24] shows that 2n neurons are sufﬁcient to approximate
a product gate with n inputs - each of these weights are as-
signed, in the proof, a value of +1 or −1 before normaliza-
tion, and all coefﬁcients aij also have +1/−1 values. This
essentially makes it a binary network. Weight normaliza-
, which
tion introduces a scaling constant of sorts,
would translate to α in our representation, with its negative
denoting β.
The above shows how binary networks are expressive
enough to approximate real-valued networks, without the
need for higher bit quantization.

1
2nn!σn

F. Experimental details

We used the Adam optimizer for all the models with a
maximum learning rate of 0.002 and a minimum learning
rate of 0.00005 with a decay factor of 2. All networks are
trained from scratch. Weights of all layers except the ﬁrst
were binarized throughout our experiments. Our FBin layer
is structured the same as the XNOR-Net. We performed our
experiments using a cluster of GeForce GTX 1080 Tis using
PyTorch v0.2.

Note: The above proofs for expressibility power have

been borrowed from [24].

