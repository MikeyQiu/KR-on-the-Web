8
1
0
2
 
r
a

M
 
6
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
2
4
0
2
0
.
3
0
8
1
:
v
i
X
r
a

Accelerated Gradient Boosting

G. Biau
Sorbonne Université, CNRS, LPSM
Paris, France
gerard.biau@upmc.fr

B. Cadre
Univ Rennes, CNRS, IRMAR
Rennes, France
benoit.cadre@ens-rennes.fr

L. Rouvière
Univ Rennes, CNRS, IRMAR
Rennes, France
laurent.rouviere@univ-rennes2.fr

Abstract

Gradient tree boosting is a prediction algorithm that sequentially produces
a model in the form of linear combinations of decision trees, by solving an
inﬁnite-dimensional optimization problem. We combine gradient boosting
and Nesterov’s accelerated descent to design a new algorithm, which we call
AGB (for Accelerated Gradient Boosting). Substantial numerical evidence
is provided on both synthetic and real-life data sets to assess the excellent
performance of the method in a large variety of prediction problems. It is
empirically shown that AGB is much less sensitive to the shrinkage parameter
and outputs predictors that are considerably more sparse in the number of
trees, while retaining the exceptional performance of gradient boosting.

1

Introduction

Gradient boosting (Friedman et al., 2000; Friedman, 2001, 2002) is a learning procedure that
combines the outputs of many simple predictors in order to produce a powerful committee
with performances improved over the single members. The approach is typically used with
decision trees of a ﬁxed size as base learners, and, in this context, is called gradient tree
boosting. This machine learning method is widely recognized for providing state-of-the-art
results on several challenging data sets, as pointed out for example in the introduction of
Chen and Guestrin (2016). To get to the point, boosted decision trees are generally regarded
as one of the best off-the-shell prediction algorithms we have today, with performance at the
level of the Lasso (Tibshirani, 1996) and random forests (Breiman, 2001), to name only two
competitors.

Gradient boosting originates in Freund and Schapire’s work (Schapire, 1990; Freund, 1995;
Freund and Schapire, 1996, 1997) on weighted iterative classiﬁcation. It was complemented

by several analyses by Breiman (1997, 1998, 1999, 2000, 2004), who made the fundamental
observation that Freund and Schapire’s AdaBoost is in fact a gradient-descent-type algorithm
in a function space, thus identifying boosting at the frontier of numerical optimization
and statistical estimation. Explicit regression and classiﬁcation boosting algorithms were
subsequently developed by Friedman (2001, 2002), who coined the name “gradient boosting”
and paid a special attention to the case where the individual components are decision trees.
Overall, this functional view of boosting has led to the development of boosting algorithms
in many areas of machine learning and statistics beyond regression and classiﬁcation (e.g.,
Blanchard et al., 2003; Bühlmann and Yu, 2003; Lugosi and Vayatis, 2004; Zhang and Yu,
2005; Bickel et al., 2006; Bühlmann and Hothorn, 2007).

In a different direction, the pressing demand of the machine learning community to build
accurate prediction mechanisms from massive amounts of high dimensional data has greatly
promoted the theory and practice of accelerated ﬁrst-order schemes. In this respect, one of
the most effective approaches among ﬁrst-order optimization techniques is the so-called
Nesterov’s accelerated gradient descent (Nesterov, 1983). In a nutshell, if we are interested
in minimizing some smooth convex function f (x) over Rd, then Nesterov’s descent may
take the following form (Beck and Teboulle, 2009): starting with x0 = y0, inductively deﬁne

xt+1 = yt − w∇ f (yt)
yt+1 = (1 − γt)xt+1 + γtxt,

(1)

where w is the step size,

λ0 = 0,

λt =

,

and

γt =

(cid:113)

1 +

1 + 4λ 2

t−1

2

1 − λt
λt+1

.

In other words, Nesterov’s descent performs a simple step of gradient to go from yt to xt+1,
and then it slides it a little bit further than xt+1 in the direction given by the previous point
xt. As acknowledged by Bubeck (2013), the intuition behind the algorithm is quite difﬁcult
to grasp. Nonetheless, Nesterov’s accelerated gradient descent is an optimal method for
smooth convex optimization: the sequence (xt)t recovers the minimum of f at a rate of order
1/t2, in contrast to vanilla gradient descent methods, which have the same computational
complexity but can only achieve a rate in O(1/t). Since the introduction of Nesterov’s
scheme, there has been much work on ﬁrst-order accelerated methods (see, e.g., Nesterov,
2004, 2005, 2013; Su et al., 2016, for theoretical developments, and Tseng, 2008, for a
uniﬁed analysis of these ideas). Notable applications can be found in sparse linear regression
(Beck and Teboulle, 2009), compressed sensing (Becker et al., 2011), distributed gradient
descent (Qu and Li, 2016), and deep and recurrent neural networks (Sutskever et al., 2013).

In this article, we present AGB (for Accelerated Gradient Boosting), a new tree boosting
algorithm that incorporates Nesterov’s mechanism (1) into Friedman’s original procedure
(Friedman, 2001). Substantial numerical evidence is provided on both synthetic and real-life
data sets to assess the excellent performance of our method in a large variety of prediction
problems. The striking feature of AGB is that it enjoys the merits of both approaches:

(i) Its predictive performance is comparable to that of standard gradient tree boosting;

2

(ii) It takes advantage of the accelerated descent to output models which are remarkably

much more sparse in their number of components.

Item (ii) is of course a decisive advantage for large-scale learning, when time and storage
issues matter. To make the concept clear, we show in Figure 1 typical test error results by
number of iterations and shrinkage (step size), both for the standard (top) and the accelerated
(bottom) algorithms. As is often the case with gradient boosting, smaller values of the
shrinkage parameter require a larger number of trees for the optimal model, when the test
error is at its minimum. However, if both approaches yield similar results in terms of
prediction, we see that the optimal number of iterations is at least one order of magnitude
smaller for AGB.

Figure 1: Adaboost exponential loss (estimated on a test data set) by number of iterations
for standard gradient boosting (top) and AGB (bottom). The data are generated according to
Model 5 with n = 5 000 observations (see page 8).

The paper is organized as follows. In Section 2, we brieﬂy recall the mathematical/statistical
context of gradient boosting, and present the principle of the AGB algorithm. Section 3 is
devoted to analyzing the results of a battery of experiments on synthetic and real-life data

3

sets. We offer an extensive comparison between the performance of Friedman’s gradient
tree boosting and AGB, with a special emphasis put on the inﬂuence of the learning rate on
the size of the optimal models. The code used for the simulations and the ﬁgures is available
at https://github.com/lrouviere/AGB.

2

(Accelerated) gradient boosting

2.1 Gradient boosting at a glance

Let Dn = {(X1,Y1), . . . , (Xn,Yn)} be a sample of i.i.d. observations, all distributed as an
independent generic pair (X,Y ) taking values in Rd × Y . Throughout, Y ⊂ R is either a
ﬁnite set of labels (for classiﬁcation) or a subset of R (for regression). The learning task
is to construct a predictor F : Rd → R that assigns a response to each possible value of
the independent random observation X. In the context of gradient boosting, this general
problem is addressed by considering a class F of elementary functions f : Rd → R (called
the weak or base learners), and by minimizing some empirical risk functional

Cn(F) =

ψ(F(Xi),Yi)

(2)

1
n

n
∑
i=1

over the linear combinations of functions in F . Thus, we are looking for an additive solution
j=0 α j f j, where (α0, . . . , αJ) ∈ RJ+1 and each component f j is picked in
of the form Fn = ∑J
the base class F .
The function ψ : R×Y → R+ is called the loss. It is assumed to be convex and differentiable
in its ﬁrst argument, and it measures the cost incurred by predicting F(Xi) when the answer
is Yi. For example, in the least squares regression problem, ψ(x, y) = (y − x)2, and

Cn(F) =

(Yi − F(Xi))2.

1
n

n
∑
i=1

In the ±1-classiﬁcation problem, the ﬁnal classiﬁcation rule is +1 if F(x) > 0 and −1
otherwise. In this context, two classical losses are ψ(x, y) = e−yx (Adaboost exponential
loss) and ψ(x, y) = ln2(1 + e−yx) (logit loss).
In the present document, we take for F the collection of all binary decision trees in Rd
using axis parallel cuts with k (small) terminal nodes (or leaves). Thus, each f ∈ F takes the
j=1 β j1A j, where (β1, . . . , βk) ∈ Rk and {A1, . . . , Ak} is a tree-structured partition
form f = ∑k
of Rd (Devroye et al., 1996, Chapter 20). An example of regression tree ﬁtted with the R
package rpart.plot with k = 3 leaves in dimension d = 2 is shown in Figure 2.

4

Figure 2: A regression tree in dimension d = 2 with k = 3 leaves.

Let us get back to the minimization problem (2) and denote by lin(F ) the set of all linear
combinations of functions in F , our basic collection of trees. So, each F ∈ lin(F ) is
an additive association of trees, of the form F = ∑J
j=0 α j f j. Finding the inﬁmum of the
functional Cn over lin(F ) is a challenging inﬁnite-dimensional optimization problem, which
requires an algorithm. This is where gradient boosting comes into play by sequentially
constructing a linear combination of trees, adding one new component at each step. This
algorithm rests upon a sort of functional gradient descent, which we brieﬂy describe in the
next paragraph. We do not go to much into the mathematical details, and refer to Mason
et al. (1999, 2000) and Biau and Cadre (2017) for a thorough analysis of the mathematical
forces in action.
Suppose that we have at step t a function Ft ∈ lin(F ) and wish to ﬁnd a new ft+1 ∈ F to add
to Ft so that the risk Cn(Ft + w ft+1) decreases at most, for some small value of w. Viewed
in function space terms, we are looking for the direction ft+1 ∈ F such that Cn(Ft + w ft+1)
most rapidly decreases. Observe that, for all F ∈ lin(F ), ∇Cn(F)(Xi) = ∂xψ(F(Xi),Yi),
where the symbol ∂x means partial derivative with respect to the ﬁrst component. Then the
knee-jerk reaction is to take ft+1(·) = −∇Cn(Ft)(·), the opposite of the gradient of Cn at Ft
(this is a function over Rd), and do something like

Ft+1 = Ft − w∇Cn(Ft).

However, since we are restricted to pick our new function in F , this will in general not be a
possible choice. The stratagem is to choose the new ft+1 by a least squares approximation
of the function −∇Cn(Ft)(·), i.e., to take

ft+1 ∈ arg min f ∈F

(−∇Cn(Ft)(Xi) − f (Xi))2.

For example, when ψ(x, y) = (y − x)2/2, then −∇Cn(Ft)(Xi) = Yi − Ft(Xi), and the algo-
rithm simply ﬁts ft+1 to the residuals Yi − Ft(Xi) at step t. This is the general principle
of Friedman’s gradient boosting (Friedman, 2001), which after T iterations outputs an

1
n

n
∑
i=1

5

additive expansion of the form FT = ∑T
t=0 αt ft. The operational algorithm includes several
regularization techniques to reduce the eventual overﬁtting. Some of these features are
incorporated in our accelerated version, which we now describe.

1: Require T ≥ 1 (number of iterations), k ≥ 1 (number of terminal nodes in the trees),

2.2 The AGB algorithm

The pseudo-code of AGB is presented in the table below.

AGB algorithm

0 < ν < 1 (shrinkage parameter).
2: Initialize F0 = G0 = arg minz ∑n
3: for t = 0 to (T − 1) do

i=1 ψ(z,Yi), λ0 = 0, γ0 = 1.

4:

For i = 1, . . . , n, compute the negative gradient instances

Zi,t+1 = −∇Cn(Gt)(Xi).

Fit a regression tree to the pairs (Xi, Zi,t+1), giving terminal nodes R j,t+1, 1 ≤ j ≤ k.

5:

6:

For j = 1, . . . , k, compute

w j,t+1 ∈ arg minw>0 ∑

ψ(Gt(Xi) + w,Yi).

Xi∈R j,t+1

7:

Update

(a)

(b)

Ft+1 = Gt + ν ∑k

j=1 w j,t+11R j,t+1.

Gt+1 = (1 − γt)Ft+1 + γtFt.

(c)

λt =

(cid:113)

1+

1+4λ 2
2

t−1

, λt+1 = 1+

1+4λ 2
t
2

.

√

(d)

γt = 1−λt
λt+1

.

8: end for

9: Output FT .

We see that the algorithm has two inner functional components, (Ft)t and (Gt)t, which
correspond respectively to the vectorial sequences (xt)t and (yt)t of Nesterov’s acceleration
scheme (1). Observe that the sequence (Gt)t is internal to the procedure while the linear
combination output by the algorithm after T iterations is FT . Line 2 initializes to the optimal
constant model. As in Friedman’s original approach, the algorithm selects at each iteration,

6

by least-squares ﬁtting, a particular tree that is in most agreement with the descent direction
(the “gradient”), and then performs an update of Gt. The essential difference is the presence
of the companion function sequence (Gt)t, which slides the iterates (Ft)t according to the
recursive parameters λt and γt (lines 7 (b)-(d)).
j=1 β j,t+11R j,t+1 be the approximate-gradient tree output at line 6 of the algo-
Let ft+1 = ∑k
rithm. The next logical step is to perform a line search to ﬁnd the step size and update the
model accordingly, as follows:

wt+1 ∈ arg minw>0

ψ(Gt(Xi) + w ft+1(Xi),Yi), Ft+1 = Gt + wt+1 ft+1.

n
∑
i=1

However, following Friedman’s gradient tree boosting (Friedman, 2001), a separate optimal
value w j,t+1 is chosen for each of the tree’s regions, instead of a single wt+1 for the whole
tree. The coefﬁcients β j,t+1 from the tree-ﬁtting procedure can be then simply discarded,
and the model update rule at epoch t becomes, for each j = 1, . . . , k,

w j,t+1 ∈ arg minw>0 ∑

ψ(Gt(Xi) + w,Yi), Ft+1 = Gt + ν

w j,t+11R j,t+1

Xi∈R j,t+1

k
∑
j=1

(lines 6 and 7 (a)). We also note that the contribution of the approximate gradient is scaled
by a factor 0 < ν < 1 when it is added to the current approximation. The parameter ν can
be regarded as controlling the learning rate of the boosting procedure. Smaller values of ν
(more shrinkage) usually lead to larger values of T for the same training risk. Therefore,
in order to reduce the number of trees composing the boosting estimate, large values for ν
are required. However, too large values of ν may break the gradient descent dynamic, as
shown for example in Biau and Cadre (2017, Lemma 3.2). All in all, both ν and T control
prediction risk on the training data and these parameters do not operate independently. This
tradeoff issue is thoroughly explored in the next section.

3 Numerical studies

This section is devoted to illustrating the potential of our AGB algorithm and to highlighting
the beneﬁts of Nesterov’s acceleration scheme in the boosting process. Synthetic models
and real-life data are considered, and an exhaustive comparison with standard gradient
tree boosting is performed. For the implementation of Friedman’s boosting, we used the R
package gbm, a description of which can be found in Ridgeway (2007). These two boosting
algorithms are compared in the last subsection with the Lasso (Tibshirani, 1996) and random
forests (Breiman, 2001) methods, respectively implemented with the packages glmnet and
randomForest.

3.1 Description of the data sets

The algorithms were benchmarked on both simulated and real-life data sets. For each of the
simulated models, we consider two designs for X = (X1, . . . , Xd): Uniform over (−1, 1)d
("Uncorrelated design") and Gaussian with mean 0 and d × d covariance matrix Σ such that

7

Σi j = 2−|i− j| ("Correlated design”). The ﬁve following models cover a wide spectrum of
regression and classiﬁcation problems. Models 1-3 and 5 come from Biau et al. (2016).
Model 4 is a slight variation of a benchmark model in Hastie et al. (2009). Models 1-3
are regression problems, while Model 4 and 5 are ±1-classiﬁcation tasks. Models 2-4 are
additive, while Models 1 and 5 include some interactions. Model 3 can be seen as a sparse
high-dimensional problem. We denote by Zµ,σ 2 a Gaussian random variable with mean µ
and variance σ 2.

Model 1. n = 1 000, d = 100, Y = X1X2 + X 2

3 − X4X7 + X8X10 − X 2

6 + Z0,0.5.

Model 2. n = 800, d = 100, Y = − sin(2X1) + X 2

2 + X3 − exp(−X4) + Z0,0.5.

Model 3. n = 1 000, d = 500, Y = X1 + 3X 2

3 − 2 exp(−X5) + X6.

Model 4. n = 2 000, d = 30,

(cid:40) 2 1
2 1

Y =

j=1 X 2
j=1 X 2

j >3.5 − 1
for uncorrelated design
j >9.34 − 1 for correlated design.

∑10

∑10

Model 5. n = 1 500, d = 50, Y = 2 1

X1+X 3

4 +X9+sin(X12X18)+Z0,0.1>0.38 − 1.

We also considered the following real-life data sets from the UCI Machine Learning
repository: Adult, Internet Advertisements, Communities and Crime, Spam, and Wine.
Their main characteristics are summarized in Table 1 (a more complete description is
available at the address https://archive.ics.uci.edu/ml/datasets.html).

Data set
Adult
Advert.
Crime
Spam
Wine

n
30 162
2 359
1 993
4 601
1 559

d
14
1 431
102
57
11

Output Y
binary
binary
continuous
binary
continuous

Table 1: Main characteristics of the ﬁve real-life data sets used in the experiments.

For each data set, simulated or real, the sample is divided into a training set (50%) Dtrain to ﬁt
the method; a validation set (25%) Dval to select the hyperparameters of the algorithms; and
a test set (25%) Dtest on which the predictive performance is evaluated. We considered two
loss functions for both standard boosting and AGB: the least squares loss ψ(x, y) = (y − x)2
for regression and the Adaboost loss ψ(x, y) = e−yx for ±1-classiﬁcation. We also tested
the logit loss function ψ(x, y) = ln2(1 + e−yx). Since the results are similar to the Adaboost
loss they are not reported.

In the boosting algorithms, the validation set is used to select the number of components of
the model, i.e., the number of iterations performed by the algorithm. Thus, denoting by FT

8

the boosting predictor after T iterations ﬁtted on Dtrain, we select the T (cid:63) that minimizes

1
(cid:93)Dval

∑
i∈Dval

ψ(FT (Xi),Yi).

(3)

For both standard gradient tree boosting and AGB, we ﬁt regression trees with two terminal
nodes. We considered ﬁve ﬁxed values for the shrinkage parameter ν (1e − 05, 0.001, 0.01,
0.1, and 0.5), and ﬁxed an arbitrary (large) limit of T = 10 000 iterations for the standard
boosting and T = 2 500 for AGB. All results are averaged over 100 replications for simulated
examples, and over 20 independent permutations of the sample for the real-life data.

3.2 Gradient boosting vs accelerated gradient boosting

In this subsection, we compare the standard gradient tree boosting and AGB algorithms in
terms of minimization of the empirical risk (2) and selected number of components T (cid:63).
Figure 3 shows the training and validation errors for Friedman’s boosting and AGB (bottom),
as a function of the number of iterations.

Figure 3: Training (solid lines) and validation (dashed lines) errors for Model 1 and Model
5. Shrinkage parameter ν is ﬁxed to 0.01.

As it is generally the case for gradient boosting (e.g., Ridgeway, 2007), the validation
error decreases until predictive performance is at its best and then starts increasing again.
The vertical blue line shows the optimal number of iterations T (cid:63), selected by minimizing
(3). We see that the validation rates at the optimal T (cid:63) are comparable for AGB and the

9

original algorithm. However, AGB outperforms gradient boosting in terms of number of
components of the output model, which is much smaller for AGB. This is a direct consequence
of Nesterov’s acceleration scheme.

This remarkable behavior is conﬁrmed by Figures 4, 5, and 6, where we plotted the re-
lationship between predictive performance, the number of iterations, and the shrinkage
parameter. On the left side of each ﬁgure, we show the boxplots of the test errors of the
selected predictors FT (cid:63), i.e.,

1
(cid:93)Dtest

∑
i∈Dtest

ψ(FT (cid:63)(Xi),Yi),

(4)

as a function of the shrinkage parameter ν. The right sides depict the boxplots of the optimal
number of components T (cid:63).

These three ﬁgures convey several messages. First of all, we notice that the predictive
performances of the two methods are close to each other, independently of the data sets
(simulated or real). Moreover, in line with the comments of Hastie et al. (2009, Chapter 10),
smaller values of the shrinkage parameter ν favor better test error. Indeed, for all examples
we observe that the best test errors are achieved for ν smaller than 0.1. However, for such
values of ν, it seems difﬁcult for standard boosting to reach the optimal T (cid:63) in a reasonable
number of iterations, and 10 000 iterations are generally not sufﬁcient as soon as ν is less
than 0.01. The accelerated algorithm allows to circumvent this problem since, for each value
of ν, the optimal model is achieved after a number of iterations considerably smaller than
with standard boosting. Besides, AGB is much less sensitive to the choice of ν. These two
features are clear advantages since, in practice, one has no or few a priori information on the
reasonable value of ν, and the usual strategy is to try several (often, small) values of the
shrinkage parameter until the validation error is the lowest. Of course, this beneﬁt is striking
when we are faced with large-scale data, i.e., when iterations have a computational price.

3.3 Comparison with the Lasso and random forests

We compare in this last subsection the performance of the standard and accelerated boosting
algorithms with that of the Lasso and random forests, respectively implemented with the
R packages glmnet and randomForest. As above, the number of components T (cid:63) of the
boosting predictors are selected by minimizing (3). The shrinkage parameter of the Lasso
(parameter lambda in glmnet) and the number of variables randomly sampled as candidates
at each split for the trees of the random forests (parameter mtry in randomForest) are
selected by minimizing the mean squared error (regression) and the misclassiﬁcation error
(classiﬁcation) computed on the validation set. The R-package caret was used to conduct
these minimization problems. The prediction performance of each predictor F were assessed
(cid:93)Dtest ∑i∈Dtest(Yi − F(Xi))2 for regression problems,
on the test set by the mean squared error
1F(Xi)(cid:54)=Yi and (ii) the area under ROC curve
and (i) the misclassiﬁcation error
(AUC) for classiﬁcation problems (computed on the test set).

(cid:93)Dtest ∑i∈Dtest

1

1

Table 2 shows the test errors for the regression problems, while Tables 3 and 4 display
misclassiﬁcation errors and AUC for classiﬁcation tasks. All results are averaged over 100

10

Figure 4: Boxplots of the test error (4) (left) and selected numbers of iterations (right), as
a function of the shrinkage parameter ν for standard gradient boosting (red, left) and AGB
(blue, right). Results are presented for simulated models with uncorrelated design.

11

Figure 5: Boxplots of the test error (4) (left) and number of selected iterations (right) as a
function of the shrinkage parameter ν, for standard gradient boosting (red, left) and AGB
(blue, right). Results are presented for simulated models with correlated design.

12

Figure 6: Boxplots of the test error (4) (left) and number of selected iterations (right) as a
function of the shrinkage parameter ν, for standard gradient boosting (red, left) and AGB
(blue, right). Results are presented for real-life data sets.

13

replications for simulated examples and over 20 permutations of the sample for real-life
data set.

As might be expected, the results depend on the data sets, with an advantage to boosting
algorithms, which are often the ﬁrst and perform uniformly well. Besides, even if there
is no clear winner between traditional boosting and AGB, we still ﬁnd that AGB is weakly
sensitive to the choice of ν and leads to more parsimonious models (T (cid:63) in the tables) for
both regression and classiﬁcation problems, and independently of the data set. They are the
take-home messages of our paper.

Model 1 (u)

Model 2 (u)

Model 3 (u)

Model 1 (c)

Model 2 (c)

Model 3 (c)

Crimes

Wine

1e-05
1.011
0.082
10 000
1.883
0.202
10 000
2.983
0.221
10 000
8.316
1.143
10 000
6.558
1.958
9 900
37.034
8.617
10 000
0.049
0.004
10 000
0.632
0.044
10 000

0.001
0.923
0.075
7 924
0.642
0.073
9 989
0.318
0.039
10000
4.483
0.668
10 000
2.424
1.120
10 000
6.323
3.883
10 000
0.019
0.001
9 960
0.417
0.032
9 999

GB
0.01
0.926
0.076
981
0.621
0.075
2 206
0.037
0.007
7 936
4.047
0.557
3 413
1.936
1.093
4 632
4.454
3.703
4 296
0.019
0.001
2 172
0.412
0.032
3727

ν
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)

0.1
0.927
0.076
99
0.621
0.074
214
0.040
0.008
956
4.051
0.559
330
1.938
1.095
458
4.480
3.708
415
0.019
0.002
214
0.412
0.032
366

0.5
0.930
0.079
11
0.650
0.079
26
0.119
0.015
97
4.220
0.573
47
2.093
1.118
70
5.879
3.948
54
0.021
0.002
86
0.419
0.032
79

1e-05
0.924
0.076
2 178
0.632
0.069
2 488
0.308
0.042
2 500
4.529
0.669
2 500
2.442
1.117
2 499
6.382
3.936
2 491
0.021
0.002
2 240
0.421
0.034
2 433

0.001
0.927
0.077
247
0.621
0.072
288
0.080
0.019
627
4.141
0.564
387
2.083
1.087
411
5.274
3.824
361
0.021
0.001
296
0.421
0.033
393

AGB
0.01
0.926
0.074
73
0.621
0.072
91
0.078
0.017
187
4.133
0.566
120
2.057
1.087
132
5.163
3.761
113
0.021
0.001
91
0.421
0.032
154

0.1
0.929
0.074
18
0.638
0.073
26
0.125
0.020
49
4.252
0.575
32
2.145
1.062
35
5.781
3.827
31
0.021
0.002
26
0.424
0.032
36

0.5
0.920
0.081
7
0.794
0.090
14
0.337
0.060
29
5.354
0.694
12
2.777
1.103
16
8.187
4.020
23
0.024
0.002
16
0.459
0.034
11

Lasso

RF

1.021
0.084

0.677
0.077

0.948
0.067

8.549
1.154

4.988
1.580

23.898
5.746

0.019
0.001

0.426
0.001

0.922
0.078

0.756
0.086

0.587
0.068

4.163
0.623

2.082
0.824

6.198
3.421

0.019
0.001

0.365
0.001

Table 2: Mean (m.) and standard deviation (sd.) of the mean squared test error for the
regression problems. Also shown for the boosting algorithms is the mean over all replications
of the optimal number of components (T (cid:63)) . Results are averaged over 100 independent
replications for simulated examples and over 20 independent permutations of the sample for
real-life data sets. For each data set, the two best performances are in bold.

14

Table 3: Mean (m.) and standard deviation (sd.) of the misclassiﬁcation test errors for
the classiﬁcation problems. Also shown for the boosting algorithms is the mean over all
replications of the optimal number of components (T (cid:63)) . Results are averaged over 100
independent replications for simulated examples and over 20 independent permutations of
the sample for real-life data sets. For each data set, the two best performances are in bold.

Model 4 (u)

Model 5 (u)

Model 4 (c)

Model 5 (c)

Adult

Advert

Spam

Model 4 (u)

Model 5 (u)

Model 4 (c)

Model 5 (c)

Adult

Advert

Spam

ν
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)

ν
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)

1e-05
0.416
0.020
9 900
0.353
0.024
10 000
0.451
0.027
10 000
0.423
0.037
10 000
0.249
0.004
10 000
0.165
0.014
10 000
0.396
0.013
10 000

1e-05
0.590
0.037
9 900
0.772
0.059
10 000
0.621
0.043
10 000
0.753
0.059
10 000
0.758
0.005
10 000
0.815
0.059
10 000
0.854
0.028
10 000

0.001
0.229
0.023
10 000
0.144
0.016
10 000
0.171
0.020
10 000
0.119
0.016
10 000
0.150
0.004
10 000
0.062
0.009
9 999
0.071
0.009
10 000

0.001
0.885
0.021
10 000
0.935
0.013
10 000
0.927
0.014
10 000
0.960
0.009
10 000
0.905
0.004
10 000
0.962
0.014
9999
0.975
0.003
10 000

GB
0.01
0.098
0.018
9 998
0.141
0.017
2 465
0.086
0.015
9 996
0.114
0.015
3 694
0.141
0.004
9 966
0.043
0.012
4 716
0.061
0.008
3 880

GB
0.01
0.971
0.008
9 998
0.936
0.012
2 465
0.978
0.006
9 996
0.962
0.008
3 694
0.915
0.004
9 966
0.974
0.011
4716
0.980
0.003
3 880

0.1
0.086
0.015
2 619
0.141
0.016
240
0.081
0.014
1 781
0.114
0.016
354
0.138
0.004
6 714
0.043
0.013
471
0.061
0.007
426

0.1
0.976
0.007
2 619
0.936
0.012
240
0.981
0.005
1 781
0.963
0.008
354
0.920
0.004
6 714
0.973
0.012
471
0.980
0.003
426

0.5
0.085
0.016
452
0.142
0.018
41
0.079
0.014
319
0.115
0.016
65
0.138
0.004
1 635
0.043
0.012
87
0.065
0.007
84

0.5
0.977
0.007
452
0.934
0.013
41
0.981
0.005
319
0.961
0.008
65
0.920
0.003
1 635
0.973
0.013
87
0.979
0.003
84

1e-05
0.248
0.023
2 500
0.145
0.017
2 500
0.185
0.022
2 500
0.123
0.018
2 500
0.151
0.004
2 500
0.063
0.008
2 500
0.077
0.009
2 500

1e-05
0.869
0.023
2 500
0.933
0.013
2 500
0.916
0.016
2 500
0.957
0.009
2 500
0.902
0.004
2 500
0.956
0.015
2500
0.973
0.004
2 500

0.001
0.085
0.016
1 404
0.142
0.018
387
0.080
0.014
1 156
0.114
0.016
493
0.140
0.005
1 853
0.043
0.013
568
0.064
0.009
479

0.001
0.977
0.007
1404
0.937
0.013
387
0.981
0.005
1 156
0.962
0.008
493
0.918
0.004
1 853
0.973
0.014
568
0.978
0.003
479

AGB
0.01
0.088
0.016
421
0.141
0.017
121
0.081
0.015
358
0.116
0.016
151
0.140
0.005
610
0.043
0.013
181
0.065
0.007
150

AGB
0.01
0.975
0.007
421
0.936
0.012
121
0.981
0.005
358
0.962
0.008
151
0.917
0.004
610
0.975
0.011
181
0.978
0.003
150

0.1
0.108
0.017
97
0.144
0.016
34
0.095
0.015
88
0.118
0.016
40
0.143
0.004
143
0.044
0.011
50
0.068
0.007
40

0.1
0.964
0.010
97
0.935
0.012
34
0.972
0.008
88
0.960
0.008
40
0.913
0.003
143
0.971
0.015
50
0.977
0.003
40

0.5
0.217
0.036
22
0.155
0.021
12
0.183
0.03
23
0.132
0.020
14
0.152
0.004
24
0.054
0.011
18
0.086
0.011
16

0.5
0.862
0.040
22
0.922
0.015
12
0.898
0.030
23
0.947
0.011
14
0.901
0.004
24
0.950
0.022
18
0.966
0.005
16

Lasso

RF

0.419
0.021

0.138
0.018

0.453
0.025

0.118
0.016

0.155
0.004

0.032
0.007

0.095
0.072

0.515
0.018

0.940
0.011

0.516
0.019

0.960
0.007

0.902
0.004

0.973
0.008

0.970
0.004

0.206
0.025

0.151
0.019

0.134
0.018

0.116
0.018

0.186
0.005

0.031
0.009

0.057
0.007

0.891
0.021

0.922
0.016

0.945
0.012

0.955
0.011

0.858
0.008

0.983
0.008

0.979
0.003

Lasso

RF

Table 4: Mean (m.) and standard deviation (sd.) of AUC for the classiﬁcation problems.
Also shown for the boosting algorithms is the mean over all replications of the optimal
number of components (T (cid:63)) . Results are averaged over 100 independent replications for
simulated examples and over 20 independent permutations of the sample for real-life data
sets. For each data set, the two best performances are in bold.

References

A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences, 2:183–202, 2009.

15

S. Becker, J. Bobin, and E.J. Candès. NESTA: A fast and accurate ﬁrst-order method for

sparse recovery. SIAM Journal on Imaging Sciences, 4:1–39, 2011.

G. Biau and B. Cadre. Optimization by gradient boosting. arXiv:1707.05023, 2017.

G. Biau, A. Fischer, B. Guedj, and J.D. Malley. COBRA: A combined regression strategy.

Journal of Multivariate Analysis, 146:18–28, 2016.

P.J. Bickel, Y. Ritov, and A. Zakai. Some theory for generalized boosting algorithms.

Journal of Machine Learning Research, 7:705–732, 2006.

G. Blanchard, G. Lugosi, and N. Vayatis. On the rate of convergence of regularized boosting

classiﬁers. Journal of Machine Learning Research, 4:861–894, 2003.

L. Breiman. Arcing the edge. Technical Report 486, Statistics Department, University of

California, Berkeley, 1997.

L. Breiman. Arcing classiﬁers (with discussion). The Annals of Statistics, 26:801–824,

L. Breiman. Prediction games and arcing algorithms. Neural Computation, 11:1493–1517,

L. Breiman. Some inﬁnite theory for predictor ensembles. Technical Report 577, Statistics

Department, University of California, Berkeley, 2000.

L. Breiman. Random forests. Machine Learning, 45:5–32, 2001.

L. Breiman. Population theory for boosting ensembles. The Annals of Statistics, 32:1–11,

1998.

1999.

2004.

S. Bubeck. ORF523: Nesterov’s accelerated gradient descent, 2013. URL https://blogs.

princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent.

P. Bühlmann and T. Hothorn. Boosting algorithms: Regularization, prediction and model

ﬁtting (with discussion). Statistical Science, 22:477–505, 2007.

P. Bühlmann and B. Yu. Boosting with the L2 loss: Regression and classiﬁcation. Journal

of the American Statistical Association, 98:324–339, 2003.

T. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of
the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pages 785–794. ACM, New York, 2016.

L. Devroye, L. Györﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition.

Y. Freund. Boosting a weak learning algorithm by majority. Information and Computation,

Springer, New York, 1996.

121:256–285, 1995.

16

Y. Freund and R.E. Schapire. Experiments with a new boosting algorithm. In S. Lorenza,
editor, Machine Learning: Proceedings of the Thirteenth International Conference on
Machine Learning, pages 148–156. Morgan Kaufmann Publishers, San Francisco, 1996.

Y. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and System Sciences, 55:119–139, 1997.

J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: A statistical view of

boosting (with discussion). The Annals of Statistics, 28:337–374, 2000.

J.H. Friedman. Greedy function approximation: A gradient boosting machine. The Annals

of Statistics, 29:1189–1232, 2001.

J.H. Friedman. Stochastic gradient boosting. Computational Statistics & Data Analysis, 38:

367–378, 2002.

T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data

Mining, Inference, and Prediction. Second Edition. Springer, New York, 2009.

G. Lugosi and N. Vayatis. On the Bayes-risk consistency of regularized boosting methods.

The Annals of Statistics, 32:30–55, 2004.

L. Mason, J. Baxter, P. Bartlett, and M. Frean. Boosting algorithms as gradient descent.
In S.A. Solla, T.K. Leen, and K. Müller, editors, Proceedings of the 12th International
Conference on Neural Information Processing Systems, pages 512–518. The MIT Press,
Cambridge, MA, 1999.

L. Mason, J. Baxter, P. Bartlett, and M. Frean. Functional gradient techniques for combining
hypotheses. In A.J. Smola, P.L. Bartlett, B. Schölkopf, and D. Schuurmans, editors,
Advances in Large Margin Classiﬁers, pages 221–246. The MIT Press, Cambridge, MA,
2000.

Y. Nesterov. A method of solving a convex programming problem with convergence rate

O(1/k2). Soviet Mathematics Doklady, 27:372–376, 1983.

Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer

Science+Business Media, New York, 2004.

Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming,

103:127–152, 2005.

Y. Nesterov. Gradient methods for minimizing composite functions. Mathematical Pro-

gramming, 140:125–161, 2013.

G. Qu and N. Li. Accelerated distributed Nesterov gradient descent.

In 54th Annual
Allerton Conference on Communication, Control, and Computing, pages 209–216. Curran
Associates, Inc., Red Hook, 2016.

G. Ridgeway. Generalized boosted models: A guide to the gbm package, 2007. URL

http://www.saedsayad.com/docs/gbm2.pdf.

17

R.E. Schapire. The strength of weak learnability. Machine Learning, 5:197–227, 1990.

W. Su, S. Boyd, and E.J. Candès. A differential equation for modeling Nesterov’s accelerated
gradient method: Theory and insights. Journal of Machine Learning Research, 17:1–43,
2016.

I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and
momentum in deep learning. In S. Dasgupta and D. McAllester, editors, Proceedings of
the 30th International Conference on Machine Learning, pages 1139–1147. Proceedings
of Machine Learning Research, 2013.

R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal

Statistical Society. Series B, 58:267–288, 1996.

P. Tseng. On accelerated proximal gradient methods for convex-concave optimization, 2008.

URL http://www.mit.edu/~dimitrib/PTseng/papers/apgm.pdf.

T. Zhang and B. Yu. Boosting with early stopping: Convergence and consistency. The

Annals of Statistics, 33:1538–1579, 2005.

18

8
1
0
2
 
r
a

M
 
6
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
2
4
0
2
0
.
3
0
8
1
:
v
i
X
r
a

Accelerated Gradient Boosting

G. Biau
Sorbonne Université, CNRS, LPSM
Paris, France
gerard.biau@upmc.fr

B. Cadre
Univ Rennes, CNRS, IRMAR
Rennes, France
benoit.cadre@ens-rennes.fr

L. Rouvière
Univ Rennes, CNRS, IRMAR
Rennes, France
laurent.rouviere@univ-rennes2.fr

Abstract

Gradient tree boosting is a prediction algorithm that sequentially produces
a model in the form of linear combinations of decision trees, by solving an
inﬁnite-dimensional optimization problem. We combine gradient boosting
and Nesterov’s accelerated descent to design a new algorithm, which we call
AGB (for Accelerated Gradient Boosting). Substantial numerical evidence
is provided on both synthetic and real-life data sets to assess the excellent
performance of the method in a large variety of prediction problems. It is
empirically shown that AGB is much less sensitive to the shrinkage parameter
and outputs predictors that are considerably more sparse in the number of
trees, while retaining the exceptional performance of gradient boosting.

1

Introduction

Gradient boosting (Friedman et al., 2000; Friedman, 2001, 2002) is a learning procedure that
combines the outputs of many simple predictors in order to produce a powerful committee
with performances improved over the single members. The approach is typically used with
decision trees of a ﬁxed size as base learners, and, in this context, is called gradient tree
boosting. This machine learning method is widely recognized for providing state-of-the-art
results on several challenging data sets, as pointed out for example in the introduction of
Chen and Guestrin (2016). To get to the point, boosted decision trees are generally regarded
as one of the best off-the-shell prediction algorithms we have today, with performance at the
level of the Lasso (Tibshirani, 1996) and random forests (Breiman, 2001), to name only two
competitors.

Gradient boosting originates in Freund and Schapire’s work (Schapire, 1990; Freund, 1995;
Freund and Schapire, 1996, 1997) on weighted iterative classiﬁcation. It was complemented

by several analyses by Breiman (1997, 1998, 1999, 2000, 2004), who made the fundamental
observation that Freund and Schapire’s AdaBoost is in fact a gradient-descent-type algorithm
in a function space, thus identifying boosting at the frontier of numerical optimization
and statistical estimation. Explicit regression and classiﬁcation boosting algorithms were
subsequently developed by Friedman (2001, 2002), who coined the name “gradient boosting”
and paid a special attention to the case where the individual components are decision trees.
Overall, this functional view of boosting has led to the development of boosting algorithms
in many areas of machine learning and statistics beyond regression and classiﬁcation (e.g.,
Blanchard et al., 2003; Bühlmann and Yu, 2003; Lugosi and Vayatis, 2004; Zhang and Yu,
2005; Bickel et al., 2006; Bühlmann and Hothorn, 2007).

In a different direction, the pressing demand of the machine learning community to build
accurate prediction mechanisms from massive amounts of high dimensional data has greatly
promoted the theory and practice of accelerated ﬁrst-order schemes. In this respect, one of
the most effective approaches among ﬁrst-order optimization techniques is the so-called
Nesterov’s accelerated gradient descent (Nesterov, 1983). In a nutshell, if we are interested
in minimizing some smooth convex function f (x) over Rd, then Nesterov’s descent may
take the following form (Beck and Teboulle, 2009): starting with x0 = y0, inductively deﬁne

xt+1 = yt − w∇ f (yt)
yt+1 = (1 − γt)xt+1 + γtxt,

(1)

where w is the step size,

λ0 = 0,

λt =

,

and

γt =

(cid:113)

1 +

1 + 4λ 2

t−1

2

1 − λt
λt+1

.

In other words, Nesterov’s descent performs a simple step of gradient to go from yt to xt+1,
and then it slides it a little bit further than xt+1 in the direction given by the previous point
xt. As acknowledged by Bubeck (2013), the intuition behind the algorithm is quite difﬁcult
to grasp. Nonetheless, Nesterov’s accelerated gradient descent is an optimal method for
smooth convex optimization: the sequence (xt)t recovers the minimum of f at a rate of order
1/t2, in contrast to vanilla gradient descent methods, which have the same computational
complexity but can only achieve a rate in O(1/t). Since the introduction of Nesterov’s
scheme, there has been much work on ﬁrst-order accelerated methods (see, e.g., Nesterov,
2004, 2005, 2013; Su et al., 2016, for theoretical developments, and Tseng, 2008, for a
uniﬁed analysis of these ideas). Notable applications can be found in sparse linear regression
(Beck and Teboulle, 2009), compressed sensing (Becker et al., 2011), distributed gradient
descent (Qu and Li, 2016), and deep and recurrent neural networks (Sutskever et al., 2013).

In this article, we present AGB (for Accelerated Gradient Boosting), a new tree boosting
algorithm that incorporates Nesterov’s mechanism (1) into Friedman’s original procedure
(Friedman, 2001). Substantial numerical evidence is provided on both synthetic and real-life
data sets to assess the excellent performance of our method in a large variety of prediction
problems. The striking feature of AGB is that it enjoys the merits of both approaches:

(i) Its predictive performance is comparable to that of standard gradient tree boosting;

2

(ii) It takes advantage of the accelerated descent to output models which are remarkably

much more sparse in their number of components.

Item (ii) is of course a decisive advantage for large-scale learning, when time and storage
issues matter. To make the concept clear, we show in Figure 1 typical test error results by
number of iterations and shrinkage (step size), both for the standard (top) and the accelerated
(bottom) algorithms. As is often the case with gradient boosting, smaller values of the
shrinkage parameter require a larger number of trees for the optimal model, when the test
error is at its minimum. However, if both approaches yield similar results in terms of
prediction, we see that the optimal number of iterations is at least one order of magnitude
smaller for AGB.

Figure 1: Adaboost exponential loss (estimated on a test data set) by number of iterations
for standard gradient boosting (top) and AGB (bottom). The data are generated according to
Model 5 with n = 5 000 observations (see page 8).

The paper is organized as follows. In Section 2, we brieﬂy recall the mathematical/statistical
context of gradient boosting, and present the principle of the AGB algorithm. Section 3 is
devoted to analyzing the results of a battery of experiments on synthetic and real-life data

3

sets. We offer an extensive comparison between the performance of Friedman’s gradient
tree boosting and AGB, with a special emphasis put on the inﬂuence of the learning rate on
the size of the optimal models. The code used for the simulations and the ﬁgures is available
at https://github.com/lrouviere/AGB.

2

(Accelerated) gradient boosting

2.1 Gradient boosting at a glance

Let Dn = {(X1,Y1), . . . , (Xn,Yn)} be a sample of i.i.d. observations, all distributed as an
independent generic pair (X,Y ) taking values in Rd × Y . Throughout, Y ⊂ R is either a
ﬁnite set of labels (for classiﬁcation) or a subset of R (for regression). The learning task
is to construct a predictor F : Rd → R that assigns a response to each possible value of
the independent random observation X. In the context of gradient boosting, this general
problem is addressed by considering a class F of elementary functions f : Rd → R (called
the weak or base learners), and by minimizing some empirical risk functional

Cn(F) =

ψ(F(Xi),Yi)

(2)

1
n

n
∑
i=1

over the linear combinations of functions in F . Thus, we are looking for an additive solution
j=0 α j f j, where (α0, . . . , αJ) ∈ RJ+1 and each component f j is picked in
of the form Fn = ∑J
the base class F .
The function ψ : R×Y → R+ is called the loss. It is assumed to be convex and differentiable
in its ﬁrst argument, and it measures the cost incurred by predicting F(Xi) when the answer
is Yi. For example, in the least squares regression problem, ψ(x, y) = (y − x)2, and

Cn(F) =

(Yi − F(Xi))2.

1
n

n
∑
i=1

In the ±1-classiﬁcation problem, the ﬁnal classiﬁcation rule is +1 if F(x) > 0 and −1
otherwise. In this context, two classical losses are ψ(x, y) = e−yx (Adaboost exponential
loss) and ψ(x, y) = ln2(1 + e−yx) (logit loss).
In the present document, we take for F the collection of all binary decision trees in Rd
using axis parallel cuts with k (small) terminal nodes (or leaves). Thus, each f ∈ F takes the
j=1 β j1A j, where (β1, . . . , βk) ∈ Rk and {A1, . . . , Ak} is a tree-structured partition
form f = ∑k
of Rd (Devroye et al., 1996, Chapter 20). An example of regression tree ﬁtted with the R
package rpart.plot with k = 3 leaves in dimension d = 2 is shown in Figure 2.

4

Figure 2: A regression tree in dimension d = 2 with k = 3 leaves.

Let us get back to the minimization problem (2) and denote by lin(F ) the set of all linear
combinations of functions in F , our basic collection of trees. So, each F ∈ lin(F ) is
an additive association of trees, of the form F = ∑J
j=0 α j f j. Finding the inﬁmum of the
functional Cn over lin(F ) is a challenging inﬁnite-dimensional optimization problem, which
requires an algorithm. This is where gradient boosting comes into play by sequentially
constructing a linear combination of trees, adding one new component at each step. This
algorithm rests upon a sort of functional gradient descent, which we brieﬂy describe in the
next paragraph. We do not go to much into the mathematical details, and refer to Mason
et al. (1999, 2000) and Biau and Cadre (2017) for a thorough analysis of the mathematical
forces in action.
Suppose that we have at step t a function Ft ∈ lin(F ) and wish to ﬁnd a new ft+1 ∈ F to add
to Ft so that the risk Cn(Ft + w ft+1) decreases at most, for some small value of w. Viewed
in function space terms, we are looking for the direction ft+1 ∈ F such that Cn(Ft + w ft+1)
most rapidly decreases. Observe that, for all F ∈ lin(F ), ∇Cn(F)(Xi) = ∂xψ(F(Xi),Yi),
where the symbol ∂x means partial derivative with respect to the ﬁrst component. Then the
knee-jerk reaction is to take ft+1(·) = −∇Cn(Ft)(·), the opposite of the gradient of Cn at Ft
(this is a function over Rd), and do something like

Ft+1 = Ft − w∇Cn(Ft).

However, since we are restricted to pick our new function in F , this will in general not be a
possible choice. The stratagem is to choose the new ft+1 by a least squares approximation
of the function −∇Cn(Ft)(·), i.e., to take

ft+1 ∈ arg min f ∈F

(−∇Cn(Ft)(Xi) − f (Xi))2.

For example, when ψ(x, y) = (y − x)2/2, then −∇Cn(Ft)(Xi) = Yi − Ft(Xi), and the algo-
rithm simply ﬁts ft+1 to the residuals Yi − Ft(Xi) at step t. This is the general principle
of Friedman’s gradient boosting (Friedman, 2001), which after T iterations outputs an

1
n

n
∑
i=1

5

additive expansion of the form FT = ∑T
t=0 αt ft. The operational algorithm includes several
regularization techniques to reduce the eventual overﬁtting. Some of these features are
incorporated in our accelerated version, which we now describe.

1: Require T ≥ 1 (number of iterations), k ≥ 1 (number of terminal nodes in the trees),

2.2 The AGB algorithm

The pseudo-code of AGB is presented in the table below.

AGB algorithm

0 < ν < 1 (shrinkage parameter).
2: Initialize F0 = G0 = arg minz ∑n
3: for t = 0 to (T − 1) do

i=1 ψ(z,Yi), λ0 = 0, γ0 = 1.

4:

For i = 1, . . . , n, compute the negative gradient instances

Zi,t+1 = −∇Cn(Gt)(Xi).

Fit a regression tree to the pairs (Xi, Zi,t+1), giving terminal nodes R j,t+1, 1 ≤ j ≤ k.

5:

6:

For j = 1, . . . , k, compute

w j,t+1 ∈ arg minw>0 ∑

ψ(Gt(Xi) + w,Yi).

Xi∈R j,t+1

7:

Update

(a)

(b)

Ft+1 = Gt + ν ∑k

j=1 w j,t+11R j,t+1.

Gt+1 = (1 − γt)Ft+1 + γtFt.

(c)

λt =

(cid:113)

1+

1+4λ 2
2

t−1

, λt+1 = 1+

1+4λ 2
t
2

.

√

(d)

γt = 1−λt
λt+1

.

8: end for

9: Output FT .

We see that the algorithm has two inner functional components, (Ft)t and (Gt)t, which
correspond respectively to the vectorial sequences (xt)t and (yt)t of Nesterov’s acceleration
scheme (1). Observe that the sequence (Gt)t is internal to the procedure while the linear
combination output by the algorithm after T iterations is FT . Line 2 initializes to the optimal
constant model. As in Friedman’s original approach, the algorithm selects at each iteration,

6

by least-squares ﬁtting, a particular tree that is in most agreement with the descent direction
(the “gradient”), and then performs an update of Gt. The essential difference is the presence
of the companion function sequence (Gt)t, which slides the iterates (Ft)t according to the
recursive parameters λt and γt (lines 7 (b)-(d)).
j=1 β j,t+11R j,t+1 be the approximate-gradient tree output at line 6 of the algo-
Let ft+1 = ∑k
rithm. The next logical step is to perform a line search to ﬁnd the step size and update the
model accordingly, as follows:

wt+1 ∈ arg minw>0

ψ(Gt(Xi) + w ft+1(Xi),Yi), Ft+1 = Gt + wt+1 ft+1.

n
∑
i=1

However, following Friedman’s gradient tree boosting (Friedman, 2001), a separate optimal
value w j,t+1 is chosen for each of the tree’s regions, instead of a single wt+1 for the whole
tree. The coefﬁcients β j,t+1 from the tree-ﬁtting procedure can be then simply discarded,
and the model update rule at epoch t becomes, for each j = 1, . . . , k,

w j,t+1 ∈ arg minw>0 ∑

ψ(Gt(Xi) + w,Yi), Ft+1 = Gt + ν

w j,t+11R j,t+1

Xi∈R j,t+1

k
∑
j=1

(lines 6 and 7 (a)). We also note that the contribution of the approximate gradient is scaled
by a factor 0 < ν < 1 when it is added to the current approximation. The parameter ν can
be regarded as controlling the learning rate of the boosting procedure. Smaller values of ν
(more shrinkage) usually lead to larger values of T for the same training risk. Therefore,
in order to reduce the number of trees composing the boosting estimate, large values for ν
are required. However, too large values of ν may break the gradient descent dynamic, as
shown for example in Biau and Cadre (2017, Lemma 3.2). All in all, both ν and T control
prediction risk on the training data and these parameters do not operate independently. This
tradeoff issue is thoroughly explored in the next section.

3 Numerical studies

This section is devoted to illustrating the potential of our AGB algorithm and to highlighting
the beneﬁts of Nesterov’s acceleration scheme in the boosting process. Synthetic models
and real-life data are considered, and an exhaustive comparison with standard gradient
tree boosting is performed. For the implementation of Friedman’s boosting, we used the R
package gbm, a description of which can be found in Ridgeway (2007). These two boosting
algorithms are compared in the last subsection with the Lasso (Tibshirani, 1996) and random
forests (Breiman, 2001) methods, respectively implemented with the packages glmnet and
randomForest.

3.1 Description of the data sets

The algorithms were benchmarked on both simulated and real-life data sets. For each of the
simulated models, we consider two designs for X = (X1, . . . , Xd): Uniform over (−1, 1)d
("Uncorrelated design") and Gaussian with mean 0 and d × d covariance matrix Σ such that

7

Σi j = 2−|i− j| ("Correlated design”). The ﬁve following models cover a wide spectrum of
regression and classiﬁcation problems. Models 1-3 and 5 come from Biau et al. (2016).
Model 4 is a slight variation of a benchmark model in Hastie et al. (2009). Models 1-3
are regression problems, while Model 4 and 5 are ±1-classiﬁcation tasks. Models 2-4 are
additive, while Models 1 and 5 include some interactions. Model 3 can be seen as a sparse
high-dimensional problem. We denote by Zµ,σ 2 a Gaussian random variable with mean µ
and variance σ 2.

Model 1. n = 1 000, d = 100, Y = X1X2 + X 2

3 − X4X7 + X8X10 − X 2

6 + Z0,0.5.

Model 2. n = 800, d = 100, Y = − sin(2X1) + X 2

2 + X3 − exp(−X4) + Z0,0.5.

Model 3. n = 1 000, d = 500, Y = X1 + 3X 2

3 − 2 exp(−X5) + X6.

Model 4. n = 2 000, d = 30,

(cid:40) 2 1
2 1

Y =

j=1 X 2
j=1 X 2

j >3.5 − 1
for uncorrelated design
j >9.34 − 1 for correlated design.

∑10

∑10

Model 5. n = 1 500, d = 50, Y = 2 1

X1+X 3

4 +X9+sin(X12X18)+Z0,0.1>0.38 − 1.

We also considered the following real-life data sets from the UCI Machine Learning
repository: Adult, Internet Advertisements, Communities and Crime, Spam, and Wine.
Their main characteristics are summarized in Table 1 (a more complete description is
available at the address https://archive.ics.uci.edu/ml/datasets.html).

Data set
Adult
Advert.
Crime
Spam
Wine

n
30 162
2 359
1 993
4 601
1 559

d
14
1 431
102
57
11

Output Y
binary
binary
continuous
binary
continuous

Table 1: Main characteristics of the ﬁve real-life data sets used in the experiments.

For each data set, simulated or real, the sample is divided into a training set (50%) Dtrain to ﬁt
the method; a validation set (25%) Dval to select the hyperparameters of the algorithms; and
a test set (25%) Dtest on which the predictive performance is evaluated. We considered two
loss functions for both standard boosting and AGB: the least squares loss ψ(x, y) = (y − x)2
for regression and the Adaboost loss ψ(x, y) = e−yx for ±1-classiﬁcation. We also tested
the logit loss function ψ(x, y) = ln2(1 + e−yx). Since the results are similar to the Adaboost
loss they are not reported.

In the boosting algorithms, the validation set is used to select the number of components of
the model, i.e., the number of iterations performed by the algorithm. Thus, denoting by FT

8

the boosting predictor after T iterations ﬁtted on Dtrain, we select the T (cid:63) that minimizes

1
(cid:93)Dval

∑
i∈Dval

ψ(FT (Xi),Yi).

(3)

For both standard gradient tree boosting and AGB, we ﬁt regression trees with two terminal
nodes. We considered ﬁve ﬁxed values for the shrinkage parameter ν (1e − 05, 0.001, 0.01,
0.1, and 0.5), and ﬁxed an arbitrary (large) limit of T = 10 000 iterations for the standard
boosting and T = 2 500 for AGB. All results are averaged over 100 replications for simulated
examples, and over 20 independent permutations of the sample for the real-life data.

3.2 Gradient boosting vs accelerated gradient boosting

In this subsection, we compare the standard gradient tree boosting and AGB algorithms in
terms of minimization of the empirical risk (2) and selected number of components T (cid:63).
Figure 3 shows the training and validation errors for Friedman’s boosting and AGB (bottom),
as a function of the number of iterations.

Figure 3: Training (solid lines) and validation (dashed lines) errors for Model 1 and Model
5. Shrinkage parameter ν is ﬁxed to 0.01.

As it is generally the case for gradient boosting (e.g., Ridgeway, 2007), the validation
error decreases until predictive performance is at its best and then starts increasing again.
The vertical blue line shows the optimal number of iterations T (cid:63), selected by minimizing
(3). We see that the validation rates at the optimal T (cid:63) are comparable for AGB and the

9

original algorithm. However, AGB outperforms gradient boosting in terms of number of
components of the output model, which is much smaller for AGB. This is a direct consequence
of Nesterov’s acceleration scheme.

This remarkable behavior is conﬁrmed by Figures 4, 5, and 6, where we plotted the re-
lationship between predictive performance, the number of iterations, and the shrinkage
parameter. On the left side of each ﬁgure, we show the boxplots of the test errors of the
selected predictors FT (cid:63), i.e.,

1
(cid:93)Dtest

∑
i∈Dtest

ψ(FT (cid:63)(Xi),Yi),

(4)

as a function of the shrinkage parameter ν. The right sides depict the boxplots of the optimal
number of components T (cid:63).

These three ﬁgures convey several messages. First of all, we notice that the predictive
performances of the two methods are close to each other, independently of the data sets
(simulated or real). Moreover, in line with the comments of Hastie et al. (2009, Chapter 10),
smaller values of the shrinkage parameter ν favor better test error. Indeed, for all examples
we observe that the best test errors are achieved for ν smaller than 0.1. However, for such
values of ν, it seems difﬁcult for standard boosting to reach the optimal T (cid:63) in a reasonable
number of iterations, and 10 000 iterations are generally not sufﬁcient as soon as ν is less
than 0.01. The accelerated algorithm allows to circumvent this problem since, for each value
of ν, the optimal model is achieved after a number of iterations considerably smaller than
with standard boosting. Besides, AGB is much less sensitive to the choice of ν. These two
features are clear advantages since, in practice, one has no or few a priori information on the
reasonable value of ν, and the usual strategy is to try several (often, small) values of the
shrinkage parameter until the validation error is the lowest. Of course, this beneﬁt is striking
when we are faced with large-scale data, i.e., when iterations have a computational price.

3.3 Comparison with the Lasso and random forests

We compare in this last subsection the performance of the standard and accelerated boosting
algorithms with that of the Lasso and random forests, respectively implemented with the
R packages glmnet and randomForest. As above, the number of components T (cid:63) of the
boosting predictors are selected by minimizing (3). The shrinkage parameter of the Lasso
(parameter lambda in glmnet) and the number of variables randomly sampled as candidates
at each split for the trees of the random forests (parameter mtry in randomForest) are
selected by minimizing the mean squared error (regression) and the misclassiﬁcation error
(classiﬁcation) computed on the validation set. The R-package caret was used to conduct
these minimization problems. The prediction performance of each predictor F were assessed
(cid:93)Dtest ∑i∈Dtest(Yi − F(Xi))2 for regression problems,
on the test set by the mean squared error
1F(Xi)(cid:54)=Yi and (ii) the area under ROC curve
and (i) the misclassiﬁcation error
(AUC) for classiﬁcation problems (computed on the test set).

(cid:93)Dtest ∑i∈Dtest

1

1

Table 2 shows the test errors for the regression problems, while Tables 3 and 4 display
misclassiﬁcation errors and AUC for classiﬁcation tasks. All results are averaged over 100

10

Figure 4: Boxplots of the test error (4) (left) and selected numbers of iterations (right), as
a function of the shrinkage parameter ν for standard gradient boosting (red, left) and AGB
(blue, right). Results are presented for simulated models with uncorrelated design.

11

Figure 5: Boxplots of the test error (4) (left) and number of selected iterations (right) as a
function of the shrinkage parameter ν, for standard gradient boosting (red, left) and AGB
(blue, right). Results are presented for simulated models with correlated design.

12

Figure 6: Boxplots of the test error (4) (left) and number of selected iterations (right) as a
function of the shrinkage parameter ν, for standard gradient boosting (red, left) and AGB
(blue, right). Results are presented for real-life data sets.

13

replications for simulated examples and over 20 permutations of the sample for real-life
data set.

As might be expected, the results depend on the data sets, with an advantage to boosting
algorithms, which are often the ﬁrst and perform uniformly well. Besides, even if there
is no clear winner between traditional boosting and AGB, we still ﬁnd that AGB is weakly
sensitive to the choice of ν and leads to more parsimonious models (T (cid:63) in the tables) for
both regression and classiﬁcation problems, and independently of the data set. They are the
take-home messages of our paper.

Model 1 (u)

Model 2 (u)

Model 3 (u)

Model 1 (c)

Model 2 (c)

Model 3 (c)

Crimes

Wine

1e-05
1.011
0.082
10 000
1.883
0.202
10 000
2.983
0.221
10 000
8.316
1.143
10 000
6.558
1.958
9 900
37.034
8.617
10 000
0.049
0.004
10 000
0.632
0.044
10 000

0.001
0.923
0.075
7 924
0.642
0.073
9 989
0.318
0.039
10000
4.483
0.668
10 000
2.424
1.120
10 000
6.323
3.883
10 000
0.019
0.001
9 960
0.417
0.032
9 999

GB
0.01
0.926
0.076
981
0.621
0.075
2 206
0.037
0.007
7 936
4.047
0.557
3 413
1.936
1.093
4 632
4.454
3.703
4 296
0.019
0.001
2 172
0.412
0.032
3727

ν
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)

0.1
0.927
0.076
99
0.621
0.074
214
0.040
0.008
956
4.051
0.559
330
1.938
1.095
458
4.480
3.708
415
0.019
0.002
214
0.412
0.032
366

0.5
0.930
0.079
11
0.650
0.079
26
0.119
0.015
97
4.220
0.573
47
2.093
1.118
70
5.879
3.948
54
0.021
0.002
86
0.419
0.032
79

1e-05
0.924
0.076
2 178
0.632
0.069
2 488
0.308
0.042
2 500
4.529
0.669
2 500
2.442
1.117
2 499
6.382
3.936
2 491
0.021
0.002
2 240
0.421
0.034
2 433

0.001
0.927
0.077
247
0.621
0.072
288
0.080
0.019
627
4.141
0.564
387
2.083
1.087
411
5.274
3.824
361
0.021
0.001
296
0.421
0.033
393

AGB
0.01
0.926
0.074
73
0.621
0.072
91
0.078
0.017
187
4.133
0.566
120
2.057
1.087
132
5.163
3.761
113
0.021
0.001
91
0.421
0.032
154

0.1
0.929
0.074
18
0.638
0.073
26
0.125
0.020
49
4.252
0.575
32
2.145
1.062
35
5.781
3.827
31
0.021
0.002
26
0.424
0.032
36

0.5
0.920
0.081
7
0.794
0.090
14
0.337
0.060
29
5.354
0.694
12
2.777
1.103
16
8.187
4.020
23
0.024
0.002
16
0.459
0.034
11

Lasso

RF

1.021
0.084

0.677
0.077

0.948
0.067

8.549
1.154

4.988
1.580

23.898
5.746

0.019
0.001

0.426
0.001

0.922
0.078

0.756
0.086

0.587
0.068

4.163
0.623

2.082
0.824

6.198
3.421

0.019
0.001

0.365
0.001

Table 2: Mean (m.) and standard deviation (sd.) of the mean squared test error for the
regression problems. Also shown for the boosting algorithms is the mean over all replications
of the optimal number of components (T (cid:63)) . Results are averaged over 100 independent
replications for simulated examples and over 20 independent permutations of the sample for
real-life data sets. For each data set, the two best performances are in bold.

14

Table 3: Mean (m.) and standard deviation (sd.) of the misclassiﬁcation test errors for
the classiﬁcation problems. Also shown for the boosting algorithms is the mean over all
replications of the optimal number of components (T (cid:63)) . Results are averaged over 100
independent replications for simulated examples and over 20 independent permutations of
the sample for real-life data sets. For each data set, the two best performances are in bold.

Model 4 (u)

Model 5 (u)

Model 4 (c)

Model 5 (c)

Adult

Advert

Spam

Model 4 (u)

Model 5 (u)

Model 4 (c)

Model 5 (c)

Adult

Advert

Spam

ν
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)

ν
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)
m.
sd.
T (cid:63)

1e-05
0.416
0.020
9 900
0.353
0.024
10 000
0.451
0.027
10 000
0.423
0.037
10 000
0.249
0.004
10 000
0.165
0.014
10 000
0.396
0.013
10 000

1e-05
0.590
0.037
9 900
0.772
0.059
10 000
0.621
0.043
10 000
0.753
0.059
10 000
0.758
0.005
10 000
0.815
0.059
10 000
0.854
0.028
10 000

0.001
0.229
0.023
10 000
0.144
0.016
10 000
0.171
0.020
10 000
0.119
0.016
10 000
0.150
0.004
10 000
0.062
0.009
9 999
0.071
0.009
10 000

0.001
0.885
0.021
10 000
0.935
0.013
10 000
0.927
0.014
10 000
0.960
0.009
10 000
0.905
0.004
10 000
0.962
0.014
9999
0.975
0.003
10 000

GB
0.01
0.098
0.018
9 998
0.141
0.017
2 465
0.086
0.015
9 996
0.114
0.015
3 694
0.141
0.004
9 966
0.043
0.012
4 716
0.061
0.008
3 880

GB
0.01
0.971
0.008
9 998
0.936
0.012
2 465
0.978
0.006
9 996
0.962
0.008
3 694
0.915
0.004
9 966
0.974
0.011
4716
0.980
0.003
3 880

0.1
0.086
0.015
2 619
0.141
0.016
240
0.081
0.014
1 781
0.114
0.016
354
0.138
0.004
6 714
0.043
0.013
471
0.061
0.007
426

0.1
0.976
0.007
2 619
0.936
0.012
240
0.981
0.005
1 781
0.963
0.008
354
0.920
0.004
6 714
0.973
0.012
471
0.980
0.003
426

0.5
0.085
0.016
452
0.142
0.018
41
0.079
0.014
319
0.115
0.016
65
0.138
0.004
1 635
0.043
0.012
87
0.065
0.007
84

0.5
0.977
0.007
452
0.934
0.013
41
0.981
0.005
319
0.961
0.008
65
0.920
0.003
1 635
0.973
0.013
87
0.979
0.003
84

1e-05
0.248
0.023
2 500
0.145
0.017
2 500
0.185
0.022
2 500
0.123
0.018
2 500
0.151
0.004
2 500
0.063
0.008
2 500
0.077
0.009
2 500

1e-05
0.869
0.023
2 500
0.933
0.013
2 500
0.916
0.016
2 500
0.957
0.009
2 500
0.902
0.004
2 500
0.956
0.015
2500
0.973
0.004
2 500

0.001
0.085
0.016
1 404
0.142
0.018
387
0.080
0.014
1 156
0.114
0.016
493
0.140
0.005
1 853
0.043
0.013
568
0.064
0.009
479

0.001
0.977
0.007
1404
0.937
0.013
387
0.981
0.005
1 156
0.962
0.008
493
0.918
0.004
1 853
0.973
0.014
568
0.978
0.003
479

AGB
0.01
0.088
0.016
421
0.141
0.017
121
0.081
0.015
358
0.116
0.016
151
0.140
0.005
610
0.043
0.013
181
0.065
0.007
150

AGB
0.01
0.975
0.007
421
0.936
0.012
121
0.981
0.005
358
0.962
0.008
151
0.917
0.004
610
0.975
0.011
181
0.978
0.003
150

0.1
0.108
0.017
97
0.144
0.016
34
0.095
0.015
88
0.118
0.016
40
0.143
0.004
143
0.044
0.011
50
0.068
0.007
40

0.1
0.964
0.010
97
0.935
0.012
34
0.972
0.008
88
0.960
0.008
40
0.913
0.003
143
0.971
0.015
50
0.977
0.003
40

0.5
0.217
0.036
22
0.155
0.021
12
0.183
0.03
23
0.132
0.020
14
0.152
0.004
24
0.054
0.011
18
0.086
0.011
16

0.5
0.862
0.040
22
0.922
0.015
12
0.898
0.030
23
0.947
0.011
14
0.901
0.004
24
0.950
0.022
18
0.966
0.005
16

Lasso

RF

0.419
0.021

0.138
0.018

0.453
0.025

0.118
0.016

0.155
0.004

0.032
0.007

0.095
0.072

0.515
0.018

0.940
0.011

0.516
0.019

0.960
0.007

0.902
0.004

0.973
0.008

0.970
0.004

0.206
0.025

0.151
0.019

0.134
0.018

0.116
0.018

0.186
0.005

0.031
0.009

0.057
0.007

0.891
0.021

0.922
0.016

0.945
0.012

0.955
0.011

0.858
0.008

0.983
0.008

0.979
0.003

Lasso

RF

Table 4: Mean (m.) and standard deviation (sd.) of AUC for the classiﬁcation problems.
Also shown for the boosting algorithms is the mean over all replications of the optimal
number of components (T (cid:63)) . Results are averaged over 100 independent replications for
simulated examples and over 20 independent permutations of the sample for real-life data
sets. For each data set, the two best performances are in bold.

References

A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences, 2:183–202, 2009.

15

S. Becker, J. Bobin, and E.J. Candès. NESTA: A fast and accurate ﬁrst-order method for

sparse recovery. SIAM Journal on Imaging Sciences, 4:1–39, 2011.

G. Biau and B. Cadre. Optimization by gradient boosting. arXiv:1707.05023, 2017.

G. Biau, A. Fischer, B. Guedj, and J.D. Malley. COBRA: A combined regression strategy.

Journal of Multivariate Analysis, 146:18–28, 2016.

P.J. Bickel, Y. Ritov, and A. Zakai. Some theory for generalized boosting algorithms.

Journal of Machine Learning Research, 7:705–732, 2006.

G. Blanchard, G. Lugosi, and N. Vayatis. On the rate of convergence of regularized boosting

classiﬁers. Journal of Machine Learning Research, 4:861–894, 2003.

L. Breiman. Arcing the edge. Technical Report 486, Statistics Department, University of

California, Berkeley, 1997.

L. Breiman. Arcing classiﬁers (with discussion). The Annals of Statistics, 26:801–824,

L. Breiman. Prediction games and arcing algorithms. Neural Computation, 11:1493–1517,

L. Breiman. Some inﬁnite theory for predictor ensembles. Technical Report 577, Statistics

Department, University of California, Berkeley, 2000.

L. Breiman. Random forests. Machine Learning, 45:5–32, 2001.

L. Breiman. Population theory for boosting ensembles. The Annals of Statistics, 32:1–11,

1998.

1999.

2004.

S. Bubeck. ORF523: Nesterov’s accelerated gradient descent, 2013. URL https://blogs.

princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent.

P. Bühlmann and T. Hothorn. Boosting algorithms: Regularization, prediction and model

ﬁtting (with discussion). Statistical Science, 22:477–505, 2007.

P. Bühlmann and B. Yu. Boosting with the L2 loss: Regression and classiﬁcation. Journal

of the American Statistical Association, 98:324–339, 2003.

T. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of
the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pages 785–794. ACM, New York, 2016.

L. Devroye, L. Györﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition.

Y. Freund. Boosting a weak learning algorithm by majority. Information and Computation,

Springer, New York, 1996.

121:256–285, 1995.

16

Y. Freund and R.E. Schapire. Experiments with a new boosting algorithm. In S. Lorenza,
editor, Machine Learning: Proceedings of the Thirteenth International Conference on
Machine Learning, pages 148–156. Morgan Kaufmann Publishers, San Francisco, 1996.

Y. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and System Sciences, 55:119–139, 1997.

J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: A statistical view of

boosting (with discussion). The Annals of Statistics, 28:337–374, 2000.

J.H. Friedman. Greedy function approximation: A gradient boosting machine. The Annals

of Statistics, 29:1189–1232, 2001.

J.H. Friedman. Stochastic gradient boosting. Computational Statistics & Data Analysis, 38:

367–378, 2002.

T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data

Mining, Inference, and Prediction. Second Edition. Springer, New York, 2009.

G. Lugosi and N. Vayatis. On the Bayes-risk consistency of regularized boosting methods.

The Annals of Statistics, 32:30–55, 2004.

L. Mason, J. Baxter, P. Bartlett, and M. Frean. Boosting algorithms as gradient descent.
In S.A. Solla, T.K. Leen, and K. Müller, editors, Proceedings of the 12th International
Conference on Neural Information Processing Systems, pages 512–518. The MIT Press,
Cambridge, MA, 1999.

L. Mason, J. Baxter, P. Bartlett, and M. Frean. Functional gradient techniques for combining
hypotheses. In A.J. Smola, P.L. Bartlett, B. Schölkopf, and D. Schuurmans, editors,
Advances in Large Margin Classiﬁers, pages 221–246. The MIT Press, Cambridge, MA,
2000.

Y. Nesterov. A method of solving a convex programming problem with convergence rate

O(1/k2). Soviet Mathematics Doklady, 27:372–376, 1983.

Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer

Science+Business Media, New York, 2004.

Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming,

103:127–152, 2005.

Y. Nesterov. Gradient methods for minimizing composite functions. Mathematical Pro-

gramming, 140:125–161, 2013.

G. Qu and N. Li. Accelerated distributed Nesterov gradient descent.

In 54th Annual
Allerton Conference on Communication, Control, and Computing, pages 209–216. Curran
Associates, Inc., Red Hook, 2016.

G. Ridgeway. Generalized boosted models: A guide to the gbm package, 2007. URL

http://www.saedsayad.com/docs/gbm2.pdf.

17

R.E. Schapire. The strength of weak learnability. Machine Learning, 5:197–227, 1990.

W. Su, S. Boyd, and E.J. Candès. A differential equation for modeling Nesterov’s accelerated
gradient method: Theory and insights. Journal of Machine Learning Research, 17:1–43,
2016.

I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and
momentum in deep learning. In S. Dasgupta and D. McAllester, editors, Proceedings of
the 30th International Conference on Machine Learning, pages 1139–1147. Proceedings
of Machine Learning Research, 2013.

R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal

Statistical Society. Series B, 58:267–288, 1996.

P. Tseng. On accelerated proximal gradient methods for convex-concave optimization, 2008.

URL http://www.mit.edu/~dimitrib/PTseng/papers/apgm.pdf.

T. Zhang and B. Yu. Boosting with early stopping: Convergence and consistency. The

Annals of Statistics, 33:1538–1579, 2005.

18

