8
1
0
2
 
l
u
J
 
6
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
0
2
4
0
.
3
0
8
1
:
v
i
X
r
a

Semiparametric Contextual Bandits

Akshay Krishnamurthy ∗2, Zhiwei Steven Wu †2, and Vasilis Syrgkanis ‡3

2Microsoft Research NYC, New York, NY
3Microsoft Research New England, Cambridge, MA

July 17, 2018

Abstract

This paper studies semiparametric contextual bandits, a generalization of the linear stochastic bandit
problem where the reward for an action is modeled as a linear function of known action features confounded
by a non-linear action-independent term. We design new algorithms that achieve ˜O(d
T ) regret over T
rounds, when the linear function is d-dimensional, which matches the best known bounds for the simpler
unconfounded case and improves on a recent result of Greenewald et al. [19]. Via an empirical evaluation,
we show that our algorithms outperform prior approaches when there are non-linear confounding effects on
the rewards. Technically, our algorithms use a new reward estimator inspired by doubly-robust approaches
and our proofs require new concentration inequalities for self-normalized martingales.

√

1 Introduction

A number of applications including online personalization, mobile health, and adaptive clinical trials require
that an agent repeatedly makes decisions based on user or patient information with the goal of optimizing some
metric, typically referred to as a reward. For example, in online personalization problems, we might serve
content based on user history and demographic information with the goal of maximizing user engagement
with our service. Since counterfactual information is typically not available, these problems require algorithms
to carefully balance exploration—making potentially suboptimal decisions to acquire new information—with
exploitation—using collected information to make better decisions. Such problems are often best modeled
with the framework of contextual bandits, which captures the exploration-exploitation tradeoff and enables
rich decision making policies but ignores the long-term temporal effects that make general reinforcement
learning challenging. Contextual bandit algorithms have seen recent success in applications, including news
recommendation [22] and mobile health [34].

Contextual bandit algorithms can be categorized as either parametric or agnostic, depending on whether
they model the relationship between the reward and the decision or not. Parametric approaches typically
assume that the reward is a (generalized) linear function of a known decision-speciﬁc feature vector [17, 11,
1, 4]. Once this function is known to high accuracy, it can be used to make near-optimal decisions. Exploiting

∗akshay@cs.umass.edu
†zsw@umn.edu
‡vasy@microsoft.com

1

this fact, algorithms for this setting focus on learning the parametric model. Unfortunately, fully parametric
assumptions are often unrealistic and challenging to verify in practice, and these algorithms may perform
poorly when the assumptions do not hold.

In contrast, agnostic approaches make no modeling assumptions about the reward and instead compete
with a large class of decision-making policies [21, 3]. While these policies are typically parametrized in
some way, these algorithms provably succeed under weaker conditions and are generally more robust than
parametric ones. On the other hand, they typically have worse statistical guarantees, are conceptually much
more complex, and have high computational overhead, technically requiring solving optimization problems
that are NP-hard in the worst case. This leads us to a natural question:

Is there an algorithm that inherits the simplicity and statistical guarantees of the parametric
methods and the robustness of the agnostic ones?

Working towards an afﬁrmative answer to this question, we consider a semiparametric contextual bandit
setup where the reward is modeled as a linear function of the decision confounded by an additive non-linear
perturbation that is independent of the decision. This setup signiﬁcantly generalizes the standard parametric
one, allowing for complex, non-stationary, and non-linear rewards (See Section 2 for a precise formulation).
On the other hand, since this perturbation is just a baseline reward for all decisions, it has no inﬂuence on the
optimal one, which depends only on the unknown linear function. In the language of econometrics and causal
modeling, the treatment effect is linear.

In this paper, we design new algorithms for the semiparametric contextual bandits problem. When the
linear part of the reward is d-dimensional, our algorithms achieve ˜O(d
T ) regret over T rounds, even when
the features and the confounder are chosen by an adaptive adversary. This guarantee matches the best results
for the simpler linear stochastic bandit problem up to logarithmic terms, showing that there is essentially no
statistical price to pay for robustness to confounding effects. On the other hand, our algorithm and analysis is
quite different, and it is not hard to see that existing algorithms for stochastic bandits fail in our more general
setting. Our regret bound also improves on a recent result of Greenewald et al. [19], who consider the same
setup but study a weaker notion of regret. Our algorithm, main theorem, and comparisons are presented
in Section 3.

√

We also compare our algorithm to approaches from both parametric and agnostic families in an empirical
study (we use a linear policy class for agnostic approaches). In Section 5, we evaluate several algorithms on
synthetic problems where the reward is (a) linear, and (b) linear with confounding. In the linear case, our
approach learns, but is slightly worse than the baselines. On the other hand, when there is confounding, our
algorithm signiﬁcantly outperforms both parametric and agnostic approaches. As such, these experiments
demonstrate that our algorithm represents a favorable trade off between statistical efﬁciency and robustness.
On a technical level, our algorithm and analysis require several new ideas. First, we derive a new estimator
for linear models in the presence of confounders, based on recent and classical work in semiparametric
statistics and econometrics [28, 10]. Second, since standard algorithms using optimism principles fail to
guarantee consistency of this new estimator, we design a new randomized algorithm, which can be viewed
as an adaptation of the action-elimination method of Even-Dar et al. [16] to the contextual bandits setting.
Finally, analyzing the semiparametric estimator requires an intricate deviation argument, for which we derive
a new self-normalized inequality for vector-valued martingales using tools from de la Peña et al. [14, 15].

2 Preliminaries

We study a generalization of the linear stochastic bandit problem with action-dependent features and action-
independent confounder. The learning process proceeds for T rounds, and in round t, the learner receives a

2

context xt (cid:44) {zt,a}a∈A where zt,a ∈ Rd and A is the action set, which we assume to be large but ﬁnite. The
learner then chooses an action at ∈ A and receives reward

rt(at) (cid:44) (cid:104)θ, zt,at(cid:105) + ft(xt) + ξt,

(1)

where θ ∈ Rd is an unknown parameter vector, ft(xt) is a confounding term that depends on the context xt
but, crucially, does not depend on the chosen action at, and ξt is a noise term that is centered and independent
of at.

For each round t, let a(cid:63)
t

(cid:44) argmaxa∈A(cid:104)θ, zt,a(cid:105) denote the optimal action for that round. The goal of our

algorithm is to minimize the regret, deﬁned as

Reg(T ) (cid:44)

rt(a(cid:63)

t ) − rt(at) =

(cid:104)θ, zt,a(cid:63)

t

− zt,at(cid:105).

T
(cid:88)

t=1

T
(cid:88)

t=1

Observe that the noise term ξt, and, more importantly, the confounding term ft(xt) are absent in the ﬁnal
expression, since they are independent of the action choice.

We consider the challenging setting where the context xt and the confounding term ft(·) are chosen by
an adaptive adversary, so they may depend on all information from previous rounds. This is formalized in the
following assumption.

Assumption 1 (Environment). We assume that xt = {zt,a}a∈A, ft, ξt are generated at the beginning of
round t, before at is chosen. We assume that xt and ft are chosen by an adaptive adversary, and that ξt
satisﬁes E[ξt|xt, ft] = 0 and |ξt| ≤ 1.

We also impose mild regularity assumptions on the parameter, the feature vectors, and the confounding

functions.

Assumption 2 (Boundedness). Assume that (cid:107)θ(cid:107)2 ≤ 1 and that (cid:107)zt,a(cid:107)2 ≤ 1 for all a ∈ A, t ∈ [T ]. Further
assume that ft(·) ∈ [−1, 1] for all t ∈ [T ].

For simplicity, we assume an upper bound of 1 in these conditions, but our algorithm and analysis can be

adapted to more generic regularity conditions.

Related work. Our setting is related to linear stochastic bandits and several variations that have been
studied in recent years. Among these, the closest is the work of Greenewald et al. [19] who consider the
same setup and provide a Thompson Sampling algorithm using a new reward estimator that eliminates the
confounding term. Motivated by applications in medical intervention, they consider a different notion of
regret from our more-standard notion and, as such, the results are somewhat incomparable. For our notion
of regret, their analysis can produce a T 2/3-style regret bound, which is worse than our optimal
T bound.
See Section 3.3 for a more detailed comparison.

√

Other results for linear stochastic bandits include upper-conﬁdence bound algorithms [29, 11, 1], Thomp-
son sampling algorithms [4, 30], and extensions to generalized linear models [17, 23]. However, none of
these models accommodate arbitrary and non-linear confounding effects. Moreover, apart from Thompson
sampling, all of these algorithms use deterministic action-selection policies (conditioning on the history),
which provably incurs Ω(T ) regret in our setting, as we will see.

One can accommodate confounded rewards via an agnostic-learning approach to contextual bandits [5,
21, 3]. In this framework, we make no assumptions about the reward, but rather compete with a class
of parameterized policies (or experts). Since a d-dimensional linear policy is optimal in our setting, an

3

agnostic algorithm with a linear policy class addresses precisely our notion of regret. However there are two
disadvantages. First, agnostic algorithms are all computationally intractable, either because they enumerate
the (inﬁnitely large) policy class, or because they assume access to optimization oracles that can solve NP-hard
K, the
problems in the worst case. Second, most agnostic approaches have regret bounds that grow with
number of actions, while our bound is completely independent of K.

√

We are aware of one approach that is independent of K, but it requires enumeration of an inﬁnitely
large policy class. This method is based on ideas from the adversarial linear and combinatorial bandits
(cid:44) (zt,a, 1) ∈ Rd+1, our setting can
literature [13, 2, 8, 9]. Writing θt (cid:44) (θ, ft(xt)) ∈ Rd+1 and z(cid:48)
be re-formulated in the adversarial linear bandits framework. However, standard linear bandit algorithms
compete with the best ﬁxed action vector in hindsight, rather than the best policy with time-varying action
sets. To resolve this, one can use the linear bandits reward estimator [32] in a contextual bandit algorithm
like EXP4 [5], but this approach is not computationally tractable with the linear policy class. For our setting,
we are not aware of any computationally efﬁcient approaches, even oracle-based approaches, that achieve
poly(d)

T regret with no dependence on the number of actions.

√

t,a

We resolve the challenge of confounded rewards with an estimator from the semiparametric statistics
literature [35], which focuses on estimating functionals of a nonparametric model. Most estimators are based
on Neyman Orthogonalization [24], which uses moment equations that are insensitive to nuisance parameters
in a method-of-moments approach [10]. These orthogonal moments typically involve a linear correction
to an initial nonparametric estimate using so-called inﬂuence functions [7, 27]. Robinson [28] used this
approach for the ofﬂine version of our setting (known as the partially linear regression (PLR) model) where
he demonstrated a form of double-robustness [26] to poor estimation of the nuisance term (in our case ft(xt)).
We generalize Robinson’s work to the online setting, showing how orthogonalized estimators can be used for
adaptive exploration. This requires several new techniques, including a novel action selection policy and a
self-normalized inequality for vector-valued martingales.

3 Algorithm and Results

In this section, we describe our algorithm and present our main theoretical result, an ˜O(d
for the semiparametric contextual bandits problem.

√

T ) regret bound

3.1 A Lower Bound

Before turning to the algorithm, we ﬁrst present a lower bound against deterministic algorithms. Since the
functions ft may be chosen by an adaptive adversary, it is not hard to show that this setup immediately
precludes the use of deterministic algorithms.

Proposition 3. Consider an algorithm that, at round t, chooses an action at as a deterministic function of
the observable history Ht (cid:44) {x1:t, a1:t−1, r1:t−1}. There exists a semiparametric contextual bandit instance
with d = 2 and K = 2 where the regret of the algorithm is at least T /2.

See Appendix B for the proof, which resembles the standard argument against deterministic online
learning algorithms [12]. The main difference is that the adversary uses the confounding term to corrupt
the information that the learner receives, whereas, in the standard proof, the adversary chooses the optimal
action in response to the learner. In fact, deterministic algorithms can succeed in the full information version
of our setting, since taking differences between rewards eliminates the confounder. Thus, bandit feedback
plays a crucial role in our construction and the bandit setting is considerably more challenging than the full
information analogue.

4

Algorithm 1: BOSE (Bandit orthogonalized semiparametric estimation)

Input :T, δ ∈ (0, 1).

1 Set λ ← 4d log(9T ) + 8 log(4T /δ) and γ(T ) ←
2 Initialize ˆθ ← 0 ∈ Rd, Γ ← λId×d.
3 for t = 1, . . . , T do
4

Observe xt = {zt,a}a∈A
Filter

√

λ + (cid:112)27d log(1 + 2T /d) + 54 log(4T /δ).

5

6

7

8

At ←

(cid:110)
a ∈ A | ∀b ∈ A, (cid:104)ˆθ, zt,b − zt,a(cid:105) ≤ γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

(cid:111)

.

Find distribution πt ∈ ∆(At) such that ∀a ∈ At (We use
Covb∼πt(zt,b) (cid:44) E[zt,bz(cid:62)

t,b] − (Ezt,b)(Ezt,b)(cid:62).)
(cid:107)zt,a − Eb∼πtzt,b(cid:107)2

Γ−1 ≤ tr(Γ−1 Cov
b∼πt

(zt,b)).

Sample at ∼ πt and play at. Observe rt(at). (rt(at) = (cid:104)θ, zt,at(cid:105) + ft(xt) + ξt.)
Let µt = Ea∼πt[zt,a | xt] and update parameters

(2)

(3)

Γ ← Γ + (zt,at − µt)(zt,at − µt)(cid:62),

ˆθ ← Γ−1

(zτ,aτ − µτ )rτ (aτ ).

(4)

t
(cid:88)

τ =1

We emphasize that, except for the Thompson Sampling approach [4], essentially all algorithms for the
linear stochastic bandit problem use deterministic strategies, so they provably fail in the semiparametric
setting. As we mentioned, Thompson Sampling was analyzed in our setting by Greenewald et al. [19], but
T -type regret bound (See Section 3.3 for a more quantitative and detailed
they do not obtain the optimal
comparison). In contrast, our algorithm is quite different from all of these approaches; it ensures enough
randomization to circumvent the lower bound and also achieves the optimal

√

√

√

T regret.
T ) lower bound for linear stochastic bandits [13],

To conclude this discussion, we remark that the Ω(d

which also applies to randomized algorithms, holds in our more general setting as well.

3.2 The Algorithm

Pseudocode for the algorithm, which we call BOSE, for “Bandit Orthogonalized Semiparametric Estimation,"
is displayed in Algorithm 1. The algorithm maintains an estimate ˆθ for the true parameter θ, which it uses in
each round to select an action via two steps: (1) an action elimination step that removes suboptimal actions,
and (2) an optimization step that ﬁnds a good distribution over the surviving actions. The algorithm then
samples and plays an action from this distribution and uses the observed reward to update the parameter
estimate ˆθ. This parameter estimation step is the third main element of the algorithm. We now describe each
of these three components in detail.

Parameter estimation. For simplicity, we use zt (cid:44) zt,at to denote the feature vector for the action that
was chosen at round t, and similarly we use rt (cid:44) rt(at). Using all previously collected data, speciﬁcally
{zτ , rτ }t
τ =1 at the end of round t, we would like to estimate the parameter θ. First, if fτ (xτ ) were identically

5

zero, by exploiting the linear parametrization we could use ridge regression, which with some λ > 0 gives

(cid:32)

(cid:33)−1 t

ˆθRidge (cid:44)

λI +

zτ z(cid:62)
τ

t
(cid:88)

τ =1

(cid:88)

τ =1

zτ rτ .

This estimator appears in most prior approaches for linear stochastic bandits [29, 11, 1]. Unfortunately,
since fτ (xτ ) is non-zero, ˆθRidge has non-trivial and non-vanishing bias, so even in benign settings it is not a
consistent estimator for θ.1

Our approach to eliminating the bias from the confounding term fτ (xτ ) is to center the feature vectors
zτ . Intuitively, in the ridge estimator, if zτ is centered, then zτ (rτ − (cid:104)θ(cid:63), zτ (cid:105)) is mean zero, even when
there is non-negligible bias in the second term. As such, the error of the corresponding estimator can be
expected to concentrate around zero. In the semiparametric statistics literature, this is known as Neyman
Orthogonalization [24], which was analyzed in the context of linear regression by Robinson [28] and in a
more general setting by Chernozhukov et al. [10].

To center the feature vector, we will, at round t, choose action at by sampling from some distribution
πt ∈ ∆(A). Let µt (cid:44) Eat∼πt[zt,at|xt] denote the mean feature vector, taking expectation only over our
random action choice. With this notation, the orthogonalized estimator is

Γ = λI +

(zτ − µτ )(zτ − µτ )(cid:62),

ˆθ = Γ−1

(zτ − µτ )rτ .

t
(cid:88)

τ =1

t
(cid:88)

τ =1

ˆθ is a Ridge regression version of Robinson’s classical semiparametric regression estimator [28]. The
estimator was originally derived for observational studies where one might not know the propensities µτ
exactly, and the standard description involves estimates ˆfτ and ˆµτ for the confounding term fτ and the
propensities µτ respectively. Informally, the estimator achieves a form of double-robustness, in the sense
that it is accurate if either of these auxilliary estimators are. In our case, since we know the propensities
µτ exactly, we can use an inconsistent estimator for the confounding term, so we simply set ˆfτ (xτ ) ≡ 0.
In Lemma 5, we prove a precise ﬁnite sample concentration inequality for this orthogonalized estimator,
showing that the confounding term ft(xt) does not introduce any bias. While the estimator has been studied
in prior works [28], to our knowledge, our error guarantee is novel.

The convergence rate of the orthogonalized estimator depends on the eigenvalues of the matrix Γ, and
we must carefully select actions to ensure these eigenvalues are sufﬁciently large. To see why, notice that
any deterministic action-selection approach with the orthogonalized estimator (including conﬁdence based
approaches), will fail, since zt = µt, so the eigenvalues of Γ do not grow rapidly and in fact the estimator is
identically 0. This argument motivates our new action selection scheme which ensure substantial conditional
covariance.

Action selection. Our action selection procedure has two main elements. First using our estimate ˆθ, we
eliminate any action that is provably suboptimal. Based on our analysis for the estimator ˆθ, at round t, we can
certify action a is suboptimal, if we can ﬁnd another action b such that

(cid:104)ˆθ, zt,b − zt,a(cid:105) > γ(T )(cid:107)zt,b − zt,a(cid:107)Γ−1.

1A related estimator can be used to evaluate the reward of a policy, as in linear and combinatorial bandits [9], but to achieve
adequate exploration, one must operate over the policy class, which leads to computational intractability. We would like to use ˆθ to drive
exploration, and this seems to require a consistent estimator. See Appendix A for a simple example demonstrating how using a biased
estimator in a conﬁdence-based approach results in linear regret.

6

Here γ(T ) is the constant speciﬁed in the algorithm, and (cid:107)x(cid:107)M (cid:44)
x(cid:62)M x denotes the Mahalanobis norm.
Using our conﬁdence bound for ˆθ in Lemma 5 below, this inequality certiﬁes that action b has higher expected
reward than action a, so we can safely eliminate a from consideration.

The next component is to ﬁnd a distribution over the surviving actions, denoted A(cid:48)

t at round t, with
t) that we use is the solution to the following feasibility

sufﬁcient covariance. The distribution πt ∈ ∆(A(cid:48)
problem

√

∀a ∈ A(cid:48)
t,

(cid:107)zt,a − Eb∼πtzt,b(cid:107)2

Γ−1 ≤ tr(Γ−1 Cov
b∼πt

(zt,b)).

For intuition, the left hand side of the constraint for action a is an upper bound on the expected regret if a is
the optimal action on this round. Thus, the constraints ensure that the regret is related to the covariance of the
distribution, which means that if we incur high regret, the covariance term Covb∼πt(zt,b) will be large. Since
we use a sample from πt to update our parameter estimate, this means that whenever the instantaneous regret
is large, we must learn substantially about the parameter. In this way, the distribution πt balances exploration
and exploitation. We will see in Lemma 8 that this program is convex and always has a feasible solution.

Our action selection scheme bears some resemblance to action-elimination approaches that have been
studied in various bandit settings [16]. The main differences are that we adapt these ideas to the contextual
setting and carefully choose a distribution over the surviving actions to balance exploration and exploitation.

3.3 The Main Result

We now turn to the main result, a regret guarantee for BOSE.

Theorem 4. Consider the semiparametric contextual bandit problem under Assumption 1 and Assumption 2.
T log(T /δ)).
For any parameter δ ∈ (0, 1), with probability at least 1−δ, Algorithm 1 has regret at most O(d

√

The constants, and indeed a bound depending on λ and γ(T ) can be extracted from the proof, provided in

the appendix. To interpret the regret bound, it is worth comparing with several related results:

√

Comparison with linear stochastic bandits. While most algorithms for linear stochastic bandits prov-
ably fail in our setting (via Proposition 3), the best regret bounds here are O((cid:112)dT log(T K/δ)) [11] and
T log(T ) + (cid:112)dT log(T ) log(1/δ)) [1] depending on whether we assume that the number of actions
O(d
K is small or not. This latter result is optimal when the number of actions is large [13], which is the setting
we are considering here. Since our bound matches this optimal regret up to logarithmic factors, and since
linear stochastic bandits are a special case of our semiparametric setting, our result is therefore also optimal
up to logarithmic factors. An interesting open question is whether an ˜O((cid:112)dT log(K/δ)) regret bound is
achievable in the semiparametric setting.

Comparison with agnostic contextual bandits. The best oracle-based agnostic approaches achieve ˜O(
regret [3], incurring a polynomial dependence on the number of actions K, although there is one inefﬁcient
method that can achieve ˜O(d
T ),2 as we discussed previously. To date, all efﬁcient methods in the agnostic
setting require some form of i.i.d. [3] or transductive assumption [33, 25] on the contexts, which we do not
assume here.

√

√

dKT )

2This follows easily by combining ideas from Auer et al. [5] and Cesa-Bianchi and Lugosi [9].

7

Comparison with Greenewald et al. [19]. Greenewald et al. [19] consider a very similar setting to ours,
where rewards are linear with confounding, but where one default action a0 always has zt,a0 ≡ 0 ∈ Rd.
Applications in mobile health motivate a restriction that the algorithm choose the a0 action with probability
∈ [p, 1 − p] for some small p ∈ (0, 1). Their work also introduces a new notion of regret where they compete
with the policy that also satisﬁes this constraint but otherwise chooses the optimal action a(cid:63)
t . In this setup,
they obtain an ˜O(d2

T ) regret bound, which has a worse dimension dependence than Theorem 4.

√

While the setup is somewhat different, we can still translate our result into a regret bound in their setting,
since BOSE can support the probability constraint, and by coupling the randomness between BOSE and
the optimal policy, the regret is unaffected.3 On the other hand, since the constant in their regret bound
scales with 1/p, their results as stated are vacuous when p = 0 which is precisely our setting. For our more
challenging regret deﬁnition, their analysis can produce a suboptimal T 2/3-style regret bound, and in this
sense, Theorem 4 provides a quantitative improvement.

Summary. BOSE achieves essentially the same regret bound as the best linear stochastic bandit methods,
but in a much more general setting. On the other hand, the agnostic methods succeed under even weaker
assumptions, but have worse regret guarantees and/or are computationally intractable. Thus, BOSE broadens
the scope for computationally efﬁcient contextual bandit learning.

4 Proof Sketch

In the two arm case, one should set γ(T ) (cid:44)

We sketch the proof of Theorem 4 in the two-action case (|A| = 2), which has a much simpler proof that
preserves the main ideas. The technical machinery needed for the general case is much more sophisticated,
and we brieﬂy describe some of these steps at the end of this section, with a complete proof in the Appendix.
λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ) and λ = O(1),
which differs slightly from the algorithm pseudocode for the more general case. Additionally, note that with
two actions, the uniform distribution over At is always feasible for Problem (3). Speciﬁcally, if the ﬁltered
set has cardinality 1, we simply play that action deterministically, otherwise we play one of the two actions
uniformly at random.

√

The proof has three main steps. First we analyze the orthogonalized regression estimator deﬁned in (4).
Second, we study the action selection mechanism and relate the regret incurred to the error bound for the
orthogonalized estimator. Finally, using a somewhat standard potential argument, we show how this leads to
T -type regret bound. For the proof, let ˆθt, Γt be the estimator and covariance matrix used on round t,
a
both based on t − 1 samples.

√

For the estimator, we prove the following lemma for the two action case. The main technical ingredient
is a self-normalized inequality for vector-valued martingales, which can be obtained using ideas from de la
Peña et al. [15].

Lemma 5. Under Assumption 1 and Assumption 2, let K = 2 and γ(T ) (cid:44)
Then, with probability at least 1 − δ, the following holds simultaneously for all t ∈ [T ]:

√

λ+(cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ).

3Technically it is actually smaller by a factor of (1 − p).

(cid:107)ˆθt − θ(cid:107)Γt ≤ γ(T ).

8

Proof. Using the deﬁnitions and Assumption 1, it is not hard to re-write

ˆθt = Γ−1

t (Γt − λI)θ + Γ−1

t

Zτ ζτ ,

t−1
(cid:88)

τ =1

where Zτ (cid:44) zτ,aτ − µτ and ζτ (cid:44) (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ . Further deﬁne St (cid:44) (cid:80)t−1
the triangle inequality the error is at most

τ =1 Zτ ζτ . Then, applying

(cid:107)ˆθt − θ(cid:107)Γt ≤ (cid:107)λθ(cid:107)Γ−1

t

+ (cid:107)St(cid:107)Γ−1

.

t

√

λ since Γt (cid:23) λI. To control the second term, we need to use a self-normalized
The ﬁrst term here is at most
concentration inequality, since Zτ is a random variable, and the normalizing term Γt = λI + (cid:80)t−1
τ =1 Zτ Z (cid:62)
τ
depends on the random realizations. In Lemma 10 in the appendix, we prove that with probability at least
1 − δ, for all t ∈ [T ]

(cid:107)St(cid:107)2

Γ−1
t

≤ 9d log(1 + T /(dλ)) + 18 log(T /δ).

(5)

The lemma follows from straightforward calculations.

Before proceeding, it is worth commenting on the difference between our self-normalized inequality (5)
and a slightly different one used by Abbasi-Yadkori et al. [1] for the linear case. In their setup, they have
that ζτ is conditionally centered and sub-Gaussian, which simpliﬁes the argument since after ﬁxing the Zτ s
(and hence Γt), the randomness in the ζτ s sufﬁces to provide concentration. In our case, we must use the
randomness in Zτ itself, which is more delicate, since Zτ affects the numerator St, but also the normalizer
Γt. In spite of this additional technical challenge, the two self-normalized processes admit similar bounds.

Next, we turn to the action selection step, where recall that either a single action is played deterministically,

or the actions are played uniformly at random.
Lemma 6. Let µt (cid:44) Ea∼πtzt,a where πt is the solution to (3), and assume that the conclusion of Lemma 5
holds. Then with probability at least 1 − δ

Reg(T ) ≤ (cid:112)2T log(1/δ) + 2γ(T )

T
(cid:88)

(cid:114)

t=1

tr(Γ−1

t Cov
b∼πt

(zt,b)).

Proof. We ﬁrst study the instantaneous regret, taking expectation over the random action. For this, we must
consider two cases. First, with Lemma 5, if |At| = 1, we argue that the regret is actually zero. This follows
from the Cauchy-Schwarz inequality since assuming At = {a} we get

(cid:104)θ, zt,a − zt,b(cid:105) ≥ (cid:104)ˆθt, zt,a − zt,b(cid:105) − γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

t

which is non-negative using the fact that b was eliminated. Therefore a is the optimal action and we incur no
regret. Since πt has no covariance, the upper bound holds.

On the other rounds, we set πt = Unif({a, b}) and hence µt = (zt,a + zt,b)/2. Assuming again that a is

the optimal action, the expected regret is

(cid:104)θ, zt,a − µt(cid:105) =

(cid:104)θ, zt,a − zt,b(cid:105) ≤

1
2

(cid:104)ˆθt, zt,a − zt,b(cid:105) + γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

t

(cid:17)

≤ γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

≤ 2γ(T )

t

t Cov
b∼πt

(zt,b)).

(cid:16)

1
2
(cid:114)
tr(Γ−1

9

Here the ﬁrst inequality uses Cauchy-Schwarz, the second uses (2), since neither action was eliminated, and
the third uses (3). This bounds the expected regret, and the lemma follows by Azuma’s inequality.

The last step of the proof is to control the sequence

T
(cid:88)

(cid:114)

t=1

tr(Γ−1

t Cov
b∼πt

(zt,b)).

First, recall that

(zt,b) (cid:44) Eb∼πt

(cid:2)(zt,b − µt)(zt,b − µt)(cid:62)(cid:3)

Cov
b∼πt

with µt (cid:44) Eb∼πt[zt,b]. Since in the two-arm case πt either chooses an arm deterministically or uniformly
randomizes between the two arms, the following always holds:

(zt,b) = (zt,at − µt)(zt,at − µt)(cid:62).

Cov
b∼πt

It follows that Γt+1 (cid:44) Γt + Covb∼πt (zt,b), and with Γ1 (cid:44) λI, the standard potential argument for online
ridge regression applies. We state the conclusion here, and provide a complete proof in the appendix.

Lemma 7. Let Γt, πt be deﬁned as above and deﬁne Mt (cid:44) (zt,at − µt)(zt,at − µt)(cid:62). Then

T
(cid:88)

(cid:113)

t=1

tr(Γ−1

t Mt) ≤ (cid:112)dT (1 + 1/λ) log(1 + T /(dλ)).

Combining the three lemmas establishes a regret bound of

Reg(T ) ≤ O

√
(cid:16)(cid:112)T d log(T /δ) log(T /d) + d

T log(T /d)

(cid:17)

with probability at least 1 − δ in the two-action case.

Extending to many actions. Several more technical steps are required for the general setting. First, the
martingale inequality used in Lemma 5 requires that the random vectors are symmetric about the origin.
This is only true for the two-action case, and in fact a similar inequality does not hold in general for the
non-symmetric situation that arises with more actions. In the non-symmetric case, both the empirical and the
population covariance must be used in the normalization, so the analogue of (5) is instead

(cid:107)St(cid:107)2

(Γt+EΓt)−1 ≤ 27d log(1 + 2T /d) + 54 log(4T /δ).

On the other hand, the error term for our estimator depends only on the empirical covariance Γt. To correct
for the discrepancy, we use a covering argument4 to establish

λI + Γt (cid:23) (λ − 6d log(T /δ))I + (Γt + EΓt)/3.

With this semideﬁnite inequality, we can translate from the Mahalanobis norm in the weaker self-normalized
bound to one with just Γt, which controls the error for the estimator.

We also argue that problem (3) is always feasible, which is the contents of the following lemma.

4For technical reasons, the Matrix Bernstein inequality does not sufﬁce here since it introduces a dependence on the maximal variance.

See Appendix for details.

10

Figure 1: Synthetic experiments with d = 10, K = 2. Left: A linear environment where action-features are
uniformly from the unit sphere. Center: A confounded environment with features from the sphere. Right: A
confounded environment with features from the sphere intersected with the positive orthant. Algorithms are
BOSE, OFUL [1], ILTCB [3], EPSGREEDY [21], and THOMPSON [4]. Agnostic approaches use a linear
policy class.

Lemma 8. Problem (3) is convex and always has a feasible solution. Speciﬁcally, for any vectors z1, . . . , zn ∈
Rd and any positive deﬁnite matrix M , there exists a distribution w ∈ ∆([n]) with mean µw (cid:44) Eb∼wzb such
that

∀i ∈ [n], (cid:107)zi − µw(cid:107)2

M ≤ tr(M Cov
b∼w

(zb)).

The proof uses convex duality. Integrating these new arguments into the proof for the two-action case

leads to Theorem 4.

5 Experiments

We conduct a simple experiment to compare BOSE with several other approaches5. We simulate three different
environments that follow the semiparametric contextual bandits model with d = 10, K = 2. In the ﬁrst
setting the reward is linear and the action features are drawn uniformly from the unit sphere. In the latter two
settings, we set ft(xt) = − maxa(cid:104)θ, zt,a(cid:105), which is related to the construction in the proof of Proposition 3.
One of these semiparametric settings has action features sampled from the unit sphere, while for the other, we
sample from the intersection of the unit sphere and the positive orthant.

In Figure 1, we plot the performance of Algorithm 1 against four baseline algorithms: (1) OFUL:
the optimistic algorithm for linear stochastic bandits [1], (2) THOMPSON sampling for linear contextual
bandits [4], (3) EPSGREEDY: the (cid:15)-greedy approach [21] with a linear policy class, (4) ILTCB: a more
sophisticated agnostic algorithm [3] with linear policy class. The ﬁrst algorithm is deterministic, so can have
linear regret in our setting, but is the natural baseline and one we hope to improve. Thompson Sampling is
another natural baseline, and a variant was used by Greenewald et al. [19] in essentially the same setting
as ours. The latter two have (Kd)1/3T 2/3 and
KdT regret bounds respectively under our assumptions,
but require solving cost-sensitive classiﬁcation problems, which are NP-hard in general. Following prior
empirical evaluations [20], we use a surrogate loss formulation based on square loss minimization in the
implementation.

√

The results of the experiment are displayed in Figure 1, where we plot the cumulative regret against the
number of rounds T . All algorithms have a single parameter that governs the degree of exploration. In BOSE

5Our code is publicly available at http://github.com/akshaykr/oracle_cb/.

11

and OFUL, this is the constant γ(T ) in the conﬁdence bound, in THOMPSON it is the variance of the prior,
and in ILTCB and EPSGREEDY it is the amount of uniform exploration performed by the algorithm. For
each algorithm we perform 10 replicates for each of 20 values of the corresponding parameter, and we plot
the best average performance, with error bars corresponding to ±2 standard deviations.

In the linear experiment (Figure 1, left panel), BOSE performs the worst, but is competitive with the
agnostic approaches, demonstrating a price to pay for robustness. The experimental setup in the center panel
is identical except with confounding, and BOSE is robust to this confounding, with essentially the same
performance, while the three baselines degrade dramatically. Finally, when the features lie in the positive
orthant (right panel), OFUL degrades further, while BOSE remains highly effective.

Regarding the baselines, we make two remarks:

1. Intuitively, the positive orthant setting is more challenging for OFUL since there is less inherent

randomness in the environment to overcome the confounding effect.

2. The agnostic approaches, despite strong regret guarantees, perform somewhat poorly in our experiments,
and we believe this for three reasons. First, our surrogate-loss implementation is based on an implicit
realizability assumption, which is not satisﬁed here. Second, we expect that the constant factors in their
regret bounds are signiﬁcantly larger than those of BOSE or OFUL. For computational reasons, we
only solve the optimization problem in ILTCB every 50 rounds, which causes a further constant factor
increase in the regret.

Overall, while BOSE is worse than other approaches in the linear environment, the experiment demonstrates
that when the environment is not perfectly linear, approaches based on realizability assumptions (either
explicitly like in OFUL, or implicitly like in implementations of ILTCB and EPSGREEDY), can fail. We
emphasize that linear environments are rare in practice, and such assumptions are typically impossible to
verify. We therefore believe that trading off a small loss in performance in the specialized linear case for
signiﬁcantly more robustness, as BOSE demonstrates, is desirable.

6 Discussion

This paper studies a generalization of the linear stochastic bandits setting, where rewards are confounded
by an adaptive adversary. Our new algorithm, BOSE, achieves the optimal regret, and also matches (up to
logarithmic factors) the best algorithms for the linear case. Our empirical evaluation shows that BOSE offers
signiﬁcantly more robustness than prior approaches, and performs well in several environments.

12

A Using the OLS Estimator

Here we construct an example problem to demonstrate how using the standard OLS estimator can fail in
the semiparametric setting. While not a comprehensive proof against all asymptotically biased approaches,
similar examples can be constructed for related estimators.

Consider a two-dimensional problem with two actions and no stochastic noise, where θ = e2, the second
standard basis vector. On the even rounds, the actions are z1 = (1, 1), z2 = (1, 1/3) and the confounding
term is f = −1. On the odd rounds, the actions are z1 = z2 = (1, 0) and the confounding term is f = 1. For
any policy for selecting actions, the OLS estimator before round t (for even t) is the solution to the following
optimization problem:

minimizew∈R2 α(w1 + w2)2 + (1 − α)(w1 + w2/3 + 2/3)2 + (w1 − 1)2 = L(w)

where α ∈ [0, 1] corresponds to the fraction of the even rounds (up to round t) where the policy chose z1.
We will argue that, for any α, the solution to this problem ˆw has ˆw2 < 0. Since there is no stochastic noise,
there is no need for conﬁdence bounds once the covariance is full rank, which happens after the second round.
Together, this implies that any sensible policy based on ˆw will prefer z2 to z1 on the even rounds, but z1 yields
higher reward by a ﬁxed constant. Thus using OLS in a conﬁdence-based approach leads to linear regret.

We now show that ˆw2 is strictly negative. We have

∂L(w)
∂w1
∂L(w)
∂w2

= 2α(w1 + w2) + 2(1 − α)(w1 + w2/3 + 2/3) + 2(w1 − 1),

= 2α(w1 + w2) +

(1 − α)(w1 + w2/3 + 2/3).

2
3

Setting both equations equal to zero yields the following system:

4w1 + (2/3 + 4α/3)w2 = 2/3 + 4α/3,

(2/3 + 4α/3)w1 + (2/9 + 16α/9)w2 = 4α/9 − 4/9.

The solution to this system is

w1 =

(2α + 1)2
−4α2 + 12α + 1

,

w2 =

4α2 + 5
4α2 − 12α − 1

,

provided that 4α2 (cid:54)= 12α + 1, which is not possible with α ∈ [0, 1]. In the interval [0, 1] we have that
4α2 − 12α − 1 < 0, and hence w2 < 0. Thus, the OLS estimator incorrectly predicts that z2 receives higher
reward than z1 on the even rounds. Since conﬁdence intervals are not needed, the algorithm suffers linear
reget.

B Proof of Proposition 3

We consider two possible values for the true parameter: θ1 = e1 ∈ R2, θ2 = e2 ∈ R2. At all rounds, the
context xt = {e1, e2} contains just two actions, and we further assume that the noise term ξt = 0 almost
surely. Since the action at is a deterministic function of the history, it can also be computed by the adaptive
adversary at the beginning of the round, and the adversary chooses

ft(xt) = −1{at = argmax

(cid:104)θ, zt,a(cid:105)}.

a

13

We show that rt(at) = 0 for all rounds t. Assume the parameter is θ1 so the optimal action is a(cid:63)
t = e1 and
the suboptimal action e2 has (cid:104)θ, e2(cid:105) = 0. If the learner chooses action e2, then the adversary sets ft(xt) = 0,
so rt(at) = 0. On the other hand, if the learner chooses action e1, then the adversary sets ft(xt) = −1 so
the reward is also zero. Similarly, if θ = θ2, the observed reward is always zero. Since the algorithm is
deterministic, it behaves identically regardless of whether the parameter is θ1 or θ2. In one of these instances
the algorithm must choose the suboptimal action at least T /2 times, leading to the lower bound.

C Proof for the Two-Action Case

We ﬁrst focus on the simpler two action case. Before turning to the main analysis, we prove two supporting
lemmas. The ﬁrst is an algebraic inequality relating matrix determinants to traces. This inequality also
appears in Abbasi-Yadkori et al. [1].

Lemma 9. Let X1, . . . , Xn denote vectors in Rd with (cid:107)Xi(cid:107)2 ≤ L for all i ∈ [n]. Deﬁne Γ (cid:44) λI +
(cid:80)n

i=1 XiX (cid:62)

i . Then

Proof. We will apply the following standard argument:

det(Γ) ≤ (λ + nL2/d)d.

det(Γ)1/d ≤

tr(Γ) =

tr(λI) +

tr(XiX (cid:62)

i ) = λ +

(cid:107)Xi(cid:107)2

2 ≤ λ + nL2/d.

1
d

1
d

1
d

n
(cid:88)

i=1

1
d

n
(cid:88)

i=1

The ﬁrst step is a spectral version of the AM-GM inequality and the remaining steps use linearity of the trace
operator and the boundedness conditions.

The second lemma is a new self-normalized concentration inequality for vector valued martingales.

Lemma 10 (Symmetric self-normalized inequality). Let {Ft}T
t=1 be a
stochastic process with Zt ∈ Rd and ζt ∈ R such that (1) (Zt, ζt) is Ft measurable, (2) |ζt| ≤ M for all
t ∈ [T ], (3) Zt ⊥⊥ ζt|Ft, (4) E[Zt|Ft] = 0, and (5) for all x ∈ Rd, L((cid:104)x, Zt(cid:105) | Ft) = L(−(cid:104)x, Zt(cid:105) | Ft)
where L denotes the probability law, so that Zt is conditionally symmetric. Let Σ (cid:44) (cid:80)T
t . Then for
any positive deﬁnite matrix Q we have

t=1 be a ﬁltration and let {(Zt, ζt)}T

t=1 ZtZ (cid:62)

P





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

Ztζt

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(Q+M 2Σ)−1

≥ 2 log

(cid:32)

(cid:115)

1
δ

det(Q + M 2Σ)
det(Q)

(cid:33)

 ≤ δ.

Proof. The proof follows the recipe in de la Peña et al. [15] (See also de la Peña et al. [14] for a more
comprehensive treatment including the univariate case). We start by applying the Chernoff method. Let
¯Σ (cid:44) Q + M 2Σ. We can write

P





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯Σ−1

Ztζt

≥ 2 log

(cid:32)

(cid:115)

1
δ

det( ¯Σ)
det(Q)

(cid:33)





 = P

exp



1
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

Ztζt



exp



(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯Σ−1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
2

T
(cid:88)

t=1



 ≥

(cid:115)





det( ¯Σ)
det(Q)

1
δ

Ztζt







 .

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯Σ−1



(cid:115)

≤ δE



det(Q)
det( ¯Σ)

14

Therefore, if we prove that this latter expectation is at most one, we will arrive at the conclusion. A similar
statement appears in Theorem 1 of de la Peña et al. [15], but our process is slightly different due to the
presence of ζt. To bound this latter expectation, ﬁx some λ ∈ Rd and consider an exponentiated process with
the increments

(cid:18)

Dλ
t

(cid:44) exp

(cid:104)λ, Ztζt(cid:105) −

M 2(cid:104)λ, Zt(cid:105)2
2

(cid:19)

.

Observe that E[Dλ

t |Ft] ≤ 1 since by the conditional symmetry of Zt, we have

E[Dλ

t |Ft] = E (cid:2)E (cid:2)Dλ

t | Ft, ζt

(cid:20)

(cid:20)

E

exp

= E

(cid:20)

(cid:20)

E

exp

= E

(cid:20)

(cid:20)

E

exp

≤ E

(cid:3)

(cid:3) | Ft
(cid:18) −M 2(cid:104)λ, Zt(cid:105)2
2
(cid:18) −M 2(cid:104)λ, Zt(cid:105)2
2
(cid:18) −M 2(cid:104)λ, Zt(cid:105)2
2

1
2

(cid:19)

(cid:19)

× cosh((cid:104)λ, Ztζt) | Ft, ζt

| Ft

(cid:21)

(cid:21)

(cid:19)

+

(cid:104)λ, Ztζt(cid:105)2
2

(cid:21)

(cid:21)

| Ft, ζt

| Ft

≤ 1.

×

(exp((cid:104)λ, Ztζt(cid:105)) + exp(−(cid:104)λ, Ztζt(cid:105)) | Ft, ζt

| Ft

(cid:21)

(cid:21)

This argument ﬁrst uses the conditional symmetry of Zt and the conditional independence of Zt, ζt, then the
identity (ex + e−x)/2 = cosh(x) and ﬁnally the analytical inequality cosh(x) ≤ ex2/2. Finally in the last
step we use the bound |ζt| ≤ M . This implies that the martingale U λ
τ is a super-martingale with
t
E[U λ

t ] ≤ 1 for all t, since by induction

τ =1 Dλ

(cid:44) (cid:81)t

E[U λ

t ] = E[U λ

t−1

E[Dλ

t |Ft]] ≤ E[U λ

t−1] ≤ . . . ≤ 1.

(6)

Now we apply the method of mixtures. In a standard application of the Chernoff method, we would choose
λ to maximize E[U λ
T ], but since we still have an expectation, we cannot swap expectation and maximum.
Instead, we integrate the inequality E[U λ
T ] ≤ 1, which holds for any λ, against λ drawn from a Gaussian
distribution with covariance Q−1. By Fubini’s theorem, we can swap the expectations to obtain

1 ≥ Eλ∼N (0,Q−1)E[U λ

T ] = E

(cid:90)

T (2π)−d/2(cid:112)det(Q) exp(−λ(cid:62)Qλ/2)dλ
U λ
(cid:32) T

M 2λ(cid:62)((cid:80)T

t )λ + λ(cid:62)Qλ

t=1 ZtZ (cid:62)
2

(cid:33)

dλ

(cid:88)

t=1

(cid:18)

= E

(2π)−d/2(cid:112)det(Q) exp

(cid:104)λ, Ztζt(cid:105) −

= E

(2π)−d/2(cid:112)det(Q) exp

(cid:104)λ, S(cid:105) −

M 2λ(cid:62)Σλ + λ(cid:62)Qλ
2

(cid:19)

dλ,

(cid:90)

(cid:90)

where S (cid:44) (cid:80)T
can be rewritten as

t=1 Ztζt and recall that Σ (cid:44) (cid:80)T

t=1 ZtZ (cid:62)

t . By completing the square, the term in the exponent

(cid:104)λ, S(cid:105) −

M 2λ(cid:62)Σλ + λ(cid:62)Qλ
2

1
2

=

(cid:0)−(λ − ¯Σ−1S)(cid:62) ¯Σ(λ − ¯Σ−1S) + S(cid:62) ¯Σ−1S(cid:1) ,

where recall that ¯Σ (cid:44) M 2Σ + Q. As such we obtain

1 ≥ E

(cid:20)
exp (cid:0)S(cid:62) ¯Σ−1S/2(cid:1) ×

(cid:90)

(2π)−d/2(cid:112)det(Q) exp

(cid:18) −(λ − ¯Σ−1S)(cid:62) ¯Σ(λ − ¯Σ−1S)
2

(cid:19)(cid:21)

dλ

(cid:115)

= E

det(Q)
det( ¯Σ)

exp (cid:0)S(cid:62) ¯Σ−1S(cid:1) .

15

This proves the lemma.

Equipped with the two lemmas, we can now turn to the analysis of the inﬂuence-adjusted estimator.

Lemma 11 (Restatement of Lemma 5). Under Assumption 1 and Assumption 2, with probability at least
1 − δ, the following holds simultaneously for all t ∈ [T ]:

(cid:107)ˆθt − θ(cid:107)Γt ≤

√

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ).

Proof. Recall that we deﬁne ˆθt, Γt to be the estimator and matrix used in round t, based on t − 1 examples.
Fixing a round t, we start by expanding the deﬁnition of ˆθt. We use the shorthand zτ (cid:44) zτ,aτ , µτ (cid:44)
Eb∼πτ [zτ,b], and rτ (cid:44) rτ (aτ ).

t−1
(cid:88)

τ =1

t−1
(cid:88)

τ =1

ˆθt = Γ−1

t

(zτ − µτ )rτ = Γ−1

t

(zτ − µτ )((cid:104)θ, zτ (cid:105) + fτ (xτ ) + ξτ )

= Γ−1
t

(zτ − µτ )((cid:104)θ, zτ − µτ (cid:105) + (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ )

= (Γt)−1(Γt − λI)θ + Γ−1

t

(zτ − µτ )((cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ ).

t−1
(cid:88)

τ =1

t−1
(cid:88)

τ =1

Let Zτ (cid:44) zτ − µτ and ζτ (cid:44) (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ . With this expansion, we can write

(cid:107)ˆθt − θ(cid:107)Γt = (cid:107) − λΓ−1

t θ + Γ−1

t

Zτ ζτ (cid:107)Γt ≤ (cid:107)λθ(cid:107)Γ−1

+

t

Zτ ζτ

t−1
(cid:88)

τ =1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

√

≤

λ +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

Zτ ζτ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

.

To ﬁnish the proof, we apply Lemma 10 to this last term. To verify the preconditions of the lemma, let
Fτ (cid:44) σ(x1, . . . , xτ , a1, . . . , aτ −1, ξ1, . . . , ξτ −1) denote the σ-algebra corresponding to the τ th round, after
observing the context xτ . Then the policy πτ and hence the action aτ are Fτ measurable and so is the noise
term ξτ . Therefore, Zτ = zτ,aτ − Ea∼πτ [zτ,a] is measurable, which veriﬁes the ﬁrst precondition. Using the
boundedness properties in Assumption 2, we know that |ζτ | ≤ 3 (cid:44) M , and by construction of the random
variables, we have Zτ ⊥⊥ ζτ |Fτ and E [Zτ |Fτ ] = 0. Finally, for the symmetry property, either Zτ |Fτ ≡ 0
if one action is eliminated, or otherwise we have µτ = 1
2 (zτ,1 + zτ,2) since there are only two actions. In
this case the random variable Zτ |Fτ = (cid:15)τ (zτ,1 − zτ2 )/2 where (cid:15)τ is a Rademacher random variable. By
inspection this is clearly conditionally symmetric. As such, we may apply Lemma 10, which reveals that with
probability at least 1 − δ,

Zτ ζτ

= M 2

Zτ ζτ

≤ 2M 2 log

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Γ−1
t

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

τ =1
(cid:18)(cid:113)

(M 2Γt)−1

(cid:19)

= 18 log

λ−d det(Γt)/δ

.

(cid:32)

(cid:115)

1
δ

det(M 2Γt)
det(M 2λI)

(cid:33)

The inequality here is Lemma 10 with Q = M 2λI, and for the last equality we use that det(cQ) = cd det(Q)
for a d × d positive semideﬁnite matrix Q. As two ﬁnal steps, we apply Lemma 9 and take a union bound

16

over all rounds T . Combining these, we get that for all T ,
(cid:115)

(cid:107)ˆθt − θ(cid:107)Γt ≤

λ +

Zτ ζτ

≤

λ +

18

log(

λ−d det(Γt)) + log(T /δ)

√

(cid:18)

(cid:113)

(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

√

√

≤

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ).

Therefore, with γ(T ) (cid:44)

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ) we can apply Lemma 6 to bound

√

the regret by

Reg(T ) ≤ (cid:112)2T log(1/δ) + 2γ(T )

T
(cid:88)

(cid:114)

t=1

tr(Γ−1

t Cov
b∼πt

(zt,b)).

Via a union bound, this inequality holds with probability at least 1 − 2δ. To ﬁnish the proof we need to
analyze this latter term. This is the contents of the following lemma. A related statement, with a similar proof,
appears in Abbasi-Yadkori et al. [1].
Lemma 12. Let X1, . . . , XT be a sequence of vectors in Rd with (cid:107)Xt(cid:107)2 ≤ 1 and deﬁne Γ1 (cid:44) λI, Γt (cid:44)
Γt−1 + Xt−1X (cid:62)

t−1. Then

tr(Γ−1

t XtX (cid:62)

t ) ≤ (cid:112)T d(1 + 1/λ) log(1 + T /(dλ)).

Proof. First, apply the Cauchy-Schwarz inequality to the left hand side to obtain

tr(Γ−1

t XtX (cid:62)

t ) ≤

tr(Γ−1

t XtX (cid:62)

t ).

(cid:118)
(cid:117)
(cid:117)
(cid:116)

√

T

T
(cid:88)

T =1

T
(cid:88)

(cid:113)

t=1

T
(cid:88)

(cid:113)

t=1

For the remainder of the proof we work only with the second term. Let us start by analyzing a slightly
different quantity, tr(Γ−1
t ). By concavity of log det(M ), we have

t+1XtX (cid:62)

log det(Γt) ≤ log det(Γt+1) + tr(Γ−1

t+1(Γt − Γt+1)),

which implies

T
(cid:88)

t=1

tr(Γ−1

t+1XtX (cid:62)

t ) = tr(Γ−1

t+1(Γt+1 − Γt)) ≤ log det(Γt+1) − log det(Γt)

As such, we obtain a telescoping sum

tr(Γ−1

t+1XtX (cid:62)

t ) ≤ log det(ΓT +1) − log det(Γ1) ≤ d log(λ + T /d) − d log λ = d log(1 + T /(dλ))

The ﬁrst inequality here uses the concavity argument and the second uses Lemma 9. To ﬁnish the proof, we
must translate back to Γ−1

. For this, we use the Sherman-Morrison-Woodbury identity, which reveals that

t

X (cid:62)

t Γ−1

t+1Xt = X (cid:62)

t (Γt + XtX (cid:62)

t )−1Xt = X (cid:62)
t

Γ−1

t −

(cid:32)

t Γ−1
t

Γ−1
t XtX (cid:62)
1 + (cid:107)Xt(cid:107)2

Γ−1
t

(cid:33)

Xt

=

(cid:107)Xt(cid:107)2
Γ−1
t
1 + (cid:107)Xt(cid:107)2

Γ−1
t

≥ (1 + 1/λ)−1(cid:107)Xt(cid:107)2

.

Γ−1
t

17

Here in the last step we use that (cid:107)Xt(cid:107)2

≤ (cid:107)Xt(cid:107)2

(λI)−1 ≤ 1/λ. Overall, we obtain

Γ−1
t

T
(cid:88)

t=1

tr(Γ−1

t XtX (cid:62)

t ) ≤ (1 + 1/λ)d log(1 + T /(dλ)),

and combined with the ﬁrst application of Cauchy-Schwarz, this proves the lemma.

Combining the lemmas, we have that with probability at least 1 − 2δ, the regret is at most

Reg(T ) ≤ (cid:112)2T log(1/δ) + 2γ(T )(cid:112)T d(1 + 1/λ) log(1 + T /(dλ))
= (cid:112)2T log(1/δ) + 2(cid:112)T d(1 + 1/λ) log(1 + T /(dλ))

(cid:16)√

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ)

(cid:17)

.

With λ = 1, this bound is O

√
(cid:16)(cid:112)T d log(T /δ) log(T /d) + d

T log(T /d)

(cid:17)
.

D Proof for the General Case

We now turn to the more general case. We need several additional lemmas.

Lemma 13 (Restatement of Lemma 8). Problem (3) is convex and always has a feasible solution. Speciﬁcally,
for any vectors z1, . . . , zn ∈ Rd and any positive deﬁnite matrix M , there exists a distribution w ∈ ∆([n])
with mean µw = Eb∼w[zb] such that

Proof. We analyze the minimax program

∀i ∈ [n],

(cid:107)zi − µw(cid:107)2

M ≤ tr(M Cov
b∼w

(zb)).

min
w∈∆([n])

max
i∈[n]

(cid:107)zi − µw(cid:107)2

M − tr(M Cov
w

(z)).

The goal is to show that the value of this program is non-negative, which will prove the result. Expanding the
deﬁnitions, we have

min
w∈∆([n])

max
i∈[n]

(cid:107)zi − µw(cid:107)2

M − tr(M Cov
w

(z))

= min

w∈∆([n])

max
v∈∆([n])

= min

v∈∆([n])

max
w∈∆([n])


(cid:107)zi − µw(cid:107)2

(cid:88)

vi


(cid:107)zi − µw(cid:107)2

(cid:88)

vi

i

i

M + µ(cid:62)

wM µw −

wjz(cid:62)

j M zj



(cid:88)

M + µ(cid:62)

wM µw −

wjz(cid:62)

j M zj

 .

j

j

(cid:88)





The last equivalence here is Sion’s Minimax Theorem [31], which is justiﬁed since both domains are compact
convex subsets of Rn and since the objective is linear in the maximizing variable v, and convex in the
minimizing variable w. This convexity is clear since µw is a linear in w, and hence the ﬁrst two terms are
convex quadratics (since M is positive deﬁnite), while the third term is linear in w. Thus Sion’s theorem lets
us swap the order of the minimization and maximization.

18

Now we upper bound the solution by setting w = v. This gives


(cid:107)zi − µv(cid:107)2

(cid:88)

vi

M + µ(cid:62)

v M µv −

vjz(cid:62)

j M zj



(cid:88)

j


(zi − µv)(cid:62)M (zi − µv) + µ(cid:62)

(cid:88)

vi

v M µv −

vjz(cid:62)

j M zj

 = 0.





(cid:88)

j

≤ max

v∈∆([n])

= max

v∈∆([n])

i

i

To prove the analog of Lemma 10, we need several additional tools. First, we use Freedman’s inequality

to derive a positive-semideﬁnite inequality relating the sample covariance matrix to the population matrix.

Lemma 14. Let X1, . . . , Xn be conditionally centered random vectors in Rd adapted to a ﬁltration {Ft}n
with (cid:107)Xi(cid:107)2 ≤ 1 almost surely. Deﬁne ˆΣ (cid:44) (cid:80)n
E[XiX (cid:62)
i
probability at least 1 − δ, the following holds simultaneously for all unit vectors v ∈ Rd:

i and Σ (cid:44) (cid:80)n

t=1
| Fi]. Then, with

i=1 XiX (cid:62)

i=1

v(cid:62)Σv ≤ 2v(cid:62) ˆΣv + 9d log(9n) + 8 log(2/δ).

This lemma is related to the Matrix Bernstein inequality, which can be used to control (cid:107)Σ − ˆΣ(cid:107)2, a
quantity that is quite similar to what we are controlling here. The Matrix Bernstein inequality can be used to
derive a high probability bound of the form

∀v ∈ Rd, (cid:107)v(cid:107)2 = 1,

v(cid:62)(Σ − ˆΣ)v ≤

(cid:107)Σ(cid:107)2 + c log(dn/δ),

1
2

for a constant c > 0. On one hand, this bound is stronger than ours since the deviation term depends only
logarithmically on the dimension. However, the variance term involves the spectral norm rather than a quantity
that depends on v as in our bound. Thus, Matrix Bernstein is worse when Σ is highly ill-conditioned, and
since we have essentially no guarantees on the spectrum of Σ, our specialized inequality, which is more
adaptive to the speciﬁc direction v, is crucial. Moreover, the worse dependence on d is inconsequential, since
the error will only appear in a lower order term.

Proof. First consider a single unit vector v ∈ Rd, we will apply a covering argument at the end of the proof.
By assumption, the sequence of sums {(cid:80)τ
τ =1 is a martingale, so we may
apply Freedman’s inequality [18, 6], which states that with probability at least 1 − δ

i=1 v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)
i

| Fi])v}n

|v(cid:62)( ˆΣ − Σ)v| ≤ 2

Var(v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)

i

| Fi])v | Fi) log(2/δ) + 2 log(2/δ).

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

Let us now upper bound the variance term: for each i,

Var(v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)
i

| Fi])v | Fi) ≤ E[(v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)
i

≤ E[(v(cid:62)Xi)4 | Fi] ≤ v(cid:62)E[XiX (cid:62)
i

| Fi] | Fi)v)2 | Fi]
| Fi]v,

where the last inequality follows from the fact that (cid:107)Xi(cid:107)2 ≤ 1 and (cid:107)v(cid:107)2 ≤ 1. Therefore, the cumulative
conditional variance is at most v(cid:62)Σv. Plugging this into Freedman’s inequality gives us

|v(cid:62)( ˆΣ − Σ)v| ≤ 2

v(cid:62)Σv log(2/δ) + 2 log(2/δ).

(cid:113)

19

Now, using the fact that 2

ab ≤ αa + b/α for any α > 0, with the choice α = 1/2, we get

√

|v(cid:62)( ˆΣ − Σ)v| ≤ v(cid:62)Σv/2 + 4 log(2/δ).

Re-arranging, this implies

v(cid:62)Σv ≤ 2v(cid:62) ˆΣv + 8 log(2/δ),

(7)

which is what we would like to prove, but we need it to hold simultaneously for all unit vectors v.

To do so, we now apply a covering argument. Let N be an (cid:15)-covering of the unit sphere in the projection
pseudo-metric d(u, v) = (cid:107)uu(cid:62) − vv(cid:62)(cid:107)2, with covering number N ((cid:15)). Then via a union bound, a version
of (7) holds simultaneously for all v ∈ N , where we rescale δ → δ/N ((cid:15)).

Consider another unit vector u and let v be the covering element. We have

u(cid:62)Σu = tr(Σ(uu(cid:62) − vv(cid:62))) + v(cid:62)Σv ≤ tr(Σ(uu(cid:62) − vv(cid:62))) + 2v(cid:62) ˆΣv + 8 log(2N ((cid:15))/δ)

= tr((Σ − 2 ˆΣ)(uu(cid:62) − vv(cid:62))) + 2u(cid:62) ˆΣu + 8 log(2N ((cid:15))/δ)
≤ (cid:107)Σ − 2 ˆΣ(cid:107)(cid:63)(cid:15) + 2u(cid:62) ˆΣu + 8 log(2N ((cid:15))/δ).

Here (cid:107) · (cid:107)(cid:63) denotes the nuclear norm, which is dual to the spectral norm (cid:107) · (cid:107)2. Since all vectors are bounded
by 1, we obtain

(cid:107)Σ − 2 ˆΣ(cid:107)(cid:63) ≤ dλmax(Σ − 2 ˆΣ) ≤ 3dn.

Overall, the following bound holds simultaneously for all unit vectors v ∈ Rd, except with probability at
most δ:

v(cid:62)Σv ≤ 3dn(cid:15) + 2v(cid:62) ˆΣv + 8 log(2N ((cid:15))/δ).

The last step of the proof is to bound the covering number N ((cid:15)). For this, we argue that a covering
of the unit sphere in the Euclidean norm sufﬁces, and by standard volumetric arguments, this set has
covering number at most (3/(cid:15))d. To see why this sufﬁces, let u be a unit vector and let v be the covering
element in the Euclidean norm, which implies that (cid:107)u − v(cid:107)2 ≤ (cid:15). Further assume that (cid:104)u, v(cid:105) > 0, which
imposes no restriction since the projection pseudo-metric is invariant to multiplying by −1. By deﬁnition
we also have (cid:104)u, v(cid:105) ≤ 1. Note that the projection norm is equivalent to the sine of the principal angle
between the two subspaces, which once we restrict to vectors with non-negative inner product means that
(cid:107)uu(cid:62) − vv(cid:62)(cid:107)2 = sin ∠(u, v). Now

sin ∠(u, v) = (cid:112)1 − (cid:104)u, v(cid:105)2 = (cid:112)(1 + (cid:104)u, v(cid:105))(1 − (cid:104)u, v(cid:105))

≤ (cid:112)2(1 − (cid:104)u, v(cid:105)) =

(cid:113)

(cid:107)u(cid:107)2

2 + (cid:107)v(cid:107)2

2 − 2(cid:104)u, v(cid:105) = (cid:107)u − v(cid:107)2 ≤ (cid:15).

Using the standard covering number bound, we now have

v(cid:62)Σv ≤ 3dn(cid:15) + 2v(cid:62) ˆΣv + 8d log(3/(cid:15)) + 8 log(2/δ).

Setting (cid:15) = 1/(3n) gives

v(cid:62)Σv ≤ d + 2v(cid:62) ˆΣv + 8d log(9n) + 8 log(2/δ) ≤ 2v(cid:62) ˆΣv + 9d log(9n) + 8 log(2/δ).

20

With the positive semideﬁnite inequality, we can work towards a self-normalized martingale concentration

bound. The following is a restatement of Lemma 7 from de la Peña et al. [15].

Lemma 15 (Lemma 7 of de la Peña et al. [15]). Let {Xi}n
valued random variables adapted to the ﬁltration {Fi}n
Then

i=1 be a sequence of conditionally centered vector-
i=1 and such that (cid:107)Xi(cid:107)2 ≤ B for some constant B.

Un(λ) = exp

λ(cid:62)

Xi − λ(cid:62)

XiX (cid:62)

i + E[XiX (cid:62)

i |Fi]

λ/2

(cid:32)

n
(cid:88)

i=1

(cid:32) n
(cid:88)

i=1

(cid:33)

(cid:33)

is a supermartingale with E[Un(λ)] ≤ 1 for all λ ∈ Rd.

The lemma is related to (6), but does not require that conditional probability law for Xi is symmetric,
which we used previously. To remove the symmetry requirement, it is crucial that the quadratic self-
normalization has both empirical and population terms. With this lemma, the same argument as in the proof
of Lemma 10, yields a self-normalized tail bound.

t=1 be a ﬁltration and let {(Zt, ζt)}T

t=1 be a stochastic process with Zt ∈ Rd and
Lemma 16. Let {Ft}T
ζt ∈ R such that (1) (Zt, ζt) is Ft measurable, (2) |ζt| ≤ M for all t ∈ [T ], (3) Zt ⊥⊥ ζt|Ft, and (4)
E[Zt|Ft] = 0. Let ˆΣ (cid:44) (cid:80)T
T |Ft]. Then for any positive deﬁnite matrix Q
we have

t and Σ (cid:44) (cid:80)T

t=1 ZtZ (cid:62)

E[ZtZ (cid:62)

t=1

P





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

Ztζt

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(Q+M 2( ˆΣ+Σ))−1

≥ 2 log

(cid:115)





1
δ

det(Q + M 2( ˆΣ + Σ))
det(Q)







 ≤ δ.

Proof. The proof is identical to Lemma 10, but uses Lemma 15 in lieu of (6).

We can now analyze the inﬂuence-adjusted estimator.

Lemma 17. Under Assumption 1 and Assumption 2 and assuming that λ ≥ 4d log(9T ) + 8 log(4T /δ), with
probability at least 1 − δ, the following holds simultaneously for all t ∈ [T ]:
√

(cid:107)ˆθt − θ(cid:107)Γt ≤

λ + (cid:112)27d log(1 + 2T /d) + 54 log(4T /δ).

Proof. Using the same argument as in the proof of Lemma 5, we get

(cid:107)ˆθt − θ(cid:107)Γt ≤

√

λ +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

Zτ ζτ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

,

where Zτ (cid:44) zτ − µτ and ζτ (cid:44) (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ , just as before. Now we must control this error
term, for which we need both Lemma 14 and Lemma 16. Apply Lemma 14 to the vectors Zτ , setting
ˆΣt (cid:44) (cid:80)t−1
E[Zτ Zτ | Fτ ]. With probability at least 1 − δ/(2T ), we have that for
all unit vectors v ∈ Rd

τ and Σt (cid:44) (cid:80)t−1

τ =1 Zτ Z (cid:62)

τ =1

v(cid:62)Σtv ≤ 2v(cid:62) ˆΣtv + 9d log(9t) + 8 log(4T /δ) ≤ 2v(cid:62) ˆΣtv + 9d log(9T ) + 8 log(4T /δ).

This implies a lower bound on all quadratic forms involving ˆΣt, which leads to positive semideﬁnite inequality

λI + ˆΣt (cid:23) (λ − 3d log(9T ) − 8/3 log(4T /δ))I + ( ˆΣt + Σt)/3.

21

This means that for any vector v, we have

(cid:107)v(cid:107)2

(λI+ ˆΣt)−1 ≤ (cid:107)v(cid:107)2
≤ 3(cid:107)v(cid:107)2

((λ−3d log(9T )−8/3 log(4T /δ))I+( ˆΣt+Σt)/3)−1
((3λ−9d log(9T )−8 log(4T /δ))I+ ˆΣt+Σt)−1.

Before we apply Lemma 16, we must introduce the range parameter M . Fix a round t and let A (cid:44)
((3λ − 9d log(9T ) − 8 log(4T /δ))I + ˆΣt + Σt) denote the matrix in the Mahalanobis norm. Then,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

A−1

Zτ ζτ

= M 2

Zτ ζτ

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(M 2A)−1

Now apply Lemma 16 with Q (cid:44) M 2(3λ − 9d log(9T ) − 8 log(4T /δ))I. Since we require Q (cid:31) 0, this
requires λ > 3d log(9T ) − 8/3 log(4T /δ), which is satisﬁed under the preconditions for the lemma. Under
this assumption, we get

(cid:107)

t−1
(cid:88)

τ =1

Zτ ζτ (cid:107)2

(λI+ ˆΣt)−1 ≤ 3M 2(cid:107)

Zτ ζτ (cid:107)2

(Q+M 2( ˆΣt+Σt))−1

t−1
(cid:88)

τ =1


≤ 6M 2 log



(cid:115)

4T
δ

det(Q + M 2( ˆΣt + Σt))
det(Q)



 ,

with probability at least 1 − δ/(2T ). With a union bound, the inequality holds simultaneously for all T , with
probability at least 1 − δ.

The last step is to analyze the determinant. Using the same argument as in the proof of Lemma 9, it is not

hard to show that

(cid:32)

det(Q + M 2( ˆΣt + Σt))
det(Q)

(cid:33)1/d

≤ 1 +

2(t − 1)
d(3λ − 9d log(9T ) − 8 log(4T /δ))

.

If we impose the slightly stronger condition that λ ≥ 4d log(9T ) + 8 log(4T /δ), then the term in the
denominator is at least 1, and then we have that

(cid:107)ˆθt − θ(cid:107)Γt ≤

λ + (cid:112)6M 2 log(4T /δ) + 3dM 2 log(1 + 2T /d).

Finally, as in the two-action case, we use the fact that |ζt| ≤ 3 (cid:44) M .

Recall the setting of γ(T ) (cid:44)

λ + (cid:112)27d log(1 + 2T /d) + 54 log(4T /δ) and the deﬁnition of λ (cid:44)
4d log(9T ) + 8 log(4T /δ). For the remainder of the proof, condition on the probability 1 − δ event
that Lemma 17 holds. We now turn to analyzing the regret.

Lemma 18. Let µt (cid:44) Ea∼πtzt,a where πt is the solution to (3) and assume the conclusion of Lemma 17
holds. Then with probability at least 1 − δ

√

√

Reg(T ) ≤ (1 + 6γ(T ))(cid:112)2T log(2/δ) + 3γ(T )

tr(Γ−1

t (zt,at − µt)(zt,at − µt)(cid:62)).

(cid:118)
(cid:117)
(cid:117)
(cid:116)T

T
(cid:88)

t=1

22

This lemma is slightly more complicated than Lemma 6.

Proof. First, using the same application of Azuma’s inequality as in the proof of Lemma 6, with probability
1 − δ/2, we have

Reg(T ) ≤ (cid:112)2T log(2/δ) +

Ea∼πt[(cid:104)θ, zt,a(cid:63)

t

− zt,a(cid:105) | Ft].

T
(cid:88)

t=1

Now we work with this latter expected regret

Ea∼πt [(cid:104)θ, zt,a(cid:63)

t

− zt,a(cid:105) | Ft] =

(cid:104)θ, zt,a(cid:63)

− µt(cid:105) ≤

t

(cid:104)ˆθ, zt,a(cid:63)

t

− µt(cid:105) + γ(T )(cid:107)zt,a(cid:63)

− µt(cid:107)Γ−1

.

t

t

For the ﬁrst term, we use the ﬁltration condition (2)

(cid:104)ˆθ, zt,a(cid:63)

t

− µt(cid:105) =

πt(a)(cid:104)ˆθ, zt,a(cid:63)

t

− zt,a(cid:105) ≤ γ(T )

πt(a)(cid:107)zt,a(cid:63)

t

− zt,a(cid:107)Γ−1

t

T
(cid:88)

t=1

(cid:88)

a∈At

≤ γ(T )(cid:107)zt,a(cid:63)

t

− µt(cid:107)Γ−1

t

+ γ(T )

πt(a)(cid:107)zt,a − µt(cid:107)Γ−1

.

t

T
(cid:88)

t=1

(cid:88)

a∈At

(cid:88)

a∈At

Applying the feasibility condition in (3), we can bound the expected regret by

Ea∼πt[(cid:104)θ, zt,a(cid:63)

t

− zt,a(cid:105) | Ft] ≤ 3γ(T )

tr(Γ−1

t Cov
a∼πt

(zt,a)) ≤ 3γ(T )

tr(Γ−1

t Cov
a∼πt

(zt,a)).

T
(cid:88)

(cid:114)

t=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)T

T
(cid:88)

t=1

To complete the proof, we need to relate the covariance, which takes expectation over the random action, with
the particular realization in the algorithm, since this realization affects the term Γt+1. Let Zt (cid:44) zt,at − µt
denote the centered realization, then the covariance term is

In order to derive a bound on (cid:80)T

t=1 tr(Γ−1

t Cova∼πt(zt,a)), we ﬁrst consider the following

Cov
a∼πt

(zt,a) = E[ZtZ (cid:62)
t

| Ft]

T
(cid:88)

t=1

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) − tr(Γ−1

t ZtZ (cid:62)

t ).

Observe that sequence of sums {(cid:80)τ
| Ft]) − tr(Γ−1
each term tr(Γ−1
the Freedman’s inequality reveals that with probability at least 1 − δ/2

| Ft]) − tr(Γ−1

E[ZtZ (cid:62)
τ =1 is a martingale. Also,
t
t ) is bounded by 1 because Γ1 = λI and λ > 1. Applying

t
t ZtZ (cid:62)

t=1 tr(Γ−1

E[ZtZ (cid:62)
t

t ZtZ (cid:62)

t )}T

t

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) − tr(Γ−1

t ZtZ (cid:62)

t ) ≤ 2

E[(Z (cid:62)

t Γ−1

t Zt)2 | Ft] log(2/δ) + 2 log(2/δ)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

t=1

T
(cid:88)

t=1

≤

1
2

T
(cid:88)

t=1

23

≤ 2

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) log(2/δ) + 2 log(2/δ)

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) + 4 log(2/δ).

Then rearranging and plugging back into our regret bound, we have

Reg(T ) ≤ (cid:112)2T log(2/δ) + 3γ(T )

(cid:118)
(cid:117)
(cid:117)
(cid:116)2T

(cid:32) T

(cid:88)

t=1

tr(Γ−1

t ZtZ (cid:62)

t ) + 4 log(2/δ)

(cid:33)

≤ (1 + 6γ(T ))(cid:112)2T log(2/δ) + 3γ(T )

tr(Γ−1

t ZtZ (cid:62)

t ).

(cid:118)
(cid:117)
(cid:117)
(cid:116)2T

T
(cid:88)

t=1

To conclude the proof of the theorem, apply Lemma 7, which applies on the last term on the RHS

of Lemma 18. Overall, with probability at least 1 − 2δ, we get

Reg(T ) ≤ (1 + 6γ(T ))(cid:112)2T log(2/δ) + 3γ(T )(cid:112)2T d(1 + 1/λ) log(1 + T /(dλ)).

Since λ = Θ(d log(T /δ)) and γ(T ) = O((cid:112)d log(T ) + (cid:112)log(T /δ)), we get with probability 1 − δ,

Reg(T ) ≤ O

√

(cid:16)

d

T log(T ) + (cid:112)dT log(T ) log(T /δ) + (cid:112)T log(T /δ) log(1/δ)

(cid:17)

.

References

[1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic

bandits. In Advances in Neural Information Processing Systems, 2011.

[2] Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efﬁcient algorithm

for bandit linear optimization. In Conference on Learning Theory, 2008.

[3] Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E Schapire. Taming the
monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine
Learning, 2014.

[4] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In

International Conference on Machine Learning, 2013.

[5] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed

bandit problem. SIAM Journal on Computing, 2002.

[6] Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandit
algorithms with supervised learning guarantees. In International Conference Artiﬁcial Intelligence and
Statistics, 2011.

[7] Peter J Bickel, Chris AJ Klaassen, Ya’acov Ritov, and Jon A Wellner. Efﬁcient and adaptive estimation

for semiparametric models. Springer New York, 1998.

[8] Sébastien Bubeck, Nicolo Cesa-Bianchi, and Sham M Kakade. Towards minimax policies for online

linear optimization with bandit feedback. In Conference on Learning Theory, 2012.

[9] Nicolo Cesa-Bianchi and Gábor Lugosi. Combinatorial bandits. Journal of Computer and System

Sciences, 2012.

24

[10] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duﬂo, Christian Hansen, Whit-
ney Newey, and James M. Robins. Double machine learning for treatment and causal parameters.
arXiv:1608.00060, 2016.

[11] Wei Chu, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandits with linear payoff functions.

In International Conference on Artiﬁcial Intelligence and Statistics, 2011.

[12] Thomas M Cover. Behavior of sequential predictors of binary sequences. In Conference on Information

Theory, Statistical Decision Functions and Random Processes, 1965.

[13] Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit

feedback. In Conference on Learning Theory, 2008.

[14] Victor H de la Peña, Tze Leung Lai, and Qi-Man Shao. Self-normalized processes: Limit theory and

statistical applications. Springer Science & Business Media, 2008.

[15] Victor H de la Peña, Michael J Klass, and Tze Leung Lai. Theory and applications of multivariate

self-normalized processes. Stochastic Processes and their Applications, 2009.

[16] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the
multi-armed bandit and reinforcement learning problems. Journal of Machine Learning Research, 2006.

[17] Sarah Filippi, Olivier Cappe, Aurélien Garivier, and Csaba Szepesvári. Parametric bandits: The

generalized linear case. In Advances in Neural Information Processing Systems, 2010.

[18] David A Freedman. On tail probabilities for martingales. The Annals of Probability, 1975.

[19] Kristjan Greenewald, Ambuj Tewari, Susan Murphy, and Predag Klasnja. Action centered contextual

bandits. In Advances in Neural Information Processing Systems, 2017.

[20] Akshay Krishnamurthy, Alekh Agarwal, and Miroslav Dudík. Contextual semibandits via supervised

learning oracles. In Advances in Neural Information Processing Systems, 2016.

[21] John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side

information. In Advances in Neural Information Processing Systems, 2008.

[22] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personal-

ized news article recommendation. In International Conference on World Wide Web, 2010.

[23] Lihong Li, Yu Lu, and Dengyong Zhou. Provable optimal algorithms for generalized linear contextual

bandits. In International Conference on Machine Learning, 2017.

[24] Jerzy Neyman. C(α) tests and their use. Sankhy¯a: The Indian Journal of Statistics, Series A, 1979.

[25] Alexander Rakhlin and Karthik Sridharan. Bistro: An efﬁcient relaxation-based method for contextual

bandits. In International Conference on Machine Learning, 2016.

[26] James M Robins and Andrea Rotnitzky. Recovery of information and adjustment for dependent

censoring using surrogate markers. In AIDS Epidemiology. Springer, 1992.

[27] James M Robins, Lingling Li, Eric Tchetgen Tchetgen, and Aad van der Vaart. Higher order inﬂuence
functions and minimax estimation of nonlinear functionals. In Probability and Statistics: Essays in
Honor of David A. Freedman. Institute of Mathematical Statistics, 2008.

25

[28] Peter M Robinson. Root-n-consistent semiparametric regression. Econometrica: Journal of the

[29] Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of

[30] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of

Econometric Society, 1988.

Operations Research, 2010.

Operations Research, 2014.

[31] Maurice Sion. On general minimax theorems. Paciﬁc Journal of Mathematics, 1958.

[32] Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dudík, John Langford, Damien
In Advances in Neural

Jose, and Imed Zitouni. Off-policy evaluation for slate recommendation.
Information Processing Systems, 2017.

[33] Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert E Schapire. Efﬁcient algorithms for adversarial

contextual learning. In International Conference on Machine Learning, 2016.

[34] Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health. In

Mobile Health. Springer, 2017.

[35] Anastasios Tsiatis. Semiparametric theory and missing data. Springer Science & Business Media, 2007.

26

8
1
0
2
 
l
u
J
 
6
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
0
2
4
0
.
3
0
8
1
:
v
i
X
r
a

Semiparametric Contextual Bandits

Akshay Krishnamurthy ∗2, Zhiwei Steven Wu †2, and Vasilis Syrgkanis ‡3

2Microsoft Research NYC, New York, NY
3Microsoft Research New England, Cambridge, MA

July 17, 2018

Abstract

This paper studies semiparametric contextual bandits, a generalization of the linear stochastic bandit
problem where the reward for an action is modeled as a linear function of known action features confounded
by a non-linear action-independent term. We design new algorithms that achieve ˜O(d
T ) regret over T
rounds, when the linear function is d-dimensional, which matches the best known bounds for the simpler
unconfounded case and improves on a recent result of Greenewald et al. [19]. Via an empirical evaluation,
we show that our algorithms outperform prior approaches when there are non-linear confounding effects on
the rewards. Technically, our algorithms use a new reward estimator inspired by doubly-robust approaches
and our proofs require new concentration inequalities for self-normalized martingales.

√

1 Introduction

A number of applications including online personalization, mobile health, and adaptive clinical trials require
that an agent repeatedly makes decisions based on user or patient information with the goal of optimizing some
metric, typically referred to as a reward. For example, in online personalization problems, we might serve
content based on user history and demographic information with the goal of maximizing user engagement
with our service. Since counterfactual information is typically not available, these problems require algorithms
to carefully balance exploration—making potentially suboptimal decisions to acquire new information—with
exploitation—using collected information to make better decisions. Such problems are often best modeled
with the framework of contextual bandits, which captures the exploration-exploitation tradeoff and enables
rich decision making policies but ignores the long-term temporal effects that make general reinforcement
learning challenging. Contextual bandit algorithms have seen recent success in applications, including news
recommendation [22] and mobile health [34].

Contextual bandit algorithms can be categorized as either parametric or agnostic, depending on whether
they model the relationship between the reward and the decision or not. Parametric approaches typically
assume that the reward is a (generalized) linear function of a known decision-speciﬁc feature vector [17, 11,
1, 4]. Once this function is known to high accuracy, it can be used to make near-optimal decisions. Exploiting

∗akshay@cs.umass.edu
†zsw@umn.edu
‡vasy@microsoft.com

1

this fact, algorithms for this setting focus on learning the parametric model. Unfortunately, fully parametric
assumptions are often unrealistic and challenging to verify in practice, and these algorithms may perform
poorly when the assumptions do not hold.

In contrast, agnostic approaches make no modeling assumptions about the reward and instead compete
with a large class of decision-making policies [21, 3]. While these policies are typically parametrized in
some way, these algorithms provably succeed under weaker conditions and are generally more robust than
parametric ones. On the other hand, they typically have worse statistical guarantees, are conceptually much
more complex, and have high computational overhead, technically requiring solving optimization problems
that are NP-hard in the worst case. This leads us to a natural question:

Is there an algorithm that inherits the simplicity and statistical guarantees of the parametric
methods and the robustness of the agnostic ones?

Working towards an afﬁrmative answer to this question, we consider a semiparametric contextual bandit
setup where the reward is modeled as a linear function of the decision confounded by an additive non-linear
perturbation that is independent of the decision. This setup signiﬁcantly generalizes the standard parametric
one, allowing for complex, non-stationary, and non-linear rewards (See Section 2 for a precise formulation).
On the other hand, since this perturbation is just a baseline reward for all decisions, it has no inﬂuence on the
optimal one, which depends only on the unknown linear function. In the language of econometrics and causal
modeling, the treatment effect is linear.

In this paper, we design new algorithms for the semiparametric contextual bandits problem. When the
linear part of the reward is d-dimensional, our algorithms achieve ˜O(d
T ) regret over T rounds, even when
the features and the confounder are chosen by an adaptive adversary. This guarantee matches the best results
for the simpler linear stochastic bandit problem up to logarithmic terms, showing that there is essentially no
statistical price to pay for robustness to confounding effects. On the other hand, our algorithm and analysis is
quite different, and it is not hard to see that existing algorithms for stochastic bandits fail in our more general
setting. Our regret bound also improves on a recent result of Greenewald et al. [19], who consider the same
setup but study a weaker notion of regret. Our algorithm, main theorem, and comparisons are presented
in Section 3.

√

We also compare our algorithm to approaches from both parametric and agnostic families in an empirical
study (we use a linear policy class for agnostic approaches). In Section 5, we evaluate several algorithms on
synthetic problems where the reward is (a) linear, and (b) linear with confounding. In the linear case, our
approach learns, but is slightly worse than the baselines. On the other hand, when there is confounding, our
algorithm signiﬁcantly outperforms both parametric and agnostic approaches. As such, these experiments
demonstrate that our algorithm represents a favorable trade off between statistical efﬁciency and robustness.
On a technical level, our algorithm and analysis require several new ideas. First, we derive a new estimator
for linear models in the presence of confounders, based on recent and classical work in semiparametric
statistics and econometrics [28, 10]. Second, since standard algorithms using optimism principles fail to
guarantee consistency of this new estimator, we design a new randomized algorithm, which can be viewed
as an adaptation of the action-elimination method of Even-Dar et al. [16] to the contextual bandits setting.
Finally, analyzing the semiparametric estimator requires an intricate deviation argument, for which we derive
a new self-normalized inequality for vector-valued martingales using tools from de la Peña et al. [14, 15].

2 Preliminaries

We study a generalization of the linear stochastic bandit problem with action-dependent features and action-
independent confounder. The learning process proceeds for T rounds, and in round t, the learner receives a

2

context xt (cid:44) {zt,a}a∈A where zt,a ∈ Rd and A is the action set, which we assume to be large but ﬁnite. The
learner then chooses an action at ∈ A and receives reward

rt(at) (cid:44) (cid:104)θ, zt,at(cid:105) + ft(xt) + ξt,

(1)

where θ ∈ Rd is an unknown parameter vector, ft(xt) is a confounding term that depends on the context xt
but, crucially, does not depend on the chosen action at, and ξt is a noise term that is centered and independent
of at.

For each round t, let a(cid:63)
t

(cid:44) argmaxa∈A(cid:104)θ, zt,a(cid:105) denote the optimal action for that round. The goal of our

algorithm is to minimize the regret, deﬁned as

Reg(T ) (cid:44)

rt(a(cid:63)

t ) − rt(at) =

(cid:104)θ, zt,a(cid:63)

t

− zt,at(cid:105).

T
(cid:88)

t=1

T
(cid:88)

t=1

Observe that the noise term ξt, and, more importantly, the confounding term ft(xt) are absent in the ﬁnal
expression, since they are independent of the action choice.

We consider the challenging setting where the context xt and the confounding term ft(·) are chosen by
an adaptive adversary, so they may depend on all information from previous rounds. This is formalized in the
following assumption.

Assumption 1 (Environment). We assume that xt = {zt,a}a∈A, ft, ξt are generated at the beginning of
round t, before at is chosen. We assume that xt and ft are chosen by an adaptive adversary, and that ξt
satisﬁes E[ξt|xt, ft] = 0 and |ξt| ≤ 1.

We also impose mild regularity assumptions on the parameter, the feature vectors, and the confounding

functions.

Assumption 2 (Boundedness). Assume that (cid:107)θ(cid:107)2 ≤ 1 and that (cid:107)zt,a(cid:107)2 ≤ 1 for all a ∈ A, t ∈ [T ]. Further
assume that ft(·) ∈ [−1, 1] for all t ∈ [T ].

For simplicity, we assume an upper bound of 1 in these conditions, but our algorithm and analysis can be

adapted to more generic regularity conditions.

Related work. Our setting is related to linear stochastic bandits and several variations that have been
studied in recent years. Among these, the closest is the work of Greenewald et al. [19] who consider the
same setup and provide a Thompson Sampling algorithm using a new reward estimator that eliminates the
confounding term. Motivated by applications in medical intervention, they consider a different notion of
regret from our more-standard notion and, as such, the results are somewhat incomparable. For our notion
of regret, their analysis can produce a T 2/3-style regret bound, which is worse than our optimal
T bound.
See Section 3.3 for a more detailed comparison.

√

Other results for linear stochastic bandits include upper-conﬁdence bound algorithms [29, 11, 1], Thomp-
son sampling algorithms [4, 30], and extensions to generalized linear models [17, 23]. However, none of
these models accommodate arbitrary and non-linear confounding effects. Moreover, apart from Thompson
sampling, all of these algorithms use deterministic action-selection policies (conditioning on the history),
which provably incurs Ω(T ) regret in our setting, as we will see.

One can accommodate confounded rewards via an agnostic-learning approach to contextual bandits [5,
21, 3]. In this framework, we make no assumptions about the reward, but rather compete with a class
of parameterized policies (or experts). Since a d-dimensional linear policy is optimal in our setting, an

3

agnostic algorithm with a linear policy class addresses precisely our notion of regret. However there are two
disadvantages. First, agnostic algorithms are all computationally intractable, either because they enumerate
the (inﬁnitely large) policy class, or because they assume access to optimization oracles that can solve NP-hard
K, the
problems in the worst case. Second, most agnostic approaches have regret bounds that grow with
number of actions, while our bound is completely independent of K.

√

We are aware of one approach that is independent of K, but it requires enumeration of an inﬁnitely
large policy class. This method is based on ideas from the adversarial linear and combinatorial bandits
(cid:44) (zt,a, 1) ∈ Rd+1, our setting can
literature [13, 2, 8, 9]. Writing θt (cid:44) (θ, ft(xt)) ∈ Rd+1 and z(cid:48)
be re-formulated in the adversarial linear bandits framework. However, standard linear bandit algorithms
compete with the best ﬁxed action vector in hindsight, rather than the best policy with time-varying action
sets. To resolve this, one can use the linear bandits reward estimator [32] in a contextual bandit algorithm
like EXP4 [5], but this approach is not computationally tractable with the linear policy class. For our setting,
we are not aware of any computationally efﬁcient approaches, even oracle-based approaches, that achieve
poly(d)

T regret with no dependence on the number of actions.

√

t,a

We resolve the challenge of confounded rewards with an estimator from the semiparametric statistics
literature [35], which focuses on estimating functionals of a nonparametric model. Most estimators are based
on Neyman Orthogonalization [24], which uses moment equations that are insensitive to nuisance parameters
in a method-of-moments approach [10]. These orthogonal moments typically involve a linear correction
to an initial nonparametric estimate using so-called inﬂuence functions [7, 27]. Robinson [28] used this
approach for the ofﬂine version of our setting (known as the partially linear regression (PLR) model) where
he demonstrated a form of double-robustness [26] to poor estimation of the nuisance term (in our case ft(xt)).
We generalize Robinson’s work to the online setting, showing how orthogonalized estimators can be used for
adaptive exploration. This requires several new techniques, including a novel action selection policy and a
self-normalized inequality for vector-valued martingales.

3 Algorithm and Results

In this section, we describe our algorithm and present our main theoretical result, an ˜O(d
for the semiparametric contextual bandits problem.

√

T ) regret bound

3.1 A Lower Bound

Before turning to the algorithm, we ﬁrst present a lower bound against deterministic algorithms. Since the
functions ft may be chosen by an adaptive adversary, it is not hard to show that this setup immediately
precludes the use of deterministic algorithms.

Proposition 3. Consider an algorithm that, at round t, chooses an action at as a deterministic function of
the observable history Ht (cid:44) {x1:t, a1:t−1, r1:t−1}. There exists a semiparametric contextual bandit instance
with d = 2 and K = 2 where the regret of the algorithm is at least T /2.

See Appendix B for the proof, which resembles the standard argument against deterministic online
learning algorithms [12]. The main difference is that the adversary uses the confounding term to corrupt
the information that the learner receives, whereas, in the standard proof, the adversary chooses the optimal
action in response to the learner. In fact, deterministic algorithms can succeed in the full information version
of our setting, since taking differences between rewards eliminates the confounder. Thus, bandit feedback
plays a crucial role in our construction and the bandit setting is considerably more challenging than the full
information analogue.

4

Algorithm 1: BOSE (Bandit orthogonalized semiparametric estimation)

Input :T, δ ∈ (0, 1).

1 Set λ ← 4d log(9T ) + 8 log(4T /δ) and γ(T ) ←
2 Initialize ˆθ ← 0 ∈ Rd, Γ ← λId×d.
3 for t = 1, . . . , T do
4

Observe xt = {zt,a}a∈A
Filter

√

λ + (cid:112)27d log(1 + 2T /d) + 54 log(4T /δ).

5

6

7

8

At ←

(cid:110)
a ∈ A | ∀b ∈ A, (cid:104)ˆθ, zt,b − zt,a(cid:105) ≤ γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

(cid:111)

.

Find distribution πt ∈ ∆(At) such that ∀a ∈ At (We use
Covb∼πt(zt,b) (cid:44) E[zt,bz(cid:62)

t,b] − (Ezt,b)(Ezt,b)(cid:62).)
(cid:107)zt,a − Eb∼πtzt,b(cid:107)2

Γ−1 ≤ tr(Γ−1 Cov
b∼πt

(zt,b)).

Sample at ∼ πt and play at. Observe rt(at). (rt(at) = (cid:104)θ, zt,at(cid:105) + ft(xt) + ξt.)
Let µt = Ea∼πt[zt,a | xt] and update parameters

(2)

(3)

Γ ← Γ + (zt,at − µt)(zt,at − µt)(cid:62),

ˆθ ← Γ−1

(zτ,aτ − µτ )rτ (aτ ).

(4)

t
(cid:88)

τ =1

We emphasize that, except for the Thompson Sampling approach [4], essentially all algorithms for the
linear stochastic bandit problem use deterministic strategies, so they provably fail in the semiparametric
setting. As we mentioned, Thompson Sampling was analyzed in our setting by Greenewald et al. [19], but
T -type regret bound (See Section 3.3 for a more quantitative and detailed
they do not obtain the optimal
comparison). In contrast, our algorithm is quite different from all of these approaches; it ensures enough
randomization to circumvent the lower bound and also achieves the optimal

√

√

√

T regret.
T ) lower bound for linear stochastic bandits [13],

To conclude this discussion, we remark that the Ω(d

which also applies to randomized algorithms, holds in our more general setting as well.

3.2 The Algorithm

Pseudocode for the algorithm, which we call BOSE, for “Bandit Orthogonalized Semiparametric Estimation,"
is displayed in Algorithm 1. The algorithm maintains an estimate ˆθ for the true parameter θ, which it uses in
each round to select an action via two steps: (1) an action elimination step that removes suboptimal actions,
and (2) an optimization step that ﬁnds a good distribution over the surviving actions. The algorithm then
samples and plays an action from this distribution and uses the observed reward to update the parameter
estimate ˆθ. This parameter estimation step is the third main element of the algorithm. We now describe each
of these three components in detail.

Parameter estimation. For simplicity, we use zt (cid:44) zt,at to denote the feature vector for the action that
was chosen at round t, and similarly we use rt (cid:44) rt(at). Using all previously collected data, speciﬁcally
{zτ , rτ }t
τ =1 at the end of round t, we would like to estimate the parameter θ. First, if fτ (xτ ) were identically

5

zero, by exploiting the linear parametrization we could use ridge regression, which with some λ > 0 gives

(cid:32)

(cid:33)−1 t

ˆθRidge (cid:44)

λI +

zτ z(cid:62)
τ

t
(cid:88)

τ =1

(cid:88)

τ =1

zτ rτ .

This estimator appears in most prior approaches for linear stochastic bandits [29, 11, 1]. Unfortunately,
since fτ (xτ ) is non-zero, ˆθRidge has non-trivial and non-vanishing bias, so even in benign settings it is not a
consistent estimator for θ.1

Our approach to eliminating the bias from the confounding term fτ (xτ ) is to center the feature vectors
zτ . Intuitively, in the ridge estimator, if zτ is centered, then zτ (rτ − (cid:104)θ(cid:63), zτ (cid:105)) is mean zero, even when
there is non-negligible bias in the second term. As such, the error of the corresponding estimator can be
expected to concentrate around zero. In the semiparametric statistics literature, this is known as Neyman
Orthogonalization [24], which was analyzed in the context of linear regression by Robinson [28] and in a
more general setting by Chernozhukov et al. [10].

To center the feature vector, we will, at round t, choose action at by sampling from some distribution
πt ∈ ∆(A). Let µt (cid:44) Eat∼πt[zt,at|xt] denote the mean feature vector, taking expectation only over our
random action choice. With this notation, the orthogonalized estimator is

Γ = λI +

(zτ − µτ )(zτ − µτ )(cid:62),

ˆθ = Γ−1

(zτ − µτ )rτ .

t
(cid:88)

τ =1

t
(cid:88)

τ =1

ˆθ is a Ridge regression version of Robinson’s classical semiparametric regression estimator [28]. The
estimator was originally derived for observational studies where one might not know the propensities µτ
exactly, and the standard description involves estimates ˆfτ and ˆµτ for the confounding term fτ and the
propensities µτ respectively. Informally, the estimator achieves a form of double-robustness, in the sense
that it is accurate if either of these auxilliary estimators are. In our case, since we know the propensities
µτ exactly, we can use an inconsistent estimator for the confounding term, so we simply set ˆfτ (xτ ) ≡ 0.
In Lemma 5, we prove a precise ﬁnite sample concentration inequality for this orthogonalized estimator,
showing that the confounding term ft(xt) does not introduce any bias. While the estimator has been studied
in prior works [28], to our knowledge, our error guarantee is novel.

The convergence rate of the orthogonalized estimator depends on the eigenvalues of the matrix Γ, and
we must carefully select actions to ensure these eigenvalues are sufﬁciently large. To see why, notice that
any deterministic action-selection approach with the orthogonalized estimator (including conﬁdence based
approaches), will fail, since zt = µt, so the eigenvalues of Γ do not grow rapidly and in fact the estimator is
identically 0. This argument motivates our new action selection scheme which ensure substantial conditional
covariance.

Action selection. Our action selection procedure has two main elements. First using our estimate ˆθ, we
eliminate any action that is provably suboptimal. Based on our analysis for the estimator ˆθ, at round t, we can
certify action a is suboptimal, if we can ﬁnd another action b such that

(cid:104)ˆθ, zt,b − zt,a(cid:105) > γ(T )(cid:107)zt,b − zt,a(cid:107)Γ−1.

1A related estimator can be used to evaluate the reward of a policy, as in linear and combinatorial bandits [9], but to achieve
adequate exploration, one must operate over the policy class, which leads to computational intractability. We would like to use ˆθ to drive
exploration, and this seems to require a consistent estimator. See Appendix A for a simple example demonstrating how using a biased
estimator in a conﬁdence-based approach results in linear regret.

6

Here γ(T ) is the constant speciﬁed in the algorithm, and (cid:107)x(cid:107)M (cid:44)
x(cid:62)M x denotes the Mahalanobis norm.
Using our conﬁdence bound for ˆθ in Lemma 5 below, this inequality certiﬁes that action b has higher expected
reward than action a, so we can safely eliminate a from consideration.

The next component is to ﬁnd a distribution over the surviving actions, denoted A(cid:48)

t at round t, with
t) that we use is the solution to the following feasibility

sufﬁcient covariance. The distribution πt ∈ ∆(A(cid:48)
problem

√

∀a ∈ A(cid:48)
t,

(cid:107)zt,a − Eb∼πtzt,b(cid:107)2

Γ−1 ≤ tr(Γ−1 Cov
b∼πt

(zt,b)).

For intuition, the left hand side of the constraint for action a is an upper bound on the expected regret if a is
the optimal action on this round. Thus, the constraints ensure that the regret is related to the covariance of the
distribution, which means that if we incur high regret, the covariance term Covb∼πt(zt,b) will be large. Since
we use a sample from πt to update our parameter estimate, this means that whenever the instantaneous regret
is large, we must learn substantially about the parameter. In this way, the distribution πt balances exploration
and exploitation. We will see in Lemma 8 that this program is convex and always has a feasible solution.

Our action selection scheme bears some resemblance to action-elimination approaches that have been
studied in various bandit settings [16]. The main differences are that we adapt these ideas to the contextual
setting and carefully choose a distribution over the surviving actions to balance exploration and exploitation.

3.3 The Main Result

We now turn to the main result, a regret guarantee for BOSE.

Theorem 4. Consider the semiparametric contextual bandit problem under Assumption 1 and Assumption 2.
T log(T /δ)).
For any parameter δ ∈ (0, 1), with probability at least 1−δ, Algorithm 1 has regret at most O(d

√

The constants, and indeed a bound depending on λ and γ(T ) can be extracted from the proof, provided in

the appendix. To interpret the regret bound, it is worth comparing with several related results:

√

Comparison with linear stochastic bandits. While most algorithms for linear stochastic bandits prov-
ably fail in our setting (via Proposition 3), the best regret bounds here are O((cid:112)dT log(T K/δ)) [11] and
T log(T ) + (cid:112)dT log(T ) log(1/δ)) [1] depending on whether we assume that the number of actions
O(d
K is small or not. This latter result is optimal when the number of actions is large [13], which is the setting
we are considering here. Since our bound matches this optimal regret up to logarithmic factors, and since
linear stochastic bandits are a special case of our semiparametric setting, our result is therefore also optimal
up to logarithmic factors. An interesting open question is whether an ˜O((cid:112)dT log(K/δ)) regret bound is
achievable in the semiparametric setting.

Comparison with agnostic contextual bandits. The best oracle-based agnostic approaches achieve ˜O(
regret [3], incurring a polynomial dependence on the number of actions K, although there is one inefﬁcient
method that can achieve ˜O(d
T ),2 as we discussed previously. To date, all efﬁcient methods in the agnostic
setting require some form of i.i.d. [3] or transductive assumption [33, 25] on the contexts, which we do not
assume here.

√

√

dKT )

2This follows easily by combining ideas from Auer et al. [5] and Cesa-Bianchi and Lugosi [9].

7

Comparison with Greenewald et al. [19]. Greenewald et al. [19] consider a very similar setting to ours,
where rewards are linear with confounding, but where one default action a0 always has zt,a0 ≡ 0 ∈ Rd.
Applications in mobile health motivate a restriction that the algorithm choose the a0 action with probability
∈ [p, 1 − p] for some small p ∈ (0, 1). Their work also introduces a new notion of regret where they compete
with the policy that also satisﬁes this constraint but otherwise chooses the optimal action a(cid:63)
t . In this setup,
they obtain an ˜O(d2

T ) regret bound, which has a worse dimension dependence than Theorem 4.

√

While the setup is somewhat different, we can still translate our result into a regret bound in their setting,
since BOSE can support the probability constraint, and by coupling the randomness between BOSE and
the optimal policy, the regret is unaffected.3 On the other hand, since the constant in their regret bound
scales with 1/p, their results as stated are vacuous when p = 0 which is precisely our setting. For our more
challenging regret deﬁnition, their analysis can produce a suboptimal T 2/3-style regret bound, and in this
sense, Theorem 4 provides a quantitative improvement.

Summary. BOSE achieves essentially the same regret bound as the best linear stochastic bandit methods,
but in a much more general setting. On the other hand, the agnostic methods succeed under even weaker
assumptions, but have worse regret guarantees and/or are computationally intractable. Thus, BOSE broadens
the scope for computationally efﬁcient contextual bandit learning.

4 Proof Sketch

In the two arm case, one should set γ(T ) (cid:44)

We sketch the proof of Theorem 4 in the two-action case (|A| = 2), which has a much simpler proof that
preserves the main ideas. The technical machinery needed for the general case is much more sophisticated,
and we brieﬂy describe some of these steps at the end of this section, with a complete proof in the Appendix.
λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ) and λ = O(1),
which differs slightly from the algorithm pseudocode for the more general case. Additionally, note that with
two actions, the uniform distribution over At is always feasible for Problem (3). Speciﬁcally, if the ﬁltered
set has cardinality 1, we simply play that action deterministically, otherwise we play one of the two actions
uniformly at random.

√

The proof has three main steps. First we analyze the orthogonalized regression estimator deﬁned in (4).
Second, we study the action selection mechanism and relate the regret incurred to the error bound for the
orthogonalized estimator. Finally, using a somewhat standard potential argument, we show how this leads to
T -type regret bound. For the proof, let ˆθt, Γt be the estimator and covariance matrix used on round t,
a
both based on t − 1 samples.

√

For the estimator, we prove the following lemma for the two action case. The main technical ingredient
is a self-normalized inequality for vector-valued martingales, which can be obtained using ideas from de la
Peña et al. [15].

Lemma 5. Under Assumption 1 and Assumption 2, let K = 2 and γ(T ) (cid:44)
Then, with probability at least 1 − δ, the following holds simultaneously for all t ∈ [T ]:

√

λ+(cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ).

3Technically it is actually smaller by a factor of (1 − p).

(cid:107)ˆθt − θ(cid:107)Γt ≤ γ(T ).

8

Proof. Using the deﬁnitions and Assumption 1, it is not hard to re-write

ˆθt = Γ−1

t (Γt − λI)θ + Γ−1

t

Zτ ζτ ,

t−1
(cid:88)

τ =1

where Zτ (cid:44) zτ,aτ − µτ and ζτ (cid:44) (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ . Further deﬁne St (cid:44) (cid:80)t−1
the triangle inequality the error is at most

τ =1 Zτ ζτ . Then, applying

(cid:107)ˆθt − θ(cid:107)Γt ≤ (cid:107)λθ(cid:107)Γ−1

t

+ (cid:107)St(cid:107)Γ−1

.

t

√

λ since Γt (cid:23) λI. To control the second term, we need to use a self-normalized
The ﬁrst term here is at most
concentration inequality, since Zτ is a random variable, and the normalizing term Γt = λI + (cid:80)t−1
τ =1 Zτ Z (cid:62)
τ
depends on the random realizations. In Lemma 10 in the appendix, we prove that with probability at least
1 − δ, for all t ∈ [T ]

(cid:107)St(cid:107)2

Γ−1
t

≤ 9d log(1 + T /(dλ)) + 18 log(T /δ).

(5)

The lemma follows from straightforward calculations.

Before proceeding, it is worth commenting on the difference between our self-normalized inequality (5)
and a slightly different one used by Abbasi-Yadkori et al. [1] for the linear case. In their setup, they have
that ζτ is conditionally centered and sub-Gaussian, which simpliﬁes the argument since after ﬁxing the Zτ s
(and hence Γt), the randomness in the ζτ s sufﬁces to provide concentration. In our case, we must use the
randomness in Zτ itself, which is more delicate, since Zτ affects the numerator St, but also the normalizer
Γt. In spite of this additional technical challenge, the two self-normalized processes admit similar bounds.

Next, we turn to the action selection step, where recall that either a single action is played deterministically,

or the actions are played uniformly at random.
Lemma 6. Let µt (cid:44) Ea∼πtzt,a where πt is the solution to (3), and assume that the conclusion of Lemma 5
holds. Then with probability at least 1 − δ

Reg(T ) ≤ (cid:112)2T log(1/δ) + 2γ(T )

T
(cid:88)

(cid:114)

t=1

tr(Γ−1

t Cov
b∼πt

(zt,b)).

Proof. We ﬁrst study the instantaneous regret, taking expectation over the random action. For this, we must
consider two cases. First, with Lemma 5, if |At| = 1, we argue that the regret is actually zero. This follows
from the Cauchy-Schwarz inequality since assuming At = {a} we get

(cid:104)θ, zt,a − zt,b(cid:105) ≥ (cid:104)ˆθt, zt,a − zt,b(cid:105) − γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

t

which is non-negative using the fact that b was eliminated. Therefore a is the optimal action and we incur no
regret. Since πt has no covariance, the upper bound holds.

On the other rounds, we set πt = Unif({a, b}) and hence µt = (zt,a + zt,b)/2. Assuming again that a is

the optimal action, the expected regret is

(cid:104)θ, zt,a − µt(cid:105) =

(cid:104)θ, zt,a − zt,b(cid:105) ≤

1
2

(cid:104)ˆθt, zt,a − zt,b(cid:105) + γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

t

(cid:17)

≤ γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

≤ 2γ(T )

t

t Cov
b∼πt

(zt,b)).

(cid:16)

1
2
(cid:114)
tr(Γ−1

9

Here the ﬁrst inequality uses Cauchy-Schwarz, the second uses (2), since neither action was eliminated, and
the third uses (3). This bounds the expected regret, and the lemma follows by Azuma’s inequality.

The last step of the proof is to control the sequence

T
(cid:88)

(cid:114)

t=1

tr(Γ−1

t Cov
b∼πt

(zt,b)).

First, recall that

(zt,b) (cid:44) Eb∼πt

(cid:2)(zt,b − µt)(zt,b − µt)(cid:62)(cid:3)

Cov
b∼πt

with µt (cid:44) Eb∼πt[zt,b]. Since in the two-arm case πt either chooses an arm deterministically or uniformly
randomizes between the two arms, the following always holds:

(zt,b) = (zt,at − µt)(zt,at − µt)(cid:62).

Cov
b∼πt

It follows that Γt+1 (cid:44) Γt + Covb∼πt (zt,b), and with Γ1 (cid:44) λI, the standard potential argument for online
ridge regression applies. We state the conclusion here, and provide a complete proof in the appendix.

Lemma 7. Let Γt, πt be deﬁned as above and deﬁne Mt (cid:44) (zt,at − µt)(zt,at − µt)(cid:62). Then

T
(cid:88)

(cid:113)

t=1

tr(Γ−1

t Mt) ≤ (cid:112)dT (1 + 1/λ) log(1 + T /(dλ)).

Combining the three lemmas establishes a regret bound of

Reg(T ) ≤ O

√
(cid:16)(cid:112)T d log(T /δ) log(T /d) + d

T log(T /d)

(cid:17)

with probability at least 1 − δ in the two-action case.

Extending to many actions. Several more technical steps are required for the general setting. First, the
martingale inequality used in Lemma 5 requires that the random vectors are symmetric about the origin.
This is only true for the two-action case, and in fact a similar inequality does not hold in general for the
non-symmetric situation that arises with more actions. In the non-symmetric case, both the empirical and the
population covariance must be used in the normalization, so the analogue of (5) is instead

(cid:107)St(cid:107)2

(Γt+EΓt)−1 ≤ 27d log(1 + 2T /d) + 54 log(4T /δ).

On the other hand, the error term for our estimator depends only on the empirical covariance Γt. To correct
for the discrepancy, we use a covering argument4 to establish

λI + Γt (cid:23) (λ − 6d log(T /δ))I + (Γt + EΓt)/3.

With this semideﬁnite inequality, we can translate from the Mahalanobis norm in the weaker self-normalized
bound to one with just Γt, which controls the error for the estimator.

We also argue that problem (3) is always feasible, which is the contents of the following lemma.

4For technical reasons, the Matrix Bernstein inequality does not sufﬁce here since it introduces a dependence on the maximal variance.

See Appendix for details.

10

Figure 1: Synthetic experiments with d = 10, K = 2. Left: A linear environment where action-features are
uniformly from the unit sphere. Center: A confounded environment with features from the sphere. Right: A
confounded environment with features from the sphere intersected with the positive orthant. Algorithms are
BOSE, OFUL [1], ILTCB [3], EPSGREEDY [21], and THOMPSON [4]. Agnostic approaches use a linear
policy class.

Lemma 8. Problem (3) is convex and always has a feasible solution. Speciﬁcally, for any vectors z1, . . . , zn ∈
Rd and any positive deﬁnite matrix M , there exists a distribution w ∈ ∆([n]) with mean µw (cid:44) Eb∼wzb such
that

∀i ∈ [n], (cid:107)zi − µw(cid:107)2

M ≤ tr(M Cov
b∼w

(zb)).

The proof uses convex duality. Integrating these new arguments into the proof for the two-action case

leads to Theorem 4.

5 Experiments

We conduct a simple experiment to compare BOSE with several other approaches5. We simulate three different
environments that follow the semiparametric contextual bandits model with d = 10, K = 2. In the ﬁrst
setting the reward is linear and the action features are drawn uniformly from the unit sphere. In the latter two
settings, we set ft(xt) = − maxa(cid:104)θ, zt,a(cid:105), which is related to the construction in the proof of Proposition 3.
One of these semiparametric settings has action features sampled from the unit sphere, while for the other, we
sample from the intersection of the unit sphere and the positive orthant.

In Figure 1, we plot the performance of Algorithm 1 against four baseline algorithms: (1) OFUL:
the optimistic algorithm for linear stochastic bandits [1], (2) THOMPSON sampling for linear contextual
bandits [4], (3) EPSGREEDY: the (cid:15)-greedy approach [21] with a linear policy class, (4) ILTCB: a more
sophisticated agnostic algorithm [3] with linear policy class. The ﬁrst algorithm is deterministic, so can have
linear regret in our setting, but is the natural baseline and one we hope to improve. Thompson Sampling is
another natural baseline, and a variant was used by Greenewald et al. [19] in essentially the same setting
as ours. The latter two have (Kd)1/3T 2/3 and
KdT regret bounds respectively under our assumptions,
but require solving cost-sensitive classiﬁcation problems, which are NP-hard in general. Following prior
empirical evaluations [20], we use a surrogate loss formulation based on square loss minimization in the
implementation.

√

The results of the experiment are displayed in Figure 1, where we plot the cumulative regret against the
number of rounds T . All algorithms have a single parameter that governs the degree of exploration. In BOSE

5Our code is publicly available at http://github.com/akshaykr/oracle_cb/.

11

and OFUL, this is the constant γ(T ) in the conﬁdence bound, in THOMPSON it is the variance of the prior,
and in ILTCB and EPSGREEDY it is the amount of uniform exploration performed by the algorithm. For
each algorithm we perform 10 replicates for each of 20 values of the corresponding parameter, and we plot
the best average performance, with error bars corresponding to ±2 standard deviations.

In the linear experiment (Figure 1, left panel), BOSE performs the worst, but is competitive with the
agnostic approaches, demonstrating a price to pay for robustness. The experimental setup in the center panel
is identical except with confounding, and BOSE is robust to this confounding, with essentially the same
performance, while the three baselines degrade dramatically. Finally, when the features lie in the positive
orthant (right panel), OFUL degrades further, while BOSE remains highly effective.

Regarding the baselines, we make two remarks:

1. Intuitively, the positive orthant setting is more challenging for OFUL since there is less inherent

randomness in the environment to overcome the confounding effect.

2. The agnostic approaches, despite strong regret guarantees, perform somewhat poorly in our experiments,
and we believe this for three reasons. First, our surrogate-loss implementation is based on an implicit
realizability assumption, which is not satisﬁed here. Second, we expect that the constant factors in their
regret bounds are signiﬁcantly larger than those of BOSE or OFUL. For computational reasons, we
only solve the optimization problem in ILTCB every 50 rounds, which causes a further constant factor
increase in the regret.

Overall, while BOSE is worse than other approaches in the linear environment, the experiment demonstrates
that when the environment is not perfectly linear, approaches based on realizability assumptions (either
explicitly like in OFUL, or implicitly like in implementations of ILTCB and EPSGREEDY), can fail. We
emphasize that linear environments are rare in practice, and such assumptions are typically impossible to
verify. We therefore believe that trading off a small loss in performance in the specialized linear case for
signiﬁcantly more robustness, as BOSE demonstrates, is desirable.

6 Discussion

This paper studies a generalization of the linear stochastic bandits setting, where rewards are confounded
by an adaptive adversary. Our new algorithm, BOSE, achieves the optimal regret, and also matches (up to
logarithmic factors) the best algorithms for the linear case. Our empirical evaluation shows that BOSE offers
signiﬁcantly more robustness than prior approaches, and performs well in several environments.

12

A Using the OLS Estimator

Here we construct an example problem to demonstrate how using the standard OLS estimator can fail in
the semiparametric setting. While not a comprehensive proof against all asymptotically biased approaches,
similar examples can be constructed for related estimators.

Consider a two-dimensional problem with two actions and no stochastic noise, where θ = e2, the second
standard basis vector. On the even rounds, the actions are z1 = (1, 1), z2 = (1, 1/3) and the confounding
term is f = −1. On the odd rounds, the actions are z1 = z2 = (1, 0) and the confounding term is f = 1. For
any policy for selecting actions, the OLS estimator before round t (for even t) is the solution to the following
optimization problem:

minimizew∈R2 α(w1 + w2)2 + (1 − α)(w1 + w2/3 + 2/3)2 + (w1 − 1)2 = L(w)

where α ∈ [0, 1] corresponds to the fraction of the even rounds (up to round t) where the policy chose z1.
We will argue that, for any α, the solution to this problem ˆw has ˆw2 < 0. Since there is no stochastic noise,
there is no need for conﬁdence bounds once the covariance is full rank, which happens after the second round.
Together, this implies that any sensible policy based on ˆw will prefer z2 to z1 on the even rounds, but z1 yields
higher reward by a ﬁxed constant. Thus using OLS in a conﬁdence-based approach leads to linear regret.

We now show that ˆw2 is strictly negative. We have

∂L(w)
∂w1
∂L(w)
∂w2

= 2α(w1 + w2) + 2(1 − α)(w1 + w2/3 + 2/3) + 2(w1 − 1),

= 2α(w1 + w2) +

(1 − α)(w1 + w2/3 + 2/3).

2
3

Setting both equations equal to zero yields the following system:

4w1 + (2/3 + 4α/3)w2 = 2/3 + 4α/3,

(2/3 + 4α/3)w1 + (2/9 + 16α/9)w2 = 4α/9 − 4/9.

The solution to this system is

w1 =

(2α + 1)2
−4α2 + 12α + 1

,

w2 =

4α2 + 5
4α2 − 12α − 1

,

provided that 4α2 (cid:54)= 12α + 1, which is not possible with α ∈ [0, 1]. In the interval [0, 1] we have that
4α2 − 12α − 1 < 0, and hence w2 < 0. Thus, the OLS estimator incorrectly predicts that z2 receives higher
reward than z1 on the even rounds. Since conﬁdence intervals are not needed, the algorithm suffers linear
reget.

B Proof of Proposition 3

We consider two possible values for the true parameter: θ1 = e1 ∈ R2, θ2 = e2 ∈ R2. At all rounds, the
context xt = {e1, e2} contains just two actions, and we further assume that the noise term ξt = 0 almost
surely. Since the action at is a deterministic function of the history, it can also be computed by the adaptive
adversary at the beginning of the round, and the adversary chooses

ft(xt) = −1{at = argmax

(cid:104)θ, zt,a(cid:105)}.

a

13

We show that rt(at) = 0 for all rounds t. Assume the parameter is θ1 so the optimal action is a(cid:63)
t = e1 and
the suboptimal action e2 has (cid:104)θ, e2(cid:105) = 0. If the learner chooses action e2, then the adversary sets ft(xt) = 0,
so rt(at) = 0. On the other hand, if the learner chooses action e1, then the adversary sets ft(xt) = −1 so
the reward is also zero. Similarly, if θ = θ2, the observed reward is always zero. Since the algorithm is
deterministic, it behaves identically regardless of whether the parameter is θ1 or θ2. In one of these instances
the algorithm must choose the suboptimal action at least T /2 times, leading to the lower bound.

C Proof for the Two-Action Case

We ﬁrst focus on the simpler two action case. Before turning to the main analysis, we prove two supporting
lemmas. The ﬁrst is an algebraic inequality relating matrix determinants to traces. This inequality also
appears in Abbasi-Yadkori et al. [1].

Lemma 9. Let X1, . . . , Xn denote vectors in Rd with (cid:107)Xi(cid:107)2 ≤ L for all i ∈ [n]. Deﬁne Γ (cid:44) λI +
(cid:80)n

i=1 XiX (cid:62)

i . Then

Proof. We will apply the following standard argument:

det(Γ) ≤ (λ + nL2/d)d.

det(Γ)1/d ≤

tr(Γ) =

tr(λI) +

tr(XiX (cid:62)

i ) = λ +

(cid:107)Xi(cid:107)2

2 ≤ λ + nL2/d.

1
d

1
d

1
d

n
(cid:88)

i=1

1
d

n
(cid:88)

i=1

The ﬁrst step is a spectral version of the AM-GM inequality and the remaining steps use linearity of the trace
operator and the boundedness conditions.

The second lemma is a new self-normalized concentration inequality for vector valued martingales.

Lemma 10 (Symmetric self-normalized inequality). Let {Ft}T
t=1 be a
stochastic process with Zt ∈ Rd and ζt ∈ R such that (1) (Zt, ζt) is Ft measurable, (2) |ζt| ≤ M for all
t ∈ [T ], (3) Zt ⊥⊥ ζt|Ft, (4) E[Zt|Ft] = 0, and (5) for all x ∈ Rd, L((cid:104)x, Zt(cid:105) | Ft) = L(−(cid:104)x, Zt(cid:105) | Ft)
where L denotes the probability law, so that Zt is conditionally symmetric. Let Σ (cid:44) (cid:80)T
t . Then for
any positive deﬁnite matrix Q we have

t=1 be a ﬁltration and let {(Zt, ζt)}T

t=1 ZtZ (cid:62)

P





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

Ztζt

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(Q+M 2Σ)−1

≥ 2 log

(cid:32)

(cid:115)

1
δ

det(Q + M 2Σ)
det(Q)

(cid:33)

 ≤ δ.

Proof. The proof follows the recipe in de la Peña et al. [15] (See also de la Peña et al. [14] for a more
comprehensive treatment including the univariate case). We start by applying the Chernoff method. Let
¯Σ (cid:44) Q + M 2Σ. We can write

P





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯Σ−1

Ztζt

≥ 2 log

(cid:32)

(cid:115)

1
δ

det( ¯Σ)
det(Q)

(cid:33)





 = P

exp



1
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

Ztζt



exp



(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯Σ−1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
2

T
(cid:88)

t=1



 ≥

(cid:115)





det( ¯Σ)
det(Q)

1
δ

Ztζt







 .

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯Σ−1



(cid:115)

≤ δE



det(Q)
det( ¯Σ)

14

Therefore, if we prove that this latter expectation is at most one, we will arrive at the conclusion. A similar
statement appears in Theorem 1 of de la Peña et al. [15], but our process is slightly different due to the
presence of ζt. To bound this latter expectation, ﬁx some λ ∈ Rd and consider an exponentiated process with
the increments

(cid:18)

Dλ
t

(cid:44) exp

(cid:104)λ, Ztζt(cid:105) −

M 2(cid:104)λ, Zt(cid:105)2
2

(cid:19)

.

Observe that E[Dλ

t |Ft] ≤ 1 since by the conditional symmetry of Zt, we have

E[Dλ

t |Ft] = E (cid:2)E (cid:2)Dλ

t | Ft, ζt

(cid:20)

(cid:20)

E

exp

= E

(cid:20)

(cid:20)

E

exp

= E

(cid:20)

(cid:20)

E

exp

≤ E

(cid:3)

(cid:3) | Ft
(cid:18) −M 2(cid:104)λ, Zt(cid:105)2
2
(cid:18) −M 2(cid:104)λ, Zt(cid:105)2
2
(cid:18) −M 2(cid:104)λ, Zt(cid:105)2
2

1
2

(cid:19)

(cid:19)

× cosh((cid:104)λ, Ztζt) | Ft, ζt

| Ft

(cid:21)

(cid:21)

(cid:19)

+

(cid:104)λ, Ztζt(cid:105)2
2

(cid:21)

(cid:21)

| Ft, ζt

| Ft

≤ 1.

×

(exp((cid:104)λ, Ztζt(cid:105)) + exp(−(cid:104)λ, Ztζt(cid:105)) | Ft, ζt

| Ft

(cid:21)

(cid:21)

This argument ﬁrst uses the conditional symmetry of Zt and the conditional independence of Zt, ζt, then the
identity (ex + e−x)/2 = cosh(x) and ﬁnally the analytical inequality cosh(x) ≤ ex2/2. Finally in the last
step we use the bound |ζt| ≤ M . This implies that the martingale U λ
τ is a super-martingale with
t
E[U λ

t ] ≤ 1 for all t, since by induction

τ =1 Dλ

(cid:44) (cid:81)t

E[U λ

t ] = E[U λ

t−1

E[Dλ

t |Ft]] ≤ E[U λ

t−1] ≤ . . . ≤ 1.

(6)

Now we apply the method of mixtures. In a standard application of the Chernoff method, we would choose
λ to maximize E[U λ
T ], but since we still have an expectation, we cannot swap expectation and maximum.
Instead, we integrate the inequality E[U λ
T ] ≤ 1, which holds for any λ, against λ drawn from a Gaussian
distribution with covariance Q−1. By Fubini’s theorem, we can swap the expectations to obtain

1 ≥ Eλ∼N (0,Q−1)E[U λ

T ] = E

(cid:90)

T (2π)−d/2(cid:112)det(Q) exp(−λ(cid:62)Qλ/2)dλ
U λ
(cid:32) T

M 2λ(cid:62)((cid:80)T

t )λ + λ(cid:62)Qλ

t=1 ZtZ (cid:62)
2

(cid:33)

dλ

(cid:88)

t=1

(cid:18)

= E

(2π)−d/2(cid:112)det(Q) exp

(cid:104)λ, Ztζt(cid:105) −

= E

(2π)−d/2(cid:112)det(Q) exp

(cid:104)λ, S(cid:105) −

M 2λ(cid:62)Σλ + λ(cid:62)Qλ
2

(cid:19)

dλ,

(cid:90)

(cid:90)

where S (cid:44) (cid:80)T
can be rewritten as

t=1 Ztζt and recall that Σ (cid:44) (cid:80)T

t=1 ZtZ (cid:62)

t . By completing the square, the term in the exponent

(cid:104)λ, S(cid:105) −

M 2λ(cid:62)Σλ + λ(cid:62)Qλ
2

1
2

=

(cid:0)−(λ − ¯Σ−1S)(cid:62) ¯Σ(λ − ¯Σ−1S) + S(cid:62) ¯Σ−1S(cid:1) ,

where recall that ¯Σ (cid:44) M 2Σ + Q. As such we obtain

1 ≥ E

(cid:20)
exp (cid:0)S(cid:62) ¯Σ−1S/2(cid:1) ×

(cid:90)

(2π)−d/2(cid:112)det(Q) exp

(cid:18) −(λ − ¯Σ−1S)(cid:62) ¯Σ(λ − ¯Σ−1S)
2

(cid:19)(cid:21)

dλ

(cid:115)

= E

det(Q)
det( ¯Σ)

exp (cid:0)S(cid:62) ¯Σ−1S(cid:1) .

15

This proves the lemma.

Equipped with the two lemmas, we can now turn to the analysis of the inﬂuence-adjusted estimator.

Lemma 11 (Restatement of Lemma 5). Under Assumption 1 and Assumption 2, with probability at least
1 − δ, the following holds simultaneously for all t ∈ [T ]:

(cid:107)ˆθt − θ(cid:107)Γt ≤

√

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ).

Proof. Recall that we deﬁne ˆθt, Γt to be the estimator and matrix used in round t, based on t − 1 examples.
Fixing a round t, we start by expanding the deﬁnition of ˆθt. We use the shorthand zτ (cid:44) zτ,aτ , µτ (cid:44)
Eb∼πτ [zτ,b], and rτ (cid:44) rτ (aτ ).

t−1
(cid:88)

τ =1

t−1
(cid:88)

τ =1

ˆθt = Γ−1

t

(zτ − µτ )rτ = Γ−1

t

(zτ − µτ )((cid:104)θ, zτ (cid:105) + fτ (xτ ) + ξτ )

= Γ−1
t

(zτ − µτ )((cid:104)θ, zτ − µτ (cid:105) + (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ )

= (Γt)−1(Γt − λI)θ + Γ−1

t

(zτ − µτ )((cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ ).

t−1
(cid:88)

τ =1

t−1
(cid:88)

τ =1

Let Zτ (cid:44) zτ − µτ and ζτ (cid:44) (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ . With this expansion, we can write

(cid:107)ˆθt − θ(cid:107)Γt = (cid:107) − λΓ−1

t θ + Γ−1

t

Zτ ζτ (cid:107)Γt ≤ (cid:107)λθ(cid:107)Γ−1

+

t

Zτ ζτ

t−1
(cid:88)

τ =1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

√

≤

λ +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

Zτ ζτ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

.

To ﬁnish the proof, we apply Lemma 10 to this last term. To verify the preconditions of the lemma, let
Fτ (cid:44) σ(x1, . . . , xτ , a1, . . . , aτ −1, ξ1, . . . , ξτ −1) denote the σ-algebra corresponding to the τ th round, after
observing the context xτ . Then the policy πτ and hence the action aτ are Fτ measurable and so is the noise
term ξτ . Therefore, Zτ = zτ,aτ − Ea∼πτ [zτ,a] is measurable, which veriﬁes the ﬁrst precondition. Using the
boundedness properties in Assumption 2, we know that |ζτ | ≤ 3 (cid:44) M , and by construction of the random
variables, we have Zτ ⊥⊥ ζτ |Fτ and E [Zτ |Fτ ] = 0. Finally, for the symmetry property, either Zτ |Fτ ≡ 0
if one action is eliminated, or otherwise we have µτ = 1
2 (zτ,1 + zτ,2) since there are only two actions. In
this case the random variable Zτ |Fτ = (cid:15)τ (zτ,1 − zτ2 )/2 where (cid:15)τ is a Rademacher random variable. By
inspection this is clearly conditionally symmetric. As such, we may apply Lemma 10, which reveals that with
probability at least 1 − δ,

Zτ ζτ

= M 2

Zτ ζτ

≤ 2M 2 log

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Γ−1
t

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

τ =1
(cid:18)(cid:113)

(M 2Γt)−1

(cid:19)

= 18 log

λ−d det(Γt)/δ

.

(cid:32)

(cid:115)

1
δ

det(M 2Γt)
det(M 2λI)

(cid:33)

The inequality here is Lemma 10 with Q = M 2λI, and for the last equality we use that det(cQ) = cd det(Q)
for a d × d positive semideﬁnite matrix Q. As two ﬁnal steps, we apply Lemma 9 and take a union bound

16

over all rounds T . Combining these, we get that for all T ,
(cid:115)

(cid:107)ˆθt − θ(cid:107)Γt ≤

λ +

Zτ ζτ

≤

λ +

18

log(

λ−d det(Γt)) + log(T /δ)

√

(cid:18)

(cid:113)

(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

√

√

≤

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ).

Therefore, with γ(T ) (cid:44)

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ) we can apply Lemma 6 to bound

√

the regret by

Reg(T ) ≤ (cid:112)2T log(1/δ) + 2γ(T )

T
(cid:88)

(cid:114)

t=1

tr(Γ−1

t Cov
b∼πt

(zt,b)).

Via a union bound, this inequality holds with probability at least 1 − 2δ. To ﬁnish the proof we need to
analyze this latter term. This is the contents of the following lemma. A related statement, with a similar proof,
appears in Abbasi-Yadkori et al. [1].
Lemma 12. Let X1, . . . , XT be a sequence of vectors in Rd with (cid:107)Xt(cid:107)2 ≤ 1 and deﬁne Γ1 (cid:44) λI, Γt (cid:44)
Γt−1 + Xt−1X (cid:62)

t−1. Then

tr(Γ−1

t XtX (cid:62)

t ) ≤ (cid:112)T d(1 + 1/λ) log(1 + T /(dλ)).

Proof. First, apply the Cauchy-Schwarz inequality to the left hand side to obtain

tr(Γ−1

t XtX (cid:62)

t ) ≤

tr(Γ−1

t XtX (cid:62)

t ).

(cid:118)
(cid:117)
(cid:117)
(cid:116)

√

T

T
(cid:88)

T =1

T
(cid:88)

(cid:113)

t=1

T
(cid:88)

(cid:113)

t=1

For the remainder of the proof we work only with the second term. Let us start by analyzing a slightly
different quantity, tr(Γ−1
t ). By concavity of log det(M ), we have

t+1XtX (cid:62)

log det(Γt) ≤ log det(Γt+1) + tr(Γ−1

t+1(Γt − Γt+1)),

which implies

T
(cid:88)

t=1

tr(Γ−1

t+1XtX (cid:62)

t ) = tr(Γ−1

t+1(Γt+1 − Γt)) ≤ log det(Γt+1) − log det(Γt)

As such, we obtain a telescoping sum

tr(Γ−1

t+1XtX (cid:62)

t ) ≤ log det(ΓT +1) − log det(Γ1) ≤ d log(λ + T /d) − d log λ = d log(1 + T /(dλ))

The ﬁrst inequality here uses the concavity argument and the second uses Lemma 9. To ﬁnish the proof, we
must translate back to Γ−1

. For this, we use the Sherman-Morrison-Woodbury identity, which reveals that

t

X (cid:62)

t Γ−1

t+1Xt = X (cid:62)

t (Γt + XtX (cid:62)

t )−1Xt = X (cid:62)
t

Γ−1

t −

(cid:32)

t Γ−1
t

Γ−1
t XtX (cid:62)
1 + (cid:107)Xt(cid:107)2

Γ−1
t

(cid:33)

Xt

=

(cid:107)Xt(cid:107)2
Γ−1
t
1 + (cid:107)Xt(cid:107)2

Γ−1
t

≥ (1 + 1/λ)−1(cid:107)Xt(cid:107)2

.

Γ−1
t

17

Here in the last step we use that (cid:107)Xt(cid:107)2

≤ (cid:107)Xt(cid:107)2

(λI)−1 ≤ 1/λ. Overall, we obtain

Γ−1
t

T
(cid:88)

t=1

tr(Γ−1

t XtX (cid:62)

t ) ≤ (1 + 1/λ)d log(1 + T /(dλ)),

and combined with the ﬁrst application of Cauchy-Schwarz, this proves the lemma.

Combining the lemmas, we have that with probability at least 1 − 2δ, the regret is at most

Reg(T ) ≤ (cid:112)2T log(1/δ) + 2γ(T )(cid:112)T d(1 + 1/λ) log(1 + T /(dλ))
= (cid:112)2T log(1/δ) + 2(cid:112)T d(1 + 1/λ) log(1 + T /(dλ))

(cid:16)√

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ)

(cid:17)

.

With λ = 1, this bound is O

√
(cid:16)(cid:112)T d log(T /δ) log(T /d) + d

T log(T /d)

(cid:17)
.

D Proof for the General Case

We now turn to the more general case. We need several additional lemmas.

Lemma 13 (Restatement of Lemma 8). Problem (3) is convex and always has a feasible solution. Speciﬁcally,
for any vectors z1, . . . , zn ∈ Rd and any positive deﬁnite matrix M , there exists a distribution w ∈ ∆([n])
with mean µw = Eb∼w[zb] such that

Proof. We analyze the minimax program

∀i ∈ [n],

(cid:107)zi − µw(cid:107)2

M ≤ tr(M Cov
b∼w

(zb)).

min
w∈∆([n])

max
i∈[n]

(cid:107)zi − µw(cid:107)2

M − tr(M Cov
w

(z)).

The goal is to show that the value of this program is non-negative, which will prove the result. Expanding the
deﬁnitions, we have

min
w∈∆([n])

max
i∈[n]

(cid:107)zi − µw(cid:107)2

M − tr(M Cov
w

(z))

= min

w∈∆([n])

max
v∈∆([n])

= min

v∈∆([n])

max
w∈∆([n])


(cid:107)zi − µw(cid:107)2

(cid:88)

vi


(cid:107)zi − µw(cid:107)2

(cid:88)

vi

i

i

M + µ(cid:62)

wM µw −

wjz(cid:62)

j M zj



(cid:88)

M + µ(cid:62)

wM µw −

wjz(cid:62)

j M zj

 .

j

j

(cid:88)





The last equivalence here is Sion’s Minimax Theorem [31], which is justiﬁed since both domains are compact
convex subsets of Rn and since the objective is linear in the maximizing variable v, and convex in the
minimizing variable w. This convexity is clear since µw is a linear in w, and hence the ﬁrst two terms are
convex quadratics (since M is positive deﬁnite), while the third term is linear in w. Thus Sion’s theorem lets
us swap the order of the minimization and maximization.

18

Now we upper bound the solution by setting w = v. This gives


(cid:107)zi − µv(cid:107)2

(cid:88)

vi

M + µ(cid:62)

v M µv −

vjz(cid:62)

j M zj



(cid:88)

j


(zi − µv)(cid:62)M (zi − µv) + µ(cid:62)

(cid:88)

vi

v M µv −

vjz(cid:62)

j M zj

 = 0.





(cid:88)

j

≤ max

v∈∆([n])

= max

v∈∆([n])

i

i

To prove the analog of Lemma 10, we need several additional tools. First, we use Freedman’s inequality

to derive a positive-semideﬁnite inequality relating the sample covariance matrix to the population matrix.

Lemma 14. Let X1, . . . , Xn be conditionally centered random vectors in Rd adapted to a ﬁltration {Ft}n
with (cid:107)Xi(cid:107)2 ≤ 1 almost surely. Deﬁne ˆΣ (cid:44) (cid:80)n
E[XiX (cid:62)
i
probability at least 1 − δ, the following holds simultaneously for all unit vectors v ∈ Rd:

i and Σ (cid:44) (cid:80)n

t=1
| Fi]. Then, with

i=1 XiX (cid:62)

i=1

v(cid:62)Σv ≤ 2v(cid:62) ˆΣv + 9d log(9n) + 8 log(2/δ).

This lemma is related to the Matrix Bernstein inequality, which can be used to control (cid:107)Σ − ˆΣ(cid:107)2, a
quantity that is quite similar to what we are controlling here. The Matrix Bernstein inequality can be used to
derive a high probability bound of the form

∀v ∈ Rd, (cid:107)v(cid:107)2 = 1,

v(cid:62)(Σ − ˆΣ)v ≤

(cid:107)Σ(cid:107)2 + c log(dn/δ),

1
2

for a constant c > 0. On one hand, this bound is stronger than ours since the deviation term depends only
logarithmically on the dimension. However, the variance term involves the spectral norm rather than a quantity
that depends on v as in our bound. Thus, Matrix Bernstein is worse when Σ is highly ill-conditioned, and
since we have essentially no guarantees on the spectrum of Σ, our specialized inequality, which is more
adaptive to the speciﬁc direction v, is crucial. Moreover, the worse dependence on d is inconsequential, since
the error will only appear in a lower order term.

Proof. First consider a single unit vector v ∈ Rd, we will apply a covering argument at the end of the proof.
By assumption, the sequence of sums {(cid:80)τ
τ =1 is a martingale, so we may
apply Freedman’s inequality [18, 6], which states that with probability at least 1 − δ

i=1 v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)
i

| Fi])v}n

|v(cid:62)( ˆΣ − Σ)v| ≤ 2

Var(v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)

i

| Fi])v | Fi) log(2/δ) + 2 log(2/δ).

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

Let us now upper bound the variance term: for each i,

Var(v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)
i

| Fi])v | Fi) ≤ E[(v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)
i

≤ E[(v(cid:62)Xi)4 | Fi] ≤ v(cid:62)E[XiX (cid:62)
i

| Fi] | Fi)v)2 | Fi]
| Fi]v,

where the last inequality follows from the fact that (cid:107)Xi(cid:107)2 ≤ 1 and (cid:107)v(cid:107)2 ≤ 1. Therefore, the cumulative
conditional variance is at most v(cid:62)Σv. Plugging this into Freedman’s inequality gives us

|v(cid:62)( ˆΣ − Σ)v| ≤ 2

v(cid:62)Σv log(2/δ) + 2 log(2/δ).

(cid:113)

19

Now, using the fact that 2

ab ≤ αa + b/α for any α > 0, with the choice α = 1/2, we get

√

|v(cid:62)( ˆΣ − Σ)v| ≤ v(cid:62)Σv/2 + 4 log(2/δ).

Re-arranging, this implies

v(cid:62)Σv ≤ 2v(cid:62) ˆΣv + 8 log(2/δ),

(7)

which is what we would like to prove, but we need it to hold simultaneously for all unit vectors v.

To do so, we now apply a covering argument. Let N be an (cid:15)-covering of the unit sphere in the projection
pseudo-metric d(u, v) = (cid:107)uu(cid:62) − vv(cid:62)(cid:107)2, with covering number N ((cid:15)). Then via a union bound, a version
of (7) holds simultaneously for all v ∈ N , where we rescale δ → δ/N ((cid:15)).

Consider another unit vector u and let v be the covering element. We have

u(cid:62)Σu = tr(Σ(uu(cid:62) − vv(cid:62))) + v(cid:62)Σv ≤ tr(Σ(uu(cid:62) − vv(cid:62))) + 2v(cid:62) ˆΣv + 8 log(2N ((cid:15))/δ)

= tr((Σ − 2 ˆΣ)(uu(cid:62) − vv(cid:62))) + 2u(cid:62) ˆΣu + 8 log(2N ((cid:15))/δ)
≤ (cid:107)Σ − 2 ˆΣ(cid:107)(cid:63)(cid:15) + 2u(cid:62) ˆΣu + 8 log(2N ((cid:15))/δ).

Here (cid:107) · (cid:107)(cid:63) denotes the nuclear norm, which is dual to the spectral norm (cid:107) · (cid:107)2. Since all vectors are bounded
by 1, we obtain

(cid:107)Σ − 2 ˆΣ(cid:107)(cid:63) ≤ dλmax(Σ − 2 ˆΣ) ≤ 3dn.

Overall, the following bound holds simultaneously for all unit vectors v ∈ Rd, except with probability at
most δ:

v(cid:62)Σv ≤ 3dn(cid:15) + 2v(cid:62) ˆΣv + 8 log(2N ((cid:15))/δ).

The last step of the proof is to bound the covering number N ((cid:15)). For this, we argue that a covering
of the unit sphere in the Euclidean norm sufﬁces, and by standard volumetric arguments, this set has
covering number at most (3/(cid:15))d. To see why this sufﬁces, let u be a unit vector and let v be the covering
element in the Euclidean norm, which implies that (cid:107)u − v(cid:107)2 ≤ (cid:15). Further assume that (cid:104)u, v(cid:105) > 0, which
imposes no restriction since the projection pseudo-metric is invariant to multiplying by −1. By deﬁnition
we also have (cid:104)u, v(cid:105) ≤ 1. Note that the projection norm is equivalent to the sine of the principal angle
between the two subspaces, which once we restrict to vectors with non-negative inner product means that
(cid:107)uu(cid:62) − vv(cid:62)(cid:107)2 = sin ∠(u, v). Now

sin ∠(u, v) = (cid:112)1 − (cid:104)u, v(cid:105)2 = (cid:112)(1 + (cid:104)u, v(cid:105))(1 − (cid:104)u, v(cid:105))

≤ (cid:112)2(1 − (cid:104)u, v(cid:105)) =

(cid:113)

(cid:107)u(cid:107)2

2 + (cid:107)v(cid:107)2

2 − 2(cid:104)u, v(cid:105) = (cid:107)u − v(cid:107)2 ≤ (cid:15).

Using the standard covering number bound, we now have

v(cid:62)Σv ≤ 3dn(cid:15) + 2v(cid:62) ˆΣv + 8d log(3/(cid:15)) + 8 log(2/δ).

Setting (cid:15) = 1/(3n) gives

v(cid:62)Σv ≤ d + 2v(cid:62) ˆΣv + 8d log(9n) + 8 log(2/δ) ≤ 2v(cid:62) ˆΣv + 9d log(9n) + 8 log(2/δ).

20

With the positive semideﬁnite inequality, we can work towards a self-normalized martingale concentration

bound. The following is a restatement of Lemma 7 from de la Peña et al. [15].

Lemma 15 (Lemma 7 of de la Peña et al. [15]). Let {Xi}n
valued random variables adapted to the ﬁltration {Fi}n
Then

i=1 be a sequence of conditionally centered vector-
i=1 and such that (cid:107)Xi(cid:107)2 ≤ B for some constant B.

Un(λ) = exp

λ(cid:62)

Xi − λ(cid:62)

XiX (cid:62)

i + E[XiX (cid:62)

i |Fi]

λ/2

(cid:32)

n
(cid:88)

i=1

(cid:32) n
(cid:88)

i=1

(cid:33)

(cid:33)

is a supermartingale with E[Un(λ)] ≤ 1 for all λ ∈ Rd.

The lemma is related to (6), but does not require that conditional probability law for Xi is symmetric,
which we used previously. To remove the symmetry requirement, it is crucial that the quadratic self-
normalization has both empirical and population terms. With this lemma, the same argument as in the proof
of Lemma 10, yields a self-normalized tail bound.

t=1 be a ﬁltration and let {(Zt, ζt)}T

t=1 be a stochastic process with Zt ∈ Rd and
Lemma 16. Let {Ft}T
ζt ∈ R such that (1) (Zt, ζt) is Ft measurable, (2) |ζt| ≤ M for all t ∈ [T ], (3) Zt ⊥⊥ ζt|Ft, and (4)
E[Zt|Ft] = 0. Let ˆΣ (cid:44) (cid:80)T
T |Ft]. Then for any positive deﬁnite matrix Q
we have

t and Σ (cid:44) (cid:80)T

t=1 ZtZ (cid:62)

E[ZtZ (cid:62)

t=1

P





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

Ztζt

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(Q+M 2( ˆΣ+Σ))−1

≥ 2 log

(cid:115)





1
δ

det(Q + M 2( ˆΣ + Σ))
det(Q)







 ≤ δ.

Proof. The proof is identical to Lemma 10, but uses Lemma 15 in lieu of (6).

We can now analyze the inﬂuence-adjusted estimator.

Lemma 17. Under Assumption 1 and Assumption 2 and assuming that λ ≥ 4d log(9T ) + 8 log(4T /δ), with
probability at least 1 − δ, the following holds simultaneously for all t ∈ [T ]:
√

(cid:107)ˆθt − θ(cid:107)Γt ≤

λ + (cid:112)27d log(1 + 2T /d) + 54 log(4T /δ).

Proof. Using the same argument as in the proof of Lemma 5, we get

(cid:107)ˆθt − θ(cid:107)Γt ≤

√

λ +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

Zτ ζτ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

,

where Zτ (cid:44) zτ − µτ and ζτ (cid:44) (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ , just as before. Now we must control this error
term, for which we need both Lemma 14 and Lemma 16. Apply Lemma 14 to the vectors Zτ , setting
ˆΣt (cid:44) (cid:80)t−1
E[Zτ Zτ | Fτ ]. With probability at least 1 − δ/(2T ), we have that for
all unit vectors v ∈ Rd

τ and Σt (cid:44) (cid:80)t−1

τ =1 Zτ Z (cid:62)

τ =1

v(cid:62)Σtv ≤ 2v(cid:62) ˆΣtv + 9d log(9t) + 8 log(4T /δ) ≤ 2v(cid:62) ˆΣtv + 9d log(9T ) + 8 log(4T /δ).

This implies a lower bound on all quadratic forms involving ˆΣt, which leads to positive semideﬁnite inequality

λI + ˆΣt (cid:23) (λ − 3d log(9T ) − 8/3 log(4T /δ))I + ( ˆΣt + Σt)/3.

21

This means that for any vector v, we have

(cid:107)v(cid:107)2

(λI+ ˆΣt)−1 ≤ (cid:107)v(cid:107)2
≤ 3(cid:107)v(cid:107)2

((λ−3d log(9T )−8/3 log(4T /δ))I+( ˆΣt+Σt)/3)−1
((3λ−9d log(9T )−8 log(4T /δ))I+ ˆΣt+Σt)−1.

Before we apply Lemma 16, we must introduce the range parameter M . Fix a round t and let A (cid:44)
((3λ − 9d log(9T ) − 8 log(4T /δ))I + ˆΣt + Σt) denote the matrix in the Mahalanobis norm. Then,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

A−1

Zτ ζτ

= M 2

Zτ ζτ

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(M 2A)−1

Now apply Lemma 16 with Q (cid:44) M 2(3λ − 9d log(9T ) − 8 log(4T /δ))I. Since we require Q (cid:31) 0, this
requires λ > 3d log(9T ) − 8/3 log(4T /δ), which is satisﬁed under the preconditions for the lemma. Under
this assumption, we get

(cid:107)

t−1
(cid:88)

τ =1

Zτ ζτ (cid:107)2

(λI+ ˆΣt)−1 ≤ 3M 2(cid:107)

Zτ ζτ (cid:107)2

(Q+M 2( ˆΣt+Σt))−1

t−1
(cid:88)

τ =1


≤ 6M 2 log



(cid:115)

4T
δ

det(Q + M 2( ˆΣt + Σt))
det(Q)



 ,

with probability at least 1 − δ/(2T ). With a union bound, the inequality holds simultaneously for all T , with
probability at least 1 − δ.

The last step is to analyze the determinant. Using the same argument as in the proof of Lemma 9, it is not

hard to show that

(cid:32)

det(Q + M 2( ˆΣt + Σt))
det(Q)

(cid:33)1/d

≤ 1 +

2(t − 1)
d(3λ − 9d log(9T ) − 8 log(4T /δ))

.

If we impose the slightly stronger condition that λ ≥ 4d log(9T ) + 8 log(4T /δ), then the term in the
denominator is at least 1, and then we have that

(cid:107)ˆθt − θ(cid:107)Γt ≤

λ + (cid:112)6M 2 log(4T /δ) + 3dM 2 log(1 + 2T /d).

Finally, as in the two-action case, we use the fact that |ζt| ≤ 3 (cid:44) M .

Recall the setting of γ(T ) (cid:44)

λ + (cid:112)27d log(1 + 2T /d) + 54 log(4T /δ) and the deﬁnition of λ (cid:44)
4d log(9T ) + 8 log(4T /δ). For the remainder of the proof, condition on the probability 1 − δ event
that Lemma 17 holds. We now turn to analyzing the regret.

Lemma 18. Let µt (cid:44) Ea∼πtzt,a where πt is the solution to (3) and assume the conclusion of Lemma 17
holds. Then with probability at least 1 − δ

√

√

Reg(T ) ≤ (1 + 6γ(T ))(cid:112)2T log(2/δ) + 3γ(T )

tr(Γ−1

t (zt,at − µt)(zt,at − µt)(cid:62)).

(cid:118)
(cid:117)
(cid:117)
(cid:116)T

T
(cid:88)

t=1

22

This lemma is slightly more complicated than Lemma 6.

Proof. First, using the same application of Azuma’s inequality as in the proof of Lemma 6, with probability
1 − δ/2, we have

Reg(T ) ≤ (cid:112)2T log(2/δ) +

Ea∼πt[(cid:104)θ, zt,a(cid:63)

t

− zt,a(cid:105) | Ft].

T
(cid:88)

t=1

Now we work with this latter expected regret

Ea∼πt [(cid:104)θ, zt,a(cid:63)

t

− zt,a(cid:105) | Ft] =

(cid:104)θ, zt,a(cid:63)

− µt(cid:105) ≤

t

(cid:104)ˆθ, zt,a(cid:63)

t

− µt(cid:105) + γ(T )(cid:107)zt,a(cid:63)

− µt(cid:107)Γ−1

.

t

t

For the ﬁrst term, we use the ﬁltration condition (2)

(cid:104)ˆθ, zt,a(cid:63)

t

− µt(cid:105) =

πt(a)(cid:104)ˆθ, zt,a(cid:63)

t

− zt,a(cid:105) ≤ γ(T )

πt(a)(cid:107)zt,a(cid:63)

t

− zt,a(cid:107)Γ−1

t

T
(cid:88)

t=1

(cid:88)

a∈At

≤ γ(T )(cid:107)zt,a(cid:63)

t

− µt(cid:107)Γ−1

t

+ γ(T )

πt(a)(cid:107)zt,a − µt(cid:107)Γ−1

.

t

T
(cid:88)

t=1

(cid:88)

a∈At

(cid:88)

a∈At

Applying the feasibility condition in (3), we can bound the expected regret by

Ea∼πt[(cid:104)θ, zt,a(cid:63)

t

− zt,a(cid:105) | Ft] ≤ 3γ(T )

tr(Γ−1

t Cov
a∼πt

(zt,a)) ≤ 3γ(T )

tr(Γ−1

t Cov
a∼πt

(zt,a)).

T
(cid:88)

(cid:114)

t=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)T

T
(cid:88)

t=1

To complete the proof, we need to relate the covariance, which takes expectation over the random action, with
the particular realization in the algorithm, since this realization affects the term Γt+1. Let Zt (cid:44) zt,at − µt
denote the centered realization, then the covariance term is

In order to derive a bound on (cid:80)T

t=1 tr(Γ−1

t Cova∼πt(zt,a)), we ﬁrst consider the following

Cov
a∼πt

(zt,a) = E[ZtZ (cid:62)
t

| Ft]

T
(cid:88)

t=1

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) − tr(Γ−1

t ZtZ (cid:62)

t ).

Observe that sequence of sums {(cid:80)τ
| Ft]) − tr(Γ−1
each term tr(Γ−1
the Freedman’s inequality reveals that with probability at least 1 − δ/2

| Ft]) − tr(Γ−1

E[ZtZ (cid:62)
τ =1 is a martingale. Also,
t
t ) is bounded by 1 because Γ1 = λI and λ > 1. Applying

t
t ZtZ (cid:62)

t=1 tr(Γ−1

E[ZtZ (cid:62)
t

t ZtZ (cid:62)

t )}T

t

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) − tr(Γ−1

t ZtZ (cid:62)

t ) ≤ 2

E[(Z (cid:62)

t Γ−1

t Zt)2 | Ft] log(2/δ) + 2 log(2/δ)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

t=1

T
(cid:88)

t=1

≤

1
2

T
(cid:88)

t=1

23

≤ 2

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) log(2/δ) + 2 log(2/δ)

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) + 4 log(2/δ).

Then rearranging and plugging back into our regret bound, we have

Reg(T ) ≤ (cid:112)2T log(2/δ) + 3γ(T )

(cid:118)
(cid:117)
(cid:117)
(cid:116)2T

(cid:32) T

(cid:88)

t=1

tr(Γ−1

t ZtZ (cid:62)

t ) + 4 log(2/δ)

(cid:33)

≤ (1 + 6γ(T ))(cid:112)2T log(2/δ) + 3γ(T )

tr(Γ−1

t ZtZ (cid:62)

t ).

(cid:118)
(cid:117)
(cid:117)
(cid:116)2T

T
(cid:88)

t=1

To conclude the proof of the theorem, apply Lemma 7, which applies on the last term on the RHS

of Lemma 18. Overall, with probability at least 1 − 2δ, we get

Reg(T ) ≤ (1 + 6γ(T ))(cid:112)2T log(2/δ) + 3γ(T )(cid:112)2T d(1 + 1/λ) log(1 + T /(dλ)).

Since λ = Θ(d log(T /δ)) and γ(T ) = O((cid:112)d log(T ) + (cid:112)log(T /δ)), we get with probability 1 − δ,

Reg(T ) ≤ O

√

(cid:16)

d

T log(T ) + (cid:112)dT log(T ) log(T /δ) + (cid:112)T log(T /δ) log(1/δ)

(cid:17)

.

References

[1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic

bandits. In Advances in Neural Information Processing Systems, 2011.

[2] Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efﬁcient algorithm

for bandit linear optimization. In Conference on Learning Theory, 2008.

[3] Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E Schapire. Taming the
monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine
Learning, 2014.

[4] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In

International Conference on Machine Learning, 2013.

[5] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed

bandit problem. SIAM Journal on Computing, 2002.

[6] Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandit
algorithms with supervised learning guarantees. In International Conference Artiﬁcial Intelligence and
Statistics, 2011.

[7] Peter J Bickel, Chris AJ Klaassen, Ya’acov Ritov, and Jon A Wellner. Efﬁcient and adaptive estimation

for semiparametric models. Springer New York, 1998.

[8] Sébastien Bubeck, Nicolo Cesa-Bianchi, and Sham M Kakade. Towards minimax policies for online

linear optimization with bandit feedback. In Conference on Learning Theory, 2012.

[9] Nicolo Cesa-Bianchi and Gábor Lugosi. Combinatorial bandits. Journal of Computer and System

Sciences, 2012.

24

[10] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duﬂo, Christian Hansen, Whit-
ney Newey, and James M. Robins. Double machine learning for treatment and causal parameters.
arXiv:1608.00060, 2016.

[11] Wei Chu, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandits with linear payoff functions.

In International Conference on Artiﬁcial Intelligence and Statistics, 2011.

[12] Thomas M Cover. Behavior of sequential predictors of binary sequences. In Conference on Information

Theory, Statistical Decision Functions and Random Processes, 1965.

[13] Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit

feedback. In Conference on Learning Theory, 2008.

[14] Victor H de la Peña, Tze Leung Lai, and Qi-Man Shao. Self-normalized processes: Limit theory and

statistical applications. Springer Science & Business Media, 2008.

[15] Victor H de la Peña, Michael J Klass, and Tze Leung Lai. Theory and applications of multivariate

self-normalized processes. Stochastic Processes and their Applications, 2009.

[16] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the
multi-armed bandit and reinforcement learning problems. Journal of Machine Learning Research, 2006.

[17] Sarah Filippi, Olivier Cappe, Aurélien Garivier, and Csaba Szepesvári. Parametric bandits: The

generalized linear case. In Advances in Neural Information Processing Systems, 2010.

[18] David A Freedman. On tail probabilities for martingales. The Annals of Probability, 1975.

[19] Kristjan Greenewald, Ambuj Tewari, Susan Murphy, and Predag Klasnja. Action centered contextual

bandits. In Advances in Neural Information Processing Systems, 2017.

[20] Akshay Krishnamurthy, Alekh Agarwal, and Miroslav Dudík. Contextual semibandits via supervised

learning oracles. In Advances in Neural Information Processing Systems, 2016.

[21] John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side

information. In Advances in Neural Information Processing Systems, 2008.

[22] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personal-

ized news article recommendation. In International Conference on World Wide Web, 2010.

[23] Lihong Li, Yu Lu, and Dengyong Zhou. Provable optimal algorithms for generalized linear contextual

bandits. In International Conference on Machine Learning, 2017.

[24] Jerzy Neyman. C(α) tests and their use. Sankhy¯a: The Indian Journal of Statistics, Series A, 1979.

[25] Alexander Rakhlin and Karthik Sridharan. Bistro: An efﬁcient relaxation-based method for contextual

bandits. In International Conference on Machine Learning, 2016.

[26] James M Robins and Andrea Rotnitzky. Recovery of information and adjustment for dependent

censoring using surrogate markers. In AIDS Epidemiology. Springer, 1992.

[27] James M Robins, Lingling Li, Eric Tchetgen Tchetgen, and Aad van der Vaart. Higher order inﬂuence
functions and minimax estimation of nonlinear functionals. In Probability and Statistics: Essays in
Honor of David A. Freedman. Institute of Mathematical Statistics, 2008.

25

[28] Peter M Robinson. Root-n-consistent semiparametric regression. Econometrica: Journal of the

[29] Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of

[30] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of

Econometric Society, 1988.

Operations Research, 2010.

Operations Research, 2014.

[31] Maurice Sion. On general minimax theorems. Paciﬁc Journal of Mathematics, 1958.

[32] Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dudík, John Langford, Damien
In Advances in Neural

Jose, and Imed Zitouni. Off-policy evaluation for slate recommendation.
Information Processing Systems, 2017.

[33] Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert E Schapire. Efﬁcient algorithms for adversarial

contextual learning. In International Conference on Machine Learning, 2016.

[34] Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health. In

Mobile Health. Springer, 2017.

[35] Anastasios Tsiatis. Semiparametric theory and missing data. Springer Science & Business Media, 2007.

26

8
1
0
2
 
l
u
J
 
6
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
0
2
4
0
.
3
0
8
1
:
v
i
X
r
a

Semiparametric Contextual Bandits

Akshay Krishnamurthy ∗2, Zhiwei Steven Wu †2, and Vasilis Syrgkanis ‡3

2Microsoft Research NYC, New York, NY
3Microsoft Research New England, Cambridge, MA

July 17, 2018

Abstract

This paper studies semiparametric contextual bandits, a generalization of the linear stochastic bandit
problem where the reward for an action is modeled as a linear function of known action features confounded
by a non-linear action-independent term. We design new algorithms that achieve ˜O(d
T ) regret over T
rounds, when the linear function is d-dimensional, which matches the best known bounds for the simpler
unconfounded case and improves on a recent result of Greenewald et al. [19]. Via an empirical evaluation,
we show that our algorithms outperform prior approaches when there are non-linear confounding effects on
the rewards. Technically, our algorithms use a new reward estimator inspired by doubly-robust approaches
and our proofs require new concentration inequalities for self-normalized martingales.

√

1 Introduction

A number of applications including online personalization, mobile health, and adaptive clinical trials require
that an agent repeatedly makes decisions based on user or patient information with the goal of optimizing some
metric, typically referred to as a reward. For example, in online personalization problems, we might serve
content based on user history and demographic information with the goal of maximizing user engagement
with our service. Since counterfactual information is typically not available, these problems require algorithms
to carefully balance exploration—making potentially suboptimal decisions to acquire new information—with
exploitation—using collected information to make better decisions. Such problems are often best modeled
with the framework of contextual bandits, which captures the exploration-exploitation tradeoff and enables
rich decision making policies but ignores the long-term temporal effects that make general reinforcement
learning challenging. Contextual bandit algorithms have seen recent success in applications, including news
recommendation [22] and mobile health [34].

Contextual bandit algorithms can be categorized as either parametric or agnostic, depending on whether
they model the relationship between the reward and the decision or not. Parametric approaches typically
assume that the reward is a (generalized) linear function of a known decision-speciﬁc feature vector [17, 11,
1, 4]. Once this function is known to high accuracy, it can be used to make near-optimal decisions. Exploiting

∗akshay@cs.umass.edu
†zsw@umn.edu
‡vasy@microsoft.com

1

this fact, algorithms for this setting focus on learning the parametric model. Unfortunately, fully parametric
assumptions are often unrealistic and challenging to verify in practice, and these algorithms may perform
poorly when the assumptions do not hold.

In contrast, agnostic approaches make no modeling assumptions about the reward and instead compete
with a large class of decision-making policies [21, 3]. While these policies are typically parametrized in
some way, these algorithms provably succeed under weaker conditions and are generally more robust than
parametric ones. On the other hand, they typically have worse statistical guarantees, are conceptually much
more complex, and have high computational overhead, technically requiring solving optimization problems
that are NP-hard in the worst case. This leads us to a natural question:

Is there an algorithm that inherits the simplicity and statistical guarantees of the parametric
methods and the robustness of the agnostic ones?

Working towards an afﬁrmative answer to this question, we consider a semiparametric contextual bandit
setup where the reward is modeled as a linear function of the decision confounded by an additive non-linear
perturbation that is independent of the decision. This setup signiﬁcantly generalizes the standard parametric
one, allowing for complex, non-stationary, and non-linear rewards (See Section 2 for a precise formulation).
On the other hand, since this perturbation is just a baseline reward for all decisions, it has no inﬂuence on the
optimal one, which depends only on the unknown linear function. In the language of econometrics and causal
modeling, the treatment effect is linear.

In this paper, we design new algorithms for the semiparametric contextual bandits problem. When the
linear part of the reward is d-dimensional, our algorithms achieve ˜O(d
T ) regret over T rounds, even when
the features and the confounder are chosen by an adaptive adversary. This guarantee matches the best results
for the simpler linear stochastic bandit problem up to logarithmic terms, showing that there is essentially no
statistical price to pay for robustness to confounding effects. On the other hand, our algorithm and analysis is
quite different, and it is not hard to see that existing algorithms for stochastic bandits fail in our more general
setting. Our regret bound also improves on a recent result of Greenewald et al. [19], who consider the same
setup but study a weaker notion of regret. Our algorithm, main theorem, and comparisons are presented
in Section 3.

√

We also compare our algorithm to approaches from both parametric and agnostic families in an empirical
study (we use a linear policy class for agnostic approaches). In Section 5, we evaluate several algorithms on
synthetic problems where the reward is (a) linear, and (b) linear with confounding. In the linear case, our
approach learns, but is slightly worse than the baselines. On the other hand, when there is confounding, our
algorithm signiﬁcantly outperforms both parametric and agnostic approaches. As such, these experiments
demonstrate that our algorithm represents a favorable trade off between statistical efﬁciency and robustness.
On a technical level, our algorithm and analysis require several new ideas. First, we derive a new estimator
for linear models in the presence of confounders, based on recent and classical work in semiparametric
statistics and econometrics [28, 10]. Second, since standard algorithms using optimism principles fail to
guarantee consistency of this new estimator, we design a new randomized algorithm, which can be viewed
as an adaptation of the action-elimination method of Even-Dar et al. [16] to the contextual bandits setting.
Finally, analyzing the semiparametric estimator requires an intricate deviation argument, for which we derive
a new self-normalized inequality for vector-valued martingales using tools from de la Peña et al. [14, 15].

2 Preliminaries

We study a generalization of the linear stochastic bandit problem with action-dependent features and action-
independent confounder. The learning process proceeds for T rounds, and in round t, the learner receives a

2

context xt (cid:44) {zt,a}a∈A where zt,a ∈ Rd and A is the action set, which we assume to be large but ﬁnite. The
learner then chooses an action at ∈ A and receives reward

rt(at) (cid:44) (cid:104)θ, zt,at(cid:105) + ft(xt) + ξt,

(1)

where θ ∈ Rd is an unknown parameter vector, ft(xt) is a confounding term that depends on the context xt
but, crucially, does not depend on the chosen action at, and ξt is a noise term that is centered and independent
of at.

For each round t, let a(cid:63)
t

(cid:44) argmaxa∈A(cid:104)θ, zt,a(cid:105) denote the optimal action for that round. The goal of our

algorithm is to minimize the regret, deﬁned as

Reg(T ) (cid:44)

rt(a(cid:63)

t ) − rt(at) =

(cid:104)θ, zt,a(cid:63)

t

− zt,at(cid:105).

T
(cid:88)

t=1

T
(cid:88)

t=1

Observe that the noise term ξt, and, more importantly, the confounding term ft(xt) are absent in the ﬁnal
expression, since they are independent of the action choice.

We consider the challenging setting where the context xt and the confounding term ft(·) are chosen by
an adaptive adversary, so they may depend on all information from previous rounds. This is formalized in the
following assumption.

Assumption 1 (Environment). We assume that xt = {zt,a}a∈A, ft, ξt are generated at the beginning of
round t, before at is chosen. We assume that xt and ft are chosen by an adaptive adversary, and that ξt
satisﬁes E[ξt|xt, ft] = 0 and |ξt| ≤ 1.

We also impose mild regularity assumptions on the parameter, the feature vectors, and the confounding

functions.

Assumption 2 (Boundedness). Assume that (cid:107)θ(cid:107)2 ≤ 1 and that (cid:107)zt,a(cid:107)2 ≤ 1 for all a ∈ A, t ∈ [T ]. Further
assume that ft(·) ∈ [−1, 1] for all t ∈ [T ].

For simplicity, we assume an upper bound of 1 in these conditions, but our algorithm and analysis can be

adapted to more generic regularity conditions.

Related work. Our setting is related to linear stochastic bandits and several variations that have been
studied in recent years. Among these, the closest is the work of Greenewald et al. [19] who consider the
same setup and provide a Thompson Sampling algorithm using a new reward estimator that eliminates the
confounding term. Motivated by applications in medical intervention, they consider a different notion of
regret from our more-standard notion and, as such, the results are somewhat incomparable. For our notion
of regret, their analysis can produce a T 2/3-style regret bound, which is worse than our optimal
T bound.
See Section 3.3 for a more detailed comparison.

√

Other results for linear stochastic bandits include upper-conﬁdence bound algorithms [29, 11, 1], Thomp-
son sampling algorithms [4, 30], and extensions to generalized linear models [17, 23]. However, none of
these models accommodate arbitrary and non-linear confounding effects. Moreover, apart from Thompson
sampling, all of these algorithms use deterministic action-selection policies (conditioning on the history),
which provably incurs Ω(T ) regret in our setting, as we will see.

One can accommodate confounded rewards via an agnostic-learning approach to contextual bandits [5,
21, 3]. In this framework, we make no assumptions about the reward, but rather compete with a class
of parameterized policies (or experts). Since a d-dimensional linear policy is optimal in our setting, an

3

agnostic algorithm with a linear policy class addresses precisely our notion of regret. However there are two
disadvantages. First, agnostic algorithms are all computationally intractable, either because they enumerate
the (inﬁnitely large) policy class, or because they assume access to optimization oracles that can solve NP-hard
K, the
problems in the worst case. Second, most agnostic approaches have regret bounds that grow with
number of actions, while our bound is completely independent of K.

√

We are aware of one approach that is independent of K, but it requires enumeration of an inﬁnitely
large policy class. This method is based on ideas from the adversarial linear and combinatorial bandits
(cid:44) (zt,a, 1) ∈ Rd+1, our setting can
literature [13, 2, 8, 9]. Writing θt (cid:44) (θ, ft(xt)) ∈ Rd+1 and z(cid:48)
be re-formulated in the adversarial linear bandits framework. However, standard linear bandit algorithms
compete with the best ﬁxed action vector in hindsight, rather than the best policy with time-varying action
sets. To resolve this, one can use the linear bandits reward estimator [32] in a contextual bandit algorithm
like EXP4 [5], but this approach is not computationally tractable with the linear policy class. For our setting,
we are not aware of any computationally efﬁcient approaches, even oracle-based approaches, that achieve
poly(d)

T regret with no dependence on the number of actions.

√

t,a

We resolve the challenge of confounded rewards with an estimator from the semiparametric statistics
literature [35], which focuses on estimating functionals of a nonparametric model. Most estimators are based
on Neyman Orthogonalization [24], which uses moment equations that are insensitive to nuisance parameters
in a method-of-moments approach [10]. These orthogonal moments typically involve a linear correction
to an initial nonparametric estimate using so-called inﬂuence functions [7, 27]. Robinson [28] used this
approach for the ofﬂine version of our setting (known as the partially linear regression (PLR) model) where
he demonstrated a form of double-robustness [26] to poor estimation of the nuisance term (in our case ft(xt)).
We generalize Robinson’s work to the online setting, showing how orthogonalized estimators can be used for
adaptive exploration. This requires several new techniques, including a novel action selection policy and a
self-normalized inequality for vector-valued martingales.

3 Algorithm and Results

In this section, we describe our algorithm and present our main theoretical result, an ˜O(d
for the semiparametric contextual bandits problem.

√

T ) regret bound

3.1 A Lower Bound

Before turning to the algorithm, we ﬁrst present a lower bound against deterministic algorithms. Since the
functions ft may be chosen by an adaptive adversary, it is not hard to show that this setup immediately
precludes the use of deterministic algorithms.

Proposition 3. Consider an algorithm that, at round t, chooses an action at as a deterministic function of
the observable history Ht (cid:44) {x1:t, a1:t−1, r1:t−1}. There exists a semiparametric contextual bandit instance
with d = 2 and K = 2 where the regret of the algorithm is at least T /2.

See Appendix B for the proof, which resembles the standard argument against deterministic online
learning algorithms [12]. The main difference is that the adversary uses the confounding term to corrupt
the information that the learner receives, whereas, in the standard proof, the adversary chooses the optimal
action in response to the learner. In fact, deterministic algorithms can succeed in the full information version
of our setting, since taking differences between rewards eliminates the confounder. Thus, bandit feedback
plays a crucial role in our construction and the bandit setting is considerably more challenging than the full
information analogue.

4

Algorithm 1: BOSE (Bandit orthogonalized semiparametric estimation)

Input :T, δ ∈ (0, 1).

1 Set λ ← 4d log(9T ) + 8 log(4T /δ) and γ(T ) ←
2 Initialize ˆθ ← 0 ∈ Rd, Γ ← λId×d.
3 for t = 1, . . . , T do
4

Observe xt = {zt,a}a∈A
Filter

√

λ + (cid:112)27d log(1 + 2T /d) + 54 log(4T /δ).

5

6

7

8

At ←

(cid:110)
a ∈ A | ∀b ∈ A, (cid:104)ˆθ, zt,b − zt,a(cid:105) ≤ γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

(cid:111)

.

Find distribution πt ∈ ∆(At) such that ∀a ∈ At (We use
Covb∼πt(zt,b) (cid:44) E[zt,bz(cid:62)

t,b] − (Ezt,b)(Ezt,b)(cid:62).)
(cid:107)zt,a − Eb∼πtzt,b(cid:107)2

Γ−1 ≤ tr(Γ−1 Cov
b∼πt

(zt,b)).

Sample at ∼ πt and play at. Observe rt(at). (rt(at) = (cid:104)θ, zt,at(cid:105) + ft(xt) + ξt.)
Let µt = Ea∼πt[zt,a | xt] and update parameters

(2)

(3)

Γ ← Γ + (zt,at − µt)(zt,at − µt)(cid:62),

ˆθ ← Γ−1

(zτ,aτ − µτ )rτ (aτ ).

(4)

t
(cid:88)

τ =1

We emphasize that, except for the Thompson Sampling approach [4], essentially all algorithms for the
linear stochastic bandit problem use deterministic strategies, so they provably fail in the semiparametric
setting. As we mentioned, Thompson Sampling was analyzed in our setting by Greenewald et al. [19], but
T -type regret bound (See Section 3.3 for a more quantitative and detailed
they do not obtain the optimal
comparison). In contrast, our algorithm is quite different from all of these approaches; it ensures enough
randomization to circumvent the lower bound and also achieves the optimal

√

√

√

T regret.
T ) lower bound for linear stochastic bandits [13],

To conclude this discussion, we remark that the Ω(d

which also applies to randomized algorithms, holds in our more general setting as well.

3.2 The Algorithm

Pseudocode for the algorithm, which we call BOSE, for “Bandit Orthogonalized Semiparametric Estimation,"
is displayed in Algorithm 1. The algorithm maintains an estimate ˆθ for the true parameter θ, which it uses in
each round to select an action via two steps: (1) an action elimination step that removes suboptimal actions,
and (2) an optimization step that ﬁnds a good distribution over the surviving actions. The algorithm then
samples and plays an action from this distribution and uses the observed reward to update the parameter
estimate ˆθ. This parameter estimation step is the third main element of the algorithm. We now describe each
of these three components in detail.

Parameter estimation. For simplicity, we use zt (cid:44) zt,at to denote the feature vector for the action that
was chosen at round t, and similarly we use rt (cid:44) rt(at). Using all previously collected data, speciﬁcally
{zτ , rτ }t
τ =1 at the end of round t, we would like to estimate the parameter θ. First, if fτ (xτ ) were identically

5

zero, by exploiting the linear parametrization we could use ridge regression, which with some λ > 0 gives

(cid:32)

(cid:33)−1 t

ˆθRidge (cid:44)

λI +

zτ z(cid:62)
τ

t
(cid:88)

τ =1

(cid:88)

τ =1

zτ rτ .

This estimator appears in most prior approaches for linear stochastic bandits [29, 11, 1]. Unfortunately,
since fτ (xτ ) is non-zero, ˆθRidge has non-trivial and non-vanishing bias, so even in benign settings it is not a
consistent estimator for θ.1

Our approach to eliminating the bias from the confounding term fτ (xτ ) is to center the feature vectors
zτ . Intuitively, in the ridge estimator, if zτ is centered, then zτ (rτ − (cid:104)θ(cid:63), zτ (cid:105)) is mean zero, even when
there is non-negligible bias in the second term. As such, the error of the corresponding estimator can be
expected to concentrate around zero. In the semiparametric statistics literature, this is known as Neyman
Orthogonalization [24], which was analyzed in the context of linear regression by Robinson [28] and in a
more general setting by Chernozhukov et al. [10].

To center the feature vector, we will, at round t, choose action at by sampling from some distribution
πt ∈ ∆(A). Let µt (cid:44) Eat∼πt[zt,at|xt] denote the mean feature vector, taking expectation only over our
random action choice. With this notation, the orthogonalized estimator is

Γ = λI +

(zτ − µτ )(zτ − µτ )(cid:62),

ˆθ = Γ−1

(zτ − µτ )rτ .

t
(cid:88)

τ =1

t
(cid:88)

τ =1

ˆθ is a Ridge regression version of Robinson’s classical semiparametric regression estimator [28]. The
estimator was originally derived for observational studies where one might not know the propensities µτ
exactly, and the standard description involves estimates ˆfτ and ˆµτ for the confounding term fτ and the
propensities µτ respectively. Informally, the estimator achieves a form of double-robustness, in the sense
that it is accurate if either of these auxilliary estimators are. In our case, since we know the propensities
µτ exactly, we can use an inconsistent estimator for the confounding term, so we simply set ˆfτ (xτ ) ≡ 0.
In Lemma 5, we prove a precise ﬁnite sample concentration inequality for this orthogonalized estimator,
showing that the confounding term ft(xt) does not introduce any bias. While the estimator has been studied
in prior works [28], to our knowledge, our error guarantee is novel.

The convergence rate of the orthogonalized estimator depends on the eigenvalues of the matrix Γ, and
we must carefully select actions to ensure these eigenvalues are sufﬁciently large. To see why, notice that
any deterministic action-selection approach with the orthogonalized estimator (including conﬁdence based
approaches), will fail, since zt = µt, so the eigenvalues of Γ do not grow rapidly and in fact the estimator is
identically 0. This argument motivates our new action selection scheme which ensure substantial conditional
covariance.

Action selection. Our action selection procedure has two main elements. First using our estimate ˆθ, we
eliminate any action that is provably suboptimal. Based on our analysis for the estimator ˆθ, at round t, we can
certify action a is suboptimal, if we can ﬁnd another action b such that

(cid:104)ˆθ, zt,b − zt,a(cid:105) > γ(T )(cid:107)zt,b − zt,a(cid:107)Γ−1.

1A related estimator can be used to evaluate the reward of a policy, as in linear and combinatorial bandits [9], but to achieve
adequate exploration, one must operate over the policy class, which leads to computational intractability. We would like to use ˆθ to drive
exploration, and this seems to require a consistent estimator. See Appendix A for a simple example demonstrating how using a biased
estimator in a conﬁdence-based approach results in linear regret.

6

Here γ(T ) is the constant speciﬁed in the algorithm, and (cid:107)x(cid:107)M (cid:44)
x(cid:62)M x denotes the Mahalanobis norm.
Using our conﬁdence bound for ˆθ in Lemma 5 below, this inequality certiﬁes that action b has higher expected
reward than action a, so we can safely eliminate a from consideration.

The next component is to ﬁnd a distribution over the surviving actions, denoted A(cid:48)

t at round t, with
t) that we use is the solution to the following feasibility

sufﬁcient covariance. The distribution πt ∈ ∆(A(cid:48)
problem

√

∀a ∈ A(cid:48)
t,

(cid:107)zt,a − Eb∼πtzt,b(cid:107)2

Γ−1 ≤ tr(Γ−1 Cov
b∼πt

(zt,b)).

For intuition, the left hand side of the constraint for action a is an upper bound on the expected regret if a is
the optimal action on this round. Thus, the constraints ensure that the regret is related to the covariance of the
distribution, which means that if we incur high regret, the covariance term Covb∼πt(zt,b) will be large. Since
we use a sample from πt to update our parameter estimate, this means that whenever the instantaneous regret
is large, we must learn substantially about the parameter. In this way, the distribution πt balances exploration
and exploitation. We will see in Lemma 8 that this program is convex and always has a feasible solution.

Our action selection scheme bears some resemblance to action-elimination approaches that have been
studied in various bandit settings [16]. The main differences are that we adapt these ideas to the contextual
setting and carefully choose a distribution over the surviving actions to balance exploration and exploitation.

3.3 The Main Result

We now turn to the main result, a regret guarantee for BOSE.

Theorem 4. Consider the semiparametric contextual bandit problem under Assumption 1 and Assumption 2.
T log(T /δ)).
For any parameter δ ∈ (0, 1), with probability at least 1−δ, Algorithm 1 has regret at most O(d

√

The constants, and indeed a bound depending on λ and γ(T ) can be extracted from the proof, provided in

the appendix. To interpret the regret bound, it is worth comparing with several related results:

√

Comparison with linear stochastic bandits. While most algorithms for linear stochastic bandits prov-
ably fail in our setting (via Proposition 3), the best regret bounds here are O((cid:112)dT log(T K/δ)) [11] and
T log(T ) + (cid:112)dT log(T ) log(1/δ)) [1] depending on whether we assume that the number of actions
O(d
K is small or not. This latter result is optimal when the number of actions is large [13], which is the setting
we are considering here. Since our bound matches this optimal regret up to logarithmic factors, and since
linear stochastic bandits are a special case of our semiparametric setting, our result is therefore also optimal
up to logarithmic factors. An interesting open question is whether an ˜O((cid:112)dT log(K/δ)) regret bound is
achievable in the semiparametric setting.

Comparison with agnostic contextual bandits. The best oracle-based agnostic approaches achieve ˜O(
regret [3], incurring a polynomial dependence on the number of actions K, although there is one inefﬁcient
method that can achieve ˜O(d
T ),2 as we discussed previously. To date, all efﬁcient methods in the agnostic
setting require some form of i.i.d. [3] or transductive assumption [33, 25] on the contexts, which we do not
assume here.

√

√

dKT )

2This follows easily by combining ideas from Auer et al. [5] and Cesa-Bianchi and Lugosi [9].

7

Comparison with Greenewald et al. [19]. Greenewald et al. [19] consider a very similar setting to ours,
where rewards are linear with confounding, but where one default action a0 always has zt,a0 ≡ 0 ∈ Rd.
Applications in mobile health motivate a restriction that the algorithm choose the a0 action with probability
∈ [p, 1 − p] for some small p ∈ (0, 1). Their work also introduces a new notion of regret where they compete
with the policy that also satisﬁes this constraint but otherwise chooses the optimal action a(cid:63)
t . In this setup,
they obtain an ˜O(d2

T ) regret bound, which has a worse dimension dependence than Theorem 4.

√

While the setup is somewhat different, we can still translate our result into a regret bound in their setting,
since BOSE can support the probability constraint, and by coupling the randomness between BOSE and
the optimal policy, the regret is unaffected.3 On the other hand, since the constant in their regret bound
scales with 1/p, their results as stated are vacuous when p = 0 which is precisely our setting. For our more
challenging regret deﬁnition, their analysis can produce a suboptimal T 2/3-style regret bound, and in this
sense, Theorem 4 provides a quantitative improvement.

Summary. BOSE achieves essentially the same regret bound as the best linear stochastic bandit methods,
but in a much more general setting. On the other hand, the agnostic methods succeed under even weaker
assumptions, but have worse regret guarantees and/or are computationally intractable. Thus, BOSE broadens
the scope for computationally efﬁcient contextual bandit learning.

4 Proof Sketch

In the two arm case, one should set γ(T ) (cid:44)

We sketch the proof of Theorem 4 in the two-action case (|A| = 2), which has a much simpler proof that
preserves the main ideas. The technical machinery needed for the general case is much more sophisticated,
and we brieﬂy describe some of these steps at the end of this section, with a complete proof in the Appendix.
λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ) and λ = O(1),
which differs slightly from the algorithm pseudocode for the more general case. Additionally, note that with
two actions, the uniform distribution over At is always feasible for Problem (3). Speciﬁcally, if the ﬁltered
set has cardinality 1, we simply play that action deterministically, otherwise we play one of the two actions
uniformly at random.

√

The proof has three main steps. First we analyze the orthogonalized regression estimator deﬁned in (4).
Second, we study the action selection mechanism and relate the regret incurred to the error bound for the
orthogonalized estimator. Finally, using a somewhat standard potential argument, we show how this leads to
T -type regret bound. For the proof, let ˆθt, Γt be the estimator and covariance matrix used on round t,
a
both based on t − 1 samples.

√

For the estimator, we prove the following lemma for the two action case. The main technical ingredient
is a self-normalized inequality for vector-valued martingales, which can be obtained using ideas from de la
Peña et al. [15].

Lemma 5. Under Assumption 1 and Assumption 2, let K = 2 and γ(T ) (cid:44)
Then, with probability at least 1 − δ, the following holds simultaneously for all t ∈ [T ]:

√

λ+(cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ).

3Technically it is actually smaller by a factor of (1 − p).

(cid:107)ˆθt − θ(cid:107)Γt ≤ γ(T ).

8

Proof. Using the deﬁnitions and Assumption 1, it is not hard to re-write

ˆθt = Γ−1

t (Γt − λI)θ + Γ−1

t

Zτ ζτ ,

t−1
(cid:88)

τ =1

where Zτ (cid:44) zτ,aτ − µτ and ζτ (cid:44) (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ . Further deﬁne St (cid:44) (cid:80)t−1
the triangle inequality the error is at most

τ =1 Zτ ζτ . Then, applying

(cid:107)ˆθt − θ(cid:107)Γt ≤ (cid:107)λθ(cid:107)Γ−1

t

+ (cid:107)St(cid:107)Γ−1

.

t

√

λ since Γt (cid:23) λI. To control the second term, we need to use a self-normalized
The ﬁrst term here is at most
concentration inequality, since Zτ is a random variable, and the normalizing term Γt = λI + (cid:80)t−1
τ =1 Zτ Z (cid:62)
τ
depends on the random realizations. In Lemma 10 in the appendix, we prove that with probability at least
1 − δ, for all t ∈ [T ]

(cid:107)St(cid:107)2

Γ−1
t

≤ 9d log(1 + T /(dλ)) + 18 log(T /δ).

(5)

The lemma follows from straightforward calculations.

Before proceeding, it is worth commenting on the difference between our self-normalized inequality (5)
and a slightly different one used by Abbasi-Yadkori et al. [1] for the linear case. In their setup, they have
that ζτ is conditionally centered and sub-Gaussian, which simpliﬁes the argument since after ﬁxing the Zτ s
(and hence Γt), the randomness in the ζτ s sufﬁces to provide concentration. In our case, we must use the
randomness in Zτ itself, which is more delicate, since Zτ affects the numerator St, but also the normalizer
Γt. In spite of this additional technical challenge, the two self-normalized processes admit similar bounds.

Next, we turn to the action selection step, where recall that either a single action is played deterministically,

or the actions are played uniformly at random.
Lemma 6. Let µt (cid:44) Ea∼πtzt,a where πt is the solution to (3), and assume that the conclusion of Lemma 5
holds. Then with probability at least 1 − δ

Reg(T ) ≤ (cid:112)2T log(1/δ) + 2γ(T )

T
(cid:88)

(cid:114)

t=1

tr(Γ−1

t Cov
b∼πt

(zt,b)).

Proof. We ﬁrst study the instantaneous regret, taking expectation over the random action. For this, we must
consider two cases. First, with Lemma 5, if |At| = 1, we argue that the regret is actually zero. This follows
from the Cauchy-Schwarz inequality since assuming At = {a} we get

(cid:104)θ, zt,a − zt,b(cid:105) ≥ (cid:104)ˆθt, zt,a − zt,b(cid:105) − γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

t

which is non-negative using the fact that b was eliminated. Therefore a is the optimal action and we incur no
regret. Since πt has no covariance, the upper bound holds.

On the other rounds, we set πt = Unif({a, b}) and hence µt = (zt,a + zt,b)/2. Assuming again that a is

the optimal action, the expected regret is

(cid:104)θ, zt,a − µt(cid:105) =

(cid:104)θ, zt,a − zt,b(cid:105) ≤

1
2

(cid:104)ˆθt, zt,a − zt,b(cid:105) + γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

t

(cid:17)

≤ γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

≤ 2γ(T )

t

t Cov
b∼πt

(zt,b)).

(cid:16)

1
2
(cid:114)
tr(Γ−1

9

Here the ﬁrst inequality uses Cauchy-Schwarz, the second uses (2), since neither action was eliminated, and
the third uses (3). This bounds the expected regret, and the lemma follows by Azuma’s inequality.

The last step of the proof is to control the sequence

T
(cid:88)

(cid:114)

t=1

tr(Γ−1

t Cov
b∼πt

(zt,b)).

First, recall that

(zt,b) (cid:44) Eb∼πt

(cid:2)(zt,b − µt)(zt,b − µt)(cid:62)(cid:3)

Cov
b∼πt

with µt (cid:44) Eb∼πt[zt,b]. Since in the two-arm case πt either chooses an arm deterministically or uniformly
randomizes between the two arms, the following always holds:

(zt,b) = (zt,at − µt)(zt,at − µt)(cid:62).

Cov
b∼πt

It follows that Γt+1 (cid:44) Γt + Covb∼πt (zt,b), and with Γ1 (cid:44) λI, the standard potential argument for online
ridge regression applies. We state the conclusion here, and provide a complete proof in the appendix.

Lemma 7. Let Γt, πt be deﬁned as above and deﬁne Mt (cid:44) (zt,at − µt)(zt,at − µt)(cid:62). Then

T
(cid:88)

(cid:113)

t=1

tr(Γ−1

t Mt) ≤ (cid:112)dT (1 + 1/λ) log(1 + T /(dλ)).

Combining the three lemmas establishes a regret bound of

Reg(T ) ≤ O

√
(cid:16)(cid:112)T d log(T /δ) log(T /d) + d

T log(T /d)

(cid:17)

with probability at least 1 − δ in the two-action case.

Extending to many actions. Several more technical steps are required for the general setting. First, the
martingale inequality used in Lemma 5 requires that the random vectors are symmetric about the origin.
This is only true for the two-action case, and in fact a similar inequality does not hold in general for the
non-symmetric situation that arises with more actions. In the non-symmetric case, both the empirical and the
population covariance must be used in the normalization, so the analogue of (5) is instead

(cid:107)St(cid:107)2

(Γt+EΓt)−1 ≤ 27d log(1 + 2T /d) + 54 log(4T /δ).

On the other hand, the error term for our estimator depends only on the empirical covariance Γt. To correct
for the discrepancy, we use a covering argument4 to establish

λI + Γt (cid:23) (λ − 6d log(T /δ))I + (Γt + EΓt)/3.

With this semideﬁnite inequality, we can translate from the Mahalanobis norm in the weaker self-normalized
bound to one with just Γt, which controls the error for the estimator.

We also argue that problem (3) is always feasible, which is the contents of the following lemma.

4For technical reasons, the Matrix Bernstein inequality does not sufﬁce here since it introduces a dependence on the maximal variance.

See Appendix for details.

10

Figure 1: Synthetic experiments with d = 10, K = 2. Left: A linear environment where action-features are
uniformly from the unit sphere. Center: A confounded environment with features from the sphere. Right: A
confounded environment with features from the sphere intersected with the positive orthant. Algorithms are
BOSE, OFUL [1], ILTCB [3], EPSGREEDY [21], and THOMPSON [4]. Agnostic approaches use a linear
policy class.

Lemma 8. Problem (3) is convex and always has a feasible solution. Speciﬁcally, for any vectors z1, . . . , zn ∈
Rd and any positive deﬁnite matrix M , there exists a distribution w ∈ ∆([n]) with mean µw (cid:44) Eb∼wzb such
that

∀i ∈ [n], (cid:107)zi − µw(cid:107)2

M ≤ tr(M Cov
b∼w

(zb)).

The proof uses convex duality. Integrating these new arguments into the proof for the two-action case

leads to Theorem 4.

5 Experiments

We conduct a simple experiment to compare BOSE with several other approaches5. We simulate three different
environments that follow the semiparametric contextual bandits model with d = 10, K = 2. In the ﬁrst
setting the reward is linear and the action features are drawn uniformly from the unit sphere. In the latter two
settings, we set ft(xt) = − maxa(cid:104)θ, zt,a(cid:105), which is related to the construction in the proof of Proposition 3.
One of these semiparametric settings has action features sampled from the unit sphere, while for the other, we
sample from the intersection of the unit sphere and the positive orthant.

In Figure 1, we plot the performance of Algorithm 1 against four baseline algorithms: (1) OFUL:
the optimistic algorithm for linear stochastic bandits [1], (2) THOMPSON sampling for linear contextual
bandits [4], (3) EPSGREEDY: the (cid:15)-greedy approach [21] with a linear policy class, (4) ILTCB: a more
sophisticated agnostic algorithm [3] with linear policy class. The ﬁrst algorithm is deterministic, so can have
linear regret in our setting, but is the natural baseline and one we hope to improve. Thompson Sampling is
another natural baseline, and a variant was used by Greenewald et al. [19] in essentially the same setting
as ours. The latter two have (Kd)1/3T 2/3 and
KdT regret bounds respectively under our assumptions,
but require solving cost-sensitive classiﬁcation problems, which are NP-hard in general. Following prior
empirical evaluations [20], we use a surrogate loss formulation based on square loss minimization in the
implementation.

√

The results of the experiment are displayed in Figure 1, where we plot the cumulative regret against the
number of rounds T . All algorithms have a single parameter that governs the degree of exploration. In BOSE

5Our code is publicly available at http://github.com/akshaykr/oracle_cb/.

11

and OFUL, this is the constant γ(T ) in the conﬁdence bound, in THOMPSON it is the variance of the prior,
and in ILTCB and EPSGREEDY it is the amount of uniform exploration performed by the algorithm. For
each algorithm we perform 10 replicates for each of 20 values of the corresponding parameter, and we plot
the best average performance, with error bars corresponding to ±2 standard deviations.

In the linear experiment (Figure 1, left panel), BOSE performs the worst, but is competitive with the
agnostic approaches, demonstrating a price to pay for robustness. The experimental setup in the center panel
is identical except with confounding, and BOSE is robust to this confounding, with essentially the same
performance, while the three baselines degrade dramatically. Finally, when the features lie in the positive
orthant (right panel), OFUL degrades further, while BOSE remains highly effective.

Regarding the baselines, we make two remarks:

1. Intuitively, the positive orthant setting is more challenging for OFUL since there is less inherent

randomness in the environment to overcome the confounding effect.

2. The agnostic approaches, despite strong regret guarantees, perform somewhat poorly in our experiments,
and we believe this for three reasons. First, our surrogate-loss implementation is based on an implicit
realizability assumption, which is not satisﬁed here. Second, we expect that the constant factors in their
regret bounds are signiﬁcantly larger than those of BOSE or OFUL. For computational reasons, we
only solve the optimization problem in ILTCB every 50 rounds, which causes a further constant factor
increase in the regret.

Overall, while BOSE is worse than other approaches in the linear environment, the experiment demonstrates
that when the environment is not perfectly linear, approaches based on realizability assumptions (either
explicitly like in OFUL, or implicitly like in implementations of ILTCB and EPSGREEDY), can fail. We
emphasize that linear environments are rare in practice, and such assumptions are typically impossible to
verify. We therefore believe that trading off a small loss in performance in the specialized linear case for
signiﬁcantly more robustness, as BOSE demonstrates, is desirable.

6 Discussion

This paper studies a generalization of the linear stochastic bandits setting, where rewards are confounded
by an adaptive adversary. Our new algorithm, BOSE, achieves the optimal regret, and also matches (up to
logarithmic factors) the best algorithms for the linear case. Our empirical evaluation shows that BOSE offers
signiﬁcantly more robustness than prior approaches, and performs well in several environments.

12

A Using the OLS Estimator

Here we construct an example problem to demonstrate how using the standard OLS estimator can fail in
the semiparametric setting. While not a comprehensive proof against all asymptotically biased approaches,
similar examples can be constructed for related estimators.

Consider a two-dimensional problem with two actions and no stochastic noise, where θ = e2, the second
standard basis vector. On the even rounds, the actions are z1 = (1, 1), z2 = (1, 1/3) and the confounding
term is f = −1. On the odd rounds, the actions are z1 = z2 = (1, 0) and the confounding term is f = 1. For
any policy for selecting actions, the OLS estimator before round t (for even t) is the solution to the following
optimization problem:

minimizew∈R2 α(w1 + w2)2 + (1 − α)(w1 + w2/3 + 2/3)2 + (w1 − 1)2 = L(w)

where α ∈ [0, 1] corresponds to the fraction of the even rounds (up to round t) where the policy chose z1.
We will argue that, for any α, the solution to this problem ˆw has ˆw2 < 0. Since there is no stochastic noise,
there is no need for conﬁdence bounds once the covariance is full rank, which happens after the second round.
Together, this implies that any sensible policy based on ˆw will prefer z2 to z1 on the even rounds, but z1 yields
higher reward by a ﬁxed constant. Thus using OLS in a conﬁdence-based approach leads to linear regret.

We now show that ˆw2 is strictly negative. We have

∂L(w)
∂w1
∂L(w)
∂w2

= 2α(w1 + w2) + 2(1 − α)(w1 + w2/3 + 2/3) + 2(w1 − 1),

= 2α(w1 + w2) +

(1 − α)(w1 + w2/3 + 2/3).

2
3

Setting both equations equal to zero yields the following system:

4w1 + (2/3 + 4α/3)w2 = 2/3 + 4α/3,

(2/3 + 4α/3)w1 + (2/9 + 16α/9)w2 = 4α/9 − 4/9.

The solution to this system is

w1 =

(2α + 1)2
−4α2 + 12α + 1

,

w2 =

4α2 + 5
4α2 − 12α − 1

,

provided that 4α2 (cid:54)= 12α + 1, which is not possible with α ∈ [0, 1]. In the interval [0, 1] we have that
4α2 − 12α − 1 < 0, and hence w2 < 0. Thus, the OLS estimator incorrectly predicts that z2 receives higher
reward than z1 on the even rounds. Since conﬁdence intervals are not needed, the algorithm suffers linear
reget.

B Proof of Proposition 3

We consider two possible values for the true parameter: θ1 = e1 ∈ R2, θ2 = e2 ∈ R2. At all rounds, the
context xt = {e1, e2} contains just two actions, and we further assume that the noise term ξt = 0 almost
surely. Since the action at is a deterministic function of the history, it can also be computed by the adaptive
adversary at the beginning of the round, and the adversary chooses

ft(xt) = −1{at = argmax

(cid:104)θ, zt,a(cid:105)}.

a

13

We show that rt(at) = 0 for all rounds t. Assume the parameter is θ1 so the optimal action is a(cid:63)
t = e1 and
the suboptimal action e2 has (cid:104)θ, e2(cid:105) = 0. If the learner chooses action e2, then the adversary sets ft(xt) = 0,
so rt(at) = 0. On the other hand, if the learner chooses action e1, then the adversary sets ft(xt) = −1 so
the reward is also zero. Similarly, if θ = θ2, the observed reward is always zero. Since the algorithm is
deterministic, it behaves identically regardless of whether the parameter is θ1 or θ2. In one of these instances
the algorithm must choose the suboptimal action at least T /2 times, leading to the lower bound.

C Proof for the Two-Action Case

We ﬁrst focus on the simpler two action case. Before turning to the main analysis, we prove two supporting
lemmas. The ﬁrst is an algebraic inequality relating matrix determinants to traces. This inequality also
appears in Abbasi-Yadkori et al. [1].

Lemma 9. Let X1, . . . , Xn denote vectors in Rd with (cid:107)Xi(cid:107)2 ≤ L for all i ∈ [n]. Deﬁne Γ (cid:44) λI +
(cid:80)n

i=1 XiX (cid:62)

i . Then

Proof. We will apply the following standard argument:

det(Γ) ≤ (λ + nL2/d)d.

det(Γ)1/d ≤

tr(Γ) =

tr(λI) +

tr(XiX (cid:62)

i ) = λ +

(cid:107)Xi(cid:107)2

2 ≤ λ + nL2/d.

1
d

1
d

1
d

n
(cid:88)

i=1

1
d

n
(cid:88)

i=1

The ﬁrst step is a spectral version of the AM-GM inequality and the remaining steps use linearity of the trace
operator and the boundedness conditions.

The second lemma is a new self-normalized concentration inequality for vector valued martingales.

Lemma 10 (Symmetric self-normalized inequality). Let {Ft}T
t=1 be a
stochastic process with Zt ∈ Rd and ζt ∈ R such that (1) (Zt, ζt) is Ft measurable, (2) |ζt| ≤ M for all
t ∈ [T ], (3) Zt ⊥⊥ ζt|Ft, (4) E[Zt|Ft] = 0, and (5) for all x ∈ Rd, L((cid:104)x, Zt(cid:105) | Ft) = L(−(cid:104)x, Zt(cid:105) | Ft)
where L denotes the probability law, so that Zt is conditionally symmetric. Let Σ (cid:44) (cid:80)T
t . Then for
any positive deﬁnite matrix Q we have

t=1 be a ﬁltration and let {(Zt, ζt)}T

t=1 ZtZ (cid:62)

P





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

Ztζt

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(Q+M 2Σ)−1

≥ 2 log

(cid:32)

(cid:115)

1
δ

det(Q + M 2Σ)
det(Q)

(cid:33)

 ≤ δ.

Proof. The proof follows the recipe in de la Peña et al. [15] (See also de la Peña et al. [14] for a more
comprehensive treatment including the univariate case). We start by applying the Chernoff method. Let
¯Σ (cid:44) Q + M 2Σ. We can write

P





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯Σ−1

Ztζt

≥ 2 log

(cid:32)

(cid:115)

1
δ

det( ¯Σ)
det(Q)

(cid:33)





 = P

exp



1
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

Ztζt



exp



(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯Σ−1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
2

T
(cid:88)

t=1



 ≥

(cid:115)





det( ¯Σ)
det(Q)

1
δ

Ztζt







 .

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯Σ−1



(cid:115)

≤ δE



det(Q)
det( ¯Σ)

14

Therefore, if we prove that this latter expectation is at most one, we will arrive at the conclusion. A similar
statement appears in Theorem 1 of de la Peña et al. [15], but our process is slightly different due to the
presence of ζt. To bound this latter expectation, ﬁx some λ ∈ Rd and consider an exponentiated process with
the increments

(cid:18)

Dλ
t

(cid:44) exp

(cid:104)λ, Ztζt(cid:105) −

M 2(cid:104)λ, Zt(cid:105)2
2

(cid:19)

.

Observe that E[Dλ

t |Ft] ≤ 1 since by the conditional symmetry of Zt, we have

E[Dλ

t |Ft] = E (cid:2)E (cid:2)Dλ

t | Ft, ζt

(cid:20)

(cid:20)

E

exp

= E

(cid:20)

(cid:20)

E

exp

= E

(cid:20)

(cid:20)

E

exp

≤ E

(cid:3)

(cid:3) | Ft
(cid:18) −M 2(cid:104)λ, Zt(cid:105)2
2
(cid:18) −M 2(cid:104)λ, Zt(cid:105)2
2
(cid:18) −M 2(cid:104)λ, Zt(cid:105)2
2

1
2

(cid:19)

(cid:19)

× cosh((cid:104)λ, Ztζt) | Ft, ζt

| Ft

(cid:21)

(cid:21)

(cid:19)

+

(cid:104)λ, Ztζt(cid:105)2
2

(cid:21)

(cid:21)

| Ft, ζt

| Ft

≤ 1.

×

(exp((cid:104)λ, Ztζt(cid:105)) + exp(−(cid:104)λ, Ztζt(cid:105)) | Ft, ζt

| Ft

(cid:21)

(cid:21)

This argument ﬁrst uses the conditional symmetry of Zt and the conditional independence of Zt, ζt, then the
identity (ex + e−x)/2 = cosh(x) and ﬁnally the analytical inequality cosh(x) ≤ ex2/2. Finally in the last
step we use the bound |ζt| ≤ M . This implies that the martingale U λ
τ is a super-martingale with
t
E[U λ

t ] ≤ 1 for all t, since by induction

τ =1 Dλ

(cid:44) (cid:81)t

E[U λ

t ] = E[U λ

t−1

E[Dλ

t |Ft]] ≤ E[U λ

t−1] ≤ . . . ≤ 1.

(6)

Now we apply the method of mixtures. In a standard application of the Chernoff method, we would choose
λ to maximize E[U λ
T ], but since we still have an expectation, we cannot swap expectation and maximum.
Instead, we integrate the inequality E[U λ
T ] ≤ 1, which holds for any λ, against λ drawn from a Gaussian
distribution with covariance Q−1. By Fubini’s theorem, we can swap the expectations to obtain

1 ≥ Eλ∼N (0,Q−1)E[U λ

T ] = E

(cid:90)

T (2π)−d/2(cid:112)det(Q) exp(−λ(cid:62)Qλ/2)dλ
U λ
(cid:32) T

M 2λ(cid:62)((cid:80)T

t )λ + λ(cid:62)Qλ

t=1 ZtZ (cid:62)
2

(cid:33)

dλ

(cid:88)

t=1

(cid:18)

= E

(2π)−d/2(cid:112)det(Q) exp

(cid:104)λ, Ztζt(cid:105) −

= E

(2π)−d/2(cid:112)det(Q) exp

(cid:104)λ, S(cid:105) −

M 2λ(cid:62)Σλ + λ(cid:62)Qλ
2

(cid:19)

dλ,

(cid:90)

(cid:90)

where S (cid:44) (cid:80)T
can be rewritten as

t=1 Ztζt and recall that Σ (cid:44) (cid:80)T

t=1 ZtZ (cid:62)

t . By completing the square, the term in the exponent

(cid:104)λ, S(cid:105) −

M 2λ(cid:62)Σλ + λ(cid:62)Qλ
2

1
2

=

(cid:0)−(λ − ¯Σ−1S)(cid:62) ¯Σ(λ − ¯Σ−1S) + S(cid:62) ¯Σ−1S(cid:1) ,

where recall that ¯Σ (cid:44) M 2Σ + Q. As such we obtain

1 ≥ E

(cid:20)
exp (cid:0)S(cid:62) ¯Σ−1S/2(cid:1) ×

(cid:90)

(2π)−d/2(cid:112)det(Q) exp

(cid:18) −(λ − ¯Σ−1S)(cid:62) ¯Σ(λ − ¯Σ−1S)
2

(cid:19)(cid:21)

dλ

(cid:115)

= E

det(Q)
det( ¯Σ)

exp (cid:0)S(cid:62) ¯Σ−1S(cid:1) .

15

This proves the lemma.

Equipped with the two lemmas, we can now turn to the analysis of the inﬂuence-adjusted estimator.

Lemma 11 (Restatement of Lemma 5). Under Assumption 1 and Assumption 2, with probability at least
1 − δ, the following holds simultaneously for all t ∈ [T ]:

(cid:107)ˆθt − θ(cid:107)Γt ≤

√

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ).

Proof. Recall that we deﬁne ˆθt, Γt to be the estimator and matrix used in round t, based on t − 1 examples.
Fixing a round t, we start by expanding the deﬁnition of ˆθt. We use the shorthand zτ (cid:44) zτ,aτ , µτ (cid:44)
Eb∼πτ [zτ,b], and rτ (cid:44) rτ (aτ ).

t−1
(cid:88)

τ =1

t−1
(cid:88)

τ =1

ˆθt = Γ−1

t

(zτ − µτ )rτ = Γ−1

t

(zτ − µτ )((cid:104)θ, zτ (cid:105) + fτ (xτ ) + ξτ )

= Γ−1
t

(zτ − µτ )((cid:104)θ, zτ − µτ (cid:105) + (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ )

= (Γt)−1(Γt − λI)θ + Γ−1

t

(zτ − µτ )((cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ ).

t−1
(cid:88)

τ =1

t−1
(cid:88)

τ =1

Let Zτ (cid:44) zτ − µτ and ζτ (cid:44) (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ . With this expansion, we can write

(cid:107)ˆθt − θ(cid:107)Γt = (cid:107) − λΓ−1

t θ + Γ−1

t

Zτ ζτ (cid:107)Γt ≤ (cid:107)λθ(cid:107)Γ−1

+

t

Zτ ζτ

t−1
(cid:88)

τ =1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

√

≤

λ +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

Zτ ζτ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

.

To ﬁnish the proof, we apply Lemma 10 to this last term. To verify the preconditions of the lemma, let
Fτ (cid:44) σ(x1, . . . , xτ , a1, . . . , aτ −1, ξ1, . . . , ξτ −1) denote the σ-algebra corresponding to the τ th round, after
observing the context xτ . Then the policy πτ and hence the action aτ are Fτ measurable and so is the noise
term ξτ . Therefore, Zτ = zτ,aτ − Ea∼πτ [zτ,a] is measurable, which veriﬁes the ﬁrst precondition. Using the
boundedness properties in Assumption 2, we know that |ζτ | ≤ 3 (cid:44) M , and by construction of the random
variables, we have Zτ ⊥⊥ ζτ |Fτ and E [Zτ |Fτ ] = 0. Finally, for the symmetry property, either Zτ |Fτ ≡ 0
if one action is eliminated, or otherwise we have µτ = 1
2 (zτ,1 + zτ,2) since there are only two actions. In
this case the random variable Zτ |Fτ = (cid:15)τ (zτ,1 − zτ2 )/2 where (cid:15)τ is a Rademacher random variable. By
inspection this is clearly conditionally symmetric. As such, we may apply Lemma 10, which reveals that with
probability at least 1 − δ,

Zτ ζτ

= M 2

Zτ ζτ

≤ 2M 2 log

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Γ−1
t

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

τ =1
(cid:18)(cid:113)

(M 2Γt)−1

(cid:19)

= 18 log

λ−d det(Γt)/δ

.

(cid:32)

(cid:115)

1
δ

det(M 2Γt)
det(M 2λI)

(cid:33)

The inequality here is Lemma 10 with Q = M 2λI, and for the last equality we use that det(cQ) = cd det(Q)
for a d × d positive semideﬁnite matrix Q. As two ﬁnal steps, we apply Lemma 9 and take a union bound

16

over all rounds T . Combining these, we get that for all T ,
(cid:115)

(cid:107)ˆθt − θ(cid:107)Γt ≤

λ +

Zτ ζτ

≤

λ +

18

log(

λ−d det(Γt)) + log(T /δ)

√

(cid:18)

(cid:113)

(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

√

√

≤

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ).

Therefore, with γ(T ) (cid:44)

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ) we can apply Lemma 6 to bound

√

the regret by

Reg(T ) ≤ (cid:112)2T log(1/δ) + 2γ(T )

T
(cid:88)

(cid:114)

t=1

tr(Γ−1

t Cov
b∼πt

(zt,b)).

Via a union bound, this inequality holds with probability at least 1 − 2δ. To ﬁnish the proof we need to
analyze this latter term. This is the contents of the following lemma. A related statement, with a similar proof,
appears in Abbasi-Yadkori et al. [1].
Lemma 12. Let X1, . . . , XT be a sequence of vectors in Rd with (cid:107)Xt(cid:107)2 ≤ 1 and deﬁne Γ1 (cid:44) λI, Γt (cid:44)
Γt−1 + Xt−1X (cid:62)

t−1. Then

tr(Γ−1

t XtX (cid:62)

t ) ≤ (cid:112)T d(1 + 1/λ) log(1 + T /(dλ)).

Proof. First, apply the Cauchy-Schwarz inequality to the left hand side to obtain

tr(Γ−1

t XtX (cid:62)

t ) ≤

tr(Γ−1

t XtX (cid:62)

t ).

(cid:118)
(cid:117)
(cid:117)
(cid:116)

√

T

T
(cid:88)

T =1

T
(cid:88)

(cid:113)

t=1

T
(cid:88)

(cid:113)

t=1

For the remainder of the proof we work only with the second term. Let us start by analyzing a slightly
different quantity, tr(Γ−1
t ). By concavity of log det(M ), we have

t+1XtX (cid:62)

log det(Γt) ≤ log det(Γt+1) + tr(Γ−1

t+1(Γt − Γt+1)),

which implies

T
(cid:88)

t=1

tr(Γ−1

t+1XtX (cid:62)

t ) = tr(Γ−1

t+1(Γt+1 − Γt)) ≤ log det(Γt+1) − log det(Γt)

As such, we obtain a telescoping sum

tr(Γ−1

t+1XtX (cid:62)

t ) ≤ log det(ΓT +1) − log det(Γ1) ≤ d log(λ + T /d) − d log λ = d log(1 + T /(dλ))

The ﬁrst inequality here uses the concavity argument and the second uses Lemma 9. To ﬁnish the proof, we
must translate back to Γ−1

. For this, we use the Sherman-Morrison-Woodbury identity, which reveals that

t

X (cid:62)

t Γ−1

t+1Xt = X (cid:62)

t (Γt + XtX (cid:62)

t )−1Xt = X (cid:62)
t

Γ−1

t −

(cid:32)

t Γ−1
t

Γ−1
t XtX (cid:62)
1 + (cid:107)Xt(cid:107)2

Γ−1
t

(cid:33)

Xt

=

(cid:107)Xt(cid:107)2
Γ−1
t
1 + (cid:107)Xt(cid:107)2

Γ−1
t

≥ (1 + 1/λ)−1(cid:107)Xt(cid:107)2

.

Γ−1
t

17

Here in the last step we use that (cid:107)Xt(cid:107)2

≤ (cid:107)Xt(cid:107)2

(λI)−1 ≤ 1/λ. Overall, we obtain

Γ−1
t

T
(cid:88)

t=1

tr(Γ−1

t XtX (cid:62)

t ) ≤ (1 + 1/λ)d log(1 + T /(dλ)),

and combined with the ﬁrst application of Cauchy-Schwarz, this proves the lemma.

Combining the lemmas, we have that with probability at least 1 − 2δ, the regret is at most

Reg(T ) ≤ (cid:112)2T log(1/δ) + 2γ(T )(cid:112)T d(1 + 1/λ) log(1 + T /(dλ))
= (cid:112)2T log(1/δ) + 2(cid:112)T d(1 + 1/λ) log(1 + T /(dλ))

(cid:16)√

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ)

(cid:17)

.

With λ = 1, this bound is O

√
(cid:16)(cid:112)T d log(T /δ) log(T /d) + d

T log(T /d)

(cid:17)
.

D Proof for the General Case

We now turn to the more general case. We need several additional lemmas.

Lemma 13 (Restatement of Lemma 8). Problem (3) is convex and always has a feasible solution. Speciﬁcally,
for any vectors z1, . . . , zn ∈ Rd and any positive deﬁnite matrix M , there exists a distribution w ∈ ∆([n])
with mean µw = Eb∼w[zb] such that

Proof. We analyze the minimax program

∀i ∈ [n],

(cid:107)zi − µw(cid:107)2

M ≤ tr(M Cov
b∼w

(zb)).

min
w∈∆([n])

max
i∈[n]

(cid:107)zi − µw(cid:107)2

M − tr(M Cov
w

(z)).

The goal is to show that the value of this program is non-negative, which will prove the result. Expanding the
deﬁnitions, we have

min
w∈∆([n])

max
i∈[n]

(cid:107)zi − µw(cid:107)2

M − tr(M Cov
w

(z))

= min

w∈∆([n])

max
v∈∆([n])

= min

v∈∆([n])

max
w∈∆([n])


(cid:107)zi − µw(cid:107)2

(cid:88)

vi


(cid:107)zi − µw(cid:107)2

(cid:88)

vi

i

i

M + µ(cid:62)

wM µw −

wjz(cid:62)

j M zj



(cid:88)

M + µ(cid:62)

wM µw −

wjz(cid:62)

j M zj

 .

j

j

(cid:88)





The last equivalence here is Sion’s Minimax Theorem [31], which is justiﬁed since both domains are compact
convex subsets of Rn and since the objective is linear in the maximizing variable v, and convex in the
minimizing variable w. This convexity is clear since µw is a linear in w, and hence the ﬁrst two terms are
convex quadratics (since M is positive deﬁnite), while the third term is linear in w. Thus Sion’s theorem lets
us swap the order of the minimization and maximization.

18

Now we upper bound the solution by setting w = v. This gives


(cid:107)zi − µv(cid:107)2

(cid:88)

vi

M + µ(cid:62)

v M µv −

vjz(cid:62)

j M zj



(cid:88)

j


(zi − µv)(cid:62)M (zi − µv) + µ(cid:62)

(cid:88)

vi

v M µv −

vjz(cid:62)

j M zj

 = 0.





(cid:88)

j

≤ max

v∈∆([n])

= max

v∈∆([n])

i

i

To prove the analog of Lemma 10, we need several additional tools. First, we use Freedman’s inequality

to derive a positive-semideﬁnite inequality relating the sample covariance matrix to the population matrix.

Lemma 14. Let X1, . . . , Xn be conditionally centered random vectors in Rd adapted to a ﬁltration {Ft}n
with (cid:107)Xi(cid:107)2 ≤ 1 almost surely. Deﬁne ˆΣ (cid:44) (cid:80)n
E[XiX (cid:62)
i
probability at least 1 − δ, the following holds simultaneously for all unit vectors v ∈ Rd:

i and Σ (cid:44) (cid:80)n

t=1
| Fi]. Then, with

i=1 XiX (cid:62)

i=1

v(cid:62)Σv ≤ 2v(cid:62) ˆΣv + 9d log(9n) + 8 log(2/δ).

This lemma is related to the Matrix Bernstein inequality, which can be used to control (cid:107)Σ − ˆΣ(cid:107)2, a
quantity that is quite similar to what we are controlling here. The Matrix Bernstein inequality can be used to
derive a high probability bound of the form

∀v ∈ Rd, (cid:107)v(cid:107)2 = 1,

v(cid:62)(Σ − ˆΣ)v ≤

(cid:107)Σ(cid:107)2 + c log(dn/δ),

1
2

for a constant c > 0. On one hand, this bound is stronger than ours since the deviation term depends only
logarithmically on the dimension. However, the variance term involves the spectral norm rather than a quantity
that depends on v as in our bound. Thus, Matrix Bernstein is worse when Σ is highly ill-conditioned, and
since we have essentially no guarantees on the spectrum of Σ, our specialized inequality, which is more
adaptive to the speciﬁc direction v, is crucial. Moreover, the worse dependence on d is inconsequential, since
the error will only appear in a lower order term.

Proof. First consider a single unit vector v ∈ Rd, we will apply a covering argument at the end of the proof.
By assumption, the sequence of sums {(cid:80)τ
τ =1 is a martingale, so we may
apply Freedman’s inequality [18, 6], which states that with probability at least 1 − δ

i=1 v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)
i

| Fi])v}n

|v(cid:62)( ˆΣ − Σ)v| ≤ 2

Var(v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)

i

| Fi])v | Fi) log(2/δ) + 2 log(2/δ).

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

Let us now upper bound the variance term: for each i,

Var(v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)
i

| Fi])v | Fi) ≤ E[(v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)
i

≤ E[(v(cid:62)Xi)4 | Fi] ≤ v(cid:62)E[XiX (cid:62)
i

| Fi] | Fi)v)2 | Fi]
| Fi]v,

where the last inequality follows from the fact that (cid:107)Xi(cid:107)2 ≤ 1 and (cid:107)v(cid:107)2 ≤ 1. Therefore, the cumulative
conditional variance is at most v(cid:62)Σv. Plugging this into Freedman’s inequality gives us

|v(cid:62)( ˆΣ − Σ)v| ≤ 2

v(cid:62)Σv log(2/δ) + 2 log(2/δ).

(cid:113)

19

Now, using the fact that 2

ab ≤ αa + b/α for any α > 0, with the choice α = 1/2, we get

√

|v(cid:62)( ˆΣ − Σ)v| ≤ v(cid:62)Σv/2 + 4 log(2/δ).

Re-arranging, this implies

v(cid:62)Σv ≤ 2v(cid:62) ˆΣv + 8 log(2/δ),

(7)

which is what we would like to prove, but we need it to hold simultaneously for all unit vectors v.

To do so, we now apply a covering argument. Let N be an (cid:15)-covering of the unit sphere in the projection
pseudo-metric d(u, v) = (cid:107)uu(cid:62) − vv(cid:62)(cid:107)2, with covering number N ((cid:15)). Then via a union bound, a version
of (7) holds simultaneously for all v ∈ N , where we rescale δ → δ/N ((cid:15)).

Consider another unit vector u and let v be the covering element. We have

u(cid:62)Σu = tr(Σ(uu(cid:62) − vv(cid:62))) + v(cid:62)Σv ≤ tr(Σ(uu(cid:62) − vv(cid:62))) + 2v(cid:62) ˆΣv + 8 log(2N ((cid:15))/δ)

= tr((Σ − 2 ˆΣ)(uu(cid:62) − vv(cid:62))) + 2u(cid:62) ˆΣu + 8 log(2N ((cid:15))/δ)
≤ (cid:107)Σ − 2 ˆΣ(cid:107)(cid:63)(cid:15) + 2u(cid:62) ˆΣu + 8 log(2N ((cid:15))/δ).

Here (cid:107) · (cid:107)(cid:63) denotes the nuclear norm, which is dual to the spectral norm (cid:107) · (cid:107)2. Since all vectors are bounded
by 1, we obtain

(cid:107)Σ − 2 ˆΣ(cid:107)(cid:63) ≤ dλmax(Σ − 2 ˆΣ) ≤ 3dn.

Overall, the following bound holds simultaneously for all unit vectors v ∈ Rd, except with probability at
most δ:

v(cid:62)Σv ≤ 3dn(cid:15) + 2v(cid:62) ˆΣv + 8 log(2N ((cid:15))/δ).

The last step of the proof is to bound the covering number N ((cid:15)). For this, we argue that a covering
of the unit sphere in the Euclidean norm sufﬁces, and by standard volumetric arguments, this set has
covering number at most (3/(cid:15))d. To see why this sufﬁces, let u be a unit vector and let v be the covering
element in the Euclidean norm, which implies that (cid:107)u − v(cid:107)2 ≤ (cid:15). Further assume that (cid:104)u, v(cid:105) > 0, which
imposes no restriction since the projection pseudo-metric is invariant to multiplying by −1. By deﬁnition
we also have (cid:104)u, v(cid:105) ≤ 1. Note that the projection norm is equivalent to the sine of the principal angle
between the two subspaces, which once we restrict to vectors with non-negative inner product means that
(cid:107)uu(cid:62) − vv(cid:62)(cid:107)2 = sin ∠(u, v). Now

sin ∠(u, v) = (cid:112)1 − (cid:104)u, v(cid:105)2 = (cid:112)(1 + (cid:104)u, v(cid:105))(1 − (cid:104)u, v(cid:105))

≤ (cid:112)2(1 − (cid:104)u, v(cid:105)) =

(cid:113)

(cid:107)u(cid:107)2

2 + (cid:107)v(cid:107)2

2 − 2(cid:104)u, v(cid:105) = (cid:107)u − v(cid:107)2 ≤ (cid:15).

Using the standard covering number bound, we now have

v(cid:62)Σv ≤ 3dn(cid:15) + 2v(cid:62) ˆΣv + 8d log(3/(cid:15)) + 8 log(2/δ).

Setting (cid:15) = 1/(3n) gives

v(cid:62)Σv ≤ d + 2v(cid:62) ˆΣv + 8d log(9n) + 8 log(2/δ) ≤ 2v(cid:62) ˆΣv + 9d log(9n) + 8 log(2/δ).

20

With the positive semideﬁnite inequality, we can work towards a self-normalized martingale concentration

bound. The following is a restatement of Lemma 7 from de la Peña et al. [15].

Lemma 15 (Lemma 7 of de la Peña et al. [15]). Let {Xi}n
valued random variables adapted to the ﬁltration {Fi}n
Then

i=1 be a sequence of conditionally centered vector-
i=1 and such that (cid:107)Xi(cid:107)2 ≤ B for some constant B.

Un(λ) = exp

λ(cid:62)

Xi − λ(cid:62)

XiX (cid:62)

i + E[XiX (cid:62)

i |Fi]

λ/2

(cid:32)

n
(cid:88)

i=1

(cid:32) n
(cid:88)

i=1

(cid:33)

(cid:33)

is a supermartingale with E[Un(λ)] ≤ 1 for all λ ∈ Rd.

The lemma is related to (6), but does not require that conditional probability law for Xi is symmetric,
which we used previously. To remove the symmetry requirement, it is crucial that the quadratic self-
normalization has both empirical and population terms. With this lemma, the same argument as in the proof
of Lemma 10, yields a self-normalized tail bound.

t=1 be a ﬁltration and let {(Zt, ζt)}T

t=1 be a stochastic process with Zt ∈ Rd and
Lemma 16. Let {Ft}T
ζt ∈ R such that (1) (Zt, ζt) is Ft measurable, (2) |ζt| ≤ M for all t ∈ [T ], (3) Zt ⊥⊥ ζt|Ft, and (4)
E[Zt|Ft] = 0. Let ˆΣ (cid:44) (cid:80)T
T |Ft]. Then for any positive deﬁnite matrix Q
we have

t and Σ (cid:44) (cid:80)T

t=1 ZtZ (cid:62)

E[ZtZ (cid:62)

t=1

P





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

Ztζt

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(Q+M 2( ˆΣ+Σ))−1

≥ 2 log

(cid:115)





1
δ

det(Q + M 2( ˆΣ + Σ))
det(Q)







 ≤ δ.

Proof. The proof is identical to Lemma 10, but uses Lemma 15 in lieu of (6).

We can now analyze the inﬂuence-adjusted estimator.

Lemma 17. Under Assumption 1 and Assumption 2 and assuming that λ ≥ 4d log(9T ) + 8 log(4T /δ), with
probability at least 1 − δ, the following holds simultaneously for all t ∈ [T ]:
√

(cid:107)ˆθt − θ(cid:107)Γt ≤

λ + (cid:112)27d log(1 + 2T /d) + 54 log(4T /δ).

Proof. Using the same argument as in the proof of Lemma 5, we get

(cid:107)ˆθt − θ(cid:107)Γt ≤

√

λ +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

Zτ ζτ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

,

where Zτ (cid:44) zτ − µτ and ζτ (cid:44) (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ , just as before. Now we must control this error
term, for which we need both Lemma 14 and Lemma 16. Apply Lemma 14 to the vectors Zτ , setting
ˆΣt (cid:44) (cid:80)t−1
E[Zτ Zτ | Fτ ]. With probability at least 1 − δ/(2T ), we have that for
all unit vectors v ∈ Rd

τ and Σt (cid:44) (cid:80)t−1

τ =1 Zτ Z (cid:62)

τ =1

v(cid:62)Σtv ≤ 2v(cid:62) ˆΣtv + 9d log(9t) + 8 log(4T /δ) ≤ 2v(cid:62) ˆΣtv + 9d log(9T ) + 8 log(4T /δ).

This implies a lower bound on all quadratic forms involving ˆΣt, which leads to positive semideﬁnite inequality

λI + ˆΣt (cid:23) (λ − 3d log(9T ) − 8/3 log(4T /δ))I + ( ˆΣt + Σt)/3.

21

This means that for any vector v, we have

(cid:107)v(cid:107)2

(λI+ ˆΣt)−1 ≤ (cid:107)v(cid:107)2
≤ 3(cid:107)v(cid:107)2

((λ−3d log(9T )−8/3 log(4T /δ))I+( ˆΣt+Σt)/3)−1
((3λ−9d log(9T )−8 log(4T /δ))I+ ˆΣt+Σt)−1.

Before we apply Lemma 16, we must introduce the range parameter M . Fix a round t and let A (cid:44)
((3λ − 9d log(9T ) − 8 log(4T /δ))I + ˆΣt + Σt) denote the matrix in the Mahalanobis norm. Then,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

A−1

Zτ ζτ

= M 2

Zτ ζτ

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(M 2A)−1

Now apply Lemma 16 with Q (cid:44) M 2(3λ − 9d log(9T ) − 8 log(4T /δ))I. Since we require Q (cid:31) 0, this
requires λ > 3d log(9T ) − 8/3 log(4T /δ), which is satisﬁed under the preconditions for the lemma. Under
this assumption, we get

(cid:107)

t−1
(cid:88)

τ =1

Zτ ζτ (cid:107)2

(λI+ ˆΣt)−1 ≤ 3M 2(cid:107)

Zτ ζτ (cid:107)2

(Q+M 2( ˆΣt+Σt))−1

t−1
(cid:88)

τ =1


≤ 6M 2 log



(cid:115)

4T
δ

det(Q + M 2( ˆΣt + Σt))
det(Q)



 ,

with probability at least 1 − δ/(2T ). With a union bound, the inequality holds simultaneously for all T , with
probability at least 1 − δ.

The last step is to analyze the determinant. Using the same argument as in the proof of Lemma 9, it is not

hard to show that

(cid:32)

det(Q + M 2( ˆΣt + Σt))
det(Q)

(cid:33)1/d

≤ 1 +

2(t − 1)
d(3λ − 9d log(9T ) − 8 log(4T /δ))

.

If we impose the slightly stronger condition that λ ≥ 4d log(9T ) + 8 log(4T /δ), then the term in the
denominator is at least 1, and then we have that

(cid:107)ˆθt − θ(cid:107)Γt ≤

λ + (cid:112)6M 2 log(4T /δ) + 3dM 2 log(1 + 2T /d).

Finally, as in the two-action case, we use the fact that |ζt| ≤ 3 (cid:44) M .

Recall the setting of γ(T ) (cid:44)

λ + (cid:112)27d log(1 + 2T /d) + 54 log(4T /δ) and the deﬁnition of λ (cid:44)
4d log(9T ) + 8 log(4T /δ). For the remainder of the proof, condition on the probability 1 − δ event
that Lemma 17 holds. We now turn to analyzing the regret.

Lemma 18. Let µt (cid:44) Ea∼πtzt,a where πt is the solution to (3) and assume the conclusion of Lemma 17
holds. Then with probability at least 1 − δ

√

√

Reg(T ) ≤ (1 + 6γ(T ))(cid:112)2T log(2/δ) + 3γ(T )

tr(Γ−1

t (zt,at − µt)(zt,at − µt)(cid:62)).

(cid:118)
(cid:117)
(cid:117)
(cid:116)T

T
(cid:88)

t=1

22

This lemma is slightly more complicated than Lemma 6.

Proof. First, using the same application of Azuma’s inequality as in the proof of Lemma 6, with probability
1 − δ/2, we have

Reg(T ) ≤ (cid:112)2T log(2/δ) +

Ea∼πt[(cid:104)θ, zt,a(cid:63)

t

− zt,a(cid:105) | Ft].

T
(cid:88)

t=1

Now we work with this latter expected regret

Ea∼πt [(cid:104)θ, zt,a(cid:63)

t

− zt,a(cid:105) | Ft] =

(cid:104)θ, zt,a(cid:63)

− µt(cid:105) ≤

t

(cid:104)ˆθ, zt,a(cid:63)

t

− µt(cid:105) + γ(T )(cid:107)zt,a(cid:63)

− µt(cid:107)Γ−1

.

t

t

For the ﬁrst term, we use the ﬁltration condition (2)

(cid:104)ˆθ, zt,a(cid:63)

t

− µt(cid:105) =

πt(a)(cid:104)ˆθ, zt,a(cid:63)

t

− zt,a(cid:105) ≤ γ(T )

πt(a)(cid:107)zt,a(cid:63)

t

− zt,a(cid:107)Γ−1

t

T
(cid:88)

t=1

(cid:88)

a∈At

≤ γ(T )(cid:107)zt,a(cid:63)

t

− µt(cid:107)Γ−1

t

+ γ(T )

πt(a)(cid:107)zt,a − µt(cid:107)Γ−1

.

t

T
(cid:88)

t=1

(cid:88)

a∈At

(cid:88)

a∈At

Applying the feasibility condition in (3), we can bound the expected regret by

Ea∼πt[(cid:104)θ, zt,a(cid:63)

t

− zt,a(cid:105) | Ft] ≤ 3γ(T )

tr(Γ−1

t Cov
a∼πt

(zt,a)) ≤ 3γ(T )

tr(Γ−1

t Cov
a∼πt

(zt,a)).

T
(cid:88)

(cid:114)

t=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)T

T
(cid:88)

t=1

To complete the proof, we need to relate the covariance, which takes expectation over the random action, with
the particular realization in the algorithm, since this realization affects the term Γt+1. Let Zt (cid:44) zt,at − µt
denote the centered realization, then the covariance term is

In order to derive a bound on (cid:80)T

t=1 tr(Γ−1

t Cova∼πt(zt,a)), we ﬁrst consider the following

Cov
a∼πt

(zt,a) = E[ZtZ (cid:62)
t

| Ft]

T
(cid:88)

t=1

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) − tr(Γ−1

t ZtZ (cid:62)

t ).

Observe that sequence of sums {(cid:80)τ
| Ft]) − tr(Γ−1
each term tr(Γ−1
the Freedman’s inequality reveals that with probability at least 1 − δ/2

| Ft]) − tr(Γ−1

E[ZtZ (cid:62)
τ =1 is a martingale. Also,
t
t ) is bounded by 1 because Γ1 = λI and λ > 1. Applying

t
t ZtZ (cid:62)

t=1 tr(Γ−1

E[ZtZ (cid:62)
t

t ZtZ (cid:62)

t )}T

t

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) − tr(Γ−1

t ZtZ (cid:62)

t ) ≤ 2

E[(Z (cid:62)

t Γ−1

t Zt)2 | Ft] log(2/δ) + 2 log(2/δ)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

t=1

T
(cid:88)

t=1

≤

1
2

T
(cid:88)

t=1

23

≤ 2

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) log(2/δ) + 2 log(2/δ)

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) + 4 log(2/δ).

Then rearranging and plugging back into our regret bound, we have

Reg(T ) ≤ (cid:112)2T log(2/δ) + 3γ(T )

(cid:118)
(cid:117)
(cid:117)
(cid:116)2T

(cid:32) T

(cid:88)

t=1

tr(Γ−1

t ZtZ (cid:62)

t ) + 4 log(2/δ)

(cid:33)

≤ (1 + 6γ(T ))(cid:112)2T log(2/δ) + 3γ(T )

tr(Γ−1

t ZtZ (cid:62)

t ).

(cid:118)
(cid:117)
(cid:117)
(cid:116)2T

T
(cid:88)

t=1

To conclude the proof of the theorem, apply Lemma 7, which applies on the last term on the RHS

of Lemma 18. Overall, with probability at least 1 − 2δ, we get

Reg(T ) ≤ (1 + 6γ(T ))(cid:112)2T log(2/δ) + 3γ(T )(cid:112)2T d(1 + 1/λ) log(1 + T /(dλ)).

Since λ = Θ(d log(T /δ)) and γ(T ) = O((cid:112)d log(T ) + (cid:112)log(T /δ)), we get with probability 1 − δ,

Reg(T ) ≤ O

√

(cid:16)

d

T log(T ) + (cid:112)dT log(T ) log(T /δ) + (cid:112)T log(T /δ) log(1/δ)

(cid:17)

.

References

[1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic

bandits. In Advances in Neural Information Processing Systems, 2011.

[2] Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efﬁcient algorithm

for bandit linear optimization. In Conference on Learning Theory, 2008.

[3] Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E Schapire. Taming the
monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine
Learning, 2014.

[4] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In

International Conference on Machine Learning, 2013.

[5] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed

bandit problem. SIAM Journal on Computing, 2002.

[6] Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandit
algorithms with supervised learning guarantees. In International Conference Artiﬁcial Intelligence and
Statistics, 2011.

[7] Peter J Bickel, Chris AJ Klaassen, Ya’acov Ritov, and Jon A Wellner. Efﬁcient and adaptive estimation

for semiparametric models. Springer New York, 1998.

[8] Sébastien Bubeck, Nicolo Cesa-Bianchi, and Sham M Kakade. Towards minimax policies for online

linear optimization with bandit feedback. In Conference on Learning Theory, 2012.

[9] Nicolo Cesa-Bianchi and Gábor Lugosi. Combinatorial bandits. Journal of Computer and System

Sciences, 2012.

24

[10] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duﬂo, Christian Hansen, Whit-
ney Newey, and James M. Robins. Double machine learning for treatment and causal parameters.
arXiv:1608.00060, 2016.

[11] Wei Chu, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandits with linear payoff functions.

In International Conference on Artiﬁcial Intelligence and Statistics, 2011.

[12] Thomas M Cover. Behavior of sequential predictors of binary sequences. In Conference on Information

Theory, Statistical Decision Functions and Random Processes, 1965.

[13] Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit

feedback. In Conference on Learning Theory, 2008.

[14] Victor H de la Peña, Tze Leung Lai, and Qi-Man Shao. Self-normalized processes: Limit theory and

statistical applications. Springer Science & Business Media, 2008.

[15] Victor H de la Peña, Michael J Klass, and Tze Leung Lai. Theory and applications of multivariate

self-normalized processes. Stochastic Processes and their Applications, 2009.

[16] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the
multi-armed bandit and reinforcement learning problems. Journal of Machine Learning Research, 2006.

[17] Sarah Filippi, Olivier Cappe, Aurélien Garivier, and Csaba Szepesvári. Parametric bandits: The

generalized linear case. In Advances in Neural Information Processing Systems, 2010.

[18] David A Freedman. On tail probabilities for martingales. The Annals of Probability, 1975.

[19] Kristjan Greenewald, Ambuj Tewari, Susan Murphy, and Predag Klasnja. Action centered contextual

bandits. In Advances in Neural Information Processing Systems, 2017.

[20] Akshay Krishnamurthy, Alekh Agarwal, and Miroslav Dudík. Contextual semibandits via supervised

learning oracles. In Advances in Neural Information Processing Systems, 2016.

[21] John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side

information. In Advances in Neural Information Processing Systems, 2008.

[22] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personal-

ized news article recommendation. In International Conference on World Wide Web, 2010.

[23] Lihong Li, Yu Lu, and Dengyong Zhou. Provable optimal algorithms for generalized linear contextual

bandits. In International Conference on Machine Learning, 2017.

[24] Jerzy Neyman. C(α) tests and their use. Sankhy¯a: The Indian Journal of Statistics, Series A, 1979.

[25] Alexander Rakhlin and Karthik Sridharan. Bistro: An efﬁcient relaxation-based method for contextual

bandits. In International Conference on Machine Learning, 2016.

[26] James M Robins and Andrea Rotnitzky. Recovery of information and adjustment for dependent

censoring using surrogate markers. In AIDS Epidemiology. Springer, 1992.

[27] James M Robins, Lingling Li, Eric Tchetgen Tchetgen, and Aad van der Vaart. Higher order inﬂuence
functions and minimax estimation of nonlinear functionals. In Probability and Statistics: Essays in
Honor of David A. Freedman. Institute of Mathematical Statistics, 2008.

25

[28] Peter M Robinson. Root-n-consistent semiparametric regression. Econometrica: Journal of the

[29] Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of

[30] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of

Econometric Society, 1988.

Operations Research, 2010.

Operations Research, 2014.

[31] Maurice Sion. On general minimax theorems. Paciﬁc Journal of Mathematics, 1958.

[32] Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dudík, John Langford, Damien
In Advances in Neural

Jose, and Imed Zitouni. Off-policy evaluation for slate recommendation.
Information Processing Systems, 2017.

[33] Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert E Schapire. Efﬁcient algorithms for adversarial

contextual learning. In International Conference on Machine Learning, 2016.

[34] Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health. In

Mobile Health. Springer, 2017.

[35] Anastasios Tsiatis. Semiparametric theory and missing data. Springer Science & Business Media, 2007.

26

8
1
0
2
 
l
u
J
 
6
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
0
2
4
0
.
3
0
8
1
:
v
i
X
r
a

Semiparametric Contextual Bandits

Akshay Krishnamurthy ∗2, Zhiwei Steven Wu †2, and Vasilis Syrgkanis ‡3

2Microsoft Research NYC, New York, NY
3Microsoft Research New England, Cambridge, MA

July 17, 2018

Abstract

This paper studies semiparametric contextual bandits, a generalization of the linear stochastic bandit
problem where the reward for an action is modeled as a linear function of known action features confounded
by a non-linear action-independent term. We design new algorithms that achieve ˜O(d
T ) regret over T
rounds, when the linear function is d-dimensional, which matches the best known bounds for the simpler
unconfounded case and improves on a recent result of Greenewald et al. [19]. Via an empirical evaluation,
we show that our algorithms outperform prior approaches when there are non-linear confounding effects on
the rewards. Technically, our algorithms use a new reward estimator inspired by doubly-robust approaches
and our proofs require new concentration inequalities for self-normalized martingales.

√

1 Introduction

A number of applications including online personalization, mobile health, and adaptive clinical trials require
that an agent repeatedly makes decisions based on user or patient information with the goal of optimizing some
metric, typically referred to as a reward. For example, in online personalization problems, we might serve
content based on user history and demographic information with the goal of maximizing user engagement
with our service. Since counterfactual information is typically not available, these problems require algorithms
to carefully balance exploration—making potentially suboptimal decisions to acquire new information—with
exploitation—using collected information to make better decisions. Such problems are often best modeled
with the framework of contextual bandits, which captures the exploration-exploitation tradeoff and enables
rich decision making policies but ignores the long-term temporal effects that make general reinforcement
learning challenging. Contextual bandit algorithms have seen recent success in applications, including news
recommendation [22] and mobile health [34].

Contextual bandit algorithms can be categorized as either parametric or agnostic, depending on whether
they model the relationship between the reward and the decision or not. Parametric approaches typically
assume that the reward is a (generalized) linear function of a known decision-speciﬁc feature vector [17, 11,
1, 4]. Once this function is known to high accuracy, it can be used to make near-optimal decisions. Exploiting

∗akshay@cs.umass.edu
†zsw@umn.edu
‡vasy@microsoft.com

1

this fact, algorithms for this setting focus on learning the parametric model. Unfortunately, fully parametric
assumptions are often unrealistic and challenging to verify in practice, and these algorithms may perform
poorly when the assumptions do not hold.

In contrast, agnostic approaches make no modeling assumptions about the reward and instead compete
with a large class of decision-making policies [21, 3]. While these policies are typically parametrized in
some way, these algorithms provably succeed under weaker conditions and are generally more robust than
parametric ones. On the other hand, they typically have worse statistical guarantees, are conceptually much
more complex, and have high computational overhead, technically requiring solving optimization problems
that are NP-hard in the worst case. This leads us to a natural question:

Is there an algorithm that inherits the simplicity and statistical guarantees of the parametric
methods and the robustness of the agnostic ones?

Working towards an afﬁrmative answer to this question, we consider a semiparametric contextual bandit
setup where the reward is modeled as a linear function of the decision confounded by an additive non-linear
perturbation that is independent of the decision. This setup signiﬁcantly generalizes the standard parametric
one, allowing for complex, non-stationary, and non-linear rewards (See Section 2 for a precise formulation).
On the other hand, since this perturbation is just a baseline reward for all decisions, it has no inﬂuence on the
optimal one, which depends only on the unknown linear function. In the language of econometrics and causal
modeling, the treatment effect is linear.

In this paper, we design new algorithms for the semiparametric contextual bandits problem. When the
linear part of the reward is d-dimensional, our algorithms achieve ˜O(d
T ) regret over T rounds, even when
the features and the confounder are chosen by an adaptive adversary. This guarantee matches the best results
for the simpler linear stochastic bandit problem up to logarithmic terms, showing that there is essentially no
statistical price to pay for robustness to confounding effects. On the other hand, our algorithm and analysis is
quite different, and it is not hard to see that existing algorithms for stochastic bandits fail in our more general
setting. Our regret bound also improves on a recent result of Greenewald et al. [19], who consider the same
setup but study a weaker notion of regret. Our algorithm, main theorem, and comparisons are presented
in Section 3.

√

We also compare our algorithm to approaches from both parametric and agnostic families in an empirical
study (we use a linear policy class for agnostic approaches). In Section 5, we evaluate several algorithms on
synthetic problems where the reward is (a) linear, and (b) linear with confounding. In the linear case, our
approach learns, but is slightly worse than the baselines. On the other hand, when there is confounding, our
algorithm signiﬁcantly outperforms both parametric and agnostic approaches. As such, these experiments
demonstrate that our algorithm represents a favorable trade off between statistical efﬁciency and robustness.
On a technical level, our algorithm and analysis require several new ideas. First, we derive a new estimator
for linear models in the presence of confounders, based on recent and classical work in semiparametric
statistics and econometrics [28, 10]. Second, since standard algorithms using optimism principles fail to
guarantee consistency of this new estimator, we design a new randomized algorithm, which can be viewed
as an adaptation of the action-elimination method of Even-Dar et al. [16] to the contextual bandits setting.
Finally, analyzing the semiparametric estimator requires an intricate deviation argument, for which we derive
a new self-normalized inequality for vector-valued martingales using tools from de la Peña et al. [14, 15].

2 Preliminaries

We study a generalization of the linear stochastic bandit problem with action-dependent features and action-
independent confounder. The learning process proceeds for T rounds, and in round t, the learner receives a

2

context xt (cid:44) {zt,a}a∈A where zt,a ∈ Rd and A is the action set, which we assume to be large but ﬁnite. The
learner then chooses an action at ∈ A and receives reward

rt(at) (cid:44) (cid:104)θ, zt,at(cid:105) + ft(xt) + ξt,

(1)

where θ ∈ Rd is an unknown parameter vector, ft(xt) is a confounding term that depends on the context xt
but, crucially, does not depend on the chosen action at, and ξt is a noise term that is centered and independent
of at.

For each round t, let a(cid:63)
t

(cid:44) argmaxa∈A(cid:104)θ, zt,a(cid:105) denote the optimal action for that round. The goal of our

algorithm is to minimize the regret, deﬁned as

Reg(T ) (cid:44)

rt(a(cid:63)

t ) − rt(at) =

(cid:104)θ, zt,a(cid:63)

t

− zt,at(cid:105).

T
(cid:88)

t=1

T
(cid:88)

t=1

Observe that the noise term ξt, and, more importantly, the confounding term ft(xt) are absent in the ﬁnal
expression, since they are independent of the action choice.

We consider the challenging setting where the context xt and the confounding term ft(·) are chosen by
an adaptive adversary, so they may depend on all information from previous rounds. This is formalized in the
following assumption.

Assumption 1 (Environment). We assume that xt = {zt,a}a∈A, ft, ξt are generated at the beginning of
round t, before at is chosen. We assume that xt and ft are chosen by an adaptive adversary, and that ξt
satisﬁes E[ξt|xt, ft] = 0 and |ξt| ≤ 1.

We also impose mild regularity assumptions on the parameter, the feature vectors, and the confounding

functions.

Assumption 2 (Boundedness). Assume that (cid:107)θ(cid:107)2 ≤ 1 and that (cid:107)zt,a(cid:107)2 ≤ 1 for all a ∈ A, t ∈ [T ]. Further
assume that ft(·) ∈ [−1, 1] for all t ∈ [T ].

For simplicity, we assume an upper bound of 1 in these conditions, but our algorithm and analysis can be

adapted to more generic regularity conditions.

Related work. Our setting is related to linear stochastic bandits and several variations that have been
studied in recent years. Among these, the closest is the work of Greenewald et al. [19] who consider the
same setup and provide a Thompson Sampling algorithm using a new reward estimator that eliminates the
confounding term. Motivated by applications in medical intervention, they consider a different notion of
regret from our more-standard notion and, as such, the results are somewhat incomparable. For our notion
of regret, their analysis can produce a T 2/3-style regret bound, which is worse than our optimal
T bound.
See Section 3.3 for a more detailed comparison.

√

Other results for linear stochastic bandits include upper-conﬁdence bound algorithms [29, 11, 1], Thomp-
son sampling algorithms [4, 30], and extensions to generalized linear models [17, 23]. However, none of
these models accommodate arbitrary and non-linear confounding effects. Moreover, apart from Thompson
sampling, all of these algorithms use deterministic action-selection policies (conditioning on the history),
which provably incurs Ω(T ) regret in our setting, as we will see.

One can accommodate confounded rewards via an agnostic-learning approach to contextual bandits [5,
21, 3]. In this framework, we make no assumptions about the reward, but rather compete with a class
of parameterized policies (or experts). Since a d-dimensional linear policy is optimal in our setting, an

3

agnostic algorithm with a linear policy class addresses precisely our notion of regret. However there are two
disadvantages. First, agnostic algorithms are all computationally intractable, either because they enumerate
the (inﬁnitely large) policy class, or because they assume access to optimization oracles that can solve NP-hard
K, the
problems in the worst case. Second, most agnostic approaches have regret bounds that grow with
number of actions, while our bound is completely independent of K.

√

We are aware of one approach that is independent of K, but it requires enumeration of an inﬁnitely
large policy class. This method is based on ideas from the adversarial linear and combinatorial bandits
(cid:44) (zt,a, 1) ∈ Rd+1, our setting can
literature [13, 2, 8, 9]. Writing θt (cid:44) (θ, ft(xt)) ∈ Rd+1 and z(cid:48)
be re-formulated in the adversarial linear bandits framework. However, standard linear bandit algorithms
compete with the best ﬁxed action vector in hindsight, rather than the best policy with time-varying action
sets. To resolve this, one can use the linear bandits reward estimator [32] in a contextual bandit algorithm
like EXP4 [5], but this approach is not computationally tractable with the linear policy class. For our setting,
we are not aware of any computationally efﬁcient approaches, even oracle-based approaches, that achieve
poly(d)

T regret with no dependence on the number of actions.

√

t,a

We resolve the challenge of confounded rewards with an estimator from the semiparametric statistics
literature [35], which focuses on estimating functionals of a nonparametric model. Most estimators are based
on Neyman Orthogonalization [24], which uses moment equations that are insensitive to nuisance parameters
in a method-of-moments approach [10]. These orthogonal moments typically involve a linear correction
to an initial nonparametric estimate using so-called inﬂuence functions [7, 27]. Robinson [28] used this
approach for the ofﬂine version of our setting (known as the partially linear regression (PLR) model) where
he demonstrated a form of double-robustness [26] to poor estimation of the nuisance term (in our case ft(xt)).
We generalize Robinson’s work to the online setting, showing how orthogonalized estimators can be used for
adaptive exploration. This requires several new techniques, including a novel action selection policy and a
self-normalized inequality for vector-valued martingales.

3 Algorithm and Results

In this section, we describe our algorithm and present our main theoretical result, an ˜O(d
for the semiparametric contextual bandits problem.

√

T ) regret bound

3.1 A Lower Bound

Before turning to the algorithm, we ﬁrst present a lower bound against deterministic algorithms. Since the
functions ft may be chosen by an adaptive adversary, it is not hard to show that this setup immediately
precludes the use of deterministic algorithms.

Proposition 3. Consider an algorithm that, at round t, chooses an action at as a deterministic function of
the observable history Ht (cid:44) {x1:t, a1:t−1, r1:t−1}. There exists a semiparametric contextual bandit instance
with d = 2 and K = 2 where the regret of the algorithm is at least T /2.

See Appendix B for the proof, which resembles the standard argument against deterministic online
learning algorithms [12]. The main difference is that the adversary uses the confounding term to corrupt
the information that the learner receives, whereas, in the standard proof, the adversary chooses the optimal
action in response to the learner. In fact, deterministic algorithms can succeed in the full information version
of our setting, since taking differences between rewards eliminates the confounder. Thus, bandit feedback
plays a crucial role in our construction and the bandit setting is considerably more challenging than the full
information analogue.

4

Algorithm 1: BOSE (Bandit orthogonalized semiparametric estimation)

Input :T, δ ∈ (0, 1).

1 Set λ ← 4d log(9T ) + 8 log(4T /δ) and γ(T ) ←
2 Initialize ˆθ ← 0 ∈ Rd, Γ ← λId×d.
3 for t = 1, . . . , T do
4

Observe xt = {zt,a}a∈A
Filter

√

λ + (cid:112)27d log(1 + 2T /d) + 54 log(4T /δ).

5

6

7

8

At ←

(cid:110)
a ∈ A | ∀b ∈ A, (cid:104)ˆθ, zt,b − zt,a(cid:105) ≤ γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

(cid:111)

.

Find distribution πt ∈ ∆(At) such that ∀a ∈ At (We use
Covb∼πt(zt,b) (cid:44) E[zt,bz(cid:62)

t,b] − (Ezt,b)(Ezt,b)(cid:62).)
(cid:107)zt,a − Eb∼πtzt,b(cid:107)2

Γ−1 ≤ tr(Γ−1 Cov
b∼πt

(zt,b)).

Sample at ∼ πt and play at. Observe rt(at). (rt(at) = (cid:104)θ, zt,at(cid:105) + ft(xt) + ξt.)
Let µt = Ea∼πt[zt,a | xt] and update parameters

(2)

(3)

Γ ← Γ + (zt,at − µt)(zt,at − µt)(cid:62),

ˆθ ← Γ−1

(zτ,aτ − µτ )rτ (aτ ).

(4)

t
(cid:88)

τ =1

We emphasize that, except for the Thompson Sampling approach [4], essentially all algorithms for the
linear stochastic bandit problem use deterministic strategies, so they provably fail in the semiparametric
setting. As we mentioned, Thompson Sampling was analyzed in our setting by Greenewald et al. [19], but
T -type regret bound (See Section 3.3 for a more quantitative and detailed
they do not obtain the optimal
comparison). In contrast, our algorithm is quite different from all of these approaches; it ensures enough
randomization to circumvent the lower bound and also achieves the optimal

√

√

√

T regret.
T ) lower bound for linear stochastic bandits [13],

To conclude this discussion, we remark that the Ω(d

which also applies to randomized algorithms, holds in our more general setting as well.

3.2 The Algorithm

Pseudocode for the algorithm, which we call BOSE, for “Bandit Orthogonalized Semiparametric Estimation,"
is displayed in Algorithm 1. The algorithm maintains an estimate ˆθ for the true parameter θ, which it uses in
each round to select an action via two steps: (1) an action elimination step that removes suboptimal actions,
and (2) an optimization step that ﬁnds a good distribution over the surviving actions. The algorithm then
samples and plays an action from this distribution and uses the observed reward to update the parameter
estimate ˆθ. This parameter estimation step is the third main element of the algorithm. We now describe each
of these three components in detail.

Parameter estimation. For simplicity, we use zt (cid:44) zt,at to denote the feature vector for the action that
was chosen at round t, and similarly we use rt (cid:44) rt(at). Using all previously collected data, speciﬁcally
{zτ , rτ }t
τ =1 at the end of round t, we would like to estimate the parameter θ. First, if fτ (xτ ) were identically

5

zero, by exploiting the linear parametrization we could use ridge regression, which with some λ > 0 gives

(cid:32)

(cid:33)−1 t

ˆθRidge (cid:44)

λI +

zτ z(cid:62)
τ

t
(cid:88)

τ =1

(cid:88)

τ =1

zτ rτ .

This estimator appears in most prior approaches for linear stochastic bandits [29, 11, 1]. Unfortunately,
since fτ (xτ ) is non-zero, ˆθRidge has non-trivial and non-vanishing bias, so even in benign settings it is not a
consistent estimator for θ.1

Our approach to eliminating the bias from the confounding term fτ (xτ ) is to center the feature vectors
zτ . Intuitively, in the ridge estimator, if zτ is centered, then zτ (rτ − (cid:104)θ(cid:63), zτ (cid:105)) is mean zero, even when
there is non-negligible bias in the second term. As such, the error of the corresponding estimator can be
expected to concentrate around zero. In the semiparametric statistics literature, this is known as Neyman
Orthogonalization [24], which was analyzed in the context of linear regression by Robinson [28] and in a
more general setting by Chernozhukov et al. [10].

To center the feature vector, we will, at round t, choose action at by sampling from some distribution
πt ∈ ∆(A). Let µt (cid:44) Eat∼πt[zt,at|xt] denote the mean feature vector, taking expectation only over our
random action choice. With this notation, the orthogonalized estimator is

Γ = λI +

(zτ − µτ )(zτ − µτ )(cid:62),

ˆθ = Γ−1

(zτ − µτ )rτ .

t
(cid:88)

τ =1

t
(cid:88)

τ =1

ˆθ is a Ridge regression version of Robinson’s classical semiparametric regression estimator [28]. The
estimator was originally derived for observational studies where one might not know the propensities µτ
exactly, and the standard description involves estimates ˆfτ and ˆµτ for the confounding term fτ and the
propensities µτ respectively. Informally, the estimator achieves a form of double-robustness, in the sense
that it is accurate if either of these auxilliary estimators are. In our case, since we know the propensities
µτ exactly, we can use an inconsistent estimator for the confounding term, so we simply set ˆfτ (xτ ) ≡ 0.
In Lemma 5, we prove a precise ﬁnite sample concentration inequality for this orthogonalized estimator,
showing that the confounding term ft(xt) does not introduce any bias. While the estimator has been studied
in prior works [28], to our knowledge, our error guarantee is novel.

The convergence rate of the orthogonalized estimator depends on the eigenvalues of the matrix Γ, and
we must carefully select actions to ensure these eigenvalues are sufﬁciently large. To see why, notice that
any deterministic action-selection approach with the orthogonalized estimator (including conﬁdence based
approaches), will fail, since zt = µt, so the eigenvalues of Γ do not grow rapidly and in fact the estimator is
identically 0. This argument motivates our new action selection scheme which ensure substantial conditional
covariance.

Action selection. Our action selection procedure has two main elements. First using our estimate ˆθ, we
eliminate any action that is provably suboptimal. Based on our analysis for the estimator ˆθ, at round t, we can
certify action a is suboptimal, if we can ﬁnd another action b such that

(cid:104)ˆθ, zt,b − zt,a(cid:105) > γ(T )(cid:107)zt,b − zt,a(cid:107)Γ−1.

1A related estimator can be used to evaluate the reward of a policy, as in linear and combinatorial bandits [9], but to achieve
adequate exploration, one must operate over the policy class, which leads to computational intractability. We would like to use ˆθ to drive
exploration, and this seems to require a consistent estimator. See Appendix A for a simple example demonstrating how using a biased
estimator in a conﬁdence-based approach results in linear regret.

6

Here γ(T ) is the constant speciﬁed in the algorithm, and (cid:107)x(cid:107)M (cid:44)
x(cid:62)M x denotes the Mahalanobis norm.
Using our conﬁdence bound for ˆθ in Lemma 5 below, this inequality certiﬁes that action b has higher expected
reward than action a, so we can safely eliminate a from consideration.

The next component is to ﬁnd a distribution over the surviving actions, denoted A(cid:48)

t at round t, with
t) that we use is the solution to the following feasibility

sufﬁcient covariance. The distribution πt ∈ ∆(A(cid:48)
problem

√

∀a ∈ A(cid:48)
t,

(cid:107)zt,a − Eb∼πtzt,b(cid:107)2

Γ−1 ≤ tr(Γ−1 Cov
b∼πt

(zt,b)).

For intuition, the left hand side of the constraint for action a is an upper bound on the expected regret if a is
the optimal action on this round. Thus, the constraints ensure that the regret is related to the covariance of the
distribution, which means that if we incur high regret, the covariance term Covb∼πt(zt,b) will be large. Since
we use a sample from πt to update our parameter estimate, this means that whenever the instantaneous regret
is large, we must learn substantially about the parameter. In this way, the distribution πt balances exploration
and exploitation. We will see in Lemma 8 that this program is convex and always has a feasible solution.

Our action selection scheme bears some resemblance to action-elimination approaches that have been
studied in various bandit settings [16]. The main differences are that we adapt these ideas to the contextual
setting and carefully choose a distribution over the surviving actions to balance exploration and exploitation.

3.3 The Main Result

We now turn to the main result, a regret guarantee for BOSE.

Theorem 4. Consider the semiparametric contextual bandit problem under Assumption 1 and Assumption 2.
T log(T /δ)).
For any parameter δ ∈ (0, 1), with probability at least 1−δ, Algorithm 1 has regret at most O(d

√

The constants, and indeed a bound depending on λ and γ(T ) can be extracted from the proof, provided in

the appendix. To interpret the regret bound, it is worth comparing with several related results:

√

Comparison with linear stochastic bandits. While most algorithms for linear stochastic bandits prov-
ably fail in our setting (via Proposition 3), the best regret bounds here are O((cid:112)dT log(T K/δ)) [11] and
T log(T ) + (cid:112)dT log(T ) log(1/δ)) [1] depending on whether we assume that the number of actions
O(d
K is small or not. This latter result is optimal when the number of actions is large [13], which is the setting
we are considering here. Since our bound matches this optimal regret up to logarithmic factors, and since
linear stochastic bandits are a special case of our semiparametric setting, our result is therefore also optimal
up to logarithmic factors. An interesting open question is whether an ˜O((cid:112)dT log(K/δ)) regret bound is
achievable in the semiparametric setting.

Comparison with agnostic contextual bandits. The best oracle-based agnostic approaches achieve ˜O(
regret [3], incurring a polynomial dependence on the number of actions K, although there is one inefﬁcient
method that can achieve ˜O(d
T ),2 as we discussed previously. To date, all efﬁcient methods in the agnostic
setting require some form of i.i.d. [3] or transductive assumption [33, 25] on the contexts, which we do not
assume here.

√

√

dKT )

2This follows easily by combining ideas from Auer et al. [5] and Cesa-Bianchi and Lugosi [9].

7

Comparison with Greenewald et al. [19]. Greenewald et al. [19] consider a very similar setting to ours,
where rewards are linear with confounding, but where one default action a0 always has zt,a0 ≡ 0 ∈ Rd.
Applications in mobile health motivate a restriction that the algorithm choose the a0 action with probability
∈ [p, 1 − p] for some small p ∈ (0, 1). Their work also introduces a new notion of regret where they compete
with the policy that also satisﬁes this constraint but otherwise chooses the optimal action a(cid:63)
t . In this setup,
they obtain an ˜O(d2

T ) regret bound, which has a worse dimension dependence than Theorem 4.

√

While the setup is somewhat different, we can still translate our result into a regret bound in their setting,
since BOSE can support the probability constraint, and by coupling the randomness between BOSE and
the optimal policy, the regret is unaffected.3 On the other hand, since the constant in their regret bound
scales with 1/p, their results as stated are vacuous when p = 0 which is precisely our setting. For our more
challenging regret deﬁnition, their analysis can produce a suboptimal T 2/3-style regret bound, and in this
sense, Theorem 4 provides a quantitative improvement.

Summary. BOSE achieves essentially the same regret bound as the best linear stochastic bandit methods,
but in a much more general setting. On the other hand, the agnostic methods succeed under even weaker
assumptions, but have worse regret guarantees and/or are computationally intractable. Thus, BOSE broadens
the scope for computationally efﬁcient contextual bandit learning.

4 Proof Sketch

In the two arm case, one should set γ(T ) (cid:44)

We sketch the proof of Theorem 4 in the two-action case (|A| = 2), which has a much simpler proof that
preserves the main ideas. The technical machinery needed for the general case is much more sophisticated,
and we brieﬂy describe some of these steps at the end of this section, with a complete proof in the Appendix.
λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ) and λ = O(1),
which differs slightly from the algorithm pseudocode for the more general case. Additionally, note that with
two actions, the uniform distribution over At is always feasible for Problem (3). Speciﬁcally, if the ﬁltered
set has cardinality 1, we simply play that action deterministically, otherwise we play one of the two actions
uniformly at random.

√

The proof has three main steps. First we analyze the orthogonalized regression estimator deﬁned in (4).
Second, we study the action selection mechanism and relate the regret incurred to the error bound for the
orthogonalized estimator. Finally, using a somewhat standard potential argument, we show how this leads to
T -type regret bound. For the proof, let ˆθt, Γt be the estimator and covariance matrix used on round t,
a
both based on t − 1 samples.

√

For the estimator, we prove the following lemma for the two action case. The main technical ingredient
is a self-normalized inequality for vector-valued martingales, which can be obtained using ideas from de la
Peña et al. [15].

Lemma 5. Under Assumption 1 and Assumption 2, let K = 2 and γ(T ) (cid:44)
Then, with probability at least 1 − δ, the following holds simultaneously for all t ∈ [T ]:

√

λ+(cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ).

3Technically it is actually smaller by a factor of (1 − p).

(cid:107)ˆθt − θ(cid:107)Γt ≤ γ(T ).

8

Proof. Using the deﬁnitions and Assumption 1, it is not hard to re-write

ˆθt = Γ−1

t (Γt − λI)θ + Γ−1

t

Zτ ζτ ,

t−1
(cid:88)

τ =1

where Zτ (cid:44) zτ,aτ − µτ and ζτ (cid:44) (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ . Further deﬁne St (cid:44) (cid:80)t−1
the triangle inequality the error is at most

τ =1 Zτ ζτ . Then, applying

(cid:107)ˆθt − θ(cid:107)Γt ≤ (cid:107)λθ(cid:107)Γ−1

t

+ (cid:107)St(cid:107)Γ−1

.

t

√

λ since Γt (cid:23) λI. To control the second term, we need to use a self-normalized
The ﬁrst term here is at most
concentration inequality, since Zτ is a random variable, and the normalizing term Γt = λI + (cid:80)t−1
τ =1 Zτ Z (cid:62)
τ
depends on the random realizations. In Lemma 10 in the appendix, we prove that with probability at least
1 − δ, for all t ∈ [T ]

(cid:107)St(cid:107)2

Γ−1
t

≤ 9d log(1 + T /(dλ)) + 18 log(T /δ).

(5)

The lemma follows from straightforward calculations.

Before proceeding, it is worth commenting on the difference between our self-normalized inequality (5)
and a slightly different one used by Abbasi-Yadkori et al. [1] for the linear case. In their setup, they have
that ζτ is conditionally centered and sub-Gaussian, which simpliﬁes the argument since after ﬁxing the Zτ s
(and hence Γt), the randomness in the ζτ s sufﬁces to provide concentration. In our case, we must use the
randomness in Zτ itself, which is more delicate, since Zτ affects the numerator St, but also the normalizer
Γt. In spite of this additional technical challenge, the two self-normalized processes admit similar bounds.

Next, we turn to the action selection step, where recall that either a single action is played deterministically,

or the actions are played uniformly at random.
Lemma 6. Let µt (cid:44) Ea∼πtzt,a where πt is the solution to (3), and assume that the conclusion of Lemma 5
holds. Then with probability at least 1 − δ

Reg(T ) ≤ (cid:112)2T log(1/δ) + 2γ(T )

T
(cid:88)

(cid:114)

t=1

tr(Γ−1

t Cov
b∼πt

(zt,b)).

Proof. We ﬁrst study the instantaneous regret, taking expectation over the random action. For this, we must
consider two cases. First, with Lemma 5, if |At| = 1, we argue that the regret is actually zero. This follows
from the Cauchy-Schwarz inequality since assuming At = {a} we get

(cid:104)θ, zt,a − zt,b(cid:105) ≥ (cid:104)ˆθt, zt,a − zt,b(cid:105) − γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

t

which is non-negative using the fact that b was eliminated. Therefore a is the optimal action and we incur no
regret. Since πt has no covariance, the upper bound holds.

On the other rounds, we set πt = Unif({a, b}) and hence µt = (zt,a + zt,b)/2. Assuming again that a is

the optimal action, the expected regret is

(cid:104)θ, zt,a − µt(cid:105) =

(cid:104)θ, zt,a − zt,b(cid:105) ≤

1
2

(cid:104)ˆθt, zt,a − zt,b(cid:105) + γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

t

(cid:17)

≤ γ(T )(cid:107)zt,a − zt,b(cid:107)Γ−1

≤ 2γ(T )

t

t Cov
b∼πt

(zt,b)).

(cid:16)

1
2
(cid:114)
tr(Γ−1

9

Here the ﬁrst inequality uses Cauchy-Schwarz, the second uses (2), since neither action was eliminated, and
the third uses (3). This bounds the expected regret, and the lemma follows by Azuma’s inequality.

The last step of the proof is to control the sequence

T
(cid:88)

(cid:114)

t=1

tr(Γ−1

t Cov
b∼πt

(zt,b)).

First, recall that

(zt,b) (cid:44) Eb∼πt

(cid:2)(zt,b − µt)(zt,b − µt)(cid:62)(cid:3)

Cov
b∼πt

with µt (cid:44) Eb∼πt[zt,b]. Since in the two-arm case πt either chooses an arm deterministically or uniformly
randomizes between the two arms, the following always holds:

(zt,b) = (zt,at − µt)(zt,at − µt)(cid:62).

Cov
b∼πt

It follows that Γt+1 (cid:44) Γt + Covb∼πt (zt,b), and with Γ1 (cid:44) λI, the standard potential argument for online
ridge regression applies. We state the conclusion here, and provide a complete proof in the appendix.

Lemma 7. Let Γt, πt be deﬁned as above and deﬁne Mt (cid:44) (zt,at − µt)(zt,at − µt)(cid:62). Then

T
(cid:88)

(cid:113)

t=1

tr(Γ−1

t Mt) ≤ (cid:112)dT (1 + 1/λ) log(1 + T /(dλ)).

Combining the three lemmas establishes a regret bound of

Reg(T ) ≤ O

√
(cid:16)(cid:112)T d log(T /δ) log(T /d) + d

T log(T /d)

(cid:17)

with probability at least 1 − δ in the two-action case.

Extending to many actions. Several more technical steps are required for the general setting. First, the
martingale inequality used in Lemma 5 requires that the random vectors are symmetric about the origin.
This is only true for the two-action case, and in fact a similar inequality does not hold in general for the
non-symmetric situation that arises with more actions. In the non-symmetric case, both the empirical and the
population covariance must be used in the normalization, so the analogue of (5) is instead

(cid:107)St(cid:107)2

(Γt+EΓt)−1 ≤ 27d log(1 + 2T /d) + 54 log(4T /δ).

On the other hand, the error term for our estimator depends only on the empirical covariance Γt. To correct
for the discrepancy, we use a covering argument4 to establish

λI + Γt (cid:23) (λ − 6d log(T /δ))I + (Γt + EΓt)/3.

With this semideﬁnite inequality, we can translate from the Mahalanobis norm in the weaker self-normalized
bound to one with just Γt, which controls the error for the estimator.

We also argue that problem (3) is always feasible, which is the contents of the following lemma.

4For technical reasons, the Matrix Bernstein inequality does not sufﬁce here since it introduces a dependence on the maximal variance.

See Appendix for details.

10

Figure 1: Synthetic experiments with d = 10, K = 2. Left: A linear environment where action-features are
uniformly from the unit sphere. Center: A confounded environment with features from the sphere. Right: A
confounded environment with features from the sphere intersected with the positive orthant. Algorithms are
BOSE, OFUL [1], ILTCB [3], EPSGREEDY [21], and THOMPSON [4]. Agnostic approaches use a linear
policy class.

Lemma 8. Problem (3) is convex and always has a feasible solution. Speciﬁcally, for any vectors z1, . . . , zn ∈
Rd and any positive deﬁnite matrix M , there exists a distribution w ∈ ∆([n]) with mean µw (cid:44) Eb∼wzb such
that

∀i ∈ [n], (cid:107)zi − µw(cid:107)2

M ≤ tr(M Cov
b∼w

(zb)).

The proof uses convex duality. Integrating these new arguments into the proof for the two-action case

leads to Theorem 4.

5 Experiments

We conduct a simple experiment to compare BOSE with several other approaches5. We simulate three different
environments that follow the semiparametric contextual bandits model with d = 10, K = 2. In the ﬁrst
setting the reward is linear and the action features are drawn uniformly from the unit sphere. In the latter two
settings, we set ft(xt) = − maxa(cid:104)θ, zt,a(cid:105), which is related to the construction in the proof of Proposition 3.
One of these semiparametric settings has action features sampled from the unit sphere, while for the other, we
sample from the intersection of the unit sphere and the positive orthant.

In Figure 1, we plot the performance of Algorithm 1 against four baseline algorithms: (1) OFUL:
the optimistic algorithm for linear stochastic bandits [1], (2) THOMPSON sampling for linear contextual
bandits [4], (3) EPSGREEDY: the (cid:15)-greedy approach [21] with a linear policy class, (4) ILTCB: a more
sophisticated agnostic algorithm [3] with linear policy class. The ﬁrst algorithm is deterministic, so can have
linear regret in our setting, but is the natural baseline and one we hope to improve. Thompson Sampling is
another natural baseline, and a variant was used by Greenewald et al. [19] in essentially the same setting
as ours. The latter two have (Kd)1/3T 2/3 and
KdT regret bounds respectively under our assumptions,
but require solving cost-sensitive classiﬁcation problems, which are NP-hard in general. Following prior
empirical evaluations [20], we use a surrogate loss formulation based on square loss minimization in the
implementation.

√

The results of the experiment are displayed in Figure 1, where we plot the cumulative regret against the
number of rounds T . All algorithms have a single parameter that governs the degree of exploration. In BOSE

5Our code is publicly available at http://github.com/akshaykr/oracle_cb/.

11

and OFUL, this is the constant γ(T ) in the conﬁdence bound, in THOMPSON it is the variance of the prior,
and in ILTCB and EPSGREEDY it is the amount of uniform exploration performed by the algorithm. For
each algorithm we perform 10 replicates for each of 20 values of the corresponding parameter, and we plot
the best average performance, with error bars corresponding to ±2 standard deviations.

In the linear experiment (Figure 1, left panel), BOSE performs the worst, but is competitive with the
agnostic approaches, demonstrating a price to pay for robustness. The experimental setup in the center panel
is identical except with confounding, and BOSE is robust to this confounding, with essentially the same
performance, while the three baselines degrade dramatically. Finally, when the features lie in the positive
orthant (right panel), OFUL degrades further, while BOSE remains highly effective.

Regarding the baselines, we make two remarks:

1. Intuitively, the positive orthant setting is more challenging for OFUL since there is less inherent

randomness in the environment to overcome the confounding effect.

2. The agnostic approaches, despite strong regret guarantees, perform somewhat poorly in our experiments,
and we believe this for three reasons. First, our surrogate-loss implementation is based on an implicit
realizability assumption, which is not satisﬁed here. Second, we expect that the constant factors in their
regret bounds are signiﬁcantly larger than those of BOSE or OFUL. For computational reasons, we
only solve the optimization problem in ILTCB every 50 rounds, which causes a further constant factor
increase in the regret.

Overall, while BOSE is worse than other approaches in the linear environment, the experiment demonstrates
that when the environment is not perfectly linear, approaches based on realizability assumptions (either
explicitly like in OFUL, or implicitly like in implementations of ILTCB and EPSGREEDY), can fail. We
emphasize that linear environments are rare in practice, and such assumptions are typically impossible to
verify. We therefore believe that trading off a small loss in performance in the specialized linear case for
signiﬁcantly more robustness, as BOSE demonstrates, is desirable.

6 Discussion

This paper studies a generalization of the linear stochastic bandits setting, where rewards are confounded
by an adaptive adversary. Our new algorithm, BOSE, achieves the optimal regret, and also matches (up to
logarithmic factors) the best algorithms for the linear case. Our empirical evaluation shows that BOSE offers
signiﬁcantly more robustness than prior approaches, and performs well in several environments.

12

A Using the OLS Estimator

Here we construct an example problem to demonstrate how using the standard OLS estimator can fail in
the semiparametric setting. While not a comprehensive proof against all asymptotically biased approaches,
similar examples can be constructed for related estimators.

Consider a two-dimensional problem with two actions and no stochastic noise, where θ = e2, the second
standard basis vector. On the even rounds, the actions are z1 = (1, 1), z2 = (1, 1/3) and the confounding
term is f = −1. On the odd rounds, the actions are z1 = z2 = (1, 0) and the confounding term is f = 1. For
any policy for selecting actions, the OLS estimator before round t (for even t) is the solution to the following
optimization problem:

minimizew∈R2 α(w1 + w2)2 + (1 − α)(w1 + w2/3 + 2/3)2 + (w1 − 1)2 = L(w)

where α ∈ [0, 1] corresponds to the fraction of the even rounds (up to round t) where the policy chose z1.
We will argue that, for any α, the solution to this problem ˆw has ˆw2 < 0. Since there is no stochastic noise,
there is no need for conﬁdence bounds once the covariance is full rank, which happens after the second round.
Together, this implies that any sensible policy based on ˆw will prefer z2 to z1 on the even rounds, but z1 yields
higher reward by a ﬁxed constant. Thus using OLS in a conﬁdence-based approach leads to linear regret.

We now show that ˆw2 is strictly negative. We have

∂L(w)
∂w1
∂L(w)
∂w2

= 2α(w1 + w2) + 2(1 − α)(w1 + w2/3 + 2/3) + 2(w1 − 1),

= 2α(w1 + w2) +

(1 − α)(w1 + w2/3 + 2/3).

2
3

Setting both equations equal to zero yields the following system:

4w1 + (2/3 + 4α/3)w2 = 2/3 + 4α/3,

(2/3 + 4α/3)w1 + (2/9 + 16α/9)w2 = 4α/9 − 4/9.

The solution to this system is

w1 =

(2α + 1)2
−4α2 + 12α + 1

,

w2 =

4α2 + 5
4α2 − 12α − 1

,

provided that 4α2 (cid:54)= 12α + 1, which is not possible with α ∈ [0, 1]. In the interval [0, 1] we have that
4α2 − 12α − 1 < 0, and hence w2 < 0. Thus, the OLS estimator incorrectly predicts that z2 receives higher
reward than z1 on the even rounds. Since conﬁdence intervals are not needed, the algorithm suffers linear
reget.

B Proof of Proposition 3

We consider two possible values for the true parameter: θ1 = e1 ∈ R2, θ2 = e2 ∈ R2. At all rounds, the
context xt = {e1, e2} contains just two actions, and we further assume that the noise term ξt = 0 almost
surely. Since the action at is a deterministic function of the history, it can also be computed by the adaptive
adversary at the beginning of the round, and the adversary chooses

ft(xt) = −1{at = argmax

(cid:104)θ, zt,a(cid:105)}.

a

13

We show that rt(at) = 0 for all rounds t. Assume the parameter is θ1 so the optimal action is a(cid:63)
t = e1 and
the suboptimal action e2 has (cid:104)θ, e2(cid:105) = 0. If the learner chooses action e2, then the adversary sets ft(xt) = 0,
so rt(at) = 0. On the other hand, if the learner chooses action e1, then the adversary sets ft(xt) = −1 so
the reward is also zero. Similarly, if θ = θ2, the observed reward is always zero. Since the algorithm is
deterministic, it behaves identically regardless of whether the parameter is θ1 or θ2. In one of these instances
the algorithm must choose the suboptimal action at least T /2 times, leading to the lower bound.

C Proof for the Two-Action Case

We ﬁrst focus on the simpler two action case. Before turning to the main analysis, we prove two supporting
lemmas. The ﬁrst is an algebraic inequality relating matrix determinants to traces. This inequality also
appears in Abbasi-Yadkori et al. [1].

Lemma 9. Let X1, . . . , Xn denote vectors in Rd with (cid:107)Xi(cid:107)2 ≤ L for all i ∈ [n]. Deﬁne Γ (cid:44) λI +
(cid:80)n

i=1 XiX (cid:62)

i . Then

Proof. We will apply the following standard argument:

det(Γ) ≤ (λ + nL2/d)d.

det(Γ)1/d ≤

tr(Γ) =

tr(λI) +

tr(XiX (cid:62)

i ) = λ +

(cid:107)Xi(cid:107)2

2 ≤ λ + nL2/d.

1
d

1
d

1
d

n
(cid:88)

i=1

1
d

n
(cid:88)

i=1

The ﬁrst step is a spectral version of the AM-GM inequality and the remaining steps use linearity of the trace
operator and the boundedness conditions.

The second lemma is a new self-normalized concentration inequality for vector valued martingales.

Lemma 10 (Symmetric self-normalized inequality). Let {Ft}T
t=1 be a
stochastic process with Zt ∈ Rd and ζt ∈ R such that (1) (Zt, ζt) is Ft measurable, (2) |ζt| ≤ M for all
t ∈ [T ], (3) Zt ⊥⊥ ζt|Ft, (4) E[Zt|Ft] = 0, and (5) for all x ∈ Rd, L((cid:104)x, Zt(cid:105) | Ft) = L(−(cid:104)x, Zt(cid:105) | Ft)
where L denotes the probability law, so that Zt is conditionally symmetric. Let Σ (cid:44) (cid:80)T
t . Then for
any positive deﬁnite matrix Q we have

t=1 be a ﬁltration and let {(Zt, ζt)}T

t=1 ZtZ (cid:62)

P





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

Ztζt

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(Q+M 2Σ)−1

≥ 2 log

(cid:32)

(cid:115)

1
δ

det(Q + M 2Σ)
det(Q)

(cid:33)

 ≤ δ.

Proof. The proof follows the recipe in de la Peña et al. [15] (See also de la Peña et al. [14] for a more
comprehensive treatment including the univariate case). We start by applying the Chernoff method. Let
¯Σ (cid:44) Q + M 2Σ. We can write

P





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯Σ−1

Ztζt

≥ 2 log

(cid:32)

(cid:115)

1
δ

det( ¯Σ)
det(Q)

(cid:33)





 = P

exp



1
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

Ztζt



exp



(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯Σ−1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
2

T
(cid:88)

t=1



 ≥

(cid:115)





det( ¯Σ)
det(Q)

1
δ

Ztζt







 .

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯Σ−1



(cid:115)

≤ δE



det(Q)
det( ¯Σ)

14

Therefore, if we prove that this latter expectation is at most one, we will arrive at the conclusion. A similar
statement appears in Theorem 1 of de la Peña et al. [15], but our process is slightly different due to the
presence of ζt. To bound this latter expectation, ﬁx some λ ∈ Rd and consider an exponentiated process with
the increments

(cid:18)

Dλ
t

(cid:44) exp

(cid:104)λ, Ztζt(cid:105) −

M 2(cid:104)λ, Zt(cid:105)2
2

(cid:19)

.

Observe that E[Dλ

t |Ft] ≤ 1 since by the conditional symmetry of Zt, we have

E[Dλ

t |Ft] = E (cid:2)E (cid:2)Dλ

t | Ft, ζt

(cid:20)

(cid:20)

E

exp

= E

(cid:20)

(cid:20)

E

exp

= E

(cid:20)

(cid:20)

E

exp

≤ E

(cid:3)

(cid:3) | Ft
(cid:18) −M 2(cid:104)λ, Zt(cid:105)2
2
(cid:18) −M 2(cid:104)λ, Zt(cid:105)2
2
(cid:18) −M 2(cid:104)λ, Zt(cid:105)2
2

1
2

(cid:19)

(cid:19)

× cosh((cid:104)λ, Ztζt) | Ft, ζt

| Ft

(cid:21)

(cid:21)

(cid:19)

+

(cid:104)λ, Ztζt(cid:105)2
2

(cid:21)

(cid:21)

| Ft, ζt

| Ft

≤ 1.

×

(exp((cid:104)λ, Ztζt(cid:105)) + exp(−(cid:104)λ, Ztζt(cid:105)) | Ft, ζt

| Ft

(cid:21)

(cid:21)

This argument ﬁrst uses the conditional symmetry of Zt and the conditional independence of Zt, ζt, then the
identity (ex + e−x)/2 = cosh(x) and ﬁnally the analytical inequality cosh(x) ≤ ex2/2. Finally in the last
step we use the bound |ζt| ≤ M . This implies that the martingale U λ
τ is a super-martingale with
t
E[U λ

t ] ≤ 1 for all t, since by induction

τ =1 Dλ

(cid:44) (cid:81)t

E[U λ

t ] = E[U λ

t−1

E[Dλ

t |Ft]] ≤ E[U λ

t−1] ≤ . . . ≤ 1.

(6)

Now we apply the method of mixtures. In a standard application of the Chernoff method, we would choose
λ to maximize E[U λ
T ], but since we still have an expectation, we cannot swap expectation and maximum.
Instead, we integrate the inequality E[U λ
T ] ≤ 1, which holds for any λ, against λ drawn from a Gaussian
distribution with covariance Q−1. By Fubini’s theorem, we can swap the expectations to obtain

1 ≥ Eλ∼N (0,Q−1)E[U λ

T ] = E

(cid:90)

T (2π)−d/2(cid:112)det(Q) exp(−λ(cid:62)Qλ/2)dλ
U λ
(cid:32) T

M 2λ(cid:62)((cid:80)T

t )λ + λ(cid:62)Qλ

t=1 ZtZ (cid:62)
2

(cid:33)

dλ

(cid:88)

t=1

(cid:18)

= E

(2π)−d/2(cid:112)det(Q) exp

(cid:104)λ, Ztζt(cid:105) −

= E

(2π)−d/2(cid:112)det(Q) exp

(cid:104)λ, S(cid:105) −

M 2λ(cid:62)Σλ + λ(cid:62)Qλ
2

(cid:19)

dλ,

(cid:90)

(cid:90)

where S (cid:44) (cid:80)T
can be rewritten as

t=1 Ztζt and recall that Σ (cid:44) (cid:80)T

t=1 ZtZ (cid:62)

t . By completing the square, the term in the exponent

(cid:104)λ, S(cid:105) −

M 2λ(cid:62)Σλ + λ(cid:62)Qλ
2

1
2

=

(cid:0)−(λ − ¯Σ−1S)(cid:62) ¯Σ(λ − ¯Σ−1S) + S(cid:62) ¯Σ−1S(cid:1) ,

where recall that ¯Σ (cid:44) M 2Σ + Q. As such we obtain

1 ≥ E

(cid:20)
exp (cid:0)S(cid:62) ¯Σ−1S/2(cid:1) ×

(cid:90)

(2π)−d/2(cid:112)det(Q) exp

(cid:18) −(λ − ¯Σ−1S)(cid:62) ¯Σ(λ − ¯Σ−1S)
2

(cid:19)(cid:21)

dλ

(cid:115)

= E

det(Q)
det( ¯Σ)

exp (cid:0)S(cid:62) ¯Σ−1S(cid:1) .

15

This proves the lemma.

Equipped with the two lemmas, we can now turn to the analysis of the inﬂuence-adjusted estimator.

Lemma 11 (Restatement of Lemma 5). Under Assumption 1 and Assumption 2, with probability at least
1 − δ, the following holds simultaneously for all t ∈ [T ]:

(cid:107)ˆθt − θ(cid:107)Γt ≤

√

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ).

Proof. Recall that we deﬁne ˆθt, Γt to be the estimator and matrix used in round t, based on t − 1 examples.
Fixing a round t, we start by expanding the deﬁnition of ˆθt. We use the shorthand zτ (cid:44) zτ,aτ , µτ (cid:44)
Eb∼πτ [zτ,b], and rτ (cid:44) rτ (aτ ).

t−1
(cid:88)

τ =1

t−1
(cid:88)

τ =1

ˆθt = Γ−1

t

(zτ − µτ )rτ = Γ−1

t

(zτ − µτ )((cid:104)θ, zτ (cid:105) + fτ (xτ ) + ξτ )

= Γ−1
t

(zτ − µτ )((cid:104)θ, zτ − µτ (cid:105) + (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ )

= (Γt)−1(Γt − λI)θ + Γ−1

t

(zτ − µτ )((cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ ).

t−1
(cid:88)

τ =1

t−1
(cid:88)

τ =1

Let Zτ (cid:44) zτ − µτ and ζτ (cid:44) (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ . With this expansion, we can write

(cid:107)ˆθt − θ(cid:107)Γt = (cid:107) − λΓ−1

t θ + Γ−1

t

Zτ ζτ (cid:107)Γt ≤ (cid:107)λθ(cid:107)Γ−1

+

t

Zτ ζτ

t−1
(cid:88)

τ =1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

√

≤

λ +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

Zτ ζτ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

.

To ﬁnish the proof, we apply Lemma 10 to this last term. To verify the preconditions of the lemma, let
Fτ (cid:44) σ(x1, . . . , xτ , a1, . . . , aτ −1, ξ1, . . . , ξτ −1) denote the σ-algebra corresponding to the τ th round, after
observing the context xτ . Then the policy πτ and hence the action aτ are Fτ measurable and so is the noise
term ξτ . Therefore, Zτ = zτ,aτ − Ea∼πτ [zτ,a] is measurable, which veriﬁes the ﬁrst precondition. Using the
boundedness properties in Assumption 2, we know that |ζτ | ≤ 3 (cid:44) M , and by construction of the random
variables, we have Zτ ⊥⊥ ζτ |Fτ and E [Zτ |Fτ ] = 0. Finally, for the symmetry property, either Zτ |Fτ ≡ 0
if one action is eliminated, or otherwise we have µτ = 1
2 (zτ,1 + zτ,2) since there are only two actions. In
this case the random variable Zτ |Fτ = (cid:15)τ (zτ,1 − zτ2 )/2 where (cid:15)τ is a Rademacher random variable. By
inspection this is clearly conditionally symmetric. As such, we may apply Lemma 10, which reveals that with
probability at least 1 − δ,

Zτ ζτ

= M 2

Zτ ζτ

≤ 2M 2 log

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Γ−1
t

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

τ =1
(cid:18)(cid:113)

(M 2Γt)−1

(cid:19)

= 18 log

λ−d det(Γt)/δ

.

(cid:32)

(cid:115)

1
δ

det(M 2Γt)
det(M 2λI)

(cid:33)

The inequality here is Lemma 10 with Q = M 2λI, and for the last equality we use that det(cQ) = cd det(Q)
for a d × d positive semideﬁnite matrix Q. As two ﬁnal steps, we apply Lemma 9 and take a union bound

16

over all rounds T . Combining these, we get that for all T ,
(cid:115)

(cid:107)ˆθt − θ(cid:107)Γt ≤

λ +

Zτ ζτ

≤

λ +

18

log(

λ−d det(Γt)) + log(T /δ)

√

(cid:18)

(cid:113)

(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

√

√

≤

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ).

Therefore, with γ(T ) (cid:44)

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ) we can apply Lemma 6 to bound

√

the regret by

Reg(T ) ≤ (cid:112)2T log(1/δ) + 2γ(T )

T
(cid:88)

(cid:114)

t=1

tr(Γ−1

t Cov
b∼πt

(zt,b)).

Via a union bound, this inequality holds with probability at least 1 − 2δ. To ﬁnish the proof we need to
analyze this latter term. This is the contents of the following lemma. A related statement, with a similar proof,
appears in Abbasi-Yadkori et al. [1].
Lemma 12. Let X1, . . . , XT be a sequence of vectors in Rd with (cid:107)Xt(cid:107)2 ≤ 1 and deﬁne Γ1 (cid:44) λI, Γt (cid:44)
Γt−1 + Xt−1X (cid:62)

t−1. Then

tr(Γ−1

t XtX (cid:62)

t ) ≤ (cid:112)T d(1 + 1/λ) log(1 + T /(dλ)).

Proof. First, apply the Cauchy-Schwarz inequality to the left hand side to obtain

tr(Γ−1

t XtX (cid:62)

t ) ≤

tr(Γ−1

t XtX (cid:62)

t ).

(cid:118)
(cid:117)
(cid:117)
(cid:116)

√

T

T
(cid:88)

T =1

T
(cid:88)

(cid:113)

t=1

T
(cid:88)

(cid:113)

t=1

For the remainder of the proof we work only with the second term. Let us start by analyzing a slightly
different quantity, tr(Γ−1
t ). By concavity of log det(M ), we have

t+1XtX (cid:62)

log det(Γt) ≤ log det(Γt+1) + tr(Γ−1

t+1(Γt − Γt+1)),

which implies

T
(cid:88)

t=1

tr(Γ−1

t+1XtX (cid:62)

t ) = tr(Γ−1

t+1(Γt+1 − Γt)) ≤ log det(Γt+1) − log det(Γt)

As such, we obtain a telescoping sum

tr(Γ−1

t+1XtX (cid:62)

t ) ≤ log det(ΓT +1) − log det(Γ1) ≤ d log(λ + T /d) − d log λ = d log(1 + T /(dλ))

The ﬁrst inequality here uses the concavity argument and the second uses Lemma 9. To ﬁnish the proof, we
must translate back to Γ−1

. For this, we use the Sherman-Morrison-Woodbury identity, which reveals that

t

X (cid:62)

t Γ−1

t+1Xt = X (cid:62)

t (Γt + XtX (cid:62)

t )−1Xt = X (cid:62)
t

Γ−1

t −

(cid:32)

t Γ−1
t

Γ−1
t XtX (cid:62)
1 + (cid:107)Xt(cid:107)2

Γ−1
t

(cid:33)

Xt

=

(cid:107)Xt(cid:107)2
Γ−1
t
1 + (cid:107)Xt(cid:107)2

Γ−1
t

≥ (1 + 1/λ)−1(cid:107)Xt(cid:107)2

.

Γ−1
t

17

Here in the last step we use that (cid:107)Xt(cid:107)2

≤ (cid:107)Xt(cid:107)2

(λI)−1 ≤ 1/λ. Overall, we obtain

Γ−1
t

T
(cid:88)

t=1

tr(Γ−1

t XtX (cid:62)

t ) ≤ (1 + 1/λ)d log(1 + T /(dλ)),

and combined with the ﬁrst application of Cauchy-Schwarz, this proves the lemma.

Combining the lemmas, we have that with probability at least 1 − 2δ, the regret is at most

Reg(T ) ≤ (cid:112)2T log(1/δ) + 2γ(T )(cid:112)T d(1 + 1/λ) log(1 + T /(dλ))
= (cid:112)2T log(1/δ) + 2(cid:112)T d(1 + 1/λ) log(1 + T /(dλ))

(cid:16)√

λ + (cid:112)9d log(1 + T /(dλ)) + 18 log(T /δ)

(cid:17)

.

With λ = 1, this bound is O

√
(cid:16)(cid:112)T d log(T /δ) log(T /d) + d

T log(T /d)

(cid:17)
.

D Proof for the General Case

We now turn to the more general case. We need several additional lemmas.

Lemma 13 (Restatement of Lemma 8). Problem (3) is convex and always has a feasible solution. Speciﬁcally,
for any vectors z1, . . . , zn ∈ Rd and any positive deﬁnite matrix M , there exists a distribution w ∈ ∆([n])
with mean µw = Eb∼w[zb] such that

Proof. We analyze the minimax program

∀i ∈ [n],

(cid:107)zi − µw(cid:107)2

M ≤ tr(M Cov
b∼w

(zb)).

min
w∈∆([n])

max
i∈[n]

(cid:107)zi − µw(cid:107)2

M − tr(M Cov
w

(z)).

The goal is to show that the value of this program is non-negative, which will prove the result. Expanding the
deﬁnitions, we have

min
w∈∆([n])

max
i∈[n]

(cid:107)zi − µw(cid:107)2

M − tr(M Cov
w

(z))

= min

w∈∆([n])

max
v∈∆([n])

= min

v∈∆([n])

max
w∈∆([n])


(cid:107)zi − µw(cid:107)2

(cid:88)

vi


(cid:107)zi − µw(cid:107)2

(cid:88)

vi

i

i

M + µ(cid:62)

wM µw −

wjz(cid:62)

j M zj



(cid:88)

M + µ(cid:62)

wM µw −

wjz(cid:62)

j M zj

 .

j

j

(cid:88)





The last equivalence here is Sion’s Minimax Theorem [31], which is justiﬁed since both domains are compact
convex subsets of Rn and since the objective is linear in the maximizing variable v, and convex in the
minimizing variable w. This convexity is clear since µw is a linear in w, and hence the ﬁrst two terms are
convex quadratics (since M is positive deﬁnite), while the third term is linear in w. Thus Sion’s theorem lets
us swap the order of the minimization and maximization.

18

Now we upper bound the solution by setting w = v. This gives


(cid:107)zi − µv(cid:107)2

(cid:88)

vi

M + µ(cid:62)

v M µv −

vjz(cid:62)

j M zj



(cid:88)

j


(zi − µv)(cid:62)M (zi − µv) + µ(cid:62)

(cid:88)

vi

v M µv −

vjz(cid:62)

j M zj

 = 0.





(cid:88)

j

≤ max

v∈∆([n])

= max

v∈∆([n])

i

i

To prove the analog of Lemma 10, we need several additional tools. First, we use Freedman’s inequality

to derive a positive-semideﬁnite inequality relating the sample covariance matrix to the population matrix.

Lemma 14. Let X1, . . . , Xn be conditionally centered random vectors in Rd adapted to a ﬁltration {Ft}n
with (cid:107)Xi(cid:107)2 ≤ 1 almost surely. Deﬁne ˆΣ (cid:44) (cid:80)n
E[XiX (cid:62)
i
probability at least 1 − δ, the following holds simultaneously for all unit vectors v ∈ Rd:

i and Σ (cid:44) (cid:80)n

t=1
| Fi]. Then, with

i=1 XiX (cid:62)

i=1

v(cid:62)Σv ≤ 2v(cid:62) ˆΣv + 9d log(9n) + 8 log(2/δ).

This lemma is related to the Matrix Bernstein inequality, which can be used to control (cid:107)Σ − ˆΣ(cid:107)2, a
quantity that is quite similar to what we are controlling here. The Matrix Bernstein inequality can be used to
derive a high probability bound of the form

∀v ∈ Rd, (cid:107)v(cid:107)2 = 1,

v(cid:62)(Σ − ˆΣ)v ≤

(cid:107)Σ(cid:107)2 + c log(dn/δ),

1
2

for a constant c > 0. On one hand, this bound is stronger than ours since the deviation term depends only
logarithmically on the dimension. However, the variance term involves the spectral norm rather than a quantity
that depends on v as in our bound. Thus, Matrix Bernstein is worse when Σ is highly ill-conditioned, and
since we have essentially no guarantees on the spectrum of Σ, our specialized inequality, which is more
adaptive to the speciﬁc direction v, is crucial. Moreover, the worse dependence on d is inconsequential, since
the error will only appear in a lower order term.

Proof. First consider a single unit vector v ∈ Rd, we will apply a covering argument at the end of the proof.
By assumption, the sequence of sums {(cid:80)τ
τ =1 is a martingale, so we may
apply Freedman’s inequality [18, 6], which states that with probability at least 1 − δ

i=1 v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)
i

| Fi])v}n

|v(cid:62)( ˆΣ − Σ)v| ≤ 2

Var(v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)

i

| Fi])v | Fi) log(2/δ) + 2 log(2/δ).

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

Let us now upper bound the variance term: for each i,

Var(v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)
i

| Fi])v | Fi) ≤ E[(v(cid:62)(XiX (cid:62)

i − E[XiX (cid:62)
i

≤ E[(v(cid:62)Xi)4 | Fi] ≤ v(cid:62)E[XiX (cid:62)
i

| Fi] | Fi)v)2 | Fi]
| Fi]v,

where the last inequality follows from the fact that (cid:107)Xi(cid:107)2 ≤ 1 and (cid:107)v(cid:107)2 ≤ 1. Therefore, the cumulative
conditional variance is at most v(cid:62)Σv. Plugging this into Freedman’s inequality gives us

|v(cid:62)( ˆΣ − Σ)v| ≤ 2

v(cid:62)Σv log(2/δ) + 2 log(2/δ).

(cid:113)

19

Now, using the fact that 2

ab ≤ αa + b/α for any α > 0, with the choice α = 1/2, we get

√

|v(cid:62)( ˆΣ − Σ)v| ≤ v(cid:62)Σv/2 + 4 log(2/δ).

Re-arranging, this implies

v(cid:62)Σv ≤ 2v(cid:62) ˆΣv + 8 log(2/δ),

(7)

which is what we would like to prove, but we need it to hold simultaneously for all unit vectors v.

To do so, we now apply a covering argument. Let N be an (cid:15)-covering of the unit sphere in the projection
pseudo-metric d(u, v) = (cid:107)uu(cid:62) − vv(cid:62)(cid:107)2, with covering number N ((cid:15)). Then via a union bound, a version
of (7) holds simultaneously for all v ∈ N , where we rescale δ → δ/N ((cid:15)).

Consider another unit vector u and let v be the covering element. We have

u(cid:62)Σu = tr(Σ(uu(cid:62) − vv(cid:62))) + v(cid:62)Σv ≤ tr(Σ(uu(cid:62) − vv(cid:62))) + 2v(cid:62) ˆΣv + 8 log(2N ((cid:15))/δ)

= tr((Σ − 2 ˆΣ)(uu(cid:62) − vv(cid:62))) + 2u(cid:62) ˆΣu + 8 log(2N ((cid:15))/δ)
≤ (cid:107)Σ − 2 ˆΣ(cid:107)(cid:63)(cid:15) + 2u(cid:62) ˆΣu + 8 log(2N ((cid:15))/δ).

Here (cid:107) · (cid:107)(cid:63) denotes the nuclear norm, which is dual to the spectral norm (cid:107) · (cid:107)2. Since all vectors are bounded
by 1, we obtain

(cid:107)Σ − 2 ˆΣ(cid:107)(cid:63) ≤ dλmax(Σ − 2 ˆΣ) ≤ 3dn.

Overall, the following bound holds simultaneously for all unit vectors v ∈ Rd, except with probability at
most δ:

v(cid:62)Σv ≤ 3dn(cid:15) + 2v(cid:62) ˆΣv + 8 log(2N ((cid:15))/δ).

The last step of the proof is to bound the covering number N ((cid:15)). For this, we argue that a covering
of the unit sphere in the Euclidean norm sufﬁces, and by standard volumetric arguments, this set has
covering number at most (3/(cid:15))d. To see why this sufﬁces, let u be a unit vector and let v be the covering
element in the Euclidean norm, which implies that (cid:107)u − v(cid:107)2 ≤ (cid:15). Further assume that (cid:104)u, v(cid:105) > 0, which
imposes no restriction since the projection pseudo-metric is invariant to multiplying by −1. By deﬁnition
we also have (cid:104)u, v(cid:105) ≤ 1. Note that the projection norm is equivalent to the sine of the principal angle
between the two subspaces, which once we restrict to vectors with non-negative inner product means that
(cid:107)uu(cid:62) − vv(cid:62)(cid:107)2 = sin ∠(u, v). Now

sin ∠(u, v) = (cid:112)1 − (cid:104)u, v(cid:105)2 = (cid:112)(1 + (cid:104)u, v(cid:105))(1 − (cid:104)u, v(cid:105))

≤ (cid:112)2(1 − (cid:104)u, v(cid:105)) =

(cid:113)

(cid:107)u(cid:107)2

2 + (cid:107)v(cid:107)2

2 − 2(cid:104)u, v(cid:105) = (cid:107)u − v(cid:107)2 ≤ (cid:15).

Using the standard covering number bound, we now have

v(cid:62)Σv ≤ 3dn(cid:15) + 2v(cid:62) ˆΣv + 8d log(3/(cid:15)) + 8 log(2/δ).

Setting (cid:15) = 1/(3n) gives

v(cid:62)Σv ≤ d + 2v(cid:62) ˆΣv + 8d log(9n) + 8 log(2/δ) ≤ 2v(cid:62) ˆΣv + 9d log(9n) + 8 log(2/δ).

20

With the positive semideﬁnite inequality, we can work towards a self-normalized martingale concentration

bound. The following is a restatement of Lemma 7 from de la Peña et al. [15].

Lemma 15 (Lemma 7 of de la Peña et al. [15]). Let {Xi}n
valued random variables adapted to the ﬁltration {Fi}n
Then

i=1 be a sequence of conditionally centered vector-
i=1 and such that (cid:107)Xi(cid:107)2 ≤ B for some constant B.

Un(λ) = exp

λ(cid:62)

Xi − λ(cid:62)

XiX (cid:62)

i + E[XiX (cid:62)

i |Fi]

λ/2

(cid:32)

n
(cid:88)

i=1

(cid:32) n
(cid:88)

i=1

(cid:33)

(cid:33)

is a supermartingale with E[Un(λ)] ≤ 1 for all λ ∈ Rd.

The lemma is related to (6), but does not require that conditional probability law for Xi is symmetric,
which we used previously. To remove the symmetry requirement, it is crucial that the quadratic self-
normalization has both empirical and population terms. With this lemma, the same argument as in the proof
of Lemma 10, yields a self-normalized tail bound.

t=1 be a ﬁltration and let {(Zt, ζt)}T

t=1 be a stochastic process with Zt ∈ Rd and
Lemma 16. Let {Ft}T
ζt ∈ R such that (1) (Zt, ζt) is Ft measurable, (2) |ζt| ≤ M for all t ∈ [T ], (3) Zt ⊥⊥ ζt|Ft, and (4)
E[Zt|Ft] = 0. Let ˆΣ (cid:44) (cid:80)T
T |Ft]. Then for any positive deﬁnite matrix Q
we have

t and Σ (cid:44) (cid:80)T

t=1 ZtZ (cid:62)

E[ZtZ (cid:62)

t=1

P





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

Ztζt

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(Q+M 2( ˆΣ+Σ))−1

≥ 2 log

(cid:115)





1
δ

det(Q + M 2( ˆΣ + Σ))
det(Q)







 ≤ δ.

Proof. The proof is identical to Lemma 10, but uses Lemma 15 in lieu of (6).

We can now analyze the inﬂuence-adjusted estimator.

Lemma 17. Under Assumption 1 and Assumption 2 and assuming that λ ≥ 4d log(9T ) + 8 log(4T /δ), with
probability at least 1 − δ, the following holds simultaneously for all t ∈ [T ]:
√

(cid:107)ˆθt − θ(cid:107)Γt ≤

λ + (cid:112)27d log(1 + 2T /d) + 54 log(4T /δ).

Proof. Using the same argument as in the proof of Lemma 5, we get

(cid:107)ˆθt − θ(cid:107)Γt ≤

√

λ +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

Zτ ζτ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Γ−1

t

,

where Zτ (cid:44) zτ − µτ and ζτ (cid:44) (cid:104)θ, µτ (cid:105) + fτ (xτ ) + ξτ , just as before. Now we must control this error
term, for which we need both Lemma 14 and Lemma 16. Apply Lemma 14 to the vectors Zτ , setting
ˆΣt (cid:44) (cid:80)t−1
E[Zτ Zτ | Fτ ]. With probability at least 1 − δ/(2T ), we have that for
all unit vectors v ∈ Rd

τ and Σt (cid:44) (cid:80)t−1

τ =1 Zτ Z (cid:62)

τ =1

v(cid:62)Σtv ≤ 2v(cid:62) ˆΣtv + 9d log(9t) + 8 log(4T /δ) ≤ 2v(cid:62) ˆΣtv + 9d log(9T ) + 8 log(4T /δ).

This implies a lower bound on all quadratic forms involving ˆΣt, which leads to positive semideﬁnite inequality

λI + ˆΣt (cid:23) (λ − 3d log(9T ) − 8/3 log(4T /δ))I + ( ˆΣt + Σt)/3.

21

This means that for any vector v, we have

(cid:107)v(cid:107)2

(λI+ ˆΣt)−1 ≤ (cid:107)v(cid:107)2
≤ 3(cid:107)v(cid:107)2

((λ−3d log(9T )−8/3 log(4T /δ))I+( ˆΣt+Σt)/3)−1
((3λ−9d log(9T )−8 log(4T /δ))I+ ˆΣt+Σt)−1.

Before we apply Lemma 16, we must introduce the range parameter M . Fix a round t and let A (cid:44)
((3λ − 9d log(9T ) − 8 log(4T /δ))I + ˆΣt + Σt) denote the matrix in the Mahalanobis norm. Then,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

A−1

Zτ ζτ

= M 2

Zτ ζτ

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

τ =1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(M 2A)−1

Now apply Lemma 16 with Q (cid:44) M 2(3λ − 9d log(9T ) − 8 log(4T /δ))I. Since we require Q (cid:31) 0, this
requires λ > 3d log(9T ) − 8/3 log(4T /δ), which is satisﬁed under the preconditions for the lemma. Under
this assumption, we get

(cid:107)

t−1
(cid:88)

τ =1

Zτ ζτ (cid:107)2

(λI+ ˆΣt)−1 ≤ 3M 2(cid:107)

Zτ ζτ (cid:107)2

(Q+M 2( ˆΣt+Σt))−1

t−1
(cid:88)

τ =1


≤ 6M 2 log



(cid:115)

4T
δ

det(Q + M 2( ˆΣt + Σt))
det(Q)



 ,

with probability at least 1 − δ/(2T ). With a union bound, the inequality holds simultaneously for all T , with
probability at least 1 − δ.

The last step is to analyze the determinant. Using the same argument as in the proof of Lemma 9, it is not

hard to show that

(cid:32)

det(Q + M 2( ˆΣt + Σt))
det(Q)

(cid:33)1/d

≤ 1 +

2(t − 1)
d(3λ − 9d log(9T ) − 8 log(4T /δ))

.

If we impose the slightly stronger condition that λ ≥ 4d log(9T ) + 8 log(4T /δ), then the term in the
denominator is at least 1, and then we have that

(cid:107)ˆθt − θ(cid:107)Γt ≤

λ + (cid:112)6M 2 log(4T /δ) + 3dM 2 log(1 + 2T /d).

Finally, as in the two-action case, we use the fact that |ζt| ≤ 3 (cid:44) M .

Recall the setting of γ(T ) (cid:44)

λ + (cid:112)27d log(1 + 2T /d) + 54 log(4T /δ) and the deﬁnition of λ (cid:44)
4d log(9T ) + 8 log(4T /δ). For the remainder of the proof, condition on the probability 1 − δ event
that Lemma 17 holds. We now turn to analyzing the regret.

Lemma 18. Let µt (cid:44) Ea∼πtzt,a where πt is the solution to (3) and assume the conclusion of Lemma 17
holds. Then with probability at least 1 − δ

√

√

Reg(T ) ≤ (1 + 6γ(T ))(cid:112)2T log(2/δ) + 3γ(T )

tr(Γ−1

t (zt,at − µt)(zt,at − µt)(cid:62)).

(cid:118)
(cid:117)
(cid:117)
(cid:116)T

T
(cid:88)

t=1

22

This lemma is slightly more complicated than Lemma 6.

Proof. First, using the same application of Azuma’s inequality as in the proof of Lemma 6, with probability
1 − δ/2, we have

Reg(T ) ≤ (cid:112)2T log(2/δ) +

Ea∼πt[(cid:104)θ, zt,a(cid:63)

t

− zt,a(cid:105) | Ft].

T
(cid:88)

t=1

Now we work with this latter expected regret

Ea∼πt [(cid:104)θ, zt,a(cid:63)

t

− zt,a(cid:105) | Ft] =

(cid:104)θ, zt,a(cid:63)

− µt(cid:105) ≤

t

(cid:104)ˆθ, zt,a(cid:63)

t

− µt(cid:105) + γ(T )(cid:107)zt,a(cid:63)

− µt(cid:107)Γ−1

.

t

t

For the ﬁrst term, we use the ﬁltration condition (2)

(cid:104)ˆθ, zt,a(cid:63)

t

− µt(cid:105) =

πt(a)(cid:104)ˆθ, zt,a(cid:63)

t

− zt,a(cid:105) ≤ γ(T )

πt(a)(cid:107)zt,a(cid:63)

t

− zt,a(cid:107)Γ−1

t

T
(cid:88)

t=1

(cid:88)

a∈At

≤ γ(T )(cid:107)zt,a(cid:63)

t

− µt(cid:107)Γ−1

t

+ γ(T )

πt(a)(cid:107)zt,a − µt(cid:107)Γ−1

.

t

T
(cid:88)

t=1

(cid:88)

a∈At

(cid:88)

a∈At

Applying the feasibility condition in (3), we can bound the expected regret by

Ea∼πt[(cid:104)θ, zt,a(cid:63)

t

− zt,a(cid:105) | Ft] ≤ 3γ(T )

tr(Γ−1

t Cov
a∼πt

(zt,a)) ≤ 3γ(T )

tr(Γ−1

t Cov
a∼πt

(zt,a)).

T
(cid:88)

(cid:114)

t=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)T

T
(cid:88)

t=1

To complete the proof, we need to relate the covariance, which takes expectation over the random action, with
the particular realization in the algorithm, since this realization affects the term Γt+1. Let Zt (cid:44) zt,at − µt
denote the centered realization, then the covariance term is

In order to derive a bound on (cid:80)T

t=1 tr(Γ−1

t Cova∼πt(zt,a)), we ﬁrst consider the following

Cov
a∼πt

(zt,a) = E[ZtZ (cid:62)
t

| Ft]

T
(cid:88)

t=1

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) − tr(Γ−1

t ZtZ (cid:62)

t ).

Observe that sequence of sums {(cid:80)τ
| Ft]) − tr(Γ−1
each term tr(Γ−1
the Freedman’s inequality reveals that with probability at least 1 − δ/2

| Ft]) − tr(Γ−1

E[ZtZ (cid:62)
τ =1 is a martingale. Also,
t
t ) is bounded by 1 because Γ1 = λI and λ > 1. Applying

t
t ZtZ (cid:62)

t=1 tr(Γ−1

E[ZtZ (cid:62)
t

t ZtZ (cid:62)

t )}T

t

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) − tr(Γ−1

t ZtZ (cid:62)

t ) ≤ 2

E[(Z (cid:62)

t Γ−1

t Zt)2 | Ft] log(2/δ) + 2 log(2/δ)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

t=1

T
(cid:88)

t=1

≤

1
2

T
(cid:88)

t=1

23

≤ 2

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) log(2/δ) + 2 log(2/δ)

tr(Γ−1
t

E[ZtZ (cid:62)
t

| Ft]) + 4 log(2/δ).

Then rearranging and plugging back into our regret bound, we have

Reg(T ) ≤ (cid:112)2T log(2/δ) + 3γ(T )

(cid:118)
(cid:117)
(cid:117)
(cid:116)2T

(cid:32) T

(cid:88)

t=1

tr(Γ−1

t ZtZ (cid:62)

t ) + 4 log(2/δ)

(cid:33)

≤ (1 + 6γ(T ))(cid:112)2T log(2/δ) + 3γ(T )

tr(Γ−1

t ZtZ (cid:62)

t ).

(cid:118)
(cid:117)
(cid:117)
(cid:116)2T

T
(cid:88)

t=1

To conclude the proof of the theorem, apply Lemma 7, which applies on the last term on the RHS

of Lemma 18. Overall, with probability at least 1 − 2δ, we get

Reg(T ) ≤ (1 + 6γ(T ))(cid:112)2T log(2/δ) + 3γ(T )(cid:112)2T d(1 + 1/λ) log(1 + T /(dλ)).

Since λ = Θ(d log(T /δ)) and γ(T ) = O((cid:112)d log(T ) + (cid:112)log(T /δ)), we get with probability 1 − δ,

Reg(T ) ≤ O

√

(cid:16)

d

T log(T ) + (cid:112)dT log(T ) log(T /δ) + (cid:112)T log(T /δ) log(1/δ)

(cid:17)

.

References

[1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic

bandits. In Advances in Neural Information Processing Systems, 2011.

[2] Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efﬁcient algorithm

for bandit linear optimization. In Conference on Learning Theory, 2008.

[3] Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E Schapire. Taming the
monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine
Learning, 2014.

[4] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In

International Conference on Machine Learning, 2013.

[5] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed

bandit problem. SIAM Journal on Computing, 2002.

[6] Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandit
algorithms with supervised learning guarantees. In International Conference Artiﬁcial Intelligence and
Statistics, 2011.

[7] Peter J Bickel, Chris AJ Klaassen, Ya’acov Ritov, and Jon A Wellner. Efﬁcient and adaptive estimation

for semiparametric models. Springer New York, 1998.

[8] Sébastien Bubeck, Nicolo Cesa-Bianchi, and Sham M Kakade. Towards minimax policies for online

linear optimization with bandit feedback. In Conference on Learning Theory, 2012.

[9] Nicolo Cesa-Bianchi and Gábor Lugosi. Combinatorial bandits. Journal of Computer and System

Sciences, 2012.

24

[10] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duﬂo, Christian Hansen, Whit-
ney Newey, and James M. Robins. Double machine learning for treatment and causal parameters.
arXiv:1608.00060, 2016.

[11] Wei Chu, Lihong Li, Lev Reyzin, and Robert E Schapire. Contextual bandits with linear payoff functions.

In International Conference on Artiﬁcial Intelligence and Statistics, 2011.

[12] Thomas M Cover. Behavior of sequential predictors of binary sequences. In Conference on Information

Theory, Statistical Decision Functions and Random Processes, 1965.

[13] Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit

feedback. In Conference on Learning Theory, 2008.

[14] Victor H de la Peña, Tze Leung Lai, and Qi-Man Shao. Self-normalized processes: Limit theory and

statistical applications. Springer Science & Business Media, 2008.

[15] Victor H de la Peña, Michael J Klass, and Tze Leung Lai. Theory and applications of multivariate

self-normalized processes. Stochastic Processes and their Applications, 2009.

[16] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the
multi-armed bandit and reinforcement learning problems. Journal of Machine Learning Research, 2006.

[17] Sarah Filippi, Olivier Cappe, Aurélien Garivier, and Csaba Szepesvári. Parametric bandits: The

generalized linear case. In Advances in Neural Information Processing Systems, 2010.

[18] David A Freedman. On tail probabilities for martingales. The Annals of Probability, 1975.

[19] Kristjan Greenewald, Ambuj Tewari, Susan Murphy, and Predag Klasnja. Action centered contextual

bandits. In Advances in Neural Information Processing Systems, 2017.

[20] Akshay Krishnamurthy, Alekh Agarwal, and Miroslav Dudík. Contextual semibandits via supervised

learning oracles. In Advances in Neural Information Processing Systems, 2016.

[21] John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side

information. In Advances in Neural Information Processing Systems, 2008.

[22] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personal-

ized news article recommendation. In International Conference on World Wide Web, 2010.

[23] Lihong Li, Yu Lu, and Dengyong Zhou. Provable optimal algorithms for generalized linear contextual

bandits. In International Conference on Machine Learning, 2017.

[24] Jerzy Neyman. C(α) tests and their use. Sankhy¯a: The Indian Journal of Statistics, Series A, 1979.

[25] Alexander Rakhlin and Karthik Sridharan. Bistro: An efﬁcient relaxation-based method for contextual

bandits. In International Conference on Machine Learning, 2016.

[26] James M Robins and Andrea Rotnitzky. Recovery of information and adjustment for dependent

censoring using surrogate markers. In AIDS Epidemiology. Springer, 1992.

[27] James M Robins, Lingling Li, Eric Tchetgen Tchetgen, and Aad van der Vaart. Higher order inﬂuence
functions and minimax estimation of nonlinear functionals. In Probability and Statistics: Essays in
Honor of David A. Freedman. Institute of Mathematical Statistics, 2008.

25

[28] Peter M Robinson. Root-n-consistent semiparametric regression. Econometrica: Journal of the

[29] Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of

[30] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of

Econometric Society, 1988.

Operations Research, 2010.

Operations Research, 2014.

[31] Maurice Sion. On general minimax theorems. Paciﬁc Journal of Mathematics, 1958.

[32] Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dudík, John Langford, Damien
In Advances in Neural

Jose, and Imed Zitouni. Off-policy evaluation for slate recommendation.
Information Processing Systems, 2017.

[33] Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert E Schapire. Efﬁcient algorithms for adversarial

contextual learning. In International Conference on Machine Learning, 2016.

[34] Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health. In

Mobile Health. Springer, 2017.

[35] Anastasios Tsiatis. Semiparametric theory and missing data. Springer Science & Business Media, 2007.

26

