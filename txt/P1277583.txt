Attributed and Predictive Entity Embedding for Fine-Grained
Entity Typing in Knowledge Bases

Hailong Jin and Lei Hou and Juanzi Li
DCST, Tsinghua University
Beijing 100084, China

Tiansi Dong
B-IT, Bonn University
53113 Bonn, Germany

{jinhl15@mails.,houlei@mail.,lijuanzi@}tsinghua.edu.cn
dongt@bit.uni-bonn.de

Abstract

Fine-grained entity typing aims at identifying the semantic type of an entity in KB. Type in-
formation is very important in knowledge bases, but is unfortunately incomplete even in some
large knowledge bases. Limitations of existing methods are either ignoring the structure and
type information in KB or requiring large scale annotated corpus. To address these issues, we
propose an attributed and predictive entity embedding method, which can fully utilize various
kinds of information comprehensively. Extensive experiments on real DBpedia dataset show that
our proposed method signiﬁcantly outperforms 8 state-of-the-art methods, with 3.4% and 2.9%
improvement in Mi-F1 and Ma-F1, respectively.

1

Introduction

Knowledge about entities in Knowledge Base (KB) is essential for language understanding. This knowl-
edge can be attributional (e.g., canFly, hasAge), relational (e.g., marriedTo, bornIn) or type-based (e.g.,
isFood, isLocation). Type information, which clusters a group of entities with same properties, is valu-
able in KB, because it is the glue that holds our mental world together (Murphy, 2004). Types in KB
are usually organized as a hierarchical structure, namely type hierarchy. Entity typing assigns types
(e.g., Person, Athlete, BasketballPlayer) to an entity (e.g., Yao Ming) in KB, and is a fundamental task
in knowledge base construction.

Traditional entity typing focuses on a small set of types, such as Person, Location and Or-
ganization (Ratinov and Roth, 2009; Nadeau and Sekine, 2007), while ﬁne-grained entity typ-
ing assigns more speciﬁc types to an entity, which normally forms a type-path in the type hier-
archy in KB (Ren et al., 2016a). As shown in Fig. 1, Yao Ming is associated with a type-path
/Thing/Agent/Person/Athlete/BasketballPlayer. Fine-grained types (e.g., Athlete and BasketballPlayer)
are more informative than coarse-grained types (e.g., Person), because they provide more speciﬁc se-
mantic information about the entity (Xu et al., 2016). Characterizing an entity with ﬁne-grained types
(type-paths) beneﬁts many real applications, such as knowledge base completion (Dong et al., 2014),
entity linking (Ling et al., 2015; Durrett and Klein, 2014), relation extraction (Liu et al., 2014), and
question answering (Yahya et al., 2014).

Many researches have been carried out on ﬁne-grained entity typing in text, which detects local types
for an entity mention in a particular plain text from predeﬁned ﬁne-grained entity types (Ling and Weld,
2012; Yogatama et al., 2015; Ren et al., 2016a; Shimaoka et al., 2017; Xin et al., 2018). However, only
a few works aim at ﬁne-grained entity typing in knowledge base, that is, inferring missing types for an
entity in KB, which is the focus of this paper. Most KBs are incomplete and lack of type information, in
part because the world is always changing. For example, 36.53% entities in DBpedia do not have type
information. Therefore, KBs are aways in need of updating and completing. To this end, there are two
main methods in the literature as follow: One method exploits various kinds of information to construct
the feature representation of an entity, such as entity textual description, property and category (Nee-
lakantan and Chang, 2015; Xu et al., 2016). After that, a predict function P (t|e) is learned to infer

This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/
Corresponding author: Lei Hou (houlei@mail.tsinghua.edu.cn)

Figure 1: An example of ﬁne-grained entity typing in KB. The left part is an entity with various infor-
mation. The red ones are assigned types, which forms a type-path.

whether entity e is an instance of type t. This method ignores rich structural information in KB (i.e., link
relation between entities). It is a promising and challenging problem to utilize structural information for
type inference. The second method utilizes annotated corpus to learn entity low-dimensional represen-
tations then makes type inference based on the learned embeddings (Yaghoobzadeh and Sch¨utze, 2015;
Yaghoobzadeh and Sch¨utze, 2017). Apparently, it requires large scale corpus in which entity mentions
are annotated. But large scale highly-qualiﬁed annotated corpus is often difﬁcult to obtain.

To address the above issues, we propose a novel attributed and predictive entity embedding method,
which can be described as follow: We ﬁrst construct a partially-labeled attributed entity network, which
contains structural, attribute and type information for entities. Here “partially-labeled” means some
entities have been labeled with type information, while others not. Then, we employ a deep neural
network to integrate the structural and attribute information for entity representation learning. For an
unlabeled entity, we jointly utilize its structural and attribute information for neighbor prediction. For
a labeled entity, besides neighbors, we also predict its types and type hierarchy via a margin-based loss
function. Finally, a multi-label classiﬁcation model is used for type inference based on the learned entity
embeddings.

The main contributions of this paper can be summarized as follows:

• We encode various kinds of information into a partially-labeled entity network and propose a deep
neural network model to effectively integrate the structural and attribute information for entity rep-
resentation learning.

• We utilize type information of labeled entities to equip the learned embeddings with strong pre-
dictive power in entity typing task, and further incorporate the structure of type hierarchy via a
margin-based loss function to meet the ﬁne-grained demand.

• We construct a dataset from DBpedia. Extensive experiments show that our proposed method sig-
niﬁcantly outperforms 8 state-of-the-art methods, with 3.4% and 2.9% improvement in Mi-F1 and
Ma-F1, respectively.

The rest of this paper is organized as follows. In Section 2, we formally deﬁne the problem of ﬁne-
grained entity typing in knowledge bases. Section 3 demonstrates the proposed approach in detail. The
dataset description and evaluation are presented in Section 4. Section 5 outlines some related works.
Section 6 concludes our work.

2 Problem Formulation

In this section, we formally deﬁne the problem of Fine-Grained Entity Typing in KB.

Deﬁnition 1 Knowledge Base is deﬁned a tuple of entities E and a type hierarchy H, formally, KB =
(cid:104)E, H(cid:105), where H is used to organize entities in E. Each entity e ∈ E denotes a speciﬁc thing in the real
world.

Deﬁnition 2 Type Hierarchy H is deﬁned as a tree or a directed acyclic graph (DAG). It provides a
natural way to categorize and organize entities in large KB. It is formalized (T , R), where T is the type
set and R = {(ti, tj)|ti, tj ∈ T ∧ i (cid:54)= j} is the relation set, in which (ti, tj) means that ti is the subtype
of tj.

We distinguish three kinds of entity knowledge which are beneﬁcial to ﬁne-grained entity typing task.

• Entity Link Relation: KB contains a large number of link relations between entities. It is natural to
convert entities in E into a network structure via these link relations. If an entity ei links to another
entity ej in KB, there will be a directed edge from entity ei to entity ej. These link relations reﬂect
the overall structure of KB and provide important semantic relatedness between entities.

• Entity Attribute: Besides the above link structure, each entity e ∈ E also has textual description,
properties and categories (shown in Fig. 1). We term all such auxiliary information of an entity as
attributes. The attributes of an entity provide valuable clues to predict its types.

• Entity Type: Part of entities in E have been assigned with types, we refer them as E l. The assigned
types for an entity form a type-path in type hierarchy H, which starts from the root and ends at one
speciﬁc type (not necessary to end at the leaf type). Those entities in E without type information
are referred as E u. The type information observed in E l can be a useful signal to infer missing type
information in E u.

From the above deﬁnitions, we achieve a partially-labeled attributed entity network, including link

structure, entity attributes, and type information. We formally deﬁne this entity network as follow:
Deﬁnition 3 Entity Network: Let G = (E, L) be an entity network. E = E l ∪ E u is the entity set, where
|E| = N and |E l| = N l. L = {(ei, ej) |ei, ej ∈ E and i (cid:54)= j} is the link relation set.

For the convenience of formula deduction, we redeﬁne this entity network in matrix form, G =
(A, X, Y). Adjacency matrix A ∈ RN ×N is equivalent to link set L, Aij = 1 if there is a link from
ei to ej, otherwise 0. Each row in A refers to an entity’s link vector Ai. Attribute matrix X ∈ RN ×M
collects attributes for all entities, where M is the number of attributes. Each row in X refers to an entity’s
attribute vector Xi. X can either be binary or take any real value. Type matrix Y ∈ RN l×K collects type
information for entities in E l, where K is the number of types. Each row in Y refers to the type vector
Yi of a labeled entity in E l. Yij = 1 if ei has been assigned with type tj, otherwise 0.

Based on the terminologies described above, we formally deﬁne the problem of ﬁne-grained entity

typing in KB as follows:

Deﬁnition 4 Fine-grained entity typing in KB: Given a partially-labeled attributed entity network G =
(E, L), we aim at learning a type predictor from E l, which comprehensively takes link structure, entity
attributes and type information into account. Then we utilize the learned type predictor to predict the
type of e ∈ E u, i.e., infer whether entity e ∈ E u is an instance of type t ∈ T .

3 The Proposed Approach

3.1 General Architecture

A general state-of-the-art architecture for ﬁne-grained entity typing consists of two components.

• Entity Representation: It learns distributed representation (cid:126)v (e) for each entity e ∈ E.

• Type Predictor: It learns a predict function P (t|e) using E l as training examples. P (t|e) denotes
the probability that entity e belongs to type t, and it is often used to infer types of those unlabeled
entities in E u.

Type predictor is regarded as a multi-label classiﬁcation model. P (t|e) is calculated through a two-

layer Multi-Layer Perceptron (MLP).

[P (t1|e) · · · P (tK|e)] = σ (Woutf (Win(cid:126)v (e)))

(1)

Each entry of output layer indicates the predicted probability of type ti for a given entity e. Win and
f is the nonlinear activation function,
Wout are MLP parameter matrices. σ is the sigmoid function.
such as ReLU and tanh.

The loss L for a prediction y = [P (t1|e) · · · P (tK|e)] when the true labels are encoded in a binary

vector y∗ ∈ {0, 1}1×K is the following cross entropy loss function:

L (y, y∗) =

−y∗(i) log y(i) −

(cid:16)

1 − y∗(i)(cid:17)

log

(cid:16)

1 − y(i)(cid:17)

(2)

K
(cid:88)

i=1

The key difﬁculty in computing P (t|e) is to learn a good representation (cid:126)v (e) for entity e. Several
models have achieved the state-of-the-art performance. However, these models still face the following
non-trivial challenges:

• Entity attributes and link structure are typically regraded as separated features without considering
their correlations. Recent research shows the importance of integrating link structure and entity
attributes for learning more informative representations (Liao et al., 2017; Li et al., 2017). Could
we seamlessly integrate entity attributes and link structure into a uniﬁed entity network? Since the
link structure and entity attributes offer different sources of information, how shall we capture the
complex non-linear relationship between them?

• The type information of the labeled entities in E l is only used in the learning phase of the type pre-
dictor, but totally ignored in the representation learning phase. Recent research shows incorporating
labeled information in representation learning could enhance the predictive power of the learning
embeddings for speciﬁc tasks (Tang et al., 2015a; Yang et al., 2016). In our task, the entity types
and their formed type hierarchy H are valuable labeled information, could we design a predictive
model to make full use of the type information in E l and the structure of type hierarchy?

To address the above challenges, we propose a novel attributed and predictive entity embedding
method to learn entity representations.
It combines the advantages of attributed network embedding
method and semi-supervised learning. We integrate entity attributes and link structure via a deep neural
network model (Section 3.2). For an entity ei in E l, we jointly predict its type and its neighbor entities
in the entity network (Section 3.3). In this way, we can fully utilize the type information in E l.

3.2 Attributed Entity Embedding

We aim at learning low-dimensional vector representations for each entity in the entity network, such
that original link structure and entity attribute proximity can be preserved in vectors. The most intuitive
idea is to directly concatenate the embeddings learned from different parts. However, this way can
not capture the complex non-linear relationship between them, because each part is trained separately
without interaction. To address this issue, we present a neural network model to integrate link structure
(structural information) and entity attributes (attribute information). For an entity ei ∈ E, we jointly
utilize its link vector Ai and attribute vector Xi to predict the link probability between ei and other
entities (shown in Fig. 2). Our model integrates structure and attribute information on the input layer, so
that these two parts closely interact with each other and all parameters can be optimized simultaneously.
In our design, each entity e has two vector representations, self representation and context representa-
tion. e is self representation of e when it is treated as an entity in entity network G. While ˜e is context
representation of e when it is treated as a speciﬁc context. To comprehensively represent an entity e, we
add e and ˜e as the ﬁnal representation (cid:126)v (e).

Input and Embeddings. For an entity ei ∈ E, Ai and Xi denote its link vector and attribute vector
as deﬁned in Section 2. Our model takes both structural information (Ai) and attribute information (Xi)
as input, and utilizes two embedding weight matrices to achieve the ﬁnal input embeddings. Wstru
projects the link vector Ai to a dense vector ustru which captures structural information. Wattr encodes
the attribute vector Xi and generates a compact vector uattr which aggregates attribute information. We
concatenate ustru and uattr, then feed this concatenated vector into a Multi-Layer Perceptron.

Information Fusion. We employ a Multi-Layer Perceptron to integrate structural and attribute infor-
mation. Recent research shows stacked multiple non-linear layers can help learn better representations

Figure 2: Attributed entity embedding framework

of data (Liao et al., 2017). Each layer is calculated as follows:

h(1) = f

h(k) = f

(cid:16)

(cid:16)

W(1) [ustru, λuattr] + b(1)(cid:17)
W(k)h(k−1) + b(k)(cid:17)

, k = 1, 2, · · · , n

λ is the trade-off factor for structure and attribute information. f denotes the activation function. n is
the number of hidden layers. From the last hidden layer, we obtain an informative representation h(n),
which is regarded as the self representation ei of the input entity ei.

Neighbor Prediction. Finally, we predict neighbors for an input entity ei (if there is a link from
ei to ej, ej is ei’s neighbor). We call it neighbor prediction, which is widely used to model network
structure (Tang et al., 2015a; Liao et al., 2017). Speciﬁcally, we utilize the self representation ei to
predict the link probability between entity ei and other entities. The predictive link probability is deﬁned
through a softmax layer as follows:

p (ej|ei) =

exp (˜ej · ei)
j(cid:48)=1 exp (cid:0)˜ej(cid:48) · ei

(cid:1)

(cid:80)N

˜ej denotes the context representation of entity ej. Let yi denotes the predicted link probability vector for
entity ei, y(j)
is the j-th entry of the vector yi. Ai is ei’s link vector (in Section 2), Aij indicates whether
there is a link from ei to ej. The loss function Lattr is deﬁned as cross entropy over all entities in E

i

Lattr =

N
(cid:88)

N
(cid:88)

i=1

j=1

−Aij log y(j)

i − (1 − Aij) log

1 − y(j)

i

(cid:16)

(cid:17)

3.3 Predictive Entity Embedding

In this section, we propose predictive entity embedding learning that makes full use of the type infor-
mation in E l, which is often ignored by existing methods in representation learning phase. Inspired by
(Tang et al., 2015a; Yang et al., 2016), we ﬁrst introduce a type prediction loss to equip learned embed-
dings with strong predictive power for entity typing task (Section 3.3.1). Then, to meet the ﬁne-grained
demand, we model the tree-structured type hierarchy in a margin-based framework (Section 3.3.2).

3.3.1 Type Prediction
We fully utilize type information in E l to make the learned embeddings more predictive. For each entity
ei ∈ E l, we not only predict its neighbors in G (Section 3.2), but also predict its types. We add another
softmax layer, the probability of entity ei being an instance of type tj is deﬁned as follows:

p (tj|ei) =

exp (tj · ei)
j(cid:48)=1 exp (cid:0)tj(cid:48) · ei

(cid:1)

(cid:80)K

(3)

(4)

(5)

(6)

(7)

tj denotes the representation of type tj. Let yi denotes the predicted type probability vector for entity
ei, y(j)
is the j-th entry of the vector yi. Yi is ei’s type vector (in Section 2), Yij indicates whether ei is
i
an instance of tj. The loss function Ltype is deﬁned as cross entropy over all entities in E l:

Ltype =

N l
(cid:88)

K
(cid:88)

i=1

j=1

−Yij log y(j)

i − (1 − Yij) log

1 − y(j)

i

(cid:16)

(cid:17)

(8)

3.3.2 Type Hierarchy Modeling
Fine-grained entity typing requires us to pay more attention to speciﬁc types (ﬁne-grained types) than
to general types (coarse-grained types), because speciﬁc types are more informative. For example, Yao
Ming belongs to BasketballPlayer and Athlete, and it should get higher score on BasketballPlayer than
Athlete, because the former characterizes Yao Ming more accurately. Besides, Yao Ming should get higher
score on the correct type BasketballPlayer than its wrong sibling type SoccerPlayer although both types
belong to Athlete. If different types are treated equally, the structural information between types will be
lost. Therefore, we use type order to model different relatedness between an entity and its related types,
which reﬂects the structural information of type hierarchy to a certain degree. In particular, we deﬁne
two kinds of type order as follow:

Deﬁnition 5 Ancestor order: In the type-path of an entity, speciﬁc types should get higher score than its
ancestor. For example, Yao Ming should get higher score on BasketballPlayer than Athlete.

Ancestor order reﬂects the relationship between a type and its ancestor types. For each triple (e, t, ta),
where t is one of the type for entity e, and ta is t’s ancestor in type hierarchy. We deﬁne adaptive margin
γtta = ξ × l(ta,t)
l(root,t) , where l(ta, t) denotes the number of steps going down from type ta to type t. The
hinge loss is deﬁned as:

max (0, s (e, ta) − s (e, t) + γtta)

Lancestor = E(e,t,ta) [max (0, s (e, ta) − s (e, t) + γtta)]

where s (e, t) is the score function, deﬁned as the inner product of e and t.

Deﬁnition 6 Sibling order: For an entity, the correct type should get higher score than its wrong sib-
ling type in type hierarchy. For example, Yao Ming should get higher score on BasketballPlayer than
SoccerPlayer.

Sibling order reﬂects the relationship between a type and its sibling types. For each triple (e, t, ts),
where t is one of the true type for entity e, and ts is its wrong sibling type of entity e. We use the ﬁxed
margin ξ. The hinge loss is deﬁned as:

max (0, s (e, ts) − s (e, t) + ξ)

Lsibling = E(e,t,ts) [max (0, s (e, ts) − s (e, t) + ξ)]

We achieve the loss function Lhier for type hierarchy modeling: Lhier = Lancestor + Lslibing.

Objective and Training. Finally, we sum these three loss function to get the ﬁnal objective function of
entity representation component.

Loss = Lattr + λ1 · Ltype + λ2 · Lhier

λ1 and λ2 are weight parameters. We apply negative sampling (Mikolov et al., 2013; Tang et al., 2015b),
where only a very small subset of entities are sampled from E. Parameters are optimized through Adap-
tive Moment Estimation (Adam). In the embedding layer and each hidden layer, we also add dropout
component to alleviate over ﬁtting.

(9)

(10)

(11)

(12)

(13)

4 Experiments and Analysis

In this section, we evaluate the proposed method using real world dataset collected from DBpedia. We
will introduce the dataset and experiment settings in Section 4.1, present the comparison results in Sec-
tion 4.2 and investigate some method details in Section 4.3. Our source code is available1 for reference.

4.1 Datasets and Experimental Setup

Datasets: To the best of our knowledge, there is no public large-scale dataset available for ﬁne-grained
entity typing in KB. We construct a dataset from DBpedia. The type hierarchy in DBpedia is of tree
structure. We extract link relation and attribute information from DBpedia2. For each entity, we use the
single type-path assigned in DBpedia as the ground truth.

For this dataset, we remove those entities which are only labeled as Thing. Because they do not provide
any semantic information. In representation learning phase, we utilize the whole dataset with only 50%
entities are labeled with types. In type predictor phase, we follow a 50/30/20 ratio to split the dataset into
training, dev and test sets. Table 1 shows the detailed statistics. Each entity has 3.27 types on average.

Table 1: Statistics of the dataset

Type

Entity

Link

214

300,000

5,243,230

Attribute

word
5,350

property
200

category
350

Metrics: To evaluate the performance of our proposed method, we use Accuracy (Strict-F1), Micro-
averaged F1 (Mi-F1) and Macro-averaged F1 (Ma-F1), which have been used in many ﬁne-grained
typing systems (Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh and Sch¨utze, 2017; Ren et al., 2016a).
Parameter Settings: Our implementation of APE is based on TensorFlow3. Regarding the choice of
activation function of hidden layers, we have tried ReLU, softsign and tanh, ﬁnding softsign leads to the
best performance in general. We randomly initialize model parameters with a Gaussian distribution (with
a mean of 0.0 and standard deviation of 0.01), optimizing the model with mini-batch Adam (Kingma
and Ba, 2014). We set ﬁxed margin ξ = 1 in our experiments. Both λ1 and λ2 are chosen among
{0.2, 0.4, 0.6, 0.8}, we found λ1 = 0.8 and λ2 = 0.4 achieve best performance.
Baseline: We denote our model as APE, and compare it with three sets of methods:

• Entity typing methods: We compare APE with ﬁve state-of-the-art entity typing methods. FIG-
MENT uses entity representation learned from annotated corpus for type inference (Yaghoobzadeh
and Sch¨utze, 2015). CUTE employs a multi-label hierarchical classiﬁcation method to address en-
tity typing task (Xu et al., 2016). MuLR uses multi-level representations of entities (character, word
and entity) to make type inference in KB (Yaghoobzadeh and Sch¨utze, 2017). Global utilizes en-
tity text description to infer missing entity type instances (Neelakantan and Chang, 2015). Corpus
designs global and context model to make type inference jointly (Yaghoobzadeh et al., 2017).

• Network Embedding methods: Attributed and predictive network embedding methods can learn
informative entity representations, so we compare APE with them. PTE learns predictive text em-
beddings from heterogeneous network (Tang et al., 2015a). Planetoid is a semi-supervised network
embedding method which makes neighbor and label prediction jointly (Yang et al., 2016). ASNE
integrates structural and attribute information via a deep neural architecture (Liao et al., 2017).

• APE’s variants: If we ignore the type information, that is, utilize the embeddings learned from
section 3.2 to make type inference, the method is denoted as APEno type. Similarly, if we omit the
type hierarchy modeling in Section 3.3.2, the method is denoted as APEno hierarchy.

4.2 Overall Comparison Results

Table 2 shows the overall performance and we have the following conclusions:

1https://github.com/Tsinghua-PhD/APE
2http://wiki.dbpedia.org/downloads-2016-10
3https://www.tensorflow.org/

Table 2: Typing performance on DBpedia dataset

Type

Entity Typing

Network Embedding ASNE

Our Variants

Algorithm
FIGMENT
CUTE
MuLR
Corpus
Global
Planetoid

PTE
APEno type
APEno hierarchy
APE

Acc Ma-F1 Mi-F1
0.628
0.619
0.463
0.677
0.673
0.515
0.662
0.654
0.501
0.659
0.662
0.488
0.615
0.608
0.457
0.590
0.585
0.434
0.573
0.568
0.417
0.518
0.512
0.352
0.657
0.649
0.492
0.689
0.694
0.531
0.711
0.702
0.545

Comparison with Entity Typing methods. Our model outperforms the state-of-the-art entity typing
methods, and achieves 3.4% and 2.9% improvement in Mi-F1 and Ma-F1 respectively, because it em-
ploys multiple kinds of information for type inference. Each part can provide complementary information
about entities for type inference. Our model leverages label information (type information) in represen-
tation learning phase, equip the learned embeddings with strong predictive power in entity typing task.
Comparison with Network Embedding methods. Our model outperforms the state-of-the-art network
embedding methods (i.e., Planetoid, ASNE and PTE), and achieves 12.1% and 11.7% improvement in
Mi-F1 and Ma-F1 respectively. We model the structure of type hierarchy in a margin-based loss function.
Speciﬁcally, ancestor order makes entities closer to speciﬁc types than general types, and sibling order
makes the learned entity embeddings have strong discriminative power between sibling types.
Comparison with Variants. APE consistently outperforms APEno type and APEno hierarchy, and
the simplest version APEno type could achieve comparable results with state-of-the-art methods. The
results verify that each of these information sources contributes complementary information for ﬁne-
grained typing of entities. Structural information (link relation) provides important semantic relatedness
between entities. Attribute information (text, property and category) provides additional clues for type
inference. Type information (type hierarchy) makes our model more powerful in classiﬁcation task.

4.3 Result Analysis

4.3.1

Impact of Features

We investigate the effect of information sources used to learn entity embeddings. In Table 3, Stru, T,
P and C is short for structural information, text description, property and category, respectively. From
Table 3, we ﬁnd different information sources play different roles in this task. Structural information
is most common in KB, it makes related entities have similar representations. It models the relation
between entities, while neglecting entities’s self-information. Attribute information (i.e., text description,
property and category) can be viewed as entity features, which is often informative for type inference.
We compare our model APE, APEno type and APEno hierarchy with different kinds of information
combination. For attribute information, category information is more import than property information
and text descriptions.

4.3.2 Effects of Labeled Data

We investigate the effect of labeled data proportion. Intuitively, our model can learn better embeddings
and achieve better performance if we have more labeled entities. The results in Fig. 3 show that three
metrics strikingly increase with more labeled data added, and gradually become stable when the propor-
tion is over 0.5. It proves that our model can achieve a satisfactory result with not too much labeled data,
and this advantage beneﬁts from the entity link relations and network embedding.

Table 3: Performance on different information source combination.

Information Type

Stru+T
Stru+P
Stru+C
Stru+P+C
Stru+T+P
Stru+T+C
Stru+T+P+C

APE
Acc Ma-F1 Mi-F1
0.631
0.623
0.467
0.637
0.633
0.475
0.646
0.641
0.481
0.649
0.642
0.479
0.655
0.647
0.484
0.653
0.643
0.481
0.657
0.649
0.492

APEno type
Acc Ma-F1 Mi-F1
0.634
0.625
0.463
0.642
0.638
0.480
0.647
0.649
0.487
0.679
0.685
0.520
0.659
0.658
0.507
0.681
0.683
0.518
0.689
0.694
0.531

APEno hierarchy
Acc Ma-F1 Mi-F1
0.633
0.632
0.472
0.653
0.651
0.489
0.647
0.650
0.486
0.669
0.677
0.523
0.660
0.657
0.510
0.673
0.678
0.525
0.711
0.702
0.545

Table 4: Type macro average F1 on test for all, fre-
quent and infrequent types, #training/test means the
average entity number for each type.

Type

infrequent

frequent

all

# training

# test

FIGMENT

MuLR

Corpus

CUTE

APE

1013

417

0.327

0.374

0.447

0.418

0.461

13,513

5,217

0.620

0.718

0.783

0.730

0.792

8,673

3,462

0.557

0.636

0.675

0.652

0.681

Figure 3: Performance on different labeled data
proportion.

4.3.3 Results of Frequent/Infrequent Types

We use type macro average F1 proposed by (Yosef et al., 2012) to evaluate the performance on frequent
types and infrequent types. Note that it is different from Ma-F1 reported in Table 2. The results for
infrequent types (occur less than 2,000 times) and frequent types (occur more than 10,000 times) are
shown in Table 4. Generally, the performance on infrequent types is worse than frequent ones. Our
model consistently outperforms the other methods on infrequent types, which demonstrates its ability on
dealing with rare types.

5 Related Work

Fine-grained entity typing in KB is an important sub-task of knowledge base completion. One way to
address this problem is making type inference based on entity’s attribute information in KB, such as
textual description, property, category and so on. (Neelakantan and Chang, 2015) infer missing types for
entities in KB based on text descriptions. (Xu et al., 2016) employ a multi-label hierarchical classiﬁca-
tion method to assign Chinese entities with DBpedia types based on property and category information.
The other way relies on large-scale annotated corpus, then extracts type information from the annotated
corpus for entities in KB. (Yaghoobzadeh and Sch¨utze, 2015) ﬁrst propose FIGMENT to address this
problem. They only used contextual information to assign types for entities in KB. After that, they
present FIGMENT-Multi to learn multi-level representations of entities on three complementary levels
(character, word and entity). FIGMENT-Multi predicts whether an entity is a member of a type based
on the learned embeddings (Yaghoobzadeh and Sch¨utze, 2017). Finally, they propose an embedding
based method which combines a global model with a context model. A global model that scores based
on aggregated context information and a context model that aggregates the scores of individual con-
texts (Yaghoobzadeh et al., 2017).

Fine-grained entity typing in text is related to our task. Different from the works mentioned above, it

seeks to detect local types of an entity mention inside an individual sentence, and the same entity could
have different types in different sentences (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014;
Dong et al., 2015; Del Corro et al., 2015; Yogatama et al., 2015; Ren et al., 2016b; Ren et al., 2016a;
Shimaoka et al., 2017; Rabinovich and Dan, 2017).

6 Conclusion

In this paper, we propose an attributed and predictive entity embedding method to address the task of
ﬁne-grained entity typing in KB. It combines the advantages of attributed network embedding method
and semi-supervised learning. Speciﬁcally, it employs a deep neural network architecture to integrate
structure and attribute information of an entity. We jointly predict a labeled entity’s neighbors and types,
and model the structure of type hierarchy in a margin-based way. Experiments on real world datasets
demonstrate the effectiveness of the proposed model.

The work is
supported by National Key Research and Development Program of China
(2017YFB1002101), NSFC key project (U1736204), Ministry of Education and China Mobile Research
Fund (No. 20181770250), and THUNUS NExT Co-Lab.

Acknowledgements

References

Luciano Del Corro, Abdalghani Abujabal, Rainer Gemulla, and Gerhard Weikum. 2015. Finet: Context-aware
ﬁne-grained named entity typing. In Conference on Empirical Methods in Natural Language Processing, pages
868–878.

Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua
Sun, and Wei Zhang. 2014. Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In
Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,
pages 601–610. ACM.

Li Dong, Furu Wei, Hong Sun, Ming Zhou, and Ke Xu. 2015. A hybrid neural model for type classiﬁcation of

entity mentions. In International Conference on Artiﬁcial Intelligence, pages 1243–1249.

Greg Durrett and Dan Klein. 2014. A joint model for entity analysis : Coreference , typing , and linking. TACL,

pages 477–490.

arXiv:1412.6980.

Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse Kirchner, and David Huynh. 2014. Context-dependent ﬁne-

grained entity type tagging. arXiv:1412.1820.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint

Hang Li, Haozheng Wang, Zhenglu Yang, and Masato Odagaki. 2017. Variation autoencoder based network

representation learning for classiﬁcation. In ACL 2017, Student Research Workshop, pages 56–61.

Lizi Liao, Xiangnan He, Hanwang Zhang, and Tat Seng Chua. 2017. Attributed social network embedding.

arXiv:1705.04969.

Xiao Ling and Daniel S Weld. 2012. Fine-grained entity recognition. In AAAI, pages 94–100.

Xiao Ling, Sameer Singh, and Daniel S Weld. 2015. Design challenges for entity linking. TACL, pages 315–328.

Yang Liu, Kang Liu, Liheng Xu, and Jun Zhao. 2014. Exploring ﬁne-grained entity type constraints for distantly

supervised relation extraction. In COLING, pages 2107–2116.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of
words and phrases and their compositionality. In International Conference on Neural Information Processing
Systems, pages 3111–3119.

Gregory Murphy. 2004. The big book of concepts. MIT press.

David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classiﬁcation. Lingvisticae

Investigationes, 30(1):3–26.

Arvind Neelakantan and Ming Wei Chang. 2015.

Inferring missing entity type instances for knowledge base

completion: New dataset and methods. Computer Science, pages 35–40.

Maxim Rabinovich and Klein Dan. 2017. Fine-grained entity typing with high-multiplicity assignments.

In

Meeting of the Association for Computational Linguistics, pages 330–334.

Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceed-
ings of the Thirteenth Conference on Computational Natural Language Learning, pages 147–155. Association
for Computational Linguistics.

Xiang Ren, Wenqi He, Meng Qu, Lifu Huang, Heng Ji, and Jiawei Han. 2016a. Afet: Automatic ﬁne-grained
entity typing by hierarchical partial-label embedding. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1369–1378.

Xiang Ren, Wenqi He, Meng Qu, Clare R Voss, Heng Ji, and Jiawei Han. 2016b. Label noise reduction in entity
In Proceedings of the 22nd ACM SIGKDD International

typing by heterogeneous partial-label embedding.
Conference on Knowledge Discovery and Data Mining, pages 1825–1834.

Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and Sebastian Riedel. 2017. Neural architectures for ﬁne-

grained entity type classiﬁcation. In EACL, pages 1271–1280.

Jian Tang, Meng Qu, and Qiaozhu Mei. 2015a. Pte: Predictive text embedding through large-scale heterogeneous
text networks. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 1165–1174. ACM.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015b. Line: Large-scale infor-
mation network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages
1067–1077. ACM.

Ji Xin, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2018.

Improving neural ﬁne-grained entity typing with

knowledge attention. In AAAI.

Bo Xu, Yi Zhang, Jiaqing Liang, Yanghua Xiao, Seung-won Hwang, and Wei Wang. 2016. Cross-lingual type

inference. In International Conference on Database Systems for Advanced Applications, pages 447–462.

Yadollah Yaghoobzadeh and Hinrich Sch¨utze. 2015. Corpus-level ﬁne-grained entity typing using contextual
information. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,
pages 715–725.

Yadollah Yaghoobzadeh and Hinrich Sch¨utze. 2017. Multi-level representations for ﬁne-grained typing of knowl-

edge base entities. arXiv:1701.02025.

Yadollah Yaghoobzadeh, Heike Adel, and Hinrich Sch¨utze. 2017. Corpus-level ﬁne-grained entity typing.

arXiv:1708.02275.

Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, and Gerhard Weikum. 2014. Robust question answering

over the web of linked data. In CIKM, pages 1107–1116.

Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. 2016. Revisiting semi-supervised learning with graph

embeddings. In ICML, pages 40–48.

Dani Yogatama, Dan Gillick, and Nevena Lazic. 2015. Embedding methods for ﬁne grained entity type classiﬁca-
tion. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language
Processing, ACL, pages 26–31.

M Amir Yosef, Sandro Bauer, Johannes Hoffart, Marc Spaniol, and Gerhard Weikum. 2012. Hyena: Hierarchical
In Proceedings of the 24th International Conference on Computational

type classiﬁcation for entity names.
Linguistics, pages 1361–1370.

Attributed and Predictive Entity Embedding for Fine-Grained
Entity Typing in Knowledge Bases

Hailong Jin and Lei Hou and Juanzi Li
DCST, Tsinghua University
Beijing 100084, China

Tiansi Dong
B-IT, Bonn University
53113 Bonn, Germany

{jinhl15@mails.,houlei@mail.,lijuanzi@}tsinghua.edu.cn
dongt@bit.uni-bonn.de

Abstract

Fine-grained entity typing aims at identifying the semantic type of an entity in KB. Type in-
formation is very important in knowledge bases, but is unfortunately incomplete even in some
large knowledge bases. Limitations of existing methods are either ignoring the structure and
type information in KB or requiring large scale annotated corpus. To address these issues, we
propose an attributed and predictive entity embedding method, which can fully utilize various
kinds of information comprehensively. Extensive experiments on real DBpedia dataset show that
our proposed method signiﬁcantly outperforms 8 state-of-the-art methods, with 3.4% and 2.9%
improvement in Mi-F1 and Ma-F1, respectively.

1

Introduction

Knowledge about entities in Knowledge Base (KB) is essential for language understanding. This knowl-
edge can be attributional (e.g., canFly, hasAge), relational (e.g., marriedTo, bornIn) or type-based (e.g.,
isFood, isLocation). Type information, which clusters a group of entities with same properties, is valu-
able in KB, because it is the glue that holds our mental world together (Murphy, 2004). Types in KB
are usually organized as a hierarchical structure, namely type hierarchy. Entity typing assigns types
(e.g., Person, Athlete, BasketballPlayer) to an entity (e.g., Yao Ming) in KB, and is a fundamental task
in knowledge base construction.

Traditional entity typing focuses on a small set of types, such as Person, Location and Or-
ganization (Ratinov and Roth, 2009; Nadeau and Sekine, 2007), while ﬁne-grained entity typ-
ing assigns more speciﬁc types to an entity, which normally forms a type-path in the type hier-
archy in KB (Ren et al., 2016a). As shown in Fig. 1, Yao Ming is associated with a type-path
/Thing/Agent/Person/Athlete/BasketballPlayer. Fine-grained types (e.g., Athlete and BasketballPlayer)
are more informative than coarse-grained types (e.g., Person), because they provide more speciﬁc se-
mantic information about the entity (Xu et al., 2016). Characterizing an entity with ﬁne-grained types
(type-paths) beneﬁts many real applications, such as knowledge base completion (Dong et al., 2014),
entity linking (Ling et al., 2015; Durrett and Klein, 2014), relation extraction (Liu et al., 2014), and
question answering (Yahya et al., 2014).

Many researches have been carried out on ﬁne-grained entity typing in text, which detects local types
for an entity mention in a particular plain text from predeﬁned ﬁne-grained entity types (Ling and Weld,
2012; Yogatama et al., 2015; Ren et al., 2016a; Shimaoka et al., 2017; Xin et al., 2018). However, only
a few works aim at ﬁne-grained entity typing in knowledge base, that is, inferring missing types for an
entity in KB, which is the focus of this paper. Most KBs are incomplete and lack of type information, in
part because the world is always changing. For example, 36.53% entities in DBpedia do not have type
information. Therefore, KBs are aways in need of updating and completing. To this end, there are two
main methods in the literature as follow: One method exploits various kinds of information to construct
the feature representation of an entity, such as entity textual description, property and category (Nee-
lakantan and Chang, 2015; Xu et al., 2016). After that, a predict function P (t|e) is learned to infer

This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/
Corresponding author: Lei Hou (houlei@mail.tsinghua.edu.cn)

Figure 1: An example of ﬁne-grained entity typing in KB. The left part is an entity with various infor-
mation. The red ones are assigned types, which forms a type-path.

whether entity e is an instance of type t. This method ignores rich structural information in KB (i.e., link
relation between entities). It is a promising and challenging problem to utilize structural information for
type inference. The second method utilizes annotated corpus to learn entity low-dimensional represen-
tations then makes type inference based on the learned embeddings (Yaghoobzadeh and Sch¨utze, 2015;
Yaghoobzadeh and Sch¨utze, 2017). Apparently, it requires large scale corpus in which entity mentions
are annotated. But large scale highly-qualiﬁed annotated corpus is often difﬁcult to obtain.

To address the above issues, we propose a novel attributed and predictive entity embedding method,
which can be described as follow: We ﬁrst construct a partially-labeled attributed entity network, which
contains structural, attribute and type information for entities. Here “partially-labeled” means some
entities have been labeled with type information, while others not. Then, we employ a deep neural
network to integrate the structural and attribute information for entity representation learning. For an
unlabeled entity, we jointly utilize its structural and attribute information for neighbor prediction. For
a labeled entity, besides neighbors, we also predict its types and type hierarchy via a margin-based loss
function. Finally, a multi-label classiﬁcation model is used for type inference based on the learned entity
embeddings.

The main contributions of this paper can be summarized as follows:

• We encode various kinds of information into a partially-labeled entity network and propose a deep
neural network model to effectively integrate the structural and attribute information for entity rep-
resentation learning.

• We utilize type information of labeled entities to equip the learned embeddings with strong pre-
dictive power in entity typing task, and further incorporate the structure of type hierarchy via a
margin-based loss function to meet the ﬁne-grained demand.

• We construct a dataset from DBpedia. Extensive experiments show that our proposed method sig-
niﬁcantly outperforms 8 state-of-the-art methods, with 3.4% and 2.9% improvement in Mi-F1 and
Ma-F1, respectively.

The rest of this paper is organized as follows. In Section 2, we formally deﬁne the problem of ﬁne-
grained entity typing in knowledge bases. Section 3 demonstrates the proposed approach in detail. The
dataset description and evaluation are presented in Section 4. Section 5 outlines some related works.
Section 6 concludes our work.

2 Problem Formulation

In this section, we formally deﬁne the problem of Fine-Grained Entity Typing in KB.

Deﬁnition 1 Knowledge Base is deﬁned a tuple of entities E and a type hierarchy H, formally, KB =
(cid:104)E, H(cid:105), where H is used to organize entities in E. Each entity e ∈ E denotes a speciﬁc thing in the real
world.

Deﬁnition 2 Type Hierarchy H is deﬁned as a tree or a directed acyclic graph (DAG). It provides a
natural way to categorize and organize entities in large KB. It is formalized (T , R), where T is the type
set and R = {(ti, tj)|ti, tj ∈ T ∧ i (cid:54)= j} is the relation set, in which (ti, tj) means that ti is the subtype
of tj.

We distinguish three kinds of entity knowledge which are beneﬁcial to ﬁne-grained entity typing task.

• Entity Link Relation: KB contains a large number of link relations between entities. It is natural to
convert entities in E into a network structure via these link relations. If an entity ei links to another
entity ej in KB, there will be a directed edge from entity ei to entity ej. These link relations reﬂect
the overall structure of KB and provide important semantic relatedness between entities.

• Entity Attribute: Besides the above link structure, each entity e ∈ E also has textual description,
properties and categories (shown in Fig. 1). We term all such auxiliary information of an entity as
attributes. The attributes of an entity provide valuable clues to predict its types.

• Entity Type: Part of entities in E have been assigned with types, we refer them as E l. The assigned
types for an entity form a type-path in type hierarchy H, which starts from the root and ends at one
speciﬁc type (not necessary to end at the leaf type). Those entities in E without type information
are referred as E u. The type information observed in E l can be a useful signal to infer missing type
information in E u.

From the above deﬁnitions, we achieve a partially-labeled attributed entity network, including link

structure, entity attributes, and type information. We formally deﬁne this entity network as follow:
Deﬁnition 3 Entity Network: Let G = (E, L) be an entity network. E = E l ∪ E u is the entity set, where
|E| = N and |E l| = N l. L = {(ei, ej) |ei, ej ∈ E and i (cid:54)= j} is the link relation set.

For the convenience of formula deduction, we redeﬁne this entity network in matrix form, G =
(A, X, Y). Adjacency matrix A ∈ RN ×N is equivalent to link set L, Aij = 1 if there is a link from
ei to ej, otherwise 0. Each row in A refers to an entity’s link vector Ai. Attribute matrix X ∈ RN ×M
collects attributes for all entities, where M is the number of attributes. Each row in X refers to an entity’s
attribute vector Xi. X can either be binary or take any real value. Type matrix Y ∈ RN l×K collects type
information for entities in E l, where K is the number of types. Each row in Y refers to the type vector
Yi of a labeled entity in E l. Yij = 1 if ei has been assigned with type tj, otherwise 0.

Based on the terminologies described above, we formally deﬁne the problem of ﬁne-grained entity

typing in KB as follows:

Deﬁnition 4 Fine-grained entity typing in KB: Given a partially-labeled attributed entity network G =
(E, L), we aim at learning a type predictor from E l, which comprehensively takes link structure, entity
attributes and type information into account. Then we utilize the learned type predictor to predict the
type of e ∈ E u, i.e., infer whether entity e ∈ E u is an instance of type t ∈ T .

3 The Proposed Approach

3.1 General Architecture

A general state-of-the-art architecture for ﬁne-grained entity typing consists of two components.

• Entity Representation: It learns distributed representation (cid:126)v (e) for each entity e ∈ E.

• Type Predictor: It learns a predict function P (t|e) using E l as training examples. P (t|e) denotes
the probability that entity e belongs to type t, and it is often used to infer types of those unlabeled
entities in E u.

Type predictor is regarded as a multi-label classiﬁcation model. P (t|e) is calculated through a two-

layer Multi-Layer Perceptron (MLP).

[P (t1|e) · · · P (tK|e)] = σ (Woutf (Win(cid:126)v (e)))

(1)

Each entry of output layer indicates the predicted probability of type ti for a given entity e. Win and
f is the nonlinear activation function,
Wout are MLP parameter matrices. σ is the sigmoid function.
such as ReLU and tanh.

The loss L for a prediction y = [P (t1|e) · · · P (tK|e)] when the true labels are encoded in a binary

vector y∗ ∈ {0, 1}1×K is the following cross entropy loss function:

L (y, y∗) =

−y∗(i) log y(i) −

(cid:16)

1 − y∗(i)(cid:17)

log

(cid:16)

1 − y(i)(cid:17)

(2)

K
(cid:88)

i=1

The key difﬁculty in computing P (t|e) is to learn a good representation (cid:126)v (e) for entity e. Several
models have achieved the state-of-the-art performance. However, these models still face the following
non-trivial challenges:

• Entity attributes and link structure are typically regraded as separated features without considering
their correlations. Recent research shows the importance of integrating link structure and entity
attributes for learning more informative representations (Liao et al., 2017; Li et al., 2017). Could
we seamlessly integrate entity attributes and link structure into a uniﬁed entity network? Since the
link structure and entity attributes offer different sources of information, how shall we capture the
complex non-linear relationship between them?

• The type information of the labeled entities in E l is only used in the learning phase of the type pre-
dictor, but totally ignored in the representation learning phase. Recent research shows incorporating
labeled information in representation learning could enhance the predictive power of the learning
embeddings for speciﬁc tasks (Tang et al., 2015a; Yang et al., 2016). In our task, the entity types
and their formed type hierarchy H are valuable labeled information, could we design a predictive
model to make full use of the type information in E l and the structure of type hierarchy?

To address the above challenges, we propose a novel attributed and predictive entity embedding
method to learn entity representations.
It combines the advantages of attributed network embedding
method and semi-supervised learning. We integrate entity attributes and link structure via a deep neural
network model (Section 3.2). For an entity ei in E l, we jointly predict its type and its neighbor entities
in the entity network (Section 3.3). In this way, we can fully utilize the type information in E l.

3.2 Attributed Entity Embedding

We aim at learning low-dimensional vector representations for each entity in the entity network, such
that original link structure and entity attribute proximity can be preserved in vectors. The most intuitive
idea is to directly concatenate the embeddings learned from different parts. However, this way can
not capture the complex non-linear relationship between them, because each part is trained separately
without interaction. To address this issue, we present a neural network model to integrate link structure
(structural information) and entity attributes (attribute information). For an entity ei ∈ E, we jointly
utilize its link vector Ai and attribute vector Xi to predict the link probability between ei and other
entities (shown in Fig. 2). Our model integrates structure and attribute information on the input layer, so
that these two parts closely interact with each other and all parameters can be optimized simultaneously.
In our design, each entity e has two vector representations, self representation and context representa-
tion. e is self representation of e when it is treated as an entity in entity network G. While ˜e is context
representation of e when it is treated as a speciﬁc context. To comprehensively represent an entity e, we
add e and ˜e as the ﬁnal representation (cid:126)v (e).

Input and Embeddings. For an entity ei ∈ E, Ai and Xi denote its link vector and attribute vector
as deﬁned in Section 2. Our model takes both structural information (Ai) and attribute information (Xi)
as input, and utilizes two embedding weight matrices to achieve the ﬁnal input embeddings. Wstru
projects the link vector Ai to a dense vector ustru which captures structural information. Wattr encodes
the attribute vector Xi and generates a compact vector uattr which aggregates attribute information. We
concatenate ustru and uattr, then feed this concatenated vector into a Multi-Layer Perceptron.

Information Fusion. We employ a Multi-Layer Perceptron to integrate structural and attribute infor-
mation. Recent research shows stacked multiple non-linear layers can help learn better representations

Figure 2: Attributed entity embedding framework

of data (Liao et al., 2017). Each layer is calculated as follows:

h(1) = f

h(k) = f

(cid:16)

(cid:16)

W(1) [ustru, λuattr] + b(1)(cid:17)
W(k)h(k−1) + b(k)(cid:17)

, k = 1, 2, · · · , n

λ is the trade-off factor for structure and attribute information. f denotes the activation function. n is
the number of hidden layers. From the last hidden layer, we obtain an informative representation h(n),
which is regarded as the self representation ei of the input entity ei.

Neighbor Prediction. Finally, we predict neighbors for an input entity ei (if there is a link from
ei to ej, ej is ei’s neighbor). We call it neighbor prediction, which is widely used to model network
structure (Tang et al., 2015a; Liao et al., 2017). Speciﬁcally, we utilize the self representation ei to
predict the link probability between entity ei and other entities. The predictive link probability is deﬁned
through a softmax layer as follows:

p (ej|ei) =

exp (˜ej · ei)
j(cid:48)=1 exp (cid:0)˜ej(cid:48) · ei

(cid:1)

(cid:80)N

˜ej denotes the context representation of entity ej. Let yi denotes the predicted link probability vector for
entity ei, y(j)
is the j-th entry of the vector yi. Ai is ei’s link vector (in Section 2), Aij indicates whether
there is a link from ei to ej. The loss function Lattr is deﬁned as cross entropy over all entities in E

i

Lattr =

N
(cid:88)

N
(cid:88)

i=1

j=1

−Aij log y(j)

i − (1 − Aij) log

1 − y(j)

i

(cid:16)

(cid:17)

3.3 Predictive Entity Embedding

In this section, we propose predictive entity embedding learning that makes full use of the type infor-
mation in E l, which is often ignored by existing methods in representation learning phase. Inspired by
(Tang et al., 2015a; Yang et al., 2016), we ﬁrst introduce a type prediction loss to equip learned embed-
dings with strong predictive power for entity typing task (Section 3.3.1). Then, to meet the ﬁne-grained
demand, we model the tree-structured type hierarchy in a margin-based framework (Section 3.3.2).

3.3.1 Type Prediction
We fully utilize type information in E l to make the learned embeddings more predictive. For each entity
ei ∈ E l, we not only predict its neighbors in G (Section 3.2), but also predict its types. We add another
softmax layer, the probability of entity ei being an instance of type tj is deﬁned as follows:

p (tj|ei) =

exp (tj · ei)
j(cid:48)=1 exp (cid:0)tj(cid:48) · ei

(cid:1)

(cid:80)K

(3)

(4)

(5)

(6)

(7)

tj denotes the representation of type tj. Let yi denotes the predicted type probability vector for entity
ei, y(j)
is the j-th entry of the vector yi. Yi is ei’s type vector (in Section 2), Yij indicates whether ei is
i
an instance of tj. The loss function Ltype is deﬁned as cross entropy over all entities in E l:

Ltype =

N l
(cid:88)

K
(cid:88)

i=1

j=1

−Yij log y(j)

i − (1 − Yij) log

1 − y(j)

i

(cid:16)

(cid:17)

(8)

3.3.2 Type Hierarchy Modeling
Fine-grained entity typing requires us to pay more attention to speciﬁc types (ﬁne-grained types) than
to general types (coarse-grained types), because speciﬁc types are more informative. For example, Yao
Ming belongs to BasketballPlayer and Athlete, and it should get higher score on BasketballPlayer than
Athlete, because the former characterizes Yao Ming more accurately. Besides, Yao Ming should get higher
score on the correct type BasketballPlayer than its wrong sibling type SoccerPlayer although both types
belong to Athlete. If different types are treated equally, the structural information between types will be
lost. Therefore, we use type order to model different relatedness between an entity and its related types,
which reﬂects the structural information of type hierarchy to a certain degree. In particular, we deﬁne
two kinds of type order as follow:

Deﬁnition 5 Ancestor order: In the type-path of an entity, speciﬁc types should get higher score than its
ancestor. For example, Yao Ming should get higher score on BasketballPlayer than Athlete.

Ancestor order reﬂects the relationship between a type and its ancestor types. For each triple (e, t, ta),
where t is one of the type for entity e, and ta is t’s ancestor in type hierarchy. We deﬁne adaptive margin
γtta = ξ × l(ta,t)
l(root,t) , where l(ta, t) denotes the number of steps going down from type ta to type t. The
hinge loss is deﬁned as:

max (0, s (e, ta) − s (e, t) + γtta)

Lancestor = E(e,t,ta) [max (0, s (e, ta) − s (e, t) + γtta)]

where s (e, t) is the score function, deﬁned as the inner product of e and t.

Deﬁnition 6 Sibling order: For an entity, the correct type should get higher score than its wrong sib-
ling type in type hierarchy. For example, Yao Ming should get higher score on BasketballPlayer than
SoccerPlayer.

Sibling order reﬂects the relationship between a type and its sibling types. For each triple (e, t, ts),
where t is one of the true type for entity e, and ts is its wrong sibling type of entity e. We use the ﬁxed
margin ξ. The hinge loss is deﬁned as:

max (0, s (e, ts) − s (e, t) + ξ)

Lsibling = E(e,t,ts) [max (0, s (e, ts) − s (e, t) + ξ)]

We achieve the loss function Lhier for type hierarchy modeling: Lhier = Lancestor + Lslibing.

Objective and Training. Finally, we sum these three loss function to get the ﬁnal objective function of
entity representation component.

Loss = Lattr + λ1 · Ltype + λ2 · Lhier

λ1 and λ2 are weight parameters. We apply negative sampling (Mikolov et al., 2013; Tang et al., 2015b),
where only a very small subset of entities are sampled from E. Parameters are optimized through Adap-
tive Moment Estimation (Adam). In the embedding layer and each hidden layer, we also add dropout
component to alleviate over ﬁtting.

(9)

(10)

(11)

(12)

(13)

4 Experiments and Analysis

In this section, we evaluate the proposed method using real world dataset collected from DBpedia. We
will introduce the dataset and experiment settings in Section 4.1, present the comparison results in Sec-
tion 4.2 and investigate some method details in Section 4.3. Our source code is available1 for reference.

4.1 Datasets and Experimental Setup

Datasets: To the best of our knowledge, there is no public large-scale dataset available for ﬁne-grained
entity typing in KB. We construct a dataset from DBpedia. The type hierarchy in DBpedia is of tree
structure. We extract link relation and attribute information from DBpedia2. For each entity, we use the
single type-path assigned in DBpedia as the ground truth.

For this dataset, we remove those entities which are only labeled as Thing. Because they do not provide
any semantic information. In representation learning phase, we utilize the whole dataset with only 50%
entities are labeled with types. In type predictor phase, we follow a 50/30/20 ratio to split the dataset into
training, dev and test sets. Table 1 shows the detailed statistics. Each entity has 3.27 types on average.

Table 1: Statistics of the dataset

Type

Entity

Link

214

300,000

5,243,230

Attribute

word
5,350

property
200

category
350

Metrics: To evaluate the performance of our proposed method, we use Accuracy (Strict-F1), Micro-
averaged F1 (Mi-F1) and Macro-averaged F1 (Ma-F1), which have been used in many ﬁne-grained
typing systems (Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh and Sch¨utze, 2017; Ren et al., 2016a).
Parameter Settings: Our implementation of APE is based on TensorFlow3. Regarding the choice of
activation function of hidden layers, we have tried ReLU, softsign and tanh, ﬁnding softsign leads to the
best performance in general. We randomly initialize model parameters with a Gaussian distribution (with
a mean of 0.0 and standard deviation of 0.01), optimizing the model with mini-batch Adam (Kingma
and Ba, 2014). We set ﬁxed margin ξ = 1 in our experiments. Both λ1 and λ2 are chosen among
{0.2, 0.4, 0.6, 0.8}, we found λ1 = 0.8 and λ2 = 0.4 achieve best performance.
Baseline: We denote our model as APE, and compare it with three sets of methods:

• Entity typing methods: We compare APE with ﬁve state-of-the-art entity typing methods. FIG-
MENT uses entity representation learned from annotated corpus for type inference (Yaghoobzadeh
and Sch¨utze, 2015). CUTE employs a multi-label hierarchical classiﬁcation method to address en-
tity typing task (Xu et al., 2016). MuLR uses multi-level representations of entities (character, word
and entity) to make type inference in KB (Yaghoobzadeh and Sch¨utze, 2017). Global utilizes en-
tity text description to infer missing entity type instances (Neelakantan and Chang, 2015). Corpus
designs global and context model to make type inference jointly (Yaghoobzadeh et al., 2017).

• Network Embedding methods: Attributed and predictive network embedding methods can learn
informative entity representations, so we compare APE with them. PTE learns predictive text em-
beddings from heterogeneous network (Tang et al., 2015a). Planetoid is a semi-supervised network
embedding method which makes neighbor and label prediction jointly (Yang et al., 2016). ASNE
integrates structural and attribute information via a deep neural architecture (Liao et al., 2017).

• APE’s variants: If we ignore the type information, that is, utilize the embeddings learned from
section 3.2 to make type inference, the method is denoted as APEno type. Similarly, if we omit the
type hierarchy modeling in Section 3.3.2, the method is denoted as APEno hierarchy.

4.2 Overall Comparison Results

Table 2 shows the overall performance and we have the following conclusions:

1https://github.com/Tsinghua-PhD/APE
2http://wiki.dbpedia.org/downloads-2016-10
3https://www.tensorflow.org/

Table 2: Typing performance on DBpedia dataset

Type

Entity Typing

Network Embedding ASNE

Our Variants

Algorithm
FIGMENT
CUTE
MuLR
Corpus
Global
Planetoid

PTE
APEno type
APEno hierarchy
APE

Acc Ma-F1 Mi-F1
0.628
0.619
0.463
0.677
0.673
0.515
0.662
0.654
0.501
0.659
0.662
0.488
0.615
0.608
0.457
0.590
0.585
0.434
0.573
0.568
0.417
0.518
0.512
0.352
0.657
0.649
0.492
0.689
0.694
0.531
0.711
0.702
0.545

Comparison with Entity Typing methods. Our model outperforms the state-of-the-art entity typing
methods, and achieves 3.4% and 2.9% improvement in Mi-F1 and Ma-F1 respectively, because it em-
ploys multiple kinds of information for type inference. Each part can provide complementary information
about entities for type inference. Our model leverages label information (type information) in represen-
tation learning phase, equip the learned embeddings with strong predictive power in entity typing task.
Comparison with Network Embedding methods. Our model outperforms the state-of-the-art network
embedding methods (i.e., Planetoid, ASNE and PTE), and achieves 12.1% and 11.7% improvement in
Mi-F1 and Ma-F1 respectively. We model the structure of type hierarchy in a margin-based loss function.
Speciﬁcally, ancestor order makes entities closer to speciﬁc types than general types, and sibling order
makes the learned entity embeddings have strong discriminative power between sibling types.
Comparison with Variants. APE consistently outperforms APEno type and APEno hierarchy, and
the simplest version APEno type could achieve comparable results with state-of-the-art methods. The
results verify that each of these information sources contributes complementary information for ﬁne-
grained typing of entities. Structural information (link relation) provides important semantic relatedness
between entities. Attribute information (text, property and category) provides additional clues for type
inference. Type information (type hierarchy) makes our model more powerful in classiﬁcation task.

4.3 Result Analysis

4.3.1

Impact of Features

We investigate the effect of information sources used to learn entity embeddings. In Table 3, Stru, T,
P and C is short for structural information, text description, property and category, respectively. From
Table 3, we ﬁnd different information sources play different roles in this task. Structural information
is most common in KB, it makes related entities have similar representations. It models the relation
between entities, while neglecting entities’s self-information. Attribute information (i.e., text description,
property and category) can be viewed as entity features, which is often informative for type inference.
We compare our model APE, APEno type and APEno hierarchy with different kinds of information
combination. For attribute information, category information is more import than property information
and text descriptions.

4.3.2 Effects of Labeled Data

We investigate the effect of labeled data proportion. Intuitively, our model can learn better embeddings
and achieve better performance if we have more labeled entities. The results in Fig. 3 show that three
metrics strikingly increase with more labeled data added, and gradually become stable when the propor-
tion is over 0.5. It proves that our model can achieve a satisfactory result with not too much labeled data,
and this advantage beneﬁts from the entity link relations and network embedding.

Table 3: Performance on different information source combination.

Information Type

Stru+T
Stru+P
Stru+C
Stru+P+C
Stru+T+P
Stru+T+C
Stru+T+P+C

APE
Acc Ma-F1 Mi-F1
0.631
0.623
0.467
0.637
0.633
0.475
0.646
0.641
0.481
0.649
0.642
0.479
0.655
0.647
0.484
0.653
0.643
0.481
0.657
0.649
0.492

APEno type
Acc Ma-F1 Mi-F1
0.634
0.625
0.463
0.642
0.638
0.480
0.647
0.649
0.487
0.679
0.685
0.520
0.659
0.658
0.507
0.681
0.683
0.518
0.689
0.694
0.531

APEno hierarchy
Acc Ma-F1 Mi-F1
0.633
0.632
0.472
0.653
0.651
0.489
0.647
0.650
0.486
0.669
0.677
0.523
0.660
0.657
0.510
0.673
0.678
0.525
0.711
0.702
0.545

Table 4: Type macro average F1 on test for all, fre-
quent and infrequent types, #training/test means the
average entity number for each type.

Type

infrequent

frequent

all

# training

# test

FIGMENT

MuLR

Corpus

CUTE

APE

1013

417

0.327

0.374

0.447

0.418

0.461

13,513

5,217

0.620

0.718

0.783

0.730

0.792

8,673

3,462

0.557

0.636

0.675

0.652

0.681

Figure 3: Performance on different labeled data
proportion.

4.3.3 Results of Frequent/Infrequent Types

We use type macro average F1 proposed by (Yosef et al., 2012) to evaluate the performance on frequent
types and infrequent types. Note that it is different from Ma-F1 reported in Table 2. The results for
infrequent types (occur less than 2,000 times) and frequent types (occur more than 10,000 times) are
shown in Table 4. Generally, the performance on infrequent types is worse than frequent ones. Our
model consistently outperforms the other methods on infrequent types, which demonstrates its ability on
dealing with rare types.

5 Related Work

Fine-grained entity typing in KB is an important sub-task of knowledge base completion. One way to
address this problem is making type inference based on entity’s attribute information in KB, such as
textual description, property, category and so on. (Neelakantan and Chang, 2015) infer missing types for
entities in KB based on text descriptions. (Xu et al., 2016) employ a multi-label hierarchical classiﬁca-
tion method to assign Chinese entities with DBpedia types based on property and category information.
The other way relies on large-scale annotated corpus, then extracts type information from the annotated
corpus for entities in KB. (Yaghoobzadeh and Sch¨utze, 2015) ﬁrst propose FIGMENT to address this
problem. They only used contextual information to assign types for entities in KB. After that, they
present FIGMENT-Multi to learn multi-level representations of entities on three complementary levels
(character, word and entity). FIGMENT-Multi predicts whether an entity is a member of a type based
on the learned embeddings (Yaghoobzadeh and Sch¨utze, 2017). Finally, they propose an embedding
based method which combines a global model with a context model. A global model that scores based
on aggregated context information and a context model that aggregates the scores of individual con-
texts (Yaghoobzadeh et al., 2017).

Fine-grained entity typing in text is related to our task. Different from the works mentioned above, it

seeks to detect local types of an entity mention inside an individual sentence, and the same entity could
have different types in different sentences (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014;
Dong et al., 2015; Del Corro et al., 2015; Yogatama et al., 2015; Ren et al., 2016b; Ren et al., 2016a;
Shimaoka et al., 2017; Rabinovich and Dan, 2017).

6 Conclusion

In this paper, we propose an attributed and predictive entity embedding method to address the task of
ﬁne-grained entity typing in KB. It combines the advantages of attributed network embedding method
and semi-supervised learning. Speciﬁcally, it employs a deep neural network architecture to integrate
structure and attribute information of an entity. We jointly predict a labeled entity’s neighbors and types,
and model the structure of type hierarchy in a margin-based way. Experiments on real world datasets
demonstrate the effectiveness of the proposed model.

The work is
supported by National Key Research and Development Program of China
(2017YFB1002101), NSFC key project (U1736204), Ministry of Education and China Mobile Research
Fund (No. 20181770250), and THUNUS NExT Co-Lab.

Acknowledgements

References

Luciano Del Corro, Abdalghani Abujabal, Rainer Gemulla, and Gerhard Weikum. 2015. Finet: Context-aware
ﬁne-grained named entity typing. In Conference on Empirical Methods in Natural Language Processing, pages
868–878.

Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua
Sun, and Wei Zhang. 2014. Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In
Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,
pages 601–610. ACM.

Li Dong, Furu Wei, Hong Sun, Ming Zhou, and Ke Xu. 2015. A hybrid neural model for type classiﬁcation of

entity mentions. In International Conference on Artiﬁcial Intelligence, pages 1243–1249.

Greg Durrett and Dan Klein. 2014. A joint model for entity analysis : Coreference , typing , and linking. TACL,

pages 477–490.

arXiv:1412.6980.

Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse Kirchner, and David Huynh. 2014. Context-dependent ﬁne-

grained entity type tagging. arXiv:1412.1820.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint

Hang Li, Haozheng Wang, Zhenglu Yang, and Masato Odagaki. 2017. Variation autoencoder based network

representation learning for classiﬁcation. In ACL 2017, Student Research Workshop, pages 56–61.

Lizi Liao, Xiangnan He, Hanwang Zhang, and Tat Seng Chua. 2017. Attributed social network embedding.

arXiv:1705.04969.

Xiao Ling and Daniel S Weld. 2012. Fine-grained entity recognition. In AAAI, pages 94–100.

Xiao Ling, Sameer Singh, and Daniel S Weld. 2015. Design challenges for entity linking. TACL, pages 315–328.

Yang Liu, Kang Liu, Liheng Xu, and Jun Zhao. 2014. Exploring ﬁne-grained entity type constraints for distantly

supervised relation extraction. In COLING, pages 2107–2116.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of
words and phrases and their compositionality. In International Conference on Neural Information Processing
Systems, pages 3111–3119.

Gregory Murphy. 2004. The big book of concepts. MIT press.

David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classiﬁcation. Lingvisticae

Investigationes, 30(1):3–26.

Arvind Neelakantan and Ming Wei Chang. 2015.

Inferring missing entity type instances for knowledge base

completion: New dataset and methods. Computer Science, pages 35–40.

Maxim Rabinovich and Klein Dan. 2017. Fine-grained entity typing with high-multiplicity assignments.

In

Meeting of the Association for Computational Linguistics, pages 330–334.

Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceed-
ings of the Thirteenth Conference on Computational Natural Language Learning, pages 147–155. Association
for Computational Linguistics.

Xiang Ren, Wenqi He, Meng Qu, Lifu Huang, Heng Ji, and Jiawei Han. 2016a. Afet: Automatic ﬁne-grained
entity typing by hierarchical partial-label embedding. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1369–1378.

Xiang Ren, Wenqi He, Meng Qu, Clare R Voss, Heng Ji, and Jiawei Han. 2016b. Label noise reduction in entity
In Proceedings of the 22nd ACM SIGKDD International

typing by heterogeneous partial-label embedding.
Conference on Knowledge Discovery and Data Mining, pages 1825–1834.

Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and Sebastian Riedel. 2017. Neural architectures for ﬁne-

grained entity type classiﬁcation. In EACL, pages 1271–1280.

Jian Tang, Meng Qu, and Qiaozhu Mei. 2015a. Pte: Predictive text embedding through large-scale heterogeneous
text networks. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 1165–1174. ACM.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015b. Line: Large-scale infor-
mation network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages
1067–1077. ACM.

Ji Xin, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2018.

Improving neural ﬁne-grained entity typing with

knowledge attention. In AAAI.

Bo Xu, Yi Zhang, Jiaqing Liang, Yanghua Xiao, Seung-won Hwang, and Wei Wang. 2016. Cross-lingual type

inference. In International Conference on Database Systems for Advanced Applications, pages 447–462.

Yadollah Yaghoobzadeh and Hinrich Sch¨utze. 2015. Corpus-level ﬁne-grained entity typing using contextual
information. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,
pages 715–725.

Yadollah Yaghoobzadeh and Hinrich Sch¨utze. 2017. Multi-level representations for ﬁne-grained typing of knowl-

edge base entities. arXiv:1701.02025.

Yadollah Yaghoobzadeh, Heike Adel, and Hinrich Sch¨utze. 2017. Corpus-level ﬁne-grained entity typing.

arXiv:1708.02275.

Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, and Gerhard Weikum. 2014. Robust question answering

over the web of linked data. In CIKM, pages 1107–1116.

Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. 2016. Revisiting semi-supervised learning with graph

embeddings. In ICML, pages 40–48.

Dani Yogatama, Dan Gillick, and Nevena Lazic. 2015. Embedding methods for ﬁne grained entity type classiﬁca-
tion. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language
Processing, ACL, pages 26–31.

M Amir Yosef, Sandro Bauer, Johannes Hoffart, Marc Spaniol, and Gerhard Weikum. 2012. Hyena: Hierarchical
In Proceedings of the 24th International Conference on Computational

type classiﬁcation for entity names.
Linguistics, pages 1361–1370.

Attributed and Predictive Entity Embedding for Fine-Grained
Entity Typing in Knowledge Bases

Hailong Jin and Lei Hou and Juanzi Li
DCST, Tsinghua University
Beijing 100084, China

Tiansi Dong
B-IT, Bonn University
53113 Bonn, Germany

{jinhl15@mails.,houlei@mail.,lijuanzi@}tsinghua.edu.cn
dongt@bit.uni-bonn.de

Abstract

Fine-grained entity typing aims at identifying the semantic type of an entity in KB. Type in-
formation is very important in knowledge bases, but is unfortunately incomplete even in some
large knowledge bases. Limitations of existing methods are either ignoring the structure and
type information in KB or requiring large scale annotated corpus. To address these issues, we
propose an attributed and predictive entity embedding method, which can fully utilize various
kinds of information comprehensively. Extensive experiments on real DBpedia dataset show that
our proposed method signiﬁcantly outperforms 8 state-of-the-art methods, with 3.4% and 2.9%
improvement in Mi-F1 and Ma-F1, respectively.

1

Introduction

Knowledge about entities in Knowledge Base (KB) is essential for language understanding. This knowl-
edge can be attributional (e.g., canFly, hasAge), relational (e.g., marriedTo, bornIn) or type-based (e.g.,
isFood, isLocation). Type information, which clusters a group of entities with same properties, is valu-
able in KB, because it is the glue that holds our mental world together (Murphy, 2004). Types in KB
are usually organized as a hierarchical structure, namely type hierarchy. Entity typing assigns types
(e.g., Person, Athlete, BasketballPlayer) to an entity (e.g., Yao Ming) in KB, and is a fundamental task
in knowledge base construction.

Traditional entity typing focuses on a small set of types, such as Person, Location and Or-
ganization (Ratinov and Roth, 2009; Nadeau and Sekine, 2007), while ﬁne-grained entity typ-
ing assigns more speciﬁc types to an entity, which normally forms a type-path in the type hier-
archy in KB (Ren et al., 2016a). As shown in Fig. 1, Yao Ming is associated with a type-path
/Thing/Agent/Person/Athlete/BasketballPlayer. Fine-grained types (e.g., Athlete and BasketballPlayer)
are more informative than coarse-grained types (e.g., Person), because they provide more speciﬁc se-
mantic information about the entity (Xu et al., 2016). Characterizing an entity with ﬁne-grained types
(type-paths) beneﬁts many real applications, such as knowledge base completion (Dong et al., 2014),
entity linking (Ling et al., 2015; Durrett and Klein, 2014), relation extraction (Liu et al., 2014), and
question answering (Yahya et al., 2014).

Many researches have been carried out on ﬁne-grained entity typing in text, which detects local types
for an entity mention in a particular plain text from predeﬁned ﬁne-grained entity types (Ling and Weld,
2012; Yogatama et al., 2015; Ren et al., 2016a; Shimaoka et al., 2017; Xin et al., 2018). However, only
a few works aim at ﬁne-grained entity typing in knowledge base, that is, inferring missing types for an
entity in KB, which is the focus of this paper. Most KBs are incomplete and lack of type information, in
part because the world is always changing. For example, 36.53% entities in DBpedia do not have type
information. Therefore, KBs are aways in need of updating and completing. To this end, there are two
main methods in the literature as follow: One method exploits various kinds of information to construct
the feature representation of an entity, such as entity textual description, property and category (Nee-
lakantan and Chang, 2015; Xu et al., 2016). After that, a predict function P (t|e) is learned to infer

This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/
Corresponding author: Lei Hou (houlei@mail.tsinghua.edu.cn)

Figure 1: An example of ﬁne-grained entity typing in KB. The left part is an entity with various infor-
mation. The red ones are assigned types, which forms a type-path.

whether entity e is an instance of type t. This method ignores rich structural information in KB (i.e., link
relation between entities). It is a promising and challenging problem to utilize structural information for
type inference. The second method utilizes annotated corpus to learn entity low-dimensional represen-
tations then makes type inference based on the learned embeddings (Yaghoobzadeh and Sch¨utze, 2015;
Yaghoobzadeh and Sch¨utze, 2017). Apparently, it requires large scale corpus in which entity mentions
are annotated. But large scale highly-qualiﬁed annotated corpus is often difﬁcult to obtain.

To address the above issues, we propose a novel attributed and predictive entity embedding method,
which can be described as follow: We ﬁrst construct a partially-labeled attributed entity network, which
contains structural, attribute and type information for entities. Here “partially-labeled” means some
entities have been labeled with type information, while others not. Then, we employ a deep neural
network to integrate the structural and attribute information for entity representation learning. For an
unlabeled entity, we jointly utilize its structural and attribute information for neighbor prediction. For
a labeled entity, besides neighbors, we also predict its types and type hierarchy via a margin-based loss
function. Finally, a multi-label classiﬁcation model is used for type inference based on the learned entity
embeddings.

The main contributions of this paper can be summarized as follows:

• We encode various kinds of information into a partially-labeled entity network and propose a deep
neural network model to effectively integrate the structural and attribute information for entity rep-
resentation learning.

• We utilize type information of labeled entities to equip the learned embeddings with strong pre-
dictive power in entity typing task, and further incorporate the structure of type hierarchy via a
margin-based loss function to meet the ﬁne-grained demand.

• We construct a dataset from DBpedia. Extensive experiments show that our proposed method sig-
niﬁcantly outperforms 8 state-of-the-art methods, with 3.4% and 2.9% improvement in Mi-F1 and
Ma-F1, respectively.

The rest of this paper is organized as follows. In Section 2, we formally deﬁne the problem of ﬁne-
grained entity typing in knowledge bases. Section 3 demonstrates the proposed approach in detail. The
dataset description and evaluation are presented in Section 4. Section 5 outlines some related works.
Section 6 concludes our work.

2 Problem Formulation

In this section, we formally deﬁne the problem of Fine-Grained Entity Typing in KB.

Deﬁnition 1 Knowledge Base is deﬁned a tuple of entities E and a type hierarchy H, formally, KB =
(cid:104)E, H(cid:105), where H is used to organize entities in E. Each entity e ∈ E denotes a speciﬁc thing in the real
world.

Deﬁnition 2 Type Hierarchy H is deﬁned as a tree or a directed acyclic graph (DAG). It provides a
natural way to categorize and organize entities in large KB. It is formalized (T , R), where T is the type
set and R = {(ti, tj)|ti, tj ∈ T ∧ i (cid:54)= j} is the relation set, in which (ti, tj) means that ti is the subtype
of tj.

We distinguish three kinds of entity knowledge which are beneﬁcial to ﬁne-grained entity typing task.

• Entity Link Relation: KB contains a large number of link relations between entities. It is natural to
convert entities in E into a network structure via these link relations. If an entity ei links to another
entity ej in KB, there will be a directed edge from entity ei to entity ej. These link relations reﬂect
the overall structure of KB and provide important semantic relatedness between entities.

• Entity Attribute: Besides the above link structure, each entity e ∈ E also has textual description,
properties and categories (shown in Fig. 1). We term all such auxiliary information of an entity as
attributes. The attributes of an entity provide valuable clues to predict its types.

• Entity Type: Part of entities in E have been assigned with types, we refer them as E l. The assigned
types for an entity form a type-path in type hierarchy H, which starts from the root and ends at one
speciﬁc type (not necessary to end at the leaf type). Those entities in E without type information
are referred as E u. The type information observed in E l can be a useful signal to infer missing type
information in E u.

From the above deﬁnitions, we achieve a partially-labeled attributed entity network, including link

structure, entity attributes, and type information. We formally deﬁne this entity network as follow:
Deﬁnition 3 Entity Network: Let G = (E, L) be an entity network. E = E l ∪ E u is the entity set, where
|E| = N and |E l| = N l. L = {(ei, ej) |ei, ej ∈ E and i (cid:54)= j} is the link relation set.

For the convenience of formula deduction, we redeﬁne this entity network in matrix form, G =
(A, X, Y). Adjacency matrix A ∈ RN ×N is equivalent to link set L, Aij = 1 if there is a link from
ei to ej, otherwise 0. Each row in A refers to an entity’s link vector Ai. Attribute matrix X ∈ RN ×M
collects attributes for all entities, where M is the number of attributes. Each row in X refers to an entity’s
attribute vector Xi. X can either be binary or take any real value. Type matrix Y ∈ RN l×K collects type
information for entities in E l, where K is the number of types. Each row in Y refers to the type vector
Yi of a labeled entity in E l. Yij = 1 if ei has been assigned with type tj, otherwise 0.

Based on the terminologies described above, we formally deﬁne the problem of ﬁne-grained entity

typing in KB as follows:

Deﬁnition 4 Fine-grained entity typing in KB: Given a partially-labeled attributed entity network G =
(E, L), we aim at learning a type predictor from E l, which comprehensively takes link structure, entity
attributes and type information into account. Then we utilize the learned type predictor to predict the
type of e ∈ E u, i.e., infer whether entity e ∈ E u is an instance of type t ∈ T .

3 The Proposed Approach

3.1 General Architecture

A general state-of-the-art architecture for ﬁne-grained entity typing consists of two components.

• Entity Representation: It learns distributed representation (cid:126)v (e) for each entity e ∈ E.

• Type Predictor: It learns a predict function P (t|e) using E l as training examples. P (t|e) denotes
the probability that entity e belongs to type t, and it is often used to infer types of those unlabeled
entities in E u.

Type predictor is regarded as a multi-label classiﬁcation model. P (t|e) is calculated through a two-

layer Multi-Layer Perceptron (MLP).

[P (t1|e) · · · P (tK|e)] = σ (Woutf (Win(cid:126)v (e)))

(1)

Each entry of output layer indicates the predicted probability of type ti for a given entity e. Win and
f is the nonlinear activation function,
Wout are MLP parameter matrices. σ is the sigmoid function.
such as ReLU and tanh.

The loss L for a prediction y = [P (t1|e) · · · P (tK|e)] when the true labels are encoded in a binary

vector y∗ ∈ {0, 1}1×K is the following cross entropy loss function:

L (y, y∗) =

−y∗(i) log y(i) −

(cid:16)

1 − y∗(i)(cid:17)

log

(cid:16)

1 − y(i)(cid:17)

(2)

K
(cid:88)

i=1

The key difﬁculty in computing P (t|e) is to learn a good representation (cid:126)v (e) for entity e. Several
models have achieved the state-of-the-art performance. However, these models still face the following
non-trivial challenges:

• Entity attributes and link structure are typically regraded as separated features without considering
their correlations. Recent research shows the importance of integrating link structure and entity
attributes for learning more informative representations (Liao et al., 2017; Li et al., 2017). Could
we seamlessly integrate entity attributes and link structure into a uniﬁed entity network? Since the
link structure and entity attributes offer different sources of information, how shall we capture the
complex non-linear relationship between them?

• The type information of the labeled entities in E l is only used in the learning phase of the type pre-
dictor, but totally ignored in the representation learning phase. Recent research shows incorporating
labeled information in representation learning could enhance the predictive power of the learning
embeddings for speciﬁc tasks (Tang et al., 2015a; Yang et al., 2016). In our task, the entity types
and their formed type hierarchy H are valuable labeled information, could we design a predictive
model to make full use of the type information in E l and the structure of type hierarchy?

To address the above challenges, we propose a novel attributed and predictive entity embedding
method to learn entity representations.
It combines the advantages of attributed network embedding
method and semi-supervised learning. We integrate entity attributes and link structure via a deep neural
network model (Section 3.2). For an entity ei in E l, we jointly predict its type and its neighbor entities
in the entity network (Section 3.3). In this way, we can fully utilize the type information in E l.

3.2 Attributed Entity Embedding

We aim at learning low-dimensional vector representations for each entity in the entity network, such
that original link structure and entity attribute proximity can be preserved in vectors. The most intuitive
idea is to directly concatenate the embeddings learned from different parts. However, this way can
not capture the complex non-linear relationship between them, because each part is trained separately
without interaction. To address this issue, we present a neural network model to integrate link structure
(structural information) and entity attributes (attribute information). For an entity ei ∈ E, we jointly
utilize its link vector Ai and attribute vector Xi to predict the link probability between ei and other
entities (shown in Fig. 2). Our model integrates structure and attribute information on the input layer, so
that these two parts closely interact with each other and all parameters can be optimized simultaneously.
In our design, each entity e has two vector representations, self representation and context representa-
tion. e is self representation of e when it is treated as an entity in entity network G. While ˜e is context
representation of e when it is treated as a speciﬁc context. To comprehensively represent an entity e, we
add e and ˜e as the ﬁnal representation (cid:126)v (e).

Input and Embeddings. For an entity ei ∈ E, Ai and Xi denote its link vector and attribute vector
as deﬁned in Section 2. Our model takes both structural information (Ai) and attribute information (Xi)
as input, and utilizes two embedding weight matrices to achieve the ﬁnal input embeddings. Wstru
projects the link vector Ai to a dense vector ustru which captures structural information. Wattr encodes
the attribute vector Xi and generates a compact vector uattr which aggregates attribute information. We
concatenate ustru and uattr, then feed this concatenated vector into a Multi-Layer Perceptron.

Information Fusion. We employ a Multi-Layer Perceptron to integrate structural and attribute infor-
mation. Recent research shows stacked multiple non-linear layers can help learn better representations

Figure 2: Attributed entity embedding framework

of data (Liao et al., 2017). Each layer is calculated as follows:

h(1) = f

h(k) = f

(cid:16)

(cid:16)

W(1) [ustru, λuattr] + b(1)(cid:17)
W(k)h(k−1) + b(k)(cid:17)

, k = 1, 2, · · · , n

λ is the trade-off factor for structure and attribute information. f denotes the activation function. n is
the number of hidden layers. From the last hidden layer, we obtain an informative representation h(n),
which is regarded as the self representation ei of the input entity ei.

Neighbor Prediction. Finally, we predict neighbors for an input entity ei (if there is a link from
ei to ej, ej is ei’s neighbor). We call it neighbor prediction, which is widely used to model network
structure (Tang et al., 2015a; Liao et al., 2017). Speciﬁcally, we utilize the self representation ei to
predict the link probability between entity ei and other entities. The predictive link probability is deﬁned
through a softmax layer as follows:

p (ej|ei) =

exp (˜ej · ei)
j(cid:48)=1 exp (cid:0)˜ej(cid:48) · ei

(cid:1)

(cid:80)N

˜ej denotes the context representation of entity ej. Let yi denotes the predicted link probability vector for
entity ei, y(j)
is the j-th entry of the vector yi. Ai is ei’s link vector (in Section 2), Aij indicates whether
there is a link from ei to ej. The loss function Lattr is deﬁned as cross entropy over all entities in E

i

Lattr =

N
(cid:88)

N
(cid:88)

i=1

j=1

−Aij log y(j)

i − (1 − Aij) log

1 − y(j)

i

(cid:16)

(cid:17)

3.3 Predictive Entity Embedding

In this section, we propose predictive entity embedding learning that makes full use of the type infor-
mation in E l, which is often ignored by existing methods in representation learning phase. Inspired by
(Tang et al., 2015a; Yang et al., 2016), we ﬁrst introduce a type prediction loss to equip learned embed-
dings with strong predictive power for entity typing task (Section 3.3.1). Then, to meet the ﬁne-grained
demand, we model the tree-structured type hierarchy in a margin-based framework (Section 3.3.2).

3.3.1 Type Prediction
We fully utilize type information in E l to make the learned embeddings more predictive. For each entity
ei ∈ E l, we not only predict its neighbors in G (Section 3.2), but also predict its types. We add another
softmax layer, the probability of entity ei being an instance of type tj is deﬁned as follows:

p (tj|ei) =

exp (tj · ei)
j(cid:48)=1 exp (cid:0)tj(cid:48) · ei

(cid:1)

(cid:80)K

(3)

(4)

(5)

(6)

(7)

tj denotes the representation of type tj. Let yi denotes the predicted type probability vector for entity
ei, y(j)
is the j-th entry of the vector yi. Yi is ei’s type vector (in Section 2), Yij indicates whether ei is
i
an instance of tj. The loss function Ltype is deﬁned as cross entropy over all entities in E l:

Ltype =

N l
(cid:88)

K
(cid:88)

i=1

j=1

−Yij log y(j)

i − (1 − Yij) log

1 − y(j)

i

(cid:16)

(cid:17)

(8)

3.3.2 Type Hierarchy Modeling
Fine-grained entity typing requires us to pay more attention to speciﬁc types (ﬁne-grained types) than
to general types (coarse-grained types), because speciﬁc types are more informative. For example, Yao
Ming belongs to BasketballPlayer and Athlete, and it should get higher score on BasketballPlayer than
Athlete, because the former characterizes Yao Ming more accurately. Besides, Yao Ming should get higher
score on the correct type BasketballPlayer than its wrong sibling type SoccerPlayer although both types
belong to Athlete. If different types are treated equally, the structural information between types will be
lost. Therefore, we use type order to model different relatedness between an entity and its related types,
which reﬂects the structural information of type hierarchy to a certain degree. In particular, we deﬁne
two kinds of type order as follow:

Deﬁnition 5 Ancestor order: In the type-path of an entity, speciﬁc types should get higher score than its
ancestor. For example, Yao Ming should get higher score on BasketballPlayer than Athlete.

Ancestor order reﬂects the relationship between a type and its ancestor types. For each triple (e, t, ta),
where t is one of the type for entity e, and ta is t’s ancestor in type hierarchy. We deﬁne adaptive margin
γtta = ξ × l(ta,t)
l(root,t) , where l(ta, t) denotes the number of steps going down from type ta to type t. The
hinge loss is deﬁned as:

max (0, s (e, ta) − s (e, t) + γtta)

Lancestor = E(e,t,ta) [max (0, s (e, ta) − s (e, t) + γtta)]

where s (e, t) is the score function, deﬁned as the inner product of e and t.

Deﬁnition 6 Sibling order: For an entity, the correct type should get higher score than its wrong sib-
ling type in type hierarchy. For example, Yao Ming should get higher score on BasketballPlayer than
SoccerPlayer.

Sibling order reﬂects the relationship between a type and its sibling types. For each triple (e, t, ts),
where t is one of the true type for entity e, and ts is its wrong sibling type of entity e. We use the ﬁxed
margin ξ. The hinge loss is deﬁned as:

max (0, s (e, ts) − s (e, t) + ξ)

Lsibling = E(e,t,ts) [max (0, s (e, ts) − s (e, t) + ξ)]

We achieve the loss function Lhier for type hierarchy modeling: Lhier = Lancestor + Lslibing.

Objective and Training. Finally, we sum these three loss function to get the ﬁnal objective function of
entity representation component.

Loss = Lattr + λ1 · Ltype + λ2 · Lhier

λ1 and λ2 are weight parameters. We apply negative sampling (Mikolov et al., 2013; Tang et al., 2015b),
where only a very small subset of entities are sampled from E. Parameters are optimized through Adap-
tive Moment Estimation (Adam). In the embedding layer and each hidden layer, we also add dropout
component to alleviate over ﬁtting.

(9)

(10)

(11)

(12)

(13)

4 Experiments and Analysis

In this section, we evaluate the proposed method using real world dataset collected from DBpedia. We
will introduce the dataset and experiment settings in Section 4.1, present the comparison results in Sec-
tion 4.2 and investigate some method details in Section 4.3. Our source code is available1 for reference.

4.1 Datasets and Experimental Setup

Datasets: To the best of our knowledge, there is no public large-scale dataset available for ﬁne-grained
entity typing in KB. We construct a dataset from DBpedia. The type hierarchy in DBpedia is of tree
structure. We extract link relation and attribute information from DBpedia2. For each entity, we use the
single type-path assigned in DBpedia as the ground truth.

For this dataset, we remove those entities which are only labeled as Thing. Because they do not provide
any semantic information. In representation learning phase, we utilize the whole dataset with only 50%
entities are labeled with types. In type predictor phase, we follow a 50/30/20 ratio to split the dataset into
training, dev and test sets. Table 1 shows the detailed statistics. Each entity has 3.27 types on average.

Table 1: Statistics of the dataset

Type

Entity

Link

214

300,000

5,243,230

Attribute

word
5,350

property
200

category
350

Metrics: To evaluate the performance of our proposed method, we use Accuracy (Strict-F1), Micro-
averaged F1 (Mi-F1) and Macro-averaged F1 (Ma-F1), which have been used in many ﬁne-grained
typing systems (Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh and Sch¨utze, 2017; Ren et al., 2016a).
Parameter Settings: Our implementation of APE is based on TensorFlow3. Regarding the choice of
activation function of hidden layers, we have tried ReLU, softsign and tanh, ﬁnding softsign leads to the
best performance in general. We randomly initialize model parameters with a Gaussian distribution (with
a mean of 0.0 and standard deviation of 0.01), optimizing the model with mini-batch Adam (Kingma
and Ba, 2014). We set ﬁxed margin ξ = 1 in our experiments. Both λ1 and λ2 are chosen among
{0.2, 0.4, 0.6, 0.8}, we found λ1 = 0.8 and λ2 = 0.4 achieve best performance.
Baseline: We denote our model as APE, and compare it with three sets of methods:

• Entity typing methods: We compare APE with ﬁve state-of-the-art entity typing methods. FIG-
MENT uses entity representation learned from annotated corpus for type inference (Yaghoobzadeh
and Sch¨utze, 2015). CUTE employs a multi-label hierarchical classiﬁcation method to address en-
tity typing task (Xu et al., 2016). MuLR uses multi-level representations of entities (character, word
and entity) to make type inference in KB (Yaghoobzadeh and Sch¨utze, 2017). Global utilizes en-
tity text description to infer missing entity type instances (Neelakantan and Chang, 2015). Corpus
designs global and context model to make type inference jointly (Yaghoobzadeh et al., 2017).

• Network Embedding methods: Attributed and predictive network embedding methods can learn
informative entity representations, so we compare APE with them. PTE learns predictive text em-
beddings from heterogeneous network (Tang et al., 2015a). Planetoid is a semi-supervised network
embedding method which makes neighbor and label prediction jointly (Yang et al., 2016). ASNE
integrates structural and attribute information via a deep neural architecture (Liao et al., 2017).

• APE’s variants: If we ignore the type information, that is, utilize the embeddings learned from
section 3.2 to make type inference, the method is denoted as APEno type. Similarly, if we omit the
type hierarchy modeling in Section 3.3.2, the method is denoted as APEno hierarchy.

4.2 Overall Comparison Results

Table 2 shows the overall performance and we have the following conclusions:

1https://github.com/Tsinghua-PhD/APE
2http://wiki.dbpedia.org/downloads-2016-10
3https://www.tensorflow.org/

Table 2: Typing performance on DBpedia dataset

Type

Entity Typing

Network Embedding ASNE

Our Variants

Algorithm
FIGMENT
CUTE
MuLR
Corpus
Global
Planetoid

PTE
APEno type
APEno hierarchy
APE

Acc Ma-F1 Mi-F1
0.628
0.619
0.463
0.677
0.673
0.515
0.662
0.654
0.501
0.659
0.662
0.488
0.615
0.608
0.457
0.590
0.585
0.434
0.573
0.568
0.417
0.518
0.512
0.352
0.657
0.649
0.492
0.689
0.694
0.531
0.711
0.702
0.545

Comparison with Entity Typing methods. Our model outperforms the state-of-the-art entity typing
methods, and achieves 3.4% and 2.9% improvement in Mi-F1 and Ma-F1 respectively, because it em-
ploys multiple kinds of information for type inference. Each part can provide complementary information
about entities for type inference. Our model leverages label information (type information) in represen-
tation learning phase, equip the learned embeddings with strong predictive power in entity typing task.
Comparison with Network Embedding methods. Our model outperforms the state-of-the-art network
embedding methods (i.e., Planetoid, ASNE and PTE), and achieves 12.1% and 11.7% improvement in
Mi-F1 and Ma-F1 respectively. We model the structure of type hierarchy in a margin-based loss function.
Speciﬁcally, ancestor order makes entities closer to speciﬁc types than general types, and sibling order
makes the learned entity embeddings have strong discriminative power between sibling types.
Comparison with Variants. APE consistently outperforms APEno type and APEno hierarchy, and
the simplest version APEno type could achieve comparable results with state-of-the-art methods. The
results verify that each of these information sources contributes complementary information for ﬁne-
grained typing of entities. Structural information (link relation) provides important semantic relatedness
between entities. Attribute information (text, property and category) provides additional clues for type
inference. Type information (type hierarchy) makes our model more powerful in classiﬁcation task.

4.3 Result Analysis

4.3.1

Impact of Features

We investigate the effect of information sources used to learn entity embeddings. In Table 3, Stru, T,
P and C is short for structural information, text description, property and category, respectively. From
Table 3, we ﬁnd different information sources play different roles in this task. Structural information
is most common in KB, it makes related entities have similar representations. It models the relation
between entities, while neglecting entities’s self-information. Attribute information (i.e., text description,
property and category) can be viewed as entity features, which is often informative for type inference.
We compare our model APE, APEno type and APEno hierarchy with different kinds of information
combination. For attribute information, category information is more import than property information
and text descriptions.

4.3.2 Effects of Labeled Data

We investigate the effect of labeled data proportion. Intuitively, our model can learn better embeddings
and achieve better performance if we have more labeled entities. The results in Fig. 3 show that three
metrics strikingly increase with more labeled data added, and gradually become stable when the propor-
tion is over 0.5. It proves that our model can achieve a satisfactory result with not too much labeled data,
and this advantage beneﬁts from the entity link relations and network embedding.

Table 3: Performance on different information source combination.

Information Type

Stru+T
Stru+P
Stru+C
Stru+P+C
Stru+T+P
Stru+T+C
Stru+T+P+C

APE
Acc Ma-F1 Mi-F1
0.631
0.623
0.467
0.637
0.633
0.475
0.646
0.641
0.481
0.649
0.642
0.479
0.655
0.647
0.484
0.653
0.643
0.481
0.657
0.649
0.492

APEno type
Acc Ma-F1 Mi-F1
0.634
0.625
0.463
0.642
0.638
0.480
0.647
0.649
0.487
0.679
0.685
0.520
0.659
0.658
0.507
0.681
0.683
0.518
0.689
0.694
0.531

APEno hierarchy
Acc Ma-F1 Mi-F1
0.633
0.632
0.472
0.653
0.651
0.489
0.647
0.650
0.486
0.669
0.677
0.523
0.660
0.657
0.510
0.673
0.678
0.525
0.711
0.702
0.545

Table 4: Type macro average F1 on test for all, fre-
quent and infrequent types, #training/test means the
average entity number for each type.

Type

infrequent

frequent

all

# training

# test

FIGMENT

MuLR

Corpus

CUTE

APE

1013

417

0.327

0.374

0.447

0.418

0.461

13,513

5,217

0.620

0.718

0.783

0.730

0.792

8,673

3,462

0.557

0.636

0.675

0.652

0.681

Figure 3: Performance on different labeled data
proportion.

4.3.3 Results of Frequent/Infrequent Types

We use type macro average F1 proposed by (Yosef et al., 2012) to evaluate the performance on frequent
types and infrequent types. Note that it is different from Ma-F1 reported in Table 2. The results for
infrequent types (occur less than 2,000 times) and frequent types (occur more than 10,000 times) are
shown in Table 4. Generally, the performance on infrequent types is worse than frequent ones. Our
model consistently outperforms the other methods on infrequent types, which demonstrates its ability on
dealing with rare types.

5 Related Work

Fine-grained entity typing in KB is an important sub-task of knowledge base completion. One way to
address this problem is making type inference based on entity’s attribute information in KB, such as
textual description, property, category and so on. (Neelakantan and Chang, 2015) infer missing types for
entities in KB based on text descriptions. (Xu et al., 2016) employ a multi-label hierarchical classiﬁca-
tion method to assign Chinese entities with DBpedia types based on property and category information.
The other way relies on large-scale annotated corpus, then extracts type information from the annotated
corpus for entities in KB. (Yaghoobzadeh and Sch¨utze, 2015) ﬁrst propose FIGMENT to address this
problem. They only used contextual information to assign types for entities in KB. After that, they
present FIGMENT-Multi to learn multi-level representations of entities on three complementary levels
(character, word and entity). FIGMENT-Multi predicts whether an entity is a member of a type based
on the learned embeddings (Yaghoobzadeh and Sch¨utze, 2017). Finally, they propose an embedding
based method which combines a global model with a context model. A global model that scores based
on aggregated context information and a context model that aggregates the scores of individual con-
texts (Yaghoobzadeh et al., 2017).

Fine-grained entity typing in text is related to our task. Different from the works mentioned above, it

seeks to detect local types of an entity mention inside an individual sentence, and the same entity could
have different types in different sentences (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014;
Dong et al., 2015; Del Corro et al., 2015; Yogatama et al., 2015; Ren et al., 2016b; Ren et al., 2016a;
Shimaoka et al., 2017; Rabinovich and Dan, 2017).

6 Conclusion

In this paper, we propose an attributed and predictive entity embedding method to address the task of
ﬁne-grained entity typing in KB. It combines the advantages of attributed network embedding method
and semi-supervised learning. Speciﬁcally, it employs a deep neural network architecture to integrate
structure and attribute information of an entity. We jointly predict a labeled entity’s neighbors and types,
and model the structure of type hierarchy in a margin-based way. Experiments on real world datasets
demonstrate the effectiveness of the proposed model.

The work is
supported by National Key Research and Development Program of China
(2017YFB1002101), NSFC key project (U1736204), Ministry of Education and China Mobile Research
Fund (No. 20181770250), and THUNUS NExT Co-Lab.

Acknowledgements

References

Luciano Del Corro, Abdalghani Abujabal, Rainer Gemulla, and Gerhard Weikum. 2015. Finet: Context-aware
ﬁne-grained named entity typing. In Conference on Empirical Methods in Natural Language Processing, pages
868–878.

Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua
Sun, and Wei Zhang. 2014. Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In
Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,
pages 601–610. ACM.

Li Dong, Furu Wei, Hong Sun, Ming Zhou, and Ke Xu. 2015. A hybrid neural model for type classiﬁcation of

entity mentions. In International Conference on Artiﬁcial Intelligence, pages 1243–1249.

Greg Durrett and Dan Klein. 2014. A joint model for entity analysis : Coreference , typing , and linking. TACL,

pages 477–490.

arXiv:1412.6980.

Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse Kirchner, and David Huynh. 2014. Context-dependent ﬁne-

grained entity type tagging. arXiv:1412.1820.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint

Hang Li, Haozheng Wang, Zhenglu Yang, and Masato Odagaki. 2017. Variation autoencoder based network

representation learning for classiﬁcation. In ACL 2017, Student Research Workshop, pages 56–61.

Lizi Liao, Xiangnan He, Hanwang Zhang, and Tat Seng Chua. 2017. Attributed social network embedding.

arXiv:1705.04969.

Xiao Ling and Daniel S Weld. 2012. Fine-grained entity recognition. In AAAI, pages 94–100.

Xiao Ling, Sameer Singh, and Daniel S Weld. 2015. Design challenges for entity linking. TACL, pages 315–328.

Yang Liu, Kang Liu, Liheng Xu, and Jun Zhao. 2014. Exploring ﬁne-grained entity type constraints for distantly

supervised relation extraction. In COLING, pages 2107–2116.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of
words and phrases and their compositionality. In International Conference on Neural Information Processing
Systems, pages 3111–3119.

Gregory Murphy. 2004. The big book of concepts. MIT press.

David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classiﬁcation. Lingvisticae

Investigationes, 30(1):3–26.

Arvind Neelakantan and Ming Wei Chang. 2015.

Inferring missing entity type instances for knowledge base

completion: New dataset and methods. Computer Science, pages 35–40.

Maxim Rabinovich and Klein Dan. 2017. Fine-grained entity typing with high-multiplicity assignments.

In

Meeting of the Association for Computational Linguistics, pages 330–334.

Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceed-
ings of the Thirteenth Conference on Computational Natural Language Learning, pages 147–155. Association
for Computational Linguistics.

Xiang Ren, Wenqi He, Meng Qu, Lifu Huang, Heng Ji, and Jiawei Han. 2016a. Afet: Automatic ﬁne-grained
entity typing by hierarchical partial-label embedding. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1369–1378.

Xiang Ren, Wenqi He, Meng Qu, Clare R Voss, Heng Ji, and Jiawei Han. 2016b. Label noise reduction in entity
In Proceedings of the 22nd ACM SIGKDD International

typing by heterogeneous partial-label embedding.
Conference on Knowledge Discovery and Data Mining, pages 1825–1834.

Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and Sebastian Riedel. 2017. Neural architectures for ﬁne-

grained entity type classiﬁcation. In EACL, pages 1271–1280.

Jian Tang, Meng Qu, and Qiaozhu Mei. 2015a. Pte: Predictive text embedding through large-scale heterogeneous
text networks. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 1165–1174. ACM.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015b. Line: Large-scale infor-
mation network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages
1067–1077. ACM.

Ji Xin, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2018.

Improving neural ﬁne-grained entity typing with

knowledge attention. In AAAI.

Bo Xu, Yi Zhang, Jiaqing Liang, Yanghua Xiao, Seung-won Hwang, and Wei Wang. 2016. Cross-lingual type

inference. In International Conference on Database Systems for Advanced Applications, pages 447–462.

Yadollah Yaghoobzadeh and Hinrich Sch¨utze. 2015. Corpus-level ﬁne-grained entity typing using contextual
information. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,
pages 715–725.

Yadollah Yaghoobzadeh and Hinrich Sch¨utze. 2017. Multi-level representations for ﬁne-grained typing of knowl-

edge base entities. arXiv:1701.02025.

Yadollah Yaghoobzadeh, Heike Adel, and Hinrich Sch¨utze. 2017. Corpus-level ﬁne-grained entity typing.

arXiv:1708.02275.

Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, and Gerhard Weikum. 2014. Robust question answering

over the web of linked data. In CIKM, pages 1107–1116.

Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. 2016. Revisiting semi-supervised learning with graph

embeddings. In ICML, pages 40–48.

Dani Yogatama, Dan Gillick, and Nevena Lazic. 2015. Embedding methods for ﬁne grained entity type classiﬁca-
tion. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language
Processing, ACL, pages 26–31.

M Amir Yosef, Sandro Bauer, Johannes Hoffart, Marc Spaniol, and Gerhard Weikum. 2012. Hyena: Hierarchical
In Proceedings of the 24th International Conference on Computational

type classiﬁcation for entity names.
Linguistics, pages 1361–1370.

Attributed and Predictive Entity Embedding for Fine-Grained
Entity Typing in Knowledge Bases

Hailong Jin and Lei Hou and Juanzi Li
DCST, Tsinghua University
Beijing 100084, China

Tiansi Dong
B-IT, Bonn University
53113 Bonn, Germany

{jinhl15@mails.,houlei@mail.,lijuanzi@}tsinghua.edu.cn
dongt@bit.uni-bonn.de

Abstract

Fine-grained entity typing aims at identifying the semantic type of an entity in KB. Type in-
formation is very important in knowledge bases, but is unfortunately incomplete even in some
large knowledge bases. Limitations of existing methods are either ignoring the structure and
type information in KB or requiring large scale annotated corpus. To address these issues, we
propose an attributed and predictive entity embedding method, which can fully utilize various
kinds of information comprehensively. Extensive experiments on real DBpedia dataset show that
our proposed method signiﬁcantly outperforms 8 state-of-the-art methods, with 3.4% and 2.9%
improvement in Mi-F1 and Ma-F1, respectively.

1

Introduction

Knowledge about entities in Knowledge Base (KB) is essential for language understanding. This knowl-
edge can be attributional (e.g., canFly, hasAge), relational (e.g., marriedTo, bornIn) or type-based (e.g.,
isFood, isLocation). Type information, which clusters a group of entities with same properties, is valu-
able in KB, because it is the glue that holds our mental world together (Murphy, 2004). Types in KB
are usually organized as a hierarchical structure, namely type hierarchy. Entity typing assigns types
(e.g., Person, Athlete, BasketballPlayer) to an entity (e.g., Yao Ming) in KB, and is a fundamental task
in knowledge base construction.

Traditional entity typing focuses on a small set of types, such as Person, Location and Or-
ganization (Ratinov and Roth, 2009; Nadeau and Sekine, 2007), while ﬁne-grained entity typ-
ing assigns more speciﬁc types to an entity, which normally forms a type-path in the type hier-
archy in KB (Ren et al., 2016a). As shown in Fig. 1, Yao Ming is associated with a type-path
/Thing/Agent/Person/Athlete/BasketballPlayer. Fine-grained types (e.g., Athlete and BasketballPlayer)
are more informative than coarse-grained types (e.g., Person), because they provide more speciﬁc se-
mantic information about the entity (Xu et al., 2016). Characterizing an entity with ﬁne-grained types
(type-paths) beneﬁts many real applications, such as knowledge base completion (Dong et al., 2014),
entity linking (Ling et al., 2015; Durrett and Klein, 2014), relation extraction (Liu et al., 2014), and
question answering (Yahya et al., 2014).

Many researches have been carried out on ﬁne-grained entity typing in text, which detects local types
for an entity mention in a particular plain text from predeﬁned ﬁne-grained entity types (Ling and Weld,
2012; Yogatama et al., 2015; Ren et al., 2016a; Shimaoka et al., 2017; Xin et al., 2018). However, only
a few works aim at ﬁne-grained entity typing in knowledge base, that is, inferring missing types for an
entity in KB, which is the focus of this paper. Most KBs are incomplete and lack of type information, in
part because the world is always changing. For example, 36.53% entities in DBpedia do not have type
information. Therefore, KBs are aways in need of updating and completing. To this end, there are two
main methods in the literature as follow: One method exploits various kinds of information to construct
the feature representation of an entity, such as entity textual description, property and category (Nee-
lakantan and Chang, 2015; Xu et al., 2016). After that, a predict function P (t|e) is learned to infer

This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/
Corresponding author: Lei Hou (houlei@mail.tsinghua.edu.cn)

Figure 1: An example of ﬁne-grained entity typing in KB. The left part is an entity with various infor-
mation. The red ones are assigned types, which forms a type-path.

whether entity e is an instance of type t. This method ignores rich structural information in KB (i.e., link
relation between entities). It is a promising and challenging problem to utilize structural information for
type inference. The second method utilizes annotated corpus to learn entity low-dimensional represen-
tations then makes type inference based on the learned embeddings (Yaghoobzadeh and Sch¨utze, 2015;
Yaghoobzadeh and Sch¨utze, 2017). Apparently, it requires large scale corpus in which entity mentions
are annotated. But large scale highly-qualiﬁed annotated corpus is often difﬁcult to obtain.

To address the above issues, we propose a novel attributed and predictive entity embedding method,
which can be described as follow: We ﬁrst construct a partially-labeled attributed entity network, which
contains structural, attribute and type information for entities. Here “partially-labeled” means some
entities have been labeled with type information, while others not. Then, we employ a deep neural
network to integrate the structural and attribute information for entity representation learning. For an
unlabeled entity, we jointly utilize its structural and attribute information for neighbor prediction. For
a labeled entity, besides neighbors, we also predict its types and type hierarchy via a margin-based loss
function. Finally, a multi-label classiﬁcation model is used for type inference based on the learned entity
embeddings.

The main contributions of this paper can be summarized as follows:

• We encode various kinds of information into a partially-labeled entity network and propose a deep
neural network model to effectively integrate the structural and attribute information for entity rep-
resentation learning.

• We utilize type information of labeled entities to equip the learned embeddings with strong pre-
dictive power in entity typing task, and further incorporate the structure of type hierarchy via a
margin-based loss function to meet the ﬁne-grained demand.

• We construct a dataset from DBpedia. Extensive experiments show that our proposed method sig-
niﬁcantly outperforms 8 state-of-the-art methods, with 3.4% and 2.9% improvement in Mi-F1 and
Ma-F1, respectively.

The rest of this paper is organized as follows. In Section 2, we formally deﬁne the problem of ﬁne-
grained entity typing in knowledge bases. Section 3 demonstrates the proposed approach in detail. The
dataset description and evaluation are presented in Section 4. Section 5 outlines some related works.
Section 6 concludes our work.

2 Problem Formulation

In this section, we formally deﬁne the problem of Fine-Grained Entity Typing in KB.

Deﬁnition 1 Knowledge Base is deﬁned a tuple of entities E and a type hierarchy H, formally, KB =
(cid:104)E, H(cid:105), where H is used to organize entities in E. Each entity e ∈ E denotes a speciﬁc thing in the real
world.

Deﬁnition 2 Type Hierarchy H is deﬁned as a tree or a directed acyclic graph (DAG). It provides a
natural way to categorize and organize entities in large KB. It is formalized (T , R), where T is the type
set and R = {(ti, tj)|ti, tj ∈ T ∧ i (cid:54)= j} is the relation set, in which (ti, tj) means that ti is the subtype
of tj.

We distinguish three kinds of entity knowledge which are beneﬁcial to ﬁne-grained entity typing task.

• Entity Link Relation: KB contains a large number of link relations between entities. It is natural to
convert entities in E into a network structure via these link relations. If an entity ei links to another
entity ej in KB, there will be a directed edge from entity ei to entity ej. These link relations reﬂect
the overall structure of KB and provide important semantic relatedness between entities.

• Entity Attribute: Besides the above link structure, each entity e ∈ E also has textual description,
properties and categories (shown in Fig. 1). We term all such auxiliary information of an entity as
attributes. The attributes of an entity provide valuable clues to predict its types.

• Entity Type: Part of entities in E have been assigned with types, we refer them as E l. The assigned
types for an entity form a type-path in type hierarchy H, which starts from the root and ends at one
speciﬁc type (not necessary to end at the leaf type). Those entities in E without type information
are referred as E u. The type information observed in E l can be a useful signal to infer missing type
information in E u.

From the above deﬁnitions, we achieve a partially-labeled attributed entity network, including link

structure, entity attributes, and type information. We formally deﬁne this entity network as follow:
Deﬁnition 3 Entity Network: Let G = (E, L) be an entity network. E = E l ∪ E u is the entity set, where
|E| = N and |E l| = N l. L = {(ei, ej) |ei, ej ∈ E and i (cid:54)= j} is the link relation set.

For the convenience of formula deduction, we redeﬁne this entity network in matrix form, G =
(A, X, Y). Adjacency matrix A ∈ RN ×N is equivalent to link set L, Aij = 1 if there is a link from
ei to ej, otherwise 0. Each row in A refers to an entity’s link vector Ai. Attribute matrix X ∈ RN ×M
collects attributes for all entities, where M is the number of attributes. Each row in X refers to an entity’s
attribute vector Xi. X can either be binary or take any real value. Type matrix Y ∈ RN l×K collects type
information for entities in E l, where K is the number of types. Each row in Y refers to the type vector
Yi of a labeled entity in E l. Yij = 1 if ei has been assigned with type tj, otherwise 0.

Based on the terminologies described above, we formally deﬁne the problem of ﬁne-grained entity

typing in KB as follows:

Deﬁnition 4 Fine-grained entity typing in KB: Given a partially-labeled attributed entity network G =
(E, L), we aim at learning a type predictor from E l, which comprehensively takes link structure, entity
attributes and type information into account. Then we utilize the learned type predictor to predict the
type of e ∈ E u, i.e., infer whether entity e ∈ E u is an instance of type t ∈ T .

3 The Proposed Approach

3.1 General Architecture

A general state-of-the-art architecture for ﬁne-grained entity typing consists of two components.

• Entity Representation: It learns distributed representation (cid:126)v (e) for each entity e ∈ E.

• Type Predictor: It learns a predict function P (t|e) using E l as training examples. P (t|e) denotes
the probability that entity e belongs to type t, and it is often used to infer types of those unlabeled
entities in E u.

Type predictor is regarded as a multi-label classiﬁcation model. P (t|e) is calculated through a two-

layer Multi-Layer Perceptron (MLP).

[P (t1|e) · · · P (tK|e)] = σ (Woutf (Win(cid:126)v (e)))

(1)

Each entry of output layer indicates the predicted probability of type ti for a given entity e. Win and
f is the nonlinear activation function,
Wout are MLP parameter matrices. σ is the sigmoid function.
such as ReLU and tanh.

The loss L for a prediction y = [P (t1|e) · · · P (tK|e)] when the true labels are encoded in a binary

vector y∗ ∈ {0, 1}1×K is the following cross entropy loss function:

L (y, y∗) =

−y∗(i) log y(i) −

(cid:16)

1 − y∗(i)(cid:17)

log

(cid:16)

1 − y(i)(cid:17)

(2)

K
(cid:88)

i=1

The key difﬁculty in computing P (t|e) is to learn a good representation (cid:126)v (e) for entity e. Several
models have achieved the state-of-the-art performance. However, these models still face the following
non-trivial challenges:

• Entity attributes and link structure are typically regraded as separated features without considering
their correlations. Recent research shows the importance of integrating link structure and entity
attributes for learning more informative representations (Liao et al., 2017; Li et al., 2017). Could
we seamlessly integrate entity attributes and link structure into a uniﬁed entity network? Since the
link structure and entity attributes offer different sources of information, how shall we capture the
complex non-linear relationship between them?

• The type information of the labeled entities in E l is only used in the learning phase of the type pre-
dictor, but totally ignored in the representation learning phase. Recent research shows incorporating
labeled information in representation learning could enhance the predictive power of the learning
embeddings for speciﬁc tasks (Tang et al., 2015a; Yang et al., 2016). In our task, the entity types
and their formed type hierarchy H are valuable labeled information, could we design a predictive
model to make full use of the type information in E l and the structure of type hierarchy?

To address the above challenges, we propose a novel attributed and predictive entity embedding
method to learn entity representations.
It combines the advantages of attributed network embedding
method and semi-supervised learning. We integrate entity attributes and link structure via a deep neural
network model (Section 3.2). For an entity ei in E l, we jointly predict its type and its neighbor entities
in the entity network (Section 3.3). In this way, we can fully utilize the type information in E l.

3.2 Attributed Entity Embedding

We aim at learning low-dimensional vector representations for each entity in the entity network, such
that original link structure and entity attribute proximity can be preserved in vectors. The most intuitive
idea is to directly concatenate the embeddings learned from different parts. However, this way can
not capture the complex non-linear relationship between them, because each part is trained separately
without interaction. To address this issue, we present a neural network model to integrate link structure
(structural information) and entity attributes (attribute information). For an entity ei ∈ E, we jointly
utilize its link vector Ai and attribute vector Xi to predict the link probability between ei and other
entities (shown in Fig. 2). Our model integrates structure and attribute information on the input layer, so
that these two parts closely interact with each other and all parameters can be optimized simultaneously.
In our design, each entity e has two vector representations, self representation and context representa-
tion. e is self representation of e when it is treated as an entity in entity network G. While ˜e is context
representation of e when it is treated as a speciﬁc context. To comprehensively represent an entity e, we
add e and ˜e as the ﬁnal representation (cid:126)v (e).

Input and Embeddings. For an entity ei ∈ E, Ai and Xi denote its link vector and attribute vector
as deﬁned in Section 2. Our model takes both structural information (Ai) and attribute information (Xi)
as input, and utilizes two embedding weight matrices to achieve the ﬁnal input embeddings. Wstru
projects the link vector Ai to a dense vector ustru which captures structural information. Wattr encodes
the attribute vector Xi and generates a compact vector uattr which aggregates attribute information. We
concatenate ustru and uattr, then feed this concatenated vector into a Multi-Layer Perceptron.

Information Fusion. We employ a Multi-Layer Perceptron to integrate structural and attribute infor-
mation. Recent research shows stacked multiple non-linear layers can help learn better representations

Figure 2: Attributed entity embedding framework

of data (Liao et al., 2017). Each layer is calculated as follows:

h(1) = f

h(k) = f

(cid:16)

(cid:16)

W(1) [ustru, λuattr] + b(1)(cid:17)
W(k)h(k−1) + b(k)(cid:17)

, k = 1, 2, · · · , n

λ is the trade-off factor for structure and attribute information. f denotes the activation function. n is
the number of hidden layers. From the last hidden layer, we obtain an informative representation h(n),
which is regarded as the self representation ei of the input entity ei.

Neighbor Prediction. Finally, we predict neighbors for an input entity ei (if there is a link from
ei to ej, ej is ei’s neighbor). We call it neighbor prediction, which is widely used to model network
structure (Tang et al., 2015a; Liao et al., 2017). Speciﬁcally, we utilize the self representation ei to
predict the link probability between entity ei and other entities. The predictive link probability is deﬁned
through a softmax layer as follows:

p (ej|ei) =

exp (˜ej · ei)
j(cid:48)=1 exp (cid:0)˜ej(cid:48) · ei

(cid:1)

(cid:80)N

˜ej denotes the context representation of entity ej. Let yi denotes the predicted link probability vector for
entity ei, y(j)
is the j-th entry of the vector yi. Ai is ei’s link vector (in Section 2), Aij indicates whether
there is a link from ei to ej. The loss function Lattr is deﬁned as cross entropy over all entities in E

i

Lattr =

N
(cid:88)

N
(cid:88)

i=1

j=1

−Aij log y(j)

i − (1 − Aij) log

1 − y(j)

i

(cid:16)

(cid:17)

3.3 Predictive Entity Embedding

In this section, we propose predictive entity embedding learning that makes full use of the type infor-
mation in E l, which is often ignored by existing methods in representation learning phase. Inspired by
(Tang et al., 2015a; Yang et al., 2016), we ﬁrst introduce a type prediction loss to equip learned embed-
dings with strong predictive power for entity typing task (Section 3.3.1). Then, to meet the ﬁne-grained
demand, we model the tree-structured type hierarchy in a margin-based framework (Section 3.3.2).

3.3.1 Type Prediction
We fully utilize type information in E l to make the learned embeddings more predictive. For each entity
ei ∈ E l, we not only predict its neighbors in G (Section 3.2), but also predict its types. We add another
softmax layer, the probability of entity ei being an instance of type tj is deﬁned as follows:

p (tj|ei) =

exp (tj · ei)
j(cid:48)=1 exp (cid:0)tj(cid:48) · ei

(cid:1)

(cid:80)K

(3)

(4)

(5)

(6)

(7)

tj denotes the representation of type tj. Let yi denotes the predicted type probability vector for entity
ei, y(j)
is the j-th entry of the vector yi. Yi is ei’s type vector (in Section 2), Yij indicates whether ei is
i
an instance of tj. The loss function Ltype is deﬁned as cross entropy over all entities in E l:

Ltype =

N l
(cid:88)

K
(cid:88)

i=1

j=1

−Yij log y(j)

i − (1 − Yij) log

1 − y(j)

i

(cid:16)

(cid:17)

(8)

3.3.2 Type Hierarchy Modeling
Fine-grained entity typing requires us to pay more attention to speciﬁc types (ﬁne-grained types) than
to general types (coarse-grained types), because speciﬁc types are more informative. For example, Yao
Ming belongs to BasketballPlayer and Athlete, and it should get higher score on BasketballPlayer than
Athlete, because the former characterizes Yao Ming more accurately. Besides, Yao Ming should get higher
score on the correct type BasketballPlayer than its wrong sibling type SoccerPlayer although both types
belong to Athlete. If different types are treated equally, the structural information between types will be
lost. Therefore, we use type order to model different relatedness between an entity and its related types,
which reﬂects the structural information of type hierarchy to a certain degree. In particular, we deﬁne
two kinds of type order as follow:

Deﬁnition 5 Ancestor order: In the type-path of an entity, speciﬁc types should get higher score than its
ancestor. For example, Yao Ming should get higher score on BasketballPlayer than Athlete.

Ancestor order reﬂects the relationship between a type and its ancestor types. For each triple (e, t, ta),
where t is one of the type for entity e, and ta is t’s ancestor in type hierarchy. We deﬁne adaptive margin
γtta = ξ × l(ta,t)
l(root,t) , where l(ta, t) denotes the number of steps going down from type ta to type t. The
hinge loss is deﬁned as:

max (0, s (e, ta) − s (e, t) + γtta)

Lancestor = E(e,t,ta) [max (0, s (e, ta) − s (e, t) + γtta)]

where s (e, t) is the score function, deﬁned as the inner product of e and t.

Deﬁnition 6 Sibling order: For an entity, the correct type should get higher score than its wrong sib-
ling type in type hierarchy. For example, Yao Ming should get higher score on BasketballPlayer than
SoccerPlayer.

Sibling order reﬂects the relationship between a type and its sibling types. For each triple (e, t, ts),
where t is one of the true type for entity e, and ts is its wrong sibling type of entity e. We use the ﬁxed
margin ξ. The hinge loss is deﬁned as:

max (0, s (e, ts) − s (e, t) + ξ)

Lsibling = E(e,t,ts) [max (0, s (e, ts) − s (e, t) + ξ)]

We achieve the loss function Lhier for type hierarchy modeling: Lhier = Lancestor + Lslibing.

Objective and Training. Finally, we sum these three loss function to get the ﬁnal objective function of
entity representation component.

Loss = Lattr + λ1 · Ltype + λ2 · Lhier

λ1 and λ2 are weight parameters. We apply negative sampling (Mikolov et al., 2013; Tang et al., 2015b),
where only a very small subset of entities are sampled from E. Parameters are optimized through Adap-
tive Moment Estimation (Adam). In the embedding layer and each hidden layer, we also add dropout
component to alleviate over ﬁtting.

(9)

(10)

(11)

(12)

(13)

4 Experiments and Analysis

In this section, we evaluate the proposed method using real world dataset collected from DBpedia. We
will introduce the dataset and experiment settings in Section 4.1, present the comparison results in Sec-
tion 4.2 and investigate some method details in Section 4.3. Our source code is available1 for reference.

4.1 Datasets and Experimental Setup

Datasets: To the best of our knowledge, there is no public large-scale dataset available for ﬁne-grained
entity typing in KB. We construct a dataset from DBpedia. The type hierarchy in DBpedia is of tree
structure. We extract link relation and attribute information from DBpedia2. For each entity, we use the
single type-path assigned in DBpedia as the ground truth.

For this dataset, we remove those entities which are only labeled as Thing. Because they do not provide
any semantic information. In representation learning phase, we utilize the whole dataset with only 50%
entities are labeled with types. In type predictor phase, we follow a 50/30/20 ratio to split the dataset into
training, dev and test sets. Table 1 shows the detailed statistics. Each entity has 3.27 types on average.

Table 1: Statistics of the dataset

Type

Entity

Link

214

300,000

5,243,230

Attribute

word
5,350

property
200

category
350

Metrics: To evaluate the performance of our proposed method, we use Accuracy (Strict-F1), Micro-
averaged F1 (Mi-F1) and Macro-averaged F1 (Ma-F1), which have been used in many ﬁne-grained
typing systems (Yaghoobzadeh and Sch¨utze, 2015; Yaghoobzadeh and Sch¨utze, 2017; Ren et al., 2016a).
Parameter Settings: Our implementation of APE is based on TensorFlow3. Regarding the choice of
activation function of hidden layers, we have tried ReLU, softsign and tanh, ﬁnding softsign leads to the
best performance in general. We randomly initialize model parameters with a Gaussian distribution (with
a mean of 0.0 and standard deviation of 0.01), optimizing the model with mini-batch Adam (Kingma
and Ba, 2014). We set ﬁxed margin ξ = 1 in our experiments. Both λ1 and λ2 are chosen among
{0.2, 0.4, 0.6, 0.8}, we found λ1 = 0.8 and λ2 = 0.4 achieve best performance.
Baseline: We denote our model as APE, and compare it with three sets of methods:

• Entity typing methods: We compare APE with ﬁve state-of-the-art entity typing methods. FIG-
MENT uses entity representation learned from annotated corpus for type inference (Yaghoobzadeh
and Sch¨utze, 2015). CUTE employs a multi-label hierarchical classiﬁcation method to address en-
tity typing task (Xu et al., 2016). MuLR uses multi-level representations of entities (character, word
and entity) to make type inference in KB (Yaghoobzadeh and Sch¨utze, 2017). Global utilizes en-
tity text description to infer missing entity type instances (Neelakantan and Chang, 2015). Corpus
designs global and context model to make type inference jointly (Yaghoobzadeh et al., 2017).

• Network Embedding methods: Attributed and predictive network embedding methods can learn
informative entity representations, so we compare APE with them. PTE learns predictive text em-
beddings from heterogeneous network (Tang et al., 2015a). Planetoid is a semi-supervised network
embedding method which makes neighbor and label prediction jointly (Yang et al., 2016). ASNE
integrates structural and attribute information via a deep neural architecture (Liao et al., 2017).

• APE’s variants: If we ignore the type information, that is, utilize the embeddings learned from
section 3.2 to make type inference, the method is denoted as APEno type. Similarly, if we omit the
type hierarchy modeling in Section 3.3.2, the method is denoted as APEno hierarchy.

4.2 Overall Comparison Results

Table 2 shows the overall performance and we have the following conclusions:

1https://github.com/Tsinghua-PhD/APE
2http://wiki.dbpedia.org/downloads-2016-10
3https://www.tensorflow.org/

Table 2: Typing performance on DBpedia dataset

Type

Entity Typing

Network Embedding ASNE

Our Variants

Algorithm
FIGMENT
CUTE
MuLR
Corpus
Global
Planetoid

PTE
APEno type
APEno hierarchy
APE

Acc Ma-F1 Mi-F1
0.628
0.619
0.463
0.677
0.673
0.515
0.662
0.654
0.501
0.659
0.662
0.488
0.615
0.608
0.457
0.590
0.585
0.434
0.573
0.568
0.417
0.518
0.512
0.352
0.657
0.649
0.492
0.689
0.694
0.531
0.711
0.702
0.545

Comparison with Entity Typing methods. Our model outperforms the state-of-the-art entity typing
methods, and achieves 3.4% and 2.9% improvement in Mi-F1 and Ma-F1 respectively, because it em-
ploys multiple kinds of information for type inference. Each part can provide complementary information
about entities for type inference. Our model leverages label information (type information) in represen-
tation learning phase, equip the learned embeddings with strong predictive power in entity typing task.
Comparison with Network Embedding methods. Our model outperforms the state-of-the-art network
embedding methods (i.e., Planetoid, ASNE and PTE), and achieves 12.1% and 11.7% improvement in
Mi-F1 and Ma-F1 respectively. We model the structure of type hierarchy in a margin-based loss function.
Speciﬁcally, ancestor order makes entities closer to speciﬁc types than general types, and sibling order
makes the learned entity embeddings have strong discriminative power between sibling types.
Comparison with Variants. APE consistently outperforms APEno type and APEno hierarchy, and
the simplest version APEno type could achieve comparable results with state-of-the-art methods. The
results verify that each of these information sources contributes complementary information for ﬁne-
grained typing of entities. Structural information (link relation) provides important semantic relatedness
between entities. Attribute information (text, property and category) provides additional clues for type
inference. Type information (type hierarchy) makes our model more powerful in classiﬁcation task.

4.3 Result Analysis

4.3.1

Impact of Features

We investigate the effect of information sources used to learn entity embeddings. In Table 3, Stru, T,
P and C is short for structural information, text description, property and category, respectively. From
Table 3, we ﬁnd different information sources play different roles in this task. Structural information
is most common in KB, it makes related entities have similar representations. It models the relation
between entities, while neglecting entities’s self-information. Attribute information (i.e., text description,
property and category) can be viewed as entity features, which is often informative for type inference.
We compare our model APE, APEno type and APEno hierarchy with different kinds of information
combination. For attribute information, category information is more import than property information
and text descriptions.

4.3.2 Effects of Labeled Data

We investigate the effect of labeled data proportion. Intuitively, our model can learn better embeddings
and achieve better performance if we have more labeled entities. The results in Fig. 3 show that three
metrics strikingly increase with more labeled data added, and gradually become stable when the propor-
tion is over 0.5. It proves that our model can achieve a satisfactory result with not too much labeled data,
and this advantage beneﬁts from the entity link relations and network embedding.

Table 3: Performance on different information source combination.

Information Type

Stru+T
Stru+P
Stru+C
Stru+P+C
Stru+T+P
Stru+T+C
Stru+T+P+C

APE
Acc Ma-F1 Mi-F1
0.631
0.623
0.467
0.637
0.633
0.475
0.646
0.641
0.481
0.649
0.642
0.479
0.655
0.647
0.484
0.653
0.643
0.481
0.657
0.649
0.492

APEno type
Acc Ma-F1 Mi-F1
0.634
0.625
0.463
0.642
0.638
0.480
0.647
0.649
0.487
0.679
0.685
0.520
0.659
0.658
0.507
0.681
0.683
0.518
0.689
0.694
0.531

APEno hierarchy
Acc Ma-F1 Mi-F1
0.633
0.632
0.472
0.653
0.651
0.489
0.647
0.650
0.486
0.669
0.677
0.523
0.660
0.657
0.510
0.673
0.678
0.525
0.711
0.702
0.545

Table 4: Type macro average F1 on test for all, fre-
quent and infrequent types, #training/test means the
average entity number for each type.

Type

infrequent

frequent

all

# training

# test

FIGMENT

MuLR

Corpus

CUTE

APE

1013

417

0.327

0.374

0.447

0.418

0.461

13,513

5,217

0.620

0.718

0.783

0.730

0.792

8,673

3,462

0.557

0.636

0.675

0.652

0.681

Figure 3: Performance on different labeled data
proportion.

4.3.3 Results of Frequent/Infrequent Types

We use type macro average F1 proposed by (Yosef et al., 2012) to evaluate the performance on frequent
types and infrequent types. Note that it is different from Ma-F1 reported in Table 2. The results for
infrequent types (occur less than 2,000 times) and frequent types (occur more than 10,000 times) are
shown in Table 4. Generally, the performance on infrequent types is worse than frequent ones. Our
model consistently outperforms the other methods on infrequent types, which demonstrates its ability on
dealing with rare types.

5 Related Work

Fine-grained entity typing in KB is an important sub-task of knowledge base completion. One way to
address this problem is making type inference based on entity’s attribute information in KB, such as
textual description, property, category and so on. (Neelakantan and Chang, 2015) infer missing types for
entities in KB based on text descriptions. (Xu et al., 2016) employ a multi-label hierarchical classiﬁca-
tion method to assign Chinese entities with DBpedia types based on property and category information.
The other way relies on large-scale annotated corpus, then extracts type information from the annotated
corpus for entities in KB. (Yaghoobzadeh and Sch¨utze, 2015) ﬁrst propose FIGMENT to address this
problem. They only used contextual information to assign types for entities in KB. After that, they
present FIGMENT-Multi to learn multi-level representations of entities on three complementary levels
(character, word and entity). FIGMENT-Multi predicts whether an entity is a member of a type based
on the learned embeddings (Yaghoobzadeh and Sch¨utze, 2017). Finally, they propose an embedding
based method which combines a global model with a context model. A global model that scores based
on aggregated context information and a context model that aggregates the scores of individual con-
texts (Yaghoobzadeh et al., 2017).

Fine-grained entity typing in text is related to our task. Different from the works mentioned above, it

seeks to detect local types of an entity mention inside an individual sentence, and the same entity could
have different types in different sentences (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014;
Dong et al., 2015; Del Corro et al., 2015; Yogatama et al., 2015; Ren et al., 2016b; Ren et al., 2016a;
Shimaoka et al., 2017; Rabinovich and Dan, 2017).

6 Conclusion

In this paper, we propose an attributed and predictive entity embedding method to address the task of
ﬁne-grained entity typing in KB. It combines the advantages of attributed network embedding method
and semi-supervised learning. Speciﬁcally, it employs a deep neural network architecture to integrate
structure and attribute information of an entity. We jointly predict a labeled entity’s neighbors and types,
and model the structure of type hierarchy in a margin-based way. Experiments on real world datasets
demonstrate the effectiveness of the proposed model.

The work is
supported by National Key Research and Development Program of China
(2017YFB1002101), NSFC key project (U1736204), Ministry of Education and China Mobile Research
Fund (No. 20181770250), and THUNUS NExT Co-Lab.

Acknowledgements

References

Luciano Del Corro, Abdalghani Abujabal, Rainer Gemulla, and Gerhard Weikum. 2015. Finet: Context-aware
ﬁne-grained named entity typing. In Conference on Empirical Methods in Natural Language Processing, pages
868–878.

Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua
Sun, and Wei Zhang. 2014. Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In
Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,
pages 601–610. ACM.

Li Dong, Furu Wei, Hong Sun, Ming Zhou, and Ke Xu. 2015. A hybrid neural model for type classiﬁcation of

entity mentions. In International Conference on Artiﬁcial Intelligence, pages 1243–1249.

Greg Durrett and Dan Klein. 2014. A joint model for entity analysis : Coreference , typing , and linking. TACL,

pages 477–490.

arXiv:1412.6980.

Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse Kirchner, and David Huynh. 2014. Context-dependent ﬁne-

grained entity type tagging. arXiv:1412.1820.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint

Hang Li, Haozheng Wang, Zhenglu Yang, and Masato Odagaki. 2017. Variation autoencoder based network

representation learning for classiﬁcation. In ACL 2017, Student Research Workshop, pages 56–61.

Lizi Liao, Xiangnan He, Hanwang Zhang, and Tat Seng Chua. 2017. Attributed social network embedding.

arXiv:1705.04969.

Xiao Ling and Daniel S Weld. 2012. Fine-grained entity recognition. In AAAI, pages 94–100.

Xiao Ling, Sameer Singh, and Daniel S Weld. 2015. Design challenges for entity linking. TACL, pages 315–328.

Yang Liu, Kang Liu, Liheng Xu, and Jun Zhao. 2014. Exploring ﬁne-grained entity type constraints for distantly

supervised relation extraction. In COLING, pages 2107–2116.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of
words and phrases and their compositionality. In International Conference on Neural Information Processing
Systems, pages 3111–3119.

Gregory Murphy. 2004. The big book of concepts. MIT press.

David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classiﬁcation. Lingvisticae

Investigationes, 30(1):3–26.

Arvind Neelakantan and Ming Wei Chang. 2015.

Inferring missing entity type instances for knowledge base

completion: New dataset and methods. Computer Science, pages 35–40.

Maxim Rabinovich and Klein Dan. 2017. Fine-grained entity typing with high-multiplicity assignments.

In

Meeting of the Association for Computational Linguistics, pages 330–334.

Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceed-
ings of the Thirteenth Conference on Computational Natural Language Learning, pages 147–155. Association
for Computational Linguistics.

Xiang Ren, Wenqi He, Meng Qu, Lifu Huang, Heng Ji, and Jiawei Han. 2016a. Afet: Automatic ﬁne-grained
entity typing by hierarchical partial-label embedding. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1369–1378.

Xiang Ren, Wenqi He, Meng Qu, Clare R Voss, Heng Ji, and Jiawei Han. 2016b. Label noise reduction in entity
In Proceedings of the 22nd ACM SIGKDD International

typing by heterogeneous partial-label embedding.
Conference on Knowledge Discovery and Data Mining, pages 1825–1834.

Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and Sebastian Riedel. 2017. Neural architectures for ﬁne-

grained entity type classiﬁcation. In EACL, pages 1271–1280.

Jian Tang, Meng Qu, and Qiaozhu Mei. 2015a. Pte: Predictive text embedding through large-scale heterogeneous
text networks. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 1165–1174. ACM.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015b. Line: Large-scale infor-
mation network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages
1067–1077. ACM.

Ji Xin, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2018.

Improving neural ﬁne-grained entity typing with

knowledge attention. In AAAI.

Bo Xu, Yi Zhang, Jiaqing Liang, Yanghua Xiao, Seung-won Hwang, and Wei Wang. 2016. Cross-lingual type

inference. In International Conference on Database Systems for Advanced Applications, pages 447–462.

Yadollah Yaghoobzadeh and Hinrich Sch¨utze. 2015. Corpus-level ﬁne-grained entity typing using contextual
information. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,
pages 715–725.

Yadollah Yaghoobzadeh and Hinrich Sch¨utze. 2017. Multi-level representations for ﬁne-grained typing of knowl-

edge base entities. arXiv:1701.02025.

Yadollah Yaghoobzadeh, Heike Adel, and Hinrich Sch¨utze. 2017. Corpus-level ﬁne-grained entity typing.

arXiv:1708.02275.

Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, and Gerhard Weikum. 2014. Robust question answering

over the web of linked data. In CIKM, pages 1107–1116.

Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. 2016. Revisiting semi-supervised learning with graph

embeddings. In ICML, pages 40–48.

Dani Yogatama, Dan Gillick, and Nevena Lazic. 2015. Embedding methods for ﬁne grained entity type classiﬁca-
tion. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language
Processing, ACL, pages 26–31.

M Amir Yosef, Sandro Bauer, Johannes Hoffart, Marc Spaniol, and Gerhard Weikum. 2012. Hyena: Hierarchical
In Proceedings of the 24th International Conference on Computational

type classiﬁcation for entity names.
Linguistics, pages 1361–1370.

