Web-scale image clustering revisited

Yannis Avrithis†, Yannis Kalantidis‡, Evangelos Anagnostopoulos†, Ioannis Z. Emiris†
†University of Athens, ‡Yahoo! Labs

Abstract

Large scale duplicate detection, clustering and mining
of documents or images has been conventionally treated
with seed detection via hashing, followed by seed growing
heuristics using fast search. Principled clustering meth-
ods, especially kernelized and spectral ones, have higher
complexity and are difﬁcult to scale above millions. Under
the assumption of documents or images embedded in Eu-
clidean space, we revisit recent advances in approximate
k-means variants, and borrow their best ingredients to in-
troduce a new one, inverted-quantized k-means (IQ-means).
Key underlying concepts are quantization of data points and
multi-index based inverted search from centroids to cells.
Its quantization is a form of hashing and analogous to seed
detection, while its updates are analogous to seed growing,
yet principled in the sense of distortion minimization. We
further design a dynamic variant that is able to determine
the number of clusters k in a single run at nearly zero ad-
ditional cost. Combined with powerful deep learned rep-
resentations, we achieve clustering of a 100 million image
collection on a single machine in less than one hour.

1. Introduction

NEARLY two decades ago [6], discovering duplicates

among millions of web documents was the motiva-
tion behind one of the ﬁrst locality sensitive hashing (LSH)
schemes, later known as MinHash [7]. The same method
was subsequently used to select seeds which, followed by
efﬁcient search and spatial veriﬁcation, would lead to clus-
tering and mining in collections of up to 105 images [10].

Many approaches followed, but problems have remained
such as failing to discover infrequent documents, seed
growing relying on heuristics, or more principled methods
like medoid shift still being too costly to scale up [38].
Pairwise matching remains a problem that is inherently
quadratic in the number of documents, and approximate
nearest neighbor (ANN) search has been employed to help.
Approximate k-means (AKM) is one such attempt [26],
where each data point is assigned to the nearest centroid
by ANN search. Binary k-means (BKM) [14] is another

(a) Ranked retrieval [8]

(b) DRVQ [1]

(c) EGM [2]

(d) This work: IQ-means

( ) Data points; ( ) cen-
Figure 1. Different k-means variants.
troids; ( ) search range; ( ) estimated cluster extent, used to dy-
namically determine k.

recent alternative where points and centroids are binarized
and ANN search follows in Hamming space. But in this
work we focus our attention on the inverse process.

Observing that data points remain ﬁxed during k-means
iterations, ranked retrieval [8] chooses to search for near-
est data points using centroids as queries, as illustrated in
Fig. 1a. This choice dispenses the need to rebuild an in-
dex at each iteration, and requires less queries because cen-
troids are naturally fewer than data points. Points are ex-
amined more than once and not all points are assigned to
centroids; it is observed however that distortion is not inﬂu-
enced much. If range queries were used, this method would
be very similar to mean shift [9], except that centroid dis-
placement is not independent here.

Dimensionality-recursive vector quantization (DRVQ)
[1] relies on the same inverted centroid-to-data queries.

1502

However, search is based on ideas extended from the in-
verted multi-index [4]. The entire search process for all cen-
troids resembles a propagation on a two-dimensional grid,
where each cell is visited only once and all cells are as-
signed to centroids, deﬁning a discrete Voronoi diagram, as
illustrated in Fig. 1b. Quantization on the grid is another
form of hashing, but thanks to the orthogonal construction,
it is now easy to visit cells by ascending distance.

As illustrated in Fig. 1c, expanding Gaussian mixtures
(EGM) [2] uses a conventional data-to-centroid search, like
AKM. What is special is that its probabilistic model allows
an estimate of overlap between clusters and dynamic deter-
mination of the appropriate number of clusters, in contrast
to Fig. 1a,b. This is a standing problem of k-means, con-
trary to other methods like medoid shift [31].

In this work we borrow the best ingredients from all
the methods above and introduce a new k-means variant,
called inverted-quantized k-means (IQ-means, or IQ-M), il-
lustrated in Fig. 1d. Its unique properties are given below,
summarizing our contributions:

1. We adopt the subspace quantization and multi-index
search of DRVQ, yielding fast centroid-to-cell search,
since actual data are in fact discarded. However, search
cost is governed by the length of a single priority queue
for all centroids, which is the bottleneck of DRVQ. Thus,
we switch to independent queries per centroid, as in
ranked retrieval. Even though cells may be visited more
than once, the use of multiple independent queues offers
a spectacular speed-up.

2. Although centroids are arbitrary vectors, they can still
be quantized on the grid. We exploit this observation
to obtain as a by-product the nearest centroids to each
centroid during the same centroid-to-cell search pro-
cess. We use this information to estimate pairwise clus-
ter overlaps and purge centroids between iterations. This
dynamically determines k, exactly as in EGM, and is re-
ferred to as dynamic IQ-means (D-IQ-M). This is unex-
pected since EGM assigns data points to multiple cen-
troids as required by its probabilistic model.

Another contribution is to revisit web-scale image clus-
tering. To scale up, we choose a global image representa-
tion, in particular state of the art deep-learned features [20],
which have been recently applied to visual search [5] and
even shown to outperform local feature-based representa-
tions in low dimensions [28]. We achieve clustering of a
100 million image collection in less than one hour on a sin-
gle machine, while estimating the number of clusters at the
same time. Given that IQ-M time complexity depends on
grid size rather than number of data points, this result opens
the way to truly web-scale image clustering, given enough
resources. We provide our implementation online1.

1http://github.com/iavr/iqm

The remaining text is organized as follows. Section 2
discusses related work, while sections 3, 4 present our algo-
rithms IQ-M and D-IQ-M, respectively. Experiments fol-
low in section 5 and conclusions are drawn in section 6.

2. Related work

The interest in large scale image duplicate detection,
clustering and mining is now stronger than ever. Lever-
aging the billions of images available in community photo
collections, many recent approaches utilize their metadata,
e.g. tags or geotags for clustering [19, 27, 11, 3]. Kennedy
et al. [19] were among the ﬁrst to extract textual and ge-
ographical patterns from metadata and use them in visual
clustering. Most methods follow a two-stage approach,
ﬁrst by geographic location and then by visual similarity.
However, clustering within each geographic cell remains
quadratic: even if fast retrieval is employed, a query per
image is still necessary. Other approaches ﬁnd iconic im-
ages [22, 38, 32], e.g. for scene summarization [32] or 3D
reconstruction [22]. With few exceptions [3], most methods
focus on landmarks and points of interest [11, 23].

Following earlier work on discovering object categories
from image segmentations [29], Chum and Matas [10] in-
troduce an approach for web-scale clustering using visual
information alone. Starting from local features, they use
minHash collisions to ﬁnd seed images which they grow
via retrieval and expansion. Although faster than most of
the aforementioned approaches due to hashing, it is limited
by the memory cost of local descriptor indexing as well as
the pairwise nature of geometric matching. It also focuses
on popular scenes. Although our quantization is another
form of hashing, we rather use a principled way of updating
seeds as in k-means. Besides, our multi-index approximate
search strategy is arguably the fastest possible, once images
are represented in a Euclidean space.

Most aforementioned approaches use local features and
descriptors, that hold the state-of-the-art in image retrieval
but incur signiﬁcant space and time cost [34]. We choose
to sacriﬁce their matching quality for scalability and repre-
sent each image by a single global representation. Although
several methods exist for aggregating local descriptors [17],
we rather choose deep learned features, which are shown
to be superior in low dimensions [5],[28]. We only assume
a Euclidean space representation, so our method is generic
and can apply well beyond image clustering.

Our approach is an approximation of k-means. Density
based approaches like mean shift [9] and medoid shift [31]
are typically inefﬁcient for large datasets. Quick shift [35]
is a fast approximate variant, but still requires nearest neigh-
bor search with each point as a query. Medoid-based meth-
ods as well as kernelized methods like kernel k-means [30]
and spectral clustering methods [36] are more generic in ap-
plying to non-Euclidean spaces but they are largely imprac-

1503

tical above a few million points. There are several methods
for estimating k even dynamically like component annihila-
tion [12], DP-Means [21] and EGM [2]; here we choose to
integrate our approach with EGM, which comes as a natural
extension at nearly zero cost.

Parallelism has widely been utilized for large scale clus-
tering [24, 15], while algorithms for distributed systems ex-
ist for many popular clustering algorithms like Parallel k-
means [39] or parallel DB-SCAN [25]. We are however
interested in large scale clustering without the need of a dis-
tributed grid. We show that we are able to provide an efﬁ-
cient k-means approximation that can cluster 100M images
in less than an hour on a single machine, while a distributed
implementation of standard k-means on the same dataset
using 300 machines on the grid takes over one day.

3. Inverted-quantized k-means (IQ-means)

Representation. We are given a dataset X of n points in
Rd, and the problem is to ﬁnd k cluster centroids minimiz-
ing distortion as in k-means. IQ-M assumes the same repre-
sentation and codebook building as in multi-indexing [4]. In
particular, assuming d is even, Rd is expressed as the Carte-
sian product of two orthogonal subspaces, S1 × S2, of d/2
dimensions each. Although this decomposition is subject to
optimization [13], which we do apply in our experiments,
we assume here the simplest decomposition whereby each
vector x is written as a tuple (x1, x2) consisting of two sub-
vectors x1, x2 ∈ Rd/2.

i ∈ U 1, u2
i , u2

We also assume there are two sub-codebooks U 1, U 2
trained independently on projections of sample data on
S1, S2 respectively. Each U ℓ contains s sub-codewords,
partitioning Sℓ into s disjoint subsets for ℓ = 1, 2. Then,
codebook U = U 1 × U 2 contains s × s codewords and par-
titions Rd into s × s cells. We thus refer to each codeword
u ∈ U as a cell, while U can be seen as a discrete two di-
j ∈ U 2
mensional grid. Given sub-codewords u1
with i, j ∈ [s] = {1, . . . , s}, we represent cell (u1
j ) ∈ U
by the multi-index notation uα with α being the integer tu-
ple (i, j) ∈ I = [s] × [s]. Every point x can be quan-
tized to a cell q(x) = (q1(x1), q2(x2)), where qℓ(xℓ) =
arg minuℓ∈U ℓ kxℓ − uℓk is the nearest sub-codeword of U ℓ
to projection xℓ of x on subspace Sℓ for ℓ = 1, 2.
Update step. Next, similarly to DRVQ [1], all points of
X are quantized on the grid and a discrete two-dimensional
distribution p of points over cells is constructed. In particu-
lar, for each cell uα, probability pα = |Xα|/n measures
the empirical frequency of points falling into uα, where
Xα = {x ∈ X : q(x) = uα}. Further, the mean
µα = 1
|Xα| Px∈Xα x of all points in Xα is kept for each
cell uα. At this point, dataset X may be discarded. An
arbitrary initial set C of k centroids is assumed.

As in all k-means variants, the algorithm then alternates

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21

1

2

3

4

5

6

7

8

9

10

11

c1

10

11

8

7

9

6

5

4

3

2

1

c1

c2

c2

5

4

6

3

7

2

8

1

9

10

11

(a) visited cells on original grid

8 9 7 10 6 11 5 12 4 13 3

12 13 11 14 10 15 9 16 8 17 7

(b) search block of c1

(c) search block of c2

Figure 2. Centroid-to-cell search during assignment, for two cen-
) cells V1 visited by
troids c1, c2. ( ) Data points; ( ) centroids; (
) other cells.
c1; (

) cells V2 visited by c2; (

between an assignment and an update step, where the latter
is simply given by weighted average

cm ←

pαµα,

1
Pm

X
α∈Am

(1)

for all centroids cm ∈ C. Here, Pm = Pα∈Am pα
is the proportion of points assigned to centroid cm and
Am = {α ∈ I : a(uα) = m} contains the indices of all
cells assigned to cm during the assignment step, where

a(u) = arg min
cm∈C

ku − cmk

(2)

is the index to the nearest centroid c to cell u ∈ U . In other
words, cells cα with their sample mean µα and probability
pα have completely replaced the original data. Still, assign-
ment (2) of cells to centroids is the bottleneck.

Assignment step. Here is where fast search is required. Al-
though assignment rule (2) implies a cell-to-centroid search,
we follow the inverse process as explained in section 1. As
in ranked retrieval [8], this process takes the form of a set of
individual queries for nearest cells, one for each centroid.
Search follows a multi-indexing approach in this work, in
particular using the multi-sequence algorithm [4].

Fig. 2a illustrates part of the grid, with two centroids
c1, c2 and the set of nearest cells to each centroid, say
V1, V2, in different colors. Recall that rows and columns
of the grid correspond to sub-codewords in U 1, U 2. These

1504

Algorithm 1: Centroid-to-cell multi-sequence search

1 function SEARCH(C, U, f )
for uα ∈ U do
2

a[α] ← 0; dist[α] ← ∞;
visit[α] ← FALSE; cen[α] ← 0

⊲ cell assignments
⊲ cen: section 4

for cm ∈ C do

m, U 1)
(k1, d1) ← NNw(c1
(k2, d2) ← NNw(c2
m, U 2)
f.INIT(m, (k1
1, k2
1)); V ← ∅
Q.INIT(); Q.PUSH((0, 0), 0)
while ¬Q.EMPTY() do

⊲ nearest..
⊲ ..sub-codewords
⊲ initialize list
⊲ initialize queue

i , k2
j )

((i, j), d) ← Q.EXTRACT-MIN()
α ← (k1
V ← V ∪ α; visit[α] ← TRUE
if f (m, α, d) then break
i−1, k2
if i = 0 ∨ visit[k1

Q.PUSH((i, j + 1), d1

j+1] then
i + d2
j−1] then

if j = 0 ∨ visit[k1

i+1, k2

⊲ current cell index
⊲ record cell
⊲ terminate?

j+1)

⊲ right

Q.PUSH((i + 1, j), d1

i+1 + d2
j )

⊲ below

for α ∈ V do visit[α] ← FALSE

⊲ reset

return a

⊲ cell assignments

21 function f.INIT(m, α)
22

n ← 0

23 function f (m, α, d)
24

⊲ number of points visited

if d < dist[α] then a[α] ← m; dist[α] ← d ⊲ re-assign
n ← n + |Xα|; return n ≥ T
⊲ target reached?

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

25

are just shown by their indices in Fig. 2a. Due to the inde-
pendent search processes, a number of cells, shown in color
overlay, belong to both V1, V2 and will be visited twice, trig-
gering a comparison to determine which of c1, c2 is nearest.
To understand the search process, Fig. 2b,c illustrate what
search looks like with c1, c2 as queries respectively.

For each query ci, the w nearest sub-codewords are
found in U 1, U 2, and ordered by ascending distance to ci,
for i = 1, 2. A w × w search block is thus determined for
ci. For w = 11, the two 11 × 11 search blocks of c1, c2
are shown in Fig. 2b,c, illustrating row/column selection
and ordering. Row/column numbers refer to the numbers
of Fig. 2a, but are re-arranged such that centroid ci and its
nearest cells appear on the top-left corner of the block. For
instance, top-left cells (8, 8) and (5, 12) of the two blocks
are indeed where c1, c2 are placed on the grid of Fig. 2a.
Observe however that due to re-arrangement, the nearest
cells to c2 are no longer contiguous in the block of c1 and
vice versa. They rather appear interlaced, and in higher di-
mensions they would appear randomly shufﬂed.

Search. The search process is outlined in Algorithm 1. For
each centroid c, the w nearest sub-codewords are given by
a list of ascending (squared) distances dℓ and indices kℓ

Algorithm 2: Centroid-to-centroid search function f
1 function f.INIT(m, α)
cen[α] ← m
2
Nm ← ∅
n ← 0

⊲ centroid per cell
⊲ (neighbors,distances) of centroid cm
⊲ number of points visited

4

3

5 function f (m, α, d)
6

if d < dist[α] then a[α] ← m; dist[α] ← d ⊲ re-assign
if cen[α] 6= 0 then Nm ← Nm ∪ (cen[α], d)
n ← n + |Xα|; return n ≥ T

7

8

for ℓ = 1, 2, specifying a search block. Nearest cells in
the block are visited by ascending (squared) distance d to
c using a priority queue Q, as in the multi-sequence algo-
rithm [4]: a cell to the right is visited if the one above right
is visited, and a cell below is visited if the one below left is
visited. There are substantial differences, though.

First, a function f determines the action to be taken at
each visited cell. Alternative functions are discussed in sec-
tion 4, but here f merely updates the current assignment a
and lowest distance dist found for each cell uα. Second,
f also controls search termination. Alternatives are again
discussed in section 4, but here f counts the total number
of underlying points in visited cells, and terminates when
this reaches a target number T . Finally, property visit is
global over the entire grid, indirectly accessed via indices
kℓ and reset after each block is searched, with the help of
an additional list V of visited cells. This implies that space
w × w and its initialization is no longer necessary [4]; the
algorithm is linear in the number of visited cells.

4. Dynamic IQ-means

While IQ-means searches from centroids to cells at each
iteration, its dynamic version also searches from centroids
to centroids, and keeps track of the nearest neighboring
centroids of each centroid, while both queries and indexed
points are constantly updated. Similarly to EGM [2], it
then uses this neighborhood information to compute cluster
overlaps and purge clusters between iterations in an attempt
to automatically determine k.
Search. The most interesting aspect of this centroid-to-
centroid search process is that it relies on the same indexing
structure; in fact, even though centroids are constantly up-
dated, it is a mere by-product of centroid-to-cell search, so
it comes at negligible cost. All that is needed is to keep
some additional information per cell and change the deﬁni-
tion of function f in Algorithm 1. The key observation is
that although centroids are arbitrary vectors, they can still
be quantized on the grid, just like data points.

The additional property cen holds up to one centroid in-
dex per cell and is initialized to zero by Algorithm 1. As
shown in Algorithm 2, each centroid cm is subsequently

1505

quantized to cell uα just before search and its index m is
recorded in cen[α]. This operation comes at no cost, since
the w nearest sub-codewords to each centroid are readily
available from NNw in Algorithm 1 and we can just take
the ﬁrst ones, α = (k1

1, k2

1).

A list Nm of nearest centroid indices and distances is
also maintained for each centroid cm, and is emptied just
before search. Then, for each cell α visited, a nonzero
cen[α] means that another centroid is found and is inserted
in Nm along with distance d. List Nm can be constrained to
hold up to a ﬁxed number of neighbors; no particular order-
ing is needed because cells, hence neighboring centroids,
are always found by ascending distance to cm.
Purging. Once neighboring centroids are found, clus-
ter overlaps may be estimated. Following EGM [2], we
model the distribution of points assigned to cluster cm by an
isotropic normal density N (x|cm, σm), where σm is simply
the standard deviation of points assigned to cluster m, esti-
mated only from cell information by

σ2
m ←

1
Pm

X
α∈Am

pαkµα − cmk2.

(3)

Then,
the same purging algorithm as in EGM applies,
roughly iterating over all clusters m in descending order of
population Pm, and purging clusters that overlap too much
with the collection of all clusters that have been kept so
far. Given the normal cluster densities, pairwise overlaps
are computed in closed form at the cost of one vector oper-
ation per pair. This algorithm is quadratic in k.

5. Experiments

In this section we evaluate the proposed approaches on
large scale clustering and compare against relevant state-
of-the-art methods. We ﬁrst present the datasets and fea-
tures used, as well as implementation details and evaluation
protocol. We then report results on three publicly available
datasets, including a dataset of 100 million images.

5.1. Experimental setup

Datasets. We experiment on three publicly available
datasets. SIFT1M [16] consists of 1M 128-dimensional
SIFT vectors, and a learning set of 100K vectors. Paris [37]
contains 500K images from Flickr and Panoramio, crawled
by geographic bounding box query around Paris city center.
The ground truth consists of 79 landmark clusters covering
94K dataset images. Yahoo Flickr Creative Commons 100M
(YFCC100M) [33] contains a subset of 100 million public
Flickr images with a creative commons license.
Features and codebooks. For Paris and YFCC, we use
convolutional neural network (CNN) features to globally
represent images.
In particular, we use the AlexNet ar-
chitecture [20] as a pre-trained model provided by Caffe

deep learning framework [18]. We use the output of the
last fully connected layer (fc7) as a 4096-dimensional fea-
ture vector for each image. By learning a covariance ma-
trix from the entire dataset, we further reduce to 128 di-
mensions, which not only speeds up the search process, but
also does not harm performance [5]. For IQ-means, we per-
mute the dimensions to balance the variance between the
two subspaces before multi-indexing [13]. For IQ-means
on SIFT1M, we use the separate learning set for off-line
learning of the sub-codebooks, while on Paris and YFCC
we use a 10M-vector random subset of YFCC.

Compared methods. For the smaller SIFT1M and Paris
datasets we compare the proposed IQ-means (IQ-M) and
dynamic IQ-means (dynamic IQ-M or D-IQ-M) methods
against the fastest approaches from the related work that
can also scale to large datasets: Ranked Retrieval (RR) [8]
and Approximate k-means (AKM) [26]. DRVQ [1] was
found to be faster than these methods but of signiﬁcantly
lower quality, so it is not included in the comparison. Bi-
nary k-means (BKM) [14] is only slightly faster than AKM,
so it is also not included. As all methods are approxima-
tions of k-means, we further report the upper bounds given
by k-means. For the large YFCC100M dataset, no related
method can run on a single machine due to space and time
requirements2. As a baseline, we apply k-means on the
non-empty multi-index cell centroid vectors, which is re-
ferred to as cell-k-means or CKM. This can be seen as an
approximation of IQ-means, where although actual points
are discarded as in IQ-means, cells are not weighted. Given
all 100M vectors as input, we also compare to a distributed
implementation of k-means, referred to as DKM, on 300
machines on the grid using Spark3. Again, this experiment
provides an upper bound on performance.

Implementation. We implement the ofﬂine learning pro-
cess and clustering interface in Matlab, using the Yael li-
brary4 for exact nearest neighbor search, assignment and
k-means clustering. Subspace search of centroids to sub-
codewords is also using Yael, while all remaining IQ-means
iteration, as outlined in Algorithm 1, is implemented in
C++, interfaced through a single MEX call. For any other
method that requires ANN search, i.e. ranked retrieval [8]
(RR) and Approximate k-means (AKM) [26] we use the
FLANN library5. Observe that RR’s own search algorithm
WAND is particularly targeted to documents and does not
apply to Euclidean spaces. Unless otherwise stated, all ex-
periments are performed on a single machine.

2The 128-dimensional visual feature vectors alone require 52GB of
space. One could of course use e.g. PQ-encoding yielding also fast search,
but again this would just be an alternative to our implementation of RR.

3http://spark.apache.org/
4https://gforge.inria.fr/projects/yael/
5http://www.cs.ubc.ca/research/flann/

1506

Evaluation protocol. We report clustering time (total or
per iteration) and average distortion on SIFT1M and Paris
with varying number of centroids k and data points n. Time
does not include off-line learning of sub-codebooks for IQ-
means; unless otherwise stated, total clustering time does
include encoding as explained in Table 1. Average dis-
tortion is the squared Euclidean distance of each point to
the nearest centroid, averaged over the dataset. Given the
ground truth labels of Paris, we also adopt the measures of
precision (or purity) and recall [37]. YFCC100M has no
associated ground truth, so in order to report more than just
clustering time, we also present precision on a public set of
noisy labels extracted through image classiﬁcation [33]. We
measure the average precision over all clusters, where pre-
cision is deﬁned as the percentage of the most popular class
in the cluster, i.e. the class present in the cluster most times.
In all algorithms, centroids C are initialized as k random
vectors from the dataset X. We run each experiment ﬁve
times and report mean measurements.

5.2. Results

Tuning. We ﬁrst evaluate the effect of the main parameters
of IQ-means on its performance, as measured by average
distortion and running time. These are the sub-codebook
size or grid size s, which determines how ﬁne the space
partition is, the size w of the search block and the search
target T ; the latter two determine the accuracy of search
from centroids to cells. The ﬁner the grid is, the higher
the quality of data representation, but the more cells need
to be visited; and the more accurate search is, the longer
it takes. For convenience, we set T = (n/k)t where t is
a normalized target parameter with respect to the average
cluster population under uniform distribution.

Table 1 presents results on SIFT1M for varying s and t,
which conﬁrm our expectations. It appears that s = 512
and t = 5 are reasonable trade-offs. We choose those set-
tings for the remaining experiments on SIFT1M and Paris,
which are of comparable size. On the other hand, we choose
s = 8K for the larger YFCC, so that the total number
of cells s2 = 64M is comparable to n = 100M. We set
the search block size w = 16 on SIFT1M and Paris, and
w = 512 on YFCC. Increasing w further would only make
search slower without improving distortion. This is particu-
larly important considering that sub-codeword search is the
most time-consuming part of Algorithm 1.

To evaluate dynamic IQ-means, Fig. 3 shows how the
ﬁnal estimated number of clusters k′ after termination de-
pends on the original one k. While k′ is nearly linear
in k for IQ-means—some clusters are still lost due to
quantization—there is a saturation effect with increasing
value of overlap threshold τ that controls purging [2].
It
is thus possible, given an unknown dataset, to begin cluster-
ing with an overestimation of k and let the algorithm purge

s (for t = 5)
512
256

128

1024

t (for s = 512)
5
2
1

encode (s) 4.570 8.380 16.44 33.70 16.44 16.44 16.44
3.153 4.366 7.760 12.78 6.418 7.557 7.760
search (s)
4.816 4.545 4.403 4.343 4.425 4.412 4.403
distortion

Table 1. Encode/search times (sec) and average distortion (×104)
for 20 iterations on SIFT1M for k = 104 and varying values of
grid size s and normalized search target t. Encoding includes
quantization of points on the grid and the inversion process to com-
pute cell population pα and means µα (1).

·104

IQ-M
D-IQ-M τ = 0.5
D-IQ-M τ = 0.6
D-IQ-M τ = 0.7

1.5

0.5

1

0

′

k

l
a
n
ﬁ

0

0.5

1

1.5

2

2.5

initial k

3

·104

Figure 3. Final k
for varying overlap threshold τ .

′ versus initial k number of centroids on SIFT1M

clusters as needed; subsequent iterations then become in-
creasingly faster. As a fair trade-off, we choose τ = 0.6 in
subsequent experiments.

Comparisons. We then evaluate the performance of IQ-
means against competing methods under varying number
of clusters k. Fig. 4a,4b measure average distortion and
running time for k up to 104 on SIFT1M. The quality of
k-means and AKM is close, indicating that most points are
correctly assigned to centroids; the quality of RR and IQ-
means is also close, indicating the loss of accuracy due to
unassigned points. There is also a clear ordering of running
times: AKM is faster than k-means by approximate search,
RR is even faster by inverted search, and IQ-means is the
fastest by more efﬁcient inverted search. Fig. 4c shows dis-
tortion versus time: the more the approximation the higher
the distortion, but at spectacular gain in speed.

Fig. 5 shows a different experiment for SIFT1M. We now
ﬁx k and vary n by clustering subsets of increasing size
from the original dataset. Otherwise distortion and time
measurements remain the same as in Fig. 4. Now both
distortion and time are increasing with n for all methods.
Again, as shown in Fig. 5a, distortion is similar for k-means
and AKM, higher for RR and slightly higher for IQ-means.
Observe that in IQ-means data points are quantized on a
ﬁxed grid and the algorithm operates on cell distributions
alone, regardless of n. The increase of time with n in Fig. 5b
is in fact only due to the encoding of data points, which is
linear in n. The gain in speed varies up to more than two

1507

0.2

0.4

0.8

1

0.2

0.4

0.8

1

0.6
k

·104

0.6
k

·104

102

103

104

time (s)

(a) average distortion vs. k

(b) time vs. k
Figure 4. Average distortion and total time for 20 iterations on SIFT1M for varying number of clusters k. Time for IQ-means includes
encoding of data points that is constant in k, but not codebook learning, which is performed on a different dataset.

(c) average distortion vs. time

·104

·104

n
o
i
t
r
o
t
s
i
d

e
g
a
r
e
v
a

4.5

5

4

n
o
i
t
r
o
t
s
i
d

e
g
a
r
e
v
a

4.4

4.2

4

3.8

k-means
AKM
RR
IQ-M

104

103

102

)
s
(

e
m

i
t

)
s
(

e
m

i
t

104

103

102

101

·104

·104

n
o
i
t
r
o
t
s
i
d

e
g
a
r
e
v
a

4.5

5

4

n
o
i
t
r
o
t
s
i
d

e
g
a
r
e
v
a

4.4

4.2

4

3.8

k-means
AKM
RR
IQ-M

0.2

0.4

0.8

0.6
n

1

·106

0.2

0.4

0.8

0.6
n

1

·106

101

102

103

104

time (s)

(a) average distortion vs. n

(b) time vs. n
Figure 5. Average distortion and total time for 20 iterations on SIFT1M for k = 104 and varying number of data points n. Time for
IQ-means includes encoding of data points that is linear in n, but not codebook learning.

(c) average distortion vs. time

0.55

0.5

0.45

0.4

0.35

0.3

n
o
i
s
i
c
e
r
p

k-means
RR
IQ-M
D-IQ-M, τ = 0.6

0

5 · 10−2

0.15

0.2

0.1

recall

Figure 6. Precision vs. recall for varying k on Paris.

CKM DKM D-IQ-M

′

k/k
time (s)
precision

100000
13068.1
0.474

100000
7920.0
0.616

85742
140.6
0.550

Table 2. Time per iteration and average precision for cell-k-means,
dynamic IQ-means and distributed k-means on YFCC100M with
initial k = 105. For DKM, we use Spark on 300 machines.

IQ-M

D-IQ-M

′

k/k
time (s)

100K 150K 200K 86K 120K 152K
212.6 271.1 325.8 140.6 249.6 277.2

Table 3. Time per iteration and k/k
IQ-means on YFCC100M.

′ for IQ-means and dynamic

(nearly three) orders of magnitude compared to k-means,
while the loss in distortion is reasonable.

Given the existing ground truth of Paris dataset, Fig. 6
further evaluates IQ-means and dynamic IQ-means against
other methods on a precision-recall diagram. Due to quan-
tization, it appears that our methods do not reach the upper-
left extreme of high precision and low recall, which can be
improved with a ﬁner grid at higher cost. Otherwise, all
methods are comparable regardless of their cost. Of course,
none of these methods is anywhere near in performance to
more expensive dedicated methods like iconoid shift [38].

One could just take into account that we are using a global
feature reduced to 128 dimensions per image. What is im-
portant is that in terms of classiﬁcation, all approximations
are equivalent to k-means in practice.

Large scale experiments. To demonstrate the scalability
of IQ-means, we perform clustering on the YFCC100M
dataset. We ﬁx the grid size to s = 8192, leaving 13M
non-empty cells for the 100 million vectors. Following the
tuning experiments, we use overlap threshold τ = 0.6 for
dynamic IQ-means and end up with k = 85742 after 20

1508

(a)

(b)

Figure 7. Mining example: subsets of similar clusters for (a) Paris and (b) Paris+YFCC100M. Images in red outline are from the Paris
ground truth.

iterations.

We report timings and average precision in Table 2 for
k = 105 clusters. We observe that the proposed approach is
two orders of magnitude faster than CKM. Both approaches
discard the initial data points but dynamic IQ-means further
weights each cell with its point statistics and gives more
consistent clusters in terms of label precision. Distributed
k-means on 300 machines takes 2.2 hours per iteration on
average, i.e. one order of magnitude slower than dynamic
IQ-means. In terms of precision, D-IQ-M performs better
than CKM and while the upper bound of DKM is high, the
latter requires far more time and resources. In Table 3 we
further present timings under varying k/k′ for IQ-means
and dynamic IQ-means.

To visualize the dynamic IQ-means result we show in
Fig. 7 a subset of a sample Paris cluster when clustering is
performed either on the Paris dataset alone or Paris along
with the entire YFCC100M dataset. We handpick the clus-
ter to depict approximately the same images from one of the
annotated landmarks of the Paris dataset. There are 2511
and 7382 images respectively in this cluster. Annotated
ground truth images are depicted in a red outline, while the
rest is a random sample in Fig. 7a and the images closest to
the annotated samples in Fig. 7b.

6. Discussion

By quantizing data points on a grid of two subspaces and
applying inverted search from centroids to cells using multi-

indexing, we have achieved an extremely fast variant of k-
means that can be used in any application where input data
lie on a Euclidean space. We have also achieved dynamic
estimation of the number of clusters in a single run at nearly
zero cost. Data points are extremely compressed (e.g. 26
bits per point on YFCC100M), which is a signiﬁcant space
improvement compared to all known methods. By using
global deep learned image representation, we have applied
this method to clustering 108 images on a single machine
in less than one hour. Although the result cannot be com-
pared to dedicated more costly mining methods, it is shown
to be on par with other k-means variants that are orders of
magnitude slower. In fact, the assignment step, convention-
ally the bottleneck of k-means, turns out to be faster than
the update step in IQ-means, which leaves small margin for
further improvement.

Acknowledgements. Y. Avrithis and I. Z. Emiris are partially
supported by the European Social Fund and Greek National
Fund through Research Funding Program ARISTEIA, project
“ESPRESSO” (70/3/11893). We thank Clayton Mellina and the
Flickr Vision Team for their help with CNN features and the dis-
tributed k-means experiment.

References

[1] Y. Avrithis. Quantize and conquer: A dimensionality-
recursive solution to clustering, vector quantization, and im-
age retrieval. In ICCV. 2013. 1, 3, 5

[2] Y. Avrithis and Y. Kalantidis. Approximate Gaussian mix-
tures for large scale vocabularies. In ECCV. 2012. 1, 2, 3, 4,

1509

mixture models. PAMI, 24(3):381–396, 2002. 3

[32] I. Simon, N. Snavely, and S. Seitz. Scene summarization for

[24] T. Liu, C. Rosenberg, and H. Rowley. Clustering billions of
images with large scale nearest neighbor search. In WACV,
2007. 3

[25] M. A. Patwary, D. Palsetia, A. Agrawal, W.-k. Liao,
F. Manne, and A. Choudhary. A new scalable parallel DB-
SCAN algorithm using the disjoint-set data structure. In SC,
2012. 3

[26] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisser-
man. Object retrieval with large vocabularies and fast spatial
matching. In CVPR, 2007. 1, 5

[27] T. Quack, B. Leibe, and L. Van Gool. World-scale mining
of objects and events from community photo collections. In
CIVR, pages 47–56, 2008. 2

[28] A. S. Razavian, J. Sullivan, A. Maki, and S. Carlsson. Visual
instance retrieval with deep convolutional networks. Techni-
cal report, 2014. 2

[29] B. Russell, A. Efros, J. Sivic, W. Freeman, and A. Zisserman.
Using multiple segmentations to discover objects and their
extent in image collections. In CVPR, 2006. 2

[30] B. Sch¨olkopf, A. Smola, and K. Muller. Nonlinear compo-
nent analysis as a kernel eigenvalue problem. Neural Com-
putation, 10(5):1299–1319, 1998. 2

[31] Y. A. Sheikh, E. A. Khan, and T. Kanade. Mode-seeking by

medoidshifts. In ICCV, 2007. 2

online image collections. In ICCV, 2007. 2

[33] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni,
D. Poland, D. Borth, and L.-J. Li. YFCC100M: The new
data in multimedia research. Communications of the ACM,
to appear. 5, 6

[34] G. Tolias, Y. Avrithis, and H. J´egou. To aggregate or not
to aggregate: Selective match kernels for image search. In
ICCV, 2013. 2

[35] A. Vedaldi and S. Soatto. Quick shift and kernel methods for

mode seeking. In ECCV, 2008. 2

[36] U. Von Luxburg. A tutorial on spectral clustering. Statistics

and Computing, 17(4):395–416, 2007. 2

[37] T. Weyand, J. Hosang, and B. Leibe. An evaluation of two
automatic landmark building discovery algorithms for city
reconstruction. In RMLE. 2010. 5, 6

[38] T. Weyand and B. Leibe. Discovering favorite views of pop-
ular places with iconoid shift. In ICCV, 2011. 1, 2, 7
[39] W. Zhao, H. Ma, and Q. He. Parallel k-means clustering
based on MapReduce. In Cloud Computing, pages 674–679.
2009. 3

5, 6

[3] Y. Avrithis, Y. Kalantidis, G. Tolias, and E. Spyrou. Re-
trieving landmark and non-landmark images from commu-
nity photo collections. In ACM Multimedia, 2010. 2

[4] A. Babenko and V. Lempitsky. The inverted multi-index. In

CVPR, 2012. 2, 3, 4

[5] A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky.
Neural codes for image retrieval. In ECCV, 2014. 2, 5
[6] A. Broder. On the resemblance and containment of docu-
ments. In Proceedings of the Compression and Complexity
of Sequences, page 21, 1997. 1

[7] A. Broder, M. Charikar, A. Frieze, and M. Mitzenmacher.
Min-wise independent permutations. In ACM Symposium on
Theory of Computing, pages 327–336, 1998. 1

[8] A. Broder, L. Garcia-Pueyo, V. Josifovski, S. Vassilvitskii,
and S. Venkatesan. Scalable k-means by ranked retrieval. In
Web Search and Data Mining, 2014. 1, 3, 5

[9] Y. Cheng. Mean shift, mode seeking, and clustering. PAMI,

17(8):790–799, 1995. 1, 2

[10] O. Chum and J. Matas. Large-scale discovery of spatially
related images. PAMI, 32(2):371–377, Feb 2010. 1, 2
[11] D. Crandall, L. Backstrom, D. Huttenlocher, and J. Klein-
berg. Mapping the world’s photos. In WWW, 2009. 2
[12] M. Figueiredo and A. Jain. Unsupervised learning of ﬁnite

[13] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product quan-
tization for approximate nearest neighbor search. In CVPR,
2013. 3, 5

[14] Y. Gong, M. Pawlowski, F. Yang, L. Brandy, L. Boundev,
and R. Fergus. Web scale photo hash clustering on a single
machine. In CVPR, 2015. 1, 5

[15] K. Heath, N. Gelfand, M. Ovsjanikov, M. Aanjaneya, and
L. J. Guibas. Image webs: Computing and exploiting con-
nectivity in image collections. In CVPR, 2010. 3

[16] H. J´egou, M. Douze, and C. Schmid. Product quantization

for nearest neighbor search. PAMI, 33(1), 2011. 5

[17] H. J´egou and A. Zisserman. Triangulation embedding and
democratic aggregation for image search. In CVPR, 2014. 2
[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014. 5

[19] L. Kennedy, M. Naaman, S. Ahern, R. Nair, and T. Ratten-
bury. How ﬂickr helps us make sense of the world: Context
and content in community-contributed media collections. In
ACM Multimedia, volume 3, pages 631–640, 2007. 2

[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS. 2012. 2, 5

Imagenet
In

[21] B. Kulis and M. I. Jordan. Revisiting k-means: New algo-
rithms via bayesian nonparametrics. In ICML, 2012. 3
[22] X. Li, C. Wu, C. Zach, S. Lazebnik, and J.-M. Frahm. Mod-
eling and recognition of landmark image collections using
iconic scene graphs. In ECCV, pages 427–440, 2008. 2
[23] Y. Li, D. J. Crandall, and D. P. Huttenlocher. Landmark clas-
siﬁcation in large-scale image collections. In ICCV, 2009.
2

1510

Web-scale image clustering revisited

Yannis Avrithis†, Yannis Kalantidis‡, Evangelos Anagnostopoulos†, Ioannis Z. Emiris†
†University of Athens, ‡Yahoo! Labs

Abstract

Large scale duplicate detection, clustering and mining
of documents or images has been conventionally treated
with seed detection via hashing, followed by seed growing
heuristics using fast search. Principled clustering meth-
ods, especially kernelized and spectral ones, have higher
complexity and are difﬁcult to scale above millions. Under
the assumption of documents or images embedded in Eu-
clidean space, we revisit recent advances in approximate
k-means variants, and borrow their best ingredients to in-
troduce a new one, inverted-quantized k-means (IQ-means).
Key underlying concepts are quantization of data points and
multi-index based inverted search from centroids to cells.
Its quantization is a form of hashing and analogous to seed
detection, while its updates are analogous to seed growing,
yet principled in the sense of distortion minimization. We
further design a dynamic variant that is able to determine
the number of clusters k in a single run at nearly zero ad-
ditional cost. Combined with powerful deep learned rep-
resentations, we achieve clustering of a 100 million image
collection on a single machine in less than one hour.

1. Introduction

NEARLY two decades ago [6], discovering duplicates

among millions of web documents was the motiva-
tion behind one of the ﬁrst locality sensitive hashing (LSH)
schemes, later known as MinHash [7]. The same method
was subsequently used to select seeds which, followed by
efﬁcient search and spatial veriﬁcation, would lead to clus-
tering and mining in collections of up to 105 images [10].

Many approaches followed, but problems have remained
such as failing to discover infrequent documents, seed
growing relying on heuristics, or more principled methods
like medoid shift still being too costly to scale up [38].
Pairwise matching remains a problem that is inherently
quadratic in the number of documents, and approximate
nearest neighbor (ANN) search has been employed to help.
Approximate k-means (AKM) is one such attempt [26],
where each data point is assigned to the nearest centroid
by ANN search. Binary k-means (BKM) [14] is another

(a) Ranked retrieval [8]

(b) DRVQ [1]

(c) EGM [2]

(d) This work: IQ-means

( ) Data points; ( ) cen-
Figure 1. Different k-means variants.
troids; ( ) search range; ( ) estimated cluster extent, used to dy-
namically determine k.

recent alternative where points and centroids are binarized
and ANN search follows in Hamming space. But in this
work we focus our attention on the inverse process.

Observing that data points remain ﬁxed during k-means
iterations, ranked retrieval [8] chooses to search for near-
est data points using centroids as queries, as illustrated in
Fig. 1a. This choice dispenses the need to rebuild an in-
dex at each iteration, and requires less queries because cen-
troids are naturally fewer than data points. Points are ex-
amined more than once and not all points are assigned to
centroids; it is observed however that distortion is not inﬂu-
enced much. If range queries were used, this method would
be very similar to mean shift [9], except that centroid dis-
placement is not independent here.

Dimensionality-recursive vector quantization (DRVQ)
[1] relies on the same inverted centroid-to-data queries.

1502

However, search is based on ideas extended from the in-
verted multi-index [4]. The entire search process for all cen-
troids resembles a propagation on a two-dimensional grid,
where each cell is visited only once and all cells are as-
signed to centroids, deﬁning a discrete Voronoi diagram, as
illustrated in Fig. 1b. Quantization on the grid is another
form of hashing, but thanks to the orthogonal construction,
it is now easy to visit cells by ascending distance.

As illustrated in Fig. 1c, expanding Gaussian mixtures
(EGM) [2] uses a conventional data-to-centroid search, like
AKM. What is special is that its probabilistic model allows
an estimate of overlap between clusters and dynamic deter-
mination of the appropriate number of clusters, in contrast
to Fig. 1a,b. This is a standing problem of k-means, con-
trary to other methods like medoid shift [31].

In this work we borrow the best ingredients from all
the methods above and introduce a new k-means variant,
called inverted-quantized k-means (IQ-means, or IQ-M), il-
lustrated in Fig. 1d. Its unique properties are given below,
summarizing our contributions:

1. We adopt the subspace quantization and multi-index
search of DRVQ, yielding fast centroid-to-cell search,
since actual data are in fact discarded. However, search
cost is governed by the length of a single priority queue
for all centroids, which is the bottleneck of DRVQ. Thus,
we switch to independent queries per centroid, as in
ranked retrieval. Even though cells may be visited more
than once, the use of multiple independent queues offers
a spectacular speed-up.

2. Although centroids are arbitrary vectors, they can still
be quantized on the grid. We exploit this observation
to obtain as a by-product the nearest centroids to each
centroid during the same centroid-to-cell search pro-
cess. We use this information to estimate pairwise clus-
ter overlaps and purge centroids between iterations. This
dynamically determines k, exactly as in EGM, and is re-
ferred to as dynamic IQ-means (D-IQ-M). This is unex-
pected since EGM assigns data points to multiple cen-
troids as required by its probabilistic model.

Another contribution is to revisit web-scale image clus-
tering. To scale up, we choose a global image representa-
tion, in particular state of the art deep-learned features [20],
which have been recently applied to visual search [5] and
even shown to outperform local feature-based representa-
tions in low dimensions [28]. We achieve clustering of a
100 million image collection in less than one hour on a sin-
gle machine, while estimating the number of clusters at the
same time. Given that IQ-M time complexity depends on
grid size rather than number of data points, this result opens
the way to truly web-scale image clustering, given enough
resources. We provide our implementation online1.

1http://github.com/iavr/iqm

The remaining text is organized as follows. Section 2
discusses related work, while sections 3, 4 present our algo-
rithms IQ-M and D-IQ-M, respectively. Experiments fol-
low in section 5 and conclusions are drawn in section 6.

2. Related work

The interest in large scale image duplicate detection,
clustering and mining is now stronger than ever. Lever-
aging the billions of images available in community photo
collections, many recent approaches utilize their metadata,
e.g. tags or geotags for clustering [19, 27, 11, 3]. Kennedy
et al. [19] were among the ﬁrst to extract textual and ge-
ographical patterns from metadata and use them in visual
clustering. Most methods follow a two-stage approach,
ﬁrst by geographic location and then by visual similarity.
However, clustering within each geographic cell remains
quadratic: even if fast retrieval is employed, a query per
image is still necessary. Other approaches ﬁnd iconic im-
ages [22, 38, 32], e.g. for scene summarization [32] or 3D
reconstruction [22]. With few exceptions [3], most methods
focus on landmarks and points of interest [11, 23].

Following earlier work on discovering object categories
from image segmentations [29], Chum and Matas [10] in-
troduce an approach for web-scale clustering using visual
information alone. Starting from local features, they use
minHash collisions to ﬁnd seed images which they grow
via retrieval and expansion. Although faster than most of
the aforementioned approaches due to hashing, it is limited
by the memory cost of local descriptor indexing as well as
the pairwise nature of geometric matching. It also focuses
on popular scenes. Although our quantization is another
form of hashing, we rather use a principled way of updating
seeds as in k-means. Besides, our multi-index approximate
search strategy is arguably the fastest possible, once images
are represented in a Euclidean space.

Most aforementioned approaches use local features and
descriptors, that hold the state-of-the-art in image retrieval
but incur signiﬁcant space and time cost [34]. We choose
to sacriﬁce their matching quality for scalability and repre-
sent each image by a single global representation. Although
several methods exist for aggregating local descriptors [17],
we rather choose deep learned features, which are shown
to be superior in low dimensions [5],[28]. We only assume
a Euclidean space representation, so our method is generic
and can apply well beyond image clustering.

Our approach is an approximation of k-means. Density
based approaches like mean shift [9] and medoid shift [31]
are typically inefﬁcient for large datasets. Quick shift [35]
is a fast approximate variant, but still requires nearest neigh-
bor search with each point as a query. Medoid-based meth-
ods as well as kernelized methods like kernel k-means [30]
and spectral clustering methods [36] are more generic in ap-
plying to non-Euclidean spaces but they are largely imprac-

1503

tical above a few million points. There are several methods
for estimating k even dynamically like component annihila-
tion [12], DP-Means [21] and EGM [2]; here we choose to
integrate our approach with EGM, which comes as a natural
extension at nearly zero cost.

Parallelism has widely been utilized for large scale clus-
tering [24, 15], while algorithms for distributed systems ex-
ist for many popular clustering algorithms like Parallel k-
means [39] or parallel DB-SCAN [25]. We are however
interested in large scale clustering without the need of a dis-
tributed grid. We show that we are able to provide an efﬁ-
cient k-means approximation that can cluster 100M images
in less than an hour on a single machine, while a distributed
implementation of standard k-means on the same dataset
using 300 machines on the grid takes over one day.

3. Inverted-quantized k-means (IQ-means)

Representation. We are given a dataset X of n points in
Rd, and the problem is to ﬁnd k cluster centroids minimiz-
ing distortion as in k-means. IQ-M assumes the same repre-
sentation and codebook building as in multi-indexing [4]. In
particular, assuming d is even, Rd is expressed as the Carte-
sian product of two orthogonal subspaces, S1 × S2, of d/2
dimensions each. Although this decomposition is subject to
optimization [13], which we do apply in our experiments,
we assume here the simplest decomposition whereby each
vector x is written as a tuple (x1, x2) consisting of two sub-
vectors x1, x2 ∈ Rd/2.

i ∈ U 1, u2
i , u2

We also assume there are two sub-codebooks U 1, U 2
trained independently on projections of sample data on
S1, S2 respectively. Each U ℓ contains s sub-codewords,
partitioning Sℓ into s disjoint subsets for ℓ = 1, 2. Then,
codebook U = U 1 × U 2 contains s × s codewords and par-
titions Rd into s × s cells. We thus refer to each codeword
u ∈ U as a cell, while U can be seen as a discrete two di-
j ∈ U 2
mensional grid. Given sub-codewords u1
with i, j ∈ [s] = {1, . . . , s}, we represent cell (u1
j ) ∈ U
by the multi-index notation uα with α being the integer tu-
ple (i, j) ∈ I = [s] × [s]. Every point x can be quan-
tized to a cell q(x) = (q1(x1), q2(x2)), where qℓ(xℓ) =
arg minuℓ∈U ℓ kxℓ − uℓk is the nearest sub-codeword of U ℓ
to projection xℓ of x on subspace Sℓ for ℓ = 1, 2.
Update step. Next, similarly to DRVQ [1], all points of
X are quantized on the grid and a discrete two-dimensional
distribution p of points over cells is constructed. In particu-
lar, for each cell uα, probability pα = |Xα|/n measures
the empirical frequency of points falling into uα, where
Xα = {x ∈ X : q(x) = uα}. Further, the mean
µα = 1
|Xα| Px∈Xα x of all points in Xα is kept for each
cell uα. At this point, dataset X may be discarded. An
arbitrary initial set C of k centroids is assumed.

As in all k-means variants, the algorithm then alternates

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21

1

2

3

4

5

6

7

8

9

10

11

c1

10

11

8

7

9

6

5

4

3

2

1

c1

c2

c2

5

4

6

3

7

2

8

1

9

10

11

(a) visited cells on original grid

8 9 7 10 6 11 5 12 4 13 3

12 13 11 14 10 15 9 16 8 17 7

(b) search block of c1

(c) search block of c2

Figure 2. Centroid-to-cell search during assignment, for two cen-
) cells V1 visited by
troids c1, c2. ( ) Data points; ( ) centroids; (
) other cells.
c1; (

) cells V2 visited by c2; (

between an assignment and an update step, where the latter
is simply given by weighted average

cm ←

pαµα,

1
Pm

X
α∈Am

(1)

for all centroids cm ∈ C. Here, Pm = Pα∈Am pα
is the proportion of points assigned to centroid cm and
Am = {α ∈ I : a(uα) = m} contains the indices of all
cells assigned to cm during the assignment step, where

a(u) = arg min
cm∈C

ku − cmk

(2)

is the index to the nearest centroid c to cell u ∈ U . In other
words, cells cα with their sample mean µα and probability
pα have completely replaced the original data. Still, assign-
ment (2) of cells to centroids is the bottleneck.

Assignment step. Here is where fast search is required. Al-
though assignment rule (2) implies a cell-to-centroid search,
we follow the inverse process as explained in section 1. As
in ranked retrieval [8], this process takes the form of a set of
individual queries for nearest cells, one for each centroid.
Search follows a multi-indexing approach in this work, in
particular using the multi-sequence algorithm [4].

Fig. 2a illustrates part of the grid, with two centroids
c1, c2 and the set of nearest cells to each centroid, say
V1, V2, in different colors. Recall that rows and columns
of the grid correspond to sub-codewords in U 1, U 2. These

1504

Algorithm 1: Centroid-to-cell multi-sequence search

1 function SEARCH(C, U, f )
for uα ∈ U do
2

a[α] ← 0; dist[α] ← ∞;
visit[α] ← FALSE; cen[α] ← 0

⊲ cell assignments
⊲ cen: section 4

for cm ∈ C do

m, U 1)
(k1, d1) ← NNw(c1
(k2, d2) ← NNw(c2
m, U 2)
f.INIT(m, (k1
1, k2
1)); V ← ∅
Q.INIT(); Q.PUSH((0, 0), 0)
while ¬Q.EMPTY() do

⊲ nearest..
⊲ ..sub-codewords
⊲ initialize list
⊲ initialize queue

i , k2
j )

((i, j), d) ← Q.EXTRACT-MIN()
α ← (k1
V ← V ∪ α; visit[α] ← TRUE
if f (m, α, d) then break
i−1, k2
if i = 0 ∨ visit[k1

Q.PUSH((i, j + 1), d1

j+1] then
i + d2
j−1] then

if j = 0 ∨ visit[k1

i+1, k2

⊲ current cell index
⊲ record cell
⊲ terminate?

j+1)

⊲ right

Q.PUSH((i + 1, j), d1

i+1 + d2
j )

⊲ below

for α ∈ V do visit[α] ← FALSE

⊲ reset

return a

⊲ cell assignments

21 function f.INIT(m, α)
22

n ← 0

23 function f (m, α, d)
24

⊲ number of points visited

if d < dist[α] then a[α] ← m; dist[α] ← d ⊲ re-assign
n ← n + |Xα|; return n ≥ T
⊲ target reached?

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

25

are just shown by their indices in Fig. 2a. Due to the inde-
pendent search processes, a number of cells, shown in color
overlay, belong to both V1, V2 and will be visited twice, trig-
gering a comparison to determine which of c1, c2 is nearest.
To understand the search process, Fig. 2b,c illustrate what
search looks like with c1, c2 as queries respectively.

For each query ci, the w nearest sub-codewords are
found in U 1, U 2, and ordered by ascending distance to ci,
for i = 1, 2. A w × w search block is thus determined for
ci. For w = 11, the two 11 × 11 search blocks of c1, c2
are shown in Fig. 2b,c, illustrating row/column selection
and ordering. Row/column numbers refer to the numbers
of Fig. 2a, but are re-arranged such that centroid ci and its
nearest cells appear on the top-left corner of the block. For
instance, top-left cells (8, 8) and (5, 12) of the two blocks
are indeed where c1, c2 are placed on the grid of Fig. 2a.
Observe however that due to re-arrangement, the nearest
cells to c2 are no longer contiguous in the block of c1 and
vice versa. They rather appear interlaced, and in higher di-
mensions they would appear randomly shufﬂed.

Search. The search process is outlined in Algorithm 1. For
each centroid c, the w nearest sub-codewords are given by
a list of ascending (squared) distances dℓ and indices kℓ

Algorithm 2: Centroid-to-centroid search function f
1 function f.INIT(m, α)
cen[α] ← m
2
Nm ← ∅
n ← 0

⊲ centroid per cell
⊲ (neighbors,distances) of centroid cm
⊲ number of points visited

4

3

5 function f (m, α, d)
6

if d < dist[α] then a[α] ← m; dist[α] ← d ⊲ re-assign
if cen[α] 6= 0 then Nm ← Nm ∪ (cen[α], d)
n ← n + |Xα|; return n ≥ T

7

8

for ℓ = 1, 2, specifying a search block. Nearest cells in
the block are visited by ascending (squared) distance d to
c using a priority queue Q, as in the multi-sequence algo-
rithm [4]: a cell to the right is visited if the one above right
is visited, and a cell below is visited if the one below left is
visited. There are substantial differences, though.

First, a function f determines the action to be taken at
each visited cell. Alternative functions are discussed in sec-
tion 4, but here f merely updates the current assignment a
and lowest distance dist found for each cell uα. Second,
f also controls search termination. Alternatives are again
discussed in section 4, but here f counts the total number
of underlying points in visited cells, and terminates when
this reaches a target number T . Finally, property visit is
global over the entire grid, indirectly accessed via indices
kℓ and reset after each block is searched, with the help of
an additional list V of visited cells. This implies that space
w × w and its initialization is no longer necessary [4]; the
algorithm is linear in the number of visited cells.

4. Dynamic IQ-means

While IQ-means searches from centroids to cells at each
iteration, its dynamic version also searches from centroids
to centroids, and keeps track of the nearest neighboring
centroids of each centroid, while both queries and indexed
points are constantly updated. Similarly to EGM [2], it
then uses this neighborhood information to compute cluster
overlaps and purge clusters between iterations in an attempt
to automatically determine k.
Search. The most interesting aspect of this centroid-to-
centroid search process is that it relies on the same indexing
structure; in fact, even though centroids are constantly up-
dated, it is a mere by-product of centroid-to-cell search, so
it comes at negligible cost. All that is needed is to keep
some additional information per cell and change the deﬁni-
tion of function f in Algorithm 1. The key observation is
that although centroids are arbitrary vectors, they can still
be quantized on the grid, just like data points.

The additional property cen holds up to one centroid in-
dex per cell and is initialized to zero by Algorithm 1. As
shown in Algorithm 2, each centroid cm is subsequently

1505

quantized to cell uα just before search and its index m is
recorded in cen[α]. This operation comes at no cost, since
the w nearest sub-codewords to each centroid are readily
available from NNw in Algorithm 1 and we can just take
the ﬁrst ones, α = (k1

1, k2

1).

A list Nm of nearest centroid indices and distances is
also maintained for each centroid cm, and is emptied just
before search. Then, for each cell α visited, a nonzero
cen[α] means that another centroid is found and is inserted
in Nm along with distance d. List Nm can be constrained to
hold up to a ﬁxed number of neighbors; no particular order-
ing is needed because cells, hence neighboring centroids,
are always found by ascending distance to cm.
Purging. Once neighboring centroids are found, clus-
ter overlaps may be estimated. Following EGM [2], we
model the distribution of points assigned to cluster cm by an
isotropic normal density N (x|cm, σm), where σm is simply
the standard deviation of points assigned to cluster m, esti-
mated only from cell information by

σ2
m ←

1
Pm

X
α∈Am

pαkµα − cmk2.

(3)

Then,
the same purging algorithm as in EGM applies,
roughly iterating over all clusters m in descending order of
population Pm, and purging clusters that overlap too much
with the collection of all clusters that have been kept so
far. Given the normal cluster densities, pairwise overlaps
are computed in closed form at the cost of one vector oper-
ation per pair. This algorithm is quadratic in k.

5. Experiments

In this section we evaluate the proposed approaches on
large scale clustering and compare against relevant state-
of-the-art methods. We ﬁrst present the datasets and fea-
tures used, as well as implementation details and evaluation
protocol. We then report results on three publicly available
datasets, including a dataset of 100 million images.

5.1. Experimental setup

Datasets. We experiment on three publicly available
datasets. SIFT1M [16] consists of 1M 128-dimensional
SIFT vectors, and a learning set of 100K vectors. Paris [37]
contains 500K images from Flickr and Panoramio, crawled
by geographic bounding box query around Paris city center.
The ground truth consists of 79 landmark clusters covering
94K dataset images. Yahoo Flickr Creative Commons 100M
(YFCC100M) [33] contains a subset of 100 million public
Flickr images with a creative commons license.
Features and codebooks. For Paris and YFCC, we use
convolutional neural network (CNN) features to globally
represent images.
In particular, we use the AlexNet ar-
chitecture [20] as a pre-trained model provided by Caffe

deep learning framework [18]. We use the output of the
last fully connected layer (fc7) as a 4096-dimensional fea-
ture vector for each image. By learning a covariance ma-
trix from the entire dataset, we further reduce to 128 di-
mensions, which not only speeds up the search process, but
also does not harm performance [5]. For IQ-means, we per-
mute the dimensions to balance the variance between the
two subspaces before multi-indexing [13]. For IQ-means
on SIFT1M, we use the separate learning set for off-line
learning of the sub-codebooks, while on Paris and YFCC
we use a 10M-vector random subset of YFCC.

Compared methods. For the smaller SIFT1M and Paris
datasets we compare the proposed IQ-means (IQ-M) and
dynamic IQ-means (dynamic IQ-M or D-IQ-M) methods
against the fastest approaches from the related work that
can also scale to large datasets: Ranked Retrieval (RR) [8]
and Approximate k-means (AKM) [26]. DRVQ [1] was
found to be faster than these methods but of signiﬁcantly
lower quality, so it is not included in the comparison. Bi-
nary k-means (BKM) [14] is only slightly faster than AKM,
so it is also not included. As all methods are approxima-
tions of k-means, we further report the upper bounds given
by k-means. For the large YFCC100M dataset, no related
method can run on a single machine due to space and time
requirements2. As a baseline, we apply k-means on the
non-empty multi-index cell centroid vectors, which is re-
ferred to as cell-k-means or CKM. This can be seen as an
approximation of IQ-means, where although actual points
are discarded as in IQ-means, cells are not weighted. Given
all 100M vectors as input, we also compare to a distributed
implementation of k-means, referred to as DKM, on 300
machines on the grid using Spark3. Again, this experiment
provides an upper bound on performance.

Implementation. We implement the ofﬂine learning pro-
cess and clustering interface in Matlab, using the Yael li-
brary4 for exact nearest neighbor search, assignment and
k-means clustering. Subspace search of centroids to sub-
codewords is also using Yael, while all remaining IQ-means
iteration, as outlined in Algorithm 1, is implemented in
C++, interfaced through a single MEX call. For any other
method that requires ANN search, i.e. ranked retrieval [8]
(RR) and Approximate k-means (AKM) [26] we use the
FLANN library5. Observe that RR’s own search algorithm
WAND is particularly targeted to documents and does not
apply to Euclidean spaces. Unless otherwise stated, all ex-
periments are performed on a single machine.

2The 128-dimensional visual feature vectors alone require 52GB of
space. One could of course use e.g. PQ-encoding yielding also fast search,
but again this would just be an alternative to our implementation of RR.

3http://spark.apache.org/
4https://gforge.inria.fr/projects/yael/
5http://www.cs.ubc.ca/research/flann/

1506

Evaluation protocol. We report clustering time (total or
per iteration) and average distortion on SIFT1M and Paris
with varying number of centroids k and data points n. Time
does not include off-line learning of sub-codebooks for IQ-
means; unless otherwise stated, total clustering time does
include encoding as explained in Table 1. Average dis-
tortion is the squared Euclidean distance of each point to
the nearest centroid, averaged over the dataset. Given the
ground truth labels of Paris, we also adopt the measures of
precision (or purity) and recall [37]. YFCC100M has no
associated ground truth, so in order to report more than just
clustering time, we also present precision on a public set of
noisy labels extracted through image classiﬁcation [33]. We
measure the average precision over all clusters, where pre-
cision is deﬁned as the percentage of the most popular class
in the cluster, i.e. the class present in the cluster most times.
In all algorithms, centroids C are initialized as k random
vectors from the dataset X. We run each experiment ﬁve
times and report mean measurements.

5.2. Results

Tuning. We ﬁrst evaluate the effect of the main parameters
of IQ-means on its performance, as measured by average
distortion and running time. These are the sub-codebook
size or grid size s, which determines how ﬁne the space
partition is, the size w of the search block and the search
target T ; the latter two determine the accuracy of search
from centroids to cells. The ﬁner the grid is, the higher
the quality of data representation, but the more cells need
to be visited; and the more accurate search is, the longer
it takes. For convenience, we set T = (n/k)t where t is
a normalized target parameter with respect to the average
cluster population under uniform distribution.

Table 1 presents results on SIFT1M for varying s and t,
which conﬁrm our expectations. It appears that s = 512
and t = 5 are reasonable trade-offs. We choose those set-
tings for the remaining experiments on SIFT1M and Paris,
which are of comparable size. On the other hand, we choose
s = 8K for the larger YFCC, so that the total number
of cells s2 = 64M is comparable to n = 100M. We set
the search block size w = 16 on SIFT1M and Paris, and
w = 512 on YFCC. Increasing w further would only make
search slower without improving distortion. This is particu-
larly important considering that sub-codeword search is the
most time-consuming part of Algorithm 1.

To evaluate dynamic IQ-means, Fig. 3 shows how the
ﬁnal estimated number of clusters k′ after termination de-
pends on the original one k. While k′ is nearly linear
in k for IQ-means—some clusters are still lost due to
quantization—there is a saturation effect with increasing
value of overlap threshold τ that controls purging [2].
It
is thus possible, given an unknown dataset, to begin cluster-
ing with an overestimation of k and let the algorithm purge

s (for t = 5)
512
256

128

1024

t (for s = 512)
5
2
1

encode (s) 4.570 8.380 16.44 33.70 16.44 16.44 16.44
3.153 4.366 7.760 12.78 6.418 7.557 7.760
search (s)
4.816 4.545 4.403 4.343 4.425 4.412 4.403
distortion

Table 1. Encode/search times (sec) and average distortion (×104)
for 20 iterations on SIFT1M for k = 104 and varying values of
grid size s and normalized search target t. Encoding includes
quantization of points on the grid and the inversion process to com-
pute cell population pα and means µα (1).

·104

IQ-M
D-IQ-M τ = 0.5
D-IQ-M τ = 0.6
D-IQ-M τ = 0.7

1.5

0.5

1

0

′

k

l
a
n
ﬁ

0

0.5

1

1.5

2

2.5

initial k

3

·104

Figure 3. Final k
for varying overlap threshold τ .

′ versus initial k number of centroids on SIFT1M

clusters as needed; subsequent iterations then become in-
creasingly faster. As a fair trade-off, we choose τ = 0.6 in
subsequent experiments.

Comparisons. We then evaluate the performance of IQ-
means against competing methods under varying number
of clusters k. Fig. 4a,4b measure average distortion and
running time for k up to 104 on SIFT1M. The quality of
k-means and AKM is close, indicating that most points are
correctly assigned to centroids; the quality of RR and IQ-
means is also close, indicating the loss of accuracy due to
unassigned points. There is also a clear ordering of running
times: AKM is faster than k-means by approximate search,
RR is even faster by inverted search, and IQ-means is the
fastest by more efﬁcient inverted search. Fig. 4c shows dis-
tortion versus time: the more the approximation the higher
the distortion, but at spectacular gain in speed.

Fig. 5 shows a different experiment for SIFT1M. We now
ﬁx k and vary n by clustering subsets of increasing size
from the original dataset. Otherwise distortion and time
measurements remain the same as in Fig. 4. Now both
distortion and time are increasing with n for all methods.
Again, as shown in Fig. 5a, distortion is similar for k-means
and AKM, higher for RR and slightly higher for IQ-means.
Observe that in IQ-means data points are quantized on a
ﬁxed grid and the algorithm operates on cell distributions
alone, regardless of n. The increase of time with n in Fig. 5b
is in fact only due to the encoding of data points, which is
linear in n. The gain in speed varies up to more than two

1507

0.2

0.4

0.8

1

0.2

0.4

0.8

1

0.6
k

·104

0.6
k

·104

102

103

104

time (s)

(a) average distortion vs. k

(b) time vs. k
Figure 4. Average distortion and total time for 20 iterations on SIFT1M for varying number of clusters k. Time for IQ-means includes
encoding of data points that is constant in k, but not codebook learning, which is performed on a different dataset.

(c) average distortion vs. time

·104

·104

n
o
i
t
r
o
t
s
i
d

e
g
a
r
e
v
a

4.5

5

4

n
o
i
t
r
o
t
s
i
d

e
g
a
r
e
v
a

4.4

4.2

4

3.8

k-means
AKM
RR
IQ-M

104

103

102

)
s
(

e
m

i
t

)
s
(

e
m

i
t

104

103

102

101

·104

·104

n
o
i
t
r
o
t
s
i
d

e
g
a
r
e
v
a

4.5

5

4

n
o
i
t
r
o
t
s
i
d

e
g
a
r
e
v
a

4.4

4.2

4

3.8

k-means
AKM
RR
IQ-M

0.2

0.4

0.8

0.6
n

1

·106

0.2

0.4

0.8

0.6
n

1

·106

101

102

103

104

time (s)

(a) average distortion vs. n

(b) time vs. n
Figure 5. Average distortion and total time for 20 iterations on SIFT1M for k = 104 and varying number of data points n. Time for
IQ-means includes encoding of data points that is linear in n, but not codebook learning.

(c) average distortion vs. time

0.55

0.5

0.45

0.4

0.35

0.3

n
o
i
s
i
c
e
r
p

k-means
RR
IQ-M
D-IQ-M, τ = 0.6

0

5 · 10−2

0.15

0.2

0.1

recall

Figure 6. Precision vs. recall for varying k on Paris.

CKM DKM D-IQ-M

′

k/k
time (s)
precision

100000
13068.1
0.474

100000
7920.0
0.616

85742
140.6
0.550

Table 2. Time per iteration and average precision for cell-k-means,
dynamic IQ-means and distributed k-means on YFCC100M with
initial k = 105. For DKM, we use Spark on 300 machines.

IQ-M

D-IQ-M

′

k/k
time (s)

100K 150K 200K 86K 120K 152K
212.6 271.1 325.8 140.6 249.6 277.2

Table 3. Time per iteration and k/k
IQ-means on YFCC100M.

′ for IQ-means and dynamic

(nearly three) orders of magnitude compared to k-means,
while the loss in distortion is reasonable.

Given the existing ground truth of Paris dataset, Fig. 6
further evaluates IQ-means and dynamic IQ-means against
other methods on a precision-recall diagram. Due to quan-
tization, it appears that our methods do not reach the upper-
left extreme of high precision and low recall, which can be
improved with a ﬁner grid at higher cost. Otherwise, all
methods are comparable regardless of their cost. Of course,
none of these methods is anywhere near in performance to
more expensive dedicated methods like iconoid shift [38].

One could just take into account that we are using a global
feature reduced to 128 dimensions per image. What is im-
portant is that in terms of classiﬁcation, all approximations
are equivalent to k-means in practice.

Large scale experiments. To demonstrate the scalability
of IQ-means, we perform clustering on the YFCC100M
dataset. We ﬁx the grid size to s = 8192, leaving 13M
non-empty cells for the 100 million vectors. Following the
tuning experiments, we use overlap threshold τ = 0.6 for
dynamic IQ-means and end up with k = 85742 after 20

1508

(a)

(b)

Figure 7. Mining example: subsets of similar clusters for (a) Paris and (b) Paris+YFCC100M. Images in red outline are from the Paris
ground truth.

iterations.

We report timings and average precision in Table 2 for
k = 105 clusters. We observe that the proposed approach is
two orders of magnitude faster than CKM. Both approaches
discard the initial data points but dynamic IQ-means further
weights each cell with its point statistics and gives more
consistent clusters in terms of label precision. Distributed
k-means on 300 machines takes 2.2 hours per iteration on
average, i.e. one order of magnitude slower than dynamic
IQ-means. In terms of precision, D-IQ-M performs better
than CKM and while the upper bound of DKM is high, the
latter requires far more time and resources. In Table 3 we
further present timings under varying k/k′ for IQ-means
and dynamic IQ-means.

To visualize the dynamic IQ-means result we show in
Fig. 7 a subset of a sample Paris cluster when clustering is
performed either on the Paris dataset alone or Paris along
with the entire YFCC100M dataset. We handpick the clus-
ter to depict approximately the same images from one of the
annotated landmarks of the Paris dataset. There are 2511
and 7382 images respectively in this cluster. Annotated
ground truth images are depicted in a red outline, while the
rest is a random sample in Fig. 7a and the images closest to
the annotated samples in Fig. 7b.

6. Discussion

By quantizing data points on a grid of two subspaces and
applying inverted search from centroids to cells using multi-

indexing, we have achieved an extremely fast variant of k-
means that can be used in any application where input data
lie on a Euclidean space. We have also achieved dynamic
estimation of the number of clusters in a single run at nearly
zero cost. Data points are extremely compressed (e.g. 26
bits per point on YFCC100M), which is a signiﬁcant space
improvement compared to all known methods. By using
global deep learned image representation, we have applied
this method to clustering 108 images on a single machine
in less than one hour. Although the result cannot be com-
pared to dedicated more costly mining methods, it is shown
to be on par with other k-means variants that are orders of
magnitude slower. In fact, the assignment step, convention-
ally the bottleneck of k-means, turns out to be faster than
the update step in IQ-means, which leaves small margin for
further improvement.

Acknowledgements. Y. Avrithis and I. Z. Emiris are partially
supported by the European Social Fund and Greek National
Fund through Research Funding Program ARISTEIA, project
“ESPRESSO” (70/3/11893). We thank Clayton Mellina and the
Flickr Vision Team for their help with CNN features and the dis-
tributed k-means experiment.

References

[1] Y. Avrithis. Quantize and conquer: A dimensionality-
recursive solution to clustering, vector quantization, and im-
age retrieval. In ICCV. 2013. 1, 3, 5

[2] Y. Avrithis and Y. Kalantidis. Approximate Gaussian mix-
tures for large scale vocabularies. In ECCV. 2012. 1, 2, 3, 4,

1509

mixture models. PAMI, 24(3):381–396, 2002. 3

[32] I. Simon, N. Snavely, and S. Seitz. Scene summarization for

[24] T. Liu, C. Rosenberg, and H. Rowley. Clustering billions of
images with large scale nearest neighbor search. In WACV,
2007. 3

[25] M. A. Patwary, D. Palsetia, A. Agrawal, W.-k. Liao,
F. Manne, and A. Choudhary. A new scalable parallel DB-
SCAN algorithm using the disjoint-set data structure. In SC,
2012. 3

[26] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisser-
man. Object retrieval with large vocabularies and fast spatial
matching. In CVPR, 2007. 1, 5

[27] T. Quack, B. Leibe, and L. Van Gool. World-scale mining
of objects and events from community photo collections. In
CIVR, pages 47–56, 2008. 2

[28] A. S. Razavian, J. Sullivan, A. Maki, and S. Carlsson. Visual
instance retrieval with deep convolutional networks. Techni-
cal report, 2014. 2

[29] B. Russell, A. Efros, J. Sivic, W. Freeman, and A. Zisserman.
Using multiple segmentations to discover objects and their
extent in image collections. In CVPR, 2006. 2

[30] B. Sch¨olkopf, A. Smola, and K. Muller. Nonlinear compo-
nent analysis as a kernel eigenvalue problem. Neural Com-
putation, 10(5):1299–1319, 1998. 2

[31] Y. A. Sheikh, E. A. Khan, and T. Kanade. Mode-seeking by

medoidshifts. In ICCV, 2007. 2

online image collections. In ICCV, 2007. 2

[33] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni,
D. Poland, D. Borth, and L.-J. Li. YFCC100M: The new
data in multimedia research. Communications of the ACM,
to appear. 5, 6

[34] G. Tolias, Y. Avrithis, and H. J´egou. To aggregate or not
to aggregate: Selective match kernels for image search. In
ICCV, 2013. 2

[35] A. Vedaldi and S. Soatto. Quick shift and kernel methods for

mode seeking. In ECCV, 2008. 2

[36] U. Von Luxburg. A tutorial on spectral clustering. Statistics

and Computing, 17(4):395–416, 2007. 2

[37] T. Weyand, J. Hosang, and B. Leibe. An evaluation of two
automatic landmark building discovery algorithms for city
reconstruction. In RMLE. 2010. 5, 6

[38] T. Weyand and B. Leibe. Discovering favorite views of pop-
ular places with iconoid shift. In ICCV, 2011. 1, 2, 7
[39] W. Zhao, H. Ma, and Q. He. Parallel k-means clustering
based on MapReduce. In Cloud Computing, pages 674–679.
2009. 3

5, 6

[3] Y. Avrithis, Y. Kalantidis, G. Tolias, and E. Spyrou. Re-
trieving landmark and non-landmark images from commu-
nity photo collections. In ACM Multimedia, 2010. 2

[4] A. Babenko and V. Lempitsky. The inverted multi-index. In

CVPR, 2012. 2, 3, 4

[5] A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky.
Neural codes for image retrieval. In ECCV, 2014. 2, 5
[6] A. Broder. On the resemblance and containment of docu-
ments. In Proceedings of the Compression and Complexity
of Sequences, page 21, 1997. 1

[7] A. Broder, M. Charikar, A. Frieze, and M. Mitzenmacher.
Min-wise independent permutations. In ACM Symposium on
Theory of Computing, pages 327–336, 1998. 1

[8] A. Broder, L. Garcia-Pueyo, V. Josifovski, S. Vassilvitskii,
and S. Venkatesan. Scalable k-means by ranked retrieval. In
Web Search and Data Mining, 2014. 1, 3, 5

[9] Y. Cheng. Mean shift, mode seeking, and clustering. PAMI,

17(8):790–799, 1995. 1, 2

[10] O. Chum and J. Matas. Large-scale discovery of spatially
related images. PAMI, 32(2):371–377, Feb 2010. 1, 2
[11] D. Crandall, L. Backstrom, D. Huttenlocher, and J. Klein-
berg. Mapping the world’s photos. In WWW, 2009. 2
[12] M. Figueiredo and A. Jain. Unsupervised learning of ﬁnite

[13] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product quan-
tization for approximate nearest neighbor search. In CVPR,
2013. 3, 5

[14] Y. Gong, M. Pawlowski, F. Yang, L. Brandy, L. Boundev,
and R. Fergus. Web scale photo hash clustering on a single
machine. In CVPR, 2015. 1, 5

[15] K. Heath, N. Gelfand, M. Ovsjanikov, M. Aanjaneya, and
L. J. Guibas. Image webs: Computing and exploiting con-
nectivity in image collections. In CVPR, 2010. 3

[16] H. J´egou, M. Douze, and C. Schmid. Product quantization

for nearest neighbor search. PAMI, 33(1), 2011. 5

[17] H. J´egou and A. Zisserman. Triangulation embedding and
democratic aggregation for image search. In CVPR, 2014. 2
[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014. 5

[19] L. Kennedy, M. Naaman, S. Ahern, R. Nair, and T. Ratten-
bury. How ﬂickr helps us make sense of the world: Context
and content in community-contributed media collections. In
ACM Multimedia, volume 3, pages 631–640, 2007. 2

[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS. 2012. 2, 5

Imagenet
In

[21] B. Kulis and M. I. Jordan. Revisiting k-means: New algo-
rithms via bayesian nonparametrics. In ICML, 2012. 3
[22] X. Li, C. Wu, C. Zach, S. Lazebnik, and J.-M. Frahm. Mod-
eling and recognition of landmark image collections using
iconic scene graphs. In ECCV, pages 427–440, 2008. 2
[23] Y. Li, D. J. Crandall, and D. P. Huttenlocher. Landmark clas-
siﬁcation in large-scale image collections. In ICCV, 2009.
2

1510

