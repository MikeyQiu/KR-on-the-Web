7
1
0
2
 
y
a
M
 
3
1
 
 
]

R

I
.
s
c
[
 
 
1
v
3
0
8
4
0
.
5
0
7
1
:
v
i
X
r
a

Benchmark for Complex Answer Retrieval

Federico Nanni
University of Mannheim
fede@informatik.uni-mannheim.de

Bhaskar Mitra
Microsoft, UCL∗
bmitra@microsoft.com

Matt Magnusson
University of New Hampshire
magnusson3@gmail.com

Laura Dietz
University of New Hampshire
dietz@cs.unh.edu

Abstract

Retrieving paragraphs to populate a Wikipedia ar-
ticle is a challenging task. The new TREC Complex
Answer Retrieval (TREC CAR) track introduces a
comprehensive dataset that targets this retrieval sce-
nario. We present early results from a variety of
approaches – from standard information retrieval
methods (e.g., tf-idf) to complex systems that using
query expansion using knowledge bases and deep
neural networks. The goal is to offer future partici-
pants of this track an overview of some promising
approaches to tackle this problem.

1 Introduction

Over the last two decades, research in information
retrieval (IR) has developed a variety of approaches
for answering queries regarding precise facts – such
as “Population New York City”, “Who is Bill de Bla-
sio?” or “Neighborhoods in Manhattan” – via the
identification, extraction, and synthesis of pieces of
information from textual sources. However, for more
complex queries, such as “Benefits of immigration
for NYC culture” current systems still rely on pre-
senting the traditional ten blue links to the user as
an answer.

To solicit works in this direction, and following
the output of the recent SWIRL 2012 workshop on

frontiers, challenges, and opportunities for infor-
mation retrieval report [2], a new TREC track on
Complex Answer Retrieval1 (TREC CAR) for open-
domain queries has been recently introduced [10].

The task and related dataset are based on the as-
sumption that each Wikipedia page represents a com-
plex topic, with further details under each sections.
Accordingly, paragraphs contained in a section such
as “Cultural diversity – Demographic” of the page
“Culture of New York City”, offer one aspect of the
open-domain query “Culture of New York City”. The
goal of the task is presented as such: given an outline
of a page (in the form of the page title and hierarchi-
cal section headings), retrieve a ranking of passages
for each section. While assessed manually in the fu-
ture, in this work, a passage is relevant for a section
if and only if it is contained in the original article in
the corresponding section.
Contribution. Many different approaches can be
applied to this problem. This can address the ways a
query can be expanded (using textual or structural
information from knowledge bases), the way query
and passages can be represented as vectors (e.g., word
embedding vectors), and applications of deep neural
networks and learning to rank. Given the recent
release of the dataset, with this work we intend to
support the future participants of the TREC CAR
task by studying the performance of a variety of
methods—to highlight which of them may be the
most promising directions.

∗The author is a part-time PhD student at University College

London.

1http://trec-car.cs.unh.edu/

1

Additionally with the publication of this paper, we
make our experimentation environment and dataset
available2, which simulates a noisy candidate gener-
ation method for this task.

Outline. We give an overview of related work in Sec-
tion 2, describe the data set and our experimentation
environment in Section 3 and 4. Section 5 provides
details of the approaches. We evaluate empirically
in Section 6 before concluding the paper.

2 Related Work

A wide variety of approaches are applicable to the
TREC CAR problem. In the following, we cover three
central ones, namely passage retrieval, query expan-
sion using knowledge bases and the recent advance-
ment in the use of deep neural network models for
information retrieval.

Passage Retrieval. Passage retrieval is often cast as
a variation on document retrieval, where the docu-
ment retrieval model is applied only to a fragment of
the text. The applications include search snippet de-
tection, which aims to summarize the query-relevant
parts of a document. Scores under the passage model
can be combined with those from the containing
document to improve performance [6] or to include
quality indicators [5]. These approaches have been
adapted to retrieve answers for questions [1]. Pas-
sage retrieval models can be extended to combine
terms and entity-centric knowledge [8, 11]. For cer-
tain Wikipedia categories, template of articles can
be extracted and automatically populated [24]. Fur-
thermore, Banerjee and Mitra [4] found that training
a lexical classifier per section heading obtains good
results for article construction.

Neural-IR. With the comeback of neural networks,
the IR community is exploring pre-trained word-
vector approaches as well as dedicated neural net-
works for ranking. Pre-trained word- and entity-
embeddings are publicly available in the form of

2URL withheld for blind review

word2vec,3 GloVe,4 DESM,5 NTLM,6 wiki2vec,7 and
RDF2Vec8 vectors. Much of the work in this area
focuses on the applications of these shallow distribu-
tional models to IR tasks, although recently deeper
architectures have also been investigated [18, 26].
Query Expansion with KB. Pseudo relevance feed-
back [15] is one of the most popular query expansion
methods in which frequent words in top documents
of a first retrieval run are extracted. This idea is gen-
eralized to expansion with multiple sources [7] based
on terms and phrases. Recent developments in entity
linking algorithms and object retrieval make it fea-
sible to efficiently tap into the rich information pro-
vided by KBs [14, 17, 9], and exploit disambiguation
and confidences for query and document represen-
tation [13, 21]. Further work on entity aspects [22]
and effective learning-to-rank approaches for latent
entities [25] are a promising avenue.

3 Data Set

The goal of TREC CAR task is to, given a title and an
outline of section headings as a query, retrieve and
rank paragraphs from a provided collection [10].

The TREC CAR organizers process the English
Wikipedia to obtain only the articles with hierarchi-
cal section headings (discarding info boxes, images
and wrappers). From each article the outline of hier-
archical headings are retained, while all paragraphs
on the article are split out. All seven million para-
graphs from all Wikipedia pages are shuffled to form
the Paragraph collection.
Train data. For 50% of all articles, both outlines
and articles are made available as training data for
supervised machine learning as well as a resource
for the Rocchio method discussed below.
Test200. The organizers provide a manually selected
test collection of outlines from 200 articles. Together,

3https://code.google.com/archive/p/word2vec/
4http://nlp.stanford.edu/projects/glove/
5https://www.microsoft.com/en-us/download/

details.aspx?id=52597

6http://www.zuccon.net/ntlm.html
7https://github.com/idio/wiki2vec
8http://data.dws.informatik.uni-mannheim.de/

rdf2vec/

2

these 200 outlines include approximately 2300 head-
ings, each resulting in a query. This test data is com-
plemented with a ground truth file of which para-
graphs are relevant for a given heading, which we
base this study on.

Based on these queries, we experiment with dif-
ferent query expansion approaches and vector space
representations of queries and paragraphs (tf-idf,
GloVe embeddings and RDF2Vec embeddings). We
examine BM25, cosine similarity, learning to rank
[16] and a state-of-the-art neural network model
[19].

4 Experimentation

Environ-

ment

In this paper, we focus on the task of retrieving and
ranking paragraphs for each heading of the outline.
We consider the 2300 sections in Test200 corpus: First
we experiment in a “safe” experimentation environ-
ment before applying approaches the entire collec-
tion of seven million paragraphs. Code and data for
a experimentation environment are provided with
this publication.

The goal is to simulate a noisy candidate genera-
tion method which is guaranteed to include all rele-
vant paragraphs for the heading with a set of nearly-
relevant negatives.

For each heading, we construct a train set by se-
lecting, for every true paragraph under the heading,
five paragraphs from different sections of the article,
and five paragraphs from a different article.

Similarly we construct a test set that includes
all paragraphs from the article, as well as the same
amount of paragraphs drawn from other articles. All
articles are provided in random order. On average
this process yields a mean of 35 paragraphs per sec-
tion.

5 Examined Approaches

The setup of TREC CAR is a bit unusual as all head-
ings of an article are given at once, with the goal of
producing a separate ranking for each heading. In
this early work, we are breaking each outline into
several independent queries, one per heading h as
follows: We identify the path from the heading to
the root, and concatenate all all headings together
with the page title to obtain the query. For example,
if h corresponds to heading H2.3.4 the query is the
concatenation of H2.3.4, H2.3, H2 and the page title.

5.1 Query Expansion Techniques

We experiment three different query expansion ap-
proaches:
Expansion terms (RM1). Feedback terms are de-
rived using pseudo relevance feedback and the rele-
vance model [15]. Here we use Galago’s implemen-
tation9 which is based on a Dirichlet smoothed lan-
guage model for the feedback run. In the experimen-
tal setting, we achieve the best performance expand-
ing the queries with top 10 terms extracted from the
top 10 feedback paragraphs.
Expansion entities (ent-RM1). Another way of
expanding queries is by retrieving relevant entities.
As for retrieving supporting terms, we derive a set of
feedback entities by a search of the index using the
heading-query and deriving several entities. In the
experimental setting, best performance are achieved
using 10 entities.
Paragraph Rocchio. Inspired by the work of Baner-
jee and Mitra [4], we retrieve other paragraphs,
which have an identical heading to our heading-
query, from folds 1 to 4 of the collection (omitting
the fold where test200 originates from). For exam-
ple, given a query such as “Demographic”, regarding
the entity United States, we collect supporting para-
graphs from the pages of other entities (e.g., United
Kingdom), which have as well a a section titled “De-
mographic”. Headings are pre-processed with tokeni-
sation, stopword/digit removal and stemming. This
way, we can retrieve at least one supporting para-
graph for 1/3 of our heading-queries. In the experi-
mental setting, we test expansion using from 1 to 100
supporting passages and we obtain best performance
expanding the query with 5 passages.

9lemurproject.org/galago.php

3

5.2 Vector Space Representations
We study three variations for representing the con-
tent in the vector space model:
TF-IDF. Representing each word in the vocabulary
as its own dimension in the vector space, queries and
paragraphs are represented as their TF-IDF vector.
We are using the logarithmic L2-normalised variant.
We perform stemming as a pre-processing step.
Word Embeddings. Using the pre-trained word
embedding GloVE [20] of 300 dimensions, every
word w in query or paragraph is represented as a
K-dimensional vector (cid:126)w. A vector representation
for the whole paragraph (cid:126)d (complete query (cid:126)q) is ob-
tained by a weighted element-wise average of word
vectors (cid:126)w in the paragraph (query). To give more
attention to infrequent word, we use the TF-IDF of
each word w to weights.

(cid:126)d =

1
|d|

(cid:88)

w∈d

TF-IDF(w) · (cid:126)w

Entity Embeddings. Queries and paragraphs are
represented as their mentioned DBpedia entities, us-
ing the entity linker TagMe [12] (with default pa-
rameters). Next, we obtain latent vector representa-
tions (cid:126)e of each linked entity e using pre-computed
RDF2Vec 300d entity embeddings [23]. Vector repre-
sentations of paragraphs (cid:126)d (queries (cid:126)q) are computed
by a weighted element-wise average of entity vectors
(cid:126)e. By casting a paragraph as a bag-of-links we adapt
TF-IDF to entity links (link statistics taken from DB-
pedia 2015-04 [3]:

(cid:126)d =

1
| {e ∈ d} |

(cid:88)

e∈d

TF-IDF(e) · (cid:126)e

Learning to Rank. We combine the ranking scores
of different baselines with supervised machine learn-
ing in a learning-to-rank setting, for producing a fi-
nal ranking of relevant paragraphs. We use RankLib
10 with 5-fold cross validation using a linear model
optimized for MAP, trained with coordinate ascent.
Deep Neural Network. The Duet model is a state-
of-the-art deep neural network (DNN) recently pro-
posed by Mitra et al. [19] for ad-hoc retrieval. The
Duet architecture learns to model query-paragraph
relevance by jointly learning good representations
of the query and the paragraph text for matching, as
well as by learning to identify good patterns of exact
matches of the query terms in the paragraph text.
We use the Duet implementation available publicly11
under the MIT license for our experiments.

Training on folds 1 to 4 of the collection, we only
consider the first ten words for the query and the
first 100 words for the passage as inputs. We use 64
hidden units in the different layers of the network,
as opposed to 300 in the original paper, to reduce the
total number of learnable parameters of the model.
We trained the model for 32 epochs with a learning
rate of 0.001 which was picked based on a subset of
the training data. Each epoch was trained over 1024
minibatches, and each minibatch contained 1024 sam-
ples. Each training sample was a triplet consisting of
a query, a positive passage, and a negative passage.
The training time was limited to 60 hours.

6 Evaluation

We present evaluation results both on the experi-
mentation environment and on the entire paragraph
collection.

5.3 Ranking Approaches
We test four different ranking approaches:
Okapi BM25. Results are ranked using Okapi BM25
with k1=1.2 and b=0.75, using the implementation of
Lucene 6.4.1. Porter stemming and stopword removal
was applied to paragraphs and queries.
Cosine Similarity. Paragraphs are ranked by co-
sine similarity (cs) between respective vector repre-
sentations of the query and the paragraph.

6.1 Experimentation Environment
The experimentation environment (Section 4) pro-
vides a “safe environment” by simulating a noisy
candidate method for each section. Results are pre-
sented in table 1. The approach bm25 query only sets
the baseline of our work.

10lemurproject.org/ranklib.php
11https://github.com/bmitra-msft/NDRM/blob/

master/notebooks/Duet.ipynb

4

Table 1: Results on experimentation environment.

Table 2: Results of the initial candidate selection.

MAP R-Prec MRR

0.320

0.232

0.409

Heading Retrieval - bm25
query only

MAP

R-Prec MRR

0.150

0.118

0.216

Heading Retrieval - tf-idf (cs)
query only
query + Rocchio

0.035
0.029

0.025
0.020

0.053
0.041

bm25
query only

tf-idf (cs)
query only
query + RM1
query + Rocchio

GloVe (cs)
query only
query + RM1
query + Rocchio

RDF2Vec (cs)
entity-query only
ent-query + ent-RM1
ent-query + ent-Rocchio

Learning to Rank
all (cs) scores

Duet model
query only

0.350
0.324
0.400

0.329
0.289
0.349

0.313
0.320
0.316

0.212
0.205
0.286

0.210
0.177
0.236

0.200
0.208
0.206

0.383
0.384
0.466

0.387
0.339
0.410

0.369
0.377
0.375

0.412

0.290

0.475

0.470

0.359

0.553

Not all query expansion approaches and vector
space representation methods improve over this base-
line. This is particularly true for query expansion
with terms or entities (through RM3 = query + RM1)
as well as RDF2Vec embeddings. On the contrary, the
most promising results among the methods which
employ cosine similarity as a ranking function, are
obtained when the query is expanded with Rocchio
vectors trained on paragraphs from sections with
the same heading. This finding reconfirms the re-
sults of previous work on the automatic generation
of Wikipedia articles based its structural information
[4]. The results show that common traits between
Wikipedia sections with the same heading are better
captured using the tf-idf word vector than through
word- and entity- embedding vectors suggest that a
possible improvement over these baselines could by
obtained by training embeddings for this task.

In comparison to these unsupervised retrieval
models, both supervised Learning to Rank and the
Neural Duet model out-perform all previously de-
scribed baselines. In particular, the Duet model yields

Figure 1: Effect of training data size on the perfor-
mance of the Duet model. Training on four folds,
reporting MAP on holdout fold.

a substantial improvement over all presented ap-
proaches, showing the potential of neural-IR for the
task. It is important to remark that neural deep mod-
els take days to train even on a GPU. In addition
they are data-hungry, with performances improving
significantly with more training data as shown in
Figure 1.

6.2 Experiments on Paragraph Collec-

tion

As a second step, we conduct experiments on the
entire paragraph collection.
Candidate Selection. Many of the previously pre-
sented methods, such as the Duet model, require a
candidate generating method. We test three candi-
date methods: bm25, tf-idf, and tf-idf with rocchio
expansion. For each query, the methods produce a

5

Table 3: Results on the Paragraph Collection after candidate method. (cid:72) Worse according to paired-t-test
with α = 5%.

bm25 candidate

tf-idf (cs)
bm25 + query + Rocchio

GloVe (cs)
bm25 + query + Rocchio

Duet model
bm25 + query only

MAP
0.150(cid:72)

R-Prec MRR

0.118

0.216

0.140(cid:72)

0.113

0.207

0.150(cid:72)

0.120

0.218

w/o bm25
MAP

-

0.085

0.082

0.161

0.130

0.229

0.093

candidate set of 100 paragraphs. The results are pre-
sented in Table 2. While this is a challenging task,
encouraging performance are obtained with the use
of bm25 query only. However as half of the queries
do not have any relevant paragraphs in the candidate
set, the theoretically achievable MRR is 0.50.

As a final experiment, we use bm25 query only as a
candidate method to obtain 100 paragraphs per query.
The three of the most-promising systems presented
above are re-evaluated, using learning to rank to
combine the scores for bm25, query, and Rocchio
expansion with 5-fold cross validation.

The combination of bm25 score and duet model
is significantly outperforming all other methods,
demonstrating the strength of the candidate method.
If the combination with the bm25 score is left out,
all methods are significantly loosing in performance
(see last column in Table 3).

7 Conclusions

In this paper, we present the performance of a va-
riety of approaches, from established baselines to
more advanced systems, in the context of the new
TREC-CAR track on Complex Answer Retrieval. Our
results show the effects of different query expansion
and embedding approaches in comparison to learn-
ing to rank and neural ranking models. The public
availability of this new dataset, gave us the oppor-
tunity to offer a benchmark on the most promising
approaches for tackling this problem to future par-

ticipants of the track and the IR community.

References
[1] Elif Aktolga, James Allan, and David A. Smith.
2011. Passage reranking for question answering
using syntactic structures and answer types. In
Advances in Information Retrieval. Springer.

[2] James Allan, Bruce Croft, Alistair Moffat, and
Mark Sanderson. 2012. Frontiers, challenges,
and opportunities for information retrieval: Re-
port from SWIRL 2012 the second strategic
workshop on information retrieval in Lorne. In
ACM SIGIR Forum, Vol. 46. ACM, 2–32.

[3] S¨oren Auer, Christian Bizer, Georgi Kobilarov,
Jens Lehmann, Richard Cyganiak, and Zachary
Ives. 2007. Dbpedia: A nucleus for a web of open
data. Springer.

[4] Siddhartha Banerjee and Prasenjit Mitra. 2015.
WikiKreator: Improving Wikipedia Stubs Auto-
matically.. In ACL (1). 867–877.

[5] Michael Bendersky, W Bruce Croft, and Yan-
lei Diao. 2011. Quality-biased ranking of web
documents. In WSDM.

[6] Michael Bendersky and Oren Kurland. 2008. Uti-
lizing passage-based language models for doc-
In Advances in Information
ument retrieval.
Retrieval. Springer.

6

[18] Bhaskar Mitra and Nick Craswell. 2017. Neural
Models for Information Retrieval. arXiv preprint
arXiv:1705.01509 (2017).

[19] Bhaskar Mitra, Fernando Diaz, and Nick
Craswell. 2017. Learning to Match Using Lo-
cal and Distributed Representations of Text for
Web Search. In www.

[20] Jeffrey Pennington, Richard Socher, and
Christopher D Manning. 2014. Glove: Global
Vectors for Word Representation.. In EMNLP,
Vol. 14.

[21] Hadas Raviv, Oren Kurland, and David Carmel.
2016. Document Retrieval Using Entity-Based
Language Models. In SIGIR.

[22] Ridho Reinanda, Edgar Meij, and Maarten de Ri-
jke. 2015. Mining, ranking and recommending
entity aspects. In SIGIR.

[23] Petar Ristoski and Heiko Paulheim. 2016.
Rdf2vec: Rdf graph embeddings for data mining.
In ISWC. Springer.

[24] Christina Sauper and Regina Barzilay. 2009. Au-
tomatically generating Wikipedia articles: A
structure-aware approach. In IJCNLP.

[25] Chenyan Xiong and Jamie Callan. 2015. Es-
drank: Connecting query and documents
through external semi-structured data.
In
CIKM.

[26] Ye Zhang, Md Mustafizur Rahman, Alex Bray-
lan, Brandon Dang, Heng-Lu Chang, Henna
Kim, Quinten McNamara, Aaron Angert, Ed-
ward Banner, Vivek Khetan, and others. 2016.
Neural Information Retrieval: A Literature Re-
view. arXiv preprint arXiv:1611.06792 (2016).

[7] Michael Bendersky, Donald Metzler, and
W Bruce Croft. 2012. Effective query formu-
lation with multiple information sources. In
WSDM.

[8] Roi Blanco and Hugo Zaragoza. 2010. Finding
support sentences for entities. In SIGIR.

[9] Jeffrey Dalton, Laura Dietz, and James Allan.
2014. Entity Query Feature Expansion Using
Knowledge Base Links. In SIGIR.

[10] Laura Dietz and Ben Gamari. 2017. TREC CAR:
A Data Set for Complex Answer Retrieval. Ver-
sion 1.4. (2017). http://trec-car.cs.unh.
edu

[11] Laura Dietz and Michael Schuhmacher. 2015.
An interface sketch for queripidia: Query-
driven knowledge portfolios from the web. In
Proc. Workshop on Exploiting Semantic Annota-
tions in IR. ACM, 43–46.

[12] Paolo Ferragina and Ugo Scaiella. 2010. Tagme:
on-the-fly annotation of short text fragments
(by wikipedia entities). In CIKM. ACM.

[13] Faegheh Hasibi, Krisztian Balog, and Svein Erik
Bratsberg. 2016. Exploiting Entity Linking in
Queries for Entity Retrieval. In ICTIR. 209–218.

[14] Alexander Kotov and ChengXiang Zhai. 2012.
Tapping into knowledge base for concept feed-
back: leveraging conceptnet to improve search
results for difficult queries. In WSDM.

[15] Victor Lavrenko and W Bruce Croft. 2001. Rel-
evance based language models. In SIGIR.

[16] Hang Li. 2014. Learning to rank for information
retrieval and natural language processing. Syn-
thesis Lectures on Human Language Technologies
7, 3 (2014).

[17] Xitong Liu and Hui Fang. 2015. Latent entity
space: a novel retrieval approach for entity-
bearing queries. Information Retrieval Journal
18, 6 (2015).

7

7
1
0
2
 
y
a
M
 
3
1
 
 
]

R

I
.
s
c
[
 
 
1
v
3
0
8
4
0
.
5
0
7
1
:
v
i
X
r
a

Benchmark for Complex Answer Retrieval

Federico Nanni
University of Mannheim
fede@informatik.uni-mannheim.de

Bhaskar Mitra
Microsoft, UCL∗
bmitra@microsoft.com

Matt Magnusson
University of New Hampshire
magnusson3@gmail.com

Laura Dietz
University of New Hampshire
dietz@cs.unh.edu

Abstract

Retrieving paragraphs to populate a Wikipedia ar-
ticle is a challenging task. The new TREC Complex
Answer Retrieval (TREC CAR) track introduces a
comprehensive dataset that targets this retrieval sce-
nario. We present early results from a variety of
approaches – from standard information retrieval
methods (e.g., tf-idf) to complex systems that using
query expansion using knowledge bases and deep
neural networks. The goal is to offer future partici-
pants of this track an overview of some promising
approaches to tackle this problem.

1 Introduction

Over the last two decades, research in information
retrieval (IR) has developed a variety of approaches
for answering queries regarding precise facts – such
as “Population New York City”, “Who is Bill de Bla-
sio?” or “Neighborhoods in Manhattan” – via the
identification, extraction, and synthesis of pieces of
information from textual sources. However, for more
complex queries, such as “Benefits of immigration
for NYC culture” current systems still rely on pre-
senting the traditional ten blue links to the user as
an answer.

To solicit works in this direction, and following
the output of the recent SWIRL 2012 workshop on

frontiers, challenges, and opportunities for infor-
mation retrieval report [2], a new TREC track on
Complex Answer Retrieval1 (TREC CAR) for open-
domain queries has been recently introduced [10].

The task and related dataset are based on the as-
sumption that each Wikipedia page represents a com-
plex topic, with further details under each sections.
Accordingly, paragraphs contained in a section such
as “Cultural diversity – Demographic” of the page
“Culture of New York City”, offer one aspect of the
open-domain query “Culture of New York City”. The
goal of the task is presented as such: given an outline
of a page (in the form of the page title and hierarchi-
cal section headings), retrieve a ranking of passages
for each section. While assessed manually in the fu-
ture, in this work, a passage is relevant for a section
if and only if it is contained in the original article in
the corresponding section.
Contribution. Many different approaches can be
applied to this problem. This can address the ways a
query can be expanded (using textual or structural
information from knowledge bases), the way query
and passages can be represented as vectors (e.g., word
embedding vectors), and applications of deep neural
networks and learning to rank. Given the recent
release of the dataset, with this work we intend to
support the future participants of the TREC CAR
task by studying the performance of a variety of
methods—to highlight which of them may be the
most promising directions.

∗The author is a part-time PhD student at University College

London.

1http://trec-car.cs.unh.edu/

1

Additionally with the publication of this paper, we
make our experimentation environment and dataset
available2, which simulates a noisy candidate gener-
ation method for this task.

Outline. We give an overview of related work in Sec-
tion 2, describe the data set and our experimentation
environment in Section 3 and 4. Section 5 provides
details of the approaches. We evaluate empirically
in Section 6 before concluding the paper.

2 Related Work

A wide variety of approaches are applicable to the
TREC CAR problem. In the following, we cover three
central ones, namely passage retrieval, query expan-
sion using knowledge bases and the recent advance-
ment in the use of deep neural network models for
information retrieval.

Passage Retrieval. Passage retrieval is often cast as
a variation on document retrieval, where the docu-
ment retrieval model is applied only to a fragment of
the text. The applications include search snippet de-
tection, which aims to summarize the query-relevant
parts of a document. Scores under the passage model
can be combined with those from the containing
document to improve performance [6] or to include
quality indicators [5]. These approaches have been
adapted to retrieve answers for questions [1]. Pas-
sage retrieval models can be extended to combine
terms and entity-centric knowledge [8, 11]. For cer-
tain Wikipedia categories, template of articles can
be extracted and automatically populated [24]. Fur-
thermore, Banerjee and Mitra [4] found that training
a lexical classifier per section heading obtains good
results for article construction.

Neural-IR. With the comeback of neural networks,
the IR community is exploring pre-trained word-
vector approaches as well as dedicated neural net-
works for ranking. Pre-trained word- and entity-
embeddings are publicly available in the form of

2URL withheld for blind review

word2vec,3 GloVe,4 DESM,5 NTLM,6 wiki2vec,7 and
RDF2Vec8 vectors. Much of the work in this area
focuses on the applications of these shallow distribu-
tional models to IR tasks, although recently deeper
architectures have also been investigated [18, 26].
Query Expansion with KB. Pseudo relevance feed-
back [15] is one of the most popular query expansion
methods in which frequent words in top documents
of a first retrieval run are extracted. This idea is gen-
eralized to expansion with multiple sources [7] based
on terms and phrases. Recent developments in entity
linking algorithms and object retrieval make it fea-
sible to efficiently tap into the rich information pro-
vided by KBs [14, 17, 9], and exploit disambiguation
and confidences for query and document represen-
tation [13, 21]. Further work on entity aspects [22]
and effective learning-to-rank approaches for latent
entities [25] are a promising avenue.

3 Data Set

The goal of TREC CAR task is to, given a title and an
outline of section headings as a query, retrieve and
rank paragraphs from a provided collection [10].

The TREC CAR organizers process the English
Wikipedia to obtain only the articles with hierarchi-
cal section headings (discarding info boxes, images
and wrappers). From each article the outline of hier-
archical headings are retained, while all paragraphs
on the article are split out. All seven million para-
graphs from all Wikipedia pages are shuffled to form
the Paragraph collection.
Train data. For 50% of all articles, both outlines
and articles are made available as training data for
supervised machine learning as well as a resource
for the Rocchio method discussed below.
Test200. The organizers provide a manually selected
test collection of outlines from 200 articles. Together,

3https://code.google.com/archive/p/word2vec/
4http://nlp.stanford.edu/projects/glove/
5https://www.microsoft.com/en-us/download/

details.aspx?id=52597

6http://www.zuccon.net/ntlm.html
7https://github.com/idio/wiki2vec
8http://data.dws.informatik.uni-mannheim.de/

rdf2vec/

2

these 200 outlines include approximately 2300 head-
ings, each resulting in a query. This test data is com-
plemented with a ground truth file of which para-
graphs are relevant for a given heading, which we
base this study on.

Based on these queries, we experiment with dif-
ferent query expansion approaches and vector space
representations of queries and paragraphs (tf-idf,
GloVe embeddings and RDF2Vec embeddings). We
examine BM25, cosine similarity, learning to rank
[16] and a state-of-the-art neural network model
[19].

4 Experimentation

Environ-

ment

In this paper, we focus on the task of retrieving and
ranking paragraphs for each heading of the outline.
We consider the 2300 sections in Test200 corpus: First
we experiment in a “safe” experimentation environ-
ment before applying approaches the entire collec-
tion of seven million paragraphs. Code and data for
a experimentation environment are provided with
this publication.

The goal is to simulate a noisy candidate genera-
tion method which is guaranteed to include all rele-
vant paragraphs for the heading with a set of nearly-
relevant negatives.

For each heading, we construct a train set by se-
lecting, for every true paragraph under the heading,
five paragraphs from different sections of the article,
and five paragraphs from a different article.

Similarly we construct a test set that includes
all paragraphs from the article, as well as the same
amount of paragraphs drawn from other articles. All
articles are provided in random order. On average
this process yields a mean of 35 paragraphs per sec-
tion.

5 Examined Approaches

The setup of TREC CAR is a bit unusual as all head-
ings of an article are given at once, with the goal of
producing a separate ranking for each heading. In
this early work, we are breaking each outline into
several independent queries, one per heading h as
follows: We identify the path from the heading to
the root, and concatenate all all headings together
with the page title to obtain the query. For example,
if h corresponds to heading H2.3.4 the query is the
concatenation of H2.3.4, H2.3, H2 and the page title.

5.1 Query Expansion Techniques

We experiment three different query expansion ap-
proaches:
Expansion terms (RM1). Feedback terms are de-
rived using pseudo relevance feedback and the rele-
vance model [15]. Here we use Galago’s implemen-
tation9 which is based on a Dirichlet smoothed lan-
guage model for the feedback run. In the experimen-
tal setting, we achieve the best performance expand-
ing the queries with top 10 terms extracted from the
top 10 feedback paragraphs.
Expansion entities (ent-RM1). Another way of
expanding queries is by retrieving relevant entities.
As for retrieving supporting terms, we derive a set of
feedback entities by a search of the index using the
heading-query and deriving several entities. In the
experimental setting, best performance are achieved
using 10 entities.
Paragraph Rocchio. Inspired by the work of Baner-
jee and Mitra [4], we retrieve other paragraphs,
which have an identical heading to our heading-
query, from folds 1 to 4 of the collection (omitting
the fold where test200 originates from). For exam-
ple, given a query such as “Demographic”, regarding
the entity United States, we collect supporting para-
graphs from the pages of other entities (e.g., United
Kingdom), which have as well a a section titled “De-
mographic”. Headings are pre-processed with tokeni-
sation, stopword/digit removal and stemming. This
way, we can retrieve at least one supporting para-
graph for 1/3 of our heading-queries. In the experi-
mental setting, we test expansion using from 1 to 100
supporting passages and we obtain best performance
expanding the query with 5 passages.

9lemurproject.org/galago.php

3

5.2 Vector Space Representations
We study three variations for representing the con-
tent in the vector space model:
TF-IDF. Representing each word in the vocabulary
as its own dimension in the vector space, queries and
paragraphs are represented as their TF-IDF vector.
We are using the logarithmic L2-normalised variant.
We perform stemming as a pre-processing step.
Word Embeddings. Using the pre-trained word
embedding GloVE [20] of 300 dimensions, every
word w in query or paragraph is represented as a
K-dimensional vector (cid:126)w. A vector representation
for the whole paragraph (cid:126)d (complete query (cid:126)q) is ob-
tained by a weighted element-wise average of word
vectors (cid:126)w in the paragraph (query). To give more
attention to infrequent word, we use the TF-IDF of
each word w to weights.

(cid:126)d =

1
|d|

(cid:88)

w∈d

TF-IDF(w) · (cid:126)w

Entity Embeddings. Queries and paragraphs are
represented as their mentioned DBpedia entities, us-
ing the entity linker TagMe [12] (with default pa-
rameters). Next, we obtain latent vector representa-
tions (cid:126)e of each linked entity e using pre-computed
RDF2Vec 300d entity embeddings [23]. Vector repre-
sentations of paragraphs (cid:126)d (queries (cid:126)q) are computed
by a weighted element-wise average of entity vectors
(cid:126)e. By casting a paragraph as a bag-of-links we adapt
TF-IDF to entity links (link statistics taken from DB-
pedia 2015-04 [3]:

(cid:126)d =

1
| {e ∈ d} |

(cid:88)

e∈d

TF-IDF(e) · (cid:126)e

Learning to Rank. We combine the ranking scores
of different baselines with supervised machine learn-
ing in a learning-to-rank setting, for producing a fi-
nal ranking of relevant paragraphs. We use RankLib
10 with 5-fold cross validation using a linear model
optimized for MAP, trained with coordinate ascent.
Deep Neural Network. The Duet model is a state-
of-the-art deep neural network (DNN) recently pro-
posed by Mitra et al. [19] for ad-hoc retrieval. The
Duet architecture learns to model query-paragraph
relevance by jointly learning good representations
of the query and the paragraph text for matching, as
well as by learning to identify good patterns of exact
matches of the query terms in the paragraph text.
We use the Duet implementation available publicly11
under the MIT license for our experiments.

Training on folds 1 to 4 of the collection, we only
consider the first ten words for the query and the
first 100 words for the passage as inputs. We use 64
hidden units in the different layers of the network,
as opposed to 300 in the original paper, to reduce the
total number of learnable parameters of the model.
We trained the model for 32 epochs with a learning
rate of 0.001 which was picked based on a subset of
the training data. Each epoch was trained over 1024
minibatches, and each minibatch contained 1024 sam-
ples. Each training sample was a triplet consisting of
a query, a positive passage, and a negative passage.
The training time was limited to 60 hours.

6 Evaluation

We present evaluation results both on the experi-
mentation environment and on the entire paragraph
collection.

5.3 Ranking Approaches
We test four different ranking approaches:
Okapi BM25. Results are ranked using Okapi BM25
with k1=1.2 and b=0.75, using the implementation of
Lucene 6.4.1. Porter stemming and stopword removal
was applied to paragraphs and queries.
Cosine Similarity. Paragraphs are ranked by co-
sine similarity (cs) between respective vector repre-
sentations of the query and the paragraph.

6.1 Experimentation Environment
The experimentation environment (Section 4) pro-
vides a “safe environment” by simulating a noisy
candidate method for each section. Results are pre-
sented in table 1. The approach bm25 query only sets
the baseline of our work.

10lemurproject.org/ranklib.php
11https://github.com/bmitra-msft/NDRM/blob/

master/notebooks/Duet.ipynb

4

Table 1: Results on experimentation environment.

Table 2: Results of the initial candidate selection.

MAP R-Prec MRR

0.320

0.232

0.409

Heading Retrieval - bm25
query only

MAP

R-Prec MRR

0.150

0.118

0.216

Heading Retrieval - tf-idf (cs)
query only
query + Rocchio

0.035
0.029

0.025
0.020

0.053
0.041

bm25
query only

tf-idf (cs)
query only
query + RM1
query + Rocchio

GloVe (cs)
query only
query + RM1
query + Rocchio

RDF2Vec (cs)
entity-query only
ent-query + ent-RM1
ent-query + ent-Rocchio

Learning to Rank
all (cs) scores

Duet model
query only

0.350
0.324
0.400

0.329
0.289
0.349

0.313
0.320
0.316

0.212
0.205
0.286

0.210
0.177
0.236

0.200
0.208
0.206

0.383
0.384
0.466

0.387
0.339
0.410

0.369
0.377
0.375

0.412

0.290

0.475

0.470

0.359

0.553

Not all query expansion approaches and vector
space representation methods improve over this base-
line. This is particularly true for query expansion
with terms or entities (through RM3 = query + RM1)
as well as RDF2Vec embeddings. On the contrary, the
most promising results among the methods which
employ cosine similarity as a ranking function, are
obtained when the query is expanded with Rocchio
vectors trained on paragraphs from sections with
the same heading. This finding reconfirms the re-
sults of previous work on the automatic generation
of Wikipedia articles based its structural information
[4]. The results show that common traits between
Wikipedia sections with the same heading are better
captured using the tf-idf word vector than through
word- and entity- embedding vectors suggest that a
possible improvement over these baselines could by
obtained by training embeddings for this task.

In comparison to these unsupervised retrieval
models, both supervised Learning to Rank and the
Neural Duet model out-perform all previously de-
scribed baselines. In particular, the Duet model yields

Figure 1: Effect of training data size on the perfor-
mance of the Duet model. Training on four folds,
reporting MAP on holdout fold.

a substantial improvement over all presented ap-
proaches, showing the potential of neural-IR for the
task. It is important to remark that neural deep mod-
els take days to train even on a GPU. In addition
they are data-hungry, with performances improving
significantly with more training data as shown in
Figure 1.

6.2 Experiments on Paragraph Collec-

tion

As a second step, we conduct experiments on the
entire paragraph collection.
Candidate Selection. Many of the previously pre-
sented methods, such as the Duet model, require a
candidate generating method. We test three candi-
date methods: bm25, tf-idf, and tf-idf with rocchio
expansion. For each query, the methods produce a

5

Table 3: Results on the Paragraph Collection after candidate method. (cid:72) Worse according to paired-t-test
with α = 5%.

bm25 candidate

tf-idf (cs)
bm25 + query + Rocchio

GloVe (cs)
bm25 + query + Rocchio

Duet model
bm25 + query only

MAP
0.150(cid:72)

R-Prec MRR

0.118

0.216

0.140(cid:72)

0.113

0.207

0.150(cid:72)

0.120

0.218

w/o bm25
MAP

-

0.085

0.082

0.161

0.130

0.229

0.093

candidate set of 100 paragraphs. The results are pre-
sented in Table 2. While this is a challenging task,
encouraging performance are obtained with the use
of bm25 query only. However as half of the queries
do not have any relevant paragraphs in the candidate
set, the theoretically achievable MRR is 0.50.

As a final experiment, we use bm25 query only as a
candidate method to obtain 100 paragraphs per query.
The three of the most-promising systems presented
above are re-evaluated, using learning to rank to
combine the scores for bm25, query, and Rocchio
expansion with 5-fold cross validation.

The combination of bm25 score and duet model
is significantly outperforming all other methods,
demonstrating the strength of the candidate method.
If the combination with the bm25 score is left out,
all methods are significantly loosing in performance
(see last column in Table 3).

7 Conclusions

In this paper, we present the performance of a va-
riety of approaches, from established baselines to
more advanced systems, in the context of the new
TREC-CAR track on Complex Answer Retrieval. Our
results show the effects of different query expansion
and embedding approaches in comparison to learn-
ing to rank and neural ranking models. The public
availability of this new dataset, gave us the oppor-
tunity to offer a benchmark on the most promising
approaches for tackling this problem to future par-

ticipants of the track and the IR community.

References
[1] Elif Aktolga, James Allan, and David A. Smith.
2011. Passage reranking for question answering
using syntactic structures and answer types. In
Advances in Information Retrieval. Springer.

[2] James Allan, Bruce Croft, Alistair Moffat, and
Mark Sanderson. 2012. Frontiers, challenges,
and opportunities for information retrieval: Re-
port from SWIRL 2012 the second strategic
workshop on information retrieval in Lorne. In
ACM SIGIR Forum, Vol. 46. ACM, 2–32.

[3] S¨oren Auer, Christian Bizer, Georgi Kobilarov,
Jens Lehmann, Richard Cyganiak, and Zachary
Ives. 2007. Dbpedia: A nucleus for a web of open
data. Springer.

[4] Siddhartha Banerjee and Prasenjit Mitra. 2015.
WikiKreator: Improving Wikipedia Stubs Auto-
matically.. In ACL (1). 867–877.

[5] Michael Bendersky, W Bruce Croft, and Yan-
lei Diao. 2011. Quality-biased ranking of web
documents. In WSDM.

[6] Michael Bendersky and Oren Kurland. 2008. Uti-
lizing passage-based language models for doc-
In Advances in Information
ument retrieval.
Retrieval. Springer.

6

[18] Bhaskar Mitra and Nick Craswell. 2017. Neural
Models for Information Retrieval. arXiv preprint
arXiv:1705.01509 (2017).

[19] Bhaskar Mitra, Fernando Diaz, and Nick
Craswell. 2017. Learning to Match Using Lo-
cal and Distributed Representations of Text for
Web Search. In www.

[20] Jeffrey Pennington, Richard Socher, and
Christopher D Manning. 2014. Glove: Global
Vectors for Word Representation.. In EMNLP,
Vol. 14.

[21] Hadas Raviv, Oren Kurland, and David Carmel.
2016. Document Retrieval Using Entity-Based
Language Models. In SIGIR.

[22] Ridho Reinanda, Edgar Meij, and Maarten de Ri-
jke. 2015. Mining, ranking and recommending
entity aspects. In SIGIR.

[23] Petar Ristoski and Heiko Paulheim. 2016.
Rdf2vec: Rdf graph embeddings for data mining.
In ISWC. Springer.

[24] Christina Sauper and Regina Barzilay. 2009. Au-
tomatically generating Wikipedia articles: A
structure-aware approach. In IJCNLP.

[25] Chenyan Xiong and Jamie Callan. 2015. Es-
drank: Connecting query and documents
through external semi-structured data.
In
CIKM.

[26] Ye Zhang, Md Mustafizur Rahman, Alex Bray-
lan, Brandon Dang, Heng-Lu Chang, Henna
Kim, Quinten McNamara, Aaron Angert, Ed-
ward Banner, Vivek Khetan, and others. 2016.
Neural Information Retrieval: A Literature Re-
view. arXiv preprint arXiv:1611.06792 (2016).

[7] Michael Bendersky, Donald Metzler, and
W Bruce Croft. 2012. Effective query formu-
lation with multiple information sources. In
WSDM.

[8] Roi Blanco and Hugo Zaragoza. 2010. Finding
support sentences for entities. In SIGIR.

[9] Jeffrey Dalton, Laura Dietz, and James Allan.
2014. Entity Query Feature Expansion Using
Knowledge Base Links. In SIGIR.

[10] Laura Dietz and Ben Gamari. 2017. TREC CAR:
A Data Set for Complex Answer Retrieval. Ver-
sion 1.4. (2017). http://trec-car.cs.unh.
edu

[11] Laura Dietz and Michael Schuhmacher. 2015.
An interface sketch for queripidia: Query-
driven knowledge portfolios from the web. In
Proc. Workshop on Exploiting Semantic Annota-
tions in IR. ACM, 43–46.

[12] Paolo Ferragina and Ugo Scaiella. 2010. Tagme:
on-the-fly annotation of short text fragments
(by wikipedia entities). In CIKM. ACM.

[13] Faegheh Hasibi, Krisztian Balog, and Svein Erik
Bratsberg. 2016. Exploiting Entity Linking in
Queries for Entity Retrieval. In ICTIR. 209–218.

[14] Alexander Kotov and ChengXiang Zhai. 2012.
Tapping into knowledge base for concept feed-
back: leveraging conceptnet to improve search
results for difficult queries. In WSDM.

[15] Victor Lavrenko and W Bruce Croft. 2001. Rel-
evance based language models. In SIGIR.

[16] Hang Li. 2014. Learning to rank for information
retrieval and natural language processing. Syn-
thesis Lectures on Human Language Technologies
7, 3 (2014).

[17] Xitong Liu and Hui Fang. 2015. Latent entity
space: a novel retrieval approach for entity-
bearing queries. Information Retrieval Journal
18, 6 (2015).

7

7
1
0
2
 
y
a
M
 
3
1
 
 
]

R

I
.
s
c
[
 
 
1
v
3
0
8
4
0
.
5
0
7
1
:
v
i
X
r
a

Benchmark for Complex Answer Retrieval

Federico Nanni
University of Mannheim
fede@informatik.uni-mannheim.de

Bhaskar Mitra
Microsoft, UCL∗
bmitra@microsoft.com

Matt Magnusson
University of New Hampshire
magnusson3@gmail.com

Laura Dietz
University of New Hampshire
dietz@cs.unh.edu

Abstract

Retrieving paragraphs to populate a Wikipedia ar-
ticle is a challenging task. The new TREC Complex
Answer Retrieval (TREC CAR) track introduces a
comprehensive dataset that targets this retrieval sce-
nario. We present early results from a variety of
approaches – from standard information retrieval
methods (e.g., tf-idf) to complex systems that using
query expansion using knowledge bases and deep
neural networks. The goal is to offer future partici-
pants of this track an overview of some promising
approaches to tackle this problem.

1 Introduction

Over the last two decades, research in information
retrieval (IR) has developed a variety of approaches
for answering queries regarding precise facts – such
as “Population New York City”, “Who is Bill de Bla-
sio?” or “Neighborhoods in Manhattan” – via the
identification, extraction, and synthesis of pieces of
information from textual sources. However, for more
complex queries, such as “Benefits of immigration
for NYC culture” current systems still rely on pre-
senting the traditional ten blue links to the user as
an answer.

To solicit works in this direction, and following
the output of the recent SWIRL 2012 workshop on

frontiers, challenges, and opportunities for infor-
mation retrieval report [2], a new TREC track on
Complex Answer Retrieval1 (TREC CAR) for open-
domain queries has been recently introduced [10].

The task and related dataset are based on the as-
sumption that each Wikipedia page represents a com-
plex topic, with further details under each sections.
Accordingly, paragraphs contained in a section such
as “Cultural diversity – Demographic” of the page
“Culture of New York City”, offer one aspect of the
open-domain query “Culture of New York City”. The
goal of the task is presented as such: given an outline
of a page (in the form of the page title and hierarchi-
cal section headings), retrieve a ranking of passages
for each section. While assessed manually in the fu-
ture, in this work, a passage is relevant for a section
if and only if it is contained in the original article in
the corresponding section.
Contribution. Many different approaches can be
applied to this problem. This can address the ways a
query can be expanded (using textual or structural
information from knowledge bases), the way query
and passages can be represented as vectors (e.g., word
embedding vectors), and applications of deep neural
networks and learning to rank. Given the recent
release of the dataset, with this work we intend to
support the future participants of the TREC CAR
task by studying the performance of a variety of
methods—to highlight which of them may be the
most promising directions.

∗The author is a part-time PhD student at University College

London.

1http://trec-car.cs.unh.edu/

1

Additionally with the publication of this paper, we
make our experimentation environment and dataset
available2, which simulates a noisy candidate gener-
ation method for this task.

Outline. We give an overview of related work in Sec-
tion 2, describe the data set and our experimentation
environment in Section 3 and 4. Section 5 provides
details of the approaches. We evaluate empirically
in Section 6 before concluding the paper.

2 Related Work

A wide variety of approaches are applicable to the
TREC CAR problem. In the following, we cover three
central ones, namely passage retrieval, query expan-
sion using knowledge bases and the recent advance-
ment in the use of deep neural network models for
information retrieval.

Passage Retrieval. Passage retrieval is often cast as
a variation on document retrieval, where the docu-
ment retrieval model is applied only to a fragment of
the text. The applications include search snippet de-
tection, which aims to summarize the query-relevant
parts of a document. Scores under the passage model
can be combined with those from the containing
document to improve performance [6] or to include
quality indicators [5]. These approaches have been
adapted to retrieve answers for questions [1]. Pas-
sage retrieval models can be extended to combine
terms and entity-centric knowledge [8, 11]. For cer-
tain Wikipedia categories, template of articles can
be extracted and automatically populated [24]. Fur-
thermore, Banerjee and Mitra [4] found that training
a lexical classifier per section heading obtains good
results for article construction.

Neural-IR. With the comeback of neural networks,
the IR community is exploring pre-trained word-
vector approaches as well as dedicated neural net-
works for ranking. Pre-trained word- and entity-
embeddings are publicly available in the form of

2URL withheld for blind review

word2vec,3 GloVe,4 DESM,5 NTLM,6 wiki2vec,7 and
RDF2Vec8 vectors. Much of the work in this area
focuses on the applications of these shallow distribu-
tional models to IR tasks, although recently deeper
architectures have also been investigated [18, 26].
Query Expansion with KB. Pseudo relevance feed-
back [15] is one of the most popular query expansion
methods in which frequent words in top documents
of a first retrieval run are extracted. This idea is gen-
eralized to expansion with multiple sources [7] based
on terms and phrases. Recent developments in entity
linking algorithms and object retrieval make it fea-
sible to efficiently tap into the rich information pro-
vided by KBs [14, 17, 9], and exploit disambiguation
and confidences for query and document represen-
tation [13, 21]. Further work on entity aspects [22]
and effective learning-to-rank approaches for latent
entities [25] are a promising avenue.

3 Data Set

The goal of TREC CAR task is to, given a title and an
outline of section headings as a query, retrieve and
rank paragraphs from a provided collection [10].

The TREC CAR organizers process the English
Wikipedia to obtain only the articles with hierarchi-
cal section headings (discarding info boxes, images
and wrappers). From each article the outline of hier-
archical headings are retained, while all paragraphs
on the article are split out. All seven million para-
graphs from all Wikipedia pages are shuffled to form
the Paragraph collection.
Train data. For 50% of all articles, both outlines
and articles are made available as training data for
supervised machine learning as well as a resource
for the Rocchio method discussed below.
Test200. The organizers provide a manually selected
test collection of outlines from 200 articles. Together,

3https://code.google.com/archive/p/word2vec/
4http://nlp.stanford.edu/projects/glove/
5https://www.microsoft.com/en-us/download/

details.aspx?id=52597

6http://www.zuccon.net/ntlm.html
7https://github.com/idio/wiki2vec
8http://data.dws.informatik.uni-mannheim.de/

rdf2vec/

2

these 200 outlines include approximately 2300 head-
ings, each resulting in a query. This test data is com-
plemented with a ground truth file of which para-
graphs are relevant for a given heading, which we
base this study on.

Based on these queries, we experiment with dif-
ferent query expansion approaches and vector space
representations of queries and paragraphs (tf-idf,
GloVe embeddings and RDF2Vec embeddings). We
examine BM25, cosine similarity, learning to rank
[16] and a state-of-the-art neural network model
[19].

4 Experimentation

Environ-

ment

In this paper, we focus on the task of retrieving and
ranking paragraphs for each heading of the outline.
We consider the 2300 sections in Test200 corpus: First
we experiment in a “safe” experimentation environ-
ment before applying approaches the entire collec-
tion of seven million paragraphs. Code and data for
a experimentation environment are provided with
this publication.

The goal is to simulate a noisy candidate genera-
tion method which is guaranteed to include all rele-
vant paragraphs for the heading with a set of nearly-
relevant negatives.

For each heading, we construct a train set by se-
lecting, for every true paragraph under the heading,
five paragraphs from different sections of the article,
and five paragraphs from a different article.

Similarly we construct a test set that includes
all paragraphs from the article, as well as the same
amount of paragraphs drawn from other articles. All
articles are provided in random order. On average
this process yields a mean of 35 paragraphs per sec-
tion.

5 Examined Approaches

The setup of TREC CAR is a bit unusual as all head-
ings of an article are given at once, with the goal of
producing a separate ranking for each heading. In
this early work, we are breaking each outline into
several independent queries, one per heading h as
follows: We identify the path from the heading to
the root, and concatenate all all headings together
with the page title to obtain the query. For example,
if h corresponds to heading H2.3.4 the query is the
concatenation of H2.3.4, H2.3, H2 and the page title.

5.1 Query Expansion Techniques

We experiment three different query expansion ap-
proaches:
Expansion terms (RM1). Feedback terms are de-
rived using pseudo relevance feedback and the rele-
vance model [15]. Here we use Galago’s implemen-
tation9 which is based on a Dirichlet smoothed lan-
guage model for the feedback run. In the experimen-
tal setting, we achieve the best performance expand-
ing the queries with top 10 terms extracted from the
top 10 feedback paragraphs.
Expansion entities (ent-RM1). Another way of
expanding queries is by retrieving relevant entities.
As for retrieving supporting terms, we derive a set of
feedback entities by a search of the index using the
heading-query and deriving several entities. In the
experimental setting, best performance are achieved
using 10 entities.
Paragraph Rocchio. Inspired by the work of Baner-
jee and Mitra [4], we retrieve other paragraphs,
which have an identical heading to our heading-
query, from folds 1 to 4 of the collection (omitting
the fold where test200 originates from). For exam-
ple, given a query such as “Demographic”, regarding
the entity United States, we collect supporting para-
graphs from the pages of other entities (e.g., United
Kingdom), which have as well a a section titled “De-
mographic”. Headings are pre-processed with tokeni-
sation, stopword/digit removal and stemming. This
way, we can retrieve at least one supporting para-
graph for 1/3 of our heading-queries. In the experi-
mental setting, we test expansion using from 1 to 100
supporting passages and we obtain best performance
expanding the query with 5 passages.

9lemurproject.org/galago.php

3

5.2 Vector Space Representations
We study three variations for representing the con-
tent in the vector space model:
TF-IDF. Representing each word in the vocabulary
as its own dimension in the vector space, queries and
paragraphs are represented as their TF-IDF vector.
We are using the logarithmic L2-normalised variant.
We perform stemming as a pre-processing step.
Word Embeddings. Using the pre-trained word
embedding GloVE [20] of 300 dimensions, every
word w in query or paragraph is represented as a
K-dimensional vector (cid:126)w. A vector representation
for the whole paragraph (cid:126)d (complete query (cid:126)q) is ob-
tained by a weighted element-wise average of word
vectors (cid:126)w in the paragraph (query). To give more
attention to infrequent word, we use the TF-IDF of
each word w to weights.

(cid:126)d =

1
|d|

(cid:88)

w∈d

TF-IDF(w) · (cid:126)w

Entity Embeddings. Queries and paragraphs are
represented as their mentioned DBpedia entities, us-
ing the entity linker TagMe [12] (with default pa-
rameters). Next, we obtain latent vector representa-
tions (cid:126)e of each linked entity e using pre-computed
RDF2Vec 300d entity embeddings [23]. Vector repre-
sentations of paragraphs (cid:126)d (queries (cid:126)q) are computed
by a weighted element-wise average of entity vectors
(cid:126)e. By casting a paragraph as a bag-of-links we adapt
TF-IDF to entity links (link statistics taken from DB-
pedia 2015-04 [3]:

(cid:126)d =

1
| {e ∈ d} |

(cid:88)

e∈d

TF-IDF(e) · (cid:126)e

Learning to Rank. We combine the ranking scores
of different baselines with supervised machine learn-
ing in a learning-to-rank setting, for producing a fi-
nal ranking of relevant paragraphs. We use RankLib
10 with 5-fold cross validation using a linear model
optimized for MAP, trained with coordinate ascent.
Deep Neural Network. The Duet model is a state-
of-the-art deep neural network (DNN) recently pro-
posed by Mitra et al. [19] for ad-hoc retrieval. The
Duet architecture learns to model query-paragraph
relevance by jointly learning good representations
of the query and the paragraph text for matching, as
well as by learning to identify good patterns of exact
matches of the query terms in the paragraph text.
We use the Duet implementation available publicly11
under the MIT license for our experiments.

Training on folds 1 to 4 of the collection, we only
consider the first ten words for the query and the
first 100 words for the passage as inputs. We use 64
hidden units in the different layers of the network,
as opposed to 300 in the original paper, to reduce the
total number of learnable parameters of the model.
We trained the model for 32 epochs with a learning
rate of 0.001 which was picked based on a subset of
the training data. Each epoch was trained over 1024
minibatches, and each minibatch contained 1024 sam-
ples. Each training sample was a triplet consisting of
a query, a positive passage, and a negative passage.
The training time was limited to 60 hours.

6 Evaluation

We present evaluation results both on the experi-
mentation environment and on the entire paragraph
collection.

5.3 Ranking Approaches
We test four different ranking approaches:
Okapi BM25. Results are ranked using Okapi BM25
with k1=1.2 and b=0.75, using the implementation of
Lucene 6.4.1. Porter stemming and stopword removal
was applied to paragraphs and queries.
Cosine Similarity. Paragraphs are ranked by co-
sine similarity (cs) between respective vector repre-
sentations of the query and the paragraph.

6.1 Experimentation Environment
The experimentation environment (Section 4) pro-
vides a “safe environment” by simulating a noisy
candidate method for each section. Results are pre-
sented in table 1. The approach bm25 query only sets
the baseline of our work.

10lemurproject.org/ranklib.php
11https://github.com/bmitra-msft/NDRM/blob/

master/notebooks/Duet.ipynb

4

Table 1: Results on experimentation environment.

Table 2: Results of the initial candidate selection.

MAP R-Prec MRR

0.320

0.232

0.409

Heading Retrieval - bm25
query only

MAP

R-Prec MRR

0.150

0.118

0.216

Heading Retrieval - tf-idf (cs)
query only
query + Rocchio

0.035
0.029

0.025
0.020

0.053
0.041

bm25
query only

tf-idf (cs)
query only
query + RM1
query + Rocchio

GloVe (cs)
query only
query + RM1
query + Rocchio

RDF2Vec (cs)
entity-query only
ent-query + ent-RM1
ent-query + ent-Rocchio

Learning to Rank
all (cs) scores

Duet model
query only

0.350
0.324
0.400

0.329
0.289
0.349

0.313
0.320
0.316

0.212
0.205
0.286

0.210
0.177
0.236

0.200
0.208
0.206

0.383
0.384
0.466

0.387
0.339
0.410

0.369
0.377
0.375

0.412

0.290

0.475

0.470

0.359

0.553

Not all query expansion approaches and vector
space representation methods improve over this base-
line. This is particularly true for query expansion
with terms or entities (through RM3 = query + RM1)
as well as RDF2Vec embeddings. On the contrary, the
most promising results among the methods which
employ cosine similarity as a ranking function, are
obtained when the query is expanded with Rocchio
vectors trained on paragraphs from sections with
the same heading. This finding reconfirms the re-
sults of previous work on the automatic generation
of Wikipedia articles based its structural information
[4]. The results show that common traits between
Wikipedia sections with the same heading are better
captured using the tf-idf word vector than through
word- and entity- embedding vectors suggest that a
possible improvement over these baselines could by
obtained by training embeddings for this task.

In comparison to these unsupervised retrieval
models, both supervised Learning to Rank and the
Neural Duet model out-perform all previously de-
scribed baselines. In particular, the Duet model yields

Figure 1: Effect of training data size on the perfor-
mance of the Duet model. Training on four folds,
reporting MAP on holdout fold.

a substantial improvement over all presented ap-
proaches, showing the potential of neural-IR for the
task. It is important to remark that neural deep mod-
els take days to train even on a GPU. In addition
they are data-hungry, with performances improving
significantly with more training data as shown in
Figure 1.

6.2 Experiments on Paragraph Collec-

tion

As a second step, we conduct experiments on the
entire paragraph collection.
Candidate Selection. Many of the previously pre-
sented methods, such as the Duet model, require a
candidate generating method. We test three candi-
date methods: bm25, tf-idf, and tf-idf with rocchio
expansion. For each query, the methods produce a

5

Table 3: Results on the Paragraph Collection after candidate method. (cid:72) Worse according to paired-t-test
with α = 5%.

bm25 candidate

tf-idf (cs)
bm25 + query + Rocchio

GloVe (cs)
bm25 + query + Rocchio

Duet model
bm25 + query only

MAP
0.150(cid:72)

R-Prec MRR

0.118

0.216

0.140(cid:72)

0.113

0.207

0.150(cid:72)

0.120

0.218

w/o bm25
MAP

-

0.085

0.082

0.161

0.130

0.229

0.093

candidate set of 100 paragraphs. The results are pre-
sented in Table 2. While this is a challenging task,
encouraging performance are obtained with the use
of bm25 query only. However as half of the queries
do not have any relevant paragraphs in the candidate
set, the theoretically achievable MRR is 0.50.

As a final experiment, we use bm25 query only as a
candidate method to obtain 100 paragraphs per query.
The three of the most-promising systems presented
above are re-evaluated, using learning to rank to
combine the scores for bm25, query, and Rocchio
expansion with 5-fold cross validation.

The combination of bm25 score and duet model
is significantly outperforming all other methods,
demonstrating the strength of the candidate method.
If the combination with the bm25 score is left out,
all methods are significantly loosing in performance
(see last column in Table 3).

7 Conclusions

In this paper, we present the performance of a va-
riety of approaches, from established baselines to
more advanced systems, in the context of the new
TREC-CAR track on Complex Answer Retrieval. Our
results show the effects of different query expansion
and embedding approaches in comparison to learn-
ing to rank and neural ranking models. The public
availability of this new dataset, gave us the oppor-
tunity to offer a benchmark on the most promising
approaches for tackling this problem to future par-

ticipants of the track and the IR community.

References
[1] Elif Aktolga, James Allan, and David A. Smith.
2011. Passage reranking for question answering
using syntactic structures and answer types. In
Advances in Information Retrieval. Springer.

[2] James Allan, Bruce Croft, Alistair Moffat, and
Mark Sanderson. 2012. Frontiers, challenges,
and opportunities for information retrieval: Re-
port from SWIRL 2012 the second strategic
workshop on information retrieval in Lorne. In
ACM SIGIR Forum, Vol. 46. ACM, 2–32.

[3] S¨oren Auer, Christian Bizer, Georgi Kobilarov,
Jens Lehmann, Richard Cyganiak, and Zachary
Ives. 2007. Dbpedia: A nucleus for a web of open
data. Springer.

[4] Siddhartha Banerjee and Prasenjit Mitra. 2015.
WikiKreator: Improving Wikipedia Stubs Auto-
matically.. In ACL (1). 867–877.

[5] Michael Bendersky, W Bruce Croft, and Yan-
lei Diao. 2011. Quality-biased ranking of web
documents. In WSDM.

[6] Michael Bendersky and Oren Kurland. 2008. Uti-
lizing passage-based language models for doc-
In Advances in Information
ument retrieval.
Retrieval. Springer.

6

[18] Bhaskar Mitra and Nick Craswell. 2017. Neural
Models for Information Retrieval. arXiv preprint
arXiv:1705.01509 (2017).

[19] Bhaskar Mitra, Fernando Diaz, and Nick
Craswell. 2017. Learning to Match Using Lo-
cal and Distributed Representations of Text for
Web Search. In www.

[20] Jeffrey Pennington, Richard Socher, and
Christopher D Manning. 2014. Glove: Global
Vectors for Word Representation.. In EMNLP,
Vol. 14.

[21] Hadas Raviv, Oren Kurland, and David Carmel.
2016. Document Retrieval Using Entity-Based
Language Models. In SIGIR.

[22] Ridho Reinanda, Edgar Meij, and Maarten de Ri-
jke. 2015. Mining, ranking and recommending
entity aspects. In SIGIR.

[23] Petar Ristoski and Heiko Paulheim. 2016.
Rdf2vec: Rdf graph embeddings for data mining.
In ISWC. Springer.

[24] Christina Sauper and Regina Barzilay. 2009. Au-
tomatically generating Wikipedia articles: A
structure-aware approach. In IJCNLP.

[25] Chenyan Xiong and Jamie Callan. 2015. Es-
drank: Connecting query and documents
through external semi-structured data.
In
CIKM.

[26] Ye Zhang, Md Mustafizur Rahman, Alex Bray-
lan, Brandon Dang, Heng-Lu Chang, Henna
Kim, Quinten McNamara, Aaron Angert, Ed-
ward Banner, Vivek Khetan, and others. 2016.
Neural Information Retrieval: A Literature Re-
view. arXiv preprint arXiv:1611.06792 (2016).

[7] Michael Bendersky, Donald Metzler, and
W Bruce Croft. 2012. Effective query formu-
lation with multiple information sources. In
WSDM.

[8] Roi Blanco and Hugo Zaragoza. 2010. Finding
support sentences for entities. In SIGIR.

[9] Jeffrey Dalton, Laura Dietz, and James Allan.
2014. Entity Query Feature Expansion Using
Knowledge Base Links. In SIGIR.

[10] Laura Dietz and Ben Gamari. 2017. TREC CAR:
A Data Set for Complex Answer Retrieval. Ver-
sion 1.4. (2017). http://trec-car.cs.unh.
edu

[11] Laura Dietz and Michael Schuhmacher. 2015.
An interface sketch for queripidia: Query-
driven knowledge portfolios from the web. In
Proc. Workshop on Exploiting Semantic Annota-
tions in IR. ACM, 43–46.

[12] Paolo Ferragina and Ugo Scaiella. 2010. Tagme:
on-the-fly annotation of short text fragments
(by wikipedia entities). In CIKM. ACM.

[13] Faegheh Hasibi, Krisztian Balog, and Svein Erik
Bratsberg. 2016. Exploiting Entity Linking in
Queries for Entity Retrieval. In ICTIR. 209–218.

[14] Alexander Kotov and ChengXiang Zhai. 2012.
Tapping into knowledge base for concept feed-
back: leveraging conceptnet to improve search
results for difficult queries. In WSDM.

[15] Victor Lavrenko and W Bruce Croft. 2001. Rel-
evance based language models. In SIGIR.

[16] Hang Li. 2014. Learning to rank for information
retrieval and natural language processing. Syn-
thesis Lectures on Human Language Technologies
7, 3 (2014).

[17] Xitong Liu and Hui Fang. 2015. Latent entity
space: a novel retrieval approach for entity-
bearing queries. Information Retrieval Journal
18, 6 (2015).

7

