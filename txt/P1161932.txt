8
1
0
2
 
l
u
J
 
6
2
 
 
]
h
p
-
p
e
h
[
 
 
4
v
0
2
0
0
0
.
5
0
8
1
:
v
i
X
r
a

A Guide to Constraining Eﬀective Field Theories with Machine Learning

Johann Brehmer,1 Kyle Cranmer,1 Gilles Louppe,2 and Juan Pavez3
1New York University, USA
2University of Liège, Belgium
3Federico Santa María Technical University, Chile
(Dated: 30th July 2018)

We develop, discuss, and compare several inference techniques to constrain theory para-
meters in collider experiments. By harnessing the latent-space structure of particle physics
processes, we extract extra information from the simulator. This augmented data can be
used to train neural networks that precisely estimate the likelihood ratio. The new methods
scale well to many observables and high-dimensional parameter spaces, do not require any
approximations of the parton shower and detector response, and can be evaluated in micro-
seconds. Using weak-boson-fusion Higgs production as an example process, we compare the
performance of several techniques. The best results are found for likelihood ratio estimators
trained with extra information about the score, the gradient of the log likelihood function with
respect to the theory parameters. The score also provides suﬃcient statistics that contain
all the information needed for inference in the neighborhood of the Standard Model. These
methods enable us to put signiﬁcantly stronger bounds on eﬀective dimension-six operators
than the traditional approach based on histograms. They also outperform generic machine
learning methods that do not make use of the particle physics structure, demonstrating their
potential to substantially improve the new physics reach of the LHC legacy results.

CONTENTS

I. Introduction

II. The EFT measurement problem

A. Eﬀective ﬁeld theory
B. Physics challenges and traditional methods
C. Structural properties of EFT measurements
D. Explicit example

III. Likelihood ratio estimation

A. Modeling likelihood ratios
B. Available information and its usefulness
C. Strategies
D. Calibration
E. Implementation
F. Challenges and diagnostics

IV. Limit setting

A. Asymptotics
B. Neyman construction
C. Nuisance parameters

V. Results

A. Idealized setup
B. Detector eﬀects

2

4
4
4
6
8

11
12
14
18
24
25
28

30
30
31
32

33
33
41

2

42

45
45
46
57

63

VI. Conclusions

A. Appendix

1. Simpliﬁed detector description
2. Model almanac
3. Additional results

References

I.

INTRODUCTION

An important aspect of the legacy of the Large Hadron Collider (LHC) experiments will be precise
constraints on indirect signatures of physics beyond the Standard Model (SM), parameterized for
instance by the dimension-six operators of the Standard Model eﬀective ﬁeld theory (SMEFT). The
relevant measurements can easily involve tens of diﬀerent parameters that predict subtle kinematic
signatures in the high-dimensional space of the data. Traditional analysis techniques do not scale
well to this complex problem, motivating the development of more powerful techniques.

The analysis of high-energy-physics data is based on an impressive suite of simulation tools that
model the hard interaction, parton shower, hadronization, and detector response. The community
has invested a tremendous amount of eﬀort into developing these tools, yielding the high-ﬁdelity
modeling of LHC data needed for precision measurements. Simulators such as Pythia [1] and
Geant4 [2] use Monte-Carlo techniques to sample the multitudinous paths through which a
particular hard scattering might develop. A single event’s path through the simulation can easily
involve many millions of random variables. While Monte-Carlo techniques can eﬃciently sample
from the distributions implicitly deﬁned by the simulators, it is not feasible to calculate the likelihood
for a particular observation because doing so would require integrating over all the possible histories
leading to that observation. Clearly it is infeasible to explicitly calculate a numerical integral over
this enormous latent space. While this problem is ubiquitous in high energy physics, it is rarely
acknowledged explicitly.

Traditionally, particle physicists have approached this problem by restricting the analysis to
one or two well-motivated discriminating variables, discarding the information contained in the
remaining observables. The probability density for the restricted set of discriminating variables is
then estimated with explicit functions or non-parametric approaches such as template histograms,
kernel density estimates, or Gaussian Processes [3]. These low-dimensional density estimates are
constructed and validated using Monte-Carlo samples from the simulation. While well-chosen
variables may yield precise bounds along individual directions of the parameter space, they often
lead to weak constraints in other directions in the parameter space [4]. The sensitivity to multiple
parameters can be substantially improved by using the fully diﬀerential cross section. This is the
forte of the Matrix Element Method [5–19] and Optimal Observables [20–22] techniques, which
are based on the parton-level structure of a given process. Shower and event deconstruction [23–
26] extend this approach to the parton shower. But all these methods still require some level
of approximations on the parton shower and either neglect or crudely approximate the detector
response. Moreover, even a simpliﬁed description of the detector eﬀects requires the numerically
expensive evaluation of complicated integrals for each observed event. None of these established
approaches scales well to high-dimensional problems with many parameters and observables, such
as the SMEFT measurements.

In recent years there has been increased appreciation that several real-world phenomena are
better described by simulators that do not admit a tractable likelihood. This appears in ﬁelds as
diverse as ecology, phylogenetics, epidemiology, cardiac simulators, quantum chemistry, and particle

3

physics. Inference in this setting is often referred to as likelihood-free inference, where the inference
strategy is restricted to samples generated from the simulator. Implicitly, these techniques aim to
estimate the likelihood. A particularly ubiquitous technique is Approximate Bayesian Computation
(Abc) [27–33]. Abc is closely related to the traditional template histogram and kernel density
estimation approach used by physicists. More recently, approximate inference techniques based on
machine learning and neural networks have been proposed [34–51]. All these techniques have in
common that they only take into account simulated samples similar to the actual observables — they
do not exploit the structure of the process that generates them.

We develop new simulation-based inference techniques that are tailored to the structure of particle
physics processes. The key insight behind these methods is that we can extract more information
than just samples from the simulations, and that this additional information can be used to eﬃciently
train neural networks that precisely estimate likelihood ratios, the preferred test statistics for LHC
measurements. These methods are designed for scalability to both high-dimensional parameter
spaces as well as to many observables. They do not require any simplifying assumptions to the
underlying physics: they support state-of-the-art event generators with parton shower, reducible
and irreducible backgrounds, and full detector simulations. After an upfront training phase, they are
very eﬃcient to evaluate. Our tools directly provide an estimator for the likelihood ratio, an intuitive
and easily interpretable quantity. Finally, limits derived from these tools with toy experiments have
the reassuring property that even if they might not be optimal, they are never wrong, i. e. no points
are said to be excluded that should not be excluded at a given conﬁdence level.

In Ref. [52], the companion paper of this publication, we focus on the key ideas and sensitivity
enabled by these techniques. Reference [53] presents the methods in a more abstract setting. Here we
describe the actual algorithms in detail, developing several diﬀerent methods side by side. Given the
number of discussed variations, this publication might have the look and feel of a review article and
we present it as a guide to the interested practitioner. We focus on the main ideas and diﬀerences
between the approaches and postpone many technical details until the appendices.

We evaluate the performance of these diﬀerent methods on a speciﬁc example problem, the
measurement of two dimension-six operators in Higgs production in weak boson fusion (WBF) in
the four-lepton mode at the LHC. For part of this analysis, we work in an idealized setting in which
we can access the true likelihood function, providing us with a ground truth for the comparison of
the diﬀerent analysis methods. After establishing the precision of the likelihood ratio estimation,
we turn towards the more physical question of how strongly the two operators can be constrained
with the diﬀerent techniques. We repeat the analysis with a simpliﬁed detector response where the
ground-truth likelihood is no longer tractable.

We begin by laying out the problem in Sec. II: we summarize the eﬀective ﬁeld theory idea,
list the challenges posed by EFT measurements, translate the problem from a physics perspective
into the language of statistics, and discuss its important structural properties. We also set up the
example process used throughout the rest of the paper. The description of the analysis methods
are split in two parts:
in Sec. III we deﬁne the diﬀerent techniques to estimate the likelihood
ratio, which includes most of the conceptual work presented here. Section IV then explains how
to set limits on the EFT parameters based on these tools. In Sec. V we evaluate the performance
of the diﬀerent tools in our example process. Finally, in Sec. VI we summarize our ﬁndings and
give recommendations for practitioners. The appendices describe the diﬀerent algorithms in more
detail and provide additional results. The code and data used for this paper are available online at
Ref. [54].

4

(1)

II. THE EFT MEASUREMENT PROBLEM

A. Eﬀective ﬁeld theory

Eﬀective ﬁeld theories (EFTs) [55–57] parameterize the eﬀects of physics at an energy scale Λ
on observables at smaller energies E (cid:28) Λ as a set of local operators. The form of these operators is
ﬁxed by the light particles and the symmetry structure of the theory and is entirely independent of
the high-energy model. Systematically expanding the Lagrangian in 1/Λ, equivalent to ordering the
operators by their canonical dimension, leaves us with a ﬁnite set of operators weighted by Wilson
coeﬃcients that describe all possible new physics eﬀects up to some order in E/Λ.

In the absence of new particles at the TeV scale, and assuming the symmetry structure of the
SM, we can thus describe any new physics signature in LHC processes in terms of a set of higher-
dimensional operators [58–63]. In this SM Eﬀective Field Theory (SMEFT), the leading eﬀects
beyond the SM come from 59 independent dimension-six operators Oo with Wilson coeﬃcients fo,

LD6 = LSM +

(cid:88)

o

fo
Λ2 Oo ,

where the SM corresponds to all fo = 0 and any measurement of a deviation hints at new physics.
The dimension-six Wilson coeﬃcients are perfectly suited as an interface between experimental
measurements and theory interpretations. They are largely model-independent, can parameterize a
wide range of observables, including novel kinematic features, and are theoretically consistent beyond
tree level. On the technical side, dimension-six operators are implemented in standard Monte-Carlo
event generators [64], allowing us to generate predictions for rates and kinematic observables for
any combination of Wilson coeﬃcients. Measured values of fo/Λ2 can easily be translated to the
parameters of speciﬁc models through well-established matching procedures [65]. All in all, SMEFT
measurements will likely be a key part of the legacy of the LHC experiments [66].

Let us brieﬂy comment on the question of EFT validity. A hierarchy of energy scales E (cid:28) Λ is
the key assumption behind the EFT construction, but in a bottom-up approach the cutoﬀ scale Λ
cannot be known without additional model assumptions. From a measurement fo/Λ2 (cid:54)= 0 we can
estimate the new physics scale Λ only by assuming a characteristic size of the new physics couplings
√
fo, and compare it to the energy scale E of the experiment. It has been found that dimension-six
operators often capture the dominant eﬀects of new physics even when there is only a moderate
scale separation E (cid:46) Λ [67]. All these concerns are not primarily of interest for the measurement of
Wilson coeﬃcients, but rather important for the interpretation of the results in speciﬁc UV theories.

B. Physics challenges and traditional methods

EFT measurements at the LHC face three fundamental challenges:

1. Individual scattering processes at the LHC are sensitive to several operators and require
simultaneous inference over a multi-dimensional parameter space. While a naive parameter
scan works well for one or two dimensions, it becomes prohibitively expensive for more than
a few parameters.

2. Most operators introduce new coupling structures and predict non-trivial kinematic features.
These do not translate one-to-one to traditional kinematic observables such as transverse
momenta, invariant masses or angular correlations. An analysis based on only one kinematic
variable typically cannot constrain the full parameter space eﬃciently. Instead, most of the

5

operator eﬀects only become fully apparent when multiple such variables including their
correlations are analysed [4, 68].

3. The likelihood function of the observables is intractable, making this the setting of “likelihood-
free inference” or “simulator-based inference”. There are simulators for the high-energy
interactions, the parton shower, and detector eﬀects that can generate events samples for
any theory parameter values, but they can only be run in the forward mode. Given a set of
reconstruction-level observables, it is not possible to evaluate the likelihood of this observation
given diﬀerent theory parameters. The reason is that this likelihood includes the integral over
all possible diﬀerent parton shower histories and particle trajectories through the detector as
a normalizing constant, which is infeasible to calculate in realistic situations. We will discuss
this property in more detail in the following section.

The last two issues are typically addressed in one of three ways. Most commonly, a small set of
discriminating variables (also referred to as summary statistics or engineered features) is handpicked
for a given problem. The likelihood in this low-dimensional space is then estimated, for instance, by
ﬁlling histograms from simulations. While well-chosen variables may lead to good constraints along
individual directions of the parameter space, there are typically directions in the parameter space
with limited sensitivity [4, 68].

The Matrix Element Method [5, 6, 8–15, 17–19] or Optimal Observables [20–22] go beyond a
few speciﬁc discriminating variables and use the matrix element for a particular process to estimate
the likelihood ratio. While these techniques can be very powerful, they suﬀer from two serious
limitations. The parton shower and detector response are either entirely neglected or approximated
through ad-hoc transfer function. Shower and event deconstruction [23–26] allow for the calculation
of likelihood ratios at the level of the parton shower, but still rely on transfer functions to describe
the detector response. Finally, even with such a simple description of the shower and detector, the
evaluation of the likelihood ratio estimator requires the numerically expensive computation of large
integrals for each observed event.

Finally, there is a class of generic methods for likelihood-free inference. For Bayesian inference,
the best-known approach is Approximate Bayesian Computation (Abc) [27–32]. Similar to the
histogram approach, it relies on the choice of appropriate low-dimensional summary statistics, which
can severely limit the sensitivity of the analysis. Diﬀerent techniques based on machine learning have
been developed recently. In particle physics, the most common example are discriminative classiﬁers
between two discrete hypotheses, such as a signal and a background process. This approach has
recently been extended to parameter measurements [34, 35]. More generally, many techniques based
on the idea of using a classiﬁcation model, such as neural networks, for inference in the absence of
a tractable likelihood function have been introduced in the machine learning community [36–51].
All of these methods only require samples of events trained according to diﬀerent parameter points.
They do not make use of the structure of the particle physics processes, and thus do not use all
available information.

All of these methods come with a price. We develop new techniques that
• are tailored to particle physics measurements and leverage their structural properties,
• scale well to high-dimensional parameter spaces,
• can accommodate many observables,
• capture the information in the fully diﬀerential cross sections, including all correlations

• fully support state-of-the art simulators with parton showers and full detector simulations,

between observables,

and

• are very eﬃcient to evaluate after an upfront training phase.

6

(2)

(3)

C. Structural properties of EFT measurements

1. Particle-physics structure

One essential step to ﬁnding the optimal measurement strategy is identifying the structures and
symmetries of the problem. Particle physics processes, in particular those described by eﬀective
ﬁeld theories, typically have two key properties that we can exploit.

First, any high-energy particle physics process factorizes into the parton-level process, which
contains the matrix element and in it the entire dependence on the EFT coeﬃcients, and a residual
part describing the parton shower and detector eﬀects. In many plausible scenarios of new physics
neither the strong interactions in the parton shower nor the electromagnetic and strong interactions
in the detector are aﬀected by the parameters of interest. The likelihood function can then be
written as

(cid:90)

(cid:90)

p(x|θ) =

dz p(x, z|θ) =

dz p(x|z) p(z|θ) .

Here and in the following x are the actual observables after the shower, detector, and reconstruction;
θ are the theory parameters of interest; and z are the parton-level momenta (a subset of the latent
variables). Table I provides a dictionary of these and other important symbols that we use.

The ﬁrst ingredient to this likelihood function is the distribution of parton-level four-momenta

p(z|θ) =

1
σ(θ)

dσ(θ)
dz

,

where σ(θ) and dσ(θ)/dz are the total and diﬀerential cross sections, respectively. Crucially, this
function is tractable: the matrix element and the parton density functions can be evaluated for
arbitrary four-momenta z and parameter values θ. In practice this means that matrix-element
codes such as MadGraph [64] can not only be run in a forward, generative mode, but also deﬁne
functions that return the squared matrix element for a given phase-space point z. Unfortunately,
there is typically no user-friendly interface to these functions, so evaluating it requires some work.
Second, the conditional density p(x|z) describes the probabilistic evolution from the parton-level
four-momenta to observable particle properties. While this symbol looks innocuous, it represents the
full parton shower, the interaction of particles with the detector material, the sensor response and
readout, and the reconstruction of observables. Diﬀerent simulators such as Pythia [1], Geant4 [2],
or Delphes [69] are often used to generate samples {x} ∼ p(x|z) for given parton-level momenta z.
This sampling involves the Monte-Carlo integration over the possible shower histories and detector
interactions,

(cid:90)

(cid:90)

p(x|z) =

dzdetector

dzshower p(x|zdetector) p(zdetector|zshower) p(zshower|z) .

(4)

This enormous latent space can easily involve many millions of random numbers, and these integrals
are clearly intractable, which we denote with the red symbol p. In other words, given a set of
reconstruction-level observables x, we cannot calculate the likelihood function p(x|z) that describes
the compatibility of parton-level momenta z with the observation. By extension, we also cannot
evaluate p(x|θ), the likelihood function of the theory parameters given the observation. The
intractable integrals in Eq. (4) are the crux of the EFT measurement problem.

The factorization of Eq. 2 together with the tractability of the parton-level likelihood p(z|θ) is
immensely important. We will refer to the combination of these two properties as particle-physics
structure. The far-reaching consequences of this structure for EFT measurements will be the topic
of Sec. III B. Many (but not all) of the inference strategies we discuss will rely on this condition.

Symbol

Physics meaning

Machine learning abstraction

Set of all observables
One or two kinematic variables

z ≡ zparton
zshower
zdetector
zall = (zparton, zshower, zdetector) Full simulation history of event
θ

Parton-level four-momenta
Parton shower trajectories
Detector interactions

Theory parameters (Wilson
coeﬃcients)
Best ﬁt for theory parameters

x
v

ˆθ

p(x|θ)

p(z|θ)

p(x|z)

Distributions of observables given
theory parameters
Parton-level distributions from
matrix element
Eﬀect of shower, detector,
reconstruction

7

Features
Low-dimensional summary
statistics /engineered feature
Latent variables
Latent variables
Latent variables
All latent variables
Parameters of interest

Estimator for parameters of
interest

Intractable likelihood

Tractable likelihood of latent
variables
Intractable density deﬁned
through stochastic generative
process

r(x|θ0, θ1)
ˆr(x|θ0, θ1)
t(x|θ)
ˆt(x|θ)

xe, ze
θo
θc, wc(z), pc(x)

Likelihood ratio between hypotheses θ0, θ1, see Eq. (11).
Estimator for likelihood ratio
Score, see Eq. (14).
Estimator for score

Event
Wilson coeﬃcient for one operator

Data point
Individual parameter of interest

Morphing basis points, coeﬃcients, densities, see Eq. (6).

Table I: Dictionary deﬁning many symbols that appear in this paper. Red symbols denote
intractable likelihood functions. The last three rows explain our conventions for indices.

Note that this Markov property holds even with reducible and irreducible backgrounds and when
a matching scheme is used to combine diﬀerent parton-level multiplicities. In these situations there
may be diﬀerent disjoint parts of z space, even with diﬀerent dimensionalities, for instance when
events with n and n + 1 partons in the ﬁnal state can lead to the same conﬁguration of observed
jets. The integral over z then has to be replaced with a sum over “zn spaces” and an integral over
each zn, but the logic remains unchanged.

2. Operator morphing

Eﬀective ﬁeld theories (and other parameterisations of indirect signatures of new physics) typically
contribute a ﬁnite number of amplitudes to a given process, each of which is multiplied by a function
of the Wilson coeﬃcients.1 In this case the likelihood can be written as

p(z|θ) =

˜wc(cid:48)(θ) fc(cid:48)(z)

(cid:88)

c(cid:48)

(5)

where c(cid:48) labels the diﬀerent amplitude components, and the functions fc(cid:48)(z) are not necessarily
properly positive deﬁnite or normalized.

1 Exceptions can arise for instance when particle masses or widths depend on the parameters of interest. But in an

EFT setting one can expand these quantities in 1/Λ, restoring the factorization.

The simplest example is a process in which one SM amplitude M0(z) interferes with one new
physics amplitude MBSM(z|θ) = θM1(z), which scales linearly with a new physics parameter
θ. The diﬀerential cross section, proportional to the squared matrix element, is then dσ(z) ∝
|M0(z)|2 + 2θ Re M0(z)†M1(z) + θ2 |M1(z)|2. There are three components, representing the SM,
interference, and pure BSM terms, each with their own parameter dependence ˜wc(cid:48)(z) and momentum
dependence fc(cid:48)(z).

We can then pick a number of basis2 parameter points θc equal to the number of components
c(cid:48) in Eq. (5). They can always be chosen such that the matrix Wcc(cid:48) = ˜wc(cid:48)(θc) is invertible, which
allows us to rewrite (5) as a mixture model

p(z|θ) =

wc(θ) pc(z)

(cid:88)

c

with weights wc(θ) = (cid:80)
cc(cid:48) and (now properly normalized) basis densities pc(z) = p(z|θc).
The weights wc(θ) depend on the choice of basis points and are analytically known. This “morphing”
procedure therefore allows us to extract the full likelihood function p(z|θ) from a ﬁnite set of
evaluations of basis densities pc(z).

c(cid:48) ˜wc(θ) W −1

Calculating the full statistical model through morphing requires the likelihood p(z|θ) to be
tractable, which is true for parton-level momenta as argued above. However, the same trick can
be applied even when the exact likelihood is intractable, but we can estimate it. For instance, the
marginal distribution of any individual kinematic variable v(x) can be reliably estimated through
histograms or other density estimation techniques, even when shower and detector eﬀects are taken
into account. The morphing procedure then lets us evaluate the full conditional distribution p(v|θ)
based on a ﬁnite number of Monte-Carlo simulations [70].
Finally, note that Eq. (6) together with Eq. (2) imply

p(x|θ) =

wc(θ) pc(x) ,

(cid:88)

c

even if the likelihood function p(x|θ) and the components pc(x) are intractable. This will later allow
us to impose the morphing structure on likelihood ratio estimators.

Not all EFT amplitudes satisfy the morphing structure in Eq. (5), so we discuss both measurement
strategies that rely on and make use of this property as well as more general ones that do not require
it to hold.

8

(6)

(7)

D. Explicit example

1. Weak-boson-fusion Higgs to four leptons

As an explicit example LHC process we consider Higgs production in weak boson fusion (WBF)

with a decay of the Higgs into four leptons,

qq → qq h → qq ZZ → qq (cid:96)+(cid:96)− (cid:96)+(cid:96)−

(8)

with (cid:96) = e, µ, as shown in Fig. 1.

While this process is rare and is likely to only be observed during the high-luminosity run of
the LHC, it has a few compelling features that make it a prime candidate to study the eﬃcient
extraction of information. First, the two jets from the quarks and in particular the four leptons can

2 Note that the morphing basis points θc are unrelated to the choice of an operator basis for the eﬀective ﬁeld theory.

9

(9)

(10)

q

q

W , Z

W , Z

Z

Z

h

q

q

(cid:96)+
(cid:96)−

(cid:96)+
(cid:96)−

Figure 1: Feynman diagram for Higgs production in weak boson fusion in the 4(cid:96) mode. The red
dots show the Higgs-gauge interactions aﬀected by the dimension-six operators of our analysis.

be reconstructed quite precisely in the LHC detectors. Even when assuming on-shell conditions and
energy-momentum conservation, the ﬁnal-state momenta span a 16-dimensional phase space, giving
rise to many potentially informative observables.

Second, both the production of the Higgs boson in weak boson fusion as well as its decay
into four leptons are highly sensitive to the eﬀects of new physics in the Higgs-gauge sector. We
parameterize these with dimension-six operators in the SMEFT, following the conventions of the
Hagiwara-Ishihara-Szalapski-Zeppenfeld basis [62]. For simplicity, we limit our analysis to the two
particularly relevant operators

L = LSM +

fW
Λ2

ig
2
(cid:124)

(Dµφ)† σa Dνφ W a
µν
(cid:123)(cid:122)
(cid:125)
OW

−

fW W
Λ2

g2
4
(cid:124)

(φ†φ) W a

µν W µν a
(cid:125)

.

(cid:123)(cid:122)
OW W

For convenience, we rescale the Wilson coeﬃcients to the dimensionless parameters of interest

θ =

(cid:18) fW v2
Λ2

,

(cid:19)T

fW W v2
Λ2

where v = 246 GeV is the electroweak vacuum expectation value. As alluded to above, the validity
range of the EFT cannot be determined in a model-independent way. For moderately weakly to
moderately strongly coupled underlying new physics models, one would naively expect |fo| (cid:46) O (1)
and the EFT description to be useful in the range E ≈ v (cid:46) Λ, or −1 (cid:46) θo (cid:46) 1. This is the parameter
range we analyse in this paper.

The interference between the Standard Model amplitudes and the dimension-six operators leads
to an intricate relation between the observables and parameters in this process, which has been
studied extensively. The precise measurement of the momenta of the four leptons provides access to
a range of angular correlations that fully characterize the h → ZZ decay [10, 71]. These variables
are sensitive to the eﬀects of dimension-six operators. But the momentum ﬂow p through the decay
vertex is limited by the Higgs mass, and the relative eﬀects of these dimension-six operators are
suppressed by a factor p2/Λ2. On the other hand, the Higgs production through two oﬀ-shell gauge
bosons with potentially high virtuality does not suﬀer from this suppression. The properties of the
two jets recoiling against them are highly sensitive to operator eﬀects in this vertex [72–75].

In Fig. 2 we show example distributions of two particularly informative observables, the transverse
momentum of the leading (higher-pT ) jet pT,j1, and the azimuthal angle between the two jets, ∆φjj.
The two quantities are sensitive to diﬀerent directions in parameter space. Note also that the
interference between the diﬀerent amplitudes can give rise to non-trivial eﬀects. The size of the
dimension-six amplitudes grows with momentum transfer, which is strongly correlated with the
transverse momentum of the leading jet. If the interference of new-physics amplitudes with the SM
diagrams is destructive, this can drive the total amplitude through zero [67]. The jet momentum

10

Figure 2: Kinematic distributions in our example process for three example parameter points. We
assume an idealized detector response to be discussed in Sec. II D 2. Left: transverse momentum of
the leading (higher-pT ) jet, a variable strongly correlated with the momentum transfer in the
process. The dip around 350 GeVis a consequence of the amplitude being driven through zero, as
discussed in the text. Right: separation in azimuthal angle between the two jets.

distribution then dips and rises again with higher energies, as seen in the red curve in the left panel
of Fig. 2. Such depleted regions of low probability can lead to very small or large likelihood ratios
and potentially pose a challenge to inference methods.

By analysing the Fisher information in these distributions, it is possible to compare the discrimin-
ation power in these two observables to the information contained in the full multivariate distribution
or to the information in the total rate. It turns out that the full multivariate distribution p(z|θ)
contains signiﬁcantly more information than the one-dimensional and two-dimensional marginal
distributions of any standard kinematic variables [4]. The total rate is found to carry much less
information on the two operators, in particular when systematic uncertainties on the cross sections
are taken into account. In this study we therefore only analyse the kinematic distributions for a
ﬁxed number of observed events.

2. Sample generation

Already in the sample generation we can make use of the structural properties of the process
discussed in Sec. II C. The amplitude of this process factorizes into a sum of parameter-dependent
factors times phase-space-dependent amplitudes, as given in Eq. (5). The eﬀect of the operators OW
and OW W on the total Higgs width breaks this decomposition, but this eﬀect is tiny and in practice
irrelevant when compared to the experimental resolution. The likelihood function of this process
therefore follows the mixture model in Eq. (6) to good approximation, and the weights wc(θ) can
be calculated. Since the parton-level likelihood function is tractable, we can reconstruct the entire
likelihood function p(z|θ) based on a ﬁnite number of simulator runs, as described in Sec. II C 2.

To this end, we ﬁrst generate a parton-level sample {ze} of 5.5·106 events with MadGraph 5 [64]
and its add-on MadMax [76–78], using the setup described in Ref. [4]. With MadMax we can
evaluate the likelihood p(ze|θc) for all events ze and for 15 diﬀerent basis parameter points θc.
Calculating the morphing weights wc(θ) ﬁnally gives us the true parton-level likelihood function

11

Figure 3: Basis points θc and some of the morphing weights wc(θ) for our example process. Each
panel shows the morphing weight of one of the components c as a function of parameter space. The
weights of the remaining 13 components (not shown) follow qualitatively similar patterns. The dots
show the position of the basis points θc, the big black dot denotes the basis point corresponding to
the morphing weight shown in that panel. Away from the morphing basis points, the morphing
weights can easily reach O (100), with large cancellations between diﬀerent components.

p(ze|θ) for each generated phase-space point ze.

In Fig. 3 we show the basis points θc and two of the morphing weights wc(θ) with their dependence
on θ. In some corners of parameter space the weights easily reach up to |wc| (cid:46) O (100), and there
are large cancellations between positive and negative weights. This will pose a challenge for the
numerical stability of every inference algorithm that directly uses the morphing structure of the
process, as we will discuss later. Other basis choices have led to comparable or larger morphing
weights.

Parton shower and detector eﬀects smear the observed particle properties x with respect to
the parton-level momenta z and make the likelihood function in Eq. (2) intractable. We develop
inference methods that can be applied exactly in this case and that do not require any simplifying
assumptions on the shower and detector response. However, in this realistic scenario we cannot
evaluate their performance by comparing them to the true likelihood ratio. We therefore test them
ﬁrst on an idealized scenario in which the four-momenta, ﬂavor, and charges of the leptons, and the
momenta of the partons, can be measured exactly, p(x|z) ≈ δ(x − z). In this approximation we can
evaluate the likelihood p(x|θ).

After establishing the performance of the various algorithms in this idealized setup, we will analyse
the eﬀect of parton shower and detector simulation on the results. We generate an approximate
detector-level sample by drawing events from a smearing distribution p(x|z) conditional on the
parton-level momenta z. This smearing function is loosely motivated by the performance of the
LHC experiments and is deﬁned in Appendix A 1.

III. LIKELIHOOD RATIO ESTIMATION

According to the Neyman-Pearson lemma, the likelihood ratio

r(x|θ0, θ1) ≡

p(x|θ0)
p(x|θ1)

=

(cid:82) dz p(x, z|θ0)
(cid:82) dz p(x, z|θ1)

(11)

12

is the most powerful test statistic to discriminate between two hypotheses θ0 and θ1. Unfortunately,
the integral over the latent space z makes the likelihood function p(x|θ) as well as the likelihood
ratio r(x|θ0, θ1) intractable. The ﬁrst and crucial stage of all our EFT measurement strategies
is therefore the construction of a likelihood ratio estimator ˆr(x|θ0, θ1) that is as close to the true
r(x|θ0, θ1) as possible and thus maximizes the discrimination power between θ0 and θ1.

This estimation problem has several diﬀerent aspects that we try to disentangle as much as
possible. The ﬁrst choice is the overall structure of the likelihood ratio estimator and its dependence
on the theory parameters θ. We discuss this in Sec. III A. Section III B analyses what information is
available and useful to construct (train) the estimators for a given process. Here we will introduce
the main ideas that harness the structure of the EFT to increase the information that is used in the
training process.

These basic concepts are combined into concrete strategies for the estimation of the likelihood ratio
in Sec. III C. After training the estimators, there is an optional additional calibration stage, which
we introduce in Sec. III D. Section III E describes the technical implementation of these strategies
in terms of neural networks. Finally, we discuss the challenges that the diﬀerent algorithms face in
Sec. III F and introduce diagnostic tools for the uncertainties.

A. Modeling likelihood ratios

1. Likelihood ratios

There are diﬀerent approaches to the structure of this estimator, in particular to the dependence

on the theory parameters θ:

Point by point (PbP): A common strategy is to scan the parameter space, randomly or in a
grid. To reduce the complexity of the scan one can keep the denominator θ1 ﬁxed, while
scanning only θ0. Likelihood ratios with other denominators can be extracted trivially as
ˆr(x|θ0, θ2) = ˆr(x|θ0, θ1)/ˆr(x|θ2, θ1). Instead of a single reference value θ1, we can also use
a composite reference hypothesis p(x|θ1) → pref(x) = (cid:82) dθ1 π(θ1) p(x|θ1) with some prior
π(θ1). This can reduce the regions in feature space with small reference likelihood p(x|θ1)
and improve the numerical stability.

For each pair (θ0, θ1) separately, the likelihood ratio ˆr(x|θ0, θ1) as a function of x is estimated.
Only the ﬁnal results are interpolated between the scanned values of θ0.
This approach is particularly simple, but discards all information about the structure and
smoothness of the parameter space. For high-dimensional parameter spaces, the parameter
scan can become prohibitively expensive. The ﬁnal interpolation may introduce additional
uncertainties.

Agnostic parameterized estimators: Alternatively we can train one estimator as the full model
ˆr(x|θ0, θ1) as a function of both x and the parameter combination (θ0, θ1) [34, 79]. A
modiﬁcation is again to leave θ1 at a ﬁxed reference value (or ﬁxed composite reference
hypothesis with a prior π(θ1)) and only learn the dependence on x and θ0.
This parameterized approach leaves it to the estimator to learn the typically smooth depend-
ence of the likelihood ratio on the physics parameters and does not require any interpolation
in the end. There are no assumptions on the form of the dependence of the likelihood on the
ratios.

Morphing-aware estimators: For problems that satisfy the morphing condition of Eq. (6) and
thus also Eq. (7), we can impose this structure and the explicit knowledge of the weights

13

(12)

(13)

(14)

wc(θ) onto the estimator. Again, one option is to keep the denominator ﬁxed at a reference
value (or composite reference hypothesis), leading to

ˆr(x|θ0, θ1) =

wc(θ0) ˆrc(x)

(cid:88)

c

where the basis estimators ˆrc(x) = ˆr(x|θc, θ1) only depend on x.

Alternatively, we can decompose both the numerator and denominator distributions to
ﬁnd [34, 80]

ˆr(x|θ0, θ1) =

(cid:34)

(cid:88)

(cid:88)

c

c(cid:48)

wc(cid:48)(θ1)
wc(cid:48)(θ0)

(cid:35)−1

ˆrc(cid:48),c(x)

with pairwise estimators ˆrc(cid:48),c(x) = ˆr(x|θc(cid:48), θc).

One remarkably powerful quantity is the score, deﬁned as the relative tangent vector

2. Score and local model

(cid:12)
(cid:12)
t(x|θ0) = ∇θ log p(x|θ)
(cid:12)θ0

.

It quantiﬁes the relative change of the likelihood under inﬁnitesimal changes in parameter space
and can be seen as a local equivalent of the likelihood ratio.

In a small patch around θ0 in which we can approximate t(x|θ) as independent of θ, Eq. (14) is

solved by the local model

plocal(x|θ) =

p(t(x|θ0)|θ0) exp[t(x|θ0) · (θ − θ0)]

(15)

1
Z(θ)

with a normalisation factor Z(θ). The local model is in the exponential family. Note that the t(x|θ0)
are the suﬃcient statistics for plocal(x|θ). This is signiﬁcant: if we can estimate the vector-valued
function t(x|θ0) (with one component per parameter of interest) of the high-dimensional x, we can
reduce the dimensionality of our space dramatically without losing any information, at least in the
local model approximation [81].

In fact, ignoring the normalization factors and in the local model the likelihood ratio between θ0
and θ1 only depends on the scalar product between the score and θ0 − θ1, which will allow us to
take this dimensionality reduction one step further and compress high-dimensional data x into a
scalar without loss of power.

In our example process, we are interested in the Wilson coeﬃcients of two dimension-six operators.
The score vector therefore has two components. In Fig. 4 we show the relation between these two
score components and two informative kinematic variables, the jet pT and the azimuthal angle
between the two jets, ∆φ. We ﬁnd that the score vector is very closely related with these two
kinematic quantities, but the relation is not quite one-to-one. Larger energy transfer, measured as
larger jet pT , increases the typical size of the score vector. The OW W component of the score is
particularly sensitive to the angular correlation variable, in agreement with detailed studies of this
process [4].

14

(16)

(17)

(18)

Figure 4: Score vector as a function of kinematic observables in our example process. Left: ﬁrst
component of the score vector, representing the relative change of the likelihood with respect to
small changes in OW direction. Right: second component of the score vector, representing the
relative change of the likelihood with respect to small changes in OW W direction. In both panels,
the axes show two important kinematic variables. We ﬁnd that the score vector is clearly correlated
with these two variables.

B. Available information and its usefulness

1. General likelihood-free case

All measurement strategies have in common that the estimator ˆr(x|θ0, θ1) is learned from data
In the most general
provided by Monte-Carlo simulations (the stochastic generative process).
likelihood-free scenario, we can only generate samples of events {xe} with xe ∼ p(x|θ) through the
simulator, and base an estimator ˆr(x|θ0, θ1) on these generated samples.

One strategy [34] is based on training a classiﬁer with decision function ˆs(x) between two
equal-sized samples {xe} ∼ p(x|θ0), labelled ye = 0, and {xe} ∼ p(x|θ1), labelled ye = 1. The
cross-entropy loss functional

L[ˆs] = −

(ye log ˆs(xe) + (1 − ye) log(1 − ˆs(xe)))

1
N

(cid:88)

e

is minimized by the optimal decision function

From the decision function ˆs(x) of a classiﬁer we can therefore extract an estimator for the likelihood
ratio as

s(x|θ0, θ1) =

p(x|θ1)
p(x|θ0) + p(x|θ1)

.

ˆr(x|θ0, θ1) =

1 − ˆs(x|θ0, θ1)
ˆs(x|θ0, θ1)

.

This idea, sometimes called the likelihood ratio trick, is visualized in the left panel of Fig. 5.

As pointed out in Ref. [34], we can use the weaker assumption of any loss functional that is
minimized by a decision function s(x) that is a strictly monotonic function of the likelihood ratio.
The underlying reason is that the likelihood ratio is invariant under any transformation s(x) with
this property. In practice, the output of any such classiﬁer can be brought closer to the form of
Eq. (17) through a calibration procedure, which we will discuss in Sec. III D.

15

Figure 5: Illustration of some key concepts with a one-dimensional Gaussian toy example. Left:
classiﬁers trained to distinguish two sets of events generated from diﬀerent hypotheses (green dots)
converge to an optimal decision function s(x|θ0, θ1) (in red) given in Eq. (17). This lets us extract
the likelihood ratio. Right: regression on the joint likelihood ratios r(xe, ze|θ0, θ1) of the simulated
events (green dots) converges to the likelihood ratio r(x|θ0, θ1) (red line).

2. Particle-physics structure

As we have argued in Sec. II C, particle physics processes have a speciﬁc structure that allow
us to extract additional information. Most processes satisfy the factorization of Eq. (2) with a
tractable parton-level likelihood p(z|θ). The generators do not only provide samples {xe}, but
also the corresponding parton-level momenta (latent variables) {ze} with (xe, ze) ∼ p(x, z|θ0). By
evaluating the matrix elements at the generated momenta ze for diﬀerent hypotheses θ0 and θ1,
we can extract the parton-level likelihood ratio p(ze|θ0)/p(ze|θ1). Since the distribution of x is
conditionally independent of the theory parameters, this is the same as the joint likelihood ratio

r(xe, zall e|θ0, θ1) ≡

p(xe, zdetector e, zshower e, ze|θ0)
p(xe, zdetector e, zshower e, ze|θ1)
p(xe|zdetector e)
p(xe|zdetector e)
p(ze|θ0)
p(ze|θ1)

p(zdetector e|zshower e)
p(zdetector e|zshower e)

.

=

=

p(zshower e|ze)
p(zshower e|ze)

p(ze|θ0)
p(ze|θ1)

(19)

So while we cannot directly evaluate the likelihood ratio at the level of measured observables
r(x|θ0, θ1), we can calculate the likelihood ratio for a generated event conditional on the latent
parton-level momenta.

The same is true for the score, i. e. the tangent vectors or relative change of the (log) likelihood
under inﬁnitesimal changes of the parameters of interest. While the score t(xe|θ0) = ∇θ log p(x|θ)|θ0

16

(20)

(cid:125)

(21)

Figure 6: Illustration of some key concepts with a one-dimensional Gaussian toy example. Left:
probability density functions for diﬀerent values of θ and the scores t(xe, ze|θ) at generated events
(xe, ze). These tangent vectors measure the relative change of the density under inﬁnitesimal
changes of θ. Right: dependence of log p(x|θ) on θ for ﬁxed x = 4. The arrows again show the
(tractable) scores t(xe, ze|θ).

is intractable, we can extract the joint score

t(xe, zall e|θ0) ≡ ∇θ log p(xe, zdetector e, zshower e, ze|θ0)
p(zdetector e|zshower e)
p(zdetector e|zshower e)

=

p(zshower e|ze)
p(zshower e|ze)

∇θp(ze|θ)
p(ze|θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)θ0

p(xe|zdetector e)
p(xe|zdetector e)
∇θp(ze|θ)
p(ze|θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)θ0

=

from the simulator. Again, all intractable parts of the likelihood cancel. We visualize the score
in Fig. 6 and all available information on the generated samples in Fig. 7. It is worth repeating
that we are not making any simplifying approximations about the process here, these statements
are valid with reducible backgrounds, for state-of-the-art generators including higher-order matrix
elements, matching of matrix element and parton shower, and with full detector simulations.

But how does the availability of the joint likelihood ratio r(x, z|θ) and score t(x, z|θ) (which
depend on the latent parton-level momenta z) help us to estimate the likelihood ratio r(x|θ), which
is the one we are interested in?

Consider the L2 squared loss functional for functions ˆg(x) that only depend on x, but which are

trying to approximate a function g(x, z),

L[ˆg(x)] =

dx dz p(x, z|θ) |g(x, z) − ˆg(x)|2

=

dx

ˆg2(x)

dz p(x, z|θ) − 2ˆg(x)

dz p(x, z|θ) g(x, z) +

(cid:90)

(cid:90)

(cid:90)

(cid:21)
dz p(x, z|θ) g2(x, z)

.

(cid:90)

(cid:90)

(cid:20)

(cid:124)

(cid:123)(cid:122)
F (x)

17

Figure 7: Illustration of some key concepts with a one-dimensional Gaussian toy example. Left: full
statistical model log r(x|θ, θ1) that we are trying to estimate. Right: available information at the
generated events (xe, ze). The dots mark the joint likelihood ratios log r(xe, ze|θ0, θ1), the arrows
the scores t(xe, ze|θ0, θ1).

Via calculus of variations we ﬁnd that the function g∗(x) that extremizes L[ˆg] is given by [53]

0 =

= 2ˆg

dz p(x, z|θ)

−2

dz p(x, z|θ) g(x, z) ,

(22)

δF
δˆg

(cid:12)
(cid:12)
(cid:12)
(cid:12)g∗

(cid:90)

(cid:124)

(cid:123)(cid:122)
=p(x|θ)

(cid:125)

(cid:90)

therefore

g∗(x) =

(cid:90)

1
p(x|θ)

dz p(x, z|θ) g(x, z) .

(23)

We can make use of this general property in our problem in two ways. Identifying g(xe, ze) with

the joint likelihood ratios r(xe, zall,e|θ0, θ1) (which we can calculate!) and θ = θ1, we ﬁnd

g∗(x) =

(cid:90)

1
p(x|θ1)

p(x, z|θ0)
p(x, z|θ1)

dz p(x, z|θ1)

= r(x|θ0, θ1) .

(24)

By minimizing the squared loss

L[ˆr(x|θ0, θ1)] =

|r(xe, zall,e|θ0, θ1) − ˆr(xe|θ0, θ1)|2

(25)

1
N

(cid:88)

(xe,ze)∼p(x,z|θ1)

of a suﬃciently expressive function ˆr(x|θ0, θ1), we can therefore regress on the true likelihood
ratio [53]! This is illustrated in the right panel of Fig. 5. Note that to get the correct minimum, the
events (xe, ze) have to be sampled according to the denominator hypothesis θ1.

We can also identify g(xe, ze) in Eq. (22) with the scores t(xe, zall,e|θ), which can also be extracted

from the generator. In this case,

g∗(x) =

(cid:90)

1
p(x|θ)

dz ∇θp(x, z|θ) = t(x|θ) .

(26)

18

General likelihood-free Particle physics

(cid:88)

Quantity

Samples
Likelihood
Likelihood ratio
Score

{xe}
p(xe|θ)
r(xe|θ0, θ1)
t(xe|θ)

Latent state
Joint likelihood
Joint likelihood ratio
Joint score

{xe, ze}
p(xe, zall,e|θ)
r(xe, zall,e|θ0, θ1)
t(xe, zall,e|θ)

(cid:88)

∗
∗

(cid:88)

(cid:88)
(cid:88)

Table II: Availability of diﬀerent quantities from the generative process in the most general
likelihood-free setup vs. in the particle-physics scenario with the structure given in Eq. (2).
Asterisks (∗) denote quantities that are not immediately available, but can be regressed from the
corresponding joint quantity, as shown in Sec. III B.

Thus minimizing

L[ˆt(x|θ)] =

1
N

(cid:88)

(xe,ze)∼p(x,z|θ)

|t(xe, zall,e|θ) − ˆt(xe|θ)|2

(27)

of a suﬃciently expressive function ˆt(x|θ) allows us to regress on the score t(x|θ) [53].3 Now the
(xe, ze) have to be sampled according to θ. We summarize the availability of the (joint) likelihood,
likelihood ratio, and score in the most general likelihood-free setup and in particle physics processes
in Table II.

This is one of our key results and opens the door for powerful new inference methods. Particle
physics processes involve the highly complex eﬀects of parton shower, detector, and reconstruction,
modelled by a generative process with a huge latent space and an intractable likelihood. Still, the
speciﬁc structure of this class of processes allows us to calculate how much more or less likely a
generated event becomes when we move in the parameter space of the theory. We have shown that
by regressing on the joint likelihood ratios or scores extracted in this way, we can recover the actual
likelihood ratio or score as a function of the observables!

C. Strategies

Let us now combine the estimator structure discussed in Sec. III A with the diﬀerent quantities
available during training discussed in Sec. III B and deﬁne our strategies to estimate the likelihood
ratio. Here we restrict ourselves to an overview over the main ideas of the diﬀerent approaches. A
more detailed explanation and technical details can be found in Appendix A 2.

1. General likelihood-free case

Some approaches are designed for the most general likelihood-free scenario and only require the

samples {xe} from the generator:

3 A similar loss function (with a non-standard use of the term “score”) was used in Ref. [82], though the derivative is

taken with respect to x and, critically, the model did not involve marginalization over the latent variable z.

19

(28)

(30)

(31)

(32)

Histograms of observables: The traditional approach to kinematic analyses relies on one or two
kinematic variables v(x), manually chosen for a given process and set of parameters. Densities
ˆp(v(x)|θ) are estimated by ﬁlling histograms with generated samples, leading to the likelihood
ratio

ˆr(x|θ0, θ1) =

ˆp(v(x)|θ0)
ˆp(v(x)|θ1)

.

We use this algorithm point by point in θ0, but a morphing-based setup is also possible (see
Sec. II C 2). We discuss the histogram approach in more detail in Appendix A 2 a.

Approximate Frequentist Computation (Afc): Approximate Bayesian Computation (Abc)
is currently the most widely used method for likelihood-free inference in a Bayesian setup.
It allows to sample parameters from the intractable posterior, θ ∼ p(θ|x) = p(x|θ)p(θ)/p(x).
Essentially, Abc relies on the approximation of the likelihood function through a rejection
probability

prejection(x|θ) = K(cid:15)(v(x), v(xe)) ,

(29)

with xe ∼ p(x|θ), a kernel K(cid:15) that depends on a bandwidth (cid:15), and a suﬃciently low-dimensional
summary statistics v(x).

Inference in particle physics is usually performed in a frequentist setup, so this sampling
mechanism is not immediately useful. But we can deﬁne a frequentist analogue, which we
call “Approximate Frequentist Computation” (Afc). In analogy to the rejection probability
in Eq. 29, we can deﬁne a kernel density estimate for the likelihood function as

The corresponding likelihood ratio estimator is

ˆp(x|θ) =

K(cid:15)(v(x), v(xe)) .

1
N

(cid:88)

e

ˆr(x|θ0, θ1) =

ˆp(x|θ0)
ˆp(x|θ1)

.

We use this approach point by point in θ0 with a ﬁxed reference θ1. As summary statistics, we
use subsets of kinematic variables, similar to the histogram approach. We give more details
in Appendix A 2 b.

Calibrated classiﬁers (Carl4): As discussed in Sec. III B 1, the decision function ˆs(x|θ0, θ1) of
a classiﬁer trained to discriminate between samples generated according to θ0 from θ1 can be
turned into an estimator for the likelihood ratio

ˆr(x|θ0, θ1) =

1 − ˆs(x|θ0, θ1)
ˆs(x|θ0, θ1)

.

This is illustrated in the left panel of Fig. 5.

If the classiﬁer does not learn the optimal decision function of Eq. (17), but any mono-
tonic function of the likelihood ratio, a calibration procedure can improve the performance
signiﬁcantly. We will discuss this in Sec. III D below.

We implement this strategy point by point in θ0, as an agnostic parameterized classiﬁer
ˆr(x|θ0, θ1) that learns the dependence on both x and θ0, as well as a morphing-aware para-
meterized classiﬁer. More details are given in Appendix A 2 c.

4 Calibrated ratios of likelihoods

20

Neural conditional density estimators (Nde): Several other methods for conditional density
estimation have been proposed, often based on neural networks [36–42]. One particularly
interesting class of methods for density estimation is based on the idea of expressing the
target density as a sequence of invertible transformations applied to a simple initial density,
such as a Gaussian [43–46, 51]. The density in the target space is then given by the Jacobian
determinant of the transformation and the base density. A closely related and successful
alternative are neural autoregressive models [47–50], which factorize the target density as a
sequence of simpler conditional densities. Both classes of estimators are trained by maximizing
the log likelihood.

We leave a detailed discussion of these techniques for particle physics problems as well as an
implementation in our example process for future work.

2. Particle-physics structure

As we argued in Sec. III B, particle physics simulations let us extract the joint likelihood ratio
r(xe, ze|θ0, θ1) and the joint score t(xe, ze|θ0, θ1), giving rise to strategies tailored to this class of
problems:

Ratio regression (Rolr5): We can directly regress the likelihood ratio ˆr(x|θ0, θ1). As shown
in the previous section, the squared error loss between a function ˆr(xe|θ0, θ1) and the avail-
able joint likelihood ratio r(xe, ze|θ0, θ1) is minimized by the likelihood ratio r(x|θ0, θ1),
provided that the samples (xe, ze) are drawn according to θ1. Conversely, the squared error
of 1/r(xe, ze|θ0, θ1) with (xe, ze) ∼ p(x, z|θ0) is also minimized by the likelihood ratio. We
can combine these two terms into a combined loss function

L[ˆr(x|θ0, θ1)] =

ye |r(xe, ze|θ0, θ1) − ˆr(x|θ0, θ1)|2

1
N

(cid:32)

(cid:88)

(xe,ze,ye)

+ (1 − ye)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
r(xe, ze|θ0, θ1)

−

1
ˆr(x|θ0, θ1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2(cid:33)

(33)

with ye = 0 for events generated according to (xe, ze) ∼ p(x, z|θ0) and ye = 1 for (xe, ze) ∼
p(x, z|θ1). The factors of ye and (1 − ye) ensure the correct sampling for each part of the loss
functional. We illustrate this approach in the right panel of Fig. 5.

This strategy is again implemented point by point in θ0, in an agnostic parameterized setup,
as well as in a morphing-aware parameterized setup. We describe it in more detail in
Appendix A 2 d.

Carl + score regression (Cascal6): The parameterized Carl strategy outlined above learns
a classiﬁer decision function ˆs(x|θ0, θ1) as a function of θ0. If the classiﬁer is realized with
a diﬀerentiable architecture such as a neural network, we can calculate the gradient of this
function and of the corresponding estimator for the likelihood ratio ˆr(x|θ0, θ1) with respect
to θ0 and derive the estimated score

(cid:12)
(cid:12)
ˆt(x|θ0) = ∇θ log ˆr(x|θ, θ1)
(cid:12)
(cid:12)
(cid:12)θ0

= ∇θ log

1 − ˆs(x|θ0, θ1)
ˆs(x|θ0, θ1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)θ0

.

(34)

5 Regression on likelihood ratio
6 CARL and score approximate likelihood ratio

21

If the estimator is perfect, we expect this estimated score to minimize the squared error
with respect to the joint score data available from the simulator, following the arguments in
Sec. III B.

We can turn this argument around and use the available score data during the training.
Instead of training the classiﬁer just by minimizing the cross-entropy, we can instead sim-
ultaneously minimize the squared error on this derived score with respect to the true joint
score t(x, z|θ0, θ1). The combined loss function is given by

(cid:88)

(cid:34)
ye log ˆs(xe)+(1−ye) log(1− ˆs(xe))+α (1−ye) (cid:12)

(cid:12)t(xe, ze|θ0) − ˆt(xe|θ0)(cid:12)
2
(cid:12)

(35)

(cid:35)

L[ˆs] =

1
N

e

with ˆt(x|θ0) deﬁned in Eq. (34) and a hyperparameter α that weights the two pieces of
the loss function relative to each other. Again, ye = 0 for events generated according to
(xe, ze) ∼ p(x, z|θ0) and ye = 1 for (xe, ze) ∼ p(x, z|θ1), and the factors of ye and (1 − ye)
ensure the correct sampling for each part of the loss functional.

This strategy relies on the parameterized modeling of the likelihood ratio. We implement
both an agnostic version as well as a morphing-aware model. See Appendix A 2 e for more
details.

Neural conditional density estimators + score (Scandal7): In the same spirit as the Cas-
cal method, neural density estimators such as autoregressive ﬂows can be augmented with
score information. We have started to explore this class of algorithms in Ref. [53], but leave
a detailed study and the application to particle physics for future work.

Ratio + score regression (Rascal8): The same trick works for the parameterized Rolr ap-
If the regressor is implemented as a diﬀerentiable architecture such as a neural
proach.
network, we can calculate the gradient of the parameterized estimator ˆr(x|θ0, θ1) with respect
to θ0 and calculate the score

(cid:12)
(cid:12)
ˆt(x|θ0) = ∇θ log ˆr(x|θ, θ1)
(cid:12)
(cid:12)
(cid:12)θ0

.

(36)

Instead of training just on the squared likelihood ratio error, we can minimize the combined
loss

L[ˆr(x|θ0, θ1)] =

ye |r(xe, ze|θ0, θ1) − ˆr(xe|θ0, θ1)|2

1
N

(cid:34)

(cid:88)

(xe,ze,ye)

+ (1 − ye)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
r(xe, ze|θ0, θ1)

−

1
ˆr(xe|θ0, θ1)

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)

(cid:35)

+ α (1 − ye) (cid:12)

(cid:12)t(xe, ze|θ0) − ˆt(xe|θ0)(cid:12)
2
(cid:12)

(37)

with ˆt(x|θ0) deﬁned in Eq. (36) and a hyperparameter α. The likelihood ratios and scores
again provide complementary information as shown in the Fig. 7.

7 Score and neural density approximate likelihood
8 Ratio and score approximate likelihood ratio

22

Once more we experiment with both an agnostic parameterized model as well as a morphing-
aware version.

This technique uses all the available data from the simulator that we discussed in Sec. III B to
train an estimator of particularly high ﬁdelity. It is essentially a machine-learning version of
the Matrix Element Method. It replaces computationally expensive numerical integrals with
an upfront regression phase, after which the likelihood ratio can be evaluated very eﬃciently.
Instead of manually specifying simpliﬁed smearing functions, the eﬀect of parton shower and
detector is learned from full simulations. For more details on Rascal, see Appendix A 2 f.

Local score regression and density estimation (Sally9): In the local model approximation
discussed in Sec. III A 2, the score evaluated at some reference point θscore is the suﬃcient
statistics, carrying all the information on θ. A precisely estimated score vector (with one
component per parameter of interest) is therefore the ideal summary statistics, at least in the
neighborhood of the Standard Model or any other reference parameter point.

In the last section we argued that we can extract the joint score t(xe, ze|θscore) from the
simulator. We showed that the squared error between a function ˆt(x|θscore) and the joint
score is minimized by the intractable score t(x|θscore), as long as the events are sampled as
(xe, ze) ∼ p(x, z|θscore). We can thus use the augmented data to train an estimator ˆt(x|θscore)
for the score at the reference point.
In a second step, we can then estimate the likelihood ˆp(ˆt(x|θscore)|θ) with histograms, KDE,
or any other density estimation technique, yielding the likelihood ratio estimator

ˆr(x|θ0, θ1) =

ˆp (cid:0)ˆt(x|θscore) (cid:12)
ˆp (cid:0)ˆt(x|θscore) (cid:12)

(cid:12) θ0
(cid:12) θ1

(cid:1)
(cid:1) .

(38)

This particularly straightforward strategy is a machine-learning analogue of Optimal Ob-
servables that learns the eﬀect of parton shower and detector from data. After an upfront
regression phase, the analysis of an event only requires the evaluation of one estimator to
draw conclusions about all parameters. See Appendix A 2 g for more details.

Local score regression, compression to scalar, and density estimation (Sallino10): The
Sally technique compresses the information in a high-dimensional vector of observables x
into a lower-dimensional estimated score vector. But for measurements in high-dimensional
parameter spaces, density estimation in the estimated score space might still be computation-
ally expensive. Fortunately, the local model of Eq. (15) motivates an even more dramatic
dimensionality reduction to one dimension, independent of the number of parameters: Disreg-
arding the normalization constants, the ratio r(x|θ0, θ1) only depends on the scalar product
between the score and θ0 − θ1.
Given the same score estimator ˆt(x|θscore) developed for the Sally method, we can deﬁne
the scalar function

ˆh(x|θ0, θ1) ≡ ˆt(x|θSM ) · (θ0 − θ1) .

(39)

Assuming a precisely trained score estimator, this scalar encapsulates all information on the
likelihood ratio between θ0 and θ1, at least in the local model approximation. The likelihood

9 Score approximates likelihood locally
10 Score approximates likelihood locally in one dimension

23

Estimator versions

Loss function

Asymptotically exact

CE ML Ratio Score

Strategy

Histograms
Afc
Carl
Nde

Rolr
Cascal
Scandal
Rascal
Sally
Sallino

(cid:88)
(cid:88)
(cid:88)
((cid:88))
(cid:88)

PbP Param Aware
((cid:88))
((cid:88))
(cid:88)
((cid:88))
(cid:88)
(cid:88)
((cid:88))
(cid:88)
((cid:88))
((cid:88))

(cid:88)
((cid:88))
(cid:88)
(cid:88)
((cid:88))
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

Table III: Overview over the discussed measurement strategies. The ﬁrst three techniques can be
applied in the general likelihood-free setup, they only require sets of generated samples {xe}. The
remaining ﬁve methods are tailored to the particle physics structure and require the availability of
r(xe, ze|θ0, θ1) or t(xe, ze|θ0) from the generator, as discussed in Sec. III B. Brackets denote possible
variations that we have not implemented for our example process. In the Sally and Sallino
strategies, “estimator versions” refers to the density estimation step. In the loss function columns,
“CE” stands for the cross-entropy, “ML” for maximum likelihood, “ratio” for losses of the type
|r(x, z) − ˆr(x)|2, and “score” for terms such as |t(x, z) − ˆt(x)|2.

ratio can then be estimated as

ˆr(x|θ0, θ1) =

(cid:16)ˆh(x|θ0, θ1)
(cid:16)ˆh(x|θ0, θ1)

ˆp

ˆp

(cid:17)

(cid:17) ,

(cid:12)
(cid:12)
(cid:12) θ0
(cid:12)
(cid:12)
(cid:12) θ1

(40)

where the ˆp(ˆh) are simple univariate density estimators.

This method allows us to condense any high-dimensional observation x into a scalar function
without losing sensitivity, at least in the local model approximation. It thus scales exceptionally
well to problems with many theory parameters. We describe Sallino in more detail in
Appendix A 2 h.

Several other variants are possible, including other combinations of the loss functionals discussed

above. We leave this for future work.11

We summarize the diﬀerent techniques in Table III. With this plethora of well-motivated analysis
methods, the remaining key question for this paper is how well they do in practice, and which of
them (if any) should be used. This will be the focus of Sec. V. In the next sections, we will ﬁrst
discuss some important additional aspects of these methods.

11 Not all initially promising strategies work in practice. We experimented with an alternative strategy based on the
morphing structure in Eq. (6). Consider the case in which the training sample consists of a number of sub-samples,
each generated according to a morphing basis point θc. Then the morphing basis sample c used in the event
generation is a latent variable that we can use instead of the parton-level momenta z to deﬁne joint likelihood
ratios and joint scores. Regressing on these quantities should converge to the true likelihood ratio and score, in
complete analogy to the discussion in Sec. III B. But the joint ratios and scores deﬁned in this way span a huge
range, and the densities of the diﬀerent basis points are very similar. The convergence is therefore extremely slow
and the results based on this method suﬀer from a huge variance.

D. Calibration

While the likelihood ratio estimators described above work well in many cases, their performance
can be further improved by an additional calibration step. Calibration takes place after the “raw”
or uncalibrated estimators ˆrraw(x|θ0, θ1) have been trained. In general, it deﬁnes a function C with
the aim that ˆrcal = C(ˆrraw) provides a better estimator of the true likelihood ratio than ˆrraw. We
consider two diﬀerent approaches to deﬁning this function C, which we call probability calibration
and expectation calibration.

1. Probability calibration

Consider the Carl strategy, which trains a classiﬁer with a decision function s(x) as the basis
for the likelihood ratio estimation. Even if this classiﬁer can separate the two classes of events from
θ0 and θ1 well, its decision function ˆs(x|θ0, θ1) might not have a direct probabilistic interpretation:
it might be any approximately monotonic function of the likelihood ratio rather than the ideal
solution given in Eq. (17). In this case, the Carl approach requires calibration, an additional
transformation of the raw output ˆrraw = ˆr(x|θ0, θ1) into a calibrated decision function

where the densities ˆp(ˆrraw|θ) are estimated through a univariate density estimation technique such as
histograms. This calibration procedure does not only apply to classiﬁers, but to any other likelihood
ratio estimation strategy.

ˆrcal = C(ˆrraw) =

ˆp(ˆrraw|θ0)
ˆp(ˆrraw|θ1)

,

2. Expectation calibration

Consider some likelihood ratio r(x|θ0, θ1). The expectation value of this ratio assuming θ1 to be

true is given by

E[r(x|θ0, θ1)|θ1] =

dx p(x|θ1)

(cid:90)

p(x|θ0)
p(x|θ1)

= 1 .

A good estimator for the likelihood ratio should reproduce this property. We can numerically
approximate this expectation value by evaluating ˆr(x|θ, θ1) on a sample {xe} of N events drawn
according to θ1,

If a likelihood ratio estimator ˆrraw(x|θ, θ1) (which might be entirely uncalibrated or already

probability calibrated) does not satisfy this condition, we can calibrate it by rescaling it as

For a perfect estimator with ˆr(x|θ0, θ1) = r(x|θ0, θ1), we can even calculate the variance of the

numeric calculation of the expectation value in Eq. (43). We ﬁnd

ˆR(θ) =

1
N

(cid:88)

xe∼θ1

ˆr(xe|θ, θ1) ≈ 1 .

ˆrcal(x|θ, θ1) =

ˆrraw(x|θ, θ1)
ˆRraw(θ)

.

var[ ˆR(θ)] =

[E [ˆr(x|θ, θ1)|θ] − 1] ,

1
N

24

(41)

(42)

(43)

(44)

(45)

where N is the number of events used to calculate the expectation value R(θ), and the expectation
E [ˆr(x|θ, θ1)|θ] (under the numerator hypothesis!) can be calculated numerically.

This calibration strategy can easily improve classiﬁers that are oﬀ by some θ-dependent factor.
However, a few rare events xe with large ˆr(xe|θ, θ1) can dominate the expectation value. If these are
mis-estimated, the expectation calibration can actually degrade the performance of the estimator
on the bulk of the distribution with smaller ˆr(x|θ, θ1).

25

E.

Implementation

1. Neural networks

With the exception of the simple histogram and AFC methods, all strategies rely on a classiﬁer
ˆs(x|θ0, θ1), score regressor ˆt(x|θ0), or ratio regressor ˆr(x|θ0, θ1) that is being learnt from training
data. For our explicit example, we implement these functions as fully connected neural networks:
• In the point-by-point setup, the neural networks take the features x as input and models
log ˆr(x|θ0, θ1). For ratio regression, this is exponentiated to yield the ﬁnal output ˆr(x|θ0, θ1).
In the Carl strategy, the network output is transformed to a decision function

ˆs(x|θ0, θ1) =

1
1 + ˆr(x|θ0, θ1)

.

(46)

• In the agnostic parameterized setup, the neural networks take both the features x as well
as the numerator parameter θ0 as input and model log ˆr(x|θ0, θ1). In addition to the same
subsequent transformations as in the point-by-point case, taking the gradient of the network
output gives the estimator score.

• In the morphing-aware setup, the estimator takes both x and θ0 as input. The features x
are fed into a number of independent networks, one for each basis component c, that model
the basis ratios log rc(x). From θ0 the estimator calculates the component weights wc(θ0)
analytically. The components are then combined with Eq. (12). For the Carl approaches,
the output is again transformed with Eq. (46), and for the score-based strategies the gradient
of the output is calculated.
We visualize these architectures in Fig. 8.

All networks are implemented in shallow, regular, and deep versions with 2, 3, and 5 hidden layers
of 100 units each and tanh activation functions. They are trained with the Adam optimizer [83]
over 50 epochs with early stopping and learning rate decay. We implement them in keras [84] with
a TensorFlow [85] backend. Experiments modeling s rather than log r, with diﬀerent activation
functions, adding dropout layers, or using other optimizers and learning rate schedules have led to
a worse performance.

2. Training samples

Starting from the weighted event sample described in Sec. II D 2, we draw events (xe, ze) randomly
with probabilities given by the corresponding p(xe, ze|θ). Due to the form of the likelihood p(x, z|θ)
and due to technical limitations of our simulator, individual data points can carry large probabilities
p(xe, ze|θ), leading to duplicate events in the training samples. However, we enforce that there is
no duplication between training and evaluation samples, so this limitation can only degrade the
performance.

For the point-by-point setup, we choose 100 values of θ0, 5 of which are ﬁxed at the SM
(θ0 = (0, 0)) and at the corners of the considered parameter space, with the remaining 95 chosen

x

θ0

x

θ0

x

26

log ˆr

for each θ0

ˆs

ˆr

ˆt

ˆs

ˆr

ˆt

ˆs

ˆr

log ˆr

wc

log ˆr

log ˆrc

for each c

Figure 8: Schematic neural network architectures for point-by-point (top), agnostic parameterized
(middle), and morphing-aware parameterized (bottom) estimators. Solid lines denote dependencies
with learnable weights, dashed lines show ﬁxed functional dependencies.

27

(47)

randomly with a ﬂat prior over −1 ≤ θo ≤ 1. For each of these training points we sample 250 000
events according to θ0 and 250 000 events according to the reference hypothesis

θ1 = (0.393, 0.492)T .

For the parameterized strategies, we compare three diﬀerent training samples, each consisting of
107 events:

Baseline: For 1000 values of θ0 chosen randomly in θ space, we draw 5000 events according to θ0

and 5000 events according to θ1 given in Eq. (47).

Random θ: In this sample, the value of θ0 is drawn randomly independently for each event. Again

we use a ﬂat prior over θ0 ∈ [−1, 1]2.

Morphing basis: For each of the 15 basis hypotheses θi from the morphing procedure, we generate

333 000 events according to θ0 = θi and 333 000 according to θ1.

Finally, for the local score regression model we use a sample of 107 events drawn according to

the SM.

Our evaluation sample consists of 50 000 events drawn according to the SM. We evaluate the
likelihood ratio for these events for a total of 1016 values of θ0, 1000 of which are the same as those
used in the baseline training sample. Again we ﬁx θ1 as in Equation (47).

Each event is characterized by 42 features:
• the energies, transverse momenta, azimuthal angles, and pseudo-rapidities of all six particles

• the energies, transverse momenta, azimuthal angles, pseudo-rapidities, and invariant mass of
the four-lepton system as well as the two-lepton systems that reconstruct the two Z bosons;
and

• the invariant mass, separation in pseudorapidity, and separation in azimuthal angle of the

in the ﬁnal state;

di-jet system.

The derived variables in the feature set help the neural networks pick up the relevant features faster,
though we did not ﬁnd that their choice aﬀects the performance signiﬁcantly.

3. Calibration and density estimation

We calibrate the classiﬁers for our example process with probability calibration as described
in Sec. III D 1. We determine the calibration function C(r) with isotonic regression [86], which
constrains C(r) to be monotonic. Experiments with other regression techniques based on histograms,
kernel density estimation, and logistic regression did not lead to a better performance. We apply
the calibration point by point in θ0. It is based on an additional event sample that is independent
of the training and evaluation data. The same events are used to calibrate each value of θ, with
an appropriate reweighting. This strategy to minimize variance is based on the availability of the
parton-level likelihood function p(z|θ).

The techniques based on local score regression require the choice of a reference point to evaluate
the score. For the EFT problem, the natural choice is θscore = θSM = (0, 0)T . In the Sally approach,
we perform the density estimation based on two-dimensional histograms of the estimated score at
the SM, point by point in θ0. For the Sallino technique, we use a one-dimensional histograms of
ˆh(x|θSM), point by point in θ0.

28

Figure 9: Uncertainty ∆ log ˆr(xe|θ, θ1) of morphing-aware estimators due to uncertainties
∆ log ˆrc(xe|θ, θ1) on the individual basis ratios as a function of θ. We ﬁx θ1 as in Eq. (47) and show
one random event xe, the results for other events are very similar. We assume iid Gaussian
uncertainties on the log ˆrc(x|θ, θ1) and use Gaussian error propagation. The white dots show the
position of the basis points θc. Small uncertainties in the individual basis estimators ˆrc(xe|θ, θ1) are
signiﬁcantly increased due to the large morphing weights and can lead to large errors of the
combined estimator ˆr(xe|θ, θ1).

F. Challenges and diagnostics

1. Uncertainties

Even the most evolved and robust estimators will have some deviations from the true likelihood
ratio, which should be taken into account in an analysis as an additional modeling uncertainty.
Most of the estimators developed above converge to the true likelihood ratio in the limit of inﬁnite
training and calibration samples. But with ﬁnite statistics, there are diﬀerent sources of variance
that aﬀect some strategies more than others.

Consider the traditional histogram approach.

In the point-by-point version, each separate
estimator ˆr(x|θ0, θ1) is trained on the small subset of the data generated from a speciﬁc value of
θ0, so the variance from the ﬁnite size of the training data, i. e. the statistical uncertainty from the
Monte-Carlo simulation, is large. At θ0 values between the training points, there are additional
sources of uncertainty from the interpolation. On the other hand, morphing-aware histograms use
all of the training data to make predictions at all points, and since the dependence on θ0 is known,
the interpolation is exact. But the large morphing weights wc(θ0) and the cancellations between
them mean that even small ﬂuctuations in the individual basis histograms can lead to huge errors
on the combined estimator.

Similar patterns hold for the ML-based inference strategies. The point-by-point versions suﬀer
from a larger variance due to small training samples at each point and interpolation uncertainties.
The agnostic parameterized models have more statistics available, but have to learn the more
complex full statistical model including the dependence on θ0. The morphing-aware versions make
maximal use of the physics structure of the process and all the training data, but large morphing
weights can dramatically increase the errors of the individual component estimators log ˆrc(x). We
demonstrate this for our example process in Fig. 9: in some regions of parameter space, in particular
far away from the basis points, the errors on a morphing-aware estimator log ˆr(x|θ, θ1) can easily

29

be 100 times larger than the individual errors on the component estimators log ˆrc(x). The θ0
dependence on this error depends on the choice of the basis points, this uncertainty can thus be
somewhat mitigated by optimizing the basis points or by combining several diﬀerent bases.

2. Diagnostics

After discussing the sources of variance, let us now turn towards diagnostic tools that can help
quantify the size of estimator errors and to assign a modeling uncertainty for the statistical analysis.
These methods are generally closure tests: we can check the likelihood ratio estimators for some
expected behaviour, and use deviations either to correct the results (as in the calibration procedures
described in Sec. III D), to deﬁne uncertainty bands, or to discard estimators altogether. We suggest
the following tests:

Ensemble variance: Repeatedly generating new training data (or bootstrapping the same training
sample) and training the estimators again gives us an ensemble of predictions {ˆrr(x|θ0, θ1)}.
We can use the ensemble variance as a measure of uncertainty of the prediction that is due to
the variance in the training data and random seeds used during the training.

Reference hypothesis variation: Any estimated likelihood ratio between two hypotheses θA, θB

ˆr(x|θA, θB) =

ˆr(x|θA, θ1)
ˆr(x|θB, θ1)

(48)

should be independent of the choice of the reference hypothesis θ1 used in the estimator ˆr.
Training several independent estimators with diﬀerent values of θ1 thus provides another
check of the stability of the results [34].

Much like the renormalization and factorization scale variations that are ubiquitous in particle
physics calculations, this technique does not have a proper statistical interpretation in terms
of a likelihood function. We can still use it to qualitatively indicate the stability of the
estimator under this hyperparameter change.

Ratio expectation: As discussed above, the expectation value of the estimated likelihood ratio
assuming the denominator hypothesis should be very close to one. We can numerically
calculate this expectation value ˆR(θ), see Eq. (43).
In Sec. III D 2 we argued that this
expectation value can be used to calibrate the estimators, but that this calibration can
actually decrease the performance in certain situations.

If expectation calibration is used, the calibration itself has a non-zero variance from the ﬁnite
sample size used to calculate the expectation value ˆR(θ). As we pointed out in Sec. III D 2,
we can calculate this source of statistical uncertainty, at least under the assumption of a
perfect estimator with ˆr(x|θ0, θ1) = r(x|θ0, θ1). The result given in Eq. (45) provides us with
a handle to calculate the statistical uncertainty of this calibration procedure from the ﬁnite
size of the calibration sample. Note that for imperfect estimators, the variance of R[ ˆR] may
be signiﬁcantly larger.

Independent of whether expectation calibration is part of the estimator, the deviation of the
expectation ˆR(θ) from one can serve as a diagnostic tool to check for mismodelling of the
estimator. We can take log ˆR(θ) as a measure of the uncertainty of log ˆr(x|θ, θ1). As in the
case of the reference hypothesis variation, there is no consistent statistical interpretation of
this uncertainty, but this does not mean that it is useless as a closure test.

30

(49)

Reweighting distributions: A good estimator ˆr(x|θ0, θ1) should satisfy

p(x|θ0) ≈ ˆr(x|θ0, θ1) p(x|θ1) .

We cannot evaluate the p(x|θ0) to check this relation explicitly. However, we can sample
events {xe} from them. This provides another diagnostic tool [34]: we can draw a ﬁrst sample
as xe ∼ p(xe|θ0), and draw a second sample as xe ∼ p(xe|θ1) and reweight it with ˆr(xe|θ0, θ1).
For a good likelihood ratio estimator, the two samples should have similar distributions. This
can easily be tested by training a discriminative classiﬁer between the samples. If a classiﬁer
can distinguish between the sample from the ﬁrst hypothesis and the sample drawn from
the second hypothesis and reweighted with the estimated likelihood ratio, then ˆr(x|θ0, θ1) is
not a good approximation of the true likelihood ratio r(x|θ0, θ1). Conversely, if the classiﬁer
cannot separate the two classes, the classiﬁer is either not eﬃcient, or the likelihood ratio is
estimated well.

Note that passing these closure tests is not a guarantee for a good estimator of the likelihood ratio.
In Sec. IV B we will discuss how we can nevertheless derive exclusion limits that are guaranteed to
be statistically correct, i. e. that might not be optimal, but are never wrong.

In our example process, we will use a combination of the ﬁrst two ideas of this list: we will create
copies of estimators with independent training samples and random seeds during training, as well
as with diﬀerent choices of the reference hypothesis θ1, and analyse the median and envelope of the
predictions.

IV. LIMIT SETTING

The ﬁnal objective of any EFT analysis are exclusion limits on the parameters of interest at a
given conﬁdence level. These can be derived in one of two ways. The Neyman construction based
on toy experiments provides a generic and fail-safe method, we will discuss it in Sec. IV B. But since
the techniques developed in the previous section directly provide an estimate for the likelihood ratio,
we can alternatively apply existing statistical methods for likelihood ratios as test statistics. This
much more eﬃcient approach will be the topic of the following section.

A. Asymptotics

Consider the test statistics

q(θ) = −2

log r(xe|θ, ˆθ) = −2

log r(xe|θ, θ1) − log r(xe|ˆθ, θ1)

(50)

(cid:17)

(cid:88)

e

(cid:88)

(cid:16)

e

for a ﬁxed number N of observed events {xe} with the maximum-likelihood estimator

ˆθ = arg max

θ

(cid:88)

e

log r(xe|θ, θ1) .

(51)

In the asymptotic limit, the distribution according to the null hypothesis, p(q(θ)|θ), is given by a
chi-squared distribution. The number of degrees of freedom k is equal to the number of parameters
θ. This result by Wilks [87] allows us to translate an observed value qobs(θ) directly to a p-value
that measures the conﬁdence with which θ can be excluded:

pθ ≡

dq p(q|θ) = 1 − Fχ2 (qobs(θ)|k)

(52)

(cid:90) ∞

qobs(θ)

31

(53)

(54)

where Fχ2(x|k) is the cumulative distribution function of the chi-squared distribution with k degrees
of freedom. In our example process k = 2, for which this simpliﬁes to

In particle physics it is common practice to calculate “expected exclusion contours” by calculating
the expected value of qobs(θ) based on a large “Asimov” data set generated according to some θ(cid:48) [88].
With Eq. (52) this value is then translated into an expected p-value.12

In practice we cannot access the true likelihood ratio deﬁned on the full observable space and
thus also not q(θ). But if the error of an estimator ˆr(x|θ0, θ1) compared to the true likelihood ratio
is negligible, we can simply calculate

pθ = exp

−

(cid:18)

(cid:19)

.

qobs(θ)
2

ˆq(θ) = −2

log ˆr(xe|θ, ˆθ)

(cid:88)

e

with maximum likelihood estimator ˆθ also based on the estimated likelihood ratio. The p-value can
then be read oﬀ directly from the estimator output, substituting ˆq for q in Eq. (52).

Under this assumption and in the asymptotic limit, constructing conﬁdence intervals is thus
remarkably simple and computationally cheap: after training an estimator ˆr(x|θ, θ1) as discussed
in the previous section, the observed events {xe} are fed into the estimator for each value of θ on
some parameter grid. From the results we can read oﬀ the maximum likelihood estimator ˆq(θ) and
calculate the observed value of the test statistics ˆq(θ) for each θ. Equation (52) then translates
these values to p-values, which can then be interpolated between the tested θ points to yield the
ﬁnal contours.

To check whether these asymptotic properties apply to a likelihood ratio estimator, we can
use the diagnostic tools discussed in Sec. III F 2. In addition, we can explicitly check whether the
distribution of q(θ) actually follows a chi-squared distribution by generating toy experiments for
a few θ points. If it does, the asymptotic results are likely to apply at other points in parameter
space as well. If the variance of the toy experiments is larger than expected from the chi-squared
distribution, the residual variance may be taken as an error estimate on the estimator prediction.

B. Neyman construction

Rather than relying on the asymptotic properties of the likelihood ratio test, we can construct
the distribution of a test statistic with toy experiments. This is computationally more expensive,
but useful if the number of events is not in the asymptotic regime or if the uncertainty of the
estimators cannot be reliably quantiﬁed. Constraints derived in this way are conservative: even if
the likelihood ratio is estimated poorly, the resulting contours might not be optimal, but they are
never wrong (at a speciﬁed conﬁdence level).

12 For k = 1, this standard procedure reproduces the median expected p-value. Note however that this is not true
anymore for more than one parameter of interest. In this case, the median expected p-value can be calculated
based on a diﬀerent, but not commonly used, procedure. It is based on the fact that the distribution of q according
to an alternate hypothesis, p(q(θ)|θ(cid:48)), is given by a non-central chi-squared distribution [89]. In the asymptotic
limit, the non-centrality parameter is equal to the expectation value E[q(θ)|θ(cid:48)] [88]. This allows us to calculate for
instance the median expected q(θ) assuming some value θ(cid:48) based on the Asimov data. Combining all the pieces,
the median expected p-value pθ with which θ can be excluded under the assumption that θ(cid:48) is true is then given by
pexpected from θ(cid:48)
2 |k, E[q(θ)|θ(cid:48)])|k), where Fχ2 (x|k) is the cumulative distribution function for the
= 1 − Fχ2 (F −1
( 1
χ2
θ
chi-squared distribution with k degrees of freedom and F −1
(p|k, Λ) is the inverse cumulative distribution function
χ2
for the non-central chi-squared distribution with k degrees of freedom and non-centrality parameter Λ.

nc

nc

A good choice for the test statistics is the estimated proﬁle log likelihood ratio ˆq(θ) given
in Eq. (54), which allows us to compare the distribution of the toy experiments directly to the
asymptotic properties discussed in the previous section. However, its construction requires ﬁnding the
maximum likelihood estimator for every toy experiment. This increases the necessary computation
time substantially, especially in high-dimensional parameter spaces. An alternative test statistics is
the estimated log likelihood ratio with respect to some ﬁxed hypothesis, which need not be identical
to the reference denominator θ1 used in the likelihood ratio estimators. In the EFT approach, the
natural choice is the estimated likelihood ratio with respect to the SM,

ˆq(cid:48)(θ) ≡ −2

log ˆr(xe|θ, θSM ) .

(cid:88)

e

Using this test statistic rather than the proﬁle likelihood ratio deﬁned in Eq. (50) is expected to
lead to stronger constraints if the true value of θ is close to the SM point, as expected in the EFT
approach, and less powerful bounds if the true value is substantially diﬀerent from the SM.

In practice we can eﬃciently calculate the distribution of ˆq((cid:48))(θ) after n events by ﬁrst calculating

the distribution of ˆq((cid:48))(θ) for one event and convolving the result with itself (n − 1) times.

C. Nuisance parameters

The tools developed above also support nuisance parameters, for instance to model systematic
uncertainties in the theory calculation or the detector model. One strategy is to train parameterized
estimators on samples generated with diﬀerent values of the nuisance parameters ν and let them
learn the likelihood ratio

with its dependence on the nuisance parameters. As test statistics we can then use the estimator
version of the usual proﬁle log likelihood ratio,

ˆr(x|θ0, θ1; ν0, ν1) ≡

ˆp(x|θ0; ν0)
ˆp(x|θ1; ν1)

ˆq(θ) = −2

log

r

xe



(cid:16)

(cid:12)
(cid:12)θ, ˆθ; ˆˆν, ˆν
(cid:12)

(cid:17) q(ˆˆν)

q(ˆν)





(cid:88)

e

with constraint terms q(ν),

(cid:20)
ˆr(xe|θ, θ1; ν, ν1)

log

and

ˆˆν = arg max

ν

(cid:88)

e
(cid:88)

(cid:20)

(θ,ν)

e

(cid:17)

(cid:16)ˆθ, ˆν

= arg max

log

ˆr(xe|θ, θ1; ν, ν1)

(cid:21)

(cid:21)

q(ν)
q(ν1)

q(ν)
q(ν1)

,

.

The proﬁle log likelihood ratio has two advantages: it is pivotal, i. e. its value and its distribution do
not depend on the value of the nuisance parameter, and it has the asymptotic properties discussed
in Sec. IV A.

Similarly, we can train the score including nuisance parameters,

(cid:12)
(cid:12)
ˆt(x|θ0; ν0) = ∇(θ,ν) log [ˆp(x|θ; ν)q(ν)]
(cid:12)
(cid:12)
(cid:12)θ0,ν0

.

32

(55)

(56)

(57)

(58)

(59)

(60)

33

(61)

(62)

If the constraints q(ν) limit the nuisance parameters to a relatively small region around some ν0,
i. e. a range in which the shape of the likelihood function does not change signiﬁcantly, the Sally
and Sallino methods seem particularly appropriate.

Finally, an adversarial component in the training procedure lets us directly train pivotal estimators
ˆr(x|θ0, θ1), i. e. that do not depend on the value of the nuisance parameters [90]. Compared to
learning the explicit dependence on ν, this can dramatically reduce the dimensionality of the
parameter space as early as possible, and does not require manual proﬁling. However, the estimators
will generally not converge to the proﬁle likelihood ratio, so its asymptotic properties do not apply
and limit setting requires the Neyman construction.

V. RESULTS

We now apply the analysis techniques to our example process of WBF Higgs production in the
4(cid:96) decay mode. We ﬁrst study the idealized setup discussed in Sec. II D 2, in which we can assess
the techniques by comparing their predictions to the true likelihood ratio. In Sec. V B we then
calculate limits in a more realistic setup.

A.

Idealized setup

1. Quality of likelihood ratio estimators

Table IV summarizes the performance of the diﬀerent likelihood ratio estimators in the idealized
setup. For 50 000 events {xe} drawn according to the SM, we evaluate the true likelihood ratio
r(xe|θ0, θ1) as well as the estimated likelihood ratios ˆr(xe|θ0, θ1) for 1000 values of θ0 sampled
randomly in [−1, 1]2. As a metric we use the expected mean squared error on the log likelihood
ratio

ε[ˆr(x)] =

π(θ0)

log ˆr(xe|θ0, θ1) − log r(xe|θ0, θ1)

(cid:17)2(cid:21)

.

(cid:88)

θ0

(cid:20)(cid:16)

(cid:88)

1
N

e

The 1000 tested values of θ0 are weighted with a Gaussian prior

π(θ) =

N(cid:0)||θ||(cid:12)

(cid:12)0, 2 · 0.22(cid:1)

1
Z

with normalization factor Z such that (cid:80)
π(θ) = 1. In addition we show the expected trimmed
mean squared error, which truncates the top 5% and bottom 5% of events for each θ. This allows
us to analyse the quality of the estimators for the bulk of the phase space without being dominated
by a few outliers. In Table IV and in the ﬁgures of this section, we only show results for a default
set of hyperparameters for each likelihood ratio estimators. These default setups are deﬁned in
Appendix A 2. Results for other hyperparameter choices are given in Appendix A 3.

θ0

The best results come from parameterized estimators that combine either a classiﬁer decision
function or ratio regression with regression on the score: the Cascal and Rascal strategies provide
very accurate estimates of the log likelihood ratio. Sally, parameterized Rolr, and parameterized
Carl perform somewhat worse. For Carl and Rolr, parameterized estimators consistently perform
better then the corresponding point-by-point versions. All these ML-based strategies signiﬁcantly
outperform the traditional one- or two-dimensional histograms and the Approximate Frequentist
Computation.

The morphing-aware versions of the parameterized estimators lead to a poor performance,
comparable or worse than the two-dimensional histogram approach. As anticipated in Sec. III F,

34

Expected MSE

All Trimmed

Figures

(cid:88)

Strategy

Setup

pT,j1, ∆φjj
pT,j1
∆φjj
pT,j1, ∆φjj
pT,j1, mZ2, mjj, ∆ηjj, ∆φjj

Histogram

Afc

Carl (PbP)
Carl (parameterized)

Carl (morphing-aware)

Rolr (PbP)
Rolr (parameterized)

Rolr (morphing-aware)

Sally
Sallino

Cascal (parameterized)

Cascal (morphing-aware) Baseline

Rascal (parameterized)

Rascal (morphing-aware) Baseline

PbP
Baseline
Random θ
Baseline
Random θ
Morphing basis

PbP
Baseline
Random θ
Baseline
Random θ
Morphing basis

Baseline
Random θ

Random θ
Morphing basis

Baseline
Random θ

Random θ
Morphing basis

(cid:88)

0.0111 Fig. 12
0.0026
0.0028
0.0200 Fig. 12
0.0226
0.0618

0.056
0.088
0.160
0.059
0.078

0.030
0.012
0.012
0.076
0.086
0.156

0.005
0.003
0.003
0.024
0.022
0.130

0.013
0.021

0.001
0.001
0.136
0.092
0.040

0.001
0.001
0.125
0.132
0.031

0.0106
0.0230
0.0433
0.0091
0.0101

0.0022
0.0017
0.0014
0.0063
0.0052
0.0485

0.0002
0.0006

0.0002
0.0002
0.0427
0.0268
0.0081

0.0004
0.0004
0.0514
0.0539
0.0072

(cid:88)

(cid:88)

(cid:88)

(cid:88)

Table IV: Performance of the diﬀerent likelihood ratio estimation techniques in our example process.
The metrics shown are the expected mean squared error on the log likelihood ratio with and
without trimming, as deﬁned in the text. Checkmarks in the last column denotes estimators shown
in the following ﬁgures. Here we only give results based on default settings, which are deﬁned in
Appendix A 2. An extended list of results that covers more estimators is given in Appendix A 3.

the large weight factors and the sizable cancellations between them blow up small errors on the
estimation of the individual basis estimators ˆri(x) to large errors on the combined estimator.

We ﬁnd that probability calibration as discussed in Sec. III D 1 improves the results in almost
all cases, in particular for the Carl method. An additional step of expectation calibration (see
Sec. III D 2) after the probability calibration does not lead to a further improvement, and in fact often
increases the variance of the estimator predictions. We therefore only use probability calibration for
the results presented here.

The choice of the training sample is less critical, with nearly identical results between the baseline
and random θ samples. For the Carl approach, shallow networks with two hidden layers perform
better, while Rolr works best for three hidden layers and the score-based strategies beneﬁt from a
deeper network with ﬁve hidden layers.

35

Figure 10: True vs. estimated likelihood ratios for a benchmark hypothesis θ0 = (−0.5, −0.5)T .
Each dot corresponds to one event xe. The Cascal (right, red), Rascal (right, orange), and
Sally (middle, blue) techniques can predict the likelihood ratio extremely accurately over the
whole phase space. All new techniques clearly lead to more precise estimates than the traditional
histogram approach (left, orange).

Figure 11: True vs. estimated expected log likelihood ratio. Each dot corresponds to one value of
θ0, where we take the expectation over x ∼ p(x|θSM). The new techniques are less biased than the
histogram approach (left, orange).

In Fig. 10 we show scatter plots between the true and estimated likelihood ratios for a ﬁxed
hypothesis θ0. The likelihood ratio estimate from histograms of observables is widely spread around
the true likelihood ratio, reﬂecting the loss of information from ignoring most directions in the
observable space. Carl performs clearly better. Rolr and Sally oﬀer a further improvement.
Again, the best results come from the Cascal and Rascal strategies, both giving predictions that
are virtually one-to-one with the true likelihood ratio.

We go beyond a single benchmark point θ0 in Fig. 11. This scatter plot compares true and
estimated likelihood ratios for diﬀerent values of θ0, taking the expectation value over x. We
ﬁnd that the Carl, Rolr, Cascal, and Rascal approaches converge to the correct likelihood
ratio in this expectation value. For the Sally and Sallino techniques we ﬁnd larger deviations,
pointing towards the breakdown of the local model approximation. Much more obvious is the loss

36

Figure 12: Comparison of the point-by-point, parameterized, and morphing-aware versions of
Carl. Top left: True vs. estimated likelihood ratios for a benchmark hypothesis
θ0 = (−0.5, −0.5)T , as in Fig. 10. Each dot corresponds to one event xe. Top right: True vs.
estimated expected log likelihood ratio, as in Fig. 11. Each dot corresponds to one value of θ0,
where we take the expectation over x ∼ p(x|θSM). The parameterized estimator outperforms the
point-by-point one and particularly the morphing-aware version.

of information in the traditional histogram approach, which is clearly not asymptotically exact.

The point-by-point, agnostic parameterized, and morphing-aware versions of the Carl strategy
are compared in Fig. 12. As expected from Table IV, the parameterized strategy performs better
than the point-by-point version, and both are clearly superior to the morphing-aware estimator.

2. Eﬃciency and speed

With inﬁnite training data, many of the algorithms should converge to the true likelihood ratio.
But generating training samples can be expensive, especially when a full detector simulation is
used. An important question is therefore how much training data the diﬀerent techniques require
to perform well. In Fig. 13 we show the performance as a function of the training sample size.

The Sally approach performs very well even with very little data. Its precision stops improving
eventually, showing the limitations of the local model approximation. For the other methods we ﬁnd
that the more information a technique uses, the less training data points it requires. The Rascal
technique utilizes the most information from the simulator, leading to an exceptional performance
with training samples of approximately 100 000 events. This is in contrast to the most general Carl
method, which does not use any of the extra information from the simulator and requires a two
orders of magnitude larger training sample for a comparable performance.

In Fig. 14 we show the evolution of the likelihood estimation error and the cross-entropy of the
classiﬁcation problem during the training of the parameterized estimators. For comparison, we also
show the optimal metrics based on the true likelihood ratio, and the results of the two-dimensional
histogram approach. Once again we see that either Cascal or Rascal leads to the best results.
This result also holds true for the cross entropy, hinting that the techniques we use to measure
continuous parameters might also improve the power of estimators in discrete classiﬁcation problems.
Note that the Carl approach is more prone to overﬁtting than the others, visible as a signiﬁcant

37

Figure 13: Performance of the techniques as a function of the training sample size. As a metric, we
show the mean squared error (left) and trimmed mean squared error on log r(r|θ0, θ1) weighted with
a Gaussian prior, as discussed in the text. Note that we do not vary the size of the calibration data
samples. The number of epochs are increased such that the number of epochs times the training
sample size is constant, all other hyperparameters are kept constant. The Sally method works
well even with very little data, but plateaus eventually due to the limitations of the local model
approximation. The other algorithms learn faster the more information from the simulator is used.

Evaluation time [µs]

per xe

per xe and θ0

Algorithm

Histogram
Carl
Sally
Rolr
Cascal
Rascal

25.4

0.2
19.7
0.1
19.7
25.1
21.7

Table V: Computation times of evaluating ˆr(x|θ0, θ1) in the diﬀerent algorithms. We distinguish
between steps that have to be calculated once per x and and those which have to be repeated for
every evaluated value of θ0. These numbers are from one run of our algorithms with default
settings on the NYU HPC cluster on machines equipped with Intel Xeon E5-2690v4 2.6GHz CPUs
and NVIDIA P40 GPUs with 24 GB RAM, using a batch of 50 000 events {xe}, and taking the
mean over 1 017 values of θ0. The local score regression method and the traditional histogram
method are particularly fast. But all techniques are many orders of magnitude faster to evaluate
than the matrix element method or optimal observables.

38

diﬀerence between the metrics evaluated on the training and validation samples.

Equally important to the training eﬃciency is the computation time taken up by evaluating
the likelihood ratio estimators ˆr(xe|θ0, θ1). We compare example evaluation times in Table V. The
traditional histogram approach takes the shortest time. But all tested algorithms are very fast:
the likelihood ratio for ﬁxed hypotheses (θ0, θ1) for 50 000 events {xe} can always be estimated
in around one second or less. The local score regression method is particularly eﬃcient, since the
estimator ˆt(x|θscore, θ1) has to be evaluated only once to estimate the likelihood ratio for any value
of θ0. Only the comparably fast step of density estimation has to be repeated for each tested value
of θ0.

So after investing some training time upfront, all the measurement strategies developed here
can be evaluated on any events with very little computational cost and amortize quickly. While
this is not the focus of our paper, note that this distinguishes our approaches from the Matrix
Element Method and Optimal Observable techniques. These well-established methods require the
computationally expensive evaluation of complicated numerical integrals for every evaluation of the
likelihood ratio estimator.

3. Physics results

The most important result of an EFT measurement are observed and expected exclusion contours,
either based on asymptotics or toy experiments. In the asymptotic approach, the expected contours
are determined just by the likelihood ratio evaluated on a large “Asimov” data set, as described in
Sec. IV A. Figure 15 shows this expected log likelihood ratio in the SM after 36 events over a one-
dimensional slice of the parameter space. In Fig. 16 we show the corresponding expected exclusion
limits on the two Wilson coeﬃcients. To estimate the robustness of the likelihood ratio estimators,
each algorithm is run ﬁve times with diﬀerent choices of the reference hypothesis; independent
training, calibration, and evaluation samples; and independent random seeds during training. The
lines show the median of the ﬁve replicas, while the shaded bands show the envelope. While this
error band does not have a clear statistic interpretation, it does provide a diagnostic tool for the
variance of the estimators.

A traditional histogram-based analysis of jet pT and ∆φjj leads to overly conservative results.
It is interesting to note that this simple analysis works reasonably well in the region of parameter
space with fW > 0 and fW W > 0, which is exactly the part of parameter space where informative
high-energy events interfere mostly constructively with the SM amplitude. In the fW < 0 region of
parameter space, destructive interference dominates in the important regions of phase space with
large momentum transfer. An extreme example is the “amplitude-through-zero” eﬀect shown in the
left panel of Fig. 2. Simple histograms with a rough binning generally lead to a poor estimation of
the likelihood ratio in such complicated kinematic signatures.

We ﬁnd that the new ML-based strategies allow us to place visibly tighter constraints on the
Wilson coeﬃcients than the doubly diﬀerential histogram. In particular the Carl + score and
regression + score estimators lead to exclusion contours that are close to the contours based on
the true likelihood ratio. In this analysis based on asymptotics, however, it is possible for the
estimated contours to be slightly too tight, wrongly marking parameter regions as excluded at a
given conﬁdence level. This problem can be mitigated by proﬁling over systematic uncertainties
assigned to the likelihood ratio estimates.

Exclusion limits based on the Neyman construction do not suﬀer from this issue: contours
derived in this way might be not optimal, but they are never wrong. We generate toy experiments
to estimate the distribution of the likelihood ratio with respect to the SM for individual events.
Repeatedly convolving this single-event distribution with itself, we ﬁnd the distribution of the

39

Figure 14: Learning curve of the parameterized models. The solid lines show the metrics evaluated
on the training sample, the dots indicate the performance on the validation sample. Note that
these numbers are not comparable to the metrics in Table IV and Fig. 13, which are weighted with
the prior in Eq. (62). These results also do not include calibration. Left: Mean squared error of
log ˆr(x|θ0, θ1). Right: binary cross-entropy of the classiﬁcation based on ˆs(x|θ0, θ1) between the
numerator and denominator samples. The solid grey line shows the “optimal” performance based
on the true likelihood ratio. The Cascal and Rascal techniques converge to a performance close
to the theoretic optimum. The Carl approach (green), based on minimizing the cross entropy,
shows signs of overﬁtting. All machine-learning-based methods outperform traditional histograms
(dashed orange).

Figure 15: Expected likelihood ratio with respect to the Standard Model along a one-dimensional
slice of the parameter space. We assume 36 observed events and the SM to be true. For each
estimator, we generate ﬁve sets of predictions with diﬀerent reference hypotheses, independent data
samples, and diﬀerent random seeds. The lines show the median of this ensemble, the shaded error
bands the envelope. All machine-learning-based methods reproduce the true likelihood function
well, while the doubly diﬀerential histogram method underestimates the likelihood ratio in the
region of negative Wilson coeﬃcients.

40

Figure 16: Expected exclusion contours based on asymptotics at 68% CL (innermost lines), 95% CL,
and 99.7% CL (outermost lines). We assume 36 observed events and the SM to be true. As test
statistics, we use the proﬁle likelihood ratio with respect to the maximum-likelihood estimator. For
each estimator, we generate ﬁve sets of predictions with diﬀerent reference hypotheses, independent
data samples, and diﬀerent random seeds. The lines show the median of this ensemble, the shaded
error bands the envelope. The new techniques based on machine learning, in particular the Cascal
and Rascal techniques, lead to expected exclusion contours very close to those based on the true
likelihood ratio. An analysis of a doubly diﬀerential histogram leads to much weaker bounds.

Figure 17: Expected exclusion contours based on the Neyman construction with toy experiments at
68% CL (innermost lines), 95% CL, and 99.7% CL (outermost lines). We assume 36 observed
events and the SM to be true. As test statistics, we use the likelihood ratio with respect to the SM.
All machine-learning-based methods let us impose much tighter bounds on the Wilson coeﬃcients
than the traditional histogram approach (left, dotted orange). The Neyman construction
guarantees statistically correct results: no contour based on estimators excludes parameter points
that should not be excluded. The expected limits based on the Cascal (right, lavender) and
Rascal (right, red) techniques are virtually indistinguishable from the true likelihood contours.

41

Figure 18: Ratio of the estimated likelihood ratio ˆr(x|θ0, θ1) to the joint likelihood ratio
r(x, z|θ0, θ1), which is conditional on the parton-level momenta and other latent variables. As a
benchmark hypothesis we use θ0 = (−0.5, −0.5)T , the events are drawn according to the SM. The
spread common to all methods shows the eﬀect of the smearing on the likelihood ratio. The
additional spread in the histogram, Carl, and Rolr methods is due to a poorer performance of
these techniques.

likelihood ratio after 36 observed events.

The expected corresponding expected exclusion limits are shown in Fig. 17. Indeed, errors in
the likelihood ratio estimation never lead to undercoverage, i. e. the exclusion of points that should
not be excluded based on the true likelihood ratio. Again we ﬁnd that histograms of kinematic
observables only allow us to place rather weak bounds on the Wilson coeﬃcients. Sally performs
clearly better, with excellent performance close to the SM. Deviations from the optimal bounds
become visible at the 2σ level, hinting at the breakdown of the local model approximation there.
The best results come once more from the Cascal and Rascal methods. Both of these strategies
yield exclusion bounds that are virtually indistinguishable from those based on the true likelihood
ratio.

As a side note, a comparison of the expected contours based on asymptotics to those based on
the Neyman construction shows the Neyman results to be tighter. This reﬂects the diﬀerent test
statistics used in the two ﬁgures: in the asymptotics case, we use the proﬁle likelihood ratio with
respect to the maximum likelihood estimator, which itself ﬂuctuates around the true value of θ
(which in our case is assumed to be the SM). In the Neyman construction we use the likelihood
ratio with respect to the SM, leading to tighter contours if the true value is in fact close to the SM,
and weaker constraints if it is very diﬀerent.

B. Detector eﬀects

We have now established that the measurement strategy work very well in an idealized setup,
where we can compare them to the true likelihood ratio. In a next step, we turn towards a setup
with a rudimentary smearing function that models the eﬀect of the parton shower and the detector
response on the observables. In this setting, the true likelihood is intractable, so we cannot use it
as a baseline to validate the predictions any more. But we can still discuss the relative ordering of
the exclusion contours predicted by the diﬀerent estimators.

Figure 18 shows the relation between the true joint likelihood ratio r(x, z|θ0, θ1), which is

42

Figure 19: Expected exclusion contours based on the Neyman construction with toy experiments at
68% CL, 95% CL, and 99.7% CL with smearing. We assume 36 observed events and the SM to be
true. As test statistics, we use the likelihood ratio with respect to the SM. In the setup with
smearing we cannot these results to the true likelihood contours. But since the Neyman
construction is guaranteed to cover, these expected limits are correct. The new techniques, in
particular Cascal (right, dashed red) and Rascal (right, dash-dotted orange), allow us to set
much tighter bounds on the Wilson coeﬃcients than a traditional histogram analysis (left, dotted
orange).

conditional on the parton-level momenta z and other latent variables, to the estimated likelihood
ratio ˆr(x|θ0, θ1), which only depends on the observables x. We see that this relation is stochastically
smeared out around 1. Recall that in the idealized scenario the best estimators described the true
likelihood ratio perfectly, as shown in Fig. 10. This strongly suggests that the spread visible here
is not due to errors of the likelihood ratio estimators, but rather shows the diﬀerence between the
joint and true likelihood ratios, as illustrated in Fig. 5.

In Fig. 19 we show the expected exclusion contours based on the Neyman construction, which
guarantees statistically correct results. The conclusions from the idealized setting are conﬁrmed: a
measurement based on the likelihood ratio estimators leads to robust bounds that are clearly more
powerful than those based on a histogram. Once again, the Cascal and Rascal algorithms lead
to the strongest limits.

VI. CONCLUSIONS

We have developed and analysed a suite of new analysis techniques for measurements of continuous
parameters in LHC experiments based on simulations and machine learning. Exploiting the structure
of particle physics processes, they extract additional information from the event generators, and use
this information to train precise estimators for likelihood ratios.

Our approach is designed for problems with large numbers of observables, where the likelihood
function is not tractable and traditional methods based on individual kinematic variables often

43

perform poorly. It scales well to high-dimensional parameter spaces such as that of eﬀective ﬁeld
theories. The new methods do not require any approximations on the hard process, parton shower,
or detector eﬀects, and the likelihood ratio for any event and hypothesis pair can be evaluated
in microseconds. These two properties set it apart from the Matrix Element Method or Optimal
Observables, which rely on crude approximations for the shower and detector and require the
evaluation of typically very expensive integrals.

Using Higgs production in weak boson fusion in the four-lepton mode as a speciﬁc example
process, we have evaluated the performance of the diﬀerent methods and compared them to a
classical analysis of the jet momentum and azimuthal angle between the tagging jets. We ﬁnd that
the new algorithms provide very precise estimates of arbitrary likelihood ratios. Using them as a
test statistics allows us to impose signiﬁcantly tighter constraints on the EFT coeﬃcients than the
traditional kinematic histograms.

Out of the several methods introduced and discussed in this paper, two stand out. The ﬁrst,

which we call Sally, is designed for parameter regions close to the Standard Model:

1. As training data, the algorithm requires a sample of fully simulated events, each accompanied
by the corresponding joint score at the SM: the relative change of the parton-level likelihood
function of the parton-level momenta associated with this event under small changes of the
theory parameters away from the SM. This can be calculated by evaluating the squared matrix
element at the same phase-space points for diﬀerent theory parameters. We can thus extract
this quantity from Monte-Carlo generators such as MadGraph.

2. Regressing on this data, we train an estimator (for instance realized as a neural network) that
takes as input an observation and returns the score at the SM. This function compresses the
high-dimensional observable space into a vector with as many components as parameters of
interest. If the parameter space is high-dimensional, this can be even further compressed into
the scalar product between the score vector and the diﬀerence between two parameter points.

3. The estimated score (or the scalar product between score and parameter diﬀerence) can then
be treated like any set of observables in a traditional analysis. We can ﬁll histograms of this
quantity for diﬀerent hypotheses, and calculate likelihood ratios from them.

There are two key ideas that underlie this strategy. First, note that the training data only
consists of the joint score, which depends on the parton-level four-momenta of an event. But during
the training the estimator converges to the actual score of the distribution of the observables, i. e. the
relative change of the actual likelihood function under inﬁnitesimal changes of the parameters. We
have proven this powerful, yet surprisingly simple relation in this paper.

Second, close to the Standard Model (or any other reference parameter point), the score provides
it encapsulates all information on the local approximation of the stat-
the suﬃcient statistics:
istical model. In other words, if the score is estimated well, the dimensionality reduction from
high-dimensional observables into a low-dimensional vector does not lose any information on the
parameters. The estimated score is a machine-learning version of the Optimal Observable idea,
but requires neither approximations of the parton shower or detector treatment nor numerically
expensive integrals.

As a matter of fact, the dimensionality reduction can be taken one step further. We have
introduced the Sallino technique that compresses the estimated score vector to a single scalar
function, again without loss of power in the local approximation, and independent of the number of
theory parameters.

In our example process, these simple and robust analysis strategies work remarkably well,
especially close to the Standard Model. Deviations appear at the 2σ level, but even there it

44

allows for much stronger constraints than a traditional analysis of kinematic variables. It requires
signiﬁcantly less data to train than the other discussed methods. Since the Sallino method can
compress any observation into a single number without losing much sensitivity, even for hundreds
of theory parameters, this approach scales exceptionally well to high-dimensional parameter spaces,
as in the case of the SMEFT.

The second algorithm we want to highlight here is the Rascal technique. Using even more
information available from the simulator, it learns a parameterized likelihood ratio estimator: one
function that takes both the observation and a theory parameter point as input and returns an
estimate for the likelihood ratio between this point and a reference hypothesis given the observation.
This estimator is constructed as follows:

1. Training this parameterized estimator requires data for many diﬀerent values of the tested
parameter point (the numerator in the likelihood ratio). For simplicity, the reference hypo-
thesis (the denominator in the likelihood ratio) can be kept ﬁxed. For each of these hypothesis
pairs, event samples are generated according to the numerator and denominator hypothesis.
In addition, we extract the joint likelihood ratio from the simulator: essentially the squared
matrix element according to the numerator theory parameters divided by the squared matrix
element according to the denominator hypothesis, evaluated at the generated parton-level
momenta. Again, we also need the joint score, i. e. the relative change of the parton-level
likelihood function under inﬁnitesimal changes of the theory parameters. Both quantities can
be extracted from matrix element codes.

2. A neural network models the estimated likelihood ratio as a function of both the observables
and the value of the theory parameters (of the numerator in the likelihood ratio). We can
calculate the gradient of the network output with respect to the theory parameter and thus
also the estimated score. The network is trained by minimizing the squared error of the
likelihood ratio plus the squared error of the score, in both cases with respect to the joint
quantities extracted from the simulator.

3. After the training phase, the likelihood ratio can optionally be calibrated, for instance through

isotonic regression.

This technique relies on a similar trick as the local score regression method: the likelihood
ratio learned during the training converges to the true likelihood ratio, even though the joint ratio
information in the training data is conditional on the parton-level momenta. The Rascal method
is among the best-performing methods of all analysed techniques. It requires signiﬁcantly smaller
training samples than all other approaches, with the exception of Sally and Sallino. Expected
exclusion limits derived in this way are virtually indistinguishable from those based on the true
likelihood ratio.

On top of these two approaches, we have developed, analysed, and compared several other
methods. We refer the reader to the main part and the appendices of this document, where all these
algorithms are discussed in depth.

All tools developed here are suitable for large-scale LHC analyses. On the software side, only few
modiﬁcations of existing tools are necessary. Most importantly, matrix-element generators should
provide a user-friendly interface to calculate the squared matrix element for a given conﬁguration
of four-momenta and a given set of physics parameters. With such an interface, one could easily
calculate the joint score and joint likelihood ratio data that is needed for the new algorithms. The
training of the estimators is then straightforward, in particular for the Sally and Sallino methods.
The limit setting follows established procedures, either based on the Neyman construction with toy
experiments, or (since the tools provide direct estimates for the likelihood ratio) using asymptotic
formulae.

45

While we have focussed on the example of eﬀective ﬁeld theory measurements, these techniques
equally apply to other measurements of continuous parameter in collider experiments as well as
to a large class of problems outside of particle physics [53]. Some of the techniques can also be
applied to improve the training of machine-learning-based classiﬁers. Finally, while we restricted
our analysis to frequentist conﬁdence intervals, as is common in particle physics, the same ideas can
be used in a Bayesian setting.

All in all, we have presented a range of new inference techniques based on machine learning,
which exploit the structure of particle physics processes to augment training data. They scale well
to large-scale LHC analyses with many observables and high-dimensional parameter spaces. They
do not require any approximations of the hard process, parton shower, or detector eﬀects, and the
likelihood ratio can be evaluated in microseconds. In an example analysis, these new techniques
have demonstrated the potential to substantially improve the precision and new physics reach of
the LHC legacy results.

Acknowledgments

We would like to thank Cyril Becot and Lukas Heinrich, who contributed to this project at an
early stage. We are grateful to Felix Kling, Tilman Plehn, and Peter Schichtel for providing the
MadMax code and helping us use it. KC wants to thank CP3 at UC Louvain for their hospitality.
Finally, we would like to thank Atılım Güneş Baydin, Lydia Brenner, Joan Bruna, Kyunghyun
Cho, Michael Gill, Ian Goodfellow, Daniela Huppenkothen, Hugo Larochelle, Yann LeCun, Fabio
Maltoni, Jean-Michel Marin, Iain Murray, George Papamakarios, Duccio Pappadopulo, Dennis
Prangle, Rajesh Ranganath, Dustin Tran, Rost Verkerke, Wouter Verkerke, Max Welling, and
Richard Wilkinson for interesting discussions.

JB, KC, and GL are grateful for the support of the Moore-Sloan data science environment at
NYU. KC and GL were supported through the NSF grants ACI-1450310 and PHY-1505463. JP
was partially supported by the Scientiﬁc and Technological Center of Valparaíso (CCTVal) under
Fondecyt grant BASAL FB0821. This work was supported in part through the NYU IT High
Performance Computing resources, services, and staﬀ expertise.

Appendix A: Appendix

1. Simpliﬁed detector description

While most of our results are based on an idealized perfect measurement of parton-level momenta,
we also consider a toy smearing representing the eﬀect of parton shower and the detector. The total
smearing function is given by

p(x|z) =

p(cid:96)(x(cid:96)|z(cid:96))

pj(xj|zj) .

(cid:89)

(cid:89)

(cid:96)∈leptons

j∈jets

(A1)

Lepton momenta x(cid:96) = ( ˆE, ˆpT , ˆη, ˆφ) are smeared by

p(cid:96)( ˆE, ˆpT , ˆη, ˆφ|E, pT , η, φ) = N(cid:0)ˆpT

(cid:12)
(cid:12)pT , (3 · 10−4GeV−1p2

T )2(cid:1)

· δ( ˆE − E0(ˆpT , ˆη; m(cid:96))) δ(ˆη − η) δ( ˆφ − φ) ,

(A2)

46

Strategy

NN layers

α Calibration / density estimation

Histogram
AFC
Carl (PbP)
Carl (parameterized)
Carl (morphing-aware)
Sally
Sallino

Rolr (PbP)
Rolr (parameterized)
Rolr (morphing-aware)
Cascal (parameterized)
Cascal (morphing-aware)
Rascal (parameterized)
Rascal (morphing-aware)

Histogram
Gaussian KDE

Isotonic probability calibration
Isotonic probability calibration
Isotonic probability calibration

Histogram
Histogram

Isotonic probability calibration
Isotonic probability calibration
Isotonic probability calibration

5
5

Isotonic probability calibration
Isotonic probability calibration

100
100

Isotonic probability calibration
Isotonic probability calibration

3
2
2

5
5

3
3
2

5
2

5
2

Table VI: Default settings for the analysis techniques. The neural network (NN) layers each have
100 units with tanh activation functions. The hyperparameter α multiplies the score squared error
in the combined loss functions of Eq. (35) and (37).

while the distribution of the jet properties depending on the quark momenta is given by

pj( ˆE, ˆpT , ˆη, ˆφ|E, pT , η, φ) =

(cid:32)

(cid:16) ˆE
N

(cid:12)
(cid:12)
(cid:12)a0 + a1

√

E + a2E, (b0 + b1

√

E + b2E)2(cid:17)

+ cN

(cid:16) ˆE

(cid:12)
(cid:12)
(cid:12)d0 + d1

√

E + d2E, (e0 + e1

√

E + e2E)2(cid:17)

(cid:33)

· δ(ˆpT − pT 0( ˆE, ˆη; m(cid:96))) N(cid:0)ˆη(cid:12)

(cid:12)η, 0.12(cid:1) N

(cid:12)
(cid:16) ˆφ
(cid:12)

(cid:12)φ, 0.12(cid:17)

.

(A3)

Here N(cid:0)x(cid:12)
(cid:12)µ, σ2(cid:1) is the Gaussian distribution with mean µ and variance σ2. The jet energy resolution
parameters ai, bi, c, di, and ei are based on the default settings of the jet transfer function in
MadWeight [91]. The functions E0(pT , η, m) and pT 0(E, η, m) refer to the energy and transverse
momentum corresponding to an on-shell particle with mass m.

2. Model almanac

In Sec. III A we developed diﬀerent estimators for the likelihood ratio, focussing on the key ideas
over technical details. Here we ﬁll in the gaps, explain all strategies in a self-contained way, and
document the settings we use for our example process. To facilitate their comparison, we describe
all models in terms of a “training” and an “evaluation” part, even if this language is not typically
used e. g. for histograms.

a. Histograms of observables

Idea: Most collider measurements are based on the number of events or the cross section of a
process in a given phase-space region or on the diﬀerential cross section or distribution of

47

Figure 20: Example distributions to illustrate the doubly diﬀerential histogram analysis (top), the
Sally technique (middle), and the Cascal method (bottom). The left panels show the diﬀerent
spaces in which the densities and their ratios are estimated. On the right we show the
corresponding distributions of the estimated ratio ˆr (solid) and compare them to the true likelihood
ratio distributions (dotted). We use the benchmark θ0 = (−0.5, −0.5)T (blue) and θ1 as in Eq. (47)
(orange).

48

one or at most a few kinematic observables v. Typical choices are the reconstructed energies,
momenta, angles, or invariant masses of particles. Choosing the right set of observables
for a given measurement problem is all but trivial, but many processes have been studied
extensively in the literature. Once this choice is made, this strategy is simple, fast, and
intuitive. We illustrate the information used by this approach in the top panels of Fig. 20.

Requirements: The histogram approach can be used in the general likelihood-free setting: it only

requires a simulator that can generate samples {x} ∼ p(x|θ).

Structure: Histograms are most commonly used point by point in θ.

If the problem has the
morphing structure discussed in Sec. II C 2, they can also be applied in a morphing-aware
parameterized way (we have not implemented this for our example process).

Training: After generating samples for both the numerator and denominator hypotheses, the values
of the chosen kinematic variables v(x) are extracted, and binned into separate histograms for
the two hypotheses.

Calibration: With suﬃcient training data, histograms should be well calibrated, so we do not

experiment with an additional calibration stage.

Evaluation: To estimate the likelihood ratio between two hypotheses θ0 and θ1 for a given set of
observables x, one has to extract the kinematic variables v(x) and look up the corresponding
bin contents in the histograms for θ0 and θ1. Assuming equal binning for both histograms,
the likelihood ratio is simply estimated as the ratio of bin contents.

Parameters: The only parameters of this approach are the choices of kinematic observables and

the histogram binning.

In our example process, we consider six diﬀerent variants:

• A one-dimensional histogram of the transverse momentum pT,j1 of the leading (higher-pT )

• A one-dimensional histogram of the absolute value of the azimuthal angle ∆φjj between

jet with 80 bins.

the two jets with 20 bins.

direction and 5 bins along ∆φjj.

direction and 10 bins along ∆φjj.

direction and 15 bins along ∆φjj.

direction and 20 bins along ∆φjj.

• A “coarse” two-dimensional histogram of these two variables with 10 bins in the pT,j1

• A “medium” two-dimensional histogram of these two variables with 20 bins in the pT,j1

• A “ﬁne” two-dimensional histogram of these two variables with 30 bins in the pT,j1

• A “very ﬁne” two-dimensional histogram of these two variables with 50 bins in the pT,j1

• An “asymmetric” two-dimensional histogram of these two variables with 50 bins in the

pT,j1 direction and 5 bins along ∆φjj.

For each pair (θ0, θ1) and each observable, the bin edges are chosen such that the same
expected number of events according to θ0 plus the expected number of events according to
θ1 is the same in each bin.

49

b. Approximate Frequentist Computation ( Afc)

Idea: Approximate Bayesian Computation is a very common technique for likelihood-free inference
in a Bayesian setup. In its simplest form it keeps samples according to the rejection probability
of Eq. (29). This amounts to an approximation of the likelihood function through kernel
density estimation, which we can isolate from the Abc sampling mechanism and use in
a frequentist setting. We call it Approximate Frequentist Computation (AFC) to stress
the relation to Abc. Just as Abc or the histogram approach, it requires the choice of a
summary statistics v(x), which in our example process we take to be a two-dimensional or
ﬁve-dimensional subset of the kinematic variables.

Requirements: AFC can be used in the general likelihood-free setting: it only requires a simulator

that can generate samples {x} ∼ p(x|θ).

Structure: We use AFC point by point in θ. If the problem has the morphing structure discussed

in Sec. II C 2, it can also be applied in a morphing-aware parameterized way.

Training: For each event in the numerator and denominator training samples, the summary

statistics v(x) are calculated and saved.

Calibration: AFC can be calibrated as any other technique on this list, but we left this for future

work.

Evaluation: The summary statistics v(x) is extracted from the observation. For numerator and
denominator hypothesis separately, the likelihood at this point is estimated with Eq. (30). The
likelihood ratio estimate is then simply given by the ratio between the estimated numerator
and denominator densities.

Parameters: Just as for histograms, the choice of the summary statistics is the most important
parameter. The performance of AFC also crucially depends on the kernel and bandwidth ε.
Too small values for the bandwidth make large training samples necessary, too large values
lead to an oversmoothening and loss of information.

In our example process, we consider two diﬀerent variants:

• A two-dimensional summary statistics space of the leading jet pT and ∆φjj (see above).

Both variables are rescaled to zero mean and unit variance.

• A ﬁve-dimensional summary statistics space of the leading jet pT , ∆φjj, the dijet
invariant mass mjj, the separation in pseudorapidity between the jets ∆ηjj, and the
invariant mass of the lighter (oﬀ-shell) reconstructed Z boson mZ2. All variables are
rescaled to zero mean and unit variance.

We use Gaussian kernels with bandwidths between 0.01 and 0.5.

c. Calibrated classiﬁers ( Carl)

Idea: Carl was developed in Ref. [34]. The authors showed that the likelihood ratio is invariant
under any transformation that is monotonic with the likelihood ratio.
In practice, this
means that we can train a classiﬁer between two samples generated from the numerator and
denominator hypotheses and turn the classiﬁer decision function ˆs(x) into an estimator for
the likelihood ratio ˆr(x). This relation between ˆs(x) and ˆr(x) can follow the ideal relation in

50

Eqs. (18) and (46). But even if this relation does not hold, we can still extract a likelihood
ratio estimator from the classiﬁer output through probability calibration.

Requirements: Carl can be used in the general likelihood-free setting: it only requires a simulator

that can generate samples {x} ∼ p(x|θ).

Structure: Carl can be used either point by point, in an agnostic parameterized version, or (if
the morphing condition in Eq. (6) holds) in a morphing-aware version. Figure 8 illustrates
the structure of the estimator in these three cases.

Training: A classiﬁer with decision function ˆs(x|θ0, θ1) is trained to discriminate between numerator
(label y = 0) and denominator (label y = 1) samples by minimizing the binary cross-entropy
given in Eq. (16) (other loss functions are possible, but we have not experimented with them).

In the point-by-point version, the inputs to the classiﬁers are just the observables x, and
the events in the numerator sample are generated according to one speciﬁc value θ0. In the
parameterized versions of the estimator, the numerator training samples do not come from a
single parameter θ0, but rather a combination of many diﬀerent subsamples. In the agnostic
parameterized setup, the value of θ0 used in each event is then one of the inputs to the neural
network. In the morphing-aware versions, it is used to calculate the weights wc(θ0) that
multiply the diﬀerent component networks ˆrc(x), as visualized in the bottom panel of Fig. 8.

Calibration: In a next step, the classiﬁer output is optionally calibrated as discussed in Sec. III D 1
using isotonic regression. The calibration curve is shown in the left panel of Fig. 21. We have
also experimented with an additional step of expectation calibration, see Sec. III D 2.

Evaluation: For a given x (and in the parameterized versions θ0), the classiﬁer decision function
ˆs(x|θ0, θ1) is evaluated. This is turned into a likelihood ratio estimator with the relation given
in Eq. (18), and optionally calibrated.

Parameters: The parameters of this approach are the architecture of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms.

For our example process we consider fully connected neural networks with two (“shallow”),
three, or ﬁve (“deep”) layers of 100 neurons each and tanh activation functions. They are
trained with the Adam optimizer [83] over 50 epochs with early stopping and learning rate
decay. Our default settings are given in Table VI. Experiments with diﬀerent architectures,
other activation functions, additional dropout layers, other optimizers, and diﬀerent learning
rate schedules yielded a worse performance.

d. Ratio regression ( Rolr)

Idea: Particle physics generators do not only provide sets of observables {xe}, but also the cor-
responding parton-level momenta {ze}. From matrix element codes such as MadGraph we
can extract the squared matrix element |M|2(z|θ) given parton-level momenta z and theory
parameter points θ. This allows us to calculate the joint likelihood ratio

r(xe, ze|θ0, θ1) =

p(ze|θ0)
p(ze|θ1)

=

|M|2(ze|θ0)
|M|2(ze|θ1)

σ(θ1)
σ(θ0)

(A4)

51

Figure 21: Calibration curves for diﬀerent estimators, comparing the uncalibrated (“raw”) estimator
to the estimator after probability calibration. The calibration curve for the truth prediction is a
cross-check for consistency, we do not actually use calibration for the truth predictions. For the
local score regression technique, we show the value of ˆh(x|θ0, θ1) (essentially the log likelihood ratio
in the local model) versus the estimated likelihood ratio after density estimation.

for any of the generated events.

In Sec. III B we have shown that regressing a function ˆr(x) on the generated events {xe} and
the corresponding joint likelihood ratios r(xe, ze|θ0, θ1) will converge to

ˆr(x) → r(x) =

p(x|θ0)
p(x|θ1)

,

(A5)

provided that the events are sampled according to xe ∼ p(x|θ1).

Requirements: The Rolr technique requires a generator with access to the joint likelihood ratios
r(xe, ze|θ0, θ1). In the particle physics case, this means we have to be able to evaluate the
squared matrix elements for given phase-space points and theory parameters.

Structure: Rolr can be used either point by point, in an agnostic parameterized version, or (if
the morphing condition in Eq. (6) holds) in a morphing-aware version. Figure 8 illustrates
the structure of the estimator in these three cases.

Training: The training phase is straightforward regression. It consists of minimizing the squared
error loss between a ﬂexible function ˆr(x|θ0, θ1) (for instance a neural network) and the
training data {xe, r(xe, ze|θ0, θ1)}, which was generated according to θ1.

In the point-by-point version, the input to the regressor are just the observables x, and
the ratio is between two ﬁxed hypotheses θ0 and θ1. In the parameterized versions of the
estimator, the ratios are based on various values θ0, while θ1 is still kept ﬁxed. In the agnostic
parameterized setup, the value of θ0 used in each event is then one of the inputs to the neural
network. In the morphing-aware versions, it us used to calculate the weights wc(θ0) that
multiply the diﬀerent component networks ˆrc(x), as visualized in the bottom panel of Fig. 8.

In all cases we can slightly improve the structure by adding samples generated according
to θ0 to the training samples, regressing on 1/r instead of r on these events. The full loss
functional is given in Eq. (33).

52

Calibration: In a next step, the classiﬁer output is optionally calibrated as discussed in Sec. III D 1
using isotonic regression. The calibration curve is shown in the middle panel of Fig. 21. We
have also experimented with an additional step of expectation calibration, see Sec. III D 2.

Evaluation: For a given x (and in the parameterized versions θ0), the regressor ˆr(x|θ0, θ1) is

evaluated. The result is optionally calibrated.

Parameters: The parameters of this approach are the architecture of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms.

For our example process we consider fully connected neural networks with two (“shallow”),
three, or ﬁve (“deep”) layers of 100 neurons each and tanh activation functions. They are
trained with the Adam optimizer [83] over 50 epochs with early stopping and learning rate
decay. Our default settings are given in Table VI. Experiments with diﬀerent architectures,
other activation functions, additional dropout layers, other optimizers, and diﬀerent learning
rate schedules yielded a worse performance.

e. Carl + score regression ( Cascal)

Idea: The parameterized Carl technique learns the full statistical model ˆr(x|θ0, θ1), including the
dependency on θ0. If it is realized as a diﬀerentiable classiﬁer (such as a neural network), we
can calculate the gradient of ˆr(x|θ0, θ1) with respect to θ0, and thus the estimated score of
this model. If the estimator is perfect, we expect this estimated score to minimize the squared
error with respect to the joint score data available for the training data. This is based on the
same argument as the local score regression technique, see Sec. III B for the proof.

We can turn this argument around and use the available score information during the training.
To this end, we combine two terms in a combined loss function: the Carl-style cross-entropy
and the squared error between the estimated score and the joint score of the training data.
These two pieces contain complementary information: the Carl part contains the information
of the likelihood ratio for a ﬁxed hypothesis comparison (θ0, θ1), while the score part describes
the relative change of the likelihood ratio under changes in θ0.

Requirements: The Cascal technique requires a generator with access to the joint score
t(xe, ze|θ0). In the particle physics case, this means we have to be able to evaluate the squared
matrix elements for given phase-space points and theory parameters.

Structure: Since the Cascal method relies on the extraction of the estimated score from the
estimator, it can only be used for parameterized estimators, either in an agnostic or morphing-
aware version. The middle and bottom panels of Fig. 8 illustrate the structure of the estimator
in these two cases.

Training: A diﬀerentiable classiﬁer with decision function ˆs(x|θ0, θ1) is trained to discriminate
between numerator (label y = 0) and denominator (label y = 1) samples, while the derived
estimated score ˆt(x|θ0) is compared to the joint score on the training samples generated
from y = 0. The loss function that is minimized is thus a combination of the Carl-style
cross-entropy and the squared error on the score, weighted by a hyperparameter α. It is given
in Eq. (35).

The numerator (y = 0) training samples do not come from a single parameter θ0, but rather a
combination of many diﬀerent subsamples. In the agnostic parameterized setup, the value of

53

θ0 used in each event is then one of the inputs to the neural network. In the morphing-aware
versions, it is used to calculate the weights wc(θ0) that multiply the diﬀerent component
networks ˆrc(x), as visualized in the bottom panel of Fig. 8.

Calibration: In a next step, the classiﬁer output is optionally calibrated as discussed in Sec. III D 1
using isotonic regression. The calibration curve is shown in the right panel of Fig. 21. We
have also experimented with an additional step of expectation calibration, see Sec. III D 2.

Evaluation: For a given x and θ0, the classiﬁer decision function ˆs(x|θ0, θ1) is evaluated. This is
turned into a likelihood ratio estimator with the relation given in Eq. (18), and optionally
calibrated.

Parameters: The key hyperparameter of this technique is the factor α that weights the two terms
in the loss function. Additional parameters set the architecture of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms.

For our example process we consider fully connected neural networks with two (“shallow”),
three, or ﬁve (“deep”) layers of 100 neurons each and tanh activation functions. They are
trained with the Adam optimizer [83] over 50 epochs with early stopping and learning rate
decay. Our default settings are given in Table VI. Experiments with diﬀerent architectures,
other activation functions, additional dropout layers, other optimizers, and diﬀerent learning
rate schedules yielded a worse performance.

f. Ratio + score regression ( Rascal)

Idea: The parameterized Rolr technique learns the full statistical model ˆr(x|θ0, θ1), including the
dependency on θ0. If it is realized as a diﬀerentiable regressor, we can calculate the gradient
of ˆr(x|θ0, θ1) with respect to θ0, and thus the score of this model. If the estimator is perfect,
we expect this estimated score to minimize the squared error with respect to the joint score
data available for the training data.

We can turn this argument around and use the available likelihood ratio and score information
during the training. To this end, we combine two terms in a combined loss function: the
squared errors on the ratio and the score. These two pieces contain complementary information:
the ratio regression part contains the information of the likelihood ratio for a ﬁxed hypothesis
comparison (θ0, θ1), while the score part describes the relative change of the likelihood ratio
under changes in θ0.

Requirements: The Rascal technique requires a generator with access to the joint likelihood
ratio r(xe, ze|θ0, θ1) and score t(xe, ze|θ0). In the particle physics case, this means we have
to be able to evaluate the squared matrix elements for given phase-space points and theory
parameters.

Structure: Since the Rascal method relies on the extraction of the estimated score from the
estimator, it can only be used for parameterized estimators, either in an agnostic or morphing-
aware version. The middle and bottom panels of Fig. 8 illustrate the structure of the estimator
in these two cases.

Training: An estimator ˆr(x|θ0, θ1) is trained through regression on the joint likelihood ratio, while
the derived estimated score ˆt(x|θ0) is compared to the joint score on the training samples

54

generated from y = 0. The loss function that is minimized is thus a combination of the
squared error on the ratio and the squared error on the score, weighted by a hyperparameter
α. It is given in Eq. (35).

The numerator (y = 0) training samples do not come from a single parameter θ0, but rather a
combination of many diﬀerent subsamples. In the agnostic parameterized setup, the value of
θ0 used in each event is then one of the inputs to the neural network. In the morphing-aware
versions, it is used to calculate the weights wc(θ0) that multiply the diﬀerent component
networks ˆrc(x), as visualized in the bottom panel of Fig. 8.

Calibration: In a next step, the classiﬁer output is optionally calibrated as discussed in Sec. III D 1
using isotonic regression. The calibration curve is shown in the right panel of Fig. 21. We
have also experimented with an additional step of expectation calibration, see Sec. III D 2.

Evaluation: For a given x and θ0, the estimator ˆr(x|θ0, θ1) is evaluated and optionally calibrated.

Parameters: The key hyperparameter of this technique is the factor α that weighs the two terms
in the loss function. Additional parameters set the architecture of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms.

For our example process we consider fully connected neural networks with two (“shallow”),
three, or ﬁve (“deep”) layers of 100 neurons each and tanh activation functions. They are
trained with the Adam optimizer [83] over 50 epochs with early stopping and learning rate
decay. Our default settings are given in Table VI. Experiments with diﬀerent architectures,
other activation functions, additional dropout layers, other optimizers, and diﬀerent learning
rate schedules yielded a worse performance.

g. Local score regression and density estimation ( Sally)

Idea: In Sec. III A 2 we introduced the score, the relative gradient of the likelihood with respect to
the theory parameters. The score evaluated at some reference parameter point is the suﬃcient
statistics of the local approximation of the likelihood given in Eq. (15). In other words, we
expect the score vector to be a set of “optimal observables” that includes all the information
on the theory parameters, at least in the vicinity of the reference parameter point. If we can
estimate the score from an observation, we can use it like any other set of observables. In
particular, we can ﬁll histograms of the score for any parameter point and thus estimate the
likelihood ratio in score space.

To estimate the score, we again make use of the particle physics structure. Particle physics
generators do not only provide sets of observables {xe}, but also the corresponding parton-
level momenta {ze}. From matrix element codes such as MadGraph we can extract the
squared matrix element |M|2(z|θ) given parton-level momenta z and theory parameter points
θ. This allows us to calculate the joint score

(cid:12)
(cid:12)
(cid:12)
t(xe, ze|θ0) = ∇θ log p(ze|θ)
(cid:12)
(cid:12)θ0

=

∇θ|M|2(ze|θ0)
|M|2(ze|θ0)

−

∇θσ(θ0)
σ(θ0)

(A6)

for any of the generated events. The derivatives in Eq. (A6) can always be evaluated
numerically.
If the process has the morphing structure of Eq. (6), one can alternatively
calculate it from the morphing weights.

55

(A7)

In Sec. III B we have shown that regressing a function ˆt(x) on the generated events {xe} and
the corresponding joint scores t(xe, ze|θ) will converge to

ˆt(x) → t(x) = ∇θ log p(x|θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)θ0

,

provided that the events are sampled according to xe ∼ p(x|θ0).

This technique is illustrated in the middle panels of Fig. 20. The middle panel of Fig. 21 shows
the relation between the scalar product of estimated score and θ0 − θ1 and the estimated
likelihood ratio.

Requirements: The Sally technique requires a generator with access to the joint score t(xe, ze|θ0).
In the particle physics case, this means we have to be able to evaluate the squared matrix
elements for given phase-space points and theory parameters.

Structure: The technique consists of two separate steps: the score regression and the density
estimation in the estimated score space. The score regression step is independent of the tested
hypothesis and realized as a simple fully connected neural network. The subsequent density
estimation is realized through multi-dimensional histograms, point by point in parameter
space (if the morphing condition in Eq. (6) holds, a morphing-aware version is also possible).

Training: The ﬁrst part of the training is regression on the score (evaluated at some reference
hypothesis). It consists of minimizing the squared error loss between a ﬂexible vector-valued
function ˆt(x|θscore) (implemented for instance as a neural network) and the training data
{xe, t(xe, ze|θscore)}, which was sampled according to θscore.

The second step is density estimation in the estimated score space. We only consider histo-
grams, but other density estimation techniques are also possible. For each value of θ0 or θ1
that is tested, we generate samples of events, estimate the corresponding score vectors, and
ﬁll a multidimensional histogram of the estimated score.

Calibration: The density estimation step already calibrates the results, so we do not experiment

with an additional calibration step.

Evaluation: For a given observation x, the score regressor ˆt(x) is evaluated. For each tested (θ0, θ1)
pair, we then extract the corresponding bin contents from the numerator and denominator
histograms, and calculate the estimated likelihood ratio with Eq. (38).

Parameters: Both the score regression part and the subsequent density estimation have parameters.
The ﬁrst and most important choice is the reference hypothesis θscore, at which the score is
evaluated. For eﬀective ﬁeld theories the Standard Model is the natural choice, and we use it
in our example process.

The score regression also depends on the hyperparameters of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms. For our example process we consider fully
connected neural networks with two (“shallow”), three, or ﬁve (“deep”) layers of 100 neurons
each and tanh activation functions. They are trained with the Adam optimizer [83] over 50
epochs with early stopping and learning rate decay. Our default settings are given in Table VI.
Experiments with diﬀerent architectures, other activation functions, additional dropout layers,
other optimizers, and diﬀerent learning rate schedules yielded a worse performance.

56

The only parameter of the density estimation stage is the histogram binning. For our example
process we consider two diﬀerent variations:

• Density estimation with a “ﬁxed” binning, where the bin axes are aligned with the score

components. We use 40 bins for each of the two score components.

• Density estimation with a “dynamic” binning, in which the bin axes are aligned with
the θ0 − θ1 direction and the orthogonal one. We use 80 bins along the ∆θ direction,
which carries the relevant information in the local model approximation, and 10 along
the orthogonal vector.

For each pair (θ0, θ1) and each dimension, the bin edges are chosen such that the expected
number of events according to θ0 plus the expected number of events according to θ1 is the
same in each bin.

h. Local score regression, compression to scalar, and density estimation ( Sallino)

Idea: In the proximity of the Standard Model (or any other reference parameter point), likelihood
ratios only depend on the scalar product between the score and the diﬀerence between
the numerator and denominator parameter points. If we can estimate the score from an
observation, we can calculate this scalar product ˆh(x|θ0, θ1), deﬁned in Eq. (39), and use it
like any other observable. In particular, we can ﬁll histograms of ˆh for any parameter point
and thus estimate the likelihood ratio in ˆh space.

To estimate the score, we once again exploit particle physics structure. Particle physics
generators do not only provide sets of observables {xe}, but also the corresponding parton-
level momenta {ze}. From matrix element codes such as MadGraph we can extract the
squared matrix element |M|2(z|θ) given parton-level momenta z and theory parameter points
θ. This allows us to calculate the joint score with Eq. (A6) for any of the generated events.
In Sec. III B we have shown that regressing a function ˆt(x) on the generated events {xe} and
the corresponding joint scores t(xe, ze|θ) will converge to t(x), provided that the events are
sampled according to xe ∼ p(x|θ0).

Requirements: The Sallino technique requires a generator with access to the joint score
In the particle physics case, this means we have to be able to evaluate the

t(xe, ze|θ0).
squared matrix elements for given phase-space points and theory parameters.

Structure: The technique consists of two separate steps: the score regression, and the density
estimation in ˆh space. The score regression step is independent of the tested hypothesis and
realized as a simple fully connected neural network. The subsequent density estimation in ˆh
space is realized through one-dimensional histograms, point by point in parameter space (if
the morphing condition in Eq. (6) holds, a morphing-aware version is also possible).

Training: The ﬁrst part of the training is regression on the score (evaluated at some reference
hypothesis). It consists of minimizing the squared error loss between a ﬂexible vector-valued
function ˆt(x|θscore) (implemented for instance as a neural network) and the training data
{xe, t(xe, ze|θscore)}, which was sampled according to θscore.
The second step is density estimation in ˆh space. We only consider histograms, but other
density estimation techniques are also possible. For each value of θ0 or θ1 that is tested, we
generate samples of events, estimate the corresponding score vectors, calculate the scalar
product in Eq. (39) to get ˆh(x|θ0, θ1), and ﬁll a one-dimensional histogram of this quantity.

57

Calibration: The density estimation step already calibrates the results, so we do not experiment

with an additional calibration step.

Evaluation: For a given observation x, the score regressor ˆt(x) is evaluated. For each tested (θ0, θ1)
pair, we multiply it with θ0 − θ1 to get ˆh(x|θ0, θ1), extract the corresponding bin contents
from the numerator and denominator histograms, and calculate the estimated likelihood ratio
with Eq. (40).

Parameters: Both the score regression part and the subsequent density estimation have parameters.
The ﬁrst and most important choice is the reference hypothesis θscore, at which the score is
evaluated. For eﬀective ﬁeld theories the Standard Model is the natural choice, and we use it
in our example process.

The score regression also depends on the hyperparameters of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms. For our example process we consider fully
connected neural networks with two (“shallow”), three, or ﬁve (“deep”) layers of 100 neurons
each and tanh activation functions. They are trained with the Adam optimizer [83] over 50
epochs with early stopping and learning rate decay. Our default settings are given in Table VI.
Experiments with diﬀerent architectures, other activation functions, additional dropout layers,
other optimizers, and diﬀerent learning rate schedules yielded a worse performance.

The only parameter of the density estimation stage is the histogram binning. We use 100
bins. For each pair (θ0, θ1) and each dimension, the bin edges are chosen such that the same
expected number of events according to θ0 plus the expected number of events according to
θ1 is the same in each bin.

3. Additional results

In Tbls. VII to XII we compare the performance of diﬀerent versions of the likelihood ratio
estimators. As metric we use the expected mean squared error on log r(x|θ0, θ1) as well as a trimmed
version, as deﬁned in Sec. V A. The estimators are an extended list of those given in Table IV,
adding variations with diﬀerent hyperparameter choices and the results for uncalibrated (“raw”)
estimators. By default, we use neural networks with 3 hidden layers, the labels “shallow” and “deep”
refer to 2 and 5 hidden layers, respectively. We highlight the versions of the estimators that were
shown in the main part of this paper.

Because of the duality between density estimation and probabilistic classiﬁcation (see Eqs. (18)
and (46)), we can use all techniques to deﬁne classiﬁers. In Fig. 22 we show the ROC curves for
two benchmark parameter points. Note how badly the two scenarios can be separated. This is not
a shortcoming of the discrimination power of the classiﬁers, but due to the genuine overlap of the
probability distributions, as can be seen from the identical ROC curve based on the true likelihood
ratio.

58

Figure 22: Receiver operating characteristic (ROC) curves of true positive rates (TPR) vs. false
positive rates (FPR) for the classiﬁcation between the benchmark scenarios θ0 = (−0.5, −0.5)T and
θ1 as in Eq. (47). The ROC AUC based on the true likelihood is 0.6276. The results show how
much the probability distributions for these two hypotheses overlap.

Strategy

Setup

Histogram pT,j1
∆φjj
2d (coarse binning)
2d (medium binning)
2d (ﬁne binning)
2d (very ﬁne binning)
2d (asymmetric binning)

Afc

2d, (cid:15) = 1
2d, (cid:15) = 0.5
2d, (cid:15) = 0.2
2d, (cid:15) = 0.1
2d, (cid:15) = 0.05
2d, (cid:15) = 0.02
2d, (cid:15) = 0.01
5d, (cid:15) = 1
5d, (cid:15) = 0.5
5d, (cid:15) = 0.2
5d, (cid:15) = 0.1
5d, (cid:15) = 0.05
5d, (cid:15) = 0.02
5d, (cid:15) = 0.01

Expected MSE

All Trimmed

Table IV Figures

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

0.0879
0.1595
0.0764
0.0630
0.0597
0.0603
0.0561

0.1243
0.0797
0.0586
0.0732
0.3961
13.6816
241.3264
0.1252
0.0779
0.0734
0.9560
38.1854
2050.5289
50024.7997

0.0230
0.0433
0.0117
0.0101
0.0115
0.0153
0.0106

0.0257
0.0144
0.0091
0.0103
0.0160
0.0550
0.2143
0.0226
0.0101
0.0128
0.1833
3.6658
57.0410
1668.8988

Table VII: Comparison of techniques based on manually selected kinematic observables. The
metrics shown are the expected mean squared error on the log likelihood ratio with and without
trimming, as deﬁned in the text. Checkmarks in the last two columns denote estimators included in
Table IV and the ﬁgures in the main part of this paper, respectively.

59

Table IV Figures

Expected MSE

All Trimmed

0.0409

0.0213

0.0301

0.0111

Fig. 12

Strategy

Carl (PbP, raw)
Carl (PbP, cal.)
Carl (parameterized, raw)

Setup

PbP

PbP

Carl (parameterized, cal.)

Carl (morphing-aware, raw) Baseline

Carl (morphing-aware, cal.) Baseline

Baseline
Baseline, shallow
Baseline, deep
Random θ
Random θ, shallow
Random θ, deep

Baseline
Baseline, shallow
Baseline, deep
Random θ
Random θ, shallow
Random θ, deep

0.0157
0.0134
0.0161
0.0148
0.0130
0.0164

0.0156
0.0124
0.0160
0.0147
0.0122
0.0155

0.1598
0.1483
Baseline, shallow
0.1743
Random θ
Random θ, shallow
0.1520
Morphing basis, shallow 10.1231

Baseline, shallow
Random θ
Random θ, shallow
Morphing basis, shallow

0.1036
0.0762
0.1076
0.0858
0.1564

0.0040
0.0035
0.0038
0.0037
0.0037
0.0038

0.0032
0.0026
0.0029
0.0029
0.0028
0.0029

0.0350
0.0331
0.0429
0.0369
7.9314

0.0282
0.0200
0.0289
0.0226
0.0618

(cid:88)

Fig. 12

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

Table VIII: Comparison of diﬀerent versions of the Carl technique. The metrics shown are the
expected mean squared error on the log likelihood ratio with and without trimming, as deﬁned in
the text. Checkmarks in the last two columns denote estimators included in Table IV and the
ﬁgures in the main part of this paper, respectively.

Strategy

Rolr (PbP, raw)
Rolr (PbP, cal.)
Rolr (param., raw)

Setup

PbP

PbP

60

Table IV Figures

Expected MSE

All Trimmed

0.0052

0.0023

0.0049

0.0022

(cid:88)

Rolr (param., cal.)

(cid:88)

Baseline
Baseline, shallow
Baseline, deep
Random θ
Random θ, shallow
Random θ, deep

Baseline
Baseline, shallow
Baseline, deep
Random θ
Random θ, shallow
Random θ, deep

Baseline, shallow
Random θ
Random θ, shallow
Morphing basis, shallow

Baseline, shallow
Random θ
Random θ, shallow
Morphing basis, shallow

0.0034
0.0069
0.0041
0.0034
0.0070
0.0036

0.0032
0.0059
0.0038
0.0030
0.0060
0.0034

0.2029
0.1672
0.1908
0.1160
5.6668

0.0328
0.0243
0.0321
0.0224
0.1300

0.0019
0.0037
0.0022
0.0017
0.0036
0.0017

0.0017
0.0030
0.0019
0.0014
0.0030
0.0015

0.1449
0.1305
0.1353
0.0755
3.8335

0.0088
0.0063
0.0089
0.0052
0.0485

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

Rolr (morph.-aware, raw) Baseline

Rolr (morph.-aware, cal.) Baseline

Table IX: Comparison of diﬀerent versions of the Rolr technique. The metrics shown are the
expected mean squared error on the log likelihood ratio with and without trimming, as deﬁned in
the text. Checkmarks in the last two columns denote estimators included in Table IV and the
ﬁgures in the main part of this paper, respectively.

Expected MSE

All Trimmed

Table IV Figures

Strategy

Setup

Sally

Fixed 2D histogram
Fixed 2D histogram, shallow
Fixed 2D histogram, deep
Dynamic 2D histogram
Dynamic 2D histogram, shallow
Dynamic 2D histogram, deep

Sallino 1D histogram

1D histogram, shallow
1D histogram, deep

0.0174
0.0170
0.0171
0.0132
0.0133
0.0132

0.0213
0.0215
0.0213

0.0005
0.0005
0.0005
0.0003
0.0003
0.0002

0.0006
0.0007
0.0006

(cid:88)

(cid:88)

(cid:88)

Table X: Comparison of diﬀerent versions of the Sally and Sallino techniques. The metrics
shown are the expected mean squared error on the log likelihood ratio with and without trimming,
as deﬁned in the text. Checkmarks in the last two columns denote estimators included in Table IV
and the ﬁgures in the main part of this paper, respectively.

Strategy

Setup

Cascal (param., raw)

61

Expected MSE

All Trimmed

Table IV Figures

Baseline, α = 5
Baseline, α = 5 , shallow
Baseline, α = 5 , deep
Baseline, α = 0.5, deep
Baseline, α = 1, deep
Baseline, α = 2, deep
Baseline, α = 10, deep
Baseline, α = 20, deep
Baseline, α = 50, deep
Random θ
Random θ, shallow
Random θ, deep

Baseline, α = 5
Baseline, α = 5 , shallow
Baseline, α = 5 , deep
Baseline, α = 0.5, deep
Baseline, α = 1, deep
Baseline, α = 2, deep
Baseline, α = 10, deep
Baseline, α = 20, deep
Baseline, α = 50, deep
Random θ, α = 5
Random θ, α = 5 , shallow
Random θ, α = 5 , deep

0.0019
0.0037
0.0010
0.0017
0.0014
0.0017
0.0013
0.0016
0.0024
0.0022
0.0038
0.0010

0.0012
0.0025
0.0008
0.0013
0.0011
0.0010
0.0010
0.0011
0.0016
0.0013
0.0027
0.0009

0.1935
0.1870
Baseline, α = 5 , shallow
Random θ, α = 5 , shallow
0.1624
Morph. basis, α = 5 , shallow 0.0707

0.1408
0.1359
Baseline, α = 5 , shallow
Random θ, α = 5 , shallow
0.0922
Morph. basis, α = 5 , shallow 0.0403

0.0004
0.0004
0.0003
0.0006
0.0005
0.0008
0.0004
0.0004
0.0007
0.0006
0.0005
0.0003

0.0002
0.0003
0.0002
0.0003
0.0003
0.0002
0.0003
0.0003
0.0005
0.0003
0.0004
0.0002

0.0810
0.0732
0.0643
0.0109

0.0508
0.0427
0.0268
0.0081

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

Cascal (param., cal.)

Cascal (morph.-aw., raw) Baseline, α = 5

Cascal (morph.-aw., cal.) Baseline, α = 5

Table XI: Comparison of diﬀerent versions of the Cascal technique. The metrics shown are the
expected mean squared error on the log likelihood ratio with and without trimming, as deﬁned in
the text. Checkmarks in the last two columns denote estimators included in Table IV and the
ﬁgures in the main part of this paper, respectively.

Strategy

Setup

Rascal (param., raw)

62

Expected MSE

All Trimmed

Table IV Figures

Baseline, α = 100
Baseline, α = 100, shallow
Baseline, α = 100, deep
Baseline, α = 10, deep
Baseline, α = 20, deep
Baseline, α = 50, deep
Baseline, α = 200, deep
Baseline, α = 500, deep
Baseline, α = 1000, deep
Random θ
Random θ, shallow
Random θ, deep

Baseline, α = 100
Baseline, α = 100, shallow
Baseline, α = 100, deep
Baseline, α = 10, deep
Baseline, α = 20, deep
Baseline, α = 50, deep
Baseline, α = 200, deep
Baseline, α = 500, deep
Baseline, α = 1000, deep
Random θ, α = 100
Random θ, α = 100, shallow
Random θ, α = 100, deep

0.0010
0.0025
0.0009
0.0011
0.0009
0.0009
0.0009
0.0011
0.0012
0.0011
0.0030
0.0008

0.0010
0.0021
0.0009
0.0010
0.0009
0.0009
0.0008
0.0009
0.0012
0.0010
0.0025
0.0008

0.2880
0.3569
Baseline, α = 100, shallow
0.2705
Random θ, α = 100
Random θ, α = 100, shallow
0.3243
Morph. basis, α = 100, shallow 0.1909

0.1530
0.1250
Baseline, α = 100, shallow
0.1358
Random θ, α = 100
Random θ, α = 100, shallow
0.1316
Morph. basis, shallow, α = 100 0.0307

0.0003
0.0006
0.0004
0.0005
0.0004
0.0004
0.0004
0.0006
0.0007
0.0004
0.0010
0.0004

0.0003
0.0005
0.0004
0.0004
0.0004
0.0004
0.0004
0.0005
0.0006
0.0004
0.0008
0.0004

0.2024
0.2861
0.1825
0.2488
0.1673

0.0673
0.0514
0.0627
0.0539
0.0072

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

Rascal (param., cal.)

Rascal (morph.-aw., raw) Baseline, α = 100

Rascal (morph.-aw., cal.) Baseline, α = 100

Table XII: Comparison of diﬀerent versions of the Rascal technique. The metrics shown are the
expected mean squared error on the log likelihood ratio with and without trimming, as deﬁned in
the text. Checkmarks in the last two columns denote estimators included in Table IV and the
ﬁgures in the main part of this paper, respectively.

63

[1] T. Sjostrand, S. Mrenna, and P. Z. Skands, Comput. Phys. Commun. 178, 852 (2008), arXiv:0710.3820.
[2] S. Agostinelli et al. (GEANT4), Nucl. Instrum. Meth. A506, 250 (2003).
[3] K. S. Cranmer, Comput. Phys. Commun. 136, 198 (2001), arXiv:hep-ex/0011057 [hep-ex]; K. Cranmer,
G. Lewis, L. Moneta, A. Shibata, and W. Verkerke (ROOT), (2012); M. Frate, K. Cranmer, S. Kalia,
A. Vandenberg-Rodes, and D. Whiteson, (2017), arXiv:1709.05681 [physics.data-an].

[4] J. Brehmer, K. Cranmer, F. Kling, and T. Plehn, Phys. Rev. D95, 073002 (2017), arXiv:1612.05261

[hep-ph].

[5] K. Kondo, J. Phys. Soc. Jap. 57, 4126 (1988).
[6] V. M. Abazov et al. (D0), Nature 429, 638 (2004), arXiv:hep-ex/0406031 [hep-ex].
[7] P. Artoisenet and O. Mattelaer, Proceedings, 2nd International Workshop on Prospects for charged
Higgs discovery at colliders (CHARGED 2008): Uppsala, Sweden, September 16-19, 2008, PoS
CHARGED2008, 025 (2008).

[8] Y. Gao, A. V. Gritsan, Z. Guo, K. Melnikov, M. Schulze, and N. V. Tran, Phys. Rev. D81, 075022

(2010), arXiv:1001.3396 [hep-ph].

[9] J. Alwall, A. Freitas, and O. Mattelaer, Phys. Rev. D83, 074010 (2011), arXiv:1010.2263 [hep-ph].
[10] S. Bolognesi, Y. Gao, A. V. Gritsan, K. Melnikov, M. Schulze, N. V. Tran, and A. Whitbeck, Phys.

Rev. D86, 095031 (2012), arXiv:1208.4018 [hep-ph].

[11] P. Avery et al., Phys. Rev. D87, 055006 (2013), arXiv:1210.0896 [hep-ph].
[12] J. R. Andersen, C. Englert, and M. Spannowsky, Phys. Rev. D87, 015019 (2013), arXiv:1211.3011

[13] J. M. Campbell, R. K. Ellis, W. T. Giele,

and C. Williams, Phys. Rev. D87, 073005 (2013),

[14] P. Artoisenet, P. de Aquino, F. Maltoni, and O. Mattelaer, Phys. Rev. Lett. 111, 091802 (2013),

[hep-ph].

arXiv:1301.7086 [hep-ph].

arXiv:1304.6414 [hep-ph].

[15] J. S. Gainer, J. Lykken, K. T. Matchev, S. Mrenna, and M. Park, in Proceedings, 2013 Community
Summer Study on the Future of U.S. Particle Physics: Snowmass on the Mississippi (CSS2013):
Minneapolis, MN, USA, July 29-August 6, 2013 (2013) arXiv:1307.3546 [hep-ph].

[16] D. Schouten, A. DeAbreu, and B. Stelzer, Comput. Phys. Commun. 192, 54 (2015), arXiv:1407.7595

[physics.comp-ph].

[hep-ph].

[17] T. Martini and P. Uwer, JHEP 09, 083 (2015), arXiv:1506.08798 [hep-ph].
[18] A. V. Gritsan, R. Röntsch, M. Schulze, and M. Xiao, Phys. Rev. D94, 055023 (2016), arXiv:1606.03107

[19] T. Martini and P. Uwer, (2017), arXiv:1712.04527 [hep-ph].
[20] D. Atwood and A. Soni, Phys. Rev. D45, 2405 (1992).
[21] M. Davier, L. Duﬂot, F. Le Diberder, and A. Rouge, Phys. Lett. B306, 411 (1993).
[22] M. Diehl and O. Nachtmann, Z. Phys. C62, 397 (1994).
[23] D. E. Soper and M. Spannowsky, Phys. Rev. D84, 074002 (2011), arXiv:1102.3480 [hep-ph].
[24] D. E. Soper and M. Spannowsky, Phys. Rev. D87, 054012 (2013), arXiv:1211.3140 [hep-ph].
[25] D. E. Soper and M. Spannowsky, Phys. Rev. D89, 094005 (2014), arXiv:1402.1189 [hep-ph].
[26] C. Englert, O. Mattelaer, and M. Spannowsky, Phys. Lett. B756, 103 (2016), arXiv:1512.03429 [hep-ph].
[27] D. B. Rubin, Ann. Statist. 12, 1151 (1984).
[28] M. A. Beaumont, W. Zhang, and D. J. Balding, Genetics 162, 2025 (2002).
[29] P. Marjoram, J. Molitor, V. Plagnol, and S. Tavaré, Proceedings of the National Academy of Sciences

[30] S. A. Sisson, Y. Fan, and M. M. Tanaka, Proceedings of the National Academy of Sciences 104, 1760

[31] S. A. Sisson and Y. Fan, Likelihood-free MCMC (Chapman & Hall/CRC, New York.[839], 2011).
[32] J.-M. Marin, P. Pudlo, C. P. Robert, and R. J. Ryder, Statistics and Computing , 1 (2012).
[33] T. Charnock, G. Lavaux, and B. D. Wandelt, Phys. Rev. D97, 083004 (2018), arXiv:1802.03537

100, 15324 (2003).

(2007).

[astro-ph.IM].

[34] K. Cranmer, J. Pavez, and G. Louppe, (2015), arXiv:1506.02169 [stat.AP].
[35] K. Cranmer and G. Louppe, J. Brief Ideas (2016), 10.5281/zenodo.198541.
[36] Y. Fan, D. J. Nott, and S. A. Sisson, ArXiv e-prints (2012), arXiv:1212.1479 [stat.CO].

64

[37] G. Papamakarios and I. Murray, in Advances in Neural Information Processing Systems 29 , edited by
D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (Curran Associates, Inc., 2016) pp.
1028–1036.

[38] B. Paige and F. Wood, ArXiv e-prints (2016), arXiv:1602.06701 [stat.ML].
[39] R. Dutta, J. Corander, S. Kaski, and M. U. Gutmann, ArXiv e-prints

(2016), arXiv:1611.10242

[stat.ML].

[cs.LG].

[cs.CV].

[40] M. U. Gutmann, R. Dutta, S. Kaski, and J. Corander, Statistics and Computing , 1 (2017).
[41] D. Tran, R. Ranganath, and D. M. Blei, ArXiv e-prints (2017), arXiv:1702.08896 [stat.ML].
[42] G. Louppe and K. Cranmer, ArXiv e-prints (2017), arXiv:1707.07113 [stat.ML].
[43] L. Dinh, D. Krueger, and Y. Bengio, ArXiv e-prints (2014), arXiv:1410.8516 [cs.LG].
[44] D. Jimenez Rezende and S. Mohamed, ArXiv e-prints (2015), arXiv:1505.05770 [stat.ML].
[45] L. Dinh, J. Sohl-Dickstein, and S. Bengio, ArXiv e-prints (2016), arXiv:1605.08803 [cs.LG].
[46] G. Papamakarios, T. Pavlakou, and I. Murray, ArXiv e-prints (2017), arXiv:1705.07057 [stat.ML].
[47] B. Uria, M.-A. Côté, K. Gregor, I. Murray, and H. Larochelle, ArXiv e-prints (2016), arXiv:1605.02226

[48] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior,

and K. Kavukcuoglu, ArXiv e-prints (2016), arXiv:1609.03499 [cs.SD].

[49] A. van den Oord, N. Kalchbrenner, O. Vinyals, L. Espeholt, A. Graves, and K. Kavukcuoglu, ArXiv

e-prints (2016), arXiv:1606.05328 [cs.CV].

[50] A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu, ArXiv e-prints (2016), arXiv:1601.06759

[51] G. Papamakarios, D. C. Sterratt, and I. Murray, ArXiv e-prints (2018), arXiv:1805.07226 [stat.ML].
[52] J. Brehmer, K. Cranmer, G. Louppe, and J. Pavez, (2018), arXiv:1805.00013 [hep-ph].
[53] J. Brehmer, G. Louppe, J. Pavez, and K. Cranmer, (2018), arXiv:1805.12244 [stat.ML].
[54] J. Brehmer, K. Cranmer, G. Louppe, and J. Pavez, “Code repository for the paper “Constraining Eﬀect-
ive Field Theories with Machine Learning”,” http://github.com/johannbrehmer/higgs_inference
(2018).

[55] S. R. Coleman, J. Wess, and B. Zumino, Phys. Rev. 177, 2239 (1969).
[56] C. G. Callan, Jr., S. R. Coleman, J. Wess, and B. Zumino, Phys. Rev. 177, 2247 (1969).
[57] S. Weinberg, Phys. Lett. B91, 51 (1980).
[58] C. J. C. Burges and H. J. Schnitzer, Nucl. Phys. B228, 464 (1983).
[59] C. N. Leung, S. T. Love, and S. Rao, Z. Phys. C31, 433 (1986).
[60] W. Buchmuller and D. Wyler, Nucl. Phys. B268, 621 (1986).
[61] C. Arzt, M. B. Einhorn, and J. Wudka, Nucl. Phys. B433, 41 (1995), arXiv:hep-ph/9405214 [hep-ph].
[62] K. Hagiwara, S. Ishihara, R. Szalapski, and D. Zeppenfeld, Phys. Rev. D48, 2182 (1993).
[63] B. Grzadkowski, M. Iskrzynski, M. Misiak, and J. Rosiek, JHEP 10, 085 (2010), arXiv:1008.4884

[hep-ph].

[64] J. Alwall, R. Frederix, S. Frixione, V. Hirschi, F. Maltoni, O. Mattelaer, H. S. Shao, T. Stelzer,

P. Torrielli, and M. Zaro, JHEP 07, 079 (2014), arXiv:1405.0301 [hep-ph].

[65] B. Henning, X. Lu, and H. Murayama, JHEP 01, 023 (2016), arXiv:1412.1837 [hep-ph].
[66] D. de Florian et al. (LHC Higgs Cross Section Working Group), (2016), 10.23731/CYRM-2017-002,

arXiv:1610.07922 [hep-ph].

[hep-ph].

[67] J. Brehmer, A. Freitas, D. Lopez-Val, and T. Plehn, Phys. Rev. D93, 075014 (2016), arXiv:1510.03443

[68] J. Brehmer, F. Kling, T. Plehn, and T. M. P. Tait, (2017), arXiv:1712.02350 [hep-ph].
[69] J. de Favereau, C. Delaere, P. Demin, A. Giammanco, V. Lemaître, A. Mertens, and M. Selvaggi

(DELPHES 3), JHEP 02, 057 (2014), arXiv:1307.6346 [hep-ex].

[70] G. Aad et al. (ATLAS), “A morphing technique for signal modelling in a multidimensional space of

coupling parameters,” (2015), Physics note ATL-PHYS-PUB-2015-047.

[71] J. R. Dell’Aquila and C. A. Nelson, Phys. Rev. D33, 80 (1986).
[72] T. Plehn, D. L. Rainwater, and D. Zeppenfeld, Phys. Rev. Lett. 88, 051801 (2002), arXiv:hep-ph/0105325

[73] V. Hankele, G. Klamke, D. Zeppenfeld, and T. Figy, Phys. Rev. D74, 095001 (2006), arXiv:hep-

[hep-ph].

ph/0609075 [hep-ph].

[74] K. Hagiwara, Q. Li, and K. Mawatari, JHEP 07, 101 (2009), arXiv:0905.4314 [hep-ph].

65

[75] C. Englert, D. Goncalves-Netto, K. Mawatari, and T. Plehn, JHEP 01, 148 (2013), arXiv:1212.0843

[hep-ph].

[76] K. Cranmer and T. Plehn, Eur. Phys. J. C51, 415 (2007), arXiv:hep-ph/0605268 [hep-ph].
[77] T. Plehn, P. Schichtel, and D. Wiegand, Phys. Rev. D89, 054002 (2014), arXiv:1311.2591 [hep-ph].
[78] F. Kling, T. Plehn, and P. Schichtel, Phys. Rev. D95, 035026 (2017), arXiv:1607.07441 [hep-ph].
[79] P. Baldi, K. Cranmer, T. Faucett, P. Sadowski, and D. Whiteson, Eur. Phys. J. C76, 235 (2016),

arXiv:1601.07913 [hep-ex].

[80] K. Cranmer, J. Pavez, G. Louppe, and W. K. Brooks, Proceedings, 17th International Workshop on
Advanced Computing and Analysis Techniques in Physics Research (ACAT 2016): Valparaiso, Chile,
January 18-22, 2016, J. Phys. Conf. Ser. 762, 012034 (2016).

[81] J. Alsing, B. Wandelt, and S. Feeney, (2018), arXiv:1801.01497 [astro-ph.CO].
[82] A. Hyvärinen, Journal of Machine Learning Research 6, 695 (2005).
[83] D. P. Kingma and J. Ba, ArXiv e-prints (2014), arXiv:1412.6980 [cs.LG].
[84] F. Chollet et al., “Keras,” https://github.com/keras-team/keras (2015).
[85] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,
M. Kudlur, J. Levenberg, D. Mane, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens,
B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viegas, O. Vinyals,
P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, ArXiv e-prints (2016), arXiv:1603.04467
[cs.DC].

[86] J. B. Kruskal, Psychometrika 29, 115 (1964).
[87] S. S. Wilks, Annals Math. Statist. 9, 60 (1938).
[88] G. Cowan, K. Cranmer, E. Gross, and O. Vitells, Eur. Phys. J. C71, 1554 (2011), [Erratum: Eur.

Phys. J. C73, p. 2501, 2013], arXiv:1007.1727 [physics.data-an].

[89] A. Wald, Transactions of the American Mathematical Society 54, 426 (1943).
[90] G. Louppe, M. Kagan, and K. Cranmer, (2016), arXiv:1611.01046 [stat.ME].
[91] A. Mertens, in Proceedings, 15th International Workshop on Advanced Computing and Analysis Tech-

niques in Physics Research (ACAT 2013), Vol. 523 (2014) p. 012028.

8
1
0
2
 
l
u
J
 
6
2
 
 
]
h
p
-
p
e
h
[
 
 
4
v
0
2
0
0
0
.
5
0
8
1
:
v
i
X
r
a

A Guide to Constraining Eﬀective Field Theories with Machine Learning

Johann Brehmer,1 Kyle Cranmer,1 Gilles Louppe,2 and Juan Pavez3
1New York University, USA
2University of Liège, Belgium
3Federico Santa María Technical University, Chile
(Dated: 30th July 2018)

We develop, discuss, and compare several inference techniques to constrain theory para-
meters in collider experiments. By harnessing the latent-space structure of particle physics
processes, we extract extra information from the simulator. This augmented data can be
used to train neural networks that precisely estimate the likelihood ratio. The new methods
scale well to many observables and high-dimensional parameter spaces, do not require any
approximations of the parton shower and detector response, and can be evaluated in micro-
seconds. Using weak-boson-fusion Higgs production as an example process, we compare the
performance of several techniques. The best results are found for likelihood ratio estimators
trained with extra information about the score, the gradient of the log likelihood function with
respect to the theory parameters. The score also provides suﬃcient statistics that contain
all the information needed for inference in the neighborhood of the Standard Model. These
methods enable us to put signiﬁcantly stronger bounds on eﬀective dimension-six operators
than the traditional approach based on histograms. They also outperform generic machine
learning methods that do not make use of the particle physics structure, demonstrating their
potential to substantially improve the new physics reach of the LHC legacy results.

CONTENTS

I. Introduction

II. The EFT measurement problem

A. Eﬀective ﬁeld theory
B. Physics challenges and traditional methods
C. Structural properties of EFT measurements
D. Explicit example

III. Likelihood ratio estimation

A. Modeling likelihood ratios
B. Available information and its usefulness
C. Strategies
D. Calibration
E. Implementation
F. Challenges and diagnostics

IV. Limit setting

A. Asymptotics
B. Neyman construction
C. Nuisance parameters

V. Results

A. Idealized setup
B. Detector eﬀects

2

4
4
4
6
8

11
12
14
18
24
25
28

30
30
31
32

33
33
41

2

42

45
45
46
57

63

VI. Conclusions

A. Appendix

1. Simpliﬁed detector description
2. Model almanac
3. Additional results

References

I.

INTRODUCTION

An important aspect of the legacy of the Large Hadron Collider (LHC) experiments will be precise
constraints on indirect signatures of physics beyond the Standard Model (SM), parameterized for
instance by the dimension-six operators of the Standard Model eﬀective ﬁeld theory (SMEFT). The
relevant measurements can easily involve tens of diﬀerent parameters that predict subtle kinematic
signatures in the high-dimensional space of the data. Traditional analysis techniques do not scale
well to this complex problem, motivating the development of more powerful techniques.

The analysis of high-energy-physics data is based on an impressive suite of simulation tools that
model the hard interaction, parton shower, hadronization, and detector response. The community
has invested a tremendous amount of eﬀort into developing these tools, yielding the high-ﬁdelity
modeling of LHC data needed for precision measurements. Simulators such as Pythia [1] and
Geant4 [2] use Monte-Carlo techniques to sample the multitudinous paths through which a
particular hard scattering might develop. A single event’s path through the simulation can easily
involve many millions of random variables. While Monte-Carlo techniques can eﬃciently sample
from the distributions implicitly deﬁned by the simulators, it is not feasible to calculate the likelihood
for a particular observation because doing so would require integrating over all the possible histories
leading to that observation. Clearly it is infeasible to explicitly calculate a numerical integral over
this enormous latent space. While this problem is ubiquitous in high energy physics, it is rarely
acknowledged explicitly.

Traditionally, particle physicists have approached this problem by restricting the analysis to
one or two well-motivated discriminating variables, discarding the information contained in the
remaining observables. The probability density for the restricted set of discriminating variables is
then estimated with explicit functions or non-parametric approaches such as template histograms,
kernel density estimates, or Gaussian Processes [3]. These low-dimensional density estimates are
constructed and validated using Monte-Carlo samples from the simulation. While well-chosen
variables may yield precise bounds along individual directions of the parameter space, they often
lead to weak constraints in other directions in the parameter space [4]. The sensitivity to multiple
parameters can be substantially improved by using the fully diﬀerential cross section. This is the
forte of the Matrix Element Method [5–19] and Optimal Observables [20–22] techniques, which
are based on the parton-level structure of a given process. Shower and event deconstruction [23–
26] extend this approach to the parton shower. But all these methods still require some level
of approximations on the parton shower and either neglect or crudely approximate the detector
response. Moreover, even a simpliﬁed description of the detector eﬀects requires the numerically
expensive evaluation of complicated integrals for each observed event. None of these established
approaches scales well to high-dimensional problems with many parameters and observables, such
as the SMEFT measurements.

In recent years there has been increased appreciation that several real-world phenomena are
better described by simulators that do not admit a tractable likelihood. This appears in ﬁelds as
diverse as ecology, phylogenetics, epidemiology, cardiac simulators, quantum chemistry, and particle

3

physics. Inference in this setting is often referred to as likelihood-free inference, where the inference
strategy is restricted to samples generated from the simulator. Implicitly, these techniques aim to
estimate the likelihood. A particularly ubiquitous technique is Approximate Bayesian Computation
(Abc) [27–33]. Abc is closely related to the traditional template histogram and kernel density
estimation approach used by physicists. More recently, approximate inference techniques based on
machine learning and neural networks have been proposed [34–51]. All these techniques have in
common that they only take into account simulated samples similar to the actual observables — they
do not exploit the structure of the process that generates them.

We develop new simulation-based inference techniques that are tailored to the structure of particle
physics processes. The key insight behind these methods is that we can extract more information
than just samples from the simulations, and that this additional information can be used to eﬃciently
train neural networks that precisely estimate likelihood ratios, the preferred test statistics for LHC
measurements. These methods are designed for scalability to both high-dimensional parameter
spaces as well as to many observables. They do not require any simplifying assumptions to the
underlying physics: they support state-of-the-art event generators with parton shower, reducible
and irreducible backgrounds, and full detector simulations. After an upfront training phase, they are
very eﬃcient to evaluate. Our tools directly provide an estimator for the likelihood ratio, an intuitive
and easily interpretable quantity. Finally, limits derived from these tools with toy experiments have
the reassuring property that even if they might not be optimal, they are never wrong, i. e. no points
are said to be excluded that should not be excluded at a given conﬁdence level.

In Ref. [52], the companion paper of this publication, we focus on the key ideas and sensitivity
enabled by these techniques. Reference [53] presents the methods in a more abstract setting. Here we
describe the actual algorithms in detail, developing several diﬀerent methods side by side. Given the
number of discussed variations, this publication might have the look and feel of a review article and
we present it as a guide to the interested practitioner. We focus on the main ideas and diﬀerences
between the approaches and postpone many technical details until the appendices.

We evaluate the performance of these diﬀerent methods on a speciﬁc example problem, the
measurement of two dimension-six operators in Higgs production in weak boson fusion (WBF) in
the four-lepton mode at the LHC. For part of this analysis, we work in an idealized setting in which
we can access the true likelihood function, providing us with a ground truth for the comparison of
the diﬀerent analysis methods. After establishing the precision of the likelihood ratio estimation,
we turn towards the more physical question of how strongly the two operators can be constrained
with the diﬀerent techniques. We repeat the analysis with a simpliﬁed detector response where the
ground-truth likelihood is no longer tractable.

We begin by laying out the problem in Sec. II: we summarize the eﬀective ﬁeld theory idea,
list the challenges posed by EFT measurements, translate the problem from a physics perspective
into the language of statistics, and discuss its important structural properties. We also set up the
example process used throughout the rest of the paper. The description of the analysis methods
are split in two parts:
in Sec. III we deﬁne the diﬀerent techniques to estimate the likelihood
ratio, which includes most of the conceptual work presented here. Section IV then explains how
to set limits on the EFT parameters based on these tools. In Sec. V we evaluate the performance
of the diﬀerent tools in our example process. Finally, in Sec. VI we summarize our ﬁndings and
give recommendations for practitioners. The appendices describe the diﬀerent algorithms in more
detail and provide additional results. The code and data used for this paper are available online at
Ref. [54].

4

(1)

II. THE EFT MEASUREMENT PROBLEM

A. Eﬀective ﬁeld theory

Eﬀective ﬁeld theories (EFTs) [55–57] parameterize the eﬀects of physics at an energy scale Λ
on observables at smaller energies E (cid:28) Λ as a set of local operators. The form of these operators is
ﬁxed by the light particles and the symmetry structure of the theory and is entirely independent of
the high-energy model. Systematically expanding the Lagrangian in 1/Λ, equivalent to ordering the
operators by their canonical dimension, leaves us with a ﬁnite set of operators weighted by Wilson
coeﬃcients that describe all possible new physics eﬀects up to some order in E/Λ.

In the absence of new particles at the TeV scale, and assuming the symmetry structure of the
SM, we can thus describe any new physics signature in LHC processes in terms of a set of higher-
dimensional operators [58–63]. In this SM Eﬀective Field Theory (SMEFT), the leading eﬀects
beyond the SM come from 59 independent dimension-six operators Oo with Wilson coeﬃcients fo,

LD6 = LSM +

(cid:88)

o

fo
Λ2 Oo ,

where the SM corresponds to all fo = 0 and any measurement of a deviation hints at new physics.
The dimension-six Wilson coeﬃcients are perfectly suited as an interface between experimental
measurements and theory interpretations. They are largely model-independent, can parameterize a
wide range of observables, including novel kinematic features, and are theoretically consistent beyond
tree level. On the technical side, dimension-six operators are implemented in standard Monte-Carlo
event generators [64], allowing us to generate predictions for rates and kinematic observables for
any combination of Wilson coeﬃcients. Measured values of fo/Λ2 can easily be translated to the
parameters of speciﬁc models through well-established matching procedures [65]. All in all, SMEFT
measurements will likely be a key part of the legacy of the LHC experiments [66].

Let us brieﬂy comment on the question of EFT validity. A hierarchy of energy scales E (cid:28) Λ is
the key assumption behind the EFT construction, but in a bottom-up approach the cutoﬀ scale Λ
cannot be known without additional model assumptions. From a measurement fo/Λ2 (cid:54)= 0 we can
estimate the new physics scale Λ only by assuming a characteristic size of the new physics couplings
√
fo, and compare it to the energy scale E of the experiment. It has been found that dimension-six
operators often capture the dominant eﬀects of new physics even when there is only a moderate
scale separation E (cid:46) Λ [67]. All these concerns are not primarily of interest for the measurement of
Wilson coeﬃcients, but rather important for the interpretation of the results in speciﬁc UV theories.

B. Physics challenges and traditional methods

EFT measurements at the LHC face three fundamental challenges:

1. Individual scattering processes at the LHC are sensitive to several operators and require
simultaneous inference over a multi-dimensional parameter space. While a naive parameter
scan works well for one or two dimensions, it becomes prohibitively expensive for more than
a few parameters.

2. Most operators introduce new coupling structures and predict non-trivial kinematic features.
These do not translate one-to-one to traditional kinematic observables such as transverse
momenta, invariant masses or angular correlations. An analysis based on only one kinematic
variable typically cannot constrain the full parameter space eﬃciently. Instead, most of the

5

operator eﬀects only become fully apparent when multiple such variables including their
correlations are analysed [4, 68].

3. The likelihood function of the observables is intractable, making this the setting of “likelihood-
free inference” or “simulator-based inference”. There are simulators for the high-energy
interactions, the parton shower, and detector eﬀects that can generate events samples for
any theory parameter values, but they can only be run in the forward mode. Given a set of
reconstruction-level observables, it is not possible to evaluate the likelihood of this observation
given diﬀerent theory parameters. The reason is that this likelihood includes the integral over
all possible diﬀerent parton shower histories and particle trajectories through the detector as
a normalizing constant, which is infeasible to calculate in realistic situations. We will discuss
this property in more detail in the following section.

The last two issues are typically addressed in one of three ways. Most commonly, a small set of
discriminating variables (also referred to as summary statistics or engineered features) is handpicked
for a given problem. The likelihood in this low-dimensional space is then estimated, for instance, by
ﬁlling histograms from simulations. While well-chosen variables may lead to good constraints along
individual directions of the parameter space, there are typically directions in the parameter space
with limited sensitivity [4, 68].

The Matrix Element Method [5, 6, 8–15, 17–19] or Optimal Observables [20–22] go beyond a
few speciﬁc discriminating variables and use the matrix element for a particular process to estimate
the likelihood ratio. While these techniques can be very powerful, they suﬀer from two serious
limitations. The parton shower and detector response are either entirely neglected or approximated
through ad-hoc transfer function. Shower and event deconstruction [23–26] allow for the calculation
of likelihood ratios at the level of the parton shower, but still rely on transfer functions to describe
the detector response. Finally, even with such a simple description of the shower and detector, the
evaluation of the likelihood ratio estimator requires the numerically expensive computation of large
integrals for each observed event.

Finally, there is a class of generic methods for likelihood-free inference. For Bayesian inference,
the best-known approach is Approximate Bayesian Computation (Abc) [27–32]. Similar to the
histogram approach, it relies on the choice of appropriate low-dimensional summary statistics, which
can severely limit the sensitivity of the analysis. Diﬀerent techniques based on machine learning have
been developed recently. In particle physics, the most common example are discriminative classiﬁers
between two discrete hypotheses, such as a signal and a background process. This approach has
recently been extended to parameter measurements [34, 35]. More generally, many techniques based
on the idea of using a classiﬁcation model, such as neural networks, for inference in the absence of
a tractable likelihood function have been introduced in the machine learning community [36–51].
All of these methods only require samples of events trained according to diﬀerent parameter points.
They do not make use of the structure of the particle physics processes, and thus do not use all
available information.

All of these methods come with a price. We develop new techniques that
• are tailored to particle physics measurements and leverage their structural properties,
• scale well to high-dimensional parameter spaces,
• can accommodate many observables,
• capture the information in the fully diﬀerential cross sections, including all correlations

• fully support state-of-the art simulators with parton showers and full detector simulations,

between observables,

and

• are very eﬃcient to evaluate after an upfront training phase.

6

(2)

(3)

C. Structural properties of EFT measurements

1. Particle-physics structure

One essential step to ﬁnding the optimal measurement strategy is identifying the structures and
symmetries of the problem. Particle physics processes, in particular those described by eﬀective
ﬁeld theories, typically have two key properties that we can exploit.

First, any high-energy particle physics process factorizes into the parton-level process, which
contains the matrix element and in it the entire dependence on the EFT coeﬃcients, and a residual
part describing the parton shower and detector eﬀects. In many plausible scenarios of new physics
neither the strong interactions in the parton shower nor the electromagnetic and strong interactions
in the detector are aﬀected by the parameters of interest. The likelihood function can then be
written as

(cid:90)

(cid:90)

p(x|θ) =

dz p(x, z|θ) =

dz p(x|z) p(z|θ) .

Here and in the following x are the actual observables after the shower, detector, and reconstruction;
θ are the theory parameters of interest; and z are the parton-level momenta (a subset of the latent
variables). Table I provides a dictionary of these and other important symbols that we use.

The ﬁrst ingredient to this likelihood function is the distribution of parton-level four-momenta

p(z|θ) =

1
σ(θ)

dσ(θ)
dz

,

where σ(θ) and dσ(θ)/dz are the total and diﬀerential cross sections, respectively. Crucially, this
function is tractable: the matrix element and the parton density functions can be evaluated for
arbitrary four-momenta z and parameter values θ. In practice this means that matrix-element
codes such as MadGraph [64] can not only be run in a forward, generative mode, but also deﬁne
functions that return the squared matrix element for a given phase-space point z. Unfortunately,
there is typically no user-friendly interface to these functions, so evaluating it requires some work.
Second, the conditional density p(x|z) describes the probabilistic evolution from the parton-level
four-momenta to observable particle properties. While this symbol looks innocuous, it represents the
full parton shower, the interaction of particles with the detector material, the sensor response and
readout, and the reconstruction of observables. Diﬀerent simulators such as Pythia [1], Geant4 [2],
or Delphes [69] are often used to generate samples {x} ∼ p(x|z) for given parton-level momenta z.
This sampling involves the Monte-Carlo integration over the possible shower histories and detector
interactions,

(cid:90)

(cid:90)

p(x|z) =

dzdetector

dzshower p(x|zdetector) p(zdetector|zshower) p(zshower|z) .

(4)

This enormous latent space can easily involve many millions of random numbers, and these integrals
are clearly intractable, which we denote with the red symbol p. In other words, given a set of
reconstruction-level observables x, we cannot calculate the likelihood function p(x|z) that describes
the compatibility of parton-level momenta z with the observation. By extension, we also cannot
evaluate p(x|θ), the likelihood function of the theory parameters given the observation. The
intractable integrals in Eq. (4) are the crux of the EFT measurement problem.

The factorization of Eq. 2 together with the tractability of the parton-level likelihood p(z|θ) is
immensely important. We will refer to the combination of these two properties as particle-physics
structure. The far-reaching consequences of this structure for EFT measurements will be the topic
of Sec. III B. Many (but not all) of the inference strategies we discuss will rely on this condition.

Symbol

Physics meaning

Machine learning abstraction

Set of all observables
One or two kinematic variables

z ≡ zparton
zshower
zdetector
zall = (zparton, zshower, zdetector) Full simulation history of event
θ

Parton-level four-momenta
Parton shower trajectories
Detector interactions

Theory parameters (Wilson
coeﬃcients)
Best ﬁt for theory parameters

x
v

ˆθ

p(x|θ)

p(z|θ)

p(x|z)

Distributions of observables given
theory parameters
Parton-level distributions from
matrix element
Eﬀect of shower, detector,
reconstruction

7

Features
Low-dimensional summary
statistics /engineered feature
Latent variables
Latent variables
Latent variables
All latent variables
Parameters of interest

Estimator for parameters of
interest

Intractable likelihood

Tractable likelihood of latent
variables
Intractable density deﬁned
through stochastic generative
process

r(x|θ0, θ1)
ˆr(x|θ0, θ1)
t(x|θ)
ˆt(x|θ)

xe, ze
θo
θc, wc(z), pc(x)

Likelihood ratio between hypotheses θ0, θ1, see Eq. (11).
Estimator for likelihood ratio
Score, see Eq. (14).
Estimator for score

Event
Wilson coeﬃcient for one operator

Data point
Individual parameter of interest

Morphing basis points, coeﬃcients, densities, see Eq. (6).

Table I: Dictionary deﬁning many symbols that appear in this paper. Red symbols denote
intractable likelihood functions. The last three rows explain our conventions for indices.

Note that this Markov property holds even with reducible and irreducible backgrounds and when
a matching scheme is used to combine diﬀerent parton-level multiplicities. In these situations there
may be diﬀerent disjoint parts of z space, even with diﬀerent dimensionalities, for instance when
events with n and n + 1 partons in the ﬁnal state can lead to the same conﬁguration of observed
jets. The integral over z then has to be replaced with a sum over “zn spaces” and an integral over
each zn, but the logic remains unchanged.

2. Operator morphing

Eﬀective ﬁeld theories (and other parameterisations of indirect signatures of new physics) typically
contribute a ﬁnite number of amplitudes to a given process, each of which is multiplied by a function
of the Wilson coeﬃcients.1 In this case the likelihood can be written as

p(z|θ) =

˜wc(cid:48)(θ) fc(cid:48)(z)

(cid:88)

c(cid:48)

(5)

where c(cid:48) labels the diﬀerent amplitude components, and the functions fc(cid:48)(z) are not necessarily
properly positive deﬁnite or normalized.

1 Exceptions can arise for instance when particle masses or widths depend on the parameters of interest. But in an

EFT setting one can expand these quantities in 1/Λ, restoring the factorization.

The simplest example is a process in which one SM amplitude M0(z) interferes with one new
physics amplitude MBSM(z|θ) = θM1(z), which scales linearly with a new physics parameter
θ. The diﬀerential cross section, proportional to the squared matrix element, is then dσ(z) ∝
|M0(z)|2 + 2θ Re M0(z)†M1(z) + θ2 |M1(z)|2. There are three components, representing the SM,
interference, and pure BSM terms, each with their own parameter dependence ˜wc(cid:48)(z) and momentum
dependence fc(cid:48)(z).

We can then pick a number of basis2 parameter points θc equal to the number of components
c(cid:48) in Eq. (5). They can always be chosen such that the matrix Wcc(cid:48) = ˜wc(cid:48)(θc) is invertible, which
allows us to rewrite (5) as a mixture model

p(z|θ) =

wc(θ) pc(z)

(cid:88)

c

with weights wc(θ) = (cid:80)
cc(cid:48) and (now properly normalized) basis densities pc(z) = p(z|θc).
The weights wc(θ) depend on the choice of basis points and are analytically known. This “morphing”
procedure therefore allows us to extract the full likelihood function p(z|θ) from a ﬁnite set of
evaluations of basis densities pc(z).

c(cid:48) ˜wc(θ) W −1

Calculating the full statistical model through morphing requires the likelihood p(z|θ) to be
tractable, which is true for parton-level momenta as argued above. However, the same trick can
be applied even when the exact likelihood is intractable, but we can estimate it. For instance, the
marginal distribution of any individual kinematic variable v(x) can be reliably estimated through
histograms or other density estimation techniques, even when shower and detector eﬀects are taken
into account. The morphing procedure then lets us evaluate the full conditional distribution p(v|θ)
based on a ﬁnite number of Monte-Carlo simulations [70].
Finally, note that Eq. (6) together with Eq. (2) imply

p(x|θ) =

wc(θ) pc(x) ,

(cid:88)

c

even if the likelihood function p(x|θ) and the components pc(x) are intractable. This will later allow
us to impose the morphing structure on likelihood ratio estimators.

Not all EFT amplitudes satisfy the morphing structure in Eq. (5), so we discuss both measurement
strategies that rely on and make use of this property as well as more general ones that do not require
it to hold.

8

(6)

(7)

D. Explicit example

1. Weak-boson-fusion Higgs to four leptons

As an explicit example LHC process we consider Higgs production in weak boson fusion (WBF)

with a decay of the Higgs into four leptons,

qq → qq h → qq ZZ → qq (cid:96)+(cid:96)− (cid:96)+(cid:96)−

(8)

with (cid:96) = e, µ, as shown in Fig. 1.

While this process is rare and is likely to only be observed during the high-luminosity run of
the LHC, it has a few compelling features that make it a prime candidate to study the eﬃcient
extraction of information. First, the two jets from the quarks and in particular the four leptons can

2 Note that the morphing basis points θc are unrelated to the choice of an operator basis for the eﬀective ﬁeld theory.

9

(9)

(10)

q

q

W , Z

W , Z

Z

Z

h

q

q

(cid:96)+
(cid:96)−

(cid:96)+
(cid:96)−

Figure 1: Feynman diagram for Higgs production in weak boson fusion in the 4(cid:96) mode. The red
dots show the Higgs-gauge interactions aﬀected by the dimension-six operators of our analysis.

be reconstructed quite precisely in the LHC detectors. Even when assuming on-shell conditions and
energy-momentum conservation, the ﬁnal-state momenta span a 16-dimensional phase space, giving
rise to many potentially informative observables.

Second, both the production of the Higgs boson in weak boson fusion as well as its decay
into four leptons are highly sensitive to the eﬀects of new physics in the Higgs-gauge sector. We
parameterize these with dimension-six operators in the SMEFT, following the conventions of the
Hagiwara-Ishihara-Szalapski-Zeppenfeld basis [62]. For simplicity, we limit our analysis to the two
particularly relevant operators

L = LSM +

fW
Λ2

ig
2
(cid:124)

(Dµφ)† σa Dνφ W a
µν
(cid:123)(cid:122)
(cid:125)
OW

−

fW W
Λ2

g2
4
(cid:124)

(φ†φ) W a

µν W µν a
(cid:125)

.

(cid:123)(cid:122)
OW W

For convenience, we rescale the Wilson coeﬃcients to the dimensionless parameters of interest

θ =

(cid:18) fW v2
Λ2

,

(cid:19)T

fW W v2
Λ2

where v = 246 GeV is the electroweak vacuum expectation value. As alluded to above, the validity
range of the EFT cannot be determined in a model-independent way. For moderately weakly to
moderately strongly coupled underlying new physics models, one would naively expect |fo| (cid:46) O (1)
and the EFT description to be useful in the range E ≈ v (cid:46) Λ, or −1 (cid:46) θo (cid:46) 1. This is the parameter
range we analyse in this paper.

The interference between the Standard Model amplitudes and the dimension-six operators leads
to an intricate relation between the observables and parameters in this process, which has been
studied extensively. The precise measurement of the momenta of the four leptons provides access to
a range of angular correlations that fully characterize the h → ZZ decay [10, 71]. These variables
are sensitive to the eﬀects of dimension-six operators. But the momentum ﬂow p through the decay
vertex is limited by the Higgs mass, and the relative eﬀects of these dimension-six operators are
suppressed by a factor p2/Λ2. On the other hand, the Higgs production through two oﬀ-shell gauge
bosons with potentially high virtuality does not suﬀer from this suppression. The properties of the
two jets recoiling against them are highly sensitive to operator eﬀects in this vertex [72–75].

In Fig. 2 we show example distributions of two particularly informative observables, the transverse
momentum of the leading (higher-pT ) jet pT,j1, and the azimuthal angle between the two jets, ∆φjj.
The two quantities are sensitive to diﬀerent directions in parameter space. Note also that the
interference between the diﬀerent amplitudes can give rise to non-trivial eﬀects. The size of the
dimension-six amplitudes grows with momentum transfer, which is strongly correlated with the
transverse momentum of the leading jet. If the interference of new-physics amplitudes with the SM
diagrams is destructive, this can drive the total amplitude through zero [67]. The jet momentum

10

Figure 2: Kinematic distributions in our example process for three example parameter points. We
assume an idealized detector response to be discussed in Sec. II D 2. Left: transverse momentum of
the leading (higher-pT ) jet, a variable strongly correlated with the momentum transfer in the
process. The dip around 350 GeVis a consequence of the amplitude being driven through zero, as
discussed in the text. Right: separation in azimuthal angle between the two jets.

distribution then dips and rises again with higher energies, as seen in the red curve in the left panel
of Fig. 2. Such depleted regions of low probability can lead to very small or large likelihood ratios
and potentially pose a challenge to inference methods.

By analysing the Fisher information in these distributions, it is possible to compare the discrimin-
ation power in these two observables to the information contained in the full multivariate distribution
or to the information in the total rate. It turns out that the full multivariate distribution p(z|θ)
contains signiﬁcantly more information than the one-dimensional and two-dimensional marginal
distributions of any standard kinematic variables [4]. The total rate is found to carry much less
information on the two operators, in particular when systematic uncertainties on the cross sections
are taken into account. In this study we therefore only analyse the kinematic distributions for a
ﬁxed number of observed events.

2. Sample generation

Already in the sample generation we can make use of the structural properties of the process
discussed in Sec. II C. The amplitude of this process factorizes into a sum of parameter-dependent
factors times phase-space-dependent amplitudes, as given in Eq. (5). The eﬀect of the operators OW
and OW W on the total Higgs width breaks this decomposition, but this eﬀect is tiny and in practice
irrelevant when compared to the experimental resolution. The likelihood function of this process
therefore follows the mixture model in Eq. (6) to good approximation, and the weights wc(θ) can
be calculated. Since the parton-level likelihood function is tractable, we can reconstruct the entire
likelihood function p(z|θ) based on a ﬁnite number of simulator runs, as described in Sec. II C 2.

To this end, we ﬁrst generate a parton-level sample {ze} of 5.5·106 events with MadGraph 5 [64]
and its add-on MadMax [76–78], using the setup described in Ref. [4]. With MadMax we can
evaluate the likelihood p(ze|θc) for all events ze and for 15 diﬀerent basis parameter points θc.
Calculating the morphing weights wc(θ) ﬁnally gives us the true parton-level likelihood function

11

Figure 3: Basis points θc and some of the morphing weights wc(θ) for our example process. Each
panel shows the morphing weight of one of the components c as a function of parameter space. The
weights of the remaining 13 components (not shown) follow qualitatively similar patterns. The dots
show the position of the basis points θc, the big black dot denotes the basis point corresponding to
the morphing weight shown in that panel. Away from the morphing basis points, the morphing
weights can easily reach O (100), with large cancellations between diﬀerent components.

p(ze|θ) for each generated phase-space point ze.

In Fig. 3 we show the basis points θc and two of the morphing weights wc(θ) with their dependence
on θ. In some corners of parameter space the weights easily reach up to |wc| (cid:46) O (100), and there
are large cancellations between positive and negative weights. This will pose a challenge for the
numerical stability of every inference algorithm that directly uses the morphing structure of the
process, as we will discuss later. Other basis choices have led to comparable or larger morphing
weights.

Parton shower and detector eﬀects smear the observed particle properties x with respect to
the parton-level momenta z and make the likelihood function in Eq. (2) intractable. We develop
inference methods that can be applied exactly in this case and that do not require any simplifying
assumptions on the shower and detector response. However, in this realistic scenario we cannot
evaluate their performance by comparing them to the true likelihood ratio. We therefore test them
ﬁrst on an idealized scenario in which the four-momenta, ﬂavor, and charges of the leptons, and the
momenta of the partons, can be measured exactly, p(x|z) ≈ δ(x − z). In this approximation we can
evaluate the likelihood p(x|θ).

After establishing the performance of the various algorithms in this idealized setup, we will analyse
the eﬀect of parton shower and detector simulation on the results. We generate an approximate
detector-level sample by drawing events from a smearing distribution p(x|z) conditional on the
parton-level momenta z. This smearing function is loosely motivated by the performance of the
LHC experiments and is deﬁned in Appendix A 1.

III. LIKELIHOOD RATIO ESTIMATION

According to the Neyman-Pearson lemma, the likelihood ratio

r(x|θ0, θ1) ≡

p(x|θ0)
p(x|θ1)

=

(cid:82) dz p(x, z|θ0)
(cid:82) dz p(x, z|θ1)

(11)

12

is the most powerful test statistic to discriminate between two hypotheses θ0 and θ1. Unfortunately,
the integral over the latent space z makes the likelihood function p(x|θ) as well as the likelihood
ratio r(x|θ0, θ1) intractable. The ﬁrst and crucial stage of all our EFT measurement strategies
is therefore the construction of a likelihood ratio estimator ˆr(x|θ0, θ1) that is as close to the true
r(x|θ0, θ1) as possible and thus maximizes the discrimination power between θ0 and θ1.

This estimation problem has several diﬀerent aspects that we try to disentangle as much as
possible. The ﬁrst choice is the overall structure of the likelihood ratio estimator and its dependence
on the theory parameters θ. We discuss this in Sec. III A. Section III B analyses what information is
available and useful to construct (train) the estimators for a given process. Here we will introduce
the main ideas that harness the structure of the EFT to increase the information that is used in the
training process.

These basic concepts are combined into concrete strategies for the estimation of the likelihood ratio
in Sec. III C. After training the estimators, there is an optional additional calibration stage, which
we introduce in Sec. III D. Section III E describes the technical implementation of these strategies
in terms of neural networks. Finally, we discuss the challenges that the diﬀerent algorithms face in
Sec. III F and introduce diagnostic tools for the uncertainties.

A. Modeling likelihood ratios

1. Likelihood ratios

There are diﬀerent approaches to the structure of this estimator, in particular to the dependence

on the theory parameters θ:

Point by point (PbP): A common strategy is to scan the parameter space, randomly or in a
grid. To reduce the complexity of the scan one can keep the denominator θ1 ﬁxed, while
scanning only θ0. Likelihood ratios with other denominators can be extracted trivially as
ˆr(x|θ0, θ2) = ˆr(x|θ0, θ1)/ˆr(x|θ2, θ1). Instead of a single reference value θ1, we can also use
a composite reference hypothesis p(x|θ1) → pref(x) = (cid:82) dθ1 π(θ1) p(x|θ1) with some prior
π(θ1). This can reduce the regions in feature space with small reference likelihood p(x|θ1)
and improve the numerical stability.

For each pair (θ0, θ1) separately, the likelihood ratio ˆr(x|θ0, θ1) as a function of x is estimated.
Only the ﬁnal results are interpolated between the scanned values of θ0.
This approach is particularly simple, but discards all information about the structure and
smoothness of the parameter space. For high-dimensional parameter spaces, the parameter
scan can become prohibitively expensive. The ﬁnal interpolation may introduce additional
uncertainties.

Agnostic parameterized estimators: Alternatively we can train one estimator as the full model
ˆr(x|θ0, θ1) as a function of both x and the parameter combination (θ0, θ1) [34, 79]. A
modiﬁcation is again to leave θ1 at a ﬁxed reference value (or ﬁxed composite reference
hypothesis with a prior π(θ1)) and only learn the dependence on x and θ0.
This parameterized approach leaves it to the estimator to learn the typically smooth depend-
ence of the likelihood ratio on the physics parameters and does not require any interpolation
in the end. There are no assumptions on the form of the dependence of the likelihood on the
ratios.

Morphing-aware estimators: For problems that satisfy the morphing condition of Eq. (6) and
thus also Eq. (7), we can impose this structure and the explicit knowledge of the weights

13

(12)

(13)

(14)

wc(θ) onto the estimator. Again, one option is to keep the denominator ﬁxed at a reference
value (or composite reference hypothesis), leading to

ˆr(x|θ0, θ1) =

wc(θ0) ˆrc(x)

(cid:88)

c

where the basis estimators ˆrc(x) = ˆr(x|θc, θ1) only depend on x.

Alternatively, we can decompose both the numerator and denominator distributions to
ﬁnd [34, 80]

ˆr(x|θ0, θ1) =

(cid:34)

(cid:88)

(cid:88)

c

c(cid:48)

wc(cid:48)(θ1)
wc(cid:48)(θ0)

(cid:35)−1

ˆrc(cid:48),c(x)

with pairwise estimators ˆrc(cid:48),c(x) = ˆr(x|θc(cid:48), θc).

One remarkably powerful quantity is the score, deﬁned as the relative tangent vector

2. Score and local model

(cid:12)
(cid:12)
t(x|θ0) = ∇θ log p(x|θ)
(cid:12)θ0

.

It quantiﬁes the relative change of the likelihood under inﬁnitesimal changes in parameter space
and can be seen as a local equivalent of the likelihood ratio.

In a small patch around θ0 in which we can approximate t(x|θ) as independent of θ, Eq. (14) is

solved by the local model

plocal(x|θ) =

p(t(x|θ0)|θ0) exp[t(x|θ0) · (θ − θ0)]

(15)

1
Z(θ)

with a normalisation factor Z(θ). The local model is in the exponential family. Note that the t(x|θ0)
are the suﬃcient statistics for plocal(x|θ). This is signiﬁcant: if we can estimate the vector-valued
function t(x|θ0) (with one component per parameter of interest) of the high-dimensional x, we can
reduce the dimensionality of our space dramatically without losing any information, at least in the
local model approximation [81].

In fact, ignoring the normalization factors and in the local model the likelihood ratio between θ0
and θ1 only depends on the scalar product between the score and θ0 − θ1, which will allow us to
take this dimensionality reduction one step further and compress high-dimensional data x into a
scalar without loss of power.

In our example process, we are interested in the Wilson coeﬃcients of two dimension-six operators.
The score vector therefore has two components. In Fig. 4 we show the relation between these two
score components and two informative kinematic variables, the jet pT and the azimuthal angle
between the two jets, ∆φ. We ﬁnd that the score vector is very closely related with these two
kinematic quantities, but the relation is not quite one-to-one. Larger energy transfer, measured as
larger jet pT , increases the typical size of the score vector. The OW W component of the score is
particularly sensitive to the angular correlation variable, in agreement with detailed studies of this
process [4].

14

(16)

(17)

(18)

Figure 4: Score vector as a function of kinematic observables in our example process. Left: ﬁrst
component of the score vector, representing the relative change of the likelihood with respect to
small changes in OW direction. Right: second component of the score vector, representing the
relative change of the likelihood with respect to small changes in OW W direction. In both panels,
the axes show two important kinematic variables. We ﬁnd that the score vector is clearly correlated
with these two variables.

B. Available information and its usefulness

1. General likelihood-free case

All measurement strategies have in common that the estimator ˆr(x|θ0, θ1) is learned from data
In the most general
provided by Monte-Carlo simulations (the stochastic generative process).
likelihood-free scenario, we can only generate samples of events {xe} with xe ∼ p(x|θ) through the
simulator, and base an estimator ˆr(x|θ0, θ1) on these generated samples.

One strategy [34] is based on training a classiﬁer with decision function ˆs(x) between two
equal-sized samples {xe} ∼ p(x|θ0), labelled ye = 0, and {xe} ∼ p(x|θ1), labelled ye = 1. The
cross-entropy loss functional

L[ˆs] = −

(ye log ˆs(xe) + (1 − ye) log(1 − ˆs(xe)))

1
N

(cid:88)

e

is minimized by the optimal decision function

From the decision function ˆs(x) of a classiﬁer we can therefore extract an estimator for the likelihood
ratio as

s(x|θ0, θ1) =

p(x|θ1)
p(x|θ0) + p(x|θ1)

.

ˆr(x|θ0, θ1) =

1 − ˆs(x|θ0, θ1)
ˆs(x|θ0, θ1)

.

This idea, sometimes called the likelihood ratio trick, is visualized in the left panel of Fig. 5.

As pointed out in Ref. [34], we can use the weaker assumption of any loss functional that is
minimized by a decision function s(x) that is a strictly monotonic function of the likelihood ratio.
The underlying reason is that the likelihood ratio is invariant under any transformation s(x) with
this property. In practice, the output of any such classiﬁer can be brought closer to the form of
Eq. (17) through a calibration procedure, which we will discuss in Sec. III D.

15

Figure 5: Illustration of some key concepts with a one-dimensional Gaussian toy example. Left:
classiﬁers trained to distinguish two sets of events generated from diﬀerent hypotheses (green dots)
converge to an optimal decision function s(x|θ0, θ1) (in red) given in Eq. (17). This lets us extract
the likelihood ratio. Right: regression on the joint likelihood ratios r(xe, ze|θ0, θ1) of the simulated
events (green dots) converges to the likelihood ratio r(x|θ0, θ1) (red line).

2. Particle-physics structure

As we have argued in Sec. II C, particle physics processes have a speciﬁc structure that allow
us to extract additional information. Most processes satisfy the factorization of Eq. (2) with a
tractable parton-level likelihood p(z|θ). The generators do not only provide samples {xe}, but
also the corresponding parton-level momenta (latent variables) {ze} with (xe, ze) ∼ p(x, z|θ0). By
evaluating the matrix elements at the generated momenta ze for diﬀerent hypotheses θ0 and θ1,
we can extract the parton-level likelihood ratio p(ze|θ0)/p(ze|θ1). Since the distribution of x is
conditionally independent of the theory parameters, this is the same as the joint likelihood ratio

r(xe, zall e|θ0, θ1) ≡

p(xe, zdetector e, zshower e, ze|θ0)
p(xe, zdetector e, zshower e, ze|θ1)
p(xe|zdetector e)
p(xe|zdetector e)
p(ze|θ0)
p(ze|θ1)

p(zdetector e|zshower e)
p(zdetector e|zshower e)

.

=

=

p(zshower e|ze)
p(zshower e|ze)

p(ze|θ0)
p(ze|θ1)

(19)

So while we cannot directly evaluate the likelihood ratio at the level of measured observables
r(x|θ0, θ1), we can calculate the likelihood ratio for a generated event conditional on the latent
parton-level momenta.

The same is true for the score, i. e. the tangent vectors or relative change of the (log) likelihood
under inﬁnitesimal changes of the parameters of interest. While the score t(xe|θ0) = ∇θ log p(x|θ)|θ0

16

(20)

(cid:125)

(21)

Figure 6: Illustration of some key concepts with a one-dimensional Gaussian toy example. Left:
probability density functions for diﬀerent values of θ and the scores t(xe, ze|θ) at generated events
(xe, ze). These tangent vectors measure the relative change of the density under inﬁnitesimal
changes of θ. Right: dependence of log p(x|θ) on θ for ﬁxed x = 4. The arrows again show the
(tractable) scores t(xe, ze|θ).

is intractable, we can extract the joint score

t(xe, zall e|θ0) ≡ ∇θ log p(xe, zdetector e, zshower e, ze|θ0)
p(zdetector e|zshower e)
p(zdetector e|zshower e)

=

p(zshower e|ze)
p(zshower e|ze)

∇θp(ze|θ)
p(ze|θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)θ0

p(xe|zdetector e)
p(xe|zdetector e)
∇θp(ze|θ)
p(ze|θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)θ0

=

from the simulator. Again, all intractable parts of the likelihood cancel. We visualize the score
in Fig. 6 and all available information on the generated samples in Fig. 7. It is worth repeating
that we are not making any simplifying approximations about the process here, these statements
are valid with reducible backgrounds, for state-of-the-art generators including higher-order matrix
elements, matching of matrix element and parton shower, and with full detector simulations.

But how does the availability of the joint likelihood ratio r(x, z|θ) and score t(x, z|θ) (which
depend on the latent parton-level momenta z) help us to estimate the likelihood ratio r(x|θ), which
is the one we are interested in?

Consider the L2 squared loss functional for functions ˆg(x) that only depend on x, but which are

trying to approximate a function g(x, z),

L[ˆg(x)] =

dx dz p(x, z|θ) |g(x, z) − ˆg(x)|2

=

dx

ˆg2(x)

dz p(x, z|θ) − 2ˆg(x)

dz p(x, z|θ) g(x, z) +

(cid:90)

(cid:90)

(cid:90)

(cid:21)
dz p(x, z|θ) g2(x, z)

.

(cid:90)

(cid:90)

(cid:20)

(cid:124)

(cid:123)(cid:122)
F (x)

17

Figure 7: Illustration of some key concepts with a one-dimensional Gaussian toy example. Left: full
statistical model log r(x|θ, θ1) that we are trying to estimate. Right: available information at the
generated events (xe, ze). The dots mark the joint likelihood ratios log r(xe, ze|θ0, θ1), the arrows
the scores t(xe, ze|θ0, θ1).

Via calculus of variations we ﬁnd that the function g∗(x) that extremizes L[ˆg] is given by [53]

0 =

= 2ˆg

dz p(x, z|θ)

−2

dz p(x, z|θ) g(x, z) ,

(22)

δF
δˆg

(cid:12)
(cid:12)
(cid:12)
(cid:12)g∗

(cid:90)

(cid:124)

(cid:123)(cid:122)
=p(x|θ)

(cid:125)

(cid:90)

therefore

g∗(x) =

(cid:90)

1
p(x|θ)

dz p(x, z|θ) g(x, z) .

(23)

We can make use of this general property in our problem in two ways. Identifying g(xe, ze) with

the joint likelihood ratios r(xe, zall,e|θ0, θ1) (which we can calculate!) and θ = θ1, we ﬁnd

g∗(x) =

(cid:90)

1
p(x|θ1)

p(x, z|θ0)
p(x, z|θ1)

dz p(x, z|θ1)

= r(x|θ0, θ1) .

(24)

By minimizing the squared loss

L[ˆr(x|θ0, θ1)] =

|r(xe, zall,e|θ0, θ1) − ˆr(xe|θ0, θ1)|2

(25)

1
N

(cid:88)

(xe,ze)∼p(x,z|θ1)

of a suﬃciently expressive function ˆr(x|θ0, θ1), we can therefore regress on the true likelihood
ratio [53]! This is illustrated in the right panel of Fig. 5. Note that to get the correct minimum, the
events (xe, ze) have to be sampled according to the denominator hypothesis θ1.

We can also identify g(xe, ze) in Eq. (22) with the scores t(xe, zall,e|θ), which can also be extracted

from the generator. In this case,

g∗(x) =

(cid:90)

1
p(x|θ)

dz ∇θp(x, z|θ) = t(x|θ) .

(26)

18

General likelihood-free Particle physics

(cid:88)

Quantity

Samples
Likelihood
Likelihood ratio
Score

{xe}
p(xe|θ)
r(xe|θ0, θ1)
t(xe|θ)

Latent state
Joint likelihood
Joint likelihood ratio
Joint score

{xe, ze}
p(xe, zall,e|θ)
r(xe, zall,e|θ0, θ1)
t(xe, zall,e|θ)

(cid:88)

∗
∗

(cid:88)

(cid:88)
(cid:88)

Table II: Availability of diﬀerent quantities from the generative process in the most general
likelihood-free setup vs. in the particle-physics scenario with the structure given in Eq. (2).
Asterisks (∗) denote quantities that are not immediately available, but can be regressed from the
corresponding joint quantity, as shown in Sec. III B.

Thus minimizing

L[ˆt(x|θ)] =

1
N

(cid:88)

(xe,ze)∼p(x,z|θ)

|t(xe, zall,e|θ) − ˆt(xe|θ)|2

(27)

of a suﬃciently expressive function ˆt(x|θ) allows us to regress on the score t(x|θ) [53].3 Now the
(xe, ze) have to be sampled according to θ. We summarize the availability of the (joint) likelihood,
likelihood ratio, and score in the most general likelihood-free setup and in particle physics processes
in Table II.

This is one of our key results and opens the door for powerful new inference methods. Particle
physics processes involve the highly complex eﬀects of parton shower, detector, and reconstruction,
modelled by a generative process with a huge latent space and an intractable likelihood. Still, the
speciﬁc structure of this class of processes allows us to calculate how much more or less likely a
generated event becomes when we move in the parameter space of the theory. We have shown that
by regressing on the joint likelihood ratios or scores extracted in this way, we can recover the actual
likelihood ratio or score as a function of the observables!

C. Strategies

Let us now combine the estimator structure discussed in Sec. III A with the diﬀerent quantities
available during training discussed in Sec. III B and deﬁne our strategies to estimate the likelihood
ratio. Here we restrict ourselves to an overview over the main ideas of the diﬀerent approaches. A
more detailed explanation and technical details can be found in Appendix A 2.

1. General likelihood-free case

Some approaches are designed for the most general likelihood-free scenario and only require the

samples {xe} from the generator:

3 A similar loss function (with a non-standard use of the term “score”) was used in Ref. [82], though the derivative is

taken with respect to x and, critically, the model did not involve marginalization over the latent variable z.

19

(28)

(30)

(31)

(32)

Histograms of observables: The traditional approach to kinematic analyses relies on one or two
kinematic variables v(x), manually chosen for a given process and set of parameters. Densities
ˆp(v(x)|θ) are estimated by ﬁlling histograms with generated samples, leading to the likelihood
ratio

ˆr(x|θ0, θ1) =

ˆp(v(x)|θ0)
ˆp(v(x)|θ1)

.

We use this algorithm point by point in θ0, but a morphing-based setup is also possible (see
Sec. II C 2). We discuss the histogram approach in more detail in Appendix A 2 a.

Approximate Frequentist Computation (Afc): Approximate Bayesian Computation (Abc)
is currently the most widely used method for likelihood-free inference in a Bayesian setup.
It allows to sample parameters from the intractable posterior, θ ∼ p(θ|x) = p(x|θ)p(θ)/p(x).
Essentially, Abc relies on the approximation of the likelihood function through a rejection
probability

prejection(x|θ) = K(cid:15)(v(x), v(xe)) ,

(29)

with xe ∼ p(x|θ), a kernel K(cid:15) that depends on a bandwidth (cid:15), and a suﬃciently low-dimensional
summary statistics v(x).

Inference in particle physics is usually performed in a frequentist setup, so this sampling
mechanism is not immediately useful. But we can deﬁne a frequentist analogue, which we
call “Approximate Frequentist Computation” (Afc). In analogy to the rejection probability
in Eq. 29, we can deﬁne a kernel density estimate for the likelihood function as

The corresponding likelihood ratio estimator is

ˆp(x|θ) =

K(cid:15)(v(x), v(xe)) .

1
N

(cid:88)

e

ˆr(x|θ0, θ1) =

ˆp(x|θ0)
ˆp(x|θ1)

.

We use this approach point by point in θ0 with a ﬁxed reference θ1. As summary statistics, we
use subsets of kinematic variables, similar to the histogram approach. We give more details
in Appendix A 2 b.

Calibrated classiﬁers (Carl4): As discussed in Sec. III B 1, the decision function ˆs(x|θ0, θ1) of
a classiﬁer trained to discriminate between samples generated according to θ0 from θ1 can be
turned into an estimator for the likelihood ratio

ˆr(x|θ0, θ1) =

1 − ˆs(x|θ0, θ1)
ˆs(x|θ0, θ1)

.

This is illustrated in the left panel of Fig. 5.

If the classiﬁer does not learn the optimal decision function of Eq. (17), but any mono-
tonic function of the likelihood ratio, a calibration procedure can improve the performance
signiﬁcantly. We will discuss this in Sec. III D below.

We implement this strategy point by point in θ0, as an agnostic parameterized classiﬁer
ˆr(x|θ0, θ1) that learns the dependence on both x and θ0, as well as a morphing-aware para-
meterized classiﬁer. More details are given in Appendix A 2 c.

4 Calibrated ratios of likelihoods

20

Neural conditional density estimators (Nde): Several other methods for conditional density
estimation have been proposed, often based on neural networks [36–42]. One particularly
interesting class of methods for density estimation is based on the idea of expressing the
target density as a sequence of invertible transformations applied to a simple initial density,
such as a Gaussian [43–46, 51]. The density in the target space is then given by the Jacobian
determinant of the transformation and the base density. A closely related and successful
alternative are neural autoregressive models [47–50], which factorize the target density as a
sequence of simpler conditional densities. Both classes of estimators are trained by maximizing
the log likelihood.

We leave a detailed discussion of these techniques for particle physics problems as well as an
implementation in our example process for future work.

2. Particle-physics structure

As we argued in Sec. III B, particle physics simulations let us extract the joint likelihood ratio
r(xe, ze|θ0, θ1) and the joint score t(xe, ze|θ0, θ1), giving rise to strategies tailored to this class of
problems:

Ratio regression (Rolr5): We can directly regress the likelihood ratio ˆr(x|θ0, θ1). As shown
in the previous section, the squared error loss between a function ˆr(xe|θ0, θ1) and the avail-
able joint likelihood ratio r(xe, ze|θ0, θ1) is minimized by the likelihood ratio r(x|θ0, θ1),
provided that the samples (xe, ze) are drawn according to θ1. Conversely, the squared error
of 1/r(xe, ze|θ0, θ1) with (xe, ze) ∼ p(x, z|θ0) is also minimized by the likelihood ratio. We
can combine these two terms into a combined loss function

L[ˆr(x|θ0, θ1)] =

ye |r(xe, ze|θ0, θ1) − ˆr(x|θ0, θ1)|2

1
N

(cid:32)

(cid:88)

(xe,ze,ye)

+ (1 − ye)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
r(xe, ze|θ0, θ1)

−

1
ˆr(x|θ0, θ1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2(cid:33)

(33)

with ye = 0 for events generated according to (xe, ze) ∼ p(x, z|θ0) and ye = 1 for (xe, ze) ∼
p(x, z|θ1). The factors of ye and (1 − ye) ensure the correct sampling for each part of the loss
functional. We illustrate this approach in the right panel of Fig. 5.

This strategy is again implemented point by point in θ0, in an agnostic parameterized setup,
as well as in a morphing-aware parameterized setup. We describe it in more detail in
Appendix A 2 d.

Carl + score regression (Cascal6): The parameterized Carl strategy outlined above learns
a classiﬁer decision function ˆs(x|θ0, θ1) as a function of θ0. If the classiﬁer is realized with
a diﬀerentiable architecture such as a neural network, we can calculate the gradient of this
function and of the corresponding estimator for the likelihood ratio ˆr(x|θ0, θ1) with respect
to θ0 and derive the estimated score

(cid:12)
(cid:12)
ˆt(x|θ0) = ∇θ log ˆr(x|θ, θ1)
(cid:12)
(cid:12)
(cid:12)θ0

= ∇θ log

1 − ˆs(x|θ0, θ1)
ˆs(x|θ0, θ1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)θ0

.

(34)

5 Regression on likelihood ratio
6 CARL and score approximate likelihood ratio

21

If the estimator is perfect, we expect this estimated score to minimize the squared error
with respect to the joint score data available from the simulator, following the arguments in
Sec. III B.

We can turn this argument around and use the available score data during the training.
Instead of training the classiﬁer just by minimizing the cross-entropy, we can instead sim-
ultaneously minimize the squared error on this derived score with respect to the true joint
score t(x, z|θ0, θ1). The combined loss function is given by

(cid:88)

(cid:34)
ye log ˆs(xe)+(1−ye) log(1− ˆs(xe))+α (1−ye) (cid:12)

(cid:12)t(xe, ze|θ0) − ˆt(xe|θ0)(cid:12)
2
(cid:12)

(35)

(cid:35)

L[ˆs] =

1
N

e

with ˆt(x|θ0) deﬁned in Eq. (34) and a hyperparameter α that weights the two pieces of
the loss function relative to each other. Again, ye = 0 for events generated according to
(xe, ze) ∼ p(x, z|θ0) and ye = 1 for (xe, ze) ∼ p(x, z|θ1), and the factors of ye and (1 − ye)
ensure the correct sampling for each part of the loss functional.

This strategy relies on the parameterized modeling of the likelihood ratio. We implement
both an agnostic version as well as a morphing-aware model. See Appendix A 2 e for more
details.

Neural conditional density estimators + score (Scandal7): In the same spirit as the Cas-
cal method, neural density estimators such as autoregressive ﬂows can be augmented with
score information. We have started to explore this class of algorithms in Ref. [53], but leave
a detailed study and the application to particle physics for future work.

Ratio + score regression (Rascal8): The same trick works for the parameterized Rolr ap-
If the regressor is implemented as a diﬀerentiable architecture such as a neural
proach.
network, we can calculate the gradient of the parameterized estimator ˆr(x|θ0, θ1) with respect
to θ0 and calculate the score

(cid:12)
(cid:12)
ˆt(x|θ0) = ∇θ log ˆr(x|θ, θ1)
(cid:12)
(cid:12)
(cid:12)θ0

.

(36)

Instead of training just on the squared likelihood ratio error, we can minimize the combined
loss

L[ˆr(x|θ0, θ1)] =

ye |r(xe, ze|θ0, θ1) − ˆr(xe|θ0, θ1)|2

1
N

(cid:34)

(cid:88)

(xe,ze,ye)

+ (1 − ye)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
r(xe, ze|θ0, θ1)

−

1
ˆr(xe|θ0, θ1)

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)

(cid:35)

+ α (1 − ye) (cid:12)

(cid:12)t(xe, ze|θ0) − ˆt(xe|θ0)(cid:12)
2
(cid:12)

(37)

with ˆt(x|θ0) deﬁned in Eq. (36) and a hyperparameter α. The likelihood ratios and scores
again provide complementary information as shown in the Fig. 7.

7 Score and neural density approximate likelihood
8 Ratio and score approximate likelihood ratio

22

Once more we experiment with both an agnostic parameterized model as well as a morphing-
aware version.

This technique uses all the available data from the simulator that we discussed in Sec. III B to
train an estimator of particularly high ﬁdelity. It is essentially a machine-learning version of
the Matrix Element Method. It replaces computationally expensive numerical integrals with
an upfront regression phase, after which the likelihood ratio can be evaluated very eﬃciently.
Instead of manually specifying simpliﬁed smearing functions, the eﬀect of parton shower and
detector is learned from full simulations. For more details on Rascal, see Appendix A 2 f.

Local score regression and density estimation (Sally9): In the local model approximation
discussed in Sec. III A 2, the score evaluated at some reference point θscore is the suﬃcient
statistics, carrying all the information on θ. A precisely estimated score vector (with one
component per parameter of interest) is therefore the ideal summary statistics, at least in the
neighborhood of the Standard Model or any other reference parameter point.

In the last section we argued that we can extract the joint score t(xe, ze|θscore) from the
simulator. We showed that the squared error between a function ˆt(x|θscore) and the joint
score is minimized by the intractable score t(x|θscore), as long as the events are sampled as
(xe, ze) ∼ p(x, z|θscore). We can thus use the augmented data to train an estimator ˆt(x|θscore)
for the score at the reference point.
In a second step, we can then estimate the likelihood ˆp(ˆt(x|θscore)|θ) with histograms, KDE,
or any other density estimation technique, yielding the likelihood ratio estimator

ˆr(x|θ0, θ1) =

ˆp (cid:0)ˆt(x|θscore) (cid:12)
ˆp (cid:0)ˆt(x|θscore) (cid:12)

(cid:12) θ0
(cid:12) θ1

(cid:1)
(cid:1) .

(38)

This particularly straightforward strategy is a machine-learning analogue of Optimal Ob-
servables that learns the eﬀect of parton shower and detector from data. After an upfront
regression phase, the analysis of an event only requires the evaluation of one estimator to
draw conclusions about all parameters. See Appendix A 2 g for more details.

Local score regression, compression to scalar, and density estimation (Sallino10): The
Sally technique compresses the information in a high-dimensional vector of observables x
into a lower-dimensional estimated score vector. But for measurements in high-dimensional
parameter spaces, density estimation in the estimated score space might still be computation-
ally expensive. Fortunately, the local model of Eq. (15) motivates an even more dramatic
dimensionality reduction to one dimension, independent of the number of parameters: Disreg-
arding the normalization constants, the ratio r(x|θ0, θ1) only depends on the scalar product
between the score and θ0 − θ1.
Given the same score estimator ˆt(x|θscore) developed for the Sally method, we can deﬁne
the scalar function

ˆh(x|θ0, θ1) ≡ ˆt(x|θSM ) · (θ0 − θ1) .

(39)

Assuming a precisely trained score estimator, this scalar encapsulates all information on the
likelihood ratio between θ0 and θ1, at least in the local model approximation. The likelihood

9 Score approximates likelihood locally
10 Score approximates likelihood locally in one dimension

23

Estimator versions

Loss function

Asymptotically exact

CE ML Ratio Score

Strategy

Histograms
Afc
Carl
Nde

Rolr
Cascal
Scandal
Rascal
Sally
Sallino

(cid:88)
(cid:88)
(cid:88)
((cid:88))
(cid:88)

PbP Param Aware
((cid:88))
((cid:88))
(cid:88)
((cid:88))
(cid:88)
(cid:88)
((cid:88))
(cid:88)
((cid:88))
((cid:88))

(cid:88)
((cid:88))
(cid:88)
(cid:88)
((cid:88))
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

Table III: Overview over the discussed measurement strategies. The ﬁrst three techniques can be
applied in the general likelihood-free setup, they only require sets of generated samples {xe}. The
remaining ﬁve methods are tailored to the particle physics structure and require the availability of
r(xe, ze|θ0, θ1) or t(xe, ze|θ0) from the generator, as discussed in Sec. III B. Brackets denote possible
variations that we have not implemented for our example process. In the Sally and Sallino
strategies, “estimator versions” refers to the density estimation step. In the loss function columns,
“CE” stands for the cross-entropy, “ML” for maximum likelihood, “ratio” for losses of the type
|r(x, z) − ˆr(x)|2, and “score” for terms such as |t(x, z) − ˆt(x)|2.

ratio can then be estimated as

ˆr(x|θ0, θ1) =

(cid:16)ˆh(x|θ0, θ1)
(cid:16)ˆh(x|θ0, θ1)

ˆp

ˆp

(cid:17)

(cid:17) ,

(cid:12)
(cid:12)
(cid:12) θ0
(cid:12)
(cid:12)
(cid:12) θ1

(40)

where the ˆp(ˆh) are simple univariate density estimators.

This method allows us to condense any high-dimensional observation x into a scalar function
without losing sensitivity, at least in the local model approximation. It thus scales exceptionally
well to problems with many theory parameters. We describe Sallino in more detail in
Appendix A 2 h.

Several other variants are possible, including other combinations of the loss functionals discussed

above. We leave this for future work.11

We summarize the diﬀerent techniques in Table III. With this plethora of well-motivated analysis
methods, the remaining key question for this paper is how well they do in practice, and which of
them (if any) should be used. This will be the focus of Sec. V. In the next sections, we will ﬁrst
discuss some important additional aspects of these methods.

11 Not all initially promising strategies work in practice. We experimented with an alternative strategy based on the
morphing structure in Eq. (6). Consider the case in which the training sample consists of a number of sub-samples,
each generated according to a morphing basis point θc. Then the morphing basis sample c used in the event
generation is a latent variable that we can use instead of the parton-level momenta z to deﬁne joint likelihood
ratios and joint scores. Regressing on these quantities should converge to the true likelihood ratio and score, in
complete analogy to the discussion in Sec. III B. But the joint ratios and scores deﬁned in this way span a huge
range, and the densities of the diﬀerent basis points are very similar. The convergence is therefore extremely slow
and the results based on this method suﬀer from a huge variance.

D. Calibration

While the likelihood ratio estimators described above work well in many cases, their performance
can be further improved by an additional calibration step. Calibration takes place after the “raw”
or uncalibrated estimators ˆrraw(x|θ0, θ1) have been trained. In general, it deﬁnes a function C with
the aim that ˆrcal = C(ˆrraw) provides a better estimator of the true likelihood ratio than ˆrraw. We
consider two diﬀerent approaches to deﬁning this function C, which we call probability calibration
and expectation calibration.

1. Probability calibration

Consider the Carl strategy, which trains a classiﬁer with a decision function s(x) as the basis
for the likelihood ratio estimation. Even if this classiﬁer can separate the two classes of events from
θ0 and θ1 well, its decision function ˆs(x|θ0, θ1) might not have a direct probabilistic interpretation:
it might be any approximately monotonic function of the likelihood ratio rather than the ideal
solution given in Eq. (17). In this case, the Carl approach requires calibration, an additional
transformation of the raw output ˆrraw = ˆr(x|θ0, θ1) into a calibrated decision function

where the densities ˆp(ˆrraw|θ) are estimated through a univariate density estimation technique such as
histograms. This calibration procedure does not only apply to classiﬁers, but to any other likelihood
ratio estimation strategy.

ˆrcal = C(ˆrraw) =

ˆp(ˆrraw|θ0)
ˆp(ˆrraw|θ1)

,

2. Expectation calibration

Consider some likelihood ratio r(x|θ0, θ1). The expectation value of this ratio assuming θ1 to be

true is given by

E[r(x|θ0, θ1)|θ1] =

dx p(x|θ1)

(cid:90)

p(x|θ0)
p(x|θ1)

= 1 .

A good estimator for the likelihood ratio should reproduce this property. We can numerically
approximate this expectation value by evaluating ˆr(x|θ, θ1) on a sample {xe} of N events drawn
according to θ1,

If a likelihood ratio estimator ˆrraw(x|θ, θ1) (which might be entirely uncalibrated or already

probability calibrated) does not satisfy this condition, we can calibrate it by rescaling it as

For a perfect estimator with ˆr(x|θ0, θ1) = r(x|θ0, θ1), we can even calculate the variance of the

numeric calculation of the expectation value in Eq. (43). We ﬁnd

ˆR(θ) =

1
N

(cid:88)

xe∼θ1

ˆr(xe|θ, θ1) ≈ 1 .

ˆrcal(x|θ, θ1) =

ˆrraw(x|θ, θ1)
ˆRraw(θ)

.

var[ ˆR(θ)] =

[E [ˆr(x|θ, θ1)|θ] − 1] ,

1
N

24

(41)

(42)

(43)

(44)

(45)

where N is the number of events used to calculate the expectation value R(θ), and the expectation
E [ˆr(x|θ, θ1)|θ] (under the numerator hypothesis!) can be calculated numerically.

This calibration strategy can easily improve classiﬁers that are oﬀ by some θ-dependent factor.
However, a few rare events xe with large ˆr(xe|θ, θ1) can dominate the expectation value. If these are
mis-estimated, the expectation calibration can actually degrade the performance of the estimator
on the bulk of the distribution with smaller ˆr(x|θ, θ1).

25

E.

Implementation

1. Neural networks

With the exception of the simple histogram and AFC methods, all strategies rely on a classiﬁer
ˆs(x|θ0, θ1), score regressor ˆt(x|θ0), or ratio regressor ˆr(x|θ0, θ1) that is being learnt from training
data. For our explicit example, we implement these functions as fully connected neural networks:
• In the point-by-point setup, the neural networks take the features x as input and models
log ˆr(x|θ0, θ1). For ratio regression, this is exponentiated to yield the ﬁnal output ˆr(x|θ0, θ1).
In the Carl strategy, the network output is transformed to a decision function

ˆs(x|θ0, θ1) =

1
1 + ˆr(x|θ0, θ1)

.

(46)

• In the agnostic parameterized setup, the neural networks take both the features x as well
as the numerator parameter θ0 as input and model log ˆr(x|θ0, θ1). In addition to the same
subsequent transformations as in the point-by-point case, taking the gradient of the network
output gives the estimator score.

• In the morphing-aware setup, the estimator takes both x and θ0 as input. The features x
are fed into a number of independent networks, one for each basis component c, that model
the basis ratios log rc(x). From θ0 the estimator calculates the component weights wc(θ0)
analytically. The components are then combined with Eq. (12). For the Carl approaches,
the output is again transformed with Eq. (46), and for the score-based strategies the gradient
of the output is calculated.
We visualize these architectures in Fig. 8.

All networks are implemented in shallow, regular, and deep versions with 2, 3, and 5 hidden layers
of 100 units each and tanh activation functions. They are trained with the Adam optimizer [83]
over 50 epochs with early stopping and learning rate decay. We implement them in keras [84] with
a TensorFlow [85] backend. Experiments modeling s rather than log r, with diﬀerent activation
functions, adding dropout layers, or using other optimizers and learning rate schedules have led to
a worse performance.

2. Training samples

Starting from the weighted event sample described in Sec. II D 2, we draw events (xe, ze) randomly
with probabilities given by the corresponding p(xe, ze|θ). Due to the form of the likelihood p(x, z|θ)
and due to technical limitations of our simulator, individual data points can carry large probabilities
p(xe, ze|θ), leading to duplicate events in the training samples. However, we enforce that there is
no duplication between training and evaluation samples, so this limitation can only degrade the
performance.

For the point-by-point setup, we choose 100 values of θ0, 5 of which are ﬁxed at the SM
(θ0 = (0, 0)) and at the corners of the considered parameter space, with the remaining 95 chosen

x

θ0

x

θ0

x

26

log ˆr

for each θ0

ˆs

ˆr

ˆt

ˆs

ˆr

ˆt

ˆs

ˆr

log ˆr

wc

log ˆr

log ˆrc

for each c

Figure 8: Schematic neural network architectures for point-by-point (top), agnostic parameterized
(middle), and morphing-aware parameterized (bottom) estimators. Solid lines denote dependencies
with learnable weights, dashed lines show ﬁxed functional dependencies.

27

(47)

randomly with a ﬂat prior over −1 ≤ θo ≤ 1. For each of these training points we sample 250 000
events according to θ0 and 250 000 events according to the reference hypothesis

θ1 = (0.393, 0.492)T .

For the parameterized strategies, we compare three diﬀerent training samples, each consisting of
107 events:

Baseline: For 1000 values of θ0 chosen randomly in θ space, we draw 5000 events according to θ0

and 5000 events according to θ1 given in Eq. (47).

Random θ: In this sample, the value of θ0 is drawn randomly independently for each event. Again

we use a ﬂat prior over θ0 ∈ [−1, 1]2.

Morphing basis: For each of the 15 basis hypotheses θi from the morphing procedure, we generate

333 000 events according to θ0 = θi and 333 000 according to θ1.

Finally, for the local score regression model we use a sample of 107 events drawn according to

the SM.

Our evaluation sample consists of 50 000 events drawn according to the SM. We evaluate the
likelihood ratio for these events for a total of 1016 values of θ0, 1000 of which are the same as those
used in the baseline training sample. Again we ﬁx θ1 as in Equation (47).

Each event is characterized by 42 features:
• the energies, transverse momenta, azimuthal angles, and pseudo-rapidities of all six particles

• the energies, transverse momenta, azimuthal angles, pseudo-rapidities, and invariant mass of
the four-lepton system as well as the two-lepton systems that reconstruct the two Z bosons;
and

• the invariant mass, separation in pseudorapidity, and separation in azimuthal angle of the

in the ﬁnal state;

di-jet system.

The derived variables in the feature set help the neural networks pick up the relevant features faster,
though we did not ﬁnd that their choice aﬀects the performance signiﬁcantly.

3. Calibration and density estimation

We calibrate the classiﬁers for our example process with probability calibration as described
in Sec. III D 1. We determine the calibration function C(r) with isotonic regression [86], which
constrains C(r) to be monotonic. Experiments with other regression techniques based on histograms,
kernel density estimation, and logistic regression did not lead to a better performance. We apply
the calibration point by point in θ0. It is based on an additional event sample that is independent
of the training and evaluation data. The same events are used to calibrate each value of θ, with
an appropriate reweighting. This strategy to minimize variance is based on the availability of the
parton-level likelihood function p(z|θ).

The techniques based on local score regression require the choice of a reference point to evaluate
the score. For the EFT problem, the natural choice is θscore = θSM = (0, 0)T . In the Sally approach,
we perform the density estimation based on two-dimensional histograms of the estimated score at
the SM, point by point in θ0. For the Sallino technique, we use a one-dimensional histograms of
ˆh(x|θSM), point by point in θ0.

28

Figure 9: Uncertainty ∆ log ˆr(xe|θ, θ1) of morphing-aware estimators due to uncertainties
∆ log ˆrc(xe|θ, θ1) on the individual basis ratios as a function of θ. We ﬁx θ1 as in Eq. (47) and show
one random event xe, the results for other events are very similar. We assume iid Gaussian
uncertainties on the log ˆrc(x|θ, θ1) and use Gaussian error propagation. The white dots show the
position of the basis points θc. Small uncertainties in the individual basis estimators ˆrc(xe|θ, θ1) are
signiﬁcantly increased due to the large morphing weights and can lead to large errors of the
combined estimator ˆr(xe|θ, θ1).

F. Challenges and diagnostics

1. Uncertainties

Even the most evolved and robust estimators will have some deviations from the true likelihood
ratio, which should be taken into account in an analysis as an additional modeling uncertainty.
Most of the estimators developed above converge to the true likelihood ratio in the limit of inﬁnite
training and calibration samples. But with ﬁnite statistics, there are diﬀerent sources of variance
that aﬀect some strategies more than others.

Consider the traditional histogram approach.

In the point-by-point version, each separate
estimator ˆr(x|θ0, θ1) is trained on the small subset of the data generated from a speciﬁc value of
θ0, so the variance from the ﬁnite size of the training data, i. e. the statistical uncertainty from the
Monte-Carlo simulation, is large. At θ0 values between the training points, there are additional
sources of uncertainty from the interpolation. On the other hand, morphing-aware histograms use
all of the training data to make predictions at all points, and since the dependence on θ0 is known,
the interpolation is exact. But the large morphing weights wc(θ0) and the cancellations between
them mean that even small ﬂuctuations in the individual basis histograms can lead to huge errors
on the combined estimator.

Similar patterns hold for the ML-based inference strategies. The point-by-point versions suﬀer
from a larger variance due to small training samples at each point and interpolation uncertainties.
The agnostic parameterized models have more statistics available, but have to learn the more
complex full statistical model including the dependence on θ0. The morphing-aware versions make
maximal use of the physics structure of the process and all the training data, but large morphing
weights can dramatically increase the errors of the individual component estimators log ˆrc(x). We
demonstrate this for our example process in Fig. 9: in some regions of parameter space, in particular
far away from the basis points, the errors on a morphing-aware estimator log ˆr(x|θ, θ1) can easily

29

be 100 times larger than the individual errors on the component estimators log ˆrc(x). The θ0
dependence on this error depends on the choice of the basis points, this uncertainty can thus be
somewhat mitigated by optimizing the basis points or by combining several diﬀerent bases.

2. Diagnostics

After discussing the sources of variance, let us now turn towards diagnostic tools that can help
quantify the size of estimator errors and to assign a modeling uncertainty for the statistical analysis.
These methods are generally closure tests: we can check the likelihood ratio estimators for some
expected behaviour, and use deviations either to correct the results (as in the calibration procedures
described in Sec. III D), to deﬁne uncertainty bands, or to discard estimators altogether. We suggest
the following tests:

Ensemble variance: Repeatedly generating new training data (or bootstrapping the same training
sample) and training the estimators again gives us an ensemble of predictions {ˆrr(x|θ0, θ1)}.
We can use the ensemble variance as a measure of uncertainty of the prediction that is due to
the variance in the training data and random seeds used during the training.

Reference hypothesis variation: Any estimated likelihood ratio between two hypotheses θA, θB

ˆr(x|θA, θB) =

ˆr(x|θA, θ1)
ˆr(x|θB, θ1)

(48)

should be independent of the choice of the reference hypothesis θ1 used in the estimator ˆr.
Training several independent estimators with diﬀerent values of θ1 thus provides another
check of the stability of the results [34].

Much like the renormalization and factorization scale variations that are ubiquitous in particle
physics calculations, this technique does not have a proper statistical interpretation in terms
of a likelihood function. We can still use it to qualitatively indicate the stability of the
estimator under this hyperparameter change.

Ratio expectation: As discussed above, the expectation value of the estimated likelihood ratio
assuming the denominator hypothesis should be very close to one. We can numerically
calculate this expectation value ˆR(θ), see Eq. (43).
In Sec. III D 2 we argued that this
expectation value can be used to calibrate the estimators, but that this calibration can
actually decrease the performance in certain situations.

If expectation calibration is used, the calibration itself has a non-zero variance from the ﬁnite
sample size used to calculate the expectation value ˆR(θ). As we pointed out in Sec. III D 2,
we can calculate this source of statistical uncertainty, at least under the assumption of a
perfect estimator with ˆr(x|θ0, θ1) = r(x|θ0, θ1). The result given in Eq. (45) provides us with
a handle to calculate the statistical uncertainty of this calibration procedure from the ﬁnite
size of the calibration sample. Note that for imperfect estimators, the variance of R[ ˆR] may
be signiﬁcantly larger.

Independent of whether expectation calibration is part of the estimator, the deviation of the
expectation ˆR(θ) from one can serve as a diagnostic tool to check for mismodelling of the
estimator. We can take log ˆR(θ) as a measure of the uncertainty of log ˆr(x|θ, θ1). As in the
case of the reference hypothesis variation, there is no consistent statistical interpretation of
this uncertainty, but this does not mean that it is useless as a closure test.

30

(49)

Reweighting distributions: A good estimator ˆr(x|θ0, θ1) should satisfy

p(x|θ0) ≈ ˆr(x|θ0, θ1) p(x|θ1) .

We cannot evaluate the p(x|θ0) to check this relation explicitly. However, we can sample
events {xe} from them. This provides another diagnostic tool [34]: we can draw a ﬁrst sample
as xe ∼ p(xe|θ0), and draw a second sample as xe ∼ p(xe|θ1) and reweight it with ˆr(xe|θ0, θ1).
For a good likelihood ratio estimator, the two samples should have similar distributions. This
can easily be tested by training a discriminative classiﬁer between the samples. If a classiﬁer
can distinguish between the sample from the ﬁrst hypothesis and the sample drawn from
the second hypothesis and reweighted with the estimated likelihood ratio, then ˆr(x|θ0, θ1) is
not a good approximation of the true likelihood ratio r(x|θ0, θ1). Conversely, if the classiﬁer
cannot separate the two classes, the classiﬁer is either not eﬃcient, or the likelihood ratio is
estimated well.

Note that passing these closure tests is not a guarantee for a good estimator of the likelihood ratio.
In Sec. IV B we will discuss how we can nevertheless derive exclusion limits that are guaranteed to
be statistically correct, i. e. that might not be optimal, but are never wrong.

In our example process, we will use a combination of the ﬁrst two ideas of this list: we will create
copies of estimators with independent training samples and random seeds during training, as well
as with diﬀerent choices of the reference hypothesis θ1, and analyse the median and envelope of the
predictions.

IV. LIMIT SETTING

The ﬁnal objective of any EFT analysis are exclusion limits on the parameters of interest at a
given conﬁdence level. These can be derived in one of two ways. The Neyman construction based
on toy experiments provides a generic and fail-safe method, we will discuss it in Sec. IV B. But since
the techniques developed in the previous section directly provide an estimate for the likelihood ratio,
we can alternatively apply existing statistical methods for likelihood ratios as test statistics. This
much more eﬃcient approach will be the topic of the following section.

A. Asymptotics

Consider the test statistics

q(θ) = −2

log r(xe|θ, ˆθ) = −2

log r(xe|θ, θ1) − log r(xe|ˆθ, θ1)

(50)

(cid:17)

(cid:88)

e

(cid:88)

(cid:16)

e

for a ﬁxed number N of observed events {xe} with the maximum-likelihood estimator

ˆθ = arg max

θ

(cid:88)

e

log r(xe|θ, θ1) .

(51)

In the asymptotic limit, the distribution according to the null hypothesis, p(q(θ)|θ), is given by a
chi-squared distribution. The number of degrees of freedom k is equal to the number of parameters
θ. This result by Wilks [87] allows us to translate an observed value qobs(θ) directly to a p-value
that measures the conﬁdence with which θ can be excluded:

pθ ≡

dq p(q|θ) = 1 − Fχ2 (qobs(θ)|k)

(52)

(cid:90) ∞

qobs(θ)

31

(53)

(54)

where Fχ2(x|k) is the cumulative distribution function of the chi-squared distribution with k degrees
of freedom. In our example process k = 2, for which this simpliﬁes to

In particle physics it is common practice to calculate “expected exclusion contours” by calculating
the expected value of qobs(θ) based on a large “Asimov” data set generated according to some θ(cid:48) [88].
With Eq. (52) this value is then translated into an expected p-value.12

In practice we cannot access the true likelihood ratio deﬁned on the full observable space and
thus also not q(θ). But if the error of an estimator ˆr(x|θ0, θ1) compared to the true likelihood ratio
is negligible, we can simply calculate

pθ = exp

−

(cid:18)

(cid:19)

.

qobs(θ)
2

ˆq(θ) = −2

log ˆr(xe|θ, ˆθ)

(cid:88)

e

with maximum likelihood estimator ˆθ also based on the estimated likelihood ratio. The p-value can
then be read oﬀ directly from the estimator output, substituting ˆq for q in Eq. (52).

Under this assumption and in the asymptotic limit, constructing conﬁdence intervals is thus
remarkably simple and computationally cheap: after training an estimator ˆr(x|θ, θ1) as discussed
in the previous section, the observed events {xe} are fed into the estimator for each value of θ on
some parameter grid. From the results we can read oﬀ the maximum likelihood estimator ˆq(θ) and
calculate the observed value of the test statistics ˆq(θ) for each θ. Equation (52) then translates
these values to p-values, which can then be interpolated between the tested θ points to yield the
ﬁnal contours.

To check whether these asymptotic properties apply to a likelihood ratio estimator, we can
use the diagnostic tools discussed in Sec. III F 2. In addition, we can explicitly check whether the
distribution of q(θ) actually follows a chi-squared distribution by generating toy experiments for
a few θ points. If it does, the asymptotic results are likely to apply at other points in parameter
space as well. If the variance of the toy experiments is larger than expected from the chi-squared
distribution, the residual variance may be taken as an error estimate on the estimator prediction.

B. Neyman construction

Rather than relying on the asymptotic properties of the likelihood ratio test, we can construct
the distribution of a test statistic with toy experiments. This is computationally more expensive,
but useful if the number of events is not in the asymptotic regime or if the uncertainty of the
estimators cannot be reliably quantiﬁed. Constraints derived in this way are conservative: even if
the likelihood ratio is estimated poorly, the resulting contours might not be optimal, but they are
never wrong (at a speciﬁed conﬁdence level).

12 For k = 1, this standard procedure reproduces the median expected p-value. Note however that this is not true
anymore for more than one parameter of interest. In this case, the median expected p-value can be calculated
based on a diﬀerent, but not commonly used, procedure. It is based on the fact that the distribution of q according
to an alternate hypothesis, p(q(θ)|θ(cid:48)), is given by a non-central chi-squared distribution [89]. In the asymptotic
limit, the non-centrality parameter is equal to the expectation value E[q(θ)|θ(cid:48)] [88]. This allows us to calculate for
instance the median expected q(θ) assuming some value θ(cid:48) based on the Asimov data. Combining all the pieces,
the median expected p-value pθ with which θ can be excluded under the assumption that θ(cid:48) is true is then given by
pexpected from θ(cid:48)
2 |k, E[q(θ)|θ(cid:48)])|k), where Fχ2 (x|k) is the cumulative distribution function for the
= 1 − Fχ2 (F −1
( 1
χ2
θ
chi-squared distribution with k degrees of freedom and F −1
(p|k, Λ) is the inverse cumulative distribution function
χ2
for the non-central chi-squared distribution with k degrees of freedom and non-centrality parameter Λ.

nc

nc

A good choice for the test statistics is the estimated proﬁle log likelihood ratio ˆq(θ) given
in Eq. (54), which allows us to compare the distribution of the toy experiments directly to the
asymptotic properties discussed in the previous section. However, its construction requires ﬁnding the
maximum likelihood estimator for every toy experiment. This increases the necessary computation
time substantially, especially in high-dimensional parameter spaces. An alternative test statistics is
the estimated log likelihood ratio with respect to some ﬁxed hypothesis, which need not be identical
to the reference denominator θ1 used in the likelihood ratio estimators. In the EFT approach, the
natural choice is the estimated likelihood ratio with respect to the SM,

ˆq(cid:48)(θ) ≡ −2

log ˆr(xe|θ, θSM ) .

(cid:88)

e

Using this test statistic rather than the proﬁle likelihood ratio deﬁned in Eq. (50) is expected to
lead to stronger constraints if the true value of θ is close to the SM point, as expected in the EFT
approach, and less powerful bounds if the true value is substantially diﬀerent from the SM.

In practice we can eﬃciently calculate the distribution of ˆq((cid:48))(θ) after n events by ﬁrst calculating

the distribution of ˆq((cid:48))(θ) for one event and convolving the result with itself (n − 1) times.

C. Nuisance parameters

The tools developed above also support nuisance parameters, for instance to model systematic
uncertainties in the theory calculation or the detector model. One strategy is to train parameterized
estimators on samples generated with diﬀerent values of the nuisance parameters ν and let them
learn the likelihood ratio

with its dependence on the nuisance parameters. As test statistics we can then use the estimator
version of the usual proﬁle log likelihood ratio,

ˆr(x|θ0, θ1; ν0, ν1) ≡

ˆp(x|θ0; ν0)
ˆp(x|θ1; ν1)

ˆq(θ) = −2

log

r

xe



(cid:16)

(cid:12)
(cid:12)θ, ˆθ; ˆˆν, ˆν
(cid:12)

(cid:17) q(ˆˆν)

q(ˆν)





(cid:88)

e

with constraint terms q(ν),

(cid:20)
ˆr(xe|θ, θ1; ν, ν1)

log

and

ˆˆν = arg max

ν

(cid:88)

e
(cid:88)

(cid:20)

(θ,ν)

e

(cid:17)

(cid:16)ˆθ, ˆν

= arg max

log

ˆr(xe|θ, θ1; ν, ν1)

(cid:21)

(cid:21)

q(ν)
q(ν1)

q(ν)
q(ν1)

,

.

The proﬁle log likelihood ratio has two advantages: it is pivotal, i. e. its value and its distribution do
not depend on the value of the nuisance parameter, and it has the asymptotic properties discussed
in Sec. IV A.

Similarly, we can train the score including nuisance parameters,

(cid:12)
(cid:12)
ˆt(x|θ0; ν0) = ∇(θ,ν) log [ˆp(x|θ; ν)q(ν)]
(cid:12)
(cid:12)
(cid:12)θ0,ν0

.

32

(55)

(56)

(57)

(58)

(59)

(60)

33

(61)

(62)

If the constraints q(ν) limit the nuisance parameters to a relatively small region around some ν0,
i. e. a range in which the shape of the likelihood function does not change signiﬁcantly, the Sally
and Sallino methods seem particularly appropriate.

Finally, an adversarial component in the training procedure lets us directly train pivotal estimators
ˆr(x|θ0, θ1), i. e. that do not depend on the value of the nuisance parameters [90]. Compared to
learning the explicit dependence on ν, this can dramatically reduce the dimensionality of the
parameter space as early as possible, and does not require manual proﬁling. However, the estimators
will generally not converge to the proﬁle likelihood ratio, so its asymptotic properties do not apply
and limit setting requires the Neyman construction.

V. RESULTS

We now apply the analysis techniques to our example process of WBF Higgs production in the
4(cid:96) decay mode. We ﬁrst study the idealized setup discussed in Sec. II D 2, in which we can assess
the techniques by comparing their predictions to the true likelihood ratio. In Sec. V B we then
calculate limits in a more realistic setup.

A.

Idealized setup

1. Quality of likelihood ratio estimators

Table IV summarizes the performance of the diﬀerent likelihood ratio estimators in the idealized
setup. For 50 000 events {xe} drawn according to the SM, we evaluate the true likelihood ratio
r(xe|θ0, θ1) as well as the estimated likelihood ratios ˆr(xe|θ0, θ1) for 1000 values of θ0 sampled
randomly in [−1, 1]2. As a metric we use the expected mean squared error on the log likelihood
ratio

ε[ˆr(x)] =

π(θ0)

log ˆr(xe|θ0, θ1) − log r(xe|θ0, θ1)

(cid:17)2(cid:21)

.

(cid:88)

θ0

(cid:20)(cid:16)

(cid:88)

1
N

e

The 1000 tested values of θ0 are weighted with a Gaussian prior

π(θ) =

N(cid:0)||θ||(cid:12)

(cid:12)0, 2 · 0.22(cid:1)

1
Z

with normalization factor Z such that (cid:80)
π(θ) = 1. In addition we show the expected trimmed
mean squared error, which truncates the top 5% and bottom 5% of events for each θ. This allows
us to analyse the quality of the estimators for the bulk of the phase space without being dominated
by a few outliers. In Table IV and in the ﬁgures of this section, we only show results for a default
set of hyperparameters for each likelihood ratio estimators. These default setups are deﬁned in
Appendix A 2. Results for other hyperparameter choices are given in Appendix A 3.

θ0

The best results come from parameterized estimators that combine either a classiﬁer decision
function or ratio regression with regression on the score: the Cascal and Rascal strategies provide
very accurate estimates of the log likelihood ratio. Sally, parameterized Rolr, and parameterized
Carl perform somewhat worse. For Carl and Rolr, parameterized estimators consistently perform
better then the corresponding point-by-point versions. All these ML-based strategies signiﬁcantly
outperform the traditional one- or two-dimensional histograms and the Approximate Frequentist
Computation.

The morphing-aware versions of the parameterized estimators lead to a poor performance,
comparable or worse than the two-dimensional histogram approach. As anticipated in Sec. III F,

34

Expected MSE

All Trimmed

Figures

(cid:88)

Strategy

Setup

pT,j1, ∆φjj
pT,j1
∆φjj
pT,j1, ∆φjj
pT,j1, mZ2, mjj, ∆ηjj, ∆φjj

Histogram

Afc

Carl (PbP)
Carl (parameterized)

Carl (morphing-aware)

Rolr (PbP)
Rolr (parameterized)

Rolr (morphing-aware)

Sally
Sallino

Cascal (parameterized)

Cascal (morphing-aware) Baseline

Rascal (parameterized)

Rascal (morphing-aware) Baseline

PbP
Baseline
Random θ
Baseline
Random θ
Morphing basis

PbP
Baseline
Random θ
Baseline
Random θ
Morphing basis

Baseline
Random θ

Random θ
Morphing basis

Baseline
Random θ

Random θ
Morphing basis

(cid:88)

0.0111 Fig. 12
0.0026
0.0028
0.0200 Fig. 12
0.0226
0.0618

0.056
0.088
0.160
0.059
0.078

0.030
0.012
0.012
0.076
0.086
0.156

0.005
0.003
0.003
0.024
0.022
0.130

0.013
0.021

0.001
0.001
0.136
0.092
0.040

0.001
0.001
0.125
0.132
0.031

0.0106
0.0230
0.0433
0.0091
0.0101

0.0022
0.0017
0.0014
0.0063
0.0052
0.0485

0.0002
0.0006

0.0002
0.0002
0.0427
0.0268
0.0081

0.0004
0.0004
0.0514
0.0539
0.0072

(cid:88)

(cid:88)

(cid:88)

(cid:88)

Table IV: Performance of the diﬀerent likelihood ratio estimation techniques in our example process.
The metrics shown are the expected mean squared error on the log likelihood ratio with and
without trimming, as deﬁned in the text. Checkmarks in the last column denotes estimators shown
in the following ﬁgures. Here we only give results based on default settings, which are deﬁned in
Appendix A 2. An extended list of results that covers more estimators is given in Appendix A 3.

the large weight factors and the sizable cancellations between them blow up small errors on the
estimation of the individual basis estimators ˆri(x) to large errors on the combined estimator.

We ﬁnd that probability calibration as discussed in Sec. III D 1 improves the results in almost
all cases, in particular for the Carl method. An additional step of expectation calibration (see
Sec. III D 2) after the probability calibration does not lead to a further improvement, and in fact often
increases the variance of the estimator predictions. We therefore only use probability calibration for
the results presented here.

The choice of the training sample is less critical, with nearly identical results between the baseline
and random θ samples. For the Carl approach, shallow networks with two hidden layers perform
better, while Rolr works best for three hidden layers and the score-based strategies beneﬁt from a
deeper network with ﬁve hidden layers.

35

Figure 10: True vs. estimated likelihood ratios for a benchmark hypothesis θ0 = (−0.5, −0.5)T .
Each dot corresponds to one event xe. The Cascal (right, red), Rascal (right, orange), and
Sally (middle, blue) techniques can predict the likelihood ratio extremely accurately over the
whole phase space. All new techniques clearly lead to more precise estimates than the traditional
histogram approach (left, orange).

Figure 11: True vs. estimated expected log likelihood ratio. Each dot corresponds to one value of
θ0, where we take the expectation over x ∼ p(x|θSM). The new techniques are less biased than the
histogram approach (left, orange).

In Fig. 10 we show scatter plots between the true and estimated likelihood ratios for a ﬁxed
hypothesis θ0. The likelihood ratio estimate from histograms of observables is widely spread around
the true likelihood ratio, reﬂecting the loss of information from ignoring most directions in the
observable space. Carl performs clearly better. Rolr and Sally oﬀer a further improvement.
Again, the best results come from the Cascal and Rascal strategies, both giving predictions that
are virtually one-to-one with the true likelihood ratio.

We go beyond a single benchmark point θ0 in Fig. 11. This scatter plot compares true and
estimated likelihood ratios for diﬀerent values of θ0, taking the expectation value over x. We
ﬁnd that the Carl, Rolr, Cascal, and Rascal approaches converge to the correct likelihood
ratio in this expectation value. For the Sally and Sallino techniques we ﬁnd larger deviations,
pointing towards the breakdown of the local model approximation. Much more obvious is the loss

36

Figure 12: Comparison of the point-by-point, parameterized, and morphing-aware versions of
Carl. Top left: True vs. estimated likelihood ratios for a benchmark hypothesis
θ0 = (−0.5, −0.5)T , as in Fig. 10. Each dot corresponds to one event xe. Top right: True vs.
estimated expected log likelihood ratio, as in Fig. 11. Each dot corresponds to one value of θ0,
where we take the expectation over x ∼ p(x|θSM). The parameterized estimator outperforms the
point-by-point one and particularly the morphing-aware version.

of information in the traditional histogram approach, which is clearly not asymptotically exact.

The point-by-point, agnostic parameterized, and morphing-aware versions of the Carl strategy
are compared in Fig. 12. As expected from Table IV, the parameterized strategy performs better
than the point-by-point version, and both are clearly superior to the morphing-aware estimator.

2. Eﬃciency and speed

With inﬁnite training data, many of the algorithms should converge to the true likelihood ratio.
But generating training samples can be expensive, especially when a full detector simulation is
used. An important question is therefore how much training data the diﬀerent techniques require
to perform well. In Fig. 13 we show the performance as a function of the training sample size.

The Sally approach performs very well even with very little data. Its precision stops improving
eventually, showing the limitations of the local model approximation. For the other methods we ﬁnd
that the more information a technique uses, the less training data points it requires. The Rascal
technique utilizes the most information from the simulator, leading to an exceptional performance
with training samples of approximately 100 000 events. This is in contrast to the most general Carl
method, which does not use any of the extra information from the simulator and requires a two
orders of magnitude larger training sample for a comparable performance.

In Fig. 14 we show the evolution of the likelihood estimation error and the cross-entropy of the
classiﬁcation problem during the training of the parameterized estimators. For comparison, we also
show the optimal metrics based on the true likelihood ratio, and the results of the two-dimensional
histogram approach. Once again we see that either Cascal or Rascal leads to the best results.
This result also holds true for the cross entropy, hinting that the techniques we use to measure
continuous parameters might also improve the power of estimators in discrete classiﬁcation problems.
Note that the Carl approach is more prone to overﬁtting than the others, visible as a signiﬁcant

37

Figure 13: Performance of the techniques as a function of the training sample size. As a metric, we
show the mean squared error (left) and trimmed mean squared error on log r(r|θ0, θ1) weighted with
a Gaussian prior, as discussed in the text. Note that we do not vary the size of the calibration data
samples. The number of epochs are increased such that the number of epochs times the training
sample size is constant, all other hyperparameters are kept constant. The Sally method works
well even with very little data, but plateaus eventually due to the limitations of the local model
approximation. The other algorithms learn faster the more information from the simulator is used.

Evaluation time [µs]

per xe

per xe and θ0

Algorithm

Histogram
Carl
Sally
Rolr
Cascal
Rascal

25.4

0.2
19.7
0.1
19.7
25.1
21.7

Table V: Computation times of evaluating ˆr(x|θ0, θ1) in the diﬀerent algorithms. We distinguish
between steps that have to be calculated once per x and and those which have to be repeated for
every evaluated value of θ0. These numbers are from one run of our algorithms with default
settings on the NYU HPC cluster on machines equipped with Intel Xeon E5-2690v4 2.6GHz CPUs
and NVIDIA P40 GPUs with 24 GB RAM, using a batch of 50 000 events {xe}, and taking the
mean over 1 017 values of θ0. The local score regression method and the traditional histogram
method are particularly fast. But all techniques are many orders of magnitude faster to evaluate
than the matrix element method or optimal observables.

38

diﬀerence between the metrics evaluated on the training and validation samples.

Equally important to the training eﬃciency is the computation time taken up by evaluating
the likelihood ratio estimators ˆr(xe|θ0, θ1). We compare example evaluation times in Table V. The
traditional histogram approach takes the shortest time. But all tested algorithms are very fast:
the likelihood ratio for ﬁxed hypotheses (θ0, θ1) for 50 000 events {xe} can always be estimated
in around one second or less. The local score regression method is particularly eﬃcient, since the
estimator ˆt(x|θscore, θ1) has to be evaluated only once to estimate the likelihood ratio for any value
of θ0. Only the comparably fast step of density estimation has to be repeated for each tested value
of θ0.

So after investing some training time upfront, all the measurement strategies developed here
can be evaluated on any events with very little computational cost and amortize quickly. While
this is not the focus of our paper, note that this distinguishes our approaches from the Matrix
Element Method and Optimal Observable techniques. These well-established methods require the
computationally expensive evaluation of complicated numerical integrals for every evaluation of the
likelihood ratio estimator.

3. Physics results

The most important result of an EFT measurement are observed and expected exclusion contours,
either based on asymptotics or toy experiments. In the asymptotic approach, the expected contours
are determined just by the likelihood ratio evaluated on a large “Asimov” data set, as described in
Sec. IV A. Figure 15 shows this expected log likelihood ratio in the SM after 36 events over a one-
dimensional slice of the parameter space. In Fig. 16 we show the corresponding expected exclusion
limits on the two Wilson coeﬃcients. To estimate the robustness of the likelihood ratio estimators,
each algorithm is run ﬁve times with diﬀerent choices of the reference hypothesis; independent
training, calibration, and evaluation samples; and independent random seeds during training. The
lines show the median of the ﬁve replicas, while the shaded bands show the envelope. While this
error band does not have a clear statistic interpretation, it does provide a diagnostic tool for the
variance of the estimators.

A traditional histogram-based analysis of jet pT and ∆φjj leads to overly conservative results.
It is interesting to note that this simple analysis works reasonably well in the region of parameter
space with fW > 0 and fW W > 0, which is exactly the part of parameter space where informative
high-energy events interfere mostly constructively with the SM amplitude. In the fW < 0 region of
parameter space, destructive interference dominates in the important regions of phase space with
large momentum transfer. An extreme example is the “amplitude-through-zero” eﬀect shown in the
left panel of Fig. 2. Simple histograms with a rough binning generally lead to a poor estimation of
the likelihood ratio in such complicated kinematic signatures.

We ﬁnd that the new ML-based strategies allow us to place visibly tighter constraints on the
Wilson coeﬃcients than the doubly diﬀerential histogram. In particular the Carl + score and
regression + score estimators lead to exclusion contours that are close to the contours based on
the true likelihood ratio. In this analysis based on asymptotics, however, it is possible for the
estimated contours to be slightly too tight, wrongly marking parameter regions as excluded at a
given conﬁdence level. This problem can be mitigated by proﬁling over systematic uncertainties
assigned to the likelihood ratio estimates.

Exclusion limits based on the Neyman construction do not suﬀer from this issue: contours
derived in this way might be not optimal, but they are never wrong. We generate toy experiments
to estimate the distribution of the likelihood ratio with respect to the SM for individual events.
Repeatedly convolving this single-event distribution with itself, we ﬁnd the distribution of the

39

Figure 14: Learning curve of the parameterized models. The solid lines show the metrics evaluated
on the training sample, the dots indicate the performance on the validation sample. Note that
these numbers are not comparable to the metrics in Table IV and Fig. 13, which are weighted with
the prior in Eq. (62). These results also do not include calibration. Left: Mean squared error of
log ˆr(x|θ0, θ1). Right: binary cross-entropy of the classiﬁcation based on ˆs(x|θ0, θ1) between the
numerator and denominator samples. The solid grey line shows the “optimal” performance based
on the true likelihood ratio. The Cascal and Rascal techniques converge to a performance close
to the theoretic optimum. The Carl approach (green), based on minimizing the cross entropy,
shows signs of overﬁtting. All machine-learning-based methods outperform traditional histograms
(dashed orange).

Figure 15: Expected likelihood ratio with respect to the Standard Model along a one-dimensional
slice of the parameter space. We assume 36 observed events and the SM to be true. For each
estimator, we generate ﬁve sets of predictions with diﬀerent reference hypotheses, independent data
samples, and diﬀerent random seeds. The lines show the median of this ensemble, the shaded error
bands the envelope. All machine-learning-based methods reproduce the true likelihood function
well, while the doubly diﬀerential histogram method underestimates the likelihood ratio in the
region of negative Wilson coeﬃcients.

40

Figure 16: Expected exclusion contours based on asymptotics at 68% CL (innermost lines), 95% CL,
and 99.7% CL (outermost lines). We assume 36 observed events and the SM to be true. As test
statistics, we use the proﬁle likelihood ratio with respect to the maximum-likelihood estimator. For
each estimator, we generate ﬁve sets of predictions with diﬀerent reference hypotheses, independent
data samples, and diﬀerent random seeds. The lines show the median of this ensemble, the shaded
error bands the envelope. The new techniques based on machine learning, in particular the Cascal
and Rascal techniques, lead to expected exclusion contours very close to those based on the true
likelihood ratio. An analysis of a doubly diﬀerential histogram leads to much weaker bounds.

Figure 17: Expected exclusion contours based on the Neyman construction with toy experiments at
68% CL (innermost lines), 95% CL, and 99.7% CL (outermost lines). We assume 36 observed
events and the SM to be true. As test statistics, we use the likelihood ratio with respect to the SM.
All machine-learning-based methods let us impose much tighter bounds on the Wilson coeﬃcients
than the traditional histogram approach (left, dotted orange). The Neyman construction
guarantees statistically correct results: no contour based on estimators excludes parameter points
that should not be excluded. The expected limits based on the Cascal (right, lavender) and
Rascal (right, red) techniques are virtually indistinguishable from the true likelihood contours.

41

Figure 18: Ratio of the estimated likelihood ratio ˆr(x|θ0, θ1) to the joint likelihood ratio
r(x, z|θ0, θ1), which is conditional on the parton-level momenta and other latent variables. As a
benchmark hypothesis we use θ0 = (−0.5, −0.5)T , the events are drawn according to the SM. The
spread common to all methods shows the eﬀect of the smearing on the likelihood ratio. The
additional spread in the histogram, Carl, and Rolr methods is due to a poorer performance of
these techniques.

likelihood ratio after 36 observed events.

The expected corresponding expected exclusion limits are shown in Fig. 17. Indeed, errors in
the likelihood ratio estimation never lead to undercoverage, i. e. the exclusion of points that should
not be excluded based on the true likelihood ratio. Again we ﬁnd that histograms of kinematic
observables only allow us to place rather weak bounds on the Wilson coeﬃcients. Sally performs
clearly better, with excellent performance close to the SM. Deviations from the optimal bounds
become visible at the 2σ level, hinting at the breakdown of the local model approximation there.
The best results come once more from the Cascal and Rascal methods. Both of these strategies
yield exclusion bounds that are virtually indistinguishable from those based on the true likelihood
ratio.

As a side note, a comparison of the expected contours based on asymptotics to those based on
the Neyman construction shows the Neyman results to be tighter. This reﬂects the diﬀerent test
statistics used in the two ﬁgures: in the asymptotics case, we use the proﬁle likelihood ratio with
respect to the maximum likelihood estimator, which itself ﬂuctuates around the true value of θ
(which in our case is assumed to be the SM). In the Neyman construction we use the likelihood
ratio with respect to the SM, leading to tighter contours if the true value is in fact close to the SM,
and weaker constraints if it is very diﬀerent.

B. Detector eﬀects

We have now established that the measurement strategy work very well in an idealized setup,
where we can compare them to the true likelihood ratio. In a next step, we turn towards a setup
with a rudimentary smearing function that models the eﬀect of the parton shower and the detector
response on the observables. In this setting, the true likelihood is intractable, so we cannot use it
as a baseline to validate the predictions any more. But we can still discuss the relative ordering of
the exclusion contours predicted by the diﬀerent estimators.

Figure 18 shows the relation between the true joint likelihood ratio r(x, z|θ0, θ1), which is

42

Figure 19: Expected exclusion contours based on the Neyman construction with toy experiments at
68% CL, 95% CL, and 99.7% CL with smearing. We assume 36 observed events and the SM to be
true. As test statistics, we use the likelihood ratio with respect to the SM. In the setup with
smearing we cannot these results to the true likelihood contours. But since the Neyman
construction is guaranteed to cover, these expected limits are correct. The new techniques, in
particular Cascal (right, dashed red) and Rascal (right, dash-dotted orange), allow us to set
much tighter bounds on the Wilson coeﬃcients than a traditional histogram analysis (left, dotted
orange).

conditional on the parton-level momenta z and other latent variables, to the estimated likelihood
ratio ˆr(x|θ0, θ1), which only depends on the observables x. We see that this relation is stochastically
smeared out around 1. Recall that in the idealized scenario the best estimators described the true
likelihood ratio perfectly, as shown in Fig. 10. This strongly suggests that the spread visible here
is not due to errors of the likelihood ratio estimators, but rather shows the diﬀerence between the
joint and true likelihood ratios, as illustrated in Fig. 5.

In Fig. 19 we show the expected exclusion contours based on the Neyman construction, which
guarantees statistically correct results. The conclusions from the idealized setting are conﬁrmed: a
measurement based on the likelihood ratio estimators leads to robust bounds that are clearly more
powerful than those based on a histogram. Once again, the Cascal and Rascal algorithms lead
to the strongest limits.

VI. CONCLUSIONS

We have developed and analysed a suite of new analysis techniques for measurements of continuous
parameters in LHC experiments based on simulations and machine learning. Exploiting the structure
of particle physics processes, they extract additional information from the event generators, and use
this information to train precise estimators for likelihood ratios.

Our approach is designed for problems with large numbers of observables, where the likelihood
function is not tractable and traditional methods based on individual kinematic variables often

43

perform poorly. It scales well to high-dimensional parameter spaces such as that of eﬀective ﬁeld
theories. The new methods do not require any approximations on the hard process, parton shower,
or detector eﬀects, and the likelihood ratio for any event and hypothesis pair can be evaluated
in microseconds. These two properties set it apart from the Matrix Element Method or Optimal
Observables, which rely on crude approximations for the shower and detector and require the
evaluation of typically very expensive integrals.

Using Higgs production in weak boson fusion in the four-lepton mode as a speciﬁc example
process, we have evaluated the performance of the diﬀerent methods and compared them to a
classical analysis of the jet momentum and azimuthal angle between the tagging jets. We ﬁnd that
the new algorithms provide very precise estimates of arbitrary likelihood ratios. Using them as a
test statistics allows us to impose signiﬁcantly tighter constraints on the EFT coeﬃcients than the
traditional kinematic histograms.

Out of the several methods introduced and discussed in this paper, two stand out. The ﬁrst,

which we call Sally, is designed for parameter regions close to the Standard Model:

1. As training data, the algorithm requires a sample of fully simulated events, each accompanied
by the corresponding joint score at the SM: the relative change of the parton-level likelihood
function of the parton-level momenta associated with this event under small changes of the
theory parameters away from the SM. This can be calculated by evaluating the squared matrix
element at the same phase-space points for diﬀerent theory parameters. We can thus extract
this quantity from Monte-Carlo generators such as MadGraph.

2. Regressing on this data, we train an estimator (for instance realized as a neural network) that
takes as input an observation and returns the score at the SM. This function compresses the
high-dimensional observable space into a vector with as many components as parameters of
interest. If the parameter space is high-dimensional, this can be even further compressed into
the scalar product between the score vector and the diﬀerence between two parameter points.

3. The estimated score (or the scalar product between score and parameter diﬀerence) can then
be treated like any set of observables in a traditional analysis. We can ﬁll histograms of this
quantity for diﬀerent hypotheses, and calculate likelihood ratios from them.

There are two key ideas that underlie this strategy. First, note that the training data only
consists of the joint score, which depends on the parton-level four-momenta of an event. But during
the training the estimator converges to the actual score of the distribution of the observables, i. e. the
relative change of the actual likelihood function under inﬁnitesimal changes of the parameters. We
have proven this powerful, yet surprisingly simple relation in this paper.

Second, close to the Standard Model (or any other reference parameter point), the score provides
it encapsulates all information on the local approximation of the stat-
the suﬃcient statistics:
istical model. In other words, if the score is estimated well, the dimensionality reduction from
high-dimensional observables into a low-dimensional vector does not lose any information on the
parameters. The estimated score is a machine-learning version of the Optimal Observable idea,
but requires neither approximations of the parton shower or detector treatment nor numerically
expensive integrals.

As a matter of fact, the dimensionality reduction can be taken one step further. We have
introduced the Sallino technique that compresses the estimated score vector to a single scalar
function, again without loss of power in the local approximation, and independent of the number of
theory parameters.

In our example process, these simple and robust analysis strategies work remarkably well,
especially close to the Standard Model. Deviations appear at the 2σ level, but even there it

44

allows for much stronger constraints than a traditional analysis of kinematic variables. It requires
signiﬁcantly less data to train than the other discussed methods. Since the Sallino method can
compress any observation into a single number without losing much sensitivity, even for hundreds
of theory parameters, this approach scales exceptionally well to high-dimensional parameter spaces,
as in the case of the SMEFT.

The second algorithm we want to highlight here is the Rascal technique. Using even more
information available from the simulator, it learns a parameterized likelihood ratio estimator: one
function that takes both the observation and a theory parameter point as input and returns an
estimate for the likelihood ratio between this point and a reference hypothesis given the observation.
This estimator is constructed as follows:

1. Training this parameterized estimator requires data for many diﬀerent values of the tested
parameter point (the numerator in the likelihood ratio). For simplicity, the reference hypo-
thesis (the denominator in the likelihood ratio) can be kept ﬁxed. For each of these hypothesis
pairs, event samples are generated according to the numerator and denominator hypothesis.
In addition, we extract the joint likelihood ratio from the simulator: essentially the squared
matrix element according to the numerator theory parameters divided by the squared matrix
element according to the denominator hypothesis, evaluated at the generated parton-level
momenta. Again, we also need the joint score, i. e. the relative change of the parton-level
likelihood function under inﬁnitesimal changes of the theory parameters. Both quantities can
be extracted from matrix element codes.

2. A neural network models the estimated likelihood ratio as a function of both the observables
and the value of the theory parameters (of the numerator in the likelihood ratio). We can
calculate the gradient of the network output with respect to the theory parameter and thus
also the estimated score. The network is trained by minimizing the squared error of the
likelihood ratio plus the squared error of the score, in both cases with respect to the joint
quantities extracted from the simulator.

3. After the training phase, the likelihood ratio can optionally be calibrated, for instance through

isotonic regression.

This technique relies on a similar trick as the local score regression method: the likelihood
ratio learned during the training converges to the true likelihood ratio, even though the joint ratio
information in the training data is conditional on the parton-level momenta. The Rascal method
is among the best-performing methods of all analysed techniques. It requires signiﬁcantly smaller
training samples than all other approaches, with the exception of Sally and Sallino. Expected
exclusion limits derived in this way are virtually indistinguishable from those based on the true
likelihood ratio.

On top of these two approaches, we have developed, analysed, and compared several other
methods. We refer the reader to the main part and the appendices of this document, where all these
algorithms are discussed in depth.

All tools developed here are suitable for large-scale LHC analyses. On the software side, only few
modiﬁcations of existing tools are necessary. Most importantly, matrix-element generators should
provide a user-friendly interface to calculate the squared matrix element for a given conﬁguration
of four-momenta and a given set of physics parameters. With such an interface, one could easily
calculate the joint score and joint likelihood ratio data that is needed for the new algorithms. The
training of the estimators is then straightforward, in particular for the Sally and Sallino methods.
The limit setting follows established procedures, either based on the Neyman construction with toy
experiments, or (since the tools provide direct estimates for the likelihood ratio) using asymptotic
formulae.

45

While we have focussed on the example of eﬀective ﬁeld theory measurements, these techniques
equally apply to other measurements of continuous parameter in collider experiments as well as
to a large class of problems outside of particle physics [53]. Some of the techniques can also be
applied to improve the training of machine-learning-based classiﬁers. Finally, while we restricted
our analysis to frequentist conﬁdence intervals, as is common in particle physics, the same ideas can
be used in a Bayesian setting.

All in all, we have presented a range of new inference techniques based on machine learning,
which exploit the structure of particle physics processes to augment training data. They scale well
to large-scale LHC analyses with many observables and high-dimensional parameter spaces. They
do not require any approximations of the hard process, parton shower, or detector eﬀects, and the
likelihood ratio can be evaluated in microseconds. In an example analysis, these new techniques
have demonstrated the potential to substantially improve the precision and new physics reach of
the LHC legacy results.

Acknowledgments

We would like to thank Cyril Becot and Lukas Heinrich, who contributed to this project at an
early stage. We are grateful to Felix Kling, Tilman Plehn, and Peter Schichtel for providing the
MadMax code and helping us use it. KC wants to thank CP3 at UC Louvain for their hospitality.
Finally, we would like to thank Atılım Güneş Baydin, Lydia Brenner, Joan Bruna, Kyunghyun
Cho, Michael Gill, Ian Goodfellow, Daniela Huppenkothen, Hugo Larochelle, Yann LeCun, Fabio
Maltoni, Jean-Michel Marin, Iain Murray, George Papamakarios, Duccio Pappadopulo, Dennis
Prangle, Rajesh Ranganath, Dustin Tran, Rost Verkerke, Wouter Verkerke, Max Welling, and
Richard Wilkinson for interesting discussions.

JB, KC, and GL are grateful for the support of the Moore-Sloan data science environment at
NYU. KC and GL were supported through the NSF grants ACI-1450310 and PHY-1505463. JP
was partially supported by the Scientiﬁc and Technological Center of Valparaíso (CCTVal) under
Fondecyt grant BASAL FB0821. This work was supported in part through the NYU IT High
Performance Computing resources, services, and staﬀ expertise.

Appendix A: Appendix

1. Simpliﬁed detector description

While most of our results are based on an idealized perfect measurement of parton-level momenta,
we also consider a toy smearing representing the eﬀect of parton shower and the detector. The total
smearing function is given by

p(x|z) =

p(cid:96)(x(cid:96)|z(cid:96))

pj(xj|zj) .

(cid:89)

(cid:89)

(cid:96)∈leptons

j∈jets

(A1)

Lepton momenta x(cid:96) = ( ˆE, ˆpT , ˆη, ˆφ) are smeared by

p(cid:96)( ˆE, ˆpT , ˆη, ˆφ|E, pT , η, φ) = N(cid:0)ˆpT

(cid:12)
(cid:12)pT , (3 · 10−4GeV−1p2

T )2(cid:1)

· δ( ˆE − E0(ˆpT , ˆη; m(cid:96))) δ(ˆη − η) δ( ˆφ − φ) ,

(A2)

46

Strategy

NN layers

α Calibration / density estimation

Histogram
AFC
Carl (PbP)
Carl (parameterized)
Carl (morphing-aware)
Sally
Sallino

Rolr (PbP)
Rolr (parameterized)
Rolr (morphing-aware)
Cascal (parameterized)
Cascal (morphing-aware)
Rascal (parameterized)
Rascal (morphing-aware)

Histogram
Gaussian KDE

Isotonic probability calibration
Isotonic probability calibration
Isotonic probability calibration

Histogram
Histogram

Isotonic probability calibration
Isotonic probability calibration
Isotonic probability calibration

5
5

Isotonic probability calibration
Isotonic probability calibration

100
100

Isotonic probability calibration
Isotonic probability calibration

3
2
2

5
5

3
3
2

5
2

5
2

Table VI: Default settings for the analysis techniques. The neural network (NN) layers each have
100 units with tanh activation functions. The hyperparameter α multiplies the score squared error
in the combined loss functions of Eq. (35) and (37).

while the distribution of the jet properties depending on the quark momenta is given by

pj( ˆE, ˆpT , ˆη, ˆφ|E, pT , η, φ) =

(cid:32)

(cid:16) ˆE
N

(cid:12)
(cid:12)
(cid:12)a0 + a1

√

E + a2E, (b0 + b1

√

E + b2E)2(cid:17)

+ cN

(cid:16) ˆE

(cid:12)
(cid:12)
(cid:12)d0 + d1

√

E + d2E, (e0 + e1

√

E + e2E)2(cid:17)

(cid:33)

· δ(ˆpT − pT 0( ˆE, ˆη; m(cid:96))) N(cid:0)ˆη(cid:12)

(cid:12)η, 0.12(cid:1) N

(cid:12)
(cid:16) ˆφ
(cid:12)

(cid:12)φ, 0.12(cid:17)

.

(A3)

Here N(cid:0)x(cid:12)
(cid:12)µ, σ2(cid:1) is the Gaussian distribution with mean µ and variance σ2. The jet energy resolution
parameters ai, bi, c, di, and ei are based on the default settings of the jet transfer function in
MadWeight [91]. The functions E0(pT , η, m) and pT 0(E, η, m) refer to the energy and transverse
momentum corresponding to an on-shell particle with mass m.

2. Model almanac

In Sec. III A we developed diﬀerent estimators for the likelihood ratio, focussing on the key ideas
over technical details. Here we ﬁll in the gaps, explain all strategies in a self-contained way, and
document the settings we use for our example process. To facilitate their comparison, we describe
all models in terms of a “training” and an “evaluation” part, even if this language is not typically
used e. g. for histograms.

a. Histograms of observables

Idea: Most collider measurements are based on the number of events or the cross section of a
process in a given phase-space region or on the diﬀerential cross section or distribution of

47

Figure 20: Example distributions to illustrate the doubly diﬀerential histogram analysis (top), the
Sally technique (middle), and the Cascal method (bottom). The left panels show the diﬀerent
spaces in which the densities and their ratios are estimated. On the right we show the
corresponding distributions of the estimated ratio ˆr (solid) and compare them to the true likelihood
ratio distributions (dotted). We use the benchmark θ0 = (−0.5, −0.5)T (blue) and θ1 as in Eq. (47)
(orange).

48

one or at most a few kinematic observables v. Typical choices are the reconstructed energies,
momenta, angles, or invariant masses of particles. Choosing the right set of observables
for a given measurement problem is all but trivial, but many processes have been studied
extensively in the literature. Once this choice is made, this strategy is simple, fast, and
intuitive. We illustrate the information used by this approach in the top panels of Fig. 20.

Requirements: The histogram approach can be used in the general likelihood-free setting: it only

requires a simulator that can generate samples {x} ∼ p(x|θ).

Structure: Histograms are most commonly used point by point in θ.

If the problem has the
morphing structure discussed in Sec. II C 2, they can also be applied in a morphing-aware
parameterized way (we have not implemented this for our example process).

Training: After generating samples for both the numerator and denominator hypotheses, the values
of the chosen kinematic variables v(x) are extracted, and binned into separate histograms for
the two hypotheses.

Calibration: With suﬃcient training data, histograms should be well calibrated, so we do not

experiment with an additional calibration stage.

Evaluation: To estimate the likelihood ratio between two hypotheses θ0 and θ1 for a given set of
observables x, one has to extract the kinematic variables v(x) and look up the corresponding
bin contents in the histograms for θ0 and θ1. Assuming equal binning for both histograms,
the likelihood ratio is simply estimated as the ratio of bin contents.

Parameters: The only parameters of this approach are the choices of kinematic observables and

the histogram binning.

In our example process, we consider six diﬀerent variants:

• A one-dimensional histogram of the transverse momentum pT,j1 of the leading (higher-pT )

• A one-dimensional histogram of the absolute value of the azimuthal angle ∆φjj between

jet with 80 bins.

the two jets with 20 bins.

direction and 5 bins along ∆φjj.

direction and 10 bins along ∆φjj.

direction and 15 bins along ∆φjj.

direction and 20 bins along ∆φjj.

• A “coarse” two-dimensional histogram of these two variables with 10 bins in the pT,j1

• A “medium” two-dimensional histogram of these two variables with 20 bins in the pT,j1

• A “ﬁne” two-dimensional histogram of these two variables with 30 bins in the pT,j1

• A “very ﬁne” two-dimensional histogram of these two variables with 50 bins in the pT,j1

• An “asymmetric” two-dimensional histogram of these two variables with 50 bins in the

pT,j1 direction and 5 bins along ∆φjj.

For each pair (θ0, θ1) and each observable, the bin edges are chosen such that the same
expected number of events according to θ0 plus the expected number of events according to
θ1 is the same in each bin.

49

b. Approximate Frequentist Computation ( Afc)

Idea: Approximate Bayesian Computation is a very common technique for likelihood-free inference
in a Bayesian setup. In its simplest form it keeps samples according to the rejection probability
of Eq. (29). This amounts to an approximation of the likelihood function through kernel
density estimation, which we can isolate from the Abc sampling mechanism and use in
a frequentist setting. We call it Approximate Frequentist Computation (AFC) to stress
the relation to Abc. Just as Abc or the histogram approach, it requires the choice of a
summary statistics v(x), which in our example process we take to be a two-dimensional or
ﬁve-dimensional subset of the kinematic variables.

Requirements: AFC can be used in the general likelihood-free setting: it only requires a simulator

that can generate samples {x} ∼ p(x|θ).

Structure: We use AFC point by point in θ. If the problem has the morphing structure discussed

in Sec. II C 2, it can also be applied in a morphing-aware parameterized way.

Training: For each event in the numerator and denominator training samples, the summary

statistics v(x) are calculated and saved.

Calibration: AFC can be calibrated as any other technique on this list, but we left this for future

work.

Evaluation: The summary statistics v(x) is extracted from the observation. For numerator and
denominator hypothesis separately, the likelihood at this point is estimated with Eq. (30). The
likelihood ratio estimate is then simply given by the ratio between the estimated numerator
and denominator densities.

Parameters: Just as for histograms, the choice of the summary statistics is the most important
parameter. The performance of AFC also crucially depends on the kernel and bandwidth ε.
Too small values for the bandwidth make large training samples necessary, too large values
lead to an oversmoothening and loss of information.

In our example process, we consider two diﬀerent variants:

• A two-dimensional summary statistics space of the leading jet pT and ∆φjj (see above).

Both variables are rescaled to zero mean and unit variance.

• A ﬁve-dimensional summary statistics space of the leading jet pT , ∆φjj, the dijet
invariant mass mjj, the separation in pseudorapidity between the jets ∆ηjj, and the
invariant mass of the lighter (oﬀ-shell) reconstructed Z boson mZ2. All variables are
rescaled to zero mean and unit variance.

We use Gaussian kernels with bandwidths between 0.01 and 0.5.

c. Calibrated classiﬁers ( Carl)

Idea: Carl was developed in Ref. [34]. The authors showed that the likelihood ratio is invariant
under any transformation that is monotonic with the likelihood ratio.
In practice, this
means that we can train a classiﬁer between two samples generated from the numerator and
denominator hypotheses and turn the classiﬁer decision function ˆs(x) into an estimator for
the likelihood ratio ˆr(x). This relation between ˆs(x) and ˆr(x) can follow the ideal relation in

50

Eqs. (18) and (46). But even if this relation does not hold, we can still extract a likelihood
ratio estimator from the classiﬁer output through probability calibration.

Requirements: Carl can be used in the general likelihood-free setting: it only requires a simulator

that can generate samples {x} ∼ p(x|θ).

Structure: Carl can be used either point by point, in an agnostic parameterized version, or (if
the morphing condition in Eq. (6) holds) in a morphing-aware version. Figure 8 illustrates
the structure of the estimator in these three cases.

Training: A classiﬁer with decision function ˆs(x|θ0, θ1) is trained to discriminate between numerator
(label y = 0) and denominator (label y = 1) samples by minimizing the binary cross-entropy
given in Eq. (16) (other loss functions are possible, but we have not experimented with them).

In the point-by-point version, the inputs to the classiﬁers are just the observables x, and
the events in the numerator sample are generated according to one speciﬁc value θ0. In the
parameterized versions of the estimator, the numerator training samples do not come from a
single parameter θ0, but rather a combination of many diﬀerent subsamples. In the agnostic
parameterized setup, the value of θ0 used in each event is then one of the inputs to the neural
network. In the morphing-aware versions, it is used to calculate the weights wc(θ0) that
multiply the diﬀerent component networks ˆrc(x), as visualized in the bottom panel of Fig. 8.

Calibration: In a next step, the classiﬁer output is optionally calibrated as discussed in Sec. III D 1
using isotonic regression. The calibration curve is shown in the left panel of Fig. 21. We have
also experimented with an additional step of expectation calibration, see Sec. III D 2.

Evaluation: For a given x (and in the parameterized versions θ0), the classiﬁer decision function
ˆs(x|θ0, θ1) is evaluated. This is turned into a likelihood ratio estimator with the relation given
in Eq. (18), and optionally calibrated.

Parameters: The parameters of this approach are the architecture of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms.

For our example process we consider fully connected neural networks with two (“shallow”),
three, or ﬁve (“deep”) layers of 100 neurons each and tanh activation functions. They are
trained with the Adam optimizer [83] over 50 epochs with early stopping and learning rate
decay. Our default settings are given in Table VI. Experiments with diﬀerent architectures,
other activation functions, additional dropout layers, other optimizers, and diﬀerent learning
rate schedules yielded a worse performance.

d. Ratio regression ( Rolr)

Idea: Particle physics generators do not only provide sets of observables {xe}, but also the cor-
responding parton-level momenta {ze}. From matrix element codes such as MadGraph we
can extract the squared matrix element |M|2(z|θ) given parton-level momenta z and theory
parameter points θ. This allows us to calculate the joint likelihood ratio

r(xe, ze|θ0, θ1) =

p(ze|θ0)
p(ze|θ1)

=

|M|2(ze|θ0)
|M|2(ze|θ1)

σ(θ1)
σ(θ0)

(A4)

51

Figure 21: Calibration curves for diﬀerent estimators, comparing the uncalibrated (“raw”) estimator
to the estimator after probability calibration. The calibration curve for the truth prediction is a
cross-check for consistency, we do not actually use calibration for the truth predictions. For the
local score regression technique, we show the value of ˆh(x|θ0, θ1) (essentially the log likelihood ratio
in the local model) versus the estimated likelihood ratio after density estimation.

for any of the generated events.

In Sec. III B we have shown that regressing a function ˆr(x) on the generated events {xe} and
the corresponding joint likelihood ratios r(xe, ze|θ0, θ1) will converge to

ˆr(x) → r(x) =

p(x|θ0)
p(x|θ1)

,

(A5)

provided that the events are sampled according to xe ∼ p(x|θ1).

Requirements: The Rolr technique requires a generator with access to the joint likelihood ratios
r(xe, ze|θ0, θ1). In the particle physics case, this means we have to be able to evaluate the
squared matrix elements for given phase-space points and theory parameters.

Structure: Rolr can be used either point by point, in an agnostic parameterized version, or (if
the morphing condition in Eq. (6) holds) in a morphing-aware version. Figure 8 illustrates
the structure of the estimator in these three cases.

Training: The training phase is straightforward regression. It consists of minimizing the squared
error loss between a ﬂexible function ˆr(x|θ0, θ1) (for instance a neural network) and the
training data {xe, r(xe, ze|θ0, θ1)}, which was generated according to θ1.

In the point-by-point version, the input to the regressor are just the observables x, and
the ratio is between two ﬁxed hypotheses θ0 and θ1. In the parameterized versions of the
estimator, the ratios are based on various values θ0, while θ1 is still kept ﬁxed. In the agnostic
parameterized setup, the value of θ0 used in each event is then one of the inputs to the neural
network. In the morphing-aware versions, it us used to calculate the weights wc(θ0) that
multiply the diﬀerent component networks ˆrc(x), as visualized in the bottom panel of Fig. 8.

In all cases we can slightly improve the structure by adding samples generated according
to θ0 to the training samples, regressing on 1/r instead of r on these events. The full loss
functional is given in Eq. (33).

52

Calibration: In a next step, the classiﬁer output is optionally calibrated as discussed in Sec. III D 1
using isotonic regression. The calibration curve is shown in the middle panel of Fig. 21. We
have also experimented with an additional step of expectation calibration, see Sec. III D 2.

Evaluation: For a given x (and in the parameterized versions θ0), the regressor ˆr(x|θ0, θ1) is

evaluated. The result is optionally calibrated.

Parameters: The parameters of this approach are the architecture of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms.

For our example process we consider fully connected neural networks with two (“shallow”),
three, or ﬁve (“deep”) layers of 100 neurons each and tanh activation functions. They are
trained with the Adam optimizer [83] over 50 epochs with early stopping and learning rate
decay. Our default settings are given in Table VI. Experiments with diﬀerent architectures,
other activation functions, additional dropout layers, other optimizers, and diﬀerent learning
rate schedules yielded a worse performance.

e. Carl + score regression ( Cascal)

Idea: The parameterized Carl technique learns the full statistical model ˆr(x|θ0, θ1), including the
dependency on θ0. If it is realized as a diﬀerentiable classiﬁer (such as a neural network), we
can calculate the gradient of ˆr(x|θ0, θ1) with respect to θ0, and thus the estimated score of
this model. If the estimator is perfect, we expect this estimated score to minimize the squared
error with respect to the joint score data available for the training data. This is based on the
same argument as the local score regression technique, see Sec. III B for the proof.

We can turn this argument around and use the available score information during the training.
To this end, we combine two terms in a combined loss function: the Carl-style cross-entropy
and the squared error between the estimated score and the joint score of the training data.
These two pieces contain complementary information: the Carl part contains the information
of the likelihood ratio for a ﬁxed hypothesis comparison (θ0, θ1), while the score part describes
the relative change of the likelihood ratio under changes in θ0.

Requirements: The Cascal technique requires a generator with access to the joint score
t(xe, ze|θ0). In the particle physics case, this means we have to be able to evaluate the squared
matrix elements for given phase-space points and theory parameters.

Structure: Since the Cascal method relies on the extraction of the estimated score from the
estimator, it can only be used for parameterized estimators, either in an agnostic or morphing-
aware version. The middle and bottom panels of Fig. 8 illustrate the structure of the estimator
in these two cases.

Training: A diﬀerentiable classiﬁer with decision function ˆs(x|θ0, θ1) is trained to discriminate
between numerator (label y = 0) and denominator (label y = 1) samples, while the derived
estimated score ˆt(x|θ0) is compared to the joint score on the training samples generated
from y = 0. The loss function that is minimized is thus a combination of the Carl-style
cross-entropy and the squared error on the score, weighted by a hyperparameter α. It is given
in Eq. (35).

The numerator (y = 0) training samples do not come from a single parameter θ0, but rather a
combination of many diﬀerent subsamples. In the agnostic parameterized setup, the value of

53

θ0 used in each event is then one of the inputs to the neural network. In the morphing-aware
versions, it is used to calculate the weights wc(θ0) that multiply the diﬀerent component
networks ˆrc(x), as visualized in the bottom panel of Fig. 8.

Calibration: In a next step, the classiﬁer output is optionally calibrated as discussed in Sec. III D 1
using isotonic regression. The calibration curve is shown in the right panel of Fig. 21. We
have also experimented with an additional step of expectation calibration, see Sec. III D 2.

Evaluation: For a given x and θ0, the classiﬁer decision function ˆs(x|θ0, θ1) is evaluated. This is
turned into a likelihood ratio estimator with the relation given in Eq. (18), and optionally
calibrated.

Parameters: The key hyperparameter of this technique is the factor α that weights the two terms
in the loss function. Additional parameters set the architecture of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms.

For our example process we consider fully connected neural networks with two (“shallow”),
three, or ﬁve (“deep”) layers of 100 neurons each and tanh activation functions. They are
trained with the Adam optimizer [83] over 50 epochs with early stopping and learning rate
decay. Our default settings are given in Table VI. Experiments with diﬀerent architectures,
other activation functions, additional dropout layers, other optimizers, and diﬀerent learning
rate schedules yielded a worse performance.

f. Ratio + score regression ( Rascal)

Idea: The parameterized Rolr technique learns the full statistical model ˆr(x|θ0, θ1), including the
dependency on θ0. If it is realized as a diﬀerentiable regressor, we can calculate the gradient
of ˆr(x|θ0, θ1) with respect to θ0, and thus the score of this model. If the estimator is perfect,
we expect this estimated score to minimize the squared error with respect to the joint score
data available for the training data.

We can turn this argument around and use the available likelihood ratio and score information
during the training. To this end, we combine two terms in a combined loss function: the
squared errors on the ratio and the score. These two pieces contain complementary information:
the ratio regression part contains the information of the likelihood ratio for a ﬁxed hypothesis
comparison (θ0, θ1), while the score part describes the relative change of the likelihood ratio
under changes in θ0.

Requirements: The Rascal technique requires a generator with access to the joint likelihood
ratio r(xe, ze|θ0, θ1) and score t(xe, ze|θ0). In the particle physics case, this means we have
to be able to evaluate the squared matrix elements for given phase-space points and theory
parameters.

Structure: Since the Rascal method relies on the extraction of the estimated score from the
estimator, it can only be used for parameterized estimators, either in an agnostic or morphing-
aware version. The middle and bottom panels of Fig. 8 illustrate the structure of the estimator
in these two cases.

Training: An estimator ˆr(x|θ0, θ1) is trained through regression on the joint likelihood ratio, while
the derived estimated score ˆt(x|θ0) is compared to the joint score on the training samples

54

generated from y = 0. The loss function that is minimized is thus a combination of the
squared error on the ratio and the squared error on the score, weighted by a hyperparameter
α. It is given in Eq. (35).

The numerator (y = 0) training samples do not come from a single parameter θ0, but rather a
combination of many diﬀerent subsamples. In the agnostic parameterized setup, the value of
θ0 used in each event is then one of the inputs to the neural network. In the morphing-aware
versions, it is used to calculate the weights wc(θ0) that multiply the diﬀerent component
networks ˆrc(x), as visualized in the bottom panel of Fig. 8.

Calibration: In a next step, the classiﬁer output is optionally calibrated as discussed in Sec. III D 1
using isotonic regression. The calibration curve is shown in the right panel of Fig. 21. We
have also experimented with an additional step of expectation calibration, see Sec. III D 2.

Evaluation: For a given x and θ0, the estimator ˆr(x|θ0, θ1) is evaluated and optionally calibrated.

Parameters: The key hyperparameter of this technique is the factor α that weighs the two terms
in the loss function. Additional parameters set the architecture of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms.

For our example process we consider fully connected neural networks with two (“shallow”),
three, or ﬁve (“deep”) layers of 100 neurons each and tanh activation functions. They are
trained with the Adam optimizer [83] over 50 epochs with early stopping and learning rate
decay. Our default settings are given in Table VI. Experiments with diﬀerent architectures,
other activation functions, additional dropout layers, other optimizers, and diﬀerent learning
rate schedules yielded a worse performance.

g. Local score regression and density estimation ( Sally)

Idea: In Sec. III A 2 we introduced the score, the relative gradient of the likelihood with respect to
the theory parameters. The score evaluated at some reference parameter point is the suﬃcient
statistics of the local approximation of the likelihood given in Eq. (15). In other words, we
expect the score vector to be a set of “optimal observables” that includes all the information
on the theory parameters, at least in the vicinity of the reference parameter point. If we can
estimate the score from an observation, we can use it like any other set of observables. In
particular, we can ﬁll histograms of the score for any parameter point and thus estimate the
likelihood ratio in score space.

To estimate the score, we again make use of the particle physics structure. Particle physics
generators do not only provide sets of observables {xe}, but also the corresponding parton-
level momenta {ze}. From matrix element codes such as MadGraph we can extract the
squared matrix element |M|2(z|θ) given parton-level momenta z and theory parameter points
θ. This allows us to calculate the joint score

(cid:12)
(cid:12)
(cid:12)
t(xe, ze|θ0) = ∇θ log p(ze|θ)
(cid:12)
(cid:12)θ0

=

∇θ|M|2(ze|θ0)
|M|2(ze|θ0)

−

∇θσ(θ0)
σ(θ0)

(A6)

for any of the generated events. The derivatives in Eq. (A6) can always be evaluated
numerically.
If the process has the morphing structure of Eq. (6), one can alternatively
calculate it from the morphing weights.

55

(A7)

In Sec. III B we have shown that regressing a function ˆt(x) on the generated events {xe} and
the corresponding joint scores t(xe, ze|θ) will converge to

ˆt(x) → t(x) = ∇θ log p(x|θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)θ0

,

provided that the events are sampled according to xe ∼ p(x|θ0).

This technique is illustrated in the middle panels of Fig. 20. The middle panel of Fig. 21 shows
the relation between the scalar product of estimated score and θ0 − θ1 and the estimated
likelihood ratio.

Requirements: The Sally technique requires a generator with access to the joint score t(xe, ze|θ0).
In the particle physics case, this means we have to be able to evaluate the squared matrix
elements for given phase-space points and theory parameters.

Structure: The technique consists of two separate steps: the score regression and the density
estimation in the estimated score space. The score regression step is independent of the tested
hypothesis and realized as a simple fully connected neural network. The subsequent density
estimation is realized through multi-dimensional histograms, point by point in parameter
space (if the morphing condition in Eq. (6) holds, a morphing-aware version is also possible).

Training: The ﬁrst part of the training is regression on the score (evaluated at some reference
hypothesis). It consists of minimizing the squared error loss between a ﬂexible vector-valued
function ˆt(x|θscore) (implemented for instance as a neural network) and the training data
{xe, t(xe, ze|θscore)}, which was sampled according to θscore.

The second step is density estimation in the estimated score space. We only consider histo-
grams, but other density estimation techniques are also possible. For each value of θ0 or θ1
that is tested, we generate samples of events, estimate the corresponding score vectors, and
ﬁll a multidimensional histogram of the estimated score.

Calibration: The density estimation step already calibrates the results, so we do not experiment

with an additional calibration step.

Evaluation: For a given observation x, the score regressor ˆt(x) is evaluated. For each tested (θ0, θ1)
pair, we then extract the corresponding bin contents from the numerator and denominator
histograms, and calculate the estimated likelihood ratio with Eq. (38).

Parameters: Both the score regression part and the subsequent density estimation have parameters.
The ﬁrst and most important choice is the reference hypothesis θscore, at which the score is
evaluated. For eﬀective ﬁeld theories the Standard Model is the natural choice, and we use it
in our example process.

The score regression also depends on the hyperparameters of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms. For our example process we consider fully
connected neural networks with two (“shallow”), three, or ﬁve (“deep”) layers of 100 neurons
each and tanh activation functions. They are trained with the Adam optimizer [83] over 50
epochs with early stopping and learning rate decay. Our default settings are given in Table VI.
Experiments with diﬀerent architectures, other activation functions, additional dropout layers,
other optimizers, and diﬀerent learning rate schedules yielded a worse performance.

56

The only parameter of the density estimation stage is the histogram binning. For our example
process we consider two diﬀerent variations:

• Density estimation with a “ﬁxed” binning, where the bin axes are aligned with the score

components. We use 40 bins for each of the two score components.

• Density estimation with a “dynamic” binning, in which the bin axes are aligned with
the θ0 − θ1 direction and the orthogonal one. We use 80 bins along the ∆θ direction,
which carries the relevant information in the local model approximation, and 10 along
the orthogonal vector.

For each pair (θ0, θ1) and each dimension, the bin edges are chosen such that the expected
number of events according to θ0 plus the expected number of events according to θ1 is the
same in each bin.

h. Local score regression, compression to scalar, and density estimation ( Sallino)

Idea: In the proximity of the Standard Model (or any other reference parameter point), likelihood
ratios only depend on the scalar product between the score and the diﬀerence between
the numerator and denominator parameter points. If we can estimate the score from an
observation, we can calculate this scalar product ˆh(x|θ0, θ1), deﬁned in Eq. (39), and use it
like any other observable. In particular, we can ﬁll histograms of ˆh for any parameter point
and thus estimate the likelihood ratio in ˆh space.

To estimate the score, we once again exploit particle physics structure. Particle physics
generators do not only provide sets of observables {xe}, but also the corresponding parton-
level momenta {ze}. From matrix element codes such as MadGraph we can extract the
squared matrix element |M|2(z|θ) given parton-level momenta z and theory parameter points
θ. This allows us to calculate the joint score with Eq. (A6) for any of the generated events.
In Sec. III B we have shown that regressing a function ˆt(x) on the generated events {xe} and
the corresponding joint scores t(xe, ze|θ) will converge to t(x), provided that the events are
sampled according to xe ∼ p(x|θ0).

Requirements: The Sallino technique requires a generator with access to the joint score
In the particle physics case, this means we have to be able to evaluate the

t(xe, ze|θ0).
squared matrix elements for given phase-space points and theory parameters.

Structure: The technique consists of two separate steps: the score regression, and the density
estimation in ˆh space. The score regression step is independent of the tested hypothesis and
realized as a simple fully connected neural network. The subsequent density estimation in ˆh
space is realized through one-dimensional histograms, point by point in parameter space (if
the morphing condition in Eq. (6) holds, a morphing-aware version is also possible).

Training: The ﬁrst part of the training is regression on the score (evaluated at some reference
hypothesis). It consists of minimizing the squared error loss between a ﬂexible vector-valued
function ˆt(x|θscore) (implemented for instance as a neural network) and the training data
{xe, t(xe, ze|θscore)}, which was sampled according to θscore.
The second step is density estimation in ˆh space. We only consider histograms, but other
density estimation techniques are also possible. For each value of θ0 or θ1 that is tested, we
generate samples of events, estimate the corresponding score vectors, calculate the scalar
product in Eq. (39) to get ˆh(x|θ0, θ1), and ﬁll a one-dimensional histogram of this quantity.

57

Calibration: The density estimation step already calibrates the results, so we do not experiment

with an additional calibration step.

Evaluation: For a given observation x, the score regressor ˆt(x) is evaluated. For each tested (θ0, θ1)
pair, we multiply it with θ0 − θ1 to get ˆh(x|θ0, θ1), extract the corresponding bin contents
from the numerator and denominator histograms, and calculate the estimated likelihood ratio
with Eq. (40).

Parameters: Both the score regression part and the subsequent density estimation have parameters.
The ﬁrst and most important choice is the reference hypothesis θscore, at which the score is
evaluated. For eﬀective ﬁeld theories the Standard Model is the natural choice, and we use it
in our example process.

The score regression also depends on the hyperparameters of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms. For our example process we consider fully
connected neural networks with two (“shallow”), three, or ﬁve (“deep”) layers of 100 neurons
each and tanh activation functions. They are trained with the Adam optimizer [83] over 50
epochs with early stopping and learning rate decay. Our default settings are given in Table VI.
Experiments with diﬀerent architectures, other activation functions, additional dropout layers,
other optimizers, and diﬀerent learning rate schedules yielded a worse performance.

The only parameter of the density estimation stage is the histogram binning. We use 100
bins. For each pair (θ0, θ1) and each dimension, the bin edges are chosen such that the same
expected number of events according to θ0 plus the expected number of events according to
θ1 is the same in each bin.

3. Additional results

In Tbls. VII to XII we compare the performance of diﬀerent versions of the likelihood ratio
estimators. As metric we use the expected mean squared error on log r(x|θ0, θ1) as well as a trimmed
version, as deﬁned in Sec. V A. The estimators are an extended list of those given in Table IV,
adding variations with diﬀerent hyperparameter choices and the results for uncalibrated (“raw”)
estimators. By default, we use neural networks with 3 hidden layers, the labels “shallow” and “deep”
refer to 2 and 5 hidden layers, respectively. We highlight the versions of the estimators that were
shown in the main part of this paper.

Because of the duality between density estimation and probabilistic classiﬁcation (see Eqs. (18)
and (46)), we can use all techniques to deﬁne classiﬁers. In Fig. 22 we show the ROC curves for
two benchmark parameter points. Note how badly the two scenarios can be separated. This is not
a shortcoming of the discrimination power of the classiﬁers, but due to the genuine overlap of the
probability distributions, as can be seen from the identical ROC curve based on the true likelihood
ratio.

58

Figure 22: Receiver operating characteristic (ROC) curves of true positive rates (TPR) vs. false
positive rates (FPR) for the classiﬁcation between the benchmark scenarios θ0 = (−0.5, −0.5)T and
θ1 as in Eq. (47). The ROC AUC based on the true likelihood is 0.6276. The results show how
much the probability distributions for these two hypotheses overlap.

Strategy

Setup

Histogram pT,j1
∆φjj
2d (coarse binning)
2d (medium binning)
2d (ﬁne binning)
2d (very ﬁne binning)
2d (asymmetric binning)

Afc

2d, (cid:15) = 1
2d, (cid:15) = 0.5
2d, (cid:15) = 0.2
2d, (cid:15) = 0.1
2d, (cid:15) = 0.05
2d, (cid:15) = 0.02
2d, (cid:15) = 0.01
5d, (cid:15) = 1
5d, (cid:15) = 0.5
5d, (cid:15) = 0.2
5d, (cid:15) = 0.1
5d, (cid:15) = 0.05
5d, (cid:15) = 0.02
5d, (cid:15) = 0.01

Expected MSE

All Trimmed

Table IV Figures

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

0.0879
0.1595
0.0764
0.0630
0.0597
0.0603
0.0561

0.1243
0.0797
0.0586
0.0732
0.3961
13.6816
241.3264
0.1252
0.0779
0.0734
0.9560
38.1854
2050.5289
50024.7997

0.0230
0.0433
0.0117
0.0101
0.0115
0.0153
0.0106

0.0257
0.0144
0.0091
0.0103
0.0160
0.0550
0.2143
0.0226
0.0101
0.0128
0.1833
3.6658
57.0410
1668.8988

Table VII: Comparison of techniques based on manually selected kinematic observables. The
metrics shown are the expected mean squared error on the log likelihood ratio with and without
trimming, as deﬁned in the text. Checkmarks in the last two columns denote estimators included in
Table IV and the ﬁgures in the main part of this paper, respectively.

59

Table IV Figures

Expected MSE

All Trimmed

0.0409

0.0213

0.0301

0.0111

Fig. 12

Strategy

Carl (PbP, raw)
Carl (PbP, cal.)
Carl (parameterized, raw)

Setup

PbP

PbP

Carl (parameterized, cal.)

Carl (morphing-aware, raw) Baseline

Carl (morphing-aware, cal.) Baseline

Baseline
Baseline, shallow
Baseline, deep
Random θ
Random θ, shallow
Random θ, deep

Baseline
Baseline, shallow
Baseline, deep
Random θ
Random θ, shallow
Random θ, deep

0.0157
0.0134
0.0161
0.0148
0.0130
0.0164

0.0156
0.0124
0.0160
0.0147
0.0122
0.0155

0.1598
0.1483
Baseline, shallow
0.1743
Random θ
Random θ, shallow
0.1520
Morphing basis, shallow 10.1231

Baseline, shallow
Random θ
Random θ, shallow
Morphing basis, shallow

0.1036
0.0762
0.1076
0.0858
0.1564

0.0040
0.0035
0.0038
0.0037
0.0037
0.0038

0.0032
0.0026
0.0029
0.0029
0.0028
0.0029

0.0350
0.0331
0.0429
0.0369
7.9314

0.0282
0.0200
0.0289
0.0226
0.0618

(cid:88)

Fig. 12

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

Table VIII: Comparison of diﬀerent versions of the Carl technique. The metrics shown are the
expected mean squared error on the log likelihood ratio with and without trimming, as deﬁned in
the text. Checkmarks in the last two columns denote estimators included in Table IV and the
ﬁgures in the main part of this paper, respectively.

Strategy

Rolr (PbP, raw)
Rolr (PbP, cal.)
Rolr (param., raw)

Setup

PbP

PbP

60

Table IV Figures

Expected MSE

All Trimmed

0.0052

0.0023

0.0049

0.0022

(cid:88)

Rolr (param., cal.)

(cid:88)

Baseline
Baseline, shallow
Baseline, deep
Random θ
Random θ, shallow
Random θ, deep

Baseline
Baseline, shallow
Baseline, deep
Random θ
Random θ, shallow
Random θ, deep

Baseline, shallow
Random θ
Random θ, shallow
Morphing basis, shallow

Baseline, shallow
Random θ
Random θ, shallow
Morphing basis, shallow

0.0034
0.0069
0.0041
0.0034
0.0070
0.0036

0.0032
0.0059
0.0038
0.0030
0.0060
0.0034

0.2029
0.1672
0.1908
0.1160
5.6668

0.0328
0.0243
0.0321
0.0224
0.1300

0.0019
0.0037
0.0022
0.0017
0.0036
0.0017

0.0017
0.0030
0.0019
0.0014
0.0030
0.0015

0.1449
0.1305
0.1353
0.0755
3.8335

0.0088
0.0063
0.0089
0.0052
0.0485

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

Rolr (morph.-aware, raw) Baseline

Rolr (morph.-aware, cal.) Baseline

Table IX: Comparison of diﬀerent versions of the Rolr technique. The metrics shown are the
expected mean squared error on the log likelihood ratio with and without trimming, as deﬁned in
the text. Checkmarks in the last two columns denote estimators included in Table IV and the
ﬁgures in the main part of this paper, respectively.

Expected MSE

All Trimmed

Table IV Figures

Strategy

Setup

Sally

Fixed 2D histogram
Fixed 2D histogram, shallow
Fixed 2D histogram, deep
Dynamic 2D histogram
Dynamic 2D histogram, shallow
Dynamic 2D histogram, deep

Sallino 1D histogram

1D histogram, shallow
1D histogram, deep

0.0174
0.0170
0.0171
0.0132
0.0133
0.0132

0.0213
0.0215
0.0213

0.0005
0.0005
0.0005
0.0003
0.0003
0.0002

0.0006
0.0007
0.0006

(cid:88)

(cid:88)

(cid:88)

Table X: Comparison of diﬀerent versions of the Sally and Sallino techniques. The metrics
shown are the expected mean squared error on the log likelihood ratio with and without trimming,
as deﬁned in the text. Checkmarks in the last two columns denote estimators included in Table IV
and the ﬁgures in the main part of this paper, respectively.

Strategy

Setup

Cascal (param., raw)

61

Expected MSE

All Trimmed

Table IV Figures

Baseline, α = 5
Baseline, α = 5 , shallow
Baseline, α = 5 , deep
Baseline, α = 0.5, deep
Baseline, α = 1, deep
Baseline, α = 2, deep
Baseline, α = 10, deep
Baseline, α = 20, deep
Baseline, α = 50, deep
Random θ
Random θ, shallow
Random θ, deep

Baseline, α = 5
Baseline, α = 5 , shallow
Baseline, α = 5 , deep
Baseline, α = 0.5, deep
Baseline, α = 1, deep
Baseline, α = 2, deep
Baseline, α = 10, deep
Baseline, α = 20, deep
Baseline, α = 50, deep
Random θ, α = 5
Random θ, α = 5 , shallow
Random θ, α = 5 , deep

0.0019
0.0037
0.0010
0.0017
0.0014
0.0017
0.0013
0.0016
0.0024
0.0022
0.0038
0.0010

0.0012
0.0025
0.0008
0.0013
0.0011
0.0010
0.0010
0.0011
0.0016
0.0013
0.0027
0.0009

0.1935
0.1870
Baseline, α = 5 , shallow
Random θ, α = 5 , shallow
0.1624
Morph. basis, α = 5 , shallow 0.0707

0.1408
0.1359
Baseline, α = 5 , shallow
Random θ, α = 5 , shallow
0.0922
Morph. basis, α = 5 , shallow 0.0403

0.0004
0.0004
0.0003
0.0006
0.0005
0.0008
0.0004
0.0004
0.0007
0.0006
0.0005
0.0003

0.0002
0.0003
0.0002
0.0003
0.0003
0.0002
0.0003
0.0003
0.0005
0.0003
0.0004
0.0002

0.0810
0.0732
0.0643
0.0109

0.0508
0.0427
0.0268
0.0081

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

Cascal (param., cal.)

Cascal (morph.-aw., raw) Baseline, α = 5

Cascal (morph.-aw., cal.) Baseline, α = 5

Table XI: Comparison of diﬀerent versions of the Cascal technique. The metrics shown are the
expected mean squared error on the log likelihood ratio with and without trimming, as deﬁned in
the text. Checkmarks in the last two columns denote estimators included in Table IV and the
ﬁgures in the main part of this paper, respectively.

Strategy

Setup

Rascal (param., raw)

62

Expected MSE

All Trimmed

Table IV Figures

Baseline, α = 100
Baseline, α = 100, shallow
Baseline, α = 100, deep
Baseline, α = 10, deep
Baseline, α = 20, deep
Baseline, α = 50, deep
Baseline, α = 200, deep
Baseline, α = 500, deep
Baseline, α = 1000, deep
Random θ
Random θ, shallow
Random θ, deep

Baseline, α = 100
Baseline, α = 100, shallow
Baseline, α = 100, deep
Baseline, α = 10, deep
Baseline, α = 20, deep
Baseline, α = 50, deep
Baseline, α = 200, deep
Baseline, α = 500, deep
Baseline, α = 1000, deep
Random θ, α = 100
Random θ, α = 100, shallow
Random θ, α = 100, deep

0.0010
0.0025
0.0009
0.0011
0.0009
0.0009
0.0009
0.0011
0.0012
0.0011
0.0030
0.0008

0.0010
0.0021
0.0009
0.0010
0.0009
0.0009
0.0008
0.0009
0.0012
0.0010
0.0025
0.0008

0.2880
0.3569
Baseline, α = 100, shallow
0.2705
Random θ, α = 100
Random θ, α = 100, shallow
0.3243
Morph. basis, α = 100, shallow 0.1909

0.1530
0.1250
Baseline, α = 100, shallow
0.1358
Random θ, α = 100
Random θ, α = 100, shallow
0.1316
Morph. basis, shallow, α = 100 0.0307

0.0003
0.0006
0.0004
0.0005
0.0004
0.0004
0.0004
0.0006
0.0007
0.0004
0.0010
0.0004

0.0003
0.0005
0.0004
0.0004
0.0004
0.0004
0.0004
0.0005
0.0006
0.0004
0.0008
0.0004

0.2024
0.2861
0.1825
0.2488
0.1673

0.0673
0.0514
0.0627
0.0539
0.0072

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

Rascal (param., cal.)

Rascal (morph.-aw., raw) Baseline, α = 100

Rascal (morph.-aw., cal.) Baseline, α = 100

Table XII: Comparison of diﬀerent versions of the Rascal technique. The metrics shown are the
expected mean squared error on the log likelihood ratio with and without trimming, as deﬁned in
the text. Checkmarks in the last two columns denote estimators included in Table IV and the
ﬁgures in the main part of this paper, respectively.

63

[1] T. Sjostrand, S. Mrenna, and P. Z. Skands, Comput. Phys. Commun. 178, 852 (2008), arXiv:0710.3820.
[2] S. Agostinelli et al. (GEANT4), Nucl. Instrum. Meth. A506, 250 (2003).
[3] K. S. Cranmer, Comput. Phys. Commun. 136, 198 (2001), arXiv:hep-ex/0011057 [hep-ex]; K. Cranmer,
G. Lewis, L. Moneta, A. Shibata, and W. Verkerke (ROOT), (2012); M. Frate, K. Cranmer, S. Kalia,
A. Vandenberg-Rodes, and D. Whiteson, (2017), arXiv:1709.05681 [physics.data-an].

[4] J. Brehmer, K. Cranmer, F. Kling, and T. Plehn, Phys. Rev. D95, 073002 (2017), arXiv:1612.05261

[hep-ph].

[5] K. Kondo, J. Phys. Soc. Jap. 57, 4126 (1988).
[6] V. M. Abazov et al. (D0), Nature 429, 638 (2004), arXiv:hep-ex/0406031 [hep-ex].
[7] P. Artoisenet and O. Mattelaer, Proceedings, 2nd International Workshop on Prospects for charged
Higgs discovery at colliders (CHARGED 2008): Uppsala, Sweden, September 16-19, 2008, PoS
CHARGED2008, 025 (2008).

[8] Y. Gao, A. V. Gritsan, Z. Guo, K. Melnikov, M. Schulze, and N. V. Tran, Phys. Rev. D81, 075022

(2010), arXiv:1001.3396 [hep-ph].

[9] J. Alwall, A. Freitas, and O. Mattelaer, Phys. Rev. D83, 074010 (2011), arXiv:1010.2263 [hep-ph].
[10] S. Bolognesi, Y. Gao, A. V. Gritsan, K. Melnikov, M. Schulze, N. V. Tran, and A. Whitbeck, Phys.

Rev. D86, 095031 (2012), arXiv:1208.4018 [hep-ph].

[11] P. Avery et al., Phys. Rev. D87, 055006 (2013), arXiv:1210.0896 [hep-ph].
[12] J. R. Andersen, C. Englert, and M. Spannowsky, Phys. Rev. D87, 015019 (2013), arXiv:1211.3011

[13] J. M. Campbell, R. K. Ellis, W. T. Giele,

and C. Williams, Phys. Rev. D87, 073005 (2013),

[14] P. Artoisenet, P. de Aquino, F. Maltoni, and O. Mattelaer, Phys. Rev. Lett. 111, 091802 (2013),

[hep-ph].

arXiv:1301.7086 [hep-ph].

arXiv:1304.6414 [hep-ph].

[15] J. S. Gainer, J. Lykken, K. T. Matchev, S. Mrenna, and M. Park, in Proceedings, 2013 Community
Summer Study on the Future of U.S. Particle Physics: Snowmass on the Mississippi (CSS2013):
Minneapolis, MN, USA, July 29-August 6, 2013 (2013) arXiv:1307.3546 [hep-ph].

[16] D. Schouten, A. DeAbreu, and B. Stelzer, Comput. Phys. Commun. 192, 54 (2015), arXiv:1407.7595

[physics.comp-ph].

[hep-ph].

[17] T. Martini and P. Uwer, JHEP 09, 083 (2015), arXiv:1506.08798 [hep-ph].
[18] A. V. Gritsan, R. Röntsch, M. Schulze, and M. Xiao, Phys. Rev. D94, 055023 (2016), arXiv:1606.03107

[19] T. Martini and P. Uwer, (2017), arXiv:1712.04527 [hep-ph].
[20] D. Atwood and A. Soni, Phys. Rev. D45, 2405 (1992).
[21] M. Davier, L. Duﬂot, F. Le Diberder, and A. Rouge, Phys. Lett. B306, 411 (1993).
[22] M. Diehl and O. Nachtmann, Z. Phys. C62, 397 (1994).
[23] D. E. Soper and M. Spannowsky, Phys. Rev. D84, 074002 (2011), arXiv:1102.3480 [hep-ph].
[24] D. E. Soper and M. Spannowsky, Phys. Rev. D87, 054012 (2013), arXiv:1211.3140 [hep-ph].
[25] D. E. Soper and M. Spannowsky, Phys. Rev. D89, 094005 (2014), arXiv:1402.1189 [hep-ph].
[26] C. Englert, O. Mattelaer, and M. Spannowsky, Phys. Lett. B756, 103 (2016), arXiv:1512.03429 [hep-ph].
[27] D. B. Rubin, Ann. Statist. 12, 1151 (1984).
[28] M. A. Beaumont, W. Zhang, and D. J. Balding, Genetics 162, 2025 (2002).
[29] P. Marjoram, J. Molitor, V. Plagnol, and S. Tavaré, Proceedings of the National Academy of Sciences

[30] S. A. Sisson, Y. Fan, and M. M. Tanaka, Proceedings of the National Academy of Sciences 104, 1760

[31] S. A. Sisson and Y. Fan, Likelihood-free MCMC (Chapman & Hall/CRC, New York.[839], 2011).
[32] J.-M. Marin, P. Pudlo, C. P. Robert, and R. J. Ryder, Statistics and Computing , 1 (2012).
[33] T. Charnock, G. Lavaux, and B. D. Wandelt, Phys. Rev. D97, 083004 (2018), arXiv:1802.03537

100, 15324 (2003).

(2007).

[astro-ph.IM].

[34] K. Cranmer, J. Pavez, and G. Louppe, (2015), arXiv:1506.02169 [stat.AP].
[35] K. Cranmer and G. Louppe, J. Brief Ideas (2016), 10.5281/zenodo.198541.
[36] Y. Fan, D. J. Nott, and S. A. Sisson, ArXiv e-prints (2012), arXiv:1212.1479 [stat.CO].

64

[37] G. Papamakarios and I. Murray, in Advances in Neural Information Processing Systems 29 , edited by
D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (Curran Associates, Inc., 2016) pp.
1028–1036.

[38] B. Paige and F. Wood, ArXiv e-prints (2016), arXiv:1602.06701 [stat.ML].
[39] R. Dutta, J. Corander, S. Kaski, and M. U. Gutmann, ArXiv e-prints

(2016), arXiv:1611.10242

[stat.ML].

[cs.LG].

[cs.CV].

[40] M. U. Gutmann, R. Dutta, S. Kaski, and J. Corander, Statistics and Computing , 1 (2017).
[41] D. Tran, R. Ranganath, and D. M. Blei, ArXiv e-prints (2017), arXiv:1702.08896 [stat.ML].
[42] G. Louppe and K. Cranmer, ArXiv e-prints (2017), arXiv:1707.07113 [stat.ML].
[43] L. Dinh, D. Krueger, and Y. Bengio, ArXiv e-prints (2014), arXiv:1410.8516 [cs.LG].
[44] D. Jimenez Rezende and S. Mohamed, ArXiv e-prints (2015), arXiv:1505.05770 [stat.ML].
[45] L. Dinh, J. Sohl-Dickstein, and S. Bengio, ArXiv e-prints (2016), arXiv:1605.08803 [cs.LG].
[46] G. Papamakarios, T. Pavlakou, and I. Murray, ArXiv e-prints (2017), arXiv:1705.07057 [stat.ML].
[47] B. Uria, M.-A. Côté, K. Gregor, I. Murray, and H. Larochelle, ArXiv e-prints (2016), arXiv:1605.02226

[48] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior,

and K. Kavukcuoglu, ArXiv e-prints (2016), arXiv:1609.03499 [cs.SD].

[49] A. van den Oord, N. Kalchbrenner, O. Vinyals, L. Espeholt, A. Graves, and K. Kavukcuoglu, ArXiv

e-prints (2016), arXiv:1606.05328 [cs.CV].

[50] A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu, ArXiv e-prints (2016), arXiv:1601.06759

[51] G. Papamakarios, D. C. Sterratt, and I. Murray, ArXiv e-prints (2018), arXiv:1805.07226 [stat.ML].
[52] J. Brehmer, K. Cranmer, G. Louppe, and J. Pavez, (2018), arXiv:1805.00013 [hep-ph].
[53] J. Brehmer, G. Louppe, J. Pavez, and K. Cranmer, (2018), arXiv:1805.12244 [stat.ML].
[54] J. Brehmer, K. Cranmer, G. Louppe, and J. Pavez, “Code repository for the paper “Constraining Eﬀect-
ive Field Theories with Machine Learning”,” http://github.com/johannbrehmer/higgs_inference
(2018).

[55] S. R. Coleman, J. Wess, and B. Zumino, Phys. Rev. 177, 2239 (1969).
[56] C. G. Callan, Jr., S. R. Coleman, J. Wess, and B. Zumino, Phys. Rev. 177, 2247 (1969).
[57] S. Weinberg, Phys. Lett. B91, 51 (1980).
[58] C. J. C. Burges and H. J. Schnitzer, Nucl. Phys. B228, 464 (1983).
[59] C. N. Leung, S. T. Love, and S. Rao, Z. Phys. C31, 433 (1986).
[60] W. Buchmuller and D. Wyler, Nucl. Phys. B268, 621 (1986).
[61] C. Arzt, M. B. Einhorn, and J. Wudka, Nucl. Phys. B433, 41 (1995), arXiv:hep-ph/9405214 [hep-ph].
[62] K. Hagiwara, S. Ishihara, R. Szalapski, and D. Zeppenfeld, Phys. Rev. D48, 2182 (1993).
[63] B. Grzadkowski, M. Iskrzynski, M. Misiak, and J. Rosiek, JHEP 10, 085 (2010), arXiv:1008.4884

[hep-ph].

[64] J. Alwall, R. Frederix, S. Frixione, V. Hirschi, F. Maltoni, O. Mattelaer, H. S. Shao, T. Stelzer,

P. Torrielli, and M. Zaro, JHEP 07, 079 (2014), arXiv:1405.0301 [hep-ph].

[65] B. Henning, X. Lu, and H. Murayama, JHEP 01, 023 (2016), arXiv:1412.1837 [hep-ph].
[66] D. de Florian et al. (LHC Higgs Cross Section Working Group), (2016), 10.23731/CYRM-2017-002,

arXiv:1610.07922 [hep-ph].

[hep-ph].

[67] J. Brehmer, A. Freitas, D. Lopez-Val, and T. Plehn, Phys. Rev. D93, 075014 (2016), arXiv:1510.03443

[68] J. Brehmer, F. Kling, T. Plehn, and T. M. P. Tait, (2017), arXiv:1712.02350 [hep-ph].
[69] J. de Favereau, C. Delaere, P. Demin, A. Giammanco, V. Lemaître, A. Mertens, and M. Selvaggi

(DELPHES 3), JHEP 02, 057 (2014), arXiv:1307.6346 [hep-ex].

[70] G. Aad et al. (ATLAS), “A morphing technique for signal modelling in a multidimensional space of

coupling parameters,” (2015), Physics note ATL-PHYS-PUB-2015-047.

[71] J. R. Dell’Aquila and C. A. Nelson, Phys. Rev. D33, 80 (1986).
[72] T. Plehn, D. L. Rainwater, and D. Zeppenfeld, Phys. Rev. Lett. 88, 051801 (2002), arXiv:hep-ph/0105325

[73] V. Hankele, G. Klamke, D. Zeppenfeld, and T. Figy, Phys. Rev. D74, 095001 (2006), arXiv:hep-

[hep-ph].

ph/0609075 [hep-ph].

[74] K. Hagiwara, Q. Li, and K. Mawatari, JHEP 07, 101 (2009), arXiv:0905.4314 [hep-ph].

65

[75] C. Englert, D. Goncalves-Netto, K. Mawatari, and T. Plehn, JHEP 01, 148 (2013), arXiv:1212.0843

[hep-ph].

[76] K. Cranmer and T. Plehn, Eur. Phys. J. C51, 415 (2007), arXiv:hep-ph/0605268 [hep-ph].
[77] T. Plehn, P. Schichtel, and D. Wiegand, Phys. Rev. D89, 054002 (2014), arXiv:1311.2591 [hep-ph].
[78] F. Kling, T. Plehn, and P. Schichtel, Phys. Rev. D95, 035026 (2017), arXiv:1607.07441 [hep-ph].
[79] P. Baldi, K. Cranmer, T. Faucett, P. Sadowski, and D. Whiteson, Eur. Phys. J. C76, 235 (2016),

arXiv:1601.07913 [hep-ex].

[80] K. Cranmer, J. Pavez, G. Louppe, and W. K. Brooks, Proceedings, 17th International Workshop on
Advanced Computing and Analysis Techniques in Physics Research (ACAT 2016): Valparaiso, Chile,
January 18-22, 2016, J. Phys. Conf. Ser. 762, 012034 (2016).

[81] J. Alsing, B. Wandelt, and S. Feeney, (2018), arXiv:1801.01497 [astro-ph.CO].
[82] A. Hyvärinen, Journal of Machine Learning Research 6, 695 (2005).
[83] D. P. Kingma and J. Ba, ArXiv e-prints (2014), arXiv:1412.6980 [cs.LG].
[84] F. Chollet et al., “Keras,” https://github.com/keras-team/keras (2015).
[85] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,
M. Kudlur, J. Levenberg, D. Mane, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens,
B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viegas, O. Vinyals,
P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, ArXiv e-prints (2016), arXiv:1603.04467
[cs.DC].

[86] J. B. Kruskal, Psychometrika 29, 115 (1964).
[87] S. S. Wilks, Annals Math. Statist. 9, 60 (1938).
[88] G. Cowan, K. Cranmer, E. Gross, and O. Vitells, Eur. Phys. J. C71, 1554 (2011), [Erratum: Eur.

Phys. J. C73, p. 2501, 2013], arXiv:1007.1727 [physics.data-an].

[89] A. Wald, Transactions of the American Mathematical Society 54, 426 (1943).
[90] G. Louppe, M. Kagan, and K. Cranmer, (2016), arXiv:1611.01046 [stat.ME].
[91] A. Mertens, in Proceedings, 15th International Workshop on Advanced Computing and Analysis Tech-

niques in Physics Research (ACAT 2013), Vol. 523 (2014) p. 012028.

8
1
0
2
 
l
u
J
 
6
2
 
 
]
h
p
-
p
e
h
[
 
 
4
v
0
2
0
0
0
.
5
0
8
1
:
v
i
X
r
a

A Guide to Constraining Eﬀective Field Theories with Machine Learning

Johann Brehmer,1 Kyle Cranmer,1 Gilles Louppe,2 and Juan Pavez3
1New York University, USA
2University of Liège, Belgium
3Federico Santa María Technical University, Chile
(Dated: 30th July 2018)

We develop, discuss, and compare several inference techniques to constrain theory para-
meters in collider experiments. By harnessing the latent-space structure of particle physics
processes, we extract extra information from the simulator. This augmented data can be
used to train neural networks that precisely estimate the likelihood ratio. The new methods
scale well to many observables and high-dimensional parameter spaces, do not require any
approximations of the parton shower and detector response, and can be evaluated in micro-
seconds. Using weak-boson-fusion Higgs production as an example process, we compare the
performance of several techniques. The best results are found for likelihood ratio estimators
trained with extra information about the score, the gradient of the log likelihood function with
respect to the theory parameters. The score also provides suﬃcient statistics that contain
all the information needed for inference in the neighborhood of the Standard Model. These
methods enable us to put signiﬁcantly stronger bounds on eﬀective dimension-six operators
than the traditional approach based on histograms. They also outperform generic machine
learning methods that do not make use of the particle physics structure, demonstrating their
potential to substantially improve the new physics reach of the LHC legacy results.

CONTENTS

I. Introduction

II. The EFT measurement problem

A. Eﬀective ﬁeld theory
B. Physics challenges and traditional methods
C. Structural properties of EFT measurements
D. Explicit example

III. Likelihood ratio estimation

A. Modeling likelihood ratios
B. Available information and its usefulness
C. Strategies
D. Calibration
E. Implementation
F. Challenges and diagnostics

IV. Limit setting

A. Asymptotics
B. Neyman construction
C. Nuisance parameters

V. Results

A. Idealized setup
B. Detector eﬀects

2

4
4
4
6
8

11
12
14
18
24
25
28

30
30
31
32

33
33
41

2

42

45
45
46
57

63

VI. Conclusions

A. Appendix

1. Simpliﬁed detector description
2. Model almanac
3. Additional results

References

I.

INTRODUCTION

An important aspect of the legacy of the Large Hadron Collider (LHC) experiments will be precise
constraints on indirect signatures of physics beyond the Standard Model (SM), parameterized for
instance by the dimension-six operators of the Standard Model eﬀective ﬁeld theory (SMEFT). The
relevant measurements can easily involve tens of diﬀerent parameters that predict subtle kinematic
signatures in the high-dimensional space of the data. Traditional analysis techniques do not scale
well to this complex problem, motivating the development of more powerful techniques.

The analysis of high-energy-physics data is based on an impressive suite of simulation tools that
model the hard interaction, parton shower, hadronization, and detector response. The community
has invested a tremendous amount of eﬀort into developing these tools, yielding the high-ﬁdelity
modeling of LHC data needed for precision measurements. Simulators such as Pythia [1] and
Geant4 [2] use Monte-Carlo techniques to sample the multitudinous paths through which a
particular hard scattering might develop. A single event’s path through the simulation can easily
involve many millions of random variables. While Monte-Carlo techniques can eﬃciently sample
from the distributions implicitly deﬁned by the simulators, it is not feasible to calculate the likelihood
for a particular observation because doing so would require integrating over all the possible histories
leading to that observation. Clearly it is infeasible to explicitly calculate a numerical integral over
this enormous latent space. While this problem is ubiquitous in high energy physics, it is rarely
acknowledged explicitly.

Traditionally, particle physicists have approached this problem by restricting the analysis to
one or two well-motivated discriminating variables, discarding the information contained in the
remaining observables. The probability density for the restricted set of discriminating variables is
then estimated with explicit functions or non-parametric approaches such as template histograms,
kernel density estimates, or Gaussian Processes [3]. These low-dimensional density estimates are
constructed and validated using Monte-Carlo samples from the simulation. While well-chosen
variables may yield precise bounds along individual directions of the parameter space, they often
lead to weak constraints in other directions in the parameter space [4]. The sensitivity to multiple
parameters can be substantially improved by using the fully diﬀerential cross section. This is the
forte of the Matrix Element Method [5–19] and Optimal Observables [20–22] techniques, which
are based on the parton-level structure of a given process. Shower and event deconstruction [23–
26] extend this approach to the parton shower. But all these methods still require some level
of approximations on the parton shower and either neglect or crudely approximate the detector
response. Moreover, even a simpliﬁed description of the detector eﬀects requires the numerically
expensive evaluation of complicated integrals for each observed event. None of these established
approaches scales well to high-dimensional problems with many parameters and observables, such
as the SMEFT measurements.

In recent years there has been increased appreciation that several real-world phenomena are
better described by simulators that do not admit a tractable likelihood. This appears in ﬁelds as
diverse as ecology, phylogenetics, epidemiology, cardiac simulators, quantum chemistry, and particle

3

physics. Inference in this setting is often referred to as likelihood-free inference, where the inference
strategy is restricted to samples generated from the simulator. Implicitly, these techniques aim to
estimate the likelihood. A particularly ubiquitous technique is Approximate Bayesian Computation
(Abc) [27–33]. Abc is closely related to the traditional template histogram and kernel density
estimation approach used by physicists. More recently, approximate inference techniques based on
machine learning and neural networks have been proposed [34–51]. All these techniques have in
common that they only take into account simulated samples similar to the actual observables — they
do not exploit the structure of the process that generates them.

We develop new simulation-based inference techniques that are tailored to the structure of particle
physics processes. The key insight behind these methods is that we can extract more information
than just samples from the simulations, and that this additional information can be used to eﬃciently
train neural networks that precisely estimate likelihood ratios, the preferred test statistics for LHC
measurements. These methods are designed for scalability to both high-dimensional parameter
spaces as well as to many observables. They do not require any simplifying assumptions to the
underlying physics: they support state-of-the-art event generators with parton shower, reducible
and irreducible backgrounds, and full detector simulations. After an upfront training phase, they are
very eﬃcient to evaluate. Our tools directly provide an estimator for the likelihood ratio, an intuitive
and easily interpretable quantity. Finally, limits derived from these tools with toy experiments have
the reassuring property that even if they might not be optimal, they are never wrong, i. e. no points
are said to be excluded that should not be excluded at a given conﬁdence level.

In Ref. [52], the companion paper of this publication, we focus on the key ideas and sensitivity
enabled by these techniques. Reference [53] presents the methods in a more abstract setting. Here we
describe the actual algorithms in detail, developing several diﬀerent methods side by side. Given the
number of discussed variations, this publication might have the look and feel of a review article and
we present it as a guide to the interested practitioner. We focus on the main ideas and diﬀerences
between the approaches and postpone many technical details until the appendices.

We evaluate the performance of these diﬀerent methods on a speciﬁc example problem, the
measurement of two dimension-six operators in Higgs production in weak boson fusion (WBF) in
the four-lepton mode at the LHC. For part of this analysis, we work in an idealized setting in which
we can access the true likelihood function, providing us with a ground truth for the comparison of
the diﬀerent analysis methods. After establishing the precision of the likelihood ratio estimation,
we turn towards the more physical question of how strongly the two operators can be constrained
with the diﬀerent techniques. We repeat the analysis with a simpliﬁed detector response where the
ground-truth likelihood is no longer tractable.

We begin by laying out the problem in Sec. II: we summarize the eﬀective ﬁeld theory idea,
list the challenges posed by EFT measurements, translate the problem from a physics perspective
into the language of statistics, and discuss its important structural properties. We also set up the
example process used throughout the rest of the paper. The description of the analysis methods
are split in two parts:
in Sec. III we deﬁne the diﬀerent techniques to estimate the likelihood
ratio, which includes most of the conceptual work presented here. Section IV then explains how
to set limits on the EFT parameters based on these tools. In Sec. V we evaluate the performance
of the diﬀerent tools in our example process. Finally, in Sec. VI we summarize our ﬁndings and
give recommendations for practitioners. The appendices describe the diﬀerent algorithms in more
detail and provide additional results. The code and data used for this paper are available online at
Ref. [54].

4

(1)

II. THE EFT MEASUREMENT PROBLEM

A. Eﬀective ﬁeld theory

Eﬀective ﬁeld theories (EFTs) [55–57] parameterize the eﬀects of physics at an energy scale Λ
on observables at smaller energies E (cid:28) Λ as a set of local operators. The form of these operators is
ﬁxed by the light particles and the symmetry structure of the theory and is entirely independent of
the high-energy model. Systematically expanding the Lagrangian in 1/Λ, equivalent to ordering the
operators by their canonical dimension, leaves us with a ﬁnite set of operators weighted by Wilson
coeﬃcients that describe all possible new physics eﬀects up to some order in E/Λ.

In the absence of new particles at the TeV scale, and assuming the symmetry structure of the
SM, we can thus describe any new physics signature in LHC processes in terms of a set of higher-
dimensional operators [58–63]. In this SM Eﬀective Field Theory (SMEFT), the leading eﬀects
beyond the SM come from 59 independent dimension-six operators Oo with Wilson coeﬃcients fo,

LD6 = LSM +

(cid:88)

o

fo
Λ2 Oo ,

where the SM corresponds to all fo = 0 and any measurement of a deviation hints at new physics.
The dimension-six Wilson coeﬃcients are perfectly suited as an interface between experimental
measurements and theory interpretations. They are largely model-independent, can parameterize a
wide range of observables, including novel kinematic features, and are theoretically consistent beyond
tree level. On the technical side, dimension-six operators are implemented in standard Monte-Carlo
event generators [64], allowing us to generate predictions for rates and kinematic observables for
any combination of Wilson coeﬃcients. Measured values of fo/Λ2 can easily be translated to the
parameters of speciﬁc models through well-established matching procedures [65]. All in all, SMEFT
measurements will likely be a key part of the legacy of the LHC experiments [66].

Let us brieﬂy comment on the question of EFT validity. A hierarchy of energy scales E (cid:28) Λ is
the key assumption behind the EFT construction, but in a bottom-up approach the cutoﬀ scale Λ
cannot be known without additional model assumptions. From a measurement fo/Λ2 (cid:54)= 0 we can
estimate the new physics scale Λ only by assuming a characteristic size of the new physics couplings
√
fo, and compare it to the energy scale E of the experiment. It has been found that dimension-six
operators often capture the dominant eﬀects of new physics even when there is only a moderate
scale separation E (cid:46) Λ [67]. All these concerns are not primarily of interest for the measurement of
Wilson coeﬃcients, but rather important for the interpretation of the results in speciﬁc UV theories.

B. Physics challenges and traditional methods

EFT measurements at the LHC face three fundamental challenges:

1. Individual scattering processes at the LHC are sensitive to several operators and require
simultaneous inference over a multi-dimensional parameter space. While a naive parameter
scan works well for one or two dimensions, it becomes prohibitively expensive for more than
a few parameters.

2. Most operators introduce new coupling structures and predict non-trivial kinematic features.
These do not translate one-to-one to traditional kinematic observables such as transverse
momenta, invariant masses or angular correlations. An analysis based on only one kinematic
variable typically cannot constrain the full parameter space eﬃciently. Instead, most of the

5

operator eﬀects only become fully apparent when multiple such variables including their
correlations are analysed [4, 68].

3. The likelihood function of the observables is intractable, making this the setting of “likelihood-
free inference” or “simulator-based inference”. There are simulators for the high-energy
interactions, the parton shower, and detector eﬀects that can generate events samples for
any theory parameter values, but they can only be run in the forward mode. Given a set of
reconstruction-level observables, it is not possible to evaluate the likelihood of this observation
given diﬀerent theory parameters. The reason is that this likelihood includes the integral over
all possible diﬀerent parton shower histories and particle trajectories through the detector as
a normalizing constant, which is infeasible to calculate in realistic situations. We will discuss
this property in more detail in the following section.

The last two issues are typically addressed in one of three ways. Most commonly, a small set of
discriminating variables (also referred to as summary statistics or engineered features) is handpicked
for a given problem. The likelihood in this low-dimensional space is then estimated, for instance, by
ﬁlling histograms from simulations. While well-chosen variables may lead to good constraints along
individual directions of the parameter space, there are typically directions in the parameter space
with limited sensitivity [4, 68].

The Matrix Element Method [5, 6, 8–15, 17–19] or Optimal Observables [20–22] go beyond a
few speciﬁc discriminating variables and use the matrix element for a particular process to estimate
the likelihood ratio. While these techniques can be very powerful, they suﬀer from two serious
limitations. The parton shower and detector response are either entirely neglected or approximated
through ad-hoc transfer function. Shower and event deconstruction [23–26] allow for the calculation
of likelihood ratios at the level of the parton shower, but still rely on transfer functions to describe
the detector response. Finally, even with such a simple description of the shower and detector, the
evaluation of the likelihood ratio estimator requires the numerically expensive computation of large
integrals for each observed event.

Finally, there is a class of generic methods for likelihood-free inference. For Bayesian inference,
the best-known approach is Approximate Bayesian Computation (Abc) [27–32]. Similar to the
histogram approach, it relies on the choice of appropriate low-dimensional summary statistics, which
can severely limit the sensitivity of the analysis. Diﬀerent techniques based on machine learning have
been developed recently. In particle physics, the most common example are discriminative classiﬁers
between two discrete hypotheses, such as a signal and a background process. This approach has
recently been extended to parameter measurements [34, 35]. More generally, many techniques based
on the idea of using a classiﬁcation model, such as neural networks, for inference in the absence of
a tractable likelihood function have been introduced in the machine learning community [36–51].
All of these methods only require samples of events trained according to diﬀerent parameter points.
They do not make use of the structure of the particle physics processes, and thus do not use all
available information.

All of these methods come with a price. We develop new techniques that
• are tailored to particle physics measurements and leverage their structural properties,
• scale well to high-dimensional parameter spaces,
• can accommodate many observables,
• capture the information in the fully diﬀerential cross sections, including all correlations

• fully support state-of-the art simulators with parton showers and full detector simulations,

between observables,

and

• are very eﬃcient to evaluate after an upfront training phase.

6

(2)

(3)

C. Structural properties of EFT measurements

1. Particle-physics structure

One essential step to ﬁnding the optimal measurement strategy is identifying the structures and
symmetries of the problem. Particle physics processes, in particular those described by eﬀective
ﬁeld theories, typically have two key properties that we can exploit.

First, any high-energy particle physics process factorizes into the parton-level process, which
contains the matrix element and in it the entire dependence on the EFT coeﬃcients, and a residual
part describing the parton shower and detector eﬀects. In many plausible scenarios of new physics
neither the strong interactions in the parton shower nor the electromagnetic and strong interactions
in the detector are aﬀected by the parameters of interest. The likelihood function can then be
written as

(cid:90)

(cid:90)

p(x|θ) =

dz p(x, z|θ) =

dz p(x|z) p(z|θ) .

Here and in the following x are the actual observables after the shower, detector, and reconstruction;
θ are the theory parameters of interest; and z are the parton-level momenta (a subset of the latent
variables). Table I provides a dictionary of these and other important symbols that we use.

The ﬁrst ingredient to this likelihood function is the distribution of parton-level four-momenta

p(z|θ) =

1
σ(θ)

dσ(θ)
dz

,

where σ(θ) and dσ(θ)/dz are the total and diﬀerential cross sections, respectively. Crucially, this
function is tractable: the matrix element and the parton density functions can be evaluated for
arbitrary four-momenta z and parameter values θ. In practice this means that matrix-element
codes such as MadGraph [64] can not only be run in a forward, generative mode, but also deﬁne
functions that return the squared matrix element for a given phase-space point z. Unfortunately,
there is typically no user-friendly interface to these functions, so evaluating it requires some work.
Second, the conditional density p(x|z) describes the probabilistic evolution from the parton-level
four-momenta to observable particle properties. While this symbol looks innocuous, it represents the
full parton shower, the interaction of particles with the detector material, the sensor response and
readout, and the reconstruction of observables. Diﬀerent simulators such as Pythia [1], Geant4 [2],
or Delphes [69] are often used to generate samples {x} ∼ p(x|z) for given parton-level momenta z.
This sampling involves the Monte-Carlo integration over the possible shower histories and detector
interactions,

(cid:90)

(cid:90)

p(x|z) =

dzdetector

dzshower p(x|zdetector) p(zdetector|zshower) p(zshower|z) .

(4)

This enormous latent space can easily involve many millions of random numbers, and these integrals
are clearly intractable, which we denote with the red symbol p. In other words, given a set of
reconstruction-level observables x, we cannot calculate the likelihood function p(x|z) that describes
the compatibility of parton-level momenta z with the observation. By extension, we also cannot
evaluate p(x|θ), the likelihood function of the theory parameters given the observation. The
intractable integrals in Eq. (4) are the crux of the EFT measurement problem.

The factorization of Eq. 2 together with the tractability of the parton-level likelihood p(z|θ) is
immensely important. We will refer to the combination of these two properties as particle-physics
structure. The far-reaching consequences of this structure for EFT measurements will be the topic
of Sec. III B. Many (but not all) of the inference strategies we discuss will rely on this condition.

Symbol

Physics meaning

Machine learning abstraction

Set of all observables
One or two kinematic variables

z ≡ zparton
zshower
zdetector
zall = (zparton, zshower, zdetector) Full simulation history of event
θ

Parton-level four-momenta
Parton shower trajectories
Detector interactions

Theory parameters (Wilson
coeﬃcients)
Best ﬁt for theory parameters

x
v

ˆθ

p(x|θ)

p(z|θ)

p(x|z)

Distributions of observables given
theory parameters
Parton-level distributions from
matrix element
Eﬀect of shower, detector,
reconstruction

7

Features
Low-dimensional summary
statistics /engineered feature
Latent variables
Latent variables
Latent variables
All latent variables
Parameters of interest

Estimator for parameters of
interest

Intractable likelihood

Tractable likelihood of latent
variables
Intractable density deﬁned
through stochastic generative
process

r(x|θ0, θ1)
ˆr(x|θ0, θ1)
t(x|θ)
ˆt(x|θ)

xe, ze
θo
θc, wc(z), pc(x)

Likelihood ratio between hypotheses θ0, θ1, see Eq. (11).
Estimator for likelihood ratio
Score, see Eq. (14).
Estimator for score

Event
Wilson coeﬃcient for one operator

Data point
Individual parameter of interest

Morphing basis points, coeﬃcients, densities, see Eq. (6).

Table I: Dictionary deﬁning many symbols that appear in this paper. Red symbols denote
intractable likelihood functions. The last three rows explain our conventions for indices.

Note that this Markov property holds even with reducible and irreducible backgrounds and when
a matching scheme is used to combine diﬀerent parton-level multiplicities. In these situations there
may be diﬀerent disjoint parts of z space, even with diﬀerent dimensionalities, for instance when
events with n and n + 1 partons in the ﬁnal state can lead to the same conﬁguration of observed
jets. The integral over z then has to be replaced with a sum over “zn spaces” and an integral over
each zn, but the logic remains unchanged.

2. Operator morphing

Eﬀective ﬁeld theories (and other parameterisations of indirect signatures of new physics) typically
contribute a ﬁnite number of amplitudes to a given process, each of which is multiplied by a function
of the Wilson coeﬃcients.1 In this case the likelihood can be written as

p(z|θ) =

˜wc(cid:48)(θ) fc(cid:48)(z)

(cid:88)

c(cid:48)

(5)

where c(cid:48) labels the diﬀerent amplitude components, and the functions fc(cid:48)(z) are not necessarily
properly positive deﬁnite or normalized.

1 Exceptions can arise for instance when particle masses or widths depend on the parameters of interest. But in an

EFT setting one can expand these quantities in 1/Λ, restoring the factorization.

The simplest example is a process in which one SM amplitude M0(z) interferes with one new
physics amplitude MBSM(z|θ) = θM1(z), which scales linearly with a new physics parameter
θ. The diﬀerential cross section, proportional to the squared matrix element, is then dσ(z) ∝
|M0(z)|2 + 2θ Re M0(z)†M1(z) + θ2 |M1(z)|2. There are three components, representing the SM,
interference, and pure BSM terms, each with their own parameter dependence ˜wc(cid:48)(z) and momentum
dependence fc(cid:48)(z).

We can then pick a number of basis2 parameter points θc equal to the number of components
c(cid:48) in Eq. (5). They can always be chosen such that the matrix Wcc(cid:48) = ˜wc(cid:48)(θc) is invertible, which
allows us to rewrite (5) as a mixture model

p(z|θ) =

wc(θ) pc(z)

(cid:88)

c

with weights wc(θ) = (cid:80)
cc(cid:48) and (now properly normalized) basis densities pc(z) = p(z|θc).
The weights wc(θ) depend on the choice of basis points and are analytically known. This “morphing”
procedure therefore allows us to extract the full likelihood function p(z|θ) from a ﬁnite set of
evaluations of basis densities pc(z).

c(cid:48) ˜wc(θ) W −1

Calculating the full statistical model through morphing requires the likelihood p(z|θ) to be
tractable, which is true for parton-level momenta as argued above. However, the same trick can
be applied even when the exact likelihood is intractable, but we can estimate it. For instance, the
marginal distribution of any individual kinematic variable v(x) can be reliably estimated through
histograms or other density estimation techniques, even when shower and detector eﬀects are taken
into account. The morphing procedure then lets us evaluate the full conditional distribution p(v|θ)
based on a ﬁnite number of Monte-Carlo simulations [70].
Finally, note that Eq. (6) together with Eq. (2) imply

p(x|θ) =

wc(θ) pc(x) ,

(cid:88)

c

even if the likelihood function p(x|θ) and the components pc(x) are intractable. This will later allow
us to impose the morphing structure on likelihood ratio estimators.

Not all EFT amplitudes satisfy the morphing structure in Eq. (5), so we discuss both measurement
strategies that rely on and make use of this property as well as more general ones that do not require
it to hold.

8

(6)

(7)

D. Explicit example

1. Weak-boson-fusion Higgs to four leptons

As an explicit example LHC process we consider Higgs production in weak boson fusion (WBF)

with a decay of the Higgs into four leptons,

qq → qq h → qq ZZ → qq (cid:96)+(cid:96)− (cid:96)+(cid:96)−

(8)

with (cid:96) = e, µ, as shown in Fig. 1.

While this process is rare and is likely to only be observed during the high-luminosity run of
the LHC, it has a few compelling features that make it a prime candidate to study the eﬃcient
extraction of information. First, the two jets from the quarks and in particular the four leptons can

2 Note that the morphing basis points θc are unrelated to the choice of an operator basis for the eﬀective ﬁeld theory.

9

(9)

(10)

q

q

W , Z

W , Z

Z

Z

h

q

q

(cid:96)+
(cid:96)−

(cid:96)+
(cid:96)−

Figure 1: Feynman diagram for Higgs production in weak boson fusion in the 4(cid:96) mode. The red
dots show the Higgs-gauge interactions aﬀected by the dimension-six operators of our analysis.

be reconstructed quite precisely in the LHC detectors. Even when assuming on-shell conditions and
energy-momentum conservation, the ﬁnal-state momenta span a 16-dimensional phase space, giving
rise to many potentially informative observables.

Second, both the production of the Higgs boson in weak boson fusion as well as its decay
into four leptons are highly sensitive to the eﬀects of new physics in the Higgs-gauge sector. We
parameterize these with dimension-six operators in the SMEFT, following the conventions of the
Hagiwara-Ishihara-Szalapski-Zeppenfeld basis [62]. For simplicity, we limit our analysis to the two
particularly relevant operators

L = LSM +

fW
Λ2

ig
2
(cid:124)

(Dµφ)† σa Dνφ W a
µν
(cid:123)(cid:122)
(cid:125)
OW

−

fW W
Λ2

g2
4
(cid:124)

(φ†φ) W a

µν W µν a
(cid:125)

.

(cid:123)(cid:122)
OW W

For convenience, we rescale the Wilson coeﬃcients to the dimensionless parameters of interest

θ =

(cid:18) fW v2
Λ2

,

(cid:19)T

fW W v2
Λ2

where v = 246 GeV is the electroweak vacuum expectation value. As alluded to above, the validity
range of the EFT cannot be determined in a model-independent way. For moderately weakly to
moderately strongly coupled underlying new physics models, one would naively expect |fo| (cid:46) O (1)
and the EFT description to be useful in the range E ≈ v (cid:46) Λ, or −1 (cid:46) θo (cid:46) 1. This is the parameter
range we analyse in this paper.

The interference between the Standard Model amplitudes and the dimension-six operators leads
to an intricate relation between the observables and parameters in this process, which has been
studied extensively. The precise measurement of the momenta of the four leptons provides access to
a range of angular correlations that fully characterize the h → ZZ decay [10, 71]. These variables
are sensitive to the eﬀects of dimension-six operators. But the momentum ﬂow p through the decay
vertex is limited by the Higgs mass, and the relative eﬀects of these dimension-six operators are
suppressed by a factor p2/Λ2. On the other hand, the Higgs production through two oﬀ-shell gauge
bosons with potentially high virtuality does not suﬀer from this suppression. The properties of the
two jets recoiling against them are highly sensitive to operator eﬀects in this vertex [72–75].

In Fig. 2 we show example distributions of two particularly informative observables, the transverse
momentum of the leading (higher-pT ) jet pT,j1, and the azimuthal angle between the two jets, ∆φjj.
The two quantities are sensitive to diﬀerent directions in parameter space. Note also that the
interference between the diﬀerent amplitudes can give rise to non-trivial eﬀects. The size of the
dimension-six amplitudes grows with momentum transfer, which is strongly correlated with the
transverse momentum of the leading jet. If the interference of new-physics amplitudes with the SM
diagrams is destructive, this can drive the total amplitude through zero [67]. The jet momentum

10

Figure 2: Kinematic distributions in our example process for three example parameter points. We
assume an idealized detector response to be discussed in Sec. II D 2. Left: transverse momentum of
the leading (higher-pT ) jet, a variable strongly correlated with the momentum transfer in the
process. The dip around 350 GeVis a consequence of the amplitude being driven through zero, as
discussed in the text. Right: separation in azimuthal angle between the two jets.

distribution then dips and rises again with higher energies, as seen in the red curve in the left panel
of Fig. 2. Such depleted regions of low probability can lead to very small or large likelihood ratios
and potentially pose a challenge to inference methods.

By analysing the Fisher information in these distributions, it is possible to compare the discrimin-
ation power in these two observables to the information contained in the full multivariate distribution
or to the information in the total rate. It turns out that the full multivariate distribution p(z|θ)
contains signiﬁcantly more information than the one-dimensional and two-dimensional marginal
distributions of any standard kinematic variables [4]. The total rate is found to carry much less
information on the two operators, in particular when systematic uncertainties on the cross sections
are taken into account. In this study we therefore only analyse the kinematic distributions for a
ﬁxed number of observed events.

2. Sample generation

Already in the sample generation we can make use of the structural properties of the process
discussed in Sec. II C. The amplitude of this process factorizes into a sum of parameter-dependent
factors times phase-space-dependent amplitudes, as given in Eq. (5). The eﬀect of the operators OW
and OW W on the total Higgs width breaks this decomposition, but this eﬀect is tiny and in practice
irrelevant when compared to the experimental resolution. The likelihood function of this process
therefore follows the mixture model in Eq. (6) to good approximation, and the weights wc(θ) can
be calculated. Since the parton-level likelihood function is tractable, we can reconstruct the entire
likelihood function p(z|θ) based on a ﬁnite number of simulator runs, as described in Sec. II C 2.

To this end, we ﬁrst generate a parton-level sample {ze} of 5.5·106 events with MadGraph 5 [64]
and its add-on MadMax [76–78], using the setup described in Ref. [4]. With MadMax we can
evaluate the likelihood p(ze|θc) for all events ze and for 15 diﬀerent basis parameter points θc.
Calculating the morphing weights wc(θ) ﬁnally gives us the true parton-level likelihood function

11

Figure 3: Basis points θc and some of the morphing weights wc(θ) for our example process. Each
panel shows the morphing weight of one of the components c as a function of parameter space. The
weights of the remaining 13 components (not shown) follow qualitatively similar patterns. The dots
show the position of the basis points θc, the big black dot denotes the basis point corresponding to
the morphing weight shown in that panel. Away from the morphing basis points, the morphing
weights can easily reach O (100), with large cancellations between diﬀerent components.

p(ze|θ) for each generated phase-space point ze.

In Fig. 3 we show the basis points θc and two of the morphing weights wc(θ) with their dependence
on θ. In some corners of parameter space the weights easily reach up to |wc| (cid:46) O (100), and there
are large cancellations between positive and negative weights. This will pose a challenge for the
numerical stability of every inference algorithm that directly uses the morphing structure of the
process, as we will discuss later. Other basis choices have led to comparable or larger morphing
weights.

Parton shower and detector eﬀects smear the observed particle properties x with respect to
the parton-level momenta z and make the likelihood function in Eq. (2) intractable. We develop
inference methods that can be applied exactly in this case and that do not require any simplifying
assumptions on the shower and detector response. However, in this realistic scenario we cannot
evaluate their performance by comparing them to the true likelihood ratio. We therefore test them
ﬁrst on an idealized scenario in which the four-momenta, ﬂavor, and charges of the leptons, and the
momenta of the partons, can be measured exactly, p(x|z) ≈ δ(x − z). In this approximation we can
evaluate the likelihood p(x|θ).

After establishing the performance of the various algorithms in this idealized setup, we will analyse
the eﬀect of parton shower and detector simulation on the results. We generate an approximate
detector-level sample by drawing events from a smearing distribution p(x|z) conditional on the
parton-level momenta z. This smearing function is loosely motivated by the performance of the
LHC experiments and is deﬁned in Appendix A 1.

III. LIKELIHOOD RATIO ESTIMATION

According to the Neyman-Pearson lemma, the likelihood ratio

r(x|θ0, θ1) ≡

p(x|θ0)
p(x|θ1)

=

(cid:82) dz p(x, z|θ0)
(cid:82) dz p(x, z|θ1)

(11)

12

is the most powerful test statistic to discriminate between two hypotheses θ0 and θ1. Unfortunately,
the integral over the latent space z makes the likelihood function p(x|θ) as well as the likelihood
ratio r(x|θ0, θ1) intractable. The ﬁrst and crucial stage of all our EFT measurement strategies
is therefore the construction of a likelihood ratio estimator ˆr(x|θ0, θ1) that is as close to the true
r(x|θ0, θ1) as possible and thus maximizes the discrimination power between θ0 and θ1.

This estimation problem has several diﬀerent aspects that we try to disentangle as much as
possible. The ﬁrst choice is the overall structure of the likelihood ratio estimator and its dependence
on the theory parameters θ. We discuss this in Sec. III A. Section III B analyses what information is
available and useful to construct (train) the estimators for a given process. Here we will introduce
the main ideas that harness the structure of the EFT to increase the information that is used in the
training process.

These basic concepts are combined into concrete strategies for the estimation of the likelihood ratio
in Sec. III C. After training the estimators, there is an optional additional calibration stage, which
we introduce in Sec. III D. Section III E describes the technical implementation of these strategies
in terms of neural networks. Finally, we discuss the challenges that the diﬀerent algorithms face in
Sec. III F and introduce diagnostic tools for the uncertainties.

A. Modeling likelihood ratios

1. Likelihood ratios

There are diﬀerent approaches to the structure of this estimator, in particular to the dependence

on the theory parameters θ:

Point by point (PbP): A common strategy is to scan the parameter space, randomly or in a
grid. To reduce the complexity of the scan one can keep the denominator θ1 ﬁxed, while
scanning only θ0. Likelihood ratios with other denominators can be extracted trivially as
ˆr(x|θ0, θ2) = ˆr(x|θ0, θ1)/ˆr(x|θ2, θ1). Instead of a single reference value θ1, we can also use
a composite reference hypothesis p(x|θ1) → pref(x) = (cid:82) dθ1 π(θ1) p(x|θ1) with some prior
π(θ1). This can reduce the regions in feature space with small reference likelihood p(x|θ1)
and improve the numerical stability.

For each pair (θ0, θ1) separately, the likelihood ratio ˆr(x|θ0, θ1) as a function of x is estimated.
Only the ﬁnal results are interpolated between the scanned values of θ0.
This approach is particularly simple, but discards all information about the structure and
smoothness of the parameter space. For high-dimensional parameter spaces, the parameter
scan can become prohibitively expensive. The ﬁnal interpolation may introduce additional
uncertainties.

Agnostic parameterized estimators: Alternatively we can train one estimator as the full model
ˆr(x|θ0, θ1) as a function of both x and the parameter combination (θ0, θ1) [34, 79]. A
modiﬁcation is again to leave θ1 at a ﬁxed reference value (or ﬁxed composite reference
hypothesis with a prior π(θ1)) and only learn the dependence on x and θ0.
This parameterized approach leaves it to the estimator to learn the typically smooth depend-
ence of the likelihood ratio on the physics parameters and does not require any interpolation
in the end. There are no assumptions on the form of the dependence of the likelihood on the
ratios.

Morphing-aware estimators: For problems that satisfy the morphing condition of Eq. (6) and
thus also Eq. (7), we can impose this structure and the explicit knowledge of the weights

13

(12)

(13)

(14)

wc(θ) onto the estimator. Again, one option is to keep the denominator ﬁxed at a reference
value (or composite reference hypothesis), leading to

ˆr(x|θ0, θ1) =

wc(θ0) ˆrc(x)

(cid:88)

c

where the basis estimators ˆrc(x) = ˆr(x|θc, θ1) only depend on x.

Alternatively, we can decompose both the numerator and denominator distributions to
ﬁnd [34, 80]

ˆr(x|θ0, θ1) =

(cid:34)

(cid:88)

(cid:88)

c

c(cid:48)

wc(cid:48)(θ1)
wc(cid:48)(θ0)

(cid:35)−1

ˆrc(cid:48),c(x)

with pairwise estimators ˆrc(cid:48),c(x) = ˆr(x|θc(cid:48), θc).

One remarkably powerful quantity is the score, deﬁned as the relative tangent vector

2. Score and local model

(cid:12)
(cid:12)
t(x|θ0) = ∇θ log p(x|θ)
(cid:12)θ0

.

It quantiﬁes the relative change of the likelihood under inﬁnitesimal changes in parameter space
and can be seen as a local equivalent of the likelihood ratio.

In a small patch around θ0 in which we can approximate t(x|θ) as independent of θ, Eq. (14) is

solved by the local model

plocal(x|θ) =

p(t(x|θ0)|θ0) exp[t(x|θ0) · (θ − θ0)]

(15)

1
Z(θ)

with a normalisation factor Z(θ). The local model is in the exponential family. Note that the t(x|θ0)
are the suﬃcient statistics for plocal(x|θ). This is signiﬁcant: if we can estimate the vector-valued
function t(x|θ0) (with one component per parameter of interest) of the high-dimensional x, we can
reduce the dimensionality of our space dramatically without losing any information, at least in the
local model approximation [81].

In fact, ignoring the normalization factors and in the local model the likelihood ratio between θ0
and θ1 only depends on the scalar product between the score and θ0 − θ1, which will allow us to
take this dimensionality reduction one step further and compress high-dimensional data x into a
scalar without loss of power.

In our example process, we are interested in the Wilson coeﬃcients of two dimension-six operators.
The score vector therefore has two components. In Fig. 4 we show the relation between these two
score components and two informative kinematic variables, the jet pT and the azimuthal angle
between the two jets, ∆φ. We ﬁnd that the score vector is very closely related with these two
kinematic quantities, but the relation is not quite one-to-one. Larger energy transfer, measured as
larger jet pT , increases the typical size of the score vector. The OW W component of the score is
particularly sensitive to the angular correlation variable, in agreement with detailed studies of this
process [4].

14

(16)

(17)

(18)

Figure 4: Score vector as a function of kinematic observables in our example process. Left: ﬁrst
component of the score vector, representing the relative change of the likelihood with respect to
small changes in OW direction. Right: second component of the score vector, representing the
relative change of the likelihood with respect to small changes in OW W direction. In both panels,
the axes show two important kinematic variables. We ﬁnd that the score vector is clearly correlated
with these two variables.

B. Available information and its usefulness

1. General likelihood-free case

All measurement strategies have in common that the estimator ˆr(x|θ0, θ1) is learned from data
In the most general
provided by Monte-Carlo simulations (the stochastic generative process).
likelihood-free scenario, we can only generate samples of events {xe} with xe ∼ p(x|θ) through the
simulator, and base an estimator ˆr(x|θ0, θ1) on these generated samples.

One strategy [34] is based on training a classiﬁer with decision function ˆs(x) between two
equal-sized samples {xe} ∼ p(x|θ0), labelled ye = 0, and {xe} ∼ p(x|θ1), labelled ye = 1. The
cross-entropy loss functional

L[ˆs] = −

(ye log ˆs(xe) + (1 − ye) log(1 − ˆs(xe)))

1
N

(cid:88)

e

is minimized by the optimal decision function

From the decision function ˆs(x) of a classiﬁer we can therefore extract an estimator for the likelihood
ratio as

s(x|θ0, θ1) =

p(x|θ1)
p(x|θ0) + p(x|θ1)

.

ˆr(x|θ0, θ1) =

1 − ˆs(x|θ0, θ1)
ˆs(x|θ0, θ1)

.

This idea, sometimes called the likelihood ratio trick, is visualized in the left panel of Fig. 5.

As pointed out in Ref. [34], we can use the weaker assumption of any loss functional that is
minimized by a decision function s(x) that is a strictly monotonic function of the likelihood ratio.
The underlying reason is that the likelihood ratio is invariant under any transformation s(x) with
this property. In practice, the output of any such classiﬁer can be brought closer to the form of
Eq. (17) through a calibration procedure, which we will discuss in Sec. III D.

15

Figure 5: Illustration of some key concepts with a one-dimensional Gaussian toy example. Left:
classiﬁers trained to distinguish two sets of events generated from diﬀerent hypotheses (green dots)
converge to an optimal decision function s(x|θ0, θ1) (in red) given in Eq. (17). This lets us extract
the likelihood ratio. Right: regression on the joint likelihood ratios r(xe, ze|θ0, θ1) of the simulated
events (green dots) converges to the likelihood ratio r(x|θ0, θ1) (red line).

2. Particle-physics structure

As we have argued in Sec. II C, particle physics processes have a speciﬁc structure that allow
us to extract additional information. Most processes satisfy the factorization of Eq. (2) with a
tractable parton-level likelihood p(z|θ). The generators do not only provide samples {xe}, but
also the corresponding parton-level momenta (latent variables) {ze} with (xe, ze) ∼ p(x, z|θ0). By
evaluating the matrix elements at the generated momenta ze for diﬀerent hypotheses θ0 and θ1,
we can extract the parton-level likelihood ratio p(ze|θ0)/p(ze|θ1). Since the distribution of x is
conditionally independent of the theory parameters, this is the same as the joint likelihood ratio

r(xe, zall e|θ0, θ1) ≡

p(xe, zdetector e, zshower e, ze|θ0)
p(xe, zdetector e, zshower e, ze|θ1)
p(xe|zdetector e)
p(xe|zdetector e)
p(ze|θ0)
p(ze|θ1)

p(zdetector e|zshower e)
p(zdetector e|zshower e)

.

=

=

p(zshower e|ze)
p(zshower e|ze)

p(ze|θ0)
p(ze|θ1)

(19)

So while we cannot directly evaluate the likelihood ratio at the level of measured observables
r(x|θ0, θ1), we can calculate the likelihood ratio for a generated event conditional on the latent
parton-level momenta.

The same is true for the score, i. e. the tangent vectors or relative change of the (log) likelihood
under inﬁnitesimal changes of the parameters of interest. While the score t(xe|θ0) = ∇θ log p(x|θ)|θ0

16

(20)

(cid:125)

(21)

Figure 6: Illustration of some key concepts with a one-dimensional Gaussian toy example. Left:
probability density functions for diﬀerent values of θ and the scores t(xe, ze|θ) at generated events
(xe, ze). These tangent vectors measure the relative change of the density under inﬁnitesimal
changes of θ. Right: dependence of log p(x|θ) on θ for ﬁxed x = 4. The arrows again show the
(tractable) scores t(xe, ze|θ).

is intractable, we can extract the joint score

t(xe, zall e|θ0) ≡ ∇θ log p(xe, zdetector e, zshower e, ze|θ0)
p(zdetector e|zshower e)
p(zdetector e|zshower e)

=

p(zshower e|ze)
p(zshower e|ze)

∇θp(ze|θ)
p(ze|θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)θ0

p(xe|zdetector e)
p(xe|zdetector e)
∇θp(ze|θ)
p(ze|θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)θ0

=

from the simulator. Again, all intractable parts of the likelihood cancel. We visualize the score
in Fig. 6 and all available information on the generated samples in Fig. 7. It is worth repeating
that we are not making any simplifying approximations about the process here, these statements
are valid with reducible backgrounds, for state-of-the-art generators including higher-order matrix
elements, matching of matrix element and parton shower, and with full detector simulations.

But how does the availability of the joint likelihood ratio r(x, z|θ) and score t(x, z|θ) (which
depend on the latent parton-level momenta z) help us to estimate the likelihood ratio r(x|θ), which
is the one we are interested in?

Consider the L2 squared loss functional for functions ˆg(x) that only depend on x, but which are

trying to approximate a function g(x, z),

L[ˆg(x)] =

dx dz p(x, z|θ) |g(x, z) − ˆg(x)|2

=

dx

ˆg2(x)

dz p(x, z|θ) − 2ˆg(x)

dz p(x, z|θ) g(x, z) +

(cid:90)

(cid:90)

(cid:90)

(cid:21)
dz p(x, z|θ) g2(x, z)

.

(cid:90)

(cid:90)

(cid:20)

(cid:124)

(cid:123)(cid:122)
F (x)

17

Figure 7: Illustration of some key concepts with a one-dimensional Gaussian toy example. Left: full
statistical model log r(x|θ, θ1) that we are trying to estimate. Right: available information at the
generated events (xe, ze). The dots mark the joint likelihood ratios log r(xe, ze|θ0, θ1), the arrows
the scores t(xe, ze|θ0, θ1).

Via calculus of variations we ﬁnd that the function g∗(x) that extremizes L[ˆg] is given by [53]

0 =

= 2ˆg

dz p(x, z|θ)

−2

dz p(x, z|θ) g(x, z) ,

(22)

δF
δˆg

(cid:12)
(cid:12)
(cid:12)
(cid:12)g∗

(cid:90)

(cid:124)

(cid:123)(cid:122)
=p(x|θ)

(cid:125)

(cid:90)

therefore

g∗(x) =

(cid:90)

1
p(x|θ)

dz p(x, z|θ) g(x, z) .

(23)

We can make use of this general property in our problem in two ways. Identifying g(xe, ze) with

the joint likelihood ratios r(xe, zall,e|θ0, θ1) (which we can calculate!) and θ = θ1, we ﬁnd

g∗(x) =

(cid:90)

1
p(x|θ1)

p(x, z|θ0)
p(x, z|θ1)

dz p(x, z|θ1)

= r(x|θ0, θ1) .

(24)

By minimizing the squared loss

L[ˆr(x|θ0, θ1)] =

|r(xe, zall,e|θ0, θ1) − ˆr(xe|θ0, θ1)|2

(25)

1
N

(cid:88)

(xe,ze)∼p(x,z|θ1)

of a suﬃciently expressive function ˆr(x|θ0, θ1), we can therefore regress on the true likelihood
ratio [53]! This is illustrated in the right panel of Fig. 5. Note that to get the correct minimum, the
events (xe, ze) have to be sampled according to the denominator hypothesis θ1.

We can also identify g(xe, ze) in Eq. (22) with the scores t(xe, zall,e|θ), which can also be extracted

from the generator. In this case,

g∗(x) =

(cid:90)

1
p(x|θ)

dz ∇θp(x, z|θ) = t(x|θ) .

(26)

18

General likelihood-free Particle physics

(cid:88)

Quantity

Samples
Likelihood
Likelihood ratio
Score

{xe}
p(xe|θ)
r(xe|θ0, θ1)
t(xe|θ)

Latent state
Joint likelihood
Joint likelihood ratio
Joint score

{xe, ze}
p(xe, zall,e|θ)
r(xe, zall,e|θ0, θ1)
t(xe, zall,e|θ)

(cid:88)

∗
∗

(cid:88)

(cid:88)
(cid:88)

Table II: Availability of diﬀerent quantities from the generative process in the most general
likelihood-free setup vs. in the particle-physics scenario with the structure given in Eq. (2).
Asterisks (∗) denote quantities that are not immediately available, but can be regressed from the
corresponding joint quantity, as shown in Sec. III B.

Thus minimizing

L[ˆt(x|θ)] =

1
N

(cid:88)

(xe,ze)∼p(x,z|θ)

|t(xe, zall,e|θ) − ˆt(xe|θ)|2

(27)

of a suﬃciently expressive function ˆt(x|θ) allows us to regress on the score t(x|θ) [53].3 Now the
(xe, ze) have to be sampled according to θ. We summarize the availability of the (joint) likelihood,
likelihood ratio, and score in the most general likelihood-free setup and in particle physics processes
in Table II.

This is one of our key results and opens the door for powerful new inference methods. Particle
physics processes involve the highly complex eﬀects of parton shower, detector, and reconstruction,
modelled by a generative process with a huge latent space and an intractable likelihood. Still, the
speciﬁc structure of this class of processes allows us to calculate how much more or less likely a
generated event becomes when we move in the parameter space of the theory. We have shown that
by regressing on the joint likelihood ratios or scores extracted in this way, we can recover the actual
likelihood ratio or score as a function of the observables!

C. Strategies

Let us now combine the estimator structure discussed in Sec. III A with the diﬀerent quantities
available during training discussed in Sec. III B and deﬁne our strategies to estimate the likelihood
ratio. Here we restrict ourselves to an overview over the main ideas of the diﬀerent approaches. A
more detailed explanation and technical details can be found in Appendix A 2.

1. General likelihood-free case

Some approaches are designed for the most general likelihood-free scenario and only require the

samples {xe} from the generator:

3 A similar loss function (with a non-standard use of the term “score”) was used in Ref. [82], though the derivative is

taken with respect to x and, critically, the model did not involve marginalization over the latent variable z.

19

(28)

(30)

(31)

(32)

Histograms of observables: The traditional approach to kinematic analyses relies on one or two
kinematic variables v(x), manually chosen for a given process and set of parameters. Densities
ˆp(v(x)|θ) are estimated by ﬁlling histograms with generated samples, leading to the likelihood
ratio

ˆr(x|θ0, θ1) =

ˆp(v(x)|θ0)
ˆp(v(x)|θ1)

.

We use this algorithm point by point in θ0, but a morphing-based setup is also possible (see
Sec. II C 2). We discuss the histogram approach in more detail in Appendix A 2 a.

Approximate Frequentist Computation (Afc): Approximate Bayesian Computation (Abc)
is currently the most widely used method for likelihood-free inference in a Bayesian setup.
It allows to sample parameters from the intractable posterior, θ ∼ p(θ|x) = p(x|θ)p(θ)/p(x).
Essentially, Abc relies on the approximation of the likelihood function through a rejection
probability

prejection(x|θ) = K(cid:15)(v(x), v(xe)) ,

(29)

with xe ∼ p(x|θ), a kernel K(cid:15) that depends on a bandwidth (cid:15), and a suﬃciently low-dimensional
summary statistics v(x).

Inference in particle physics is usually performed in a frequentist setup, so this sampling
mechanism is not immediately useful. But we can deﬁne a frequentist analogue, which we
call “Approximate Frequentist Computation” (Afc). In analogy to the rejection probability
in Eq. 29, we can deﬁne a kernel density estimate for the likelihood function as

The corresponding likelihood ratio estimator is

ˆp(x|θ) =

K(cid:15)(v(x), v(xe)) .

1
N

(cid:88)

e

ˆr(x|θ0, θ1) =

ˆp(x|θ0)
ˆp(x|θ1)

.

We use this approach point by point in θ0 with a ﬁxed reference θ1. As summary statistics, we
use subsets of kinematic variables, similar to the histogram approach. We give more details
in Appendix A 2 b.

Calibrated classiﬁers (Carl4): As discussed in Sec. III B 1, the decision function ˆs(x|θ0, θ1) of
a classiﬁer trained to discriminate between samples generated according to θ0 from θ1 can be
turned into an estimator for the likelihood ratio

ˆr(x|θ0, θ1) =

1 − ˆs(x|θ0, θ1)
ˆs(x|θ0, θ1)

.

This is illustrated in the left panel of Fig. 5.

If the classiﬁer does not learn the optimal decision function of Eq. (17), but any mono-
tonic function of the likelihood ratio, a calibration procedure can improve the performance
signiﬁcantly. We will discuss this in Sec. III D below.

We implement this strategy point by point in θ0, as an agnostic parameterized classiﬁer
ˆr(x|θ0, θ1) that learns the dependence on both x and θ0, as well as a morphing-aware para-
meterized classiﬁer. More details are given in Appendix A 2 c.

4 Calibrated ratios of likelihoods

20

Neural conditional density estimators (Nde): Several other methods for conditional density
estimation have been proposed, often based on neural networks [36–42]. One particularly
interesting class of methods for density estimation is based on the idea of expressing the
target density as a sequence of invertible transformations applied to a simple initial density,
such as a Gaussian [43–46, 51]. The density in the target space is then given by the Jacobian
determinant of the transformation and the base density. A closely related and successful
alternative are neural autoregressive models [47–50], which factorize the target density as a
sequence of simpler conditional densities. Both classes of estimators are trained by maximizing
the log likelihood.

We leave a detailed discussion of these techniques for particle physics problems as well as an
implementation in our example process for future work.

2. Particle-physics structure

As we argued in Sec. III B, particle physics simulations let us extract the joint likelihood ratio
r(xe, ze|θ0, θ1) and the joint score t(xe, ze|θ0, θ1), giving rise to strategies tailored to this class of
problems:

Ratio regression (Rolr5): We can directly regress the likelihood ratio ˆr(x|θ0, θ1). As shown
in the previous section, the squared error loss between a function ˆr(xe|θ0, θ1) and the avail-
able joint likelihood ratio r(xe, ze|θ0, θ1) is minimized by the likelihood ratio r(x|θ0, θ1),
provided that the samples (xe, ze) are drawn according to θ1. Conversely, the squared error
of 1/r(xe, ze|θ0, θ1) with (xe, ze) ∼ p(x, z|θ0) is also minimized by the likelihood ratio. We
can combine these two terms into a combined loss function

L[ˆr(x|θ0, θ1)] =

ye |r(xe, ze|θ0, θ1) − ˆr(x|θ0, θ1)|2

1
N

(cid:32)

(cid:88)

(xe,ze,ye)

+ (1 − ye)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
r(xe, ze|θ0, θ1)

−

1
ˆr(x|θ0, θ1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2(cid:33)

(33)

with ye = 0 for events generated according to (xe, ze) ∼ p(x, z|θ0) and ye = 1 for (xe, ze) ∼
p(x, z|θ1). The factors of ye and (1 − ye) ensure the correct sampling for each part of the loss
functional. We illustrate this approach in the right panel of Fig. 5.

This strategy is again implemented point by point in θ0, in an agnostic parameterized setup,
as well as in a morphing-aware parameterized setup. We describe it in more detail in
Appendix A 2 d.

Carl + score regression (Cascal6): The parameterized Carl strategy outlined above learns
a classiﬁer decision function ˆs(x|θ0, θ1) as a function of θ0. If the classiﬁer is realized with
a diﬀerentiable architecture such as a neural network, we can calculate the gradient of this
function and of the corresponding estimator for the likelihood ratio ˆr(x|θ0, θ1) with respect
to θ0 and derive the estimated score

(cid:12)
(cid:12)
ˆt(x|θ0) = ∇θ log ˆr(x|θ, θ1)
(cid:12)
(cid:12)
(cid:12)θ0

= ∇θ log

1 − ˆs(x|θ0, θ1)
ˆs(x|θ0, θ1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)θ0

.

(34)

5 Regression on likelihood ratio
6 CARL and score approximate likelihood ratio

21

If the estimator is perfect, we expect this estimated score to minimize the squared error
with respect to the joint score data available from the simulator, following the arguments in
Sec. III B.

We can turn this argument around and use the available score data during the training.
Instead of training the classiﬁer just by minimizing the cross-entropy, we can instead sim-
ultaneously minimize the squared error on this derived score with respect to the true joint
score t(x, z|θ0, θ1). The combined loss function is given by

(cid:88)

(cid:34)
ye log ˆs(xe)+(1−ye) log(1− ˆs(xe))+α (1−ye) (cid:12)

(cid:12)t(xe, ze|θ0) − ˆt(xe|θ0)(cid:12)
2
(cid:12)

(35)

(cid:35)

L[ˆs] =

1
N

e

with ˆt(x|θ0) deﬁned in Eq. (34) and a hyperparameter α that weights the two pieces of
the loss function relative to each other. Again, ye = 0 for events generated according to
(xe, ze) ∼ p(x, z|θ0) and ye = 1 for (xe, ze) ∼ p(x, z|θ1), and the factors of ye and (1 − ye)
ensure the correct sampling for each part of the loss functional.

This strategy relies on the parameterized modeling of the likelihood ratio. We implement
both an agnostic version as well as a morphing-aware model. See Appendix A 2 e for more
details.

Neural conditional density estimators + score (Scandal7): In the same spirit as the Cas-
cal method, neural density estimators such as autoregressive ﬂows can be augmented with
score information. We have started to explore this class of algorithms in Ref. [53], but leave
a detailed study and the application to particle physics for future work.

Ratio + score regression (Rascal8): The same trick works for the parameterized Rolr ap-
If the regressor is implemented as a diﬀerentiable architecture such as a neural
proach.
network, we can calculate the gradient of the parameterized estimator ˆr(x|θ0, θ1) with respect
to θ0 and calculate the score

(cid:12)
(cid:12)
ˆt(x|θ0) = ∇θ log ˆr(x|θ, θ1)
(cid:12)
(cid:12)
(cid:12)θ0

.

(36)

Instead of training just on the squared likelihood ratio error, we can minimize the combined
loss

L[ˆr(x|θ0, θ1)] =

ye |r(xe, ze|θ0, θ1) − ˆr(xe|θ0, θ1)|2

1
N

(cid:34)

(cid:88)

(xe,ze,ye)

+ (1 − ye)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
r(xe, ze|θ0, θ1)

−

1
ˆr(xe|θ0, θ1)

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)

(cid:35)

+ α (1 − ye) (cid:12)

(cid:12)t(xe, ze|θ0) − ˆt(xe|θ0)(cid:12)
2
(cid:12)

(37)

with ˆt(x|θ0) deﬁned in Eq. (36) and a hyperparameter α. The likelihood ratios and scores
again provide complementary information as shown in the Fig. 7.

7 Score and neural density approximate likelihood
8 Ratio and score approximate likelihood ratio

22

Once more we experiment with both an agnostic parameterized model as well as a morphing-
aware version.

This technique uses all the available data from the simulator that we discussed in Sec. III B to
train an estimator of particularly high ﬁdelity. It is essentially a machine-learning version of
the Matrix Element Method. It replaces computationally expensive numerical integrals with
an upfront regression phase, after which the likelihood ratio can be evaluated very eﬃciently.
Instead of manually specifying simpliﬁed smearing functions, the eﬀect of parton shower and
detector is learned from full simulations. For more details on Rascal, see Appendix A 2 f.

Local score regression and density estimation (Sally9): In the local model approximation
discussed in Sec. III A 2, the score evaluated at some reference point θscore is the suﬃcient
statistics, carrying all the information on θ. A precisely estimated score vector (with one
component per parameter of interest) is therefore the ideal summary statistics, at least in the
neighborhood of the Standard Model or any other reference parameter point.

In the last section we argued that we can extract the joint score t(xe, ze|θscore) from the
simulator. We showed that the squared error between a function ˆt(x|θscore) and the joint
score is minimized by the intractable score t(x|θscore), as long as the events are sampled as
(xe, ze) ∼ p(x, z|θscore). We can thus use the augmented data to train an estimator ˆt(x|θscore)
for the score at the reference point.
In a second step, we can then estimate the likelihood ˆp(ˆt(x|θscore)|θ) with histograms, KDE,
or any other density estimation technique, yielding the likelihood ratio estimator

ˆr(x|θ0, θ1) =

ˆp (cid:0)ˆt(x|θscore) (cid:12)
ˆp (cid:0)ˆt(x|θscore) (cid:12)

(cid:12) θ0
(cid:12) θ1

(cid:1)
(cid:1) .

(38)

This particularly straightforward strategy is a machine-learning analogue of Optimal Ob-
servables that learns the eﬀect of parton shower and detector from data. After an upfront
regression phase, the analysis of an event only requires the evaluation of one estimator to
draw conclusions about all parameters. See Appendix A 2 g for more details.

Local score regression, compression to scalar, and density estimation (Sallino10): The
Sally technique compresses the information in a high-dimensional vector of observables x
into a lower-dimensional estimated score vector. But for measurements in high-dimensional
parameter spaces, density estimation in the estimated score space might still be computation-
ally expensive. Fortunately, the local model of Eq. (15) motivates an even more dramatic
dimensionality reduction to one dimension, independent of the number of parameters: Disreg-
arding the normalization constants, the ratio r(x|θ0, θ1) only depends on the scalar product
between the score and θ0 − θ1.
Given the same score estimator ˆt(x|θscore) developed for the Sally method, we can deﬁne
the scalar function

ˆh(x|θ0, θ1) ≡ ˆt(x|θSM ) · (θ0 − θ1) .

(39)

Assuming a precisely trained score estimator, this scalar encapsulates all information on the
likelihood ratio between θ0 and θ1, at least in the local model approximation. The likelihood

9 Score approximates likelihood locally
10 Score approximates likelihood locally in one dimension

23

Estimator versions

Loss function

Asymptotically exact

CE ML Ratio Score

Strategy

Histograms
Afc
Carl
Nde

Rolr
Cascal
Scandal
Rascal
Sally
Sallino

(cid:88)
(cid:88)
(cid:88)
((cid:88))
(cid:88)

PbP Param Aware
((cid:88))
((cid:88))
(cid:88)
((cid:88))
(cid:88)
(cid:88)
((cid:88))
(cid:88)
((cid:88))
((cid:88))

(cid:88)
((cid:88))
(cid:88)
(cid:88)
((cid:88))
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

Table III: Overview over the discussed measurement strategies. The ﬁrst three techniques can be
applied in the general likelihood-free setup, they only require sets of generated samples {xe}. The
remaining ﬁve methods are tailored to the particle physics structure and require the availability of
r(xe, ze|θ0, θ1) or t(xe, ze|θ0) from the generator, as discussed in Sec. III B. Brackets denote possible
variations that we have not implemented for our example process. In the Sally and Sallino
strategies, “estimator versions” refers to the density estimation step. In the loss function columns,
“CE” stands for the cross-entropy, “ML” for maximum likelihood, “ratio” for losses of the type
|r(x, z) − ˆr(x)|2, and “score” for terms such as |t(x, z) − ˆt(x)|2.

ratio can then be estimated as

ˆr(x|θ0, θ1) =

(cid:16)ˆh(x|θ0, θ1)
(cid:16)ˆh(x|θ0, θ1)

ˆp

ˆp

(cid:17)

(cid:17) ,

(cid:12)
(cid:12)
(cid:12) θ0
(cid:12)
(cid:12)
(cid:12) θ1

(40)

where the ˆp(ˆh) are simple univariate density estimators.

This method allows us to condense any high-dimensional observation x into a scalar function
without losing sensitivity, at least in the local model approximation. It thus scales exceptionally
well to problems with many theory parameters. We describe Sallino in more detail in
Appendix A 2 h.

Several other variants are possible, including other combinations of the loss functionals discussed

above. We leave this for future work.11

We summarize the diﬀerent techniques in Table III. With this plethora of well-motivated analysis
methods, the remaining key question for this paper is how well they do in practice, and which of
them (if any) should be used. This will be the focus of Sec. V. In the next sections, we will ﬁrst
discuss some important additional aspects of these methods.

11 Not all initially promising strategies work in practice. We experimented with an alternative strategy based on the
morphing structure in Eq. (6). Consider the case in which the training sample consists of a number of sub-samples,
each generated according to a morphing basis point θc. Then the morphing basis sample c used in the event
generation is a latent variable that we can use instead of the parton-level momenta z to deﬁne joint likelihood
ratios and joint scores. Regressing on these quantities should converge to the true likelihood ratio and score, in
complete analogy to the discussion in Sec. III B. But the joint ratios and scores deﬁned in this way span a huge
range, and the densities of the diﬀerent basis points are very similar. The convergence is therefore extremely slow
and the results based on this method suﬀer from a huge variance.

D. Calibration

While the likelihood ratio estimators described above work well in many cases, their performance
can be further improved by an additional calibration step. Calibration takes place after the “raw”
or uncalibrated estimators ˆrraw(x|θ0, θ1) have been trained. In general, it deﬁnes a function C with
the aim that ˆrcal = C(ˆrraw) provides a better estimator of the true likelihood ratio than ˆrraw. We
consider two diﬀerent approaches to deﬁning this function C, which we call probability calibration
and expectation calibration.

1. Probability calibration

Consider the Carl strategy, which trains a classiﬁer with a decision function s(x) as the basis
for the likelihood ratio estimation. Even if this classiﬁer can separate the two classes of events from
θ0 and θ1 well, its decision function ˆs(x|θ0, θ1) might not have a direct probabilistic interpretation:
it might be any approximately monotonic function of the likelihood ratio rather than the ideal
solution given in Eq. (17). In this case, the Carl approach requires calibration, an additional
transformation of the raw output ˆrraw = ˆr(x|θ0, θ1) into a calibrated decision function

where the densities ˆp(ˆrraw|θ) are estimated through a univariate density estimation technique such as
histograms. This calibration procedure does not only apply to classiﬁers, but to any other likelihood
ratio estimation strategy.

ˆrcal = C(ˆrraw) =

ˆp(ˆrraw|θ0)
ˆp(ˆrraw|θ1)

,

2. Expectation calibration

Consider some likelihood ratio r(x|θ0, θ1). The expectation value of this ratio assuming θ1 to be

true is given by

E[r(x|θ0, θ1)|θ1] =

dx p(x|θ1)

(cid:90)

p(x|θ0)
p(x|θ1)

= 1 .

A good estimator for the likelihood ratio should reproduce this property. We can numerically
approximate this expectation value by evaluating ˆr(x|θ, θ1) on a sample {xe} of N events drawn
according to θ1,

If a likelihood ratio estimator ˆrraw(x|θ, θ1) (which might be entirely uncalibrated or already

probability calibrated) does not satisfy this condition, we can calibrate it by rescaling it as

For a perfect estimator with ˆr(x|θ0, θ1) = r(x|θ0, θ1), we can even calculate the variance of the

numeric calculation of the expectation value in Eq. (43). We ﬁnd

ˆR(θ) =

1
N

(cid:88)

xe∼θ1

ˆr(xe|θ, θ1) ≈ 1 .

ˆrcal(x|θ, θ1) =

ˆrraw(x|θ, θ1)
ˆRraw(θ)

.

var[ ˆR(θ)] =

[E [ˆr(x|θ, θ1)|θ] − 1] ,

1
N

24

(41)

(42)

(43)

(44)

(45)

where N is the number of events used to calculate the expectation value R(θ), and the expectation
E [ˆr(x|θ, θ1)|θ] (under the numerator hypothesis!) can be calculated numerically.

This calibration strategy can easily improve classiﬁers that are oﬀ by some θ-dependent factor.
However, a few rare events xe with large ˆr(xe|θ, θ1) can dominate the expectation value. If these are
mis-estimated, the expectation calibration can actually degrade the performance of the estimator
on the bulk of the distribution with smaller ˆr(x|θ, θ1).

25

E.

Implementation

1. Neural networks

With the exception of the simple histogram and AFC methods, all strategies rely on a classiﬁer
ˆs(x|θ0, θ1), score regressor ˆt(x|θ0), or ratio regressor ˆr(x|θ0, θ1) that is being learnt from training
data. For our explicit example, we implement these functions as fully connected neural networks:
• In the point-by-point setup, the neural networks take the features x as input and models
log ˆr(x|θ0, θ1). For ratio regression, this is exponentiated to yield the ﬁnal output ˆr(x|θ0, θ1).
In the Carl strategy, the network output is transformed to a decision function

ˆs(x|θ0, θ1) =

1
1 + ˆr(x|θ0, θ1)

.

(46)

• In the agnostic parameterized setup, the neural networks take both the features x as well
as the numerator parameter θ0 as input and model log ˆr(x|θ0, θ1). In addition to the same
subsequent transformations as in the point-by-point case, taking the gradient of the network
output gives the estimator score.

• In the morphing-aware setup, the estimator takes both x and θ0 as input. The features x
are fed into a number of independent networks, one for each basis component c, that model
the basis ratios log rc(x). From θ0 the estimator calculates the component weights wc(θ0)
analytically. The components are then combined with Eq. (12). For the Carl approaches,
the output is again transformed with Eq. (46), and for the score-based strategies the gradient
of the output is calculated.
We visualize these architectures in Fig. 8.

All networks are implemented in shallow, regular, and deep versions with 2, 3, and 5 hidden layers
of 100 units each and tanh activation functions. They are trained with the Adam optimizer [83]
over 50 epochs with early stopping and learning rate decay. We implement them in keras [84] with
a TensorFlow [85] backend. Experiments modeling s rather than log r, with diﬀerent activation
functions, adding dropout layers, or using other optimizers and learning rate schedules have led to
a worse performance.

2. Training samples

Starting from the weighted event sample described in Sec. II D 2, we draw events (xe, ze) randomly
with probabilities given by the corresponding p(xe, ze|θ). Due to the form of the likelihood p(x, z|θ)
and due to technical limitations of our simulator, individual data points can carry large probabilities
p(xe, ze|θ), leading to duplicate events in the training samples. However, we enforce that there is
no duplication between training and evaluation samples, so this limitation can only degrade the
performance.

For the point-by-point setup, we choose 100 values of θ0, 5 of which are ﬁxed at the SM
(θ0 = (0, 0)) and at the corners of the considered parameter space, with the remaining 95 chosen

x

θ0

x

θ0

x

26

log ˆr

for each θ0

ˆs

ˆr

ˆt

ˆs

ˆr

ˆt

ˆs

ˆr

log ˆr

wc

log ˆr

log ˆrc

for each c

Figure 8: Schematic neural network architectures for point-by-point (top), agnostic parameterized
(middle), and morphing-aware parameterized (bottom) estimators. Solid lines denote dependencies
with learnable weights, dashed lines show ﬁxed functional dependencies.

27

(47)

randomly with a ﬂat prior over −1 ≤ θo ≤ 1. For each of these training points we sample 250 000
events according to θ0 and 250 000 events according to the reference hypothesis

θ1 = (0.393, 0.492)T .

For the parameterized strategies, we compare three diﬀerent training samples, each consisting of
107 events:

Baseline: For 1000 values of θ0 chosen randomly in θ space, we draw 5000 events according to θ0

and 5000 events according to θ1 given in Eq. (47).

Random θ: In this sample, the value of θ0 is drawn randomly independently for each event. Again

we use a ﬂat prior over θ0 ∈ [−1, 1]2.

Morphing basis: For each of the 15 basis hypotheses θi from the morphing procedure, we generate

333 000 events according to θ0 = θi and 333 000 according to θ1.

Finally, for the local score regression model we use a sample of 107 events drawn according to

the SM.

Our evaluation sample consists of 50 000 events drawn according to the SM. We evaluate the
likelihood ratio for these events for a total of 1016 values of θ0, 1000 of which are the same as those
used in the baseline training sample. Again we ﬁx θ1 as in Equation (47).

Each event is characterized by 42 features:
• the energies, transverse momenta, azimuthal angles, and pseudo-rapidities of all six particles

• the energies, transverse momenta, azimuthal angles, pseudo-rapidities, and invariant mass of
the four-lepton system as well as the two-lepton systems that reconstruct the two Z bosons;
and

• the invariant mass, separation in pseudorapidity, and separation in azimuthal angle of the

in the ﬁnal state;

di-jet system.

The derived variables in the feature set help the neural networks pick up the relevant features faster,
though we did not ﬁnd that their choice aﬀects the performance signiﬁcantly.

3. Calibration and density estimation

We calibrate the classiﬁers for our example process with probability calibration as described
in Sec. III D 1. We determine the calibration function C(r) with isotonic regression [86], which
constrains C(r) to be monotonic. Experiments with other regression techniques based on histograms,
kernel density estimation, and logistic regression did not lead to a better performance. We apply
the calibration point by point in θ0. It is based on an additional event sample that is independent
of the training and evaluation data. The same events are used to calibrate each value of θ, with
an appropriate reweighting. This strategy to minimize variance is based on the availability of the
parton-level likelihood function p(z|θ).

The techniques based on local score regression require the choice of a reference point to evaluate
the score. For the EFT problem, the natural choice is θscore = θSM = (0, 0)T . In the Sally approach,
we perform the density estimation based on two-dimensional histograms of the estimated score at
the SM, point by point in θ0. For the Sallino technique, we use a one-dimensional histograms of
ˆh(x|θSM), point by point in θ0.

28

Figure 9: Uncertainty ∆ log ˆr(xe|θ, θ1) of morphing-aware estimators due to uncertainties
∆ log ˆrc(xe|θ, θ1) on the individual basis ratios as a function of θ. We ﬁx θ1 as in Eq. (47) and show
one random event xe, the results for other events are very similar. We assume iid Gaussian
uncertainties on the log ˆrc(x|θ, θ1) and use Gaussian error propagation. The white dots show the
position of the basis points θc. Small uncertainties in the individual basis estimators ˆrc(xe|θ, θ1) are
signiﬁcantly increased due to the large morphing weights and can lead to large errors of the
combined estimator ˆr(xe|θ, θ1).

F. Challenges and diagnostics

1. Uncertainties

Even the most evolved and robust estimators will have some deviations from the true likelihood
ratio, which should be taken into account in an analysis as an additional modeling uncertainty.
Most of the estimators developed above converge to the true likelihood ratio in the limit of inﬁnite
training and calibration samples. But with ﬁnite statistics, there are diﬀerent sources of variance
that aﬀect some strategies more than others.

Consider the traditional histogram approach.

In the point-by-point version, each separate
estimator ˆr(x|θ0, θ1) is trained on the small subset of the data generated from a speciﬁc value of
θ0, so the variance from the ﬁnite size of the training data, i. e. the statistical uncertainty from the
Monte-Carlo simulation, is large. At θ0 values between the training points, there are additional
sources of uncertainty from the interpolation. On the other hand, morphing-aware histograms use
all of the training data to make predictions at all points, and since the dependence on θ0 is known,
the interpolation is exact. But the large morphing weights wc(θ0) and the cancellations between
them mean that even small ﬂuctuations in the individual basis histograms can lead to huge errors
on the combined estimator.

Similar patterns hold for the ML-based inference strategies. The point-by-point versions suﬀer
from a larger variance due to small training samples at each point and interpolation uncertainties.
The agnostic parameterized models have more statistics available, but have to learn the more
complex full statistical model including the dependence on θ0. The morphing-aware versions make
maximal use of the physics structure of the process and all the training data, but large morphing
weights can dramatically increase the errors of the individual component estimators log ˆrc(x). We
demonstrate this for our example process in Fig. 9: in some regions of parameter space, in particular
far away from the basis points, the errors on a morphing-aware estimator log ˆr(x|θ, θ1) can easily

29

be 100 times larger than the individual errors on the component estimators log ˆrc(x). The θ0
dependence on this error depends on the choice of the basis points, this uncertainty can thus be
somewhat mitigated by optimizing the basis points or by combining several diﬀerent bases.

2. Diagnostics

After discussing the sources of variance, let us now turn towards diagnostic tools that can help
quantify the size of estimator errors and to assign a modeling uncertainty for the statistical analysis.
These methods are generally closure tests: we can check the likelihood ratio estimators for some
expected behaviour, and use deviations either to correct the results (as in the calibration procedures
described in Sec. III D), to deﬁne uncertainty bands, or to discard estimators altogether. We suggest
the following tests:

Ensemble variance: Repeatedly generating new training data (or bootstrapping the same training
sample) and training the estimators again gives us an ensemble of predictions {ˆrr(x|θ0, θ1)}.
We can use the ensemble variance as a measure of uncertainty of the prediction that is due to
the variance in the training data and random seeds used during the training.

Reference hypothesis variation: Any estimated likelihood ratio between two hypotheses θA, θB

ˆr(x|θA, θB) =

ˆr(x|θA, θ1)
ˆr(x|θB, θ1)

(48)

should be independent of the choice of the reference hypothesis θ1 used in the estimator ˆr.
Training several independent estimators with diﬀerent values of θ1 thus provides another
check of the stability of the results [34].

Much like the renormalization and factorization scale variations that are ubiquitous in particle
physics calculations, this technique does not have a proper statistical interpretation in terms
of a likelihood function. We can still use it to qualitatively indicate the stability of the
estimator under this hyperparameter change.

Ratio expectation: As discussed above, the expectation value of the estimated likelihood ratio
assuming the denominator hypothesis should be very close to one. We can numerically
calculate this expectation value ˆR(θ), see Eq. (43).
In Sec. III D 2 we argued that this
expectation value can be used to calibrate the estimators, but that this calibration can
actually decrease the performance in certain situations.

If expectation calibration is used, the calibration itself has a non-zero variance from the ﬁnite
sample size used to calculate the expectation value ˆR(θ). As we pointed out in Sec. III D 2,
we can calculate this source of statistical uncertainty, at least under the assumption of a
perfect estimator with ˆr(x|θ0, θ1) = r(x|θ0, θ1). The result given in Eq. (45) provides us with
a handle to calculate the statistical uncertainty of this calibration procedure from the ﬁnite
size of the calibration sample. Note that for imperfect estimators, the variance of R[ ˆR] may
be signiﬁcantly larger.

Independent of whether expectation calibration is part of the estimator, the deviation of the
expectation ˆR(θ) from one can serve as a diagnostic tool to check for mismodelling of the
estimator. We can take log ˆR(θ) as a measure of the uncertainty of log ˆr(x|θ, θ1). As in the
case of the reference hypothesis variation, there is no consistent statistical interpretation of
this uncertainty, but this does not mean that it is useless as a closure test.

30

(49)

Reweighting distributions: A good estimator ˆr(x|θ0, θ1) should satisfy

p(x|θ0) ≈ ˆr(x|θ0, θ1) p(x|θ1) .

We cannot evaluate the p(x|θ0) to check this relation explicitly. However, we can sample
events {xe} from them. This provides another diagnostic tool [34]: we can draw a ﬁrst sample
as xe ∼ p(xe|θ0), and draw a second sample as xe ∼ p(xe|θ1) and reweight it with ˆr(xe|θ0, θ1).
For a good likelihood ratio estimator, the two samples should have similar distributions. This
can easily be tested by training a discriminative classiﬁer between the samples. If a classiﬁer
can distinguish between the sample from the ﬁrst hypothesis and the sample drawn from
the second hypothesis and reweighted with the estimated likelihood ratio, then ˆr(x|θ0, θ1) is
not a good approximation of the true likelihood ratio r(x|θ0, θ1). Conversely, if the classiﬁer
cannot separate the two classes, the classiﬁer is either not eﬃcient, or the likelihood ratio is
estimated well.

Note that passing these closure tests is not a guarantee for a good estimator of the likelihood ratio.
In Sec. IV B we will discuss how we can nevertheless derive exclusion limits that are guaranteed to
be statistically correct, i. e. that might not be optimal, but are never wrong.

In our example process, we will use a combination of the ﬁrst two ideas of this list: we will create
copies of estimators with independent training samples and random seeds during training, as well
as with diﬀerent choices of the reference hypothesis θ1, and analyse the median and envelope of the
predictions.

IV. LIMIT SETTING

The ﬁnal objective of any EFT analysis are exclusion limits on the parameters of interest at a
given conﬁdence level. These can be derived in one of two ways. The Neyman construction based
on toy experiments provides a generic and fail-safe method, we will discuss it in Sec. IV B. But since
the techniques developed in the previous section directly provide an estimate for the likelihood ratio,
we can alternatively apply existing statistical methods for likelihood ratios as test statistics. This
much more eﬃcient approach will be the topic of the following section.

A. Asymptotics

Consider the test statistics

q(θ) = −2

log r(xe|θ, ˆθ) = −2

log r(xe|θ, θ1) − log r(xe|ˆθ, θ1)

(50)

(cid:17)

(cid:88)

e

(cid:88)

(cid:16)

e

for a ﬁxed number N of observed events {xe} with the maximum-likelihood estimator

ˆθ = arg max

θ

(cid:88)

e

log r(xe|θ, θ1) .

(51)

In the asymptotic limit, the distribution according to the null hypothesis, p(q(θ)|θ), is given by a
chi-squared distribution. The number of degrees of freedom k is equal to the number of parameters
θ. This result by Wilks [87] allows us to translate an observed value qobs(θ) directly to a p-value
that measures the conﬁdence with which θ can be excluded:

pθ ≡

dq p(q|θ) = 1 − Fχ2 (qobs(θ)|k)

(52)

(cid:90) ∞

qobs(θ)

31

(53)

(54)

where Fχ2(x|k) is the cumulative distribution function of the chi-squared distribution with k degrees
of freedom. In our example process k = 2, for which this simpliﬁes to

In particle physics it is common practice to calculate “expected exclusion contours” by calculating
the expected value of qobs(θ) based on a large “Asimov” data set generated according to some θ(cid:48) [88].
With Eq. (52) this value is then translated into an expected p-value.12

In practice we cannot access the true likelihood ratio deﬁned on the full observable space and
thus also not q(θ). But if the error of an estimator ˆr(x|θ0, θ1) compared to the true likelihood ratio
is negligible, we can simply calculate

pθ = exp

−

(cid:18)

(cid:19)

.

qobs(θ)
2

ˆq(θ) = −2

log ˆr(xe|θ, ˆθ)

(cid:88)

e

with maximum likelihood estimator ˆθ also based on the estimated likelihood ratio. The p-value can
then be read oﬀ directly from the estimator output, substituting ˆq for q in Eq. (52).

Under this assumption and in the asymptotic limit, constructing conﬁdence intervals is thus
remarkably simple and computationally cheap: after training an estimator ˆr(x|θ, θ1) as discussed
in the previous section, the observed events {xe} are fed into the estimator for each value of θ on
some parameter grid. From the results we can read oﬀ the maximum likelihood estimator ˆq(θ) and
calculate the observed value of the test statistics ˆq(θ) for each θ. Equation (52) then translates
these values to p-values, which can then be interpolated between the tested θ points to yield the
ﬁnal contours.

To check whether these asymptotic properties apply to a likelihood ratio estimator, we can
use the diagnostic tools discussed in Sec. III F 2. In addition, we can explicitly check whether the
distribution of q(θ) actually follows a chi-squared distribution by generating toy experiments for
a few θ points. If it does, the asymptotic results are likely to apply at other points in parameter
space as well. If the variance of the toy experiments is larger than expected from the chi-squared
distribution, the residual variance may be taken as an error estimate on the estimator prediction.

B. Neyman construction

Rather than relying on the asymptotic properties of the likelihood ratio test, we can construct
the distribution of a test statistic with toy experiments. This is computationally more expensive,
but useful if the number of events is not in the asymptotic regime or if the uncertainty of the
estimators cannot be reliably quantiﬁed. Constraints derived in this way are conservative: even if
the likelihood ratio is estimated poorly, the resulting contours might not be optimal, but they are
never wrong (at a speciﬁed conﬁdence level).

12 For k = 1, this standard procedure reproduces the median expected p-value. Note however that this is not true
anymore for more than one parameter of interest. In this case, the median expected p-value can be calculated
based on a diﬀerent, but not commonly used, procedure. It is based on the fact that the distribution of q according
to an alternate hypothesis, p(q(θ)|θ(cid:48)), is given by a non-central chi-squared distribution [89]. In the asymptotic
limit, the non-centrality parameter is equal to the expectation value E[q(θ)|θ(cid:48)] [88]. This allows us to calculate for
instance the median expected q(θ) assuming some value θ(cid:48) based on the Asimov data. Combining all the pieces,
the median expected p-value pθ with which θ can be excluded under the assumption that θ(cid:48) is true is then given by
pexpected from θ(cid:48)
2 |k, E[q(θ)|θ(cid:48)])|k), where Fχ2 (x|k) is the cumulative distribution function for the
= 1 − Fχ2 (F −1
( 1
χ2
θ
chi-squared distribution with k degrees of freedom and F −1
(p|k, Λ) is the inverse cumulative distribution function
χ2
for the non-central chi-squared distribution with k degrees of freedom and non-centrality parameter Λ.

nc

nc

A good choice for the test statistics is the estimated proﬁle log likelihood ratio ˆq(θ) given
in Eq. (54), which allows us to compare the distribution of the toy experiments directly to the
asymptotic properties discussed in the previous section. However, its construction requires ﬁnding the
maximum likelihood estimator for every toy experiment. This increases the necessary computation
time substantially, especially in high-dimensional parameter spaces. An alternative test statistics is
the estimated log likelihood ratio with respect to some ﬁxed hypothesis, which need not be identical
to the reference denominator θ1 used in the likelihood ratio estimators. In the EFT approach, the
natural choice is the estimated likelihood ratio with respect to the SM,

ˆq(cid:48)(θ) ≡ −2

log ˆr(xe|θ, θSM ) .

(cid:88)

e

Using this test statistic rather than the proﬁle likelihood ratio deﬁned in Eq. (50) is expected to
lead to stronger constraints if the true value of θ is close to the SM point, as expected in the EFT
approach, and less powerful bounds if the true value is substantially diﬀerent from the SM.

In practice we can eﬃciently calculate the distribution of ˆq((cid:48))(θ) after n events by ﬁrst calculating

the distribution of ˆq((cid:48))(θ) for one event and convolving the result with itself (n − 1) times.

C. Nuisance parameters

The tools developed above also support nuisance parameters, for instance to model systematic
uncertainties in the theory calculation or the detector model. One strategy is to train parameterized
estimators on samples generated with diﬀerent values of the nuisance parameters ν and let them
learn the likelihood ratio

with its dependence on the nuisance parameters. As test statistics we can then use the estimator
version of the usual proﬁle log likelihood ratio,

ˆr(x|θ0, θ1; ν0, ν1) ≡

ˆp(x|θ0; ν0)
ˆp(x|θ1; ν1)

ˆq(θ) = −2

log

r

xe



(cid:16)

(cid:12)
(cid:12)θ, ˆθ; ˆˆν, ˆν
(cid:12)

(cid:17) q(ˆˆν)

q(ˆν)





(cid:88)

e

with constraint terms q(ν),

(cid:20)
ˆr(xe|θ, θ1; ν, ν1)

log

and

ˆˆν = arg max

ν

(cid:88)

e
(cid:88)

(cid:20)

(θ,ν)

e

(cid:17)

(cid:16)ˆθ, ˆν

= arg max

log

ˆr(xe|θ, θ1; ν, ν1)

(cid:21)

(cid:21)

q(ν)
q(ν1)

q(ν)
q(ν1)

,

.

The proﬁle log likelihood ratio has two advantages: it is pivotal, i. e. its value and its distribution do
not depend on the value of the nuisance parameter, and it has the asymptotic properties discussed
in Sec. IV A.

Similarly, we can train the score including nuisance parameters,

(cid:12)
(cid:12)
ˆt(x|θ0; ν0) = ∇(θ,ν) log [ˆp(x|θ; ν)q(ν)]
(cid:12)
(cid:12)
(cid:12)θ0,ν0

.

32

(55)

(56)

(57)

(58)

(59)

(60)

33

(61)

(62)

If the constraints q(ν) limit the nuisance parameters to a relatively small region around some ν0,
i. e. a range in which the shape of the likelihood function does not change signiﬁcantly, the Sally
and Sallino methods seem particularly appropriate.

Finally, an adversarial component in the training procedure lets us directly train pivotal estimators
ˆr(x|θ0, θ1), i. e. that do not depend on the value of the nuisance parameters [90]. Compared to
learning the explicit dependence on ν, this can dramatically reduce the dimensionality of the
parameter space as early as possible, and does not require manual proﬁling. However, the estimators
will generally not converge to the proﬁle likelihood ratio, so its asymptotic properties do not apply
and limit setting requires the Neyman construction.

V. RESULTS

We now apply the analysis techniques to our example process of WBF Higgs production in the
4(cid:96) decay mode. We ﬁrst study the idealized setup discussed in Sec. II D 2, in which we can assess
the techniques by comparing their predictions to the true likelihood ratio. In Sec. V B we then
calculate limits in a more realistic setup.

A.

Idealized setup

1. Quality of likelihood ratio estimators

Table IV summarizes the performance of the diﬀerent likelihood ratio estimators in the idealized
setup. For 50 000 events {xe} drawn according to the SM, we evaluate the true likelihood ratio
r(xe|θ0, θ1) as well as the estimated likelihood ratios ˆr(xe|θ0, θ1) for 1000 values of θ0 sampled
randomly in [−1, 1]2. As a metric we use the expected mean squared error on the log likelihood
ratio

ε[ˆr(x)] =

π(θ0)

log ˆr(xe|θ0, θ1) − log r(xe|θ0, θ1)

(cid:17)2(cid:21)

.

(cid:88)

θ0

(cid:20)(cid:16)

(cid:88)

1
N

e

The 1000 tested values of θ0 are weighted with a Gaussian prior

π(θ) =

N(cid:0)||θ||(cid:12)

(cid:12)0, 2 · 0.22(cid:1)

1
Z

with normalization factor Z such that (cid:80)
π(θ) = 1. In addition we show the expected trimmed
mean squared error, which truncates the top 5% and bottom 5% of events for each θ. This allows
us to analyse the quality of the estimators for the bulk of the phase space without being dominated
by a few outliers. In Table IV and in the ﬁgures of this section, we only show results for a default
set of hyperparameters for each likelihood ratio estimators. These default setups are deﬁned in
Appendix A 2. Results for other hyperparameter choices are given in Appendix A 3.

θ0

The best results come from parameterized estimators that combine either a classiﬁer decision
function or ratio regression with regression on the score: the Cascal and Rascal strategies provide
very accurate estimates of the log likelihood ratio. Sally, parameterized Rolr, and parameterized
Carl perform somewhat worse. For Carl and Rolr, parameterized estimators consistently perform
better then the corresponding point-by-point versions. All these ML-based strategies signiﬁcantly
outperform the traditional one- or two-dimensional histograms and the Approximate Frequentist
Computation.

The morphing-aware versions of the parameterized estimators lead to a poor performance,
comparable or worse than the two-dimensional histogram approach. As anticipated in Sec. III F,

34

Expected MSE

All Trimmed

Figures

(cid:88)

Strategy

Setup

pT,j1, ∆φjj
pT,j1
∆φjj
pT,j1, ∆φjj
pT,j1, mZ2, mjj, ∆ηjj, ∆φjj

Histogram

Afc

Carl (PbP)
Carl (parameterized)

Carl (morphing-aware)

Rolr (PbP)
Rolr (parameterized)

Rolr (morphing-aware)

Sally
Sallino

Cascal (parameterized)

Cascal (morphing-aware) Baseline

Rascal (parameterized)

Rascal (morphing-aware) Baseline

PbP
Baseline
Random θ
Baseline
Random θ
Morphing basis

PbP
Baseline
Random θ
Baseline
Random θ
Morphing basis

Baseline
Random θ

Random θ
Morphing basis

Baseline
Random θ

Random θ
Morphing basis

(cid:88)

0.0111 Fig. 12
0.0026
0.0028
0.0200 Fig. 12
0.0226
0.0618

0.056
0.088
0.160
0.059
0.078

0.030
0.012
0.012
0.076
0.086
0.156

0.005
0.003
0.003
0.024
0.022
0.130

0.013
0.021

0.001
0.001
0.136
0.092
0.040

0.001
0.001
0.125
0.132
0.031

0.0106
0.0230
0.0433
0.0091
0.0101

0.0022
0.0017
0.0014
0.0063
0.0052
0.0485

0.0002
0.0006

0.0002
0.0002
0.0427
0.0268
0.0081

0.0004
0.0004
0.0514
0.0539
0.0072

(cid:88)

(cid:88)

(cid:88)

(cid:88)

Table IV: Performance of the diﬀerent likelihood ratio estimation techniques in our example process.
The metrics shown are the expected mean squared error on the log likelihood ratio with and
without trimming, as deﬁned in the text. Checkmarks in the last column denotes estimators shown
in the following ﬁgures. Here we only give results based on default settings, which are deﬁned in
Appendix A 2. An extended list of results that covers more estimators is given in Appendix A 3.

the large weight factors and the sizable cancellations between them blow up small errors on the
estimation of the individual basis estimators ˆri(x) to large errors on the combined estimator.

We ﬁnd that probability calibration as discussed in Sec. III D 1 improves the results in almost
all cases, in particular for the Carl method. An additional step of expectation calibration (see
Sec. III D 2) after the probability calibration does not lead to a further improvement, and in fact often
increases the variance of the estimator predictions. We therefore only use probability calibration for
the results presented here.

The choice of the training sample is less critical, with nearly identical results between the baseline
and random θ samples. For the Carl approach, shallow networks with two hidden layers perform
better, while Rolr works best for three hidden layers and the score-based strategies beneﬁt from a
deeper network with ﬁve hidden layers.

35

Figure 10: True vs. estimated likelihood ratios for a benchmark hypothesis θ0 = (−0.5, −0.5)T .
Each dot corresponds to one event xe. The Cascal (right, red), Rascal (right, orange), and
Sally (middle, blue) techniques can predict the likelihood ratio extremely accurately over the
whole phase space. All new techniques clearly lead to more precise estimates than the traditional
histogram approach (left, orange).

Figure 11: True vs. estimated expected log likelihood ratio. Each dot corresponds to one value of
θ0, where we take the expectation over x ∼ p(x|θSM). The new techniques are less biased than the
histogram approach (left, orange).

In Fig. 10 we show scatter plots between the true and estimated likelihood ratios for a ﬁxed
hypothesis θ0. The likelihood ratio estimate from histograms of observables is widely spread around
the true likelihood ratio, reﬂecting the loss of information from ignoring most directions in the
observable space. Carl performs clearly better. Rolr and Sally oﬀer a further improvement.
Again, the best results come from the Cascal and Rascal strategies, both giving predictions that
are virtually one-to-one with the true likelihood ratio.

We go beyond a single benchmark point θ0 in Fig. 11. This scatter plot compares true and
estimated likelihood ratios for diﬀerent values of θ0, taking the expectation value over x. We
ﬁnd that the Carl, Rolr, Cascal, and Rascal approaches converge to the correct likelihood
ratio in this expectation value. For the Sally and Sallino techniques we ﬁnd larger deviations,
pointing towards the breakdown of the local model approximation. Much more obvious is the loss

36

Figure 12: Comparison of the point-by-point, parameterized, and morphing-aware versions of
Carl. Top left: True vs. estimated likelihood ratios for a benchmark hypothesis
θ0 = (−0.5, −0.5)T , as in Fig. 10. Each dot corresponds to one event xe. Top right: True vs.
estimated expected log likelihood ratio, as in Fig. 11. Each dot corresponds to one value of θ0,
where we take the expectation over x ∼ p(x|θSM). The parameterized estimator outperforms the
point-by-point one and particularly the morphing-aware version.

of information in the traditional histogram approach, which is clearly not asymptotically exact.

The point-by-point, agnostic parameterized, and morphing-aware versions of the Carl strategy
are compared in Fig. 12. As expected from Table IV, the parameterized strategy performs better
than the point-by-point version, and both are clearly superior to the morphing-aware estimator.

2. Eﬃciency and speed

With inﬁnite training data, many of the algorithms should converge to the true likelihood ratio.
But generating training samples can be expensive, especially when a full detector simulation is
used. An important question is therefore how much training data the diﬀerent techniques require
to perform well. In Fig. 13 we show the performance as a function of the training sample size.

The Sally approach performs very well even with very little data. Its precision stops improving
eventually, showing the limitations of the local model approximation. For the other methods we ﬁnd
that the more information a technique uses, the less training data points it requires. The Rascal
technique utilizes the most information from the simulator, leading to an exceptional performance
with training samples of approximately 100 000 events. This is in contrast to the most general Carl
method, which does not use any of the extra information from the simulator and requires a two
orders of magnitude larger training sample for a comparable performance.

In Fig. 14 we show the evolution of the likelihood estimation error and the cross-entropy of the
classiﬁcation problem during the training of the parameterized estimators. For comparison, we also
show the optimal metrics based on the true likelihood ratio, and the results of the two-dimensional
histogram approach. Once again we see that either Cascal or Rascal leads to the best results.
This result also holds true for the cross entropy, hinting that the techniques we use to measure
continuous parameters might also improve the power of estimators in discrete classiﬁcation problems.
Note that the Carl approach is more prone to overﬁtting than the others, visible as a signiﬁcant

37

Figure 13: Performance of the techniques as a function of the training sample size. As a metric, we
show the mean squared error (left) and trimmed mean squared error on log r(r|θ0, θ1) weighted with
a Gaussian prior, as discussed in the text. Note that we do not vary the size of the calibration data
samples. The number of epochs are increased such that the number of epochs times the training
sample size is constant, all other hyperparameters are kept constant. The Sally method works
well even with very little data, but plateaus eventually due to the limitations of the local model
approximation. The other algorithms learn faster the more information from the simulator is used.

Evaluation time [µs]

per xe

per xe and θ0

Algorithm

Histogram
Carl
Sally
Rolr
Cascal
Rascal

25.4

0.2
19.7
0.1
19.7
25.1
21.7

Table V: Computation times of evaluating ˆr(x|θ0, θ1) in the diﬀerent algorithms. We distinguish
between steps that have to be calculated once per x and and those which have to be repeated for
every evaluated value of θ0. These numbers are from one run of our algorithms with default
settings on the NYU HPC cluster on machines equipped with Intel Xeon E5-2690v4 2.6GHz CPUs
and NVIDIA P40 GPUs with 24 GB RAM, using a batch of 50 000 events {xe}, and taking the
mean over 1 017 values of θ0. The local score regression method and the traditional histogram
method are particularly fast. But all techniques are many orders of magnitude faster to evaluate
than the matrix element method or optimal observables.

38

diﬀerence between the metrics evaluated on the training and validation samples.

Equally important to the training eﬃciency is the computation time taken up by evaluating
the likelihood ratio estimators ˆr(xe|θ0, θ1). We compare example evaluation times in Table V. The
traditional histogram approach takes the shortest time. But all tested algorithms are very fast:
the likelihood ratio for ﬁxed hypotheses (θ0, θ1) for 50 000 events {xe} can always be estimated
in around one second or less. The local score regression method is particularly eﬃcient, since the
estimator ˆt(x|θscore, θ1) has to be evaluated only once to estimate the likelihood ratio for any value
of θ0. Only the comparably fast step of density estimation has to be repeated for each tested value
of θ0.

So after investing some training time upfront, all the measurement strategies developed here
can be evaluated on any events with very little computational cost and amortize quickly. While
this is not the focus of our paper, note that this distinguishes our approaches from the Matrix
Element Method and Optimal Observable techniques. These well-established methods require the
computationally expensive evaluation of complicated numerical integrals for every evaluation of the
likelihood ratio estimator.

3. Physics results

The most important result of an EFT measurement are observed and expected exclusion contours,
either based on asymptotics or toy experiments. In the asymptotic approach, the expected contours
are determined just by the likelihood ratio evaluated on a large “Asimov” data set, as described in
Sec. IV A. Figure 15 shows this expected log likelihood ratio in the SM after 36 events over a one-
dimensional slice of the parameter space. In Fig. 16 we show the corresponding expected exclusion
limits on the two Wilson coeﬃcients. To estimate the robustness of the likelihood ratio estimators,
each algorithm is run ﬁve times with diﬀerent choices of the reference hypothesis; independent
training, calibration, and evaluation samples; and independent random seeds during training. The
lines show the median of the ﬁve replicas, while the shaded bands show the envelope. While this
error band does not have a clear statistic interpretation, it does provide a diagnostic tool for the
variance of the estimators.

A traditional histogram-based analysis of jet pT and ∆φjj leads to overly conservative results.
It is interesting to note that this simple analysis works reasonably well in the region of parameter
space with fW > 0 and fW W > 0, which is exactly the part of parameter space where informative
high-energy events interfere mostly constructively with the SM amplitude. In the fW < 0 region of
parameter space, destructive interference dominates in the important regions of phase space with
large momentum transfer. An extreme example is the “amplitude-through-zero” eﬀect shown in the
left panel of Fig. 2. Simple histograms with a rough binning generally lead to a poor estimation of
the likelihood ratio in such complicated kinematic signatures.

We ﬁnd that the new ML-based strategies allow us to place visibly tighter constraints on the
Wilson coeﬃcients than the doubly diﬀerential histogram. In particular the Carl + score and
regression + score estimators lead to exclusion contours that are close to the contours based on
the true likelihood ratio. In this analysis based on asymptotics, however, it is possible for the
estimated contours to be slightly too tight, wrongly marking parameter regions as excluded at a
given conﬁdence level. This problem can be mitigated by proﬁling over systematic uncertainties
assigned to the likelihood ratio estimates.

Exclusion limits based on the Neyman construction do not suﬀer from this issue: contours
derived in this way might be not optimal, but they are never wrong. We generate toy experiments
to estimate the distribution of the likelihood ratio with respect to the SM for individual events.
Repeatedly convolving this single-event distribution with itself, we ﬁnd the distribution of the

39

Figure 14: Learning curve of the parameterized models. The solid lines show the metrics evaluated
on the training sample, the dots indicate the performance on the validation sample. Note that
these numbers are not comparable to the metrics in Table IV and Fig. 13, which are weighted with
the prior in Eq. (62). These results also do not include calibration. Left: Mean squared error of
log ˆr(x|θ0, θ1). Right: binary cross-entropy of the classiﬁcation based on ˆs(x|θ0, θ1) between the
numerator and denominator samples. The solid grey line shows the “optimal” performance based
on the true likelihood ratio. The Cascal and Rascal techniques converge to a performance close
to the theoretic optimum. The Carl approach (green), based on minimizing the cross entropy,
shows signs of overﬁtting. All machine-learning-based methods outperform traditional histograms
(dashed orange).

Figure 15: Expected likelihood ratio with respect to the Standard Model along a one-dimensional
slice of the parameter space. We assume 36 observed events and the SM to be true. For each
estimator, we generate ﬁve sets of predictions with diﬀerent reference hypotheses, independent data
samples, and diﬀerent random seeds. The lines show the median of this ensemble, the shaded error
bands the envelope. All machine-learning-based methods reproduce the true likelihood function
well, while the doubly diﬀerential histogram method underestimates the likelihood ratio in the
region of negative Wilson coeﬃcients.

40

Figure 16: Expected exclusion contours based on asymptotics at 68% CL (innermost lines), 95% CL,
and 99.7% CL (outermost lines). We assume 36 observed events and the SM to be true. As test
statistics, we use the proﬁle likelihood ratio with respect to the maximum-likelihood estimator. For
each estimator, we generate ﬁve sets of predictions with diﬀerent reference hypotheses, independent
data samples, and diﬀerent random seeds. The lines show the median of this ensemble, the shaded
error bands the envelope. The new techniques based on machine learning, in particular the Cascal
and Rascal techniques, lead to expected exclusion contours very close to those based on the true
likelihood ratio. An analysis of a doubly diﬀerential histogram leads to much weaker bounds.

Figure 17: Expected exclusion contours based on the Neyman construction with toy experiments at
68% CL (innermost lines), 95% CL, and 99.7% CL (outermost lines). We assume 36 observed
events and the SM to be true. As test statistics, we use the likelihood ratio with respect to the SM.
All machine-learning-based methods let us impose much tighter bounds on the Wilson coeﬃcients
than the traditional histogram approach (left, dotted orange). The Neyman construction
guarantees statistically correct results: no contour based on estimators excludes parameter points
that should not be excluded. The expected limits based on the Cascal (right, lavender) and
Rascal (right, red) techniques are virtually indistinguishable from the true likelihood contours.

41

Figure 18: Ratio of the estimated likelihood ratio ˆr(x|θ0, θ1) to the joint likelihood ratio
r(x, z|θ0, θ1), which is conditional on the parton-level momenta and other latent variables. As a
benchmark hypothesis we use θ0 = (−0.5, −0.5)T , the events are drawn according to the SM. The
spread common to all methods shows the eﬀect of the smearing on the likelihood ratio. The
additional spread in the histogram, Carl, and Rolr methods is due to a poorer performance of
these techniques.

likelihood ratio after 36 observed events.

The expected corresponding expected exclusion limits are shown in Fig. 17. Indeed, errors in
the likelihood ratio estimation never lead to undercoverage, i. e. the exclusion of points that should
not be excluded based on the true likelihood ratio. Again we ﬁnd that histograms of kinematic
observables only allow us to place rather weak bounds on the Wilson coeﬃcients. Sally performs
clearly better, with excellent performance close to the SM. Deviations from the optimal bounds
become visible at the 2σ level, hinting at the breakdown of the local model approximation there.
The best results come once more from the Cascal and Rascal methods. Both of these strategies
yield exclusion bounds that are virtually indistinguishable from those based on the true likelihood
ratio.

As a side note, a comparison of the expected contours based on asymptotics to those based on
the Neyman construction shows the Neyman results to be tighter. This reﬂects the diﬀerent test
statistics used in the two ﬁgures: in the asymptotics case, we use the proﬁle likelihood ratio with
respect to the maximum likelihood estimator, which itself ﬂuctuates around the true value of θ
(which in our case is assumed to be the SM). In the Neyman construction we use the likelihood
ratio with respect to the SM, leading to tighter contours if the true value is in fact close to the SM,
and weaker constraints if it is very diﬀerent.

B. Detector eﬀects

We have now established that the measurement strategy work very well in an idealized setup,
where we can compare them to the true likelihood ratio. In a next step, we turn towards a setup
with a rudimentary smearing function that models the eﬀect of the parton shower and the detector
response on the observables. In this setting, the true likelihood is intractable, so we cannot use it
as a baseline to validate the predictions any more. But we can still discuss the relative ordering of
the exclusion contours predicted by the diﬀerent estimators.

Figure 18 shows the relation between the true joint likelihood ratio r(x, z|θ0, θ1), which is

42

Figure 19: Expected exclusion contours based on the Neyman construction with toy experiments at
68% CL, 95% CL, and 99.7% CL with smearing. We assume 36 observed events and the SM to be
true. As test statistics, we use the likelihood ratio with respect to the SM. In the setup with
smearing we cannot these results to the true likelihood contours. But since the Neyman
construction is guaranteed to cover, these expected limits are correct. The new techniques, in
particular Cascal (right, dashed red) and Rascal (right, dash-dotted orange), allow us to set
much tighter bounds on the Wilson coeﬃcients than a traditional histogram analysis (left, dotted
orange).

conditional on the parton-level momenta z and other latent variables, to the estimated likelihood
ratio ˆr(x|θ0, θ1), which only depends on the observables x. We see that this relation is stochastically
smeared out around 1. Recall that in the idealized scenario the best estimators described the true
likelihood ratio perfectly, as shown in Fig. 10. This strongly suggests that the spread visible here
is not due to errors of the likelihood ratio estimators, but rather shows the diﬀerence between the
joint and true likelihood ratios, as illustrated in Fig. 5.

In Fig. 19 we show the expected exclusion contours based on the Neyman construction, which
guarantees statistically correct results. The conclusions from the idealized setting are conﬁrmed: a
measurement based on the likelihood ratio estimators leads to robust bounds that are clearly more
powerful than those based on a histogram. Once again, the Cascal and Rascal algorithms lead
to the strongest limits.

VI. CONCLUSIONS

We have developed and analysed a suite of new analysis techniques for measurements of continuous
parameters in LHC experiments based on simulations and machine learning. Exploiting the structure
of particle physics processes, they extract additional information from the event generators, and use
this information to train precise estimators for likelihood ratios.

Our approach is designed for problems with large numbers of observables, where the likelihood
function is not tractable and traditional methods based on individual kinematic variables often

43

perform poorly. It scales well to high-dimensional parameter spaces such as that of eﬀective ﬁeld
theories. The new methods do not require any approximations on the hard process, parton shower,
or detector eﬀects, and the likelihood ratio for any event and hypothesis pair can be evaluated
in microseconds. These two properties set it apart from the Matrix Element Method or Optimal
Observables, which rely on crude approximations for the shower and detector and require the
evaluation of typically very expensive integrals.

Using Higgs production in weak boson fusion in the four-lepton mode as a speciﬁc example
process, we have evaluated the performance of the diﬀerent methods and compared them to a
classical analysis of the jet momentum and azimuthal angle between the tagging jets. We ﬁnd that
the new algorithms provide very precise estimates of arbitrary likelihood ratios. Using them as a
test statistics allows us to impose signiﬁcantly tighter constraints on the EFT coeﬃcients than the
traditional kinematic histograms.

Out of the several methods introduced and discussed in this paper, two stand out. The ﬁrst,

which we call Sally, is designed for parameter regions close to the Standard Model:

1. As training data, the algorithm requires a sample of fully simulated events, each accompanied
by the corresponding joint score at the SM: the relative change of the parton-level likelihood
function of the parton-level momenta associated with this event under small changes of the
theory parameters away from the SM. This can be calculated by evaluating the squared matrix
element at the same phase-space points for diﬀerent theory parameters. We can thus extract
this quantity from Monte-Carlo generators such as MadGraph.

2. Regressing on this data, we train an estimator (for instance realized as a neural network) that
takes as input an observation and returns the score at the SM. This function compresses the
high-dimensional observable space into a vector with as many components as parameters of
interest. If the parameter space is high-dimensional, this can be even further compressed into
the scalar product between the score vector and the diﬀerence between two parameter points.

3. The estimated score (or the scalar product between score and parameter diﬀerence) can then
be treated like any set of observables in a traditional analysis. We can ﬁll histograms of this
quantity for diﬀerent hypotheses, and calculate likelihood ratios from them.

There are two key ideas that underlie this strategy. First, note that the training data only
consists of the joint score, which depends on the parton-level four-momenta of an event. But during
the training the estimator converges to the actual score of the distribution of the observables, i. e. the
relative change of the actual likelihood function under inﬁnitesimal changes of the parameters. We
have proven this powerful, yet surprisingly simple relation in this paper.

Second, close to the Standard Model (or any other reference parameter point), the score provides
it encapsulates all information on the local approximation of the stat-
the suﬃcient statistics:
istical model. In other words, if the score is estimated well, the dimensionality reduction from
high-dimensional observables into a low-dimensional vector does not lose any information on the
parameters. The estimated score is a machine-learning version of the Optimal Observable idea,
but requires neither approximations of the parton shower or detector treatment nor numerically
expensive integrals.

As a matter of fact, the dimensionality reduction can be taken one step further. We have
introduced the Sallino technique that compresses the estimated score vector to a single scalar
function, again without loss of power in the local approximation, and independent of the number of
theory parameters.

In our example process, these simple and robust analysis strategies work remarkably well,
especially close to the Standard Model. Deviations appear at the 2σ level, but even there it

44

allows for much stronger constraints than a traditional analysis of kinematic variables. It requires
signiﬁcantly less data to train than the other discussed methods. Since the Sallino method can
compress any observation into a single number without losing much sensitivity, even for hundreds
of theory parameters, this approach scales exceptionally well to high-dimensional parameter spaces,
as in the case of the SMEFT.

The second algorithm we want to highlight here is the Rascal technique. Using even more
information available from the simulator, it learns a parameterized likelihood ratio estimator: one
function that takes both the observation and a theory parameter point as input and returns an
estimate for the likelihood ratio between this point and a reference hypothesis given the observation.
This estimator is constructed as follows:

1. Training this parameterized estimator requires data for many diﬀerent values of the tested
parameter point (the numerator in the likelihood ratio). For simplicity, the reference hypo-
thesis (the denominator in the likelihood ratio) can be kept ﬁxed. For each of these hypothesis
pairs, event samples are generated according to the numerator and denominator hypothesis.
In addition, we extract the joint likelihood ratio from the simulator: essentially the squared
matrix element according to the numerator theory parameters divided by the squared matrix
element according to the denominator hypothesis, evaluated at the generated parton-level
momenta. Again, we also need the joint score, i. e. the relative change of the parton-level
likelihood function under inﬁnitesimal changes of the theory parameters. Both quantities can
be extracted from matrix element codes.

2. A neural network models the estimated likelihood ratio as a function of both the observables
and the value of the theory parameters (of the numerator in the likelihood ratio). We can
calculate the gradient of the network output with respect to the theory parameter and thus
also the estimated score. The network is trained by minimizing the squared error of the
likelihood ratio plus the squared error of the score, in both cases with respect to the joint
quantities extracted from the simulator.

3. After the training phase, the likelihood ratio can optionally be calibrated, for instance through

isotonic regression.

This technique relies on a similar trick as the local score regression method: the likelihood
ratio learned during the training converges to the true likelihood ratio, even though the joint ratio
information in the training data is conditional on the parton-level momenta. The Rascal method
is among the best-performing methods of all analysed techniques. It requires signiﬁcantly smaller
training samples than all other approaches, with the exception of Sally and Sallino. Expected
exclusion limits derived in this way are virtually indistinguishable from those based on the true
likelihood ratio.

On top of these two approaches, we have developed, analysed, and compared several other
methods. We refer the reader to the main part and the appendices of this document, where all these
algorithms are discussed in depth.

All tools developed here are suitable for large-scale LHC analyses. On the software side, only few
modiﬁcations of existing tools are necessary. Most importantly, matrix-element generators should
provide a user-friendly interface to calculate the squared matrix element for a given conﬁguration
of four-momenta and a given set of physics parameters. With such an interface, one could easily
calculate the joint score and joint likelihood ratio data that is needed for the new algorithms. The
training of the estimators is then straightforward, in particular for the Sally and Sallino methods.
The limit setting follows established procedures, either based on the Neyman construction with toy
experiments, or (since the tools provide direct estimates for the likelihood ratio) using asymptotic
formulae.

45

While we have focussed on the example of eﬀective ﬁeld theory measurements, these techniques
equally apply to other measurements of continuous parameter in collider experiments as well as
to a large class of problems outside of particle physics [53]. Some of the techniques can also be
applied to improve the training of machine-learning-based classiﬁers. Finally, while we restricted
our analysis to frequentist conﬁdence intervals, as is common in particle physics, the same ideas can
be used in a Bayesian setting.

All in all, we have presented a range of new inference techniques based on machine learning,
which exploit the structure of particle physics processes to augment training data. They scale well
to large-scale LHC analyses with many observables and high-dimensional parameter spaces. They
do not require any approximations of the hard process, parton shower, or detector eﬀects, and the
likelihood ratio can be evaluated in microseconds. In an example analysis, these new techniques
have demonstrated the potential to substantially improve the precision and new physics reach of
the LHC legacy results.

Acknowledgments

We would like to thank Cyril Becot and Lukas Heinrich, who contributed to this project at an
early stage. We are grateful to Felix Kling, Tilman Plehn, and Peter Schichtel for providing the
MadMax code and helping us use it. KC wants to thank CP3 at UC Louvain for their hospitality.
Finally, we would like to thank Atılım Güneş Baydin, Lydia Brenner, Joan Bruna, Kyunghyun
Cho, Michael Gill, Ian Goodfellow, Daniela Huppenkothen, Hugo Larochelle, Yann LeCun, Fabio
Maltoni, Jean-Michel Marin, Iain Murray, George Papamakarios, Duccio Pappadopulo, Dennis
Prangle, Rajesh Ranganath, Dustin Tran, Rost Verkerke, Wouter Verkerke, Max Welling, and
Richard Wilkinson for interesting discussions.

JB, KC, and GL are grateful for the support of the Moore-Sloan data science environment at
NYU. KC and GL were supported through the NSF grants ACI-1450310 and PHY-1505463. JP
was partially supported by the Scientiﬁc and Technological Center of Valparaíso (CCTVal) under
Fondecyt grant BASAL FB0821. This work was supported in part through the NYU IT High
Performance Computing resources, services, and staﬀ expertise.

Appendix A: Appendix

1. Simpliﬁed detector description

While most of our results are based on an idealized perfect measurement of parton-level momenta,
we also consider a toy smearing representing the eﬀect of parton shower and the detector. The total
smearing function is given by

p(x|z) =

p(cid:96)(x(cid:96)|z(cid:96))

pj(xj|zj) .

(cid:89)

(cid:89)

(cid:96)∈leptons

j∈jets

(A1)

Lepton momenta x(cid:96) = ( ˆE, ˆpT , ˆη, ˆφ) are smeared by

p(cid:96)( ˆE, ˆpT , ˆη, ˆφ|E, pT , η, φ) = N(cid:0)ˆpT

(cid:12)
(cid:12)pT , (3 · 10−4GeV−1p2

T )2(cid:1)

· δ( ˆE − E0(ˆpT , ˆη; m(cid:96))) δ(ˆη − η) δ( ˆφ − φ) ,

(A2)

46

Strategy

NN layers

α Calibration / density estimation

Histogram
AFC
Carl (PbP)
Carl (parameterized)
Carl (morphing-aware)
Sally
Sallino

Rolr (PbP)
Rolr (parameterized)
Rolr (morphing-aware)
Cascal (parameterized)
Cascal (morphing-aware)
Rascal (parameterized)
Rascal (morphing-aware)

Histogram
Gaussian KDE

Isotonic probability calibration
Isotonic probability calibration
Isotonic probability calibration

Histogram
Histogram

Isotonic probability calibration
Isotonic probability calibration
Isotonic probability calibration

5
5

Isotonic probability calibration
Isotonic probability calibration

100
100

Isotonic probability calibration
Isotonic probability calibration

3
2
2

5
5

3
3
2

5
2

5
2

Table VI: Default settings for the analysis techniques. The neural network (NN) layers each have
100 units with tanh activation functions. The hyperparameter α multiplies the score squared error
in the combined loss functions of Eq. (35) and (37).

while the distribution of the jet properties depending on the quark momenta is given by

pj( ˆE, ˆpT , ˆη, ˆφ|E, pT , η, φ) =

(cid:32)

(cid:16) ˆE
N

(cid:12)
(cid:12)
(cid:12)a0 + a1

√

E + a2E, (b0 + b1

√

E + b2E)2(cid:17)

+ cN

(cid:16) ˆE

(cid:12)
(cid:12)
(cid:12)d0 + d1

√

E + d2E, (e0 + e1

√

E + e2E)2(cid:17)

(cid:33)

· δ(ˆpT − pT 0( ˆE, ˆη; m(cid:96))) N(cid:0)ˆη(cid:12)

(cid:12)η, 0.12(cid:1) N

(cid:12)
(cid:16) ˆφ
(cid:12)

(cid:12)φ, 0.12(cid:17)

.

(A3)

Here N(cid:0)x(cid:12)
(cid:12)µ, σ2(cid:1) is the Gaussian distribution with mean µ and variance σ2. The jet energy resolution
parameters ai, bi, c, di, and ei are based on the default settings of the jet transfer function in
MadWeight [91]. The functions E0(pT , η, m) and pT 0(E, η, m) refer to the energy and transverse
momentum corresponding to an on-shell particle with mass m.

2. Model almanac

In Sec. III A we developed diﬀerent estimators for the likelihood ratio, focussing on the key ideas
over technical details. Here we ﬁll in the gaps, explain all strategies in a self-contained way, and
document the settings we use for our example process. To facilitate their comparison, we describe
all models in terms of a “training” and an “evaluation” part, even if this language is not typically
used e. g. for histograms.

a. Histograms of observables

Idea: Most collider measurements are based on the number of events or the cross section of a
process in a given phase-space region or on the diﬀerential cross section or distribution of

47

Figure 20: Example distributions to illustrate the doubly diﬀerential histogram analysis (top), the
Sally technique (middle), and the Cascal method (bottom). The left panels show the diﬀerent
spaces in which the densities and their ratios are estimated. On the right we show the
corresponding distributions of the estimated ratio ˆr (solid) and compare them to the true likelihood
ratio distributions (dotted). We use the benchmark θ0 = (−0.5, −0.5)T (blue) and θ1 as in Eq. (47)
(orange).

48

one or at most a few kinematic observables v. Typical choices are the reconstructed energies,
momenta, angles, or invariant masses of particles. Choosing the right set of observables
for a given measurement problem is all but trivial, but many processes have been studied
extensively in the literature. Once this choice is made, this strategy is simple, fast, and
intuitive. We illustrate the information used by this approach in the top panels of Fig. 20.

Requirements: The histogram approach can be used in the general likelihood-free setting: it only

requires a simulator that can generate samples {x} ∼ p(x|θ).

Structure: Histograms are most commonly used point by point in θ.

If the problem has the
morphing structure discussed in Sec. II C 2, they can also be applied in a morphing-aware
parameterized way (we have not implemented this for our example process).

Training: After generating samples for both the numerator and denominator hypotheses, the values
of the chosen kinematic variables v(x) are extracted, and binned into separate histograms for
the two hypotheses.

Calibration: With suﬃcient training data, histograms should be well calibrated, so we do not

experiment with an additional calibration stage.

Evaluation: To estimate the likelihood ratio between two hypotheses θ0 and θ1 for a given set of
observables x, one has to extract the kinematic variables v(x) and look up the corresponding
bin contents in the histograms for θ0 and θ1. Assuming equal binning for both histograms,
the likelihood ratio is simply estimated as the ratio of bin contents.

Parameters: The only parameters of this approach are the choices of kinematic observables and

the histogram binning.

In our example process, we consider six diﬀerent variants:

• A one-dimensional histogram of the transverse momentum pT,j1 of the leading (higher-pT )

• A one-dimensional histogram of the absolute value of the azimuthal angle ∆φjj between

jet with 80 bins.

the two jets with 20 bins.

direction and 5 bins along ∆φjj.

direction and 10 bins along ∆φjj.

direction and 15 bins along ∆φjj.

direction and 20 bins along ∆φjj.

• A “coarse” two-dimensional histogram of these two variables with 10 bins in the pT,j1

• A “medium” two-dimensional histogram of these two variables with 20 bins in the pT,j1

• A “ﬁne” two-dimensional histogram of these two variables with 30 bins in the pT,j1

• A “very ﬁne” two-dimensional histogram of these two variables with 50 bins in the pT,j1

• An “asymmetric” two-dimensional histogram of these two variables with 50 bins in the

pT,j1 direction and 5 bins along ∆φjj.

For each pair (θ0, θ1) and each observable, the bin edges are chosen such that the same
expected number of events according to θ0 plus the expected number of events according to
θ1 is the same in each bin.

49

b. Approximate Frequentist Computation ( Afc)

Idea: Approximate Bayesian Computation is a very common technique for likelihood-free inference
in a Bayesian setup. In its simplest form it keeps samples according to the rejection probability
of Eq. (29). This amounts to an approximation of the likelihood function through kernel
density estimation, which we can isolate from the Abc sampling mechanism and use in
a frequentist setting. We call it Approximate Frequentist Computation (AFC) to stress
the relation to Abc. Just as Abc or the histogram approach, it requires the choice of a
summary statistics v(x), which in our example process we take to be a two-dimensional or
ﬁve-dimensional subset of the kinematic variables.

Requirements: AFC can be used in the general likelihood-free setting: it only requires a simulator

that can generate samples {x} ∼ p(x|θ).

Structure: We use AFC point by point in θ. If the problem has the morphing structure discussed

in Sec. II C 2, it can also be applied in a morphing-aware parameterized way.

Training: For each event in the numerator and denominator training samples, the summary

statistics v(x) are calculated and saved.

Calibration: AFC can be calibrated as any other technique on this list, but we left this for future

work.

Evaluation: The summary statistics v(x) is extracted from the observation. For numerator and
denominator hypothesis separately, the likelihood at this point is estimated with Eq. (30). The
likelihood ratio estimate is then simply given by the ratio between the estimated numerator
and denominator densities.

Parameters: Just as for histograms, the choice of the summary statistics is the most important
parameter. The performance of AFC also crucially depends on the kernel and bandwidth ε.
Too small values for the bandwidth make large training samples necessary, too large values
lead to an oversmoothening and loss of information.

In our example process, we consider two diﬀerent variants:

• A two-dimensional summary statistics space of the leading jet pT and ∆φjj (see above).

Both variables are rescaled to zero mean and unit variance.

• A ﬁve-dimensional summary statistics space of the leading jet pT , ∆φjj, the dijet
invariant mass mjj, the separation in pseudorapidity between the jets ∆ηjj, and the
invariant mass of the lighter (oﬀ-shell) reconstructed Z boson mZ2. All variables are
rescaled to zero mean and unit variance.

We use Gaussian kernels with bandwidths between 0.01 and 0.5.

c. Calibrated classiﬁers ( Carl)

Idea: Carl was developed in Ref. [34]. The authors showed that the likelihood ratio is invariant
under any transformation that is monotonic with the likelihood ratio.
In practice, this
means that we can train a classiﬁer between two samples generated from the numerator and
denominator hypotheses and turn the classiﬁer decision function ˆs(x) into an estimator for
the likelihood ratio ˆr(x). This relation between ˆs(x) and ˆr(x) can follow the ideal relation in

50

Eqs. (18) and (46). But even if this relation does not hold, we can still extract a likelihood
ratio estimator from the classiﬁer output through probability calibration.

Requirements: Carl can be used in the general likelihood-free setting: it only requires a simulator

that can generate samples {x} ∼ p(x|θ).

Structure: Carl can be used either point by point, in an agnostic parameterized version, or (if
the morphing condition in Eq. (6) holds) in a morphing-aware version. Figure 8 illustrates
the structure of the estimator in these three cases.

Training: A classiﬁer with decision function ˆs(x|θ0, θ1) is trained to discriminate between numerator
(label y = 0) and denominator (label y = 1) samples by minimizing the binary cross-entropy
given in Eq. (16) (other loss functions are possible, but we have not experimented with them).

In the point-by-point version, the inputs to the classiﬁers are just the observables x, and
the events in the numerator sample are generated according to one speciﬁc value θ0. In the
parameterized versions of the estimator, the numerator training samples do not come from a
single parameter θ0, but rather a combination of many diﬀerent subsamples. In the agnostic
parameterized setup, the value of θ0 used in each event is then one of the inputs to the neural
network. In the morphing-aware versions, it is used to calculate the weights wc(θ0) that
multiply the diﬀerent component networks ˆrc(x), as visualized in the bottom panel of Fig. 8.

Calibration: In a next step, the classiﬁer output is optionally calibrated as discussed in Sec. III D 1
using isotonic regression. The calibration curve is shown in the left panel of Fig. 21. We have
also experimented with an additional step of expectation calibration, see Sec. III D 2.

Evaluation: For a given x (and in the parameterized versions θ0), the classiﬁer decision function
ˆs(x|θ0, θ1) is evaluated. This is turned into a likelihood ratio estimator with the relation given
in Eq. (18), and optionally calibrated.

Parameters: The parameters of this approach are the architecture of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms.

For our example process we consider fully connected neural networks with two (“shallow”),
three, or ﬁve (“deep”) layers of 100 neurons each and tanh activation functions. They are
trained with the Adam optimizer [83] over 50 epochs with early stopping and learning rate
decay. Our default settings are given in Table VI. Experiments with diﬀerent architectures,
other activation functions, additional dropout layers, other optimizers, and diﬀerent learning
rate schedules yielded a worse performance.

d. Ratio regression ( Rolr)

Idea: Particle physics generators do not only provide sets of observables {xe}, but also the cor-
responding parton-level momenta {ze}. From matrix element codes such as MadGraph we
can extract the squared matrix element |M|2(z|θ) given parton-level momenta z and theory
parameter points θ. This allows us to calculate the joint likelihood ratio

r(xe, ze|θ0, θ1) =

p(ze|θ0)
p(ze|θ1)

=

|M|2(ze|θ0)
|M|2(ze|θ1)

σ(θ1)
σ(θ0)

(A4)

51

Figure 21: Calibration curves for diﬀerent estimators, comparing the uncalibrated (“raw”) estimator
to the estimator after probability calibration. The calibration curve for the truth prediction is a
cross-check for consistency, we do not actually use calibration for the truth predictions. For the
local score regression technique, we show the value of ˆh(x|θ0, θ1) (essentially the log likelihood ratio
in the local model) versus the estimated likelihood ratio after density estimation.

for any of the generated events.

In Sec. III B we have shown that regressing a function ˆr(x) on the generated events {xe} and
the corresponding joint likelihood ratios r(xe, ze|θ0, θ1) will converge to

ˆr(x) → r(x) =

p(x|θ0)
p(x|θ1)

,

(A5)

provided that the events are sampled according to xe ∼ p(x|θ1).

Requirements: The Rolr technique requires a generator with access to the joint likelihood ratios
r(xe, ze|θ0, θ1). In the particle physics case, this means we have to be able to evaluate the
squared matrix elements for given phase-space points and theory parameters.

Structure: Rolr can be used either point by point, in an agnostic parameterized version, or (if
the morphing condition in Eq. (6) holds) in a morphing-aware version. Figure 8 illustrates
the structure of the estimator in these three cases.

Training: The training phase is straightforward regression. It consists of minimizing the squared
error loss between a ﬂexible function ˆr(x|θ0, θ1) (for instance a neural network) and the
training data {xe, r(xe, ze|θ0, θ1)}, which was generated according to θ1.

In the point-by-point version, the input to the regressor are just the observables x, and
the ratio is between two ﬁxed hypotheses θ0 and θ1. In the parameterized versions of the
estimator, the ratios are based on various values θ0, while θ1 is still kept ﬁxed. In the agnostic
parameterized setup, the value of θ0 used in each event is then one of the inputs to the neural
network. In the morphing-aware versions, it us used to calculate the weights wc(θ0) that
multiply the diﬀerent component networks ˆrc(x), as visualized in the bottom panel of Fig. 8.

In all cases we can slightly improve the structure by adding samples generated according
to θ0 to the training samples, regressing on 1/r instead of r on these events. The full loss
functional is given in Eq. (33).

52

Calibration: In a next step, the classiﬁer output is optionally calibrated as discussed in Sec. III D 1
using isotonic regression. The calibration curve is shown in the middle panel of Fig. 21. We
have also experimented with an additional step of expectation calibration, see Sec. III D 2.

Evaluation: For a given x (and in the parameterized versions θ0), the regressor ˆr(x|θ0, θ1) is

evaluated. The result is optionally calibrated.

Parameters: The parameters of this approach are the architecture of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms.

For our example process we consider fully connected neural networks with two (“shallow”),
three, or ﬁve (“deep”) layers of 100 neurons each and tanh activation functions. They are
trained with the Adam optimizer [83] over 50 epochs with early stopping and learning rate
decay. Our default settings are given in Table VI. Experiments with diﬀerent architectures,
other activation functions, additional dropout layers, other optimizers, and diﬀerent learning
rate schedules yielded a worse performance.

e. Carl + score regression ( Cascal)

Idea: The parameterized Carl technique learns the full statistical model ˆr(x|θ0, θ1), including the
dependency on θ0. If it is realized as a diﬀerentiable classiﬁer (such as a neural network), we
can calculate the gradient of ˆr(x|θ0, θ1) with respect to θ0, and thus the estimated score of
this model. If the estimator is perfect, we expect this estimated score to minimize the squared
error with respect to the joint score data available for the training data. This is based on the
same argument as the local score regression technique, see Sec. III B for the proof.

We can turn this argument around and use the available score information during the training.
To this end, we combine two terms in a combined loss function: the Carl-style cross-entropy
and the squared error between the estimated score and the joint score of the training data.
These two pieces contain complementary information: the Carl part contains the information
of the likelihood ratio for a ﬁxed hypothesis comparison (θ0, θ1), while the score part describes
the relative change of the likelihood ratio under changes in θ0.

Requirements: The Cascal technique requires a generator with access to the joint score
t(xe, ze|θ0). In the particle physics case, this means we have to be able to evaluate the squared
matrix elements for given phase-space points and theory parameters.

Structure: Since the Cascal method relies on the extraction of the estimated score from the
estimator, it can only be used for parameterized estimators, either in an agnostic or morphing-
aware version. The middle and bottom panels of Fig. 8 illustrate the structure of the estimator
in these two cases.

Training: A diﬀerentiable classiﬁer with decision function ˆs(x|θ0, θ1) is trained to discriminate
between numerator (label y = 0) and denominator (label y = 1) samples, while the derived
estimated score ˆt(x|θ0) is compared to the joint score on the training samples generated
from y = 0. The loss function that is minimized is thus a combination of the Carl-style
cross-entropy and the squared error on the score, weighted by a hyperparameter α. It is given
in Eq. (35).

The numerator (y = 0) training samples do not come from a single parameter θ0, but rather a
combination of many diﬀerent subsamples. In the agnostic parameterized setup, the value of

53

θ0 used in each event is then one of the inputs to the neural network. In the morphing-aware
versions, it is used to calculate the weights wc(θ0) that multiply the diﬀerent component
networks ˆrc(x), as visualized in the bottom panel of Fig. 8.

Calibration: In a next step, the classiﬁer output is optionally calibrated as discussed in Sec. III D 1
using isotonic regression. The calibration curve is shown in the right panel of Fig. 21. We
have also experimented with an additional step of expectation calibration, see Sec. III D 2.

Evaluation: For a given x and θ0, the classiﬁer decision function ˆs(x|θ0, θ1) is evaluated. This is
turned into a likelihood ratio estimator with the relation given in Eq. (18), and optionally
calibrated.

Parameters: The key hyperparameter of this technique is the factor α that weights the two terms
in the loss function. Additional parameters set the architecture of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms.

For our example process we consider fully connected neural networks with two (“shallow”),
three, or ﬁve (“deep”) layers of 100 neurons each and tanh activation functions. They are
trained with the Adam optimizer [83] over 50 epochs with early stopping and learning rate
decay. Our default settings are given in Table VI. Experiments with diﬀerent architectures,
other activation functions, additional dropout layers, other optimizers, and diﬀerent learning
rate schedules yielded a worse performance.

f. Ratio + score regression ( Rascal)

Idea: The parameterized Rolr technique learns the full statistical model ˆr(x|θ0, θ1), including the
dependency on θ0. If it is realized as a diﬀerentiable regressor, we can calculate the gradient
of ˆr(x|θ0, θ1) with respect to θ0, and thus the score of this model. If the estimator is perfect,
we expect this estimated score to minimize the squared error with respect to the joint score
data available for the training data.

We can turn this argument around and use the available likelihood ratio and score information
during the training. To this end, we combine two terms in a combined loss function: the
squared errors on the ratio and the score. These two pieces contain complementary information:
the ratio regression part contains the information of the likelihood ratio for a ﬁxed hypothesis
comparison (θ0, θ1), while the score part describes the relative change of the likelihood ratio
under changes in θ0.

Requirements: The Rascal technique requires a generator with access to the joint likelihood
ratio r(xe, ze|θ0, θ1) and score t(xe, ze|θ0). In the particle physics case, this means we have
to be able to evaluate the squared matrix elements for given phase-space points and theory
parameters.

Structure: Since the Rascal method relies on the extraction of the estimated score from the
estimator, it can only be used for parameterized estimators, either in an agnostic or morphing-
aware version. The middle and bottom panels of Fig. 8 illustrate the structure of the estimator
in these two cases.

Training: An estimator ˆr(x|θ0, θ1) is trained through regression on the joint likelihood ratio, while
the derived estimated score ˆt(x|θ0) is compared to the joint score on the training samples

54

generated from y = 0. The loss function that is minimized is thus a combination of the
squared error on the ratio and the squared error on the score, weighted by a hyperparameter
α. It is given in Eq. (35).

The numerator (y = 0) training samples do not come from a single parameter θ0, but rather a
combination of many diﬀerent subsamples. In the agnostic parameterized setup, the value of
θ0 used in each event is then one of the inputs to the neural network. In the morphing-aware
versions, it is used to calculate the weights wc(θ0) that multiply the diﬀerent component
networks ˆrc(x), as visualized in the bottom panel of Fig. 8.

Calibration: In a next step, the classiﬁer output is optionally calibrated as discussed in Sec. III D 1
using isotonic regression. The calibration curve is shown in the right panel of Fig. 21. We
have also experimented with an additional step of expectation calibration, see Sec. III D 2.

Evaluation: For a given x and θ0, the estimator ˆr(x|θ0, θ1) is evaluated and optionally calibrated.

Parameters: The key hyperparameter of this technique is the factor α that weighs the two terms
in the loss function. Additional parameters set the architecture of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms.

For our example process we consider fully connected neural networks with two (“shallow”),
three, or ﬁve (“deep”) layers of 100 neurons each and tanh activation functions. They are
trained with the Adam optimizer [83] over 50 epochs with early stopping and learning rate
decay. Our default settings are given in Table VI. Experiments with diﬀerent architectures,
other activation functions, additional dropout layers, other optimizers, and diﬀerent learning
rate schedules yielded a worse performance.

g. Local score regression and density estimation ( Sally)

Idea: In Sec. III A 2 we introduced the score, the relative gradient of the likelihood with respect to
the theory parameters. The score evaluated at some reference parameter point is the suﬃcient
statistics of the local approximation of the likelihood given in Eq. (15). In other words, we
expect the score vector to be a set of “optimal observables” that includes all the information
on the theory parameters, at least in the vicinity of the reference parameter point. If we can
estimate the score from an observation, we can use it like any other set of observables. In
particular, we can ﬁll histograms of the score for any parameter point and thus estimate the
likelihood ratio in score space.

To estimate the score, we again make use of the particle physics structure. Particle physics
generators do not only provide sets of observables {xe}, but also the corresponding parton-
level momenta {ze}. From matrix element codes such as MadGraph we can extract the
squared matrix element |M|2(z|θ) given parton-level momenta z and theory parameter points
θ. This allows us to calculate the joint score

(cid:12)
(cid:12)
(cid:12)
t(xe, ze|θ0) = ∇θ log p(ze|θ)
(cid:12)
(cid:12)θ0

=

∇θ|M|2(ze|θ0)
|M|2(ze|θ0)

−

∇θσ(θ0)
σ(θ0)

(A6)

for any of the generated events. The derivatives in Eq. (A6) can always be evaluated
numerically.
If the process has the morphing structure of Eq. (6), one can alternatively
calculate it from the morphing weights.

55

(A7)

In Sec. III B we have shown that regressing a function ˆt(x) on the generated events {xe} and
the corresponding joint scores t(xe, ze|θ) will converge to

ˆt(x) → t(x) = ∇θ log p(x|θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)θ0

,

provided that the events are sampled according to xe ∼ p(x|θ0).

This technique is illustrated in the middle panels of Fig. 20. The middle panel of Fig. 21 shows
the relation between the scalar product of estimated score and θ0 − θ1 and the estimated
likelihood ratio.

Requirements: The Sally technique requires a generator with access to the joint score t(xe, ze|θ0).
In the particle physics case, this means we have to be able to evaluate the squared matrix
elements for given phase-space points and theory parameters.

Structure: The technique consists of two separate steps: the score regression and the density
estimation in the estimated score space. The score regression step is independent of the tested
hypothesis and realized as a simple fully connected neural network. The subsequent density
estimation is realized through multi-dimensional histograms, point by point in parameter
space (if the morphing condition in Eq. (6) holds, a morphing-aware version is also possible).

Training: The ﬁrst part of the training is regression on the score (evaluated at some reference
hypothesis). It consists of minimizing the squared error loss between a ﬂexible vector-valued
function ˆt(x|θscore) (implemented for instance as a neural network) and the training data
{xe, t(xe, ze|θscore)}, which was sampled according to θscore.

The second step is density estimation in the estimated score space. We only consider histo-
grams, but other density estimation techniques are also possible. For each value of θ0 or θ1
that is tested, we generate samples of events, estimate the corresponding score vectors, and
ﬁll a multidimensional histogram of the estimated score.

Calibration: The density estimation step already calibrates the results, so we do not experiment

with an additional calibration step.

Evaluation: For a given observation x, the score regressor ˆt(x) is evaluated. For each tested (θ0, θ1)
pair, we then extract the corresponding bin contents from the numerator and denominator
histograms, and calculate the estimated likelihood ratio with Eq. (38).

Parameters: Both the score regression part and the subsequent density estimation have parameters.
The ﬁrst and most important choice is the reference hypothesis θscore, at which the score is
evaluated. For eﬀective ﬁeld theories the Standard Model is the natural choice, and we use it
in our example process.

The score regression also depends on the hyperparameters of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms. For our example process we consider fully
connected neural networks with two (“shallow”), three, or ﬁve (“deep”) layers of 100 neurons
each and tanh activation functions. They are trained with the Adam optimizer [83] over 50
epochs with early stopping and learning rate decay. Our default settings are given in Table VI.
Experiments with diﬀerent architectures, other activation functions, additional dropout layers,
other optimizers, and diﬀerent learning rate schedules yielded a worse performance.

56

The only parameter of the density estimation stage is the histogram binning. For our example
process we consider two diﬀerent variations:

• Density estimation with a “ﬁxed” binning, where the bin axes are aligned with the score

components. We use 40 bins for each of the two score components.

• Density estimation with a “dynamic” binning, in which the bin axes are aligned with
the θ0 − θ1 direction and the orthogonal one. We use 80 bins along the ∆θ direction,
which carries the relevant information in the local model approximation, and 10 along
the orthogonal vector.

For each pair (θ0, θ1) and each dimension, the bin edges are chosen such that the expected
number of events according to θ0 plus the expected number of events according to θ1 is the
same in each bin.

h. Local score regression, compression to scalar, and density estimation ( Sallino)

Idea: In the proximity of the Standard Model (or any other reference parameter point), likelihood
ratios only depend on the scalar product between the score and the diﬀerence between
the numerator and denominator parameter points. If we can estimate the score from an
observation, we can calculate this scalar product ˆh(x|θ0, θ1), deﬁned in Eq. (39), and use it
like any other observable. In particular, we can ﬁll histograms of ˆh for any parameter point
and thus estimate the likelihood ratio in ˆh space.

To estimate the score, we once again exploit particle physics structure. Particle physics
generators do not only provide sets of observables {xe}, but also the corresponding parton-
level momenta {ze}. From matrix element codes such as MadGraph we can extract the
squared matrix element |M|2(z|θ) given parton-level momenta z and theory parameter points
θ. This allows us to calculate the joint score with Eq. (A6) for any of the generated events.
In Sec. III B we have shown that regressing a function ˆt(x) on the generated events {xe} and
the corresponding joint scores t(xe, ze|θ) will converge to t(x), provided that the events are
sampled according to xe ∼ p(x|θ0).

Requirements: The Sallino technique requires a generator with access to the joint score
In the particle physics case, this means we have to be able to evaluate the

t(xe, ze|θ0).
squared matrix elements for given phase-space points and theory parameters.

Structure: The technique consists of two separate steps: the score regression, and the density
estimation in ˆh space. The score regression step is independent of the tested hypothesis and
realized as a simple fully connected neural network. The subsequent density estimation in ˆh
space is realized through one-dimensional histograms, point by point in parameter space (if
the morphing condition in Eq. (6) holds, a morphing-aware version is also possible).

Training: The ﬁrst part of the training is regression on the score (evaluated at some reference
hypothesis). It consists of minimizing the squared error loss between a ﬂexible vector-valued
function ˆt(x|θscore) (implemented for instance as a neural network) and the training data
{xe, t(xe, ze|θscore)}, which was sampled according to θscore.
The second step is density estimation in ˆh space. We only consider histograms, but other
density estimation techniques are also possible. For each value of θ0 or θ1 that is tested, we
generate samples of events, estimate the corresponding score vectors, calculate the scalar
product in Eq. (39) to get ˆh(x|θ0, θ1), and ﬁll a one-dimensional histogram of this quantity.

57

Calibration: The density estimation step already calibrates the results, so we do not experiment

with an additional calibration step.

Evaluation: For a given observation x, the score regressor ˆt(x) is evaluated. For each tested (θ0, θ1)
pair, we multiply it with θ0 − θ1 to get ˆh(x|θ0, θ1), extract the corresponding bin contents
from the numerator and denominator histograms, and calculate the estimated likelihood ratio
with Eq. (40).

Parameters: Both the score regression part and the subsequent density estimation have parameters.
The ﬁrst and most important choice is the reference hypothesis θscore, at which the score is
evaluated. For eﬀective ﬁeld theories the Standard Model is the natural choice, and we use it
in our example process.

The score regression also depends on the hyperparameters of the neural network, i. e. the
number of layers and elements, the activation function, the optimizer used for training, its
parameters, and optionally regularization terms. For our example process we consider fully
connected neural networks with two (“shallow”), three, or ﬁve (“deep”) layers of 100 neurons
each and tanh activation functions. They are trained with the Adam optimizer [83] over 50
epochs with early stopping and learning rate decay. Our default settings are given in Table VI.
Experiments with diﬀerent architectures, other activation functions, additional dropout layers,
other optimizers, and diﬀerent learning rate schedules yielded a worse performance.

The only parameter of the density estimation stage is the histogram binning. We use 100
bins. For each pair (θ0, θ1) and each dimension, the bin edges are chosen such that the same
expected number of events according to θ0 plus the expected number of events according to
θ1 is the same in each bin.

3. Additional results

In Tbls. VII to XII we compare the performance of diﬀerent versions of the likelihood ratio
estimators. As metric we use the expected mean squared error on log r(x|θ0, θ1) as well as a trimmed
version, as deﬁned in Sec. V A. The estimators are an extended list of those given in Table IV,
adding variations with diﬀerent hyperparameter choices and the results for uncalibrated (“raw”)
estimators. By default, we use neural networks with 3 hidden layers, the labels “shallow” and “deep”
refer to 2 and 5 hidden layers, respectively. We highlight the versions of the estimators that were
shown in the main part of this paper.

Because of the duality between density estimation and probabilistic classiﬁcation (see Eqs. (18)
and (46)), we can use all techniques to deﬁne classiﬁers. In Fig. 22 we show the ROC curves for
two benchmark parameter points. Note how badly the two scenarios can be separated. This is not
a shortcoming of the discrimination power of the classiﬁers, but due to the genuine overlap of the
probability distributions, as can be seen from the identical ROC curve based on the true likelihood
ratio.

58

Figure 22: Receiver operating characteristic (ROC) curves of true positive rates (TPR) vs. false
positive rates (FPR) for the classiﬁcation between the benchmark scenarios θ0 = (−0.5, −0.5)T and
θ1 as in Eq. (47). The ROC AUC based on the true likelihood is 0.6276. The results show how
much the probability distributions for these two hypotheses overlap.

Strategy

Setup

Histogram pT,j1
∆φjj
2d (coarse binning)
2d (medium binning)
2d (ﬁne binning)
2d (very ﬁne binning)
2d (asymmetric binning)

Afc

2d, (cid:15) = 1
2d, (cid:15) = 0.5
2d, (cid:15) = 0.2
2d, (cid:15) = 0.1
2d, (cid:15) = 0.05
2d, (cid:15) = 0.02
2d, (cid:15) = 0.01
5d, (cid:15) = 1
5d, (cid:15) = 0.5
5d, (cid:15) = 0.2
5d, (cid:15) = 0.1
5d, (cid:15) = 0.05
5d, (cid:15) = 0.02
5d, (cid:15) = 0.01

Expected MSE

All Trimmed

Table IV Figures

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

0.0879
0.1595
0.0764
0.0630
0.0597
0.0603
0.0561

0.1243
0.0797
0.0586
0.0732
0.3961
13.6816
241.3264
0.1252
0.0779
0.0734
0.9560
38.1854
2050.5289
50024.7997

0.0230
0.0433
0.0117
0.0101
0.0115
0.0153
0.0106

0.0257
0.0144
0.0091
0.0103
0.0160
0.0550
0.2143
0.0226
0.0101
0.0128
0.1833
3.6658
57.0410
1668.8988

Table VII: Comparison of techniques based on manually selected kinematic observables. The
metrics shown are the expected mean squared error on the log likelihood ratio with and without
trimming, as deﬁned in the text. Checkmarks in the last two columns denote estimators included in
Table IV and the ﬁgures in the main part of this paper, respectively.

59

Table IV Figures

Expected MSE

All Trimmed

0.0409

0.0213

0.0301

0.0111

Fig. 12

Strategy

Carl (PbP, raw)
Carl (PbP, cal.)
Carl (parameterized, raw)

Setup

PbP

PbP

Carl (parameterized, cal.)

Carl (morphing-aware, raw) Baseline

Carl (morphing-aware, cal.) Baseline

Baseline
Baseline, shallow
Baseline, deep
Random θ
Random θ, shallow
Random θ, deep

Baseline
Baseline, shallow
Baseline, deep
Random θ
Random θ, shallow
Random θ, deep

0.0157
0.0134
0.0161
0.0148
0.0130
0.0164

0.0156
0.0124
0.0160
0.0147
0.0122
0.0155

0.1598
0.1483
Baseline, shallow
0.1743
Random θ
Random θ, shallow
0.1520
Morphing basis, shallow 10.1231

Baseline, shallow
Random θ
Random θ, shallow
Morphing basis, shallow

0.1036
0.0762
0.1076
0.0858
0.1564

0.0040
0.0035
0.0038
0.0037
0.0037
0.0038

0.0032
0.0026
0.0029
0.0029
0.0028
0.0029

0.0350
0.0331
0.0429
0.0369
7.9314

0.0282
0.0200
0.0289
0.0226
0.0618

(cid:88)

Fig. 12

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

Table VIII: Comparison of diﬀerent versions of the Carl technique. The metrics shown are the
expected mean squared error on the log likelihood ratio with and without trimming, as deﬁned in
the text. Checkmarks in the last two columns denote estimators included in Table IV and the
ﬁgures in the main part of this paper, respectively.

Strategy

Rolr (PbP, raw)
Rolr (PbP, cal.)
Rolr (param., raw)

Setup

PbP

PbP

60

Table IV Figures

Expected MSE

All Trimmed

0.0052

0.0023

0.0049

0.0022

(cid:88)

Rolr (param., cal.)

(cid:88)

Baseline
Baseline, shallow
Baseline, deep
Random θ
Random θ, shallow
Random θ, deep

Baseline
Baseline, shallow
Baseline, deep
Random θ
Random θ, shallow
Random θ, deep

Baseline, shallow
Random θ
Random θ, shallow
Morphing basis, shallow

Baseline, shallow
Random θ
Random θ, shallow
Morphing basis, shallow

0.0034
0.0069
0.0041
0.0034
0.0070
0.0036

0.0032
0.0059
0.0038
0.0030
0.0060
0.0034

0.2029
0.1672
0.1908
0.1160
5.6668

0.0328
0.0243
0.0321
0.0224
0.1300

0.0019
0.0037
0.0022
0.0017
0.0036
0.0017

0.0017
0.0030
0.0019
0.0014
0.0030
0.0015

0.1449
0.1305
0.1353
0.0755
3.8335

0.0088
0.0063
0.0089
0.0052
0.0485

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

Rolr (morph.-aware, raw) Baseline

Rolr (morph.-aware, cal.) Baseline

Table IX: Comparison of diﬀerent versions of the Rolr technique. The metrics shown are the
expected mean squared error on the log likelihood ratio with and without trimming, as deﬁned in
the text. Checkmarks in the last two columns denote estimators included in Table IV and the
ﬁgures in the main part of this paper, respectively.

Expected MSE

All Trimmed

Table IV Figures

Strategy

Setup

Sally

Fixed 2D histogram
Fixed 2D histogram, shallow
Fixed 2D histogram, deep
Dynamic 2D histogram
Dynamic 2D histogram, shallow
Dynamic 2D histogram, deep

Sallino 1D histogram

1D histogram, shallow
1D histogram, deep

0.0174
0.0170
0.0171
0.0132
0.0133
0.0132

0.0213
0.0215
0.0213

0.0005
0.0005
0.0005
0.0003
0.0003
0.0002

0.0006
0.0007
0.0006

(cid:88)

(cid:88)

(cid:88)

Table X: Comparison of diﬀerent versions of the Sally and Sallino techniques. The metrics
shown are the expected mean squared error on the log likelihood ratio with and without trimming,
as deﬁned in the text. Checkmarks in the last two columns denote estimators included in Table IV
and the ﬁgures in the main part of this paper, respectively.

Strategy

Setup

Cascal (param., raw)

61

Expected MSE

All Trimmed

Table IV Figures

Baseline, α = 5
Baseline, α = 5 , shallow
Baseline, α = 5 , deep
Baseline, α = 0.5, deep
Baseline, α = 1, deep
Baseline, α = 2, deep
Baseline, α = 10, deep
Baseline, α = 20, deep
Baseline, α = 50, deep
Random θ
Random θ, shallow
Random θ, deep

Baseline, α = 5
Baseline, α = 5 , shallow
Baseline, α = 5 , deep
Baseline, α = 0.5, deep
Baseline, α = 1, deep
Baseline, α = 2, deep
Baseline, α = 10, deep
Baseline, α = 20, deep
Baseline, α = 50, deep
Random θ, α = 5
Random θ, α = 5 , shallow
Random θ, α = 5 , deep

0.0019
0.0037
0.0010
0.0017
0.0014
0.0017
0.0013
0.0016
0.0024
0.0022
0.0038
0.0010

0.0012
0.0025
0.0008
0.0013
0.0011
0.0010
0.0010
0.0011
0.0016
0.0013
0.0027
0.0009

0.1935
0.1870
Baseline, α = 5 , shallow
Random θ, α = 5 , shallow
0.1624
Morph. basis, α = 5 , shallow 0.0707

0.1408
0.1359
Baseline, α = 5 , shallow
Random θ, α = 5 , shallow
0.0922
Morph. basis, α = 5 , shallow 0.0403

0.0004
0.0004
0.0003
0.0006
0.0005
0.0008
0.0004
0.0004
0.0007
0.0006
0.0005
0.0003

0.0002
0.0003
0.0002
0.0003
0.0003
0.0002
0.0003
0.0003
0.0005
0.0003
0.0004
0.0002

0.0810
0.0732
0.0643
0.0109

0.0508
0.0427
0.0268
0.0081

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

Cascal (param., cal.)

Cascal (morph.-aw., raw) Baseline, α = 5

Cascal (morph.-aw., cal.) Baseline, α = 5

Table XI: Comparison of diﬀerent versions of the Cascal technique. The metrics shown are the
expected mean squared error on the log likelihood ratio with and without trimming, as deﬁned in
the text. Checkmarks in the last two columns denote estimators included in Table IV and the
ﬁgures in the main part of this paper, respectively.

Strategy

Setup

Rascal (param., raw)

62

Expected MSE

All Trimmed

Table IV Figures

Baseline, α = 100
Baseline, α = 100, shallow
Baseline, α = 100, deep
Baseline, α = 10, deep
Baseline, α = 20, deep
Baseline, α = 50, deep
Baseline, α = 200, deep
Baseline, α = 500, deep
Baseline, α = 1000, deep
Random θ
Random θ, shallow
Random θ, deep

Baseline, α = 100
Baseline, α = 100, shallow
Baseline, α = 100, deep
Baseline, α = 10, deep
Baseline, α = 20, deep
Baseline, α = 50, deep
Baseline, α = 200, deep
Baseline, α = 500, deep
Baseline, α = 1000, deep
Random θ, α = 100
Random θ, α = 100, shallow
Random θ, α = 100, deep

0.0010
0.0025
0.0009
0.0011
0.0009
0.0009
0.0009
0.0011
0.0012
0.0011
0.0030
0.0008

0.0010
0.0021
0.0009
0.0010
0.0009
0.0009
0.0008
0.0009
0.0012
0.0010
0.0025
0.0008

0.2880
0.3569
Baseline, α = 100, shallow
0.2705
Random θ, α = 100
Random θ, α = 100, shallow
0.3243
Morph. basis, α = 100, shallow 0.1909

0.1530
0.1250
Baseline, α = 100, shallow
0.1358
Random θ, α = 100
Random θ, α = 100, shallow
0.1316
Morph. basis, shallow, α = 100 0.0307

0.0003
0.0006
0.0004
0.0005
0.0004
0.0004
0.0004
0.0006
0.0007
0.0004
0.0010
0.0004

0.0003
0.0005
0.0004
0.0004
0.0004
0.0004
0.0004
0.0005
0.0006
0.0004
0.0008
0.0004

0.2024
0.2861
0.1825
0.2488
0.1673

0.0673
0.0514
0.0627
0.0539
0.0072

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

Rascal (param., cal.)

Rascal (morph.-aw., raw) Baseline, α = 100

Rascal (morph.-aw., cal.) Baseline, α = 100

Table XII: Comparison of diﬀerent versions of the Rascal technique. The metrics shown are the
expected mean squared error on the log likelihood ratio with and without trimming, as deﬁned in
the text. Checkmarks in the last two columns denote estimators included in Table IV and the
ﬁgures in the main part of this paper, respectively.

63

[1] T. Sjostrand, S. Mrenna, and P. Z. Skands, Comput. Phys. Commun. 178, 852 (2008), arXiv:0710.3820.
[2] S. Agostinelli et al. (GEANT4), Nucl. Instrum. Meth. A506, 250 (2003).
[3] K. S. Cranmer, Comput. Phys. Commun. 136, 198 (2001), arXiv:hep-ex/0011057 [hep-ex]; K. Cranmer,
G. Lewis, L. Moneta, A. Shibata, and W. Verkerke (ROOT), (2012); M. Frate, K. Cranmer, S. Kalia,
A. Vandenberg-Rodes, and D. Whiteson, (2017), arXiv:1709.05681 [physics.data-an].

[4] J. Brehmer, K. Cranmer, F. Kling, and T. Plehn, Phys. Rev. D95, 073002 (2017), arXiv:1612.05261

[hep-ph].

[5] K. Kondo, J. Phys. Soc. Jap. 57, 4126 (1988).
[6] V. M. Abazov et al. (D0), Nature 429, 638 (2004), arXiv:hep-ex/0406031 [hep-ex].
[7] P. Artoisenet and O. Mattelaer, Proceedings, 2nd International Workshop on Prospects for charged
Higgs discovery at colliders (CHARGED 2008): Uppsala, Sweden, September 16-19, 2008, PoS
CHARGED2008, 025 (2008).

[8] Y. Gao, A. V. Gritsan, Z. Guo, K. Melnikov, M. Schulze, and N. V. Tran, Phys. Rev. D81, 075022

(2010), arXiv:1001.3396 [hep-ph].

[9] J. Alwall, A. Freitas, and O. Mattelaer, Phys. Rev. D83, 074010 (2011), arXiv:1010.2263 [hep-ph].
[10] S. Bolognesi, Y. Gao, A. V. Gritsan, K. Melnikov, M. Schulze, N. V. Tran, and A. Whitbeck, Phys.

Rev. D86, 095031 (2012), arXiv:1208.4018 [hep-ph].

[11] P. Avery et al., Phys. Rev. D87, 055006 (2013), arXiv:1210.0896 [hep-ph].
[12] J. R. Andersen, C. Englert, and M. Spannowsky, Phys. Rev. D87, 015019 (2013), arXiv:1211.3011

[13] J. M. Campbell, R. K. Ellis, W. T. Giele,

and C. Williams, Phys. Rev. D87, 073005 (2013),

[14] P. Artoisenet, P. de Aquino, F. Maltoni, and O. Mattelaer, Phys. Rev. Lett. 111, 091802 (2013),

[hep-ph].

arXiv:1301.7086 [hep-ph].

arXiv:1304.6414 [hep-ph].

[15] J. S. Gainer, J. Lykken, K. T. Matchev, S. Mrenna, and M. Park, in Proceedings, 2013 Community
Summer Study on the Future of U.S. Particle Physics: Snowmass on the Mississippi (CSS2013):
Minneapolis, MN, USA, July 29-August 6, 2013 (2013) arXiv:1307.3546 [hep-ph].

[16] D. Schouten, A. DeAbreu, and B. Stelzer, Comput. Phys. Commun. 192, 54 (2015), arXiv:1407.7595

[physics.comp-ph].

[hep-ph].

[17] T. Martini and P. Uwer, JHEP 09, 083 (2015), arXiv:1506.08798 [hep-ph].
[18] A. V. Gritsan, R. Röntsch, M. Schulze, and M. Xiao, Phys. Rev. D94, 055023 (2016), arXiv:1606.03107

[19] T. Martini and P. Uwer, (2017), arXiv:1712.04527 [hep-ph].
[20] D. Atwood and A. Soni, Phys. Rev. D45, 2405 (1992).
[21] M. Davier, L. Duﬂot, F. Le Diberder, and A. Rouge, Phys. Lett. B306, 411 (1993).
[22] M. Diehl and O. Nachtmann, Z. Phys. C62, 397 (1994).
[23] D. E. Soper and M. Spannowsky, Phys. Rev. D84, 074002 (2011), arXiv:1102.3480 [hep-ph].
[24] D. E. Soper and M. Spannowsky, Phys. Rev. D87, 054012 (2013), arXiv:1211.3140 [hep-ph].
[25] D. E. Soper and M. Spannowsky, Phys. Rev. D89, 094005 (2014), arXiv:1402.1189 [hep-ph].
[26] C. Englert, O. Mattelaer, and M. Spannowsky, Phys. Lett. B756, 103 (2016), arXiv:1512.03429 [hep-ph].
[27] D. B. Rubin, Ann. Statist. 12, 1151 (1984).
[28] M. A. Beaumont, W. Zhang, and D. J. Balding, Genetics 162, 2025 (2002).
[29] P. Marjoram, J. Molitor, V. Plagnol, and S. Tavaré, Proceedings of the National Academy of Sciences

[30] S. A. Sisson, Y. Fan, and M. M. Tanaka, Proceedings of the National Academy of Sciences 104, 1760

[31] S. A. Sisson and Y. Fan, Likelihood-free MCMC (Chapman & Hall/CRC, New York.[839], 2011).
[32] J.-M. Marin, P. Pudlo, C. P. Robert, and R. J. Ryder, Statistics and Computing , 1 (2012).
[33] T. Charnock, G. Lavaux, and B. D. Wandelt, Phys. Rev. D97, 083004 (2018), arXiv:1802.03537

100, 15324 (2003).

(2007).

[astro-ph.IM].

[34] K. Cranmer, J. Pavez, and G. Louppe, (2015), arXiv:1506.02169 [stat.AP].
[35] K. Cranmer and G. Louppe, J. Brief Ideas (2016), 10.5281/zenodo.198541.
[36] Y. Fan, D. J. Nott, and S. A. Sisson, ArXiv e-prints (2012), arXiv:1212.1479 [stat.CO].

64

[37] G. Papamakarios and I. Murray, in Advances in Neural Information Processing Systems 29 , edited by
D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (Curran Associates, Inc., 2016) pp.
1028–1036.

[38] B. Paige and F. Wood, ArXiv e-prints (2016), arXiv:1602.06701 [stat.ML].
[39] R. Dutta, J. Corander, S. Kaski, and M. U. Gutmann, ArXiv e-prints

(2016), arXiv:1611.10242

[stat.ML].

[cs.LG].

[cs.CV].

[40] M. U. Gutmann, R. Dutta, S. Kaski, and J. Corander, Statistics and Computing , 1 (2017).
[41] D. Tran, R. Ranganath, and D. M. Blei, ArXiv e-prints (2017), arXiv:1702.08896 [stat.ML].
[42] G. Louppe and K. Cranmer, ArXiv e-prints (2017), arXiv:1707.07113 [stat.ML].
[43] L. Dinh, D. Krueger, and Y. Bengio, ArXiv e-prints (2014), arXiv:1410.8516 [cs.LG].
[44] D. Jimenez Rezende and S. Mohamed, ArXiv e-prints (2015), arXiv:1505.05770 [stat.ML].
[45] L. Dinh, J. Sohl-Dickstein, and S. Bengio, ArXiv e-prints (2016), arXiv:1605.08803 [cs.LG].
[46] G. Papamakarios, T. Pavlakou, and I. Murray, ArXiv e-prints (2017), arXiv:1705.07057 [stat.ML].
[47] B. Uria, M.-A. Côté, K. Gregor, I. Murray, and H. Larochelle, ArXiv e-prints (2016), arXiv:1605.02226

[48] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior,

and K. Kavukcuoglu, ArXiv e-prints (2016), arXiv:1609.03499 [cs.SD].

[49] A. van den Oord, N. Kalchbrenner, O. Vinyals, L. Espeholt, A. Graves, and K. Kavukcuoglu, ArXiv

e-prints (2016), arXiv:1606.05328 [cs.CV].

[50] A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu, ArXiv e-prints (2016), arXiv:1601.06759

[51] G. Papamakarios, D. C. Sterratt, and I. Murray, ArXiv e-prints (2018), arXiv:1805.07226 [stat.ML].
[52] J. Brehmer, K. Cranmer, G. Louppe, and J. Pavez, (2018), arXiv:1805.00013 [hep-ph].
[53] J. Brehmer, G. Louppe, J. Pavez, and K. Cranmer, (2018), arXiv:1805.12244 [stat.ML].
[54] J. Brehmer, K. Cranmer, G. Louppe, and J. Pavez, “Code repository for the paper “Constraining Eﬀect-
ive Field Theories with Machine Learning”,” http://github.com/johannbrehmer/higgs_inference
(2018).

[55] S. R. Coleman, J. Wess, and B. Zumino, Phys. Rev. 177, 2239 (1969).
[56] C. G. Callan, Jr., S. R. Coleman, J. Wess, and B. Zumino, Phys. Rev. 177, 2247 (1969).
[57] S. Weinberg, Phys. Lett. B91, 51 (1980).
[58] C. J. C. Burges and H. J. Schnitzer, Nucl. Phys. B228, 464 (1983).
[59] C. N. Leung, S. T. Love, and S. Rao, Z. Phys. C31, 433 (1986).
[60] W. Buchmuller and D. Wyler, Nucl. Phys. B268, 621 (1986).
[61] C. Arzt, M. B. Einhorn, and J. Wudka, Nucl. Phys. B433, 41 (1995), arXiv:hep-ph/9405214 [hep-ph].
[62] K. Hagiwara, S. Ishihara, R. Szalapski, and D. Zeppenfeld, Phys. Rev. D48, 2182 (1993).
[63] B. Grzadkowski, M. Iskrzynski, M. Misiak, and J. Rosiek, JHEP 10, 085 (2010), arXiv:1008.4884

[hep-ph].

[64] J. Alwall, R. Frederix, S. Frixione, V. Hirschi, F. Maltoni, O. Mattelaer, H. S. Shao, T. Stelzer,

P. Torrielli, and M. Zaro, JHEP 07, 079 (2014), arXiv:1405.0301 [hep-ph].

[65] B. Henning, X. Lu, and H. Murayama, JHEP 01, 023 (2016), arXiv:1412.1837 [hep-ph].
[66] D. de Florian et al. (LHC Higgs Cross Section Working Group), (2016), 10.23731/CYRM-2017-002,

arXiv:1610.07922 [hep-ph].

[hep-ph].

[67] J. Brehmer, A. Freitas, D. Lopez-Val, and T. Plehn, Phys. Rev. D93, 075014 (2016), arXiv:1510.03443

[68] J. Brehmer, F. Kling, T. Plehn, and T. M. P. Tait, (2017), arXiv:1712.02350 [hep-ph].
[69] J. de Favereau, C. Delaere, P. Demin, A. Giammanco, V. Lemaître, A. Mertens, and M. Selvaggi

(DELPHES 3), JHEP 02, 057 (2014), arXiv:1307.6346 [hep-ex].

[70] G. Aad et al. (ATLAS), “A morphing technique for signal modelling in a multidimensional space of

coupling parameters,” (2015), Physics note ATL-PHYS-PUB-2015-047.

[71] J. R. Dell’Aquila and C. A. Nelson, Phys. Rev. D33, 80 (1986).
[72] T. Plehn, D. L. Rainwater, and D. Zeppenfeld, Phys. Rev. Lett. 88, 051801 (2002), arXiv:hep-ph/0105325

[73] V. Hankele, G. Klamke, D. Zeppenfeld, and T. Figy, Phys. Rev. D74, 095001 (2006), arXiv:hep-

[hep-ph].

ph/0609075 [hep-ph].

[74] K. Hagiwara, Q. Li, and K. Mawatari, JHEP 07, 101 (2009), arXiv:0905.4314 [hep-ph].

65

[75] C. Englert, D. Goncalves-Netto, K. Mawatari, and T. Plehn, JHEP 01, 148 (2013), arXiv:1212.0843

[hep-ph].

[76] K. Cranmer and T. Plehn, Eur. Phys. J. C51, 415 (2007), arXiv:hep-ph/0605268 [hep-ph].
[77] T. Plehn, P. Schichtel, and D. Wiegand, Phys. Rev. D89, 054002 (2014), arXiv:1311.2591 [hep-ph].
[78] F. Kling, T. Plehn, and P. Schichtel, Phys. Rev. D95, 035026 (2017), arXiv:1607.07441 [hep-ph].
[79] P. Baldi, K. Cranmer, T. Faucett, P. Sadowski, and D. Whiteson, Eur. Phys. J. C76, 235 (2016),

arXiv:1601.07913 [hep-ex].

[80] K. Cranmer, J. Pavez, G. Louppe, and W. K. Brooks, Proceedings, 17th International Workshop on
Advanced Computing and Analysis Techniques in Physics Research (ACAT 2016): Valparaiso, Chile,
January 18-22, 2016, J. Phys. Conf. Ser. 762, 012034 (2016).

[81] J. Alsing, B. Wandelt, and S. Feeney, (2018), arXiv:1801.01497 [astro-ph.CO].
[82] A. Hyvärinen, Journal of Machine Learning Research 6, 695 (2005).
[83] D. P. Kingma and J. Ba, ArXiv e-prints (2014), arXiv:1412.6980 [cs.LG].
[84] F. Chollet et al., “Keras,” https://github.com/keras-team/keras (2015).
[85] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,
M. Kudlur, J. Levenberg, D. Mane, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens,
B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viegas, O. Vinyals,
P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, ArXiv e-prints (2016), arXiv:1603.04467
[cs.DC].

[86] J. B. Kruskal, Psychometrika 29, 115 (1964).
[87] S. S. Wilks, Annals Math. Statist. 9, 60 (1938).
[88] G. Cowan, K. Cranmer, E. Gross, and O. Vitells, Eur. Phys. J. C71, 1554 (2011), [Erratum: Eur.

Phys. J. C73, p. 2501, 2013], arXiv:1007.1727 [physics.data-an].

[89] A. Wald, Transactions of the American Mathematical Society 54, 426 (1943).
[90] G. Louppe, M. Kagan, and K. Cranmer, (2016), arXiv:1611.01046 [stat.ME].
[91] A. Mertens, in Proceedings, 15th International Workshop on Advanced Computing and Analysis Tech-

niques in Physics Research (ACAT 2013), Vol. 523 (2014) p. 012028.

