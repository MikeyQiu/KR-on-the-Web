8
1
0
2
 
n
u
J
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
2
1
1
9
0
.
5
0
8
1
:
v
i
X
r
a

Hyperbolic Neural Networks

Octavian-Eugen Ganea∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
octavian.ganea@inf.ethz.ch

Gary Bécigneul∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
gary.becigneul@inf.ethz.ch

Thomas Hofmann
Department of Computer Science
ETH Zürich
Zurich, Switzerland
thomas.hofmann@inf.ethz.ch

Abstract

Hyperbolic spaces have recently gained momentum in the context of machine
learning due to their high capacity and tree-likeliness properties. However, the
representational power of hyperbolic geometry is not yet on par with Euclidean
geometry, mostly because of the absence of corresponding hyperbolic neural
network layers. This makes it hard to use hyperbolic embeddings in downstream
tasks. Here, we bridge this gap in a principled manner by combining the formalism
of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model
of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep
learning tools: multinomial logistic regression, feed-forward and recurrent neural
networks such as gated recurrent units. This allows to embed sequential data and
perform classiﬁcation in the hyperbolic space. Empirically, we show that, even if
hyperbolic optimization tools are limited, hyperbolic sentence embeddings either
outperform or are on par with their Euclidean variants on textual entailment and
noisy-preﬁx recognition tasks.

1

Introduction

It is common in machine learning to represent data as being embedded in the Euclidean space Rn. The
main reason for such a choice is simply convenience, as this space has a vectorial structure, closed-
form formulas of distance and inner-product, and is the natural generalization of our intuition-friendly,
visual three-dimensional space. Moreover, embedding entities in such a continuous space allows to
feed them as input to neural networks, which has led to unprecedented performance on a broad range
of problems, including sentiment detection [15], machine translation [3], textual entailment [22] or
knowledge base link prediction [20, 6].

Despite the success of Euclidean embeddings, recent research has proven that many types of com-
plex data (e.g. graph data) from a multitude of ﬁelds (e.g. Biology, Network Science, Computer
Graphics or Computer Vision) exhibit a highly non-Euclidean latent anatomy [8]. In such cases, the
Euclidean space does not provide the most powerful or meaningful geometrical representations. For
example, [10] shows that arbitrary tree structures cannot be embedded with arbitrary low distortion
(i.e. almost preserving their metric) in the Euclidean space with unbounded number of dimensions,
but this task becomes surprisingly easy in the hyperbolic space with only 2 dimensions where the
exponential growth of distances matches the exponential growth of nodes with the tree depth.

∗Equal contribution.

The adoption of neural networks and deep learning in these non-Euclidean settings has been rather
limited until very recently, the main reason being the non-trivial or impossible principled general-
izations of basic operations (e.g. vector addition, matrix-vector multiplication, vector translation,
vector inner product) as well as, in more complex geometries, the lack of closed form expressions for
basic objects (e.g. distances, geodesics, parallel transport). Thus, classic tools such as multinomial
logistic regression (MLR), feed forward (FFNN) or recurrent neural networks (RNN) did not have a
correspondence in these geometries.

How should one generalize deep neural models to non-Euclidean domains ? In this paper we address
this question for one of the simplest, yet useful, non-Euclidean domains: spaces of constant negative
curvature, i.e. hyperbolic. Their tree-likeness properties have been extensively studied [12, 13, 26]
and used to visualize large taxonomies [18] or to embed heterogeneous complex networks [17]. In
machine learning, recently, hyperbolic representations greatly outperformed Euclidean embeddings
for hierarchical, taxonomic or entailment data [21, 10, 11]. Disjoint subtrees from the latent hierar-
chical structure surprisingly disentangle and cluster in the embedding space as a simple reﬂection of
the space’s negative curvature. However, appropriate deep learning tools are needed to embed feature
data in this space and use it in downstream tasks. For example, implicitly hierarchical sequence data
(e.g. textual entailment data, phylogenetic trees of DNA sequences or hierarchial captions of images)
would beneﬁt from suitable hyperbolic RNNs.

The main contribution of this paper is to bridge the gap between hyperbolic and Euclidean geometry
in the context of neural networks and deep learning by generalizing in a principled manner both the
basic operations as well as multinomial logistic regression (MLR), feed-forward (FFNN), simple and
gated (GRU) recurrent neural networks (RNN) to the Poincaré model of the hyperbolic geometry.
We do it by connecting the theory of gyrovector spaces and generalized Möbius transformations
introduced by [2, 26] with the Riemannian geometry properties of the manifold. We smoothly
parametrize basic operations and objects in all spaces of constant negative curvature using a uniﬁed
framework that depends only on the curvature value. Thus, we show how Euclidean and hyperbolic
spaces can be continuously deformed into each other. On a series of experiments and datasets we
showcase the effectiveness of our hyperbolic neural network layers compared to their "classic"
Euclidean variants on textual entailment and noisy-preﬁx recognition tasks. We hope that this paper
will open exciting future directions in the nascent ﬁeld of Geometric Deep Learning.

2 The Geometry of the Poincaré Ball

2.1 Basics of Riemannian geometry

We brieﬂy introduce basic concepts of differential geometry largely needed for a principled general-
ization of Euclidean neural networks. For more rigorous and in-depth expositions, see [23, 14].
An n-dimensional manifold M is a space that can locally be approximated by Rn: it is a generalization
to higher dimensions of the notion of a 2D surface. For x ∈ M, one can deﬁne the tangent space
TxM of M at x as the ﬁrst order linear approximation of M around x. A Riemannian metric
g = (gx)x∈M on M is a collection of inner-products gx : TxM × TxM → R varying smoothly
with x. A Riemannian manifold (M, g) is a manifold M equipped with a Riemannian metric g.
Although a choice of a Riemannian metric g on M only seems to deﬁne the geometry locally on M,
it induces global distances by integrating the length (of the speed vector living in the tangent space)
of a shortest path between two points:

(cid:90) 1

(cid:113)

d(x, y) = inf
γ

0

gγ(t)( ˙γ(t), ˙γ(t))dt,

(1)

where γ ∈ C∞([0, 1], M) is such that γ(0) = x and γ(1) = y. A smooth path γ of minimal length
between two points x and y is called a geodesic, and can be seen as the generalization of a straight-line
in Euclidean space. The parallel transport Px→y : TxM → TyM is a linear isometry between
tangent spaces which corresponds to moving tangent vectors along geodesics and deﬁnes a canonical
way to connect tangent spaces. The exponential map expx at x, when well-deﬁned, gives a way to
project back a vector v of the tangent space TxM at x, to a point expx(v) ∈ M on the manifold.
This map is often used to parametrize a geodesic γ starting from γ(0) := x ∈ M with unit-norm
direction ˙γ(0) := v ∈ TxM as t (cid:55)→ expx(tv). For geodesically complete manifolds, such as the
Poincaré ball considered in this work, expx is well-deﬁned on the full tangent space TxM. Finally, a

2

(2)

(3)

(4)

(5)

metric ˜g is said to be conformal to another metric g if it deﬁnes the same angles, i.e.

˜gx(u, v)
(cid:112)˜gx(u, u)(cid:112)˜gx(v, v)

=

gx(u, v)
(cid:112)gx(u, u)(cid:112)gx(v, v)

,

for all x ∈ M, u, v ∈ TxM \ {0}. This is equivalent to the existence of a smooth function
λ : M → R, called the conformal factor, such that ˜gx = λ2

xgx for all x ∈ M.

2.2 Hyperbolic space: the Poincaré ball

The hyperbolic space has ﬁve isometric models that one can work with [9]. Similarly as in [21] and
[11], we choose to work in the Poincaré ball. The Poincaré ball model (Dn, gD) is deﬁned by the
manifold Dn = {x ∈ Rn : (cid:107)x(cid:107) < 1} equipped with the following Riemannian metric:

gD
x = λ2

xgE, where λx :=

2
1 − (cid:107)x(cid:107)2 ,

gE = In being the Euclidean metric tensor. Note that the hyperbolic metric tensor is conformal to
the Euclidean one. The induced distance between two points x, y ∈ Dn is known to be given by

dD(x, y) = cosh−1

1 + 2

(cid:18)

(cid:107)x − y(cid:107)2
(1 − (cid:107)x(cid:107)2)(1 − (cid:107)y(cid:107)2)

(cid:19)

.

Since the Poincaré ball is conformal to Euclidean space, the angle between two vectors u, v ∈
TxDn \ {0} is given by

cos(∠(u, v)) =

gD
x (u, v)
x (u, u)(cid:112)gD

(cid:112)gD

x (v, v)

=

(cid:104)u, v(cid:105)
(cid:107)u(cid:107)(cid:107)v(cid:107)

.

2.3 Gyrovector spaces

In Euclidean space, natural operations inherited from the vectorial structure, such as vector addition,
subtraction and scalar multiplication are often useful. The framework of gyrovector spaces provides
an elegant non-associative algebraic formalism for hyperbolic geometry just as vector spaces provide
the algebraic setting for Euclidean geometry [2, 25, 26].

In particular, these operations are used in special relativity, allowing to add speed vectors belonging
to the Poincaré ball of radius c (the celerity, i.e. the speed of light) so that they remain in the ball,
hence not exceeding the speed of light.

We will make extensive use of these operations in our deﬁnitions of hyperbolic neural networks.
For c ≥ 0, denote2 by Dn
then Dn

c := {x ∈ Rn | c(cid:107)x(cid:107)2 < 1}. Note that if c = 0, then Dn
c. If c = 1 then we recover the usual ball Dn.

c is the open ball of radius 1/

c = Rn; if c > 0,

√

Möbius addition. The Möbius addition of x and y in Dn

c is deﬁned as

x ⊕c y :=

(1 + 2c(cid:104)x, y(cid:105) + c(cid:107)y(cid:107)2)x + (1 − c(cid:107)x(cid:107)2)y
1 + 2c(cid:104)x, y(cid:105) + c2(cid:107)x(cid:107)2(cid:107)y(cid:107)2

.

(6)

In particular, when c = 0, one recovers the Euclidean addition of two vectors in Rn. Note that
without loss of generality, the case c > 0 can be reduced to c = 1. Unless stated otherwise, we
will use ⊕ as ⊕1 to simplify notations. For general c > 0, this operation is not commutative nor
associative. However, it satisﬁes x ⊕c 0 = 0 ⊕c x = 0. Moreover, for any x, y ∈ Dn
c , we have
(−x) ⊕c x = x ⊕c (−x) = 0 and (−x) ⊕c (x ⊕c y) = y (left-cancellation law). The Möbius
substraction is then deﬁned by the use of the following notation: x (cid:9)c y := x ⊕c (−y). See [29,
section 2.1] for a geometric interpretation of the Möbius addition.

2We take different notations as in [25] where the author uses s = 1/

c.

√

3

Möbius scalar multiplication. For c > 0, the Möbius scalar multiplication of x ∈ Dn
r ∈ R is deﬁned as

c \ {0} by

r ⊗c x := (1/

c) tanh(r tanh−1(

c(cid:107)x(cid:107)))

√

√

x
(cid:107)x(cid:107)

,

(7)

and r ⊗c 0 := 0. Note that similarly as for the Möbius addition, one recovers the Euclidean scalar
multiplication when c goes to zero: limc→0 r ⊗c x = rx. This operation satisﬁes desirable properties
such as n ⊗c x = x ⊕c · · · ⊕c x (n additions), (r + r(cid:48)) ⊗c x = r ⊗c x ⊕c r(cid:48) ⊗c x (scalar distributivity3),
(rr(cid:48)) ⊗c x = r ⊗c (r(cid:48) ⊗c x) (scalar associativity) and |r| ⊗c x/(cid:107)r ⊗c x(cid:107) = x/(cid:107)x(cid:107) (scaling property).

c , gc) is given by4

Distance.
Euclidean one, with conformal factor λc
(Dn

If one deﬁnes the generalized hyperbolic metric tensor gc as the metric conformal to the
x := 2/(1 − c(cid:107)x(cid:107)2), then the induced distance function on
√

c) tanh−1 (cid:0)√
Again, observe that limc→0 dc(x, y) = 2(cid:107)x − y(cid:107), i.e. we recover Euclidean geometry in the limit5.
Moreover, for c = 1 we recover dD of Eq. (4).

c(cid:107) − x ⊕c y(cid:107)(cid:1) .

dc(x, y) = (2/

(8)

Hyperbolic trigonometry. Similarly as in the Euclidean space, one can deﬁne the notions of
hyperbolic angles or gyroangles (when using the ⊕c), as well as hyperbolic law of sines in the
generalized Poincaré ball (Dn

c , gc). We make use of these notions in our proofs. See Appendix A.

2.4 Connecting Gyrovector spaces and Riemannian geometry of the Poincaré ball

In this subsection, we present how geodesics in the Poincaré ball model are usually described with
Möbius operations, and push one step further the existing connection between gyrovector spaces and
the Poincaré ball by ﬁnding new identities involving the exponential map, and parallel transport.

In particular, these ﬁndings provide us with a simpler formulation of Möbius scalar multiplication,
yielding a natural deﬁnition of matrix-vector multiplication in the Poincaré ball.

Riemannian gyroline element. The Riemannian gyroline element is deﬁned for an inﬁnitesimal
dx as ds := (x + dx) (cid:9)c x, and its size is given by [26, section 3.7]:

(cid:107)ds(cid:107) = (cid:107)(x + dx) (cid:9)c x(cid:107) = (cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2).

(9)

What is remarkable is that it turns out to be identical, up to a scaling factor of 2, to the usual line
element 2(cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2) of the Riemannian manifold (Dn

c , gc).

Geodesics. The geodesic connecting points x, y ∈ Dn

c is shown in [2, 26] to be given by:

γx→y(t) := x ⊕c (−x ⊕c y) ⊗c t, with γx→y : R → Dn

c s.t. γx→y(0) = x and γx→y(1) = y.

Note that when c goes to 0, geodesics become straight-lines, recovering Euclidean geometry. In the
remainder of this subsection, we connect the gyrospace framework with Riemannian geometry.
Lemma 1. For any x ∈ Dn and v ∈ TxDn
c s.t. gc
x with direction v is given by:

x(v, v) = 1, the unit-speed geodesic starting from

γx,v(t) = x ⊕c

tanh

(cid:18)

(cid:18)√

(cid:19) v
√

c

t
2

c(cid:107)v(cid:107)

(cid:19)

, where γx,v : R → Dn s.t. γx,v(0) = x and ˙γx,v(0) = v.

(10)

(11)

Proof. One can use Eq. (10) and reparametrize it to unit-speed using Eq. (8). Alternatively, direct
computation and identiﬁcation with the formula in [11, Thm. 1] would give the same result. Using
Eq. (8) and Eq. (11), one can sanity-check that dc(γ(0), γ(t)) = t, ∀t ∈ [0, 1].

3⊗c has priority over ⊕c in the sense that a ⊗c b ⊕c c := (a ⊗c b) ⊕c c and a ⊕c b ⊗c c := a ⊕c (b ⊗c c).
4The notation −x ⊕c y should always be read as (−x) ⊕c y and not −(x ⊕c y).
5The factor 2 comes from the conformal factor λx = 2/(1 − (cid:107)x(cid:107)2), which is a convention setting the

curvature to −1.

4

Exponential and logarithmic maps. The following lemma gives the closed-form derivation of
exponential and logarithmic maps.
Lemma 2. For any point x ∈ Dn
map logc
c → TxDn
(cid:18)

c are given for v (cid:54)= 0 and y (cid:54)= x by:
(cid:18)√

c , the exponential map expc

c and the logarithmic

x : TxDn

c → Dn

x : Dn

√

(cid:19)

, logc

x(y) =

√

tanh−1(

c(cid:107) − x ⊕c y(cid:107))

expc

x(v) = x ⊕c

tanh

c

λc
x(cid:107)v(cid:107)
2

(cid:19) v
√

c(cid:107)v(cid:107)

2
cλc
x

−x ⊕c y
(cid:107) − x ⊕c y(cid:107)
(12)

.

Proof. Following the proof of [11, Cor. 1.1], one gets expc
gives the formula for expc

x. Algebraic check of the identity logc

x(v) = γx,
x(expc

v

x(cid:107)v(cid:107) (λc
λc

x(cid:107)v(cid:107)). Using Eq. (11)

x(v)) = v concludes.

The above maps have more appealing forms when x = 0, namely for v ∈ T0Dn

c \ {0}, y ∈ Dn

c \ {0}:

expc

0(v) = tanh(

c(cid:107)v(cid:107))

√

, logc

0(y) = tanh−1(

c(cid:107)y(cid:107))

√

(13)

√

y
c(cid:107)y(cid:107)

.

√

v
c(cid:107)v(cid:107)

Moreover, we still recover Euclidean geometry in the limit c → 0, as limc→0 expc
Euclidean exponential map, and limc→0 logc

x(y) = y − x is the Euclidean logarithmic map.

x(v) = x + v is the

Möbius scalar multiplication using exponential and logarithmic maps. We studied the expo-
nential and logarithmic maps in order to gain a better understanding of the Möbius scalar multiplica-
tion (Eq. (7)). We found the following:
Lemma 3. The quantity r ⊗ x can actually be obtained by projecting x in the tangent space at 0
with the logarithmic map, multiplying this projection by the scalar r in T0Dn
c , and then projecting it
back on the manifold with the exponential map:
0(r logc

∀r ∈ R, x ∈ Dn
c .

r ⊗c x = expc

0(x)),

(14)

In addition, we recover the well-known relation between geodesics connecting two points and the
exponential map:

γx→y(t) = x ⊕c (−x ⊕c y) ⊗c t = expc

x(t logc

x(y)),

t ∈ [0, 1].

(15)

This last result enables us to generalize scalar multiplication in order to deﬁne matrix-vector multipli-
cation between Poincaré balls, one of the essential building blocks of hyperbolic neural networks.

Parallel transport. Finally, we connect parallel transport (from T0Dn
the following theorem, which we prove in appendix B.
Theorem 4. In the manifold (Dn
vector v ∈ T0Dn

c to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a
c is given by the following isometry:
λc
0
λc
x

x(x ⊕c expc

0(v)) =

0→x(v) = logc
P c

(16)

v.

c ) to gyrovector spaces with

As we’ll see later, this result is crucial in order to deﬁne and optimize parameters shared between
different tangent spaces, such as biases in hyperbolic neural layers or parameters of hyperbolic MLR.

3 Hyperbolic Neural Networks

Neural networks can be seen as being made of compositions of basic operations, such as linear
maps, bias translations, pointwise non-linearities and a ﬁnal sigmoid or softmax layer. We ﬁrst
explain how to construct a softmax layer for logits lying in a Poincaré ball. Then, we explain how
to transform a mapping between two Euclidean spaces as one between Poincaré balls, yielding
matrix-vector multiplication and pointwise non-linearities in the Poincaré ball. Finally, we present
possible adaptations of various recurrent neural networks to the hyperbolic domain.

5

3.1 Hyperbolic multiclass logistic regression

In order to perform multi-class classiﬁcation on the Poincaré ball, one needs to generalize multinomial
logistic regression (MLR) − also called softmax regression − to the Poincaré ball.

Reformulating Euclidean MLR. Let’s ﬁrst reformulate Euclidean MLR from the perspective of
distances to margin hyperplanes, as in [19, Section 5]. This will allow us to easily generalize it.

Given K classes, one learns a margin hyperplane for each such class using softmax probabilities:

∀k ∈ {1, ..., K},

p(y = k|x) ∝ exp (((cid:104)ak, x(cid:105) − bk)) , where bk ∈ R, x, ak ∈ Rn.

(17)

Note that any afﬁne hyperplane in Rn can be written with a normal vector a and a scalar shift b:

Ha,b = {x ∈ Rn : (cid:104)a, x(cid:105) − b = 0}, where a ∈ Rn \ {0}, and b ∈ R.

(18)

As in [19, Section 5], we note that (cid:104)a, x(cid:105) − b = sign((cid:104)a, x(cid:105) − b)(cid:107)a(cid:107)d(x, Ha,b). Using Eq. (17):

p(y = k|x) ∝ exp(sign((cid:104)ak, x(cid:105) − bk)(cid:107)ak(cid:107)d(x, Hak,bk )), bk ∈ R, x, ak ∈ Rn.

(19)

As it is not immediately obvious how to generalize the Euclidean hyperplane of Eq. (18) to other
spaces such as the Poincaré ball, we reformulate it as follows:

˜Ha,p = {x ∈ Rn : (cid:104)−p + x, a(cid:105) = 0} = p + {a}⊥, where p ∈ Rn, a ∈ Rn \ {0}.

(20)

This new deﬁnition relates to the previous one as ˜Ha,p = Ha,(cid:104)a,p(cid:105). Rewriting Eq. (19) with b = (cid:104)a, p(cid:105):
p(y = k|x) ∝ exp(sign((cid:104)−pk + x, ak(cid:105))(cid:107)ak(cid:107)d(x, ˜Hak,pk )), with pk, x, ak ∈ Rn.

(21)

It is now natural to adapt the previous deﬁnition to the hyperbolic setting by replacing + by ⊕c:
Deﬁnition 3.1 (Poincaré hyperplanes). For p ∈ Dn
p(z, a) = 0} = {z ∈ TpDn
gc

c : (cid:104)z, a(cid:105) = 0}. Then, we deﬁne Poincaré hyperplanes as

c \ {0}, let {a}⊥ := {z ∈ TpDn
c :

c , a ∈ TpDn

˜H c

a,p := {x ∈ Dn

c : (cid:104)logc

p(x), a(cid:105)p = 0} = expc

p({a}⊥) = {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}.

(22)

The last equality is shown appendix C. ˜H c
all geodesics in Dn
hypergyroplanes, see [27, deﬁnition 5.8]. A 3D hyperplane example is depicted in Fig. 1.

a,p can also be described as the union of images of
c orthogonal to a and containing p. Notice that our deﬁnition matches that of

Next, we need the following theorem, proved in appendix D:
Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(23)

Final formula for MLR in the Poincaré ball. Putting together Eq. (21) and Thm. 5, we get the
hyperbolic MLR formulation. Given K classes and k ∈ {1, . . . , K}, pk ∈ Dn
c \ {0}:

c , ak ∈ Tpk

Dn

p(y = k|x) ∝ exp(sign((cid:104)−pk ⊕c x, ak(cid:105))

gc
pk

(ak, ak)dc(x, ˜H c

)),

ak,pk

∀x ∈ Dn
c ,

(24)

or, equivalently

p(y = k|x) ∝ exp

(cid:18) λc
pk

(cid:107)ak(cid:107)
√
c

sinh−1

(cid:18)

√
2

c(cid:104)−pk ⊕c x, ak(cid:105)

(cid:19)(cid:19)

(1 − c(cid:107) − pk ⊕c x(cid:107)2)(cid:107)ak(cid:107)

,

∀x ∈ Dn
c .

(25)

this goes to p(y = k|x) ∝ exp(4(cid:104)−pk + x, ak(cid:105)) =

Notice that when c goes to zero,
exp((λ0
pk

)2(cid:104)−pk + x, ak(cid:105)) = exp((cid:104)−pk + x, ak(cid:105)0), recovering the usual Euclidean softmax.
However, at this point it is unclear how to perform optimization over ak, since it lives in Tpk
hence depends on pk. The solution is that one should write ak = P c
)a(cid:48)
k ∈ T0Dn
a(cid:48)

c = Rn, and optimize a(cid:48)

k as a Euclidean parameter.

k) = (λc

0/λc
pk

0→pk

(a(cid:48)

Dn
c and
k, where

1
√
c

(cid:113)

6

3.2 Hyperbolic feed-forward layers

In order to deﬁne hyperbolic neural networks, it is crucial to de-
ﬁne a canonically simple parametric family of transformations,
playing the role of linear mappings in usual Euclidean neural
networks, and to know how to apply pointwise non-linearities.
Inspiring ourselves from our reformulation of Möbius scalar
multiplication in Eq. (14), we deﬁne:
Deﬁnition 3.2 (Möbius version). For f : Rn → Rm, we deﬁne
the Möbius version of f as the map from Dn

c to Dm

c by:

f ⊗c(x) := expc

0(f (logc

0(x))),

(26)

where expc

0 : T0m

Dm

c → Dm

c and logc

0 : Dn

c → T0n

Dn
c .

Figure 1: An example of a hyper-
bolic hyperplane in D3
1 plotted us-
ing sampling. The red point is p.
The shown normal axis to the hy-
perplane through p is parallel to a.

Note that similarly as for other Möbius operations, we recover
the Euclidean mapping in the limit c → 0 if f is continuous, as limc→0 f ⊗c(x) = f (x). This
deﬁnition satisﬁes a few desirable properties too, such as: (f ◦ g)⊗c = f ⊗c ◦ g⊗c for f : Rm → Rl
and g : Rn → Rm (morphism property), and f ⊗c(x)/(cid:107)f ⊗c(x)(cid:107) = f (x)/(cid:107)f (x)(cid:107) for f (x) (cid:54)= 0
(direction preserving). It is then straight-forward to prove the following result:
Lemma 6 (Möbius matrix-vector multiplication). If M : Rn → Rm is a linear map, which we
identify with its matrix representation, then ∀x ∈ Dn

c , if M x (cid:54)= 0 we have

M ⊗c(x) = (1/

c) tanh

√

(cid:18) (cid:107)M x(cid:107)
(cid:107)x(cid:107)

√

tanh−1(

c(cid:107)x(cid:107))

(cid:19) M x
(cid:107)M x(cid:107)

,

(27)

and M ⊗c(x) = 0 if M x = 0. Moreover, if we deﬁne the Möbius matrix-vector multiplication of
M ∈ Mm,n(R) and x ∈ Dn
c by M ⊗c x := M ⊗c(x), then we have (M M (cid:48)) ⊗c x = M ⊗c (M (cid:48) ⊗c x)
for M ∈ Ml,m(R) and M (cid:48) ∈ Mm,n(R) (matrix associativity), (rM ) ⊗c x = r ⊗c (M ⊗c x) for
r ∈ R and M ∈ Mm,n(R) (scalar-matrix associativity) and M ⊗c x = M x for all M ∈ On(R)
(rotations are preserved).

Pointwise non-linearity.
ϕ⊗c can be applied to elements of the Poincaré ball.

If ϕ : Rn → Rn is a pointwise non-linearity, then its Möbius version

Bias translation. The generalization of a translation in the Poincaré ball is naturally given by
moving along geodesics. But should we use the Möbius sum x ⊕c b with a hyperbolic bias b or the
x(b(cid:48)) with a Euclidean bias b(cid:48)? These views are uniﬁed with parallel transport
exponential map expc
c by a bias b ∈ Dn
(see Thm 4). Möbius translation of a point x ∈ Dn
(cid:18) λc
0
λc
x

c is given by
(cid:19)

x ← x ⊕c b = expc

0(b))) = expc
x

0→x(logc

x(P c

logc

0(b)

(28)

.

We recover Euclidean translations in the limit c → 0. Note that bias translations play a particular
Indeed, consider multiple layers of the form fk(x) = ϕk(Mkx), each of
role in this model.
which having Möbius version f ⊗c
k (Mk ⊗c x). Then their composition can be re-written
f ⊗c
k ◦ · · · ◦ f ⊗c
1 = expc
0. This means that these operations can essentially be
performed in Euclidean space. Therefore, it is the interposition between those with the bias translation
of Eq. (28) which differentiates this model from its Euclidean counterpart.

k (x) = ϕ⊗c
0 ◦fk ◦ · · · ◦ f1 ◦ logc

If a vector x ∈ Rn+p is the (vertical) concatenation
Concatenation of multiple input vectors.
of two vectors x1 ∈ Rn, x2 ∈ Rp, and M ∈ Mm,n+p(R) can be written as the (horizontal)
concatenation of two matrices M1 ∈ Mm,n(R) and M2 ∈ Mm,p(R), then M x = M1x1 + M2x2.
We generalize this to hyperbolic spaces: if we are given x1 ∈ Dn
c ×Dp
c ,
and M, M1, M2 as before, then we deﬁne M ⊗c x := M1 ⊗c x1 ⊕c M2 ⊗c x2. Note that when c goes
to zero, we recover the Euclidean formulation, as limc→0 M ⊗c x = limc→0 M1 ⊗c x1 ⊕c M2 ⊗c x2 =
M1x1 + M2x2 = M x. Moreover, hyperbolic vectors x ∈ Dn
c can also be "concatenated" with real
features y ∈ R by doing: M ⊗c x ⊕c y ⊗c b with learnable b ∈ Dm

c , x = (x1 x2)T ∈ Dn

c and M ∈ Mm,n(R).

c , x2 ∈ Dp

7

3.3 Hyperbolic RNN

Naive RNN. A simple RNN can be deﬁned by ht+1 = ϕ(W ht + U xt + b) where ϕ is a pointwise
non-linearity, typically tanh, sigmoid, ReLU, etc. This formula can be naturally generalized to the
hyperbolic space as follows. For parameters W ∈ Mm,n(R), U ∈ Mm,d(R), b ∈ Dm
c , we deﬁne:

ht+1 = ϕ⊗c (W ⊗c ht ⊕c U ⊗c xt ⊕c b),

ht ∈ Dn

c , xt ∈ Dd
c .

(29)

Note that if inputs xt’s are Euclidean, one can write ˜xt := expc
expc

(U xt)) = W ⊗c ht ⊕c expc

(P c

W ⊗cht

0→W ⊗cht

0(U xt) = W ⊗c ht ⊕c U ⊗c ˜xt.

0(xt) and use the above formula, since

GRU architecture. One can also adapt the GRU architecture:
rt = σ(W rht−1 + U rxt + br),
zt = σ(W zht−1 + U zxt + bz),
˜ht = ϕ(W (rt (cid:12) ht−1) + U xt + b), ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht,

(30)

where (cid:12) denotes pointwise product. First, how should we adapt the pointwise multiplication by a
scaling gate? Note that the deﬁnition of the Möbius version (see Eq. (26)) can be naturally extended
to maps f : Rn × Rp → Rm as f ⊗c : (h, h(cid:48)) ∈ Dn
0(h(cid:48)))). In
c (cid:55)→ expc
0(h(cid:48))) =
particular, choosing f (h, h(cid:48)) := σ(h) (cid:12) h(cid:48) yields6 f ⊗c(h, h(cid:48)) = expc
diag(σ(logc

0(h))) ⊗c h(cid:48). Hence we adapt rt (cid:12) ht−1 to diag(rt) ⊗c ht−1 and the reset gate rt to:
0(W r ⊗c ht−1 ⊕c U r ⊗c xt ⊕c br),

0(h), logc
0(h)) (cid:12) logc

0(f (logc
0(σ(logc

rt = σ logc

c × Dp

(31)

and similarly for the update gate zt. Note that as the argument of σ in the above is unbounded, rt and
zt can a priori take values onto the full range (0, 1). Now the intermediate hidden state becomes:
˜ht = ϕ⊗c ((W diag(rt)) ⊗c ht−1 ⊕c U ⊗c xt ⊕ b),

(32)

where Möbius matrix associativity simpliﬁes W ⊗c (diag(rt) ⊗c ht−1) into (W diag(rt)) ⊗c ht−1.
Finally, we propose to adapt the update-gate equation as

ht = ht−1 ⊕c diag(zt) ⊗c (−ht−1 ⊕c

˜ht).

(33)

Note that when c goes to zero, one recovers the usual GRU. Moreover, if zt = 0 or zt = 1, then ht
becomes ht−1 or ˜ht respectively, similarly as in the usual GRU. This adaptation was obtained by
adapting [24]: in this work, the authors re-derive the update-gate mechanism from a ﬁrst principle
called time-warping invariance. We adapted their derivation to the hyperbolic setting by using the
notion of gyroderivative [4] and proving a gyro-chain-rule (see appendix E).

4 Experiments

SNLI task and dataset. We evaluate our method on two tasks. The ﬁrst is natural language
inference, or textual entailment. Given two sentences, a premise (e.g. "Little kids A. and B. are
playing soccer.") and a hypothesis (e.g. "Two children are playing outdoors."), the binary classiﬁcation
task is to predict whether the second sentence can be inferred from the ﬁrst one. This deﬁnes a partial
order in the sentence space. We test hyperbolic networks on the biggest real dataset for this task,
SNLI [7]. It consists of 570K training, 10K validation and 10K test sentence pairs. Following [28],
we merge the "contradiction" and "neutral" classes into a single class of negative sentence pairs, while
the "entailment" class gives the positive pairs.

PREFIX task and datasets. We conjecture that the improvements of hyperbolic neural networks
are more signiﬁcant when the underlying data structure is closer to a tree. To test this, we design a
proof-of-concept task of detection of noisy preﬁxes, i.e. given two sentences, one has to decide if the
second sentence is a noisy preﬁx of the ﬁrst, or a random sentence. We thus build synthetic datasets
PREFIX-Z% (for Z being 10, 30 or 50) as follows: for each random ﬁrst sentence of random length
at most 20 and one random preﬁx of it, a second positive sentence is generated by randomly replacing
Z% of the words of the preﬁx, and a second negative sentence of same length is randomly generated.
Word vocabulary size is 100, and we generate 500K training, 10K validation and 10K test pairs.

6If x has n coordinates, then diag(x) denotes the diagonal matrix of size n with xi’s on its diagonal.

8

Models architecture. Our neural network layers can be used in a plug-n-play manner exactly like
standard Euclidean layers. They can also be combined with Euclidean layers. However, optimization
w.r.t. hyperbolic parameters is different (see below) and based on Riemannian gradients which
are just rescaled Euclidean gradients when working in the conformal Poincaré model [21]. Thus,
back-propagation can be applied in the standard way.

In our setting, we embed the two sentences using two distinct hyperbolic RNNs or GRUs. The
sentence embeddings are then fed together with their squared distance (hyperbolic or Euclidean,
depending on their geometry) to a FFNN (Euclidean or hyperbolic, see Sec. 3.2) which is further
fed to an MLR (Euclidean or hyperbolic, see Sec. 3.1) that gives probabilities of the two classes
(entailment vs neutral). We use cross-entropy loss on top. Note that hyperbolic and Euclidean layers
can be mixed, e.g. the full network can be hyperbolic and only the last layer be Euclidean, in which
case one has to use log0 and exp0 functions to move between the two manifolds in a correct manner
as explained for Eq. 26.

Optimization. Our models have both Euclidean (e.g. weight matrices in both Euclidean and
hyperbolic FFNNs, RNNs or GRUs) and hyperbolic parameters (e.g. word embeddings or biases for
the hyperbolic layers). We optimize the Euclidean parameters with Adam [16] (learning rate 0.001).
Hyperbolic parameters cannot be updated with an equivalent method that keeps track of gradient
history due to the absence of a Riemannian Adam. Thus, they are optimized using full Riemannian
stochastic gradient descent (RSGD) [5, 11]. We also experiment with projected RSGD [21], but
optimization was sometimes less stable. We use a different constant learning rate for word embeddings
(0.1) and other hyperbolic weights (0.01) because words are updated less frequently.

Numerical errors. Gradients of the basic operations deﬁned above (e.g. ⊕c, exponential map) are
c(cid:107)x(cid:107) = 1. Thus, we
not deﬁned when the hyperbolic argument vectors are on the ball border, i.e.
always project results of these operations in the ball of radius 1 − (cid:15), where (cid:15) = 10−5. Numerical
errors also appear when hyperbolic vectors get closer to 0, thus we perturb them with an (cid:15)(cid:48) = 10−15
before they are used in any of the above operations. Finally, arguments of the tanh function are
clipped between ±15 to avoid numerical errors, while arguments of tanh−1 are clipped to at most
1 − 10−5.

√

Hyperparameters. For all methods, baselines and datasets, we use c = 1, word and hidden state
embedding dimension of 5 (we focus on the low dimensional setting that was shown to already
be effective [21]), batch size of 64. We ran all methods for a ﬁxed number of 30 epochs. For all
models, we experiment with both identity (no non-linearity) or tanh non-linearity in the RNN/GRU
cell, as well as identity or ReLU after the FFNN layer and before MLR. As expected, for the fully
Euclidean models, tanh and ReLU respectively surpassed the identity variant by a large margin. We
only report the best Euclidean results. Interestingly, for the hyperbolic models, using only identity for
both non-linearities works slightly better and this is likely due to two facts: i) our hyperbolic layers
already contain non-linearities by their nature, ii) tanh is limiting the output domain of the sentence
embeddings, but the hyperbolic speciﬁc geometry is more pronounced at the ball border, i.e. at the
hyperbolic "inﬁnity", compared to the center of the ball.

For the results shown in Tab. 1, we run each model (baseline or ours) exactly 3 times and report the
test result corresponding to the best validation result from these 3 runs. We do this because the highly
non-convex spectrum of hyperbolic neural networks sometimes results in convergence to poor local
minima, suggesting that initialization is very important.

Results. Results are shown in Tab. 1. Note that the fully Euclidean baseline models might have
an advantage over hyperbolic baselines because more sophisticated optimization algorithms such
as Adam do not have a hyperbolic analogue at the moment. We ﬁrst observe that all GRU models
overpass their RNN variants. Hyperbolic RNNs and GRUs have the most signiﬁcant improvement
over their Euclidean variants when the underlying data structure is more tree-like, e.g. for PREFIX-
10% − for which the tree relation between sentences and their preﬁxes is more prominent − we
reduce the error by a factor of 3.35 for hyperbolic vs Euclidean RNN, and by a factor of 1.5 for
hyperbolic vs Euclidean GRU. As soon as the underlying structure diverges more and more from
a tree, the accuracy gap decreases − for example, for PREFIX-50% the noise heavily affects the
representational power of hyperbolic networks. Also, note that on SNLI our methods perform
similarly as with their Euclidean variants. Moreover, hyperbolic and Euclidean MLR are on par when

9

SNLI

PREFIX-10% PREFIX-30% PREFIX-50%

FULLY EUCLIDEAN RNN
HYPERBOLIC RNN+FFNN, EUCL MLR
FULLY HYPERBOLIC RNN
FULLY EUCLIDEAN GRU
HYPERBOLIC GRU+FFNN, EUCL MLR
FULLY HYPERBOLIC GRU

79.34 %
79.18 %
78.21 %
81.52 %
79.76 %
81.19 %

89.62 %
96.36 %
96.91 %
95.96 %
97.36 %
97.14 %

81.71 %
87.83 %
87.25 %
86.47 %
88.47 %
88.26 %

72.10 %
76.50 %
62.94 %
75.04 %
76.87 %
76.44 %

Table 1: Test accuracies for various models and four datasets. "Eucl" denotes Euclidean. All word
and sentence embeddings have dimension 5. We highlight in bold the best baseline (or baselines, if
the difference is less than 0.5%).

used in conjunction with hyperbolic sentence embeddings, suggesting further empirical investigation
is needed for this direction (see below).

We also observe that, in the hyperbolic setting, accuracy tends to increase when sentence embeddings
start increasing, and gets better as their norms converge towards 1 (the ball border for c = 1). Unlike
in the Euclidean case, this behavior does happen only after a few epochs and suggests that the model
should ﬁrst adjust the angular layout in order to disentangle the representations, before increasing their
norms to fully exploit the strong clustering property of the hyperbolic geometry. Similar behavior
was observed in the context of embedding trees by [21]. Details in appendix F.

MLR classiﬁcation experiments.
For the sentence entailment classi-
ﬁcation task we do not see a clear
advantage of hyperbolic MLR com-
pared to its Euclidean variant. A pos-
sible reason is that, when trained end-
to-end, the model might decide to
place positive and negative embed-
dings in a manner that is already well
separated with a classic MLR. As a
consequence, we further investigate
MLR for the task of subtree classiﬁ-
cation. Using an open source imple-
mentation7 of [21], we pre-trained
Poincaré embeddings of the Word-
Net noun hierarchy (82,115 nodes).
We then choose one node in this tree
(see Table 2) and classify all other
nodes (solely based on their embed-
dings) as being part of the subtree
rooted at this node. All nodes in such a subtree are divided into positive training nodes (80%) and
positive test nodes (20%). The same splitting procedure is applied for the remaining WordNet nodes
that are divided into a negative training and negative test set respectively. Three variants of MLR
are then trained on top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation
task: hyperbolic MLR, Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean
MLR applied after mapping all embeddings in the tangent space at 0 using the log0 map. We use
different embedding dimensions : 2, 3, 5 and 10. For the hyperbolic MLR, we use full Riemannian
SGD with a learning rate of 0.001. For the two Euclidean models we use ADAM optimizer and the
same learning rate. During training, we always sample the same number of negative and positive
nodes in each minibatch of size 16; thus positive nodes are frequently resampled. All methods are
trained for 30 epochs and the ﬁnal F1 score is reported (no hyperparameters to validate are used, thus
we do not require a validation set). This procedure is repeated for four subtrees of different sizes.

Figure 2: Hyperbolic (left) vs Direct Euclidean (right) binary
MLR used to classify nodes as being part in the GROUP.N.01
subtree of the WordNet noun hierarchy solely based on their
Poincaré embeddings. The positive points (from the subtree)
are in blue, the negative points (the rest) are in red and the
trained positive separation hyperplane is depicted in green.

Quantitative results are presented in Table 2. We can see that the hyperbolic MLR overpasses
its Euclidean variants in almost all settings, sometimes by a large margin. Moreover, to provide

7https://github.com/dalab/hyperbolic_cones

10

WORDNET
SUBTREE

ANIMAL.N.01
3218 / 798

GROUP.N.01
6649 / 1727

WORKER.N.01
861 / 254

MAMMAL.N.01
953 / 228

MODEL

D = 2

D = 3

D = 5

D = 10

HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL

47.43 ± 1.07%
41.69 ± 0.19%
38.89 ± 0.01%
81.72 ± 0.17%
61.13 ± 0.42%
60.75 ± 0.24%
12.68 ± 0.82%
10.86 ± 0.01%
9.04 ± 0.06%
32.01 ± 17.14%
15.58 ± 0.04%
13.10 ± 0.13%

91.92 ± 0.61%
68.43 ± 3.90%
62.57 ± 0.61%
89.87 ± 2.73%
63.56 ± 1.22%
61.98 ± 0.57%
24.09 ± 1.49%
22.39 ± 0.04%
22.57 ± 0.20%
87.54 ± 4.55%
44.68 ± 1.87%
44.89 ± 1.18%

98.07 ± 0.55%
95.59 ± 1.18%
89.21 ± 1.34%
87.89 ± 0.80%
67.82 ± 0.81%
67.92 ± 0.74%
55.46 ± 5.49%
35.23 ± 3.16%
26.47 ± 0.78%
88.73 ± 3.22%
59.35 ± 1.31%
52.51 ± 0.85%

99.26 ± 0.59%
99.36 ± 0.18%
98.27 ± 0.70%
91.91 ± 3.07%
91.38 ± 1.19%
91.41 ± 0.18%
66.83 ± 11.38%
47.29 ± 3.93%
36.66 ± 2.74%
91.37 ± 6.09%
77.76 ± 5.08%
56.11 ± 2.21%

Table 2: Test F1 classiﬁcation scores for four different subtrees of WordNet noun tree. All nodes
in such a subtree are divided into positive training nodes (80%) and positive test nodes (20%);
these counts are shown below each subtree root. The same splitting procedure is applied for the
remaining nodes to obtain negative training and test sets. Three variants of MLR are then trained on
top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation task: hyperbolic MLR,
Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean MLR applied after
mapping all embeddings in the tangent space at 0 using the log0 map. 95% conﬁdence intervals for 3
different runs are shown for each method and each different embedding dimension (2, 3, 5 or 10).

further understanding, we plot the 2-dimensional embeddings and the trained separation hyperplanes
(geodesics in this case) in Figure 2. We can see that respecting the hyperbolic geometry is very
important for a quality classiﬁcation model.

5 Conclusion

We showed how classic Euclidean deep learning tools such as MLR, FFNNs, RNNs or GRUs can be
generalized in a principled manner to all spaces of constant negative curvature combining Riemannian
geometry with the elegant theory of gyrovector spaces. Empirically we found that our models
outperform or are on par with corresponding Euclidean architectures on sequential data with implicit
hierarchical structure. We hope to trigger exciting future research related to better understanding
of the hyperbolic non-convexity spectrum and development of other non-Euclidean deep learning
methods.
Our data and Tensorﬂow [1] code are publicly available8.

Acknowledgements

We thank Igor Petrovski for useful pointers regarding the implementation.

This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement
number 167176. Gary Bécigneul is also funded by the Max Planck ETH Center for Learning
Systems.

References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for
large-scale machine learning. 2016.

[2] Ungar Abraham Albert. Analytic hyperbolic geometry and Albert Einstein’s special theory of

relativity. World scientiﬁc, 2008.

8https://github.com/dalab/hyperbolic_nn

11

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of the International Conference on Learning
Representations (ICLR), 2015.

[4] Graciela S Birman and Abraham A Ungar. The hyperbolic derivative in the poincaré ball model
of hyperbolic geometry. Journal of mathematical analysis and applications, 254(1):321–333,
2001.

[5] S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on

Automatic Control, 58(9):2217–2229, Sept 2013.

[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems (NIPS), pages 2787–2795, 2013.

[7] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 632–642. Association
for Computational Linguistics, 2015.

[8] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,
34(4):18–42, 2017.

[9] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry.

Flavors of geometry, 31:59–115, 1997.

[10] Christopher De Sa, Albert Gu, Christopher Ré, and Frederic Sala. Representation tradeoffs for

hyperbolic embeddings. arXiv preprint arXiv:1804.03329, 2018.

[11] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In Proceedings of the thirty-ﬁfth international conference
on machine learning (ICML), 2018.

[12] Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pages 75–263. Springer, 1987.

[13] Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the

Cambridge Philosophical Society, page 1–17, 2017.

[14] Christopher Hopper and Ben Andrews. The Ricci ﬂow in Riemannian geometry. Springer, 2010.

[15] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1746–1751. Association for Computational Linguistics, 2014.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations (ICLR), 2015.

[17] Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marián Boguná.

Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.

[18] John Lamping, Ramana Rao, and Peter Pirolli. A focus+ context technique based on hyperbolic
geometry for visualizing large hierarchies. In Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 401–408. ACM Press/Addison-Wesley Publishing Co.,
1995.

[19] Guy Lebanon and John Lafferty. Hyperplane margin classiﬁers on the multinomial manifold. In
Proceedings of the international conference on machine learning (ICML), page 66. ACM, 2004.

[20] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective
learning on multi-relational data. In Proceedings of the international conference on machine
learning (ICML), volume 11, pages 809–816, 2011.

12

[21] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical repre-
sentations. In Advances in Neural Information Processing Systems (NIPS), pages 6341–6350,
2017.

[22] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blun-
som. Reasoning about entailment with neural attention. In Proceedings of the International
Conference on Learning Representations (ICLR), 2015.

[23] Michael Spivak. A comprehensive introduction to differential geometry. Publish or perish, 1979.

[24] Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In Proceedings of

the International Conference on Learning Representations (ICLR), 2018.

[25] Abraham A Ungar. Hyperbolic trigonometry and its application in the poincaré ball model of

hyperbolic geometry. Computers & Mathematics with Applications, 41(1-2):135–147, 2001.

[26] Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis

Lectures on Mathematics and Statistics, 1(1):1–194, 2008.

[27] Abraham Albert Ungar. Analytic hyperbolic geometry in n dimensions: An introduction. CRC

Press, 2014.

[28] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. In Proceedings of the International Conference on Learning Representations (ICLR),
2016.

[29] J Vermeer. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic

plane. Topology and its Applications, 152(3):226–242, 2005.

13

A Hyperbolic Trigonometry

Hyperbolic angles. For A, B, C ∈ Dn
c , we denote by ∠A := ∠BAC the angle between the two
geodesics starting from A and ending at B and C respectively. This angle can be deﬁned in two
equivalent ways: i) either using the angle between the initial velocities of the two geodesics as given
by Eq. 5, or ii) using the formula

cos(∠A) =

(cid:28) (−A) ⊕c B
(cid:107)(−A) ⊕c B(cid:107)

,

(−A) ⊕c C
(cid:107)(−A) ⊕c C(cid:107)

(cid:29)

,

In this case, ∠A is also called a gyroangle in the work of [26, section 4].

Hyperbolic law of sines. We state here the hyperbolic law of sines. If for A, B, C ∈ Dn
c , we
denote by ∠B := ∠ABC the angle between the two geodesics starting from B and ending at A and
C respectively, and by ˜c = dc(B, A) the length of the hyperbolic segment BA (and similarly for
others), then we have:

sin(∠A)
√
c˜a)
sinh(

=

sin(∠B)
√
c˜b)
sinh(

=

sin(∠C)
√
c˜c)
sinh(

.

Note that one can also adapt the hyperbolic law of cosines to the hyperbolic space.

B Proof of Theorem 4

Theorem 4.
In the manifold (Dn
to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a vector v ∈ T0Dn
c

c is given by the following isometry:
λc
0
λc
x

0→x(v) = logc
P c

x(x ⊕c expc

0(v)) =

v.

Proof. The geodesic in Dn
v ∈ T0Dn
γ (i.e. X(t) ∈ Tγ(t)Dn

c from 0 to x is given in Eq. (10) by γ(t) = x ⊗c t, for t ∈ [0, 1]. Let
c . Then it is of common knowledge that there exists a unique parallel9 vector ﬁeld X along

c , ∀t ∈ [0, 1]) such that X(0) = v. Let’s deﬁne:
X : t ∈ [0, 1] (cid:55)→ logc

γ(t)(γ(t) ⊕c expc

0(v)) ∈ Tγ(t)Dn
c .

Clearly, X is a vector ﬁeld along γ such that X(0) = v. Now deﬁne
0→x : v ∈ T0Dn
P c

x(x ⊕c expc

0(v)) ∈ TxDn
c .

c (cid:55)→ logc
0→x(v) = λc

c . Since P c

0→x is a linear isometry from T0Dn
v, hence P c
c
0→x(v) = X(1), it is enough to prove that X is parallel in order to guarantee that

From Eq. (12), it is easily seen that P c
to TxDn
c to TxDn
0→x is the parallel transport from T0Dn
P c
c .
Since X is a vector ﬁeld along γ, its covariant derivative can be expressed with the Levi-Civita
connection ∇c associated to gc:

0
λc
x

DX
∂t

= ∇c

˙γ(t)X.

Let’s compute the Levi-Civita connection from its Christoffel symbols. In a local coordinate system,
they can be written as

Γi

jk =

(gc)il(∂jgc

lk + ∂kgc

lj − ∂lgc

jk),

1
2

where superscripts denote the inverse metric tensor and using Einstein’s notations. As gc
at γ(t) ∈ Dn

c this yields:

ij = (λc)2δij,

jk = cλc
Γi

γ(t)(δikγ(t)j + δijγ(t)k − δjkγ(t)i).

9i.e. that DX

∂t = 0 for t ∈ [0, 1], where D

∂t denotes the covariant derivative.

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

14

On the other hand, since X(t) = (λc

∇c

˙γ(t)X = ˙γ(t)i∇c

i X = ˙γ(t)i∇c
i

= vj ˙γ(t)i∇c
i

0/λc

γ(t))v, we have
(cid:32)

(cid:33)

λc
0
λc

γ(t)

v

(cid:32)

λc
0
λc

γ(t)

(cid:33)

ej

.

√

√

Since γ(t) = (1/
Hence there exists K x

c) tanh(t tanh−1(
t ∈ R such that ˙γ(t) = K x

c(cid:107)x(cid:107))) x

(cid:107)x(cid:107) , it is easily seen that ˙γ(t) is colinear to γ(t).
t γ(t). Moreover, we have the following Leibniz rule:
(cid:33)

(cid:32)

(cid:32)

∇c
i

λc
0
λc

γ(t)

(cid:33)

ej

=

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

ej.

Combining these yields

DX
∂t

= K x

t vjγ(t)i

(cid:32)

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

(cid:32)

(cid:33)

(cid:33)

ej

.

Replacing with the Christoffel symbols of ∇c at γ(t) gives

Moreover,

λc
0
λc

γ(t)

λc
0
λc

γ(t)

∇c

i ej =

ijek = 2c[δk
Γk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek.

∂
∂γ(t)i

(cid:32)

(cid:33)

λc
0
λc

γ(t)

ej =

∂
∂γ(t)i

(cid:0)−c(cid:107)γ(t)(cid:107)2(cid:1) ej = −2cγ(t)iej.

Putting together everything, we obtain

DX
∂t

= K x

t vjγ(t)i (cid:0)2c[δk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek − 2cγ(t)iej

(cid:1)

t vjγ(t)i (cid:0)γ(t)jei − δijγ(t)kek
t vj (cid:0)γ(t)jγ(t)iei − γ(t)iδijγ(t)kek
(cid:1)
t vj (cid:0)γ(t)jγ(t)iei − γ(t)jγ(t)kek

(cid:1)

(cid:1)

= 2cK x
= 2cK x
= 2cK x
= 0,

which concludes the proof.

C Proof of Eq. (22)

Proof. Two steps proof:
i) expc

p({a}⊥) ⊆ {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}:

Let z ∈ {a}⊥. From Eq. (12), we have that:

This, together with the left-cancellation law in gyrospaces (see section 2.3), implies that

expc

p(z) = −p ⊕c βz,

for some β ∈ R.

(cid:104)−p ⊕c expc

p(z), a(cid:105) = (cid:104)βz, a(cid:105) = 0

which is what we wanted.

ii) {x ∈ Dn
Let x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0} ⊆ expc
c s.t. (cid:104)−p ⊕c x, a(cid:105) = 0. Then, using Eq. (12), we derive that:
for some β ∈ R,

p(x) = β(−p ⊕c x),

p({a}⊥):

logc

which is orthogonal to a, by assumption. This implies logc

p(x) ∈ {a}⊥, hence x ∈ expc

p({a}⊥).

15

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(54)

D Proof of Theorem 5

Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

1
√
c

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(55)

Proof. We ﬁrst need to prove the following lemma, trivial in the Euclidean space, but not in the
Poincaré ball:
Lemma 7. (Orthogonal projection on a geodesic) Any point in the Poincaré ball has a unique
orthogonal projection on any given geodesic that does not pass through the point. Formally, for all
y ∈ Dn
c and for all geodesics γx→z(·) s.t. y /∈ Im γx→z, there exists an unique w ∈ Im γx→z s.t.
∠(γw→y, γx→z) = π/2.

Proof. We ﬁrst note that any geodesic in Dn
and has two "points at inﬁnity" lying on the ball border (v (cid:54)= 0):

c has the form γ(t) = u ⊕c v ⊗c t as given by Eq. 11,

γ(±∞) = u ⊕c

√

±v
c(cid:107)v(cid:107)

∈ ∂Dn
c .

(56)

Using the notations in the lemma statement, the closed-form of γx→z is given by Eq. (10):

γx→z(t) = x ⊕c (−x ⊕c z) ⊗c t

We denote by x(cid:48), z(cid:48) ∈ ∂Dn
∠ywx(cid:48) is well deﬁned from Eq. (34):

c its points at inﬁnity as described by Eq. (56). Then, the hyperbolic angle

cos(∠(γw→y, γx→z)) = cos(∠ywz(cid:48)) =

(cid:104)−w ⊕c y, −w ⊕c z(cid:48)(cid:105)
(cid:107) − w ⊕c y(cid:107) · (cid:107) − w ⊕c z(cid:48)(cid:107)

.

(57)

We now perform 2 steps for this proof.

i) Existence of w:

The angle function from Eq. (57) is continuous w.r.t t when w = γx→z(t). So we ﬁrst prove existence
of an angle of π/2 by continuously moving w from x(cid:48) to z(cid:48) when t goes from −∞ to ∞, and
observing that cos(∠ywz(cid:48)) goes from −1 to 1 as follows:

cos(∠yx(cid:48)z(cid:48)) = 1 & lim
w→z(cid:48)

cos(∠ywz(cid:48)) = −1.

(58)

The left part of Eq. (58) follows from Eq. (57) and from the fact (easy to show from the deﬁnition
c (which is the case of x(cid:48)). The right part of Eq. (58)
of ⊕c) that a ⊕c b = a, when (cid:107)a(cid:107) = 1/
follows from the fact that ∠ywz(cid:48) = π − ∠ywx(cid:48) (from the conformal property, or from Eq. (34)) and
cos(∠yz(cid:48)x(cid:48)) = 1 (proved as above).
Hence cos(∠ywz(cid:48)) has to pass through 0 when going from −1 to 1, which achieves the proof of
existence.

√

ii) Uniqueness of w:
Assume by contradiction that there are two w and w(cid:48) on γx→z that form angles ∠ywx(cid:48) and ∠yw(cid:48)x(cid:48)
of π/2. Since w, w(cid:48), x(cid:48) are on the same geodesic, we have

π/2 = ∠yw(cid:48)x(cid:48) = ∠yw(cid:48)w = ∠ywx(cid:48) = ∠yw(cid:48)w
So ∆yww(cid:48) has two right angles, but in the Poincaré ball this is impossible.

(59)

Now, we need two more lemmas:
Lemma 8. (Minimizing distance from point to geodesic) The orthogonal projection of a point to
a geodesic (not passing through the point) is minimizing the distance between the point and the
geodesic.

Proof. The proof is similar with the Euclidean case and it’s based on hyperbolic sine law and the fact
that in any right hyperbolic triangle the hypotenuse is strictly longer than any of the other sides.

16

Lemma 9. (Geodesics through p) Let ˜H c
all points on the geodesic γp→w are included in ˜H c

a,p.

a,p be a Poincaré hyperplane. Then, for any w ∈ ˜H c

a,p \ {p},

Proof. γp→w(t) = p ⊕c (−p ⊕c w) ⊗c t. Then, it is easy to check the condition in Eq. (22):

(cid:104)−p ⊕c γp→w(t), a(cid:105) = (cid:104)(−p ⊕c w) ⊗c t, a(cid:105) ∝ (cid:104)−p ⊕c w, a(cid:105) = 0.

(60)

We now turn back to our proof. Let x ∈ Dn
We prove that there is at least one point w∗ ∈ ˜H c

c be an arbitrary point and ˜H c

a,p a Poincaré hyperplane.

a,p that achieves the inﬁmum distance

dc(x, w∗) = inf
w∈ ˜H c

a,p

dc(x, w),

and, moreover, that this distance is the same as the one in the theorem’s statement.
We ﬁrst note that for any point w ∈ ˜H c
and Lemma 9, it is obvious that the projection of x to γp→w will give a strictly lower distance.
Thus, we only consider w ∈ ˜H c
triangle ∆xwp, one gets:

a,p such that ∠xwp = π/2. Applying hyperbolic sine law in the right

a,p, if ∠xwp (cid:54)= π/2, then w (cid:54)= w∗. Indeed, using Lemma 8

dc(x, w) = (1/

c) sinh−1 (cid:0)sinh(

c dc(x, p)) · sin(∠xpw)(cid:1) .

√

√

One of the above quantities does not depend on w:

√

√

sinh(

c dc(x, p)) = sinh(2 tanh−1(

c(cid:107) − p ⊕c x(cid:107))) =

√
2
c(cid:107) − p ⊕c x(cid:107)
1 − c(cid:107) − p ⊕c x(cid:107)2 .

The other quantity is sin(∠xpw) which is minimized when the angle ∠xpw is minimized (be-
cause ∠xpw < π/2 for the hyperbolic right triangle ∆xwp), or, alternatively, when cos(∠xpw) is
maximized. But, we already have from Eq. (34) that:

cos(∠xpw) =

(cid:104)−p ⊕c x, −p ⊕c w(cid:105)
(cid:107) − p ⊕c x(cid:107) · (cid:107) − p ⊕c w(cid:107)

.

To maximize the above, the constraint on the right angle at w can be dropped because cos(∠xpw)
depends only on the geodesic γp→w and not on w itself, and because there is always an orthogonal
projection from any point x to any geodesic as stated by Lemma 7. Thus, it remains to ﬁnd the
maximum of Eq. (64) when w ∈ ˜H c
a,p from Eq. (22), one can easily
prove that

a,p. Using the deﬁnition of ˜H c

Using that fact that logc

p(w)/(cid:107) logc

p(w)(cid:107) = −p ⊕c w/(cid:107) − p ⊕c w(cid:107), we just have to ﬁnd

and we are left with a well known Euclidean problem which is equivalent to ﬁnding the minimum
angle between the vector −p ⊕c x (viewed as Euclidean) and the hyperplane {a}⊥. This angle
is given by the Euclidean orthogonal projection whose sin value is the distance from the vector’s
endpoint to the hyperplane divided by the vector’s length:

{logc

p(w) : w ∈ ˜H c

a,p} = {a}⊥.

max
z∈{a}⊥

(cid:18) (cid:104)−p ⊕c x, z(cid:105)

(cid:107) − p ⊕c x(cid:107) · (cid:107)z(cid:107)

(cid:19)

,

sin(∠xpw∗) =

|(cid:104)−p ⊕c x, a
(cid:107) − p ⊕c x(cid:107)

(cid:107)a(cid:107) (cid:105)|

.

17

It follows that a point w∗ ∈ ˜H c
Eqs. (61),(62),(63) and (67) concludes the proof.

a,p satisfying Eq. (67) exists (but might not be unique). Combining

(61)

(62)

(63)

(64)

(65)

(66)

(67)

(cid:3)

E Derivation of the Hyperbolic GRU Update-gate

In [24], the authors recover the update/forget-gate mechanism of a GRU/LSTM by requiring that the
class of neural networks given by the chosen architecture be invariant to time-warpings. The idea is
the following.

Recovering the update-gate from time-warping. A naive RNN is given by the equation

h(t + 1) = ϕ(W h(t) + U x(t) + b)

Let’s drop the bias b to simplify notations. If h is seen as a differentiable function of time, then a
ﬁrst-order Taylor development gives h(t + δt) ≈ h(t) + δt dh
dt (t) for small δt. Combining this for
δt = 1 with the naive RNN equation, one gets

dh
dt

dα
dt

(t) = ϕ(W h(t) + U x(t)) − h(t).

As this is written for any t, one can replace it by t ← α(t) where α is a (smooth) increasing function
of t called the time-warping. Denoting by ˜h(t) := h(α(t)) and ˜x(t) := x(α(t)), using the chain rule
d˜h
dt (t) = dα

dt (α(t)), one gets

dt (t) dh

d˜h
dt

dα
dt

(t) =

(t)ϕ(W ˜h(t) + U ˜x(t)) −

(t)˜h(t).

(70)

Removing the tildas to simplify notations, discretizing back with dh

dt (t) ≈ h(t + 1) − h(t) yields

h(t + 1) =

(t)ϕ(W h(t) + U x(t)) +

1 −

(t)

h(t).

(71)

dα
dt

(cid:18)

(cid:19)

dα
dt

Requiring that our class of neural networks be invariant to time-warpings means that this class should
contain RNNs deﬁned by Eq. (71), i.e. that dα
dt (t) can be learned. As this is a positive quantity, we
can parametrize it as z(t) = σ(W zh(t) + U zx(t)), recovering the forget-gate equation:

h(t + 1) = z(t)ϕ(W h(t) + U x(t)) + (1 − z(t))h(t).

Adapting this idea to hyperbolic RNNs. The gyroderivative [4] of a map h : R → Dn
as

c is deﬁned

dh
dt

(t) = lim
δt→0

1
δt

⊗c (−h(t) ⊕c h(t + δt)).

Using Möbius scalar associativity and the left-cancellation law leads us to

h(t + δt) ≈ h(t) ⊕c δt ⊗c

(t),

dh
dt

for small δt. Combining this with the equation of a simple hyperbolic RNN of Eq. (29) with δt = 1,
one gets

dh
dt

(t) = −h(t) ⊕c ϕ⊗c(W ⊗c h(t) ⊕c U ⊗c x(t)).

For the next step, we need the following lemma:
Lemma 10 (Gyro-chain-rule). For α : R → R differentiable and h : R → Dn
gyro-derivative, if ˜h := h ◦ α, then we have

c with a well-deﬁned

(68)

(69)

(72)

(73)

(74)

(75)

(76)

where dα

dt (t) denotes the usual derivative.

d˜h
dt

(t) =

(t) ⊗c

(α(t)),

dα
dt

dh
dt

18

(77)

(78)

(79)

(80)

(81)

Proof.

d˜h
dt

(t) = lim
δt→0

1
δt
1
δt

⊗c [−˜h(t) ⊕c

˜h(t + δt)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

= lim
δt→0

= lim
δt→0

= lim
δt→0

= lim
u→0
dα
dt

=

α(cid:48)(t) + O(δt)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
u

(t) ⊗c

(α(t))

dh
dt

⊗c [−h(α(t)) ⊕c h(α(t) + u)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

(Möbius scalar associativity) (82)

where we set u = δt(α(cid:48)(t) + O(δt)), with u → 0 when δt → 0, which concludes.

Using lemma 10 and Eq. (75), with similar notations as in Eq. (70) we have

d˜h
dt

dα
dt

(t) =

(t) ⊗c (−˜h(t) ⊕c ϕ⊗c(W ⊗c

˜h(t) ⊕c U ⊗c ˜x(t))).

(83)

Finally, discretizing back with Eq. (74), using the left-cancellation law and dropping the tildas yields

h(t + 1) = h(t) ⊕c

(t) ⊗c (−h(t) ⊕c ϕ⊗c (W ⊗c h(t) ⊕c U ⊗c x(t))).

(84)

dα
dt

Since α is a time-warping, by deﬁnition its derivative is positive and one can choose to parametrize
it with an update-gate zt (a scalar) deﬁned with a sigmoid. Generalizing this scalar scaling by the
Möbius version of the pointwise scaling (cid:12) yields the Möbius matrix scaling diag(zt) ⊗c ·, leading to
our proposed Eq. (33) for the hyperbolic GRU.

F More Experimental Investigations

The following empirical facts were observed for both hyperbolic RNNs and GRUs.

We observed that, in the hyperbolic setting, accuracy is often much higher when sentence embeddings
can go close to the border (hyperbolic "inﬁnity"), hence exploiting the hyperbolic nature of the space.
Moreover, the faster the two sentence norms go to 1, the more it’s likely that a good local minima
was reached. See ﬁgures 3 and 5.

We often observe that test accuracy starts increasing exactly when sentence embedding norms do.
However, in the hyperbolic setting, the sentence embeddings norms remain close to 0 for a few
epochs, which does not happen in the Euclidean case. See ﬁgures 3, 5 and 4. This mysterious fact
was also exhibited in a similar way by [21] which suggests that the model ﬁrst has to adjust the
angular layout in the almost Euclidean vicinity of 0 before increasing norms and fully exploiting
hyperbolic geometry.

19

(a) Test accuracy

(a) Test accuracy

20

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 3: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the same
architecture: hyperbolic GRU followed by hyperbolic FFNN and hyperbolic/Euclidean (half-half)
MLR. The X axis shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 4: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: Euclidean GRU followed by Euclidean FFNN and Euclidean MLR. The X axis
shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 5: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: hyperbolic RNN followed by hyperbolic FFNN and hyperbolic MLR. The X axis
shows millions of training examples processed.

(a) Test accuracy

21

8
1
0
2
 
n
u
J
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
2
1
1
9
0
.
5
0
8
1
:
v
i
X
r
a

Hyperbolic Neural Networks

Octavian-Eugen Ganea∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
octavian.ganea@inf.ethz.ch

Gary Bécigneul∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
gary.becigneul@inf.ethz.ch

Thomas Hofmann
Department of Computer Science
ETH Zürich
Zurich, Switzerland
thomas.hofmann@inf.ethz.ch

Abstract

Hyperbolic spaces have recently gained momentum in the context of machine
learning due to their high capacity and tree-likeliness properties. However, the
representational power of hyperbolic geometry is not yet on par with Euclidean
geometry, mostly because of the absence of corresponding hyperbolic neural
network layers. This makes it hard to use hyperbolic embeddings in downstream
tasks. Here, we bridge this gap in a principled manner by combining the formalism
of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model
of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep
learning tools: multinomial logistic regression, feed-forward and recurrent neural
networks such as gated recurrent units. This allows to embed sequential data and
perform classiﬁcation in the hyperbolic space. Empirically, we show that, even if
hyperbolic optimization tools are limited, hyperbolic sentence embeddings either
outperform or are on par with their Euclidean variants on textual entailment and
noisy-preﬁx recognition tasks.

1

Introduction

It is common in machine learning to represent data as being embedded in the Euclidean space Rn. The
main reason for such a choice is simply convenience, as this space has a vectorial structure, closed-
form formulas of distance and inner-product, and is the natural generalization of our intuition-friendly,
visual three-dimensional space. Moreover, embedding entities in such a continuous space allows to
feed them as input to neural networks, which has led to unprecedented performance on a broad range
of problems, including sentiment detection [15], machine translation [3], textual entailment [22] or
knowledge base link prediction [20, 6].

Despite the success of Euclidean embeddings, recent research has proven that many types of com-
plex data (e.g. graph data) from a multitude of ﬁelds (e.g. Biology, Network Science, Computer
Graphics or Computer Vision) exhibit a highly non-Euclidean latent anatomy [8]. In such cases, the
Euclidean space does not provide the most powerful or meaningful geometrical representations. For
example, [10] shows that arbitrary tree structures cannot be embedded with arbitrary low distortion
(i.e. almost preserving their metric) in the Euclidean space with unbounded number of dimensions,
but this task becomes surprisingly easy in the hyperbolic space with only 2 dimensions where the
exponential growth of distances matches the exponential growth of nodes with the tree depth.

∗Equal contribution.

The adoption of neural networks and deep learning in these non-Euclidean settings has been rather
limited until very recently, the main reason being the non-trivial or impossible principled general-
izations of basic operations (e.g. vector addition, matrix-vector multiplication, vector translation,
vector inner product) as well as, in more complex geometries, the lack of closed form expressions for
basic objects (e.g. distances, geodesics, parallel transport). Thus, classic tools such as multinomial
logistic regression (MLR), feed forward (FFNN) or recurrent neural networks (RNN) did not have a
correspondence in these geometries.

How should one generalize deep neural models to non-Euclidean domains ? In this paper we address
this question for one of the simplest, yet useful, non-Euclidean domains: spaces of constant negative
curvature, i.e. hyperbolic. Their tree-likeness properties have been extensively studied [12, 13, 26]
and used to visualize large taxonomies [18] or to embed heterogeneous complex networks [17]. In
machine learning, recently, hyperbolic representations greatly outperformed Euclidean embeddings
for hierarchical, taxonomic or entailment data [21, 10, 11]. Disjoint subtrees from the latent hierar-
chical structure surprisingly disentangle and cluster in the embedding space as a simple reﬂection of
the space’s negative curvature. However, appropriate deep learning tools are needed to embed feature
data in this space and use it in downstream tasks. For example, implicitly hierarchical sequence data
(e.g. textual entailment data, phylogenetic trees of DNA sequences or hierarchial captions of images)
would beneﬁt from suitable hyperbolic RNNs.

The main contribution of this paper is to bridge the gap between hyperbolic and Euclidean geometry
in the context of neural networks and deep learning by generalizing in a principled manner both the
basic operations as well as multinomial logistic regression (MLR), feed-forward (FFNN), simple and
gated (GRU) recurrent neural networks (RNN) to the Poincaré model of the hyperbolic geometry.
We do it by connecting the theory of gyrovector spaces and generalized Möbius transformations
introduced by [2, 26] with the Riemannian geometry properties of the manifold. We smoothly
parametrize basic operations and objects in all spaces of constant negative curvature using a uniﬁed
framework that depends only on the curvature value. Thus, we show how Euclidean and hyperbolic
spaces can be continuously deformed into each other. On a series of experiments and datasets we
showcase the effectiveness of our hyperbolic neural network layers compared to their "classic"
Euclidean variants on textual entailment and noisy-preﬁx recognition tasks. We hope that this paper
will open exciting future directions in the nascent ﬁeld of Geometric Deep Learning.

2 The Geometry of the Poincaré Ball

2.1 Basics of Riemannian geometry

We brieﬂy introduce basic concepts of differential geometry largely needed for a principled general-
ization of Euclidean neural networks. For more rigorous and in-depth expositions, see [23, 14].
An n-dimensional manifold M is a space that can locally be approximated by Rn: it is a generalization
to higher dimensions of the notion of a 2D surface. For x ∈ M, one can deﬁne the tangent space
TxM of M at x as the ﬁrst order linear approximation of M around x. A Riemannian metric
g = (gx)x∈M on M is a collection of inner-products gx : TxM × TxM → R varying smoothly
with x. A Riemannian manifold (M, g) is a manifold M equipped with a Riemannian metric g.
Although a choice of a Riemannian metric g on M only seems to deﬁne the geometry locally on M,
it induces global distances by integrating the length (of the speed vector living in the tangent space)
of a shortest path between two points:

(cid:90) 1

(cid:113)

d(x, y) = inf
γ

0

gγ(t)( ˙γ(t), ˙γ(t))dt,

(1)

where γ ∈ C∞([0, 1], M) is such that γ(0) = x and γ(1) = y. A smooth path γ of minimal length
between two points x and y is called a geodesic, and can be seen as the generalization of a straight-line
in Euclidean space. The parallel transport Px→y : TxM → TyM is a linear isometry between
tangent spaces which corresponds to moving tangent vectors along geodesics and deﬁnes a canonical
way to connect tangent spaces. The exponential map expx at x, when well-deﬁned, gives a way to
project back a vector v of the tangent space TxM at x, to a point expx(v) ∈ M on the manifold.
This map is often used to parametrize a geodesic γ starting from γ(0) := x ∈ M with unit-norm
direction ˙γ(0) := v ∈ TxM as t (cid:55)→ expx(tv). For geodesically complete manifolds, such as the
Poincaré ball considered in this work, expx is well-deﬁned on the full tangent space TxM. Finally, a

2

(2)

(3)

(4)

(5)

metric ˜g is said to be conformal to another metric g if it deﬁnes the same angles, i.e.

˜gx(u, v)
(cid:112)˜gx(u, u)(cid:112)˜gx(v, v)

=

gx(u, v)
(cid:112)gx(u, u)(cid:112)gx(v, v)

,

for all x ∈ M, u, v ∈ TxM \ {0}. This is equivalent to the existence of a smooth function
λ : M → R, called the conformal factor, such that ˜gx = λ2

xgx for all x ∈ M.

2.2 Hyperbolic space: the Poincaré ball

The hyperbolic space has ﬁve isometric models that one can work with [9]. Similarly as in [21] and
[11], we choose to work in the Poincaré ball. The Poincaré ball model (Dn, gD) is deﬁned by the
manifold Dn = {x ∈ Rn : (cid:107)x(cid:107) < 1} equipped with the following Riemannian metric:

gD
x = λ2

xgE, where λx :=

2
1 − (cid:107)x(cid:107)2 ,

gE = In being the Euclidean metric tensor. Note that the hyperbolic metric tensor is conformal to
the Euclidean one. The induced distance between two points x, y ∈ Dn is known to be given by

dD(x, y) = cosh−1

1 + 2

(cid:18)

(cid:107)x − y(cid:107)2
(1 − (cid:107)x(cid:107)2)(1 − (cid:107)y(cid:107)2)

(cid:19)

.

Since the Poincaré ball is conformal to Euclidean space, the angle between two vectors u, v ∈
TxDn \ {0} is given by

cos(∠(u, v)) =

gD
x (u, v)
x (u, u)(cid:112)gD

(cid:112)gD

x (v, v)

=

(cid:104)u, v(cid:105)
(cid:107)u(cid:107)(cid:107)v(cid:107)

.

2.3 Gyrovector spaces

In Euclidean space, natural operations inherited from the vectorial structure, such as vector addition,
subtraction and scalar multiplication are often useful. The framework of gyrovector spaces provides
an elegant non-associative algebraic formalism for hyperbolic geometry just as vector spaces provide
the algebraic setting for Euclidean geometry [2, 25, 26].

In particular, these operations are used in special relativity, allowing to add speed vectors belonging
to the Poincaré ball of radius c (the celerity, i.e. the speed of light) so that they remain in the ball,
hence not exceeding the speed of light.

We will make extensive use of these operations in our deﬁnitions of hyperbolic neural networks.
For c ≥ 0, denote2 by Dn
then Dn

c := {x ∈ Rn | c(cid:107)x(cid:107)2 < 1}. Note that if c = 0, then Dn
c. If c = 1 then we recover the usual ball Dn.

c is the open ball of radius 1/

c = Rn; if c > 0,

√

Möbius addition. The Möbius addition of x and y in Dn

c is deﬁned as

x ⊕c y :=

(1 + 2c(cid:104)x, y(cid:105) + c(cid:107)y(cid:107)2)x + (1 − c(cid:107)x(cid:107)2)y
1 + 2c(cid:104)x, y(cid:105) + c2(cid:107)x(cid:107)2(cid:107)y(cid:107)2

.

(6)

In particular, when c = 0, one recovers the Euclidean addition of two vectors in Rn. Note that
without loss of generality, the case c > 0 can be reduced to c = 1. Unless stated otherwise, we
will use ⊕ as ⊕1 to simplify notations. For general c > 0, this operation is not commutative nor
associative. However, it satisﬁes x ⊕c 0 = 0 ⊕c x = 0. Moreover, for any x, y ∈ Dn
c , we have
(−x) ⊕c x = x ⊕c (−x) = 0 and (−x) ⊕c (x ⊕c y) = y (left-cancellation law). The Möbius
substraction is then deﬁned by the use of the following notation: x (cid:9)c y := x ⊕c (−y). See [29,
section 2.1] for a geometric interpretation of the Möbius addition.

2We take different notations as in [25] where the author uses s = 1/

c.

√

3

Möbius scalar multiplication. For c > 0, the Möbius scalar multiplication of x ∈ Dn
r ∈ R is deﬁned as

c \ {0} by

r ⊗c x := (1/

c) tanh(r tanh−1(

c(cid:107)x(cid:107)))

√

√

x
(cid:107)x(cid:107)

,

(7)

and r ⊗c 0 := 0. Note that similarly as for the Möbius addition, one recovers the Euclidean scalar
multiplication when c goes to zero: limc→0 r ⊗c x = rx. This operation satisﬁes desirable properties
such as n ⊗c x = x ⊕c · · · ⊕c x (n additions), (r + r(cid:48)) ⊗c x = r ⊗c x ⊕c r(cid:48) ⊗c x (scalar distributivity3),
(rr(cid:48)) ⊗c x = r ⊗c (r(cid:48) ⊗c x) (scalar associativity) and |r| ⊗c x/(cid:107)r ⊗c x(cid:107) = x/(cid:107)x(cid:107) (scaling property).

c , gc) is given by4

Distance.
Euclidean one, with conformal factor λc
(Dn

If one deﬁnes the generalized hyperbolic metric tensor gc as the metric conformal to the
x := 2/(1 − c(cid:107)x(cid:107)2), then the induced distance function on
√

c) tanh−1 (cid:0)√
Again, observe that limc→0 dc(x, y) = 2(cid:107)x − y(cid:107), i.e. we recover Euclidean geometry in the limit5.
Moreover, for c = 1 we recover dD of Eq. (4).

c(cid:107) − x ⊕c y(cid:107)(cid:1) .

dc(x, y) = (2/

(8)

Hyperbolic trigonometry. Similarly as in the Euclidean space, one can deﬁne the notions of
hyperbolic angles or gyroangles (when using the ⊕c), as well as hyperbolic law of sines in the
generalized Poincaré ball (Dn

c , gc). We make use of these notions in our proofs. See Appendix A.

2.4 Connecting Gyrovector spaces and Riemannian geometry of the Poincaré ball

In this subsection, we present how geodesics in the Poincaré ball model are usually described with
Möbius operations, and push one step further the existing connection between gyrovector spaces and
the Poincaré ball by ﬁnding new identities involving the exponential map, and parallel transport.

In particular, these ﬁndings provide us with a simpler formulation of Möbius scalar multiplication,
yielding a natural deﬁnition of matrix-vector multiplication in the Poincaré ball.

Riemannian gyroline element. The Riemannian gyroline element is deﬁned for an inﬁnitesimal
dx as ds := (x + dx) (cid:9)c x, and its size is given by [26, section 3.7]:

(cid:107)ds(cid:107) = (cid:107)(x + dx) (cid:9)c x(cid:107) = (cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2).

(9)

What is remarkable is that it turns out to be identical, up to a scaling factor of 2, to the usual line
element 2(cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2) of the Riemannian manifold (Dn

c , gc).

Geodesics. The geodesic connecting points x, y ∈ Dn

c is shown in [2, 26] to be given by:

γx→y(t) := x ⊕c (−x ⊕c y) ⊗c t, with γx→y : R → Dn

c s.t. γx→y(0) = x and γx→y(1) = y.

Note that when c goes to 0, geodesics become straight-lines, recovering Euclidean geometry. In the
remainder of this subsection, we connect the gyrospace framework with Riemannian geometry.
Lemma 1. For any x ∈ Dn and v ∈ TxDn
c s.t. gc
x with direction v is given by:

x(v, v) = 1, the unit-speed geodesic starting from

γx,v(t) = x ⊕c

tanh

(cid:18)

(cid:18)√

(cid:19) v
√

c

t
2

c(cid:107)v(cid:107)

(cid:19)

, where γx,v : R → Dn s.t. γx,v(0) = x and ˙γx,v(0) = v.

(10)

(11)

Proof. One can use Eq. (10) and reparametrize it to unit-speed using Eq. (8). Alternatively, direct
computation and identiﬁcation with the formula in [11, Thm. 1] would give the same result. Using
Eq. (8) and Eq. (11), one can sanity-check that dc(γ(0), γ(t)) = t, ∀t ∈ [0, 1].

3⊗c has priority over ⊕c in the sense that a ⊗c b ⊕c c := (a ⊗c b) ⊕c c and a ⊕c b ⊗c c := a ⊕c (b ⊗c c).
4The notation −x ⊕c y should always be read as (−x) ⊕c y and not −(x ⊕c y).
5The factor 2 comes from the conformal factor λx = 2/(1 − (cid:107)x(cid:107)2), which is a convention setting the

curvature to −1.

4

Exponential and logarithmic maps. The following lemma gives the closed-form derivation of
exponential and logarithmic maps.
Lemma 2. For any point x ∈ Dn
map logc
c → TxDn
(cid:18)

c are given for v (cid:54)= 0 and y (cid:54)= x by:
(cid:18)√

c , the exponential map expc

c and the logarithmic

x : TxDn

c → Dn

x : Dn

√

(cid:19)

, logc

x(y) =

√

tanh−1(

c(cid:107) − x ⊕c y(cid:107))

expc

x(v) = x ⊕c

tanh

c

λc
x(cid:107)v(cid:107)
2

(cid:19) v
√

c(cid:107)v(cid:107)

2
cλc
x

−x ⊕c y
(cid:107) − x ⊕c y(cid:107)
(12)

.

Proof. Following the proof of [11, Cor. 1.1], one gets expc
gives the formula for expc

x. Algebraic check of the identity logc

x(v) = γx,
x(expc

v

x(cid:107)v(cid:107) (λc
λc

x(cid:107)v(cid:107)). Using Eq. (11)

x(v)) = v concludes.

The above maps have more appealing forms when x = 0, namely for v ∈ T0Dn

c \ {0}, y ∈ Dn

c \ {0}:

expc

0(v) = tanh(

c(cid:107)v(cid:107))

√

, logc

0(y) = tanh−1(

c(cid:107)y(cid:107))

√

(13)

√

y
c(cid:107)y(cid:107)

.

√

v
c(cid:107)v(cid:107)

Moreover, we still recover Euclidean geometry in the limit c → 0, as limc→0 expc
Euclidean exponential map, and limc→0 logc

x(y) = y − x is the Euclidean logarithmic map.

x(v) = x + v is the

Möbius scalar multiplication using exponential and logarithmic maps. We studied the expo-
nential and logarithmic maps in order to gain a better understanding of the Möbius scalar multiplica-
tion (Eq. (7)). We found the following:
Lemma 3. The quantity r ⊗ x can actually be obtained by projecting x in the tangent space at 0
with the logarithmic map, multiplying this projection by the scalar r in T0Dn
c , and then projecting it
back on the manifold with the exponential map:
0(r logc

∀r ∈ R, x ∈ Dn
c .

r ⊗c x = expc

0(x)),

(14)

In addition, we recover the well-known relation between geodesics connecting two points and the
exponential map:

γx→y(t) = x ⊕c (−x ⊕c y) ⊗c t = expc

x(t logc

x(y)),

t ∈ [0, 1].

(15)

This last result enables us to generalize scalar multiplication in order to deﬁne matrix-vector multipli-
cation between Poincaré balls, one of the essential building blocks of hyperbolic neural networks.

Parallel transport. Finally, we connect parallel transport (from T0Dn
the following theorem, which we prove in appendix B.
Theorem 4. In the manifold (Dn
vector v ∈ T0Dn

c to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a
c is given by the following isometry:
λc
0
λc
x

x(x ⊕c expc

0(v)) =

0→x(v) = logc
P c

(16)

v.

c ) to gyrovector spaces with

As we’ll see later, this result is crucial in order to deﬁne and optimize parameters shared between
different tangent spaces, such as biases in hyperbolic neural layers or parameters of hyperbolic MLR.

3 Hyperbolic Neural Networks

Neural networks can be seen as being made of compositions of basic operations, such as linear
maps, bias translations, pointwise non-linearities and a ﬁnal sigmoid or softmax layer. We ﬁrst
explain how to construct a softmax layer for logits lying in a Poincaré ball. Then, we explain how
to transform a mapping between two Euclidean spaces as one between Poincaré balls, yielding
matrix-vector multiplication and pointwise non-linearities in the Poincaré ball. Finally, we present
possible adaptations of various recurrent neural networks to the hyperbolic domain.

5

3.1 Hyperbolic multiclass logistic regression

In order to perform multi-class classiﬁcation on the Poincaré ball, one needs to generalize multinomial
logistic regression (MLR) − also called softmax regression − to the Poincaré ball.

Reformulating Euclidean MLR. Let’s ﬁrst reformulate Euclidean MLR from the perspective of
distances to margin hyperplanes, as in [19, Section 5]. This will allow us to easily generalize it.

Given K classes, one learns a margin hyperplane for each such class using softmax probabilities:

∀k ∈ {1, ..., K},

p(y = k|x) ∝ exp (((cid:104)ak, x(cid:105) − bk)) , where bk ∈ R, x, ak ∈ Rn.

(17)

Note that any afﬁne hyperplane in Rn can be written with a normal vector a and a scalar shift b:

Ha,b = {x ∈ Rn : (cid:104)a, x(cid:105) − b = 0}, where a ∈ Rn \ {0}, and b ∈ R.

(18)

As in [19, Section 5], we note that (cid:104)a, x(cid:105) − b = sign((cid:104)a, x(cid:105) − b)(cid:107)a(cid:107)d(x, Ha,b). Using Eq. (17):

p(y = k|x) ∝ exp(sign((cid:104)ak, x(cid:105) − bk)(cid:107)ak(cid:107)d(x, Hak,bk )), bk ∈ R, x, ak ∈ Rn.

(19)

As it is not immediately obvious how to generalize the Euclidean hyperplane of Eq. (18) to other
spaces such as the Poincaré ball, we reformulate it as follows:

˜Ha,p = {x ∈ Rn : (cid:104)−p + x, a(cid:105) = 0} = p + {a}⊥, where p ∈ Rn, a ∈ Rn \ {0}.

(20)

This new deﬁnition relates to the previous one as ˜Ha,p = Ha,(cid:104)a,p(cid:105). Rewriting Eq. (19) with b = (cid:104)a, p(cid:105):
p(y = k|x) ∝ exp(sign((cid:104)−pk + x, ak(cid:105))(cid:107)ak(cid:107)d(x, ˜Hak,pk )), with pk, x, ak ∈ Rn.

(21)

It is now natural to adapt the previous deﬁnition to the hyperbolic setting by replacing + by ⊕c:
Deﬁnition 3.1 (Poincaré hyperplanes). For p ∈ Dn
p(z, a) = 0} = {z ∈ TpDn
gc

c : (cid:104)z, a(cid:105) = 0}. Then, we deﬁne Poincaré hyperplanes as

c \ {0}, let {a}⊥ := {z ∈ TpDn
c :

c , a ∈ TpDn

˜H c

a,p := {x ∈ Dn

c : (cid:104)logc

p(x), a(cid:105)p = 0} = expc

p({a}⊥) = {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}.

(22)

The last equality is shown appendix C. ˜H c
all geodesics in Dn
hypergyroplanes, see [27, deﬁnition 5.8]. A 3D hyperplane example is depicted in Fig. 1.

a,p can also be described as the union of images of
c orthogonal to a and containing p. Notice that our deﬁnition matches that of

Next, we need the following theorem, proved in appendix D:
Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(23)

Final formula for MLR in the Poincaré ball. Putting together Eq. (21) and Thm. 5, we get the
hyperbolic MLR formulation. Given K classes and k ∈ {1, . . . , K}, pk ∈ Dn
c \ {0}:

c , ak ∈ Tpk

Dn

p(y = k|x) ∝ exp(sign((cid:104)−pk ⊕c x, ak(cid:105))

gc
pk

(ak, ak)dc(x, ˜H c

)),

ak,pk

∀x ∈ Dn
c ,

(24)

or, equivalently

p(y = k|x) ∝ exp

(cid:18) λc
pk

(cid:107)ak(cid:107)
√
c

sinh−1

(cid:18)

√
2

c(cid:104)−pk ⊕c x, ak(cid:105)

(cid:19)(cid:19)

(1 − c(cid:107) − pk ⊕c x(cid:107)2)(cid:107)ak(cid:107)

,

∀x ∈ Dn
c .

(25)

this goes to p(y = k|x) ∝ exp(4(cid:104)−pk + x, ak(cid:105)) =

Notice that when c goes to zero,
exp((λ0
pk

)2(cid:104)−pk + x, ak(cid:105)) = exp((cid:104)−pk + x, ak(cid:105)0), recovering the usual Euclidean softmax.
However, at this point it is unclear how to perform optimization over ak, since it lives in Tpk
hence depends on pk. The solution is that one should write ak = P c
)a(cid:48)
k ∈ T0Dn
a(cid:48)

c = Rn, and optimize a(cid:48)

k as a Euclidean parameter.

k) = (λc

0/λc
pk

0→pk

(a(cid:48)

Dn
c and
k, where

1
√
c

(cid:113)

6

3.2 Hyperbolic feed-forward layers

In order to deﬁne hyperbolic neural networks, it is crucial to de-
ﬁne a canonically simple parametric family of transformations,
playing the role of linear mappings in usual Euclidean neural
networks, and to know how to apply pointwise non-linearities.
Inspiring ourselves from our reformulation of Möbius scalar
multiplication in Eq. (14), we deﬁne:
Deﬁnition 3.2 (Möbius version). For f : Rn → Rm, we deﬁne
the Möbius version of f as the map from Dn

c to Dm

c by:

f ⊗c(x) := expc

0(f (logc

0(x))),

(26)

where expc

0 : T0m

Dm

c → Dm

c and logc

0 : Dn

c → T0n

Dn
c .

Figure 1: An example of a hyper-
bolic hyperplane in D3
1 plotted us-
ing sampling. The red point is p.
The shown normal axis to the hy-
perplane through p is parallel to a.

Note that similarly as for other Möbius operations, we recover
the Euclidean mapping in the limit c → 0 if f is continuous, as limc→0 f ⊗c(x) = f (x). This
deﬁnition satisﬁes a few desirable properties too, such as: (f ◦ g)⊗c = f ⊗c ◦ g⊗c for f : Rm → Rl
and g : Rn → Rm (morphism property), and f ⊗c(x)/(cid:107)f ⊗c(x)(cid:107) = f (x)/(cid:107)f (x)(cid:107) for f (x) (cid:54)= 0
(direction preserving). It is then straight-forward to prove the following result:
Lemma 6 (Möbius matrix-vector multiplication). If M : Rn → Rm is a linear map, which we
identify with its matrix representation, then ∀x ∈ Dn

c , if M x (cid:54)= 0 we have

M ⊗c(x) = (1/

c) tanh

√

(cid:18) (cid:107)M x(cid:107)
(cid:107)x(cid:107)

√

tanh−1(

c(cid:107)x(cid:107))

(cid:19) M x
(cid:107)M x(cid:107)

,

(27)

and M ⊗c(x) = 0 if M x = 0. Moreover, if we deﬁne the Möbius matrix-vector multiplication of
M ∈ Mm,n(R) and x ∈ Dn
c by M ⊗c x := M ⊗c(x), then we have (M M (cid:48)) ⊗c x = M ⊗c (M (cid:48) ⊗c x)
for M ∈ Ml,m(R) and M (cid:48) ∈ Mm,n(R) (matrix associativity), (rM ) ⊗c x = r ⊗c (M ⊗c x) for
r ∈ R and M ∈ Mm,n(R) (scalar-matrix associativity) and M ⊗c x = M x for all M ∈ On(R)
(rotations are preserved).

Pointwise non-linearity.
ϕ⊗c can be applied to elements of the Poincaré ball.

If ϕ : Rn → Rn is a pointwise non-linearity, then its Möbius version

Bias translation. The generalization of a translation in the Poincaré ball is naturally given by
moving along geodesics. But should we use the Möbius sum x ⊕c b with a hyperbolic bias b or the
x(b(cid:48)) with a Euclidean bias b(cid:48)? These views are uniﬁed with parallel transport
exponential map expc
c by a bias b ∈ Dn
(see Thm 4). Möbius translation of a point x ∈ Dn
(cid:18) λc
0
λc
x

c is given by
(cid:19)

x ← x ⊕c b = expc

0(b))) = expc
x

0→x(logc

x(P c

logc

0(b)

(28)

.

We recover Euclidean translations in the limit c → 0. Note that bias translations play a particular
Indeed, consider multiple layers of the form fk(x) = ϕk(Mkx), each of
role in this model.
which having Möbius version f ⊗c
k (Mk ⊗c x). Then their composition can be re-written
f ⊗c
k ◦ · · · ◦ f ⊗c
1 = expc
0. This means that these operations can essentially be
performed in Euclidean space. Therefore, it is the interposition between those with the bias translation
of Eq. (28) which differentiates this model from its Euclidean counterpart.

k (x) = ϕ⊗c
0 ◦fk ◦ · · · ◦ f1 ◦ logc

If a vector x ∈ Rn+p is the (vertical) concatenation
Concatenation of multiple input vectors.
of two vectors x1 ∈ Rn, x2 ∈ Rp, and M ∈ Mm,n+p(R) can be written as the (horizontal)
concatenation of two matrices M1 ∈ Mm,n(R) and M2 ∈ Mm,p(R), then M x = M1x1 + M2x2.
We generalize this to hyperbolic spaces: if we are given x1 ∈ Dn
c ×Dp
c ,
and M, M1, M2 as before, then we deﬁne M ⊗c x := M1 ⊗c x1 ⊕c M2 ⊗c x2. Note that when c goes
to zero, we recover the Euclidean formulation, as limc→0 M ⊗c x = limc→0 M1 ⊗c x1 ⊕c M2 ⊗c x2 =
M1x1 + M2x2 = M x. Moreover, hyperbolic vectors x ∈ Dn
c can also be "concatenated" with real
features y ∈ R by doing: M ⊗c x ⊕c y ⊗c b with learnable b ∈ Dm

c , x = (x1 x2)T ∈ Dn

c and M ∈ Mm,n(R).

c , x2 ∈ Dp

7

3.3 Hyperbolic RNN

Naive RNN. A simple RNN can be deﬁned by ht+1 = ϕ(W ht + U xt + b) where ϕ is a pointwise
non-linearity, typically tanh, sigmoid, ReLU, etc. This formula can be naturally generalized to the
hyperbolic space as follows. For parameters W ∈ Mm,n(R), U ∈ Mm,d(R), b ∈ Dm
c , we deﬁne:

ht+1 = ϕ⊗c (W ⊗c ht ⊕c U ⊗c xt ⊕c b),

ht ∈ Dn

c , xt ∈ Dd
c .

(29)

Note that if inputs xt’s are Euclidean, one can write ˜xt := expc
expc

(U xt)) = W ⊗c ht ⊕c expc

(P c

W ⊗cht

0→W ⊗cht

0(U xt) = W ⊗c ht ⊕c U ⊗c ˜xt.

0(xt) and use the above formula, since

GRU architecture. One can also adapt the GRU architecture:
rt = σ(W rht−1 + U rxt + br),
zt = σ(W zht−1 + U zxt + bz),
˜ht = ϕ(W (rt (cid:12) ht−1) + U xt + b), ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht,

(30)

where (cid:12) denotes pointwise product. First, how should we adapt the pointwise multiplication by a
scaling gate? Note that the deﬁnition of the Möbius version (see Eq. (26)) can be naturally extended
to maps f : Rn × Rp → Rm as f ⊗c : (h, h(cid:48)) ∈ Dn
0(h(cid:48)))). In
c (cid:55)→ expc
0(h(cid:48))) =
particular, choosing f (h, h(cid:48)) := σ(h) (cid:12) h(cid:48) yields6 f ⊗c(h, h(cid:48)) = expc
diag(σ(logc

0(h))) ⊗c h(cid:48). Hence we adapt rt (cid:12) ht−1 to diag(rt) ⊗c ht−1 and the reset gate rt to:
0(W r ⊗c ht−1 ⊕c U r ⊗c xt ⊕c br),

0(h), logc
0(h)) (cid:12) logc

0(f (logc
0(σ(logc

rt = σ logc

c × Dp

(31)

and similarly for the update gate zt. Note that as the argument of σ in the above is unbounded, rt and
zt can a priori take values onto the full range (0, 1). Now the intermediate hidden state becomes:
˜ht = ϕ⊗c ((W diag(rt)) ⊗c ht−1 ⊕c U ⊗c xt ⊕ b),

(32)

where Möbius matrix associativity simpliﬁes W ⊗c (diag(rt) ⊗c ht−1) into (W diag(rt)) ⊗c ht−1.
Finally, we propose to adapt the update-gate equation as

ht = ht−1 ⊕c diag(zt) ⊗c (−ht−1 ⊕c

˜ht).

(33)

Note that when c goes to zero, one recovers the usual GRU. Moreover, if zt = 0 or zt = 1, then ht
becomes ht−1 or ˜ht respectively, similarly as in the usual GRU. This adaptation was obtained by
adapting [24]: in this work, the authors re-derive the update-gate mechanism from a ﬁrst principle
called time-warping invariance. We adapted their derivation to the hyperbolic setting by using the
notion of gyroderivative [4] and proving a gyro-chain-rule (see appendix E).

4 Experiments

SNLI task and dataset. We evaluate our method on two tasks. The ﬁrst is natural language
inference, or textual entailment. Given two sentences, a premise (e.g. "Little kids A. and B. are
playing soccer.") and a hypothesis (e.g. "Two children are playing outdoors."), the binary classiﬁcation
task is to predict whether the second sentence can be inferred from the ﬁrst one. This deﬁnes a partial
order in the sentence space. We test hyperbolic networks on the biggest real dataset for this task,
SNLI [7]. It consists of 570K training, 10K validation and 10K test sentence pairs. Following [28],
we merge the "contradiction" and "neutral" classes into a single class of negative sentence pairs, while
the "entailment" class gives the positive pairs.

PREFIX task and datasets. We conjecture that the improvements of hyperbolic neural networks
are more signiﬁcant when the underlying data structure is closer to a tree. To test this, we design a
proof-of-concept task of detection of noisy preﬁxes, i.e. given two sentences, one has to decide if the
second sentence is a noisy preﬁx of the ﬁrst, or a random sentence. We thus build synthetic datasets
PREFIX-Z% (for Z being 10, 30 or 50) as follows: for each random ﬁrst sentence of random length
at most 20 and one random preﬁx of it, a second positive sentence is generated by randomly replacing
Z% of the words of the preﬁx, and a second negative sentence of same length is randomly generated.
Word vocabulary size is 100, and we generate 500K training, 10K validation and 10K test pairs.

6If x has n coordinates, then diag(x) denotes the diagonal matrix of size n with xi’s on its diagonal.

8

Models architecture. Our neural network layers can be used in a plug-n-play manner exactly like
standard Euclidean layers. They can also be combined with Euclidean layers. However, optimization
w.r.t. hyperbolic parameters is different (see below) and based on Riemannian gradients which
are just rescaled Euclidean gradients when working in the conformal Poincaré model [21]. Thus,
back-propagation can be applied in the standard way.

In our setting, we embed the two sentences using two distinct hyperbolic RNNs or GRUs. The
sentence embeddings are then fed together with their squared distance (hyperbolic or Euclidean,
depending on their geometry) to a FFNN (Euclidean or hyperbolic, see Sec. 3.2) which is further
fed to an MLR (Euclidean or hyperbolic, see Sec. 3.1) that gives probabilities of the two classes
(entailment vs neutral). We use cross-entropy loss on top. Note that hyperbolic and Euclidean layers
can be mixed, e.g. the full network can be hyperbolic and only the last layer be Euclidean, in which
case one has to use log0 and exp0 functions to move between the two manifolds in a correct manner
as explained for Eq. 26.

Optimization. Our models have both Euclidean (e.g. weight matrices in both Euclidean and
hyperbolic FFNNs, RNNs or GRUs) and hyperbolic parameters (e.g. word embeddings or biases for
the hyperbolic layers). We optimize the Euclidean parameters with Adam [16] (learning rate 0.001).
Hyperbolic parameters cannot be updated with an equivalent method that keeps track of gradient
history due to the absence of a Riemannian Adam. Thus, they are optimized using full Riemannian
stochastic gradient descent (RSGD) [5, 11]. We also experiment with projected RSGD [21], but
optimization was sometimes less stable. We use a different constant learning rate for word embeddings
(0.1) and other hyperbolic weights (0.01) because words are updated less frequently.

Numerical errors. Gradients of the basic operations deﬁned above (e.g. ⊕c, exponential map) are
c(cid:107)x(cid:107) = 1. Thus, we
not deﬁned when the hyperbolic argument vectors are on the ball border, i.e.
always project results of these operations in the ball of radius 1 − (cid:15), where (cid:15) = 10−5. Numerical
errors also appear when hyperbolic vectors get closer to 0, thus we perturb them with an (cid:15)(cid:48) = 10−15
before they are used in any of the above operations. Finally, arguments of the tanh function are
clipped between ±15 to avoid numerical errors, while arguments of tanh−1 are clipped to at most
1 − 10−5.

√

Hyperparameters. For all methods, baselines and datasets, we use c = 1, word and hidden state
embedding dimension of 5 (we focus on the low dimensional setting that was shown to already
be effective [21]), batch size of 64. We ran all methods for a ﬁxed number of 30 epochs. For all
models, we experiment with both identity (no non-linearity) or tanh non-linearity in the RNN/GRU
cell, as well as identity or ReLU after the FFNN layer and before MLR. As expected, for the fully
Euclidean models, tanh and ReLU respectively surpassed the identity variant by a large margin. We
only report the best Euclidean results. Interestingly, for the hyperbolic models, using only identity for
both non-linearities works slightly better and this is likely due to two facts: i) our hyperbolic layers
already contain non-linearities by their nature, ii) tanh is limiting the output domain of the sentence
embeddings, but the hyperbolic speciﬁc geometry is more pronounced at the ball border, i.e. at the
hyperbolic "inﬁnity", compared to the center of the ball.

For the results shown in Tab. 1, we run each model (baseline or ours) exactly 3 times and report the
test result corresponding to the best validation result from these 3 runs. We do this because the highly
non-convex spectrum of hyperbolic neural networks sometimes results in convergence to poor local
minima, suggesting that initialization is very important.

Results. Results are shown in Tab. 1. Note that the fully Euclidean baseline models might have
an advantage over hyperbolic baselines because more sophisticated optimization algorithms such
as Adam do not have a hyperbolic analogue at the moment. We ﬁrst observe that all GRU models
overpass their RNN variants. Hyperbolic RNNs and GRUs have the most signiﬁcant improvement
over their Euclidean variants when the underlying data structure is more tree-like, e.g. for PREFIX-
10% − for which the tree relation between sentences and their preﬁxes is more prominent − we
reduce the error by a factor of 3.35 for hyperbolic vs Euclidean RNN, and by a factor of 1.5 for
hyperbolic vs Euclidean GRU. As soon as the underlying structure diverges more and more from
a tree, the accuracy gap decreases − for example, for PREFIX-50% the noise heavily affects the
representational power of hyperbolic networks. Also, note that on SNLI our methods perform
similarly as with their Euclidean variants. Moreover, hyperbolic and Euclidean MLR are on par when

9

SNLI

PREFIX-10% PREFIX-30% PREFIX-50%

FULLY EUCLIDEAN RNN
HYPERBOLIC RNN+FFNN, EUCL MLR
FULLY HYPERBOLIC RNN
FULLY EUCLIDEAN GRU
HYPERBOLIC GRU+FFNN, EUCL MLR
FULLY HYPERBOLIC GRU

79.34 %
79.18 %
78.21 %
81.52 %
79.76 %
81.19 %

89.62 %
96.36 %
96.91 %
95.96 %
97.36 %
97.14 %

81.71 %
87.83 %
87.25 %
86.47 %
88.47 %
88.26 %

72.10 %
76.50 %
62.94 %
75.04 %
76.87 %
76.44 %

Table 1: Test accuracies for various models and four datasets. "Eucl" denotes Euclidean. All word
and sentence embeddings have dimension 5. We highlight in bold the best baseline (or baselines, if
the difference is less than 0.5%).

used in conjunction with hyperbolic sentence embeddings, suggesting further empirical investigation
is needed for this direction (see below).

We also observe that, in the hyperbolic setting, accuracy tends to increase when sentence embeddings
start increasing, and gets better as their norms converge towards 1 (the ball border for c = 1). Unlike
in the Euclidean case, this behavior does happen only after a few epochs and suggests that the model
should ﬁrst adjust the angular layout in order to disentangle the representations, before increasing their
norms to fully exploit the strong clustering property of the hyperbolic geometry. Similar behavior
was observed in the context of embedding trees by [21]. Details in appendix F.

MLR classiﬁcation experiments.
For the sentence entailment classi-
ﬁcation task we do not see a clear
advantage of hyperbolic MLR com-
pared to its Euclidean variant. A pos-
sible reason is that, when trained end-
to-end, the model might decide to
place positive and negative embed-
dings in a manner that is already well
separated with a classic MLR. As a
consequence, we further investigate
MLR for the task of subtree classiﬁ-
cation. Using an open source imple-
mentation7 of [21], we pre-trained
Poincaré embeddings of the Word-
Net noun hierarchy (82,115 nodes).
We then choose one node in this tree
(see Table 2) and classify all other
nodes (solely based on their embed-
dings) as being part of the subtree
rooted at this node. All nodes in such a subtree are divided into positive training nodes (80%) and
positive test nodes (20%). The same splitting procedure is applied for the remaining WordNet nodes
that are divided into a negative training and negative test set respectively. Three variants of MLR
are then trained on top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation
task: hyperbolic MLR, Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean
MLR applied after mapping all embeddings in the tangent space at 0 using the log0 map. We use
different embedding dimensions : 2, 3, 5 and 10. For the hyperbolic MLR, we use full Riemannian
SGD with a learning rate of 0.001. For the two Euclidean models we use ADAM optimizer and the
same learning rate. During training, we always sample the same number of negative and positive
nodes in each minibatch of size 16; thus positive nodes are frequently resampled. All methods are
trained for 30 epochs and the ﬁnal F1 score is reported (no hyperparameters to validate are used, thus
we do not require a validation set). This procedure is repeated for four subtrees of different sizes.

Figure 2: Hyperbolic (left) vs Direct Euclidean (right) binary
MLR used to classify nodes as being part in the GROUP.N.01
subtree of the WordNet noun hierarchy solely based on their
Poincaré embeddings. The positive points (from the subtree)
are in blue, the negative points (the rest) are in red and the
trained positive separation hyperplane is depicted in green.

Quantitative results are presented in Table 2. We can see that the hyperbolic MLR overpasses
its Euclidean variants in almost all settings, sometimes by a large margin. Moreover, to provide

7https://github.com/dalab/hyperbolic_cones

10

WORDNET
SUBTREE

ANIMAL.N.01
3218 / 798

GROUP.N.01
6649 / 1727

WORKER.N.01
861 / 254

MAMMAL.N.01
953 / 228

MODEL

D = 2

D = 3

D = 5

D = 10

HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL

47.43 ± 1.07%
41.69 ± 0.19%
38.89 ± 0.01%
81.72 ± 0.17%
61.13 ± 0.42%
60.75 ± 0.24%
12.68 ± 0.82%
10.86 ± 0.01%
9.04 ± 0.06%
32.01 ± 17.14%
15.58 ± 0.04%
13.10 ± 0.13%

91.92 ± 0.61%
68.43 ± 3.90%
62.57 ± 0.61%
89.87 ± 2.73%
63.56 ± 1.22%
61.98 ± 0.57%
24.09 ± 1.49%
22.39 ± 0.04%
22.57 ± 0.20%
87.54 ± 4.55%
44.68 ± 1.87%
44.89 ± 1.18%

98.07 ± 0.55%
95.59 ± 1.18%
89.21 ± 1.34%
87.89 ± 0.80%
67.82 ± 0.81%
67.92 ± 0.74%
55.46 ± 5.49%
35.23 ± 3.16%
26.47 ± 0.78%
88.73 ± 3.22%
59.35 ± 1.31%
52.51 ± 0.85%

99.26 ± 0.59%
99.36 ± 0.18%
98.27 ± 0.70%
91.91 ± 3.07%
91.38 ± 1.19%
91.41 ± 0.18%
66.83 ± 11.38%
47.29 ± 3.93%
36.66 ± 2.74%
91.37 ± 6.09%
77.76 ± 5.08%
56.11 ± 2.21%

Table 2: Test F1 classiﬁcation scores for four different subtrees of WordNet noun tree. All nodes
in such a subtree are divided into positive training nodes (80%) and positive test nodes (20%);
these counts are shown below each subtree root. The same splitting procedure is applied for the
remaining nodes to obtain negative training and test sets. Three variants of MLR are then trained on
top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation task: hyperbolic MLR,
Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean MLR applied after
mapping all embeddings in the tangent space at 0 using the log0 map. 95% conﬁdence intervals for 3
different runs are shown for each method and each different embedding dimension (2, 3, 5 or 10).

further understanding, we plot the 2-dimensional embeddings and the trained separation hyperplanes
(geodesics in this case) in Figure 2. We can see that respecting the hyperbolic geometry is very
important for a quality classiﬁcation model.

5 Conclusion

We showed how classic Euclidean deep learning tools such as MLR, FFNNs, RNNs or GRUs can be
generalized in a principled manner to all spaces of constant negative curvature combining Riemannian
geometry with the elegant theory of gyrovector spaces. Empirically we found that our models
outperform or are on par with corresponding Euclidean architectures on sequential data with implicit
hierarchical structure. We hope to trigger exciting future research related to better understanding
of the hyperbolic non-convexity spectrum and development of other non-Euclidean deep learning
methods.
Our data and Tensorﬂow [1] code are publicly available8.

Acknowledgements

We thank Igor Petrovski for useful pointers regarding the implementation.

This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement
number 167176. Gary Bécigneul is also funded by the Max Planck ETH Center for Learning
Systems.

References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for
large-scale machine learning. 2016.

[2] Ungar Abraham Albert. Analytic hyperbolic geometry and Albert Einstein’s special theory of

relativity. World scientiﬁc, 2008.

8https://github.com/dalab/hyperbolic_nn

11

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of the International Conference on Learning
Representations (ICLR), 2015.

[4] Graciela S Birman and Abraham A Ungar. The hyperbolic derivative in the poincaré ball model
of hyperbolic geometry. Journal of mathematical analysis and applications, 254(1):321–333,
2001.

[5] S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on

Automatic Control, 58(9):2217–2229, Sept 2013.

[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems (NIPS), pages 2787–2795, 2013.

[7] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 632–642. Association
for Computational Linguistics, 2015.

[8] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,
34(4):18–42, 2017.

[9] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry.

Flavors of geometry, 31:59–115, 1997.

[10] Christopher De Sa, Albert Gu, Christopher Ré, and Frederic Sala. Representation tradeoffs for

hyperbolic embeddings. arXiv preprint arXiv:1804.03329, 2018.

[11] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In Proceedings of the thirty-ﬁfth international conference
on machine learning (ICML), 2018.

[12] Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pages 75–263. Springer, 1987.

[13] Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the

Cambridge Philosophical Society, page 1–17, 2017.

[14] Christopher Hopper and Ben Andrews. The Ricci ﬂow in Riemannian geometry. Springer, 2010.

[15] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1746–1751. Association for Computational Linguistics, 2014.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations (ICLR), 2015.

[17] Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marián Boguná.

Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.

[18] John Lamping, Ramana Rao, and Peter Pirolli. A focus+ context technique based on hyperbolic
geometry for visualizing large hierarchies. In Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 401–408. ACM Press/Addison-Wesley Publishing Co.,
1995.

[19] Guy Lebanon and John Lafferty. Hyperplane margin classiﬁers on the multinomial manifold. In
Proceedings of the international conference on machine learning (ICML), page 66. ACM, 2004.

[20] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective
learning on multi-relational data. In Proceedings of the international conference on machine
learning (ICML), volume 11, pages 809–816, 2011.

12

[21] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical repre-
sentations. In Advances in Neural Information Processing Systems (NIPS), pages 6341–6350,
2017.

[22] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blun-
som. Reasoning about entailment with neural attention. In Proceedings of the International
Conference on Learning Representations (ICLR), 2015.

[23] Michael Spivak. A comprehensive introduction to differential geometry. Publish or perish, 1979.

[24] Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In Proceedings of

the International Conference on Learning Representations (ICLR), 2018.

[25] Abraham A Ungar. Hyperbolic trigonometry and its application in the poincaré ball model of

hyperbolic geometry. Computers & Mathematics with Applications, 41(1-2):135–147, 2001.

[26] Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis

Lectures on Mathematics and Statistics, 1(1):1–194, 2008.

[27] Abraham Albert Ungar. Analytic hyperbolic geometry in n dimensions: An introduction. CRC

Press, 2014.

[28] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. In Proceedings of the International Conference on Learning Representations (ICLR),
2016.

[29] J Vermeer. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic

plane. Topology and its Applications, 152(3):226–242, 2005.

13

A Hyperbolic Trigonometry

Hyperbolic angles. For A, B, C ∈ Dn
c , we denote by ∠A := ∠BAC the angle between the two
geodesics starting from A and ending at B and C respectively. This angle can be deﬁned in two
equivalent ways: i) either using the angle between the initial velocities of the two geodesics as given
by Eq. 5, or ii) using the formula

cos(∠A) =

(cid:28) (−A) ⊕c B
(cid:107)(−A) ⊕c B(cid:107)

,

(−A) ⊕c C
(cid:107)(−A) ⊕c C(cid:107)

(cid:29)

,

In this case, ∠A is also called a gyroangle in the work of [26, section 4].

Hyperbolic law of sines. We state here the hyperbolic law of sines. If for A, B, C ∈ Dn
c , we
denote by ∠B := ∠ABC the angle between the two geodesics starting from B and ending at A and
C respectively, and by ˜c = dc(B, A) the length of the hyperbolic segment BA (and similarly for
others), then we have:

sin(∠A)
√
c˜a)
sinh(

=

sin(∠B)
√
c˜b)
sinh(

=

sin(∠C)
√
c˜c)
sinh(

.

Note that one can also adapt the hyperbolic law of cosines to the hyperbolic space.

B Proof of Theorem 4

Theorem 4.
In the manifold (Dn
to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a vector v ∈ T0Dn
c

c is given by the following isometry:
λc
0
λc
x

0→x(v) = logc
P c

x(x ⊕c expc

0(v)) =

v.

Proof. The geodesic in Dn
v ∈ T0Dn
γ (i.e. X(t) ∈ Tγ(t)Dn

c from 0 to x is given in Eq. (10) by γ(t) = x ⊗c t, for t ∈ [0, 1]. Let
c . Then it is of common knowledge that there exists a unique parallel9 vector ﬁeld X along

c , ∀t ∈ [0, 1]) such that X(0) = v. Let’s deﬁne:
X : t ∈ [0, 1] (cid:55)→ logc

γ(t)(γ(t) ⊕c expc

0(v)) ∈ Tγ(t)Dn
c .

Clearly, X is a vector ﬁeld along γ such that X(0) = v. Now deﬁne
0→x : v ∈ T0Dn
P c

x(x ⊕c expc

0(v)) ∈ TxDn
c .

c (cid:55)→ logc
0→x(v) = λc

c . Since P c

0→x is a linear isometry from T0Dn
v, hence P c
c
0→x(v) = X(1), it is enough to prove that X is parallel in order to guarantee that

From Eq. (12), it is easily seen that P c
to TxDn
c to TxDn
0→x is the parallel transport from T0Dn
P c
c .
Since X is a vector ﬁeld along γ, its covariant derivative can be expressed with the Levi-Civita
connection ∇c associated to gc:

0
λc
x

DX
∂t

= ∇c

˙γ(t)X.

Let’s compute the Levi-Civita connection from its Christoffel symbols. In a local coordinate system,
they can be written as

Γi

jk =

(gc)il(∂jgc

lk + ∂kgc

lj − ∂lgc

jk),

1
2

where superscripts denote the inverse metric tensor and using Einstein’s notations. As gc
at γ(t) ∈ Dn

c this yields:

ij = (λc)2δij,

jk = cλc
Γi

γ(t)(δikγ(t)j + δijγ(t)k − δjkγ(t)i).

9i.e. that DX

∂t = 0 for t ∈ [0, 1], where D

∂t denotes the covariant derivative.

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

14

On the other hand, since X(t) = (λc

∇c

˙γ(t)X = ˙γ(t)i∇c

i X = ˙γ(t)i∇c
i

= vj ˙γ(t)i∇c
i

0/λc

γ(t))v, we have
(cid:32)

(cid:33)

λc
0
λc

γ(t)

v

(cid:32)

λc
0
λc

γ(t)

(cid:33)

ej

.

√

√

Since γ(t) = (1/
Hence there exists K x

c) tanh(t tanh−1(
t ∈ R such that ˙γ(t) = K x

c(cid:107)x(cid:107))) x

(cid:107)x(cid:107) , it is easily seen that ˙γ(t) is colinear to γ(t).
t γ(t). Moreover, we have the following Leibniz rule:
(cid:33)

(cid:32)

(cid:32)

∇c
i

λc
0
λc

γ(t)

(cid:33)

ej

=

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

ej.

Combining these yields

DX
∂t

= K x

t vjγ(t)i

(cid:32)

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

(cid:32)

(cid:33)

(cid:33)

ej

.

Replacing with the Christoffel symbols of ∇c at γ(t) gives

Moreover,

λc
0
λc

γ(t)

λc
0
λc

γ(t)

∇c

i ej =

ijek = 2c[δk
Γk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek.

∂
∂γ(t)i

(cid:32)

(cid:33)

λc
0
λc

γ(t)

ej =

∂
∂γ(t)i

(cid:0)−c(cid:107)γ(t)(cid:107)2(cid:1) ej = −2cγ(t)iej.

Putting together everything, we obtain

DX
∂t

= K x

t vjγ(t)i (cid:0)2c[δk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek − 2cγ(t)iej

(cid:1)

t vjγ(t)i (cid:0)γ(t)jei − δijγ(t)kek
t vj (cid:0)γ(t)jγ(t)iei − γ(t)iδijγ(t)kek
(cid:1)
t vj (cid:0)γ(t)jγ(t)iei − γ(t)jγ(t)kek

(cid:1)

(cid:1)

= 2cK x
= 2cK x
= 2cK x
= 0,

which concludes the proof.

C Proof of Eq. (22)

Proof. Two steps proof:
i) expc

p({a}⊥) ⊆ {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}:

Let z ∈ {a}⊥. From Eq. (12), we have that:

This, together with the left-cancellation law in gyrospaces (see section 2.3), implies that

expc

p(z) = −p ⊕c βz,

for some β ∈ R.

(cid:104)−p ⊕c expc

p(z), a(cid:105) = (cid:104)βz, a(cid:105) = 0

which is what we wanted.

ii) {x ∈ Dn
Let x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0} ⊆ expc
c s.t. (cid:104)−p ⊕c x, a(cid:105) = 0. Then, using Eq. (12), we derive that:
for some β ∈ R,

p(x) = β(−p ⊕c x),

p({a}⊥):

logc

which is orthogonal to a, by assumption. This implies logc

p(x) ∈ {a}⊥, hence x ∈ expc

p({a}⊥).

15

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(54)

D Proof of Theorem 5

Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

1
√
c

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(55)

Proof. We ﬁrst need to prove the following lemma, trivial in the Euclidean space, but not in the
Poincaré ball:
Lemma 7. (Orthogonal projection on a geodesic) Any point in the Poincaré ball has a unique
orthogonal projection on any given geodesic that does not pass through the point. Formally, for all
y ∈ Dn
c and for all geodesics γx→z(·) s.t. y /∈ Im γx→z, there exists an unique w ∈ Im γx→z s.t.
∠(γw→y, γx→z) = π/2.

Proof. We ﬁrst note that any geodesic in Dn
and has two "points at inﬁnity" lying on the ball border (v (cid:54)= 0):

c has the form γ(t) = u ⊕c v ⊗c t as given by Eq. 11,

γ(±∞) = u ⊕c

√

±v
c(cid:107)v(cid:107)

∈ ∂Dn
c .

(56)

Using the notations in the lemma statement, the closed-form of γx→z is given by Eq. (10):

γx→z(t) = x ⊕c (−x ⊕c z) ⊗c t

We denote by x(cid:48), z(cid:48) ∈ ∂Dn
∠ywx(cid:48) is well deﬁned from Eq. (34):

c its points at inﬁnity as described by Eq. (56). Then, the hyperbolic angle

cos(∠(γw→y, γx→z)) = cos(∠ywz(cid:48)) =

(cid:104)−w ⊕c y, −w ⊕c z(cid:48)(cid:105)
(cid:107) − w ⊕c y(cid:107) · (cid:107) − w ⊕c z(cid:48)(cid:107)

.

(57)

We now perform 2 steps for this proof.

i) Existence of w:

The angle function from Eq. (57) is continuous w.r.t t when w = γx→z(t). So we ﬁrst prove existence
of an angle of π/2 by continuously moving w from x(cid:48) to z(cid:48) when t goes from −∞ to ∞, and
observing that cos(∠ywz(cid:48)) goes from −1 to 1 as follows:

cos(∠yx(cid:48)z(cid:48)) = 1 & lim
w→z(cid:48)

cos(∠ywz(cid:48)) = −1.

(58)

The left part of Eq. (58) follows from Eq. (57) and from the fact (easy to show from the deﬁnition
c (which is the case of x(cid:48)). The right part of Eq. (58)
of ⊕c) that a ⊕c b = a, when (cid:107)a(cid:107) = 1/
follows from the fact that ∠ywz(cid:48) = π − ∠ywx(cid:48) (from the conformal property, or from Eq. (34)) and
cos(∠yz(cid:48)x(cid:48)) = 1 (proved as above).
Hence cos(∠ywz(cid:48)) has to pass through 0 when going from −1 to 1, which achieves the proof of
existence.

√

ii) Uniqueness of w:
Assume by contradiction that there are two w and w(cid:48) on γx→z that form angles ∠ywx(cid:48) and ∠yw(cid:48)x(cid:48)
of π/2. Since w, w(cid:48), x(cid:48) are on the same geodesic, we have

π/2 = ∠yw(cid:48)x(cid:48) = ∠yw(cid:48)w = ∠ywx(cid:48) = ∠yw(cid:48)w
So ∆yww(cid:48) has two right angles, but in the Poincaré ball this is impossible.

(59)

Now, we need two more lemmas:
Lemma 8. (Minimizing distance from point to geodesic) The orthogonal projection of a point to
a geodesic (not passing through the point) is minimizing the distance between the point and the
geodesic.

Proof. The proof is similar with the Euclidean case and it’s based on hyperbolic sine law and the fact
that in any right hyperbolic triangle the hypotenuse is strictly longer than any of the other sides.

16

Lemma 9. (Geodesics through p) Let ˜H c
all points on the geodesic γp→w are included in ˜H c

a,p.

a,p be a Poincaré hyperplane. Then, for any w ∈ ˜H c

a,p \ {p},

Proof. γp→w(t) = p ⊕c (−p ⊕c w) ⊗c t. Then, it is easy to check the condition in Eq. (22):

(cid:104)−p ⊕c γp→w(t), a(cid:105) = (cid:104)(−p ⊕c w) ⊗c t, a(cid:105) ∝ (cid:104)−p ⊕c w, a(cid:105) = 0.

(60)

We now turn back to our proof. Let x ∈ Dn
We prove that there is at least one point w∗ ∈ ˜H c

c be an arbitrary point and ˜H c

a,p a Poincaré hyperplane.

a,p that achieves the inﬁmum distance

dc(x, w∗) = inf
w∈ ˜H c

a,p

dc(x, w),

and, moreover, that this distance is the same as the one in the theorem’s statement.
We ﬁrst note that for any point w ∈ ˜H c
and Lemma 9, it is obvious that the projection of x to γp→w will give a strictly lower distance.
Thus, we only consider w ∈ ˜H c
triangle ∆xwp, one gets:

a,p such that ∠xwp = π/2. Applying hyperbolic sine law in the right

a,p, if ∠xwp (cid:54)= π/2, then w (cid:54)= w∗. Indeed, using Lemma 8

dc(x, w) = (1/

c) sinh−1 (cid:0)sinh(

c dc(x, p)) · sin(∠xpw)(cid:1) .

√

√

One of the above quantities does not depend on w:

√

√

sinh(

c dc(x, p)) = sinh(2 tanh−1(

c(cid:107) − p ⊕c x(cid:107))) =

√
2
c(cid:107) − p ⊕c x(cid:107)
1 − c(cid:107) − p ⊕c x(cid:107)2 .

The other quantity is sin(∠xpw) which is minimized when the angle ∠xpw is minimized (be-
cause ∠xpw < π/2 for the hyperbolic right triangle ∆xwp), or, alternatively, when cos(∠xpw) is
maximized. But, we already have from Eq. (34) that:

cos(∠xpw) =

(cid:104)−p ⊕c x, −p ⊕c w(cid:105)
(cid:107) − p ⊕c x(cid:107) · (cid:107) − p ⊕c w(cid:107)

.

To maximize the above, the constraint on the right angle at w can be dropped because cos(∠xpw)
depends only on the geodesic γp→w and not on w itself, and because there is always an orthogonal
projection from any point x to any geodesic as stated by Lemma 7. Thus, it remains to ﬁnd the
maximum of Eq. (64) when w ∈ ˜H c
a,p from Eq. (22), one can easily
prove that

a,p. Using the deﬁnition of ˜H c

Using that fact that logc

p(w)/(cid:107) logc

p(w)(cid:107) = −p ⊕c w/(cid:107) − p ⊕c w(cid:107), we just have to ﬁnd

and we are left with a well known Euclidean problem which is equivalent to ﬁnding the minimum
angle between the vector −p ⊕c x (viewed as Euclidean) and the hyperplane {a}⊥. This angle
is given by the Euclidean orthogonal projection whose sin value is the distance from the vector’s
endpoint to the hyperplane divided by the vector’s length:

{logc

p(w) : w ∈ ˜H c

a,p} = {a}⊥.

max
z∈{a}⊥

(cid:18) (cid:104)−p ⊕c x, z(cid:105)

(cid:107) − p ⊕c x(cid:107) · (cid:107)z(cid:107)

(cid:19)

,

sin(∠xpw∗) =

|(cid:104)−p ⊕c x, a
(cid:107) − p ⊕c x(cid:107)

(cid:107)a(cid:107) (cid:105)|

.

17

It follows that a point w∗ ∈ ˜H c
Eqs. (61),(62),(63) and (67) concludes the proof.

a,p satisfying Eq. (67) exists (but might not be unique). Combining

(61)

(62)

(63)

(64)

(65)

(66)

(67)

(cid:3)

E Derivation of the Hyperbolic GRU Update-gate

In [24], the authors recover the update/forget-gate mechanism of a GRU/LSTM by requiring that the
class of neural networks given by the chosen architecture be invariant to time-warpings. The idea is
the following.

Recovering the update-gate from time-warping. A naive RNN is given by the equation

h(t + 1) = ϕ(W h(t) + U x(t) + b)

Let’s drop the bias b to simplify notations. If h is seen as a differentiable function of time, then a
ﬁrst-order Taylor development gives h(t + δt) ≈ h(t) + δt dh
dt (t) for small δt. Combining this for
δt = 1 with the naive RNN equation, one gets

dh
dt

dα
dt

(t) = ϕ(W h(t) + U x(t)) − h(t).

As this is written for any t, one can replace it by t ← α(t) where α is a (smooth) increasing function
of t called the time-warping. Denoting by ˜h(t) := h(α(t)) and ˜x(t) := x(α(t)), using the chain rule
d˜h
dt (t) = dα

dt (α(t)), one gets

dt (t) dh

d˜h
dt

dα
dt

(t) =

(t)ϕ(W ˜h(t) + U ˜x(t)) −

(t)˜h(t).

(70)

Removing the tildas to simplify notations, discretizing back with dh

dt (t) ≈ h(t + 1) − h(t) yields

h(t + 1) =

(t)ϕ(W h(t) + U x(t)) +

1 −

(t)

h(t).

(71)

dα
dt

(cid:18)

(cid:19)

dα
dt

Requiring that our class of neural networks be invariant to time-warpings means that this class should
contain RNNs deﬁned by Eq. (71), i.e. that dα
dt (t) can be learned. As this is a positive quantity, we
can parametrize it as z(t) = σ(W zh(t) + U zx(t)), recovering the forget-gate equation:

h(t + 1) = z(t)ϕ(W h(t) + U x(t)) + (1 − z(t))h(t).

Adapting this idea to hyperbolic RNNs. The gyroderivative [4] of a map h : R → Dn
as

c is deﬁned

dh
dt

(t) = lim
δt→0

1
δt

⊗c (−h(t) ⊕c h(t + δt)).

Using Möbius scalar associativity and the left-cancellation law leads us to

h(t + δt) ≈ h(t) ⊕c δt ⊗c

(t),

dh
dt

for small δt. Combining this with the equation of a simple hyperbolic RNN of Eq. (29) with δt = 1,
one gets

dh
dt

(t) = −h(t) ⊕c ϕ⊗c(W ⊗c h(t) ⊕c U ⊗c x(t)).

For the next step, we need the following lemma:
Lemma 10 (Gyro-chain-rule). For α : R → R differentiable and h : R → Dn
gyro-derivative, if ˜h := h ◦ α, then we have

c with a well-deﬁned

(68)

(69)

(72)

(73)

(74)

(75)

(76)

where dα

dt (t) denotes the usual derivative.

d˜h
dt

(t) =

(t) ⊗c

(α(t)),

dα
dt

dh
dt

18

(77)

(78)

(79)

(80)

(81)

Proof.

d˜h
dt

(t) = lim
δt→0

1
δt
1
δt

⊗c [−˜h(t) ⊕c

˜h(t + δt)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

= lim
δt→0

= lim
δt→0

= lim
δt→0

= lim
u→0
dα
dt

=

α(cid:48)(t) + O(δt)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
u

(t) ⊗c

(α(t))

dh
dt

⊗c [−h(α(t)) ⊕c h(α(t) + u)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

(Möbius scalar associativity) (82)

where we set u = δt(α(cid:48)(t) + O(δt)), with u → 0 when δt → 0, which concludes.

Using lemma 10 and Eq. (75), with similar notations as in Eq. (70) we have

d˜h
dt

dα
dt

(t) =

(t) ⊗c (−˜h(t) ⊕c ϕ⊗c(W ⊗c

˜h(t) ⊕c U ⊗c ˜x(t))).

(83)

Finally, discretizing back with Eq. (74), using the left-cancellation law and dropping the tildas yields

h(t + 1) = h(t) ⊕c

(t) ⊗c (−h(t) ⊕c ϕ⊗c (W ⊗c h(t) ⊕c U ⊗c x(t))).

(84)

dα
dt

Since α is a time-warping, by deﬁnition its derivative is positive and one can choose to parametrize
it with an update-gate zt (a scalar) deﬁned with a sigmoid. Generalizing this scalar scaling by the
Möbius version of the pointwise scaling (cid:12) yields the Möbius matrix scaling diag(zt) ⊗c ·, leading to
our proposed Eq. (33) for the hyperbolic GRU.

F More Experimental Investigations

The following empirical facts were observed for both hyperbolic RNNs and GRUs.

We observed that, in the hyperbolic setting, accuracy is often much higher when sentence embeddings
can go close to the border (hyperbolic "inﬁnity"), hence exploiting the hyperbolic nature of the space.
Moreover, the faster the two sentence norms go to 1, the more it’s likely that a good local minima
was reached. See ﬁgures 3 and 5.

We often observe that test accuracy starts increasing exactly when sentence embedding norms do.
However, in the hyperbolic setting, the sentence embeddings norms remain close to 0 for a few
epochs, which does not happen in the Euclidean case. See ﬁgures 3, 5 and 4. This mysterious fact
was also exhibited in a similar way by [21] which suggests that the model ﬁrst has to adjust the
angular layout in the almost Euclidean vicinity of 0 before increasing norms and fully exploiting
hyperbolic geometry.

19

(a) Test accuracy

(a) Test accuracy

20

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 3: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the same
architecture: hyperbolic GRU followed by hyperbolic FFNN and hyperbolic/Euclidean (half-half)
MLR. The X axis shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 4: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: Euclidean GRU followed by Euclidean FFNN and Euclidean MLR. The X axis
shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 5: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: hyperbolic RNN followed by hyperbolic FFNN and hyperbolic MLR. The X axis
shows millions of training examples processed.

(a) Test accuracy

21

8
1
0
2
 
n
u
J
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
2
1
1
9
0
.
5
0
8
1
:
v
i
X
r
a

Hyperbolic Neural Networks

Octavian-Eugen Ganea∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
octavian.ganea@inf.ethz.ch

Gary Bécigneul∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
gary.becigneul@inf.ethz.ch

Thomas Hofmann
Department of Computer Science
ETH Zürich
Zurich, Switzerland
thomas.hofmann@inf.ethz.ch

Abstract

Hyperbolic spaces have recently gained momentum in the context of machine
learning due to their high capacity and tree-likeliness properties. However, the
representational power of hyperbolic geometry is not yet on par with Euclidean
geometry, mostly because of the absence of corresponding hyperbolic neural
network layers. This makes it hard to use hyperbolic embeddings in downstream
tasks. Here, we bridge this gap in a principled manner by combining the formalism
of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model
of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep
learning tools: multinomial logistic regression, feed-forward and recurrent neural
networks such as gated recurrent units. This allows to embed sequential data and
perform classiﬁcation in the hyperbolic space. Empirically, we show that, even if
hyperbolic optimization tools are limited, hyperbolic sentence embeddings either
outperform or are on par with their Euclidean variants on textual entailment and
noisy-preﬁx recognition tasks.

1

Introduction

It is common in machine learning to represent data as being embedded in the Euclidean space Rn. The
main reason for such a choice is simply convenience, as this space has a vectorial structure, closed-
form formulas of distance and inner-product, and is the natural generalization of our intuition-friendly,
visual three-dimensional space. Moreover, embedding entities in such a continuous space allows to
feed them as input to neural networks, which has led to unprecedented performance on a broad range
of problems, including sentiment detection [15], machine translation [3], textual entailment [22] or
knowledge base link prediction [20, 6].

Despite the success of Euclidean embeddings, recent research has proven that many types of com-
plex data (e.g. graph data) from a multitude of ﬁelds (e.g. Biology, Network Science, Computer
Graphics or Computer Vision) exhibit a highly non-Euclidean latent anatomy [8]. In such cases, the
Euclidean space does not provide the most powerful or meaningful geometrical representations. For
example, [10] shows that arbitrary tree structures cannot be embedded with arbitrary low distortion
(i.e. almost preserving their metric) in the Euclidean space with unbounded number of dimensions,
but this task becomes surprisingly easy in the hyperbolic space with only 2 dimensions where the
exponential growth of distances matches the exponential growth of nodes with the tree depth.

∗Equal contribution.

The adoption of neural networks and deep learning in these non-Euclidean settings has been rather
limited until very recently, the main reason being the non-trivial or impossible principled general-
izations of basic operations (e.g. vector addition, matrix-vector multiplication, vector translation,
vector inner product) as well as, in more complex geometries, the lack of closed form expressions for
basic objects (e.g. distances, geodesics, parallel transport). Thus, classic tools such as multinomial
logistic regression (MLR), feed forward (FFNN) or recurrent neural networks (RNN) did not have a
correspondence in these geometries.

How should one generalize deep neural models to non-Euclidean domains ? In this paper we address
this question for one of the simplest, yet useful, non-Euclidean domains: spaces of constant negative
curvature, i.e. hyperbolic. Their tree-likeness properties have been extensively studied [12, 13, 26]
and used to visualize large taxonomies [18] or to embed heterogeneous complex networks [17]. In
machine learning, recently, hyperbolic representations greatly outperformed Euclidean embeddings
for hierarchical, taxonomic or entailment data [21, 10, 11]. Disjoint subtrees from the latent hierar-
chical structure surprisingly disentangle and cluster in the embedding space as a simple reﬂection of
the space’s negative curvature. However, appropriate deep learning tools are needed to embed feature
data in this space and use it in downstream tasks. For example, implicitly hierarchical sequence data
(e.g. textual entailment data, phylogenetic trees of DNA sequences or hierarchial captions of images)
would beneﬁt from suitable hyperbolic RNNs.

The main contribution of this paper is to bridge the gap between hyperbolic and Euclidean geometry
in the context of neural networks and deep learning by generalizing in a principled manner both the
basic operations as well as multinomial logistic regression (MLR), feed-forward (FFNN), simple and
gated (GRU) recurrent neural networks (RNN) to the Poincaré model of the hyperbolic geometry.
We do it by connecting the theory of gyrovector spaces and generalized Möbius transformations
introduced by [2, 26] with the Riemannian geometry properties of the manifold. We smoothly
parametrize basic operations and objects in all spaces of constant negative curvature using a uniﬁed
framework that depends only on the curvature value. Thus, we show how Euclidean and hyperbolic
spaces can be continuously deformed into each other. On a series of experiments and datasets we
showcase the effectiveness of our hyperbolic neural network layers compared to their "classic"
Euclidean variants on textual entailment and noisy-preﬁx recognition tasks. We hope that this paper
will open exciting future directions in the nascent ﬁeld of Geometric Deep Learning.

2 The Geometry of the Poincaré Ball

2.1 Basics of Riemannian geometry

We brieﬂy introduce basic concepts of differential geometry largely needed for a principled general-
ization of Euclidean neural networks. For more rigorous and in-depth expositions, see [23, 14].
An n-dimensional manifold M is a space that can locally be approximated by Rn: it is a generalization
to higher dimensions of the notion of a 2D surface. For x ∈ M, one can deﬁne the tangent space
TxM of M at x as the ﬁrst order linear approximation of M around x. A Riemannian metric
g = (gx)x∈M on M is a collection of inner-products gx : TxM × TxM → R varying smoothly
with x. A Riemannian manifold (M, g) is a manifold M equipped with a Riemannian metric g.
Although a choice of a Riemannian metric g on M only seems to deﬁne the geometry locally on M,
it induces global distances by integrating the length (of the speed vector living in the tangent space)
of a shortest path between two points:

(cid:90) 1

(cid:113)

d(x, y) = inf
γ

0

gγ(t)( ˙γ(t), ˙γ(t))dt,

(1)

where γ ∈ C∞([0, 1], M) is such that γ(0) = x and γ(1) = y. A smooth path γ of minimal length
between two points x and y is called a geodesic, and can be seen as the generalization of a straight-line
in Euclidean space. The parallel transport Px→y : TxM → TyM is a linear isometry between
tangent spaces which corresponds to moving tangent vectors along geodesics and deﬁnes a canonical
way to connect tangent spaces. The exponential map expx at x, when well-deﬁned, gives a way to
project back a vector v of the tangent space TxM at x, to a point expx(v) ∈ M on the manifold.
This map is often used to parametrize a geodesic γ starting from γ(0) := x ∈ M with unit-norm
direction ˙γ(0) := v ∈ TxM as t (cid:55)→ expx(tv). For geodesically complete manifolds, such as the
Poincaré ball considered in this work, expx is well-deﬁned on the full tangent space TxM. Finally, a

2

(2)

(3)

(4)

(5)

metric ˜g is said to be conformal to another metric g if it deﬁnes the same angles, i.e.

˜gx(u, v)
(cid:112)˜gx(u, u)(cid:112)˜gx(v, v)

=

gx(u, v)
(cid:112)gx(u, u)(cid:112)gx(v, v)

,

for all x ∈ M, u, v ∈ TxM \ {0}. This is equivalent to the existence of a smooth function
λ : M → R, called the conformal factor, such that ˜gx = λ2

xgx for all x ∈ M.

2.2 Hyperbolic space: the Poincaré ball

The hyperbolic space has ﬁve isometric models that one can work with [9]. Similarly as in [21] and
[11], we choose to work in the Poincaré ball. The Poincaré ball model (Dn, gD) is deﬁned by the
manifold Dn = {x ∈ Rn : (cid:107)x(cid:107) < 1} equipped with the following Riemannian metric:

gD
x = λ2

xgE, where λx :=

2
1 − (cid:107)x(cid:107)2 ,

gE = In being the Euclidean metric tensor. Note that the hyperbolic metric tensor is conformal to
the Euclidean one. The induced distance between two points x, y ∈ Dn is known to be given by

dD(x, y) = cosh−1

1 + 2

(cid:18)

(cid:107)x − y(cid:107)2
(1 − (cid:107)x(cid:107)2)(1 − (cid:107)y(cid:107)2)

(cid:19)

.

Since the Poincaré ball is conformal to Euclidean space, the angle between two vectors u, v ∈
TxDn \ {0} is given by

cos(∠(u, v)) =

gD
x (u, v)
x (u, u)(cid:112)gD

(cid:112)gD

x (v, v)

=

(cid:104)u, v(cid:105)
(cid:107)u(cid:107)(cid:107)v(cid:107)

.

2.3 Gyrovector spaces

In Euclidean space, natural operations inherited from the vectorial structure, such as vector addition,
subtraction and scalar multiplication are often useful. The framework of gyrovector spaces provides
an elegant non-associative algebraic formalism for hyperbolic geometry just as vector spaces provide
the algebraic setting for Euclidean geometry [2, 25, 26].

In particular, these operations are used in special relativity, allowing to add speed vectors belonging
to the Poincaré ball of radius c (the celerity, i.e. the speed of light) so that they remain in the ball,
hence not exceeding the speed of light.

We will make extensive use of these operations in our deﬁnitions of hyperbolic neural networks.
For c ≥ 0, denote2 by Dn
then Dn

c := {x ∈ Rn | c(cid:107)x(cid:107)2 < 1}. Note that if c = 0, then Dn
c. If c = 1 then we recover the usual ball Dn.

c is the open ball of radius 1/

c = Rn; if c > 0,

√

Möbius addition. The Möbius addition of x and y in Dn

c is deﬁned as

x ⊕c y :=

(1 + 2c(cid:104)x, y(cid:105) + c(cid:107)y(cid:107)2)x + (1 − c(cid:107)x(cid:107)2)y
1 + 2c(cid:104)x, y(cid:105) + c2(cid:107)x(cid:107)2(cid:107)y(cid:107)2

.

(6)

In particular, when c = 0, one recovers the Euclidean addition of two vectors in Rn. Note that
without loss of generality, the case c > 0 can be reduced to c = 1. Unless stated otherwise, we
will use ⊕ as ⊕1 to simplify notations. For general c > 0, this operation is not commutative nor
associative. However, it satisﬁes x ⊕c 0 = 0 ⊕c x = 0. Moreover, for any x, y ∈ Dn
c , we have
(−x) ⊕c x = x ⊕c (−x) = 0 and (−x) ⊕c (x ⊕c y) = y (left-cancellation law). The Möbius
substraction is then deﬁned by the use of the following notation: x (cid:9)c y := x ⊕c (−y). See [29,
section 2.1] for a geometric interpretation of the Möbius addition.

2We take different notations as in [25] where the author uses s = 1/

c.

√

3

Möbius scalar multiplication. For c > 0, the Möbius scalar multiplication of x ∈ Dn
r ∈ R is deﬁned as

c \ {0} by

r ⊗c x := (1/

c) tanh(r tanh−1(

c(cid:107)x(cid:107)))

√

√

x
(cid:107)x(cid:107)

,

(7)

and r ⊗c 0 := 0. Note that similarly as for the Möbius addition, one recovers the Euclidean scalar
multiplication when c goes to zero: limc→0 r ⊗c x = rx. This operation satisﬁes desirable properties
such as n ⊗c x = x ⊕c · · · ⊕c x (n additions), (r + r(cid:48)) ⊗c x = r ⊗c x ⊕c r(cid:48) ⊗c x (scalar distributivity3),
(rr(cid:48)) ⊗c x = r ⊗c (r(cid:48) ⊗c x) (scalar associativity) and |r| ⊗c x/(cid:107)r ⊗c x(cid:107) = x/(cid:107)x(cid:107) (scaling property).

c , gc) is given by4

Distance.
Euclidean one, with conformal factor λc
(Dn

If one deﬁnes the generalized hyperbolic metric tensor gc as the metric conformal to the
x := 2/(1 − c(cid:107)x(cid:107)2), then the induced distance function on
√

c) tanh−1 (cid:0)√
Again, observe that limc→0 dc(x, y) = 2(cid:107)x − y(cid:107), i.e. we recover Euclidean geometry in the limit5.
Moreover, for c = 1 we recover dD of Eq. (4).

c(cid:107) − x ⊕c y(cid:107)(cid:1) .

dc(x, y) = (2/

(8)

Hyperbolic trigonometry. Similarly as in the Euclidean space, one can deﬁne the notions of
hyperbolic angles or gyroangles (when using the ⊕c), as well as hyperbolic law of sines in the
generalized Poincaré ball (Dn

c , gc). We make use of these notions in our proofs. See Appendix A.

2.4 Connecting Gyrovector spaces and Riemannian geometry of the Poincaré ball

In this subsection, we present how geodesics in the Poincaré ball model are usually described with
Möbius operations, and push one step further the existing connection between gyrovector spaces and
the Poincaré ball by ﬁnding new identities involving the exponential map, and parallel transport.

In particular, these ﬁndings provide us with a simpler formulation of Möbius scalar multiplication,
yielding a natural deﬁnition of matrix-vector multiplication in the Poincaré ball.

Riemannian gyroline element. The Riemannian gyroline element is deﬁned for an inﬁnitesimal
dx as ds := (x + dx) (cid:9)c x, and its size is given by [26, section 3.7]:

(cid:107)ds(cid:107) = (cid:107)(x + dx) (cid:9)c x(cid:107) = (cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2).

(9)

What is remarkable is that it turns out to be identical, up to a scaling factor of 2, to the usual line
element 2(cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2) of the Riemannian manifold (Dn

c , gc).

Geodesics. The geodesic connecting points x, y ∈ Dn

c is shown in [2, 26] to be given by:

γx→y(t) := x ⊕c (−x ⊕c y) ⊗c t, with γx→y : R → Dn

c s.t. γx→y(0) = x and γx→y(1) = y.

Note that when c goes to 0, geodesics become straight-lines, recovering Euclidean geometry. In the
remainder of this subsection, we connect the gyrospace framework with Riemannian geometry.
Lemma 1. For any x ∈ Dn and v ∈ TxDn
c s.t. gc
x with direction v is given by:

x(v, v) = 1, the unit-speed geodesic starting from

γx,v(t) = x ⊕c

tanh

(cid:18)

(cid:18)√

(cid:19) v
√

c

t
2

c(cid:107)v(cid:107)

(cid:19)

, where γx,v : R → Dn s.t. γx,v(0) = x and ˙γx,v(0) = v.

(10)

(11)

Proof. One can use Eq. (10) and reparametrize it to unit-speed using Eq. (8). Alternatively, direct
computation and identiﬁcation with the formula in [11, Thm. 1] would give the same result. Using
Eq. (8) and Eq. (11), one can sanity-check that dc(γ(0), γ(t)) = t, ∀t ∈ [0, 1].

3⊗c has priority over ⊕c in the sense that a ⊗c b ⊕c c := (a ⊗c b) ⊕c c and a ⊕c b ⊗c c := a ⊕c (b ⊗c c).
4The notation −x ⊕c y should always be read as (−x) ⊕c y and not −(x ⊕c y).
5The factor 2 comes from the conformal factor λx = 2/(1 − (cid:107)x(cid:107)2), which is a convention setting the

curvature to −1.

4

Exponential and logarithmic maps. The following lemma gives the closed-form derivation of
exponential and logarithmic maps.
Lemma 2. For any point x ∈ Dn
map logc
c → TxDn
(cid:18)

c are given for v (cid:54)= 0 and y (cid:54)= x by:
(cid:18)√

c , the exponential map expc

c and the logarithmic

x : TxDn

c → Dn

x : Dn

√

(cid:19)

, logc

x(y) =

√

tanh−1(

c(cid:107) − x ⊕c y(cid:107))

expc

x(v) = x ⊕c

tanh

c

λc
x(cid:107)v(cid:107)
2

(cid:19) v
√

c(cid:107)v(cid:107)

2
cλc
x

−x ⊕c y
(cid:107) − x ⊕c y(cid:107)
(12)

.

Proof. Following the proof of [11, Cor. 1.1], one gets expc
gives the formula for expc

x. Algebraic check of the identity logc

x(v) = γx,
x(expc

v

x(cid:107)v(cid:107) (λc
λc

x(cid:107)v(cid:107)). Using Eq. (11)

x(v)) = v concludes.

The above maps have more appealing forms when x = 0, namely for v ∈ T0Dn

c \ {0}, y ∈ Dn

c \ {0}:

expc

0(v) = tanh(

c(cid:107)v(cid:107))

√

, logc

0(y) = tanh−1(

c(cid:107)y(cid:107))

√

(13)

√

y
c(cid:107)y(cid:107)

.

√

v
c(cid:107)v(cid:107)

Moreover, we still recover Euclidean geometry in the limit c → 0, as limc→0 expc
Euclidean exponential map, and limc→0 logc

x(y) = y − x is the Euclidean logarithmic map.

x(v) = x + v is the

Möbius scalar multiplication using exponential and logarithmic maps. We studied the expo-
nential and logarithmic maps in order to gain a better understanding of the Möbius scalar multiplica-
tion (Eq. (7)). We found the following:
Lemma 3. The quantity r ⊗ x can actually be obtained by projecting x in the tangent space at 0
with the logarithmic map, multiplying this projection by the scalar r in T0Dn
c , and then projecting it
back on the manifold with the exponential map:
0(r logc

∀r ∈ R, x ∈ Dn
c .

r ⊗c x = expc

0(x)),

(14)

In addition, we recover the well-known relation between geodesics connecting two points and the
exponential map:

γx→y(t) = x ⊕c (−x ⊕c y) ⊗c t = expc

x(t logc

x(y)),

t ∈ [0, 1].

(15)

This last result enables us to generalize scalar multiplication in order to deﬁne matrix-vector multipli-
cation between Poincaré balls, one of the essential building blocks of hyperbolic neural networks.

Parallel transport. Finally, we connect parallel transport (from T0Dn
the following theorem, which we prove in appendix B.
Theorem 4. In the manifold (Dn
vector v ∈ T0Dn

c to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a
c is given by the following isometry:
λc
0
λc
x

x(x ⊕c expc

0(v)) =

0→x(v) = logc
P c

(16)

v.

c ) to gyrovector spaces with

As we’ll see later, this result is crucial in order to deﬁne and optimize parameters shared between
different tangent spaces, such as biases in hyperbolic neural layers or parameters of hyperbolic MLR.

3 Hyperbolic Neural Networks

Neural networks can be seen as being made of compositions of basic operations, such as linear
maps, bias translations, pointwise non-linearities and a ﬁnal sigmoid or softmax layer. We ﬁrst
explain how to construct a softmax layer for logits lying in a Poincaré ball. Then, we explain how
to transform a mapping between two Euclidean spaces as one between Poincaré balls, yielding
matrix-vector multiplication and pointwise non-linearities in the Poincaré ball. Finally, we present
possible adaptations of various recurrent neural networks to the hyperbolic domain.

5

3.1 Hyperbolic multiclass logistic regression

In order to perform multi-class classiﬁcation on the Poincaré ball, one needs to generalize multinomial
logistic regression (MLR) − also called softmax regression − to the Poincaré ball.

Reformulating Euclidean MLR. Let’s ﬁrst reformulate Euclidean MLR from the perspective of
distances to margin hyperplanes, as in [19, Section 5]. This will allow us to easily generalize it.

Given K classes, one learns a margin hyperplane for each such class using softmax probabilities:

∀k ∈ {1, ..., K},

p(y = k|x) ∝ exp (((cid:104)ak, x(cid:105) − bk)) , where bk ∈ R, x, ak ∈ Rn.

(17)

Note that any afﬁne hyperplane in Rn can be written with a normal vector a and a scalar shift b:

Ha,b = {x ∈ Rn : (cid:104)a, x(cid:105) − b = 0}, where a ∈ Rn \ {0}, and b ∈ R.

(18)

As in [19, Section 5], we note that (cid:104)a, x(cid:105) − b = sign((cid:104)a, x(cid:105) − b)(cid:107)a(cid:107)d(x, Ha,b). Using Eq. (17):

p(y = k|x) ∝ exp(sign((cid:104)ak, x(cid:105) − bk)(cid:107)ak(cid:107)d(x, Hak,bk )), bk ∈ R, x, ak ∈ Rn.

(19)

As it is not immediately obvious how to generalize the Euclidean hyperplane of Eq. (18) to other
spaces such as the Poincaré ball, we reformulate it as follows:

˜Ha,p = {x ∈ Rn : (cid:104)−p + x, a(cid:105) = 0} = p + {a}⊥, where p ∈ Rn, a ∈ Rn \ {0}.

(20)

This new deﬁnition relates to the previous one as ˜Ha,p = Ha,(cid:104)a,p(cid:105). Rewriting Eq. (19) with b = (cid:104)a, p(cid:105):
p(y = k|x) ∝ exp(sign((cid:104)−pk + x, ak(cid:105))(cid:107)ak(cid:107)d(x, ˜Hak,pk )), with pk, x, ak ∈ Rn.

(21)

It is now natural to adapt the previous deﬁnition to the hyperbolic setting by replacing + by ⊕c:
Deﬁnition 3.1 (Poincaré hyperplanes). For p ∈ Dn
p(z, a) = 0} = {z ∈ TpDn
gc

c : (cid:104)z, a(cid:105) = 0}. Then, we deﬁne Poincaré hyperplanes as

c \ {0}, let {a}⊥ := {z ∈ TpDn
c :

c , a ∈ TpDn

˜H c

a,p := {x ∈ Dn

c : (cid:104)logc

p(x), a(cid:105)p = 0} = expc

p({a}⊥) = {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}.

(22)

The last equality is shown appendix C. ˜H c
all geodesics in Dn
hypergyroplanes, see [27, deﬁnition 5.8]. A 3D hyperplane example is depicted in Fig. 1.

a,p can also be described as the union of images of
c orthogonal to a and containing p. Notice that our deﬁnition matches that of

Next, we need the following theorem, proved in appendix D:
Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(23)

Final formula for MLR in the Poincaré ball. Putting together Eq. (21) and Thm. 5, we get the
hyperbolic MLR formulation. Given K classes and k ∈ {1, . . . , K}, pk ∈ Dn
c \ {0}:

c , ak ∈ Tpk

Dn

p(y = k|x) ∝ exp(sign((cid:104)−pk ⊕c x, ak(cid:105))

gc
pk

(ak, ak)dc(x, ˜H c

)),

ak,pk

∀x ∈ Dn
c ,

(24)

or, equivalently

p(y = k|x) ∝ exp

(cid:18) λc
pk

(cid:107)ak(cid:107)
√
c

sinh−1

(cid:18)

√
2

c(cid:104)−pk ⊕c x, ak(cid:105)

(cid:19)(cid:19)

(1 − c(cid:107) − pk ⊕c x(cid:107)2)(cid:107)ak(cid:107)

,

∀x ∈ Dn
c .

(25)

this goes to p(y = k|x) ∝ exp(4(cid:104)−pk + x, ak(cid:105)) =

Notice that when c goes to zero,
exp((λ0
pk

)2(cid:104)−pk + x, ak(cid:105)) = exp((cid:104)−pk + x, ak(cid:105)0), recovering the usual Euclidean softmax.
However, at this point it is unclear how to perform optimization over ak, since it lives in Tpk
hence depends on pk. The solution is that one should write ak = P c
)a(cid:48)
k ∈ T0Dn
a(cid:48)

c = Rn, and optimize a(cid:48)

k as a Euclidean parameter.

k) = (λc

0/λc
pk

0→pk

(a(cid:48)

Dn
c and
k, where

1
√
c

(cid:113)

6

3.2 Hyperbolic feed-forward layers

In order to deﬁne hyperbolic neural networks, it is crucial to de-
ﬁne a canonically simple parametric family of transformations,
playing the role of linear mappings in usual Euclidean neural
networks, and to know how to apply pointwise non-linearities.
Inspiring ourselves from our reformulation of Möbius scalar
multiplication in Eq. (14), we deﬁne:
Deﬁnition 3.2 (Möbius version). For f : Rn → Rm, we deﬁne
the Möbius version of f as the map from Dn

c to Dm

c by:

f ⊗c(x) := expc

0(f (logc

0(x))),

(26)

where expc

0 : T0m

Dm

c → Dm

c and logc

0 : Dn

c → T0n

Dn
c .

Figure 1: An example of a hyper-
bolic hyperplane in D3
1 plotted us-
ing sampling. The red point is p.
The shown normal axis to the hy-
perplane through p is parallel to a.

Note that similarly as for other Möbius operations, we recover
the Euclidean mapping in the limit c → 0 if f is continuous, as limc→0 f ⊗c(x) = f (x). This
deﬁnition satisﬁes a few desirable properties too, such as: (f ◦ g)⊗c = f ⊗c ◦ g⊗c for f : Rm → Rl
and g : Rn → Rm (morphism property), and f ⊗c(x)/(cid:107)f ⊗c(x)(cid:107) = f (x)/(cid:107)f (x)(cid:107) for f (x) (cid:54)= 0
(direction preserving). It is then straight-forward to prove the following result:
Lemma 6 (Möbius matrix-vector multiplication). If M : Rn → Rm is a linear map, which we
identify with its matrix representation, then ∀x ∈ Dn

c , if M x (cid:54)= 0 we have

M ⊗c(x) = (1/

c) tanh

√

(cid:18) (cid:107)M x(cid:107)
(cid:107)x(cid:107)

√

tanh−1(

c(cid:107)x(cid:107))

(cid:19) M x
(cid:107)M x(cid:107)

,

(27)

and M ⊗c(x) = 0 if M x = 0. Moreover, if we deﬁne the Möbius matrix-vector multiplication of
M ∈ Mm,n(R) and x ∈ Dn
c by M ⊗c x := M ⊗c(x), then we have (M M (cid:48)) ⊗c x = M ⊗c (M (cid:48) ⊗c x)
for M ∈ Ml,m(R) and M (cid:48) ∈ Mm,n(R) (matrix associativity), (rM ) ⊗c x = r ⊗c (M ⊗c x) for
r ∈ R and M ∈ Mm,n(R) (scalar-matrix associativity) and M ⊗c x = M x for all M ∈ On(R)
(rotations are preserved).

Pointwise non-linearity.
ϕ⊗c can be applied to elements of the Poincaré ball.

If ϕ : Rn → Rn is a pointwise non-linearity, then its Möbius version

Bias translation. The generalization of a translation in the Poincaré ball is naturally given by
moving along geodesics. But should we use the Möbius sum x ⊕c b with a hyperbolic bias b or the
x(b(cid:48)) with a Euclidean bias b(cid:48)? These views are uniﬁed with parallel transport
exponential map expc
c by a bias b ∈ Dn
(see Thm 4). Möbius translation of a point x ∈ Dn
(cid:18) λc
0
λc
x

c is given by
(cid:19)

x ← x ⊕c b = expc

0(b))) = expc
x

0→x(logc

x(P c

logc

0(b)

(28)

.

We recover Euclidean translations in the limit c → 0. Note that bias translations play a particular
Indeed, consider multiple layers of the form fk(x) = ϕk(Mkx), each of
role in this model.
which having Möbius version f ⊗c
k (Mk ⊗c x). Then their composition can be re-written
f ⊗c
k ◦ · · · ◦ f ⊗c
1 = expc
0. This means that these operations can essentially be
performed in Euclidean space. Therefore, it is the interposition between those with the bias translation
of Eq. (28) which differentiates this model from its Euclidean counterpart.

k (x) = ϕ⊗c
0 ◦fk ◦ · · · ◦ f1 ◦ logc

If a vector x ∈ Rn+p is the (vertical) concatenation
Concatenation of multiple input vectors.
of two vectors x1 ∈ Rn, x2 ∈ Rp, and M ∈ Mm,n+p(R) can be written as the (horizontal)
concatenation of two matrices M1 ∈ Mm,n(R) and M2 ∈ Mm,p(R), then M x = M1x1 + M2x2.
We generalize this to hyperbolic spaces: if we are given x1 ∈ Dn
c ×Dp
c ,
and M, M1, M2 as before, then we deﬁne M ⊗c x := M1 ⊗c x1 ⊕c M2 ⊗c x2. Note that when c goes
to zero, we recover the Euclidean formulation, as limc→0 M ⊗c x = limc→0 M1 ⊗c x1 ⊕c M2 ⊗c x2 =
M1x1 + M2x2 = M x. Moreover, hyperbolic vectors x ∈ Dn
c can also be "concatenated" with real
features y ∈ R by doing: M ⊗c x ⊕c y ⊗c b with learnable b ∈ Dm

c , x = (x1 x2)T ∈ Dn

c and M ∈ Mm,n(R).

c , x2 ∈ Dp

7

3.3 Hyperbolic RNN

Naive RNN. A simple RNN can be deﬁned by ht+1 = ϕ(W ht + U xt + b) where ϕ is a pointwise
non-linearity, typically tanh, sigmoid, ReLU, etc. This formula can be naturally generalized to the
hyperbolic space as follows. For parameters W ∈ Mm,n(R), U ∈ Mm,d(R), b ∈ Dm
c , we deﬁne:

ht+1 = ϕ⊗c (W ⊗c ht ⊕c U ⊗c xt ⊕c b),

ht ∈ Dn

c , xt ∈ Dd
c .

(29)

Note that if inputs xt’s are Euclidean, one can write ˜xt := expc
expc

(U xt)) = W ⊗c ht ⊕c expc

(P c

W ⊗cht

0→W ⊗cht

0(U xt) = W ⊗c ht ⊕c U ⊗c ˜xt.

0(xt) and use the above formula, since

GRU architecture. One can also adapt the GRU architecture:
rt = σ(W rht−1 + U rxt + br),
zt = σ(W zht−1 + U zxt + bz),
˜ht = ϕ(W (rt (cid:12) ht−1) + U xt + b), ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht,

(30)

where (cid:12) denotes pointwise product. First, how should we adapt the pointwise multiplication by a
scaling gate? Note that the deﬁnition of the Möbius version (see Eq. (26)) can be naturally extended
to maps f : Rn × Rp → Rm as f ⊗c : (h, h(cid:48)) ∈ Dn
0(h(cid:48)))). In
c (cid:55)→ expc
0(h(cid:48))) =
particular, choosing f (h, h(cid:48)) := σ(h) (cid:12) h(cid:48) yields6 f ⊗c(h, h(cid:48)) = expc
diag(σ(logc

0(h))) ⊗c h(cid:48). Hence we adapt rt (cid:12) ht−1 to diag(rt) ⊗c ht−1 and the reset gate rt to:
0(W r ⊗c ht−1 ⊕c U r ⊗c xt ⊕c br),

0(h), logc
0(h)) (cid:12) logc

0(f (logc
0(σ(logc

rt = σ logc

c × Dp

(31)

and similarly for the update gate zt. Note that as the argument of σ in the above is unbounded, rt and
zt can a priori take values onto the full range (0, 1). Now the intermediate hidden state becomes:
˜ht = ϕ⊗c ((W diag(rt)) ⊗c ht−1 ⊕c U ⊗c xt ⊕ b),

(32)

where Möbius matrix associativity simpliﬁes W ⊗c (diag(rt) ⊗c ht−1) into (W diag(rt)) ⊗c ht−1.
Finally, we propose to adapt the update-gate equation as

ht = ht−1 ⊕c diag(zt) ⊗c (−ht−1 ⊕c

˜ht).

(33)

Note that when c goes to zero, one recovers the usual GRU. Moreover, if zt = 0 or zt = 1, then ht
becomes ht−1 or ˜ht respectively, similarly as in the usual GRU. This adaptation was obtained by
adapting [24]: in this work, the authors re-derive the update-gate mechanism from a ﬁrst principle
called time-warping invariance. We adapted their derivation to the hyperbolic setting by using the
notion of gyroderivative [4] and proving a gyro-chain-rule (see appendix E).

4 Experiments

SNLI task and dataset. We evaluate our method on two tasks. The ﬁrst is natural language
inference, or textual entailment. Given two sentences, a premise (e.g. "Little kids A. and B. are
playing soccer.") and a hypothesis (e.g. "Two children are playing outdoors."), the binary classiﬁcation
task is to predict whether the second sentence can be inferred from the ﬁrst one. This deﬁnes a partial
order in the sentence space. We test hyperbolic networks on the biggest real dataset for this task,
SNLI [7]. It consists of 570K training, 10K validation and 10K test sentence pairs. Following [28],
we merge the "contradiction" and "neutral" classes into a single class of negative sentence pairs, while
the "entailment" class gives the positive pairs.

PREFIX task and datasets. We conjecture that the improvements of hyperbolic neural networks
are more signiﬁcant when the underlying data structure is closer to a tree. To test this, we design a
proof-of-concept task of detection of noisy preﬁxes, i.e. given two sentences, one has to decide if the
second sentence is a noisy preﬁx of the ﬁrst, or a random sentence. We thus build synthetic datasets
PREFIX-Z% (for Z being 10, 30 or 50) as follows: for each random ﬁrst sentence of random length
at most 20 and one random preﬁx of it, a second positive sentence is generated by randomly replacing
Z% of the words of the preﬁx, and a second negative sentence of same length is randomly generated.
Word vocabulary size is 100, and we generate 500K training, 10K validation and 10K test pairs.

6If x has n coordinates, then diag(x) denotes the diagonal matrix of size n with xi’s on its diagonal.

8

Models architecture. Our neural network layers can be used in a plug-n-play manner exactly like
standard Euclidean layers. They can also be combined with Euclidean layers. However, optimization
w.r.t. hyperbolic parameters is different (see below) and based on Riemannian gradients which
are just rescaled Euclidean gradients when working in the conformal Poincaré model [21]. Thus,
back-propagation can be applied in the standard way.

In our setting, we embed the two sentences using two distinct hyperbolic RNNs or GRUs. The
sentence embeddings are then fed together with their squared distance (hyperbolic or Euclidean,
depending on their geometry) to a FFNN (Euclidean or hyperbolic, see Sec. 3.2) which is further
fed to an MLR (Euclidean or hyperbolic, see Sec. 3.1) that gives probabilities of the two classes
(entailment vs neutral). We use cross-entropy loss on top. Note that hyperbolic and Euclidean layers
can be mixed, e.g. the full network can be hyperbolic and only the last layer be Euclidean, in which
case one has to use log0 and exp0 functions to move between the two manifolds in a correct manner
as explained for Eq. 26.

Optimization. Our models have both Euclidean (e.g. weight matrices in both Euclidean and
hyperbolic FFNNs, RNNs or GRUs) and hyperbolic parameters (e.g. word embeddings or biases for
the hyperbolic layers). We optimize the Euclidean parameters with Adam [16] (learning rate 0.001).
Hyperbolic parameters cannot be updated with an equivalent method that keeps track of gradient
history due to the absence of a Riemannian Adam. Thus, they are optimized using full Riemannian
stochastic gradient descent (RSGD) [5, 11]. We also experiment with projected RSGD [21], but
optimization was sometimes less stable. We use a different constant learning rate for word embeddings
(0.1) and other hyperbolic weights (0.01) because words are updated less frequently.

Numerical errors. Gradients of the basic operations deﬁned above (e.g. ⊕c, exponential map) are
c(cid:107)x(cid:107) = 1. Thus, we
not deﬁned when the hyperbolic argument vectors are on the ball border, i.e.
always project results of these operations in the ball of radius 1 − (cid:15), where (cid:15) = 10−5. Numerical
errors also appear when hyperbolic vectors get closer to 0, thus we perturb them with an (cid:15)(cid:48) = 10−15
before they are used in any of the above operations. Finally, arguments of the tanh function are
clipped between ±15 to avoid numerical errors, while arguments of tanh−1 are clipped to at most
1 − 10−5.

√

Hyperparameters. For all methods, baselines and datasets, we use c = 1, word and hidden state
embedding dimension of 5 (we focus on the low dimensional setting that was shown to already
be effective [21]), batch size of 64. We ran all methods for a ﬁxed number of 30 epochs. For all
models, we experiment with both identity (no non-linearity) or tanh non-linearity in the RNN/GRU
cell, as well as identity or ReLU after the FFNN layer and before MLR. As expected, for the fully
Euclidean models, tanh and ReLU respectively surpassed the identity variant by a large margin. We
only report the best Euclidean results. Interestingly, for the hyperbolic models, using only identity for
both non-linearities works slightly better and this is likely due to two facts: i) our hyperbolic layers
already contain non-linearities by their nature, ii) tanh is limiting the output domain of the sentence
embeddings, but the hyperbolic speciﬁc geometry is more pronounced at the ball border, i.e. at the
hyperbolic "inﬁnity", compared to the center of the ball.

For the results shown in Tab. 1, we run each model (baseline or ours) exactly 3 times and report the
test result corresponding to the best validation result from these 3 runs. We do this because the highly
non-convex spectrum of hyperbolic neural networks sometimes results in convergence to poor local
minima, suggesting that initialization is very important.

Results. Results are shown in Tab. 1. Note that the fully Euclidean baseline models might have
an advantage over hyperbolic baselines because more sophisticated optimization algorithms such
as Adam do not have a hyperbolic analogue at the moment. We ﬁrst observe that all GRU models
overpass their RNN variants. Hyperbolic RNNs and GRUs have the most signiﬁcant improvement
over their Euclidean variants when the underlying data structure is more tree-like, e.g. for PREFIX-
10% − for which the tree relation between sentences and their preﬁxes is more prominent − we
reduce the error by a factor of 3.35 for hyperbolic vs Euclidean RNN, and by a factor of 1.5 for
hyperbolic vs Euclidean GRU. As soon as the underlying structure diverges more and more from
a tree, the accuracy gap decreases − for example, for PREFIX-50% the noise heavily affects the
representational power of hyperbolic networks. Also, note that on SNLI our methods perform
similarly as with their Euclidean variants. Moreover, hyperbolic and Euclidean MLR are on par when

9

SNLI

PREFIX-10% PREFIX-30% PREFIX-50%

FULLY EUCLIDEAN RNN
HYPERBOLIC RNN+FFNN, EUCL MLR
FULLY HYPERBOLIC RNN
FULLY EUCLIDEAN GRU
HYPERBOLIC GRU+FFNN, EUCL MLR
FULLY HYPERBOLIC GRU

79.34 %
79.18 %
78.21 %
81.52 %
79.76 %
81.19 %

89.62 %
96.36 %
96.91 %
95.96 %
97.36 %
97.14 %

81.71 %
87.83 %
87.25 %
86.47 %
88.47 %
88.26 %

72.10 %
76.50 %
62.94 %
75.04 %
76.87 %
76.44 %

Table 1: Test accuracies for various models and four datasets. "Eucl" denotes Euclidean. All word
and sentence embeddings have dimension 5. We highlight in bold the best baseline (or baselines, if
the difference is less than 0.5%).

used in conjunction with hyperbolic sentence embeddings, suggesting further empirical investigation
is needed for this direction (see below).

We also observe that, in the hyperbolic setting, accuracy tends to increase when sentence embeddings
start increasing, and gets better as their norms converge towards 1 (the ball border for c = 1). Unlike
in the Euclidean case, this behavior does happen only after a few epochs and suggests that the model
should ﬁrst adjust the angular layout in order to disentangle the representations, before increasing their
norms to fully exploit the strong clustering property of the hyperbolic geometry. Similar behavior
was observed in the context of embedding trees by [21]. Details in appendix F.

MLR classiﬁcation experiments.
For the sentence entailment classi-
ﬁcation task we do not see a clear
advantage of hyperbolic MLR com-
pared to its Euclidean variant. A pos-
sible reason is that, when trained end-
to-end, the model might decide to
place positive and negative embed-
dings in a manner that is already well
separated with a classic MLR. As a
consequence, we further investigate
MLR for the task of subtree classiﬁ-
cation. Using an open source imple-
mentation7 of [21], we pre-trained
Poincaré embeddings of the Word-
Net noun hierarchy (82,115 nodes).
We then choose one node in this tree
(see Table 2) and classify all other
nodes (solely based on their embed-
dings) as being part of the subtree
rooted at this node. All nodes in such a subtree are divided into positive training nodes (80%) and
positive test nodes (20%). The same splitting procedure is applied for the remaining WordNet nodes
that are divided into a negative training and negative test set respectively. Three variants of MLR
are then trained on top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation
task: hyperbolic MLR, Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean
MLR applied after mapping all embeddings in the tangent space at 0 using the log0 map. We use
different embedding dimensions : 2, 3, 5 and 10. For the hyperbolic MLR, we use full Riemannian
SGD with a learning rate of 0.001. For the two Euclidean models we use ADAM optimizer and the
same learning rate. During training, we always sample the same number of negative and positive
nodes in each minibatch of size 16; thus positive nodes are frequently resampled. All methods are
trained for 30 epochs and the ﬁnal F1 score is reported (no hyperparameters to validate are used, thus
we do not require a validation set). This procedure is repeated for four subtrees of different sizes.

Figure 2: Hyperbolic (left) vs Direct Euclidean (right) binary
MLR used to classify nodes as being part in the GROUP.N.01
subtree of the WordNet noun hierarchy solely based on their
Poincaré embeddings. The positive points (from the subtree)
are in blue, the negative points (the rest) are in red and the
trained positive separation hyperplane is depicted in green.

Quantitative results are presented in Table 2. We can see that the hyperbolic MLR overpasses
its Euclidean variants in almost all settings, sometimes by a large margin. Moreover, to provide

7https://github.com/dalab/hyperbolic_cones

10

WORDNET
SUBTREE

ANIMAL.N.01
3218 / 798

GROUP.N.01
6649 / 1727

WORKER.N.01
861 / 254

MAMMAL.N.01
953 / 228

MODEL

D = 2

D = 3

D = 5

D = 10

HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL

47.43 ± 1.07%
41.69 ± 0.19%
38.89 ± 0.01%
81.72 ± 0.17%
61.13 ± 0.42%
60.75 ± 0.24%
12.68 ± 0.82%
10.86 ± 0.01%
9.04 ± 0.06%
32.01 ± 17.14%
15.58 ± 0.04%
13.10 ± 0.13%

91.92 ± 0.61%
68.43 ± 3.90%
62.57 ± 0.61%
89.87 ± 2.73%
63.56 ± 1.22%
61.98 ± 0.57%
24.09 ± 1.49%
22.39 ± 0.04%
22.57 ± 0.20%
87.54 ± 4.55%
44.68 ± 1.87%
44.89 ± 1.18%

98.07 ± 0.55%
95.59 ± 1.18%
89.21 ± 1.34%
87.89 ± 0.80%
67.82 ± 0.81%
67.92 ± 0.74%
55.46 ± 5.49%
35.23 ± 3.16%
26.47 ± 0.78%
88.73 ± 3.22%
59.35 ± 1.31%
52.51 ± 0.85%

99.26 ± 0.59%
99.36 ± 0.18%
98.27 ± 0.70%
91.91 ± 3.07%
91.38 ± 1.19%
91.41 ± 0.18%
66.83 ± 11.38%
47.29 ± 3.93%
36.66 ± 2.74%
91.37 ± 6.09%
77.76 ± 5.08%
56.11 ± 2.21%

Table 2: Test F1 classiﬁcation scores for four different subtrees of WordNet noun tree. All nodes
in such a subtree are divided into positive training nodes (80%) and positive test nodes (20%);
these counts are shown below each subtree root. The same splitting procedure is applied for the
remaining nodes to obtain negative training and test sets. Three variants of MLR are then trained on
top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation task: hyperbolic MLR,
Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean MLR applied after
mapping all embeddings in the tangent space at 0 using the log0 map. 95% conﬁdence intervals for 3
different runs are shown for each method and each different embedding dimension (2, 3, 5 or 10).

further understanding, we plot the 2-dimensional embeddings and the trained separation hyperplanes
(geodesics in this case) in Figure 2. We can see that respecting the hyperbolic geometry is very
important for a quality classiﬁcation model.

5 Conclusion

We showed how classic Euclidean deep learning tools such as MLR, FFNNs, RNNs or GRUs can be
generalized in a principled manner to all spaces of constant negative curvature combining Riemannian
geometry with the elegant theory of gyrovector spaces. Empirically we found that our models
outperform or are on par with corresponding Euclidean architectures on sequential data with implicit
hierarchical structure. We hope to trigger exciting future research related to better understanding
of the hyperbolic non-convexity spectrum and development of other non-Euclidean deep learning
methods.
Our data and Tensorﬂow [1] code are publicly available8.

Acknowledgements

We thank Igor Petrovski for useful pointers regarding the implementation.

This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement
number 167176. Gary Bécigneul is also funded by the Max Planck ETH Center for Learning
Systems.

References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for
large-scale machine learning. 2016.

[2] Ungar Abraham Albert. Analytic hyperbolic geometry and Albert Einstein’s special theory of

relativity. World scientiﬁc, 2008.

8https://github.com/dalab/hyperbolic_nn

11

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of the International Conference on Learning
Representations (ICLR), 2015.

[4] Graciela S Birman and Abraham A Ungar. The hyperbolic derivative in the poincaré ball model
of hyperbolic geometry. Journal of mathematical analysis and applications, 254(1):321–333,
2001.

[5] S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on

Automatic Control, 58(9):2217–2229, Sept 2013.

[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems (NIPS), pages 2787–2795, 2013.

[7] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 632–642. Association
for Computational Linguistics, 2015.

[8] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,
34(4):18–42, 2017.

[9] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry.

Flavors of geometry, 31:59–115, 1997.

[10] Christopher De Sa, Albert Gu, Christopher Ré, and Frederic Sala. Representation tradeoffs for

hyperbolic embeddings. arXiv preprint arXiv:1804.03329, 2018.

[11] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In Proceedings of the thirty-ﬁfth international conference
on machine learning (ICML), 2018.

[12] Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pages 75–263. Springer, 1987.

[13] Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the

Cambridge Philosophical Society, page 1–17, 2017.

[14] Christopher Hopper and Ben Andrews. The Ricci ﬂow in Riemannian geometry. Springer, 2010.

[15] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1746–1751. Association for Computational Linguistics, 2014.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations (ICLR), 2015.

[17] Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marián Boguná.

Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.

[18] John Lamping, Ramana Rao, and Peter Pirolli. A focus+ context technique based on hyperbolic
geometry for visualizing large hierarchies. In Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 401–408. ACM Press/Addison-Wesley Publishing Co.,
1995.

[19] Guy Lebanon and John Lafferty. Hyperplane margin classiﬁers on the multinomial manifold. In
Proceedings of the international conference on machine learning (ICML), page 66. ACM, 2004.

[20] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective
learning on multi-relational data. In Proceedings of the international conference on machine
learning (ICML), volume 11, pages 809–816, 2011.

12

[21] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical repre-
sentations. In Advances in Neural Information Processing Systems (NIPS), pages 6341–6350,
2017.

[22] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blun-
som. Reasoning about entailment with neural attention. In Proceedings of the International
Conference on Learning Representations (ICLR), 2015.

[23] Michael Spivak. A comprehensive introduction to differential geometry. Publish or perish, 1979.

[24] Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In Proceedings of

the International Conference on Learning Representations (ICLR), 2018.

[25] Abraham A Ungar. Hyperbolic trigonometry and its application in the poincaré ball model of

hyperbolic geometry. Computers & Mathematics with Applications, 41(1-2):135–147, 2001.

[26] Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis

Lectures on Mathematics and Statistics, 1(1):1–194, 2008.

[27] Abraham Albert Ungar. Analytic hyperbolic geometry in n dimensions: An introduction. CRC

Press, 2014.

[28] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. In Proceedings of the International Conference on Learning Representations (ICLR),
2016.

[29] J Vermeer. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic

plane. Topology and its Applications, 152(3):226–242, 2005.

13

A Hyperbolic Trigonometry

Hyperbolic angles. For A, B, C ∈ Dn
c , we denote by ∠A := ∠BAC the angle between the two
geodesics starting from A and ending at B and C respectively. This angle can be deﬁned in two
equivalent ways: i) either using the angle between the initial velocities of the two geodesics as given
by Eq. 5, or ii) using the formula

cos(∠A) =

(cid:28) (−A) ⊕c B
(cid:107)(−A) ⊕c B(cid:107)

,

(−A) ⊕c C
(cid:107)(−A) ⊕c C(cid:107)

(cid:29)

,

In this case, ∠A is also called a gyroangle in the work of [26, section 4].

Hyperbolic law of sines. We state here the hyperbolic law of sines. If for A, B, C ∈ Dn
c , we
denote by ∠B := ∠ABC the angle between the two geodesics starting from B and ending at A and
C respectively, and by ˜c = dc(B, A) the length of the hyperbolic segment BA (and similarly for
others), then we have:

sin(∠A)
√
c˜a)
sinh(

=

sin(∠B)
√
c˜b)
sinh(

=

sin(∠C)
√
c˜c)
sinh(

.

Note that one can also adapt the hyperbolic law of cosines to the hyperbolic space.

B Proof of Theorem 4

Theorem 4.
In the manifold (Dn
to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a vector v ∈ T0Dn
c

c is given by the following isometry:
λc
0
λc
x

0→x(v) = logc
P c

x(x ⊕c expc

0(v)) =

v.

Proof. The geodesic in Dn
v ∈ T0Dn
γ (i.e. X(t) ∈ Tγ(t)Dn

c from 0 to x is given in Eq. (10) by γ(t) = x ⊗c t, for t ∈ [0, 1]. Let
c . Then it is of common knowledge that there exists a unique parallel9 vector ﬁeld X along

c , ∀t ∈ [0, 1]) such that X(0) = v. Let’s deﬁne:
X : t ∈ [0, 1] (cid:55)→ logc

γ(t)(γ(t) ⊕c expc

0(v)) ∈ Tγ(t)Dn
c .

Clearly, X is a vector ﬁeld along γ such that X(0) = v. Now deﬁne
0→x : v ∈ T0Dn
P c

x(x ⊕c expc

0(v)) ∈ TxDn
c .

c (cid:55)→ logc
0→x(v) = λc

c . Since P c

0→x is a linear isometry from T0Dn
v, hence P c
c
0→x(v) = X(1), it is enough to prove that X is parallel in order to guarantee that

From Eq. (12), it is easily seen that P c
to TxDn
c to TxDn
0→x is the parallel transport from T0Dn
P c
c .
Since X is a vector ﬁeld along γ, its covariant derivative can be expressed with the Levi-Civita
connection ∇c associated to gc:

0
λc
x

DX
∂t

= ∇c

˙γ(t)X.

Let’s compute the Levi-Civita connection from its Christoffel symbols. In a local coordinate system,
they can be written as

Γi

jk =

(gc)il(∂jgc

lk + ∂kgc

lj − ∂lgc

jk),

1
2

where superscripts denote the inverse metric tensor and using Einstein’s notations. As gc
at γ(t) ∈ Dn

c this yields:

ij = (λc)2δij,

jk = cλc
Γi

γ(t)(δikγ(t)j + δijγ(t)k − δjkγ(t)i).

9i.e. that DX

∂t = 0 for t ∈ [0, 1], where D

∂t denotes the covariant derivative.

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

14

On the other hand, since X(t) = (λc

∇c

˙γ(t)X = ˙γ(t)i∇c

i X = ˙γ(t)i∇c
i

= vj ˙γ(t)i∇c
i

0/λc

γ(t))v, we have
(cid:32)

(cid:33)

λc
0
λc

γ(t)

v

(cid:32)

λc
0
λc

γ(t)

(cid:33)

ej

.

√

√

Since γ(t) = (1/
Hence there exists K x

c) tanh(t tanh−1(
t ∈ R such that ˙γ(t) = K x

c(cid:107)x(cid:107))) x

(cid:107)x(cid:107) , it is easily seen that ˙γ(t) is colinear to γ(t).
t γ(t). Moreover, we have the following Leibniz rule:
(cid:33)

(cid:32)

(cid:32)

∇c
i

λc
0
λc

γ(t)

(cid:33)

ej

=

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

ej.

Combining these yields

DX
∂t

= K x

t vjγ(t)i

(cid:32)

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

(cid:32)

(cid:33)

(cid:33)

ej

.

Replacing with the Christoffel symbols of ∇c at γ(t) gives

Moreover,

λc
0
λc

γ(t)

λc
0
λc

γ(t)

∇c

i ej =

ijek = 2c[δk
Γk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek.

∂
∂γ(t)i

(cid:32)

(cid:33)

λc
0
λc

γ(t)

ej =

∂
∂γ(t)i

(cid:0)−c(cid:107)γ(t)(cid:107)2(cid:1) ej = −2cγ(t)iej.

Putting together everything, we obtain

DX
∂t

= K x

t vjγ(t)i (cid:0)2c[δk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek − 2cγ(t)iej

(cid:1)

t vjγ(t)i (cid:0)γ(t)jei − δijγ(t)kek
t vj (cid:0)γ(t)jγ(t)iei − γ(t)iδijγ(t)kek
(cid:1)
t vj (cid:0)γ(t)jγ(t)iei − γ(t)jγ(t)kek

(cid:1)

(cid:1)

= 2cK x
= 2cK x
= 2cK x
= 0,

which concludes the proof.

C Proof of Eq. (22)

Proof. Two steps proof:
i) expc

p({a}⊥) ⊆ {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}:

Let z ∈ {a}⊥. From Eq. (12), we have that:

This, together with the left-cancellation law in gyrospaces (see section 2.3), implies that

expc

p(z) = −p ⊕c βz,

for some β ∈ R.

(cid:104)−p ⊕c expc

p(z), a(cid:105) = (cid:104)βz, a(cid:105) = 0

which is what we wanted.

ii) {x ∈ Dn
Let x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0} ⊆ expc
c s.t. (cid:104)−p ⊕c x, a(cid:105) = 0. Then, using Eq. (12), we derive that:
for some β ∈ R,

p(x) = β(−p ⊕c x),

p({a}⊥):

logc

which is orthogonal to a, by assumption. This implies logc

p(x) ∈ {a}⊥, hence x ∈ expc

p({a}⊥).

15

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(54)

D Proof of Theorem 5

Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

1
√
c

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(55)

Proof. We ﬁrst need to prove the following lemma, trivial in the Euclidean space, but not in the
Poincaré ball:
Lemma 7. (Orthogonal projection on a geodesic) Any point in the Poincaré ball has a unique
orthogonal projection on any given geodesic that does not pass through the point. Formally, for all
y ∈ Dn
c and for all geodesics γx→z(·) s.t. y /∈ Im γx→z, there exists an unique w ∈ Im γx→z s.t.
∠(γw→y, γx→z) = π/2.

Proof. We ﬁrst note that any geodesic in Dn
and has two "points at inﬁnity" lying on the ball border (v (cid:54)= 0):

c has the form γ(t) = u ⊕c v ⊗c t as given by Eq. 11,

γ(±∞) = u ⊕c

√

±v
c(cid:107)v(cid:107)

∈ ∂Dn
c .

(56)

Using the notations in the lemma statement, the closed-form of γx→z is given by Eq. (10):

γx→z(t) = x ⊕c (−x ⊕c z) ⊗c t

We denote by x(cid:48), z(cid:48) ∈ ∂Dn
∠ywx(cid:48) is well deﬁned from Eq. (34):

c its points at inﬁnity as described by Eq. (56). Then, the hyperbolic angle

cos(∠(γw→y, γx→z)) = cos(∠ywz(cid:48)) =

(cid:104)−w ⊕c y, −w ⊕c z(cid:48)(cid:105)
(cid:107) − w ⊕c y(cid:107) · (cid:107) − w ⊕c z(cid:48)(cid:107)

.

(57)

We now perform 2 steps for this proof.

i) Existence of w:

The angle function from Eq. (57) is continuous w.r.t t when w = γx→z(t). So we ﬁrst prove existence
of an angle of π/2 by continuously moving w from x(cid:48) to z(cid:48) when t goes from −∞ to ∞, and
observing that cos(∠ywz(cid:48)) goes from −1 to 1 as follows:

cos(∠yx(cid:48)z(cid:48)) = 1 & lim
w→z(cid:48)

cos(∠ywz(cid:48)) = −1.

(58)

The left part of Eq. (58) follows from Eq. (57) and from the fact (easy to show from the deﬁnition
c (which is the case of x(cid:48)). The right part of Eq. (58)
of ⊕c) that a ⊕c b = a, when (cid:107)a(cid:107) = 1/
follows from the fact that ∠ywz(cid:48) = π − ∠ywx(cid:48) (from the conformal property, or from Eq. (34)) and
cos(∠yz(cid:48)x(cid:48)) = 1 (proved as above).
Hence cos(∠ywz(cid:48)) has to pass through 0 when going from −1 to 1, which achieves the proof of
existence.

√

ii) Uniqueness of w:
Assume by contradiction that there are two w and w(cid:48) on γx→z that form angles ∠ywx(cid:48) and ∠yw(cid:48)x(cid:48)
of π/2. Since w, w(cid:48), x(cid:48) are on the same geodesic, we have

π/2 = ∠yw(cid:48)x(cid:48) = ∠yw(cid:48)w = ∠ywx(cid:48) = ∠yw(cid:48)w
So ∆yww(cid:48) has two right angles, but in the Poincaré ball this is impossible.

(59)

Now, we need two more lemmas:
Lemma 8. (Minimizing distance from point to geodesic) The orthogonal projection of a point to
a geodesic (not passing through the point) is minimizing the distance between the point and the
geodesic.

Proof. The proof is similar with the Euclidean case and it’s based on hyperbolic sine law and the fact
that in any right hyperbolic triangle the hypotenuse is strictly longer than any of the other sides.

16

Lemma 9. (Geodesics through p) Let ˜H c
all points on the geodesic γp→w are included in ˜H c

a,p.

a,p be a Poincaré hyperplane. Then, for any w ∈ ˜H c

a,p \ {p},

Proof. γp→w(t) = p ⊕c (−p ⊕c w) ⊗c t. Then, it is easy to check the condition in Eq. (22):

(cid:104)−p ⊕c γp→w(t), a(cid:105) = (cid:104)(−p ⊕c w) ⊗c t, a(cid:105) ∝ (cid:104)−p ⊕c w, a(cid:105) = 0.

(60)

We now turn back to our proof. Let x ∈ Dn
We prove that there is at least one point w∗ ∈ ˜H c

c be an arbitrary point and ˜H c

a,p a Poincaré hyperplane.

a,p that achieves the inﬁmum distance

dc(x, w∗) = inf
w∈ ˜H c

a,p

dc(x, w),

and, moreover, that this distance is the same as the one in the theorem’s statement.
We ﬁrst note that for any point w ∈ ˜H c
and Lemma 9, it is obvious that the projection of x to γp→w will give a strictly lower distance.
Thus, we only consider w ∈ ˜H c
triangle ∆xwp, one gets:

a,p such that ∠xwp = π/2. Applying hyperbolic sine law in the right

a,p, if ∠xwp (cid:54)= π/2, then w (cid:54)= w∗. Indeed, using Lemma 8

dc(x, w) = (1/

c) sinh−1 (cid:0)sinh(

c dc(x, p)) · sin(∠xpw)(cid:1) .

√

√

One of the above quantities does not depend on w:

√

√

sinh(

c dc(x, p)) = sinh(2 tanh−1(

c(cid:107) − p ⊕c x(cid:107))) =

√
2
c(cid:107) − p ⊕c x(cid:107)
1 − c(cid:107) − p ⊕c x(cid:107)2 .

The other quantity is sin(∠xpw) which is minimized when the angle ∠xpw is minimized (be-
cause ∠xpw < π/2 for the hyperbolic right triangle ∆xwp), or, alternatively, when cos(∠xpw) is
maximized. But, we already have from Eq. (34) that:

cos(∠xpw) =

(cid:104)−p ⊕c x, −p ⊕c w(cid:105)
(cid:107) − p ⊕c x(cid:107) · (cid:107) − p ⊕c w(cid:107)

.

To maximize the above, the constraint on the right angle at w can be dropped because cos(∠xpw)
depends only on the geodesic γp→w and not on w itself, and because there is always an orthogonal
projection from any point x to any geodesic as stated by Lemma 7. Thus, it remains to ﬁnd the
maximum of Eq. (64) when w ∈ ˜H c
a,p from Eq. (22), one can easily
prove that

a,p. Using the deﬁnition of ˜H c

Using that fact that logc

p(w)/(cid:107) logc

p(w)(cid:107) = −p ⊕c w/(cid:107) − p ⊕c w(cid:107), we just have to ﬁnd

and we are left with a well known Euclidean problem which is equivalent to ﬁnding the minimum
angle between the vector −p ⊕c x (viewed as Euclidean) and the hyperplane {a}⊥. This angle
is given by the Euclidean orthogonal projection whose sin value is the distance from the vector’s
endpoint to the hyperplane divided by the vector’s length:

{logc

p(w) : w ∈ ˜H c

a,p} = {a}⊥.

max
z∈{a}⊥

(cid:18) (cid:104)−p ⊕c x, z(cid:105)

(cid:107) − p ⊕c x(cid:107) · (cid:107)z(cid:107)

(cid:19)

,

sin(∠xpw∗) =

|(cid:104)−p ⊕c x, a
(cid:107) − p ⊕c x(cid:107)

(cid:107)a(cid:107) (cid:105)|

.

17

It follows that a point w∗ ∈ ˜H c
Eqs. (61),(62),(63) and (67) concludes the proof.

a,p satisfying Eq. (67) exists (but might not be unique). Combining

(61)

(62)

(63)

(64)

(65)

(66)

(67)

(cid:3)

E Derivation of the Hyperbolic GRU Update-gate

In [24], the authors recover the update/forget-gate mechanism of a GRU/LSTM by requiring that the
class of neural networks given by the chosen architecture be invariant to time-warpings. The idea is
the following.

Recovering the update-gate from time-warping. A naive RNN is given by the equation

h(t + 1) = ϕ(W h(t) + U x(t) + b)

Let’s drop the bias b to simplify notations. If h is seen as a differentiable function of time, then a
ﬁrst-order Taylor development gives h(t + δt) ≈ h(t) + δt dh
dt (t) for small δt. Combining this for
δt = 1 with the naive RNN equation, one gets

dh
dt

dα
dt

(t) = ϕ(W h(t) + U x(t)) − h(t).

As this is written for any t, one can replace it by t ← α(t) where α is a (smooth) increasing function
of t called the time-warping. Denoting by ˜h(t) := h(α(t)) and ˜x(t) := x(α(t)), using the chain rule
d˜h
dt (t) = dα

dt (α(t)), one gets

dt (t) dh

d˜h
dt

dα
dt

(t) =

(t)ϕ(W ˜h(t) + U ˜x(t)) −

(t)˜h(t).

(70)

Removing the tildas to simplify notations, discretizing back with dh

dt (t) ≈ h(t + 1) − h(t) yields

h(t + 1) =

(t)ϕ(W h(t) + U x(t)) +

1 −

(t)

h(t).

(71)

dα
dt

(cid:18)

(cid:19)

dα
dt

Requiring that our class of neural networks be invariant to time-warpings means that this class should
contain RNNs deﬁned by Eq. (71), i.e. that dα
dt (t) can be learned. As this is a positive quantity, we
can parametrize it as z(t) = σ(W zh(t) + U zx(t)), recovering the forget-gate equation:

h(t + 1) = z(t)ϕ(W h(t) + U x(t)) + (1 − z(t))h(t).

Adapting this idea to hyperbolic RNNs. The gyroderivative [4] of a map h : R → Dn
as

c is deﬁned

dh
dt

(t) = lim
δt→0

1
δt

⊗c (−h(t) ⊕c h(t + δt)).

Using Möbius scalar associativity and the left-cancellation law leads us to

h(t + δt) ≈ h(t) ⊕c δt ⊗c

(t),

dh
dt

for small δt. Combining this with the equation of a simple hyperbolic RNN of Eq. (29) with δt = 1,
one gets

dh
dt

(t) = −h(t) ⊕c ϕ⊗c(W ⊗c h(t) ⊕c U ⊗c x(t)).

For the next step, we need the following lemma:
Lemma 10 (Gyro-chain-rule). For α : R → R differentiable and h : R → Dn
gyro-derivative, if ˜h := h ◦ α, then we have

c with a well-deﬁned

(68)

(69)

(72)

(73)

(74)

(75)

(76)

where dα

dt (t) denotes the usual derivative.

d˜h
dt

(t) =

(t) ⊗c

(α(t)),

dα
dt

dh
dt

18

(77)

(78)

(79)

(80)

(81)

Proof.

d˜h
dt

(t) = lim
δt→0

1
δt
1
δt

⊗c [−˜h(t) ⊕c

˜h(t + δt)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

= lim
δt→0

= lim
δt→0

= lim
δt→0

= lim
u→0
dα
dt

=

α(cid:48)(t) + O(δt)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
u

(t) ⊗c

(α(t))

dh
dt

⊗c [−h(α(t)) ⊕c h(α(t) + u)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

(Möbius scalar associativity) (82)

where we set u = δt(α(cid:48)(t) + O(δt)), with u → 0 when δt → 0, which concludes.

Using lemma 10 and Eq. (75), with similar notations as in Eq. (70) we have

d˜h
dt

dα
dt

(t) =

(t) ⊗c (−˜h(t) ⊕c ϕ⊗c(W ⊗c

˜h(t) ⊕c U ⊗c ˜x(t))).

(83)

Finally, discretizing back with Eq. (74), using the left-cancellation law and dropping the tildas yields

h(t + 1) = h(t) ⊕c

(t) ⊗c (−h(t) ⊕c ϕ⊗c (W ⊗c h(t) ⊕c U ⊗c x(t))).

(84)

dα
dt

Since α is a time-warping, by deﬁnition its derivative is positive and one can choose to parametrize
it with an update-gate zt (a scalar) deﬁned with a sigmoid. Generalizing this scalar scaling by the
Möbius version of the pointwise scaling (cid:12) yields the Möbius matrix scaling diag(zt) ⊗c ·, leading to
our proposed Eq. (33) for the hyperbolic GRU.

F More Experimental Investigations

The following empirical facts were observed for both hyperbolic RNNs and GRUs.

We observed that, in the hyperbolic setting, accuracy is often much higher when sentence embeddings
can go close to the border (hyperbolic "inﬁnity"), hence exploiting the hyperbolic nature of the space.
Moreover, the faster the two sentence norms go to 1, the more it’s likely that a good local minima
was reached. See ﬁgures 3 and 5.

We often observe that test accuracy starts increasing exactly when sentence embedding norms do.
However, in the hyperbolic setting, the sentence embeddings norms remain close to 0 for a few
epochs, which does not happen in the Euclidean case. See ﬁgures 3, 5 and 4. This mysterious fact
was also exhibited in a similar way by [21] which suggests that the model ﬁrst has to adjust the
angular layout in the almost Euclidean vicinity of 0 before increasing norms and fully exploiting
hyperbolic geometry.

19

(a) Test accuracy

(a) Test accuracy

20

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 3: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the same
architecture: hyperbolic GRU followed by hyperbolic FFNN and hyperbolic/Euclidean (half-half)
MLR. The X axis shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 4: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: Euclidean GRU followed by Euclidean FFNN and Euclidean MLR. The X axis
shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 5: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: hyperbolic RNN followed by hyperbolic FFNN and hyperbolic MLR. The X axis
shows millions of training examples processed.

(a) Test accuracy

21

8
1
0
2
 
n
u
J
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
2
1
1
9
0
.
5
0
8
1
:
v
i
X
r
a

Hyperbolic Neural Networks

Octavian-Eugen Ganea∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
octavian.ganea@inf.ethz.ch

Gary Bécigneul∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
gary.becigneul@inf.ethz.ch

Thomas Hofmann
Department of Computer Science
ETH Zürich
Zurich, Switzerland
thomas.hofmann@inf.ethz.ch

Abstract

Hyperbolic spaces have recently gained momentum in the context of machine
learning due to their high capacity and tree-likeliness properties. However, the
representational power of hyperbolic geometry is not yet on par with Euclidean
geometry, mostly because of the absence of corresponding hyperbolic neural
network layers. This makes it hard to use hyperbolic embeddings in downstream
tasks. Here, we bridge this gap in a principled manner by combining the formalism
of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model
of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep
learning tools: multinomial logistic regression, feed-forward and recurrent neural
networks such as gated recurrent units. This allows to embed sequential data and
perform classiﬁcation in the hyperbolic space. Empirically, we show that, even if
hyperbolic optimization tools are limited, hyperbolic sentence embeddings either
outperform or are on par with their Euclidean variants on textual entailment and
noisy-preﬁx recognition tasks.

1

Introduction

It is common in machine learning to represent data as being embedded in the Euclidean space Rn. The
main reason for such a choice is simply convenience, as this space has a vectorial structure, closed-
form formulas of distance and inner-product, and is the natural generalization of our intuition-friendly,
visual three-dimensional space. Moreover, embedding entities in such a continuous space allows to
feed them as input to neural networks, which has led to unprecedented performance on a broad range
of problems, including sentiment detection [15], machine translation [3], textual entailment [22] or
knowledge base link prediction [20, 6].

Despite the success of Euclidean embeddings, recent research has proven that many types of com-
plex data (e.g. graph data) from a multitude of ﬁelds (e.g. Biology, Network Science, Computer
Graphics or Computer Vision) exhibit a highly non-Euclidean latent anatomy [8]. In such cases, the
Euclidean space does not provide the most powerful or meaningful geometrical representations. For
example, [10] shows that arbitrary tree structures cannot be embedded with arbitrary low distortion
(i.e. almost preserving their metric) in the Euclidean space with unbounded number of dimensions,
but this task becomes surprisingly easy in the hyperbolic space with only 2 dimensions where the
exponential growth of distances matches the exponential growth of nodes with the tree depth.

∗Equal contribution.

The adoption of neural networks and deep learning in these non-Euclidean settings has been rather
limited until very recently, the main reason being the non-trivial or impossible principled general-
izations of basic operations (e.g. vector addition, matrix-vector multiplication, vector translation,
vector inner product) as well as, in more complex geometries, the lack of closed form expressions for
basic objects (e.g. distances, geodesics, parallel transport). Thus, classic tools such as multinomial
logistic regression (MLR), feed forward (FFNN) or recurrent neural networks (RNN) did not have a
correspondence in these geometries.

How should one generalize deep neural models to non-Euclidean domains ? In this paper we address
this question for one of the simplest, yet useful, non-Euclidean domains: spaces of constant negative
curvature, i.e. hyperbolic. Their tree-likeness properties have been extensively studied [12, 13, 26]
and used to visualize large taxonomies [18] or to embed heterogeneous complex networks [17]. In
machine learning, recently, hyperbolic representations greatly outperformed Euclidean embeddings
for hierarchical, taxonomic or entailment data [21, 10, 11]. Disjoint subtrees from the latent hierar-
chical structure surprisingly disentangle and cluster in the embedding space as a simple reﬂection of
the space’s negative curvature. However, appropriate deep learning tools are needed to embed feature
data in this space and use it in downstream tasks. For example, implicitly hierarchical sequence data
(e.g. textual entailment data, phylogenetic trees of DNA sequences or hierarchial captions of images)
would beneﬁt from suitable hyperbolic RNNs.

The main contribution of this paper is to bridge the gap between hyperbolic and Euclidean geometry
in the context of neural networks and deep learning by generalizing in a principled manner both the
basic operations as well as multinomial logistic regression (MLR), feed-forward (FFNN), simple and
gated (GRU) recurrent neural networks (RNN) to the Poincaré model of the hyperbolic geometry.
We do it by connecting the theory of gyrovector spaces and generalized Möbius transformations
introduced by [2, 26] with the Riemannian geometry properties of the manifold. We smoothly
parametrize basic operations and objects in all spaces of constant negative curvature using a uniﬁed
framework that depends only on the curvature value. Thus, we show how Euclidean and hyperbolic
spaces can be continuously deformed into each other. On a series of experiments and datasets we
showcase the effectiveness of our hyperbolic neural network layers compared to their "classic"
Euclidean variants on textual entailment and noisy-preﬁx recognition tasks. We hope that this paper
will open exciting future directions in the nascent ﬁeld of Geometric Deep Learning.

2 The Geometry of the Poincaré Ball

2.1 Basics of Riemannian geometry

We brieﬂy introduce basic concepts of differential geometry largely needed for a principled general-
ization of Euclidean neural networks. For more rigorous and in-depth expositions, see [23, 14].
An n-dimensional manifold M is a space that can locally be approximated by Rn: it is a generalization
to higher dimensions of the notion of a 2D surface. For x ∈ M, one can deﬁne the tangent space
TxM of M at x as the ﬁrst order linear approximation of M around x. A Riemannian metric
g = (gx)x∈M on M is a collection of inner-products gx : TxM × TxM → R varying smoothly
with x. A Riemannian manifold (M, g) is a manifold M equipped with a Riemannian metric g.
Although a choice of a Riemannian metric g on M only seems to deﬁne the geometry locally on M,
it induces global distances by integrating the length (of the speed vector living in the tangent space)
of a shortest path between two points:

(cid:90) 1

(cid:113)

d(x, y) = inf
γ

0

gγ(t)( ˙γ(t), ˙γ(t))dt,

(1)

where γ ∈ C∞([0, 1], M) is such that γ(0) = x and γ(1) = y. A smooth path γ of minimal length
between two points x and y is called a geodesic, and can be seen as the generalization of a straight-line
in Euclidean space. The parallel transport Px→y : TxM → TyM is a linear isometry between
tangent spaces which corresponds to moving tangent vectors along geodesics and deﬁnes a canonical
way to connect tangent spaces. The exponential map expx at x, when well-deﬁned, gives a way to
project back a vector v of the tangent space TxM at x, to a point expx(v) ∈ M on the manifold.
This map is often used to parametrize a geodesic γ starting from γ(0) := x ∈ M with unit-norm
direction ˙γ(0) := v ∈ TxM as t (cid:55)→ expx(tv). For geodesically complete manifolds, such as the
Poincaré ball considered in this work, expx is well-deﬁned on the full tangent space TxM. Finally, a

2

(2)

(3)

(4)

(5)

metric ˜g is said to be conformal to another metric g if it deﬁnes the same angles, i.e.

˜gx(u, v)
(cid:112)˜gx(u, u)(cid:112)˜gx(v, v)

=

gx(u, v)
(cid:112)gx(u, u)(cid:112)gx(v, v)

,

for all x ∈ M, u, v ∈ TxM \ {0}. This is equivalent to the existence of a smooth function
λ : M → R, called the conformal factor, such that ˜gx = λ2

xgx for all x ∈ M.

2.2 Hyperbolic space: the Poincaré ball

The hyperbolic space has ﬁve isometric models that one can work with [9]. Similarly as in [21] and
[11], we choose to work in the Poincaré ball. The Poincaré ball model (Dn, gD) is deﬁned by the
manifold Dn = {x ∈ Rn : (cid:107)x(cid:107) < 1} equipped with the following Riemannian metric:

gD
x = λ2

xgE, where λx :=

2
1 − (cid:107)x(cid:107)2 ,

gE = In being the Euclidean metric tensor. Note that the hyperbolic metric tensor is conformal to
the Euclidean one. The induced distance between two points x, y ∈ Dn is known to be given by

dD(x, y) = cosh−1

1 + 2

(cid:18)

(cid:107)x − y(cid:107)2
(1 − (cid:107)x(cid:107)2)(1 − (cid:107)y(cid:107)2)

(cid:19)

.

Since the Poincaré ball is conformal to Euclidean space, the angle between two vectors u, v ∈
TxDn \ {0} is given by

cos(∠(u, v)) =

gD
x (u, v)
x (u, u)(cid:112)gD

(cid:112)gD

x (v, v)

=

(cid:104)u, v(cid:105)
(cid:107)u(cid:107)(cid:107)v(cid:107)

.

2.3 Gyrovector spaces

In Euclidean space, natural operations inherited from the vectorial structure, such as vector addition,
subtraction and scalar multiplication are often useful. The framework of gyrovector spaces provides
an elegant non-associative algebraic formalism for hyperbolic geometry just as vector spaces provide
the algebraic setting for Euclidean geometry [2, 25, 26].

In particular, these operations are used in special relativity, allowing to add speed vectors belonging
to the Poincaré ball of radius c (the celerity, i.e. the speed of light) so that they remain in the ball,
hence not exceeding the speed of light.

We will make extensive use of these operations in our deﬁnitions of hyperbolic neural networks.
For c ≥ 0, denote2 by Dn
then Dn

c := {x ∈ Rn | c(cid:107)x(cid:107)2 < 1}. Note that if c = 0, then Dn
c. If c = 1 then we recover the usual ball Dn.

c is the open ball of radius 1/

c = Rn; if c > 0,

√

Möbius addition. The Möbius addition of x and y in Dn

c is deﬁned as

x ⊕c y :=

(1 + 2c(cid:104)x, y(cid:105) + c(cid:107)y(cid:107)2)x + (1 − c(cid:107)x(cid:107)2)y
1 + 2c(cid:104)x, y(cid:105) + c2(cid:107)x(cid:107)2(cid:107)y(cid:107)2

.

(6)

In particular, when c = 0, one recovers the Euclidean addition of two vectors in Rn. Note that
without loss of generality, the case c > 0 can be reduced to c = 1. Unless stated otherwise, we
will use ⊕ as ⊕1 to simplify notations. For general c > 0, this operation is not commutative nor
associative. However, it satisﬁes x ⊕c 0 = 0 ⊕c x = 0. Moreover, for any x, y ∈ Dn
c , we have
(−x) ⊕c x = x ⊕c (−x) = 0 and (−x) ⊕c (x ⊕c y) = y (left-cancellation law). The Möbius
substraction is then deﬁned by the use of the following notation: x (cid:9)c y := x ⊕c (−y). See [29,
section 2.1] for a geometric interpretation of the Möbius addition.

2We take different notations as in [25] where the author uses s = 1/

c.

√

3

Möbius scalar multiplication. For c > 0, the Möbius scalar multiplication of x ∈ Dn
r ∈ R is deﬁned as

c \ {0} by

r ⊗c x := (1/

c) tanh(r tanh−1(

c(cid:107)x(cid:107)))

√

√

x
(cid:107)x(cid:107)

,

(7)

and r ⊗c 0 := 0. Note that similarly as for the Möbius addition, one recovers the Euclidean scalar
multiplication when c goes to zero: limc→0 r ⊗c x = rx. This operation satisﬁes desirable properties
such as n ⊗c x = x ⊕c · · · ⊕c x (n additions), (r + r(cid:48)) ⊗c x = r ⊗c x ⊕c r(cid:48) ⊗c x (scalar distributivity3),
(rr(cid:48)) ⊗c x = r ⊗c (r(cid:48) ⊗c x) (scalar associativity) and |r| ⊗c x/(cid:107)r ⊗c x(cid:107) = x/(cid:107)x(cid:107) (scaling property).

c , gc) is given by4

Distance.
Euclidean one, with conformal factor λc
(Dn

If one deﬁnes the generalized hyperbolic metric tensor gc as the metric conformal to the
x := 2/(1 − c(cid:107)x(cid:107)2), then the induced distance function on
√

c) tanh−1 (cid:0)√
Again, observe that limc→0 dc(x, y) = 2(cid:107)x − y(cid:107), i.e. we recover Euclidean geometry in the limit5.
Moreover, for c = 1 we recover dD of Eq. (4).

c(cid:107) − x ⊕c y(cid:107)(cid:1) .

dc(x, y) = (2/

(8)

Hyperbolic trigonometry. Similarly as in the Euclidean space, one can deﬁne the notions of
hyperbolic angles or gyroangles (when using the ⊕c), as well as hyperbolic law of sines in the
generalized Poincaré ball (Dn

c , gc). We make use of these notions in our proofs. See Appendix A.

2.4 Connecting Gyrovector spaces and Riemannian geometry of the Poincaré ball

In this subsection, we present how geodesics in the Poincaré ball model are usually described with
Möbius operations, and push one step further the existing connection between gyrovector spaces and
the Poincaré ball by ﬁnding new identities involving the exponential map, and parallel transport.

In particular, these ﬁndings provide us with a simpler formulation of Möbius scalar multiplication,
yielding a natural deﬁnition of matrix-vector multiplication in the Poincaré ball.

Riemannian gyroline element. The Riemannian gyroline element is deﬁned for an inﬁnitesimal
dx as ds := (x + dx) (cid:9)c x, and its size is given by [26, section 3.7]:

(cid:107)ds(cid:107) = (cid:107)(x + dx) (cid:9)c x(cid:107) = (cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2).

(9)

What is remarkable is that it turns out to be identical, up to a scaling factor of 2, to the usual line
element 2(cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2) of the Riemannian manifold (Dn

c , gc).

Geodesics. The geodesic connecting points x, y ∈ Dn

c is shown in [2, 26] to be given by:

γx→y(t) := x ⊕c (−x ⊕c y) ⊗c t, with γx→y : R → Dn

c s.t. γx→y(0) = x and γx→y(1) = y.

Note that when c goes to 0, geodesics become straight-lines, recovering Euclidean geometry. In the
remainder of this subsection, we connect the gyrospace framework with Riemannian geometry.
Lemma 1. For any x ∈ Dn and v ∈ TxDn
c s.t. gc
x with direction v is given by:

x(v, v) = 1, the unit-speed geodesic starting from

γx,v(t) = x ⊕c

tanh

(cid:18)

(cid:18)√

(cid:19) v
√

c

t
2

c(cid:107)v(cid:107)

(cid:19)

, where γx,v : R → Dn s.t. γx,v(0) = x and ˙γx,v(0) = v.

(10)

(11)

Proof. One can use Eq. (10) and reparametrize it to unit-speed using Eq. (8). Alternatively, direct
computation and identiﬁcation with the formula in [11, Thm. 1] would give the same result. Using
Eq. (8) and Eq. (11), one can sanity-check that dc(γ(0), γ(t)) = t, ∀t ∈ [0, 1].

3⊗c has priority over ⊕c in the sense that a ⊗c b ⊕c c := (a ⊗c b) ⊕c c and a ⊕c b ⊗c c := a ⊕c (b ⊗c c).
4The notation −x ⊕c y should always be read as (−x) ⊕c y and not −(x ⊕c y).
5The factor 2 comes from the conformal factor λx = 2/(1 − (cid:107)x(cid:107)2), which is a convention setting the

curvature to −1.

4

Exponential and logarithmic maps. The following lemma gives the closed-form derivation of
exponential and logarithmic maps.
Lemma 2. For any point x ∈ Dn
map logc
c → TxDn
(cid:18)

c are given for v (cid:54)= 0 and y (cid:54)= x by:
(cid:18)√

c , the exponential map expc

c and the logarithmic

x : TxDn

c → Dn

x : Dn

√

(cid:19)

, logc

x(y) =

√

tanh−1(

c(cid:107) − x ⊕c y(cid:107))

expc

x(v) = x ⊕c

tanh

c

λc
x(cid:107)v(cid:107)
2

(cid:19) v
√

c(cid:107)v(cid:107)

2
cλc
x

−x ⊕c y
(cid:107) − x ⊕c y(cid:107)
(12)

.

Proof. Following the proof of [11, Cor. 1.1], one gets expc
gives the formula for expc

x. Algebraic check of the identity logc

x(v) = γx,
x(expc

v

x(cid:107)v(cid:107) (λc
λc

x(cid:107)v(cid:107)). Using Eq. (11)

x(v)) = v concludes.

The above maps have more appealing forms when x = 0, namely for v ∈ T0Dn

c \ {0}, y ∈ Dn

c \ {0}:

expc

0(v) = tanh(

c(cid:107)v(cid:107))

√

, logc

0(y) = tanh−1(

c(cid:107)y(cid:107))

√

(13)

√

y
c(cid:107)y(cid:107)

.

√

v
c(cid:107)v(cid:107)

Moreover, we still recover Euclidean geometry in the limit c → 0, as limc→0 expc
Euclidean exponential map, and limc→0 logc

x(y) = y − x is the Euclidean logarithmic map.

x(v) = x + v is the

Möbius scalar multiplication using exponential and logarithmic maps. We studied the expo-
nential and logarithmic maps in order to gain a better understanding of the Möbius scalar multiplica-
tion (Eq. (7)). We found the following:
Lemma 3. The quantity r ⊗ x can actually be obtained by projecting x in the tangent space at 0
with the logarithmic map, multiplying this projection by the scalar r in T0Dn
c , and then projecting it
back on the manifold with the exponential map:
0(r logc

∀r ∈ R, x ∈ Dn
c .

r ⊗c x = expc

0(x)),

(14)

In addition, we recover the well-known relation between geodesics connecting two points and the
exponential map:

γx→y(t) = x ⊕c (−x ⊕c y) ⊗c t = expc

x(t logc

x(y)),

t ∈ [0, 1].

(15)

This last result enables us to generalize scalar multiplication in order to deﬁne matrix-vector multipli-
cation between Poincaré balls, one of the essential building blocks of hyperbolic neural networks.

Parallel transport. Finally, we connect parallel transport (from T0Dn
the following theorem, which we prove in appendix B.
Theorem 4. In the manifold (Dn
vector v ∈ T0Dn

c to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a
c is given by the following isometry:
λc
0
λc
x

x(x ⊕c expc

0(v)) =

0→x(v) = logc
P c

(16)

v.

c ) to gyrovector spaces with

As we’ll see later, this result is crucial in order to deﬁne and optimize parameters shared between
different tangent spaces, such as biases in hyperbolic neural layers or parameters of hyperbolic MLR.

3 Hyperbolic Neural Networks

Neural networks can be seen as being made of compositions of basic operations, such as linear
maps, bias translations, pointwise non-linearities and a ﬁnal sigmoid or softmax layer. We ﬁrst
explain how to construct a softmax layer for logits lying in a Poincaré ball. Then, we explain how
to transform a mapping between two Euclidean spaces as one between Poincaré balls, yielding
matrix-vector multiplication and pointwise non-linearities in the Poincaré ball. Finally, we present
possible adaptations of various recurrent neural networks to the hyperbolic domain.

5

3.1 Hyperbolic multiclass logistic regression

In order to perform multi-class classiﬁcation on the Poincaré ball, one needs to generalize multinomial
logistic regression (MLR) − also called softmax regression − to the Poincaré ball.

Reformulating Euclidean MLR. Let’s ﬁrst reformulate Euclidean MLR from the perspective of
distances to margin hyperplanes, as in [19, Section 5]. This will allow us to easily generalize it.

Given K classes, one learns a margin hyperplane for each such class using softmax probabilities:

∀k ∈ {1, ..., K},

p(y = k|x) ∝ exp (((cid:104)ak, x(cid:105) − bk)) , where bk ∈ R, x, ak ∈ Rn.

(17)

Note that any afﬁne hyperplane in Rn can be written with a normal vector a and a scalar shift b:

Ha,b = {x ∈ Rn : (cid:104)a, x(cid:105) − b = 0}, where a ∈ Rn \ {0}, and b ∈ R.

(18)

As in [19, Section 5], we note that (cid:104)a, x(cid:105) − b = sign((cid:104)a, x(cid:105) − b)(cid:107)a(cid:107)d(x, Ha,b). Using Eq. (17):

p(y = k|x) ∝ exp(sign((cid:104)ak, x(cid:105) − bk)(cid:107)ak(cid:107)d(x, Hak,bk )), bk ∈ R, x, ak ∈ Rn.

(19)

As it is not immediately obvious how to generalize the Euclidean hyperplane of Eq. (18) to other
spaces such as the Poincaré ball, we reformulate it as follows:

˜Ha,p = {x ∈ Rn : (cid:104)−p + x, a(cid:105) = 0} = p + {a}⊥, where p ∈ Rn, a ∈ Rn \ {0}.

(20)

This new deﬁnition relates to the previous one as ˜Ha,p = Ha,(cid:104)a,p(cid:105). Rewriting Eq. (19) with b = (cid:104)a, p(cid:105):
p(y = k|x) ∝ exp(sign((cid:104)−pk + x, ak(cid:105))(cid:107)ak(cid:107)d(x, ˜Hak,pk )), with pk, x, ak ∈ Rn.

(21)

It is now natural to adapt the previous deﬁnition to the hyperbolic setting by replacing + by ⊕c:
Deﬁnition 3.1 (Poincaré hyperplanes). For p ∈ Dn
p(z, a) = 0} = {z ∈ TpDn
gc

c : (cid:104)z, a(cid:105) = 0}. Then, we deﬁne Poincaré hyperplanes as

c \ {0}, let {a}⊥ := {z ∈ TpDn
c :

c , a ∈ TpDn

˜H c

a,p := {x ∈ Dn

c : (cid:104)logc

p(x), a(cid:105)p = 0} = expc

p({a}⊥) = {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}.

(22)

The last equality is shown appendix C. ˜H c
all geodesics in Dn
hypergyroplanes, see [27, deﬁnition 5.8]. A 3D hyperplane example is depicted in Fig. 1.

a,p can also be described as the union of images of
c orthogonal to a and containing p. Notice that our deﬁnition matches that of

Next, we need the following theorem, proved in appendix D:
Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(23)

Final formula for MLR in the Poincaré ball. Putting together Eq. (21) and Thm. 5, we get the
hyperbolic MLR formulation. Given K classes and k ∈ {1, . . . , K}, pk ∈ Dn
c \ {0}:

c , ak ∈ Tpk

Dn

p(y = k|x) ∝ exp(sign((cid:104)−pk ⊕c x, ak(cid:105))

gc
pk

(ak, ak)dc(x, ˜H c

)),

ak,pk

∀x ∈ Dn
c ,

(24)

or, equivalently

p(y = k|x) ∝ exp

(cid:18) λc
pk

(cid:107)ak(cid:107)
√
c

sinh−1

(cid:18)

√
2

c(cid:104)−pk ⊕c x, ak(cid:105)

(cid:19)(cid:19)

(1 − c(cid:107) − pk ⊕c x(cid:107)2)(cid:107)ak(cid:107)

,

∀x ∈ Dn
c .

(25)

this goes to p(y = k|x) ∝ exp(4(cid:104)−pk + x, ak(cid:105)) =

Notice that when c goes to zero,
exp((λ0
pk

)2(cid:104)−pk + x, ak(cid:105)) = exp((cid:104)−pk + x, ak(cid:105)0), recovering the usual Euclidean softmax.
However, at this point it is unclear how to perform optimization over ak, since it lives in Tpk
hence depends on pk. The solution is that one should write ak = P c
)a(cid:48)
k ∈ T0Dn
a(cid:48)

c = Rn, and optimize a(cid:48)

k as a Euclidean parameter.

k) = (λc

0/λc
pk

0→pk

(a(cid:48)

Dn
c and
k, where

1
√
c

(cid:113)

6

3.2 Hyperbolic feed-forward layers

In order to deﬁne hyperbolic neural networks, it is crucial to de-
ﬁne a canonically simple parametric family of transformations,
playing the role of linear mappings in usual Euclidean neural
networks, and to know how to apply pointwise non-linearities.
Inspiring ourselves from our reformulation of Möbius scalar
multiplication in Eq. (14), we deﬁne:
Deﬁnition 3.2 (Möbius version). For f : Rn → Rm, we deﬁne
the Möbius version of f as the map from Dn

c to Dm

c by:

f ⊗c(x) := expc

0(f (logc

0(x))),

(26)

where expc

0 : T0m

Dm

c → Dm

c and logc

0 : Dn

c → T0n

Dn
c .

Figure 1: An example of a hyper-
bolic hyperplane in D3
1 plotted us-
ing sampling. The red point is p.
The shown normal axis to the hy-
perplane through p is parallel to a.

Note that similarly as for other Möbius operations, we recover
the Euclidean mapping in the limit c → 0 if f is continuous, as limc→0 f ⊗c(x) = f (x). This
deﬁnition satisﬁes a few desirable properties too, such as: (f ◦ g)⊗c = f ⊗c ◦ g⊗c for f : Rm → Rl
and g : Rn → Rm (morphism property), and f ⊗c(x)/(cid:107)f ⊗c(x)(cid:107) = f (x)/(cid:107)f (x)(cid:107) for f (x) (cid:54)= 0
(direction preserving). It is then straight-forward to prove the following result:
Lemma 6 (Möbius matrix-vector multiplication). If M : Rn → Rm is a linear map, which we
identify with its matrix representation, then ∀x ∈ Dn

c , if M x (cid:54)= 0 we have

M ⊗c(x) = (1/

c) tanh

√

(cid:18) (cid:107)M x(cid:107)
(cid:107)x(cid:107)

√

tanh−1(

c(cid:107)x(cid:107))

(cid:19) M x
(cid:107)M x(cid:107)

,

(27)

and M ⊗c(x) = 0 if M x = 0. Moreover, if we deﬁne the Möbius matrix-vector multiplication of
M ∈ Mm,n(R) and x ∈ Dn
c by M ⊗c x := M ⊗c(x), then we have (M M (cid:48)) ⊗c x = M ⊗c (M (cid:48) ⊗c x)
for M ∈ Ml,m(R) and M (cid:48) ∈ Mm,n(R) (matrix associativity), (rM ) ⊗c x = r ⊗c (M ⊗c x) for
r ∈ R and M ∈ Mm,n(R) (scalar-matrix associativity) and M ⊗c x = M x for all M ∈ On(R)
(rotations are preserved).

Pointwise non-linearity.
ϕ⊗c can be applied to elements of the Poincaré ball.

If ϕ : Rn → Rn is a pointwise non-linearity, then its Möbius version

Bias translation. The generalization of a translation in the Poincaré ball is naturally given by
moving along geodesics. But should we use the Möbius sum x ⊕c b with a hyperbolic bias b or the
x(b(cid:48)) with a Euclidean bias b(cid:48)? These views are uniﬁed with parallel transport
exponential map expc
c by a bias b ∈ Dn
(see Thm 4). Möbius translation of a point x ∈ Dn
(cid:18) λc
0
λc
x

c is given by
(cid:19)

x ← x ⊕c b = expc

0(b))) = expc
x

0→x(logc

x(P c

logc

0(b)

(28)

.

We recover Euclidean translations in the limit c → 0. Note that bias translations play a particular
Indeed, consider multiple layers of the form fk(x) = ϕk(Mkx), each of
role in this model.
which having Möbius version f ⊗c
k (Mk ⊗c x). Then their composition can be re-written
f ⊗c
k ◦ · · · ◦ f ⊗c
1 = expc
0. This means that these operations can essentially be
performed in Euclidean space. Therefore, it is the interposition between those with the bias translation
of Eq. (28) which differentiates this model from its Euclidean counterpart.

k (x) = ϕ⊗c
0 ◦fk ◦ · · · ◦ f1 ◦ logc

If a vector x ∈ Rn+p is the (vertical) concatenation
Concatenation of multiple input vectors.
of two vectors x1 ∈ Rn, x2 ∈ Rp, and M ∈ Mm,n+p(R) can be written as the (horizontal)
concatenation of two matrices M1 ∈ Mm,n(R) and M2 ∈ Mm,p(R), then M x = M1x1 + M2x2.
We generalize this to hyperbolic spaces: if we are given x1 ∈ Dn
c ×Dp
c ,
and M, M1, M2 as before, then we deﬁne M ⊗c x := M1 ⊗c x1 ⊕c M2 ⊗c x2. Note that when c goes
to zero, we recover the Euclidean formulation, as limc→0 M ⊗c x = limc→0 M1 ⊗c x1 ⊕c M2 ⊗c x2 =
M1x1 + M2x2 = M x. Moreover, hyperbolic vectors x ∈ Dn
c can also be "concatenated" with real
features y ∈ R by doing: M ⊗c x ⊕c y ⊗c b with learnable b ∈ Dm

c , x = (x1 x2)T ∈ Dn

c and M ∈ Mm,n(R).

c , x2 ∈ Dp

7

3.3 Hyperbolic RNN

Naive RNN. A simple RNN can be deﬁned by ht+1 = ϕ(W ht + U xt + b) where ϕ is a pointwise
non-linearity, typically tanh, sigmoid, ReLU, etc. This formula can be naturally generalized to the
hyperbolic space as follows. For parameters W ∈ Mm,n(R), U ∈ Mm,d(R), b ∈ Dm
c , we deﬁne:

ht+1 = ϕ⊗c (W ⊗c ht ⊕c U ⊗c xt ⊕c b),

ht ∈ Dn

c , xt ∈ Dd
c .

(29)

Note that if inputs xt’s are Euclidean, one can write ˜xt := expc
expc

(U xt)) = W ⊗c ht ⊕c expc

(P c

W ⊗cht

0→W ⊗cht

0(U xt) = W ⊗c ht ⊕c U ⊗c ˜xt.

0(xt) and use the above formula, since

GRU architecture. One can also adapt the GRU architecture:
rt = σ(W rht−1 + U rxt + br),
zt = σ(W zht−1 + U zxt + bz),
˜ht = ϕ(W (rt (cid:12) ht−1) + U xt + b), ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht,

(30)

where (cid:12) denotes pointwise product. First, how should we adapt the pointwise multiplication by a
scaling gate? Note that the deﬁnition of the Möbius version (see Eq. (26)) can be naturally extended
to maps f : Rn × Rp → Rm as f ⊗c : (h, h(cid:48)) ∈ Dn
0(h(cid:48)))). In
c (cid:55)→ expc
0(h(cid:48))) =
particular, choosing f (h, h(cid:48)) := σ(h) (cid:12) h(cid:48) yields6 f ⊗c(h, h(cid:48)) = expc
diag(σ(logc

0(h))) ⊗c h(cid:48). Hence we adapt rt (cid:12) ht−1 to diag(rt) ⊗c ht−1 and the reset gate rt to:
0(W r ⊗c ht−1 ⊕c U r ⊗c xt ⊕c br),

0(h), logc
0(h)) (cid:12) logc

0(f (logc
0(σ(logc

rt = σ logc

c × Dp

(31)

and similarly for the update gate zt. Note that as the argument of σ in the above is unbounded, rt and
zt can a priori take values onto the full range (0, 1). Now the intermediate hidden state becomes:
˜ht = ϕ⊗c ((W diag(rt)) ⊗c ht−1 ⊕c U ⊗c xt ⊕ b),

(32)

where Möbius matrix associativity simpliﬁes W ⊗c (diag(rt) ⊗c ht−1) into (W diag(rt)) ⊗c ht−1.
Finally, we propose to adapt the update-gate equation as

ht = ht−1 ⊕c diag(zt) ⊗c (−ht−1 ⊕c

˜ht).

(33)

Note that when c goes to zero, one recovers the usual GRU. Moreover, if zt = 0 or zt = 1, then ht
becomes ht−1 or ˜ht respectively, similarly as in the usual GRU. This adaptation was obtained by
adapting [24]: in this work, the authors re-derive the update-gate mechanism from a ﬁrst principle
called time-warping invariance. We adapted their derivation to the hyperbolic setting by using the
notion of gyroderivative [4] and proving a gyro-chain-rule (see appendix E).

4 Experiments

SNLI task and dataset. We evaluate our method on two tasks. The ﬁrst is natural language
inference, or textual entailment. Given two sentences, a premise (e.g. "Little kids A. and B. are
playing soccer.") and a hypothesis (e.g. "Two children are playing outdoors."), the binary classiﬁcation
task is to predict whether the second sentence can be inferred from the ﬁrst one. This deﬁnes a partial
order in the sentence space. We test hyperbolic networks on the biggest real dataset for this task,
SNLI [7]. It consists of 570K training, 10K validation and 10K test sentence pairs. Following [28],
we merge the "contradiction" and "neutral" classes into a single class of negative sentence pairs, while
the "entailment" class gives the positive pairs.

PREFIX task and datasets. We conjecture that the improvements of hyperbolic neural networks
are more signiﬁcant when the underlying data structure is closer to a tree. To test this, we design a
proof-of-concept task of detection of noisy preﬁxes, i.e. given two sentences, one has to decide if the
second sentence is a noisy preﬁx of the ﬁrst, or a random sentence. We thus build synthetic datasets
PREFIX-Z% (for Z being 10, 30 or 50) as follows: for each random ﬁrst sentence of random length
at most 20 and one random preﬁx of it, a second positive sentence is generated by randomly replacing
Z% of the words of the preﬁx, and a second negative sentence of same length is randomly generated.
Word vocabulary size is 100, and we generate 500K training, 10K validation and 10K test pairs.

6If x has n coordinates, then diag(x) denotes the diagonal matrix of size n with xi’s on its diagonal.

8

Models architecture. Our neural network layers can be used in a plug-n-play manner exactly like
standard Euclidean layers. They can also be combined with Euclidean layers. However, optimization
w.r.t. hyperbolic parameters is different (see below) and based on Riemannian gradients which
are just rescaled Euclidean gradients when working in the conformal Poincaré model [21]. Thus,
back-propagation can be applied in the standard way.

In our setting, we embed the two sentences using two distinct hyperbolic RNNs or GRUs. The
sentence embeddings are then fed together with their squared distance (hyperbolic or Euclidean,
depending on their geometry) to a FFNN (Euclidean or hyperbolic, see Sec. 3.2) which is further
fed to an MLR (Euclidean or hyperbolic, see Sec. 3.1) that gives probabilities of the two classes
(entailment vs neutral). We use cross-entropy loss on top. Note that hyperbolic and Euclidean layers
can be mixed, e.g. the full network can be hyperbolic and only the last layer be Euclidean, in which
case one has to use log0 and exp0 functions to move between the two manifolds in a correct manner
as explained for Eq. 26.

Optimization. Our models have both Euclidean (e.g. weight matrices in both Euclidean and
hyperbolic FFNNs, RNNs or GRUs) and hyperbolic parameters (e.g. word embeddings or biases for
the hyperbolic layers). We optimize the Euclidean parameters with Adam [16] (learning rate 0.001).
Hyperbolic parameters cannot be updated with an equivalent method that keeps track of gradient
history due to the absence of a Riemannian Adam. Thus, they are optimized using full Riemannian
stochastic gradient descent (RSGD) [5, 11]. We also experiment with projected RSGD [21], but
optimization was sometimes less stable. We use a different constant learning rate for word embeddings
(0.1) and other hyperbolic weights (0.01) because words are updated less frequently.

Numerical errors. Gradients of the basic operations deﬁned above (e.g. ⊕c, exponential map) are
c(cid:107)x(cid:107) = 1. Thus, we
not deﬁned when the hyperbolic argument vectors are on the ball border, i.e.
always project results of these operations in the ball of radius 1 − (cid:15), where (cid:15) = 10−5. Numerical
errors also appear when hyperbolic vectors get closer to 0, thus we perturb them with an (cid:15)(cid:48) = 10−15
before they are used in any of the above operations. Finally, arguments of the tanh function are
clipped between ±15 to avoid numerical errors, while arguments of tanh−1 are clipped to at most
1 − 10−5.

√

Hyperparameters. For all methods, baselines and datasets, we use c = 1, word and hidden state
embedding dimension of 5 (we focus on the low dimensional setting that was shown to already
be effective [21]), batch size of 64. We ran all methods for a ﬁxed number of 30 epochs. For all
models, we experiment with both identity (no non-linearity) or tanh non-linearity in the RNN/GRU
cell, as well as identity or ReLU after the FFNN layer and before MLR. As expected, for the fully
Euclidean models, tanh and ReLU respectively surpassed the identity variant by a large margin. We
only report the best Euclidean results. Interestingly, for the hyperbolic models, using only identity for
both non-linearities works slightly better and this is likely due to two facts: i) our hyperbolic layers
already contain non-linearities by their nature, ii) tanh is limiting the output domain of the sentence
embeddings, but the hyperbolic speciﬁc geometry is more pronounced at the ball border, i.e. at the
hyperbolic "inﬁnity", compared to the center of the ball.

For the results shown in Tab. 1, we run each model (baseline or ours) exactly 3 times and report the
test result corresponding to the best validation result from these 3 runs. We do this because the highly
non-convex spectrum of hyperbolic neural networks sometimes results in convergence to poor local
minima, suggesting that initialization is very important.

Results. Results are shown in Tab. 1. Note that the fully Euclidean baseline models might have
an advantage over hyperbolic baselines because more sophisticated optimization algorithms such
as Adam do not have a hyperbolic analogue at the moment. We ﬁrst observe that all GRU models
overpass their RNN variants. Hyperbolic RNNs and GRUs have the most signiﬁcant improvement
over their Euclidean variants when the underlying data structure is more tree-like, e.g. for PREFIX-
10% − for which the tree relation between sentences and their preﬁxes is more prominent − we
reduce the error by a factor of 3.35 for hyperbolic vs Euclidean RNN, and by a factor of 1.5 for
hyperbolic vs Euclidean GRU. As soon as the underlying structure diverges more and more from
a tree, the accuracy gap decreases − for example, for PREFIX-50% the noise heavily affects the
representational power of hyperbolic networks. Also, note that on SNLI our methods perform
similarly as with their Euclidean variants. Moreover, hyperbolic and Euclidean MLR are on par when

9

SNLI

PREFIX-10% PREFIX-30% PREFIX-50%

FULLY EUCLIDEAN RNN
HYPERBOLIC RNN+FFNN, EUCL MLR
FULLY HYPERBOLIC RNN
FULLY EUCLIDEAN GRU
HYPERBOLIC GRU+FFNN, EUCL MLR
FULLY HYPERBOLIC GRU

79.34 %
79.18 %
78.21 %
81.52 %
79.76 %
81.19 %

89.62 %
96.36 %
96.91 %
95.96 %
97.36 %
97.14 %

81.71 %
87.83 %
87.25 %
86.47 %
88.47 %
88.26 %

72.10 %
76.50 %
62.94 %
75.04 %
76.87 %
76.44 %

Table 1: Test accuracies for various models and four datasets. "Eucl" denotes Euclidean. All word
and sentence embeddings have dimension 5. We highlight in bold the best baseline (or baselines, if
the difference is less than 0.5%).

used in conjunction with hyperbolic sentence embeddings, suggesting further empirical investigation
is needed for this direction (see below).

We also observe that, in the hyperbolic setting, accuracy tends to increase when sentence embeddings
start increasing, and gets better as their norms converge towards 1 (the ball border for c = 1). Unlike
in the Euclidean case, this behavior does happen only after a few epochs and suggests that the model
should ﬁrst adjust the angular layout in order to disentangle the representations, before increasing their
norms to fully exploit the strong clustering property of the hyperbolic geometry. Similar behavior
was observed in the context of embedding trees by [21]. Details in appendix F.

MLR classiﬁcation experiments.
For the sentence entailment classi-
ﬁcation task we do not see a clear
advantage of hyperbolic MLR com-
pared to its Euclidean variant. A pos-
sible reason is that, when trained end-
to-end, the model might decide to
place positive and negative embed-
dings in a manner that is already well
separated with a classic MLR. As a
consequence, we further investigate
MLR for the task of subtree classiﬁ-
cation. Using an open source imple-
mentation7 of [21], we pre-trained
Poincaré embeddings of the Word-
Net noun hierarchy (82,115 nodes).
We then choose one node in this tree
(see Table 2) and classify all other
nodes (solely based on their embed-
dings) as being part of the subtree
rooted at this node. All nodes in such a subtree are divided into positive training nodes (80%) and
positive test nodes (20%). The same splitting procedure is applied for the remaining WordNet nodes
that are divided into a negative training and negative test set respectively. Three variants of MLR
are then trained on top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation
task: hyperbolic MLR, Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean
MLR applied after mapping all embeddings in the tangent space at 0 using the log0 map. We use
different embedding dimensions : 2, 3, 5 and 10. For the hyperbolic MLR, we use full Riemannian
SGD with a learning rate of 0.001. For the two Euclidean models we use ADAM optimizer and the
same learning rate. During training, we always sample the same number of negative and positive
nodes in each minibatch of size 16; thus positive nodes are frequently resampled. All methods are
trained for 30 epochs and the ﬁnal F1 score is reported (no hyperparameters to validate are used, thus
we do not require a validation set). This procedure is repeated for four subtrees of different sizes.

Figure 2: Hyperbolic (left) vs Direct Euclidean (right) binary
MLR used to classify nodes as being part in the GROUP.N.01
subtree of the WordNet noun hierarchy solely based on their
Poincaré embeddings. The positive points (from the subtree)
are in blue, the negative points (the rest) are in red and the
trained positive separation hyperplane is depicted in green.

Quantitative results are presented in Table 2. We can see that the hyperbolic MLR overpasses
its Euclidean variants in almost all settings, sometimes by a large margin. Moreover, to provide

7https://github.com/dalab/hyperbolic_cones

10

WORDNET
SUBTREE

ANIMAL.N.01
3218 / 798

GROUP.N.01
6649 / 1727

WORKER.N.01
861 / 254

MAMMAL.N.01
953 / 228

MODEL

D = 2

D = 3

D = 5

D = 10

HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL

47.43 ± 1.07%
41.69 ± 0.19%
38.89 ± 0.01%
81.72 ± 0.17%
61.13 ± 0.42%
60.75 ± 0.24%
12.68 ± 0.82%
10.86 ± 0.01%
9.04 ± 0.06%
32.01 ± 17.14%
15.58 ± 0.04%
13.10 ± 0.13%

91.92 ± 0.61%
68.43 ± 3.90%
62.57 ± 0.61%
89.87 ± 2.73%
63.56 ± 1.22%
61.98 ± 0.57%
24.09 ± 1.49%
22.39 ± 0.04%
22.57 ± 0.20%
87.54 ± 4.55%
44.68 ± 1.87%
44.89 ± 1.18%

98.07 ± 0.55%
95.59 ± 1.18%
89.21 ± 1.34%
87.89 ± 0.80%
67.82 ± 0.81%
67.92 ± 0.74%
55.46 ± 5.49%
35.23 ± 3.16%
26.47 ± 0.78%
88.73 ± 3.22%
59.35 ± 1.31%
52.51 ± 0.85%

99.26 ± 0.59%
99.36 ± 0.18%
98.27 ± 0.70%
91.91 ± 3.07%
91.38 ± 1.19%
91.41 ± 0.18%
66.83 ± 11.38%
47.29 ± 3.93%
36.66 ± 2.74%
91.37 ± 6.09%
77.76 ± 5.08%
56.11 ± 2.21%

Table 2: Test F1 classiﬁcation scores for four different subtrees of WordNet noun tree. All nodes
in such a subtree are divided into positive training nodes (80%) and positive test nodes (20%);
these counts are shown below each subtree root. The same splitting procedure is applied for the
remaining nodes to obtain negative training and test sets. Three variants of MLR are then trained on
top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation task: hyperbolic MLR,
Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean MLR applied after
mapping all embeddings in the tangent space at 0 using the log0 map. 95% conﬁdence intervals for 3
different runs are shown for each method and each different embedding dimension (2, 3, 5 or 10).

further understanding, we plot the 2-dimensional embeddings and the trained separation hyperplanes
(geodesics in this case) in Figure 2. We can see that respecting the hyperbolic geometry is very
important for a quality classiﬁcation model.

5 Conclusion

We showed how classic Euclidean deep learning tools such as MLR, FFNNs, RNNs or GRUs can be
generalized in a principled manner to all spaces of constant negative curvature combining Riemannian
geometry with the elegant theory of gyrovector spaces. Empirically we found that our models
outperform or are on par with corresponding Euclidean architectures on sequential data with implicit
hierarchical structure. We hope to trigger exciting future research related to better understanding
of the hyperbolic non-convexity spectrum and development of other non-Euclidean deep learning
methods.
Our data and Tensorﬂow [1] code are publicly available8.

Acknowledgements

We thank Igor Petrovski for useful pointers regarding the implementation.

This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement
number 167176. Gary Bécigneul is also funded by the Max Planck ETH Center for Learning
Systems.

References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for
large-scale machine learning. 2016.

[2] Ungar Abraham Albert. Analytic hyperbolic geometry and Albert Einstein’s special theory of

relativity. World scientiﬁc, 2008.

8https://github.com/dalab/hyperbolic_nn

11

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of the International Conference on Learning
Representations (ICLR), 2015.

[4] Graciela S Birman and Abraham A Ungar. The hyperbolic derivative in the poincaré ball model
of hyperbolic geometry. Journal of mathematical analysis and applications, 254(1):321–333,
2001.

[5] S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on

Automatic Control, 58(9):2217–2229, Sept 2013.

[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems (NIPS), pages 2787–2795, 2013.

[7] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 632–642. Association
for Computational Linguistics, 2015.

[8] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,
34(4):18–42, 2017.

[9] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry.

Flavors of geometry, 31:59–115, 1997.

[10] Christopher De Sa, Albert Gu, Christopher Ré, and Frederic Sala. Representation tradeoffs for

hyperbolic embeddings. arXiv preprint arXiv:1804.03329, 2018.

[11] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In Proceedings of the thirty-ﬁfth international conference
on machine learning (ICML), 2018.

[12] Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pages 75–263. Springer, 1987.

[13] Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the

Cambridge Philosophical Society, page 1–17, 2017.

[14] Christopher Hopper and Ben Andrews. The Ricci ﬂow in Riemannian geometry. Springer, 2010.

[15] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1746–1751. Association for Computational Linguistics, 2014.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations (ICLR), 2015.

[17] Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marián Boguná.

Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.

[18] John Lamping, Ramana Rao, and Peter Pirolli. A focus+ context technique based on hyperbolic
geometry for visualizing large hierarchies. In Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 401–408. ACM Press/Addison-Wesley Publishing Co.,
1995.

[19] Guy Lebanon and John Lafferty. Hyperplane margin classiﬁers on the multinomial manifold. In
Proceedings of the international conference on machine learning (ICML), page 66. ACM, 2004.

[20] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective
learning on multi-relational data. In Proceedings of the international conference on machine
learning (ICML), volume 11, pages 809–816, 2011.

12

[21] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical repre-
sentations. In Advances in Neural Information Processing Systems (NIPS), pages 6341–6350,
2017.

[22] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blun-
som. Reasoning about entailment with neural attention. In Proceedings of the International
Conference on Learning Representations (ICLR), 2015.

[23] Michael Spivak. A comprehensive introduction to differential geometry. Publish or perish, 1979.

[24] Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In Proceedings of

the International Conference on Learning Representations (ICLR), 2018.

[25] Abraham A Ungar. Hyperbolic trigonometry and its application in the poincaré ball model of

hyperbolic geometry. Computers & Mathematics with Applications, 41(1-2):135–147, 2001.

[26] Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis

Lectures on Mathematics and Statistics, 1(1):1–194, 2008.

[27] Abraham Albert Ungar. Analytic hyperbolic geometry in n dimensions: An introduction. CRC

Press, 2014.

[28] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. In Proceedings of the International Conference on Learning Representations (ICLR),
2016.

[29] J Vermeer. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic

plane. Topology and its Applications, 152(3):226–242, 2005.

13

A Hyperbolic Trigonometry

Hyperbolic angles. For A, B, C ∈ Dn
c , we denote by ∠A := ∠BAC the angle between the two
geodesics starting from A and ending at B and C respectively. This angle can be deﬁned in two
equivalent ways: i) either using the angle between the initial velocities of the two geodesics as given
by Eq. 5, or ii) using the formula

cos(∠A) =

(cid:28) (−A) ⊕c B
(cid:107)(−A) ⊕c B(cid:107)

,

(−A) ⊕c C
(cid:107)(−A) ⊕c C(cid:107)

(cid:29)

,

In this case, ∠A is also called a gyroangle in the work of [26, section 4].

Hyperbolic law of sines. We state here the hyperbolic law of sines. If for A, B, C ∈ Dn
c , we
denote by ∠B := ∠ABC the angle between the two geodesics starting from B and ending at A and
C respectively, and by ˜c = dc(B, A) the length of the hyperbolic segment BA (and similarly for
others), then we have:

sin(∠A)
√
c˜a)
sinh(

=

sin(∠B)
√
c˜b)
sinh(

=

sin(∠C)
√
c˜c)
sinh(

.

Note that one can also adapt the hyperbolic law of cosines to the hyperbolic space.

B Proof of Theorem 4

Theorem 4.
In the manifold (Dn
to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a vector v ∈ T0Dn
c

c is given by the following isometry:
λc
0
λc
x

0→x(v) = logc
P c

x(x ⊕c expc

0(v)) =

v.

Proof. The geodesic in Dn
v ∈ T0Dn
γ (i.e. X(t) ∈ Tγ(t)Dn

c from 0 to x is given in Eq. (10) by γ(t) = x ⊗c t, for t ∈ [0, 1]. Let
c . Then it is of common knowledge that there exists a unique parallel9 vector ﬁeld X along

c , ∀t ∈ [0, 1]) such that X(0) = v. Let’s deﬁne:
X : t ∈ [0, 1] (cid:55)→ logc

γ(t)(γ(t) ⊕c expc

0(v)) ∈ Tγ(t)Dn
c .

Clearly, X is a vector ﬁeld along γ such that X(0) = v. Now deﬁne
0→x : v ∈ T0Dn
P c

x(x ⊕c expc

0(v)) ∈ TxDn
c .

c (cid:55)→ logc
0→x(v) = λc

c . Since P c

0→x is a linear isometry from T0Dn
v, hence P c
c
0→x(v) = X(1), it is enough to prove that X is parallel in order to guarantee that

From Eq. (12), it is easily seen that P c
to TxDn
c to TxDn
0→x is the parallel transport from T0Dn
P c
c .
Since X is a vector ﬁeld along γ, its covariant derivative can be expressed with the Levi-Civita
connection ∇c associated to gc:

0
λc
x

DX
∂t

= ∇c

˙γ(t)X.

Let’s compute the Levi-Civita connection from its Christoffel symbols. In a local coordinate system,
they can be written as

Γi

jk =

(gc)il(∂jgc

lk + ∂kgc

lj − ∂lgc

jk),

1
2

where superscripts denote the inverse metric tensor and using Einstein’s notations. As gc
at γ(t) ∈ Dn

c this yields:

ij = (λc)2δij,

jk = cλc
Γi

γ(t)(δikγ(t)j + δijγ(t)k − δjkγ(t)i).

9i.e. that DX

∂t = 0 for t ∈ [0, 1], where D

∂t denotes the covariant derivative.

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

14

On the other hand, since X(t) = (λc

∇c

˙γ(t)X = ˙γ(t)i∇c

i X = ˙γ(t)i∇c
i

= vj ˙γ(t)i∇c
i

0/λc

γ(t))v, we have
(cid:32)

(cid:33)

λc
0
λc

γ(t)

v

(cid:32)

λc
0
λc

γ(t)

(cid:33)

ej

.

√

√

Since γ(t) = (1/
Hence there exists K x

c) tanh(t tanh−1(
t ∈ R such that ˙γ(t) = K x

c(cid:107)x(cid:107))) x

(cid:107)x(cid:107) , it is easily seen that ˙γ(t) is colinear to γ(t).
t γ(t). Moreover, we have the following Leibniz rule:
(cid:33)

(cid:32)

(cid:32)

∇c
i

λc
0
λc

γ(t)

(cid:33)

ej

=

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

ej.

Combining these yields

DX
∂t

= K x

t vjγ(t)i

(cid:32)

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

(cid:32)

(cid:33)

(cid:33)

ej

.

Replacing with the Christoffel symbols of ∇c at γ(t) gives

Moreover,

λc
0
λc

γ(t)

λc
0
λc

γ(t)

∇c

i ej =

ijek = 2c[δk
Γk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek.

∂
∂γ(t)i

(cid:32)

(cid:33)

λc
0
λc

γ(t)

ej =

∂
∂γ(t)i

(cid:0)−c(cid:107)γ(t)(cid:107)2(cid:1) ej = −2cγ(t)iej.

Putting together everything, we obtain

DX
∂t

= K x

t vjγ(t)i (cid:0)2c[δk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek − 2cγ(t)iej

(cid:1)

t vjγ(t)i (cid:0)γ(t)jei − δijγ(t)kek
t vj (cid:0)γ(t)jγ(t)iei − γ(t)iδijγ(t)kek
(cid:1)
t vj (cid:0)γ(t)jγ(t)iei − γ(t)jγ(t)kek

(cid:1)

(cid:1)

= 2cK x
= 2cK x
= 2cK x
= 0,

which concludes the proof.

C Proof of Eq. (22)

Proof. Two steps proof:
i) expc

p({a}⊥) ⊆ {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}:

Let z ∈ {a}⊥. From Eq. (12), we have that:

This, together with the left-cancellation law in gyrospaces (see section 2.3), implies that

expc

p(z) = −p ⊕c βz,

for some β ∈ R.

(cid:104)−p ⊕c expc

p(z), a(cid:105) = (cid:104)βz, a(cid:105) = 0

which is what we wanted.

ii) {x ∈ Dn
Let x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0} ⊆ expc
c s.t. (cid:104)−p ⊕c x, a(cid:105) = 0. Then, using Eq. (12), we derive that:
for some β ∈ R,

p(x) = β(−p ⊕c x),

p({a}⊥):

logc

which is orthogonal to a, by assumption. This implies logc

p(x) ∈ {a}⊥, hence x ∈ expc

p({a}⊥).

15

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(54)

D Proof of Theorem 5

Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

1
√
c

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(55)

Proof. We ﬁrst need to prove the following lemma, trivial in the Euclidean space, but not in the
Poincaré ball:
Lemma 7. (Orthogonal projection on a geodesic) Any point in the Poincaré ball has a unique
orthogonal projection on any given geodesic that does not pass through the point. Formally, for all
y ∈ Dn
c and for all geodesics γx→z(·) s.t. y /∈ Im γx→z, there exists an unique w ∈ Im γx→z s.t.
∠(γw→y, γx→z) = π/2.

Proof. We ﬁrst note that any geodesic in Dn
and has two "points at inﬁnity" lying on the ball border (v (cid:54)= 0):

c has the form γ(t) = u ⊕c v ⊗c t as given by Eq. 11,

γ(±∞) = u ⊕c

√

±v
c(cid:107)v(cid:107)

∈ ∂Dn
c .

(56)

Using the notations in the lemma statement, the closed-form of γx→z is given by Eq. (10):

γx→z(t) = x ⊕c (−x ⊕c z) ⊗c t

We denote by x(cid:48), z(cid:48) ∈ ∂Dn
∠ywx(cid:48) is well deﬁned from Eq. (34):

c its points at inﬁnity as described by Eq. (56). Then, the hyperbolic angle

cos(∠(γw→y, γx→z)) = cos(∠ywz(cid:48)) =

(cid:104)−w ⊕c y, −w ⊕c z(cid:48)(cid:105)
(cid:107) − w ⊕c y(cid:107) · (cid:107) − w ⊕c z(cid:48)(cid:107)

.

(57)

We now perform 2 steps for this proof.

i) Existence of w:

The angle function from Eq. (57) is continuous w.r.t t when w = γx→z(t). So we ﬁrst prove existence
of an angle of π/2 by continuously moving w from x(cid:48) to z(cid:48) when t goes from −∞ to ∞, and
observing that cos(∠ywz(cid:48)) goes from −1 to 1 as follows:

cos(∠yx(cid:48)z(cid:48)) = 1 & lim
w→z(cid:48)

cos(∠ywz(cid:48)) = −1.

(58)

The left part of Eq. (58) follows from Eq. (57) and from the fact (easy to show from the deﬁnition
c (which is the case of x(cid:48)). The right part of Eq. (58)
of ⊕c) that a ⊕c b = a, when (cid:107)a(cid:107) = 1/
follows from the fact that ∠ywz(cid:48) = π − ∠ywx(cid:48) (from the conformal property, or from Eq. (34)) and
cos(∠yz(cid:48)x(cid:48)) = 1 (proved as above).
Hence cos(∠ywz(cid:48)) has to pass through 0 when going from −1 to 1, which achieves the proof of
existence.

√

ii) Uniqueness of w:
Assume by contradiction that there are two w and w(cid:48) on γx→z that form angles ∠ywx(cid:48) and ∠yw(cid:48)x(cid:48)
of π/2. Since w, w(cid:48), x(cid:48) are on the same geodesic, we have

π/2 = ∠yw(cid:48)x(cid:48) = ∠yw(cid:48)w = ∠ywx(cid:48) = ∠yw(cid:48)w
So ∆yww(cid:48) has two right angles, but in the Poincaré ball this is impossible.

(59)

Now, we need two more lemmas:
Lemma 8. (Minimizing distance from point to geodesic) The orthogonal projection of a point to
a geodesic (not passing through the point) is minimizing the distance between the point and the
geodesic.

Proof. The proof is similar with the Euclidean case and it’s based on hyperbolic sine law and the fact
that in any right hyperbolic triangle the hypotenuse is strictly longer than any of the other sides.

16

Lemma 9. (Geodesics through p) Let ˜H c
all points on the geodesic γp→w are included in ˜H c

a,p.

a,p be a Poincaré hyperplane. Then, for any w ∈ ˜H c

a,p \ {p},

Proof. γp→w(t) = p ⊕c (−p ⊕c w) ⊗c t. Then, it is easy to check the condition in Eq. (22):

(cid:104)−p ⊕c γp→w(t), a(cid:105) = (cid:104)(−p ⊕c w) ⊗c t, a(cid:105) ∝ (cid:104)−p ⊕c w, a(cid:105) = 0.

(60)

We now turn back to our proof. Let x ∈ Dn
We prove that there is at least one point w∗ ∈ ˜H c

c be an arbitrary point and ˜H c

a,p a Poincaré hyperplane.

a,p that achieves the inﬁmum distance

dc(x, w∗) = inf
w∈ ˜H c

a,p

dc(x, w),

and, moreover, that this distance is the same as the one in the theorem’s statement.
We ﬁrst note that for any point w ∈ ˜H c
and Lemma 9, it is obvious that the projection of x to γp→w will give a strictly lower distance.
Thus, we only consider w ∈ ˜H c
triangle ∆xwp, one gets:

a,p such that ∠xwp = π/2. Applying hyperbolic sine law in the right

a,p, if ∠xwp (cid:54)= π/2, then w (cid:54)= w∗. Indeed, using Lemma 8

dc(x, w) = (1/

c) sinh−1 (cid:0)sinh(

c dc(x, p)) · sin(∠xpw)(cid:1) .

√

√

One of the above quantities does not depend on w:

√

√

sinh(

c dc(x, p)) = sinh(2 tanh−1(

c(cid:107) − p ⊕c x(cid:107))) =

√
2
c(cid:107) − p ⊕c x(cid:107)
1 − c(cid:107) − p ⊕c x(cid:107)2 .

The other quantity is sin(∠xpw) which is minimized when the angle ∠xpw is minimized (be-
cause ∠xpw < π/2 for the hyperbolic right triangle ∆xwp), or, alternatively, when cos(∠xpw) is
maximized. But, we already have from Eq. (34) that:

cos(∠xpw) =

(cid:104)−p ⊕c x, −p ⊕c w(cid:105)
(cid:107) − p ⊕c x(cid:107) · (cid:107) − p ⊕c w(cid:107)

.

To maximize the above, the constraint on the right angle at w can be dropped because cos(∠xpw)
depends only on the geodesic γp→w and not on w itself, and because there is always an orthogonal
projection from any point x to any geodesic as stated by Lemma 7. Thus, it remains to ﬁnd the
maximum of Eq. (64) when w ∈ ˜H c
a,p from Eq. (22), one can easily
prove that

a,p. Using the deﬁnition of ˜H c

Using that fact that logc

p(w)/(cid:107) logc

p(w)(cid:107) = −p ⊕c w/(cid:107) − p ⊕c w(cid:107), we just have to ﬁnd

and we are left with a well known Euclidean problem which is equivalent to ﬁnding the minimum
angle between the vector −p ⊕c x (viewed as Euclidean) and the hyperplane {a}⊥. This angle
is given by the Euclidean orthogonal projection whose sin value is the distance from the vector’s
endpoint to the hyperplane divided by the vector’s length:

{logc

p(w) : w ∈ ˜H c

a,p} = {a}⊥.

max
z∈{a}⊥

(cid:18) (cid:104)−p ⊕c x, z(cid:105)

(cid:107) − p ⊕c x(cid:107) · (cid:107)z(cid:107)

(cid:19)

,

sin(∠xpw∗) =

|(cid:104)−p ⊕c x, a
(cid:107) − p ⊕c x(cid:107)

(cid:107)a(cid:107) (cid:105)|

.

17

It follows that a point w∗ ∈ ˜H c
Eqs. (61),(62),(63) and (67) concludes the proof.

a,p satisfying Eq. (67) exists (but might not be unique). Combining

(61)

(62)

(63)

(64)

(65)

(66)

(67)

(cid:3)

E Derivation of the Hyperbolic GRU Update-gate

In [24], the authors recover the update/forget-gate mechanism of a GRU/LSTM by requiring that the
class of neural networks given by the chosen architecture be invariant to time-warpings. The idea is
the following.

Recovering the update-gate from time-warping. A naive RNN is given by the equation

h(t + 1) = ϕ(W h(t) + U x(t) + b)

Let’s drop the bias b to simplify notations. If h is seen as a differentiable function of time, then a
ﬁrst-order Taylor development gives h(t + δt) ≈ h(t) + δt dh
dt (t) for small δt. Combining this for
δt = 1 with the naive RNN equation, one gets

dh
dt

dα
dt

(t) = ϕ(W h(t) + U x(t)) − h(t).

As this is written for any t, one can replace it by t ← α(t) where α is a (smooth) increasing function
of t called the time-warping. Denoting by ˜h(t) := h(α(t)) and ˜x(t) := x(α(t)), using the chain rule
d˜h
dt (t) = dα

dt (α(t)), one gets

dt (t) dh

d˜h
dt

dα
dt

(t) =

(t)ϕ(W ˜h(t) + U ˜x(t)) −

(t)˜h(t).

(70)

Removing the tildas to simplify notations, discretizing back with dh

dt (t) ≈ h(t + 1) − h(t) yields

h(t + 1) =

(t)ϕ(W h(t) + U x(t)) +

1 −

(t)

h(t).

(71)

dα
dt

(cid:18)

(cid:19)

dα
dt

Requiring that our class of neural networks be invariant to time-warpings means that this class should
contain RNNs deﬁned by Eq. (71), i.e. that dα
dt (t) can be learned. As this is a positive quantity, we
can parametrize it as z(t) = σ(W zh(t) + U zx(t)), recovering the forget-gate equation:

h(t + 1) = z(t)ϕ(W h(t) + U x(t)) + (1 − z(t))h(t).

Adapting this idea to hyperbolic RNNs. The gyroderivative [4] of a map h : R → Dn
as

c is deﬁned

dh
dt

(t) = lim
δt→0

1
δt

⊗c (−h(t) ⊕c h(t + δt)).

Using Möbius scalar associativity and the left-cancellation law leads us to

h(t + δt) ≈ h(t) ⊕c δt ⊗c

(t),

dh
dt

for small δt. Combining this with the equation of a simple hyperbolic RNN of Eq. (29) with δt = 1,
one gets

dh
dt

(t) = −h(t) ⊕c ϕ⊗c(W ⊗c h(t) ⊕c U ⊗c x(t)).

For the next step, we need the following lemma:
Lemma 10 (Gyro-chain-rule). For α : R → R differentiable and h : R → Dn
gyro-derivative, if ˜h := h ◦ α, then we have

c with a well-deﬁned

(68)

(69)

(72)

(73)

(74)

(75)

(76)

where dα

dt (t) denotes the usual derivative.

d˜h
dt

(t) =

(t) ⊗c

(α(t)),

dα
dt

dh
dt

18

(77)

(78)

(79)

(80)

(81)

Proof.

d˜h
dt

(t) = lim
δt→0

1
δt
1
δt

⊗c [−˜h(t) ⊕c

˜h(t + δt)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

= lim
δt→0

= lim
δt→0

= lim
δt→0

= lim
u→0
dα
dt

=

α(cid:48)(t) + O(δt)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
u

(t) ⊗c

(α(t))

dh
dt

⊗c [−h(α(t)) ⊕c h(α(t) + u)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

(Möbius scalar associativity) (82)

where we set u = δt(α(cid:48)(t) + O(δt)), with u → 0 when δt → 0, which concludes.

Using lemma 10 and Eq. (75), with similar notations as in Eq. (70) we have

d˜h
dt

dα
dt

(t) =

(t) ⊗c (−˜h(t) ⊕c ϕ⊗c(W ⊗c

˜h(t) ⊕c U ⊗c ˜x(t))).

(83)

Finally, discretizing back with Eq. (74), using the left-cancellation law and dropping the tildas yields

h(t + 1) = h(t) ⊕c

(t) ⊗c (−h(t) ⊕c ϕ⊗c (W ⊗c h(t) ⊕c U ⊗c x(t))).

(84)

dα
dt

Since α is a time-warping, by deﬁnition its derivative is positive and one can choose to parametrize
it with an update-gate zt (a scalar) deﬁned with a sigmoid. Generalizing this scalar scaling by the
Möbius version of the pointwise scaling (cid:12) yields the Möbius matrix scaling diag(zt) ⊗c ·, leading to
our proposed Eq. (33) for the hyperbolic GRU.

F More Experimental Investigations

The following empirical facts were observed for both hyperbolic RNNs and GRUs.

We observed that, in the hyperbolic setting, accuracy is often much higher when sentence embeddings
can go close to the border (hyperbolic "inﬁnity"), hence exploiting the hyperbolic nature of the space.
Moreover, the faster the two sentence norms go to 1, the more it’s likely that a good local minima
was reached. See ﬁgures 3 and 5.

We often observe that test accuracy starts increasing exactly when sentence embedding norms do.
However, in the hyperbolic setting, the sentence embeddings norms remain close to 0 for a few
epochs, which does not happen in the Euclidean case. See ﬁgures 3, 5 and 4. This mysterious fact
was also exhibited in a similar way by [21] which suggests that the model ﬁrst has to adjust the
angular layout in the almost Euclidean vicinity of 0 before increasing norms and fully exploiting
hyperbolic geometry.

19

(a) Test accuracy

(a) Test accuracy

20

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 3: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the same
architecture: hyperbolic GRU followed by hyperbolic FFNN and hyperbolic/Euclidean (half-half)
MLR. The X axis shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 4: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: Euclidean GRU followed by Euclidean FFNN and Euclidean MLR. The X axis
shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 5: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: hyperbolic RNN followed by hyperbolic FFNN and hyperbolic MLR. The X axis
shows millions of training examples processed.

(a) Test accuracy

21

8
1
0
2
 
n
u
J
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
2
1
1
9
0
.
5
0
8
1
:
v
i
X
r
a

Hyperbolic Neural Networks

Octavian-Eugen Ganea∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
octavian.ganea@inf.ethz.ch

Gary Bécigneul∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
gary.becigneul@inf.ethz.ch

Thomas Hofmann
Department of Computer Science
ETH Zürich
Zurich, Switzerland
thomas.hofmann@inf.ethz.ch

Abstract

Hyperbolic spaces have recently gained momentum in the context of machine
learning due to their high capacity and tree-likeliness properties. However, the
representational power of hyperbolic geometry is not yet on par with Euclidean
geometry, mostly because of the absence of corresponding hyperbolic neural
network layers. This makes it hard to use hyperbolic embeddings in downstream
tasks. Here, we bridge this gap in a principled manner by combining the formalism
of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model
of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep
learning tools: multinomial logistic regression, feed-forward and recurrent neural
networks such as gated recurrent units. This allows to embed sequential data and
perform classiﬁcation in the hyperbolic space. Empirically, we show that, even if
hyperbolic optimization tools are limited, hyperbolic sentence embeddings either
outperform or are on par with their Euclidean variants on textual entailment and
noisy-preﬁx recognition tasks.

1

Introduction

It is common in machine learning to represent data as being embedded in the Euclidean space Rn. The
main reason for such a choice is simply convenience, as this space has a vectorial structure, closed-
form formulas of distance and inner-product, and is the natural generalization of our intuition-friendly,
visual three-dimensional space. Moreover, embedding entities in such a continuous space allows to
feed them as input to neural networks, which has led to unprecedented performance on a broad range
of problems, including sentiment detection [15], machine translation [3], textual entailment [22] or
knowledge base link prediction [20, 6].

Despite the success of Euclidean embeddings, recent research has proven that many types of com-
plex data (e.g. graph data) from a multitude of ﬁelds (e.g. Biology, Network Science, Computer
Graphics or Computer Vision) exhibit a highly non-Euclidean latent anatomy [8]. In such cases, the
Euclidean space does not provide the most powerful or meaningful geometrical representations. For
example, [10] shows that arbitrary tree structures cannot be embedded with arbitrary low distortion
(i.e. almost preserving their metric) in the Euclidean space with unbounded number of dimensions,
but this task becomes surprisingly easy in the hyperbolic space with only 2 dimensions where the
exponential growth of distances matches the exponential growth of nodes with the tree depth.

∗Equal contribution.

The adoption of neural networks and deep learning in these non-Euclidean settings has been rather
limited until very recently, the main reason being the non-trivial or impossible principled general-
izations of basic operations (e.g. vector addition, matrix-vector multiplication, vector translation,
vector inner product) as well as, in more complex geometries, the lack of closed form expressions for
basic objects (e.g. distances, geodesics, parallel transport). Thus, classic tools such as multinomial
logistic regression (MLR), feed forward (FFNN) or recurrent neural networks (RNN) did not have a
correspondence in these geometries.

How should one generalize deep neural models to non-Euclidean domains ? In this paper we address
this question for one of the simplest, yet useful, non-Euclidean domains: spaces of constant negative
curvature, i.e. hyperbolic. Their tree-likeness properties have been extensively studied [12, 13, 26]
and used to visualize large taxonomies [18] or to embed heterogeneous complex networks [17]. In
machine learning, recently, hyperbolic representations greatly outperformed Euclidean embeddings
for hierarchical, taxonomic or entailment data [21, 10, 11]. Disjoint subtrees from the latent hierar-
chical structure surprisingly disentangle and cluster in the embedding space as a simple reﬂection of
the space’s negative curvature. However, appropriate deep learning tools are needed to embed feature
data in this space and use it in downstream tasks. For example, implicitly hierarchical sequence data
(e.g. textual entailment data, phylogenetic trees of DNA sequences or hierarchial captions of images)
would beneﬁt from suitable hyperbolic RNNs.

The main contribution of this paper is to bridge the gap between hyperbolic and Euclidean geometry
in the context of neural networks and deep learning by generalizing in a principled manner both the
basic operations as well as multinomial logistic regression (MLR), feed-forward (FFNN), simple and
gated (GRU) recurrent neural networks (RNN) to the Poincaré model of the hyperbolic geometry.
We do it by connecting the theory of gyrovector spaces and generalized Möbius transformations
introduced by [2, 26] with the Riemannian geometry properties of the manifold. We smoothly
parametrize basic operations and objects in all spaces of constant negative curvature using a uniﬁed
framework that depends only on the curvature value. Thus, we show how Euclidean and hyperbolic
spaces can be continuously deformed into each other. On a series of experiments and datasets we
showcase the effectiveness of our hyperbolic neural network layers compared to their "classic"
Euclidean variants on textual entailment and noisy-preﬁx recognition tasks. We hope that this paper
will open exciting future directions in the nascent ﬁeld of Geometric Deep Learning.

2 The Geometry of the Poincaré Ball

2.1 Basics of Riemannian geometry

We brieﬂy introduce basic concepts of differential geometry largely needed for a principled general-
ization of Euclidean neural networks. For more rigorous and in-depth expositions, see [23, 14].
An n-dimensional manifold M is a space that can locally be approximated by Rn: it is a generalization
to higher dimensions of the notion of a 2D surface. For x ∈ M, one can deﬁne the tangent space
TxM of M at x as the ﬁrst order linear approximation of M around x. A Riemannian metric
g = (gx)x∈M on M is a collection of inner-products gx : TxM × TxM → R varying smoothly
with x. A Riemannian manifold (M, g) is a manifold M equipped with a Riemannian metric g.
Although a choice of a Riemannian metric g on M only seems to deﬁne the geometry locally on M,
it induces global distances by integrating the length (of the speed vector living in the tangent space)
of a shortest path between two points:

(cid:90) 1

(cid:113)

d(x, y) = inf
γ

0

gγ(t)( ˙γ(t), ˙γ(t))dt,

(1)

where γ ∈ C∞([0, 1], M) is such that γ(0) = x and γ(1) = y. A smooth path γ of minimal length
between two points x and y is called a geodesic, and can be seen as the generalization of a straight-line
in Euclidean space. The parallel transport Px→y : TxM → TyM is a linear isometry between
tangent spaces which corresponds to moving tangent vectors along geodesics and deﬁnes a canonical
way to connect tangent spaces. The exponential map expx at x, when well-deﬁned, gives a way to
project back a vector v of the tangent space TxM at x, to a point expx(v) ∈ M on the manifold.
This map is often used to parametrize a geodesic γ starting from γ(0) := x ∈ M with unit-norm
direction ˙γ(0) := v ∈ TxM as t (cid:55)→ expx(tv). For geodesically complete manifolds, such as the
Poincaré ball considered in this work, expx is well-deﬁned on the full tangent space TxM. Finally, a

2

(2)

(3)

(4)

(5)

metric ˜g is said to be conformal to another metric g if it deﬁnes the same angles, i.e.

˜gx(u, v)
(cid:112)˜gx(u, u)(cid:112)˜gx(v, v)

=

gx(u, v)
(cid:112)gx(u, u)(cid:112)gx(v, v)

,

for all x ∈ M, u, v ∈ TxM \ {0}. This is equivalent to the existence of a smooth function
λ : M → R, called the conformal factor, such that ˜gx = λ2

xgx for all x ∈ M.

2.2 Hyperbolic space: the Poincaré ball

The hyperbolic space has ﬁve isometric models that one can work with [9]. Similarly as in [21] and
[11], we choose to work in the Poincaré ball. The Poincaré ball model (Dn, gD) is deﬁned by the
manifold Dn = {x ∈ Rn : (cid:107)x(cid:107) < 1} equipped with the following Riemannian metric:

gD
x = λ2

xgE, where λx :=

2
1 − (cid:107)x(cid:107)2 ,

gE = In being the Euclidean metric tensor. Note that the hyperbolic metric tensor is conformal to
the Euclidean one. The induced distance between two points x, y ∈ Dn is known to be given by

dD(x, y) = cosh−1

1 + 2

(cid:18)

(cid:107)x − y(cid:107)2
(1 − (cid:107)x(cid:107)2)(1 − (cid:107)y(cid:107)2)

(cid:19)

.

Since the Poincaré ball is conformal to Euclidean space, the angle between two vectors u, v ∈
TxDn \ {0} is given by

cos(∠(u, v)) =

gD
x (u, v)
x (u, u)(cid:112)gD

(cid:112)gD

x (v, v)

=

(cid:104)u, v(cid:105)
(cid:107)u(cid:107)(cid:107)v(cid:107)

.

2.3 Gyrovector spaces

In Euclidean space, natural operations inherited from the vectorial structure, such as vector addition,
subtraction and scalar multiplication are often useful. The framework of gyrovector spaces provides
an elegant non-associative algebraic formalism for hyperbolic geometry just as vector spaces provide
the algebraic setting for Euclidean geometry [2, 25, 26].

In particular, these operations are used in special relativity, allowing to add speed vectors belonging
to the Poincaré ball of radius c (the celerity, i.e. the speed of light) so that they remain in the ball,
hence not exceeding the speed of light.

We will make extensive use of these operations in our deﬁnitions of hyperbolic neural networks.
For c ≥ 0, denote2 by Dn
then Dn

c := {x ∈ Rn | c(cid:107)x(cid:107)2 < 1}. Note that if c = 0, then Dn
c. If c = 1 then we recover the usual ball Dn.

c is the open ball of radius 1/

c = Rn; if c > 0,

√

Möbius addition. The Möbius addition of x and y in Dn

c is deﬁned as

x ⊕c y :=

(1 + 2c(cid:104)x, y(cid:105) + c(cid:107)y(cid:107)2)x + (1 − c(cid:107)x(cid:107)2)y
1 + 2c(cid:104)x, y(cid:105) + c2(cid:107)x(cid:107)2(cid:107)y(cid:107)2

.

(6)

In particular, when c = 0, one recovers the Euclidean addition of two vectors in Rn. Note that
without loss of generality, the case c > 0 can be reduced to c = 1. Unless stated otherwise, we
will use ⊕ as ⊕1 to simplify notations. For general c > 0, this operation is not commutative nor
associative. However, it satisﬁes x ⊕c 0 = 0 ⊕c x = 0. Moreover, for any x, y ∈ Dn
c , we have
(−x) ⊕c x = x ⊕c (−x) = 0 and (−x) ⊕c (x ⊕c y) = y (left-cancellation law). The Möbius
substraction is then deﬁned by the use of the following notation: x (cid:9)c y := x ⊕c (−y). See [29,
section 2.1] for a geometric interpretation of the Möbius addition.

2We take different notations as in [25] where the author uses s = 1/

c.

√

3

Möbius scalar multiplication. For c > 0, the Möbius scalar multiplication of x ∈ Dn
r ∈ R is deﬁned as

c \ {0} by

r ⊗c x := (1/

c) tanh(r tanh−1(

c(cid:107)x(cid:107)))

√

√

x
(cid:107)x(cid:107)

,

(7)

and r ⊗c 0 := 0. Note that similarly as for the Möbius addition, one recovers the Euclidean scalar
multiplication when c goes to zero: limc→0 r ⊗c x = rx. This operation satisﬁes desirable properties
such as n ⊗c x = x ⊕c · · · ⊕c x (n additions), (r + r(cid:48)) ⊗c x = r ⊗c x ⊕c r(cid:48) ⊗c x (scalar distributivity3),
(rr(cid:48)) ⊗c x = r ⊗c (r(cid:48) ⊗c x) (scalar associativity) and |r| ⊗c x/(cid:107)r ⊗c x(cid:107) = x/(cid:107)x(cid:107) (scaling property).

c , gc) is given by4

Distance.
Euclidean one, with conformal factor λc
(Dn

If one deﬁnes the generalized hyperbolic metric tensor gc as the metric conformal to the
x := 2/(1 − c(cid:107)x(cid:107)2), then the induced distance function on
√

c) tanh−1 (cid:0)√
Again, observe that limc→0 dc(x, y) = 2(cid:107)x − y(cid:107), i.e. we recover Euclidean geometry in the limit5.
Moreover, for c = 1 we recover dD of Eq. (4).

c(cid:107) − x ⊕c y(cid:107)(cid:1) .

dc(x, y) = (2/

(8)

Hyperbolic trigonometry. Similarly as in the Euclidean space, one can deﬁne the notions of
hyperbolic angles or gyroangles (when using the ⊕c), as well as hyperbolic law of sines in the
generalized Poincaré ball (Dn

c , gc). We make use of these notions in our proofs. See Appendix A.

2.4 Connecting Gyrovector spaces and Riemannian geometry of the Poincaré ball

In this subsection, we present how geodesics in the Poincaré ball model are usually described with
Möbius operations, and push one step further the existing connection between gyrovector spaces and
the Poincaré ball by ﬁnding new identities involving the exponential map, and parallel transport.

In particular, these ﬁndings provide us with a simpler formulation of Möbius scalar multiplication,
yielding a natural deﬁnition of matrix-vector multiplication in the Poincaré ball.

Riemannian gyroline element. The Riemannian gyroline element is deﬁned for an inﬁnitesimal
dx as ds := (x + dx) (cid:9)c x, and its size is given by [26, section 3.7]:

(cid:107)ds(cid:107) = (cid:107)(x + dx) (cid:9)c x(cid:107) = (cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2).

(9)

What is remarkable is that it turns out to be identical, up to a scaling factor of 2, to the usual line
element 2(cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2) of the Riemannian manifold (Dn

c , gc).

Geodesics. The geodesic connecting points x, y ∈ Dn

c is shown in [2, 26] to be given by:

γx→y(t) := x ⊕c (−x ⊕c y) ⊗c t, with γx→y : R → Dn

c s.t. γx→y(0) = x and γx→y(1) = y.

Note that when c goes to 0, geodesics become straight-lines, recovering Euclidean geometry. In the
remainder of this subsection, we connect the gyrospace framework with Riemannian geometry.
Lemma 1. For any x ∈ Dn and v ∈ TxDn
c s.t. gc
x with direction v is given by:

x(v, v) = 1, the unit-speed geodesic starting from

γx,v(t) = x ⊕c

tanh

(cid:18)

(cid:18)√

(cid:19) v
√

c

t
2

c(cid:107)v(cid:107)

(cid:19)

, where γx,v : R → Dn s.t. γx,v(0) = x and ˙γx,v(0) = v.

(10)

(11)

Proof. One can use Eq. (10) and reparametrize it to unit-speed using Eq. (8). Alternatively, direct
computation and identiﬁcation with the formula in [11, Thm. 1] would give the same result. Using
Eq. (8) and Eq. (11), one can sanity-check that dc(γ(0), γ(t)) = t, ∀t ∈ [0, 1].

3⊗c has priority over ⊕c in the sense that a ⊗c b ⊕c c := (a ⊗c b) ⊕c c and a ⊕c b ⊗c c := a ⊕c (b ⊗c c).
4The notation −x ⊕c y should always be read as (−x) ⊕c y and not −(x ⊕c y).
5The factor 2 comes from the conformal factor λx = 2/(1 − (cid:107)x(cid:107)2), which is a convention setting the

curvature to −1.

4

Exponential and logarithmic maps. The following lemma gives the closed-form derivation of
exponential and logarithmic maps.
Lemma 2. For any point x ∈ Dn
map logc
c → TxDn
(cid:18)

c are given for v (cid:54)= 0 and y (cid:54)= x by:
(cid:18)√

c , the exponential map expc

c and the logarithmic

x : TxDn

c → Dn

x : Dn

√

(cid:19)

, logc

x(y) =

√

tanh−1(

c(cid:107) − x ⊕c y(cid:107))

expc

x(v) = x ⊕c

tanh

c

λc
x(cid:107)v(cid:107)
2

(cid:19) v
√

c(cid:107)v(cid:107)

2
cλc
x

−x ⊕c y
(cid:107) − x ⊕c y(cid:107)
(12)

.

Proof. Following the proof of [11, Cor. 1.1], one gets expc
gives the formula for expc

x. Algebraic check of the identity logc

x(v) = γx,
x(expc

v

x(cid:107)v(cid:107) (λc
λc

x(cid:107)v(cid:107)). Using Eq. (11)

x(v)) = v concludes.

The above maps have more appealing forms when x = 0, namely for v ∈ T0Dn

c \ {0}, y ∈ Dn

c \ {0}:

expc

0(v) = tanh(

c(cid:107)v(cid:107))

√

, logc

0(y) = tanh−1(

c(cid:107)y(cid:107))

√

(13)

√

y
c(cid:107)y(cid:107)

.

√

v
c(cid:107)v(cid:107)

Moreover, we still recover Euclidean geometry in the limit c → 0, as limc→0 expc
Euclidean exponential map, and limc→0 logc

x(y) = y − x is the Euclidean logarithmic map.

x(v) = x + v is the

Möbius scalar multiplication using exponential and logarithmic maps. We studied the expo-
nential and logarithmic maps in order to gain a better understanding of the Möbius scalar multiplica-
tion (Eq. (7)). We found the following:
Lemma 3. The quantity r ⊗ x can actually be obtained by projecting x in the tangent space at 0
with the logarithmic map, multiplying this projection by the scalar r in T0Dn
c , and then projecting it
back on the manifold with the exponential map:
0(r logc

∀r ∈ R, x ∈ Dn
c .

r ⊗c x = expc

0(x)),

(14)

In addition, we recover the well-known relation between geodesics connecting two points and the
exponential map:

γx→y(t) = x ⊕c (−x ⊕c y) ⊗c t = expc

x(t logc

x(y)),

t ∈ [0, 1].

(15)

This last result enables us to generalize scalar multiplication in order to deﬁne matrix-vector multipli-
cation between Poincaré balls, one of the essential building blocks of hyperbolic neural networks.

Parallel transport. Finally, we connect parallel transport (from T0Dn
the following theorem, which we prove in appendix B.
Theorem 4. In the manifold (Dn
vector v ∈ T0Dn

c to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a
c is given by the following isometry:
λc
0
λc
x

x(x ⊕c expc

0(v)) =

0→x(v) = logc
P c

(16)

v.

c ) to gyrovector spaces with

As we’ll see later, this result is crucial in order to deﬁne and optimize parameters shared between
different tangent spaces, such as biases in hyperbolic neural layers or parameters of hyperbolic MLR.

3 Hyperbolic Neural Networks

Neural networks can be seen as being made of compositions of basic operations, such as linear
maps, bias translations, pointwise non-linearities and a ﬁnal sigmoid or softmax layer. We ﬁrst
explain how to construct a softmax layer for logits lying in a Poincaré ball. Then, we explain how
to transform a mapping between two Euclidean spaces as one between Poincaré balls, yielding
matrix-vector multiplication and pointwise non-linearities in the Poincaré ball. Finally, we present
possible adaptations of various recurrent neural networks to the hyperbolic domain.

5

3.1 Hyperbolic multiclass logistic regression

In order to perform multi-class classiﬁcation on the Poincaré ball, one needs to generalize multinomial
logistic regression (MLR) − also called softmax regression − to the Poincaré ball.

Reformulating Euclidean MLR. Let’s ﬁrst reformulate Euclidean MLR from the perspective of
distances to margin hyperplanes, as in [19, Section 5]. This will allow us to easily generalize it.

Given K classes, one learns a margin hyperplane for each such class using softmax probabilities:

∀k ∈ {1, ..., K},

p(y = k|x) ∝ exp (((cid:104)ak, x(cid:105) − bk)) , where bk ∈ R, x, ak ∈ Rn.

(17)

Note that any afﬁne hyperplane in Rn can be written with a normal vector a and a scalar shift b:

Ha,b = {x ∈ Rn : (cid:104)a, x(cid:105) − b = 0}, where a ∈ Rn \ {0}, and b ∈ R.

(18)

As in [19, Section 5], we note that (cid:104)a, x(cid:105) − b = sign((cid:104)a, x(cid:105) − b)(cid:107)a(cid:107)d(x, Ha,b). Using Eq. (17):

p(y = k|x) ∝ exp(sign((cid:104)ak, x(cid:105) − bk)(cid:107)ak(cid:107)d(x, Hak,bk )), bk ∈ R, x, ak ∈ Rn.

(19)

As it is not immediately obvious how to generalize the Euclidean hyperplane of Eq. (18) to other
spaces such as the Poincaré ball, we reformulate it as follows:

˜Ha,p = {x ∈ Rn : (cid:104)−p + x, a(cid:105) = 0} = p + {a}⊥, where p ∈ Rn, a ∈ Rn \ {0}.

(20)

This new deﬁnition relates to the previous one as ˜Ha,p = Ha,(cid:104)a,p(cid:105). Rewriting Eq. (19) with b = (cid:104)a, p(cid:105):
p(y = k|x) ∝ exp(sign((cid:104)−pk + x, ak(cid:105))(cid:107)ak(cid:107)d(x, ˜Hak,pk )), with pk, x, ak ∈ Rn.

(21)

It is now natural to adapt the previous deﬁnition to the hyperbolic setting by replacing + by ⊕c:
Deﬁnition 3.1 (Poincaré hyperplanes). For p ∈ Dn
p(z, a) = 0} = {z ∈ TpDn
gc

c : (cid:104)z, a(cid:105) = 0}. Then, we deﬁne Poincaré hyperplanes as

c \ {0}, let {a}⊥ := {z ∈ TpDn
c :

c , a ∈ TpDn

˜H c

a,p := {x ∈ Dn

c : (cid:104)logc

p(x), a(cid:105)p = 0} = expc

p({a}⊥) = {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}.

(22)

The last equality is shown appendix C. ˜H c
all geodesics in Dn
hypergyroplanes, see [27, deﬁnition 5.8]. A 3D hyperplane example is depicted in Fig. 1.

a,p can also be described as the union of images of
c orthogonal to a and containing p. Notice that our deﬁnition matches that of

Next, we need the following theorem, proved in appendix D:
Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(23)

Final formula for MLR in the Poincaré ball. Putting together Eq. (21) and Thm. 5, we get the
hyperbolic MLR formulation. Given K classes and k ∈ {1, . . . , K}, pk ∈ Dn
c \ {0}:

c , ak ∈ Tpk

Dn

p(y = k|x) ∝ exp(sign((cid:104)−pk ⊕c x, ak(cid:105))

gc
pk

(ak, ak)dc(x, ˜H c

)),

ak,pk

∀x ∈ Dn
c ,

(24)

or, equivalently

p(y = k|x) ∝ exp

(cid:18) λc
pk

(cid:107)ak(cid:107)
√
c

sinh−1

(cid:18)

√
2

c(cid:104)−pk ⊕c x, ak(cid:105)

(cid:19)(cid:19)

(1 − c(cid:107) − pk ⊕c x(cid:107)2)(cid:107)ak(cid:107)

,

∀x ∈ Dn
c .

(25)

this goes to p(y = k|x) ∝ exp(4(cid:104)−pk + x, ak(cid:105)) =

Notice that when c goes to zero,
exp((λ0
pk

)2(cid:104)−pk + x, ak(cid:105)) = exp((cid:104)−pk + x, ak(cid:105)0), recovering the usual Euclidean softmax.
However, at this point it is unclear how to perform optimization over ak, since it lives in Tpk
hence depends on pk. The solution is that one should write ak = P c
)a(cid:48)
k ∈ T0Dn
a(cid:48)

c = Rn, and optimize a(cid:48)

k as a Euclidean parameter.

k) = (λc

0/λc
pk

0→pk

(a(cid:48)

Dn
c and
k, where

1
√
c

(cid:113)

6

3.2 Hyperbolic feed-forward layers

In order to deﬁne hyperbolic neural networks, it is crucial to de-
ﬁne a canonically simple parametric family of transformations,
playing the role of linear mappings in usual Euclidean neural
networks, and to know how to apply pointwise non-linearities.
Inspiring ourselves from our reformulation of Möbius scalar
multiplication in Eq. (14), we deﬁne:
Deﬁnition 3.2 (Möbius version). For f : Rn → Rm, we deﬁne
the Möbius version of f as the map from Dn

c to Dm

c by:

f ⊗c(x) := expc

0(f (logc

0(x))),

(26)

where expc

0 : T0m

Dm

c → Dm

c and logc

0 : Dn

c → T0n

Dn
c .

Figure 1: An example of a hyper-
bolic hyperplane in D3
1 plotted us-
ing sampling. The red point is p.
The shown normal axis to the hy-
perplane through p is parallel to a.

Note that similarly as for other Möbius operations, we recover
the Euclidean mapping in the limit c → 0 if f is continuous, as limc→0 f ⊗c(x) = f (x). This
deﬁnition satisﬁes a few desirable properties too, such as: (f ◦ g)⊗c = f ⊗c ◦ g⊗c for f : Rm → Rl
and g : Rn → Rm (morphism property), and f ⊗c(x)/(cid:107)f ⊗c(x)(cid:107) = f (x)/(cid:107)f (x)(cid:107) for f (x) (cid:54)= 0
(direction preserving). It is then straight-forward to prove the following result:
Lemma 6 (Möbius matrix-vector multiplication). If M : Rn → Rm is a linear map, which we
identify with its matrix representation, then ∀x ∈ Dn

c , if M x (cid:54)= 0 we have

M ⊗c(x) = (1/

c) tanh

√

(cid:18) (cid:107)M x(cid:107)
(cid:107)x(cid:107)

√

tanh−1(

c(cid:107)x(cid:107))

(cid:19) M x
(cid:107)M x(cid:107)

,

(27)

and M ⊗c(x) = 0 if M x = 0. Moreover, if we deﬁne the Möbius matrix-vector multiplication of
M ∈ Mm,n(R) and x ∈ Dn
c by M ⊗c x := M ⊗c(x), then we have (M M (cid:48)) ⊗c x = M ⊗c (M (cid:48) ⊗c x)
for M ∈ Ml,m(R) and M (cid:48) ∈ Mm,n(R) (matrix associativity), (rM ) ⊗c x = r ⊗c (M ⊗c x) for
r ∈ R and M ∈ Mm,n(R) (scalar-matrix associativity) and M ⊗c x = M x for all M ∈ On(R)
(rotations are preserved).

Pointwise non-linearity.
ϕ⊗c can be applied to elements of the Poincaré ball.

If ϕ : Rn → Rn is a pointwise non-linearity, then its Möbius version

Bias translation. The generalization of a translation in the Poincaré ball is naturally given by
moving along geodesics. But should we use the Möbius sum x ⊕c b with a hyperbolic bias b or the
x(b(cid:48)) with a Euclidean bias b(cid:48)? These views are uniﬁed with parallel transport
exponential map expc
c by a bias b ∈ Dn
(see Thm 4). Möbius translation of a point x ∈ Dn
(cid:18) λc
0
λc
x

c is given by
(cid:19)

x ← x ⊕c b = expc

0(b))) = expc
x

0→x(logc

x(P c

logc

0(b)

(28)

.

We recover Euclidean translations in the limit c → 0. Note that bias translations play a particular
Indeed, consider multiple layers of the form fk(x) = ϕk(Mkx), each of
role in this model.
which having Möbius version f ⊗c
k (Mk ⊗c x). Then their composition can be re-written
f ⊗c
k ◦ · · · ◦ f ⊗c
1 = expc
0. This means that these operations can essentially be
performed in Euclidean space. Therefore, it is the interposition between those with the bias translation
of Eq. (28) which differentiates this model from its Euclidean counterpart.

k (x) = ϕ⊗c
0 ◦fk ◦ · · · ◦ f1 ◦ logc

If a vector x ∈ Rn+p is the (vertical) concatenation
Concatenation of multiple input vectors.
of two vectors x1 ∈ Rn, x2 ∈ Rp, and M ∈ Mm,n+p(R) can be written as the (horizontal)
concatenation of two matrices M1 ∈ Mm,n(R) and M2 ∈ Mm,p(R), then M x = M1x1 + M2x2.
We generalize this to hyperbolic spaces: if we are given x1 ∈ Dn
c ×Dp
c ,
and M, M1, M2 as before, then we deﬁne M ⊗c x := M1 ⊗c x1 ⊕c M2 ⊗c x2. Note that when c goes
to zero, we recover the Euclidean formulation, as limc→0 M ⊗c x = limc→0 M1 ⊗c x1 ⊕c M2 ⊗c x2 =
M1x1 + M2x2 = M x. Moreover, hyperbolic vectors x ∈ Dn
c can also be "concatenated" with real
features y ∈ R by doing: M ⊗c x ⊕c y ⊗c b with learnable b ∈ Dm

c , x = (x1 x2)T ∈ Dn

c and M ∈ Mm,n(R).

c , x2 ∈ Dp

7

3.3 Hyperbolic RNN

Naive RNN. A simple RNN can be deﬁned by ht+1 = ϕ(W ht + U xt + b) where ϕ is a pointwise
non-linearity, typically tanh, sigmoid, ReLU, etc. This formula can be naturally generalized to the
hyperbolic space as follows. For parameters W ∈ Mm,n(R), U ∈ Mm,d(R), b ∈ Dm
c , we deﬁne:

ht+1 = ϕ⊗c (W ⊗c ht ⊕c U ⊗c xt ⊕c b),

ht ∈ Dn

c , xt ∈ Dd
c .

(29)

Note that if inputs xt’s are Euclidean, one can write ˜xt := expc
expc

(U xt)) = W ⊗c ht ⊕c expc

(P c

W ⊗cht

0→W ⊗cht

0(U xt) = W ⊗c ht ⊕c U ⊗c ˜xt.

0(xt) and use the above formula, since

GRU architecture. One can also adapt the GRU architecture:
rt = σ(W rht−1 + U rxt + br),
zt = σ(W zht−1 + U zxt + bz),
˜ht = ϕ(W (rt (cid:12) ht−1) + U xt + b), ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht,

(30)

where (cid:12) denotes pointwise product. First, how should we adapt the pointwise multiplication by a
scaling gate? Note that the deﬁnition of the Möbius version (see Eq. (26)) can be naturally extended
to maps f : Rn × Rp → Rm as f ⊗c : (h, h(cid:48)) ∈ Dn
0(h(cid:48)))). In
c (cid:55)→ expc
0(h(cid:48))) =
particular, choosing f (h, h(cid:48)) := σ(h) (cid:12) h(cid:48) yields6 f ⊗c(h, h(cid:48)) = expc
diag(σ(logc

0(h))) ⊗c h(cid:48). Hence we adapt rt (cid:12) ht−1 to diag(rt) ⊗c ht−1 and the reset gate rt to:
0(W r ⊗c ht−1 ⊕c U r ⊗c xt ⊕c br),

0(h), logc
0(h)) (cid:12) logc

0(f (logc
0(σ(logc

rt = σ logc

c × Dp

(31)

and similarly for the update gate zt. Note that as the argument of σ in the above is unbounded, rt and
zt can a priori take values onto the full range (0, 1). Now the intermediate hidden state becomes:
˜ht = ϕ⊗c ((W diag(rt)) ⊗c ht−1 ⊕c U ⊗c xt ⊕ b),

(32)

where Möbius matrix associativity simpliﬁes W ⊗c (diag(rt) ⊗c ht−1) into (W diag(rt)) ⊗c ht−1.
Finally, we propose to adapt the update-gate equation as

ht = ht−1 ⊕c diag(zt) ⊗c (−ht−1 ⊕c

˜ht).

(33)

Note that when c goes to zero, one recovers the usual GRU. Moreover, if zt = 0 or zt = 1, then ht
becomes ht−1 or ˜ht respectively, similarly as in the usual GRU. This adaptation was obtained by
adapting [24]: in this work, the authors re-derive the update-gate mechanism from a ﬁrst principle
called time-warping invariance. We adapted their derivation to the hyperbolic setting by using the
notion of gyroderivative [4] and proving a gyro-chain-rule (see appendix E).

4 Experiments

SNLI task and dataset. We evaluate our method on two tasks. The ﬁrst is natural language
inference, or textual entailment. Given two sentences, a premise (e.g. "Little kids A. and B. are
playing soccer.") and a hypothesis (e.g. "Two children are playing outdoors."), the binary classiﬁcation
task is to predict whether the second sentence can be inferred from the ﬁrst one. This deﬁnes a partial
order in the sentence space. We test hyperbolic networks on the biggest real dataset for this task,
SNLI [7]. It consists of 570K training, 10K validation and 10K test sentence pairs. Following [28],
we merge the "contradiction" and "neutral" classes into a single class of negative sentence pairs, while
the "entailment" class gives the positive pairs.

PREFIX task and datasets. We conjecture that the improvements of hyperbolic neural networks
are more signiﬁcant when the underlying data structure is closer to a tree. To test this, we design a
proof-of-concept task of detection of noisy preﬁxes, i.e. given two sentences, one has to decide if the
second sentence is a noisy preﬁx of the ﬁrst, or a random sentence. We thus build synthetic datasets
PREFIX-Z% (for Z being 10, 30 or 50) as follows: for each random ﬁrst sentence of random length
at most 20 and one random preﬁx of it, a second positive sentence is generated by randomly replacing
Z% of the words of the preﬁx, and a second negative sentence of same length is randomly generated.
Word vocabulary size is 100, and we generate 500K training, 10K validation and 10K test pairs.

6If x has n coordinates, then diag(x) denotes the diagonal matrix of size n with xi’s on its diagonal.

8

Models architecture. Our neural network layers can be used in a plug-n-play manner exactly like
standard Euclidean layers. They can also be combined with Euclidean layers. However, optimization
w.r.t. hyperbolic parameters is different (see below) and based on Riemannian gradients which
are just rescaled Euclidean gradients when working in the conformal Poincaré model [21]. Thus,
back-propagation can be applied in the standard way.

In our setting, we embed the two sentences using two distinct hyperbolic RNNs or GRUs. The
sentence embeddings are then fed together with their squared distance (hyperbolic or Euclidean,
depending on their geometry) to a FFNN (Euclidean or hyperbolic, see Sec. 3.2) which is further
fed to an MLR (Euclidean or hyperbolic, see Sec. 3.1) that gives probabilities of the two classes
(entailment vs neutral). We use cross-entropy loss on top. Note that hyperbolic and Euclidean layers
can be mixed, e.g. the full network can be hyperbolic and only the last layer be Euclidean, in which
case one has to use log0 and exp0 functions to move between the two manifolds in a correct manner
as explained for Eq. 26.

Optimization. Our models have both Euclidean (e.g. weight matrices in both Euclidean and
hyperbolic FFNNs, RNNs or GRUs) and hyperbolic parameters (e.g. word embeddings or biases for
the hyperbolic layers). We optimize the Euclidean parameters with Adam [16] (learning rate 0.001).
Hyperbolic parameters cannot be updated with an equivalent method that keeps track of gradient
history due to the absence of a Riemannian Adam. Thus, they are optimized using full Riemannian
stochastic gradient descent (RSGD) [5, 11]. We also experiment with projected RSGD [21], but
optimization was sometimes less stable. We use a different constant learning rate for word embeddings
(0.1) and other hyperbolic weights (0.01) because words are updated less frequently.

Numerical errors. Gradients of the basic operations deﬁned above (e.g. ⊕c, exponential map) are
c(cid:107)x(cid:107) = 1. Thus, we
not deﬁned when the hyperbolic argument vectors are on the ball border, i.e.
always project results of these operations in the ball of radius 1 − (cid:15), where (cid:15) = 10−5. Numerical
errors also appear when hyperbolic vectors get closer to 0, thus we perturb them with an (cid:15)(cid:48) = 10−15
before they are used in any of the above operations. Finally, arguments of the tanh function are
clipped between ±15 to avoid numerical errors, while arguments of tanh−1 are clipped to at most
1 − 10−5.

√

Hyperparameters. For all methods, baselines and datasets, we use c = 1, word and hidden state
embedding dimension of 5 (we focus on the low dimensional setting that was shown to already
be effective [21]), batch size of 64. We ran all methods for a ﬁxed number of 30 epochs. For all
models, we experiment with both identity (no non-linearity) or tanh non-linearity in the RNN/GRU
cell, as well as identity or ReLU after the FFNN layer and before MLR. As expected, for the fully
Euclidean models, tanh and ReLU respectively surpassed the identity variant by a large margin. We
only report the best Euclidean results. Interestingly, for the hyperbolic models, using only identity for
both non-linearities works slightly better and this is likely due to two facts: i) our hyperbolic layers
already contain non-linearities by their nature, ii) tanh is limiting the output domain of the sentence
embeddings, but the hyperbolic speciﬁc geometry is more pronounced at the ball border, i.e. at the
hyperbolic "inﬁnity", compared to the center of the ball.

For the results shown in Tab. 1, we run each model (baseline or ours) exactly 3 times and report the
test result corresponding to the best validation result from these 3 runs. We do this because the highly
non-convex spectrum of hyperbolic neural networks sometimes results in convergence to poor local
minima, suggesting that initialization is very important.

Results. Results are shown in Tab. 1. Note that the fully Euclidean baseline models might have
an advantage over hyperbolic baselines because more sophisticated optimization algorithms such
as Adam do not have a hyperbolic analogue at the moment. We ﬁrst observe that all GRU models
overpass their RNN variants. Hyperbolic RNNs and GRUs have the most signiﬁcant improvement
over their Euclidean variants when the underlying data structure is more tree-like, e.g. for PREFIX-
10% − for which the tree relation between sentences and their preﬁxes is more prominent − we
reduce the error by a factor of 3.35 for hyperbolic vs Euclidean RNN, and by a factor of 1.5 for
hyperbolic vs Euclidean GRU. As soon as the underlying structure diverges more and more from
a tree, the accuracy gap decreases − for example, for PREFIX-50% the noise heavily affects the
representational power of hyperbolic networks. Also, note that on SNLI our methods perform
similarly as with their Euclidean variants. Moreover, hyperbolic and Euclidean MLR are on par when

9

SNLI

PREFIX-10% PREFIX-30% PREFIX-50%

FULLY EUCLIDEAN RNN
HYPERBOLIC RNN+FFNN, EUCL MLR
FULLY HYPERBOLIC RNN
FULLY EUCLIDEAN GRU
HYPERBOLIC GRU+FFNN, EUCL MLR
FULLY HYPERBOLIC GRU

79.34 %
79.18 %
78.21 %
81.52 %
79.76 %
81.19 %

89.62 %
96.36 %
96.91 %
95.96 %
97.36 %
97.14 %

81.71 %
87.83 %
87.25 %
86.47 %
88.47 %
88.26 %

72.10 %
76.50 %
62.94 %
75.04 %
76.87 %
76.44 %

Table 1: Test accuracies for various models and four datasets. "Eucl" denotes Euclidean. All word
and sentence embeddings have dimension 5. We highlight in bold the best baseline (or baselines, if
the difference is less than 0.5%).

used in conjunction with hyperbolic sentence embeddings, suggesting further empirical investigation
is needed for this direction (see below).

We also observe that, in the hyperbolic setting, accuracy tends to increase when sentence embeddings
start increasing, and gets better as their norms converge towards 1 (the ball border for c = 1). Unlike
in the Euclidean case, this behavior does happen only after a few epochs and suggests that the model
should ﬁrst adjust the angular layout in order to disentangle the representations, before increasing their
norms to fully exploit the strong clustering property of the hyperbolic geometry. Similar behavior
was observed in the context of embedding trees by [21]. Details in appendix F.

MLR classiﬁcation experiments.
For the sentence entailment classi-
ﬁcation task we do not see a clear
advantage of hyperbolic MLR com-
pared to its Euclidean variant. A pos-
sible reason is that, when trained end-
to-end, the model might decide to
place positive and negative embed-
dings in a manner that is already well
separated with a classic MLR. As a
consequence, we further investigate
MLR for the task of subtree classiﬁ-
cation. Using an open source imple-
mentation7 of [21], we pre-trained
Poincaré embeddings of the Word-
Net noun hierarchy (82,115 nodes).
We then choose one node in this tree
(see Table 2) and classify all other
nodes (solely based on their embed-
dings) as being part of the subtree
rooted at this node. All nodes in such a subtree are divided into positive training nodes (80%) and
positive test nodes (20%). The same splitting procedure is applied for the remaining WordNet nodes
that are divided into a negative training and negative test set respectively. Three variants of MLR
are then trained on top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation
task: hyperbolic MLR, Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean
MLR applied after mapping all embeddings in the tangent space at 0 using the log0 map. We use
different embedding dimensions : 2, 3, 5 and 10. For the hyperbolic MLR, we use full Riemannian
SGD with a learning rate of 0.001. For the two Euclidean models we use ADAM optimizer and the
same learning rate. During training, we always sample the same number of negative and positive
nodes in each minibatch of size 16; thus positive nodes are frequently resampled. All methods are
trained for 30 epochs and the ﬁnal F1 score is reported (no hyperparameters to validate are used, thus
we do not require a validation set). This procedure is repeated for four subtrees of different sizes.

Figure 2: Hyperbolic (left) vs Direct Euclidean (right) binary
MLR used to classify nodes as being part in the GROUP.N.01
subtree of the WordNet noun hierarchy solely based on their
Poincaré embeddings. The positive points (from the subtree)
are in blue, the negative points (the rest) are in red and the
trained positive separation hyperplane is depicted in green.

Quantitative results are presented in Table 2. We can see that the hyperbolic MLR overpasses
its Euclidean variants in almost all settings, sometimes by a large margin. Moreover, to provide

7https://github.com/dalab/hyperbolic_cones

10

WORDNET
SUBTREE

ANIMAL.N.01
3218 / 798

GROUP.N.01
6649 / 1727

WORKER.N.01
861 / 254

MAMMAL.N.01
953 / 228

MODEL

D = 2

D = 3

D = 5

D = 10

HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL

47.43 ± 1.07%
41.69 ± 0.19%
38.89 ± 0.01%
81.72 ± 0.17%
61.13 ± 0.42%
60.75 ± 0.24%
12.68 ± 0.82%
10.86 ± 0.01%
9.04 ± 0.06%
32.01 ± 17.14%
15.58 ± 0.04%
13.10 ± 0.13%

91.92 ± 0.61%
68.43 ± 3.90%
62.57 ± 0.61%
89.87 ± 2.73%
63.56 ± 1.22%
61.98 ± 0.57%
24.09 ± 1.49%
22.39 ± 0.04%
22.57 ± 0.20%
87.54 ± 4.55%
44.68 ± 1.87%
44.89 ± 1.18%

98.07 ± 0.55%
95.59 ± 1.18%
89.21 ± 1.34%
87.89 ± 0.80%
67.82 ± 0.81%
67.92 ± 0.74%
55.46 ± 5.49%
35.23 ± 3.16%
26.47 ± 0.78%
88.73 ± 3.22%
59.35 ± 1.31%
52.51 ± 0.85%

99.26 ± 0.59%
99.36 ± 0.18%
98.27 ± 0.70%
91.91 ± 3.07%
91.38 ± 1.19%
91.41 ± 0.18%
66.83 ± 11.38%
47.29 ± 3.93%
36.66 ± 2.74%
91.37 ± 6.09%
77.76 ± 5.08%
56.11 ± 2.21%

Table 2: Test F1 classiﬁcation scores for four different subtrees of WordNet noun tree. All nodes
in such a subtree are divided into positive training nodes (80%) and positive test nodes (20%);
these counts are shown below each subtree root. The same splitting procedure is applied for the
remaining nodes to obtain negative training and test sets. Three variants of MLR are then trained on
top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation task: hyperbolic MLR,
Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean MLR applied after
mapping all embeddings in the tangent space at 0 using the log0 map. 95% conﬁdence intervals for 3
different runs are shown for each method and each different embedding dimension (2, 3, 5 or 10).

further understanding, we plot the 2-dimensional embeddings and the trained separation hyperplanes
(geodesics in this case) in Figure 2. We can see that respecting the hyperbolic geometry is very
important for a quality classiﬁcation model.

5 Conclusion

We showed how classic Euclidean deep learning tools such as MLR, FFNNs, RNNs or GRUs can be
generalized in a principled manner to all spaces of constant negative curvature combining Riemannian
geometry with the elegant theory of gyrovector spaces. Empirically we found that our models
outperform or are on par with corresponding Euclidean architectures on sequential data with implicit
hierarchical structure. We hope to trigger exciting future research related to better understanding
of the hyperbolic non-convexity spectrum and development of other non-Euclidean deep learning
methods.
Our data and Tensorﬂow [1] code are publicly available8.

Acknowledgements

We thank Igor Petrovski for useful pointers regarding the implementation.

This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement
number 167176. Gary Bécigneul is also funded by the Max Planck ETH Center for Learning
Systems.

References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for
large-scale machine learning. 2016.

[2] Ungar Abraham Albert. Analytic hyperbolic geometry and Albert Einstein’s special theory of

relativity. World scientiﬁc, 2008.

8https://github.com/dalab/hyperbolic_nn

11

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of the International Conference on Learning
Representations (ICLR), 2015.

[4] Graciela S Birman and Abraham A Ungar. The hyperbolic derivative in the poincaré ball model
of hyperbolic geometry. Journal of mathematical analysis and applications, 254(1):321–333,
2001.

[5] S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on

Automatic Control, 58(9):2217–2229, Sept 2013.

[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems (NIPS), pages 2787–2795, 2013.

[7] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 632–642. Association
for Computational Linguistics, 2015.

[8] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,
34(4):18–42, 2017.

[9] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry.

Flavors of geometry, 31:59–115, 1997.

[10] Christopher De Sa, Albert Gu, Christopher Ré, and Frederic Sala. Representation tradeoffs for

hyperbolic embeddings. arXiv preprint arXiv:1804.03329, 2018.

[11] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In Proceedings of the thirty-ﬁfth international conference
on machine learning (ICML), 2018.

[12] Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pages 75–263. Springer, 1987.

[13] Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the

Cambridge Philosophical Society, page 1–17, 2017.

[14] Christopher Hopper and Ben Andrews. The Ricci ﬂow in Riemannian geometry. Springer, 2010.

[15] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1746–1751. Association for Computational Linguistics, 2014.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations (ICLR), 2015.

[17] Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marián Boguná.

Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.

[18] John Lamping, Ramana Rao, and Peter Pirolli. A focus+ context technique based on hyperbolic
geometry for visualizing large hierarchies. In Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 401–408. ACM Press/Addison-Wesley Publishing Co.,
1995.

[19] Guy Lebanon and John Lafferty. Hyperplane margin classiﬁers on the multinomial manifold. In
Proceedings of the international conference on machine learning (ICML), page 66. ACM, 2004.

[20] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective
learning on multi-relational data. In Proceedings of the international conference on machine
learning (ICML), volume 11, pages 809–816, 2011.

12

[21] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical repre-
sentations. In Advances in Neural Information Processing Systems (NIPS), pages 6341–6350,
2017.

[22] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blun-
som. Reasoning about entailment with neural attention. In Proceedings of the International
Conference on Learning Representations (ICLR), 2015.

[23] Michael Spivak. A comprehensive introduction to differential geometry. Publish or perish, 1979.

[24] Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In Proceedings of

the International Conference on Learning Representations (ICLR), 2018.

[25] Abraham A Ungar. Hyperbolic trigonometry and its application in the poincaré ball model of

hyperbolic geometry. Computers & Mathematics with Applications, 41(1-2):135–147, 2001.

[26] Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis

Lectures on Mathematics and Statistics, 1(1):1–194, 2008.

[27] Abraham Albert Ungar. Analytic hyperbolic geometry in n dimensions: An introduction. CRC

Press, 2014.

[28] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. In Proceedings of the International Conference on Learning Representations (ICLR),
2016.

[29] J Vermeer. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic

plane. Topology and its Applications, 152(3):226–242, 2005.

13

A Hyperbolic Trigonometry

Hyperbolic angles. For A, B, C ∈ Dn
c , we denote by ∠A := ∠BAC the angle between the two
geodesics starting from A and ending at B and C respectively. This angle can be deﬁned in two
equivalent ways: i) either using the angle between the initial velocities of the two geodesics as given
by Eq. 5, or ii) using the formula

cos(∠A) =

(cid:28) (−A) ⊕c B
(cid:107)(−A) ⊕c B(cid:107)

,

(−A) ⊕c C
(cid:107)(−A) ⊕c C(cid:107)

(cid:29)

,

In this case, ∠A is also called a gyroangle in the work of [26, section 4].

Hyperbolic law of sines. We state here the hyperbolic law of sines. If for A, B, C ∈ Dn
c , we
denote by ∠B := ∠ABC the angle between the two geodesics starting from B and ending at A and
C respectively, and by ˜c = dc(B, A) the length of the hyperbolic segment BA (and similarly for
others), then we have:

sin(∠A)
√
c˜a)
sinh(

=

sin(∠B)
√
c˜b)
sinh(

=

sin(∠C)
√
c˜c)
sinh(

.

Note that one can also adapt the hyperbolic law of cosines to the hyperbolic space.

B Proof of Theorem 4

Theorem 4.
In the manifold (Dn
to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a vector v ∈ T0Dn
c

c is given by the following isometry:
λc
0
λc
x

0→x(v) = logc
P c

x(x ⊕c expc

0(v)) =

v.

Proof. The geodesic in Dn
v ∈ T0Dn
γ (i.e. X(t) ∈ Tγ(t)Dn

c from 0 to x is given in Eq. (10) by γ(t) = x ⊗c t, for t ∈ [0, 1]. Let
c . Then it is of common knowledge that there exists a unique parallel9 vector ﬁeld X along

c , ∀t ∈ [0, 1]) such that X(0) = v. Let’s deﬁne:
X : t ∈ [0, 1] (cid:55)→ logc

γ(t)(γ(t) ⊕c expc

0(v)) ∈ Tγ(t)Dn
c .

Clearly, X is a vector ﬁeld along γ such that X(0) = v. Now deﬁne
0→x : v ∈ T0Dn
P c

x(x ⊕c expc

0(v)) ∈ TxDn
c .

c (cid:55)→ logc
0→x(v) = λc

c . Since P c

0→x is a linear isometry from T0Dn
v, hence P c
c
0→x(v) = X(1), it is enough to prove that X is parallel in order to guarantee that

From Eq. (12), it is easily seen that P c
to TxDn
c to TxDn
0→x is the parallel transport from T0Dn
P c
c .
Since X is a vector ﬁeld along γ, its covariant derivative can be expressed with the Levi-Civita
connection ∇c associated to gc:

0
λc
x

DX
∂t

= ∇c

˙γ(t)X.

Let’s compute the Levi-Civita connection from its Christoffel symbols. In a local coordinate system,
they can be written as

Γi

jk =

(gc)il(∂jgc

lk + ∂kgc

lj − ∂lgc

jk),

1
2

where superscripts denote the inverse metric tensor and using Einstein’s notations. As gc
at γ(t) ∈ Dn

c this yields:

ij = (λc)2δij,

jk = cλc
Γi

γ(t)(δikγ(t)j + δijγ(t)k − δjkγ(t)i).

9i.e. that DX

∂t = 0 for t ∈ [0, 1], where D

∂t denotes the covariant derivative.

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

14

On the other hand, since X(t) = (λc

∇c

˙γ(t)X = ˙γ(t)i∇c

i X = ˙γ(t)i∇c
i

= vj ˙γ(t)i∇c
i

0/λc

γ(t))v, we have
(cid:32)

(cid:33)

λc
0
λc

γ(t)

v

(cid:32)

λc
0
λc

γ(t)

(cid:33)

ej

.

√

√

Since γ(t) = (1/
Hence there exists K x

c) tanh(t tanh−1(
t ∈ R such that ˙γ(t) = K x

c(cid:107)x(cid:107))) x

(cid:107)x(cid:107) , it is easily seen that ˙γ(t) is colinear to γ(t).
t γ(t). Moreover, we have the following Leibniz rule:
(cid:33)

(cid:32)

(cid:32)

∇c
i

λc
0
λc

γ(t)

(cid:33)

ej

=

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

ej.

Combining these yields

DX
∂t

= K x

t vjγ(t)i

(cid:32)

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

(cid:32)

(cid:33)

(cid:33)

ej

.

Replacing with the Christoffel symbols of ∇c at γ(t) gives

Moreover,

λc
0
λc

γ(t)

λc
0
λc

γ(t)

∇c

i ej =

ijek = 2c[δk
Γk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek.

∂
∂γ(t)i

(cid:32)

(cid:33)

λc
0
λc

γ(t)

ej =

∂
∂γ(t)i

(cid:0)−c(cid:107)γ(t)(cid:107)2(cid:1) ej = −2cγ(t)iej.

Putting together everything, we obtain

DX
∂t

= K x

t vjγ(t)i (cid:0)2c[δk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek − 2cγ(t)iej

(cid:1)

t vjγ(t)i (cid:0)γ(t)jei − δijγ(t)kek
t vj (cid:0)γ(t)jγ(t)iei − γ(t)iδijγ(t)kek
(cid:1)
t vj (cid:0)γ(t)jγ(t)iei − γ(t)jγ(t)kek

(cid:1)

(cid:1)

= 2cK x
= 2cK x
= 2cK x
= 0,

which concludes the proof.

C Proof of Eq. (22)

Proof. Two steps proof:
i) expc

p({a}⊥) ⊆ {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}:

Let z ∈ {a}⊥. From Eq. (12), we have that:

This, together with the left-cancellation law in gyrospaces (see section 2.3), implies that

expc

p(z) = −p ⊕c βz,

for some β ∈ R.

(cid:104)−p ⊕c expc

p(z), a(cid:105) = (cid:104)βz, a(cid:105) = 0

which is what we wanted.

ii) {x ∈ Dn
Let x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0} ⊆ expc
c s.t. (cid:104)−p ⊕c x, a(cid:105) = 0. Then, using Eq. (12), we derive that:
for some β ∈ R,

p(x) = β(−p ⊕c x),

p({a}⊥):

logc

which is orthogonal to a, by assumption. This implies logc

p(x) ∈ {a}⊥, hence x ∈ expc

p({a}⊥).

15

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(54)

D Proof of Theorem 5

Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

1
√
c

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(55)

Proof. We ﬁrst need to prove the following lemma, trivial in the Euclidean space, but not in the
Poincaré ball:
Lemma 7. (Orthogonal projection on a geodesic) Any point in the Poincaré ball has a unique
orthogonal projection on any given geodesic that does not pass through the point. Formally, for all
y ∈ Dn
c and for all geodesics γx→z(·) s.t. y /∈ Im γx→z, there exists an unique w ∈ Im γx→z s.t.
∠(γw→y, γx→z) = π/2.

Proof. We ﬁrst note that any geodesic in Dn
and has two "points at inﬁnity" lying on the ball border (v (cid:54)= 0):

c has the form γ(t) = u ⊕c v ⊗c t as given by Eq. 11,

γ(±∞) = u ⊕c

√

±v
c(cid:107)v(cid:107)

∈ ∂Dn
c .

(56)

Using the notations in the lemma statement, the closed-form of γx→z is given by Eq. (10):

γx→z(t) = x ⊕c (−x ⊕c z) ⊗c t

We denote by x(cid:48), z(cid:48) ∈ ∂Dn
∠ywx(cid:48) is well deﬁned from Eq. (34):

c its points at inﬁnity as described by Eq. (56). Then, the hyperbolic angle

cos(∠(γw→y, γx→z)) = cos(∠ywz(cid:48)) =

(cid:104)−w ⊕c y, −w ⊕c z(cid:48)(cid:105)
(cid:107) − w ⊕c y(cid:107) · (cid:107) − w ⊕c z(cid:48)(cid:107)

.

(57)

We now perform 2 steps for this proof.

i) Existence of w:

The angle function from Eq. (57) is continuous w.r.t t when w = γx→z(t). So we ﬁrst prove existence
of an angle of π/2 by continuously moving w from x(cid:48) to z(cid:48) when t goes from −∞ to ∞, and
observing that cos(∠ywz(cid:48)) goes from −1 to 1 as follows:

cos(∠yx(cid:48)z(cid:48)) = 1 & lim
w→z(cid:48)

cos(∠ywz(cid:48)) = −1.

(58)

The left part of Eq. (58) follows from Eq. (57) and from the fact (easy to show from the deﬁnition
c (which is the case of x(cid:48)). The right part of Eq. (58)
of ⊕c) that a ⊕c b = a, when (cid:107)a(cid:107) = 1/
follows from the fact that ∠ywz(cid:48) = π − ∠ywx(cid:48) (from the conformal property, or from Eq. (34)) and
cos(∠yz(cid:48)x(cid:48)) = 1 (proved as above).
Hence cos(∠ywz(cid:48)) has to pass through 0 when going from −1 to 1, which achieves the proof of
existence.

√

ii) Uniqueness of w:
Assume by contradiction that there are two w and w(cid:48) on γx→z that form angles ∠ywx(cid:48) and ∠yw(cid:48)x(cid:48)
of π/2. Since w, w(cid:48), x(cid:48) are on the same geodesic, we have

π/2 = ∠yw(cid:48)x(cid:48) = ∠yw(cid:48)w = ∠ywx(cid:48) = ∠yw(cid:48)w
So ∆yww(cid:48) has two right angles, but in the Poincaré ball this is impossible.

(59)

Now, we need two more lemmas:
Lemma 8. (Minimizing distance from point to geodesic) The orthogonal projection of a point to
a geodesic (not passing through the point) is minimizing the distance between the point and the
geodesic.

Proof. The proof is similar with the Euclidean case and it’s based on hyperbolic sine law and the fact
that in any right hyperbolic triangle the hypotenuse is strictly longer than any of the other sides.

16

Lemma 9. (Geodesics through p) Let ˜H c
all points on the geodesic γp→w are included in ˜H c

a,p.

a,p be a Poincaré hyperplane. Then, for any w ∈ ˜H c

a,p \ {p},

Proof. γp→w(t) = p ⊕c (−p ⊕c w) ⊗c t. Then, it is easy to check the condition in Eq. (22):

(cid:104)−p ⊕c γp→w(t), a(cid:105) = (cid:104)(−p ⊕c w) ⊗c t, a(cid:105) ∝ (cid:104)−p ⊕c w, a(cid:105) = 0.

(60)

We now turn back to our proof. Let x ∈ Dn
We prove that there is at least one point w∗ ∈ ˜H c

c be an arbitrary point and ˜H c

a,p a Poincaré hyperplane.

a,p that achieves the inﬁmum distance

dc(x, w∗) = inf
w∈ ˜H c

a,p

dc(x, w),

and, moreover, that this distance is the same as the one in the theorem’s statement.
We ﬁrst note that for any point w ∈ ˜H c
and Lemma 9, it is obvious that the projection of x to γp→w will give a strictly lower distance.
Thus, we only consider w ∈ ˜H c
triangle ∆xwp, one gets:

a,p such that ∠xwp = π/2. Applying hyperbolic sine law in the right

a,p, if ∠xwp (cid:54)= π/2, then w (cid:54)= w∗. Indeed, using Lemma 8

dc(x, w) = (1/

c) sinh−1 (cid:0)sinh(

c dc(x, p)) · sin(∠xpw)(cid:1) .

√

√

One of the above quantities does not depend on w:

√

√

sinh(

c dc(x, p)) = sinh(2 tanh−1(

c(cid:107) − p ⊕c x(cid:107))) =

√
2
c(cid:107) − p ⊕c x(cid:107)
1 − c(cid:107) − p ⊕c x(cid:107)2 .

The other quantity is sin(∠xpw) which is minimized when the angle ∠xpw is minimized (be-
cause ∠xpw < π/2 for the hyperbolic right triangle ∆xwp), or, alternatively, when cos(∠xpw) is
maximized. But, we already have from Eq. (34) that:

cos(∠xpw) =

(cid:104)−p ⊕c x, −p ⊕c w(cid:105)
(cid:107) − p ⊕c x(cid:107) · (cid:107) − p ⊕c w(cid:107)

.

To maximize the above, the constraint on the right angle at w can be dropped because cos(∠xpw)
depends only on the geodesic γp→w and not on w itself, and because there is always an orthogonal
projection from any point x to any geodesic as stated by Lemma 7. Thus, it remains to ﬁnd the
maximum of Eq. (64) when w ∈ ˜H c
a,p from Eq. (22), one can easily
prove that

a,p. Using the deﬁnition of ˜H c

Using that fact that logc

p(w)/(cid:107) logc

p(w)(cid:107) = −p ⊕c w/(cid:107) − p ⊕c w(cid:107), we just have to ﬁnd

and we are left with a well known Euclidean problem which is equivalent to ﬁnding the minimum
angle between the vector −p ⊕c x (viewed as Euclidean) and the hyperplane {a}⊥. This angle
is given by the Euclidean orthogonal projection whose sin value is the distance from the vector’s
endpoint to the hyperplane divided by the vector’s length:

{logc

p(w) : w ∈ ˜H c

a,p} = {a}⊥.

max
z∈{a}⊥

(cid:18) (cid:104)−p ⊕c x, z(cid:105)

(cid:107) − p ⊕c x(cid:107) · (cid:107)z(cid:107)

(cid:19)

,

sin(∠xpw∗) =

|(cid:104)−p ⊕c x, a
(cid:107) − p ⊕c x(cid:107)

(cid:107)a(cid:107) (cid:105)|

.

17

It follows that a point w∗ ∈ ˜H c
Eqs. (61),(62),(63) and (67) concludes the proof.

a,p satisfying Eq. (67) exists (but might not be unique). Combining

(61)

(62)

(63)

(64)

(65)

(66)

(67)

(cid:3)

E Derivation of the Hyperbolic GRU Update-gate

In [24], the authors recover the update/forget-gate mechanism of a GRU/LSTM by requiring that the
class of neural networks given by the chosen architecture be invariant to time-warpings. The idea is
the following.

Recovering the update-gate from time-warping. A naive RNN is given by the equation

h(t + 1) = ϕ(W h(t) + U x(t) + b)

Let’s drop the bias b to simplify notations. If h is seen as a differentiable function of time, then a
ﬁrst-order Taylor development gives h(t + δt) ≈ h(t) + δt dh
dt (t) for small δt. Combining this for
δt = 1 with the naive RNN equation, one gets

dh
dt

dα
dt

(t) = ϕ(W h(t) + U x(t)) − h(t).

As this is written for any t, one can replace it by t ← α(t) where α is a (smooth) increasing function
of t called the time-warping. Denoting by ˜h(t) := h(α(t)) and ˜x(t) := x(α(t)), using the chain rule
d˜h
dt (t) = dα

dt (α(t)), one gets

dt (t) dh

d˜h
dt

dα
dt

(t) =

(t)ϕ(W ˜h(t) + U ˜x(t)) −

(t)˜h(t).

(70)

Removing the tildas to simplify notations, discretizing back with dh

dt (t) ≈ h(t + 1) − h(t) yields

h(t + 1) =

(t)ϕ(W h(t) + U x(t)) +

1 −

(t)

h(t).

(71)

dα
dt

(cid:18)

(cid:19)

dα
dt

Requiring that our class of neural networks be invariant to time-warpings means that this class should
contain RNNs deﬁned by Eq. (71), i.e. that dα
dt (t) can be learned. As this is a positive quantity, we
can parametrize it as z(t) = σ(W zh(t) + U zx(t)), recovering the forget-gate equation:

h(t + 1) = z(t)ϕ(W h(t) + U x(t)) + (1 − z(t))h(t).

Adapting this idea to hyperbolic RNNs. The gyroderivative [4] of a map h : R → Dn
as

c is deﬁned

dh
dt

(t) = lim
δt→0

1
δt

⊗c (−h(t) ⊕c h(t + δt)).

Using Möbius scalar associativity and the left-cancellation law leads us to

h(t + δt) ≈ h(t) ⊕c δt ⊗c

(t),

dh
dt

for small δt. Combining this with the equation of a simple hyperbolic RNN of Eq. (29) with δt = 1,
one gets

dh
dt

(t) = −h(t) ⊕c ϕ⊗c(W ⊗c h(t) ⊕c U ⊗c x(t)).

For the next step, we need the following lemma:
Lemma 10 (Gyro-chain-rule). For α : R → R differentiable and h : R → Dn
gyro-derivative, if ˜h := h ◦ α, then we have

c with a well-deﬁned

(68)

(69)

(72)

(73)

(74)

(75)

(76)

where dα

dt (t) denotes the usual derivative.

d˜h
dt

(t) =

(t) ⊗c

(α(t)),

dα
dt

dh
dt

18

(77)

(78)

(79)

(80)

(81)

Proof.

d˜h
dt

(t) = lim
δt→0

1
δt
1
δt

⊗c [−˜h(t) ⊕c

˜h(t + δt)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

= lim
δt→0

= lim
δt→0

= lim
δt→0

= lim
u→0
dα
dt

=

α(cid:48)(t) + O(δt)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
u

(t) ⊗c

(α(t))

dh
dt

⊗c [−h(α(t)) ⊕c h(α(t) + u)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

(Möbius scalar associativity) (82)

where we set u = δt(α(cid:48)(t) + O(δt)), with u → 0 when δt → 0, which concludes.

Using lemma 10 and Eq. (75), with similar notations as in Eq. (70) we have

d˜h
dt

dα
dt

(t) =

(t) ⊗c (−˜h(t) ⊕c ϕ⊗c(W ⊗c

˜h(t) ⊕c U ⊗c ˜x(t))).

(83)

Finally, discretizing back with Eq. (74), using the left-cancellation law and dropping the tildas yields

h(t + 1) = h(t) ⊕c

(t) ⊗c (−h(t) ⊕c ϕ⊗c (W ⊗c h(t) ⊕c U ⊗c x(t))).

(84)

dα
dt

Since α is a time-warping, by deﬁnition its derivative is positive and one can choose to parametrize
it with an update-gate zt (a scalar) deﬁned with a sigmoid. Generalizing this scalar scaling by the
Möbius version of the pointwise scaling (cid:12) yields the Möbius matrix scaling diag(zt) ⊗c ·, leading to
our proposed Eq. (33) for the hyperbolic GRU.

F More Experimental Investigations

The following empirical facts were observed for both hyperbolic RNNs and GRUs.

We observed that, in the hyperbolic setting, accuracy is often much higher when sentence embeddings
can go close to the border (hyperbolic "inﬁnity"), hence exploiting the hyperbolic nature of the space.
Moreover, the faster the two sentence norms go to 1, the more it’s likely that a good local minima
was reached. See ﬁgures 3 and 5.

We often observe that test accuracy starts increasing exactly when sentence embedding norms do.
However, in the hyperbolic setting, the sentence embeddings norms remain close to 0 for a few
epochs, which does not happen in the Euclidean case. See ﬁgures 3, 5 and 4. This mysterious fact
was also exhibited in a similar way by [21] which suggests that the model ﬁrst has to adjust the
angular layout in the almost Euclidean vicinity of 0 before increasing norms and fully exploiting
hyperbolic geometry.

19

(a) Test accuracy

(a) Test accuracy

20

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 3: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the same
architecture: hyperbolic GRU followed by hyperbolic FFNN and hyperbolic/Euclidean (half-half)
MLR. The X axis shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 4: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: Euclidean GRU followed by Euclidean FFNN and Euclidean MLR. The X axis
shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 5: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: hyperbolic RNN followed by hyperbolic FFNN and hyperbolic MLR. The X axis
shows millions of training examples processed.

(a) Test accuracy

21

8
1
0
2
 
n
u
J
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
2
1
1
9
0
.
5
0
8
1
:
v
i
X
r
a

Hyperbolic Neural Networks

Octavian-Eugen Ganea∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
octavian.ganea@inf.ethz.ch

Gary Bécigneul∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
gary.becigneul@inf.ethz.ch

Thomas Hofmann
Department of Computer Science
ETH Zürich
Zurich, Switzerland
thomas.hofmann@inf.ethz.ch

Abstract

Hyperbolic spaces have recently gained momentum in the context of machine
learning due to their high capacity and tree-likeliness properties. However, the
representational power of hyperbolic geometry is not yet on par with Euclidean
geometry, mostly because of the absence of corresponding hyperbolic neural
network layers. This makes it hard to use hyperbolic embeddings in downstream
tasks. Here, we bridge this gap in a principled manner by combining the formalism
of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model
of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep
learning tools: multinomial logistic regression, feed-forward and recurrent neural
networks such as gated recurrent units. This allows to embed sequential data and
perform classiﬁcation in the hyperbolic space. Empirically, we show that, even if
hyperbolic optimization tools are limited, hyperbolic sentence embeddings either
outperform or are on par with their Euclidean variants on textual entailment and
noisy-preﬁx recognition tasks.

1

Introduction

It is common in machine learning to represent data as being embedded in the Euclidean space Rn. The
main reason for such a choice is simply convenience, as this space has a vectorial structure, closed-
form formulas of distance and inner-product, and is the natural generalization of our intuition-friendly,
visual three-dimensional space. Moreover, embedding entities in such a continuous space allows to
feed them as input to neural networks, which has led to unprecedented performance on a broad range
of problems, including sentiment detection [15], machine translation [3], textual entailment [22] or
knowledge base link prediction [20, 6].

Despite the success of Euclidean embeddings, recent research has proven that many types of com-
plex data (e.g. graph data) from a multitude of ﬁelds (e.g. Biology, Network Science, Computer
Graphics or Computer Vision) exhibit a highly non-Euclidean latent anatomy [8]. In such cases, the
Euclidean space does not provide the most powerful or meaningful geometrical representations. For
example, [10] shows that arbitrary tree structures cannot be embedded with arbitrary low distortion
(i.e. almost preserving their metric) in the Euclidean space with unbounded number of dimensions,
but this task becomes surprisingly easy in the hyperbolic space with only 2 dimensions where the
exponential growth of distances matches the exponential growth of nodes with the tree depth.

∗Equal contribution.

The adoption of neural networks and deep learning in these non-Euclidean settings has been rather
limited until very recently, the main reason being the non-trivial or impossible principled general-
izations of basic operations (e.g. vector addition, matrix-vector multiplication, vector translation,
vector inner product) as well as, in more complex geometries, the lack of closed form expressions for
basic objects (e.g. distances, geodesics, parallel transport). Thus, classic tools such as multinomial
logistic regression (MLR), feed forward (FFNN) or recurrent neural networks (RNN) did not have a
correspondence in these geometries.

How should one generalize deep neural models to non-Euclidean domains ? In this paper we address
this question for one of the simplest, yet useful, non-Euclidean domains: spaces of constant negative
curvature, i.e. hyperbolic. Their tree-likeness properties have been extensively studied [12, 13, 26]
and used to visualize large taxonomies [18] or to embed heterogeneous complex networks [17]. In
machine learning, recently, hyperbolic representations greatly outperformed Euclidean embeddings
for hierarchical, taxonomic or entailment data [21, 10, 11]. Disjoint subtrees from the latent hierar-
chical structure surprisingly disentangle and cluster in the embedding space as a simple reﬂection of
the space’s negative curvature. However, appropriate deep learning tools are needed to embed feature
data in this space and use it in downstream tasks. For example, implicitly hierarchical sequence data
(e.g. textual entailment data, phylogenetic trees of DNA sequences or hierarchial captions of images)
would beneﬁt from suitable hyperbolic RNNs.

The main contribution of this paper is to bridge the gap between hyperbolic and Euclidean geometry
in the context of neural networks and deep learning by generalizing in a principled manner both the
basic operations as well as multinomial logistic regression (MLR), feed-forward (FFNN), simple and
gated (GRU) recurrent neural networks (RNN) to the Poincaré model of the hyperbolic geometry.
We do it by connecting the theory of gyrovector spaces and generalized Möbius transformations
introduced by [2, 26] with the Riemannian geometry properties of the manifold. We smoothly
parametrize basic operations and objects in all spaces of constant negative curvature using a uniﬁed
framework that depends only on the curvature value. Thus, we show how Euclidean and hyperbolic
spaces can be continuously deformed into each other. On a series of experiments and datasets we
showcase the effectiveness of our hyperbolic neural network layers compared to their "classic"
Euclidean variants on textual entailment and noisy-preﬁx recognition tasks. We hope that this paper
will open exciting future directions in the nascent ﬁeld of Geometric Deep Learning.

2 The Geometry of the Poincaré Ball

2.1 Basics of Riemannian geometry

We brieﬂy introduce basic concepts of differential geometry largely needed for a principled general-
ization of Euclidean neural networks. For more rigorous and in-depth expositions, see [23, 14].
An n-dimensional manifold M is a space that can locally be approximated by Rn: it is a generalization
to higher dimensions of the notion of a 2D surface. For x ∈ M, one can deﬁne the tangent space
TxM of M at x as the ﬁrst order linear approximation of M around x. A Riemannian metric
g = (gx)x∈M on M is a collection of inner-products gx : TxM × TxM → R varying smoothly
with x. A Riemannian manifold (M, g) is a manifold M equipped with a Riemannian metric g.
Although a choice of a Riemannian metric g on M only seems to deﬁne the geometry locally on M,
it induces global distances by integrating the length (of the speed vector living in the tangent space)
of a shortest path between two points:

(cid:90) 1

(cid:113)

d(x, y) = inf
γ

0

gγ(t)( ˙γ(t), ˙γ(t))dt,

(1)

where γ ∈ C∞([0, 1], M) is such that γ(0) = x and γ(1) = y. A smooth path γ of minimal length
between two points x and y is called a geodesic, and can be seen as the generalization of a straight-line
in Euclidean space. The parallel transport Px→y : TxM → TyM is a linear isometry between
tangent spaces which corresponds to moving tangent vectors along geodesics and deﬁnes a canonical
way to connect tangent spaces. The exponential map expx at x, when well-deﬁned, gives a way to
project back a vector v of the tangent space TxM at x, to a point expx(v) ∈ M on the manifold.
This map is often used to parametrize a geodesic γ starting from γ(0) := x ∈ M with unit-norm
direction ˙γ(0) := v ∈ TxM as t (cid:55)→ expx(tv). For geodesically complete manifolds, such as the
Poincaré ball considered in this work, expx is well-deﬁned on the full tangent space TxM. Finally, a

2

(2)

(3)

(4)

(5)

metric ˜g is said to be conformal to another metric g if it deﬁnes the same angles, i.e.

˜gx(u, v)
(cid:112)˜gx(u, u)(cid:112)˜gx(v, v)

=

gx(u, v)
(cid:112)gx(u, u)(cid:112)gx(v, v)

,

for all x ∈ M, u, v ∈ TxM \ {0}. This is equivalent to the existence of a smooth function
λ : M → R, called the conformal factor, such that ˜gx = λ2

xgx for all x ∈ M.

2.2 Hyperbolic space: the Poincaré ball

The hyperbolic space has ﬁve isometric models that one can work with [9]. Similarly as in [21] and
[11], we choose to work in the Poincaré ball. The Poincaré ball model (Dn, gD) is deﬁned by the
manifold Dn = {x ∈ Rn : (cid:107)x(cid:107) < 1} equipped with the following Riemannian metric:

gD
x = λ2

xgE, where λx :=

2
1 − (cid:107)x(cid:107)2 ,

gE = In being the Euclidean metric tensor. Note that the hyperbolic metric tensor is conformal to
the Euclidean one. The induced distance between two points x, y ∈ Dn is known to be given by

dD(x, y) = cosh−1

1 + 2

(cid:18)

(cid:107)x − y(cid:107)2
(1 − (cid:107)x(cid:107)2)(1 − (cid:107)y(cid:107)2)

(cid:19)

.

Since the Poincaré ball is conformal to Euclidean space, the angle between two vectors u, v ∈
TxDn \ {0} is given by

cos(∠(u, v)) =

gD
x (u, v)
x (u, u)(cid:112)gD

(cid:112)gD

x (v, v)

=

(cid:104)u, v(cid:105)
(cid:107)u(cid:107)(cid:107)v(cid:107)

.

2.3 Gyrovector spaces

In Euclidean space, natural operations inherited from the vectorial structure, such as vector addition,
subtraction and scalar multiplication are often useful. The framework of gyrovector spaces provides
an elegant non-associative algebraic formalism for hyperbolic geometry just as vector spaces provide
the algebraic setting for Euclidean geometry [2, 25, 26].

In particular, these operations are used in special relativity, allowing to add speed vectors belonging
to the Poincaré ball of radius c (the celerity, i.e. the speed of light) so that they remain in the ball,
hence not exceeding the speed of light.

We will make extensive use of these operations in our deﬁnitions of hyperbolic neural networks.
For c ≥ 0, denote2 by Dn
then Dn

c := {x ∈ Rn | c(cid:107)x(cid:107)2 < 1}. Note that if c = 0, then Dn
c. If c = 1 then we recover the usual ball Dn.

c is the open ball of radius 1/

c = Rn; if c > 0,

√

Möbius addition. The Möbius addition of x and y in Dn

c is deﬁned as

x ⊕c y :=

(1 + 2c(cid:104)x, y(cid:105) + c(cid:107)y(cid:107)2)x + (1 − c(cid:107)x(cid:107)2)y
1 + 2c(cid:104)x, y(cid:105) + c2(cid:107)x(cid:107)2(cid:107)y(cid:107)2

.

(6)

In particular, when c = 0, one recovers the Euclidean addition of two vectors in Rn. Note that
without loss of generality, the case c > 0 can be reduced to c = 1. Unless stated otherwise, we
will use ⊕ as ⊕1 to simplify notations. For general c > 0, this operation is not commutative nor
associative. However, it satisﬁes x ⊕c 0 = 0 ⊕c x = 0. Moreover, for any x, y ∈ Dn
c , we have
(−x) ⊕c x = x ⊕c (−x) = 0 and (−x) ⊕c (x ⊕c y) = y (left-cancellation law). The Möbius
substraction is then deﬁned by the use of the following notation: x (cid:9)c y := x ⊕c (−y). See [29,
section 2.1] for a geometric interpretation of the Möbius addition.

2We take different notations as in [25] where the author uses s = 1/

c.

√

3

Möbius scalar multiplication. For c > 0, the Möbius scalar multiplication of x ∈ Dn
r ∈ R is deﬁned as

c \ {0} by

r ⊗c x := (1/

c) tanh(r tanh−1(

c(cid:107)x(cid:107)))

√

√

x
(cid:107)x(cid:107)

,

(7)

and r ⊗c 0 := 0. Note that similarly as for the Möbius addition, one recovers the Euclidean scalar
multiplication when c goes to zero: limc→0 r ⊗c x = rx. This operation satisﬁes desirable properties
such as n ⊗c x = x ⊕c · · · ⊕c x (n additions), (r + r(cid:48)) ⊗c x = r ⊗c x ⊕c r(cid:48) ⊗c x (scalar distributivity3),
(rr(cid:48)) ⊗c x = r ⊗c (r(cid:48) ⊗c x) (scalar associativity) and |r| ⊗c x/(cid:107)r ⊗c x(cid:107) = x/(cid:107)x(cid:107) (scaling property).

c , gc) is given by4

Distance.
Euclidean one, with conformal factor λc
(Dn

If one deﬁnes the generalized hyperbolic metric tensor gc as the metric conformal to the
x := 2/(1 − c(cid:107)x(cid:107)2), then the induced distance function on
√

c) tanh−1 (cid:0)√
Again, observe that limc→0 dc(x, y) = 2(cid:107)x − y(cid:107), i.e. we recover Euclidean geometry in the limit5.
Moreover, for c = 1 we recover dD of Eq. (4).

c(cid:107) − x ⊕c y(cid:107)(cid:1) .

dc(x, y) = (2/

(8)

Hyperbolic trigonometry. Similarly as in the Euclidean space, one can deﬁne the notions of
hyperbolic angles or gyroangles (when using the ⊕c), as well as hyperbolic law of sines in the
generalized Poincaré ball (Dn

c , gc). We make use of these notions in our proofs. See Appendix A.

2.4 Connecting Gyrovector spaces and Riemannian geometry of the Poincaré ball

In this subsection, we present how geodesics in the Poincaré ball model are usually described with
Möbius operations, and push one step further the existing connection between gyrovector spaces and
the Poincaré ball by ﬁnding new identities involving the exponential map, and parallel transport.

In particular, these ﬁndings provide us with a simpler formulation of Möbius scalar multiplication,
yielding a natural deﬁnition of matrix-vector multiplication in the Poincaré ball.

Riemannian gyroline element. The Riemannian gyroline element is deﬁned for an inﬁnitesimal
dx as ds := (x + dx) (cid:9)c x, and its size is given by [26, section 3.7]:

(cid:107)ds(cid:107) = (cid:107)(x + dx) (cid:9)c x(cid:107) = (cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2).

(9)

What is remarkable is that it turns out to be identical, up to a scaling factor of 2, to the usual line
element 2(cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2) of the Riemannian manifold (Dn

c , gc).

Geodesics. The geodesic connecting points x, y ∈ Dn

c is shown in [2, 26] to be given by:

γx→y(t) := x ⊕c (−x ⊕c y) ⊗c t, with γx→y : R → Dn

c s.t. γx→y(0) = x and γx→y(1) = y.

Note that when c goes to 0, geodesics become straight-lines, recovering Euclidean geometry. In the
remainder of this subsection, we connect the gyrospace framework with Riemannian geometry.
Lemma 1. For any x ∈ Dn and v ∈ TxDn
c s.t. gc
x with direction v is given by:

x(v, v) = 1, the unit-speed geodesic starting from

γx,v(t) = x ⊕c

tanh

(cid:18)

(cid:18)√

(cid:19) v
√

c

t
2

c(cid:107)v(cid:107)

(cid:19)

, where γx,v : R → Dn s.t. γx,v(0) = x and ˙γx,v(0) = v.

(10)

(11)

Proof. One can use Eq. (10) and reparametrize it to unit-speed using Eq. (8). Alternatively, direct
computation and identiﬁcation with the formula in [11, Thm. 1] would give the same result. Using
Eq. (8) and Eq. (11), one can sanity-check that dc(γ(0), γ(t)) = t, ∀t ∈ [0, 1].

3⊗c has priority over ⊕c in the sense that a ⊗c b ⊕c c := (a ⊗c b) ⊕c c and a ⊕c b ⊗c c := a ⊕c (b ⊗c c).
4The notation −x ⊕c y should always be read as (−x) ⊕c y and not −(x ⊕c y).
5The factor 2 comes from the conformal factor λx = 2/(1 − (cid:107)x(cid:107)2), which is a convention setting the

curvature to −1.

4

Exponential and logarithmic maps. The following lemma gives the closed-form derivation of
exponential and logarithmic maps.
Lemma 2. For any point x ∈ Dn
map logc
c → TxDn
(cid:18)

c are given for v (cid:54)= 0 and y (cid:54)= x by:
(cid:18)√

c , the exponential map expc

c and the logarithmic

x : TxDn

c → Dn

x : Dn

√

(cid:19)

, logc

x(y) =

√

tanh−1(

c(cid:107) − x ⊕c y(cid:107))

expc

x(v) = x ⊕c

tanh

c

λc
x(cid:107)v(cid:107)
2

(cid:19) v
√

c(cid:107)v(cid:107)

2
cλc
x

−x ⊕c y
(cid:107) − x ⊕c y(cid:107)
(12)

.

Proof. Following the proof of [11, Cor. 1.1], one gets expc
gives the formula for expc

x. Algebraic check of the identity logc

x(v) = γx,
x(expc

v

x(cid:107)v(cid:107) (λc
λc

x(cid:107)v(cid:107)). Using Eq. (11)

x(v)) = v concludes.

The above maps have more appealing forms when x = 0, namely for v ∈ T0Dn

c \ {0}, y ∈ Dn

c \ {0}:

expc

0(v) = tanh(

c(cid:107)v(cid:107))

√

, logc

0(y) = tanh−1(

c(cid:107)y(cid:107))

√

(13)

√

y
c(cid:107)y(cid:107)

.

√

v
c(cid:107)v(cid:107)

Moreover, we still recover Euclidean geometry in the limit c → 0, as limc→0 expc
Euclidean exponential map, and limc→0 logc

x(y) = y − x is the Euclidean logarithmic map.

x(v) = x + v is the

Möbius scalar multiplication using exponential and logarithmic maps. We studied the expo-
nential and logarithmic maps in order to gain a better understanding of the Möbius scalar multiplica-
tion (Eq. (7)). We found the following:
Lemma 3. The quantity r ⊗ x can actually be obtained by projecting x in the tangent space at 0
with the logarithmic map, multiplying this projection by the scalar r in T0Dn
c , and then projecting it
back on the manifold with the exponential map:
0(r logc

∀r ∈ R, x ∈ Dn
c .

r ⊗c x = expc

0(x)),

(14)

In addition, we recover the well-known relation between geodesics connecting two points and the
exponential map:

γx→y(t) = x ⊕c (−x ⊕c y) ⊗c t = expc

x(t logc

x(y)),

t ∈ [0, 1].

(15)

This last result enables us to generalize scalar multiplication in order to deﬁne matrix-vector multipli-
cation between Poincaré balls, one of the essential building blocks of hyperbolic neural networks.

Parallel transport. Finally, we connect parallel transport (from T0Dn
the following theorem, which we prove in appendix B.
Theorem 4. In the manifold (Dn
vector v ∈ T0Dn

c to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a
c is given by the following isometry:
λc
0
λc
x

x(x ⊕c expc

0(v)) =

0→x(v) = logc
P c

(16)

v.

c ) to gyrovector spaces with

As we’ll see later, this result is crucial in order to deﬁne and optimize parameters shared between
different tangent spaces, such as biases in hyperbolic neural layers or parameters of hyperbolic MLR.

3 Hyperbolic Neural Networks

Neural networks can be seen as being made of compositions of basic operations, such as linear
maps, bias translations, pointwise non-linearities and a ﬁnal sigmoid or softmax layer. We ﬁrst
explain how to construct a softmax layer for logits lying in a Poincaré ball. Then, we explain how
to transform a mapping between two Euclidean spaces as one between Poincaré balls, yielding
matrix-vector multiplication and pointwise non-linearities in the Poincaré ball. Finally, we present
possible adaptations of various recurrent neural networks to the hyperbolic domain.

5

3.1 Hyperbolic multiclass logistic regression

In order to perform multi-class classiﬁcation on the Poincaré ball, one needs to generalize multinomial
logistic regression (MLR) − also called softmax regression − to the Poincaré ball.

Reformulating Euclidean MLR. Let’s ﬁrst reformulate Euclidean MLR from the perspective of
distances to margin hyperplanes, as in [19, Section 5]. This will allow us to easily generalize it.

Given K classes, one learns a margin hyperplane for each such class using softmax probabilities:

∀k ∈ {1, ..., K},

p(y = k|x) ∝ exp (((cid:104)ak, x(cid:105) − bk)) , where bk ∈ R, x, ak ∈ Rn.

(17)

Note that any afﬁne hyperplane in Rn can be written with a normal vector a and a scalar shift b:

Ha,b = {x ∈ Rn : (cid:104)a, x(cid:105) − b = 0}, where a ∈ Rn \ {0}, and b ∈ R.

(18)

As in [19, Section 5], we note that (cid:104)a, x(cid:105) − b = sign((cid:104)a, x(cid:105) − b)(cid:107)a(cid:107)d(x, Ha,b). Using Eq. (17):

p(y = k|x) ∝ exp(sign((cid:104)ak, x(cid:105) − bk)(cid:107)ak(cid:107)d(x, Hak,bk )), bk ∈ R, x, ak ∈ Rn.

(19)

As it is not immediately obvious how to generalize the Euclidean hyperplane of Eq. (18) to other
spaces such as the Poincaré ball, we reformulate it as follows:

˜Ha,p = {x ∈ Rn : (cid:104)−p + x, a(cid:105) = 0} = p + {a}⊥, where p ∈ Rn, a ∈ Rn \ {0}.

(20)

This new deﬁnition relates to the previous one as ˜Ha,p = Ha,(cid:104)a,p(cid:105). Rewriting Eq. (19) with b = (cid:104)a, p(cid:105):
p(y = k|x) ∝ exp(sign((cid:104)−pk + x, ak(cid:105))(cid:107)ak(cid:107)d(x, ˜Hak,pk )), with pk, x, ak ∈ Rn.

(21)

It is now natural to adapt the previous deﬁnition to the hyperbolic setting by replacing + by ⊕c:
Deﬁnition 3.1 (Poincaré hyperplanes). For p ∈ Dn
p(z, a) = 0} = {z ∈ TpDn
gc

c : (cid:104)z, a(cid:105) = 0}. Then, we deﬁne Poincaré hyperplanes as

c \ {0}, let {a}⊥ := {z ∈ TpDn
c :

c , a ∈ TpDn

˜H c

a,p := {x ∈ Dn

c : (cid:104)logc

p(x), a(cid:105)p = 0} = expc

p({a}⊥) = {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}.

(22)

The last equality is shown appendix C. ˜H c
all geodesics in Dn
hypergyroplanes, see [27, deﬁnition 5.8]. A 3D hyperplane example is depicted in Fig. 1.

a,p can also be described as the union of images of
c orthogonal to a and containing p. Notice that our deﬁnition matches that of

Next, we need the following theorem, proved in appendix D:
Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(23)

Final formula for MLR in the Poincaré ball. Putting together Eq. (21) and Thm. 5, we get the
hyperbolic MLR formulation. Given K classes and k ∈ {1, . . . , K}, pk ∈ Dn
c \ {0}:

c , ak ∈ Tpk

Dn

p(y = k|x) ∝ exp(sign((cid:104)−pk ⊕c x, ak(cid:105))

gc
pk

(ak, ak)dc(x, ˜H c

)),

ak,pk

∀x ∈ Dn
c ,

(24)

or, equivalently

p(y = k|x) ∝ exp

(cid:18) λc
pk

(cid:107)ak(cid:107)
√
c

sinh−1

(cid:18)

√
2

c(cid:104)−pk ⊕c x, ak(cid:105)

(cid:19)(cid:19)

(1 − c(cid:107) − pk ⊕c x(cid:107)2)(cid:107)ak(cid:107)

,

∀x ∈ Dn
c .

(25)

this goes to p(y = k|x) ∝ exp(4(cid:104)−pk + x, ak(cid:105)) =

Notice that when c goes to zero,
exp((λ0
pk

)2(cid:104)−pk + x, ak(cid:105)) = exp((cid:104)−pk + x, ak(cid:105)0), recovering the usual Euclidean softmax.
However, at this point it is unclear how to perform optimization over ak, since it lives in Tpk
hence depends on pk. The solution is that one should write ak = P c
)a(cid:48)
k ∈ T0Dn
a(cid:48)

c = Rn, and optimize a(cid:48)

k as a Euclidean parameter.

k) = (λc

0/λc
pk

0→pk

(a(cid:48)

Dn
c and
k, where

1
√
c

(cid:113)

6

3.2 Hyperbolic feed-forward layers

In order to deﬁne hyperbolic neural networks, it is crucial to de-
ﬁne a canonically simple parametric family of transformations,
playing the role of linear mappings in usual Euclidean neural
networks, and to know how to apply pointwise non-linearities.
Inspiring ourselves from our reformulation of Möbius scalar
multiplication in Eq. (14), we deﬁne:
Deﬁnition 3.2 (Möbius version). For f : Rn → Rm, we deﬁne
the Möbius version of f as the map from Dn

c to Dm

c by:

f ⊗c(x) := expc

0(f (logc

0(x))),

(26)

where expc

0 : T0m

Dm

c → Dm

c and logc

0 : Dn

c → T0n

Dn
c .

Figure 1: An example of a hyper-
bolic hyperplane in D3
1 plotted us-
ing sampling. The red point is p.
The shown normal axis to the hy-
perplane through p is parallel to a.

Note that similarly as for other Möbius operations, we recover
the Euclidean mapping in the limit c → 0 if f is continuous, as limc→0 f ⊗c(x) = f (x). This
deﬁnition satisﬁes a few desirable properties too, such as: (f ◦ g)⊗c = f ⊗c ◦ g⊗c for f : Rm → Rl
and g : Rn → Rm (morphism property), and f ⊗c(x)/(cid:107)f ⊗c(x)(cid:107) = f (x)/(cid:107)f (x)(cid:107) for f (x) (cid:54)= 0
(direction preserving). It is then straight-forward to prove the following result:
Lemma 6 (Möbius matrix-vector multiplication). If M : Rn → Rm is a linear map, which we
identify with its matrix representation, then ∀x ∈ Dn

c , if M x (cid:54)= 0 we have

M ⊗c(x) = (1/

c) tanh

√

(cid:18) (cid:107)M x(cid:107)
(cid:107)x(cid:107)

√

tanh−1(

c(cid:107)x(cid:107))

(cid:19) M x
(cid:107)M x(cid:107)

,

(27)

and M ⊗c(x) = 0 if M x = 0. Moreover, if we deﬁne the Möbius matrix-vector multiplication of
M ∈ Mm,n(R) and x ∈ Dn
c by M ⊗c x := M ⊗c(x), then we have (M M (cid:48)) ⊗c x = M ⊗c (M (cid:48) ⊗c x)
for M ∈ Ml,m(R) and M (cid:48) ∈ Mm,n(R) (matrix associativity), (rM ) ⊗c x = r ⊗c (M ⊗c x) for
r ∈ R and M ∈ Mm,n(R) (scalar-matrix associativity) and M ⊗c x = M x for all M ∈ On(R)
(rotations are preserved).

Pointwise non-linearity.
ϕ⊗c can be applied to elements of the Poincaré ball.

If ϕ : Rn → Rn is a pointwise non-linearity, then its Möbius version

Bias translation. The generalization of a translation in the Poincaré ball is naturally given by
moving along geodesics. But should we use the Möbius sum x ⊕c b with a hyperbolic bias b or the
x(b(cid:48)) with a Euclidean bias b(cid:48)? These views are uniﬁed with parallel transport
exponential map expc
c by a bias b ∈ Dn
(see Thm 4). Möbius translation of a point x ∈ Dn
(cid:18) λc
0
λc
x

c is given by
(cid:19)

x ← x ⊕c b = expc

0(b))) = expc
x

0→x(logc

x(P c

logc

0(b)

(28)

.

We recover Euclidean translations in the limit c → 0. Note that bias translations play a particular
Indeed, consider multiple layers of the form fk(x) = ϕk(Mkx), each of
role in this model.
which having Möbius version f ⊗c
k (Mk ⊗c x). Then their composition can be re-written
f ⊗c
k ◦ · · · ◦ f ⊗c
1 = expc
0. This means that these operations can essentially be
performed in Euclidean space. Therefore, it is the interposition between those with the bias translation
of Eq. (28) which differentiates this model from its Euclidean counterpart.

k (x) = ϕ⊗c
0 ◦fk ◦ · · · ◦ f1 ◦ logc

If a vector x ∈ Rn+p is the (vertical) concatenation
Concatenation of multiple input vectors.
of two vectors x1 ∈ Rn, x2 ∈ Rp, and M ∈ Mm,n+p(R) can be written as the (horizontal)
concatenation of two matrices M1 ∈ Mm,n(R) and M2 ∈ Mm,p(R), then M x = M1x1 + M2x2.
We generalize this to hyperbolic spaces: if we are given x1 ∈ Dn
c ×Dp
c ,
and M, M1, M2 as before, then we deﬁne M ⊗c x := M1 ⊗c x1 ⊕c M2 ⊗c x2. Note that when c goes
to zero, we recover the Euclidean formulation, as limc→0 M ⊗c x = limc→0 M1 ⊗c x1 ⊕c M2 ⊗c x2 =
M1x1 + M2x2 = M x. Moreover, hyperbolic vectors x ∈ Dn
c can also be "concatenated" with real
features y ∈ R by doing: M ⊗c x ⊕c y ⊗c b with learnable b ∈ Dm

c , x = (x1 x2)T ∈ Dn

c and M ∈ Mm,n(R).

c , x2 ∈ Dp

7

3.3 Hyperbolic RNN

Naive RNN. A simple RNN can be deﬁned by ht+1 = ϕ(W ht + U xt + b) where ϕ is a pointwise
non-linearity, typically tanh, sigmoid, ReLU, etc. This formula can be naturally generalized to the
hyperbolic space as follows. For parameters W ∈ Mm,n(R), U ∈ Mm,d(R), b ∈ Dm
c , we deﬁne:

ht+1 = ϕ⊗c (W ⊗c ht ⊕c U ⊗c xt ⊕c b),

ht ∈ Dn

c , xt ∈ Dd
c .

(29)

Note that if inputs xt’s are Euclidean, one can write ˜xt := expc
expc

(U xt)) = W ⊗c ht ⊕c expc

(P c

W ⊗cht

0→W ⊗cht

0(U xt) = W ⊗c ht ⊕c U ⊗c ˜xt.

0(xt) and use the above formula, since

GRU architecture. One can also adapt the GRU architecture:
rt = σ(W rht−1 + U rxt + br),
zt = σ(W zht−1 + U zxt + bz),
˜ht = ϕ(W (rt (cid:12) ht−1) + U xt + b), ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht,

(30)

where (cid:12) denotes pointwise product. First, how should we adapt the pointwise multiplication by a
scaling gate? Note that the deﬁnition of the Möbius version (see Eq. (26)) can be naturally extended
to maps f : Rn × Rp → Rm as f ⊗c : (h, h(cid:48)) ∈ Dn
0(h(cid:48)))). In
c (cid:55)→ expc
0(h(cid:48))) =
particular, choosing f (h, h(cid:48)) := σ(h) (cid:12) h(cid:48) yields6 f ⊗c(h, h(cid:48)) = expc
diag(σ(logc

0(h))) ⊗c h(cid:48). Hence we adapt rt (cid:12) ht−1 to diag(rt) ⊗c ht−1 and the reset gate rt to:
0(W r ⊗c ht−1 ⊕c U r ⊗c xt ⊕c br),

0(h), logc
0(h)) (cid:12) logc

0(f (logc
0(σ(logc

rt = σ logc

c × Dp

(31)

and similarly for the update gate zt. Note that as the argument of σ in the above is unbounded, rt and
zt can a priori take values onto the full range (0, 1). Now the intermediate hidden state becomes:
˜ht = ϕ⊗c ((W diag(rt)) ⊗c ht−1 ⊕c U ⊗c xt ⊕ b),

(32)

where Möbius matrix associativity simpliﬁes W ⊗c (diag(rt) ⊗c ht−1) into (W diag(rt)) ⊗c ht−1.
Finally, we propose to adapt the update-gate equation as

ht = ht−1 ⊕c diag(zt) ⊗c (−ht−1 ⊕c

˜ht).

(33)

Note that when c goes to zero, one recovers the usual GRU. Moreover, if zt = 0 or zt = 1, then ht
becomes ht−1 or ˜ht respectively, similarly as in the usual GRU. This adaptation was obtained by
adapting [24]: in this work, the authors re-derive the update-gate mechanism from a ﬁrst principle
called time-warping invariance. We adapted their derivation to the hyperbolic setting by using the
notion of gyroderivative [4] and proving a gyro-chain-rule (see appendix E).

4 Experiments

SNLI task and dataset. We evaluate our method on two tasks. The ﬁrst is natural language
inference, or textual entailment. Given two sentences, a premise (e.g. "Little kids A. and B. are
playing soccer.") and a hypothesis (e.g. "Two children are playing outdoors."), the binary classiﬁcation
task is to predict whether the second sentence can be inferred from the ﬁrst one. This deﬁnes a partial
order in the sentence space. We test hyperbolic networks on the biggest real dataset for this task,
SNLI [7]. It consists of 570K training, 10K validation and 10K test sentence pairs. Following [28],
we merge the "contradiction" and "neutral" classes into a single class of negative sentence pairs, while
the "entailment" class gives the positive pairs.

PREFIX task and datasets. We conjecture that the improvements of hyperbolic neural networks
are more signiﬁcant when the underlying data structure is closer to a tree. To test this, we design a
proof-of-concept task of detection of noisy preﬁxes, i.e. given two sentences, one has to decide if the
second sentence is a noisy preﬁx of the ﬁrst, or a random sentence. We thus build synthetic datasets
PREFIX-Z% (for Z being 10, 30 or 50) as follows: for each random ﬁrst sentence of random length
at most 20 and one random preﬁx of it, a second positive sentence is generated by randomly replacing
Z% of the words of the preﬁx, and a second negative sentence of same length is randomly generated.
Word vocabulary size is 100, and we generate 500K training, 10K validation and 10K test pairs.

6If x has n coordinates, then diag(x) denotes the diagonal matrix of size n with xi’s on its diagonal.

8

Models architecture. Our neural network layers can be used in a plug-n-play manner exactly like
standard Euclidean layers. They can also be combined with Euclidean layers. However, optimization
w.r.t. hyperbolic parameters is different (see below) and based on Riemannian gradients which
are just rescaled Euclidean gradients when working in the conformal Poincaré model [21]. Thus,
back-propagation can be applied in the standard way.

In our setting, we embed the two sentences using two distinct hyperbolic RNNs or GRUs. The
sentence embeddings are then fed together with their squared distance (hyperbolic or Euclidean,
depending on their geometry) to a FFNN (Euclidean or hyperbolic, see Sec. 3.2) which is further
fed to an MLR (Euclidean or hyperbolic, see Sec. 3.1) that gives probabilities of the two classes
(entailment vs neutral). We use cross-entropy loss on top. Note that hyperbolic and Euclidean layers
can be mixed, e.g. the full network can be hyperbolic and only the last layer be Euclidean, in which
case one has to use log0 and exp0 functions to move between the two manifolds in a correct manner
as explained for Eq. 26.

Optimization. Our models have both Euclidean (e.g. weight matrices in both Euclidean and
hyperbolic FFNNs, RNNs or GRUs) and hyperbolic parameters (e.g. word embeddings or biases for
the hyperbolic layers). We optimize the Euclidean parameters with Adam [16] (learning rate 0.001).
Hyperbolic parameters cannot be updated with an equivalent method that keeps track of gradient
history due to the absence of a Riemannian Adam. Thus, they are optimized using full Riemannian
stochastic gradient descent (RSGD) [5, 11]. We also experiment with projected RSGD [21], but
optimization was sometimes less stable. We use a different constant learning rate for word embeddings
(0.1) and other hyperbolic weights (0.01) because words are updated less frequently.

Numerical errors. Gradients of the basic operations deﬁned above (e.g. ⊕c, exponential map) are
c(cid:107)x(cid:107) = 1. Thus, we
not deﬁned when the hyperbolic argument vectors are on the ball border, i.e.
always project results of these operations in the ball of radius 1 − (cid:15), where (cid:15) = 10−5. Numerical
errors also appear when hyperbolic vectors get closer to 0, thus we perturb them with an (cid:15)(cid:48) = 10−15
before they are used in any of the above operations. Finally, arguments of the tanh function are
clipped between ±15 to avoid numerical errors, while arguments of tanh−1 are clipped to at most
1 − 10−5.

√

Hyperparameters. For all methods, baselines and datasets, we use c = 1, word and hidden state
embedding dimension of 5 (we focus on the low dimensional setting that was shown to already
be effective [21]), batch size of 64. We ran all methods for a ﬁxed number of 30 epochs. For all
models, we experiment with both identity (no non-linearity) or tanh non-linearity in the RNN/GRU
cell, as well as identity or ReLU after the FFNN layer and before MLR. As expected, for the fully
Euclidean models, tanh and ReLU respectively surpassed the identity variant by a large margin. We
only report the best Euclidean results. Interestingly, for the hyperbolic models, using only identity for
both non-linearities works slightly better and this is likely due to two facts: i) our hyperbolic layers
already contain non-linearities by their nature, ii) tanh is limiting the output domain of the sentence
embeddings, but the hyperbolic speciﬁc geometry is more pronounced at the ball border, i.e. at the
hyperbolic "inﬁnity", compared to the center of the ball.

For the results shown in Tab. 1, we run each model (baseline or ours) exactly 3 times and report the
test result corresponding to the best validation result from these 3 runs. We do this because the highly
non-convex spectrum of hyperbolic neural networks sometimes results in convergence to poor local
minima, suggesting that initialization is very important.

Results. Results are shown in Tab. 1. Note that the fully Euclidean baseline models might have
an advantage over hyperbolic baselines because more sophisticated optimization algorithms such
as Adam do not have a hyperbolic analogue at the moment. We ﬁrst observe that all GRU models
overpass their RNN variants. Hyperbolic RNNs and GRUs have the most signiﬁcant improvement
over their Euclidean variants when the underlying data structure is more tree-like, e.g. for PREFIX-
10% − for which the tree relation between sentences and their preﬁxes is more prominent − we
reduce the error by a factor of 3.35 for hyperbolic vs Euclidean RNN, and by a factor of 1.5 for
hyperbolic vs Euclidean GRU. As soon as the underlying structure diverges more and more from
a tree, the accuracy gap decreases − for example, for PREFIX-50% the noise heavily affects the
representational power of hyperbolic networks. Also, note that on SNLI our methods perform
similarly as with their Euclidean variants. Moreover, hyperbolic and Euclidean MLR are on par when

9

SNLI

PREFIX-10% PREFIX-30% PREFIX-50%

FULLY EUCLIDEAN RNN
HYPERBOLIC RNN+FFNN, EUCL MLR
FULLY HYPERBOLIC RNN
FULLY EUCLIDEAN GRU
HYPERBOLIC GRU+FFNN, EUCL MLR
FULLY HYPERBOLIC GRU

79.34 %
79.18 %
78.21 %
81.52 %
79.76 %
81.19 %

89.62 %
96.36 %
96.91 %
95.96 %
97.36 %
97.14 %

81.71 %
87.83 %
87.25 %
86.47 %
88.47 %
88.26 %

72.10 %
76.50 %
62.94 %
75.04 %
76.87 %
76.44 %

Table 1: Test accuracies for various models and four datasets. "Eucl" denotes Euclidean. All word
and sentence embeddings have dimension 5. We highlight in bold the best baseline (or baselines, if
the difference is less than 0.5%).

used in conjunction with hyperbolic sentence embeddings, suggesting further empirical investigation
is needed for this direction (see below).

We also observe that, in the hyperbolic setting, accuracy tends to increase when sentence embeddings
start increasing, and gets better as their norms converge towards 1 (the ball border for c = 1). Unlike
in the Euclidean case, this behavior does happen only after a few epochs and suggests that the model
should ﬁrst adjust the angular layout in order to disentangle the representations, before increasing their
norms to fully exploit the strong clustering property of the hyperbolic geometry. Similar behavior
was observed in the context of embedding trees by [21]. Details in appendix F.

MLR classiﬁcation experiments.
For the sentence entailment classi-
ﬁcation task we do not see a clear
advantage of hyperbolic MLR com-
pared to its Euclidean variant. A pos-
sible reason is that, when trained end-
to-end, the model might decide to
place positive and negative embed-
dings in a manner that is already well
separated with a classic MLR. As a
consequence, we further investigate
MLR for the task of subtree classiﬁ-
cation. Using an open source imple-
mentation7 of [21], we pre-trained
Poincaré embeddings of the Word-
Net noun hierarchy (82,115 nodes).
We then choose one node in this tree
(see Table 2) and classify all other
nodes (solely based on their embed-
dings) as being part of the subtree
rooted at this node. All nodes in such a subtree are divided into positive training nodes (80%) and
positive test nodes (20%). The same splitting procedure is applied for the remaining WordNet nodes
that are divided into a negative training and negative test set respectively. Three variants of MLR
are then trained on top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation
task: hyperbolic MLR, Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean
MLR applied after mapping all embeddings in the tangent space at 0 using the log0 map. We use
different embedding dimensions : 2, 3, 5 and 10. For the hyperbolic MLR, we use full Riemannian
SGD with a learning rate of 0.001. For the two Euclidean models we use ADAM optimizer and the
same learning rate. During training, we always sample the same number of negative and positive
nodes in each minibatch of size 16; thus positive nodes are frequently resampled. All methods are
trained for 30 epochs and the ﬁnal F1 score is reported (no hyperparameters to validate are used, thus
we do not require a validation set). This procedure is repeated for four subtrees of different sizes.

Figure 2: Hyperbolic (left) vs Direct Euclidean (right) binary
MLR used to classify nodes as being part in the GROUP.N.01
subtree of the WordNet noun hierarchy solely based on their
Poincaré embeddings. The positive points (from the subtree)
are in blue, the negative points (the rest) are in red and the
trained positive separation hyperplane is depicted in green.

Quantitative results are presented in Table 2. We can see that the hyperbolic MLR overpasses
its Euclidean variants in almost all settings, sometimes by a large margin. Moreover, to provide

7https://github.com/dalab/hyperbolic_cones

10

WORDNET
SUBTREE

ANIMAL.N.01
3218 / 798

GROUP.N.01
6649 / 1727

WORKER.N.01
861 / 254

MAMMAL.N.01
953 / 228

MODEL

D = 2

D = 3

D = 5

D = 10

HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL

47.43 ± 1.07%
41.69 ± 0.19%
38.89 ± 0.01%
81.72 ± 0.17%
61.13 ± 0.42%
60.75 ± 0.24%
12.68 ± 0.82%
10.86 ± 0.01%
9.04 ± 0.06%
32.01 ± 17.14%
15.58 ± 0.04%
13.10 ± 0.13%

91.92 ± 0.61%
68.43 ± 3.90%
62.57 ± 0.61%
89.87 ± 2.73%
63.56 ± 1.22%
61.98 ± 0.57%
24.09 ± 1.49%
22.39 ± 0.04%
22.57 ± 0.20%
87.54 ± 4.55%
44.68 ± 1.87%
44.89 ± 1.18%

98.07 ± 0.55%
95.59 ± 1.18%
89.21 ± 1.34%
87.89 ± 0.80%
67.82 ± 0.81%
67.92 ± 0.74%
55.46 ± 5.49%
35.23 ± 3.16%
26.47 ± 0.78%
88.73 ± 3.22%
59.35 ± 1.31%
52.51 ± 0.85%

99.26 ± 0.59%
99.36 ± 0.18%
98.27 ± 0.70%
91.91 ± 3.07%
91.38 ± 1.19%
91.41 ± 0.18%
66.83 ± 11.38%
47.29 ± 3.93%
36.66 ± 2.74%
91.37 ± 6.09%
77.76 ± 5.08%
56.11 ± 2.21%

Table 2: Test F1 classiﬁcation scores for four different subtrees of WordNet noun tree. All nodes
in such a subtree are divided into positive training nodes (80%) and positive test nodes (20%);
these counts are shown below each subtree root. The same splitting procedure is applied for the
remaining nodes to obtain negative training and test sets. Three variants of MLR are then trained on
top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation task: hyperbolic MLR,
Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean MLR applied after
mapping all embeddings in the tangent space at 0 using the log0 map. 95% conﬁdence intervals for 3
different runs are shown for each method and each different embedding dimension (2, 3, 5 or 10).

further understanding, we plot the 2-dimensional embeddings and the trained separation hyperplanes
(geodesics in this case) in Figure 2. We can see that respecting the hyperbolic geometry is very
important for a quality classiﬁcation model.

5 Conclusion

We showed how classic Euclidean deep learning tools such as MLR, FFNNs, RNNs or GRUs can be
generalized in a principled manner to all spaces of constant negative curvature combining Riemannian
geometry with the elegant theory of gyrovector spaces. Empirically we found that our models
outperform or are on par with corresponding Euclidean architectures on sequential data with implicit
hierarchical structure. We hope to trigger exciting future research related to better understanding
of the hyperbolic non-convexity spectrum and development of other non-Euclidean deep learning
methods.
Our data and Tensorﬂow [1] code are publicly available8.

Acknowledgements

We thank Igor Petrovski for useful pointers regarding the implementation.

This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement
number 167176. Gary Bécigneul is also funded by the Max Planck ETH Center for Learning
Systems.

References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for
large-scale machine learning. 2016.

[2] Ungar Abraham Albert. Analytic hyperbolic geometry and Albert Einstein’s special theory of

relativity. World scientiﬁc, 2008.

8https://github.com/dalab/hyperbolic_nn

11

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of the International Conference on Learning
Representations (ICLR), 2015.

[4] Graciela S Birman and Abraham A Ungar. The hyperbolic derivative in the poincaré ball model
of hyperbolic geometry. Journal of mathematical analysis and applications, 254(1):321–333,
2001.

[5] S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on

Automatic Control, 58(9):2217–2229, Sept 2013.

[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems (NIPS), pages 2787–2795, 2013.

[7] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 632–642. Association
for Computational Linguistics, 2015.

[8] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,
34(4):18–42, 2017.

[9] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry.

Flavors of geometry, 31:59–115, 1997.

[10] Christopher De Sa, Albert Gu, Christopher Ré, and Frederic Sala. Representation tradeoffs for

hyperbolic embeddings. arXiv preprint arXiv:1804.03329, 2018.

[11] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In Proceedings of the thirty-ﬁfth international conference
on machine learning (ICML), 2018.

[12] Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pages 75–263. Springer, 1987.

[13] Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the

Cambridge Philosophical Society, page 1–17, 2017.

[14] Christopher Hopper and Ben Andrews. The Ricci ﬂow in Riemannian geometry. Springer, 2010.

[15] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1746–1751. Association for Computational Linguistics, 2014.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations (ICLR), 2015.

[17] Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marián Boguná.

Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.

[18] John Lamping, Ramana Rao, and Peter Pirolli. A focus+ context technique based on hyperbolic
geometry for visualizing large hierarchies. In Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 401–408. ACM Press/Addison-Wesley Publishing Co.,
1995.

[19] Guy Lebanon and John Lafferty. Hyperplane margin classiﬁers on the multinomial manifold. In
Proceedings of the international conference on machine learning (ICML), page 66. ACM, 2004.

[20] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective
learning on multi-relational data. In Proceedings of the international conference on machine
learning (ICML), volume 11, pages 809–816, 2011.

12

[21] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical repre-
sentations. In Advances in Neural Information Processing Systems (NIPS), pages 6341–6350,
2017.

[22] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blun-
som. Reasoning about entailment with neural attention. In Proceedings of the International
Conference on Learning Representations (ICLR), 2015.

[23] Michael Spivak. A comprehensive introduction to differential geometry. Publish or perish, 1979.

[24] Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In Proceedings of

the International Conference on Learning Representations (ICLR), 2018.

[25] Abraham A Ungar. Hyperbolic trigonometry and its application in the poincaré ball model of

hyperbolic geometry. Computers & Mathematics with Applications, 41(1-2):135–147, 2001.

[26] Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis

Lectures on Mathematics and Statistics, 1(1):1–194, 2008.

[27] Abraham Albert Ungar. Analytic hyperbolic geometry in n dimensions: An introduction. CRC

Press, 2014.

[28] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. In Proceedings of the International Conference on Learning Representations (ICLR),
2016.

[29] J Vermeer. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic

plane. Topology and its Applications, 152(3):226–242, 2005.

13

A Hyperbolic Trigonometry

Hyperbolic angles. For A, B, C ∈ Dn
c , we denote by ∠A := ∠BAC the angle between the two
geodesics starting from A and ending at B and C respectively. This angle can be deﬁned in two
equivalent ways: i) either using the angle between the initial velocities of the two geodesics as given
by Eq. 5, or ii) using the formula

cos(∠A) =

(cid:28) (−A) ⊕c B
(cid:107)(−A) ⊕c B(cid:107)

,

(−A) ⊕c C
(cid:107)(−A) ⊕c C(cid:107)

(cid:29)

,

In this case, ∠A is also called a gyroangle in the work of [26, section 4].

Hyperbolic law of sines. We state here the hyperbolic law of sines. If for A, B, C ∈ Dn
c , we
denote by ∠B := ∠ABC the angle between the two geodesics starting from B and ending at A and
C respectively, and by ˜c = dc(B, A) the length of the hyperbolic segment BA (and similarly for
others), then we have:

sin(∠A)
√
c˜a)
sinh(

=

sin(∠B)
√
c˜b)
sinh(

=

sin(∠C)
√
c˜c)
sinh(

.

Note that one can also adapt the hyperbolic law of cosines to the hyperbolic space.

B Proof of Theorem 4

Theorem 4.
In the manifold (Dn
to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a vector v ∈ T0Dn
c

c is given by the following isometry:
λc
0
λc
x

0→x(v) = logc
P c

x(x ⊕c expc

0(v)) =

v.

Proof. The geodesic in Dn
v ∈ T0Dn
γ (i.e. X(t) ∈ Tγ(t)Dn

c from 0 to x is given in Eq. (10) by γ(t) = x ⊗c t, for t ∈ [0, 1]. Let
c . Then it is of common knowledge that there exists a unique parallel9 vector ﬁeld X along

c , ∀t ∈ [0, 1]) such that X(0) = v. Let’s deﬁne:
X : t ∈ [0, 1] (cid:55)→ logc

γ(t)(γ(t) ⊕c expc

0(v)) ∈ Tγ(t)Dn
c .

Clearly, X is a vector ﬁeld along γ such that X(0) = v. Now deﬁne
0→x : v ∈ T0Dn
P c

x(x ⊕c expc

0(v)) ∈ TxDn
c .

c (cid:55)→ logc
0→x(v) = λc

c . Since P c

0→x is a linear isometry from T0Dn
v, hence P c
c
0→x(v) = X(1), it is enough to prove that X is parallel in order to guarantee that

From Eq. (12), it is easily seen that P c
to TxDn
c to TxDn
0→x is the parallel transport from T0Dn
P c
c .
Since X is a vector ﬁeld along γ, its covariant derivative can be expressed with the Levi-Civita
connection ∇c associated to gc:

0
λc
x

DX
∂t

= ∇c

˙γ(t)X.

Let’s compute the Levi-Civita connection from its Christoffel symbols. In a local coordinate system,
they can be written as

Γi

jk =

(gc)il(∂jgc

lk + ∂kgc

lj − ∂lgc

jk),

1
2

where superscripts denote the inverse metric tensor and using Einstein’s notations. As gc
at γ(t) ∈ Dn

c this yields:

ij = (λc)2δij,

jk = cλc
Γi

γ(t)(δikγ(t)j + δijγ(t)k − δjkγ(t)i).

9i.e. that DX

∂t = 0 for t ∈ [0, 1], where D

∂t denotes the covariant derivative.

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

14

On the other hand, since X(t) = (λc

∇c

˙γ(t)X = ˙γ(t)i∇c

i X = ˙γ(t)i∇c
i

= vj ˙γ(t)i∇c
i

0/λc

γ(t))v, we have
(cid:32)

(cid:33)

λc
0
λc

γ(t)

v

(cid:32)

λc
0
λc

γ(t)

(cid:33)

ej

.

√

√

Since γ(t) = (1/
Hence there exists K x

c) tanh(t tanh−1(
t ∈ R such that ˙γ(t) = K x

c(cid:107)x(cid:107))) x

(cid:107)x(cid:107) , it is easily seen that ˙γ(t) is colinear to γ(t).
t γ(t). Moreover, we have the following Leibniz rule:
(cid:33)

(cid:32)

(cid:32)

∇c
i

λc
0
λc

γ(t)

(cid:33)

ej

=

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

ej.

Combining these yields

DX
∂t

= K x

t vjγ(t)i

(cid:32)

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

(cid:32)

(cid:33)

(cid:33)

ej

.

Replacing with the Christoffel symbols of ∇c at γ(t) gives

Moreover,

λc
0
λc

γ(t)

λc
0
λc

γ(t)

∇c

i ej =

ijek = 2c[δk
Γk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek.

∂
∂γ(t)i

(cid:32)

(cid:33)

λc
0
λc

γ(t)

ej =

∂
∂γ(t)i

(cid:0)−c(cid:107)γ(t)(cid:107)2(cid:1) ej = −2cγ(t)iej.

Putting together everything, we obtain

DX
∂t

= K x

t vjγ(t)i (cid:0)2c[δk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek − 2cγ(t)iej

(cid:1)

t vjγ(t)i (cid:0)γ(t)jei − δijγ(t)kek
t vj (cid:0)γ(t)jγ(t)iei − γ(t)iδijγ(t)kek
(cid:1)
t vj (cid:0)γ(t)jγ(t)iei − γ(t)jγ(t)kek

(cid:1)

(cid:1)

= 2cK x
= 2cK x
= 2cK x
= 0,

which concludes the proof.

C Proof of Eq. (22)

Proof. Two steps proof:
i) expc

p({a}⊥) ⊆ {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}:

Let z ∈ {a}⊥. From Eq. (12), we have that:

This, together with the left-cancellation law in gyrospaces (see section 2.3), implies that

expc

p(z) = −p ⊕c βz,

for some β ∈ R.

(cid:104)−p ⊕c expc

p(z), a(cid:105) = (cid:104)βz, a(cid:105) = 0

which is what we wanted.

ii) {x ∈ Dn
Let x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0} ⊆ expc
c s.t. (cid:104)−p ⊕c x, a(cid:105) = 0. Then, using Eq. (12), we derive that:
for some β ∈ R,

p(x) = β(−p ⊕c x),

p({a}⊥):

logc

which is orthogonal to a, by assumption. This implies logc

p(x) ∈ {a}⊥, hence x ∈ expc

p({a}⊥).

15

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(54)

D Proof of Theorem 5

Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

1
√
c

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(55)

Proof. We ﬁrst need to prove the following lemma, trivial in the Euclidean space, but not in the
Poincaré ball:
Lemma 7. (Orthogonal projection on a geodesic) Any point in the Poincaré ball has a unique
orthogonal projection on any given geodesic that does not pass through the point. Formally, for all
y ∈ Dn
c and for all geodesics γx→z(·) s.t. y /∈ Im γx→z, there exists an unique w ∈ Im γx→z s.t.
∠(γw→y, γx→z) = π/2.

Proof. We ﬁrst note that any geodesic in Dn
and has two "points at inﬁnity" lying on the ball border (v (cid:54)= 0):

c has the form γ(t) = u ⊕c v ⊗c t as given by Eq. 11,

γ(±∞) = u ⊕c

√

±v
c(cid:107)v(cid:107)

∈ ∂Dn
c .

(56)

Using the notations in the lemma statement, the closed-form of γx→z is given by Eq. (10):

γx→z(t) = x ⊕c (−x ⊕c z) ⊗c t

We denote by x(cid:48), z(cid:48) ∈ ∂Dn
∠ywx(cid:48) is well deﬁned from Eq. (34):

c its points at inﬁnity as described by Eq. (56). Then, the hyperbolic angle

cos(∠(γw→y, γx→z)) = cos(∠ywz(cid:48)) =

(cid:104)−w ⊕c y, −w ⊕c z(cid:48)(cid:105)
(cid:107) − w ⊕c y(cid:107) · (cid:107) − w ⊕c z(cid:48)(cid:107)

.

(57)

We now perform 2 steps for this proof.

i) Existence of w:

The angle function from Eq. (57) is continuous w.r.t t when w = γx→z(t). So we ﬁrst prove existence
of an angle of π/2 by continuously moving w from x(cid:48) to z(cid:48) when t goes from −∞ to ∞, and
observing that cos(∠ywz(cid:48)) goes from −1 to 1 as follows:

cos(∠yx(cid:48)z(cid:48)) = 1 & lim
w→z(cid:48)

cos(∠ywz(cid:48)) = −1.

(58)

The left part of Eq. (58) follows from Eq. (57) and from the fact (easy to show from the deﬁnition
c (which is the case of x(cid:48)). The right part of Eq. (58)
of ⊕c) that a ⊕c b = a, when (cid:107)a(cid:107) = 1/
follows from the fact that ∠ywz(cid:48) = π − ∠ywx(cid:48) (from the conformal property, or from Eq. (34)) and
cos(∠yz(cid:48)x(cid:48)) = 1 (proved as above).
Hence cos(∠ywz(cid:48)) has to pass through 0 when going from −1 to 1, which achieves the proof of
existence.

√

ii) Uniqueness of w:
Assume by contradiction that there are two w and w(cid:48) on γx→z that form angles ∠ywx(cid:48) and ∠yw(cid:48)x(cid:48)
of π/2. Since w, w(cid:48), x(cid:48) are on the same geodesic, we have

π/2 = ∠yw(cid:48)x(cid:48) = ∠yw(cid:48)w = ∠ywx(cid:48) = ∠yw(cid:48)w
So ∆yww(cid:48) has two right angles, but in the Poincaré ball this is impossible.

(59)

Now, we need two more lemmas:
Lemma 8. (Minimizing distance from point to geodesic) The orthogonal projection of a point to
a geodesic (not passing through the point) is minimizing the distance between the point and the
geodesic.

Proof. The proof is similar with the Euclidean case and it’s based on hyperbolic sine law and the fact
that in any right hyperbolic triangle the hypotenuse is strictly longer than any of the other sides.

16

Lemma 9. (Geodesics through p) Let ˜H c
all points on the geodesic γp→w are included in ˜H c

a,p.

a,p be a Poincaré hyperplane. Then, for any w ∈ ˜H c

a,p \ {p},

Proof. γp→w(t) = p ⊕c (−p ⊕c w) ⊗c t. Then, it is easy to check the condition in Eq. (22):

(cid:104)−p ⊕c γp→w(t), a(cid:105) = (cid:104)(−p ⊕c w) ⊗c t, a(cid:105) ∝ (cid:104)−p ⊕c w, a(cid:105) = 0.

(60)

We now turn back to our proof. Let x ∈ Dn
We prove that there is at least one point w∗ ∈ ˜H c

c be an arbitrary point and ˜H c

a,p a Poincaré hyperplane.

a,p that achieves the inﬁmum distance

dc(x, w∗) = inf
w∈ ˜H c

a,p

dc(x, w),

and, moreover, that this distance is the same as the one in the theorem’s statement.
We ﬁrst note that for any point w ∈ ˜H c
and Lemma 9, it is obvious that the projection of x to γp→w will give a strictly lower distance.
Thus, we only consider w ∈ ˜H c
triangle ∆xwp, one gets:

a,p such that ∠xwp = π/2. Applying hyperbolic sine law in the right

a,p, if ∠xwp (cid:54)= π/2, then w (cid:54)= w∗. Indeed, using Lemma 8

dc(x, w) = (1/

c) sinh−1 (cid:0)sinh(

c dc(x, p)) · sin(∠xpw)(cid:1) .

√

√

One of the above quantities does not depend on w:

√

√

sinh(

c dc(x, p)) = sinh(2 tanh−1(

c(cid:107) − p ⊕c x(cid:107))) =

√
2
c(cid:107) − p ⊕c x(cid:107)
1 − c(cid:107) − p ⊕c x(cid:107)2 .

The other quantity is sin(∠xpw) which is minimized when the angle ∠xpw is minimized (be-
cause ∠xpw < π/2 for the hyperbolic right triangle ∆xwp), or, alternatively, when cos(∠xpw) is
maximized. But, we already have from Eq. (34) that:

cos(∠xpw) =

(cid:104)−p ⊕c x, −p ⊕c w(cid:105)
(cid:107) − p ⊕c x(cid:107) · (cid:107) − p ⊕c w(cid:107)

.

To maximize the above, the constraint on the right angle at w can be dropped because cos(∠xpw)
depends only on the geodesic γp→w and not on w itself, and because there is always an orthogonal
projection from any point x to any geodesic as stated by Lemma 7. Thus, it remains to ﬁnd the
maximum of Eq. (64) when w ∈ ˜H c
a,p from Eq. (22), one can easily
prove that

a,p. Using the deﬁnition of ˜H c

Using that fact that logc

p(w)/(cid:107) logc

p(w)(cid:107) = −p ⊕c w/(cid:107) − p ⊕c w(cid:107), we just have to ﬁnd

and we are left with a well known Euclidean problem which is equivalent to ﬁnding the minimum
angle between the vector −p ⊕c x (viewed as Euclidean) and the hyperplane {a}⊥. This angle
is given by the Euclidean orthogonal projection whose sin value is the distance from the vector’s
endpoint to the hyperplane divided by the vector’s length:

{logc

p(w) : w ∈ ˜H c

a,p} = {a}⊥.

max
z∈{a}⊥

(cid:18) (cid:104)−p ⊕c x, z(cid:105)

(cid:107) − p ⊕c x(cid:107) · (cid:107)z(cid:107)

(cid:19)

,

sin(∠xpw∗) =

|(cid:104)−p ⊕c x, a
(cid:107) − p ⊕c x(cid:107)

(cid:107)a(cid:107) (cid:105)|

.

17

It follows that a point w∗ ∈ ˜H c
Eqs. (61),(62),(63) and (67) concludes the proof.

a,p satisfying Eq. (67) exists (but might not be unique). Combining

(61)

(62)

(63)

(64)

(65)

(66)

(67)

(cid:3)

E Derivation of the Hyperbolic GRU Update-gate

In [24], the authors recover the update/forget-gate mechanism of a GRU/LSTM by requiring that the
class of neural networks given by the chosen architecture be invariant to time-warpings. The idea is
the following.

Recovering the update-gate from time-warping. A naive RNN is given by the equation

h(t + 1) = ϕ(W h(t) + U x(t) + b)

Let’s drop the bias b to simplify notations. If h is seen as a differentiable function of time, then a
ﬁrst-order Taylor development gives h(t + δt) ≈ h(t) + δt dh
dt (t) for small δt. Combining this for
δt = 1 with the naive RNN equation, one gets

dh
dt

dα
dt

(t) = ϕ(W h(t) + U x(t)) − h(t).

As this is written for any t, one can replace it by t ← α(t) where α is a (smooth) increasing function
of t called the time-warping. Denoting by ˜h(t) := h(α(t)) and ˜x(t) := x(α(t)), using the chain rule
d˜h
dt (t) = dα

dt (α(t)), one gets

dt (t) dh

d˜h
dt

dα
dt

(t) =

(t)ϕ(W ˜h(t) + U ˜x(t)) −

(t)˜h(t).

(70)

Removing the tildas to simplify notations, discretizing back with dh

dt (t) ≈ h(t + 1) − h(t) yields

h(t + 1) =

(t)ϕ(W h(t) + U x(t)) +

1 −

(t)

h(t).

(71)

dα
dt

(cid:18)

(cid:19)

dα
dt

Requiring that our class of neural networks be invariant to time-warpings means that this class should
contain RNNs deﬁned by Eq. (71), i.e. that dα
dt (t) can be learned. As this is a positive quantity, we
can parametrize it as z(t) = σ(W zh(t) + U zx(t)), recovering the forget-gate equation:

h(t + 1) = z(t)ϕ(W h(t) + U x(t)) + (1 − z(t))h(t).

Adapting this idea to hyperbolic RNNs. The gyroderivative [4] of a map h : R → Dn
as

c is deﬁned

dh
dt

(t) = lim
δt→0

1
δt

⊗c (−h(t) ⊕c h(t + δt)).

Using Möbius scalar associativity and the left-cancellation law leads us to

h(t + δt) ≈ h(t) ⊕c δt ⊗c

(t),

dh
dt

for small δt. Combining this with the equation of a simple hyperbolic RNN of Eq. (29) with δt = 1,
one gets

dh
dt

(t) = −h(t) ⊕c ϕ⊗c(W ⊗c h(t) ⊕c U ⊗c x(t)).

For the next step, we need the following lemma:
Lemma 10 (Gyro-chain-rule). For α : R → R differentiable and h : R → Dn
gyro-derivative, if ˜h := h ◦ α, then we have

c with a well-deﬁned

(68)

(69)

(72)

(73)

(74)

(75)

(76)

where dα

dt (t) denotes the usual derivative.

d˜h
dt

(t) =

(t) ⊗c

(α(t)),

dα
dt

dh
dt

18

(77)

(78)

(79)

(80)

(81)

Proof.

d˜h
dt

(t) = lim
δt→0

1
δt
1
δt

⊗c [−˜h(t) ⊕c

˜h(t + δt)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

= lim
δt→0

= lim
δt→0

= lim
δt→0

= lim
u→0
dα
dt

=

α(cid:48)(t) + O(δt)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
u

(t) ⊗c

(α(t))

dh
dt

⊗c [−h(α(t)) ⊕c h(α(t) + u)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

(Möbius scalar associativity) (82)

where we set u = δt(α(cid:48)(t) + O(δt)), with u → 0 when δt → 0, which concludes.

Using lemma 10 and Eq. (75), with similar notations as in Eq. (70) we have

d˜h
dt

dα
dt

(t) =

(t) ⊗c (−˜h(t) ⊕c ϕ⊗c(W ⊗c

˜h(t) ⊕c U ⊗c ˜x(t))).

(83)

Finally, discretizing back with Eq. (74), using the left-cancellation law and dropping the tildas yields

h(t + 1) = h(t) ⊕c

(t) ⊗c (−h(t) ⊕c ϕ⊗c (W ⊗c h(t) ⊕c U ⊗c x(t))).

(84)

dα
dt

Since α is a time-warping, by deﬁnition its derivative is positive and one can choose to parametrize
it with an update-gate zt (a scalar) deﬁned with a sigmoid. Generalizing this scalar scaling by the
Möbius version of the pointwise scaling (cid:12) yields the Möbius matrix scaling diag(zt) ⊗c ·, leading to
our proposed Eq. (33) for the hyperbolic GRU.

F More Experimental Investigations

The following empirical facts were observed for both hyperbolic RNNs and GRUs.

We observed that, in the hyperbolic setting, accuracy is often much higher when sentence embeddings
can go close to the border (hyperbolic "inﬁnity"), hence exploiting the hyperbolic nature of the space.
Moreover, the faster the two sentence norms go to 1, the more it’s likely that a good local minima
was reached. See ﬁgures 3 and 5.

We often observe that test accuracy starts increasing exactly when sentence embedding norms do.
However, in the hyperbolic setting, the sentence embeddings norms remain close to 0 for a few
epochs, which does not happen in the Euclidean case. See ﬁgures 3, 5 and 4. This mysterious fact
was also exhibited in a similar way by [21] which suggests that the model ﬁrst has to adjust the
angular layout in the almost Euclidean vicinity of 0 before increasing norms and fully exploiting
hyperbolic geometry.

19

(a) Test accuracy

(a) Test accuracy

20

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 3: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the same
architecture: hyperbolic GRU followed by hyperbolic FFNN and hyperbolic/Euclidean (half-half)
MLR. The X axis shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 4: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: Euclidean GRU followed by Euclidean FFNN and Euclidean MLR. The X axis
shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 5: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: hyperbolic RNN followed by hyperbolic FFNN and hyperbolic MLR. The X axis
shows millions of training examples processed.

(a) Test accuracy

21

8
1
0
2
 
n
u
J
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
2
1
1
9
0
.
5
0
8
1
:
v
i
X
r
a

Hyperbolic Neural Networks

Octavian-Eugen Ganea∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
octavian.ganea@inf.ethz.ch

Gary Bécigneul∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
gary.becigneul@inf.ethz.ch

Thomas Hofmann
Department of Computer Science
ETH Zürich
Zurich, Switzerland
thomas.hofmann@inf.ethz.ch

Abstract

Hyperbolic spaces have recently gained momentum in the context of machine
learning due to their high capacity and tree-likeliness properties. However, the
representational power of hyperbolic geometry is not yet on par with Euclidean
geometry, mostly because of the absence of corresponding hyperbolic neural
network layers. This makes it hard to use hyperbolic embeddings in downstream
tasks. Here, we bridge this gap in a principled manner by combining the formalism
of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model
of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep
learning tools: multinomial logistic regression, feed-forward and recurrent neural
networks such as gated recurrent units. This allows to embed sequential data and
perform classiﬁcation in the hyperbolic space. Empirically, we show that, even if
hyperbolic optimization tools are limited, hyperbolic sentence embeddings either
outperform or are on par with their Euclidean variants on textual entailment and
noisy-preﬁx recognition tasks.

1

Introduction

It is common in machine learning to represent data as being embedded in the Euclidean space Rn. The
main reason for such a choice is simply convenience, as this space has a vectorial structure, closed-
form formulas of distance and inner-product, and is the natural generalization of our intuition-friendly,
visual three-dimensional space. Moreover, embedding entities in such a continuous space allows to
feed them as input to neural networks, which has led to unprecedented performance on a broad range
of problems, including sentiment detection [15], machine translation [3], textual entailment [22] or
knowledge base link prediction [20, 6].

Despite the success of Euclidean embeddings, recent research has proven that many types of com-
plex data (e.g. graph data) from a multitude of ﬁelds (e.g. Biology, Network Science, Computer
Graphics or Computer Vision) exhibit a highly non-Euclidean latent anatomy [8]. In such cases, the
Euclidean space does not provide the most powerful or meaningful geometrical representations. For
example, [10] shows that arbitrary tree structures cannot be embedded with arbitrary low distortion
(i.e. almost preserving their metric) in the Euclidean space with unbounded number of dimensions,
but this task becomes surprisingly easy in the hyperbolic space with only 2 dimensions where the
exponential growth of distances matches the exponential growth of nodes with the tree depth.

∗Equal contribution.

The adoption of neural networks and deep learning in these non-Euclidean settings has been rather
limited until very recently, the main reason being the non-trivial or impossible principled general-
izations of basic operations (e.g. vector addition, matrix-vector multiplication, vector translation,
vector inner product) as well as, in more complex geometries, the lack of closed form expressions for
basic objects (e.g. distances, geodesics, parallel transport). Thus, classic tools such as multinomial
logistic regression (MLR), feed forward (FFNN) or recurrent neural networks (RNN) did not have a
correspondence in these geometries.

How should one generalize deep neural models to non-Euclidean domains ? In this paper we address
this question for one of the simplest, yet useful, non-Euclidean domains: spaces of constant negative
curvature, i.e. hyperbolic. Their tree-likeness properties have been extensively studied [12, 13, 26]
and used to visualize large taxonomies [18] or to embed heterogeneous complex networks [17]. In
machine learning, recently, hyperbolic representations greatly outperformed Euclidean embeddings
for hierarchical, taxonomic or entailment data [21, 10, 11]. Disjoint subtrees from the latent hierar-
chical structure surprisingly disentangle and cluster in the embedding space as a simple reﬂection of
the space’s negative curvature. However, appropriate deep learning tools are needed to embed feature
data in this space and use it in downstream tasks. For example, implicitly hierarchical sequence data
(e.g. textual entailment data, phylogenetic trees of DNA sequences or hierarchial captions of images)
would beneﬁt from suitable hyperbolic RNNs.

The main contribution of this paper is to bridge the gap between hyperbolic and Euclidean geometry
in the context of neural networks and deep learning by generalizing in a principled manner both the
basic operations as well as multinomial logistic regression (MLR), feed-forward (FFNN), simple and
gated (GRU) recurrent neural networks (RNN) to the Poincaré model of the hyperbolic geometry.
We do it by connecting the theory of gyrovector spaces and generalized Möbius transformations
introduced by [2, 26] with the Riemannian geometry properties of the manifold. We smoothly
parametrize basic operations and objects in all spaces of constant negative curvature using a uniﬁed
framework that depends only on the curvature value. Thus, we show how Euclidean and hyperbolic
spaces can be continuously deformed into each other. On a series of experiments and datasets we
showcase the effectiveness of our hyperbolic neural network layers compared to their "classic"
Euclidean variants on textual entailment and noisy-preﬁx recognition tasks. We hope that this paper
will open exciting future directions in the nascent ﬁeld of Geometric Deep Learning.

2 The Geometry of the Poincaré Ball

2.1 Basics of Riemannian geometry

We brieﬂy introduce basic concepts of differential geometry largely needed for a principled general-
ization of Euclidean neural networks. For more rigorous and in-depth expositions, see [23, 14].
An n-dimensional manifold M is a space that can locally be approximated by Rn: it is a generalization
to higher dimensions of the notion of a 2D surface. For x ∈ M, one can deﬁne the tangent space
TxM of M at x as the ﬁrst order linear approximation of M around x. A Riemannian metric
g = (gx)x∈M on M is a collection of inner-products gx : TxM × TxM → R varying smoothly
with x. A Riemannian manifold (M, g) is a manifold M equipped with a Riemannian metric g.
Although a choice of a Riemannian metric g on M only seems to deﬁne the geometry locally on M,
it induces global distances by integrating the length (of the speed vector living in the tangent space)
of a shortest path between two points:

(cid:90) 1

(cid:113)

d(x, y) = inf
γ

0

gγ(t)( ˙γ(t), ˙γ(t))dt,

(1)

where γ ∈ C∞([0, 1], M) is such that γ(0) = x and γ(1) = y. A smooth path γ of minimal length
between two points x and y is called a geodesic, and can be seen as the generalization of a straight-line
in Euclidean space. The parallel transport Px→y : TxM → TyM is a linear isometry between
tangent spaces which corresponds to moving tangent vectors along geodesics and deﬁnes a canonical
way to connect tangent spaces. The exponential map expx at x, when well-deﬁned, gives a way to
project back a vector v of the tangent space TxM at x, to a point expx(v) ∈ M on the manifold.
This map is often used to parametrize a geodesic γ starting from γ(0) := x ∈ M with unit-norm
direction ˙γ(0) := v ∈ TxM as t (cid:55)→ expx(tv). For geodesically complete manifolds, such as the
Poincaré ball considered in this work, expx is well-deﬁned on the full tangent space TxM. Finally, a

2

(2)

(3)

(4)

(5)

metric ˜g is said to be conformal to another metric g if it deﬁnes the same angles, i.e.

˜gx(u, v)
(cid:112)˜gx(u, u)(cid:112)˜gx(v, v)

=

gx(u, v)
(cid:112)gx(u, u)(cid:112)gx(v, v)

,

for all x ∈ M, u, v ∈ TxM \ {0}. This is equivalent to the existence of a smooth function
λ : M → R, called the conformal factor, such that ˜gx = λ2

xgx for all x ∈ M.

2.2 Hyperbolic space: the Poincaré ball

The hyperbolic space has ﬁve isometric models that one can work with [9]. Similarly as in [21] and
[11], we choose to work in the Poincaré ball. The Poincaré ball model (Dn, gD) is deﬁned by the
manifold Dn = {x ∈ Rn : (cid:107)x(cid:107) < 1} equipped with the following Riemannian metric:

gD
x = λ2

xgE, where λx :=

2
1 − (cid:107)x(cid:107)2 ,

gE = In being the Euclidean metric tensor. Note that the hyperbolic metric tensor is conformal to
the Euclidean one. The induced distance between two points x, y ∈ Dn is known to be given by

dD(x, y) = cosh−1

1 + 2

(cid:18)

(cid:107)x − y(cid:107)2
(1 − (cid:107)x(cid:107)2)(1 − (cid:107)y(cid:107)2)

(cid:19)

.

Since the Poincaré ball is conformal to Euclidean space, the angle between two vectors u, v ∈
TxDn \ {0} is given by

cos(∠(u, v)) =

gD
x (u, v)
x (u, u)(cid:112)gD

(cid:112)gD

x (v, v)

=

(cid:104)u, v(cid:105)
(cid:107)u(cid:107)(cid:107)v(cid:107)

.

2.3 Gyrovector spaces

In Euclidean space, natural operations inherited from the vectorial structure, such as vector addition,
subtraction and scalar multiplication are often useful. The framework of gyrovector spaces provides
an elegant non-associative algebraic formalism for hyperbolic geometry just as vector spaces provide
the algebraic setting for Euclidean geometry [2, 25, 26].

In particular, these operations are used in special relativity, allowing to add speed vectors belonging
to the Poincaré ball of radius c (the celerity, i.e. the speed of light) so that they remain in the ball,
hence not exceeding the speed of light.

We will make extensive use of these operations in our deﬁnitions of hyperbolic neural networks.
For c ≥ 0, denote2 by Dn
then Dn

c := {x ∈ Rn | c(cid:107)x(cid:107)2 < 1}. Note that if c = 0, then Dn
c. If c = 1 then we recover the usual ball Dn.

c is the open ball of radius 1/

c = Rn; if c > 0,

√

Möbius addition. The Möbius addition of x and y in Dn

c is deﬁned as

x ⊕c y :=

(1 + 2c(cid:104)x, y(cid:105) + c(cid:107)y(cid:107)2)x + (1 − c(cid:107)x(cid:107)2)y
1 + 2c(cid:104)x, y(cid:105) + c2(cid:107)x(cid:107)2(cid:107)y(cid:107)2

.

(6)

In particular, when c = 0, one recovers the Euclidean addition of two vectors in Rn. Note that
without loss of generality, the case c > 0 can be reduced to c = 1. Unless stated otherwise, we
will use ⊕ as ⊕1 to simplify notations. For general c > 0, this operation is not commutative nor
associative. However, it satisﬁes x ⊕c 0 = 0 ⊕c x = 0. Moreover, for any x, y ∈ Dn
c , we have
(−x) ⊕c x = x ⊕c (−x) = 0 and (−x) ⊕c (x ⊕c y) = y (left-cancellation law). The Möbius
substraction is then deﬁned by the use of the following notation: x (cid:9)c y := x ⊕c (−y). See [29,
section 2.1] for a geometric interpretation of the Möbius addition.

2We take different notations as in [25] where the author uses s = 1/

c.

√

3

Möbius scalar multiplication. For c > 0, the Möbius scalar multiplication of x ∈ Dn
r ∈ R is deﬁned as

c \ {0} by

r ⊗c x := (1/

c) tanh(r tanh−1(

c(cid:107)x(cid:107)))

√

√

x
(cid:107)x(cid:107)

,

(7)

and r ⊗c 0 := 0. Note that similarly as for the Möbius addition, one recovers the Euclidean scalar
multiplication when c goes to zero: limc→0 r ⊗c x = rx. This operation satisﬁes desirable properties
such as n ⊗c x = x ⊕c · · · ⊕c x (n additions), (r + r(cid:48)) ⊗c x = r ⊗c x ⊕c r(cid:48) ⊗c x (scalar distributivity3),
(rr(cid:48)) ⊗c x = r ⊗c (r(cid:48) ⊗c x) (scalar associativity) and |r| ⊗c x/(cid:107)r ⊗c x(cid:107) = x/(cid:107)x(cid:107) (scaling property).

c , gc) is given by4

Distance.
Euclidean one, with conformal factor λc
(Dn

If one deﬁnes the generalized hyperbolic metric tensor gc as the metric conformal to the
x := 2/(1 − c(cid:107)x(cid:107)2), then the induced distance function on
√

c) tanh−1 (cid:0)√
Again, observe that limc→0 dc(x, y) = 2(cid:107)x − y(cid:107), i.e. we recover Euclidean geometry in the limit5.
Moreover, for c = 1 we recover dD of Eq. (4).

c(cid:107) − x ⊕c y(cid:107)(cid:1) .

dc(x, y) = (2/

(8)

Hyperbolic trigonometry. Similarly as in the Euclidean space, one can deﬁne the notions of
hyperbolic angles or gyroangles (when using the ⊕c), as well as hyperbolic law of sines in the
generalized Poincaré ball (Dn

c , gc). We make use of these notions in our proofs. See Appendix A.

2.4 Connecting Gyrovector spaces and Riemannian geometry of the Poincaré ball

In this subsection, we present how geodesics in the Poincaré ball model are usually described with
Möbius operations, and push one step further the existing connection between gyrovector spaces and
the Poincaré ball by ﬁnding new identities involving the exponential map, and parallel transport.

In particular, these ﬁndings provide us with a simpler formulation of Möbius scalar multiplication,
yielding a natural deﬁnition of matrix-vector multiplication in the Poincaré ball.

Riemannian gyroline element. The Riemannian gyroline element is deﬁned for an inﬁnitesimal
dx as ds := (x + dx) (cid:9)c x, and its size is given by [26, section 3.7]:

(cid:107)ds(cid:107) = (cid:107)(x + dx) (cid:9)c x(cid:107) = (cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2).

(9)

What is remarkable is that it turns out to be identical, up to a scaling factor of 2, to the usual line
element 2(cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2) of the Riemannian manifold (Dn

c , gc).

Geodesics. The geodesic connecting points x, y ∈ Dn

c is shown in [2, 26] to be given by:

γx→y(t) := x ⊕c (−x ⊕c y) ⊗c t, with γx→y : R → Dn

c s.t. γx→y(0) = x and γx→y(1) = y.

Note that when c goes to 0, geodesics become straight-lines, recovering Euclidean geometry. In the
remainder of this subsection, we connect the gyrospace framework with Riemannian geometry.
Lemma 1. For any x ∈ Dn and v ∈ TxDn
c s.t. gc
x with direction v is given by:

x(v, v) = 1, the unit-speed geodesic starting from

γx,v(t) = x ⊕c

tanh

(cid:18)

(cid:18)√

(cid:19) v
√

c

t
2

c(cid:107)v(cid:107)

(cid:19)

, where γx,v : R → Dn s.t. γx,v(0) = x and ˙γx,v(0) = v.

(10)

(11)

Proof. One can use Eq. (10) and reparametrize it to unit-speed using Eq. (8). Alternatively, direct
computation and identiﬁcation with the formula in [11, Thm. 1] would give the same result. Using
Eq. (8) and Eq. (11), one can sanity-check that dc(γ(0), γ(t)) = t, ∀t ∈ [0, 1].

3⊗c has priority over ⊕c in the sense that a ⊗c b ⊕c c := (a ⊗c b) ⊕c c and a ⊕c b ⊗c c := a ⊕c (b ⊗c c).
4The notation −x ⊕c y should always be read as (−x) ⊕c y and not −(x ⊕c y).
5The factor 2 comes from the conformal factor λx = 2/(1 − (cid:107)x(cid:107)2), which is a convention setting the

curvature to −1.

4

Exponential and logarithmic maps. The following lemma gives the closed-form derivation of
exponential and logarithmic maps.
Lemma 2. For any point x ∈ Dn
map logc
c → TxDn
(cid:18)

c are given for v (cid:54)= 0 and y (cid:54)= x by:
(cid:18)√

c , the exponential map expc

c and the logarithmic

x : TxDn

c → Dn

x : Dn

√

(cid:19)

, logc

x(y) =

√

tanh−1(

c(cid:107) − x ⊕c y(cid:107))

expc

x(v) = x ⊕c

tanh

c

λc
x(cid:107)v(cid:107)
2

(cid:19) v
√

c(cid:107)v(cid:107)

2
cλc
x

−x ⊕c y
(cid:107) − x ⊕c y(cid:107)
(12)

.

Proof. Following the proof of [11, Cor. 1.1], one gets expc
gives the formula for expc

x. Algebraic check of the identity logc

x(v) = γx,
x(expc

v

x(cid:107)v(cid:107) (λc
λc

x(cid:107)v(cid:107)). Using Eq. (11)

x(v)) = v concludes.

The above maps have more appealing forms when x = 0, namely for v ∈ T0Dn

c \ {0}, y ∈ Dn

c \ {0}:

expc

0(v) = tanh(

c(cid:107)v(cid:107))

√

, logc

0(y) = tanh−1(

c(cid:107)y(cid:107))

√

(13)

√

y
c(cid:107)y(cid:107)

.

√

v
c(cid:107)v(cid:107)

Moreover, we still recover Euclidean geometry in the limit c → 0, as limc→0 expc
Euclidean exponential map, and limc→0 logc

x(y) = y − x is the Euclidean logarithmic map.

x(v) = x + v is the

Möbius scalar multiplication using exponential and logarithmic maps. We studied the expo-
nential and logarithmic maps in order to gain a better understanding of the Möbius scalar multiplica-
tion (Eq. (7)). We found the following:
Lemma 3. The quantity r ⊗ x can actually be obtained by projecting x in the tangent space at 0
with the logarithmic map, multiplying this projection by the scalar r in T0Dn
c , and then projecting it
back on the manifold with the exponential map:
0(r logc

∀r ∈ R, x ∈ Dn
c .

r ⊗c x = expc

0(x)),

(14)

In addition, we recover the well-known relation between geodesics connecting two points and the
exponential map:

γx→y(t) = x ⊕c (−x ⊕c y) ⊗c t = expc

x(t logc

x(y)),

t ∈ [0, 1].

(15)

This last result enables us to generalize scalar multiplication in order to deﬁne matrix-vector multipli-
cation between Poincaré balls, one of the essential building blocks of hyperbolic neural networks.

Parallel transport. Finally, we connect parallel transport (from T0Dn
the following theorem, which we prove in appendix B.
Theorem 4. In the manifold (Dn
vector v ∈ T0Dn

c to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a
c is given by the following isometry:
λc
0
λc
x

x(x ⊕c expc

0(v)) =

0→x(v) = logc
P c

(16)

v.

c ) to gyrovector spaces with

As we’ll see later, this result is crucial in order to deﬁne and optimize parameters shared between
different tangent spaces, such as biases in hyperbolic neural layers or parameters of hyperbolic MLR.

3 Hyperbolic Neural Networks

Neural networks can be seen as being made of compositions of basic operations, such as linear
maps, bias translations, pointwise non-linearities and a ﬁnal sigmoid or softmax layer. We ﬁrst
explain how to construct a softmax layer for logits lying in a Poincaré ball. Then, we explain how
to transform a mapping between two Euclidean spaces as one between Poincaré balls, yielding
matrix-vector multiplication and pointwise non-linearities in the Poincaré ball. Finally, we present
possible adaptations of various recurrent neural networks to the hyperbolic domain.

5

3.1 Hyperbolic multiclass logistic regression

In order to perform multi-class classiﬁcation on the Poincaré ball, one needs to generalize multinomial
logistic regression (MLR) − also called softmax regression − to the Poincaré ball.

Reformulating Euclidean MLR. Let’s ﬁrst reformulate Euclidean MLR from the perspective of
distances to margin hyperplanes, as in [19, Section 5]. This will allow us to easily generalize it.

Given K classes, one learns a margin hyperplane for each such class using softmax probabilities:

∀k ∈ {1, ..., K},

p(y = k|x) ∝ exp (((cid:104)ak, x(cid:105) − bk)) , where bk ∈ R, x, ak ∈ Rn.

(17)

Note that any afﬁne hyperplane in Rn can be written with a normal vector a and a scalar shift b:

Ha,b = {x ∈ Rn : (cid:104)a, x(cid:105) − b = 0}, where a ∈ Rn \ {0}, and b ∈ R.

(18)

As in [19, Section 5], we note that (cid:104)a, x(cid:105) − b = sign((cid:104)a, x(cid:105) − b)(cid:107)a(cid:107)d(x, Ha,b). Using Eq. (17):

p(y = k|x) ∝ exp(sign((cid:104)ak, x(cid:105) − bk)(cid:107)ak(cid:107)d(x, Hak,bk )), bk ∈ R, x, ak ∈ Rn.

(19)

As it is not immediately obvious how to generalize the Euclidean hyperplane of Eq. (18) to other
spaces such as the Poincaré ball, we reformulate it as follows:

˜Ha,p = {x ∈ Rn : (cid:104)−p + x, a(cid:105) = 0} = p + {a}⊥, where p ∈ Rn, a ∈ Rn \ {0}.

(20)

This new deﬁnition relates to the previous one as ˜Ha,p = Ha,(cid:104)a,p(cid:105). Rewriting Eq. (19) with b = (cid:104)a, p(cid:105):
p(y = k|x) ∝ exp(sign((cid:104)−pk + x, ak(cid:105))(cid:107)ak(cid:107)d(x, ˜Hak,pk )), with pk, x, ak ∈ Rn.

(21)

It is now natural to adapt the previous deﬁnition to the hyperbolic setting by replacing + by ⊕c:
Deﬁnition 3.1 (Poincaré hyperplanes). For p ∈ Dn
p(z, a) = 0} = {z ∈ TpDn
gc

c : (cid:104)z, a(cid:105) = 0}. Then, we deﬁne Poincaré hyperplanes as

c \ {0}, let {a}⊥ := {z ∈ TpDn
c :

c , a ∈ TpDn

˜H c

a,p := {x ∈ Dn

c : (cid:104)logc

p(x), a(cid:105)p = 0} = expc

p({a}⊥) = {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}.

(22)

The last equality is shown appendix C. ˜H c
all geodesics in Dn
hypergyroplanes, see [27, deﬁnition 5.8]. A 3D hyperplane example is depicted in Fig. 1.

a,p can also be described as the union of images of
c orthogonal to a and containing p. Notice that our deﬁnition matches that of

Next, we need the following theorem, proved in appendix D:
Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(23)

Final formula for MLR in the Poincaré ball. Putting together Eq. (21) and Thm. 5, we get the
hyperbolic MLR formulation. Given K classes and k ∈ {1, . . . , K}, pk ∈ Dn
c \ {0}:

c , ak ∈ Tpk

Dn

p(y = k|x) ∝ exp(sign((cid:104)−pk ⊕c x, ak(cid:105))

gc
pk

(ak, ak)dc(x, ˜H c

)),

ak,pk

∀x ∈ Dn
c ,

(24)

or, equivalently

p(y = k|x) ∝ exp

(cid:18) λc
pk

(cid:107)ak(cid:107)
√
c

sinh−1

(cid:18)

√
2

c(cid:104)−pk ⊕c x, ak(cid:105)

(cid:19)(cid:19)

(1 − c(cid:107) − pk ⊕c x(cid:107)2)(cid:107)ak(cid:107)

,

∀x ∈ Dn
c .

(25)

this goes to p(y = k|x) ∝ exp(4(cid:104)−pk + x, ak(cid:105)) =

Notice that when c goes to zero,
exp((λ0
pk

)2(cid:104)−pk + x, ak(cid:105)) = exp((cid:104)−pk + x, ak(cid:105)0), recovering the usual Euclidean softmax.
However, at this point it is unclear how to perform optimization over ak, since it lives in Tpk
hence depends on pk. The solution is that one should write ak = P c
)a(cid:48)
k ∈ T0Dn
a(cid:48)

c = Rn, and optimize a(cid:48)

k as a Euclidean parameter.

k) = (λc

0/λc
pk

0→pk

(a(cid:48)

Dn
c and
k, where

1
√
c

(cid:113)

6

3.2 Hyperbolic feed-forward layers

In order to deﬁne hyperbolic neural networks, it is crucial to de-
ﬁne a canonically simple parametric family of transformations,
playing the role of linear mappings in usual Euclidean neural
networks, and to know how to apply pointwise non-linearities.
Inspiring ourselves from our reformulation of Möbius scalar
multiplication in Eq. (14), we deﬁne:
Deﬁnition 3.2 (Möbius version). For f : Rn → Rm, we deﬁne
the Möbius version of f as the map from Dn

c to Dm

c by:

f ⊗c(x) := expc

0(f (logc

0(x))),

(26)

where expc

0 : T0m

Dm

c → Dm

c and logc

0 : Dn

c → T0n

Dn
c .

Figure 1: An example of a hyper-
bolic hyperplane in D3
1 plotted us-
ing sampling. The red point is p.
The shown normal axis to the hy-
perplane through p is parallel to a.

Note that similarly as for other Möbius operations, we recover
the Euclidean mapping in the limit c → 0 if f is continuous, as limc→0 f ⊗c(x) = f (x). This
deﬁnition satisﬁes a few desirable properties too, such as: (f ◦ g)⊗c = f ⊗c ◦ g⊗c for f : Rm → Rl
and g : Rn → Rm (morphism property), and f ⊗c(x)/(cid:107)f ⊗c(x)(cid:107) = f (x)/(cid:107)f (x)(cid:107) for f (x) (cid:54)= 0
(direction preserving). It is then straight-forward to prove the following result:
Lemma 6 (Möbius matrix-vector multiplication). If M : Rn → Rm is a linear map, which we
identify with its matrix representation, then ∀x ∈ Dn

c , if M x (cid:54)= 0 we have

M ⊗c(x) = (1/

c) tanh

√

(cid:18) (cid:107)M x(cid:107)
(cid:107)x(cid:107)

√

tanh−1(

c(cid:107)x(cid:107))

(cid:19) M x
(cid:107)M x(cid:107)

,

(27)

and M ⊗c(x) = 0 if M x = 0. Moreover, if we deﬁne the Möbius matrix-vector multiplication of
M ∈ Mm,n(R) and x ∈ Dn
c by M ⊗c x := M ⊗c(x), then we have (M M (cid:48)) ⊗c x = M ⊗c (M (cid:48) ⊗c x)
for M ∈ Ml,m(R) and M (cid:48) ∈ Mm,n(R) (matrix associativity), (rM ) ⊗c x = r ⊗c (M ⊗c x) for
r ∈ R and M ∈ Mm,n(R) (scalar-matrix associativity) and M ⊗c x = M x for all M ∈ On(R)
(rotations are preserved).

Pointwise non-linearity.
ϕ⊗c can be applied to elements of the Poincaré ball.

If ϕ : Rn → Rn is a pointwise non-linearity, then its Möbius version

Bias translation. The generalization of a translation in the Poincaré ball is naturally given by
moving along geodesics. But should we use the Möbius sum x ⊕c b with a hyperbolic bias b or the
x(b(cid:48)) with a Euclidean bias b(cid:48)? These views are uniﬁed with parallel transport
exponential map expc
c by a bias b ∈ Dn
(see Thm 4). Möbius translation of a point x ∈ Dn
(cid:18) λc
0
λc
x

c is given by
(cid:19)

x ← x ⊕c b = expc

0(b))) = expc
x

0→x(logc

x(P c

logc

0(b)

(28)

.

We recover Euclidean translations in the limit c → 0. Note that bias translations play a particular
Indeed, consider multiple layers of the form fk(x) = ϕk(Mkx), each of
role in this model.
which having Möbius version f ⊗c
k (Mk ⊗c x). Then their composition can be re-written
f ⊗c
k ◦ · · · ◦ f ⊗c
1 = expc
0. This means that these operations can essentially be
performed in Euclidean space. Therefore, it is the interposition between those with the bias translation
of Eq. (28) which differentiates this model from its Euclidean counterpart.

k (x) = ϕ⊗c
0 ◦fk ◦ · · · ◦ f1 ◦ logc

If a vector x ∈ Rn+p is the (vertical) concatenation
Concatenation of multiple input vectors.
of two vectors x1 ∈ Rn, x2 ∈ Rp, and M ∈ Mm,n+p(R) can be written as the (horizontal)
concatenation of two matrices M1 ∈ Mm,n(R) and M2 ∈ Mm,p(R), then M x = M1x1 + M2x2.
We generalize this to hyperbolic spaces: if we are given x1 ∈ Dn
c ×Dp
c ,
and M, M1, M2 as before, then we deﬁne M ⊗c x := M1 ⊗c x1 ⊕c M2 ⊗c x2. Note that when c goes
to zero, we recover the Euclidean formulation, as limc→0 M ⊗c x = limc→0 M1 ⊗c x1 ⊕c M2 ⊗c x2 =
M1x1 + M2x2 = M x. Moreover, hyperbolic vectors x ∈ Dn
c can also be "concatenated" with real
features y ∈ R by doing: M ⊗c x ⊕c y ⊗c b with learnable b ∈ Dm

c , x = (x1 x2)T ∈ Dn

c and M ∈ Mm,n(R).

c , x2 ∈ Dp

7

3.3 Hyperbolic RNN

Naive RNN. A simple RNN can be deﬁned by ht+1 = ϕ(W ht + U xt + b) where ϕ is a pointwise
non-linearity, typically tanh, sigmoid, ReLU, etc. This formula can be naturally generalized to the
hyperbolic space as follows. For parameters W ∈ Mm,n(R), U ∈ Mm,d(R), b ∈ Dm
c , we deﬁne:

ht+1 = ϕ⊗c (W ⊗c ht ⊕c U ⊗c xt ⊕c b),

ht ∈ Dn

c , xt ∈ Dd
c .

(29)

Note that if inputs xt’s are Euclidean, one can write ˜xt := expc
expc

(U xt)) = W ⊗c ht ⊕c expc

(P c

W ⊗cht

0→W ⊗cht

0(U xt) = W ⊗c ht ⊕c U ⊗c ˜xt.

0(xt) and use the above formula, since

GRU architecture. One can also adapt the GRU architecture:
rt = σ(W rht−1 + U rxt + br),
zt = σ(W zht−1 + U zxt + bz),
˜ht = ϕ(W (rt (cid:12) ht−1) + U xt + b), ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht,

(30)

where (cid:12) denotes pointwise product. First, how should we adapt the pointwise multiplication by a
scaling gate? Note that the deﬁnition of the Möbius version (see Eq. (26)) can be naturally extended
to maps f : Rn × Rp → Rm as f ⊗c : (h, h(cid:48)) ∈ Dn
0(h(cid:48)))). In
c (cid:55)→ expc
0(h(cid:48))) =
particular, choosing f (h, h(cid:48)) := σ(h) (cid:12) h(cid:48) yields6 f ⊗c(h, h(cid:48)) = expc
diag(σ(logc

0(h))) ⊗c h(cid:48). Hence we adapt rt (cid:12) ht−1 to diag(rt) ⊗c ht−1 and the reset gate rt to:
0(W r ⊗c ht−1 ⊕c U r ⊗c xt ⊕c br),

0(h), logc
0(h)) (cid:12) logc

0(f (logc
0(σ(logc

rt = σ logc

c × Dp

(31)

and similarly for the update gate zt. Note that as the argument of σ in the above is unbounded, rt and
zt can a priori take values onto the full range (0, 1). Now the intermediate hidden state becomes:
˜ht = ϕ⊗c ((W diag(rt)) ⊗c ht−1 ⊕c U ⊗c xt ⊕ b),

(32)

where Möbius matrix associativity simpliﬁes W ⊗c (diag(rt) ⊗c ht−1) into (W diag(rt)) ⊗c ht−1.
Finally, we propose to adapt the update-gate equation as

ht = ht−1 ⊕c diag(zt) ⊗c (−ht−1 ⊕c

˜ht).

(33)

Note that when c goes to zero, one recovers the usual GRU. Moreover, if zt = 0 or zt = 1, then ht
becomes ht−1 or ˜ht respectively, similarly as in the usual GRU. This adaptation was obtained by
adapting [24]: in this work, the authors re-derive the update-gate mechanism from a ﬁrst principle
called time-warping invariance. We adapted their derivation to the hyperbolic setting by using the
notion of gyroderivative [4] and proving a gyro-chain-rule (see appendix E).

4 Experiments

SNLI task and dataset. We evaluate our method on two tasks. The ﬁrst is natural language
inference, or textual entailment. Given two sentences, a premise (e.g. "Little kids A. and B. are
playing soccer.") and a hypothesis (e.g. "Two children are playing outdoors."), the binary classiﬁcation
task is to predict whether the second sentence can be inferred from the ﬁrst one. This deﬁnes a partial
order in the sentence space. We test hyperbolic networks on the biggest real dataset for this task,
SNLI [7]. It consists of 570K training, 10K validation and 10K test sentence pairs. Following [28],
we merge the "contradiction" and "neutral" classes into a single class of negative sentence pairs, while
the "entailment" class gives the positive pairs.

PREFIX task and datasets. We conjecture that the improvements of hyperbolic neural networks
are more signiﬁcant when the underlying data structure is closer to a tree. To test this, we design a
proof-of-concept task of detection of noisy preﬁxes, i.e. given two sentences, one has to decide if the
second sentence is a noisy preﬁx of the ﬁrst, or a random sentence. We thus build synthetic datasets
PREFIX-Z% (for Z being 10, 30 or 50) as follows: for each random ﬁrst sentence of random length
at most 20 and one random preﬁx of it, a second positive sentence is generated by randomly replacing
Z% of the words of the preﬁx, and a second negative sentence of same length is randomly generated.
Word vocabulary size is 100, and we generate 500K training, 10K validation and 10K test pairs.

6If x has n coordinates, then diag(x) denotes the diagonal matrix of size n with xi’s on its diagonal.

8

Models architecture. Our neural network layers can be used in a plug-n-play manner exactly like
standard Euclidean layers. They can also be combined with Euclidean layers. However, optimization
w.r.t. hyperbolic parameters is different (see below) and based on Riemannian gradients which
are just rescaled Euclidean gradients when working in the conformal Poincaré model [21]. Thus,
back-propagation can be applied in the standard way.

In our setting, we embed the two sentences using two distinct hyperbolic RNNs or GRUs. The
sentence embeddings are then fed together with their squared distance (hyperbolic or Euclidean,
depending on their geometry) to a FFNN (Euclidean or hyperbolic, see Sec. 3.2) which is further
fed to an MLR (Euclidean or hyperbolic, see Sec. 3.1) that gives probabilities of the two classes
(entailment vs neutral). We use cross-entropy loss on top. Note that hyperbolic and Euclidean layers
can be mixed, e.g. the full network can be hyperbolic and only the last layer be Euclidean, in which
case one has to use log0 and exp0 functions to move between the two manifolds in a correct manner
as explained for Eq. 26.

Optimization. Our models have both Euclidean (e.g. weight matrices in both Euclidean and
hyperbolic FFNNs, RNNs or GRUs) and hyperbolic parameters (e.g. word embeddings or biases for
the hyperbolic layers). We optimize the Euclidean parameters with Adam [16] (learning rate 0.001).
Hyperbolic parameters cannot be updated with an equivalent method that keeps track of gradient
history due to the absence of a Riemannian Adam. Thus, they are optimized using full Riemannian
stochastic gradient descent (RSGD) [5, 11]. We also experiment with projected RSGD [21], but
optimization was sometimes less stable. We use a different constant learning rate for word embeddings
(0.1) and other hyperbolic weights (0.01) because words are updated less frequently.

Numerical errors. Gradients of the basic operations deﬁned above (e.g. ⊕c, exponential map) are
c(cid:107)x(cid:107) = 1. Thus, we
not deﬁned when the hyperbolic argument vectors are on the ball border, i.e.
always project results of these operations in the ball of radius 1 − (cid:15), where (cid:15) = 10−5. Numerical
errors also appear when hyperbolic vectors get closer to 0, thus we perturb them with an (cid:15)(cid:48) = 10−15
before they are used in any of the above operations. Finally, arguments of the tanh function are
clipped between ±15 to avoid numerical errors, while arguments of tanh−1 are clipped to at most
1 − 10−5.

√

Hyperparameters. For all methods, baselines and datasets, we use c = 1, word and hidden state
embedding dimension of 5 (we focus on the low dimensional setting that was shown to already
be effective [21]), batch size of 64. We ran all methods for a ﬁxed number of 30 epochs. For all
models, we experiment with both identity (no non-linearity) or tanh non-linearity in the RNN/GRU
cell, as well as identity or ReLU after the FFNN layer and before MLR. As expected, for the fully
Euclidean models, tanh and ReLU respectively surpassed the identity variant by a large margin. We
only report the best Euclidean results. Interestingly, for the hyperbolic models, using only identity for
both non-linearities works slightly better and this is likely due to two facts: i) our hyperbolic layers
already contain non-linearities by their nature, ii) tanh is limiting the output domain of the sentence
embeddings, but the hyperbolic speciﬁc geometry is more pronounced at the ball border, i.e. at the
hyperbolic "inﬁnity", compared to the center of the ball.

For the results shown in Tab. 1, we run each model (baseline or ours) exactly 3 times and report the
test result corresponding to the best validation result from these 3 runs. We do this because the highly
non-convex spectrum of hyperbolic neural networks sometimes results in convergence to poor local
minima, suggesting that initialization is very important.

Results. Results are shown in Tab. 1. Note that the fully Euclidean baseline models might have
an advantage over hyperbolic baselines because more sophisticated optimization algorithms such
as Adam do not have a hyperbolic analogue at the moment. We ﬁrst observe that all GRU models
overpass their RNN variants. Hyperbolic RNNs and GRUs have the most signiﬁcant improvement
over their Euclidean variants when the underlying data structure is more tree-like, e.g. for PREFIX-
10% − for which the tree relation between sentences and their preﬁxes is more prominent − we
reduce the error by a factor of 3.35 for hyperbolic vs Euclidean RNN, and by a factor of 1.5 for
hyperbolic vs Euclidean GRU. As soon as the underlying structure diverges more and more from
a tree, the accuracy gap decreases − for example, for PREFIX-50% the noise heavily affects the
representational power of hyperbolic networks. Also, note that on SNLI our methods perform
similarly as with their Euclidean variants. Moreover, hyperbolic and Euclidean MLR are on par when

9

SNLI

PREFIX-10% PREFIX-30% PREFIX-50%

FULLY EUCLIDEAN RNN
HYPERBOLIC RNN+FFNN, EUCL MLR
FULLY HYPERBOLIC RNN
FULLY EUCLIDEAN GRU
HYPERBOLIC GRU+FFNN, EUCL MLR
FULLY HYPERBOLIC GRU

79.34 %
79.18 %
78.21 %
81.52 %
79.76 %
81.19 %

89.62 %
96.36 %
96.91 %
95.96 %
97.36 %
97.14 %

81.71 %
87.83 %
87.25 %
86.47 %
88.47 %
88.26 %

72.10 %
76.50 %
62.94 %
75.04 %
76.87 %
76.44 %

Table 1: Test accuracies for various models and four datasets. "Eucl" denotes Euclidean. All word
and sentence embeddings have dimension 5. We highlight in bold the best baseline (or baselines, if
the difference is less than 0.5%).

used in conjunction with hyperbolic sentence embeddings, suggesting further empirical investigation
is needed for this direction (see below).

We also observe that, in the hyperbolic setting, accuracy tends to increase when sentence embeddings
start increasing, and gets better as their norms converge towards 1 (the ball border for c = 1). Unlike
in the Euclidean case, this behavior does happen only after a few epochs and suggests that the model
should ﬁrst adjust the angular layout in order to disentangle the representations, before increasing their
norms to fully exploit the strong clustering property of the hyperbolic geometry. Similar behavior
was observed in the context of embedding trees by [21]. Details in appendix F.

MLR classiﬁcation experiments.
For the sentence entailment classi-
ﬁcation task we do not see a clear
advantage of hyperbolic MLR com-
pared to its Euclidean variant. A pos-
sible reason is that, when trained end-
to-end, the model might decide to
place positive and negative embed-
dings in a manner that is already well
separated with a classic MLR. As a
consequence, we further investigate
MLR for the task of subtree classiﬁ-
cation. Using an open source imple-
mentation7 of [21], we pre-trained
Poincaré embeddings of the Word-
Net noun hierarchy (82,115 nodes).
We then choose one node in this tree
(see Table 2) and classify all other
nodes (solely based on their embed-
dings) as being part of the subtree
rooted at this node. All nodes in such a subtree are divided into positive training nodes (80%) and
positive test nodes (20%). The same splitting procedure is applied for the remaining WordNet nodes
that are divided into a negative training and negative test set respectively. Three variants of MLR
are then trained on top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation
task: hyperbolic MLR, Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean
MLR applied after mapping all embeddings in the tangent space at 0 using the log0 map. We use
different embedding dimensions : 2, 3, 5 and 10. For the hyperbolic MLR, we use full Riemannian
SGD with a learning rate of 0.001. For the two Euclidean models we use ADAM optimizer and the
same learning rate. During training, we always sample the same number of negative and positive
nodes in each minibatch of size 16; thus positive nodes are frequently resampled. All methods are
trained for 30 epochs and the ﬁnal F1 score is reported (no hyperparameters to validate are used, thus
we do not require a validation set). This procedure is repeated for four subtrees of different sizes.

Figure 2: Hyperbolic (left) vs Direct Euclidean (right) binary
MLR used to classify nodes as being part in the GROUP.N.01
subtree of the WordNet noun hierarchy solely based on their
Poincaré embeddings. The positive points (from the subtree)
are in blue, the negative points (the rest) are in red and the
trained positive separation hyperplane is depicted in green.

Quantitative results are presented in Table 2. We can see that the hyperbolic MLR overpasses
its Euclidean variants in almost all settings, sometimes by a large margin. Moreover, to provide

7https://github.com/dalab/hyperbolic_cones

10

WORDNET
SUBTREE

ANIMAL.N.01
3218 / 798

GROUP.N.01
6649 / 1727

WORKER.N.01
861 / 254

MAMMAL.N.01
953 / 228

MODEL

D = 2

D = 3

D = 5

D = 10

HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL

47.43 ± 1.07%
41.69 ± 0.19%
38.89 ± 0.01%
81.72 ± 0.17%
61.13 ± 0.42%
60.75 ± 0.24%
12.68 ± 0.82%
10.86 ± 0.01%
9.04 ± 0.06%
32.01 ± 17.14%
15.58 ± 0.04%
13.10 ± 0.13%

91.92 ± 0.61%
68.43 ± 3.90%
62.57 ± 0.61%
89.87 ± 2.73%
63.56 ± 1.22%
61.98 ± 0.57%
24.09 ± 1.49%
22.39 ± 0.04%
22.57 ± 0.20%
87.54 ± 4.55%
44.68 ± 1.87%
44.89 ± 1.18%

98.07 ± 0.55%
95.59 ± 1.18%
89.21 ± 1.34%
87.89 ± 0.80%
67.82 ± 0.81%
67.92 ± 0.74%
55.46 ± 5.49%
35.23 ± 3.16%
26.47 ± 0.78%
88.73 ± 3.22%
59.35 ± 1.31%
52.51 ± 0.85%

99.26 ± 0.59%
99.36 ± 0.18%
98.27 ± 0.70%
91.91 ± 3.07%
91.38 ± 1.19%
91.41 ± 0.18%
66.83 ± 11.38%
47.29 ± 3.93%
36.66 ± 2.74%
91.37 ± 6.09%
77.76 ± 5.08%
56.11 ± 2.21%

Table 2: Test F1 classiﬁcation scores for four different subtrees of WordNet noun tree. All nodes
in such a subtree are divided into positive training nodes (80%) and positive test nodes (20%);
these counts are shown below each subtree root. The same splitting procedure is applied for the
remaining nodes to obtain negative training and test sets. Three variants of MLR are then trained on
top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation task: hyperbolic MLR,
Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean MLR applied after
mapping all embeddings in the tangent space at 0 using the log0 map. 95% conﬁdence intervals for 3
different runs are shown for each method and each different embedding dimension (2, 3, 5 or 10).

further understanding, we plot the 2-dimensional embeddings and the trained separation hyperplanes
(geodesics in this case) in Figure 2. We can see that respecting the hyperbolic geometry is very
important for a quality classiﬁcation model.

5 Conclusion

We showed how classic Euclidean deep learning tools such as MLR, FFNNs, RNNs or GRUs can be
generalized in a principled manner to all spaces of constant negative curvature combining Riemannian
geometry with the elegant theory of gyrovector spaces. Empirically we found that our models
outperform or are on par with corresponding Euclidean architectures on sequential data with implicit
hierarchical structure. We hope to trigger exciting future research related to better understanding
of the hyperbolic non-convexity spectrum and development of other non-Euclidean deep learning
methods.
Our data and Tensorﬂow [1] code are publicly available8.

Acknowledgements

We thank Igor Petrovski for useful pointers regarding the implementation.

This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement
number 167176. Gary Bécigneul is also funded by the Max Planck ETH Center for Learning
Systems.

References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for
large-scale machine learning. 2016.

[2] Ungar Abraham Albert. Analytic hyperbolic geometry and Albert Einstein’s special theory of

relativity. World scientiﬁc, 2008.

8https://github.com/dalab/hyperbolic_nn

11

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of the International Conference on Learning
Representations (ICLR), 2015.

[4] Graciela S Birman and Abraham A Ungar. The hyperbolic derivative in the poincaré ball model
of hyperbolic geometry. Journal of mathematical analysis and applications, 254(1):321–333,
2001.

[5] S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on

Automatic Control, 58(9):2217–2229, Sept 2013.

[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems (NIPS), pages 2787–2795, 2013.

[7] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 632–642. Association
for Computational Linguistics, 2015.

[8] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,
34(4):18–42, 2017.

[9] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry.

Flavors of geometry, 31:59–115, 1997.

[10] Christopher De Sa, Albert Gu, Christopher Ré, and Frederic Sala. Representation tradeoffs for

hyperbolic embeddings. arXiv preprint arXiv:1804.03329, 2018.

[11] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In Proceedings of the thirty-ﬁfth international conference
on machine learning (ICML), 2018.

[12] Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pages 75–263. Springer, 1987.

[13] Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the

Cambridge Philosophical Society, page 1–17, 2017.

[14] Christopher Hopper and Ben Andrews. The Ricci ﬂow in Riemannian geometry. Springer, 2010.

[15] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1746–1751. Association for Computational Linguistics, 2014.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations (ICLR), 2015.

[17] Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marián Boguná.

Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.

[18] John Lamping, Ramana Rao, and Peter Pirolli. A focus+ context technique based on hyperbolic
geometry for visualizing large hierarchies. In Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 401–408. ACM Press/Addison-Wesley Publishing Co.,
1995.

[19] Guy Lebanon and John Lafferty. Hyperplane margin classiﬁers on the multinomial manifold. In
Proceedings of the international conference on machine learning (ICML), page 66. ACM, 2004.

[20] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective
learning on multi-relational data. In Proceedings of the international conference on machine
learning (ICML), volume 11, pages 809–816, 2011.

12

[21] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical repre-
sentations. In Advances in Neural Information Processing Systems (NIPS), pages 6341–6350,
2017.

[22] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blun-
som. Reasoning about entailment with neural attention. In Proceedings of the International
Conference on Learning Representations (ICLR), 2015.

[23] Michael Spivak. A comprehensive introduction to differential geometry. Publish or perish, 1979.

[24] Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In Proceedings of

the International Conference on Learning Representations (ICLR), 2018.

[25] Abraham A Ungar. Hyperbolic trigonometry and its application in the poincaré ball model of

hyperbolic geometry. Computers & Mathematics with Applications, 41(1-2):135–147, 2001.

[26] Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis

Lectures on Mathematics and Statistics, 1(1):1–194, 2008.

[27] Abraham Albert Ungar. Analytic hyperbolic geometry in n dimensions: An introduction. CRC

Press, 2014.

[28] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. In Proceedings of the International Conference on Learning Representations (ICLR),
2016.

[29] J Vermeer. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic

plane. Topology and its Applications, 152(3):226–242, 2005.

13

A Hyperbolic Trigonometry

Hyperbolic angles. For A, B, C ∈ Dn
c , we denote by ∠A := ∠BAC the angle between the two
geodesics starting from A and ending at B and C respectively. This angle can be deﬁned in two
equivalent ways: i) either using the angle between the initial velocities of the two geodesics as given
by Eq. 5, or ii) using the formula

cos(∠A) =

(cid:28) (−A) ⊕c B
(cid:107)(−A) ⊕c B(cid:107)

,

(−A) ⊕c C
(cid:107)(−A) ⊕c C(cid:107)

(cid:29)

,

In this case, ∠A is also called a gyroangle in the work of [26, section 4].

Hyperbolic law of sines. We state here the hyperbolic law of sines. If for A, B, C ∈ Dn
c , we
denote by ∠B := ∠ABC the angle between the two geodesics starting from B and ending at A and
C respectively, and by ˜c = dc(B, A) the length of the hyperbolic segment BA (and similarly for
others), then we have:

sin(∠A)
√
c˜a)
sinh(

=

sin(∠B)
√
c˜b)
sinh(

=

sin(∠C)
√
c˜c)
sinh(

.

Note that one can also adapt the hyperbolic law of cosines to the hyperbolic space.

B Proof of Theorem 4

Theorem 4.
In the manifold (Dn
to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a vector v ∈ T0Dn
c

c is given by the following isometry:
λc
0
λc
x

0→x(v) = logc
P c

x(x ⊕c expc

0(v)) =

v.

Proof. The geodesic in Dn
v ∈ T0Dn
γ (i.e. X(t) ∈ Tγ(t)Dn

c from 0 to x is given in Eq. (10) by γ(t) = x ⊗c t, for t ∈ [0, 1]. Let
c . Then it is of common knowledge that there exists a unique parallel9 vector ﬁeld X along

c , ∀t ∈ [0, 1]) such that X(0) = v. Let’s deﬁne:
X : t ∈ [0, 1] (cid:55)→ logc

γ(t)(γ(t) ⊕c expc

0(v)) ∈ Tγ(t)Dn
c .

Clearly, X is a vector ﬁeld along γ such that X(0) = v. Now deﬁne
0→x : v ∈ T0Dn
P c

x(x ⊕c expc

0(v)) ∈ TxDn
c .

c (cid:55)→ logc
0→x(v) = λc

c . Since P c

0→x is a linear isometry from T0Dn
v, hence P c
c
0→x(v) = X(1), it is enough to prove that X is parallel in order to guarantee that

From Eq. (12), it is easily seen that P c
to TxDn
c to TxDn
0→x is the parallel transport from T0Dn
P c
c .
Since X is a vector ﬁeld along γ, its covariant derivative can be expressed with the Levi-Civita
connection ∇c associated to gc:

0
λc
x

DX
∂t

= ∇c

˙γ(t)X.

Let’s compute the Levi-Civita connection from its Christoffel symbols. In a local coordinate system,
they can be written as

Γi

jk =

(gc)il(∂jgc

lk + ∂kgc

lj − ∂lgc

jk),

1
2

where superscripts denote the inverse metric tensor and using Einstein’s notations. As gc
at γ(t) ∈ Dn

c this yields:

ij = (λc)2δij,

jk = cλc
Γi

γ(t)(δikγ(t)j + δijγ(t)k − δjkγ(t)i).

9i.e. that DX

∂t = 0 for t ∈ [0, 1], where D

∂t denotes the covariant derivative.

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

14

On the other hand, since X(t) = (λc

∇c

˙γ(t)X = ˙γ(t)i∇c

i X = ˙γ(t)i∇c
i

= vj ˙γ(t)i∇c
i

0/λc

γ(t))v, we have
(cid:32)

(cid:33)

λc
0
λc

γ(t)

v

(cid:32)

λc
0
λc

γ(t)

(cid:33)

ej

.

√

√

Since γ(t) = (1/
Hence there exists K x

c) tanh(t tanh−1(
t ∈ R such that ˙γ(t) = K x

c(cid:107)x(cid:107))) x

(cid:107)x(cid:107) , it is easily seen that ˙γ(t) is colinear to γ(t).
t γ(t). Moreover, we have the following Leibniz rule:
(cid:33)

(cid:32)

(cid:32)

∇c
i

λc
0
λc

γ(t)

(cid:33)

ej

=

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

ej.

Combining these yields

DX
∂t

= K x

t vjγ(t)i

(cid:32)

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

(cid:32)

(cid:33)

(cid:33)

ej

.

Replacing with the Christoffel symbols of ∇c at γ(t) gives

Moreover,

λc
0
λc

γ(t)

λc
0
λc

γ(t)

∇c

i ej =

ijek = 2c[δk
Γk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek.

∂
∂γ(t)i

(cid:32)

(cid:33)

λc
0
λc

γ(t)

ej =

∂
∂γ(t)i

(cid:0)−c(cid:107)γ(t)(cid:107)2(cid:1) ej = −2cγ(t)iej.

Putting together everything, we obtain

DX
∂t

= K x

t vjγ(t)i (cid:0)2c[δk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek − 2cγ(t)iej

(cid:1)

t vjγ(t)i (cid:0)γ(t)jei − δijγ(t)kek
t vj (cid:0)γ(t)jγ(t)iei − γ(t)iδijγ(t)kek
(cid:1)
t vj (cid:0)γ(t)jγ(t)iei − γ(t)jγ(t)kek

(cid:1)

(cid:1)

= 2cK x
= 2cK x
= 2cK x
= 0,

which concludes the proof.

C Proof of Eq. (22)

Proof. Two steps proof:
i) expc

p({a}⊥) ⊆ {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}:

Let z ∈ {a}⊥. From Eq. (12), we have that:

This, together with the left-cancellation law in gyrospaces (see section 2.3), implies that

expc

p(z) = −p ⊕c βz,

for some β ∈ R.

(cid:104)−p ⊕c expc

p(z), a(cid:105) = (cid:104)βz, a(cid:105) = 0

which is what we wanted.

ii) {x ∈ Dn
Let x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0} ⊆ expc
c s.t. (cid:104)−p ⊕c x, a(cid:105) = 0. Then, using Eq. (12), we derive that:
for some β ∈ R,

p(x) = β(−p ⊕c x),

p({a}⊥):

logc

which is orthogonal to a, by assumption. This implies logc

p(x) ∈ {a}⊥, hence x ∈ expc

p({a}⊥).

15

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(54)

D Proof of Theorem 5

Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

1
√
c

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(55)

Proof. We ﬁrst need to prove the following lemma, trivial in the Euclidean space, but not in the
Poincaré ball:
Lemma 7. (Orthogonal projection on a geodesic) Any point in the Poincaré ball has a unique
orthogonal projection on any given geodesic that does not pass through the point. Formally, for all
y ∈ Dn
c and for all geodesics γx→z(·) s.t. y /∈ Im γx→z, there exists an unique w ∈ Im γx→z s.t.
∠(γw→y, γx→z) = π/2.

Proof. We ﬁrst note that any geodesic in Dn
and has two "points at inﬁnity" lying on the ball border (v (cid:54)= 0):

c has the form γ(t) = u ⊕c v ⊗c t as given by Eq. 11,

γ(±∞) = u ⊕c

√

±v
c(cid:107)v(cid:107)

∈ ∂Dn
c .

(56)

Using the notations in the lemma statement, the closed-form of γx→z is given by Eq. (10):

γx→z(t) = x ⊕c (−x ⊕c z) ⊗c t

We denote by x(cid:48), z(cid:48) ∈ ∂Dn
∠ywx(cid:48) is well deﬁned from Eq. (34):

c its points at inﬁnity as described by Eq. (56). Then, the hyperbolic angle

cos(∠(γw→y, γx→z)) = cos(∠ywz(cid:48)) =

(cid:104)−w ⊕c y, −w ⊕c z(cid:48)(cid:105)
(cid:107) − w ⊕c y(cid:107) · (cid:107) − w ⊕c z(cid:48)(cid:107)

.

(57)

We now perform 2 steps for this proof.

i) Existence of w:

The angle function from Eq. (57) is continuous w.r.t t when w = γx→z(t). So we ﬁrst prove existence
of an angle of π/2 by continuously moving w from x(cid:48) to z(cid:48) when t goes from −∞ to ∞, and
observing that cos(∠ywz(cid:48)) goes from −1 to 1 as follows:

cos(∠yx(cid:48)z(cid:48)) = 1 & lim
w→z(cid:48)

cos(∠ywz(cid:48)) = −1.

(58)

The left part of Eq. (58) follows from Eq. (57) and from the fact (easy to show from the deﬁnition
c (which is the case of x(cid:48)). The right part of Eq. (58)
of ⊕c) that a ⊕c b = a, when (cid:107)a(cid:107) = 1/
follows from the fact that ∠ywz(cid:48) = π − ∠ywx(cid:48) (from the conformal property, or from Eq. (34)) and
cos(∠yz(cid:48)x(cid:48)) = 1 (proved as above).
Hence cos(∠ywz(cid:48)) has to pass through 0 when going from −1 to 1, which achieves the proof of
existence.

√

ii) Uniqueness of w:
Assume by contradiction that there are two w and w(cid:48) on γx→z that form angles ∠ywx(cid:48) and ∠yw(cid:48)x(cid:48)
of π/2. Since w, w(cid:48), x(cid:48) are on the same geodesic, we have

π/2 = ∠yw(cid:48)x(cid:48) = ∠yw(cid:48)w = ∠ywx(cid:48) = ∠yw(cid:48)w
So ∆yww(cid:48) has two right angles, but in the Poincaré ball this is impossible.

(59)

Now, we need two more lemmas:
Lemma 8. (Minimizing distance from point to geodesic) The orthogonal projection of a point to
a geodesic (not passing through the point) is minimizing the distance between the point and the
geodesic.

Proof. The proof is similar with the Euclidean case and it’s based on hyperbolic sine law and the fact
that in any right hyperbolic triangle the hypotenuse is strictly longer than any of the other sides.

16

Lemma 9. (Geodesics through p) Let ˜H c
all points on the geodesic γp→w are included in ˜H c

a,p.

a,p be a Poincaré hyperplane. Then, for any w ∈ ˜H c

a,p \ {p},

Proof. γp→w(t) = p ⊕c (−p ⊕c w) ⊗c t. Then, it is easy to check the condition in Eq. (22):

(cid:104)−p ⊕c γp→w(t), a(cid:105) = (cid:104)(−p ⊕c w) ⊗c t, a(cid:105) ∝ (cid:104)−p ⊕c w, a(cid:105) = 0.

(60)

We now turn back to our proof. Let x ∈ Dn
We prove that there is at least one point w∗ ∈ ˜H c

c be an arbitrary point and ˜H c

a,p a Poincaré hyperplane.

a,p that achieves the inﬁmum distance

dc(x, w∗) = inf
w∈ ˜H c

a,p

dc(x, w),

and, moreover, that this distance is the same as the one in the theorem’s statement.
We ﬁrst note that for any point w ∈ ˜H c
and Lemma 9, it is obvious that the projection of x to γp→w will give a strictly lower distance.
Thus, we only consider w ∈ ˜H c
triangle ∆xwp, one gets:

a,p such that ∠xwp = π/2. Applying hyperbolic sine law in the right

a,p, if ∠xwp (cid:54)= π/2, then w (cid:54)= w∗. Indeed, using Lemma 8

dc(x, w) = (1/

c) sinh−1 (cid:0)sinh(

c dc(x, p)) · sin(∠xpw)(cid:1) .

√

√

One of the above quantities does not depend on w:

√

√

sinh(

c dc(x, p)) = sinh(2 tanh−1(

c(cid:107) − p ⊕c x(cid:107))) =

√
2
c(cid:107) − p ⊕c x(cid:107)
1 − c(cid:107) − p ⊕c x(cid:107)2 .

The other quantity is sin(∠xpw) which is minimized when the angle ∠xpw is minimized (be-
cause ∠xpw < π/2 for the hyperbolic right triangle ∆xwp), or, alternatively, when cos(∠xpw) is
maximized. But, we already have from Eq. (34) that:

cos(∠xpw) =

(cid:104)−p ⊕c x, −p ⊕c w(cid:105)
(cid:107) − p ⊕c x(cid:107) · (cid:107) − p ⊕c w(cid:107)

.

To maximize the above, the constraint on the right angle at w can be dropped because cos(∠xpw)
depends only on the geodesic γp→w and not on w itself, and because there is always an orthogonal
projection from any point x to any geodesic as stated by Lemma 7. Thus, it remains to ﬁnd the
maximum of Eq. (64) when w ∈ ˜H c
a,p from Eq. (22), one can easily
prove that

a,p. Using the deﬁnition of ˜H c

Using that fact that logc

p(w)/(cid:107) logc

p(w)(cid:107) = −p ⊕c w/(cid:107) − p ⊕c w(cid:107), we just have to ﬁnd

and we are left with a well known Euclidean problem which is equivalent to ﬁnding the minimum
angle between the vector −p ⊕c x (viewed as Euclidean) and the hyperplane {a}⊥. This angle
is given by the Euclidean orthogonal projection whose sin value is the distance from the vector’s
endpoint to the hyperplane divided by the vector’s length:

{logc

p(w) : w ∈ ˜H c

a,p} = {a}⊥.

max
z∈{a}⊥

(cid:18) (cid:104)−p ⊕c x, z(cid:105)

(cid:107) − p ⊕c x(cid:107) · (cid:107)z(cid:107)

(cid:19)

,

sin(∠xpw∗) =

|(cid:104)−p ⊕c x, a
(cid:107) − p ⊕c x(cid:107)

(cid:107)a(cid:107) (cid:105)|

.

17

It follows that a point w∗ ∈ ˜H c
Eqs. (61),(62),(63) and (67) concludes the proof.

a,p satisfying Eq. (67) exists (but might not be unique). Combining

(61)

(62)

(63)

(64)

(65)

(66)

(67)

(cid:3)

E Derivation of the Hyperbolic GRU Update-gate

In [24], the authors recover the update/forget-gate mechanism of a GRU/LSTM by requiring that the
class of neural networks given by the chosen architecture be invariant to time-warpings. The idea is
the following.

Recovering the update-gate from time-warping. A naive RNN is given by the equation

h(t + 1) = ϕ(W h(t) + U x(t) + b)

Let’s drop the bias b to simplify notations. If h is seen as a differentiable function of time, then a
ﬁrst-order Taylor development gives h(t + δt) ≈ h(t) + δt dh
dt (t) for small δt. Combining this for
δt = 1 with the naive RNN equation, one gets

dh
dt

dα
dt

(t) = ϕ(W h(t) + U x(t)) − h(t).

As this is written for any t, one can replace it by t ← α(t) where α is a (smooth) increasing function
of t called the time-warping. Denoting by ˜h(t) := h(α(t)) and ˜x(t) := x(α(t)), using the chain rule
d˜h
dt (t) = dα

dt (α(t)), one gets

dt (t) dh

d˜h
dt

dα
dt

(t) =

(t)ϕ(W ˜h(t) + U ˜x(t)) −

(t)˜h(t).

(70)

Removing the tildas to simplify notations, discretizing back with dh

dt (t) ≈ h(t + 1) − h(t) yields

h(t + 1) =

(t)ϕ(W h(t) + U x(t)) +

1 −

(t)

h(t).

(71)

dα
dt

(cid:18)

(cid:19)

dα
dt

Requiring that our class of neural networks be invariant to time-warpings means that this class should
contain RNNs deﬁned by Eq. (71), i.e. that dα
dt (t) can be learned. As this is a positive quantity, we
can parametrize it as z(t) = σ(W zh(t) + U zx(t)), recovering the forget-gate equation:

h(t + 1) = z(t)ϕ(W h(t) + U x(t)) + (1 − z(t))h(t).

Adapting this idea to hyperbolic RNNs. The gyroderivative [4] of a map h : R → Dn
as

c is deﬁned

dh
dt

(t) = lim
δt→0

1
δt

⊗c (−h(t) ⊕c h(t + δt)).

Using Möbius scalar associativity and the left-cancellation law leads us to

h(t + δt) ≈ h(t) ⊕c δt ⊗c

(t),

dh
dt

for small δt. Combining this with the equation of a simple hyperbolic RNN of Eq. (29) with δt = 1,
one gets

dh
dt

(t) = −h(t) ⊕c ϕ⊗c(W ⊗c h(t) ⊕c U ⊗c x(t)).

For the next step, we need the following lemma:
Lemma 10 (Gyro-chain-rule). For α : R → R differentiable and h : R → Dn
gyro-derivative, if ˜h := h ◦ α, then we have

c with a well-deﬁned

(68)

(69)

(72)

(73)

(74)

(75)

(76)

where dα

dt (t) denotes the usual derivative.

d˜h
dt

(t) =

(t) ⊗c

(α(t)),

dα
dt

dh
dt

18

(77)

(78)

(79)

(80)

(81)

Proof.

d˜h
dt

(t) = lim
δt→0

1
δt
1
δt

⊗c [−˜h(t) ⊕c

˜h(t + δt)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

= lim
δt→0

= lim
δt→0

= lim
δt→0

= lim
u→0
dα
dt

=

α(cid:48)(t) + O(δt)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
u

(t) ⊗c

(α(t))

dh
dt

⊗c [−h(α(t)) ⊕c h(α(t) + u)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

(Möbius scalar associativity) (82)

where we set u = δt(α(cid:48)(t) + O(δt)), with u → 0 when δt → 0, which concludes.

Using lemma 10 and Eq. (75), with similar notations as in Eq. (70) we have

d˜h
dt

dα
dt

(t) =

(t) ⊗c (−˜h(t) ⊕c ϕ⊗c(W ⊗c

˜h(t) ⊕c U ⊗c ˜x(t))).

(83)

Finally, discretizing back with Eq. (74), using the left-cancellation law and dropping the tildas yields

h(t + 1) = h(t) ⊕c

(t) ⊗c (−h(t) ⊕c ϕ⊗c (W ⊗c h(t) ⊕c U ⊗c x(t))).

(84)

dα
dt

Since α is a time-warping, by deﬁnition its derivative is positive and one can choose to parametrize
it with an update-gate zt (a scalar) deﬁned with a sigmoid. Generalizing this scalar scaling by the
Möbius version of the pointwise scaling (cid:12) yields the Möbius matrix scaling diag(zt) ⊗c ·, leading to
our proposed Eq. (33) for the hyperbolic GRU.

F More Experimental Investigations

The following empirical facts were observed for both hyperbolic RNNs and GRUs.

We observed that, in the hyperbolic setting, accuracy is often much higher when sentence embeddings
can go close to the border (hyperbolic "inﬁnity"), hence exploiting the hyperbolic nature of the space.
Moreover, the faster the two sentence norms go to 1, the more it’s likely that a good local minima
was reached. See ﬁgures 3 and 5.

We often observe that test accuracy starts increasing exactly when sentence embedding norms do.
However, in the hyperbolic setting, the sentence embeddings norms remain close to 0 for a few
epochs, which does not happen in the Euclidean case. See ﬁgures 3, 5 and 4. This mysterious fact
was also exhibited in a similar way by [21] which suggests that the model ﬁrst has to adjust the
angular layout in the almost Euclidean vicinity of 0 before increasing norms and fully exploiting
hyperbolic geometry.

19

(a) Test accuracy

(a) Test accuracy

20

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 3: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the same
architecture: hyperbolic GRU followed by hyperbolic FFNN and hyperbolic/Euclidean (half-half)
MLR. The X axis shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 4: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: Euclidean GRU followed by Euclidean FFNN and Euclidean MLR. The X axis
shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 5: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: hyperbolic RNN followed by hyperbolic FFNN and hyperbolic MLR. The X axis
shows millions of training examples processed.

(a) Test accuracy

21

8
1
0
2
 
n
u
J
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
2
1
1
9
0
.
5
0
8
1
:
v
i
X
r
a

Hyperbolic Neural Networks

Octavian-Eugen Ganea∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
octavian.ganea@inf.ethz.ch

Gary Bécigneul∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
gary.becigneul@inf.ethz.ch

Thomas Hofmann
Department of Computer Science
ETH Zürich
Zurich, Switzerland
thomas.hofmann@inf.ethz.ch

Abstract

Hyperbolic spaces have recently gained momentum in the context of machine
learning due to their high capacity and tree-likeliness properties. However, the
representational power of hyperbolic geometry is not yet on par with Euclidean
geometry, mostly because of the absence of corresponding hyperbolic neural
network layers. This makes it hard to use hyperbolic embeddings in downstream
tasks. Here, we bridge this gap in a principled manner by combining the formalism
of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model
of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep
learning tools: multinomial logistic regression, feed-forward and recurrent neural
networks such as gated recurrent units. This allows to embed sequential data and
perform classiﬁcation in the hyperbolic space. Empirically, we show that, even if
hyperbolic optimization tools are limited, hyperbolic sentence embeddings either
outperform or are on par with their Euclidean variants on textual entailment and
noisy-preﬁx recognition tasks.

1

Introduction

It is common in machine learning to represent data as being embedded in the Euclidean space Rn. The
main reason for such a choice is simply convenience, as this space has a vectorial structure, closed-
form formulas of distance and inner-product, and is the natural generalization of our intuition-friendly,
visual three-dimensional space. Moreover, embedding entities in such a continuous space allows to
feed them as input to neural networks, which has led to unprecedented performance on a broad range
of problems, including sentiment detection [15], machine translation [3], textual entailment [22] or
knowledge base link prediction [20, 6].

Despite the success of Euclidean embeddings, recent research has proven that many types of com-
plex data (e.g. graph data) from a multitude of ﬁelds (e.g. Biology, Network Science, Computer
Graphics or Computer Vision) exhibit a highly non-Euclidean latent anatomy [8]. In such cases, the
Euclidean space does not provide the most powerful or meaningful geometrical representations. For
example, [10] shows that arbitrary tree structures cannot be embedded with arbitrary low distortion
(i.e. almost preserving their metric) in the Euclidean space with unbounded number of dimensions,
but this task becomes surprisingly easy in the hyperbolic space with only 2 dimensions where the
exponential growth of distances matches the exponential growth of nodes with the tree depth.

∗Equal contribution.

The adoption of neural networks and deep learning in these non-Euclidean settings has been rather
limited until very recently, the main reason being the non-trivial or impossible principled general-
izations of basic operations (e.g. vector addition, matrix-vector multiplication, vector translation,
vector inner product) as well as, in more complex geometries, the lack of closed form expressions for
basic objects (e.g. distances, geodesics, parallel transport). Thus, classic tools such as multinomial
logistic regression (MLR), feed forward (FFNN) or recurrent neural networks (RNN) did not have a
correspondence in these geometries.

How should one generalize deep neural models to non-Euclidean domains ? In this paper we address
this question for one of the simplest, yet useful, non-Euclidean domains: spaces of constant negative
curvature, i.e. hyperbolic. Their tree-likeness properties have been extensively studied [12, 13, 26]
and used to visualize large taxonomies [18] or to embed heterogeneous complex networks [17]. In
machine learning, recently, hyperbolic representations greatly outperformed Euclidean embeddings
for hierarchical, taxonomic or entailment data [21, 10, 11]. Disjoint subtrees from the latent hierar-
chical structure surprisingly disentangle and cluster in the embedding space as a simple reﬂection of
the space’s negative curvature. However, appropriate deep learning tools are needed to embed feature
data in this space and use it in downstream tasks. For example, implicitly hierarchical sequence data
(e.g. textual entailment data, phylogenetic trees of DNA sequences or hierarchial captions of images)
would beneﬁt from suitable hyperbolic RNNs.

The main contribution of this paper is to bridge the gap between hyperbolic and Euclidean geometry
in the context of neural networks and deep learning by generalizing in a principled manner both the
basic operations as well as multinomial logistic regression (MLR), feed-forward (FFNN), simple and
gated (GRU) recurrent neural networks (RNN) to the Poincaré model of the hyperbolic geometry.
We do it by connecting the theory of gyrovector spaces and generalized Möbius transformations
introduced by [2, 26] with the Riemannian geometry properties of the manifold. We smoothly
parametrize basic operations and objects in all spaces of constant negative curvature using a uniﬁed
framework that depends only on the curvature value. Thus, we show how Euclidean and hyperbolic
spaces can be continuously deformed into each other. On a series of experiments and datasets we
showcase the effectiveness of our hyperbolic neural network layers compared to their "classic"
Euclidean variants on textual entailment and noisy-preﬁx recognition tasks. We hope that this paper
will open exciting future directions in the nascent ﬁeld of Geometric Deep Learning.

2 The Geometry of the Poincaré Ball

2.1 Basics of Riemannian geometry

We brieﬂy introduce basic concepts of differential geometry largely needed for a principled general-
ization of Euclidean neural networks. For more rigorous and in-depth expositions, see [23, 14].
An n-dimensional manifold M is a space that can locally be approximated by Rn: it is a generalization
to higher dimensions of the notion of a 2D surface. For x ∈ M, one can deﬁne the tangent space
TxM of M at x as the ﬁrst order linear approximation of M around x. A Riemannian metric
g = (gx)x∈M on M is a collection of inner-products gx : TxM × TxM → R varying smoothly
with x. A Riemannian manifold (M, g) is a manifold M equipped with a Riemannian metric g.
Although a choice of a Riemannian metric g on M only seems to deﬁne the geometry locally on M,
it induces global distances by integrating the length (of the speed vector living in the tangent space)
of a shortest path between two points:

(cid:90) 1

(cid:113)

d(x, y) = inf
γ

0

gγ(t)( ˙γ(t), ˙γ(t))dt,

(1)

where γ ∈ C∞([0, 1], M) is such that γ(0) = x and γ(1) = y. A smooth path γ of minimal length
between two points x and y is called a geodesic, and can be seen as the generalization of a straight-line
in Euclidean space. The parallel transport Px→y : TxM → TyM is a linear isometry between
tangent spaces which corresponds to moving tangent vectors along geodesics and deﬁnes a canonical
way to connect tangent spaces. The exponential map expx at x, when well-deﬁned, gives a way to
project back a vector v of the tangent space TxM at x, to a point expx(v) ∈ M on the manifold.
This map is often used to parametrize a geodesic γ starting from γ(0) := x ∈ M with unit-norm
direction ˙γ(0) := v ∈ TxM as t (cid:55)→ expx(tv). For geodesically complete manifolds, such as the
Poincaré ball considered in this work, expx is well-deﬁned on the full tangent space TxM. Finally, a

2

(2)

(3)

(4)

(5)

metric ˜g is said to be conformal to another metric g if it deﬁnes the same angles, i.e.

˜gx(u, v)
(cid:112)˜gx(u, u)(cid:112)˜gx(v, v)

=

gx(u, v)
(cid:112)gx(u, u)(cid:112)gx(v, v)

,

for all x ∈ M, u, v ∈ TxM \ {0}. This is equivalent to the existence of a smooth function
λ : M → R, called the conformal factor, such that ˜gx = λ2

xgx for all x ∈ M.

2.2 Hyperbolic space: the Poincaré ball

The hyperbolic space has ﬁve isometric models that one can work with [9]. Similarly as in [21] and
[11], we choose to work in the Poincaré ball. The Poincaré ball model (Dn, gD) is deﬁned by the
manifold Dn = {x ∈ Rn : (cid:107)x(cid:107) < 1} equipped with the following Riemannian metric:

gD
x = λ2

xgE, where λx :=

2
1 − (cid:107)x(cid:107)2 ,

gE = In being the Euclidean metric tensor. Note that the hyperbolic metric tensor is conformal to
the Euclidean one. The induced distance between two points x, y ∈ Dn is known to be given by

dD(x, y) = cosh−1

1 + 2

(cid:18)

(cid:107)x − y(cid:107)2
(1 − (cid:107)x(cid:107)2)(1 − (cid:107)y(cid:107)2)

(cid:19)

.

Since the Poincaré ball is conformal to Euclidean space, the angle between two vectors u, v ∈
TxDn \ {0} is given by

cos(∠(u, v)) =

gD
x (u, v)
x (u, u)(cid:112)gD

(cid:112)gD

x (v, v)

=

(cid:104)u, v(cid:105)
(cid:107)u(cid:107)(cid:107)v(cid:107)

.

2.3 Gyrovector spaces

In Euclidean space, natural operations inherited from the vectorial structure, such as vector addition,
subtraction and scalar multiplication are often useful. The framework of gyrovector spaces provides
an elegant non-associative algebraic formalism for hyperbolic geometry just as vector spaces provide
the algebraic setting for Euclidean geometry [2, 25, 26].

In particular, these operations are used in special relativity, allowing to add speed vectors belonging
to the Poincaré ball of radius c (the celerity, i.e. the speed of light) so that they remain in the ball,
hence not exceeding the speed of light.

We will make extensive use of these operations in our deﬁnitions of hyperbolic neural networks.
For c ≥ 0, denote2 by Dn
then Dn

c := {x ∈ Rn | c(cid:107)x(cid:107)2 < 1}. Note that if c = 0, then Dn
c. If c = 1 then we recover the usual ball Dn.

c is the open ball of radius 1/

c = Rn; if c > 0,

√

Möbius addition. The Möbius addition of x and y in Dn

c is deﬁned as

x ⊕c y :=

(1 + 2c(cid:104)x, y(cid:105) + c(cid:107)y(cid:107)2)x + (1 − c(cid:107)x(cid:107)2)y
1 + 2c(cid:104)x, y(cid:105) + c2(cid:107)x(cid:107)2(cid:107)y(cid:107)2

.

(6)

In particular, when c = 0, one recovers the Euclidean addition of two vectors in Rn. Note that
without loss of generality, the case c > 0 can be reduced to c = 1. Unless stated otherwise, we
will use ⊕ as ⊕1 to simplify notations. For general c > 0, this operation is not commutative nor
associative. However, it satisﬁes x ⊕c 0 = 0 ⊕c x = 0. Moreover, for any x, y ∈ Dn
c , we have
(−x) ⊕c x = x ⊕c (−x) = 0 and (−x) ⊕c (x ⊕c y) = y (left-cancellation law). The Möbius
substraction is then deﬁned by the use of the following notation: x (cid:9)c y := x ⊕c (−y). See [29,
section 2.1] for a geometric interpretation of the Möbius addition.

2We take different notations as in [25] where the author uses s = 1/

c.

√

3

Möbius scalar multiplication. For c > 0, the Möbius scalar multiplication of x ∈ Dn
r ∈ R is deﬁned as

c \ {0} by

r ⊗c x := (1/

c) tanh(r tanh−1(

c(cid:107)x(cid:107)))

√

√

x
(cid:107)x(cid:107)

,

(7)

and r ⊗c 0 := 0. Note that similarly as for the Möbius addition, one recovers the Euclidean scalar
multiplication when c goes to zero: limc→0 r ⊗c x = rx. This operation satisﬁes desirable properties
such as n ⊗c x = x ⊕c · · · ⊕c x (n additions), (r + r(cid:48)) ⊗c x = r ⊗c x ⊕c r(cid:48) ⊗c x (scalar distributivity3),
(rr(cid:48)) ⊗c x = r ⊗c (r(cid:48) ⊗c x) (scalar associativity) and |r| ⊗c x/(cid:107)r ⊗c x(cid:107) = x/(cid:107)x(cid:107) (scaling property).

c , gc) is given by4

Distance.
Euclidean one, with conformal factor λc
(Dn

If one deﬁnes the generalized hyperbolic metric tensor gc as the metric conformal to the
x := 2/(1 − c(cid:107)x(cid:107)2), then the induced distance function on
√

c) tanh−1 (cid:0)√
Again, observe that limc→0 dc(x, y) = 2(cid:107)x − y(cid:107), i.e. we recover Euclidean geometry in the limit5.
Moreover, for c = 1 we recover dD of Eq. (4).

c(cid:107) − x ⊕c y(cid:107)(cid:1) .

dc(x, y) = (2/

(8)

Hyperbolic trigonometry. Similarly as in the Euclidean space, one can deﬁne the notions of
hyperbolic angles or gyroangles (when using the ⊕c), as well as hyperbolic law of sines in the
generalized Poincaré ball (Dn

c , gc). We make use of these notions in our proofs. See Appendix A.

2.4 Connecting Gyrovector spaces and Riemannian geometry of the Poincaré ball

In this subsection, we present how geodesics in the Poincaré ball model are usually described with
Möbius operations, and push one step further the existing connection between gyrovector spaces and
the Poincaré ball by ﬁnding new identities involving the exponential map, and parallel transport.

In particular, these ﬁndings provide us with a simpler formulation of Möbius scalar multiplication,
yielding a natural deﬁnition of matrix-vector multiplication in the Poincaré ball.

Riemannian gyroline element. The Riemannian gyroline element is deﬁned for an inﬁnitesimal
dx as ds := (x + dx) (cid:9)c x, and its size is given by [26, section 3.7]:

(cid:107)ds(cid:107) = (cid:107)(x + dx) (cid:9)c x(cid:107) = (cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2).

(9)

What is remarkable is that it turns out to be identical, up to a scaling factor of 2, to the usual line
element 2(cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2) of the Riemannian manifold (Dn

c , gc).

Geodesics. The geodesic connecting points x, y ∈ Dn

c is shown in [2, 26] to be given by:

γx→y(t) := x ⊕c (−x ⊕c y) ⊗c t, with γx→y : R → Dn

c s.t. γx→y(0) = x and γx→y(1) = y.

Note that when c goes to 0, geodesics become straight-lines, recovering Euclidean geometry. In the
remainder of this subsection, we connect the gyrospace framework with Riemannian geometry.
Lemma 1. For any x ∈ Dn and v ∈ TxDn
c s.t. gc
x with direction v is given by:

x(v, v) = 1, the unit-speed geodesic starting from

γx,v(t) = x ⊕c

tanh

(cid:18)

(cid:18)√

(cid:19) v
√

c

t
2

c(cid:107)v(cid:107)

(cid:19)

, where γx,v : R → Dn s.t. γx,v(0) = x and ˙γx,v(0) = v.

(10)

(11)

Proof. One can use Eq. (10) and reparametrize it to unit-speed using Eq. (8). Alternatively, direct
computation and identiﬁcation with the formula in [11, Thm. 1] would give the same result. Using
Eq. (8) and Eq. (11), one can sanity-check that dc(γ(0), γ(t)) = t, ∀t ∈ [0, 1].

3⊗c has priority over ⊕c in the sense that a ⊗c b ⊕c c := (a ⊗c b) ⊕c c and a ⊕c b ⊗c c := a ⊕c (b ⊗c c).
4The notation −x ⊕c y should always be read as (−x) ⊕c y and not −(x ⊕c y).
5The factor 2 comes from the conformal factor λx = 2/(1 − (cid:107)x(cid:107)2), which is a convention setting the

curvature to −1.

4

Exponential and logarithmic maps. The following lemma gives the closed-form derivation of
exponential and logarithmic maps.
Lemma 2. For any point x ∈ Dn
map logc
c → TxDn
(cid:18)

c are given for v (cid:54)= 0 and y (cid:54)= x by:
(cid:18)√

c , the exponential map expc

c and the logarithmic

x : TxDn

c → Dn

x : Dn

√

(cid:19)

, logc

x(y) =

√

tanh−1(

c(cid:107) − x ⊕c y(cid:107))

expc

x(v) = x ⊕c

tanh

c

λc
x(cid:107)v(cid:107)
2

(cid:19) v
√

c(cid:107)v(cid:107)

2
cλc
x

−x ⊕c y
(cid:107) − x ⊕c y(cid:107)
(12)

.

Proof. Following the proof of [11, Cor. 1.1], one gets expc
gives the formula for expc

x. Algebraic check of the identity logc

x(v) = γx,
x(expc

v

x(cid:107)v(cid:107) (λc
λc

x(cid:107)v(cid:107)). Using Eq. (11)

x(v)) = v concludes.

The above maps have more appealing forms when x = 0, namely for v ∈ T0Dn

c \ {0}, y ∈ Dn

c \ {0}:

expc

0(v) = tanh(

c(cid:107)v(cid:107))

√

, logc

0(y) = tanh−1(

c(cid:107)y(cid:107))

√

(13)

√

y
c(cid:107)y(cid:107)

.

√

v
c(cid:107)v(cid:107)

Moreover, we still recover Euclidean geometry in the limit c → 0, as limc→0 expc
Euclidean exponential map, and limc→0 logc

x(y) = y − x is the Euclidean logarithmic map.

x(v) = x + v is the

Möbius scalar multiplication using exponential and logarithmic maps. We studied the expo-
nential and logarithmic maps in order to gain a better understanding of the Möbius scalar multiplica-
tion (Eq. (7)). We found the following:
Lemma 3. The quantity r ⊗ x can actually be obtained by projecting x in the tangent space at 0
with the logarithmic map, multiplying this projection by the scalar r in T0Dn
c , and then projecting it
back on the manifold with the exponential map:
0(r logc

∀r ∈ R, x ∈ Dn
c .

r ⊗c x = expc

0(x)),

(14)

In addition, we recover the well-known relation between geodesics connecting two points and the
exponential map:

γx→y(t) = x ⊕c (−x ⊕c y) ⊗c t = expc

x(t logc

x(y)),

t ∈ [0, 1].

(15)

This last result enables us to generalize scalar multiplication in order to deﬁne matrix-vector multipli-
cation between Poincaré balls, one of the essential building blocks of hyperbolic neural networks.

Parallel transport. Finally, we connect parallel transport (from T0Dn
the following theorem, which we prove in appendix B.
Theorem 4. In the manifold (Dn
vector v ∈ T0Dn

c to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a
c is given by the following isometry:
λc
0
λc
x

x(x ⊕c expc

0(v)) =

0→x(v) = logc
P c

(16)

v.

c ) to gyrovector spaces with

As we’ll see later, this result is crucial in order to deﬁne and optimize parameters shared between
different tangent spaces, such as biases in hyperbolic neural layers or parameters of hyperbolic MLR.

3 Hyperbolic Neural Networks

Neural networks can be seen as being made of compositions of basic operations, such as linear
maps, bias translations, pointwise non-linearities and a ﬁnal sigmoid or softmax layer. We ﬁrst
explain how to construct a softmax layer for logits lying in a Poincaré ball. Then, we explain how
to transform a mapping between two Euclidean spaces as one between Poincaré balls, yielding
matrix-vector multiplication and pointwise non-linearities in the Poincaré ball. Finally, we present
possible adaptations of various recurrent neural networks to the hyperbolic domain.

5

3.1 Hyperbolic multiclass logistic regression

In order to perform multi-class classiﬁcation on the Poincaré ball, one needs to generalize multinomial
logistic regression (MLR) − also called softmax regression − to the Poincaré ball.

Reformulating Euclidean MLR. Let’s ﬁrst reformulate Euclidean MLR from the perspective of
distances to margin hyperplanes, as in [19, Section 5]. This will allow us to easily generalize it.

Given K classes, one learns a margin hyperplane for each such class using softmax probabilities:

∀k ∈ {1, ..., K},

p(y = k|x) ∝ exp (((cid:104)ak, x(cid:105) − bk)) , where bk ∈ R, x, ak ∈ Rn.

(17)

Note that any afﬁne hyperplane in Rn can be written with a normal vector a and a scalar shift b:

Ha,b = {x ∈ Rn : (cid:104)a, x(cid:105) − b = 0}, where a ∈ Rn \ {0}, and b ∈ R.

(18)

As in [19, Section 5], we note that (cid:104)a, x(cid:105) − b = sign((cid:104)a, x(cid:105) − b)(cid:107)a(cid:107)d(x, Ha,b). Using Eq. (17):

p(y = k|x) ∝ exp(sign((cid:104)ak, x(cid:105) − bk)(cid:107)ak(cid:107)d(x, Hak,bk )), bk ∈ R, x, ak ∈ Rn.

(19)

As it is not immediately obvious how to generalize the Euclidean hyperplane of Eq. (18) to other
spaces such as the Poincaré ball, we reformulate it as follows:

˜Ha,p = {x ∈ Rn : (cid:104)−p + x, a(cid:105) = 0} = p + {a}⊥, where p ∈ Rn, a ∈ Rn \ {0}.

(20)

This new deﬁnition relates to the previous one as ˜Ha,p = Ha,(cid:104)a,p(cid:105). Rewriting Eq. (19) with b = (cid:104)a, p(cid:105):
p(y = k|x) ∝ exp(sign((cid:104)−pk + x, ak(cid:105))(cid:107)ak(cid:107)d(x, ˜Hak,pk )), with pk, x, ak ∈ Rn.

(21)

It is now natural to adapt the previous deﬁnition to the hyperbolic setting by replacing + by ⊕c:
Deﬁnition 3.1 (Poincaré hyperplanes). For p ∈ Dn
p(z, a) = 0} = {z ∈ TpDn
gc

c : (cid:104)z, a(cid:105) = 0}. Then, we deﬁne Poincaré hyperplanes as

c \ {0}, let {a}⊥ := {z ∈ TpDn
c :

c , a ∈ TpDn

˜H c

a,p := {x ∈ Dn

c : (cid:104)logc

p(x), a(cid:105)p = 0} = expc

p({a}⊥) = {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}.

(22)

The last equality is shown appendix C. ˜H c
all geodesics in Dn
hypergyroplanes, see [27, deﬁnition 5.8]. A 3D hyperplane example is depicted in Fig. 1.

a,p can also be described as the union of images of
c orthogonal to a and containing p. Notice that our deﬁnition matches that of

Next, we need the following theorem, proved in appendix D:
Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(23)

Final formula for MLR in the Poincaré ball. Putting together Eq. (21) and Thm. 5, we get the
hyperbolic MLR formulation. Given K classes and k ∈ {1, . . . , K}, pk ∈ Dn
c \ {0}:

c , ak ∈ Tpk

Dn

p(y = k|x) ∝ exp(sign((cid:104)−pk ⊕c x, ak(cid:105))

gc
pk

(ak, ak)dc(x, ˜H c

)),

ak,pk

∀x ∈ Dn
c ,

(24)

or, equivalently

p(y = k|x) ∝ exp

(cid:18) λc
pk

(cid:107)ak(cid:107)
√
c

sinh−1

(cid:18)

√
2

c(cid:104)−pk ⊕c x, ak(cid:105)

(cid:19)(cid:19)

(1 − c(cid:107) − pk ⊕c x(cid:107)2)(cid:107)ak(cid:107)

,

∀x ∈ Dn
c .

(25)

this goes to p(y = k|x) ∝ exp(4(cid:104)−pk + x, ak(cid:105)) =

Notice that when c goes to zero,
exp((λ0
pk

)2(cid:104)−pk + x, ak(cid:105)) = exp((cid:104)−pk + x, ak(cid:105)0), recovering the usual Euclidean softmax.
However, at this point it is unclear how to perform optimization over ak, since it lives in Tpk
hence depends on pk. The solution is that one should write ak = P c
)a(cid:48)
k ∈ T0Dn
a(cid:48)

c = Rn, and optimize a(cid:48)

k as a Euclidean parameter.

k) = (λc

0/λc
pk

0→pk

(a(cid:48)

Dn
c and
k, where

1
√
c

(cid:113)

6

3.2 Hyperbolic feed-forward layers

In order to deﬁne hyperbolic neural networks, it is crucial to de-
ﬁne a canonically simple parametric family of transformations,
playing the role of linear mappings in usual Euclidean neural
networks, and to know how to apply pointwise non-linearities.
Inspiring ourselves from our reformulation of Möbius scalar
multiplication in Eq. (14), we deﬁne:
Deﬁnition 3.2 (Möbius version). For f : Rn → Rm, we deﬁne
the Möbius version of f as the map from Dn

c to Dm

c by:

f ⊗c(x) := expc

0(f (logc

0(x))),

(26)

where expc

0 : T0m

Dm

c → Dm

c and logc

0 : Dn

c → T0n

Dn
c .

Figure 1: An example of a hyper-
bolic hyperplane in D3
1 plotted us-
ing sampling. The red point is p.
The shown normal axis to the hy-
perplane through p is parallel to a.

Note that similarly as for other Möbius operations, we recover
the Euclidean mapping in the limit c → 0 if f is continuous, as limc→0 f ⊗c(x) = f (x). This
deﬁnition satisﬁes a few desirable properties too, such as: (f ◦ g)⊗c = f ⊗c ◦ g⊗c for f : Rm → Rl
and g : Rn → Rm (morphism property), and f ⊗c(x)/(cid:107)f ⊗c(x)(cid:107) = f (x)/(cid:107)f (x)(cid:107) for f (x) (cid:54)= 0
(direction preserving). It is then straight-forward to prove the following result:
Lemma 6 (Möbius matrix-vector multiplication). If M : Rn → Rm is a linear map, which we
identify with its matrix representation, then ∀x ∈ Dn

c , if M x (cid:54)= 0 we have

M ⊗c(x) = (1/

c) tanh

√

(cid:18) (cid:107)M x(cid:107)
(cid:107)x(cid:107)

√

tanh−1(

c(cid:107)x(cid:107))

(cid:19) M x
(cid:107)M x(cid:107)

,

(27)

and M ⊗c(x) = 0 if M x = 0. Moreover, if we deﬁne the Möbius matrix-vector multiplication of
M ∈ Mm,n(R) and x ∈ Dn
c by M ⊗c x := M ⊗c(x), then we have (M M (cid:48)) ⊗c x = M ⊗c (M (cid:48) ⊗c x)
for M ∈ Ml,m(R) and M (cid:48) ∈ Mm,n(R) (matrix associativity), (rM ) ⊗c x = r ⊗c (M ⊗c x) for
r ∈ R and M ∈ Mm,n(R) (scalar-matrix associativity) and M ⊗c x = M x for all M ∈ On(R)
(rotations are preserved).

Pointwise non-linearity.
ϕ⊗c can be applied to elements of the Poincaré ball.

If ϕ : Rn → Rn is a pointwise non-linearity, then its Möbius version

Bias translation. The generalization of a translation in the Poincaré ball is naturally given by
moving along geodesics. But should we use the Möbius sum x ⊕c b with a hyperbolic bias b or the
x(b(cid:48)) with a Euclidean bias b(cid:48)? These views are uniﬁed with parallel transport
exponential map expc
c by a bias b ∈ Dn
(see Thm 4). Möbius translation of a point x ∈ Dn
(cid:18) λc
0
λc
x

c is given by
(cid:19)

x ← x ⊕c b = expc

0(b))) = expc
x

0→x(logc

x(P c

logc

0(b)

(28)

.

We recover Euclidean translations in the limit c → 0. Note that bias translations play a particular
Indeed, consider multiple layers of the form fk(x) = ϕk(Mkx), each of
role in this model.
which having Möbius version f ⊗c
k (Mk ⊗c x). Then their composition can be re-written
f ⊗c
k ◦ · · · ◦ f ⊗c
1 = expc
0. This means that these operations can essentially be
performed in Euclidean space. Therefore, it is the interposition between those with the bias translation
of Eq. (28) which differentiates this model from its Euclidean counterpart.

k (x) = ϕ⊗c
0 ◦fk ◦ · · · ◦ f1 ◦ logc

If a vector x ∈ Rn+p is the (vertical) concatenation
Concatenation of multiple input vectors.
of two vectors x1 ∈ Rn, x2 ∈ Rp, and M ∈ Mm,n+p(R) can be written as the (horizontal)
concatenation of two matrices M1 ∈ Mm,n(R) and M2 ∈ Mm,p(R), then M x = M1x1 + M2x2.
We generalize this to hyperbolic spaces: if we are given x1 ∈ Dn
c ×Dp
c ,
and M, M1, M2 as before, then we deﬁne M ⊗c x := M1 ⊗c x1 ⊕c M2 ⊗c x2. Note that when c goes
to zero, we recover the Euclidean formulation, as limc→0 M ⊗c x = limc→0 M1 ⊗c x1 ⊕c M2 ⊗c x2 =
M1x1 + M2x2 = M x. Moreover, hyperbolic vectors x ∈ Dn
c can also be "concatenated" with real
features y ∈ R by doing: M ⊗c x ⊕c y ⊗c b with learnable b ∈ Dm

c , x = (x1 x2)T ∈ Dn

c and M ∈ Mm,n(R).

c , x2 ∈ Dp

7

3.3 Hyperbolic RNN

Naive RNN. A simple RNN can be deﬁned by ht+1 = ϕ(W ht + U xt + b) where ϕ is a pointwise
non-linearity, typically tanh, sigmoid, ReLU, etc. This formula can be naturally generalized to the
hyperbolic space as follows. For parameters W ∈ Mm,n(R), U ∈ Mm,d(R), b ∈ Dm
c , we deﬁne:

ht+1 = ϕ⊗c (W ⊗c ht ⊕c U ⊗c xt ⊕c b),

ht ∈ Dn

c , xt ∈ Dd
c .

(29)

Note that if inputs xt’s are Euclidean, one can write ˜xt := expc
expc

(U xt)) = W ⊗c ht ⊕c expc

(P c

W ⊗cht

0→W ⊗cht

0(U xt) = W ⊗c ht ⊕c U ⊗c ˜xt.

0(xt) and use the above formula, since

GRU architecture. One can also adapt the GRU architecture:
rt = σ(W rht−1 + U rxt + br),
zt = σ(W zht−1 + U zxt + bz),
˜ht = ϕ(W (rt (cid:12) ht−1) + U xt + b), ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht,

(30)

where (cid:12) denotes pointwise product. First, how should we adapt the pointwise multiplication by a
scaling gate? Note that the deﬁnition of the Möbius version (see Eq. (26)) can be naturally extended
to maps f : Rn × Rp → Rm as f ⊗c : (h, h(cid:48)) ∈ Dn
0(h(cid:48)))). In
c (cid:55)→ expc
0(h(cid:48))) =
particular, choosing f (h, h(cid:48)) := σ(h) (cid:12) h(cid:48) yields6 f ⊗c(h, h(cid:48)) = expc
diag(σ(logc

0(h))) ⊗c h(cid:48). Hence we adapt rt (cid:12) ht−1 to diag(rt) ⊗c ht−1 and the reset gate rt to:
0(W r ⊗c ht−1 ⊕c U r ⊗c xt ⊕c br),

0(h), logc
0(h)) (cid:12) logc

0(f (logc
0(σ(logc

rt = σ logc

c × Dp

(31)

and similarly for the update gate zt. Note that as the argument of σ in the above is unbounded, rt and
zt can a priori take values onto the full range (0, 1). Now the intermediate hidden state becomes:
˜ht = ϕ⊗c ((W diag(rt)) ⊗c ht−1 ⊕c U ⊗c xt ⊕ b),

(32)

where Möbius matrix associativity simpliﬁes W ⊗c (diag(rt) ⊗c ht−1) into (W diag(rt)) ⊗c ht−1.
Finally, we propose to adapt the update-gate equation as

ht = ht−1 ⊕c diag(zt) ⊗c (−ht−1 ⊕c

˜ht).

(33)

Note that when c goes to zero, one recovers the usual GRU. Moreover, if zt = 0 or zt = 1, then ht
becomes ht−1 or ˜ht respectively, similarly as in the usual GRU. This adaptation was obtained by
adapting [24]: in this work, the authors re-derive the update-gate mechanism from a ﬁrst principle
called time-warping invariance. We adapted their derivation to the hyperbolic setting by using the
notion of gyroderivative [4] and proving a gyro-chain-rule (see appendix E).

4 Experiments

SNLI task and dataset. We evaluate our method on two tasks. The ﬁrst is natural language
inference, or textual entailment. Given two sentences, a premise (e.g. "Little kids A. and B. are
playing soccer.") and a hypothesis (e.g. "Two children are playing outdoors."), the binary classiﬁcation
task is to predict whether the second sentence can be inferred from the ﬁrst one. This deﬁnes a partial
order in the sentence space. We test hyperbolic networks on the biggest real dataset for this task,
SNLI [7]. It consists of 570K training, 10K validation and 10K test sentence pairs. Following [28],
we merge the "contradiction" and "neutral" classes into a single class of negative sentence pairs, while
the "entailment" class gives the positive pairs.

PREFIX task and datasets. We conjecture that the improvements of hyperbolic neural networks
are more signiﬁcant when the underlying data structure is closer to a tree. To test this, we design a
proof-of-concept task of detection of noisy preﬁxes, i.e. given two sentences, one has to decide if the
second sentence is a noisy preﬁx of the ﬁrst, or a random sentence. We thus build synthetic datasets
PREFIX-Z% (for Z being 10, 30 or 50) as follows: for each random ﬁrst sentence of random length
at most 20 and one random preﬁx of it, a second positive sentence is generated by randomly replacing
Z% of the words of the preﬁx, and a second negative sentence of same length is randomly generated.
Word vocabulary size is 100, and we generate 500K training, 10K validation and 10K test pairs.

6If x has n coordinates, then diag(x) denotes the diagonal matrix of size n with xi’s on its diagonal.

8

Models architecture. Our neural network layers can be used in a plug-n-play manner exactly like
standard Euclidean layers. They can also be combined with Euclidean layers. However, optimization
w.r.t. hyperbolic parameters is different (see below) and based on Riemannian gradients which
are just rescaled Euclidean gradients when working in the conformal Poincaré model [21]. Thus,
back-propagation can be applied in the standard way.

In our setting, we embed the two sentences using two distinct hyperbolic RNNs or GRUs. The
sentence embeddings are then fed together with their squared distance (hyperbolic or Euclidean,
depending on their geometry) to a FFNN (Euclidean or hyperbolic, see Sec. 3.2) which is further
fed to an MLR (Euclidean or hyperbolic, see Sec. 3.1) that gives probabilities of the two classes
(entailment vs neutral). We use cross-entropy loss on top. Note that hyperbolic and Euclidean layers
can be mixed, e.g. the full network can be hyperbolic and only the last layer be Euclidean, in which
case one has to use log0 and exp0 functions to move between the two manifolds in a correct manner
as explained for Eq. 26.

Optimization. Our models have both Euclidean (e.g. weight matrices in both Euclidean and
hyperbolic FFNNs, RNNs or GRUs) and hyperbolic parameters (e.g. word embeddings or biases for
the hyperbolic layers). We optimize the Euclidean parameters with Adam [16] (learning rate 0.001).
Hyperbolic parameters cannot be updated with an equivalent method that keeps track of gradient
history due to the absence of a Riemannian Adam. Thus, they are optimized using full Riemannian
stochastic gradient descent (RSGD) [5, 11]. We also experiment with projected RSGD [21], but
optimization was sometimes less stable. We use a different constant learning rate for word embeddings
(0.1) and other hyperbolic weights (0.01) because words are updated less frequently.

Numerical errors. Gradients of the basic operations deﬁned above (e.g. ⊕c, exponential map) are
c(cid:107)x(cid:107) = 1. Thus, we
not deﬁned when the hyperbolic argument vectors are on the ball border, i.e.
always project results of these operations in the ball of radius 1 − (cid:15), where (cid:15) = 10−5. Numerical
errors also appear when hyperbolic vectors get closer to 0, thus we perturb them with an (cid:15)(cid:48) = 10−15
before they are used in any of the above operations. Finally, arguments of the tanh function are
clipped between ±15 to avoid numerical errors, while arguments of tanh−1 are clipped to at most
1 − 10−5.

√

Hyperparameters. For all methods, baselines and datasets, we use c = 1, word and hidden state
embedding dimension of 5 (we focus on the low dimensional setting that was shown to already
be effective [21]), batch size of 64. We ran all methods for a ﬁxed number of 30 epochs. For all
models, we experiment with both identity (no non-linearity) or tanh non-linearity in the RNN/GRU
cell, as well as identity or ReLU after the FFNN layer and before MLR. As expected, for the fully
Euclidean models, tanh and ReLU respectively surpassed the identity variant by a large margin. We
only report the best Euclidean results. Interestingly, for the hyperbolic models, using only identity for
both non-linearities works slightly better and this is likely due to two facts: i) our hyperbolic layers
already contain non-linearities by their nature, ii) tanh is limiting the output domain of the sentence
embeddings, but the hyperbolic speciﬁc geometry is more pronounced at the ball border, i.e. at the
hyperbolic "inﬁnity", compared to the center of the ball.

For the results shown in Tab. 1, we run each model (baseline or ours) exactly 3 times and report the
test result corresponding to the best validation result from these 3 runs. We do this because the highly
non-convex spectrum of hyperbolic neural networks sometimes results in convergence to poor local
minima, suggesting that initialization is very important.

Results. Results are shown in Tab. 1. Note that the fully Euclidean baseline models might have
an advantage over hyperbolic baselines because more sophisticated optimization algorithms such
as Adam do not have a hyperbolic analogue at the moment. We ﬁrst observe that all GRU models
overpass their RNN variants. Hyperbolic RNNs and GRUs have the most signiﬁcant improvement
over their Euclidean variants when the underlying data structure is more tree-like, e.g. for PREFIX-
10% − for which the tree relation between sentences and their preﬁxes is more prominent − we
reduce the error by a factor of 3.35 for hyperbolic vs Euclidean RNN, and by a factor of 1.5 for
hyperbolic vs Euclidean GRU. As soon as the underlying structure diverges more and more from
a tree, the accuracy gap decreases − for example, for PREFIX-50% the noise heavily affects the
representational power of hyperbolic networks. Also, note that on SNLI our methods perform
similarly as with their Euclidean variants. Moreover, hyperbolic and Euclidean MLR are on par when

9

SNLI

PREFIX-10% PREFIX-30% PREFIX-50%

FULLY EUCLIDEAN RNN
HYPERBOLIC RNN+FFNN, EUCL MLR
FULLY HYPERBOLIC RNN
FULLY EUCLIDEAN GRU
HYPERBOLIC GRU+FFNN, EUCL MLR
FULLY HYPERBOLIC GRU

79.34 %
79.18 %
78.21 %
81.52 %
79.76 %
81.19 %

89.62 %
96.36 %
96.91 %
95.96 %
97.36 %
97.14 %

81.71 %
87.83 %
87.25 %
86.47 %
88.47 %
88.26 %

72.10 %
76.50 %
62.94 %
75.04 %
76.87 %
76.44 %

Table 1: Test accuracies for various models and four datasets. "Eucl" denotes Euclidean. All word
and sentence embeddings have dimension 5. We highlight in bold the best baseline (or baselines, if
the difference is less than 0.5%).

used in conjunction with hyperbolic sentence embeddings, suggesting further empirical investigation
is needed for this direction (see below).

We also observe that, in the hyperbolic setting, accuracy tends to increase when sentence embeddings
start increasing, and gets better as their norms converge towards 1 (the ball border for c = 1). Unlike
in the Euclidean case, this behavior does happen only after a few epochs and suggests that the model
should ﬁrst adjust the angular layout in order to disentangle the representations, before increasing their
norms to fully exploit the strong clustering property of the hyperbolic geometry. Similar behavior
was observed in the context of embedding trees by [21]. Details in appendix F.

MLR classiﬁcation experiments.
For the sentence entailment classi-
ﬁcation task we do not see a clear
advantage of hyperbolic MLR com-
pared to its Euclidean variant. A pos-
sible reason is that, when trained end-
to-end, the model might decide to
place positive and negative embed-
dings in a manner that is already well
separated with a classic MLR. As a
consequence, we further investigate
MLR for the task of subtree classiﬁ-
cation. Using an open source imple-
mentation7 of [21], we pre-trained
Poincaré embeddings of the Word-
Net noun hierarchy (82,115 nodes).
We then choose one node in this tree
(see Table 2) and classify all other
nodes (solely based on their embed-
dings) as being part of the subtree
rooted at this node. All nodes in such a subtree are divided into positive training nodes (80%) and
positive test nodes (20%). The same splitting procedure is applied for the remaining WordNet nodes
that are divided into a negative training and negative test set respectively. Three variants of MLR
are then trained on top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation
task: hyperbolic MLR, Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean
MLR applied after mapping all embeddings in the tangent space at 0 using the log0 map. We use
different embedding dimensions : 2, 3, 5 and 10. For the hyperbolic MLR, we use full Riemannian
SGD with a learning rate of 0.001. For the two Euclidean models we use ADAM optimizer and the
same learning rate. During training, we always sample the same number of negative and positive
nodes in each minibatch of size 16; thus positive nodes are frequently resampled. All methods are
trained for 30 epochs and the ﬁnal F1 score is reported (no hyperparameters to validate are used, thus
we do not require a validation set). This procedure is repeated for four subtrees of different sizes.

Figure 2: Hyperbolic (left) vs Direct Euclidean (right) binary
MLR used to classify nodes as being part in the GROUP.N.01
subtree of the WordNet noun hierarchy solely based on their
Poincaré embeddings. The positive points (from the subtree)
are in blue, the negative points (the rest) are in red and the
trained positive separation hyperplane is depicted in green.

Quantitative results are presented in Table 2. We can see that the hyperbolic MLR overpasses
its Euclidean variants in almost all settings, sometimes by a large margin. Moreover, to provide

7https://github.com/dalab/hyperbolic_cones

10

WORDNET
SUBTREE

ANIMAL.N.01
3218 / 798

GROUP.N.01
6649 / 1727

WORKER.N.01
861 / 254

MAMMAL.N.01
953 / 228

MODEL

D = 2

D = 3

D = 5

D = 10

HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL

47.43 ± 1.07%
41.69 ± 0.19%
38.89 ± 0.01%
81.72 ± 0.17%
61.13 ± 0.42%
60.75 ± 0.24%
12.68 ± 0.82%
10.86 ± 0.01%
9.04 ± 0.06%
32.01 ± 17.14%
15.58 ± 0.04%
13.10 ± 0.13%

91.92 ± 0.61%
68.43 ± 3.90%
62.57 ± 0.61%
89.87 ± 2.73%
63.56 ± 1.22%
61.98 ± 0.57%
24.09 ± 1.49%
22.39 ± 0.04%
22.57 ± 0.20%
87.54 ± 4.55%
44.68 ± 1.87%
44.89 ± 1.18%

98.07 ± 0.55%
95.59 ± 1.18%
89.21 ± 1.34%
87.89 ± 0.80%
67.82 ± 0.81%
67.92 ± 0.74%
55.46 ± 5.49%
35.23 ± 3.16%
26.47 ± 0.78%
88.73 ± 3.22%
59.35 ± 1.31%
52.51 ± 0.85%

99.26 ± 0.59%
99.36 ± 0.18%
98.27 ± 0.70%
91.91 ± 3.07%
91.38 ± 1.19%
91.41 ± 0.18%
66.83 ± 11.38%
47.29 ± 3.93%
36.66 ± 2.74%
91.37 ± 6.09%
77.76 ± 5.08%
56.11 ± 2.21%

Table 2: Test F1 classiﬁcation scores for four different subtrees of WordNet noun tree. All nodes
in such a subtree are divided into positive training nodes (80%) and positive test nodes (20%);
these counts are shown below each subtree root. The same splitting procedure is applied for the
remaining nodes to obtain negative training and test sets. Three variants of MLR are then trained on
top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation task: hyperbolic MLR,
Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean MLR applied after
mapping all embeddings in the tangent space at 0 using the log0 map. 95% conﬁdence intervals for 3
different runs are shown for each method and each different embedding dimension (2, 3, 5 or 10).

further understanding, we plot the 2-dimensional embeddings and the trained separation hyperplanes
(geodesics in this case) in Figure 2. We can see that respecting the hyperbolic geometry is very
important for a quality classiﬁcation model.

5 Conclusion

We showed how classic Euclidean deep learning tools such as MLR, FFNNs, RNNs or GRUs can be
generalized in a principled manner to all spaces of constant negative curvature combining Riemannian
geometry with the elegant theory of gyrovector spaces. Empirically we found that our models
outperform or are on par with corresponding Euclidean architectures on sequential data with implicit
hierarchical structure. We hope to trigger exciting future research related to better understanding
of the hyperbolic non-convexity spectrum and development of other non-Euclidean deep learning
methods.
Our data and Tensorﬂow [1] code are publicly available8.

Acknowledgements

We thank Igor Petrovski for useful pointers regarding the implementation.

This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement
number 167176. Gary Bécigneul is also funded by the Max Planck ETH Center for Learning
Systems.

References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for
large-scale machine learning. 2016.

[2] Ungar Abraham Albert. Analytic hyperbolic geometry and Albert Einstein’s special theory of

relativity. World scientiﬁc, 2008.

8https://github.com/dalab/hyperbolic_nn

11

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of the International Conference on Learning
Representations (ICLR), 2015.

[4] Graciela S Birman and Abraham A Ungar. The hyperbolic derivative in the poincaré ball model
of hyperbolic geometry. Journal of mathematical analysis and applications, 254(1):321–333,
2001.

[5] S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on

Automatic Control, 58(9):2217–2229, Sept 2013.

[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems (NIPS), pages 2787–2795, 2013.

[7] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 632–642. Association
for Computational Linguistics, 2015.

[8] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,
34(4):18–42, 2017.

[9] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry.

Flavors of geometry, 31:59–115, 1997.

[10] Christopher De Sa, Albert Gu, Christopher Ré, and Frederic Sala. Representation tradeoffs for

hyperbolic embeddings. arXiv preprint arXiv:1804.03329, 2018.

[11] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In Proceedings of the thirty-ﬁfth international conference
on machine learning (ICML), 2018.

[12] Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pages 75–263. Springer, 1987.

[13] Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the

Cambridge Philosophical Society, page 1–17, 2017.

[14] Christopher Hopper and Ben Andrews. The Ricci ﬂow in Riemannian geometry. Springer, 2010.

[15] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1746–1751. Association for Computational Linguistics, 2014.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations (ICLR), 2015.

[17] Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marián Boguná.

Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.

[18] John Lamping, Ramana Rao, and Peter Pirolli. A focus+ context technique based on hyperbolic
geometry for visualizing large hierarchies. In Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 401–408. ACM Press/Addison-Wesley Publishing Co.,
1995.

[19] Guy Lebanon and John Lafferty. Hyperplane margin classiﬁers on the multinomial manifold. In
Proceedings of the international conference on machine learning (ICML), page 66. ACM, 2004.

[20] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective
learning on multi-relational data. In Proceedings of the international conference on machine
learning (ICML), volume 11, pages 809–816, 2011.

12

[21] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical repre-
sentations. In Advances in Neural Information Processing Systems (NIPS), pages 6341–6350,
2017.

[22] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blun-
som. Reasoning about entailment with neural attention. In Proceedings of the International
Conference on Learning Representations (ICLR), 2015.

[23] Michael Spivak. A comprehensive introduction to differential geometry. Publish or perish, 1979.

[24] Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In Proceedings of

the International Conference on Learning Representations (ICLR), 2018.

[25] Abraham A Ungar. Hyperbolic trigonometry and its application in the poincaré ball model of

hyperbolic geometry. Computers & Mathematics with Applications, 41(1-2):135–147, 2001.

[26] Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis

Lectures on Mathematics and Statistics, 1(1):1–194, 2008.

[27] Abraham Albert Ungar. Analytic hyperbolic geometry in n dimensions: An introduction. CRC

Press, 2014.

[28] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. In Proceedings of the International Conference on Learning Representations (ICLR),
2016.

[29] J Vermeer. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic

plane. Topology and its Applications, 152(3):226–242, 2005.

13

A Hyperbolic Trigonometry

Hyperbolic angles. For A, B, C ∈ Dn
c , we denote by ∠A := ∠BAC the angle between the two
geodesics starting from A and ending at B and C respectively. This angle can be deﬁned in two
equivalent ways: i) either using the angle between the initial velocities of the two geodesics as given
by Eq. 5, or ii) using the formula

cos(∠A) =

(cid:28) (−A) ⊕c B
(cid:107)(−A) ⊕c B(cid:107)

,

(−A) ⊕c C
(cid:107)(−A) ⊕c C(cid:107)

(cid:29)

,

In this case, ∠A is also called a gyroangle in the work of [26, section 4].

Hyperbolic law of sines. We state here the hyperbolic law of sines. If for A, B, C ∈ Dn
c , we
denote by ∠B := ∠ABC the angle between the two geodesics starting from B and ending at A and
C respectively, and by ˜c = dc(B, A) the length of the hyperbolic segment BA (and similarly for
others), then we have:

sin(∠A)
√
c˜a)
sinh(

=

sin(∠B)
√
c˜b)
sinh(

=

sin(∠C)
√
c˜c)
sinh(

.

Note that one can also adapt the hyperbolic law of cosines to the hyperbolic space.

B Proof of Theorem 4

Theorem 4.
In the manifold (Dn
to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a vector v ∈ T0Dn
c

c is given by the following isometry:
λc
0
λc
x

0→x(v) = logc
P c

x(x ⊕c expc

0(v)) =

v.

Proof. The geodesic in Dn
v ∈ T0Dn
γ (i.e. X(t) ∈ Tγ(t)Dn

c from 0 to x is given in Eq. (10) by γ(t) = x ⊗c t, for t ∈ [0, 1]. Let
c . Then it is of common knowledge that there exists a unique parallel9 vector ﬁeld X along

c , ∀t ∈ [0, 1]) such that X(0) = v. Let’s deﬁne:
X : t ∈ [0, 1] (cid:55)→ logc

γ(t)(γ(t) ⊕c expc

0(v)) ∈ Tγ(t)Dn
c .

Clearly, X is a vector ﬁeld along γ such that X(0) = v. Now deﬁne
0→x : v ∈ T0Dn
P c

x(x ⊕c expc

0(v)) ∈ TxDn
c .

c (cid:55)→ logc
0→x(v) = λc

c . Since P c

0→x is a linear isometry from T0Dn
v, hence P c
c
0→x(v) = X(1), it is enough to prove that X is parallel in order to guarantee that

From Eq. (12), it is easily seen that P c
to TxDn
c to TxDn
0→x is the parallel transport from T0Dn
P c
c .
Since X is a vector ﬁeld along γ, its covariant derivative can be expressed with the Levi-Civita
connection ∇c associated to gc:

0
λc
x

DX
∂t

= ∇c

˙γ(t)X.

Let’s compute the Levi-Civita connection from its Christoffel symbols. In a local coordinate system,
they can be written as

Γi

jk =

(gc)il(∂jgc

lk + ∂kgc

lj − ∂lgc

jk),

1
2

where superscripts denote the inverse metric tensor and using Einstein’s notations. As gc
at γ(t) ∈ Dn

c this yields:

ij = (λc)2δij,

jk = cλc
Γi

γ(t)(δikγ(t)j + δijγ(t)k − δjkγ(t)i).

9i.e. that DX

∂t = 0 for t ∈ [0, 1], where D

∂t denotes the covariant derivative.

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

14

On the other hand, since X(t) = (λc

∇c

˙γ(t)X = ˙γ(t)i∇c

i X = ˙γ(t)i∇c
i

= vj ˙γ(t)i∇c
i

0/λc

γ(t))v, we have
(cid:32)

(cid:33)

λc
0
λc

γ(t)

v

(cid:32)

λc
0
λc

γ(t)

(cid:33)

ej

.

√

√

Since γ(t) = (1/
Hence there exists K x

c) tanh(t tanh−1(
t ∈ R such that ˙γ(t) = K x

c(cid:107)x(cid:107))) x

(cid:107)x(cid:107) , it is easily seen that ˙γ(t) is colinear to γ(t).
t γ(t). Moreover, we have the following Leibniz rule:
(cid:33)

(cid:32)

(cid:32)

∇c
i

λc
0
λc

γ(t)

(cid:33)

ej

=

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

ej.

Combining these yields

DX
∂t

= K x

t vjγ(t)i

(cid:32)

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

(cid:32)

(cid:33)

(cid:33)

ej

.

Replacing with the Christoffel symbols of ∇c at γ(t) gives

Moreover,

λc
0
λc

γ(t)

λc
0
λc

γ(t)

∇c

i ej =

ijek = 2c[δk
Γk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek.

∂
∂γ(t)i

(cid:32)

(cid:33)

λc
0
λc

γ(t)

ej =

∂
∂γ(t)i

(cid:0)−c(cid:107)γ(t)(cid:107)2(cid:1) ej = −2cγ(t)iej.

Putting together everything, we obtain

DX
∂t

= K x

t vjγ(t)i (cid:0)2c[δk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek − 2cγ(t)iej

(cid:1)

t vjγ(t)i (cid:0)γ(t)jei − δijγ(t)kek
t vj (cid:0)γ(t)jγ(t)iei − γ(t)iδijγ(t)kek
(cid:1)
t vj (cid:0)γ(t)jγ(t)iei − γ(t)jγ(t)kek

(cid:1)

(cid:1)

= 2cK x
= 2cK x
= 2cK x
= 0,

which concludes the proof.

C Proof of Eq. (22)

Proof. Two steps proof:
i) expc

p({a}⊥) ⊆ {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}:

Let z ∈ {a}⊥. From Eq. (12), we have that:

This, together with the left-cancellation law in gyrospaces (see section 2.3), implies that

expc

p(z) = −p ⊕c βz,

for some β ∈ R.

(cid:104)−p ⊕c expc

p(z), a(cid:105) = (cid:104)βz, a(cid:105) = 0

which is what we wanted.

ii) {x ∈ Dn
Let x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0} ⊆ expc
c s.t. (cid:104)−p ⊕c x, a(cid:105) = 0. Then, using Eq. (12), we derive that:
for some β ∈ R,

p(x) = β(−p ⊕c x),

p({a}⊥):

logc

which is orthogonal to a, by assumption. This implies logc

p(x) ∈ {a}⊥, hence x ∈ expc

p({a}⊥).

15

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(54)

D Proof of Theorem 5

Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

1
√
c

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(55)

Proof. We ﬁrst need to prove the following lemma, trivial in the Euclidean space, but not in the
Poincaré ball:
Lemma 7. (Orthogonal projection on a geodesic) Any point in the Poincaré ball has a unique
orthogonal projection on any given geodesic that does not pass through the point. Formally, for all
y ∈ Dn
c and for all geodesics γx→z(·) s.t. y /∈ Im γx→z, there exists an unique w ∈ Im γx→z s.t.
∠(γw→y, γx→z) = π/2.

Proof. We ﬁrst note that any geodesic in Dn
and has two "points at inﬁnity" lying on the ball border (v (cid:54)= 0):

c has the form γ(t) = u ⊕c v ⊗c t as given by Eq. 11,

γ(±∞) = u ⊕c

√

±v
c(cid:107)v(cid:107)

∈ ∂Dn
c .

(56)

Using the notations in the lemma statement, the closed-form of γx→z is given by Eq. (10):

γx→z(t) = x ⊕c (−x ⊕c z) ⊗c t

We denote by x(cid:48), z(cid:48) ∈ ∂Dn
∠ywx(cid:48) is well deﬁned from Eq. (34):

c its points at inﬁnity as described by Eq. (56). Then, the hyperbolic angle

cos(∠(γw→y, γx→z)) = cos(∠ywz(cid:48)) =

(cid:104)−w ⊕c y, −w ⊕c z(cid:48)(cid:105)
(cid:107) − w ⊕c y(cid:107) · (cid:107) − w ⊕c z(cid:48)(cid:107)

.

(57)

We now perform 2 steps for this proof.

i) Existence of w:

The angle function from Eq. (57) is continuous w.r.t t when w = γx→z(t). So we ﬁrst prove existence
of an angle of π/2 by continuously moving w from x(cid:48) to z(cid:48) when t goes from −∞ to ∞, and
observing that cos(∠ywz(cid:48)) goes from −1 to 1 as follows:

cos(∠yx(cid:48)z(cid:48)) = 1 & lim
w→z(cid:48)

cos(∠ywz(cid:48)) = −1.

(58)

The left part of Eq. (58) follows from Eq. (57) and from the fact (easy to show from the deﬁnition
c (which is the case of x(cid:48)). The right part of Eq. (58)
of ⊕c) that a ⊕c b = a, when (cid:107)a(cid:107) = 1/
follows from the fact that ∠ywz(cid:48) = π − ∠ywx(cid:48) (from the conformal property, or from Eq. (34)) and
cos(∠yz(cid:48)x(cid:48)) = 1 (proved as above).
Hence cos(∠ywz(cid:48)) has to pass through 0 when going from −1 to 1, which achieves the proof of
existence.

√

ii) Uniqueness of w:
Assume by contradiction that there are two w and w(cid:48) on γx→z that form angles ∠ywx(cid:48) and ∠yw(cid:48)x(cid:48)
of π/2. Since w, w(cid:48), x(cid:48) are on the same geodesic, we have

π/2 = ∠yw(cid:48)x(cid:48) = ∠yw(cid:48)w = ∠ywx(cid:48) = ∠yw(cid:48)w
So ∆yww(cid:48) has two right angles, but in the Poincaré ball this is impossible.

(59)

Now, we need two more lemmas:
Lemma 8. (Minimizing distance from point to geodesic) The orthogonal projection of a point to
a geodesic (not passing through the point) is minimizing the distance between the point and the
geodesic.

Proof. The proof is similar with the Euclidean case and it’s based on hyperbolic sine law and the fact
that in any right hyperbolic triangle the hypotenuse is strictly longer than any of the other sides.

16

Lemma 9. (Geodesics through p) Let ˜H c
all points on the geodesic γp→w are included in ˜H c

a,p.

a,p be a Poincaré hyperplane. Then, for any w ∈ ˜H c

a,p \ {p},

Proof. γp→w(t) = p ⊕c (−p ⊕c w) ⊗c t. Then, it is easy to check the condition in Eq. (22):

(cid:104)−p ⊕c γp→w(t), a(cid:105) = (cid:104)(−p ⊕c w) ⊗c t, a(cid:105) ∝ (cid:104)−p ⊕c w, a(cid:105) = 0.

(60)

We now turn back to our proof. Let x ∈ Dn
We prove that there is at least one point w∗ ∈ ˜H c

c be an arbitrary point and ˜H c

a,p a Poincaré hyperplane.

a,p that achieves the inﬁmum distance

dc(x, w∗) = inf
w∈ ˜H c

a,p

dc(x, w),

and, moreover, that this distance is the same as the one in the theorem’s statement.
We ﬁrst note that for any point w ∈ ˜H c
and Lemma 9, it is obvious that the projection of x to γp→w will give a strictly lower distance.
Thus, we only consider w ∈ ˜H c
triangle ∆xwp, one gets:

a,p such that ∠xwp = π/2. Applying hyperbolic sine law in the right

a,p, if ∠xwp (cid:54)= π/2, then w (cid:54)= w∗. Indeed, using Lemma 8

dc(x, w) = (1/

c) sinh−1 (cid:0)sinh(

c dc(x, p)) · sin(∠xpw)(cid:1) .

√

√

One of the above quantities does not depend on w:

√

√

sinh(

c dc(x, p)) = sinh(2 tanh−1(

c(cid:107) − p ⊕c x(cid:107))) =

√
2
c(cid:107) − p ⊕c x(cid:107)
1 − c(cid:107) − p ⊕c x(cid:107)2 .

The other quantity is sin(∠xpw) which is minimized when the angle ∠xpw is minimized (be-
cause ∠xpw < π/2 for the hyperbolic right triangle ∆xwp), or, alternatively, when cos(∠xpw) is
maximized. But, we already have from Eq. (34) that:

cos(∠xpw) =

(cid:104)−p ⊕c x, −p ⊕c w(cid:105)
(cid:107) − p ⊕c x(cid:107) · (cid:107) − p ⊕c w(cid:107)

.

To maximize the above, the constraint on the right angle at w can be dropped because cos(∠xpw)
depends only on the geodesic γp→w and not on w itself, and because there is always an orthogonal
projection from any point x to any geodesic as stated by Lemma 7. Thus, it remains to ﬁnd the
maximum of Eq. (64) when w ∈ ˜H c
a,p from Eq. (22), one can easily
prove that

a,p. Using the deﬁnition of ˜H c

Using that fact that logc

p(w)/(cid:107) logc

p(w)(cid:107) = −p ⊕c w/(cid:107) − p ⊕c w(cid:107), we just have to ﬁnd

and we are left with a well known Euclidean problem which is equivalent to ﬁnding the minimum
angle between the vector −p ⊕c x (viewed as Euclidean) and the hyperplane {a}⊥. This angle
is given by the Euclidean orthogonal projection whose sin value is the distance from the vector’s
endpoint to the hyperplane divided by the vector’s length:

{logc

p(w) : w ∈ ˜H c

a,p} = {a}⊥.

max
z∈{a}⊥

(cid:18) (cid:104)−p ⊕c x, z(cid:105)

(cid:107) − p ⊕c x(cid:107) · (cid:107)z(cid:107)

(cid:19)

,

sin(∠xpw∗) =

|(cid:104)−p ⊕c x, a
(cid:107) − p ⊕c x(cid:107)

(cid:107)a(cid:107) (cid:105)|

.

17

It follows that a point w∗ ∈ ˜H c
Eqs. (61),(62),(63) and (67) concludes the proof.

a,p satisfying Eq. (67) exists (but might not be unique). Combining

(61)

(62)

(63)

(64)

(65)

(66)

(67)

(cid:3)

E Derivation of the Hyperbolic GRU Update-gate

In [24], the authors recover the update/forget-gate mechanism of a GRU/LSTM by requiring that the
class of neural networks given by the chosen architecture be invariant to time-warpings. The idea is
the following.

Recovering the update-gate from time-warping. A naive RNN is given by the equation

h(t + 1) = ϕ(W h(t) + U x(t) + b)

Let’s drop the bias b to simplify notations. If h is seen as a differentiable function of time, then a
ﬁrst-order Taylor development gives h(t + δt) ≈ h(t) + δt dh
dt (t) for small δt. Combining this for
δt = 1 with the naive RNN equation, one gets

dh
dt

dα
dt

(t) = ϕ(W h(t) + U x(t)) − h(t).

As this is written for any t, one can replace it by t ← α(t) where α is a (smooth) increasing function
of t called the time-warping. Denoting by ˜h(t) := h(α(t)) and ˜x(t) := x(α(t)), using the chain rule
d˜h
dt (t) = dα

dt (α(t)), one gets

dt (t) dh

d˜h
dt

dα
dt

(t) =

(t)ϕ(W ˜h(t) + U ˜x(t)) −

(t)˜h(t).

(70)

Removing the tildas to simplify notations, discretizing back with dh

dt (t) ≈ h(t + 1) − h(t) yields

h(t + 1) =

(t)ϕ(W h(t) + U x(t)) +

1 −

(t)

h(t).

(71)

dα
dt

(cid:18)

(cid:19)

dα
dt

Requiring that our class of neural networks be invariant to time-warpings means that this class should
contain RNNs deﬁned by Eq. (71), i.e. that dα
dt (t) can be learned. As this is a positive quantity, we
can parametrize it as z(t) = σ(W zh(t) + U zx(t)), recovering the forget-gate equation:

h(t + 1) = z(t)ϕ(W h(t) + U x(t)) + (1 − z(t))h(t).

Adapting this idea to hyperbolic RNNs. The gyroderivative [4] of a map h : R → Dn
as

c is deﬁned

dh
dt

(t) = lim
δt→0

1
δt

⊗c (−h(t) ⊕c h(t + δt)).

Using Möbius scalar associativity and the left-cancellation law leads us to

h(t + δt) ≈ h(t) ⊕c δt ⊗c

(t),

dh
dt

for small δt. Combining this with the equation of a simple hyperbolic RNN of Eq. (29) with δt = 1,
one gets

dh
dt

(t) = −h(t) ⊕c ϕ⊗c(W ⊗c h(t) ⊕c U ⊗c x(t)).

For the next step, we need the following lemma:
Lemma 10 (Gyro-chain-rule). For α : R → R differentiable and h : R → Dn
gyro-derivative, if ˜h := h ◦ α, then we have

c with a well-deﬁned

(68)

(69)

(72)

(73)

(74)

(75)

(76)

where dα

dt (t) denotes the usual derivative.

d˜h
dt

(t) =

(t) ⊗c

(α(t)),

dα
dt

dh
dt

18

(77)

(78)

(79)

(80)

(81)

Proof.

d˜h
dt

(t) = lim
δt→0

1
δt
1
δt

⊗c [−˜h(t) ⊕c

˜h(t + δt)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

= lim
δt→0

= lim
δt→0

= lim
δt→0

= lim
u→0
dα
dt

=

α(cid:48)(t) + O(δt)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
u

(t) ⊗c

(α(t))

dh
dt

⊗c [−h(α(t)) ⊕c h(α(t) + u)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

(Möbius scalar associativity) (82)

where we set u = δt(α(cid:48)(t) + O(δt)), with u → 0 when δt → 0, which concludes.

Using lemma 10 and Eq. (75), with similar notations as in Eq. (70) we have

d˜h
dt

dα
dt

(t) =

(t) ⊗c (−˜h(t) ⊕c ϕ⊗c(W ⊗c

˜h(t) ⊕c U ⊗c ˜x(t))).

(83)

Finally, discretizing back with Eq. (74), using the left-cancellation law and dropping the tildas yields

h(t + 1) = h(t) ⊕c

(t) ⊗c (−h(t) ⊕c ϕ⊗c (W ⊗c h(t) ⊕c U ⊗c x(t))).

(84)

dα
dt

Since α is a time-warping, by deﬁnition its derivative is positive and one can choose to parametrize
it with an update-gate zt (a scalar) deﬁned with a sigmoid. Generalizing this scalar scaling by the
Möbius version of the pointwise scaling (cid:12) yields the Möbius matrix scaling diag(zt) ⊗c ·, leading to
our proposed Eq. (33) for the hyperbolic GRU.

F More Experimental Investigations

The following empirical facts were observed for both hyperbolic RNNs and GRUs.

We observed that, in the hyperbolic setting, accuracy is often much higher when sentence embeddings
can go close to the border (hyperbolic "inﬁnity"), hence exploiting the hyperbolic nature of the space.
Moreover, the faster the two sentence norms go to 1, the more it’s likely that a good local minima
was reached. See ﬁgures 3 and 5.

We often observe that test accuracy starts increasing exactly when sentence embedding norms do.
However, in the hyperbolic setting, the sentence embeddings norms remain close to 0 for a few
epochs, which does not happen in the Euclidean case. See ﬁgures 3, 5 and 4. This mysterious fact
was also exhibited in a similar way by [21] which suggests that the model ﬁrst has to adjust the
angular layout in the almost Euclidean vicinity of 0 before increasing norms and fully exploiting
hyperbolic geometry.

19

(a) Test accuracy

(a) Test accuracy

20

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 3: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the same
architecture: hyperbolic GRU followed by hyperbolic FFNN and hyperbolic/Euclidean (half-half)
MLR. The X axis shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 4: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: Euclidean GRU followed by Euclidean FFNN and Euclidean MLR. The X axis
shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 5: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: hyperbolic RNN followed by hyperbolic FFNN and hyperbolic MLR. The X axis
shows millions of training examples processed.

(a) Test accuracy

21

8
1
0
2
 
n
u
J
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
2
1
1
9
0
.
5
0
8
1
:
v
i
X
r
a

Hyperbolic Neural Networks

Octavian-Eugen Ganea∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
octavian.ganea@inf.ethz.ch

Gary Bécigneul∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
gary.becigneul@inf.ethz.ch

Thomas Hofmann
Department of Computer Science
ETH Zürich
Zurich, Switzerland
thomas.hofmann@inf.ethz.ch

Abstract

Hyperbolic spaces have recently gained momentum in the context of machine
learning due to their high capacity and tree-likeliness properties. However, the
representational power of hyperbolic geometry is not yet on par with Euclidean
geometry, mostly because of the absence of corresponding hyperbolic neural
network layers. This makes it hard to use hyperbolic embeddings in downstream
tasks. Here, we bridge this gap in a principled manner by combining the formalism
of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model
of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep
learning tools: multinomial logistic regression, feed-forward and recurrent neural
networks such as gated recurrent units. This allows to embed sequential data and
perform classiﬁcation in the hyperbolic space. Empirically, we show that, even if
hyperbolic optimization tools are limited, hyperbolic sentence embeddings either
outperform or are on par with their Euclidean variants on textual entailment and
noisy-preﬁx recognition tasks.

1

Introduction

It is common in machine learning to represent data as being embedded in the Euclidean space Rn. The
main reason for such a choice is simply convenience, as this space has a vectorial structure, closed-
form formulas of distance and inner-product, and is the natural generalization of our intuition-friendly,
visual three-dimensional space. Moreover, embedding entities in such a continuous space allows to
feed them as input to neural networks, which has led to unprecedented performance on a broad range
of problems, including sentiment detection [15], machine translation [3], textual entailment [22] or
knowledge base link prediction [20, 6].

Despite the success of Euclidean embeddings, recent research has proven that many types of com-
plex data (e.g. graph data) from a multitude of ﬁelds (e.g. Biology, Network Science, Computer
Graphics or Computer Vision) exhibit a highly non-Euclidean latent anatomy [8]. In such cases, the
Euclidean space does not provide the most powerful or meaningful geometrical representations. For
example, [10] shows that arbitrary tree structures cannot be embedded with arbitrary low distortion
(i.e. almost preserving their metric) in the Euclidean space with unbounded number of dimensions,
but this task becomes surprisingly easy in the hyperbolic space with only 2 dimensions where the
exponential growth of distances matches the exponential growth of nodes with the tree depth.

∗Equal contribution.

The adoption of neural networks and deep learning in these non-Euclidean settings has been rather
limited until very recently, the main reason being the non-trivial or impossible principled general-
izations of basic operations (e.g. vector addition, matrix-vector multiplication, vector translation,
vector inner product) as well as, in more complex geometries, the lack of closed form expressions for
basic objects (e.g. distances, geodesics, parallel transport). Thus, classic tools such as multinomial
logistic regression (MLR), feed forward (FFNN) or recurrent neural networks (RNN) did not have a
correspondence in these geometries.

How should one generalize deep neural models to non-Euclidean domains ? In this paper we address
this question for one of the simplest, yet useful, non-Euclidean domains: spaces of constant negative
curvature, i.e. hyperbolic. Their tree-likeness properties have been extensively studied [12, 13, 26]
and used to visualize large taxonomies [18] or to embed heterogeneous complex networks [17]. In
machine learning, recently, hyperbolic representations greatly outperformed Euclidean embeddings
for hierarchical, taxonomic or entailment data [21, 10, 11]. Disjoint subtrees from the latent hierar-
chical structure surprisingly disentangle and cluster in the embedding space as a simple reﬂection of
the space’s negative curvature. However, appropriate deep learning tools are needed to embed feature
data in this space and use it in downstream tasks. For example, implicitly hierarchical sequence data
(e.g. textual entailment data, phylogenetic trees of DNA sequences or hierarchial captions of images)
would beneﬁt from suitable hyperbolic RNNs.

The main contribution of this paper is to bridge the gap between hyperbolic and Euclidean geometry
in the context of neural networks and deep learning by generalizing in a principled manner both the
basic operations as well as multinomial logistic regression (MLR), feed-forward (FFNN), simple and
gated (GRU) recurrent neural networks (RNN) to the Poincaré model of the hyperbolic geometry.
We do it by connecting the theory of gyrovector spaces and generalized Möbius transformations
introduced by [2, 26] with the Riemannian geometry properties of the manifold. We smoothly
parametrize basic operations and objects in all spaces of constant negative curvature using a uniﬁed
framework that depends only on the curvature value. Thus, we show how Euclidean and hyperbolic
spaces can be continuously deformed into each other. On a series of experiments and datasets we
showcase the effectiveness of our hyperbolic neural network layers compared to their "classic"
Euclidean variants on textual entailment and noisy-preﬁx recognition tasks. We hope that this paper
will open exciting future directions in the nascent ﬁeld of Geometric Deep Learning.

2 The Geometry of the Poincaré Ball

2.1 Basics of Riemannian geometry

We brieﬂy introduce basic concepts of differential geometry largely needed for a principled general-
ization of Euclidean neural networks. For more rigorous and in-depth expositions, see [23, 14].
An n-dimensional manifold M is a space that can locally be approximated by Rn: it is a generalization
to higher dimensions of the notion of a 2D surface. For x ∈ M, one can deﬁne the tangent space
TxM of M at x as the ﬁrst order linear approximation of M around x. A Riemannian metric
g = (gx)x∈M on M is a collection of inner-products gx : TxM × TxM → R varying smoothly
with x. A Riemannian manifold (M, g) is a manifold M equipped with a Riemannian metric g.
Although a choice of a Riemannian metric g on M only seems to deﬁne the geometry locally on M,
it induces global distances by integrating the length (of the speed vector living in the tangent space)
of a shortest path between two points:

(cid:90) 1

(cid:113)

d(x, y) = inf
γ

0

gγ(t)( ˙γ(t), ˙γ(t))dt,

(1)

where γ ∈ C∞([0, 1], M) is such that γ(0) = x and γ(1) = y. A smooth path γ of minimal length
between two points x and y is called a geodesic, and can be seen as the generalization of a straight-line
in Euclidean space. The parallel transport Px→y : TxM → TyM is a linear isometry between
tangent spaces which corresponds to moving tangent vectors along geodesics and deﬁnes a canonical
way to connect tangent spaces. The exponential map expx at x, when well-deﬁned, gives a way to
project back a vector v of the tangent space TxM at x, to a point expx(v) ∈ M on the manifold.
This map is often used to parametrize a geodesic γ starting from γ(0) := x ∈ M with unit-norm
direction ˙γ(0) := v ∈ TxM as t (cid:55)→ expx(tv). For geodesically complete manifolds, such as the
Poincaré ball considered in this work, expx is well-deﬁned on the full tangent space TxM. Finally, a

2

(2)

(3)

(4)

(5)

metric ˜g is said to be conformal to another metric g if it deﬁnes the same angles, i.e.

˜gx(u, v)
(cid:112)˜gx(u, u)(cid:112)˜gx(v, v)

=

gx(u, v)
(cid:112)gx(u, u)(cid:112)gx(v, v)

,

for all x ∈ M, u, v ∈ TxM \ {0}. This is equivalent to the existence of a smooth function
λ : M → R, called the conformal factor, such that ˜gx = λ2

xgx for all x ∈ M.

2.2 Hyperbolic space: the Poincaré ball

The hyperbolic space has ﬁve isometric models that one can work with [9]. Similarly as in [21] and
[11], we choose to work in the Poincaré ball. The Poincaré ball model (Dn, gD) is deﬁned by the
manifold Dn = {x ∈ Rn : (cid:107)x(cid:107) < 1} equipped with the following Riemannian metric:

gD
x = λ2

xgE, where λx :=

2
1 − (cid:107)x(cid:107)2 ,

gE = In being the Euclidean metric tensor. Note that the hyperbolic metric tensor is conformal to
the Euclidean one. The induced distance between two points x, y ∈ Dn is known to be given by

dD(x, y) = cosh−1

1 + 2

(cid:18)

(cid:107)x − y(cid:107)2
(1 − (cid:107)x(cid:107)2)(1 − (cid:107)y(cid:107)2)

(cid:19)

.

Since the Poincaré ball is conformal to Euclidean space, the angle between two vectors u, v ∈
TxDn \ {0} is given by

cos(∠(u, v)) =

gD
x (u, v)
x (u, u)(cid:112)gD

(cid:112)gD

x (v, v)

=

(cid:104)u, v(cid:105)
(cid:107)u(cid:107)(cid:107)v(cid:107)

.

2.3 Gyrovector spaces

In Euclidean space, natural operations inherited from the vectorial structure, such as vector addition,
subtraction and scalar multiplication are often useful. The framework of gyrovector spaces provides
an elegant non-associative algebraic formalism for hyperbolic geometry just as vector spaces provide
the algebraic setting for Euclidean geometry [2, 25, 26].

In particular, these operations are used in special relativity, allowing to add speed vectors belonging
to the Poincaré ball of radius c (the celerity, i.e. the speed of light) so that they remain in the ball,
hence not exceeding the speed of light.

We will make extensive use of these operations in our deﬁnitions of hyperbolic neural networks.
For c ≥ 0, denote2 by Dn
then Dn

c := {x ∈ Rn | c(cid:107)x(cid:107)2 < 1}. Note that if c = 0, then Dn
c. If c = 1 then we recover the usual ball Dn.

c is the open ball of radius 1/

c = Rn; if c > 0,

√

Möbius addition. The Möbius addition of x and y in Dn

c is deﬁned as

x ⊕c y :=

(1 + 2c(cid:104)x, y(cid:105) + c(cid:107)y(cid:107)2)x + (1 − c(cid:107)x(cid:107)2)y
1 + 2c(cid:104)x, y(cid:105) + c2(cid:107)x(cid:107)2(cid:107)y(cid:107)2

.

(6)

In particular, when c = 0, one recovers the Euclidean addition of two vectors in Rn. Note that
without loss of generality, the case c > 0 can be reduced to c = 1. Unless stated otherwise, we
will use ⊕ as ⊕1 to simplify notations. For general c > 0, this operation is not commutative nor
associative. However, it satisﬁes x ⊕c 0 = 0 ⊕c x = 0. Moreover, for any x, y ∈ Dn
c , we have
(−x) ⊕c x = x ⊕c (−x) = 0 and (−x) ⊕c (x ⊕c y) = y (left-cancellation law). The Möbius
substraction is then deﬁned by the use of the following notation: x (cid:9)c y := x ⊕c (−y). See [29,
section 2.1] for a geometric interpretation of the Möbius addition.

2We take different notations as in [25] where the author uses s = 1/

c.

√

3

Möbius scalar multiplication. For c > 0, the Möbius scalar multiplication of x ∈ Dn
r ∈ R is deﬁned as

c \ {0} by

r ⊗c x := (1/

c) tanh(r tanh−1(

c(cid:107)x(cid:107)))

√

√

x
(cid:107)x(cid:107)

,

(7)

and r ⊗c 0 := 0. Note that similarly as for the Möbius addition, one recovers the Euclidean scalar
multiplication when c goes to zero: limc→0 r ⊗c x = rx. This operation satisﬁes desirable properties
such as n ⊗c x = x ⊕c · · · ⊕c x (n additions), (r + r(cid:48)) ⊗c x = r ⊗c x ⊕c r(cid:48) ⊗c x (scalar distributivity3),
(rr(cid:48)) ⊗c x = r ⊗c (r(cid:48) ⊗c x) (scalar associativity) and |r| ⊗c x/(cid:107)r ⊗c x(cid:107) = x/(cid:107)x(cid:107) (scaling property).

c , gc) is given by4

Distance.
Euclidean one, with conformal factor λc
(Dn

If one deﬁnes the generalized hyperbolic metric tensor gc as the metric conformal to the
x := 2/(1 − c(cid:107)x(cid:107)2), then the induced distance function on
√

c) tanh−1 (cid:0)√
Again, observe that limc→0 dc(x, y) = 2(cid:107)x − y(cid:107), i.e. we recover Euclidean geometry in the limit5.
Moreover, for c = 1 we recover dD of Eq. (4).

c(cid:107) − x ⊕c y(cid:107)(cid:1) .

dc(x, y) = (2/

(8)

Hyperbolic trigonometry. Similarly as in the Euclidean space, one can deﬁne the notions of
hyperbolic angles or gyroangles (when using the ⊕c), as well as hyperbolic law of sines in the
generalized Poincaré ball (Dn

c , gc). We make use of these notions in our proofs. See Appendix A.

2.4 Connecting Gyrovector spaces and Riemannian geometry of the Poincaré ball

In this subsection, we present how geodesics in the Poincaré ball model are usually described with
Möbius operations, and push one step further the existing connection between gyrovector spaces and
the Poincaré ball by ﬁnding new identities involving the exponential map, and parallel transport.

In particular, these ﬁndings provide us with a simpler formulation of Möbius scalar multiplication,
yielding a natural deﬁnition of matrix-vector multiplication in the Poincaré ball.

Riemannian gyroline element. The Riemannian gyroline element is deﬁned for an inﬁnitesimal
dx as ds := (x + dx) (cid:9)c x, and its size is given by [26, section 3.7]:

(cid:107)ds(cid:107) = (cid:107)(x + dx) (cid:9)c x(cid:107) = (cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2).

(9)

What is remarkable is that it turns out to be identical, up to a scaling factor of 2, to the usual line
element 2(cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2) of the Riemannian manifold (Dn

c , gc).

Geodesics. The geodesic connecting points x, y ∈ Dn

c is shown in [2, 26] to be given by:

γx→y(t) := x ⊕c (−x ⊕c y) ⊗c t, with γx→y : R → Dn

c s.t. γx→y(0) = x and γx→y(1) = y.

Note that when c goes to 0, geodesics become straight-lines, recovering Euclidean geometry. In the
remainder of this subsection, we connect the gyrospace framework with Riemannian geometry.
Lemma 1. For any x ∈ Dn and v ∈ TxDn
c s.t. gc
x with direction v is given by:

x(v, v) = 1, the unit-speed geodesic starting from

γx,v(t) = x ⊕c

tanh

(cid:18)

(cid:18)√

(cid:19) v
√

c

t
2

c(cid:107)v(cid:107)

(cid:19)

, where γx,v : R → Dn s.t. γx,v(0) = x and ˙γx,v(0) = v.

(10)

(11)

Proof. One can use Eq. (10) and reparametrize it to unit-speed using Eq. (8). Alternatively, direct
computation and identiﬁcation with the formula in [11, Thm. 1] would give the same result. Using
Eq. (8) and Eq. (11), one can sanity-check that dc(γ(0), γ(t)) = t, ∀t ∈ [0, 1].

3⊗c has priority over ⊕c in the sense that a ⊗c b ⊕c c := (a ⊗c b) ⊕c c and a ⊕c b ⊗c c := a ⊕c (b ⊗c c).
4The notation −x ⊕c y should always be read as (−x) ⊕c y and not −(x ⊕c y).
5The factor 2 comes from the conformal factor λx = 2/(1 − (cid:107)x(cid:107)2), which is a convention setting the

curvature to −1.

4

Exponential and logarithmic maps. The following lemma gives the closed-form derivation of
exponential and logarithmic maps.
Lemma 2. For any point x ∈ Dn
map logc
c → TxDn
(cid:18)

c are given for v (cid:54)= 0 and y (cid:54)= x by:
(cid:18)√

c , the exponential map expc

c and the logarithmic

x : TxDn

c → Dn

x : Dn

√

(cid:19)

, logc

x(y) =

√

tanh−1(

c(cid:107) − x ⊕c y(cid:107))

expc

x(v) = x ⊕c

tanh

c

λc
x(cid:107)v(cid:107)
2

(cid:19) v
√

c(cid:107)v(cid:107)

2
cλc
x

−x ⊕c y
(cid:107) − x ⊕c y(cid:107)
(12)

.

Proof. Following the proof of [11, Cor. 1.1], one gets expc
gives the formula for expc

x. Algebraic check of the identity logc

x(v) = γx,
x(expc

v

x(cid:107)v(cid:107) (λc
λc

x(cid:107)v(cid:107)). Using Eq. (11)

x(v)) = v concludes.

The above maps have more appealing forms when x = 0, namely for v ∈ T0Dn

c \ {0}, y ∈ Dn

c \ {0}:

expc

0(v) = tanh(

c(cid:107)v(cid:107))

√

, logc

0(y) = tanh−1(

c(cid:107)y(cid:107))

√

(13)

√

y
c(cid:107)y(cid:107)

.

√

v
c(cid:107)v(cid:107)

Moreover, we still recover Euclidean geometry in the limit c → 0, as limc→0 expc
Euclidean exponential map, and limc→0 logc

x(y) = y − x is the Euclidean logarithmic map.

x(v) = x + v is the

Möbius scalar multiplication using exponential and logarithmic maps. We studied the expo-
nential and logarithmic maps in order to gain a better understanding of the Möbius scalar multiplica-
tion (Eq. (7)). We found the following:
Lemma 3. The quantity r ⊗ x can actually be obtained by projecting x in the tangent space at 0
with the logarithmic map, multiplying this projection by the scalar r in T0Dn
c , and then projecting it
back on the manifold with the exponential map:
0(r logc

∀r ∈ R, x ∈ Dn
c .

r ⊗c x = expc

0(x)),

(14)

In addition, we recover the well-known relation between geodesics connecting two points and the
exponential map:

γx→y(t) = x ⊕c (−x ⊕c y) ⊗c t = expc

x(t logc

x(y)),

t ∈ [0, 1].

(15)

This last result enables us to generalize scalar multiplication in order to deﬁne matrix-vector multipli-
cation between Poincaré balls, one of the essential building blocks of hyperbolic neural networks.

Parallel transport. Finally, we connect parallel transport (from T0Dn
the following theorem, which we prove in appendix B.
Theorem 4. In the manifold (Dn
vector v ∈ T0Dn

c to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a
c is given by the following isometry:
λc
0
λc
x

x(x ⊕c expc

0(v)) =

0→x(v) = logc
P c

(16)

v.

c ) to gyrovector spaces with

As we’ll see later, this result is crucial in order to deﬁne and optimize parameters shared between
different tangent spaces, such as biases in hyperbolic neural layers or parameters of hyperbolic MLR.

3 Hyperbolic Neural Networks

Neural networks can be seen as being made of compositions of basic operations, such as linear
maps, bias translations, pointwise non-linearities and a ﬁnal sigmoid or softmax layer. We ﬁrst
explain how to construct a softmax layer for logits lying in a Poincaré ball. Then, we explain how
to transform a mapping between two Euclidean spaces as one between Poincaré balls, yielding
matrix-vector multiplication and pointwise non-linearities in the Poincaré ball. Finally, we present
possible adaptations of various recurrent neural networks to the hyperbolic domain.

5

3.1 Hyperbolic multiclass logistic regression

In order to perform multi-class classiﬁcation on the Poincaré ball, one needs to generalize multinomial
logistic regression (MLR) − also called softmax regression − to the Poincaré ball.

Reformulating Euclidean MLR. Let’s ﬁrst reformulate Euclidean MLR from the perspective of
distances to margin hyperplanes, as in [19, Section 5]. This will allow us to easily generalize it.

Given K classes, one learns a margin hyperplane for each such class using softmax probabilities:

∀k ∈ {1, ..., K},

p(y = k|x) ∝ exp (((cid:104)ak, x(cid:105) − bk)) , where bk ∈ R, x, ak ∈ Rn.

(17)

Note that any afﬁne hyperplane in Rn can be written with a normal vector a and a scalar shift b:

Ha,b = {x ∈ Rn : (cid:104)a, x(cid:105) − b = 0}, where a ∈ Rn \ {0}, and b ∈ R.

(18)

As in [19, Section 5], we note that (cid:104)a, x(cid:105) − b = sign((cid:104)a, x(cid:105) − b)(cid:107)a(cid:107)d(x, Ha,b). Using Eq. (17):

p(y = k|x) ∝ exp(sign((cid:104)ak, x(cid:105) − bk)(cid:107)ak(cid:107)d(x, Hak,bk )), bk ∈ R, x, ak ∈ Rn.

(19)

As it is not immediately obvious how to generalize the Euclidean hyperplane of Eq. (18) to other
spaces such as the Poincaré ball, we reformulate it as follows:

˜Ha,p = {x ∈ Rn : (cid:104)−p + x, a(cid:105) = 0} = p + {a}⊥, where p ∈ Rn, a ∈ Rn \ {0}.

(20)

This new deﬁnition relates to the previous one as ˜Ha,p = Ha,(cid:104)a,p(cid:105). Rewriting Eq. (19) with b = (cid:104)a, p(cid:105):
p(y = k|x) ∝ exp(sign((cid:104)−pk + x, ak(cid:105))(cid:107)ak(cid:107)d(x, ˜Hak,pk )), with pk, x, ak ∈ Rn.

(21)

It is now natural to adapt the previous deﬁnition to the hyperbolic setting by replacing + by ⊕c:
Deﬁnition 3.1 (Poincaré hyperplanes). For p ∈ Dn
p(z, a) = 0} = {z ∈ TpDn
gc

c : (cid:104)z, a(cid:105) = 0}. Then, we deﬁne Poincaré hyperplanes as

c \ {0}, let {a}⊥ := {z ∈ TpDn
c :

c , a ∈ TpDn

˜H c

a,p := {x ∈ Dn

c : (cid:104)logc

p(x), a(cid:105)p = 0} = expc

p({a}⊥) = {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}.

(22)

The last equality is shown appendix C. ˜H c
all geodesics in Dn
hypergyroplanes, see [27, deﬁnition 5.8]. A 3D hyperplane example is depicted in Fig. 1.

a,p can also be described as the union of images of
c orthogonal to a and containing p. Notice that our deﬁnition matches that of

Next, we need the following theorem, proved in appendix D:
Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(23)

Final formula for MLR in the Poincaré ball. Putting together Eq. (21) and Thm. 5, we get the
hyperbolic MLR formulation. Given K classes and k ∈ {1, . . . , K}, pk ∈ Dn
c \ {0}:

c , ak ∈ Tpk

Dn

p(y = k|x) ∝ exp(sign((cid:104)−pk ⊕c x, ak(cid:105))

gc
pk

(ak, ak)dc(x, ˜H c

)),

ak,pk

∀x ∈ Dn
c ,

(24)

or, equivalently

p(y = k|x) ∝ exp

(cid:18) λc
pk

(cid:107)ak(cid:107)
√
c

sinh−1

(cid:18)

√
2

c(cid:104)−pk ⊕c x, ak(cid:105)

(cid:19)(cid:19)

(1 − c(cid:107) − pk ⊕c x(cid:107)2)(cid:107)ak(cid:107)

,

∀x ∈ Dn
c .

(25)

this goes to p(y = k|x) ∝ exp(4(cid:104)−pk + x, ak(cid:105)) =

Notice that when c goes to zero,
exp((λ0
pk

)2(cid:104)−pk + x, ak(cid:105)) = exp((cid:104)−pk + x, ak(cid:105)0), recovering the usual Euclidean softmax.
However, at this point it is unclear how to perform optimization over ak, since it lives in Tpk
hence depends on pk. The solution is that one should write ak = P c
)a(cid:48)
k ∈ T0Dn
a(cid:48)

c = Rn, and optimize a(cid:48)

k as a Euclidean parameter.

k) = (λc

0/λc
pk

0→pk

(a(cid:48)

Dn
c and
k, where

1
√
c

(cid:113)

6

3.2 Hyperbolic feed-forward layers

In order to deﬁne hyperbolic neural networks, it is crucial to de-
ﬁne a canonically simple parametric family of transformations,
playing the role of linear mappings in usual Euclidean neural
networks, and to know how to apply pointwise non-linearities.
Inspiring ourselves from our reformulation of Möbius scalar
multiplication in Eq. (14), we deﬁne:
Deﬁnition 3.2 (Möbius version). For f : Rn → Rm, we deﬁne
the Möbius version of f as the map from Dn

c to Dm

c by:

f ⊗c(x) := expc

0(f (logc

0(x))),

(26)

where expc

0 : T0m

Dm

c → Dm

c and logc

0 : Dn

c → T0n

Dn
c .

Figure 1: An example of a hyper-
bolic hyperplane in D3
1 plotted us-
ing sampling. The red point is p.
The shown normal axis to the hy-
perplane through p is parallel to a.

Note that similarly as for other Möbius operations, we recover
the Euclidean mapping in the limit c → 0 if f is continuous, as limc→0 f ⊗c(x) = f (x). This
deﬁnition satisﬁes a few desirable properties too, such as: (f ◦ g)⊗c = f ⊗c ◦ g⊗c for f : Rm → Rl
and g : Rn → Rm (morphism property), and f ⊗c(x)/(cid:107)f ⊗c(x)(cid:107) = f (x)/(cid:107)f (x)(cid:107) for f (x) (cid:54)= 0
(direction preserving). It is then straight-forward to prove the following result:
Lemma 6 (Möbius matrix-vector multiplication). If M : Rn → Rm is a linear map, which we
identify with its matrix representation, then ∀x ∈ Dn

c , if M x (cid:54)= 0 we have

M ⊗c(x) = (1/

c) tanh

√

(cid:18) (cid:107)M x(cid:107)
(cid:107)x(cid:107)

√

tanh−1(

c(cid:107)x(cid:107))

(cid:19) M x
(cid:107)M x(cid:107)

,

(27)

and M ⊗c(x) = 0 if M x = 0. Moreover, if we deﬁne the Möbius matrix-vector multiplication of
M ∈ Mm,n(R) and x ∈ Dn
c by M ⊗c x := M ⊗c(x), then we have (M M (cid:48)) ⊗c x = M ⊗c (M (cid:48) ⊗c x)
for M ∈ Ml,m(R) and M (cid:48) ∈ Mm,n(R) (matrix associativity), (rM ) ⊗c x = r ⊗c (M ⊗c x) for
r ∈ R and M ∈ Mm,n(R) (scalar-matrix associativity) and M ⊗c x = M x for all M ∈ On(R)
(rotations are preserved).

Pointwise non-linearity.
ϕ⊗c can be applied to elements of the Poincaré ball.

If ϕ : Rn → Rn is a pointwise non-linearity, then its Möbius version

Bias translation. The generalization of a translation in the Poincaré ball is naturally given by
moving along geodesics. But should we use the Möbius sum x ⊕c b with a hyperbolic bias b or the
x(b(cid:48)) with a Euclidean bias b(cid:48)? These views are uniﬁed with parallel transport
exponential map expc
c by a bias b ∈ Dn
(see Thm 4). Möbius translation of a point x ∈ Dn
(cid:18) λc
0
λc
x

c is given by
(cid:19)

x ← x ⊕c b = expc

0(b))) = expc
x

0→x(logc

x(P c

logc

0(b)

(28)

.

We recover Euclidean translations in the limit c → 0. Note that bias translations play a particular
Indeed, consider multiple layers of the form fk(x) = ϕk(Mkx), each of
role in this model.
which having Möbius version f ⊗c
k (Mk ⊗c x). Then their composition can be re-written
f ⊗c
k ◦ · · · ◦ f ⊗c
1 = expc
0. This means that these operations can essentially be
performed in Euclidean space. Therefore, it is the interposition between those with the bias translation
of Eq. (28) which differentiates this model from its Euclidean counterpart.

k (x) = ϕ⊗c
0 ◦fk ◦ · · · ◦ f1 ◦ logc

If a vector x ∈ Rn+p is the (vertical) concatenation
Concatenation of multiple input vectors.
of two vectors x1 ∈ Rn, x2 ∈ Rp, and M ∈ Mm,n+p(R) can be written as the (horizontal)
concatenation of two matrices M1 ∈ Mm,n(R) and M2 ∈ Mm,p(R), then M x = M1x1 + M2x2.
We generalize this to hyperbolic spaces: if we are given x1 ∈ Dn
c ×Dp
c ,
and M, M1, M2 as before, then we deﬁne M ⊗c x := M1 ⊗c x1 ⊕c M2 ⊗c x2. Note that when c goes
to zero, we recover the Euclidean formulation, as limc→0 M ⊗c x = limc→0 M1 ⊗c x1 ⊕c M2 ⊗c x2 =
M1x1 + M2x2 = M x. Moreover, hyperbolic vectors x ∈ Dn
c can also be "concatenated" with real
features y ∈ R by doing: M ⊗c x ⊕c y ⊗c b with learnable b ∈ Dm

c , x = (x1 x2)T ∈ Dn

c and M ∈ Mm,n(R).

c , x2 ∈ Dp

7

3.3 Hyperbolic RNN

Naive RNN. A simple RNN can be deﬁned by ht+1 = ϕ(W ht + U xt + b) where ϕ is a pointwise
non-linearity, typically tanh, sigmoid, ReLU, etc. This formula can be naturally generalized to the
hyperbolic space as follows. For parameters W ∈ Mm,n(R), U ∈ Mm,d(R), b ∈ Dm
c , we deﬁne:

ht+1 = ϕ⊗c (W ⊗c ht ⊕c U ⊗c xt ⊕c b),

ht ∈ Dn

c , xt ∈ Dd
c .

(29)

Note that if inputs xt’s are Euclidean, one can write ˜xt := expc
expc

(U xt)) = W ⊗c ht ⊕c expc

(P c

W ⊗cht

0→W ⊗cht

0(U xt) = W ⊗c ht ⊕c U ⊗c ˜xt.

0(xt) and use the above formula, since

GRU architecture. One can also adapt the GRU architecture:
rt = σ(W rht−1 + U rxt + br),
zt = σ(W zht−1 + U zxt + bz),
˜ht = ϕ(W (rt (cid:12) ht−1) + U xt + b), ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht,

(30)

where (cid:12) denotes pointwise product. First, how should we adapt the pointwise multiplication by a
scaling gate? Note that the deﬁnition of the Möbius version (see Eq. (26)) can be naturally extended
to maps f : Rn × Rp → Rm as f ⊗c : (h, h(cid:48)) ∈ Dn
0(h(cid:48)))). In
c (cid:55)→ expc
0(h(cid:48))) =
particular, choosing f (h, h(cid:48)) := σ(h) (cid:12) h(cid:48) yields6 f ⊗c(h, h(cid:48)) = expc
diag(σ(logc

0(h))) ⊗c h(cid:48). Hence we adapt rt (cid:12) ht−1 to diag(rt) ⊗c ht−1 and the reset gate rt to:
0(W r ⊗c ht−1 ⊕c U r ⊗c xt ⊕c br),

0(h), logc
0(h)) (cid:12) logc

0(f (logc
0(σ(logc

rt = σ logc

c × Dp

(31)

and similarly for the update gate zt. Note that as the argument of σ in the above is unbounded, rt and
zt can a priori take values onto the full range (0, 1). Now the intermediate hidden state becomes:
˜ht = ϕ⊗c ((W diag(rt)) ⊗c ht−1 ⊕c U ⊗c xt ⊕ b),

(32)

where Möbius matrix associativity simpliﬁes W ⊗c (diag(rt) ⊗c ht−1) into (W diag(rt)) ⊗c ht−1.
Finally, we propose to adapt the update-gate equation as

ht = ht−1 ⊕c diag(zt) ⊗c (−ht−1 ⊕c

˜ht).

(33)

Note that when c goes to zero, one recovers the usual GRU. Moreover, if zt = 0 or zt = 1, then ht
becomes ht−1 or ˜ht respectively, similarly as in the usual GRU. This adaptation was obtained by
adapting [24]: in this work, the authors re-derive the update-gate mechanism from a ﬁrst principle
called time-warping invariance. We adapted their derivation to the hyperbolic setting by using the
notion of gyroderivative [4] and proving a gyro-chain-rule (see appendix E).

4 Experiments

SNLI task and dataset. We evaluate our method on two tasks. The ﬁrst is natural language
inference, or textual entailment. Given two sentences, a premise (e.g. "Little kids A. and B. are
playing soccer.") and a hypothesis (e.g. "Two children are playing outdoors."), the binary classiﬁcation
task is to predict whether the second sentence can be inferred from the ﬁrst one. This deﬁnes a partial
order in the sentence space. We test hyperbolic networks on the biggest real dataset for this task,
SNLI [7]. It consists of 570K training, 10K validation and 10K test sentence pairs. Following [28],
we merge the "contradiction" and "neutral" classes into a single class of negative sentence pairs, while
the "entailment" class gives the positive pairs.

PREFIX task and datasets. We conjecture that the improvements of hyperbolic neural networks
are more signiﬁcant when the underlying data structure is closer to a tree. To test this, we design a
proof-of-concept task of detection of noisy preﬁxes, i.e. given two sentences, one has to decide if the
second sentence is a noisy preﬁx of the ﬁrst, or a random sentence. We thus build synthetic datasets
PREFIX-Z% (for Z being 10, 30 or 50) as follows: for each random ﬁrst sentence of random length
at most 20 and one random preﬁx of it, a second positive sentence is generated by randomly replacing
Z% of the words of the preﬁx, and a second negative sentence of same length is randomly generated.
Word vocabulary size is 100, and we generate 500K training, 10K validation and 10K test pairs.

6If x has n coordinates, then diag(x) denotes the diagonal matrix of size n with xi’s on its diagonal.

8

Models architecture. Our neural network layers can be used in a plug-n-play manner exactly like
standard Euclidean layers. They can also be combined with Euclidean layers. However, optimization
w.r.t. hyperbolic parameters is different (see below) and based on Riemannian gradients which
are just rescaled Euclidean gradients when working in the conformal Poincaré model [21]. Thus,
back-propagation can be applied in the standard way.

In our setting, we embed the two sentences using two distinct hyperbolic RNNs or GRUs. The
sentence embeddings are then fed together with their squared distance (hyperbolic or Euclidean,
depending on their geometry) to a FFNN (Euclidean or hyperbolic, see Sec. 3.2) which is further
fed to an MLR (Euclidean or hyperbolic, see Sec. 3.1) that gives probabilities of the two classes
(entailment vs neutral). We use cross-entropy loss on top. Note that hyperbolic and Euclidean layers
can be mixed, e.g. the full network can be hyperbolic and only the last layer be Euclidean, in which
case one has to use log0 and exp0 functions to move between the two manifolds in a correct manner
as explained for Eq. 26.

Optimization. Our models have both Euclidean (e.g. weight matrices in both Euclidean and
hyperbolic FFNNs, RNNs or GRUs) and hyperbolic parameters (e.g. word embeddings or biases for
the hyperbolic layers). We optimize the Euclidean parameters with Adam [16] (learning rate 0.001).
Hyperbolic parameters cannot be updated with an equivalent method that keeps track of gradient
history due to the absence of a Riemannian Adam. Thus, they are optimized using full Riemannian
stochastic gradient descent (RSGD) [5, 11]. We also experiment with projected RSGD [21], but
optimization was sometimes less stable. We use a different constant learning rate for word embeddings
(0.1) and other hyperbolic weights (0.01) because words are updated less frequently.

Numerical errors. Gradients of the basic operations deﬁned above (e.g. ⊕c, exponential map) are
c(cid:107)x(cid:107) = 1. Thus, we
not deﬁned when the hyperbolic argument vectors are on the ball border, i.e.
always project results of these operations in the ball of radius 1 − (cid:15), where (cid:15) = 10−5. Numerical
errors also appear when hyperbolic vectors get closer to 0, thus we perturb them with an (cid:15)(cid:48) = 10−15
before they are used in any of the above operations. Finally, arguments of the tanh function are
clipped between ±15 to avoid numerical errors, while arguments of tanh−1 are clipped to at most
1 − 10−5.

√

Hyperparameters. For all methods, baselines and datasets, we use c = 1, word and hidden state
embedding dimension of 5 (we focus on the low dimensional setting that was shown to already
be effective [21]), batch size of 64. We ran all methods for a ﬁxed number of 30 epochs. For all
models, we experiment with both identity (no non-linearity) or tanh non-linearity in the RNN/GRU
cell, as well as identity or ReLU after the FFNN layer and before MLR. As expected, for the fully
Euclidean models, tanh and ReLU respectively surpassed the identity variant by a large margin. We
only report the best Euclidean results. Interestingly, for the hyperbolic models, using only identity for
both non-linearities works slightly better and this is likely due to two facts: i) our hyperbolic layers
already contain non-linearities by their nature, ii) tanh is limiting the output domain of the sentence
embeddings, but the hyperbolic speciﬁc geometry is more pronounced at the ball border, i.e. at the
hyperbolic "inﬁnity", compared to the center of the ball.

For the results shown in Tab. 1, we run each model (baseline or ours) exactly 3 times and report the
test result corresponding to the best validation result from these 3 runs. We do this because the highly
non-convex spectrum of hyperbolic neural networks sometimes results in convergence to poor local
minima, suggesting that initialization is very important.

Results. Results are shown in Tab. 1. Note that the fully Euclidean baseline models might have
an advantage over hyperbolic baselines because more sophisticated optimization algorithms such
as Adam do not have a hyperbolic analogue at the moment. We ﬁrst observe that all GRU models
overpass their RNN variants. Hyperbolic RNNs and GRUs have the most signiﬁcant improvement
over their Euclidean variants when the underlying data structure is more tree-like, e.g. for PREFIX-
10% − for which the tree relation between sentences and their preﬁxes is more prominent − we
reduce the error by a factor of 3.35 for hyperbolic vs Euclidean RNN, and by a factor of 1.5 for
hyperbolic vs Euclidean GRU. As soon as the underlying structure diverges more and more from
a tree, the accuracy gap decreases − for example, for PREFIX-50% the noise heavily affects the
representational power of hyperbolic networks. Also, note that on SNLI our methods perform
similarly as with their Euclidean variants. Moreover, hyperbolic and Euclidean MLR are on par when

9

SNLI

PREFIX-10% PREFIX-30% PREFIX-50%

FULLY EUCLIDEAN RNN
HYPERBOLIC RNN+FFNN, EUCL MLR
FULLY HYPERBOLIC RNN
FULLY EUCLIDEAN GRU
HYPERBOLIC GRU+FFNN, EUCL MLR
FULLY HYPERBOLIC GRU

79.34 %
79.18 %
78.21 %
81.52 %
79.76 %
81.19 %

89.62 %
96.36 %
96.91 %
95.96 %
97.36 %
97.14 %

81.71 %
87.83 %
87.25 %
86.47 %
88.47 %
88.26 %

72.10 %
76.50 %
62.94 %
75.04 %
76.87 %
76.44 %

Table 1: Test accuracies for various models and four datasets. "Eucl" denotes Euclidean. All word
and sentence embeddings have dimension 5. We highlight in bold the best baseline (or baselines, if
the difference is less than 0.5%).

used in conjunction with hyperbolic sentence embeddings, suggesting further empirical investigation
is needed for this direction (see below).

We also observe that, in the hyperbolic setting, accuracy tends to increase when sentence embeddings
start increasing, and gets better as their norms converge towards 1 (the ball border for c = 1). Unlike
in the Euclidean case, this behavior does happen only after a few epochs and suggests that the model
should ﬁrst adjust the angular layout in order to disentangle the representations, before increasing their
norms to fully exploit the strong clustering property of the hyperbolic geometry. Similar behavior
was observed in the context of embedding trees by [21]. Details in appendix F.

MLR classiﬁcation experiments.
For the sentence entailment classi-
ﬁcation task we do not see a clear
advantage of hyperbolic MLR com-
pared to its Euclidean variant. A pos-
sible reason is that, when trained end-
to-end, the model might decide to
place positive and negative embed-
dings in a manner that is already well
separated with a classic MLR. As a
consequence, we further investigate
MLR for the task of subtree classiﬁ-
cation. Using an open source imple-
mentation7 of [21], we pre-trained
Poincaré embeddings of the Word-
Net noun hierarchy (82,115 nodes).
We then choose one node in this tree
(see Table 2) and classify all other
nodes (solely based on their embed-
dings) as being part of the subtree
rooted at this node. All nodes in such a subtree are divided into positive training nodes (80%) and
positive test nodes (20%). The same splitting procedure is applied for the remaining WordNet nodes
that are divided into a negative training and negative test set respectively. Three variants of MLR
are then trained on top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation
task: hyperbolic MLR, Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean
MLR applied after mapping all embeddings in the tangent space at 0 using the log0 map. We use
different embedding dimensions : 2, 3, 5 and 10. For the hyperbolic MLR, we use full Riemannian
SGD with a learning rate of 0.001. For the two Euclidean models we use ADAM optimizer and the
same learning rate. During training, we always sample the same number of negative and positive
nodes in each minibatch of size 16; thus positive nodes are frequently resampled. All methods are
trained for 30 epochs and the ﬁnal F1 score is reported (no hyperparameters to validate are used, thus
we do not require a validation set). This procedure is repeated for four subtrees of different sizes.

Figure 2: Hyperbolic (left) vs Direct Euclidean (right) binary
MLR used to classify nodes as being part in the GROUP.N.01
subtree of the WordNet noun hierarchy solely based on their
Poincaré embeddings. The positive points (from the subtree)
are in blue, the negative points (the rest) are in red and the
trained positive separation hyperplane is depicted in green.

Quantitative results are presented in Table 2. We can see that the hyperbolic MLR overpasses
its Euclidean variants in almost all settings, sometimes by a large margin. Moreover, to provide

7https://github.com/dalab/hyperbolic_cones

10

WORDNET
SUBTREE

ANIMAL.N.01
3218 / 798

GROUP.N.01
6649 / 1727

WORKER.N.01
861 / 254

MAMMAL.N.01
953 / 228

MODEL

D = 2

D = 3

D = 5

D = 10

HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL

47.43 ± 1.07%
41.69 ± 0.19%
38.89 ± 0.01%
81.72 ± 0.17%
61.13 ± 0.42%
60.75 ± 0.24%
12.68 ± 0.82%
10.86 ± 0.01%
9.04 ± 0.06%
32.01 ± 17.14%
15.58 ± 0.04%
13.10 ± 0.13%

91.92 ± 0.61%
68.43 ± 3.90%
62.57 ± 0.61%
89.87 ± 2.73%
63.56 ± 1.22%
61.98 ± 0.57%
24.09 ± 1.49%
22.39 ± 0.04%
22.57 ± 0.20%
87.54 ± 4.55%
44.68 ± 1.87%
44.89 ± 1.18%

98.07 ± 0.55%
95.59 ± 1.18%
89.21 ± 1.34%
87.89 ± 0.80%
67.82 ± 0.81%
67.92 ± 0.74%
55.46 ± 5.49%
35.23 ± 3.16%
26.47 ± 0.78%
88.73 ± 3.22%
59.35 ± 1.31%
52.51 ± 0.85%

99.26 ± 0.59%
99.36 ± 0.18%
98.27 ± 0.70%
91.91 ± 3.07%
91.38 ± 1.19%
91.41 ± 0.18%
66.83 ± 11.38%
47.29 ± 3.93%
36.66 ± 2.74%
91.37 ± 6.09%
77.76 ± 5.08%
56.11 ± 2.21%

Table 2: Test F1 classiﬁcation scores for four different subtrees of WordNet noun tree. All nodes
in such a subtree are divided into positive training nodes (80%) and positive test nodes (20%);
these counts are shown below each subtree root. The same splitting procedure is applied for the
remaining nodes to obtain negative training and test sets. Three variants of MLR are then trained on
top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation task: hyperbolic MLR,
Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean MLR applied after
mapping all embeddings in the tangent space at 0 using the log0 map. 95% conﬁdence intervals for 3
different runs are shown for each method and each different embedding dimension (2, 3, 5 or 10).

further understanding, we plot the 2-dimensional embeddings and the trained separation hyperplanes
(geodesics in this case) in Figure 2. We can see that respecting the hyperbolic geometry is very
important for a quality classiﬁcation model.

5 Conclusion

We showed how classic Euclidean deep learning tools such as MLR, FFNNs, RNNs or GRUs can be
generalized in a principled manner to all spaces of constant negative curvature combining Riemannian
geometry with the elegant theory of gyrovector spaces. Empirically we found that our models
outperform or are on par with corresponding Euclidean architectures on sequential data with implicit
hierarchical structure. We hope to trigger exciting future research related to better understanding
of the hyperbolic non-convexity spectrum and development of other non-Euclidean deep learning
methods.
Our data and Tensorﬂow [1] code are publicly available8.

Acknowledgements

We thank Igor Petrovski for useful pointers regarding the implementation.

This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement
number 167176. Gary Bécigneul is also funded by the Max Planck ETH Center for Learning
Systems.

References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for
large-scale machine learning. 2016.

[2] Ungar Abraham Albert. Analytic hyperbolic geometry and Albert Einstein’s special theory of

relativity. World scientiﬁc, 2008.

8https://github.com/dalab/hyperbolic_nn

11

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of the International Conference on Learning
Representations (ICLR), 2015.

[4] Graciela S Birman and Abraham A Ungar. The hyperbolic derivative in the poincaré ball model
of hyperbolic geometry. Journal of mathematical analysis and applications, 254(1):321–333,
2001.

[5] S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on

Automatic Control, 58(9):2217–2229, Sept 2013.

[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems (NIPS), pages 2787–2795, 2013.

[7] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 632–642. Association
for Computational Linguistics, 2015.

[8] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,
34(4):18–42, 2017.

[9] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry.

Flavors of geometry, 31:59–115, 1997.

[10] Christopher De Sa, Albert Gu, Christopher Ré, and Frederic Sala. Representation tradeoffs for

hyperbolic embeddings. arXiv preprint arXiv:1804.03329, 2018.

[11] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In Proceedings of the thirty-ﬁfth international conference
on machine learning (ICML), 2018.

[12] Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pages 75–263. Springer, 1987.

[13] Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the

Cambridge Philosophical Society, page 1–17, 2017.

[14] Christopher Hopper and Ben Andrews. The Ricci ﬂow in Riemannian geometry. Springer, 2010.

[15] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1746–1751. Association for Computational Linguistics, 2014.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations (ICLR), 2015.

[17] Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marián Boguná.

Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.

[18] John Lamping, Ramana Rao, and Peter Pirolli. A focus+ context technique based on hyperbolic
geometry for visualizing large hierarchies. In Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 401–408. ACM Press/Addison-Wesley Publishing Co.,
1995.

[19] Guy Lebanon and John Lafferty. Hyperplane margin classiﬁers on the multinomial manifold. In
Proceedings of the international conference on machine learning (ICML), page 66. ACM, 2004.

[20] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective
learning on multi-relational data. In Proceedings of the international conference on machine
learning (ICML), volume 11, pages 809–816, 2011.

12

[21] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical repre-
sentations. In Advances in Neural Information Processing Systems (NIPS), pages 6341–6350,
2017.

[22] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blun-
som. Reasoning about entailment with neural attention. In Proceedings of the International
Conference on Learning Representations (ICLR), 2015.

[23] Michael Spivak. A comprehensive introduction to differential geometry. Publish or perish, 1979.

[24] Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In Proceedings of

the International Conference on Learning Representations (ICLR), 2018.

[25] Abraham A Ungar. Hyperbolic trigonometry and its application in the poincaré ball model of

hyperbolic geometry. Computers & Mathematics with Applications, 41(1-2):135–147, 2001.

[26] Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis

Lectures on Mathematics and Statistics, 1(1):1–194, 2008.

[27] Abraham Albert Ungar. Analytic hyperbolic geometry in n dimensions: An introduction. CRC

Press, 2014.

[28] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. In Proceedings of the International Conference on Learning Representations (ICLR),
2016.

[29] J Vermeer. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic

plane. Topology and its Applications, 152(3):226–242, 2005.

13

A Hyperbolic Trigonometry

Hyperbolic angles. For A, B, C ∈ Dn
c , we denote by ∠A := ∠BAC the angle between the two
geodesics starting from A and ending at B and C respectively. This angle can be deﬁned in two
equivalent ways: i) either using the angle between the initial velocities of the two geodesics as given
by Eq. 5, or ii) using the formula

cos(∠A) =

(cid:28) (−A) ⊕c B
(cid:107)(−A) ⊕c B(cid:107)

,

(−A) ⊕c C
(cid:107)(−A) ⊕c C(cid:107)

(cid:29)

,

In this case, ∠A is also called a gyroangle in the work of [26, section 4].

Hyperbolic law of sines. We state here the hyperbolic law of sines. If for A, B, C ∈ Dn
c , we
denote by ∠B := ∠ABC the angle between the two geodesics starting from B and ending at A and
C respectively, and by ˜c = dc(B, A) the length of the hyperbolic segment BA (and similarly for
others), then we have:

sin(∠A)
√
c˜a)
sinh(

=

sin(∠B)
√
c˜b)
sinh(

=

sin(∠C)
√
c˜c)
sinh(

.

Note that one can also adapt the hyperbolic law of cosines to the hyperbolic space.

B Proof of Theorem 4

Theorem 4.
In the manifold (Dn
to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a vector v ∈ T0Dn
c

c is given by the following isometry:
λc
0
λc
x

0→x(v) = logc
P c

x(x ⊕c expc

0(v)) =

v.

Proof. The geodesic in Dn
v ∈ T0Dn
γ (i.e. X(t) ∈ Tγ(t)Dn

c from 0 to x is given in Eq. (10) by γ(t) = x ⊗c t, for t ∈ [0, 1]. Let
c . Then it is of common knowledge that there exists a unique parallel9 vector ﬁeld X along

c , ∀t ∈ [0, 1]) such that X(0) = v. Let’s deﬁne:
X : t ∈ [0, 1] (cid:55)→ logc

γ(t)(γ(t) ⊕c expc

0(v)) ∈ Tγ(t)Dn
c .

Clearly, X is a vector ﬁeld along γ such that X(0) = v. Now deﬁne
0→x : v ∈ T0Dn
P c

x(x ⊕c expc

0(v)) ∈ TxDn
c .

c (cid:55)→ logc
0→x(v) = λc

c . Since P c

0→x is a linear isometry from T0Dn
v, hence P c
c
0→x(v) = X(1), it is enough to prove that X is parallel in order to guarantee that

From Eq. (12), it is easily seen that P c
to TxDn
c to TxDn
0→x is the parallel transport from T0Dn
P c
c .
Since X is a vector ﬁeld along γ, its covariant derivative can be expressed with the Levi-Civita
connection ∇c associated to gc:

0
λc
x

DX
∂t

= ∇c

˙γ(t)X.

Let’s compute the Levi-Civita connection from its Christoffel symbols. In a local coordinate system,
they can be written as

Γi

jk =

(gc)il(∂jgc

lk + ∂kgc

lj − ∂lgc

jk),

1
2

where superscripts denote the inverse metric tensor and using Einstein’s notations. As gc
at γ(t) ∈ Dn

c this yields:

ij = (λc)2δij,

jk = cλc
Γi

γ(t)(δikγ(t)j + δijγ(t)k − δjkγ(t)i).

9i.e. that DX

∂t = 0 for t ∈ [0, 1], where D

∂t denotes the covariant derivative.

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

14

On the other hand, since X(t) = (λc

∇c

˙γ(t)X = ˙γ(t)i∇c

i X = ˙γ(t)i∇c
i

= vj ˙γ(t)i∇c
i

0/λc

γ(t))v, we have
(cid:32)

(cid:33)

λc
0
λc

γ(t)

v

(cid:32)

λc
0
λc

γ(t)

(cid:33)

ej

.

√

√

Since γ(t) = (1/
Hence there exists K x

c) tanh(t tanh−1(
t ∈ R such that ˙γ(t) = K x

c(cid:107)x(cid:107))) x

(cid:107)x(cid:107) , it is easily seen that ˙γ(t) is colinear to γ(t).
t γ(t). Moreover, we have the following Leibniz rule:
(cid:33)

(cid:32)

(cid:32)

∇c
i

λc
0
λc

γ(t)

(cid:33)

ej

=

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

ej.

Combining these yields

DX
∂t

= K x

t vjγ(t)i

(cid:32)

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

(cid:32)

(cid:33)

(cid:33)

ej

.

Replacing with the Christoffel symbols of ∇c at γ(t) gives

Moreover,

λc
0
λc

γ(t)

λc
0
λc

γ(t)

∇c

i ej =

ijek = 2c[δk
Γk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek.

∂
∂γ(t)i

(cid:32)

(cid:33)

λc
0
λc

γ(t)

ej =

∂
∂γ(t)i

(cid:0)−c(cid:107)γ(t)(cid:107)2(cid:1) ej = −2cγ(t)iej.

Putting together everything, we obtain

DX
∂t

= K x

t vjγ(t)i (cid:0)2c[δk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek − 2cγ(t)iej

(cid:1)

t vjγ(t)i (cid:0)γ(t)jei − δijγ(t)kek
t vj (cid:0)γ(t)jγ(t)iei − γ(t)iδijγ(t)kek
(cid:1)
t vj (cid:0)γ(t)jγ(t)iei − γ(t)jγ(t)kek

(cid:1)

(cid:1)

= 2cK x
= 2cK x
= 2cK x
= 0,

which concludes the proof.

C Proof of Eq. (22)

Proof. Two steps proof:
i) expc

p({a}⊥) ⊆ {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}:

Let z ∈ {a}⊥. From Eq. (12), we have that:

This, together with the left-cancellation law in gyrospaces (see section 2.3), implies that

expc

p(z) = −p ⊕c βz,

for some β ∈ R.

(cid:104)−p ⊕c expc

p(z), a(cid:105) = (cid:104)βz, a(cid:105) = 0

which is what we wanted.

ii) {x ∈ Dn
Let x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0} ⊆ expc
c s.t. (cid:104)−p ⊕c x, a(cid:105) = 0. Then, using Eq. (12), we derive that:
for some β ∈ R,

p(x) = β(−p ⊕c x),

p({a}⊥):

logc

which is orthogonal to a, by assumption. This implies logc

p(x) ∈ {a}⊥, hence x ∈ expc

p({a}⊥).

15

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(54)

D Proof of Theorem 5

Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

1
√
c

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(55)

Proof. We ﬁrst need to prove the following lemma, trivial in the Euclidean space, but not in the
Poincaré ball:
Lemma 7. (Orthogonal projection on a geodesic) Any point in the Poincaré ball has a unique
orthogonal projection on any given geodesic that does not pass through the point. Formally, for all
y ∈ Dn
c and for all geodesics γx→z(·) s.t. y /∈ Im γx→z, there exists an unique w ∈ Im γx→z s.t.
∠(γw→y, γx→z) = π/2.

Proof. We ﬁrst note that any geodesic in Dn
and has two "points at inﬁnity" lying on the ball border (v (cid:54)= 0):

c has the form γ(t) = u ⊕c v ⊗c t as given by Eq. 11,

γ(±∞) = u ⊕c

√

±v
c(cid:107)v(cid:107)

∈ ∂Dn
c .

(56)

Using the notations in the lemma statement, the closed-form of γx→z is given by Eq. (10):

γx→z(t) = x ⊕c (−x ⊕c z) ⊗c t

We denote by x(cid:48), z(cid:48) ∈ ∂Dn
∠ywx(cid:48) is well deﬁned from Eq. (34):

c its points at inﬁnity as described by Eq. (56). Then, the hyperbolic angle

cos(∠(γw→y, γx→z)) = cos(∠ywz(cid:48)) =

(cid:104)−w ⊕c y, −w ⊕c z(cid:48)(cid:105)
(cid:107) − w ⊕c y(cid:107) · (cid:107) − w ⊕c z(cid:48)(cid:107)

.

(57)

We now perform 2 steps for this proof.

i) Existence of w:

The angle function from Eq. (57) is continuous w.r.t t when w = γx→z(t). So we ﬁrst prove existence
of an angle of π/2 by continuously moving w from x(cid:48) to z(cid:48) when t goes from −∞ to ∞, and
observing that cos(∠ywz(cid:48)) goes from −1 to 1 as follows:

cos(∠yx(cid:48)z(cid:48)) = 1 & lim
w→z(cid:48)

cos(∠ywz(cid:48)) = −1.

(58)

The left part of Eq. (58) follows from Eq. (57) and from the fact (easy to show from the deﬁnition
c (which is the case of x(cid:48)). The right part of Eq. (58)
of ⊕c) that a ⊕c b = a, when (cid:107)a(cid:107) = 1/
follows from the fact that ∠ywz(cid:48) = π − ∠ywx(cid:48) (from the conformal property, or from Eq. (34)) and
cos(∠yz(cid:48)x(cid:48)) = 1 (proved as above).
Hence cos(∠ywz(cid:48)) has to pass through 0 when going from −1 to 1, which achieves the proof of
existence.

√

ii) Uniqueness of w:
Assume by contradiction that there are two w and w(cid:48) on γx→z that form angles ∠ywx(cid:48) and ∠yw(cid:48)x(cid:48)
of π/2. Since w, w(cid:48), x(cid:48) are on the same geodesic, we have

π/2 = ∠yw(cid:48)x(cid:48) = ∠yw(cid:48)w = ∠ywx(cid:48) = ∠yw(cid:48)w
So ∆yww(cid:48) has two right angles, but in the Poincaré ball this is impossible.

(59)

Now, we need two more lemmas:
Lemma 8. (Minimizing distance from point to geodesic) The orthogonal projection of a point to
a geodesic (not passing through the point) is minimizing the distance between the point and the
geodesic.

Proof. The proof is similar with the Euclidean case and it’s based on hyperbolic sine law and the fact
that in any right hyperbolic triangle the hypotenuse is strictly longer than any of the other sides.

16

Lemma 9. (Geodesics through p) Let ˜H c
all points on the geodesic γp→w are included in ˜H c

a,p.

a,p be a Poincaré hyperplane. Then, for any w ∈ ˜H c

a,p \ {p},

Proof. γp→w(t) = p ⊕c (−p ⊕c w) ⊗c t. Then, it is easy to check the condition in Eq. (22):

(cid:104)−p ⊕c γp→w(t), a(cid:105) = (cid:104)(−p ⊕c w) ⊗c t, a(cid:105) ∝ (cid:104)−p ⊕c w, a(cid:105) = 0.

(60)

We now turn back to our proof. Let x ∈ Dn
We prove that there is at least one point w∗ ∈ ˜H c

c be an arbitrary point and ˜H c

a,p a Poincaré hyperplane.

a,p that achieves the inﬁmum distance

dc(x, w∗) = inf
w∈ ˜H c

a,p

dc(x, w),

and, moreover, that this distance is the same as the one in the theorem’s statement.
We ﬁrst note that for any point w ∈ ˜H c
and Lemma 9, it is obvious that the projection of x to γp→w will give a strictly lower distance.
Thus, we only consider w ∈ ˜H c
triangle ∆xwp, one gets:

a,p such that ∠xwp = π/2. Applying hyperbolic sine law in the right

a,p, if ∠xwp (cid:54)= π/2, then w (cid:54)= w∗. Indeed, using Lemma 8

dc(x, w) = (1/

c) sinh−1 (cid:0)sinh(

c dc(x, p)) · sin(∠xpw)(cid:1) .

√

√

One of the above quantities does not depend on w:

√

√

sinh(

c dc(x, p)) = sinh(2 tanh−1(

c(cid:107) − p ⊕c x(cid:107))) =

√
2
c(cid:107) − p ⊕c x(cid:107)
1 − c(cid:107) − p ⊕c x(cid:107)2 .

The other quantity is sin(∠xpw) which is minimized when the angle ∠xpw is minimized (be-
cause ∠xpw < π/2 for the hyperbolic right triangle ∆xwp), or, alternatively, when cos(∠xpw) is
maximized. But, we already have from Eq. (34) that:

cos(∠xpw) =

(cid:104)−p ⊕c x, −p ⊕c w(cid:105)
(cid:107) − p ⊕c x(cid:107) · (cid:107) − p ⊕c w(cid:107)

.

To maximize the above, the constraint on the right angle at w can be dropped because cos(∠xpw)
depends only on the geodesic γp→w and not on w itself, and because there is always an orthogonal
projection from any point x to any geodesic as stated by Lemma 7. Thus, it remains to ﬁnd the
maximum of Eq. (64) when w ∈ ˜H c
a,p from Eq. (22), one can easily
prove that

a,p. Using the deﬁnition of ˜H c

Using that fact that logc

p(w)/(cid:107) logc

p(w)(cid:107) = −p ⊕c w/(cid:107) − p ⊕c w(cid:107), we just have to ﬁnd

and we are left with a well known Euclidean problem which is equivalent to ﬁnding the minimum
angle between the vector −p ⊕c x (viewed as Euclidean) and the hyperplane {a}⊥. This angle
is given by the Euclidean orthogonal projection whose sin value is the distance from the vector’s
endpoint to the hyperplane divided by the vector’s length:

{logc

p(w) : w ∈ ˜H c

a,p} = {a}⊥.

max
z∈{a}⊥

(cid:18) (cid:104)−p ⊕c x, z(cid:105)

(cid:107) − p ⊕c x(cid:107) · (cid:107)z(cid:107)

(cid:19)

,

sin(∠xpw∗) =

|(cid:104)−p ⊕c x, a
(cid:107) − p ⊕c x(cid:107)

(cid:107)a(cid:107) (cid:105)|

.

17

It follows that a point w∗ ∈ ˜H c
Eqs. (61),(62),(63) and (67) concludes the proof.

a,p satisfying Eq. (67) exists (but might not be unique). Combining

(61)

(62)

(63)

(64)

(65)

(66)

(67)

(cid:3)

E Derivation of the Hyperbolic GRU Update-gate

In [24], the authors recover the update/forget-gate mechanism of a GRU/LSTM by requiring that the
class of neural networks given by the chosen architecture be invariant to time-warpings. The idea is
the following.

Recovering the update-gate from time-warping. A naive RNN is given by the equation

h(t + 1) = ϕ(W h(t) + U x(t) + b)

Let’s drop the bias b to simplify notations. If h is seen as a differentiable function of time, then a
ﬁrst-order Taylor development gives h(t + δt) ≈ h(t) + δt dh
dt (t) for small δt. Combining this for
δt = 1 with the naive RNN equation, one gets

dh
dt

dα
dt

(t) = ϕ(W h(t) + U x(t)) − h(t).

As this is written for any t, one can replace it by t ← α(t) where α is a (smooth) increasing function
of t called the time-warping. Denoting by ˜h(t) := h(α(t)) and ˜x(t) := x(α(t)), using the chain rule
d˜h
dt (t) = dα

dt (α(t)), one gets

dt (t) dh

d˜h
dt

dα
dt

(t) =

(t)ϕ(W ˜h(t) + U ˜x(t)) −

(t)˜h(t).

(70)

Removing the tildas to simplify notations, discretizing back with dh

dt (t) ≈ h(t + 1) − h(t) yields

h(t + 1) =

(t)ϕ(W h(t) + U x(t)) +

1 −

(t)

h(t).

(71)

dα
dt

(cid:18)

(cid:19)

dα
dt

Requiring that our class of neural networks be invariant to time-warpings means that this class should
contain RNNs deﬁned by Eq. (71), i.e. that dα
dt (t) can be learned. As this is a positive quantity, we
can parametrize it as z(t) = σ(W zh(t) + U zx(t)), recovering the forget-gate equation:

h(t + 1) = z(t)ϕ(W h(t) + U x(t)) + (1 − z(t))h(t).

Adapting this idea to hyperbolic RNNs. The gyroderivative [4] of a map h : R → Dn
as

c is deﬁned

dh
dt

(t) = lim
δt→0

1
δt

⊗c (−h(t) ⊕c h(t + δt)).

Using Möbius scalar associativity and the left-cancellation law leads us to

h(t + δt) ≈ h(t) ⊕c δt ⊗c

(t),

dh
dt

for small δt. Combining this with the equation of a simple hyperbolic RNN of Eq. (29) with δt = 1,
one gets

dh
dt

(t) = −h(t) ⊕c ϕ⊗c(W ⊗c h(t) ⊕c U ⊗c x(t)).

For the next step, we need the following lemma:
Lemma 10 (Gyro-chain-rule). For α : R → R differentiable and h : R → Dn
gyro-derivative, if ˜h := h ◦ α, then we have

c with a well-deﬁned

(68)

(69)

(72)

(73)

(74)

(75)

(76)

where dα

dt (t) denotes the usual derivative.

d˜h
dt

(t) =

(t) ⊗c

(α(t)),

dα
dt

dh
dt

18

(77)

(78)

(79)

(80)

(81)

Proof.

d˜h
dt

(t) = lim
δt→0

1
δt
1
δt

⊗c [−˜h(t) ⊕c

˜h(t + δt)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

= lim
δt→0

= lim
δt→0

= lim
δt→0

= lim
u→0
dα
dt

=

α(cid:48)(t) + O(δt)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
u

(t) ⊗c

(α(t))

dh
dt

⊗c [−h(α(t)) ⊕c h(α(t) + u)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

(Möbius scalar associativity) (82)

where we set u = δt(α(cid:48)(t) + O(δt)), with u → 0 when δt → 0, which concludes.

Using lemma 10 and Eq. (75), with similar notations as in Eq. (70) we have

d˜h
dt

dα
dt

(t) =

(t) ⊗c (−˜h(t) ⊕c ϕ⊗c(W ⊗c

˜h(t) ⊕c U ⊗c ˜x(t))).

(83)

Finally, discretizing back with Eq. (74), using the left-cancellation law and dropping the tildas yields

h(t + 1) = h(t) ⊕c

(t) ⊗c (−h(t) ⊕c ϕ⊗c (W ⊗c h(t) ⊕c U ⊗c x(t))).

(84)

dα
dt

Since α is a time-warping, by deﬁnition its derivative is positive and one can choose to parametrize
it with an update-gate zt (a scalar) deﬁned with a sigmoid. Generalizing this scalar scaling by the
Möbius version of the pointwise scaling (cid:12) yields the Möbius matrix scaling diag(zt) ⊗c ·, leading to
our proposed Eq. (33) for the hyperbolic GRU.

F More Experimental Investigations

The following empirical facts were observed for both hyperbolic RNNs and GRUs.

We observed that, in the hyperbolic setting, accuracy is often much higher when sentence embeddings
can go close to the border (hyperbolic "inﬁnity"), hence exploiting the hyperbolic nature of the space.
Moreover, the faster the two sentence norms go to 1, the more it’s likely that a good local minima
was reached. See ﬁgures 3 and 5.

We often observe that test accuracy starts increasing exactly when sentence embedding norms do.
However, in the hyperbolic setting, the sentence embeddings norms remain close to 0 for a few
epochs, which does not happen in the Euclidean case. See ﬁgures 3, 5 and 4. This mysterious fact
was also exhibited in a similar way by [21] which suggests that the model ﬁrst has to adjust the
angular layout in the almost Euclidean vicinity of 0 before increasing norms and fully exploiting
hyperbolic geometry.

19

(a) Test accuracy

(a) Test accuracy

20

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 3: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the same
architecture: hyperbolic GRU followed by hyperbolic FFNN and hyperbolic/Euclidean (half-half)
MLR. The X axis shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 4: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: Euclidean GRU followed by Euclidean FFNN and Euclidean MLR. The X axis
shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 5: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: hyperbolic RNN followed by hyperbolic FFNN and hyperbolic MLR. The X axis
shows millions of training examples processed.

(a) Test accuracy

21

8
1
0
2
 
n
u
J
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
2
1
1
9
0
.
5
0
8
1
:
v
i
X
r
a

Hyperbolic Neural Networks

Octavian-Eugen Ganea∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
octavian.ganea@inf.ethz.ch

Gary Bécigneul∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
gary.becigneul@inf.ethz.ch

Thomas Hofmann
Department of Computer Science
ETH Zürich
Zurich, Switzerland
thomas.hofmann@inf.ethz.ch

Abstract

Hyperbolic spaces have recently gained momentum in the context of machine
learning due to their high capacity and tree-likeliness properties. However, the
representational power of hyperbolic geometry is not yet on par with Euclidean
geometry, mostly because of the absence of corresponding hyperbolic neural
network layers. This makes it hard to use hyperbolic embeddings in downstream
tasks. Here, we bridge this gap in a principled manner by combining the formalism
of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model
of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep
learning tools: multinomial logistic regression, feed-forward and recurrent neural
networks such as gated recurrent units. This allows to embed sequential data and
perform classiﬁcation in the hyperbolic space. Empirically, we show that, even if
hyperbolic optimization tools are limited, hyperbolic sentence embeddings either
outperform or are on par with their Euclidean variants on textual entailment and
noisy-preﬁx recognition tasks.

1

Introduction

It is common in machine learning to represent data as being embedded in the Euclidean space Rn. The
main reason for such a choice is simply convenience, as this space has a vectorial structure, closed-
form formulas of distance and inner-product, and is the natural generalization of our intuition-friendly,
visual three-dimensional space. Moreover, embedding entities in such a continuous space allows to
feed them as input to neural networks, which has led to unprecedented performance on a broad range
of problems, including sentiment detection [15], machine translation [3], textual entailment [22] or
knowledge base link prediction [20, 6].

Despite the success of Euclidean embeddings, recent research has proven that many types of com-
plex data (e.g. graph data) from a multitude of ﬁelds (e.g. Biology, Network Science, Computer
Graphics or Computer Vision) exhibit a highly non-Euclidean latent anatomy [8]. In such cases, the
Euclidean space does not provide the most powerful or meaningful geometrical representations. For
example, [10] shows that arbitrary tree structures cannot be embedded with arbitrary low distortion
(i.e. almost preserving their metric) in the Euclidean space with unbounded number of dimensions,
but this task becomes surprisingly easy in the hyperbolic space with only 2 dimensions where the
exponential growth of distances matches the exponential growth of nodes with the tree depth.

∗Equal contribution.

The adoption of neural networks and deep learning in these non-Euclidean settings has been rather
limited until very recently, the main reason being the non-trivial or impossible principled general-
izations of basic operations (e.g. vector addition, matrix-vector multiplication, vector translation,
vector inner product) as well as, in more complex geometries, the lack of closed form expressions for
basic objects (e.g. distances, geodesics, parallel transport). Thus, classic tools such as multinomial
logistic regression (MLR), feed forward (FFNN) or recurrent neural networks (RNN) did not have a
correspondence in these geometries.

How should one generalize deep neural models to non-Euclidean domains ? In this paper we address
this question for one of the simplest, yet useful, non-Euclidean domains: spaces of constant negative
curvature, i.e. hyperbolic. Their tree-likeness properties have been extensively studied [12, 13, 26]
and used to visualize large taxonomies [18] or to embed heterogeneous complex networks [17]. In
machine learning, recently, hyperbolic representations greatly outperformed Euclidean embeddings
for hierarchical, taxonomic or entailment data [21, 10, 11]. Disjoint subtrees from the latent hierar-
chical structure surprisingly disentangle and cluster in the embedding space as a simple reﬂection of
the space’s negative curvature. However, appropriate deep learning tools are needed to embed feature
data in this space and use it in downstream tasks. For example, implicitly hierarchical sequence data
(e.g. textual entailment data, phylogenetic trees of DNA sequences or hierarchial captions of images)
would beneﬁt from suitable hyperbolic RNNs.

The main contribution of this paper is to bridge the gap between hyperbolic and Euclidean geometry
in the context of neural networks and deep learning by generalizing in a principled manner both the
basic operations as well as multinomial logistic regression (MLR), feed-forward (FFNN), simple and
gated (GRU) recurrent neural networks (RNN) to the Poincaré model of the hyperbolic geometry.
We do it by connecting the theory of gyrovector spaces and generalized Möbius transformations
introduced by [2, 26] with the Riemannian geometry properties of the manifold. We smoothly
parametrize basic operations and objects in all spaces of constant negative curvature using a uniﬁed
framework that depends only on the curvature value. Thus, we show how Euclidean and hyperbolic
spaces can be continuously deformed into each other. On a series of experiments and datasets we
showcase the effectiveness of our hyperbolic neural network layers compared to their "classic"
Euclidean variants on textual entailment and noisy-preﬁx recognition tasks. We hope that this paper
will open exciting future directions in the nascent ﬁeld of Geometric Deep Learning.

2 The Geometry of the Poincaré Ball

2.1 Basics of Riemannian geometry

We brieﬂy introduce basic concepts of differential geometry largely needed for a principled general-
ization of Euclidean neural networks. For more rigorous and in-depth expositions, see [23, 14].
An n-dimensional manifold M is a space that can locally be approximated by Rn: it is a generalization
to higher dimensions of the notion of a 2D surface. For x ∈ M, one can deﬁne the tangent space
TxM of M at x as the ﬁrst order linear approximation of M around x. A Riemannian metric
g = (gx)x∈M on M is a collection of inner-products gx : TxM × TxM → R varying smoothly
with x. A Riemannian manifold (M, g) is a manifold M equipped with a Riemannian metric g.
Although a choice of a Riemannian metric g on M only seems to deﬁne the geometry locally on M,
it induces global distances by integrating the length (of the speed vector living in the tangent space)
of a shortest path between two points:

(cid:90) 1

(cid:113)

d(x, y) = inf
γ

0

gγ(t)( ˙γ(t), ˙γ(t))dt,

(1)

where γ ∈ C∞([0, 1], M) is such that γ(0) = x and γ(1) = y. A smooth path γ of minimal length
between two points x and y is called a geodesic, and can be seen as the generalization of a straight-line
in Euclidean space. The parallel transport Px→y : TxM → TyM is a linear isometry between
tangent spaces which corresponds to moving tangent vectors along geodesics and deﬁnes a canonical
way to connect tangent spaces. The exponential map expx at x, when well-deﬁned, gives a way to
project back a vector v of the tangent space TxM at x, to a point expx(v) ∈ M on the manifold.
This map is often used to parametrize a geodesic γ starting from γ(0) := x ∈ M with unit-norm
direction ˙γ(0) := v ∈ TxM as t (cid:55)→ expx(tv). For geodesically complete manifolds, such as the
Poincaré ball considered in this work, expx is well-deﬁned on the full tangent space TxM. Finally, a

2

(2)

(3)

(4)

(5)

metric ˜g is said to be conformal to another metric g if it deﬁnes the same angles, i.e.

˜gx(u, v)
(cid:112)˜gx(u, u)(cid:112)˜gx(v, v)

=

gx(u, v)
(cid:112)gx(u, u)(cid:112)gx(v, v)

,

for all x ∈ M, u, v ∈ TxM \ {0}. This is equivalent to the existence of a smooth function
λ : M → R, called the conformal factor, such that ˜gx = λ2

xgx for all x ∈ M.

2.2 Hyperbolic space: the Poincaré ball

The hyperbolic space has ﬁve isometric models that one can work with [9]. Similarly as in [21] and
[11], we choose to work in the Poincaré ball. The Poincaré ball model (Dn, gD) is deﬁned by the
manifold Dn = {x ∈ Rn : (cid:107)x(cid:107) < 1} equipped with the following Riemannian metric:

gD
x = λ2

xgE, where λx :=

2
1 − (cid:107)x(cid:107)2 ,

gE = In being the Euclidean metric tensor. Note that the hyperbolic metric tensor is conformal to
the Euclidean one. The induced distance between two points x, y ∈ Dn is known to be given by

dD(x, y) = cosh−1

1 + 2

(cid:18)

(cid:107)x − y(cid:107)2
(1 − (cid:107)x(cid:107)2)(1 − (cid:107)y(cid:107)2)

(cid:19)

.

Since the Poincaré ball is conformal to Euclidean space, the angle between two vectors u, v ∈
TxDn \ {0} is given by

cos(∠(u, v)) =

gD
x (u, v)
x (u, u)(cid:112)gD

(cid:112)gD

x (v, v)

=

(cid:104)u, v(cid:105)
(cid:107)u(cid:107)(cid:107)v(cid:107)

.

2.3 Gyrovector spaces

In Euclidean space, natural operations inherited from the vectorial structure, such as vector addition,
subtraction and scalar multiplication are often useful. The framework of gyrovector spaces provides
an elegant non-associative algebraic formalism for hyperbolic geometry just as vector spaces provide
the algebraic setting for Euclidean geometry [2, 25, 26].

In particular, these operations are used in special relativity, allowing to add speed vectors belonging
to the Poincaré ball of radius c (the celerity, i.e. the speed of light) so that they remain in the ball,
hence not exceeding the speed of light.

We will make extensive use of these operations in our deﬁnitions of hyperbolic neural networks.
For c ≥ 0, denote2 by Dn
then Dn

c := {x ∈ Rn | c(cid:107)x(cid:107)2 < 1}. Note that if c = 0, then Dn
c. If c = 1 then we recover the usual ball Dn.

c is the open ball of radius 1/

c = Rn; if c > 0,

√

Möbius addition. The Möbius addition of x and y in Dn

c is deﬁned as

x ⊕c y :=

(1 + 2c(cid:104)x, y(cid:105) + c(cid:107)y(cid:107)2)x + (1 − c(cid:107)x(cid:107)2)y
1 + 2c(cid:104)x, y(cid:105) + c2(cid:107)x(cid:107)2(cid:107)y(cid:107)2

.

(6)

In particular, when c = 0, one recovers the Euclidean addition of two vectors in Rn. Note that
without loss of generality, the case c > 0 can be reduced to c = 1. Unless stated otherwise, we
will use ⊕ as ⊕1 to simplify notations. For general c > 0, this operation is not commutative nor
associative. However, it satisﬁes x ⊕c 0 = 0 ⊕c x = 0. Moreover, for any x, y ∈ Dn
c , we have
(−x) ⊕c x = x ⊕c (−x) = 0 and (−x) ⊕c (x ⊕c y) = y (left-cancellation law). The Möbius
substraction is then deﬁned by the use of the following notation: x (cid:9)c y := x ⊕c (−y). See [29,
section 2.1] for a geometric interpretation of the Möbius addition.

2We take different notations as in [25] where the author uses s = 1/

c.

√

3

Möbius scalar multiplication. For c > 0, the Möbius scalar multiplication of x ∈ Dn
r ∈ R is deﬁned as

c \ {0} by

r ⊗c x := (1/

c) tanh(r tanh−1(

c(cid:107)x(cid:107)))

√

√

x
(cid:107)x(cid:107)

,

(7)

and r ⊗c 0 := 0. Note that similarly as for the Möbius addition, one recovers the Euclidean scalar
multiplication when c goes to zero: limc→0 r ⊗c x = rx. This operation satisﬁes desirable properties
such as n ⊗c x = x ⊕c · · · ⊕c x (n additions), (r + r(cid:48)) ⊗c x = r ⊗c x ⊕c r(cid:48) ⊗c x (scalar distributivity3),
(rr(cid:48)) ⊗c x = r ⊗c (r(cid:48) ⊗c x) (scalar associativity) and |r| ⊗c x/(cid:107)r ⊗c x(cid:107) = x/(cid:107)x(cid:107) (scaling property).

c , gc) is given by4

Distance.
Euclidean one, with conformal factor λc
(Dn

If one deﬁnes the generalized hyperbolic metric tensor gc as the metric conformal to the
x := 2/(1 − c(cid:107)x(cid:107)2), then the induced distance function on
√

c) tanh−1 (cid:0)√
Again, observe that limc→0 dc(x, y) = 2(cid:107)x − y(cid:107), i.e. we recover Euclidean geometry in the limit5.
Moreover, for c = 1 we recover dD of Eq. (4).

c(cid:107) − x ⊕c y(cid:107)(cid:1) .

dc(x, y) = (2/

(8)

Hyperbolic trigonometry. Similarly as in the Euclidean space, one can deﬁne the notions of
hyperbolic angles or gyroangles (when using the ⊕c), as well as hyperbolic law of sines in the
generalized Poincaré ball (Dn

c , gc). We make use of these notions in our proofs. See Appendix A.

2.4 Connecting Gyrovector spaces and Riemannian geometry of the Poincaré ball

In this subsection, we present how geodesics in the Poincaré ball model are usually described with
Möbius operations, and push one step further the existing connection between gyrovector spaces and
the Poincaré ball by ﬁnding new identities involving the exponential map, and parallel transport.

In particular, these ﬁndings provide us with a simpler formulation of Möbius scalar multiplication,
yielding a natural deﬁnition of matrix-vector multiplication in the Poincaré ball.

Riemannian gyroline element. The Riemannian gyroline element is deﬁned for an inﬁnitesimal
dx as ds := (x + dx) (cid:9)c x, and its size is given by [26, section 3.7]:

(cid:107)ds(cid:107) = (cid:107)(x + dx) (cid:9)c x(cid:107) = (cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2).

(9)

What is remarkable is that it turns out to be identical, up to a scaling factor of 2, to the usual line
element 2(cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2) of the Riemannian manifold (Dn

c , gc).

Geodesics. The geodesic connecting points x, y ∈ Dn

c is shown in [2, 26] to be given by:

γx→y(t) := x ⊕c (−x ⊕c y) ⊗c t, with γx→y : R → Dn

c s.t. γx→y(0) = x and γx→y(1) = y.

Note that when c goes to 0, geodesics become straight-lines, recovering Euclidean geometry. In the
remainder of this subsection, we connect the gyrospace framework with Riemannian geometry.
Lemma 1. For any x ∈ Dn and v ∈ TxDn
c s.t. gc
x with direction v is given by:

x(v, v) = 1, the unit-speed geodesic starting from

γx,v(t) = x ⊕c

tanh

(cid:18)

(cid:18)√

(cid:19) v
√

c

t
2

c(cid:107)v(cid:107)

(cid:19)

, where γx,v : R → Dn s.t. γx,v(0) = x and ˙γx,v(0) = v.

(10)

(11)

Proof. One can use Eq. (10) and reparametrize it to unit-speed using Eq. (8). Alternatively, direct
computation and identiﬁcation with the formula in [11, Thm. 1] would give the same result. Using
Eq. (8) and Eq. (11), one can sanity-check that dc(γ(0), γ(t)) = t, ∀t ∈ [0, 1].

3⊗c has priority over ⊕c in the sense that a ⊗c b ⊕c c := (a ⊗c b) ⊕c c and a ⊕c b ⊗c c := a ⊕c (b ⊗c c).
4The notation −x ⊕c y should always be read as (−x) ⊕c y and not −(x ⊕c y).
5The factor 2 comes from the conformal factor λx = 2/(1 − (cid:107)x(cid:107)2), which is a convention setting the

curvature to −1.

4

Exponential and logarithmic maps. The following lemma gives the closed-form derivation of
exponential and logarithmic maps.
Lemma 2. For any point x ∈ Dn
map logc
c → TxDn
(cid:18)

c are given for v (cid:54)= 0 and y (cid:54)= x by:
(cid:18)√

c , the exponential map expc

c and the logarithmic

x : TxDn

c → Dn

x : Dn

√

(cid:19)

, logc

x(y) =

√

tanh−1(

c(cid:107) − x ⊕c y(cid:107))

expc

x(v) = x ⊕c

tanh

c

λc
x(cid:107)v(cid:107)
2

(cid:19) v
√

c(cid:107)v(cid:107)

2
cλc
x

−x ⊕c y
(cid:107) − x ⊕c y(cid:107)
(12)

.

Proof. Following the proof of [11, Cor. 1.1], one gets expc
gives the formula for expc

x. Algebraic check of the identity logc

x(v) = γx,
x(expc

v

x(cid:107)v(cid:107) (λc
λc

x(cid:107)v(cid:107)). Using Eq. (11)

x(v)) = v concludes.

The above maps have more appealing forms when x = 0, namely for v ∈ T0Dn

c \ {0}, y ∈ Dn

c \ {0}:

expc

0(v) = tanh(

c(cid:107)v(cid:107))

√

, logc

0(y) = tanh−1(

c(cid:107)y(cid:107))

√

(13)

√

y
c(cid:107)y(cid:107)

.

√

v
c(cid:107)v(cid:107)

Moreover, we still recover Euclidean geometry in the limit c → 0, as limc→0 expc
Euclidean exponential map, and limc→0 logc

x(y) = y − x is the Euclidean logarithmic map.

x(v) = x + v is the

Möbius scalar multiplication using exponential and logarithmic maps. We studied the expo-
nential and logarithmic maps in order to gain a better understanding of the Möbius scalar multiplica-
tion (Eq. (7)). We found the following:
Lemma 3. The quantity r ⊗ x can actually be obtained by projecting x in the tangent space at 0
with the logarithmic map, multiplying this projection by the scalar r in T0Dn
c , and then projecting it
back on the manifold with the exponential map:
0(r logc

∀r ∈ R, x ∈ Dn
c .

r ⊗c x = expc

0(x)),

(14)

In addition, we recover the well-known relation between geodesics connecting two points and the
exponential map:

γx→y(t) = x ⊕c (−x ⊕c y) ⊗c t = expc

x(t logc

x(y)),

t ∈ [0, 1].

(15)

This last result enables us to generalize scalar multiplication in order to deﬁne matrix-vector multipli-
cation between Poincaré balls, one of the essential building blocks of hyperbolic neural networks.

Parallel transport. Finally, we connect parallel transport (from T0Dn
the following theorem, which we prove in appendix B.
Theorem 4. In the manifold (Dn
vector v ∈ T0Dn

c to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a
c is given by the following isometry:
λc
0
λc
x

x(x ⊕c expc

0(v)) =

0→x(v) = logc
P c

(16)

v.

c ) to gyrovector spaces with

As we’ll see later, this result is crucial in order to deﬁne and optimize parameters shared between
different tangent spaces, such as biases in hyperbolic neural layers or parameters of hyperbolic MLR.

3 Hyperbolic Neural Networks

Neural networks can be seen as being made of compositions of basic operations, such as linear
maps, bias translations, pointwise non-linearities and a ﬁnal sigmoid or softmax layer. We ﬁrst
explain how to construct a softmax layer for logits lying in a Poincaré ball. Then, we explain how
to transform a mapping between two Euclidean spaces as one between Poincaré balls, yielding
matrix-vector multiplication and pointwise non-linearities in the Poincaré ball. Finally, we present
possible adaptations of various recurrent neural networks to the hyperbolic domain.

5

3.1 Hyperbolic multiclass logistic regression

In order to perform multi-class classiﬁcation on the Poincaré ball, one needs to generalize multinomial
logistic regression (MLR) − also called softmax regression − to the Poincaré ball.

Reformulating Euclidean MLR. Let’s ﬁrst reformulate Euclidean MLR from the perspective of
distances to margin hyperplanes, as in [19, Section 5]. This will allow us to easily generalize it.

Given K classes, one learns a margin hyperplane for each such class using softmax probabilities:

∀k ∈ {1, ..., K},

p(y = k|x) ∝ exp (((cid:104)ak, x(cid:105) − bk)) , where bk ∈ R, x, ak ∈ Rn.

(17)

Note that any afﬁne hyperplane in Rn can be written with a normal vector a and a scalar shift b:

Ha,b = {x ∈ Rn : (cid:104)a, x(cid:105) − b = 0}, where a ∈ Rn \ {0}, and b ∈ R.

(18)

As in [19, Section 5], we note that (cid:104)a, x(cid:105) − b = sign((cid:104)a, x(cid:105) − b)(cid:107)a(cid:107)d(x, Ha,b). Using Eq. (17):

p(y = k|x) ∝ exp(sign((cid:104)ak, x(cid:105) − bk)(cid:107)ak(cid:107)d(x, Hak,bk )), bk ∈ R, x, ak ∈ Rn.

(19)

As it is not immediately obvious how to generalize the Euclidean hyperplane of Eq. (18) to other
spaces such as the Poincaré ball, we reformulate it as follows:

˜Ha,p = {x ∈ Rn : (cid:104)−p + x, a(cid:105) = 0} = p + {a}⊥, where p ∈ Rn, a ∈ Rn \ {0}.

(20)

This new deﬁnition relates to the previous one as ˜Ha,p = Ha,(cid:104)a,p(cid:105). Rewriting Eq. (19) with b = (cid:104)a, p(cid:105):
p(y = k|x) ∝ exp(sign((cid:104)−pk + x, ak(cid:105))(cid:107)ak(cid:107)d(x, ˜Hak,pk )), with pk, x, ak ∈ Rn.

(21)

It is now natural to adapt the previous deﬁnition to the hyperbolic setting by replacing + by ⊕c:
Deﬁnition 3.1 (Poincaré hyperplanes). For p ∈ Dn
p(z, a) = 0} = {z ∈ TpDn
gc

c : (cid:104)z, a(cid:105) = 0}. Then, we deﬁne Poincaré hyperplanes as

c \ {0}, let {a}⊥ := {z ∈ TpDn
c :

c , a ∈ TpDn

˜H c

a,p := {x ∈ Dn

c : (cid:104)logc

p(x), a(cid:105)p = 0} = expc

p({a}⊥) = {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}.

(22)

The last equality is shown appendix C. ˜H c
all geodesics in Dn
hypergyroplanes, see [27, deﬁnition 5.8]. A 3D hyperplane example is depicted in Fig. 1.

a,p can also be described as the union of images of
c orthogonal to a and containing p. Notice that our deﬁnition matches that of

Next, we need the following theorem, proved in appendix D:
Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(23)

Final formula for MLR in the Poincaré ball. Putting together Eq. (21) and Thm. 5, we get the
hyperbolic MLR formulation. Given K classes and k ∈ {1, . . . , K}, pk ∈ Dn
c \ {0}:

c , ak ∈ Tpk

Dn

p(y = k|x) ∝ exp(sign((cid:104)−pk ⊕c x, ak(cid:105))

gc
pk

(ak, ak)dc(x, ˜H c

)),

ak,pk

∀x ∈ Dn
c ,

(24)

or, equivalently

p(y = k|x) ∝ exp

(cid:18) λc
pk

(cid:107)ak(cid:107)
√
c

sinh−1

(cid:18)

√
2

c(cid:104)−pk ⊕c x, ak(cid:105)

(cid:19)(cid:19)

(1 − c(cid:107) − pk ⊕c x(cid:107)2)(cid:107)ak(cid:107)

,

∀x ∈ Dn
c .

(25)

this goes to p(y = k|x) ∝ exp(4(cid:104)−pk + x, ak(cid:105)) =

Notice that when c goes to zero,
exp((λ0
pk

)2(cid:104)−pk + x, ak(cid:105)) = exp((cid:104)−pk + x, ak(cid:105)0), recovering the usual Euclidean softmax.
However, at this point it is unclear how to perform optimization over ak, since it lives in Tpk
hence depends on pk. The solution is that one should write ak = P c
)a(cid:48)
k ∈ T0Dn
a(cid:48)

c = Rn, and optimize a(cid:48)

k as a Euclidean parameter.

k) = (λc

0/λc
pk

0→pk

(a(cid:48)

Dn
c and
k, where

1
√
c

(cid:113)

6

3.2 Hyperbolic feed-forward layers

In order to deﬁne hyperbolic neural networks, it is crucial to de-
ﬁne a canonically simple parametric family of transformations,
playing the role of linear mappings in usual Euclidean neural
networks, and to know how to apply pointwise non-linearities.
Inspiring ourselves from our reformulation of Möbius scalar
multiplication in Eq. (14), we deﬁne:
Deﬁnition 3.2 (Möbius version). For f : Rn → Rm, we deﬁne
the Möbius version of f as the map from Dn

c to Dm

c by:

f ⊗c(x) := expc

0(f (logc

0(x))),

(26)

where expc

0 : T0m

Dm

c → Dm

c and logc

0 : Dn

c → T0n

Dn
c .

Figure 1: An example of a hyper-
bolic hyperplane in D3
1 plotted us-
ing sampling. The red point is p.
The shown normal axis to the hy-
perplane through p is parallel to a.

Note that similarly as for other Möbius operations, we recover
the Euclidean mapping in the limit c → 0 if f is continuous, as limc→0 f ⊗c(x) = f (x). This
deﬁnition satisﬁes a few desirable properties too, such as: (f ◦ g)⊗c = f ⊗c ◦ g⊗c for f : Rm → Rl
and g : Rn → Rm (morphism property), and f ⊗c(x)/(cid:107)f ⊗c(x)(cid:107) = f (x)/(cid:107)f (x)(cid:107) for f (x) (cid:54)= 0
(direction preserving). It is then straight-forward to prove the following result:
Lemma 6 (Möbius matrix-vector multiplication). If M : Rn → Rm is a linear map, which we
identify with its matrix representation, then ∀x ∈ Dn

c , if M x (cid:54)= 0 we have

M ⊗c(x) = (1/

c) tanh

√

(cid:18) (cid:107)M x(cid:107)
(cid:107)x(cid:107)

√

tanh−1(

c(cid:107)x(cid:107))

(cid:19) M x
(cid:107)M x(cid:107)

,

(27)

and M ⊗c(x) = 0 if M x = 0. Moreover, if we deﬁne the Möbius matrix-vector multiplication of
M ∈ Mm,n(R) and x ∈ Dn
c by M ⊗c x := M ⊗c(x), then we have (M M (cid:48)) ⊗c x = M ⊗c (M (cid:48) ⊗c x)
for M ∈ Ml,m(R) and M (cid:48) ∈ Mm,n(R) (matrix associativity), (rM ) ⊗c x = r ⊗c (M ⊗c x) for
r ∈ R and M ∈ Mm,n(R) (scalar-matrix associativity) and M ⊗c x = M x for all M ∈ On(R)
(rotations are preserved).

Pointwise non-linearity.
ϕ⊗c can be applied to elements of the Poincaré ball.

If ϕ : Rn → Rn is a pointwise non-linearity, then its Möbius version

Bias translation. The generalization of a translation in the Poincaré ball is naturally given by
moving along geodesics. But should we use the Möbius sum x ⊕c b with a hyperbolic bias b or the
x(b(cid:48)) with a Euclidean bias b(cid:48)? These views are uniﬁed with parallel transport
exponential map expc
c by a bias b ∈ Dn
(see Thm 4). Möbius translation of a point x ∈ Dn
(cid:18) λc
0
λc
x

c is given by
(cid:19)

x ← x ⊕c b = expc

0(b))) = expc
x

0→x(logc

x(P c

logc

0(b)

(28)

.

We recover Euclidean translations in the limit c → 0. Note that bias translations play a particular
Indeed, consider multiple layers of the form fk(x) = ϕk(Mkx), each of
role in this model.
which having Möbius version f ⊗c
k (Mk ⊗c x). Then their composition can be re-written
f ⊗c
k ◦ · · · ◦ f ⊗c
1 = expc
0. This means that these operations can essentially be
performed in Euclidean space. Therefore, it is the interposition between those with the bias translation
of Eq. (28) which differentiates this model from its Euclidean counterpart.

k (x) = ϕ⊗c
0 ◦fk ◦ · · · ◦ f1 ◦ logc

If a vector x ∈ Rn+p is the (vertical) concatenation
Concatenation of multiple input vectors.
of two vectors x1 ∈ Rn, x2 ∈ Rp, and M ∈ Mm,n+p(R) can be written as the (horizontal)
concatenation of two matrices M1 ∈ Mm,n(R) and M2 ∈ Mm,p(R), then M x = M1x1 + M2x2.
We generalize this to hyperbolic spaces: if we are given x1 ∈ Dn
c ×Dp
c ,
and M, M1, M2 as before, then we deﬁne M ⊗c x := M1 ⊗c x1 ⊕c M2 ⊗c x2. Note that when c goes
to zero, we recover the Euclidean formulation, as limc→0 M ⊗c x = limc→0 M1 ⊗c x1 ⊕c M2 ⊗c x2 =
M1x1 + M2x2 = M x. Moreover, hyperbolic vectors x ∈ Dn
c can also be "concatenated" with real
features y ∈ R by doing: M ⊗c x ⊕c y ⊗c b with learnable b ∈ Dm

c , x = (x1 x2)T ∈ Dn

c and M ∈ Mm,n(R).

c , x2 ∈ Dp

7

3.3 Hyperbolic RNN

Naive RNN. A simple RNN can be deﬁned by ht+1 = ϕ(W ht + U xt + b) where ϕ is a pointwise
non-linearity, typically tanh, sigmoid, ReLU, etc. This formula can be naturally generalized to the
hyperbolic space as follows. For parameters W ∈ Mm,n(R), U ∈ Mm,d(R), b ∈ Dm
c , we deﬁne:

ht+1 = ϕ⊗c (W ⊗c ht ⊕c U ⊗c xt ⊕c b),

ht ∈ Dn

c , xt ∈ Dd
c .

(29)

Note that if inputs xt’s are Euclidean, one can write ˜xt := expc
expc

(U xt)) = W ⊗c ht ⊕c expc

(P c

W ⊗cht

0→W ⊗cht

0(U xt) = W ⊗c ht ⊕c U ⊗c ˜xt.

0(xt) and use the above formula, since

GRU architecture. One can also adapt the GRU architecture:
rt = σ(W rht−1 + U rxt + br),
zt = σ(W zht−1 + U zxt + bz),
˜ht = ϕ(W (rt (cid:12) ht−1) + U xt + b), ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht,

(30)

where (cid:12) denotes pointwise product. First, how should we adapt the pointwise multiplication by a
scaling gate? Note that the deﬁnition of the Möbius version (see Eq. (26)) can be naturally extended
to maps f : Rn × Rp → Rm as f ⊗c : (h, h(cid:48)) ∈ Dn
0(h(cid:48)))). In
c (cid:55)→ expc
0(h(cid:48))) =
particular, choosing f (h, h(cid:48)) := σ(h) (cid:12) h(cid:48) yields6 f ⊗c(h, h(cid:48)) = expc
diag(σ(logc

0(h))) ⊗c h(cid:48). Hence we adapt rt (cid:12) ht−1 to diag(rt) ⊗c ht−1 and the reset gate rt to:
0(W r ⊗c ht−1 ⊕c U r ⊗c xt ⊕c br),

0(h), logc
0(h)) (cid:12) logc

0(f (logc
0(σ(logc

rt = σ logc

c × Dp

(31)

and similarly for the update gate zt. Note that as the argument of σ in the above is unbounded, rt and
zt can a priori take values onto the full range (0, 1). Now the intermediate hidden state becomes:
˜ht = ϕ⊗c ((W diag(rt)) ⊗c ht−1 ⊕c U ⊗c xt ⊕ b),

(32)

where Möbius matrix associativity simpliﬁes W ⊗c (diag(rt) ⊗c ht−1) into (W diag(rt)) ⊗c ht−1.
Finally, we propose to adapt the update-gate equation as

ht = ht−1 ⊕c diag(zt) ⊗c (−ht−1 ⊕c

˜ht).

(33)

Note that when c goes to zero, one recovers the usual GRU. Moreover, if zt = 0 or zt = 1, then ht
becomes ht−1 or ˜ht respectively, similarly as in the usual GRU. This adaptation was obtained by
adapting [24]: in this work, the authors re-derive the update-gate mechanism from a ﬁrst principle
called time-warping invariance. We adapted their derivation to the hyperbolic setting by using the
notion of gyroderivative [4] and proving a gyro-chain-rule (see appendix E).

4 Experiments

SNLI task and dataset. We evaluate our method on two tasks. The ﬁrst is natural language
inference, or textual entailment. Given two sentences, a premise (e.g. "Little kids A. and B. are
playing soccer.") and a hypothesis (e.g. "Two children are playing outdoors."), the binary classiﬁcation
task is to predict whether the second sentence can be inferred from the ﬁrst one. This deﬁnes a partial
order in the sentence space. We test hyperbolic networks on the biggest real dataset for this task,
SNLI [7]. It consists of 570K training, 10K validation and 10K test sentence pairs. Following [28],
we merge the "contradiction" and "neutral" classes into a single class of negative sentence pairs, while
the "entailment" class gives the positive pairs.

PREFIX task and datasets. We conjecture that the improvements of hyperbolic neural networks
are more signiﬁcant when the underlying data structure is closer to a tree. To test this, we design a
proof-of-concept task of detection of noisy preﬁxes, i.e. given two sentences, one has to decide if the
second sentence is a noisy preﬁx of the ﬁrst, or a random sentence. We thus build synthetic datasets
PREFIX-Z% (for Z being 10, 30 or 50) as follows: for each random ﬁrst sentence of random length
at most 20 and one random preﬁx of it, a second positive sentence is generated by randomly replacing
Z% of the words of the preﬁx, and a second negative sentence of same length is randomly generated.
Word vocabulary size is 100, and we generate 500K training, 10K validation and 10K test pairs.

6If x has n coordinates, then diag(x) denotes the diagonal matrix of size n with xi’s on its diagonal.

8

Models architecture. Our neural network layers can be used in a plug-n-play manner exactly like
standard Euclidean layers. They can also be combined with Euclidean layers. However, optimization
w.r.t. hyperbolic parameters is different (see below) and based on Riemannian gradients which
are just rescaled Euclidean gradients when working in the conformal Poincaré model [21]. Thus,
back-propagation can be applied in the standard way.

In our setting, we embed the two sentences using two distinct hyperbolic RNNs or GRUs. The
sentence embeddings are then fed together with their squared distance (hyperbolic or Euclidean,
depending on their geometry) to a FFNN (Euclidean or hyperbolic, see Sec. 3.2) which is further
fed to an MLR (Euclidean or hyperbolic, see Sec. 3.1) that gives probabilities of the two classes
(entailment vs neutral). We use cross-entropy loss on top. Note that hyperbolic and Euclidean layers
can be mixed, e.g. the full network can be hyperbolic and only the last layer be Euclidean, in which
case one has to use log0 and exp0 functions to move between the two manifolds in a correct manner
as explained for Eq. 26.

Optimization. Our models have both Euclidean (e.g. weight matrices in both Euclidean and
hyperbolic FFNNs, RNNs or GRUs) and hyperbolic parameters (e.g. word embeddings or biases for
the hyperbolic layers). We optimize the Euclidean parameters with Adam [16] (learning rate 0.001).
Hyperbolic parameters cannot be updated with an equivalent method that keeps track of gradient
history due to the absence of a Riemannian Adam. Thus, they are optimized using full Riemannian
stochastic gradient descent (RSGD) [5, 11]. We also experiment with projected RSGD [21], but
optimization was sometimes less stable. We use a different constant learning rate for word embeddings
(0.1) and other hyperbolic weights (0.01) because words are updated less frequently.

Numerical errors. Gradients of the basic operations deﬁned above (e.g. ⊕c, exponential map) are
c(cid:107)x(cid:107) = 1. Thus, we
not deﬁned when the hyperbolic argument vectors are on the ball border, i.e.
always project results of these operations in the ball of radius 1 − (cid:15), where (cid:15) = 10−5. Numerical
errors also appear when hyperbolic vectors get closer to 0, thus we perturb them with an (cid:15)(cid:48) = 10−15
before they are used in any of the above operations. Finally, arguments of the tanh function are
clipped between ±15 to avoid numerical errors, while arguments of tanh−1 are clipped to at most
1 − 10−5.

√

Hyperparameters. For all methods, baselines and datasets, we use c = 1, word and hidden state
embedding dimension of 5 (we focus on the low dimensional setting that was shown to already
be effective [21]), batch size of 64. We ran all methods for a ﬁxed number of 30 epochs. For all
models, we experiment with both identity (no non-linearity) or tanh non-linearity in the RNN/GRU
cell, as well as identity or ReLU after the FFNN layer and before MLR. As expected, for the fully
Euclidean models, tanh and ReLU respectively surpassed the identity variant by a large margin. We
only report the best Euclidean results. Interestingly, for the hyperbolic models, using only identity for
both non-linearities works slightly better and this is likely due to two facts: i) our hyperbolic layers
already contain non-linearities by their nature, ii) tanh is limiting the output domain of the sentence
embeddings, but the hyperbolic speciﬁc geometry is more pronounced at the ball border, i.e. at the
hyperbolic "inﬁnity", compared to the center of the ball.

For the results shown in Tab. 1, we run each model (baseline or ours) exactly 3 times and report the
test result corresponding to the best validation result from these 3 runs. We do this because the highly
non-convex spectrum of hyperbolic neural networks sometimes results in convergence to poor local
minima, suggesting that initialization is very important.

Results. Results are shown in Tab. 1. Note that the fully Euclidean baseline models might have
an advantage over hyperbolic baselines because more sophisticated optimization algorithms such
as Adam do not have a hyperbolic analogue at the moment. We ﬁrst observe that all GRU models
overpass their RNN variants. Hyperbolic RNNs and GRUs have the most signiﬁcant improvement
over their Euclidean variants when the underlying data structure is more tree-like, e.g. for PREFIX-
10% − for which the tree relation between sentences and their preﬁxes is more prominent − we
reduce the error by a factor of 3.35 for hyperbolic vs Euclidean RNN, and by a factor of 1.5 for
hyperbolic vs Euclidean GRU. As soon as the underlying structure diverges more and more from
a tree, the accuracy gap decreases − for example, for PREFIX-50% the noise heavily affects the
representational power of hyperbolic networks. Also, note that on SNLI our methods perform
similarly as with their Euclidean variants. Moreover, hyperbolic and Euclidean MLR are on par when

9

SNLI

PREFIX-10% PREFIX-30% PREFIX-50%

FULLY EUCLIDEAN RNN
HYPERBOLIC RNN+FFNN, EUCL MLR
FULLY HYPERBOLIC RNN
FULLY EUCLIDEAN GRU
HYPERBOLIC GRU+FFNN, EUCL MLR
FULLY HYPERBOLIC GRU

79.34 %
79.18 %
78.21 %
81.52 %
79.76 %
81.19 %

89.62 %
96.36 %
96.91 %
95.96 %
97.36 %
97.14 %

81.71 %
87.83 %
87.25 %
86.47 %
88.47 %
88.26 %

72.10 %
76.50 %
62.94 %
75.04 %
76.87 %
76.44 %

Table 1: Test accuracies for various models and four datasets. "Eucl" denotes Euclidean. All word
and sentence embeddings have dimension 5. We highlight in bold the best baseline (or baselines, if
the difference is less than 0.5%).

used in conjunction with hyperbolic sentence embeddings, suggesting further empirical investigation
is needed for this direction (see below).

We also observe that, in the hyperbolic setting, accuracy tends to increase when sentence embeddings
start increasing, and gets better as their norms converge towards 1 (the ball border for c = 1). Unlike
in the Euclidean case, this behavior does happen only after a few epochs and suggests that the model
should ﬁrst adjust the angular layout in order to disentangle the representations, before increasing their
norms to fully exploit the strong clustering property of the hyperbolic geometry. Similar behavior
was observed in the context of embedding trees by [21]. Details in appendix F.

MLR classiﬁcation experiments.
For the sentence entailment classi-
ﬁcation task we do not see a clear
advantage of hyperbolic MLR com-
pared to its Euclidean variant. A pos-
sible reason is that, when trained end-
to-end, the model might decide to
place positive and negative embed-
dings in a manner that is already well
separated with a classic MLR. As a
consequence, we further investigate
MLR for the task of subtree classiﬁ-
cation. Using an open source imple-
mentation7 of [21], we pre-trained
Poincaré embeddings of the Word-
Net noun hierarchy (82,115 nodes).
We then choose one node in this tree
(see Table 2) and classify all other
nodes (solely based on their embed-
dings) as being part of the subtree
rooted at this node. All nodes in such a subtree are divided into positive training nodes (80%) and
positive test nodes (20%). The same splitting procedure is applied for the remaining WordNet nodes
that are divided into a negative training and negative test set respectively. Three variants of MLR
are then trained on top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation
task: hyperbolic MLR, Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean
MLR applied after mapping all embeddings in the tangent space at 0 using the log0 map. We use
different embedding dimensions : 2, 3, 5 and 10. For the hyperbolic MLR, we use full Riemannian
SGD with a learning rate of 0.001. For the two Euclidean models we use ADAM optimizer and the
same learning rate. During training, we always sample the same number of negative and positive
nodes in each minibatch of size 16; thus positive nodes are frequently resampled. All methods are
trained for 30 epochs and the ﬁnal F1 score is reported (no hyperparameters to validate are used, thus
we do not require a validation set). This procedure is repeated for four subtrees of different sizes.

Figure 2: Hyperbolic (left) vs Direct Euclidean (right) binary
MLR used to classify nodes as being part in the GROUP.N.01
subtree of the WordNet noun hierarchy solely based on their
Poincaré embeddings. The positive points (from the subtree)
are in blue, the negative points (the rest) are in red and the
trained positive separation hyperplane is depicted in green.

Quantitative results are presented in Table 2. We can see that the hyperbolic MLR overpasses
its Euclidean variants in almost all settings, sometimes by a large margin. Moreover, to provide

7https://github.com/dalab/hyperbolic_cones

10

WORDNET
SUBTREE

ANIMAL.N.01
3218 / 798

GROUP.N.01
6649 / 1727

WORKER.N.01
861 / 254

MAMMAL.N.01
953 / 228

MODEL

D = 2

D = 3

D = 5

D = 10

HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL

47.43 ± 1.07%
41.69 ± 0.19%
38.89 ± 0.01%
81.72 ± 0.17%
61.13 ± 0.42%
60.75 ± 0.24%
12.68 ± 0.82%
10.86 ± 0.01%
9.04 ± 0.06%
32.01 ± 17.14%
15.58 ± 0.04%
13.10 ± 0.13%

91.92 ± 0.61%
68.43 ± 3.90%
62.57 ± 0.61%
89.87 ± 2.73%
63.56 ± 1.22%
61.98 ± 0.57%
24.09 ± 1.49%
22.39 ± 0.04%
22.57 ± 0.20%
87.54 ± 4.55%
44.68 ± 1.87%
44.89 ± 1.18%

98.07 ± 0.55%
95.59 ± 1.18%
89.21 ± 1.34%
87.89 ± 0.80%
67.82 ± 0.81%
67.92 ± 0.74%
55.46 ± 5.49%
35.23 ± 3.16%
26.47 ± 0.78%
88.73 ± 3.22%
59.35 ± 1.31%
52.51 ± 0.85%

99.26 ± 0.59%
99.36 ± 0.18%
98.27 ± 0.70%
91.91 ± 3.07%
91.38 ± 1.19%
91.41 ± 0.18%
66.83 ± 11.38%
47.29 ± 3.93%
36.66 ± 2.74%
91.37 ± 6.09%
77.76 ± 5.08%
56.11 ± 2.21%

Table 2: Test F1 classiﬁcation scores for four different subtrees of WordNet noun tree. All nodes
in such a subtree are divided into positive training nodes (80%) and positive test nodes (20%);
these counts are shown below each subtree root. The same splitting procedure is applied for the
remaining nodes to obtain negative training and test sets. Three variants of MLR are then trained on
top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation task: hyperbolic MLR,
Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean MLR applied after
mapping all embeddings in the tangent space at 0 using the log0 map. 95% conﬁdence intervals for 3
different runs are shown for each method and each different embedding dimension (2, 3, 5 or 10).

further understanding, we plot the 2-dimensional embeddings and the trained separation hyperplanes
(geodesics in this case) in Figure 2. We can see that respecting the hyperbolic geometry is very
important for a quality classiﬁcation model.

5 Conclusion

We showed how classic Euclidean deep learning tools such as MLR, FFNNs, RNNs or GRUs can be
generalized in a principled manner to all spaces of constant negative curvature combining Riemannian
geometry with the elegant theory of gyrovector spaces. Empirically we found that our models
outperform or are on par with corresponding Euclidean architectures on sequential data with implicit
hierarchical structure. We hope to trigger exciting future research related to better understanding
of the hyperbolic non-convexity spectrum and development of other non-Euclidean deep learning
methods.
Our data and Tensorﬂow [1] code are publicly available8.

Acknowledgements

We thank Igor Petrovski for useful pointers regarding the implementation.

This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement
number 167176. Gary Bécigneul is also funded by the Max Planck ETH Center for Learning
Systems.

References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for
large-scale machine learning. 2016.

[2] Ungar Abraham Albert. Analytic hyperbolic geometry and Albert Einstein’s special theory of

relativity. World scientiﬁc, 2008.

8https://github.com/dalab/hyperbolic_nn

11

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of the International Conference on Learning
Representations (ICLR), 2015.

[4] Graciela S Birman and Abraham A Ungar. The hyperbolic derivative in the poincaré ball model
of hyperbolic geometry. Journal of mathematical analysis and applications, 254(1):321–333,
2001.

[5] S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on

Automatic Control, 58(9):2217–2229, Sept 2013.

[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems (NIPS), pages 2787–2795, 2013.

[7] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 632–642. Association
for Computational Linguistics, 2015.

[8] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,
34(4):18–42, 2017.

[9] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry.

Flavors of geometry, 31:59–115, 1997.

[10] Christopher De Sa, Albert Gu, Christopher Ré, and Frederic Sala. Representation tradeoffs for

hyperbolic embeddings. arXiv preprint arXiv:1804.03329, 2018.

[11] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In Proceedings of the thirty-ﬁfth international conference
on machine learning (ICML), 2018.

[12] Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pages 75–263. Springer, 1987.

[13] Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the

Cambridge Philosophical Society, page 1–17, 2017.

[14] Christopher Hopper and Ben Andrews. The Ricci ﬂow in Riemannian geometry. Springer, 2010.

[15] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1746–1751. Association for Computational Linguistics, 2014.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations (ICLR), 2015.

[17] Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marián Boguná.

Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.

[18] John Lamping, Ramana Rao, and Peter Pirolli. A focus+ context technique based on hyperbolic
geometry for visualizing large hierarchies. In Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 401–408. ACM Press/Addison-Wesley Publishing Co.,
1995.

[19] Guy Lebanon and John Lafferty. Hyperplane margin classiﬁers on the multinomial manifold. In
Proceedings of the international conference on machine learning (ICML), page 66. ACM, 2004.

[20] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective
learning on multi-relational data. In Proceedings of the international conference on machine
learning (ICML), volume 11, pages 809–816, 2011.

12

[21] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical repre-
sentations. In Advances in Neural Information Processing Systems (NIPS), pages 6341–6350,
2017.

[22] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blun-
som. Reasoning about entailment with neural attention. In Proceedings of the International
Conference on Learning Representations (ICLR), 2015.

[23] Michael Spivak. A comprehensive introduction to differential geometry. Publish or perish, 1979.

[24] Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In Proceedings of

the International Conference on Learning Representations (ICLR), 2018.

[25] Abraham A Ungar. Hyperbolic trigonometry and its application in the poincaré ball model of

hyperbolic geometry. Computers & Mathematics with Applications, 41(1-2):135–147, 2001.

[26] Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis

Lectures on Mathematics and Statistics, 1(1):1–194, 2008.

[27] Abraham Albert Ungar. Analytic hyperbolic geometry in n dimensions: An introduction. CRC

Press, 2014.

[28] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. In Proceedings of the International Conference on Learning Representations (ICLR),
2016.

[29] J Vermeer. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic

plane. Topology and its Applications, 152(3):226–242, 2005.

13

A Hyperbolic Trigonometry

Hyperbolic angles. For A, B, C ∈ Dn
c , we denote by ∠A := ∠BAC the angle between the two
geodesics starting from A and ending at B and C respectively. This angle can be deﬁned in two
equivalent ways: i) either using the angle between the initial velocities of the two geodesics as given
by Eq. 5, or ii) using the formula

cos(∠A) =

(cid:28) (−A) ⊕c B
(cid:107)(−A) ⊕c B(cid:107)

,

(−A) ⊕c C
(cid:107)(−A) ⊕c C(cid:107)

(cid:29)

,

In this case, ∠A is also called a gyroangle in the work of [26, section 4].

Hyperbolic law of sines. We state here the hyperbolic law of sines. If for A, B, C ∈ Dn
c , we
denote by ∠B := ∠ABC the angle between the two geodesics starting from B and ending at A and
C respectively, and by ˜c = dc(B, A) the length of the hyperbolic segment BA (and similarly for
others), then we have:

sin(∠A)
√
c˜a)
sinh(

=

sin(∠B)
√
c˜b)
sinh(

=

sin(∠C)
√
c˜c)
sinh(

.

Note that one can also adapt the hyperbolic law of cosines to the hyperbolic space.

B Proof of Theorem 4

Theorem 4.
In the manifold (Dn
to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a vector v ∈ T0Dn
c

c is given by the following isometry:
λc
0
λc
x

0→x(v) = logc
P c

x(x ⊕c expc

0(v)) =

v.

Proof. The geodesic in Dn
v ∈ T0Dn
γ (i.e. X(t) ∈ Tγ(t)Dn

c from 0 to x is given in Eq. (10) by γ(t) = x ⊗c t, for t ∈ [0, 1]. Let
c . Then it is of common knowledge that there exists a unique parallel9 vector ﬁeld X along

c , ∀t ∈ [0, 1]) such that X(0) = v. Let’s deﬁne:
X : t ∈ [0, 1] (cid:55)→ logc

γ(t)(γ(t) ⊕c expc

0(v)) ∈ Tγ(t)Dn
c .

Clearly, X is a vector ﬁeld along γ such that X(0) = v. Now deﬁne
0→x : v ∈ T0Dn
P c

x(x ⊕c expc

0(v)) ∈ TxDn
c .

c (cid:55)→ logc
0→x(v) = λc

c . Since P c

0→x is a linear isometry from T0Dn
v, hence P c
c
0→x(v) = X(1), it is enough to prove that X is parallel in order to guarantee that

From Eq. (12), it is easily seen that P c
to TxDn
c to TxDn
0→x is the parallel transport from T0Dn
P c
c .
Since X is a vector ﬁeld along γ, its covariant derivative can be expressed with the Levi-Civita
connection ∇c associated to gc:

0
λc
x

DX
∂t

= ∇c

˙γ(t)X.

Let’s compute the Levi-Civita connection from its Christoffel symbols. In a local coordinate system,
they can be written as

Γi

jk =

(gc)il(∂jgc

lk + ∂kgc

lj − ∂lgc

jk),

1
2

where superscripts denote the inverse metric tensor and using Einstein’s notations. As gc
at γ(t) ∈ Dn

c this yields:

ij = (λc)2δij,

jk = cλc
Γi

γ(t)(δikγ(t)j + δijγ(t)k − δjkγ(t)i).

9i.e. that DX

∂t = 0 for t ∈ [0, 1], where D

∂t denotes the covariant derivative.

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

14

On the other hand, since X(t) = (λc

∇c

˙γ(t)X = ˙γ(t)i∇c

i X = ˙γ(t)i∇c
i

= vj ˙γ(t)i∇c
i

0/λc

γ(t))v, we have
(cid:32)

(cid:33)

λc
0
λc

γ(t)

v

(cid:32)

λc
0
λc

γ(t)

(cid:33)

ej

.

√

√

Since γ(t) = (1/
Hence there exists K x

c) tanh(t tanh−1(
t ∈ R such that ˙γ(t) = K x

c(cid:107)x(cid:107))) x

(cid:107)x(cid:107) , it is easily seen that ˙γ(t) is colinear to γ(t).
t γ(t). Moreover, we have the following Leibniz rule:
(cid:33)

(cid:32)

(cid:32)

∇c
i

λc
0
λc

γ(t)

(cid:33)

ej

=

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

ej.

Combining these yields

DX
∂t

= K x

t vjγ(t)i

(cid:32)

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

(cid:32)

(cid:33)

(cid:33)

ej

.

Replacing with the Christoffel symbols of ∇c at γ(t) gives

Moreover,

λc
0
λc

γ(t)

λc
0
λc

γ(t)

∇c

i ej =

ijek = 2c[δk
Γk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek.

∂
∂γ(t)i

(cid:32)

(cid:33)

λc
0
λc

γ(t)

ej =

∂
∂γ(t)i

(cid:0)−c(cid:107)γ(t)(cid:107)2(cid:1) ej = −2cγ(t)iej.

Putting together everything, we obtain

DX
∂t

= K x

t vjγ(t)i (cid:0)2c[δk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek − 2cγ(t)iej

(cid:1)

t vjγ(t)i (cid:0)γ(t)jei − δijγ(t)kek
t vj (cid:0)γ(t)jγ(t)iei − γ(t)iδijγ(t)kek
(cid:1)
t vj (cid:0)γ(t)jγ(t)iei − γ(t)jγ(t)kek

(cid:1)

(cid:1)

= 2cK x
= 2cK x
= 2cK x
= 0,

which concludes the proof.

C Proof of Eq. (22)

Proof. Two steps proof:
i) expc

p({a}⊥) ⊆ {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}:

Let z ∈ {a}⊥. From Eq. (12), we have that:

This, together with the left-cancellation law in gyrospaces (see section 2.3), implies that

expc

p(z) = −p ⊕c βz,

for some β ∈ R.

(cid:104)−p ⊕c expc

p(z), a(cid:105) = (cid:104)βz, a(cid:105) = 0

which is what we wanted.

ii) {x ∈ Dn
Let x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0} ⊆ expc
c s.t. (cid:104)−p ⊕c x, a(cid:105) = 0. Then, using Eq. (12), we derive that:
for some β ∈ R,

p(x) = β(−p ⊕c x),

p({a}⊥):

logc

which is orthogonal to a, by assumption. This implies logc

p(x) ∈ {a}⊥, hence x ∈ expc

p({a}⊥).

15

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(54)

D Proof of Theorem 5

Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

1
√
c

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(55)

Proof. We ﬁrst need to prove the following lemma, trivial in the Euclidean space, but not in the
Poincaré ball:
Lemma 7. (Orthogonal projection on a geodesic) Any point in the Poincaré ball has a unique
orthogonal projection on any given geodesic that does not pass through the point. Formally, for all
y ∈ Dn
c and for all geodesics γx→z(·) s.t. y /∈ Im γx→z, there exists an unique w ∈ Im γx→z s.t.
∠(γw→y, γx→z) = π/2.

Proof. We ﬁrst note that any geodesic in Dn
and has two "points at inﬁnity" lying on the ball border (v (cid:54)= 0):

c has the form γ(t) = u ⊕c v ⊗c t as given by Eq. 11,

γ(±∞) = u ⊕c

√

±v
c(cid:107)v(cid:107)

∈ ∂Dn
c .

(56)

Using the notations in the lemma statement, the closed-form of γx→z is given by Eq. (10):

γx→z(t) = x ⊕c (−x ⊕c z) ⊗c t

We denote by x(cid:48), z(cid:48) ∈ ∂Dn
∠ywx(cid:48) is well deﬁned from Eq. (34):

c its points at inﬁnity as described by Eq. (56). Then, the hyperbolic angle

cos(∠(γw→y, γx→z)) = cos(∠ywz(cid:48)) =

(cid:104)−w ⊕c y, −w ⊕c z(cid:48)(cid:105)
(cid:107) − w ⊕c y(cid:107) · (cid:107) − w ⊕c z(cid:48)(cid:107)

.

(57)

We now perform 2 steps for this proof.

i) Existence of w:

The angle function from Eq. (57) is continuous w.r.t t when w = γx→z(t). So we ﬁrst prove existence
of an angle of π/2 by continuously moving w from x(cid:48) to z(cid:48) when t goes from −∞ to ∞, and
observing that cos(∠ywz(cid:48)) goes from −1 to 1 as follows:

cos(∠yx(cid:48)z(cid:48)) = 1 & lim
w→z(cid:48)

cos(∠ywz(cid:48)) = −1.

(58)

The left part of Eq. (58) follows from Eq. (57) and from the fact (easy to show from the deﬁnition
c (which is the case of x(cid:48)). The right part of Eq. (58)
of ⊕c) that a ⊕c b = a, when (cid:107)a(cid:107) = 1/
follows from the fact that ∠ywz(cid:48) = π − ∠ywx(cid:48) (from the conformal property, or from Eq. (34)) and
cos(∠yz(cid:48)x(cid:48)) = 1 (proved as above).
Hence cos(∠ywz(cid:48)) has to pass through 0 when going from −1 to 1, which achieves the proof of
existence.

√

ii) Uniqueness of w:
Assume by contradiction that there are two w and w(cid:48) on γx→z that form angles ∠ywx(cid:48) and ∠yw(cid:48)x(cid:48)
of π/2. Since w, w(cid:48), x(cid:48) are on the same geodesic, we have

π/2 = ∠yw(cid:48)x(cid:48) = ∠yw(cid:48)w = ∠ywx(cid:48) = ∠yw(cid:48)w
So ∆yww(cid:48) has two right angles, but in the Poincaré ball this is impossible.

(59)

Now, we need two more lemmas:
Lemma 8. (Minimizing distance from point to geodesic) The orthogonal projection of a point to
a geodesic (not passing through the point) is minimizing the distance between the point and the
geodesic.

Proof. The proof is similar with the Euclidean case and it’s based on hyperbolic sine law and the fact
that in any right hyperbolic triangle the hypotenuse is strictly longer than any of the other sides.

16

Lemma 9. (Geodesics through p) Let ˜H c
all points on the geodesic γp→w are included in ˜H c

a,p.

a,p be a Poincaré hyperplane. Then, for any w ∈ ˜H c

a,p \ {p},

Proof. γp→w(t) = p ⊕c (−p ⊕c w) ⊗c t. Then, it is easy to check the condition in Eq. (22):

(cid:104)−p ⊕c γp→w(t), a(cid:105) = (cid:104)(−p ⊕c w) ⊗c t, a(cid:105) ∝ (cid:104)−p ⊕c w, a(cid:105) = 0.

(60)

We now turn back to our proof. Let x ∈ Dn
We prove that there is at least one point w∗ ∈ ˜H c

c be an arbitrary point and ˜H c

a,p a Poincaré hyperplane.

a,p that achieves the inﬁmum distance

dc(x, w∗) = inf
w∈ ˜H c

a,p

dc(x, w),

and, moreover, that this distance is the same as the one in the theorem’s statement.
We ﬁrst note that for any point w ∈ ˜H c
and Lemma 9, it is obvious that the projection of x to γp→w will give a strictly lower distance.
Thus, we only consider w ∈ ˜H c
triangle ∆xwp, one gets:

a,p such that ∠xwp = π/2. Applying hyperbolic sine law in the right

a,p, if ∠xwp (cid:54)= π/2, then w (cid:54)= w∗. Indeed, using Lemma 8

dc(x, w) = (1/

c) sinh−1 (cid:0)sinh(

c dc(x, p)) · sin(∠xpw)(cid:1) .

√

√

One of the above quantities does not depend on w:

√

√

sinh(

c dc(x, p)) = sinh(2 tanh−1(

c(cid:107) − p ⊕c x(cid:107))) =

√
2
c(cid:107) − p ⊕c x(cid:107)
1 − c(cid:107) − p ⊕c x(cid:107)2 .

The other quantity is sin(∠xpw) which is minimized when the angle ∠xpw is minimized (be-
cause ∠xpw < π/2 for the hyperbolic right triangle ∆xwp), or, alternatively, when cos(∠xpw) is
maximized. But, we already have from Eq. (34) that:

cos(∠xpw) =

(cid:104)−p ⊕c x, −p ⊕c w(cid:105)
(cid:107) − p ⊕c x(cid:107) · (cid:107) − p ⊕c w(cid:107)

.

To maximize the above, the constraint on the right angle at w can be dropped because cos(∠xpw)
depends only on the geodesic γp→w and not on w itself, and because there is always an orthogonal
projection from any point x to any geodesic as stated by Lemma 7. Thus, it remains to ﬁnd the
maximum of Eq. (64) when w ∈ ˜H c
a,p from Eq. (22), one can easily
prove that

a,p. Using the deﬁnition of ˜H c

Using that fact that logc

p(w)/(cid:107) logc

p(w)(cid:107) = −p ⊕c w/(cid:107) − p ⊕c w(cid:107), we just have to ﬁnd

and we are left with a well known Euclidean problem which is equivalent to ﬁnding the minimum
angle between the vector −p ⊕c x (viewed as Euclidean) and the hyperplane {a}⊥. This angle
is given by the Euclidean orthogonal projection whose sin value is the distance from the vector’s
endpoint to the hyperplane divided by the vector’s length:

{logc

p(w) : w ∈ ˜H c

a,p} = {a}⊥.

max
z∈{a}⊥

(cid:18) (cid:104)−p ⊕c x, z(cid:105)

(cid:107) − p ⊕c x(cid:107) · (cid:107)z(cid:107)

(cid:19)

,

sin(∠xpw∗) =

|(cid:104)−p ⊕c x, a
(cid:107) − p ⊕c x(cid:107)

(cid:107)a(cid:107) (cid:105)|

.

17

It follows that a point w∗ ∈ ˜H c
Eqs. (61),(62),(63) and (67) concludes the proof.

a,p satisfying Eq. (67) exists (but might not be unique). Combining

(61)

(62)

(63)

(64)

(65)

(66)

(67)

(cid:3)

E Derivation of the Hyperbolic GRU Update-gate

In [24], the authors recover the update/forget-gate mechanism of a GRU/LSTM by requiring that the
class of neural networks given by the chosen architecture be invariant to time-warpings. The idea is
the following.

Recovering the update-gate from time-warping. A naive RNN is given by the equation

h(t + 1) = ϕ(W h(t) + U x(t) + b)

Let’s drop the bias b to simplify notations. If h is seen as a differentiable function of time, then a
ﬁrst-order Taylor development gives h(t + δt) ≈ h(t) + δt dh
dt (t) for small δt. Combining this for
δt = 1 with the naive RNN equation, one gets

dh
dt

dα
dt

(t) = ϕ(W h(t) + U x(t)) − h(t).

As this is written for any t, one can replace it by t ← α(t) where α is a (smooth) increasing function
of t called the time-warping. Denoting by ˜h(t) := h(α(t)) and ˜x(t) := x(α(t)), using the chain rule
d˜h
dt (t) = dα

dt (α(t)), one gets

dt (t) dh

d˜h
dt

dα
dt

(t) =

(t)ϕ(W ˜h(t) + U ˜x(t)) −

(t)˜h(t).

(70)

Removing the tildas to simplify notations, discretizing back with dh

dt (t) ≈ h(t + 1) − h(t) yields

h(t + 1) =

(t)ϕ(W h(t) + U x(t)) +

1 −

(t)

h(t).

(71)

dα
dt

(cid:18)

(cid:19)

dα
dt

Requiring that our class of neural networks be invariant to time-warpings means that this class should
contain RNNs deﬁned by Eq. (71), i.e. that dα
dt (t) can be learned. As this is a positive quantity, we
can parametrize it as z(t) = σ(W zh(t) + U zx(t)), recovering the forget-gate equation:

h(t + 1) = z(t)ϕ(W h(t) + U x(t)) + (1 − z(t))h(t).

Adapting this idea to hyperbolic RNNs. The gyroderivative [4] of a map h : R → Dn
as

c is deﬁned

dh
dt

(t) = lim
δt→0

1
δt

⊗c (−h(t) ⊕c h(t + δt)).

Using Möbius scalar associativity and the left-cancellation law leads us to

h(t + δt) ≈ h(t) ⊕c δt ⊗c

(t),

dh
dt

for small δt. Combining this with the equation of a simple hyperbolic RNN of Eq. (29) with δt = 1,
one gets

dh
dt

(t) = −h(t) ⊕c ϕ⊗c(W ⊗c h(t) ⊕c U ⊗c x(t)).

For the next step, we need the following lemma:
Lemma 10 (Gyro-chain-rule). For α : R → R differentiable and h : R → Dn
gyro-derivative, if ˜h := h ◦ α, then we have

c with a well-deﬁned

(68)

(69)

(72)

(73)

(74)

(75)

(76)

where dα

dt (t) denotes the usual derivative.

d˜h
dt

(t) =

(t) ⊗c

(α(t)),

dα
dt

dh
dt

18

(77)

(78)

(79)

(80)

(81)

Proof.

d˜h
dt

(t) = lim
δt→0

1
δt
1
δt

⊗c [−˜h(t) ⊕c

˜h(t + δt)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

= lim
δt→0

= lim
δt→0

= lim
δt→0

= lim
u→0
dα
dt

=

α(cid:48)(t) + O(δt)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
u

(t) ⊗c

(α(t))

dh
dt

⊗c [−h(α(t)) ⊕c h(α(t) + u)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

(Möbius scalar associativity) (82)

where we set u = δt(α(cid:48)(t) + O(δt)), with u → 0 when δt → 0, which concludes.

Using lemma 10 and Eq. (75), with similar notations as in Eq. (70) we have

d˜h
dt

dα
dt

(t) =

(t) ⊗c (−˜h(t) ⊕c ϕ⊗c(W ⊗c

˜h(t) ⊕c U ⊗c ˜x(t))).

(83)

Finally, discretizing back with Eq. (74), using the left-cancellation law and dropping the tildas yields

h(t + 1) = h(t) ⊕c

(t) ⊗c (−h(t) ⊕c ϕ⊗c (W ⊗c h(t) ⊕c U ⊗c x(t))).

(84)

dα
dt

Since α is a time-warping, by deﬁnition its derivative is positive and one can choose to parametrize
it with an update-gate zt (a scalar) deﬁned with a sigmoid. Generalizing this scalar scaling by the
Möbius version of the pointwise scaling (cid:12) yields the Möbius matrix scaling diag(zt) ⊗c ·, leading to
our proposed Eq. (33) for the hyperbolic GRU.

F More Experimental Investigations

The following empirical facts were observed for both hyperbolic RNNs and GRUs.

We observed that, in the hyperbolic setting, accuracy is often much higher when sentence embeddings
can go close to the border (hyperbolic "inﬁnity"), hence exploiting the hyperbolic nature of the space.
Moreover, the faster the two sentence norms go to 1, the more it’s likely that a good local minima
was reached. See ﬁgures 3 and 5.

We often observe that test accuracy starts increasing exactly when sentence embedding norms do.
However, in the hyperbolic setting, the sentence embeddings norms remain close to 0 for a few
epochs, which does not happen in the Euclidean case. See ﬁgures 3, 5 and 4. This mysterious fact
was also exhibited in a similar way by [21] which suggests that the model ﬁrst has to adjust the
angular layout in the almost Euclidean vicinity of 0 before increasing norms and fully exploiting
hyperbolic geometry.

19

(a) Test accuracy

(a) Test accuracy

20

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 3: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the same
architecture: hyperbolic GRU followed by hyperbolic FFNN and hyperbolic/Euclidean (half-half)
MLR. The X axis shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 4: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: Euclidean GRU followed by Euclidean FFNN and Euclidean MLR. The X axis
shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 5: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: hyperbolic RNN followed by hyperbolic FFNN and hyperbolic MLR. The X axis
shows millions of training examples processed.

(a) Test accuracy

21

8
1
0
2
 
n
u
J
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
2
1
1
9
0
.
5
0
8
1
:
v
i
X
r
a

Hyperbolic Neural Networks

Octavian-Eugen Ganea∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
octavian.ganea@inf.ethz.ch

Gary Bécigneul∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
gary.becigneul@inf.ethz.ch

Thomas Hofmann
Department of Computer Science
ETH Zürich
Zurich, Switzerland
thomas.hofmann@inf.ethz.ch

Abstract

Hyperbolic spaces have recently gained momentum in the context of machine
learning due to their high capacity and tree-likeliness properties. However, the
representational power of hyperbolic geometry is not yet on par with Euclidean
geometry, mostly because of the absence of corresponding hyperbolic neural
network layers. This makes it hard to use hyperbolic embeddings in downstream
tasks. Here, we bridge this gap in a principled manner by combining the formalism
of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model
of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep
learning tools: multinomial logistic regression, feed-forward and recurrent neural
networks such as gated recurrent units. This allows to embed sequential data and
perform classiﬁcation in the hyperbolic space. Empirically, we show that, even if
hyperbolic optimization tools are limited, hyperbolic sentence embeddings either
outperform or are on par with their Euclidean variants on textual entailment and
noisy-preﬁx recognition tasks.

1

Introduction

It is common in machine learning to represent data as being embedded in the Euclidean space Rn. The
main reason for such a choice is simply convenience, as this space has a vectorial structure, closed-
form formulas of distance and inner-product, and is the natural generalization of our intuition-friendly,
visual three-dimensional space. Moreover, embedding entities in such a continuous space allows to
feed them as input to neural networks, which has led to unprecedented performance on a broad range
of problems, including sentiment detection [15], machine translation [3], textual entailment [22] or
knowledge base link prediction [20, 6].

Despite the success of Euclidean embeddings, recent research has proven that many types of com-
plex data (e.g. graph data) from a multitude of ﬁelds (e.g. Biology, Network Science, Computer
Graphics or Computer Vision) exhibit a highly non-Euclidean latent anatomy [8]. In such cases, the
Euclidean space does not provide the most powerful or meaningful geometrical representations. For
example, [10] shows that arbitrary tree structures cannot be embedded with arbitrary low distortion
(i.e. almost preserving their metric) in the Euclidean space with unbounded number of dimensions,
but this task becomes surprisingly easy in the hyperbolic space with only 2 dimensions where the
exponential growth of distances matches the exponential growth of nodes with the tree depth.

∗Equal contribution.

The adoption of neural networks and deep learning in these non-Euclidean settings has been rather
limited until very recently, the main reason being the non-trivial or impossible principled general-
izations of basic operations (e.g. vector addition, matrix-vector multiplication, vector translation,
vector inner product) as well as, in more complex geometries, the lack of closed form expressions for
basic objects (e.g. distances, geodesics, parallel transport). Thus, classic tools such as multinomial
logistic regression (MLR), feed forward (FFNN) or recurrent neural networks (RNN) did not have a
correspondence in these geometries.

How should one generalize deep neural models to non-Euclidean domains ? In this paper we address
this question for one of the simplest, yet useful, non-Euclidean domains: spaces of constant negative
curvature, i.e. hyperbolic. Their tree-likeness properties have been extensively studied [12, 13, 26]
and used to visualize large taxonomies [18] or to embed heterogeneous complex networks [17]. In
machine learning, recently, hyperbolic representations greatly outperformed Euclidean embeddings
for hierarchical, taxonomic or entailment data [21, 10, 11]. Disjoint subtrees from the latent hierar-
chical structure surprisingly disentangle and cluster in the embedding space as a simple reﬂection of
the space’s negative curvature. However, appropriate deep learning tools are needed to embed feature
data in this space and use it in downstream tasks. For example, implicitly hierarchical sequence data
(e.g. textual entailment data, phylogenetic trees of DNA sequences or hierarchial captions of images)
would beneﬁt from suitable hyperbolic RNNs.

The main contribution of this paper is to bridge the gap between hyperbolic and Euclidean geometry
in the context of neural networks and deep learning by generalizing in a principled manner both the
basic operations as well as multinomial logistic regression (MLR), feed-forward (FFNN), simple and
gated (GRU) recurrent neural networks (RNN) to the Poincaré model of the hyperbolic geometry.
We do it by connecting the theory of gyrovector spaces and generalized Möbius transformations
introduced by [2, 26] with the Riemannian geometry properties of the manifold. We smoothly
parametrize basic operations and objects in all spaces of constant negative curvature using a uniﬁed
framework that depends only on the curvature value. Thus, we show how Euclidean and hyperbolic
spaces can be continuously deformed into each other. On a series of experiments and datasets we
showcase the effectiveness of our hyperbolic neural network layers compared to their "classic"
Euclidean variants on textual entailment and noisy-preﬁx recognition tasks. We hope that this paper
will open exciting future directions in the nascent ﬁeld of Geometric Deep Learning.

2 The Geometry of the Poincaré Ball

2.1 Basics of Riemannian geometry

We brieﬂy introduce basic concepts of differential geometry largely needed for a principled general-
ization of Euclidean neural networks. For more rigorous and in-depth expositions, see [23, 14].
An n-dimensional manifold M is a space that can locally be approximated by Rn: it is a generalization
to higher dimensions of the notion of a 2D surface. For x ∈ M, one can deﬁne the tangent space
TxM of M at x as the ﬁrst order linear approximation of M around x. A Riemannian metric
g = (gx)x∈M on M is a collection of inner-products gx : TxM × TxM → R varying smoothly
with x. A Riemannian manifold (M, g) is a manifold M equipped with a Riemannian metric g.
Although a choice of a Riemannian metric g on M only seems to deﬁne the geometry locally on M,
it induces global distances by integrating the length (of the speed vector living in the tangent space)
of a shortest path between two points:

(cid:90) 1

(cid:113)

d(x, y) = inf
γ

0

gγ(t)( ˙γ(t), ˙γ(t))dt,

(1)

where γ ∈ C∞([0, 1], M) is such that γ(0) = x and γ(1) = y. A smooth path γ of minimal length
between two points x and y is called a geodesic, and can be seen as the generalization of a straight-line
in Euclidean space. The parallel transport Px→y : TxM → TyM is a linear isometry between
tangent spaces which corresponds to moving tangent vectors along geodesics and deﬁnes a canonical
way to connect tangent spaces. The exponential map expx at x, when well-deﬁned, gives a way to
project back a vector v of the tangent space TxM at x, to a point expx(v) ∈ M on the manifold.
This map is often used to parametrize a geodesic γ starting from γ(0) := x ∈ M with unit-norm
direction ˙γ(0) := v ∈ TxM as t (cid:55)→ expx(tv). For geodesically complete manifolds, such as the
Poincaré ball considered in this work, expx is well-deﬁned on the full tangent space TxM. Finally, a

2

(2)

(3)

(4)

(5)

metric ˜g is said to be conformal to another metric g if it deﬁnes the same angles, i.e.

˜gx(u, v)
(cid:112)˜gx(u, u)(cid:112)˜gx(v, v)

=

gx(u, v)
(cid:112)gx(u, u)(cid:112)gx(v, v)

,

for all x ∈ M, u, v ∈ TxM \ {0}. This is equivalent to the existence of a smooth function
λ : M → R, called the conformal factor, such that ˜gx = λ2

xgx for all x ∈ M.

2.2 Hyperbolic space: the Poincaré ball

The hyperbolic space has ﬁve isometric models that one can work with [9]. Similarly as in [21] and
[11], we choose to work in the Poincaré ball. The Poincaré ball model (Dn, gD) is deﬁned by the
manifold Dn = {x ∈ Rn : (cid:107)x(cid:107) < 1} equipped with the following Riemannian metric:

gD
x = λ2

xgE, where λx :=

2
1 − (cid:107)x(cid:107)2 ,

gE = In being the Euclidean metric tensor. Note that the hyperbolic metric tensor is conformal to
the Euclidean one. The induced distance between two points x, y ∈ Dn is known to be given by

dD(x, y) = cosh−1

1 + 2

(cid:18)

(cid:107)x − y(cid:107)2
(1 − (cid:107)x(cid:107)2)(1 − (cid:107)y(cid:107)2)

(cid:19)

.

Since the Poincaré ball is conformal to Euclidean space, the angle between two vectors u, v ∈
TxDn \ {0} is given by

cos(∠(u, v)) =

gD
x (u, v)
x (u, u)(cid:112)gD

(cid:112)gD

x (v, v)

=

(cid:104)u, v(cid:105)
(cid:107)u(cid:107)(cid:107)v(cid:107)

.

2.3 Gyrovector spaces

In Euclidean space, natural operations inherited from the vectorial structure, such as vector addition,
subtraction and scalar multiplication are often useful. The framework of gyrovector spaces provides
an elegant non-associative algebraic formalism for hyperbolic geometry just as vector spaces provide
the algebraic setting for Euclidean geometry [2, 25, 26].

In particular, these operations are used in special relativity, allowing to add speed vectors belonging
to the Poincaré ball of radius c (the celerity, i.e. the speed of light) so that they remain in the ball,
hence not exceeding the speed of light.

We will make extensive use of these operations in our deﬁnitions of hyperbolic neural networks.
For c ≥ 0, denote2 by Dn
then Dn

c := {x ∈ Rn | c(cid:107)x(cid:107)2 < 1}. Note that if c = 0, then Dn
c. If c = 1 then we recover the usual ball Dn.

c is the open ball of radius 1/

c = Rn; if c > 0,

√

Möbius addition. The Möbius addition of x and y in Dn

c is deﬁned as

x ⊕c y :=

(1 + 2c(cid:104)x, y(cid:105) + c(cid:107)y(cid:107)2)x + (1 − c(cid:107)x(cid:107)2)y
1 + 2c(cid:104)x, y(cid:105) + c2(cid:107)x(cid:107)2(cid:107)y(cid:107)2

.

(6)

In particular, when c = 0, one recovers the Euclidean addition of two vectors in Rn. Note that
without loss of generality, the case c > 0 can be reduced to c = 1. Unless stated otherwise, we
will use ⊕ as ⊕1 to simplify notations. For general c > 0, this operation is not commutative nor
associative. However, it satisﬁes x ⊕c 0 = 0 ⊕c x = 0. Moreover, for any x, y ∈ Dn
c , we have
(−x) ⊕c x = x ⊕c (−x) = 0 and (−x) ⊕c (x ⊕c y) = y (left-cancellation law). The Möbius
substraction is then deﬁned by the use of the following notation: x (cid:9)c y := x ⊕c (−y). See [29,
section 2.1] for a geometric interpretation of the Möbius addition.

2We take different notations as in [25] where the author uses s = 1/

c.

√

3

Möbius scalar multiplication. For c > 0, the Möbius scalar multiplication of x ∈ Dn
r ∈ R is deﬁned as

c \ {0} by

r ⊗c x := (1/

c) tanh(r tanh−1(

c(cid:107)x(cid:107)))

√

√

x
(cid:107)x(cid:107)

,

(7)

and r ⊗c 0 := 0. Note that similarly as for the Möbius addition, one recovers the Euclidean scalar
multiplication when c goes to zero: limc→0 r ⊗c x = rx. This operation satisﬁes desirable properties
such as n ⊗c x = x ⊕c · · · ⊕c x (n additions), (r + r(cid:48)) ⊗c x = r ⊗c x ⊕c r(cid:48) ⊗c x (scalar distributivity3),
(rr(cid:48)) ⊗c x = r ⊗c (r(cid:48) ⊗c x) (scalar associativity) and |r| ⊗c x/(cid:107)r ⊗c x(cid:107) = x/(cid:107)x(cid:107) (scaling property).

c , gc) is given by4

Distance.
Euclidean one, with conformal factor λc
(Dn

If one deﬁnes the generalized hyperbolic metric tensor gc as the metric conformal to the
x := 2/(1 − c(cid:107)x(cid:107)2), then the induced distance function on
√

c) tanh−1 (cid:0)√
Again, observe that limc→0 dc(x, y) = 2(cid:107)x − y(cid:107), i.e. we recover Euclidean geometry in the limit5.
Moreover, for c = 1 we recover dD of Eq. (4).

c(cid:107) − x ⊕c y(cid:107)(cid:1) .

dc(x, y) = (2/

(8)

Hyperbolic trigonometry. Similarly as in the Euclidean space, one can deﬁne the notions of
hyperbolic angles or gyroangles (when using the ⊕c), as well as hyperbolic law of sines in the
generalized Poincaré ball (Dn

c , gc). We make use of these notions in our proofs. See Appendix A.

2.4 Connecting Gyrovector spaces and Riemannian geometry of the Poincaré ball

In this subsection, we present how geodesics in the Poincaré ball model are usually described with
Möbius operations, and push one step further the existing connection between gyrovector spaces and
the Poincaré ball by ﬁnding new identities involving the exponential map, and parallel transport.

In particular, these ﬁndings provide us with a simpler formulation of Möbius scalar multiplication,
yielding a natural deﬁnition of matrix-vector multiplication in the Poincaré ball.

Riemannian gyroline element. The Riemannian gyroline element is deﬁned for an inﬁnitesimal
dx as ds := (x + dx) (cid:9)c x, and its size is given by [26, section 3.7]:

(cid:107)ds(cid:107) = (cid:107)(x + dx) (cid:9)c x(cid:107) = (cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2).

(9)

What is remarkable is that it turns out to be identical, up to a scaling factor of 2, to the usual line
element 2(cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2) of the Riemannian manifold (Dn

c , gc).

Geodesics. The geodesic connecting points x, y ∈ Dn

c is shown in [2, 26] to be given by:

γx→y(t) := x ⊕c (−x ⊕c y) ⊗c t, with γx→y : R → Dn

c s.t. γx→y(0) = x and γx→y(1) = y.

Note that when c goes to 0, geodesics become straight-lines, recovering Euclidean geometry. In the
remainder of this subsection, we connect the gyrospace framework with Riemannian geometry.
Lemma 1. For any x ∈ Dn and v ∈ TxDn
c s.t. gc
x with direction v is given by:

x(v, v) = 1, the unit-speed geodesic starting from

γx,v(t) = x ⊕c

tanh

(cid:18)

(cid:18)√

(cid:19) v
√

c

t
2

c(cid:107)v(cid:107)

(cid:19)

, where γx,v : R → Dn s.t. γx,v(0) = x and ˙γx,v(0) = v.

(10)

(11)

Proof. One can use Eq. (10) and reparametrize it to unit-speed using Eq. (8). Alternatively, direct
computation and identiﬁcation with the formula in [11, Thm. 1] would give the same result. Using
Eq. (8) and Eq. (11), one can sanity-check that dc(γ(0), γ(t)) = t, ∀t ∈ [0, 1].

3⊗c has priority over ⊕c in the sense that a ⊗c b ⊕c c := (a ⊗c b) ⊕c c and a ⊕c b ⊗c c := a ⊕c (b ⊗c c).
4The notation −x ⊕c y should always be read as (−x) ⊕c y and not −(x ⊕c y).
5The factor 2 comes from the conformal factor λx = 2/(1 − (cid:107)x(cid:107)2), which is a convention setting the

curvature to −1.

4

Exponential and logarithmic maps. The following lemma gives the closed-form derivation of
exponential and logarithmic maps.
Lemma 2. For any point x ∈ Dn
map logc
c → TxDn
(cid:18)

c are given for v (cid:54)= 0 and y (cid:54)= x by:
(cid:18)√

c , the exponential map expc

c and the logarithmic

x : TxDn

c → Dn

x : Dn

√

(cid:19)

, logc

x(y) =

√

tanh−1(

c(cid:107) − x ⊕c y(cid:107))

expc

x(v) = x ⊕c

tanh

c

λc
x(cid:107)v(cid:107)
2

(cid:19) v
√

c(cid:107)v(cid:107)

2
cλc
x

−x ⊕c y
(cid:107) − x ⊕c y(cid:107)
(12)

.

Proof. Following the proof of [11, Cor. 1.1], one gets expc
gives the formula for expc

x. Algebraic check of the identity logc

x(v) = γx,
x(expc

v

x(cid:107)v(cid:107) (λc
λc

x(cid:107)v(cid:107)). Using Eq. (11)

x(v)) = v concludes.

The above maps have more appealing forms when x = 0, namely for v ∈ T0Dn

c \ {0}, y ∈ Dn

c \ {0}:

expc

0(v) = tanh(

c(cid:107)v(cid:107))

√

, logc

0(y) = tanh−1(

c(cid:107)y(cid:107))

√

(13)

√

y
c(cid:107)y(cid:107)

.

√

v
c(cid:107)v(cid:107)

Moreover, we still recover Euclidean geometry in the limit c → 0, as limc→0 expc
Euclidean exponential map, and limc→0 logc

x(y) = y − x is the Euclidean logarithmic map.

x(v) = x + v is the

Möbius scalar multiplication using exponential and logarithmic maps. We studied the expo-
nential and logarithmic maps in order to gain a better understanding of the Möbius scalar multiplica-
tion (Eq. (7)). We found the following:
Lemma 3. The quantity r ⊗ x can actually be obtained by projecting x in the tangent space at 0
with the logarithmic map, multiplying this projection by the scalar r in T0Dn
c , and then projecting it
back on the manifold with the exponential map:
0(r logc

∀r ∈ R, x ∈ Dn
c .

r ⊗c x = expc

0(x)),

(14)

In addition, we recover the well-known relation between geodesics connecting two points and the
exponential map:

γx→y(t) = x ⊕c (−x ⊕c y) ⊗c t = expc

x(t logc

x(y)),

t ∈ [0, 1].

(15)

This last result enables us to generalize scalar multiplication in order to deﬁne matrix-vector multipli-
cation between Poincaré balls, one of the essential building blocks of hyperbolic neural networks.

Parallel transport. Finally, we connect parallel transport (from T0Dn
the following theorem, which we prove in appendix B.
Theorem 4. In the manifold (Dn
vector v ∈ T0Dn

c to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a
c is given by the following isometry:
λc
0
λc
x

x(x ⊕c expc

0(v)) =

0→x(v) = logc
P c

(16)

v.

c ) to gyrovector spaces with

As we’ll see later, this result is crucial in order to deﬁne and optimize parameters shared between
different tangent spaces, such as biases in hyperbolic neural layers or parameters of hyperbolic MLR.

3 Hyperbolic Neural Networks

Neural networks can be seen as being made of compositions of basic operations, such as linear
maps, bias translations, pointwise non-linearities and a ﬁnal sigmoid or softmax layer. We ﬁrst
explain how to construct a softmax layer for logits lying in a Poincaré ball. Then, we explain how
to transform a mapping between two Euclidean spaces as one between Poincaré balls, yielding
matrix-vector multiplication and pointwise non-linearities in the Poincaré ball. Finally, we present
possible adaptations of various recurrent neural networks to the hyperbolic domain.

5

3.1 Hyperbolic multiclass logistic regression

In order to perform multi-class classiﬁcation on the Poincaré ball, one needs to generalize multinomial
logistic regression (MLR) − also called softmax regression − to the Poincaré ball.

Reformulating Euclidean MLR. Let’s ﬁrst reformulate Euclidean MLR from the perspective of
distances to margin hyperplanes, as in [19, Section 5]. This will allow us to easily generalize it.

Given K classes, one learns a margin hyperplane for each such class using softmax probabilities:

∀k ∈ {1, ..., K},

p(y = k|x) ∝ exp (((cid:104)ak, x(cid:105) − bk)) , where bk ∈ R, x, ak ∈ Rn.

(17)

Note that any afﬁne hyperplane in Rn can be written with a normal vector a and a scalar shift b:

Ha,b = {x ∈ Rn : (cid:104)a, x(cid:105) − b = 0}, where a ∈ Rn \ {0}, and b ∈ R.

(18)

As in [19, Section 5], we note that (cid:104)a, x(cid:105) − b = sign((cid:104)a, x(cid:105) − b)(cid:107)a(cid:107)d(x, Ha,b). Using Eq. (17):

p(y = k|x) ∝ exp(sign((cid:104)ak, x(cid:105) − bk)(cid:107)ak(cid:107)d(x, Hak,bk )), bk ∈ R, x, ak ∈ Rn.

(19)

As it is not immediately obvious how to generalize the Euclidean hyperplane of Eq. (18) to other
spaces such as the Poincaré ball, we reformulate it as follows:

˜Ha,p = {x ∈ Rn : (cid:104)−p + x, a(cid:105) = 0} = p + {a}⊥, where p ∈ Rn, a ∈ Rn \ {0}.

(20)

This new deﬁnition relates to the previous one as ˜Ha,p = Ha,(cid:104)a,p(cid:105). Rewriting Eq. (19) with b = (cid:104)a, p(cid:105):
p(y = k|x) ∝ exp(sign((cid:104)−pk + x, ak(cid:105))(cid:107)ak(cid:107)d(x, ˜Hak,pk )), with pk, x, ak ∈ Rn.

(21)

It is now natural to adapt the previous deﬁnition to the hyperbolic setting by replacing + by ⊕c:
Deﬁnition 3.1 (Poincaré hyperplanes). For p ∈ Dn
p(z, a) = 0} = {z ∈ TpDn
gc

c : (cid:104)z, a(cid:105) = 0}. Then, we deﬁne Poincaré hyperplanes as

c \ {0}, let {a}⊥ := {z ∈ TpDn
c :

c , a ∈ TpDn

˜H c

a,p := {x ∈ Dn

c : (cid:104)logc

p(x), a(cid:105)p = 0} = expc

p({a}⊥) = {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}.

(22)

The last equality is shown appendix C. ˜H c
all geodesics in Dn
hypergyroplanes, see [27, deﬁnition 5.8]. A 3D hyperplane example is depicted in Fig. 1.

a,p can also be described as the union of images of
c orthogonal to a and containing p. Notice that our deﬁnition matches that of

Next, we need the following theorem, proved in appendix D:
Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(23)

Final formula for MLR in the Poincaré ball. Putting together Eq. (21) and Thm. 5, we get the
hyperbolic MLR formulation. Given K classes and k ∈ {1, . . . , K}, pk ∈ Dn
c \ {0}:

c , ak ∈ Tpk

Dn

p(y = k|x) ∝ exp(sign((cid:104)−pk ⊕c x, ak(cid:105))

gc
pk

(ak, ak)dc(x, ˜H c

)),

ak,pk

∀x ∈ Dn
c ,

(24)

or, equivalently

p(y = k|x) ∝ exp

(cid:18) λc
pk

(cid:107)ak(cid:107)
√
c

sinh−1

(cid:18)

√
2

c(cid:104)−pk ⊕c x, ak(cid:105)

(cid:19)(cid:19)

(1 − c(cid:107) − pk ⊕c x(cid:107)2)(cid:107)ak(cid:107)

,

∀x ∈ Dn
c .

(25)

this goes to p(y = k|x) ∝ exp(4(cid:104)−pk + x, ak(cid:105)) =

Notice that when c goes to zero,
exp((λ0
pk

)2(cid:104)−pk + x, ak(cid:105)) = exp((cid:104)−pk + x, ak(cid:105)0), recovering the usual Euclidean softmax.
However, at this point it is unclear how to perform optimization over ak, since it lives in Tpk
hence depends on pk. The solution is that one should write ak = P c
)a(cid:48)
k ∈ T0Dn
a(cid:48)

c = Rn, and optimize a(cid:48)

k as a Euclidean parameter.

k) = (λc

0/λc
pk

0→pk

(a(cid:48)

Dn
c and
k, where

1
√
c

(cid:113)

6

3.2 Hyperbolic feed-forward layers

In order to deﬁne hyperbolic neural networks, it is crucial to de-
ﬁne a canonically simple parametric family of transformations,
playing the role of linear mappings in usual Euclidean neural
networks, and to know how to apply pointwise non-linearities.
Inspiring ourselves from our reformulation of Möbius scalar
multiplication in Eq. (14), we deﬁne:
Deﬁnition 3.2 (Möbius version). For f : Rn → Rm, we deﬁne
the Möbius version of f as the map from Dn

c to Dm

c by:

f ⊗c(x) := expc

0(f (logc

0(x))),

(26)

where expc

0 : T0m

Dm

c → Dm

c and logc

0 : Dn

c → T0n

Dn
c .

Figure 1: An example of a hyper-
bolic hyperplane in D3
1 plotted us-
ing sampling. The red point is p.
The shown normal axis to the hy-
perplane through p is parallel to a.

Note that similarly as for other Möbius operations, we recover
the Euclidean mapping in the limit c → 0 if f is continuous, as limc→0 f ⊗c(x) = f (x). This
deﬁnition satisﬁes a few desirable properties too, such as: (f ◦ g)⊗c = f ⊗c ◦ g⊗c for f : Rm → Rl
and g : Rn → Rm (morphism property), and f ⊗c(x)/(cid:107)f ⊗c(x)(cid:107) = f (x)/(cid:107)f (x)(cid:107) for f (x) (cid:54)= 0
(direction preserving). It is then straight-forward to prove the following result:
Lemma 6 (Möbius matrix-vector multiplication). If M : Rn → Rm is a linear map, which we
identify with its matrix representation, then ∀x ∈ Dn

c , if M x (cid:54)= 0 we have

M ⊗c(x) = (1/

c) tanh

√

(cid:18) (cid:107)M x(cid:107)
(cid:107)x(cid:107)

√

tanh−1(

c(cid:107)x(cid:107))

(cid:19) M x
(cid:107)M x(cid:107)

,

(27)

and M ⊗c(x) = 0 if M x = 0. Moreover, if we deﬁne the Möbius matrix-vector multiplication of
M ∈ Mm,n(R) and x ∈ Dn
c by M ⊗c x := M ⊗c(x), then we have (M M (cid:48)) ⊗c x = M ⊗c (M (cid:48) ⊗c x)
for M ∈ Ml,m(R) and M (cid:48) ∈ Mm,n(R) (matrix associativity), (rM ) ⊗c x = r ⊗c (M ⊗c x) for
r ∈ R and M ∈ Mm,n(R) (scalar-matrix associativity) and M ⊗c x = M x for all M ∈ On(R)
(rotations are preserved).

Pointwise non-linearity.
ϕ⊗c can be applied to elements of the Poincaré ball.

If ϕ : Rn → Rn is a pointwise non-linearity, then its Möbius version

Bias translation. The generalization of a translation in the Poincaré ball is naturally given by
moving along geodesics. But should we use the Möbius sum x ⊕c b with a hyperbolic bias b or the
x(b(cid:48)) with a Euclidean bias b(cid:48)? These views are uniﬁed with parallel transport
exponential map expc
c by a bias b ∈ Dn
(see Thm 4). Möbius translation of a point x ∈ Dn
(cid:18) λc
0
λc
x

c is given by
(cid:19)

x ← x ⊕c b = expc

0(b))) = expc
x

0→x(logc

x(P c

logc

0(b)

(28)

.

We recover Euclidean translations in the limit c → 0. Note that bias translations play a particular
Indeed, consider multiple layers of the form fk(x) = ϕk(Mkx), each of
role in this model.
which having Möbius version f ⊗c
k (Mk ⊗c x). Then their composition can be re-written
f ⊗c
k ◦ · · · ◦ f ⊗c
1 = expc
0. This means that these operations can essentially be
performed in Euclidean space. Therefore, it is the interposition between those with the bias translation
of Eq. (28) which differentiates this model from its Euclidean counterpart.

k (x) = ϕ⊗c
0 ◦fk ◦ · · · ◦ f1 ◦ logc

If a vector x ∈ Rn+p is the (vertical) concatenation
Concatenation of multiple input vectors.
of two vectors x1 ∈ Rn, x2 ∈ Rp, and M ∈ Mm,n+p(R) can be written as the (horizontal)
concatenation of two matrices M1 ∈ Mm,n(R) and M2 ∈ Mm,p(R), then M x = M1x1 + M2x2.
We generalize this to hyperbolic spaces: if we are given x1 ∈ Dn
c ×Dp
c ,
and M, M1, M2 as before, then we deﬁne M ⊗c x := M1 ⊗c x1 ⊕c M2 ⊗c x2. Note that when c goes
to zero, we recover the Euclidean formulation, as limc→0 M ⊗c x = limc→0 M1 ⊗c x1 ⊕c M2 ⊗c x2 =
M1x1 + M2x2 = M x. Moreover, hyperbolic vectors x ∈ Dn
c can also be "concatenated" with real
features y ∈ R by doing: M ⊗c x ⊕c y ⊗c b with learnable b ∈ Dm

c , x = (x1 x2)T ∈ Dn

c and M ∈ Mm,n(R).

c , x2 ∈ Dp

7

3.3 Hyperbolic RNN

Naive RNN. A simple RNN can be deﬁned by ht+1 = ϕ(W ht + U xt + b) where ϕ is a pointwise
non-linearity, typically tanh, sigmoid, ReLU, etc. This formula can be naturally generalized to the
hyperbolic space as follows. For parameters W ∈ Mm,n(R), U ∈ Mm,d(R), b ∈ Dm
c , we deﬁne:

ht+1 = ϕ⊗c (W ⊗c ht ⊕c U ⊗c xt ⊕c b),

ht ∈ Dn

c , xt ∈ Dd
c .

(29)

Note that if inputs xt’s are Euclidean, one can write ˜xt := expc
expc

(U xt)) = W ⊗c ht ⊕c expc

(P c

W ⊗cht

0→W ⊗cht

0(U xt) = W ⊗c ht ⊕c U ⊗c ˜xt.

0(xt) and use the above formula, since

GRU architecture. One can also adapt the GRU architecture:
rt = σ(W rht−1 + U rxt + br),
zt = σ(W zht−1 + U zxt + bz),
˜ht = ϕ(W (rt (cid:12) ht−1) + U xt + b), ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht,

(30)

where (cid:12) denotes pointwise product. First, how should we adapt the pointwise multiplication by a
scaling gate? Note that the deﬁnition of the Möbius version (see Eq. (26)) can be naturally extended
to maps f : Rn × Rp → Rm as f ⊗c : (h, h(cid:48)) ∈ Dn
0(h(cid:48)))). In
c (cid:55)→ expc
0(h(cid:48))) =
particular, choosing f (h, h(cid:48)) := σ(h) (cid:12) h(cid:48) yields6 f ⊗c(h, h(cid:48)) = expc
diag(σ(logc

0(h))) ⊗c h(cid:48). Hence we adapt rt (cid:12) ht−1 to diag(rt) ⊗c ht−1 and the reset gate rt to:
0(W r ⊗c ht−1 ⊕c U r ⊗c xt ⊕c br),

0(h), logc
0(h)) (cid:12) logc

0(f (logc
0(σ(logc

rt = σ logc

c × Dp

(31)

and similarly for the update gate zt. Note that as the argument of σ in the above is unbounded, rt and
zt can a priori take values onto the full range (0, 1). Now the intermediate hidden state becomes:
˜ht = ϕ⊗c ((W diag(rt)) ⊗c ht−1 ⊕c U ⊗c xt ⊕ b),

(32)

where Möbius matrix associativity simpliﬁes W ⊗c (diag(rt) ⊗c ht−1) into (W diag(rt)) ⊗c ht−1.
Finally, we propose to adapt the update-gate equation as

ht = ht−1 ⊕c diag(zt) ⊗c (−ht−1 ⊕c

˜ht).

(33)

Note that when c goes to zero, one recovers the usual GRU. Moreover, if zt = 0 or zt = 1, then ht
becomes ht−1 or ˜ht respectively, similarly as in the usual GRU. This adaptation was obtained by
adapting [24]: in this work, the authors re-derive the update-gate mechanism from a ﬁrst principle
called time-warping invariance. We adapted their derivation to the hyperbolic setting by using the
notion of gyroderivative [4] and proving a gyro-chain-rule (see appendix E).

4 Experiments

SNLI task and dataset. We evaluate our method on two tasks. The ﬁrst is natural language
inference, or textual entailment. Given two sentences, a premise (e.g. "Little kids A. and B. are
playing soccer.") and a hypothesis (e.g. "Two children are playing outdoors."), the binary classiﬁcation
task is to predict whether the second sentence can be inferred from the ﬁrst one. This deﬁnes a partial
order in the sentence space. We test hyperbolic networks on the biggest real dataset for this task,
SNLI [7]. It consists of 570K training, 10K validation and 10K test sentence pairs. Following [28],
we merge the "contradiction" and "neutral" classes into a single class of negative sentence pairs, while
the "entailment" class gives the positive pairs.

PREFIX task and datasets. We conjecture that the improvements of hyperbolic neural networks
are more signiﬁcant when the underlying data structure is closer to a tree. To test this, we design a
proof-of-concept task of detection of noisy preﬁxes, i.e. given two sentences, one has to decide if the
second sentence is a noisy preﬁx of the ﬁrst, or a random sentence. We thus build synthetic datasets
PREFIX-Z% (for Z being 10, 30 or 50) as follows: for each random ﬁrst sentence of random length
at most 20 and one random preﬁx of it, a second positive sentence is generated by randomly replacing
Z% of the words of the preﬁx, and a second negative sentence of same length is randomly generated.
Word vocabulary size is 100, and we generate 500K training, 10K validation and 10K test pairs.

6If x has n coordinates, then diag(x) denotes the diagonal matrix of size n with xi’s on its diagonal.

8

Models architecture. Our neural network layers can be used in a plug-n-play manner exactly like
standard Euclidean layers. They can also be combined with Euclidean layers. However, optimization
w.r.t. hyperbolic parameters is different (see below) and based on Riemannian gradients which
are just rescaled Euclidean gradients when working in the conformal Poincaré model [21]. Thus,
back-propagation can be applied in the standard way.

In our setting, we embed the two sentences using two distinct hyperbolic RNNs or GRUs. The
sentence embeddings are then fed together with their squared distance (hyperbolic or Euclidean,
depending on their geometry) to a FFNN (Euclidean or hyperbolic, see Sec. 3.2) which is further
fed to an MLR (Euclidean or hyperbolic, see Sec. 3.1) that gives probabilities of the two classes
(entailment vs neutral). We use cross-entropy loss on top. Note that hyperbolic and Euclidean layers
can be mixed, e.g. the full network can be hyperbolic and only the last layer be Euclidean, in which
case one has to use log0 and exp0 functions to move between the two manifolds in a correct manner
as explained for Eq. 26.

Optimization. Our models have both Euclidean (e.g. weight matrices in both Euclidean and
hyperbolic FFNNs, RNNs or GRUs) and hyperbolic parameters (e.g. word embeddings or biases for
the hyperbolic layers). We optimize the Euclidean parameters with Adam [16] (learning rate 0.001).
Hyperbolic parameters cannot be updated with an equivalent method that keeps track of gradient
history due to the absence of a Riemannian Adam. Thus, they are optimized using full Riemannian
stochastic gradient descent (RSGD) [5, 11]. We also experiment with projected RSGD [21], but
optimization was sometimes less stable. We use a different constant learning rate for word embeddings
(0.1) and other hyperbolic weights (0.01) because words are updated less frequently.

Numerical errors. Gradients of the basic operations deﬁned above (e.g. ⊕c, exponential map) are
c(cid:107)x(cid:107) = 1. Thus, we
not deﬁned when the hyperbolic argument vectors are on the ball border, i.e.
always project results of these operations in the ball of radius 1 − (cid:15), where (cid:15) = 10−5. Numerical
errors also appear when hyperbolic vectors get closer to 0, thus we perturb them with an (cid:15)(cid:48) = 10−15
before they are used in any of the above operations. Finally, arguments of the tanh function are
clipped between ±15 to avoid numerical errors, while arguments of tanh−1 are clipped to at most
1 − 10−5.

√

Hyperparameters. For all methods, baselines and datasets, we use c = 1, word and hidden state
embedding dimension of 5 (we focus on the low dimensional setting that was shown to already
be effective [21]), batch size of 64. We ran all methods for a ﬁxed number of 30 epochs. For all
models, we experiment with both identity (no non-linearity) or tanh non-linearity in the RNN/GRU
cell, as well as identity or ReLU after the FFNN layer and before MLR. As expected, for the fully
Euclidean models, tanh and ReLU respectively surpassed the identity variant by a large margin. We
only report the best Euclidean results. Interestingly, for the hyperbolic models, using only identity for
both non-linearities works slightly better and this is likely due to two facts: i) our hyperbolic layers
already contain non-linearities by their nature, ii) tanh is limiting the output domain of the sentence
embeddings, but the hyperbolic speciﬁc geometry is more pronounced at the ball border, i.e. at the
hyperbolic "inﬁnity", compared to the center of the ball.

For the results shown in Tab. 1, we run each model (baseline or ours) exactly 3 times and report the
test result corresponding to the best validation result from these 3 runs. We do this because the highly
non-convex spectrum of hyperbolic neural networks sometimes results in convergence to poor local
minima, suggesting that initialization is very important.

Results. Results are shown in Tab. 1. Note that the fully Euclidean baseline models might have
an advantage over hyperbolic baselines because more sophisticated optimization algorithms such
as Adam do not have a hyperbolic analogue at the moment. We ﬁrst observe that all GRU models
overpass their RNN variants. Hyperbolic RNNs and GRUs have the most signiﬁcant improvement
over their Euclidean variants when the underlying data structure is more tree-like, e.g. for PREFIX-
10% − for which the tree relation between sentences and their preﬁxes is more prominent − we
reduce the error by a factor of 3.35 for hyperbolic vs Euclidean RNN, and by a factor of 1.5 for
hyperbolic vs Euclidean GRU. As soon as the underlying structure diverges more and more from
a tree, the accuracy gap decreases − for example, for PREFIX-50% the noise heavily affects the
representational power of hyperbolic networks. Also, note that on SNLI our methods perform
similarly as with their Euclidean variants. Moreover, hyperbolic and Euclidean MLR are on par when

9

SNLI

PREFIX-10% PREFIX-30% PREFIX-50%

FULLY EUCLIDEAN RNN
HYPERBOLIC RNN+FFNN, EUCL MLR
FULLY HYPERBOLIC RNN
FULLY EUCLIDEAN GRU
HYPERBOLIC GRU+FFNN, EUCL MLR
FULLY HYPERBOLIC GRU

79.34 %
79.18 %
78.21 %
81.52 %
79.76 %
81.19 %

89.62 %
96.36 %
96.91 %
95.96 %
97.36 %
97.14 %

81.71 %
87.83 %
87.25 %
86.47 %
88.47 %
88.26 %

72.10 %
76.50 %
62.94 %
75.04 %
76.87 %
76.44 %

Table 1: Test accuracies for various models and four datasets. "Eucl" denotes Euclidean. All word
and sentence embeddings have dimension 5. We highlight in bold the best baseline (or baselines, if
the difference is less than 0.5%).

used in conjunction with hyperbolic sentence embeddings, suggesting further empirical investigation
is needed for this direction (see below).

We also observe that, in the hyperbolic setting, accuracy tends to increase when sentence embeddings
start increasing, and gets better as their norms converge towards 1 (the ball border for c = 1). Unlike
in the Euclidean case, this behavior does happen only after a few epochs and suggests that the model
should ﬁrst adjust the angular layout in order to disentangle the representations, before increasing their
norms to fully exploit the strong clustering property of the hyperbolic geometry. Similar behavior
was observed in the context of embedding trees by [21]. Details in appendix F.

MLR classiﬁcation experiments.
For the sentence entailment classi-
ﬁcation task we do not see a clear
advantage of hyperbolic MLR com-
pared to its Euclidean variant. A pos-
sible reason is that, when trained end-
to-end, the model might decide to
place positive and negative embed-
dings in a manner that is already well
separated with a classic MLR. As a
consequence, we further investigate
MLR for the task of subtree classiﬁ-
cation. Using an open source imple-
mentation7 of [21], we pre-trained
Poincaré embeddings of the Word-
Net noun hierarchy (82,115 nodes).
We then choose one node in this tree
(see Table 2) and classify all other
nodes (solely based on their embed-
dings) as being part of the subtree
rooted at this node. All nodes in such a subtree are divided into positive training nodes (80%) and
positive test nodes (20%). The same splitting procedure is applied for the remaining WordNet nodes
that are divided into a negative training and negative test set respectively. Three variants of MLR
are then trained on top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation
task: hyperbolic MLR, Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean
MLR applied after mapping all embeddings in the tangent space at 0 using the log0 map. We use
different embedding dimensions : 2, 3, 5 and 10. For the hyperbolic MLR, we use full Riemannian
SGD with a learning rate of 0.001. For the two Euclidean models we use ADAM optimizer and the
same learning rate. During training, we always sample the same number of negative and positive
nodes in each minibatch of size 16; thus positive nodes are frequently resampled. All methods are
trained for 30 epochs and the ﬁnal F1 score is reported (no hyperparameters to validate are used, thus
we do not require a validation set). This procedure is repeated for four subtrees of different sizes.

Figure 2: Hyperbolic (left) vs Direct Euclidean (right) binary
MLR used to classify nodes as being part in the GROUP.N.01
subtree of the WordNet noun hierarchy solely based on their
Poincaré embeddings. The positive points (from the subtree)
are in blue, the negative points (the rest) are in red and the
trained positive separation hyperplane is depicted in green.

Quantitative results are presented in Table 2. We can see that the hyperbolic MLR overpasses
its Euclidean variants in almost all settings, sometimes by a large margin. Moreover, to provide

7https://github.com/dalab/hyperbolic_cones

10

WORDNET
SUBTREE

ANIMAL.N.01
3218 / 798

GROUP.N.01
6649 / 1727

WORKER.N.01
861 / 254

MAMMAL.N.01
953 / 228

MODEL

D = 2

D = 3

D = 5

D = 10

HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL

47.43 ± 1.07%
41.69 ± 0.19%
38.89 ± 0.01%
81.72 ± 0.17%
61.13 ± 0.42%
60.75 ± 0.24%
12.68 ± 0.82%
10.86 ± 0.01%
9.04 ± 0.06%
32.01 ± 17.14%
15.58 ± 0.04%
13.10 ± 0.13%

91.92 ± 0.61%
68.43 ± 3.90%
62.57 ± 0.61%
89.87 ± 2.73%
63.56 ± 1.22%
61.98 ± 0.57%
24.09 ± 1.49%
22.39 ± 0.04%
22.57 ± 0.20%
87.54 ± 4.55%
44.68 ± 1.87%
44.89 ± 1.18%

98.07 ± 0.55%
95.59 ± 1.18%
89.21 ± 1.34%
87.89 ± 0.80%
67.82 ± 0.81%
67.92 ± 0.74%
55.46 ± 5.49%
35.23 ± 3.16%
26.47 ± 0.78%
88.73 ± 3.22%
59.35 ± 1.31%
52.51 ± 0.85%

99.26 ± 0.59%
99.36 ± 0.18%
98.27 ± 0.70%
91.91 ± 3.07%
91.38 ± 1.19%
91.41 ± 0.18%
66.83 ± 11.38%
47.29 ± 3.93%
36.66 ± 2.74%
91.37 ± 6.09%
77.76 ± 5.08%
56.11 ± 2.21%

Table 2: Test F1 classiﬁcation scores for four different subtrees of WordNet noun tree. All nodes
in such a subtree are divided into positive training nodes (80%) and positive test nodes (20%);
these counts are shown below each subtree root. The same splitting procedure is applied for the
remaining nodes to obtain negative training and test sets. Three variants of MLR are then trained on
top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation task: hyperbolic MLR,
Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean MLR applied after
mapping all embeddings in the tangent space at 0 using the log0 map. 95% conﬁdence intervals for 3
different runs are shown for each method and each different embedding dimension (2, 3, 5 or 10).

further understanding, we plot the 2-dimensional embeddings and the trained separation hyperplanes
(geodesics in this case) in Figure 2. We can see that respecting the hyperbolic geometry is very
important for a quality classiﬁcation model.

5 Conclusion

We showed how classic Euclidean deep learning tools such as MLR, FFNNs, RNNs or GRUs can be
generalized in a principled manner to all spaces of constant negative curvature combining Riemannian
geometry with the elegant theory of gyrovector spaces. Empirically we found that our models
outperform or are on par with corresponding Euclidean architectures on sequential data with implicit
hierarchical structure. We hope to trigger exciting future research related to better understanding
of the hyperbolic non-convexity spectrum and development of other non-Euclidean deep learning
methods.
Our data and Tensorﬂow [1] code are publicly available8.

Acknowledgements

We thank Igor Petrovski for useful pointers regarding the implementation.

This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement
number 167176. Gary Bécigneul is also funded by the Max Planck ETH Center for Learning
Systems.

References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for
large-scale machine learning. 2016.

[2] Ungar Abraham Albert. Analytic hyperbolic geometry and Albert Einstein’s special theory of

relativity. World scientiﬁc, 2008.

8https://github.com/dalab/hyperbolic_nn

11

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of the International Conference on Learning
Representations (ICLR), 2015.

[4] Graciela S Birman and Abraham A Ungar. The hyperbolic derivative in the poincaré ball model
of hyperbolic geometry. Journal of mathematical analysis and applications, 254(1):321–333,
2001.

[5] S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on

Automatic Control, 58(9):2217–2229, Sept 2013.

[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems (NIPS), pages 2787–2795, 2013.

[7] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 632–642. Association
for Computational Linguistics, 2015.

[8] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,
34(4):18–42, 2017.

[9] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry.

Flavors of geometry, 31:59–115, 1997.

[10] Christopher De Sa, Albert Gu, Christopher Ré, and Frederic Sala. Representation tradeoffs for

hyperbolic embeddings. arXiv preprint arXiv:1804.03329, 2018.

[11] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In Proceedings of the thirty-ﬁfth international conference
on machine learning (ICML), 2018.

[12] Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pages 75–263. Springer, 1987.

[13] Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the

Cambridge Philosophical Society, page 1–17, 2017.

[14] Christopher Hopper and Ben Andrews. The Ricci ﬂow in Riemannian geometry. Springer, 2010.

[15] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1746–1751. Association for Computational Linguistics, 2014.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations (ICLR), 2015.

[17] Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marián Boguná.

Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.

[18] John Lamping, Ramana Rao, and Peter Pirolli. A focus+ context technique based on hyperbolic
geometry for visualizing large hierarchies. In Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 401–408. ACM Press/Addison-Wesley Publishing Co.,
1995.

[19] Guy Lebanon and John Lafferty. Hyperplane margin classiﬁers on the multinomial manifold. In
Proceedings of the international conference on machine learning (ICML), page 66. ACM, 2004.

[20] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective
learning on multi-relational data. In Proceedings of the international conference on machine
learning (ICML), volume 11, pages 809–816, 2011.

12

[21] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical repre-
sentations. In Advances in Neural Information Processing Systems (NIPS), pages 6341–6350,
2017.

[22] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blun-
som. Reasoning about entailment with neural attention. In Proceedings of the International
Conference on Learning Representations (ICLR), 2015.

[23] Michael Spivak. A comprehensive introduction to differential geometry. Publish or perish, 1979.

[24] Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In Proceedings of

the International Conference on Learning Representations (ICLR), 2018.

[25] Abraham A Ungar. Hyperbolic trigonometry and its application in the poincaré ball model of

hyperbolic geometry. Computers & Mathematics with Applications, 41(1-2):135–147, 2001.

[26] Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis

Lectures on Mathematics and Statistics, 1(1):1–194, 2008.

[27] Abraham Albert Ungar. Analytic hyperbolic geometry in n dimensions: An introduction. CRC

Press, 2014.

[28] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. In Proceedings of the International Conference on Learning Representations (ICLR),
2016.

[29] J Vermeer. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic

plane. Topology and its Applications, 152(3):226–242, 2005.

13

A Hyperbolic Trigonometry

Hyperbolic angles. For A, B, C ∈ Dn
c , we denote by ∠A := ∠BAC the angle between the two
geodesics starting from A and ending at B and C respectively. This angle can be deﬁned in two
equivalent ways: i) either using the angle between the initial velocities of the two geodesics as given
by Eq. 5, or ii) using the formula

cos(∠A) =

(cid:28) (−A) ⊕c B
(cid:107)(−A) ⊕c B(cid:107)

,

(−A) ⊕c C
(cid:107)(−A) ⊕c C(cid:107)

(cid:29)

,

In this case, ∠A is also called a gyroangle in the work of [26, section 4].

Hyperbolic law of sines. We state here the hyperbolic law of sines. If for A, B, C ∈ Dn
c , we
denote by ∠B := ∠ABC the angle between the two geodesics starting from B and ending at A and
C respectively, and by ˜c = dc(B, A) the length of the hyperbolic segment BA (and similarly for
others), then we have:

sin(∠A)
√
c˜a)
sinh(

=

sin(∠B)
√
c˜b)
sinh(

=

sin(∠C)
√
c˜c)
sinh(

.

Note that one can also adapt the hyperbolic law of cosines to the hyperbolic space.

B Proof of Theorem 4

Theorem 4.
In the manifold (Dn
to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a vector v ∈ T0Dn
c

c is given by the following isometry:
λc
0
λc
x

0→x(v) = logc
P c

x(x ⊕c expc

0(v)) =

v.

Proof. The geodesic in Dn
v ∈ T0Dn
γ (i.e. X(t) ∈ Tγ(t)Dn

c from 0 to x is given in Eq. (10) by γ(t) = x ⊗c t, for t ∈ [0, 1]. Let
c . Then it is of common knowledge that there exists a unique parallel9 vector ﬁeld X along

c , ∀t ∈ [0, 1]) such that X(0) = v. Let’s deﬁne:
X : t ∈ [0, 1] (cid:55)→ logc

γ(t)(γ(t) ⊕c expc

0(v)) ∈ Tγ(t)Dn
c .

Clearly, X is a vector ﬁeld along γ such that X(0) = v. Now deﬁne
0→x : v ∈ T0Dn
P c

x(x ⊕c expc

0(v)) ∈ TxDn
c .

c (cid:55)→ logc
0→x(v) = λc

c . Since P c

0→x is a linear isometry from T0Dn
v, hence P c
c
0→x(v) = X(1), it is enough to prove that X is parallel in order to guarantee that

From Eq. (12), it is easily seen that P c
to TxDn
c to TxDn
0→x is the parallel transport from T0Dn
P c
c .
Since X is a vector ﬁeld along γ, its covariant derivative can be expressed with the Levi-Civita
connection ∇c associated to gc:

0
λc
x

DX
∂t

= ∇c

˙γ(t)X.

Let’s compute the Levi-Civita connection from its Christoffel symbols. In a local coordinate system,
they can be written as

Γi

jk =

(gc)il(∂jgc

lk + ∂kgc

lj − ∂lgc

jk),

1
2

where superscripts denote the inverse metric tensor and using Einstein’s notations. As gc
at γ(t) ∈ Dn

c this yields:

ij = (λc)2δij,

jk = cλc
Γi

γ(t)(δikγ(t)j + δijγ(t)k − δjkγ(t)i).

9i.e. that DX

∂t = 0 for t ∈ [0, 1], where D

∂t denotes the covariant derivative.

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

14

On the other hand, since X(t) = (λc

∇c

˙γ(t)X = ˙γ(t)i∇c

i X = ˙γ(t)i∇c
i

= vj ˙γ(t)i∇c
i

0/λc

γ(t))v, we have
(cid:32)

(cid:33)

λc
0
λc

γ(t)

v

(cid:32)

λc
0
λc

γ(t)

(cid:33)

ej

.

√

√

Since γ(t) = (1/
Hence there exists K x

c) tanh(t tanh−1(
t ∈ R such that ˙γ(t) = K x

c(cid:107)x(cid:107))) x

(cid:107)x(cid:107) , it is easily seen that ˙γ(t) is colinear to γ(t).
t γ(t). Moreover, we have the following Leibniz rule:
(cid:33)

(cid:32)

(cid:32)

∇c
i

λc
0
λc

γ(t)

(cid:33)

ej

=

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

ej.

Combining these yields

DX
∂t

= K x

t vjγ(t)i

(cid:32)

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

(cid:32)

(cid:33)

(cid:33)

ej

.

Replacing with the Christoffel symbols of ∇c at γ(t) gives

Moreover,

λc
0
λc

γ(t)

λc
0
λc

γ(t)

∇c

i ej =

ijek = 2c[δk
Γk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek.

∂
∂γ(t)i

(cid:32)

(cid:33)

λc
0
λc

γ(t)

ej =

∂
∂γ(t)i

(cid:0)−c(cid:107)γ(t)(cid:107)2(cid:1) ej = −2cγ(t)iej.

Putting together everything, we obtain

DX
∂t

= K x

t vjγ(t)i (cid:0)2c[δk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek − 2cγ(t)iej

(cid:1)

t vjγ(t)i (cid:0)γ(t)jei − δijγ(t)kek
t vj (cid:0)γ(t)jγ(t)iei − γ(t)iδijγ(t)kek
(cid:1)
t vj (cid:0)γ(t)jγ(t)iei − γ(t)jγ(t)kek

(cid:1)

(cid:1)

= 2cK x
= 2cK x
= 2cK x
= 0,

which concludes the proof.

C Proof of Eq. (22)

Proof. Two steps proof:
i) expc

p({a}⊥) ⊆ {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}:

Let z ∈ {a}⊥. From Eq. (12), we have that:

This, together with the left-cancellation law in gyrospaces (see section 2.3), implies that

expc

p(z) = −p ⊕c βz,

for some β ∈ R.

(cid:104)−p ⊕c expc

p(z), a(cid:105) = (cid:104)βz, a(cid:105) = 0

which is what we wanted.

ii) {x ∈ Dn
Let x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0} ⊆ expc
c s.t. (cid:104)−p ⊕c x, a(cid:105) = 0. Then, using Eq. (12), we derive that:
for some β ∈ R,

p(x) = β(−p ⊕c x),

p({a}⊥):

logc

which is orthogonal to a, by assumption. This implies logc

p(x) ∈ {a}⊥, hence x ∈ expc

p({a}⊥).

15

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(54)

D Proof of Theorem 5

Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

1
√
c

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(55)

Proof. We ﬁrst need to prove the following lemma, trivial in the Euclidean space, but not in the
Poincaré ball:
Lemma 7. (Orthogonal projection on a geodesic) Any point in the Poincaré ball has a unique
orthogonal projection on any given geodesic that does not pass through the point. Formally, for all
y ∈ Dn
c and for all geodesics γx→z(·) s.t. y /∈ Im γx→z, there exists an unique w ∈ Im γx→z s.t.
∠(γw→y, γx→z) = π/2.

Proof. We ﬁrst note that any geodesic in Dn
and has two "points at inﬁnity" lying on the ball border (v (cid:54)= 0):

c has the form γ(t) = u ⊕c v ⊗c t as given by Eq. 11,

γ(±∞) = u ⊕c

√

±v
c(cid:107)v(cid:107)

∈ ∂Dn
c .

(56)

Using the notations in the lemma statement, the closed-form of γx→z is given by Eq. (10):

γx→z(t) = x ⊕c (−x ⊕c z) ⊗c t

We denote by x(cid:48), z(cid:48) ∈ ∂Dn
∠ywx(cid:48) is well deﬁned from Eq. (34):

c its points at inﬁnity as described by Eq. (56). Then, the hyperbolic angle

cos(∠(γw→y, γx→z)) = cos(∠ywz(cid:48)) =

(cid:104)−w ⊕c y, −w ⊕c z(cid:48)(cid:105)
(cid:107) − w ⊕c y(cid:107) · (cid:107) − w ⊕c z(cid:48)(cid:107)

.

(57)

We now perform 2 steps for this proof.

i) Existence of w:

The angle function from Eq. (57) is continuous w.r.t t when w = γx→z(t). So we ﬁrst prove existence
of an angle of π/2 by continuously moving w from x(cid:48) to z(cid:48) when t goes from −∞ to ∞, and
observing that cos(∠ywz(cid:48)) goes from −1 to 1 as follows:

cos(∠yx(cid:48)z(cid:48)) = 1 & lim
w→z(cid:48)

cos(∠ywz(cid:48)) = −1.

(58)

The left part of Eq. (58) follows from Eq. (57) and from the fact (easy to show from the deﬁnition
c (which is the case of x(cid:48)). The right part of Eq. (58)
of ⊕c) that a ⊕c b = a, when (cid:107)a(cid:107) = 1/
follows from the fact that ∠ywz(cid:48) = π − ∠ywx(cid:48) (from the conformal property, or from Eq. (34)) and
cos(∠yz(cid:48)x(cid:48)) = 1 (proved as above).
Hence cos(∠ywz(cid:48)) has to pass through 0 when going from −1 to 1, which achieves the proof of
existence.

√

ii) Uniqueness of w:
Assume by contradiction that there are two w and w(cid:48) on γx→z that form angles ∠ywx(cid:48) and ∠yw(cid:48)x(cid:48)
of π/2. Since w, w(cid:48), x(cid:48) are on the same geodesic, we have

π/2 = ∠yw(cid:48)x(cid:48) = ∠yw(cid:48)w = ∠ywx(cid:48) = ∠yw(cid:48)w
So ∆yww(cid:48) has two right angles, but in the Poincaré ball this is impossible.

(59)

Now, we need two more lemmas:
Lemma 8. (Minimizing distance from point to geodesic) The orthogonal projection of a point to
a geodesic (not passing through the point) is minimizing the distance between the point and the
geodesic.

Proof. The proof is similar with the Euclidean case and it’s based on hyperbolic sine law and the fact
that in any right hyperbolic triangle the hypotenuse is strictly longer than any of the other sides.

16

Lemma 9. (Geodesics through p) Let ˜H c
all points on the geodesic γp→w are included in ˜H c

a,p.

a,p be a Poincaré hyperplane. Then, for any w ∈ ˜H c

a,p \ {p},

Proof. γp→w(t) = p ⊕c (−p ⊕c w) ⊗c t. Then, it is easy to check the condition in Eq. (22):

(cid:104)−p ⊕c γp→w(t), a(cid:105) = (cid:104)(−p ⊕c w) ⊗c t, a(cid:105) ∝ (cid:104)−p ⊕c w, a(cid:105) = 0.

(60)

We now turn back to our proof. Let x ∈ Dn
We prove that there is at least one point w∗ ∈ ˜H c

c be an arbitrary point and ˜H c

a,p a Poincaré hyperplane.

a,p that achieves the inﬁmum distance

dc(x, w∗) = inf
w∈ ˜H c

a,p

dc(x, w),

and, moreover, that this distance is the same as the one in the theorem’s statement.
We ﬁrst note that for any point w ∈ ˜H c
and Lemma 9, it is obvious that the projection of x to γp→w will give a strictly lower distance.
Thus, we only consider w ∈ ˜H c
triangle ∆xwp, one gets:

a,p such that ∠xwp = π/2. Applying hyperbolic sine law in the right

a,p, if ∠xwp (cid:54)= π/2, then w (cid:54)= w∗. Indeed, using Lemma 8

dc(x, w) = (1/

c) sinh−1 (cid:0)sinh(

c dc(x, p)) · sin(∠xpw)(cid:1) .

√

√

One of the above quantities does not depend on w:

√

√

sinh(

c dc(x, p)) = sinh(2 tanh−1(

c(cid:107) − p ⊕c x(cid:107))) =

√
2
c(cid:107) − p ⊕c x(cid:107)
1 − c(cid:107) − p ⊕c x(cid:107)2 .

The other quantity is sin(∠xpw) which is minimized when the angle ∠xpw is minimized (be-
cause ∠xpw < π/2 for the hyperbolic right triangle ∆xwp), or, alternatively, when cos(∠xpw) is
maximized. But, we already have from Eq. (34) that:

cos(∠xpw) =

(cid:104)−p ⊕c x, −p ⊕c w(cid:105)
(cid:107) − p ⊕c x(cid:107) · (cid:107) − p ⊕c w(cid:107)

.

To maximize the above, the constraint on the right angle at w can be dropped because cos(∠xpw)
depends only on the geodesic γp→w and not on w itself, and because there is always an orthogonal
projection from any point x to any geodesic as stated by Lemma 7. Thus, it remains to ﬁnd the
maximum of Eq. (64) when w ∈ ˜H c
a,p from Eq. (22), one can easily
prove that

a,p. Using the deﬁnition of ˜H c

Using that fact that logc

p(w)/(cid:107) logc

p(w)(cid:107) = −p ⊕c w/(cid:107) − p ⊕c w(cid:107), we just have to ﬁnd

and we are left with a well known Euclidean problem which is equivalent to ﬁnding the minimum
angle between the vector −p ⊕c x (viewed as Euclidean) and the hyperplane {a}⊥. This angle
is given by the Euclidean orthogonal projection whose sin value is the distance from the vector’s
endpoint to the hyperplane divided by the vector’s length:

{logc

p(w) : w ∈ ˜H c

a,p} = {a}⊥.

max
z∈{a}⊥

(cid:18) (cid:104)−p ⊕c x, z(cid:105)

(cid:107) − p ⊕c x(cid:107) · (cid:107)z(cid:107)

(cid:19)

,

sin(∠xpw∗) =

|(cid:104)−p ⊕c x, a
(cid:107) − p ⊕c x(cid:107)

(cid:107)a(cid:107) (cid:105)|

.

17

It follows that a point w∗ ∈ ˜H c
Eqs. (61),(62),(63) and (67) concludes the proof.

a,p satisfying Eq. (67) exists (but might not be unique). Combining

(61)

(62)

(63)

(64)

(65)

(66)

(67)

(cid:3)

E Derivation of the Hyperbolic GRU Update-gate

In [24], the authors recover the update/forget-gate mechanism of a GRU/LSTM by requiring that the
class of neural networks given by the chosen architecture be invariant to time-warpings. The idea is
the following.

Recovering the update-gate from time-warping. A naive RNN is given by the equation

h(t + 1) = ϕ(W h(t) + U x(t) + b)

Let’s drop the bias b to simplify notations. If h is seen as a differentiable function of time, then a
ﬁrst-order Taylor development gives h(t + δt) ≈ h(t) + δt dh
dt (t) for small δt. Combining this for
δt = 1 with the naive RNN equation, one gets

dh
dt

dα
dt

(t) = ϕ(W h(t) + U x(t)) − h(t).

As this is written for any t, one can replace it by t ← α(t) where α is a (smooth) increasing function
of t called the time-warping. Denoting by ˜h(t) := h(α(t)) and ˜x(t) := x(α(t)), using the chain rule
d˜h
dt (t) = dα

dt (α(t)), one gets

dt (t) dh

d˜h
dt

dα
dt

(t) =

(t)ϕ(W ˜h(t) + U ˜x(t)) −

(t)˜h(t).

(70)

Removing the tildas to simplify notations, discretizing back with dh

dt (t) ≈ h(t + 1) − h(t) yields

h(t + 1) =

(t)ϕ(W h(t) + U x(t)) +

1 −

(t)

h(t).

(71)

dα
dt

(cid:18)

(cid:19)

dα
dt

Requiring that our class of neural networks be invariant to time-warpings means that this class should
contain RNNs deﬁned by Eq. (71), i.e. that dα
dt (t) can be learned. As this is a positive quantity, we
can parametrize it as z(t) = σ(W zh(t) + U zx(t)), recovering the forget-gate equation:

h(t + 1) = z(t)ϕ(W h(t) + U x(t)) + (1 − z(t))h(t).

Adapting this idea to hyperbolic RNNs. The gyroderivative [4] of a map h : R → Dn
as

c is deﬁned

dh
dt

(t) = lim
δt→0

1
δt

⊗c (−h(t) ⊕c h(t + δt)).

Using Möbius scalar associativity and the left-cancellation law leads us to

h(t + δt) ≈ h(t) ⊕c δt ⊗c

(t),

dh
dt

for small δt. Combining this with the equation of a simple hyperbolic RNN of Eq. (29) with δt = 1,
one gets

dh
dt

(t) = −h(t) ⊕c ϕ⊗c(W ⊗c h(t) ⊕c U ⊗c x(t)).

For the next step, we need the following lemma:
Lemma 10 (Gyro-chain-rule). For α : R → R differentiable and h : R → Dn
gyro-derivative, if ˜h := h ◦ α, then we have

c with a well-deﬁned

(68)

(69)

(72)

(73)

(74)

(75)

(76)

where dα

dt (t) denotes the usual derivative.

d˜h
dt

(t) =

(t) ⊗c

(α(t)),

dα
dt

dh
dt

18

(77)

(78)

(79)

(80)

(81)

Proof.

d˜h
dt

(t) = lim
δt→0

1
δt
1
δt

⊗c [−˜h(t) ⊕c

˜h(t + δt)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

= lim
δt→0

= lim
δt→0

= lim
δt→0

= lim
u→0
dα
dt

=

α(cid:48)(t) + O(δt)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
u

(t) ⊗c

(α(t))

dh
dt

⊗c [−h(α(t)) ⊕c h(α(t) + u)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

(Möbius scalar associativity) (82)

where we set u = δt(α(cid:48)(t) + O(δt)), with u → 0 when δt → 0, which concludes.

Using lemma 10 and Eq. (75), with similar notations as in Eq. (70) we have

d˜h
dt

dα
dt

(t) =

(t) ⊗c (−˜h(t) ⊕c ϕ⊗c(W ⊗c

˜h(t) ⊕c U ⊗c ˜x(t))).

(83)

Finally, discretizing back with Eq. (74), using the left-cancellation law and dropping the tildas yields

h(t + 1) = h(t) ⊕c

(t) ⊗c (−h(t) ⊕c ϕ⊗c (W ⊗c h(t) ⊕c U ⊗c x(t))).

(84)

dα
dt

Since α is a time-warping, by deﬁnition its derivative is positive and one can choose to parametrize
it with an update-gate zt (a scalar) deﬁned with a sigmoid. Generalizing this scalar scaling by the
Möbius version of the pointwise scaling (cid:12) yields the Möbius matrix scaling diag(zt) ⊗c ·, leading to
our proposed Eq. (33) for the hyperbolic GRU.

F More Experimental Investigations

The following empirical facts were observed for both hyperbolic RNNs and GRUs.

We observed that, in the hyperbolic setting, accuracy is often much higher when sentence embeddings
can go close to the border (hyperbolic "inﬁnity"), hence exploiting the hyperbolic nature of the space.
Moreover, the faster the two sentence norms go to 1, the more it’s likely that a good local minima
was reached. See ﬁgures 3 and 5.

We often observe that test accuracy starts increasing exactly when sentence embedding norms do.
However, in the hyperbolic setting, the sentence embeddings norms remain close to 0 for a few
epochs, which does not happen in the Euclidean case. See ﬁgures 3, 5 and 4. This mysterious fact
was also exhibited in a similar way by [21] which suggests that the model ﬁrst has to adjust the
angular layout in the almost Euclidean vicinity of 0 before increasing norms and fully exploiting
hyperbolic geometry.

19

(a) Test accuracy

(a) Test accuracy

20

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 3: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the same
architecture: hyperbolic GRU followed by hyperbolic FFNN and hyperbolic/Euclidean (half-half)
MLR. The X axis shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 4: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: Euclidean GRU followed by Euclidean FFNN and Euclidean MLR. The X axis
shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 5: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: hyperbolic RNN followed by hyperbolic FFNN and hyperbolic MLR. The X axis
shows millions of training examples processed.

(a) Test accuracy

21

8
1
0
2
 
n
u
J
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
2
1
1
9
0
.
5
0
8
1
:
v
i
X
r
a

Hyperbolic Neural Networks

Octavian-Eugen Ganea∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
octavian.ganea@inf.ethz.ch

Gary Bécigneul∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
gary.becigneul@inf.ethz.ch

Thomas Hofmann
Department of Computer Science
ETH Zürich
Zurich, Switzerland
thomas.hofmann@inf.ethz.ch

Abstract

Hyperbolic spaces have recently gained momentum in the context of machine
learning due to their high capacity and tree-likeliness properties. However, the
representational power of hyperbolic geometry is not yet on par with Euclidean
geometry, mostly because of the absence of corresponding hyperbolic neural
network layers. This makes it hard to use hyperbolic embeddings in downstream
tasks. Here, we bridge this gap in a principled manner by combining the formalism
of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model
of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep
learning tools: multinomial logistic regression, feed-forward and recurrent neural
networks such as gated recurrent units. This allows to embed sequential data and
perform classiﬁcation in the hyperbolic space. Empirically, we show that, even if
hyperbolic optimization tools are limited, hyperbolic sentence embeddings either
outperform or are on par with their Euclidean variants on textual entailment and
noisy-preﬁx recognition tasks.

1

Introduction

It is common in machine learning to represent data as being embedded in the Euclidean space Rn. The
main reason for such a choice is simply convenience, as this space has a vectorial structure, closed-
form formulas of distance and inner-product, and is the natural generalization of our intuition-friendly,
visual three-dimensional space. Moreover, embedding entities in such a continuous space allows to
feed them as input to neural networks, which has led to unprecedented performance on a broad range
of problems, including sentiment detection [15], machine translation [3], textual entailment [22] or
knowledge base link prediction [20, 6].

Despite the success of Euclidean embeddings, recent research has proven that many types of com-
plex data (e.g. graph data) from a multitude of ﬁelds (e.g. Biology, Network Science, Computer
Graphics or Computer Vision) exhibit a highly non-Euclidean latent anatomy [8]. In such cases, the
Euclidean space does not provide the most powerful or meaningful geometrical representations. For
example, [10] shows that arbitrary tree structures cannot be embedded with arbitrary low distortion
(i.e. almost preserving their metric) in the Euclidean space with unbounded number of dimensions,
but this task becomes surprisingly easy in the hyperbolic space with only 2 dimensions where the
exponential growth of distances matches the exponential growth of nodes with the tree depth.

∗Equal contribution.

The adoption of neural networks and deep learning in these non-Euclidean settings has been rather
limited until very recently, the main reason being the non-trivial or impossible principled general-
izations of basic operations (e.g. vector addition, matrix-vector multiplication, vector translation,
vector inner product) as well as, in more complex geometries, the lack of closed form expressions for
basic objects (e.g. distances, geodesics, parallel transport). Thus, classic tools such as multinomial
logistic regression (MLR), feed forward (FFNN) or recurrent neural networks (RNN) did not have a
correspondence in these geometries.

How should one generalize deep neural models to non-Euclidean domains ? In this paper we address
this question for one of the simplest, yet useful, non-Euclidean domains: spaces of constant negative
curvature, i.e. hyperbolic. Their tree-likeness properties have been extensively studied [12, 13, 26]
and used to visualize large taxonomies [18] or to embed heterogeneous complex networks [17]. In
machine learning, recently, hyperbolic representations greatly outperformed Euclidean embeddings
for hierarchical, taxonomic or entailment data [21, 10, 11]. Disjoint subtrees from the latent hierar-
chical structure surprisingly disentangle and cluster in the embedding space as a simple reﬂection of
the space’s negative curvature. However, appropriate deep learning tools are needed to embed feature
data in this space and use it in downstream tasks. For example, implicitly hierarchical sequence data
(e.g. textual entailment data, phylogenetic trees of DNA sequences or hierarchial captions of images)
would beneﬁt from suitable hyperbolic RNNs.

The main contribution of this paper is to bridge the gap between hyperbolic and Euclidean geometry
in the context of neural networks and deep learning by generalizing in a principled manner both the
basic operations as well as multinomial logistic regression (MLR), feed-forward (FFNN), simple and
gated (GRU) recurrent neural networks (RNN) to the Poincaré model of the hyperbolic geometry.
We do it by connecting the theory of gyrovector spaces and generalized Möbius transformations
introduced by [2, 26] with the Riemannian geometry properties of the manifold. We smoothly
parametrize basic operations and objects in all spaces of constant negative curvature using a uniﬁed
framework that depends only on the curvature value. Thus, we show how Euclidean and hyperbolic
spaces can be continuously deformed into each other. On a series of experiments and datasets we
showcase the effectiveness of our hyperbolic neural network layers compared to their "classic"
Euclidean variants on textual entailment and noisy-preﬁx recognition tasks. We hope that this paper
will open exciting future directions in the nascent ﬁeld of Geometric Deep Learning.

2 The Geometry of the Poincaré Ball

2.1 Basics of Riemannian geometry

We brieﬂy introduce basic concepts of differential geometry largely needed for a principled general-
ization of Euclidean neural networks. For more rigorous and in-depth expositions, see [23, 14].
An n-dimensional manifold M is a space that can locally be approximated by Rn: it is a generalization
to higher dimensions of the notion of a 2D surface. For x ∈ M, one can deﬁne the tangent space
TxM of M at x as the ﬁrst order linear approximation of M around x. A Riemannian metric
g = (gx)x∈M on M is a collection of inner-products gx : TxM × TxM → R varying smoothly
with x. A Riemannian manifold (M, g) is a manifold M equipped with a Riemannian metric g.
Although a choice of a Riemannian metric g on M only seems to deﬁne the geometry locally on M,
it induces global distances by integrating the length (of the speed vector living in the tangent space)
of a shortest path between two points:

(cid:90) 1

(cid:113)

d(x, y) = inf
γ

0

gγ(t)( ˙γ(t), ˙γ(t))dt,

(1)

where γ ∈ C∞([0, 1], M) is such that γ(0) = x and γ(1) = y. A smooth path γ of minimal length
between two points x and y is called a geodesic, and can be seen as the generalization of a straight-line
in Euclidean space. The parallel transport Px→y : TxM → TyM is a linear isometry between
tangent spaces which corresponds to moving tangent vectors along geodesics and deﬁnes a canonical
way to connect tangent spaces. The exponential map expx at x, when well-deﬁned, gives a way to
project back a vector v of the tangent space TxM at x, to a point expx(v) ∈ M on the manifold.
This map is often used to parametrize a geodesic γ starting from γ(0) := x ∈ M with unit-norm
direction ˙γ(0) := v ∈ TxM as t (cid:55)→ expx(tv). For geodesically complete manifolds, such as the
Poincaré ball considered in this work, expx is well-deﬁned on the full tangent space TxM. Finally, a

2

(2)

(3)

(4)

(5)

metric ˜g is said to be conformal to another metric g if it deﬁnes the same angles, i.e.

˜gx(u, v)
(cid:112)˜gx(u, u)(cid:112)˜gx(v, v)

=

gx(u, v)
(cid:112)gx(u, u)(cid:112)gx(v, v)

,

for all x ∈ M, u, v ∈ TxM \ {0}. This is equivalent to the existence of a smooth function
λ : M → R, called the conformal factor, such that ˜gx = λ2

xgx for all x ∈ M.

2.2 Hyperbolic space: the Poincaré ball

The hyperbolic space has ﬁve isometric models that one can work with [9]. Similarly as in [21] and
[11], we choose to work in the Poincaré ball. The Poincaré ball model (Dn, gD) is deﬁned by the
manifold Dn = {x ∈ Rn : (cid:107)x(cid:107) < 1} equipped with the following Riemannian metric:

gD
x = λ2

xgE, where λx :=

2
1 − (cid:107)x(cid:107)2 ,

gE = In being the Euclidean metric tensor. Note that the hyperbolic metric tensor is conformal to
the Euclidean one. The induced distance between two points x, y ∈ Dn is known to be given by

dD(x, y) = cosh−1

1 + 2

(cid:18)

(cid:107)x − y(cid:107)2
(1 − (cid:107)x(cid:107)2)(1 − (cid:107)y(cid:107)2)

(cid:19)

.

Since the Poincaré ball is conformal to Euclidean space, the angle between two vectors u, v ∈
TxDn \ {0} is given by

cos(∠(u, v)) =

gD
x (u, v)
x (u, u)(cid:112)gD

(cid:112)gD

x (v, v)

=

(cid:104)u, v(cid:105)
(cid:107)u(cid:107)(cid:107)v(cid:107)

.

2.3 Gyrovector spaces

In Euclidean space, natural operations inherited from the vectorial structure, such as vector addition,
subtraction and scalar multiplication are often useful. The framework of gyrovector spaces provides
an elegant non-associative algebraic formalism for hyperbolic geometry just as vector spaces provide
the algebraic setting for Euclidean geometry [2, 25, 26].

In particular, these operations are used in special relativity, allowing to add speed vectors belonging
to the Poincaré ball of radius c (the celerity, i.e. the speed of light) so that they remain in the ball,
hence not exceeding the speed of light.

We will make extensive use of these operations in our deﬁnitions of hyperbolic neural networks.
For c ≥ 0, denote2 by Dn
then Dn

c := {x ∈ Rn | c(cid:107)x(cid:107)2 < 1}. Note that if c = 0, then Dn
c. If c = 1 then we recover the usual ball Dn.

c is the open ball of radius 1/

c = Rn; if c > 0,

√

Möbius addition. The Möbius addition of x and y in Dn

c is deﬁned as

x ⊕c y :=

(1 + 2c(cid:104)x, y(cid:105) + c(cid:107)y(cid:107)2)x + (1 − c(cid:107)x(cid:107)2)y
1 + 2c(cid:104)x, y(cid:105) + c2(cid:107)x(cid:107)2(cid:107)y(cid:107)2

.

(6)

In particular, when c = 0, one recovers the Euclidean addition of two vectors in Rn. Note that
without loss of generality, the case c > 0 can be reduced to c = 1. Unless stated otherwise, we
will use ⊕ as ⊕1 to simplify notations. For general c > 0, this operation is not commutative nor
associative. However, it satisﬁes x ⊕c 0 = 0 ⊕c x = 0. Moreover, for any x, y ∈ Dn
c , we have
(−x) ⊕c x = x ⊕c (−x) = 0 and (−x) ⊕c (x ⊕c y) = y (left-cancellation law). The Möbius
substraction is then deﬁned by the use of the following notation: x (cid:9)c y := x ⊕c (−y). See [29,
section 2.1] for a geometric interpretation of the Möbius addition.

2We take different notations as in [25] where the author uses s = 1/

c.

√

3

Möbius scalar multiplication. For c > 0, the Möbius scalar multiplication of x ∈ Dn
r ∈ R is deﬁned as

c \ {0} by

r ⊗c x := (1/

c) tanh(r tanh−1(

c(cid:107)x(cid:107)))

√

√

x
(cid:107)x(cid:107)

,

(7)

and r ⊗c 0 := 0. Note that similarly as for the Möbius addition, one recovers the Euclidean scalar
multiplication when c goes to zero: limc→0 r ⊗c x = rx. This operation satisﬁes desirable properties
such as n ⊗c x = x ⊕c · · · ⊕c x (n additions), (r + r(cid:48)) ⊗c x = r ⊗c x ⊕c r(cid:48) ⊗c x (scalar distributivity3),
(rr(cid:48)) ⊗c x = r ⊗c (r(cid:48) ⊗c x) (scalar associativity) and |r| ⊗c x/(cid:107)r ⊗c x(cid:107) = x/(cid:107)x(cid:107) (scaling property).

c , gc) is given by4

Distance.
Euclidean one, with conformal factor λc
(Dn

If one deﬁnes the generalized hyperbolic metric tensor gc as the metric conformal to the
x := 2/(1 − c(cid:107)x(cid:107)2), then the induced distance function on
√

c) tanh−1 (cid:0)√
Again, observe that limc→0 dc(x, y) = 2(cid:107)x − y(cid:107), i.e. we recover Euclidean geometry in the limit5.
Moreover, for c = 1 we recover dD of Eq. (4).

c(cid:107) − x ⊕c y(cid:107)(cid:1) .

dc(x, y) = (2/

(8)

Hyperbolic trigonometry. Similarly as in the Euclidean space, one can deﬁne the notions of
hyperbolic angles or gyroangles (when using the ⊕c), as well as hyperbolic law of sines in the
generalized Poincaré ball (Dn

c , gc). We make use of these notions in our proofs. See Appendix A.

2.4 Connecting Gyrovector spaces and Riemannian geometry of the Poincaré ball

In this subsection, we present how geodesics in the Poincaré ball model are usually described with
Möbius operations, and push one step further the existing connection between gyrovector spaces and
the Poincaré ball by ﬁnding new identities involving the exponential map, and parallel transport.

In particular, these ﬁndings provide us with a simpler formulation of Möbius scalar multiplication,
yielding a natural deﬁnition of matrix-vector multiplication in the Poincaré ball.

Riemannian gyroline element. The Riemannian gyroline element is deﬁned for an inﬁnitesimal
dx as ds := (x + dx) (cid:9)c x, and its size is given by [26, section 3.7]:

(cid:107)ds(cid:107) = (cid:107)(x + dx) (cid:9)c x(cid:107) = (cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2).

(9)

What is remarkable is that it turns out to be identical, up to a scaling factor of 2, to the usual line
element 2(cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2) of the Riemannian manifold (Dn

c , gc).

Geodesics. The geodesic connecting points x, y ∈ Dn

c is shown in [2, 26] to be given by:

γx→y(t) := x ⊕c (−x ⊕c y) ⊗c t, with γx→y : R → Dn

c s.t. γx→y(0) = x and γx→y(1) = y.

Note that when c goes to 0, geodesics become straight-lines, recovering Euclidean geometry. In the
remainder of this subsection, we connect the gyrospace framework with Riemannian geometry.
Lemma 1. For any x ∈ Dn and v ∈ TxDn
c s.t. gc
x with direction v is given by:

x(v, v) = 1, the unit-speed geodesic starting from

γx,v(t) = x ⊕c

tanh

(cid:18)

(cid:18)√

(cid:19) v
√

c

t
2

c(cid:107)v(cid:107)

(cid:19)

, where γx,v : R → Dn s.t. γx,v(0) = x and ˙γx,v(0) = v.

(10)

(11)

Proof. One can use Eq. (10) and reparametrize it to unit-speed using Eq. (8). Alternatively, direct
computation and identiﬁcation with the formula in [11, Thm. 1] would give the same result. Using
Eq. (8) and Eq. (11), one can sanity-check that dc(γ(0), γ(t)) = t, ∀t ∈ [0, 1].

3⊗c has priority over ⊕c in the sense that a ⊗c b ⊕c c := (a ⊗c b) ⊕c c and a ⊕c b ⊗c c := a ⊕c (b ⊗c c).
4The notation −x ⊕c y should always be read as (−x) ⊕c y and not −(x ⊕c y).
5The factor 2 comes from the conformal factor λx = 2/(1 − (cid:107)x(cid:107)2), which is a convention setting the

curvature to −1.

4

Exponential and logarithmic maps. The following lemma gives the closed-form derivation of
exponential and logarithmic maps.
Lemma 2. For any point x ∈ Dn
map logc
c → TxDn
(cid:18)

c are given for v (cid:54)= 0 and y (cid:54)= x by:
(cid:18)√

c , the exponential map expc

c and the logarithmic

x : TxDn

c → Dn

x : Dn

√

(cid:19)

, logc

x(y) =

√

tanh−1(

c(cid:107) − x ⊕c y(cid:107))

expc

x(v) = x ⊕c

tanh

c

λc
x(cid:107)v(cid:107)
2

(cid:19) v
√

c(cid:107)v(cid:107)

2
cλc
x

−x ⊕c y
(cid:107) − x ⊕c y(cid:107)
(12)

.

Proof. Following the proof of [11, Cor. 1.1], one gets expc
gives the formula for expc

x. Algebraic check of the identity logc

x(v) = γx,
x(expc

v

x(cid:107)v(cid:107) (λc
λc

x(cid:107)v(cid:107)). Using Eq. (11)

x(v)) = v concludes.

The above maps have more appealing forms when x = 0, namely for v ∈ T0Dn

c \ {0}, y ∈ Dn

c \ {0}:

expc

0(v) = tanh(

c(cid:107)v(cid:107))

√

, logc

0(y) = tanh−1(

c(cid:107)y(cid:107))

√

(13)

√

y
c(cid:107)y(cid:107)

.

√

v
c(cid:107)v(cid:107)

Moreover, we still recover Euclidean geometry in the limit c → 0, as limc→0 expc
Euclidean exponential map, and limc→0 logc

x(y) = y − x is the Euclidean logarithmic map.

x(v) = x + v is the

Möbius scalar multiplication using exponential and logarithmic maps. We studied the expo-
nential and logarithmic maps in order to gain a better understanding of the Möbius scalar multiplica-
tion (Eq. (7)). We found the following:
Lemma 3. The quantity r ⊗ x can actually be obtained by projecting x in the tangent space at 0
with the logarithmic map, multiplying this projection by the scalar r in T0Dn
c , and then projecting it
back on the manifold with the exponential map:
0(r logc

∀r ∈ R, x ∈ Dn
c .

r ⊗c x = expc

0(x)),

(14)

In addition, we recover the well-known relation between geodesics connecting two points and the
exponential map:

γx→y(t) = x ⊕c (−x ⊕c y) ⊗c t = expc

x(t logc

x(y)),

t ∈ [0, 1].

(15)

This last result enables us to generalize scalar multiplication in order to deﬁne matrix-vector multipli-
cation between Poincaré balls, one of the essential building blocks of hyperbolic neural networks.

Parallel transport. Finally, we connect parallel transport (from T0Dn
the following theorem, which we prove in appendix B.
Theorem 4. In the manifold (Dn
vector v ∈ T0Dn

c to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a
c is given by the following isometry:
λc
0
λc
x

x(x ⊕c expc

0(v)) =

0→x(v) = logc
P c

(16)

v.

c ) to gyrovector spaces with

As we’ll see later, this result is crucial in order to deﬁne and optimize parameters shared between
different tangent spaces, such as biases in hyperbolic neural layers or parameters of hyperbolic MLR.

3 Hyperbolic Neural Networks

Neural networks can be seen as being made of compositions of basic operations, such as linear
maps, bias translations, pointwise non-linearities and a ﬁnal sigmoid or softmax layer. We ﬁrst
explain how to construct a softmax layer for logits lying in a Poincaré ball. Then, we explain how
to transform a mapping between two Euclidean spaces as one between Poincaré balls, yielding
matrix-vector multiplication and pointwise non-linearities in the Poincaré ball. Finally, we present
possible adaptations of various recurrent neural networks to the hyperbolic domain.

5

3.1 Hyperbolic multiclass logistic regression

In order to perform multi-class classiﬁcation on the Poincaré ball, one needs to generalize multinomial
logistic regression (MLR) − also called softmax regression − to the Poincaré ball.

Reformulating Euclidean MLR. Let’s ﬁrst reformulate Euclidean MLR from the perspective of
distances to margin hyperplanes, as in [19, Section 5]. This will allow us to easily generalize it.

Given K classes, one learns a margin hyperplane for each such class using softmax probabilities:

∀k ∈ {1, ..., K},

p(y = k|x) ∝ exp (((cid:104)ak, x(cid:105) − bk)) , where bk ∈ R, x, ak ∈ Rn.

(17)

Note that any afﬁne hyperplane in Rn can be written with a normal vector a and a scalar shift b:

Ha,b = {x ∈ Rn : (cid:104)a, x(cid:105) − b = 0}, where a ∈ Rn \ {0}, and b ∈ R.

(18)

As in [19, Section 5], we note that (cid:104)a, x(cid:105) − b = sign((cid:104)a, x(cid:105) − b)(cid:107)a(cid:107)d(x, Ha,b). Using Eq. (17):

p(y = k|x) ∝ exp(sign((cid:104)ak, x(cid:105) − bk)(cid:107)ak(cid:107)d(x, Hak,bk )), bk ∈ R, x, ak ∈ Rn.

(19)

As it is not immediately obvious how to generalize the Euclidean hyperplane of Eq. (18) to other
spaces such as the Poincaré ball, we reformulate it as follows:

˜Ha,p = {x ∈ Rn : (cid:104)−p + x, a(cid:105) = 0} = p + {a}⊥, where p ∈ Rn, a ∈ Rn \ {0}.

(20)

This new deﬁnition relates to the previous one as ˜Ha,p = Ha,(cid:104)a,p(cid:105). Rewriting Eq. (19) with b = (cid:104)a, p(cid:105):
p(y = k|x) ∝ exp(sign((cid:104)−pk + x, ak(cid:105))(cid:107)ak(cid:107)d(x, ˜Hak,pk )), with pk, x, ak ∈ Rn.

(21)

It is now natural to adapt the previous deﬁnition to the hyperbolic setting by replacing + by ⊕c:
Deﬁnition 3.1 (Poincaré hyperplanes). For p ∈ Dn
p(z, a) = 0} = {z ∈ TpDn
gc

c : (cid:104)z, a(cid:105) = 0}. Then, we deﬁne Poincaré hyperplanes as

c \ {0}, let {a}⊥ := {z ∈ TpDn
c :

c , a ∈ TpDn

˜H c

a,p := {x ∈ Dn

c : (cid:104)logc

p(x), a(cid:105)p = 0} = expc

p({a}⊥) = {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}.

(22)

The last equality is shown appendix C. ˜H c
all geodesics in Dn
hypergyroplanes, see [27, deﬁnition 5.8]. A 3D hyperplane example is depicted in Fig. 1.

a,p can also be described as the union of images of
c orthogonal to a and containing p. Notice that our deﬁnition matches that of

Next, we need the following theorem, proved in appendix D:
Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(23)

Final formula for MLR in the Poincaré ball. Putting together Eq. (21) and Thm. 5, we get the
hyperbolic MLR formulation. Given K classes and k ∈ {1, . . . , K}, pk ∈ Dn
c \ {0}:

c , ak ∈ Tpk

Dn

p(y = k|x) ∝ exp(sign((cid:104)−pk ⊕c x, ak(cid:105))

gc
pk

(ak, ak)dc(x, ˜H c

)),

ak,pk

∀x ∈ Dn
c ,

(24)

or, equivalently

p(y = k|x) ∝ exp

(cid:18) λc
pk

(cid:107)ak(cid:107)
√
c

sinh−1

(cid:18)

√
2

c(cid:104)−pk ⊕c x, ak(cid:105)

(cid:19)(cid:19)

(1 − c(cid:107) − pk ⊕c x(cid:107)2)(cid:107)ak(cid:107)

,

∀x ∈ Dn
c .

(25)

this goes to p(y = k|x) ∝ exp(4(cid:104)−pk + x, ak(cid:105)) =

Notice that when c goes to zero,
exp((λ0
pk

)2(cid:104)−pk + x, ak(cid:105)) = exp((cid:104)−pk + x, ak(cid:105)0), recovering the usual Euclidean softmax.
However, at this point it is unclear how to perform optimization over ak, since it lives in Tpk
hence depends on pk. The solution is that one should write ak = P c
)a(cid:48)
k ∈ T0Dn
a(cid:48)

c = Rn, and optimize a(cid:48)

k as a Euclidean parameter.

k) = (λc

0/λc
pk

0→pk

(a(cid:48)

Dn
c and
k, where

1
√
c

(cid:113)

6

3.2 Hyperbolic feed-forward layers

In order to deﬁne hyperbolic neural networks, it is crucial to de-
ﬁne a canonically simple parametric family of transformations,
playing the role of linear mappings in usual Euclidean neural
networks, and to know how to apply pointwise non-linearities.
Inspiring ourselves from our reformulation of Möbius scalar
multiplication in Eq. (14), we deﬁne:
Deﬁnition 3.2 (Möbius version). For f : Rn → Rm, we deﬁne
the Möbius version of f as the map from Dn

c to Dm

c by:

f ⊗c(x) := expc

0(f (logc

0(x))),

(26)

where expc

0 : T0m

Dm

c → Dm

c and logc

0 : Dn

c → T0n

Dn
c .

Figure 1: An example of a hyper-
bolic hyperplane in D3
1 plotted us-
ing sampling. The red point is p.
The shown normal axis to the hy-
perplane through p is parallel to a.

Note that similarly as for other Möbius operations, we recover
the Euclidean mapping in the limit c → 0 if f is continuous, as limc→0 f ⊗c(x) = f (x). This
deﬁnition satisﬁes a few desirable properties too, such as: (f ◦ g)⊗c = f ⊗c ◦ g⊗c for f : Rm → Rl
and g : Rn → Rm (morphism property), and f ⊗c(x)/(cid:107)f ⊗c(x)(cid:107) = f (x)/(cid:107)f (x)(cid:107) for f (x) (cid:54)= 0
(direction preserving). It is then straight-forward to prove the following result:
Lemma 6 (Möbius matrix-vector multiplication). If M : Rn → Rm is a linear map, which we
identify with its matrix representation, then ∀x ∈ Dn

c , if M x (cid:54)= 0 we have

M ⊗c(x) = (1/

c) tanh

√

(cid:18) (cid:107)M x(cid:107)
(cid:107)x(cid:107)

√

tanh−1(

c(cid:107)x(cid:107))

(cid:19) M x
(cid:107)M x(cid:107)

,

(27)

and M ⊗c(x) = 0 if M x = 0. Moreover, if we deﬁne the Möbius matrix-vector multiplication of
M ∈ Mm,n(R) and x ∈ Dn
c by M ⊗c x := M ⊗c(x), then we have (M M (cid:48)) ⊗c x = M ⊗c (M (cid:48) ⊗c x)
for M ∈ Ml,m(R) and M (cid:48) ∈ Mm,n(R) (matrix associativity), (rM ) ⊗c x = r ⊗c (M ⊗c x) for
r ∈ R and M ∈ Mm,n(R) (scalar-matrix associativity) and M ⊗c x = M x for all M ∈ On(R)
(rotations are preserved).

Pointwise non-linearity.
ϕ⊗c can be applied to elements of the Poincaré ball.

If ϕ : Rn → Rn is a pointwise non-linearity, then its Möbius version

Bias translation. The generalization of a translation in the Poincaré ball is naturally given by
moving along geodesics. But should we use the Möbius sum x ⊕c b with a hyperbolic bias b or the
x(b(cid:48)) with a Euclidean bias b(cid:48)? These views are uniﬁed with parallel transport
exponential map expc
c by a bias b ∈ Dn
(see Thm 4). Möbius translation of a point x ∈ Dn
(cid:18) λc
0
λc
x

c is given by
(cid:19)

x ← x ⊕c b = expc

0(b))) = expc
x

0→x(logc

x(P c

logc

0(b)

(28)

.

We recover Euclidean translations in the limit c → 0. Note that bias translations play a particular
Indeed, consider multiple layers of the form fk(x) = ϕk(Mkx), each of
role in this model.
which having Möbius version f ⊗c
k (Mk ⊗c x). Then their composition can be re-written
f ⊗c
k ◦ · · · ◦ f ⊗c
1 = expc
0. This means that these operations can essentially be
performed in Euclidean space. Therefore, it is the interposition between those with the bias translation
of Eq. (28) which differentiates this model from its Euclidean counterpart.

k (x) = ϕ⊗c
0 ◦fk ◦ · · · ◦ f1 ◦ logc

If a vector x ∈ Rn+p is the (vertical) concatenation
Concatenation of multiple input vectors.
of two vectors x1 ∈ Rn, x2 ∈ Rp, and M ∈ Mm,n+p(R) can be written as the (horizontal)
concatenation of two matrices M1 ∈ Mm,n(R) and M2 ∈ Mm,p(R), then M x = M1x1 + M2x2.
We generalize this to hyperbolic spaces: if we are given x1 ∈ Dn
c ×Dp
c ,
and M, M1, M2 as before, then we deﬁne M ⊗c x := M1 ⊗c x1 ⊕c M2 ⊗c x2. Note that when c goes
to zero, we recover the Euclidean formulation, as limc→0 M ⊗c x = limc→0 M1 ⊗c x1 ⊕c M2 ⊗c x2 =
M1x1 + M2x2 = M x. Moreover, hyperbolic vectors x ∈ Dn
c can also be "concatenated" with real
features y ∈ R by doing: M ⊗c x ⊕c y ⊗c b with learnable b ∈ Dm

c , x = (x1 x2)T ∈ Dn

c and M ∈ Mm,n(R).

c , x2 ∈ Dp

7

3.3 Hyperbolic RNN

Naive RNN. A simple RNN can be deﬁned by ht+1 = ϕ(W ht + U xt + b) where ϕ is a pointwise
non-linearity, typically tanh, sigmoid, ReLU, etc. This formula can be naturally generalized to the
hyperbolic space as follows. For parameters W ∈ Mm,n(R), U ∈ Mm,d(R), b ∈ Dm
c , we deﬁne:

ht+1 = ϕ⊗c (W ⊗c ht ⊕c U ⊗c xt ⊕c b),

ht ∈ Dn

c , xt ∈ Dd
c .

(29)

Note that if inputs xt’s are Euclidean, one can write ˜xt := expc
expc

(U xt)) = W ⊗c ht ⊕c expc

(P c

W ⊗cht

0→W ⊗cht

0(U xt) = W ⊗c ht ⊕c U ⊗c ˜xt.

0(xt) and use the above formula, since

GRU architecture. One can also adapt the GRU architecture:
rt = σ(W rht−1 + U rxt + br),
zt = σ(W zht−1 + U zxt + bz),
˜ht = ϕ(W (rt (cid:12) ht−1) + U xt + b), ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht,

(30)

where (cid:12) denotes pointwise product. First, how should we adapt the pointwise multiplication by a
scaling gate? Note that the deﬁnition of the Möbius version (see Eq. (26)) can be naturally extended
to maps f : Rn × Rp → Rm as f ⊗c : (h, h(cid:48)) ∈ Dn
0(h(cid:48)))). In
c (cid:55)→ expc
0(h(cid:48))) =
particular, choosing f (h, h(cid:48)) := σ(h) (cid:12) h(cid:48) yields6 f ⊗c(h, h(cid:48)) = expc
diag(σ(logc

0(h))) ⊗c h(cid:48). Hence we adapt rt (cid:12) ht−1 to diag(rt) ⊗c ht−1 and the reset gate rt to:
0(W r ⊗c ht−1 ⊕c U r ⊗c xt ⊕c br),

0(h), logc
0(h)) (cid:12) logc

0(f (logc
0(σ(logc

rt = σ logc

c × Dp

(31)

and similarly for the update gate zt. Note that as the argument of σ in the above is unbounded, rt and
zt can a priori take values onto the full range (0, 1). Now the intermediate hidden state becomes:
˜ht = ϕ⊗c ((W diag(rt)) ⊗c ht−1 ⊕c U ⊗c xt ⊕ b),

(32)

where Möbius matrix associativity simpliﬁes W ⊗c (diag(rt) ⊗c ht−1) into (W diag(rt)) ⊗c ht−1.
Finally, we propose to adapt the update-gate equation as

ht = ht−1 ⊕c diag(zt) ⊗c (−ht−1 ⊕c

˜ht).

(33)

Note that when c goes to zero, one recovers the usual GRU. Moreover, if zt = 0 or zt = 1, then ht
becomes ht−1 or ˜ht respectively, similarly as in the usual GRU. This adaptation was obtained by
adapting [24]: in this work, the authors re-derive the update-gate mechanism from a ﬁrst principle
called time-warping invariance. We adapted their derivation to the hyperbolic setting by using the
notion of gyroderivative [4] and proving a gyro-chain-rule (see appendix E).

4 Experiments

SNLI task and dataset. We evaluate our method on two tasks. The ﬁrst is natural language
inference, or textual entailment. Given two sentences, a premise (e.g. "Little kids A. and B. are
playing soccer.") and a hypothesis (e.g. "Two children are playing outdoors."), the binary classiﬁcation
task is to predict whether the second sentence can be inferred from the ﬁrst one. This deﬁnes a partial
order in the sentence space. We test hyperbolic networks on the biggest real dataset for this task,
SNLI [7]. It consists of 570K training, 10K validation and 10K test sentence pairs. Following [28],
we merge the "contradiction" and "neutral" classes into a single class of negative sentence pairs, while
the "entailment" class gives the positive pairs.

PREFIX task and datasets. We conjecture that the improvements of hyperbolic neural networks
are more signiﬁcant when the underlying data structure is closer to a tree. To test this, we design a
proof-of-concept task of detection of noisy preﬁxes, i.e. given two sentences, one has to decide if the
second sentence is a noisy preﬁx of the ﬁrst, or a random sentence. We thus build synthetic datasets
PREFIX-Z% (for Z being 10, 30 or 50) as follows: for each random ﬁrst sentence of random length
at most 20 and one random preﬁx of it, a second positive sentence is generated by randomly replacing
Z% of the words of the preﬁx, and a second negative sentence of same length is randomly generated.
Word vocabulary size is 100, and we generate 500K training, 10K validation and 10K test pairs.

6If x has n coordinates, then diag(x) denotes the diagonal matrix of size n with xi’s on its diagonal.

8

Models architecture. Our neural network layers can be used in a plug-n-play manner exactly like
standard Euclidean layers. They can also be combined with Euclidean layers. However, optimization
w.r.t. hyperbolic parameters is different (see below) and based on Riemannian gradients which
are just rescaled Euclidean gradients when working in the conformal Poincaré model [21]. Thus,
back-propagation can be applied in the standard way.

In our setting, we embed the two sentences using two distinct hyperbolic RNNs or GRUs. The
sentence embeddings are then fed together with their squared distance (hyperbolic or Euclidean,
depending on their geometry) to a FFNN (Euclidean or hyperbolic, see Sec. 3.2) which is further
fed to an MLR (Euclidean or hyperbolic, see Sec. 3.1) that gives probabilities of the two classes
(entailment vs neutral). We use cross-entropy loss on top. Note that hyperbolic and Euclidean layers
can be mixed, e.g. the full network can be hyperbolic and only the last layer be Euclidean, in which
case one has to use log0 and exp0 functions to move between the two manifolds in a correct manner
as explained for Eq. 26.

Optimization. Our models have both Euclidean (e.g. weight matrices in both Euclidean and
hyperbolic FFNNs, RNNs or GRUs) and hyperbolic parameters (e.g. word embeddings or biases for
the hyperbolic layers). We optimize the Euclidean parameters with Adam [16] (learning rate 0.001).
Hyperbolic parameters cannot be updated with an equivalent method that keeps track of gradient
history due to the absence of a Riemannian Adam. Thus, they are optimized using full Riemannian
stochastic gradient descent (RSGD) [5, 11]. We also experiment with projected RSGD [21], but
optimization was sometimes less stable. We use a different constant learning rate for word embeddings
(0.1) and other hyperbolic weights (0.01) because words are updated less frequently.

Numerical errors. Gradients of the basic operations deﬁned above (e.g. ⊕c, exponential map) are
c(cid:107)x(cid:107) = 1. Thus, we
not deﬁned when the hyperbolic argument vectors are on the ball border, i.e.
always project results of these operations in the ball of radius 1 − (cid:15), where (cid:15) = 10−5. Numerical
errors also appear when hyperbolic vectors get closer to 0, thus we perturb them with an (cid:15)(cid:48) = 10−15
before they are used in any of the above operations. Finally, arguments of the tanh function are
clipped between ±15 to avoid numerical errors, while arguments of tanh−1 are clipped to at most
1 − 10−5.

√

Hyperparameters. For all methods, baselines and datasets, we use c = 1, word and hidden state
embedding dimension of 5 (we focus on the low dimensional setting that was shown to already
be effective [21]), batch size of 64. We ran all methods for a ﬁxed number of 30 epochs. For all
models, we experiment with both identity (no non-linearity) or tanh non-linearity in the RNN/GRU
cell, as well as identity or ReLU after the FFNN layer and before MLR. As expected, for the fully
Euclidean models, tanh and ReLU respectively surpassed the identity variant by a large margin. We
only report the best Euclidean results. Interestingly, for the hyperbolic models, using only identity for
both non-linearities works slightly better and this is likely due to two facts: i) our hyperbolic layers
already contain non-linearities by their nature, ii) tanh is limiting the output domain of the sentence
embeddings, but the hyperbolic speciﬁc geometry is more pronounced at the ball border, i.e. at the
hyperbolic "inﬁnity", compared to the center of the ball.

For the results shown in Tab. 1, we run each model (baseline or ours) exactly 3 times and report the
test result corresponding to the best validation result from these 3 runs. We do this because the highly
non-convex spectrum of hyperbolic neural networks sometimes results in convergence to poor local
minima, suggesting that initialization is very important.

Results. Results are shown in Tab. 1. Note that the fully Euclidean baseline models might have
an advantage over hyperbolic baselines because more sophisticated optimization algorithms such
as Adam do not have a hyperbolic analogue at the moment. We ﬁrst observe that all GRU models
overpass their RNN variants. Hyperbolic RNNs and GRUs have the most signiﬁcant improvement
over their Euclidean variants when the underlying data structure is more tree-like, e.g. for PREFIX-
10% − for which the tree relation between sentences and their preﬁxes is more prominent − we
reduce the error by a factor of 3.35 for hyperbolic vs Euclidean RNN, and by a factor of 1.5 for
hyperbolic vs Euclidean GRU. As soon as the underlying structure diverges more and more from
a tree, the accuracy gap decreases − for example, for PREFIX-50% the noise heavily affects the
representational power of hyperbolic networks. Also, note that on SNLI our methods perform
similarly as with their Euclidean variants. Moreover, hyperbolic and Euclidean MLR are on par when

9

SNLI

PREFIX-10% PREFIX-30% PREFIX-50%

FULLY EUCLIDEAN RNN
HYPERBOLIC RNN+FFNN, EUCL MLR
FULLY HYPERBOLIC RNN
FULLY EUCLIDEAN GRU
HYPERBOLIC GRU+FFNN, EUCL MLR
FULLY HYPERBOLIC GRU

79.34 %
79.18 %
78.21 %
81.52 %
79.76 %
81.19 %

89.62 %
96.36 %
96.91 %
95.96 %
97.36 %
97.14 %

81.71 %
87.83 %
87.25 %
86.47 %
88.47 %
88.26 %

72.10 %
76.50 %
62.94 %
75.04 %
76.87 %
76.44 %

Table 1: Test accuracies for various models and four datasets. "Eucl" denotes Euclidean. All word
and sentence embeddings have dimension 5. We highlight in bold the best baseline (or baselines, if
the difference is less than 0.5%).

used in conjunction with hyperbolic sentence embeddings, suggesting further empirical investigation
is needed for this direction (see below).

We also observe that, in the hyperbolic setting, accuracy tends to increase when sentence embeddings
start increasing, and gets better as their norms converge towards 1 (the ball border for c = 1). Unlike
in the Euclidean case, this behavior does happen only after a few epochs and suggests that the model
should ﬁrst adjust the angular layout in order to disentangle the representations, before increasing their
norms to fully exploit the strong clustering property of the hyperbolic geometry. Similar behavior
was observed in the context of embedding trees by [21]. Details in appendix F.

MLR classiﬁcation experiments.
For the sentence entailment classi-
ﬁcation task we do not see a clear
advantage of hyperbolic MLR com-
pared to its Euclidean variant. A pos-
sible reason is that, when trained end-
to-end, the model might decide to
place positive and negative embed-
dings in a manner that is already well
separated with a classic MLR. As a
consequence, we further investigate
MLR for the task of subtree classiﬁ-
cation. Using an open source imple-
mentation7 of [21], we pre-trained
Poincaré embeddings of the Word-
Net noun hierarchy (82,115 nodes).
We then choose one node in this tree
(see Table 2) and classify all other
nodes (solely based on their embed-
dings) as being part of the subtree
rooted at this node. All nodes in such a subtree are divided into positive training nodes (80%) and
positive test nodes (20%). The same splitting procedure is applied for the remaining WordNet nodes
that are divided into a negative training and negative test set respectively. Three variants of MLR
are then trained on top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation
task: hyperbolic MLR, Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean
MLR applied after mapping all embeddings in the tangent space at 0 using the log0 map. We use
different embedding dimensions : 2, 3, 5 and 10. For the hyperbolic MLR, we use full Riemannian
SGD with a learning rate of 0.001. For the two Euclidean models we use ADAM optimizer and the
same learning rate. During training, we always sample the same number of negative and positive
nodes in each minibatch of size 16; thus positive nodes are frequently resampled. All methods are
trained for 30 epochs and the ﬁnal F1 score is reported (no hyperparameters to validate are used, thus
we do not require a validation set). This procedure is repeated for four subtrees of different sizes.

Figure 2: Hyperbolic (left) vs Direct Euclidean (right) binary
MLR used to classify nodes as being part in the GROUP.N.01
subtree of the WordNet noun hierarchy solely based on their
Poincaré embeddings. The positive points (from the subtree)
are in blue, the negative points (the rest) are in red and the
trained positive separation hyperplane is depicted in green.

Quantitative results are presented in Table 2. We can see that the hyperbolic MLR overpasses
its Euclidean variants in almost all settings, sometimes by a large margin. Moreover, to provide

7https://github.com/dalab/hyperbolic_cones

10

WORDNET
SUBTREE

ANIMAL.N.01
3218 / 798

GROUP.N.01
6649 / 1727

WORKER.N.01
861 / 254

MAMMAL.N.01
953 / 228

MODEL

D = 2

D = 3

D = 5

D = 10

HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL

47.43 ± 1.07%
41.69 ± 0.19%
38.89 ± 0.01%
81.72 ± 0.17%
61.13 ± 0.42%
60.75 ± 0.24%
12.68 ± 0.82%
10.86 ± 0.01%
9.04 ± 0.06%
32.01 ± 17.14%
15.58 ± 0.04%
13.10 ± 0.13%

91.92 ± 0.61%
68.43 ± 3.90%
62.57 ± 0.61%
89.87 ± 2.73%
63.56 ± 1.22%
61.98 ± 0.57%
24.09 ± 1.49%
22.39 ± 0.04%
22.57 ± 0.20%
87.54 ± 4.55%
44.68 ± 1.87%
44.89 ± 1.18%

98.07 ± 0.55%
95.59 ± 1.18%
89.21 ± 1.34%
87.89 ± 0.80%
67.82 ± 0.81%
67.92 ± 0.74%
55.46 ± 5.49%
35.23 ± 3.16%
26.47 ± 0.78%
88.73 ± 3.22%
59.35 ± 1.31%
52.51 ± 0.85%

99.26 ± 0.59%
99.36 ± 0.18%
98.27 ± 0.70%
91.91 ± 3.07%
91.38 ± 1.19%
91.41 ± 0.18%
66.83 ± 11.38%
47.29 ± 3.93%
36.66 ± 2.74%
91.37 ± 6.09%
77.76 ± 5.08%
56.11 ± 2.21%

Table 2: Test F1 classiﬁcation scores for four different subtrees of WordNet noun tree. All nodes
in such a subtree are divided into positive training nodes (80%) and positive test nodes (20%);
these counts are shown below each subtree root. The same splitting procedure is applied for the
remaining nodes to obtain negative training and test sets. Three variants of MLR are then trained on
top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation task: hyperbolic MLR,
Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean MLR applied after
mapping all embeddings in the tangent space at 0 using the log0 map. 95% conﬁdence intervals for 3
different runs are shown for each method and each different embedding dimension (2, 3, 5 or 10).

further understanding, we plot the 2-dimensional embeddings and the trained separation hyperplanes
(geodesics in this case) in Figure 2. We can see that respecting the hyperbolic geometry is very
important for a quality classiﬁcation model.

5 Conclusion

We showed how classic Euclidean deep learning tools such as MLR, FFNNs, RNNs or GRUs can be
generalized in a principled manner to all spaces of constant negative curvature combining Riemannian
geometry with the elegant theory of gyrovector spaces. Empirically we found that our models
outperform or are on par with corresponding Euclidean architectures on sequential data with implicit
hierarchical structure. We hope to trigger exciting future research related to better understanding
of the hyperbolic non-convexity spectrum and development of other non-Euclidean deep learning
methods.
Our data and Tensorﬂow [1] code are publicly available8.

Acknowledgements

We thank Igor Petrovski for useful pointers regarding the implementation.

This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement
number 167176. Gary Bécigneul is also funded by the Max Planck ETH Center for Learning
Systems.

References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for
large-scale machine learning. 2016.

[2] Ungar Abraham Albert. Analytic hyperbolic geometry and Albert Einstein’s special theory of

relativity. World scientiﬁc, 2008.

8https://github.com/dalab/hyperbolic_nn

11

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of the International Conference on Learning
Representations (ICLR), 2015.

[4] Graciela S Birman and Abraham A Ungar. The hyperbolic derivative in the poincaré ball model
of hyperbolic geometry. Journal of mathematical analysis and applications, 254(1):321–333,
2001.

[5] S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on

Automatic Control, 58(9):2217–2229, Sept 2013.

[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems (NIPS), pages 2787–2795, 2013.

[7] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 632–642. Association
for Computational Linguistics, 2015.

[8] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,
34(4):18–42, 2017.

[9] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry.

Flavors of geometry, 31:59–115, 1997.

[10] Christopher De Sa, Albert Gu, Christopher Ré, and Frederic Sala. Representation tradeoffs for

hyperbolic embeddings. arXiv preprint arXiv:1804.03329, 2018.

[11] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In Proceedings of the thirty-ﬁfth international conference
on machine learning (ICML), 2018.

[12] Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pages 75–263. Springer, 1987.

[13] Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the

Cambridge Philosophical Society, page 1–17, 2017.

[14] Christopher Hopper and Ben Andrews. The Ricci ﬂow in Riemannian geometry. Springer, 2010.

[15] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1746–1751. Association for Computational Linguistics, 2014.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations (ICLR), 2015.

[17] Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marián Boguná.

Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.

[18] John Lamping, Ramana Rao, and Peter Pirolli. A focus+ context technique based on hyperbolic
geometry for visualizing large hierarchies. In Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 401–408. ACM Press/Addison-Wesley Publishing Co.,
1995.

[19] Guy Lebanon and John Lafferty. Hyperplane margin classiﬁers on the multinomial manifold. In
Proceedings of the international conference on machine learning (ICML), page 66. ACM, 2004.

[20] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective
learning on multi-relational data. In Proceedings of the international conference on machine
learning (ICML), volume 11, pages 809–816, 2011.

12

[21] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical repre-
sentations. In Advances in Neural Information Processing Systems (NIPS), pages 6341–6350,
2017.

[22] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blun-
som. Reasoning about entailment with neural attention. In Proceedings of the International
Conference on Learning Representations (ICLR), 2015.

[23] Michael Spivak. A comprehensive introduction to differential geometry. Publish or perish, 1979.

[24] Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In Proceedings of

the International Conference on Learning Representations (ICLR), 2018.

[25] Abraham A Ungar. Hyperbolic trigonometry and its application in the poincaré ball model of

hyperbolic geometry. Computers & Mathematics with Applications, 41(1-2):135–147, 2001.

[26] Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis

Lectures on Mathematics and Statistics, 1(1):1–194, 2008.

[27] Abraham Albert Ungar. Analytic hyperbolic geometry in n dimensions: An introduction. CRC

Press, 2014.

[28] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. In Proceedings of the International Conference on Learning Representations (ICLR),
2016.

[29] J Vermeer. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic

plane. Topology and its Applications, 152(3):226–242, 2005.

13

A Hyperbolic Trigonometry

Hyperbolic angles. For A, B, C ∈ Dn
c , we denote by ∠A := ∠BAC the angle between the two
geodesics starting from A and ending at B and C respectively. This angle can be deﬁned in two
equivalent ways: i) either using the angle between the initial velocities of the two geodesics as given
by Eq. 5, or ii) using the formula

cos(∠A) =

(cid:28) (−A) ⊕c B
(cid:107)(−A) ⊕c B(cid:107)

,

(−A) ⊕c C
(cid:107)(−A) ⊕c C(cid:107)

(cid:29)

,

In this case, ∠A is also called a gyroangle in the work of [26, section 4].

Hyperbolic law of sines. We state here the hyperbolic law of sines. If for A, B, C ∈ Dn
c , we
denote by ∠B := ∠ABC the angle between the two geodesics starting from B and ending at A and
C respectively, and by ˜c = dc(B, A) the length of the hyperbolic segment BA (and similarly for
others), then we have:

sin(∠A)
√
c˜a)
sinh(

=

sin(∠B)
√
c˜b)
sinh(

=

sin(∠C)
√
c˜c)
sinh(

.

Note that one can also adapt the hyperbolic law of cosines to the hyperbolic space.

B Proof of Theorem 4

Theorem 4.
In the manifold (Dn
to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a vector v ∈ T0Dn
c

c is given by the following isometry:
λc
0
λc
x

0→x(v) = logc
P c

x(x ⊕c expc

0(v)) =

v.

Proof. The geodesic in Dn
v ∈ T0Dn
γ (i.e. X(t) ∈ Tγ(t)Dn

c from 0 to x is given in Eq. (10) by γ(t) = x ⊗c t, for t ∈ [0, 1]. Let
c . Then it is of common knowledge that there exists a unique parallel9 vector ﬁeld X along

c , ∀t ∈ [0, 1]) such that X(0) = v. Let’s deﬁne:
X : t ∈ [0, 1] (cid:55)→ logc

γ(t)(γ(t) ⊕c expc

0(v)) ∈ Tγ(t)Dn
c .

Clearly, X is a vector ﬁeld along γ such that X(0) = v. Now deﬁne
0→x : v ∈ T0Dn
P c

x(x ⊕c expc

0(v)) ∈ TxDn
c .

c (cid:55)→ logc
0→x(v) = λc

c . Since P c

0→x is a linear isometry from T0Dn
v, hence P c
c
0→x(v) = X(1), it is enough to prove that X is parallel in order to guarantee that

From Eq. (12), it is easily seen that P c
to TxDn
c to TxDn
0→x is the parallel transport from T0Dn
P c
c .
Since X is a vector ﬁeld along γ, its covariant derivative can be expressed with the Levi-Civita
connection ∇c associated to gc:

0
λc
x

DX
∂t

= ∇c

˙γ(t)X.

Let’s compute the Levi-Civita connection from its Christoffel symbols. In a local coordinate system,
they can be written as

Γi

jk =

(gc)il(∂jgc

lk + ∂kgc

lj − ∂lgc

jk),

1
2

where superscripts denote the inverse metric tensor and using Einstein’s notations. As gc
at γ(t) ∈ Dn

c this yields:

ij = (λc)2δij,

jk = cλc
Γi

γ(t)(δikγ(t)j + δijγ(t)k − δjkγ(t)i).

9i.e. that DX

∂t = 0 for t ∈ [0, 1], where D

∂t denotes the covariant derivative.

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

14

On the other hand, since X(t) = (λc

∇c

˙γ(t)X = ˙γ(t)i∇c

i X = ˙γ(t)i∇c
i

= vj ˙γ(t)i∇c
i

0/λc

γ(t))v, we have
(cid:32)

(cid:33)

λc
0
λc

γ(t)

v

(cid:32)

λc
0
λc

γ(t)

(cid:33)

ej

.

√

√

Since γ(t) = (1/
Hence there exists K x

c) tanh(t tanh−1(
t ∈ R such that ˙γ(t) = K x

c(cid:107)x(cid:107))) x

(cid:107)x(cid:107) , it is easily seen that ˙γ(t) is colinear to γ(t).
t γ(t). Moreover, we have the following Leibniz rule:
(cid:33)

(cid:32)

(cid:32)

∇c
i

λc
0
λc

γ(t)

(cid:33)

ej

=

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

ej.

Combining these yields

DX
∂t

= K x

t vjγ(t)i

(cid:32)

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

(cid:32)

(cid:33)

(cid:33)

ej

.

Replacing with the Christoffel symbols of ∇c at γ(t) gives

Moreover,

λc
0
λc

γ(t)

λc
0
λc

γ(t)

∇c

i ej =

ijek = 2c[δk
Γk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek.

∂
∂γ(t)i

(cid:32)

(cid:33)

λc
0
λc

γ(t)

ej =

∂
∂γ(t)i

(cid:0)−c(cid:107)γ(t)(cid:107)2(cid:1) ej = −2cγ(t)iej.

Putting together everything, we obtain

DX
∂t

= K x

t vjγ(t)i (cid:0)2c[δk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek − 2cγ(t)iej

(cid:1)

t vjγ(t)i (cid:0)γ(t)jei − δijγ(t)kek
t vj (cid:0)γ(t)jγ(t)iei − γ(t)iδijγ(t)kek
(cid:1)
t vj (cid:0)γ(t)jγ(t)iei − γ(t)jγ(t)kek

(cid:1)

(cid:1)

= 2cK x
= 2cK x
= 2cK x
= 0,

which concludes the proof.

C Proof of Eq. (22)

Proof. Two steps proof:
i) expc

p({a}⊥) ⊆ {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}:

Let z ∈ {a}⊥. From Eq. (12), we have that:

This, together with the left-cancellation law in gyrospaces (see section 2.3), implies that

expc

p(z) = −p ⊕c βz,

for some β ∈ R.

(cid:104)−p ⊕c expc

p(z), a(cid:105) = (cid:104)βz, a(cid:105) = 0

which is what we wanted.

ii) {x ∈ Dn
Let x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0} ⊆ expc
c s.t. (cid:104)−p ⊕c x, a(cid:105) = 0. Then, using Eq. (12), we derive that:
for some β ∈ R,

p(x) = β(−p ⊕c x),

p({a}⊥):

logc

which is orthogonal to a, by assumption. This implies logc

p(x) ∈ {a}⊥, hence x ∈ expc

p({a}⊥).

15

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(54)

D Proof of Theorem 5

Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

1
√
c

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(55)

Proof. We ﬁrst need to prove the following lemma, trivial in the Euclidean space, but not in the
Poincaré ball:
Lemma 7. (Orthogonal projection on a geodesic) Any point in the Poincaré ball has a unique
orthogonal projection on any given geodesic that does not pass through the point. Formally, for all
y ∈ Dn
c and for all geodesics γx→z(·) s.t. y /∈ Im γx→z, there exists an unique w ∈ Im γx→z s.t.
∠(γw→y, γx→z) = π/2.

Proof. We ﬁrst note that any geodesic in Dn
and has two "points at inﬁnity" lying on the ball border (v (cid:54)= 0):

c has the form γ(t) = u ⊕c v ⊗c t as given by Eq. 11,

γ(±∞) = u ⊕c

√

±v
c(cid:107)v(cid:107)

∈ ∂Dn
c .

(56)

Using the notations in the lemma statement, the closed-form of γx→z is given by Eq. (10):

γx→z(t) = x ⊕c (−x ⊕c z) ⊗c t

We denote by x(cid:48), z(cid:48) ∈ ∂Dn
∠ywx(cid:48) is well deﬁned from Eq. (34):

c its points at inﬁnity as described by Eq. (56). Then, the hyperbolic angle

cos(∠(γw→y, γx→z)) = cos(∠ywz(cid:48)) =

(cid:104)−w ⊕c y, −w ⊕c z(cid:48)(cid:105)
(cid:107) − w ⊕c y(cid:107) · (cid:107) − w ⊕c z(cid:48)(cid:107)

.

(57)

We now perform 2 steps for this proof.

i) Existence of w:

The angle function from Eq. (57) is continuous w.r.t t when w = γx→z(t). So we ﬁrst prove existence
of an angle of π/2 by continuously moving w from x(cid:48) to z(cid:48) when t goes from −∞ to ∞, and
observing that cos(∠ywz(cid:48)) goes from −1 to 1 as follows:

cos(∠yx(cid:48)z(cid:48)) = 1 & lim
w→z(cid:48)

cos(∠ywz(cid:48)) = −1.

(58)

The left part of Eq. (58) follows from Eq. (57) and from the fact (easy to show from the deﬁnition
c (which is the case of x(cid:48)). The right part of Eq. (58)
of ⊕c) that a ⊕c b = a, when (cid:107)a(cid:107) = 1/
follows from the fact that ∠ywz(cid:48) = π − ∠ywx(cid:48) (from the conformal property, or from Eq. (34)) and
cos(∠yz(cid:48)x(cid:48)) = 1 (proved as above).
Hence cos(∠ywz(cid:48)) has to pass through 0 when going from −1 to 1, which achieves the proof of
existence.

√

ii) Uniqueness of w:
Assume by contradiction that there are two w and w(cid:48) on γx→z that form angles ∠ywx(cid:48) and ∠yw(cid:48)x(cid:48)
of π/2. Since w, w(cid:48), x(cid:48) are on the same geodesic, we have

π/2 = ∠yw(cid:48)x(cid:48) = ∠yw(cid:48)w = ∠ywx(cid:48) = ∠yw(cid:48)w
So ∆yww(cid:48) has two right angles, but in the Poincaré ball this is impossible.

(59)

Now, we need two more lemmas:
Lemma 8. (Minimizing distance from point to geodesic) The orthogonal projection of a point to
a geodesic (not passing through the point) is minimizing the distance between the point and the
geodesic.

Proof. The proof is similar with the Euclidean case and it’s based on hyperbolic sine law and the fact
that in any right hyperbolic triangle the hypotenuse is strictly longer than any of the other sides.

16

Lemma 9. (Geodesics through p) Let ˜H c
all points on the geodesic γp→w are included in ˜H c

a,p.

a,p be a Poincaré hyperplane. Then, for any w ∈ ˜H c

a,p \ {p},

Proof. γp→w(t) = p ⊕c (−p ⊕c w) ⊗c t. Then, it is easy to check the condition in Eq. (22):

(cid:104)−p ⊕c γp→w(t), a(cid:105) = (cid:104)(−p ⊕c w) ⊗c t, a(cid:105) ∝ (cid:104)−p ⊕c w, a(cid:105) = 0.

(60)

We now turn back to our proof. Let x ∈ Dn
We prove that there is at least one point w∗ ∈ ˜H c

c be an arbitrary point and ˜H c

a,p a Poincaré hyperplane.

a,p that achieves the inﬁmum distance

dc(x, w∗) = inf
w∈ ˜H c

a,p

dc(x, w),

and, moreover, that this distance is the same as the one in the theorem’s statement.
We ﬁrst note that for any point w ∈ ˜H c
and Lemma 9, it is obvious that the projection of x to γp→w will give a strictly lower distance.
Thus, we only consider w ∈ ˜H c
triangle ∆xwp, one gets:

a,p such that ∠xwp = π/2. Applying hyperbolic sine law in the right

a,p, if ∠xwp (cid:54)= π/2, then w (cid:54)= w∗. Indeed, using Lemma 8

dc(x, w) = (1/

c) sinh−1 (cid:0)sinh(

c dc(x, p)) · sin(∠xpw)(cid:1) .

√

√

One of the above quantities does not depend on w:

√

√

sinh(

c dc(x, p)) = sinh(2 tanh−1(

c(cid:107) − p ⊕c x(cid:107))) =

√
2
c(cid:107) − p ⊕c x(cid:107)
1 − c(cid:107) − p ⊕c x(cid:107)2 .

The other quantity is sin(∠xpw) which is minimized when the angle ∠xpw is minimized (be-
cause ∠xpw < π/2 for the hyperbolic right triangle ∆xwp), or, alternatively, when cos(∠xpw) is
maximized. But, we already have from Eq. (34) that:

cos(∠xpw) =

(cid:104)−p ⊕c x, −p ⊕c w(cid:105)
(cid:107) − p ⊕c x(cid:107) · (cid:107) − p ⊕c w(cid:107)

.

To maximize the above, the constraint on the right angle at w can be dropped because cos(∠xpw)
depends only on the geodesic γp→w and not on w itself, and because there is always an orthogonal
projection from any point x to any geodesic as stated by Lemma 7. Thus, it remains to ﬁnd the
maximum of Eq. (64) when w ∈ ˜H c
a,p from Eq. (22), one can easily
prove that

a,p. Using the deﬁnition of ˜H c

Using that fact that logc

p(w)/(cid:107) logc

p(w)(cid:107) = −p ⊕c w/(cid:107) − p ⊕c w(cid:107), we just have to ﬁnd

and we are left with a well known Euclidean problem which is equivalent to ﬁnding the minimum
angle between the vector −p ⊕c x (viewed as Euclidean) and the hyperplane {a}⊥. This angle
is given by the Euclidean orthogonal projection whose sin value is the distance from the vector’s
endpoint to the hyperplane divided by the vector’s length:

{logc

p(w) : w ∈ ˜H c

a,p} = {a}⊥.

max
z∈{a}⊥

(cid:18) (cid:104)−p ⊕c x, z(cid:105)

(cid:107) − p ⊕c x(cid:107) · (cid:107)z(cid:107)

(cid:19)

,

sin(∠xpw∗) =

|(cid:104)−p ⊕c x, a
(cid:107) − p ⊕c x(cid:107)

(cid:107)a(cid:107) (cid:105)|

.

17

It follows that a point w∗ ∈ ˜H c
Eqs. (61),(62),(63) and (67) concludes the proof.

a,p satisfying Eq. (67) exists (but might not be unique). Combining

(61)

(62)

(63)

(64)

(65)

(66)

(67)

(cid:3)

E Derivation of the Hyperbolic GRU Update-gate

In [24], the authors recover the update/forget-gate mechanism of a GRU/LSTM by requiring that the
class of neural networks given by the chosen architecture be invariant to time-warpings. The idea is
the following.

Recovering the update-gate from time-warping. A naive RNN is given by the equation

h(t + 1) = ϕ(W h(t) + U x(t) + b)

Let’s drop the bias b to simplify notations. If h is seen as a differentiable function of time, then a
ﬁrst-order Taylor development gives h(t + δt) ≈ h(t) + δt dh
dt (t) for small δt. Combining this for
δt = 1 with the naive RNN equation, one gets

dh
dt

dα
dt

(t) = ϕ(W h(t) + U x(t)) − h(t).

As this is written for any t, one can replace it by t ← α(t) where α is a (smooth) increasing function
of t called the time-warping. Denoting by ˜h(t) := h(α(t)) and ˜x(t) := x(α(t)), using the chain rule
d˜h
dt (t) = dα

dt (α(t)), one gets

dt (t) dh

d˜h
dt

dα
dt

(t) =

(t)ϕ(W ˜h(t) + U ˜x(t)) −

(t)˜h(t).

(70)

Removing the tildas to simplify notations, discretizing back with dh

dt (t) ≈ h(t + 1) − h(t) yields

h(t + 1) =

(t)ϕ(W h(t) + U x(t)) +

1 −

(t)

h(t).

(71)

dα
dt

(cid:18)

(cid:19)

dα
dt

Requiring that our class of neural networks be invariant to time-warpings means that this class should
contain RNNs deﬁned by Eq. (71), i.e. that dα
dt (t) can be learned. As this is a positive quantity, we
can parametrize it as z(t) = σ(W zh(t) + U zx(t)), recovering the forget-gate equation:

h(t + 1) = z(t)ϕ(W h(t) + U x(t)) + (1 − z(t))h(t).

Adapting this idea to hyperbolic RNNs. The gyroderivative [4] of a map h : R → Dn
as

c is deﬁned

dh
dt

(t) = lim
δt→0

1
δt

⊗c (−h(t) ⊕c h(t + δt)).

Using Möbius scalar associativity and the left-cancellation law leads us to

h(t + δt) ≈ h(t) ⊕c δt ⊗c

(t),

dh
dt

for small δt. Combining this with the equation of a simple hyperbolic RNN of Eq. (29) with δt = 1,
one gets

dh
dt

(t) = −h(t) ⊕c ϕ⊗c(W ⊗c h(t) ⊕c U ⊗c x(t)).

For the next step, we need the following lemma:
Lemma 10 (Gyro-chain-rule). For α : R → R differentiable and h : R → Dn
gyro-derivative, if ˜h := h ◦ α, then we have

c with a well-deﬁned

(68)

(69)

(72)

(73)

(74)

(75)

(76)

where dα

dt (t) denotes the usual derivative.

d˜h
dt

(t) =

(t) ⊗c

(α(t)),

dα
dt

dh
dt

18

(77)

(78)

(79)

(80)

(81)

Proof.

d˜h
dt

(t) = lim
δt→0

1
δt
1
δt

⊗c [−˜h(t) ⊕c

˜h(t + δt)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

= lim
δt→0

= lim
δt→0

= lim
δt→0

= lim
u→0
dα
dt

=

α(cid:48)(t) + O(δt)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
u

(t) ⊗c

(α(t))

dh
dt

⊗c [−h(α(t)) ⊕c h(α(t) + u)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

(Möbius scalar associativity) (82)

where we set u = δt(α(cid:48)(t) + O(δt)), with u → 0 when δt → 0, which concludes.

Using lemma 10 and Eq. (75), with similar notations as in Eq. (70) we have

d˜h
dt

dα
dt

(t) =

(t) ⊗c (−˜h(t) ⊕c ϕ⊗c(W ⊗c

˜h(t) ⊕c U ⊗c ˜x(t))).

(83)

Finally, discretizing back with Eq. (74), using the left-cancellation law and dropping the tildas yields

h(t + 1) = h(t) ⊕c

(t) ⊗c (−h(t) ⊕c ϕ⊗c (W ⊗c h(t) ⊕c U ⊗c x(t))).

(84)

dα
dt

Since α is a time-warping, by deﬁnition its derivative is positive and one can choose to parametrize
it with an update-gate zt (a scalar) deﬁned with a sigmoid. Generalizing this scalar scaling by the
Möbius version of the pointwise scaling (cid:12) yields the Möbius matrix scaling diag(zt) ⊗c ·, leading to
our proposed Eq. (33) for the hyperbolic GRU.

F More Experimental Investigations

The following empirical facts were observed for both hyperbolic RNNs and GRUs.

We observed that, in the hyperbolic setting, accuracy is often much higher when sentence embeddings
can go close to the border (hyperbolic "inﬁnity"), hence exploiting the hyperbolic nature of the space.
Moreover, the faster the two sentence norms go to 1, the more it’s likely that a good local minima
was reached. See ﬁgures 3 and 5.

We often observe that test accuracy starts increasing exactly when sentence embedding norms do.
However, in the hyperbolic setting, the sentence embeddings norms remain close to 0 for a few
epochs, which does not happen in the Euclidean case. See ﬁgures 3, 5 and 4. This mysterious fact
was also exhibited in a similar way by [21] which suggests that the model ﬁrst has to adjust the
angular layout in the almost Euclidean vicinity of 0 before increasing norms and fully exploiting
hyperbolic geometry.

19

(a) Test accuracy

(a) Test accuracy

20

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 3: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the same
architecture: hyperbolic GRU followed by hyperbolic FFNN and hyperbolic/Euclidean (half-half)
MLR. The X axis shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 4: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: Euclidean GRU followed by Euclidean FFNN and Euclidean MLR. The X axis
shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 5: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: hyperbolic RNN followed by hyperbolic FFNN and hyperbolic MLR. The X axis
shows millions of training examples processed.

(a) Test accuracy

21

8
1
0
2
 
n
u
J
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
2
1
1
9
0
.
5
0
8
1
:
v
i
X
r
a

Hyperbolic Neural Networks

Octavian-Eugen Ganea∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
octavian.ganea@inf.ethz.ch

Gary Bécigneul∗
Department of Computer Science
ETH Zürich
Zurich, Switzerland
gary.becigneul@inf.ethz.ch

Thomas Hofmann
Department of Computer Science
ETH Zürich
Zurich, Switzerland
thomas.hofmann@inf.ethz.ch

Abstract

Hyperbolic spaces have recently gained momentum in the context of machine
learning due to their high capacity and tree-likeliness properties. However, the
representational power of hyperbolic geometry is not yet on par with Euclidean
geometry, mostly because of the absence of corresponding hyperbolic neural
network layers. This makes it hard to use hyperbolic embeddings in downstream
tasks. Here, we bridge this gap in a principled manner by combining the formalism
of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model
of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep
learning tools: multinomial logistic regression, feed-forward and recurrent neural
networks such as gated recurrent units. This allows to embed sequential data and
perform classiﬁcation in the hyperbolic space. Empirically, we show that, even if
hyperbolic optimization tools are limited, hyperbolic sentence embeddings either
outperform or are on par with their Euclidean variants on textual entailment and
noisy-preﬁx recognition tasks.

1

Introduction

It is common in machine learning to represent data as being embedded in the Euclidean space Rn. The
main reason for such a choice is simply convenience, as this space has a vectorial structure, closed-
form formulas of distance and inner-product, and is the natural generalization of our intuition-friendly,
visual three-dimensional space. Moreover, embedding entities in such a continuous space allows to
feed them as input to neural networks, which has led to unprecedented performance on a broad range
of problems, including sentiment detection [15], machine translation [3], textual entailment [22] or
knowledge base link prediction [20, 6].

Despite the success of Euclidean embeddings, recent research has proven that many types of com-
plex data (e.g. graph data) from a multitude of ﬁelds (e.g. Biology, Network Science, Computer
Graphics or Computer Vision) exhibit a highly non-Euclidean latent anatomy [8]. In such cases, the
Euclidean space does not provide the most powerful or meaningful geometrical representations. For
example, [10] shows that arbitrary tree structures cannot be embedded with arbitrary low distortion
(i.e. almost preserving their metric) in the Euclidean space with unbounded number of dimensions,
but this task becomes surprisingly easy in the hyperbolic space with only 2 dimensions where the
exponential growth of distances matches the exponential growth of nodes with the tree depth.

∗Equal contribution.

The adoption of neural networks and deep learning in these non-Euclidean settings has been rather
limited until very recently, the main reason being the non-trivial or impossible principled general-
izations of basic operations (e.g. vector addition, matrix-vector multiplication, vector translation,
vector inner product) as well as, in more complex geometries, the lack of closed form expressions for
basic objects (e.g. distances, geodesics, parallel transport). Thus, classic tools such as multinomial
logistic regression (MLR), feed forward (FFNN) or recurrent neural networks (RNN) did not have a
correspondence in these geometries.

How should one generalize deep neural models to non-Euclidean domains ? In this paper we address
this question for one of the simplest, yet useful, non-Euclidean domains: spaces of constant negative
curvature, i.e. hyperbolic. Their tree-likeness properties have been extensively studied [12, 13, 26]
and used to visualize large taxonomies [18] or to embed heterogeneous complex networks [17]. In
machine learning, recently, hyperbolic representations greatly outperformed Euclidean embeddings
for hierarchical, taxonomic or entailment data [21, 10, 11]. Disjoint subtrees from the latent hierar-
chical structure surprisingly disentangle and cluster in the embedding space as a simple reﬂection of
the space’s negative curvature. However, appropriate deep learning tools are needed to embed feature
data in this space and use it in downstream tasks. For example, implicitly hierarchical sequence data
(e.g. textual entailment data, phylogenetic trees of DNA sequences or hierarchial captions of images)
would beneﬁt from suitable hyperbolic RNNs.

The main contribution of this paper is to bridge the gap between hyperbolic and Euclidean geometry
in the context of neural networks and deep learning by generalizing in a principled manner both the
basic operations as well as multinomial logistic regression (MLR), feed-forward (FFNN), simple and
gated (GRU) recurrent neural networks (RNN) to the Poincaré model of the hyperbolic geometry.
We do it by connecting the theory of gyrovector spaces and generalized Möbius transformations
introduced by [2, 26] with the Riemannian geometry properties of the manifold. We smoothly
parametrize basic operations and objects in all spaces of constant negative curvature using a uniﬁed
framework that depends only on the curvature value. Thus, we show how Euclidean and hyperbolic
spaces can be continuously deformed into each other. On a series of experiments and datasets we
showcase the effectiveness of our hyperbolic neural network layers compared to their "classic"
Euclidean variants on textual entailment and noisy-preﬁx recognition tasks. We hope that this paper
will open exciting future directions in the nascent ﬁeld of Geometric Deep Learning.

2 The Geometry of the Poincaré Ball

2.1 Basics of Riemannian geometry

We brieﬂy introduce basic concepts of differential geometry largely needed for a principled general-
ization of Euclidean neural networks. For more rigorous and in-depth expositions, see [23, 14].
An n-dimensional manifold M is a space that can locally be approximated by Rn: it is a generalization
to higher dimensions of the notion of a 2D surface. For x ∈ M, one can deﬁne the tangent space
TxM of M at x as the ﬁrst order linear approximation of M around x. A Riemannian metric
g = (gx)x∈M on M is a collection of inner-products gx : TxM × TxM → R varying smoothly
with x. A Riemannian manifold (M, g) is a manifold M equipped with a Riemannian metric g.
Although a choice of a Riemannian metric g on M only seems to deﬁne the geometry locally on M,
it induces global distances by integrating the length (of the speed vector living in the tangent space)
of a shortest path between two points:

(cid:90) 1

(cid:113)

d(x, y) = inf
γ

0

gγ(t)( ˙γ(t), ˙γ(t))dt,

(1)

where γ ∈ C∞([0, 1], M) is such that γ(0) = x and γ(1) = y. A smooth path γ of minimal length
between two points x and y is called a geodesic, and can be seen as the generalization of a straight-line
in Euclidean space. The parallel transport Px→y : TxM → TyM is a linear isometry between
tangent spaces which corresponds to moving tangent vectors along geodesics and deﬁnes a canonical
way to connect tangent spaces. The exponential map expx at x, when well-deﬁned, gives a way to
project back a vector v of the tangent space TxM at x, to a point expx(v) ∈ M on the manifold.
This map is often used to parametrize a geodesic γ starting from γ(0) := x ∈ M with unit-norm
direction ˙γ(0) := v ∈ TxM as t (cid:55)→ expx(tv). For geodesically complete manifolds, such as the
Poincaré ball considered in this work, expx is well-deﬁned on the full tangent space TxM. Finally, a

2

(2)

(3)

(4)

(5)

metric ˜g is said to be conformal to another metric g if it deﬁnes the same angles, i.e.

˜gx(u, v)
(cid:112)˜gx(u, u)(cid:112)˜gx(v, v)

=

gx(u, v)
(cid:112)gx(u, u)(cid:112)gx(v, v)

,

for all x ∈ M, u, v ∈ TxM \ {0}. This is equivalent to the existence of a smooth function
λ : M → R, called the conformal factor, such that ˜gx = λ2

xgx for all x ∈ M.

2.2 Hyperbolic space: the Poincaré ball

The hyperbolic space has ﬁve isometric models that one can work with [9]. Similarly as in [21] and
[11], we choose to work in the Poincaré ball. The Poincaré ball model (Dn, gD) is deﬁned by the
manifold Dn = {x ∈ Rn : (cid:107)x(cid:107) < 1} equipped with the following Riemannian metric:

gD
x = λ2

xgE, where λx :=

2
1 − (cid:107)x(cid:107)2 ,

gE = In being the Euclidean metric tensor. Note that the hyperbolic metric tensor is conformal to
the Euclidean one. The induced distance between two points x, y ∈ Dn is known to be given by

dD(x, y) = cosh−1

1 + 2

(cid:18)

(cid:107)x − y(cid:107)2
(1 − (cid:107)x(cid:107)2)(1 − (cid:107)y(cid:107)2)

(cid:19)

.

Since the Poincaré ball is conformal to Euclidean space, the angle between two vectors u, v ∈
TxDn \ {0} is given by

cos(∠(u, v)) =

gD
x (u, v)
x (u, u)(cid:112)gD

(cid:112)gD

x (v, v)

=

(cid:104)u, v(cid:105)
(cid:107)u(cid:107)(cid:107)v(cid:107)

.

2.3 Gyrovector spaces

In Euclidean space, natural operations inherited from the vectorial structure, such as vector addition,
subtraction and scalar multiplication are often useful. The framework of gyrovector spaces provides
an elegant non-associative algebraic formalism for hyperbolic geometry just as vector spaces provide
the algebraic setting for Euclidean geometry [2, 25, 26].

In particular, these operations are used in special relativity, allowing to add speed vectors belonging
to the Poincaré ball of radius c (the celerity, i.e. the speed of light) so that they remain in the ball,
hence not exceeding the speed of light.

We will make extensive use of these operations in our deﬁnitions of hyperbolic neural networks.
For c ≥ 0, denote2 by Dn
then Dn

c := {x ∈ Rn | c(cid:107)x(cid:107)2 < 1}. Note that if c = 0, then Dn
c. If c = 1 then we recover the usual ball Dn.

c is the open ball of radius 1/

c = Rn; if c > 0,

√

Möbius addition. The Möbius addition of x and y in Dn

c is deﬁned as

x ⊕c y :=

(1 + 2c(cid:104)x, y(cid:105) + c(cid:107)y(cid:107)2)x + (1 − c(cid:107)x(cid:107)2)y
1 + 2c(cid:104)x, y(cid:105) + c2(cid:107)x(cid:107)2(cid:107)y(cid:107)2

.

(6)

In particular, when c = 0, one recovers the Euclidean addition of two vectors in Rn. Note that
without loss of generality, the case c > 0 can be reduced to c = 1. Unless stated otherwise, we
will use ⊕ as ⊕1 to simplify notations. For general c > 0, this operation is not commutative nor
associative. However, it satisﬁes x ⊕c 0 = 0 ⊕c x = 0. Moreover, for any x, y ∈ Dn
c , we have
(−x) ⊕c x = x ⊕c (−x) = 0 and (−x) ⊕c (x ⊕c y) = y (left-cancellation law). The Möbius
substraction is then deﬁned by the use of the following notation: x (cid:9)c y := x ⊕c (−y). See [29,
section 2.1] for a geometric interpretation of the Möbius addition.

2We take different notations as in [25] where the author uses s = 1/

c.

√

3

Möbius scalar multiplication. For c > 0, the Möbius scalar multiplication of x ∈ Dn
r ∈ R is deﬁned as

c \ {0} by

r ⊗c x := (1/

c) tanh(r tanh−1(

c(cid:107)x(cid:107)))

√

√

x
(cid:107)x(cid:107)

,

(7)

and r ⊗c 0 := 0. Note that similarly as for the Möbius addition, one recovers the Euclidean scalar
multiplication when c goes to zero: limc→0 r ⊗c x = rx. This operation satisﬁes desirable properties
such as n ⊗c x = x ⊕c · · · ⊕c x (n additions), (r + r(cid:48)) ⊗c x = r ⊗c x ⊕c r(cid:48) ⊗c x (scalar distributivity3),
(rr(cid:48)) ⊗c x = r ⊗c (r(cid:48) ⊗c x) (scalar associativity) and |r| ⊗c x/(cid:107)r ⊗c x(cid:107) = x/(cid:107)x(cid:107) (scaling property).

c , gc) is given by4

Distance.
Euclidean one, with conformal factor λc
(Dn

If one deﬁnes the generalized hyperbolic metric tensor gc as the metric conformal to the
x := 2/(1 − c(cid:107)x(cid:107)2), then the induced distance function on
√

c) tanh−1 (cid:0)√
Again, observe that limc→0 dc(x, y) = 2(cid:107)x − y(cid:107), i.e. we recover Euclidean geometry in the limit5.
Moreover, for c = 1 we recover dD of Eq. (4).

c(cid:107) − x ⊕c y(cid:107)(cid:1) .

dc(x, y) = (2/

(8)

Hyperbolic trigonometry. Similarly as in the Euclidean space, one can deﬁne the notions of
hyperbolic angles or gyroangles (when using the ⊕c), as well as hyperbolic law of sines in the
generalized Poincaré ball (Dn

c , gc). We make use of these notions in our proofs. See Appendix A.

2.4 Connecting Gyrovector spaces and Riemannian geometry of the Poincaré ball

In this subsection, we present how geodesics in the Poincaré ball model are usually described with
Möbius operations, and push one step further the existing connection between gyrovector spaces and
the Poincaré ball by ﬁnding new identities involving the exponential map, and parallel transport.

In particular, these ﬁndings provide us with a simpler formulation of Möbius scalar multiplication,
yielding a natural deﬁnition of matrix-vector multiplication in the Poincaré ball.

Riemannian gyroline element. The Riemannian gyroline element is deﬁned for an inﬁnitesimal
dx as ds := (x + dx) (cid:9)c x, and its size is given by [26, section 3.7]:

(cid:107)ds(cid:107) = (cid:107)(x + dx) (cid:9)c x(cid:107) = (cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2).

(9)

What is remarkable is that it turns out to be identical, up to a scaling factor of 2, to the usual line
element 2(cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2) of the Riemannian manifold (Dn

c , gc).

Geodesics. The geodesic connecting points x, y ∈ Dn

c is shown in [2, 26] to be given by:

γx→y(t) := x ⊕c (−x ⊕c y) ⊗c t, with γx→y : R → Dn

c s.t. γx→y(0) = x and γx→y(1) = y.

Note that when c goes to 0, geodesics become straight-lines, recovering Euclidean geometry. In the
remainder of this subsection, we connect the gyrospace framework with Riemannian geometry.
Lemma 1. For any x ∈ Dn and v ∈ TxDn
c s.t. gc
x with direction v is given by:

x(v, v) = 1, the unit-speed geodesic starting from

γx,v(t) = x ⊕c

tanh

(cid:18)

(cid:18)√

(cid:19) v
√

c

t
2

c(cid:107)v(cid:107)

(cid:19)

, where γx,v : R → Dn s.t. γx,v(0) = x and ˙γx,v(0) = v.

(10)

(11)

Proof. One can use Eq. (10) and reparametrize it to unit-speed using Eq. (8). Alternatively, direct
computation and identiﬁcation with the formula in [11, Thm. 1] would give the same result. Using
Eq. (8) and Eq. (11), one can sanity-check that dc(γ(0), γ(t)) = t, ∀t ∈ [0, 1].

3⊗c has priority over ⊕c in the sense that a ⊗c b ⊕c c := (a ⊗c b) ⊕c c and a ⊕c b ⊗c c := a ⊕c (b ⊗c c).
4The notation −x ⊕c y should always be read as (−x) ⊕c y and not −(x ⊕c y).
5The factor 2 comes from the conformal factor λx = 2/(1 − (cid:107)x(cid:107)2), which is a convention setting the

curvature to −1.

4

Exponential and logarithmic maps. The following lemma gives the closed-form derivation of
exponential and logarithmic maps.
Lemma 2. For any point x ∈ Dn
map logc
c → TxDn
(cid:18)

c are given for v (cid:54)= 0 and y (cid:54)= x by:
(cid:18)√

c , the exponential map expc

c and the logarithmic

x : TxDn

c → Dn

x : Dn

√

(cid:19)

, logc

x(y) =

√

tanh−1(

c(cid:107) − x ⊕c y(cid:107))

expc

x(v) = x ⊕c

tanh

c

λc
x(cid:107)v(cid:107)
2

(cid:19) v
√

c(cid:107)v(cid:107)

2
cλc
x

−x ⊕c y
(cid:107) − x ⊕c y(cid:107)
(12)

.

Proof. Following the proof of [11, Cor. 1.1], one gets expc
gives the formula for expc

x. Algebraic check of the identity logc

x(v) = γx,
x(expc

v

x(cid:107)v(cid:107) (λc
λc

x(cid:107)v(cid:107)). Using Eq. (11)

x(v)) = v concludes.

The above maps have more appealing forms when x = 0, namely for v ∈ T0Dn

c \ {0}, y ∈ Dn

c \ {0}:

expc

0(v) = tanh(

c(cid:107)v(cid:107))

√

, logc

0(y) = tanh−1(

c(cid:107)y(cid:107))

√

(13)

√

y
c(cid:107)y(cid:107)

.

√

v
c(cid:107)v(cid:107)

Moreover, we still recover Euclidean geometry in the limit c → 0, as limc→0 expc
Euclidean exponential map, and limc→0 logc

x(y) = y − x is the Euclidean logarithmic map.

x(v) = x + v is the

Möbius scalar multiplication using exponential and logarithmic maps. We studied the expo-
nential and logarithmic maps in order to gain a better understanding of the Möbius scalar multiplica-
tion (Eq. (7)). We found the following:
Lemma 3. The quantity r ⊗ x can actually be obtained by projecting x in the tangent space at 0
with the logarithmic map, multiplying this projection by the scalar r in T0Dn
c , and then projecting it
back on the manifold with the exponential map:
0(r logc

∀r ∈ R, x ∈ Dn
c .

r ⊗c x = expc

0(x)),

(14)

In addition, we recover the well-known relation between geodesics connecting two points and the
exponential map:

γx→y(t) = x ⊕c (−x ⊕c y) ⊗c t = expc

x(t logc

x(y)),

t ∈ [0, 1].

(15)

This last result enables us to generalize scalar multiplication in order to deﬁne matrix-vector multipli-
cation between Poincaré balls, one of the essential building blocks of hyperbolic neural networks.

Parallel transport. Finally, we connect parallel transport (from T0Dn
the following theorem, which we prove in appendix B.
Theorem 4. In the manifold (Dn
vector v ∈ T0Dn

c to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a
c is given by the following isometry:
λc
0
λc
x

x(x ⊕c expc

0(v)) =

0→x(v) = logc
P c

(16)

v.

c ) to gyrovector spaces with

As we’ll see later, this result is crucial in order to deﬁne and optimize parameters shared between
different tangent spaces, such as biases in hyperbolic neural layers or parameters of hyperbolic MLR.

3 Hyperbolic Neural Networks

Neural networks can be seen as being made of compositions of basic operations, such as linear
maps, bias translations, pointwise non-linearities and a ﬁnal sigmoid or softmax layer. We ﬁrst
explain how to construct a softmax layer for logits lying in a Poincaré ball. Then, we explain how
to transform a mapping between two Euclidean spaces as one between Poincaré balls, yielding
matrix-vector multiplication and pointwise non-linearities in the Poincaré ball. Finally, we present
possible adaptations of various recurrent neural networks to the hyperbolic domain.

5

3.1 Hyperbolic multiclass logistic regression

In order to perform multi-class classiﬁcation on the Poincaré ball, one needs to generalize multinomial
logistic regression (MLR) − also called softmax regression − to the Poincaré ball.

Reformulating Euclidean MLR. Let’s ﬁrst reformulate Euclidean MLR from the perspective of
distances to margin hyperplanes, as in [19, Section 5]. This will allow us to easily generalize it.

Given K classes, one learns a margin hyperplane for each such class using softmax probabilities:

∀k ∈ {1, ..., K},

p(y = k|x) ∝ exp (((cid:104)ak, x(cid:105) − bk)) , where bk ∈ R, x, ak ∈ Rn.

(17)

Note that any afﬁne hyperplane in Rn can be written with a normal vector a and a scalar shift b:

Ha,b = {x ∈ Rn : (cid:104)a, x(cid:105) − b = 0}, where a ∈ Rn \ {0}, and b ∈ R.

(18)

As in [19, Section 5], we note that (cid:104)a, x(cid:105) − b = sign((cid:104)a, x(cid:105) − b)(cid:107)a(cid:107)d(x, Ha,b). Using Eq. (17):

p(y = k|x) ∝ exp(sign((cid:104)ak, x(cid:105) − bk)(cid:107)ak(cid:107)d(x, Hak,bk )), bk ∈ R, x, ak ∈ Rn.

(19)

As it is not immediately obvious how to generalize the Euclidean hyperplane of Eq. (18) to other
spaces such as the Poincaré ball, we reformulate it as follows:

˜Ha,p = {x ∈ Rn : (cid:104)−p + x, a(cid:105) = 0} = p + {a}⊥, where p ∈ Rn, a ∈ Rn \ {0}.

(20)

This new deﬁnition relates to the previous one as ˜Ha,p = Ha,(cid:104)a,p(cid:105). Rewriting Eq. (19) with b = (cid:104)a, p(cid:105):
p(y = k|x) ∝ exp(sign((cid:104)−pk + x, ak(cid:105))(cid:107)ak(cid:107)d(x, ˜Hak,pk )), with pk, x, ak ∈ Rn.

(21)

It is now natural to adapt the previous deﬁnition to the hyperbolic setting by replacing + by ⊕c:
Deﬁnition 3.1 (Poincaré hyperplanes). For p ∈ Dn
p(z, a) = 0} = {z ∈ TpDn
gc

c : (cid:104)z, a(cid:105) = 0}. Then, we deﬁne Poincaré hyperplanes as

c \ {0}, let {a}⊥ := {z ∈ TpDn
c :

c , a ∈ TpDn

˜H c

a,p := {x ∈ Dn

c : (cid:104)logc

p(x), a(cid:105)p = 0} = expc

p({a}⊥) = {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}.

(22)

The last equality is shown appendix C. ˜H c
all geodesics in Dn
hypergyroplanes, see [27, deﬁnition 5.8]. A 3D hyperplane example is depicted in Fig. 1.

a,p can also be described as the union of images of
c orthogonal to a and containing p. Notice that our deﬁnition matches that of

Next, we need the following theorem, proved in appendix D:
Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(23)

Final formula for MLR in the Poincaré ball. Putting together Eq. (21) and Thm. 5, we get the
hyperbolic MLR formulation. Given K classes and k ∈ {1, . . . , K}, pk ∈ Dn
c \ {0}:

c , ak ∈ Tpk

Dn

p(y = k|x) ∝ exp(sign((cid:104)−pk ⊕c x, ak(cid:105))

gc
pk

(ak, ak)dc(x, ˜H c

)),

ak,pk

∀x ∈ Dn
c ,

(24)

or, equivalently

p(y = k|x) ∝ exp

(cid:18) λc
pk

(cid:107)ak(cid:107)
√
c

sinh−1

(cid:18)

√
2

c(cid:104)−pk ⊕c x, ak(cid:105)

(cid:19)(cid:19)

(1 − c(cid:107) − pk ⊕c x(cid:107)2)(cid:107)ak(cid:107)

,

∀x ∈ Dn
c .

(25)

this goes to p(y = k|x) ∝ exp(4(cid:104)−pk + x, ak(cid:105)) =

Notice that when c goes to zero,
exp((λ0
pk

)2(cid:104)−pk + x, ak(cid:105)) = exp((cid:104)−pk + x, ak(cid:105)0), recovering the usual Euclidean softmax.
However, at this point it is unclear how to perform optimization over ak, since it lives in Tpk
hence depends on pk. The solution is that one should write ak = P c
)a(cid:48)
k ∈ T0Dn
a(cid:48)

c = Rn, and optimize a(cid:48)

k as a Euclidean parameter.

k) = (λc

0/λc
pk

0→pk

(a(cid:48)

Dn
c and
k, where

1
√
c

(cid:113)

6

3.2 Hyperbolic feed-forward layers

In order to deﬁne hyperbolic neural networks, it is crucial to de-
ﬁne a canonically simple parametric family of transformations,
playing the role of linear mappings in usual Euclidean neural
networks, and to know how to apply pointwise non-linearities.
Inspiring ourselves from our reformulation of Möbius scalar
multiplication in Eq. (14), we deﬁne:
Deﬁnition 3.2 (Möbius version). For f : Rn → Rm, we deﬁne
the Möbius version of f as the map from Dn

c to Dm

c by:

f ⊗c(x) := expc

0(f (logc

0(x))),

(26)

where expc

0 : T0m

Dm

c → Dm

c and logc

0 : Dn

c → T0n

Dn
c .

Figure 1: An example of a hyper-
bolic hyperplane in D3
1 plotted us-
ing sampling. The red point is p.
The shown normal axis to the hy-
perplane through p is parallel to a.

Note that similarly as for other Möbius operations, we recover
the Euclidean mapping in the limit c → 0 if f is continuous, as limc→0 f ⊗c(x) = f (x). This
deﬁnition satisﬁes a few desirable properties too, such as: (f ◦ g)⊗c = f ⊗c ◦ g⊗c for f : Rm → Rl
and g : Rn → Rm (morphism property), and f ⊗c(x)/(cid:107)f ⊗c(x)(cid:107) = f (x)/(cid:107)f (x)(cid:107) for f (x) (cid:54)= 0
(direction preserving). It is then straight-forward to prove the following result:
Lemma 6 (Möbius matrix-vector multiplication). If M : Rn → Rm is a linear map, which we
identify with its matrix representation, then ∀x ∈ Dn

c , if M x (cid:54)= 0 we have

M ⊗c(x) = (1/

c) tanh

√

(cid:18) (cid:107)M x(cid:107)
(cid:107)x(cid:107)

√

tanh−1(

c(cid:107)x(cid:107))

(cid:19) M x
(cid:107)M x(cid:107)

,

(27)

and M ⊗c(x) = 0 if M x = 0. Moreover, if we deﬁne the Möbius matrix-vector multiplication of
M ∈ Mm,n(R) and x ∈ Dn
c by M ⊗c x := M ⊗c(x), then we have (M M (cid:48)) ⊗c x = M ⊗c (M (cid:48) ⊗c x)
for M ∈ Ml,m(R) and M (cid:48) ∈ Mm,n(R) (matrix associativity), (rM ) ⊗c x = r ⊗c (M ⊗c x) for
r ∈ R and M ∈ Mm,n(R) (scalar-matrix associativity) and M ⊗c x = M x for all M ∈ On(R)
(rotations are preserved).

Pointwise non-linearity.
ϕ⊗c can be applied to elements of the Poincaré ball.

If ϕ : Rn → Rn is a pointwise non-linearity, then its Möbius version

Bias translation. The generalization of a translation in the Poincaré ball is naturally given by
moving along geodesics. But should we use the Möbius sum x ⊕c b with a hyperbolic bias b or the
x(b(cid:48)) with a Euclidean bias b(cid:48)? These views are uniﬁed with parallel transport
exponential map expc
c by a bias b ∈ Dn
(see Thm 4). Möbius translation of a point x ∈ Dn
(cid:18) λc
0
λc
x

c is given by
(cid:19)

x ← x ⊕c b = expc

0(b))) = expc
x

0→x(logc

x(P c

logc

0(b)

(28)

.

We recover Euclidean translations in the limit c → 0. Note that bias translations play a particular
Indeed, consider multiple layers of the form fk(x) = ϕk(Mkx), each of
role in this model.
which having Möbius version f ⊗c
k (Mk ⊗c x). Then their composition can be re-written
f ⊗c
k ◦ · · · ◦ f ⊗c
1 = expc
0. This means that these operations can essentially be
performed in Euclidean space. Therefore, it is the interposition between those with the bias translation
of Eq. (28) which differentiates this model from its Euclidean counterpart.

k (x) = ϕ⊗c
0 ◦fk ◦ · · · ◦ f1 ◦ logc

If a vector x ∈ Rn+p is the (vertical) concatenation
Concatenation of multiple input vectors.
of two vectors x1 ∈ Rn, x2 ∈ Rp, and M ∈ Mm,n+p(R) can be written as the (horizontal)
concatenation of two matrices M1 ∈ Mm,n(R) and M2 ∈ Mm,p(R), then M x = M1x1 + M2x2.
We generalize this to hyperbolic spaces: if we are given x1 ∈ Dn
c ×Dp
c ,
and M, M1, M2 as before, then we deﬁne M ⊗c x := M1 ⊗c x1 ⊕c M2 ⊗c x2. Note that when c goes
to zero, we recover the Euclidean formulation, as limc→0 M ⊗c x = limc→0 M1 ⊗c x1 ⊕c M2 ⊗c x2 =
M1x1 + M2x2 = M x. Moreover, hyperbolic vectors x ∈ Dn
c can also be "concatenated" with real
features y ∈ R by doing: M ⊗c x ⊕c y ⊗c b with learnable b ∈ Dm

c , x = (x1 x2)T ∈ Dn

c and M ∈ Mm,n(R).

c , x2 ∈ Dp

7

3.3 Hyperbolic RNN

Naive RNN. A simple RNN can be deﬁned by ht+1 = ϕ(W ht + U xt + b) where ϕ is a pointwise
non-linearity, typically tanh, sigmoid, ReLU, etc. This formula can be naturally generalized to the
hyperbolic space as follows. For parameters W ∈ Mm,n(R), U ∈ Mm,d(R), b ∈ Dm
c , we deﬁne:

ht+1 = ϕ⊗c (W ⊗c ht ⊕c U ⊗c xt ⊕c b),

ht ∈ Dn

c , xt ∈ Dd
c .

(29)

Note that if inputs xt’s are Euclidean, one can write ˜xt := expc
expc

(U xt)) = W ⊗c ht ⊕c expc

(P c

W ⊗cht

0→W ⊗cht

0(U xt) = W ⊗c ht ⊕c U ⊗c ˜xt.

0(xt) and use the above formula, since

GRU architecture. One can also adapt the GRU architecture:
rt = σ(W rht−1 + U rxt + br),
zt = σ(W zht−1 + U zxt + bz),
˜ht = ϕ(W (rt (cid:12) ht−1) + U xt + b), ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht,

(30)

where (cid:12) denotes pointwise product. First, how should we adapt the pointwise multiplication by a
scaling gate? Note that the deﬁnition of the Möbius version (see Eq. (26)) can be naturally extended
to maps f : Rn × Rp → Rm as f ⊗c : (h, h(cid:48)) ∈ Dn
0(h(cid:48)))). In
c (cid:55)→ expc
0(h(cid:48))) =
particular, choosing f (h, h(cid:48)) := σ(h) (cid:12) h(cid:48) yields6 f ⊗c(h, h(cid:48)) = expc
diag(σ(logc

0(h))) ⊗c h(cid:48). Hence we adapt rt (cid:12) ht−1 to diag(rt) ⊗c ht−1 and the reset gate rt to:
0(W r ⊗c ht−1 ⊕c U r ⊗c xt ⊕c br),

0(h), logc
0(h)) (cid:12) logc

0(f (logc
0(σ(logc

rt = σ logc

c × Dp

(31)

and similarly for the update gate zt. Note that as the argument of σ in the above is unbounded, rt and
zt can a priori take values onto the full range (0, 1). Now the intermediate hidden state becomes:
˜ht = ϕ⊗c ((W diag(rt)) ⊗c ht−1 ⊕c U ⊗c xt ⊕ b),

(32)

where Möbius matrix associativity simpliﬁes W ⊗c (diag(rt) ⊗c ht−1) into (W diag(rt)) ⊗c ht−1.
Finally, we propose to adapt the update-gate equation as

ht = ht−1 ⊕c diag(zt) ⊗c (−ht−1 ⊕c

˜ht).

(33)

Note that when c goes to zero, one recovers the usual GRU. Moreover, if zt = 0 or zt = 1, then ht
becomes ht−1 or ˜ht respectively, similarly as in the usual GRU. This adaptation was obtained by
adapting [24]: in this work, the authors re-derive the update-gate mechanism from a ﬁrst principle
called time-warping invariance. We adapted their derivation to the hyperbolic setting by using the
notion of gyroderivative [4] and proving a gyro-chain-rule (see appendix E).

4 Experiments

SNLI task and dataset. We evaluate our method on two tasks. The ﬁrst is natural language
inference, or textual entailment. Given two sentences, a premise (e.g. "Little kids A. and B. are
playing soccer.") and a hypothesis (e.g. "Two children are playing outdoors."), the binary classiﬁcation
task is to predict whether the second sentence can be inferred from the ﬁrst one. This deﬁnes a partial
order in the sentence space. We test hyperbolic networks on the biggest real dataset for this task,
SNLI [7]. It consists of 570K training, 10K validation and 10K test sentence pairs. Following [28],
we merge the "contradiction" and "neutral" classes into a single class of negative sentence pairs, while
the "entailment" class gives the positive pairs.

PREFIX task and datasets. We conjecture that the improvements of hyperbolic neural networks
are more signiﬁcant when the underlying data structure is closer to a tree. To test this, we design a
proof-of-concept task of detection of noisy preﬁxes, i.e. given two sentences, one has to decide if the
second sentence is a noisy preﬁx of the ﬁrst, or a random sentence. We thus build synthetic datasets
PREFIX-Z% (for Z being 10, 30 or 50) as follows: for each random ﬁrst sentence of random length
at most 20 and one random preﬁx of it, a second positive sentence is generated by randomly replacing
Z% of the words of the preﬁx, and a second negative sentence of same length is randomly generated.
Word vocabulary size is 100, and we generate 500K training, 10K validation and 10K test pairs.

6If x has n coordinates, then diag(x) denotes the diagonal matrix of size n with xi’s on its diagonal.

8

Models architecture. Our neural network layers can be used in a plug-n-play manner exactly like
standard Euclidean layers. They can also be combined with Euclidean layers. However, optimization
w.r.t. hyperbolic parameters is different (see below) and based on Riemannian gradients which
are just rescaled Euclidean gradients when working in the conformal Poincaré model [21]. Thus,
back-propagation can be applied in the standard way.

In our setting, we embed the two sentences using two distinct hyperbolic RNNs or GRUs. The
sentence embeddings are then fed together with their squared distance (hyperbolic or Euclidean,
depending on their geometry) to a FFNN (Euclidean or hyperbolic, see Sec. 3.2) which is further
fed to an MLR (Euclidean or hyperbolic, see Sec. 3.1) that gives probabilities of the two classes
(entailment vs neutral). We use cross-entropy loss on top. Note that hyperbolic and Euclidean layers
can be mixed, e.g. the full network can be hyperbolic and only the last layer be Euclidean, in which
case one has to use log0 and exp0 functions to move between the two manifolds in a correct manner
as explained for Eq. 26.

Optimization. Our models have both Euclidean (e.g. weight matrices in both Euclidean and
hyperbolic FFNNs, RNNs or GRUs) and hyperbolic parameters (e.g. word embeddings or biases for
the hyperbolic layers). We optimize the Euclidean parameters with Adam [16] (learning rate 0.001).
Hyperbolic parameters cannot be updated with an equivalent method that keeps track of gradient
history due to the absence of a Riemannian Adam. Thus, they are optimized using full Riemannian
stochastic gradient descent (RSGD) [5, 11]. We also experiment with projected RSGD [21], but
optimization was sometimes less stable. We use a different constant learning rate for word embeddings
(0.1) and other hyperbolic weights (0.01) because words are updated less frequently.

Numerical errors. Gradients of the basic operations deﬁned above (e.g. ⊕c, exponential map) are
c(cid:107)x(cid:107) = 1. Thus, we
not deﬁned when the hyperbolic argument vectors are on the ball border, i.e.
always project results of these operations in the ball of radius 1 − (cid:15), where (cid:15) = 10−5. Numerical
errors also appear when hyperbolic vectors get closer to 0, thus we perturb them with an (cid:15)(cid:48) = 10−15
before they are used in any of the above operations. Finally, arguments of the tanh function are
clipped between ±15 to avoid numerical errors, while arguments of tanh−1 are clipped to at most
1 − 10−5.

√

Hyperparameters. For all methods, baselines and datasets, we use c = 1, word and hidden state
embedding dimension of 5 (we focus on the low dimensional setting that was shown to already
be effective [21]), batch size of 64. We ran all methods for a ﬁxed number of 30 epochs. For all
models, we experiment with both identity (no non-linearity) or tanh non-linearity in the RNN/GRU
cell, as well as identity or ReLU after the FFNN layer and before MLR. As expected, for the fully
Euclidean models, tanh and ReLU respectively surpassed the identity variant by a large margin. We
only report the best Euclidean results. Interestingly, for the hyperbolic models, using only identity for
both non-linearities works slightly better and this is likely due to two facts: i) our hyperbolic layers
already contain non-linearities by their nature, ii) tanh is limiting the output domain of the sentence
embeddings, but the hyperbolic speciﬁc geometry is more pronounced at the ball border, i.e. at the
hyperbolic "inﬁnity", compared to the center of the ball.

For the results shown in Tab. 1, we run each model (baseline or ours) exactly 3 times and report the
test result corresponding to the best validation result from these 3 runs. We do this because the highly
non-convex spectrum of hyperbolic neural networks sometimes results in convergence to poor local
minima, suggesting that initialization is very important.

Results. Results are shown in Tab. 1. Note that the fully Euclidean baseline models might have
an advantage over hyperbolic baselines because more sophisticated optimization algorithms such
as Adam do not have a hyperbolic analogue at the moment. We ﬁrst observe that all GRU models
overpass their RNN variants. Hyperbolic RNNs and GRUs have the most signiﬁcant improvement
over their Euclidean variants when the underlying data structure is more tree-like, e.g. for PREFIX-
10% − for which the tree relation between sentences and their preﬁxes is more prominent − we
reduce the error by a factor of 3.35 for hyperbolic vs Euclidean RNN, and by a factor of 1.5 for
hyperbolic vs Euclidean GRU. As soon as the underlying structure diverges more and more from
a tree, the accuracy gap decreases − for example, for PREFIX-50% the noise heavily affects the
representational power of hyperbolic networks. Also, note that on SNLI our methods perform
similarly as with their Euclidean variants. Moreover, hyperbolic and Euclidean MLR are on par when

9

SNLI

PREFIX-10% PREFIX-30% PREFIX-50%

FULLY EUCLIDEAN RNN
HYPERBOLIC RNN+FFNN, EUCL MLR
FULLY HYPERBOLIC RNN
FULLY EUCLIDEAN GRU
HYPERBOLIC GRU+FFNN, EUCL MLR
FULLY HYPERBOLIC GRU

79.34 %
79.18 %
78.21 %
81.52 %
79.76 %
81.19 %

89.62 %
96.36 %
96.91 %
95.96 %
97.36 %
97.14 %

81.71 %
87.83 %
87.25 %
86.47 %
88.47 %
88.26 %

72.10 %
76.50 %
62.94 %
75.04 %
76.87 %
76.44 %

Table 1: Test accuracies for various models and four datasets. "Eucl" denotes Euclidean. All word
and sentence embeddings have dimension 5. We highlight in bold the best baseline (or baselines, if
the difference is less than 0.5%).

used in conjunction with hyperbolic sentence embeddings, suggesting further empirical investigation
is needed for this direction (see below).

We also observe that, in the hyperbolic setting, accuracy tends to increase when sentence embeddings
start increasing, and gets better as their norms converge towards 1 (the ball border for c = 1). Unlike
in the Euclidean case, this behavior does happen only after a few epochs and suggests that the model
should ﬁrst adjust the angular layout in order to disentangle the representations, before increasing their
norms to fully exploit the strong clustering property of the hyperbolic geometry. Similar behavior
was observed in the context of embedding trees by [21]. Details in appendix F.

MLR classiﬁcation experiments.
For the sentence entailment classi-
ﬁcation task we do not see a clear
advantage of hyperbolic MLR com-
pared to its Euclidean variant. A pos-
sible reason is that, when trained end-
to-end, the model might decide to
place positive and negative embed-
dings in a manner that is already well
separated with a classic MLR. As a
consequence, we further investigate
MLR for the task of subtree classiﬁ-
cation. Using an open source imple-
mentation7 of [21], we pre-trained
Poincaré embeddings of the Word-
Net noun hierarchy (82,115 nodes).
We then choose one node in this tree
(see Table 2) and classify all other
nodes (solely based on their embed-
dings) as being part of the subtree
rooted at this node. All nodes in such a subtree are divided into positive training nodes (80%) and
positive test nodes (20%). The same splitting procedure is applied for the remaining WordNet nodes
that are divided into a negative training and negative test set respectively. Three variants of MLR
are then trained on top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation
task: hyperbolic MLR, Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean
MLR applied after mapping all embeddings in the tangent space at 0 using the log0 map. We use
different embedding dimensions : 2, 3, 5 and 10. For the hyperbolic MLR, we use full Riemannian
SGD with a learning rate of 0.001. For the two Euclidean models we use ADAM optimizer and the
same learning rate. During training, we always sample the same number of negative and positive
nodes in each minibatch of size 16; thus positive nodes are frequently resampled. All methods are
trained for 30 epochs and the ﬁnal F1 score is reported (no hyperparameters to validate are used, thus
we do not require a validation set). This procedure is repeated for four subtrees of different sizes.

Figure 2: Hyperbolic (left) vs Direct Euclidean (right) binary
MLR used to classify nodes as being part in the GROUP.N.01
subtree of the WordNet noun hierarchy solely based on their
Poincaré embeddings. The positive points (from the subtree)
are in blue, the negative points (the rest) are in red and the
trained positive separation hyperplane is depicted in green.

Quantitative results are presented in Table 2. We can see that the hyperbolic MLR overpasses
its Euclidean variants in almost all settings, sometimes by a large margin. Moreover, to provide

7https://github.com/dalab/hyperbolic_cones

10

WORDNET
SUBTREE

ANIMAL.N.01
3218 / 798

GROUP.N.01
6649 / 1727

WORKER.N.01
861 / 254

MAMMAL.N.01
953 / 228

MODEL

D = 2

D = 3

D = 5

D = 10

HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL
HYPERBOLIC
DIRECT EUCL
log0 + EUCL

47.43 ± 1.07%
41.69 ± 0.19%
38.89 ± 0.01%
81.72 ± 0.17%
61.13 ± 0.42%
60.75 ± 0.24%
12.68 ± 0.82%
10.86 ± 0.01%
9.04 ± 0.06%
32.01 ± 17.14%
15.58 ± 0.04%
13.10 ± 0.13%

91.92 ± 0.61%
68.43 ± 3.90%
62.57 ± 0.61%
89.87 ± 2.73%
63.56 ± 1.22%
61.98 ± 0.57%
24.09 ± 1.49%
22.39 ± 0.04%
22.57 ± 0.20%
87.54 ± 4.55%
44.68 ± 1.87%
44.89 ± 1.18%

98.07 ± 0.55%
95.59 ± 1.18%
89.21 ± 1.34%
87.89 ± 0.80%
67.82 ± 0.81%
67.92 ± 0.74%
55.46 ± 5.49%
35.23 ± 3.16%
26.47 ± 0.78%
88.73 ± 3.22%
59.35 ± 1.31%
52.51 ± 0.85%

99.26 ± 0.59%
99.36 ± 0.18%
98.27 ± 0.70%
91.91 ± 3.07%
91.38 ± 1.19%
91.41 ± 0.18%
66.83 ± 11.38%
47.29 ± 3.93%
36.66 ± 2.74%
91.37 ± 6.09%
77.76 ± 5.08%
56.11 ± 2.21%

Table 2: Test F1 classiﬁcation scores for four different subtrees of WordNet noun tree. All nodes
in such a subtree are divided into positive training nodes (80%) and positive test nodes (20%);
these counts are shown below each subtree root. The same splitting procedure is applied for the
remaining nodes to obtain negative training and test sets. Three variants of MLR are then trained on
top of pre-trained Poincaré embeddings [21] to solve this binary classiﬁcation task: hyperbolic MLR,
Euclidean MLR applied directly on the hyperbolic embeddings and Euclidean MLR applied after
mapping all embeddings in the tangent space at 0 using the log0 map. 95% conﬁdence intervals for 3
different runs are shown for each method and each different embedding dimension (2, 3, 5 or 10).

further understanding, we plot the 2-dimensional embeddings and the trained separation hyperplanes
(geodesics in this case) in Figure 2. We can see that respecting the hyperbolic geometry is very
important for a quality classiﬁcation model.

5 Conclusion

We showed how classic Euclidean deep learning tools such as MLR, FFNNs, RNNs or GRUs can be
generalized in a principled manner to all spaces of constant negative curvature combining Riemannian
geometry with the elegant theory of gyrovector spaces. Empirically we found that our models
outperform or are on par with corresponding Euclidean architectures on sequential data with implicit
hierarchical structure. We hope to trigger exciting future research related to better understanding
of the hyperbolic non-convexity spectrum and development of other non-Euclidean deep learning
methods.
Our data and Tensorﬂow [1] code are publicly available8.

Acknowledgements

We thank Igor Petrovski for useful pointers regarding the implementation.

This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement
number 167176. Gary Bécigneul is also funded by the Max Planck ETH Center for Learning
Systems.

References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for
large-scale machine learning. 2016.

[2] Ungar Abraham Albert. Analytic hyperbolic geometry and Albert Einstein’s special theory of

relativity. World scientiﬁc, 2008.

8https://github.com/dalab/hyperbolic_nn

11

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of the International Conference on Learning
Representations (ICLR), 2015.

[4] Graciela S Birman and Abraham A Ungar. The hyperbolic derivative in the poincaré ball model
of hyperbolic geometry. Journal of mathematical analysis and applications, 254(1):321–333,
2001.

[5] S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on

Automatic Control, 58(9):2217–2229, Sept 2013.

[6] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems (NIPS), pages 2787–2795, 2013.

[7] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 632–642. Association
for Computational Linguistics, 2015.

[8] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,
34(4):18–42, 2017.

[9] James W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry.

Flavors of geometry, 31:59–115, 1997.

[10] Christopher De Sa, Albert Gu, Christopher Ré, and Frederic Sala. Representation tradeoffs for

hyperbolic embeddings. arXiv preprint arXiv:1804.03329, 2018.

[11] Octavian-Eugen Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In Proceedings of the thirty-ﬁfth international conference
on machine learning (ICML), 2018.

[12] Mikhael Gromov. Hyperbolic groups. In Essays in group theory, pages 75–263. Springer, 1987.

[13] Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the

Cambridge Philosophical Society, page 1–17, 2017.

[14] Christopher Hopper and Ben Andrews. The Ricci ﬂow in Riemannian geometry. Springer, 2010.

[15] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1746–1751. Association for Computational Linguistics, 2014.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations (ICLR), 2015.

[17] Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marián Boguná.

Hyperbolic geometry of complex networks. Physical Review E, 82(3):036106, 2010.

[18] John Lamping, Ramana Rao, and Peter Pirolli. A focus+ context technique based on hyperbolic
geometry for visualizing large hierarchies. In Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 401–408. ACM Press/Addison-Wesley Publishing Co.,
1995.

[19] Guy Lebanon and John Lafferty. Hyperplane margin classiﬁers on the multinomial manifold. In
Proceedings of the international conference on machine learning (ICML), page 66. ACM, 2004.

[20] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective
learning on multi-relational data. In Proceedings of the international conference on machine
learning (ICML), volume 11, pages 809–816, 2011.

12

[21] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical repre-
sentations. In Advances in Neural Information Processing Systems (NIPS), pages 6341–6350,
2017.

[22] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blun-
som. Reasoning about entailment with neural attention. In Proceedings of the International
Conference on Learning Representations (ICLR), 2015.

[23] Michael Spivak. A comprehensive introduction to differential geometry. Publish or perish, 1979.

[24] Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In Proceedings of

the International Conference on Learning Representations (ICLR), 2018.

[25] Abraham A Ungar. Hyperbolic trigonometry and its application in the poincaré ball model of

hyperbolic geometry. Computers & Mathematics with Applications, 41(1-2):135–147, 2001.

[26] Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis

Lectures on Mathematics and Statistics, 1(1):1–194, 2008.

[27] Abraham Albert Ungar. Analytic hyperbolic geometry in n dimensions: An introduction. CRC

Press, 2014.

[28] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. In Proceedings of the International Conference on Learning Representations (ICLR),
2016.

[29] J Vermeer. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic

plane. Topology and its Applications, 152(3):226–242, 2005.

13

A Hyperbolic Trigonometry

Hyperbolic angles. For A, B, C ∈ Dn
c , we denote by ∠A := ∠BAC the angle between the two
geodesics starting from A and ending at B and C respectively. This angle can be deﬁned in two
equivalent ways: i) either using the angle between the initial velocities of the two geodesics as given
by Eq. 5, or ii) using the formula

cos(∠A) =

(cid:28) (−A) ⊕c B
(cid:107)(−A) ⊕c B(cid:107)

,

(−A) ⊕c C
(cid:107)(−A) ⊕c C(cid:107)

(cid:29)

,

In this case, ∠A is also called a gyroangle in the work of [26, section 4].

Hyperbolic law of sines. We state here the hyperbolic law of sines. If for A, B, C ∈ Dn
c , we
denote by ∠B := ∠ABC the angle between the two geodesics starting from B and ending at A and
C respectively, and by ˜c = dc(B, A) the length of the hyperbolic segment BA (and similarly for
others), then we have:

sin(∠A)
√
c˜a)
sinh(

=

sin(∠B)
√
c˜b)
sinh(

=

sin(∠C)
√
c˜c)
sinh(

.

Note that one can also adapt the hyperbolic law of cosines to the hyperbolic space.

B Proof of Theorem 4

Theorem 4.
In the manifold (Dn
to another tangent space TxDn

c , gc), the parallel transport w.r.t. the Levi-Civita connection of a vector v ∈ T0Dn
c

c is given by the following isometry:
λc
0
λc
x

0→x(v) = logc
P c

x(x ⊕c expc

0(v)) =

v.

Proof. The geodesic in Dn
v ∈ T0Dn
γ (i.e. X(t) ∈ Tγ(t)Dn

c from 0 to x is given in Eq. (10) by γ(t) = x ⊗c t, for t ∈ [0, 1]. Let
c . Then it is of common knowledge that there exists a unique parallel9 vector ﬁeld X along

c , ∀t ∈ [0, 1]) such that X(0) = v. Let’s deﬁne:
X : t ∈ [0, 1] (cid:55)→ logc

γ(t)(γ(t) ⊕c expc

0(v)) ∈ Tγ(t)Dn
c .

Clearly, X is a vector ﬁeld along γ such that X(0) = v. Now deﬁne
0→x : v ∈ T0Dn
P c

x(x ⊕c expc

0(v)) ∈ TxDn
c .

c (cid:55)→ logc
0→x(v) = λc

c . Since P c

0→x is a linear isometry from T0Dn
v, hence P c
c
0→x(v) = X(1), it is enough to prove that X is parallel in order to guarantee that

From Eq. (12), it is easily seen that P c
to TxDn
c to TxDn
0→x is the parallel transport from T0Dn
P c
c .
Since X is a vector ﬁeld along γ, its covariant derivative can be expressed with the Levi-Civita
connection ∇c associated to gc:

0
λc
x

DX
∂t

= ∇c

˙γ(t)X.

Let’s compute the Levi-Civita connection from its Christoffel symbols. In a local coordinate system,
they can be written as

Γi

jk =

(gc)il(∂jgc

lk + ∂kgc

lj − ∂lgc

jk),

1
2

where superscripts denote the inverse metric tensor and using Einstein’s notations. As gc
at γ(t) ∈ Dn

c this yields:

ij = (λc)2δij,

jk = cλc
Γi

γ(t)(δikγ(t)j + δijγ(t)k − δjkγ(t)i).

9i.e. that DX

∂t = 0 for t ∈ [0, 1], where D

∂t denotes the covariant derivative.

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

14

On the other hand, since X(t) = (λc

∇c

˙γ(t)X = ˙γ(t)i∇c

i X = ˙γ(t)i∇c
i

= vj ˙γ(t)i∇c
i

0/λc

γ(t))v, we have
(cid:32)

(cid:33)

λc
0
λc

γ(t)

v

(cid:32)

λc
0
λc

γ(t)

(cid:33)

ej

.

√

√

Since γ(t) = (1/
Hence there exists K x

c) tanh(t tanh−1(
t ∈ R such that ˙γ(t) = K x

c(cid:107)x(cid:107))) x

(cid:107)x(cid:107) , it is easily seen that ˙γ(t) is colinear to γ(t).
t γ(t). Moreover, we have the following Leibniz rule:
(cid:33)

(cid:32)

(cid:32)

∇c
i

λc
0
λc

γ(t)

(cid:33)

ej

=

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

ej.

Combining these yields

DX
∂t

= K x

t vjγ(t)i

(cid:32)

λc
0
λc

γ(t)

∇c

i ej +

∂
∂γ(t)i

λc
0
λc

γ(t)

(cid:32)

(cid:33)

(cid:33)

ej

.

Replacing with the Christoffel symbols of ∇c at γ(t) gives

Moreover,

λc
0
λc

γ(t)

λc
0
λc

γ(t)

∇c

i ej =

ijek = 2c[δk
Γk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek.

∂
∂γ(t)i

(cid:32)

(cid:33)

λc
0
λc

γ(t)

ej =

∂
∂γ(t)i

(cid:0)−c(cid:107)γ(t)(cid:107)2(cid:1) ej = −2cγ(t)iej.

Putting together everything, we obtain

DX
∂t

= K x

t vjγ(t)i (cid:0)2c[δk

j γ(t)i + δk

i γ(t)j − δijγ(t)k]ek − 2cγ(t)iej

(cid:1)

t vjγ(t)i (cid:0)γ(t)jei − δijγ(t)kek
t vj (cid:0)γ(t)jγ(t)iei − γ(t)iδijγ(t)kek
(cid:1)
t vj (cid:0)γ(t)jγ(t)iei − γ(t)jγ(t)kek

(cid:1)

(cid:1)

= 2cK x
= 2cK x
= 2cK x
= 0,

which concludes the proof.

C Proof of Eq. (22)

Proof. Two steps proof:
i) expc

p({a}⊥) ⊆ {x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0}:

Let z ∈ {a}⊥. From Eq. (12), we have that:

This, together with the left-cancellation law in gyrospaces (see section 2.3), implies that

expc

p(z) = −p ⊕c βz,

for some β ∈ R.

(cid:104)−p ⊕c expc

p(z), a(cid:105) = (cid:104)βz, a(cid:105) = 0

which is what we wanted.

ii) {x ∈ Dn
Let x ∈ Dn

c : (cid:104)−p ⊕c x, a(cid:105) = 0} ⊆ expc
c s.t. (cid:104)−p ⊕c x, a(cid:105) = 0. Then, using Eq. (12), we derive that:
for some β ∈ R,

p(x) = β(−p ⊕c x),

p({a}⊥):

logc

which is orthogonal to a, by assumption. This implies logc

p(x) ∈ {a}⊥, hence x ∈ expc

p({a}⊥).

15

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(54)

D Proof of Theorem 5

Theorem 5.

dc(x, ˜H c

a,p) := inf
w∈ ˜H c

a,p

dc(x, w) =

sinh−1

1
√
c

√

(cid:18) 2

c|(cid:104)−p ⊕c x, a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

(cid:19)

.

(55)

Proof. We ﬁrst need to prove the following lemma, trivial in the Euclidean space, but not in the
Poincaré ball:
Lemma 7. (Orthogonal projection on a geodesic) Any point in the Poincaré ball has a unique
orthogonal projection on any given geodesic that does not pass through the point. Formally, for all
y ∈ Dn
c and for all geodesics γx→z(·) s.t. y /∈ Im γx→z, there exists an unique w ∈ Im γx→z s.t.
∠(γw→y, γx→z) = π/2.

Proof. We ﬁrst note that any geodesic in Dn
and has two "points at inﬁnity" lying on the ball border (v (cid:54)= 0):

c has the form γ(t) = u ⊕c v ⊗c t as given by Eq. 11,

γ(±∞) = u ⊕c

√

±v
c(cid:107)v(cid:107)

∈ ∂Dn
c .

(56)

Using the notations in the lemma statement, the closed-form of γx→z is given by Eq. (10):

γx→z(t) = x ⊕c (−x ⊕c z) ⊗c t

We denote by x(cid:48), z(cid:48) ∈ ∂Dn
∠ywx(cid:48) is well deﬁned from Eq. (34):

c its points at inﬁnity as described by Eq. (56). Then, the hyperbolic angle

cos(∠(γw→y, γx→z)) = cos(∠ywz(cid:48)) =

(cid:104)−w ⊕c y, −w ⊕c z(cid:48)(cid:105)
(cid:107) − w ⊕c y(cid:107) · (cid:107) − w ⊕c z(cid:48)(cid:107)

.

(57)

We now perform 2 steps for this proof.

i) Existence of w:

The angle function from Eq. (57) is continuous w.r.t t when w = γx→z(t). So we ﬁrst prove existence
of an angle of π/2 by continuously moving w from x(cid:48) to z(cid:48) when t goes from −∞ to ∞, and
observing that cos(∠ywz(cid:48)) goes from −1 to 1 as follows:

cos(∠yx(cid:48)z(cid:48)) = 1 & lim
w→z(cid:48)

cos(∠ywz(cid:48)) = −1.

(58)

The left part of Eq. (58) follows from Eq. (57) and from the fact (easy to show from the deﬁnition
c (which is the case of x(cid:48)). The right part of Eq. (58)
of ⊕c) that a ⊕c b = a, when (cid:107)a(cid:107) = 1/
follows from the fact that ∠ywz(cid:48) = π − ∠ywx(cid:48) (from the conformal property, or from Eq. (34)) and
cos(∠yz(cid:48)x(cid:48)) = 1 (proved as above).
Hence cos(∠ywz(cid:48)) has to pass through 0 when going from −1 to 1, which achieves the proof of
existence.

√

ii) Uniqueness of w:
Assume by contradiction that there are two w and w(cid:48) on γx→z that form angles ∠ywx(cid:48) and ∠yw(cid:48)x(cid:48)
of π/2. Since w, w(cid:48), x(cid:48) are on the same geodesic, we have

π/2 = ∠yw(cid:48)x(cid:48) = ∠yw(cid:48)w = ∠ywx(cid:48) = ∠yw(cid:48)w
So ∆yww(cid:48) has two right angles, but in the Poincaré ball this is impossible.

(59)

Now, we need two more lemmas:
Lemma 8. (Minimizing distance from point to geodesic) The orthogonal projection of a point to
a geodesic (not passing through the point) is minimizing the distance between the point and the
geodesic.

Proof. The proof is similar with the Euclidean case and it’s based on hyperbolic sine law and the fact
that in any right hyperbolic triangle the hypotenuse is strictly longer than any of the other sides.

16

Lemma 9. (Geodesics through p) Let ˜H c
all points on the geodesic γp→w are included in ˜H c

a,p.

a,p be a Poincaré hyperplane. Then, for any w ∈ ˜H c

a,p \ {p},

Proof. γp→w(t) = p ⊕c (−p ⊕c w) ⊗c t. Then, it is easy to check the condition in Eq. (22):

(cid:104)−p ⊕c γp→w(t), a(cid:105) = (cid:104)(−p ⊕c w) ⊗c t, a(cid:105) ∝ (cid:104)−p ⊕c w, a(cid:105) = 0.

(60)

We now turn back to our proof. Let x ∈ Dn
We prove that there is at least one point w∗ ∈ ˜H c

c be an arbitrary point and ˜H c

a,p a Poincaré hyperplane.

a,p that achieves the inﬁmum distance

dc(x, w∗) = inf
w∈ ˜H c

a,p

dc(x, w),

and, moreover, that this distance is the same as the one in the theorem’s statement.
We ﬁrst note that for any point w ∈ ˜H c
and Lemma 9, it is obvious that the projection of x to γp→w will give a strictly lower distance.
Thus, we only consider w ∈ ˜H c
triangle ∆xwp, one gets:

a,p such that ∠xwp = π/2. Applying hyperbolic sine law in the right

a,p, if ∠xwp (cid:54)= π/2, then w (cid:54)= w∗. Indeed, using Lemma 8

dc(x, w) = (1/

c) sinh−1 (cid:0)sinh(

c dc(x, p)) · sin(∠xpw)(cid:1) .

√

√

One of the above quantities does not depend on w:

√

√

sinh(

c dc(x, p)) = sinh(2 tanh−1(

c(cid:107) − p ⊕c x(cid:107))) =

√
2
c(cid:107) − p ⊕c x(cid:107)
1 − c(cid:107) − p ⊕c x(cid:107)2 .

The other quantity is sin(∠xpw) which is minimized when the angle ∠xpw is minimized (be-
cause ∠xpw < π/2 for the hyperbolic right triangle ∆xwp), or, alternatively, when cos(∠xpw) is
maximized. But, we already have from Eq. (34) that:

cos(∠xpw) =

(cid:104)−p ⊕c x, −p ⊕c w(cid:105)
(cid:107) − p ⊕c x(cid:107) · (cid:107) − p ⊕c w(cid:107)

.

To maximize the above, the constraint on the right angle at w can be dropped because cos(∠xpw)
depends only on the geodesic γp→w and not on w itself, and because there is always an orthogonal
projection from any point x to any geodesic as stated by Lemma 7. Thus, it remains to ﬁnd the
maximum of Eq. (64) when w ∈ ˜H c
a,p from Eq. (22), one can easily
prove that

a,p. Using the deﬁnition of ˜H c

Using that fact that logc

p(w)/(cid:107) logc

p(w)(cid:107) = −p ⊕c w/(cid:107) − p ⊕c w(cid:107), we just have to ﬁnd

and we are left with a well known Euclidean problem which is equivalent to ﬁnding the minimum
angle between the vector −p ⊕c x (viewed as Euclidean) and the hyperplane {a}⊥. This angle
is given by the Euclidean orthogonal projection whose sin value is the distance from the vector’s
endpoint to the hyperplane divided by the vector’s length:

{logc

p(w) : w ∈ ˜H c

a,p} = {a}⊥.

max
z∈{a}⊥

(cid:18) (cid:104)−p ⊕c x, z(cid:105)

(cid:107) − p ⊕c x(cid:107) · (cid:107)z(cid:107)

(cid:19)

,

sin(∠xpw∗) =

|(cid:104)−p ⊕c x, a
(cid:107) − p ⊕c x(cid:107)

(cid:107)a(cid:107) (cid:105)|

.

17

It follows that a point w∗ ∈ ˜H c
Eqs. (61),(62),(63) and (67) concludes the proof.

a,p satisfying Eq. (67) exists (but might not be unique). Combining

(61)

(62)

(63)

(64)

(65)

(66)

(67)

(cid:3)

E Derivation of the Hyperbolic GRU Update-gate

In [24], the authors recover the update/forget-gate mechanism of a GRU/LSTM by requiring that the
class of neural networks given by the chosen architecture be invariant to time-warpings. The idea is
the following.

Recovering the update-gate from time-warping. A naive RNN is given by the equation

h(t + 1) = ϕ(W h(t) + U x(t) + b)

Let’s drop the bias b to simplify notations. If h is seen as a differentiable function of time, then a
ﬁrst-order Taylor development gives h(t + δt) ≈ h(t) + δt dh
dt (t) for small δt. Combining this for
δt = 1 with the naive RNN equation, one gets

dh
dt

dα
dt

(t) = ϕ(W h(t) + U x(t)) − h(t).

As this is written for any t, one can replace it by t ← α(t) where α is a (smooth) increasing function
of t called the time-warping. Denoting by ˜h(t) := h(α(t)) and ˜x(t) := x(α(t)), using the chain rule
d˜h
dt (t) = dα

dt (α(t)), one gets

dt (t) dh

d˜h
dt

dα
dt

(t) =

(t)ϕ(W ˜h(t) + U ˜x(t)) −

(t)˜h(t).

(70)

Removing the tildas to simplify notations, discretizing back with dh

dt (t) ≈ h(t + 1) − h(t) yields

h(t + 1) =

(t)ϕ(W h(t) + U x(t)) +

1 −

(t)

h(t).

(71)

dα
dt

(cid:18)

(cid:19)

dα
dt

Requiring that our class of neural networks be invariant to time-warpings means that this class should
contain RNNs deﬁned by Eq. (71), i.e. that dα
dt (t) can be learned. As this is a positive quantity, we
can parametrize it as z(t) = σ(W zh(t) + U zx(t)), recovering the forget-gate equation:

h(t + 1) = z(t)ϕ(W h(t) + U x(t)) + (1 − z(t))h(t).

Adapting this idea to hyperbolic RNNs. The gyroderivative [4] of a map h : R → Dn
as

c is deﬁned

dh
dt

(t) = lim
δt→0

1
δt

⊗c (−h(t) ⊕c h(t + δt)).

Using Möbius scalar associativity and the left-cancellation law leads us to

h(t + δt) ≈ h(t) ⊕c δt ⊗c

(t),

dh
dt

for small δt. Combining this with the equation of a simple hyperbolic RNN of Eq. (29) with δt = 1,
one gets

dh
dt

(t) = −h(t) ⊕c ϕ⊗c(W ⊗c h(t) ⊕c U ⊗c x(t)).

For the next step, we need the following lemma:
Lemma 10 (Gyro-chain-rule). For α : R → R differentiable and h : R → Dn
gyro-derivative, if ˜h := h ◦ α, then we have

c with a well-deﬁned

(68)

(69)

(72)

(73)

(74)

(75)

(76)

where dα

dt (t) denotes the usual derivative.

d˜h
dt

(t) =

(t) ⊗c

(α(t)),

dα
dt

dh
dt

18

(77)

(78)

(79)

(80)

(81)

Proof.

d˜h
dt

(t) = lim
δt→0

1
δt
1
δt

⊗c [−˜h(t) ⊕c

˜h(t + δt)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

= lim
δt→0

= lim
δt→0

= lim
δt→0

= lim
u→0
dα
dt

=

α(cid:48)(t) + O(δt)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
δt(α(cid:48)(t) + O(δt))
α(cid:48)(t)
u

(t) ⊗c

(α(t))

dh
dt

⊗c [−h(α(t)) ⊕c h(α(t) + u)]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

⊗c [−h(α(t)) ⊕c h(α(t) + δt(α(cid:48)(t) + O(δt)))]

(Möbius scalar associativity) (82)

where we set u = δt(α(cid:48)(t) + O(δt)), with u → 0 when δt → 0, which concludes.

Using lemma 10 and Eq. (75), with similar notations as in Eq. (70) we have

d˜h
dt

dα
dt

(t) =

(t) ⊗c (−˜h(t) ⊕c ϕ⊗c(W ⊗c

˜h(t) ⊕c U ⊗c ˜x(t))).

(83)

Finally, discretizing back with Eq. (74), using the left-cancellation law and dropping the tildas yields

h(t + 1) = h(t) ⊕c

(t) ⊗c (−h(t) ⊕c ϕ⊗c (W ⊗c h(t) ⊕c U ⊗c x(t))).

(84)

dα
dt

Since α is a time-warping, by deﬁnition its derivative is positive and one can choose to parametrize
it with an update-gate zt (a scalar) deﬁned with a sigmoid. Generalizing this scalar scaling by the
Möbius version of the pointwise scaling (cid:12) yields the Möbius matrix scaling diag(zt) ⊗c ·, leading to
our proposed Eq. (33) for the hyperbolic GRU.

F More Experimental Investigations

The following empirical facts were observed for both hyperbolic RNNs and GRUs.

We observed that, in the hyperbolic setting, accuracy is often much higher when sentence embeddings
can go close to the border (hyperbolic "inﬁnity"), hence exploiting the hyperbolic nature of the space.
Moreover, the faster the two sentence norms go to 1, the more it’s likely that a good local minima
was reached. See ﬁgures 3 and 5.

We often observe that test accuracy starts increasing exactly when sentence embedding norms do.
However, in the hyperbolic setting, the sentence embeddings norms remain close to 0 for a few
epochs, which does not happen in the Euclidean case. See ﬁgures 3, 5 and 4. This mysterious fact
was also exhibited in a similar way by [21] which suggests that the model ﬁrst has to adjust the
angular layout in the almost Euclidean vicinity of 0 before increasing norms and fully exploiting
hyperbolic geometry.

19

(a) Test accuracy

(a) Test accuracy

20

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 3: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the same
architecture: hyperbolic GRU followed by hyperbolic FFNN and hyperbolic/Euclidean (half-half)
MLR. The X axis shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 4: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: Euclidean GRU followed by Euclidean FFNN and Euclidean MLR. The X axis
shows millions of training examples processed.

(b) Norm of the ﬁrst sentence. Averaged over all sentences in the test set.

Figure 5: PREFIX-30% accuracy and ﬁrst (premise) sentence norm plots for different runs of the
same architecture: hyperbolic RNN followed by hyperbolic FFNN and hyperbolic MLR. The X axis
shows millions of training examples processed.

(a) Test accuracy

21

