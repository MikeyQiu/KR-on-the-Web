0
2
0
2
 
r
a

M
 
3
 
 
]

G
L
.
s
c
[
 
 
1
v
4
0
7
1
0
.
3
0
0
2
:
v
i
X
r
a

Model Selection in Contextual Stochastic Bandit Problems

Aldo Pacchiano∗
pacchiano@berkeley.edu
UC Berkeley

My Phan∗
myphan@cs.umass.edu
University of Massachusetts

Yasin Abbasi-Yadkori
yasin.abbasi@gmail.com
VinAI

Anup Rao
anuprao@adobe.com
Adobe

Julian Zimmert
zimmert@google.com
Google Research

Tor Lattimore
lattimore@google.com
DeepMind

Csaba Szepesvari
szepi@google.com
DeepMind

March 4, 2020

Abstract

We study model selection in stochastic bandit problems. Our approach relies on a master algorithm
that selects its actions among candidate base algorithms. While this problem is studied for speciﬁc classes
of stochastic base algorithms, our objective is to provide a method that can work with more general
classes of stochastic base algorithms. We propose a master algorithm inspired by CORRAL Agarwal
et al. (2017) and introduce a novel and generic smoothing transformation for stochastic bandit algorithms
that permits us to obtain O(
T ) regret guarantees for a wide class of base algorithms when working
along with our master. We exhibit a lower bound showing that even when one of the base algorithms
T ) regret in model selection, even
has O(log T ) regret, in general it is impossible to get better than Ω(
asymptotically. We apply our algorithm to choose among diﬀerent values of (cid:15) for the (cid:15)-greedy algorithm,
and to choose between the k-armed UCB and linear UCB algorithms. Our empirical studies further
conﬁrm the eﬀectiveness of our model-selection method.

√

√

1

Introduction

Bandit algorithms have been applied in a variety of decision making and personalization problems in industry.
There are many specialized algorithms each designed to perform well in speciﬁc environments. For example,
algorithms are designed to exploit low variance (Audibert et al., 2009), extra context information and linear
reward structure (Dani et al., 2008; Li et al., 2010; Abbasi-Yadkori et al., 2011), sparsity (Abbasi-Yadkori
et al., 2012; Carpentier and Munos, 2012), etc. The exact properties of the current environment however
might not be known in advance, and we might not know which algorithm is going to perform best. Given the
online nature of the problem, batch model selection is not possible in many practical situations. Therefore, it
is desired to develop a method to perform model-selection with bandit information in an online fashion.

As an example, consider the application of bandit algorithms in online personalization problems where
the task is to assign one of the available oﬀers to each visiting user. Often a context vector is available
that provides extra information about the user (such as location, browser type, etc). Contextual bandit
algorithms such as LinUCB (Li et al., 2010) are designed for such problems. When the context vectors are
high-dimensional and arrive in an i.i.d fashion, and the time horizon is small, then by the bias-variance

∗Equal contribution.

1

trade-oﬀ we might be better oﬀ using a simpler non-contextual bandit algorithm instead of a contextual
algorithm. Here, we might want to choose between UCB and LinUCB in an adaptive fashion.

As another application, consider the problem of tuning the exploration rate of bandit algorithms such as
(cid:15)-greedy, UCB, etc. The exploration rate recommended by the theoretical analysis can be overly conservative.
It might be tempting to decrease the exploration rate manually when deploying the algorithm in practice.
The danger is that if the exploration rate is too small, the algorithm might perform poorly. We would like to
design a mechanism to tune the exploration rate in an adaptive data-dependant fashion.

Maillard and Munos (2011) are perhaps the ﬁrst to address the bandit model-selection problem. These
results are improved by Agarwal et al. (2017). The main idea of Agarwal et al. (2017) is to combine the base
algorithms using an online mirror descent master algorithm that sends importance weighted rewards to the
base algorithms. Given the application of importance weighting, the approach is better suited for combining
adversarial base algorithms.

Chatterji et al. (2019) and Foster et al. (2019) study bandit model-selection problem when the reward
is stochastic and has a linear structure of unknown order. Chatterji et al. (2019) propose an algorithm
for model-selection and show strong guarantees but under strong conditions. More speciﬁcally, Chatterji
et al. (2019) assume that the contexts are sampled in an i.i.d. fashion from a distribution and the smallest
eigenvalue of the covariance matrix of the distribution is suﬃciently large. Under such assumptions, Bastani
et al. (2017) and Kannan et al. (2018) suggest that advanced exploration might not be necessary. Foster et al.
(2019) consider the linear contextual bandit problem with multiple policy classes of diﬀerent dimensions.
√
Foster et al. (2019) show ˜O(T 2/3d∗1/3) and ˜O(T 3/4 +
T d∗) regret guarantees where T is the time horizon
and d∗ is the true dimension of the reward function. These bounds are sub-optimal when d∗ is not too large.
Further, Foster et al. (2019) require a lower bound on the average eigenvalues of the co-variance matrices
of all actions. They pose the question of whether model selection is possible without eigenvalue conditions.
Apart from strong assumptions, the above results are limited to model-selection among linear classes. A
general and eﬃcient method to combine multiple stochastic base algorithms is missing.

In this work, we focus on bandit model-selection in general stochastic environments. Notice that for the
approach of Agarwal et al. (2017) to be applicable, a base algorithm needs to be properly modiﬁed to satisfy
the stability condition of Agarwal et al. (2017). For example, for the UCB algorithm we would need to use a
Bernstein type concentration inequality instead of the usual Hoeﬀding bound. This approach is tedious as
each algorithm needs to be individually modiﬁed. We would like to provide a generic procedure applicable
to most base algorithms in a stochastic environment. We provide such result by introducing a smoothing
technique that transforms almost any algorithm in a stochastic environment to one that satisﬁes a stability
condition so that it can be used along with the model selection master algorithm. In particular, we show
how the approach can be used to combine UCB and LinUCB in contextual problems. We can also use our
model-selection procedure to obtain a near optimal exploration rate for (cid:15)-greedy algorithms without a prior
knowledge of the smallest gap. Our empirical studies conﬁrm the eﬀectiveness of the proposed approach in
these two applications.

In the stochastic domain, an important question is whether a model selection procedure can inherit the
O(log T ) regret of a fast stochastic base algorithm (such as UCB when the “gap” is large). We show via a
lower bound construction that such a result is impossible in general.

Let us also mention the literature on the “best of the both worlds” problems. These papers aim to design
a single algorithm that can handle both stochastic and adversarial environments (Audibert and Bubeck, 2009;
Bubeck and Slivkins, 2012; Seldin and Slivkins, 2014; Auer and Chiang, 2016; Abbasi-Yadkori et al., 2018;
Zimmert and Seldin, 2019).

2 Problem statement

We consider a contextual multi-armed bandit problem with K actions. In round t, the learner observes a
d-dimensional context vector xt ∈ X , that arrives in an i.i.d fashion. Let ht ∈ Ht denote the history at time

2

1 in a policy class Π and at ∈ [K] such
t. The learner’s policy at time t is a mapping πt : X × H → ∆K
that at ∼ πt(xt, ht) is the action’s index taken by the learning agent in round t. For simplicity when the
history is clear we use πt(xt). After taking action at the learner observes a noisy reward signal gat,t such that
f (xt, πt) = E[gat,t] where f is called the reward function. For example, in the case of contextual linear bandits
with contexts X = Rd×K, a policy π maps from the space of d × K matrices to ∆K, gi = x(cid:62)
i θ∗ + ξ where ξ is
i θ∗(cid:3) for some θ∗ ∈ Rd, where
a random zero-mean noise and the reward function satisﬁes f (x, π) = Ei∼π
xi denotes the i−th column of x.

(cid:2)x(cid:62)

We are interested in designing an algorithm with small regret, deﬁned as

R(T ) = max
π∗∈Π

E

f (xt, π∗) −

f (xt, πt)

.

(cid:35)

T
(cid:88)

t=1

(cid:34) T

(cid:88)

t=1

(1)

We assume M base algorithms are available. Let Ri(T ) be the regret of the ith base algorithm. We
want to design a bandit method that plays one of these base algorithms in each round and its overall
regret satisﬁes R(T ) ≤ O(mini Ri(T )). For any algorithm B we deﬁne its instantaneous regret at time t as
rt = f (xt, π∗) − f (xt, πt) where π∗ is the optimal policy in Equation 1 and πt is the possibly path dependent
policy B uses at time t.

3 Stochastic Corral

In this Section we introduce our algorithm and provide its regret analysis. In Section 3.1 we describe our
algorithm. In Section 4.3 we show the regret analysis provided the base algorithm satisﬁes a condition. In
Section 4.4 we show a "smoothing" procedure that will transform a wide class of algorithms to satisfy the
condition.

3.1 Algorithm

Our algorithm is a variant of the CORRAL algorithm Agarwal et al. (2017) modiﬁed for stochastic environ-
ments. First, we explain the CORRAL algorithm and then introduce the new variant.

The basic structure of the CORRAL algorithm is the following: The master receives M base algorithms
{Bi}M
i=1. During any time t of the algorithm’s execution, CORRAL maintains a distribution pt over ∆M
used to select the index it ∼ pt of the algorithm to use during that round. After an algorithm Bit is selected,
its policy πt,it is used by the master to select an action at ∼ πt,it(xt) where xt ∼ D is the context sampled
at time t. The resulting reward signal gt = f (xt, δat) + ξt where ξt denotes a zero-mean random noise and
δi denotes the Dirac distribution at action i is fed back to all the M base algorithms in the form of an
importance weighted estimate ˜gi
for all i. Subsequently each of the base algorithms i ∈ [M ]
update their internal state based on the feedback ˜gi

t received.
CORRAL requires its base algorithms to satisfy a stability condition to work along with the importance
weighting feedback. Because importance weighting can change the loss range and distribution throughout
the run of the algorithm in an unpredictable fashion, it is not directly compatible with a stochastic reward
environment. Agarwal et al. (2017) change the details of many algorithms in a case by case basis to make
them stable. To avoid having to know the speciﬁc workings inside each base algorithm, we introduce a variant
of the CORRAL algorithm and a generic algorithmic smoothing transformation that allows us to prove model
selection regret guarantees for a wide class of algorithms in a stochastic reward environment.

t = 1(i=it)gt

pi
t

Stochastic CORRAL (see Algorithm 1) preserves most of the structure from CORRAL with 2 main
diﬀerences. First, the base algorithms receive an unweighted feedback and updates their internal policy only
when they are chosen and repeat their recommended policy to the master until they are chosen again. This
ensures our algorithm is compatible with the internal workings of many algorithms without requiring major
modiﬁcations. Intuitively, if the base is chosen every c time steps where c > 1 is a constant, its regret can

1We denote the d − 1 dimensional simplex as ∆d.

3

i=1, learning rate η.

Algorithm 1 Stochastic Corral
Input: Base Algorithms {Bi}M
Let πt,i be the policy of Bi indexed by round t.
Let π(1)
t,i , π(2)
Initialize p1 to pi
for t = 1, · · · , T do
Sample it ∼ pt.

t,i be the policies used by ˜Bi in round t.
M for all i ∈ [M ].

1 = 1

Step 1 Play action a(1)

t ∼ D.

Receive context x(1)
Receive policy π(1)
= πt,it from Bit.
t,it
t ∼ π(1)
(x(1)
t ).
t,it
Receive feedback g(1)
t = f (x(1)
Update Bi using g(1)

, δa(1)

.

t

t

t

) + ξ(1)

t

Step 2 Play action a(2)

t ∼ D.

Receive context x(2)
Sample s ∼ Uniform(0, · · · , t).
Receive policy π(2)
= πs,it from Bit.
t,it
t ∼ π(2)
(x(2)
t ).
t,it
t = f (x(2)
Receive feedback g(2)

, δa(2)

t

t

) + ξ(2)

t

Update pt using g(1)
(2017).

t + g(2)

t

via the Corral Update. See Appendix A or Algorithm 1 in Agarwal et al.

be upper bounded by cR(T /c) because it updates its policy T /c times and repeats a policy for c time steps
between two updates.

Second, we introduce a "smoothing" procedure which converts any algorithm to one with non-increasing
1, . . . , pi
T } be the
. We need the instantaneous
t at every time step is the worst case because the

instantaneous regret with high probability (Deﬁnition 4.2). The reason is as follows. Let {pi
(random) probabilities that M chooses the i-th base algorithm and let ρi
regret to decrease with high probability so that using mint pi
base will be updated the least often. Therefore the regret can be upper bounded by E (cid:2)ρi

∗ = 1

mint pi
t

∗R(T /ρi

∗)(cid:3).

We use a two time step structure, Step 1 to update the policy of the base algorithm, and Step 2 to play a
smoothed decision, ensuring its conditional instantaneous regret of Step 2 is upper bounded by a decreasing
function with high probability. Details of the smoothing trick is given in Section 4.4.

t

t

.

, g(2)
t

Let {pi

t + g(2)

as one g(1)

T } be the (random) probabilities that M chooses the i-th base algorithm and let ρi

Henceforth we refer to Algorithm 1 as Stochastic CORRAL and to CORRAL Agarwal et al. (2017) as
Vanilla CORRAL. We use η to denote the input learning rate of Stochastic CORRAL. The distribution pt is
updated using a log barrier that follows the same update rules as in Vanilla CORRAL. We reproduce the full
Vanilla CORRAL algorithm in Appendix A. We use M to denote the master algorithm. In the remainder we
call each time indexed by t a round. Each round is split in two steps of type 1 and 2. The master treats
each round’s two rewards g(1)
1, . . . , pi

.
mint pi
t
We drop the superscript i when it is clear. We use ni
t to denote the number of rounds base i is chosen up to
time t. Let tl,i be the round index of the l−th time the master chooses algorithm Bi and let bl,i = tl,i − tl−1,i
T +1,i = T + 1. If a base algorithm is ran for T rounds, we use T (j) to denote its T steps
with t0,i = 0 and tni
of type j for j ∈ {1, 2}. We use r(j)
to denote the master algorithm’s instantaneous regret in step j of round
t. Similarly, we denote by x(1)
, π(1)
the contexts and policies used by the master during
t
round t step 1 and 2. Analogously we call π(1)
t,i the policies proposed by base algorithm Bi at time t,
even when it is not selected (it (cid:54)= i) by M.
Base repeated policies. During the round when Bi is not selected, we assume it repeats its future Step 2’s
policy. More precisely for j ∈ {1, 2} and t = tl−1 + 1, · · · , tl − 1, π(j)
i + 1. For all rounds
t and steps j, regardless of whether the master selected Bi or not we denote base i instantaneous regret by
r(j)
t,i . Our main result implies the following:

and π(2)
t,i and π(2)

tl,i for all l ≤ nT

t
and x(2)

t,i = π(2)

∗ = 1

t

t

t

Theorem 3.1 (Informal). Let α ∈ [1/2, 1), if base algorithm Bi satisﬁes a high probability regret bound
Ri(t) = O(tα) for all t ∈ [T ], the regret of M when running with the smoothed version of Bi satisﬁes,

4

R(T ) ≤ ˜O

(cid:16) M

η + T η + T η

(cid:17)

1−α
α

. Choosing η ≈ M α

T α yields R(T ) = ˜O (cid:0)M 1−αT α(cid:1).

4 Regret Analysis

In this section we analyze the regret of our Stochastic CORRAL algorithm. Our regret analysis follows a
similar structure as in (Agarwal et al., 2017). We split the regret in two terms (Section 4.2): the regret of the
master algorithm with respect to a ﬁxed base (I) and the regret of this base algorithm with respect to the
optimal policy (II). Controlling term I makes use of the repeated policy structure of modiﬁcation 1) and
Lemma 13 of Agarwal et al. (2017). Bounding term II (Section 4.3) is the main focus of the regret analysis
in this paper. In Section 4.1 we deﬁne the condition necessary for a base algorithm to have low regret while
running with our Stochastic CORRAL master.

4.1 Non-increasing instantaneous regret

As explained above, we require the base algorithms to satisfy a smoothness condition ensuring an upper
bound on the conditional instantaneous regret to be non-increasing. Since this condition need not be true for
general bandit algorithms, we produce a generic procedure (Step 2 of the proposed algorithm) to modify
an input base algorithm B into what we term a "smoothed" version ˜B that satisﬁes it. Given an algorithm
with concave (in t) cumulative regret bound U (t, δ) that holds with high probability, we construct a new
algorithm with instantaneous regret bound u(t, δ) = U (t, δ)/t. With high probability, since U (t, δ) is concave,
u(t, δ) will be non-increasing in t.

The smoothed version ˜Bi of a base algorithm Bi works as follows. We have two steps at each round t. In
step 1, we play Bi. In step 2, at time t, we pick a time step s in [1, 2, .., t] uniformly at random, and re-play
the policy made by Bi at time s. Since the policy of Bi at each round [1, 2, ...t] is chosen with probability 1/t
to be played at step 2, the instantaneous regret of step 2 at round t is 1/t times the cumulative regret of B
up to time t.

The following three properties will ensure low regret for the overall algorithm: 1) The regret of Step 1 is
bounded by U (t, δ) with high probability (Deﬁnition 4.1). 2) Since the instantaneous regret of Step 2 is 1/t
times the cumulative regret of Step 1, the cumulative regret of Step 2 is bounded roughly by (cid:80)T
t=1 1/t ≈ log(T )
times that of step 1. 3) The instantaneous regret of step 2 is U (t, δ)/t, which is non-increasing (Deﬁnition 4.2)
if U (t, δ) is concave. The master receives this feedback from Step 2.

Properties (1) and (2) ensure that regret of the smoothed version is low. Property (3) ensures that using
i at time t results in the largest regret. Therefore, regret when running with a master can be

the smallest pt
∗)(cid:3). We deﬁne these properties more precisely:
upper bounded by E (cid:2)ρi
Deﬁnition 4.1 ((U, δ, S)−Boundedness). Let U : R × [0, 1] → R+ and S ⊂ [T ]. We say an algorithm B is
(U, δ, S)−bounded if it is updated only on rounds S and with probability at least 1 − δ and for all rounds t ∈ S,
the cumulative pseudo-regret of rounds in S is bounded above by U (t, δ):

∗U (T /ρi

(cid:88)

j∈S,j≤t

f (x(1)
j

, π∗) − f (x(1)

j

, πj) ≤ U (t, δ), ∀t ∈ S.

Deﬁnition 4.2 ((U, δ, T (2))−Smoothness). Let U : R×[0, 1] → R+. We say an algorithm ˜B is (U, δ, T (2))−smooth
if with probability 1 − δ and for all rounds t ∈ [T ], the conditional expected instantaneous regret of type 2
steps is bounded above by U (t, δ)/t. In other words, with probability 1 − δ:

Ext∼D[r(2)

t

|Ft−1] ≤

U (t, δ)
t

, ∀t ∈ [T ].

Here Ft−1 denotes the sigma algebra of all randomness up to the beginning of round t.

Throughout proofs we assume that base algorithms satisfy (U, δ(cid:48)(cid:48), T (1))−boundedness (on all type 1 steps)
and the smoothed versions satisfy (U, δ(cid:48), T (2))−smoothness for an appropriate function U and constants
δ, δ(cid:48)(cid:48) ∈ [0, 1]:

(2)

(3)

5

Assumption A1 (Base Boundedness and Smoothness) All input algorithms {Bi}M
(Ui, δ
concave function in term of t.

M , T (1))−bounded and their smooth versions { ˜Bi}M

i=1 to M are
M , T (2))−smooth where Ui(t, δ) is a

i=1 are (Ui, δ

In Proposition 4.7 we show that the above assumption is satisﬁed if all base algorithms are (U, δ, [T ])−bounded
for an appropriate function U .

Let Ei be the event that Equations 3 and 2 hold for all t ∈ [T ]. Throughout the paper we condition on
i=1Ei. A simple application of the union bound yields P(E) ≥ 1 − δ. In the remainder of this

the event E = ∩M
section we prove our main result (Theorem 3.1).

4.2 Regret Decomposition

In order to analyze the regret of Algorithm 1, we use the same decomposition as in (Agarwal et al., 2017):
we decompose the regret into the regret of the master algorithm with respect to base algorithm i, and the
regret of this base algorithm with respect to the optimal choice. Algorithm Bi’s internal state is updated
only during steps of type 1 in rounds t ∈ Ti. Recall that by our construction, a base algorithm repeats its
recommended policy in rounds that it is not being selected, i.e. for all l ≤ ni
T and during both steps of rounds
t,i = π(2)
t = tl−1,i + 1, · · · , tl,i − 1, π(1)
tl,i. In what follows we drop the i subscript from {tl,i} and {bl,i}
whenever clear. The following holds:

t,i = π(2)

R(T ) = max
π∈Π

E



f (x(j)
t

, π) − f (x(j)

t

, π(j)
t

)




T
(cid:88)

2
(cid:88)

t=1

j=1

f (x(j)
t

, π(j)

t,i ) − f (x(j)

t

, π(j)
t

)

+

= E

2
(cid:88)

j=1

T
(cid:88)






t=1
(cid:124)





E

2
(cid:88)

j=1

T
(cid:88)






t=1
(cid:124)

(cid:123)(cid:122)
I

(cid:123)(cid:122)
II











(cid:125)







(cid:125)

f (x(j)
t

, π∗) − f (x(j)

t

, π(j)
t,i )

(4)

We identify the maximizing policy, a deterministic object, with π∗. We bound ﬁrst the expectation of term I
in Equation 4 (the regret of the master algorithm with respect to the base).

Term I can be upper bounded following Lemma 13 of Agarwal et al. (2017):

Lemma 4.1 (Lemma 13 of Agarwal et al. (2017)). We have

E [I] ≤ O

(cid:18) M ln T
η

(cid:19)

+ T η

−

E [ρ∗]
40η ln T

.

Crucially, this result holds since the importance weighted update of pt, along with the base repeated policy
structure ensures that the master’s loss estimates are indeed unbiased estimators of the base algorithm’s
rewards. We discuss in more detail in the Appendix. The rest of the paper is devoted to bounding E [II].

4.3 Main results

We split the proof of the main result of this section (Theorem 4.3) in two parts. First we show in Lemma 4.2
an upper bound on the base algorithm’s regret provided the pi
t sequence is lower bounded with probability 1
by a constant p. We leverage this result to prove Theorem 4.3 that shows a bound on the expected regret

6

of an algorithm satisfying Assumption A1 whose invocations are controlled by a pi
running Stochastic CORRAL on top.

t sequence resulting from

Let Ti ⊂ [T ] be the set of rounds where base i is chosen and Tc
deﬁne the regret of the base algorithm during Step j of rounds S as R(j)
The following decomposition of E [II] holds:

i

i = [T ]\Ti. For S ⊂ [T ] and j ∈ {1, 2}, we
, π(j)
t,i ).
t

, π∗) − f (x(j)

t∈S f (x(j)

(S) = (cid:80)

t

E [II] = E


R(1)


i

(Ti) + R(2)

i

(cid:124)

(Ti) + R(1)
(Tc
(cid:123)(cid:122)
II0

i

i ) + R(2)

i

(Tc

i )
 .
(cid:125)



R(1)
(Ti) consists of the regret when the base was updated in Step 1 while the remaining 3 terms consists of
i
the regret when the policies are reused. Since E [II] ≤ E [II1{E}] + δT , we focus on bounding E [II1{E}].
T )1(E)(cid:3). We proceed to bound the regret corresponding

≤ E (cid:2)Ui(δ, ni

(cid:105)
(Ti)1(E)

(cid:104)

R(1)
Under Assumption A1, E
i
to the remaining terms in II0:

E [II01(E)] = E

1{E}(2bl − 1)E

ni
T +1
(cid:88)

l=1

ni
T +1
(cid:88)









l=1

(cid:104)

r(2)
tl,i|Ftl−1

Ui(l, δ/2M )
l

(cid:105)









≤ E

1{E}(2bl − 1)

(5)

The multiplier 2bl − 1 arises because the policies proposed by the base algorithm during the rounds it is not
selected by M satisfy π(1)
i + 1 and t = tl−1 + 1, · · · , tl − 1. The factorization is
(cid:3) where Ftl−1 already includes
2M , T (2))−smooth and

a result of conditional independence between E
algorithm ˜Bi update right after round tl−1. The inequality holds because ˜Bi is (Ui, δ
therefore satisﬁes Equation 3 on event E.

tl,i for all l ≤ nT
(cid:104)
r(2)
tl,i|Ftl−1

and E (cid:2)bl|Ftl−1

t,i = π(2)

t,i = π(2)

(cid:105)

Lemma 4.2 (Fixed p). If 1

ρ = p ≤ pi

1, · · · , pi

T with probability one, then, E [II] ≤ 4ρ Ui(T /ρ, δ) log T + δT .

Since the conditional instantaneous regret (Deﬁnition 4.2) has a non-increasing upper bound, using p at
every time step will result in the largest upper bound on its regret because the base is updated the least
often (see length of bl intervals in Eq. 5). In this case the base will be updated every ρ time-steps and the
regret upper bound will be roughly ρ Ui(T /ρ, δ). The proof is in Appendix D.

Notice the bound in Lemma 4.2 in addition to Lemma 4.1 would yield a regret guarantee for Stochastic
Corral vs the base algorithm in terms of a deterministic lower bound p for the probabilities pi
T . This
is of course unsatisfactory because these probability values are random. We use a restarting trick to address
this concern.

1, · · · , pi

Restarting trick: Initialize p = 1
Therefore the time horizon is divided into phases, and in each phase the lower bound p is deterministic.

2 and restart the base.

2M . If pi

t

t < p, set p = pi

We provide the analysis below:

Theorem 4.3. [Path dependent p] When we run the base algorithm with the CORRAL master algorithm,
and restart the base every time Line 10 of the vanilla CORRAL algorithm (Agarwal et al., 2017) is executed
(as described above),

E [II] ≤ 4E [ρ∗ Ui(T /ρ∗, δ) log T ] + δT (log T + 1).

Here, the expectation is over the random variable ρ∗ = maxt
21−α−1 T αg(δ)E (cid:2)ρ1−α
g : R → R+, and α ∈ [1/2, 1) then, I ≤ 4 21−α

∗

1
pi
t

(cid:3) + δT (log T + 1).

. If U (t, δ) = tαg(δ) for some function

The proof in Appendix F follows that of Theorem 15 in (Agarwal et al., 2017). Putting it all together we

conclude our main theorem:

7

Theorem 4.4. Let U (t, δ) = tαg(δ) for some 0 ≤ α ≤ 1 and some function g : R → R+. If Algorithm Bi
satisﬁes (U, δ
M , T (2))−smooth, stochastic CORRAL with
the restarting trick satisﬁes:

M , T (1))−boundedness and its smooth version is (U, δ

R(T ) ≤ O

(cid:19)

+ T η

(cid:18) M ln T
η
ρ∗
40η ln T

(cid:20)

− E

− 2ρ∗ U (T /ρ∗, δ) log T

+ δT .

(cid:21)

Proof. The result follows from Equation 4 and the bounds of Lemma 4.1 and Theorem 4.3.

Maximizing over ρ∗ gives us the following worst-case bound:

Corollary 4.5. If a base algorithm is (U, δ, T (1))-bounded and its smooth version is (U, δ, T (2))−smooth for
U (T, δ) = T αg(δ) for some α ∈ [1/2, 1), then the regret of the master algorithm is bounded as

R(T ) ≤ ˜O

+ T η + T g(δ)

1
α η

1−α
α

+ δT.

(cid:19)

(cid:18) M
η

When η = M α

g(δ)T α then R(T ) ≤ ˜O (cid:0)M 1−αg(δ)T α(cid:1) + δT .

The proof is in Appendix G.
In Section 6, we show explicit bounds for some applications.

4.4 Algorithm smoothing
Recall from Section 4.1 that the smoothed version ˜Bi of a base algorithm Bi works as follows. In step 1, we
play Bi and use the feedback to update its internal structure. In step 2, at time (cid:96), we uniformly pick at
random a time step s in [1, 2, .., (cid:96)], and re-play the policy that was made by Bi in Step 1 at time s. Since the
policy of Bi at each time step [1, 2, ...(cid:96)] is chosen with probability 1/(cid:96) to be played at step 2, the instantaneous
regret of step 2 at time (cid:96) is 1/(cid:96) of the cumulative regret of Bi up to time (cid:96).

Note that we are replaying the decision of Bi at time s learned from a sequence of contexts x(1)

to
another context x(2)
. Since the contexts are sampled i.i.d from the same distribution, in Lemma 4.6 we will
show that when we reuse the policy learned from a series of contexts x1, ..., xt to another series of context
x(cid:48)
1, ..., x(cid:48)
t, the regret is multiplied only by a constant factor. We call the regret when using a policy learned
from a series of context to another series of contexts "replay regret".

1 , ..., x(1)

s

(cid:96)

Deﬁnition 4.3 (Expected Replay Regret). Let h be a generic history of algorithm B and h(t) the history h
up to time t. If x1, · · · , xt are i.i.d. contexts from D and π1, · · · , πt is the sequence of policies used by B on
these contexts, the "expected replay regret" R(t, h) is:

R(t, h) = Ex(cid:48)

1,··· ,x(cid:48)
t

f (x(cid:48)

l, π∗) − f (x(cid:48)

(6)

(cid:35)
l, πt)

(cid:113)

Lemma 4.6. If B is (U, δ, [T ])−bounded, maxx,π |f (x, π)| ≤ 1, U (t, δ) > 8
expected replay regret satisﬁes: R(t, h) ≤ 4U (t, δ) + 2δt ≤ 5U (t, δ).

t log( t2

δ ), and δ ≤ 1√

T

, then B’s

Lemma 4.6 is a consequence of a simple martingale concentration bound (The proof is in Appendix E).

We now present the main result of this section:

Proposition 4.7. If U (t, δ) > 8
(5U, δ, T (2))−smooth.

(cid:113)

t log( t2

δ ), δ ≤ 1√

T

and B satisﬁes (U, δ, [T ])−boundedness, then ˜B is

(cid:34) t

(cid:88)

l=1

8

Proof. Since the conditional instantaneous regret on Step 2 of round t equals the average replay regret of the
type 1 steps up to t, Lemma 4.6 implies E[r(2)
|Ft−1] ≤ 5U (t,δ)

.

t

t

Consequently:

Corollary 4.8 (Informal ). All (U, δ, [T ])−bounded algorithms can be smoothed and used with Stochastic
Corral.

In Appendix H and I we show that several algorithms are (U, δ, [T ])-bounded for appropriate functions U .

Lemma 4.9. Assuming that the noise ξt is conditionally 1-sub-Gaussian, UCB is (U, δ, [T ])-bounded with
U (t, δ) = O(

tk log tk

√

δ ).

Lemma 4.10 (Theorem 3 in (Abbasi-Yadkori et al., 2011)). LinUCB is (U, δ, [T ])-bounded with U (t, δ) =
O(d

t log(1/δ)).

√

Lemma 4.11. If c = 10K log( 1
δ )
then (cid:15)−greedy with (cid:15)t = c

∆2
∗

t satisﬁes a (U, δ, [T ])−bounded for δ ≤ ∆2

T 3 and:

∗

where ∆j is the gap between the optimal arm and arm j and ∆∗ = minj ∆j,

1. U (t, δ) = 16

log( 1

δ )t when K = 2.

2. U (t, δ) = 20

K log( 1
δ )

(cid:16)(cid:80)K

j=2 ∆j

(cid:17)(cid:17)1/3

t2/3 when K > 2.

(cid:113)

(cid:16)

Lemma 4.12 (Theorem 1 in (Seldin et al., 2013)). Exp3 is (U, δ, [T ])−bounded where U (t, δ) = O(

√

tk log tk

δ ).

5 Lower bound

In stochastic environments with suﬃciently large “gap”, algorithms such as UCB achieve logarithmic regret
T ) overall regret even in stochastic problems. In this
bounds. Our model selection procedure has a O(
T ). More speciﬁcally, we
section, we show that in general it is impossible to obtain a regret better than Ω(
construct an example in which there are 2 base algorithms, one of which has 0 regret, and show that when
running these 2 base algorithms with any master, it is impossible to have better than ˜Ω(

T ) regret.

√

√

√

Theorem 5.1. There exists an algorithm selection problem, such that the regret for any time T is lower
bounded by R(T ) = Ω

(cid:16) √

(cid:17)

.

T
log(T )

Proof sketch, full proof in Appendix K. The two base algorithms are constructed such that the gap between
the algorithms closes at a rate of Θ(1/(
t log(t))). We show that at this rate, any master will have a constant
probability of misidentifying the optimal algorithm even after observing inﬁnite pulls. Hence the regret of the
master is of order Ω

t log(t))

(cid:16)(cid:80)T

= ˜Ω(

T ).

√

√

√

(cid:17)

t=1 1/(

6 Applications

In this section, we show two applications of our results. First, we show how the results can be used to combine
contextual and non-contextual stochastic algorithms and match the regret lower bound. Second, we design a
method to ﬁnd a near optimal exploration rate for (cid:15)-greedy in an adaptive fashion.

9

6.1 Contextual vs non-contextual UCB
The regret of UCB Lattimore and Szepesvari for k-armed bandit problem is ˜O(
kT ) where k is the number
of arms. The regret of LinUCB Lattimore and Szepesvari for linear bandit problem is ˜O(d
T ) where d is the
dimension of the context vectors. In this section we show how to run our Stochastic CORRAL with UCB
and LinUCB as base algorithms and achieve the regret matching the lower bound.

√

√

Lemma 6.1 (Implied by Theorem 24.4 in (Lattimore and Szepesvari)). Let Rν(T ) denote the cumulative
regret at time T on bandit environment ν. For any algorithm there exist a 1-dimensional linear bandit
environment ν1 and a k-armed bandit environment ν2 such that: Rν1(T ) · Rν2 (T ) ≥ T (k − 1)e−2. Without
knowing the environment, the regret is at least max{Rν1 (T ), Rν2(T )}.

We show that the regret of Stochastic CORRAL with base algorithms LinUCB and UCB matches the

lower bound. The proof is in Section J.2.

Theorem 6.2. The regret of stochastic CORRAL with base algorithms LinUCB and UCB and rate η =
is upper bounded by2:

(cid:113) 2
√
T d

k

(cid:32)√

(cid:18)

˜O

2T

d0.5k0.25 +

(cid:19)

(cid:18)

∨

d0.5k0.25 +

k0.75
d0.5

(cid:19) (cid:33)
.

d1.5
k0.25

In terms of dependence in k and T , the product of the two terms in the bound matches the lower bound in
Lemma 6.1 (the product of the two terms being of order kT ).

As we mentioned earlier, Chatterji et al. (2019) and Foster et al. (2019) study related problems. However,
the assumptions in Chatterji et al. (2019) appear to be too strong, while the regret bounds in Foster et al.
(2019) are not optimal in scaling with T . Our approach has the additional advantage that it can also handle
model mis-speciﬁcation.

6.2 Tuning the exploration rate of (cid:15)-greedy

For a given positive constant c, the (cid:15)-greedy algorithm pulls a random arm in round t with probability
(cid:15)t = c/t, and otherwise pulls the arm with the largest empirical average reward. It can be proven that the
optimal value for (cid:15)t is min{1, 5K
∗t } where ∆∗ is the smallest gap between the optimal arm and the sub-optimal
∆2
arms. We would like to ﬁnd the optimal value of c without knowing ∆∗. In this section we will use stochastic
CORRAL to ﬁnd the best c.

Given the time horizon T as an input, we divide the interval [1, KT ] into an exponential grid [1, 2, 22, ..., 2log(KT )].

We use (cid:15)-greedy with each value of c in the grid as a base algorithm for CORRAL. The following theorem
shows the regret of CORRAL using these base algorithms. The proof is in Appendix J.1.

Theorem 6.3. The regret of stochastic CORRAL using (cid:15)-greedy base algorithms deﬁned on the grid with η
chosen as in Corollary 4.5 is bounded by ˜O(T 2/3) when K > 2, and by ˜O(T 1/2) when K = 2.

7 Experiments

Now we show results of running the proposed algorithm (referred to as Corral in the plots) in a couple of
scenarios. We do this in two settings. The ﬁst problem we consider is the choice of (cid:15) parameter in (cid:15)-greedy.
The second problem we consider is model selection between UCB and LinUCB. In all the experiments, we
T . We repeat each experiment several times and
take the initial learning rate of CORRAL to be η = 20/
the shading denotes the mean squared error. Note that we have not implemented smoothing of the base
algorithms in these experiments.

√

2We use ∨ to denote the max operation.

10

For (cid:15)-greedy, we consider the case of 2 Bernoulli arms with means p1 = 0.5 and p2 = 0.5 − ∆. We consider
two cases, ∆ = 0.1 and ∆ = 0.05. We consider eighteen base algorithms diﬀering in their choice of (cid:15) in the
exploration rate (cid:15)t = (cid:15)/t. We take T = 100, 000 and (cid:15)’s to lie on a geometric grid in [1, 2T ]. The results
are shown in Figure 1. While performance of (cid:15)-greedy with a ﬁxed (cid:15) can be sensitive to the environment,
CORRAL shows stability and a relatively good performance in diﬀerent environments.

(a) ∆ = 0.1

(b) ∆ = 0.05.

Figure 1: (cid:15)-Greedy. Plot of cumulative regret as a function of time (t).

In the other experiment, we take UCB and LinUCB as the base algorithms, and consider a contextual
bandit environment. The contexts are independent and identically distributed. We again have two arms,
and each arm i has an associated vector θi ∈ Rd−1. We also let xi ∼ N (0, 1) and independent of each other.
We let the reward of arm i be µi + x(cid:62)θi + ηi, where ηi ∼ N (0, c), for some c. Regret is deﬁned as in the
contextual case. We consider two diﬀerent choices of θi and c: in the ﬁrst case, θi is small, which suggests a
non-contextual algorithm might perform better. The two plots are given in Figure 2.

(a) µ1 = 0.5 and µ2 = 0.4.

(b) µ1 = 0.5 and µ2 = 0.45.

Figure 2: UCB vs. LinUCB. Plot of cumulative regret as a function of time (t). Parameters: d = 10, K = 2.

The plots indicate the master has sub-linear regret. More importantly, the regret of the master lies in

11

between the best and worst base algorithms.

8 Conclusions

We study the bandit model-selection problem in stochastic environments. Our approach is general and
applicable to a diverse set of stochastic base algorithms. We introduce a smoothing trick that is applicable
under mild conditions and can transform a bandit algorithm so that the algorithm is stable with respect
to frequency of updates. The smoothing trick allows us to perform model selection using an online mirror
descent as the master algorithm. Finally, we perform empirical studies in syntethic environments and we show
the eﬀectiveness of the approach in two cases: tuning the exploration rate of (cid:15)-greedy, and model selection
between contextual and non-contextual UCB algorithms.

References

Y. Abbasi-Yadkori, P. Bartlett, V. Gabillon, A. Malek, and M. Valko. Best of both worlds: Stochastic and
adversarial best-arm identiﬁcation. In In Proceedings of the International Conference on Computational
Learning Theory (COLT), 2018.

Yasin Abbasi-Yadkori, David Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits.

In NIPS, 2011.

Yasin Abbasi-Yadkori, David Pál, and Csaba Szepesvári. Online-to-conﬁdence-set conversions and application

to sparse stochastic bandits. In AISTATS, 2012.

Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E. Schapire. Corralling a band of bandit
algorithms. In COLT, volume 65 of Proceedings of Machine Learning Research, pages 12–38. PMLR, 2017.

J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In In Proceedings of

the International Conference on Computational Learning Theory (COLT), 2009.

Jean-Yves Audibert, Rémi Munos, and Csaba Szepesvári. Exploration-exploitation tradeoﬀ using variance

estimates in multi-armed bandits. Theoretical Computer Science, 410(19):1876–1902, 2009.

P. Auer and C.-K. Chiang. An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial
bandits. In In Proceedings of the International Conference on Computational Learning Theory (COLT),
2016.

Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.

Mach. Learn., 47(2-3):235–256, 2002.

Hamsa Bastani, Mohsen Bayati, and Khashayar Khosravi. Mostly exploration-free algorithms for contextual

bandits, 2017.

S. Bubeck and A. Slivkins. The best of both worlds: stochastic and adversarial bandits. In In Proceedings of

the International Conference on Computational Learning Theory (COLT), 2012.

Alexandra Carpentier and Remi Munos. Bandit theory meets compressed sensing for high-dimensional

stochastic linear bandit. In AISTATS, 2012.

Niladri Chatterji, Vidya Muthukumar, and Peter L. Bartlett. Osom: A simultaneously optimal algorithm for

multi-armed and linear contextual bandits. In arXiv, 2019.

Varsha Dani, Thomas P. Hayes, and Sham M. Kakade. Stochastic linear optimization under bandit feedback.

In COLT, 2008.

12

Dylan Foster, Akshay Krishnamurthy, and Haipeng Luo. Model selection for contextual bandits. In Advances

in Neural Information Processing Systems, 2019.

Sampath Kannan, Jamie Morgenstern, Aaron Roth, Bo Waggoner, and Zhiwei Steven Wu. A smoothed
analysis of the greedy algorithm for the linear contextual bandit problem. In Proceedings of the 32Nd
International Conference on Neural Information Processing Systems, NIPS’18, pages 2231–2241, USA, 2018.
Curran Associates Inc. URL http://dl.acm.org/citation.cfm?id=3327144.3327150.

Tor Lattimore and Csaba Szepesvari. Bandit Algorithms.

Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to personalized

news article recommendation. In WWW, 2010.

Odalric-Ambrym Maillard and Rémi Munos. Adaptive bandits: Towards the best history-dependent strategy.

In AISTATS, 2011.

Y. Seldin and A. Slivkins. One practical algorithm for both stochastic and adversarial bandits.

In In

Proceedings of the International Conference on Machine Learning (ICML), 2014.

Yevgeny Seldin, Csaba Szepesvari, Peter Auer, and Yasin Abbasi-Yadkori. Evaluation and analysis of
the performance of the exp3 algorithm in stochastic environments. In Marc Peter Deisenroth, Csaba
Szepesvári, and Jan Peters, editors, Proceedings of the Tenth European Workshop on Reinforcement
Learning, volume 24 of Proceedings of Machine Learning Research, pages 103–116, Edinburgh, Scotland,
30 Jun–01 Jul 2013. PMLR. URL http://proceedings.mlr.press/v24/seldin12a.html.

Julian Zimmert and Yevgeny Seldin. An optimal algorithm for stochastic and adversarial bandits.

In

AISTATS, 2019.

13

The original Corral algorithm (Agarwal et al., 2017) is reproduced below.

A Corral Algorithm

A.1 Original Corral

Algorithm 2 Original Corral
Input: Base Algorithms {Bi}M
Initialize: γ = 1/T, β = e 1
Initialize all base algorithms.
for t = 1, · · · , T do

i=1, learning rate η.
1 = 2M, pi
1

ln T , η1,i = η, ρi

= 1
ρi
1

, pi

1 = 1/M for all i ∈ [M ].

Receive context xt ∼ D.
Receive policy πt,i from Bi for all i ∈ [M ].
Sample it ∼ pt.
Play action at ∼ πt,it(xt).
Receive feedback gt = f (xt, δat) + ξt.
Send feedback gt
pt,it

1{i = it} to Bi for all i ∈ [M ].

Update pt, ηt and p
t
for i = 1, · · · , M do

Set ρi

t+1 = 1
pi

t+1

to pt+1, ηt+1 and p

using gt via Corral-Update.

t+1

Algorithm 3 Corral-Update
Input: learning rate vector ηt, distribution pt, lower bound p
t
Output: updated distribution πt+1, learning rate ηt+1 and loss range ρt+1
Update pt+1 = Log-Barrier-OMD(pt, gt
pt,it
Set pt+1 = (1 − γ)pt+1 + γ 1
M .
for i = 1, · · · , M do
if pi
> pi
t+1 then
t
pi
Set pi
2 , ηt+1,i = βηt,i,
=
t+1

eit, ηt).

t+1

and current loss gt

else

Set pi

= pi
t

t+1

, ηt+1,i = ηt,i.

Return pt+1, ηt+1 and p

.

t+1

Algorithm 4 Log-Barrier-OMD(pt, (cid:96)t, ηt)
Input: learning rate vector ηt, previous distribution pt and current loss (cid:96)t
Output: updated distribution pt+1
Find λ ∈ [mini (cid:96)t,i, maxi (cid:96)t,i] such that (cid:80)M

1

+ηt,i((cid:96)t,i−λ) = 1

Return pt+1 such that

1
pi

t+1

= 1
pi
t

i=1

1
pi
t
+ ηt,i((cid:96)t,i − λ)

A.2 Stochastic Corral

We reproduce our stochastic Corral algorithm below for reference.

14

Algorithm 5 Stochastic Corral
Input: Base Algorithms {Bi}M
Let πt,i be the policy of Bi indexed by round t.
Initialize: γ = 1/T, β = e 1
for t = 1, · · · , T do
Sample it ∼ pt.

ln T , η1,i = η, pi

i=1, learning rate η.

1 = 1/M, pi
1

Step 1 Play action a(1)

t ∼ D.

Receive context x(1)
Receive policy π(1)
= πt,it from Bit.
t,it
t ∼ π(1)
(x(1)
t ).
t,it
Receive feedback g(1)
t = f (x(1)
Update Bit using g(1)

, δa(1)

.

t

t

t

) + ξ(1)

t

= 1

2M for all i ∈ [M ].

Step 2 Play action a(2)

t ∼ D.

Receive context x(2)
Sample s ∼ Uniform(0, · · · , t).
Receive policy π(2)
= πs,it from Bit.
t,it
(x(2)
t ∼ π(2)
t ).
t,it
t = f (x(2)
Receive feedback g(2)
to pt+1, ηt+1 and p

, δa(2)

t

t+1

Update pt, ηt and p

t

) + ξ(2)
t
using g(1)

t

t

, g(2)
t

via Corral-Update.

B Some useful lemmas

Lemma B.1. If U (t, δ) = tβg(δ), for 0 ≤ β ≤ 1 then:

U (l, δ) ≤

l
(cid:88)

t=1

U (t, δ)
t

≤

U (l, δ)

1
β

Proof. The LHS follows immediately from observing U (t,δ)
(cid:80)l
substituting the deﬁnition U (t, δ) = tβg(δ) and solving it.

t ≥ l U (l,δ)

U (t,δ)

l = U (l, δ). The RHS is a consequence of bounding the sum by the integral (cid:82) l

is decreasing as a function of t and therefore
dt,

U (t,δ)
t

t=1

0

t

Lemma B.2. If f (x) is a concave and doubly diﬀerentiable function on x > 0 and f (0) ≥ 0 then f (x)/x is
decreasing on x > 0

Proof. In order to show that f (x)/x is decreasing when x > 0, we want to show that
< 0
when x > 0. Since 0f (cid:48)(0) − f (0) ≤ 0, we will show that g(x) = xf (cid:48)(x) − f (x) is a non-increasing
function on x > 0. We have g(cid:48)(x) = xf (cid:48)(cid:48)(x) ≤ 0 when x ≥ 0 because f (x) is concave. Therefore
xf (cid:48)(x) − f (x) ≤ 0f (cid:48)(0) − f (0) ≤ 0 for all x ≥ 0, which completes the proof.

(cid:17)(cid:48)

(cid:16) f (x)
x

= xf (cid:48)(x)−f (x)
x2

Lemma B.3. For any ∆ ≤ 1

4 : kl( 1

2 , 1

2 − ∆) ≤ 3∆2.

Proof. By deﬁnition kl(p, q) = p log(p/q) + (1 − p) log( 1−p

kl

(cid:18) 1
2

,

1
2

(cid:19)

(cid:18)

− ∆

=

log(

) + log(

1
2
1
2

1
1 − 2∆
1
1 − 4∆2

(cid:18)

(cid:19)

1
2

=

log

=

log

1 +

1−q ), so
(cid:19)
)

1
1 + 2∆
(cid:18)

4∆2
1 − 4∆2

(cid:19)

≤

2∆2
1 − 4∆2 ≤

2∆2
3
4

≤ 3∆2

15

C Bounding E [I]

Recall I = (cid:80)T
listened to ˜B, (it = i). We split this term in two as follows:

t,i ) − f (x(j)

j=1 f (x(j)

, π(j)
t

, π(j)

(cid:80)2

t=1

t

t

) and that Ti equals the subset of random rounds where M

I =

f (x(j)
t

, π(j)

t,i ) − f (x(j)

t

, π(j)
t

)

f (x(j)
t

, π(j)

t,i ) − f (x(j)

t

, π(j)
t

)

+

f (x(j)
t

, π(j)

t,i ) − f (x(j)

t

, π(j)
t

)

f (x(j)
t

, π(2)

t,i ) − f (x(j)

t

, π(2)
t

)

+

f (x(j)
t

, π(2)

t,i ) − f (x(j)

t

, π(j)
t

)

(cid:125)

(cid:125)

T
(cid:88)

2
(cid:88)

t=1

j=1

T
(cid:88)

2
(cid:88)

j=1

t∈Ti
(cid:124)

T
(cid:88)

2
(cid:88)

j=1

t∈Ti
(cid:124)

=

(i)
=

T
(cid:88)

2
(cid:88)

(ii)
=

j=1

t=1
(cid:124)

(cid:123)(cid:122)
I0

(cid:123)(cid:122)
I (cid:48)
0

(cid:123)(cid:122)
IA

T
(cid:88)

2
(cid:88)

j=1

t∈Tc
i
(cid:124)

(cid:125)

T
(cid:88)

2
(cid:88)

j=1

t∈Tc
i
(cid:124)

T
(cid:88)

t∈Tc
i
(cid:124)

(cid:125)

(cid:125)

f (x(j)
t

, π(2)

t,i ) − f (x(j)

t

, π(2)
t

)

+

f (x(1)
t

, π(2)
t

) − f (x(1)

t

, π(1)
t

)

(cid:123)(cid:122)
IB

(cid:125)

Equality (i) holds because term I0 equals zero and therefore I0 = I (cid:48)

i , base i repeated
1. Equality (ii) follows from adding and subtracting term IB. We now focus

0 and in all steps t ∈ Tc

a policy of Type 2 so that I1 = I (cid:48)
on bounding E [IA] and E [IB].

C.1 Bounding E [IA]

Notice that:

E [IA] = E

2f (x(2)

t

, π(2)

t,i ) − 2f (x(2)

t

, π(2)
t

(cid:34) T

(cid:88)

t=1

We can easily bound this term using Lemma 13 from Agarwal et al. (2017). Indeed, in term IA, the policy
m=1 during any round t is chosen before the value of it is revealed. This
and 0 for all i (cid:54)= it are indeed unbiased estimators of the base algorithm’s rewards.

choice for all base algorithms { ˜Bm}M
ensures the estimates 2g(2)
t
pit
t

We conclude:

E [IA] ≤ O

(cid:18) M ln T
η

(cid:19)

+ T η

−

E [ρ∗]
40η ln T

C.2 Bounding E [IB]

Notice that:

(cid:123)(cid:122)
I1

(cid:123)(cid:122)
I (cid:48)
1

(cid:35)
)





E [IB] = E

f (x(1)
t

, π(2)
t

) − f (x(1)

t

, π(1)
t

)



= E

, π(2)
t

f (x(2)
t
(cid:124)

) − f (x(1)
(cid:123)(cid:122)
I (cid:48)
B

t

, π(1)

)
t

(cid:125)










(cid:88)

t∈Tc
i

(cid:88)

t∈Tc
i

16

In order to bound this term we will make an extra assumption. We avoided this discussion in the main to
avoid detracting the reader from the main discussion: our bound on term E [II].

Assumption A2 (Bounded rewards) We assume the norm of the rewards is bounded by 1. 3

Assumption A3 (Sub-optimal policy) The learner has access to a sub-optimal policy π with a known
lower bound in its sub-optimality gap4 of 1
2 .

We will show that under the right deﬁnition for Steps 2, term E [IB] ≤ 0. We will need to modify sightly the
deﬁnition of the policy played during steps 2. Instead of playing the "original" (Um, δ
M , T (2))-smooth policy
given by any base algorithm m, we play a mixture policy. If we denote by π(2)
t,m(0) the original smooth policy
t,m to be αtπ + (1 − αt)π(2)
of ˜Bm at round t, step 2, we now declare instead π(2)
t,m(0) for αt = min(1, 8Um(t,δ)
).
We also declare that π(2)
m,t be now this mixture policy instead. Crucially this is possible because the base
update is performed based on the feedback obtained from steps of type (1) so that this redeﬁnition does not
aﬀect the previous bound on E [IA].

t

Notice that under this new deﬁnition for π(2)

M , T (2))−smoothness then
M its conditional instantaneous regret satisﬁes both the following upper and

t,m, if an algorithm ˜B satisﬁes (Um, δ

with probability at least 1 − δ
lower bounds during all rounds t ∈ Tm:

(cid:32)

min

1,

1
2

8Um(t, δ
t

M )

(cid:33)

≤ Ext∼D

(cid:104)
r(2)
t,m|Ft−1

(cid:105)

≤

6Um(t, δ
t

M )

,

∀t ∈ Tm

(7)

Notice that by deﬁnition of this modiﬁed Step 2 policy and under Assumption A3, the lower bound on the
conditional expectation of the instantaneous conditional regret holds in Equation 7 holds. For convenience,
in the remainder of this section we will use this deﬁnition of (Um, δ
M , T (2))-smoothness. We revert back to its
usual deﬁnition in the later sections.

We now show Equation 7 implies that for any base algorithm m ∈ [M ] the cumulative regret of terms of
M , T (1))−boundedness.

Steps 2 upper bounds the cumulative regret of Steps 1 provided ˜Bm satisﬁes (Um, δ
We use these observations to bound term E [I (cid:48)

Let’s introduce a convenient notational deﬁnition, for all m ∈ [M ] let Tm(l) be the ﬁrst set of l rounds

B].

where il = m. It is easy to see that:
Proposition C.1. If ˜Bm is (Um, δ

2M , T (1))−bounded and (Um, δ

2M , T (2))−smooth with Um(t, δ

2M ) = tβg( δ

2M ):

(cid:34)

(cid:88)

E

(cid:35)

I (cid:48)
B

≤

t∈Tm

δ
M

T − 3E [Um(|Tm|, δ)|Em] +

Dm
2

−

Cm
2

.

Where Cm is the index of the ﬁrst round where 8Um(t, δ
M )

≤ 1 and Dm = (cid:80)Cm

t=1

8Um(t, δ
t

M )

.

t

Proof. We start by conditioning on the event given to us by the boundedness and smoothness assumptions,
let’s call it Em. On the complementary event E c
M ) we pay a linear regret of T .
In order to upper bound the expected regret on Em we make use of the observations above.

m (an event with probability δ

As a consequence of boundedness, for all l ≤ Tm conditioned on Em, (cid:80)

t∈Tm(l) f (x(1)

t

, π∗) − f (x(1)

t

, π(1)
t

) ≤

Um(l, δ

M ). Therefore,

(cid:34)

(cid:88)

E

t∈Tm

f (x(1)
t

, π∗) − f (x(1)

t

, π(1)
t

) ≤ Um

l,

|Em

≤ E

(cid:18)

(cid:19)

(cid:35)

δ
M

(cid:18)

(cid:20)
Um

|Tm|,

(cid:19)

(cid:21)

|Em

.

δ
M

On the other hand as a consequence of smoothness, conditioned on Em, the tower property implies

(cid:34)

(cid:88)

E

t∈Tm

f (x(2)
t

, π∗) − f (x(2)

t

, π(2)
t

)|Em

≥ E

|Tm|
(cid:88)





l=1

4Um(l, δ
l

M )



 −

Dm
2

+

Cm
2

.

3This can be relaxed to subgaussianity
4We can extend this to a sub-optimality gap ∆, at the cost of propagating this value throughout the remaining theorems.

(8)

(9)

(cid:35)

17

Since E

(cid:104)(cid:80)

t∈Tm

f (x(1)
t

(cid:105)
, π∗)

(cid:104)(cid:80)

= E

f (x(2)
t

, π∗)

t∈Tm

(cid:105)
. These inequalities 8 and 9, we conclude:

(cid:34)

(cid:88)

E

(cid:35)

I (cid:48)
B

t∈Tm

≤ E [Um(|Tm|, δ)|Em] − E

|Tm|
(cid:88)





l=1

4Um(l, δ)
l



 +

δ
M

T +

Dm
2

−

Cm
2

.

By Lemma B.1:

The result follows.

E [Um(|Tm|, δ)|Em] − E

|Tm|
(cid:88)





l=1

4Um(l, δ)
l


 ≤ −E [3Um(|Tm|, δ)|Em]

These guarantees have been derived under the assumption that base algorithm ˜Bm satisﬁes both the
boundedness and smoothness properties. Although we conditioned on these properties in the main, it should
be noted that these may not hold for base algorithms that are not "adapted" to the environment at hand.
Nevertheless in case a base algorithm violates boundedness of smoothness, there is no reason to keep said
base algorithm as an option for the master. We formalize how to use this intuition in what follows.

The following lemma can be used to show a high probability lower bound on the regret of steps 2 (under

mixture policy incorporating a known suboptimal arm):

Lemma C.2. Let Um(t, δ

M ) = tβg( δ

M ) with β ∈ [ 1

2 , 1]. For any l ∈ [T ], with probability at least 1 − τ :

(cid:88)

t∈Tm(l)

f (x(2)
t

, π∗) − f (x(2)

t

, π(2)
t

) ≥

l
(cid:88)

t=1

4Um(t, δ
t

M )

(cid:19)

(cid:18) δ
M

− 8g

Λ − Dm + Cm

Where Λ =

(cid:113)

(cid:113)






1

2β−1 lβ− 1

log( 1
τ )
log( 1
τ ) log(l)

2

if β > 1
2
if β = 1
2 .

.

Where Cm is the index of the ﬁrst round where 8Um(t, δ
M )

≤ 1 and Dm = (cid:80)Cm

t=1

8Um(t, δ
t

M )

.

t

Proof.

(cid:88)

t∈Tm(l)

f (x(2)
t

, π∗) − f (x(2)

t

, π(2)
t

) ≥ αt

f (x(2)
t

, π∗) − f (x(2)

, π)

t

(cid:88)

t∈Tm(l)

Notice that (cid:80)l

t=1 α2

t ≤ (cid:0)g( δ

M )(cid:1)2 (cid:80)l

t=1 t2β−2 ≤ (cid:0)g( δ

M )(cid:1)2 (cid:82) l

1 t2β−2dt ≤






M ))2
(g( δ
2β−1 l2β−1
M )(cid:1)2

if β > 1
2
log(l) β = 1
2 .

(cid:0)g( δ

.

By the Azuma-Hoeﬀding inequality5, (we use the fact αt|f (x, π∗) − f (x, π)| ≤ 2αt ) with probability 1 − τ :

(cid:88)

αt

t∈Tm(l)

f (x(2)
t

, π∗) − f (x(2)

, π) ≥

t

min(1,

1
2

l
(cid:88)

t=1

8Um(t, δ)
t

) − 8g

(cid:19)

Λ

(cid:18) δ
M

=

l
(cid:88)

t=1

4Um(t, δ)
t

− Dm + Cm − 8g

(cid:19)

Λ

(cid:18) δ
M

5We use the following version of Azuma-Hoeﬀding: If Xn, n ≥ 1 is a martingale such that |Xi − Xi−1| ≤ di, 1 ≤ i ≤ n then

for every t > 0, P(Xn > r) ≤ exp

(cid:18)

−

r2
2 (cid:80)n
i=1 d2
i

(cid:19)

18

Where Λ =

(cid:113)

(cid:113)






1

2β−1 lβ− 1

log( 1
τ )
log( 1
τ ) log(l)

2

if β > 1
2
if β = 1
2 .

We can specialize the bound in Lemma C.2 to

Proposition C.3. Let Um(t, δ

M ) = tβg( δ

M ) with β ∈ [ 1

2 , 1]. For any l ∈ [T ] with probability at least 1 − τ :

f (x(2)
t

, π∗) − f (x(2)

t

, π(2)
t

) ≥ 3Um

l,

− Dm + Cm − Lm

(cid:18)

(cid:19)

δ
M

(cid:88)

t∈Tm(l)

M ) for lm the ﬁrst index such that such that U (l, δ

M ) ≥ Λ.

Where Lm = (cid:80)lm

For Λ(l) =

r=1 Λ(r) − U (r, δ
1
log( 1
τ )
log( 1
τ ) log(l)

2β−1 lβ− 1

2

(cid:113)

(cid:113)






if β > 1
2
if β = 1
2 .

.

And Cm is the index of the ﬁrst round where 8Um(t, δ
M )

≤ 1 and Dm = (cid:80)Cm

t=1

8Um(t, δ
t

M )

.

t

Proof. By Lemma B.1, (cid:80)l

4Um(t, δ
t

M )

t=1

≥ 4Um(l, δ

M ). And it is easy to see that for all l suﬃciently large:



2β−1 lβ− 1

(cid:113)

2

Um(l,

) ≥ 8g

δ
M

(cid:19)

(cid:18) δ
M

1

log( 1
τ )
log( 1
τ ) log(l)

(cid:113)



if β > 1
2
if β = 1
2 .

The result follows.

Again making use of Martingale Concentration bounds we can show:

Remark C.4. For any l ∈ [T ] with probability at least 1 − τ we have that (cid:80)
(cid:113)

t∈Tm(l) f (x(2)

t

, π∗) − f (x(1)

, π∗) <

t

2 log( 1

τ )l.

m such that Um(l, δ

We assume there is an l(cid:48)

M ) = g(δ)tβ for β ≥ 1

consider Um(l, δ
g. We will subsequently show that g(δ) will be required to be of the form (cid:0)log (cid:0) 1
deﬁne Gm = (cid:80)l(cid:48)
m ).

τ )l − Um(r, δ

2 log( 1

M ) ≥

τ )l for all l ≥ l(cid:48)
m. This is possible since we
2 and can be achieved by modulating the leading constants included in
for some γ ≥ 0. Let s

2 log( 1

m
r=1

(cid:1)(cid:1)γ

(cid:113)

δ

As a consequence of remark C.4, and proposition C.3, with probability at least 1 − δ

M :

f (x(1)
t

, π∗) − f (x(2)

t

, π(2)
t

) ≥ 3U

l,

−

2 log

l − Dm + Cm − Lm

(cid:18)

(cid:19)

(cid:115)

(cid:19)

(cid:18) 1
τ

(cid:88)

t∈Tm(l)

≥ 2U (l,

) − Dm + Cm − Lm − Gm.

If ˜Bm satisﬁes (Um, δ

M , T (1))−boundedness, it must hold that, conditioned on Em:

f (x(1)
t

, π∗) − f (x(1)

t

, π(1)
t

) ≤ U

l,

(cid:18)

(cid:19)

δ
M

(cid:88)

t∈Tm(l)

(10)

(11)

As a consequence of Equations 10 and 11 we conclude that with probability at least 1 − δ
l ∈ [T ]:

M − 2T τ , for all

(cid:113)

δ
M

δ
M

19

(cid:88)

f (x(1)
t

, π∗) − f (x(1)

t

, π(1)
t

) ≤ U (l,

δ
M

)

− Dm + Cm − Lm − Gm ≤

f (x(1)
t

, π∗) − f (x(2)

t

, π(2)
t

)

(cid:18)

t∈Tm(l)
(cid:19)
δ
M

l,

2U

l
(cid:88)

t∈Tm(l)

(cid:88)

t∈Tm(l)

(cid:18)

(cid:19)

δ
M

And therefore for all l ∈ [T ] with probability 1 − δ

M − 2T τ :

f (x(1)
t

, π(1)
t

) − f (x(2)

t

, π(2)
t

) ≥ U

t,

− Dm + Cm − Lm − Gm

(12)

Equation 12 holds with probability 1 − δ

M − 2T τ as long as (U, δ
M , T (1))−boundedness holds. Furthermore,
notice that whenever this event holds, E [IB] ≤ T δ + 2T 2τ + Dm − Cm + Lm + Gm, thus preserving our regret
guarantees. Crucially notice that all terms Dm, Cm, Lm, Gm are constant (possibly dependent on U (t, δ
M ),
but independent on T ). Since boundedness is only true for algorithms ˜Bm that are "adapted" to the current
environment, we introduce a small change to make sure the "domination" property between rewards of steps
1 and steps 2 holds for all times so that the bound on the expectation of term IB remains valid.

1. For all base algorithms { ˜Bm}M

m=1, if it = m, suppose algorithm m has been selected l-times by the
master, and whenever Equation 12 starts failing for base algorithm ˜Bm, we "drop" this base algorithm
by declaring the rewards of subsequent steps 2 in subsequent rounds to be 0 before sending this feedback
into the master and repeating step 1 twice, alternatively we can send a zero feedback into the master
and execute the policy supported on the remaining algorithms whenever a "dropped" base is selected.

If this extra step 1 is executed for algorithm ˜Bm, we know that (with high probability) (U, δ, T (1))−boundedness

was violated, and therefore that algorithm ˜Bm couldn’t have possibly been optimal for the environment at
hand.

After all these arguments we can conclude that:

E [I] ≤ O

(cid:18) M ln T
η

(cid:19)

+ T η

−

E [ρ∗]
40η ln T

−

Up to a δT + 2T τ + Em, (where Em is a constant) additive factor.

D Proof of Lemma 4.2

Proof of Lemma 4.2. Recall E [II] ≤ E
E (cid:2)Ui(δ, ni
(cid:80)t

l=1 ul ≥ Ui(t, δ/M ) for all t, and so,

T )1(E)(cid:3) while the second term satisﬁes the bound in (5). Let ul = Ui(l,δ/2M )

l

. By Lemma B.1,

(cid:104)

R(1)
i

(Ti)1(E) + I01{E}

(cid:105)

+ δT . The ﬁrst term is bounded by

E (cid:2)1 {E} Ui(δ, ni

T )(cid:3) ≤ E

1 {E} ul

 .



ni
T +1
(cid:88)





l=1

(13)

By (5) and (13),

(cid:104)

E

R(1)
i

(Ti)1(E) + I01{E}

≤ E

(cid:105)

ni
T +1
(cid:88)





l=1



1{E}2blul

 .

Let al = E[bl] for all l. Consider a master algorithm that uses p instead of pi
the corresponding rounds when the base is selected, ¯ni

l be
T be the total number of rounds the base is selected,

t. In this new process let t(cid:48)

20

l−1

l − t(cid:48)

(cid:3). Since p ≤ pi

and cl = E (cid:2)t(cid:48)
t for all t it holds that (cid:80)j
coin ﬂips used to generate tl to generate t(cid:48)
l, we observe that t(cid:48)
decreasing function such that for integer i, f (i) = ui. Then (cid:80)ni
integral (cid:82) T

l ⊂ tl and ul is a decreasing sequence in l,

l=1 al ≤ (cid:80)j
l ⊂ tl and ¯ni
l=1 alul and (cid:80)¯ni
T +1

l=1 cl for all j. If we use the same
T . Let f : R → [0, 1] be a
clul are two estimates of

T ≤ ni
T +1

l=1

0 f (x)dx. Given that t(cid:48)
ni
T +1
(cid:88)

l=1

and thus

E [tl − tl−1] ul ≤

E (cid:2)t(cid:48)

l − t(cid:48)

l−1

(cid:3) ul ,

¯ni
T +1
(cid:88)

l=1

¯ni
T +1
(cid:88)

l=1

(cid:104)

E

R(1)
i

(Ti)1(E) + I01{E}

≤ E

(cid:105)

2E (cid:2)t(cid:48)

l − t(cid:48)

l−1

(cid:3) ul .

We proceed to upper bound the right hand side of this inequality:




E



¯ni
T +1
(cid:88)

l=1

ulE (cid:2)t(cid:48)

l − t(cid:48)

l−1


(cid:3)
 ≤ E



¯ni
T +1
(cid:88)





ul
p

l=1

The ﬁrst inequality holds because E (cid:2)t(cid:48)
as a function of t. The proof follows.

l − t(cid:48)

l−1

(cid:3) ≤ 1

≤ 2ρUi(T /ρ, δ) log(T ).

p and the second inequality follows by concavity of Ui(t, δ)

E Proof of Lemma 4.6

We restate Lemma 4.6 for readability.

Lemma E.1. If B is (U, δ, [T ])−bounded, maxx,π |f (x, π)| ≤ 1, U (t, δ) > 8
expected replay regret R(t, h) satisﬁes:

(cid:113)

t log( t2

δ ), and δ ≤ 1√

T

, then B’s

Proof. Consider the following two martingale sequences:

R(t, h) ≤ 4U (t, δ) + 2δt ≤ 5U (t, δ)

{M 1
l
{M 2
l

:= f (xl, π∗) − f (x(cid:48)(cid:48)
:= f (x(cid:48)(cid:48)

l , π∗)}t
l , πt) − f (xl, πt)}t

l=1

l=1

Since max (cid:0)|M 1

l |, |M 2

l |(cid:1) ≤ 2 for all t, a simple use of Azuma-Hoeﬀding yields:
(cid:18) 8t2
δ

≥ U (t, δ)

8t log

≤ P

M i
l

M i
l

(cid:88)

(cid:115)

(cid:33)

≥

P

(cid:32)(cid:12)
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

l

(cid:19)(cid:33)

l
(cid:32)

(cid:33)

8t log( 8t2
δ )
8t

≤ 2 exp

−

=

δ
4t2 .

Summing over all t, and all i ∈ {1, 2} and applying the union bound, using the fact that (cid:80)T
that for all t, with probability 1 − δ,

t=1

1
t2 < 2 implies

(cid:12)
(cid:12)
(cid:12)

(cid:32) t

(cid:88)

l=1

(cid:124)

t
(cid:88)

l=1

(cid:123)(cid:122)
I

f (xl, π∗) −

f (xl, πl)

−

f (x(cid:48)(cid:48)

l , π∗) −

f (x(cid:48)(cid:48)

l , πl)

t
(cid:88)

l=1

(cid:123)(cid:122)
II

(cid:12)
(cid:12)
(cid:12) ≤ 2U (t, 2δ) .

(cid:33)

(cid:125)

(cid:33)

(cid:32) t

(cid:88)

l=1

(cid:125)

(cid:124)

21

Since with probability 1 − δ term I is upper bounded by U (t, δ) for all t a simple union bound implies that
(cid:1) ≤ 4U (t, δ) for all t. The replay
with probability 1 − 2δ term II is upper bounded by U (t, δ) + 2
expected regret R(t, h) can be upper bounded by: (1 − 2δ)4U (t, δ) + 2δt ≤ 4U (t, δ) + 2δt. The result follows.

8t log (cid:0) 8t2

(cid:113)

δ

F Proof of Theorem 4.3

Proof of Theorem 4.3. The proof follows that of Theorem 15 in (Agarwal et al., 2017). Let (cid:96)1, · · · , (cid:96)di < T be
the rounds where Line 10 of the CORRAL is executed. Let (cid:96)0 = 0 and (cid:96)di+1 = T for notational convenience.
Let el = [(cid:96)l−1 + 1, · · · , (cid:96)l]. Denote by p
the probability lower bound maintained by Corral during timesteps
t ∈ [(cid:96)l−1, · · · , (cid:96)l] and ρ(cid:96)l = 1/p
. In the proof of Lemma 13 in (Agarwal et al., 2017), the authors prove
di ≤ log(T ) with probability one. Therefore,

(cid:96)l

(cid:96)l

E [II] =

(cid:100)log(T )(cid:101)
(cid:88)

l=1

(cid:104)

)E

P(di + 1 ≥ l
(cid:123)(cid:122)
(cid:125)
I(l)

(cid:124)

R(1)
i

(el) + R(2)

i

(el)|di + 1 ≥ l

(cid:105)

(cid:100)log(T )(cid:101)
(cid:88)

≤ log T

P(I(l))E [4ρ(cid:96)l Ui(T /ρ(cid:96)l , δ)|I(l)] + δT (log T + 1)

= log T E

4ρ(cid:96)l Ui(T /ρ(cid:96)l , δ)

+ δT (log T + 1).

(cid:35)

l=1
(cid:34)bi+1
(cid:88)

l=1

The inequality is a consequence of Lemma 4.2 applied to the restarted segment [(cid:96)l−1, · · · , (cid:96)l]. This step is
valid because by assumption 1
ρ(cid:96)l

≤ mint∈[(cid:96)l−1,··· ,(cid:96)l] pt.

If Ui(t, δ) = tαg(δ) for some function g : R → R+, then ρU (T /ρ, δ) = ρ1−αT αg(δ). And therefore:
(cid:35)

(cid:35)

(cid:34)bi+1
(cid:88)

E

l=1

ρ(cid:96)lUi(T /ρ(cid:96)l , δ)

≤ T αg(δ)E

(cid:34)bi+1
(cid:88)

l=1

ρ1−α
(cid:96)l

≤

2 ¯α
2 ¯α − 1

T αg(δ)E (cid:2)ρ1−α

(cid:3)

∗

Where ¯α = 1 − α. The last inequality follows from the same argument as in Theorem 15 in (Agarwal et al.,
2017).

G Proof of Corollary 4.5

Proof of Corollary 4.5. By Theorem 4.4,

R(T ) ≤ O

(cid:18) M ln T
η

(cid:19)

+ T η

− E

(cid:20)

ρ
40η ln T

≤ O

(cid:18) M ln T
η

(cid:19)

+ T η

− E

(cid:20)

ρ
40η ln T

− 2ρ U (T /ρ, δ) log T

+ δT

(cid:21)

(cid:21)

− 2ρ1−αT αg(δ) log T

+ δT

≤ ˜O

(cid:18) M
η

+ T η + T g(δ)

1
α η

1−α
α

+ δT,

(cid:19)

22

where the last step is by maximizing the function over ρ. Substituting η = M α

g(δ)T α ﬁnishes the proof.

H High probability regret bounds for (cid:15)-greedy

In this section we show that epsilon greedy satisﬁes a high probability regret bound. We adapt the notation
to this setup. Let µ1, · · · , µK be the unknown means of the K arms. Recall that at time t the epsilon Greedy
algorithm selects with probability (cid:15)t = min(c/t, 1) an arm uniformly at random, and with probability 1 − (cid:15)t it
selects the arm whose empirical estimate of the mean is largest so far. Let ˆµ(t)
j denote the empirical estimate
of the mean of arm j after using t samples.

Without loss of generality let µ1 be the optimal arm. We denote the gaps as ∆j = µ1 − µj for all j. Let
∆∗ be the smallest nonzero gap. We follow the discussion in (Auer et al., 2002) and start by showing that
under the right assumptions, and for a horizon of size T , the algorithm satisﬁes a high probability regret
bound for all t ≤ T . The objective of this section is to prove the following Lemma:
Lemma H.1. If c = 10K log( 1
δ )
30K log( 1
δ )
∆2
∗

t is (δ, U, T )−stable for δ ≤ ∆2

6, then (cid:15)−greedy with (cid:15)t = c

∆2
∗
(cid:17)
log(t + 1).

T 3 and U (t, δ) =

(cid:16)(cid:80)K

+ ∆j

∆j
∆2
∗

j=2

∗

Proof. Let E(t) = 1
l=1 (cid:15)l and denote by Tj(t) the random variable denoting the number of times arm j
2K
was selected up to time t. We start by analyzing the probability that a suboptimal arm j > 1 is selected at
time t:

(cid:80)t

P(j is selected at time t) ≤

(cid:15)t
K

(cid:16)

+

1 −

(cid:17)

P

(cid:16)

(cid:15)t
K

ˆµ(Tj (t))
j

≥ ˆµ(T1(t))
1

(cid:17)

(14)

Let’s bound the second term.

(cid:16)

P

ˆµ(Tj (t))
j

≥ ˆµ(T1(t))
1

(cid:18)

(cid:17)

≤ P

ˆµ(Tj (t))
j

≥ µj +

+ P

ˆµ(T1(t))
1

≤ µ1 −

(cid:19)

(cid:18)

∆j
2

(cid:19)

∆j
2

The analysis of these two terms is the same. Denote by T R

j (t) the number of times arm j was played as a

result of a random epsilon greedy move. We have:

(cid:18)

P

ˆµ(Tj (t))
j

≥ µj +

(cid:19)

(cid:18)

t
(cid:88)

P

=

∆j
2

Tj(t) = l and ˆµ(l)

j ≥ µj +

Tj(t) = l|ˆµ(l)

j ≥ µj +

ˆµ(l)
j ≥ µj +

(cid:19)

∆j
2

Tj(t) = l|ˆµ(l)

j ≥ µj +

exp(−∆2

j t/2)

Tj(t) = l|ˆµ(l)

j ≥ µj +

exp(−∆2

j (cid:98)E(t)(cid:99)/2)

j (t) = l|ˆµ(l)
T R

j ≥ µj +

exp(−∆2

j (cid:98)E(t)(cid:99)/2)

(cid:19)

∆j
2

(cid:19)

(cid:18)

P

∆j
2

∆j
2

(cid:19)

(cid:19)

+

∆j
2

2
∆2
j

(cid:19)

+

∆j
2

2
∆2
j

l=1
t
(cid:88)

l=1
t
(cid:88)

l=1

(cid:18)

P

(cid:18)

P

(cid:98)E(t)(cid:99)
(cid:88)

(cid:18)

P

l=1

(cid:98)E(t)(cid:99)
(cid:88)

(cid:18)

P

l=1

=

I
≤

II
≤

≤

≤ (cid:98)E(t)(cid:99)P (cid:0)Tj(t)R ≤ (cid:98)E(t)(cid:99)(cid:1)
(cid:123)(cid:122)
(cid:125)
(1)

(cid:124)

+

2
∆2
j
(cid:124)

exp(−∆2

j (cid:98)E(t)(cid:99)/2)

(cid:123)(cid:122)
(2)

(cid:125)

Inequality I is a consequence of a Chernoﬀ bound. Inequality II follows because (cid:80)∞

l=E+1 exp(−αl) ≤
1
a exp(−αE). Term (1) corresponds to the probability that within the interval [1, · · · , t], the number of greedy
pulls to arm j is at most half its expectation. Term (2) is already "small".

6This choice of c is robust to multiplication by a constant.

23

Recall (cid:15)t = min(c/t, 1). Let c = 10K log(T 3/γ)

∆2
∗

assumptions we can lower bound E(t): Indeed if t ≥ 10K log(T 3/γ)

:

∆2
∗

for some γ ∈ (0, 1) satisfying γ ≤ ∆2

j . Under these

1
2K

t
(cid:88)

l=1

(cid:15)l =

5 log(T 3/γ)
∆2
∗

5 log(T 3/δ)
∆2
∗

t
(cid:88)

1
l

l=log(T 3/γ)

+

+

≥

≥

5 log(T 3/γ)
∆2
∗
5 log(T 3/γ)
∆2
∗

5 log(T 3/γ) log(t)
2∆2
∗

By Bernstein’s inequality (see derivation of equation (13) in (Auer et al., 2002)) it is possible to show that :

P (cid:0)T R

j (t) ≤ E(t)(cid:1) ≤ exp (−E(t)/5)

(15)

Hence for t ≥ 10K log(T 3/γ)

:

∆2
∗

And therefore since E(t) ≤ T and 1
∆∗

≥ 1 we can upper bound (1) as:

P (cid:0)T R

j (t) ≤ E(t)(cid:1) ≤

(cid:17) 1
∆2
∗

(cid:16) γ
T 3

Now we proceed with term (2):

(cid:98)E(t)(cid:99)P (cid:0)Tj(t)R ≤ (cid:98)E(t)(cid:99)(cid:1) ≤

(cid:17) 1
∆2

∗ ≤

(cid:16) γ
T 2

γ
T 2

2
∆2
j

exp (cid:0)−∆2

j (cid:98)E(t)(cid:99)/2(cid:1) ≤

exp

−5K log(

(cid:33)

T 3
γ

)

∆2
j
∆2
∗
(cid:19)

T 3
γ

)

(cid:32)

(cid:18)

exp

−5K log(

(cid:17)5K

(cid:16) γ
T 3

By the assumption γ ≤ ∆2
The previous discussion implies that for c = 10K log(T 3/γ)

j the last term is upper bounded by γ
T 3 .

, the probability of choosing a suboptimal arm

j ≥ 2 at time t for t ≥ 10K log(T 3/γ)
t ≥ 10K log(T 3/γ)
uniformly random epsilon greedy action.

∆2
∗

∆2
∗

, suboptimal arms with probability 1 − 1

as a greedy choice is upper bounded by 2 γ

T . In other words after
T over all t are only chosen as a result of a exploration

A similar argument as the one that gave us Equation 15 can be used to upper bound the probability that

at a round t, Tj(t)R be much larger than its mean:

P (cid:0)T R

j (t) ≥ 3E(j)(cid:1) ≤ exp(−E(t)/5)

We can conclude that with probability more than 1 − Kγ
j (t) ≤ 3E(t). Combining
this with the obsevation that after t ≥ 10K log(T 3/γ)
T over all t simultaneously (by
union bound) regret is only incurred by random exploration pulls (and not greedy actions), we can conclude
that with probability 1 − 2Kγ
the regret incurred is upper bounded
T
by:

simultaneously for all t ≥ 10K log(T 3/γ)

T and for all t and arms j, T R

and with probability 1 − Kγ

∆2
∗

∆2
∗

≤

=

2
∆2
j

2
∆2
j
2
∆2
j

∆2
∗

24

10K log(T 3/γ)
∆2
∗

·

1
K

K
(cid:88)

j=2

(cid:124)

(cid:123)(cid:122)
I

∆j

+ 3E(t)

∆j

(cid:125)

(cid:124)

(cid:125)

K
(cid:88)

j=2
(cid:123)(cid:122)
II

Where I is a crude upper bound on the regret incurred in the ﬁrst 10K log(T 3/γ)
bound for the regret incurred in the subsequent rounds.

∆2
∗

rounds and II is an upper

Since E(t) ≤ 20K log(T 3/γ)

∆2
∗

regret of epsilon greedy is upper bounded by f (t) = 30K log(T 3/γ)
follows by identifying δ = γ/T 3.

log(t) we can conclude that with probability 1− 2Kγ
T
+ ∆j

(cid:16)(cid:80)K

for all t ≤ T the cumulative
(cid:17)

max(log(t), 1), the result

∆j
∆2
∗

j=2

We now show the proof of Lemma 4.11 the instance-independent regret bound for (cid:15)-greedy:

Lemma H.2 (Lemma 4.11). If c = 10K log( 1
δ )

, then (cid:15)−greedy with (cid:15)t = c

t is (δ, U, T )−stable for δ ≤ ∆2

T 3 and:

∗

∆2
∗

1. U (t, δ) = 16

log( 1

δ )t when K = 2.

2. U (t, δ) = 20

K log( 1
δ )

(cid:16)(cid:80)K

j=2 ∆j

(cid:17)(cid:17)1/3

t2/3 when K > 2.

(cid:113)

(cid:16)

Proof. Let ∆ be some arbitrary gap value. Let R(t) denote the expected regret up to round t. We recycle
the notation from the proof of Lemma H.1, recall δ = γ/T 3.

R(t) =

∆jE [Tj(t)] +

∆jE [Tj(t)]

(cid:88)

∆j ≤∆

(cid:88)

∆j ≥∆

≤ ∆t +

∆jE [Tj(t)]

(cid:88)

∆j ≥∆

≤ ∆t + 30K log(T 3/γ)

≤ ∆t + 30K log(T 3/γ)









K
(cid:88)

∆j ≥∆

K
(cid:88)

∆j ≥∆

∆j
∆2
∗

∆j
∆2
∗



+ ∆j

 log(t)

When K = 2, ∆2 = ∆∗ and therefore (assuming ∆ < ∆2):


 + 30K log(T 3/γ) log(t)





K
(cid:88)



∆j



∆j ≥∆

(16)

R(t) ≤ ∆t +

+ 30K log(T 3/γ) log(t)∆2

30K log(T 3/γ)
∆2
30K log(T 3/γ)
∆

≤ ∆t +

+ 30K log(T 3/γ) log(t)∆2

I

≤

≤ (cid:112)30K log(T 3/γ)t + 30K log(T 3/γ) log(t)∆2
II 8(cid:112)K log(T 3/γ)t
≤ 16(cid:112)log(T 3/γ)t

Inequality I follows from setting ∆ to the optimizer, which equals ∆ =
II is satisﬁed for T large enough. We choose this expression for simplicity of exposition.

. The second inequality

(cid:113) 30K log(T 3/γ)
t

When K > 2 notice that we can arrive to a bound similar to 16:

25

R(t) ≤ ∆t + 30K log(T 3/γ)





K
(cid:88)

∆j ≥∆

∆j
∆2


 + 30K log(T 3/γ) log(t)





K
(cid:88)



∆j



∆j ≥∆

Where ∆∗ is substituted by ∆. This can be obtained from Lemma H.1 by simply substituting ∆∗ with ∆

in the argument for arms j : ∆j ≥ ∆.

We upper bound (cid:80)

(cid:18) 30K log(T 3/γ)((cid:80)K

j=2 ∆j)

∆j ≥∆ ∆j by (cid:80)K
(cid:19)1/3

t

j=2 ∆j. Setting ∆ to the optimizer of the expression yields ∆ =

, and plugging this back into the equation we obtain:


30K log(T 3/γ)





R(t) ≤ 2





1/3

∆j





t2/3 + 30K log(T 3/γ) log(t)



∆j







K
(cid:88)

j=2


K log(T 3/γ)





ξ
≤ 20





1/3

∆j





t2/3

K
(cid:88)

j=2

K
(cid:88)

j=2

The inequality ξ is true for T large enough. We choose this expression for simplicity of exposition.

I High Probability Regret Bound for UCB

Lemma I.1 (Lemma 4.9). Assuming that the noise ξt is conditionally 1-sub-Gaussian, UCB is (U, δ, [T ])-
bounded with U (t, δ) = O(

tk log tk

√

δ ).

Proof. The regret of UCB is bounded as (cid:80)
(Theorem 7 of Abbasi-Yadkori et al.
(2011)) where ∆i is the gap between arm i and the best arm. By substituting the worst-case ∆i in the regret
bound, U (T, δ) = O(

3∆i + 16
∆i

T k log T k

log 2k
∆iδ

i:∆i>0

√

(cid:16)

(cid:17)

δ ).

J Section 6 Proofs.

J.1 Proof of Theorem 6.3

From Corollary 4.11, we lower bound the smallest gap by 1/T (because the gaps smaller than 1/T will cause
constant regret in T time steps) and choose δ = 1/T 5. Using an appropriate η as discussed in Corollary 4.5
will result in the regret of the same order ˜O(T 2/3) when K > 2 and ˜O(T 1/2) when K = 2 with the base
running alone.

Next we show that the best value of c in the exponential grid gives a regret that is within a constant
factor of the regret above where we known the smallest non-zero gap ∆∗. An exploration rates can be at most
KT . Since 5K
> 1, we need to search only in the interval [1, KT ]. Let c1 be the element in the exponential
∆2
∗
grid such that c1 ≤ c∗ ≤ 2c1. Then 2c1 = γc∗ where γ < 2 is a constant, and therefore using 2c1 = γc∗ will
give a regret up to a constant factor of the optimal regret.

J.2 Proof of Theorem 6.2

Proof. Using Corollary 4.5, we obtain the regret of stochastic CORRAL with the smooth versions of UCB
δ ). Therefore from Corollary 4.5, running
and LinUCB. From Lemma 4.9, for UCB, U (T, δ) = O(

T k log T k

√

26

stochastic CORRAL with smooth UCB results in the following regret bound:

(cid:32)

O

2 ln T
η

(cid:18)√

+ T η + T

k log

η

+ δT.

(cid:19)2

(cid:33)

T k
δ

(cid:16) 2

η + T kη

(cid:17)

.

If we choose δ = 1/T and hide some log factors, we get ˜O

√

Similarly, for LinUCB, U (t, δ) = O(d

t log(1/δ)) and running stochastic CORRAL with smooth LinUCB

(cid:16) 2

results in ˜O

η + T d2η
By the choice of η =

(cid:17)

regret bound.
(cid:113) 2
√
T d

k

be bounded as

, the regret of stochastic CORRAL with the smooth UCB and LinUCB will

(cid:32)

(cid:40)√

(cid:18)

˜O

max

2T

d0.5k0.25 +

√

(cid:18)

2T

d0.5k0.25 +

d1.5
k0.25

(cid:19)

k0.75
d0.5
(cid:19) (cid:41)(cid:33)

,

.

K Proof of Lower Bound

√

Proof of Theorem 5.1. Consider a stochastic 2-arm bandit problem where the best arm has expected reward
1/2 and the second best arm has expected reward 1/4. We construct base algorithms B1, B2 as follows. B1
always chooses the optimal arm and its expected instantaneous reward is 1/2. B2 chooses the second best arm
(c will be speciﬁed later), and chooses the best arm otherwise.
at time step t with probability
The expected reward at time step t of B2 is 1
2 −

c
t+2 log(t+2)
Let A∗ be uniformly sampled from {1, 2}. Consider two environments ν1 and ν2 for the master, each
made up of two base algorithms ˜B1, ˜B2. Under ν1, ˜B1 and ˜B2 are both instantiations of B1. Under ν2, ˜BA∗ ,
where A∗ is a uniformly sampled index in {1, 2}, is a copy of B1 and ˜B3−A∗ is a copy of B2.

4c
t+2 log(t+2)

Let P1, P2 denote the probability measures induced by interaction of the master with ν1 and ν2 respectively.
Let ˜BAt denote the base algorithm chosen by the master at time t. We have P1(At (cid:54)= A∗) = 1
2 for all t, since
the learner has no information available to identify which algorithm is considered optimal. By Pinskers’
inequality we have

√

.

By the divergence decomposition (see Lattimore and Szepesvari, proof of Lemma 15.1 for the decomposition
technique) and using that for ∆ < 1

2 − ∆) ≤ 3∆2 (Lemma B.3), we have

4 : kl( 1

2 , 1

P2(At (cid:54)= A∗) ≥ P1(At (cid:54)= A∗) −

KL(P1||P2)

(cid:114) 1
2

KL(P1||P2) =

1
2

kl

(cid:18) 1
2

,

1
2

−

√

c
t + 1 log(t + 1)

(cid:19)

∞
(cid:88)

t=2
∞
(cid:88)

t=2

≤

3c2

2t log(t)2 ≤ 3c2 .

27

Picking c =

24 leads to P2(At (cid:54)= A∗) ≥ 1

4 , and the regret in environment ν2 is lower bounded by

(cid:113) 1

R(T ) ≥

P2(At (cid:54)= A∗)

√

T
(cid:88)

t=1

c
t + 1 log(t + 1)
√

≥

c
4 log(T + 1)

T
(cid:88)

t=1

√

1
t + 1

= Ω(

T
log(T )

) .

28

