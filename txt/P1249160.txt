Gated-Attention Architectures for Task-Oriented Language Grounding

Devendra Singh Chaplot
chaplot@cs.cmu.edu
School of Computer Science
Carnegie Mellon University

Kanthashree Mysore Sathyendra∗
ksathyen@cs.cmu.edu
School of Computer Science
Carnegie Mellon University

Rama Kumar Pasumarthi∗
rpasumar@cs.cmu.edu
School of Computer Science
Carnegie Mellon University

Dheeraj Rajagopal∗
dheeraj@cs.cmu.edu
School of Computer Science
Carnegie Mellon University

Ruslan Salakhutdinov
rsalakhu@cs.cmu.edu
School of Computer Science
Carnegie Mellon University

8
1
0
2
 
n
a
J
 
9
 
 
]

G
L
.
s
c
[
 
 
2
v
0
3
2
7
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

To perform tasks speciﬁed by natural language instructions,
autonomous agents need to extract semantically meaning-
ful representations of language and map it to visual ele-
ments and actions in the environment. This problem is called
task-oriented language grounding. We propose an end-to-
end trainable neural architecture for task-oriented language
grounding in 3D environments which assumes no prior lin-
guistic or perceptual knowledge and requires only raw pixels
from the environment and the natural language instruction as
input. The proposed model combines the image and text rep-
resentations using a Gated-Attention mechanism and learns a
policy to execute the natural language instruction using stan-
dard reinforcement and imitation learning methods. We show
the effectiveness of the proposed model on unseen instruc-
tions as well as unseen maps, both quantitatively and qual-
itatively. We also introduce a novel environment based on a
3D game engine to simulate the challenges of task-oriented
language grounding over a rich set of instructions and envi-
ronment states.

1

Introduction

Artiﬁcial Intelligence (AI) systems are expected to perceive
the environment and take actions to perform a certain task
(Russell and Norvig 1995). Task-oriented language ground-
ing refers to the process of extracting semantically mean-
ingful representations of language by mapping it to visual
elements and actions in the environment in order to perform
the task speciﬁed by the instruction.

Consider the scenario shown in Figure 1, where an agent
takes natural language instruction and pixel-level visual in-
formation as input to carry out the task in the real world. To
accomplish this goal, the agent has to draw semantic cor-
respondences between the visual and verbal modalities and
learn a policy to perform the task. This problem poses sev-
eral challenges: the agent has to learn to recognize objects
in raw pixel input, explore the environment as the objects
might be occluded or outside the ﬁeld-of-view of the agent,
ground each concept of the instruction in visual elements
or actions in the environment, reason about the pragmatics
of language based on the objects in the current environment
(for example instructions with superlative tokens, such as

Figure 1: An example of task-oriented language grounding in the
3D Doom environment with sample instructions. The test set con-
sists of unseen instructions.

‘Go to the largest object’) and navigate to the correct object
while avoiding incorrect ones.

To tackle this problem, we propose an architecture that
comprises of a state processing module that creates a joint
representation of the instruction and the images observed by
the agent, and a policy learner to predict the optimal ac-
tion the agent has to take in that timestep. The state pro-
cessing module consists of a novel Gated-Attention multi-
modal fusion mechanism, which is based on multiplicative
interactions between both modalities (Dhingra et al. 2017;
Wu et al. 2016).

The contributions of this paper are summarized as fol-
lows: 1) We propose an end-to-end trainable architecture
that handles raw pixel-based input for task-oriented lan-
guage grounding in a 3D environment and assumes no prior
linguistic or perceptual knowledge1. We show that the pro-
posed model generalizes well to unseen instructions as well
as unseen maps2. 2) We develop a novel Gated-Attention
mechanism for multimodal fusion of representations of ver-
bal and visual modalities. We show that the gated-attention
mechanism outperforms the baseline method of concatenat-
ing the representations using various policy learning meth-
ods. The visualization of the attention weights in the gated-
attention unit shows that the model learns to associate at-
tributes of the object mentioned in the instruction with the
visual representations learned by the model. 3) We intro-

∗Equal contribution

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

1The environment and code is available at https://

github.com/devendrachaplot/DeepRL-Grounding

2See demo videos at https://goo.gl/rPWlMy

Figure 2: The proposed model architecture to estimate the policy given the natural language instruction and the image showing the ﬁrst-person
view of the environment.

duce a new environment, built over ViZDoom (Kempka et
al. 2016), for task-oriented language grounding with a rich
set of actions, objects and their attributes. The environment
provides a ﬁrst-person view of the world state, and allows for
simulating complex scenarios for tasks such as navigation.

2 Related Work
Grounding Language in Robotics. In the context of
grounding language in objects and their attributes, Guadar-
rama et al. (2014) present a method to ground open vocab-
ulary to objects in the environment. Several works look at
grounding concepts through human-robot interaction (Chao,
Cakmak, and Thomaz 2011; Lemaignan et al. 2012). Other
works in grounding include attempts to ground natural
language instructions in haptic signals (Chu et al. 2013)
and teaching robot to ground natural language using ac-
tive learning (Kulick et al. 2013). Some of the work that
aims to ground navigational instructions include (Guadar-
rama et al. 2013), (Bollini et al. 2013) and (Beetz et al.
2011), where the focus was to ground verbs like go, fol-
low, etc. and spatial relations of verbs (Tellex et al. 2011;
Fasola and Mataric 2013).

Mapping Instructions to Action Sequences. Chen and
Mooney (2011) and Artzi and Zettlemoyer (2013) present
methods based on semantic parsing to map navigational in-
structions to a sequence of actions. Mei, Bansal, and Walter
(2015) look at neural mapping of instructions to sequence
of actions, along with input from bag-of-word features ex-
tracted from the visual image. While these works focus on
grounding navigational instructions to actions in the envi-
ronment, we aim to ground visual attributes of objects such
as shape, size and color.

Deep reinforcement learning using visual data. Prior
work has explored using Deep Reinforcement learning ap-
proaches for playing FPS games (Lample and Chaplot 2016;
Kempka et al. 2016; Kulkarni et al. 2016). The challenge
here is to learn optimal policy for a variety of tasks, includ-
ing navigation using raw visual pixel information. Chaplot
et al. (2016) look at transfer learning between different tasks
in the Doom Environment. In all these methods, the policy
for each task is learned separately using a deep Q-Learning
(Mnih et al. 2013). In contrast, we train a single network for
multiple tasks/instructions. Zhu et al. (2016) look at target-
driven visual navigation, given the image of the target ob-
ject. We use the natural language instruction and do not have
the visual image of the object. Yu, Zhang, and Xu (2017)

look at learning to navigate in a 2D maze-like environment
and execute commands, for both seen and zero-shot setting,
where the combination of words are not seen before. Misra,
Langford, and Artzi (2017) also look at mapping raw visual
observations and text input to actions in a 2D Blocks envi-
ronment. While these works also looks at executing a variety
of instructions, they tackle only 2D environments. Oh et al.
(2017) look at zero-shot task generalization in a 3D environ-
ment. Their method tackles long instructions with several
subtasks and a wide variety of action verbs. However, the
position of the agent is discretized like 2D Mazes and their
method encodes some prior linguistic knowledge in a anal-
ogy making objective.

Compared to the prior work, this paper aims to address vi-
sual language grounding in a challenging 3D setting involv-
ing raw-pixel input, continuous agent positions and partially
observable envrionment, which poses additional challenges
of perception, exploration and reasoning. Unlike many of the
previous methods, our model assumes no prior linguistic or
perceptual knowledge, and is trainable end-to-end.

3 Problem Formulation
We tackle the problem of task-oriented language grounding
in the context of target-driven visual navigation conditioned
on a natural language instruction, where the agent has to
navigate to the object described in the instruction. Consider
an agent interacting with an episodic environment E. In the
beginning of each episode, the agent receives a natural lan-
guage instruction (L) which indicates the description of the
target, a visual object in the environment. At each time step,
the agent receives a raw pixel-level image of the ﬁrst per-
son view of the environment (It), and performs an action
at. The episode terminates whenever the agent reaches any
object or the number of time steps exceeds the maximum
episode length. Let st = {It, L} denote the state at each
time step. The objective of the agent is to learn an optimal
policy π(at|st), which maps the observed states to actions,
eventually leading to successful completion of the task. In
this case, the task is to reach the correct object before the
episode terminates. We consider two different learning ap-
proaches: (1) Imitation Learning (Bagnell 2015): where the
agent has access to an oracle which speciﬁes the optimal ac-
tion given any state in the environment; (2) Reinforcement
Learning (Sutton and Barto 1998): where the agent receives
a positive reward when it reaches the target object and a neg-
ative reward when it reaches any other object.

Figure 3: Gated-Attention unit architecture.

4 Proposed Approach
We propose a novel architecture for task-oriented visual lan-
guage grounding, which assumes no prior linguistic or per-
ceptual knowledge and can be trained end-to-end. The pro-
posed model is divided into two modules, state processing
and policy learning, as shown in Figure 2.

State Processing Module: The state processing module
takes the current state st = {It, L} as the input and cre-
ates a joint representation for the image and the instruc-
tion. This joint representation is used by the policy learner
to predict the optimal action to take at that timestep. It
consists of a convolutional network (LeCun, Bengio, and
others 1995) to process the image It, a Gated Recurrent
Unit (GRU) (Cho et al. 2014) network to process the in-
struction L and a multimodal fusion unit that combines
the representations of the instruction and the image. Let
xI = f (It; θconv) ∈ Rd×H×W be the representation of
the image, where θconv denote the parameters of the con-
volutional network, d denotes number of feature maps (in-
termediate representations) in the convolutional network
output, while H and W denote the height and width of
each feature map. Let xL = f (L; θgru) be the represen-
tation of the instruction, where θgru denotes the param-
eters of the GRU network. The multimodal fusion unit,
M (xI , xL) combines the image and instruction representa-
tions. Many prior methods combine the multimodal repre-
sentations by concatenation (Mei, Bansal, and Walter 2015;
Misra, Langford, and Artzi 2017). We develop a multimodal
fusion unit, Gated-Attention, based on multiplicative inter-
actions between instruction and image representation.

Concatenation: In this approach, the representations of
the image and instruction are simply ﬂattened and concate-
nated to create a joint state representation:

Mconcat(xI , xL) = [vec(xI ); vec(xL)],

where vec(.) denotes the ﬂattening operation. The concate-
nation unit is used as a baseline for the proposed Gated-
Attention unit as it is used by prior methods (Mei, Bansal,
and Walter 2015; Misra, Langford, and Artzi 2017).

Gated-Attention: In the Gated-Attention unit, the in-
struction embedding is passed through a fully-connected lin-
ear layer with a sigmoid activation. The output dimension of
this linear layer, d, is equal to the number of feature maps
in the output of the convolutional network (ﬁrst dimension
of xI ). The output of this linear layer is called the attention

Figure 4: A3C policy model architecture.
vector aL = h(xL) ∈ Rd, where h(.) denotes the fully-
connected layer with sigmoid activation. Each element of
aL is expanded to a H × W matrix. This results in a 3-
dimensional matrix, M (aL) ∈ Rd×H×W whose (i, j, k)th
element is given by: MaL [i, j, k] = aL[i]. This matrix is
multiplied element-wise with the output of the convolutional
network:

MGA(xI , xL) = M (h(xL)) (cid:12) xI = M (aL) (cid:12) xI ,

where (cid:12) denotes the Hadamard product (Horn 1990). The
architecture of the Gated-Attention unit is shown in Figure 3.
The whole unit is differentiable which makes the architec-
ture end-to-end trainable.

The proposed Gated-Attention unit is inspired by the
Gated-Attention Reader architecture for text comprehension
(Dhingra et al. 2017). They integrate a multi-hop archi-
tecture with a Gated-attention mechanism, which is based
on multiplicative interactions between the query embed-
ding and the intermediate states of a recurrent neural net-
work document reader. In contrast, we propose a Gated-
Attention multimodal fusion unit which is based on multi-
plicative interactions between the instruction representation
and the convolutional feature maps of the image representa-
tion. This architecture can be extended to any application of
multimodal fusion of verbal and visual modalities.

The intuition behind Gated-Attention unit is that the
trained convolutional feature maps detect different attributes
of the objects in the frame, such as color and shape. The
agent needs to attend to speciﬁc attributes of the objects
based on the instruction. For example, depending on the
whether the instruction is “Go to the green object”, “Go to
the pillar” or “Go to the green pillar” the agent needs to at-
tend to objects which are ‘green’, objects which look like a
‘pillar’ or both. The Gated-Attention unit is designed to gate
speciﬁc feature maps based on the attention vector from the
instruction, aL.

Policy Learning Module
The output of the multimodal fusion unit (Mconcat or MGA)
is fed to the policy learning module. The architecture of the
policy learning module is speciﬁc to the learning paradigm:
(1) Imitation Learning or (2) Reinforcement Learning.

For imitation learning, we consider two algorithms, Be-
havioral Cloning (Bagnell 2015) and DAgger (Ross, Gor-
don, and Bagnell 2011). Both the algorithms require an ora-
cle that can return an optimal action given the current state.

rectly. The customizable nature of the environment enables
us to create scenarios with varying levels of difﬁculty which
we believe leads to designing sophisticated learning algo-
rithms to address the challenge of multi-task and zero-shot
reinforcement learning.

An instruction is a combination of (action, attribute(s),
object) triple. Each instruction can have more than one at-
tribute but we limit the number of actions and objects to
one each. The environment allows a variety of objects to be
spawned at different locations in the map. The objects can
have various visual attributes such as color, shape and size3.
We provide a set of 70 manually generated instructions3. For
each of these instructions, the environment allows for auto-
matic creation of multiple episodes, each randomly created
with its own set of correct object and incorrect objects. Al-
though the number of instructions are limited, the combi-
nations of correct and incorrect objects for each instruction
allows us to create multiple settings for the same instruc-
tion. Each time an instruction is selected, the environment
generates a random combination of incorrect objects and the
correct object in randomized locations. One of the signiﬁ-
cant challenges posed for a learning algorithm is to under-
stand that the same instruction can refer to different objects
in the different episodes. For example, “Go to the red object”
can refer to a red keycard in one episode, and a red torch
in another episode. Similarly, “Go to the keycard” can refer
to keycards of various colors in different episodes. Objects
could also occlude each other, or might not even be present
in the agent’s ﬁeld of view, or the map could be more com-
plicated, making it difﬁcult for the agent to make a decision
based solely on the current input, stressing the need for efﬁ-
cient exploration.

Our environment also provides different modes with re-
spect to spawning of objects each with varying difﬁculty
levels (Figure 5): Easy: The agent is spawned at a ﬁxed lo-
cation. The candidate objects are spawned at ﬁve ﬁxed loca-
tions along a single horizontal line along the ﬁeld of view of
the agent. Medium: The candidate objects are spawned in
random locations, but the environment ensures that they are
in the ﬁeld of view of the agent. The agent is still spawned at
a ﬁxed location. Hard: The candidate objects and the agent
are spawned at random locations and the objects may or may
not be in the agents ﬁeld of view in the initial conﬁguration.
The agent needs to explore the map to view all the objects.

6 Experimental Setup
We perform our experiments in all of the three environment
difﬁculty modes, where we restrict the number of objects to
5 for each episode (one correct object, four incorrect objects
and the agent are spawned for each episode). During train-
ing, the objects are spawned from a training set of 55 instruc-
tions, while 15 instructions pertaining to unseen attribute-
object combinations are held out in a test set for zero-shot
evaluation. During training, at the start of each episode, one
of the train instructions is selected randomly. A correct target
object is selected and 4 incorrect objects are selected at ran-
dom. These objects are placed at random locations depend-

3See Appendix for the list of objects and instructions

Figure 5: Sample starting states and bird’s eye view of the map (not
visible to the agent) showing agent and object locations in Easy,
Medium and Hard modes.

The oracle is implemented by extracting agent and target
object locations and orientations from the Doom game en-
gine. Given any state, the oracle determines the optimal ac-
tion as follows: The agent ﬁrst reorients (using turn left,
turn right actions) towards the target object. It moves for-
ward (move forward action), reorienting towards the target
object if deviation of the agent’s orientation is greater than
the minimum turn angle supported by the environment.

For reinforcement learning, we use the Asynchronous Ad-
vantage Actor-Critic (A3C) algorithm (Mnih et al. 2016)
which uses a deep neural network to learn the policy and
value functions and runs multiple parallel threads to update
the network parameters. We also use the entropy regulariza-
tion for improved exploration as described by (Mnih et al.
2016). In addition, we use the Generalized Advantage Esti-
mator (Schulman et al. 2015) to reduce the variance of the
policy gradient (Williams 1992) updates.

The policy learning module for imitation learning con-
tains a fully connected layer to estimate the policy function.
The policy learning module for reinforcement learning us-
ing A3C (shown in Figure 4) consists of an LSTM layer,
followed by fully connected layers to estimate the policy
function as well as the value function. The LSTM layer is
introduced so that the agent can have some memory of pre-
vious states. This is important as a reinforcement learning
agent might explore states where all objects are not visible
and need to remember the objects seen previously.

5 Environment

for

We create an environment
task-oriented language
grounding, where the agent can execute a natural language
instruction and obtain a positive reward on successful com-
pletion of the task. Our environment is built on top of the
ViZDoom API (Kempka et al. 2016), based on Doom, a clas-
sic ﬁrst person shooting game. It provides the raw visual in-
formation from a ﬁrst-person perspective at every timestep.
Each scenario in the environment comprises of an agent and
a list of objects (a subset of ViZDoom objects) - one correct
and rest incorrect in a customized map. The agent can in-
teract with the environment by performing navigational ac-
tions such as turn left, turn right, move forward. Given an
instruction “Go to the green torch”, the task is considered
successful if the agent is able to reach the green torch cor-

Figure 6: Comparison of the performance of the proposed Gated-Attention (GA) unit to the baseline Concatenation unit using Reinforcement
learning algorithm, A3C for (a) easy, (b) medium and (c) hard environments.

ing on the difﬁculty level of the environment. The episode
terminates if the agent reaches any object or time step ex-
ceeds the maximum episode length (T = 30). The evalua-
tion metric is the accuracy of the agent which is success rate
of reaching the correct object before the episode terminates.
We consider two scenarios for evaluation:
(1) Multitask Generalization (MT), where the agent is
evaluated on unseen maps with instructions in the train set.
Unseen maps comprise of unseen combination of objects
placed at randomized locations. This scenario tests that the
agent doesn’t overﬁt to or memorize the training maps and
can execute multiple instructions or tasks in unseen maps.
(2) Zero-shot Task Generalization (ZSL), where the agent
is evaluated on unseen test instructions. This scenario tests
whether the agent can generalize to new combinations of
attribute-object pairs which are not seen during the training.
The maps in this scenario are also unseen.

Baseline Approaches
Reinforcement Learning: We adapt (Misra, Langford, and
Artzi 2017) as a reinforcement learning baseline in the pro-
posed environment. Misra, Langford, and Artzi (2017) looks
at jointly reasoning on linguistic and visual inputs for mov-
ing blocks in a 2D grid environment to execute an instruc-
tion. Their work uses raw features from the 2D grid, pro-
cessed by a CNN, while the instruction is processed by an
LSTM. Text and visual representations are combined us-
ing concatenation. The agent is trained using reinforcement
learning and enhanced using distance based reward shaping.
We do not use reward shaping as we would like the method
to generalize to environments where the distance from the
target is not available.
Imitation Learning: We adapt (Mei, Bansal, and Walter
2015) as an imitation learning baseline in the proposed en-
vironment. Mei, Bansal, and Walter (2015) map sequence
of instructions to actions, treated as a sequence-to-sequence
learning problem, with visual state input received by the de-
coder at each decode timestep. While they use a bag-of-
visual words representation for visual state, we adapt the
baseline to directly process raw pixels from the 3D environ-
ment using CNNs.

To ensure fairness in comparison, we use exact same ar-
chitecture of CNNs (to process visual input), GRUs (to pro-
cess textual instruction) and policy learning across baseline
and proposed models. This reduces the reinforcement learn-

ing baseline to A3C algorithm with concatenation multi-
modal fusion (A3C-Concat), and imitation learning baseline
to Behavioral Cloning with Concatenation (BC-Concat).

Hyper-parameters
The input to the neural network is the instruction and an
RGB image of size 3x300x168. The ﬁrst layer convolves
the image with 128 ﬁlters of 8x8 kernel size with stride 4,
followed by 64 ﬁlters of 4x4 kernel size with stride 2 and
another 64 ﬁlters of 4x4 kernel size with stride 2. The archi-
tecture of the convolutional layers is adapted from previous
work on playing deathmatches in Doom (Chaplot and Lam-
ple 2017). The input instruction is encoded through a Gated
Recurrent Unit (GRU) (Chung et al. 2014) of size 256.

For the imitation learning approach, we run experiments
with Behavioral Cloning (BC) and DAgger algorithms in an
online fashion, which have data generation and policy up-
date function per outer iteration. The policy learner for imi-
tation learning comprises of a linear layer of size 512 which
is fully-connected to 3 neurons to predict the policy func-
tion (i.e. probability of each action). In each data generation
step, we sample state trajectories based on oracle’s policy in
BC and based on a mixture of oracle’s policy and the cur-
rently learned policy in DAgger. The mixing of the policies
is governed by an exploration coefﬁcient, which has a linear
decay from 1 to 0. For each state, we collect the optimal ac-
tion given by the policy oracle. Then the policy is updated
for 10 epochs over all the state-action pairs collected so far,
using the RMSProp optimizer (Tieleman and Hinton 2012).
Both methods use Huber loss (Huber 1964) between the es-
timated policy and the optimal policy given by the policy
oracle.

For reinforcement learning, we run experiments with A3C
algorithm. The policy learning module has a linear layer of
size 256 followed by an LSTM layer of size 256 which en-
codes the history of state observations. The LSTM layer’s
output is fully-connected to a single neuron to predict the
value function as well as three other neurons to predict the
policy function. All the network parameters are shared for
predicting both the value function and the policy function
except the ﬁnal fully connected layer. All the convolutional
layers and fully-connected linear layers have ReLu activa-
tions (Nair and Hinton 2010). The A3C model was trained
using Stochastic Gradient Descent (SGD) with a learning
rate of 0.001. We used a discount factor of 0.99 for calculat-

Model

BC Concat
BC GA
DAgger Concat
DAgger GA

Imitation
Learning

Reinforcement
Learning

A3C Concat
A3C GA

Parameters

Easy

Medium

Hard

5.21M
5.09M
5.21M
5.09M

3.44M
3.39M

MT

0.86
0.97
0.92
0.94

1.00
1.00

ZSL

0.71
0.81
0.73
0.85

0.80
0.81

MT

0.23
0.30
0.45
0.55

0.80
0.89

ZSL

0.15
0.23
0.23
0.40

0.54
0.75

MT

0.20
0.36
0.19
0.29

0.24
0.83

ZSL

0.15
0.29
0.13
0.30

0.12
0.73

Table 1: The accuracy of all the models with Concatenation and Gated-Attention (GA) units. A3C Concat and BC Concat are the adapted
versions of Misra, Langford, and Artzi (2017) and Mei, Bansal, and Walter (2015) respectively for the proposed environment. All the accuracy
values are averaged over 100 episodes.

ing expected rewards and run 16 parallel threads for each ex-
periment. We use mean-squared loss between the estimated
value function and discounted sum of rewards for training
with respect to the value function, and the policy gradient
loss using for training with respect to the policy function.

7 Results & Discussions
For all the models described in section 4, the performance
on both Multitask and Zero-shot Generalization is shown in
Table 1. The performance of A3C models on Multitask Gen-
eralization during training is plotted in Figure 6.

Performance of GA models: We observe that models
with the Gated-Attention (GA) unit outperform models with
the Concatenation unit for Multitask and Zero-Shot General-
ization. From Figure 6 we observe that A3C models with GA
units learn faster than Concat models and converge to higher
levels of accuracy. In hard mode, GA achieves 83% accuracy
on Multitask Generalization and 73% on Zero-Shot General-
ization, whereas Concat achieves 24% and 12% respectively
and fails to show any considerable performance. For Imita-
tion Learning, we observe that GA models perform better
than Concat, and that as the environment modes get harder,
imitation learning does not perform very well as there is a
need for exploration in medium and hard settings. In con-
trast, the inherent extensive exploration of the reinforcement
learning algorithm makes the A3C model more robust to the
agent’s location and covers more state trajectories.

Policy Execution : Figure 9 shows a policy execution of
the A3C model in the hard mode for the instruction short
green torch. In this ﬁgure, we demonstrate the agent’s abil-
ity to explore the environment and handle occlusion. In this
example, none of the objects are in the ﬁeld-of-view of the
agent in the initial frame. The agent explores the environ-
ment (makes a 300 degree turn) and eventually navigates
towards the target object. It has also learned to distinguish
between a short green torch and tall green torch and to avoid
the tall torch before reaching the short torch4.

Analysis of Attention Maps: Figure 7 shows the heatmap
for values of the attention vector for different instructions
grouped by object type of the target object (additional at-
tention maps are given in the supplementary material). As
seen in the ﬁgure, dimension 18 corresponds to ‘armor’, di-
mensions 8 corresponds to the ‘skullkey’ and dimension 36
corresponds to the ‘pillar’. Also, note that there is no dimen-

4Demo videos: https://goo.gl/rPWlMy

sion which is high for all the instructions in the ﬁrst group.
This indicates that the model also recognizes that the word
‘object’ does not correspond to a particular object type, but
rather refers to any object of that color (indicated by dotted
red boxes in 7). These observations indicate that the model
is learning to recognize the attributes of objects such as color
and type, and speciﬁc feature maps are gated based on these
attributes. Furthermore, the attention vector weights on the
test instructions (marked by * in ﬁgure 7) also indicate that
the Gated-Attention unit is also able to recognize attributes
of the object in unseen instructions. We also visualize the t-
SNE plots for the attention vectors based on attributes, color
and object type as shown in Figure 8. The attention vectors
for objects of red, blue, green, and yellow are present in clus-
ters whereas those for instructions which do not mention the
object’s color are spread across and belong to the clusters
corresponding to the object type. Similarly, objects of a par-
ticular type present themselves in clusters. The clusters in-
dicate that the model is able to recognize object attributes as
it learns similar attention vectors for objects with similar.

8 Conclusion

In this paper we proposed an end-to-end architecture for
task-oriented language grounding from raw pixels in a 3D
environment, for both reinforcement learning and imita-
tion learning. The architecture uses a novel multimodal
fusion mechanism, Gated-Attention, which learns a joint
state representation based on multiplicative interactions be-
tween instruction and image representation. We observe that
the models (A3C for reinforcement learning and Behav-
ioral Cloning/DAgger for imitation learning) which use the
Gated-Attention unit outperform the models with concate-
nation units for both Multitask and Zero-Shot task general-
ization, across three modes of difﬁculty. The visualization of
the attention weights for the Gated-Attention unit indicates
that the agent learns to recognize objects, color attributes
and size attributes.

Acknowledgements

We would like to thank Prof. Louis-Philippe Morency
and Dr. Tadas Baltruaitis for their valuable comments and
guidance throughout the development of this work. This
work was partially supported by BAE grants ADeLAIDE
FA8750-16C-0130-001 and ConTAIN HR0011-16-C-0136.

Figure 7: Heatmap of the values of the 64-dimensional attention vector for different instructions
grouped by object type and sub-grouped by object color. The test instructions are marked by *. The
red boxes indicate that certain dimensions of the attention vector get activated for particular attributes
of the target object referred in the instruction.

Figure 8: The t-SNE visualization of
the attention vectors showing clus-
ters based on object color, type and
size.

Figure 9: This ﬁgure shows an example of the A3C policy execution at different points for the instruction ‘Go to the short green torch’. Left:
Navigation map of the agent, Right: frames at each point. A: Initial frame: None of the objects are visible. B: agent has turned so that objects
are in the ﬁeld of view. C : agent successfully avoids the tall green torch. D : agent moves towards the short green torch. E :agent reaches
target.

References
[Artzi and Zettlemoyer 2013] Artzi, Y., and Zettlemoyer, L.
2013. Weakly supervised learning of semantic parsers for
mapping instructions to actions. Transactions of the Associ-
ation for Computational Linguistics 1:49–62.
[Bagnell 2015] Bagnell, J. A. 2015. An invitation to imita-
tion. Technical report, DTIC Document.
[Beetz et al. 2011] Beetz, M.; Klank, U.; Kresse, I.; Maldon-
ado, A.; M¨osenlechner, L.; Pangercic, D.; R¨uhr, T.; and
Tenorth, M. 2011. Robotic roommates making pancakes. In
Humanoid Robots (Humanoids), 2011 11th IEEE-RAS In-
ternational Conference on, 529–536. IEEE.
[Bollini et al. 2013] Bollini, M.; Tellex, S.; Thompson, T.;
Interpreting and executing
Roy, N.; and Rus, D. 2013.
In Experimental Robotics,
recipes with a cooking robot.
481–495. Springer.
[Chao, Cakmak, and Thomaz 2011] Chao, C.; Cakmak, M.;
and Thomaz, A. L. 2011. Towards grounding concepts for
transfer in goal learning from demonstration. In Develop-
ment and Learning (ICDL), 2011 IEEE International Con-
ference on, volume 2, 1–6. IEEE.
[Chaplot and Lample 2017] Chaplot, D. S., and Lample, G.
2017. Arnold: An autonomous agent to play fps games. In
AAAI, 5085–5086.
[Chaplot et al. 2016] Chaplot, D. S.; Lample, G.; Sathyen-
dra, K. M.; and Salakhutdinov, R. 2016. Transfer deep rein-
forcement learning in 3d environments: An empirical study.
In NIPS Deep Reinforcemente Leaning Workshop.
[Chen and Mooney 2011] Chen, D. L., and Mooney, R. J.
2011. Learning to interpret natural language navigation in-
structions from observations. In AAAI, volume 2, 1–2.
[Cho et al. 2014] Cho, K.; Van Merri¨enboer, B.; Bahdanau,
2014. On the properties of neural
D.; and Bengio, Y.
machine translation: Encoder-decoder approaches. arXiv
preprint arXiv:1409.1259.
[Chu et al. 2013] Chu, V.; McMahon, I.; Riano, L.; McDon-
ald, C. G.; He, Q.; Perez-Tejada, J. M.; Arrigo, M.; Fitter,
N.; Nappo, J. C.; and Darrell, T. 2013. Using robotic ex-
ploratory procedures to learn the meaning of haptic adjec-
tives. In Robotics and Automation (ICRA), 3048–3055.
[Chung et al. 2014] Chung, J.; Gulcehre, C.; Cho, K.; and
Bengio, Y.
2014. Empirical evaluation of gated recur-
rent neural networks on sequence modeling. arXiv preprint
arXiv:1412.3555.
[Dhingra et al. 2017] Dhingra, B.; Liu, H.; Yang, Z.; Cohen,
W.; and Salakhutdinov, R. 2017. Gated-attention readers for
text comprehension. ACL.
[Fasola and Mataric 2013] Fasola, J., and Mataric, M. J.
2013. Using semantic ﬁelds to model dynamic spatial rela-
tions in a robot architecture for natural language instruction
of service robots. In Intelligent Robots and Systems (IROS),
143–150. IEEE.
[Guadarrama et al. 2013] Guadarrama, S.; Riano, L.; Gol-
land, D.; Go, D.; Jia, Y.; Klein, D.; Abbeel, P.; Darrell, T.;
et al. 2013. Grounding spatial relations for human-robot
interaction. In IROS, 1640–1647. IEEE.

[Guadarrama et al. 2014] Guadarrama, S.; Rodner, E.;
Saenko, K.; Zhang, N.; Farrell, R.; Donahue, J.; and Darrell,
In Robotics:
T. 2014. Open-vocabulary object retrieval.
science and systems, volume 2, 6. Citeseer.
[Horn 1990] Horn, R. A. 1990. The hadamard product. In
Proc. Symp. Appl. Math, volume 40, 87–169.
[Huber 1964] Huber, P. J. 1964. Robust estimation of a
location parameter. The Annals of Mathematical Statistics
35(1):73–101.
[Kempka et al. 2016] Kempka, M.; Wydmuch, M.; Runc, G.;
Toczek, J.; and Ja´skowski, W. 2016. Vizdoom: A doom-
based ai research platform for visual reinforcement learning.
arXiv preprint arXiv:1605.02097.
[Kulick et al. 2013] Kulick, J.; Toussaint, M.; Lang, T.; and
2013. Active learning for teaching a robot
Lopes, M.
grounded relational symbols. In IJCAI.
[Kulkarni et al. 2016] Kulkarni, T. D.; Saeedi, A.; Gautam,
S.; and Gershman, S. J. 2016. Deep successor reinforcement
learning. arXiv preprint arXiv:1606.02396.
[Lample and Chaplot 2016] Lample, G., and Chaplot, D. S.
2016. Playing fps games with deep reinforcement learning.
arXiv preprint arXiv:1609.05521.
[LeCun, Bengio, and others 1995] LeCun, Y.; Bengio, Y.;
et al. 1995. Convolutional networks for images, speech,
and time series. The handbook of brain theory and neural
networks 3361(10):1995.
[Lemaignan et al. 2012] Lemaignan, S.; Ros, R.; Sisbot,
E. A.; Alami, R.; and Beetz, M. 2012. Grounding the in-
teraction: Anchoring situated discourse in everyday human-
robot interaction. International Journal of Social Robotics
4(2):181–199.
[Mei, Bansal, and Walter 2015] Mei, H.; Bansal, M.; and
Walter, M. R. 2015. Listen, attend, and walk: Neural map-
ping of navigational instructions to action sequences. arXiv
preprint arXiv:1506.04089.
[Misra, Langford, and Artzi 2017] Misra, D. K.; Langford,
J.; and Artzi, Y. 2017. Mapping instructions and visual
observations to actions with reinforcement learning. arXiv
preprint arXiv:1704.08795.
[Mnih et al. 2013] Mnih, V.; Kavukcuoglu, K.; Silver, D.;
Graves, A.; Antonoglou, I.; Wierstra, D.; and Riedmiller, M.
2013. Playing atari with deep reinforcement learning. arXiv
preprint arXiv:1312.5602.
[Mnih et al. 2016] Mnih, V.; Badia, A. P.; Mirza, M.; Graves,
A.; Lillicrap, T.; Harley, T.; Silver, D.; and Kavukcuoglu,
K. 2016. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning,
1928–1937.
[Nair and Hinton 2010] Nair, V., and Hinton, G. E. 2010.
Rectiﬁed linear units improve restricted boltzmann ma-
chines. In Proceedings of the 27th international conference
on machine learning (ICML-10), 807–814.
[Oh et al. 2017] Oh, J.; Singh, S.; Lee, H.; and Kohli, P.
2017. Zero-shot task generalization with multi-task deep
reinforcement learning. arXiv preprint arXiv:1706.05064.

[Ross, Gordon, and Bagnell 2011] Ross, S.; Gordon, G. J.;
and Bagnell, D. 2011. A reduction of imitation learning
and structured prediction to no-regret online learning.
[Russell and Norvig 1995] Russell, S., and Norvig, P. 1995.
A modern approach. Artiﬁcial Intelligence. Prentice-Hall,
Egnlewood Cliffs 25:27.
[Schulman et al. 2015] Schulman, J.; Moritz, P.; Levine, S.;
Jordan, M.; and Abbeel, P. 2015. High-dimensional contin-
uous control using generalized advantage estimation. arXiv
preprint arXiv:1506.02438.
[Sutton and Barto 1998] Sutton, R. S., and Barto, A. G.
1998. Reinforcement learning: An introduction, volume 1.
MIT press Cambridge.
[Tellex et al. 2011] Tellex, S. A.; Kollar, T. F.; Dickerson,
S. R.; Walter, M. R.; Banerjee, A.; Teller, S.; and Roy,
N. 2011. Understanding natural language commands for
robotic navigation and mobile manipulation.
[Tieleman and Hinton 2012] Tieleman, T., and Hinton, G.
2012. Lecture 6.5-rmsprop: Divide the gradient by a run-
ning average of its recent magnitude. COURSERA: Neural
networks for machine learning 4(2).
[Williams 1992] Williams, R. J. 1992. Simple statistical
gradient-following algorithms for connectionist reinforce-
ment learning. Machine learning 8(3-4):229–256.
[Wu et al. 2016] Wu, Y.; Zhang, S.; Zhang, Y.; Bengio, Y.;
and Salakhutdinov, R. 2016. On multiplicative integration
with recurrent neural networks. In NIPS.
[Yu, Zhang, and Xu 2017] Yu, H.; Zhang, H.; and Xu, W.
2017. A deep compositional framework for human-like lan-
guage acquisition in virtual environment. arXiv preprint
arXiv:1703.09831.
[Zhu et al. 2016] Zhu, Y.; Mottaghi, R.; Kolve, E.; Lim, J. J.;
Gupta, A.; Fei-Fei, L.; and Farhadi, A. 2016. Target-driven
visual navigation in indoor scenes using deep reinforcement
learning. arXiv preprint arXiv:1609.05143.

A Doom objects
The ViZDoom environment supports spawning of several objects of various colors and sizes. The types of objects available are
Columns, Torches, Armors and Keycards. In our experiments, we use several of these objects, which are shown in Figure 10.

Figure 10: Objects of various colors and sizes used in the environment

B Instructions

The list of 70 navigational instructions that was used to train and test the system in given in the Table 2.

Instruction Type

Instruction

Size + Color

Color + Size

Color

tall green torch, short red object, short red pillar, short red torch, tall red object,
tall blue object, tall green object, tall red pillar, tall green pillar, short blue torch,
tall red torch, short green torch, short green object, short blue object,
tall blue torch, short green pillar

red short object, green tall torch, red short pillar, red short torch, red tall object,
green tall object, blue tall object, red tall pillar, green tall pillar,
red tall torch, blue tall torch, green short object, green short torch,
blue short object, green short pillar, blue short torch

blue torch, red torch, green torch, yellow object,
green armor, tall object, red skullkey, red object, green object
blue object, red pillar, green pillar, red keycard, red armor, blue skullkey,
blue keycard, yellow keycard, yellow skullkey

Object Type

torch, keycard, skullkey, pillar, armor

SuperlativeSize+Color

smallest yellow object, smallest blue object, smallest green object,
largest blue object, largest red object, largest green object,
largest yellow object, smallest red object

SuperlativeSize

largest object, smallest object

Size

short torch, tall torch ,tall pillar ,short pillar ,short object, tall object

Table 2: List of instructions. Each instruction of Go to the X, where each ‘X’ is each entry in the table

C Attention Maps
The attention maps for different instructions grouped based on description is shown in 11 and grouped based on color is shown
in ﬁgure 12.

Figure 11: Attention vector output for different instructions grouped by description. The test instructions are marked by *.

Figure 12: Attention vector output for different instructions grouped by color. The test instructions are marked by *.

Gated-Attention Architectures for Task-Oriented Language Grounding

Devendra Singh Chaplot
chaplot@cs.cmu.edu
School of Computer Science
Carnegie Mellon University

Kanthashree Mysore Sathyendra∗
ksathyen@cs.cmu.edu
School of Computer Science
Carnegie Mellon University

Rama Kumar Pasumarthi∗
rpasumar@cs.cmu.edu
School of Computer Science
Carnegie Mellon University

Dheeraj Rajagopal∗
dheeraj@cs.cmu.edu
School of Computer Science
Carnegie Mellon University

Ruslan Salakhutdinov
rsalakhu@cs.cmu.edu
School of Computer Science
Carnegie Mellon University

8
1
0
2
 
n
a
J
 
9
 
 
]

G
L
.
s
c
[
 
 
2
v
0
3
2
7
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

To perform tasks speciﬁed by natural language instructions,
autonomous agents need to extract semantically meaning-
ful representations of language and map it to visual ele-
ments and actions in the environment. This problem is called
task-oriented language grounding. We propose an end-to-
end trainable neural architecture for task-oriented language
grounding in 3D environments which assumes no prior lin-
guistic or perceptual knowledge and requires only raw pixels
from the environment and the natural language instruction as
input. The proposed model combines the image and text rep-
resentations using a Gated-Attention mechanism and learns a
policy to execute the natural language instruction using stan-
dard reinforcement and imitation learning methods. We show
the effectiveness of the proposed model on unseen instruc-
tions as well as unseen maps, both quantitatively and qual-
itatively. We also introduce a novel environment based on a
3D game engine to simulate the challenges of task-oriented
language grounding over a rich set of instructions and envi-
ronment states.

1

Introduction

Artiﬁcial Intelligence (AI) systems are expected to perceive
the environment and take actions to perform a certain task
(Russell and Norvig 1995). Task-oriented language ground-
ing refers to the process of extracting semantically mean-
ingful representations of language by mapping it to visual
elements and actions in the environment in order to perform
the task speciﬁed by the instruction.

Consider the scenario shown in Figure 1, where an agent
takes natural language instruction and pixel-level visual in-
formation as input to carry out the task in the real world. To
accomplish this goal, the agent has to draw semantic cor-
respondences between the visual and verbal modalities and
learn a policy to perform the task. This problem poses sev-
eral challenges: the agent has to learn to recognize objects
in raw pixel input, explore the environment as the objects
might be occluded or outside the ﬁeld-of-view of the agent,
ground each concept of the instruction in visual elements
or actions in the environment, reason about the pragmatics
of language based on the objects in the current environment
(for example instructions with superlative tokens, such as

Figure 1: An example of task-oriented language grounding in the
3D Doom environment with sample instructions. The test set con-
sists of unseen instructions.

‘Go to the largest object’) and navigate to the correct object
while avoiding incorrect ones.

To tackle this problem, we propose an architecture that
comprises of a state processing module that creates a joint
representation of the instruction and the images observed by
the agent, and a policy learner to predict the optimal ac-
tion the agent has to take in that timestep. The state pro-
cessing module consists of a novel Gated-Attention multi-
modal fusion mechanism, which is based on multiplicative
interactions between both modalities (Dhingra et al. 2017;
Wu et al. 2016).

The contributions of this paper are summarized as fol-
lows: 1) We propose an end-to-end trainable architecture
that handles raw pixel-based input for task-oriented lan-
guage grounding in a 3D environment and assumes no prior
linguistic or perceptual knowledge1. We show that the pro-
posed model generalizes well to unseen instructions as well
as unseen maps2. 2) We develop a novel Gated-Attention
mechanism for multimodal fusion of representations of ver-
bal and visual modalities. We show that the gated-attention
mechanism outperforms the baseline method of concatenat-
ing the representations using various policy learning meth-
ods. The visualization of the attention weights in the gated-
attention unit shows that the model learns to associate at-
tributes of the object mentioned in the instruction with the
visual representations learned by the model. 3) We intro-

∗Equal contribution

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

1The environment and code is available at https://

github.com/devendrachaplot/DeepRL-Grounding

2See demo videos at https://goo.gl/rPWlMy

Figure 2: The proposed model architecture to estimate the policy given the natural language instruction and the image showing the ﬁrst-person
view of the environment.

duce a new environment, built over ViZDoom (Kempka et
al. 2016), for task-oriented language grounding with a rich
set of actions, objects and their attributes. The environment
provides a ﬁrst-person view of the world state, and allows for
simulating complex scenarios for tasks such as navigation.

2 Related Work
Grounding Language in Robotics. In the context of
grounding language in objects and their attributes, Guadar-
rama et al. (2014) present a method to ground open vocab-
ulary to objects in the environment. Several works look at
grounding concepts through human-robot interaction (Chao,
Cakmak, and Thomaz 2011; Lemaignan et al. 2012). Other
works in grounding include attempts to ground natural
language instructions in haptic signals (Chu et al. 2013)
and teaching robot to ground natural language using ac-
tive learning (Kulick et al. 2013). Some of the work that
aims to ground navigational instructions include (Guadar-
rama et al. 2013), (Bollini et al. 2013) and (Beetz et al.
2011), where the focus was to ground verbs like go, fol-
low, etc. and spatial relations of verbs (Tellex et al. 2011;
Fasola and Mataric 2013).

Mapping Instructions to Action Sequences. Chen and
Mooney (2011) and Artzi and Zettlemoyer (2013) present
methods based on semantic parsing to map navigational in-
structions to a sequence of actions. Mei, Bansal, and Walter
(2015) look at neural mapping of instructions to sequence
of actions, along with input from bag-of-word features ex-
tracted from the visual image. While these works focus on
grounding navigational instructions to actions in the envi-
ronment, we aim to ground visual attributes of objects such
as shape, size and color.

Deep reinforcement learning using visual data. Prior
work has explored using Deep Reinforcement learning ap-
proaches for playing FPS games (Lample and Chaplot 2016;
Kempka et al. 2016; Kulkarni et al. 2016). The challenge
here is to learn optimal policy for a variety of tasks, includ-
ing navigation using raw visual pixel information. Chaplot
et al. (2016) look at transfer learning between different tasks
in the Doom Environment. In all these methods, the policy
for each task is learned separately using a deep Q-Learning
(Mnih et al. 2013). In contrast, we train a single network for
multiple tasks/instructions. Zhu et al. (2016) look at target-
driven visual navigation, given the image of the target ob-
ject. We use the natural language instruction and do not have
the visual image of the object. Yu, Zhang, and Xu (2017)

look at learning to navigate in a 2D maze-like environment
and execute commands, for both seen and zero-shot setting,
where the combination of words are not seen before. Misra,
Langford, and Artzi (2017) also look at mapping raw visual
observations and text input to actions in a 2D Blocks envi-
ronment. While these works also looks at executing a variety
of instructions, they tackle only 2D environments. Oh et al.
(2017) look at zero-shot task generalization in a 3D environ-
ment. Their method tackles long instructions with several
subtasks and a wide variety of action verbs. However, the
position of the agent is discretized like 2D Mazes and their
method encodes some prior linguistic knowledge in a anal-
ogy making objective.

Compared to the prior work, this paper aims to address vi-
sual language grounding in a challenging 3D setting involv-
ing raw-pixel input, continuous agent positions and partially
observable envrionment, which poses additional challenges
of perception, exploration and reasoning. Unlike many of the
previous methods, our model assumes no prior linguistic or
perceptual knowledge, and is trainable end-to-end.

3 Problem Formulation
We tackle the problem of task-oriented language grounding
in the context of target-driven visual navigation conditioned
on a natural language instruction, where the agent has to
navigate to the object described in the instruction. Consider
an agent interacting with an episodic environment E. In the
beginning of each episode, the agent receives a natural lan-
guage instruction (L) which indicates the description of the
target, a visual object in the environment. At each time step,
the agent receives a raw pixel-level image of the ﬁrst per-
son view of the environment (It), and performs an action
at. The episode terminates whenever the agent reaches any
object or the number of time steps exceeds the maximum
episode length. Let st = {It, L} denote the state at each
time step. The objective of the agent is to learn an optimal
policy π(at|st), which maps the observed states to actions,
eventually leading to successful completion of the task. In
this case, the task is to reach the correct object before the
episode terminates. We consider two different learning ap-
proaches: (1) Imitation Learning (Bagnell 2015): where the
agent has access to an oracle which speciﬁes the optimal ac-
tion given any state in the environment; (2) Reinforcement
Learning (Sutton and Barto 1998): where the agent receives
a positive reward when it reaches the target object and a neg-
ative reward when it reaches any other object.

Figure 3: Gated-Attention unit architecture.

4 Proposed Approach
We propose a novel architecture for task-oriented visual lan-
guage grounding, which assumes no prior linguistic or per-
ceptual knowledge and can be trained end-to-end. The pro-
posed model is divided into two modules, state processing
and policy learning, as shown in Figure 2.

State Processing Module: The state processing module
takes the current state st = {It, L} as the input and cre-
ates a joint representation for the image and the instruc-
tion. This joint representation is used by the policy learner
to predict the optimal action to take at that timestep. It
consists of a convolutional network (LeCun, Bengio, and
others 1995) to process the image It, a Gated Recurrent
Unit (GRU) (Cho et al. 2014) network to process the in-
struction L and a multimodal fusion unit that combines
the representations of the instruction and the image. Let
xI = f (It; θconv) ∈ Rd×H×W be the representation of
the image, where θconv denote the parameters of the con-
volutional network, d denotes number of feature maps (in-
termediate representations) in the convolutional network
output, while H and W denote the height and width of
each feature map. Let xL = f (L; θgru) be the represen-
tation of the instruction, where θgru denotes the param-
eters of the GRU network. The multimodal fusion unit,
M (xI , xL) combines the image and instruction representa-
tions. Many prior methods combine the multimodal repre-
sentations by concatenation (Mei, Bansal, and Walter 2015;
Misra, Langford, and Artzi 2017). We develop a multimodal
fusion unit, Gated-Attention, based on multiplicative inter-
actions between instruction and image representation.

Concatenation: In this approach, the representations of
the image and instruction are simply ﬂattened and concate-
nated to create a joint state representation:

Mconcat(xI , xL) = [vec(xI ); vec(xL)],

where vec(.) denotes the ﬂattening operation. The concate-
nation unit is used as a baseline for the proposed Gated-
Attention unit as it is used by prior methods (Mei, Bansal,
and Walter 2015; Misra, Langford, and Artzi 2017).

Gated-Attention: In the Gated-Attention unit, the in-
struction embedding is passed through a fully-connected lin-
ear layer with a sigmoid activation. The output dimension of
this linear layer, d, is equal to the number of feature maps
in the output of the convolutional network (ﬁrst dimension
of xI ). The output of this linear layer is called the attention

Figure 4: A3C policy model architecture.
vector aL = h(xL) ∈ Rd, where h(.) denotes the fully-
connected layer with sigmoid activation. Each element of
aL is expanded to a H × W matrix. This results in a 3-
dimensional matrix, M (aL) ∈ Rd×H×W whose (i, j, k)th
element is given by: MaL [i, j, k] = aL[i]. This matrix is
multiplied element-wise with the output of the convolutional
network:

MGA(xI , xL) = M (h(xL)) (cid:12) xI = M (aL) (cid:12) xI ,

where (cid:12) denotes the Hadamard product (Horn 1990). The
architecture of the Gated-Attention unit is shown in Figure 3.
The whole unit is differentiable which makes the architec-
ture end-to-end trainable.

The proposed Gated-Attention unit is inspired by the
Gated-Attention Reader architecture for text comprehension
(Dhingra et al. 2017). They integrate a multi-hop archi-
tecture with a Gated-attention mechanism, which is based
on multiplicative interactions between the query embed-
ding and the intermediate states of a recurrent neural net-
work document reader. In contrast, we propose a Gated-
Attention multimodal fusion unit which is based on multi-
plicative interactions between the instruction representation
and the convolutional feature maps of the image representa-
tion. This architecture can be extended to any application of
multimodal fusion of verbal and visual modalities.

The intuition behind Gated-Attention unit is that the
trained convolutional feature maps detect different attributes
of the objects in the frame, such as color and shape. The
agent needs to attend to speciﬁc attributes of the objects
based on the instruction. For example, depending on the
whether the instruction is “Go to the green object”, “Go to
the pillar” or “Go to the green pillar” the agent needs to at-
tend to objects which are ‘green’, objects which look like a
‘pillar’ or both. The Gated-Attention unit is designed to gate
speciﬁc feature maps based on the attention vector from the
instruction, aL.

Policy Learning Module
The output of the multimodal fusion unit (Mconcat or MGA)
is fed to the policy learning module. The architecture of the
policy learning module is speciﬁc to the learning paradigm:
(1) Imitation Learning or (2) Reinforcement Learning.

For imitation learning, we consider two algorithms, Be-
havioral Cloning (Bagnell 2015) and DAgger (Ross, Gor-
don, and Bagnell 2011). Both the algorithms require an ora-
cle that can return an optimal action given the current state.

rectly. The customizable nature of the environment enables
us to create scenarios with varying levels of difﬁculty which
we believe leads to designing sophisticated learning algo-
rithms to address the challenge of multi-task and zero-shot
reinforcement learning.

An instruction is a combination of (action, attribute(s),
object) triple. Each instruction can have more than one at-
tribute but we limit the number of actions and objects to
one each. The environment allows a variety of objects to be
spawned at different locations in the map. The objects can
have various visual attributes such as color, shape and size3.
We provide a set of 70 manually generated instructions3. For
each of these instructions, the environment allows for auto-
matic creation of multiple episodes, each randomly created
with its own set of correct object and incorrect objects. Al-
though the number of instructions are limited, the combi-
nations of correct and incorrect objects for each instruction
allows us to create multiple settings for the same instruc-
tion. Each time an instruction is selected, the environment
generates a random combination of incorrect objects and the
correct object in randomized locations. One of the signiﬁ-
cant challenges posed for a learning algorithm is to under-
stand that the same instruction can refer to different objects
in the different episodes. For example, “Go to the red object”
can refer to a red keycard in one episode, and a red torch
in another episode. Similarly, “Go to the keycard” can refer
to keycards of various colors in different episodes. Objects
could also occlude each other, or might not even be present
in the agent’s ﬁeld of view, or the map could be more com-
plicated, making it difﬁcult for the agent to make a decision
based solely on the current input, stressing the need for efﬁ-
cient exploration.

Our environment also provides different modes with re-
spect to spawning of objects each with varying difﬁculty
levels (Figure 5): Easy: The agent is spawned at a ﬁxed lo-
cation. The candidate objects are spawned at ﬁve ﬁxed loca-
tions along a single horizontal line along the ﬁeld of view of
the agent. Medium: The candidate objects are spawned in
random locations, but the environment ensures that they are
in the ﬁeld of view of the agent. The agent is still spawned at
a ﬁxed location. Hard: The candidate objects and the agent
are spawned at random locations and the objects may or may
not be in the agents ﬁeld of view in the initial conﬁguration.
The agent needs to explore the map to view all the objects.

6 Experimental Setup
We perform our experiments in all of the three environment
difﬁculty modes, where we restrict the number of objects to
5 for each episode (one correct object, four incorrect objects
and the agent are spawned for each episode). During train-
ing, the objects are spawned from a training set of 55 instruc-
tions, while 15 instructions pertaining to unseen attribute-
object combinations are held out in a test set for zero-shot
evaluation. During training, at the start of each episode, one
of the train instructions is selected randomly. A correct target
object is selected and 4 incorrect objects are selected at ran-
dom. These objects are placed at random locations depend-

3See Appendix for the list of objects and instructions

Figure 5: Sample starting states and bird’s eye view of the map (not
visible to the agent) showing agent and object locations in Easy,
Medium and Hard modes.

The oracle is implemented by extracting agent and target
object locations and orientations from the Doom game en-
gine. Given any state, the oracle determines the optimal ac-
tion as follows: The agent ﬁrst reorients (using turn left,
turn right actions) towards the target object. It moves for-
ward (move forward action), reorienting towards the target
object if deviation of the agent’s orientation is greater than
the minimum turn angle supported by the environment.

For reinforcement learning, we use the Asynchronous Ad-
vantage Actor-Critic (A3C) algorithm (Mnih et al. 2016)
which uses a deep neural network to learn the policy and
value functions and runs multiple parallel threads to update
the network parameters. We also use the entropy regulariza-
tion for improved exploration as described by (Mnih et al.
2016). In addition, we use the Generalized Advantage Esti-
mator (Schulman et al. 2015) to reduce the variance of the
policy gradient (Williams 1992) updates.

The policy learning module for imitation learning con-
tains a fully connected layer to estimate the policy function.
The policy learning module for reinforcement learning us-
ing A3C (shown in Figure 4) consists of an LSTM layer,
followed by fully connected layers to estimate the policy
function as well as the value function. The LSTM layer is
introduced so that the agent can have some memory of pre-
vious states. This is important as a reinforcement learning
agent might explore states where all objects are not visible
and need to remember the objects seen previously.

5 Environment

for

We create an environment
task-oriented language
grounding, where the agent can execute a natural language
instruction and obtain a positive reward on successful com-
pletion of the task. Our environment is built on top of the
ViZDoom API (Kempka et al. 2016), based on Doom, a clas-
sic ﬁrst person shooting game. It provides the raw visual in-
formation from a ﬁrst-person perspective at every timestep.
Each scenario in the environment comprises of an agent and
a list of objects (a subset of ViZDoom objects) - one correct
and rest incorrect in a customized map. The agent can in-
teract with the environment by performing navigational ac-
tions such as turn left, turn right, move forward. Given an
instruction “Go to the green torch”, the task is considered
successful if the agent is able to reach the green torch cor-

Figure 6: Comparison of the performance of the proposed Gated-Attention (GA) unit to the baseline Concatenation unit using Reinforcement
learning algorithm, A3C for (a) easy, (b) medium and (c) hard environments.

ing on the difﬁculty level of the environment. The episode
terminates if the agent reaches any object or time step ex-
ceeds the maximum episode length (T = 30). The evalua-
tion metric is the accuracy of the agent which is success rate
of reaching the correct object before the episode terminates.
We consider two scenarios for evaluation:
(1) Multitask Generalization (MT), where the agent is
evaluated on unseen maps with instructions in the train set.
Unseen maps comprise of unseen combination of objects
placed at randomized locations. This scenario tests that the
agent doesn’t overﬁt to or memorize the training maps and
can execute multiple instructions or tasks in unseen maps.
(2) Zero-shot Task Generalization (ZSL), where the agent
is evaluated on unseen test instructions. This scenario tests
whether the agent can generalize to new combinations of
attribute-object pairs which are not seen during the training.
The maps in this scenario are also unseen.

Baseline Approaches
Reinforcement Learning: We adapt (Misra, Langford, and
Artzi 2017) as a reinforcement learning baseline in the pro-
posed environment. Misra, Langford, and Artzi (2017) looks
at jointly reasoning on linguistic and visual inputs for mov-
ing blocks in a 2D grid environment to execute an instruc-
tion. Their work uses raw features from the 2D grid, pro-
cessed by a CNN, while the instruction is processed by an
LSTM. Text and visual representations are combined us-
ing concatenation. The agent is trained using reinforcement
learning and enhanced using distance based reward shaping.
We do not use reward shaping as we would like the method
to generalize to environments where the distance from the
target is not available.
Imitation Learning: We adapt (Mei, Bansal, and Walter
2015) as an imitation learning baseline in the proposed en-
vironment. Mei, Bansal, and Walter (2015) map sequence
of instructions to actions, treated as a sequence-to-sequence
learning problem, with visual state input received by the de-
coder at each decode timestep. While they use a bag-of-
visual words representation for visual state, we adapt the
baseline to directly process raw pixels from the 3D environ-
ment using CNNs.

To ensure fairness in comparison, we use exact same ar-
chitecture of CNNs (to process visual input), GRUs (to pro-
cess textual instruction) and policy learning across baseline
and proposed models. This reduces the reinforcement learn-

ing baseline to A3C algorithm with concatenation multi-
modal fusion (A3C-Concat), and imitation learning baseline
to Behavioral Cloning with Concatenation (BC-Concat).

Hyper-parameters
The input to the neural network is the instruction and an
RGB image of size 3x300x168. The ﬁrst layer convolves
the image with 128 ﬁlters of 8x8 kernel size with stride 4,
followed by 64 ﬁlters of 4x4 kernel size with stride 2 and
another 64 ﬁlters of 4x4 kernel size with stride 2. The archi-
tecture of the convolutional layers is adapted from previous
work on playing deathmatches in Doom (Chaplot and Lam-
ple 2017). The input instruction is encoded through a Gated
Recurrent Unit (GRU) (Chung et al. 2014) of size 256.

For the imitation learning approach, we run experiments
with Behavioral Cloning (BC) and DAgger algorithms in an
online fashion, which have data generation and policy up-
date function per outer iteration. The policy learner for imi-
tation learning comprises of a linear layer of size 512 which
is fully-connected to 3 neurons to predict the policy func-
tion (i.e. probability of each action). In each data generation
step, we sample state trajectories based on oracle’s policy in
BC and based on a mixture of oracle’s policy and the cur-
rently learned policy in DAgger. The mixing of the policies
is governed by an exploration coefﬁcient, which has a linear
decay from 1 to 0. For each state, we collect the optimal ac-
tion given by the policy oracle. Then the policy is updated
for 10 epochs over all the state-action pairs collected so far,
using the RMSProp optimizer (Tieleman and Hinton 2012).
Both methods use Huber loss (Huber 1964) between the es-
timated policy and the optimal policy given by the policy
oracle.

For reinforcement learning, we run experiments with A3C
algorithm. The policy learning module has a linear layer of
size 256 followed by an LSTM layer of size 256 which en-
codes the history of state observations. The LSTM layer’s
output is fully-connected to a single neuron to predict the
value function as well as three other neurons to predict the
policy function. All the network parameters are shared for
predicting both the value function and the policy function
except the ﬁnal fully connected layer. All the convolutional
layers and fully-connected linear layers have ReLu activa-
tions (Nair and Hinton 2010). The A3C model was trained
using Stochastic Gradient Descent (SGD) with a learning
rate of 0.001. We used a discount factor of 0.99 for calculat-

Model

BC Concat
BC GA
DAgger Concat
DAgger GA

Imitation
Learning

Reinforcement
Learning

A3C Concat
A3C GA

Parameters

Easy

Medium

Hard

5.21M
5.09M
5.21M
5.09M

3.44M
3.39M

MT

0.86
0.97
0.92
0.94

1.00
1.00

ZSL

0.71
0.81
0.73
0.85

0.80
0.81

MT

0.23
0.30
0.45
0.55

0.80
0.89

ZSL

0.15
0.23
0.23
0.40

0.54
0.75

MT

0.20
0.36
0.19
0.29

0.24
0.83

ZSL

0.15
0.29
0.13
0.30

0.12
0.73

Table 1: The accuracy of all the models with Concatenation and Gated-Attention (GA) units. A3C Concat and BC Concat are the adapted
versions of Misra, Langford, and Artzi (2017) and Mei, Bansal, and Walter (2015) respectively for the proposed environment. All the accuracy
values are averaged over 100 episodes.

ing expected rewards and run 16 parallel threads for each ex-
periment. We use mean-squared loss between the estimated
value function and discounted sum of rewards for training
with respect to the value function, and the policy gradient
loss using for training with respect to the policy function.

7 Results & Discussions
For all the models described in section 4, the performance
on both Multitask and Zero-shot Generalization is shown in
Table 1. The performance of A3C models on Multitask Gen-
eralization during training is plotted in Figure 6.

Performance of GA models: We observe that models
with the Gated-Attention (GA) unit outperform models with
the Concatenation unit for Multitask and Zero-Shot General-
ization. From Figure 6 we observe that A3C models with GA
units learn faster than Concat models and converge to higher
levels of accuracy. In hard mode, GA achieves 83% accuracy
on Multitask Generalization and 73% on Zero-Shot General-
ization, whereas Concat achieves 24% and 12% respectively
and fails to show any considerable performance. For Imita-
tion Learning, we observe that GA models perform better
than Concat, and that as the environment modes get harder,
imitation learning does not perform very well as there is a
need for exploration in medium and hard settings. In con-
trast, the inherent extensive exploration of the reinforcement
learning algorithm makes the A3C model more robust to the
agent’s location and covers more state trajectories.

Policy Execution : Figure 9 shows a policy execution of
the A3C model in the hard mode for the instruction short
green torch. In this ﬁgure, we demonstrate the agent’s abil-
ity to explore the environment and handle occlusion. In this
example, none of the objects are in the ﬁeld-of-view of the
agent in the initial frame. The agent explores the environ-
ment (makes a 300 degree turn) and eventually navigates
towards the target object. It has also learned to distinguish
between a short green torch and tall green torch and to avoid
the tall torch before reaching the short torch4.

Analysis of Attention Maps: Figure 7 shows the heatmap
for values of the attention vector for different instructions
grouped by object type of the target object (additional at-
tention maps are given in the supplementary material). As
seen in the ﬁgure, dimension 18 corresponds to ‘armor’, di-
mensions 8 corresponds to the ‘skullkey’ and dimension 36
corresponds to the ‘pillar’. Also, note that there is no dimen-

4Demo videos: https://goo.gl/rPWlMy

sion which is high for all the instructions in the ﬁrst group.
This indicates that the model also recognizes that the word
‘object’ does not correspond to a particular object type, but
rather refers to any object of that color (indicated by dotted
red boxes in 7). These observations indicate that the model
is learning to recognize the attributes of objects such as color
and type, and speciﬁc feature maps are gated based on these
attributes. Furthermore, the attention vector weights on the
test instructions (marked by * in ﬁgure 7) also indicate that
the Gated-Attention unit is also able to recognize attributes
of the object in unseen instructions. We also visualize the t-
SNE plots for the attention vectors based on attributes, color
and object type as shown in Figure 8. The attention vectors
for objects of red, blue, green, and yellow are present in clus-
ters whereas those for instructions which do not mention the
object’s color are spread across and belong to the clusters
corresponding to the object type. Similarly, objects of a par-
ticular type present themselves in clusters. The clusters in-
dicate that the model is able to recognize object attributes as
it learns similar attention vectors for objects with similar.

8 Conclusion

In this paper we proposed an end-to-end architecture for
task-oriented language grounding from raw pixels in a 3D
environment, for both reinforcement learning and imita-
tion learning. The architecture uses a novel multimodal
fusion mechanism, Gated-Attention, which learns a joint
state representation based on multiplicative interactions be-
tween instruction and image representation. We observe that
the models (A3C for reinforcement learning and Behav-
ioral Cloning/DAgger for imitation learning) which use the
Gated-Attention unit outperform the models with concate-
nation units for both Multitask and Zero-Shot task general-
ization, across three modes of difﬁculty. The visualization of
the attention weights for the Gated-Attention unit indicates
that the agent learns to recognize objects, color attributes
and size attributes.

Acknowledgements

We would like to thank Prof. Louis-Philippe Morency
and Dr. Tadas Baltruaitis for their valuable comments and
guidance throughout the development of this work. This
work was partially supported by BAE grants ADeLAIDE
FA8750-16C-0130-001 and ConTAIN HR0011-16-C-0136.

Figure 7: Heatmap of the values of the 64-dimensional attention vector for different instructions
grouped by object type and sub-grouped by object color. The test instructions are marked by *. The
red boxes indicate that certain dimensions of the attention vector get activated for particular attributes
of the target object referred in the instruction.

Figure 8: The t-SNE visualization of
the attention vectors showing clus-
ters based on object color, type and
size.

Figure 9: This ﬁgure shows an example of the A3C policy execution at different points for the instruction ‘Go to the short green torch’. Left:
Navigation map of the agent, Right: frames at each point. A: Initial frame: None of the objects are visible. B: agent has turned so that objects
are in the ﬁeld of view. C : agent successfully avoids the tall green torch. D : agent moves towards the short green torch. E :agent reaches
target.

References
[Artzi and Zettlemoyer 2013] Artzi, Y., and Zettlemoyer, L.
2013. Weakly supervised learning of semantic parsers for
mapping instructions to actions. Transactions of the Associ-
ation for Computational Linguistics 1:49–62.
[Bagnell 2015] Bagnell, J. A. 2015. An invitation to imita-
tion. Technical report, DTIC Document.
[Beetz et al. 2011] Beetz, M.; Klank, U.; Kresse, I.; Maldon-
ado, A.; M¨osenlechner, L.; Pangercic, D.; R¨uhr, T.; and
Tenorth, M. 2011. Robotic roommates making pancakes. In
Humanoid Robots (Humanoids), 2011 11th IEEE-RAS In-
ternational Conference on, 529–536. IEEE.
[Bollini et al. 2013] Bollini, M.; Tellex, S.; Thompson, T.;
Interpreting and executing
Roy, N.; and Rus, D. 2013.
In Experimental Robotics,
recipes with a cooking robot.
481–495. Springer.
[Chao, Cakmak, and Thomaz 2011] Chao, C.; Cakmak, M.;
and Thomaz, A. L. 2011. Towards grounding concepts for
transfer in goal learning from demonstration. In Develop-
ment and Learning (ICDL), 2011 IEEE International Con-
ference on, volume 2, 1–6. IEEE.
[Chaplot and Lample 2017] Chaplot, D. S., and Lample, G.
2017. Arnold: An autonomous agent to play fps games. In
AAAI, 5085–5086.
[Chaplot et al. 2016] Chaplot, D. S.; Lample, G.; Sathyen-
dra, K. M.; and Salakhutdinov, R. 2016. Transfer deep rein-
forcement learning in 3d environments: An empirical study.
In NIPS Deep Reinforcemente Leaning Workshop.
[Chen and Mooney 2011] Chen, D. L., and Mooney, R. J.
2011. Learning to interpret natural language navigation in-
structions from observations. In AAAI, volume 2, 1–2.
[Cho et al. 2014] Cho, K.; Van Merri¨enboer, B.; Bahdanau,
2014. On the properties of neural
D.; and Bengio, Y.
machine translation: Encoder-decoder approaches. arXiv
preprint arXiv:1409.1259.
[Chu et al. 2013] Chu, V.; McMahon, I.; Riano, L.; McDon-
ald, C. G.; He, Q.; Perez-Tejada, J. M.; Arrigo, M.; Fitter,
N.; Nappo, J. C.; and Darrell, T. 2013. Using robotic ex-
ploratory procedures to learn the meaning of haptic adjec-
tives. In Robotics and Automation (ICRA), 3048–3055.
[Chung et al. 2014] Chung, J.; Gulcehre, C.; Cho, K.; and
Bengio, Y.
2014. Empirical evaluation of gated recur-
rent neural networks on sequence modeling. arXiv preprint
arXiv:1412.3555.
[Dhingra et al. 2017] Dhingra, B.; Liu, H.; Yang, Z.; Cohen,
W.; and Salakhutdinov, R. 2017. Gated-attention readers for
text comprehension. ACL.
[Fasola and Mataric 2013] Fasola, J., and Mataric, M. J.
2013. Using semantic ﬁelds to model dynamic spatial rela-
tions in a robot architecture for natural language instruction
of service robots. In Intelligent Robots and Systems (IROS),
143–150. IEEE.
[Guadarrama et al. 2013] Guadarrama, S.; Riano, L.; Gol-
land, D.; Go, D.; Jia, Y.; Klein, D.; Abbeel, P.; Darrell, T.;
et al. 2013. Grounding spatial relations for human-robot
interaction. In IROS, 1640–1647. IEEE.

[Guadarrama et al. 2014] Guadarrama, S.; Rodner, E.;
Saenko, K.; Zhang, N.; Farrell, R.; Donahue, J.; and Darrell,
In Robotics:
T. 2014. Open-vocabulary object retrieval.
science and systems, volume 2, 6. Citeseer.
[Horn 1990] Horn, R. A. 1990. The hadamard product. In
Proc. Symp. Appl. Math, volume 40, 87–169.
[Huber 1964] Huber, P. J. 1964. Robust estimation of a
location parameter. The Annals of Mathematical Statistics
35(1):73–101.
[Kempka et al. 2016] Kempka, M.; Wydmuch, M.; Runc, G.;
Toczek, J.; and Ja´skowski, W. 2016. Vizdoom: A doom-
based ai research platform for visual reinforcement learning.
arXiv preprint arXiv:1605.02097.
[Kulick et al. 2013] Kulick, J.; Toussaint, M.; Lang, T.; and
2013. Active learning for teaching a robot
Lopes, M.
grounded relational symbols. In IJCAI.
[Kulkarni et al. 2016] Kulkarni, T. D.; Saeedi, A.; Gautam,
S.; and Gershman, S. J. 2016. Deep successor reinforcement
learning. arXiv preprint arXiv:1606.02396.
[Lample and Chaplot 2016] Lample, G., and Chaplot, D. S.
2016. Playing fps games with deep reinforcement learning.
arXiv preprint arXiv:1609.05521.
[LeCun, Bengio, and others 1995] LeCun, Y.; Bengio, Y.;
et al. 1995. Convolutional networks for images, speech,
and time series. The handbook of brain theory and neural
networks 3361(10):1995.
[Lemaignan et al. 2012] Lemaignan, S.; Ros, R.; Sisbot,
E. A.; Alami, R.; and Beetz, M. 2012. Grounding the in-
teraction: Anchoring situated discourse in everyday human-
robot interaction. International Journal of Social Robotics
4(2):181–199.
[Mei, Bansal, and Walter 2015] Mei, H.; Bansal, M.; and
Walter, M. R. 2015. Listen, attend, and walk: Neural map-
ping of navigational instructions to action sequences. arXiv
preprint arXiv:1506.04089.
[Misra, Langford, and Artzi 2017] Misra, D. K.; Langford,
J.; and Artzi, Y. 2017. Mapping instructions and visual
observations to actions with reinforcement learning. arXiv
preprint arXiv:1704.08795.
[Mnih et al. 2013] Mnih, V.; Kavukcuoglu, K.; Silver, D.;
Graves, A.; Antonoglou, I.; Wierstra, D.; and Riedmiller, M.
2013. Playing atari with deep reinforcement learning. arXiv
preprint arXiv:1312.5602.
[Mnih et al. 2016] Mnih, V.; Badia, A. P.; Mirza, M.; Graves,
A.; Lillicrap, T.; Harley, T.; Silver, D.; and Kavukcuoglu,
K. 2016. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning,
1928–1937.
[Nair and Hinton 2010] Nair, V., and Hinton, G. E. 2010.
Rectiﬁed linear units improve restricted boltzmann ma-
chines. In Proceedings of the 27th international conference
on machine learning (ICML-10), 807–814.
[Oh et al. 2017] Oh, J.; Singh, S.; Lee, H.; and Kohli, P.
2017. Zero-shot task generalization with multi-task deep
reinforcement learning. arXiv preprint arXiv:1706.05064.

[Ross, Gordon, and Bagnell 2011] Ross, S.; Gordon, G. J.;
and Bagnell, D. 2011. A reduction of imitation learning
and structured prediction to no-regret online learning.
[Russell and Norvig 1995] Russell, S., and Norvig, P. 1995.
A modern approach. Artiﬁcial Intelligence. Prentice-Hall,
Egnlewood Cliffs 25:27.
[Schulman et al. 2015] Schulman, J.; Moritz, P.; Levine, S.;
Jordan, M.; and Abbeel, P. 2015. High-dimensional contin-
uous control using generalized advantage estimation. arXiv
preprint arXiv:1506.02438.
[Sutton and Barto 1998] Sutton, R. S., and Barto, A. G.
1998. Reinforcement learning: An introduction, volume 1.
MIT press Cambridge.
[Tellex et al. 2011] Tellex, S. A.; Kollar, T. F.; Dickerson,
S. R.; Walter, M. R.; Banerjee, A.; Teller, S.; and Roy,
N. 2011. Understanding natural language commands for
robotic navigation and mobile manipulation.
[Tieleman and Hinton 2012] Tieleman, T., and Hinton, G.
2012. Lecture 6.5-rmsprop: Divide the gradient by a run-
ning average of its recent magnitude. COURSERA: Neural
networks for machine learning 4(2).
[Williams 1992] Williams, R. J. 1992. Simple statistical
gradient-following algorithms for connectionist reinforce-
ment learning. Machine learning 8(3-4):229–256.
[Wu et al. 2016] Wu, Y.; Zhang, S.; Zhang, Y.; Bengio, Y.;
and Salakhutdinov, R. 2016. On multiplicative integration
with recurrent neural networks. In NIPS.
[Yu, Zhang, and Xu 2017] Yu, H.; Zhang, H.; and Xu, W.
2017. A deep compositional framework for human-like lan-
guage acquisition in virtual environment. arXiv preprint
arXiv:1703.09831.
[Zhu et al. 2016] Zhu, Y.; Mottaghi, R.; Kolve, E.; Lim, J. J.;
Gupta, A.; Fei-Fei, L.; and Farhadi, A. 2016. Target-driven
visual navigation in indoor scenes using deep reinforcement
learning. arXiv preprint arXiv:1609.05143.

A Doom objects
The ViZDoom environment supports spawning of several objects of various colors and sizes. The types of objects available are
Columns, Torches, Armors and Keycards. In our experiments, we use several of these objects, which are shown in Figure 10.

Figure 10: Objects of various colors and sizes used in the environment

B Instructions

The list of 70 navigational instructions that was used to train and test the system in given in the Table 2.

Instruction Type

Instruction

Size + Color

Color + Size

Color

tall green torch, short red object, short red pillar, short red torch, tall red object,
tall blue object, tall green object, tall red pillar, tall green pillar, short blue torch,
tall red torch, short green torch, short green object, short blue object,
tall blue torch, short green pillar

red short object, green tall torch, red short pillar, red short torch, red tall object,
green tall object, blue tall object, red tall pillar, green tall pillar,
red tall torch, blue tall torch, green short object, green short torch,
blue short object, green short pillar, blue short torch

blue torch, red torch, green torch, yellow object,
green armor, tall object, red skullkey, red object, green object
blue object, red pillar, green pillar, red keycard, red armor, blue skullkey,
blue keycard, yellow keycard, yellow skullkey

Object Type

torch, keycard, skullkey, pillar, armor

SuperlativeSize+Color

smallest yellow object, smallest blue object, smallest green object,
largest blue object, largest red object, largest green object,
largest yellow object, smallest red object

SuperlativeSize

largest object, smallest object

Size

short torch, tall torch ,tall pillar ,short pillar ,short object, tall object

Table 2: List of instructions. Each instruction of Go to the X, where each ‘X’ is each entry in the table

C Attention Maps
The attention maps for different instructions grouped based on description is shown in 11 and grouped based on color is shown
in ﬁgure 12.

Figure 11: Attention vector output for different instructions grouped by description. The test instructions are marked by *.

Figure 12: Attention vector output for different instructions grouped by color. The test instructions are marked by *.

