8
1
0
2
 
n
a
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
3
v
5
7
4
8
0
.
3
0
7
1
:
v
i
X
r
a

Overcoming Catastrophic Forgetting by
Incremental Moment Matching

Sang-Woo Lee1, Jin-Hwa Kim1, Jaehyun Jun1, Jung-Woo Ha2, and Byoung-Tak Zhang1,3

Seoul National University1
Clova AI Research, NAVER Corp2
Surromind Robotics3

{slee,jhkim,jhjun}@bi.snu.ac.kr jungwoo.ha@navercorp.com
btzhang@bi.snu.ac.kr

Abstract

Catastrophic forgetting is a problem of neural networks that loses the information
of the ﬁrst task after training the second task. Here, we propose a method, i.e. in-
cremental moment matching (IMM), to resolve this problem. IMM incrementally
matches the moment of the posterior distribution of the neural network which is
trained on the ﬁrst and the second task, respectively. To make the search space
of posterior parameter smooth, the IMM procedure is complemented by various
transfer learning techniques including weight transfer, L2-norm of the old and the
new parameter, and a variant of dropout with the old parameter. We analyze our ap-
proach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSD-
Birds, and Lifelog datasets. The experimental results show that IMM achieves
state-of-the-art performance by balancing the information between an old and a
new network.

1

Introduction

Catastrophic forgetting is a fundamental challenge for artiﬁcial general intelligence based on neural
networks. The models that use stochastic gradient descent often forget the information of previous
tasks after being trained on a new task [1, 2]. Online multi-task learning that handles such problems
is described as continual learning. This classic problem has resurfaced with the renaissance of deep
learning research [3, 4].

Recently, the concept of applying a regularization function to a network trained by the old task to
learning a new task has received much attention. This approach can be interpreted as an approxima-
tion of sequential Bayesian [5, 6]. Representative examples of this regularization approach include
learning without forgetting [7] and elastic weight consolidation [8]. These algorithms succeeded in
some experiments where their own assumption of the regularization function ﬁts the problem.

Here, we propose incremental moment matching (IMM) to resolve the catastrophic forgetting prob-
lem. IMM uses the framework of Bayesian neural networks, which implies that uncertainty is intro-
duced on the parameters in neural networks, and that the posterior distribution is calculated [9, 10].
The dimension of the random variable in the posterior distribution is the number of the parameters
in the neural networks. IMM approximates the mixture of Gaussian posterior with each component
representing parameters for a single task to one Gaussian distribution for a combined task. To merge
the posteriors, we introduce two novel methods of moment matching. One is mean-IMM, which
simply averages the parameters of two networks for old and new tasks as the minimization of the
average of KL-divergence between one approximated posterior distribution for the combined task

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

Figure 1: Geometric illustration of incremental moment matching (IMM). Mean-IMM simply av-
erages the parameters of two neural networks, whereas mode-IMM tries to ﬁnd a maximum of the
mixture of Gaussian posteriors. To make IMM be reasonable, the search space of the loss function
between the posterior means µ1 and µ2 should be reasonably smooth and convex-like. To ﬁnd a
µ2 which satisﬁes this condition of a smooth and convex-like path from µ1, we propose applying
various transfer techniques for the IMM procedure.

and each Gaussian posterior for the single task [11]. The other is mode-IMM, which merges the pa-
rameters of two networks using a Laplacian approximation [9] to approximate a mode of the mixture
of two Gaussian posteriors, which represent the parameters of the two networks.

In general, it is too naïve to assume that the ﬁnal posterior distribution for the whole task is Gaussian.
To make our IMM work, the search space of the loss function between the posterior means needs to
be smooth and convex-like. In other words, there should not be high cost barriers between the means
of the two networks for an old and a new task. To make our assumption of Gaussian distribution for
neural network reasonable, we applied three main transfer learning techniques on the IMM proce-
dure: weight transfer, L2-norm of the old and the new parameters, and our newly proposed variant
of dropout using the old parameters. The whole procedure of IMM is illustrated in Figure 1.

2 Previous Works on Catastrophic Forgetting

One of the major approaches preventing catastrophic forgetting is to use an ensemble of neural net-
works. When a new task arrives, the algorithm makes a new network, and shares the representation
between the tasks [12, 13]. However, this approach has a complexity issue, especially in inference,
because the number of networks increases as the number of new tasks that need to be learned in-
creases.

Another approach studies the methods using implicit distributed storage of information, in typical
stochastic gradient descent (SGD) learning. These methods use the idea of dropout, maxout, or neu-
ral module to distributively store the information for each task by making use of the large capacity of
the neural network [4]. Unfortunately, most studies following this approach had limited success and
failed to preserve performance on the old task when an extreme change to the environment occurred
[3]. Alternatively, Fernando et al. [14] proposed PathNet, which extends the idea of the ensemble
approach for parameter reuse [13] within a single network. In PathNet, a neural network has ten or
twenty modules in each layer, and three or four modules are picked for one task in each layer by
an evolutionary approach. This method alleviates the complexity issue of the ensemble approach to
continual learning in a plausible way.

The approach with a regularization term also has received attention. Learning without forgetting
(LwF) is one example of this approach, which uses the pseudo-training data from the old task [7].
Before learning the new task, LwF puts the training data of the new task into the old network,
and uses the output as pseudo-labels of the pseudo-training data. By optimizing both the pseudo-
training data of the old task and the real data of the new task, LwF attempts to prevent catastrophic
forgetting. This framework is promising where the properties of the pseudo training set is similar to
the ideal training set. Elastic weight consolidation (EWC), another example of this approach, uses
sequential Bayesian estimation to update neural networks for continual learning [8]. In EWC, the
posterior distribution trained by the previous task is used to update the new prior distribution. This
new prior is used for learning the new posterior distribution of the new task in a Bayesian manner.

2

EWC assumes that the covariance matrix of the posterior is diagonal and there are no correlations
between the nodes. Though this assumption is fragile, EWC performs well in some domains.

EWC is a monumental recent work that uses sequential Bayesian for continual learning of neural
networks. However, updating the parameter of complex hierarchical models by sequential Bayesian
estimation is not new [5]. Sequential Bayes was used to learn topic models from stream data by
Broderick et al. [6]. Huang et al. applied sequential Bayesian to adapt a deep neural network to
the speciﬁc user in the speech recognition domain [15, 16]. They assigned the layer for the user
adaptation and applied MAP estimation to this single layer. Similar to our IMM method, Bayesian
moment matching is used for sum-product networks, a kind of deep hierarchical probabilistic model
[17]. Though sum-product networks are usually not scalable to large datasets, their online learning
method is useful, and it achieves similar performance to the batch learner. Our method using moment
matching focuses on continual learning and deals with signiﬁcantly different statistics between tasks,
unlike the previous method.

3

Incremental Moment Matching

In incremental moment matching (IMM), the moments of posterior distributions are matched in an
incremental way. In our work, we use a Gaussian distribution to approximate the posterior distri-
bution of parameters. Given K sequential tasks, we want to ﬁnd the optimal parameter µ∗
1:K and
Σ∗
1:K of the Gaussian approximation function q1:K from the posterior parameter for each kth task,
(µk, Σk).

p1:K ≡ p(θ|X1, · · · , XK, y1, · · · , yK) ≈ q1:K ≡ q(θ|µ1:K, Σ1:K)
pk ≡ p(θ|Xk, yk) ≈ qk ≡ q(θ|µk, Σk)

(1)
(2)

q1:K denotes an approximation of the true posterior distribution p1:K for the whole task, and qk
denotes an approximation of the true posterior distribution pk over the training dataset (Xk, yk) for
the kth task. θ denotes the vectorized parameter of the neural network. The dimension of µk and
µ1:k is D, and the dimension of Σk and Σ1:k is D × D, respectively, where D is the dimension of θ.
For example, a multi-layer perceptrons (MLP) with [784-800-800-800-10] has the number of nodes,
D = 1917610 including bias terms.

Next, we explain two proposed moment matching algorithms for the continual learning of modern
deep neural networks. The two algorithms generate two different moments of Gaussian with different
objective functions for the same dataset.

3.1 Mean-based Incremental Moment Matching (mean-IMM)

Mean-IMM averages the parameters of two networks in each layer, using mixing ratios αk with
(cid:80)K
k αk = 1. The objective function of mean-IMM is to minimize the following local KL-distance
or the weighted sum of KL-divergence between each qk and q1:K [11, 18]:

1:K, Σ∗
µ∗

k αkKL(qk||q1:K)

(cid:80)K

1:K = argmin
µ1:K ,Σ1:K
1:K = (cid:80)K
µ∗
k αk(Σk + (µk − µ∗

k αkµk

1:K = (cid:80)K
Σ∗

1:K)(µk − µ∗

1:K)T )

(3)

(4)

(5)

µ∗
1:K and Σ∗
1:K are the optimal solution of the local KL-distance. Notice that covariance information
is not needed for mean-IMM, since calculating µ∗
1:K does not require any Σk. A series of µk is
sufﬁcient to perform the task. The idea of mean-IMM is commonly used in shallow networks [19,
20]. However, the contribution of this paper is to discover when and how mean-IMM can be applied
in modern deep neural networks and to show it can performs better with other transfer techniques.

Future works may include other measures to merge the networks, including the KL-divergence be-
tween q1:K and the mixture of each qk (i.e. KL(q1:K||(cid:80)K

k αkqk)) [18].

3

3.2 Mode-based Incremental Moment Matching (mode-IMM)

Mode-IMM is a variant of mean-IMM which uses the covariance information of the posterior of
Gaussian distribution. In general, a weighted average of two mean vectors of Gaussian distributions
is not a mode of MoG. In discriminative learning, the maximum of the distribution is of primary
interest. According to Ray and Lindsay [21], all the modes of MoG with K clusters lie on (K − 1)-
dimensional hypersurface {θ|θ = ((cid:80)K
k )−1((cid:80)K
k ak = 1}.
See Appendix A for more details.

k µk), 0 < ak < 1 and (cid:80)

k akΣ−1

k akΣ−1

Motivated by the above description, a mode-IMM approximate MoG with Laplacian approximation,
in which the logarithm of the function is expressed by the Taylor expansion [9]. Using Laplacian
approximation, the MoG is approximated as follows:

log q1:K ≈ (cid:80)K

k αk log qk + C = −

θT ((cid:80)K

k αkΣ−1

k )θ + ((cid:80)K

k αkΣ−1

k µk)θ + C (cid:48)

1
2

1:K · ((cid:80)K

1:K = Σ∗
µ∗
1:K = ((cid:80)K
Σ∗

k αkΣ−1
k )−1

k αkΣ−1

k µk)

(6)

(7)

(8)

For Equation 8, we add (cid:15)I to the term to be inverted in practice, with an identity matrix I and a small
constant (cid:15).

Here, we assume diagonal covariance matrices, which means that there is no correlation among
parameters. This diagonal assumption is useful, since it decreases the number of parameters for
each covariance matrix from O(D2) to O(D) for the dimension of the parameters D.

For covariance, we use the inverse of a Fisher information matrix, following [8, 22]. The main
idea of this approximation is that the square of gradients for parameters is a good indicator of their
precision, which is the inverse of the variance. The Fisher information matrix for the kth task, Fk is
deﬁned as:

Fk = E

ln p(˜y|x, µk) ·

ln p(˜y|x, µk)T

(9)

(cid:20) ∂
∂µk

∂
∂µk

(cid:21)

,

where the probability of the expectation follows x ∼ πk and ˜y ∼ p(y|x, µk), where πk denotes an
empirical distribution of Xk.

4 Transfer Techniques for Incremental Moment Matching

In general, the loss function of neural networks is not convex. Consider that shufﬂing nodes and
their weights in a neural network preserves the original performance. If the parameters of two neural
networks initialized independently are averaged, it might perform poorly because of the high cost
barriers between the parameters of the two neural networks [23]. However, we will show that various
transfer learning techniques can be used to ease this problem, and make the assumption of Gaussian
distribution for neural networks reasonable. In this section, we introduce three practical techniques
for IMM, including weight-transfer, L2-transfer, and drop-transfer.

4.1 Weight-Transfer

Weight-transfer initialize the parameters for the new task µk with the parameters of the previous
task µk−1 [24]. In our experiments, the use of weight-transfer was critical to the continual learn-
ing performance. For this reason, the experiments on IMM in this paper use the weight-transfer
technique by default.

The weight-transfer technique is motivated by the geometrical property of neural networks discov-
ered in the previous work [23]. They found that there is a straight path from the initial point to the
solution without any high cost barrier, in various types of neural networks and datasets. This dis-
covery suggests that the weight-transfer from the previous task to the new task makes a smooth loss

4

Figure 2: Experimental results on visualizing the effect of weight-transfer. The geometric property
of the parameter space of the neural network is analyzed. Brighter is better. θ1, θ2, and θ3 are the
vectorized parameters of trained networks from randomly selected subsets of the CIFAR-10 dataset.
This ﬁgure shows that there are better solutions between the three locally optimized parameters.

surface between two solutions for the tasks, so that the optimal solution for both tasks lies on the
interpolated point of the two solutions.

To empirically validate the concept of weight-transfer, we use the linear path analysis proposed by
Goodfellow et al. [23] (Figure 2). We randomly chose 18,000 instances from the training dataset
of CIFAR-10, and divided them into three subsets of 6,000 instances each. These three subsets are
used for sequential training by CNN models, parameterized by θ1, θ2, and θ3, respectively. Here, θ2
is initialized from θ1, and then θ3 is initialized from θ2, in the same way as weight-transfer. In this
analysis, each loss and accuracy is evaluated at a series of points θ = θ1 + α(θ2 − θ1) + β(θ3 −
θ2), varying α and β. In Figure 2, the loss surface of the model on each online subset is nearly
convex. The ﬁgure shows that the parameter at 1
3 (θ1 + θ2 + θ3), which is the same as the solution
of mean-IMM, performs better than any other reference points θ1, θ2, or θ3. However, when θ2 is
not initialized by θ1, the convex-like shape disappears, since there is a high cost barrier between the
loss function of θ1 and θ2.

4.2 L2-transfer

L2-transfer is a variant of L2-regularization. L2-transfer can be interpreted as a special case of
EWC where the prior distribution is Gaussian with λI as a covariance matrix. In L2-transfer, a
regularization term of the distance between µk−1 and µk is added to the following objective function
for ﬁnding µk, where λ is a hyperparameter:

log p(yk|Xk, µk) − λ · ||µk − µk−1||2
2

(10)

The concept of L2-transfer is commonly used in transfer learning [25, 26] and continual learning
[7, 8] with large λ. Unlike the previous usage of large λ, we use small λ for the IMM procedure.
In other words, µk is ﬁrst trained by Equation 10 with small λ, and then merged to µ1:k in our
IMM. Since we want to make the loss surface between µk−1 and µk smooth, and not to minimize
the distance between µk−1 and µk. In convex optimization, the L2-regularizer makes the convex
function strictly convex. Similarly, we hope L2-transfer with small λ help to ﬁnd a µk with a convex-
like loss space between µk−1 and µk.

4.3 Drop-transfer

Drop-transfer is a novel method devised in this paper. Drop-transfer is a variant of dropout where
µk−1 is the zero point of the dropout procedure. In the training phase, the following ˆµk,i is used for
the weight vector corresponding to the ith node µk,i:

(cid:40)

ˆµk,i =

µk−1,i,
1−p · µk,i − p

1

1−p · µk−1,i, otherwise

if ith node is turned off

(11)

where p is the dropout ratio. Notice that the expectation of ˆµk,i is µk,i.

5

Table 1: The averaged accuracies on the disjoint MNIST for two sequential tasks (Top) and the
shufﬂed MNIST for three sequential tasks (Bottom). The untuned setting refers to the most natural
hyperparameter in the equation of each algorithm, whereas the tuned setting refers to using heuristic
hand-tuned hyperparameters. Hyperparam denotes the main hyperparameter of each algorithm. For
IMM with transfer, only α is tuned. The numbers in the parentheses refer to standard deviation.
Every IMM uses weight-transfer.

Disjoint MNIST Experiment
SGD [3]
L2-transfer [25]
Drop-transfer
EWC [8]
Mean-IMM
Mode-IMM
L2-transfer + Mean-IMM
L2-transfer + Mode-IMM
Drop-transfer + Mean-IMM
Drop-transfer + Mode-IMM
L2, Drop-transfer + Mean-IMM
L2, Drop-transfer + Mode-IMM

Shufﬂed MNIST Experiment
SGD [3]
L2-transfer [25]
Drop-transfer
EWC [8]
Mean-IMM
Mode-IMM
L2-transfer + Mean-IMM
L2-transfer + Mode-IMM
Drop-transfer + Mean-IMM
Drop-transfer + Mode-IMM
L2, Drop-transfer + Mean-IMM
L2, Drop-transfer + Mode-IMM

Explanation of
Hyperparam
epoch per dataset
λ in (10)
p in (11)
λ in (20)
α2 in (4)
α2 in (7)
λ / α2
λ / α2
p / α2
p / α2
λ / p / α2
λ / p / α2

epoch per dataset
λ in (10)
p in (11)
λ in (20)
α3 in (4)
α3 in (7)
λ / α3
λ / α3
p / α3
p / α3
λ / p / α3
λ / p / α3

Untuned

Tuned

Hyperparam
10
-
0.5
1.0
0.50
0.50
0.001 / 0.50
0.001 / 0.50
0.5 / 0.50
0.5 / 0.50
0.001 / 0.5 / 0.50
0.001 / 0.5 / 0.50

Hyperparam
60
-
0.5
-
0.33
0.33
1e-4 / 0.33
1e-4 / 0.33
0.5 / 0.33
0.5 / 0.33
1e-4 / 0.5 / 0.33
1e-4 / 0.5 / 0.33

Accuracy
47.72 (± 0.11)
-
51.72 (± 0.79)
47.84 (± 0.04)
90.45 (± 2.24)
91.49 (± 0.98)
78.34 (± 1.82)
92.52 (± 0.41)
80.75 (± 1.28)
93.35 (± 0.49)
66.10 (± 3.19)
93.97 (± 0.32)

Accuracy
89.15 (± 2.34)
-
94.75 (± 0.62)
-
93.23 (± 1.37)
98.02 (± 0.05)
90.38 (± 1.74)
98.16 (± 0.08)
90.79 (± 1.30)
97.80 (± 0.07)
89.51 (± 2.85)
97.83 (± 0.10)

Hyperparam
0.05
0.05
0.5
600M
0.55
0.45
0.001 / 0.60
0.001 / 0.45
0.5 / 0.60
0.5 / 0.50
0.001 / 0.5 / 0.75
0.001 / 0.5 / 0.45

Hyperparam
-
1e-3
0.2
-
0.55
0.60
1e-4 / 0.65
1e-4 / 0.60
0.5 / 0.65
0.5 / 0.55
1e-4 / 0.5 / 0.90
1e-4 / 0.5 / 0.50

Accuracy
71.32 (± 1.54)
85.81 (± 0.52)
51.72 (± 0.79)
52.72 (± 1.36)
91.92 (± 0.98)
92.02 (± 0.73)
92.62 (± 0.95)
92.73 (± 0.35)
92.64 (± 0.60)
93.35 (± 0.49)
93.97 (± 0.23)
94.12 (± 0.27)

Accuracy
∼95.5 [8]
96.37 (± 0.62)
96.86 (± 0.21)
∼98.2 [8]
95.02 (± 0.42)
98.08 (± 0.08)
95.93 (± 0.31)
98.30 (± 0.08)
96.49 (± 0.44)
97.95 (± 0.08)
97.36 (± 0.19)
97.92 (± 0.05)

There are studies [27, 20] that have interpreted dropout as an exponential ensemble of weak learners.
By this perspective, since the marginalization of output distribution over the whole weak learner is
intractable, the parameters multiplied by the inverse of the dropout rate are used for the test phase
in the procedure. In other words, the parameters of the weak learners are, in effect, simply averaged
oversampled learners by dropout. At the process of drop-transfer in our continual learning setting,
we hypothesize that the dropout process makes the averaged point of two arbitrary sampled points
using Equation 11 a good estimator.

We investigated the search space of the loss function of the MLP trained from the MNIST handwrit-
ten digit recognition dataset for with and without dropout regularization, to supplement the evidence
of the described hypothesis. Dropout regularization makes the accuracy of a sampled point from
dropout distribution and an average point of two sampled parameters, from 0.450 (± 0.084) to 0.950
(± 0.009) and 0.757 (± 0.065) to 0.974 (± 0.003), respectively. For the case of both with and without
dropout, the space between two arbitrary samples is empirically convex, and ﬁts to the second-order
equation. Based on this experiment, we expect not only that the search space of the loss function
between modern neural networks can be easily nearly convex [23], but also that regularizers, such
as dropout, make the search space smooth and the point in the search space have a good accuracy in
continual learning.

5 Experimental Results

We evaluate our approach on four experiments, whose settings are intensively used in the previous
works [4, 8, 7, 12]. For more details and experimental results, see Appendix D. The source code for
the experiments is available in Github repository1.

Disjoint MNIST Experiment. The ﬁrst experiment is the disjoint MNIST experiment [4]. In this
experiment, the MNIST dataset is divided into two datasets: the ﬁrst dataset consists of only digits
{0, 1, 2, 3, 4} and the second dataset consists of the remaining digits {5, 6, 7, 8, 9}. Our task is 10-

1https://github.com/btjhjeon/IMM_tensorﬂow

6

Figure 3: Test accuracies of two IMM models with weight-transfer on the disjoint MNIST (Left),
the shufﬂed MNIST (Middle), and the ImageNet2CUB experiment (Right). α is a hyperparameter
that balances the information between the old and the new task.

Figure 4: Test accuracies of IMM with various transfer techniques on the disjoint MNIST. Both L2-
transfer and drop-transfer boost the performance of IMM and make the optimal value of α larger
than 1/2. However, drop-transfer tends to make the accuracy curve more smooth than L2-transfer
does.

class joint categorization, unlike the setting in the previous work, which considers two independent
tasks of 5-class categorization. Because the inference should decide whether a new instance comes
from the ﬁrst or the second task, our task is more difﬁcult than the task of the previous work.

We evaluate the models both on the untuned setting and the tuned setting. The untuned setting refers
to the most natural hyperparameter in the equation of each algorithm. The tuned setting refers to
using heuristic hand-tuned hyperparameters. Consider that tuned hyperparameter setting is often
used in previous works of continual learning as it is difﬁcult to deﬁne a validation set in their setting.
For example, when the model needs to learn from the new task after learning from the old task, a low
learning rate or early stopping without a validation set, or arbitrary hyperparameter for balancing is
used [3, 8]. We discover hyperparameters in the tuned setting not only to ﬁnd the oracle performance
of each algorithm, but also to show that there exist some paths consisting of the point that performs
reasonably for both tasks. Hyperparam in Table 1 denotes hyperparameter mainly searched in the
tuned setting. Table 1 (Top) and Figure 3 (Left) shows the experimental results from the disjoint
MNIST experiment.

In our experimental setting, the usual SGD-based optimizers always perform less than 50%, because
the biases of the output layer for the old task are always pushed to large negative values, which
implies that our task is difﬁcult. Figure 4 also shows that mode-IMM is robust with α and the
optimal α of mean-IMM is larger than 1/2 in the disjoint MNIST experiment.

Shufﬂed MNIST Experiment. The second experiment is the shufﬂed MNIST experiment [3, 8] of
three sequential tasks. In this experiment, the ﬁrst dataset is the same as the original MNIST dataset.
However, in the second dataset, the input pixels of all images are shufﬂed with a ﬁxed, random per-
mutation. In previous work, EWC reaches the performance level of the batch learner, and it is argued
that EWC overcomes catastrophic forgetting in some domains. The experimental details are similar
to the disjoint MNIST experiment, except all models are allowed to use dropout regularization. In
the experiment, the ﬁrst dataset is the same as the original MNIST dataset. However, in the second
and the third dataset, the input pixels of all images are shufﬂed with a ﬁxed, random permutation,

7

Table 2: Experimental results on the Lifelog dataset among different classes (location, sub-location,
and activity) and different subjects (A, B, C). Every IMM uses weight-transfer.

Dual memory architecture [12]
Mean-IMM
Mode-IMM

Location
78.11
77.60
77.14

Sub-location Activity

72.36
73.78
75.76

52.92
52.74
54.07

A
67.02
67.03
67.97

B
58.80
57.73
60.12

C
77.57
79.35
78.89

respectively. Therefore, the difﬁculty of the three datasets is the same, though a different solution is
required for each dataset.

Table 1 (Bottom) and Figure 3 (Middle) shows the experimental results from the shufﬂed MNIST
experiment. Notice that accuracy of drop-transfer (p = 0.2) alone is 96.86 (± 0.21) and L2-transfer
(λ = 1e-4) + drop-transfer (p = 0.4) alone is 97.61 (± 0.15). These results are competitive to EWC
without dropout, whose performance is around 97.0.

ImageNet to CUB Dataset. The third experiment is the ImageNet2CUB experiment [7], the con-
tinual learning problem from the ImageNet dataset to the Caltech-UCSD Birds-200-2011 ﬁne-
grained classiﬁcation (CUB) dataset [28]. The numbers of classes of ImageNet and CUB dataset
are around 1K and 200, and the numbers of training instances are 1M and 5K, respectively. In the
ImageNet2CUB experiment, the last-layer is separated for the ImageNet and the CUB task. The
structure of AlexNet is used for the trained model of ImageNet [29]. In our experiment, we match
the moments of the last-layer ﬁne-tuning model and the LwF model, with mean-IMM and mode-
IMM.

Figure 3 (Right) shows that mean-IMM moderately balances the performance of two tasks between
two networks. However, the balanced hyperparameter of mode-IMM is far from α = 0.5. We think
that it is because the scale of the Fisher matrix F is different between the ImageNet and the CUB
task. Since the number of training data of the two tasks is different, the mean of the square of the
gradient, which is the deﬁnition of F , tends to be different. This implies that the assumption of
mode-IMM does not always hold for heterogeneous tasks. See Appendix D.3 for more information
including the learning methods of IMM where a different class output layer or a different scale of
the dataset is used.

Our results of IMM with LwF exceed the previous state-of-the-art performance, whose model is
also LwF. This is because, in the previous works, the LwF model is initialized by the last-layer ﬁne-
tuning model, not directly by the original AlexNet. In this case, the performance loss of the old task
is not only decreased, but also the performance gain of the new task is decreased. The accuracies of
our mean-IMM (α = 0.5) are 56.20 and 56.73 for the ImageNet task and the CUB task, respectively.
The gains compared to the previous state-of-the-art are +1.13 and -1.14. In the case of mean-IMM
(α = 0.8) and mode-IMM (α = 0.99), the accuracies are 55.08 and 59.08 (+0.01, +1.12), and 55.10
and 59.12 (+0.02, +1.35), respectively.

Lifelog Dataset. Lastly, we evaluate the proposed methods on the Lifelog dataset [12]. The Lifelog
dataset consists of 660,000 instances of egocentric video stream data, collected over 46 days from
three participants using Google Glass [30]. Three class categories, location, sub-location, and activ-
ity, are labeled on each frame of video. In the Lifelog dataset, the class distribution changes con-
tinuously and new classes appear as the day passes. Table 2 shows that mean-IMM and mode-IMM
are competitive to the dual-memory architecture, the previous state-of-the-art ensemble model, even
though IMM uses single network.

6 Discussion

A Shift of Optimal Hyperparameter of IMM. The tuned setting shows there often exists some α
which makes the performance of the mean-IMM close to the mode-IMM. However, in the untuned
hyperparameter setting, mean-IMM performs worse when more transfer techniques are applied. Our
Bayesian interpretation in IMM assumes that the SGD training of the k-th network µk is mainly
affected by the k-th task and is rarely affected by the information of the previous tasks. However,
transfer techniques break this assumption; thus the optimal α is shifted to larger than 1/k. For-
tunately, mode-IMM works more robustly than mean-IMM where transfer techniques are applied.

8

Figure 4 illustrates the change of the test accuracy curve corresponding to the applied transfer tech-
niques and the following shift of the optimal α in mean-IMM and mode-IMM.

Bayesian Approach on Continual Learning. Kirkpatrick et al. [8] interpreted that the Fisher
matrix F as weight importance in explaining their EWC model. In the shufﬂed MNIST experiment,
since a large number of pixels always have a value of zero, the corresponding elements of the Fisher
matrix are also zero. Therefore, EWC does work by allowing weights to change, which are not used
in the previous tasks. On the other hand, mode-IMM also works by selectively balancing between
two weights using variance information. However, these assumptions on weight importance do not
always hold, especially in the disjoint MNIST experiment. The most important weight in the disjoint
MNIST experiment is the bias term in the output layer. Nevertheless, these bias parts of the Fisher
matrix are not guaranteed to be the highest value nor can they be used to balance the class distribution
between the ﬁrst and second task. We believe that using only the diagonal of the covariance matrix
in Bayesian neural networks is too naïve in general and that this is why EWC failed in the disjoint
MNIST experiment. We think it could be alleviated in future work by using a more complex prior,
such as a matrix Gaussian distribution considering the correlations between nodes in the network
[31].

Balancing the Information of an Old and a New Task. The IMM procedure produces a neural
network without a performance loss for kth task µk, which is better than the ﬁnal solution µ1:k in
terms of the performance of the kth task. Furthermore, IMM can easily weigh the importance of
tasks in IMM models in real time. For example, αt can be easily changed for the solution of mean-
IMM µ1:k = (cid:80)k
t αtµt . In actual service situations of IT companies, the importance of the old
and the new task frequently changes in real time, and IMM can handle this problem. This property
differentiates IMM from the other continual learning methods using the regularization approach,
including LwF and EWC.

7 Conclusion

Our contributions are four folds. First, we applied mean-IMM to the continual learning of modern
deep neural networks. Mean-IMM makes competitive results to comparative models and balances
the information between an old and a new network. We also interpreted the success of IMM by the
Bayesian framework with Gaussian posterior. Second, we extended mean-IMM to mode-IMM with
the interpretation of mode-ﬁnding in the mixture of Gaussian posterior. Mode-IMM outperforms
mean-IMM and comparative models in various datasets. Third, we introduced drop-transfer, a novel
method proposed in the paper. Experimental results showed that drop-transfer alone performs well
and is similar to the EWC without dropout, in the domain where EWC rarely forgets. Fourth, We
applied various transfer techniques in the IMM procedure to make our assumption of Gaussian
distribution reasonable. We argued that not only the search space of the loss function among neural
networks can easily be nearly convex, but also regularizers, such as dropout, make the search space
smooth, and the point in the search space have good accuracy. Experimental results showed that
applying transfer techniques often boost the performance of IMM. Overall, we made state-of-the-
art performance in various datasets of continual learning and explored geometrical properties and a
Bayesian perspective of deep neural networks.

Acknowledgments

The authors would like to thank Jiseob Kim, Min-Oh Heo, Donghyun Kwak, Insu Jeon, Christina
Baek, and Heidi Tessmer for helpful comments and editing. This work was supported by the
Naver Corp. and partly by the Korean government (IITP-R0126-16-1072-SW.StarLab, IITP-2017-0-
01772-VTT, KEIT-10044009-HRI.MESSI, KEIT-10060086-RISF). Byoung-Tak Zhang is the cor-
responding author.

References

[1] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks:
The sequential learning problem. Psychology of learning and motivation, 24:109–165, 1989.

9

[2] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive

sciences, 3(4):128–135, 1999.

[3] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empiri-
cal investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint
arXiv:1312.6211, 2013.

[4] Rupesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and Jürgen
Schmidhuber. Compete to compute. In Advances in neural information processing systems,
pages 2310–2318, 2013.

[5] Zoubin Ghahramani. Online variational bayesian learning. In NIPS workshop on Online Learn-

ing, 2000.

[6] Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C Wilson, and Michael I Jordan.
Streaming variational bayes. In Advances in Neural Information Processing Systems, pages
1727–1735, 2013.

[7] Zhizhong Li and Derek Hoiem. Learning without forgetting.

In European Conference on

Computer Vision, pages 614–629. Springer, 2016.

[8] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, An-
drei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.
Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy
of Sciences, 2017.

[9] David JC MacKay. A practical bayesian framework for backpropagation networks. Neural

computation, 4(3):448–472, 1992.

[10] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncer-
tainty in neural network. In Proceedings of the 32nd International Conference on Machine
Learning (ICML-15), pages 1613–1622, 2015.

[11] Jacob Goldberger and Sam T Roweis. Hierarchical clustering of a mixture model. In Advances

in Neural Information Processing Systems, pages 505–512, 2005.

[12] Sang-Woo Lee, Chung-Yeon Lee, Dong Hyun Kwak, Jiwon Kim, Jeonghee Kim, and Byoung-
Tak Zhang. Dual-memory deep learning architectures for lifelong learning of everyday human
In Twenty-Fifth International Joint Conference on Artiﬁcial Intelligencee, pages
behaviors.
1669–1675, 2016.

[13] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,
Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv
preprint arXiv:1606.04671, 2016.

[14] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu,
Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super
neural networks. arXiv preprint arXiv:1701.08734, 2017.

[15] Zhen Huang, Jinyu Li, Sabato Marco Siniscalchi, I-Fan Chen, Chao Weng, and Chin-Hui Lee.
Feature space maximum a posteriori linear regression for adaptation of deep neural networks.
In Fifteenth Annual Conference of the International Speech Communication Association, 2014.

[16] Zhen Huang, Sabato Marco Siniscalchi, I-Fan Chen, Jinyu Li, Jiadong Wu, and Chin-Hui Lee.
Maximum a posteriori adaptation of network parameters in deep models. In Sixteenth Annual
Conference of the International Speech Communication Association, 2015.

[17] Abdullah Rashwan, Han Zhao, and Pascal Poupart. Online and distributed bayesian moment
matching for parameter learning in sum-product networks. In Proceedings of the 19th Interna-
tional Conference on Artiﬁcial Intelligence and Statistics, pages 1469–1477, 2016.

[18] Kai Zhang and James T Kwok. Simplifying mixture models through function approximation.

Neural Networks, IEEE Transactions on, 21(4):644–658, 2010.

10

[19] Manas Pathak, Shantanu Rane, and Bhiksha Raj. Multiparty differential privacy via aggre-
gation of locally trained classiﬁers. In Advances in Neural Information Processing Systems,
pages 1876–1884, 2010.

[20] Pierre Baldi and Peter J Sadowski. Understanding dropout. In Advances in Neural Information

Processing Systems, pages 2814–2822, 2013.

[21] Surajit Ray and Bruce G Lindsay. The topography of multivariate normal mixtures. Annals of

Statistics, pages 2042–2065, 2005.

preprint arXiv:1301.3584, 2013.

[22] Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv

[23] Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural

network optimization problems. arXiv preprint arXiv:1412.6544, 2014.

[24] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features
in deep neural networks? In Advances in neural information processing systems, pages 3320–
3328, 2014.

[25] Theodoros Evgeniou and Massimiliano Pontil. Regularized multi–task learning. In Proceed-
ings of the tenth ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 109–117. ACM, 2004.

[26] Wolf Kienzle and Kumar Chellapilla. Personalized handwriting recognition via biased regu-
larization. In Proceedings of the 23rd international conference on Machine learning, pages
457–464. ACM, 2006.

[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine
Learning Research, 15(1):1929–1958, 2014.

[28] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The

caltech-ucsd birds-200-2011 dataset. Tech. Rep. CNS-TR-2011-001, 2011.

[29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems, pages
1097–1105, 2012.

[30] Sang-Woo Lee, Chung-Yeon Lee, Dong-Hyun Kwak, Jung-Woo Ha, Jeonghee Kim, and
Byoung-Tak Zhang. Dual-memory neural networks for modeling cognitive activities of hu-
mans via wearable sensors. Neural Networks, 2017.

[31] Christos Louizos and Max Welling. Structured and efﬁcient variational deep learning with

matrix gaussian posteriors. arXiv preprint arXiv:1603.04733, 2016.

[32] Surajit Ray and Dan Ren. On the upper bound of the number of modes of a multivariate normal

mixture. Journal of Multivariate Analysis, 108:41–52, 2012.

[33] Carlos Améndola, Alexander Engström, and Christian Haase. Maximum number of modes of

gaussian mixtures. arXiv preprint arXiv:1702.05066, 2017.

[34] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114, 2013.

11

APPENDIX A. Modes in the Mixture of Gaussian

According to Ray and Lindsay [21], all the critical points θ of a mixture of Gaussian (MoG) with
two components are in one curve as the following equation with 0 < α < 1.

θ = ((1 − α)Σ−1

1 + αΣ−1

2 )−1((1 − α)Σ−1

1 µ1 + αΣ−1

2 µ2)

The proof is as follows. Imagine two Gaussian distribution q1 and q2, such as in Equation 2.

q1 ≡ q1(θ; µ1, Σ1) =

exp

−

(θ − µ1)T Σ−1

1 (θ − µ1)

q2 ≡ q2(θ; µ2, Σ2) =

exp

−

(θ − µ2)T Σ−1

1 (θ − µ2)

1
(cid:112)(2π)D|Σ1|
1
(cid:112)(2π)D|Σ2|

(cid:18)

(cid:18)

1
2

1
2

(cid:19)

(cid:19)

D is the dimension of the Gaussian distribution. Mixture of two Gaussian q1 and q2 with the equal
mixing ratio (i.e., 1:1) is q1/2 + q2/2. The derivation of the MoG is as follows:

∂(q1/2 + q2/2)
∂θ

q1
2

= −

(Σ−1

1 (θ − µ1)) −

(Σ−1

2 (θ − µ2)) = 0

q2
2

If we set Equation 15 to 0, to ﬁnd all critical points, the following equation holds:

θ = (q1Σ−1

1 + q2Σ−1

2 )−1(q1Σ−1

1 µ1 + q2Σ−1

2 µ2)

When α is set to

, Equation 12 holds.

q2
q1+q2

Note that αk is a function of θ, so θ cannot be calculated in a closed-form from Equation 16. How-
ever, the optimal θ is in the set {θ|θ = ((1 − α)Σ−1
2 µ2), 0 <
α < 1}, which motivates our mode-IMM method.

2 )−1((1 − α)Σ−1

1 µ1 + αΣ−1

1 + αΣ−1

In our study IMM uses diagonal covariance matrices, which means that there is no correlation be-
tween parameters. This diagonal assumption is useful, since it decreases the number of parameters
for each covariance matrix from O(D2) to O(D). Based on this, the θ in Equation 12 is deﬁned as
follows:

θv =

(1 − α) · µ1,v/σ2
(1 − α)/σ2

1,v + α · µ2,v/σ2
2,v
1,v + α/σ2
2,v

v denotes an index of the parameter vector. µ·,v and σ2
For MoG with two components in K dimension, the number of modes can be at most K + 1 [32].
Therefore, it is hard to ﬁnd all modes in high-dimensional Gaussian in general.

·,v are scalar.

The property of critical points of a MoG with two components can be extended to the case of K
components. The following equation holds:

θ = (

αkΣ−1

k )−1(

αkΣ−1

k µk),

K
(cid:88)

k=1

K
(cid:88)

k=1

where 0 < αk < 1 for all k and (cid:80)
of MoG in general. There is a guess that, for all D, K ≥ 1, the upper bound is (D+K−1)CD [33].

k αk = 1. There is no tight upper bound on the number of modes

APPENDIX B. Bayesian Neural Networks and Continual Learning

Bayesian Neural Networks. Bayesian neural networks (BNN) assume an uncertainty for the whole
parameter in neural networks so that the posterior distribution can be obtained [10]. Previous studies

(12)

(13)

(14)

(15)

(16)

(17)

(18)

12

Algorithm 1 IMM with weight-transfer, L2-transfer

Input: data {(X1, y1),...,(XK, yK)}, balancing hyperparameter α
Output: w1:K
w0 ← InitializeNN()
for k = 1:K do
wk∗ ← wk−1
Train(wk∗, Xk, yk) with L(wk∗, Xk, yk) + λ · ||wk∗ − wk−1||2
2
if type is mean-IMM then

w1:k ← (cid:80)k

t αtwt∗

else if type is mode-IMM then

Fk∗ ← CalculateFisherMatrix(wk∗, Xk, yk)
Σ1:k ← ((cid:80)k
w1:k ← Σ1:k · ((cid:80)k

t αtFt∗)−1

t αtFt∗wt∗)

end if
end for

have argued that BNN regularizes better than NN, and provides a conﬁdence interval for the output
estimation of each input instance. Current research on BNN, to the best of our knowledge, uses
Gaussian distributions as the posteriors of the parameters. In the Gaussian assumption, because
tracking the entire information of a covariance matrix is too expensive, researchers usually use only
the diagonal term for the covariance matrix, where the posterior distribution is fully factorized for
each parameter. However, the methods using full covariance were also suggested recently [31]. To
estimate a covariance matrix most studies use stochastic gradient variational Bayes (SGVB), where
a sampled point from the posterior distribution by Monte Carlo is used in the training phases [34].
Alternatively, Kirkpatrick et al. [8] approximated the covariance matrix as an inverse of a Fisher
matrix. This approximation makes the computational cost of the inference of a covariance matrix
cheaper when the update of covariance information is not needed in the training phase. Our method
follows the approach using the Fisher matrix.

Elastic Weight Consolidation. We compare the work of Kirkpatrick et al. [8] to the results of our
framework. The mechanism of EWC follows sequential Bayesian estimation. EWC maximizes the
following terms by gradient descent to get the solution µ1:K.

log p1:K ≈ log p(yK|XK, θ) + λ · log p1:(K−1) + C

≈ log p(yK|XK, θ) + λ ·

log q1:k + C

K−1
(cid:88)

k=1

K−1
(cid:88)

k=1

λ
2

·

= log p(yK|XK, θ) −

(θ − µ1:k)T ˜Σ−1

k (θ − µ1:k) + C (cid:48)

(19)

pk is empirical posterior distribution of kth task, and qk ∼ N (µk, Σk) is an approximation of pk. In
EWC, ˜Σ−1
is also approximated by the diagonal term of Fisher matrix ˜Fk with respect to µ1:k and
k
Xk.

When moving to a third task, EWC uses the penalty term of both ﬁrst and second network (i.e., µ1
and µ1:2). Although this heuristic works reasonably in the experiments in their paper, it does not
match to the philosophy of Bayesian.

Learning without Forgetting. We compare the work of Li and Hoiem [7]. Although LwF does not
explicitly assume Bayesian, the approach can be represented nonetheless as follows:

log p1:K ≈ log p(yK|XK, θ) + λ ·

log p(ˆyk|XK, θ)

(20)

Where ˆyk is the output from µk with input XK. This framework is promising where the properties
of a pseudo training set of kth task (XK, ˆyk) is similar to the ideal training set (Xk, yk).

K−1
(cid:88)

k=1

13

APPENDIX C. Example Algorithms of Incremental Moment Matching

Two moment matching methods: mean-IMM and mode-IMM, and three transfer learning tech-
niques: weight-transfer, L2-transfer, and drop-transfer, are combined to make various continual
learning algorithms in our study. Algorithm 1 describes mean-IMM and mode-IMM with weight-
transfer and L2-transfer.

APPENDIX D. Experimental Details

Appendix D further explains following issues, 1) additional explanation of the untuned setting and
tuned setting 2) techniques for IMM with a different class output layer for each task 3) other exper-
imental details.

D.1 Disjoint MNIST Experiment

We ﬁrst explain the untuned setting and the tuned setting in detail. The untuned setting refers to the
most natural hyperparameter in the equation of each algorithm, whereas the tuned setting refers to
using heuristic hand-tuned hyperparameters. For mean-IMM, it is most natural to evenly average
K models and 1/K is the most natural αk value for K sequential tasks. For EWC, 1 is the most
natural λ value in Equation 19, because EWC is derived from the equation of sequential Bayesian.
For L2-transfer, there is no natural hyperparameter value in Equation 10, so we need to heuristically
choose a λ value, which is not too small but does not damage the performance of the new network
for the new task.

In the SGD, the number of epochs for the dataset (epoch per dataset) for the second task corresponds
to the hyperparameter. The unit is how much of the network is trained from the whole data at once.
In the L2-transfer and EWC, λ in Equations 10 and 19 corresponds to their hyperparameter. In the
mean-IMM and mode-IMM, αK in Equations 4 and 7 corresponds to the hyperparameter. In the
drop-transfer, dropout ratio p in Equation 11 corresponds to the hyperparameter.

All of the explained hyperparameters are devised to balance the information between the old and
new tasks. If λ/(1 + λ) = 1 or α1 = 1, the ﬁnal network of the algorithms is the same as the
network for the ﬁrst task. If 1/(1 + λ) = 1 or αK = 1, the ﬁnal network is the same as the network
for the last task.

We used multi-layer perceptrons (MLP) with [784-800-800-10] as the number of nodes, ReLU as
the activation function, and vanilla SGD as the optimizer for all of the experiments. We set the epoch
per dataset to 10, unless otherwise noted. The entire IMM model uses weight-transfer to smooth the
loss surface of the model. Without weight-transfer, our IMM model does not work at all. In our
experiments, all models only use one 10-way softmax output layer. For only SGD, dropout is used
as proposed in Goodfellow et al. [3], but dropout does not help much.

Each accuracy was measured by averaging the results of 10 experiments. In the experiment, IMM
outperforms comparative models by a signiﬁcant margin. In the tuned experiment, the performance
of the IMM models exceeds 90%, and the performance increases more when more transfer tech-
niques are applied. Among all the models, weight-transfer + L2-transfer + drop-transfer + mode-
IMM performs the best and its performance is greater than 94%. However, the comparative models
fail to reach greater than 90%. Existing regularizer including dropout does not improve the compar-
ative models.

D.2 Shufﬂed MNIST Experiment

The second experiment is the shufﬂed MNIST experiment for three sequential tasks. For the hyper-
parameter of IMM, we set α1 and α2 as the same value, and tune only α3. Table 1 (Bottom) shows
the experimental results. The performance of SGD + dropout and EWC + dropout comes from the
report in [8]. Changing only the epoch does not signiﬁcantly increase the performance in SGD. The
results show that our IMM paradigm performs similarly to EWC in a case where EWC performs
well. Dropout regularization in the task makes both our models and comparative models perform
better.

14

Figure 5: (Left) Illustration of the effect of the strategy of re-weighing on the new last-layer. Mode-
IMM refers to the original mode-IMM devised for the ImageNet2CUB experiments. In naïve mode-
IMM, the second last-layer of the second network is used for the second last-layer of the ﬁnal IMM
model. (Right) The results of mode-IMM with changing the balancing hyperparameter α to the
re-scaled balancing hayperparameter ˆα with the scale of the Fisher matrix of each network.

In our IMM framework, weight-transfer, L2-transfer, and drop-transfer all take µk−1 as the reference
models of the transfer for training µk. In other words, weight-transfer initializes µk with µk−1, L2-
transfer uses a regularization term to minimize the Euclidean distance between µk−1 and µk, and
drop-transfer uses a µk−1 as the zero point of the dropout procedure. All three transfer techniques
can be considered to change the reference point to, for example, µmean
1:(k−1), as previous
works do [8]. However, all these alternatives make performances worse in our shufﬂed MNIST
experiment. We argued that our utilization of transfer techniques is devised not to minimize the
distance between µk−1 and µk, but to help ﬁnd a µk with a smooth and convex-like loss space
between µk−1 and µk.

1:(k−1) or µmode

D.3 ImageNet to Other Image Datasets

When each task needs a different class output layer, IMM requires additional techniques. There is
no counterpart weight matrix in the last-layer of the ﬁrst network representing the second task, nor
the second last-layer of the ﬁrst network. To tackle this problem, we add the training process of the
last-layer ﬁne-tuning model to the IMM procedure; we match the moments of the last-layer ﬁne-
tuning model with the original new network for the new task. Last-layer ﬁne-tuning is the model the
last-layer is only ﬁne-tuned for each new task; thus it does not make a performance loss for the ﬁrst
task, but does not often learn enough for new tasks.

The technique utilizing the last-layer ﬁne-tuning model makes mean-IMM work in the case of dif-
ferent class output layers, but it is not enough for mode-IMM. It is not possible to calculate a proper
Fisher matrix of the second last-layer in the ﬁrst network for the ﬁrst dataset. As the Fisher matrix is
deﬁned with the gradient from the loss of the ﬁrst task, elements of the Fisher matrix have a value of
zero. However, a zero matrix not only is what we do not want but also degenerates the performance
of mode-IMM. To tackle this problem, we apply mean-IMM for the last-layer with a re-scaling. We
change the mixing ratios α1 : α2 to ˆα1 : ˆα2 = α1 : α2 ·
| ˆw1|+| ˆw2| for the re-scaling, where | ˆw1| and
| ˆw2| is the average of the whole element of weight matrix in the layer before the last-layer, in the
ﬁrst and the second task.

| ˆw1|

In our ImageNet2CUB experiment, the moments of the last-layer ﬁne-tuning model and the LwF
model are matched. Though LwF does not perform well in our previous experiments, it is known
that LwF performs well when the size of a new dataset is small relative to the old dataset, as in the
ImageNet2CUB experiment.

Figure 5 (Left) compares the performances of mode-IMM models with different assumptions on the
Fisher matrix. In naïve mode-IMM, the Fisher matrix of the second last-layer of the ﬁrst network
is a zero matrix. In other words, the second last-layer of the ﬁnal naïve mode-IMM is the second
last-layer of the second network. Naïve mode-IMM does not yield a good performance as we expect.

15

Table 3: Experimental results on the Lifelog dataset. Mean-IMM uses weight-transfer. Classiﬁcation
accuracies among different classes (Top) and different subjects (Bottom). In the experiment, our
IMM paradigm achieves competitive results with the approach using an ensemble network, without
additional cost for inference and learning.

Sub-location Activity

Algorithm
Dual memory architecture [12]
Mean-IMM
Mode-IMM
Online ﬁne-tuning
Last-layer ﬁne-tuning
Naïve incremental bagging
Incremental bagging w/ transfer

Location
78.11
77.60
77.14
68.27
74.58
74.48
74.95

Algorithm
Dual memory architecture [12]
Mean-IMM
Mode-IMM
Online ﬁne-tuning
Last-layer ﬁne-tuning
Naïve incremental bagging
Incremental bagging w/ transfer

A
67.02
67.03
67.97
53.01
63.31
62.24
61.21

72.36
73.78
75.76
64.13
69.30
67.18
68.53

B
58.80
57.73
60.12
56.54
55.83
53.57
56.71

52.92
52.74
54.07
50.00
52.22
47.92
49.66

C
77.57
79.35
78.89
72.85
76.97
73.77
75.23

In Figure 5, scaled mode-IMM denotes the results of mode-IMM re-plotted by the ˆα as we de-
ﬁned above. The result shows that re-scaled mode-IMM performs similarly to mean-IMM in the
ImageNet2CUB experiment.

D.4 Lifelog Dataset

The Lifelog dataset is the dataset recorded from Google Glass over 46 days from three participants.
The 660,000 seconds of the egocentric video stream data reﬂects the behaviors of the participants.
The dataset consists of 10 days of training data and 4 days of test data in order of time for each
participant respectively. In the framework of Lee et al. [12], the network can be updated every day,
but a new network can be made for the 3rd, 7th, and 10th day, with training data of 3, 4, and 3 days,
respectively. Following this framework, our network is made in the 3rd, 7th, and 10th day, and then
merged to previously trained networks. Our IMM used AlexNet pretrained by the ImageNet dataset
[29] as the initial network. The experimental results on the Lifelog dataset are in Table 3, where the
performance of models is from Lee et al. [12] except IMM.

16

8
1
0
2
 
n
a
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
3
v
5
7
4
8
0
.
3
0
7
1
:
v
i
X
r
a

Overcoming Catastrophic Forgetting by
Incremental Moment Matching

Sang-Woo Lee1, Jin-Hwa Kim1, Jaehyun Jun1, Jung-Woo Ha2, and Byoung-Tak Zhang1,3

Seoul National University1
Clova AI Research, NAVER Corp2
Surromind Robotics3

{slee,jhkim,jhjun}@bi.snu.ac.kr jungwoo.ha@navercorp.com
btzhang@bi.snu.ac.kr

Abstract

Catastrophic forgetting is a problem of neural networks that loses the information
of the ﬁrst task after training the second task. Here, we propose a method, i.e. in-
cremental moment matching (IMM), to resolve this problem. IMM incrementally
matches the moment of the posterior distribution of the neural network which is
trained on the ﬁrst and the second task, respectively. To make the search space
of posterior parameter smooth, the IMM procedure is complemented by various
transfer learning techniques including weight transfer, L2-norm of the old and the
new parameter, and a variant of dropout with the old parameter. We analyze our ap-
proach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSD-
Birds, and Lifelog datasets. The experimental results show that IMM achieves
state-of-the-art performance by balancing the information between an old and a
new network.

1

Introduction

Catastrophic forgetting is a fundamental challenge for artiﬁcial general intelligence based on neural
networks. The models that use stochastic gradient descent often forget the information of previous
tasks after being trained on a new task [1, 2]. Online multi-task learning that handles such problems
is described as continual learning. This classic problem has resurfaced with the renaissance of deep
learning research [3, 4].

Recently, the concept of applying a regularization function to a network trained by the old task to
learning a new task has received much attention. This approach can be interpreted as an approxima-
tion of sequential Bayesian [5, 6]. Representative examples of this regularization approach include
learning without forgetting [7] and elastic weight consolidation [8]. These algorithms succeeded in
some experiments where their own assumption of the regularization function ﬁts the problem.

Here, we propose incremental moment matching (IMM) to resolve the catastrophic forgetting prob-
lem. IMM uses the framework of Bayesian neural networks, which implies that uncertainty is intro-
duced on the parameters in neural networks, and that the posterior distribution is calculated [9, 10].
The dimension of the random variable in the posterior distribution is the number of the parameters
in the neural networks. IMM approximates the mixture of Gaussian posterior with each component
representing parameters for a single task to one Gaussian distribution for a combined task. To merge
the posteriors, we introduce two novel methods of moment matching. One is mean-IMM, which
simply averages the parameters of two networks for old and new tasks as the minimization of the
average of KL-divergence between one approximated posterior distribution for the combined task

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

Figure 1: Geometric illustration of incremental moment matching (IMM). Mean-IMM simply av-
erages the parameters of two neural networks, whereas mode-IMM tries to ﬁnd a maximum of the
mixture of Gaussian posteriors. To make IMM be reasonable, the search space of the loss function
between the posterior means µ1 and µ2 should be reasonably smooth and convex-like. To ﬁnd a
µ2 which satisﬁes this condition of a smooth and convex-like path from µ1, we propose applying
various transfer techniques for the IMM procedure.

and each Gaussian posterior for the single task [11]. The other is mode-IMM, which merges the pa-
rameters of two networks using a Laplacian approximation [9] to approximate a mode of the mixture
of two Gaussian posteriors, which represent the parameters of the two networks.

In general, it is too naïve to assume that the ﬁnal posterior distribution for the whole task is Gaussian.
To make our IMM work, the search space of the loss function between the posterior means needs to
be smooth and convex-like. In other words, there should not be high cost barriers between the means
of the two networks for an old and a new task. To make our assumption of Gaussian distribution for
neural network reasonable, we applied three main transfer learning techniques on the IMM proce-
dure: weight transfer, L2-norm of the old and the new parameters, and our newly proposed variant
of dropout using the old parameters. The whole procedure of IMM is illustrated in Figure 1.

2 Previous Works on Catastrophic Forgetting

One of the major approaches preventing catastrophic forgetting is to use an ensemble of neural net-
works. When a new task arrives, the algorithm makes a new network, and shares the representation
between the tasks [12, 13]. However, this approach has a complexity issue, especially in inference,
because the number of networks increases as the number of new tasks that need to be learned in-
creases.

Another approach studies the methods using implicit distributed storage of information, in typical
stochastic gradient descent (SGD) learning. These methods use the idea of dropout, maxout, or neu-
ral module to distributively store the information for each task by making use of the large capacity of
the neural network [4]. Unfortunately, most studies following this approach had limited success and
failed to preserve performance on the old task when an extreme change to the environment occurred
[3]. Alternatively, Fernando et al. [14] proposed PathNet, which extends the idea of the ensemble
approach for parameter reuse [13] within a single network. In PathNet, a neural network has ten or
twenty modules in each layer, and three or four modules are picked for one task in each layer by
an evolutionary approach. This method alleviates the complexity issue of the ensemble approach to
continual learning in a plausible way.

The approach with a regularization term also has received attention. Learning without forgetting
(LwF) is one example of this approach, which uses the pseudo-training data from the old task [7].
Before learning the new task, LwF puts the training data of the new task into the old network,
and uses the output as pseudo-labels of the pseudo-training data. By optimizing both the pseudo-
training data of the old task and the real data of the new task, LwF attempts to prevent catastrophic
forgetting. This framework is promising where the properties of the pseudo training set is similar to
the ideal training set. Elastic weight consolidation (EWC), another example of this approach, uses
sequential Bayesian estimation to update neural networks for continual learning [8]. In EWC, the
posterior distribution trained by the previous task is used to update the new prior distribution. This
new prior is used for learning the new posterior distribution of the new task in a Bayesian manner.

2

EWC assumes that the covariance matrix of the posterior is diagonal and there are no correlations
between the nodes. Though this assumption is fragile, EWC performs well in some domains.

EWC is a monumental recent work that uses sequential Bayesian for continual learning of neural
networks. However, updating the parameter of complex hierarchical models by sequential Bayesian
estimation is not new [5]. Sequential Bayes was used to learn topic models from stream data by
Broderick et al. [6]. Huang et al. applied sequential Bayesian to adapt a deep neural network to
the speciﬁc user in the speech recognition domain [15, 16]. They assigned the layer for the user
adaptation and applied MAP estimation to this single layer. Similar to our IMM method, Bayesian
moment matching is used for sum-product networks, a kind of deep hierarchical probabilistic model
[17]. Though sum-product networks are usually not scalable to large datasets, their online learning
method is useful, and it achieves similar performance to the batch learner. Our method using moment
matching focuses on continual learning and deals with signiﬁcantly different statistics between tasks,
unlike the previous method.

3

Incremental Moment Matching

In incremental moment matching (IMM), the moments of posterior distributions are matched in an
incremental way. In our work, we use a Gaussian distribution to approximate the posterior distri-
bution of parameters. Given K sequential tasks, we want to ﬁnd the optimal parameter µ∗
1:K and
Σ∗
1:K of the Gaussian approximation function q1:K from the posterior parameter for each kth task,
(µk, Σk).

p1:K ≡ p(θ|X1, · · · , XK, y1, · · · , yK) ≈ q1:K ≡ q(θ|µ1:K, Σ1:K)
pk ≡ p(θ|Xk, yk) ≈ qk ≡ q(θ|µk, Σk)

(1)
(2)

q1:K denotes an approximation of the true posterior distribution p1:K for the whole task, and qk
denotes an approximation of the true posterior distribution pk over the training dataset (Xk, yk) for
the kth task. θ denotes the vectorized parameter of the neural network. The dimension of µk and
µ1:k is D, and the dimension of Σk and Σ1:k is D × D, respectively, where D is the dimension of θ.
For example, a multi-layer perceptrons (MLP) with [784-800-800-800-10] has the number of nodes,
D = 1917610 including bias terms.

Next, we explain two proposed moment matching algorithms for the continual learning of modern
deep neural networks. The two algorithms generate two different moments of Gaussian with different
objective functions for the same dataset.

3.1 Mean-based Incremental Moment Matching (mean-IMM)

Mean-IMM averages the parameters of two networks in each layer, using mixing ratios αk with
(cid:80)K
k αk = 1. The objective function of mean-IMM is to minimize the following local KL-distance
or the weighted sum of KL-divergence between each qk and q1:K [11, 18]:

1:K, Σ∗
µ∗

k αkKL(qk||q1:K)

(cid:80)K

1:K = argmin
µ1:K ,Σ1:K
1:K = (cid:80)K
µ∗
k αk(Σk + (µk − µ∗

k αkµk

1:K = (cid:80)K
Σ∗

1:K)(µk − µ∗

1:K)T )

(3)

(4)

(5)

µ∗
1:K and Σ∗
1:K are the optimal solution of the local KL-distance. Notice that covariance information
is not needed for mean-IMM, since calculating µ∗
1:K does not require any Σk. A series of µk is
sufﬁcient to perform the task. The idea of mean-IMM is commonly used in shallow networks [19,
20]. However, the contribution of this paper is to discover when and how mean-IMM can be applied
in modern deep neural networks and to show it can performs better with other transfer techniques.

Future works may include other measures to merge the networks, including the KL-divergence be-
tween q1:K and the mixture of each qk (i.e. KL(q1:K||(cid:80)K

k αkqk)) [18].

3

3.2 Mode-based Incremental Moment Matching (mode-IMM)

Mode-IMM is a variant of mean-IMM which uses the covariance information of the posterior of
Gaussian distribution. In general, a weighted average of two mean vectors of Gaussian distributions
is not a mode of MoG. In discriminative learning, the maximum of the distribution is of primary
interest. According to Ray and Lindsay [21], all the modes of MoG with K clusters lie on (K − 1)-
dimensional hypersurface {θ|θ = ((cid:80)K
k )−1((cid:80)K
k ak = 1}.
See Appendix A for more details.

k µk), 0 < ak < 1 and (cid:80)

k akΣ−1

k akΣ−1

Motivated by the above description, a mode-IMM approximate MoG with Laplacian approximation,
in which the logarithm of the function is expressed by the Taylor expansion [9]. Using Laplacian
approximation, the MoG is approximated as follows:

log q1:K ≈ (cid:80)K

k αk log qk + C = −

θT ((cid:80)K

k αkΣ−1

k )θ + ((cid:80)K

k αkΣ−1

k µk)θ + C (cid:48)

1
2

1:K · ((cid:80)K

1:K = Σ∗
µ∗
1:K = ((cid:80)K
Σ∗

k αkΣ−1
k )−1

k αkΣ−1

k µk)

(6)

(7)

(8)

For Equation 8, we add (cid:15)I to the term to be inverted in practice, with an identity matrix I and a small
constant (cid:15).

Here, we assume diagonal covariance matrices, which means that there is no correlation among
parameters. This diagonal assumption is useful, since it decreases the number of parameters for
each covariance matrix from O(D2) to O(D) for the dimension of the parameters D.

For covariance, we use the inverse of a Fisher information matrix, following [8, 22]. The main
idea of this approximation is that the square of gradients for parameters is a good indicator of their
precision, which is the inverse of the variance. The Fisher information matrix for the kth task, Fk is
deﬁned as:

Fk = E

ln p(˜y|x, µk) ·

ln p(˜y|x, µk)T

(9)

(cid:20) ∂
∂µk

∂
∂µk

(cid:21)

,

where the probability of the expectation follows x ∼ πk and ˜y ∼ p(y|x, µk), where πk denotes an
empirical distribution of Xk.

4 Transfer Techniques for Incremental Moment Matching

In general, the loss function of neural networks is not convex. Consider that shufﬂing nodes and
their weights in a neural network preserves the original performance. If the parameters of two neural
networks initialized independently are averaged, it might perform poorly because of the high cost
barriers between the parameters of the two neural networks [23]. However, we will show that various
transfer learning techniques can be used to ease this problem, and make the assumption of Gaussian
distribution for neural networks reasonable. In this section, we introduce three practical techniques
for IMM, including weight-transfer, L2-transfer, and drop-transfer.

4.1 Weight-Transfer

Weight-transfer initialize the parameters for the new task µk with the parameters of the previous
task µk−1 [24]. In our experiments, the use of weight-transfer was critical to the continual learn-
ing performance. For this reason, the experiments on IMM in this paper use the weight-transfer
technique by default.

The weight-transfer technique is motivated by the geometrical property of neural networks discov-
ered in the previous work [23]. They found that there is a straight path from the initial point to the
solution without any high cost barrier, in various types of neural networks and datasets. This dis-
covery suggests that the weight-transfer from the previous task to the new task makes a smooth loss

4

Figure 2: Experimental results on visualizing the effect of weight-transfer. The geometric property
of the parameter space of the neural network is analyzed. Brighter is better. θ1, θ2, and θ3 are the
vectorized parameters of trained networks from randomly selected subsets of the CIFAR-10 dataset.
This ﬁgure shows that there are better solutions between the three locally optimized parameters.

surface between two solutions for the tasks, so that the optimal solution for both tasks lies on the
interpolated point of the two solutions.

To empirically validate the concept of weight-transfer, we use the linear path analysis proposed by
Goodfellow et al. [23] (Figure 2). We randomly chose 18,000 instances from the training dataset
of CIFAR-10, and divided them into three subsets of 6,000 instances each. These three subsets are
used for sequential training by CNN models, parameterized by θ1, θ2, and θ3, respectively. Here, θ2
is initialized from θ1, and then θ3 is initialized from θ2, in the same way as weight-transfer. In this
analysis, each loss and accuracy is evaluated at a series of points θ = θ1 + α(θ2 − θ1) + β(θ3 −
θ2), varying α and β. In Figure 2, the loss surface of the model on each online subset is nearly
convex. The ﬁgure shows that the parameter at 1
3 (θ1 + θ2 + θ3), which is the same as the solution
of mean-IMM, performs better than any other reference points θ1, θ2, or θ3. However, when θ2 is
not initialized by θ1, the convex-like shape disappears, since there is a high cost barrier between the
loss function of θ1 and θ2.

4.2 L2-transfer

L2-transfer is a variant of L2-regularization. L2-transfer can be interpreted as a special case of
EWC where the prior distribution is Gaussian with λI as a covariance matrix. In L2-transfer, a
regularization term of the distance between µk−1 and µk is added to the following objective function
for ﬁnding µk, where λ is a hyperparameter:

log p(yk|Xk, µk) − λ · ||µk − µk−1||2
2

(10)

The concept of L2-transfer is commonly used in transfer learning [25, 26] and continual learning
[7, 8] with large λ. Unlike the previous usage of large λ, we use small λ for the IMM procedure.
In other words, µk is ﬁrst trained by Equation 10 with small λ, and then merged to µ1:k in our
IMM. Since we want to make the loss surface between µk−1 and µk smooth, and not to minimize
the distance between µk−1 and µk. In convex optimization, the L2-regularizer makes the convex
function strictly convex. Similarly, we hope L2-transfer with small λ help to ﬁnd a µk with a convex-
like loss space between µk−1 and µk.

4.3 Drop-transfer

Drop-transfer is a novel method devised in this paper. Drop-transfer is a variant of dropout where
µk−1 is the zero point of the dropout procedure. In the training phase, the following ˆµk,i is used for
the weight vector corresponding to the ith node µk,i:

(cid:40)

ˆµk,i =

µk−1,i,
1−p · µk,i − p

1

1−p · µk−1,i, otherwise

if ith node is turned off

(11)

where p is the dropout ratio. Notice that the expectation of ˆµk,i is µk,i.

5

Table 1: The averaged accuracies on the disjoint MNIST for two sequential tasks (Top) and the
shufﬂed MNIST for three sequential tasks (Bottom). The untuned setting refers to the most natural
hyperparameter in the equation of each algorithm, whereas the tuned setting refers to using heuristic
hand-tuned hyperparameters. Hyperparam denotes the main hyperparameter of each algorithm. For
IMM with transfer, only α is tuned. The numbers in the parentheses refer to standard deviation.
Every IMM uses weight-transfer.

Disjoint MNIST Experiment
SGD [3]
L2-transfer [25]
Drop-transfer
EWC [8]
Mean-IMM
Mode-IMM
L2-transfer + Mean-IMM
L2-transfer + Mode-IMM
Drop-transfer + Mean-IMM
Drop-transfer + Mode-IMM
L2, Drop-transfer + Mean-IMM
L2, Drop-transfer + Mode-IMM

Shufﬂed MNIST Experiment
SGD [3]
L2-transfer [25]
Drop-transfer
EWC [8]
Mean-IMM
Mode-IMM
L2-transfer + Mean-IMM
L2-transfer + Mode-IMM
Drop-transfer + Mean-IMM
Drop-transfer + Mode-IMM
L2, Drop-transfer + Mean-IMM
L2, Drop-transfer + Mode-IMM

Explanation of
Hyperparam
epoch per dataset
λ in (10)
p in (11)
λ in (20)
α2 in (4)
α2 in (7)
λ / α2
λ / α2
p / α2
p / α2
λ / p / α2
λ / p / α2

epoch per dataset
λ in (10)
p in (11)
λ in (20)
α3 in (4)
α3 in (7)
λ / α3
λ / α3
p / α3
p / α3
λ / p / α3
λ / p / α3

Untuned

Tuned

Hyperparam
10
-
0.5
1.0
0.50
0.50
0.001 / 0.50
0.001 / 0.50
0.5 / 0.50
0.5 / 0.50
0.001 / 0.5 / 0.50
0.001 / 0.5 / 0.50

Hyperparam
60
-
0.5
-
0.33
0.33
1e-4 / 0.33
1e-4 / 0.33
0.5 / 0.33
0.5 / 0.33
1e-4 / 0.5 / 0.33
1e-4 / 0.5 / 0.33

Accuracy
47.72 (± 0.11)
-
51.72 (± 0.79)
47.84 (± 0.04)
90.45 (± 2.24)
91.49 (± 0.98)
78.34 (± 1.82)
92.52 (± 0.41)
80.75 (± 1.28)
93.35 (± 0.49)
66.10 (± 3.19)
93.97 (± 0.32)

Accuracy
89.15 (± 2.34)
-
94.75 (± 0.62)
-
93.23 (± 1.37)
98.02 (± 0.05)
90.38 (± 1.74)
98.16 (± 0.08)
90.79 (± 1.30)
97.80 (± 0.07)
89.51 (± 2.85)
97.83 (± 0.10)

Hyperparam
0.05
0.05
0.5
600M
0.55
0.45
0.001 / 0.60
0.001 / 0.45
0.5 / 0.60
0.5 / 0.50
0.001 / 0.5 / 0.75
0.001 / 0.5 / 0.45

Hyperparam
-
1e-3
0.2
-
0.55
0.60
1e-4 / 0.65
1e-4 / 0.60
0.5 / 0.65
0.5 / 0.55
1e-4 / 0.5 / 0.90
1e-4 / 0.5 / 0.50

Accuracy
71.32 (± 1.54)
85.81 (± 0.52)
51.72 (± 0.79)
52.72 (± 1.36)
91.92 (± 0.98)
92.02 (± 0.73)
92.62 (± 0.95)
92.73 (± 0.35)
92.64 (± 0.60)
93.35 (± 0.49)
93.97 (± 0.23)
94.12 (± 0.27)

Accuracy
∼95.5 [8]
96.37 (± 0.62)
96.86 (± 0.21)
∼98.2 [8]
95.02 (± 0.42)
98.08 (± 0.08)
95.93 (± 0.31)
98.30 (± 0.08)
96.49 (± 0.44)
97.95 (± 0.08)
97.36 (± 0.19)
97.92 (± 0.05)

There are studies [27, 20] that have interpreted dropout as an exponential ensemble of weak learners.
By this perspective, since the marginalization of output distribution over the whole weak learner is
intractable, the parameters multiplied by the inverse of the dropout rate are used for the test phase
in the procedure. In other words, the parameters of the weak learners are, in effect, simply averaged
oversampled learners by dropout. At the process of drop-transfer in our continual learning setting,
we hypothesize that the dropout process makes the averaged point of two arbitrary sampled points
using Equation 11 a good estimator.

We investigated the search space of the loss function of the MLP trained from the MNIST handwrit-
ten digit recognition dataset for with and without dropout regularization, to supplement the evidence
of the described hypothesis. Dropout regularization makes the accuracy of a sampled point from
dropout distribution and an average point of two sampled parameters, from 0.450 (± 0.084) to 0.950
(± 0.009) and 0.757 (± 0.065) to 0.974 (± 0.003), respectively. For the case of both with and without
dropout, the space between two arbitrary samples is empirically convex, and ﬁts to the second-order
equation. Based on this experiment, we expect not only that the search space of the loss function
between modern neural networks can be easily nearly convex [23], but also that regularizers, such
as dropout, make the search space smooth and the point in the search space have a good accuracy in
continual learning.

5 Experimental Results

We evaluate our approach on four experiments, whose settings are intensively used in the previous
works [4, 8, 7, 12]. For more details and experimental results, see Appendix D. The source code for
the experiments is available in Github repository1.

Disjoint MNIST Experiment. The ﬁrst experiment is the disjoint MNIST experiment [4]. In this
experiment, the MNIST dataset is divided into two datasets: the ﬁrst dataset consists of only digits
{0, 1, 2, 3, 4} and the second dataset consists of the remaining digits {5, 6, 7, 8, 9}. Our task is 10-

1https://github.com/btjhjeon/IMM_tensorﬂow

6

Figure 3: Test accuracies of two IMM models with weight-transfer on the disjoint MNIST (Left),
the shufﬂed MNIST (Middle), and the ImageNet2CUB experiment (Right). α is a hyperparameter
that balances the information between the old and the new task.

Figure 4: Test accuracies of IMM with various transfer techniques on the disjoint MNIST. Both L2-
transfer and drop-transfer boost the performance of IMM and make the optimal value of α larger
than 1/2. However, drop-transfer tends to make the accuracy curve more smooth than L2-transfer
does.

class joint categorization, unlike the setting in the previous work, which considers two independent
tasks of 5-class categorization. Because the inference should decide whether a new instance comes
from the ﬁrst or the second task, our task is more difﬁcult than the task of the previous work.

We evaluate the models both on the untuned setting and the tuned setting. The untuned setting refers
to the most natural hyperparameter in the equation of each algorithm. The tuned setting refers to
using heuristic hand-tuned hyperparameters. Consider that tuned hyperparameter setting is often
used in previous works of continual learning as it is difﬁcult to deﬁne a validation set in their setting.
For example, when the model needs to learn from the new task after learning from the old task, a low
learning rate or early stopping without a validation set, or arbitrary hyperparameter for balancing is
used [3, 8]. We discover hyperparameters in the tuned setting not only to ﬁnd the oracle performance
of each algorithm, but also to show that there exist some paths consisting of the point that performs
reasonably for both tasks. Hyperparam in Table 1 denotes hyperparameter mainly searched in the
tuned setting. Table 1 (Top) and Figure 3 (Left) shows the experimental results from the disjoint
MNIST experiment.

In our experimental setting, the usual SGD-based optimizers always perform less than 50%, because
the biases of the output layer for the old task are always pushed to large negative values, which
implies that our task is difﬁcult. Figure 4 also shows that mode-IMM is robust with α and the
optimal α of mean-IMM is larger than 1/2 in the disjoint MNIST experiment.

Shufﬂed MNIST Experiment. The second experiment is the shufﬂed MNIST experiment [3, 8] of
three sequential tasks. In this experiment, the ﬁrst dataset is the same as the original MNIST dataset.
However, in the second dataset, the input pixels of all images are shufﬂed with a ﬁxed, random per-
mutation. In previous work, EWC reaches the performance level of the batch learner, and it is argued
that EWC overcomes catastrophic forgetting in some domains. The experimental details are similar
to the disjoint MNIST experiment, except all models are allowed to use dropout regularization. In
the experiment, the ﬁrst dataset is the same as the original MNIST dataset. However, in the second
and the third dataset, the input pixels of all images are shufﬂed with a ﬁxed, random permutation,

7

Table 2: Experimental results on the Lifelog dataset among different classes (location, sub-location,
and activity) and different subjects (A, B, C). Every IMM uses weight-transfer.

Dual memory architecture [12]
Mean-IMM
Mode-IMM

Location
78.11
77.60
77.14

Sub-location Activity

72.36
73.78
75.76

52.92
52.74
54.07

A
67.02
67.03
67.97

B
58.80
57.73
60.12

C
77.57
79.35
78.89

respectively. Therefore, the difﬁculty of the three datasets is the same, though a different solution is
required for each dataset.

Table 1 (Bottom) and Figure 3 (Middle) shows the experimental results from the shufﬂed MNIST
experiment. Notice that accuracy of drop-transfer (p = 0.2) alone is 96.86 (± 0.21) and L2-transfer
(λ = 1e-4) + drop-transfer (p = 0.4) alone is 97.61 (± 0.15). These results are competitive to EWC
without dropout, whose performance is around 97.0.

ImageNet to CUB Dataset. The third experiment is the ImageNet2CUB experiment [7], the con-
tinual learning problem from the ImageNet dataset to the Caltech-UCSD Birds-200-2011 ﬁne-
grained classiﬁcation (CUB) dataset [28]. The numbers of classes of ImageNet and CUB dataset
are around 1K and 200, and the numbers of training instances are 1M and 5K, respectively. In the
ImageNet2CUB experiment, the last-layer is separated for the ImageNet and the CUB task. The
structure of AlexNet is used for the trained model of ImageNet [29]. In our experiment, we match
the moments of the last-layer ﬁne-tuning model and the LwF model, with mean-IMM and mode-
IMM.

Figure 3 (Right) shows that mean-IMM moderately balances the performance of two tasks between
two networks. However, the balanced hyperparameter of mode-IMM is far from α = 0.5. We think
that it is because the scale of the Fisher matrix F is different between the ImageNet and the CUB
task. Since the number of training data of the two tasks is different, the mean of the square of the
gradient, which is the deﬁnition of F , tends to be different. This implies that the assumption of
mode-IMM does not always hold for heterogeneous tasks. See Appendix D.3 for more information
including the learning methods of IMM where a different class output layer or a different scale of
the dataset is used.

Our results of IMM with LwF exceed the previous state-of-the-art performance, whose model is
also LwF. This is because, in the previous works, the LwF model is initialized by the last-layer ﬁne-
tuning model, not directly by the original AlexNet. In this case, the performance loss of the old task
is not only decreased, but also the performance gain of the new task is decreased. The accuracies of
our mean-IMM (α = 0.5) are 56.20 and 56.73 for the ImageNet task and the CUB task, respectively.
The gains compared to the previous state-of-the-art are +1.13 and -1.14. In the case of mean-IMM
(α = 0.8) and mode-IMM (α = 0.99), the accuracies are 55.08 and 59.08 (+0.01, +1.12), and 55.10
and 59.12 (+0.02, +1.35), respectively.

Lifelog Dataset. Lastly, we evaluate the proposed methods on the Lifelog dataset [12]. The Lifelog
dataset consists of 660,000 instances of egocentric video stream data, collected over 46 days from
three participants using Google Glass [30]. Three class categories, location, sub-location, and activ-
ity, are labeled on each frame of video. In the Lifelog dataset, the class distribution changes con-
tinuously and new classes appear as the day passes. Table 2 shows that mean-IMM and mode-IMM
are competitive to the dual-memory architecture, the previous state-of-the-art ensemble model, even
though IMM uses single network.

6 Discussion

A Shift of Optimal Hyperparameter of IMM. The tuned setting shows there often exists some α
which makes the performance of the mean-IMM close to the mode-IMM. However, in the untuned
hyperparameter setting, mean-IMM performs worse when more transfer techniques are applied. Our
Bayesian interpretation in IMM assumes that the SGD training of the k-th network µk is mainly
affected by the k-th task and is rarely affected by the information of the previous tasks. However,
transfer techniques break this assumption; thus the optimal α is shifted to larger than 1/k. For-
tunately, mode-IMM works more robustly than mean-IMM where transfer techniques are applied.

8

Figure 4 illustrates the change of the test accuracy curve corresponding to the applied transfer tech-
niques and the following shift of the optimal α in mean-IMM and mode-IMM.

Bayesian Approach on Continual Learning. Kirkpatrick et al. [8] interpreted that the Fisher
matrix F as weight importance in explaining their EWC model. In the shufﬂed MNIST experiment,
since a large number of pixels always have a value of zero, the corresponding elements of the Fisher
matrix are also zero. Therefore, EWC does work by allowing weights to change, which are not used
in the previous tasks. On the other hand, mode-IMM also works by selectively balancing between
two weights using variance information. However, these assumptions on weight importance do not
always hold, especially in the disjoint MNIST experiment. The most important weight in the disjoint
MNIST experiment is the bias term in the output layer. Nevertheless, these bias parts of the Fisher
matrix are not guaranteed to be the highest value nor can they be used to balance the class distribution
between the ﬁrst and second task. We believe that using only the diagonal of the covariance matrix
in Bayesian neural networks is too naïve in general and that this is why EWC failed in the disjoint
MNIST experiment. We think it could be alleviated in future work by using a more complex prior,
such as a matrix Gaussian distribution considering the correlations between nodes in the network
[31].

Balancing the Information of an Old and a New Task. The IMM procedure produces a neural
network without a performance loss for kth task µk, which is better than the ﬁnal solution µ1:k in
terms of the performance of the kth task. Furthermore, IMM can easily weigh the importance of
tasks in IMM models in real time. For example, αt can be easily changed for the solution of mean-
IMM µ1:k = (cid:80)k
t αtµt . In actual service situations of IT companies, the importance of the old
and the new task frequently changes in real time, and IMM can handle this problem. This property
differentiates IMM from the other continual learning methods using the regularization approach,
including LwF and EWC.

7 Conclusion

Our contributions are four folds. First, we applied mean-IMM to the continual learning of modern
deep neural networks. Mean-IMM makes competitive results to comparative models and balances
the information between an old and a new network. We also interpreted the success of IMM by the
Bayesian framework with Gaussian posterior. Second, we extended mean-IMM to mode-IMM with
the interpretation of mode-ﬁnding in the mixture of Gaussian posterior. Mode-IMM outperforms
mean-IMM and comparative models in various datasets. Third, we introduced drop-transfer, a novel
method proposed in the paper. Experimental results showed that drop-transfer alone performs well
and is similar to the EWC without dropout, in the domain where EWC rarely forgets. Fourth, We
applied various transfer techniques in the IMM procedure to make our assumption of Gaussian
distribution reasonable. We argued that not only the search space of the loss function among neural
networks can easily be nearly convex, but also regularizers, such as dropout, make the search space
smooth, and the point in the search space have good accuracy. Experimental results showed that
applying transfer techniques often boost the performance of IMM. Overall, we made state-of-the-
art performance in various datasets of continual learning and explored geometrical properties and a
Bayesian perspective of deep neural networks.

Acknowledgments

The authors would like to thank Jiseob Kim, Min-Oh Heo, Donghyun Kwak, Insu Jeon, Christina
Baek, and Heidi Tessmer for helpful comments and editing. This work was supported by the
Naver Corp. and partly by the Korean government (IITP-R0126-16-1072-SW.StarLab, IITP-2017-0-
01772-VTT, KEIT-10044009-HRI.MESSI, KEIT-10060086-RISF). Byoung-Tak Zhang is the cor-
responding author.

References

[1] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks:
The sequential learning problem. Psychology of learning and motivation, 24:109–165, 1989.

9

[2] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive

sciences, 3(4):128–135, 1999.

[3] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empiri-
cal investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint
arXiv:1312.6211, 2013.

[4] Rupesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and Jürgen
Schmidhuber. Compete to compute. In Advances in neural information processing systems,
pages 2310–2318, 2013.

[5] Zoubin Ghahramani. Online variational bayesian learning. In NIPS workshop on Online Learn-

ing, 2000.

[6] Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C Wilson, and Michael I Jordan.
Streaming variational bayes. In Advances in Neural Information Processing Systems, pages
1727–1735, 2013.

[7] Zhizhong Li and Derek Hoiem. Learning without forgetting.

In European Conference on

Computer Vision, pages 614–629. Springer, 2016.

[8] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, An-
drei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.
Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy
of Sciences, 2017.

[9] David JC MacKay. A practical bayesian framework for backpropagation networks. Neural

computation, 4(3):448–472, 1992.

[10] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncer-
tainty in neural network. In Proceedings of the 32nd International Conference on Machine
Learning (ICML-15), pages 1613–1622, 2015.

[11] Jacob Goldberger and Sam T Roweis. Hierarchical clustering of a mixture model. In Advances

in Neural Information Processing Systems, pages 505–512, 2005.

[12] Sang-Woo Lee, Chung-Yeon Lee, Dong Hyun Kwak, Jiwon Kim, Jeonghee Kim, and Byoung-
Tak Zhang. Dual-memory deep learning architectures for lifelong learning of everyday human
In Twenty-Fifth International Joint Conference on Artiﬁcial Intelligencee, pages
behaviors.
1669–1675, 2016.

[13] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,
Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv
preprint arXiv:1606.04671, 2016.

[14] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu,
Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super
neural networks. arXiv preprint arXiv:1701.08734, 2017.

[15] Zhen Huang, Jinyu Li, Sabato Marco Siniscalchi, I-Fan Chen, Chao Weng, and Chin-Hui Lee.
Feature space maximum a posteriori linear regression for adaptation of deep neural networks.
In Fifteenth Annual Conference of the International Speech Communication Association, 2014.

[16] Zhen Huang, Sabato Marco Siniscalchi, I-Fan Chen, Jinyu Li, Jiadong Wu, and Chin-Hui Lee.
Maximum a posteriori adaptation of network parameters in deep models. In Sixteenth Annual
Conference of the International Speech Communication Association, 2015.

[17] Abdullah Rashwan, Han Zhao, and Pascal Poupart. Online and distributed bayesian moment
matching for parameter learning in sum-product networks. In Proceedings of the 19th Interna-
tional Conference on Artiﬁcial Intelligence and Statistics, pages 1469–1477, 2016.

[18] Kai Zhang and James T Kwok. Simplifying mixture models through function approximation.

Neural Networks, IEEE Transactions on, 21(4):644–658, 2010.

10

[19] Manas Pathak, Shantanu Rane, and Bhiksha Raj. Multiparty differential privacy via aggre-
gation of locally trained classiﬁers. In Advances in Neural Information Processing Systems,
pages 1876–1884, 2010.

[20] Pierre Baldi and Peter J Sadowski. Understanding dropout. In Advances in Neural Information

Processing Systems, pages 2814–2822, 2013.

[21] Surajit Ray and Bruce G Lindsay. The topography of multivariate normal mixtures. Annals of

Statistics, pages 2042–2065, 2005.

preprint arXiv:1301.3584, 2013.

[22] Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv

[23] Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural

network optimization problems. arXiv preprint arXiv:1412.6544, 2014.

[24] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features
in deep neural networks? In Advances in neural information processing systems, pages 3320–
3328, 2014.

[25] Theodoros Evgeniou and Massimiliano Pontil. Regularized multi–task learning. In Proceed-
ings of the tenth ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 109–117. ACM, 2004.

[26] Wolf Kienzle and Kumar Chellapilla. Personalized handwriting recognition via biased regu-
larization. In Proceedings of the 23rd international conference on Machine learning, pages
457–464. ACM, 2006.

[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine
Learning Research, 15(1):1929–1958, 2014.

[28] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The

caltech-ucsd birds-200-2011 dataset. Tech. Rep. CNS-TR-2011-001, 2011.

[29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems, pages
1097–1105, 2012.

[30] Sang-Woo Lee, Chung-Yeon Lee, Dong-Hyun Kwak, Jung-Woo Ha, Jeonghee Kim, and
Byoung-Tak Zhang. Dual-memory neural networks for modeling cognitive activities of hu-
mans via wearable sensors. Neural Networks, 2017.

[31] Christos Louizos and Max Welling. Structured and efﬁcient variational deep learning with

matrix gaussian posteriors. arXiv preprint arXiv:1603.04733, 2016.

[32] Surajit Ray and Dan Ren. On the upper bound of the number of modes of a multivariate normal

mixture. Journal of Multivariate Analysis, 108:41–52, 2012.

[33] Carlos Améndola, Alexander Engström, and Christian Haase. Maximum number of modes of

gaussian mixtures. arXiv preprint arXiv:1702.05066, 2017.

[34] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114, 2013.

11

APPENDIX A. Modes in the Mixture of Gaussian

According to Ray and Lindsay [21], all the critical points θ of a mixture of Gaussian (MoG) with
two components are in one curve as the following equation with 0 < α < 1.

θ = ((1 − α)Σ−1

1 + αΣ−1

2 )−1((1 − α)Σ−1

1 µ1 + αΣ−1

2 µ2)

The proof is as follows. Imagine two Gaussian distribution q1 and q2, such as in Equation 2.

q1 ≡ q1(θ; µ1, Σ1) =

exp

−

(θ − µ1)T Σ−1

1 (θ − µ1)

q2 ≡ q2(θ; µ2, Σ2) =

exp

−

(θ − µ2)T Σ−1

1 (θ − µ2)

1
(cid:112)(2π)D|Σ1|
1
(cid:112)(2π)D|Σ2|

(cid:18)

(cid:18)

1
2

1
2

(cid:19)

(cid:19)

D is the dimension of the Gaussian distribution. Mixture of two Gaussian q1 and q2 with the equal
mixing ratio (i.e., 1:1) is q1/2 + q2/2. The derivation of the MoG is as follows:

∂(q1/2 + q2/2)
∂θ

q1
2

= −

(Σ−1

1 (θ − µ1)) −

(Σ−1

2 (θ − µ2)) = 0

q2
2

If we set Equation 15 to 0, to ﬁnd all critical points, the following equation holds:

θ = (q1Σ−1

1 + q2Σ−1

2 )−1(q1Σ−1

1 µ1 + q2Σ−1

2 µ2)

When α is set to

, Equation 12 holds.

q2
q1+q2

Note that αk is a function of θ, so θ cannot be calculated in a closed-form from Equation 16. How-
ever, the optimal θ is in the set {θ|θ = ((1 − α)Σ−1
2 µ2), 0 <
α < 1}, which motivates our mode-IMM method.

2 )−1((1 − α)Σ−1

1 µ1 + αΣ−1

1 + αΣ−1

In our study IMM uses diagonal covariance matrices, which means that there is no correlation be-
tween parameters. This diagonal assumption is useful, since it decreases the number of parameters
for each covariance matrix from O(D2) to O(D). Based on this, the θ in Equation 12 is deﬁned as
follows:

θv =

(1 − α) · µ1,v/σ2
(1 − α)/σ2

1,v + α · µ2,v/σ2
2,v
1,v + α/σ2
2,v

v denotes an index of the parameter vector. µ·,v and σ2
For MoG with two components in K dimension, the number of modes can be at most K + 1 [32].
Therefore, it is hard to ﬁnd all modes in high-dimensional Gaussian in general.

·,v are scalar.

The property of critical points of a MoG with two components can be extended to the case of K
components. The following equation holds:

θ = (

αkΣ−1

k )−1(

αkΣ−1

k µk),

K
(cid:88)

k=1

K
(cid:88)

k=1

where 0 < αk < 1 for all k and (cid:80)
of MoG in general. There is a guess that, for all D, K ≥ 1, the upper bound is (D+K−1)CD [33].

k αk = 1. There is no tight upper bound on the number of modes

APPENDIX B. Bayesian Neural Networks and Continual Learning

Bayesian Neural Networks. Bayesian neural networks (BNN) assume an uncertainty for the whole
parameter in neural networks so that the posterior distribution can be obtained [10]. Previous studies

(12)

(13)

(14)

(15)

(16)

(17)

(18)

12

Algorithm 1 IMM with weight-transfer, L2-transfer

Input: data {(X1, y1),...,(XK, yK)}, balancing hyperparameter α
Output: w1:K
w0 ← InitializeNN()
for k = 1:K do
wk∗ ← wk−1
Train(wk∗, Xk, yk) with L(wk∗, Xk, yk) + λ · ||wk∗ − wk−1||2
2
if type is mean-IMM then

w1:k ← (cid:80)k

t αtwt∗

else if type is mode-IMM then

Fk∗ ← CalculateFisherMatrix(wk∗, Xk, yk)
Σ1:k ← ((cid:80)k
w1:k ← Σ1:k · ((cid:80)k

t αtFt∗)−1

t αtFt∗wt∗)

end if
end for

have argued that BNN regularizes better than NN, and provides a conﬁdence interval for the output
estimation of each input instance. Current research on BNN, to the best of our knowledge, uses
Gaussian distributions as the posteriors of the parameters. In the Gaussian assumption, because
tracking the entire information of a covariance matrix is too expensive, researchers usually use only
the diagonal term for the covariance matrix, where the posterior distribution is fully factorized for
each parameter. However, the methods using full covariance were also suggested recently [31]. To
estimate a covariance matrix most studies use stochastic gradient variational Bayes (SGVB), where
a sampled point from the posterior distribution by Monte Carlo is used in the training phases [34].
Alternatively, Kirkpatrick et al. [8] approximated the covariance matrix as an inverse of a Fisher
matrix. This approximation makes the computational cost of the inference of a covariance matrix
cheaper when the update of covariance information is not needed in the training phase. Our method
follows the approach using the Fisher matrix.

Elastic Weight Consolidation. We compare the work of Kirkpatrick et al. [8] to the results of our
framework. The mechanism of EWC follows sequential Bayesian estimation. EWC maximizes the
following terms by gradient descent to get the solution µ1:K.

log p1:K ≈ log p(yK|XK, θ) + λ · log p1:(K−1) + C

≈ log p(yK|XK, θ) + λ ·

log q1:k + C

K−1
(cid:88)

k=1

K−1
(cid:88)

k=1

λ
2

·

= log p(yK|XK, θ) −

(θ − µ1:k)T ˜Σ−1

k (θ − µ1:k) + C (cid:48)

(19)

pk is empirical posterior distribution of kth task, and qk ∼ N (µk, Σk) is an approximation of pk. In
EWC, ˜Σ−1
is also approximated by the diagonal term of Fisher matrix ˜Fk with respect to µ1:k and
k
Xk.

When moving to a third task, EWC uses the penalty term of both ﬁrst and second network (i.e., µ1
and µ1:2). Although this heuristic works reasonably in the experiments in their paper, it does not
match to the philosophy of Bayesian.

Learning without Forgetting. We compare the work of Li and Hoiem [7]. Although LwF does not
explicitly assume Bayesian, the approach can be represented nonetheless as follows:

log p1:K ≈ log p(yK|XK, θ) + λ ·

log p(ˆyk|XK, θ)

(20)

Where ˆyk is the output from µk with input XK. This framework is promising where the properties
of a pseudo training set of kth task (XK, ˆyk) is similar to the ideal training set (Xk, yk).

K−1
(cid:88)

k=1

13

APPENDIX C. Example Algorithms of Incremental Moment Matching

Two moment matching methods: mean-IMM and mode-IMM, and three transfer learning tech-
niques: weight-transfer, L2-transfer, and drop-transfer, are combined to make various continual
learning algorithms in our study. Algorithm 1 describes mean-IMM and mode-IMM with weight-
transfer and L2-transfer.

APPENDIX D. Experimental Details

Appendix D further explains following issues, 1) additional explanation of the untuned setting and
tuned setting 2) techniques for IMM with a different class output layer for each task 3) other exper-
imental details.

D.1 Disjoint MNIST Experiment

We ﬁrst explain the untuned setting and the tuned setting in detail. The untuned setting refers to the
most natural hyperparameter in the equation of each algorithm, whereas the tuned setting refers to
using heuristic hand-tuned hyperparameters. For mean-IMM, it is most natural to evenly average
K models and 1/K is the most natural αk value for K sequential tasks. For EWC, 1 is the most
natural λ value in Equation 19, because EWC is derived from the equation of sequential Bayesian.
For L2-transfer, there is no natural hyperparameter value in Equation 10, so we need to heuristically
choose a λ value, which is not too small but does not damage the performance of the new network
for the new task.

In the SGD, the number of epochs for the dataset (epoch per dataset) for the second task corresponds
to the hyperparameter. The unit is how much of the network is trained from the whole data at once.
In the L2-transfer and EWC, λ in Equations 10 and 19 corresponds to their hyperparameter. In the
mean-IMM and mode-IMM, αK in Equations 4 and 7 corresponds to the hyperparameter. In the
drop-transfer, dropout ratio p in Equation 11 corresponds to the hyperparameter.

All of the explained hyperparameters are devised to balance the information between the old and
new tasks. If λ/(1 + λ) = 1 or α1 = 1, the ﬁnal network of the algorithms is the same as the
network for the ﬁrst task. If 1/(1 + λ) = 1 or αK = 1, the ﬁnal network is the same as the network
for the last task.

We used multi-layer perceptrons (MLP) with [784-800-800-10] as the number of nodes, ReLU as
the activation function, and vanilla SGD as the optimizer for all of the experiments. We set the epoch
per dataset to 10, unless otherwise noted. The entire IMM model uses weight-transfer to smooth the
loss surface of the model. Without weight-transfer, our IMM model does not work at all. In our
experiments, all models only use one 10-way softmax output layer. For only SGD, dropout is used
as proposed in Goodfellow et al. [3], but dropout does not help much.

Each accuracy was measured by averaging the results of 10 experiments. In the experiment, IMM
outperforms comparative models by a signiﬁcant margin. In the tuned experiment, the performance
of the IMM models exceeds 90%, and the performance increases more when more transfer tech-
niques are applied. Among all the models, weight-transfer + L2-transfer + drop-transfer + mode-
IMM performs the best and its performance is greater than 94%. However, the comparative models
fail to reach greater than 90%. Existing regularizer including dropout does not improve the compar-
ative models.

D.2 Shufﬂed MNIST Experiment

The second experiment is the shufﬂed MNIST experiment for three sequential tasks. For the hyper-
parameter of IMM, we set α1 and α2 as the same value, and tune only α3. Table 1 (Bottom) shows
the experimental results. The performance of SGD + dropout and EWC + dropout comes from the
report in [8]. Changing only the epoch does not signiﬁcantly increase the performance in SGD. The
results show that our IMM paradigm performs similarly to EWC in a case where EWC performs
well. Dropout regularization in the task makes both our models and comparative models perform
better.

14

Figure 5: (Left) Illustration of the effect of the strategy of re-weighing on the new last-layer. Mode-
IMM refers to the original mode-IMM devised for the ImageNet2CUB experiments. In naïve mode-
IMM, the second last-layer of the second network is used for the second last-layer of the ﬁnal IMM
model. (Right) The results of mode-IMM with changing the balancing hyperparameter α to the
re-scaled balancing hayperparameter ˆα with the scale of the Fisher matrix of each network.

In our IMM framework, weight-transfer, L2-transfer, and drop-transfer all take µk−1 as the reference
models of the transfer for training µk. In other words, weight-transfer initializes µk with µk−1, L2-
transfer uses a regularization term to minimize the Euclidean distance between µk−1 and µk, and
drop-transfer uses a µk−1 as the zero point of the dropout procedure. All three transfer techniques
can be considered to change the reference point to, for example, µmean
1:(k−1), as previous
works do [8]. However, all these alternatives make performances worse in our shufﬂed MNIST
experiment. We argued that our utilization of transfer techniques is devised not to minimize the
distance between µk−1 and µk, but to help ﬁnd a µk with a smooth and convex-like loss space
between µk−1 and µk.

1:(k−1) or µmode

D.3 ImageNet to Other Image Datasets

When each task needs a different class output layer, IMM requires additional techniques. There is
no counterpart weight matrix in the last-layer of the ﬁrst network representing the second task, nor
the second last-layer of the ﬁrst network. To tackle this problem, we add the training process of the
last-layer ﬁne-tuning model to the IMM procedure; we match the moments of the last-layer ﬁne-
tuning model with the original new network for the new task. Last-layer ﬁne-tuning is the model the
last-layer is only ﬁne-tuned for each new task; thus it does not make a performance loss for the ﬁrst
task, but does not often learn enough for new tasks.

The technique utilizing the last-layer ﬁne-tuning model makes mean-IMM work in the case of dif-
ferent class output layers, but it is not enough for mode-IMM. It is not possible to calculate a proper
Fisher matrix of the second last-layer in the ﬁrst network for the ﬁrst dataset. As the Fisher matrix is
deﬁned with the gradient from the loss of the ﬁrst task, elements of the Fisher matrix have a value of
zero. However, a zero matrix not only is what we do not want but also degenerates the performance
of mode-IMM. To tackle this problem, we apply mean-IMM for the last-layer with a re-scaling. We
change the mixing ratios α1 : α2 to ˆα1 : ˆα2 = α1 : α2 ·
| ˆw1|+| ˆw2| for the re-scaling, where | ˆw1| and
| ˆw2| is the average of the whole element of weight matrix in the layer before the last-layer, in the
ﬁrst and the second task.

| ˆw1|

In our ImageNet2CUB experiment, the moments of the last-layer ﬁne-tuning model and the LwF
model are matched. Though LwF does not perform well in our previous experiments, it is known
that LwF performs well when the size of a new dataset is small relative to the old dataset, as in the
ImageNet2CUB experiment.

Figure 5 (Left) compares the performances of mode-IMM models with different assumptions on the
Fisher matrix. In naïve mode-IMM, the Fisher matrix of the second last-layer of the ﬁrst network
is a zero matrix. In other words, the second last-layer of the ﬁnal naïve mode-IMM is the second
last-layer of the second network. Naïve mode-IMM does not yield a good performance as we expect.

15

Table 3: Experimental results on the Lifelog dataset. Mean-IMM uses weight-transfer. Classiﬁcation
accuracies among different classes (Top) and different subjects (Bottom). In the experiment, our
IMM paradigm achieves competitive results with the approach using an ensemble network, without
additional cost for inference and learning.

Sub-location Activity

Algorithm
Dual memory architecture [12]
Mean-IMM
Mode-IMM
Online ﬁne-tuning
Last-layer ﬁne-tuning
Naïve incremental bagging
Incremental bagging w/ transfer

Location
78.11
77.60
77.14
68.27
74.58
74.48
74.95

Algorithm
Dual memory architecture [12]
Mean-IMM
Mode-IMM
Online ﬁne-tuning
Last-layer ﬁne-tuning
Naïve incremental bagging
Incremental bagging w/ transfer

A
67.02
67.03
67.97
53.01
63.31
62.24
61.21

72.36
73.78
75.76
64.13
69.30
67.18
68.53

B
58.80
57.73
60.12
56.54
55.83
53.57
56.71

52.92
52.74
54.07
50.00
52.22
47.92
49.66

C
77.57
79.35
78.89
72.85
76.97
73.77
75.23

In Figure 5, scaled mode-IMM denotes the results of mode-IMM re-plotted by the ˆα as we de-
ﬁned above. The result shows that re-scaled mode-IMM performs similarly to mean-IMM in the
ImageNet2CUB experiment.

D.4 Lifelog Dataset

The Lifelog dataset is the dataset recorded from Google Glass over 46 days from three participants.
The 660,000 seconds of the egocentric video stream data reﬂects the behaviors of the participants.
The dataset consists of 10 days of training data and 4 days of test data in order of time for each
participant respectively. In the framework of Lee et al. [12], the network can be updated every day,
but a new network can be made for the 3rd, 7th, and 10th day, with training data of 3, 4, and 3 days,
respectively. Following this framework, our network is made in the 3rd, 7th, and 10th day, and then
merged to previously trained networks. Our IMM used AlexNet pretrained by the ImageNet dataset
[29] as the initial network. The experimental results on the Lifelog dataset are in Table 3, where the
performance of models is from Lee et al. [12] except IMM.

16

