0
2
0
2
 
b
e
F
 
0
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
2
2
7
3
0
.
2
0
0
2
:
v
i
X
r
a

Super-eﬃciency of automatic diﬀerentiation for
functions deﬁned as a minimum

Pierre Ablin
CNRS and DMA
Ecole Normale Supérieure - PSL University
Paris, 75005, France

Gabriel Peyré
CNRS and DMA
Ecole Normale Supérieure - PSL University
Paris, 75005, France

Thomas Moreau
INRIA, CEA
Université Paris-Saclay
Palaiseau, 91200, France

February 11, 2020

Abstract

In min-min optimization or max-min optimization, one has to compute
the gradient of a function deﬁned as a minimum.
In most cases, the
minimum has no closed-form, and an approximation is obtained via an
iterative algorithm. There are two usual ways of estimating the gradient
of the function: using either an analytic formula obtained by assuming
exactness of the approximation, or automatic diﬀerentiation through the
algorithm. In this paper, we study the asymptotic error made by these
estimators as a function of the optimization error. We ﬁnd that the error
of the automatic estimator is close to the square of the error of the analytic
estimator, reﬂecting a super-eﬃciency phenomenon. The convergence
of the automatic estimator greatly depends on the convergence of the
Jacobian of the algorithm. We analyze it for gradient descent and stochastic
gradient descent and derive convergence rates for the estimators in these
cases. Our analysis is backed by numerical experiments on toy problems
and on Wasserstein barycenter computation. Finally, we discuss the
computational complexity of these estimators and give practical guidelines
to chose between them.

1

1

Introduction

In machine learning, many objective functions are expressed as the minimum of
another function: functions (cid:96) deﬁned as

(cid:96)(x) = min

(z, x) ,

z∈Rm L

(1)

: Rm

Rn

L

L

×

→

R. Such formulation arises for instance in dictionary
where
is the Lasso cost [Mairal
learning, where x is a dictionary, z a sparse code, and
et al., 2010]. In this case, (cid:96) measures the ability of the dictionary x to encode the
input data. Another example is the computation of the Wassertein barycenter
of distributions in optimal transport [Agueh and Carlier, 2011]: x represents
the barycenter, (cid:96) is the sum of distances to x, and the distances themselves
In the ﬁeld of optimization,
are deﬁned by minimizing the transport cost.
formulation (1) is also encountered as a smoothing technique, for instance in
reweighted least-squares [Daubechies et al., 2010] where
is smooth but not (cid:96). In
game theory, such problems naturally appear in two-players maximin games[von
Neumann, 1928], with applications for instance to generative adversarial nets
[Goodfellow et al., 2014]. In this setting, (cid:96) should be maximized.

L

A key point to optimize (cid:96) – either maximize or minimize – is usually to com-
(z, x)
(z∗(x), x) = 0

pute the gradient of (cid:96), g∗(x) (cid:44)
is available, the ﬁrst order optimality conditions impose that
and the gradient is given by

x(cid:96)(x). If the minimizer z∗(x) = arg minz L

∇

∇

L

z

g∗(x) =

(z∗(x), x) .

x

L
However, in most cases the minimizer z∗(x) of the function is not available in
closed-form. It is approximated via an iterative algorithm, which produces a
sequence of iterates zt(x). There are then three ways to estimate g∗(x):

∇

The analytic estimator corresponds to plugging the approximation zt(x)

in (2)

t (x) (cid:44)
g1
The automatic estimator is g2

x

(zt(x), x) .

∇
L
t (x) (cid:44) ∂x [

L

is computed with respect to zt(x) as well. The chain rule gives

(zt(x), x)], where the derivative

g2
t (x) =

x

∇

L

(zt(x), x) +

(zt(x), x) .

∂zt
∂x ∇

z

L

This expression can be computed eﬃciently using automatic diﬀerentiation [Bay-
din et al., 2018], in most cases at a cost similar to that of computing zt(x).

If

(z∗(x), x) is invertible, the implicit function theorem gives ∂z∗(x)
L
(z∗(x), x) where

∂x =
(z, x)]−1. The implicit esti-

(z, x) (cid:44)

(z, x) [

∇

zz

xz

−∇

L

zz

∇

L

J
mator is

(zt(x),x)+

(zt(x),x)

(zt(x),x).

(5)

z

∇

L

J
t (x) (cid:44)
g3

x

∇

L

(2)

(3)

(4)

J

2

This estimator can be more costly to compute than the previous ones, as a
m

m linear system has to be solved.

×

L

These estimates have been proposed and used by diﬀerent communities. The
, where one updates x
analytic one corresponds to alternate optimization of
while considering that z is ﬁxed. It is used for instance in dictionary learning
[Olshausen and Field, 1997, Mairal et al., 2010] or in optimal transport [Feydy
et al., 2019]. The second is common in the deep learning community as a way
to diﬀerentiate through optimization problems [Gregor and Le Cun, 2010]. Re-
cently, it has been used as a way to accelerate convolutional dictionary learning
[Tolooshams et al., 2018]. It has also been used to diﬀerentiate through the
Sinkhorn algorithm in optimal transport applications [Boursier and Perchet, 2019,
Genevay et al., 2018]. It integrates smoothly in a machine learning framework,
with dedicated libraries [Abadi et al., 2016, Paszke et al., 2019]. The third one
is found in bi-level optimization, for instance for hyperparameter optimization
[Bengio, 2000]. It is also the cornerstone of the use of convex optimization as
layers in neural networks [Agrawal et al., 2019].

Contribution In this article, we want to answer the following question:
which one of these estimators is the best? The central result, presented in sec-
tion 2, is the following convergence speed, when
is diﬀerentiable and under
L
mild regularity hypothesis (Proposition 1, 2 and 3)

g1
t (x)
g2
t (x)
g3
t (x)

|

|

|

−

−

−

g∗(x)
g∗(x)
g∗(x)

|

|

|

zt(x)
= O (
|
zt(x)
zt(x)

= o (
|

= O

|

z∗(x)
) ,
|
z∗(x)
) ,
|
2
z∗(x)
|

−

−

−

.

(cid:1)

(cid:0)
This is a super-eﬃciency phenomenon for the automatic estimator, illustrated in
Figure 1 on a toy example. As our analysis reveals, the bound on g2 depends
on the convergence speed of the Jacobian of zt, which itself depends on the
optimization algorithm used to produce zt. In section 3, we build on the work of
Gilbert [1992] and give accurate bounds on the convergence of the Jacobian for
gradient descent (Proposition 5) and stochastic gradient descent (Proposition 8
and 9) in the strongly convex case. We then study a simple case of non-strongly
convex problem (Proposition 12). To the best of our knowledge, these bounds
are novel. This analysis allows us to reﬁne the convergence rates of the gradient
estimators. In section 4, we start by recalling and extending the consequence
of using wrong gradients in an optimization algorithm (Proposition 13 and 14).
Then, since each gradient estimator comes at a diﬀerent cost, we put the
convergence bounds developed in the paper in perspective with a complexity
analysis. This leads to practical and principled guidelines about which estimator
should be used in which case. Finally, we provide numerical illustrations of the
aforementioned results in section 5.

Notation The (cid:96)2 norm of z
of M

Rm×n is

Rm is
M z

∈

M
(cid:107)

(cid:107)

= sup|z|=1 |

∈

=

m
i=1 z2
z
|
and the Frobenius norm is

i . The operator norm
F =
(cid:107)

M
(cid:107)

(cid:112)(cid:80)

|

|

3

Figure 1: Convergence of the gradient estimators.
is strongly convex, x is a
random point and zt(x) corresponds to t iterations of gradient descent. As t
increases, zt goes to z∗ at a linear rate. g1
t converges at the same rate while g2
t
and g3

t are twice as fast.

L

ij. The vector of size n full of 1(cid:48)s is 1n. The Euclidean scalar product

,
is
(cid:113)(cid:80)
(cid:104)·
The proofs are only sketched in the article, full proofs are deferred to appendix.

i,j M 2
.
·(cid:105)

2 Convergence speed of gradient estimates

L

We consider a compact set K = Kz
.
assumptions on

×

Kx

⊂

×

Rm

Rn. We make the following

L

is twice diﬀerentiable over K with second derivatives

H1:
respectively Lxz and Lzz-Lipschitz.
H2: For all x
Kx, z
mapping z∗(x) is diﬀerentiable, with Jacobian J ∗(x)

(z, x) has a unique minimizer z∗(x)
Rn×m.

→ L

∈

∈

xz

∇

L

and

zz

∇

L

int(Kz). The

H1 implies that
∇
Jacobian of zt at x
consider a point x

z

∇

are Lipschitz, with constants Lz and Lx. The
and
x
L
L
Kx is Jt (cid:44) ∂zt(x)
Rn×m. For the rest of the section, we
∈
Kx, and we denote g∗ = g∗(x), z∗ = z(x) and zt = zt(x).
∈

∂x ∈

∈

2.1 Analytic estimator g1
The analytic estimator (3) approximates g∗ as well as zt approximates z∗ by
deﬁnition of the Lx-smoothness.

Proposition 1 (Convergence of the analytic estimator). The analytic estimator
veriﬁes

g∗

z∗

g1
t −

|

Lx

zt
|

−

| ≤

.
|

4

2.2 Automatic estimator g2

The automatic estimator (4) can be written as

g2 = g∗ + R(Jt)(zt

z∗) + Rxz + JtRzz

(6)

−

where

Rxz (cid:44)
Rzz (cid:44)

∇

L

x

z

(zt, x)

(zt, x)

− ∇

(z∗, x)
− ∇
(z∗, x)(zt

x

L
zz

− ∇

L

(z∗, x)(zt

xz

z∗)

−

L
z∗) .

−

∇
are Taylor’s rests and

L

R(J) (cid:44) J

(z∗, x) +

(z∗, x) .

zz

∇

L

xz

∇

L

(7)

The implicit function theorem states that R(J ∗) = 0. Importantly, in a non
(z∗, x) is not invertible, it might happen that
strongly-convex setting where
R(Jt) goes to 0 even though Jt does not converge to J ∗. H1 implies a quadratic
bound on the rests

∇

L

zz

Rxz
|
We assume that Jt is bounded
(cid:107) ≤
is the subject of section 3. The triangle inequality in Equation 6 gives:

−
LJ . This holds when Jt converges, which

Rzz
|

Jt
(cid:107)

| ≤

| ≤

(8)

zt

zt

−

|

z∗

2 and

z∗

2.
|

Lxz
2 |

Lzz
2 |

Proposition 2 (Convergence of the automatic estimator). We deﬁne

L (cid:44) Lxz + LJ Lzz

(9)

Then

g∗

g2
t −

R(Jt)

zt

z∗

+ L
2 |

zt

z∗

2.
|

|

−
This proposition shows that the rate of convergence of g2 depends on the

| ≤ (cid:107)

(cid:107)|

−

|

speed of convergence of R(Jt). For instance, if R(Jt) goes to 0, we have

g2
t −

g∗ = o(
zt
|
Unfortunately, it might happen that, even though zt goes to z∗, R(Jt) does not
go to 0 since diﬀerentiation is not a continuous operation. In section 3, we reﬁne
this convergence rate by analyzing the convergence speed of the Jacobian in
diﬀerent settings.

) .
|

z∗

−

2.3

Implicit estimator g3

The implicit estimator (5) is well deﬁned provided that
obtain convergence bounds by making a Lipschitz assumption on

is invertible. We
(z, x) =

∇

L

zz

xz

(z, x) [

zz

(z, x)]−1.

L

L

∇

−∇
Proposition 3. [Convergence of the implicit estimator] Assume that
Lipschitz with respect to its ﬁrst argument, and that
deﬁned in (9),

t
(cid:107)J

(cid:107) ≤

is LJ -
LJ . Then, for L as

J

J

g3
t −
|

g∗

| ≤

(

L
2

zt
+ LJ Lz)
|

−

z∗

2 .
|

(10)

5

Sketch of proof. The proof is similar to that of Proposition 2, using
LzLJ

z∗

zt

.

R(
(cid:107)

J

(zt, x))

(cid:107) ≤

|

−

|

Therefore this estimator converges twice as fast as g1, and at least as fast as
g2. Just like g1 this estimator does not need to store the past iterates in memory,
since it is a function of zt and x. However, it is usually much more costly to
compute.

2.4 Link with bi-level optimization

Bi-level optimization appears in a variety of machine-learning problems, such as
hyperparameter optimization [Pedregosa, 2016] or supervised dictionary learning
[Mairal et al., 2012]. It considers problems of the form

(cid:96)(cid:48)(x) (cid:44)

(cid:48) (z∗(x), x) s.t. z∗(x)

min
x∈Rn

L

arg min

z∈Rm L

∈

(z, x),

(11)

(cid:48) : Rm

Rn

where
is a special instance of bi-level optimization where

R is another objective function. The setting of our paper
. The gradient of (cid:96)(cid:48) is

(cid:48) =

→

×

L

g(cid:48)∗ =

x(cid:96)(cid:48)(x) =

∇

x

∇

L

(cid:48)(z∗, x) + J ∗

(cid:48)(z∗, x) .

L

L

z

∇

L

When zt(x) is a sequence of approximate minimizers of
can be deﬁned as

L

, gradient estimates

g(cid:48)1 =
g(cid:48)2 =
g(cid:48)3 =

L

L

∇

∇

x

x

x

(cid:48)(zt(x), x),
(cid:48)(zt(x), x) + Jt
(cid:48)(zt(x), x) +

(cid:48)(zt(x), x),

z

∇
L
(zt(x), x)

(cid:48)(zt(x), x).

z

Here,
estimate g(cid:48)∗. Moreover, in general,

(cid:48)(z∗, x)

∇

L

z

L

∇

L
= 0, since z∗ does not minimize

∇

J

(cid:48). Hence, g(cid:48)1 does not

L

xz

∇

L

(cid:48)(z∗, x) + J ∗

(cid:48)(z∗, x)

= 0.

zz

∇

L

Therefore, there is no cancellation to allow super-eﬃciency of g2 and g3 and we
only obtain linear rates

g(cid:48)2

g∗ = O(
|

zt

−

z∗

), g(cid:48)3

−

|

g∗ = O(
|

zt

−

z∗

) .

−

|

3 Convergence speed of the Jacobian

In order to get a better understanding of the convergence properties of the
gradient estimators – in particular g2 – we analyze it in diﬀerent settings. A
large portion of the analysis is devoted to the convergence of R(Jt) to 0, since
it does not directly follow from the convergence of zt. In most cases, we show
convergence of Jt to J ∗, and use

R(Jt)

(cid:107)

Lz

Jt

(cid:107)

−

J ∗

(cid:107)

(cid:107) ≤

(12)

in the bound of Proposition 2.

6

3.1 Contractive setting

When zt are the iterates of a ﬁxed point iteration with a contractive mapping,
we recall the following result due to Gilbert [1992].

Proposition 4 (Convergence speed of the Jacobian). Assume that zt is produced
by a ﬁxed point iteration

zt+1 = Φ (zt, x) ,

Kx

where Φ : Kz
→
there exists κ < 1 such that for all (z, z(cid:48), x)
κ
|

×
. Under mild regularity conditions on Φ:
|

Kz is diﬀerentiable. We suppose that Φ is contractive:
Kz

Φ(z(cid:48), x)

Kx,

Kz

Φ(z, x)
|

| ≤

z(cid:48)

×

−

×

−

∈

z

•

zt converges to a diﬀerentiable function z∗ such that z∗ = Φ(z∗, x), with
Jacobian J ∗.
z∗

= O(κt) and

= O(tκt)

J ∗

zt

• |

−

|

Jt
(cid:107)

−

(cid:107)

3.2 Gradient descent in the strongly convex case

ρ

We consider the gradient descent iterations produced by the mapping Φ(z, x) =
z
is µ-strongly
convex with respect to z, i.e.
Kx. In this setting,
Φ satisﬁes the hypothesis of Proposition 4, and we obtain precise bounds.

1/Lz. We assume that

(z, x), with a step-size ρ

µId for all z

≤
L (cid:23)

Kz, x

∇

∇

−

L

L

∈

∈

zz

z

Proposition 5. [Convergence speed of the Jacobian of gradient descent in a
(zt, x)
strongly convex setting] Let zt produced by the recursion zt+1 = zt
L
1/Lz and κ (cid:44) 1
J ∗
with ρ
ρµ. We have
≤
−
tκt−1ρL
z0
|

zt
|
where L is deﬁned in (9).

ρ
−
∇
Jt
and
(cid:107)

z0
|

(cid:107) ≤

| ≤

z∗

z∗

z∗

κt

−

−

−

−

|

|

z

Sketch of proof (C.1). We show that δt =
z0
κδt + ρL
equality δt+1
|

κt.

z∗

−

≤

|

Jt
(cid:107)

−

J ∗

(cid:107)

satisﬁes the recursive in-

As a consequence, Prop. 1, 2, 3 together with Eq. (12) give in this case

g1

g2

g3

|

|

|

g∗

g∗

g∗

| ≤

| ≤

| ≤

−

−

−

Lx

z0
|
−
(ρLzt +

κt,
|
)L

z∗
κ
2

z0
|

(

L
2

+ LJ Lz)

z0
|

−

z∗

|

−

z∗

2κ2t−1,

|
2κ2t .

(13)

We get the convergence speed g2
g∗ = O(tκ2t), which is almost twice better
than the rate for g1. Importantly, the order of magnitude in Proposition 5 is
tight, as it can be seen in Proposition 15.

−

3.3 Stochastic gradient descent in z

We provide an analysis of the convergence of Jt in the stochastic gradient descent
setting, assuming once again the µ-strong convexity of
is
an expectation

. We suppose that

L

L

(z, x) = Eξ[C(z, x, ξ)] ,

L

7

where ξ is drawn from a distribution d, and C is twice diﬀerentiable. Stochastic
gradient descent (SGD) with steps ρt iterates

zt+1(x) = zt(x)

ρt

zC (zt(x), x, ξt+1) where ξt+1

d .

−

∇

∼

In the stochastic setting, Proposition 2 becomes

Proposition 6. Deﬁne

δt = E

Jt

J ∗

2
F
(cid:107)

(cid:107)
−
(cid:2)
(cid:3)
Lz√δt√dt + L

2 dt.

and dt = E

zt
|
(cid:2)

z∗

2
|

−

.

(cid:3)

We have E[
|

g2

g∗

]

−

|

≤

(14)

Sketch of proof (C.3). We use Cauchy-Schwarz and the norm inequality
].
|

F to bound E [

R(Jt)
(cid:107)

(cid:107) · (cid:107)

z∗

zt

(cid:107)|

−

(cid:107) · (cid:107) ≤

We begin by deriving a recursive inequality on δt, inspired by the analysis

techniques of dt.

Proposition 7. [Bounding inequality for the Jacobian] We assume bounded Hes-
zzC(z, x, ξ)
sian noise, in the sense that E
xz + L2
xz. Let r = min(n, m), and B2 = σ2
σ2

2
xzC(z, x, ξ)
F
(cid:107)

σ2
2
zz and E
F
(cid:107)
J σ2
zz. We have
(cid:3)

(cid:107)∇

(cid:107)∇

≤

≤

(cid:2)
2ρtµ)δt + 2ρt√rL

dt

δt + ρ2

(cid:2)
t B2.

(cid:3)
(15)

δt+1

(1

≤

−

Sketch of proof (C.4). A standard strong convexity argument gives the bound

(cid:112)

(cid:112)

δt+1

(1

2ρtµ)δt+2ρt√rLE[
(cid:107)

Jt

−

≤

−

J ∗

zt
|

F

(cid:107)

−

|

z0

]+ρ2

t B2.

The middle term is then bounded using Cauchy-Schwarz inequality.

Therefore, any convergence bound on dt provides another convergence bound
on δt by unrolling Eq. (15). We ﬁrst analyze the ﬁxed step-size case by using
the simple “bounded gradients” hypothesis and bounds on dt from Moulines and
Bach [2011]. In this setting, the iterates converge linearly until they reach a
threshold caused by gradient variance.

Proposition 8. [SGD with constant step-size] Assume that the gradients have
σ2. Assume ρt = ρ < 1/Lz, and let
bounded variance Eξ[
κ2 = √1

2]
zC(z, x, ξ)
|
σ2ρ
2µ . In this setting

|∇
2ρµ and β =

≤

−

where α = ρ

√

rL

κ2

z∗

|

z0

|

−

(cid:113)
δt

κt
2 (

J ∗
(cid:107)

F + tα) + B2
(cid:107)
≤
√
and B2 = ρ
κ2(1−κ2) + ρB

rLβ

(cid:1)
(1−κ2) .

(cid:0)

2

,

Sketch of proof (C.5). Moulines and Bach [2011] give dt
which implies √dt

z∗

+ β . A bit of work on Eq. (15) then gives

z0

≤

−

z∗

2 + β2,
|

κ2t
2 |

≤

z0

κt
2|
−
δt+1

|
κ2

−
Unrolling the recursion gives the proposed bound.

(cid:112)

(cid:112)

≤

δt + ρκt

2 + (1

κ2)B2

8

This bound showcases that the familiar “two-stages” behavior also stands for
2ρµ)t in the ﬁrst iterations, and then
2 . This bound is not tight enough to
3 the limit of the sequence

δt: a transient linear decay at rate (1
convergence to a stationary noise level B2
provide a good estimate of the noise level in δt. Let B2
deﬁned by the recursion (15). We ﬁnd

−

B2

3 = (1

2ρµ)B2

3 + 2ρ√rLβB3 + ρ2B2,

−

which gives by expanding β

B3 = √ρ

√rLσ
(2µ) 3

2 (cid:32)

1 +

1 +

(cid:114)

4µ2B2
rL2σ2

.

(cid:33)

This noise level scales as √ρ, which is observed in practice. In this scenario,
g∗ reaches a noise level proportional to √ρ, just
Prop. 1, 2 and 3 show that g1
like dt, while both g2
g∗ reach a noise level proportional to ρ: g2
and g3 can estimate g∗ to an accuracy that can never be reached by g1. We now
turn to the decreasing step-size case.

g∗ and g3

−

−

−

Proposition 9. [SGD with decreasing step-size] Assume that ρt = ρ0t−α with
α

(0, 1). Assume a bound on dt of the form dt

d2t−α. Then

∈

≤

4

ρ0B2µ + rL2d2
µ2

δt

≤

t−α + o(t−α) .

Sketch of proof (C.6). We use (15) to obtain a recursion

δt+1

1

−

≤

µρ0t−α

δt + (B2ρ2

0 +

rL2d2ρ0
µ

)t−2α ,

(cid:0)
which is then unrolled.

(cid:1)

When ρt

t−α, we have dt = O(t−α) so the assumption dt

d2t−α is
veriﬁed for some d. One could use the precise bounds of [Moulines and Bach,
2011] to obtain non-asymptotic bounds on δt as well.

≤

∝

Overall, we recover bounds for δt with the same behavior than the bounds
for dt. Pluging them in Proposition 6 gives the asymptotic behaviors for the
gradient estimators.

Proposition 10 (Convergence speed of the gradient estimators for SGD with
decreasing step). Assume that ρt = Ct−α with α

(0, 1). Then

g1

Eξ[
|

−

g∗

|

] = O(√t−α), Eξ[
g3
Eξ[
|

g2
|
−
] = O(t−α)
|

g∗

−

∈
g∗

] = O(t−α)
|

The super-eﬃciency of g2 and g3 is once again illustrated, as they converge

at the same speed as dt.

9

3.4 Beyond strong convexity

f (z)

≤ (cid:107)∇

All the previous results rely critically on the strong convexity of
f with minimizer z∗ is p-Łojasiewicz [Attouch and Bolte, 2009] when µ(f (z)
−
f (z∗))p−1
p for some µ > 0. Any strongly convex function is 2-
(cid:107)
2 oﬀers a framework
Łojasiewicz: the set of p-Łojasiewicz functions for p
beyond strong-convexity that still provides convergence rates on the iterates.
The general study of gradient descent on this class of function is out of scope for
this paper. We analyze a simple class of p-Łojasiewicz functions, the least mean
p-th problem, where

. A function

≥

L

n

(z, x) (cid:44) 1
p
L

(xi

[Dz]i)p

−

(16)

i=1
(cid:88)
for p an even integer and D is overcomplete (rank(D) = n). In this simple case,

(
·

, x) is minimized by cancelling x
In the case of least squares (p = 2) we can perfectly describe the behavior of

Dz, and g∗ = (x

Dz∗)p−1 = 0.

−

−

L

gradient descent, which converges linearly.

Proposition 11. Let zt the iterates of gradient descent with step ρ
with p = 2, and z∗

(z, x). It holds

arg min

≤

1
Lz

in (16)

∈
g1 = D(zt

L
z∗), g2 = D(z2t

z∗) and g3 = 0.

−

−
Proof. The iterates verify zt
(In

D(cid:62)D)t)(x

(In

z∗ = (I

D(cid:62)D)t(z0

z∗), and we ﬁnd Jt

(zt, x) =

z

∇

L

−

−
−
Dzt). The result follows.

−

−

−
The automatic estimator therefore goes exactly twice as fast as the analytic
one to g∗, while the implicit estimator is exact. Then, we analyze the case where
p

4 in a more restrictive setting.

≥

Proposition 12. For p

4, we assume DD(cid:62) = In. Let α (cid:44) p−1

p−2 . We have

≥

= O(t−α),

g1
t |
|

g2
t |

|

= O(t−2α),

g3
t = 0 .

Sketch of proof (C.7). We ﬁrst show that the residuals rt = x

1
p−2

1
ρ(p−2)t

(1 + O( log(t)

t

)), which gives the result for g1. We ﬁnd g2

1)ρdiag(rp−2
(cid:16)
where Mt = In
development of rt and unrolling the recursion concludes the proof.

JtD(cid:62) veriﬁes Mt+1 = Mt(In

(p

−

−

−

(cid:17)

t

−

Dzt verify rt =
t = Mtrp−1
)). Using the

t

For this problem, g2 is of the order of magnitude of g1 squared and as
, we see that the rate of convergence of g1 goes to t−1, while the one of
+

p
∞
g2 goes to t−2.

→

4 Consequence on optimization

In this section, we study the impact of using the previous inexact estimators for
ﬁrst order optimization. These estimators nicely ﬁt in the framework of inexact
oracles introduced by Devolder et al. [2014].

10

| ≤

∈

(17)

(18)

(19)

4.1

Inexact oracle

We assume that (cid:96) is µx-strongly convex and Lx-smooth with minimizer x∗. A
(δ, µ, L)-inexact oracle is a couple ((cid:96)δ, gδ) such that (cid:96)δ : Rm
R is the inexact
value function, gδ : Rm

Rm is the inexact gradient and for all x, y

→

→

µ
2 |

x

y

2
|

−

≤

(cid:96)(x)

(cid:96)δ(y)

−

gδ(y)

x
|

− (cid:104)

y

−

(cid:105) ≤

L
2 |

x

y

2 + δ .

−

|

Devolder et al. [2013] show that if the gradient approximation gi veriﬁes
gi(x)
2 , 2Lx)-inexact oracle, with

∆i for all x, then ((cid:96), gi) is a (δi, µx

g∗(x)
|

−

δi = ∆2
i (

1
µx

+

1
2Lx

)

.

We consider the optimization of (cid:96) with inexact gradient descent: starting from
x0

Rn, it iterates

with η = 1
2Lx

−
, a ﬁxed t and i = 1, 2 or 3.

xq+1 = xq

ηgi

t(xq) ,

Proposition 13. [Devolder et al. 2013, Theorem 4] The iterates xq with estimate
gi verify

(cid:96)(xq)

(cid:96)(x∗)

2Lx(1

−

≤

µx
4Lx

)q

x0

|

−

x∗

2 + δi

−

|

with δi deﬁned in (18).

As q goes to inﬁnity, the error made on (cid:96)(x∗) tends towards δi =

2).
|
Thus, a more precise gradient estimate achieves lower optimization error. This
illustrates the importance of using gradients estimates with an error ∆i as small
as possible.

gi
t −
|

g∗

O

(

We now consider stochastic optimization for our problem, with loss (cid:96) deﬁned

as

(cid:96)(x) = Eυ[h(x, υ)] with h(x, υ) = min

H(z, x, υ) .

z

Stochastic gradient descent with constant step-size η
iterates

≤

where gi

−
t(xq, υq+1) is computed by an approximate minimization of z

xq+1 = xq

ηgi

t(xq, υq+1) ,

1
2Lx

and inexact gradients

H(z, xq, υq+1).

→

Proposition 14. We assume that H is µx-strongly convex, Lx-smooth and
veriﬁes

The iterates xq of SGD with approximate gradient gi and step-size η verify

E[

xh(x, υ)

|∇

x(cid:96)(x)

2]
|

≤

σ2.

− ∇

xq
E
|

−

x∗

2
|

≤

(1

−

ηµx
2

)q

x0

|

x∗

+

−

|

2η
µx

σ2 +

4
µx

δi

with δi = ∆2

i ( 1
µx

+ 1
2Lx

+ 2η).

11

Table 1: Computational cost for a quadratic loss
L
the relative added cost of automatic diﬀerentiation.
Computational cost
(mnt)
(cmnt)
(mnt + m3 + m2n))

Gradient estimate
g1
t
g2
t
g3
t

. Here c

O
O

≥

O

1 corresponds to

The proof is deferred to Appendix D. In this case, it is pointless to achieve
an estimation error on the gradient ∆i smaller than some fraction of the gradient
variance σ2.

As a ﬁnal note, these results extend without diﬃculty to the problem of

maximizing (cid:96), by considering gradient ascent or stochastic gradient ascent.

4.2 Time and memory complexity

In the following, we put our results in perspective with a computational and
memory complexity analysis, allowing us to provide practical guidelines for
optimization of (cid:96).

z

L

L

∇

O
O

Computational complexity of the estimators The cost of computing the
. We give a complexity analysis in the
estimators depends on the cost function
least squares case (16) which is summarized in Table 1. In this case, computing
(mn) operations, therefore the cost of computing zt
the gradient
takes
(mnt). Computing g1
with gradient descent is
t comes at the same price. The
estimator g2 requires a reverse pass on the computational graph, which costs
a factor c
(cmn).
[2, 3]. Finally, computing
Griewank and Walther [2008] showed that typically c
(m3) linear system
g3 requires a costly
t and g2
inversion. The ﬁnal cost is
t
is highlighted in Figure A.3. In addition, computing g2 usually requires to store
in memory all intermediate variables, which might be a burden. However, some
optimization algorithms are invertible, such as SGD with momentum [Maclaurin
et al., 2015]. In this case, no additional memory is needed.

1 of the forward computational cost: the ﬁnal cost is

(mnt + m3 + m2n). The linear scaling of g1

(m2n) Hessian computation, and a

O

O

O

O

≥

∈

In the time it
Linear convergence: a case for the analytic estimator
takes to compute g2
t , one can at the same cost compute g1
ct. If zt converges
g∗ = O(κct), while Proposition 2
linearly at rate κt, Proposition 1 shows that g1
gives, at best, g2
t , provided
2t. Further, computing
that c
g2 might requires additional memory: g1
t in this
setting. However, our analysis is only asymptotic, and other eﬀects might come
into play to tip the balance in favor of g2.

2. In the quadratic case, we even have g2

ct is a better estimator of g∗ than g2

ct should be preferred over g2

g∗ = O(κ2t): g1

t = g1

ct−

t −

≥

As it appears clearly in Table 1, choosing g1 over g3 depends on t: when
m3 + m2n, the additional cost of computing g3 is negligible, and it should
mnt
be preferred since it is more accurate. This is however a rare situation in a large
scale setting.

(cid:29)

12

g∗

gi
t −

with the number of iteration t for (a) the Ridge
Figure 2: Evolution of
2, (c) the Least Mean
Regression
4. In all cases, we
p-th Norm
can see the asymptotic super-eﬃciency of the g2 estimator compared to g1. The
g3 estimator is better in most cases but it is unstable in (d ).

1, (b) the Regularized Logistic Regression
3 in log-scale and (d ) the Wasserstein Distance

L
L

L

L

|

|

Sublinear convergence We have provided two settings where zt converges
sub-linearly. In the stochastic gradient descent case with a ﬁxed step-size, one
can beneﬁt from using g2 over g1, since it allows to reach an accuracy that can
never be reached by g1. With a decreasing step-size, reaching
g1
ε
t −
ε only takes O(ε−1/α)
requires O(ε−2/α) iterations, while reaching
iterations. For ε small enough, we have cε−1/α < ε−2/α: it is always beneﬁcial
to use g2 if memory capacity allows it.

g2
t −
|

| ≤

| ≤

g∗

g∗

|

The story is similar for the simple non-strongly convex problem studied in
subsection 3.4: because of the slow convergence of the algorithms, g2
t is much
closer to g∗ than g1
ct. Although our analysis was carried in the simple least mean
p-th problem, we conjecture it could be extended to the more general setting of
p-Łojasiewicz functions [Attouch and Bolte, 2009].

5 Experiments

All experiments are performed in Python using pytorch [Paszke et al., 2019].
The code to reproduce the ﬁgures is available online.1

5.1 Considered losses

In our experiments, we considered several losses with diﬀerent properties. For
each experiments, the details on the size of the problems are reported in subsec-
tion A.1.

Regression For a design matrix D

Rn×m and a regularization parameter

1See Appendix.

∈

13

λ > 0, we deﬁne

1(z, x) =

Dz

2 +
|

−

λ
z
2 |

2 ,
|

x

1
2 |
n

2(z, x) =

log

1 + e−xi[Dz]i

+

3(z, x) =

i=1
(cid:88)
1
x
p |

(cid:16)
Dz

p;

|

−

(cid:17)
p = 4 .

L

L

L

λ
z
2 |

|

2 ,

L
when λ > 0.
when λ > 0.

1 corresponds to Ridge Regression, which is quadratic and strongly convex
2 is the Regularized Logistic Regression. It is strongly convex
L
3 is studied in subsection 3.4, and deﬁned with DD(cid:62) = In.
L

+ =

Regularized Wasserstein Distance The Wasserstein distance deﬁnes a
distance between probability distributions. In Cuturi [2013], a regularization
of the problem is proposed, which allows to compute it eﬃciently using the
Sinkhorn algorithm, enabling many large scale applications. As we will see, the
formulation of the problem ﬁts nicely in our framework. The set of histograms is
∆mb
∆m
+ .
∈
The set of couplings is U (a, b) =
. The
}
histogram a (resp. b) is associated with set of ma (resp. mb) points in dimension
Rma×mb
k, (X1, . . . , Xma )
2. For (cid:15) > 0, the entropic regularized Wasserstein
Yj
such that Cij =
|
distance is W 2
. The dual formulation
(cid:15) (a, b) = minP ∈U (a,b)(cid:104)
(cid:105)
of the previous variational formulation is [Peyré and Cuturi, 2019, Prop. 4.4.]:

m
. Consider two histograms a
i=1 ai = 1
}
Rma×mb

Rk (resp (Y1, . . . , Ymb )). The cost matrix is C

+ and b
∈
P 1mb = a, P (cid:62)1ma = b

log(P ), P
(cid:104)

Rm
+ |

∈
Xi
|

∆ma

P
{

C, P

a
{

+ (cid:15)

(cid:80)

−

∈

∈

∈

+

(cid:105)

|

W 2

(cid:15) (a, b) = min

a, za

+

za,zb (cid:104)

(cid:105)

b, zb
(cid:104)

+ (cid:15)

e−za/(cid:15), e−C/(cid:15)e−zb/(cid:15)
(cid:105)
(cid:104)
L4((za,zb),a)

(20)

This loss is strongly convex up to a constant shift on za, zb. The Sinkhorn
algorithm performs alternate minimization of

(cid:124)

(cid:123)(cid:122)
4 :

L

(cid:105)

(cid:125)

za

zb

←

←

(cid:15)(log(e−C/(cid:15)e−zb/(cid:15))
−
(cid:15)(log(e−C(cid:62)/(cid:15)e−za/(cid:15))

log(a)),

log(b)) .

−

This optimization technique is not covered by the results in section 3, but we
will see that the same conclusions hold in practice.

5.2 Examples of super-eﬃciency

To illustrate the tightness of our bounds, we evaluate numerically the convergence
of the diﬀerent estimators g1, g2 and g3 toward g∗ for the losses introduced above.
For all problems, g∗ is computed by estimating z∗(x) with gradient descent for
a very large number of iterations and then using (2).

Gradient Descent Figure 2 reports the evolution of
losses

gi
t −
4
j=1, where zt is obtained by gradient descent for
L
}

{L

|

j

g∗
1,

|
L

with t for the
3, and
2 and

L

14

Figure 3: Expected performances of gi for the SGD; (a) noise level as t
∞
for a constant step-size ρ; (b) Expected error as a function of the number of
iteration for decreasing step-size ρt = Ct−0.8. The solid line displays the mean
values and the shaded area the ﬁrst and last decile.

→

+

L

z∗

while

4. For the strongly convex losses (a),(b),

g∗
g1
by Sinkhorn iterations for
t −
|
|
converges linearly with the same rate as
converges about
zt
|
twice as fast. This conﬁrms the theoretical ﬁndings of Proposition 4 and (13).
The estimator g3 also converges with the predicted rates in (a),(b), however,
4 is ill-conditionned, leading to numerical
it fails in (d ) as the Hessian of
3, Figure 2.(c) shows that the
instabilities. For the non-strongly convex loss
L
rates given in Proposition 12 are correct as g1 converges with a rate t− 3
2 while
g2
t converges as t−3. Here, we did not include g3
t as it is equal to 0 due to the
particular form of

g2
t −

g∗

−

L

|

|

|

3.

L

Stochastic Gradient Descent
In Figure 3, we investigate the evolution of
expected performances of gi for the SGD, in order to validate the results of
2. The left part (a) displays the asymptotic
subsection 3.3. We consider
L
g∗
expected performance E[
] in the ﬁxed step case, as a function of the
|
|
step ρ, computed by running the SGD with suﬃciently many iterations to reach
a plateau. As predicted in subsection 3.3, the noise level scales as √ρ for g1
while it scales like ρ for g2 and g3. The right part (b) displays the evolution of
t−α. Here
E[
] as a function of t, where the step-size is decreasing ρt
|
|
again, the asymptotic rates predicted by Proposition 10 is showcased: g1
g∗ is
O(√t−α) while g2

g∗ are O(t−α).

g∗ and g3

gi
t −

gi
t −

g∗

∝

−

−

−

5.3 Example on a full training problem

We are now interested in the minimization of (cid:96) with respect to x, possibly under
constraints. We consider the problem of computing Wasserstein barycenters

15

Figure 4: Final optimization error δi relatively to (a) the number of inner
iterations used to estimation gi; (b) the time taken to reach this optimization
error level.

using mirror descent, as proposed in [Cuturi and Doucet, 2014]. For a set
Rn×m, the entropic
of histograms b1, . . . , bN
regularized Wasserstein barycenter of the bi’s is

+ and a cost matrix C

∆m

∈

∈

N

i=1
(cid:88)

N

4

L

(cid:0)

i=1
(cid:88)

x

arg min
x∈∆n
+

∈

(cid:96)(x) =

W 2

(cid:15) (x, bi) ,

where W 2

(cid:15) is deﬁned in (20), and we have:

(cid:96)(x) =

min
x ,z1

x,...,zN
z1

b ,...,zN
b

(zi

x, zi

b), x

.

(21)

(cid:1)

L

x, zi
The dual variables zi
b are obtained with t iterations of the Sinkhorn algorithm.
4 ((zx, zb), x) = zx. The cost function is then optimized
In this simple setting,
x
∇
ηgi)xq),
by mirror descent, with approximate gradient gi: xq+1 = P∆(exp(
n
i=1 xi is the projection on ∆n
+. Figure 4 displays the scale
where P∆(x) = x/
(cid:96)(x∗). We excluded g3 here as the computation were
of the error δi = (cid:96)(xq)
unstable – as seen in Figure 2.(c) – and too expensive. The error decreases much
faster with number of inner iteration t by using g2
t . However,
when looking at the time taken to reach the asymptotic error, we can see that
g1 is a better estimator in this case. This illustrates the fact that while g2 is
almost twice as good at approximating g∗ as g1, it is at least twice as expensive,
as discussed in subsection 4.2.

t compared to g1

(cid:80)

−

−

Conclusion

In this work, we have described the asymptotic behavior of three classical gradi-
ent estimators for a special instance of bi-level estimation. We have highlighted
a super-eﬃciency phenomenon of automatic diﬀerentiation. However, our com-
plexity analysis shows that it is faster to use the standard analytic estimator
when the optimization algorithm converges linearly, and that the super-eﬃciency

16

can be leveraged for algorithms with sub-linear convergence. This conclusion
should be taken with caution, as our analysis is only asymptotic. This sug-
gests a new line of research interested in the non-asymptotic behavior of these
estimators. Extending our results to a broader class of non-strongly convex
functions would be another interesting direction, as we observe empirically that
g∗)2. However, as the convexity alone does
for logistic-regression, g2
not ensure the convergence of the iterates, it raises interesting question for the
gradient estimation. Finally, it would also be interesting to extend our analysis
to non-smooth problems, for instance when zt is obtained with the proximal
gradient descent algorithm as in the case of ISTA for dictionary learning.

(g1

g∗

−

(cid:39)

−

Acknowledgement

P.A. and G.P. acknowledge support from the European Research Council (ERC-
NORIA). This work was funded in part by the French government under man-
agement of Agence Nationale de la Recherche as part of the "Investissements
d’avenir" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute).

References

Martın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeﬀrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoﬀrey Irving, Michael Isard,
Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G
Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: A system for large-scale
machine learning. In 12th USENIX Symposium on Operating Systems Design
and Implementation (OSDI), pages 265–283, 2016.

Akshay Agrawal, Brandon Amos, Shane Barratt, and Stephen Boyd. Diﬀer-
entiable Convex Optimization Layers. In Advances in Neural Information
Processing Systems (NeurIPS), pages 9558–9570, Vancouver, BC, Canada,
2019.

Martial Agueh and Guillaume Carlier. Barycenters in the wasserstein space.

SIAM Journal on Mathematical Analysis, 43(2):904–924, 2011.

Hedy Attouch and Jérôme Bolte. On the convergence of the proximal algo-
rithm for nonsmooth functions involving analytic features. Mathematical
Programming, 116(1-2):5–16, 2009.

Atılım Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and
Jeﬀrey Mark Siskind. Automatic Diﬀerentiation in Machine Learning: A
Survey. Journal of Machine Learning Research (JMLR), 18:1–43, 2018.

Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural com-

putation, 12(8):1889–1900, 2000.

17

Etienne Boursier and Vianney Perchet. Utility/Privacy Trade-oﬀ through the lens
of Optimal Transport. In International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS), Palermo, Italie, October 2019.

Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport.
In Advances in neural information processing systems, pages 2292–2300, 2013.

Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters.

Journal of Machine Learning Research (JMLR), 2014.

Ingrid Daubechies, Ronald DeVore, Massimo Fornasier, and C Sinan Güntürk.
Iteratively reweighted least squares minimization for sparse recovery. Commu-
nications on Pure and Applied Mathematics: A Journal Issued by the Courant
Institute of Mathematical Sciences, 63(1):1–38, 2010.

Olivier Devolder, François Glineur, and Yurii Nesterov. First-order methods
with inexact oracle: The strongly convex case. CORE Discussion Paper CORE
Discussion paper, CORE, 2013.

Olivier Devolder, François Glineur, and Yurii Nesterov. First-order methods of
smooth convex optimization with inexact oracle. Mathematical Programming,
146(1-2):37–75, August 2014.

Jean Feydy, Thibault Séjourné, François-Xavier Vialard, Shun-ichi Amari, Alain
Trouvé, and Gabriel Peyré. Interpolating between Optimal Transport and
MMD using Sinkhorn Divergences. In International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), pages 2681–2690, Okinawa, Japan,
2019.

Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models
with sinkhorn divergences. In International Conference on Artiﬁcial Intelli-
gence and Statistics, pages 1608–1617, 2018.

Jean-Charles Gilbert. Automatic diﬀerentiation and Iterative Processes. Opti-

mization Methods and Software, 1:13–21, 1992.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial
nets. In Advances in neural information processing systems, pages 2672–2680,
2014.

Karol Gregor and Yann Le Cun. Learning Fast Approximations of Sparse Coding.
In International Conference on Machine Learning (ICML), pages 399–406,
2010.

Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and

techniques of algorithmic diﬀerentiation, volume 105. Siam, 2008.

Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperpa-
rameter optimization through reversible learning. In International Conference
on Machine Learning, pages 2113–2122, 2015.

18

Julien Mairal, Francis R. Bach, Jean Ponce, and Guillermo Sapiro. Online
Learning for Matrix Factorization and Sparse Coding. Journal of Machine
Learning Research (JMLR), 11(1):19–60, 2010.

Julien Mairal, Francis R. Bach, and Jean Ponce. Task-driven dictionary learning.
IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 34
(4):791–804, 2012.

Eric Moulines and Francis R Bach. Non-Asymptotic Analysis of Stochastic
In Advances in Neural
Approximation Algorithms for Machine Learning.
Information Processing Systems (NeurIPS), pages 451–459, Grenada, Spain,
2011.

Bruno A Olshausen and David J Field. Sparse coding with an overcomplete
basis set: A strategy employed by v1? Vision research, 37(23):3311–3325,
1997.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,
Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,
Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin
Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie
Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance
Deep Learning Library. In Advances in Neural Information Processing Systems
(NeurIPS), page 12, Vancouver, BC, Canada, 2019.

Fabian Pedregosa. Hyperparameter optimization with approximate gradient.

arXiv preprint arXiv:1602.02355, 2016.

Gabriel Peyré and Marco Cuturi. Computational optimal transport, volume 11.

Now Publishers, Inc., 2019.

Bahareh Tolooshams, Sourav Dey, and Demba Ba. Scalable convolutional
dictionary learning with constrained recurrent sparse auto-encoders. In IEEE
International Workshop on Machine Learning for Signal Processing (MLSP),
2018.

John von Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen,

100(1):295–320, 1928.

19

20

A Experiments details and extra experiments

A.1 Experiments details

Super-eﬃciency of g2 for gradient descent
sizes are:

For Figure 2, the problem

•

•

•

•

L

Ridge regression
1: we use an overcomplete design matrix D with
n = 50 and m = 100 with entries Di,j drawn iid from a normal distribution
(0, 1). The vector x to evaluate the gradient is sampled also with iid
n . To compute g∗(x),

N
entries following a normal distribution. We take λ = 1
we used the gradient descent with step-size 1

L z for 14, 000 iterations.
2: we use an overcomplete design
Regularized Logistic regression
matrix D with n = 50 and m = 100 with entries Di,j drawn iid from
a normal distribution
(0, 1). The vector x to evaluate the gradient is
sampled also with iid entries following a normal distribution. We take
λ = 1
n . To compute g∗(x), we used the gradient descent with step-size 1
L z
for 40, 000 iterations.

N

L

L

Least mean p-th norm
3: In this setting, the convergence is much
slower than in the previous ones. We use an overcomplete design matrix
D with n = 5 and m = 10. To meet the condition of Proposition 12, we
Rn×m iid with normal distribution
sample the entries of a matrix A
∈
Rn×n unitary and V and
N
deﬁne D = U (cid:62)V . This ensures that DD(cid:62) = In We choose p = 4, and use
g∗ = 0.

(0, 1), take the SVD of A = U (cid:62)ΛV with U

∈

L

Wasserstein Distance
4: we consider here the problem of computing
the Wasserstein distance between two distributions supported on an Eu-
clidean grid in [0, 1] and C is deﬁned as the (cid:96)2 distance between the points
∆m
of the grid. q is in ∆n
+ with m = 30. We sample
∆m
+ by ﬁrst sampling the
a iid from a uniform distribution
(0, 1) and
a
(cid:98)a
l=1 al| . We used (cid:15) = 0.1 and g∗(x) is computed by running
then take a =
(cid:80)m
2, 000 iteration of Sinkhorn.

+ with n = 100 and b

∈

∈

U

(cid:98)

Super-eﬃciency of g2 for SGD For Figure 3, we consider for both
experiments the penalized logistic loss and an over-complete design matrix D
with n = 30 and m = 50 with entries Di,j drawn iid from a normal distribution
(0, 1). The vector x to evaluate the gradient is sampled also with iid entries
n . To compute g∗(x), we used the

N
following a normal distribution. We take λ = 1
gradient descent with step-size 1
Lz

for 1, 000 iterations.

In Figure 3.(a), we compute the gradient estimates gi

t using the output zt of
[0.001, 0.1] in log-scale for
the SGD with constant step-size ρ for 20 values of ρ
50 realization of the SGD. We report the mean value of
computed for t
large enough to have reached the regime where only the noise term is signiﬁcant
estimated by taking
in Proposition 8. This corresponds to the value of E
|

gi
t −
|
g∗

gi
t −

g∗

∈

|

|

21

gi
with the number of iterations t for (a) SGD
Figure A.1: Evolution of E
|
with a constant step-size ρ = 0.02; (b) SGD with a decreasing step-size ρt = t−α
for α = 0.8.

g∗

−

|

the value at the right end of the curve displayed in Figure A.1.(a) for diﬀerent
values of ρ.

In Figure 3.(b), we illustrate the evolution with t of E

gi
t computed
t −
|
for SGD with decreasing step-sizes t−α for α = .8. The expectation is estimated
and the area around the curve correspond
by averaging 50 realizations of
to the ﬁrst and 9-th deciles.

gi
t −
|

for gi

g∗

g∗

|

|

A.2 Gradient descent with inexact gradients

∈

∈

∆n

∆m

+ and bi

To evaluate the impact of the diﬀerent gradient estimators on the optimization
of the global function (cid:96), we run mirror descent for the loss (cid:96) deﬁned in (21). We
use n = m = 1, 000 and N = 20 for the dimensions of x
+ and
we sample following the same procedure as for the Wasserstein Distance. We
set (cid:15) = 0.05 and we used a step-size of η = 0.05. We compute x∗ by running
the mirror descent algorithm with analytic gradient estimator g1
t for t = 1, 000
(cid:96)(x∗) for g1
and q = 5, 000. Figure A.3 reports the residual errors (cid:96)(xq)
t and
g2
t relatively to the number of iterations t used to compute them (a) as well
as to the time taken to compute them (b). We exclude g3 from this analysis
as it is much more costly to compute in this case (see subsection A.3) and it
can be ill-conditionned – as it is illustrated in Figure 2. Figure A.2 displays for
each t used to compute Figure 4 the evolution of the cost function in iteration
q and in time. We can see here in both ﬁgures that as the number of iteration
t to compute the gradient increases, the ﬁnal optimization error decrease, as
predicted in Proposition 13. However, the computational cost for g2
t scales with
a factor c compared to computing g1
t and c is larger than 2 in this case (see

−

22

Figure A.2: Evolution of (cid:96)(xq)
estimator g1

t and automatic estimator g2
t .

(cid:96)
∗

−

with q for diﬀerent values of t and for analytic

subsection A.3). As the convergence of zt is linear, using g2
the global optimization as it possible to compute g1
optimization error compared to g1

t , as discussed in subsection 4.2.

t is not beneﬁcial for
ct instead which reduces the

A.3 Computation time of the gradient

∈

∆n

+ and bi

To evaluate the relative computational cost of the gradient estimates gi
t, we time
the computation of the gradient using the loss (cid:96) deﬁned in (21). We use n = 500,
m = 1, 000 and N = 100 for the dimensions of x
+ and we
sample following the same procedure as for the Wasserstein Distance. Then, we
time the computational time for the gradient estimators gi
t for diﬀerent values of
t, and report in Figure A.3 the median value of this computation time computed
on 50 realization as well as the ﬁrst and last decile values to get an idea of
the variation of this value. The results are coherent with the computational
complexity analysis in Table 1, g1 and g2 computation time scales linearly with t,
with a constant factor between them which capture the value of c which is around
3.5 in our case. For this scale of problem, g3 requires inverting N matrices n
n.
This cost dominates the cost of computing zt, g1
t for small value of t and
it becomes less prohibitive as t grows.

t and g2

∆m

×

∈

B Proof for section 2

Proposition 3. [Convergence of the implicit estimator] Assume that
LJ -Lipschitz with respect to its ﬁrst argument, and that
L as deﬁned in (9),

is
LJ . Then, for

t
(cid:107)J

(cid:107) ≤

J

Proof. We deﬁne

t =

J
J
g3 = g∗ + R(

L
2

(

g∗

g3
t −
|
(zt, x). The implicit gradient g3 reads

zt
+ LJ Lz)
|

2 .
|

| ≤

z∗

−

t)(zt

J

−

z∗) + Rxz +

tRzz .

J

(10)

23

