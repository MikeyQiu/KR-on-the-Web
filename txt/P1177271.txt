Exploring Models and Data for
Remote Sensing Image Caption Generation

Xiaoqiang Lu, Senior Member, IEEE, Binqiang Wang, Xiangtao Zheng, and Xuelong Li, Fellow, IEEE

1

7
1
0
2
 
c
e
D
 
1
2
 
 
]

V
C
.
s
c
[
 
 
1
v
5
3
8
7
0
.
2
1
7
1
:
v
i
X
r
a

Abstract—Inspired by recent development of artiﬁcial satellite,
remote sensing images have attracted extensive attention. Re-
cently, noticeable progress has been made in scene classiﬁcation
and target detection. However, it is still not clear how to describe
the remote sensing image content with accurate and concise
sentences. In this paper, we investigate to describe the remote
sensing images with accurate and ﬂexible sentences. First, some
annotated instructions are presented to better describe the remote
sensing images considering the special characteristics of remote
sensing images. Second,
in order to exhaustively exploit the
contents of remote sensing images, a large-scale aerial image
dataset is constructed for remote sensing image caption. Finally,
a comprehensive review is presented on the proposed dataset
to fully advance the task of remote sensing caption. Extensive
experiments on the proposed dataset demonstrate that the content
of the remote sensing image can be completely described by
generating language descriptions. The dataset is available at
https://github.com/201528014227051/RSICD optimal

Index Terms—remote sensing image, semantic understanding,

image captioning.

I. INTRODUCTION

W ITH the development of remote sensing technology,

remote sensing images have become available and
attracted extensive attention in numerous applications [1], [2],
[3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14],
[15]. However, the studies of the remote sensing images are
still concentrate on scene classiﬁcation [5], [16], [17], [18],
[19], object recognition [20], [21] and segmentation [22], [23],
[24], [25]. These works only recognize the objects in the
images or get the class labels of the images, but ignore the
attributes of the objects and the relation between each object
[26]. To present semantic information of the remote sensing
image, remote sensing image captioning aims to generate
comprehensive sentences that summarize the image content in
a semantic level [26]. This concise description of the remote
sensing scene plays a vital role in numerous ﬁelds, such
as image retrieval [27], scene classiﬁcation [9], and military
intelligence generation [28].

Image captioning is a difﬁcult but fundamental problem in
artiﬁcial intelligence. In the past few decades, many methods
have been designed for natural image captioning [29], [30],

This work was supported in part by the National Natural Science Foundation
of China under Grant 61761130079, in part by the Key Research Program of
Frontier Sciences, CAS under Grant QYZDY-SSW-JSC044, in part by the
National Natural Science Foundation of China under Grant 61472413, in part
by the National Natural Science Foundation of China under Grant 61772510,
and in part by the Young Top-notch Talent Program of Chinese Academy of
Sciences under Grant QYZDB-SSW-JSC015.

The authors are with the Xi’an Institute of Optics and Precision Mechanics,
Chinese Academy of Sciences, Xi’an 710119, Shaanxi, P. R. China and with
The University of Chinese Academy of Sciences, Beijing 100049, P. R. China.

[31], [32], [33], [34]. For natural image captioning, image
representation and sentence generation are two aspects that
the conventional methods is concentrated on. For image rep-
resentation, deep convolutional representation has conquered
the handcrafted representation. As for sentence generation, the
studies has developed from traditional retrieved-based method
to Recurrent Neural Network (RNN).

To better describe the image content, many image represen-
tations, including static global representations and dynamic
region representations, are considered. The global representa-
tions compress the whole image into a static representation
[29], while the dynamic region representations dynamically
analyze the image content based on multiple visual regions
[32]. To decode the image representations into natural lan-
guage sentences, several methods have been proposed for
generating image descriptions [29], [32], [33], [34], such as
Recurrent Neural Network (RNN), Long-Short Term Memory
networks (LSTM), retrieve based method and object detection
based method. All
these methods have shown promising
potentials in describing the images with concise sentences.

Although the aforementioned methods have achieved suc-
cess in natural image captioning [35], [36], [37], [32], they
may be inappropriate for the remote sensing image caption-
ing. Specially, the remote sensing image captioning is more
complex than the natural image captioning [26], [28], and the
semantics in remote sensing image become much ambiguous
from the “view of God”. For example, the remote sensing
images are captured from airplanes or satellites, making the
image contents complicated for ordinary people to describe.

Recently, some researchers have studied the remote sensing
image caption and generated the sentences from the remote
sensing image [26], [28]. Qu et al. [26] ﬁrstly proposed a deep
multimodal neural network model for semantic understanding
of the high resolution remote sensing images. Shi et al. [28]
proposed a remote sensing image captioning framework by
leveraging the techniques of Convolutional Neural Network
(CNN). Both methods used CNN to represent the image and
generated the corresponding sentences from the trained model
( recurrent neural networks in [26] and pre-deﬁned templates
in [28]). However, the generated sentences in both methods
are simple, which cannot describe the complex content in
remote sensing images as detailed as possible. Furthermore,
the evaluations of remote sensing captioning are typically
conducted on small datasets with a low diversity. The limited
datasets are insufﬁcient to approximate the remote sensing
applications.

In this paper, we investigate to describe the remote sensing
images with accurate and ﬂexible sentences. Firstly, some

special characteristics should be considered while annotating
the sentences in remote sensing captioning: 1) Scale Am-
biguity. The ground objects in remote sensing images may
present different semantics under different scales. 2) Category
Ambiguity. There are many categories in some regions of
remote sensing images. It is hard to describe the fused regions
with a single category label. 3) Rotation Ambiguity. The
remote sensing image can be viewed from different rotations,
since it
is took from the “view of God” without a ﬁxed
direction.

Then, a large-scale aerial image dataset is constructed for
remote sensing caption. In this dataset, more than ten thou-
sands remote sensing images are collected from Google Earth
[3], Baidu Map, MapABC, Tianditu. The images are ﬁxed to
224×224 pixels with various resolutions. The total number
of remote sensing images are 10921, with ﬁve sentences
descriptions per image. To the best of our knowledge, this
dataset is the largest dataset for remote sensing captioning.
The sample images in the dataset are with high intra-class
diversity and low inter-class dissimilarity. Thus, this dataset
provides the researchers a data resource to advance the task
of remote sensing captioning.

Finally, a comprehensive review is presented on the pro-
posed benchmark dataset (named as RSICD). Our dataset is
publicly available in BaiduPan1, Github2 and Google Drive3
. In this paper, we focus on the encoder-decoder frameworks
which are analogous to translating an image to a sentence [30].
To fully advance the task of remote sensing captioning, the
representative encoder-decoder frameworks [30] are evaluated
with various experimental protocols on the new dataset.

In summary, the main contributions of this paper contains

three aspects:

• To better describe the remote sensing images, some
special characteristics are considered: scale ambiguity,
category ambiguity and rotation ambiguity.

• A large-scale benchmark dataset of remote sensing im-
ages is presented to advance the task of remote sensing
image captioning.

• We present a comprehensive review of popular caption
methods on our dataset, and evaluate various image
representations and sentence generations methods using
handcrafted features and deep feature.

The rest of this paper is organized as follows: In Section II,
we review the related work of image captioning. The details of
RSICD dataset are described in Section III. Then, we provide
the description of encoder-decoder methods for benchmark
evaluation in Section IV. In Section V,
the evaluation of
baseline methods under different experimental settings are
given. Finally, we summarize the conclusions of the paper in
Section VI.

II. RELATED WORK

This section comprehensively reviews the existing image
image captioning and remote

captioning including natural

1http://pan.baidu.com/s/1bp71tE3
2https://github.com/201528014227051/RSICD optimal
3https://drive.google.com/open?id=0B1jt7lJDEXy3aE90cG9YSl9ScUk

2

sensing image captioning. Then the differences between nat-
ural
image and remote sensing image are discussed. The
difﬁculty of describing a remote sensing image and remote
sensing image caption problem are presented at last.

A. Natural image captioning

Natural image captioning, which generates a sentence to
describe a natural image, has been studied in computer vision
for several years [29], [30], [31], [32], [33], [34]. The methods
image captioning can be divided to three
of the natural
categories, including retrieved-based method, object detection
based method and encoder-decoder method.

The retrieved-based method generated sentences based on
the retrieval result [32]. These methods search for similar im-
ages with the query image ﬁrstly, and then generate sentences
based on the sentences which describe same kind images
in datasets. In this case, the grammatical structure of the
generated sentences is very poor, sometimes even unreadable.
This is because the quality of generated sentences is closely
dependent on the result of image retrieval. The retrieved result
would be embarrassed when the content of the image is very
different from the pictures in the database which are retrieved.
The second kind of method is based on object detection
[33], [34]. Object detection based methods detect the objects in
an image and then model the relation between the detected ob-
jects. The sentence is generated lastly by a sentence generating
model using the information describing the relations of objects
in image. The performance of the relation model is vital to the
generated sentence. And the result of the object detection also
has tremendous inﬂuence on the ﬁnally generated sentences.
The encoder-decoder methods [29], [30], [31] complete the
task in an end-to-end manner. These methods encoded an
image to a vector ﬁrstly, and then utilized a model to decode
the vector to a sentence. The deep neural models, usually using
Convolutional Neural Network (CNN) to extract features of
images and then using language generating models such as
Recurrent Neural Network (RNN) and a special RNN called
Long-Short Term Memory networks (LSTM), have made a
great progress in natural image captioning.

The encoder-decoder methods can generate promising sen-
tences to describe the natural
images. Inspired by recent
advances in computer vision and machine translation [38], a
deep model [39] is proposed to maximize the likelihood of the
target description sentence given the training image. Karpathy
et al. [29] presented a Multimodal Recurrent Neural Network
architecture to generate concise descriptions of image regions.
The Multimodal Recurrent Neural Network architecture is a
novel combination of three networks, including convolutional
neural networks to extract
image feature, bidirectional re-
current neural networks to represent sentences, and a struc-
tured objective embedding the image feature and sentence
representation. Xu et al. [30] proposed a caption generation
method using two kinds of attention mechanisms to decide
“where” or “what” to look in an image. Johnson et al. [31]
introduced a caption task, which asked computer vision system
to localize and describe outburst regions of a image using
natural language. To further complete the task, Johnson et

al. [31] proposed a Fully Convolutional Localization Network
(FCLN) architecture that can localize and describe regions
of an image at the same time. Yang et al. [40] proposed
a novel encoder-decoder framework, called review network.
It mimics a machine translation system, which translate the
source language sentence into the target language sentence,
take place the former sentence with image. Meanwhile, the
review network also takes attention mechanism into consid-
eration, by introducing attentive input reviewer and attentive
output reviewer.

B. Remote sensing image captioning

Although many methods have been proposed for natural
image captioning, only few studies on remote sensing image
captioning can be focused [28]. This is because there is
not an accredited dataset like Common Objects in Context
(COCO) dataset in natural image datasets. Qu et al. [26]
ﬁrstly proposed a deep multimodal neural network model
to generate sentences for the high resolution remote sensing
images. Shi et al. [28] proposed a remote sensing image
captioning framework by leveraging the techniques of deep
learning and Convolutional Neural Network (CNN). However,
the generated sentences of both methods are simple, which
cannot describe the special characteristics in remote sensing
images as detailed as possible. The special characteristics of
remote sensing images are as follows:

• The regions in remote sensing scene may cover many
land cover categories. For example, green plants are the
main things we can see in remote sensing image. And the
green plants include green grass, green crops, and green
trees, which are difﬁcult to distinguish one from another.
• The objects in remote sensing images are different from
those of natural images. Because there is no focus like
the
that of natural pictures, we need to describe all
important things in a remote sensing image.

• The objects in remote sensing exhibit rotation and transla-
tion variations. For a natural image, a building is usually
from bottom to up and a person often stand on ground
with their feet, hardly can we see a person with his head
on the ground and his feet up to the sky. For remote
sensing images, while, there is no difference between up
and down, left and right. This is because it is hard for
us to get the information how a remote sensing image be
captured in most conditions. In this case, the information
of the direction generated from a given remote sensing
image is not clear.

Considered the above characteristics, a large-scale remote
sensing captioning dataset is presented including more than
ten thousands remote sensing images.

III. DATASET FOR REMOTE SENSING IMAGE CAPTIONING

In this section, we ﬁrst reviews the existed remote sensing
image captioning dataset, and then described the proposed
Remote Sensing Image Captioning Dataset (RSICD).

3

A. Existing datasets for remote sensing image captioning

1) UCM-captions dataset: This dataset is proposed in [26],
which is based on the UC Merced Land Use Dataset [41].
It contains 21 classes land use image, including agricultural,
airplane, baseball diamond, beach, buildings, chaparral, dense
residential, forest, freeway, golf course, harbor, intersection,
medium residential, mobile home park, overpass, parking lot,
river, runway, sparse residential, storage tanks and tennis court,
with 100 images for each class. Every image is 256×256
pixels. The pixel resolution of these images is 0.3048m.
The images in UC Merced Land Use Dataset were manually
extracted from many large images from the United States
Geological Survey (USGS) National Map Urban Area Imagery.
Based on [26], ﬁve different sentences were exploited to
describe every image. The diversity of ﬁve coherent sentences
for one image are totally different, but the sentence difference
between images of the same class is very small.

2) Sydney-captions dataset: This dataset is also provided
by [26], which is based on the Sydney Data Set [42]. The
18000×14000 pixels large image of Sydney, Australia, was
got from Google Earth. The pixel resolution of the image is
0.5m. It contains seven classes after cropping and selecting,
including residential, airport, meadow, rivers, ocean, industrial,
and runway. Similar with UCM-captions dataset, ﬁve differ-
ent sentences were given to describe every image [26]. To
describe a remote sensing image exhaustively, we should pay
attention to the different attention of different people to an
image and different sentence patterns. Both datasets, including
UCM-captions and Sydney-captions, only focus on the latter
problem, but the ﬁrst problem should also be considered.

3) A untitled dataset: In [28], a untitled and undisclosed
dataset about remote sensing image captioning is proposed.
However, the sentences in [28] are more like a ﬁxed semantic
modal added on multi-objects detection. The sentences of this
dataset are just like “this image shows an urban area”, “this
image consists of some land structures”, “there is one large
airplane in this picture” and so on. These sentences are lack
of ﬂexibility and diversity.

B. RSICD: A new Dataset for Remote Sensing Image Cap-
tioning

To advance the state-of-the-art performances in remote
sensing image captioning, we construct a new remote sensing
image captioning dataset, named RSICD, for remote sensing
image captioning task. According to the variable scales and
rotation invariant of remote sensing images, some instructions
are provided to annotate RSICD dataset with comprehensive
sentences. As the images are captured from the airplane or
satellite, there is no concept of direction, such as up, down,
left, right, instead, we use ‘near’ and ‘next to’ to replace. In
the course of formulating instructions, we take the work of
natural image captioning [35], [36], [37] as a reference. All
the captions of RSICD are generated by volunteers with an-
notation experience and related knowledge of remote sensing
ﬁeld. We give ﬁve sentences to every remote sensing image.
To prove the diversity, we let every volunteer to provide one

4

Figure 1: The example of images and corresponding ﬁve sentences each image selected from our dataset.

or two sentences for a remote sensing image. And there are
some annotated instructions as follows.

• Describe all the important parts of the remote sensing

A. Multimodal method

• Do not start the sentences with “There is” when there are

more than one object in an image.

• Do not use the vague concept of words like large, tall,

many, in the absence of contrast.

• Do not use direction nouns, such as north, south, east and

image.

west.

• The sentences should contain at least six words.
The total number of sentences in RSICD is 24333, and the
total words of these sentences are 3323. The detailed informa-
tion on the annotations is as follows: 724 images are described
by ﬁve different sentences, 1495 images are described by
four different sentences, 2182 images are described by three
different sentences, 1667 images are described by two different
sentences and 4853 are described by one sentence. In order to
make the sentences richer, we extended the sentences to 54605
sentences by duplicating randomly the existing sentences when
there are not ﬁve different sentences to describe the same
image.

Some example sentences of images are shown in Figure 1.
The detailed information of the dataset is shown in Table I,
and the object classes in the proposed dataset are determined
by experts in remote sensing ﬁeld. The relations of objects in
showed images can be listed in “near”, “in two rows”, “in”,
“on”, “surrounded by”, “in two sides of”.

IV. REMOTE SENSING IMAGE CAPTIONING

We present the outline of encoder-decoder (encoding an
image to a vector, then decoding the vector to a sentence)
for remote sensing image captioning task in Figure 2. First,
we present remote sensing image representation methods (en-
coder). Then, the sentences generating models are introduced
(decoder).

The ﬁrst method [26] utilizes a deep multimodal neural
network to generate a coherent sentence for a remote sensing
image. We introduce the method in three parts: representing
remote sensing image, representing sentences and sentences
generation.

1) Representing remote sensing images: To represent the
remote sensing images, the existing image representation can
be classiﬁed into two groups: handcrafted feature methods and
learned feature methods. The handcrafted feature methods ﬁrst
extract the handcrafted features from each image and then
obtain image representation by feature encoding techniques
such as Bag Of Words (BOW) [43], Fisher Vector (FV) [44]
and Vector of Locally Aggregated Descriptors (VLAD) [45].
The learned deep feature methods can automatically learn
the image representation from the training data via machine
learning algorithms.

The performance of handcrafted feature methods strongly
relies on the extraction of handcrafted local features and
feature encoding methods. Lowe et al. [46] proposed a method
to extract Scale-Invariant Feature Transform (SIFT) features
which are invariant to image scale change and rotation. BOW
[43] represented a given image with the frequencies of local
visual words, while FV [44] uses gaussian mixture model
to encode the handcrafted local features. The VLAD [45]
concatenates the accumulation of the difference of the local
feature vector and its cluster center vector.

Compared with the handcrafted features, deep features in
computer vision have become more and more popular. The
features extracted by CNNs have made great progress in many
applications during the past several years [47], [48], [49], [50],
[51]. We use fully connected layers of several CNNs, including
AlexNet [47], VGGNet [50], GoogLeNet [51], pre-trained on
ImageNet dataset, to extract features of remote sensing images.

e0 = fF R (I) ,

(1)

5

Table I: Number of images of each class in RSICD datasets (the total number of images is 10921)

Class
Airport
Bare Land
Baseball Field
Beach
Bridge
Center
Church
Commercial
Dense Residential
Desert

Number
420
310
276
400
459
260
240
350
410
300

Class
Farmland
Forest
Industrial
Meadow
Medium Residential
Mountain
Park
School
Square
Parking

Number
370
250
390
280
290
340
350
300
330
390

Class
Playground
Pond
Viaduct
Port
Railway Station
Resort
River
Sparse Residential
Storage Tanks
Stadium

Number
1031
420
420
389
260
290
410
300
396
290

Figure 2: The outline of encoder-decoder for remote sensing image caption: including training process and test process

where I is a remote sensing image, e0 is the feature of the
remote sensing image whose dimension is notated u, and fF R
is the feature representations process which the feature can be
handcrafted feature or deep feature.

2) Representing sentences: In the ﬁrst method, every word
in a sentence is represented by a one-hot K dimension word
vector wi, where K is the size of the vocabulary. Then
the word vector is projected to an embedding space by the
embedding matrix Es. Es is a h × K matrix, where h is the
dimension of embedding space. Sentence y is encoded as a
sequence of h dimension projected word vectors [52]:

(2)

w(cid:48)

i = Es · wi,
1, ..., w(cid:48)

L},

y = {w(cid:48)

i, ..., w(cid:48)
where L is the length of the sentence.
3) Sentences Generation: In this subsection, a special Re-
current Neural Network (RNN), called Long Short-Term Mem-
ory networks (LSTM), is exploited to generate the sentences.
This is because Long Short-Term Memory networks is more
complicated than original recurrent neural networks.

(3)

Sentence generating process is more like a “human”, which
means that the human thoughts have persistence, and the next
word can be predicted according to previous words to some
extent. The Recurrent Neural Networks (RNNs) [53] satisfy
this property with loops allowing information to persist.

The structure of RNNs is shown in Figure 3. As shown in
Figure 3, some input xt, go through a neural network, A, and
output a value ht (the range of t is from 1 to N ), where the t
is the state of RNNs. The information contained by the input
is passed from one step to the next. The previous information
contained by the state of RNNs can be passed to the present
state is the main superiority of RNNs. But in some cases, the
information that present task needs is far away from the current

state. The problem of this long-term dependencies cannot be
well solved with RNNs [54].

To address the aforementioned problem, Long Short-Term
Memory networks (LSTM) is proposed in [55] to handle this
long-term dependencies problem. LSTM is designed to address
the long-dependency problem by using gates to control the
information passed through the networks. The structure of
LSTM is shown in Figure 4. The ﬁrst step of LSTM is using
a forget gate to decide what information to throw away. Then,
an input gate decides the values we will update, and a tanh
layer is used to generate candidate values that could be added
to the state of networks. The output is the cell state ﬁltered
by an output gate.

The process of sentence generating is considered as a
problem that maximizing the correct sentence generating prob-
ability conditioned on the given image information.

At training stage, the feature of the remote sensing image
I can be obtained by CNNs. Then the image feature and the
corresponding sentences are fed into RNN or LSTM to train
a model to predict word one by one given the image. The
procedure of the training can be represented by the following
formulations:

(cid:40)

ht =

g (w(cid:48)
g (w(cid:48)

t + Enht−1 + Eme0) ,
t + Enht−1) ,

t = 1,
t = 2, ..., N,

(4)

p(wt+1)i =

exp((Eoht)i)
k=1 exp((Eoht)k)

(cid:80)K+1

, i = 1, ..., K + 1,

(5)

where e0 is the feature of remote sensing image, En ∈ Rh×h,
Em ∈ Rh×u, Eo ∈ R(K+1)×h are learnable parameters. The
feature is imported only when t = 1 and the range of t is from
1 to N . g(·) represents the process of the RNN or LSTM.

6

Figure 3: The structure of RNNs.

Figure 4: The structure of LSTM.

ht is the output of state t whose dimension is h. w(cid:48)
t means
the corresponding words in the sentence y, and w(cid:48)
1 and w(cid:48)
N
are special token representing the START and END vector
respectively. The ﬁnal output Eoht (including the END token,
so the output dimension is K + 1) goes through a softmax
function 5 to get the probability of the next word p(wt+1) ∈
RK+1.

And the best parameters of the the proposed model can be
obtained at training stage by minimizing the following loss
function:

loss(I, y) = −

logp(wt),

(6)

N
(cid:88)

t=1

At sentence generating stage, the feature of remote sensing
image is fed into the model to generate word one by one
forming a sentence.

B. Attention based method

The second method is an attention based model proposed
in [39]. Two kinds of attention based manners are introduced
including a deterministic manner and a stochastic manner.
The deterministic manner uses standard backpropagation tech-
niques to train the model, while the stochastic manner trains
the model by maximizing a lower bound. In order to look
different parts of an image, attention based method uses a
different image representing method which is introduced as
follows.

1) Representing remote sensing images: The features of
lower convolution layers of CNNs represent the local feature
compared with the fully connected layer [56]. In order to
obtain the correspondence between the original remote sensing

image and the feature vectors, the features of convolution
layers are extracted in this method. For each remote sensing
image, M vectors are extracted and the dimension of each
vector corresponding to a part of the remote sensing image is
D.

a = {a1, ..., aM }, ai ∈ RD,

(7)

where a is called annotation vector contain a set of feature
vectors.

2) Representing sentences: In the attention based method,
a sentence y is encoded as a sequence of 1-of-K encoded
words:

y = {y1, ..., yL}, yi ∈ RK,

(8)

where K is the size of the vocabulary and L is the length of
the sentence.

3) Sentences Generation: The key idea of attention based
methods is to exploit the information of former state to decide
“where to look” in this state, and then the next word is
predicted based on this information. The method of generating
sentences in this subsection is LSTM.

The deterministic method, named as “soft” attention, gives a
weight to different parts of an image according to the informa-
tion of former state to decide “where to look”. The stochastic
method, named as “hard” attention, uses a sampling strategy
to look different parts of an image, then uses reinforcement
learning to get an overall better result.

In order to generate sentences using LSTM, the inputs of
LSTM in the second method are yt−1 and ˆzt unlike in the
ﬁrst method which instead are e0 and wt. ˆzt is called context
vector which is computed by different attention manners from
the annotation vectors ai.

7

The initial states of LSTM are predicted by an average of
annotation vectors imported through two separate Multi-layer
Perceptron (MLPs):

sentences in dataset. The range of CIDEr in tables II, III, IV,
VI, VII, VIII, IX, X is between 0 and 5. The bigger the CIDEr
is, the better the generated sentences are.

c0 = finit,c

h0 = finit,h

(cid:32)

(cid:32)

1
M

1
M

M
(cid:88)

(cid:33)

ai

,

M
(cid:88)

(cid:33)

ai

,

i

i

(9)

(10)

In the attention based method, a deep output layer is used
to compute output word probability given the context vector,
LSTM state and previous word:

p (cid:0)y|a, yt−1

(cid:1) = exp (Loyt−1 + Lhht + Lz ˆzt) ,

1

(11)

where Lo ∈ RK×m, Lh ∈ Rm×n, Lz ∈ Rm×D, and an
embedding matrix E are parameters initialized randomly.

V. EXPERIMENTS

We evaluate two methods mentioned before: multimodal
method and attention based method. The feature used for mul-
timodal method including deep CNNs representations (models
are pre-trained on ImageNet dataset) and handcrafted repre-
sentations such as SIFT, BOW, FV and VLAD. The datesets
used in this section are introduced in Section III. Firstly, we
introduce the metrics of remote sense image captioning. Sec-
ondly, we present the result of multimodal method. Then, the
result of attention based method is given. Finally, experimental
analysis is present to show that our dataset is more scientiﬁc
than others.

The experimental setup in this section: the word embedding
dimension and hidden state dimension of RNN are respectively
set to 256 and 256 for multimodal method, and the learning
rate of multimodal method is 0.0001. The word embedding
dimension and hidden state dimension of LSTM are respec-
tively set to 512 and 512 for attention based method, and the
learning rate of attention based method is 0.0001.

A. Metrics of remote sensing image captioning

The metrics used in this paper including BLEU [57],
ROUGE L [58], METEOR [59], CIDEr [60]. BLEU measures
the co-occurrences of n-gram between the generated sentence
and the reference sentence, where n-gram is a set of one
or more ordered words. n in this paper is 1, 2, 3 and 4.
ROUGE L is a metric calculating F-measure given the length
of the Longest Common Subsequence (LCS). Unlike n-gram,
the calculation of LCS includes the situation that there are
other words between the n-gram. METEOR is computed
by generating an alignment between the reference sentence
and generated sentence. CIDEr measures the consensus by
adding a Term Frequency Inverse Document Frequency (TF-
IDF) weighting for every n-gram. The CIDEr metric has more
reference value compared with BLEU, ROUGE L, METEOR
[60].

The range of the bleu1, bleu2, bleu3, bleu4, METEOR and
ROUGE L is between 0 and 1. And the closer to 1 the metrics
are, the more likely the generated sentence is to the reference

Except for objective metrics mentioned above, subjective
metrics are also given to better understand the quality of
generated sentences and the generalization capabilities of
models trained on different datasets in Section V-D2.

B. The results of multimodal method

In this subsection, we evaluate multimodal method based
on different kinds of features with randomly 80% for training,
10% for validation and 10% for test on UCM-captions dataset
[26], Sydney-captions [26] datasets and our dataset RSICD.
We ﬁrstly test four handcrafted representations for captioning,
and then use the different CNNs.

1) Results based on handcrafted representations: To eval-
uate the generated sentences based on handcrafted representa-
tions, four handcrafted representations are conducted including
SIFT, BOW, FV and VLAD. Each remote sensing image
sized 224×224 is segmented evenly to sixteen patches sized
56×56. For each patch, a SIFT feature is obtained by Prin-
cipal Component Analysis (PCA) of the origin SIFT features.
Finally, sixteen SIFT features are concatenated into a vector
to represent the image. Other handcrafted representations are
based on SIFT. For BOW representation, the dictionary size
is 1000. Tables II-IV illustrate that the result of LSTM is
better than that of RNN on UCM-captions dataset and RSICD
dataset. Since the LSTM solves the long-term dependencies
problem of RNNs, the sentence generated by LSTM can better
depict a remote sensing image than the one generated by RNNs
[55]. For all four handcrafted representations, VLAD performs
the best on UCM-captions dataset and RSICD dataset. The
result of LSTM on Sydney-captions is not as good as RNN.
But according to [61], LSTM should outperform RNN. This
is perhaps caused by the imbalance of Sydney-captions and
the further analysis is presented in Section V-D1.

2) Results based on different CNNs: In order to evaluate the
generated sentences based on CNNs features, the experiments
based on CNNs features are conducted in this subsection.
The experiments of different CNNs features on dataset UCM-
captions and Sydney-captions have been done in [26], and the
result on our dataset RSICD is shown in Table VI. The result
shows that all the CNNs features are better than handcrafted
features. All the CNNs features get almost the same result.
In CNNs features, AlexNet gets the best result on ROUGE L
and CIDEr and VGG19 gets the best result on other objective
metrics with a little superiority than others. This means that
representation of CNNs features are powerful for remote
sensing image captioning task.

3) Results of different training ratios: In order to study the
inﬂuence of the ratio of training images to the caption result,
We use 10% images of dataset for validation, and change the
ratio of training and testing images to observe the result based
on VGG16 CNNs features. The results of different sentence
generating methods based on different datasets are shown in
Figure 5. The abscissa represents the proportion of training
samples, and the ordinate is the corresponding result. The
different metrics are shown in different colors and shapes.

8

Table II: Result of multimodal method on UCM-captions dataset

Table III: Result of multimodal method on Sydney-captions dataset

bleu1
0.57303
SIFT
BOW 0.41067
0.5908
FV
0.63108
VLAD
SIFT
0.55168
BOW 0.39109
0.58972
0.70159

FV
VLAD

bleu1
0.58891
SIFT
BOW 0.53103
0.60545
FV
0.56581
VLAD
SIFT
0.57931
BOW 0.53103
0.63312
0.49129

FV
VLAD

bleu1
SIFT
0.47652
BOW 0.44007
0.48591
FV
0.49377
VLAD
SIFT
0.48591
BOW 0.48176
0.43418
0.50037

FV
VLAD

bleu2
0.44354
0.22494
0.46026
0.51928
0.41656
0.18767
0.46678
0.60853

bleu2
0.48179
0.40756
0.49113
0.45141
0.47741
0.40756
0.53325
0.34715

bleu2
0.28275
0.23826
0.30325
0.30914
0.30325
0.29082
0.24527
0.3195

bleu3
0.37963
0.14519
0.39681
0.46057
0.34891
0.10892
0.40799
0.54961

bleu3
0.42676
0.33192
0.4259
0.38065
0.41828
0.33192
0.47352
0.27598

bleu3
0.19611
0.15138
0.21861
0.22091
0.21861
0.20423
0.16345
0.23193

bleu4
0.33883
0.10951
0.35446
0.42087
0.30403
0.07058
0.36832
0.50302

bleu4
0.38935
0.2788
0.37855
0.32787
0.37400
0.2788
0.43031
0.23144

bleu4
0.14558
0.1041
0.16779
0.16767
0.16779
0.15334
0.11746
0.17777

METEOR
0.24611
0.10976
0.25975
0.2971
0.24324
0.1315
0.26975
0.34635

METEOR
0.26162
0.24905
0.27578
0.26718
0.27072
0.24905
0.29672
0.19295

METEOR
0.18814
0.16844
0.19662
0.19955
0.19662
0.19436
0.17115
0.20459

ROUGE L
0.53344
0.34394
0.54575
0.5878
0.52354
0.33136
0.5595
0.65197

ROUGE L
0.53923
0.49218
0.55402
0.52706
0.53660
0.49218
0.57937
0.42009

ROUGE L
0.39913
0.3605
0.41742
0.42417
0.41742
0.39948
0.38181
0.43338

CIDEr
1.70329
0.30717
1.68732
2.00655
1.36033
0.17812
1.84382
2.31314

CIDEr
1.21719
0.70189
1.1777
0.93724
0.98730
0.70189
1.47605
0.91644

CIDEr
0.78827
0.46671
1.05284
1.03918
1.05284
0.91521
0.65306
1.18011

Table IV: Result of multimodal method on RSICD dataset

RNN

LSTM

RNN

LSTM

RNN

LSTM

Figure 5: (a) Metrics of multimodal method using RNN on UCM-captions dataset. (b) Metrics of multimodal method using RNN on Sydney-captions dataset. (c) Metrics of
multimodal method using RNN on RSICD dataset. (d) Metrics of multimodal method using LSTM on UCM-captions dataset. (d) Metrics of multimodal method using LSTM on
Sydney-captions dataset. (f) Metrics of multimodal method using LSTM on RSICD dataset.

As shown in Figure 5, the results of all metrics are getting
higher following the increasement of the training ratio on

UCM-captions dataset [3]. But as for Sydney-captions dataset
in Figures 5 (b) and 5 (e), the metrics increase at ﬁrst and then

Table V: Number of images of each class in Sydney-captions dataset

Class
Residential
Airport
Meadow
River
Ocean
Industrial
Runway
Total

Number
242
22
50
45
92
96
66
613

Figure 6: Metrics of reference sentences on RSICD dataset.

Figure 7: Metrics of attention based method using LSTM on RSICD dataset.

keep an almost stable result. This is caused by the imbalanced
of the Sydney-captions dataset. As aforementioned, the most
generated sentences on Sydney-captions dataset are related to
“residential area”. From Figures 5 (c) and 5 (f), we can ﬁnd
that the performance is getting better following the increase
of the training ratio on RSICD ﬁrstly. Then a ﬂuctuation
occurs when the training ratio is between 60% to 80%. The
occurrence of this ﬂuctuation is because the some sentences in
RSICD are obtained by duplicating the existing sentences. In
order to evaluate the reference sentences of RSICD, the most
reliable metric CIDEr is used. As shown in Figure 6, the trend
of the CIDEr of reference sentences is the same as the trend
in Figures 5 (c) and 5 (f).

9

C. The results of attention based method

In this subsection, the results of attention based method are

discussed.

1) Results based on different CNNs: In order to evaluate the
attention based method based on different CNNs, experiments
are conducted below. Since the attention based method is based
on the convolutional feature of CNNs, the features used in
attention based method are all convolutional features extracted
by different CNN models. Speciﬁcally, for VGG16, the feature
maps of conv5 3 sized 14×14×512 are used; for VGG19,
the feature maps of conv5 4 sized 14×14×512 are used; for
AlexNet, the feature maps of conv5 sized 13×13×256 are
used; for GoogLeNet, the feature maps of inception 4c/3×3
sized 14×14×512 are used. The results of convolutional
features extracted by different models are shown in Tables
VII-IX. We can see that the results of “hard” attention mech-
anism are better than results of “soft” attention mechanism
in most conditions. The “hard” attention mechanism based
on the convolutional features extracted by GoogLeNet gets
the best result on UCM-captions dataset and RSICD dataset.
But for Sydney-captions dataset, the result of “soft” attention
mechanism based on the convolutional feature extracted by
VGG16 gets the best result.

2) Results of different training ratios: To evaluate the inﬂu-
ence of different training ratios, the training ratio is changed
to get different results. The results of attention based “hard”
method of different training ratios are shown in Figure 7. It
can be found that the metrics ﬁrstly increase and then keep
an almost stable result. Features extracted from convolutional
layer of AlexNet can represent exhaustive content of remote
sensing images. As shown in Figure 7, the metrics almost
keep stable when the training ratio is more than 20%. This is
because the representation capability of AlexNet for remote
sensing image captioning task is powerful.

Figure 8 shows some remote sensing image captioning
results. Most of the sentences generated describe the image
accurately. Some wrong places are shown in red color. In
fact, the wrong places of generated sentences are related to
the images. For example, the shape of many buildings in
the second image of the second row in Figure 8 is like a
railway station. The other reason of wrong places of generated
sentences is the high co-occurrences of two words in dataset.
For example, the “trees” and “buildings” are high frequency
co-occurrences words in dataset, so the generated sentence
of the third image of the second row in Figure 8 includes
“buildings” even though there is no building in the image.

D. Experimental analysis

To further validate the proposed dataset, two sets of ex-
periments are conducted in this subsection. The imbalance of
Sydney-captions datasets is analyzed ﬁrstly. Then, human sub-
jective evaluation is performed to compare the generalization
capabilities of models trained on different datasets.

1) The imbalance of Sydney-captions: To verify the in-
ﬂuence of unbalance of different kinds image numbers, we
present the result of FV using different numbers of cluster
center as the results of the FV is related to the number of

10

Table VI: Results of multimodal method using different CNNs on RSICD using LSTM

CNN
VGG19
VGG16
AlexNet
GoogLeNet

bleu1
0.58330
0.57951
0.57905
0.57847

bleu2
0.42259
0.41729
0.41871
0.41363

bleu3
0.33098
0.32462
0.32628
0.32042

bleu4
0.27022
0.26358
0.26552
0.2595

METEOR
0.26133
0.26116
0.26103
0.25856

ROUGE L
0.51891
0.51811
0.51913
0.51318

CIDEr
2.03324
2.01663
2.05261
2.00075

Table VII: Result of attention based method using different CNNs on UCM-captions dataset

CNN

VGG19

VGG16

AlexNet

GoogLeNet

CNN

VGG19

VGG16

AlexNet

GoogLeNet

CNN

VGG19

VGG16

AlexNet

GoogLeNet

Table VIII: Result of attention based method using different CNNs on Sydney-captions dataset

model
soft
hard
soft
hard
soft
hard
soft
hard

model
soft
hard
soft
hard
soft
hard
soft
hard

model
soft
hard
soft
hard
soft
hard
soft
hard

bleu1
0.74569
0.78493
0.7454
0.81571
0.79693
0.78498
0.76356
0.83751

bleu1
0.72818
0.73876
0.73216
0.75907
0.74128
0.74082
0.71284
0.76893

bleu1
0.64633
0.67925
0.67534
0.66685
0.65638
0.68968
0.67371
0.68813

bleu2
0.65976
0.70892
0.6545
0.73124
0.71345
0.70929
0.67662
0.76217

bleu2
0.63847
0.63988
0.66744
0.66095
0.65842
0.65373
0.62387
0.66125

bleu2
0.50718
0.53052
0.53084
0.51815
0.51489
0.5446
0.5303
0.54523

bleu3
0.59485
0.65387
0.58553
0.67024
0.6514
0.65182
0.61034
0.70420

bleu3
0.56323
0.56413
0.62226
0.58894
0.59043
0.58528
0.55274
0.58399

bleu3
0.41137
0.4296
0.43326
0.4164
0.41764
0.44396
0.43237
0.44701

bleu4
0.53932
0.60604
0.52502
0.61816
0.59895
0.60167
0.55371
0.65624

bleu4
0.50056
0.50223
0.58202
0.52582
0.53139
0.52538
0.4924
0.51699

bleu4
0.33959
0.35594
0.36168
0.34066
0.34464
0.36895
0.35982
0.3725

METEOR
0.39629
0.42760
0.38857
0.42630
0.41676
0.43058
0.40103
0.44887

METEOR
0.38699
0.37551
0.3942
0.38977
0.39634
0.37027
0.36746
0.37193

METEOR
0.33332
0.3286
0.32554
0.32007
0.32924
0.33521
0.33389
0.33224

ROUGE L
0.72421
0.76564
0.72368
0.76975
0.74952
0.77357
0.73996
0.79621

ROUGE L
0.71483
0.69795
0.71271
0.71885
0.7214
0.69774
0.6913
0.68421

ROUGE L
0.61632
0.61769
0.61089
0.6084
0.61039
0.62673
0.62119
0.62837

CIDEr
2.62906
2.84756
2.61236
2.99472
2.82368
3.01696
2.85671
3.2001

CIDEr
2.11849
2.04721
2.4993
2.18186
2.12846
2.19594
2.03435
1.9863

CIDEr
1.81861
1.89538
1.96432
1.79251
1.87415
1.98312
1.96519
2.02145

Table IX: Result of attention based method using different CNNs on RSICD dataset

cluster centers to construct a Gaussian Mixture Model (GMM).
The relations between the metrics with number of cluster
centers of FV are shown in Figs. 9, 10, 11 and 12. As shown in
Figs. 9 and 12, the metrics almost decrease with the increase
of the number of center centers of FV. From the result of
multimodal method based on FV of Sydney-captions dataset
in Figure 10, it can be seen that the change of the metrics
scores is not signiﬁcantly related to cluster center numbers
when the number of cluster centers are in the range of 1 to
10. We look through the generated sentences and ﬁnd that
almost all the generated sentences are related to “residential
area”. After considering the imbalanced of the dataset, as
shown in Table V, the reason is the imbalance of data between
different classes of images. The number of residential images
is reduced to 50, including 40 for training, 5 for validation
and 5 for test, with other classes unchanged. The result of the
experiment is shown in Figure 11. The generated sentences on
balanced Sydney-captions dataset get diversity, which means
the imbalance of dataset has a bad inﬂuence on the metrics
scores of generated sentences. Hence we consider the balance
factor when constructing dataset RSICD.

2) Generalization capabilities: To clarify the generalization
capabilities of models trained on different datasets, we illus-
trate the performance in Table X. For simplicity, the metrics
shown in Table X are METEOR and CIDEr. UCM model

represents the model trained on UCM-captions datasets. From
Table X, it can be found that the generalization capabilities
of the model trained on RSICD is better than model trained
on UCM-captions dataset evaluated on the test images of
Sydney-captions dataset. And the generalization capabilities
of the model trained on UCM-captions dataset is better than
model trained on Sydney-captions dataset evaluated on the
test images of RSICD. Compared with the result of model
trained on corresponding dataset, the models trained on other
dataset suffers a rapid decline in metrics. From Table X, the

Table X: Result of models trained on different datasets

model

METEOR

CIDEr

UCM-captions dataset

UCM model
Sydney model
RSICD model

0.34635
0.12727
0.10796

Sydney-captions dataset

UCM model
Sydney model
RSICD model

UCM model
Sydney model
RSICD model

0.09660
0.19295
0.10381
RSICD dataset
0.07416
0.08172
0.20459

2.31314
0.16630
0.13634

0.05614
0.91644
0.08680

0.08015
0.05765
1.18011

generalization of model trained on Sydney-captions dataset is
better than model trained on RSICD dataset on test images

11

Figure 8: The result of image captioning on RSICD dataset.

Figure 9: The relations between metrics of multimodal method using LSTM with number
of cluster centers of FV feature on UCM-captions dataset.

Figure 10: The relations between metrics of multimodal method using LSTM with
number of cluster centers of FV feature on Sydney-captions dataset.

of UCM-captions dataset. However, metrics used in Table
X are objective reference methods to evaluate the quality of
generated sentences. It is unfair to utilize the objective metrics
to judge the generalization capabilities of different model,
since the metrics are computed by comparing the generated

sentences with the reference sentences in datasets. When
computing the objective metrics, the sentences generated by
model trained on RSICD and sentences of test images from
other datasets are used to calculate. But subjective evaluation
criterion can give a far better evaluation of the generated

12

Table XIII: Result of subjective evaluation on RSICD

unrelated to image
unrelated to image
totally depict image

UCM model
84%
12%
4%

Sydney model
83%
15%
2%

RSICD model
46%
28%
26%

that 40% sentences generated by model trained on RSICD are
unrelated to test images of Sydney-captions dataset. So the
generalization capability of model trained on RSICD are better
than the models trained on other datasets, while almost half of
generated sentences are unrelated to the images. The remote
sensing image caption generation task needs to be studied in
the future work.

VI. CONCLUSION AND FUTURE WORK

In this paper, we give the instructions to describe remote
sensing images comprehensively, and construct a remote sens-
ing image captioning dataset (RSICD). Furthermore, to make
the dataset more comprehensive and balanced, we evaluate
different caption methods based on handcrafted representa-
tions and convolutional features on different datasets. Through
extensive experiments, we give benchmarks on our dataset
using the BLEU, METEOR, ROUGE L and CIDEr metric.
The experimental results show that the image caption methods
for natural image can be transferred to remote sensing image
to obtain only acceptable descriptions. But considering the
characteristics of remote sensing images, more works need
to be done on remote sensing image caption generation.

In future work, RSICD will be more comprehensive than
present version because some of the sentences are obtained by
duplicating the existing sentences. And we plan to apply some
new techniques in image processing ﬁeld and natural language
processing ﬁeld to remote sensing image caption generation
task.

REFERENCES

[1] Y. Gu, T. Liu, X. Jia, J. A. Benediktsson, and J. Chanussot, “Nonlinear
multiple kernel learning with multi-structure-element extended morpho-
logical proﬁles for hyperspectral image classiﬁcation,” IEEE Transaction
on Geoscience and Remote Sensing, vol. 54, no. 6, pp. 3235–3247, 2016.
[2] Y. Gu, Q. Wang, and B. Xie, “Multiple kernel sparse representation for
airborne lidar data classiﬁcation,” IEEE Transactions on Geoscience and
Remote Sensing, vol. PP, no. 99, pp. 1–21, 2017.

[3] G. S. Xia, J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong, L. Zhang, and X. Lu,
“Aid: A benchmark data set for performance evaluation of aerial scene
classiﬁcation,” IEEE Transactions on Geoscience and Remote Sensing,
vol. PP, no. 99, pp. 1–17, 2016.

[4] Y. Yuan, M. Fu, and X. Lu, “Substance dependence constrained sparse
nmf for hyperspectral unmixing,” IEEE Transactions on Geoscience and
Remote Sensing, vol. 53, no. 6, pp. 2975–2986, 2015.

[5] X. Lu, X. Zheng, and Y. Yuan, “Remote sensing scene classiﬁcation by
unsupervised representation learning,” IEEE Transactions on Geoscience
and Remote Sensing, vol. PP, no. 99, pp. 1–10, 2017.

[6] Y. Yuan, X. Zheng, and X. Lu, “Spectral–spatial kernel regularized for
hyperspectral image denoising,” IEEE Transactions on Geoscience and
Remote Sensing, vol. 53, no. 7, pp. 3815–3832, 2015.

[7] Z. Du, X. Li, and X. Lu, “Local structure learning in high resolution
remote sensing image retrieval,” Neurocomputing, vol. 207, pp. 813–822,
2016.

[8] M. Lu, L. Hu, T. Yue, Z. Chen, B. Chen, X. Lu, and B. Xu, “Penalized
linear discriminant analysis of hyperspectral imagery for noise removal,”
IEEE Geoscience and Remote Sensing Letters, vol. 14, no. 3, pp. 359–
363, 2017.

Figure 11: The relations between metrics of multimodal method using LSTM with
number of cluster centers of FV feature on balanced Sydney-captions dataset.

Figure 12: The relations between metrics of multimodal method using LSTM with
number of cluster centers of FV feature on RSICD dataset.

sentences. To evaluate the sentences in subjective criterion,
the sentences generated are divided into three levels: “related
to image”, “unrelated to image”, and “totally depict image”.
“related to image” means the sentences can capture the main
topics of the image, and maybe have some errors; “unrelated
to image” means the sentences have no relation with the main
topics of the image; “totally depict image” means the sentences
can depict main topics of the image correctly. The results of
models trained on different datasets evaluating on test images
of different datasets are shown in Tables XI-XIII.

Table XI: Result of subjective evaluation on UCM-captions dataset

unrelated to image
unrelated to image
totally depict image

UCM model
19%
20%
61%

Sydney model
86%
11%
3%

RSICD model
62%
32%
6%

Table XII: Result of subjective evaluation on Sydney-captions dataset

unrelated to image
unrelated to image
totally depict image

UCM model
79%
21%
0%

Sydney model
51%
17%
32%

RSICD model
60%
40%
0%

From Table XI, 38% sentences generated by model trained
on RSICD can describe the main topics of the test images
of UCM-captions dataset. From Table XII, it can be shown

13

[9] G. Cheng, J. Han, and X. Lu, “Remote sensing image scene classiﬁca-
tion: Benchmark and state of the art,” Proceedings of the IEEE, vol. PP,
no. 99, pp. 1–19, 2017.

[10] G. Cheng, P. Zhou, and J. Han, “Learning rotation-invariant convolu-
tional neural networks for object detection in vhr optical remote sensing
images,” IEEE Transactions on Geoscience and Remote Sensing, vol. 54,
no. 12, pp. 7405–7415, 2016.

[11] X. Yao, J. Han, G. Cheng, X. Qian, and L. Guo, “Semantic annotation of
high-resolution satellite images via weakly supervised learning,” IEEE
Transactions on Geoscience and Remote Sensing, vol. 54, no. 6, pp.
3660–3671, 2016.

[12] J. Han, D. Zhang, G. Cheng, L. Guo, and J. Ren, “Object detection
in optical remote sensing images based on weakly supervised learning
and high-level feature learning,” IEEE Transactions on Geoscience and
Remote Sensing, vol. 53, no. 6, pp. 3325–3337, 2015.

[13] X. Zheng, Y. Yuan, and X. Lu, “A target detection method for hyper-
spectral image based on mixture noise model,” Neurocomputing, vol.
216, pp. 331–341, 2016.

[14] Y. Yuan, X. Zheng, and X. Lu, “Discovering diverse subset for un-
supervised hyperspectral band selection,” IEEE Transactions on Image
Processing, vol. 26, no. 1, pp. 51–64, 2017.

[15] X. Lu, Y. Yuan, and X. Zheng, “Joint dictionary learning for multispec-
tral change detection,” IEEE Transactions on cybernetics, vol. 47, no. 4,
pp. 884–897, 2017.

[16] G. Cheng, J. Han, L. Guo, Z. Liu, S. Bu, and J. Ren, “Effective and
efﬁcient midlevel visual elements-oriented land-use classiﬁcation using
vhr remote sensing images,” IEEE Transactions on Geoscience and
Remote Sensing, vol. 53, no. 8, pp. 4238–4249, 2015.

[17] D. Tuia, F. Ratle, F. Paciﬁci, M. F. Kanevski, and W. J. Emery,
“Active learning methods for remote sensing image classiﬁcation,” IEEE
Transactions on Geoscience and Remote Sensing, vol. 47, no. 7, pp.
2218–2232, 2009.

[18] Y. Gu, Q. Wang, X. Jia, and J. A. Benediktsson, “A novel mkl model
of integrating lidar data and msi for urban area classiﬁcation,” IEEE
Transaction on Geoscience and Remote Sensing, vol. 53, no. 10, pp.
5312–5326, 2015.

generation with visual attention,” International Conference on Machine
Learning, pp. 2048–2057, 2015.

[31] J. Johnson, A. Karpathy, and L. Fei-Fei, “Densecap: Fully convolutional
localization networks for dense captioning,” IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 4565–4574, 2016.

[32] V. Ordonez, G. Kulkarni, and T. L. Berg, “Im2text: Describing images
using 1 million captioned photographs,” in Advances in Neural Infor-
mation Processing Systems, 2011, pp. 1143–1151.

[33] S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y. Choi, “Compos-
ing simple image descriptions using web-scale n-grams,” in Fifteenth
Conference on Computational Natural Language Learning, 2011, pp.
220–228.

[34] Y. Yang, C. L. Teo, D. H. Iii, and Y. Aloimonos, “Corpus-guided sen-
tence generation of natural images,” Conference on Empirical Methods
in Natural Language Processing, pp. 444–454, 2011.

[35] M. Hodosh, P. Young, and J. Hockenmaier, “Framing image description
as a ranking task: data, models and evaluation metrics,” Journal of
Artiﬁcial Intelligence Research, vol. 47, no. 1, pp. 853–899, 2013.
[36] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image
descriptions to visual denotations: New similarity metrics for semantic
inference over event descriptions,” Transactions of the Association for
Computational Linguistics, vol. 2, pp. 67–78, 2014.

[37] X. Chen, H. Fang, T. Y. Lin, R. Vedantam, S. Gupta, P. Dollar, and
C. L. Zitnick, “Microsoft coco captions: Data collection and evaluation
server,” Computer Science, 2015.

[38] K. Cho, B. V. Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using rnn
encoder-decoder for statistical machine translation,” Proceedings of the
Conference on Empirical Methods in Natural Language Processing, pp.
1724–1734, 2014.

[39] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A
neural image caption generator,” IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3156–3164, 2015.

[40] Z. Yang, Y. Yuan, Y. Wu, R. Salakhutdinov, and W. W. Cohen, “Review
networks for caption generation,” Advances in Neural Information
Processing Systems, pp. 2361–2369, 2016.

[19] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Transactions on
geoscience and remote sensing, vol. 42, no. 8, pp. 1778–1790, 2004.

[41] Y. Yang and S. Newsam, “Bag-of-visual-words and spatial extensions
for land-use classiﬁcation,” ACM SIGSPATIAL International Conference
on Advances in Geographic Information Systems, pp. 270–279, 2010.

[20] O. A. Penatti, K. Nogueira, and J. A. dos Santos, “Do deep features
generalize from everyday objects to remote sensing and aerial scenes
domains?” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition Workshops, 2015, pp. 44–51.

[21] J. Inglada, “Automatic recognition of man-made objects in high resolu-
tion optical remote sensing images by svm classiﬁcation of geometric
image features,” ISPRS Journal of Photogrammetry and Remote Sensing,
vol. 62, no. 3, pp. 236–248, 2007.

[22] J. Yuan, D. Wang, and R. Li, “Remote sensing image segmentation
by combining spectral and texture features,” IEEE Transactions on
geoscience and remote sensing, vol. 52, no. 1, pp. 16–24, 2014.
[23] G. Meinel and M. Neubert, “A comparison of segmentation programs
for high resolution remote sensing data,” International Archives of
Photogrammetry and Remote Sensing, vol. 35, no. Part B, pp. 1097–
1105, 2004.

[24] J. Fan, M. Han, and J. Wang, “Single point iterative weighted fuzzy
c-means clustering algorithm for remote sensing image segmentation,”
Pattern Recognition, vol. 42, no. 11, pp. 2527–2540, 2009.

[25] A. A. Farag, R. M. Mohamed, and A. El-Baz, “A uniﬁed framework
for map estimation in remote sensing image segmentation,” IEEE
Transactions on Geoscience and Remote Sensing, vol. 43, no. 7, pp.
1617–1634, 2005.

[26] B. Qu, X. Li, D. Tao, and X. Lu, “Deep semantic understanding of
high resolution remote sensing image,” International Conference on
Computer, Information and Telecommunication Systems, pp. 124–128,
2016.

[27] X. Lu, X. Zheng, and X. Li, “Latent semantic minimal hashing for image
retrieval,” IEEE Transactions on Image Processing, vol. 26, no. 1, pp.
355–368, 2017.

[28] Z. Shi and Z. Zou, “Can a machine generate humanlike language descrip-
tions for a remote sensing image?” IEEE Transactions on Geoscience
and Remote Sensing, vol. PP, no. 99, pp. 1–12, 2017.

[29] A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for
generating image descriptions,” IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3128–3137, 2015.

[30] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov,
R. Zemel, and Y. Bengio, “Show, attend and tell: Neural image caption

[42] F. Zhang, B. Du, and L. Zhang, “Saliency-guided unsupervised feature
learning for scene classiﬁcation,” IEEE Transactions on Geoscience and
Remote Sensing, vol. 53, no. 4, pp. 2175–2184, 2015.

[43] J. Sivic, A. Zisserman et al., “Video google: A text retrieval approach
to object matching in videos.” in IEEE International Conference on
Computer Vision, vol. 2, no. 1470, 2003, pp. 1470–1477.

[44] F. Perronnin, Y. Liu, J. Sanchez, and H. Poirier, “Large-scale image
retrieval with compressed ﬁsher vectors,” in Computer Vision and
Pattern Recognition, 2010, pp. 3384–3391.

[45] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and C. Schmid,
“Aggregating local image descriptors into compact codes,” IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, vol. 34, pp. 1704–
1716, 2011.

[46] Lowe and D. G, “Distinctive image features from scale-invariant key-
points,” International Journal of Computer Vision, vol. 60, no. 2, pp.
91–110, 2004.

[47] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” International Conference on
Neural Information Processing Systems, pp. 1097–1105, 2012.

[48] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for
fast feature embedding,” ACM International Conference on Multimedia,
pp. 675–678, 2014.

[49] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-
tional networks,” European Conference on Computer Vision, pp. 818–
833, 2014.

[50] K. Simonyan and A. Zisserman, “Very deep convolutional networks for

large-scale image recognition,” Computer Science, 2014.

[51] C. Szegedy, W. Liu, Y. Jia, and P. Sermanet, “Going deeper with convo-
lutions,” IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1–9, 2015.

[52] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning represen-
tations by back-propagating errors,” Cognitive modeling, vol. 5, no. 3,
p. 1, 1988.

[53] R. J. Williams and D. Zipser, “A learning algorithm for continually
running fully recurrent neural networks,” Neural computation, vol. 1,
no. 2, pp. 270–280, 1989.

14

[54] S. Hochreiter, “Untersuchungen zu dynamischen neuronalen netzen,”
in Master’s Thesis, Institut Fur Informatik, Technische Universitat,
Munchen, 1991.

[55] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

computation, vol. 9, no. 8, pp. 1735–1780, 1997.

[56] M. D. Zeiler and R. Fergus, “Visualizing and understanding con-
volutional networks,” in European Conference on Computer Vision.
Springer, 2014, pp. 818–833.

[57] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu, “Bleu: A method for
automatic evaluation of machine translation,” Association for Computa-
tional Linguistics, pp. 311–318, 2002.

[58] C. Flick, “Rouge: A package for automatic evaluation of summaries,”

in The Workshop on Text Summarization Branches Out, 2004, p. 10.

[59] M. Denkowski and A. Lavie, “Meteor universal: Language speciﬁc
translation evaluation for any target language,” in The Workshop on
Statistical Machine Translation, 2014, pp. 376–380.

[60] R. Vedantam, C. L. Zitnick, and D. Parikh, “Cider: Consensus-based
image description evaluation,” IEEE Conference on Computer Vision
and Pattern Recognition, pp. 4566–4575, 2015.

[61] J. Schmidhuber, F. Gers, and D. Eck, “Learning nonregular languages:
A comparison of simple recurrent networks and lstm,” Neural Compu-
tation, vol. 14, no. 9, pp. 2039–2041, 2002.

Xiaoqiang Lu (M’14-SM’15) is a full professor
with the Center for OPTical IMagery Analysis and
Learning (OPTIMAL), Xi’an Institute of Optics and
Precision Mechanics, Chinese Academy of Sciences,
Xi’an, Shaanxi, P. R. China. His current research in-
terests include pattern recognition, machine learning,
hyperspectral image analysis, cellular automata, and
medical imaging.

Binqiang Wang received the B.S. degree in the
school of computer science and engineering, North-
western Polytechnical University, Xi’an, Shaanxi, P.
R. China. He is currently working toward the Ph.D.
degree in signal and information processing in the
Center for OPTical IMagery Analysis and Learning
(OPTIMAL), Xi’an Institute of Optics and Precision
Mechanics, Chinese Academy of Sciences, Xi’an,
Shaanxi, P. R. China. His current research interests
include pattern recognition, computer vision and
machine learning.

Xiangtao Zheng received the M.Sc. and Ph.D.
degrees in signal and information processing from
the Chinese Academy of Sciences, Xi’an, China, in
2014 and 2017, respectively.

He is currently an Assistant Professor with the
Center for OPTical IMagery Analysis and Learning
(OPTIMAL), Xi’an Institute of Optics and Precision
Mechanics, Chinese Academy of Sciences, Xi’an,
Shaanxi, P. R. China. His main research interests
include computer vision and pattern recognition.

Xuelong Li (M’02-SM’07-F’12) is a full professor with the Xi’an Institute
of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an
710119, Shaanxi, P. R. China and with The University of Chinese Academy
of Sciences, Beijing 100049, P. R. China.

