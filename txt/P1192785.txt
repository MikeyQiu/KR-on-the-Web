Multilingual Clustering of Streaming News

Sebasti˜ao Miranda♦ Art ¯urs Znotin¸ ˇs†(cid:63)
Shay B. Cohen♥ Guntis Barzdins†(cid:63)
♦Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal
†Innovation Labs LETA, Marijas Str. 2, Riga LV-1050, Latvia
(cid:63)University of Latvia, IMCS, Rainis Blvd. 29, Riga LV-1459, Latvia
♥School of Informatics, University of Edinburgh, Edinburgh EH8 9AB
sebastiao.miranda@priberam.pt, arturs.znotins@leta.lv,
scohen@inf.ed.ac.uk, guntis.barzdins@lu.lv

Abstract

Clustering news across languages enables
efﬁcient media monitoring by aggregating ar-
ticles from multilingual sources into coherent
stories. Doing so in an online setting allows
scalable processing of massive news streams.
To this end, we describe a novel method for
clustering an incoming stream of multilingual
documents into monolingual and crosslingual
story clusters. Unlike
clustering
approaches that consider a small and known
number of labels, we tackle the problem of
discovering an ever growing number of cluster
labels in an online fashion, using real news
datasets in multiple languages. Our method
is
simple to implement, computationally
efﬁcient and produces state-of-the-art results
on datasets in German, English and Spanish.

typical

1

Introduction

Following developing news stories is imperative to
making real-time decisions on important political
and public safety matters. Given the abundance of
media providers and languages, this endeavor is an
extremely difﬁcult task. As such, there is a strong
demand for automatic clustering of news streams,
so that they can be organized into stories or themes
for further processing. Performing this task in an
online and efﬁcient manner is a challenging prob-
lem, not only for newswire, but also for scientiﬁc
articles, online reviews, forum posts, blogs, and
microblogs.

A key challenge in handling document streams
is that the story clusters must be generated on the
ﬂy in an online fashion: this requires handling doc-
uments one-by-one as they appear in the document
stream. In this paper, we provide a treatment to the
problem of online document clustering, i.e. the task
of clustering a stream of documents into themes.
For example, for news articles, we would want to
cluster them into related news stories.

To this end, we introduce a system which aggre-
gates news articles into ﬁne-grained story clusters
across different languages in a completely online
and scalable fashion from a continuous stream. Our
clustering approach is part of a larger media mon-
itoring project to solve the problem of monitor-
ing massive text and TV/Radio streams (speech-
to-text). In particular, media monitors write intelli-
gence reports about the most relevant events, and
being able to search, visualize and explore news
clusters assists in gathering more insight about
a particular story. Since relevant events may be
spawned from any part of the world (and from
many multilingual sources), it becomes imperative
to cluster news across different languages.

In terms of granularity, the type of story clusters
we are interested in are the group of articles which,
for example : (i) Narrate recent air-strikes in East-
ern Ghouta (Syria); (ii) Describe the recent launch
of Space X’s Falcon Heavy rocket.

Main Contributions While most existing news
clustering approaches assume a monolingual docu-
ment stream – a non-realistic scenario given the di-
versity of languages on the Web – we assume a gen-
eral, multilingual, document stream. This means
that in our problem-formulation story documents
appear in multiple languages and we need to cluster
them to crosslingual clusters. Our main contribu-
tions are as follows:

• We develop a system that aggregates news arti-
cles into ﬁne-grained story clusters across differ-
ent languages in a completely online and scalable
fashion from a continuous stream. As discussed
in the introduction, this is a highly relevant task
for the use-case of media monitoring.

• We formulate the problem of online multilingual
document clustering and the representation that
such clustering takes by interlacing the problem

8
1
0
2
 
p
e
S
 
3
 
 
]
L
C
.
s
c
[
 
 
1
v
0
4
5
0
0
.
9
0
8
1
:
v
i
X
r
a

of monolingual clustering with crosslingual clus-
tering. The representation of our clusters is inter-
pretable, and similarly to topic models, consists
of a set of keywords and weights associated with
the relevant cluster. In our formulation, a mono-
lingual cluster is a group of documents, and a
crosslingual cluster is a group of monolingual
clusters in different languages.

• We compare our approach to our own imple-
mentation of a state-of-the-art streaming method,
and show much superior results for a dataset in
English, Spanish and German.

2 Problem Formulation

We focus on clustering of a stream of documents,
where the number of clusters is not ﬁxed and
learned automatically. We denote by D a (poten-
tially inﬁnite) space of multilingual documents.
Each document d is associated with a language in
which it is written through a function L : D → L
where L is a set of languages. For example, L(d)
could return English, Spanish or German. (In the
rest of the paper, for an integer n, we denote by [n]
the set {1, . . . , n}.)

We are interested in associating each document
with a monolingual cluster via the function C(d) ∈
N, which returns the cluster label given a document.
This is done independently for each language, such
that the space of indices we use for each language
is separate.

Furthermore, we interlace the problem of mono-
lingual clustering with crosslingual clustering. This
means that as part of our problem formulation we
are also interested in a function E : N×L → N that
associates each monolingual cluster with a crosslin-
gual cluster, such that each crosslingual cluster only
groups one monolingual cluster per different lan-
guage, at a given time. The crosslingual cluster
for a document d is E(C(d), L(d)). As such, a
crosslingual cluster groups together monolingual
clusters, at most one for each different language.

Intuitively, building both monolingual and
crosslingual clusters allows the system to leverage
high-precision monolingual features (e.g., words,
named entities) to cluster documents of the same
language, while simplifying the task of crosslingual
clustering to the computation of similarity scores
across monolingual clusters - which is a smaller
problem space, since there are (by deﬁnition) less
clusters than articles. We validate this choice in §5.

3 The Clustering Algorithm

Each document d is represented by two vectors in
Rk1 and Rk2. The ﬁrst vector exists in a “mono-
lingual space” (of dimensionality k1) and is based
on a bag-of-words representation of the document.
The second vector exists in a “crosslingual space”
(of dimensionality k2) which is common to all lan-
guages. More details about these representations
are discussed in §4.

Online Clustering With our clustering algo-
rithm, we maintain two types of centroid functions
for each monolingual cluster. The ﬁrst is a cen-
troid function H : N × L → Rk1 ∪ {⊥} that as-
sists in associating each document with a mono-
lingual cluster. The second is a centroid function
G : N → Rk2 ∪ {⊥} that assists in associating each
monolingual cluster with a crosslingual cluster. The
⊥ symbol is reserved to denote documents which
are not associated with any cluster yet.

In our algorithm, we need to incrementally con-
struct the functions H, G (the two centroid func-
tions), C (the monolingual clustering function) and
E (the crosslingual clustering function). Informally,
we do so by ﬁrst identifying a monolingual cluster
for an incoming document by ﬁnding the closest
centroid with the function H, and then associate
that monolingual cluster with the crosslingual clus-
ter that is closest based on the function G. The ﬁrst
update changes C and the second update changes
E. Once we do that, we also update H and G to
reﬂect the new information that exists in the new
incoming document.

Example Figure 1 depicts the algorithm and the
state it maintains. A document in some language
(d9) appears in the stream, and is clustered into
one of the monolingual clusters (circles) that group
together documents about the same story (for ex-
ample, (cid:104)c2, DE(cid:105) could be a German cluster about a
recent political event). Then, following this mono-
lingual update, the online clustering algorithm up-
dates the crosslingual clusters (round rectangles),
each grouping together a set of monolingual clus-
ters, one per language at the most. The centroids
for the monolingual clusters are maintained by the
function H. For example, H(2, English) gives the
centroid of the upper left English monolingual clus-
ter. The function G maintains the crosslingual clus-
ters. Considering the upper-left most crosslingual
cluster, a1, then G(1) returns its centroid.

given crosslingual cluster. In our experiments, we
mostly use English as the pivot language.

H Update To update H, we maintain a centroid
for each cluster that is created as the average of
all monolingual representations of documents that
belong to that cluster. This is done for each lan-
guage separately. This update can be done in O(k1)
time in each step. Similarly, the update of G can be
done in O(k2) time. In principle, we consider an
“inﬁnite” stream of documents, which means the
number of documents in each cluster can be large.
As such, for efﬁciency purposes, updates to H are
immutable, which means that when a document is
assigned to a monolingual cluster, that assignment
is never changed.

G Update As described, updates to function G
result in associating a monolingual cluster with a
crosslingual cluster (and consequently, other mono-
lingual clusters). Therefore, errors committed in
updating G are of a higher magnitude than those
committed in H, since they involve groups of doc-
uments. We also note that the best crosslingual
cluster for a particular monolingual cluster might
not be found right at the beginning of the process.
We experiment with two types of updates to G.
One which is immutable, in which changes to G
are not reversed (and are described above), and
one in which we introduce a novel technique to
make a sequence of changes to G if necessary, as a
mechanism to self-correct past clustering decisions.
When a past decision is modiﬁed, it may result in a
chaining of consequent modiﬁcations (“toppling of
dominoes”) which need to be evaluated. We coin
this method “domino-toppling”.

The motivation behind this technique is the
change in news stories over time. The technique
allows the method to modify past crosslingual clus-
tering decisions and enables higher quality clus-
tering results. When a past decision is modiﬁed, it
may result in a chain of consequent modiﬁcations
which need to be evaluated.

Our method of “domino-toppling” works by
making (potentially sequences) of changes to pre-
vious clustering decisions for the crosslingual clus-
ters, at each step placing a residual monolingual
cluster in a crosslingual cluster that is most similar
to it. Figure 2 gives the pseudocode for domino
toppling.

This “domino-toppling” technique could have
in principle a quadratic complexity in the number

Figure 1: A pictorial description of the algorithm and
the state it maintains. The algorithm maintains a mono-
lingual cluster space, in which each cluster is a set
of documents in a speciﬁc language. The algorithm
also maintains a crosslingual cluster space, in which
a cluster is a set of monolingual clusters in different
languages. Documents are denoted by di, monolingual
clusters by ci (circles) and crosslingual clusters by ai.

Algorithm To be more precise, the online clus-
tering process works as follows. H and G start with
just returning ⊥ for any cluster number, both mono-
lingual and crosslingual. With a new incoming doc-
ument d, represented as a vector, we compute a
similarity metric Γ0 : Rk1 × Rk1 → R between the
document vector and each of the existing centroids
{i | H(i, L(d)) (cid:54)= ⊥}. If the largest similarity ex-
ceeds a threshold τ for cluster index j, then we set
C(d) = j. In that case, we also update the value of
H(i, L(d)) to include new information from doc-
ument d, as detailed below under “H update.” If
none of the similarity values exceed a threshold
τ , we ﬁnd the ﬁrst i such that H(i, L(d)) = ⊥
(the ﬁrst cluster id which is still unassigned), and
set C(d) = i, therefore creating a new cluster. We
again follow an “H update” – this time for starting
a new cluster.

In both cases, we also update the function G,
by selecting the best crosslingual cluster for the
recently updated (or created) monolingual clus-
ter. To this end, we use another similarity metric
Γ1 : Rk2 × Rk2 → R. Accordingly, we compute
the similarity (using Γ1) between the updated (or
created) monolingual cluster and all monolingual
clusters in each candidate crosslingual cluster, in
the crosslingual feature space. The crosslingual
cluster with highest sum of similarity scores is then
selected. We also experimented computing this sim-
ilarity by considering just the monolingual cluster
of a particular “pivot language”. The pivot language
is a language that serves as the main indicator for a

Inputs: A monolingual cluster c and a list of pairs
(cid:104)aj, Γ1(c, aj)(cid:105), j ∈ [N ].

Algorithm:

• For all pairs (cid:104)aj, Γ1(c, aj)(cid:105), j ∈ [N ], ordered by the

second coordinate:

• If L(c) is not in aj, add c to aj and break.
• Otherwise, let y ← M (aj, L(c)). If Γ1(c, aj) >

Γ1(y, aj) then:

• Add c to aj, remove y from aj and call
domino toppling with y playing the role of c
and break.

• If c is left unassigned, create aN +1 and add c to it.

Figure 2: Crosslingual “domino-toppling”. aj is the jth
crosslingual cluster (out of total N clusters) and Γ1 is
the similarity between them as in §4. L(c) is the lan-
guage for cluster c. M (a, (cid:96)) returns the monolingual
cluster for language (cid:96) ∈ L in crosslingual cluster a.
See text for details.

of crosslingual clusters. However, we have veri-
ﬁed that in practice it converges very fast, and in
our evaluation dataset only 1% of the crosslingual
updates result in topples. We apply this technique
only to update G (and not H) because reversing
cluster assignments in G can be done much more
efﬁciently than in H – the total number of mono-
lingual clusters (the clustered elements in G) is
signiﬁcantly smaller than the number of documents
(the clustered elements in H). Crosslingual cluster-
ing is also a harder problem, which motivated the
additional effort of developing this algorithm.

4 Document Representation

In this section, we give more details about the way
we construct the document representations in the
monolingual and crosslingual spaces. In particu-
lar, we introduce the deﬁnition of the similarity
functions Γ0 and Γ1 that were referred in §3.

Monolingual Representation The monolingual
representation for each document d in language
L(d) is a vector in Rk1 constructed from several
TF-IDF subvectors with words, word lemmas and
named entities. Each subvector is repeated for dif-
ferent sections of the document, the title, the body
and both of them together. Besides these text ﬁelds
and document timestamps, no other metadata was
used. To detect named entities, we used Priberam’s
Text Analysis (Amaral et al., 2008) for English and
Spanish, and Turbo Parser (Martins et al., 2013) for
German. The extracted entities consist of people,
organizations, places and other types.

Crosslingual Representation In the crosslin-
gual space, a document representation is a vec-
tor in Rk2. Let e(d, i) be a crosslingual embed-
ding of word i in the document d, which is a vec-
tor of length m. Then the document representa-
tion v(d) of d consists of subvectors of the form
v(d) = (cid:80)n
i=1 tie(d, i), where ti is the TF-IDF
score of the ith word in the relevant section of the
document (title, body or both). As detailed further
in §5 we compute IDF values from a large pre-
training dataset. Furthermore, for both the mono-
lingual and crosslingual cases, we also experiment
with using document timestamp features, as ex-
plained in §4.1. We use a new set of diverse times-
tamp features in addition to the simple absolute
difference (in hours) between timestamps used by
Rupnik et al. (2016).

4.1 Similarity Metrics

Our similarity metric computes weighted cosine
similarity on the different subvectors, both in the
case of monolingual clustering and crosslingual
clustering. Formally, for the monolingual case, the
similarity is given by a function deﬁned as:

3
(cid:88)

i=1

K
(cid:88)

i=1

Γ0(dj, cl) =

φi(dj, cl) · q0

i +

γi(dj, cl) · q1
i .

(1)
and is computed on the TF-IDF subvectors where
K is the number of subvectors for the relevant
document representation. For the crosslingual case,
we discuss below the function Γ1, which has a
similar structure.

Here, dj is the jth document in the stream and
cl is a monolingual cluster. The function φi(dj, cl)
returns the cosine similarity between the document
representation of the jth document and the centroid
for cluster cl. The vector q0 denotes the weights
through which each of the cosine similarity values
for each subvectors are weighted, whereas q1 de-
notes the weights for the timestamp features, as
detailed further. Details on learning the weights q0
and q1 are discussed in §4.2.

The function γ(d, c) that maps a pair of docu-

ment and cluster to R3 is deﬁned as follows. Let

f (t) = exp

−

(cid:18)

(cid:19)

(t − µ)2
2σ2

(2)

for a given µ and σ > 0. For each document
d and cluster c, we generate the following three-
dimensional vector γ(d, c) = (s1, s2, s3):

• s1 = f (t(d) − n1(c)) where t(dj) is the times-
tamp for document d and n1(c) is the timestamp
for the newest document in cluster c.

• s2 = f (t(d)−n2(c)) where n2(c) is the average

timestamp for all documents in cluster c.

• s3 = f (t(d) − n3(c)) where n3(c) is the times-

tamp for the oldest document in cluster c.

These three timestamp1 features model the time
aspect of the online stream of news data and help
disambiguate clustering decisions, since time is a
valuable indicator that a news story has changed,
even if a cluster representation has a reasonable
match in the textual features with the incoming
document. The same way a news story becomes
popular and fades over time (Lerman and Hogg,
2010), we model the probability of a document
belonging to a cluster (in terms of timestamp dif-
ference) with a probability distribution.

For the case of crosslingual clustering, we in-
troduce Γ1, which has a similar deﬁnition to Γ0,
only instead of passing document/cluster similarity
feature vectors, we pass cluster/cluster similarities,
across all language pairs. Furthermore, the features
are the crosslingual embedding vectors of the sec-
tions title, body and both combined (similarly to the
monolingual case) and the timestamp features. For
denoting the cluster timestamp, we use the average
timestamps of all articles in it.

i = 1 ∀i and q1

4.2 Learning to Rank Candidates
In §4.1 we introduced q0 and q1 as the weight
vectors for the several document representation
features. We experiment with both setting these
weights to just 1 (q0
j = 1 ∀j ∈ [3])
and also learning these weights using support vec-
tor machines (SVMs). To generate the SVM train-
ing data, we simulate the execution of the algorithm
on a training data partition (which we do not get
evaluated on) and in which the gold standard labels
are given. We run the algorithm using only the ﬁrst
subvector φ1(dj, cl), which is the TF-IDF vector
with the words of the document in the body and
title. For each incoming document, we create a col-
lection of positive examples, for the document and
the clusters which share at least one document in
the gold labeling. We then generate 20 negative ex-
amples for the document from the 20 best-matching
clusters which are not correct. To ﬁnd out the best-
matching clusters, we rank them according to their

1Timestamps are given in hours since 1970.

similarity to the input document using only the ﬁrst
subvector φ1(dj, cl).

Using this scheme we generate a collection
of ranking examples (one for each document in
the dataset, with the ranking of the best cluster
matches), which are then trained using the SVM-
Rank algorithm (Joachims, 2002). We run 5-fold
cross-validation on this data to select the best
model, and train both a separate model for each
language according to Γ0 and a crosslingual model
according to Γ1.

5 Experiments

Our system was designed to cluster documents
from a (potentially inﬁnite) real-word data stream.
The datasets typically used in the literature (TDT,
Reuters) have a small number of clusters (≈ 20)
with coarse topics (economy, society, etc.), and
therefore are not relevant to the use case of me-
dia monitoring we treat - as it requires much more
ﬁne-grained story clusters about particular events.
To evaluate our approach, we adapted a dataset
constructed for the different purpose of binary clas-
siﬁcation of joining cluster pairs.2 We processed it
to become a collection of articles annotated with
monolingual and crosslingual cluster labels.3

Statistics about this dataset are given in Ta-
ble 1. As described further, we tune the hyper-
parameter τ on the development set. As for the
hyper-parameters related to the timestamp features,
we ﬁxed µ = 0 and tuned σ on the development set,
yielding σ = 72 hours (3 days).4 To compute IDF
scores (which are global numbers computed across
a corpus), we used a different and much larger
dataset that we collected from Deutsche Welle’s
news website (http://www.dw.com/). The
dataset consists of 77,268, 118,045 and 134,243
documents for Spanish, English and German, re-
spectively.

The conclusions from our experiments are: (a)
the weighting of the similarity metric features using
SVM signiﬁcantly outperforms unsupervised base-
lines such as CluStream (Table 2); (b) the SVM
approach signiﬁcantly helps to learn when to cre-
ate a new cluster, compared to simple grid search

2https://github.com/rupnikj/jair_paper
3The code and data we used is available at https://

github.com/priberam/news-clustering.

4This shows a relative robustness to reordering of the arti-
cles – articles within 3 days of each other could appear any-
where in that window, and the algorithm would still perform
well.

i
a
r
t

Dataset
n English
German
Spanish
t English
German
Spanish

s
e
t

Size
12,233
4,043
4,527
8,726
2,101
2,177

Avg. L.
434
282
355
521
440
392

C
593
377
416
222
118
149

Avg. S.
21
11
11
39
18
15

Table 1: Statistics for the development and evaluation
datasets, constructed from the dataset in Rupnik et al.
(2016), as explained in §5. “Size” denotes the number
of documents in the collection, “Avg. L.” is the aver-
age number of words in a document, “C” denotes the
number of clusters in the collection and “Avg. S.” is the
average number of documents in each cluster.

for the optimal τ (Table 4); (c) separating the fea-
ture space into one for monolingual clusters in the
form of keywords and the other for crosslingual
clusters based on crosslingual embeddings signiﬁ-
cantly helps performance.

Evaluation Method We evaluate clustering in
the following manner: let tp be the number of cor-
rectly clustered-together document pairs, let fp be
the number of incorrectly clustered-together docu-
ment pairs and let fn be the number of incorrectly
not-clustered-together document pairs. Then we
report precision as
tp+fn and F1
as the harmonic mean of the precision and recall
measures.

tp+fp , recall as

tp

tp

We do the same to evaluate crosslingual cluster-
ing, but on a higher level: we count tp, fn and fp for
the decisions of clustering clusters, as crosslingual
clusters are groups of monolingual gold clusters.

5.1 Monolingual Results

In our ﬁrst set of experiments, we report results
on monolingual clustering for each language sepa-
rately. Monolingual clustering of a stream of doc-
uments is an important problem that has been in-
spected by others, such as by Ahmed et al. (2011)
and by Aggarwal and Yu (2006). We compare our
results to our own implementation of the online
micro-clustering routine presented by Aggarwal
and Yu (2006), which shall be referred to as CluS-
tream. We note that CluStream of Aggarwal and
Yu (2006) has been a widely used state-of-the-art
system in media monitoring companies as well as
academia, and serves as a strong baseline to this
day.

In our preliminary experiments, we also evalu-
ated an online latent semantic analysis method, in
which the centroids we keep for the function H (see

TOKENS+LEMMAS+ENTS

s
i
l
g
n
E

algorithm
h CluStream

+TS
n CluStream
a
m
r
e
G

+TS
h CluStream
s
i
n
a
p
S

+TS

TOKENS+LEMMAS+ENTS

TOKENS+LEMMAS+ENTS

F1
79.0
92.7
94.1
89.7
90.7
97.1
78.1
88.8
94.2

P
98.6
92.9
98.2
99.9
99.7
99.9
73.4
95.9
97.0

R
65.9
92.5
90.3
81.3
83.2
94.5
83.5
82.7
91.6

Table 2: Clustering results on the labeled dataset. We
compare our algorithm (with and without timestamps)
with the online micro-clustering routine of Aggarwal
and Yu (2006) (denoted by CluStream). The F1 values
are for the precision (P) and recall (R) in the follow-
ing columns. See Table 3 for a legend of the different
models. Best result for each language is in bold.

§3) are the average of reduced dimensional vectors
of the incoming documents as generated by an in-
cremental singular value decomposition (SVD) of
a document-term matrix that is updated after each
incoming document. However, we discovered that
online LSA performs signiﬁcantly worse than rep-
resenting the documents the way is described in §4.
Furthermore, it was also signiﬁcantly slower than
our algorithm due to the time it took to perform
singular value decomposition.5

Clustering experiments Table 2 gives the ﬁnal
monolingual results on the three datasets. For En-
glish, we see that the signiﬁcant improvement we
get using our algorithm over the algorithm of Ag-
garwal and Yu (2006) is due to an increased recall
score. We also note that the trained models surpass
the baseline for all languages, and that the times-
tamp feature (denoted by TS), while not required
to beat the baseline, has a very relevant contribu-
tion in all cases. Although the results for both the
baseline and our models seem to differ across lan-
guages, one can verify a consistent improvement
from the latter to the former, suggesting that the
score differences should be mostly tied to the differ-
ent difﬁculty found across the datasets for each lan-
guage. The presented scores show that our learning
framework generalizes well to different languages
and enables high quality clustering results.

To investigate the impact of the timestamp fea-

5More speciﬁcally, we used an object of type lsimodel
from the GenSim package that implements algorithms from
ˇReh˚uˇrek (2010). The GenSim package can be found at
https://pypi.python.org/pypi/gensim.

feature
TOKENS
TOKENS+LEMMAS
TOKENS+LEMMAS+ENTS
TOKENS+LEMMAS+ENTS+TS

accuracy
85.5
85.9
86.5
96.9

Table 3: Accuracy of the SVM ranker on the English
training set. TOKENS are the word token features, LEM-
MAS are the lemma features for title and body, ENTS
are named entity features and TS are timestamp fea-
tures. All features are described in detail in §4, and are
listed for both the title and the body.

tures, we ran an additional experiment using only
the same three timestamp features as used in the
best model on the English dataset. This experi-
ment yielded scores of F1 = 61.1, P = 44.5 and
R = 97.6, which lead us to conclude that while
these features are not competitive when used alone
(hence temporal information by itself is not sufﬁ-
cient to predict the clusters), they contribute signif-
icantly to recall with the ﬁnal feature ensemble.

We note that as described in §3, the optimiza-
tion of the τ parameter is part of the development
process. The parameter τ is a similarity thresh-
old used to decide when an incoming document
should merge to the best cluster or create a new
one. We tune τ on the development set for each
language, and the sensitivity to it is demonstrated
in Figure 3 (this process is further referred to as
τsearch). Although applying grid-search on this pa-
rameter is the most immediate approach to this
problem, we experimented with a different method
which yielded superior results: as described further,
we discuss how to do this process with an additional
classiﬁer (denoted SVM-merge), which captures
more information about the incoming documents
and the existing clusters.

Additionally, we also experimented with comput-
ing the monolingual clusters with the same embed-
dings as used in the crosslingual clustering phase,
which yielded poor results. In particular, this sys-
tem achieved F1 score of 74.8 for English, which
is below the bag-of-words baseline presented in
Table 2. This result supports the approach we then
followed of having two separate feature spaces for
the monolingual and crosslingual clustering sys-
tems, where the monolingual space is discrete and
the crosslingual space is based on embeddings.

SVM ranker experiments To investigate the im-
portance of each feature, we now consider in Ta-

Figure 3: The F1 score of the different language de-
velopment sets as a function of the threshold τ . The
ﬁrst point for each language is identiﬁed using binary
search.

model
τsearch
SVM-merge

F1
82.8
94.1

P
96.5
98.2

R
72.4
90.3

Table 4: Comparison of two different cluster decision
techniques for the English SVM model with all fea-
tures (see Table 2). The ﬁrst method, τsearch, corre-
sponds to executing grid-search to ﬁnd the optimal clus-
tering τ parameter (see §3). SVM-merge is an alterna-
tive method in which we train an SVM binary classi-
ﬁer to decide if a new cluster should be created or not,
where we use as features the maximal value of each
coordinate for each document in a cluster.

ble 3 the accuracy of the SVM ranker for English
as described in §4.1. We note that adding features
increases the accuracy of the SVM ranker, espe-
cially the timestamp features. However, the times-
tamp feature actually interferes with our optimiza-
tion of τ to identify when new clusters are needed,
although they improve the SVM reranking accu-
racy. We speculate this is true because high accu-
racy in the reranking problem does not necessarily
help with identifying when new clusters need to be
opened. To investigate this issue, we experimented
with a different technique to learn when to create a
new cluster. To this end, we trained another SVM
classiﬁer just to learn this decision, this time a bi-
nary classiﬁer using LIBLINEAR (Fan et al., 2008),
by passing the max of the similarity of each feature
between the incoming document and the current
clustering pool as the input feature vector. This
way, the classiﬁer learns when the current clusters,

crosslingual model
τsearch (global)
τsearch (pivot)

F1
72.7
84.0

P
89.8
83.0

R
61.0
85.0

6 Related Work

Table 5: Crosslingual clustering results when consid-
ering two different approaches to compute distances
across crosslingual clusters on the test set for Spanish,
German and English. See text for details.

as a whole, are of a different news story than the
incoming document. As presented in Table 4, this
method, which we refer to as SVM-merge, solved
the issue of searching for the optimal τ parame-
ter for the SVM-rank model with timestamps, by
greatly improving the F1 score in respect to the
original grid-search approach (τsearch).

5.2 Crosslingual Results

As mentioned in §3, crosslingual embeddings are
used for crosslingual clustering. We experimented
with the crosslingual embeddings of Gardner et al.
(2015) and Ammar et al. (2016). In our preliminary
experiments we found that the former worked better
for our use-case than the latter.

We test two different scenarios for optimizing
the similarity threshold τ for the crosslingual case.
Table 5 shows the results for these experiments.
First, we consider the simpler case of adjusting a
global τ parameter for the crosslingual distances, as
also described for the monolingual case. As shown,
this method works poorly, since the τ grid-search
could not ﬁnd a reasonable τ which worked well
for every possible language pair.

Subsequently, we also consider the case of using
English as a pivot language (see §3), where dis-
tances for every other language are only compared
to English, and crosslingual clustering decisions
are made only based on this distance.6 This yielded
our best crosslingual score of F1=84.0, conﬁrm-
ing that crosslingual similarity is of higher quality
between each language and English, for the em-
beddings we used. This score represents only a
small degradation in respect to the monolingual
results, since clustering across different languages
is a harder problem.

6In this case, all crosslingual clusters will have at least one
pivot monolingual cluster, except for clusters which might
stay unmerged as single-language degenerated crosslingual
clusters.

Early research efforts, such as the TDT pro-
gram (Allan et al., 1998), have studied news clus-
tering for some time. The problem of online mono-
lingual clustering algorithms (for English) has also
received a fair amount of attention in the litera-
ture. One of the earlier papers by Aggarwal and
Yu (2006) introduced a two-step clustering system
with both ofﬂine and online components, where the
online model is based on a streaming implemen-
tation of k-means and a bag-of-words document
representation. Other authors have experimented
with distributed representations, such as Ahmed
et al. (2011), who cluster news into storylines us-
ing Markov chain Monte Carlo methods, ˇReh˚uˇrek
and Sojka (2010) who used incremental Singular
Value Decomposition (SVD) to ﬁnd relevant topics
from streaming data, and Sato et al. (2017) who
used the paragraph vector model (Le and Mikolov,
2014) in an ofﬂine clustering setting.

More recently, crosslingual linking of clusters
has been discussed by Rupnik et al. (2016) in the
context of linking existing clusters from the Event
Registry (Leban et al., 2014) in a batch fashion,
and by Steinberger (2016) who also present a batch
clustering linking system. However, these are not
“truly” online crosslingual clustering systems since
they only decide on the linking of already-built
monolingual clusters. In particular, Rupnik et al.
(2016) compute distances of document pairs across
clusters using nearest neighbors, which might not
scale well in an online setting. As detailed before,
we adapted the cluster-linking dataset from Rup-
nik et al. (2016) to evaluate our online crosslingual
clustering approach. Preliminary work makes use
of deep learning techniques (Xie et al., 2016; Guo
et al., 2017) to cluster documents while learning
their representations, but not in an online or mul-
tilingual fashion, and with a very small number of
cluster labels (4, in the case of the text benchmark).

In our work, we studied the problem of mono-
lingual and crosslingual clustering, having exper-
imented several directions and methods and the
impact they have on the ﬁnal clustering quality. We
described the ﬁrst system which aggregates news
articles into ﬁne-grained story clusters across differ-
ent languages in a completely online and scalable
fashion from a continuous stream.

7 Conclusion

We described a method for monolingual and
crosslingual clustering of an incoming stream of
documents. The method works by maintaining cen-
troids for the monolingual and crosslingual clus-
ters, where a monolingual cluster groups a set of
documents and a crosslingual cluster groups a set
of monolingual clusters. We presented an online
crosslingual clustering method which auto-corrects
past decisions in an efﬁcient way. We showed that
our method gives state-of-the-art results on a mul-
tilingual news article dataset for English, Spanish
and German. Finally, we discussed how to leverage
different SVM training procedures for ranking and
classiﬁcation to improve monolingual and crosslin-
gual clustering decisions. Our system is integrated
in a larger media monitoring project (Liepins et al.,
2017; Germann et al., 2018) and solving the use-
cases of monitors and journalists, having been vali-
dated with qualitative user testing.

Acknowledgments

We would like to thank Esma Balkır, Nikos Pa-
pasarantopoulos, Afonso Mendes, Shashi Narayan
and the anonymous reviewers for their feedback.
This project was supported by the European H2020
project SUMMA, grant agreement 688139 (see
http://www.summa-project.eu) and by a
grant from Bloomberg.

References

Charu C. Aggarwal and Philip S. Yu. 2006. A frame-
work for clustering massive text and categorical data
streams. In SDM, pages 479–483. SIAM.

Amr Ahmed, Qirong Ho, Jacob Eisenstein, Eric Xing,
Alexander J Smola, and Choon Hui Teo. 2011. Uni-
In Proceedings
ﬁed analysis of streaming news.
of the 20th international conference on World wide
web, pages 267–276. ACM.

James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, Yiming Yang, et al. 1998. Topic
detection and tracking pilot study: Final report. In
Proceedings of the DARPA broadcast news tran-
scription and understanding workshop.

Carlos Amaral, Ad´an Cassan, Helena Figueira, Andr´e
Martins, Afonso Mendes, Pedro Mendes, Jos´e Pina,
and Cl´audia Pinto. 2008. Priberam’s question an-
In Proceed-
swering system in QA@ CLEF 2008.
ings of the Workshop of the Cross-Language Evalu-
ation Forum for European Languages.

Waleed Ammar, George Mulcaire, Yulia Tsvetkov,
Guillaume Lample, Chris Dyer, and Noah A Smith.
2016. Massively multilingual word embeddings.
arXiv preprint arXiv:1602.01925.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classiﬁcation. Journal of ma-
chine learning research, 9(Aug):1871–1874.

Matt Gardner, Kejun Huang, Evangelos Papalex-
akis, Xiao Fu, Partha Talukdar, Christos Falout-
sos, Nicholas Sidiropoulos, and Tom Mitchell. 2015.
Translation invariant word embeddings. In Proceed-
ings of EMNLP.

Ulrich Germann, Renars Liepins, Guntis Barzdins,
Didzis Gosko, Sebasti˜ao Miranda, and David
Nogueira. 2018. The summa platform: A scalable
infrastructure for multi-lingual multi-media monitor-
ing. Proceedings of ACL 2018, System Demonstra-
tions.

Xifeng Guo, Long Gao, Xinwang Liu, and Jianping
Yin. 2017. Improved deep embedded clustering with
In Proceedings of IJ-
local structure preservation.
CAI.

Thorsten Joachims. 2002. Optimizing search engines
In Proceedings ACM

using clickthrough data.
SIGKDD.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proceed-
ings of ICML.

Gregor Leban, Blaz Fortuna, Janez Brank, and Marko
Grobelnik. 2014. Event registry: learning about
world events from news. In Proceedings of WWW.

Kristina Lerman and Tad Hogg. 2010. Using a model
of social dynamics to predict popularity of news. In
Proceedings of WWW.

Renars Liepins, Ulrich Germann, Guntis Barzdins,
Alexandra Birch, Steve Renals, Susanne Weber,
Peggy van der Kreeft, Herve Bourlard, Jo˜ao Pri-
eto, Ondrej Klejch, Peter Bell, Alexandros Lazaridis,
Alfonso Mendes, Sebastian Riedel, Mariana S. C.
Almeida, Pedro Balage, Shay B. Cohen, Tomasz
Dwojak, Philip N. Garner, Andreas Giefer, Marcin
Junczys-Dowmunt, Hina Imran, David Nogueira,
Ahmed Ali, Sebasti˜ao Miranda, Andrei Popescu-
Belis, Lesly Miculicich Werlen, Nikos Papasaran-
topoulos, Abiola Obamuyide, Clive Jones, Fahim
Dalvi, Andreas Vlachos, Yang Wang, Sibo Tong,
Rico Sennrich, Nikolaos Pappas, Shashi Narayan,
Marco Damonte, Nadir Durrani, Sameer Khurana,
Ahmed Abdelali, Hassan Sajjad, Stephan Vogel,
David Sheppey, Chris Hernon, and Jeff Mitchell.
In Pro-
2017. The SUMMA platform prototype.
ceedings of the Software Demonstrations of EACL.

Andr´e F. T. Martins, Miguel B. Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
In Proceedings of
non-projective turbo parsers.
ACL.

Radim ˇReh˚uˇrek. 2010. Fast and faster: A comparison
of two streamed matrix decomposition algorithms.

Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora.
In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks.

Jan Rupnik, Andrej Muhic, Gregor Leban, Primoz
Skraba, Blaz Fortuna, and Marko Grobelnik. 2016.
News across languages-cross-lingual document sim-
ilarity and event tracking. Journal of Artiﬁcial Intel-
ligence Research, 55:283–316.

Motoki Sato, Austin J Brockmeier, Georgios Kontonat-
sios, Tingting Mu, John Y Goulermas, Jun’ichi Tsu-
jii, and Sophia Ananiadou. 2017. Distributed docu-
ment and phrase co-embeddings for descriptive clus-
tering. In Proceedings of EACL.

Josef Steinberger. 2016. MediaGist: A cross-lingual
analyser of aggregated news and commentaries. In
Proceedings of ACL.

Junyuan Xie, Ross Girshick, and Ali Farhadi. 2016.
Unsupervised deep embedding for clustering analy-
sis. In Proceedings of ICML.

Multilingual Clustering of Streaming News

Sebasti˜ao Miranda♦ Art ¯urs Znotin¸ ˇs†(cid:63)
Shay B. Cohen♥ Guntis Barzdins†(cid:63)
♦Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal
†Innovation Labs LETA, Marijas Str. 2, Riga LV-1050, Latvia
(cid:63)University of Latvia, IMCS, Rainis Blvd. 29, Riga LV-1459, Latvia
♥School of Informatics, University of Edinburgh, Edinburgh EH8 9AB
sebastiao.miranda@priberam.pt, arturs.znotins@leta.lv,
scohen@inf.ed.ac.uk, guntis.barzdins@lu.lv

Abstract

Clustering news across languages enables
efﬁcient media monitoring by aggregating ar-
ticles from multilingual sources into coherent
stories. Doing so in an online setting allows
scalable processing of massive news streams.
To this end, we describe a novel method for
clustering an incoming stream of multilingual
documents into monolingual and crosslingual
story clusters. Unlike
clustering
approaches that consider a small and known
number of labels, we tackle the problem of
discovering an ever growing number of cluster
labels in an online fashion, using real news
datasets in multiple languages. Our method
is
simple to implement, computationally
efﬁcient and produces state-of-the-art results
on datasets in German, English and Spanish.

typical

1

Introduction

Following developing news stories is imperative to
making real-time decisions on important political
and public safety matters. Given the abundance of
media providers and languages, this endeavor is an
extremely difﬁcult task. As such, there is a strong
demand for automatic clustering of news streams,
so that they can be organized into stories or themes
for further processing. Performing this task in an
online and efﬁcient manner is a challenging prob-
lem, not only for newswire, but also for scientiﬁc
articles, online reviews, forum posts, blogs, and
microblogs.

A key challenge in handling document streams
is that the story clusters must be generated on the
ﬂy in an online fashion: this requires handling doc-
uments one-by-one as they appear in the document
stream. In this paper, we provide a treatment to the
problem of online document clustering, i.e. the task
of clustering a stream of documents into themes.
For example, for news articles, we would want to
cluster them into related news stories.

To this end, we introduce a system which aggre-
gates news articles into ﬁne-grained story clusters
across different languages in a completely online
and scalable fashion from a continuous stream. Our
clustering approach is part of a larger media mon-
itoring project to solve the problem of monitor-
ing massive text and TV/Radio streams (speech-
to-text). In particular, media monitors write intelli-
gence reports about the most relevant events, and
being able to search, visualize and explore news
clusters assists in gathering more insight about
a particular story. Since relevant events may be
spawned from any part of the world (and from
many multilingual sources), it becomes imperative
to cluster news across different languages.

In terms of granularity, the type of story clusters
we are interested in are the group of articles which,
for example : (i) Narrate recent air-strikes in East-
ern Ghouta (Syria); (ii) Describe the recent launch
of Space X’s Falcon Heavy rocket.

Main Contributions While most existing news
clustering approaches assume a monolingual docu-
ment stream – a non-realistic scenario given the di-
versity of languages on the Web – we assume a gen-
eral, multilingual, document stream. This means
that in our problem-formulation story documents
appear in multiple languages and we need to cluster
them to crosslingual clusters. Our main contribu-
tions are as follows:

• We develop a system that aggregates news arti-
cles into ﬁne-grained story clusters across differ-
ent languages in a completely online and scalable
fashion from a continuous stream. As discussed
in the introduction, this is a highly relevant task
for the use-case of media monitoring.

• We formulate the problem of online multilingual
document clustering and the representation that
such clustering takes by interlacing the problem

8
1
0
2
 
p
e
S
 
3
 
 
]
L
C
.
s
c
[
 
 
1
v
0
4
5
0
0
.
9
0
8
1
:
v
i
X
r
a

of monolingual clustering with crosslingual clus-
tering. The representation of our clusters is inter-
pretable, and similarly to topic models, consists
of a set of keywords and weights associated with
the relevant cluster. In our formulation, a mono-
lingual cluster is a group of documents, and a
crosslingual cluster is a group of monolingual
clusters in different languages.

• We compare our approach to our own imple-
mentation of a state-of-the-art streaming method,
and show much superior results for a dataset in
English, Spanish and German.

2 Problem Formulation

We focus on clustering of a stream of documents,
where the number of clusters is not ﬁxed and
learned automatically. We denote by D a (poten-
tially inﬁnite) space of multilingual documents.
Each document d is associated with a language in
which it is written through a function L : D → L
where L is a set of languages. For example, L(d)
could return English, Spanish or German. (In the
rest of the paper, for an integer n, we denote by [n]
the set {1, . . . , n}.)

We are interested in associating each document
with a monolingual cluster via the function C(d) ∈
N, which returns the cluster label given a document.
This is done independently for each language, such
that the space of indices we use for each language
is separate.

Furthermore, we interlace the problem of mono-
lingual clustering with crosslingual clustering. This
means that as part of our problem formulation we
are also interested in a function E : N×L → N that
associates each monolingual cluster with a crosslin-
gual cluster, such that each crosslingual cluster only
groups one monolingual cluster per different lan-
guage, at a given time. The crosslingual cluster
for a document d is E(C(d), L(d)). As such, a
crosslingual cluster groups together monolingual
clusters, at most one for each different language.

Intuitively, building both monolingual and
crosslingual clusters allows the system to leverage
high-precision monolingual features (e.g., words,
named entities) to cluster documents of the same
language, while simplifying the task of crosslingual
clustering to the computation of similarity scores
across monolingual clusters - which is a smaller
problem space, since there are (by deﬁnition) less
clusters than articles. We validate this choice in §5.

3 The Clustering Algorithm

Each document d is represented by two vectors in
Rk1 and Rk2. The ﬁrst vector exists in a “mono-
lingual space” (of dimensionality k1) and is based
on a bag-of-words representation of the document.
The second vector exists in a “crosslingual space”
(of dimensionality k2) which is common to all lan-
guages. More details about these representations
are discussed in §4.

Online Clustering With our clustering algo-
rithm, we maintain two types of centroid functions
for each monolingual cluster. The ﬁrst is a cen-
troid function H : N × L → Rk1 ∪ {⊥} that as-
sists in associating each document with a mono-
lingual cluster. The second is a centroid function
G : N → Rk2 ∪ {⊥} that assists in associating each
monolingual cluster with a crosslingual cluster. The
⊥ symbol is reserved to denote documents which
are not associated with any cluster yet.

In our algorithm, we need to incrementally con-
struct the functions H, G (the two centroid func-
tions), C (the monolingual clustering function) and
E (the crosslingual clustering function). Informally,
we do so by ﬁrst identifying a monolingual cluster
for an incoming document by ﬁnding the closest
centroid with the function H, and then associate
that monolingual cluster with the crosslingual clus-
ter that is closest based on the function G. The ﬁrst
update changes C and the second update changes
E. Once we do that, we also update H and G to
reﬂect the new information that exists in the new
incoming document.

Example Figure 1 depicts the algorithm and the
state it maintains. A document in some language
(d9) appears in the stream, and is clustered into
one of the monolingual clusters (circles) that group
together documents about the same story (for ex-
ample, (cid:104)c2, DE(cid:105) could be a German cluster about a
recent political event). Then, following this mono-
lingual update, the online clustering algorithm up-
dates the crosslingual clusters (round rectangles),
each grouping together a set of monolingual clus-
ters, one per language at the most. The centroids
for the monolingual clusters are maintained by the
function H. For example, H(2, English) gives the
centroid of the upper left English monolingual clus-
ter. The function G maintains the crosslingual clus-
ters. Considering the upper-left most crosslingual
cluster, a1, then G(1) returns its centroid.

given crosslingual cluster. In our experiments, we
mostly use English as the pivot language.

H Update To update H, we maintain a centroid
for each cluster that is created as the average of
all monolingual representations of documents that
belong to that cluster. This is done for each lan-
guage separately. This update can be done in O(k1)
time in each step. Similarly, the update of G can be
done in O(k2) time. In principle, we consider an
“inﬁnite” stream of documents, which means the
number of documents in each cluster can be large.
As such, for efﬁciency purposes, updates to H are
immutable, which means that when a document is
assigned to a monolingual cluster, that assignment
is never changed.

G Update As described, updates to function G
result in associating a monolingual cluster with a
crosslingual cluster (and consequently, other mono-
lingual clusters). Therefore, errors committed in
updating G are of a higher magnitude than those
committed in H, since they involve groups of doc-
uments. We also note that the best crosslingual
cluster for a particular monolingual cluster might
not be found right at the beginning of the process.
We experiment with two types of updates to G.
One which is immutable, in which changes to G
are not reversed (and are described above), and
one in which we introduce a novel technique to
make a sequence of changes to G if necessary, as a
mechanism to self-correct past clustering decisions.
When a past decision is modiﬁed, it may result in a
chaining of consequent modiﬁcations (“toppling of
dominoes”) which need to be evaluated. We coin
this method “domino-toppling”.

The motivation behind this technique is the
change in news stories over time. The technique
allows the method to modify past crosslingual clus-
tering decisions and enables higher quality clus-
tering results. When a past decision is modiﬁed, it
may result in a chain of consequent modiﬁcations
which need to be evaluated.

Our method of “domino-toppling” works by
making (potentially sequences) of changes to pre-
vious clustering decisions for the crosslingual clus-
ters, at each step placing a residual monolingual
cluster in a crosslingual cluster that is most similar
to it. Figure 2 gives the pseudocode for domino
toppling.

This “domino-toppling” technique could have
in principle a quadratic complexity in the number

Figure 1: A pictorial description of the algorithm and
the state it maintains. The algorithm maintains a mono-
lingual cluster space, in which each cluster is a set
of documents in a speciﬁc language. The algorithm
also maintains a crosslingual cluster space, in which
a cluster is a set of monolingual clusters in different
languages. Documents are denoted by di, monolingual
clusters by ci (circles) and crosslingual clusters by ai.

Algorithm To be more precise, the online clus-
tering process works as follows. H and G start with
just returning ⊥ for any cluster number, both mono-
lingual and crosslingual. With a new incoming doc-
ument d, represented as a vector, we compute a
similarity metric Γ0 : Rk1 × Rk1 → R between the
document vector and each of the existing centroids
{i | H(i, L(d)) (cid:54)= ⊥}. If the largest similarity ex-
ceeds a threshold τ for cluster index j, then we set
C(d) = j. In that case, we also update the value of
H(i, L(d)) to include new information from doc-
ument d, as detailed below under “H update.” If
none of the similarity values exceed a threshold
τ , we ﬁnd the ﬁrst i such that H(i, L(d)) = ⊥
(the ﬁrst cluster id which is still unassigned), and
set C(d) = i, therefore creating a new cluster. We
again follow an “H update” – this time for starting
a new cluster.

In both cases, we also update the function G,
by selecting the best crosslingual cluster for the
recently updated (or created) monolingual clus-
ter. To this end, we use another similarity metric
Γ1 : Rk2 × Rk2 → R. Accordingly, we compute
the similarity (using Γ1) between the updated (or
created) monolingual cluster and all monolingual
clusters in each candidate crosslingual cluster, in
the crosslingual feature space. The crosslingual
cluster with highest sum of similarity scores is then
selected. We also experimented computing this sim-
ilarity by considering just the monolingual cluster
of a particular “pivot language”. The pivot language
is a language that serves as the main indicator for a

Inputs: A monolingual cluster c and a list of pairs
(cid:104)aj, Γ1(c, aj)(cid:105), j ∈ [N ].

Algorithm:

• For all pairs (cid:104)aj, Γ1(c, aj)(cid:105), j ∈ [N ], ordered by the

second coordinate:

• If L(c) is not in aj, add c to aj and break.
• Otherwise, let y ← M (aj, L(c)). If Γ1(c, aj) >

Γ1(y, aj) then:

• Add c to aj, remove y from aj and call
domino toppling with y playing the role of c
and break.

• If c is left unassigned, create aN +1 and add c to it.

Figure 2: Crosslingual “domino-toppling”. aj is the jth
crosslingual cluster (out of total N clusters) and Γ1 is
the similarity between them as in §4. L(c) is the lan-
guage for cluster c. M (a, (cid:96)) returns the monolingual
cluster for language (cid:96) ∈ L in crosslingual cluster a.
See text for details.

of crosslingual clusters. However, we have veri-
ﬁed that in practice it converges very fast, and in
our evaluation dataset only 1% of the crosslingual
updates result in topples. We apply this technique
only to update G (and not H) because reversing
cluster assignments in G can be done much more
efﬁciently than in H – the total number of mono-
lingual clusters (the clustered elements in G) is
signiﬁcantly smaller than the number of documents
(the clustered elements in H). Crosslingual cluster-
ing is also a harder problem, which motivated the
additional effort of developing this algorithm.

4 Document Representation

In this section, we give more details about the way
we construct the document representations in the
monolingual and crosslingual spaces. In particu-
lar, we introduce the deﬁnition of the similarity
functions Γ0 and Γ1 that were referred in §3.

Monolingual Representation The monolingual
representation for each document d in language
L(d) is a vector in Rk1 constructed from several
TF-IDF subvectors with words, word lemmas and
named entities. Each subvector is repeated for dif-
ferent sections of the document, the title, the body
and both of them together. Besides these text ﬁelds
and document timestamps, no other metadata was
used. To detect named entities, we used Priberam’s
Text Analysis (Amaral et al., 2008) for English and
Spanish, and Turbo Parser (Martins et al., 2013) for
German. The extracted entities consist of people,
organizations, places and other types.

Crosslingual Representation In the crosslin-
gual space, a document representation is a vec-
tor in Rk2. Let e(d, i) be a crosslingual embed-
ding of word i in the document d, which is a vec-
tor of length m. Then the document representa-
tion v(d) of d consists of subvectors of the form
v(d) = (cid:80)n
i=1 tie(d, i), where ti is the TF-IDF
score of the ith word in the relevant section of the
document (title, body or both). As detailed further
in §5 we compute IDF values from a large pre-
training dataset. Furthermore, for both the mono-
lingual and crosslingual cases, we also experiment
with using document timestamp features, as ex-
plained in §4.1. We use a new set of diverse times-
tamp features in addition to the simple absolute
difference (in hours) between timestamps used by
Rupnik et al. (2016).

4.1 Similarity Metrics

Our similarity metric computes weighted cosine
similarity on the different subvectors, both in the
case of monolingual clustering and crosslingual
clustering. Formally, for the monolingual case, the
similarity is given by a function deﬁned as:

3
(cid:88)

i=1

K
(cid:88)

i=1

Γ0(dj, cl) =

φi(dj, cl) · q0

i +

γi(dj, cl) · q1
i .

(1)
and is computed on the TF-IDF subvectors where
K is the number of subvectors for the relevant
document representation. For the crosslingual case,
we discuss below the function Γ1, which has a
similar structure.

Here, dj is the jth document in the stream and
cl is a monolingual cluster. The function φi(dj, cl)
returns the cosine similarity between the document
representation of the jth document and the centroid
for cluster cl. The vector q0 denotes the weights
through which each of the cosine similarity values
for each subvectors are weighted, whereas q1 de-
notes the weights for the timestamp features, as
detailed further. Details on learning the weights q0
and q1 are discussed in §4.2.

The function γ(d, c) that maps a pair of docu-

ment and cluster to R3 is deﬁned as follows. Let

f (t) = exp

−

(cid:18)

(cid:19)

(t − µ)2
2σ2

(2)

for a given µ and σ > 0. For each document
d and cluster c, we generate the following three-
dimensional vector γ(d, c) = (s1, s2, s3):

• s1 = f (t(d) − n1(c)) where t(dj) is the times-
tamp for document d and n1(c) is the timestamp
for the newest document in cluster c.

• s2 = f (t(d)−n2(c)) where n2(c) is the average

timestamp for all documents in cluster c.

• s3 = f (t(d) − n3(c)) where n3(c) is the times-

tamp for the oldest document in cluster c.

These three timestamp1 features model the time
aspect of the online stream of news data and help
disambiguate clustering decisions, since time is a
valuable indicator that a news story has changed,
even if a cluster representation has a reasonable
match in the textual features with the incoming
document. The same way a news story becomes
popular and fades over time (Lerman and Hogg,
2010), we model the probability of a document
belonging to a cluster (in terms of timestamp dif-
ference) with a probability distribution.

For the case of crosslingual clustering, we in-
troduce Γ1, which has a similar deﬁnition to Γ0,
only instead of passing document/cluster similarity
feature vectors, we pass cluster/cluster similarities,
across all language pairs. Furthermore, the features
are the crosslingual embedding vectors of the sec-
tions title, body and both combined (similarly to the
monolingual case) and the timestamp features. For
denoting the cluster timestamp, we use the average
timestamps of all articles in it.

i = 1 ∀i and q1

4.2 Learning to Rank Candidates
In §4.1 we introduced q0 and q1 as the weight
vectors for the several document representation
features. We experiment with both setting these
weights to just 1 (q0
j = 1 ∀j ∈ [3])
and also learning these weights using support vec-
tor machines (SVMs). To generate the SVM train-
ing data, we simulate the execution of the algorithm
on a training data partition (which we do not get
evaluated on) and in which the gold standard labels
are given. We run the algorithm using only the ﬁrst
subvector φ1(dj, cl), which is the TF-IDF vector
with the words of the document in the body and
title. For each incoming document, we create a col-
lection of positive examples, for the document and
the clusters which share at least one document in
the gold labeling. We then generate 20 negative ex-
amples for the document from the 20 best-matching
clusters which are not correct. To ﬁnd out the best-
matching clusters, we rank them according to their

1Timestamps are given in hours since 1970.

similarity to the input document using only the ﬁrst
subvector φ1(dj, cl).

Using this scheme we generate a collection
of ranking examples (one for each document in
the dataset, with the ranking of the best cluster
matches), which are then trained using the SVM-
Rank algorithm (Joachims, 2002). We run 5-fold
cross-validation on this data to select the best
model, and train both a separate model for each
language according to Γ0 and a crosslingual model
according to Γ1.

5 Experiments

Our system was designed to cluster documents
from a (potentially inﬁnite) real-word data stream.
The datasets typically used in the literature (TDT,
Reuters) have a small number of clusters (≈ 20)
with coarse topics (economy, society, etc.), and
therefore are not relevant to the use case of me-
dia monitoring we treat - as it requires much more
ﬁne-grained story clusters about particular events.
To evaluate our approach, we adapted a dataset
constructed for the different purpose of binary clas-
siﬁcation of joining cluster pairs.2 We processed it
to become a collection of articles annotated with
monolingual and crosslingual cluster labels.3

Statistics about this dataset are given in Ta-
ble 1. As described further, we tune the hyper-
parameter τ on the development set. As for the
hyper-parameters related to the timestamp features,
we ﬁxed µ = 0 and tuned σ on the development set,
yielding σ = 72 hours (3 days).4 To compute IDF
scores (which are global numbers computed across
a corpus), we used a different and much larger
dataset that we collected from Deutsche Welle’s
news website (http://www.dw.com/). The
dataset consists of 77,268, 118,045 and 134,243
documents for Spanish, English and German, re-
spectively.

The conclusions from our experiments are: (a)
the weighting of the similarity metric features using
SVM signiﬁcantly outperforms unsupervised base-
lines such as CluStream (Table 2); (b) the SVM
approach signiﬁcantly helps to learn when to cre-
ate a new cluster, compared to simple grid search

2https://github.com/rupnikj/jair_paper
3The code and data we used is available at https://

github.com/priberam/news-clustering.

4This shows a relative robustness to reordering of the arti-
cles – articles within 3 days of each other could appear any-
where in that window, and the algorithm would still perform
well.

i
a
r
t

Dataset
n English
German
Spanish
t English
German
Spanish

s
e
t

Size
12,233
4,043
4,527
8,726
2,101
2,177

Avg. L.
434
282
355
521
440
392

C
593
377
416
222
118
149

Avg. S.
21
11
11
39
18
15

Table 1: Statistics for the development and evaluation
datasets, constructed from the dataset in Rupnik et al.
(2016), as explained in §5. “Size” denotes the number
of documents in the collection, “Avg. L.” is the aver-
age number of words in a document, “C” denotes the
number of clusters in the collection and “Avg. S.” is the
average number of documents in each cluster.

for the optimal τ (Table 4); (c) separating the fea-
ture space into one for monolingual clusters in the
form of keywords and the other for crosslingual
clusters based on crosslingual embeddings signiﬁ-
cantly helps performance.

Evaluation Method We evaluate clustering in
the following manner: let tp be the number of cor-
rectly clustered-together document pairs, let fp be
the number of incorrectly clustered-together docu-
ment pairs and let fn be the number of incorrectly
not-clustered-together document pairs. Then we
report precision as
tp+fn and F1
as the harmonic mean of the precision and recall
measures.

tp+fp , recall as

tp

tp

We do the same to evaluate crosslingual cluster-
ing, but on a higher level: we count tp, fn and fp for
the decisions of clustering clusters, as crosslingual
clusters are groups of monolingual gold clusters.

5.1 Monolingual Results

In our ﬁrst set of experiments, we report results
on monolingual clustering for each language sepa-
rately. Monolingual clustering of a stream of doc-
uments is an important problem that has been in-
spected by others, such as by Ahmed et al. (2011)
and by Aggarwal and Yu (2006). We compare our
results to our own implementation of the online
micro-clustering routine presented by Aggarwal
and Yu (2006), which shall be referred to as CluS-
tream. We note that CluStream of Aggarwal and
Yu (2006) has been a widely used state-of-the-art
system in media monitoring companies as well as
academia, and serves as a strong baseline to this
day.

In our preliminary experiments, we also evalu-
ated an online latent semantic analysis method, in
which the centroids we keep for the function H (see

TOKENS+LEMMAS+ENTS

s
i
l
g
n
E

algorithm
h CluStream

+TS
n CluStream
a
m
r
e
G

+TS
h CluStream
s
i
n
a
p
S

+TS

TOKENS+LEMMAS+ENTS

TOKENS+LEMMAS+ENTS

F1
79.0
92.7
94.1
89.7
90.7
97.1
78.1
88.8
94.2

P
98.6
92.9
98.2
99.9
99.7
99.9
73.4
95.9
97.0

R
65.9
92.5
90.3
81.3
83.2
94.5
83.5
82.7
91.6

Table 2: Clustering results on the labeled dataset. We
compare our algorithm (with and without timestamps)
with the online micro-clustering routine of Aggarwal
and Yu (2006) (denoted by CluStream). The F1 values
are for the precision (P) and recall (R) in the follow-
ing columns. See Table 3 for a legend of the different
models. Best result for each language is in bold.

§3) are the average of reduced dimensional vectors
of the incoming documents as generated by an in-
cremental singular value decomposition (SVD) of
a document-term matrix that is updated after each
incoming document. However, we discovered that
online LSA performs signiﬁcantly worse than rep-
resenting the documents the way is described in §4.
Furthermore, it was also signiﬁcantly slower than
our algorithm due to the time it took to perform
singular value decomposition.5

Clustering experiments Table 2 gives the ﬁnal
monolingual results on the three datasets. For En-
glish, we see that the signiﬁcant improvement we
get using our algorithm over the algorithm of Ag-
garwal and Yu (2006) is due to an increased recall
score. We also note that the trained models surpass
the baseline for all languages, and that the times-
tamp feature (denoted by TS), while not required
to beat the baseline, has a very relevant contribu-
tion in all cases. Although the results for both the
baseline and our models seem to differ across lan-
guages, one can verify a consistent improvement
from the latter to the former, suggesting that the
score differences should be mostly tied to the differ-
ent difﬁculty found across the datasets for each lan-
guage. The presented scores show that our learning
framework generalizes well to different languages
and enables high quality clustering results.

To investigate the impact of the timestamp fea-

5More speciﬁcally, we used an object of type lsimodel
from the GenSim package that implements algorithms from
ˇReh˚uˇrek (2010). The GenSim package can be found at
https://pypi.python.org/pypi/gensim.

feature
TOKENS
TOKENS+LEMMAS
TOKENS+LEMMAS+ENTS
TOKENS+LEMMAS+ENTS+TS

accuracy
85.5
85.9
86.5
96.9

Table 3: Accuracy of the SVM ranker on the English
training set. TOKENS are the word token features, LEM-
MAS are the lemma features for title and body, ENTS
are named entity features and TS are timestamp fea-
tures. All features are described in detail in §4, and are
listed for both the title and the body.

tures, we ran an additional experiment using only
the same three timestamp features as used in the
best model on the English dataset. This experi-
ment yielded scores of F1 = 61.1, P = 44.5 and
R = 97.6, which lead us to conclude that while
these features are not competitive when used alone
(hence temporal information by itself is not sufﬁ-
cient to predict the clusters), they contribute signif-
icantly to recall with the ﬁnal feature ensemble.

We note that as described in §3, the optimiza-
tion of the τ parameter is part of the development
process. The parameter τ is a similarity thresh-
old used to decide when an incoming document
should merge to the best cluster or create a new
one. We tune τ on the development set for each
language, and the sensitivity to it is demonstrated
in Figure 3 (this process is further referred to as
τsearch). Although applying grid-search on this pa-
rameter is the most immediate approach to this
problem, we experimented with a different method
which yielded superior results: as described further,
we discuss how to do this process with an additional
classiﬁer (denoted SVM-merge), which captures
more information about the incoming documents
and the existing clusters.

Additionally, we also experimented with comput-
ing the monolingual clusters with the same embed-
dings as used in the crosslingual clustering phase,
which yielded poor results. In particular, this sys-
tem achieved F1 score of 74.8 for English, which
is below the bag-of-words baseline presented in
Table 2. This result supports the approach we then
followed of having two separate feature spaces for
the monolingual and crosslingual clustering sys-
tems, where the monolingual space is discrete and
the crosslingual space is based on embeddings.

SVM ranker experiments To investigate the im-
portance of each feature, we now consider in Ta-

Figure 3: The F1 score of the different language de-
velopment sets as a function of the threshold τ . The
ﬁrst point for each language is identiﬁed using binary
search.

model
τsearch
SVM-merge

F1
82.8
94.1

P
96.5
98.2

R
72.4
90.3

Table 4: Comparison of two different cluster decision
techniques for the English SVM model with all fea-
tures (see Table 2). The ﬁrst method, τsearch, corre-
sponds to executing grid-search to ﬁnd the optimal clus-
tering τ parameter (see §3). SVM-merge is an alterna-
tive method in which we train an SVM binary classi-
ﬁer to decide if a new cluster should be created or not,
where we use as features the maximal value of each
coordinate for each document in a cluster.

ble 3 the accuracy of the SVM ranker for English
as described in §4.1. We note that adding features
increases the accuracy of the SVM ranker, espe-
cially the timestamp features. However, the times-
tamp feature actually interferes with our optimiza-
tion of τ to identify when new clusters are needed,
although they improve the SVM reranking accu-
racy. We speculate this is true because high accu-
racy in the reranking problem does not necessarily
help with identifying when new clusters need to be
opened. To investigate this issue, we experimented
with a different technique to learn when to create a
new cluster. To this end, we trained another SVM
classiﬁer just to learn this decision, this time a bi-
nary classiﬁer using LIBLINEAR (Fan et al., 2008),
by passing the max of the similarity of each feature
between the incoming document and the current
clustering pool as the input feature vector. This
way, the classiﬁer learns when the current clusters,

crosslingual model
τsearch (global)
τsearch (pivot)

F1
72.7
84.0

P
89.8
83.0

R
61.0
85.0

6 Related Work

Table 5: Crosslingual clustering results when consid-
ering two different approaches to compute distances
across crosslingual clusters on the test set for Spanish,
German and English. See text for details.

as a whole, are of a different news story than the
incoming document. As presented in Table 4, this
method, which we refer to as SVM-merge, solved
the issue of searching for the optimal τ parame-
ter for the SVM-rank model with timestamps, by
greatly improving the F1 score in respect to the
original grid-search approach (τsearch).

5.2 Crosslingual Results

As mentioned in §3, crosslingual embeddings are
used for crosslingual clustering. We experimented
with the crosslingual embeddings of Gardner et al.
(2015) and Ammar et al. (2016). In our preliminary
experiments we found that the former worked better
for our use-case than the latter.

We test two different scenarios for optimizing
the similarity threshold τ for the crosslingual case.
Table 5 shows the results for these experiments.
First, we consider the simpler case of adjusting a
global τ parameter for the crosslingual distances, as
also described for the monolingual case. As shown,
this method works poorly, since the τ grid-search
could not ﬁnd a reasonable τ which worked well
for every possible language pair.

Subsequently, we also consider the case of using
English as a pivot language (see §3), where dis-
tances for every other language are only compared
to English, and crosslingual clustering decisions
are made only based on this distance.6 This yielded
our best crosslingual score of F1=84.0, conﬁrm-
ing that crosslingual similarity is of higher quality
between each language and English, for the em-
beddings we used. This score represents only a
small degradation in respect to the monolingual
results, since clustering across different languages
is a harder problem.

6In this case, all crosslingual clusters will have at least one
pivot monolingual cluster, except for clusters which might
stay unmerged as single-language degenerated crosslingual
clusters.

Early research efforts, such as the TDT pro-
gram (Allan et al., 1998), have studied news clus-
tering for some time. The problem of online mono-
lingual clustering algorithms (for English) has also
received a fair amount of attention in the litera-
ture. One of the earlier papers by Aggarwal and
Yu (2006) introduced a two-step clustering system
with both ofﬂine and online components, where the
online model is based on a streaming implemen-
tation of k-means and a bag-of-words document
representation. Other authors have experimented
with distributed representations, such as Ahmed
et al. (2011), who cluster news into storylines us-
ing Markov chain Monte Carlo methods, ˇReh˚uˇrek
and Sojka (2010) who used incremental Singular
Value Decomposition (SVD) to ﬁnd relevant topics
from streaming data, and Sato et al. (2017) who
used the paragraph vector model (Le and Mikolov,
2014) in an ofﬂine clustering setting.

More recently, crosslingual linking of clusters
has been discussed by Rupnik et al. (2016) in the
context of linking existing clusters from the Event
Registry (Leban et al., 2014) in a batch fashion,
and by Steinberger (2016) who also present a batch
clustering linking system. However, these are not
“truly” online crosslingual clustering systems since
they only decide on the linking of already-built
monolingual clusters. In particular, Rupnik et al.
(2016) compute distances of document pairs across
clusters using nearest neighbors, which might not
scale well in an online setting. As detailed before,
we adapted the cluster-linking dataset from Rup-
nik et al. (2016) to evaluate our online crosslingual
clustering approach. Preliminary work makes use
of deep learning techniques (Xie et al., 2016; Guo
et al., 2017) to cluster documents while learning
their representations, but not in an online or mul-
tilingual fashion, and with a very small number of
cluster labels (4, in the case of the text benchmark).

In our work, we studied the problem of mono-
lingual and crosslingual clustering, having exper-
imented several directions and methods and the
impact they have on the ﬁnal clustering quality. We
described the ﬁrst system which aggregates news
articles into ﬁne-grained story clusters across differ-
ent languages in a completely online and scalable
fashion from a continuous stream.

7 Conclusion

We described a method for monolingual and
crosslingual clustering of an incoming stream of
documents. The method works by maintaining cen-
troids for the monolingual and crosslingual clus-
ters, where a monolingual cluster groups a set of
documents and a crosslingual cluster groups a set
of monolingual clusters. We presented an online
crosslingual clustering method which auto-corrects
past decisions in an efﬁcient way. We showed that
our method gives state-of-the-art results on a mul-
tilingual news article dataset for English, Spanish
and German. Finally, we discussed how to leverage
different SVM training procedures for ranking and
classiﬁcation to improve monolingual and crosslin-
gual clustering decisions. Our system is integrated
in a larger media monitoring project (Liepins et al.,
2017; Germann et al., 2018) and solving the use-
cases of monitors and journalists, having been vali-
dated with qualitative user testing.

Acknowledgments

We would like to thank Esma Balkır, Nikos Pa-
pasarantopoulos, Afonso Mendes, Shashi Narayan
and the anonymous reviewers for their feedback.
This project was supported by the European H2020
project SUMMA, grant agreement 688139 (see
http://www.summa-project.eu) and by a
grant from Bloomberg.

References

Charu C. Aggarwal and Philip S. Yu. 2006. A frame-
work for clustering massive text and categorical data
streams. In SDM, pages 479–483. SIAM.

Amr Ahmed, Qirong Ho, Jacob Eisenstein, Eric Xing,
Alexander J Smola, and Choon Hui Teo. 2011. Uni-
In Proceedings
ﬁed analysis of streaming news.
of the 20th international conference on World wide
web, pages 267–276. ACM.

James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, Yiming Yang, et al. 1998. Topic
detection and tracking pilot study: Final report. In
Proceedings of the DARPA broadcast news tran-
scription and understanding workshop.

Carlos Amaral, Ad´an Cassan, Helena Figueira, Andr´e
Martins, Afonso Mendes, Pedro Mendes, Jos´e Pina,
and Cl´audia Pinto. 2008. Priberam’s question an-
In Proceed-
swering system in QA@ CLEF 2008.
ings of the Workshop of the Cross-Language Evalu-
ation Forum for European Languages.

Waleed Ammar, George Mulcaire, Yulia Tsvetkov,
Guillaume Lample, Chris Dyer, and Noah A Smith.
2016. Massively multilingual word embeddings.
arXiv preprint arXiv:1602.01925.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classiﬁcation. Journal of ma-
chine learning research, 9(Aug):1871–1874.

Matt Gardner, Kejun Huang, Evangelos Papalex-
akis, Xiao Fu, Partha Talukdar, Christos Falout-
sos, Nicholas Sidiropoulos, and Tom Mitchell. 2015.
Translation invariant word embeddings. In Proceed-
ings of EMNLP.

Ulrich Germann, Renars Liepins, Guntis Barzdins,
Didzis Gosko, Sebasti˜ao Miranda, and David
Nogueira. 2018. The summa platform: A scalable
infrastructure for multi-lingual multi-media monitor-
ing. Proceedings of ACL 2018, System Demonstra-
tions.

Xifeng Guo, Long Gao, Xinwang Liu, and Jianping
Yin. 2017. Improved deep embedded clustering with
In Proceedings of IJ-
local structure preservation.
CAI.

Thorsten Joachims. 2002. Optimizing search engines
In Proceedings ACM

using clickthrough data.
SIGKDD.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proceed-
ings of ICML.

Gregor Leban, Blaz Fortuna, Janez Brank, and Marko
Grobelnik. 2014. Event registry: learning about
world events from news. In Proceedings of WWW.

Kristina Lerman and Tad Hogg. 2010. Using a model
of social dynamics to predict popularity of news. In
Proceedings of WWW.

Renars Liepins, Ulrich Germann, Guntis Barzdins,
Alexandra Birch, Steve Renals, Susanne Weber,
Peggy van der Kreeft, Herve Bourlard, Jo˜ao Pri-
eto, Ondrej Klejch, Peter Bell, Alexandros Lazaridis,
Alfonso Mendes, Sebastian Riedel, Mariana S. C.
Almeida, Pedro Balage, Shay B. Cohen, Tomasz
Dwojak, Philip N. Garner, Andreas Giefer, Marcin
Junczys-Dowmunt, Hina Imran, David Nogueira,
Ahmed Ali, Sebasti˜ao Miranda, Andrei Popescu-
Belis, Lesly Miculicich Werlen, Nikos Papasaran-
topoulos, Abiola Obamuyide, Clive Jones, Fahim
Dalvi, Andreas Vlachos, Yang Wang, Sibo Tong,
Rico Sennrich, Nikolaos Pappas, Shashi Narayan,
Marco Damonte, Nadir Durrani, Sameer Khurana,
Ahmed Abdelali, Hassan Sajjad, Stephan Vogel,
David Sheppey, Chris Hernon, and Jeff Mitchell.
In Pro-
2017. The SUMMA platform prototype.
ceedings of the Software Demonstrations of EACL.

Andr´e F. T. Martins, Miguel B. Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
In Proceedings of
non-projective turbo parsers.
ACL.

Radim ˇReh˚uˇrek. 2010. Fast and faster: A comparison
of two streamed matrix decomposition algorithms.

Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora.
In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks.

Jan Rupnik, Andrej Muhic, Gregor Leban, Primoz
Skraba, Blaz Fortuna, and Marko Grobelnik. 2016.
News across languages-cross-lingual document sim-
ilarity and event tracking. Journal of Artiﬁcial Intel-
ligence Research, 55:283–316.

Motoki Sato, Austin J Brockmeier, Georgios Kontonat-
sios, Tingting Mu, John Y Goulermas, Jun’ichi Tsu-
jii, and Sophia Ananiadou. 2017. Distributed docu-
ment and phrase co-embeddings for descriptive clus-
tering. In Proceedings of EACL.

Josef Steinberger. 2016. MediaGist: A cross-lingual
analyser of aggregated news and commentaries. In
Proceedings of ACL.

Junyuan Xie, Ross Girshick, and Ali Farhadi. 2016.
Unsupervised deep embedding for clustering analy-
sis. In Proceedings of ICML.

Multilingual Clustering of Streaming News

Sebasti˜ao Miranda♦ Art ¯urs Znotin¸ ˇs†(cid:63)
Shay B. Cohen♥ Guntis Barzdins†(cid:63)
♦Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal
†Innovation Labs LETA, Marijas Str. 2, Riga LV-1050, Latvia
(cid:63)University of Latvia, IMCS, Rainis Blvd. 29, Riga LV-1459, Latvia
♥School of Informatics, University of Edinburgh, Edinburgh EH8 9AB
sebastiao.miranda@priberam.pt, arturs.znotins@leta.lv,
scohen@inf.ed.ac.uk, guntis.barzdins@lu.lv

Abstract

Clustering news across languages enables
efﬁcient media monitoring by aggregating ar-
ticles from multilingual sources into coherent
stories. Doing so in an online setting allows
scalable processing of massive news streams.
To this end, we describe a novel method for
clustering an incoming stream of multilingual
documents into monolingual and crosslingual
story clusters. Unlike
clustering
approaches that consider a small and known
number of labels, we tackle the problem of
discovering an ever growing number of cluster
labels in an online fashion, using real news
datasets in multiple languages. Our method
is
simple to implement, computationally
efﬁcient and produces state-of-the-art results
on datasets in German, English and Spanish.

typical

1

Introduction

Following developing news stories is imperative to
making real-time decisions on important political
and public safety matters. Given the abundance of
media providers and languages, this endeavor is an
extremely difﬁcult task. As such, there is a strong
demand for automatic clustering of news streams,
so that they can be organized into stories or themes
for further processing. Performing this task in an
online and efﬁcient manner is a challenging prob-
lem, not only for newswire, but also for scientiﬁc
articles, online reviews, forum posts, blogs, and
microblogs.

A key challenge in handling document streams
is that the story clusters must be generated on the
ﬂy in an online fashion: this requires handling doc-
uments one-by-one as they appear in the document
stream. In this paper, we provide a treatment to the
problem of online document clustering, i.e. the task
of clustering a stream of documents into themes.
For example, for news articles, we would want to
cluster them into related news stories.

To this end, we introduce a system which aggre-
gates news articles into ﬁne-grained story clusters
across different languages in a completely online
and scalable fashion from a continuous stream. Our
clustering approach is part of a larger media mon-
itoring project to solve the problem of monitor-
ing massive text and TV/Radio streams (speech-
to-text). In particular, media monitors write intelli-
gence reports about the most relevant events, and
being able to search, visualize and explore news
clusters assists in gathering more insight about
a particular story. Since relevant events may be
spawned from any part of the world (and from
many multilingual sources), it becomes imperative
to cluster news across different languages.

In terms of granularity, the type of story clusters
we are interested in are the group of articles which,
for example : (i) Narrate recent air-strikes in East-
ern Ghouta (Syria); (ii) Describe the recent launch
of Space X’s Falcon Heavy rocket.

Main Contributions While most existing news
clustering approaches assume a monolingual docu-
ment stream – a non-realistic scenario given the di-
versity of languages on the Web – we assume a gen-
eral, multilingual, document stream. This means
that in our problem-formulation story documents
appear in multiple languages and we need to cluster
them to crosslingual clusters. Our main contribu-
tions are as follows:

• We develop a system that aggregates news arti-
cles into ﬁne-grained story clusters across differ-
ent languages in a completely online and scalable
fashion from a continuous stream. As discussed
in the introduction, this is a highly relevant task
for the use-case of media monitoring.

• We formulate the problem of online multilingual
document clustering and the representation that
such clustering takes by interlacing the problem

8
1
0
2
 
p
e
S
 
3
 
 
]
L
C
.
s
c
[
 
 
1
v
0
4
5
0
0
.
9
0
8
1
:
v
i
X
r
a

of monolingual clustering with crosslingual clus-
tering. The representation of our clusters is inter-
pretable, and similarly to topic models, consists
of a set of keywords and weights associated with
the relevant cluster. In our formulation, a mono-
lingual cluster is a group of documents, and a
crosslingual cluster is a group of monolingual
clusters in different languages.

• We compare our approach to our own imple-
mentation of a state-of-the-art streaming method,
and show much superior results for a dataset in
English, Spanish and German.

2 Problem Formulation

We focus on clustering of a stream of documents,
where the number of clusters is not ﬁxed and
learned automatically. We denote by D a (poten-
tially inﬁnite) space of multilingual documents.
Each document d is associated with a language in
which it is written through a function L : D → L
where L is a set of languages. For example, L(d)
could return English, Spanish or German. (In the
rest of the paper, for an integer n, we denote by [n]
the set {1, . . . , n}.)

We are interested in associating each document
with a monolingual cluster via the function C(d) ∈
N, which returns the cluster label given a document.
This is done independently for each language, such
that the space of indices we use for each language
is separate.

Furthermore, we interlace the problem of mono-
lingual clustering with crosslingual clustering. This
means that as part of our problem formulation we
are also interested in a function E : N×L → N that
associates each monolingual cluster with a crosslin-
gual cluster, such that each crosslingual cluster only
groups one monolingual cluster per different lan-
guage, at a given time. The crosslingual cluster
for a document d is E(C(d), L(d)). As such, a
crosslingual cluster groups together monolingual
clusters, at most one for each different language.

Intuitively, building both monolingual and
crosslingual clusters allows the system to leverage
high-precision monolingual features (e.g., words,
named entities) to cluster documents of the same
language, while simplifying the task of crosslingual
clustering to the computation of similarity scores
across monolingual clusters - which is a smaller
problem space, since there are (by deﬁnition) less
clusters than articles. We validate this choice in §5.

3 The Clustering Algorithm

Each document d is represented by two vectors in
Rk1 and Rk2. The ﬁrst vector exists in a “mono-
lingual space” (of dimensionality k1) and is based
on a bag-of-words representation of the document.
The second vector exists in a “crosslingual space”
(of dimensionality k2) which is common to all lan-
guages. More details about these representations
are discussed in §4.

Online Clustering With our clustering algo-
rithm, we maintain two types of centroid functions
for each monolingual cluster. The ﬁrst is a cen-
troid function H : N × L → Rk1 ∪ {⊥} that as-
sists in associating each document with a mono-
lingual cluster. The second is a centroid function
G : N → Rk2 ∪ {⊥} that assists in associating each
monolingual cluster with a crosslingual cluster. The
⊥ symbol is reserved to denote documents which
are not associated with any cluster yet.

In our algorithm, we need to incrementally con-
struct the functions H, G (the two centroid func-
tions), C (the monolingual clustering function) and
E (the crosslingual clustering function). Informally,
we do so by ﬁrst identifying a monolingual cluster
for an incoming document by ﬁnding the closest
centroid with the function H, and then associate
that monolingual cluster with the crosslingual clus-
ter that is closest based on the function G. The ﬁrst
update changes C and the second update changes
E. Once we do that, we also update H and G to
reﬂect the new information that exists in the new
incoming document.

Example Figure 1 depicts the algorithm and the
state it maintains. A document in some language
(d9) appears in the stream, and is clustered into
one of the monolingual clusters (circles) that group
together documents about the same story (for ex-
ample, (cid:104)c2, DE(cid:105) could be a German cluster about a
recent political event). Then, following this mono-
lingual update, the online clustering algorithm up-
dates the crosslingual clusters (round rectangles),
each grouping together a set of monolingual clus-
ters, one per language at the most. The centroids
for the monolingual clusters are maintained by the
function H. For example, H(2, English) gives the
centroid of the upper left English monolingual clus-
ter. The function G maintains the crosslingual clus-
ters. Considering the upper-left most crosslingual
cluster, a1, then G(1) returns its centroid.

given crosslingual cluster. In our experiments, we
mostly use English as the pivot language.

H Update To update H, we maintain a centroid
for each cluster that is created as the average of
all monolingual representations of documents that
belong to that cluster. This is done for each lan-
guage separately. This update can be done in O(k1)
time in each step. Similarly, the update of G can be
done in O(k2) time. In principle, we consider an
“inﬁnite” stream of documents, which means the
number of documents in each cluster can be large.
As such, for efﬁciency purposes, updates to H are
immutable, which means that when a document is
assigned to a monolingual cluster, that assignment
is never changed.

G Update As described, updates to function G
result in associating a monolingual cluster with a
crosslingual cluster (and consequently, other mono-
lingual clusters). Therefore, errors committed in
updating G are of a higher magnitude than those
committed in H, since they involve groups of doc-
uments. We also note that the best crosslingual
cluster for a particular monolingual cluster might
not be found right at the beginning of the process.
We experiment with two types of updates to G.
One which is immutable, in which changes to G
are not reversed (and are described above), and
one in which we introduce a novel technique to
make a sequence of changes to G if necessary, as a
mechanism to self-correct past clustering decisions.
When a past decision is modiﬁed, it may result in a
chaining of consequent modiﬁcations (“toppling of
dominoes”) which need to be evaluated. We coin
this method “domino-toppling”.

The motivation behind this technique is the
change in news stories over time. The technique
allows the method to modify past crosslingual clus-
tering decisions and enables higher quality clus-
tering results. When a past decision is modiﬁed, it
may result in a chain of consequent modiﬁcations
which need to be evaluated.

Our method of “domino-toppling” works by
making (potentially sequences) of changes to pre-
vious clustering decisions for the crosslingual clus-
ters, at each step placing a residual monolingual
cluster in a crosslingual cluster that is most similar
to it. Figure 2 gives the pseudocode for domino
toppling.

This “domino-toppling” technique could have
in principle a quadratic complexity in the number

Figure 1: A pictorial description of the algorithm and
the state it maintains. The algorithm maintains a mono-
lingual cluster space, in which each cluster is a set
of documents in a speciﬁc language. The algorithm
also maintains a crosslingual cluster space, in which
a cluster is a set of monolingual clusters in different
languages. Documents are denoted by di, monolingual
clusters by ci (circles) and crosslingual clusters by ai.

Algorithm To be more precise, the online clus-
tering process works as follows. H and G start with
just returning ⊥ for any cluster number, both mono-
lingual and crosslingual. With a new incoming doc-
ument d, represented as a vector, we compute a
similarity metric Γ0 : Rk1 × Rk1 → R between the
document vector and each of the existing centroids
{i | H(i, L(d)) (cid:54)= ⊥}. If the largest similarity ex-
ceeds a threshold τ for cluster index j, then we set
C(d) = j. In that case, we also update the value of
H(i, L(d)) to include new information from doc-
ument d, as detailed below under “H update.” If
none of the similarity values exceed a threshold
τ , we ﬁnd the ﬁrst i such that H(i, L(d)) = ⊥
(the ﬁrst cluster id which is still unassigned), and
set C(d) = i, therefore creating a new cluster. We
again follow an “H update” – this time for starting
a new cluster.

In both cases, we also update the function G,
by selecting the best crosslingual cluster for the
recently updated (or created) monolingual clus-
ter. To this end, we use another similarity metric
Γ1 : Rk2 × Rk2 → R. Accordingly, we compute
the similarity (using Γ1) between the updated (or
created) monolingual cluster and all monolingual
clusters in each candidate crosslingual cluster, in
the crosslingual feature space. The crosslingual
cluster with highest sum of similarity scores is then
selected. We also experimented computing this sim-
ilarity by considering just the monolingual cluster
of a particular “pivot language”. The pivot language
is a language that serves as the main indicator for a

Inputs: A monolingual cluster c and a list of pairs
(cid:104)aj, Γ1(c, aj)(cid:105), j ∈ [N ].

Algorithm:

• For all pairs (cid:104)aj, Γ1(c, aj)(cid:105), j ∈ [N ], ordered by the

second coordinate:

• If L(c) is not in aj, add c to aj and break.
• Otherwise, let y ← M (aj, L(c)). If Γ1(c, aj) >

Γ1(y, aj) then:

• Add c to aj, remove y from aj and call
domino toppling with y playing the role of c
and break.

• If c is left unassigned, create aN +1 and add c to it.

Figure 2: Crosslingual “domino-toppling”. aj is the jth
crosslingual cluster (out of total N clusters) and Γ1 is
the similarity between them as in §4. L(c) is the lan-
guage for cluster c. M (a, (cid:96)) returns the monolingual
cluster for language (cid:96) ∈ L in crosslingual cluster a.
See text for details.

of crosslingual clusters. However, we have veri-
ﬁed that in practice it converges very fast, and in
our evaluation dataset only 1% of the crosslingual
updates result in topples. We apply this technique
only to update G (and not H) because reversing
cluster assignments in G can be done much more
efﬁciently than in H – the total number of mono-
lingual clusters (the clustered elements in G) is
signiﬁcantly smaller than the number of documents
(the clustered elements in H). Crosslingual cluster-
ing is also a harder problem, which motivated the
additional effort of developing this algorithm.

4 Document Representation

In this section, we give more details about the way
we construct the document representations in the
monolingual and crosslingual spaces. In particu-
lar, we introduce the deﬁnition of the similarity
functions Γ0 and Γ1 that were referred in §3.

Monolingual Representation The monolingual
representation for each document d in language
L(d) is a vector in Rk1 constructed from several
TF-IDF subvectors with words, word lemmas and
named entities. Each subvector is repeated for dif-
ferent sections of the document, the title, the body
and both of them together. Besides these text ﬁelds
and document timestamps, no other metadata was
used. To detect named entities, we used Priberam’s
Text Analysis (Amaral et al., 2008) for English and
Spanish, and Turbo Parser (Martins et al., 2013) for
German. The extracted entities consist of people,
organizations, places and other types.

Crosslingual Representation In the crosslin-
gual space, a document representation is a vec-
tor in Rk2. Let e(d, i) be a crosslingual embed-
ding of word i in the document d, which is a vec-
tor of length m. Then the document representa-
tion v(d) of d consists of subvectors of the form
v(d) = (cid:80)n
i=1 tie(d, i), where ti is the TF-IDF
score of the ith word in the relevant section of the
document (title, body or both). As detailed further
in §5 we compute IDF values from a large pre-
training dataset. Furthermore, for both the mono-
lingual and crosslingual cases, we also experiment
with using document timestamp features, as ex-
plained in §4.1. We use a new set of diverse times-
tamp features in addition to the simple absolute
difference (in hours) between timestamps used by
Rupnik et al. (2016).

4.1 Similarity Metrics

Our similarity metric computes weighted cosine
similarity on the different subvectors, both in the
case of monolingual clustering and crosslingual
clustering. Formally, for the monolingual case, the
similarity is given by a function deﬁned as:

3
(cid:88)

i=1

K
(cid:88)

i=1

Γ0(dj, cl) =

φi(dj, cl) · q0

i +

γi(dj, cl) · q1
i .

(1)
and is computed on the TF-IDF subvectors where
K is the number of subvectors for the relevant
document representation. For the crosslingual case,
we discuss below the function Γ1, which has a
similar structure.

Here, dj is the jth document in the stream and
cl is a monolingual cluster. The function φi(dj, cl)
returns the cosine similarity between the document
representation of the jth document and the centroid
for cluster cl. The vector q0 denotes the weights
through which each of the cosine similarity values
for each subvectors are weighted, whereas q1 de-
notes the weights for the timestamp features, as
detailed further. Details on learning the weights q0
and q1 are discussed in §4.2.

The function γ(d, c) that maps a pair of docu-

ment and cluster to R3 is deﬁned as follows. Let

f (t) = exp

−

(cid:18)

(cid:19)

(t − µ)2
2σ2

(2)

for a given µ and σ > 0. For each document
d and cluster c, we generate the following three-
dimensional vector γ(d, c) = (s1, s2, s3):

• s1 = f (t(d) − n1(c)) where t(dj) is the times-
tamp for document d and n1(c) is the timestamp
for the newest document in cluster c.

• s2 = f (t(d)−n2(c)) where n2(c) is the average

timestamp for all documents in cluster c.

• s3 = f (t(d) − n3(c)) where n3(c) is the times-

tamp for the oldest document in cluster c.

These three timestamp1 features model the time
aspect of the online stream of news data and help
disambiguate clustering decisions, since time is a
valuable indicator that a news story has changed,
even if a cluster representation has a reasonable
match in the textual features with the incoming
document. The same way a news story becomes
popular and fades over time (Lerman and Hogg,
2010), we model the probability of a document
belonging to a cluster (in terms of timestamp dif-
ference) with a probability distribution.

For the case of crosslingual clustering, we in-
troduce Γ1, which has a similar deﬁnition to Γ0,
only instead of passing document/cluster similarity
feature vectors, we pass cluster/cluster similarities,
across all language pairs. Furthermore, the features
are the crosslingual embedding vectors of the sec-
tions title, body and both combined (similarly to the
monolingual case) and the timestamp features. For
denoting the cluster timestamp, we use the average
timestamps of all articles in it.

i = 1 ∀i and q1

4.2 Learning to Rank Candidates
In §4.1 we introduced q0 and q1 as the weight
vectors for the several document representation
features. We experiment with both setting these
weights to just 1 (q0
j = 1 ∀j ∈ [3])
and also learning these weights using support vec-
tor machines (SVMs). To generate the SVM train-
ing data, we simulate the execution of the algorithm
on a training data partition (which we do not get
evaluated on) and in which the gold standard labels
are given. We run the algorithm using only the ﬁrst
subvector φ1(dj, cl), which is the TF-IDF vector
with the words of the document in the body and
title. For each incoming document, we create a col-
lection of positive examples, for the document and
the clusters which share at least one document in
the gold labeling. We then generate 20 negative ex-
amples for the document from the 20 best-matching
clusters which are not correct. To ﬁnd out the best-
matching clusters, we rank them according to their

1Timestamps are given in hours since 1970.

similarity to the input document using only the ﬁrst
subvector φ1(dj, cl).

Using this scheme we generate a collection
of ranking examples (one for each document in
the dataset, with the ranking of the best cluster
matches), which are then trained using the SVM-
Rank algorithm (Joachims, 2002). We run 5-fold
cross-validation on this data to select the best
model, and train both a separate model for each
language according to Γ0 and a crosslingual model
according to Γ1.

5 Experiments

Our system was designed to cluster documents
from a (potentially inﬁnite) real-word data stream.
The datasets typically used in the literature (TDT,
Reuters) have a small number of clusters (≈ 20)
with coarse topics (economy, society, etc.), and
therefore are not relevant to the use case of me-
dia monitoring we treat - as it requires much more
ﬁne-grained story clusters about particular events.
To evaluate our approach, we adapted a dataset
constructed for the different purpose of binary clas-
siﬁcation of joining cluster pairs.2 We processed it
to become a collection of articles annotated with
monolingual and crosslingual cluster labels.3

Statistics about this dataset are given in Ta-
ble 1. As described further, we tune the hyper-
parameter τ on the development set. As for the
hyper-parameters related to the timestamp features,
we ﬁxed µ = 0 and tuned σ on the development set,
yielding σ = 72 hours (3 days).4 To compute IDF
scores (which are global numbers computed across
a corpus), we used a different and much larger
dataset that we collected from Deutsche Welle’s
news website (http://www.dw.com/). The
dataset consists of 77,268, 118,045 and 134,243
documents for Spanish, English and German, re-
spectively.

The conclusions from our experiments are: (a)
the weighting of the similarity metric features using
SVM signiﬁcantly outperforms unsupervised base-
lines such as CluStream (Table 2); (b) the SVM
approach signiﬁcantly helps to learn when to cre-
ate a new cluster, compared to simple grid search

2https://github.com/rupnikj/jair_paper
3The code and data we used is available at https://

github.com/priberam/news-clustering.

4This shows a relative robustness to reordering of the arti-
cles – articles within 3 days of each other could appear any-
where in that window, and the algorithm would still perform
well.

i
a
r
t

Dataset
n English
German
Spanish
t English
German
Spanish

s
e
t

Size
12,233
4,043
4,527
8,726
2,101
2,177

Avg. L.
434
282
355
521
440
392

C
593
377
416
222
118
149

Avg. S.
21
11
11
39
18
15

Table 1: Statistics for the development and evaluation
datasets, constructed from the dataset in Rupnik et al.
(2016), as explained in §5. “Size” denotes the number
of documents in the collection, “Avg. L.” is the aver-
age number of words in a document, “C” denotes the
number of clusters in the collection and “Avg. S.” is the
average number of documents in each cluster.

for the optimal τ (Table 4); (c) separating the fea-
ture space into one for monolingual clusters in the
form of keywords and the other for crosslingual
clusters based on crosslingual embeddings signiﬁ-
cantly helps performance.

Evaluation Method We evaluate clustering in
the following manner: let tp be the number of cor-
rectly clustered-together document pairs, let fp be
the number of incorrectly clustered-together docu-
ment pairs and let fn be the number of incorrectly
not-clustered-together document pairs. Then we
report precision as
tp+fn and F1
as the harmonic mean of the precision and recall
measures.

tp+fp , recall as

tp

tp

We do the same to evaluate crosslingual cluster-
ing, but on a higher level: we count tp, fn and fp for
the decisions of clustering clusters, as crosslingual
clusters are groups of monolingual gold clusters.

5.1 Monolingual Results

In our ﬁrst set of experiments, we report results
on monolingual clustering for each language sepa-
rately. Monolingual clustering of a stream of doc-
uments is an important problem that has been in-
spected by others, such as by Ahmed et al. (2011)
and by Aggarwal and Yu (2006). We compare our
results to our own implementation of the online
micro-clustering routine presented by Aggarwal
and Yu (2006), which shall be referred to as CluS-
tream. We note that CluStream of Aggarwal and
Yu (2006) has been a widely used state-of-the-art
system in media monitoring companies as well as
academia, and serves as a strong baseline to this
day.

In our preliminary experiments, we also evalu-
ated an online latent semantic analysis method, in
which the centroids we keep for the function H (see

TOKENS+LEMMAS+ENTS

s
i
l
g
n
E

algorithm
h CluStream

+TS
n CluStream
a
m
r
e
G

+TS
h CluStream
s
i
n
a
p
S

+TS

TOKENS+LEMMAS+ENTS

TOKENS+LEMMAS+ENTS

F1
79.0
92.7
94.1
89.7
90.7
97.1
78.1
88.8
94.2

P
98.6
92.9
98.2
99.9
99.7
99.9
73.4
95.9
97.0

R
65.9
92.5
90.3
81.3
83.2
94.5
83.5
82.7
91.6

Table 2: Clustering results on the labeled dataset. We
compare our algorithm (with and without timestamps)
with the online micro-clustering routine of Aggarwal
and Yu (2006) (denoted by CluStream). The F1 values
are for the precision (P) and recall (R) in the follow-
ing columns. See Table 3 for a legend of the different
models. Best result for each language is in bold.

§3) are the average of reduced dimensional vectors
of the incoming documents as generated by an in-
cremental singular value decomposition (SVD) of
a document-term matrix that is updated after each
incoming document. However, we discovered that
online LSA performs signiﬁcantly worse than rep-
resenting the documents the way is described in §4.
Furthermore, it was also signiﬁcantly slower than
our algorithm due to the time it took to perform
singular value decomposition.5

Clustering experiments Table 2 gives the ﬁnal
monolingual results on the three datasets. For En-
glish, we see that the signiﬁcant improvement we
get using our algorithm over the algorithm of Ag-
garwal and Yu (2006) is due to an increased recall
score. We also note that the trained models surpass
the baseline for all languages, and that the times-
tamp feature (denoted by TS), while not required
to beat the baseline, has a very relevant contribu-
tion in all cases. Although the results for both the
baseline and our models seem to differ across lan-
guages, one can verify a consistent improvement
from the latter to the former, suggesting that the
score differences should be mostly tied to the differ-
ent difﬁculty found across the datasets for each lan-
guage. The presented scores show that our learning
framework generalizes well to different languages
and enables high quality clustering results.

To investigate the impact of the timestamp fea-

5More speciﬁcally, we used an object of type lsimodel
from the GenSim package that implements algorithms from
ˇReh˚uˇrek (2010). The GenSim package can be found at
https://pypi.python.org/pypi/gensim.

feature
TOKENS
TOKENS+LEMMAS
TOKENS+LEMMAS+ENTS
TOKENS+LEMMAS+ENTS+TS

accuracy
85.5
85.9
86.5
96.9

Table 3: Accuracy of the SVM ranker on the English
training set. TOKENS are the word token features, LEM-
MAS are the lemma features for title and body, ENTS
are named entity features and TS are timestamp fea-
tures. All features are described in detail in §4, and are
listed for both the title and the body.

tures, we ran an additional experiment using only
the same three timestamp features as used in the
best model on the English dataset. This experi-
ment yielded scores of F1 = 61.1, P = 44.5 and
R = 97.6, which lead us to conclude that while
these features are not competitive when used alone
(hence temporal information by itself is not sufﬁ-
cient to predict the clusters), they contribute signif-
icantly to recall with the ﬁnal feature ensemble.

We note that as described in §3, the optimiza-
tion of the τ parameter is part of the development
process. The parameter τ is a similarity thresh-
old used to decide when an incoming document
should merge to the best cluster or create a new
one. We tune τ on the development set for each
language, and the sensitivity to it is demonstrated
in Figure 3 (this process is further referred to as
τsearch). Although applying grid-search on this pa-
rameter is the most immediate approach to this
problem, we experimented with a different method
which yielded superior results: as described further,
we discuss how to do this process with an additional
classiﬁer (denoted SVM-merge), which captures
more information about the incoming documents
and the existing clusters.

Additionally, we also experimented with comput-
ing the monolingual clusters with the same embed-
dings as used in the crosslingual clustering phase,
which yielded poor results. In particular, this sys-
tem achieved F1 score of 74.8 for English, which
is below the bag-of-words baseline presented in
Table 2. This result supports the approach we then
followed of having two separate feature spaces for
the monolingual and crosslingual clustering sys-
tems, where the monolingual space is discrete and
the crosslingual space is based on embeddings.

SVM ranker experiments To investigate the im-
portance of each feature, we now consider in Ta-

Figure 3: The F1 score of the different language de-
velopment sets as a function of the threshold τ . The
ﬁrst point for each language is identiﬁed using binary
search.

model
τsearch
SVM-merge

F1
82.8
94.1

P
96.5
98.2

R
72.4
90.3

Table 4: Comparison of two different cluster decision
techniques for the English SVM model with all fea-
tures (see Table 2). The ﬁrst method, τsearch, corre-
sponds to executing grid-search to ﬁnd the optimal clus-
tering τ parameter (see §3). SVM-merge is an alterna-
tive method in which we train an SVM binary classi-
ﬁer to decide if a new cluster should be created or not,
where we use as features the maximal value of each
coordinate for each document in a cluster.

ble 3 the accuracy of the SVM ranker for English
as described in §4.1. We note that adding features
increases the accuracy of the SVM ranker, espe-
cially the timestamp features. However, the times-
tamp feature actually interferes with our optimiza-
tion of τ to identify when new clusters are needed,
although they improve the SVM reranking accu-
racy. We speculate this is true because high accu-
racy in the reranking problem does not necessarily
help with identifying when new clusters need to be
opened. To investigate this issue, we experimented
with a different technique to learn when to create a
new cluster. To this end, we trained another SVM
classiﬁer just to learn this decision, this time a bi-
nary classiﬁer using LIBLINEAR (Fan et al., 2008),
by passing the max of the similarity of each feature
between the incoming document and the current
clustering pool as the input feature vector. This
way, the classiﬁer learns when the current clusters,

crosslingual model
τsearch (global)
τsearch (pivot)

F1
72.7
84.0

P
89.8
83.0

R
61.0
85.0

6 Related Work

Table 5: Crosslingual clustering results when consid-
ering two different approaches to compute distances
across crosslingual clusters on the test set for Spanish,
German and English. See text for details.

as a whole, are of a different news story than the
incoming document. As presented in Table 4, this
method, which we refer to as SVM-merge, solved
the issue of searching for the optimal τ parame-
ter for the SVM-rank model with timestamps, by
greatly improving the F1 score in respect to the
original grid-search approach (τsearch).

5.2 Crosslingual Results

As mentioned in §3, crosslingual embeddings are
used for crosslingual clustering. We experimented
with the crosslingual embeddings of Gardner et al.
(2015) and Ammar et al. (2016). In our preliminary
experiments we found that the former worked better
for our use-case than the latter.

We test two different scenarios for optimizing
the similarity threshold τ for the crosslingual case.
Table 5 shows the results for these experiments.
First, we consider the simpler case of adjusting a
global τ parameter for the crosslingual distances, as
also described for the monolingual case. As shown,
this method works poorly, since the τ grid-search
could not ﬁnd a reasonable τ which worked well
for every possible language pair.

Subsequently, we also consider the case of using
English as a pivot language (see §3), where dis-
tances for every other language are only compared
to English, and crosslingual clustering decisions
are made only based on this distance.6 This yielded
our best crosslingual score of F1=84.0, conﬁrm-
ing that crosslingual similarity is of higher quality
between each language and English, for the em-
beddings we used. This score represents only a
small degradation in respect to the monolingual
results, since clustering across different languages
is a harder problem.

6In this case, all crosslingual clusters will have at least one
pivot monolingual cluster, except for clusters which might
stay unmerged as single-language degenerated crosslingual
clusters.

Early research efforts, such as the TDT pro-
gram (Allan et al., 1998), have studied news clus-
tering for some time. The problem of online mono-
lingual clustering algorithms (for English) has also
received a fair amount of attention in the litera-
ture. One of the earlier papers by Aggarwal and
Yu (2006) introduced a two-step clustering system
with both ofﬂine and online components, where the
online model is based on a streaming implemen-
tation of k-means and a bag-of-words document
representation. Other authors have experimented
with distributed representations, such as Ahmed
et al. (2011), who cluster news into storylines us-
ing Markov chain Monte Carlo methods, ˇReh˚uˇrek
and Sojka (2010) who used incremental Singular
Value Decomposition (SVD) to ﬁnd relevant topics
from streaming data, and Sato et al. (2017) who
used the paragraph vector model (Le and Mikolov,
2014) in an ofﬂine clustering setting.

More recently, crosslingual linking of clusters
has been discussed by Rupnik et al. (2016) in the
context of linking existing clusters from the Event
Registry (Leban et al., 2014) in a batch fashion,
and by Steinberger (2016) who also present a batch
clustering linking system. However, these are not
“truly” online crosslingual clustering systems since
they only decide on the linking of already-built
monolingual clusters. In particular, Rupnik et al.
(2016) compute distances of document pairs across
clusters using nearest neighbors, which might not
scale well in an online setting. As detailed before,
we adapted the cluster-linking dataset from Rup-
nik et al. (2016) to evaluate our online crosslingual
clustering approach. Preliminary work makes use
of deep learning techniques (Xie et al., 2016; Guo
et al., 2017) to cluster documents while learning
their representations, but not in an online or mul-
tilingual fashion, and with a very small number of
cluster labels (4, in the case of the text benchmark).

In our work, we studied the problem of mono-
lingual and crosslingual clustering, having exper-
imented several directions and methods and the
impact they have on the ﬁnal clustering quality. We
described the ﬁrst system which aggregates news
articles into ﬁne-grained story clusters across differ-
ent languages in a completely online and scalable
fashion from a continuous stream.

7 Conclusion

We described a method for monolingual and
crosslingual clustering of an incoming stream of
documents. The method works by maintaining cen-
troids for the monolingual and crosslingual clus-
ters, where a monolingual cluster groups a set of
documents and a crosslingual cluster groups a set
of monolingual clusters. We presented an online
crosslingual clustering method which auto-corrects
past decisions in an efﬁcient way. We showed that
our method gives state-of-the-art results on a mul-
tilingual news article dataset for English, Spanish
and German. Finally, we discussed how to leverage
different SVM training procedures for ranking and
classiﬁcation to improve monolingual and crosslin-
gual clustering decisions. Our system is integrated
in a larger media monitoring project (Liepins et al.,
2017; Germann et al., 2018) and solving the use-
cases of monitors and journalists, having been vali-
dated with qualitative user testing.

Acknowledgments

We would like to thank Esma Balkır, Nikos Pa-
pasarantopoulos, Afonso Mendes, Shashi Narayan
and the anonymous reviewers for their feedback.
This project was supported by the European H2020
project SUMMA, grant agreement 688139 (see
http://www.summa-project.eu) and by a
grant from Bloomberg.

References

Charu C. Aggarwal and Philip S. Yu. 2006. A frame-
work for clustering massive text and categorical data
streams. In SDM, pages 479–483. SIAM.

Amr Ahmed, Qirong Ho, Jacob Eisenstein, Eric Xing,
Alexander J Smola, and Choon Hui Teo. 2011. Uni-
In Proceedings
ﬁed analysis of streaming news.
of the 20th international conference on World wide
web, pages 267–276. ACM.

James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, Yiming Yang, et al. 1998. Topic
detection and tracking pilot study: Final report. In
Proceedings of the DARPA broadcast news tran-
scription and understanding workshop.

Carlos Amaral, Ad´an Cassan, Helena Figueira, Andr´e
Martins, Afonso Mendes, Pedro Mendes, Jos´e Pina,
and Cl´audia Pinto. 2008. Priberam’s question an-
In Proceed-
swering system in QA@ CLEF 2008.
ings of the Workshop of the Cross-Language Evalu-
ation Forum for European Languages.

Waleed Ammar, George Mulcaire, Yulia Tsvetkov,
Guillaume Lample, Chris Dyer, and Noah A Smith.
2016. Massively multilingual word embeddings.
arXiv preprint arXiv:1602.01925.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classiﬁcation. Journal of ma-
chine learning research, 9(Aug):1871–1874.

Matt Gardner, Kejun Huang, Evangelos Papalex-
akis, Xiao Fu, Partha Talukdar, Christos Falout-
sos, Nicholas Sidiropoulos, and Tom Mitchell. 2015.
Translation invariant word embeddings. In Proceed-
ings of EMNLP.

Ulrich Germann, Renars Liepins, Guntis Barzdins,
Didzis Gosko, Sebasti˜ao Miranda, and David
Nogueira. 2018. The summa platform: A scalable
infrastructure for multi-lingual multi-media monitor-
ing. Proceedings of ACL 2018, System Demonstra-
tions.

Xifeng Guo, Long Gao, Xinwang Liu, and Jianping
Yin. 2017. Improved deep embedded clustering with
In Proceedings of IJ-
local structure preservation.
CAI.

Thorsten Joachims. 2002. Optimizing search engines
In Proceedings ACM

using clickthrough data.
SIGKDD.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proceed-
ings of ICML.

Gregor Leban, Blaz Fortuna, Janez Brank, and Marko
Grobelnik. 2014. Event registry: learning about
world events from news. In Proceedings of WWW.

Kristina Lerman and Tad Hogg. 2010. Using a model
of social dynamics to predict popularity of news. In
Proceedings of WWW.

Renars Liepins, Ulrich Germann, Guntis Barzdins,
Alexandra Birch, Steve Renals, Susanne Weber,
Peggy van der Kreeft, Herve Bourlard, Jo˜ao Pri-
eto, Ondrej Klejch, Peter Bell, Alexandros Lazaridis,
Alfonso Mendes, Sebastian Riedel, Mariana S. C.
Almeida, Pedro Balage, Shay B. Cohen, Tomasz
Dwojak, Philip N. Garner, Andreas Giefer, Marcin
Junczys-Dowmunt, Hina Imran, David Nogueira,
Ahmed Ali, Sebasti˜ao Miranda, Andrei Popescu-
Belis, Lesly Miculicich Werlen, Nikos Papasaran-
topoulos, Abiola Obamuyide, Clive Jones, Fahim
Dalvi, Andreas Vlachos, Yang Wang, Sibo Tong,
Rico Sennrich, Nikolaos Pappas, Shashi Narayan,
Marco Damonte, Nadir Durrani, Sameer Khurana,
Ahmed Abdelali, Hassan Sajjad, Stephan Vogel,
David Sheppey, Chris Hernon, and Jeff Mitchell.
In Pro-
2017. The SUMMA platform prototype.
ceedings of the Software Demonstrations of EACL.

Andr´e F. T. Martins, Miguel B. Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
In Proceedings of
non-projective turbo parsers.
ACL.

Radim ˇReh˚uˇrek. 2010. Fast and faster: A comparison
of two streamed matrix decomposition algorithms.

Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora.
In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks.

Jan Rupnik, Andrej Muhic, Gregor Leban, Primoz
Skraba, Blaz Fortuna, and Marko Grobelnik. 2016.
News across languages-cross-lingual document sim-
ilarity and event tracking. Journal of Artiﬁcial Intel-
ligence Research, 55:283–316.

Motoki Sato, Austin J Brockmeier, Georgios Kontonat-
sios, Tingting Mu, John Y Goulermas, Jun’ichi Tsu-
jii, and Sophia Ananiadou. 2017. Distributed docu-
ment and phrase co-embeddings for descriptive clus-
tering. In Proceedings of EACL.

Josef Steinberger. 2016. MediaGist: A cross-lingual
analyser of aggregated news and commentaries. In
Proceedings of ACL.

Junyuan Xie, Ross Girshick, and Ali Farhadi. 2016.
Unsupervised deep embedding for clustering analy-
sis. In Proceedings of ICML.

Multilingual Clustering of Streaming News

Sebasti˜ao Miranda♦ Art ¯urs Znotin¸ ˇs†(cid:63)
Shay B. Cohen♥ Guntis Barzdins†(cid:63)
♦Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal
†Innovation Labs LETA, Marijas Str. 2, Riga LV-1050, Latvia
(cid:63)University of Latvia, IMCS, Rainis Blvd. 29, Riga LV-1459, Latvia
♥School of Informatics, University of Edinburgh, Edinburgh EH8 9AB
sebastiao.miranda@priberam.pt, arturs.znotins@leta.lv,
scohen@inf.ed.ac.uk, guntis.barzdins@lu.lv

Abstract

Clustering news across languages enables
efﬁcient media monitoring by aggregating ar-
ticles from multilingual sources into coherent
stories. Doing so in an online setting allows
scalable processing of massive news streams.
To this end, we describe a novel method for
clustering an incoming stream of multilingual
documents into monolingual and crosslingual
story clusters. Unlike
clustering
approaches that consider a small and known
number of labels, we tackle the problem of
discovering an ever growing number of cluster
labels in an online fashion, using real news
datasets in multiple languages. Our method
is
simple to implement, computationally
efﬁcient and produces state-of-the-art results
on datasets in German, English and Spanish.

typical

1

Introduction

Following developing news stories is imperative to
making real-time decisions on important political
and public safety matters. Given the abundance of
media providers and languages, this endeavor is an
extremely difﬁcult task. As such, there is a strong
demand for automatic clustering of news streams,
so that they can be organized into stories or themes
for further processing. Performing this task in an
online and efﬁcient manner is a challenging prob-
lem, not only for newswire, but also for scientiﬁc
articles, online reviews, forum posts, blogs, and
microblogs.

A key challenge in handling document streams
is that the story clusters must be generated on the
ﬂy in an online fashion: this requires handling doc-
uments one-by-one as they appear in the document
stream. In this paper, we provide a treatment to the
problem of online document clustering, i.e. the task
of clustering a stream of documents into themes.
For example, for news articles, we would want to
cluster them into related news stories.

To this end, we introduce a system which aggre-
gates news articles into ﬁne-grained story clusters
across different languages in a completely online
and scalable fashion from a continuous stream. Our
clustering approach is part of a larger media mon-
itoring project to solve the problem of monitor-
ing massive text and TV/Radio streams (speech-
to-text). In particular, media monitors write intelli-
gence reports about the most relevant events, and
being able to search, visualize and explore news
clusters assists in gathering more insight about
a particular story. Since relevant events may be
spawned from any part of the world (and from
many multilingual sources), it becomes imperative
to cluster news across different languages.

In terms of granularity, the type of story clusters
we are interested in are the group of articles which,
for example : (i) Narrate recent air-strikes in East-
ern Ghouta (Syria); (ii) Describe the recent launch
of Space X’s Falcon Heavy rocket.

Main Contributions While most existing news
clustering approaches assume a monolingual docu-
ment stream – a non-realistic scenario given the di-
versity of languages on the Web – we assume a gen-
eral, multilingual, document stream. This means
that in our problem-formulation story documents
appear in multiple languages and we need to cluster
them to crosslingual clusters. Our main contribu-
tions are as follows:

• We develop a system that aggregates news arti-
cles into ﬁne-grained story clusters across differ-
ent languages in a completely online and scalable
fashion from a continuous stream. As discussed
in the introduction, this is a highly relevant task
for the use-case of media monitoring.

• We formulate the problem of online multilingual
document clustering and the representation that
such clustering takes by interlacing the problem

8
1
0
2
 
p
e
S
 
3
 
 
]
L
C
.
s
c
[
 
 
1
v
0
4
5
0
0
.
9
0
8
1
:
v
i
X
r
a

of monolingual clustering with crosslingual clus-
tering. The representation of our clusters is inter-
pretable, and similarly to topic models, consists
of a set of keywords and weights associated with
the relevant cluster. In our formulation, a mono-
lingual cluster is a group of documents, and a
crosslingual cluster is a group of monolingual
clusters in different languages.

• We compare our approach to our own imple-
mentation of a state-of-the-art streaming method,
and show much superior results for a dataset in
English, Spanish and German.

2 Problem Formulation

We focus on clustering of a stream of documents,
where the number of clusters is not ﬁxed and
learned automatically. We denote by D a (poten-
tially inﬁnite) space of multilingual documents.
Each document d is associated with a language in
which it is written through a function L : D → L
where L is a set of languages. For example, L(d)
could return English, Spanish or German. (In the
rest of the paper, for an integer n, we denote by [n]
the set {1, . . . , n}.)

We are interested in associating each document
with a monolingual cluster via the function C(d) ∈
N, which returns the cluster label given a document.
This is done independently for each language, such
that the space of indices we use for each language
is separate.

Furthermore, we interlace the problem of mono-
lingual clustering with crosslingual clustering. This
means that as part of our problem formulation we
are also interested in a function E : N×L → N that
associates each monolingual cluster with a crosslin-
gual cluster, such that each crosslingual cluster only
groups one monolingual cluster per different lan-
guage, at a given time. The crosslingual cluster
for a document d is E(C(d), L(d)). As such, a
crosslingual cluster groups together monolingual
clusters, at most one for each different language.

Intuitively, building both monolingual and
crosslingual clusters allows the system to leverage
high-precision monolingual features (e.g., words,
named entities) to cluster documents of the same
language, while simplifying the task of crosslingual
clustering to the computation of similarity scores
across monolingual clusters - which is a smaller
problem space, since there are (by deﬁnition) less
clusters than articles. We validate this choice in §5.

3 The Clustering Algorithm

Each document d is represented by two vectors in
Rk1 and Rk2. The ﬁrst vector exists in a “mono-
lingual space” (of dimensionality k1) and is based
on a bag-of-words representation of the document.
The second vector exists in a “crosslingual space”
(of dimensionality k2) which is common to all lan-
guages. More details about these representations
are discussed in §4.

Online Clustering With our clustering algo-
rithm, we maintain two types of centroid functions
for each monolingual cluster. The ﬁrst is a cen-
troid function H : N × L → Rk1 ∪ {⊥} that as-
sists in associating each document with a mono-
lingual cluster. The second is a centroid function
G : N → Rk2 ∪ {⊥} that assists in associating each
monolingual cluster with a crosslingual cluster. The
⊥ symbol is reserved to denote documents which
are not associated with any cluster yet.

In our algorithm, we need to incrementally con-
struct the functions H, G (the two centroid func-
tions), C (the monolingual clustering function) and
E (the crosslingual clustering function). Informally,
we do so by ﬁrst identifying a monolingual cluster
for an incoming document by ﬁnding the closest
centroid with the function H, and then associate
that monolingual cluster with the crosslingual clus-
ter that is closest based on the function G. The ﬁrst
update changes C and the second update changes
E. Once we do that, we also update H and G to
reﬂect the new information that exists in the new
incoming document.

Example Figure 1 depicts the algorithm and the
state it maintains. A document in some language
(d9) appears in the stream, and is clustered into
one of the monolingual clusters (circles) that group
together documents about the same story (for ex-
ample, (cid:104)c2, DE(cid:105) could be a German cluster about a
recent political event). Then, following this mono-
lingual update, the online clustering algorithm up-
dates the crosslingual clusters (round rectangles),
each grouping together a set of monolingual clus-
ters, one per language at the most. The centroids
for the monolingual clusters are maintained by the
function H. For example, H(2, English) gives the
centroid of the upper left English monolingual clus-
ter. The function G maintains the crosslingual clus-
ters. Considering the upper-left most crosslingual
cluster, a1, then G(1) returns its centroid.

given crosslingual cluster. In our experiments, we
mostly use English as the pivot language.

H Update To update H, we maintain a centroid
for each cluster that is created as the average of
all monolingual representations of documents that
belong to that cluster. This is done for each lan-
guage separately. This update can be done in O(k1)
time in each step. Similarly, the update of G can be
done in O(k2) time. In principle, we consider an
“inﬁnite” stream of documents, which means the
number of documents in each cluster can be large.
As such, for efﬁciency purposes, updates to H are
immutable, which means that when a document is
assigned to a monolingual cluster, that assignment
is never changed.

G Update As described, updates to function G
result in associating a monolingual cluster with a
crosslingual cluster (and consequently, other mono-
lingual clusters). Therefore, errors committed in
updating G are of a higher magnitude than those
committed in H, since they involve groups of doc-
uments. We also note that the best crosslingual
cluster for a particular monolingual cluster might
not be found right at the beginning of the process.
We experiment with two types of updates to G.
One which is immutable, in which changes to G
are not reversed (and are described above), and
one in which we introduce a novel technique to
make a sequence of changes to G if necessary, as a
mechanism to self-correct past clustering decisions.
When a past decision is modiﬁed, it may result in a
chaining of consequent modiﬁcations (“toppling of
dominoes”) which need to be evaluated. We coin
this method “domino-toppling”.

The motivation behind this technique is the
change in news stories over time. The technique
allows the method to modify past crosslingual clus-
tering decisions and enables higher quality clus-
tering results. When a past decision is modiﬁed, it
may result in a chain of consequent modiﬁcations
which need to be evaluated.

Our method of “domino-toppling” works by
making (potentially sequences) of changes to pre-
vious clustering decisions for the crosslingual clus-
ters, at each step placing a residual monolingual
cluster in a crosslingual cluster that is most similar
to it. Figure 2 gives the pseudocode for domino
toppling.

This “domino-toppling” technique could have
in principle a quadratic complexity in the number

Figure 1: A pictorial description of the algorithm and
the state it maintains. The algorithm maintains a mono-
lingual cluster space, in which each cluster is a set
of documents in a speciﬁc language. The algorithm
also maintains a crosslingual cluster space, in which
a cluster is a set of monolingual clusters in different
languages. Documents are denoted by di, monolingual
clusters by ci (circles) and crosslingual clusters by ai.

Algorithm To be more precise, the online clus-
tering process works as follows. H and G start with
just returning ⊥ for any cluster number, both mono-
lingual and crosslingual. With a new incoming doc-
ument d, represented as a vector, we compute a
similarity metric Γ0 : Rk1 × Rk1 → R between the
document vector and each of the existing centroids
{i | H(i, L(d)) (cid:54)= ⊥}. If the largest similarity ex-
ceeds a threshold τ for cluster index j, then we set
C(d) = j. In that case, we also update the value of
H(i, L(d)) to include new information from doc-
ument d, as detailed below under “H update.” If
none of the similarity values exceed a threshold
τ , we ﬁnd the ﬁrst i such that H(i, L(d)) = ⊥
(the ﬁrst cluster id which is still unassigned), and
set C(d) = i, therefore creating a new cluster. We
again follow an “H update” – this time for starting
a new cluster.

In both cases, we also update the function G,
by selecting the best crosslingual cluster for the
recently updated (or created) monolingual clus-
ter. To this end, we use another similarity metric
Γ1 : Rk2 × Rk2 → R. Accordingly, we compute
the similarity (using Γ1) between the updated (or
created) monolingual cluster and all monolingual
clusters in each candidate crosslingual cluster, in
the crosslingual feature space. The crosslingual
cluster with highest sum of similarity scores is then
selected. We also experimented computing this sim-
ilarity by considering just the monolingual cluster
of a particular “pivot language”. The pivot language
is a language that serves as the main indicator for a

Inputs: A monolingual cluster c and a list of pairs
(cid:104)aj, Γ1(c, aj)(cid:105), j ∈ [N ].

Algorithm:

• For all pairs (cid:104)aj, Γ1(c, aj)(cid:105), j ∈ [N ], ordered by the

second coordinate:

• If L(c) is not in aj, add c to aj and break.
• Otherwise, let y ← M (aj, L(c)). If Γ1(c, aj) >

Γ1(y, aj) then:

• Add c to aj, remove y from aj and call
domino toppling with y playing the role of c
and break.

• If c is left unassigned, create aN +1 and add c to it.

Figure 2: Crosslingual “domino-toppling”. aj is the jth
crosslingual cluster (out of total N clusters) and Γ1 is
the similarity between them as in §4. L(c) is the lan-
guage for cluster c. M (a, (cid:96)) returns the monolingual
cluster for language (cid:96) ∈ L in crosslingual cluster a.
See text for details.

of crosslingual clusters. However, we have veri-
ﬁed that in practice it converges very fast, and in
our evaluation dataset only 1% of the crosslingual
updates result in topples. We apply this technique
only to update G (and not H) because reversing
cluster assignments in G can be done much more
efﬁciently than in H – the total number of mono-
lingual clusters (the clustered elements in G) is
signiﬁcantly smaller than the number of documents
(the clustered elements in H). Crosslingual cluster-
ing is also a harder problem, which motivated the
additional effort of developing this algorithm.

4 Document Representation

In this section, we give more details about the way
we construct the document representations in the
monolingual and crosslingual spaces. In particu-
lar, we introduce the deﬁnition of the similarity
functions Γ0 and Γ1 that were referred in §3.

Monolingual Representation The monolingual
representation for each document d in language
L(d) is a vector in Rk1 constructed from several
TF-IDF subvectors with words, word lemmas and
named entities. Each subvector is repeated for dif-
ferent sections of the document, the title, the body
and both of them together. Besides these text ﬁelds
and document timestamps, no other metadata was
used. To detect named entities, we used Priberam’s
Text Analysis (Amaral et al., 2008) for English and
Spanish, and Turbo Parser (Martins et al., 2013) for
German. The extracted entities consist of people,
organizations, places and other types.

Crosslingual Representation In the crosslin-
gual space, a document representation is a vec-
tor in Rk2. Let e(d, i) be a crosslingual embed-
ding of word i in the document d, which is a vec-
tor of length m. Then the document representa-
tion v(d) of d consists of subvectors of the form
v(d) = (cid:80)n
i=1 tie(d, i), where ti is the TF-IDF
score of the ith word in the relevant section of the
document (title, body or both). As detailed further
in §5 we compute IDF values from a large pre-
training dataset. Furthermore, for both the mono-
lingual and crosslingual cases, we also experiment
with using document timestamp features, as ex-
plained in §4.1. We use a new set of diverse times-
tamp features in addition to the simple absolute
difference (in hours) between timestamps used by
Rupnik et al. (2016).

4.1 Similarity Metrics

Our similarity metric computes weighted cosine
similarity on the different subvectors, both in the
case of monolingual clustering and crosslingual
clustering. Formally, for the monolingual case, the
similarity is given by a function deﬁned as:

3
(cid:88)

i=1

K
(cid:88)

i=1

Γ0(dj, cl) =

φi(dj, cl) · q0

i +

γi(dj, cl) · q1
i .

(1)
and is computed on the TF-IDF subvectors where
K is the number of subvectors for the relevant
document representation. For the crosslingual case,
we discuss below the function Γ1, which has a
similar structure.

Here, dj is the jth document in the stream and
cl is a monolingual cluster. The function φi(dj, cl)
returns the cosine similarity between the document
representation of the jth document and the centroid
for cluster cl. The vector q0 denotes the weights
through which each of the cosine similarity values
for each subvectors are weighted, whereas q1 de-
notes the weights for the timestamp features, as
detailed further. Details on learning the weights q0
and q1 are discussed in §4.2.

The function γ(d, c) that maps a pair of docu-

ment and cluster to R3 is deﬁned as follows. Let

f (t) = exp

−

(cid:18)

(cid:19)

(t − µ)2
2σ2

(2)

for a given µ and σ > 0. For each document
d and cluster c, we generate the following three-
dimensional vector γ(d, c) = (s1, s2, s3):

• s1 = f (t(d) − n1(c)) where t(dj) is the times-
tamp for document d and n1(c) is the timestamp
for the newest document in cluster c.

• s2 = f (t(d)−n2(c)) where n2(c) is the average

timestamp for all documents in cluster c.

• s3 = f (t(d) − n3(c)) where n3(c) is the times-

tamp for the oldest document in cluster c.

These three timestamp1 features model the time
aspect of the online stream of news data and help
disambiguate clustering decisions, since time is a
valuable indicator that a news story has changed,
even if a cluster representation has a reasonable
match in the textual features with the incoming
document. The same way a news story becomes
popular and fades over time (Lerman and Hogg,
2010), we model the probability of a document
belonging to a cluster (in terms of timestamp dif-
ference) with a probability distribution.

For the case of crosslingual clustering, we in-
troduce Γ1, which has a similar deﬁnition to Γ0,
only instead of passing document/cluster similarity
feature vectors, we pass cluster/cluster similarities,
across all language pairs. Furthermore, the features
are the crosslingual embedding vectors of the sec-
tions title, body and both combined (similarly to the
monolingual case) and the timestamp features. For
denoting the cluster timestamp, we use the average
timestamps of all articles in it.

i = 1 ∀i and q1

4.2 Learning to Rank Candidates
In §4.1 we introduced q0 and q1 as the weight
vectors for the several document representation
features. We experiment with both setting these
weights to just 1 (q0
j = 1 ∀j ∈ [3])
and also learning these weights using support vec-
tor machines (SVMs). To generate the SVM train-
ing data, we simulate the execution of the algorithm
on a training data partition (which we do not get
evaluated on) and in which the gold standard labels
are given. We run the algorithm using only the ﬁrst
subvector φ1(dj, cl), which is the TF-IDF vector
with the words of the document in the body and
title. For each incoming document, we create a col-
lection of positive examples, for the document and
the clusters which share at least one document in
the gold labeling. We then generate 20 negative ex-
amples for the document from the 20 best-matching
clusters which are not correct. To ﬁnd out the best-
matching clusters, we rank them according to their

1Timestamps are given in hours since 1970.

similarity to the input document using only the ﬁrst
subvector φ1(dj, cl).

Using this scheme we generate a collection
of ranking examples (one for each document in
the dataset, with the ranking of the best cluster
matches), which are then trained using the SVM-
Rank algorithm (Joachims, 2002). We run 5-fold
cross-validation on this data to select the best
model, and train both a separate model for each
language according to Γ0 and a crosslingual model
according to Γ1.

5 Experiments

Our system was designed to cluster documents
from a (potentially inﬁnite) real-word data stream.
The datasets typically used in the literature (TDT,
Reuters) have a small number of clusters (≈ 20)
with coarse topics (economy, society, etc.), and
therefore are not relevant to the use case of me-
dia monitoring we treat - as it requires much more
ﬁne-grained story clusters about particular events.
To evaluate our approach, we adapted a dataset
constructed for the different purpose of binary clas-
siﬁcation of joining cluster pairs.2 We processed it
to become a collection of articles annotated with
monolingual and crosslingual cluster labels.3

Statistics about this dataset are given in Ta-
ble 1. As described further, we tune the hyper-
parameter τ on the development set. As for the
hyper-parameters related to the timestamp features,
we ﬁxed µ = 0 and tuned σ on the development set,
yielding σ = 72 hours (3 days).4 To compute IDF
scores (which are global numbers computed across
a corpus), we used a different and much larger
dataset that we collected from Deutsche Welle’s
news website (http://www.dw.com/). The
dataset consists of 77,268, 118,045 and 134,243
documents for Spanish, English and German, re-
spectively.

The conclusions from our experiments are: (a)
the weighting of the similarity metric features using
SVM signiﬁcantly outperforms unsupervised base-
lines such as CluStream (Table 2); (b) the SVM
approach signiﬁcantly helps to learn when to cre-
ate a new cluster, compared to simple grid search

2https://github.com/rupnikj/jair_paper
3The code and data we used is available at https://

github.com/priberam/news-clustering.

4This shows a relative robustness to reordering of the arti-
cles – articles within 3 days of each other could appear any-
where in that window, and the algorithm would still perform
well.

i
a
r
t

Dataset
n English
German
Spanish
t English
German
Spanish

s
e
t

Size
12,233
4,043
4,527
8,726
2,101
2,177

Avg. L.
434
282
355
521
440
392

C
593
377
416
222
118
149

Avg. S.
21
11
11
39
18
15

Table 1: Statistics for the development and evaluation
datasets, constructed from the dataset in Rupnik et al.
(2016), as explained in §5. “Size” denotes the number
of documents in the collection, “Avg. L.” is the aver-
age number of words in a document, “C” denotes the
number of clusters in the collection and “Avg. S.” is the
average number of documents in each cluster.

for the optimal τ (Table 4); (c) separating the fea-
ture space into one for monolingual clusters in the
form of keywords and the other for crosslingual
clusters based on crosslingual embeddings signiﬁ-
cantly helps performance.

Evaluation Method We evaluate clustering in
the following manner: let tp be the number of cor-
rectly clustered-together document pairs, let fp be
the number of incorrectly clustered-together docu-
ment pairs and let fn be the number of incorrectly
not-clustered-together document pairs. Then we
report precision as
tp+fn and F1
as the harmonic mean of the precision and recall
measures.

tp+fp , recall as

tp

tp

We do the same to evaluate crosslingual cluster-
ing, but on a higher level: we count tp, fn and fp for
the decisions of clustering clusters, as crosslingual
clusters are groups of monolingual gold clusters.

5.1 Monolingual Results

In our ﬁrst set of experiments, we report results
on monolingual clustering for each language sepa-
rately. Monolingual clustering of a stream of doc-
uments is an important problem that has been in-
spected by others, such as by Ahmed et al. (2011)
and by Aggarwal and Yu (2006). We compare our
results to our own implementation of the online
micro-clustering routine presented by Aggarwal
and Yu (2006), which shall be referred to as CluS-
tream. We note that CluStream of Aggarwal and
Yu (2006) has been a widely used state-of-the-art
system in media monitoring companies as well as
academia, and serves as a strong baseline to this
day.

In our preliminary experiments, we also evalu-
ated an online latent semantic analysis method, in
which the centroids we keep for the function H (see

TOKENS+LEMMAS+ENTS

s
i
l
g
n
E

algorithm
h CluStream

+TS
n CluStream
a
m
r
e
G

+TS
h CluStream
s
i
n
a
p
S

+TS

TOKENS+LEMMAS+ENTS

TOKENS+LEMMAS+ENTS

F1
79.0
92.7
94.1
89.7
90.7
97.1
78.1
88.8
94.2

P
98.6
92.9
98.2
99.9
99.7
99.9
73.4
95.9
97.0

R
65.9
92.5
90.3
81.3
83.2
94.5
83.5
82.7
91.6

Table 2: Clustering results on the labeled dataset. We
compare our algorithm (with and without timestamps)
with the online micro-clustering routine of Aggarwal
and Yu (2006) (denoted by CluStream). The F1 values
are for the precision (P) and recall (R) in the follow-
ing columns. See Table 3 for a legend of the different
models. Best result for each language is in bold.

§3) are the average of reduced dimensional vectors
of the incoming documents as generated by an in-
cremental singular value decomposition (SVD) of
a document-term matrix that is updated after each
incoming document. However, we discovered that
online LSA performs signiﬁcantly worse than rep-
resenting the documents the way is described in §4.
Furthermore, it was also signiﬁcantly slower than
our algorithm due to the time it took to perform
singular value decomposition.5

Clustering experiments Table 2 gives the ﬁnal
monolingual results on the three datasets. For En-
glish, we see that the signiﬁcant improvement we
get using our algorithm over the algorithm of Ag-
garwal and Yu (2006) is due to an increased recall
score. We also note that the trained models surpass
the baseline for all languages, and that the times-
tamp feature (denoted by TS), while not required
to beat the baseline, has a very relevant contribu-
tion in all cases. Although the results for both the
baseline and our models seem to differ across lan-
guages, one can verify a consistent improvement
from the latter to the former, suggesting that the
score differences should be mostly tied to the differ-
ent difﬁculty found across the datasets for each lan-
guage. The presented scores show that our learning
framework generalizes well to different languages
and enables high quality clustering results.

To investigate the impact of the timestamp fea-

5More speciﬁcally, we used an object of type lsimodel
from the GenSim package that implements algorithms from
ˇReh˚uˇrek (2010). The GenSim package can be found at
https://pypi.python.org/pypi/gensim.

feature
TOKENS
TOKENS+LEMMAS
TOKENS+LEMMAS+ENTS
TOKENS+LEMMAS+ENTS+TS

accuracy
85.5
85.9
86.5
96.9

Table 3: Accuracy of the SVM ranker on the English
training set. TOKENS are the word token features, LEM-
MAS are the lemma features for title and body, ENTS
are named entity features and TS are timestamp fea-
tures. All features are described in detail in §4, and are
listed for both the title and the body.

tures, we ran an additional experiment using only
the same three timestamp features as used in the
best model on the English dataset. This experi-
ment yielded scores of F1 = 61.1, P = 44.5 and
R = 97.6, which lead us to conclude that while
these features are not competitive when used alone
(hence temporal information by itself is not sufﬁ-
cient to predict the clusters), they contribute signif-
icantly to recall with the ﬁnal feature ensemble.

We note that as described in §3, the optimiza-
tion of the τ parameter is part of the development
process. The parameter τ is a similarity thresh-
old used to decide when an incoming document
should merge to the best cluster or create a new
one. We tune τ on the development set for each
language, and the sensitivity to it is demonstrated
in Figure 3 (this process is further referred to as
τsearch). Although applying grid-search on this pa-
rameter is the most immediate approach to this
problem, we experimented with a different method
which yielded superior results: as described further,
we discuss how to do this process with an additional
classiﬁer (denoted SVM-merge), which captures
more information about the incoming documents
and the existing clusters.

Additionally, we also experimented with comput-
ing the monolingual clusters with the same embed-
dings as used in the crosslingual clustering phase,
which yielded poor results. In particular, this sys-
tem achieved F1 score of 74.8 for English, which
is below the bag-of-words baseline presented in
Table 2. This result supports the approach we then
followed of having two separate feature spaces for
the monolingual and crosslingual clustering sys-
tems, where the monolingual space is discrete and
the crosslingual space is based on embeddings.

SVM ranker experiments To investigate the im-
portance of each feature, we now consider in Ta-

Figure 3: The F1 score of the different language de-
velopment sets as a function of the threshold τ . The
ﬁrst point for each language is identiﬁed using binary
search.

model
τsearch
SVM-merge

F1
82.8
94.1

P
96.5
98.2

R
72.4
90.3

Table 4: Comparison of two different cluster decision
techniques for the English SVM model with all fea-
tures (see Table 2). The ﬁrst method, τsearch, corre-
sponds to executing grid-search to ﬁnd the optimal clus-
tering τ parameter (see §3). SVM-merge is an alterna-
tive method in which we train an SVM binary classi-
ﬁer to decide if a new cluster should be created or not,
where we use as features the maximal value of each
coordinate for each document in a cluster.

ble 3 the accuracy of the SVM ranker for English
as described in §4.1. We note that adding features
increases the accuracy of the SVM ranker, espe-
cially the timestamp features. However, the times-
tamp feature actually interferes with our optimiza-
tion of τ to identify when new clusters are needed,
although they improve the SVM reranking accu-
racy. We speculate this is true because high accu-
racy in the reranking problem does not necessarily
help with identifying when new clusters need to be
opened. To investigate this issue, we experimented
with a different technique to learn when to create a
new cluster. To this end, we trained another SVM
classiﬁer just to learn this decision, this time a bi-
nary classiﬁer using LIBLINEAR (Fan et al., 2008),
by passing the max of the similarity of each feature
between the incoming document and the current
clustering pool as the input feature vector. This
way, the classiﬁer learns when the current clusters,

crosslingual model
τsearch (global)
τsearch (pivot)

F1
72.7
84.0

P
89.8
83.0

R
61.0
85.0

6 Related Work

Table 5: Crosslingual clustering results when consid-
ering two different approaches to compute distances
across crosslingual clusters on the test set for Spanish,
German and English. See text for details.

as a whole, are of a different news story than the
incoming document. As presented in Table 4, this
method, which we refer to as SVM-merge, solved
the issue of searching for the optimal τ parame-
ter for the SVM-rank model with timestamps, by
greatly improving the F1 score in respect to the
original grid-search approach (τsearch).

5.2 Crosslingual Results

As mentioned in §3, crosslingual embeddings are
used for crosslingual clustering. We experimented
with the crosslingual embeddings of Gardner et al.
(2015) and Ammar et al. (2016). In our preliminary
experiments we found that the former worked better
for our use-case than the latter.

We test two different scenarios for optimizing
the similarity threshold τ for the crosslingual case.
Table 5 shows the results for these experiments.
First, we consider the simpler case of adjusting a
global τ parameter for the crosslingual distances, as
also described for the monolingual case. As shown,
this method works poorly, since the τ grid-search
could not ﬁnd a reasonable τ which worked well
for every possible language pair.

Subsequently, we also consider the case of using
English as a pivot language (see §3), where dis-
tances for every other language are only compared
to English, and crosslingual clustering decisions
are made only based on this distance.6 This yielded
our best crosslingual score of F1=84.0, conﬁrm-
ing that crosslingual similarity is of higher quality
between each language and English, for the em-
beddings we used. This score represents only a
small degradation in respect to the monolingual
results, since clustering across different languages
is a harder problem.

6In this case, all crosslingual clusters will have at least one
pivot monolingual cluster, except for clusters which might
stay unmerged as single-language degenerated crosslingual
clusters.

Early research efforts, such as the TDT pro-
gram (Allan et al., 1998), have studied news clus-
tering for some time. The problem of online mono-
lingual clustering algorithms (for English) has also
received a fair amount of attention in the litera-
ture. One of the earlier papers by Aggarwal and
Yu (2006) introduced a two-step clustering system
with both ofﬂine and online components, where the
online model is based on a streaming implemen-
tation of k-means and a bag-of-words document
representation. Other authors have experimented
with distributed representations, such as Ahmed
et al. (2011), who cluster news into storylines us-
ing Markov chain Monte Carlo methods, ˇReh˚uˇrek
and Sojka (2010) who used incremental Singular
Value Decomposition (SVD) to ﬁnd relevant topics
from streaming data, and Sato et al. (2017) who
used the paragraph vector model (Le and Mikolov,
2014) in an ofﬂine clustering setting.

More recently, crosslingual linking of clusters
has been discussed by Rupnik et al. (2016) in the
context of linking existing clusters from the Event
Registry (Leban et al., 2014) in a batch fashion,
and by Steinberger (2016) who also present a batch
clustering linking system. However, these are not
“truly” online crosslingual clustering systems since
they only decide on the linking of already-built
monolingual clusters. In particular, Rupnik et al.
(2016) compute distances of document pairs across
clusters using nearest neighbors, which might not
scale well in an online setting. As detailed before,
we adapted the cluster-linking dataset from Rup-
nik et al. (2016) to evaluate our online crosslingual
clustering approach. Preliminary work makes use
of deep learning techniques (Xie et al., 2016; Guo
et al., 2017) to cluster documents while learning
their representations, but not in an online or mul-
tilingual fashion, and with a very small number of
cluster labels (4, in the case of the text benchmark).

In our work, we studied the problem of mono-
lingual and crosslingual clustering, having exper-
imented several directions and methods and the
impact they have on the ﬁnal clustering quality. We
described the ﬁrst system which aggregates news
articles into ﬁne-grained story clusters across differ-
ent languages in a completely online and scalable
fashion from a continuous stream.

7 Conclusion

We described a method for monolingual and
crosslingual clustering of an incoming stream of
documents. The method works by maintaining cen-
troids for the monolingual and crosslingual clus-
ters, where a monolingual cluster groups a set of
documents and a crosslingual cluster groups a set
of monolingual clusters. We presented an online
crosslingual clustering method which auto-corrects
past decisions in an efﬁcient way. We showed that
our method gives state-of-the-art results on a mul-
tilingual news article dataset for English, Spanish
and German. Finally, we discussed how to leverage
different SVM training procedures for ranking and
classiﬁcation to improve monolingual and crosslin-
gual clustering decisions. Our system is integrated
in a larger media monitoring project (Liepins et al.,
2017; Germann et al., 2018) and solving the use-
cases of monitors and journalists, having been vali-
dated with qualitative user testing.

Acknowledgments

We would like to thank Esma Balkır, Nikos Pa-
pasarantopoulos, Afonso Mendes, Shashi Narayan
and the anonymous reviewers for their feedback.
This project was supported by the European H2020
project SUMMA, grant agreement 688139 (see
http://www.summa-project.eu) and by a
grant from Bloomberg.

References

Charu C. Aggarwal and Philip S. Yu. 2006. A frame-
work for clustering massive text and categorical data
streams. In SDM, pages 479–483. SIAM.

Amr Ahmed, Qirong Ho, Jacob Eisenstein, Eric Xing,
Alexander J Smola, and Choon Hui Teo. 2011. Uni-
In Proceedings
ﬁed analysis of streaming news.
of the 20th international conference on World wide
web, pages 267–276. ACM.

James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, Yiming Yang, et al. 1998. Topic
detection and tracking pilot study: Final report. In
Proceedings of the DARPA broadcast news tran-
scription and understanding workshop.

Carlos Amaral, Ad´an Cassan, Helena Figueira, Andr´e
Martins, Afonso Mendes, Pedro Mendes, Jos´e Pina,
and Cl´audia Pinto. 2008. Priberam’s question an-
In Proceed-
swering system in QA@ CLEF 2008.
ings of the Workshop of the Cross-Language Evalu-
ation Forum for European Languages.

Waleed Ammar, George Mulcaire, Yulia Tsvetkov,
Guillaume Lample, Chris Dyer, and Noah A Smith.
2016. Massively multilingual word embeddings.
arXiv preprint arXiv:1602.01925.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classiﬁcation. Journal of ma-
chine learning research, 9(Aug):1871–1874.

Matt Gardner, Kejun Huang, Evangelos Papalex-
akis, Xiao Fu, Partha Talukdar, Christos Falout-
sos, Nicholas Sidiropoulos, and Tom Mitchell. 2015.
Translation invariant word embeddings. In Proceed-
ings of EMNLP.

Ulrich Germann, Renars Liepins, Guntis Barzdins,
Didzis Gosko, Sebasti˜ao Miranda, and David
Nogueira. 2018. The summa platform: A scalable
infrastructure for multi-lingual multi-media monitor-
ing. Proceedings of ACL 2018, System Demonstra-
tions.

Xifeng Guo, Long Gao, Xinwang Liu, and Jianping
Yin. 2017. Improved deep embedded clustering with
In Proceedings of IJ-
local structure preservation.
CAI.

Thorsten Joachims. 2002. Optimizing search engines
In Proceedings ACM

using clickthrough data.
SIGKDD.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proceed-
ings of ICML.

Gregor Leban, Blaz Fortuna, Janez Brank, and Marko
Grobelnik. 2014. Event registry: learning about
world events from news. In Proceedings of WWW.

Kristina Lerman and Tad Hogg. 2010. Using a model
of social dynamics to predict popularity of news. In
Proceedings of WWW.

Renars Liepins, Ulrich Germann, Guntis Barzdins,
Alexandra Birch, Steve Renals, Susanne Weber,
Peggy van der Kreeft, Herve Bourlard, Jo˜ao Pri-
eto, Ondrej Klejch, Peter Bell, Alexandros Lazaridis,
Alfonso Mendes, Sebastian Riedel, Mariana S. C.
Almeida, Pedro Balage, Shay B. Cohen, Tomasz
Dwojak, Philip N. Garner, Andreas Giefer, Marcin
Junczys-Dowmunt, Hina Imran, David Nogueira,
Ahmed Ali, Sebasti˜ao Miranda, Andrei Popescu-
Belis, Lesly Miculicich Werlen, Nikos Papasaran-
topoulos, Abiola Obamuyide, Clive Jones, Fahim
Dalvi, Andreas Vlachos, Yang Wang, Sibo Tong,
Rico Sennrich, Nikolaos Pappas, Shashi Narayan,
Marco Damonte, Nadir Durrani, Sameer Khurana,
Ahmed Abdelali, Hassan Sajjad, Stephan Vogel,
David Sheppey, Chris Hernon, and Jeff Mitchell.
In Pro-
2017. The SUMMA platform prototype.
ceedings of the Software Demonstrations of EACL.

Andr´e F. T. Martins, Miguel B. Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
In Proceedings of
non-projective turbo parsers.
ACL.

Radim ˇReh˚uˇrek. 2010. Fast and faster: A comparison
of two streamed matrix decomposition algorithms.

Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora.
In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks.

Jan Rupnik, Andrej Muhic, Gregor Leban, Primoz
Skraba, Blaz Fortuna, and Marko Grobelnik. 2016.
News across languages-cross-lingual document sim-
ilarity and event tracking. Journal of Artiﬁcial Intel-
ligence Research, 55:283–316.

Motoki Sato, Austin J Brockmeier, Georgios Kontonat-
sios, Tingting Mu, John Y Goulermas, Jun’ichi Tsu-
jii, and Sophia Ananiadou. 2017. Distributed docu-
ment and phrase co-embeddings for descriptive clus-
tering. In Proceedings of EACL.

Josef Steinberger. 2016. MediaGist: A cross-lingual
analyser of aggregated news and commentaries. In
Proceedings of ACL.

Junyuan Xie, Ross Girshick, and Ali Farhadi. 2016.
Unsupervised deep embedding for clustering analy-
sis. In Proceedings of ICML.

Multilingual Clustering of Streaming News

Sebasti˜ao Miranda♦ Art ¯urs Znotin¸ ˇs†(cid:63)
Shay B. Cohen♥ Guntis Barzdins†(cid:63)
♦Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal
†Innovation Labs LETA, Marijas Str. 2, Riga LV-1050, Latvia
(cid:63)University of Latvia, IMCS, Rainis Blvd. 29, Riga LV-1459, Latvia
♥School of Informatics, University of Edinburgh, Edinburgh EH8 9AB
sebastiao.miranda@priberam.pt, arturs.znotins@leta.lv,
scohen@inf.ed.ac.uk, guntis.barzdins@lu.lv

Abstract

Clustering news across languages enables
efﬁcient media monitoring by aggregating ar-
ticles from multilingual sources into coherent
stories. Doing so in an online setting allows
scalable processing of massive news streams.
To this end, we describe a novel method for
clustering an incoming stream of multilingual
documents into monolingual and crosslingual
story clusters. Unlike
clustering
approaches that consider a small and known
number of labels, we tackle the problem of
discovering an ever growing number of cluster
labels in an online fashion, using real news
datasets in multiple languages. Our method
is
simple to implement, computationally
efﬁcient and produces state-of-the-art results
on datasets in German, English and Spanish.

typical

1

Introduction

Following developing news stories is imperative to
making real-time decisions on important political
and public safety matters. Given the abundance of
media providers and languages, this endeavor is an
extremely difﬁcult task. As such, there is a strong
demand for automatic clustering of news streams,
so that they can be organized into stories or themes
for further processing. Performing this task in an
online and efﬁcient manner is a challenging prob-
lem, not only for newswire, but also for scientiﬁc
articles, online reviews, forum posts, blogs, and
microblogs.

A key challenge in handling document streams
is that the story clusters must be generated on the
ﬂy in an online fashion: this requires handling doc-
uments one-by-one as they appear in the document
stream. In this paper, we provide a treatment to the
problem of online document clustering, i.e. the task
of clustering a stream of documents into themes.
For example, for news articles, we would want to
cluster them into related news stories.

To this end, we introduce a system which aggre-
gates news articles into ﬁne-grained story clusters
across different languages in a completely online
and scalable fashion from a continuous stream. Our
clustering approach is part of a larger media mon-
itoring project to solve the problem of monitor-
ing massive text and TV/Radio streams (speech-
to-text). In particular, media monitors write intelli-
gence reports about the most relevant events, and
being able to search, visualize and explore news
clusters assists in gathering more insight about
a particular story. Since relevant events may be
spawned from any part of the world (and from
many multilingual sources), it becomes imperative
to cluster news across different languages.

In terms of granularity, the type of story clusters
we are interested in are the group of articles which,
for example : (i) Narrate recent air-strikes in East-
ern Ghouta (Syria); (ii) Describe the recent launch
of Space X’s Falcon Heavy rocket.

Main Contributions While most existing news
clustering approaches assume a monolingual docu-
ment stream – a non-realistic scenario given the di-
versity of languages on the Web – we assume a gen-
eral, multilingual, document stream. This means
that in our problem-formulation story documents
appear in multiple languages and we need to cluster
them to crosslingual clusters. Our main contribu-
tions are as follows:

• We develop a system that aggregates news arti-
cles into ﬁne-grained story clusters across differ-
ent languages in a completely online and scalable
fashion from a continuous stream. As discussed
in the introduction, this is a highly relevant task
for the use-case of media monitoring.

• We formulate the problem of online multilingual
document clustering and the representation that
such clustering takes by interlacing the problem

8
1
0
2
 
p
e
S
 
3
 
 
]
L
C
.
s
c
[
 
 
1
v
0
4
5
0
0
.
9
0
8
1
:
v
i
X
r
a

of monolingual clustering with crosslingual clus-
tering. The representation of our clusters is inter-
pretable, and similarly to topic models, consists
of a set of keywords and weights associated with
the relevant cluster. In our formulation, a mono-
lingual cluster is a group of documents, and a
crosslingual cluster is a group of monolingual
clusters in different languages.

• We compare our approach to our own imple-
mentation of a state-of-the-art streaming method,
and show much superior results for a dataset in
English, Spanish and German.

2 Problem Formulation

We focus on clustering of a stream of documents,
where the number of clusters is not ﬁxed and
learned automatically. We denote by D a (poten-
tially inﬁnite) space of multilingual documents.
Each document d is associated with a language in
which it is written through a function L : D → L
where L is a set of languages. For example, L(d)
could return English, Spanish or German. (In the
rest of the paper, for an integer n, we denote by [n]
the set {1, . . . , n}.)

We are interested in associating each document
with a monolingual cluster via the function C(d) ∈
N, which returns the cluster label given a document.
This is done independently for each language, such
that the space of indices we use for each language
is separate.

Furthermore, we interlace the problem of mono-
lingual clustering with crosslingual clustering. This
means that as part of our problem formulation we
are also interested in a function E : N×L → N that
associates each monolingual cluster with a crosslin-
gual cluster, such that each crosslingual cluster only
groups one monolingual cluster per different lan-
guage, at a given time. The crosslingual cluster
for a document d is E(C(d), L(d)). As such, a
crosslingual cluster groups together monolingual
clusters, at most one for each different language.

Intuitively, building both monolingual and
crosslingual clusters allows the system to leverage
high-precision monolingual features (e.g., words,
named entities) to cluster documents of the same
language, while simplifying the task of crosslingual
clustering to the computation of similarity scores
across monolingual clusters - which is a smaller
problem space, since there are (by deﬁnition) less
clusters than articles. We validate this choice in §5.

3 The Clustering Algorithm

Each document d is represented by two vectors in
Rk1 and Rk2. The ﬁrst vector exists in a “mono-
lingual space” (of dimensionality k1) and is based
on a bag-of-words representation of the document.
The second vector exists in a “crosslingual space”
(of dimensionality k2) which is common to all lan-
guages. More details about these representations
are discussed in §4.

Online Clustering With our clustering algo-
rithm, we maintain two types of centroid functions
for each monolingual cluster. The ﬁrst is a cen-
troid function H : N × L → Rk1 ∪ {⊥} that as-
sists in associating each document with a mono-
lingual cluster. The second is a centroid function
G : N → Rk2 ∪ {⊥} that assists in associating each
monolingual cluster with a crosslingual cluster. The
⊥ symbol is reserved to denote documents which
are not associated with any cluster yet.

In our algorithm, we need to incrementally con-
struct the functions H, G (the two centroid func-
tions), C (the monolingual clustering function) and
E (the crosslingual clustering function). Informally,
we do so by ﬁrst identifying a monolingual cluster
for an incoming document by ﬁnding the closest
centroid with the function H, and then associate
that monolingual cluster with the crosslingual clus-
ter that is closest based on the function G. The ﬁrst
update changes C and the second update changes
E. Once we do that, we also update H and G to
reﬂect the new information that exists in the new
incoming document.

Example Figure 1 depicts the algorithm and the
state it maintains. A document in some language
(d9) appears in the stream, and is clustered into
one of the monolingual clusters (circles) that group
together documents about the same story (for ex-
ample, (cid:104)c2, DE(cid:105) could be a German cluster about a
recent political event). Then, following this mono-
lingual update, the online clustering algorithm up-
dates the crosslingual clusters (round rectangles),
each grouping together a set of monolingual clus-
ters, one per language at the most. The centroids
for the monolingual clusters are maintained by the
function H. For example, H(2, English) gives the
centroid of the upper left English monolingual clus-
ter. The function G maintains the crosslingual clus-
ters. Considering the upper-left most crosslingual
cluster, a1, then G(1) returns its centroid.

given crosslingual cluster. In our experiments, we
mostly use English as the pivot language.

H Update To update H, we maintain a centroid
for each cluster that is created as the average of
all monolingual representations of documents that
belong to that cluster. This is done for each lan-
guage separately. This update can be done in O(k1)
time in each step. Similarly, the update of G can be
done in O(k2) time. In principle, we consider an
“inﬁnite” stream of documents, which means the
number of documents in each cluster can be large.
As such, for efﬁciency purposes, updates to H are
immutable, which means that when a document is
assigned to a monolingual cluster, that assignment
is never changed.

G Update As described, updates to function G
result in associating a monolingual cluster with a
crosslingual cluster (and consequently, other mono-
lingual clusters). Therefore, errors committed in
updating G are of a higher magnitude than those
committed in H, since they involve groups of doc-
uments. We also note that the best crosslingual
cluster for a particular monolingual cluster might
not be found right at the beginning of the process.
We experiment with two types of updates to G.
One which is immutable, in which changes to G
are not reversed (and are described above), and
one in which we introduce a novel technique to
make a sequence of changes to G if necessary, as a
mechanism to self-correct past clustering decisions.
When a past decision is modiﬁed, it may result in a
chaining of consequent modiﬁcations (“toppling of
dominoes”) which need to be evaluated. We coin
this method “domino-toppling”.

The motivation behind this technique is the
change in news stories over time. The technique
allows the method to modify past crosslingual clus-
tering decisions and enables higher quality clus-
tering results. When a past decision is modiﬁed, it
may result in a chain of consequent modiﬁcations
which need to be evaluated.

Our method of “domino-toppling” works by
making (potentially sequences) of changes to pre-
vious clustering decisions for the crosslingual clus-
ters, at each step placing a residual monolingual
cluster in a crosslingual cluster that is most similar
to it. Figure 2 gives the pseudocode for domino
toppling.

This “domino-toppling” technique could have
in principle a quadratic complexity in the number

Figure 1: A pictorial description of the algorithm and
the state it maintains. The algorithm maintains a mono-
lingual cluster space, in which each cluster is a set
of documents in a speciﬁc language. The algorithm
also maintains a crosslingual cluster space, in which
a cluster is a set of monolingual clusters in different
languages. Documents are denoted by di, monolingual
clusters by ci (circles) and crosslingual clusters by ai.

Algorithm To be more precise, the online clus-
tering process works as follows. H and G start with
just returning ⊥ for any cluster number, both mono-
lingual and crosslingual. With a new incoming doc-
ument d, represented as a vector, we compute a
similarity metric Γ0 : Rk1 × Rk1 → R between the
document vector and each of the existing centroids
{i | H(i, L(d)) (cid:54)= ⊥}. If the largest similarity ex-
ceeds a threshold τ for cluster index j, then we set
C(d) = j. In that case, we also update the value of
H(i, L(d)) to include new information from doc-
ument d, as detailed below under “H update.” If
none of the similarity values exceed a threshold
τ , we ﬁnd the ﬁrst i such that H(i, L(d)) = ⊥
(the ﬁrst cluster id which is still unassigned), and
set C(d) = i, therefore creating a new cluster. We
again follow an “H update” – this time for starting
a new cluster.

In both cases, we also update the function G,
by selecting the best crosslingual cluster for the
recently updated (or created) monolingual clus-
ter. To this end, we use another similarity metric
Γ1 : Rk2 × Rk2 → R. Accordingly, we compute
the similarity (using Γ1) between the updated (or
created) monolingual cluster and all monolingual
clusters in each candidate crosslingual cluster, in
the crosslingual feature space. The crosslingual
cluster with highest sum of similarity scores is then
selected. We also experimented computing this sim-
ilarity by considering just the monolingual cluster
of a particular “pivot language”. The pivot language
is a language that serves as the main indicator for a

Inputs: A monolingual cluster c and a list of pairs
(cid:104)aj, Γ1(c, aj)(cid:105), j ∈ [N ].

Algorithm:

• For all pairs (cid:104)aj, Γ1(c, aj)(cid:105), j ∈ [N ], ordered by the

second coordinate:

• If L(c) is not in aj, add c to aj and break.
• Otherwise, let y ← M (aj, L(c)). If Γ1(c, aj) >

Γ1(y, aj) then:

• Add c to aj, remove y from aj and call
domino toppling with y playing the role of c
and break.

• If c is left unassigned, create aN +1 and add c to it.

Figure 2: Crosslingual “domino-toppling”. aj is the jth
crosslingual cluster (out of total N clusters) and Γ1 is
the similarity between them as in §4. L(c) is the lan-
guage for cluster c. M (a, (cid:96)) returns the monolingual
cluster for language (cid:96) ∈ L in crosslingual cluster a.
See text for details.

of crosslingual clusters. However, we have veri-
ﬁed that in practice it converges very fast, and in
our evaluation dataset only 1% of the crosslingual
updates result in topples. We apply this technique
only to update G (and not H) because reversing
cluster assignments in G can be done much more
efﬁciently than in H – the total number of mono-
lingual clusters (the clustered elements in G) is
signiﬁcantly smaller than the number of documents
(the clustered elements in H). Crosslingual cluster-
ing is also a harder problem, which motivated the
additional effort of developing this algorithm.

4 Document Representation

In this section, we give more details about the way
we construct the document representations in the
monolingual and crosslingual spaces. In particu-
lar, we introduce the deﬁnition of the similarity
functions Γ0 and Γ1 that were referred in §3.

Monolingual Representation The monolingual
representation for each document d in language
L(d) is a vector in Rk1 constructed from several
TF-IDF subvectors with words, word lemmas and
named entities. Each subvector is repeated for dif-
ferent sections of the document, the title, the body
and both of them together. Besides these text ﬁelds
and document timestamps, no other metadata was
used. To detect named entities, we used Priberam’s
Text Analysis (Amaral et al., 2008) for English and
Spanish, and Turbo Parser (Martins et al., 2013) for
German. The extracted entities consist of people,
organizations, places and other types.

Crosslingual Representation In the crosslin-
gual space, a document representation is a vec-
tor in Rk2. Let e(d, i) be a crosslingual embed-
ding of word i in the document d, which is a vec-
tor of length m. Then the document representa-
tion v(d) of d consists of subvectors of the form
v(d) = (cid:80)n
i=1 tie(d, i), where ti is the TF-IDF
score of the ith word in the relevant section of the
document (title, body or both). As detailed further
in §5 we compute IDF values from a large pre-
training dataset. Furthermore, for both the mono-
lingual and crosslingual cases, we also experiment
with using document timestamp features, as ex-
plained in §4.1. We use a new set of diverse times-
tamp features in addition to the simple absolute
difference (in hours) between timestamps used by
Rupnik et al. (2016).

4.1 Similarity Metrics

Our similarity metric computes weighted cosine
similarity on the different subvectors, both in the
case of monolingual clustering and crosslingual
clustering. Formally, for the monolingual case, the
similarity is given by a function deﬁned as:

3
(cid:88)

i=1

K
(cid:88)

i=1

Γ0(dj, cl) =

φi(dj, cl) · q0

i +

γi(dj, cl) · q1
i .

(1)
and is computed on the TF-IDF subvectors where
K is the number of subvectors for the relevant
document representation. For the crosslingual case,
we discuss below the function Γ1, which has a
similar structure.

Here, dj is the jth document in the stream and
cl is a monolingual cluster. The function φi(dj, cl)
returns the cosine similarity between the document
representation of the jth document and the centroid
for cluster cl. The vector q0 denotes the weights
through which each of the cosine similarity values
for each subvectors are weighted, whereas q1 de-
notes the weights for the timestamp features, as
detailed further. Details on learning the weights q0
and q1 are discussed in §4.2.

The function γ(d, c) that maps a pair of docu-

ment and cluster to R3 is deﬁned as follows. Let

f (t) = exp

−

(cid:18)

(cid:19)

(t − µ)2
2σ2

(2)

for a given µ and σ > 0. For each document
d and cluster c, we generate the following three-
dimensional vector γ(d, c) = (s1, s2, s3):

• s1 = f (t(d) − n1(c)) where t(dj) is the times-
tamp for document d and n1(c) is the timestamp
for the newest document in cluster c.

• s2 = f (t(d)−n2(c)) where n2(c) is the average

timestamp for all documents in cluster c.

• s3 = f (t(d) − n3(c)) where n3(c) is the times-

tamp for the oldest document in cluster c.

These three timestamp1 features model the time
aspect of the online stream of news data and help
disambiguate clustering decisions, since time is a
valuable indicator that a news story has changed,
even if a cluster representation has a reasonable
match in the textual features with the incoming
document. The same way a news story becomes
popular and fades over time (Lerman and Hogg,
2010), we model the probability of a document
belonging to a cluster (in terms of timestamp dif-
ference) with a probability distribution.

For the case of crosslingual clustering, we in-
troduce Γ1, which has a similar deﬁnition to Γ0,
only instead of passing document/cluster similarity
feature vectors, we pass cluster/cluster similarities,
across all language pairs. Furthermore, the features
are the crosslingual embedding vectors of the sec-
tions title, body and both combined (similarly to the
monolingual case) and the timestamp features. For
denoting the cluster timestamp, we use the average
timestamps of all articles in it.

i = 1 ∀i and q1

4.2 Learning to Rank Candidates
In §4.1 we introduced q0 and q1 as the weight
vectors for the several document representation
features. We experiment with both setting these
weights to just 1 (q0
j = 1 ∀j ∈ [3])
and also learning these weights using support vec-
tor machines (SVMs). To generate the SVM train-
ing data, we simulate the execution of the algorithm
on a training data partition (which we do not get
evaluated on) and in which the gold standard labels
are given. We run the algorithm using only the ﬁrst
subvector φ1(dj, cl), which is the TF-IDF vector
with the words of the document in the body and
title. For each incoming document, we create a col-
lection of positive examples, for the document and
the clusters which share at least one document in
the gold labeling. We then generate 20 negative ex-
amples for the document from the 20 best-matching
clusters which are not correct. To ﬁnd out the best-
matching clusters, we rank them according to their

1Timestamps are given in hours since 1970.

similarity to the input document using only the ﬁrst
subvector φ1(dj, cl).

Using this scheme we generate a collection
of ranking examples (one for each document in
the dataset, with the ranking of the best cluster
matches), which are then trained using the SVM-
Rank algorithm (Joachims, 2002). We run 5-fold
cross-validation on this data to select the best
model, and train both a separate model for each
language according to Γ0 and a crosslingual model
according to Γ1.

5 Experiments

Our system was designed to cluster documents
from a (potentially inﬁnite) real-word data stream.
The datasets typically used in the literature (TDT,
Reuters) have a small number of clusters (≈ 20)
with coarse topics (economy, society, etc.), and
therefore are not relevant to the use case of me-
dia monitoring we treat - as it requires much more
ﬁne-grained story clusters about particular events.
To evaluate our approach, we adapted a dataset
constructed for the different purpose of binary clas-
siﬁcation of joining cluster pairs.2 We processed it
to become a collection of articles annotated with
monolingual and crosslingual cluster labels.3

Statistics about this dataset are given in Ta-
ble 1. As described further, we tune the hyper-
parameter τ on the development set. As for the
hyper-parameters related to the timestamp features,
we ﬁxed µ = 0 and tuned σ on the development set,
yielding σ = 72 hours (3 days).4 To compute IDF
scores (which are global numbers computed across
a corpus), we used a different and much larger
dataset that we collected from Deutsche Welle’s
news website (http://www.dw.com/). The
dataset consists of 77,268, 118,045 and 134,243
documents for Spanish, English and German, re-
spectively.

The conclusions from our experiments are: (a)
the weighting of the similarity metric features using
SVM signiﬁcantly outperforms unsupervised base-
lines such as CluStream (Table 2); (b) the SVM
approach signiﬁcantly helps to learn when to cre-
ate a new cluster, compared to simple grid search

2https://github.com/rupnikj/jair_paper
3The code and data we used is available at https://

github.com/priberam/news-clustering.

4This shows a relative robustness to reordering of the arti-
cles – articles within 3 days of each other could appear any-
where in that window, and the algorithm would still perform
well.

i
a
r
t

Dataset
n English
German
Spanish
t English
German
Spanish

s
e
t

Size
12,233
4,043
4,527
8,726
2,101
2,177

Avg. L.
434
282
355
521
440
392

C
593
377
416
222
118
149

Avg. S.
21
11
11
39
18
15

Table 1: Statistics for the development and evaluation
datasets, constructed from the dataset in Rupnik et al.
(2016), as explained in §5. “Size” denotes the number
of documents in the collection, “Avg. L.” is the aver-
age number of words in a document, “C” denotes the
number of clusters in the collection and “Avg. S.” is the
average number of documents in each cluster.

for the optimal τ (Table 4); (c) separating the fea-
ture space into one for monolingual clusters in the
form of keywords and the other for crosslingual
clusters based on crosslingual embeddings signiﬁ-
cantly helps performance.

Evaluation Method We evaluate clustering in
the following manner: let tp be the number of cor-
rectly clustered-together document pairs, let fp be
the number of incorrectly clustered-together docu-
ment pairs and let fn be the number of incorrectly
not-clustered-together document pairs. Then we
report precision as
tp+fn and F1
as the harmonic mean of the precision and recall
measures.

tp+fp , recall as

tp

tp

We do the same to evaluate crosslingual cluster-
ing, but on a higher level: we count tp, fn and fp for
the decisions of clustering clusters, as crosslingual
clusters are groups of monolingual gold clusters.

5.1 Monolingual Results

In our ﬁrst set of experiments, we report results
on monolingual clustering for each language sepa-
rately. Monolingual clustering of a stream of doc-
uments is an important problem that has been in-
spected by others, such as by Ahmed et al. (2011)
and by Aggarwal and Yu (2006). We compare our
results to our own implementation of the online
micro-clustering routine presented by Aggarwal
and Yu (2006), which shall be referred to as CluS-
tream. We note that CluStream of Aggarwal and
Yu (2006) has been a widely used state-of-the-art
system in media monitoring companies as well as
academia, and serves as a strong baseline to this
day.

In our preliminary experiments, we also evalu-
ated an online latent semantic analysis method, in
which the centroids we keep for the function H (see

TOKENS+LEMMAS+ENTS

s
i
l
g
n
E

algorithm
h CluStream

+TS
n CluStream
a
m
r
e
G

+TS
h CluStream
s
i
n
a
p
S

+TS

TOKENS+LEMMAS+ENTS

TOKENS+LEMMAS+ENTS

F1
79.0
92.7
94.1
89.7
90.7
97.1
78.1
88.8
94.2

P
98.6
92.9
98.2
99.9
99.7
99.9
73.4
95.9
97.0

R
65.9
92.5
90.3
81.3
83.2
94.5
83.5
82.7
91.6

Table 2: Clustering results on the labeled dataset. We
compare our algorithm (with and without timestamps)
with the online micro-clustering routine of Aggarwal
and Yu (2006) (denoted by CluStream). The F1 values
are for the precision (P) and recall (R) in the follow-
ing columns. See Table 3 for a legend of the different
models. Best result for each language is in bold.

§3) are the average of reduced dimensional vectors
of the incoming documents as generated by an in-
cremental singular value decomposition (SVD) of
a document-term matrix that is updated after each
incoming document. However, we discovered that
online LSA performs signiﬁcantly worse than rep-
resenting the documents the way is described in §4.
Furthermore, it was also signiﬁcantly slower than
our algorithm due to the time it took to perform
singular value decomposition.5

Clustering experiments Table 2 gives the ﬁnal
monolingual results on the three datasets. For En-
glish, we see that the signiﬁcant improvement we
get using our algorithm over the algorithm of Ag-
garwal and Yu (2006) is due to an increased recall
score. We also note that the trained models surpass
the baseline for all languages, and that the times-
tamp feature (denoted by TS), while not required
to beat the baseline, has a very relevant contribu-
tion in all cases. Although the results for both the
baseline and our models seem to differ across lan-
guages, one can verify a consistent improvement
from the latter to the former, suggesting that the
score differences should be mostly tied to the differ-
ent difﬁculty found across the datasets for each lan-
guage. The presented scores show that our learning
framework generalizes well to different languages
and enables high quality clustering results.

To investigate the impact of the timestamp fea-

5More speciﬁcally, we used an object of type lsimodel
from the GenSim package that implements algorithms from
ˇReh˚uˇrek (2010). The GenSim package can be found at
https://pypi.python.org/pypi/gensim.

feature
TOKENS
TOKENS+LEMMAS
TOKENS+LEMMAS+ENTS
TOKENS+LEMMAS+ENTS+TS

accuracy
85.5
85.9
86.5
96.9

Table 3: Accuracy of the SVM ranker on the English
training set. TOKENS are the word token features, LEM-
MAS are the lemma features for title and body, ENTS
are named entity features and TS are timestamp fea-
tures. All features are described in detail in §4, and are
listed for both the title and the body.

tures, we ran an additional experiment using only
the same three timestamp features as used in the
best model on the English dataset. This experi-
ment yielded scores of F1 = 61.1, P = 44.5 and
R = 97.6, which lead us to conclude that while
these features are not competitive when used alone
(hence temporal information by itself is not sufﬁ-
cient to predict the clusters), they contribute signif-
icantly to recall with the ﬁnal feature ensemble.

We note that as described in §3, the optimiza-
tion of the τ parameter is part of the development
process. The parameter τ is a similarity thresh-
old used to decide when an incoming document
should merge to the best cluster or create a new
one. We tune τ on the development set for each
language, and the sensitivity to it is demonstrated
in Figure 3 (this process is further referred to as
τsearch). Although applying grid-search on this pa-
rameter is the most immediate approach to this
problem, we experimented with a different method
which yielded superior results: as described further,
we discuss how to do this process with an additional
classiﬁer (denoted SVM-merge), which captures
more information about the incoming documents
and the existing clusters.

Additionally, we also experimented with comput-
ing the monolingual clusters with the same embed-
dings as used in the crosslingual clustering phase,
which yielded poor results. In particular, this sys-
tem achieved F1 score of 74.8 for English, which
is below the bag-of-words baseline presented in
Table 2. This result supports the approach we then
followed of having two separate feature spaces for
the monolingual and crosslingual clustering sys-
tems, where the monolingual space is discrete and
the crosslingual space is based on embeddings.

SVM ranker experiments To investigate the im-
portance of each feature, we now consider in Ta-

Figure 3: The F1 score of the different language de-
velopment sets as a function of the threshold τ . The
ﬁrst point for each language is identiﬁed using binary
search.

model
τsearch
SVM-merge

F1
82.8
94.1

P
96.5
98.2

R
72.4
90.3

Table 4: Comparison of two different cluster decision
techniques for the English SVM model with all fea-
tures (see Table 2). The ﬁrst method, τsearch, corre-
sponds to executing grid-search to ﬁnd the optimal clus-
tering τ parameter (see §3). SVM-merge is an alterna-
tive method in which we train an SVM binary classi-
ﬁer to decide if a new cluster should be created or not,
where we use as features the maximal value of each
coordinate for each document in a cluster.

ble 3 the accuracy of the SVM ranker for English
as described in §4.1. We note that adding features
increases the accuracy of the SVM ranker, espe-
cially the timestamp features. However, the times-
tamp feature actually interferes with our optimiza-
tion of τ to identify when new clusters are needed,
although they improve the SVM reranking accu-
racy. We speculate this is true because high accu-
racy in the reranking problem does not necessarily
help with identifying when new clusters need to be
opened. To investigate this issue, we experimented
with a different technique to learn when to create a
new cluster. To this end, we trained another SVM
classiﬁer just to learn this decision, this time a bi-
nary classiﬁer using LIBLINEAR (Fan et al., 2008),
by passing the max of the similarity of each feature
between the incoming document and the current
clustering pool as the input feature vector. This
way, the classiﬁer learns when the current clusters,

crosslingual model
τsearch (global)
τsearch (pivot)

F1
72.7
84.0

P
89.8
83.0

R
61.0
85.0

6 Related Work

Table 5: Crosslingual clustering results when consid-
ering two different approaches to compute distances
across crosslingual clusters on the test set for Spanish,
German and English. See text for details.

as a whole, are of a different news story than the
incoming document. As presented in Table 4, this
method, which we refer to as SVM-merge, solved
the issue of searching for the optimal τ parame-
ter for the SVM-rank model with timestamps, by
greatly improving the F1 score in respect to the
original grid-search approach (τsearch).

5.2 Crosslingual Results

As mentioned in §3, crosslingual embeddings are
used for crosslingual clustering. We experimented
with the crosslingual embeddings of Gardner et al.
(2015) and Ammar et al. (2016). In our preliminary
experiments we found that the former worked better
for our use-case than the latter.

We test two different scenarios for optimizing
the similarity threshold τ for the crosslingual case.
Table 5 shows the results for these experiments.
First, we consider the simpler case of adjusting a
global τ parameter for the crosslingual distances, as
also described for the monolingual case. As shown,
this method works poorly, since the τ grid-search
could not ﬁnd a reasonable τ which worked well
for every possible language pair.

Subsequently, we also consider the case of using
English as a pivot language (see §3), where dis-
tances for every other language are only compared
to English, and crosslingual clustering decisions
are made only based on this distance.6 This yielded
our best crosslingual score of F1=84.0, conﬁrm-
ing that crosslingual similarity is of higher quality
between each language and English, for the em-
beddings we used. This score represents only a
small degradation in respect to the monolingual
results, since clustering across different languages
is a harder problem.

6In this case, all crosslingual clusters will have at least one
pivot monolingual cluster, except for clusters which might
stay unmerged as single-language degenerated crosslingual
clusters.

Early research efforts, such as the TDT pro-
gram (Allan et al., 1998), have studied news clus-
tering for some time. The problem of online mono-
lingual clustering algorithms (for English) has also
received a fair amount of attention in the litera-
ture. One of the earlier papers by Aggarwal and
Yu (2006) introduced a two-step clustering system
with both ofﬂine and online components, where the
online model is based on a streaming implemen-
tation of k-means and a bag-of-words document
representation. Other authors have experimented
with distributed representations, such as Ahmed
et al. (2011), who cluster news into storylines us-
ing Markov chain Monte Carlo methods, ˇReh˚uˇrek
and Sojka (2010) who used incremental Singular
Value Decomposition (SVD) to ﬁnd relevant topics
from streaming data, and Sato et al. (2017) who
used the paragraph vector model (Le and Mikolov,
2014) in an ofﬂine clustering setting.

More recently, crosslingual linking of clusters
has been discussed by Rupnik et al. (2016) in the
context of linking existing clusters from the Event
Registry (Leban et al., 2014) in a batch fashion,
and by Steinberger (2016) who also present a batch
clustering linking system. However, these are not
“truly” online crosslingual clustering systems since
they only decide on the linking of already-built
monolingual clusters. In particular, Rupnik et al.
(2016) compute distances of document pairs across
clusters using nearest neighbors, which might not
scale well in an online setting. As detailed before,
we adapted the cluster-linking dataset from Rup-
nik et al. (2016) to evaluate our online crosslingual
clustering approach. Preliminary work makes use
of deep learning techniques (Xie et al., 2016; Guo
et al., 2017) to cluster documents while learning
their representations, but not in an online or mul-
tilingual fashion, and with a very small number of
cluster labels (4, in the case of the text benchmark).

In our work, we studied the problem of mono-
lingual and crosslingual clustering, having exper-
imented several directions and methods and the
impact they have on the ﬁnal clustering quality. We
described the ﬁrst system which aggregates news
articles into ﬁne-grained story clusters across differ-
ent languages in a completely online and scalable
fashion from a continuous stream.

7 Conclusion

We described a method for monolingual and
crosslingual clustering of an incoming stream of
documents. The method works by maintaining cen-
troids for the monolingual and crosslingual clus-
ters, where a monolingual cluster groups a set of
documents and a crosslingual cluster groups a set
of monolingual clusters. We presented an online
crosslingual clustering method which auto-corrects
past decisions in an efﬁcient way. We showed that
our method gives state-of-the-art results on a mul-
tilingual news article dataset for English, Spanish
and German. Finally, we discussed how to leverage
different SVM training procedures for ranking and
classiﬁcation to improve monolingual and crosslin-
gual clustering decisions. Our system is integrated
in a larger media monitoring project (Liepins et al.,
2017; Germann et al., 2018) and solving the use-
cases of monitors and journalists, having been vali-
dated with qualitative user testing.

Acknowledgments

We would like to thank Esma Balkır, Nikos Pa-
pasarantopoulos, Afonso Mendes, Shashi Narayan
and the anonymous reviewers for their feedback.
This project was supported by the European H2020
project SUMMA, grant agreement 688139 (see
http://www.summa-project.eu) and by a
grant from Bloomberg.

References

Charu C. Aggarwal and Philip S. Yu. 2006. A frame-
work for clustering massive text and categorical data
streams. In SDM, pages 479–483. SIAM.

Amr Ahmed, Qirong Ho, Jacob Eisenstein, Eric Xing,
Alexander J Smola, and Choon Hui Teo. 2011. Uni-
In Proceedings
ﬁed analysis of streaming news.
of the 20th international conference on World wide
web, pages 267–276. ACM.

James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, Yiming Yang, et al. 1998. Topic
detection and tracking pilot study: Final report. In
Proceedings of the DARPA broadcast news tran-
scription and understanding workshop.

Carlos Amaral, Ad´an Cassan, Helena Figueira, Andr´e
Martins, Afonso Mendes, Pedro Mendes, Jos´e Pina,
and Cl´audia Pinto. 2008. Priberam’s question an-
In Proceed-
swering system in QA@ CLEF 2008.
ings of the Workshop of the Cross-Language Evalu-
ation Forum for European Languages.

Waleed Ammar, George Mulcaire, Yulia Tsvetkov,
Guillaume Lample, Chris Dyer, and Noah A Smith.
2016. Massively multilingual word embeddings.
arXiv preprint arXiv:1602.01925.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classiﬁcation. Journal of ma-
chine learning research, 9(Aug):1871–1874.

Matt Gardner, Kejun Huang, Evangelos Papalex-
akis, Xiao Fu, Partha Talukdar, Christos Falout-
sos, Nicholas Sidiropoulos, and Tom Mitchell. 2015.
Translation invariant word embeddings. In Proceed-
ings of EMNLP.

Ulrich Germann, Renars Liepins, Guntis Barzdins,
Didzis Gosko, Sebasti˜ao Miranda, and David
Nogueira. 2018. The summa platform: A scalable
infrastructure for multi-lingual multi-media monitor-
ing. Proceedings of ACL 2018, System Demonstra-
tions.

Xifeng Guo, Long Gao, Xinwang Liu, and Jianping
Yin. 2017. Improved deep embedded clustering with
In Proceedings of IJ-
local structure preservation.
CAI.

Thorsten Joachims. 2002. Optimizing search engines
In Proceedings ACM

using clickthrough data.
SIGKDD.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proceed-
ings of ICML.

Gregor Leban, Blaz Fortuna, Janez Brank, and Marko
Grobelnik. 2014. Event registry: learning about
world events from news. In Proceedings of WWW.

Kristina Lerman and Tad Hogg. 2010. Using a model
of social dynamics to predict popularity of news. In
Proceedings of WWW.

Renars Liepins, Ulrich Germann, Guntis Barzdins,
Alexandra Birch, Steve Renals, Susanne Weber,
Peggy van der Kreeft, Herve Bourlard, Jo˜ao Pri-
eto, Ondrej Klejch, Peter Bell, Alexandros Lazaridis,
Alfonso Mendes, Sebastian Riedel, Mariana S. C.
Almeida, Pedro Balage, Shay B. Cohen, Tomasz
Dwojak, Philip N. Garner, Andreas Giefer, Marcin
Junczys-Dowmunt, Hina Imran, David Nogueira,
Ahmed Ali, Sebasti˜ao Miranda, Andrei Popescu-
Belis, Lesly Miculicich Werlen, Nikos Papasaran-
topoulos, Abiola Obamuyide, Clive Jones, Fahim
Dalvi, Andreas Vlachos, Yang Wang, Sibo Tong,
Rico Sennrich, Nikolaos Pappas, Shashi Narayan,
Marco Damonte, Nadir Durrani, Sameer Khurana,
Ahmed Abdelali, Hassan Sajjad, Stephan Vogel,
David Sheppey, Chris Hernon, and Jeff Mitchell.
In Pro-
2017. The SUMMA platform prototype.
ceedings of the Software Demonstrations of EACL.

Andr´e F. T. Martins, Miguel B. Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
In Proceedings of
non-projective turbo parsers.
ACL.

Radim ˇReh˚uˇrek. 2010. Fast and faster: A comparison
of two streamed matrix decomposition algorithms.

Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora.
In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks.

Jan Rupnik, Andrej Muhic, Gregor Leban, Primoz
Skraba, Blaz Fortuna, and Marko Grobelnik. 2016.
News across languages-cross-lingual document sim-
ilarity and event tracking. Journal of Artiﬁcial Intel-
ligence Research, 55:283–316.

Motoki Sato, Austin J Brockmeier, Georgios Kontonat-
sios, Tingting Mu, John Y Goulermas, Jun’ichi Tsu-
jii, and Sophia Ananiadou. 2017. Distributed docu-
ment and phrase co-embeddings for descriptive clus-
tering. In Proceedings of EACL.

Josef Steinberger. 2016. MediaGist: A cross-lingual
analyser of aggregated news and commentaries. In
Proceedings of ACL.

Junyuan Xie, Ross Girshick, and Ali Farhadi. 2016.
Unsupervised deep embedding for clustering analy-
sis. In Proceedings of ICML.

