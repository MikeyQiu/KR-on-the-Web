Multi-stream CNN based Video Semantic Segmentation
for Automated Driving

Ganesh Sistu1, Sumanth Chennupati2 and Senthil Yogamani1
1Valeo Vision Systems, Ireland
2Valeo Troy, United States
{ganesh.sistu,sumanth.chennupati,senthil.yogamani}@valeo.com

9
1
0
2
 
n
a
J
 
8
 
 
]

V
C
.
s
c
[
 
 
1
v
1
1
5
2
0
.
1
0
9
1
:
v
i
X
r
a

Keywords:

Semantic Segmentation, Visual Perception, Automated Driving.

Abstract:

Majority of semantic segmentation algorithms operate on a single frame even in the case of videos. In this
work, the goal is to exploit temporal information within the algorithm model for leveraging motion cues and
temporal consistency. We propose two simple high-level architectures based on Recurrent FCN (RFCN) and
Multi-Stream FCN (MSFCN) networks. In case of RFCN, a recurrent network namely LSTM is inserted
between the encoder and decoder. MSFCN combines the encoders of different frames into a fused encoder
via 1x1 channel-wise convolution. We use a ResNet50 network as the baseline encoder and construct three
networks namely MSFCN of order 2 & 3 and RFCN of order 2. MSFCN-3 produces the best results with
an accuracy improvement of 9% and 15% for Highway and New York-like city scenarios in the SYNTHIA-
CVPR’16 dataset using mean IoU metric. MSFCN-3 also produced 11% and 6% for SegTrack V2 and DAVIS
datasets over the baseline FCN network. We also designed an efﬁcient version of MSFCN-2 and RFCN-2
using weight sharing among the two encoders. The efﬁcient MSFCN-2 provided an improvement of 11% and
5% for KITTI and SYNTHIA with negligible increase in computational complexity compared to the baseline
version.

1 Introduction

Semantic segmentation provides complete seman-
tic scene understanding wherein each pixel in an im-
age is assigned a class label.
It has applications in
various ﬁelds including automated driving (Horgan
et al., 2015) (Heimberger et al., 2017), augmented re-
ality and medical image processing. Our work is fo-
cused on semantic segmentation applied to automated
driving which is discussed in detail in the survey pa-
per (Siam et al., 2017a). Recently, this algorithm has
matured in accuracy which is sufﬁcient for commer-
cial deployment due to advancements in deep learn-
ing. Most of the standard architectures make use of
a single frame even when the algorithm is run on a
video sequence. Efﬁcient real-time semantic segmen-
tation architectures are an important aspect for auto-
mated driving (Siam et al., 2018). For automated driv-
ing videos, there is a strong temporal continuity and
constant ego-motion of the camera which can be ex-
ploited within the semantic segmentation model. This
inspired us to explore temporal based video semantic
segmentation. This paper is an extension of our pre-
vious work on RFCN (Siam et al., 2017c).

In this paper, we propose two types of ar-
chitectures namely Recurrent FCN (RFCN) and
Multi-Stream FCN (MSFCN)
inspired by FCN
and Long short-term memory (LSTM) networks.
Multi-Stream Architectures were ﬁrst introduced in
(Simonyan and Zisserman, 2014) in which a two
stream CNN was proposed for action recognition.
They were also successfully used for other appli-
cations like Optical Flow (Ilg et al., 2016), moving
object detection (Siam et al., 2017b) and depth
estimation (Ummenhofer et al., 2016). However,
this has not been explored for semantic segmentation
using consecutive video frames to the best of our
knowledge.
The main motivation is to leverage
temporal continuity in video streams. In RFCN, we
temporally processed FCN encoders using LSTM
In MSFCN architecture, we combine the
network.
encoder of current and previous frames to produce a
new fused encoder of same feature map dimension.
This would enable keeping the same decoder.

The list of contributions include:

• Design of RFCN & MSFCN architectures that ex-
tends semantic segmentation models for videos.

Figure 1: Comparison of different approaches to extend semantic segmentation to videos - a) Frame-level output b) Detect
and track c) Temporal post processing d) Recurrent encoder model and e) Fused multi-stream encoder model.

• Exploration of weight sharing among encoders for

computational efﬁciency.

• Implementation of an end-to-end training method

for spatio-temporal video segmentation.

• Detailed experimental analysis of video seman-
tic segmentation with automated driving datasets
KITTI & SYNTHIA and binary video segmenta-
tion with DAVIS & SegTrack V2 datasets.

The rest of the paper is structured as follows. Sec-
tion 2 discusses different approaches for extending se-
mantic segmentation to videos. Section 3 explains the
different multi-stream architectures designed in this
work. Experimental setup and results are discussed in
section 4. Finally, section 5 provides the conclusion
and future work.

2 Extending Semantic segmentation

to Videos

In this section, we provide motivation for incor-
porating temporal models in automated driving and
explain different high level methods to accomplish
the same. Motion is a dominant cue in automated
driving due to persistent motion of the vehicle on
which the camera is mounted. The objects of interest
in automotive are split into static infrastructure like
road, trafﬁc signs, etc and dynamic objects which are
interacting like vehicles and pedestrians. The main
challenges are posed due to the uncertain behavior
of dynamic objects. Dense optical ﬂow is commonly
used to detect moving objects purely based on motion
cues. Recently, HD maps is becoming a commonly
used cue which enables detection of static infrastruc-
ture which is previously mapped and encoded. In this
work, we explore the usage of temporal continuity
to improve accuracy by implicitly learning motion
cues and tracking. We discuss the various types
of temporal models in Fig 1 which illustrates the
different ways to extend image based segmentation
algorithm to videos.

Single frame baseline: Fig 1 (a) illustrates the typi-
cal way the detector is run every frame independently.
This would be the reference baseline for comparing
accuracy of improvements by other methods.

Detect and Track approach: The premise of this
approach is to leverage the previously obtained
estimate of semantic segmentation as the next frame
has only incrementally changed. This can reduce
the computational complexity signiﬁcantly as a
lighter model can be employed to reﬁne the previous
semantic segmentation output for the current frame.
The high level block diagram is illustrated in Fig1
(b). This approach has been successfully used for
detection of bounding box objects where tracking
could even help when detector fails in certain frames.
However,
it for semantic
segmentation as the output representation is quite
complex and it is challenging to handle appearance
of new regions in the next frame.

is difﬁcult

to model

it

Temporal post processing: The third approach is
to use a post-processing ﬁlter on output estimates
to smooth out the noise.
Probabilistic Graphical
Models (PGM)
like Conditional Random Fields
(CRF) are commonly used to accomplish this. The
block diagram of this method is shown in Fig 1 (c)
where recurrence is built on the output. This step
is computationally complex because the recurrence
operation is on the image dimension which is large.

Recurrent encoder model:
In this approach, the
intermediate feature maps from the encoders are
fed into a recurrent unit. The recurrent unit in the
network can be an RNN, LSTM or a GRU. Then the
resulting features are fed to a decoder which outputs
semantic labels.
In Fig 2a, the ResNet50 encoder
conv5 layer features from consecutive image streams
are passed as temporal features for LSTM network.
While conv4 and conv3 layer features can also be
processed via the LSTM layer, the conv4 and conv3
features from two stream are concatenated followed
by a convolution layer to keep the architecture simple

and memory efﬁcient.

Fused multi-stream encoder model: This method
can be seen as a special case of Recurrent model in
some sense. But the perspective of multi-stream en-
coder will enable the design of new architectures. As
this is the main contribution of this work, we will de-
scribe it in more detail in next section.

3 Proposed CNN Architectures

In this section, we discuss the details of the
proposed multi-stream networks shown in Fig 2b, 2c
& 2d. Multi stream fused architectures (MSFCN-2 &
MSFCN-3) concatenate the output from each encoder
and fuse them via 1x1 channel-wise convolutions
to obtain a fused encoder which is then fed to the
decoder. Recurrent based architecture (RFCN) uses
an LSTM unit to feed the decoder.

Single stream architecture: A fully convolution
network (FCN) shown in Fig 2a is inspired from
(Long et al., 2015) is used as the baseline architec-
ture. We used ResNet50 (Kaiming He, 2015) as the
encoder and conventional up-sampling with skip-
connections to predict pixel wise labels. Initializing
model weights by pre-trained ResNet50 weights,
alleviates over-ﬁtting problems as these weights are
the result of training on a much larger dataset namely
ImageNet.

Multi-stream fused architectures: Multi-Stream
FCN architecture is illustrated in Fig 2b & 2c.
We used multiple ResNet50 encoders to construct
the multi-stream architectures. Consecutive input
frames are processed by multiple ResNet50 encoders
The intermediate feature maps
independently.
obtained at 3 different stages (conv3, conv4 and
conv5) of encoder are concatenated and added to
the up-sampling layers of the decoder. MSFCN-2 is
constructed using 2 encoders while MSFCN-3 uses 3
encoders. A channel-wise 1x1 convolution is applied
to fuse the multiple encoder streams into a single one
of the same dimension. This will enable the usage of
the same decoder.

Multi-stream recurrent architecture: A recurrent
fully convolutional network (RFCN) is designed to
incorporate a recurrent network into a convolutional
encoder-decoder architecture. It is illustrated in Fig
2d. We use the generic recurrent unit LSTM which
can specialize to simpler RNNs and GRUs. LSTM
operates over the encoder of previous N frames and

(a) FCN: Single Encoder Baseline

(b) MSFCN-2: Two stream fusion architecture

(c) MSFCN-3: Three stream fusion architecture

(d) RMSFCN-2: Two stream LSTM architecture

Figure 2: Four types of architectures constructed and tested
(a) Single stream baseline, (b) Two stream
in the paper.
fusion architecture, (c) Three stream fusion architecture and
(d) Two stream LSTM architecture.

produces a ﬁltered encoder of the same dimension
which is then fed to the decoder.

Weight sharing across encoders: The generic form
of multi-stream architectures have different weights
for the different encoders. In Fig 1 (e), the three en-
coders can be different and they have to be recom-
puted each frame. Thus the computational complex-

Fence
37.25
62.60
59.73
59.60
58.53

Car
72.75
88.22
87.6
87.86
87.5

Table 1: Semantic Segmentation Results on SYNTHIA Sequences. We split the test sequences into two parts, one is Highway
for high speeds and the other is City for medium speeds.

Dataset

Highway

City

Architecture
FCN
MSFCN-2
RFCN-2
MSFCN-3
FCN
MSFCN-2
RFCN-2
MSFCN-3

Mean IoU
85.42
93.44
94.17
94.38
73.88
87.77
88.24
88.89

Sky
0.91
0.92
0.93
0.93
0.94
0.87
0.91
0.88

Building
0.67
0.66
0.71
0.69
0.94
0.94
0.92
0.89

Road
0.89
0.94
0.95
0.96
0.72
0.84
0.87
0.86

Sidewalk
0.02
0.28
0.31
0.31
0.78
0.83
0.78
0.74

Fence
0.71
0.85
0.82
0.87
0.34
0.68
0.56
0.64

Vegetation
0.79
0.78
0.83
0.81
0.54
0.64
0.67
0.53

Pole
0.01
0.11
0.13
0.12
0
0
0
0

Car
0.81
0.82
0.87
0.87
0.69
0.8
0.8
0.71

Lane
0.72
0.71
0.7
0.72
0.56
0.8
0.74
0.72

Table 2: Semantic Segmentation Results on KITTI Video Sequence.

Architecture
FCN
MSFCN-2 (shared weights)
RFCN-2 (shared weights)
MSFCN-2
RFCN-2

NumParams
23,668,680
23,715,272
31,847,828
47,302,984
55,435,540

Mean IoU
74.00
85.31
84.19
85.47
83.38

Sky
46.18
47.89
50.20
48.72
44.80

Building
86.50
91.08
93.74
92.29
92.84

Road
80.60
97.58
94.90
96.36
91.77

Sidewalk
69.10
88.02
88.17
90.21
91.67

Vegetation
81.94
92.01
87.73
92.43
86.01

Car
74.35
90.26
87.66
89.27
87.25

Sign
35.11
58.11
55.55
70.47
52.87

Table 3: Semantic Segmentation Results on SYNTHIA Video Sequence.

Architecture
FCN
MSFCN-2 (shared)
RFCN-2 (shared)
MSFCN-2
RFCN-2

Mean IoU
84.08
88.88
88.16
90.01
89.48

Sky
97.2
97.08
96.85
97.34
97.15

Building
92.97
93.14
91.07
95.97
94.01

Road
87.74
93.58
94.17
93.14
93.76

Sidewalk
81.58
86.81
85.62
86.76
85.88

Fence
34.44
47.47
28.29
73.52
76.26

Vegetation
62
75.11
83.2
73.63
70.35

Pole
1.87
46.78
47.28
35.02
39.86

Sign
0.21
0.27
19.12
3.62
8.16

Pedestrain
0.01
32.12
16.89
27.57
28.05

Cyclist
0.33
2.27
3.01
1.11
1.28

Lane
93.08
95.26
93.97
95.35
94.67

ity of the encoder increases by a factor of three. How-
ever, if the weights are shared between the encoders,
there is no need of recomputing it each frame. One
encoder feature extraction per frame sufﬁces and the
fused encoder is computed by combination of previ-
ously computed encoders. This weight sharing ap-
proach drastically brings down the complexity with
negligible additional computation relative to the sin-
gle stream encoder. We demonstrate experimentally
that the weight shared encoder can still provide a sig-
niﬁcant improvement in accuracy.

4 Experiments

We implemented the different proposed multi-
stream architectures using Keras (Chollet et al.,
2015). We used ADAM optimizer as it provided
faster convergence. The maximum order (number
of consecutive frames) used in the training is three
(MSFCN-3) because of limitation of memory needed
for training. Categorical cross-entropy is used as
loss function for the optimizer. Maximum number
of training epochs is set to 30 and early stopping
with a patience of 10 epochs monitoring the gains is
added. Mean class IoU and per-class IoU were used
as accuracy metrics. All input images were resized to
224x384 because of memory requirements needed for
multiple streams.

In this section, we explain the experimental setting
including the datasets used, training algorithm details,
etc and discuss the results.

4.2 Experimental Results and

Discussion

4.1 Experimental Setup

In most datasets,
the frames in a video sequence
are sparsely sampled temporally to have better di-
versity of objects. Thus consecutive video frames
are not provided for training our multi-stream algo-
rithm. Synthetic datasets have no cost for annota-
tion and ground truth annotation is available for all
consecutive frames. Hence we made use of the syn-
thetic autonomous driving dataset SYNTHIA (Ros
et al., 2016) for our experiments. We also made
use of DAVIS2017 (Pont-Tuset et al., 2017) and Seg-
Track V2 (Li et al., 2013) which provides consecutive
frames, they are not automotive datasets but realistic.

We performed four sets of experiments summarized
in four tables. Qualitative results are provided in Fig-
ure 4 for KITTI, Figure 5 for DAVIS and Figure 6 for
SYNTHIA. We also provide a video sequence demon-
strating qualitative results for larger set of frames.

Table 1: Firstly, we wanted to evaluate differ-
ent orders on multi-stream and understand the im-
pact. We also wanted to understand the impact on
high speed and medium speed scenarios. SYNTHIA
dataset was used for this experiment as it had sepa-
ration of various speed sequences and it was also a
relatively larger dataset. Two-stream networks pro-
vided a considerable increase in accuracy compared
to the baseline. MSFCN-2 provided an accuracy im-
provement of 8% for Highway and 14% for City se-

quence. RFCN-2 provided a slightly better accuracy
relative to MSFCN-2. MSFCN-3 provided marginal
improvement over MSFCN-2 and thus we did not ex-
plore higher orders.

Table 2: KITTI is a popular automotive dataset
and thus we used it to perform experiments on this
real life automated driving dataset. We reduced
our experiments to MSFCN-2 and RFCN-2 but we
added shared weight versions of the same. MSFCN-
2 provided an accuracy improvement of 11% and the
shared weight version only lagged behind slightly.

Table 3: We repeated the experiments of the same
networks used in Table 2 on a larger SYNTHIA se-
quence. MSFCN-2 provided an accuracy improve-
ment of 6% in Mean IoU. MSFCN-2 with shared
weights lagged by 1%. RFCN-2 versions had slightly
lesser accuracy compared to its MSFCN-2 counter-
parts with and without weight sharing.

Table 4: As most automotive semantic segmenta-
tion datasets do not provide consecutive frames for
temporal models, we tested in real non-auomotive
datasets namely SegTrack and DAVIS. MSFCN-3
provided an accuracy improvement of 11% in Seg-
Track and 6% in DAVIS. This demonstrates that
the constructed networks provide consistent improve-
ments in various datasets.

With shared weights encoder, increase in computa-
tional complexity is minimal. However, it increases
memory usage and memory bandwidth quite signiﬁ-
cantly due to maintenance of additional encoder fea-
ture maps. It also increases the latency of output by 33
ms for a 30 fps video sequence. From visual inspec-
tion, the improvements are seen mainly in reﬁning the
boundaries and detecting smaller regions. It is likely
due to temporal aggregation of feature maps for each
pixel from past frames.

MSFCN vs FCN: The single frame based FCN
suffers to segment weaker classes like poles and ob-
jects at further distances. Table 3 shows IoU metrics
for weaker classes like Pole, Fence and Sidewalk have
signiﬁcantly improved in case of multi stream net-
works compared to single stream FCN. Fig 4 visually
demonstrates that the temporal encoder modules help
in preserving the small structures and boundaries in
segmentation.

MSFCN-2 vs MSFCN-3: The increase in the
temporal information has clearly increased the perfor-
mance of the semantic segmentation. But this brings
an extra latency for real time applications.

MSFCN-2 vs RFCN: For a multi stream network
the recurrent encoder feature fusion has shown quite
a decent improvement compared to feature concate-
nation technique. It is also observed that the recur-
rent networks helped in preserving the boundaries
of the weaker classes like poles and lane markings.
However, RFCN demands more parameters and takes
longer training time for convergence as shown in Fig
3.

Weight Sharing:

In most of the experiments,
MSFCN-2 with shared weights provided good im-
provement over the baseline and its performance
deﬁcit relative to the generic MSFCN-2 is usually
small around 1%. However, shared weights version
provide a drastic improvement in computational com-
plexity as shown by the number of parameters in Ta-
ble 2. Shared weights MSFCN-2 has a negligible in-
crease in number of parameters and computational
complexity as well whereas the generic MSFCN-2
has double the number of parameters. Thus it is im-
portant to make use of weight sharing.

Figure 3: Accuracy over epochs for SYNTHIA dataset

Table 4: Comparison of Multi-stream network with its base-
line counterpart on DAVIS and SegTrack

Dataset

SegTrack V2

DAVIS

Architecture
FCN
MSFCN-3
FCN
MSFCN-3
BVS(M¨arki et al., 2016)
FCP(Perazzi et al., 2015)

Mean IoU
83.82
94.61
77.64
83.42
66.52
63.14

5 Conclusions

We have chosen a moderately sized based en-
coder namely ResNet50 and we will be experiment-
ing with various sizes like ResNet10, ResNet101, etc
In general, multi-stream provides
for future work.
a signiﬁcant improvement in accuracy for this mod-
erately sized encoder. The improvements might be
larger for smaller networks which are less accurate.

In this paper, we designed and evaluated two
video semantic segmentation architectures namely
Recurrent FCN (RFCN) and Multi-Stream FCN (MS-
FCN) networks to exploit temporal information. We
implemented three architectures namely RFCN-2,
MSFCN-2 and MSFCN-3 using ResNet50 as base en-

Figure 4: Results on KITTI dataset

Figure 5: Results over DAVIS dataset. Left to right: RGB image, Ground Truth, Single encoder (FCN), Two stream encoder
(MSFCN-2), Two stream encoder + LSTM (RFCN-2), Three stream encoder (MSFCN-3).

coder and evaluated on SYNTHIA sequences. We
obtain promising improvements of 9% and 15% for
Highway and New York-like city scenarios over the
baseline network. We also tested MSFCN-3 on real
datasets like SegTrack V2 and DAVIS datasets where
11% and 6% accuracy improvement was achieved, re-
spectively. We also explored weight sharing among

encoders for better efﬁciency and produced an im-
provement of 11% and 5% for KITTI and SYNTHIA
using MSFCN-2 with roughly the same complexity as
the baseline encoder. In future work, we plan to ex-
plore more sophisticated encoder fusion techniques.

Figure 6: Qualitative results of experiments with SYNTHIA dataset. Left to right: RGB image, Single encoder (FCN), Two
stream encoder (MSFCN-2), Ground Truth, Two stream encoder + LSTM (RFCN-2) and Three stream encoder (MSFCN-3).

Siam, M., Valipour, S., Jagersand, M., Ray, N., and Yoga-
mani, S. (2017c). Convolutional gated recurrent net-
works for video semantic segmentation in automated
In International Conference on Intelligent
driving.
Transportation Systems (ITSC), pages 29–36. IEEE.

Simonyan, K. and Zisserman, A. (2014). Two-stream con-
volutional networks for action recognition in videos.
In Advances in neural information processing sys-
tems, pages 568–576.

Ummenhofer, B., Zhou, H., Uhrig, J., Mayer, N., Ilg, E.,
Dosovitskiy, A., and Brox, T. (2016). Demon: Depth
and motion network for learning monocular stereo.
arXiv preprint arXiv:1612.02401.

REFERENCES

Chollet, F. et al. (2015). Keras. https://keras.io.
Heimberger, M., Horgan, J., Hughes, C., McDonald, J., and
Yogamani, S. (2017). Computer vision in automated
parking systems: Design, implementation and chal-
lenges. Image and Vision Computing, 68:88–101.
Horgan, J., Hughes, C., McDonald, J., and Yogamani, S.
(2015). Vision-based driver assistance systems: Sur-
In Intelligent Trans-
vey, taxonomy and advances.
portation Systems (ITSC), 2015 IEEE 18th Interna-
tional Conference on, pages 2032–2039. IEEE.
Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A.,
and Brox, T. (2016). Flownet 2.0: Evolution of optical
ﬂow estimation with deep networks. arXiv preprint
arXiv:1612.01925.

Kaiming He, Xiangyu Zhang, S. R. J. S. (2015). Deep
CoRR,

learning for image recognition.

residual
abs/1512.03385.

Li, F., Kim, T., Humayun, A., Tsai, D., and Rehg, J. M.
(2013). Video segmentation by tracking many ﬁgure-
ground segments. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, pages 2192–
2199.

Long, J., Shelhamer, E., and Darrell, T. (2015). Fully con-
volutional networks for semantic segmentation.
In
Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, pages 3431–3440.
M¨arki, N., Perazzi, F., Wang, O., and Sorkine-Hornung, A.
(2016). Bilateral space video segmentation. In Pro-
ceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 743–751.

Perazzi, F., Wang, O., Gross, M., and Sorkine-Hornung, A.
(2015). Fully connected object proposals for video
In Proceedings of the IEEE Interna-
segmentation.
tional Conference on Computer Vision, pages 3227–
3234.

Pont-Tuset, J., Perazzi, F., Caelles, S., Arbel´aez, P.,
Sorkine-Hornung, A., and Van Gool, L. (2017). The
2017 davis challenge on video object segmentation.
arXiv:1704.00675.

Ros, G., Sellart, L., Materzynska, J., Vazquez, D., and
Lopez, A. M. (2016). The synthia dataset: A large
collection of synthetic images for semantic segmen-
In The IEEE Conference on
tation of urban scenes.
Computer Vision and Pattern Recognition (CVPR).

Siam, M., Elkerdawy, S., Jagersand, M., and Yogamani, S.
(2017a). Deep semantic segmentation for automated
driving: Taxonomy, roadmap and challenges. In In-
ternational Conference on Intelligent Transportation
Systems (ITSC), pages 29–36. IEEE.

Siam, M., Gamal, M., Abdel-Razek, M., Yogamani, S.,
and Jagersand, M. (2018). Rtseg: Real-time seman-
tic segmentation comparative study. arXiv preprint
arXiv:1803.02758.

Siam, M., Mahgoub, H., Zahran, M., Yogamani, S., Jager-
sand, M., and El-Sallab, A. (2017b). Modnet: Mov-
ing object detection network with motion and ap-
arXiv preprint
pearance for autonomous driving.
arXiv:1709.04821.

