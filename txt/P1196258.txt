9
1
0
2
 
b
e
F
 
8
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
8
2
2
3
0
.
2
0
9
1
:
v
i
X
r
a

A Smoother Way to Train Structured Prediction Models

Krishna Pillutla1

Vincent Roulet2

Sham M. Kakade1,2

Zaid Harchaoui2

1 Paul G. Allen School of Computer Science and Engineering, University of Washington
2 Department of Statistics, University of Washington
{pillutla,sham}@cs.washington.edu, {vroulet,zaid}@uw.edu

Abstract

We present a framework to train a structured prediction model by performing smoothing on the in-
ference algorithm it builds upon. Smoothing overcomes the non-smoothness inherent to the maximum
margin structured prediction objective, and paves the way for the use of fast primal gradient-based op-
timization algorithms. We illustrate the proposed framework by developing a novel primal incremental
optimization algorithm for the structural support vector machine. The proposed algorithm blends an
extrapolation scheme for acceleration and an adaptive smoothing scheme and builds upon the stochastic
variance-reduced gradient algorithm. We establish its worst-case global complexity bound and study
several practical variants, including extensions to deep structured prediction. We present experimen-
tal results on two real-world problems, namely named entity recognition and visual object localization.
The experimental results show that the proposed framework allows us to build upon efﬁcient inference
algorithms to develop large-scale optimization algorithms for structured prediction which can achieve
competitive performance on the two real-world problems.

1 Introduction

Consider the optimization problem arising when training maximum margin structured prediction models:

(cid:34)

min
w∈Rd

F (w) :=

1
n

n
(cid:88)

i=1

(cid:35)

f (i)(w) +

(cid:107)w(cid:107)2
2

,

λ
2

(1)

where each f (i) is the structural hinge loss. Max-margin structured prediction was designed to forecast
discrete data structures such as sequences and trees [Taskar et al., 2004, Tsochantaridis et al., 2004].

Batch non-smooth optimization algorithms such as cutting plane methods are appropriate for problems
with small or moderate sample sizes [Tsochantaridis et al., 2004, Joachims et al., 2009]. Stochastic non-
smooth optimization algorithms such as stochastic subgradient methods can tackle problems with large
sample sizes [Ratliff et al., 2007, Shalev-Shwartz et al., 2011]. However, both families of methods achieve
the typical worst-case complexity bounds of non-smooth optimization algorithms and cannot easily leverage
a possible hidden smoothness of the objective.

Furthermore, as signiﬁcant progress is being made on incremental smooth optimization algorithms for
training unstructured prediction models [Lin et al., 2018], we would like to transfer such advances and design
faster optimization algorithms to train structured prediction models. Indeed if each term in the ﬁnite-sum
were L-smooth, incremental optimization algorithms such as MISO [Mairal, 2015], SAG [Le Roux et al.,
2012, Schmidt et al., 2017], SAGA [Defazio et al., 2014], SDCA [Shalev-Shwartz and Zhang, 2013], and

1

SVRG [Johnson and Zhang, 2013] could leverage the ﬁnite-sum structure of the objective (1) and achieve
faster convergence than batch algorithms on large-scale problems.

Incremental optimization algorithms can be further accelerated, either on a case-by-case basis [Shalev-
Shwartz and Zhang, 2014, Frostig et al., 2015, Allen-Zhu, 2017, Defazio, 2016] or using the Catalyst accel-
eration scheme [Lin et al., 2015, 2018], to achieve near-optimal convergence rates [Woodworth and Srebro,
2016]. Accelerated incremental optimization algorithms demonstrate stable and fast convergence behavior
on a wide range of problems, in particular for ill-conditioned ones.

We introduce a general framework that allows us to bring the power of accelerated incremental optimiza-
tion algorithms to the realm of structured prediction problems. To illustrate our framework, we focus on the
problem of training a structural support vector machine (SSVM), and extend the developed algorithms to
deep structured prediction models with nonlinear mappings.

We seek primal optimization algorithms, as opposed to saddle-point or primal-dual optimization algo-
rithms, in order to be able to tackle structured prediction models with afﬁne mappings such as SSVM as
well as deep structured prediction models with nonlinear mappings. We show how to shade off the inherent
non-smoothness of the objective while still being able to rely on efﬁcient inference algorithms.

Smooth Inference Oracles. We introduce a notion of smooth inference oracles that gracefully ﬁts the
framework of black-box ﬁrst-order optimization. While the exp inference oracle reveals the relation-
ship between max-margin and probabilistic structured prediction models, the top-K inference oracle
can be efﬁciently computed using simple modiﬁcations of efﬁcient inference algorithms in many cases
of interest.

Incremental Optimization Algorithms. We present a new algorithm built on top of SVRG, blending an
extrapolation scheme for acceleration and an adaptive smoothing scheme. We establish the worst-
case complexity bounds of the proposed algorithm and extend it to the case of non-linear mappings.
Finally, we demonstrate its effectiveness compared to competing algorithms on two tasks, namely
named entity recognition and visual object localization.

The code is publicly available as a software library called Casimir1. The outline of the paper is as
follows: Sec. 1.1 reviews related work. Sec. 2 discusses smoothing for structured prediction followed by
Sec. 3, which deﬁnes and studies the properties of inference oracles and Sec. 4, which describes the concrete
implementation of these inference oracles in several settings of interest. Then, we switch gears to study
accelerated incremental algorithms in convex case (Sec. 5) and their extensions to deep structured prediction
(Sec. 6). Finally, we evaluate the proposed algorithms on two tasks, namely named entity recognition and
visual object localization in Sec. 7.

1.1 Related Work

Optimization for Structural Support Vector Machines Table 1 gives an overview of different opti-
mization algorithms designed for structural support vector machines. Early works [Taskar et al., 2004,
Tsochantaridis et al., 2004, Joachims et al., 2009, Teo et al., 2009] considered batch dual quadratic opti-
mization (QP) algorithms. The stochastic subgradient method operated directly on the non-smooth primal
formulation [Ratliff et al., 2007, Shalev-Shwartz et al., 2011]. More recently, Lacoste-Julien et al. [2013]
proposed a block coordinate Frank-Wolfe (BCFW) algorithm to optimize the dual formulation of structural

1https://github.com/krishnap25/casimir

2

Table 1: Convergence rates given in terms of the number of calls to various oracles for different optimization algo-
rithms on the learning problem (1) in case of structural support vector machines (4). The rates are speciﬁed in terms
of the target accuracy (cid:15), the number of training examples n, the regularization λ, the size of the label space |Y|, the
max feature norm R = maxi (cid:107)Φ(x(i), y) − Φ(x(i), y(i))(cid:107)2 and (cid:101)R ≥ R (see Remark 28 for explicit form). The rates
are speciﬁed up to constants and factors logarithmic in the problem parameters. The dependence on the initial error is
ignored. * denotes algorithms that make O(1) oracle calls per iteration.

Algo. (exp oracle)

# Oracle calls

Exponentiated
gradient*
[Collins et al., 2008]
Excessive gap
reduction
[Zhang et al., 2014]
Prop. 29*,
entropy smoother

Prop. 30*,
entropy smoother

(n + log |Y|)R2
λ(cid:15)

nR

(cid:114)

(cid:114)

log |Y|
λ(cid:15)

nR2 log|Y|
λ(cid:15)

n +

R2 log|Y|
λ(cid:15)

Algo. (max oracle)

# Oracle calls

BMRM
[Teo et al., 2009]

QP 1-slack
[Joachims et al., 2009]

Stochastic
subgradient*
[Shalev-Shwartz et al., 2011]
Block-Coordinate
Frank-Wolfe*
[Lacoste-Julien et al., 2013]

nR2
λ(cid:15)
nR2
λ(cid:15)

R2
λ(cid:15)

n +

R2
λ(cid:15)

Algo.
(top-K oracle)

# Oracle calls

Prop. 29*,
(cid:96)2
2 smoother

Prop. 30*,
(cid:96)2
2 smoother

(cid:114)

n (cid:101)R2
λ(cid:15)
n + (cid:101)R2
λ(cid:15)

support vector machines; see also Osokin et al. [2016] for variants and extensions. Saddle-point or primal-
dual approaches include the mirror-prox algorithm [Taskar et al., 2006, Cox et al., 2014, He and Harchaoui,
2015]. Palaniappan and Bach [2016] propose an incremental optimization algorithm for saddle-point prob-
lems. However, it is unclear how to extend it to the structured prediction problems considered here. Incre-
mental optimization algorithms for conditional random ﬁelds were proposed by Schmidt et al. [2015]. We
focus here on primal optimization algorithms in order to be able to train structured prediction models with
afﬁne or nonlinear mappings with a uniﬁed approach, and on incremental optimization algorithms which
can scale to large datasets.

Inference The ideas of dynamic programming inference in tree structured graphical models have been
around since the pioneering works of Pearl [1988] and Dawid [1992]. Other techniques emerged based on
graph cuts [Greig et al., 1989, Ishikawa and Geiger, 1998], bipartite matchings [Cheng et al., 1996, Taskar
et al., 2005] and search algorithms [Daum´e III and Marcu, 2005, Lampert et al., 2008, Lewis and Steedman,
2014, He et al., 2017]. For graphical models that admit no such a discrete structure, techniques based on
loopy belief propagation [McEliece et al., 1998, Murphy et al., 1999], linear programming (LP) [Schlesinger,
1976], dual decomposition [Johnson, 2008] and variational inference [Wainwright et al., 2005, Wainwright
and Jordan, 2008] gained popularity.

Top-K Inference Smooth inference oracles with (cid:96)2
2 smoothing echo older heuristics in speech and lan-
guage processing [Jurafsky et al., 2014]. Combinatorial algorithms for top-K inference have been studied
extensively by the graphical models community under the name “M -best MAP”. Seroussi and Golmard
[1994] and Nilsson [1998] ﬁrst considered the problem of ﬁnding the K most probable conﬁgurations in a
tree structured graphical model. Later, Yanover and Weiss [2004] presented the Best Max-Marginal First
algorithm which solves this problem with access only to an oracle that computes max-marginals. We also
use this algorithm in Sec. 4.2. Fromer and Globerson [2009] study top-K inference for LP relaxation, while
Batra [2012] considers the dual problem to exploit graph structure. Flerova et al. [2016] study top-K exten-
sions of the popular A(cid:63) and branch and bound search algorithms in the context of graphical models. Other

3

related approaches include diverse K-best solutions [Batra et al., 2012] and ﬁnding K-most probable modes
[Chen et al., 2013].

Smoothing Inference Smoothing for inference was used to speed up iterative algorithms for continu-
ous relaxations. Johnson [2008] considered smoothing dual decomposition inference using the entropy
smoother, followed by Jojic et al. [2010] and Savchynskyy et al. [2011] who studied its theoretical prop-
erties. Meshi et al. [2012] expand on this study to include (cid:96)2
2 smoothing. Explicitly smoothing discrete
inference algorithms in order to smooth the learning problem was considered by Zhang et al. [2014] and
Song et al. [2014] using the entropy and (cid:96)2
2 smoother was also used by Mar-
tins and Astudillo [2016]. Hazan et al. [2016] consider the approach of blending learning and inference,
instead of using inference algorithms as black-box procedures.

2 smoothers respectively. The (cid:96)2

Related ideas to ours appear in the independent works [Mensch and Blondel, 2018, Niculae et al., 2018].
These works partially overlap with ours, but the papers choose different perspectives, making them com-
plementary to each other. Mensch and Blondel [2018] proceed differently when, e.g., smoothing inference
based on dynamic programming. Moreover, they do not establish complexity bounds for optimization al-
gorithms making calls to the resulting smooth inference oracles. We deﬁne smooth inference oracles in the
context of black-box ﬁrst-order optimization and establish worst-case complexity bounds for incremental
optimization algorithms making calls to these oracles. Indeed we relate the amount of smoothing controlled
by µ to the resulting complexity of the optimization algorithms relying on smooth inference oracles.

End-to-end Training of Structured Prediction The general framework for global training of structured
prediction models was introduced by Bottou and Gallinari [1990] and applied to handwriting recognition
by Bengio et al. [1995] and to document processing by Bottou et al. [1997]. This approach, now called
“deep structured prediction”, was used, e.g., by Collobert et al. [2011] and Belanger and McCallum [2016].

1.2 Notation

Vectors are denoted by bold lowercase characters as w ∈ Rd while matrices are denoted by bold uppercase
characters as A ∈ Rd×n. For a matrix A ∈ Rm×n, deﬁne the norm for α, β ∈ {1, 2, ∞},

For any function f : Rd → R ∪ {+∞}, its convex conjugate f ∗ : Rd → R ∪ {+∞} is deﬁned as

(cid:107)A(cid:107)β,α = max{(cid:104)y, Ax(cid:105) | (cid:107)y(cid:107)α ≤ 1 , (cid:107)x(cid:107)β ≤ 1} .

(2)

f ∗(z) = sup
w∈Rd

{(cid:104)z, w(cid:105) − f (w)} .

A function f : Rd → R is said to be L-smooth with respect to an arbitrary norm (cid:107)·(cid:107) if it is continuously
differentiable and its gradient ∇f is L-Lipschitz with respect to (cid:107)·(cid:107). When left unspeciﬁed, (cid:107)·(cid:107) refers to
(cid:107) · (cid:107)2. Given a continuously differentiable map g : Rd → Rm, its Jacobian ∇g(w) ∈ Rm×d at w ∈ Rd is
deﬁned so that its ijth entry is [∇g(w)]ij = ∂gi(w)/wj where gi is the ith element of g and wj is the jth
element of w. The vector valued function g : Rd → Rm is said to be L-smooth with respect to (cid:107)·(cid:107) if it is
continuously differentiable and its Jacobian ∇g is L-Lipschitz with respect to (cid:107)·(cid:107).

For a vector z ∈ Rm, z(1) ≥ · · · ≥ z(m) refer to its components enumerated in non-increasing order
where ties are broken arbitrarily. Further, we let z[k] = (z(1), · · · , z(k)) ∈ Rk denote the vector of the k
largest components of z. We denote by ∆m−1 the standard probability simplex in Rm. When the dimension
is clear from the context, we shall simply denote it by ∆. Moreover, for a positive integer p, [p] refers to the
set {1, . . . , p}. Lastly, (cid:101)O in the big-O notation hides factors logarithmic in problem parameters.

4

2 Smooth Structured Prediction

Structured prediction aims to search for score functions φ parameterized by w ∈ Rd that model the compat-
ibility of input x ∈ X and output y ∈ Y as φ(x, y; w) through a graphical model. Given a score function
φ(·, ·; w), predictions are made using an inference procedure which, when given an input x, produces the
best output

y∗(x; w) ∈ arg max

φ(x, y; w) .

y∈Y

(3)

We shall return to the score functions and the inference procedures in Sec. 3. First, given such a score
function φ, we deﬁne the structural hinge loss and describe how it can be smoothed.

2.1 Structural Hinge Loss

On a given input-output pair (x, y), the error of prediction of y by the inference procedure with a score func-
tion φ(·, ·; w), is measured by a task loss (cid:96)(cid:0)y, y∗(x; w)(cid:1) such as the Hamming loss. The learning procedure
would then aim to ﬁnd the best parameter w that minimizes the loss on a given dataset of input-output train-
ing examples. However, the resulting problem is piecewise constant and hard to optimize. Instead, Altun
et al. [2003], Taskar et al. [2004], Tsochantaridis et al. [2004] propose to minimize a majorizing surrogate
of the task loss, called the structural hinge loss deﬁned on an input-output pair (x(i), y(i)) as

f (i)(w) = max
y∈Y

(cid:110)

φ(x(i), y; w) + (cid:96)(y(i), y)

(cid:111)

− φ(x(i), y(i); w) = max
y∈Y

ψ(i)(y, w) .

(4)

where ψ(i)(y; w) = φ(x(i), y; w) + (cid:96)(y(i), y) − φ(x(i), y(i); w) is the augmented score function.

This approach, known as max-margin structured prediction, builds upon binary and multi-class support
vector machines [Crammer and Singer, 2001], where the term (cid:96)(y(i), y) inside the maximization in (4)
generalizes the notion of margin. The task loss (cid:96) is assumed to possess appropriate structure so that the
maximization inside (4), known as loss augmented inference, is no harder than the inference problem in (3).
When considering a ﬁxed input-output pair (x(i), y(i)), we drop the index with respect to the sample i and
consider the structural hinge loss as

f (w) = max
y∈Y

ψ(y; w),

(5)

When the map w (cid:55)→ ψ(y; w) is afﬁne, the structural hinge loss f and the objective F from (1) are both
convex - we refer to this case as the structural support vector machine. When w (cid:55)→ ψ(y; w) is a nonlinear
but smooth map, then the structural hinge loss f and the objective F are nonconvex.

2.2 Smoothing Strategy

A convex, non-smooth function h can be smoothed by taking its inﬁmal convolution with a smooth func-
tion [Beck and Teboulle, 2012]. We now recall its dual representation, which Nesterov [2005b] ﬁrst used to
relate the amount of smoothing to optimal complexity bounds.

Deﬁnition 1. For a given convex function h : Rm → R, a smoothing function ω : dom h∗ → R which is
1-strongly convex with respect to (cid:107) · (cid:107)α (for α ∈ {1, 2}), and a parameter µ > 0, deﬁne

hµω(z) = max

u∈dom h∗

{(cid:104)u, z(cid:105) − h∗(u) − µω(u)} .

as the smoothing of h by µω.

5

We now state a classical result showing how the parameter µ controls both the approximation error and the
level of the smoothing. For a proof, see Beck and Teboulle [2012, Thm. 4.1, Lemma 4.2] or Prop. 39 of
Appendix A.

Proposition 2. Consider the setting of Def. 1. The smoothing hµω is continuously differentiable and its
gradient, given by

∇hµω(z) = arg max
u∈dom h∗

{(cid:104)u, z(cid:105) − h∗(u) − µω(u)}

is 1/µ-Lipschitz with respect to (cid:107) · (cid:107)∗
µ1 ≥ µ2 ≥ 0,

α. Moreover, letting hµω ≡ h for µ = 0, the smoothing satisﬁes, for all

(µ1 − µ2)

inf
u∈dom h∗

ω(u) ≤ hµ2ω(z) − hµ1ω(z) ≤ (µ1 − µ2)

sup
u∈dom h∗

ω(u) .

Smoothing the Structural Hinge Loss We rewrite the structural hinge loss as a composition

(cid:40)Rd → Rm

g :

(cid:40)Rm → R

h :

w (cid:55)→ (ψ(y; w))y∈Y ,

z

(cid:55)→ maxi∈[m] zi,

where m = |Y| so that the structural hinge loss reads

(6)

(7)

We smooth the structural hinge loss (7) by simply smoothing the non-smooth max function h as

f (w) = h ◦ g(w) .

fµω = hµω ◦ g.

When g is smooth and Lipschitz continuous, fµω is a smooth approximation of the structural hinge loss,
whose gradient is readily given by the chain-rule. In particular, when g is an afﬁne map g(w) = Aw + b, if
follows that fµω is ((cid:107)A(cid:107)2
β,α/µ)-smooth with respect to (cid:107) · (cid:107)β (cf. Lemma 40 in Appendix A). Furthermore,
for µ1 ≥ µ2 ≥ 0, we have,

(µ1 − µ2) min

ω(u) ≤ fµ2ω(w) − fµ1ω(w) ≤ (µ1 − µ2) max

ω(u) .

u∈∆m−1

u∈∆m−1

In the context of smoothing the max function, we now describe two popular choices for the smoothing
function ω, followed by computational considerations.

2.3 Smoothing Variants

2.3.1 Entropy and (cid:96)2

2 smoothing

When h is the max function, the smoothing operation can be computed analytically for the entropy smoother
and the (cid:96)2

2 smoother, denoted respectively as

−H(u) := (cid:104)u, log u(cid:105)

and

(cid:96)2
2(u) := 1

2 ((cid:107)u(cid:107)2

2 − 1) .

These lead respectively to the log-sum-exp function [Nesterov, 2005b, Lemma 4]

h−µH (z) = µ log

ezi/µ

, ∇h−µH (z) =

(cid:33)

(cid:32) m
(cid:88)

i=1

(cid:35)

(cid:34)

ezi/µ
j=1 ezj /µ

(cid:80)m

,

i=1,...,m

6

and an orthogonal projection onto the simplex,

hµ(cid:96)2

2

(z) = (cid:104)z, proj∆m−1(z/µ)(cid:105) − µ

2 (cid:107) proj∆m−1(z/µ)(cid:107)2 + µ

2 , ∇hµ(cid:96)2

2

(z) = proj∆m−1(z/µ) .

Furthermore, the following holds for all µ1 ≥ µ2 ≥ 0 from Prop. 2:

0 ≤ h−µ1H (z) − h−µ2H (z) ≤ (µ1 − µ2) log m,

and, 0 ≤ hµ1(cid:96)2

(z) − hµ2(cid:96)2

2

2

(z) ≤ 1

2 (µ1 − µ2) .

2.3.2 Top-K Strategy

Though the gradient of the composition fµω = hµω ◦ g can be written using the chain rule, its actual
computation for structured prediction problems involves computing ∇g over all m = |Y| of its components,
which may be intractable. However, in the case of (cid:96)2
2 smoothing, projections onto the simplex are sparse, as
pointed out by the following proposition.

Proposition 3. Consider the Euclidean projection u∗ = arg minu∈∆m−1 (cid:107)u − z/µ(cid:107)2
the simplex, where µ > 0. The projection u∗ has exactly k ∈ [m] non-zeros if and only if

2 of z/µ ∈ Rm onto

where z(1) ≥ · · · ≥ z(m) are the components of z in non-decreasing order and z(m+1) := −∞. In this case,
u∗ is given by

k
(cid:88)

i=1

(cid:0)z(i) − z(k)

(cid:1) < µ ≤

(cid:0)z(i) − z(k+1)

(cid:1) ,

k
(cid:88)

i=1

(cid:26)

u∗
i = max

0, 1
kµ

(cid:0)zi − z(j)

(cid:1) + 1
k

(cid:27)

.

k
(cid:88)

j=1

m
(cid:88)

i=1

(cid:18) zi
µ

+ ρ

= 1 ,

(cid:19)

+

Proof. The projection u∗ satisﬁes u∗

i = (zi/µ + ρ∗)+, where ρ∗ is the unique solution of ρ in the equation

where α+ = max{0, α}. See, e.g., Held et al. [1974] for a proof of this fact. Note that z(i)/µ + ρ∗ ≤ 0
implies that z(j)/µ + ρ∗ ≤ 0 for all j ≥ i. Therefore u∗ has k non-zeros if and only if z(k)/µ + ρ∗ > 0 and
z(k+1)/µ + ρ∗ ≤ 0.

Now suppose that u∗ has exactly k non-zeros, we can then solve (9) to obtain ρ∗ = ϕk(z/µ), which is

deﬁned as

ϕk

(cid:19)

(cid:18) z
µ

:=

−

1
k

1
k

k
(cid:88)

i=1

z(i)
µ

.

Plugging in the value of ρ∗ in z(k)/µ + ρ∗ > 0 gives µ > (cid:80)k
(cid:1).
gives µ ≤ (cid:80)k

(cid:0)z(i) − z(k+1)

i=1

i=1

(cid:0)z(i) − z(k)

(cid:1). Likewise, z(k+1)/µ + ρ∗ ≤ 0

Conversely assume (8) and let (cid:98)ρ = ϕk(z/µ). Eq. (8) can be written as z(k)/µ+(cid:98)ρ > 0 and z(k+1)/µ+(cid:98)ρ ≤
0. Furthermore, we verify that (cid:98)ρ satisﬁes Eq. (9), and so (cid:98)ρ = ρ∗ is its unique root. It follows, therefore, that
the sparsity of u∗ is k.

7

(8)

(9)

(10)

Thus, the projection of z/µ onto the simplex picks out some number Kz/µ of the largest entries of z/µ
- we refer to this as the sparsity of proj∆m−1(z/µ). This fact motivates the top-K strategy: given µ > 0, ﬁx
an integer K a priori and consider as surrogates for hµ(cid:96)2

and ∇hµ(cid:96)2

respectively

2

2

hµ,K(z) := max

u∈∆K−1

(cid:8)(cid:10)z[K], u(cid:11) − µ(cid:96)2

2(u)(cid:9) ,

and,

(cid:101)∇hµ,K(z) := ΩK(z)(cid:62) proj∆K−1

(cid:19)

(cid:18) z[K]
µ

,

where z[K] denotes the vector composed of the K largest entries of z and ΩK : Rm → {0, 1}K×m deﬁnes
their extraction, i.e., ΩK(z) = (e(cid:62)
)(cid:62) ∈ {0, 1}K×m where j1, · · · , jK satisfy zj1 ≥ · · · ≥ zjK
j1
such that z[K] = ΩK(z)z . A surrogate of the (cid:96)2

2 smoothing is then given by

, . . . , e(cid:62)
jK

fµ,K := hµ,K ◦ g ,

and,

(cid:101)∇fµ,K(w) := ∇g(w)(cid:62) (cid:101)∇hµ,K(g(w)) .

(11)

Exactness of Top-K Strategy We say that the top-K strategy is exact at z for µ > 0 when it recovers
(z) = (cid:101)∇hµ,K(z). The next
the ﬁrst order information of hµ(cid:96)2
proposition outlines when this is the case. Note that if the top-K strategy is exact at z for a smoothing
parameter µ > 0 then it will be exact at z for any µ(cid:48) < µ.

(z) = hµ,K(z) and ∇hµ(cid:96)2

, i.e. when hµ(cid:96)2

2

2

2

Proposition 4. The top-K strategy is exact at z for µ > 0 if

µ ≤

(cid:0)z(i) − z(K+1)

(cid:1) .

K
(cid:88)

i=1

(12)

Moreover, for any ﬁxed z ∈ Rm such that the vector z[K+1] = ΩK+1(z)z has at least two unique elements,
the top-K strategy is exact at z for all µ satisfying 0 < µ ≤ z(1) − z(K+1).

Proof. First, we note that the top-K strategy is exact when the sparsity Kz/µ of the projection proj∆m−1(z/µ)
satisﬁes Kz/µ ≤ K. From Prop. 3, the condition that Kz/µ ∈ {1, 2, · · · , K} happens when

µ ∈

K
(cid:91)

(cid:32) k

(cid:88)

k=1

i=1

(cid:0)z(i) − z(k)

(cid:1) ,

(cid:0)z(i) − z(k+1)

(cid:1)

k
(cid:88)

i=1

(cid:35)

(cid:32)

=

0,

K
(cid:88)

i=1

(cid:35)

(cid:0)z(i) − z(K+1)

(cid:1)

,

since the intervals in the union are contiguous. This establishes (12).

The only case when (12) cannot hold for any value of µ > 0 is when the right hand size of (12) is zero.
In the opposite case when z[K+1] has at least two unique components, or equivalently, z(1) − z(K+1) > 0,
the condition 0 < µ ≤ z(1) − z(K+1) implies (12).

If the top-K strategy is exact at g(w) for µ, then

fµ,K(w) = fµ(cid:96)2

(w)

2

and

(cid:101)∇fµ,K(w) = ∇fµ(cid:96)2

(w) ,

2

where the latter follows from the chain rule. When used instead of (cid:96)2
2 smoothing in the algorithms presented
in Sec. 5, the top-K strategy provides a computationally efﬁcient heuristic to smooth the structural hinge
loss. Though we do not have theoretical guarantees using this surrogate, experiments presented in Sec. 7
show its efﬁciency and its robustness to the choice of K.

8

This section studies ﬁrst order oracles used in standard and smoothed structured prediction. We ﬁrst describe
the parameterization of the score functions through graphical models.

3 Inference Oracles

3.1 Score Functions

Structured prediction is deﬁned by the structure of the output y, while input x ∈ X can be arbitrary.
Each output y ∈ Y is composed of p components y1, . . . , yp that are linked through a graphical model
G = (V, E) - the nodes V = {1, · · · , p} represent the components of the output y while the edges E deﬁne
the dependencies between various components. The value of each component yv for v ∈ V represents the
state of the node v and takes values from a ﬁnite set Yv. The set of all output structures Y = Y1 × · · · × Yp
is then ﬁnite yet potentially intractably large.

The structure of the graph (i.e., its edge structure) depends on the task. For the task of sequence labeling,
the graph is a chain, while for the task of parsing, the graph is a tree. On the other hand, the graph used in
image segmentation is a grid.

For a given input x and a score function φ(·, ·; w), the value φ(x, y; w) measures the compatibility of
the output y for the input x. The essential characteristic of the score function is that it decomposes over the
nodes and edges of the graph as

φ(x, y; w) =

φv(x, yv; w) +

φv,v(cid:48)(x, yv, yv(cid:48); w) .

(13)

(cid:88)

v∈V

(cid:88)

(v,v(cid:48))∈E

For a ﬁxed w, each input x deﬁnes a speciﬁc compatibility function φ(x, · ; w). The nature of the
problem and the optimization algorithms we consider hinge upon whether φ is an afﬁne function of w or
not. The two settings studied here are the following:

Pre-deﬁned Feature Map. In this structured prediction framework, a pre-speciﬁed feature map Φ : X ×

Y → Rd is employed and the score φ is then deﬁned as the linear function

φ(x, y; w) = (cid:104)Φ(x, y), w(cid:105) =

(cid:104)Φv(x, yv), w(cid:105) +

(cid:104)Φv,v(cid:48)(x, yv, yv(cid:48)), w(cid:105) .

(14)

(cid:88)

v∈V

(cid:88)

(v,v(cid:48))∈E

Learning the Feature Map. We also consider the setting where the feature map Φ is parameterized by
w0, for example, using a neural network, and is learned from the data. The score function can then be
written as

φ(x, y; w) = (cid:104)Φ(x, y; w0), w1(cid:105)

(15)

where w = (w0, w1) and the scalar product decomposes into nodes and edges as above.

Note that we only need the decomposition of the score function over nodes and edges of the G as in Eq. (13).
In particular, while Eq. (15) is helpful to understand the use of neural networks in structured prediction, the
optimization algorithms developed in Sec. 6 apply to general nonlinear but smooth score functions.

This framework captures both generative probabilistic models such as Hidden Markov Models (HMMs)
that model the joint distribution between x and y as well as discriminative probabilistic models, such as
conditional random ﬁelds [Lafferty et al., 2001] where dependencies among the input variables x do not
need to be explicitly represented. In these cases, the log joint and conditional probabilities respectively play
the role of the score φ.

9

Example 5 (Sequence Tagging). Consider the task of sequence tagging in natural language processing
where each x = (x1, · · · , xp) ∈ X is a sequence of words and y = (y1, · · · , yp) ∈ Y is a sequence of
labels, both of length p. Common examples include part of speech tagging and named entity recognition.
Each word xv in the sequence x comes from a ﬁnite dictionary D, and each tag yv in y takes values from a
ﬁnite set Yv = Ytag. The corresponding graph is simply a linear chain.

The score function measures the compatibility of a sequence y ∈ Y for the input x ∈ X using parame-

ters w = (wunary, wpair) as, for instance,

φ(x, y; w) =

(cid:104)Φunary(xv, yv), wunary(cid:105) +

(cid:104)Φpair(yv, yv+1), wpair(cid:105) ,

p
(cid:88)

v=1

p
(cid:88)

v=0

where, using wunary ∈ R|D||Ytag| and wpair ∈ R|Ytag|2
each v ∈ [p],

as node and edge weights respectively, we deﬁne for

(cid:104)Φunary(xv, yv), wunary(cid:105) =

wunary, x,j I(x = xv) I(j = yv) .

(cid:88)

x∈D, j∈Ytag

The pairwise term (cid:104)Φpair(yv, yv+1), wpair(cid:105) is analogously deﬁned. Here, y0, yp+1 are special “start” and
“stop” symbols respectively. This can be written as a dot product of w with a pre-speciﬁed feature map as
in (14), by deﬁning

Φ(x, y) = (cid:0)

exv ⊗ eyv

(cid:1) ⊕ (cid:0)

eyv ⊗ eyv+1

(cid:1) ,

p
(cid:88)

v=1

p
(cid:88)

v=0

where exv is the unit vector (I(x = xv))x∈D ∈ R|D|, eyv is the unit vector (I(j = yv))j∈Ytag ∈ R|Ytag|, ⊗
denotes the Kronecker product between vectors and ⊕ denotes vector concatenation.

3.2

Inference Oracles

We deﬁne now inference oracles as ﬁrst order oracles in structured prediction. These are used later to
understand the information-based complexity of optimization algorithms.

3.2.1 First Order Oracles in Structured Prediction

A ﬁrst order oracle for a function f : Rd → R is a routine which, given a point w ∈ Rd, returns on output a
value f (w) and a (sub)gradient v ∈ ∂f (w), where ∂f is the Fr´echet (or regular) subdifferential [Rockafellar
and Wets, 2009, Def. 8.3]. We now deﬁne inference oracles as ﬁrst order oracles for the structural hinge
loss f and its smoothed variants fµω. Note that these deﬁnitions are independent of the graphical structure.
However, as we shall see, the graphical structure plays a crucial role in the implementation of the inference
oracles.

Deﬁnition 6. Consider an augmented score function ψ, a level of smoothing µ > 0 and the structural hinge
loss f (w) = maxy∈Y ψ(y; w). For a given w ∈ Rd,

(i)

the max oracle returns f (w) and v ∈ ∂f (w).

(ii)

the exp oracle returns f−µH (w) and ∇f−µH (w).

(iii) the top-K oracle returns fµ,K(w) and (cid:101)∇fµ,K(w) as surrogates for fµ(cid:96)2

(w) and ∇fµ(cid:96)2

(w) respec-

2

2

tively.

10

(a) Non-smooth.

(b) (cid:96)2

2 smoothing.

(c) Entropy smoothing.

Figure 1: Viterbi trellis for a chain graph with p = 4 nodes and 3 labels.

Note that the exp oracle gets its name since it can be written as an expectation over all y, as revealed by the
next lemma, which gives analytical expressions for the gradients returned by the oracles.

Lemma 7. Consider the setting of Def. 6. We have the following:

(i) For any y∗ ∈ arg maxy∈Y ψ(y; w), we have that ∇wψ(y∗; w) ∈ ∂f (w). That is, the max oracle can

be implemented by inference.

(ii) The output of the exp oracle satisﬁes ∇f−µH (w) = (cid:80)

y∈Y Pψ,µ(y; w)∇ψ(y; w), where

Pψ,µ(y; w) =

exp

(cid:16) 1

(cid:17)
µ ψ(y; w)

(cid:80)

y(cid:48)∈Y exp

µ ψ(y(cid:48); w)

(cid:16) 1

(cid:17) .

(iii) The output of the top-K oracle satisﬁes (cid:101)∇fµ,K(w) = (cid:80)K

i=1 u∗
(cid:9) is the set of K largest scoring outputs satisfying

(cid:8)y(1), · · · , y(K)

ψ,µ,i(w)∇ψ(y(i); w) , where YK =

ψ(y(1); w) ≥ · · · ≥ ψ(y(K); w) ≥ max
y∈Y\YK

ψ(y; w) ,

and u∗

ψ,µ = proj∆K−1

(cid:16)(cid:2)ψ(y(1); w), · · · , ψ(y(K); w)(cid:3)(cid:62)(cid:17)

.

Proof. Part (ii) deals with the composition of differentiable functions, and follows from the chain rule.
Part (iii) follows from the deﬁnition in Eq. (11). The proof of Part (i) follows from the chain rule for Fr´echet
subdifferentials of compositions [Rockafellar and Wets, 2009, Theorem 10.6] together with the fact that
by convexity and Danskin’s theorem [Bertsekas, 1999, Proposition B.25], the subdifferential of the max
function is given by ∂h(z) = conv{ei | i ∈ [m] such that zi = h(z)}.

Example 8. Consider the task of sequence tagging from Example 5. The inference problem (3) is a search
over all |Y| = |Ytag|p label sequences. For chain graphs, this is equivalent to searching for the shortest
path in the associated trellis, shown in Fig. 1. An efﬁcient dynamic programming approach called the
Viterbi algorithm [Viterbi, 1967] can solve this problem in space and time polynomial in p and |Ytag|. The
structural hinge loss is non-smooth because a small change in w might lead to a radical change in the best
scoring path shown in Fig. 1.

11

When smoothing f with ω = (cid:96)2

is given by a projection onto the simplex,
which picks out some number Kψ/µ of the highest scoring outputs y ∈ Y or equivalently, Kψ/µ shortest
paths in the Viterbi trellis (Fig. 1b). The top-K oracle then uses the top-K strategy to approximate fµ(cid:96)2
with
fµ,K.

2, the smoothed function fµ(cid:96)2

2

2

On the other hand, with entropy smoothing ω = −H, we get the log-sum-exp function and its gra-
dient is obtained by averaging over paths with weights such that shorter paths have a larger weight (cf.
Lemma 7(ii)). This is visualized in Fig. 1c.

3.2.2 Exp Oracles and Conditional Random Fields

Recall that a Conditional Random Field (CRF) [Lafferty et al., 2001] with augmented score function ψ and
parameters w ∈ Rd is a probabilistic model that assigns to output y ∈ Y the probability

P(y | ψ; w) = exp (ψ(y; w) − Aψ(w)) ,

(16)

where Aψ(w) is known as the log-partition function, a normalizer so that the probabilities sum to one.
Gradient-based maximum likelihood learning algorithms for CRFs require computation of the log-partition
function Aψ(w) and its gradient ∇Aψ(w). Next proposition relates the computational costs of the exp
oracle and the log-partition function.
Proposition 9. The exp oracle for an augmented score function ψ with parameters w ∈ Rd is equivalent
in hardness to computing the log-partition function Aψ(w) and its gradient ∇Aψ(w) for a conditional
random ﬁeld with augmented score function ψ.

Proof. Fix a smoothing parameter µ > 0. Consider a CRF with augmented score function ψ(cid:48)(y; w) =
y∈Y exp (cid:0)µ−1ψ(y; w)(cid:1). The
µ−1ψ(y; w). Its log-partition function Aψ(cid:48)(w) satisﬁes exp(Aψ(cid:48)(w)) = (cid:80)
claim now follows from the bijection f−µH (w) = µ Aψ(cid:48)(w) between f−µH and Aψ(cid:48).

4 Implementation of Inference Oracles

We now turn to the concrete implementation of the inference oracles. This depends crucially on the structure
of the graph G = (V, E). If the graph G is a tree, then the inference oracles can be computed exactly with
efﬁcient procedures, as we shall see in in the Sec. 4.1. When the graph G is not a tree, we study special cases
when speciﬁc discrete structure can be exploited to efﬁciently implement some of the inference oracles in
Sec. 4.2. The results of this section are summarized in Table 2.

Throughout this section, we ﬁx an input-output pair (x(i), y(i)) and consider the augmented score func-
tion ψ(y; w) = φ(x(i), y; w) + (cid:96)(y(i), y) − φ(x(i), y(i); w) it deﬁnes, where the index of the sample is
dropped by convenience. From (13) and the decomposability of the loss, we get that ψ decomposes along
nodes V and edges E of G as:

ψ(y; w) =

ψv(yv; w) +

ψv,v(cid:48)(yv, yv(cid:48); w) .

(17)

(cid:88)

v∈V

(cid:88)

(v,v(cid:48))∈E

When w is clear from the context, we denote ψ(· ; w) by ψ. Likewise for ψv and ψv,v(cid:48).

4.1

Inference Oracles in Trees

We ﬁrst consider algorithms implementing the inference algorithms in trees and examine their computational
complexity.

12

Table 2: Smooth inference oracles, algorithms and complexity. Here, p is the size of each y ∈ Y. The time
complexity is phrased in terms of the time complexity T of the max oracle.

Max oracle
Algo

Max-product

Graph cut

Graph matching

Branch and
Bound search

Algo
Top-K
max-product

BMMF

BMMF

Top-K oracle

Time

Exp oracle
Algo

Time

O(KT log K)

Sum-Product

O(T )

O(pKT )

O(KT )

Intractable

Intractable

Intractable

Top-K search

N/A

4.1.1

Implementation of Inference Oracles

Max Oracle
In tree structured graphical models, the inference problem (3), and thus the max oracle (cf.
Lemma 7(i)) can always be solved exactly in polynomial time by the max-product algorithm [Pearl, 1988],
which uses the technique of dynamic programming [Bellman, 1957]. The Viterbi algorithm (Algo. 1) for
chain graphs from Example 8 is a special case. See Algo. 7 in Appendix B for the max-product algorithm
in full generality.

Top-K Oracle The top-K oracle uses a generalization of the max-product algorithm that we name top-K
max-product algorithm. Following the work of Seroussi and Golmard [1994], it keeps track of the K-best
intermediate structures while the max-product algorithm just tracks the single best intermediate structure.
Formally, the kth largest element from a discrete set S is deﬁned as

(cid:40)

max(k)
x∈S

f (x) =

kth largest element of {f (y) | y ∈ S} k ≤ |S|
k > |S| .
−∞,

We present the algorithm in the simple case of chain structured graphical models in Algo. 2. The top-K
max-product algorithm for general trees is given in Algo. 8 in Appendix B. Note that it requires (cid:101)O(K) times
the time and space of the max oracle.

Exp oracle The relationship of the exp oracle with CRFs (Prop. 9) leads directly to Algo. 3, which is
based on marginal computations from the sum-product algorithm.

Remark 10. We note that clique trees allow the generalization of the algorithms of this section to general
graphs with cycles. However, the construction of a clique tree requires time and space exponential in the
treewidth of the graph.

Example 11. Consider the task of sequence tagging from Example 5. The Viterbi algorithm (Algo. 1)
maintains a table πv(yv), which stores the best length-v preﬁx ending in label yv. One the other hand, the
top-K Viterbi algorithm (Algo. 2) must store in π(k)
v (yv) the score of kth best length-v preﬁx that ends in
yv for each k ∈ [K]. In the vanilla Viterbi algorithm, the entry πv(yv) is updated by looking the previous
column πv−1 following (18). Compare this to update (19) of the top-K Viterbi algorithm. In this case,
the exp oracle is implemented by the forward-backward algorithm, a specialization of the sum-product
algorithm to chain graphs.

13

Algorithm 1 Max-product (Viterbi) algorithm for chain graphs

1: Input: Augmented score function ψ(·, ·; w) deﬁned on a chain graph G.
2: Set π1(y1) ← ψ1(y1) for all y1 ∈ Y1.
3: for v = 2, · · · p do
4:

For all yv ∈ Yv, set

πv(yv) ← ψv(yv) + max

{πv−1(yv−1) + ψv,v−1(yv, yv−1)} .

(18)

yv−1∈Yv−1

Assign to δv(yv) the yv−1 that attains the max above for each yv ∈ Yv.

5:
6: end for
7: Set ψ∗ ← maxyp∈Yp πp(yp) and store the maximizing assignments of yp in y∗
p.
8: for v = p − 1, · · · , 1 do
Set y∗
v ← δv+1(yv+1).
9:
10: end for
11: return ψ∗, y∗ := (y∗

1, · · · , y∗

p).

4.1.2 Complexity of Inference Oracles

The next proposition presents the correctness guarantee and complexity of each of the aforementioned algo-
rithms. Its proof has been placed in Appendix B.

Proposition 12. Consider as inputs an augmented score function ψ(·, ·; w) deﬁned on a tree structured
graph G, an integer K > 0 and a smoothing parameter µ > 0.

(i) The output (ψ∗, y∗) of the max-product algorithm (Algo. 1 for the special case when G is chain struc-
tured Algo. 7 from Appendix B in general) satisﬁes ψ∗ = ψ(y∗; w) = maxy∈Y ψ(y; w). Thus, the pair
(cid:0)ψ∗, ∇ψ(y∗; w)(cid:1) is a correct implementation of the max oracle. It requires time O(p maxv∈V |Yv|2)
and space O(p maxv∈V |Yv|).

(ii) The output {ψ(k), y(k)}K

k=1 of the top-K max-product algorithm (Algo. 2 for the special case when G
is chain structured or Algo. 8 from Appendix B in general) satisﬁes ψ(k) = ψ(y(k)) = max(k)
y∈Y ψ(y).
Thus, the top-K max-product algorithm followed by a projection onto the simplex (Algo. 6 in Ap-
pendix A) is a correct implementation of the top-K oracle. It requires time O(pK log K maxv∈V |Yv|2)
and space O(pK maxv∈V |Yv|).

(iii) Algo. 3 returns (cid:0)f−µH (w), ∇f−µH (w)(cid:1). Thus, Algo. 3 is a correct implementation of the exp oracle.

It requires time O(p maxv∈V |Yv|2) and space O(p maxv∈V |Yv|).

4.2

Inference Oracles in Loopy Graphs

For general loopy graphs with high tree-width, the inference problem (3) is NP-hard [Cooper, 1990]. In
particular cases, graph cut, matching or search algorithms can be used for exact inference in dense loopy
graphs, and therefore, to implement the max oracle as well (cf. Lemma 7(i)). In each of these cases, we ﬁnd
that the top-K oracle can be implemented, but the exp oracle is intractable. Appendix C contains a review
of the algorithms and guarantees referenced in this section.

14

Algorithm 2 Top-K max-product (top-K Viterbi) algorithm for chain graphs

1: Input: Augmented score function ψ(·, ·; w) deﬁned on chain graph G, integer K > 0.
2: For k = 1, · · · , K, set π(k)
1 (y1) ← ψ1(y1) if k = 1 and −∞ otherwise for all y1 ∈ Y1.
3: for v = 2, · · · p and k = 1, · · · , K do
4:

For all yv ∈ Yv, set

v (yv) ← ψv(yv) + max(k)
π(k)

yv−1∈Yv−1,(cid:96)∈[K]

(cid:110)

π((cid:96))
v−1(yv−1) + ψv,v−1(yv, yv−1)

(cid:111)

.

(19)

v (yv) the yv−1, (cid:96) that attain the max(k) above for each yv ∈ Yv.

yp∈Yp,k∈[K] π(k)

p (yp) and store in y(k)

p , (cid:96)(k) respectively the maxi-

Assign to δ(k)

v (yv), κ(k)

5:
6: end for
7: For k = 1, · · · , K, set ψ(k) ← max(k)

mizing assignments of yp, k.

8: for v = p − 1, · · · 1 and k = 1, · · · , K do

Set y(k)

v ← δ((cid:96)(k))

v+1

(cid:0)y(k)
v+1

(cid:1) and (cid:96)(k) ← κ((cid:96)(k))
v+1

(cid:0)y(k)
v+1

(cid:1).

9:
10: end for
11: return

(cid:110)

ψ(k), y(k) := (y(k)

1 , · · · , y(k)
p )

(cid:111)K

.

k=1

4.2.1

Inference Oracles using Max-Marginals

We now deﬁne a max-marginal, which is a constrained maximum of the augmented score ψ.

Deﬁnition 13. The max-marginal of ψ relative to a variable yv is deﬁned, for j ∈ Yv as

ψv;j(w) := max

ψ(y; w) .

y∈Y : yv=j

In cases where exact inference is tractable using graph cut or matching algorithms, it is possible to extract
max-marginals as well. This, as we shall see next, allows the implementation of the max and top-K oracles.
When the augmented score function ψ is unambiguous, i.e., no two distinct y1, y2 ∈ Y have the same
augmented score, the output y∗(w) is unique can be decoded from the max-marginals as (see Pearl [1988],
Dawid [1992] or Thm. 45 in Appendix C)

(20)

(21)

y∗
v(w) = arg max

ψv;j(w) .

j∈Yv

If one has access to an algorithm M that can compute max-marginals, the top-K oracle is also eas-
ily implemented via the Best Max-Marginal First (BMMF) algorithm of Yanover and Weiss [2004]. This
algorithm requires computations of 2K sets of max-marginals, where a set of max-marginals refers to max-
marginals for all yv in y. Therefore, the BMMF algorithm followed by a projection onto the simplex
(Algo. 6 in Appendix A) is a correct implementation of the top-K oracle at a computational cost of 2K sets
of max-marginals. The BMMF algorithm and its guarantee are recalled in Appendix C.1 for completeness.

Graph Cut and Matching Inference Kolmogorov and Zabin [2004] showed that submodular energy
functions [Lov´asz, 1983] over binary variables can be efﬁciently minimized exactly via a minimum cut
algorithm. For a class of alignment problems, e.g., Taskar et al. [2005], inference amounts to ﬁnding the
best bipartite matching. In both these cases, max-marginals can be computed exactly and efﬁciently by

15

Algorithm 3 Entropy smoothed max-product algorithm

1: Input: Augmented score function ψ(·, ·; w) deﬁned on tree structured graph G, µ > 0.
2: Compute the log-partition function and marginals using the sum-product algorithm (Algo. 9 in Ap-

pendix B)

Aψ/µ, {Pv for v ∈ V}, {Pv,v(cid:48) for (v, v(cid:48)) ∈ E} ← SUMPRODUCT

(cid:16) 1

µ ψ(· ; w), G

(cid:17)

.

3: Set f−µH (w) ← µAψ/µ and

∇f−µH (w) ←

Pv(yv)∇ψv(yv; w) +

Pv,v(cid:48)(yv, yv(cid:48))∇ψv,v(cid:48)(yv; w) .

(cid:88)

(cid:88)

v∈V

yv∈Yv

(cid:88)

(cid:88)

(cid:88)

(v,v(cid:48))∈E

yv∈Yv

yv(cid:48) ∈Yv(cid:48)

4: return f−µH (w), ∇f−µH (w).

combinatorial algorithms. This gives us a way to implement the max and top-K oracles. However, in
both settings, computing the log-partition function Aψ(w) of a CRF with score ψ is known to be #P-
complete [Jerrum and Sinclair, 1993]. Prop. 9 immediately extends this result to the exp oracle. This
discussion is summarized by the following proposition, whose proof is provided in Appendix C.4.

Proposition 14. Consider as inputs an augmented score function ψ(·, ·; w), an integer K > 0 and a smooth-
ing parameter µ > 0. Further, suppose that ψ is unambiguous, that is, ψ(y(cid:48); w) (cid:54)= ψ(y(cid:48)(cid:48); w) for all distinct
y(cid:48), y(cid:48)(cid:48) ∈ Y. Consider one of the two settings:

(A)

the output space Yv = {0, 1} for each v ∈ V, and the function −ψ is submodular (see Appendix C.2
and, in particular, (72) for the precise deﬁnition), or,

(B)

the augmented score corresponds to an alignment task where the inference problem (3) corresponds to
a maximum weight bipartite matching (see Appendix C.3 for a precise deﬁnition).

In these cases, we have the following:

(i) The max oracle can be implemented at a computational complexity of O(p) minimum cut computations

in Case (A), and in time O(p3) in Case (B).

(ii) The top-K oracle can be implemented at a computational complexity of O(pK) minimum cut compu-

tations in Case (A), and in time O(p3K) in Case (B).

(iii) The exp oracle is #P-complete in both cases.

Prop. 14 is loose in that the max oracle can be implemented with just one minimum cut computation

instead of p in in Case (A) [Kolmogorov and Zabin, 2004].

4.2.2 Branch and Bound Search

Max oracles implemented via search algorithms can often be extended to implement the top-K oracle. We
restrict our attention to best-ﬁrst branch and bound search such as the celebrated Efﬁcient Subwindow Search
[Lampert et al., 2008].

16

Branch and bound methods partition the search space into disjoint subsets, while keeping an upper
bound (cid:98)ψ : X × 2Y → R, on the maximal augmented score for each of the subsets (cid:98)Y ⊆ Y. Using a best-ﬁrst
strategy, promising parts of the search space are explored ﬁrst. Parts of the search space whose upper bound
indicates that they cannot contain the maximum do not have to be examined further.

The top-K oracle is implemented by simply continuing the search procedure until K outputs have been
produced - see Algo. 13 in Appendix C.5. Both the max oracle and the top-K oracle can degenerate to an
exhaustive search in the worst case, so we do not have sharp running time guarantees. However, we have
the following correctness guarantee.

Proposition 15. Consider an augmented score function ψ(·, ·; w), an integer K > 0 and a smoothing
parameter µ > 0. Suppose the upper bound function (cid:98)ψ(·, ·; w) : X × 2Y → R satisﬁes the following
properties:

(a) (cid:98)ψ( (cid:98)Y; w) is ﬁnite for every (cid:98)Y ⊆ Y,

(b) (cid:98)ψ( (cid:98)Y; w) ≥ maxy∈ (cid:98)Y ψ(y; w) for all (cid:98)Y ⊆ Y, and,

(c) (cid:98)ψ({y}; w) = ψ(y; w) for every y ∈ Y.

Then, we have the following:

(i) Algo. 13 with K = 1 is a correct implementation of the max oracle.

(ii) Algo. 13 followed by a projection onto the simplex (Algo. 6 in Appendix A) is a correct implementation

of the top-K oracle.

See Appendix C.5 for a proof. The discrete structure that allows inference via branch and bound search
cannot be leveraged to implement the exp oracle.

5 The Casimir Algorithm

We come back to the optimization problem (1) with f (i) deﬁned in (7). We assume in this section that the
mappings g(i) deﬁned in (6) are afﬁne. Problem (1) now reads

(cid:34)

min
w∈Rd

F (w) :=

1
n

n
(cid:88)

i=1

h(A(i)w + b(i)) +

(cid:107)w(cid:107)2
2

.

(cid:35)

λ
2

For a single input (n = 1), the problem reads

min
w∈Rd

h(Aw + b) +

λ
2

(cid:107)w(cid:107)2
2.

(22)

(23)

where h is a simple non-smooth convex function and λ ≥ 0. Nesterov [2005b,a] ﬁrst analyzed such set-
ting: while the problem suffers from its non-smoothness, fast methods can be developed by considering
smooth approximations of the objectives. We combine this idea with the Catalyst acceleration scheme [Lin
et al., 2018] to accelerate a linearly convergent smooth optimization algorithm resulting in a scheme called
Casimir.

17

5.1 Casimir: Catalyst with Smoothing

The Catalyst [Lin et al., 2018] approach minimizes regularized objectives centered around the current it-
erate. The algorithm proceeds by computing approximate proximal point steps instead of the classical
(sub)-gradient steps. A proximal point step from a point w with step-size κ−1 is deﬁned as the minimizer
of

min
z∈Rm

F (z) +

κ
2

(cid:107)z − w(cid:107)2
2,

(24)

which can also be seen as a gradient step on the Moreau envelope of F - see Lin et al. [2018] for a detailed
discussion. While solving the subproblem (24) might be as hard as the original problem we only require
an approximate solution returned by a given optimization method M. The Catalyst approach is then an
inexact accelerated proximal point algorithm that carefully mixes approximate proximal point steps with
the extrapolation scheme of Nesterov [1983]. The Casimir scheme extends this approach to non-smooth
optimization.

For the overall method to be efﬁcient, subproblems (24) must have a low complexity. That is, there must
exist an optimization algorithm M that solves them linearly. For the Casimir approach to be able to handle
non-smooth objectives, it means that we need not only to regularize the objective but also to smooth it. To
this end we deﬁne

Fµω(w) :=

hµω(A(i)w + b(i)) +

(cid:107)w(cid:107)2
2

λ
2

1
n

n
(cid:88)

i=1

as a smooth approximation of the objective F , and,

Fµω,κ(w; z) :=

hµω(A(i)w + b(i)) +

(cid:107)w(cid:107)2

2 +

(cid:107)w − z(cid:107)2
2

λ
2

κ
2

1
n

n
(cid:88)

i=1

a smooth and regularized approximation of the objective centered around a given point z ∈ Rd. While the
original Catalyst algorithm considered a ﬁxed regularization term κ, we vary κ and µ along the iterations.
This enables us to get adaptive smoothing strategies.

The overall method is presented in Algo. 4. We ﬁrst analyze in Sec. 5.2 its complexity for a generic lin-
early convergent algorithm M. Thereafter, in Sec. 5.3, we compute the total complexity with SVRG [John-
son and Zhang, 2013] as M. Before that, we specify two practical aspects of the implementation: a proper
stopping criterion (26) and a good initialization of subproblems (Line 4).

Stopping Criterion Following Lin et al. [2018], we solve subproblem k in Line 4 to a degree of relative ac-
curacy speciﬁed by δk ∈ [0, 1). In view of the (λ + κk)-strong convexity of Fµkω,κk (· ; zk−1), the functional
gap can be controlled by the norm of the gradient, precisely it can be seen that (cid:107)∇Fµkω,κk ( (cid:98)w; zk−1)(cid:107)2
2 ≤
(λ + κk)δkκk(cid:107) (cid:98)w − zk−1(cid:107)2

2 is a sufﬁcient condition for the stopping criterion (26).
A practical alternate stopping criterion proposed by Lin et al. [2018] is to ﬁx an iteration budget Tbudget
and run the inner solver M for exactly Tbudget steps. We do not have a theoretical analysis for this scheme
but ﬁnd that it works well in experiments.

Warm Start of Subproblems Rate of convergence of ﬁrst order optimization algorithms depends on the
initialization and we must warm start M at an appropriate initial point in order to obtain the best convergence
of subproblem (25) in Line 4 of Algo. 4. We advocate the use of the prox center zk−1 in iteration k as the
warm start strategy. We also experiment with other warm start strategies in Section 7.

18

Algorithm 4 The Casimir algorithm

1: Input: Smoothable objective F of the form (23) with h simple, smoothing function ω, linearly con-
vergent algorithm M, non-negative and non-increasing sequence of smoothing parameters (µk)k≥1,
positive and non-decreasing sequence of regularization parameters (κk)k≥1, non-negative sequence of
relative target accuracies (δk)k≥1 and, initial point w0, α0 ∈ (0, 1), time horizon K.

Using M with zk−1 as the starting point, ﬁnd wk ≈ arg minw∈Rd Fµkω,κk (w; zk−1) where

Fµkω,κk (w; zk−1) :=

hµkω(A(i)w + b(i)) +

(cid:107)w(cid:107)2

2 +

(cid:107)w − zk−1(cid:107)2
2

(25)

λ
2

κk
2

1
n

n
(cid:88)

i=1

Fµkω,κk (wk; zk−1) − min
w

Fµkω,κk (w; zk−1) ≤ δkκk

2 (cid:107)wk − zk−1(cid:107)2

2

k(κk+1 + λ) = (1 − αk)α2
α2

k−1(κk + λ) + αkλ.

zk = wk + βk(wk − wk−1),

βk =

αk−1(1 − αk−1)(κk + λ)
k−1(κk + λ) + αk(κk+1 + λ)

α2

.

(26)

(27)

(28)

(29)

2: Initialize: z0 = w0.
3: for k = 1 to K do
4:

such that

5:

Solve for αk ≥ 0

6:

Set

where

7: end for
8: return wK.

5.2 Convergence Analysis of Casimir

We ﬁrst state the outer loop complexity results of Algo. 4 for any generic linearly convergent algorithm M in
Sec. 5.2.1, prove it in Sec. 5.2.2. Then, we consider the complexity of each inner optimization problem (25)
in Sec. 5.2.3 based on properties of M.

5.2.1 Outer Loop Complexity Results

The following theorem states the convergence of the algorithm for general choice of parameters, where we
denote w∗ ∈ arg minw∈Rd F (w) and F ∗ = F (w∗).

Theorem 16. Consider Problem (22). Suppose δk ∈ [0, 1) for all k ≥ 1, the sequence (µk)k≥1 is non-
negative and non-increasing, and the sequence (κk)k≥1 is strictly positive and non-decreasing. Further,
suppose the smoothing function ω : dom h∗ → R satisﬁes −Dω ≤ ω(u) ≤ 0 for all u ∈ dom h∗ and that

19

0 ≥ λ/(λ + κ1). Then, the sequence (αk)k≥0 generated by Algo. 4 satisﬁes 0 < αk ≤ αk−1 < 1 for all

α2
k ≥ 1. Furthermore, the sequence (wk)k≥0 of iterates generated by Algo. 4 satisﬁes

F (wk) − F ∗ ≤

∆0 + µkDω +

(µj−1 − (1 − δj)µj) Dω ,

(30)

Ak−1
0
Bk
1

k
(cid:88)

j=1

Ak−1
j
Bk
j

where Aj
µ0 := 2µ1.

i := (cid:81)j

r=i(1 − αr), Bj

i := (cid:81)j

r=i(1 − δr), ∆0 := F (w0) − F ∗ + (κ1+λ)α2

0−λα0

2(1−α0)

(cid:107)w0 − w∗(cid:107)2

2 and

Before giving its proof, we present various parameters strategies as corollaries. Table 3 summarizes the
parameter settings and the rates obtained for each setting. Overall, the target accuracies δk are chosen such
that Bk
j is a constant and the parameters µk and κk are then carefully chosen for an almost parameter-free
algorithm with the right rate of convergence. Proofs of these corollaries are provided in Appendix D.2.

The ﬁrst corollary considers the strongly convex case (λ > 0) with constant smoothing µk = µ, as-
suming that (cid:15) is known a priori. We note that this is, up to constants, the same complexity obtained by the
original Catalyst scheme on a ﬁxed smooth approximation Fµω with µ = O((cid:15)Dω).

Corollary 17. Consider the setting of Thm. 16. Let q = λ/(λ + κ). Suppose λ > 0 and µk = µ, κk = κ,
√
for all k ≥ 1. Choose α0 =

q) . Then, we have,

q and, δk =

q/(2 −

√

√

F (wk) − F ∗ ≤

µDω + 2

1 −

(F (w0) − F ∗) .

√
√

q
q

3 −
1 −

(cid:18)

(cid:19)k

√

q
2

Next, we consider the strongly convex case where the target accuracy (cid:15) is not known in advance. We
let smoothing parameters (µk)k≥0 decrease over time to obtain an adaptive smoothing scheme that gives
progressively better surrogates of the original objective.

Corollary 18. Consider the setting of Thm. 16. Let q = λ/(λ + κ) and η = 1 −
κk = κ, for all k ≥ 1. Choose α0 =

q and, the sequences (µk)k≥1 and (δk)k≥1 as

√

√

q/2. Suppose λ > 0 and

µk = µηk/2 ,

and,

δk =

√

q
√

,

q

2 −

where µ > 0 is any constant. Then, we have,

F (wk) − F ∗ ≤ ηk/2

(cid:20)
2 (F (w0) − F ∗) +

µDω
√
q
1 −

(cid:18)

√

2 −

q +

√

(cid:19)(cid:21)

.

q
√
η

1 −

The next two corollaries consider the unregularized problem, i.e., λ = 0 with constant and adaptive smooth-
ing respectively.

Corollary 19. Consider the setting of Thm. 16. Suppose µk = µ, κk = κ, for all k ≥ 1 and λ = 0. Choose
α0 = (

5 − 1)/2 and δk = (k + 1)−2 . Then, we have,

√

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2
2

+ µDω

1 +

(cid:16)

8
(k + 2)2

(cid:17)

(cid:18)

12
k + 2

+

30
(k + 2)2

(cid:19)

.

κ
2

20

Table 3: Summary of outer iteration complexity for Algorithm 4 for different parameter settings. We use shorthand
∆F0 := F (w0) − F ∗ and ∆0 = (cid:107)w0 − w∗(cid:107)2. Absolute constants are omitted from the rates.

Cor.

λ > 0

κk

17

18

19

20

Yes

Yes

No

No

κ

κ

κ

µk

µ
√

q
2

µ

(cid:16)

µ

1 −

(cid:17)k/2

κ k

µ/k

δk
√
q
√

q

2−
√

q
√

2−
q
k−2

k−2

α0
√

√

q

q

c

c

F (wk) − F ∗
√

(cid:16)

1 −
√

q
2

(cid:16)

1 −

(cid:17)k

q
2
(cid:17)k/2 (cid:16)

∆F0 + µD
√
1−

q
∆F0 + µD
√
1−
(cid:1) + µD

(cid:17)

q

(cid:0)∆F0 + κ∆2
1
0
k2
log k
k (∆F0 + κ∆2
0 + µD)

Remark

q = λ
λ+κ

q = λ
λ+κ
√
5 − 1)/2

c = (

√

c = (

5 − 1)/2

Corollary 20. Consider the setting of Thm. 16 with λ = 0. Choose α0 = (
non-negative constants κ, µ, deﬁne sequences (κk)k≥1, (µk)k≥1, (δk)k≥1 as

√

5 − 1)/2, and for some

κk = κ k , µk =

and,

δk =

µ
k

1
(k + 1)2 .

Then, for k ≥ 2, we have,

F (wk) − F ∗ ≤

log(k + 1)
k + 1

(cid:0)2(F (w0) − F ∗) + κ(cid:107)w0 − w∗(cid:107)2

2 + 27µDω

(cid:1) .

For the ﬁrst iteration (i.e., k = 1), this bound is off by a constant factor 1/ log 2.

5.2.2 Outer Loop Convergence Analysis

We now prove Thm. 16. The proof technique largely follows that of Lin et al. [2018], with the added
challenges of accounting for smoothing and varying Moreau-Yosida regularization. We ﬁrst analyze the
sequence (αk)k≥0. The proof follows from the algebra of Eq. (27) and has been given in Appendix D.1.

Lemma 21. Given a positive, non-decreasing sequence (κk)k≥1 and λ ≥ 0, consider the sequence (αk)k≥0
deﬁned by (27), where α0 ∈ (0, 1) such that α2
0 ≥ λ/(λ + κ1). Then, we have for every k ≥ 1 that
0 < αk ≤ αk−1 and, α2

k ≥ λ/(λ + κk+1) .

We now characterize the effect of an approximate proximal point step on Fµω.
Lemma 22. Suppose (cid:98)w ∈ Rd satisﬁes Fµω,κ( (cid:98)w; z) − minw∈Rd Fµω,κ(w; z) ≤ (cid:98)(cid:15) for some (cid:98)(cid:15) > 0. Then, for
all 0 < θ < 1 and all w ∈ Rd, we have,

Fµω( (cid:98)w) +

(cid:107) (cid:98)w − z(cid:107)2

2 +

(1 − θ)(cid:107)w − (cid:98)w(cid:107)2

2 ≤ Fµω(w) +

(cid:107)w − z(cid:107)2

κ
2

2 + (cid:98)(cid:15)
θ

.

(31)

κ + λ
2

κ
2

Proof. Let (cid:98)F ∗ = minw∈Rd Fµω,κ(w; z). Let (cid:98)w∗ be the unique minimizer of Fµω,κ(· ; z). We have, from
(κ + λ)-strong convexity of Fµω,κ(· ; z),

Fµω,κ(w; z) ≥ (cid:98)F ∗ +

κ + λ
2

(cid:107)w − (cid:98)w∗(cid:107)2

2

≥ (Fµω,κ( (cid:98)w; z) − (cid:98)(cid:15)) +

(1 − θ)(cid:107)w − (cid:98)w(cid:107)2

2 −

κ + λ
2

κ + λ
2

(cid:18) 1
θ

(cid:19)

− 1

(cid:107) (cid:98)w − (cid:98)w∗(cid:107)2
2 ,

21

where we used that (cid:98)(cid:15) was sub-optimality of (cid:98)w and Lemma 51 from Appendix D.7. From (κ + λ)-strong
convexity of Fµω,κ(·; z), we have,

κ + λ
2

(cid:107) (cid:98)w − (cid:98)w∗(cid:107)2

2 ≤ Fµω,κ( (cid:98)w; z) − (cid:98)F ∗ ≤ (cid:98)(cid:15) ,

Since (1/θ − 1) is non-negative, we can plug this into the previous statement to get,

Fµω,κ(w; z) ≥ Fµω,κ( (cid:98)w; z) +

κ + λ
2

(1 − θ)(cid:107)w − (cid:98)w(cid:107)2

2 − (cid:98)(cid:15)
θ

.

Substituting the deﬁnition of Fµω,κ(· ; z) from (25) completes the proof.

We now deﬁne a few auxiliary sequences integral to the proof. Deﬁne sequences (vk)k≥0, (γk)k≥0,

(ηk)k≥0, and (rk)k≥1 as

One might recognize γk and vk from their resemblance to counterparts from the proof of Nesterov [2013].
Now, we claim some properties of these sequences.

Claim 23. For the sequences deﬁned in (32)-(37), we have,

v0 = w0

vk = wk−1 +

(wk − wk−1) , k ≥ 1 ,

1
αk−1

γ0 =

(κ1 + λ)α2
1 − α0
γk = (κk + λ)α2

ηk =

αkγk
γk+1 + αkγk

0 − λα0

,

k−1 , k ≥ 1 ,

, k ≥ 0 ,

rk = αk−1w∗ + (1 − αk−1)wk−1 , k ≥ 1 .

γk =

(κk+1 + λ)α2
1 − αk
γk+1 = (1 − αk)γk + λαk , k ≥ 0 ,

k − λαk

, k ≥ 0 ,

ηk =

αkγk
γk + αkλ

, k ≥ 0

zk = ηkvk + (1 − ηk)wk , k ≥ 0 , .

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

Proof. Eq. (38) follows from plugging in (27) in (35) for k ≥ 1, while for k = 0, it is true by deﬁnition.
Eq. (39) follows from plugging (35) in (38). Eq. (40) follows from (39) and (36). Lastly, to show (41), we
shall show instead that (41) is equivalent to the update (28) for zk. We have,

zk = ηkvk + (1 − ηk)wk

(cid:18)

(33)
= ηk

(cid:19)

(wk − wk−1)

+ (1 − ηk)wk

= wk + ηk

− 1

(wk − wk−1) .

1
αk−1

wk−1 +
(cid:18) 1

αk−1

(cid:19)

22

Now,

ηk

(cid:18) 1

αk−1

(cid:19) (36)
=

− 1

·

αkγk
γk+1 + αkγk

1 − αk−1
αk−1
αk(κk + λ)α2
k(κk+1 + λ) + αk(κk + λ)α2
α2

k−1

k−1

(35)
=

·

1 − αk−1
αk−1

(29)
= βk ,

completing the proof.

Claim 24. The sequence (rk)k≥1 from (37) satisﬁes

(cid:107)rk − zk−1(cid:107)2

2 ≤ αk−1(αk−1 − ηk−1)(cid:107)wk−1 − w∗(cid:107)2

2 + αk−1ηk−1(cid:107)vk−1 − w∗(cid:107)2
2 .

(42)

Proof. Notice that ηk
get,

(40)
= αk ·

γk

γk+αkλ ≤ αk. Hence, using convexity of the squared Euclidean norm, we

(cid:107)rk − zk−1(cid:107)2
2

(41)
= (cid:107)(αk−1 − ηk−1)(w∗ − wk−1) + ηk−1(w∗ − vk−1)(cid:107)2
2

= α2

k−1

1 −

(w∗ − wk−1) +

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:18)

(cid:19)

ηk−1
αk−1
(cid:19)
ηk−1
αk−1

(∗)
≤ α2

k−1

1 −

(cid:107)wk−1 − w∗(cid:107)2

2 + α2

k−1

(cid:107)vk−1 − w∗(cid:107)2
2

ηk−1
αk−1

ηk−1
αk−1

(cid:13)
2
(cid:13)
(w∗ − vk−1)
(cid:13)
(cid:13)
2

= αk−1(αk−1 − ηk−1)(cid:107)wk−1 − w∗(cid:107)2

2 + αk−1ηk−1(cid:107)vk−1 − w∗(cid:107)2
2 .

For all µ ≥ µ(cid:48) ≥ 0, we know from Prop. 2 that

0 ≤ Fµω(w) − Fµ(cid:48)ω(w) ≤ (µ − µ(cid:48))Dω .

We now deﬁne the sequence (Sk)k≥0 to play the role of a potential function here.

S0 = (1 − α0)(F (w0) − F (w∗)) +

α0κ1η0
2
Sk = (1 − αk)(Fµkω(wk) − Fµkω(w∗)) +

(cid:107)w0 − w∗(cid:107)2
2 ,
αkκk+1ηk
2

(cid:107)vk − w∗(cid:107)2

2 , k ≥ 1 .

(43)

(44)

We are now ready to analyze the effect of one outer loop. This lemma is the crux of the analysis.

Lemma 25. Suppose Fµkω,κk (wk; z) − minw∈Rd Fµkω,κk (w; z) ≤ (cid:15)k for some (cid:15)k > 0. The following
statement holds for all 0 < θk < 1:

Sk
1 − αk

≤ Sk−1 + (µk−1 − µk)Dω +

(cid:107)wk − zk−1(cid:107)2

2 +

(cid:107)vk − w∗(cid:107)2
2 ,

(45)

(cid:15)k
θk

−

κk
2

κk+1ηkαkθk
2(1 − αk)

where we set µ0 := 2µ1.

Proof. For ease of notation, let Fk := Fµkω, and D := Dω. By λ-strong convexity of Fµkω, we have,

Fk(rk) ≤ αk−1Fk(w∗) + (1 − αk−1)Fk(wk−1) −

λαk−1(1 − αk−1)
2

(cid:107)wk−1 − w∗(cid:107)2
2 .

(46)

23

We now invoke Lemma 22 on the function Fµkω,κk (·; zk−1) with (cid:98)(cid:15) = (cid:15)k and w = rk to get,

Fk(wk) +

(cid:107)wk − zk−1(cid:107)2

2 +

(1 − θk)(cid:107)rk − wk(cid:107)2

2 ≤ Fk(rk) +

(cid:107)rk − zk−1(cid:107)2

2 +

.

(47)

κk
2

κk + λ
2

κk
2

(cid:15)k
θk

We shall separately manipulate the left and right hand sides of (47), starting with the right hand side, which
we call R. We have, using (46) and (42),

R ≤ (1 − αk−1)Fk(wk−1) + αk−1Fk(w∗) −

+

κk
2

αk−1(αk−1 − ηk−1)(cid:107)wk−1 − w∗(cid:107)2

λαk−1(1 − αk−1)
2
κkαk−1ηk−1
2

2 +

(cid:107)wk−1 − w∗(cid:107)2
2

(cid:107)vk−1 − w∗(cid:107)2

2 +

(cid:15)k
θk

.

We notice now that

αk−1 − ηk−1

(40)
= αk−1 −

αk−1γk−1
γk + αk−1γk−1
(cid:18) γk − γk−1(1 − αk−1)
γk + αk−1γk−1

(cid:19)

= αk−1

(39)
=

(38)
=

=

α2

k−1λ
γk−1 + αk−1λ

(κk + λ)α2
λ
κk

(1 − αk−1) ,

α2

k−1λ(1 − αk−1)

k−1 − λαk−1 + (1 − αk−1)αk−1λ

and hence the terms containing (cid:107)wk−1 − w∗(cid:107)2

2 cancel out. Therefore, we get,

R ≤ (1 − αk−1)Fk(wk−1) + αk−1Fk(w∗) +

κkαk−1ηk−1
2

(cid:107)vk−1 − w∗(cid:107)2

2 +

(cid:15)k
θk

.

To move on to the left hand side, we note that

αkηk

(40)
=

α2
kγk
γk + αkλ

(35),(38)
=

kα2
α2
(κk+1+λ)α2
1−αk

k−1(κk + λ)
k−λαk

+ αkλ

=

(1 − αk)(κk + λ)α2
(κk+1 + λ)α2

k − λα2
k

k−1α2
k

= (1 − αk)α2

k−1

κk + λ
κk+1

.

Therefore,

Fk(wk) − Fk(w∗) +

κk + λ
2

k−1(cid:107)vk − w∗(cid:107)2
α2
2

(44),(50)
=

Sk
1 − αk

.

Using rk − wk

(33)
= αk−1(w∗ − vk), we simplify the left hand side of (47), which we call L, as

L = Fk(wk) − Fk(w∗) +

(cid:107)wk − zk−1(cid:107)2

2 +

(1 − θk)α2

k−1(cid:107)vk − w∗(cid:107)2
2

κk + λ
2

(51)
=

Sk
1 − αk

+ Fk(w∗) +

(cid:107)wk − zk−1(cid:107)2

2 −

κk+1αkηkθk
2(1 − αk)

(cid:107)vk − w∗(cid:107)2
2 .

κk
2
κk
2

24

(48)

(49)

(50)

(51)

(52)

In view of (49) and (52), we can simplify (47) as

Sk
1 − αk

+

κk
2

(cid:107)wk − zk−1(cid:107)2

2 −

κk+1αkηkθk
2(1 − αk)

≤ (1 − αk−1) (Fk(wk−1) − Fk(w∗)) +

(cid:107)vk − w∗(cid:107)2
2
κkαk−1ηk−1
2

(cid:107)vk−1 − w∗(cid:107)2

2 +

(cid:15)k
θk

.

We make a distinction for k ≥ 2 and k = 1 here. For k ≥ 2, the condition that µk−1 ≥ µk gives us,

Fk(wk−1) − Fk(w∗)

(43)
≤ Fk−1(wk−1) − Fk−1(w∗) + (µk−1 − µk)D .

(53)

(54)

The right hand side of (53) can now be upper bounded by

(1 − αk−1)(µk−1 − µk)D + Sk−1 +

(cid:15)k
θk

,

and noting that 1 − αk−1 ≤ 1 yields (45) for k ≥ 2.

For k = 1, we note that Sk−1(= S0) is deﬁned in terms of F (w). So we have,

F1(w0) − F1(w∗) ≤ F (w0) − F (w∗) + µ1D = F (w0) − F (w∗) + (µ0 − µ1)D ,

because we used µ0 = 2µ1. This is of the same form as (54). Therefore, (45) holds for k = 1 as well.

We now prove Thm. 16.

Proof of Thm. 16. We continue to use shorthand Fk := Fµkω, and D := Dω. We now apply Lemma 25. In
order to satisfy the supposition of Lemma 25 that wk is (cid:15)k-suboptimal, we make the choice (cid:15)k = δkκk
2 (cid:107)wk −
zk−1(cid:107)2

2 (cf. (26)). Plugging this in and setting θk = δk < 1, we get from (45),

Sk
1 − αk

−

κk+1ηkαkδk
2(1 − αk)

(cid:107)vk − w∗(cid:107)2

2 ≤ Sk−1 + (µk−1 − µk)D .

The left hand side simpliﬁes to Sk (1 − δk)/(1 − αk) + δk(Fk(wk) − Fk(w∗)). Note that Fk(wk) −
(43)
≥ F (wk) − F (w∗) − µkD ≥ −µkD. From this, noting that αk ∈ (0, 1) for all k, we get,
Fk(w∗)

Sk

(cid:19)

(cid:18) 1 − δk
1 − αk

≤ Sk−1 + δkµkD + (µk−1 − µk)D ,

or equivalently,

Sk ≤

(cid:19)

(cid:18) 1 − αk
1 − δk

(cid:19)

(cid:18) 1 − αk
1 − δk

Sk−1 +

(µk−1 − (1 − δk)µk)D .

Unrolling the recursion for Sk, we now have,

Sk ≤





k
(cid:89)

j=1

1 − αj
1 − δj



 S0 +

k
(cid:88)

k
(cid:89)





j=1

i=j

1 − αi
1 − δi



 (µj−1 − (1 − δj)µj)D .

(55)

25

Now, we need to reason about S0 and Sk to complete the proof. To this end, consider η0:

(36)
=

η0

α0γ0
γ1 + α0γ0

α0γ0

(34)
=

=

(κ1 + λ)α2

0 + α0
1−α0

α0γ0(1 − α0)

(κ1 + λ)α2

0 − λα2
0

(cid:1)

(cid:0)(κ1 + λ)α2
0 − λα0
γ0
κ1α0

= (1 − α0)

.

(56)

With this, we can expand out S0 to get

S0

(44)
= (1 − α0) (F (w0) − F (w∗)) +
γ0
2

F (w0) − F ∗ +

(56)
= (1 − α0)

(cid:16)

(cid:107)w0 − w∗(cid:107)2
2

α0κ1η0
2
(cid:107)w0 − w∗(cid:107)2
2

(cid:17)

.

Lastly, we reason about Sk for k ≥ 1 as,

(44)
≥ (1 − αk) (Fk(wk) − Fk(w∗))

(43)
≥ (1 − αk) (F (wk) − F (w∗) − µkD) .

Sk

Plugging this into the left hand side of (55) completes the proof.

5.2.3

Inner Loop Complexity

Consider a class FL,λ of functions deﬁned as

(cid:110)

FL,λ =

f : Rd → R such that f is L-smooth and λ-strongly convex

(cid:111)

.

We now formally deﬁne a linearly convergent algorithm on this class of functions.

Deﬁnition 26. A ﬁrst order algorithm M is said to be linearly convergent with parameters C : R+ × R+ →
R+ and τ : R+ × R+ → (0, 1) if the following holds: for all L ≥ λ > 0, and every f ∈ FL,λ and w0 ∈ Rd,
M started at w0 generates a sequence (wk)k≥0 that satisﬁes:

Ef (wk) − f ∗ ≤ C(L, λ) (1 − τ (L, λ))k (f (w0) − f ∗) ,

(57)

where f ∗ := minw∈Rd f (w) and the expectation is over the randomness of M.

The parameter τ determines the rate of convergence of the algorithm. For instance, batch gradient
descent is a deterministic linearly convergent algorithm with τ (L, λ)−1 = L/λ and incremental algorithms
such as SVRG and SAGA satisfy requirement (57) with τ (L, λ)−1 = c(n+ L/λ) for some universal constant
c.

The warm start strategy in step k of Algo. 4 is to initialize M at the prox center zk−1. The next
proposition, due to Lin et al. [2018, Cor. 16] bounds the expected number of iterations of M required to
ensure that wk satisﬁes (26). Its proof has been given in Appendix D.3 for completeness.

Proposition 27. Consider Fµω,κ(· ; z) deﬁned in Eq. (25), and a linearly convergent algorithm M with
parameters C, τ . Let δ ∈ [0, 1). Suppose Fµω is Lµω-smooth and λ-strongly convex. Then the expected
number of iterations E[ (cid:98)T ] of M when started at z in order to obtain (cid:98)w ∈ Rd that satisﬁes

Fµω,κ( (cid:98)w; z) − min
w

Fµω,κ(w; z) ≤ δκ

2 (cid:107)w − z(cid:107)2

2

26

Table 4: Summary of global complexity of Casimir-SVRG, i.e., Algorithm 4 with SVRG as the inner solver for various
parameter settings. We show E[N ], the expected total number of SVRG iterations required to obtain an accuracy (cid:15), up
to constants and factors logarithmic in problem parameters. We denote ∆F0 := F (w0) − F ∗ and ∆0 = (cid:107)w0 − w∗(cid:107)2.
Constants D, A are short for Dω, Aω (see (58)).

Prop.

λ > 0

µk

κk

δk
(cid:113) λ(cid:15)n
AD

c(cid:48)

(cid:15)/D AD/(cid:15)n − λ

λ

29

30

31

32

Yes

Yes

No

No

µck

(cid:15)/D

µ/k

AD/(cid:15)n

1/k2

n

κ0 k

1/k2

E[N ]
(cid:113) ADn
λ(cid:15)

n +

n + A
λ(cid:15)
(cid:113) ∆F0

∆F0+µD
µ

√

(cid:15) +
(cid:16)

ADn∆0
(cid:15)
(cid:17)

(cid:98)∆0
(cid:15)

n + A
µκ0

Remark

ﬁx (cid:15) in advance

c, c(cid:48) < 1 are universal constants

ﬁx (cid:15) in advance

(cid:98)∆0 = ∆F0 + κ0

2 ∆2

0 + µD

is upper bounded by

E[ (cid:98)T ] ≤

1
τ (Lµω + κ, λ + κ)

log

(cid:18) 8C(Lµω + κ, λ + κ)
τ (Lµω + κ, λ + κ)

·

Lµω + κ
κδ

(cid:19)

+ 1 .

5.3 Casimir with SVRG

We now choose SVRG [Johnson and Zhang, 2013] to be the linearly convergent algorithm M, resulting
in an algorithm called Casimir-SVRG. The rest of this section analyzes the total iteration complexity of
Casimir-SVRG to solve Problem (22). The proofs of the results from this section are calculations stemming
from combining the outer loop complexity from Cor. 17 to 20 with the inner loop complexity from Prop. 27,
and are relegated to Appendix D.4. Table 4 summarizes the results of this section.

Recall that if ω is 1-strongly convex with respect to (cid:107) · (cid:107)α, then hµω(Aw + b) is Lµω-smooth with
2,α/µ. Therefore, the complexity of solving problem (22) will depend

respect to (cid:107) · (cid:107)2, where Lµω = (cid:107)A(cid:107)2
on

Aω := max
i=1,··· ,n

(cid:107)A(i)(cid:107)2

2,α .

(58)

Remark 28. We have that (cid:107)A(cid:107)2,2 = (cid:107)A(cid:107)2 is the spectral norm of A and (cid:107)A(cid:107)2,1 = maxj (cid:107)aj(cid:107)2 is the
largest row norm, where aj is the jth row of A. Moreover, we have that (cid:107)A(cid:107)2,2 ≥ (cid:107)A(cid:107)2,1.

We start with the strongly convex case with constant smoothing.

Proposition 29. Consider the setting of Thm. 16 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as the inner
solver with parameters: µk = µ = (cid:15)/10Dω, κk = k chosen as

√

√

√

q = λ/(λ + κ), α0 =
F (w) − F (w∗) ≤ (cid:15) is bounded in expectation as
(cid:32)

q, and δ =

q/(2 −

q). Then, the number of iterations N to obtain w such that

κ =

(cid:40) A

µn − λ , if A
λ , otherwise

µn > 4λ

,

E[N ] ≤ (cid:101)O

n +

(cid:114)

(cid:33)

.

AωDωn
λ(cid:15)

27

Here, we note that κ was chosen to minimize the total complexity (cf. Lin et al. [2018]). This bound is
known to be tight, up to logarithmic factors [Woodworth and Srebro, 2016]. Next, we turn to the strongly
convex case with decreasing smoothing.

Proposition 30. Consider the setting of Thm. 16. Suppose λ > 0 and κk = κ, for all k ≥ 1 and that α0,
(µk)k≥1 and (δk)k≥1 are chosen as in Cor. 18, with q = λ/(λ+κ) and η = 1−
q/2. If we run Algo. 4 with
SVRG as the inner solver with these parameters, the number of iterations N of SVRG required to obtain w
such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

√

(cid:18)

E[N ] ≤ (cid:101)O

n +

(cid:18)

Aω
µ(λ + κ)(cid:15)

F (w0) − F ∗ +

µDω
√
1 −

q

(cid:19)(cid:19)

.

Unlike the previous case, there is no obvious choice of κ, such as to minimize the global complexity. Notice
that we do not get the accelerated rate of Prop. 29. We now turn to the case when λ = 0 and µk = µ for all
k.

Proposition 31. Consider the setting of Thm. 16 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as the inner
5 − 1)/2, δk = 1/(k + 1)2, and κk = κ =
solver with parameters: µk = µ = (cid:15)/20Dω, α0 = (
Aω/µ(n + 1). Then, the number of iterations N to get a point w such that F (w) − F ∗ ≤ (cid:15) is bounded in
expectation as

√

(cid:32)

(cid:114)

E[N ] ≤ (cid:101)O

n

F (w0) − F ∗
(cid:15)

(cid:112)

+

AωDωn

(cid:107)w0 − w∗(cid:107)2
(cid:15)

(cid:33)

.

This rate is tight up to log factors [Woodworth and Srebro, 2016]. Lastly, we consider the non-strongly
convex case (λ = 0) together with decreasing smoothing. As with Prop. 30, we do not obtain an accelerated
rate here.

Proposition 32. Consider the setting of Thm. 16. Suppose λ = 0 and that α0, (µk)k≥1,(κk)k≥1 and (δk)k≥1
are chosen as in Cor. 20. If we run Algo. 4 with SVRG as the inner solver with these parameters, the number
of iterations N of SVRG required to obtain w such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

E[N ] ≤ (cid:101)O

(cid:0)F (w0) − F ∗ + κ(cid:107)w0 − w∗(cid:107)2

2 + µD(cid:1)

(cid:18) 1
(cid:15)

(cid:18)

n +

(cid:19)(cid:19)

.

Aω
µκ

6 Extension to Non-Convex Optimization

Let us now turn to the optimization problem (1) in full generality where the mappings g(i) deﬁned in (6) are
not constrained to be afﬁne:

(cid:34)

min
w∈Rd

F (w) :=

1
n

n
(cid:88)

i=1

h(g(i)(w)) +

(cid:107)w(cid:107)2
2

,

λ
2

(cid:35)

(59)

where h is a simple, non-smooth, convex function, and each g(i) is a continuously differentiable nonlinear
map and λ ≥ 0.

We describe the prox-linear algorithm in Sec. 6.1, followed by the convergence guarantee in Sec. 6.2

and the total complexity of using Casimir-SVRG together with the prox-linear algorithm in Sec. 6.3.

28

Algorithm 5 (Inexact) Prox-linear algorithm: outer loop

1: Input: Smoothable objective F of the form (59) with h simple, step length η, tolerances ((cid:15)k)k≥1, initial

point w0, non-smooth convex optimization algorithm, M, time horizon K

2: for k = 1 to K do
3:

Using M with wk−1 as the starting point, ﬁnd

(cid:20)

(cid:98)wk ≈ arg min

w

Fη(w; wk−1) :=

1
n

n
(cid:88)

i=1

h(cid:0)g(i)(wk−1) + ∇g(i)(wk−1)(w − wk−1)(cid:1)

+

(cid:107)w(cid:107)2

2 +

(cid:107)w − wk−1(cid:107)2
2 ,

λ
2

1
2η

(cid:21)

(61)

(62)

such that

4:
5: end for
6: return wK.

Fη( (cid:98)wk; wk−1) − min
w∈Rd

Fη(w; wk−1) ≤ (cid:15)k .

Set wk = (cid:98)wk if F ( (cid:98)wk) ≤ F (wk−1), else set wk = wk−1.

6.1 The Prox-Linear Algorithm

The exact prox-linear algorithm of Burke [1985] generalizes the proximal gradient algorithm (see e.g.,
Nesterov [2013]) to compositions of convex functions with smooth mappings such as (59). When given a
function f = h ◦ g, the prox-linear algorithm deﬁnes a local convex approximation f (· ; wk) about some
point w ∈ Rd by linearizing the smooth map g as f (w; wk) := h(g(wk) + ∇g(wk)(w − wk)) . With this,
it builds a convex model F (· ; wk) of F about wk as

F (w; wk) :=

h(g(i)(wk) + ∇g(i)(wk)(w − wk)) +

(cid:107)w(cid:107)2
2 .

λ
2

Given a step length η > 0, each iteration of the exact prox-linear algorithm then minimizes the local convex
model plus a proximal term as

wk+1 = arg min

Fη(w; wk) := F (w; wk) +

(60)

(cid:21)

(cid:107)w − wk(cid:107)2
2

.

1
2η

Following Drusvyatskiy and Paquette [2018], we consider an inexact prox-linear algorithm, which ap-
proximately solves (60) using an iterative algorithm. In particular, since the function to be minimized in
(60) is precisely of the form (23), we employ the fast convex solvers developed in the previous section as
subroutines. Concretely, the prox-linear outer loop is displayed in Algo. 5. We now delve into details about
the algorithm and convergence guarantees.

6.1.1

Inexactness Criterion

As in Section 5, we must be prudent in choosing when to terminate the inner optimization (Line 3 of Algo. 5).
Function value suboptimality is used as the inexactness criterion here. In particular, for some speciﬁed

1
n

n
(cid:88)

i=1

(cid:20)

w∈Rd

29

tolerance (cid:15)k > 0, iteration k of the prox-linear algorithm accepts a solution (cid:98)w that satisﬁes Fη( (cid:98)wk; wk−1) −
minw Fη(w; wk−1) ≤ (cid:15)k.

Implementation In view of the (λ + η−1)-strong convexity of Fη(· ; wk−1), it sufﬁces to ensure that
(λ + η−1)(cid:107)v(cid:107)2

2 ≤ (cid:15)k for a subgradient v ∈ ∂Fη( (cid:98)wk; wk−1).

Fixed Iteration Budget As in the convex case, we consider as a practical alternative a ﬁxed iteration
budget Tbudget and optimize Fη(· ; wk) for exactly Tbudget iterations of M. Again, we do not have a
theoretical analysis for this scheme but ﬁnd it to be effective in practice.

6.1.2 Warm Start of Subproblems

As in the convex case, we advocate the use of the prox center wk−1 to warm start the inner optimization
problem in iteration k (Line 3 of Algo. 5).

6.2 Convergence analysis of the prox-linear algorithm

We now state the assumptions and the convergence guarantee of the prox-linear algorithm.

6.2.1 Assumptions

For the prox-linear algorithm to work, the only requirement is that we minimize an upper model. The
assumption below makes this concrete.

Assumption 33. The map g(i) is continuously differentiable everywhere for each i ∈ [n]. Moreover, there
exists a constant L > 0 such that for all w, w(cid:48) ∈ Rd and i ∈ [n], it holds that

h(cid:0)g(i)(w(cid:48))(cid:1) ≤ h(cid:0)g(i)(w) + ∇g(i)(w)(w(cid:48) − w)(cid:1) +

L
2

(cid:107)w(cid:48) − w(cid:107)2
2 .

When h is G-Lipschitz and each g(i) is (cid:101)L-smooth, both with respect to (cid:107) · (cid:107)2, then Assumption 33 holds
with L = G(cid:101)L [Drusvyatskiy and Paquette, 2018]. In the case of structured prediction, Assumption 33 holds
when the augmented score ψ as a function of w is L-smooth. The next lemma makes this precise and its
proof is in Appendix D.5.

Lemma 34. Consider the structural hinge loss f (w) = maxy∈Y ψ(y; w) = h ◦ g(w) where h, g are as
deﬁned in (6). If the mapping w (cid:55)→ ψ(y; w) is L-smooth with respect to (cid:107) · (cid:107)2 for all y ∈ Y, then it holds
for all w, z ∈ Rd that

|h(g(w + z)) − h(g(w) + ∇g(w)z)| ≤

6.2.2 Convergence Guarantee

Convergence is measured via the norm of the prox-gradient (cid:37)η(·), also known as the gradient mapping,
deﬁned as

(cid:37)η(w) =

w − arg min

Fη(z; w)

.

(63)

L
2

(cid:107)z(cid:107)2
2 .

(cid:33)

(cid:32)

1
η

z∈Rd

30

The measure of stationarity (cid:107)(cid:37)η(w)(cid:107) turns out to be related to the norm of the gradient of the Moreau
envelope of F under certain conditions - see Drusvyatskiy and Paquette [2018, Section 4] for a discussion.
In particular, a point w with small (cid:107)(cid:37)η(w)(cid:107) means that w is close to w(cid:48) = arg minz∈Rd Fη(z; w), which
is nearly stationary for F .

The prox-linear outer loop shown in Algo. 5 has the following convergence guarantee [Drusvyatskiy and

Paquette, 2018, Thm. 5.2].

Theorem 35. Consider F of the form (59) that satisﬁes Assumption 33, a step length 0 < η ≤ 1/L and a
non-negative sequence ((cid:15)k)k≥1. With these inputs, Algo. 5 produces a sequence (wk)k≥0 that satisﬁes

min
k=0,··· ,K−1

(cid:107)(cid:37)η(wk)(cid:107)2

2 ≤

F (w0) − F ∗ +

(cid:32)

2
ηK

(cid:33)

(cid:15)k

,

K
(cid:88)

k=1

where F ∗ = inf w∈Rd F (w). In addition, we have that the sequence (F (wk))k≥0 is non-increasing.

Remark 36. Algo. 5 accepts an update only if it improves the function value (Line 4). A variant of Algo. 5
which always accepts the update has a guarantee identical to that of Thm. 35, but the sequence (F (wk))k≥0
would not guaranteed to be non-increasing.

6.3 Prox-Linear with Casimir-SVRG

We now analyze the total complexity of minimizing the ﬁnite sum problem (59) with Casimir-SVRG to
approximately solve the subproblems of Algo. 5.

For the algorithm to converge, the map w (cid:55)→ g(i)(wk) + ∇g(i)(wk)(w − wk) must be Lipschitz for

each i and each iterate wk. To be precise, we assume that

Aω := max
i=1,··· ,n

sup
w∈Rd

(cid:107)∇g(i)(w)(cid:107)2

2,α

(64)

is ﬁnite, where ω, the smoothing function, is 1-strongly convex with respect to (cid:107) · (cid:107)α. When g(i) is the linear
map w (cid:55)→ A(i)w, this reduces to (58).

We choose the tolerance (cid:15)k to decrease as 1/k. When using the Casimir-SVRG algorithm with constant
smoothing (Prop. 29) as the inner solver, this method effectively smooths the kth prox-linear subproblem as
1/k. We have the following rate of convergence for this method, which is proved in Appendix D.6.

Proposition 37. Consider the setting of Thm. 35. Suppose the sequence ((cid:15)k)k≥1 satisﬁes (cid:15)k = (cid:15)0/k for
some (cid:15)0 > 0 and that the subproblem of Line 3 of Algo. 5 is solved using Casimir-SVRG with the settings
of Prop. 29. Then, total number of SVRG iterations N required to produce a w such that (cid:107)(cid:37)η(w)(cid:107)2 ≤ (cid:15) is
bounded as

E[N ] ≤ (cid:101)O





n
η(cid:15)2 (F (w0) − F ∗ + (cid:15)0) +

(cid:113)

AωDωn(cid:15)−1
0
η(cid:15)3



(F (w0) − F ∗ + (cid:15)0)3/2

 .

Remark 38. When an estimate or an upper bound B on F (w0) − F ∗, one could set (cid:15)0 = O(B). This is
true, for instance, in the structured prediction task where F ∗ ≥ 0 whenever the task loss (cid:96) is non-negative
(cf. (4)).

31

7 Experiments

In this section, we study the experimental behavior of the proposed algorithms on two structured prediction
tasks, namely named entity recognition and visual object localization. Recall that given training examples
{(x(i), y(i))}n

i=1, we wish to solve the problem:

(cid:34)

min
w∈Rd

F (w) :=

(cid:107)w(cid:107)2

2 +

λ
2

1
n

n
(cid:88)

i=1

(cid:110)

max
y(cid:48)∈Y(x(i))

φ(x(i), y(cid:48); w) + (cid:96)(y(i), y(cid:48))

− φ(x(i), y(i); w)

.

(cid:111)

(cid:35)

Note that we now allow the output space Y(x) to depend on the instance x - the analysis from the previous
sections applies to this setting as well. In all the plots, the shaded region represents one standard deviation
over ten random runs.

We compare the performance of various optimization algorithms based on the number of calls to a
smooth inference oracle. Moreover, following literature for algorithms based on SVRG [Schmidt et al.,
2017, Lin et al., 2018], we exclude the cost of computing the full gradients.

The results must be interpreted keeping in mind that the running time of all inference oracles is not
the same. These choices were motivated by the following reasons, which may not be appropriate in all
contexts. The ultimate yardstick to benchmark the performance of optimization algorithms is wall clock
time. However, this depends heavily on implementation, system and ambient system conditions. With
regards to the differing running times of different oracles, we ﬁnd that a small value of K, e.g., 5 sufﬁces,
so that our highly optimized implementations of the top-K oracle incurs negligible running time penalties
over the max oracle. Moreover, the computations of the batch gradient have been neglected as they are
embarrassingly parallel.

The outline of the rest of this section is as follows. First, we describe the datasets and task description
in Sec. 7.1, followed by methods compared in Sec. 7.2 and their hyperparameter settings in Sec. 7.3. Lastly,
Sec. 7.4 presents the experimental studies.

7.1 Dataset and Task Description

For each of the tasks, we specify below the following: (a) the dataset {(x(i), y(i))}n
i=1, (b) the output
structure Y, (c) the loss function (cid:96), (d) the score function φ(x, y; w), (e) implementation of inference
oracles, and lastly, (f) the evaluation metric used to assess the quality of predictions.

7.1.1 CoNLL 2003: Named Entity Recognition

Named entities are phrases that contain the names of persons, organization, locations, etc, and the task is
to predict the label (tag) of each entity. Named entity recognition can be formulated as a sequence tagging
problem where the set Ytag of individual tags is of size 7.

Each datapoint x is a sequence of words x = (x1, · · · , xp), and the label y = (y1, · · · , yp) ∈ Y(x) is a

sequence of the same length, where each yi ∈ Ytag is a tag.

Loss Function The loss function is the Hamming Loss (cid:96)(y, y(cid:48)) = (cid:80)
i

I(yi (cid:54)= y(cid:48)

i).

Score Function We use a chain graph to represent this task. In other words, the observation-label depen-
dencies are encoded as a Markov chain of order 1 to enable efﬁcient inference using the Viterbi algorithm.
We only consider the case of linear score φ(x, y; w) = (cid:104)w, Φ(x, y)(cid:105) for this task. The feature map Φ here

32

is very similar to that given in Example 5. Following Tkachenko and Simanovsky [2012], we use local con-
text Ψi(x) around ith word xi of x. In particular, deﬁne Ψi(x) = exi−2 ⊗ · · · ⊗ exi+2, where ⊗ denotes the
Kronecker product between column vectors, and exi denotes a one hot encoding of word xi, concatenated
with the one hot encoding of its the part of speech tag and syntactic chunk tag which are provided with the
input. Now, we can deﬁne the feature map Φ as

Φ(x, y) =

Ψv(x) ⊗ eyv

⊕

eyv ⊗ eyv+1

,

(cid:35)

(cid:34) p

(cid:88)

i=0

(cid:35)

(cid:34) p

(cid:88)

v=1

where ey ∈ R|Ytag| is a one hot-encoding of y ∈ Ytag, and ⊕ denotes vector concatenation.

Inference We use the Viterbi algorithm as the max oracle (Algo. 1) and top-K Viterbi algorithm (Algo. 2)
for the top-K oracle.

Dataset The dataset used was CoNLL 2003 [Tjong Kim Sang and De Meulder, 2003], which contains
about ∼ 20K sentences.

Evaluation Metric We follow the ofﬁcial CoNLL metric: the F1 measure excluding the ‘O’ tags. In
addition, we report the objective function value measured on the training set (“train loss”).

Other Implementation Details The sparse feature vectors obtained above are hashed onto 216 − 1 dimen-
sions for efﬁciency.

7.1.2 PASCAL VOC 2007: Visual Object Localization

Given an image and an object of interest, the task is to localize the object in the given image, i.e., determine
the best bounding box around the object. A related, but harder task is object detection, which requires
identifying and localizing any number of objects of interest, if any, in the image. Here, we restrict ourselves
to pure localization with a single instance of each object. Given an image x ∈ X of size n1 × n2, the label
y ∈ Y(x) is a bounding box, where Y(x) is the set of all bounding boxes in an image of size n1 × n2. Note
that |Y(x)| = O(n2

1n2

2).

Loss Function The PASCAL IoU metric [Everingham et al., 2010] is used to measure the quality of
localization. Given bounding boxes y, y(cid:48), the IoU is deﬁned as the ratio of the intersection of the bounding
boxes to the union:

IoU(y, y(cid:48)) =

Area(y ∩ y(cid:48))
Area(y ∪ y(cid:48))

.

We then use the 1 − IoU loss deﬁned as (cid:96)(y, y(cid:48)) = 1 − IoU(y, y(cid:48)).

Score Function The formulation we use is based on the popular R-CNN approach [Girshick et al., 2014].
linear score and non-linear score φ, both of which are based on the following
We consider two cases:
deﬁnition of the feature map Φ(x, y).

• Consider a patch x|y of image x cropped to box y, and rescale it to 64 × 64. Call this Π(x|y).

33

• Consider a convolutional neural network known as AlexNet [Krizhevsky et al., 2012] pre-trained on
ImageNet [Russakovsky et al., 2015] and pass Π(x|y) through it. Take the output of conv4, the
penultimate convolutional layer as the feature map Φ(x, y). It is of size 3 × 3 × 256.

In the case of linear score functions, we take φ(x, y; w) = (cid:104)w, Φ(x, y)(cid:105). In the case of non-linear score
functions, we deﬁne the score φ as the the result of a convolution composed with a non-linearity and followed
by a linear map. Concretely, for θ ∈ RH×W ×C1 and w ∈ RC1×C2 let the map θ (cid:55)→ θ (cid:63) w ∈ RH×W ×C2
denote a two dimensional convolution with stride 1 and kernel size 1, and σ : R → R denote the exponential
linear unit, deﬁned respectively as

[θ (cid:63) w]ij = w(cid:62)[θ]ij

and σ(x) = x I(x ≥ 0) + (exp(x) − 1) I(x < 0) ,

where [θ]ij ∈ RC1 is such that its lth entry is θijl and likewise for [θ (cid:63) w]ij. We overload notation to
let σ : Rd → Rd denote the exponential linear unit applied element-wise. Notice that σ is smooth. The
non-linear score function φ is now deﬁned, with w1 ∈ R256×16, w2 ∈ R16×3×3 and w = (w1, w2), as,

φ(x, y; w) = (cid:104)σ(Φ(x, y) (cid:63) w1), w2(cid:105) .

Inference For a given input image x, we follow the R-CNN approach [Girshick et al., 2014] and use
selective search [Van de Sande et al., 2011] to prune the search space. In particular, for an image x, we use
the selective search implementation provided by OpenCV [Bradski, 2000] and take the top 1000 candidates
returned to be the set (cid:98)Y(x), which we use as a proxy for Y(x). The max oracle and the top-K oracle are
then implemented as exhaustive searches over this reduced set (cid:98)Y(x).

Dataset We use the PASCAL VOC 2007 dataset [Everingham et al., 2010], which contains ∼ 5K an-
notated consumer (real world) images shared on the photo-sharing site Flickr from 20 different object cat-
egories. For each class, we consider all images with only a single occurrence of the object, and train an
independent model for each class.

Evaluation Metric We keep track of two metrics. The ﬁrst is the localization accuracy, also known as
CorLoc (for correct localization), following Deselaers et al. [2010]. A bounding box with IoU > 0.5 with the
ground truth is considered correct and the localization accuracy is the fraction of images labeled correctly.
The second metric is average precision (AP), which requires a conﬁdence score for each prediction. We use
φ(x, y(cid:48); w) as the conﬁdence score of y(cid:48). As previously, we also plot the objective function value measured
on the training examples.

Other Implementation Details For a given input-output pair (x, y) in the dataset, we instead use (x, (cid:98)y)
as a training example, where (cid:98)y = arg maxy(cid:48)∈ (cid:98)Y(x) IoU(y, y(cid:48)) is the element of (cid:98)Y(x) which overlaps the
most with the true output y.

7.2 Methods Compared

The experiments compare various convex stochastic and incremental optimization methods for structured
prediction.

34

• SGD: Stochastic subgradient method with a learning rate γt = γ0/(1+(cid:98)t/t0(cid:99)), where η0, t0 are tuning
parameters. Note that this scheme of learning rates does not have a theoretical analysis. However,
the averaged iterate wt = 2/(t2 + t) (cid:80)t
τ =1 τ wτ obtained from the related scheme γt = 1/(λt) was
shown to have a convergence rate of O((λ(cid:15))−1) [Shalev-Shwartz et al., 2011, Lacoste-Julien et al.,
2012]. It works on the non-smooth formulation directly.

• BCFW: The block coordinate Frank-Wolfe algorithm of Lacoste-Julien et al. [2013]. We use the
version that was found to work best in practice, namely, one that uses the weighted averaged iterate
wt = 2/(t2 + t) (cid:80)t
τ =1 τ wτ (called bcfw-wavg by the authors) with optimal tuning of learning
rates. This algorithm also works on the non-smooth formulation and does not require any tuning.

• SVRG: The SVRG algorithm proposed by Johnson and Zhang [2013], with each epoch making one
pass through the dataset and using the averaged iterate to compute the full gradient and restart the next
epoch. This algorithm requires smoothing.

• Casimir-SVRG-const: Algo. 4 with SVRG as the inner optimization algorithm. The parameters µk
and κk as chosen in Prop. 29, where µ and κ are hyperparameters. This algorithm requires smoothing.

• Casimir-SVRG-adapt: Algo. 4 with SVRG as the inner optimization algorithm. The parameters µk
and κk as chosen in Prop. 30, where µ and κ are hyperparameters. This algorithm requires smoothing.

On the other hand, for non-convex structured prediction, we only have two methods:

• SGD: The stochastic subgradient method [Davis and Drusvyatskiy, 2018], which we call as SGD. This
√
algorithm works directly on the non-smooth formulation. We try learning rates γt = γ0, γt = γ0/
t
and γt = γ0/t, where γ0 is found by grid search in each of these cases. We use the names SGD-const,
SGD-t−1/2 and SGD-t−1 respectively for these variants. We note that SGD-t−1 does not have any
theoretical analysis in the non-convex case.

• PL-Casimir-SVRG: Algo. 5 with Casimir-SVRG-const as the inner solver using the settings of

Prop. 37. This algorithm requires smoothing the inner subproblem.

7.3 Hyperparameters and Variants

Smoothing In light of the discussion of Sec. 4, we use the (cid:96)2
strategy for efﬁcient computation. We then have Dω = 1/2.

2 smoother ω(u) = (cid:107)u(cid:107)2

2/2 and use the top-K

Regularization The regularization coefﬁcient λ is chosen as c/n, where c is varied in {0.01, 0.1, 1, 10}.

Choice of K The experiments use K = 5 for named entity recognition where the performance of the
top-K oracle is K times slower, and K = 10 for visual object localization, where the running time of the
top-K oracle is independent of K. We also present results for other values of K in Fig. 5d and ﬁnd that the
performance of the tested algorithms is robust to the value of K.

35

Tuning Criteria Some algorithms require tuning one or more hyperparameters such as the learning rate.
We use grid search to ﬁnd the best choice of the hyperparameters using the following criteria: For the named
entity recognition experiments, the train function value and the validation F1 metric were only weakly
correlated. For instance, the 3 best learning rates in the grid in terms of F1 score, the best F1 score attained
the worst train function value and vice versa. Therefore, we choose the value of the tuning parameter that
attained the best objective function value within 1% of the best validation F1 score in order to measure the
optimization performance while still remaining relevant to the named entity recognition task. For the visual
object localization task, a wide range of hyperparameter values achieved nearly equal performance in terms
of the best CorLoc over the given time horizon, so we choose the value of the hyperparameter that achieves
the best objective function value within a given iteration budget.

7.3.1 Hyperparameters for Convex Optimization

This corresponds to the setting of Section 5.

Learning Rate The algorithms SVRG and Casimir-SVRG-adapt require tuning of a learning rate, while
SGD requires η0, t0 and Casimir-SVRG-const requires tuning of the Lipschitz constant L of ∇Fµω, which
determines the learning rate γ = 1/(L + λ + κ). Therefore, tuning the Lipschitz parameter is similar
to tuning the learning rate. For both the learning rate and Lipschitz parameter, we use grid search on a
logarithmic grid, with consecutive entries chosen a factor of two apart.

Choice of κ For Casimir-SVRG-const, with the Lipschitz constant in hand, the parameter κ is chosen to
minimize the overall complexity as in Prop. 29. For Casimir-SVRG-adapt, we use κ = λ.

Stopping Criteria Following the discussion of Sec. 5, we use an iteration budget of Tbudget = n.

Warm Start The warm start criterion determines the starting iterate of an epoch of the inner optimization
algorithm. Recall that we solve the following subproblem using SVRG for the kth iterate (cf. (25)):

wk ≈ arg min

Fµkω,κk (wk; zk−1) .

w∈Rd

Here, we consider the following warm start strategy to choose the initial iterate (cid:98)w0 for this subproblem:

• Prox-center: (cid:98)w0 = zk−1.

In addition, we also try out the following warm start strategies of Lin et al. [2018]:

• Extrapolation: (cid:98)w0 = wk−1 + c(zk−1 − zk−2) where c = κ
• Prev-iterate: (cid:98)w0 = wk−1.

κ+λ .

We use the Prox-center strategy unless mentioned otherwise.

Level of Smoothing and Decay Strategy For SVRG and Casimir-SVRG-const with constant smoothing,
we try various values of the smoothing parameter in a logarithmic grid. On the other hand, Casimir-SVRG-
adapt is more robust to the choice of the smoothing parameter (Fig. 5a). We use the defaults of µ = 2 for
named entity recognition and µ = 10 for visual object localization.

36

Figure 2: Comparison of convex optimization algorithms for the task of Named Entity Recognition on
CoNLL 2003.

7.3.2 Hyperparameters for Non-Convex Optimization

This corresponds to the setting of Section 6.

Prox-Linear Learning Rate η We perform grid search in powers of 10 to ﬁnd the best prox-linear learning
rate η. We ﬁnd that the performance of the algorithm is robust to the choice of η (Fig. 7a).

Stopping Criteria We used a ﬁxed budget of 5 iterations of Casimir-SVRG-const. In Fig. 7b, we experi-
ment with different iteration budgets.

In order to solve the kth prox-linear subproblem with Casimir-
Level of Smoothing and Decay Strategy
SVRG-const, we must specify the level of smoothing µk. We experiment with two schemes, (a) constant
smoothing µk = µ, and (b) adaptive smoothing µk = µ/k. Here, µ is a tuning parameters, and the adaptive
smoothing scheme is designed based on Prop. 37 and Remark 38. We use the adaptive smoothing strategy
as a default, but compare the two in Fig. 6.

Gradient Lipschitz Parameter for Inner Optimization The inner optimization algorithm Casimir-SVRG-
const still requires a hyperparameter Lk to serve as an estimate to the Lipschitz parameter of the gradient
∇Fη,µkω(· ; wk). We set this parameter as follows, based on the smoothing strategy: (a) Lk = L0 with the
constant smoothing strategy, and (b) Lk = k L0 with the adaptive smoothing strategy (cf. Prop. 2). We note
that the latter choice has the effect of decaying the learning rate as 1/k in the kth outer iteration.

37

7.4 Experimental study of different methods

Convex Optimization For the named entity recognition task, Fig. 2 plots the performance of various
methods on CoNLL 2003. On the other hand, Fig. 3 presents plots for various classes of PASCAL VOC
2007 for visual object localization.

The plots reveal that smoothing-based methods converge faster in terms of training error while achieving
a competitive performance in terms of the performance metric on a held-out set. Furthermore, BCFW and
SGD make twice as many actual passes as SVRG based algorithms.

Non-Convex Optimization Fig. 4 plots the performance of various algorithms on the task of visual object
localization on PASCAL VOC.

7.5 Experimental Study of Effect of Hyperparameters: Convex Optimization

We now study the effects of various hyperparameter choices.

Effect of Smoothing Fig. 5a plots the effect of the level of smoothing for Casimir-SVRG-const and
Casimir-SVRG-adapt. The plots reveal that, in general, small values of the smoothing parameter lead to
better optimization performance for Casimir-SVRG-const. Casimir-SVRG-adapt is robust to the choice of
µ. Fig. 5b shows how the smooth optimization algorithms work when used heuristically on the non-smooth
problem.

Effect of Warm Start Strategies Fig. 5c plots different warm start strategies for Casimir-SVRG-const and
Casimir-SVRG-adapt. We ﬁnd that Casimir-SVRG-adapt is robust to the choice of the warm start strategy
while Casimir-SVRG-const is not. For the latter, we observe that Extrapolation is less stable (i.e., tends
to diverge more) than Prox-center, which is in turn less stable than Prev-iterate, which always
works (cf. Fig. 5c). However, when they do work, Extrapolation and Prox-center provide greater
acceleration than Prev-iterate. We use Prox-center as the default choice to trade-off between
acceleration and applicability.

Effect of K Fig. 5d illustrates the robustness of the method to choice of K: we observe that the results are
all within one standard deviation of each other.

7.6 Experimental Study of Effect of Hyperparameters: Non-Convex Optimization

We now study the effect of various hyperparameters for the non-convex optimization algorithms. All of
these comparisons have been made for λ = 1/n.

Effect of Smoothing Fig. 6a compares the adaptive and constant smoothing strategies. Fig. 6b and Fig. 6c
compare the effect of the level of smoothing on the the both of these. As previously, the adaptive smoothing
strategy is more robust to the choice of the smoothing parameter.

Effect of Prox-Linear Learning Rate η Fig. 7a shows the robustness of the proposed method to the
choice of η.

38

Effect of Iteration Budget Fig. 7b also shows the robustness of the proposed method to the choice of
iteration budget of the inner solver, Casimir-SVRG-const.

Effect of Warm Start of the Inner Solver Fig. 7c studies the effect of the warm start strategy used within
the inner solver Casimir-SVRG-const in each inner prox-linear iteration. The results are similar to those
obtained in the convex case, with Prox-center choice being the best compromise between acceleration
and compatibility.

8 Future Directions

We introduced a general notion of smooth inference oracles in the context of black-box ﬁrst-order opti-
mization. This allows us to set the scene to extend the scope of fast incremental optimization algorithms to
structured prediction problems owing to a careful blend of a smoothing strategy and an acceleration scheme.
We illustrated the potential of our framework by proposing a new incremental optimization algorithm to
train structural support vector machines both enjoying worst-case complexity bounds and demonstrating
competitive performance on two real-world problems. This work paves also the way to faster incremental
primal optimization algorithms for deep structured prediction models.

There are several potential venues for future work. When there is no discrete structure that admits
efﬁcient inference algorithms, it could be beneﬁcial to not treat inference as a black-box numerical proce-
dure [Meshi et al., 2010, Hazan and Urtasun, 2010, Hazan et al., 2016]. Instance-level improved algorithms
along the lines of Hazan et al. [2016] could also be interesting to explore.

Acknowledgments This work was supported by NSF Award CCF-1740551, the Washington Research
Foundation for innovation in Data-intensive Discovery, and the program “Learning in Machines and Brains”
of CIFAR.

39

Figure 3: Comparison of convex optimization algorithms for the task of visual object localization on PAS-
CAL VOC 2007 for λ = 10/n. Plots for all other classes are in Appendix E.

40

Figure 4: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n. Plots for all other classes are in Appendix E.

41

(a) Effect of level of smoothing.

(b) Effect of smoothing: use of smooth optimization with smoothing (labeled “smooth”) versus the
heuristic use of these algorithms without smoothing (labeled “non-smooth”) for λ = 0.01/n.

(c) Effect of warm start strategies for λ = 0.01/n (ﬁrst row) and λ = 1/n (second row).

(d) Effect of K in the top-K oracle (λ = 0.01/n).

Figure 5: Effect of hyperparameters for the task of Named Entity Recognition on CoNLL 2003. C-SVRG
stands for Casimir-SVRG in these plots.

42

(a) Comparison of adaptive and constant smoothing strategies.

(b) Effect of µ of the adaptive smoothing strategy.

(c) Effect of µ of the constant smoothing strategy.

Figure 6: Effect of smoothing on PL-Casimir-SVRG for the task of visual object localization on PASCAL
VOC 2007.

43

(a) Effect of the hyperparameter η.

(b) Effect of the iteration budget of the inner solver.

44

(c) Effect of the warm start strategy of the inner Casimir-SVRG-const algorithm.

Figure 7: Effect of hyperparameters on PL-Casimir-SVRG for the task of visual object localization on
PASCAL VOC 2007.

References

Z. Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. Journal of Machine

Learning Research, 18:221:1–221:51, 2017.

Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden Markov Support Vector Machines. In International

Conference on Machine Learning, pages 3–10, 2003.

D. Batra. An efﬁcient message-passing algorithm for the M -best MAP problem. In Conference on Uncer-

tainty in Artiﬁcial Intelligence, pages 121–130, 2012.

D. Batra, P. Yadollahpour, A. Guzm´an-Rivera, and G. Shakhnarovich. Diverse M -best Solutions in Markov

Random Fields. In European Conference on Computer Vision, pages 1–16, 2012.

A. Beck and M. Teboulle. Smoothing and ﬁrst order methods: A uniﬁed framework. SIAM Journal on

Optimization, 22(2):557–580, 2012.

D. Belanger and A. McCallum. Structured prediction energy networks.

In International Conference on

Machine Learning, pages 983–992, 2016.

R. Bellman. Dynamic Programming. Courier Dover Publications, 1957.

Y. Bengio, Y. LeCun, C. Nohl, and C. Burges. LeRec: A NN/HMM Hybrid for On-Line Handwriting

Recognition. Neural Computation, 7(6):1289–1303, 1995.

D. P. Bertsekas. Dynamic programming and optimal control, volume 1. Athena scientiﬁc Belmont, MA,

1995.

D. P. Bertsekas. Nonlinear programming. Athena Scientiﬁc Belmont, 1999.

L. Bottou and P. Gallinari. A Framework for the Cooperation of Learning Algorithms.

In Advances in

Neural Information Processing Systems, pages 781–788, 1990.

L. Bottou, Y. Bengio, and Y. LeCun. Global Training of Document Processing Systems Using Graph
In Conference on Computer Vision and Pattern Recognition, pages 489–494,

Transformer Networks.
1997.

G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000.

J. V. Burke. Descent methods for composite nondifferentiable optimization problems. Mathematical Pro-

gramming, 33(3):260–279, 1985.

C. Chen, V. Kolmogorov, Y. Zhu, D. N. Metaxas, and C. H. Lampert. Computing the M Most Probable
Modes of a Graphical Model. In International Conference on Artiﬁcial Intelligence and Statistics, pages
161–169, 2013.

Y.-Q. Cheng, V. Wu, R. Collins, A. R. Hanson, and E. M. Riseman. Maximum-weight bipartite matching
technique and its application in image feature matching. In Visual Communications and Image Process-
ing, volume 2727, pages 453–463, 1996.

45

M. Collins, A. Globerson, T. Koo, X. Carreras, and P. L. Bartlett. Exponentiated gradient algorithms for
conditional random ﬁelds and max-margin markov networks. Journal of Machine Learning Research, 9
(Aug):1775–1822, 2008.

R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. P. Kuksa. Natural language process-

ing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537, 2011.

G. F. Cooper. The computational complexity of probabilistic inference using bayesian belief networks.

Artiﬁcial Intelligence, 42(2-3):393–405, 1990.

B. Cox, A. Juditsky, and A. Nemirovski. Dual subgradient algorithms for large-scale nonsmooth learning

problems. Mathematical Programming, 148(1-2):143–180, 2014.

K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines.

Journal of Machine Learning Research, 2(Dec):265–292, 2001.

H. Daum´e III and D. Marcu. Learning as search optimization: approximate large margin methods for

structured prediction. In International Conference on Machine Learning, pages 169–176, 2005.

D. Davis and D. Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. arXiv

preprint arXiv:1803.06523, 2018.

and Computing, 2(1):25–36, 1992.

A. P. Dawid. Applications of a general propagation algorithm for probabilistic expert systems. Statistics

A. Defazio. A simple practical accelerated method for ﬁnite sums.

In Advances in Neural Information

Processing Systems, pages 676–684, 2016.

A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for
non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages
1646–1654, 2014.

T. Deselaers, B. Alexe, and V. Ferrari. Localizing objects while learning their appearance. In European

Conference on Computer Vision, pages 452–466, 2010.

D. Drusvyatskiy and C. Paquette. Efﬁciency of minimizing compositions of convex functions and smooth

maps. Mathematical Programming, Jul 2018.

J. C. Duchi, D. Tarlow, G. Elidan, and D. Koller. Using Combinatorial Optimization within Max-Product

Belief Propagation. In Advances in Neural Information Processing Systems, pages 369–376, 2006.

M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The Pascal Visual Object Classes

(VOC) challenge. International Journal of Computer Vision, 88(2):303–338, 2010.

N. Flerova, R. Marinescu, and R. Dechter. Searching for the M Best Solutions in Graphical Models. Journal

of Artiﬁcial Intelligence Research, 55:889–952, 2016.

M. Fromer and A. Globerson. An LP view of the M -best MAP problem. In Advances in Neural Information

Processing Systems, pages 567–575, 2009.

46

R. Frostig, R. Ge, S. Kakade, and A. Sidford. Un-regularizing: approximate proximal point and faster
stochastic algorithms for empirical risk minimization. In International Conference on Machine Learning,
pages 2540–2548, 2015.

R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Conference on Computer Vision and Pattern Recognition, pages 580–587,
2014.

D. M. Greig, B. T. Porteous, and A. H. Seheult. Exact maximum a posteriori estimation for binary images.

Journal of the Royal Statistical Society. Series B (Methodological), pages 271–279, 1989.

T. Hazan and R. Urtasun. A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Struc-

tured Prediction. In Advances in Neural Information Processing Systems, pages 838–846, 2010.

T. Hazan, A. G. Schwing, and R. Urtasun. Blending Learning and Inference in Conditional Random Fields.

Journal of Machine Learning Research, 17:237:1–237:25, 2016.

L. He, K. Lee, M. Lewis, and L. Zettlemoyer. Deep Semantic Role Labeling: What Works and What’s Next.

In Annual Meeting of the Association for Computational Linguistics, pages 473–483, 2017.

N. He and Z. Harchaoui. Semi-Proximal Mirror-Prox for Nonsmooth Composite Minimization. In Advances

in Neural Information Processing Systems, pages 3411–3419, 2015.

M. Held, P. Wolfe, and H. P. Crowder. Validation of subgradient optimization. Mathematical Programming,

6(1):62–88, Dec 1974.

T. Hofmann, A. Lucchi, S. Lacoste-Julien, and B. McWilliams. Variance reduced stochastic gradient descent

with neighbors. In Advances in Neural Information Processing Systems, pages 2305–2313, 2015.

H. Ishikawa and D. Geiger. Segmentation by grouping junctions. In Conference on Computer Vision and

Pattern Recognition, pages 125–131, 1998.

M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for the Ising model. SIAM Journal

on computing, 22(5):1087–1116, 1993.

T. Joachims, T. Finley, and C.-N. J. Yu. Cutting-plane training of structural SVMs. Machine Learning, 77

(1):27–59, 2009.

J. K. Johnson. Convex relaxation methods for graphical models: Lagrangian and maximum entropy ap-

proaches. PhD thesis, Massachusetts Institute of Technology, Cambridge, MA, USA, 2008.

R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In

Advances in Neural Information Processing Systems, pages 315–323, 2013.

V. Jojic, S. Gould, and D. Koller. Accelerated dual decomposition for MAP inference. In International

Conference on Machine Learning, pages 503–510, 2010.

D. Jurafsky, J. H. Martin, P. Norvig, and S. Russell. Speech and Language Processing. Pearson Education,

2014. ISBN 9780133252934.

ISBN 978-0-262-01319-2.

D. Koller and N. Friedman. Probabilistic Graphical Models - Principles and Techniques. MIT Press, 2009.

47

V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? IEEE Transactions

on Pattern Analysis and Machine Intelligence, 26(2):147–159, 2004.

A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet classiﬁcation with deep convolutional neural

networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012.

S. Lacoste-Julien, M. Schmidt, and F. Bach. A simpler approach to obtaining an O(1/t) convergence rate

for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002, 2012.

S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Block-Coordinate Frank-Wolfe Optimization for

Structural SVMs. In International Conference on Machine Learning, pages 53–61, 2013.

J. Lafferty, A. McCallum, and F. C. Pereira. Conditional Random Fields: Probabilistic Models for Segment-
In International Conference on Machine Learning, pages 282–289,

ing and Labeling Sequence Data.
2001.

C. H. Lampert, M. B. Blaschko, and T. Hofmann. Beyond sliding windows: Object localization by efﬁcient

subwindow search. In Conference on Computer Vision and Pattern Recognition, pages 1–8, 2008.

N. Le Roux, M. W. Schmidt, and F. R. Bach. A Stochastic Gradient Method with an Exponential Conver-
gence Rate for Strongly-Convex Optimization with Finite Training Sets. In Advances in Neural Informa-
tion Processing Systems, pages 2672–2680, 2012.

M. Lewis and M. Steedman. A* CCG parsing with a supertag-factored model. In Conference on Empirical

Methods in Natural Language Processing, pages 990–1000, 2014.

H. Lin, J. Mairal, and Z. Harchaoui. A universal catalyst for ﬁrst-order optimization. In Advances in Neural

Information Processing Systems, pages 3384–3392, 2015.

H. Lin, J. Mairal, and Z. Harchaoui. Catalyst Acceleration for First-order Convex Optimization: from

Theory to Practice. Journal of Machine Learning Research, 18(212):1–54, 2018.

L. Lov´asz. Submodular functions and convexity. In Mathematical Programming The State of the Art, pages

235–257. Springer, 1983.

J. Mairal.

Incremental majorization-minimization optimization with application to large-scale machine

learning. SIAM Journal on Optimization, 25(2):829–855, 2015.

A. F. T. Martins and R. F. Astudillo. From Softmax to Sparsemax: A Sparse Model of Attention and
Multi-Label Classiﬁcation. In International Conference on Machine Learning, pages 1614–1623, 2016.

R. J. McEliece, D. J. C. MacKay, and J. Cheng. Turbo Decoding as an Instance of Pearl’s ”Belief Propaga-

tion” Algorithm. IEEE Journal on Selected Areas in Communications, 16(2):140–152, 1998.

A. Mensch and M. Blondel. Differentiable dynamic programming for structured prediction and attention.

In International Conference on Machine Learning, pages 3459–3468, 2018.

O. Meshi, D. Sontag, T. S. Jaakkola, and A. Globerson. Learning Efﬁciently with Approximate Inference

via Dual Losses. In International Conference on Machine Learning, pages 783–790, 2010.

O. Meshi, T. S. Jaakkola, and A. Globerson. Convergence Rate Analysis of MAP Coordinate Minimization

Algorithms. In Advances in Neural Information Processing Systems, pages 3023–3031, 2012.

48

K. P. Murphy, Y. Weiss, and M. I. Jordan. Loopy belief propagation for approximate inference: An empirical

study. In Conference on Uncertainty in Artiﬁcial Intelligence, pages 467–475, 1999.

Y. Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2). In Soviet

Mathematics Doklady, volume 27, pages 372–376, 1983.

Y. Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on Optimization,

Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103(1):127–152,

Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science &

16(1):235–249, 2005a.

2005b.

Business Media, 2013.

V. Niculae, A. F. Martins, M. Blondel, and C. Cardie. SparseMAP: Differentiable Sparse Structured Infer-

ence. In International Conference on Machine Learning, pages 3796–3805, 2018.

D. Nilsson. An efﬁcient algorithm for ﬁnding the M most probable conﬁgurations in probabilistic expert

systems. Statistics and Computing, 8(2):159–173, 1998.

A. Osokin, J.-B. Alayrac, I. Lukasewitz, P. Dokania, and S. Lacoste-Julien. Minding the gaps for block
Frank-Wolfe optimization of structured SVMs. In International Conference on Machine Learning, pages
593–602, 2016.

B. Palaniappan and F. Bach. Stochastic variance reduction methods for saddle-point problems. In Advances

in Neural Information Processing Systems, pages 1408–1416, 2016.

C. Paquette, H. Lin, D. Drusvyatskiy, J. Mairal, and Z. Harchaoui. Catalyst for gradient-based nonconvex
optimization. In International Conference on Artiﬁcial Intelligence and Statistics, pages 613–622, 2018.

J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann,

N. D. Ratliff, J. A. Bagnell, and M. Zinkevich. (Approximate) Subgradient Methods for Structured Predic-

tion. In International Conference on Artiﬁcial Intelligence and Statistics, pages 380–387, 2007.

R. T. Rockafellar and R. J.-B. Wets. Variational analysis, volume 317. Springer Science & Business Media,

1988.

2009.

O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bern-
stein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International
Journal of Computer Vision, 115(3):211–252, 2015.

B. Savchynskyy, J. H. Kappes, S. Schmidt, and C. Schn¨orr. A study of Nesterov’s scheme for Lagrangian
decomposition and MAP labeling. In Conference on Computer Vision and Pattern Recognition, pages
1817–1823, 2011.

M. I. Schlesinger. Syntactic analysis of two-dimensional visual signals in noisy conditions. Kibernetika, 4

(113-130):1, 1976.

49

M. Schmidt, R. Babanezhad, M. Ahmed, A. Defazio, A. Clifton, and A. Sarkar. Non-uniform stochastic
average gradient method for training conditional random ﬁelds. In International Conference on Artiﬁcial
Intelligence and Statistics, pages 819–828, 2015.

M. Schmidt, N. Le Roux, and F. Bach. Minimizing ﬁnite sums with the stochastic average gradient. Math-

ematical Programming, 162(1-2):83–112, 2017.

A. Schrijver. Combinatorial Optimization - Polyhedra and Efﬁciency. Springer, 2003.

B. Seroussi and J. Golmard. An algorithm directly ﬁnding the K most probable conﬁgurations in Bayesian

networks. International Journal of Approximate Reasoning, 11(3):205 – 233, 1994.

S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss minimiza-

tion. Journal of Machine Learning Research, 14(Feb):567–599, 2013.

S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized

loss minimization. In International Conference on Machine Learning, pages 64–72, 2014.

S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-gradient solver for

SVM. Mathematical programming, 127(1):3–30, 2011.

H. O. Song, R. B. Girshick, S. Jegelka, J. Mairal, Z. Harchaoui, and T. Darrell. On learning to localize
objects with minimal supervision. In International Conference on Machine Learning, pages 1611–1619,
2014.

B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In Advances in Neural Information

Processing Systems, pages 25–32, 2004.

B. Taskar, S. Lacoste-Julien, and D. Klein. A discriminative matching approach to word alignment.

In
Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing, pages 73–80, 2005.

B. Taskar, S. Lacoste-Julien, and M. I. Jordan. Structured prediction, dual extragradient and Bregman

projections. Journal of Machine Learning Research, 7(Jul):1627–1653, 2006.

C. H. Teo, S. Vishwanathan, A. Smola, and Q. V. Le. Bundle methods for regularized risk minimization.

Journal of Machine Learning Research, 1(55), 2009.

E. F. Tjong Kim Sang and F. De Meulder.

Introduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Conference on Natural Language Learning, pages 142–147,
2003.

M. Tkachenko and A. Simanovsky. Named entity recognition: Exploring features. In Empirical Methods in

Natural Language Processing, pages 118–127, 2012.

I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdepen-
dent and structured output spaces. In International Conference on Machine Learning, page 104, 2004.

K. E. Van de Sande, J. R. Uijlings, T. Gevers, and A. W. Smeulders. Segmentation as selective search for

object recognition. In International Conference on Computer Vision, pages 1879–1886, 2011.

50

A. J. Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.

IEEE Trans. Information Theory, 13(2):260–269, 1967. doi: 10.1109/TIT.1967.1054010.

M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference.

Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008.

M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. MAP estimation via agreement on trees: message-
passing and linear programming. IEEE transactions on information theory, 51(11):3697–3717, 2005.

B. E. Woodworth and N. Srebro. Tight complexity bounds for optimizing composite objectives. In Advances

in Neural Information Processing Systems, pages 3639–3647, 2016.

C. Yanover and Y. Weiss. Finding the M most probable conﬁgurations using loopy belief propagation. In

Advances in Neural Information Processing Systems, pages 289–296, 2004.

X. Zhang, A. Saha, and S. Vishwanathan. Accelerated training of max-margin markov networks with ker-

nels. Theoretical Computer Science, 519:88–102, 2014.

51

A Smoothing

We ﬁrst prove an extension of Lemma 4.2 of Beck and Teboulle [2012], which proves the following state-
ment for the special case of µ2 = 0. Recall that we deﬁned hµω ≡ h when µ = 0.

Proposition 39. Consider the setting of Def. 1. For µ1 ≥ µ2 ≥ 0, we have for every z ∈ Rm that

(µ1 − µ2)

inf
u∈dom h∗

ω(u) ≤ hµ2ω(z) − hµ1ω(z) ≤ (µ1 − µ2)

sup
u∈dom h∗

ω(u) .

Proof. We successively deduce,

hµ1ω(z) = sup

{(cid:104)u, z(cid:105) − h∗(u) − µ1ω(u)}

u∈dom h∗

= sup

u∈dom h∗

≥ sup

u∈dom h∗

(cid:26)

{(cid:104)u, z(cid:105) − h∗(u) − µ2ω(u) − (µ1 − µ2)ω(u)}

(cid:104)u, z(cid:105) − h∗(u) − µ2ω(u) + inf

u(cid:48)∈dom h∗

(cid:27)
(cid:8)−(µ1 − µ2)ω(u(cid:48))}(cid:9)

= hµ2ω(z) − (µ1 − µ2)

sup
u(cid:48)∈dom h∗

ω(u(cid:48)) ,

since µ1 − µ2 ≥ 0. The other side follows using instead that

−(µ1 − µ2)ω(u) ≤ sup

(cid:8)−(µ1 − µ2)ω(u(cid:48))}(cid:9) .

u(cid:48)∈dom h∗

Next, we recall the following equivalent deﬁnition of a matrix norm deﬁned in Eq. (2).

(cid:107)A(cid:107)β,α = sup
y(cid:54)=0

(cid:107)A(cid:62)y(cid:107)∗
β
(cid:107)y(cid:107)α

= sup
x(cid:54)=0

(cid:107)Ax(cid:107)∗
α
(cid:107)y(cid:107)β

= (cid:107)A(cid:62)(cid:107)α,β .

(65)

Now, we consider the smoothness of a composition of a smooth function with an afﬁne map.

Lemma 40. Suppose h : Rm → R is L-smooth with respect to (cid:107) · (cid:107)∗
b ∈ Rm, we have that the map Rd (cid:51) w (cid:55)→ h(Aw + b) is (cid:0)L(cid:107)A(cid:62)(cid:107)2

α. Then, for any A ∈ Rm×d and
(cid:1)-smooth with respect to (cid:107) · (cid:107)β.

α,β

Proof. Fix A ∈ Rm×d, b ∈ Rm and deﬁne f : Rd → R as f (w) = h(Aw + b). By the chain rule, we have
that ∇f (w) = A(cid:62)∇h(Aw + b). Using smoothness of h, we successively deduce,

(cid:107)∇f (w1) − ∇f (w2)(cid:107)∗

β = (cid:107)A(cid:62)(∇h(Aw1 + b) − ∇h(Aw2 + b))(cid:107)∗
β

(65)
≤ (cid:107)A(cid:62)(cid:107)α,β(cid:107)∇h(Aw1 + b) − ∇h(Aw2 + b)(cid:107)α
≤ (cid:107)A(cid:62)(cid:107)α,β L(cid:107)A(w1 − w2)(cid:107)∗
α
(65)
≤ L(cid:107)A(cid:62)(cid:107)2

α,β(cid:107)w1 − w2(cid:107)β .

Shown in Algo. 6 is the procedure to compute the outputs of the top-K oracle from the K best scoring

outputs obtained, for instance, from the top-K max-product algorithm.

52

Algorithm 6 Top-K oracle from top-K outputs
1: Input: Augmented score function ψ, w ∈ Rd, µ > 0 YK = {y1, · · · , yK} such that yk =

max(k)

y∈Y ψ(y; w).

2: Populate z ∈ RK so that zk = 1
µ ψ(yk; w).
3: Compute u∗ = arg minu∈∆K−1 (cid:107)u − z(cid:107)2
4: return s = (cid:80)K
k=1 u∗

k ψ(yk; w) and v = (cid:80)K

2 by a projection on the simplex.
k ∇wψ(yk; w).

k=1 u∗

Algorithm 7 Standard max-product algorithm

1: Input: Augmented score function ψ(·, ·; w) deﬁned on tree structured graph G with root r ∈ V.
2: Initialize: Let V be a list of nodes from V\{r} arranged in increasing order of height.
3: for v in V do
4:

Set mv(yρ(v)) ← maxyv∈Yv
Yρ(v).
Assign to δv(yρ(v)) a maximizing assignment of yv from above for each yρ(v) ∈ Yρ(v).

(cid:110)
ψv(yv) + ψv,ρ(v)(yv, yρ(v)) + (cid:80)

v(cid:48)∈C(v) mv(cid:48)(yv)

(cid:111)

for each yρ(v) ∈

(cid:110)
ψr(yr) + (cid:80)

v(cid:48)∈C(r) mv(cid:48)(yr)

(cid:111)
.

(cid:110)
ψr(yr) + (cid:80)

v(cid:48)∈C(r) mv(cid:48)(yr)

(cid:111)
.

5:
6: end for
7: ψ∗ ← maxyr∈Yr
8: y∗
r ← arg maxyr∈Yr
9: for v in reverse(V ) do
v = δv(y∗
y∗
10:
11: end for
12: return ψ∗, y∗ = (y∗

ρ(v)).

1, · · · , y∗

p).

B Smooth Inference in Trees

A graph G is a tree if it is connected, directed and each node has at most one incoming edge. It has one root
r ∈ V with no incoming edge. An undirected graph with no loops can be converted to a tree by ﬁxing an
arbitrary root and directing all edges way from the root. We say that G is a chain if it is a tree with root p
where all edges are of the form (v + 1, v). For a node v in a tree G, we denote by ρ(v) and C(v) respectively
the parent of v and the children of v in the tree.

Recall ﬁrst that the height of a node in a rooted tree is the number of edges on the longest directed
path from the node to a leaf where each edge is directed away from the root. We ﬁrst review the standard
max-product algorithm for maximum a posteriori (MAP) inference [Dawid, 1992] - Algo. 7. It runs in time
O(p maxv∈V |Yv|2) and requires space O(p maxv∈V |Yv|).

B.1 Proof of Correctness of Top-K Max-Product

We now consider the top-K max-product algorithm, shown in full generality in Algo. 8. The following
proposition proves its correctness.

Proposition 41. Consider as inputs to Algo. 8 an augmented score function ψ(·, ·; w) deﬁned on tree
structured graph G, and an integer K > 0. Then, the outputs of Algo. 8 satisfy ψ(k) = ψ(y(k)) =
max(k)
Proof. For a node v ∈ V, let τ (v) denote the sub-tree of G rooted at v. Let yτ (v) denote (cid:0)yv(cid:48) for v(cid:48) ∈ τ (v)(cid:1).

y∈Y ψ(y). Moreover, Algo. 8 runs in time O(pK log K maxv∈V |Yv|2) and uses space O(pK maxv∈V |Yv|).

53

Algorithm 8 Top-K max-product algorithm

1: Input: Augmented score function ψ(·, ·; w) deﬁned on tree structured graph G with root r ∈ V, and

integer K > 0.

2: Initialize: Let V be a list of nodes from V\{r} arranged in increasing order of height.
3: for v in V and k = 1, · · · , K do
4:

if v is a leaf then

m(k)

v (yρ(v)) ← max(k)

yv∈Yv

(cid:8)ψv(yv) + ψv,ρ(v)(yv, yρ(v))(cid:9) for each yρ(v) ∈ Yρ(v).

else

Assign for each yρ(v) ∈ Yρ(v),

m(k)

v (yρ(v)) ← max(k)

(cid:40)

ψv(yv) + ψv,ρ(v)(yv, yρ(v))
v(cid:48)∈C(v) m(lv(cid:48) )

+ (cid:80)

(yv)

v(cid:48)

yv ∈ Yv and
lv(cid:48) ∈ [K] for v(cid:48) ∈ C(v)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)

.

(66)

v(cid:48) (yρ(v)) for v(cid:48) ∈ C(v) store the maximizing assignment of yv and l(cid:48)

v from

5:

6:

7:

8:

Let δ(k)
v (yρ(v)) and κ(k)
above for each yρ(v) ∈ Yρ(v).

end if
9:
10: end for
11: For k = 1, · · · , K, set

ψ(k) ← max(k)

ψr(yr) +

(cid:26)

(cid:88)

m(lv(cid:48) )
v(cid:48)

(yr)

v(cid:48)∈C(r)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

yr ∈ Yr and lv(cid:48) ∈ [K] for v(cid:48) ∈ C(r)

(cid:27)

and assign maximizing assignments of yr, lv(cid:48) above respectively to y(k)

r

and l(k)
v(cid:48)

for v(cid:48) ∈ C(r).

12: for v in reverse(V ) and k = 1, · · · , K do
v )

(cid:0)y(k)
ρ(v)
(cid:0)y(k)
ρ(v)

(cid:1).
(cid:1) for all v(cid:48) ∈ C(v).

13:

Set y(k)
Set l(k)

v

v ← δ(l(k)
v(cid:48) = κ(l(k)
(cid:110)

v )

v(cid:48)

14:
15: end for
16: return

ψ(k), y(k) := (y(k)

1 , · · · , y(k)
p )

(cid:111)K

.

k=1

54

(67)

(68)

(69)

Deﬁne ψτ (v) as follows: if v is a leaf, yτ (v) = (yv) and ψτ (v)(yτ (v)) := ψv(yv). For a non-leaf v, deﬁne
recursively

ψτ (v)(yτ (v)) := ψv(yv) +

(cid:2)ψv,v(cid:48)(yv, yv(cid:48)) + ψτ (v(cid:48))(yτ (v(cid:48)))(cid:3) .

(cid:88)

v(cid:48)∈C(v)

We will need some identities about choosing the kth largest element from a ﬁnite collection. For ﬁnite sets
S1, · · · , Sn and functions fj : Sj → R, h : S1 × S2 → R, we have,












max(k)
u1∈S1,··· ,un∈Sn

n
(cid:88)



j=1

fj(uj)

= max(k)
l1,··· ,ln∈[k]



max(lj )
uj ∈Sj

fj(uj)

,



n
(cid:88)



j=1

(cid:26)

max(k)
u1∈S1,u2∈S2

{f1(u1) + h(u1, u2)} = max(k)
u1∈S1,l∈[k]

f1(u1) + max(l)

h(u1, u2)

.

u2∈S2

(cid:27)

The identities above state that for a sum to take its kth largest value, each component of the sum must take
one of its k largest values. Indeed, if one of the components of the sum took its lth largest value for l > k,
replacing it with any of the k largest values cannot decrease the value of the sum. Eq. (69) is a generalized
version of Bellman’s principle of optimality (see Bellman [1957, Chap. III.3.] or Bertsekas [1995, Vol. I,
Chap. 1]).

For the rest of the proof, yτ (v)\yv is used as shorthand for {yv(cid:48) | v(cid:48) ∈ τ (v)\{v}}. Moreover, maxyτ (v)
represents maximization over yτ (v) ∈ ×v(cid:48)∈τ (v) Yv(cid:48). Likewise for maxyτ (v)\yv . Now, we shall show by
induction that for all v ∈ V, yv ∈ Yv and k = 1, · · · , K,

max(k)
yτ (v)\yv

ψτ (v)(yτ (v)) = ψv(yv) + max(k)

(cid:26) (cid:88)

v(cid:48)∈C(v)

m(lv(cid:48) )
v(cid:48)

(cid:12)
(cid:12)
lv(cid:48) ∈ [K] for v(cid:48) ∈ C(v)
(yv(cid:48))
(cid:12)
(cid:12)

(cid:27)

.

(70)

The induction is based on the height of a node. The statement is clearly true for a leaf v since C(v) = ∅.
Suppose (70) holds for all nodes of height ≤ h. For a node v of height h + 1, we observe that τ (v)\v can
be partitioned into {τ (v(cid:48)) for v(cid:48) ∈ C(v)} to get,

max(k)
yτ (v)\yv

ψτ (v)(yτ (v)) − ψv(yv)

(67)
= max(k)
yτ (v)\yv

(cid:26) (cid:88)

v(cid:48)∈C(v)

ψv,v(cid:48)(yv, yv(cid:48)) + ψτ (v(cid:48))(yτ (v(cid:48)))

(cid:27)

(cid:27)

{ψv,v(cid:48)(yv, yv(cid:48)) + ψτ (v(cid:48))(yτ (v(cid:48)))}

lv(cid:48) ∈ [K] for v(cid:48) ∈ C(v)

.

(71)

(68)
= max(k)

(cid:26) (cid:88)

v(cid:48)∈C(v)

max(lv(cid:48) )
yτ (v(cid:48))
(cid:124)

(cid:123)(cid:122)
=:Tv(cid:48) (yv)

Let us analyze the term in the underbrace, Tv(cid:48)(yv). We successively deduce, with the argument l in the
maximization below taking values in {1, · · · , K},

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:125)

(cid:27)

Tv(cid:48)(yv)

(69)
= max(lv(cid:48) )
yv(cid:48) ,l

ψv,v(cid:48)(yv, yv(cid:48)) + max(l)
yτ (v(cid:48))\yv(cid:48)

ψτ (v(cid:48))(yτ (v(cid:48)))

ψv(cid:48)(yv(cid:48)) + ψv,v(cid:48)(yv, yv(cid:48))+

(cid:27)
(yv(cid:48)) | lv(cid:48)(cid:48) ∈ [K] for v(cid:48)(cid:48) ∈ C(v(cid:48))(cid:9)

max(l) (cid:8) (cid:80)

v(cid:48)(cid:48)
(cid:26)ψv(cid:48)(yv(cid:48)) + ψv(cid:48),v(yv(cid:48), yv)
v(cid:48)(cid:48)∈C(v(cid:48)) m(lv(cid:48)(cid:48) )
(yv(cid:48))

v(cid:48)(cid:48)∈C(v(cid:48)) m(lv(cid:48)(cid:48) )
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ (cid:80)

v(cid:48)(cid:48)

yv(cid:48) ∈ Yv(cid:48) and
lv(cid:48)(cid:48) ∈ [K] for v(cid:48)(cid:48) ∈ C(v)

(cid:27)

(cid:26)

(cid:26)

(70)
= max(lv(cid:48) )
yv(cid:48) ,l

(69)
= max(lv(cid:48) )

(66)
= m(lv(cid:48) )
v(cid:48)

(yv) .

55

Here, the penultimate step followed from applying in reverse the identity (69) with u1, u2 being by yv(cid:48), {lv(cid:48)(cid:48) for v(cid:48)(cid:48) ∈
C(v(cid:48))} respectively, and f1 and h respectively being ψv(cid:48)(yv(cid:48)) + ψv(cid:48),v(yv(cid:48), yv) and (cid:80)
(yv(cid:48)). Plug-
ging this into (71) completes the induction argument. To complete the proof, we repeat the same argument
over the root as follows. We note that τ (r) is the entire tree G. Therefore, yτ (r) = y and ψτ (r) = ψ. We
now apply the identity (69) with u1 and u2 being yr and yτ (r)\r respectively and f1 ≡ 0 to get

v(cid:48)(cid:48) m(lv(cid:48)(cid:48) )
v(cid:48)(cid:48)

max(k)
y∈Y

ψ(y)

(69)
= max(k)
yr,l

max(l)
y\yr

ψ(y)

= max(k)
yr,l

max(l)
yτ (r)\yr

ψτ (r)(yτ (r))

(cid:27)

(cid:40)

(cid:41)

(cid:26)

(cid:26)

(cid:26)

(70)
= max(k)
yr,l

(69)
= max(k)

= ψ(k) ,

ψr(yr) + max(l) (cid:8) (cid:80)

v∈C(r) m(lv)

v

(yr) | lv ∈ [K] for v ∈ C(r)(cid:9)

(cid:27)

ψr(yr)+
v∈C(r) m(lv)

v

(cid:80)

(yr)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

yr ∈ Yr and
lv ∈ [K] for v ∈ C(r)

(cid:27)

where the last equality follows from Line 11 of Algo. 8.

The algorithm requires storage of m(k)

v , an array of size maxv∈V |Yv| for each k = 1, · · · , K, and v ∈ V.
The backpointers δ, κ are of the same size. This adds up to a total storage of O(pK maxv|Yv|). To bound
the running time, consider Line 7 of Algo. 8. For a ﬁxed v(cid:48) ∈ C(v), the computation

max(k)
yv,lv(cid:48)

(cid:110)
ψv(yv) + ψv,ρ(v)(yv, yρ(v)) + m(lv(cid:48) )

(yv)

(cid:111)

v(cid:48)

for k = 1, · · · , K takes time O(K log K maxv|Yv|). This operation is repeated for each yv ∈ Yv and once
for every (v, v(cid:48)) ∈ E. Since |E| = p − 1, the total running time is O(pK log K maxv|Yv|2).

B.2 Proof of Correctness of Entropy Smoothing of Max-Product

Next, we consider entropy smoothing.

Proposition 42. Given an augmented score function ψ(·, ·; w) deﬁned on tree structured graph G and
µ > 0 as input, Algo. 3 correctly computes f−µH (w) and ∇f−µH (w). Furthermore, Algo. 3 runs in
time O(p maxv∈V |Yv|2) and requires space O(p maxv∈V |Yv|).

Proof. The correctness of the function value f−µH follows from the bijection f−µH (w) = µ Aψ/µ(w) (cf.
Prop. 9), where Thm. 43 shows correctness of Aψ/µ. To show the correctness of the gradient, deﬁne the
probability distribution Pψ,µ as the probability distribution from Lemma 7(ii) and Pψ,µ,v, Pψ,µ,v,v(cid:48) as its
node and edge marginal probabilities respectively as

Pψ,µ(y; w) =

exp

(cid:16) 1

(cid:17)
µ ψ(y; w)

(cid:16) 1

y(cid:48)∈Y exp

µ ψ(y(cid:48); w)

(cid:17) ,

Pψ,µ,v(yv; w) =

Pψ,µ(y; w)

for yv ∈ Yv, v ∈ V , and,

Pψ,µ,v,v(cid:48)(yv, yv(cid:48); w) =

Pψ,µ(y; w)

for yv ∈ Yv, yv(cid:48) ∈ Yv(cid:48), (v, v(cid:48)) ∈ E .

(cid:80)

(cid:88)

y∈Y :
yv=yv
(cid:88)

y∈Y:
yv=yv,
yv(cid:48) =yv(cid:48)

56

Thm. 43 again shows that Algo. 9 correctly produces marginals Pψ,µ,v and Pψ,µ,v,v(cid:48). We now start with
Lemma 7(ii) and invoke (17) to get

∇f−µH (w) =

Pψ,µ(y; w)∇ψ(y; w)

(cid:88)

y∈Y

(cid:88)

y∈Y
(cid:88)

=

=

=

(cid:88)

v∈V
(cid:88)

y∈Y
(cid:88)

(cid:88)





(cid:88)

v∈V

Pψ,µ(y; w)

∇ψv(yv; w) +

∇ψv,v(cid:48)(yv, yv(cid:48); w)

 ,



Pψ,µ(y; w)∇ψv(yv; w) +

Pψ,µ(y; w)∇ψv,v(cid:48)(yv, yv(cid:48); w)

(cid:88)

(v,v(cid:48))∈E
(cid:88)

(cid:88)

(v,v(cid:48))∈E

y∈Y

Pψ,µ(y; w)∇ψv(yv; w)

v∈V

yv∈Yv

y∈Y : yv=yv
(cid:88)
(cid:88)

+

(cid:88)

(cid:88)

(v,v(cid:48))∈E

yv∈Yv

yv(cid:48) ∈Yv(cid:48)

y∈Y :

yv=yv
yv(cid:48) =yv(cid:48)

Pψ,µ,v(yv; w)∇ψv(yv; w)

(cid:88)

(cid:88)

=

v∈V

yv∈Yv

(cid:88)

(cid:88)

(cid:88)

+

(v,v(cid:48))∈E

yv∈Yv

yv(cid:48) ∈Yv(cid:48)

Pψ,µ,v,v(cid:48)(yv, yv(cid:48); w)∇ψv,v(cid:48)(yv, yv(cid:48); w) .

Pψ,µ(y; w)∇ψv,v(cid:48)(yv, yv(cid:48); w)

Here, the penultimate equality followed from breaking the sum over y ∈ Y into an outer sum that sums over
every yv ∈ Yv and an inner sum over y ∈ Y : yv = yv, and likewise for the edges. The last equality above
followed from the deﬁnitions of the marginals. Therefore, Line 3 of Algo. 3 correctly computes the gradient.
The storage complexity of the algorithm is O(p maxv|Yv|) provided that the edge marginals Pψ,µ,v,v(cid:48) are
computed on the ﬂy as needed. The time overhead of Algo. 3 after Algo. 9 is O(p maxv|Yv|2), by noting
that each edge marginal can be computed in constant time (Remark 44).

Given below is the guarantee of the sum-product algorithm (Algo. 9). See, for instance, Koller and

Friedman [2009, Ch. 10] for a proof.

Theorem 43. Consider an augmented score function ψ deﬁned over a tree structured graphical model G.
Then, the output of Algo. 9 satisﬁes

Pv(yv) =

exp(ψ(y) − A)

for all yv ∈ Yv, v ∈ V, and,

Pv,v(cid:48)(yv, yv(cid:48)) =

exp(ψ(y) − A)

for all yv ∈ Yv, yv(cid:48) ∈ Yv(cid:48), (v, v(cid:48)) ∈ E.

A = log

exp(ψ(y)) ,

(cid:88)

y∈Y
(cid:88)

y∈Y : yv=yv
(cid:88)

y∈Y :

yv=yv,
yv(cid:48) =yv(cid:48)

Furthermore, Algo. 9 runs in time O(p maxv∈V |Yv|2) and requires an intermediate storage of O(p maxv∈V |Yv|).

57

Algorithm 9 Sum-product algorithm

1: Procedure: SUMPRODUCT
2: Input: Augmented score function ψ deﬁned on tree structured graph G with root r ∈ V.
3: Notation: Let N (v) = C(v) ∪ {ρ(v)} denote all the neighbors of v ∈ V if the orientation of the edges

were ignored.

4: Initialize: Let V be a list of nodes from V arranged in increasing order of height.
5: for v in V \{r} do
6:

Set for each yρ(v) ∈ Yρ(v):

mv→ρ(v)(yρ(v)) ←

(cid:88)

yv∈Yv


exp (cid:0)ψv(yv) + ψv,ρ(v)(yv, yρ(v))(cid:1) (cid:89)

mv(cid:48)→v(yv)

 .

v(cid:48)∈C(v)

exp (ψr(yr)) (cid:81)

(cid:105)
v(cid:48)∈C(r) mv(cid:48)→r(yr)

.

(cid:104)

7: end for
8: A ← log (cid:80)
9: for v in reverse(V ) do
for v(cid:48) ∈ C(v) do
10:

yr∈Yr

11:

Set for each yv(cid:48) ∈ Yv(cid:48):

mv→v(cid:48)(yv(cid:48)) =

(cid:88)

yv∈Yv


exp (cid:0)ψv(yv) + ψv(cid:48),v(yv(cid:48), yv)(cid:1) (cid:89)

mv(cid:48)(cid:48)→v(yv)

 .

v(cid:48)(cid:48)∈N (v)\{v(cid:48)}

end for

12:
13: end for
14: for v in V do
15:
16: end for
17: for (v, v(cid:48)) in E do
18:

For every pair (yv, yv(cid:48)) ∈ Yv × Yv(cid:48), set

Set Pv(yv) ← exp (ψv(yv) − A) (cid:81)

v(cid:48)(cid:48)∈N (v) mv(cid:48)(cid:48)→v(yv) for every yv ∈ Yv.

Pv,v(cid:48)(yv, yv(cid:48)) ← exp (cid:0)ψv(yv) + ψv(cid:48)(yv(cid:48)) + ψv,v(cid:48)(yv, yv(cid:48)) − A(cid:1)

(cid:89)

mv(cid:48)(cid:48)→v(yv)

mv(cid:48)(cid:48)→v(cid:48)(yv(cid:48)) .

(cid:89)

v(cid:48)(cid:48)∈N (v)\{v(cid:48)}

v(cid:48)(cid:48)∈N (v(cid:48))\{v}

19: end for
20: return A, {Pv for v ∈ V}, {Pv,v(cid:48) for (v, v(cid:48)) ∈ E}.





58

Remark 44. Line 18 of Algo. 9 can be implemented in constant time by reusing the node marginals Pv and
messages mv→v(cid:48), mv(cid:48)→v as

Pv,v(cid:48)(yv, yv(cid:48)) =

Pv(yv)Pv(cid:48)(yv(cid:48)) exp(ψv,v(cid:48)(yv, yv(cid:48)) + A)
mv(cid:48)→v(yv)mv→v(cid:48)(yv(cid:48))

.

C Inference Oracles in Loopy Graphs

This section presents the missing details and recalls from literature the relevant algorithms and results re-
quired in Sec. 4.2. First, we review the BMMF algorithm of Yanover and Weiss [2004], followed by graph
cut inference and graph matching inference.

We now recall and prove the correctness of the decoding scheme (21) for completeness. The result is

due to Pearl [1988], Dawid [1992].

Theorem 45. Consider an unambiguous augmented score function ψ , that is, ψ(y(cid:48); w) (cid:54)= ψ(y(cid:48)(cid:48); w)
for all distinct y(cid:48), y(cid:48)(cid:48) ∈ Y. Then, the result (cid:98)y of the decoding (cid:98)yv = arg maxj∈Yv ψv;j satisﬁes (cid:98)y =
arg maxy∈Y ψ(y).

Proof. Suppose for the sake of contradiction that (cid:98)y (cid:54)= y∗ := arg maxy∈Y ψ(y). Let v ∈ V be such that
yv = j and y∗
v = j(cid:48) where j (cid:54)= j(cid:48). By the fact that y∗ has the highest augmented score and unambiguity, we
get that

max
y∈Y,yv=j(cid:48)

ψ(y) = ψ(y∗) > ψ((cid:98)y) = max

y∈Y,yv=j

ψ(y) ,

which contradicts the deﬁnition of (cid:98)yv.

C.1 Review of Best Max-Marginal First

If one has access to an algorithm M that can compute max-marginals, the top-K oracle is easily imple-
mented via the Best Max Marginal First (BMMF) algorithm of Yanover and Weiss [2004], which is recalled
in Algo. 10. This algorithm requires computations of two sets of max-marginals per iteration, where a set
of max-marginals refers to max-marginals for all variables yv in y.

Details The algorithm runs by maintaining a partitioning of the search space Y and a table ϕ(k)(v, j)
that stores the best score in partition k (deﬁned by constraints C(k)) subject to the additional constraint that
yv = j. In iteration k, the algorithm looks at the k − 1 existing partitions and picks the best partition
sk (Line 9). This partition is further divided into two parts: the max-marginals in the promising partition
(corresponding to yvk = jk) are computed (Line 11) and decoded (Line 12) to yield kth best scoring y(k).
The scores of the less promising partition are updated via a second round of max-marginal computations
(Line 14).

Guarantee The following theorem shows that Algo. 10 provably implements the top-K oracle as long
as the max-marginals can be computed exactly under the assumption of unambiguity. With approximate
max-marginals however, Algo. 10 comes with no guarantees.

59

Algorithm 10 Best Max Marginal First (BMMF)

1: Input: Augmented score function ψ, parameters w, non-negative integer K, algorithm M to compute

max-marginals of ψ.

2: Initialization: C(1) = ∅ and U (2) = ∅.
3: for v ∈ [p] do
4:

5:
6: end for
7: for k = 2, · · · , K do

For j ∈ Yv, set ϕ(1)(v; j) = max{ψ(y; w) | y ∈ Y s.t. yv = j} using M.
Set y(1)

v = arg maxj∈Yv ϕ(1)(v, j).

8:

9:

10:

11:

12:

13:

14:

(cid:110)
(v, j, s) ∈ [p] × Yv × [k − 1] (cid:12)

Deﬁne search space S (k) =
Find indices (vk, jk, sk) = arg max(v,j,s)∈S(k) ϕ(s)(v, j) and set constraints C(k) = C(sk) ∪ {yvk =
jk}.
for v ∈ [p] do

(cid:54)= j, and (v, j, s) /∈ U (t)(cid:111)
.

(cid:12) y(s)

v

For each j ∈ Yv, use M to set ϕ(k)(v, j) = max (cid:8)ψ(y; w) | y ∈ Y s.t. constraints C(k) hold and yv = j(cid:9).

Set y(k)

v = arg maxj∈Yv ϕ(k)(v, j).

end for
Update U (k+1) = U (k) ∪ {(vk, jk, sk)} and C(sk) = C(sk) ∪ {yvk (cid:54)= jk} and the max-marginal table
ϕ(sk)(v, j) = maxy∈Y,C(sk ),yv=j ψ(y; w) using M.

15: end for
16: return (cid:8)(cid:0)ψ(y(k); w), y(k)(cid:1)(cid:9)K

k=1.

Theorem 46 (Yanover and Weiss [2004]). Suppose the score function ψ is unambiguous, that is, ψ(y(cid:48); w) (cid:54)=
ψ(y(cid:48)(cid:48); w) for all distinct y(cid:48), y(cid:48)(cid:48) ∈ Y. Given an algorithm M that can compute the max-marginals of ψ
exactly, Algo. 10 makes at most 2K calls to M and its output satisﬁes ψ(yk; w) = max(k)
y∈Y ψ(y; w).
Thus, the BMMF algorithm followed by a projection onto the simplex (Algo. 6 in Appendix A) is a correct
implementation of the top-K oracle. It makes 2K calls to M.

Constrained Max-Marginals The algorithm requires computation of max-marginals subject to constraints
of the form yv ∈ Yv for some set Yv ⊆ Yv. This is accomplished by redeﬁning for a constraint yv ∈ Yv:

(cid:40)

ψ(y) =

if yv ∈ Yv
ψ(y),
−∞, otherwise

.

C.2 Max-Marginals Using Graph Cuts

This section recalls a simple procedure to compute max-marginals using graph cuts. Such a construction

was used, for instance, by Kolmogorov and Zabin [2004].

Notation In the literature on graph cut inference, it is customary to work with the energy function, which
is deﬁned as the negative of the augmented score −ψ. For this section, we also assume that the labels are
binary, i.e., Yv = {0, 1} for each v ∈ [p]. Recall the decomposition (17) of the augmented score function

60

Algorithm 11 Max-marginal computation via Graph Cuts
1: Input: Augmented score function ψ(·, ·; w) with Y = {0, 1}p, constraints C of the form yv = b for

b ∈ {0, 1}.

Add to E(cid:48) the (edge, cost) pairs (s → yv, θv;0) and (yv → t, θv;1).

2: Using artiﬁcial source s and sink t, set V (cid:48) = V ∪ {s, t} and E(cid:48) = ∅.
3: for v ∈ [p] do
4:
5: end for
6: for v, v(cid:48) ∈ R such that v < v(cid:48) do
7:

Add to E(cid:48) the (edge, cost) pairs (s → yv, θvv(cid:48);00), (yv(cid:48) → t, θvv(cid:48);11), (yv → yv(cid:48), θvv(cid:48);10), (yv(cid:48) →
yv, θvv(cid:48);01 − θvv(cid:48);00 − θvv(cid:48);11).

Add to E(cid:48) the edge yv → t if b = 0 or edge s → yv if b = 1 with cost +∞.

8: end for
9: for constraint yv = b in C do
10:
11: end for
12: Create graph G(cid:48) = (V (cid:48), E(cid:48)), where parallel edges are merged by adding weights.
13: Compute minimum cost s, t-cut of G(cid:48). Let C be its cost.
14: Create (cid:98)y ∈ {0, 1}p as follows: for each v ∈ V, set (cid:98)yv = 0 if the edge s → v is cut. Else (cid:98)yv = 1.
15: return −C, (cid:98)y.

over nodes and edges. Deﬁne a reparameterization

θv;z(w) = −ψv(z; w) for v ∈ V, z ∈ {0, 1}
if (v, v(cid:48)) ∈ E
θvv(cid:48);z,z(cid:48)(w) = −ψv,v(cid:48)(z, z(cid:48); w) ,

for (v, v(cid:48)) ∈ E, (z, z(cid:48)) ∈ {0, 1}2 .

We then get

−ψ(y) =

θv;z I(yv = z) +

θvv(cid:48);zz(cid:48) I(yv = z) I(yv(cid:48) = z(cid:48)) I((v, v(cid:48)) ∈ E) ,

p
(cid:88)

(cid:88)

v=1

z∈{0,1}

p
(cid:88)

p
(cid:88)

(cid:88)

v=1

v(cid:48)=i+1

z,z(cid:48)∈{0,1}

where we dropped the dependence on w for simplicity. We require the energies to be submodular, i.e., for
every v, v(cid:48) ∈ [p], we have that

θvv(cid:48);00 + θvv(cid:48);11 ≤ θvv(cid:48);01 + θvv(cid:48);10 .

(72)

Also, assume without loss of generality that θv;z, θvv(cid:48);zz(cid:48) are non-negative [Kolmogorov and Zabin, 2004].

Algorithm and Correctness Algo. 11 shows how to compute the max-marginal relative to a single vari-
able yv. The next theorem shows its correctness.

Theorem 47 (Kolmogorov and Zabin [2004]). Given a binary pairwise graphical model with augmented
score function ψ which satisﬁes (72), and a set of constraints C, Algo. 11 returns maxy∈YC ψ(y; w), where
YC denotes the subset of Y that satisﬁes constraints C. Moreover, Algo. 11 requires one maximum ﬂow
computation.

61

C.3 Max-Marginals Using Graph Matchings

The alignment problem that we consider in this section is as follows: given two sets V, V (cid:48), both of equal size
(for simplicity), and a weight function ϕ : V × V (cid:48) → R, the task is to ﬁnd a map σ : V → V (cid:48) so that each
v ∈ V is mapped to a unique z ∈ V (cid:48) and the total weight (cid:80)
v∈V ϕ(v, σ(v)) is maximized. For example, V
and V (cid:48) might represent two natural language sentences and this task is to align the two sentences.

Graphical Model This problem is framed as a graphical model as follows. Suppose V and V (cid:48) are of size
p. Deﬁne y = (y1, · · · , yp) so that yv denotes σ(v). The graph G = (V, E) is constructed as the fully
connected graph over V = {1, · · · , p}. The range Yv of each yv is simply V (cid:48) in the unconstrained case.
Note that when considering constrained max-marginal computations, Yv might be subset of V (cid:48). The score
function ψ is deﬁned as node and edge potentials as in Eq. (17). Again, we suppress dependence of ψ on w
for simplicity. Deﬁne unary and pairwise scores as

ψv(yv) = ϕ(v, yv)

and ψv,v(cid:48)(yv, yv(cid:48)) =

(cid:40)

0, if yv (cid:54)= yv(cid:48)
−∞, otherwise

.

Max Oracle The max oracle with ψ deﬁned as above, or equivalently, the inference problem (3) (cf.
Lemma 7(i)) can be cast as a maximum weight bipartite matching, see e.g., Taskar et al. [2005]. Deﬁne a
fully connected bipartite graph G = (V ∪ V (cid:48), E) with partitions V, V (cid:48), and directed edges from each v ∈ V
to each vertex z ∈ V (cid:48) with weight ϕ(v, z). The maximum weight bipartite matching in this graph G gives
the mapping σ, and thus implements the max oracle. It can be written as the following linear program:

max
{θv,z for (v,z)∈E}

(cid:88)

ϕ(v, z)θv,z ,

s.t.

(v,z)∈E
0 ≤ θv,z ≤ 1 ∀(v, z) ∈ V × V (cid:48)
(cid:88)

θv,z ≤ 1 ∀z ∈ V (cid:48)

v∈V
(cid:88)

z∈V (cid:48)

θv,z ≤ 1 ∀v ∈ V .

Max-Marginal For the graphical model deﬁned above, the max-marginal ψ¯v;¯z is the constrained maxi-
mum weight matching in the graph G deﬁned above subject to the constraint that ¯v is mapped to ¯z. The
linear program above can be modiﬁed to include the constraint θ¯v,¯z = 1:

max
{θv,z for (v,z)∈E}

(cid:88)

ϕ(v, z)θv,z ,

s.t.

(v,z)∈E
0 ≤ θv,z ≤ 1 ∀(v, z) ∈ V × V (cid:48)
(cid:88)

θv,z ≤ 1 ∀z ∈ V (cid:48)

(73)

θv,z ≤ 1 ∀v ∈ V

v∈V
(cid:88)

z∈V (cid:48)
θ¯v,¯z = 1 .

62

Algorithm 12 Max marginal computation via Graph matchings
1: Input: Directed bipartite graph G = (V ∪ V (cid:48), E), weights ϕ : V × V (cid:48) → R.
2: Find a maximum weight bipartite matching σ∗ in the graph G. Let the maximum weight be ψ∗.
3: Deﬁne a weighted residual bipartite graph (cid:98)G = (V ∪ V (cid:48), (cid:98)E), where the set (cid:98)E is populated as follows:
for (v, z) ∈ E, add an edge (v, z) to (cid:98)E with weight 1 − I(σ∗(v) = z), add (z, v) to (cid:98)E with weights
− I(σ∗(v) = z).

4: Find the maximum weight path from every vertex z ∈ V (cid:48) to every vertex v ∈ V and denote this by

∆(z, v).

5: Assign the max-marginals ψv;z = ψ∗ + I(σ∗(v) (cid:54)= z) (∆(z, v) + ϕ(v, z)) for all (v, z) ∈ V × V (cid:48).
6: return Max-marginals ψv;z for all (v, z) ∈ V × V (cid:48).

Algorithm to Compute Max-Marginals Algo. 12, which shows how to compute max-marginals is due
to Duchi et al. [2006]. Its running time complexity is as follows: the initial maximum weight matching
computation takes O(p3) via computation of a maximum ﬂow [Schrijver, 2003, Ch. 10]. Line 4 of Algo. 12
can be performed by the all-pairs shortest paths algorithm [Schrijver, 2003, Ch. 8.4] in time O(p3). Its
correctness is shown by the following theorem:

Theorem 48 (Duchi et al. [2006]). Given a directed bipartite graph G and weights ϕ : V × V (cid:48) → R, the
output ψv;z from Algo. 12 are valid max-marginals, i.e., ψv;z coincides with the optimal value of the linear
program (73). Moreover, Algo. 12 runs in time O(p3) where p = |V | = |V (cid:48)|.

C.4 Proof of Proposition 14

Proposition 14. Consider as inputs an augmented score function ψ(·, ·; w), an integer K > 0 and a smooth-
ing parameter µ > 0. Further, suppose that ψ is unambiguous, that is, ψ(y(cid:48); w) (cid:54)= ψ(y(cid:48)(cid:48); w) for all distinct
y(cid:48), y(cid:48)(cid:48) ∈ Y. Consider one of the two settings:

(A)

the output space Yv = {0, 1} for each v ∈ V, and the function −ψ is submodular (see Appendix C.2
and, in particular, (72) for the precise deﬁnition), or,

(B)

the augmented score corresponds to an alignment task where the inference problem (3) corresponds to
a maximum weight bipartite matching (see Appendix C.3 for a precise deﬁnition).

In these cases, we have the following:

(i) The max oracle can be implemented at a computational complexity of O(p) minimum cut computations

in Case (A), and in time O(p3) in Case (B).

(ii) The top-K oracle can be implemented at a computational complexity of O(pK) minimum cut compu-

tations in Case (A), and in time O(p3K) in Case (B).

(iii) The exp oracle is #P-complete in both cases.

Proof. A set of max-marginals can be computed by an algorithm M deﬁned as follows:

• In Case (A), invoke Algo. 11 a total of 2p times, with yv = 0, and yv = 1 for each v ∈ V. This takes

a total of 2p min-cut computations.

63

Algorithm 13 Top-K best-ﬁrst branch and bound search

1: Input: Augmented score function ψ(·, ·; w), integer K > 0, search space Y, upper bound (cid:98)ψ, split

2: Initialization: Initialize priority queue with single entry Y with priority (cid:98)ψ(Y; w), and solution set S as

strategy.

the empty list.
3: while |S| < K do
4:

Pop (cid:98)Y from the priority queue.
if (cid:98)Y = {(cid:98)y} is a singleton then
Append ((cid:98)y, ψ((cid:98)y; w)) to S.

else

5:

6:

7:

8:

9:

end if

10:
11: end while
12: return S.

Y1, Y2 ← split( (cid:98)Y).
Add Y1 with priority (cid:98)ψ(Y1; w) and Y2 with priority (cid:98)ψ(Y2; w) to the priority queue.

• In Case (B), M is simply Algo. 12, which takes time O(p3).

The max oracle can then be implmented by the decoding in Eq. (21), whose correctness is guaranteed by
Thm. 45. The top-K oracle is implemented by invoking the BMMF algorithm with M deﬁned above,
followed by a projection onto the simplex (Algo. 6 in Appendix A) and its correctness is guaranteed by
Thm. 46. Lastly, the result of exp oracle follows from Jerrum and Sinclair [1993, Thm. 15] in conjunction
with Prop. 9.

C.5

Inference using branch and bound search

Algo. 13 with the input K = 1 is the standard best-ﬁrst branch and bound search algorithm. Effectively, the
top-K oracle is implemented by simply continuing the search procedure until K outputs have been produced
- compare Algo. 13 with inputs K = 1 and K > 1. We now prove the correctness guarantee.

Proposition 15. Consider an augmented score function ψ(·, ·, w), an integer K > 0 and a smoothing
parameter µ > 0. Suppose the upper bound function (cid:98)ψ(·, ·; w) : X × 2Y → R satisﬁes the following
properties:

(a) (cid:98)ψ( (cid:98)Y; w) is ﬁnite for every (cid:98)Y ⊆ Y,

(b) (cid:98)ψ( (cid:98)Y; w) ≥ maxy∈ (cid:98)Y ψ(y; w) for all (cid:98)Y ⊆ Y, and,

(c) (cid:98)ψ({y}; w) = ψ(y; w) for every y ∈ Y.

Then, we have the following:

(i) Algo. 13 with K = 1 is a valid implementation of the max oracle.

(ii) Algo. 13 followed by a projection onto the simplex (Algo. 6 in Appendix A) is a valid implementation of

the top-K oracle.

64

Proof. Suppose at some point during the execution of the algorithm, we have a (cid:98)Y = {(cid:98)y} on Line 5 and that
|S| = k for some 0 ≤ k < K. From the properties of the quality upper bound (cid:98)ψ, and using the fact that {(cid:98)y}
had the highest priority in the priority queue (denoted by (∗)), we get,

ψ((cid:98)y; w) = (cid:98)ψ({(cid:98)y}; w)
(∗)
≥ max
Y ∈P
≥ max
Y ∈P

max
y∈Y

(cid:98)ψ(Y ; w)

ψ(y; w)

(#)
= max
y∈Y−S

ψ(y; w) ,

where the equality (#) followed from the fact that any y ∈ Y exits the priority queue only if it is added to
S. This shows that if a (cid:98)y is added to S, it has a score that is no less than that of any y ∈ Y − S. In other
words, Algo. 13 returns the top-K highest scoring y’s.

D The Casimir Algorithm and Non-Convex Extensions: Missing Proofs

This appendix contains missing proofs from Sections 5 and 6. Throughout, we shall assume that ω is ﬁxed
and drop the subscript in Aω, Dω. Moreover, an unqualiﬁed norm (cid:107)·(cid:107) refers to the Euclidean norm (cid:107) · (cid:107)2.

D.1 Behavior of the Sequence (αk)k≥0

Lemma 21. Given a positive, non-decreasing sequence (κk)k≥1 and λ ≥ 0, consider the sequence (αk)k≥0
deﬁned by (27), where α0 ∈ (0, 1) such that α2
0 ≥ λ/(λ + κ1). Then, we have for every k ≥ 1 that
0 < αk ≤ αk−1 and, α2

k ≥ λ/(λ + κk+1) .

Proof. It is clear that (27) always has a positive root, so the update is well deﬁned. Deﬁne sequences
(ck)k≥1, (dk)k≥0 as

ck =

λ + κk
λ + κk+1

,

and dk =

λ
λ + κk+1

.

Therefore, we have that ckdk−1 = dk, 0 < ck ≤ 1 and 0 ≤ dk < 1. With these in hand, the rule for αk can
be written as

−(ckα2

k−1 − dk) +

k−1 − dk)2 + 4ckα2

k−1

αk =

.

(74)

(cid:113)

(ckα2
2

We show by induction that that dk ≤ α2
satisﬁes the hypothesis for some k ≥ 1. Noting that α2
that

k < 1. The base case holds by assumption. Suppose that αk−1
k−1 − dk ≥ 0, we get

k−1 ≥ dk−1 is equivalent to ckα2

(cid:113)

(ckα2

k−1 − dk)2 + 4ckα2

k−1 ≤

(cid:113)

(ckα2

k−1 − dk)2 + 4ckα2
√
ckαk−1 .

k−1 − dk + 2

= ckα2

k−1 + 2(ckα2

k−1 − dk)(2

ckαk−1)

√

(75)

65

We now conclude from (74) and (75) that
−(ckα2

k−1 − dk) + (ckα2
k−1 − dk + 2
2

√

ckαk−1)

αk ≤

√

=

ckαk−1 ≤ αk−1 < 1 ,

(76)

since ck ≤ 1 and αk−1 < 1. To show the other side, we expand out (74) and apply (75) again to get

α2

k − dk =

(ckα2

(ckα2

=

1
2
1
2
1
2
= (ckα2

≥

k−1 − dk)2 + (ckα2
(cid:16)

2 + (ckα2

k−1 − dk)
k−1 − dk) (cid:0)2 + (ckα2
k−1 − dk)(1 −

√

(ckα2

ckαk−1) ≥ 0 .

k−1 − dk) −

(cid:113)

k−1 − dk)

(ckα2

1
2

(ckα2
(cid:113)

k−1 − dk)2 + 4ckα2
(cid:17)

k−1

k−1 − dk) −

(ckα2

k−1 − dk) − (ckα2

k−1 − dk + 2

k−1 − dk)2 + 4ckα2
√

k−1

ckαk−1)(cid:1)

The fact that (αk)k≥0 is a non-increasing sequence follows from (76).

D.2 Proofs of Corollaries to Theorem 16

We rewrite (30) from Theorem 16 as follows:




F (wk) − F ∗ ≤



k
(cid:89)

j=1

1 − αj−1
1 − δj

(cid:16)



+

1
1 − αk











k
(cid:89)

j=1

1 − αj
1 − δj

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2(cid:17)

+ µkDω

γ0
2







k
(cid:88)

k
(cid:89)

j=2

i=j

1 − αi
1 − δi

 (1 + δ1)µ1Dω +

 (µj−1 − (1 − δj)µj) Dω

 ,

(77)



Next, we have proofs of Corollaries 17 to 20.
Corollary 17. Consider the setting of Thm. 16. Let q = λ
k ≥ 1. Choose α0 =
q . Then, we have,
q and, δk =

√

q
√

2−

√

λ+κ . Suppose λ > 0 and µk = µ, κk = κ, for all

F (wk) − F ∗ ≤

√
q
√
q

3 −
1 −

(cid:18)

µD + 2

1 −

(cid:19)k

√

q
2

√

√

(F (w0) − F ∗) .

Proof. Notice that when α0 =
all k, j, 1−αk
1−δj
conditions as

= 1 −

q, we have, αk =

q for all k. Moreover, for our choice of δk, we get, for
√
q
2 . Under this choice of α0, we have, γ0 = λ. So, we get the dependence on initial

∆0 = F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2 ≤ 2(F (w0) − F ∗) ,

λ
2

by λ-strong convexity of F . The last term of (77) is now,



(cid:18)








(cid:124)


µD
√

1 −

q

1 −

√

q
2
(cid:123)(cid:122)
≤1

(cid:19)k−1

√

q
2

(cid:124)

+

(cid:125)

k
(cid:88)

(cid:18)

j=2

1 −

(cid:123)(cid:122)
(∗)
≤ 1

(cid:19)k−j

√

q
2

≤

2µD
√

1 −

,

q










(cid:125)


66

where (∗) holds since

k
(cid:88)

(cid:18)

1 −

j=2

√

q
2

(cid:19)k−j

∞
(cid:88)

(cid:18)

≤

1 −

j=0

(cid:19)j

√

q
2

=

2
√
q

.

Corollary 18. Consider the setting of Thm. 16. Let q = λ
all k ≥ 1. Choose α0 =

q and, the sequences (µk)k≥1 and (δk)k≥1 as

λ+κ , η = 1 −

√

√
q
2 . Suppose λ > 0 and κk = κ, for

µk = µηk/2 ,

and,

δk =

√

q
√

,

q

2 −

where µ > 0 is any constant. Then, we have,

F (wk) − F ∗ ≤ ηk/2

(cid:20)
2 (F (w0) − F ∗) +

µDω
√
q
1 −

(cid:18)

√

2 −

q +

√

(cid:19)(cid:21)

.

q
√
η

1 −

√

q for each k, and 1−δ

Proof. As previously in Corollary 17, notice that under the speciﬁc parameter choices here, we have, γ0 = λ,
√
q
αk =
2 = η. By λ-strong convexity of F and the fact that γ0 = λ, the
contribution of w0 can be upper bounded by 2(F (w0) − F ∗). Now, we plugging these into (77) and
collecting the terms dependent on δk separately, we get,

1−α = 1 −

F (wk) − F ∗ ≤ 2ηk(F (w0) − F ∗)
(cid:124)
(cid:125)

(cid:123)(cid:122)
=:T1

+ µkD
(cid:124)(cid:123)(cid:122)(cid:125)
=:T2

+

1
√
1 −

q









+

ηkµ1D
(cid:124) (cid:123)(cid:122) (cid:125)
=:T3

k
(cid:88)

j=2
(cid:124)

ηk−j+1(µj−1 − µj)D

+

ηk−j+1µjδjD

.

(78)

(cid:123)(cid:122)
=:T4

(cid:125)

(cid:123)(cid:122)
=:T5








(cid:125)

We shall consider each of these terms. Since ηk ≤ ηk/2, we get T1 ≤ 2ηk/2(F (w0) − F ∗) and T3 =
ηkµ1D ≤ ηkµD ≤ ηk/2µD. Moreover, T2 = µkD = ηk/2µD. Next, using 1 −

η ≤ 1 − η =

√

√
q
2 ,

k
(cid:88)

j=1
(cid:124)

√

T4 =

ηk−j+1(µj−1 − µj)D =

ηk−j+1µη(j−1)/2(1 −

η)D

≤

µD

ηk− j−1

2 =

µDη(k+1)/2

ηj/2 ≤

µD

k−2
(cid:88)

j=0

√

q
2

η(k+1)/2
√
η
1 −

k
(cid:88)

j=2

√

q
2

Similarly, using δj =

q/2η, we have,

T5 =

ηk−j+1µηj/2D

=

µD

ηk−j/2 ≤

µD

√

q
2η

√

q
2

k
(cid:88)

j=1

√
q
2

ηk/2
√

1 −

η

.

Plugging these into (78) completes the proof.

67

k
(cid:88)

j=2

µD

ηk/2
√

1 −

η

.

≤

√

k
(cid:88)

j=2
√

q
2
√

q
2

k
(cid:88)

j=1

Corollary 19. Consider the setting of Thm. 16. Suppose µk = µ, κk = κ, for all k ≥ 1 and λ = 0. Choose
α0 =

and δk = 1

(1+k)2 . Then, we have,

5−1
2

√

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2
2

+ µDω

1 +

κ
2

(cid:17)

(cid:18)

12
k + 2

+

30
(k + 2)2

(cid:19)

.

(cid:16)

8
(k + 2)2

Proof. Firstly, note that γ0 = κ α2
0
1−α0

= κ. Now, deﬁne

Ak =

(1 − αi), and, Bk =

(1 − δi) .

k
(cid:89)

i=0

k
(cid:89)

i=1

We have,

Therefore,

k
(cid:89)

(cid:18)

1 −

Bk =

i=1

1
(i + 1)2

(cid:19)

=

k
(cid:89)

i=1

i(i + 2)
(i + 1)2 =

1
2

+

1
2(k + 1)

.

(79)

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:16)

Ak−1
Bk

+

µD
1 − α0





k
(cid:89)

j=1

1 − αj−1
1 − δk

γ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)


+ µD

 (1 + δ1) + µD

≤

(cid:16)

Ak−1
Bk

F (w0) − F ∗ +

γ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)
(cid:125)

+µD

(cid:124)

+

Ak−1
Bk

5
4 µD
1 − α0
(cid:123)(cid:122)
(cid:124)
=:T2

(cid:125)

(cid:123)(cid:122)
=:T1

+ µD

(cid:124)

k
(cid:88)

j=2

Ak−1/Aj−2
Bk/Bj−1

(cid:123)(cid:122)
=:T3

.

δj
1 − αj−1
(cid:125)

k
(cid:88)

k
(cid:89)





j=2

i=j





1 − αi−1
1 − δi

δj
1 − αj−1

From Lemma 52, which analyzes the evolution of (αk) and (Ak), we get that
αk ≤ 2

k+3 for k ≥ 0. Since Bk ≥ 1
2 ,

2

(k+2)2 ≤ Ak−1 ≤ 4

(k+2)2 and

T1 ≤

(cid:16)

8
(k + 2)2

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2(cid:17)

.

γ0
2

Moreover, since α0 ≤ 2/3,

Lastly, we have,

T2 ≤

30
(k + 2)2 .

T3 ≤

k
(cid:88)

j=2

4
(k + 2)2 ×

(j + 1)2
2

× 2

(cid:19)

(cid:18) 1
2

+

1
2j

×

1
(j + 1)2 ×

1
1 − 2/j+2

≤ 2

2
(k + 2)2

k
(cid:88)

j=2

j + 2
j

4

≤

(k + 2)2 (k − 1 + 2 log k) ≤

12
k + 2

,

where we have used the simpliﬁcations (cid:80)k

j=2 1/k ≤ log k and k − 1 + 2 log k ≤ 3k.

68

Corollary 20. Consider the setting of Thm. 16 with λ = 0. Choose α0 =
constants κ, µ, deﬁne sequences (κk)k≥1, (µk)k≥1, (δk)k≥1 as

√

5−1
2

, and for some non-negative

κk = κ k , µk =

and,

δk =

µ
k

1
(k + 1)2 .

Then, for k ≥ 2, we have,

F (wk) − F ∗ ≤

log(k + 1)
k + 1

(cid:0)2(F (w0) − F ∗) + κ(cid:107)w0 − w∗(cid:107)2

2 + 27µDω

(cid:1) .

(80)

For the ﬁrst iteration (i.e., k = 1), this bound is off by a constant factor 1/ log 2.

Proof. Notice that γ0 = κ1

= κ. As in Corollary 19, deﬁne

α2
0
1−α0

From Lemma 53 and (79) respectively, we have for k ≥ 1,

Ak =

(1 − αi) ,

and, Bk =

(1 − δi) .

k
(cid:89)

i=0

1 − 1√
2
k + 1

≤ Ak ≤

and,

≤ Bk ≤ 1 .

1
k + 2

,

k
(cid:89)

i=1

1
2

Now, invoking Theorem 16, we get,

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:16)

Ak−1
Bk

(cid:124)

k
(cid:88)

j=2
(cid:124)

Ak−1/Aj−1
Bk/Bj−1

(cid:123)(cid:122)
=:T4

γ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)
(cid:125)

+

+ µkD
(cid:124)(cid:123)(cid:122)(cid:125)
=:T2

1
1 − α0
(cid:124)

Ak−1
Bk

(cid:123)(cid:122)
=:T3

(cid:123)(cid:122)
=:T1

µ1D(1 + δ1)

+

(cid:125)

(µj−1 − µj)D

+

k
(cid:88)

j=2
(cid:124)

(cid:125)

Ak−1/Aj−1
Bk/Bj−1

δjµjD

.

(cid:123)(cid:122)
=:T5

(cid:125)

(81)

We shall bound each of these terms as follows.

T1 =

(cid:16)

Ak−1
Bk

F (w0) − F ∗ +

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2(cid:17)

,

κ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)

=

γ0
2

T2 = µkD =

µD
k

(cid:16)

2
k + 1

,

≤

2µD
k + 1
2
k + 1

T3 =

1
1 − α0
where we used the fact that α0 ≤ 2/3. Next, using (cid:80)k
1 + log(k − 1), we get,

Ak−1
Bk

µ1D(1 + δ1) ≤ 3 ×

× µ ×

15
2
j=2 1/j ≤ 1 + (cid:82) k−1
j=2 1/(j − 1) = 1 + (cid:80)k−1

µD
k + 1

D =

5
4

,

1

dx/x =

T4 =

k
(cid:88)

j=2
√

2
k + 1

·

√

≤ 2

2(

2 + 1)µD

(cid:19)

(cid:18) µ

−

µ
j

j
1 − 1√
j − 1
2
(cid:18) 1 + log(k + 1)
k + 1

(cid:19)

.

√

√

D = 2

2(

2 + 1)

µD
k + 1

k
(cid:88)

j=2

1
j − 1

69

Moreover, from (cid:80)k

j=2 1/(j + 1)2 ≤ (cid:82) k+1

2

dx/x2 ≤ 1/2, it follows that

T5 =

k
(cid:88)

j=2

2
k + 1

·

j
1 − 1√
2

µ
j

·

1

√
(j + 1)2 D = 2

√

2(

2 + 1)

µD
k + 1

k
(cid:88)

j=2

1
(j + 1)2 ≤

√

√

2(

2 + 1)

µD
k + 1

.

Plugging these back into (81), we get

F (wk) − F ∗ ≤

2
k + 1
µD
k + 1

(cid:16)

F (wk) − F ∗ +
(cid:18)

√

κ
2

(cid:107)w0 − w∗(cid:107)2(cid:17)
(cid:19)
√
√

+

√

2 +

+

2(1 +

2)

+ 2

2(1 +

2)µD

15
2

1 + log(k + 1)
k + 1

.

To complete the proof, note that log(k + 1) ≥ 1 for k ≥ 2 and numerically verify that the coefﬁcient of µD
is smaller than 27.

D.3

Inner Loop Complexity Analysis for Casimir

Before proving Prop. 27, the following lemmas will be helpful. First, we present a lemma from Lin et al.
[2018, Lemma 11] about the expected number of iterations a randomized linearly convergent ﬁrst order
methods requires to achieve a certain target accuracy.
Lemma 49. Let M be a linearly convergent algorithm and f ∈ FL,λ. Deﬁne f ∗ = minw∈Rd f (w). Given
a starting point w0 and a target accuracy (cid:15), let (wk)k≥0 be the sequence of iterates generated by M. Deﬁne
T ((cid:15)) = inf {k ≥ 0 | f (wk) − f ∗ ≤ (cid:15)} . We then have,

E[T ((cid:15))] ≤

1
τ (L, λ)

log

(cid:18) 2C(L, λ)
τ (L, λ)(cid:15)

(cid:19)

(f (w0) − f ∗)

+ 1 .

(82)

This next lemma is due to Lin et al. [2018, Lemma 14, Prop. 15].

Lemma 50. Consider Fµω,κ(· ; z) deﬁned in Eq. (25) and let δ ∈ [0, 1). Let (cid:98)F ∗ = minw∈Rd Fµω,κ(w; z)
and (cid:98)w∗ = arg minw∈Rd Fµω,κ(w; z). Further let Fµω(· ; z) be Lµω-smooth. We then have the following:
Lµω + κ
2

Fµω,κ(z; z) − (cid:98)F ∗ ≤

(cid:107)z − (cid:98)w∗(cid:107)2
2 ,

and,

Fµω,κ( (cid:98)w; z) − (cid:98)F ∗ ≤

(cid:107)z − (cid:98)w∗(cid:107)2

2 =⇒ Fµω,κ( (cid:98)w; z) − (cid:98)F ∗ ≤

(cid:107) (cid:98)w − z(cid:107)2
2 .

δκ
2

δκ
8

We now restate and prove Prop. 27.

Proposition 27. Consider Fµω,κ(· ; z) deﬁned in Eq. (25), and a linearly convergent algorithm M with
parameters C, τ . Let δ ∈ [0, 1). Suppose Fµω is Lµω-smooth and λ-strongly convex. Then the expected
number of iterations E[ (cid:98)T ] of M when started at z in order to obtain (cid:98)w ∈ Rd that satisﬁes

Fµω,κ( (cid:98)w; z) − min
w

Fµω,κ(w; z) ≤ δκ

2 (cid:107)w − z(cid:107)2

2

(83)

is upper bounded by

E[ (cid:98)T ] ≤

1
τ (Lµω + κ, λ + κ)

log

(cid:18) 8C(Lµω + κ, λ + κ)
τ (Lµω + κ, λ + κ)

·

Lµω + κ
κδ

(cid:19)

+ 1 .

Proof. In order to invoke Lemma 49, we must appropriately set (cid:15) for (cid:98)w to satisfy (83) and then bound the
ratio (Fµω,κ(z; z) − (cid:98)F ∗)/(cid:15). Firstly, Lemma 50 tells us that choosing (cid:15) = δkκk
2 guarantees that
the (cid:98)w so obtained satisﬁes (83), where (cid:98)w∗ := arg minw∈Rd Fµω,κ(w; z), Therefore, (Fµω,κ(z; z) − (cid:98)F ∗)/(cid:15)
is bounded from above by 4(Lµω + κ)/κδ.

8 (cid:107)zk−1 − (cid:98)w∗(cid:107)2

70

D.4

Information Based Complexity of Casimir-SVRG

Presented below are the proofs of Propositions 29 to 32 from Section 5.3. We use the following values of
C, τ , see e.g., Hofmann et al. [2015].

τ (L, λ) =

8 L

1
λ + n
(cid:32)

L
λ

1
λ + n(cid:1)
(cid:33)

.

≥

8 (cid:0) L
n L
λ
λ + n

8 L

C(L, λ) =

1 +

κ =

(cid:40) A

µn − λ , if A
λ , otherwise

µn > 4λ

,

(cid:32)

(cid:114)

E[N ] ≤ (cid:101)O

n +

(cid:33)

.

AωDωn
λ(cid:15)

K ≤

log

2
√
q

(cid:18) 2∆F0

(cid:19)

(cid:15) − cqµD

,

Proposition 29. Consider the setting of Thm. 16 with λ > 0 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as
the inner solver with parameters: µk = µ = (cid:15)/10Dω, κk = k chosen as

q = λ/(λ + κ), α0 =
q/(2 −
F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

q, and δ =

√

√

√

q). Then, the number of iterations N to obtain w such that

Proof. We use shorthand A := Aω, D := Dω, Lµ = λ + A/µ and ∆F0 = F (w0) − F ∗. Let C, τ be the
linear convergence parameters of SVRG. From Cor. 17, the number of outer iterations K required to obtain
F (wK) − F ∗ ≤ (cid:15) is

where cq = (3 −
√
δk =

q/(2 −

√

√

q),

√

q)/(1 −

q). From Prop. 27, the number Tk of inner iterations for inner loop k is, from

E[Tk] ≤

1
τ (Lµ + κ, λ + κ)
2
τ (Lµ + κ, λ + κ)

log

log

(cid:18) 8C(Lµ + κ, λ + κ)
τ (Lµ + κ, λ + κ)
(cid:18) 8C(Lµ + κ, λ + κ)
τ (Lµ + κ, λ + κ)

·

·

Lµ + κ
κ
Lµ + κ
κ

≤

2 −
√

2 −
√

·

·

√

q
√

q

(cid:19)

q

+ 1

(cid:19)

q

.

Let the total number N of iterations of SVRG to obtain an iterate w that satisﬁes F (w) − F ∗ ≤ (cid:15). Next,
we upper bound E[N ] ≤ (cid:80)K
i=1

E[Tk] as

E[N ] ≤

√

4
qτ (Lµ + κ, λ + κ)

log

(cid:18) 8C(Lµ + κ, λ + κ)
τ (Lµ + κ, λ + κ)

Lµ + κ
κ

2 −
√

q

log

(cid:18) 2(F (w0) − F ∗)
(cid:15) − cqµD

(cid:19)

.

√

(cid:19)

q

(84)

Next, we shall plug in C, τ for SVRG in two different cases:

• Case 1: A > 4µλn, in which case κ + λ = A/(µn) and q < 1/4.

71

• Case 2: A ≤ 4µλn, in which case, κ = λ and q = 1/2.

We ﬁrst consider the term outside the logarithm. It is, up to constants,

(cid:18)

n +

1
√
q

A
µ(λ + κ)

(cid:19)

(cid:114)

= n

λ + κ
λ

+

A
µ(cid:112)λ(λ + κ)

.

For Case 1, plug in κ + λ = A/(µn) so this term evaluates to (cid:112)ADn/(λ(cid:15)). For Case 2, we use the fact
that A ≤ 4µλn so that this term can be upper bounded by,

(cid:32)(cid:114)

n

λ + κ
λ

+ 4

(cid:114) λ

(cid:33)

λ + κ

√

= 3

2n ,

since we chose κ = λ. It remains to consider the logarithmic terms. Noting that κ ≥ λ always, it follows
that the ﬁrst log term of (84) is clearly logarithmic in the problem parameters.

As for the second logarithmic term, we must evaluate cq. For Case 1, we have that q < 1/4 so that
cq < 5 and cqµD < (cid:15)/2. For Case 2, we get that q = 1/2 and cq < 8 so that cqµD < 4(cid:15)/5. Thus, the
second log term of (84) is also logarithmic in problem parameters.

Proposition 30. Consider the setting of Thm. 16. Suppose λ > 0 and κk = κ, for all k ≥ 1 and that α0,
q/2. If we run Algo. 4 with
(µk)k≥1 and (δk)k≥1 are chosen as in Cor. 18, with q = λ/(λ+κ) and η = 1−
SVRG as the inner solver with these parameters, the number of iterations N of SVRG required to obtain w
such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

√

(cid:18)

E[N ] ≤ (cid:101)O

n +

(cid:18)

Aω
µ(λ + κ)(cid:15)

F (w0) − F ∗ +

µDω
√
1 −

q

(cid:19)(cid:19)

.

Proof. We continue to use shorthand A := Aω, D := Dω. First, let us consider the minimum number of
outer iterations K required to achieve F (wK) − F ∗ ≤ (cid:15). From Cor. 18, if we have η−K/2∆0 ≤ (cid:15), or,

K ≥ Kmin :=

log (∆0/(cid:15))
η(cid:1) .
log (cid:0)1/

√

For this smallest value, we have,

µKmin = µηKmin/2 =

µ(cid:15)
∆0

.

(85)

Let C, τ be the linear convergence parameters of SVRG, and deﬁne Lk := λ + A/µk for each k ≥ 1.
Further, let T (cid:48) be such that

T (cid:48) ≥

max
k∈{1,··· ,Kmin}

log

(cid:18)
8

C(Lk + κ, λ + κ)
τ (Lk + κ, λ + κ)

Lk + κ
κδ

(cid:19)

.

72

Then, the total complexity is, from Prop. 27, (ignoring absolute constants)

E[N ] ≤

n +

λ + κ + A
µk
λ + κ

(cid:33)

T (cid:48)

(cid:32)

Kmin(cid:88)

k=1

Kmin(cid:88)

(cid:18)

k=1
(cid:32)

(cid:32)

(cid:32)

=

n + 1 +

(cid:19)

η−k/2

T (cid:48)

A/µ
λ + κ

=

Kmin(n + 1) +

≤

Kmin(n + 1) +

A/µ
λ + κ

A/µ
λ + κ

Kmin(cid:88)

(cid:33)

η−k/2

T (cid:48)

k=1
η−Kmin/2
1 − η1/2

(cid:33)

T (cid:48)

=

(n + 1)

log (cid:0) ∆0
(cid:15)
log(1/√

(cid:1)

η)

+

A/µ
λ + κ

1
√
1 −

η

∆0
(cid:15)

(cid:33)

T (cid:48) .

(86)

It remains to bound T (cid:48). Here, we use λ + A
T (cid:48) is logarithmic in ∆0/(cid:15), n, AD, µ, κ, λ−1.

µ ≤ Lk ≤ λ + A
µK

for all k ≤ K together with (85) to note that

Proposition 31. Consider the setting of Thm. 16 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as the inner
√
, δk = 1/(k + 1)2, and κk = κ = Aω/µ(n + 1).
solver with parameters: µk = µ = (cid:15)/20Dω, α0 =
Then, the number of iterations N to get a point w such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

5−1
2

(cid:32)

(cid:114)

E[N ] ≤ (cid:101)O

n

F (w0) − F ∗
(cid:15)

(cid:112)

+

AωDωn

(cid:107)w0 − w∗(cid:107)2
(cid:15)

(cid:33)

.

Proof. We use shorthand A := Aω, D := Dω, Lµ = A/µ and ∆F0 = F (w0) − F ∗ + κ
2 (cid:107)w0 − w∗(cid:107)2.
Further, let C, τ be the linear convergence parameters of SVRG. In Cor. 19, the fact that K ≥ 1 allows us to
bound the contribution of the smoothing as 10µD. So, we get that the number of outer iterations K required
to get F (wK) − F ∗ ≤ (cid:15) can be bounded as

K + 1 ≤

(cid:115)

8∆F0
(cid:15) − 10µD

.

Moreover, from our choice δk = 1/(k + 1)2, the number of inner iterations Tk for inner loop k is, from
Prop. 27,

E[Tk] ≤

1
τ (Lµ + κ, κ)
2
τ (Lµ + κ, κ)

log

log

(cid:18) 8C(Lµ + κ, κ)
τ (Lµ + κ, κ)
(cid:18) 8C(Lµ + κ, κ)
τ (Lµ + κ, κ)

·

·

Lµ + κ
κ
Lµ + κ
κ

≤

(cid:19)

· (k + 1)2

+ 1

·

8∆F0
(cid:15) − 10µD

(cid:19)

.

Next, we consider the total number N of iterations of SVRG to obtain an iterate w such that F (w) −

F ∗ ≤ (cid:15). Using the fact that E[N ] ≤ (cid:80)K
i=1

E[Tk], we bound it as

E[N ] ≤

1
τ (Lµ + κ, κ)

8∆F0
(cid:15) − 10µD

log

(cid:18) 64C(Lµ + κ, κ)
τ (Lµ + κ, κ)

Lµ + κ
κ

∆F0
(cid:15) − 10µD

(cid:19)

.

(cid:115)

(87)

73

Now, we plug into (87) the values of C, τ for SVRG. Note that κ = Lµ/(n + 1). So we have,

1
τ (Lµ + κ, κ)

= 8

(cid:18) Lµ + κ
κ

(cid:19)

+ n

= 16(n + 1) , and,

C(Lµ + κ, κ) =

Lµ + κ
κ

(cid:32)

1 +

(cid:33)

n Lµ+κ
κ
8 L+κ
κ + n

≤ (n + 2) (cid:0)1 + n
8

(cid:1) .

It now remains to assign µ = (cid:15)/(20D) and plug C, τ from above into (87), noting that κ = 20AD/((cid:15)(n + 1)).

Proposition 32. Consider the setting of Thm. 16. Suppose λ = 0 and that α0, (µk)k≥1,(κk)k≥1 and (δk)k≥1
are chosen as in Cor. 20. If we run Algo. 4 with SVRG as the inner solver with these parameters, the number
of iterations N of SVRG required to obtain w such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

E[N ] ≤ (cid:101)O

(cid:0)F (w0) − F ∗ + κ(cid:107)w0 − w∗(cid:107)2

2 + µD(cid:1)

(cid:18) 1
(cid:15)

(cid:18)

n +

(cid:19)(cid:19)

.

Aω
µκ

Proof. Deﬁne short hand A := Aω, D := Dω and

∆0 := 2(F (w0) − F ∗) + κ(cid:107)w0 − w∗(cid:107)2 + 27µD .

From Cor. 20, the number of iterations K required to obtain F (wK) − F ∗ ≤ log(K+1)
Lemma 54),

K+1 ∆0 ≤ (cid:15) is (see

K + 1 =

2∆0
(cid:15)

log

2∆0
(cid:15)

.

Let C, τ be such that SVRG is linearly convergent with parameters C, τ , and deﬁne Lk := A/µk for each
k ≥ 1. Further, let T (cid:48) be such that

T (cid:48) ≥ max

k∈{1,··· ,K}

(cid:18)
8

log

C(Lk + κ, κ)
τ (Lk + κ, κ)

Lk + κ
κδk

(cid:19)

.

Clearly, T (cid:48) is logarithmic in K, n, AD, µ, κ. From Prop. 27, the minimum total complexity is (ignoring

absolute constants)

E[N ] =

K
(cid:88)

(cid:18)

n +

A/µk + κk
κk

(cid:19)

T (cid:48)

n + 1 +

(cid:19)

T (cid:48)

A
µkκk

=

=

(cid:18)

(cid:18)

k=1
K
(cid:88)

k=1
K
(cid:88)

k=1
(cid:18)

n + 1 +

(cid:19)

T (cid:48)

A
µκ
(cid:19)

≤

n + 1 +

KT (cid:48) ,

A
µκ

and plugging in K from (89) completes the proof.

74

(88)

(89)

(90)

D.5 Prox-Linear Convergence Analysis

We ﬁrst prove Lemma 34 that speciﬁes the assumption required by the prox-linear in the case of structured
prediction.
Lemma 34. Consider the structural hinge loss f (w) = maxy∈Y ψ(y; w) = h ◦ g(w) where h, g are as
deﬁned in (6). If the mapping w (cid:55)→ ψ(y; w) is L-smooth with respect to (cid:107) · (cid:107)2 for all y ∈ Y, then it holds
for all w, z ∈ Rd that

|h(g(w + z)) − h(g(w) + ∇g(w)z)| ≤

L
2

(cid:107)z(cid:107)2
2 .

Proof. For any A ∈ Rm×d and w ∈ Rd, and (cid:107)A(cid:107)2,1 deﬁned in (2), notice that

(cid:107)Aw(cid:107)∞ ≤ (cid:107)A(cid:107)2,1(cid:107)w(cid:107)2 .

(91)

Now using the fact that max function h satisﬁes |h(u(cid:48)) − h(u)| ≤ (cid:107)u(cid:48) − u(cid:107)∞ and the fundamental theorem
of calculus (∗), we deduce

|h(g(w + z)) − h(g(w) + ∇g(w)z)| ≤ (cid:107)g(w + z) − (g(w) + ∇g(w)z) (cid:107)∞
(cid:13)
(∗)
(cid:13)
≤
(cid:13)
(cid:13)∞

(∇g(w + tz) − ∇g(w))z dt

(cid:90) 1

(cid:107)∇g(w + tz) − ∇g(w)(cid:107)2,1(cid:107)z(cid:107)2 dt .

(92)

Note that the deﬁnition (2) can equivalently be stated as (cid:107)A(cid:107)2,1 = max(cid:107)u(cid:107)1≤1 (cid:107)A(cid:62)u(cid:107)2. Given u ∈ Rm,
we index its entries uy by y ∈ Y. Then, the matrix norm in (92) can be simpliﬁed as

(cid:107)∇g(w + tz) − ∇g(w)(cid:107)2,1 = max
(cid:107)u(cid:107)1≤1

uy(∇ψ(y; w + tz) − ∇ψ(y; w))

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

|uy|(cid:107)∇ψ(y; w + tz) − ∇ψ(y; w)(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
0
(cid:90) 1

0

(91)
≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

y∈Y
(cid:88)

y∈Y

≤ max
(cid:107)u(cid:107)1≤1

≤ Lt(cid:107)z(cid:107)2 ,

from the L-smoothness of ψ. Plugging this back into (92) completes the proof. The bound on the smothing
approximation holds similarly by noticing that if h is 1-Lipschitz then hµω too since ∇hµω(u) ∈ dom h∗
for any u ∈ dom h.

D.6

Information Based Complexity of the Prox-Linear Algorithm with Casimir-SVRG

Proposition 37. Consider the setting of Thm. 35. Suppose the sequence {(cid:15)k}k≥1 satisﬁes (cid:15)k = (cid:15)0/k for
some (cid:15)0 > 0 and that the subproblem of Line 3 of Algo. 5 is solved using Casimir-SVRG with the settings
of Prop. 29. Then, total number of SVRG iterations N required to produce a w such that (cid:107)(cid:37)η(w)(cid:107)2 ≤ (cid:15) is
bounded as

E[N ] ≤ (cid:101)O





n
η(cid:15)2 (F (w0) − F ∗ + (cid:15)0) +

(cid:113)

AωDωn(cid:15)−1
0
η(cid:15)3



(F (w0) − F ∗ + (cid:15)0)3/2

 .

75

Proof. First note that (cid:80)K
k=1 k−1 ≤ 4(cid:15)0 log K for K ≥ 2. Let ∆F0 := F (w0) − F ∗ and use
shorthand A, D for Aω, Dω respectively. From Thm. 35, the number K of prox-linear iterations required to
ﬁnd a w such that (cid:107)(cid:37)η(w)(cid:107)2 ≤ (cid:15) must satisfy

k=1 (cid:15)k ≤ (cid:15)0

(cid:80)K

For this, it sufﬁces to have (see e.g., Lemma 54)

2
ηK

(∆F0 + 4(cid:15)0 log K) ≤ (cid:15) .

K ≥

4(∆F0 + 4(cid:15)0)
η(cid:15)2

log

(cid:18) 4(∆F0 + 4(cid:15)0)
η(cid:15)2

(cid:19)

.

Before we can invoke Prop. 29, we need to bound the dependence of each inner loop on its warm start:
Fη(wk−1; wk−1) − Fη(w∗
k; wk−1) in terms of problem parameters, where w∗
k = arg minw Fη(w; wk−1)
is the exact result of an exact prox-linear step. We note that Fη(wk−1; wk−1) = F (wk−1) ≤ F (w0), by
Line 4 of Algo. 5. Moreover, from η ≤ 1/L and Asmp. 33, we have,

Fη(w∗

k; wk−1) =

h(cid:0)g(i)(wk−1) + ∇g(i)(wk−1)(w∗

k − wk−1)(cid:1) +

(cid:107)w∗

k(cid:107)2

2 +

(cid:107)w∗

k − wk−1(cid:107)2
2

h(cid:0)g(i)(wk−1) + ∇g(i)(wk−1)(w∗

k − wk−1)(cid:1) +

(cid:107)w∗

k(cid:107)2

2 +

(cid:107)w∗

k − wk−1(cid:107)2
2

λ
2

λ
2

1
2η

L
2

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

1
n

1
n

1
n

≥

≥

h(cid:0)g(i)(w∗

k)(cid:1) +

(cid:107)w∗

k(cid:107)2
2

λ
2

i=1
= F (w∗

k) ≥ F ∗ .

Thus, we bound Fη(wk−1; wk−1) − Fη(w∗
k; wk−1) ≤ ∆F0. We now invoke Prop. 29 and collect all
constants and terms logarithmic in n, (cid:15)−1, (cid:15)−1
0 , ∆F0, η−1, AωDω in T , T (cid:48), T (cid:48)(cid:48). We note that all terms in the
logarithm in Prop. 29 are logarithmic in the problem parameters here. Letting Nk be the number of SVRG
iterations required for iteration k, we get,

E[N ] =

E[Nk] ≤

(cid:32)

K
(cid:88)

n +

(cid:33)

T

(cid:114) ηADn
(cid:15)k
(cid:33)(cid:35)

√

k

T

k=1
(cid:114) ηADn
(cid:15)0
(cid:114) ηADn
(cid:15)0

(cid:32) K
(cid:88)

k=1
(cid:35)

K3/2

T (cid:48)

K
(cid:88)

k=1
(cid:34)

≤

nK +

≤

nK +

(cid:34)

(cid:34)

(cid:34)

≤

=

n
η(cid:15)2 (∆F0 + (cid:15)0) +

(cid:114) ηADn
(cid:15)0

(cid:18) ∆F0 + (cid:15)0
η(cid:15)2

(cid:19)3/2(cid:35)

T (cid:48)(cid:48)

n
η(cid:15)2 (∆F0 + (cid:15)0) +

ADn
η(cid:15)3

(∆F0 + (cid:15)0)3/2
√
(cid:15)0

(cid:35)

T (cid:48)(cid:48) .

√

76

D.7 Some Helper Lemmas

The ﬁrst lemma is a property of the squared Euclidean norm from Lin et al. [2018, Lemma 5], which we
restate here.

Lemma 51. For any vectors, w, z, r ∈ Rd, we have, for any θ > 0,

(cid:107)w − z(cid:107)2 ≥ (1 − θ)(cid:107)w − r(cid:107)2 +

1 −

(cid:107)r − z(cid:107)2 .

(cid:18)

(cid:19)

1
θ

The next lemmas consider rates of the sequences (αk) and (Ak) under different recursions.

Lemma 52. Deﬁne a sequence (αk)k≥0 as

Then this sequence satisﬁes

Moreover, Ak := (cid:81)k

j=0(1 − αk) satisﬁes

√

5 − 1
α0 =
2
k = (1 − αk)α2
α2

k−1 .

√

2
k + 3

≤ αk ≤

2
k + 3

.

2

(k + 3)2 ≤ Ak ≤

4
(k + 3)2 .

Proof. Notice that α0 satisﬁes α2
Hence, we can deﬁne a sequence (bk)k≥0 such that bk := 1/αk. It satisﬁes the recurrence, b2
for k ≥ 1, or in other words, bk = 1
2

0 = 1 − α0. Further, it is clear from deﬁnition that αk ∈ (0, 1) ∀k ≥ 0.
k−1

. Form this we get,

k − bk = b2

1 + 4b2

1 +

k−1

(cid:113)

(cid:16)

(cid:17)

bk ≥ bk−1 +

≥ b0 +

≥

+

k
2

3
2

k
2

.

1
2

since b0 =

. This gives us the upper bound on αk. Moreover, unrolling the recursion,

√

5+1
2

k = (1 − αk)α2
α2

k−1 = Ak

α2
0
1 − α0

= Ak .

(93)

Since αk ≤ 2/(k + 3), (93) yields the upper bound on Ak. The upper bound on αk again gives us,

Ak ≥

k
(cid:89)

(cid:18)

1 −

i=0

(cid:19)

2
i + 3

=

2
(k + 2)(k + 3)

≥

2
(k + 3)2 ,

to get the lower bound on Ak. Invoking (93) again to obtain the lower bound on αk completes the proof.

The next lemma considers the evolution of the sequences (αk) and (Ak) with a different recursion.

77

Lemma 53. Consider a sequence (αk)k≥0 deﬁned by α0 =

, and αk+1 as the non-negative root of

Further, deﬁne

Then, we have for all k ≥ 0,

√

5−1
2

α2
k
1 − αk

= α2

k−1

k
k + 1

.

Ak =

(1 − αi) .

k
(cid:89)

i=0

1
k + 1

(cid:18)

1 −

(cid:19)

1
√
2

≤ Ak ≤

1
k + 2

.

Proof. Deﬁne a sequence (bk)k≥0 such that bk = 1/αk, for each k. This is well-deﬁned because αk (cid:54)=
0, which may be veriﬁed by induction. This sequence satisﬁes the recursion for k ≥ 1: bk(bk − 1) =
(cid:0) k+1
k

(cid:1) bk−1. From this recursion, we get,

(cid:32)

(cid:115)

bk =

1 +

1 + 4b2

k−1

(cid:19)(cid:33)

(cid:18) k + 1
k

1
2

1
2

1
2
√

≥

≥

+ bk−1
(cid:32)

(cid:114)

1 +

(cid:114)

k + 1
k

k + 1
k

k + 1
2

k + 1

√

=

(∗)
≥

(cid:16)

√

(cid:16)√

1/

2 + · · · + 1/

k + 1

+ b0

k + 1

+ · · · +

+ b0

k + 1

(cid:114)

(cid:33)

k + 1
2

√

√

(cid:17)
2

(cid:17)

√

√

√

√

(cid:16)√

k + 2 + b0 −
√

=

k + 1

k + 2 + b0 −

k + 1 ≥ (cid:82) k+2

2

dx√
x = 2(

√

k + 2 −

2) . Since

√

(cid:17)
2

,

√

where (∗) followed from noting that 1/

2 + · · · + 1/

b0 = 1/α0 =

2, we have, for k ≥ 1,

√

5+1
2 >

√

αk ≤

√

1

√

k + 1(

k + 2 + b0 −

2)

√

≤

√

1
√
k + 1

.

k + 2

This relation also clearly holds for k = 0. Next, we claim that

Ak = (k + 1)α2

k ≤

√

k + 1
√

(

k + 1

k + 2)2

=

1
k + 2

.

Indeed, this is true because

For the lower bound, we have,

k = (1 − αk)α2
α2

k−1

k
k + 1

= Ak

α2
0
1 − α0

1
k + 1

=

Ak
k + 1

.

Ak =

(1 − αi) ≥

1 −

√

k
(cid:89)

(cid:18)

i=0

1
√
i + 1

i + 2

(cid:19)

(cid:18)

≥

1 −

(cid:19) k
(cid:89)

(cid:18)

1 −

1
√
2

(cid:19)

=

1
i + 1

1 − 1√
2
k + 1

.

i=1

k
(cid:89)

i=0

(94)

(95)

(96)

(97)

78

Lemma 54. Fix some (cid:15) > 0. If k ≥ 2

(cid:15) log 2

(cid:15) , then we have that log k

k ≤ (cid:15).

Proof. We have, since log x ≤ x for x > 0,

log k
k

≤

log 2

(cid:15) + log log 2
(cid:15) log 2

2

(cid:15)

(cid:15)

(cid:32)

(cid:15)
2

=

1 +

(cid:33)

log log 2
(cid:15)
log 2
(cid:15)

≤ (cid:15) .

E Experiments: Extended Evaluation

Given here are plots for all missing classes of PASCAL VOC 2007. Figures 8 to 10 contain the extension of
Figure 3 while Figures 11 to 13 contain the extension of Figure 4 to all classes.

79

Figure 8: Comparison of convex optimization algorithms for the task of visual object localization on PAS-
CAL VOC 2007 for λ = 10/n for all other classes (1/3).

80

Figure 9: Comparison of convex optimization algorithms for the task of visual object localization on PAS-
CAL VOC 2007 for λ = 10/n for all other classes (2/3).

81

Figure 10: Comparison of convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 10/n for all other classes (3/3).

82

Figure 11: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n for all other classes (1/3).

83

Figure 12: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n for all other classes (2/3).

84

Figure 13: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n for all other classes (3/3).

85

9
1
0
2
 
b
e
F
 
8
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
8
2
2
3
0
.
2
0
9
1
:
v
i
X
r
a

A Smoother Way to Train Structured Prediction Models

Krishna Pillutla1

Vincent Roulet2

Sham M. Kakade1,2

Zaid Harchaoui2

1 Paul G. Allen School of Computer Science and Engineering, University of Washington
2 Department of Statistics, University of Washington
{pillutla,sham}@cs.washington.edu, {vroulet,zaid}@uw.edu

Abstract

We present a framework to train a structured prediction model by performing smoothing on the in-
ference algorithm it builds upon. Smoothing overcomes the non-smoothness inherent to the maximum
margin structured prediction objective, and paves the way for the use of fast primal gradient-based op-
timization algorithms. We illustrate the proposed framework by developing a novel primal incremental
optimization algorithm for the structural support vector machine. The proposed algorithm blends an
extrapolation scheme for acceleration and an adaptive smoothing scheme and builds upon the stochastic
variance-reduced gradient algorithm. We establish its worst-case global complexity bound and study
several practical variants, including extensions to deep structured prediction. We present experimen-
tal results on two real-world problems, namely named entity recognition and visual object localization.
The experimental results show that the proposed framework allows us to build upon efﬁcient inference
algorithms to develop large-scale optimization algorithms for structured prediction which can achieve
competitive performance on the two real-world problems.

1 Introduction

Consider the optimization problem arising when training maximum margin structured prediction models:

(cid:34)

min
w∈Rd

F (w) :=

1
n

n
(cid:88)

i=1

(cid:35)

f (i)(w) +

(cid:107)w(cid:107)2
2

,

λ
2

(1)

where each f (i) is the structural hinge loss. Max-margin structured prediction was designed to forecast
discrete data structures such as sequences and trees [Taskar et al., 2004, Tsochantaridis et al., 2004].

Batch non-smooth optimization algorithms such as cutting plane methods are appropriate for problems
with small or moderate sample sizes [Tsochantaridis et al., 2004, Joachims et al., 2009]. Stochastic non-
smooth optimization algorithms such as stochastic subgradient methods can tackle problems with large
sample sizes [Ratliff et al., 2007, Shalev-Shwartz et al., 2011]. However, both families of methods achieve
the typical worst-case complexity bounds of non-smooth optimization algorithms and cannot easily leverage
a possible hidden smoothness of the objective.

Furthermore, as signiﬁcant progress is being made on incremental smooth optimization algorithms for
training unstructured prediction models [Lin et al., 2018], we would like to transfer such advances and design
faster optimization algorithms to train structured prediction models. Indeed if each term in the ﬁnite-sum
were L-smooth, incremental optimization algorithms such as MISO [Mairal, 2015], SAG [Le Roux et al.,
2012, Schmidt et al., 2017], SAGA [Defazio et al., 2014], SDCA [Shalev-Shwartz and Zhang, 2013], and

1

SVRG [Johnson and Zhang, 2013] could leverage the ﬁnite-sum structure of the objective (1) and achieve
faster convergence than batch algorithms on large-scale problems.

Incremental optimization algorithms can be further accelerated, either on a case-by-case basis [Shalev-
Shwartz and Zhang, 2014, Frostig et al., 2015, Allen-Zhu, 2017, Defazio, 2016] or using the Catalyst accel-
eration scheme [Lin et al., 2015, 2018], to achieve near-optimal convergence rates [Woodworth and Srebro,
2016]. Accelerated incremental optimization algorithms demonstrate stable and fast convergence behavior
on a wide range of problems, in particular for ill-conditioned ones.

We introduce a general framework that allows us to bring the power of accelerated incremental optimiza-
tion algorithms to the realm of structured prediction problems. To illustrate our framework, we focus on the
problem of training a structural support vector machine (SSVM), and extend the developed algorithms to
deep structured prediction models with nonlinear mappings.

We seek primal optimization algorithms, as opposed to saddle-point or primal-dual optimization algo-
rithms, in order to be able to tackle structured prediction models with afﬁne mappings such as SSVM as
well as deep structured prediction models with nonlinear mappings. We show how to shade off the inherent
non-smoothness of the objective while still being able to rely on efﬁcient inference algorithms.

Smooth Inference Oracles. We introduce a notion of smooth inference oracles that gracefully ﬁts the
framework of black-box ﬁrst-order optimization. While the exp inference oracle reveals the relation-
ship between max-margin and probabilistic structured prediction models, the top-K inference oracle
can be efﬁciently computed using simple modiﬁcations of efﬁcient inference algorithms in many cases
of interest.

Incremental Optimization Algorithms. We present a new algorithm built on top of SVRG, blending an
extrapolation scheme for acceleration and an adaptive smoothing scheme. We establish the worst-
case complexity bounds of the proposed algorithm and extend it to the case of non-linear mappings.
Finally, we demonstrate its effectiveness compared to competing algorithms on two tasks, namely
named entity recognition and visual object localization.

The code is publicly available as a software library called Casimir1. The outline of the paper is as
follows: Sec. 1.1 reviews related work. Sec. 2 discusses smoothing for structured prediction followed by
Sec. 3, which deﬁnes and studies the properties of inference oracles and Sec. 4, which describes the concrete
implementation of these inference oracles in several settings of interest. Then, we switch gears to study
accelerated incremental algorithms in convex case (Sec. 5) and their extensions to deep structured prediction
(Sec. 6). Finally, we evaluate the proposed algorithms on two tasks, namely named entity recognition and
visual object localization in Sec. 7.

1.1 Related Work

Optimization for Structural Support Vector Machines Table 1 gives an overview of different opti-
mization algorithms designed for structural support vector machines. Early works [Taskar et al., 2004,
Tsochantaridis et al., 2004, Joachims et al., 2009, Teo et al., 2009] considered batch dual quadratic opti-
mization (QP) algorithms. The stochastic subgradient method operated directly on the non-smooth primal
formulation [Ratliff et al., 2007, Shalev-Shwartz et al., 2011]. More recently, Lacoste-Julien et al. [2013]
proposed a block coordinate Frank-Wolfe (BCFW) algorithm to optimize the dual formulation of structural

1https://github.com/krishnap25/casimir

2

Table 1: Convergence rates given in terms of the number of calls to various oracles for different optimization algo-
rithms on the learning problem (1) in case of structural support vector machines (4). The rates are speciﬁed in terms
of the target accuracy (cid:15), the number of training examples n, the regularization λ, the size of the label space |Y|, the
max feature norm R = maxi (cid:107)Φ(x(i), y) − Φ(x(i), y(i))(cid:107)2 and (cid:101)R ≥ R (see Remark 28 for explicit form). The rates
are speciﬁed up to constants and factors logarithmic in the problem parameters. The dependence on the initial error is
ignored. * denotes algorithms that make O(1) oracle calls per iteration.

Algo. (exp oracle)

# Oracle calls

Exponentiated
gradient*
[Collins et al., 2008]
Excessive gap
reduction
[Zhang et al., 2014]
Prop. 29*,
entropy smoother

Prop. 30*,
entropy smoother

(n + log |Y|)R2
λ(cid:15)

nR

(cid:114)

(cid:114)

log |Y|
λ(cid:15)

nR2 log|Y|
λ(cid:15)

n +

R2 log|Y|
λ(cid:15)

Algo. (max oracle)

# Oracle calls

BMRM
[Teo et al., 2009]

QP 1-slack
[Joachims et al., 2009]

Stochastic
subgradient*
[Shalev-Shwartz et al., 2011]
Block-Coordinate
Frank-Wolfe*
[Lacoste-Julien et al., 2013]

nR2
λ(cid:15)
nR2
λ(cid:15)

R2
λ(cid:15)

n +

R2
λ(cid:15)

Algo.
(top-K oracle)

# Oracle calls

Prop. 29*,
(cid:96)2
2 smoother

Prop. 30*,
(cid:96)2
2 smoother

(cid:114)

n (cid:101)R2
λ(cid:15)
n + (cid:101)R2
λ(cid:15)

support vector machines; see also Osokin et al. [2016] for variants and extensions. Saddle-point or primal-
dual approaches include the mirror-prox algorithm [Taskar et al., 2006, Cox et al., 2014, He and Harchaoui,
2015]. Palaniappan and Bach [2016] propose an incremental optimization algorithm for saddle-point prob-
lems. However, it is unclear how to extend it to the structured prediction problems considered here. Incre-
mental optimization algorithms for conditional random ﬁelds were proposed by Schmidt et al. [2015]. We
focus here on primal optimization algorithms in order to be able to train structured prediction models with
afﬁne or nonlinear mappings with a uniﬁed approach, and on incremental optimization algorithms which
can scale to large datasets.

Inference The ideas of dynamic programming inference in tree structured graphical models have been
around since the pioneering works of Pearl [1988] and Dawid [1992]. Other techniques emerged based on
graph cuts [Greig et al., 1989, Ishikawa and Geiger, 1998], bipartite matchings [Cheng et al., 1996, Taskar
et al., 2005] and search algorithms [Daum´e III and Marcu, 2005, Lampert et al., 2008, Lewis and Steedman,
2014, He et al., 2017]. For graphical models that admit no such a discrete structure, techniques based on
loopy belief propagation [McEliece et al., 1998, Murphy et al., 1999], linear programming (LP) [Schlesinger,
1976], dual decomposition [Johnson, 2008] and variational inference [Wainwright et al., 2005, Wainwright
and Jordan, 2008] gained popularity.

Top-K Inference Smooth inference oracles with (cid:96)2
2 smoothing echo older heuristics in speech and lan-
guage processing [Jurafsky et al., 2014]. Combinatorial algorithms for top-K inference have been studied
extensively by the graphical models community under the name “M -best MAP”. Seroussi and Golmard
[1994] and Nilsson [1998] ﬁrst considered the problem of ﬁnding the K most probable conﬁgurations in a
tree structured graphical model. Later, Yanover and Weiss [2004] presented the Best Max-Marginal First
algorithm which solves this problem with access only to an oracle that computes max-marginals. We also
use this algorithm in Sec. 4.2. Fromer and Globerson [2009] study top-K inference for LP relaxation, while
Batra [2012] considers the dual problem to exploit graph structure. Flerova et al. [2016] study top-K exten-
sions of the popular A(cid:63) and branch and bound search algorithms in the context of graphical models. Other

3

related approaches include diverse K-best solutions [Batra et al., 2012] and ﬁnding K-most probable modes
[Chen et al., 2013].

Smoothing Inference Smoothing for inference was used to speed up iterative algorithms for continu-
ous relaxations. Johnson [2008] considered smoothing dual decomposition inference using the entropy
smoother, followed by Jojic et al. [2010] and Savchynskyy et al. [2011] who studied its theoretical prop-
erties. Meshi et al. [2012] expand on this study to include (cid:96)2
2 smoothing. Explicitly smoothing discrete
inference algorithms in order to smooth the learning problem was considered by Zhang et al. [2014] and
Song et al. [2014] using the entropy and (cid:96)2
2 smoother was also used by Mar-
tins and Astudillo [2016]. Hazan et al. [2016] consider the approach of blending learning and inference,
instead of using inference algorithms as black-box procedures.

2 smoothers respectively. The (cid:96)2

Related ideas to ours appear in the independent works [Mensch and Blondel, 2018, Niculae et al., 2018].
These works partially overlap with ours, but the papers choose different perspectives, making them com-
plementary to each other. Mensch and Blondel [2018] proceed differently when, e.g., smoothing inference
based on dynamic programming. Moreover, they do not establish complexity bounds for optimization al-
gorithms making calls to the resulting smooth inference oracles. We deﬁne smooth inference oracles in the
context of black-box ﬁrst-order optimization and establish worst-case complexity bounds for incremental
optimization algorithms making calls to these oracles. Indeed we relate the amount of smoothing controlled
by µ to the resulting complexity of the optimization algorithms relying on smooth inference oracles.

End-to-end Training of Structured Prediction The general framework for global training of structured
prediction models was introduced by Bottou and Gallinari [1990] and applied to handwriting recognition
by Bengio et al. [1995] and to document processing by Bottou et al. [1997]. This approach, now called
“deep structured prediction”, was used, e.g., by Collobert et al. [2011] and Belanger and McCallum [2016].

1.2 Notation

Vectors are denoted by bold lowercase characters as w ∈ Rd while matrices are denoted by bold uppercase
characters as A ∈ Rd×n. For a matrix A ∈ Rm×n, deﬁne the norm for α, β ∈ {1, 2, ∞},

For any function f : Rd → R ∪ {+∞}, its convex conjugate f ∗ : Rd → R ∪ {+∞} is deﬁned as

(cid:107)A(cid:107)β,α = max{(cid:104)y, Ax(cid:105) | (cid:107)y(cid:107)α ≤ 1 , (cid:107)x(cid:107)β ≤ 1} .

(2)

f ∗(z) = sup
w∈Rd

{(cid:104)z, w(cid:105) − f (w)} .

A function f : Rd → R is said to be L-smooth with respect to an arbitrary norm (cid:107)·(cid:107) if it is continuously
differentiable and its gradient ∇f is L-Lipschitz with respect to (cid:107)·(cid:107). When left unspeciﬁed, (cid:107)·(cid:107) refers to
(cid:107) · (cid:107)2. Given a continuously differentiable map g : Rd → Rm, its Jacobian ∇g(w) ∈ Rm×d at w ∈ Rd is
deﬁned so that its ijth entry is [∇g(w)]ij = ∂gi(w)/wj where gi is the ith element of g and wj is the jth
element of w. The vector valued function g : Rd → Rm is said to be L-smooth with respect to (cid:107)·(cid:107) if it is
continuously differentiable and its Jacobian ∇g is L-Lipschitz with respect to (cid:107)·(cid:107).

For a vector z ∈ Rm, z(1) ≥ · · · ≥ z(m) refer to its components enumerated in non-increasing order
where ties are broken arbitrarily. Further, we let z[k] = (z(1), · · · , z(k)) ∈ Rk denote the vector of the k
largest components of z. We denote by ∆m−1 the standard probability simplex in Rm. When the dimension
is clear from the context, we shall simply denote it by ∆. Moreover, for a positive integer p, [p] refers to the
set {1, . . . , p}. Lastly, (cid:101)O in the big-O notation hides factors logarithmic in problem parameters.

4

2 Smooth Structured Prediction

Structured prediction aims to search for score functions φ parameterized by w ∈ Rd that model the compat-
ibility of input x ∈ X and output y ∈ Y as φ(x, y; w) through a graphical model. Given a score function
φ(·, ·; w), predictions are made using an inference procedure which, when given an input x, produces the
best output

y∗(x; w) ∈ arg max

φ(x, y; w) .

y∈Y

(3)

We shall return to the score functions and the inference procedures in Sec. 3. First, given such a score
function φ, we deﬁne the structural hinge loss and describe how it can be smoothed.

2.1 Structural Hinge Loss

On a given input-output pair (x, y), the error of prediction of y by the inference procedure with a score func-
tion φ(·, ·; w), is measured by a task loss (cid:96)(cid:0)y, y∗(x; w)(cid:1) such as the Hamming loss. The learning procedure
would then aim to ﬁnd the best parameter w that minimizes the loss on a given dataset of input-output train-
ing examples. However, the resulting problem is piecewise constant and hard to optimize. Instead, Altun
et al. [2003], Taskar et al. [2004], Tsochantaridis et al. [2004] propose to minimize a majorizing surrogate
of the task loss, called the structural hinge loss deﬁned on an input-output pair (x(i), y(i)) as

f (i)(w) = max
y∈Y

(cid:110)

φ(x(i), y; w) + (cid:96)(y(i), y)

(cid:111)

− φ(x(i), y(i); w) = max
y∈Y

ψ(i)(y, w) .

(4)

where ψ(i)(y; w) = φ(x(i), y; w) + (cid:96)(y(i), y) − φ(x(i), y(i); w) is the augmented score function.

This approach, known as max-margin structured prediction, builds upon binary and multi-class support
vector machines [Crammer and Singer, 2001], where the term (cid:96)(y(i), y) inside the maximization in (4)
generalizes the notion of margin. The task loss (cid:96) is assumed to possess appropriate structure so that the
maximization inside (4), known as loss augmented inference, is no harder than the inference problem in (3).
When considering a ﬁxed input-output pair (x(i), y(i)), we drop the index with respect to the sample i and
consider the structural hinge loss as

f (w) = max
y∈Y

ψ(y; w),

(5)

When the map w (cid:55)→ ψ(y; w) is afﬁne, the structural hinge loss f and the objective F from (1) are both
convex - we refer to this case as the structural support vector machine. When w (cid:55)→ ψ(y; w) is a nonlinear
but smooth map, then the structural hinge loss f and the objective F are nonconvex.

2.2 Smoothing Strategy

A convex, non-smooth function h can be smoothed by taking its inﬁmal convolution with a smooth func-
tion [Beck and Teboulle, 2012]. We now recall its dual representation, which Nesterov [2005b] ﬁrst used to
relate the amount of smoothing to optimal complexity bounds.

Deﬁnition 1. For a given convex function h : Rm → R, a smoothing function ω : dom h∗ → R which is
1-strongly convex with respect to (cid:107) · (cid:107)α (for α ∈ {1, 2}), and a parameter µ > 0, deﬁne

hµω(z) = max

u∈dom h∗

{(cid:104)u, z(cid:105) − h∗(u) − µω(u)} .

as the smoothing of h by µω.

5

We now state a classical result showing how the parameter µ controls both the approximation error and the
level of the smoothing. For a proof, see Beck and Teboulle [2012, Thm. 4.1, Lemma 4.2] or Prop. 39 of
Appendix A.

Proposition 2. Consider the setting of Def. 1. The smoothing hµω is continuously differentiable and its
gradient, given by

∇hµω(z) = arg max
u∈dom h∗

{(cid:104)u, z(cid:105) − h∗(u) − µω(u)}

is 1/µ-Lipschitz with respect to (cid:107) · (cid:107)∗
µ1 ≥ µ2 ≥ 0,

α. Moreover, letting hµω ≡ h for µ = 0, the smoothing satisﬁes, for all

(µ1 − µ2)

inf
u∈dom h∗

ω(u) ≤ hµ2ω(z) − hµ1ω(z) ≤ (µ1 − µ2)

sup
u∈dom h∗

ω(u) .

Smoothing the Structural Hinge Loss We rewrite the structural hinge loss as a composition

(cid:40)Rd → Rm

g :

(cid:40)Rm → R

h :

w (cid:55)→ (ψ(y; w))y∈Y ,

z

(cid:55)→ maxi∈[m] zi,

where m = |Y| so that the structural hinge loss reads

(6)

(7)

We smooth the structural hinge loss (7) by simply smoothing the non-smooth max function h as

f (w) = h ◦ g(w) .

fµω = hµω ◦ g.

When g is smooth and Lipschitz continuous, fµω is a smooth approximation of the structural hinge loss,
whose gradient is readily given by the chain-rule. In particular, when g is an afﬁne map g(w) = Aw + b, if
follows that fµω is ((cid:107)A(cid:107)2
β,α/µ)-smooth with respect to (cid:107) · (cid:107)β (cf. Lemma 40 in Appendix A). Furthermore,
for µ1 ≥ µ2 ≥ 0, we have,

(µ1 − µ2) min

ω(u) ≤ fµ2ω(w) − fµ1ω(w) ≤ (µ1 − µ2) max

ω(u) .

u∈∆m−1

u∈∆m−1

In the context of smoothing the max function, we now describe two popular choices for the smoothing
function ω, followed by computational considerations.

2.3 Smoothing Variants

2.3.1 Entropy and (cid:96)2

2 smoothing

When h is the max function, the smoothing operation can be computed analytically for the entropy smoother
and the (cid:96)2

2 smoother, denoted respectively as

−H(u) := (cid:104)u, log u(cid:105)

and

(cid:96)2
2(u) := 1

2 ((cid:107)u(cid:107)2

2 − 1) .

These lead respectively to the log-sum-exp function [Nesterov, 2005b, Lemma 4]

h−µH (z) = µ log

ezi/µ

, ∇h−µH (z) =

(cid:33)

(cid:32) m
(cid:88)

i=1

(cid:35)

(cid:34)

ezi/µ
j=1 ezj /µ

(cid:80)m

,

i=1,...,m

6

and an orthogonal projection onto the simplex,

hµ(cid:96)2

2

(z) = (cid:104)z, proj∆m−1(z/µ)(cid:105) − µ

2 (cid:107) proj∆m−1(z/µ)(cid:107)2 + µ

2 , ∇hµ(cid:96)2

2

(z) = proj∆m−1(z/µ) .

Furthermore, the following holds for all µ1 ≥ µ2 ≥ 0 from Prop. 2:

0 ≤ h−µ1H (z) − h−µ2H (z) ≤ (µ1 − µ2) log m,

and, 0 ≤ hµ1(cid:96)2

(z) − hµ2(cid:96)2

2

2

(z) ≤ 1

2 (µ1 − µ2) .

2.3.2 Top-K Strategy

Though the gradient of the composition fµω = hµω ◦ g can be written using the chain rule, its actual
computation for structured prediction problems involves computing ∇g over all m = |Y| of its components,
which may be intractable. However, in the case of (cid:96)2
2 smoothing, projections onto the simplex are sparse, as
pointed out by the following proposition.

Proposition 3. Consider the Euclidean projection u∗ = arg minu∈∆m−1 (cid:107)u − z/µ(cid:107)2
the simplex, where µ > 0. The projection u∗ has exactly k ∈ [m] non-zeros if and only if

2 of z/µ ∈ Rm onto

where z(1) ≥ · · · ≥ z(m) are the components of z in non-decreasing order and z(m+1) := −∞. In this case,
u∗ is given by

k
(cid:88)

i=1

(cid:0)z(i) − z(k)

(cid:1) < µ ≤

(cid:0)z(i) − z(k+1)

(cid:1) ,

k
(cid:88)

i=1

(cid:26)

u∗
i = max

0, 1
kµ

(cid:0)zi − z(j)

(cid:1) + 1
k

(cid:27)

.

k
(cid:88)

j=1

m
(cid:88)

i=1

(cid:18) zi
µ

+ ρ

= 1 ,

(cid:19)

+

Proof. The projection u∗ satisﬁes u∗

i = (zi/µ + ρ∗)+, where ρ∗ is the unique solution of ρ in the equation

where α+ = max{0, α}. See, e.g., Held et al. [1974] for a proof of this fact. Note that z(i)/µ + ρ∗ ≤ 0
implies that z(j)/µ + ρ∗ ≤ 0 for all j ≥ i. Therefore u∗ has k non-zeros if and only if z(k)/µ + ρ∗ > 0 and
z(k+1)/µ + ρ∗ ≤ 0.

Now suppose that u∗ has exactly k non-zeros, we can then solve (9) to obtain ρ∗ = ϕk(z/µ), which is

deﬁned as

ϕk

(cid:19)

(cid:18) z
µ

:=

−

1
k

1
k

k
(cid:88)

i=1

z(i)
µ

.

Plugging in the value of ρ∗ in z(k)/µ + ρ∗ > 0 gives µ > (cid:80)k
(cid:1).
gives µ ≤ (cid:80)k

(cid:0)z(i) − z(k+1)

i=1

i=1

(cid:0)z(i) − z(k)

(cid:1). Likewise, z(k+1)/µ + ρ∗ ≤ 0

Conversely assume (8) and let (cid:98)ρ = ϕk(z/µ). Eq. (8) can be written as z(k)/µ+(cid:98)ρ > 0 and z(k+1)/µ+(cid:98)ρ ≤
0. Furthermore, we verify that (cid:98)ρ satisﬁes Eq. (9), and so (cid:98)ρ = ρ∗ is its unique root. It follows, therefore, that
the sparsity of u∗ is k.

7

(8)

(9)

(10)

Thus, the projection of z/µ onto the simplex picks out some number Kz/µ of the largest entries of z/µ
- we refer to this as the sparsity of proj∆m−1(z/µ). This fact motivates the top-K strategy: given µ > 0, ﬁx
an integer K a priori and consider as surrogates for hµ(cid:96)2

and ∇hµ(cid:96)2

respectively

2

2

hµ,K(z) := max

u∈∆K−1

(cid:8)(cid:10)z[K], u(cid:11) − µ(cid:96)2

2(u)(cid:9) ,

and,

(cid:101)∇hµ,K(z) := ΩK(z)(cid:62) proj∆K−1

(cid:19)

(cid:18) z[K]
µ

,

where z[K] denotes the vector composed of the K largest entries of z and ΩK : Rm → {0, 1}K×m deﬁnes
their extraction, i.e., ΩK(z) = (e(cid:62)
)(cid:62) ∈ {0, 1}K×m where j1, · · · , jK satisfy zj1 ≥ · · · ≥ zjK
j1
such that z[K] = ΩK(z)z . A surrogate of the (cid:96)2

2 smoothing is then given by

, . . . , e(cid:62)
jK

fµ,K := hµ,K ◦ g ,

and,

(cid:101)∇fµ,K(w) := ∇g(w)(cid:62) (cid:101)∇hµ,K(g(w)) .

(11)

Exactness of Top-K Strategy We say that the top-K strategy is exact at z for µ > 0 when it recovers
(z) = (cid:101)∇hµ,K(z). The next
the ﬁrst order information of hµ(cid:96)2
proposition outlines when this is the case. Note that if the top-K strategy is exact at z for a smoothing
parameter µ > 0 then it will be exact at z for any µ(cid:48) < µ.

(z) = hµ,K(z) and ∇hµ(cid:96)2

, i.e. when hµ(cid:96)2

2

2

2

Proposition 4. The top-K strategy is exact at z for µ > 0 if

µ ≤

(cid:0)z(i) − z(K+1)

(cid:1) .

K
(cid:88)

i=1

(12)

Moreover, for any ﬁxed z ∈ Rm such that the vector z[K+1] = ΩK+1(z)z has at least two unique elements,
the top-K strategy is exact at z for all µ satisfying 0 < µ ≤ z(1) − z(K+1).

Proof. First, we note that the top-K strategy is exact when the sparsity Kz/µ of the projection proj∆m−1(z/µ)
satisﬁes Kz/µ ≤ K. From Prop. 3, the condition that Kz/µ ∈ {1, 2, · · · , K} happens when

µ ∈

K
(cid:91)

(cid:32) k

(cid:88)

k=1

i=1

(cid:0)z(i) − z(k)

(cid:1) ,

(cid:0)z(i) − z(k+1)

(cid:1)

k
(cid:88)

i=1

(cid:35)

(cid:32)

=

0,

K
(cid:88)

i=1

(cid:35)

(cid:0)z(i) − z(K+1)

(cid:1)

,

since the intervals in the union are contiguous. This establishes (12).

The only case when (12) cannot hold for any value of µ > 0 is when the right hand size of (12) is zero.
In the opposite case when z[K+1] has at least two unique components, or equivalently, z(1) − z(K+1) > 0,
the condition 0 < µ ≤ z(1) − z(K+1) implies (12).

If the top-K strategy is exact at g(w) for µ, then

fµ,K(w) = fµ(cid:96)2

(w)

2

and

(cid:101)∇fµ,K(w) = ∇fµ(cid:96)2

(w) ,

2

where the latter follows from the chain rule. When used instead of (cid:96)2
2 smoothing in the algorithms presented
in Sec. 5, the top-K strategy provides a computationally efﬁcient heuristic to smooth the structural hinge
loss. Though we do not have theoretical guarantees using this surrogate, experiments presented in Sec. 7
show its efﬁciency and its robustness to the choice of K.

8

This section studies ﬁrst order oracles used in standard and smoothed structured prediction. We ﬁrst describe
the parameterization of the score functions through graphical models.

3 Inference Oracles

3.1 Score Functions

Structured prediction is deﬁned by the structure of the output y, while input x ∈ X can be arbitrary.
Each output y ∈ Y is composed of p components y1, . . . , yp that are linked through a graphical model
G = (V, E) - the nodes V = {1, · · · , p} represent the components of the output y while the edges E deﬁne
the dependencies between various components. The value of each component yv for v ∈ V represents the
state of the node v and takes values from a ﬁnite set Yv. The set of all output structures Y = Y1 × · · · × Yp
is then ﬁnite yet potentially intractably large.

The structure of the graph (i.e., its edge structure) depends on the task. For the task of sequence labeling,
the graph is a chain, while for the task of parsing, the graph is a tree. On the other hand, the graph used in
image segmentation is a grid.

For a given input x and a score function φ(·, ·; w), the value φ(x, y; w) measures the compatibility of
the output y for the input x. The essential characteristic of the score function is that it decomposes over the
nodes and edges of the graph as

φ(x, y; w) =

φv(x, yv; w) +

φv,v(cid:48)(x, yv, yv(cid:48); w) .

(13)

(cid:88)

v∈V

(cid:88)

(v,v(cid:48))∈E

For a ﬁxed w, each input x deﬁnes a speciﬁc compatibility function φ(x, · ; w). The nature of the
problem and the optimization algorithms we consider hinge upon whether φ is an afﬁne function of w or
not. The two settings studied here are the following:

Pre-deﬁned Feature Map. In this structured prediction framework, a pre-speciﬁed feature map Φ : X ×

Y → Rd is employed and the score φ is then deﬁned as the linear function

φ(x, y; w) = (cid:104)Φ(x, y), w(cid:105) =

(cid:104)Φv(x, yv), w(cid:105) +

(cid:104)Φv,v(cid:48)(x, yv, yv(cid:48)), w(cid:105) .

(14)

(cid:88)

v∈V

(cid:88)

(v,v(cid:48))∈E

Learning the Feature Map. We also consider the setting where the feature map Φ is parameterized by
w0, for example, using a neural network, and is learned from the data. The score function can then be
written as

φ(x, y; w) = (cid:104)Φ(x, y; w0), w1(cid:105)

(15)

where w = (w0, w1) and the scalar product decomposes into nodes and edges as above.

Note that we only need the decomposition of the score function over nodes and edges of the G as in Eq. (13).
In particular, while Eq. (15) is helpful to understand the use of neural networks in structured prediction, the
optimization algorithms developed in Sec. 6 apply to general nonlinear but smooth score functions.

This framework captures both generative probabilistic models such as Hidden Markov Models (HMMs)
that model the joint distribution between x and y as well as discriminative probabilistic models, such as
conditional random ﬁelds [Lafferty et al., 2001] where dependencies among the input variables x do not
need to be explicitly represented. In these cases, the log joint and conditional probabilities respectively play
the role of the score φ.

9

Example 5 (Sequence Tagging). Consider the task of sequence tagging in natural language processing
where each x = (x1, · · · , xp) ∈ X is a sequence of words and y = (y1, · · · , yp) ∈ Y is a sequence of
labels, both of length p. Common examples include part of speech tagging and named entity recognition.
Each word xv in the sequence x comes from a ﬁnite dictionary D, and each tag yv in y takes values from a
ﬁnite set Yv = Ytag. The corresponding graph is simply a linear chain.

The score function measures the compatibility of a sequence y ∈ Y for the input x ∈ X using parame-

ters w = (wunary, wpair) as, for instance,

φ(x, y; w) =

(cid:104)Φunary(xv, yv), wunary(cid:105) +

(cid:104)Φpair(yv, yv+1), wpair(cid:105) ,

p
(cid:88)

v=1

p
(cid:88)

v=0

where, using wunary ∈ R|D||Ytag| and wpair ∈ R|Ytag|2
each v ∈ [p],

as node and edge weights respectively, we deﬁne for

(cid:104)Φunary(xv, yv), wunary(cid:105) =

wunary, x,j I(x = xv) I(j = yv) .

(cid:88)

x∈D, j∈Ytag

The pairwise term (cid:104)Φpair(yv, yv+1), wpair(cid:105) is analogously deﬁned. Here, y0, yp+1 are special “start” and
“stop” symbols respectively. This can be written as a dot product of w with a pre-speciﬁed feature map as
in (14), by deﬁning

Φ(x, y) = (cid:0)

exv ⊗ eyv

(cid:1) ⊕ (cid:0)

eyv ⊗ eyv+1

(cid:1) ,

p
(cid:88)

v=1

p
(cid:88)

v=0

where exv is the unit vector (I(x = xv))x∈D ∈ R|D|, eyv is the unit vector (I(j = yv))j∈Ytag ∈ R|Ytag|, ⊗
denotes the Kronecker product between vectors and ⊕ denotes vector concatenation.

3.2

Inference Oracles

We deﬁne now inference oracles as ﬁrst order oracles in structured prediction. These are used later to
understand the information-based complexity of optimization algorithms.

3.2.1 First Order Oracles in Structured Prediction

A ﬁrst order oracle for a function f : Rd → R is a routine which, given a point w ∈ Rd, returns on output a
value f (w) and a (sub)gradient v ∈ ∂f (w), where ∂f is the Fr´echet (or regular) subdifferential [Rockafellar
and Wets, 2009, Def. 8.3]. We now deﬁne inference oracles as ﬁrst order oracles for the structural hinge
loss f and its smoothed variants fµω. Note that these deﬁnitions are independent of the graphical structure.
However, as we shall see, the graphical structure plays a crucial role in the implementation of the inference
oracles.

Deﬁnition 6. Consider an augmented score function ψ, a level of smoothing µ > 0 and the structural hinge
loss f (w) = maxy∈Y ψ(y; w). For a given w ∈ Rd,

(i)

the max oracle returns f (w) and v ∈ ∂f (w).

(ii)

the exp oracle returns f−µH (w) and ∇f−µH (w).

(iii) the top-K oracle returns fµ,K(w) and (cid:101)∇fµ,K(w) as surrogates for fµ(cid:96)2

(w) and ∇fµ(cid:96)2

(w) respec-

2

2

tively.

10

(a) Non-smooth.

(b) (cid:96)2

2 smoothing.

(c) Entropy smoothing.

Figure 1: Viterbi trellis for a chain graph with p = 4 nodes and 3 labels.

Note that the exp oracle gets its name since it can be written as an expectation over all y, as revealed by the
next lemma, which gives analytical expressions for the gradients returned by the oracles.

Lemma 7. Consider the setting of Def. 6. We have the following:

(i) For any y∗ ∈ arg maxy∈Y ψ(y; w), we have that ∇wψ(y∗; w) ∈ ∂f (w). That is, the max oracle can

be implemented by inference.

(ii) The output of the exp oracle satisﬁes ∇f−µH (w) = (cid:80)

y∈Y Pψ,µ(y; w)∇ψ(y; w), where

Pψ,µ(y; w) =

exp

(cid:16) 1

(cid:17)
µ ψ(y; w)

(cid:80)

y(cid:48)∈Y exp

µ ψ(y(cid:48); w)

(cid:16) 1

(cid:17) .

(iii) The output of the top-K oracle satisﬁes (cid:101)∇fµ,K(w) = (cid:80)K

i=1 u∗
(cid:9) is the set of K largest scoring outputs satisfying

(cid:8)y(1), · · · , y(K)

ψ,µ,i(w)∇ψ(y(i); w) , where YK =

ψ(y(1); w) ≥ · · · ≥ ψ(y(K); w) ≥ max
y∈Y\YK

ψ(y; w) ,

and u∗

ψ,µ = proj∆K−1

(cid:16)(cid:2)ψ(y(1); w), · · · , ψ(y(K); w)(cid:3)(cid:62)(cid:17)

.

Proof. Part (ii) deals with the composition of differentiable functions, and follows from the chain rule.
Part (iii) follows from the deﬁnition in Eq. (11). The proof of Part (i) follows from the chain rule for Fr´echet
subdifferentials of compositions [Rockafellar and Wets, 2009, Theorem 10.6] together with the fact that
by convexity and Danskin’s theorem [Bertsekas, 1999, Proposition B.25], the subdifferential of the max
function is given by ∂h(z) = conv{ei | i ∈ [m] such that zi = h(z)}.

Example 8. Consider the task of sequence tagging from Example 5. The inference problem (3) is a search
over all |Y| = |Ytag|p label sequences. For chain graphs, this is equivalent to searching for the shortest
path in the associated trellis, shown in Fig. 1. An efﬁcient dynamic programming approach called the
Viterbi algorithm [Viterbi, 1967] can solve this problem in space and time polynomial in p and |Ytag|. The
structural hinge loss is non-smooth because a small change in w might lead to a radical change in the best
scoring path shown in Fig. 1.

11

When smoothing f with ω = (cid:96)2

is given by a projection onto the simplex,
which picks out some number Kψ/µ of the highest scoring outputs y ∈ Y or equivalently, Kψ/µ shortest
paths in the Viterbi trellis (Fig. 1b). The top-K oracle then uses the top-K strategy to approximate fµ(cid:96)2
with
fµ,K.

2, the smoothed function fµ(cid:96)2

2

2

On the other hand, with entropy smoothing ω = −H, we get the log-sum-exp function and its gra-
dient is obtained by averaging over paths with weights such that shorter paths have a larger weight (cf.
Lemma 7(ii)). This is visualized in Fig. 1c.

3.2.2 Exp Oracles and Conditional Random Fields

Recall that a Conditional Random Field (CRF) [Lafferty et al., 2001] with augmented score function ψ and
parameters w ∈ Rd is a probabilistic model that assigns to output y ∈ Y the probability

P(y | ψ; w) = exp (ψ(y; w) − Aψ(w)) ,

(16)

where Aψ(w) is known as the log-partition function, a normalizer so that the probabilities sum to one.
Gradient-based maximum likelihood learning algorithms for CRFs require computation of the log-partition
function Aψ(w) and its gradient ∇Aψ(w). Next proposition relates the computational costs of the exp
oracle and the log-partition function.
Proposition 9. The exp oracle for an augmented score function ψ with parameters w ∈ Rd is equivalent
in hardness to computing the log-partition function Aψ(w) and its gradient ∇Aψ(w) for a conditional
random ﬁeld with augmented score function ψ.

Proof. Fix a smoothing parameter µ > 0. Consider a CRF with augmented score function ψ(cid:48)(y; w) =
y∈Y exp (cid:0)µ−1ψ(y; w)(cid:1). The
µ−1ψ(y; w). Its log-partition function Aψ(cid:48)(w) satisﬁes exp(Aψ(cid:48)(w)) = (cid:80)
claim now follows from the bijection f−µH (w) = µ Aψ(cid:48)(w) between f−µH and Aψ(cid:48).

4 Implementation of Inference Oracles

We now turn to the concrete implementation of the inference oracles. This depends crucially on the structure
of the graph G = (V, E). If the graph G is a tree, then the inference oracles can be computed exactly with
efﬁcient procedures, as we shall see in in the Sec. 4.1. When the graph G is not a tree, we study special cases
when speciﬁc discrete structure can be exploited to efﬁciently implement some of the inference oracles in
Sec. 4.2. The results of this section are summarized in Table 2.

Throughout this section, we ﬁx an input-output pair (x(i), y(i)) and consider the augmented score func-
tion ψ(y; w) = φ(x(i), y; w) + (cid:96)(y(i), y) − φ(x(i), y(i); w) it deﬁnes, where the index of the sample is
dropped by convenience. From (13) and the decomposability of the loss, we get that ψ decomposes along
nodes V and edges E of G as:

ψ(y; w) =

ψv(yv; w) +

ψv,v(cid:48)(yv, yv(cid:48); w) .

(17)

(cid:88)

v∈V

(cid:88)

(v,v(cid:48))∈E

When w is clear from the context, we denote ψ(· ; w) by ψ. Likewise for ψv and ψv,v(cid:48).

4.1

Inference Oracles in Trees

We ﬁrst consider algorithms implementing the inference algorithms in trees and examine their computational
complexity.

12

Table 2: Smooth inference oracles, algorithms and complexity. Here, p is the size of each y ∈ Y. The time
complexity is phrased in terms of the time complexity T of the max oracle.

Max oracle
Algo

Max-product

Graph cut

Graph matching

Branch and
Bound search

Algo
Top-K
max-product

BMMF

BMMF

Top-K oracle

Time

Exp oracle
Algo

Time

O(KT log K)

Sum-Product

O(T )

O(pKT )

O(KT )

Intractable

Intractable

Intractable

Top-K search

N/A

4.1.1

Implementation of Inference Oracles

Max Oracle
In tree structured graphical models, the inference problem (3), and thus the max oracle (cf.
Lemma 7(i)) can always be solved exactly in polynomial time by the max-product algorithm [Pearl, 1988],
which uses the technique of dynamic programming [Bellman, 1957]. The Viterbi algorithm (Algo. 1) for
chain graphs from Example 8 is a special case. See Algo. 7 in Appendix B for the max-product algorithm
in full generality.

Top-K Oracle The top-K oracle uses a generalization of the max-product algorithm that we name top-K
max-product algorithm. Following the work of Seroussi and Golmard [1994], it keeps track of the K-best
intermediate structures while the max-product algorithm just tracks the single best intermediate structure.
Formally, the kth largest element from a discrete set S is deﬁned as

(cid:40)

max(k)
x∈S

f (x) =

kth largest element of {f (y) | y ∈ S} k ≤ |S|
k > |S| .
−∞,

We present the algorithm in the simple case of chain structured graphical models in Algo. 2. The top-K
max-product algorithm for general trees is given in Algo. 8 in Appendix B. Note that it requires (cid:101)O(K) times
the time and space of the max oracle.

Exp oracle The relationship of the exp oracle with CRFs (Prop. 9) leads directly to Algo. 3, which is
based on marginal computations from the sum-product algorithm.

Remark 10. We note that clique trees allow the generalization of the algorithms of this section to general
graphs with cycles. However, the construction of a clique tree requires time and space exponential in the
treewidth of the graph.

Example 11. Consider the task of sequence tagging from Example 5. The Viterbi algorithm (Algo. 1)
maintains a table πv(yv), which stores the best length-v preﬁx ending in label yv. One the other hand, the
top-K Viterbi algorithm (Algo. 2) must store in π(k)
v (yv) the score of kth best length-v preﬁx that ends in
yv for each k ∈ [K]. In the vanilla Viterbi algorithm, the entry πv(yv) is updated by looking the previous
column πv−1 following (18). Compare this to update (19) of the top-K Viterbi algorithm. In this case,
the exp oracle is implemented by the forward-backward algorithm, a specialization of the sum-product
algorithm to chain graphs.

13

Algorithm 1 Max-product (Viterbi) algorithm for chain graphs

1: Input: Augmented score function ψ(·, ·; w) deﬁned on a chain graph G.
2: Set π1(y1) ← ψ1(y1) for all y1 ∈ Y1.
3: for v = 2, · · · p do
4:

For all yv ∈ Yv, set

πv(yv) ← ψv(yv) + max

{πv−1(yv−1) + ψv,v−1(yv, yv−1)} .

(18)

yv−1∈Yv−1

Assign to δv(yv) the yv−1 that attains the max above for each yv ∈ Yv.

5:
6: end for
7: Set ψ∗ ← maxyp∈Yp πp(yp) and store the maximizing assignments of yp in y∗
p.
8: for v = p − 1, · · · , 1 do
Set y∗
v ← δv+1(yv+1).
9:
10: end for
11: return ψ∗, y∗ := (y∗

1, · · · , y∗

p).

4.1.2 Complexity of Inference Oracles

The next proposition presents the correctness guarantee and complexity of each of the aforementioned algo-
rithms. Its proof has been placed in Appendix B.

Proposition 12. Consider as inputs an augmented score function ψ(·, ·; w) deﬁned on a tree structured
graph G, an integer K > 0 and a smoothing parameter µ > 0.

(i) The output (ψ∗, y∗) of the max-product algorithm (Algo. 1 for the special case when G is chain struc-
tured Algo. 7 from Appendix B in general) satisﬁes ψ∗ = ψ(y∗; w) = maxy∈Y ψ(y; w). Thus, the pair
(cid:0)ψ∗, ∇ψ(y∗; w)(cid:1) is a correct implementation of the max oracle. It requires time O(p maxv∈V |Yv|2)
and space O(p maxv∈V |Yv|).

(ii) The output {ψ(k), y(k)}K

k=1 of the top-K max-product algorithm (Algo. 2 for the special case when G
is chain structured or Algo. 8 from Appendix B in general) satisﬁes ψ(k) = ψ(y(k)) = max(k)
y∈Y ψ(y).
Thus, the top-K max-product algorithm followed by a projection onto the simplex (Algo. 6 in Ap-
pendix A) is a correct implementation of the top-K oracle. It requires time O(pK log K maxv∈V |Yv|2)
and space O(pK maxv∈V |Yv|).

(iii) Algo. 3 returns (cid:0)f−µH (w), ∇f−µH (w)(cid:1). Thus, Algo. 3 is a correct implementation of the exp oracle.

It requires time O(p maxv∈V |Yv|2) and space O(p maxv∈V |Yv|).

4.2

Inference Oracles in Loopy Graphs

For general loopy graphs with high tree-width, the inference problem (3) is NP-hard [Cooper, 1990]. In
particular cases, graph cut, matching or search algorithms can be used for exact inference in dense loopy
graphs, and therefore, to implement the max oracle as well (cf. Lemma 7(i)). In each of these cases, we ﬁnd
that the top-K oracle can be implemented, but the exp oracle is intractable. Appendix C contains a review
of the algorithms and guarantees referenced in this section.

14

Algorithm 2 Top-K max-product (top-K Viterbi) algorithm for chain graphs

1: Input: Augmented score function ψ(·, ·; w) deﬁned on chain graph G, integer K > 0.
2: For k = 1, · · · , K, set π(k)
1 (y1) ← ψ1(y1) if k = 1 and −∞ otherwise for all y1 ∈ Y1.
3: for v = 2, · · · p and k = 1, · · · , K do
4:

For all yv ∈ Yv, set

v (yv) ← ψv(yv) + max(k)
π(k)

yv−1∈Yv−1,(cid:96)∈[K]

(cid:110)

π((cid:96))
v−1(yv−1) + ψv,v−1(yv, yv−1)

(cid:111)

.

(19)

v (yv) the yv−1, (cid:96) that attain the max(k) above for each yv ∈ Yv.

yp∈Yp,k∈[K] π(k)

p (yp) and store in y(k)

p , (cid:96)(k) respectively the maxi-

Assign to δ(k)

v (yv), κ(k)

5:
6: end for
7: For k = 1, · · · , K, set ψ(k) ← max(k)

mizing assignments of yp, k.

8: for v = p − 1, · · · 1 and k = 1, · · · , K do

Set y(k)

v ← δ((cid:96)(k))

v+1

(cid:0)y(k)
v+1

(cid:1) and (cid:96)(k) ← κ((cid:96)(k))
v+1

(cid:0)y(k)
v+1

(cid:1).

9:
10: end for
11: return

(cid:110)

ψ(k), y(k) := (y(k)

1 , · · · , y(k)
p )

(cid:111)K

.

k=1

4.2.1

Inference Oracles using Max-Marginals

We now deﬁne a max-marginal, which is a constrained maximum of the augmented score ψ.

Deﬁnition 13. The max-marginal of ψ relative to a variable yv is deﬁned, for j ∈ Yv as

ψv;j(w) := max

ψ(y; w) .

y∈Y : yv=j

In cases where exact inference is tractable using graph cut or matching algorithms, it is possible to extract
max-marginals as well. This, as we shall see next, allows the implementation of the max and top-K oracles.
When the augmented score function ψ is unambiguous, i.e., no two distinct y1, y2 ∈ Y have the same
augmented score, the output y∗(w) is unique can be decoded from the max-marginals as (see Pearl [1988],
Dawid [1992] or Thm. 45 in Appendix C)

(20)

(21)

y∗
v(w) = arg max

ψv;j(w) .

j∈Yv

If one has access to an algorithm M that can compute max-marginals, the top-K oracle is also eas-
ily implemented via the Best Max-Marginal First (BMMF) algorithm of Yanover and Weiss [2004]. This
algorithm requires computations of 2K sets of max-marginals, where a set of max-marginals refers to max-
marginals for all yv in y. Therefore, the BMMF algorithm followed by a projection onto the simplex
(Algo. 6 in Appendix A) is a correct implementation of the top-K oracle at a computational cost of 2K sets
of max-marginals. The BMMF algorithm and its guarantee are recalled in Appendix C.1 for completeness.

Graph Cut and Matching Inference Kolmogorov and Zabin [2004] showed that submodular energy
functions [Lov´asz, 1983] over binary variables can be efﬁciently minimized exactly via a minimum cut
algorithm. For a class of alignment problems, e.g., Taskar et al. [2005], inference amounts to ﬁnding the
best bipartite matching. In both these cases, max-marginals can be computed exactly and efﬁciently by

15

Algorithm 3 Entropy smoothed max-product algorithm

1: Input: Augmented score function ψ(·, ·; w) deﬁned on tree structured graph G, µ > 0.
2: Compute the log-partition function and marginals using the sum-product algorithm (Algo. 9 in Ap-

pendix B)

Aψ/µ, {Pv for v ∈ V}, {Pv,v(cid:48) for (v, v(cid:48)) ∈ E} ← SUMPRODUCT

(cid:16) 1

µ ψ(· ; w), G

(cid:17)

.

3: Set f−µH (w) ← µAψ/µ and

∇f−µH (w) ←

Pv(yv)∇ψv(yv; w) +

Pv,v(cid:48)(yv, yv(cid:48))∇ψv,v(cid:48)(yv; w) .

(cid:88)

(cid:88)

v∈V

yv∈Yv

(cid:88)

(cid:88)

(cid:88)

(v,v(cid:48))∈E

yv∈Yv

yv(cid:48) ∈Yv(cid:48)

4: return f−µH (w), ∇f−µH (w).

combinatorial algorithms. This gives us a way to implement the max and top-K oracles. However, in
both settings, computing the log-partition function Aψ(w) of a CRF with score ψ is known to be #P-
complete [Jerrum and Sinclair, 1993]. Prop. 9 immediately extends this result to the exp oracle. This
discussion is summarized by the following proposition, whose proof is provided in Appendix C.4.

Proposition 14. Consider as inputs an augmented score function ψ(·, ·; w), an integer K > 0 and a smooth-
ing parameter µ > 0. Further, suppose that ψ is unambiguous, that is, ψ(y(cid:48); w) (cid:54)= ψ(y(cid:48)(cid:48); w) for all distinct
y(cid:48), y(cid:48)(cid:48) ∈ Y. Consider one of the two settings:

(A)

the output space Yv = {0, 1} for each v ∈ V, and the function −ψ is submodular (see Appendix C.2
and, in particular, (72) for the precise deﬁnition), or,

(B)

the augmented score corresponds to an alignment task where the inference problem (3) corresponds to
a maximum weight bipartite matching (see Appendix C.3 for a precise deﬁnition).

In these cases, we have the following:

(i) The max oracle can be implemented at a computational complexity of O(p) minimum cut computations

in Case (A), and in time O(p3) in Case (B).

(ii) The top-K oracle can be implemented at a computational complexity of O(pK) minimum cut compu-

tations in Case (A), and in time O(p3K) in Case (B).

(iii) The exp oracle is #P-complete in both cases.

Prop. 14 is loose in that the max oracle can be implemented with just one minimum cut computation

instead of p in in Case (A) [Kolmogorov and Zabin, 2004].

4.2.2 Branch and Bound Search

Max oracles implemented via search algorithms can often be extended to implement the top-K oracle. We
restrict our attention to best-ﬁrst branch and bound search such as the celebrated Efﬁcient Subwindow Search
[Lampert et al., 2008].

16

Branch and bound methods partition the search space into disjoint subsets, while keeping an upper
bound (cid:98)ψ : X × 2Y → R, on the maximal augmented score for each of the subsets (cid:98)Y ⊆ Y. Using a best-ﬁrst
strategy, promising parts of the search space are explored ﬁrst. Parts of the search space whose upper bound
indicates that they cannot contain the maximum do not have to be examined further.

The top-K oracle is implemented by simply continuing the search procedure until K outputs have been
produced - see Algo. 13 in Appendix C.5. Both the max oracle and the top-K oracle can degenerate to an
exhaustive search in the worst case, so we do not have sharp running time guarantees. However, we have
the following correctness guarantee.

Proposition 15. Consider an augmented score function ψ(·, ·; w), an integer K > 0 and a smoothing
parameter µ > 0. Suppose the upper bound function (cid:98)ψ(·, ·; w) : X × 2Y → R satisﬁes the following
properties:

(a) (cid:98)ψ( (cid:98)Y; w) is ﬁnite for every (cid:98)Y ⊆ Y,

(b) (cid:98)ψ( (cid:98)Y; w) ≥ maxy∈ (cid:98)Y ψ(y; w) for all (cid:98)Y ⊆ Y, and,

(c) (cid:98)ψ({y}; w) = ψ(y; w) for every y ∈ Y.

Then, we have the following:

(i) Algo. 13 with K = 1 is a correct implementation of the max oracle.

(ii) Algo. 13 followed by a projection onto the simplex (Algo. 6 in Appendix A) is a correct implementation

of the top-K oracle.

See Appendix C.5 for a proof. The discrete structure that allows inference via branch and bound search
cannot be leveraged to implement the exp oracle.

5 The Casimir Algorithm

We come back to the optimization problem (1) with f (i) deﬁned in (7). We assume in this section that the
mappings g(i) deﬁned in (6) are afﬁne. Problem (1) now reads

(cid:34)

min
w∈Rd

F (w) :=

1
n

n
(cid:88)

i=1

h(A(i)w + b(i)) +

(cid:107)w(cid:107)2
2

.

(cid:35)

λ
2

For a single input (n = 1), the problem reads

min
w∈Rd

h(Aw + b) +

λ
2

(cid:107)w(cid:107)2
2.

(22)

(23)

where h is a simple non-smooth convex function and λ ≥ 0. Nesterov [2005b,a] ﬁrst analyzed such set-
ting: while the problem suffers from its non-smoothness, fast methods can be developed by considering
smooth approximations of the objectives. We combine this idea with the Catalyst acceleration scheme [Lin
et al., 2018] to accelerate a linearly convergent smooth optimization algorithm resulting in a scheme called
Casimir.

17

5.1 Casimir: Catalyst with Smoothing

The Catalyst [Lin et al., 2018] approach minimizes regularized objectives centered around the current it-
erate. The algorithm proceeds by computing approximate proximal point steps instead of the classical
(sub)-gradient steps. A proximal point step from a point w with step-size κ−1 is deﬁned as the minimizer
of

min
z∈Rm

F (z) +

κ
2

(cid:107)z − w(cid:107)2
2,

(24)

which can also be seen as a gradient step on the Moreau envelope of F - see Lin et al. [2018] for a detailed
discussion. While solving the subproblem (24) might be as hard as the original problem we only require
an approximate solution returned by a given optimization method M. The Catalyst approach is then an
inexact accelerated proximal point algorithm that carefully mixes approximate proximal point steps with
the extrapolation scheme of Nesterov [1983]. The Casimir scheme extends this approach to non-smooth
optimization.

For the overall method to be efﬁcient, subproblems (24) must have a low complexity. That is, there must
exist an optimization algorithm M that solves them linearly. For the Casimir approach to be able to handle
non-smooth objectives, it means that we need not only to regularize the objective but also to smooth it. To
this end we deﬁne

Fµω(w) :=

hµω(A(i)w + b(i)) +

(cid:107)w(cid:107)2
2

λ
2

1
n

n
(cid:88)

i=1

as a smooth approximation of the objective F , and,

Fµω,κ(w; z) :=

hµω(A(i)w + b(i)) +

(cid:107)w(cid:107)2

2 +

(cid:107)w − z(cid:107)2
2

λ
2

κ
2

1
n

n
(cid:88)

i=1

a smooth and regularized approximation of the objective centered around a given point z ∈ Rd. While the
original Catalyst algorithm considered a ﬁxed regularization term κ, we vary κ and µ along the iterations.
This enables us to get adaptive smoothing strategies.

The overall method is presented in Algo. 4. We ﬁrst analyze in Sec. 5.2 its complexity for a generic lin-
early convergent algorithm M. Thereafter, in Sec. 5.3, we compute the total complexity with SVRG [John-
son and Zhang, 2013] as M. Before that, we specify two practical aspects of the implementation: a proper
stopping criterion (26) and a good initialization of subproblems (Line 4).

Stopping Criterion Following Lin et al. [2018], we solve subproblem k in Line 4 to a degree of relative ac-
curacy speciﬁed by δk ∈ [0, 1). In view of the (λ + κk)-strong convexity of Fµkω,κk (· ; zk−1), the functional
gap can be controlled by the norm of the gradient, precisely it can be seen that (cid:107)∇Fµkω,κk ( (cid:98)w; zk−1)(cid:107)2
2 ≤
(λ + κk)δkκk(cid:107) (cid:98)w − zk−1(cid:107)2

2 is a sufﬁcient condition for the stopping criterion (26).
A practical alternate stopping criterion proposed by Lin et al. [2018] is to ﬁx an iteration budget Tbudget
and run the inner solver M for exactly Tbudget steps. We do not have a theoretical analysis for this scheme
but ﬁnd that it works well in experiments.

Warm Start of Subproblems Rate of convergence of ﬁrst order optimization algorithms depends on the
initialization and we must warm start M at an appropriate initial point in order to obtain the best convergence
of subproblem (25) in Line 4 of Algo. 4. We advocate the use of the prox center zk−1 in iteration k as the
warm start strategy. We also experiment with other warm start strategies in Section 7.

18

Algorithm 4 The Casimir algorithm

1: Input: Smoothable objective F of the form (23) with h simple, smoothing function ω, linearly con-
vergent algorithm M, non-negative and non-increasing sequence of smoothing parameters (µk)k≥1,
positive and non-decreasing sequence of regularization parameters (κk)k≥1, non-negative sequence of
relative target accuracies (δk)k≥1 and, initial point w0, α0 ∈ (0, 1), time horizon K.

Using M with zk−1 as the starting point, ﬁnd wk ≈ arg minw∈Rd Fµkω,κk (w; zk−1) where

Fµkω,κk (w; zk−1) :=

hµkω(A(i)w + b(i)) +

(cid:107)w(cid:107)2

2 +

(cid:107)w − zk−1(cid:107)2
2

(25)

λ
2

κk
2

1
n

n
(cid:88)

i=1

Fµkω,κk (wk; zk−1) − min
w

Fµkω,κk (w; zk−1) ≤ δkκk

2 (cid:107)wk − zk−1(cid:107)2

2

k(κk+1 + λ) = (1 − αk)α2
α2

k−1(κk + λ) + αkλ.

zk = wk + βk(wk − wk−1),

βk =

αk−1(1 − αk−1)(κk + λ)
k−1(κk + λ) + αk(κk+1 + λ)

α2

.

(26)

(27)

(28)

(29)

2: Initialize: z0 = w0.
3: for k = 1 to K do
4:

such that

5:

Solve for αk ≥ 0

6:

Set

where

7: end for
8: return wK.

5.2 Convergence Analysis of Casimir

We ﬁrst state the outer loop complexity results of Algo. 4 for any generic linearly convergent algorithm M in
Sec. 5.2.1, prove it in Sec. 5.2.2. Then, we consider the complexity of each inner optimization problem (25)
in Sec. 5.2.3 based on properties of M.

5.2.1 Outer Loop Complexity Results

The following theorem states the convergence of the algorithm for general choice of parameters, where we
denote w∗ ∈ arg minw∈Rd F (w) and F ∗ = F (w∗).

Theorem 16. Consider Problem (22). Suppose δk ∈ [0, 1) for all k ≥ 1, the sequence (µk)k≥1 is non-
negative and non-increasing, and the sequence (κk)k≥1 is strictly positive and non-decreasing. Further,
suppose the smoothing function ω : dom h∗ → R satisﬁes −Dω ≤ ω(u) ≤ 0 for all u ∈ dom h∗ and that

19

0 ≥ λ/(λ + κ1). Then, the sequence (αk)k≥0 generated by Algo. 4 satisﬁes 0 < αk ≤ αk−1 < 1 for all

α2
k ≥ 1. Furthermore, the sequence (wk)k≥0 of iterates generated by Algo. 4 satisﬁes

F (wk) − F ∗ ≤

∆0 + µkDω +

(µj−1 − (1 − δj)µj) Dω ,

(30)

Ak−1
0
Bk
1

k
(cid:88)

j=1

Ak−1
j
Bk
j

where Aj
µ0 := 2µ1.

i := (cid:81)j

r=i(1 − αr), Bj

i := (cid:81)j

r=i(1 − δr), ∆0 := F (w0) − F ∗ + (κ1+λ)α2

0−λα0

2(1−α0)

(cid:107)w0 − w∗(cid:107)2

2 and

Before giving its proof, we present various parameters strategies as corollaries. Table 3 summarizes the
parameter settings and the rates obtained for each setting. Overall, the target accuracies δk are chosen such
that Bk
j is a constant and the parameters µk and κk are then carefully chosen for an almost parameter-free
algorithm with the right rate of convergence. Proofs of these corollaries are provided in Appendix D.2.

The ﬁrst corollary considers the strongly convex case (λ > 0) with constant smoothing µk = µ, as-
suming that (cid:15) is known a priori. We note that this is, up to constants, the same complexity obtained by the
original Catalyst scheme on a ﬁxed smooth approximation Fµω with µ = O((cid:15)Dω).

Corollary 17. Consider the setting of Thm. 16. Let q = λ/(λ + κ). Suppose λ > 0 and µk = µ, κk = κ,
√
for all k ≥ 1. Choose α0 =

q) . Then, we have,

q and, δk =

q/(2 −

√

√

F (wk) − F ∗ ≤

µDω + 2

1 −

(F (w0) − F ∗) .

√
√

q
q

3 −
1 −

(cid:18)

(cid:19)k

√

q
2

Next, we consider the strongly convex case where the target accuracy (cid:15) is not known in advance. We
let smoothing parameters (µk)k≥0 decrease over time to obtain an adaptive smoothing scheme that gives
progressively better surrogates of the original objective.

Corollary 18. Consider the setting of Thm. 16. Let q = λ/(λ + κ) and η = 1 −
κk = κ, for all k ≥ 1. Choose α0 =

q and, the sequences (µk)k≥1 and (δk)k≥1 as

√

√

q/2. Suppose λ > 0 and

µk = µηk/2 ,

and,

δk =

√

q
√

,

q

2 −

where µ > 0 is any constant. Then, we have,

F (wk) − F ∗ ≤ ηk/2

(cid:20)
2 (F (w0) − F ∗) +

µDω
√
q
1 −

(cid:18)

√

2 −

q +

√

(cid:19)(cid:21)

.

q
√
η

1 −

The next two corollaries consider the unregularized problem, i.e., λ = 0 with constant and adaptive smooth-
ing respectively.

Corollary 19. Consider the setting of Thm. 16. Suppose µk = µ, κk = κ, for all k ≥ 1 and λ = 0. Choose
α0 = (

5 − 1)/2 and δk = (k + 1)−2 . Then, we have,

√

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2
2

+ µDω

1 +

(cid:16)

8
(k + 2)2

(cid:17)

(cid:18)

12
k + 2

+

30
(k + 2)2

(cid:19)

.

κ
2

20

Table 3: Summary of outer iteration complexity for Algorithm 4 for different parameter settings. We use shorthand
∆F0 := F (w0) − F ∗ and ∆0 = (cid:107)w0 − w∗(cid:107)2. Absolute constants are omitted from the rates.

Cor.

λ > 0

κk

17

18

19

20

Yes

Yes

No

No

κ

κ

κ

µk

µ
√

q
2

µ

(cid:16)

µ

1 −

(cid:17)k/2

κ k

µ/k

δk
√
q
√

q

2−
√

q
√

2−
q
k−2

k−2

α0
√

√

q

q

c

c

F (wk) − F ∗
√

(cid:16)

1 −
√

q
2

(cid:16)

1 −

(cid:17)k

q
2
(cid:17)k/2 (cid:16)

∆F0 + µD
√
1−

q
∆F0 + µD
√
1−
(cid:1) + µD

(cid:17)

q

(cid:0)∆F0 + κ∆2
1
0
k2
log k
k (∆F0 + κ∆2
0 + µD)

Remark

q = λ
λ+κ

q = λ
λ+κ
√
5 − 1)/2

c = (

√

c = (

5 − 1)/2

Corollary 20. Consider the setting of Thm. 16 with λ = 0. Choose α0 = (
non-negative constants κ, µ, deﬁne sequences (κk)k≥1, (µk)k≥1, (δk)k≥1 as

√

5 − 1)/2, and for some

κk = κ k , µk =

and,

δk =

µ
k

1
(k + 1)2 .

Then, for k ≥ 2, we have,

F (wk) − F ∗ ≤

log(k + 1)
k + 1

(cid:0)2(F (w0) − F ∗) + κ(cid:107)w0 − w∗(cid:107)2

2 + 27µDω

(cid:1) .

For the ﬁrst iteration (i.e., k = 1), this bound is off by a constant factor 1/ log 2.

5.2.2 Outer Loop Convergence Analysis

We now prove Thm. 16. The proof technique largely follows that of Lin et al. [2018], with the added
challenges of accounting for smoothing and varying Moreau-Yosida regularization. We ﬁrst analyze the
sequence (αk)k≥0. The proof follows from the algebra of Eq. (27) and has been given in Appendix D.1.

Lemma 21. Given a positive, non-decreasing sequence (κk)k≥1 and λ ≥ 0, consider the sequence (αk)k≥0
deﬁned by (27), where α0 ∈ (0, 1) such that α2
0 ≥ λ/(λ + κ1). Then, we have for every k ≥ 1 that
0 < αk ≤ αk−1 and, α2

k ≥ λ/(λ + κk+1) .

We now characterize the effect of an approximate proximal point step on Fµω.
Lemma 22. Suppose (cid:98)w ∈ Rd satisﬁes Fµω,κ( (cid:98)w; z) − minw∈Rd Fµω,κ(w; z) ≤ (cid:98)(cid:15) for some (cid:98)(cid:15) > 0. Then, for
all 0 < θ < 1 and all w ∈ Rd, we have,

Fµω( (cid:98)w) +

(cid:107) (cid:98)w − z(cid:107)2

2 +

(1 − θ)(cid:107)w − (cid:98)w(cid:107)2

2 ≤ Fµω(w) +

(cid:107)w − z(cid:107)2

κ
2

2 + (cid:98)(cid:15)
θ

.

(31)

κ + λ
2

κ
2

Proof. Let (cid:98)F ∗ = minw∈Rd Fµω,κ(w; z). Let (cid:98)w∗ be the unique minimizer of Fµω,κ(· ; z). We have, from
(κ + λ)-strong convexity of Fµω,κ(· ; z),

Fµω,κ(w; z) ≥ (cid:98)F ∗ +

κ + λ
2

(cid:107)w − (cid:98)w∗(cid:107)2

2

≥ (Fµω,κ( (cid:98)w; z) − (cid:98)(cid:15)) +

(1 − θ)(cid:107)w − (cid:98)w(cid:107)2

2 −

κ + λ
2

κ + λ
2

(cid:18) 1
θ

(cid:19)

− 1

(cid:107) (cid:98)w − (cid:98)w∗(cid:107)2
2 ,

21

where we used that (cid:98)(cid:15) was sub-optimality of (cid:98)w and Lemma 51 from Appendix D.7. From (κ + λ)-strong
convexity of Fµω,κ(·; z), we have,

κ + λ
2

(cid:107) (cid:98)w − (cid:98)w∗(cid:107)2

2 ≤ Fµω,κ( (cid:98)w; z) − (cid:98)F ∗ ≤ (cid:98)(cid:15) ,

Since (1/θ − 1) is non-negative, we can plug this into the previous statement to get,

Fµω,κ(w; z) ≥ Fµω,κ( (cid:98)w; z) +

κ + λ
2

(1 − θ)(cid:107)w − (cid:98)w(cid:107)2

2 − (cid:98)(cid:15)
θ

.

Substituting the deﬁnition of Fµω,κ(· ; z) from (25) completes the proof.

We now deﬁne a few auxiliary sequences integral to the proof. Deﬁne sequences (vk)k≥0, (γk)k≥0,

(ηk)k≥0, and (rk)k≥1 as

One might recognize γk and vk from their resemblance to counterparts from the proof of Nesterov [2013].
Now, we claim some properties of these sequences.

Claim 23. For the sequences deﬁned in (32)-(37), we have,

v0 = w0

vk = wk−1 +

(wk − wk−1) , k ≥ 1 ,

1
αk−1

γ0 =

(κ1 + λ)α2
1 − α0
γk = (κk + λ)α2

ηk =

αkγk
γk+1 + αkγk

0 − λα0

,

k−1 , k ≥ 1 ,

, k ≥ 0 ,

rk = αk−1w∗ + (1 − αk−1)wk−1 , k ≥ 1 .

γk =

(κk+1 + λ)α2
1 − αk
γk+1 = (1 − αk)γk + λαk , k ≥ 0 ,

k − λαk

, k ≥ 0 ,

ηk =

αkγk
γk + αkλ

, k ≥ 0

zk = ηkvk + (1 − ηk)wk , k ≥ 0 , .

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

Proof. Eq. (38) follows from plugging in (27) in (35) for k ≥ 1, while for k = 0, it is true by deﬁnition.
Eq. (39) follows from plugging (35) in (38). Eq. (40) follows from (39) and (36). Lastly, to show (41), we
shall show instead that (41) is equivalent to the update (28) for zk. We have,

zk = ηkvk + (1 − ηk)wk

(cid:18)

(33)
= ηk

(cid:19)

(wk − wk−1)

+ (1 − ηk)wk

= wk + ηk

− 1

(wk − wk−1) .

1
αk−1

wk−1 +
(cid:18) 1

αk−1

(cid:19)

22

Now,

ηk

(cid:18) 1

αk−1

(cid:19) (36)
=

− 1

·

αkγk
γk+1 + αkγk

1 − αk−1
αk−1
αk(κk + λ)α2
k(κk+1 + λ) + αk(κk + λ)α2
α2

k−1

k−1

(35)
=

·

1 − αk−1
αk−1

(29)
= βk ,

completing the proof.

Claim 24. The sequence (rk)k≥1 from (37) satisﬁes

(cid:107)rk − zk−1(cid:107)2

2 ≤ αk−1(αk−1 − ηk−1)(cid:107)wk−1 − w∗(cid:107)2

2 + αk−1ηk−1(cid:107)vk−1 − w∗(cid:107)2
2 .

(42)

Proof. Notice that ηk
get,

(40)
= αk ·

γk

γk+αkλ ≤ αk. Hence, using convexity of the squared Euclidean norm, we

(cid:107)rk − zk−1(cid:107)2
2

(41)
= (cid:107)(αk−1 − ηk−1)(w∗ − wk−1) + ηk−1(w∗ − vk−1)(cid:107)2
2

= α2

k−1

1 −

(w∗ − wk−1) +

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:18)

(cid:19)

ηk−1
αk−1
(cid:19)
ηk−1
αk−1

(∗)
≤ α2

k−1

1 −

(cid:107)wk−1 − w∗(cid:107)2

2 + α2

k−1

(cid:107)vk−1 − w∗(cid:107)2
2

ηk−1
αk−1

ηk−1
αk−1

(cid:13)
2
(cid:13)
(w∗ − vk−1)
(cid:13)
(cid:13)
2

= αk−1(αk−1 − ηk−1)(cid:107)wk−1 − w∗(cid:107)2

2 + αk−1ηk−1(cid:107)vk−1 − w∗(cid:107)2
2 .

For all µ ≥ µ(cid:48) ≥ 0, we know from Prop. 2 that

0 ≤ Fµω(w) − Fµ(cid:48)ω(w) ≤ (µ − µ(cid:48))Dω .

We now deﬁne the sequence (Sk)k≥0 to play the role of a potential function here.

S0 = (1 − α0)(F (w0) − F (w∗)) +

α0κ1η0
2
Sk = (1 − αk)(Fµkω(wk) − Fµkω(w∗)) +

(cid:107)w0 − w∗(cid:107)2
2 ,
αkκk+1ηk
2

(cid:107)vk − w∗(cid:107)2

2 , k ≥ 1 .

(43)

(44)

We are now ready to analyze the effect of one outer loop. This lemma is the crux of the analysis.

Lemma 25. Suppose Fµkω,κk (wk; z) − minw∈Rd Fµkω,κk (w; z) ≤ (cid:15)k for some (cid:15)k > 0. The following
statement holds for all 0 < θk < 1:

Sk
1 − αk

≤ Sk−1 + (µk−1 − µk)Dω +

(cid:107)wk − zk−1(cid:107)2

2 +

(cid:107)vk − w∗(cid:107)2
2 ,

(45)

(cid:15)k
θk

−

κk
2

κk+1ηkαkθk
2(1 − αk)

where we set µ0 := 2µ1.

Proof. For ease of notation, let Fk := Fµkω, and D := Dω. By λ-strong convexity of Fµkω, we have,

Fk(rk) ≤ αk−1Fk(w∗) + (1 − αk−1)Fk(wk−1) −

λαk−1(1 − αk−1)
2

(cid:107)wk−1 − w∗(cid:107)2
2 .

(46)

23

We now invoke Lemma 22 on the function Fµkω,κk (·; zk−1) with (cid:98)(cid:15) = (cid:15)k and w = rk to get,

Fk(wk) +

(cid:107)wk − zk−1(cid:107)2

2 +

(1 − θk)(cid:107)rk − wk(cid:107)2

2 ≤ Fk(rk) +

(cid:107)rk − zk−1(cid:107)2

2 +

.

(47)

κk
2

κk + λ
2

κk
2

(cid:15)k
θk

We shall separately manipulate the left and right hand sides of (47), starting with the right hand side, which
we call R. We have, using (46) and (42),

R ≤ (1 − αk−1)Fk(wk−1) + αk−1Fk(w∗) −

+

κk
2

αk−1(αk−1 − ηk−1)(cid:107)wk−1 − w∗(cid:107)2

λαk−1(1 − αk−1)
2
κkαk−1ηk−1
2

2 +

(cid:107)wk−1 − w∗(cid:107)2
2

(cid:107)vk−1 − w∗(cid:107)2

2 +

(cid:15)k
θk

.

We notice now that

αk−1 − ηk−1

(40)
= αk−1 −

αk−1γk−1
γk + αk−1γk−1
(cid:18) γk − γk−1(1 − αk−1)
γk + αk−1γk−1

(cid:19)

= αk−1

(39)
=

(38)
=

=

α2

k−1λ
γk−1 + αk−1λ

(κk + λ)α2
λ
κk

(1 − αk−1) ,

α2

k−1λ(1 − αk−1)

k−1 − λαk−1 + (1 − αk−1)αk−1λ

and hence the terms containing (cid:107)wk−1 − w∗(cid:107)2

2 cancel out. Therefore, we get,

R ≤ (1 − αk−1)Fk(wk−1) + αk−1Fk(w∗) +

κkαk−1ηk−1
2

(cid:107)vk−1 − w∗(cid:107)2

2 +

(cid:15)k
θk

.

To move on to the left hand side, we note that

αkηk

(40)
=

α2
kγk
γk + αkλ

(35),(38)
=

kα2
α2
(κk+1+λ)α2
1−αk

k−1(κk + λ)
k−λαk

+ αkλ

=

(1 − αk)(κk + λ)α2
(κk+1 + λ)α2

k − λα2
k

k−1α2
k

= (1 − αk)α2

k−1

κk + λ
κk+1

.

Therefore,

Fk(wk) − Fk(w∗) +

κk + λ
2

k−1(cid:107)vk − w∗(cid:107)2
α2
2

(44),(50)
=

Sk
1 − αk

.

Using rk − wk

(33)
= αk−1(w∗ − vk), we simplify the left hand side of (47), which we call L, as

L = Fk(wk) − Fk(w∗) +

(cid:107)wk − zk−1(cid:107)2

2 +

(1 − θk)α2

k−1(cid:107)vk − w∗(cid:107)2
2

κk + λ
2

(51)
=

Sk
1 − αk

+ Fk(w∗) +

(cid:107)wk − zk−1(cid:107)2

2 −

κk+1αkηkθk
2(1 − αk)

(cid:107)vk − w∗(cid:107)2
2 .

κk
2
κk
2

24

(48)

(49)

(50)

(51)

(52)

In view of (49) and (52), we can simplify (47) as

Sk
1 − αk

+

κk
2

(cid:107)wk − zk−1(cid:107)2

2 −

κk+1αkηkθk
2(1 − αk)

≤ (1 − αk−1) (Fk(wk−1) − Fk(w∗)) +

(cid:107)vk − w∗(cid:107)2
2
κkαk−1ηk−1
2

(cid:107)vk−1 − w∗(cid:107)2

2 +

(cid:15)k
θk

.

We make a distinction for k ≥ 2 and k = 1 here. For k ≥ 2, the condition that µk−1 ≥ µk gives us,

Fk(wk−1) − Fk(w∗)

(43)
≤ Fk−1(wk−1) − Fk−1(w∗) + (µk−1 − µk)D .

(53)

(54)

The right hand side of (53) can now be upper bounded by

(1 − αk−1)(µk−1 − µk)D + Sk−1 +

(cid:15)k
θk

,

and noting that 1 − αk−1 ≤ 1 yields (45) for k ≥ 2.

For k = 1, we note that Sk−1(= S0) is deﬁned in terms of F (w). So we have,

F1(w0) − F1(w∗) ≤ F (w0) − F (w∗) + µ1D = F (w0) − F (w∗) + (µ0 − µ1)D ,

because we used µ0 = 2µ1. This is of the same form as (54). Therefore, (45) holds for k = 1 as well.

We now prove Thm. 16.

Proof of Thm. 16. We continue to use shorthand Fk := Fµkω, and D := Dω. We now apply Lemma 25. In
order to satisfy the supposition of Lemma 25 that wk is (cid:15)k-suboptimal, we make the choice (cid:15)k = δkκk
2 (cid:107)wk −
zk−1(cid:107)2

2 (cf. (26)). Plugging this in and setting θk = δk < 1, we get from (45),

Sk
1 − αk

−

κk+1ηkαkδk
2(1 − αk)

(cid:107)vk − w∗(cid:107)2

2 ≤ Sk−1 + (µk−1 − µk)D .

The left hand side simpliﬁes to Sk (1 − δk)/(1 − αk) + δk(Fk(wk) − Fk(w∗)). Note that Fk(wk) −
(43)
≥ F (wk) − F (w∗) − µkD ≥ −µkD. From this, noting that αk ∈ (0, 1) for all k, we get,
Fk(w∗)

Sk

(cid:19)

(cid:18) 1 − δk
1 − αk

≤ Sk−1 + δkµkD + (µk−1 − µk)D ,

or equivalently,

Sk ≤

(cid:19)

(cid:18) 1 − αk
1 − δk

(cid:19)

(cid:18) 1 − αk
1 − δk

Sk−1 +

(µk−1 − (1 − δk)µk)D .

Unrolling the recursion for Sk, we now have,

Sk ≤





k
(cid:89)

j=1

1 − αj
1 − δj



 S0 +

k
(cid:88)

k
(cid:89)





j=1

i=j

1 − αi
1 − δi



 (µj−1 − (1 − δj)µj)D .

(55)

25

Now, we need to reason about S0 and Sk to complete the proof. To this end, consider η0:

(36)
=

η0

α0γ0
γ1 + α0γ0

α0γ0

(34)
=

=

(κ1 + λ)α2

0 + α0
1−α0

α0γ0(1 − α0)

(κ1 + λ)α2

0 − λα2
0

(cid:1)

(cid:0)(κ1 + λ)α2
0 − λα0
γ0
κ1α0

= (1 − α0)

.

(56)

With this, we can expand out S0 to get

S0

(44)
= (1 − α0) (F (w0) − F (w∗)) +
γ0
2

F (w0) − F ∗ +

(56)
= (1 − α0)

(cid:16)

(cid:107)w0 − w∗(cid:107)2
2

α0κ1η0
2
(cid:107)w0 − w∗(cid:107)2
2

(cid:17)

.

Lastly, we reason about Sk for k ≥ 1 as,

(44)
≥ (1 − αk) (Fk(wk) − Fk(w∗))

(43)
≥ (1 − αk) (F (wk) − F (w∗) − µkD) .

Sk

Plugging this into the left hand side of (55) completes the proof.

5.2.3

Inner Loop Complexity

Consider a class FL,λ of functions deﬁned as

(cid:110)

FL,λ =

f : Rd → R such that f is L-smooth and λ-strongly convex

(cid:111)

.

We now formally deﬁne a linearly convergent algorithm on this class of functions.

Deﬁnition 26. A ﬁrst order algorithm M is said to be linearly convergent with parameters C : R+ × R+ →
R+ and τ : R+ × R+ → (0, 1) if the following holds: for all L ≥ λ > 0, and every f ∈ FL,λ and w0 ∈ Rd,
M started at w0 generates a sequence (wk)k≥0 that satisﬁes:

Ef (wk) − f ∗ ≤ C(L, λ) (1 − τ (L, λ))k (f (w0) − f ∗) ,

(57)

where f ∗ := minw∈Rd f (w) and the expectation is over the randomness of M.

The parameter τ determines the rate of convergence of the algorithm. For instance, batch gradient
descent is a deterministic linearly convergent algorithm with τ (L, λ)−1 = L/λ and incremental algorithms
such as SVRG and SAGA satisfy requirement (57) with τ (L, λ)−1 = c(n+ L/λ) for some universal constant
c.

The warm start strategy in step k of Algo. 4 is to initialize M at the prox center zk−1. The next
proposition, due to Lin et al. [2018, Cor. 16] bounds the expected number of iterations of M required to
ensure that wk satisﬁes (26). Its proof has been given in Appendix D.3 for completeness.

Proposition 27. Consider Fµω,κ(· ; z) deﬁned in Eq. (25), and a linearly convergent algorithm M with
parameters C, τ . Let δ ∈ [0, 1). Suppose Fµω is Lµω-smooth and λ-strongly convex. Then the expected
number of iterations E[ (cid:98)T ] of M when started at z in order to obtain (cid:98)w ∈ Rd that satisﬁes

Fµω,κ( (cid:98)w; z) − min
w

Fµω,κ(w; z) ≤ δκ

2 (cid:107)w − z(cid:107)2

2

26

Table 4: Summary of global complexity of Casimir-SVRG, i.e., Algorithm 4 with SVRG as the inner solver for various
parameter settings. We show E[N ], the expected total number of SVRG iterations required to obtain an accuracy (cid:15), up
to constants and factors logarithmic in problem parameters. We denote ∆F0 := F (w0) − F ∗ and ∆0 = (cid:107)w0 − w∗(cid:107)2.
Constants D, A are short for Dω, Aω (see (58)).

Prop.

λ > 0

µk

κk

δk
(cid:113) λ(cid:15)n
AD

c(cid:48)

(cid:15)/D AD/(cid:15)n − λ

λ

29

30

31

32

Yes

Yes

No

No

µck

(cid:15)/D

µ/k

AD/(cid:15)n

1/k2

n

κ0 k

1/k2

E[N ]
(cid:113) ADn
λ(cid:15)

n +

n + A
λ(cid:15)
(cid:113) ∆F0

∆F0+µD
µ

√

(cid:15) +
(cid:16)

ADn∆0
(cid:15)
(cid:17)

(cid:98)∆0
(cid:15)

n + A
µκ0

Remark

ﬁx (cid:15) in advance

c, c(cid:48) < 1 are universal constants

ﬁx (cid:15) in advance

(cid:98)∆0 = ∆F0 + κ0

2 ∆2

0 + µD

is upper bounded by

E[ (cid:98)T ] ≤

1
τ (Lµω + κ, λ + κ)

log

(cid:18) 8C(Lµω + κ, λ + κ)
τ (Lµω + κ, λ + κ)

·

Lµω + κ
κδ

(cid:19)

+ 1 .

5.3 Casimir with SVRG

We now choose SVRG [Johnson and Zhang, 2013] to be the linearly convergent algorithm M, resulting
in an algorithm called Casimir-SVRG. The rest of this section analyzes the total iteration complexity of
Casimir-SVRG to solve Problem (22). The proofs of the results from this section are calculations stemming
from combining the outer loop complexity from Cor. 17 to 20 with the inner loop complexity from Prop. 27,
and are relegated to Appendix D.4. Table 4 summarizes the results of this section.

Recall that if ω is 1-strongly convex with respect to (cid:107) · (cid:107)α, then hµω(Aw + b) is Lµω-smooth with
2,α/µ. Therefore, the complexity of solving problem (22) will depend

respect to (cid:107) · (cid:107)2, where Lµω = (cid:107)A(cid:107)2
on

Aω := max
i=1,··· ,n

(cid:107)A(i)(cid:107)2

2,α .

(58)

Remark 28. We have that (cid:107)A(cid:107)2,2 = (cid:107)A(cid:107)2 is the spectral norm of A and (cid:107)A(cid:107)2,1 = maxj (cid:107)aj(cid:107)2 is the
largest row norm, where aj is the jth row of A. Moreover, we have that (cid:107)A(cid:107)2,2 ≥ (cid:107)A(cid:107)2,1.

We start with the strongly convex case with constant smoothing.

Proposition 29. Consider the setting of Thm. 16 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as the inner
solver with parameters: µk = µ = (cid:15)/10Dω, κk = k chosen as

√

√

√

q = λ/(λ + κ), α0 =
F (w) − F (w∗) ≤ (cid:15) is bounded in expectation as
(cid:32)

q, and δ =

q/(2 −

q). Then, the number of iterations N to obtain w such that

κ =

(cid:40) A

µn − λ , if A
λ , otherwise

µn > 4λ

,

E[N ] ≤ (cid:101)O

n +

(cid:114)

(cid:33)

.

AωDωn
λ(cid:15)

27

Here, we note that κ was chosen to minimize the total complexity (cf. Lin et al. [2018]). This bound is
known to be tight, up to logarithmic factors [Woodworth and Srebro, 2016]. Next, we turn to the strongly
convex case with decreasing smoothing.

Proposition 30. Consider the setting of Thm. 16. Suppose λ > 0 and κk = κ, for all k ≥ 1 and that α0,
(µk)k≥1 and (δk)k≥1 are chosen as in Cor. 18, with q = λ/(λ+κ) and η = 1−
q/2. If we run Algo. 4 with
SVRG as the inner solver with these parameters, the number of iterations N of SVRG required to obtain w
such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

√

(cid:18)

E[N ] ≤ (cid:101)O

n +

(cid:18)

Aω
µ(λ + κ)(cid:15)

F (w0) − F ∗ +

µDω
√
1 −

q

(cid:19)(cid:19)

.

Unlike the previous case, there is no obvious choice of κ, such as to minimize the global complexity. Notice
that we do not get the accelerated rate of Prop. 29. We now turn to the case when λ = 0 and µk = µ for all
k.

Proposition 31. Consider the setting of Thm. 16 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as the inner
5 − 1)/2, δk = 1/(k + 1)2, and κk = κ =
solver with parameters: µk = µ = (cid:15)/20Dω, α0 = (
Aω/µ(n + 1). Then, the number of iterations N to get a point w such that F (w) − F ∗ ≤ (cid:15) is bounded in
expectation as

√

(cid:32)

(cid:114)

E[N ] ≤ (cid:101)O

n

F (w0) − F ∗
(cid:15)

(cid:112)

+

AωDωn

(cid:107)w0 − w∗(cid:107)2
(cid:15)

(cid:33)

.

This rate is tight up to log factors [Woodworth and Srebro, 2016]. Lastly, we consider the non-strongly
convex case (λ = 0) together with decreasing smoothing. As with Prop. 30, we do not obtain an accelerated
rate here.

Proposition 32. Consider the setting of Thm. 16. Suppose λ = 0 and that α0, (µk)k≥1,(κk)k≥1 and (δk)k≥1
are chosen as in Cor. 20. If we run Algo. 4 with SVRG as the inner solver with these parameters, the number
of iterations N of SVRG required to obtain w such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

E[N ] ≤ (cid:101)O

(cid:0)F (w0) − F ∗ + κ(cid:107)w0 − w∗(cid:107)2

2 + µD(cid:1)

(cid:18) 1
(cid:15)

(cid:18)

n +

(cid:19)(cid:19)

.

Aω
µκ

6 Extension to Non-Convex Optimization

Let us now turn to the optimization problem (1) in full generality where the mappings g(i) deﬁned in (6) are
not constrained to be afﬁne:

(cid:34)

min
w∈Rd

F (w) :=

1
n

n
(cid:88)

i=1

h(g(i)(w)) +

(cid:107)w(cid:107)2
2

,

λ
2

(cid:35)

(59)

where h is a simple, non-smooth, convex function, and each g(i) is a continuously differentiable nonlinear
map and λ ≥ 0.

We describe the prox-linear algorithm in Sec. 6.1, followed by the convergence guarantee in Sec. 6.2

and the total complexity of using Casimir-SVRG together with the prox-linear algorithm in Sec. 6.3.

28

Algorithm 5 (Inexact) Prox-linear algorithm: outer loop

1: Input: Smoothable objective F of the form (59) with h simple, step length η, tolerances ((cid:15)k)k≥1, initial

point w0, non-smooth convex optimization algorithm, M, time horizon K

2: for k = 1 to K do
3:

Using M with wk−1 as the starting point, ﬁnd

(cid:20)

(cid:98)wk ≈ arg min

w

Fη(w; wk−1) :=

1
n

n
(cid:88)

i=1

h(cid:0)g(i)(wk−1) + ∇g(i)(wk−1)(w − wk−1)(cid:1)

+

(cid:107)w(cid:107)2

2 +

(cid:107)w − wk−1(cid:107)2
2 ,

λ
2

1
2η

(cid:21)

(61)

(62)

such that

4:
5: end for
6: return wK.

Fη( (cid:98)wk; wk−1) − min
w∈Rd

Fη(w; wk−1) ≤ (cid:15)k .

Set wk = (cid:98)wk if F ( (cid:98)wk) ≤ F (wk−1), else set wk = wk−1.

6.1 The Prox-Linear Algorithm

The exact prox-linear algorithm of Burke [1985] generalizes the proximal gradient algorithm (see e.g.,
Nesterov [2013]) to compositions of convex functions with smooth mappings such as (59). When given a
function f = h ◦ g, the prox-linear algorithm deﬁnes a local convex approximation f (· ; wk) about some
point w ∈ Rd by linearizing the smooth map g as f (w; wk) := h(g(wk) + ∇g(wk)(w − wk)) . With this,
it builds a convex model F (· ; wk) of F about wk as

F (w; wk) :=

h(g(i)(wk) + ∇g(i)(wk)(w − wk)) +

(cid:107)w(cid:107)2
2 .

λ
2

Given a step length η > 0, each iteration of the exact prox-linear algorithm then minimizes the local convex
model plus a proximal term as

wk+1 = arg min

Fη(w; wk) := F (w; wk) +

(60)

(cid:21)

(cid:107)w − wk(cid:107)2
2

.

1
2η

Following Drusvyatskiy and Paquette [2018], we consider an inexact prox-linear algorithm, which ap-
proximately solves (60) using an iterative algorithm. In particular, since the function to be minimized in
(60) is precisely of the form (23), we employ the fast convex solvers developed in the previous section as
subroutines. Concretely, the prox-linear outer loop is displayed in Algo. 5. We now delve into details about
the algorithm and convergence guarantees.

6.1.1

Inexactness Criterion

As in Section 5, we must be prudent in choosing when to terminate the inner optimization (Line 3 of Algo. 5).
Function value suboptimality is used as the inexactness criterion here. In particular, for some speciﬁed

1
n

n
(cid:88)

i=1

(cid:20)

w∈Rd

29

tolerance (cid:15)k > 0, iteration k of the prox-linear algorithm accepts a solution (cid:98)w that satisﬁes Fη( (cid:98)wk; wk−1) −
minw Fη(w; wk−1) ≤ (cid:15)k.

Implementation In view of the (λ + η−1)-strong convexity of Fη(· ; wk−1), it sufﬁces to ensure that
(λ + η−1)(cid:107)v(cid:107)2

2 ≤ (cid:15)k for a subgradient v ∈ ∂Fη( (cid:98)wk; wk−1).

Fixed Iteration Budget As in the convex case, we consider as a practical alternative a ﬁxed iteration
budget Tbudget and optimize Fη(· ; wk) for exactly Tbudget iterations of M. Again, we do not have a
theoretical analysis for this scheme but ﬁnd it to be effective in practice.

6.1.2 Warm Start of Subproblems

As in the convex case, we advocate the use of the prox center wk−1 to warm start the inner optimization
problem in iteration k (Line 3 of Algo. 5).

6.2 Convergence analysis of the prox-linear algorithm

We now state the assumptions and the convergence guarantee of the prox-linear algorithm.

6.2.1 Assumptions

For the prox-linear algorithm to work, the only requirement is that we minimize an upper model. The
assumption below makes this concrete.

Assumption 33. The map g(i) is continuously differentiable everywhere for each i ∈ [n]. Moreover, there
exists a constant L > 0 such that for all w, w(cid:48) ∈ Rd and i ∈ [n], it holds that

h(cid:0)g(i)(w(cid:48))(cid:1) ≤ h(cid:0)g(i)(w) + ∇g(i)(w)(w(cid:48) − w)(cid:1) +

L
2

(cid:107)w(cid:48) − w(cid:107)2
2 .

When h is G-Lipschitz and each g(i) is (cid:101)L-smooth, both with respect to (cid:107) · (cid:107)2, then Assumption 33 holds
with L = G(cid:101)L [Drusvyatskiy and Paquette, 2018]. In the case of structured prediction, Assumption 33 holds
when the augmented score ψ as a function of w is L-smooth. The next lemma makes this precise and its
proof is in Appendix D.5.

Lemma 34. Consider the structural hinge loss f (w) = maxy∈Y ψ(y; w) = h ◦ g(w) where h, g are as
deﬁned in (6). If the mapping w (cid:55)→ ψ(y; w) is L-smooth with respect to (cid:107) · (cid:107)2 for all y ∈ Y, then it holds
for all w, z ∈ Rd that

|h(g(w + z)) − h(g(w) + ∇g(w)z)| ≤

6.2.2 Convergence Guarantee

Convergence is measured via the norm of the prox-gradient (cid:37)η(·), also known as the gradient mapping,
deﬁned as

(cid:37)η(w) =

w − arg min

Fη(z; w)

.

(63)

L
2

(cid:107)z(cid:107)2
2 .

(cid:33)

(cid:32)

1
η

z∈Rd

30

The measure of stationarity (cid:107)(cid:37)η(w)(cid:107) turns out to be related to the norm of the gradient of the Moreau
envelope of F under certain conditions - see Drusvyatskiy and Paquette [2018, Section 4] for a discussion.
In particular, a point w with small (cid:107)(cid:37)η(w)(cid:107) means that w is close to w(cid:48) = arg minz∈Rd Fη(z; w), which
is nearly stationary for F .

The prox-linear outer loop shown in Algo. 5 has the following convergence guarantee [Drusvyatskiy and

Paquette, 2018, Thm. 5.2].

Theorem 35. Consider F of the form (59) that satisﬁes Assumption 33, a step length 0 < η ≤ 1/L and a
non-negative sequence ((cid:15)k)k≥1. With these inputs, Algo. 5 produces a sequence (wk)k≥0 that satisﬁes

min
k=0,··· ,K−1

(cid:107)(cid:37)η(wk)(cid:107)2

2 ≤

F (w0) − F ∗ +

(cid:32)

2
ηK

(cid:33)

(cid:15)k

,

K
(cid:88)

k=1

where F ∗ = inf w∈Rd F (w). In addition, we have that the sequence (F (wk))k≥0 is non-increasing.

Remark 36. Algo. 5 accepts an update only if it improves the function value (Line 4). A variant of Algo. 5
which always accepts the update has a guarantee identical to that of Thm. 35, but the sequence (F (wk))k≥0
would not guaranteed to be non-increasing.

6.3 Prox-Linear with Casimir-SVRG

We now analyze the total complexity of minimizing the ﬁnite sum problem (59) with Casimir-SVRG to
approximately solve the subproblems of Algo. 5.

For the algorithm to converge, the map w (cid:55)→ g(i)(wk) + ∇g(i)(wk)(w − wk) must be Lipschitz for

each i and each iterate wk. To be precise, we assume that

Aω := max
i=1,··· ,n

sup
w∈Rd

(cid:107)∇g(i)(w)(cid:107)2

2,α

(64)

is ﬁnite, where ω, the smoothing function, is 1-strongly convex with respect to (cid:107) · (cid:107)α. When g(i) is the linear
map w (cid:55)→ A(i)w, this reduces to (58).

We choose the tolerance (cid:15)k to decrease as 1/k. When using the Casimir-SVRG algorithm with constant
smoothing (Prop. 29) as the inner solver, this method effectively smooths the kth prox-linear subproblem as
1/k. We have the following rate of convergence for this method, which is proved in Appendix D.6.

Proposition 37. Consider the setting of Thm. 35. Suppose the sequence ((cid:15)k)k≥1 satisﬁes (cid:15)k = (cid:15)0/k for
some (cid:15)0 > 0 and that the subproblem of Line 3 of Algo. 5 is solved using Casimir-SVRG with the settings
of Prop. 29. Then, total number of SVRG iterations N required to produce a w such that (cid:107)(cid:37)η(w)(cid:107)2 ≤ (cid:15) is
bounded as

E[N ] ≤ (cid:101)O





n
η(cid:15)2 (F (w0) − F ∗ + (cid:15)0) +

(cid:113)

AωDωn(cid:15)−1
0
η(cid:15)3



(F (w0) − F ∗ + (cid:15)0)3/2

 .

Remark 38. When an estimate or an upper bound B on F (w0) − F ∗, one could set (cid:15)0 = O(B). This is
true, for instance, in the structured prediction task where F ∗ ≥ 0 whenever the task loss (cid:96) is non-negative
(cf. (4)).

31

7 Experiments

In this section, we study the experimental behavior of the proposed algorithms on two structured prediction
tasks, namely named entity recognition and visual object localization. Recall that given training examples
{(x(i), y(i))}n

i=1, we wish to solve the problem:

(cid:34)

min
w∈Rd

F (w) :=

(cid:107)w(cid:107)2

2 +

λ
2

1
n

n
(cid:88)

i=1

(cid:110)

max
y(cid:48)∈Y(x(i))

φ(x(i), y(cid:48); w) + (cid:96)(y(i), y(cid:48))

− φ(x(i), y(i); w)

.

(cid:111)

(cid:35)

Note that we now allow the output space Y(x) to depend on the instance x - the analysis from the previous
sections applies to this setting as well. In all the plots, the shaded region represents one standard deviation
over ten random runs.

We compare the performance of various optimization algorithms based on the number of calls to a
smooth inference oracle. Moreover, following literature for algorithms based on SVRG [Schmidt et al.,
2017, Lin et al., 2018], we exclude the cost of computing the full gradients.

The results must be interpreted keeping in mind that the running time of all inference oracles is not
the same. These choices were motivated by the following reasons, which may not be appropriate in all
contexts. The ultimate yardstick to benchmark the performance of optimization algorithms is wall clock
time. However, this depends heavily on implementation, system and ambient system conditions. With
regards to the differing running times of different oracles, we ﬁnd that a small value of K, e.g., 5 sufﬁces,
so that our highly optimized implementations of the top-K oracle incurs negligible running time penalties
over the max oracle. Moreover, the computations of the batch gradient have been neglected as they are
embarrassingly parallel.

The outline of the rest of this section is as follows. First, we describe the datasets and task description
in Sec. 7.1, followed by methods compared in Sec. 7.2 and their hyperparameter settings in Sec. 7.3. Lastly,
Sec. 7.4 presents the experimental studies.

7.1 Dataset and Task Description

For each of the tasks, we specify below the following: (a) the dataset {(x(i), y(i))}n
i=1, (b) the output
structure Y, (c) the loss function (cid:96), (d) the score function φ(x, y; w), (e) implementation of inference
oracles, and lastly, (f) the evaluation metric used to assess the quality of predictions.

7.1.1 CoNLL 2003: Named Entity Recognition

Named entities are phrases that contain the names of persons, organization, locations, etc, and the task is
to predict the label (tag) of each entity. Named entity recognition can be formulated as a sequence tagging
problem where the set Ytag of individual tags is of size 7.

Each datapoint x is a sequence of words x = (x1, · · · , xp), and the label y = (y1, · · · , yp) ∈ Y(x) is a

sequence of the same length, where each yi ∈ Ytag is a tag.

Loss Function The loss function is the Hamming Loss (cid:96)(y, y(cid:48)) = (cid:80)
i

I(yi (cid:54)= y(cid:48)

i).

Score Function We use a chain graph to represent this task. In other words, the observation-label depen-
dencies are encoded as a Markov chain of order 1 to enable efﬁcient inference using the Viterbi algorithm.
We only consider the case of linear score φ(x, y; w) = (cid:104)w, Φ(x, y)(cid:105) for this task. The feature map Φ here

32

is very similar to that given in Example 5. Following Tkachenko and Simanovsky [2012], we use local con-
text Ψi(x) around ith word xi of x. In particular, deﬁne Ψi(x) = exi−2 ⊗ · · · ⊗ exi+2, where ⊗ denotes the
Kronecker product between column vectors, and exi denotes a one hot encoding of word xi, concatenated
with the one hot encoding of its the part of speech tag and syntactic chunk tag which are provided with the
input. Now, we can deﬁne the feature map Φ as

Φ(x, y) =

Ψv(x) ⊗ eyv

⊕

eyv ⊗ eyv+1

,

(cid:35)

(cid:34) p

(cid:88)

i=0

(cid:35)

(cid:34) p

(cid:88)

v=1

where ey ∈ R|Ytag| is a one hot-encoding of y ∈ Ytag, and ⊕ denotes vector concatenation.

Inference We use the Viterbi algorithm as the max oracle (Algo. 1) and top-K Viterbi algorithm (Algo. 2)
for the top-K oracle.

Dataset The dataset used was CoNLL 2003 [Tjong Kim Sang and De Meulder, 2003], which contains
about ∼ 20K sentences.

Evaluation Metric We follow the ofﬁcial CoNLL metric: the F1 measure excluding the ‘O’ tags. In
addition, we report the objective function value measured on the training set (“train loss”).

Other Implementation Details The sparse feature vectors obtained above are hashed onto 216 − 1 dimen-
sions for efﬁciency.

7.1.2 PASCAL VOC 2007: Visual Object Localization

Given an image and an object of interest, the task is to localize the object in the given image, i.e., determine
the best bounding box around the object. A related, but harder task is object detection, which requires
identifying and localizing any number of objects of interest, if any, in the image. Here, we restrict ourselves
to pure localization with a single instance of each object. Given an image x ∈ X of size n1 × n2, the label
y ∈ Y(x) is a bounding box, where Y(x) is the set of all bounding boxes in an image of size n1 × n2. Note
that |Y(x)| = O(n2

1n2

2).

Loss Function The PASCAL IoU metric [Everingham et al., 2010] is used to measure the quality of
localization. Given bounding boxes y, y(cid:48), the IoU is deﬁned as the ratio of the intersection of the bounding
boxes to the union:

IoU(y, y(cid:48)) =

Area(y ∩ y(cid:48))
Area(y ∪ y(cid:48))

.

We then use the 1 − IoU loss deﬁned as (cid:96)(y, y(cid:48)) = 1 − IoU(y, y(cid:48)).

Score Function The formulation we use is based on the popular R-CNN approach [Girshick et al., 2014].
linear score and non-linear score φ, both of which are based on the following
We consider two cases:
deﬁnition of the feature map Φ(x, y).

• Consider a patch x|y of image x cropped to box y, and rescale it to 64 × 64. Call this Π(x|y).

33

• Consider a convolutional neural network known as AlexNet [Krizhevsky et al., 2012] pre-trained on
ImageNet [Russakovsky et al., 2015] and pass Π(x|y) through it. Take the output of conv4, the
penultimate convolutional layer as the feature map Φ(x, y). It is of size 3 × 3 × 256.

In the case of linear score functions, we take φ(x, y; w) = (cid:104)w, Φ(x, y)(cid:105). In the case of non-linear score
functions, we deﬁne the score φ as the the result of a convolution composed with a non-linearity and followed
by a linear map. Concretely, for θ ∈ RH×W ×C1 and w ∈ RC1×C2 let the map θ (cid:55)→ θ (cid:63) w ∈ RH×W ×C2
denote a two dimensional convolution with stride 1 and kernel size 1, and σ : R → R denote the exponential
linear unit, deﬁned respectively as

[θ (cid:63) w]ij = w(cid:62)[θ]ij

and σ(x) = x I(x ≥ 0) + (exp(x) − 1) I(x < 0) ,

where [θ]ij ∈ RC1 is such that its lth entry is θijl and likewise for [θ (cid:63) w]ij. We overload notation to
let σ : Rd → Rd denote the exponential linear unit applied element-wise. Notice that σ is smooth. The
non-linear score function φ is now deﬁned, with w1 ∈ R256×16, w2 ∈ R16×3×3 and w = (w1, w2), as,

φ(x, y; w) = (cid:104)σ(Φ(x, y) (cid:63) w1), w2(cid:105) .

Inference For a given input image x, we follow the R-CNN approach [Girshick et al., 2014] and use
selective search [Van de Sande et al., 2011] to prune the search space. In particular, for an image x, we use
the selective search implementation provided by OpenCV [Bradski, 2000] and take the top 1000 candidates
returned to be the set (cid:98)Y(x), which we use as a proxy for Y(x). The max oracle and the top-K oracle are
then implemented as exhaustive searches over this reduced set (cid:98)Y(x).

Dataset We use the PASCAL VOC 2007 dataset [Everingham et al., 2010], which contains ∼ 5K an-
notated consumer (real world) images shared on the photo-sharing site Flickr from 20 different object cat-
egories. For each class, we consider all images with only a single occurrence of the object, and train an
independent model for each class.

Evaluation Metric We keep track of two metrics. The ﬁrst is the localization accuracy, also known as
CorLoc (for correct localization), following Deselaers et al. [2010]. A bounding box with IoU > 0.5 with the
ground truth is considered correct and the localization accuracy is the fraction of images labeled correctly.
The second metric is average precision (AP), which requires a conﬁdence score for each prediction. We use
φ(x, y(cid:48); w) as the conﬁdence score of y(cid:48). As previously, we also plot the objective function value measured
on the training examples.

Other Implementation Details For a given input-output pair (x, y) in the dataset, we instead use (x, (cid:98)y)
as a training example, where (cid:98)y = arg maxy(cid:48)∈ (cid:98)Y(x) IoU(y, y(cid:48)) is the element of (cid:98)Y(x) which overlaps the
most with the true output y.

7.2 Methods Compared

The experiments compare various convex stochastic and incremental optimization methods for structured
prediction.

34

• SGD: Stochastic subgradient method with a learning rate γt = γ0/(1+(cid:98)t/t0(cid:99)), where η0, t0 are tuning
parameters. Note that this scheme of learning rates does not have a theoretical analysis. However,
the averaged iterate wt = 2/(t2 + t) (cid:80)t
τ =1 τ wτ obtained from the related scheme γt = 1/(λt) was
shown to have a convergence rate of O((λ(cid:15))−1) [Shalev-Shwartz et al., 2011, Lacoste-Julien et al.,
2012]. It works on the non-smooth formulation directly.

• BCFW: The block coordinate Frank-Wolfe algorithm of Lacoste-Julien et al. [2013]. We use the
version that was found to work best in practice, namely, one that uses the weighted averaged iterate
wt = 2/(t2 + t) (cid:80)t
τ =1 τ wτ (called bcfw-wavg by the authors) with optimal tuning of learning
rates. This algorithm also works on the non-smooth formulation and does not require any tuning.

• SVRG: The SVRG algorithm proposed by Johnson and Zhang [2013], with each epoch making one
pass through the dataset and using the averaged iterate to compute the full gradient and restart the next
epoch. This algorithm requires smoothing.

• Casimir-SVRG-const: Algo. 4 with SVRG as the inner optimization algorithm. The parameters µk
and κk as chosen in Prop. 29, where µ and κ are hyperparameters. This algorithm requires smoothing.

• Casimir-SVRG-adapt: Algo. 4 with SVRG as the inner optimization algorithm. The parameters µk
and κk as chosen in Prop. 30, where µ and κ are hyperparameters. This algorithm requires smoothing.

On the other hand, for non-convex structured prediction, we only have two methods:

• SGD: The stochastic subgradient method [Davis and Drusvyatskiy, 2018], which we call as SGD. This
√
algorithm works directly on the non-smooth formulation. We try learning rates γt = γ0, γt = γ0/
t
and γt = γ0/t, where γ0 is found by grid search in each of these cases. We use the names SGD-const,
SGD-t−1/2 and SGD-t−1 respectively for these variants. We note that SGD-t−1 does not have any
theoretical analysis in the non-convex case.

• PL-Casimir-SVRG: Algo. 5 with Casimir-SVRG-const as the inner solver using the settings of

Prop. 37. This algorithm requires smoothing the inner subproblem.

7.3 Hyperparameters and Variants

Smoothing In light of the discussion of Sec. 4, we use the (cid:96)2
strategy for efﬁcient computation. We then have Dω = 1/2.

2 smoother ω(u) = (cid:107)u(cid:107)2

2/2 and use the top-K

Regularization The regularization coefﬁcient λ is chosen as c/n, where c is varied in {0.01, 0.1, 1, 10}.

Choice of K The experiments use K = 5 for named entity recognition where the performance of the
top-K oracle is K times slower, and K = 10 for visual object localization, where the running time of the
top-K oracle is independent of K. We also present results for other values of K in Fig. 5d and ﬁnd that the
performance of the tested algorithms is robust to the value of K.

35

Tuning Criteria Some algorithms require tuning one or more hyperparameters such as the learning rate.
We use grid search to ﬁnd the best choice of the hyperparameters using the following criteria: For the named
entity recognition experiments, the train function value and the validation F1 metric were only weakly
correlated. For instance, the 3 best learning rates in the grid in terms of F1 score, the best F1 score attained
the worst train function value and vice versa. Therefore, we choose the value of the tuning parameter that
attained the best objective function value within 1% of the best validation F1 score in order to measure the
optimization performance while still remaining relevant to the named entity recognition task. For the visual
object localization task, a wide range of hyperparameter values achieved nearly equal performance in terms
of the best CorLoc over the given time horizon, so we choose the value of the hyperparameter that achieves
the best objective function value within a given iteration budget.

7.3.1 Hyperparameters for Convex Optimization

This corresponds to the setting of Section 5.

Learning Rate The algorithms SVRG and Casimir-SVRG-adapt require tuning of a learning rate, while
SGD requires η0, t0 and Casimir-SVRG-const requires tuning of the Lipschitz constant L of ∇Fµω, which
determines the learning rate γ = 1/(L + λ + κ). Therefore, tuning the Lipschitz parameter is similar
to tuning the learning rate. For both the learning rate and Lipschitz parameter, we use grid search on a
logarithmic grid, with consecutive entries chosen a factor of two apart.

Choice of κ For Casimir-SVRG-const, with the Lipschitz constant in hand, the parameter κ is chosen to
minimize the overall complexity as in Prop. 29. For Casimir-SVRG-adapt, we use κ = λ.

Stopping Criteria Following the discussion of Sec. 5, we use an iteration budget of Tbudget = n.

Warm Start The warm start criterion determines the starting iterate of an epoch of the inner optimization
algorithm. Recall that we solve the following subproblem using SVRG for the kth iterate (cf. (25)):

wk ≈ arg min

Fµkω,κk (wk; zk−1) .

w∈Rd

Here, we consider the following warm start strategy to choose the initial iterate (cid:98)w0 for this subproblem:

• Prox-center: (cid:98)w0 = zk−1.

In addition, we also try out the following warm start strategies of Lin et al. [2018]:

• Extrapolation: (cid:98)w0 = wk−1 + c(zk−1 − zk−2) where c = κ
• Prev-iterate: (cid:98)w0 = wk−1.

κ+λ .

We use the Prox-center strategy unless mentioned otherwise.

Level of Smoothing and Decay Strategy For SVRG and Casimir-SVRG-const with constant smoothing,
we try various values of the smoothing parameter in a logarithmic grid. On the other hand, Casimir-SVRG-
adapt is more robust to the choice of the smoothing parameter (Fig. 5a). We use the defaults of µ = 2 for
named entity recognition and µ = 10 for visual object localization.

36

Figure 2: Comparison of convex optimization algorithms for the task of Named Entity Recognition on
CoNLL 2003.

7.3.2 Hyperparameters for Non-Convex Optimization

This corresponds to the setting of Section 6.

Prox-Linear Learning Rate η We perform grid search in powers of 10 to ﬁnd the best prox-linear learning
rate η. We ﬁnd that the performance of the algorithm is robust to the choice of η (Fig. 7a).

Stopping Criteria We used a ﬁxed budget of 5 iterations of Casimir-SVRG-const. In Fig. 7b, we experi-
ment with different iteration budgets.

In order to solve the kth prox-linear subproblem with Casimir-
Level of Smoothing and Decay Strategy
SVRG-const, we must specify the level of smoothing µk. We experiment with two schemes, (a) constant
smoothing µk = µ, and (b) adaptive smoothing µk = µ/k. Here, µ is a tuning parameters, and the adaptive
smoothing scheme is designed based on Prop. 37 and Remark 38. We use the adaptive smoothing strategy
as a default, but compare the two in Fig. 6.

Gradient Lipschitz Parameter for Inner Optimization The inner optimization algorithm Casimir-SVRG-
const still requires a hyperparameter Lk to serve as an estimate to the Lipschitz parameter of the gradient
∇Fη,µkω(· ; wk). We set this parameter as follows, based on the smoothing strategy: (a) Lk = L0 with the
constant smoothing strategy, and (b) Lk = k L0 with the adaptive smoothing strategy (cf. Prop. 2). We note
that the latter choice has the effect of decaying the learning rate as 1/k in the kth outer iteration.

37

7.4 Experimental study of different methods

Convex Optimization For the named entity recognition task, Fig. 2 plots the performance of various
methods on CoNLL 2003. On the other hand, Fig. 3 presents plots for various classes of PASCAL VOC
2007 for visual object localization.

The plots reveal that smoothing-based methods converge faster in terms of training error while achieving
a competitive performance in terms of the performance metric on a held-out set. Furthermore, BCFW and
SGD make twice as many actual passes as SVRG based algorithms.

Non-Convex Optimization Fig. 4 plots the performance of various algorithms on the task of visual object
localization on PASCAL VOC.

7.5 Experimental Study of Effect of Hyperparameters: Convex Optimization

We now study the effects of various hyperparameter choices.

Effect of Smoothing Fig. 5a plots the effect of the level of smoothing for Casimir-SVRG-const and
Casimir-SVRG-adapt. The plots reveal that, in general, small values of the smoothing parameter lead to
better optimization performance for Casimir-SVRG-const. Casimir-SVRG-adapt is robust to the choice of
µ. Fig. 5b shows how the smooth optimization algorithms work when used heuristically on the non-smooth
problem.

Effect of Warm Start Strategies Fig. 5c plots different warm start strategies for Casimir-SVRG-const and
Casimir-SVRG-adapt. We ﬁnd that Casimir-SVRG-adapt is robust to the choice of the warm start strategy
while Casimir-SVRG-const is not. For the latter, we observe that Extrapolation is less stable (i.e., tends
to diverge more) than Prox-center, which is in turn less stable than Prev-iterate, which always
works (cf. Fig. 5c). However, when they do work, Extrapolation and Prox-center provide greater
acceleration than Prev-iterate. We use Prox-center as the default choice to trade-off between
acceleration and applicability.

Effect of K Fig. 5d illustrates the robustness of the method to choice of K: we observe that the results are
all within one standard deviation of each other.

7.6 Experimental Study of Effect of Hyperparameters: Non-Convex Optimization

We now study the effect of various hyperparameters for the non-convex optimization algorithms. All of
these comparisons have been made for λ = 1/n.

Effect of Smoothing Fig. 6a compares the adaptive and constant smoothing strategies. Fig. 6b and Fig. 6c
compare the effect of the level of smoothing on the the both of these. As previously, the adaptive smoothing
strategy is more robust to the choice of the smoothing parameter.

Effect of Prox-Linear Learning Rate η Fig. 7a shows the robustness of the proposed method to the
choice of η.

38

Effect of Iteration Budget Fig. 7b also shows the robustness of the proposed method to the choice of
iteration budget of the inner solver, Casimir-SVRG-const.

Effect of Warm Start of the Inner Solver Fig. 7c studies the effect of the warm start strategy used within
the inner solver Casimir-SVRG-const in each inner prox-linear iteration. The results are similar to those
obtained in the convex case, with Prox-center choice being the best compromise between acceleration
and compatibility.

8 Future Directions

We introduced a general notion of smooth inference oracles in the context of black-box ﬁrst-order opti-
mization. This allows us to set the scene to extend the scope of fast incremental optimization algorithms to
structured prediction problems owing to a careful blend of a smoothing strategy and an acceleration scheme.
We illustrated the potential of our framework by proposing a new incremental optimization algorithm to
train structural support vector machines both enjoying worst-case complexity bounds and demonstrating
competitive performance on two real-world problems. This work paves also the way to faster incremental
primal optimization algorithms for deep structured prediction models.

There are several potential venues for future work. When there is no discrete structure that admits
efﬁcient inference algorithms, it could be beneﬁcial to not treat inference as a black-box numerical proce-
dure [Meshi et al., 2010, Hazan and Urtasun, 2010, Hazan et al., 2016]. Instance-level improved algorithms
along the lines of Hazan et al. [2016] could also be interesting to explore.

Acknowledgments This work was supported by NSF Award CCF-1740551, the Washington Research
Foundation for innovation in Data-intensive Discovery, and the program “Learning in Machines and Brains”
of CIFAR.

39

Figure 3: Comparison of convex optimization algorithms for the task of visual object localization on PAS-
CAL VOC 2007 for λ = 10/n. Plots for all other classes are in Appendix E.

40

Figure 4: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n. Plots for all other classes are in Appendix E.

41

(a) Effect of level of smoothing.

(b) Effect of smoothing: use of smooth optimization with smoothing (labeled “smooth”) versus the
heuristic use of these algorithms without smoothing (labeled “non-smooth”) for λ = 0.01/n.

(c) Effect of warm start strategies for λ = 0.01/n (ﬁrst row) and λ = 1/n (second row).

(d) Effect of K in the top-K oracle (λ = 0.01/n).

Figure 5: Effect of hyperparameters for the task of Named Entity Recognition on CoNLL 2003. C-SVRG
stands for Casimir-SVRG in these plots.

42

(a) Comparison of adaptive and constant smoothing strategies.

(b) Effect of µ of the adaptive smoothing strategy.

(c) Effect of µ of the constant smoothing strategy.

Figure 6: Effect of smoothing on PL-Casimir-SVRG for the task of visual object localization on PASCAL
VOC 2007.

43

(a) Effect of the hyperparameter η.

(b) Effect of the iteration budget of the inner solver.

44

(c) Effect of the warm start strategy of the inner Casimir-SVRG-const algorithm.

Figure 7: Effect of hyperparameters on PL-Casimir-SVRG for the task of visual object localization on
PASCAL VOC 2007.

References

Z. Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. Journal of Machine

Learning Research, 18:221:1–221:51, 2017.

Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden Markov Support Vector Machines. In International

Conference on Machine Learning, pages 3–10, 2003.

D. Batra. An efﬁcient message-passing algorithm for the M -best MAP problem. In Conference on Uncer-

tainty in Artiﬁcial Intelligence, pages 121–130, 2012.

D. Batra, P. Yadollahpour, A. Guzm´an-Rivera, and G. Shakhnarovich. Diverse M -best Solutions in Markov

Random Fields. In European Conference on Computer Vision, pages 1–16, 2012.

A. Beck and M. Teboulle. Smoothing and ﬁrst order methods: A uniﬁed framework. SIAM Journal on

Optimization, 22(2):557–580, 2012.

D. Belanger and A. McCallum. Structured prediction energy networks.

In International Conference on

Machine Learning, pages 983–992, 2016.

R. Bellman. Dynamic Programming. Courier Dover Publications, 1957.

Y. Bengio, Y. LeCun, C. Nohl, and C. Burges. LeRec: A NN/HMM Hybrid for On-Line Handwriting

Recognition. Neural Computation, 7(6):1289–1303, 1995.

D. P. Bertsekas. Dynamic programming and optimal control, volume 1. Athena scientiﬁc Belmont, MA,

1995.

D. P. Bertsekas. Nonlinear programming. Athena Scientiﬁc Belmont, 1999.

L. Bottou and P. Gallinari. A Framework for the Cooperation of Learning Algorithms.

In Advances in

Neural Information Processing Systems, pages 781–788, 1990.

L. Bottou, Y. Bengio, and Y. LeCun. Global Training of Document Processing Systems Using Graph
In Conference on Computer Vision and Pattern Recognition, pages 489–494,

Transformer Networks.
1997.

G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000.

J. V. Burke. Descent methods for composite nondifferentiable optimization problems. Mathematical Pro-

gramming, 33(3):260–279, 1985.

C. Chen, V. Kolmogorov, Y. Zhu, D. N. Metaxas, and C. H. Lampert. Computing the M Most Probable
Modes of a Graphical Model. In International Conference on Artiﬁcial Intelligence and Statistics, pages
161–169, 2013.

Y.-Q. Cheng, V. Wu, R. Collins, A. R. Hanson, and E. M. Riseman. Maximum-weight bipartite matching
technique and its application in image feature matching. In Visual Communications and Image Process-
ing, volume 2727, pages 453–463, 1996.

45

M. Collins, A. Globerson, T. Koo, X. Carreras, and P. L. Bartlett. Exponentiated gradient algorithms for
conditional random ﬁelds and max-margin markov networks. Journal of Machine Learning Research, 9
(Aug):1775–1822, 2008.

R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. P. Kuksa. Natural language process-

ing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537, 2011.

G. F. Cooper. The computational complexity of probabilistic inference using bayesian belief networks.

Artiﬁcial Intelligence, 42(2-3):393–405, 1990.

B. Cox, A. Juditsky, and A. Nemirovski. Dual subgradient algorithms for large-scale nonsmooth learning

problems. Mathematical Programming, 148(1-2):143–180, 2014.

K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines.

Journal of Machine Learning Research, 2(Dec):265–292, 2001.

H. Daum´e III and D. Marcu. Learning as search optimization: approximate large margin methods for

structured prediction. In International Conference on Machine Learning, pages 169–176, 2005.

D. Davis and D. Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. arXiv

preprint arXiv:1803.06523, 2018.

and Computing, 2(1):25–36, 1992.

A. P. Dawid. Applications of a general propagation algorithm for probabilistic expert systems. Statistics

A. Defazio. A simple practical accelerated method for ﬁnite sums.

In Advances in Neural Information

Processing Systems, pages 676–684, 2016.

A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for
non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages
1646–1654, 2014.

T. Deselaers, B. Alexe, and V. Ferrari. Localizing objects while learning their appearance. In European

Conference on Computer Vision, pages 452–466, 2010.

D. Drusvyatskiy and C. Paquette. Efﬁciency of minimizing compositions of convex functions and smooth

maps. Mathematical Programming, Jul 2018.

J. C. Duchi, D. Tarlow, G. Elidan, and D. Koller. Using Combinatorial Optimization within Max-Product

Belief Propagation. In Advances in Neural Information Processing Systems, pages 369–376, 2006.

M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The Pascal Visual Object Classes

(VOC) challenge. International Journal of Computer Vision, 88(2):303–338, 2010.

N. Flerova, R. Marinescu, and R. Dechter. Searching for the M Best Solutions in Graphical Models. Journal

of Artiﬁcial Intelligence Research, 55:889–952, 2016.

M. Fromer and A. Globerson. An LP view of the M -best MAP problem. In Advances in Neural Information

Processing Systems, pages 567–575, 2009.

46

R. Frostig, R. Ge, S. Kakade, and A. Sidford. Un-regularizing: approximate proximal point and faster
stochastic algorithms for empirical risk minimization. In International Conference on Machine Learning,
pages 2540–2548, 2015.

R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Conference on Computer Vision and Pattern Recognition, pages 580–587,
2014.

D. M. Greig, B. T. Porteous, and A. H. Seheult. Exact maximum a posteriori estimation for binary images.

Journal of the Royal Statistical Society. Series B (Methodological), pages 271–279, 1989.

T. Hazan and R. Urtasun. A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Struc-

tured Prediction. In Advances in Neural Information Processing Systems, pages 838–846, 2010.

T. Hazan, A. G. Schwing, and R. Urtasun. Blending Learning and Inference in Conditional Random Fields.

Journal of Machine Learning Research, 17:237:1–237:25, 2016.

L. He, K. Lee, M. Lewis, and L. Zettlemoyer. Deep Semantic Role Labeling: What Works and What’s Next.

In Annual Meeting of the Association for Computational Linguistics, pages 473–483, 2017.

N. He and Z. Harchaoui. Semi-Proximal Mirror-Prox for Nonsmooth Composite Minimization. In Advances

in Neural Information Processing Systems, pages 3411–3419, 2015.

M. Held, P. Wolfe, and H. P. Crowder. Validation of subgradient optimization. Mathematical Programming,

6(1):62–88, Dec 1974.

T. Hofmann, A. Lucchi, S. Lacoste-Julien, and B. McWilliams. Variance reduced stochastic gradient descent

with neighbors. In Advances in Neural Information Processing Systems, pages 2305–2313, 2015.

H. Ishikawa and D. Geiger. Segmentation by grouping junctions. In Conference on Computer Vision and

Pattern Recognition, pages 125–131, 1998.

M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for the Ising model. SIAM Journal

on computing, 22(5):1087–1116, 1993.

T. Joachims, T. Finley, and C.-N. J. Yu. Cutting-plane training of structural SVMs. Machine Learning, 77

(1):27–59, 2009.

J. K. Johnson. Convex relaxation methods for graphical models: Lagrangian and maximum entropy ap-

proaches. PhD thesis, Massachusetts Institute of Technology, Cambridge, MA, USA, 2008.

R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In

Advances in Neural Information Processing Systems, pages 315–323, 2013.

V. Jojic, S. Gould, and D. Koller. Accelerated dual decomposition for MAP inference. In International

Conference on Machine Learning, pages 503–510, 2010.

D. Jurafsky, J. H. Martin, P. Norvig, and S. Russell. Speech and Language Processing. Pearson Education,

2014. ISBN 9780133252934.

ISBN 978-0-262-01319-2.

D. Koller and N. Friedman. Probabilistic Graphical Models - Principles and Techniques. MIT Press, 2009.

47

V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? IEEE Transactions

on Pattern Analysis and Machine Intelligence, 26(2):147–159, 2004.

A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet classiﬁcation with deep convolutional neural

networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012.

S. Lacoste-Julien, M. Schmidt, and F. Bach. A simpler approach to obtaining an O(1/t) convergence rate

for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002, 2012.

S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Block-Coordinate Frank-Wolfe Optimization for

Structural SVMs. In International Conference on Machine Learning, pages 53–61, 2013.

J. Lafferty, A. McCallum, and F. C. Pereira. Conditional Random Fields: Probabilistic Models for Segment-
In International Conference on Machine Learning, pages 282–289,

ing and Labeling Sequence Data.
2001.

C. H. Lampert, M. B. Blaschko, and T. Hofmann. Beyond sliding windows: Object localization by efﬁcient

subwindow search. In Conference on Computer Vision and Pattern Recognition, pages 1–8, 2008.

N. Le Roux, M. W. Schmidt, and F. R. Bach. A Stochastic Gradient Method with an Exponential Conver-
gence Rate for Strongly-Convex Optimization with Finite Training Sets. In Advances in Neural Informa-
tion Processing Systems, pages 2672–2680, 2012.

M. Lewis and M. Steedman. A* CCG parsing with a supertag-factored model. In Conference on Empirical

Methods in Natural Language Processing, pages 990–1000, 2014.

H. Lin, J. Mairal, and Z. Harchaoui. A universal catalyst for ﬁrst-order optimization. In Advances in Neural

Information Processing Systems, pages 3384–3392, 2015.

H. Lin, J. Mairal, and Z. Harchaoui. Catalyst Acceleration for First-order Convex Optimization: from

Theory to Practice. Journal of Machine Learning Research, 18(212):1–54, 2018.

L. Lov´asz. Submodular functions and convexity. In Mathematical Programming The State of the Art, pages

235–257. Springer, 1983.

J. Mairal.

Incremental majorization-minimization optimization with application to large-scale machine

learning. SIAM Journal on Optimization, 25(2):829–855, 2015.

A. F. T. Martins and R. F. Astudillo. From Softmax to Sparsemax: A Sparse Model of Attention and
Multi-Label Classiﬁcation. In International Conference on Machine Learning, pages 1614–1623, 2016.

R. J. McEliece, D. J. C. MacKay, and J. Cheng. Turbo Decoding as an Instance of Pearl’s ”Belief Propaga-

tion” Algorithm. IEEE Journal on Selected Areas in Communications, 16(2):140–152, 1998.

A. Mensch and M. Blondel. Differentiable dynamic programming for structured prediction and attention.

In International Conference on Machine Learning, pages 3459–3468, 2018.

O. Meshi, D. Sontag, T. S. Jaakkola, and A. Globerson. Learning Efﬁciently with Approximate Inference

via Dual Losses. In International Conference on Machine Learning, pages 783–790, 2010.

O. Meshi, T. S. Jaakkola, and A. Globerson. Convergence Rate Analysis of MAP Coordinate Minimization

Algorithms. In Advances in Neural Information Processing Systems, pages 3023–3031, 2012.

48

K. P. Murphy, Y. Weiss, and M. I. Jordan. Loopy belief propagation for approximate inference: An empirical

study. In Conference on Uncertainty in Artiﬁcial Intelligence, pages 467–475, 1999.

Y. Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2). In Soviet

Mathematics Doklady, volume 27, pages 372–376, 1983.

Y. Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on Optimization,

Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103(1):127–152,

Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science &

16(1):235–249, 2005a.

2005b.

Business Media, 2013.

V. Niculae, A. F. Martins, M. Blondel, and C. Cardie. SparseMAP: Differentiable Sparse Structured Infer-

ence. In International Conference on Machine Learning, pages 3796–3805, 2018.

D. Nilsson. An efﬁcient algorithm for ﬁnding the M most probable conﬁgurations in probabilistic expert

systems. Statistics and Computing, 8(2):159–173, 1998.

A. Osokin, J.-B. Alayrac, I. Lukasewitz, P. Dokania, and S. Lacoste-Julien. Minding the gaps for block
Frank-Wolfe optimization of structured SVMs. In International Conference on Machine Learning, pages
593–602, 2016.

B. Palaniappan and F. Bach. Stochastic variance reduction methods for saddle-point problems. In Advances

in Neural Information Processing Systems, pages 1408–1416, 2016.

C. Paquette, H. Lin, D. Drusvyatskiy, J. Mairal, and Z. Harchaoui. Catalyst for gradient-based nonconvex
optimization. In International Conference on Artiﬁcial Intelligence and Statistics, pages 613–622, 2018.

J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann,

N. D. Ratliff, J. A. Bagnell, and M. Zinkevich. (Approximate) Subgradient Methods for Structured Predic-

tion. In International Conference on Artiﬁcial Intelligence and Statistics, pages 380–387, 2007.

R. T. Rockafellar and R. J.-B. Wets. Variational analysis, volume 317. Springer Science & Business Media,

1988.

2009.

O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bern-
stein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International
Journal of Computer Vision, 115(3):211–252, 2015.

B. Savchynskyy, J. H. Kappes, S. Schmidt, and C. Schn¨orr. A study of Nesterov’s scheme for Lagrangian
decomposition and MAP labeling. In Conference on Computer Vision and Pattern Recognition, pages
1817–1823, 2011.

M. I. Schlesinger. Syntactic analysis of two-dimensional visual signals in noisy conditions. Kibernetika, 4

(113-130):1, 1976.

49

M. Schmidt, R. Babanezhad, M. Ahmed, A. Defazio, A. Clifton, and A. Sarkar. Non-uniform stochastic
average gradient method for training conditional random ﬁelds. In International Conference on Artiﬁcial
Intelligence and Statistics, pages 819–828, 2015.

M. Schmidt, N. Le Roux, and F. Bach. Minimizing ﬁnite sums with the stochastic average gradient. Math-

ematical Programming, 162(1-2):83–112, 2017.

A. Schrijver. Combinatorial Optimization - Polyhedra and Efﬁciency. Springer, 2003.

B. Seroussi and J. Golmard. An algorithm directly ﬁnding the K most probable conﬁgurations in Bayesian

networks. International Journal of Approximate Reasoning, 11(3):205 – 233, 1994.

S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss minimiza-

tion. Journal of Machine Learning Research, 14(Feb):567–599, 2013.

S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized

loss minimization. In International Conference on Machine Learning, pages 64–72, 2014.

S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-gradient solver for

SVM. Mathematical programming, 127(1):3–30, 2011.

H. O. Song, R. B. Girshick, S. Jegelka, J. Mairal, Z. Harchaoui, and T. Darrell. On learning to localize
objects with minimal supervision. In International Conference on Machine Learning, pages 1611–1619,
2014.

B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In Advances in Neural Information

Processing Systems, pages 25–32, 2004.

B. Taskar, S. Lacoste-Julien, and D. Klein. A discriminative matching approach to word alignment.

In
Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing, pages 73–80, 2005.

B. Taskar, S. Lacoste-Julien, and M. I. Jordan. Structured prediction, dual extragradient and Bregman

projections. Journal of Machine Learning Research, 7(Jul):1627–1653, 2006.

C. H. Teo, S. Vishwanathan, A. Smola, and Q. V. Le. Bundle methods for regularized risk minimization.

Journal of Machine Learning Research, 1(55), 2009.

E. F. Tjong Kim Sang and F. De Meulder.

Introduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Conference on Natural Language Learning, pages 142–147,
2003.

M. Tkachenko and A. Simanovsky. Named entity recognition: Exploring features. In Empirical Methods in

Natural Language Processing, pages 118–127, 2012.

I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdepen-
dent and structured output spaces. In International Conference on Machine Learning, page 104, 2004.

K. E. Van de Sande, J. R. Uijlings, T. Gevers, and A. W. Smeulders. Segmentation as selective search for

object recognition. In International Conference on Computer Vision, pages 1879–1886, 2011.

50

A. J. Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.

IEEE Trans. Information Theory, 13(2):260–269, 1967. doi: 10.1109/TIT.1967.1054010.

M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference.

Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008.

M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. MAP estimation via agreement on trees: message-
passing and linear programming. IEEE transactions on information theory, 51(11):3697–3717, 2005.

B. E. Woodworth and N. Srebro. Tight complexity bounds for optimizing composite objectives. In Advances

in Neural Information Processing Systems, pages 3639–3647, 2016.

C. Yanover and Y. Weiss. Finding the M most probable conﬁgurations using loopy belief propagation. In

Advances in Neural Information Processing Systems, pages 289–296, 2004.

X. Zhang, A. Saha, and S. Vishwanathan. Accelerated training of max-margin markov networks with ker-

nels. Theoretical Computer Science, 519:88–102, 2014.

51

A Smoothing

We ﬁrst prove an extension of Lemma 4.2 of Beck and Teboulle [2012], which proves the following state-
ment for the special case of µ2 = 0. Recall that we deﬁned hµω ≡ h when µ = 0.

Proposition 39. Consider the setting of Def. 1. For µ1 ≥ µ2 ≥ 0, we have for every z ∈ Rm that

(µ1 − µ2)

inf
u∈dom h∗

ω(u) ≤ hµ2ω(z) − hµ1ω(z) ≤ (µ1 − µ2)

sup
u∈dom h∗

ω(u) .

Proof. We successively deduce,

hµ1ω(z) = sup

{(cid:104)u, z(cid:105) − h∗(u) − µ1ω(u)}

u∈dom h∗

= sup

u∈dom h∗

≥ sup

u∈dom h∗

(cid:26)

{(cid:104)u, z(cid:105) − h∗(u) − µ2ω(u) − (µ1 − µ2)ω(u)}

(cid:104)u, z(cid:105) − h∗(u) − µ2ω(u) + inf

u(cid:48)∈dom h∗

(cid:27)
(cid:8)−(µ1 − µ2)ω(u(cid:48))}(cid:9)

= hµ2ω(z) − (µ1 − µ2)

sup
u(cid:48)∈dom h∗

ω(u(cid:48)) ,

since µ1 − µ2 ≥ 0. The other side follows using instead that

−(µ1 − µ2)ω(u) ≤ sup

(cid:8)−(µ1 − µ2)ω(u(cid:48))}(cid:9) .

u(cid:48)∈dom h∗

Next, we recall the following equivalent deﬁnition of a matrix norm deﬁned in Eq. (2).

(cid:107)A(cid:107)β,α = sup
y(cid:54)=0

(cid:107)A(cid:62)y(cid:107)∗
β
(cid:107)y(cid:107)α

= sup
x(cid:54)=0

(cid:107)Ax(cid:107)∗
α
(cid:107)y(cid:107)β

= (cid:107)A(cid:62)(cid:107)α,β .

(65)

Now, we consider the smoothness of a composition of a smooth function with an afﬁne map.

Lemma 40. Suppose h : Rm → R is L-smooth with respect to (cid:107) · (cid:107)∗
b ∈ Rm, we have that the map Rd (cid:51) w (cid:55)→ h(Aw + b) is (cid:0)L(cid:107)A(cid:62)(cid:107)2

α. Then, for any A ∈ Rm×d and
(cid:1)-smooth with respect to (cid:107) · (cid:107)β.

α,β

Proof. Fix A ∈ Rm×d, b ∈ Rm and deﬁne f : Rd → R as f (w) = h(Aw + b). By the chain rule, we have
that ∇f (w) = A(cid:62)∇h(Aw + b). Using smoothness of h, we successively deduce,

(cid:107)∇f (w1) − ∇f (w2)(cid:107)∗

β = (cid:107)A(cid:62)(∇h(Aw1 + b) − ∇h(Aw2 + b))(cid:107)∗
β

(65)
≤ (cid:107)A(cid:62)(cid:107)α,β(cid:107)∇h(Aw1 + b) − ∇h(Aw2 + b)(cid:107)α
≤ (cid:107)A(cid:62)(cid:107)α,β L(cid:107)A(w1 − w2)(cid:107)∗
α
(65)
≤ L(cid:107)A(cid:62)(cid:107)2

α,β(cid:107)w1 − w2(cid:107)β .

Shown in Algo. 6 is the procedure to compute the outputs of the top-K oracle from the K best scoring

outputs obtained, for instance, from the top-K max-product algorithm.

52

Algorithm 6 Top-K oracle from top-K outputs
1: Input: Augmented score function ψ, w ∈ Rd, µ > 0 YK = {y1, · · · , yK} such that yk =

max(k)

y∈Y ψ(y; w).

2: Populate z ∈ RK so that zk = 1
µ ψ(yk; w).
3: Compute u∗ = arg minu∈∆K−1 (cid:107)u − z(cid:107)2
4: return s = (cid:80)K
k=1 u∗

k ψ(yk; w) and v = (cid:80)K

2 by a projection on the simplex.
k ∇wψ(yk; w).

k=1 u∗

Algorithm 7 Standard max-product algorithm

1: Input: Augmented score function ψ(·, ·; w) deﬁned on tree structured graph G with root r ∈ V.
2: Initialize: Let V be a list of nodes from V\{r} arranged in increasing order of height.
3: for v in V do
4:

Set mv(yρ(v)) ← maxyv∈Yv
Yρ(v).
Assign to δv(yρ(v)) a maximizing assignment of yv from above for each yρ(v) ∈ Yρ(v).

(cid:110)
ψv(yv) + ψv,ρ(v)(yv, yρ(v)) + (cid:80)

v(cid:48)∈C(v) mv(cid:48)(yv)

(cid:111)

for each yρ(v) ∈

(cid:110)
ψr(yr) + (cid:80)

v(cid:48)∈C(r) mv(cid:48)(yr)

(cid:111)
.

(cid:110)
ψr(yr) + (cid:80)

v(cid:48)∈C(r) mv(cid:48)(yr)

(cid:111)
.

5:
6: end for
7: ψ∗ ← maxyr∈Yr
8: y∗
r ← arg maxyr∈Yr
9: for v in reverse(V ) do
v = δv(y∗
y∗
10:
11: end for
12: return ψ∗, y∗ = (y∗

ρ(v)).

1, · · · , y∗

p).

B Smooth Inference in Trees

A graph G is a tree if it is connected, directed and each node has at most one incoming edge. It has one root
r ∈ V with no incoming edge. An undirected graph with no loops can be converted to a tree by ﬁxing an
arbitrary root and directing all edges way from the root. We say that G is a chain if it is a tree with root p
where all edges are of the form (v + 1, v). For a node v in a tree G, we denote by ρ(v) and C(v) respectively
the parent of v and the children of v in the tree.

Recall ﬁrst that the height of a node in a rooted tree is the number of edges on the longest directed
path from the node to a leaf where each edge is directed away from the root. We ﬁrst review the standard
max-product algorithm for maximum a posteriori (MAP) inference [Dawid, 1992] - Algo. 7. It runs in time
O(p maxv∈V |Yv|2) and requires space O(p maxv∈V |Yv|).

B.1 Proof of Correctness of Top-K Max-Product

We now consider the top-K max-product algorithm, shown in full generality in Algo. 8. The following
proposition proves its correctness.

Proposition 41. Consider as inputs to Algo. 8 an augmented score function ψ(·, ·; w) deﬁned on tree
structured graph G, and an integer K > 0. Then, the outputs of Algo. 8 satisfy ψ(k) = ψ(y(k)) =
max(k)
Proof. For a node v ∈ V, let τ (v) denote the sub-tree of G rooted at v. Let yτ (v) denote (cid:0)yv(cid:48) for v(cid:48) ∈ τ (v)(cid:1).

y∈Y ψ(y). Moreover, Algo. 8 runs in time O(pK log K maxv∈V |Yv|2) and uses space O(pK maxv∈V |Yv|).

53

Algorithm 8 Top-K max-product algorithm

1: Input: Augmented score function ψ(·, ·; w) deﬁned on tree structured graph G with root r ∈ V, and

integer K > 0.

2: Initialize: Let V be a list of nodes from V\{r} arranged in increasing order of height.
3: for v in V and k = 1, · · · , K do
4:

if v is a leaf then

m(k)

v (yρ(v)) ← max(k)

yv∈Yv

(cid:8)ψv(yv) + ψv,ρ(v)(yv, yρ(v))(cid:9) for each yρ(v) ∈ Yρ(v).

else

Assign for each yρ(v) ∈ Yρ(v),

m(k)

v (yρ(v)) ← max(k)

(cid:40)

ψv(yv) + ψv,ρ(v)(yv, yρ(v))
v(cid:48)∈C(v) m(lv(cid:48) )

+ (cid:80)

(yv)

v(cid:48)

yv ∈ Yv and
lv(cid:48) ∈ [K] for v(cid:48) ∈ C(v)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)

.

(66)

v(cid:48) (yρ(v)) for v(cid:48) ∈ C(v) store the maximizing assignment of yv and l(cid:48)

v from

5:

6:

7:

8:

Let δ(k)
v (yρ(v)) and κ(k)
above for each yρ(v) ∈ Yρ(v).

end if
9:
10: end for
11: For k = 1, · · · , K, set

ψ(k) ← max(k)

ψr(yr) +

(cid:26)

(cid:88)

m(lv(cid:48) )
v(cid:48)

(yr)

v(cid:48)∈C(r)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

yr ∈ Yr and lv(cid:48) ∈ [K] for v(cid:48) ∈ C(r)

(cid:27)

and assign maximizing assignments of yr, lv(cid:48) above respectively to y(k)

r

and l(k)
v(cid:48)

for v(cid:48) ∈ C(r).

12: for v in reverse(V ) and k = 1, · · · , K do
v )

(cid:0)y(k)
ρ(v)
(cid:0)y(k)
ρ(v)

(cid:1).
(cid:1) for all v(cid:48) ∈ C(v).

13:

Set y(k)
Set l(k)

v

v ← δ(l(k)
v(cid:48) = κ(l(k)
(cid:110)

v )

v(cid:48)

14:
15: end for
16: return

ψ(k), y(k) := (y(k)

1 , · · · , y(k)
p )

(cid:111)K

.

k=1

54

(67)

(68)

(69)

Deﬁne ψτ (v) as follows: if v is a leaf, yτ (v) = (yv) and ψτ (v)(yτ (v)) := ψv(yv). For a non-leaf v, deﬁne
recursively

ψτ (v)(yτ (v)) := ψv(yv) +

(cid:2)ψv,v(cid:48)(yv, yv(cid:48)) + ψτ (v(cid:48))(yτ (v(cid:48)))(cid:3) .

(cid:88)

v(cid:48)∈C(v)

We will need some identities about choosing the kth largest element from a ﬁnite collection. For ﬁnite sets
S1, · · · , Sn and functions fj : Sj → R, h : S1 × S2 → R, we have,












max(k)
u1∈S1,··· ,un∈Sn

n
(cid:88)



j=1

fj(uj)

= max(k)
l1,··· ,ln∈[k]



max(lj )
uj ∈Sj

fj(uj)

,



n
(cid:88)



j=1

(cid:26)

max(k)
u1∈S1,u2∈S2

{f1(u1) + h(u1, u2)} = max(k)
u1∈S1,l∈[k]

f1(u1) + max(l)

h(u1, u2)

.

u2∈S2

(cid:27)

The identities above state that for a sum to take its kth largest value, each component of the sum must take
one of its k largest values. Indeed, if one of the components of the sum took its lth largest value for l > k,
replacing it with any of the k largest values cannot decrease the value of the sum. Eq. (69) is a generalized
version of Bellman’s principle of optimality (see Bellman [1957, Chap. III.3.] or Bertsekas [1995, Vol. I,
Chap. 1]).

For the rest of the proof, yτ (v)\yv is used as shorthand for {yv(cid:48) | v(cid:48) ∈ τ (v)\{v}}. Moreover, maxyτ (v)
represents maximization over yτ (v) ∈ ×v(cid:48)∈τ (v) Yv(cid:48). Likewise for maxyτ (v)\yv . Now, we shall show by
induction that for all v ∈ V, yv ∈ Yv and k = 1, · · · , K,

max(k)
yτ (v)\yv

ψτ (v)(yτ (v)) = ψv(yv) + max(k)

(cid:26) (cid:88)

v(cid:48)∈C(v)

m(lv(cid:48) )
v(cid:48)

(cid:12)
(cid:12)
lv(cid:48) ∈ [K] for v(cid:48) ∈ C(v)
(yv(cid:48))
(cid:12)
(cid:12)

(cid:27)

.

(70)

The induction is based on the height of a node. The statement is clearly true for a leaf v since C(v) = ∅.
Suppose (70) holds for all nodes of height ≤ h. For a node v of height h + 1, we observe that τ (v)\v can
be partitioned into {τ (v(cid:48)) for v(cid:48) ∈ C(v)} to get,

max(k)
yτ (v)\yv

ψτ (v)(yτ (v)) − ψv(yv)

(67)
= max(k)
yτ (v)\yv

(cid:26) (cid:88)

v(cid:48)∈C(v)

ψv,v(cid:48)(yv, yv(cid:48)) + ψτ (v(cid:48))(yτ (v(cid:48)))

(cid:27)

(cid:27)

{ψv,v(cid:48)(yv, yv(cid:48)) + ψτ (v(cid:48))(yτ (v(cid:48)))}

lv(cid:48) ∈ [K] for v(cid:48) ∈ C(v)

.

(71)

(68)
= max(k)

(cid:26) (cid:88)

v(cid:48)∈C(v)

max(lv(cid:48) )
yτ (v(cid:48))
(cid:124)

(cid:123)(cid:122)
=:Tv(cid:48) (yv)

Let us analyze the term in the underbrace, Tv(cid:48)(yv). We successively deduce, with the argument l in the
maximization below taking values in {1, · · · , K},

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:125)

(cid:27)

Tv(cid:48)(yv)

(69)
= max(lv(cid:48) )
yv(cid:48) ,l

ψv,v(cid:48)(yv, yv(cid:48)) + max(l)
yτ (v(cid:48))\yv(cid:48)

ψτ (v(cid:48))(yτ (v(cid:48)))

ψv(cid:48)(yv(cid:48)) + ψv,v(cid:48)(yv, yv(cid:48))+

(cid:27)
(yv(cid:48)) | lv(cid:48)(cid:48) ∈ [K] for v(cid:48)(cid:48) ∈ C(v(cid:48))(cid:9)

max(l) (cid:8) (cid:80)

v(cid:48)(cid:48)
(cid:26)ψv(cid:48)(yv(cid:48)) + ψv(cid:48),v(yv(cid:48), yv)
v(cid:48)(cid:48)∈C(v(cid:48)) m(lv(cid:48)(cid:48) )
(yv(cid:48))

v(cid:48)(cid:48)∈C(v(cid:48)) m(lv(cid:48)(cid:48) )
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ (cid:80)

v(cid:48)(cid:48)

yv(cid:48) ∈ Yv(cid:48) and
lv(cid:48)(cid:48) ∈ [K] for v(cid:48)(cid:48) ∈ C(v)

(cid:27)

(cid:26)

(cid:26)

(70)
= max(lv(cid:48) )
yv(cid:48) ,l

(69)
= max(lv(cid:48) )

(66)
= m(lv(cid:48) )
v(cid:48)

(yv) .

55

Here, the penultimate step followed from applying in reverse the identity (69) with u1, u2 being by yv(cid:48), {lv(cid:48)(cid:48) for v(cid:48)(cid:48) ∈
C(v(cid:48))} respectively, and f1 and h respectively being ψv(cid:48)(yv(cid:48)) + ψv(cid:48),v(yv(cid:48), yv) and (cid:80)
(yv(cid:48)). Plug-
ging this into (71) completes the induction argument. To complete the proof, we repeat the same argument
over the root as follows. We note that τ (r) is the entire tree G. Therefore, yτ (r) = y and ψτ (r) = ψ. We
now apply the identity (69) with u1 and u2 being yr and yτ (r)\r respectively and f1 ≡ 0 to get

v(cid:48)(cid:48) m(lv(cid:48)(cid:48) )
v(cid:48)(cid:48)

max(k)
y∈Y

ψ(y)

(69)
= max(k)
yr,l

max(l)
y\yr

ψ(y)

= max(k)
yr,l

max(l)
yτ (r)\yr

ψτ (r)(yτ (r))

(cid:27)

(cid:40)

(cid:41)

(cid:26)

(cid:26)

(cid:26)

(70)
= max(k)
yr,l

(69)
= max(k)

= ψ(k) ,

ψr(yr) + max(l) (cid:8) (cid:80)

v∈C(r) m(lv)

v

(yr) | lv ∈ [K] for v ∈ C(r)(cid:9)

(cid:27)

ψr(yr)+
v∈C(r) m(lv)

v

(cid:80)

(yr)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

yr ∈ Yr and
lv ∈ [K] for v ∈ C(r)

(cid:27)

where the last equality follows from Line 11 of Algo. 8.

The algorithm requires storage of m(k)

v , an array of size maxv∈V |Yv| for each k = 1, · · · , K, and v ∈ V.
The backpointers δ, κ are of the same size. This adds up to a total storage of O(pK maxv|Yv|). To bound
the running time, consider Line 7 of Algo. 8. For a ﬁxed v(cid:48) ∈ C(v), the computation

max(k)
yv,lv(cid:48)

(cid:110)
ψv(yv) + ψv,ρ(v)(yv, yρ(v)) + m(lv(cid:48) )

(yv)

(cid:111)

v(cid:48)

for k = 1, · · · , K takes time O(K log K maxv|Yv|). This operation is repeated for each yv ∈ Yv and once
for every (v, v(cid:48)) ∈ E. Since |E| = p − 1, the total running time is O(pK log K maxv|Yv|2).

B.2 Proof of Correctness of Entropy Smoothing of Max-Product

Next, we consider entropy smoothing.

Proposition 42. Given an augmented score function ψ(·, ·; w) deﬁned on tree structured graph G and
µ > 0 as input, Algo. 3 correctly computes f−µH (w) and ∇f−µH (w). Furthermore, Algo. 3 runs in
time O(p maxv∈V |Yv|2) and requires space O(p maxv∈V |Yv|).

Proof. The correctness of the function value f−µH follows from the bijection f−µH (w) = µ Aψ/µ(w) (cf.
Prop. 9), where Thm. 43 shows correctness of Aψ/µ. To show the correctness of the gradient, deﬁne the
probability distribution Pψ,µ as the probability distribution from Lemma 7(ii) and Pψ,µ,v, Pψ,µ,v,v(cid:48) as its
node and edge marginal probabilities respectively as

Pψ,µ(y; w) =

exp

(cid:16) 1

(cid:17)
µ ψ(y; w)

(cid:16) 1

y(cid:48)∈Y exp

µ ψ(y(cid:48); w)

(cid:17) ,

Pψ,µ,v(yv; w) =

Pψ,µ(y; w)

for yv ∈ Yv, v ∈ V , and,

Pψ,µ,v,v(cid:48)(yv, yv(cid:48); w) =

Pψ,µ(y; w)

for yv ∈ Yv, yv(cid:48) ∈ Yv(cid:48), (v, v(cid:48)) ∈ E .

(cid:80)

(cid:88)

y∈Y :
yv=yv
(cid:88)

y∈Y:
yv=yv,
yv(cid:48) =yv(cid:48)

56

Thm. 43 again shows that Algo. 9 correctly produces marginals Pψ,µ,v and Pψ,µ,v,v(cid:48). We now start with
Lemma 7(ii) and invoke (17) to get

∇f−µH (w) =

Pψ,µ(y; w)∇ψ(y; w)

(cid:88)

y∈Y

(cid:88)

y∈Y
(cid:88)

=

=

=

(cid:88)

v∈V
(cid:88)

y∈Y
(cid:88)

(cid:88)





(cid:88)

v∈V

Pψ,µ(y; w)

∇ψv(yv; w) +

∇ψv,v(cid:48)(yv, yv(cid:48); w)

 ,



Pψ,µ(y; w)∇ψv(yv; w) +

Pψ,µ(y; w)∇ψv,v(cid:48)(yv, yv(cid:48); w)

(cid:88)

(v,v(cid:48))∈E
(cid:88)

(cid:88)

(v,v(cid:48))∈E

y∈Y

Pψ,µ(y; w)∇ψv(yv; w)

v∈V

yv∈Yv

y∈Y : yv=yv
(cid:88)
(cid:88)

+

(cid:88)

(cid:88)

(v,v(cid:48))∈E

yv∈Yv

yv(cid:48) ∈Yv(cid:48)

y∈Y :

yv=yv
yv(cid:48) =yv(cid:48)

Pψ,µ,v(yv; w)∇ψv(yv; w)

(cid:88)

(cid:88)

=

v∈V

yv∈Yv

(cid:88)

(cid:88)

(cid:88)

+

(v,v(cid:48))∈E

yv∈Yv

yv(cid:48) ∈Yv(cid:48)

Pψ,µ,v,v(cid:48)(yv, yv(cid:48); w)∇ψv,v(cid:48)(yv, yv(cid:48); w) .

Pψ,µ(y; w)∇ψv,v(cid:48)(yv, yv(cid:48); w)

Here, the penultimate equality followed from breaking the sum over y ∈ Y into an outer sum that sums over
every yv ∈ Yv and an inner sum over y ∈ Y : yv = yv, and likewise for the edges. The last equality above
followed from the deﬁnitions of the marginals. Therefore, Line 3 of Algo. 3 correctly computes the gradient.
The storage complexity of the algorithm is O(p maxv|Yv|) provided that the edge marginals Pψ,µ,v,v(cid:48) are
computed on the ﬂy as needed. The time overhead of Algo. 3 after Algo. 9 is O(p maxv|Yv|2), by noting
that each edge marginal can be computed in constant time (Remark 44).

Given below is the guarantee of the sum-product algorithm (Algo. 9). See, for instance, Koller and

Friedman [2009, Ch. 10] for a proof.

Theorem 43. Consider an augmented score function ψ deﬁned over a tree structured graphical model G.
Then, the output of Algo. 9 satisﬁes

Pv(yv) =

exp(ψ(y) − A)

for all yv ∈ Yv, v ∈ V, and,

Pv,v(cid:48)(yv, yv(cid:48)) =

exp(ψ(y) − A)

for all yv ∈ Yv, yv(cid:48) ∈ Yv(cid:48), (v, v(cid:48)) ∈ E.

A = log

exp(ψ(y)) ,

(cid:88)

y∈Y
(cid:88)

y∈Y : yv=yv
(cid:88)

y∈Y :

yv=yv,
yv(cid:48) =yv(cid:48)

Furthermore, Algo. 9 runs in time O(p maxv∈V |Yv|2) and requires an intermediate storage of O(p maxv∈V |Yv|).

57

Algorithm 9 Sum-product algorithm

1: Procedure: SUMPRODUCT
2: Input: Augmented score function ψ deﬁned on tree structured graph G with root r ∈ V.
3: Notation: Let N (v) = C(v) ∪ {ρ(v)} denote all the neighbors of v ∈ V if the orientation of the edges

were ignored.

4: Initialize: Let V be a list of nodes from V arranged in increasing order of height.
5: for v in V \{r} do
6:

Set for each yρ(v) ∈ Yρ(v):

mv→ρ(v)(yρ(v)) ←

(cid:88)

yv∈Yv


exp (cid:0)ψv(yv) + ψv,ρ(v)(yv, yρ(v))(cid:1) (cid:89)

mv(cid:48)→v(yv)

 .

v(cid:48)∈C(v)

exp (ψr(yr)) (cid:81)

(cid:105)
v(cid:48)∈C(r) mv(cid:48)→r(yr)

.

(cid:104)

7: end for
8: A ← log (cid:80)
9: for v in reverse(V ) do
for v(cid:48) ∈ C(v) do
10:

yr∈Yr

11:

Set for each yv(cid:48) ∈ Yv(cid:48):

mv→v(cid:48)(yv(cid:48)) =

(cid:88)

yv∈Yv


exp (cid:0)ψv(yv) + ψv(cid:48),v(yv(cid:48), yv)(cid:1) (cid:89)

mv(cid:48)(cid:48)→v(yv)

 .

v(cid:48)(cid:48)∈N (v)\{v(cid:48)}

end for

12:
13: end for
14: for v in V do
15:
16: end for
17: for (v, v(cid:48)) in E do
18:

For every pair (yv, yv(cid:48)) ∈ Yv × Yv(cid:48), set

Set Pv(yv) ← exp (ψv(yv) − A) (cid:81)

v(cid:48)(cid:48)∈N (v) mv(cid:48)(cid:48)→v(yv) for every yv ∈ Yv.

Pv,v(cid:48)(yv, yv(cid:48)) ← exp (cid:0)ψv(yv) + ψv(cid:48)(yv(cid:48)) + ψv,v(cid:48)(yv, yv(cid:48)) − A(cid:1)

(cid:89)

mv(cid:48)(cid:48)→v(yv)

mv(cid:48)(cid:48)→v(cid:48)(yv(cid:48)) .

(cid:89)

v(cid:48)(cid:48)∈N (v)\{v(cid:48)}

v(cid:48)(cid:48)∈N (v(cid:48))\{v}

19: end for
20: return A, {Pv for v ∈ V}, {Pv,v(cid:48) for (v, v(cid:48)) ∈ E}.





58

Remark 44. Line 18 of Algo. 9 can be implemented in constant time by reusing the node marginals Pv and
messages mv→v(cid:48), mv(cid:48)→v as

Pv,v(cid:48)(yv, yv(cid:48)) =

Pv(yv)Pv(cid:48)(yv(cid:48)) exp(ψv,v(cid:48)(yv, yv(cid:48)) + A)
mv(cid:48)→v(yv)mv→v(cid:48)(yv(cid:48))

.

C Inference Oracles in Loopy Graphs

This section presents the missing details and recalls from literature the relevant algorithms and results re-
quired in Sec. 4.2. First, we review the BMMF algorithm of Yanover and Weiss [2004], followed by graph
cut inference and graph matching inference.

We now recall and prove the correctness of the decoding scheme (21) for completeness. The result is

due to Pearl [1988], Dawid [1992].

Theorem 45. Consider an unambiguous augmented score function ψ , that is, ψ(y(cid:48); w) (cid:54)= ψ(y(cid:48)(cid:48); w)
for all distinct y(cid:48), y(cid:48)(cid:48) ∈ Y. Then, the result (cid:98)y of the decoding (cid:98)yv = arg maxj∈Yv ψv;j satisﬁes (cid:98)y =
arg maxy∈Y ψ(y).

Proof. Suppose for the sake of contradiction that (cid:98)y (cid:54)= y∗ := arg maxy∈Y ψ(y). Let v ∈ V be such that
yv = j and y∗
v = j(cid:48) where j (cid:54)= j(cid:48). By the fact that y∗ has the highest augmented score and unambiguity, we
get that

max
y∈Y,yv=j(cid:48)

ψ(y) = ψ(y∗) > ψ((cid:98)y) = max

y∈Y,yv=j

ψ(y) ,

which contradicts the deﬁnition of (cid:98)yv.

C.1 Review of Best Max-Marginal First

If one has access to an algorithm M that can compute max-marginals, the top-K oracle is easily imple-
mented via the Best Max Marginal First (BMMF) algorithm of Yanover and Weiss [2004], which is recalled
in Algo. 10. This algorithm requires computations of two sets of max-marginals per iteration, where a set
of max-marginals refers to max-marginals for all variables yv in y.

Details The algorithm runs by maintaining a partitioning of the search space Y and a table ϕ(k)(v, j)
that stores the best score in partition k (deﬁned by constraints C(k)) subject to the additional constraint that
yv = j. In iteration k, the algorithm looks at the k − 1 existing partitions and picks the best partition
sk (Line 9). This partition is further divided into two parts: the max-marginals in the promising partition
(corresponding to yvk = jk) are computed (Line 11) and decoded (Line 12) to yield kth best scoring y(k).
The scores of the less promising partition are updated via a second round of max-marginal computations
(Line 14).

Guarantee The following theorem shows that Algo. 10 provably implements the top-K oracle as long
as the max-marginals can be computed exactly under the assumption of unambiguity. With approximate
max-marginals however, Algo. 10 comes with no guarantees.

59

Algorithm 10 Best Max Marginal First (BMMF)

1: Input: Augmented score function ψ, parameters w, non-negative integer K, algorithm M to compute

max-marginals of ψ.

2: Initialization: C(1) = ∅ and U (2) = ∅.
3: for v ∈ [p] do
4:

5:
6: end for
7: for k = 2, · · · , K do

For j ∈ Yv, set ϕ(1)(v; j) = max{ψ(y; w) | y ∈ Y s.t. yv = j} using M.
Set y(1)

v = arg maxj∈Yv ϕ(1)(v, j).

8:

9:

10:

11:

12:

13:

14:

(cid:110)
(v, j, s) ∈ [p] × Yv × [k − 1] (cid:12)

Deﬁne search space S (k) =
Find indices (vk, jk, sk) = arg max(v,j,s)∈S(k) ϕ(s)(v, j) and set constraints C(k) = C(sk) ∪ {yvk =
jk}.
for v ∈ [p] do

(cid:54)= j, and (v, j, s) /∈ U (t)(cid:111)
.

(cid:12) y(s)

v

For each j ∈ Yv, use M to set ϕ(k)(v, j) = max (cid:8)ψ(y; w) | y ∈ Y s.t. constraints C(k) hold and yv = j(cid:9).

Set y(k)

v = arg maxj∈Yv ϕ(k)(v, j).

end for
Update U (k+1) = U (k) ∪ {(vk, jk, sk)} and C(sk) = C(sk) ∪ {yvk (cid:54)= jk} and the max-marginal table
ϕ(sk)(v, j) = maxy∈Y,C(sk ),yv=j ψ(y; w) using M.

15: end for
16: return (cid:8)(cid:0)ψ(y(k); w), y(k)(cid:1)(cid:9)K

k=1.

Theorem 46 (Yanover and Weiss [2004]). Suppose the score function ψ is unambiguous, that is, ψ(y(cid:48); w) (cid:54)=
ψ(y(cid:48)(cid:48); w) for all distinct y(cid:48), y(cid:48)(cid:48) ∈ Y. Given an algorithm M that can compute the max-marginals of ψ
exactly, Algo. 10 makes at most 2K calls to M and its output satisﬁes ψ(yk; w) = max(k)
y∈Y ψ(y; w).
Thus, the BMMF algorithm followed by a projection onto the simplex (Algo. 6 in Appendix A) is a correct
implementation of the top-K oracle. It makes 2K calls to M.

Constrained Max-Marginals The algorithm requires computation of max-marginals subject to constraints
of the form yv ∈ Yv for some set Yv ⊆ Yv. This is accomplished by redeﬁning for a constraint yv ∈ Yv:

(cid:40)

ψ(y) =

if yv ∈ Yv
ψ(y),
−∞, otherwise

.

C.2 Max-Marginals Using Graph Cuts

This section recalls a simple procedure to compute max-marginals using graph cuts. Such a construction

was used, for instance, by Kolmogorov and Zabin [2004].

Notation In the literature on graph cut inference, it is customary to work with the energy function, which
is deﬁned as the negative of the augmented score −ψ. For this section, we also assume that the labels are
binary, i.e., Yv = {0, 1} for each v ∈ [p]. Recall the decomposition (17) of the augmented score function

60

Algorithm 11 Max-marginal computation via Graph Cuts
1: Input: Augmented score function ψ(·, ·; w) with Y = {0, 1}p, constraints C of the form yv = b for

b ∈ {0, 1}.

Add to E(cid:48) the (edge, cost) pairs (s → yv, θv;0) and (yv → t, θv;1).

2: Using artiﬁcial source s and sink t, set V (cid:48) = V ∪ {s, t} and E(cid:48) = ∅.
3: for v ∈ [p] do
4:
5: end for
6: for v, v(cid:48) ∈ R such that v < v(cid:48) do
7:

Add to E(cid:48) the (edge, cost) pairs (s → yv, θvv(cid:48);00), (yv(cid:48) → t, θvv(cid:48);11), (yv → yv(cid:48), θvv(cid:48);10), (yv(cid:48) →
yv, θvv(cid:48);01 − θvv(cid:48);00 − θvv(cid:48);11).

Add to E(cid:48) the edge yv → t if b = 0 or edge s → yv if b = 1 with cost +∞.

8: end for
9: for constraint yv = b in C do
10:
11: end for
12: Create graph G(cid:48) = (V (cid:48), E(cid:48)), where parallel edges are merged by adding weights.
13: Compute minimum cost s, t-cut of G(cid:48). Let C be its cost.
14: Create (cid:98)y ∈ {0, 1}p as follows: for each v ∈ V, set (cid:98)yv = 0 if the edge s → v is cut. Else (cid:98)yv = 1.
15: return −C, (cid:98)y.

over nodes and edges. Deﬁne a reparameterization

θv;z(w) = −ψv(z; w) for v ∈ V, z ∈ {0, 1}
if (v, v(cid:48)) ∈ E
θvv(cid:48);z,z(cid:48)(w) = −ψv,v(cid:48)(z, z(cid:48); w) ,

for (v, v(cid:48)) ∈ E, (z, z(cid:48)) ∈ {0, 1}2 .

We then get

−ψ(y) =

θv;z I(yv = z) +

θvv(cid:48);zz(cid:48) I(yv = z) I(yv(cid:48) = z(cid:48)) I((v, v(cid:48)) ∈ E) ,

p
(cid:88)

(cid:88)

v=1

z∈{0,1}

p
(cid:88)

p
(cid:88)

(cid:88)

v=1

v(cid:48)=i+1

z,z(cid:48)∈{0,1}

where we dropped the dependence on w for simplicity. We require the energies to be submodular, i.e., for
every v, v(cid:48) ∈ [p], we have that

θvv(cid:48);00 + θvv(cid:48);11 ≤ θvv(cid:48);01 + θvv(cid:48);10 .

(72)

Also, assume without loss of generality that θv;z, θvv(cid:48);zz(cid:48) are non-negative [Kolmogorov and Zabin, 2004].

Algorithm and Correctness Algo. 11 shows how to compute the max-marginal relative to a single vari-
able yv. The next theorem shows its correctness.

Theorem 47 (Kolmogorov and Zabin [2004]). Given a binary pairwise graphical model with augmented
score function ψ which satisﬁes (72), and a set of constraints C, Algo. 11 returns maxy∈YC ψ(y; w), where
YC denotes the subset of Y that satisﬁes constraints C. Moreover, Algo. 11 requires one maximum ﬂow
computation.

61

C.3 Max-Marginals Using Graph Matchings

The alignment problem that we consider in this section is as follows: given two sets V, V (cid:48), both of equal size
(for simplicity), and a weight function ϕ : V × V (cid:48) → R, the task is to ﬁnd a map σ : V → V (cid:48) so that each
v ∈ V is mapped to a unique z ∈ V (cid:48) and the total weight (cid:80)
v∈V ϕ(v, σ(v)) is maximized. For example, V
and V (cid:48) might represent two natural language sentences and this task is to align the two sentences.

Graphical Model This problem is framed as a graphical model as follows. Suppose V and V (cid:48) are of size
p. Deﬁne y = (y1, · · · , yp) so that yv denotes σ(v). The graph G = (V, E) is constructed as the fully
connected graph over V = {1, · · · , p}. The range Yv of each yv is simply V (cid:48) in the unconstrained case.
Note that when considering constrained max-marginal computations, Yv might be subset of V (cid:48). The score
function ψ is deﬁned as node and edge potentials as in Eq. (17). Again, we suppress dependence of ψ on w
for simplicity. Deﬁne unary and pairwise scores as

ψv(yv) = ϕ(v, yv)

and ψv,v(cid:48)(yv, yv(cid:48)) =

(cid:40)

0, if yv (cid:54)= yv(cid:48)
−∞, otherwise

.

Max Oracle The max oracle with ψ deﬁned as above, or equivalently, the inference problem (3) (cf.
Lemma 7(i)) can be cast as a maximum weight bipartite matching, see e.g., Taskar et al. [2005]. Deﬁne a
fully connected bipartite graph G = (V ∪ V (cid:48), E) with partitions V, V (cid:48), and directed edges from each v ∈ V
to each vertex z ∈ V (cid:48) with weight ϕ(v, z). The maximum weight bipartite matching in this graph G gives
the mapping σ, and thus implements the max oracle. It can be written as the following linear program:

max
{θv,z for (v,z)∈E}

(cid:88)

ϕ(v, z)θv,z ,

s.t.

(v,z)∈E
0 ≤ θv,z ≤ 1 ∀(v, z) ∈ V × V (cid:48)
(cid:88)

θv,z ≤ 1 ∀z ∈ V (cid:48)

v∈V
(cid:88)

z∈V (cid:48)

θv,z ≤ 1 ∀v ∈ V .

Max-Marginal For the graphical model deﬁned above, the max-marginal ψ¯v;¯z is the constrained maxi-
mum weight matching in the graph G deﬁned above subject to the constraint that ¯v is mapped to ¯z. The
linear program above can be modiﬁed to include the constraint θ¯v,¯z = 1:

max
{θv,z for (v,z)∈E}

(cid:88)

ϕ(v, z)θv,z ,

s.t.

(v,z)∈E
0 ≤ θv,z ≤ 1 ∀(v, z) ∈ V × V (cid:48)
(cid:88)

θv,z ≤ 1 ∀z ∈ V (cid:48)

(73)

θv,z ≤ 1 ∀v ∈ V

v∈V
(cid:88)

z∈V (cid:48)
θ¯v,¯z = 1 .

62

Algorithm 12 Max marginal computation via Graph matchings
1: Input: Directed bipartite graph G = (V ∪ V (cid:48), E), weights ϕ : V × V (cid:48) → R.
2: Find a maximum weight bipartite matching σ∗ in the graph G. Let the maximum weight be ψ∗.
3: Deﬁne a weighted residual bipartite graph (cid:98)G = (V ∪ V (cid:48), (cid:98)E), where the set (cid:98)E is populated as follows:
for (v, z) ∈ E, add an edge (v, z) to (cid:98)E with weight 1 − I(σ∗(v) = z), add (z, v) to (cid:98)E with weights
− I(σ∗(v) = z).

4: Find the maximum weight path from every vertex z ∈ V (cid:48) to every vertex v ∈ V and denote this by

∆(z, v).

5: Assign the max-marginals ψv;z = ψ∗ + I(σ∗(v) (cid:54)= z) (∆(z, v) + ϕ(v, z)) for all (v, z) ∈ V × V (cid:48).
6: return Max-marginals ψv;z for all (v, z) ∈ V × V (cid:48).

Algorithm to Compute Max-Marginals Algo. 12, which shows how to compute max-marginals is due
to Duchi et al. [2006]. Its running time complexity is as follows: the initial maximum weight matching
computation takes O(p3) via computation of a maximum ﬂow [Schrijver, 2003, Ch. 10]. Line 4 of Algo. 12
can be performed by the all-pairs shortest paths algorithm [Schrijver, 2003, Ch. 8.4] in time O(p3). Its
correctness is shown by the following theorem:

Theorem 48 (Duchi et al. [2006]). Given a directed bipartite graph G and weights ϕ : V × V (cid:48) → R, the
output ψv;z from Algo. 12 are valid max-marginals, i.e., ψv;z coincides with the optimal value of the linear
program (73). Moreover, Algo. 12 runs in time O(p3) where p = |V | = |V (cid:48)|.

C.4 Proof of Proposition 14

Proposition 14. Consider as inputs an augmented score function ψ(·, ·; w), an integer K > 0 and a smooth-
ing parameter µ > 0. Further, suppose that ψ is unambiguous, that is, ψ(y(cid:48); w) (cid:54)= ψ(y(cid:48)(cid:48); w) for all distinct
y(cid:48), y(cid:48)(cid:48) ∈ Y. Consider one of the two settings:

(A)

the output space Yv = {0, 1} for each v ∈ V, and the function −ψ is submodular (see Appendix C.2
and, in particular, (72) for the precise deﬁnition), or,

(B)

the augmented score corresponds to an alignment task where the inference problem (3) corresponds to
a maximum weight bipartite matching (see Appendix C.3 for a precise deﬁnition).

In these cases, we have the following:

(i) The max oracle can be implemented at a computational complexity of O(p) minimum cut computations

in Case (A), and in time O(p3) in Case (B).

(ii) The top-K oracle can be implemented at a computational complexity of O(pK) minimum cut compu-

tations in Case (A), and in time O(p3K) in Case (B).

(iii) The exp oracle is #P-complete in both cases.

Proof. A set of max-marginals can be computed by an algorithm M deﬁned as follows:

• In Case (A), invoke Algo. 11 a total of 2p times, with yv = 0, and yv = 1 for each v ∈ V. This takes

a total of 2p min-cut computations.

63

Algorithm 13 Top-K best-ﬁrst branch and bound search

1: Input: Augmented score function ψ(·, ·; w), integer K > 0, search space Y, upper bound (cid:98)ψ, split

2: Initialization: Initialize priority queue with single entry Y with priority (cid:98)ψ(Y; w), and solution set S as

strategy.

the empty list.
3: while |S| < K do
4:

Pop (cid:98)Y from the priority queue.
if (cid:98)Y = {(cid:98)y} is a singleton then
Append ((cid:98)y, ψ((cid:98)y; w)) to S.

else

5:

6:

7:

8:

9:

end if

10:
11: end while
12: return S.

Y1, Y2 ← split( (cid:98)Y).
Add Y1 with priority (cid:98)ψ(Y1; w) and Y2 with priority (cid:98)ψ(Y2; w) to the priority queue.

• In Case (B), M is simply Algo. 12, which takes time O(p3).

The max oracle can then be implmented by the decoding in Eq. (21), whose correctness is guaranteed by
Thm. 45. The top-K oracle is implemented by invoking the BMMF algorithm with M deﬁned above,
followed by a projection onto the simplex (Algo. 6 in Appendix A) and its correctness is guaranteed by
Thm. 46. Lastly, the result of exp oracle follows from Jerrum and Sinclair [1993, Thm. 15] in conjunction
with Prop. 9.

C.5

Inference using branch and bound search

Algo. 13 with the input K = 1 is the standard best-ﬁrst branch and bound search algorithm. Effectively, the
top-K oracle is implemented by simply continuing the search procedure until K outputs have been produced
- compare Algo. 13 with inputs K = 1 and K > 1. We now prove the correctness guarantee.

Proposition 15. Consider an augmented score function ψ(·, ·, w), an integer K > 0 and a smoothing
parameter µ > 0. Suppose the upper bound function (cid:98)ψ(·, ·; w) : X × 2Y → R satisﬁes the following
properties:

(a) (cid:98)ψ( (cid:98)Y; w) is ﬁnite for every (cid:98)Y ⊆ Y,

(b) (cid:98)ψ( (cid:98)Y; w) ≥ maxy∈ (cid:98)Y ψ(y; w) for all (cid:98)Y ⊆ Y, and,

(c) (cid:98)ψ({y}; w) = ψ(y; w) for every y ∈ Y.

Then, we have the following:

(i) Algo. 13 with K = 1 is a valid implementation of the max oracle.

(ii) Algo. 13 followed by a projection onto the simplex (Algo. 6 in Appendix A) is a valid implementation of

the top-K oracle.

64

Proof. Suppose at some point during the execution of the algorithm, we have a (cid:98)Y = {(cid:98)y} on Line 5 and that
|S| = k for some 0 ≤ k < K. From the properties of the quality upper bound (cid:98)ψ, and using the fact that {(cid:98)y}
had the highest priority in the priority queue (denoted by (∗)), we get,

ψ((cid:98)y; w) = (cid:98)ψ({(cid:98)y}; w)
(∗)
≥ max
Y ∈P
≥ max
Y ∈P

max
y∈Y

(cid:98)ψ(Y ; w)

ψ(y; w)

(#)
= max
y∈Y−S

ψ(y; w) ,

where the equality (#) followed from the fact that any y ∈ Y exits the priority queue only if it is added to
S. This shows that if a (cid:98)y is added to S, it has a score that is no less than that of any y ∈ Y − S. In other
words, Algo. 13 returns the top-K highest scoring y’s.

D The Casimir Algorithm and Non-Convex Extensions: Missing Proofs

This appendix contains missing proofs from Sections 5 and 6. Throughout, we shall assume that ω is ﬁxed
and drop the subscript in Aω, Dω. Moreover, an unqualiﬁed norm (cid:107)·(cid:107) refers to the Euclidean norm (cid:107) · (cid:107)2.

D.1 Behavior of the Sequence (αk)k≥0

Lemma 21. Given a positive, non-decreasing sequence (κk)k≥1 and λ ≥ 0, consider the sequence (αk)k≥0
deﬁned by (27), where α0 ∈ (0, 1) such that α2
0 ≥ λ/(λ + κ1). Then, we have for every k ≥ 1 that
0 < αk ≤ αk−1 and, α2

k ≥ λ/(λ + κk+1) .

Proof. It is clear that (27) always has a positive root, so the update is well deﬁned. Deﬁne sequences
(ck)k≥1, (dk)k≥0 as

ck =

λ + κk
λ + κk+1

,

and dk =

λ
λ + κk+1

.

Therefore, we have that ckdk−1 = dk, 0 < ck ≤ 1 and 0 ≤ dk < 1. With these in hand, the rule for αk can
be written as

−(ckα2

k−1 − dk) +

k−1 − dk)2 + 4ckα2

k−1

αk =

.

(74)

(cid:113)

(ckα2
2

We show by induction that that dk ≤ α2
satisﬁes the hypothesis for some k ≥ 1. Noting that α2
that

k < 1. The base case holds by assumption. Suppose that αk−1
k−1 − dk ≥ 0, we get

k−1 ≥ dk−1 is equivalent to ckα2

(cid:113)

(ckα2

k−1 − dk)2 + 4ckα2

k−1 ≤

(cid:113)

(ckα2

k−1 − dk)2 + 4ckα2
√
ckαk−1 .

k−1 − dk + 2

= ckα2

k−1 + 2(ckα2

k−1 − dk)(2

ckαk−1)

√

(75)

65

We now conclude from (74) and (75) that
−(ckα2

k−1 − dk) + (ckα2
k−1 − dk + 2
2

√

ckαk−1)

αk ≤

√

=

ckαk−1 ≤ αk−1 < 1 ,

(76)

since ck ≤ 1 and αk−1 < 1. To show the other side, we expand out (74) and apply (75) again to get

α2

k − dk =

(ckα2

(ckα2

=

1
2
1
2
1
2
= (ckα2

≥

k−1 − dk)2 + (ckα2
(cid:16)

2 + (ckα2

k−1 − dk)
k−1 − dk) (cid:0)2 + (ckα2
k−1 − dk)(1 −

√

(ckα2

ckαk−1) ≥ 0 .

k−1 − dk) −

(cid:113)

k−1 − dk)

(ckα2

1
2

(ckα2
(cid:113)

k−1 − dk)2 + 4ckα2
(cid:17)

k−1

k−1 − dk) −

(ckα2

k−1 − dk) − (ckα2

k−1 − dk + 2

k−1 − dk)2 + 4ckα2
√

k−1

ckαk−1)(cid:1)

The fact that (αk)k≥0 is a non-increasing sequence follows from (76).

D.2 Proofs of Corollaries to Theorem 16

We rewrite (30) from Theorem 16 as follows:




F (wk) − F ∗ ≤



k
(cid:89)

j=1

1 − αj−1
1 − δj

(cid:16)



+

1
1 − αk











k
(cid:89)

j=1

1 − αj
1 − δj

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2(cid:17)

+ µkDω

γ0
2







k
(cid:88)

k
(cid:89)

j=2

i=j

1 − αi
1 − δi

 (1 + δ1)µ1Dω +

 (µj−1 − (1 − δj)µj) Dω

 ,

(77)



Next, we have proofs of Corollaries 17 to 20.
Corollary 17. Consider the setting of Thm. 16. Let q = λ
k ≥ 1. Choose α0 =
q . Then, we have,
q and, δk =

√

q
√

2−

√

λ+κ . Suppose λ > 0 and µk = µ, κk = κ, for all

F (wk) − F ∗ ≤

√
q
√
q

3 −
1 −

(cid:18)

µD + 2

1 −

(cid:19)k

√

q
2

√

√

(F (w0) − F ∗) .

Proof. Notice that when α0 =
all k, j, 1−αk
1−δj
conditions as

= 1 −

q, we have, αk =

q for all k. Moreover, for our choice of δk, we get, for
√
q
2 . Under this choice of α0, we have, γ0 = λ. So, we get the dependence on initial

∆0 = F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2 ≤ 2(F (w0) − F ∗) ,

λ
2

by λ-strong convexity of F . The last term of (77) is now,



(cid:18)








(cid:124)


µD
√

1 −

q

1 −

√

q
2
(cid:123)(cid:122)
≤1

(cid:19)k−1

√

q
2

(cid:124)

+

(cid:125)

k
(cid:88)

(cid:18)

j=2

1 −

(cid:123)(cid:122)
(∗)
≤ 1

(cid:19)k−j

√

q
2

≤

2µD
√

1 −

,

q










(cid:125)


66

where (∗) holds since

k
(cid:88)

(cid:18)

1 −

j=2

√

q
2

(cid:19)k−j

∞
(cid:88)

(cid:18)

≤

1 −

j=0

(cid:19)j

√

q
2

=

2
√
q

.

Corollary 18. Consider the setting of Thm. 16. Let q = λ
all k ≥ 1. Choose α0 =

q and, the sequences (µk)k≥1 and (δk)k≥1 as

λ+κ , η = 1 −

√

√
q
2 . Suppose λ > 0 and κk = κ, for

µk = µηk/2 ,

and,

δk =

√

q
√

,

q

2 −

where µ > 0 is any constant. Then, we have,

F (wk) − F ∗ ≤ ηk/2

(cid:20)
2 (F (w0) − F ∗) +

µDω
√
q
1 −

(cid:18)

√

2 −

q +

√

(cid:19)(cid:21)

.

q
√
η

1 −

√

q for each k, and 1−δ

Proof. As previously in Corollary 17, notice that under the speciﬁc parameter choices here, we have, γ0 = λ,
√
q
αk =
2 = η. By λ-strong convexity of F and the fact that γ0 = λ, the
contribution of w0 can be upper bounded by 2(F (w0) − F ∗). Now, we plugging these into (77) and
collecting the terms dependent on δk separately, we get,

1−α = 1 −

F (wk) − F ∗ ≤ 2ηk(F (w0) − F ∗)
(cid:124)
(cid:125)

(cid:123)(cid:122)
=:T1

+ µkD
(cid:124)(cid:123)(cid:122)(cid:125)
=:T2

+

1
√
1 −

q









+

ηkµ1D
(cid:124) (cid:123)(cid:122) (cid:125)
=:T3

k
(cid:88)

j=2
(cid:124)

ηk−j+1(µj−1 − µj)D

+

ηk−j+1µjδjD

.

(78)

(cid:123)(cid:122)
=:T4

(cid:125)

(cid:123)(cid:122)
=:T5








(cid:125)

We shall consider each of these terms. Since ηk ≤ ηk/2, we get T1 ≤ 2ηk/2(F (w0) − F ∗) and T3 =
ηkµ1D ≤ ηkµD ≤ ηk/2µD. Moreover, T2 = µkD = ηk/2µD. Next, using 1 −

η ≤ 1 − η =

√

√
q
2 ,

k
(cid:88)

j=1
(cid:124)

√

T4 =

ηk−j+1(µj−1 − µj)D =

ηk−j+1µη(j−1)/2(1 −

η)D

≤

µD

ηk− j−1

2 =

µDη(k+1)/2

ηj/2 ≤

µD

k−2
(cid:88)

j=0

√

q
2

η(k+1)/2
√
η
1 −

k
(cid:88)

j=2

√

q
2

Similarly, using δj =

q/2η, we have,

T5 =

ηk−j+1µηj/2D

=

µD

ηk−j/2 ≤

µD

√

q
2η

√

q
2

k
(cid:88)

j=1

√
q
2

ηk/2
√

1 −

η

.

Plugging these into (78) completes the proof.

67

k
(cid:88)

j=2

µD

ηk/2
√

1 −

η

.

≤

√

k
(cid:88)

j=2
√

q
2
√

q
2

k
(cid:88)

j=1

Corollary 19. Consider the setting of Thm. 16. Suppose µk = µ, κk = κ, for all k ≥ 1 and λ = 0. Choose
α0 =

and δk = 1

(1+k)2 . Then, we have,

5−1
2

√

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2
2

+ µDω

1 +

κ
2

(cid:17)

(cid:18)

12
k + 2

+

30
(k + 2)2

(cid:19)

.

(cid:16)

8
(k + 2)2

Proof. Firstly, note that γ0 = κ α2
0
1−α0

= κ. Now, deﬁne

Ak =

(1 − αi), and, Bk =

(1 − δi) .

k
(cid:89)

i=0

k
(cid:89)

i=1

We have,

Therefore,

k
(cid:89)

(cid:18)

1 −

Bk =

i=1

1
(i + 1)2

(cid:19)

=

k
(cid:89)

i=1

i(i + 2)
(i + 1)2 =

1
2

+

1
2(k + 1)

.

(79)

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:16)

Ak−1
Bk

+

µD
1 − α0





k
(cid:89)

j=1

1 − αj−1
1 − δk

γ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)


+ µD

 (1 + δ1) + µD

≤

(cid:16)

Ak−1
Bk

F (w0) − F ∗ +

γ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)
(cid:125)

+µD

(cid:124)

+

Ak−1
Bk

5
4 µD
1 − α0
(cid:123)(cid:122)
(cid:124)
=:T2

(cid:125)

(cid:123)(cid:122)
=:T1

+ µD

(cid:124)

k
(cid:88)

j=2

Ak−1/Aj−2
Bk/Bj−1

(cid:123)(cid:122)
=:T3

.

δj
1 − αj−1
(cid:125)

k
(cid:88)

k
(cid:89)





j=2

i=j





1 − αi−1
1 − δi

δj
1 − αj−1

From Lemma 52, which analyzes the evolution of (αk) and (Ak), we get that
αk ≤ 2

k+3 for k ≥ 0. Since Bk ≥ 1
2 ,

2

(k+2)2 ≤ Ak−1 ≤ 4

(k+2)2 and

T1 ≤

(cid:16)

8
(k + 2)2

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2(cid:17)

.

γ0
2

Moreover, since α0 ≤ 2/3,

Lastly, we have,

T2 ≤

30
(k + 2)2 .

T3 ≤

k
(cid:88)

j=2

4
(k + 2)2 ×

(j + 1)2
2

× 2

(cid:19)

(cid:18) 1
2

+

1
2j

×

1
(j + 1)2 ×

1
1 − 2/j+2

≤ 2

2
(k + 2)2

k
(cid:88)

j=2

j + 2
j

4

≤

(k + 2)2 (k − 1 + 2 log k) ≤

12
k + 2

,

where we have used the simpliﬁcations (cid:80)k

j=2 1/k ≤ log k and k − 1 + 2 log k ≤ 3k.

68

Corollary 20. Consider the setting of Thm. 16 with λ = 0. Choose α0 =
constants κ, µ, deﬁne sequences (κk)k≥1, (µk)k≥1, (δk)k≥1 as

√

5−1
2

, and for some non-negative

κk = κ k , µk =

and,

δk =

µ
k

1
(k + 1)2 .

Then, for k ≥ 2, we have,

F (wk) − F ∗ ≤

log(k + 1)
k + 1

(cid:0)2(F (w0) − F ∗) + κ(cid:107)w0 − w∗(cid:107)2

2 + 27µDω

(cid:1) .

(80)

For the ﬁrst iteration (i.e., k = 1), this bound is off by a constant factor 1/ log 2.

Proof. Notice that γ0 = κ1

= κ. As in Corollary 19, deﬁne

α2
0
1−α0

From Lemma 53 and (79) respectively, we have for k ≥ 1,

Ak =

(1 − αi) ,

and, Bk =

(1 − δi) .

k
(cid:89)

i=0

1 − 1√
2
k + 1

≤ Ak ≤

and,

≤ Bk ≤ 1 .

1
k + 2

,

k
(cid:89)

i=1

1
2

Now, invoking Theorem 16, we get,

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:16)

Ak−1
Bk

(cid:124)

k
(cid:88)

j=2
(cid:124)

Ak−1/Aj−1
Bk/Bj−1

(cid:123)(cid:122)
=:T4

γ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)
(cid:125)

+

+ µkD
(cid:124)(cid:123)(cid:122)(cid:125)
=:T2

1
1 − α0
(cid:124)

Ak−1
Bk

(cid:123)(cid:122)
=:T3

(cid:123)(cid:122)
=:T1

µ1D(1 + δ1)

+

(cid:125)

(µj−1 − µj)D

+

k
(cid:88)

j=2
(cid:124)

(cid:125)

Ak−1/Aj−1
Bk/Bj−1

δjµjD

.

(cid:123)(cid:122)
=:T5

(cid:125)

(81)

We shall bound each of these terms as follows.

T1 =

(cid:16)

Ak−1
Bk

F (w0) − F ∗ +

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2(cid:17)

,

κ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)

=

γ0
2

T2 = µkD =

µD
k

(cid:16)

2
k + 1

,

≤

2µD
k + 1
2
k + 1

T3 =

1
1 − α0
where we used the fact that α0 ≤ 2/3. Next, using (cid:80)k
1 + log(k − 1), we get,

Ak−1
Bk

µ1D(1 + δ1) ≤ 3 ×

× µ ×

15
2
j=2 1/j ≤ 1 + (cid:82) k−1
j=2 1/(j − 1) = 1 + (cid:80)k−1

µD
k + 1

D =

5
4

,

1

dx/x =

T4 =

k
(cid:88)

j=2
√

2
k + 1

·

√

≤ 2

2(

2 + 1)µD

(cid:19)

(cid:18) µ

−

µ
j

j
1 − 1√
j − 1
2
(cid:18) 1 + log(k + 1)
k + 1

(cid:19)

.

√

√

D = 2

2(

2 + 1)

µD
k + 1

k
(cid:88)

j=2

1
j − 1

69

Moreover, from (cid:80)k

j=2 1/(j + 1)2 ≤ (cid:82) k+1

2

dx/x2 ≤ 1/2, it follows that

T5 =

k
(cid:88)

j=2

2
k + 1

·

j
1 − 1√
2

µ
j

·

1

√
(j + 1)2 D = 2

√

2(

2 + 1)

µD
k + 1

k
(cid:88)

j=2

1
(j + 1)2 ≤

√

√

2(

2 + 1)

µD
k + 1

.

Plugging these back into (81), we get

F (wk) − F ∗ ≤

2
k + 1
µD
k + 1

(cid:16)

F (wk) − F ∗ +
(cid:18)

√

κ
2

(cid:107)w0 − w∗(cid:107)2(cid:17)
(cid:19)
√
√

+

√

2 +

+

2(1 +

2)

+ 2

2(1 +

2)µD

15
2

1 + log(k + 1)
k + 1

.

To complete the proof, note that log(k + 1) ≥ 1 for k ≥ 2 and numerically verify that the coefﬁcient of µD
is smaller than 27.

D.3

Inner Loop Complexity Analysis for Casimir

Before proving Prop. 27, the following lemmas will be helpful. First, we present a lemma from Lin et al.
[2018, Lemma 11] about the expected number of iterations a randomized linearly convergent ﬁrst order
methods requires to achieve a certain target accuracy.
Lemma 49. Let M be a linearly convergent algorithm and f ∈ FL,λ. Deﬁne f ∗ = minw∈Rd f (w). Given
a starting point w0 and a target accuracy (cid:15), let (wk)k≥0 be the sequence of iterates generated by M. Deﬁne
T ((cid:15)) = inf {k ≥ 0 | f (wk) − f ∗ ≤ (cid:15)} . We then have,

E[T ((cid:15))] ≤

1
τ (L, λ)

log

(cid:18) 2C(L, λ)
τ (L, λ)(cid:15)

(cid:19)

(f (w0) − f ∗)

+ 1 .

(82)

This next lemma is due to Lin et al. [2018, Lemma 14, Prop. 15].

Lemma 50. Consider Fµω,κ(· ; z) deﬁned in Eq. (25) and let δ ∈ [0, 1). Let (cid:98)F ∗ = minw∈Rd Fµω,κ(w; z)
and (cid:98)w∗ = arg minw∈Rd Fµω,κ(w; z). Further let Fµω(· ; z) be Lµω-smooth. We then have the following:
Lµω + κ
2

Fµω,κ(z; z) − (cid:98)F ∗ ≤

(cid:107)z − (cid:98)w∗(cid:107)2
2 ,

and,

Fµω,κ( (cid:98)w; z) − (cid:98)F ∗ ≤

(cid:107)z − (cid:98)w∗(cid:107)2

2 =⇒ Fµω,κ( (cid:98)w; z) − (cid:98)F ∗ ≤

(cid:107) (cid:98)w − z(cid:107)2
2 .

δκ
2

δκ
8

We now restate and prove Prop. 27.

Proposition 27. Consider Fµω,κ(· ; z) deﬁned in Eq. (25), and a linearly convergent algorithm M with
parameters C, τ . Let δ ∈ [0, 1). Suppose Fµω is Lµω-smooth and λ-strongly convex. Then the expected
number of iterations E[ (cid:98)T ] of M when started at z in order to obtain (cid:98)w ∈ Rd that satisﬁes

Fµω,κ( (cid:98)w; z) − min
w

Fµω,κ(w; z) ≤ δκ

2 (cid:107)w − z(cid:107)2

2

(83)

is upper bounded by

E[ (cid:98)T ] ≤

1
τ (Lµω + κ, λ + κ)

log

(cid:18) 8C(Lµω + κ, λ + κ)
τ (Lµω + κ, λ + κ)

·

Lµω + κ
κδ

(cid:19)

+ 1 .

Proof. In order to invoke Lemma 49, we must appropriately set (cid:15) for (cid:98)w to satisfy (83) and then bound the
ratio (Fµω,κ(z; z) − (cid:98)F ∗)/(cid:15). Firstly, Lemma 50 tells us that choosing (cid:15) = δkκk
2 guarantees that
the (cid:98)w so obtained satisﬁes (83), where (cid:98)w∗ := arg minw∈Rd Fµω,κ(w; z), Therefore, (Fµω,κ(z; z) − (cid:98)F ∗)/(cid:15)
is bounded from above by 4(Lµω + κ)/κδ.

8 (cid:107)zk−1 − (cid:98)w∗(cid:107)2

70

D.4

Information Based Complexity of Casimir-SVRG

Presented below are the proofs of Propositions 29 to 32 from Section 5.3. We use the following values of
C, τ , see e.g., Hofmann et al. [2015].

τ (L, λ) =

8 L

1
λ + n
(cid:32)

L
λ

1
λ + n(cid:1)
(cid:33)

.

≥

8 (cid:0) L
n L
λ
λ + n

8 L

C(L, λ) =

1 +

κ =

(cid:40) A

µn − λ , if A
λ , otherwise

µn > 4λ

,

(cid:32)

(cid:114)

E[N ] ≤ (cid:101)O

n +

(cid:33)

.

AωDωn
λ(cid:15)

K ≤

log

2
√
q

(cid:18) 2∆F0

(cid:19)

(cid:15) − cqµD

,

Proposition 29. Consider the setting of Thm. 16 with λ > 0 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as
the inner solver with parameters: µk = µ = (cid:15)/10Dω, κk = k chosen as

q = λ/(λ + κ), α0 =
q/(2 −
F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

q, and δ =

√

√

√

q). Then, the number of iterations N to obtain w such that

Proof. We use shorthand A := Aω, D := Dω, Lµ = λ + A/µ and ∆F0 = F (w0) − F ∗. Let C, τ be the
linear convergence parameters of SVRG. From Cor. 17, the number of outer iterations K required to obtain
F (wK) − F ∗ ≤ (cid:15) is

where cq = (3 −
√
δk =

q/(2 −

√

√

q),

√

q)/(1 −

q). From Prop. 27, the number Tk of inner iterations for inner loop k is, from

E[Tk] ≤

1
τ (Lµ + κ, λ + κ)
2
τ (Lµ + κ, λ + κ)

log

log

(cid:18) 8C(Lµ + κ, λ + κ)
τ (Lµ + κ, λ + κ)
(cid:18) 8C(Lµ + κ, λ + κ)
τ (Lµ + κ, λ + κ)

·

·

Lµ + κ
κ
Lµ + κ
κ

≤

2 −
√

2 −
√

·

·

√

q
√

q

(cid:19)

q

+ 1

(cid:19)

q

.

Let the total number N of iterations of SVRG to obtain an iterate w that satisﬁes F (w) − F ∗ ≤ (cid:15). Next,
we upper bound E[N ] ≤ (cid:80)K
i=1

E[Tk] as

E[N ] ≤

√

4
qτ (Lµ + κ, λ + κ)

log

(cid:18) 8C(Lµ + κ, λ + κ)
τ (Lµ + κ, λ + κ)

Lµ + κ
κ

2 −
√

q

log

(cid:18) 2(F (w0) − F ∗)
(cid:15) − cqµD

(cid:19)

.

√

(cid:19)

q

(84)

Next, we shall plug in C, τ for SVRG in two different cases:

• Case 1: A > 4µλn, in which case κ + λ = A/(µn) and q < 1/4.

71

• Case 2: A ≤ 4µλn, in which case, κ = λ and q = 1/2.

We ﬁrst consider the term outside the logarithm. It is, up to constants,

(cid:18)

n +

1
√
q

A
µ(λ + κ)

(cid:19)

(cid:114)

= n

λ + κ
λ

+

A
µ(cid:112)λ(λ + κ)

.

For Case 1, plug in κ + λ = A/(µn) so this term evaluates to (cid:112)ADn/(λ(cid:15)). For Case 2, we use the fact
that A ≤ 4µλn so that this term can be upper bounded by,

(cid:32)(cid:114)

n

λ + κ
λ

+ 4

(cid:114) λ

(cid:33)

λ + κ

√

= 3

2n ,

since we chose κ = λ. It remains to consider the logarithmic terms. Noting that κ ≥ λ always, it follows
that the ﬁrst log term of (84) is clearly logarithmic in the problem parameters.

As for the second logarithmic term, we must evaluate cq. For Case 1, we have that q < 1/4 so that
cq < 5 and cqµD < (cid:15)/2. For Case 2, we get that q = 1/2 and cq < 8 so that cqµD < 4(cid:15)/5. Thus, the
second log term of (84) is also logarithmic in problem parameters.

Proposition 30. Consider the setting of Thm. 16. Suppose λ > 0 and κk = κ, for all k ≥ 1 and that α0,
q/2. If we run Algo. 4 with
(µk)k≥1 and (δk)k≥1 are chosen as in Cor. 18, with q = λ/(λ+κ) and η = 1−
SVRG as the inner solver with these parameters, the number of iterations N of SVRG required to obtain w
such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

√

(cid:18)

E[N ] ≤ (cid:101)O

n +

(cid:18)

Aω
µ(λ + κ)(cid:15)

F (w0) − F ∗ +

µDω
√
1 −

q

(cid:19)(cid:19)

.

Proof. We continue to use shorthand A := Aω, D := Dω. First, let us consider the minimum number of
outer iterations K required to achieve F (wK) − F ∗ ≤ (cid:15). From Cor. 18, if we have η−K/2∆0 ≤ (cid:15), or,

K ≥ Kmin :=

log (∆0/(cid:15))
η(cid:1) .
log (cid:0)1/

√

For this smallest value, we have,

µKmin = µηKmin/2 =

µ(cid:15)
∆0

.

(85)

Let C, τ be the linear convergence parameters of SVRG, and deﬁne Lk := λ + A/µk for each k ≥ 1.
Further, let T (cid:48) be such that

T (cid:48) ≥

max
k∈{1,··· ,Kmin}

log

(cid:18)
8

C(Lk + κ, λ + κ)
τ (Lk + κ, λ + κ)

Lk + κ
κδ

(cid:19)

.

72

Then, the total complexity is, from Prop. 27, (ignoring absolute constants)

E[N ] ≤

n +

λ + κ + A
µk
λ + κ

(cid:33)

T (cid:48)

(cid:32)

Kmin(cid:88)

k=1

Kmin(cid:88)

(cid:18)

k=1
(cid:32)

(cid:32)

(cid:32)

=

n + 1 +

(cid:19)

η−k/2

T (cid:48)

A/µ
λ + κ

=

Kmin(n + 1) +

≤

Kmin(n + 1) +

A/µ
λ + κ

A/µ
λ + κ

Kmin(cid:88)

(cid:33)

η−k/2

T (cid:48)

k=1
η−Kmin/2
1 − η1/2

(cid:33)

T (cid:48)

=

(n + 1)

log (cid:0) ∆0
(cid:15)
log(1/√

(cid:1)

η)

+

A/µ
λ + κ

1
√
1 −

η

∆0
(cid:15)

(cid:33)

T (cid:48) .

(86)

It remains to bound T (cid:48). Here, we use λ + A
T (cid:48) is logarithmic in ∆0/(cid:15), n, AD, µ, κ, λ−1.

µ ≤ Lk ≤ λ + A
µK

for all k ≤ K together with (85) to note that

Proposition 31. Consider the setting of Thm. 16 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as the inner
√
, δk = 1/(k + 1)2, and κk = κ = Aω/µ(n + 1).
solver with parameters: µk = µ = (cid:15)/20Dω, α0 =
Then, the number of iterations N to get a point w such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

5−1
2

(cid:32)

(cid:114)

E[N ] ≤ (cid:101)O

n

F (w0) − F ∗
(cid:15)

(cid:112)

+

AωDωn

(cid:107)w0 − w∗(cid:107)2
(cid:15)

(cid:33)

.

Proof. We use shorthand A := Aω, D := Dω, Lµ = A/µ and ∆F0 = F (w0) − F ∗ + κ
2 (cid:107)w0 − w∗(cid:107)2.
Further, let C, τ be the linear convergence parameters of SVRG. In Cor. 19, the fact that K ≥ 1 allows us to
bound the contribution of the smoothing as 10µD. So, we get that the number of outer iterations K required
to get F (wK) − F ∗ ≤ (cid:15) can be bounded as

K + 1 ≤

(cid:115)

8∆F0
(cid:15) − 10µD

.

Moreover, from our choice δk = 1/(k + 1)2, the number of inner iterations Tk for inner loop k is, from
Prop. 27,

E[Tk] ≤

1
τ (Lµ + κ, κ)
2
τ (Lµ + κ, κ)

log

log

(cid:18) 8C(Lµ + κ, κ)
τ (Lµ + κ, κ)
(cid:18) 8C(Lµ + κ, κ)
τ (Lµ + κ, κ)

·

·

Lµ + κ
κ
Lµ + κ
κ

≤

(cid:19)

· (k + 1)2

+ 1

·

8∆F0
(cid:15) − 10µD

(cid:19)

.

Next, we consider the total number N of iterations of SVRG to obtain an iterate w such that F (w) −

F ∗ ≤ (cid:15). Using the fact that E[N ] ≤ (cid:80)K
i=1

E[Tk], we bound it as

E[N ] ≤

1
τ (Lµ + κ, κ)

8∆F0
(cid:15) − 10µD

log

(cid:18) 64C(Lµ + κ, κ)
τ (Lµ + κ, κ)

Lµ + κ
κ

∆F0
(cid:15) − 10µD

(cid:19)

.

(cid:115)

(87)

73

Now, we plug into (87) the values of C, τ for SVRG. Note that κ = Lµ/(n + 1). So we have,

1
τ (Lµ + κ, κ)

= 8

(cid:18) Lµ + κ
κ

(cid:19)

+ n

= 16(n + 1) , and,

C(Lµ + κ, κ) =

Lµ + κ
κ

(cid:32)

1 +

(cid:33)

n Lµ+κ
κ
8 L+κ
κ + n

≤ (n + 2) (cid:0)1 + n
8

(cid:1) .

It now remains to assign µ = (cid:15)/(20D) and plug C, τ from above into (87), noting that κ = 20AD/((cid:15)(n + 1)).

Proposition 32. Consider the setting of Thm. 16. Suppose λ = 0 and that α0, (µk)k≥1,(κk)k≥1 and (δk)k≥1
are chosen as in Cor. 20. If we run Algo. 4 with SVRG as the inner solver with these parameters, the number
of iterations N of SVRG required to obtain w such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

E[N ] ≤ (cid:101)O

(cid:0)F (w0) − F ∗ + κ(cid:107)w0 − w∗(cid:107)2

2 + µD(cid:1)

(cid:18) 1
(cid:15)

(cid:18)

n +

(cid:19)(cid:19)

.

Aω
µκ

Proof. Deﬁne short hand A := Aω, D := Dω and

∆0 := 2(F (w0) − F ∗) + κ(cid:107)w0 − w∗(cid:107)2 + 27µD .

From Cor. 20, the number of iterations K required to obtain F (wK) − F ∗ ≤ log(K+1)
Lemma 54),

K+1 ∆0 ≤ (cid:15) is (see

K + 1 =

2∆0
(cid:15)

log

2∆0
(cid:15)

.

Let C, τ be such that SVRG is linearly convergent with parameters C, τ , and deﬁne Lk := A/µk for each
k ≥ 1. Further, let T (cid:48) be such that

T (cid:48) ≥ max

k∈{1,··· ,K}

(cid:18)
8

log

C(Lk + κ, κ)
τ (Lk + κ, κ)

Lk + κ
κδk

(cid:19)

.

Clearly, T (cid:48) is logarithmic in K, n, AD, µ, κ. From Prop. 27, the minimum total complexity is (ignoring

absolute constants)

E[N ] =

K
(cid:88)

(cid:18)

n +

A/µk + κk
κk

(cid:19)

T (cid:48)

n + 1 +

(cid:19)

T (cid:48)

A
µkκk

=

=

(cid:18)

(cid:18)

k=1
K
(cid:88)

k=1
K
(cid:88)

k=1
(cid:18)

n + 1 +

(cid:19)

T (cid:48)

A
µκ
(cid:19)

≤

n + 1 +

KT (cid:48) ,

A
µκ

and plugging in K from (89) completes the proof.

74

(88)

(89)

(90)

D.5 Prox-Linear Convergence Analysis

We ﬁrst prove Lemma 34 that speciﬁes the assumption required by the prox-linear in the case of structured
prediction.
Lemma 34. Consider the structural hinge loss f (w) = maxy∈Y ψ(y; w) = h ◦ g(w) where h, g are as
deﬁned in (6). If the mapping w (cid:55)→ ψ(y; w) is L-smooth with respect to (cid:107) · (cid:107)2 for all y ∈ Y, then it holds
for all w, z ∈ Rd that

|h(g(w + z)) − h(g(w) + ∇g(w)z)| ≤

L
2

(cid:107)z(cid:107)2
2 .

Proof. For any A ∈ Rm×d and w ∈ Rd, and (cid:107)A(cid:107)2,1 deﬁned in (2), notice that

(cid:107)Aw(cid:107)∞ ≤ (cid:107)A(cid:107)2,1(cid:107)w(cid:107)2 .

(91)

Now using the fact that max function h satisﬁes |h(u(cid:48)) − h(u)| ≤ (cid:107)u(cid:48) − u(cid:107)∞ and the fundamental theorem
of calculus (∗), we deduce

|h(g(w + z)) − h(g(w) + ∇g(w)z)| ≤ (cid:107)g(w + z) − (g(w) + ∇g(w)z) (cid:107)∞
(cid:13)
(∗)
(cid:13)
≤
(cid:13)
(cid:13)∞

(∇g(w + tz) − ∇g(w))z dt

(cid:90) 1

(cid:107)∇g(w + tz) − ∇g(w)(cid:107)2,1(cid:107)z(cid:107)2 dt .

(92)

Note that the deﬁnition (2) can equivalently be stated as (cid:107)A(cid:107)2,1 = max(cid:107)u(cid:107)1≤1 (cid:107)A(cid:62)u(cid:107)2. Given u ∈ Rm,
we index its entries uy by y ∈ Y. Then, the matrix norm in (92) can be simpliﬁed as

(cid:107)∇g(w + tz) − ∇g(w)(cid:107)2,1 = max
(cid:107)u(cid:107)1≤1

uy(∇ψ(y; w + tz) − ∇ψ(y; w))

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

|uy|(cid:107)∇ψ(y; w + tz) − ∇ψ(y; w)(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
0
(cid:90) 1

0

(91)
≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

y∈Y
(cid:88)

y∈Y

≤ max
(cid:107)u(cid:107)1≤1

≤ Lt(cid:107)z(cid:107)2 ,

from the L-smoothness of ψ. Plugging this back into (92) completes the proof. The bound on the smothing
approximation holds similarly by noticing that if h is 1-Lipschitz then hµω too since ∇hµω(u) ∈ dom h∗
for any u ∈ dom h.

D.6

Information Based Complexity of the Prox-Linear Algorithm with Casimir-SVRG

Proposition 37. Consider the setting of Thm. 35. Suppose the sequence {(cid:15)k}k≥1 satisﬁes (cid:15)k = (cid:15)0/k for
some (cid:15)0 > 0 and that the subproblem of Line 3 of Algo. 5 is solved using Casimir-SVRG with the settings
of Prop. 29. Then, total number of SVRG iterations N required to produce a w such that (cid:107)(cid:37)η(w)(cid:107)2 ≤ (cid:15) is
bounded as

E[N ] ≤ (cid:101)O





n
η(cid:15)2 (F (w0) − F ∗ + (cid:15)0) +

(cid:113)

AωDωn(cid:15)−1
0
η(cid:15)3



(F (w0) − F ∗ + (cid:15)0)3/2

 .

75

Proof. First note that (cid:80)K
k=1 k−1 ≤ 4(cid:15)0 log K for K ≥ 2. Let ∆F0 := F (w0) − F ∗ and use
shorthand A, D for Aω, Dω respectively. From Thm. 35, the number K of prox-linear iterations required to
ﬁnd a w such that (cid:107)(cid:37)η(w)(cid:107)2 ≤ (cid:15) must satisfy

k=1 (cid:15)k ≤ (cid:15)0

(cid:80)K

For this, it sufﬁces to have (see e.g., Lemma 54)

2
ηK

(∆F0 + 4(cid:15)0 log K) ≤ (cid:15) .

K ≥

4(∆F0 + 4(cid:15)0)
η(cid:15)2

log

(cid:18) 4(∆F0 + 4(cid:15)0)
η(cid:15)2

(cid:19)

.

Before we can invoke Prop. 29, we need to bound the dependence of each inner loop on its warm start:
Fη(wk−1; wk−1) − Fη(w∗
k; wk−1) in terms of problem parameters, where w∗
k = arg minw Fη(w; wk−1)
is the exact result of an exact prox-linear step. We note that Fη(wk−1; wk−1) = F (wk−1) ≤ F (w0), by
Line 4 of Algo. 5. Moreover, from η ≤ 1/L and Asmp. 33, we have,

Fη(w∗

k; wk−1) =

h(cid:0)g(i)(wk−1) + ∇g(i)(wk−1)(w∗

k − wk−1)(cid:1) +

(cid:107)w∗

k(cid:107)2

2 +

(cid:107)w∗

k − wk−1(cid:107)2
2

h(cid:0)g(i)(wk−1) + ∇g(i)(wk−1)(w∗

k − wk−1)(cid:1) +

(cid:107)w∗

k(cid:107)2

2 +

(cid:107)w∗

k − wk−1(cid:107)2
2

λ
2

λ
2

1
2η

L
2

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

1
n

1
n

1
n

≥

≥

h(cid:0)g(i)(w∗

k)(cid:1) +

(cid:107)w∗

k(cid:107)2
2

λ
2

i=1
= F (w∗

k) ≥ F ∗ .

Thus, we bound Fη(wk−1; wk−1) − Fη(w∗
k; wk−1) ≤ ∆F0. We now invoke Prop. 29 and collect all
constants and terms logarithmic in n, (cid:15)−1, (cid:15)−1
0 , ∆F0, η−1, AωDω in T , T (cid:48), T (cid:48)(cid:48). We note that all terms in the
logarithm in Prop. 29 are logarithmic in the problem parameters here. Letting Nk be the number of SVRG
iterations required for iteration k, we get,

E[N ] =

E[Nk] ≤

(cid:32)

K
(cid:88)

n +

(cid:33)

T

(cid:114) ηADn
(cid:15)k
(cid:33)(cid:35)

√

k

T

k=1
(cid:114) ηADn
(cid:15)0
(cid:114) ηADn
(cid:15)0

(cid:32) K
(cid:88)

k=1
(cid:35)

K3/2

T (cid:48)

K
(cid:88)

k=1
(cid:34)

≤

nK +

≤

nK +

(cid:34)

(cid:34)

(cid:34)

≤

=

n
η(cid:15)2 (∆F0 + (cid:15)0) +

(cid:114) ηADn
(cid:15)0

(cid:18) ∆F0 + (cid:15)0
η(cid:15)2

(cid:19)3/2(cid:35)

T (cid:48)(cid:48)

n
η(cid:15)2 (∆F0 + (cid:15)0) +

ADn
η(cid:15)3

(∆F0 + (cid:15)0)3/2
√
(cid:15)0

(cid:35)

T (cid:48)(cid:48) .

√

76

D.7 Some Helper Lemmas

The ﬁrst lemma is a property of the squared Euclidean norm from Lin et al. [2018, Lemma 5], which we
restate here.

Lemma 51. For any vectors, w, z, r ∈ Rd, we have, for any θ > 0,

(cid:107)w − z(cid:107)2 ≥ (1 − θ)(cid:107)w − r(cid:107)2 +

1 −

(cid:107)r − z(cid:107)2 .

(cid:18)

(cid:19)

1
θ

The next lemmas consider rates of the sequences (αk) and (Ak) under different recursions.

Lemma 52. Deﬁne a sequence (αk)k≥0 as

Then this sequence satisﬁes

Moreover, Ak := (cid:81)k

j=0(1 − αk) satisﬁes

√

5 − 1
α0 =
2
k = (1 − αk)α2
α2

k−1 .

√

2
k + 3

≤ αk ≤

2
k + 3

.

2

(k + 3)2 ≤ Ak ≤

4
(k + 3)2 .

Proof. Notice that α0 satisﬁes α2
Hence, we can deﬁne a sequence (bk)k≥0 such that bk := 1/αk. It satisﬁes the recurrence, b2
for k ≥ 1, or in other words, bk = 1
2

0 = 1 − α0. Further, it is clear from deﬁnition that αk ∈ (0, 1) ∀k ≥ 0.
k−1

. Form this we get,

k − bk = b2

1 + 4b2

1 +

k−1

(cid:113)

(cid:16)

(cid:17)

bk ≥ bk−1 +

≥ b0 +

≥

+

k
2

3
2

k
2

.

1
2

since b0 =

. This gives us the upper bound on αk. Moreover, unrolling the recursion,

√

5+1
2

k = (1 − αk)α2
α2

k−1 = Ak

α2
0
1 − α0

= Ak .

(93)

Since αk ≤ 2/(k + 3), (93) yields the upper bound on Ak. The upper bound on αk again gives us,

Ak ≥

k
(cid:89)

(cid:18)

1 −

i=0

(cid:19)

2
i + 3

=

2
(k + 2)(k + 3)

≥

2
(k + 3)2 ,

to get the lower bound on Ak. Invoking (93) again to obtain the lower bound on αk completes the proof.

The next lemma considers the evolution of the sequences (αk) and (Ak) with a different recursion.

77

Lemma 53. Consider a sequence (αk)k≥0 deﬁned by α0 =

, and αk+1 as the non-negative root of

Further, deﬁne

Then, we have for all k ≥ 0,

√

5−1
2

α2
k
1 − αk

= α2

k−1

k
k + 1

.

Ak =

(1 − αi) .

k
(cid:89)

i=0

1
k + 1

(cid:18)

1 −

(cid:19)

1
√
2

≤ Ak ≤

1
k + 2

.

Proof. Deﬁne a sequence (bk)k≥0 such that bk = 1/αk, for each k. This is well-deﬁned because αk (cid:54)=
0, which may be veriﬁed by induction. This sequence satisﬁes the recursion for k ≥ 1: bk(bk − 1) =
(cid:0) k+1
k

(cid:1) bk−1. From this recursion, we get,

(cid:32)

(cid:115)

bk =

1 +

1 + 4b2

k−1

(cid:19)(cid:33)

(cid:18) k + 1
k

1
2

1
2

1
2
√

≥

≥

+ bk−1
(cid:32)

(cid:114)

1 +

(cid:114)

k + 1
k

k + 1
k

k + 1
2

k + 1

√

=

(∗)
≥

(cid:16)

√

(cid:16)√

1/

2 + · · · + 1/

k + 1

+ b0

k + 1

+ · · · +

+ b0

k + 1

(cid:114)

(cid:33)

k + 1
2

√

√

(cid:17)
2

(cid:17)

√

√

√

√

(cid:16)√

k + 2 + b0 −
√

=

k + 1

k + 2 + b0 −

k + 1 ≥ (cid:82) k+2

2

dx√
x = 2(

√

k + 2 −

2) . Since

√

(cid:17)
2

,

√

where (∗) followed from noting that 1/

2 + · · · + 1/

b0 = 1/α0 =

2, we have, for k ≥ 1,

√

5+1
2 >

√

αk ≤

√

1

√

k + 1(

k + 2 + b0 −

2)

√

≤

√

1
√
k + 1

.

k + 2

This relation also clearly holds for k = 0. Next, we claim that

Ak = (k + 1)α2

k ≤

√

k + 1
√

(

k + 1

k + 2)2

=

1
k + 2

.

Indeed, this is true because

For the lower bound, we have,

k = (1 − αk)α2
α2

k−1

k
k + 1

= Ak

α2
0
1 − α0

1
k + 1

=

Ak
k + 1

.

Ak =

(1 − αi) ≥

1 −

√

k
(cid:89)

(cid:18)

i=0

1
√
i + 1

i + 2

(cid:19)

(cid:18)

≥

1 −

(cid:19) k
(cid:89)

(cid:18)

1 −

1
√
2

(cid:19)

=

1
i + 1

1 − 1√
2
k + 1

.

i=1

k
(cid:89)

i=0

(94)

(95)

(96)

(97)

78

Lemma 54. Fix some (cid:15) > 0. If k ≥ 2

(cid:15) log 2

(cid:15) , then we have that log k

k ≤ (cid:15).

Proof. We have, since log x ≤ x for x > 0,

log k
k

≤

log 2

(cid:15) + log log 2
(cid:15) log 2

2

(cid:15)

(cid:15)

(cid:32)

(cid:15)
2

=

1 +

(cid:33)

log log 2
(cid:15)
log 2
(cid:15)

≤ (cid:15) .

E Experiments: Extended Evaluation

Given here are plots for all missing classes of PASCAL VOC 2007. Figures 8 to 10 contain the extension of
Figure 3 while Figures 11 to 13 contain the extension of Figure 4 to all classes.

79

Figure 8: Comparison of convex optimization algorithms for the task of visual object localization on PAS-
CAL VOC 2007 for λ = 10/n for all other classes (1/3).

80

Figure 9: Comparison of convex optimization algorithms for the task of visual object localization on PAS-
CAL VOC 2007 for λ = 10/n for all other classes (2/3).

81

Figure 10: Comparison of convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 10/n for all other classes (3/3).

82

Figure 11: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n for all other classes (1/3).

83

Figure 12: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n for all other classes (2/3).

84

Figure 13: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n for all other classes (3/3).

85

9
1
0
2
 
b
e
F
 
8
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
8
2
2
3
0
.
2
0
9
1
:
v
i
X
r
a

A Smoother Way to Train Structured Prediction Models

Krishna Pillutla1

Vincent Roulet2

Sham M. Kakade1,2

Zaid Harchaoui2

1 Paul G. Allen School of Computer Science and Engineering, University of Washington
2 Department of Statistics, University of Washington
{pillutla,sham}@cs.washington.edu, {vroulet,zaid}@uw.edu

Abstract

We present a framework to train a structured prediction model by performing smoothing on the in-
ference algorithm it builds upon. Smoothing overcomes the non-smoothness inherent to the maximum
margin structured prediction objective, and paves the way for the use of fast primal gradient-based op-
timization algorithms. We illustrate the proposed framework by developing a novel primal incremental
optimization algorithm for the structural support vector machine. The proposed algorithm blends an
extrapolation scheme for acceleration and an adaptive smoothing scheme and builds upon the stochastic
variance-reduced gradient algorithm. We establish its worst-case global complexity bound and study
several practical variants, including extensions to deep structured prediction. We present experimen-
tal results on two real-world problems, namely named entity recognition and visual object localization.
The experimental results show that the proposed framework allows us to build upon efﬁcient inference
algorithms to develop large-scale optimization algorithms for structured prediction which can achieve
competitive performance on the two real-world problems.

1 Introduction

Consider the optimization problem arising when training maximum margin structured prediction models:

(cid:34)

min
w∈Rd

F (w) :=

1
n

n
(cid:88)

i=1

(cid:35)

f (i)(w) +

(cid:107)w(cid:107)2
2

,

λ
2

(1)

where each f (i) is the structural hinge loss. Max-margin structured prediction was designed to forecast
discrete data structures such as sequences and trees [Taskar et al., 2004, Tsochantaridis et al., 2004].

Batch non-smooth optimization algorithms such as cutting plane methods are appropriate for problems
with small or moderate sample sizes [Tsochantaridis et al., 2004, Joachims et al., 2009]. Stochastic non-
smooth optimization algorithms such as stochastic subgradient methods can tackle problems with large
sample sizes [Ratliff et al., 2007, Shalev-Shwartz et al., 2011]. However, both families of methods achieve
the typical worst-case complexity bounds of non-smooth optimization algorithms and cannot easily leverage
a possible hidden smoothness of the objective.

Furthermore, as signiﬁcant progress is being made on incremental smooth optimization algorithms for
training unstructured prediction models [Lin et al., 2018], we would like to transfer such advances and design
faster optimization algorithms to train structured prediction models. Indeed if each term in the ﬁnite-sum
were L-smooth, incremental optimization algorithms such as MISO [Mairal, 2015], SAG [Le Roux et al.,
2012, Schmidt et al., 2017], SAGA [Defazio et al., 2014], SDCA [Shalev-Shwartz and Zhang, 2013], and

1

SVRG [Johnson and Zhang, 2013] could leverage the ﬁnite-sum structure of the objective (1) and achieve
faster convergence than batch algorithms on large-scale problems.

Incremental optimization algorithms can be further accelerated, either on a case-by-case basis [Shalev-
Shwartz and Zhang, 2014, Frostig et al., 2015, Allen-Zhu, 2017, Defazio, 2016] or using the Catalyst accel-
eration scheme [Lin et al., 2015, 2018], to achieve near-optimal convergence rates [Woodworth and Srebro,
2016]. Accelerated incremental optimization algorithms demonstrate stable and fast convergence behavior
on a wide range of problems, in particular for ill-conditioned ones.

We introduce a general framework that allows us to bring the power of accelerated incremental optimiza-
tion algorithms to the realm of structured prediction problems. To illustrate our framework, we focus on the
problem of training a structural support vector machine (SSVM), and extend the developed algorithms to
deep structured prediction models with nonlinear mappings.

We seek primal optimization algorithms, as opposed to saddle-point or primal-dual optimization algo-
rithms, in order to be able to tackle structured prediction models with afﬁne mappings such as SSVM as
well as deep structured prediction models with nonlinear mappings. We show how to shade off the inherent
non-smoothness of the objective while still being able to rely on efﬁcient inference algorithms.

Smooth Inference Oracles. We introduce a notion of smooth inference oracles that gracefully ﬁts the
framework of black-box ﬁrst-order optimization. While the exp inference oracle reveals the relation-
ship between max-margin and probabilistic structured prediction models, the top-K inference oracle
can be efﬁciently computed using simple modiﬁcations of efﬁcient inference algorithms in many cases
of interest.

Incremental Optimization Algorithms. We present a new algorithm built on top of SVRG, blending an
extrapolation scheme for acceleration and an adaptive smoothing scheme. We establish the worst-
case complexity bounds of the proposed algorithm and extend it to the case of non-linear mappings.
Finally, we demonstrate its effectiveness compared to competing algorithms on two tasks, namely
named entity recognition and visual object localization.

The code is publicly available as a software library called Casimir1. The outline of the paper is as
follows: Sec. 1.1 reviews related work. Sec. 2 discusses smoothing for structured prediction followed by
Sec. 3, which deﬁnes and studies the properties of inference oracles and Sec. 4, which describes the concrete
implementation of these inference oracles in several settings of interest. Then, we switch gears to study
accelerated incremental algorithms in convex case (Sec. 5) and their extensions to deep structured prediction
(Sec. 6). Finally, we evaluate the proposed algorithms on two tasks, namely named entity recognition and
visual object localization in Sec. 7.

1.1 Related Work

Optimization for Structural Support Vector Machines Table 1 gives an overview of different opti-
mization algorithms designed for structural support vector machines. Early works [Taskar et al., 2004,
Tsochantaridis et al., 2004, Joachims et al., 2009, Teo et al., 2009] considered batch dual quadratic opti-
mization (QP) algorithms. The stochastic subgradient method operated directly on the non-smooth primal
formulation [Ratliff et al., 2007, Shalev-Shwartz et al., 2011]. More recently, Lacoste-Julien et al. [2013]
proposed a block coordinate Frank-Wolfe (BCFW) algorithm to optimize the dual formulation of structural

1https://github.com/krishnap25/casimir

2

Table 1: Convergence rates given in terms of the number of calls to various oracles for different optimization algo-
rithms on the learning problem (1) in case of structural support vector machines (4). The rates are speciﬁed in terms
of the target accuracy (cid:15), the number of training examples n, the regularization λ, the size of the label space |Y|, the
max feature norm R = maxi (cid:107)Φ(x(i), y) − Φ(x(i), y(i))(cid:107)2 and (cid:101)R ≥ R (see Remark 28 for explicit form). The rates
are speciﬁed up to constants and factors logarithmic in the problem parameters. The dependence on the initial error is
ignored. * denotes algorithms that make O(1) oracle calls per iteration.

Algo. (exp oracle)

# Oracle calls

Exponentiated
gradient*
[Collins et al., 2008]
Excessive gap
reduction
[Zhang et al., 2014]
Prop. 29*,
entropy smoother

Prop. 30*,
entropy smoother

(n + log |Y|)R2
λ(cid:15)

nR

(cid:114)

(cid:114)

log |Y|
λ(cid:15)

nR2 log|Y|
λ(cid:15)

n +

R2 log|Y|
λ(cid:15)

Algo. (max oracle)

# Oracle calls

BMRM
[Teo et al., 2009]

QP 1-slack
[Joachims et al., 2009]

Stochastic
subgradient*
[Shalev-Shwartz et al., 2011]
Block-Coordinate
Frank-Wolfe*
[Lacoste-Julien et al., 2013]

nR2
λ(cid:15)
nR2
λ(cid:15)

R2
λ(cid:15)

n +

R2
λ(cid:15)

Algo.
(top-K oracle)

# Oracle calls

Prop. 29*,
(cid:96)2
2 smoother

Prop. 30*,
(cid:96)2
2 smoother

(cid:114)

n (cid:101)R2
λ(cid:15)
n + (cid:101)R2
λ(cid:15)

support vector machines; see also Osokin et al. [2016] for variants and extensions. Saddle-point or primal-
dual approaches include the mirror-prox algorithm [Taskar et al., 2006, Cox et al., 2014, He and Harchaoui,
2015]. Palaniappan and Bach [2016] propose an incremental optimization algorithm for saddle-point prob-
lems. However, it is unclear how to extend it to the structured prediction problems considered here. Incre-
mental optimization algorithms for conditional random ﬁelds were proposed by Schmidt et al. [2015]. We
focus here on primal optimization algorithms in order to be able to train structured prediction models with
afﬁne or nonlinear mappings with a uniﬁed approach, and on incremental optimization algorithms which
can scale to large datasets.

Inference The ideas of dynamic programming inference in tree structured graphical models have been
around since the pioneering works of Pearl [1988] and Dawid [1992]. Other techniques emerged based on
graph cuts [Greig et al., 1989, Ishikawa and Geiger, 1998], bipartite matchings [Cheng et al., 1996, Taskar
et al., 2005] and search algorithms [Daum´e III and Marcu, 2005, Lampert et al., 2008, Lewis and Steedman,
2014, He et al., 2017]. For graphical models that admit no such a discrete structure, techniques based on
loopy belief propagation [McEliece et al., 1998, Murphy et al., 1999], linear programming (LP) [Schlesinger,
1976], dual decomposition [Johnson, 2008] and variational inference [Wainwright et al., 2005, Wainwright
and Jordan, 2008] gained popularity.

Top-K Inference Smooth inference oracles with (cid:96)2
2 smoothing echo older heuristics in speech and lan-
guage processing [Jurafsky et al., 2014]. Combinatorial algorithms for top-K inference have been studied
extensively by the graphical models community under the name “M -best MAP”. Seroussi and Golmard
[1994] and Nilsson [1998] ﬁrst considered the problem of ﬁnding the K most probable conﬁgurations in a
tree structured graphical model. Later, Yanover and Weiss [2004] presented the Best Max-Marginal First
algorithm which solves this problem with access only to an oracle that computes max-marginals. We also
use this algorithm in Sec. 4.2. Fromer and Globerson [2009] study top-K inference for LP relaxation, while
Batra [2012] considers the dual problem to exploit graph structure. Flerova et al. [2016] study top-K exten-
sions of the popular A(cid:63) and branch and bound search algorithms in the context of graphical models. Other

3

related approaches include diverse K-best solutions [Batra et al., 2012] and ﬁnding K-most probable modes
[Chen et al., 2013].

Smoothing Inference Smoothing for inference was used to speed up iterative algorithms for continu-
ous relaxations. Johnson [2008] considered smoothing dual decomposition inference using the entropy
smoother, followed by Jojic et al. [2010] and Savchynskyy et al. [2011] who studied its theoretical prop-
erties. Meshi et al. [2012] expand on this study to include (cid:96)2
2 smoothing. Explicitly smoothing discrete
inference algorithms in order to smooth the learning problem was considered by Zhang et al. [2014] and
Song et al. [2014] using the entropy and (cid:96)2
2 smoother was also used by Mar-
tins and Astudillo [2016]. Hazan et al. [2016] consider the approach of blending learning and inference,
instead of using inference algorithms as black-box procedures.

2 smoothers respectively. The (cid:96)2

Related ideas to ours appear in the independent works [Mensch and Blondel, 2018, Niculae et al., 2018].
These works partially overlap with ours, but the papers choose different perspectives, making them com-
plementary to each other. Mensch and Blondel [2018] proceed differently when, e.g., smoothing inference
based on dynamic programming. Moreover, they do not establish complexity bounds for optimization al-
gorithms making calls to the resulting smooth inference oracles. We deﬁne smooth inference oracles in the
context of black-box ﬁrst-order optimization and establish worst-case complexity bounds for incremental
optimization algorithms making calls to these oracles. Indeed we relate the amount of smoothing controlled
by µ to the resulting complexity of the optimization algorithms relying on smooth inference oracles.

End-to-end Training of Structured Prediction The general framework for global training of structured
prediction models was introduced by Bottou and Gallinari [1990] and applied to handwriting recognition
by Bengio et al. [1995] and to document processing by Bottou et al. [1997]. This approach, now called
“deep structured prediction”, was used, e.g., by Collobert et al. [2011] and Belanger and McCallum [2016].

1.2 Notation

Vectors are denoted by bold lowercase characters as w ∈ Rd while matrices are denoted by bold uppercase
characters as A ∈ Rd×n. For a matrix A ∈ Rm×n, deﬁne the norm for α, β ∈ {1, 2, ∞},

For any function f : Rd → R ∪ {+∞}, its convex conjugate f ∗ : Rd → R ∪ {+∞} is deﬁned as

(cid:107)A(cid:107)β,α = max{(cid:104)y, Ax(cid:105) | (cid:107)y(cid:107)α ≤ 1 , (cid:107)x(cid:107)β ≤ 1} .

(2)

f ∗(z) = sup
w∈Rd

{(cid:104)z, w(cid:105) − f (w)} .

A function f : Rd → R is said to be L-smooth with respect to an arbitrary norm (cid:107)·(cid:107) if it is continuously
differentiable and its gradient ∇f is L-Lipschitz with respect to (cid:107)·(cid:107). When left unspeciﬁed, (cid:107)·(cid:107) refers to
(cid:107) · (cid:107)2. Given a continuously differentiable map g : Rd → Rm, its Jacobian ∇g(w) ∈ Rm×d at w ∈ Rd is
deﬁned so that its ijth entry is [∇g(w)]ij = ∂gi(w)/wj where gi is the ith element of g and wj is the jth
element of w. The vector valued function g : Rd → Rm is said to be L-smooth with respect to (cid:107)·(cid:107) if it is
continuously differentiable and its Jacobian ∇g is L-Lipschitz with respect to (cid:107)·(cid:107).

For a vector z ∈ Rm, z(1) ≥ · · · ≥ z(m) refer to its components enumerated in non-increasing order
where ties are broken arbitrarily. Further, we let z[k] = (z(1), · · · , z(k)) ∈ Rk denote the vector of the k
largest components of z. We denote by ∆m−1 the standard probability simplex in Rm. When the dimension
is clear from the context, we shall simply denote it by ∆. Moreover, for a positive integer p, [p] refers to the
set {1, . . . , p}. Lastly, (cid:101)O in the big-O notation hides factors logarithmic in problem parameters.

4

2 Smooth Structured Prediction

Structured prediction aims to search for score functions φ parameterized by w ∈ Rd that model the compat-
ibility of input x ∈ X and output y ∈ Y as φ(x, y; w) through a graphical model. Given a score function
φ(·, ·; w), predictions are made using an inference procedure which, when given an input x, produces the
best output

y∗(x; w) ∈ arg max

φ(x, y; w) .

y∈Y

(3)

We shall return to the score functions and the inference procedures in Sec. 3. First, given such a score
function φ, we deﬁne the structural hinge loss and describe how it can be smoothed.

2.1 Structural Hinge Loss

On a given input-output pair (x, y), the error of prediction of y by the inference procedure with a score func-
tion φ(·, ·; w), is measured by a task loss (cid:96)(cid:0)y, y∗(x; w)(cid:1) such as the Hamming loss. The learning procedure
would then aim to ﬁnd the best parameter w that minimizes the loss on a given dataset of input-output train-
ing examples. However, the resulting problem is piecewise constant and hard to optimize. Instead, Altun
et al. [2003], Taskar et al. [2004], Tsochantaridis et al. [2004] propose to minimize a majorizing surrogate
of the task loss, called the structural hinge loss deﬁned on an input-output pair (x(i), y(i)) as

f (i)(w) = max
y∈Y

(cid:110)

φ(x(i), y; w) + (cid:96)(y(i), y)

(cid:111)

− φ(x(i), y(i); w) = max
y∈Y

ψ(i)(y, w) .

(4)

where ψ(i)(y; w) = φ(x(i), y; w) + (cid:96)(y(i), y) − φ(x(i), y(i); w) is the augmented score function.

This approach, known as max-margin structured prediction, builds upon binary and multi-class support
vector machines [Crammer and Singer, 2001], where the term (cid:96)(y(i), y) inside the maximization in (4)
generalizes the notion of margin. The task loss (cid:96) is assumed to possess appropriate structure so that the
maximization inside (4), known as loss augmented inference, is no harder than the inference problem in (3).
When considering a ﬁxed input-output pair (x(i), y(i)), we drop the index with respect to the sample i and
consider the structural hinge loss as

f (w) = max
y∈Y

ψ(y; w),

(5)

When the map w (cid:55)→ ψ(y; w) is afﬁne, the structural hinge loss f and the objective F from (1) are both
convex - we refer to this case as the structural support vector machine. When w (cid:55)→ ψ(y; w) is a nonlinear
but smooth map, then the structural hinge loss f and the objective F are nonconvex.

2.2 Smoothing Strategy

A convex, non-smooth function h can be smoothed by taking its inﬁmal convolution with a smooth func-
tion [Beck and Teboulle, 2012]. We now recall its dual representation, which Nesterov [2005b] ﬁrst used to
relate the amount of smoothing to optimal complexity bounds.

Deﬁnition 1. For a given convex function h : Rm → R, a smoothing function ω : dom h∗ → R which is
1-strongly convex with respect to (cid:107) · (cid:107)α (for α ∈ {1, 2}), and a parameter µ > 0, deﬁne

hµω(z) = max

u∈dom h∗

{(cid:104)u, z(cid:105) − h∗(u) − µω(u)} .

as the smoothing of h by µω.

5

We now state a classical result showing how the parameter µ controls both the approximation error and the
level of the smoothing. For a proof, see Beck and Teboulle [2012, Thm. 4.1, Lemma 4.2] or Prop. 39 of
Appendix A.

Proposition 2. Consider the setting of Def. 1. The smoothing hµω is continuously differentiable and its
gradient, given by

∇hµω(z) = arg max
u∈dom h∗

{(cid:104)u, z(cid:105) − h∗(u) − µω(u)}

is 1/µ-Lipschitz with respect to (cid:107) · (cid:107)∗
µ1 ≥ µ2 ≥ 0,

α. Moreover, letting hµω ≡ h for µ = 0, the smoothing satisﬁes, for all

(µ1 − µ2)

inf
u∈dom h∗

ω(u) ≤ hµ2ω(z) − hµ1ω(z) ≤ (µ1 − µ2)

sup
u∈dom h∗

ω(u) .

Smoothing the Structural Hinge Loss We rewrite the structural hinge loss as a composition

(cid:40)Rd → Rm

g :

(cid:40)Rm → R

h :

w (cid:55)→ (ψ(y; w))y∈Y ,

z

(cid:55)→ maxi∈[m] zi,

where m = |Y| so that the structural hinge loss reads

(6)

(7)

We smooth the structural hinge loss (7) by simply smoothing the non-smooth max function h as

f (w) = h ◦ g(w) .

fµω = hµω ◦ g.

When g is smooth and Lipschitz continuous, fµω is a smooth approximation of the structural hinge loss,
whose gradient is readily given by the chain-rule. In particular, when g is an afﬁne map g(w) = Aw + b, if
follows that fµω is ((cid:107)A(cid:107)2
β,α/µ)-smooth with respect to (cid:107) · (cid:107)β (cf. Lemma 40 in Appendix A). Furthermore,
for µ1 ≥ µ2 ≥ 0, we have,

(µ1 − µ2) min

ω(u) ≤ fµ2ω(w) − fµ1ω(w) ≤ (µ1 − µ2) max

ω(u) .

u∈∆m−1

u∈∆m−1

In the context of smoothing the max function, we now describe two popular choices for the smoothing
function ω, followed by computational considerations.

2.3 Smoothing Variants

2.3.1 Entropy and (cid:96)2

2 smoothing

When h is the max function, the smoothing operation can be computed analytically for the entropy smoother
and the (cid:96)2

2 smoother, denoted respectively as

−H(u) := (cid:104)u, log u(cid:105)

and

(cid:96)2
2(u) := 1

2 ((cid:107)u(cid:107)2

2 − 1) .

These lead respectively to the log-sum-exp function [Nesterov, 2005b, Lemma 4]

h−µH (z) = µ log

ezi/µ

, ∇h−µH (z) =

(cid:33)

(cid:32) m
(cid:88)

i=1

(cid:35)

(cid:34)

ezi/µ
j=1 ezj /µ

(cid:80)m

,

i=1,...,m

6

and an orthogonal projection onto the simplex,

hµ(cid:96)2

2

(z) = (cid:104)z, proj∆m−1(z/µ)(cid:105) − µ

2 (cid:107) proj∆m−1(z/µ)(cid:107)2 + µ

2 , ∇hµ(cid:96)2

2

(z) = proj∆m−1(z/µ) .

Furthermore, the following holds for all µ1 ≥ µ2 ≥ 0 from Prop. 2:

0 ≤ h−µ1H (z) − h−µ2H (z) ≤ (µ1 − µ2) log m,

and, 0 ≤ hµ1(cid:96)2

(z) − hµ2(cid:96)2

2

2

(z) ≤ 1

2 (µ1 − µ2) .

2.3.2 Top-K Strategy

Though the gradient of the composition fµω = hµω ◦ g can be written using the chain rule, its actual
computation for structured prediction problems involves computing ∇g over all m = |Y| of its components,
which may be intractable. However, in the case of (cid:96)2
2 smoothing, projections onto the simplex are sparse, as
pointed out by the following proposition.

Proposition 3. Consider the Euclidean projection u∗ = arg minu∈∆m−1 (cid:107)u − z/µ(cid:107)2
the simplex, where µ > 0. The projection u∗ has exactly k ∈ [m] non-zeros if and only if

2 of z/µ ∈ Rm onto

where z(1) ≥ · · · ≥ z(m) are the components of z in non-decreasing order and z(m+1) := −∞. In this case,
u∗ is given by

k
(cid:88)

i=1

(cid:0)z(i) − z(k)

(cid:1) < µ ≤

(cid:0)z(i) − z(k+1)

(cid:1) ,

k
(cid:88)

i=1

(cid:26)

u∗
i = max

0, 1
kµ

(cid:0)zi − z(j)

(cid:1) + 1
k

(cid:27)

.

k
(cid:88)

j=1

m
(cid:88)

i=1

(cid:18) zi
µ

+ ρ

= 1 ,

(cid:19)

+

Proof. The projection u∗ satisﬁes u∗

i = (zi/µ + ρ∗)+, where ρ∗ is the unique solution of ρ in the equation

where α+ = max{0, α}. See, e.g., Held et al. [1974] for a proof of this fact. Note that z(i)/µ + ρ∗ ≤ 0
implies that z(j)/µ + ρ∗ ≤ 0 for all j ≥ i. Therefore u∗ has k non-zeros if and only if z(k)/µ + ρ∗ > 0 and
z(k+1)/µ + ρ∗ ≤ 0.

Now suppose that u∗ has exactly k non-zeros, we can then solve (9) to obtain ρ∗ = ϕk(z/µ), which is

deﬁned as

ϕk

(cid:19)

(cid:18) z
µ

:=

−

1
k

1
k

k
(cid:88)

i=1

z(i)
µ

.

Plugging in the value of ρ∗ in z(k)/µ + ρ∗ > 0 gives µ > (cid:80)k
(cid:1).
gives µ ≤ (cid:80)k

(cid:0)z(i) − z(k+1)

i=1

i=1

(cid:0)z(i) − z(k)

(cid:1). Likewise, z(k+1)/µ + ρ∗ ≤ 0

Conversely assume (8) and let (cid:98)ρ = ϕk(z/µ). Eq. (8) can be written as z(k)/µ+(cid:98)ρ > 0 and z(k+1)/µ+(cid:98)ρ ≤
0. Furthermore, we verify that (cid:98)ρ satisﬁes Eq. (9), and so (cid:98)ρ = ρ∗ is its unique root. It follows, therefore, that
the sparsity of u∗ is k.

7

(8)

(9)

(10)

Thus, the projection of z/µ onto the simplex picks out some number Kz/µ of the largest entries of z/µ
- we refer to this as the sparsity of proj∆m−1(z/µ). This fact motivates the top-K strategy: given µ > 0, ﬁx
an integer K a priori and consider as surrogates for hµ(cid:96)2

and ∇hµ(cid:96)2

respectively

2

2

hµ,K(z) := max

u∈∆K−1

(cid:8)(cid:10)z[K], u(cid:11) − µ(cid:96)2

2(u)(cid:9) ,

and,

(cid:101)∇hµ,K(z) := ΩK(z)(cid:62) proj∆K−1

(cid:19)

(cid:18) z[K]
µ

,

where z[K] denotes the vector composed of the K largest entries of z and ΩK : Rm → {0, 1}K×m deﬁnes
their extraction, i.e., ΩK(z) = (e(cid:62)
)(cid:62) ∈ {0, 1}K×m where j1, · · · , jK satisfy zj1 ≥ · · · ≥ zjK
j1
such that z[K] = ΩK(z)z . A surrogate of the (cid:96)2

2 smoothing is then given by

, . . . , e(cid:62)
jK

fµ,K := hµ,K ◦ g ,

and,

(cid:101)∇fµ,K(w) := ∇g(w)(cid:62) (cid:101)∇hµ,K(g(w)) .

(11)

Exactness of Top-K Strategy We say that the top-K strategy is exact at z for µ > 0 when it recovers
(z) = (cid:101)∇hµ,K(z). The next
the ﬁrst order information of hµ(cid:96)2
proposition outlines when this is the case. Note that if the top-K strategy is exact at z for a smoothing
parameter µ > 0 then it will be exact at z for any µ(cid:48) < µ.

(z) = hµ,K(z) and ∇hµ(cid:96)2

, i.e. when hµ(cid:96)2

2

2

2

Proposition 4. The top-K strategy is exact at z for µ > 0 if

µ ≤

(cid:0)z(i) − z(K+1)

(cid:1) .

K
(cid:88)

i=1

(12)

Moreover, for any ﬁxed z ∈ Rm such that the vector z[K+1] = ΩK+1(z)z has at least two unique elements,
the top-K strategy is exact at z for all µ satisfying 0 < µ ≤ z(1) − z(K+1).

Proof. First, we note that the top-K strategy is exact when the sparsity Kz/µ of the projection proj∆m−1(z/µ)
satisﬁes Kz/µ ≤ K. From Prop. 3, the condition that Kz/µ ∈ {1, 2, · · · , K} happens when

µ ∈

K
(cid:91)

(cid:32) k

(cid:88)

k=1

i=1

(cid:0)z(i) − z(k)

(cid:1) ,

(cid:0)z(i) − z(k+1)

(cid:1)

k
(cid:88)

i=1

(cid:35)

(cid:32)

=

0,

K
(cid:88)

i=1

(cid:35)

(cid:0)z(i) − z(K+1)

(cid:1)

,

since the intervals in the union are contiguous. This establishes (12).

The only case when (12) cannot hold for any value of µ > 0 is when the right hand size of (12) is zero.
In the opposite case when z[K+1] has at least two unique components, or equivalently, z(1) − z(K+1) > 0,
the condition 0 < µ ≤ z(1) − z(K+1) implies (12).

If the top-K strategy is exact at g(w) for µ, then

fµ,K(w) = fµ(cid:96)2

(w)

2

and

(cid:101)∇fµ,K(w) = ∇fµ(cid:96)2

(w) ,

2

where the latter follows from the chain rule. When used instead of (cid:96)2
2 smoothing in the algorithms presented
in Sec. 5, the top-K strategy provides a computationally efﬁcient heuristic to smooth the structural hinge
loss. Though we do not have theoretical guarantees using this surrogate, experiments presented in Sec. 7
show its efﬁciency and its robustness to the choice of K.

8

This section studies ﬁrst order oracles used in standard and smoothed structured prediction. We ﬁrst describe
the parameterization of the score functions through graphical models.

3 Inference Oracles

3.1 Score Functions

Structured prediction is deﬁned by the structure of the output y, while input x ∈ X can be arbitrary.
Each output y ∈ Y is composed of p components y1, . . . , yp that are linked through a graphical model
G = (V, E) - the nodes V = {1, · · · , p} represent the components of the output y while the edges E deﬁne
the dependencies between various components. The value of each component yv for v ∈ V represents the
state of the node v and takes values from a ﬁnite set Yv. The set of all output structures Y = Y1 × · · · × Yp
is then ﬁnite yet potentially intractably large.

The structure of the graph (i.e., its edge structure) depends on the task. For the task of sequence labeling,
the graph is a chain, while for the task of parsing, the graph is a tree. On the other hand, the graph used in
image segmentation is a grid.

For a given input x and a score function φ(·, ·; w), the value φ(x, y; w) measures the compatibility of
the output y for the input x. The essential characteristic of the score function is that it decomposes over the
nodes and edges of the graph as

φ(x, y; w) =

φv(x, yv; w) +

φv,v(cid:48)(x, yv, yv(cid:48); w) .

(13)

(cid:88)

v∈V

(cid:88)

(v,v(cid:48))∈E

For a ﬁxed w, each input x deﬁnes a speciﬁc compatibility function φ(x, · ; w). The nature of the
problem and the optimization algorithms we consider hinge upon whether φ is an afﬁne function of w or
not. The two settings studied here are the following:

Pre-deﬁned Feature Map. In this structured prediction framework, a pre-speciﬁed feature map Φ : X ×

Y → Rd is employed and the score φ is then deﬁned as the linear function

φ(x, y; w) = (cid:104)Φ(x, y), w(cid:105) =

(cid:104)Φv(x, yv), w(cid:105) +

(cid:104)Φv,v(cid:48)(x, yv, yv(cid:48)), w(cid:105) .

(14)

(cid:88)

v∈V

(cid:88)

(v,v(cid:48))∈E

Learning the Feature Map. We also consider the setting where the feature map Φ is parameterized by
w0, for example, using a neural network, and is learned from the data. The score function can then be
written as

φ(x, y; w) = (cid:104)Φ(x, y; w0), w1(cid:105)

(15)

where w = (w0, w1) and the scalar product decomposes into nodes and edges as above.

Note that we only need the decomposition of the score function over nodes and edges of the G as in Eq. (13).
In particular, while Eq. (15) is helpful to understand the use of neural networks in structured prediction, the
optimization algorithms developed in Sec. 6 apply to general nonlinear but smooth score functions.

This framework captures both generative probabilistic models such as Hidden Markov Models (HMMs)
that model the joint distribution between x and y as well as discriminative probabilistic models, such as
conditional random ﬁelds [Lafferty et al., 2001] where dependencies among the input variables x do not
need to be explicitly represented. In these cases, the log joint and conditional probabilities respectively play
the role of the score φ.

9

Example 5 (Sequence Tagging). Consider the task of sequence tagging in natural language processing
where each x = (x1, · · · , xp) ∈ X is a sequence of words and y = (y1, · · · , yp) ∈ Y is a sequence of
labels, both of length p. Common examples include part of speech tagging and named entity recognition.
Each word xv in the sequence x comes from a ﬁnite dictionary D, and each tag yv in y takes values from a
ﬁnite set Yv = Ytag. The corresponding graph is simply a linear chain.

The score function measures the compatibility of a sequence y ∈ Y for the input x ∈ X using parame-

ters w = (wunary, wpair) as, for instance,

φ(x, y; w) =

(cid:104)Φunary(xv, yv), wunary(cid:105) +

(cid:104)Φpair(yv, yv+1), wpair(cid:105) ,

p
(cid:88)

v=1

p
(cid:88)

v=0

where, using wunary ∈ R|D||Ytag| and wpair ∈ R|Ytag|2
each v ∈ [p],

as node and edge weights respectively, we deﬁne for

(cid:104)Φunary(xv, yv), wunary(cid:105) =

wunary, x,j I(x = xv) I(j = yv) .

(cid:88)

x∈D, j∈Ytag

The pairwise term (cid:104)Φpair(yv, yv+1), wpair(cid:105) is analogously deﬁned. Here, y0, yp+1 are special “start” and
“stop” symbols respectively. This can be written as a dot product of w with a pre-speciﬁed feature map as
in (14), by deﬁning

Φ(x, y) = (cid:0)

exv ⊗ eyv

(cid:1) ⊕ (cid:0)

eyv ⊗ eyv+1

(cid:1) ,

p
(cid:88)

v=1

p
(cid:88)

v=0

where exv is the unit vector (I(x = xv))x∈D ∈ R|D|, eyv is the unit vector (I(j = yv))j∈Ytag ∈ R|Ytag|, ⊗
denotes the Kronecker product between vectors and ⊕ denotes vector concatenation.

3.2

Inference Oracles

We deﬁne now inference oracles as ﬁrst order oracles in structured prediction. These are used later to
understand the information-based complexity of optimization algorithms.

3.2.1 First Order Oracles in Structured Prediction

A ﬁrst order oracle for a function f : Rd → R is a routine which, given a point w ∈ Rd, returns on output a
value f (w) and a (sub)gradient v ∈ ∂f (w), where ∂f is the Fr´echet (or regular) subdifferential [Rockafellar
and Wets, 2009, Def. 8.3]. We now deﬁne inference oracles as ﬁrst order oracles for the structural hinge
loss f and its smoothed variants fµω. Note that these deﬁnitions are independent of the graphical structure.
However, as we shall see, the graphical structure plays a crucial role in the implementation of the inference
oracles.

Deﬁnition 6. Consider an augmented score function ψ, a level of smoothing µ > 0 and the structural hinge
loss f (w) = maxy∈Y ψ(y; w). For a given w ∈ Rd,

(i)

the max oracle returns f (w) and v ∈ ∂f (w).

(ii)

the exp oracle returns f−µH (w) and ∇f−µH (w).

(iii) the top-K oracle returns fµ,K(w) and (cid:101)∇fµ,K(w) as surrogates for fµ(cid:96)2

(w) and ∇fµ(cid:96)2

(w) respec-

2

2

tively.

10

(a) Non-smooth.

(b) (cid:96)2

2 smoothing.

(c) Entropy smoothing.

Figure 1: Viterbi trellis for a chain graph with p = 4 nodes and 3 labels.

Note that the exp oracle gets its name since it can be written as an expectation over all y, as revealed by the
next lemma, which gives analytical expressions for the gradients returned by the oracles.

Lemma 7. Consider the setting of Def. 6. We have the following:

(i) For any y∗ ∈ arg maxy∈Y ψ(y; w), we have that ∇wψ(y∗; w) ∈ ∂f (w). That is, the max oracle can

be implemented by inference.

(ii) The output of the exp oracle satisﬁes ∇f−µH (w) = (cid:80)

y∈Y Pψ,µ(y; w)∇ψ(y; w), where

Pψ,µ(y; w) =

exp

(cid:16) 1

(cid:17)
µ ψ(y; w)

(cid:80)

y(cid:48)∈Y exp

µ ψ(y(cid:48); w)

(cid:16) 1

(cid:17) .

(iii) The output of the top-K oracle satisﬁes (cid:101)∇fµ,K(w) = (cid:80)K

i=1 u∗
(cid:9) is the set of K largest scoring outputs satisfying

(cid:8)y(1), · · · , y(K)

ψ,µ,i(w)∇ψ(y(i); w) , where YK =

ψ(y(1); w) ≥ · · · ≥ ψ(y(K); w) ≥ max
y∈Y\YK

ψ(y; w) ,

and u∗

ψ,µ = proj∆K−1

(cid:16)(cid:2)ψ(y(1); w), · · · , ψ(y(K); w)(cid:3)(cid:62)(cid:17)

.

Proof. Part (ii) deals with the composition of differentiable functions, and follows from the chain rule.
Part (iii) follows from the deﬁnition in Eq. (11). The proof of Part (i) follows from the chain rule for Fr´echet
subdifferentials of compositions [Rockafellar and Wets, 2009, Theorem 10.6] together with the fact that
by convexity and Danskin’s theorem [Bertsekas, 1999, Proposition B.25], the subdifferential of the max
function is given by ∂h(z) = conv{ei | i ∈ [m] such that zi = h(z)}.

Example 8. Consider the task of sequence tagging from Example 5. The inference problem (3) is a search
over all |Y| = |Ytag|p label sequences. For chain graphs, this is equivalent to searching for the shortest
path in the associated trellis, shown in Fig. 1. An efﬁcient dynamic programming approach called the
Viterbi algorithm [Viterbi, 1967] can solve this problem in space and time polynomial in p and |Ytag|. The
structural hinge loss is non-smooth because a small change in w might lead to a radical change in the best
scoring path shown in Fig. 1.

11

When smoothing f with ω = (cid:96)2

is given by a projection onto the simplex,
which picks out some number Kψ/µ of the highest scoring outputs y ∈ Y or equivalently, Kψ/µ shortest
paths in the Viterbi trellis (Fig. 1b). The top-K oracle then uses the top-K strategy to approximate fµ(cid:96)2
with
fµ,K.

2, the smoothed function fµ(cid:96)2

2

2

On the other hand, with entropy smoothing ω = −H, we get the log-sum-exp function and its gra-
dient is obtained by averaging over paths with weights such that shorter paths have a larger weight (cf.
Lemma 7(ii)). This is visualized in Fig. 1c.

3.2.2 Exp Oracles and Conditional Random Fields

Recall that a Conditional Random Field (CRF) [Lafferty et al., 2001] with augmented score function ψ and
parameters w ∈ Rd is a probabilistic model that assigns to output y ∈ Y the probability

P(y | ψ; w) = exp (ψ(y; w) − Aψ(w)) ,

(16)

where Aψ(w) is known as the log-partition function, a normalizer so that the probabilities sum to one.
Gradient-based maximum likelihood learning algorithms for CRFs require computation of the log-partition
function Aψ(w) and its gradient ∇Aψ(w). Next proposition relates the computational costs of the exp
oracle and the log-partition function.
Proposition 9. The exp oracle for an augmented score function ψ with parameters w ∈ Rd is equivalent
in hardness to computing the log-partition function Aψ(w) and its gradient ∇Aψ(w) for a conditional
random ﬁeld with augmented score function ψ.

Proof. Fix a smoothing parameter µ > 0. Consider a CRF with augmented score function ψ(cid:48)(y; w) =
y∈Y exp (cid:0)µ−1ψ(y; w)(cid:1). The
µ−1ψ(y; w). Its log-partition function Aψ(cid:48)(w) satisﬁes exp(Aψ(cid:48)(w)) = (cid:80)
claim now follows from the bijection f−µH (w) = µ Aψ(cid:48)(w) between f−µH and Aψ(cid:48).

4 Implementation of Inference Oracles

We now turn to the concrete implementation of the inference oracles. This depends crucially on the structure
of the graph G = (V, E). If the graph G is a tree, then the inference oracles can be computed exactly with
efﬁcient procedures, as we shall see in in the Sec. 4.1. When the graph G is not a tree, we study special cases
when speciﬁc discrete structure can be exploited to efﬁciently implement some of the inference oracles in
Sec. 4.2. The results of this section are summarized in Table 2.

Throughout this section, we ﬁx an input-output pair (x(i), y(i)) and consider the augmented score func-
tion ψ(y; w) = φ(x(i), y; w) + (cid:96)(y(i), y) − φ(x(i), y(i); w) it deﬁnes, where the index of the sample is
dropped by convenience. From (13) and the decomposability of the loss, we get that ψ decomposes along
nodes V and edges E of G as:

ψ(y; w) =

ψv(yv; w) +

ψv,v(cid:48)(yv, yv(cid:48); w) .

(17)

(cid:88)

v∈V

(cid:88)

(v,v(cid:48))∈E

When w is clear from the context, we denote ψ(· ; w) by ψ. Likewise for ψv and ψv,v(cid:48).

4.1

Inference Oracles in Trees

We ﬁrst consider algorithms implementing the inference algorithms in trees and examine their computational
complexity.

12

Table 2: Smooth inference oracles, algorithms and complexity. Here, p is the size of each y ∈ Y. The time
complexity is phrased in terms of the time complexity T of the max oracle.

Max oracle
Algo

Max-product

Graph cut

Graph matching

Branch and
Bound search

Algo
Top-K
max-product

BMMF

BMMF

Top-K oracle

Time

Exp oracle
Algo

Time

O(KT log K)

Sum-Product

O(T )

O(pKT )

O(KT )

Intractable

Intractable

Intractable

Top-K search

N/A

4.1.1

Implementation of Inference Oracles

Max Oracle
In tree structured graphical models, the inference problem (3), and thus the max oracle (cf.
Lemma 7(i)) can always be solved exactly in polynomial time by the max-product algorithm [Pearl, 1988],
which uses the technique of dynamic programming [Bellman, 1957]. The Viterbi algorithm (Algo. 1) for
chain graphs from Example 8 is a special case. See Algo. 7 in Appendix B for the max-product algorithm
in full generality.

Top-K Oracle The top-K oracle uses a generalization of the max-product algorithm that we name top-K
max-product algorithm. Following the work of Seroussi and Golmard [1994], it keeps track of the K-best
intermediate structures while the max-product algorithm just tracks the single best intermediate structure.
Formally, the kth largest element from a discrete set S is deﬁned as

(cid:40)

max(k)
x∈S

f (x) =

kth largest element of {f (y) | y ∈ S} k ≤ |S|
k > |S| .
−∞,

We present the algorithm in the simple case of chain structured graphical models in Algo. 2. The top-K
max-product algorithm for general trees is given in Algo. 8 in Appendix B. Note that it requires (cid:101)O(K) times
the time and space of the max oracle.

Exp oracle The relationship of the exp oracle with CRFs (Prop. 9) leads directly to Algo. 3, which is
based on marginal computations from the sum-product algorithm.

Remark 10. We note that clique trees allow the generalization of the algorithms of this section to general
graphs with cycles. However, the construction of a clique tree requires time and space exponential in the
treewidth of the graph.

Example 11. Consider the task of sequence tagging from Example 5. The Viterbi algorithm (Algo. 1)
maintains a table πv(yv), which stores the best length-v preﬁx ending in label yv. One the other hand, the
top-K Viterbi algorithm (Algo. 2) must store in π(k)
v (yv) the score of kth best length-v preﬁx that ends in
yv for each k ∈ [K]. In the vanilla Viterbi algorithm, the entry πv(yv) is updated by looking the previous
column πv−1 following (18). Compare this to update (19) of the top-K Viterbi algorithm. In this case,
the exp oracle is implemented by the forward-backward algorithm, a specialization of the sum-product
algorithm to chain graphs.

13

Algorithm 1 Max-product (Viterbi) algorithm for chain graphs

1: Input: Augmented score function ψ(·, ·; w) deﬁned on a chain graph G.
2: Set π1(y1) ← ψ1(y1) for all y1 ∈ Y1.
3: for v = 2, · · · p do
4:

For all yv ∈ Yv, set

πv(yv) ← ψv(yv) + max

{πv−1(yv−1) + ψv,v−1(yv, yv−1)} .

(18)

yv−1∈Yv−1

Assign to δv(yv) the yv−1 that attains the max above for each yv ∈ Yv.

5:
6: end for
7: Set ψ∗ ← maxyp∈Yp πp(yp) and store the maximizing assignments of yp in y∗
p.
8: for v = p − 1, · · · , 1 do
Set y∗
v ← δv+1(yv+1).
9:
10: end for
11: return ψ∗, y∗ := (y∗

1, · · · , y∗

p).

4.1.2 Complexity of Inference Oracles

The next proposition presents the correctness guarantee and complexity of each of the aforementioned algo-
rithms. Its proof has been placed in Appendix B.

Proposition 12. Consider as inputs an augmented score function ψ(·, ·; w) deﬁned on a tree structured
graph G, an integer K > 0 and a smoothing parameter µ > 0.

(i) The output (ψ∗, y∗) of the max-product algorithm (Algo. 1 for the special case when G is chain struc-
tured Algo. 7 from Appendix B in general) satisﬁes ψ∗ = ψ(y∗; w) = maxy∈Y ψ(y; w). Thus, the pair
(cid:0)ψ∗, ∇ψ(y∗; w)(cid:1) is a correct implementation of the max oracle. It requires time O(p maxv∈V |Yv|2)
and space O(p maxv∈V |Yv|).

(ii) The output {ψ(k), y(k)}K

k=1 of the top-K max-product algorithm (Algo. 2 for the special case when G
is chain structured or Algo. 8 from Appendix B in general) satisﬁes ψ(k) = ψ(y(k)) = max(k)
y∈Y ψ(y).
Thus, the top-K max-product algorithm followed by a projection onto the simplex (Algo. 6 in Ap-
pendix A) is a correct implementation of the top-K oracle. It requires time O(pK log K maxv∈V |Yv|2)
and space O(pK maxv∈V |Yv|).

(iii) Algo. 3 returns (cid:0)f−µH (w), ∇f−µH (w)(cid:1). Thus, Algo. 3 is a correct implementation of the exp oracle.

It requires time O(p maxv∈V |Yv|2) and space O(p maxv∈V |Yv|).

4.2

Inference Oracles in Loopy Graphs

For general loopy graphs with high tree-width, the inference problem (3) is NP-hard [Cooper, 1990]. In
particular cases, graph cut, matching or search algorithms can be used for exact inference in dense loopy
graphs, and therefore, to implement the max oracle as well (cf. Lemma 7(i)). In each of these cases, we ﬁnd
that the top-K oracle can be implemented, but the exp oracle is intractable. Appendix C contains a review
of the algorithms and guarantees referenced in this section.

14

Algorithm 2 Top-K max-product (top-K Viterbi) algorithm for chain graphs

1: Input: Augmented score function ψ(·, ·; w) deﬁned on chain graph G, integer K > 0.
2: For k = 1, · · · , K, set π(k)
1 (y1) ← ψ1(y1) if k = 1 and −∞ otherwise for all y1 ∈ Y1.
3: for v = 2, · · · p and k = 1, · · · , K do
4:

For all yv ∈ Yv, set

v (yv) ← ψv(yv) + max(k)
π(k)

yv−1∈Yv−1,(cid:96)∈[K]

(cid:110)

π((cid:96))
v−1(yv−1) + ψv,v−1(yv, yv−1)

(cid:111)

.

(19)

v (yv) the yv−1, (cid:96) that attain the max(k) above for each yv ∈ Yv.

yp∈Yp,k∈[K] π(k)

p (yp) and store in y(k)

p , (cid:96)(k) respectively the maxi-

Assign to δ(k)

v (yv), κ(k)

5:
6: end for
7: For k = 1, · · · , K, set ψ(k) ← max(k)

mizing assignments of yp, k.

8: for v = p − 1, · · · 1 and k = 1, · · · , K do

Set y(k)

v ← δ((cid:96)(k))

v+1

(cid:0)y(k)
v+1

(cid:1) and (cid:96)(k) ← κ((cid:96)(k))
v+1

(cid:0)y(k)
v+1

(cid:1).

9:
10: end for
11: return

(cid:110)

ψ(k), y(k) := (y(k)

1 , · · · , y(k)
p )

(cid:111)K

.

k=1

4.2.1

Inference Oracles using Max-Marginals

We now deﬁne a max-marginal, which is a constrained maximum of the augmented score ψ.

Deﬁnition 13. The max-marginal of ψ relative to a variable yv is deﬁned, for j ∈ Yv as

ψv;j(w) := max

ψ(y; w) .

y∈Y : yv=j

In cases where exact inference is tractable using graph cut or matching algorithms, it is possible to extract
max-marginals as well. This, as we shall see next, allows the implementation of the max and top-K oracles.
When the augmented score function ψ is unambiguous, i.e., no two distinct y1, y2 ∈ Y have the same
augmented score, the output y∗(w) is unique can be decoded from the max-marginals as (see Pearl [1988],
Dawid [1992] or Thm. 45 in Appendix C)

(20)

(21)

y∗
v(w) = arg max

ψv;j(w) .

j∈Yv

If one has access to an algorithm M that can compute max-marginals, the top-K oracle is also eas-
ily implemented via the Best Max-Marginal First (BMMF) algorithm of Yanover and Weiss [2004]. This
algorithm requires computations of 2K sets of max-marginals, where a set of max-marginals refers to max-
marginals for all yv in y. Therefore, the BMMF algorithm followed by a projection onto the simplex
(Algo. 6 in Appendix A) is a correct implementation of the top-K oracle at a computational cost of 2K sets
of max-marginals. The BMMF algorithm and its guarantee are recalled in Appendix C.1 for completeness.

Graph Cut and Matching Inference Kolmogorov and Zabin [2004] showed that submodular energy
functions [Lov´asz, 1983] over binary variables can be efﬁciently minimized exactly via a minimum cut
algorithm. For a class of alignment problems, e.g., Taskar et al. [2005], inference amounts to ﬁnding the
best bipartite matching. In both these cases, max-marginals can be computed exactly and efﬁciently by

15

Algorithm 3 Entropy smoothed max-product algorithm

1: Input: Augmented score function ψ(·, ·; w) deﬁned on tree structured graph G, µ > 0.
2: Compute the log-partition function and marginals using the sum-product algorithm (Algo. 9 in Ap-

pendix B)

Aψ/µ, {Pv for v ∈ V}, {Pv,v(cid:48) for (v, v(cid:48)) ∈ E} ← SUMPRODUCT

(cid:16) 1

µ ψ(· ; w), G

(cid:17)

.

3: Set f−µH (w) ← µAψ/µ and

∇f−µH (w) ←

Pv(yv)∇ψv(yv; w) +

Pv,v(cid:48)(yv, yv(cid:48))∇ψv,v(cid:48)(yv; w) .

(cid:88)

(cid:88)

v∈V

yv∈Yv

(cid:88)

(cid:88)

(cid:88)

(v,v(cid:48))∈E

yv∈Yv

yv(cid:48) ∈Yv(cid:48)

4: return f−µH (w), ∇f−µH (w).

combinatorial algorithms. This gives us a way to implement the max and top-K oracles. However, in
both settings, computing the log-partition function Aψ(w) of a CRF with score ψ is known to be #P-
complete [Jerrum and Sinclair, 1993]. Prop. 9 immediately extends this result to the exp oracle. This
discussion is summarized by the following proposition, whose proof is provided in Appendix C.4.

Proposition 14. Consider as inputs an augmented score function ψ(·, ·; w), an integer K > 0 and a smooth-
ing parameter µ > 0. Further, suppose that ψ is unambiguous, that is, ψ(y(cid:48); w) (cid:54)= ψ(y(cid:48)(cid:48); w) for all distinct
y(cid:48), y(cid:48)(cid:48) ∈ Y. Consider one of the two settings:

(A)

the output space Yv = {0, 1} for each v ∈ V, and the function −ψ is submodular (see Appendix C.2
and, in particular, (72) for the precise deﬁnition), or,

(B)

the augmented score corresponds to an alignment task where the inference problem (3) corresponds to
a maximum weight bipartite matching (see Appendix C.3 for a precise deﬁnition).

In these cases, we have the following:

(i) The max oracle can be implemented at a computational complexity of O(p) minimum cut computations

in Case (A), and in time O(p3) in Case (B).

(ii) The top-K oracle can be implemented at a computational complexity of O(pK) minimum cut compu-

tations in Case (A), and in time O(p3K) in Case (B).

(iii) The exp oracle is #P-complete in both cases.

Prop. 14 is loose in that the max oracle can be implemented with just one minimum cut computation

instead of p in in Case (A) [Kolmogorov and Zabin, 2004].

4.2.2 Branch and Bound Search

Max oracles implemented via search algorithms can often be extended to implement the top-K oracle. We
restrict our attention to best-ﬁrst branch and bound search such as the celebrated Efﬁcient Subwindow Search
[Lampert et al., 2008].

16

Branch and bound methods partition the search space into disjoint subsets, while keeping an upper
bound (cid:98)ψ : X × 2Y → R, on the maximal augmented score for each of the subsets (cid:98)Y ⊆ Y. Using a best-ﬁrst
strategy, promising parts of the search space are explored ﬁrst. Parts of the search space whose upper bound
indicates that they cannot contain the maximum do not have to be examined further.

The top-K oracle is implemented by simply continuing the search procedure until K outputs have been
produced - see Algo. 13 in Appendix C.5. Both the max oracle and the top-K oracle can degenerate to an
exhaustive search in the worst case, so we do not have sharp running time guarantees. However, we have
the following correctness guarantee.

Proposition 15. Consider an augmented score function ψ(·, ·; w), an integer K > 0 and a smoothing
parameter µ > 0. Suppose the upper bound function (cid:98)ψ(·, ·; w) : X × 2Y → R satisﬁes the following
properties:

(a) (cid:98)ψ( (cid:98)Y; w) is ﬁnite for every (cid:98)Y ⊆ Y,

(b) (cid:98)ψ( (cid:98)Y; w) ≥ maxy∈ (cid:98)Y ψ(y; w) for all (cid:98)Y ⊆ Y, and,

(c) (cid:98)ψ({y}; w) = ψ(y; w) for every y ∈ Y.

Then, we have the following:

(i) Algo. 13 with K = 1 is a correct implementation of the max oracle.

(ii) Algo. 13 followed by a projection onto the simplex (Algo. 6 in Appendix A) is a correct implementation

of the top-K oracle.

See Appendix C.5 for a proof. The discrete structure that allows inference via branch and bound search
cannot be leveraged to implement the exp oracle.

5 The Casimir Algorithm

We come back to the optimization problem (1) with f (i) deﬁned in (7). We assume in this section that the
mappings g(i) deﬁned in (6) are afﬁne. Problem (1) now reads

(cid:34)

min
w∈Rd

F (w) :=

1
n

n
(cid:88)

i=1

h(A(i)w + b(i)) +

(cid:107)w(cid:107)2
2

.

(cid:35)

λ
2

For a single input (n = 1), the problem reads

min
w∈Rd

h(Aw + b) +

λ
2

(cid:107)w(cid:107)2
2.

(22)

(23)

where h is a simple non-smooth convex function and λ ≥ 0. Nesterov [2005b,a] ﬁrst analyzed such set-
ting: while the problem suffers from its non-smoothness, fast methods can be developed by considering
smooth approximations of the objectives. We combine this idea with the Catalyst acceleration scheme [Lin
et al., 2018] to accelerate a linearly convergent smooth optimization algorithm resulting in a scheme called
Casimir.

17

5.1 Casimir: Catalyst with Smoothing

The Catalyst [Lin et al., 2018] approach minimizes regularized objectives centered around the current it-
erate. The algorithm proceeds by computing approximate proximal point steps instead of the classical
(sub)-gradient steps. A proximal point step from a point w with step-size κ−1 is deﬁned as the minimizer
of

min
z∈Rm

F (z) +

κ
2

(cid:107)z − w(cid:107)2
2,

(24)

which can also be seen as a gradient step on the Moreau envelope of F - see Lin et al. [2018] for a detailed
discussion. While solving the subproblem (24) might be as hard as the original problem we only require
an approximate solution returned by a given optimization method M. The Catalyst approach is then an
inexact accelerated proximal point algorithm that carefully mixes approximate proximal point steps with
the extrapolation scheme of Nesterov [1983]. The Casimir scheme extends this approach to non-smooth
optimization.

For the overall method to be efﬁcient, subproblems (24) must have a low complexity. That is, there must
exist an optimization algorithm M that solves them linearly. For the Casimir approach to be able to handle
non-smooth objectives, it means that we need not only to regularize the objective but also to smooth it. To
this end we deﬁne

Fµω(w) :=

hµω(A(i)w + b(i)) +

(cid:107)w(cid:107)2
2

λ
2

1
n

n
(cid:88)

i=1

as a smooth approximation of the objective F , and,

Fµω,κ(w; z) :=

hµω(A(i)w + b(i)) +

(cid:107)w(cid:107)2

2 +

(cid:107)w − z(cid:107)2
2

λ
2

κ
2

1
n

n
(cid:88)

i=1

a smooth and regularized approximation of the objective centered around a given point z ∈ Rd. While the
original Catalyst algorithm considered a ﬁxed regularization term κ, we vary κ and µ along the iterations.
This enables us to get adaptive smoothing strategies.

The overall method is presented in Algo. 4. We ﬁrst analyze in Sec. 5.2 its complexity for a generic lin-
early convergent algorithm M. Thereafter, in Sec. 5.3, we compute the total complexity with SVRG [John-
son and Zhang, 2013] as M. Before that, we specify two practical aspects of the implementation: a proper
stopping criterion (26) and a good initialization of subproblems (Line 4).

Stopping Criterion Following Lin et al. [2018], we solve subproblem k in Line 4 to a degree of relative ac-
curacy speciﬁed by δk ∈ [0, 1). In view of the (λ + κk)-strong convexity of Fµkω,κk (· ; zk−1), the functional
gap can be controlled by the norm of the gradient, precisely it can be seen that (cid:107)∇Fµkω,κk ( (cid:98)w; zk−1)(cid:107)2
2 ≤
(λ + κk)δkκk(cid:107) (cid:98)w − zk−1(cid:107)2

2 is a sufﬁcient condition for the stopping criterion (26).
A practical alternate stopping criterion proposed by Lin et al. [2018] is to ﬁx an iteration budget Tbudget
and run the inner solver M for exactly Tbudget steps. We do not have a theoretical analysis for this scheme
but ﬁnd that it works well in experiments.

Warm Start of Subproblems Rate of convergence of ﬁrst order optimization algorithms depends on the
initialization and we must warm start M at an appropriate initial point in order to obtain the best convergence
of subproblem (25) in Line 4 of Algo. 4. We advocate the use of the prox center zk−1 in iteration k as the
warm start strategy. We also experiment with other warm start strategies in Section 7.

18

Algorithm 4 The Casimir algorithm

1: Input: Smoothable objective F of the form (23) with h simple, smoothing function ω, linearly con-
vergent algorithm M, non-negative and non-increasing sequence of smoothing parameters (µk)k≥1,
positive and non-decreasing sequence of regularization parameters (κk)k≥1, non-negative sequence of
relative target accuracies (δk)k≥1 and, initial point w0, α0 ∈ (0, 1), time horizon K.

Using M with zk−1 as the starting point, ﬁnd wk ≈ arg minw∈Rd Fµkω,κk (w; zk−1) where

Fµkω,κk (w; zk−1) :=

hµkω(A(i)w + b(i)) +

(cid:107)w(cid:107)2

2 +

(cid:107)w − zk−1(cid:107)2
2

(25)

λ
2

κk
2

1
n

n
(cid:88)

i=1

Fµkω,κk (wk; zk−1) − min
w

Fµkω,κk (w; zk−1) ≤ δkκk

2 (cid:107)wk − zk−1(cid:107)2

2

k(κk+1 + λ) = (1 − αk)α2
α2

k−1(κk + λ) + αkλ.

zk = wk + βk(wk − wk−1),

βk =

αk−1(1 − αk−1)(κk + λ)
k−1(κk + λ) + αk(κk+1 + λ)

α2

.

(26)

(27)

(28)

(29)

2: Initialize: z0 = w0.
3: for k = 1 to K do
4:

such that

5:

Solve for αk ≥ 0

6:

Set

where

7: end for
8: return wK.

5.2 Convergence Analysis of Casimir

We ﬁrst state the outer loop complexity results of Algo. 4 for any generic linearly convergent algorithm M in
Sec. 5.2.1, prove it in Sec. 5.2.2. Then, we consider the complexity of each inner optimization problem (25)
in Sec. 5.2.3 based on properties of M.

5.2.1 Outer Loop Complexity Results

The following theorem states the convergence of the algorithm for general choice of parameters, where we
denote w∗ ∈ arg minw∈Rd F (w) and F ∗ = F (w∗).

Theorem 16. Consider Problem (22). Suppose δk ∈ [0, 1) for all k ≥ 1, the sequence (µk)k≥1 is non-
negative and non-increasing, and the sequence (κk)k≥1 is strictly positive and non-decreasing. Further,
suppose the smoothing function ω : dom h∗ → R satisﬁes −Dω ≤ ω(u) ≤ 0 for all u ∈ dom h∗ and that

19

0 ≥ λ/(λ + κ1). Then, the sequence (αk)k≥0 generated by Algo. 4 satisﬁes 0 < αk ≤ αk−1 < 1 for all

α2
k ≥ 1. Furthermore, the sequence (wk)k≥0 of iterates generated by Algo. 4 satisﬁes

F (wk) − F ∗ ≤

∆0 + µkDω +

(µj−1 − (1 − δj)µj) Dω ,

(30)

Ak−1
0
Bk
1

k
(cid:88)

j=1

Ak−1
j
Bk
j

where Aj
µ0 := 2µ1.

i := (cid:81)j

r=i(1 − αr), Bj

i := (cid:81)j

r=i(1 − δr), ∆0 := F (w0) − F ∗ + (κ1+λ)α2

0−λα0

2(1−α0)

(cid:107)w0 − w∗(cid:107)2

2 and

Before giving its proof, we present various parameters strategies as corollaries. Table 3 summarizes the
parameter settings and the rates obtained for each setting. Overall, the target accuracies δk are chosen such
that Bk
j is a constant and the parameters µk and κk are then carefully chosen for an almost parameter-free
algorithm with the right rate of convergence. Proofs of these corollaries are provided in Appendix D.2.

The ﬁrst corollary considers the strongly convex case (λ > 0) with constant smoothing µk = µ, as-
suming that (cid:15) is known a priori. We note that this is, up to constants, the same complexity obtained by the
original Catalyst scheme on a ﬁxed smooth approximation Fµω with µ = O((cid:15)Dω).

Corollary 17. Consider the setting of Thm. 16. Let q = λ/(λ + κ). Suppose λ > 0 and µk = µ, κk = κ,
√
for all k ≥ 1. Choose α0 =

q) . Then, we have,

q and, δk =

q/(2 −

√

√

F (wk) − F ∗ ≤

µDω + 2

1 −

(F (w0) − F ∗) .

√
√

q
q

3 −
1 −

(cid:18)

(cid:19)k

√

q
2

Next, we consider the strongly convex case where the target accuracy (cid:15) is not known in advance. We
let smoothing parameters (µk)k≥0 decrease over time to obtain an adaptive smoothing scheme that gives
progressively better surrogates of the original objective.

Corollary 18. Consider the setting of Thm. 16. Let q = λ/(λ + κ) and η = 1 −
κk = κ, for all k ≥ 1. Choose α0 =

q and, the sequences (µk)k≥1 and (δk)k≥1 as

√

√

q/2. Suppose λ > 0 and

µk = µηk/2 ,

and,

δk =

√

q
√

,

q

2 −

where µ > 0 is any constant. Then, we have,

F (wk) − F ∗ ≤ ηk/2

(cid:20)
2 (F (w0) − F ∗) +

µDω
√
q
1 −

(cid:18)

√

2 −

q +

√

(cid:19)(cid:21)

.

q
√
η

1 −

The next two corollaries consider the unregularized problem, i.e., λ = 0 with constant and adaptive smooth-
ing respectively.

Corollary 19. Consider the setting of Thm. 16. Suppose µk = µ, κk = κ, for all k ≥ 1 and λ = 0. Choose
α0 = (

5 − 1)/2 and δk = (k + 1)−2 . Then, we have,

√

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2
2

+ µDω

1 +

(cid:16)

8
(k + 2)2

(cid:17)

(cid:18)

12
k + 2

+

30
(k + 2)2

(cid:19)

.

κ
2

20

Table 3: Summary of outer iteration complexity for Algorithm 4 for different parameter settings. We use shorthand
∆F0 := F (w0) − F ∗ and ∆0 = (cid:107)w0 − w∗(cid:107)2. Absolute constants are omitted from the rates.

Cor.

λ > 0

κk

17

18

19

20

Yes

Yes

No

No

κ

κ

κ

µk

µ
√

q
2

µ

(cid:16)

µ

1 −

(cid:17)k/2

κ k

µ/k

δk
√
q
√

q

2−
√

q
√

2−
q
k−2

k−2

α0
√

√

q

q

c

c

F (wk) − F ∗
√

(cid:16)

1 −
√

q
2

(cid:16)

1 −

(cid:17)k

q
2
(cid:17)k/2 (cid:16)

∆F0 + µD
√
1−

q
∆F0 + µD
√
1−
(cid:1) + µD

(cid:17)

q

(cid:0)∆F0 + κ∆2
1
0
k2
log k
k (∆F0 + κ∆2
0 + µD)

Remark

q = λ
λ+κ

q = λ
λ+κ
√
5 − 1)/2

c = (

√

c = (

5 − 1)/2

Corollary 20. Consider the setting of Thm. 16 with λ = 0. Choose α0 = (
non-negative constants κ, µ, deﬁne sequences (κk)k≥1, (µk)k≥1, (δk)k≥1 as

√

5 − 1)/2, and for some

κk = κ k , µk =

and,

δk =

µ
k

1
(k + 1)2 .

Then, for k ≥ 2, we have,

F (wk) − F ∗ ≤

log(k + 1)
k + 1

(cid:0)2(F (w0) − F ∗) + κ(cid:107)w0 − w∗(cid:107)2

2 + 27µDω

(cid:1) .

For the ﬁrst iteration (i.e., k = 1), this bound is off by a constant factor 1/ log 2.

5.2.2 Outer Loop Convergence Analysis

We now prove Thm. 16. The proof technique largely follows that of Lin et al. [2018], with the added
challenges of accounting for smoothing and varying Moreau-Yosida regularization. We ﬁrst analyze the
sequence (αk)k≥0. The proof follows from the algebra of Eq. (27) and has been given in Appendix D.1.

Lemma 21. Given a positive, non-decreasing sequence (κk)k≥1 and λ ≥ 0, consider the sequence (αk)k≥0
deﬁned by (27), where α0 ∈ (0, 1) such that α2
0 ≥ λ/(λ + κ1). Then, we have for every k ≥ 1 that
0 < αk ≤ αk−1 and, α2

k ≥ λ/(λ + κk+1) .

We now characterize the effect of an approximate proximal point step on Fµω.
Lemma 22. Suppose (cid:98)w ∈ Rd satisﬁes Fµω,κ( (cid:98)w; z) − minw∈Rd Fµω,κ(w; z) ≤ (cid:98)(cid:15) for some (cid:98)(cid:15) > 0. Then, for
all 0 < θ < 1 and all w ∈ Rd, we have,

Fµω( (cid:98)w) +

(cid:107) (cid:98)w − z(cid:107)2

2 +

(1 − θ)(cid:107)w − (cid:98)w(cid:107)2

2 ≤ Fµω(w) +

(cid:107)w − z(cid:107)2

κ
2

2 + (cid:98)(cid:15)
θ

.

(31)

κ + λ
2

κ
2

Proof. Let (cid:98)F ∗ = minw∈Rd Fµω,κ(w; z). Let (cid:98)w∗ be the unique minimizer of Fµω,κ(· ; z). We have, from
(κ + λ)-strong convexity of Fµω,κ(· ; z),

Fµω,κ(w; z) ≥ (cid:98)F ∗ +

κ + λ
2

(cid:107)w − (cid:98)w∗(cid:107)2

2

≥ (Fµω,κ( (cid:98)w; z) − (cid:98)(cid:15)) +

(1 − θ)(cid:107)w − (cid:98)w(cid:107)2

2 −

κ + λ
2

κ + λ
2

(cid:18) 1
θ

(cid:19)

− 1

(cid:107) (cid:98)w − (cid:98)w∗(cid:107)2
2 ,

21

where we used that (cid:98)(cid:15) was sub-optimality of (cid:98)w and Lemma 51 from Appendix D.7. From (κ + λ)-strong
convexity of Fµω,κ(·; z), we have,

κ + λ
2

(cid:107) (cid:98)w − (cid:98)w∗(cid:107)2

2 ≤ Fµω,κ( (cid:98)w; z) − (cid:98)F ∗ ≤ (cid:98)(cid:15) ,

Since (1/θ − 1) is non-negative, we can plug this into the previous statement to get,

Fµω,κ(w; z) ≥ Fµω,κ( (cid:98)w; z) +

κ + λ
2

(1 − θ)(cid:107)w − (cid:98)w(cid:107)2

2 − (cid:98)(cid:15)
θ

.

Substituting the deﬁnition of Fµω,κ(· ; z) from (25) completes the proof.

We now deﬁne a few auxiliary sequences integral to the proof. Deﬁne sequences (vk)k≥0, (γk)k≥0,

(ηk)k≥0, and (rk)k≥1 as

One might recognize γk and vk from their resemblance to counterparts from the proof of Nesterov [2013].
Now, we claim some properties of these sequences.

Claim 23. For the sequences deﬁned in (32)-(37), we have,

v0 = w0

vk = wk−1 +

(wk − wk−1) , k ≥ 1 ,

1
αk−1

γ0 =

(κ1 + λ)α2
1 − α0
γk = (κk + λ)α2

ηk =

αkγk
γk+1 + αkγk

0 − λα0

,

k−1 , k ≥ 1 ,

, k ≥ 0 ,

rk = αk−1w∗ + (1 − αk−1)wk−1 , k ≥ 1 .

γk =

(κk+1 + λ)α2
1 − αk
γk+1 = (1 − αk)γk + λαk , k ≥ 0 ,

k − λαk

, k ≥ 0 ,

ηk =

αkγk
γk + αkλ

, k ≥ 0

zk = ηkvk + (1 − ηk)wk , k ≥ 0 , .

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

Proof. Eq. (38) follows from plugging in (27) in (35) for k ≥ 1, while for k = 0, it is true by deﬁnition.
Eq. (39) follows from plugging (35) in (38). Eq. (40) follows from (39) and (36). Lastly, to show (41), we
shall show instead that (41) is equivalent to the update (28) for zk. We have,

zk = ηkvk + (1 − ηk)wk

(cid:18)

(33)
= ηk

(cid:19)

(wk − wk−1)

+ (1 − ηk)wk

= wk + ηk

− 1

(wk − wk−1) .

1
αk−1

wk−1 +
(cid:18) 1

αk−1

(cid:19)

22

Now,

ηk

(cid:18) 1

αk−1

(cid:19) (36)
=

− 1

·

αkγk
γk+1 + αkγk

1 − αk−1
αk−1
αk(κk + λ)α2
k(κk+1 + λ) + αk(κk + λ)α2
α2

k−1

k−1

(35)
=

·

1 − αk−1
αk−1

(29)
= βk ,

completing the proof.

Claim 24. The sequence (rk)k≥1 from (37) satisﬁes

(cid:107)rk − zk−1(cid:107)2

2 ≤ αk−1(αk−1 − ηk−1)(cid:107)wk−1 − w∗(cid:107)2

2 + αk−1ηk−1(cid:107)vk−1 − w∗(cid:107)2
2 .

(42)

Proof. Notice that ηk
get,

(40)
= αk ·

γk

γk+αkλ ≤ αk. Hence, using convexity of the squared Euclidean norm, we

(cid:107)rk − zk−1(cid:107)2
2

(41)
= (cid:107)(αk−1 − ηk−1)(w∗ − wk−1) + ηk−1(w∗ − vk−1)(cid:107)2
2

= α2

k−1

1 −

(w∗ − wk−1) +

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:18)

(cid:19)

ηk−1
αk−1
(cid:19)
ηk−1
αk−1

(∗)
≤ α2

k−1

1 −

(cid:107)wk−1 − w∗(cid:107)2

2 + α2

k−1

(cid:107)vk−1 − w∗(cid:107)2
2

ηk−1
αk−1

ηk−1
αk−1

(cid:13)
2
(cid:13)
(w∗ − vk−1)
(cid:13)
(cid:13)
2

= αk−1(αk−1 − ηk−1)(cid:107)wk−1 − w∗(cid:107)2

2 + αk−1ηk−1(cid:107)vk−1 − w∗(cid:107)2
2 .

For all µ ≥ µ(cid:48) ≥ 0, we know from Prop. 2 that

0 ≤ Fµω(w) − Fµ(cid:48)ω(w) ≤ (µ − µ(cid:48))Dω .

We now deﬁne the sequence (Sk)k≥0 to play the role of a potential function here.

S0 = (1 − α0)(F (w0) − F (w∗)) +

α0κ1η0
2
Sk = (1 − αk)(Fµkω(wk) − Fµkω(w∗)) +

(cid:107)w0 − w∗(cid:107)2
2 ,
αkκk+1ηk
2

(cid:107)vk − w∗(cid:107)2

2 , k ≥ 1 .

(43)

(44)

We are now ready to analyze the effect of one outer loop. This lemma is the crux of the analysis.

Lemma 25. Suppose Fµkω,κk (wk; z) − minw∈Rd Fµkω,κk (w; z) ≤ (cid:15)k for some (cid:15)k > 0. The following
statement holds for all 0 < θk < 1:

Sk
1 − αk

≤ Sk−1 + (µk−1 − µk)Dω +

(cid:107)wk − zk−1(cid:107)2

2 +

(cid:107)vk − w∗(cid:107)2
2 ,

(45)

(cid:15)k
θk

−

κk
2

κk+1ηkαkθk
2(1 − αk)

where we set µ0 := 2µ1.

Proof. For ease of notation, let Fk := Fµkω, and D := Dω. By λ-strong convexity of Fµkω, we have,

Fk(rk) ≤ αk−1Fk(w∗) + (1 − αk−1)Fk(wk−1) −

λαk−1(1 − αk−1)
2

(cid:107)wk−1 − w∗(cid:107)2
2 .

(46)

23

We now invoke Lemma 22 on the function Fµkω,κk (·; zk−1) with (cid:98)(cid:15) = (cid:15)k and w = rk to get,

Fk(wk) +

(cid:107)wk − zk−1(cid:107)2

2 +

(1 − θk)(cid:107)rk − wk(cid:107)2

2 ≤ Fk(rk) +

(cid:107)rk − zk−1(cid:107)2

2 +

.

(47)

κk
2

κk + λ
2

κk
2

(cid:15)k
θk

We shall separately manipulate the left and right hand sides of (47), starting with the right hand side, which
we call R. We have, using (46) and (42),

R ≤ (1 − αk−1)Fk(wk−1) + αk−1Fk(w∗) −

+

κk
2

αk−1(αk−1 − ηk−1)(cid:107)wk−1 − w∗(cid:107)2

λαk−1(1 − αk−1)
2
κkαk−1ηk−1
2

2 +

(cid:107)wk−1 − w∗(cid:107)2
2

(cid:107)vk−1 − w∗(cid:107)2

2 +

(cid:15)k
θk

.

We notice now that

αk−1 − ηk−1

(40)
= αk−1 −

αk−1γk−1
γk + αk−1γk−1
(cid:18) γk − γk−1(1 − αk−1)
γk + αk−1γk−1

(cid:19)

= αk−1

(39)
=

(38)
=

=

α2

k−1λ
γk−1 + αk−1λ

(κk + λ)α2
λ
κk

(1 − αk−1) ,

α2

k−1λ(1 − αk−1)

k−1 − λαk−1 + (1 − αk−1)αk−1λ

and hence the terms containing (cid:107)wk−1 − w∗(cid:107)2

2 cancel out. Therefore, we get,

R ≤ (1 − αk−1)Fk(wk−1) + αk−1Fk(w∗) +

κkαk−1ηk−1
2

(cid:107)vk−1 − w∗(cid:107)2

2 +

(cid:15)k
θk

.

To move on to the left hand side, we note that

αkηk

(40)
=

α2
kγk
γk + αkλ

(35),(38)
=

kα2
α2
(κk+1+λ)α2
1−αk

k−1(κk + λ)
k−λαk

+ αkλ

=

(1 − αk)(κk + λ)α2
(κk+1 + λ)α2

k − λα2
k

k−1α2
k

= (1 − αk)α2

k−1

κk + λ
κk+1

.

Therefore,

Fk(wk) − Fk(w∗) +

κk + λ
2

k−1(cid:107)vk − w∗(cid:107)2
α2
2

(44),(50)
=

Sk
1 − αk

.

Using rk − wk

(33)
= αk−1(w∗ − vk), we simplify the left hand side of (47), which we call L, as

L = Fk(wk) − Fk(w∗) +

(cid:107)wk − zk−1(cid:107)2

2 +

(1 − θk)α2

k−1(cid:107)vk − w∗(cid:107)2
2

κk + λ
2

(51)
=

Sk
1 − αk

+ Fk(w∗) +

(cid:107)wk − zk−1(cid:107)2

2 −

κk+1αkηkθk
2(1 − αk)

(cid:107)vk − w∗(cid:107)2
2 .

κk
2
κk
2

24

(48)

(49)

(50)

(51)

(52)

In view of (49) and (52), we can simplify (47) as

Sk
1 − αk

+

κk
2

(cid:107)wk − zk−1(cid:107)2

2 −

κk+1αkηkθk
2(1 − αk)

≤ (1 − αk−1) (Fk(wk−1) − Fk(w∗)) +

(cid:107)vk − w∗(cid:107)2
2
κkαk−1ηk−1
2

(cid:107)vk−1 − w∗(cid:107)2

2 +

(cid:15)k
θk

.

We make a distinction for k ≥ 2 and k = 1 here. For k ≥ 2, the condition that µk−1 ≥ µk gives us,

Fk(wk−1) − Fk(w∗)

(43)
≤ Fk−1(wk−1) − Fk−1(w∗) + (µk−1 − µk)D .

(53)

(54)

The right hand side of (53) can now be upper bounded by

(1 − αk−1)(µk−1 − µk)D + Sk−1 +

(cid:15)k
θk

,

and noting that 1 − αk−1 ≤ 1 yields (45) for k ≥ 2.

For k = 1, we note that Sk−1(= S0) is deﬁned in terms of F (w). So we have,

F1(w0) − F1(w∗) ≤ F (w0) − F (w∗) + µ1D = F (w0) − F (w∗) + (µ0 − µ1)D ,

because we used µ0 = 2µ1. This is of the same form as (54). Therefore, (45) holds for k = 1 as well.

We now prove Thm. 16.

Proof of Thm. 16. We continue to use shorthand Fk := Fµkω, and D := Dω. We now apply Lemma 25. In
order to satisfy the supposition of Lemma 25 that wk is (cid:15)k-suboptimal, we make the choice (cid:15)k = δkκk
2 (cid:107)wk −
zk−1(cid:107)2

2 (cf. (26)). Plugging this in and setting θk = δk < 1, we get from (45),

Sk
1 − αk

−

κk+1ηkαkδk
2(1 − αk)

(cid:107)vk − w∗(cid:107)2

2 ≤ Sk−1 + (µk−1 − µk)D .

The left hand side simpliﬁes to Sk (1 − δk)/(1 − αk) + δk(Fk(wk) − Fk(w∗)). Note that Fk(wk) −
(43)
≥ F (wk) − F (w∗) − µkD ≥ −µkD. From this, noting that αk ∈ (0, 1) for all k, we get,
Fk(w∗)

Sk

(cid:19)

(cid:18) 1 − δk
1 − αk

≤ Sk−1 + δkµkD + (µk−1 − µk)D ,

or equivalently,

Sk ≤

(cid:19)

(cid:18) 1 − αk
1 − δk

(cid:19)

(cid:18) 1 − αk
1 − δk

Sk−1 +

(µk−1 − (1 − δk)µk)D .

Unrolling the recursion for Sk, we now have,

Sk ≤





k
(cid:89)

j=1

1 − αj
1 − δj



 S0 +

k
(cid:88)

k
(cid:89)





j=1

i=j

1 − αi
1 − δi



 (µj−1 − (1 − δj)µj)D .

(55)

25

Now, we need to reason about S0 and Sk to complete the proof. To this end, consider η0:

(36)
=

η0

α0γ0
γ1 + α0γ0

α0γ0

(34)
=

=

(κ1 + λ)α2

0 + α0
1−α0

α0γ0(1 − α0)

(κ1 + λ)α2

0 − λα2
0

(cid:1)

(cid:0)(κ1 + λ)α2
0 − λα0
γ0
κ1α0

= (1 − α0)

.

(56)

With this, we can expand out S0 to get

S0

(44)
= (1 − α0) (F (w0) − F (w∗)) +
γ0
2

F (w0) − F ∗ +

(56)
= (1 − α0)

(cid:16)

(cid:107)w0 − w∗(cid:107)2
2

α0κ1η0
2
(cid:107)w0 − w∗(cid:107)2
2

(cid:17)

.

Lastly, we reason about Sk for k ≥ 1 as,

(44)
≥ (1 − αk) (Fk(wk) − Fk(w∗))

(43)
≥ (1 − αk) (F (wk) − F (w∗) − µkD) .

Sk

Plugging this into the left hand side of (55) completes the proof.

5.2.3

Inner Loop Complexity

Consider a class FL,λ of functions deﬁned as

(cid:110)

FL,λ =

f : Rd → R such that f is L-smooth and λ-strongly convex

(cid:111)

.

We now formally deﬁne a linearly convergent algorithm on this class of functions.

Deﬁnition 26. A ﬁrst order algorithm M is said to be linearly convergent with parameters C : R+ × R+ →
R+ and τ : R+ × R+ → (0, 1) if the following holds: for all L ≥ λ > 0, and every f ∈ FL,λ and w0 ∈ Rd,
M started at w0 generates a sequence (wk)k≥0 that satisﬁes:

Ef (wk) − f ∗ ≤ C(L, λ) (1 − τ (L, λ))k (f (w0) − f ∗) ,

(57)

where f ∗ := minw∈Rd f (w) and the expectation is over the randomness of M.

The parameter τ determines the rate of convergence of the algorithm. For instance, batch gradient
descent is a deterministic linearly convergent algorithm with τ (L, λ)−1 = L/λ and incremental algorithms
such as SVRG and SAGA satisfy requirement (57) with τ (L, λ)−1 = c(n+ L/λ) for some universal constant
c.

The warm start strategy in step k of Algo. 4 is to initialize M at the prox center zk−1. The next
proposition, due to Lin et al. [2018, Cor. 16] bounds the expected number of iterations of M required to
ensure that wk satisﬁes (26). Its proof has been given in Appendix D.3 for completeness.

Proposition 27. Consider Fµω,κ(· ; z) deﬁned in Eq. (25), and a linearly convergent algorithm M with
parameters C, τ . Let δ ∈ [0, 1). Suppose Fµω is Lµω-smooth and λ-strongly convex. Then the expected
number of iterations E[ (cid:98)T ] of M when started at z in order to obtain (cid:98)w ∈ Rd that satisﬁes

Fµω,κ( (cid:98)w; z) − min
w

Fµω,κ(w; z) ≤ δκ

2 (cid:107)w − z(cid:107)2

2

26

Table 4: Summary of global complexity of Casimir-SVRG, i.e., Algorithm 4 with SVRG as the inner solver for various
parameter settings. We show E[N ], the expected total number of SVRG iterations required to obtain an accuracy (cid:15), up
to constants and factors logarithmic in problem parameters. We denote ∆F0 := F (w0) − F ∗ and ∆0 = (cid:107)w0 − w∗(cid:107)2.
Constants D, A are short for Dω, Aω (see (58)).

Prop.

λ > 0

µk

κk

δk
(cid:113) λ(cid:15)n
AD

c(cid:48)

(cid:15)/D AD/(cid:15)n − λ

λ

29

30

31

32

Yes

Yes

No

No

µck

(cid:15)/D

µ/k

AD/(cid:15)n

1/k2

n

κ0 k

1/k2

E[N ]
(cid:113) ADn
λ(cid:15)

n +

n + A
λ(cid:15)
(cid:113) ∆F0

∆F0+µD
µ

√

(cid:15) +
(cid:16)

ADn∆0
(cid:15)
(cid:17)

(cid:98)∆0
(cid:15)

n + A
µκ0

Remark

ﬁx (cid:15) in advance

c, c(cid:48) < 1 are universal constants

ﬁx (cid:15) in advance

(cid:98)∆0 = ∆F0 + κ0

2 ∆2

0 + µD

is upper bounded by

E[ (cid:98)T ] ≤

1
τ (Lµω + κ, λ + κ)

log

(cid:18) 8C(Lµω + κ, λ + κ)
τ (Lµω + κ, λ + κ)

·

Lµω + κ
κδ

(cid:19)

+ 1 .

5.3 Casimir with SVRG

We now choose SVRG [Johnson and Zhang, 2013] to be the linearly convergent algorithm M, resulting
in an algorithm called Casimir-SVRG. The rest of this section analyzes the total iteration complexity of
Casimir-SVRG to solve Problem (22). The proofs of the results from this section are calculations stemming
from combining the outer loop complexity from Cor. 17 to 20 with the inner loop complexity from Prop. 27,
and are relegated to Appendix D.4. Table 4 summarizes the results of this section.

Recall that if ω is 1-strongly convex with respect to (cid:107) · (cid:107)α, then hµω(Aw + b) is Lµω-smooth with
2,α/µ. Therefore, the complexity of solving problem (22) will depend

respect to (cid:107) · (cid:107)2, where Lµω = (cid:107)A(cid:107)2
on

Aω := max
i=1,··· ,n

(cid:107)A(i)(cid:107)2

2,α .

(58)

Remark 28. We have that (cid:107)A(cid:107)2,2 = (cid:107)A(cid:107)2 is the spectral norm of A and (cid:107)A(cid:107)2,1 = maxj (cid:107)aj(cid:107)2 is the
largest row norm, where aj is the jth row of A. Moreover, we have that (cid:107)A(cid:107)2,2 ≥ (cid:107)A(cid:107)2,1.

We start with the strongly convex case with constant smoothing.

Proposition 29. Consider the setting of Thm. 16 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as the inner
solver with parameters: µk = µ = (cid:15)/10Dω, κk = k chosen as

√

√

√

q = λ/(λ + κ), α0 =
F (w) − F (w∗) ≤ (cid:15) is bounded in expectation as
(cid:32)

q, and δ =

q/(2 −

q). Then, the number of iterations N to obtain w such that

κ =

(cid:40) A

µn − λ , if A
λ , otherwise

µn > 4λ

,

E[N ] ≤ (cid:101)O

n +

(cid:114)

(cid:33)

.

AωDωn
λ(cid:15)

27

Here, we note that κ was chosen to minimize the total complexity (cf. Lin et al. [2018]). This bound is
known to be tight, up to logarithmic factors [Woodworth and Srebro, 2016]. Next, we turn to the strongly
convex case with decreasing smoothing.

Proposition 30. Consider the setting of Thm. 16. Suppose λ > 0 and κk = κ, for all k ≥ 1 and that α0,
(µk)k≥1 and (δk)k≥1 are chosen as in Cor. 18, with q = λ/(λ+κ) and η = 1−
q/2. If we run Algo. 4 with
SVRG as the inner solver with these parameters, the number of iterations N of SVRG required to obtain w
such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

√

(cid:18)

E[N ] ≤ (cid:101)O

n +

(cid:18)

Aω
µ(λ + κ)(cid:15)

F (w0) − F ∗ +

µDω
√
1 −

q

(cid:19)(cid:19)

.

Unlike the previous case, there is no obvious choice of κ, such as to minimize the global complexity. Notice
that we do not get the accelerated rate of Prop. 29. We now turn to the case when λ = 0 and µk = µ for all
k.

Proposition 31. Consider the setting of Thm. 16 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as the inner
5 − 1)/2, δk = 1/(k + 1)2, and κk = κ =
solver with parameters: µk = µ = (cid:15)/20Dω, α0 = (
Aω/µ(n + 1). Then, the number of iterations N to get a point w such that F (w) − F ∗ ≤ (cid:15) is bounded in
expectation as

√

(cid:32)

(cid:114)

E[N ] ≤ (cid:101)O

n

F (w0) − F ∗
(cid:15)

(cid:112)

+

AωDωn

(cid:107)w0 − w∗(cid:107)2
(cid:15)

(cid:33)

.

This rate is tight up to log factors [Woodworth and Srebro, 2016]. Lastly, we consider the non-strongly
convex case (λ = 0) together with decreasing smoothing. As with Prop. 30, we do not obtain an accelerated
rate here.

Proposition 32. Consider the setting of Thm. 16. Suppose λ = 0 and that α0, (µk)k≥1,(κk)k≥1 and (δk)k≥1
are chosen as in Cor. 20. If we run Algo. 4 with SVRG as the inner solver with these parameters, the number
of iterations N of SVRG required to obtain w such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

E[N ] ≤ (cid:101)O

(cid:0)F (w0) − F ∗ + κ(cid:107)w0 − w∗(cid:107)2

2 + µD(cid:1)

(cid:18) 1
(cid:15)

(cid:18)

n +

(cid:19)(cid:19)

.

Aω
µκ

6 Extension to Non-Convex Optimization

Let us now turn to the optimization problem (1) in full generality where the mappings g(i) deﬁned in (6) are
not constrained to be afﬁne:

(cid:34)

min
w∈Rd

F (w) :=

1
n

n
(cid:88)

i=1

h(g(i)(w)) +

(cid:107)w(cid:107)2
2

,

λ
2

(cid:35)

(59)

where h is a simple, non-smooth, convex function, and each g(i) is a continuously differentiable nonlinear
map and λ ≥ 0.

We describe the prox-linear algorithm in Sec. 6.1, followed by the convergence guarantee in Sec. 6.2

and the total complexity of using Casimir-SVRG together with the prox-linear algorithm in Sec. 6.3.

28

Algorithm 5 (Inexact) Prox-linear algorithm: outer loop

1: Input: Smoothable objective F of the form (59) with h simple, step length η, tolerances ((cid:15)k)k≥1, initial

point w0, non-smooth convex optimization algorithm, M, time horizon K

2: for k = 1 to K do
3:

Using M with wk−1 as the starting point, ﬁnd

(cid:20)

(cid:98)wk ≈ arg min

w

Fη(w; wk−1) :=

1
n

n
(cid:88)

i=1

h(cid:0)g(i)(wk−1) + ∇g(i)(wk−1)(w − wk−1)(cid:1)

+

(cid:107)w(cid:107)2

2 +

(cid:107)w − wk−1(cid:107)2
2 ,

λ
2

1
2η

(cid:21)

(61)

(62)

such that

4:
5: end for
6: return wK.

Fη( (cid:98)wk; wk−1) − min
w∈Rd

Fη(w; wk−1) ≤ (cid:15)k .

Set wk = (cid:98)wk if F ( (cid:98)wk) ≤ F (wk−1), else set wk = wk−1.

6.1 The Prox-Linear Algorithm

The exact prox-linear algorithm of Burke [1985] generalizes the proximal gradient algorithm (see e.g.,
Nesterov [2013]) to compositions of convex functions with smooth mappings such as (59). When given a
function f = h ◦ g, the prox-linear algorithm deﬁnes a local convex approximation f (· ; wk) about some
point w ∈ Rd by linearizing the smooth map g as f (w; wk) := h(g(wk) + ∇g(wk)(w − wk)) . With this,
it builds a convex model F (· ; wk) of F about wk as

F (w; wk) :=

h(g(i)(wk) + ∇g(i)(wk)(w − wk)) +

(cid:107)w(cid:107)2
2 .

λ
2

Given a step length η > 0, each iteration of the exact prox-linear algorithm then minimizes the local convex
model plus a proximal term as

wk+1 = arg min

Fη(w; wk) := F (w; wk) +

(60)

(cid:21)

(cid:107)w − wk(cid:107)2
2

.

1
2η

Following Drusvyatskiy and Paquette [2018], we consider an inexact prox-linear algorithm, which ap-
proximately solves (60) using an iterative algorithm. In particular, since the function to be minimized in
(60) is precisely of the form (23), we employ the fast convex solvers developed in the previous section as
subroutines. Concretely, the prox-linear outer loop is displayed in Algo. 5. We now delve into details about
the algorithm and convergence guarantees.

6.1.1

Inexactness Criterion

As in Section 5, we must be prudent in choosing when to terminate the inner optimization (Line 3 of Algo. 5).
Function value suboptimality is used as the inexactness criterion here. In particular, for some speciﬁed

1
n

n
(cid:88)

i=1

(cid:20)

w∈Rd

29

tolerance (cid:15)k > 0, iteration k of the prox-linear algorithm accepts a solution (cid:98)w that satisﬁes Fη( (cid:98)wk; wk−1) −
minw Fη(w; wk−1) ≤ (cid:15)k.

Implementation In view of the (λ + η−1)-strong convexity of Fη(· ; wk−1), it sufﬁces to ensure that
(λ + η−1)(cid:107)v(cid:107)2

2 ≤ (cid:15)k for a subgradient v ∈ ∂Fη( (cid:98)wk; wk−1).

Fixed Iteration Budget As in the convex case, we consider as a practical alternative a ﬁxed iteration
budget Tbudget and optimize Fη(· ; wk) for exactly Tbudget iterations of M. Again, we do not have a
theoretical analysis for this scheme but ﬁnd it to be effective in practice.

6.1.2 Warm Start of Subproblems

As in the convex case, we advocate the use of the prox center wk−1 to warm start the inner optimization
problem in iteration k (Line 3 of Algo. 5).

6.2 Convergence analysis of the prox-linear algorithm

We now state the assumptions and the convergence guarantee of the prox-linear algorithm.

6.2.1 Assumptions

For the prox-linear algorithm to work, the only requirement is that we minimize an upper model. The
assumption below makes this concrete.

Assumption 33. The map g(i) is continuously differentiable everywhere for each i ∈ [n]. Moreover, there
exists a constant L > 0 such that for all w, w(cid:48) ∈ Rd and i ∈ [n], it holds that

h(cid:0)g(i)(w(cid:48))(cid:1) ≤ h(cid:0)g(i)(w) + ∇g(i)(w)(w(cid:48) − w)(cid:1) +

L
2

(cid:107)w(cid:48) − w(cid:107)2
2 .

When h is G-Lipschitz and each g(i) is (cid:101)L-smooth, both with respect to (cid:107) · (cid:107)2, then Assumption 33 holds
with L = G(cid:101)L [Drusvyatskiy and Paquette, 2018]. In the case of structured prediction, Assumption 33 holds
when the augmented score ψ as a function of w is L-smooth. The next lemma makes this precise and its
proof is in Appendix D.5.

Lemma 34. Consider the structural hinge loss f (w) = maxy∈Y ψ(y; w) = h ◦ g(w) where h, g are as
deﬁned in (6). If the mapping w (cid:55)→ ψ(y; w) is L-smooth with respect to (cid:107) · (cid:107)2 for all y ∈ Y, then it holds
for all w, z ∈ Rd that

|h(g(w + z)) − h(g(w) + ∇g(w)z)| ≤

6.2.2 Convergence Guarantee

Convergence is measured via the norm of the prox-gradient (cid:37)η(·), also known as the gradient mapping,
deﬁned as

(cid:37)η(w) =

w − arg min

Fη(z; w)

.

(63)

L
2

(cid:107)z(cid:107)2
2 .

(cid:33)

(cid:32)

1
η

z∈Rd

30

The measure of stationarity (cid:107)(cid:37)η(w)(cid:107) turns out to be related to the norm of the gradient of the Moreau
envelope of F under certain conditions - see Drusvyatskiy and Paquette [2018, Section 4] for a discussion.
In particular, a point w with small (cid:107)(cid:37)η(w)(cid:107) means that w is close to w(cid:48) = arg minz∈Rd Fη(z; w), which
is nearly stationary for F .

The prox-linear outer loop shown in Algo. 5 has the following convergence guarantee [Drusvyatskiy and

Paquette, 2018, Thm. 5.2].

Theorem 35. Consider F of the form (59) that satisﬁes Assumption 33, a step length 0 < η ≤ 1/L and a
non-negative sequence ((cid:15)k)k≥1. With these inputs, Algo. 5 produces a sequence (wk)k≥0 that satisﬁes

min
k=0,··· ,K−1

(cid:107)(cid:37)η(wk)(cid:107)2

2 ≤

F (w0) − F ∗ +

(cid:32)

2
ηK

(cid:33)

(cid:15)k

,

K
(cid:88)

k=1

where F ∗ = inf w∈Rd F (w). In addition, we have that the sequence (F (wk))k≥0 is non-increasing.

Remark 36. Algo. 5 accepts an update only if it improves the function value (Line 4). A variant of Algo. 5
which always accepts the update has a guarantee identical to that of Thm. 35, but the sequence (F (wk))k≥0
would not guaranteed to be non-increasing.

6.3 Prox-Linear with Casimir-SVRG

We now analyze the total complexity of minimizing the ﬁnite sum problem (59) with Casimir-SVRG to
approximately solve the subproblems of Algo. 5.

For the algorithm to converge, the map w (cid:55)→ g(i)(wk) + ∇g(i)(wk)(w − wk) must be Lipschitz for

each i and each iterate wk. To be precise, we assume that

Aω := max
i=1,··· ,n

sup
w∈Rd

(cid:107)∇g(i)(w)(cid:107)2

2,α

(64)

is ﬁnite, where ω, the smoothing function, is 1-strongly convex with respect to (cid:107) · (cid:107)α. When g(i) is the linear
map w (cid:55)→ A(i)w, this reduces to (58).

We choose the tolerance (cid:15)k to decrease as 1/k. When using the Casimir-SVRG algorithm with constant
smoothing (Prop. 29) as the inner solver, this method effectively smooths the kth prox-linear subproblem as
1/k. We have the following rate of convergence for this method, which is proved in Appendix D.6.

Proposition 37. Consider the setting of Thm. 35. Suppose the sequence ((cid:15)k)k≥1 satisﬁes (cid:15)k = (cid:15)0/k for
some (cid:15)0 > 0 and that the subproblem of Line 3 of Algo. 5 is solved using Casimir-SVRG with the settings
of Prop. 29. Then, total number of SVRG iterations N required to produce a w such that (cid:107)(cid:37)η(w)(cid:107)2 ≤ (cid:15) is
bounded as

E[N ] ≤ (cid:101)O





n
η(cid:15)2 (F (w0) − F ∗ + (cid:15)0) +

(cid:113)

AωDωn(cid:15)−1
0
η(cid:15)3



(F (w0) − F ∗ + (cid:15)0)3/2

 .

Remark 38. When an estimate or an upper bound B on F (w0) − F ∗, one could set (cid:15)0 = O(B). This is
true, for instance, in the structured prediction task where F ∗ ≥ 0 whenever the task loss (cid:96) is non-negative
(cf. (4)).

31

7 Experiments

In this section, we study the experimental behavior of the proposed algorithms on two structured prediction
tasks, namely named entity recognition and visual object localization. Recall that given training examples
{(x(i), y(i))}n

i=1, we wish to solve the problem:

(cid:34)

min
w∈Rd

F (w) :=

(cid:107)w(cid:107)2

2 +

λ
2

1
n

n
(cid:88)

i=1

(cid:110)

max
y(cid:48)∈Y(x(i))

φ(x(i), y(cid:48); w) + (cid:96)(y(i), y(cid:48))

− φ(x(i), y(i); w)

.

(cid:111)

(cid:35)

Note that we now allow the output space Y(x) to depend on the instance x - the analysis from the previous
sections applies to this setting as well. In all the plots, the shaded region represents one standard deviation
over ten random runs.

We compare the performance of various optimization algorithms based on the number of calls to a
smooth inference oracle. Moreover, following literature for algorithms based on SVRG [Schmidt et al.,
2017, Lin et al., 2018], we exclude the cost of computing the full gradients.

The results must be interpreted keeping in mind that the running time of all inference oracles is not
the same. These choices were motivated by the following reasons, which may not be appropriate in all
contexts. The ultimate yardstick to benchmark the performance of optimization algorithms is wall clock
time. However, this depends heavily on implementation, system and ambient system conditions. With
regards to the differing running times of different oracles, we ﬁnd that a small value of K, e.g., 5 sufﬁces,
so that our highly optimized implementations of the top-K oracle incurs negligible running time penalties
over the max oracle. Moreover, the computations of the batch gradient have been neglected as they are
embarrassingly parallel.

The outline of the rest of this section is as follows. First, we describe the datasets and task description
in Sec. 7.1, followed by methods compared in Sec. 7.2 and their hyperparameter settings in Sec. 7.3. Lastly,
Sec. 7.4 presents the experimental studies.

7.1 Dataset and Task Description

For each of the tasks, we specify below the following: (a) the dataset {(x(i), y(i))}n
i=1, (b) the output
structure Y, (c) the loss function (cid:96), (d) the score function φ(x, y; w), (e) implementation of inference
oracles, and lastly, (f) the evaluation metric used to assess the quality of predictions.

7.1.1 CoNLL 2003: Named Entity Recognition

Named entities are phrases that contain the names of persons, organization, locations, etc, and the task is
to predict the label (tag) of each entity. Named entity recognition can be formulated as a sequence tagging
problem where the set Ytag of individual tags is of size 7.

Each datapoint x is a sequence of words x = (x1, · · · , xp), and the label y = (y1, · · · , yp) ∈ Y(x) is a

sequence of the same length, where each yi ∈ Ytag is a tag.

Loss Function The loss function is the Hamming Loss (cid:96)(y, y(cid:48)) = (cid:80)
i

I(yi (cid:54)= y(cid:48)

i).

Score Function We use a chain graph to represent this task. In other words, the observation-label depen-
dencies are encoded as a Markov chain of order 1 to enable efﬁcient inference using the Viterbi algorithm.
We only consider the case of linear score φ(x, y; w) = (cid:104)w, Φ(x, y)(cid:105) for this task. The feature map Φ here

32

is very similar to that given in Example 5. Following Tkachenko and Simanovsky [2012], we use local con-
text Ψi(x) around ith word xi of x. In particular, deﬁne Ψi(x) = exi−2 ⊗ · · · ⊗ exi+2, where ⊗ denotes the
Kronecker product between column vectors, and exi denotes a one hot encoding of word xi, concatenated
with the one hot encoding of its the part of speech tag and syntactic chunk tag which are provided with the
input. Now, we can deﬁne the feature map Φ as

Φ(x, y) =

Ψv(x) ⊗ eyv

⊕

eyv ⊗ eyv+1

,

(cid:35)

(cid:34) p

(cid:88)

i=0

(cid:35)

(cid:34) p

(cid:88)

v=1

where ey ∈ R|Ytag| is a one hot-encoding of y ∈ Ytag, and ⊕ denotes vector concatenation.

Inference We use the Viterbi algorithm as the max oracle (Algo. 1) and top-K Viterbi algorithm (Algo. 2)
for the top-K oracle.

Dataset The dataset used was CoNLL 2003 [Tjong Kim Sang and De Meulder, 2003], which contains
about ∼ 20K sentences.

Evaluation Metric We follow the ofﬁcial CoNLL metric: the F1 measure excluding the ‘O’ tags. In
addition, we report the objective function value measured on the training set (“train loss”).

Other Implementation Details The sparse feature vectors obtained above are hashed onto 216 − 1 dimen-
sions for efﬁciency.

7.1.2 PASCAL VOC 2007: Visual Object Localization

Given an image and an object of interest, the task is to localize the object in the given image, i.e., determine
the best bounding box around the object. A related, but harder task is object detection, which requires
identifying and localizing any number of objects of interest, if any, in the image. Here, we restrict ourselves
to pure localization with a single instance of each object. Given an image x ∈ X of size n1 × n2, the label
y ∈ Y(x) is a bounding box, where Y(x) is the set of all bounding boxes in an image of size n1 × n2. Note
that |Y(x)| = O(n2

1n2

2).

Loss Function The PASCAL IoU metric [Everingham et al., 2010] is used to measure the quality of
localization. Given bounding boxes y, y(cid:48), the IoU is deﬁned as the ratio of the intersection of the bounding
boxes to the union:

IoU(y, y(cid:48)) =

Area(y ∩ y(cid:48))
Area(y ∪ y(cid:48))

.

We then use the 1 − IoU loss deﬁned as (cid:96)(y, y(cid:48)) = 1 − IoU(y, y(cid:48)).

Score Function The formulation we use is based on the popular R-CNN approach [Girshick et al., 2014].
linear score and non-linear score φ, both of which are based on the following
We consider two cases:
deﬁnition of the feature map Φ(x, y).

• Consider a patch x|y of image x cropped to box y, and rescale it to 64 × 64. Call this Π(x|y).

33

• Consider a convolutional neural network known as AlexNet [Krizhevsky et al., 2012] pre-trained on
ImageNet [Russakovsky et al., 2015] and pass Π(x|y) through it. Take the output of conv4, the
penultimate convolutional layer as the feature map Φ(x, y). It is of size 3 × 3 × 256.

In the case of linear score functions, we take φ(x, y; w) = (cid:104)w, Φ(x, y)(cid:105). In the case of non-linear score
functions, we deﬁne the score φ as the the result of a convolution composed with a non-linearity and followed
by a linear map. Concretely, for θ ∈ RH×W ×C1 and w ∈ RC1×C2 let the map θ (cid:55)→ θ (cid:63) w ∈ RH×W ×C2
denote a two dimensional convolution with stride 1 and kernel size 1, and σ : R → R denote the exponential
linear unit, deﬁned respectively as

[θ (cid:63) w]ij = w(cid:62)[θ]ij

and σ(x) = x I(x ≥ 0) + (exp(x) − 1) I(x < 0) ,

where [θ]ij ∈ RC1 is such that its lth entry is θijl and likewise for [θ (cid:63) w]ij. We overload notation to
let σ : Rd → Rd denote the exponential linear unit applied element-wise. Notice that σ is smooth. The
non-linear score function φ is now deﬁned, with w1 ∈ R256×16, w2 ∈ R16×3×3 and w = (w1, w2), as,

φ(x, y; w) = (cid:104)σ(Φ(x, y) (cid:63) w1), w2(cid:105) .

Inference For a given input image x, we follow the R-CNN approach [Girshick et al., 2014] and use
selective search [Van de Sande et al., 2011] to prune the search space. In particular, for an image x, we use
the selective search implementation provided by OpenCV [Bradski, 2000] and take the top 1000 candidates
returned to be the set (cid:98)Y(x), which we use as a proxy for Y(x). The max oracle and the top-K oracle are
then implemented as exhaustive searches over this reduced set (cid:98)Y(x).

Dataset We use the PASCAL VOC 2007 dataset [Everingham et al., 2010], which contains ∼ 5K an-
notated consumer (real world) images shared on the photo-sharing site Flickr from 20 different object cat-
egories. For each class, we consider all images with only a single occurrence of the object, and train an
independent model for each class.

Evaluation Metric We keep track of two metrics. The ﬁrst is the localization accuracy, also known as
CorLoc (for correct localization), following Deselaers et al. [2010]. A bounding box with IoU > 0.5 with the
ground truth is considered correct and the localization accuracy is the fraction of images labeled correctly.
The second metric is average precision (AP), which requires a conﬁdence score for each prediction. We use
φ(x, y(cid:48); w) as the conﬁdence score of y(cid:48). As previously, we also plot the objective function value measured
on the training examples.

Other Implementation Details For a given input-output pair (x, y) in the dataset, we instead use (x, (cid:98)y)
as a training example, where (cid:98)y = arg maxy(cid:48)∈ (cid:98)Y(x) IoU(y, y(cid:48)) is the element of (cid:98)Y(x) which overlaps the
most with the true output y.

7.2 Methods Compared

The experiments compare various convex stochastic and incremental optimization methods for structured
prediction.

34

• SGD: Stochastic subgradient method with a learning rate γt = γ0/(1+(cid:98)t/t0(cid:99)), where η0, t0 are tuning
parameters. Note that this scheme of learning rates does not have a theoretical analysis. However,
the averaged iterate wt = 2/(t2 + t) (cid:80)t
τ =1 τ wτ obtained from the related scheme γt = 1/(λt) was
shown to have a convergence rate of O((λ(cid:15))−1) [Shalev-Shwartz et al., 2011, Lacoste-Julien et al.,
2012]. It works on the non-smooth formulation directly.

• BCFW: The block coordinate Frank-Wolfe algorithm of Lacoste-Julien et al. [2013]. We use the
version that was found to work best in practice, namely, one that uses the weighted averaged iterate
wt = 2/(t2 + t) (cid:80)t
τ =1 τ wτ (called bcfw-wavg by the authors) with optimal tuning of learning
rates. This algorithm also works on the non-smooth formulation and does not require any tuning.

• SVRG: The SVRG algorithm proposed by Johnson and Zhang [2013], with each epoch making one
pass through the dataset and using the averaged iterate to compute the full gradient and restart the next
epoch. This algorithm requires smoothing.

• Casimir-SVRG-const: Algo. 4 with SVRG as the inner optimization algorithm. The parameters µk
and κk as chosen in Prop. 29, where µ and κ are hyperparameters. This algorithm requires smoothing.

• Casimir-SVRG-adapt: Algo. 4 with SVRG as the inner optimization algorithm. The parameters µk
and κk as chosen in Prop. 30, where µ and κ are hyperparameters. This algorithm requires smoothing.

On the other hand, for non-convex structured prediction, we only have two methods:

• SGD: The stochastic subgradient method [Davis and Drusvyatskiy, 2018], which we call as SGD. This
√
algorithm works directly on the non-smooth formulation. We try learning rates γt = γ0, γt = γ0/
t
and γt = γ0/t, where γ0 is found by grid search in each of these cases. We use the names SGD-const,
SGD-t−1/2 and SGD-t−1 respectively for these variants. We note that SGD-t−1 does not have any
theoretical analysis in the non-convex case.

• PL-Casimir-SVRG: Algo. 5 with Casimir-SVRG-const as the inner solver using the settings of

Prop. 37. This algorithm requires smoothing the inner subproblem.

7.3 Hyperparameters and Variants

Smoothing In light of the discussion of Sec. 4, we use the (cid:96)2
strategy for efﬁcient computation. We then have Dω = 1/2.

2 smoother ω(u) = (cid:107)u(cid:107)2

2/2 and use the top-K

Regularization The regularization coefﬁcient λ is chosen as c/n, where c is varied in {0.01, 0.1, 1, 10}.

Choice of K The experiments use K = 5 for named entity recognition where the performance of the
top-K oracle is K times slower, and K = 10 for visual object localization, where the running time of the
top-K oracle is independent of K. We also present results for other values of K in Fig. 5d and ﬁnd that the
performance of the tested algorithms is robust to the value of K.

35

Tuning Criteria Some algorithms require tuning one or more hyperparameters such as the learning rate.
We use grid search to ﬁnd the best choice of the hyperparameters using the following criteria: For the named
entity recognition experiments, the train function value and the validation F1 metric were only weakly
correlated. For instance, the 3 best learning rates in the grid in terms of F1 score, the best F1 score attained
the worst train function value and vice versa. Therefore, we choose the value of the tuning parameter that
attained the best objective function value within 1% of the best validation F1 score in order to measure the
optimization performance while still remaining relevant to the named entity recognition task. For the visual
object localization task, a wide range of hyperparameter values achieved nearly equal performance in terms
of the best CorLoc over the given time horizon, so we choose the value of the hyperparameter that achieves
the best objective function value within a given iteration budget.

7.3.1 Hyperparameters for Convex Optimization

This corresponds to the setting of Section 5.

Learning Rate The algorithms SVRG and Casimir-SVRG-adapt require tuning of a learning rate, while
SGD requires η0, t0 and Casimir-SVRG-const requires tuning of the Lipschitz constant L of ∇Fµω, which
determines the learning rate γ = 1/(L + λ + κ). Therefore, tuning the Lipschitz parameter is similar
to tuning the learning rate. For both the learning rate and Lipschitz parameter, we use grid search on a
logarithmic grid, with consecutive entries chosen a factor of two apart.

Choice of κ For Casimir-SVRG-const, with the Lipschitz constant in hand, the parameter κ is chosen to
minimize the overall complexity as in Prop. 29. For Casimir-SVRG-adapt, we use κ = λ.

Stopping Criteria Following the discussion of Sec. 5, we use an iteration budget of Tbudget = n.

Warm Start The warm start criterion determines the starting iterate of an epoch of the inner optimization
algorithm. Recall that we solve the following subproblem using SVRG for the kth iterate (cf. (25)):

wk ≈ arg min

Fµkω,κk (wk; zk−1) .

w∈Rd

Here, we consider the following warm start strategy to choose the initial iterate (cid:98)w0 for this subproblem:

• Prox-center: (cid:98)w0 = zk−1.

In addition, we also try out the following warm start strategies of Lin et al. [2018]:

• Extrapolation: (cid:98)w0 = wk−1 + c(zk−1 − zk−2) where c = κ
• Prev-iterate: (cid:98)w0 = wk−1.

κ+λ .

We use the Prox-center strategy unless mentioned otherwise.

Level of Smoothing and Decay Strategy For SVRG and Casimir-SVRG-const with constant smoothing,
we try various values of the smoothing parameter in a logarithmic grid. On the other hand, Casimir-SVRG-
adapt is more robust to the choice of the smoothing parameter (Fig. 5a). We use the defaults of µ = 2 for
named entity recognition and µ = 10 for visual object localization.

36

Figure 2: Comparison of convex optimization algorithms for the task of Named Entity Recognition on
CoNLL 2003.

7.3.2 Hyperparameters for Non-Convex Optimization

This corresponds to the setting of Section 6.

Prox-Linear Learning Rate η We perform grid search in powers of 10 to ﬁnd the best prox-linear learning
rate η. We ﬁnd that the performance of the algorithm is robust to the choice of η (Fig. 7a).

Stopping Criteria We used a ﬁxed budget of 5 iterations of Casimir-SVRG-const. In Fig. 7b, we experi-
ment with different iteration budgets.

In order to solve the kth prox-linear subproblem with Casimir-
Level of Smoothing and Decay Strategy
SVRG-const, we must specify the level of smoothing µk. We experiment with two schemes, (a) constant
smoothing µk = µ, and (b) adaptive smoothing µk = µ/k. Here, µ is a tuning parameters, and the adaptive
smoothing scheme is designed based on Prop. 37 and Remark 38. We use the adaptive smoothing strategy
as a default, but compare the two in Fig. 6.

Gradient Lipschitz Parameter for Inner Optimization The inner optimization algorithm Casimir-SVRG-
const still requires a hyperparameter Lk to serve as an estimate to the Lipschitz parameter of the gradient
∇Fη,µkω(· ; wk). We set this parameter as follows, based on the smoothing strategy: (a) Lk = L0 with the
constant smoothing strategy, and (b) Lk = k L0 with the adaptive smoothing strategy (cf. Prop. 2). We note
that the latter choice has the effect of decaying the learning rate as 1/k in the kth outer iteration.

37

7.4 Experimental study of different methods

Convex Optimization For the named entity recognition task, Fig. 2 plots the performance of various
methods on CoNLL 2003. On the other hand, Fig. 3 presents plots for various classes of PASCAL VOC
2007 for visual object localization.

The plots reveal that smoothing-based methods converge faster in terms of training error while achieving
a competitive performance in terms of the performance metric on a held-out set. Furthermore, BCFW and
SGD make twice as many actual passes as SVRG based algorithms.

Non-Convex Optimization Fig. 4 plots the performance of various algorithms on the task of visual object
localization on PASCAL VOC.

7.5 Experimental Study of Effect of Hyperparameters: Convex Optimization

We now study the effects of various hyperparameter choices.

Effect of Smoothing Fig. 5a plots the effect of the level of smoothing for Casimir-SVRG-const and
Casimir-SVRG-adapt. The plots reveal that, in general, small values of the smoothing parameter lead to
better optimization performance for Casimir-SVRG-const. Casimir-SVRG-adapt is robust to the choice of
µ. Fig. 5b shows how the smooth optimization algorithms work when used heuristically on the non-smooth
problem.

Effect of Warm Start Strategies Fig. 5c plots different warm start strategies for Casimir-SVRG-const and
Casimir-SVRG-adapt. We ﬁnd that Casimir-SVRG-adapt is robust to the choice of the warm start strategy
while Casimir-SVRG-const is not. For the latter, we observe that Extrapolation is less stable (i.e., tends
to diverge more) than Prox-center, which is in turn less stable than Prev-iterate, which always
works (cf. Fig. 5c). However, when they do work, Extrapolation and Prox-center provide greater
acceleration than Prev-iterate. We use Prox-center as the default choice to trade-off between
acceleration and applicability.

Effect of K Fig. 5d illustrates the robustness of the method to choice of K: we observe that the results are
all within one standard deviation of each other.

7.6 Experimental Study of Effect of Hyperparameters: Non-Convex Optimization

We now study the effect of various hyperparameters for the non-convex optimization algorithms. All of
these comparisons have been made for λ = 1/n.

Effect of Smoothing Fig. 6a compares the adaptive and constant smoothing strategies. Fig. 6b and Fig. 6c
compare the effect of the level of smoothing on the the both of these. As previously, the adaptive smoothing
strategy is more robust to the choice of the smoothing parameter.

Effect of Prox-Linear Learning Rate η Fig. 7a shows the robustness of the proposed method to the
choice of η.

38

Effect of Iteration Budget Fig. 7b also shows the robustness of the proposed method to the choice of
iteration budget of the inner solver, Casimir-SVRG-const.

Effect of Warm Start of the Inner Solver Fig. 7c studies the effect of the warm start strategy used within
the inner solver Casimir-SVRG-const in each inner prox-linear iteration. The results are similar to those
obtained in the convex case, with Prox-center choice being the best compromise between acceleration
and compatibility.

8 Future Directions

We introduced a general notion of smooth inference oracles in the context of black-box ﬁrst-order opti-
mization. This allows us to set the scene to extend the scope of fast incremental optimization algorithms to
structured prediction problems owing to a careful blend of a smoothing strategy and an acceleration scheme.
We illustrated the potential of our framework by proposing a new incremental optimization algorithm to
train structural support vector machines both enjoying worst-case complexity bounds and demonstrating
competitive performance on two real-world problems. This work paves also the way to faster incremental
primal optimization algorithms for deep structured prediction models.

There are several potential venues for future work. When there is no discrete structure that admits
efﬁcient inference algorithms, it could be beneﬁcial to not treat inference as a black-box numerical proce-
dure [Meshi et al., 2010, Hazan and Urtasun, 2010, Hazan et al., 2016]. Instance-level improved algorithms
along the lines of Hazan et al. [2016] could also be interesting to explore.

Acknowledgments This work was supported by NSF Award CCF-1740551, the Washington Research
Foundation for innovation in Data-intensive Discovery, and the program “Learning in Machines and Brains”
of CIFAR.

39

Figure 3: Comparison of convex optimization algorithms for the task of visual object localization on PAS-
CAL VOC 2007 for λ = 10/n. Plots for all other classes are in Appendix E.

40

Figure 4: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n. Plots for all other classes are in Appendix E.

41

(a) Effect of level of smoothing.

(b) Effect of smoothing: use of smooth optimization with smoothing (labeled “smooth”) versus the
heuristic use of these algorithms without smoothing (labeled “non-smooth”) for λ = 0.01/n.

(c) Effect of warm start strategies for λ = 0.01/n (ﬁrst row) and λ = 1/n (second row).

(d) Effect of K in the top-K oracle (λ = 0.01/n).

Figure 5: Effect of hyperparameters for the task of Named Entity Recognition on CoNLL 2003. C-SVRG
stands for Casimir-SVRG in these plots.

42

(a) Comparison of adaptive and constant smoothing strategies.

(b) Effect of µ of the adaptive smoothing strategy.

(c) Effect of µ of the constant smoothing strategy.

Figure 6: Effect of smoothing on PL-Casimir-SVRG for the task of visual object localization on PASCAL
VOC 2007.

43

(a) Effect of the hyperparameter η.

(b) Effect of the iteration budget of the inner solver.

44

(c) Effect of the warm start strategy of the inner Casimir-SVRG-const algorithm.

Figure 7: Effect of hyperparameters on PL-Casimir-SVRG for the task of visual object localization on
PASCAL VOC 2007.

References

Z. Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. Journal of Machine

Learning Research, 18:221:1–221:51, 2017.

Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden Markov Support Vector Machines. In International

Conference on Machine Learning, pages 3–10, 2003.

D. Batra. An efﬁcient message-passing algorithm for the M -best MAP problem. In Conference on Uncer-

tainty in Artiﬁcial Intelligence, pages 121–130, 2012.

D. Batra, P. Yadollahpour, A. Guzm´an-Rivera, and G. Shakhnarovich. Diverse M -best Solutions in Markov

Random Fields. In European Conference on Computer Vision, pages 1–16, 2012.

A. Beck and M. Teboulle. Smoothing and ﬁrst order methods: A uniﬁed framework. SIAM Journal on

Optimization, 22(2):557–580, 2012.

D. Belanger and A. McCallum. Structured prediction energy networks.

In International Conference on

Machine Learning, pages 983–992, 2016.

R. Bellman. Dynamic Programming. Courier Dover Publications, 1957.

Y. Bengio, Y. LeCun, C. Nohl, and C. Burges. LeRec: A NN/HMM Hybrid for On-Line Handwriting

Recognition. Neural Computation, 7(6):1289–1303, 1995.

D. P. Bertsekas. Dynamic programming and optimal control, volume 1. Athena scientiﬁc Belmont, MA,

1995.

D. P. Bertsekas. Nonlinear programming. Athena Scientiﬁc Belmont, 1999.

L. Bottou and P. Gallinari. A Framework for the Cooperation of Learning Algorithms.

In Advances in

Neural Information Processing Systems, pages 781–788, 1990.

L. Bottou, Y. Bengio, and Y. LeCun. Global Training of Document Processing Systems Using Graph
In Conference on Computer Vision and Pattern Recognition, pages 489–494,

Transformer Networks.
1997.

G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000.

J. V. Burke. Descent methods for composite nondifferentiable optimization problems. Mathematical Pro-

gramming, 33(3):260–279, 1985.

C. Chen, V. Kolmogorov, Y. Zhu, D. N. Metaxas, and C. H. Lampert. Computing the M Most Probable
Modes of a Graphical Model. In International Conference on Artiﬁcial Intelligence and Statistics, pages
161–169, 2013.

Y.-Q. Cheng, V. Wu, R. Collins, A. R. Hanson, and E. M. Riseman. Maximum-weight bipartite matching
technique and its application in image feature matching. In Visual Communications and Image Process-
ing, volume 2727, pages 453–463, 1996.

45

M. Collins, A. Globerson, T. Koo, X. Carreras, and P. L. Bartlett. Exponentiated gradient algorithms for
conditional random ﬁelds and max-margin markov networks. Journal of Machine Learning Research, 9
(Aug):1775–1822, 2008.

R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. P. Kuksa. Natural language process-

ing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537, 2011.

G. F. Cooper. The computational complexity of probabilistic inference using bayesian belief networks.

Artiﬁcial Intelligence, 42(2-3):393–405, 1990.

B. Cox, A. Juditsky, and A. Nemirovski. Dual subgradient algorithms for large-scale nonsmooth learning

problems. Mathematical Programming, 148(1-2):143–180, 2014.

K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines.

Journal of Machine Learning Research, 2(Dec):265–292, 2001.

H. Daum´e III and D. Marcu. Learning as search optimization: approximate large margin methods for

structured prediction. In International Conference on Machine Learning, pages 169–176, 2005.

D. Davis and D. Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. arXiv

preprint arXiv:1803.06523, 2018.

and Computing, 2(1):25–36, 1992.

A. P. Dawid. Applications of a general propagation algorithm for probabilistic expert systems. Statistics

A. Defazio. A simple practical accelerated method for ﬁnite sums.

In Advances in Neural Information

Processing Systems, pages 676–684, 2016.

A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for
non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages
1646–1654, 2014.

T. Deselaers, B. Alexe, and V. Ferrari. Localizing objects while learning their appearance. In European

Conference on Computer Vision, pages 452–466, 2010.

D. Drusvyatskiy and C. Paquette. Efﬁciency of minimizing compositions of convex functions and smooth

maps. Mathematical Programming, Jul 2018.

J. C. Duchi, D. Tarlow, G. Elidan, and D. Koller. Using Combinatorial Optimization within Max-Product

Belief Propagation. In Advances in Neural Information Processing Systems, pages 369–376, 2006.

M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The Pascal Visual Object Classes

(VOC) challenge. International Journal of Computer Vision, 88(2):303–338, 2010.

N. Flerova, R. Marinescu, and R. Dechter. Searching for the M Best Solutions in Graphical Models. Journal

of Artiﬁcial Intelligence Research, 55:889–952, 2016.

M. Fromer and A. Globerson. An LP view of the M -best MAP problem. In Advances in Neural Information

Processing Systems, pages 567–575, 2009.

46

R. Frostig, R. Ge, S. Kakade, and A. Sidford. Un-regularizing: approximate proximal point and faster
stochastic algorithms for empirical risk minimization. In International Conference on Machine Learning,
pages 2540–2548, 2015.

R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Conference on Computer Vision and Pattern Recognition, pages 580–587,
2014.

D. M. Greig, B. T. Porteous, and A. H. Seheult. Exact maximum a posteriori estimation for binary images.

Journal of the Royal Statistical Society. Series B (Methodological), pages 271–279, 1989.

T. Hazan and R. Urtasun. A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Struc-

tured Prediction. In Advances in Neural Information Processing Systems, pages 838–846, 2010.

T. Hazan, A. G. Schwing, and R. Urtasun. Blending Learning and Inference in Conditional Random Fields.

Journal of Machine Learning Research, 17:237:1–237:25, 2016.

L. He, K. Lee, M. Lewis, and L. Zettlemoyer. Deep Semantic Role Labeling: What Works and What’s Next.

In Annual Meeting of the Association for Computational Linguistics, pages 473–483, 2017.

N. He and Z. Harchaoui. Semi-Proximal Mirror-Prox for Nonsmooth Composite Minimization. In Advances

in Neural Information Processing Systems, pages 3411–3419, 2015.

M. Held, P. Wolfe, and H. P. Crowder. Validation of subgradient optimization. Mathematical Programming,

6(1):62–88, Dec 1974.

T. Hofmann, A. Lucchi, S. Lacoste-Julien, and B. McWilliams. Variance reduced stochastic gradient descent

with neighbors. In Advances in Neural Information Processing Systems, pages 2305–2313, 2015.

H. Ishikawa and D. Geiger. Segmentation by grouping junctions. In Conference on Computer Vision and

Pattern Recognition, pages 125–131, 1998.

M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for the Ising model. SIAM Journal

on computing, 22(5):1087–1116, 1993.

T. Joachims, T. Finley, and C.-N. J. Yu. Cutting-plane training of structural SVMs. Machine Learning, 77

(1):27–59, 2009.

J. K. Johnson. Convex relaxation methods for graphical models: Lagrangian and maximum entropy ap-

proaches. PhD thesis, Massachusetts Institute of Technology, Cambridge, MA, USA, 2008.

R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In

Advances in Neural Information Processing Systems, pages 315–323, 2013.

V. Jojic, S. Gould, and D. Koller. Accelerated dual decomposition for MAP inference. In International

Conference on Machine Learning, pages 503–510, 2010.

D. Jurafsky, J. H. Martin, P. Norvig, and S. Russell. Speech and Language Processing. Pearson Education,

2014. ISBN 9780133252934.

ISBN 978-0-262-01319-2.

D. Koller and N. Friedman. Probabilistic Graphical Models - Principles and Techniques. MIT Press, 2009.

47

V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? IEEE Transactions

on Pattern Analysis and Machine Intelligence, 26(2):147–159, 2004.

A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet classiﬁcation with deep convolutional neural

networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012.

S. Lacoste-Julien, M. Schmidt, and F. Bach. A simpler approach to obtaining an O(1/t) convergence rate

for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002, 2012.

S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Block-Coordinate Frank-Wolfe Optimization for

Structural SVMs. In International Conference on Machine Learning, pages 53–61, 2013.

J. Lafferty, A. McCallum, and F. C. Pereira. Conditional Random Fields: Probabilistic Models for Segment-
In International Conference on Machine Learning, pages 282–289,

ing and Labeling Sequence Data.
2001.

C. H. Lampert, M. B. Blaschko, and T. Hofmann. Beyond sliding windows: Object localization by efﬁcient

subwindow search. In Conference on Computer Vision and Pattern Recognition, pages 1–8, 2008.

N. Le Roux, M. W. Schmidt, and F. R. Bach. A Stochastic Gradient Method with an Exponential Conver-
gence Rate for Strongly-Convex Optimization with Finite Training Sets. In Advances in Neural Informa-
tion Processing Systems, pages 2672–2680, 2012.

M. Lewis and M. Steedman. A* CCG parsing with a supertag-factored model. In Conference on Empirical

Methods in Natural Language Processing, pages 990–1000, 2014.

H. Lin, J. Mairal, and Z. Harchaoui. A universal catalyst for ﬁrst-order optimization. In Advances in Neural

Information Processing Systems, pages 3384–3392, 2015.

H. Lin, J. Mairal, and Z. Harchaoui. Catalyst Acceleration for First-order Convex Optimization: from

Theory to Practice. Journal of Machine Learning Research, 18(212):1–54, 2018.

L. Lov´asz. Submodular functions and convexity. In Mathematical Programming The State of the Art, pages

235–257. Springer, 1983.

J. Mairal.

Incremental majorization-minimization optimization with application to large-scale machine

learning. SIAM Journal on Optimization, 25(2):829–855, 2015.

A. F. T. Martins and R. F. Astudillo. From Softmax to Sparsemax: A Sparse Model of Attention and
Multi-Label Classiﬁcation. In International Conference on Machine Learning, pages 1614–1623, 2016.

R. J. McEliece, D. J. C. MacKay, and J. Cheng. Turbo Decoding as an Instance of Pearl’s ”Belief Propaga-

tion” Algorithm. IEEE Journal on Selected Areas in Communications, 16(2):140–152, 1998.

A. Mensch and M. Blondel. Differentiable dynamic programming for structured prediction and attention.

In International Conference on Machine Learning, pages 3459–3468, 2018.

O. Meshi, D. Sontag, T. S. Jaakkola, and A. Globerson. Learning Efﬁciently with Approximate Inference

via Dual Losses. In International Conference on Machine Learning, pages 783–790, 2010.

O. Meshi, T. S. Jaakkola, and A. Globerson. Convergence Rate Analysis of MAP Coordinate Minimization

Algorithms. In Advances in Neural Information Processing Systems, pages 3023–3031, 2012.

48

K. P. Murphy, Y. Weiss, and M. I. Jordan. Loopy belief propagation for approximate inference: An empirical

study. In Conference on Uncertainty in Artiﬁcial Intelligence, pages 467–475, 1999.

Y. Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2). In Soviet

Mathematics Doklady, volume 27, pages 372–376, 1983.

Y. Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on Optimization,

Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103(1):127–152,

Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science &

16(1):235–249, 2005a.

2005b.

Business Media, 2013.

V. Niculae, A. F. Martins, M. Blondel, and C. Cardie. SparseMAP: Differentiable Sparse Structured Infer-

ence. In International Conference on Machine Learning, pages 3796–3805, 2018.

D. Nilsson. An efﬁcient algorithm for ﬁnding the M most probable conﬁgurations in probabilistic expert

systems. Statistics and Computing, 8(2):159–173, 1998.

A. Osokin, J.-B. Alayrac, I. Lukasewitz, P. Dokania, and S. Lacoste-Julien. Minding the gaps for block
Frank-Wolfe optimization of structured SVMs. In International Conference on Machine Learning, pages
593–602, 2016.

B. Palaniappan and F. Bach. Stochastic variance reduction methods for saddle-point problems. In Advances

in Neural Information Processing Systems, pages 1408–1416, 2016.

C. Paquette, H. Lin, D. Drusvyatskiy, J. Mairal, and Z. Harchaoui. Catalyst for gradient-based nonconvex
optimization. In International Conference on Artiﬁcial Intelligence and Statistics, pages 613–622, 2018.

J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann,

N. D. Ratliff, J. A. Bagnell, and M. Zinkevich. (Approximate) Subgradient Methods for Structured Predic-

tion. In International Conference on Artiﬁcial Intelligence and Statistics, pages 380–387, 2007.

R. T. Rockafellar and R. J.-B. Wets. Variational analysis, volume 317. Springer Science & Business Media,

1988.

2009.

O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bern-
stein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International
Journal of Computer Vision, 115(3):211–252, 2015.

B. Savchynskyy, J. H. Kappes, S. Schmidt, and C. Schn¨orr. A study of Nesterov’s scheme for Lagrangian
decomposition and MAP labeling. In Conference on Computer Vision and Pattern Recognition, pages
1817–1823, 2011.

M. I. Schlesinger. Syntactic analysis of two-dimensional visual signals in noisy conditions. Kibernetika, 4

(113-130):1, 1976.

49

M. Schmidt, R. Babanezhad, M. Ahmed, A. Defazio, A. Clifton, and A. Sarkar. Non-uniform stochastic
average gradient method for training conditional random ﬁelds. In International Conference on Artiﬁcial
Intelligence and Statistics, pages 819–828, 2015.

M. Schmidt, N. Le Roux, and F. Bach. Minimizing ﬁnite sums with the stochastic average gradient. Math-

ematical Programming, 162(1-2):83–112, 2017.

A. Schrijver. Combinatorial Optimization - Polyhedra and Efﬁciency. Springer, 2003.

B. Seroussi and J. Golmard. An algorithm directly ﬁnding the K most probable conﬁgurations in Bayesian

networks. International Journal of Approximate Reasoning, 11(3):205 – 233, 1994.

S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss minimiza-

tion. Journal of Machine Learning Research, 14(Feb):567–599, 2013.

S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized

loss minimization. In International Conference on Machine Learning, pages 64–72, 2014.

S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-gradient solver for

SVM. Mathematical programming, 127(1):3–30, 2011.

H. O. Song, R. B. Girshick, S. Jegelka, J. Mairal, Z. Harchaoui, and T. Darrell. On learning to localize
objects with minimal supervision. In International Conference on Machine Learning, pages 1611–1619,
2014.

B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In Advances in Neural Information

Processing Systems, pages 25–32, 2004.

B. Taskar, S. Lacoste-Julien, and D. Klein. A discriminative matching approach to word alignment.

In
Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing, pages 73–80, 2005.

B. Taskar, S. Lacoste-Julien, and M. I. Jordan. Structured prediction, dual extragradient and Bregman

projections. Journal of Machine Learning Research, 7(Jul):1627–1653, 2006.

C. H. Teo, S. Vishwanathan, A. Smola, and Q. V. Le. Bundle methods for regularized risk minimization.

Journal of Machine Learning Research, 1(55), 2009.

E. F. Tjong Kim Sang and F. De Meulder.

Introduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Conference on Natural Language Learning, pages 142–147,
2003.

M. Tkachenko and A. Simanovsky. Named entity recognition: Exploring features. In Empirical Methods in

Natural Language Processing, pages 118–127, 2012.

I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdepen-
dent and structured output spaces. In International Conference on Machine Learning, page 104, 2004.

K. E. Van de Sande, J. R. Uijlings, T. Gevers, and A. W. Smeulders. Segmentation as selective search for

object recognition. In International Conference on Computer Vision, pages 1879–1886, 2011.

50

A. J. Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.

IEEE Trans. Information Theory, 13(2):260–269, 1967. doi: 10.1109/TIT.1967.1054010.

M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference.

Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008.

M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. MAP estimation via agreement on trees: message-
passing and linear programming. IEEE transactions on information theory, 51(11):3697–3717, 2005.

B. E. Woodworth and N. Srebro. Tight complexity bounds for optimizing composite objectives. In Advances

in Neural Information Processing Systems, pages 3639–3647, 2016.

C. Yanover and Y. Weiss. Finding the M most probable conﬁgurations using loopy belief propagation. In

Advances in Neural Information Processing Systems, pages 289–296, 2004.

X. Zhang, A. Saha, and S. Vishwanathan. Accelerated training of max-margin markov networks with ker-

nels. Theoretical Computer Science, 519:88–102, 2014.

51

A Smoothing

We ﬁrst prove an extension of Lemma 4.2 of Beck and Teboulle [2012], which proves the following state-
ment for the special case of µ2 = 0. Recall that we deﬁned hµω ≡ h when µ = 0.

Proposition 39. Consider the setting of Def. 1. For µ1 ≥ µ2 ≥ 0, we have for every z ∈ Rm that

(µ1 − µ2)

inf
u∈dom h∗

ω(u) ≤ hµ2ω(z) − hµ1ω(z) ≤ (µ1 − µ2)

sup
u∈dom h∗

ω(u) .

Proof. We successively deduce,

hµ1ω(z) = sup

{(cid:104)u, z(cid:105) − h∗(u) − µ1ω(u)}

u∈dom h∗

= sup

u∈dom h∗

≥ sup

u∈dom h∗

(cid:26)

{(cid:104)u, z(cid:105) − h∗(u) − µ2ω(u) − (µ1 − µ2)ω(u)}

(cid:104)u, z(cid:105) − h∗(u) − µ2ω(u) + inf

u(cid:48)∈dom h∗

(cid:27)
(cid:8)−(µ1 − µ2)ω(u(cid:48))}(cid:9)

= hµ2ω(z) − (µ1 − µ2)

sup
u(cid:48)∈dom h∗

ω(u(cid:48)) ,

since µ1 − µ2 ≥ 0. The other side follows using instead that

−(µ1 − µ2)ω(u) ≤ sup

(cid:8)−(µ1 − µ2)ω(u(cid:48))}(cid:9) .

u(cid:48)∈dom h∗

Next, we recall the following equivalent deﬁnition of a matrix norm deﬁned in Eq. (2).

(cid:107)A(cid:107)β,α = sup
y(cid:54)=0

(cid:107)A(cid:62)y(cid:107)∗
β
(cid:107)y(cid:107)α

= sup
x(cid:54)=0

(cid:107)Ax(cid:107)∗
α
(cid:107)y(cid:107)β

= (cid:107)A(cid:62)(cid:107)α,β .

(65)

Now, we consider the smoothness of a composition of a smooth function with an afﬁne map.

Lemma 40. Suppose h : Rm → R is L-smooth with respect to (cid:107) · (cid:107)∗
b ∈ Rm, we have that the map Rd (cid:51) w (cid:55)→ h(Aw + b) is (cid:0)L(cid:107)A(cid:62)(cid:107)2

α. Then, for any A ∈ Rm×d and
(cid:1)-smooth with respect to (cid:107) · (cid:107)β.

α,β

Proof. Fix A ∈ Rm×d, b ∈ Rm and deﬁne f : Rd → R as f (w) = h(Aw + b). By the chain rule, we have
that ∇f (w) = A(cid:62)∇h(Aw + b). Using smoothness of h, we successively deduce,

(cid:107)∇f (w1) − ∇f (w2)(cid:107)∗

β = (cid:107)A(cid:62)(∇h(Aw1 + b) − ∇h(Aw2 + b))(cid:107)∗
β

(65)
≤ (cid:107)A(cid:62)(cid:107)α,β(cid:107)∇h(Aw1 + b) − ∇h(Aw2 + b)(cid:107)α
≤ (cid:107)A(cid:62)(cid:107)α,β L(cid:107)A(w1 − w2)(cid:107)∗
α
(65)
≤ L(cid:107)A(cid:62)(cid:107)2

α,β(cid:107)w1 − w2(cid:107)β .

Shown in Algo. 6 is the procedure to compute the outputs of the top-K oracle from the K best scoring

outputs obtained, for instance, from the top-K max-product algorithm.

52

Algorithm 6 Top-K oracle from top-K outputs
1: Input: Augmented score function ψ, w ∈ Rd, µ > 0 YK = {y1, · · · , yK} such that yk =

max(k)

y∈Y ψ(y; w).

2: Populate z ∈ RK so that zk = 1
µ ψ(yk; w).
3: Compute u∗ = arg minu∈∆K−1 (cid:107)u − z(cid:107)2
4: return s = (cid:80)K
k=1 u∗

k ψ(yk; w) and v = (cid:80)K

2 by a projection on the simplex.
k ∇wψ(yk; w).

k=1 u∗

Algorithm 7 Standard max-product algorithm

1: Input: Augmented score function ψ(·, ·; w) deﬁned on tree structured graph G with root r ∈ V.
2: Initialize: Let V be a list of nodes from V\{r} arranged in increasing order of height.
3: for v in V do
4:

Set mv(yρ(v)) ← maxyv∈Yv
Yρ(v).
Assign to δv(yρ(v)) a maximizing assignment of yv from above for each yρ(v) ∈ Yρ(v).

(cid:110)
ψv(yv) + ψv,ρ(v)(yv, yρ(v)) + (cid:80)

v(cid:48)∈C(v) mv(cid:48)(yv)

(cid:111)

for each yρ(v) ∈

(cid:110)
ψr(yr) + (cid:80)

v(cid:48)∈C(r) mv(cid:48)(yr)

(cid:111)
.

(cid:110)
ψr(yr) + (cid:80)

v(cid:48)∈C(r) mv(cid:48)(yr)

(cid:111)
.

5:
6: end for
7: ψ∗ ← maxyr∈Yr
8: y∗
r ← arg maxyr∈Yr
9: for v in reverse(V ) do
v = δv(y∗
y∗
10:
11: end for
12: return ψ∗, y∗ = (y∗

ρ(v)).

1, · · · , y∗

p).

B Smooth Inference in Trees

A graph G is a tree if it is connected, directed and each node has at most one incoming edge. It has one root
r ∈ V with no incoming edge. An undirected graph with no loops can be converted to a tree by ﬁxing an
arbitrary root and directing all edges way from the root. We say that G is a chain if it is a tree with root p
where all edges are of the form (v + 1, v). For a node v in a tree G, we denote by ρ(v) and C(v) respectively
the parent of v and the children of v in the tree.

Recall ﬁrst that the height of a node in a rooted tree is the number of edges on the longest directed
path from the node to a leaf where each edge is directed away from the root. We ﬁrst review the standard
max-product algorithm for maximum a posteriori (MAP) inference [Dawid, 1992] - Algo. 7. It runs in time
O(p maxv∈V |Yv|2) and requires space O(p maxv∈V |Yv|).

B.1 Proof of Correctness of Top-K Max-Product

We now consider the top-K max-product algorithm, shown in full generality in Algo. 8. The following
proposition proves its correctness.

Proposition 41. Consider as inputs to Algo. 8 an augmented score function ψ(·, ·; w) deﬁned on tree
structured graph G, and an integer K > 0. Then, the outputs of Algo. 8 satisfy ψ(k) = ψ(y(k)) =
max(k)
Proof. For a node v ∈ V, let τ (v) denote the sub-tree of G rooted at v. Let yτ (v) denote (cid:0)yv(cid:48) for v(cid:48) ∈ τ (v)(cid:1).

y∈Y ψ(y). Moreover, Algo. 8 runs in time O(pK log K maxv∈V |Yv|2) and uses space O(pK maxv∈V |Yv|).

53

Algorithm 8 Top-K max-product algorithm

1: Input: Augmented score function ψ(·, ·; w) deﬁned on tree structured graph G with root r ∈ V, and

integer K > 0.

2: Initialize: Let V be a list of nodes from V\{r} arranged in increasing order of height.
3: for v in V and k = 1, · · · , K do
4:

if v is a leaf then

m(k)

v (yρ(v)) ← max(k)

yv∈Yv

(cid:8)ψv(yv) + ψv,ρ(v)(yv, yρ(v))(cid:9) for each yρ(v) ∈ Yρ(v).

else

Assign for each yρ(v) ∈ Yρ(v),

m(k)

v (yρ(v)) ← max(k)

(cid:40)

ψv(yv) + ψv,ρ(v)(yv, yρ(v))
v(cid:48)∈C(v) m(lv(cid:48) )

+ (cid:80)

(yv)

v(cid:48)

yv ∈ Yv and
lv(cid:48) ∈ [K] for v(cid:48) ∈ C(v)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)

.

(66)

v(cid:48) (yρ(v)) for v(cid:48) ∈ C(v) store the maximizing assignment of yv and l(cid:48)

v from

5:

6:

7:

8:

Let δ(k)
v (yρ(v)) and κ(k)
above for each yρ(v) ∈ Yρ(v).

end if
9:
10: end for
11: For k = 1, · · · , K, set

ψ(k) ← max(k)

ψr(yr) +

(cid:26)

(cid:88)

m(lv(cid:48) )
v(cid:48)

(yr)

v(cid:48)∈C(r)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

yr ∈ Yr and lv(cid:48) ∈ [K] for v(cid:48) ∈ C(r)

(cid:27)

and assign maximizing assignments of yr, lv(cid:48) above respectively to y(k)

r

and l(k)
v(cid:48)

for v(cid:48) ∈ C(r).

12: for v in reverse(V ) and k = 1, · · · , K do
v )

(cid:0)y(k)
ρ(v)
(cid:0)y(k)
ρ(v)

(cid:1).
(cid:1) for all v(cid:48) ∈ C(v).

13:

Set y(k)
Set l(k)

v

v ← δ(l(k)
v(cid:48) = κ(l(k)
(cid:110)

v )

v(cid:48)

14:
15: end for
16: return

ψ(k), y(k) := (y(k)

1 , · · · , y(k)
p )

(cid:111)K

.

k=1

54

(67)

(68)

(69)

Deﬁne ψτ (v) as follows: if v is a leaf, yτ (v) = (yv) and ψτ (v)(yτ (v)) := ψv(yv). For a non-leaf v, deﬁne
recursively

ψτ (v)(yτ (v)) := ψv(yv) +

(cid:2)ψv,v(cid:48)(yv, yv(cid:48)) + ψτ (v(cid:48))(yτ (v(cid:48)))(cid:3) .

(cid:88)

v(cid:48)∈C(v)

We will need some identities about choosing the kth largest element from a ﬁnite collection. For ﬁnite sets
S1, · · · , Sn and functions fj : Sj → R, h : S1 × S2 → R, we have,












max(k)
u1∈S1,··· ,un∈Sn

n
(cid:88)



j=1

fj(uj)

= max(k)
l1,··· ,ln∈[k]



max(lj )
uj ∈Sj

fj(uj)

,



n
(cid:88)



j=1

(cid:26)

max(k)
u1∈S1,u2∈S2

{f1(u1) + h(u1, u2)} = max(k)
u1∈S1,l∈[k]

f1(u1) + max(l)

h(u1, u2)

.

u2∈S2

(cid:27)

The identities above state that for a sum to take its kth largest value, each component of the sum must take
one of its k largest values. Indeed, if one of the components of the sum took its lth largest value for l > k,
replacing it with any of the k largest values cannot decrease the value of the sum. Eq. (69) is a generalized
version of Bellman’s principle of optimality (see Bellman [1957, Chap. III.3.] or Bertsekas [1995, Vol. I,
Chap. 1]).

For the rest of the proof, yτ (v)\yv is used as shorthand for {yv(cid:48) | v(cid:48) ∈ τ (v)\{v}}. Moreover, maxyτ (v)
represents maximization over yτ (v) ∈ ×v(cid:48)∈τ (v) Yv(cid:48). Likewise for maxyτ (v)\yv . Now, we shall show by
induction that for all v ∈ V, yv ∈ Yv and k = 1, · · · , K,

max(k)
yτ (v)\yv

ψτ (v)(yτ (v)) = ψv(yv) + max(k)

(cid:26) (cid:88)

v(cid:48)∈C(v)

m(lv(cid:48) )
v(cid:48)

(cid:12)
(cid:12)
lv(cid:48) ∈ [K] for v(cid:48) ∈ C(v)
(yv(cid:48))
(cid:12)
(cid:12)

(cid:27)

.

(70)

The induction is based on the height of a node. The statement is clearly true for a leaf v since C(v) = ∅.
Suppose (70) holds for all nodes of height ≤ h. For a node v of height h + 1, we observe that τ (v)\v can
be partitioned into {τ (v(cid:48)) for v(cid:48) ∈ C(v)} to get,

max(k)
yτ (v)\yv

ψτ (v)(yτ (v)) − ψv(yv)

(67)
= max(k)
yτ (v)\yv

(cid:26) (cid:88)

v(cid:48)∈C(v)

ψv,v(cid:48)(yv, yv(cid:48)) + ψτ (v(cid:48))(yτ (v(cid:48)))

(cid:27)

(cid:27)

{ψv,v(cid:48)(yv, yv(cid:48)) + ψτ (v(cid:48))(yτ (v(cid:48)))}

lv(cid:48) ∈ [K] for v(cid:48) ∈ C(v)

.

(71)

(68)
= max(k)

(cid:26) (cid:88)

v(cid:48)∈C(v)

max(lv(cid:48) )
yτ (v(cid:48))
(cid:124)

(cid:123)(cid:122)
=:Tv(cid:48) (yv)

Let us analyze the term in the underbrace, Tv(cid:48)(yv). We successively deduce, with the argument l in the
maximization below taking values in {1, · · · , K},

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:125)

(cid:27)

Tv(cid:48)(yv)

(69)
= max(lv(cid:48) )
yv(cid:48) ,l

ψv,v(cid:48)(yv, yv(cid:48)) + max(l)
yτ (v(cid:48))\yv(cid:48)

ψτ (v(cid:48))(yτ (v(cid:48)))

ψv(cid:48)(yv(cid:48)) + ψv,v(cid:48)(yv, yv(cid:48))+

(cid:27)
(yv(cid:48)) | lv(cid:48)(cid:48) ∈ [K] for v(cid:48)(cid:48) ∈ C(v(cid:48))(cid:9)

max(l) (cid:8) (cid:80)

v(cid:48)(cid:48)
(cid:26)ψv(cid:48)(yv(cid:48)) + ψv(cid:48),v(yv(cid:48), yv)
v(cid:48)(cid:48)∈C(v(cid:48)) m(lv(cid:48)(cid:48) )
(yv(cid:48))

v(cid:48)(cid:48)∈C(v(cid:48)) m(lv(cid:48)(cid:48) )
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ (cid:80)

v(cid:48)(cid:48)

yv(cid:48) ∈ Yv(cid:48) and
lv(cid:48)(cid:48) ∈ [K] for v(cid:48)(cid:48) ∈ C(v)

(cid:27)

(cid:26)

(cid:26)

(70)
= max(lv(cid:48) )
yv(cid:48) ,l

(69)
= max(lv(cid:48) )

(66)
= m(lv(cid:48) )
v(cid:48)

(yv) .

55

Here, the penultimate step followed from applying in reverse the identity (69) with u1, u2 being by yv(cid:48), {lv(cid:48)(cid:48) for v(cid:48)(cid:48) ∈
C(v(cid:48))} respectively, and f1 and h respectively being ψv(cid:48)(yv(cid:48)) + ψv(cid:48),v(yv(cid:48), yv) and (cid:80)
(yv(cid:48)). Plug-
ging this into (71) completes the induction argument. To complete the proof, we repeat the same argument
over the root as follows. We note that τ (r) is the entire tree G. Therefore, yτ (r) = y and ψτ (r) = ψ. We
now apply the identity (69) with u1 and u2 being yr and yτ (r)\r respectively and f1 ≡ 0 to get

v(cid:48)(cid:48) m(lv(cid:48)(cid:48) )
v(cid:48)(cid:48)

max(k)
y∈Y

ψ(y)

(69)
= max(k)
yr,l

max(l)
y\yr

ψ(y)

= max(k)
yr,l

max(l)
yτ (r)\yr

ψτ (r)(yτ (r))

(cid:27)

(cid:40)

(cid:41)

(cid:26)

(cid:26)

(cid:26)

(70)
= max(k)
yr,l

(69)
= max(k)

= ψ(k) ,

ψr(yr) + max(l) (cid:8) (cid:80)

v∈C(r) m(lv)

v

(yr) | lv ∈ [K] for v ∈ C(r)(cid:9)

(cid:27)

ψr(yr)+
v∈C(r) m(lv)

v

(cid:80)

(yr)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

yr ∈ Yr and
lv ∈ [K] for v ∈ C(r)

(cid:27)

where the last equality follows from Line 11 of Algo. 8.

The algorithm requires storage of m(k)

v , an array of size maxv∈V |Yv| for each k = 1, · · · , K, and v ∈ V.
The backpointers δ, κ are of the same size. This adds up to a total storage of O(pK maxv|Yv|). To bound
the running time, consider Line 7 of Algo. 8. For a ﬁxed v(cid:48) ∈ C(v), the computation

max(k)
yv,lv(cid:48)

(cid:110)
ψv(yv) + ψv,ρ(v)(yv, yρ(v)) + m(lv(cid:48) )

(yv)

(cid:111)

v(cid:48)

for k = 1, · · · , K takes time O(K log K maxv|Yv|). This operation is repeated for each yv ∈ Yv and once
for every (v, v(cid:48)) ∈ E. Since |E| = p − 1, the total running time is O(pK log K maxv|Yv|2).

B.2 Proof of Correctness of Entropy Smoothing of Max-Product

Next, we consider entropy smoothing.

Proposition 42. Given an augmented score function ψ(·, ·; w) deﬁned on tree structured graph G and
µ > 0 as input, Algo. 3 correctly computes f−µH (w) and ∇f−µH (w). Furthermore, Algo. 3 runs in
time O(p maxv∈V |Yv|2) and requires space O(p maxv∈V |Yv|).

Proof. The correctness of the function value f−µH follows from the bijection f−µH (w) = µ Aψ/µ(w) (cf.
Prop. 9), where Thm. 43 shows correctness of Aψ/µ. To show the correctness of the gradient, deﬁne the
probability distribution Pψ,µ as the probability distribution from Lemma 7(ii) and Pψ,µ,v, Pψ,µ,v,v(cid:48) as its
node and edge marginal probabilities respectively as

Pψ,µ(y; w) =

exp

(cid:16) 1

(cid:17)
µ ψ(y; w)

(cid:16) 1

y(cid:48)∈Y exp

µ ψ(y(cid:48); w)

(cid:17) ,

Pψ,µ,v(yv; w) =

Pψ,µ(y; w)

for yv ∈ Yv, v ∈ V , and,

Pψ,µ,v,v(cid:48)(yv, yv(cid:48); w) =

Pψ,µ(y; w)

for yv ∈ Yv, yv(cid:48) ∈ Yv(cid:48), (v, v(cid:48)) ∈ E .

(cid:80)

(cid:88)

y∈Y :
yv=yv
(cid:88)

y∈Y:
yv=yv,
yv(cid:48) =yv(cid:48)

56

Thm. 43 again shows that Algo. 9 correctly produces marginals Pψ,µ,v and Pψ,µ,v,v(cid:48). We now start with
Lemma 7(ii) and invoke (17) to get

∇f−µH (w) =

Pψ,µ(y; w)∇ψ(y; w)

(cid:88)

y∈Y

(cid:88)

y∈Y
(cid:88)

=

=

=

(cid:88)

v∈V
(cid:88)

y∈Y
(cid:88)

(cid:88)





(cid:88)

v∈V

Pψ,µ(y; w)

∇ψv(yv; w) +

∇ψv,v(cid:48)(yv, yv(cid:48); w)

 ,



Pψ,µ(y; w)∇ψv(yv; w) +

Pψ,µ(y; w)∇ψv,v(cid:48)(yv, yv(cid:48); w)

(cid:88)

(v,v(cid:48))∈E
(cid:88)

(cid:88)

(v,v(cid:48))∈E

y∈Y

Pψ,µ(y; w)∇ψv(yv; w)

v∈V

yv∈Yv

y∈Y : yv=yv
(cid:88)
(cid:88)

+

(cid:88)

(cid:88)

(v,v(cid:48))∈E

yv∈Yv

yv(cid:48) ∈Yv(cid:48)

y∈Y :

yv=yv
yv(cid:48) =yv(cid:48)

Pψ,µ,v(yv; w)∇ψv(yv; w)

(cid:88)

(cid:88)

=

v∈V

yv∈Yv

(cid:88)

(cid:88)

(cid:88)

+

(v,v(cid:48))∈E

yv∈Yv

yv(cid:48) ∈Yv(cid:48)

Pψ,µ,v,v(cid:48)(yv, yv(cid:48); w)∇ψv,v(cid:48)(yv, yv(cid:48); w) .

Pψ,µ(y; w)∇ψv,v(cid:48)(yv, yv(cid:48); w)

Here, the penultimate equality followed from breaking the sum over y ∈ Y into an outer sum that sums over
every yv ∈ Yv and an inner sum over y ∈ Y : yv = yv, and likewise for the edges. The last equality above
followed from the deﬁnitions of the marginals. Therefore, Line 3 of Algo. 3 correctly computes the gradient.
The storage complexity of the algorithm is O(p maxv|Yv|) provided that the edge marginals Pψ,µ,v,v(cid:48) are
computed on the ﬂy as needed. The time overhead of Algo. 3 after Algo. 9 is O(p maxv|Yv|2), by noting
that each edge marginal can be computed in constant time (Remark 44).

Given below is the guarantee of the sum-product algorithm (Algo. 9). See, for instance, Koller and

Friedman [2009, Ch. 10] for a proof.

Theorem 43. Consider an augmented score function ψ deﬁned over a tree structured graphical model G.
Then, the output of Algo. 9 satisﬁes

Pv(yv) =

exp(ψ(y) − A)

for all yv ∈ Yv, v ∈ V, and,

Pv,v(cid:48)(yv, yv(cid:48)) =

exp(ψ(y) − A)

for all yv ∈ Yv, yv(cid:48) ∈ Yv(cid:48), (v, v(cid:48)) ∈ E.

A = log

exp(ψ(y)) ,

(cid:88)

y∈Y
(cid:88)

y∈Y : yv=yv
(cid:88)

y∈Y :

yv=yv,
yv(cid:48) =yv(cid:48)

Furthermore, Algo. 9 runs in time O(p maxv∈V |Yv|2) and requires an intermediate storage of O(p maxv∈V |Yv|).

57

Algorithm 9 Sum-product algorithm

1: Procedure: SUMPRODUCT
2: Input: Augmented score function ψ deﬁned on tree structured graph G with root r ∈ V.
3: Notation: Let N (v) = C(v) ∪ {ρ(v)} denote all the neighbors of v ∈ V if the orientation of the edges

were ignored.

4: Initialize: Let V be a list of nodes from V arranged in increasing order of height.
5: for v in V \{r} do
6:

Set for each yρ(v) ∈ Yρ(v):

mv→ρ(v)(yρ(v)) ←

(cid:88)

yv∈Yv


exp (cid:0)ψv(yv) + ψv,ρ(v)(yv, yρ(v))(cid:1) (cid:89)

mv(cid:48)→v(yv)

 .

v(cid:48)∈C(v)

exp (ψr(yr)) (cid:81)

(cid:105)
v(cid:48)∈C(r) mv(cid:48)→r(yr)

.

(cid:104)

7: end for
8: A ← log (cid:80)
9: for v in reverse(V ) do
for v(cid:48) ∈ C(v) do
10:

yr∈Yr

11:

Set for each yv(cid:48) ∈ Yv(cid:48):

mv→v(cid:48)(yv(cid:48)) =

(cid:88)

yv∈Yv


exp (cid:0)ψv(yv) + ψv(cid:48),v(yv(cid:48), yv)(cid:1) (cid:89)

mv(cid:48)(cid:48)→v(yv)

 .

v(cid:48)(cid:48)∈N (v)\{v(cid:48)}

end for

12:
13: end for
14: for v in V do
15:
16: end for
17: for (v, v(cid:48)) in E do
18:

For every pair (yv, yv(cid:48)) ∈ Yv × Yv(cid:48), set

Set Pv(yv) ← exp (ψv(yv) − A) (cid:81)

v(cid:48)(cid:48)∈N (v) mv(cid:48)(cid:48)→v(yv) for every yv ∈ Yv.

Pv,v(cid:48)(yv, yv(cid:48)) ← exp (cid:0)ψv(yv) + ψv(cid:48)(yv(cid:48)) + ψv,v(cid:48)(yv, yv(cid:48)) − A(cid:1)

(cid:89)

mv(cid:48)(cid:48)→v(yv)

mv(cid:48)(cid:48)→v(cid:48)(yv(cid:48)) .

(cid:89)

v(cid:48)(cid:48)∈N (v)\{v(cid:48)}

v(cid:48)(cid:48)∈N (v(cid:48))\{v}

19: end for
20: return A, {Pv for v ∈ V}, {Pv,v(cid:48) for (v, v(cid:48)) ∈ E}.





58

Remark 44. Line 18 of Algo. 9 can be implemented in constant time by reusing the node marginals Pv and
messages mv→v(cid:48), mv(cid:48)→v as

Pv,v(cid:48)(yv, yv(cid:48)) =

Pv(yv)Pv(cid:48)(yv(cid:48)) exp(ψv,v(cid:48)(yv, yv(cid:48)) + A)
mv(cid:48)→v(yv)mv→v(cid:48)(yv(cid:48))

.

C Inference Oracles in Loopy Graphs

This section presents the missing details and recalls from literature the relevant algorithms and results re-
quired in Sec. 4.2. First, we review the BMMF algorithm of Yanover and Weiss [2004], followed by graph
cut inference and graph matching inference.

We now recall and prove the correctness of the decoding scheme (21) for completeness. The result is

due to Pearl [1988], Dawid [1992].

Theorem 45. Consider an unambiguous augmented score function ψ , that is, ψ(y(cid:48); w) (cid:54)= ψ(y(cid:48)(cid:48); w)
for all distinct y(cid:48), y(cid:48)(cid:48) ∈ Y. Then, the result (cid:98)y of the decoding (cid:98)yv = arg maxj∈Yv ψv;j satisﬁes (cid:98)y =
arg maxy∈Y ψ(y).

Proof. Suppose for the sake of contradiction that (cid:98)y (cid:54)= y∗ := arg maxy∈Y ψ(y). Let v ∈ V be such that
yv = j and y∗
v = j(cid:48) where j (cid:54)= j(cid:48). By the fact that y∗ has the highest augmented score and unambiguity, we
get that

max
y∈Y,yv=j(cid:48)

ψ(y) = ψ(y∗) > ψ((cid:98)y) = max

y∈Y,yv=j

ψ(y) ,

which contradicts the deﬁnition of (cid:98)yv.

C.1 Review of Best Max-Marginal First

If one has access to an algorithm M that can compute max-marginals, the top-K oracle is easily imple-
mented via the Best Max Marginal First (BMMF) algorithm of Yanover and Weiss [2004], which is recalled
in Algo. 10. This algorithm requires computations of two sets of max-marginals per iteration, where a set
of max-marginals refers to max-marginals for all variables yv in y.

Details The algorithm runs by maintaining a partitioning of the search space Y and a table ϕ(k)(v, j)
that stores the best score in partition k (deﬁned by constraints C(k)) subject to the additional constraint that
yv = j. In iteration k, the algorithm looks at the k − 1 existing partitions and picks the best partition
sk (Line 9). This partition is further divided into two parts: the max-marginals in the promising partition
(corresponding to yvk = jk) are computed (Line 11) and decoded (Line 12) to yield kth best scoring y(k).
The scores of the less promising partition are updated via a second round of max-marginal computations
(Line 14).

Guarantee The following theorem shows that Algo. 10 provably implements the top-K oracle as long
as the max-marginals can be computed exactly under the assumption of unambiguity. With approximate
max-marginals however, Algo. 10 comes with no guarantees.

59

Algorithm 10 Best Max Marginal First (BMMF)

1: Input: Augmented score function ψ, parameters w, non-negative integer K, algorithm M to compute

max-marginals of ψ.

2: Initialization: C(1) = ∅ and U (2) = ∅.
3: for v ∈ [p] do
4:

5:
6: end for
7: for k = 2, · · · , K do

For j ∈ Yv, set ϕ(1)(v; j) = max{ψ(y; w) | y ∈ Y s.t. yv = j} using M.
Set y(1)

v = arg maxj∈Yv ϕ(1)(v, j).

8:

9:

10:

11:

12:

13:

14:

(cid:110)
(v, j, s) ∈ [p] × Yv × [k − 1] (cid:12)

Deﬁne search space S (k) =
Find indices (vk, jk, sk) = arg max(v,j,s)∈S(k) ϕ(s)(v, j) and set constraints C(k) = C(sk) ∪ {yvk =
jk}.
for v ∈ [p] do

(cid:54)= j, and (v, j, s) /∈ U (t)(cid:111)
.

(cid:12) y(s)

v

For each j ∈ Yv, use M to set ϕ(k)(v, j) = max (cid:8)ψ(y; w) | y ∈ Y s.t. constraints C(k) hold and yv = j(cid:9).

Set y(k)

v = arg maxj∈Yv ϕ(k)(v, j).

end for
Update U (k+1) = U (k) ∪ {(vk, jk, sk)} and C(sk) = C(sk) ∪ {yvk (cid:54)= jk} and the max-marginal table
ϕ(sk)(v, j) = maxy∈Y,C(sk ),yv=j ψ(y; w) using M.

15: end for
16: return (cid:8)(cid:0)ψ(y(k); w), y(k)(cid:1)(cid:9)K

k=1.

Theorem 46 (Yanover and Weiss [2004]). Suppose the score function ψ is unambiguous, that is, ψ(y(cid:48); w) (cid:54)=
ψ(y(cid:48)(cid:48); w) for all distinct y(cid:48), y(cid:48)(cid:48) ∈ Y. Given an algorithm M that can compute the max-marginals of ψ
exactly, Algo. 10 makes at most 2K calls to M and its output satisﬁes ψ(yk; w) = max(k)
y∈Y ψ(y; w).
Thus, the BMMF algorithm followed by a projection onto the simplex (Algo. 6 in Appendix A) is a correct
implementation of the top-K oracle. It makes 2K calls to M.

Constrained Max-Marginals The algorithm requires computation of max-marginals subject to constraints
of the form yv ∈ Yv for some set Yv ⊆ Yv. This is accomplished by redeﬁning for a constraint yv ∈ Yv:

(cid:40)

ψ(y) =

if yv ∈ Yv
ψ(y),
−∞, otherwise

.

C.2 Max-Marginals Using Graph Cuts

This section recalls a simple procedure to compute max-marginals using graph cuts. Such a construction

was used, for instance, by Kolmogorov and Zabin [2004].

Notation In the literature on graph cut inference, it is customary to work with the energy function, which
is deﬁned as the negative of the augmented score −ψ. For this section, we also assume that the labels are
binary, i.e., Yv = {0, 1} for each v ∈ [p]. Recall the decomposition (17) of the augmented score function

60

Algorithm 11 Max-marginal computation via Graph Cuts
1: Input: Augmented score function ψ(·, ·; w) with Y = {0, 1}p, constraints C of the form yv = b for

b ∈ {0, 1}.

Add to E(cid:48) the (edge, cost) pairs (s → yv, θv;0) and (yv → t, θv;1).

2: Using artiﬁcial source s and sink t, set V (cid:48) = V ∪ {s, t} and E(cid:48) = ∅.
3: for v ∈ [p] do
4:
5: end for
6: for v, v(cid:48) ∈ R such that v < v(cid:48) do
7:

Add to E(cid:48) the (edge, cost) pairs (s → yv, θvv(cid:48);00), (yv(cid:48) → t, θvv(cid:48);11), (yv → yv(cid:48), θvv(cid:48);10), (yv(cid:48) →
yv, θvv(cid:48);01 − θvv(cid:48);00 − θvv(cid:48);11).

Add to E(cid:48) the edge yv → t if b = 0 or edge s → yv if b = 1 with cost +∞.

8: end for
9: for constraint yv = b in C do
10:
11: end for
12: Create graph G(cid:48) = (V (cid:48), E(cid:48)), where parallel edges are merged by adding weights.
13: Compute minimum cost s, t-cut of G(cid:48). Let C be its cost.
14: Create (cid:98)y ∈ {0, 1}p as follows: for each v ∈ V, set (cid:98)yv = 0 if the edge s → v is cut. Else (cid:98)yv = 1.
15: return −C, (cid:98)y.

over nodes and edges. Deﬁne a reparameterization

θv;z(w) = −ψv(z; w) for v ∈ V, z ∈ {0, 1}
if (v, v(cid:48)) ∈ E
θvv(cid:48);z,z(cid:48)(w) = −ψv,v(cid:48)(z, z(cid:48); w) ,

for (v, v(cid:48)) ∈ E, (z, z(cid:48)) ∈ {0, 1}2 .

We then get

−ψ(y) =

θv;z I(yv = z) +

θvv(cid:48);zz(cid:48) I(yv = z) I(yv(cid:48) = z(cid:48)) I((v, v(cid:48)) ∈ E) ,

p
(cid:88)

(cid:88)

v=1

z∈{0,1}

p
(cid:88)

p
(cid:88)

(cid:88)

v=1

v(cid:48)=i+1

z,z(cid:48)∈{0,1}

where we dropped the dependence on w for simplicity. We require the energies to be submodular, i.e., for
every v, v(cid:48) ∈ [p], we have that

θvv(cid:48);00 + θvv(cid:48);11 ≤ θvv(cid:48);01 + θvv(cid:48);10 .

(72)

Also, assume without loss of generality that θv;z, θvv(cid:48);zz(cid:48) are non-negative [Kolmogorov and Zabin, 2004].

Algorithm and Correctness Algo. 11 shows how to compute the max-marginal relative to a single vari-
able yv. The next theorem shows its correctness.

Theorem 47 (Kolmogorov and Zabin [2004]). Given a binary pairwise graphical model with augmented
score function ψ which satisﬁes (72), and a set of constraints C, Algo. 11 returns maxy∈YC ψ(y; w), where
YC denotes the subset of Y that satisﬁes constraints C. Moreover, Algo. 11 requires one maximum ﬂow
computation.

61

C.3 Max-Marginals Using Graph Matchings

The alignment problem that we consider in this section is as follows: given two sets V, V (cid:48), both of equal size
(for simplicity), and a weight function ϕ : V × V (cid:48) → R, the task is to ﬁnd a map σ : V → V (cid:48) so that each
v ∈ V is mapped to a unique z ∈ V (cid:48) and the total weight (cid:80)
v∈V ϕ(v, σ(v)) is maximized. For example, V
and V (cid:48) might represent two natural language sentences and this task is to align the two sentences.

Graphical Model This problem is framed as a graphical model as follows. Suppose V and V (cid:48) are of size
p. Deﬁne y = (y1, · · · , yp) so that yv denotes σ(v). The graph G = (V, E) is constructed as the fully
connected graph over V = {1, · · · , p}. The range Yv of each yv is simply V (cid:48) in the unconstrained case.
Note that when considering constrained max-marginal computations, Yv might be subset of V (cid:48). The score
function ψ is deﬁned as node and edge potentials as in Eq. (17). Again, we suppress dependence of ψ on w
for simplicity. Deﬁne unary and pairwise scores as

ψv(yv) = ϕ(v, yv)

and ψv,v(cid:48)(yv, yv(cid:48)) =

(cid:40)

0, if yv (cid:54)= yv(cid:48)
−∞, otherwise

.

Max Oracle The max oracle with ψ deﬁned as above, or equivalently, the inference problem (3) (cf.
Lemma 7(i)) can be cast as a maximum weight bipartite matching, see e.g., Taskar et al. [2005]. Deﬁne a
fully connected bipartite graph G = (V ∪ V (cid:48), E) with partitions V, V (cid:48), and directed edges from each v ∈ V
to each vertex z ∈ V (cid:48) with weight ϕ(v, z). The maximum weight bipartite matching in this graph G gives
the mapping σ, and thus implements the max oracle. It can be written as the following linear program:

max
{θv,z for (v,z)∈E}

(cid:88)

ϕ(v, z)θv,z ,

s.t.

(v,z)∈E
0 ≤ θv,z ≤ 1 ∀(v, z) ∈ V × V (cid:48)
(cid:88)

θv,z ≤ 1 ∀z ∈ V (cid:48)

v∈V
(cid:88)

z∈V (cid:48)

θv,z ≤ 1 ∀v ∈ V .

Max-Marginal For the graphical model deﬁned above, the max-marginal ψ¯v;¯z is the constrained maxi-
mum weight matching in the graph G deﬁned above subject to the constraint that ¯v is mapped to ¯z. The
linear program above can be modiﬁed to include the constraint θ¯v,¯z = 1:

max
{θv,z for (v,z)∈E}

(cid:88)

ϕ(v, z)θv,z ,

s.t.

(v,z)∈E
0 ≤ θv,z ≤ 1 ∀(v, z) ∈ V × V (cid:48)
(cid:88)

θv,z ≤ 1 ∀z ∈ V (cid:48)

(73)

θv,z ≤ 1 ∀v ∈ V

v∈V
(cid:88)

z∈V (cid:48)
θ¯v,¯z = 1 .

62

Algorithm 12 Max marginal computation via Graph matchings
1: Input: Directed bipartite graph G = (V ∪ V (cid:48), E), weights ϕ : V × V (cid:48) → R.
2: Find a maximum weight bipartite matching σ∗ in the graph G. Let the maximum weight be ψ∗.
3: Deﬁne a weighted residual bipartite graph (cid:98)G = (V ∪ V (cid:48), (cid:98)E), where the set (cid:98)E is populated as follows:
for (v, z) ∈ E, add an edge (v, z) to (cid:98)E with weight 1 − I(σ∗(v) = z), add (z, v) to (cid:98)E with weights
− I(σ∗(v) = z).

4: Find the maximum weight path from every vertex z ∈ V (cid:48) to every vertex v ∈ V and denote this by

∆(z, v).

5: Assign the max-marginals ψv;z = ψ∗ + I(σ∗(v) (cid:54)= z) (∆(z, v) + ϕ(v, z)) for all (v, z) ∈ V × V (cid:48).
6: return Max-marginals ψv;z for all (v, z) ∈ V × V (cid:48).

Algorithm to Compute Max-Marginals Algo. 12, which shows how to compute max-marginals is due
to Duchi et al. [2006]. Its running time complexity is as follows: the initial maximum weight matching
computation takes O(p3) via computation of a maximum ﬂow [Schrijver, 2003, Ch. 10]. Line 4 of Algo. 12
can be performed by the all-pairs shortest paths algorithm [Schrijver, 2003, Ch. 8.4] in time O(p3). Its
correctness is shown by the following theorem:

Theorem 48 (Duchi et al. [2006]). Given a directed bipartite graph G and weights ϕ : V × V (cid:48) → R, the
output ψv;z from Algo. 12 are valid max-marginals, i.e., ψv;z coincides with the optimal value of the linear
program (73). Moreover, Algo. 12 runs in time O(p3) where p = |V | = |V (cid:48)|.

C.4 Proof of Proposition 14

Proposition 14. Consider as inputs an augmented score function ψ(·, ·; w), an integer K > 0 and a smooth-
ing parameter µ > 0. Further, suppose that ψ is unambiguous, that is, ψ(y(cid:48); w) (cid:54)= ψ(y(cid:48)(cid:48); w) for all distinct
y(cid:48), y(cid:48)(cid:48) ∈ Y. Consider one of the two settings:

(A)

the output space Yv = {0, 1} for each v ∈ V, and the function −ψ is submodular (see Appendix C.2
and, in particular, (72) for the precise deﬁnition), or,

(B)

the augmented score corresponds to an alignment task where the inference problem (3) corresponds to
a maximum weight bipartite matching (see Appendix C.3 for a precise deﬁnition).

In these cases, we have the following:

(i) The max oracle can be implemented at a computational complexity of O(p) minimum cut computations

in Case (A), and in time O(p3) in Case (B).

(ii) The top-K oracle can be implemented at a computational complexity of O(pK) minimum cut compu-

tations in Case (A), and in time O(p3K) in Case (B).

(iii) The exp oracle is #P-complete in both cases.

Proof. A set of max-marginals can be computed by an algorithm M deﬁned as follows:

• In Case (A), invoke Algo. 11 a total of 2p times, with yv = 0, and yv = 1 for each v ∈ V. This takes

a total of 2p min-cut computations.

63

Algorithm 13 Top-K best-ﬁrst branch and bound search

1: Input: Augmented score function ψ(·, ·; w), integer K > 0, search space Y, upper bound (cid:98)ψ, split

2: Initialization: Initialize priority queue with single entry Y with priority (cid:98)ψ(Y; w), and solution set S as

strategy.

the empty list.
3: while |S| < K do
4:

Pop (cid:98)Y from the priority queue.
if (cid:98)Y = {(cid:98)y} is a singleton then
Append ((cid:98)y, ψ((cid:98)y; w)) to S.

else

5:

6:

7:

8:

9:

end if

10:
11: end while
12: return S.

Y1, Y2 ← split( (cid:98)Y).
Add Y1 with priority (cid:98)ψ(Y1; w) and Y2 with priority (cid:98)ψ(Y2; w) to the priority queue.

• In Case (B), M is simply Algo. 12, which takes time O(p3).

The max oracle can then be implmented by the decoding in Eq. (21), whose correctness is guaranteed by
Thm. 45. The top-K oracle is implemented by invoking the BMMF algorithm with M deﬁned above,
followed by a projection onto the simplex (Algo. 6 in Appendix A) and its correctness is guaranteed by
Thm. 46. Lastly, the result of exp oracle follows from Jerrum and Sinclair [1993, Thm. 15] in conjunction
with Prop. 9.

C.5

Inference using branch and bound search

Algo. 13 with the input K = 1 is the standard best-ﬁrst branch and bound search algorithm. Effectively, the
top-K oracle is implemented by simply continuing the search procedure until K outputs have been produced
- compare Algo. 13 with inputs K = 1 and K > 1. We now prove the correctness guarantee.

Proposition 15. Consider an augmented score function ψ(·, ·, w), an integer K > 0 and a smoothing
parameter µ > 0. Suppose the upper bound function (cid:98)ψ(·, ·; w) : X × 2Y → R satisﬁes the following
properties:

(a) (cid:98)ψ( (cid:98)Y; w) is ﬁnite for every (cid:98)Y ⊆ Y,

(b) (cid:98)ψ( (cid:98)Y; w) ≥ maxy∈ (cid:98)Y ψ(y; w) for all (cid:98)Y ⊆ Y, and,

(c) (cid:98)ψ({y}; w) = ψ(y; w) for every y ∈ Y.

Then, we have the following:

(i) Algo. 13 with K = 1 is a valid implementation of the max oracle.

(ii) Algo. 13 followed by a projection onto the simplex (Algo. 6 in Appendix A) is a valid implementation of

the top-K oracle.

64

Proof. Suppose at some point during the execution of the algorithm, we have a (cid:98)Y = {(cid:98)y} on Line 5 and that
|S| = k for some 0 ≤ k < K. From the properties of the quality upper bound (cid:98)ψ, and using the fact that {(cid:98)y}
had the highest priority in the priority queue (denoted by (∗)), we get,

ψ((cid:98)y; w) = (cid:98)ψ({(cid:98)y}; w)
(∗)
≥ max
Y ∈P
≥ max
Y ∈P

max
y∈Y

(cid:98)ψ(Y ; w)

ψ(y; w)

(#)
= max
y∈Y−S

ψ(y; w) ,

where the equality (#) followed from the fact that any y ∈ Y exits the priority queue only if it is added to
S. This shows that if a (cid:98)y is added to S, it has a score that is no less than that of any y ∈ Y − S. In other
words, Algo. 13 returns the top-K highest scoring y’s.

D The Casimir Algorithm and Non-Convex Extensions: Missing Proofs

This appendix contains missing proofs from Sections 5 and 6. Throughout, we shall assume that ω is ﬁxed
and drop the subscript in Aω, Dω. Moreover, an unqualiﬁed norm (cid:107)·(cid:107) refers to the Euclidean norm (cid:107) · (cid:107)2.

D.1 Behavior of the Sequence (αk)k≥0

Lemma 21. Given a positive, non-decreasing sequence (κk)k≥1 and λ ≥ 0, consider the sequence (αk)k≥0
deﬁned by (27), where α0 ∈ (0, 1) such that α2
0 ≥ λ/(λ + κ1). Then, we have for every k ≥ 1 that
0 < αk ≤ αk−1 and, α2

k ≥ λ/(λ + κk+1) .

Proof. It is clear that (27) always has a positive root, so the update is well deﬁned. Deﬁne sequences
(ck)k≥1, (dk)k≥0 as

ck =

λ + κk
λ + κk+1

,

and dk =

λ
λ + κk+1

.

Therefore, we have that ckdk−1 = dk, 0 < ck ≤ 1 and 0 ≤ dk < 1. With these in hand, the rule for αk can
be written as

−(ckα2

k−1 − dk) +

k−1 − dk)2 + 4ckα2

k−1

αk =

.

(74)

(cid:113)

(ckα2
2

We show by induction that that dk ≤ α2
satisﬁes the hypothesis for some k ≥ 1. Noting that α2
that

k < 1. The base case holds by assumption. Suppose that αk−1
k−1 − dk ≥ 0, we get

k−1 ≥ dk−1 is equivalent to ckα2

(cid:113)

(ckα2

k−1 − dk)2 + 4ckα2

k−1 ≤

(cid:113)

(ckα2

k−1 − dk)2 + 4ckα2
√
ckαk−1 .

k−1 − dk + 2

= ckα2

k−1 + 2(ckα2

k−1 − dk)(2

ckαk−1)

√

(75)

65

We now conclude from (74) and (75) that
−(ckα2

k−1 − dk) + (ckα2
k−1 − dk + 2
2

√

ckαk−1)

αk ≤

√

=

ckαk−1 ≤ αk−1 < 1 ,

(76)

since ck ≤ 1 and αk−1 < 1. To show the other side, we expand out (74) and apply (75) again to get

α2

k − dk =

(ckα2

(ckα2

=

1
2
1
2
1
2
= (ckα2

≥

k−1 − dk)2 + (ckα2
(cid:16)

2 + (ckα2

k−1 − dk)
k−1 − dk) (cid:0)2 + (ckα2
k−1 − dk)(1 −

√

(ckα2

ckαk−1) ≥ 0 .

k−1 − dk) −

(cid:113)

k−1 − dk)

(ckα2

1
2

(ckα2
(cid:113)

k−1 − dk)2 + 4ckα2
(cid:17)

k−1

k−1 − dk) −

(ckα2

k−1 − dk) − (ckα2

k−1 − dk + 2

k−1 − dk)2 + 4ckα2
√

k−1

ckαk−1)(cid:1)

The fact that (αk)k≥0 is a non-increasing sequence follows from (76).

D.2 Proofs of Corollaries to Theorem 16

We rewrite (30) from Theorem 16 as follows:




F (wk) − F ∗ ≤



k
(cid:89)

j=1

1 − αj−1
1 − δj

(cid:16)



+

1
1 − αk











k
(cid:89)

j=1

1 − αj
1 − δj

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2(cid:17)

+ µkDω

γ0
2







k
(cid:88)

k
(cid:89)

j=2

i=j

1 − αi
1 − δi

 (1 + δ1)µ1Dω +

 (µj−1 − (1 − δj)µj) Dω

 ,

(77)



Next, we have proofs of Corollaries 17 to 20.
Corollary 17. Consider the setting of Thm. 16. Let q = λ
k ≥ 1. Choose α0 =
q . Then, we have,
q and, δk =

√

q
√

2−

√

λ+κ . Suppose λ > 0 and µk = µ, κk = κ, for all

F (wk) − F ∗ ≤

√
q
√
q

3 −
1 −

(cid:18)

µD + 2

1 −

(cid:19)k

√

q
2

√

√

(F (w0) − F ∗) .

Proof. Notice that when α0 =
all k, j, 1−αk
1−δj
conditions as

= 1 −

q, we have, αk =

q for all k. Moreover, for our choice of δk, we get, for
√
q
2 . Under this choice of α0, we have, γ0 = λ. So, we get the dependence on initial

∆0 = F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2 ≤ 2(F (w0) − F ∗) ,

λ
2

by λ-strong convexity of F . The last term of (77) is now,



(cid:18)








(cid:124)


µD
√

1 −

q

1 −

√

q
2
(cid:123)(cid:122)
≤1

(cid:19)k−1

√

q
2

(cid:124)

+

(cid:125)

k
(cid:88)

(cid:18)

j=2

1 −

(cid:123)(cid:122)
(∗)
≤ 1

(cid:19)k−j

√

q
2

≤

2µD
√

1 −

,

q










(cid:125)


66

where (∗) holds since

k
(cid:88)

(cid:18)

1 −

j=2

√

q
2

(cid:19)k−j

∞
(cid:88)

(cid:18)

≤

1 −

j=0

(cid:19)j

√

q
2

=

2
√
q

.

Corollary 18. Consider the setting of Thm. 16. Let q = λ
all k ≥ 1. Choose α0 =

q and, the sequences (µk)k≥1 and (δk)k≥1 as

λ+κ , η = 1 −

√

√
q
2 . Suppose λ > 0 and κk = κ, for

µk = µηk/2 ,

and,

δk =

√

q
√

,

q

2 −

where µ > 0 is any constant. Then, we have,

F (wk) − F ∗ ≤ ηk/2

(cid:20)
2 (F (w0) − F ∗) +

µDω
√
q
1 −

(cid:18)

√

2 −

q +

√

(cid:19)(cid:21)

.

q
√
η

1 −

√

q for each k, and 1−δ

Proof. As previously in Corollary 17, notice that under the speciﬁc parameter choices here, we have, γ0 = λ,
√
q
αk =
2 = η. By λ-strong convexity of F and the fact that γ0 = λ, the
contribution of w0 can be upper bounded by 2(F (w0) − F ∗). Now, we plugging these into (77) and
collecting the terms dependent on δk separately, we get,

1−α = 1 −

F (wk) − F ∗ ≤ 2ηk(F (w0) − F ∗)
(cid:124)
(cid:125)

(cid:123)(cid:122)
=:T1

+ µkD
(cid:124)(cid:123)(cid:122)(cid:125)
=:T2

+

1
√
1 −

q









+

ηkµ1D
(cid:124) (cid:123)(cid:122) (cid:125)
=:T3

k
(cid:88)

j=2
(cid:124)

ηk−j+1(µj−1 − µj)D

+

ηk−j+1µjδjD

.

(78)

(cid:123)(cid:122)
=:T4

(cid:125)

(cid:123)(cid:122)
=:T5








(cid:125)

We shall consider each of these terms. Since ηk ≤ ηk/2, we get T1 ≤ 2ηk/2(F (w0) − F ∗) and T3 =
ηkµ1D ≤ ηkµD ≤ ηk/2µD. Moreover, T2 = µkD = ηk/2µD. Next, using 1 −

η ≤ 1 − η =

√

√
q
2 ,

k
(cid:88)

j=1
(cid:124)

√

T4 =

ηk−j+1(µj−1 − µj)D =

ηk−j+1µη(j−1)/2(1 −

η)D

≤

µD

ηk− j−1

2 =

µDη(k+1)/2

ηj/2 ≤

µD

k−2
(cid:88)

j=0

√

q
2

η(k+1)/2
√
η
1 −

k
(cid:88)

j=2

√

q
2

Similarly, using δj =

q/2η, we have,

T5 =

ηk−j+1µηj/2D

=

µD

ηk−j/2 ≤

µD

√

q
2η

√

q
2

k
(cid:88)

j=1

√
q
2

ηk/2
√

1 −

η

.

Plugging these into (78) completes the proof.

67

k
(cid:88)

j=2

µD

ηk/2
√

1 −

η

.

≤

√

k
(cid:88)

j=2
√

q
2
√

q
2

k
(cid:88)

j=1

Corollary 19. Consider the setting of Thm. 16. Suppose µk = µ, κk = κ, for all k ≥ 1 and λ = 0. Choose
α0 =

and δk = 1

(1+k)2 . Then, we have,

5−1
2

√

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2
2

+ µDω

1 +

κ
2

(cid:17)

(cid:18)

12
k + 2

+

30
(k + 2)2

(cid:19)

.

(cid:16)

8
(k + 2)2

Proof. Firstly, note that γ0 = κ α2
0
1−α0

= κ. Now, deﬁne

Ak =

(1 − αi), and, Bk =

(1 − δi) .

k
(cid:89)

i=0

k
(cid:89)

i=1

We have,

Therefore,

k
(cid:89)

(cid:18)

1 −

Bk =

i=1

1
(i + 1)2

(cid:19)

=

k
(cid:89)

i=1

i(i + 2)
(i + 1)2 =

1
2

+

1
2(k + 1)

.

(79)

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:16)

Ak−1
Bk

+

µD
1 − α0





k
(cid:89)

j=1

1 − αj−1
1 − δk

γ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)


+ µD

 (1 + δ1) + µD

≤

(cid:16)

Ak−1
Bk

F (w0) − F ∗ +

γ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)
(cid:125)

+µD

(cid:124)

+

Ak−1
Bk

5
4 µD
1 − α0
(cid:123)(cid:122)
(cid:124)
=:T2

(cid:125)

(cid:123)(cid:122)
=:T1

+ µD

(cid:124)

k
(cid:88)

j=2

Ak−1/Aj−2
Bk/Bj−1

(cid:123)(cid:122)
=:T3

.

δj
1 − αj−1
(cid:125)

k
(cid:88)

k
(cid:89)





j=2

i=j





1 − αi−1
1 − δi

δj
1 − αj−1

From Lemma 52, which analyzes the evolution of (αk) and (Ak), we get that
αk ≤ 2

k+3 for k ≥ 0. Since Bk ≥ 1
2 ,

2

(k+2)2 ≤ Ak−1 ≤ 4

(k+2)2 and

T1 ≤

(cid:16)

8
(k + 2)2

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2(cid:17)

.

γ0
2

Moreover, since α0 ≤ 2/3,

Lastly, we have,

T2 ≤

30
(k + 2)2 .

T3 ≤

k
(cid:88)

j=2

4
(k + 2)2 ×

(j + 1)2
2

× 2

(cid:19)

(cid:18) 1
2

+

1
2j

×

1
(j + 1)2 ×

1
1 − 2/j+2

≤ 2

2
(k + 2)2

k
(cid:88)

j=2

j + 2
j

4

≤

(k + 2)2 (k − 1 + 2 log k) ≤

12
k + 2

,

where we have used the simpliﬁcations (cid:80)k

j=2 1/k ≤ log k and k − 1 + 2 log k ≤ 3k.

68

Corollary 20. Consider the setting of Thm. 16 with λ = 0. Choose α0 =
constants κ, µ, deﬁne sequences (κk)k≥1, (µk)k≥1, (δk)k≥1 as

√

5−1
2

, and for some non-negative

κk = κ k , µk =

and,

δk =

µ
k

1
(k + 1)2 .

Then, for k ≥ 2, we have,

F (wk) − F ∗ ≤

log(k + 1)
k + 1

(cid:0)2(F (w0) − F ∗) + κ(cid:107)w0 − w∗(cid:107)2

2 + 27µDω

(cid:1) .

(80)

For the ﬁrst iteration (i.e., k = 1), this bound is off by a constant factor 1/ log 2.

Proof. Notice that γ0 = κ1

= κ. As in Corollary 19, deﬁne

α2
0
1−α0

From Lemma 53 and (79) respectively, we have for k ≥ 1,

Ak =

(1 − αi) ,

and, Bk =

(1 − δi) .

k
(cid:89)

i=0

1 − 1√
2
k + 1

≤ Ak ≤

and,

≤ Bk ≤ 1 .

1
k + 2

,

k
(cid:89)

i=1

1
2

Now, invoking Theorem 16, we get,

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:16)

Ak−1
Bk

(cid:124)

k
(cid:88)

j=2
(cid:124)

Ak−1/Aj−1
Bk/Bj−1

(cid:123)(cid:122)
=:T4

γ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)
(cid:125)

+

+ µkD
(cid:124)(cid:123)(cid:122)(cid:125)
=:T2

1
1 − α0
(cid:124)

Ak−1
Bk

(cid:123)(cid:122)
=:T3

(cid:123)(cid:122)
=:T1

µ1D(1 + δ1)

+

(cid:125)

(µj−1 − µj)D

+

k
(cid:88)

j=2
(cid:124)

(cid:125)

Ak−1/Aj−1
Bk/Bj−1

δjµjD

.

(cid:123)(cid:122)
=:T5

(cid:125)

(81)

We shall bound each of these terms as follows.

T1 =

(cid:16)

Ak−1
Bk

F (w0) − F ∗ +

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2(cid:17)

,

κ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)

=

γ0
2

T2 = µkD =

µD
k

(cid:16)

2
k + 1

,

≤

2µD
k + 1
2
k + 1

T3 =

1
1 − α0
where we used the fact that α0 ≤ 2/3. Next, using (cid:80)k
1 + log(k − 1), we get,

Ak−1
Bk

µ1D(1 + δ1) ≤ 3 ×

× µ ×

15
2
j=2 1/j ≤ 1 + (cid:82) k−1
j=2 1/(j − 1) = 1 + (cid:80)k−1

µD
k + 1

D =

5
4

,

1

dx/x =

T4 =

k
(cid:88)

j=2
√

2
k + 1

·

√

≤ 2

2(

2 + 1)µD

(cid:19)

(cid:18) µ

−

µ
j

j
1 − 1√
j − 1
2
(cid:18) 1 + log(k + 1)
k + 1

(cid:19)

.

√

√

D = 2

2(

2 + 1)

µD
k + 1

k
(cid:88)

j=2

1
j − 1

69

Moreover, from (cid:80)k

j=2 1/(j + 1)2 ≤ (cid:82) k+1

2

dx/x2 ≤ 1/2, it follows that

T5 =

k
(cid:88)

j=2

2
k + 1

·

j
1 − 1√
2

µ
j

·

1

√
(j + 1)2 D = 2

√

2(

2 + 1)

µD
k + 1

k
(cid:88)

j=2

1
(j + 1)2 ≤

√

√

2(

2 + 1)

µD
k + 1

.

Plugging these back into (81), we get

F (wk) − F ∗ ≤

2
k + 1
µD
k + 1

(cid:16)

F (wk) − F ∗ +
(cid:18)

√

κ
2

(cid:107)w0 − w∗(cid:107)2(cid:17)
(cid:19)
√
√

+

√

2 +

+

2(1 +

2)

+ 2

2(1 +

2)µD

15
2

1 + log(k + 1)
k + 1

.

To complete the proof, note that log(k + 1) ≥ 1 for k ≥ 2 and numerically verify that the coefﬁcient of µD
is smaller than 27.

D.3

Inner Loop Complexity Analysis for Casimir

Before proving Prop. 27, the following lemmas will be helpful. First, we present a lemma from Lin et al.
[2018, Lemma 11] about the expected number of iterations a randomized linearly convergent ﬁrst order
methods requires to achieve a certain target accuracy.
Lemma 49. Let M be a linearly convergent algorithm and f ∈ FL,λ. Deﬁne f ∗ = minw∈Rd f (w). Given
a starting point w0 and a target accuracy (cid:15), let (wk)k≥0 be the sequence of iterates generated by M. Deﬁne
T ((cid:15)) = inf {k ≥ 0 | f (wk) − f ∗ ≤ (cid:15)} . We then have,

E[T ((cid:15))] ≤

1
τ (L, λ)

log

(cid:18) 2C(L, λ)
τ (L, λ)(cid:15)

(cid:19)

(f (w0) − f ∗)

+ 1 .

(82)

This next lemma is due to Lin et al. [2018, Lemma 14, Prop. 15].

Lemma 50. Consider Fµω,κ(· ; z) deﬁned in Eq. (25) and let δ ∈ [0, 1). Let (cid:98)F ∗ = minw∈Rd Fµω,κ(w; z)
and (cid:98)w∗ = arg minw∈Rd Fµω,κ(w; z). Further let Fµω(· ; z) be Lµω-smooth. We then have the following:
Lµω + κ
2

Fµω,κ(z; z) − (cid:98)F ∗ ≤

(cid:107)z − (cid:98)w∗(cid:107)2
2 ,

and,

Fµω,κ( (cid:98)w; z) − (cid:98)F ∗ ≤

(cid:107)z − (cid:98)w∗(cid:107)2

2 =⇒ Fµω,κ( (cid:98)w; z) − (cid:98)F ∗ ≤

(cid:107) (cid:98)w − z(cid:107)2
2 .

δκ
2

δκ
8

We now restate and prove Prop. 27.

Proposition 27. Consider Fµω,κ(· ; z) deﬁned in Eq. (25), and a linearly convergent algorithm M with
parameters C, τ . Let δ ∈ [0, 1). Suppose Fµω is Lµω-smooth and λ-strongly convex. Then the expected
number of iterations E[ (cid:98)T ] of M when started at z in order to obtain (cid:98)w ∈ Rd that satisﬁes

Fµω,κ( (cid:98)w; z) − min
w

Fµω,κ(w; z) ≤ δκ

2 (cid:107)w − z(cid:107)2

2

(83)

is upper bounded by

E[ (cid:98)T ] ≤

1
τ (Lµω + κ, λ + κ)

log

(cid:18) 8C(Lµω + κ, λ + κ)
τ (Lµω + κ, λ + κ)

·

Lµω + κ
κδ

(cid:19)

+ 1 .

Proof. In order to invoke Lemma 49, we must appropriately set (cid:15) for (cid:98)w to satisfy (83) and then bound the
ratio (Fµω,κ(z; z) − (cid:98)F ∗)/(cid:15). Firstly, Lemma 50 tells us that choosing (cid:15) = δkκk
2 guarantees that
the (cid:98)w so obtained satisﬁes (83), where (cid:98)w∗ := arg minw∈Rd Fµω,κ(w; z), Therefore, (Fµω,κ(z; z) − (cid:98)F ∗)/(cid:15)
is bounded from above by 4(Lµω + κ)/κδ.

8 (cid:107)zk−1 − (cid:98)w∗(cid:107)2

70

D.4

Information Based Complexity of Casimir-SVRG

Presented below are the proofs of Propositions 29 to 32 from Section 5.3. We use the following values of
C, τ , see e.g., Hofmann et al. [2015].

τ (L, λ) =

8 L

1
λ + n
(cid:32)

L
λ

1
λ + n(cid:1)
(cid:33)

.

≥

8 (cid:0) L
n L
λ
λ + n

8 L

C(L, λ) =

1 +

κ =

(cid:40) A

µn − λ , if A
λ , otherwise

µn > 4λ

,

(cid:32)

(cid:114)

E[N ] ≤ (cid:101)O

n +

(cid:33)

.

AωDωn
λ(cid:15)

K ≤

log

2
√
q

(cid:18) 2∆F0

(cid:19)

(cid:15) − cqµD

,

Proposition 29. Consider the setting of Thm. 16 with λ > 0 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as
the inner solver with parameters: µk = µ = (cid:15)/10Dω, κk = k chosen as

q = λ/(λ + κ), α0 =
q/(2 −
F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

q, and δ =

√

√

√

q). Then, the number of iterations N to obtain w such that

Proof. We use shorthand A := Aω, D := Dω, Lµ = λ + A/µ and ∆F0 = F (w0) − F ∗. Let C, τ be the
linear convergence parameters of SVRG. From Cor. 17, the number of outer iterations K required to obtain
F (wK) − F ∗ ≤ (cid:15) is

where cq = (3 −
√
δk =

q/(2 −

√

√

q),

√

q)/(1 −

q). From Prop. 27, the number Tk of inner iterations for inner loop k is, from

E[Tk] ≤

1
τ (Lµ + κ, λ + κ)
2
τ (Lµ + κ, λ + κ)

log

log

(cid:18) 8C(Lµ + κ, λ + κ)
τ (Lµ + κ, λ + κ)
(cid:18) 8C(Lµ + κ, λ + κ)
τ (Lµ + κ, λ + κ)

·

·

Lµ + κ
κ
Lµ + κ
κ

≤

2 −
√

2 −
√

·

·

√

q
√

q

(cid:19)

q

+ 1

(cid:19)

q

.

Let the total number N of iterations of SVRG to obtain an iterate w that satisﬁes F (w) − F ∗ ≤ (cid:15). Next,
we upper bound E[N ] ≤ (cid:80)K
i=1

E[Tk] as

E[N ] ≤

√

4
qτ (Lµ + κ, λ + κ)

log

(cid:18) 8C(Lµ + κ, λ + κ)
τ (Lµ + κ, λ + κ)

Lµ + κ
κ

2 −
√

q

log

(cid:18) 2(F (w0) − F ∗)
(cid:15) − cqµD

(cid:19)

.

√

(cid:19)

q

(84)

Next, we shall plug in C, τ for SVRG in two different cases:

• Case 1: A > 4µλn, in which case κ + λ = A/(µn) and q < 1/4.

71

• Case 2: A ≤ 4µλn, in which case, κ = λ and q = 1/2.

We ﬁrst consider the term outside the logarithm. It is, up to constants,

(cid:18)

n +

1
√
q

A
µ(λ + κ)

(cid:19)

(cid:114)

= n

λ + κ
λ

+

A
µ(cid:112)λ(λ + κ)

.

For Case 1, plug in κ + λ = A/(µn) so this term evaluates to (cid:112)ADn/(λ(cid:15)). For Case 2, we use the fact
that A ≤ 4µλn so that this term can be upper bounded by,

(cid:32)(cid:114)

n

λ + κ
λ

+ 4

(cid:114) λ

(cid:33)

λ + κ

√

= 3

2n ,

since we chose κ = λ. It remains to consider the logarithmic terms. Noting that κ ≥ λ always, it follows
that the ﬁrst log term of (84) is clearly logarithmic in the problem parameters.

As for the second logarithmic term, we must evaluate cq. For Case 1, we have that q < 1/4 so that
cq < 5 and cqµD < (cid:15)/2. For Case 2, we get that q = 1/2 and cq < 8 so that cqµD < 4(cid:15)/5. Thus, the
second log term of (84) is also logarithmic in problem parameters.

Proposition 30. Consider the setting of Thm. 16. Suppose λ > 0 and κk = κ, for all k ≥ 1 and that α0,
q/2. If we run Algo. 4 with
(µk)k≥1 and (δk)k≥1 are chosen as in Cor. 18, with q = λ/(λ+κ) and η = 1−
SVRG as the inner solver with these parameters, the number of iterations N of SVRG required to obtain w
such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

√

(cid:18)

E[N ] ≤ (cid:101)O

n +

(cid:18)

Aω
µ(λ + κ)(cid:15)

F (w0) − F ∗ +

µDω
√
1 −

q

(cid:19)(cid:19)

.

Proof. We continue to use shorthand A := Aω, D := Dω. First, let us consider the minimum number of
outer iterations K required to achieve F (wK) − F ∗ ≤ (cid:15). From Cor. 18, if we have η−K/2∆0 ≤ (cid:15), or,

K ≥ Kmin :=

log (∆0/(cid:15))
η(cid:1) .
log (cid:0)1/

√

For this smallest value, we have,

µKmin = µηKmin/2 =

µ(cid:15)
∆0

.

(85)

Let C, τ be the linear convergence parameters of SVRG, and deﬁne Lk := λ + A/µk for each k ≥ 1.
Further, let T (cid:48) be such that

T (cid:48) ≥

max
k∈{1,··· ,Kmin}

log

(cid:18)
8

C(Lk + κ, λ + κ)
τ (Lk + κ, λ + κ)

Lk + κ
κδ

(cid:19)

.

72

Then, the total complexity is, from Prop. 27, (ignoring absolute constants)

E[N ] ≤

n +

λ + κ + A
µk
λ + κ

(cid:33)

T (cid:48)

(cid:32)

Kmin(cid:88)

k=1

Kmin(cid:88)

(cid:18)

k=1
(cid:32)

(cid:32)

(cid:32)

=

n + 1 +

(cid:19)

η−k/2

T (cid:48)

A/µ
λ + κ

=

Kmin(n + 1) +

≤

Kmin(n + 1) +

A/µ
λ + κ

A/µ
λ + κ

Kmin(cid:88)

(cid:33)

η−k/2

T (cid:48)

k=1
η−Kmin/2
1 − η1/2

(cid:33)

T (cid:48)

=

(n + 1)

log (cid:0) ∆0
(cid:15)
log(1/√

(cid:1)

η)

+

A/µ
λ + κ

1
√
1 −

η

∆0
(cid:15)

(cid:33)

T (cid:48) .

(86)

It remains to bound T (cid:48). Here, we use λ + A
T (cid:48) is logarithmic in ∆0/(cid:15), n, AD, µ, κ, λ−1.

µ ≤ Lk ≤ λ + A
µK

for all k ≤ K together with (85) to note that

Proposition 31. Consider the setting of Thm. 16 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as the inner
√
, δk = 1/(k + 1)2, and κk = κ = Aω/µ(n + 1).
solver with parameters: µk = µ = (cid:15)/20Dω, α0 =
Then, the number of iterations N to get a point w such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

5−1
2

(cid:32)

(cid:114)

E[N ] ≤ (cid:101)O

n

F (w0) − F ∗
(cid:15)

(cid:112)

+

AωDωn

(cid:107)w0 − w∗(cid:107)2
(cid:15)

(cid:33)

.

Proof. We use shorthand A := Aω, D := Dω, Lµ = A/µ and ∆F0 = F (w0) − F ∗ + κ
2 (cid:107)w0 − w∗(cid:107)2.
Further, let C, τ be the linear convergence parameters of SVRG. In Cor. 19, the fact that K ≥ 1 allows us to
bound the contribution of the smoothing as 10µD. So, we get that the number of outer iterations K required
to get F (wK) − F ∗ ≤ (cid:15) can be bounded as

K + 1 ≤

(cid:115)

8∆F0
(cid:15) − 10µD

.

Moreover, from our choice δk = 1/(k + 1)2, the number of inner iterations Tk for inner loop k is, from
Prop. 27,

E[Tk] ≤

1
τ (Lµ + κ, κ)
2
τ (Lµ + κ, κ)

log

log

(cid:18) 8C(Lµ + κ, κ)
τ (Lµ + κ, κ)
(cid:18) 8C(Lµ + κ, κ)
τ (Lµ + κ, κ)

·

·

Lµ + κ
κ
Lµ + κ
κ

≤

(cid:19)

· (k + 1)2

+ 1

·

8∆F0
(cid:15) − 10µD

(cid:19)

.

Next, we consider the total number N of iterations of SVRG to obtain an iterate w such that F (w) −

F ∗ ≤ (cid:15). Using the fact that E[N ] ≤ (cid:80)K
i=1

E[Tk], we bound it as

E[N ] ≤

1
τ (Lµ + κ, κ)

8∆F0
(cid:15) − 10µD

log

(cid:18) 64C(Lµ + κ, κ)
τ (Lµ + κ, κ)

Lµ + κ
κ

∆F0
(cid:15) − 10µD

(cid:19)

.

(cid:115)

(87)

73

Now, we plug into (87) the values of C, τ for SVRG. Note that κ = Lµ/(n + 1). So we have,

1
τ (Lµ + κ, κ)

= 8

(cid:18) Lµ + κ
κ

(cid:19)

+ n

= 16(n + 1) , and,

C(Lµ + κ, κ) =

Lµ + κ
κ

(cid:32)

1 +

(cid:33)

n Lµ+κ
κ
8 L+κ
κ + n

≤ (n + 2) (cid:0)1 + n
8

(cid:1) .

It now remains to assign µ = (cid:15)/(20D) and plug C, τ from above into (87), noting that κ = 20AD/((cid:15)(n + 1)).

Proposition 32. Consider the setting of Thm. 16. Suppose λ = 0 and that α0, (µk)k≥1,(κk)k≥1 and (δk)k≥1
are chosen as in Cor. 20. If we run Algo. 4 with SVRG as the inner solver with these parameters, the number
of iterations N of SVRG required to obtain w such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

E[N ] ≤ (cid:101)O

(cid:0)F (w0) − F ∗ + κ(cid:107)w0 − w∗(cid:107)2

2 + µD(cid:1)

(cid:18) 1
(cid:15)

(cid:18)

n +

(cid:19)(cid:19)

.

Aω
µκ

Proof. Deﬁne short hand A := Aω, D := Dω and

∆0 := 2(F (w0) − F ∗) + κ(cid:107)w0 − w∗(cid:107)2 + 27µD .

From Cor. 20, the number of iterations K required to obtain F (wK) − F ∗ ≤ log(K+1)
Lemma 54),

K+1 ∆0 ≤ (cid:15) is (see

K + 1 =

2∆0
(cid:15)

log

2∆0
(cid:15)

.

Let C, τ be such that SVRG is linearly convergent with parameters C, τ , and deﬁne Lk := A/µk for each
k ≥ 1. Further, let T (cid:48) be such that

T (cid:48) ≥ max

k∈{1,··· ,K}

(cid:18)
8

log

C(Lk + κ, κ)
τ (Lk + κ, κ)

Lk + κ
κδk

(cid:19)

.

Clearly, T (cid:48) is logarithmic in K, n, AD, µ, κ. From Prop. 27, the minimum total complexity is (ignoring

absolute constants)

E[N ] =

K
(cid:88)

(cid:18)

n +

A/µk + κk
κk

(cid:19)

T (cid:48)

n + 1 +

(cid:19)

T (cid:48)

A
µkκk

=

=

(cid:18)

(cid:18)

k=1
K
(cid:88)

k=1
K
(cid:88)

k=1
(cid:18)

n + 1 +

(cid:19)

T (cid:48)

A
µκ
(cid:19)

≤

n + 1 +

KT (cid:48) ,

A
µκ

and plugging in K from (89) completes the proof.

74

(88)

(89)

(90)

D.5 Prox-Linear Convergence Analysis

We ﬁrst prove Lemma 34 that speciﬁes the assumption required by the prox-linear in the case of structured
prediction.
Lemma 34. Consider the structural hinge loss f (w) = maxy∈Y ψ(y; w) = h ◦ g(w) where h, g are as
deﬁned in (6). If the mapping w (cid:55)→ ψ(y; w) is L-smooth with respect to (cid:107) · (cid:107)2 for all y ∈ Y, then it holds
for all w, z ∈ Rd that

|h(g(w + z)) − h(g(w) + ∇g(w)z)| ≤

L
2

(cid:107)z(cid:107)2
2 .

Proof. For any A ∈ Rm×d and w ∈ Rd, and (cid:107)A(cid:107)2,1 deﬁned in (2), notice that

(cid:107)Aw(cid:107)∞ ≤ (cid:107)A(cid:107)2,1(cid:107)w(cid:107)2 .

(91)

Now using the fact that max function h satisﬁes |h(u(cid:48)) − h(u)| ≤ (cid:107)u(cid:48) − u(cid:107)∞ and the fundamental theorem
of calculus (∗), we deduce

|h(g(w + z)) − h(g(w) + ∇g(w)z)| ≤ (cid:107)g(w + z) − (g(w) + ∇g(w)z) (cid:107)∞
(cid:13)
(∗)
(cid:13)
≤
(cid:13)
(cid:13)∞

(∇g(w + tz) − ∇g(w))z dt

(cid:90) 1

(cid:107)∇g(w + tz) − ∇g(w)(cid:107)2,1(cid:107)z(cid:107)2 dt .

(92)

Note that the deﬁnition (2) can equivalently be stated as (cid:107)A(cid:107)2,1 = max(cid:107)u(cid:107)1≤1 (cid:107)A(cid:62)u(cid:107)2. Given u ∈ Rm,
we index its entries uy by y ∈ Y. Then, the matrix norm in (92) can be simpliﬁed as

(cid:107)∇g(w + tz) − ∇g(w)(cid:107)2,1 = max
(cid:107)u(cid:107)1≤1

uy(∇ψ(y; w + tz) − ∇ψ(y; w))

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

|uy|(cid:107)∇ψ(y; w + tz) − ∇ψ(y; w)(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
0
(cid:90) 1

0

(91)
≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

y∈Y
(cid:88)

y∈Y

≤ max
(cid:107)u(cid:107)1≤1

≤ Lt(cid:107)z(cid:107)2 ,

from the L-smoothness of ψ. Plugging this back into (92) completes the proof. The bound on the smothing
approximation holds similarly by noticing that if h is 1-Lipschitz then hµω too since ∇hµω(u) ∈ dom h∗
for any u ∈ dom h.

D.6

Information Based Complexity of the Prox-Linear Algorithm with Casimir-SVRG

Proposition 37. Consider the setting of Thm. 35. Suppose the sequence {(cid:15)k}k≥1 satisﬁes (cid:15)k = (cid:15)0/k for
some (cid:15)0 > 0 and that the subproblem of Line 3 of Algo. 5 is solved using Casimir-SVRG with the settings
of Prop. 29. Then, total number of SVRG iterations N required to produce a w such that (cid:107)(cid:37)η(w)(cid:107)2 ≤ (cid:15) is
bounded as

E[N ] ≤ (cid:101)O





n
η(cid:15)2 (F (w0) − F ∗ + (cid:15)0) +

(cid:113)

AωDωn(cid:15)−1
0
η(cid:15)3



(F (w0) − F ∗ + (cid:15)0)3/2

 .

75

Proof. First note that (cid:80)K
k=1 k−1 ≤ 4(cid:15)0 log K for K ≥ 2. Let ∆F0 := F (w0) − F ∗ and use
shorthand A, D for Aω, Dω respectively. From Thm. 35, the number K of prox-linear iterations required to
ﬁnd a w such that (cid:107)(cid:37)η(w)(cid:107)2 ≤ (cid:15) must satisfy

k=1 (cid:15)k ≤ (cid:15)0

(cid:80)K

For this, it sufﬁces to have (see e.g., Lemma 54)

2
ηK

(∆F0 + 4(cid:15)0 log K) ≤ (cid:15) .

K ≥

4(∆F0 + 4(cid:15)0)
η(cid:15)2

log

(cid:18) 4(∆F0 + 4(cid:15)0)
η(cid:15)2

(cid:19)

.

Before we can invoke Prop. 29, we need to bound the dependence of each inner loop on its warm start:
Fη(wk−1; wk−1) − Fη(w∗
k; wk−1) in terms of problem parameters, where w∗
k = arg minw Fη(w; wk−1)
is the exact result of an exact prox-linear step. We note that Fη(wk−1; wk−1) = F (wk−1) ≤ F (w0), by
Line 4 of Algo. 5. Moreover, from η ≤ 1/L and Asmp. 33, we have,

Fη(w∗

k; wk−1) =

h(cid:0)g(i)(wk−1) + ∇g(i)(wk−1)(w∗

k − wk−1)(cid:1) +

(cid:107)w∗

k(cid:107)2

2 +

(cid:107)w∗

k − wk−1(cid:107)2
2

h(cid:0)g(i)(wk−1) + ∇g(i)(wk−1)(w∗

k − wk−1)(cid:1) +

(cid:107)w∗

k(cid:107)2

2 +

(cid:107)w∗

k − wk−1(cid:107)2
2

λ
2

λ
2

1
2η

L
2

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

1
n

1
n

1
n

≥

≥

h(cid:0)g(i)(w∗

k)(cid:1) +

(cid:107)w∗

k(cid:107)2
2

λ
2

i=1
= F (w∗

k) ≥ F ∗ .

Thus, we bound Fη(wk−1; wk−1) − Fη(w∗
k; wk−1) ≤ ∆F0. We now invoke Prop. 29 and collect all
constants and terms logarithmic in n, (cid:15)−1, (cid:15)−1
0 , ∆F0, η−1, AωDω in T , T (cid:48), T (cid:48)(cid:48). We note that all terms in the
logarithm in Prop. 29 are logarithmic in the problem parameters here. Letting Nk be the number of SVRG
iterations required for iteration k, we get,

E[N ] =

E[Nk] ≤

(cid:32)

K
(cid:88)

n +

(cid:33)

T

(cid:114) ηADn
(cid:15)k
(cid:33)(cid:35)

√

k

T

k=1
(cid:114) ηADn
(cid:15)0
(cid:114) ηADn
(cid:15)0

(cid:32) K
(cid:88)

k=1
(cid:35)

K3/2

T (cid:48)

K
(cid:88)

k=1
(cid:34)

≤

nK +

≤

nK +

(cid:34)

(cid:34)

(cid:34)

≤

=

n
η(cid:15)2 (∆F0 + (cid:15)0) +

(cid:114) ηADn
(cid:15)0

(cid:18) ∆F0 + (cid:15)0
η(cid:15)2

(cid:19)3/2(cid:35)

T (cid:48)(cid:48)

n
η(cid:15)2 (∆F0 + (cid:15)0) +

ADn
η(cid:15)3

(∆F0 + (cid:15)0)3/2
√
(cid:15)0

(cid:35)

T (cid:48)(cid:48) .

√

76

D.7 Some Helper Lemmas

The ﬁrst lemma is a property of the squared Euclidean norm from Lin et al. [2018, Lemma 5], which we
restate here.

Lemma 51. For any vectors, w, z, r ∈ Rd, we have, for any θ > 0,

(cid:107)w − z(cid:107)2 ≥ (1 − θ)(cid:107)w − r(cid:107)2 +

1 −

(cid:107)r − z(cid:107)2 .

(cid:18)

(cid:19)

1
θ

The next lemmas consider rates of the sequences (αk) and (Ak) under different recursions.

Lemma 52. Deﬁne a sequence (αk)k≥0 as

Then this sequence satisﬁes

Moreover, Ak := (cid:81)k

j=0(1 − αk) satisﬁes

√

5 − 1
α0 =
2
k = (1 − αk)α2
α2

k−1 .

√

2
k + 3

≤ αk ≤

2
k + 3

.

2

(k + 3)2 ≤ Ak ≤

4
(k + 3)2 .

Proof. Notice that α0 satisﬁes α2
Hence, we can deﬁne a sequence (bk)k≥0 such that bk := 1/αk. It satisﬁes the recurrence, b2
for k ≥ 1, or in other words, bk = 1
2

0 = 1 − α0. Further, it is clear from deﬁnition that αk ∈ (0, 1) ∀k ≥ 0.
k−1

. Form this we get,

k − bk = b2

1 + 4b2

1 +

k−1

(cid:113)

(cid:16)

(cid:17)

bk ≥ bk−1 +

≥ b0 +

≥

+

k
2

3
2

k
2

.

1
2

since b0 =

. This gives us the upper bound on αk. Moreover, unrolling the recursion,

√

5+1
2

k = (1 − αk)α2
α2

k−1 = Ak

α2
0
1 − α0

= Ak .

(93)

Since αk ≤ 2/(k + 3), (93) yields the upper bound on Ak. The upper bound on αk again gives us,

Ak ≥

k
(cid:89)

(cid:18)

1 −

i=0

(cid:19)

2
i + 3

=

2
(k + 2)(k + 3)

≥

2
(k + 3)2 ,

to get the lower bound on Ak. Invoking (93) again to obtain the lower bound on αk completes the proof.

The next lemma considers the evolution of the sequences (αk) and (Ak) with a different recursion.

77

Lemma 53. Consider a sequence (αk)k≥0 deﬁned by α0 =

, and αk+1 as the non-negative root of

Further, deﬁne

Then, we have for all k ≥ 0,

√

5−1
2

α2
k
1 − αk

= α2

k−1

k
k + 1

.

Ak =

(1 − αi) .

k
(cid:89)

i=0

1
k + 1

(cid:18)

1 −

(cid:19)

1
√
2

≤ Ak ≤

1
k + 2

.

Proof. Deﬁne a sequence (bk)k≥0 such that bk = 1/αk, for each k. This is well-deﬁned because αk (cid:54)=
0, which may be veriﬁed by induction. This sequence satisﬁes the recursion for k ≥ 1: bk(bk − 1) =
(cid:0) k+1
k

(cid:1) bk−1. From this recursion, we get,

(cid:32)

(cid:115)

bk =

1 +

1 + 4b2

k−1

(cid:19)(cid:33)

(cid:18) k + 1
k

1
2

1
2

1
2
√

≥

≥

+ bk−1
(cid:32)

(cid:114)

1 +

(cid:114)

k + 1
k

k + 1
k

k + 1
2

k + 1

√

=

(∗)
≥

(cid:16)

√

(cid:16)√

1/

2 + · · · + 1/

k + 1

+ b0

k + 1

+ · · · +

+ b0

k + 1

(cid:114)

(cid:33)

k + 1
2

√

√

(cid:17)
2

(cid:17)

√

√

√

√

(cid:16)√

k + 2 + b0 −
√

=

k + 1

k + 2 + b0 −

k + 1 ≥ (cid:82) k+2

2

dx√
x = 2(

√

k + 2 −

2) . Since

√

(cid:17)
2

,

√

where (∗) followed from noting that 1/

2 + · · · + 1/

b0 = 1/α0 =

2, we have, for k ≥ 1,

√

5+1
2 >

√

αk ≤

√

1

√

k + 1(

k + 2 + b0 −

2)

√

≤

√

1
√
k + 1

.

k + 2

This relation also clearly holds for k = 0. Next, we claim that

Ak = (k + 1)α2

k ≤

√

k + 1
√

(

k + 1

k + 2)2

=

1
k + 2

.

Indeed, this is true because

For the lower bound, we have,

k = (1 − αk)α2
α2

k−1

k
k + 1

= Ak

α2
0
1 − α0

1
k + 1

=

Ak
k + 1

.

Ak =

(1 − αi) ≥

1 −

√

k
(cid:89)

(cid:18)

i=0

1
√
i + 1

i + 2

(cid:19)

(cid:18)

≥

1 −

(cid:19) k
(cid:89)

(cid:18)

1 −

1
√
2

(cid:19)

=

1
i + 1

1 − 1√
2
k + 1

.

i=1

k
(cid:89)

i=0

(94)

(95)

(96)

(97)

78

Lemma 54. Fix some (cid:15) > 0. If k ≥ 2

(cid:15) log 2

(cid:15) , then we have that log k

k ≤ (cid:15).

Proof. We have, since log x ≤ x for x > 0,

log k
k

≤

log 2

(cid:15) + log log 2
(cid:15) log 2

2

(cid:15)

(cid:15)

(cid:32)

(cid:15)
2

=

1 +

(cid:33)

log log 2
(cid:15)
log 2
(cid:15)

≤ (cid:15) .

E Experiments: Extended Evaluation

Given here are plots for all missing classes of PASCAL VOC 2007. Figures 8 to 10 contain the extension of
Figure 3 while Figures 11 to 13 contain the extension of Figure 4 to all classes.

79

Figure 8: Comparison of convex optimization algorithms for the task of visual object localization on PAS-
CAL VOC 2007 for λ = 10/n for all other classes (1/3).

80

Figure 9: Comparison of convex optimization algorithms for the task of visual object localization on PAS-
CAL VOC 2007 for λ = 10/n for all other classes (2/3).

81

Figure 10: Comparison of convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 10/n for all other classes (3/3).

82

Figure 11: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n for all other classes (1/3).

83

Figure 12: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n for all other classes (2/3).

84

Figure 13: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n for all other classes (3/3).

85

9
1
0
2
 
b
e
F
 
8
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
8
2
2
3
0
.
2
0
9
1
:
v
i
X
r
a

A Smoother Way to Train Structured Prediction Models

Krishna Pillutla1

Vincent Roulet2

Sham M. Kakade1,2

Zaid Harchaoui2

1 Paul G. Allen School of Computer Science and Engineering, University of Washington
2 Department of Statistics, University of Washington
{pillutla,sham}@cs.washington.edu, {vroulet,zaid}@uw.edu

Abstract

We present a framework to train a structured prediction model by performing smoothing on the in-
ference algorithm it builds upon. Smoothing overcomes the non-smoothness inherent to the maximum
margin structured prediction objective, and paves the way for the use of fast primal gradient-based op-
timization algorithms. We illustrate the proposed framework by developing a novel primal incremental
optimization algorithm for the structural support vector machine. The proposed algorithm blends an
extrapolation scheme for acceleration and an adaptive smoothing scheme and builds upon the stochastic
variance-reduced gradient algorithm. We establish its worst-case global complexity bound and study
several practical variants, including extensions to deep structured prediction. We present experimen-
tal results on two real-world problems, namely named entity recognition and visual object localization.
The experimental results show that the proposed framework allows us to build upon efﬁcient inference
algorithms to develop large-scale optimization algorithms for structured prediction which can achieve
competitive performance on the two real-world problems.

1 Introduction

Consider the optimization problem arising when training maximum margin structured prediction models:

(cid:34)

min
w∈Rd

F (w) :=

1
n

n
(cid:88)

i=1

(cid:35)

f (i)(w) +

(cid:107)w(cid:107)2
2

,

λ
2

(1)

where each f (i) is the structural hinge loss. Max-margin structured prediction was designed to forecast
discrete data structures such as sequences and trees [Taskar et al., 2004, Tsochantaridis et al., 2004].

Batch non-smooth optimization algorithms such as cutting plane methods are appropriate for problems
with small or moderate sample sizes [Tsochantaridis et al., 2004, Joachims et al., 2009]. Stochastic non-
smooth optimization algorithms such as stochastic subgradient methods can tackle problems with large
sample sizes [Ratliff et al., 2007, Shalev-Shwartz et al., 2011]. However, both families of methods achieve
the typical worst-case complexity bounds of non-smooth optimization algorithms and cannot easily leverage
a possible hidden smoothness of the objective.

Furthermore, as signiﬁcant progress is being made on incremental smooth optimization algorithms for
training unstructured prediction models [Lin et al., 2018], we would like to transfer such advances and design
faster optimization algorithms to train structured prediction models. Indeed if each term in the ﬁnite-sum
were L-smooth, incremental optimization algorithms such as MISO [Mairal, 2015], SAG [Le Roux et al.,
2012, Schmidt et al., 2017], SAGA [Defazio et al., 2014], SDCA [Shalev-Shwartz and Zhang, 2013], and

1

SVRG [Johnson and Zhang, 2013] could leverage the ﬁnite-sum structure of the objective (1) and achieve
faster convergence than batch algorithms on large-scale problems.

Incremental optimization algorithms can be further accelerated, either on a case-by-case basis [Shalev-
Shwartz and Zhang, 2014, Frostig et al., 2015, Allen-Zhu, 2017, Defazio, 2016] or using the Catalyst accel-
eration scheme [Lin et al., 2015, 2018], to achieve near-optimal convergence rates [Woodworth and Srebro,
2016]. Accelerated incremental optimization algorithms demonstrate stable and fast convergence behavior
on a wide range of problems, in particular for ill-conditioned ones.

We introduce a general framework that allows us to bring the power of accelerated incremental optimiza-
tion algorithms to the realm of structured prediction problems. To illustrate our framework, we focus on the
problem of training a structural support vector machine (SSVM), and extend the developed algorithms to
deep structured prediction models with nonlinear mappings.

We seek primal optimization algorithms, as opposed to saddle-point or primal-dual optimization algo-
rithms, in order to be able to tackle structured prediction models with afﬁne mappings such as SSVM as
well as deep structured prediction models with nonlinear mappings. We show how to shade off the inherent
non-smoothness of the objective while still being able to rely on efﬁcient inference algorithms.

Smooth Inference Oracles. We introduce a notion of smooth inference oracles that gracefully ﬁts the
framework of black-box ﬁrst-order optimization. While the exp inference oracle reveals the relation-
ship between max-margin and probabilistic structured prediction models, the top-K inference oracle
can be efﬁciently computed using simple modiﬁcations of efﬁcient inference algorithms in many cases
of interest.

Incremental Optimization Algorithms. We present a new algorithm built on top of SVRG, blending an
extrapolation scheme for acceleration and an adaptive smoothing scheme. We establish the worst-
case complexity bounds of the proposed algorithm and extend it to the case of non-linear mappings.
Finally, we demonstrate its effectiveness compared to competing algorithms on two tasks, namely
named entity recognition and visual object localization.

The code is publicly available as a software library called Casimir1. The outline of the paper is as
follows: Sec. 1.1 reviews related work. Sec. 2 discusses smoothing for structured prediction followed by
Sec. 3, which deﬁnes and studies the properties of inference oracles and Sec. 4, which describes the concrete
implementation of these inference oracles in several settings of interest. Then, we switch gears to study
accelerated incremental algorithms in convex case (Sec. 5) and their extensions to deep structured prediction
(Sec. 6). Finally, we evaluate the proposed algorithms on two tasks, namely named entity recognition and
visual object localization in Sec. 7.

1.1 Related Work

Optimization for Structural Support Vector Machines Table 1 gives an overview of different opti-
mization algorithms designed for structural support vector machines. Early works [Taskar et al., 2004,
Tsochantaridis et al., 2004, Joachims et al., 2009, Teo et al., 2009] considered batch dual quadratic opti-
mization (QP) algorithms. The stochastic subgradient method operated directly on the non-smooth primal
formulation [Ratliff et al., 2007, Shalev-Shwartz et al., 2011]. More recently, Lacoste-Julien et al. [2013]
proposed a block coordinate Frank-Wolfe (BCFW) algorithm to optimize the dual formulation of structural

1https://github.com/krishnap25/casimir

2

Table 1: Convergence rates given in terms of the number of calls to various oracles for different optimization algo-
rithms on the learning problem (1) in case of structural support vector machines (4). The rates are speciﬁed in terms
of the target accuracy (cid:15), the number of training examples n, the regularization λ, the size of the label space |Y|, the
max feature norm R = maxi (cid:107)Φ(x(i), y) − Φ(x(i), y(i))(cid:107)2 and (cid:101)R ≥ R (see Remark 28 for explicit form). The rates
are speciﬁed up to constants and factors logarithmic in the problem parameters. The dependence on the initial error is
ignored. * denotes algorithms that make O(1) oracle calls per iteration.

Algo. (exp oracle)

# Oracle calls

Exponentiated
gradient*
[Collins et al., 2008]
Excessive gap
reduction
[Zhang et al., 2014]
Prop. 29*,
entropy smoother

Prop. 30*,
entropy smoother

(n + log |Y|)R2
λ(cid:15)

nR

(cid:114)

(cid:114)

log |Y|
λ(cid:15)

nR2 log|Y|
λ(cid:15)

n +

R2 log|Y|
λ(cid:15)

Algo. (max oracle)

# Oracle calls

BMRM
[Teo et al., 2009]

QP 1-slack
[Joachims et al., 2009]

Stochastic
subgradient*
[Shalev-Shwartz et al., 2011]
Block-Coordinate
Frank-Wolfe*
[Lacoste-Julien et al., 2013]

nR2
λ(cid:15)
nR2
λ(cid:15)

R2
λ(cid:15)

n +

R2
λ(cid:15)

Algo.
(top-K oracle)

# Oracle calls

Prop. 29*,
(cid:96)2
2 smoother

Prop. 30*,
(cid:96)2
2 smoother

(cid:114)

n (cid:101)R2
λ(cid:15)
n + (cid:101)R2
λ(cid:15)

support vector machines; see also Osokin et al. [2016] for variants and extensions. Saddle-point or primal-
dual approaches include the mirror-prox algorithm [Taskar et al., 2006, Cox et al., 2014, He and Harchaoui,
2015]. Palaniappan and Bach [2016] propose an incremental optimization algorithm for saddle-point prob-
lems. However, it is unclear how to extend it to the structured prediction problems considered here. Incre-
mental optimization algorithms for conditional random ﬁelds were proposed by Schmidt et al. [2015]. We
focus here on primal optimization algorithms in order to be able to train structured prediction models with
afﬁne or nonlinear mappings with a uniﬁed approach, and on incremental optimization algorithms which
can scale to large datasets.

Inference The ideas of dynamic programming inference in tree structured graphical models have been
around since the pioneering works of Pearl [1988] and Dawid [1992]. Other techniques emerged based on
graph cuts [Greig et al., 1989, Ishikawa and Geiger, 1998], bipartite matchings [Cheng et al., 1996, Taskar
et al., 2005] and search algorithms [Daum´e III and Marcu, 2005, Lampert et al., 2008, Lewis and Steedman,
2014, He et al., 2017]. For graphical models that admit no such a discrete structure, techniques based on
loopy belief propagation [McEliece et al., 1998, Murphy et al., 1999], linear programming (LP) [Schlesinger,
1976], dual decomposition [Johnson, 2008] and variational inference [Wainwright et al., 2005, Wainwright
and Jordan, 2008] gained popularity.

Top-K Inference Smooth inference oracles with (cid:96)2
2 smoothing echo older heuristics in speech and lan-
guage processing [Jurafsky et al., 2014]. Combinatorial algorithms for top-K inference have been studied
extensively by the graphical models community under the name “M -best MAP”. Seroussi and Golmard
[1994] and Nilsson [1998] ﬁrst considered the problem of ﬁnding the K most probable conﬁgurations in a
tree structured graphical model. Later, Yanover and Weiss [2004] presented the Best Max-Marginal First
algorithm which solves this problem with access only to an oracle that computes max-marginals. We also
use this algorithm in Sec. 4.2. Fromer and Globerson [2009] study top-K inference for LP relaxation, while
Batra [2012] considers the dual problem to exploit graph structure. Flerova et al. [2016] study top-K exten-
sions of the popular A(cid:63) and branch and bound search algorithms in the context of graphical models. Other

3

related approaches include diverse K-best solutions [Batra et al., 2012] and ﬁnding K-most probable modes
[Chen et al., 2013].

Smoothing Inference Smoothing for inference was used to speed up iterative algorithms for continu-
ous relaxations. Johnson [2008] considered smoothing dual decomposition inference using the entropy
smoother, followed by Jojic et al. [2010] and Savchynskyy et al. [2011] who studied its theoretical prop-
erties. Meshi et al. [2012] expand on this study to include (cid:96)2
2 smoothing. Explicitly smoothing discrete
inference algorithms in order to smooth the learning problem was considered by Zhang et al. [2014] and
Song et al. [2014] using the entropy and (cid:96)2
2 smoother was also used by Mar-
tins and Astudillo [2016]. Hazan et al. [2016] consider the approach of blending learning and inference,
instead of using inference algorithms as black-box procedures.

2 smoothers respectively. The (cid:96)2

Related ideas to ours appear in the independent works [Mensch and Blondel, 2018, Niculae et al., 2018].
These works partially overlap with ours, but the papers choose different perspectives, making them com-
plementary to each other. Mensch and Blondel [2018] proceed differently when, e.g., smoothing inference
based on dynamic programming. Moreover, they do not establish complexity bounds for optimization al-
gorithms making calls to the resulting smooth inference oracles. We deﬁne smooth inference oracles in the
context of black-box ﬁrst-order optimization and establish worst-case complexity bounds for incremental
optimization algorithms making calls to these oracles. Indeed we relate the amount of smoothing controlled
by µ to the resulting complexity of the optimization algorithms relying on smooth inference oracles.

End-to-end Training of Structured Prediction The general framework for global training of structured
prediction models was introduced by Bottou and Gallinari [1990] and applied to handwriting recognition
by Bengio et al. [1995] and to document processing by Bottou et al. [1997]. This approach, now called
“deep structured prediction”, was used, e.g., by Collobert et al. [2011] and Belanger and McCallum [2016].

1.2 Notation

Vectors are denoted by bold lowercase characters as w ∈ Rd while matrices are denoted by bold uppercase
characters as A ∈ Rd×n. For a matrix A ∈ Rm×n, deﬁne the norm for α, β ∈ {1, 2, ∞},

For any function f : Rd → R ∪ {+∞}, its convex conjugate f ∗ : Rd → R ∪ {+∞} is deﬁned as

(cid:107)A(cid:107)β,α = max{(cid:104)y, Ax(cid:105) | (cid:107)y(cid:107)α ≤ 1 , (cid:107)x(cid:107)β ≤ 1} .

(2)

f ∗(z) = sup
w∈Rd

{(cid:104)z, w(cid:105) − f (w)} .

A function f : Rd → R is said to be L-smooth with respect to an arbitrary norm (cid:107)·(cid:107) if it is continuously
differentiable and its gradient ∇f is L-Lipschitz with respect to (cid:107)·(cid:107). When left unspeciﬁed, (cid:107)·(cid:107) refers to
(cid:107) · (cid:107)2. Given a continuously differentiable map g : Rd → Rm, its Jacobian ∇g(w) ∈ Rm×d at w ∈ Rd is
deﬁned so that its ijth entry is [∇g(w)]ij = ∂gi(w)/wj where gi is the ith element of g and wj is the jth
element of w. The vector valued function g : Rd → Rm is said to be L-smooth with respect to (cid:107)·(cid:107) if it is
continuously differentiable and its Jacobian ∇g is L-Lipschitz with respect to (cid:107)·(cid:107).

For a vector z ∈ Rm, z(1) ≥ · · · ≥ z(m) refer to its components enumerated in non-increasing order
where ties are broken arbitrarily. Further, we let z[k] = (z(1), · · · , z(k)) ∈ Rk denote the vector of the k
largest components of z. We denote by ∆m−1 the standard probability simplex in Rm. When the dimension
is clear from the context, we shall simply denote it by ∆. Moreover, for a positive integer p, [p] refers to the
set {1, . . . , p}. Lastly, (cid:101)O in the big-O notation hides factors logarithmic in problem parameters.

4

2 Smooth Structured Prediction

Structured prediction aims to search for score functions φ parameterized by w ∈ Rd that model the compat-
ibility of input x ∈ X and output y ∈ Y as φ(x, y; w) through a graphical model. Given a score function
φ(·, ·; w), predictions are made using an inference procedure which, when given an input x, produces the
best output

y∗(x; w) ∈ arg max

φ(x, y; w) .

y∈Y

(3)

We shall return to the score functions and the inference procedures in Sec. 3. First, given such a score
function φ, we deﬁne the structural hinge loss and describe how it can be smoothed.

2.1 Structural Hinge Loss

On a given input-output pair (x, y), the error of prediction of y by the inference procedure with a score func-
tion φ(·, ·; w), is measured by a task loss (cid:96)(cid:0)y, y∗(x; w)(cid:1) such as the Hamming loss. The learning procedure
would then aim to ﬁnd the best parameter w that minimizes the loss on a given dataset of input-output train-
ing examples. However, the resulting problem is piecewise constant and hard to optimize. Instead, Altun
et al. [2003], Taskar et al. [2004], Tsochantaridis et al. [2004] propose to minimize a majorizing surrogate
of the task loss, called the structural hinge loss deﬁned on an input-output pair (x(i), y(i)) as

f (i)(w) = max
y∈Y

(cid:110)

φ(x(i), y; w) + (cid:96)(y(i), y)

(cid:111)

− φ(x(i), y(i); w) = max
y∈Y

ψ(i)(y, w) .

(4)

where ψ(i)(y; w) = φ(x(i), y; w) + (cid:96)(y(i), y) − φ(x(i), y(i); w) is the augmented score function.

This approach, known as max-margin structured prediction, builds upon binary and multi-class support
vector machines [Crammer and Singer, 2001], where the term (cid:96)(y(i), y) inside the maximization in (4)
generalizes the notion of margin. The task loss (cid:96) is assumed to possess appropriate structure so that the
maximization inside (4), known as loss augmented inference, is no harder than the inference problem in (3).
When considering a ﬁxed input-output pair (x(i), y(i)), we drop the index with respect to the sample i and
consider the structural hinge loss as

f (w) = max
y∈Y

ψ(y; w),

(5)

When the map w (cid:55)→ ψ(y; w) is afﬁne, the structural hinge loss f and the objective F from (1) are both
convex - we refer to this case as the structural support vector machine. When w (cid:55)→ ψ(y; w) is a nonlinear
but smooth map, then the structural hinge loss f and the objective F are nonconvex.

2.2 Smoothing Strategy

A convex, non-smooth function h can be smoothed by taking its inﬁmal convolution with a smooth func-
tion [Beck and Teboulle, 2012]. We now recall its dual representation, which Nesterov [2005b] ﬁrst used to
relate the amount of smoothing to optimal complexity bounds.

Deﬁnition 1. For a given convex function h : Rm → R, a smoothing function ω : dom h∗ → R which is
1-strongly convex with respect to (cid:107) · (cid:107)α (for α ∈ {1, 2}), and a parameter µ > 0, deﬁne

hµω(z) = max

u∈dom h∗

{(cid:104)u, z(cid:105) − h∗(u) − µω(u)} .

as the smoothing of h by µω.

5

We now state a classical result showing how the parameter µ controls both the approximation error and the
level of the smoothing. For a proof, see Beck and Teboulle [2012, Thm. 4.1, Lemma 4.2] or Prop. 39 of
Appendix A.

Proposition 2. Consider the setting of Def. 1. The smoothing hµω is continuously differentiable and its
gradient, given by

∇hµω(z) = arg max
u∈dom h∗

{(cid:104)u, z(cid:105) − h∗(u) − µω(u)}

is 1/µ-Lipschitz with respect to (cid:107) · (cid:107)∗
µ1 ≥ µ2 ≥ 0,

α. Moreover, letting hµω ≡ h for µ = 0, the smoothing satisﬁes, for all

(µ1 − µ2)

inf
u∈dom h∗

ω(u) ≤ hµ2ω(z) − hµ1ω(z) ≤ (µ1 − µ2)

sup
u∈dom h∗

ω(u) .

Smoothing the Structural Hinge Loss We rewrite the structural hinge loss as a composition

(cid:40)Rd → Rm

g :

(cid:40)Rm → R

h :

w (cid:55)→ (ψ(y; w))y∈Y ,

z

(cid:55)→ maxi∈[m] zi,

where m = |Y| so that the structural hinge loss reads

(6)

(7)

We smooth the structural hinge loss (7) by simply smoothing the non-smooth max function h as

f (w) = h ◦ g(w) .

fµω = hµω ◦ g.

When g is smooth and Lipschitz continuous, fµω is a smooth approximation of the structural hinge loss,
whose gradient is readily given by the chain-rule. In particular, when g is an afﬁne map g(w) = Aw + b, if
follows that fµω is ((cid:107)A(cid:107)2
β,α/µ)-smooth with respect to (cid:107) · (cid:107)β (cf. Lemma 40 in Appendix A). Furthermore,
for µ1 ≥ µ2 ≥ 0, we have,

(µ1 − µ2) min

ω(u) ≤ fµ2ω(w) − fµ1ω(w) ≤ (µ1 − µ2) max

ω(u) .

u∈∆m−1

u∈∆m−1

In the context of smoothing the max function, we now describe two popular choices for the smoothing
function ω, followed by computational considerations.

2.3 Smoothing Variants

2.3.1 Entropy and (cid:96)2

2 smoothing

When h is the max function, the smoothing operation can be computed analytically for the entropy smoother
and the (cid:96)2

2 smoother, denoted respectively as

−H(u) := (cid:104)u, log u(cid:105)

and

(cid:96)2
2(u) := 1

2 ((cid:107)u(cid:107)2

2 − 1) .

These lead respectively to the log-sum-exp function [Nesterov, 2005b, Lemma 4]

h−µH (z) = µ log

ezi/µ

, ∇h−µH (z) =

(cid:33)

(cid:32) m
(cid:88)

i=1

(cid:35)

(cid:34)

ezi/µ
j=1 ezj /µ

(cid:80)m

,

i=1,...,m

6

and an orthogonal projection onto the simplex,

hµ(cid:96)2

2

(z) = (cid:104)z, proj∆m−1(z/µ)(cid:105) − µ

2 (cid:107) proj∆m−1(z/µ)(cid:107)2 + µ

2 , ∇hµ(cid:96)2

2

(z) = proj∆m−1(z/µ) .

Furthermore, the following holds for all µ1 ≥ µ2 ≥ 0 from Prop. 2:

0 ≤ h−µ1H (z) − h−µ2H (z) ≤ (µ1 − µ2) log m,

and, 0 ≤ hµ1(cid:96)2

(z) − hµ2(cid:96)2

2

2

(z) ≤ 1

2 (µ1 − µ2) .

2.3.2 Top-K Strategy

Though the gradient of the composition fµω = hµω ◦ g can be written using the chain rule, its actual
computation for structured prediction problems involves computing ∇g over all m = |Y| of its components,
which may be intractable. However, in the case of (cid:96)2
2 smoothing, projections onto the simplex are sparse, as
pointed out by the following proposition.

Proposition 3. Consider the Euclidean projection u∗ = arg minu∈∆m−1 (cid:107)u − z/µ(cid:107)2
the simplex, where µ > 0. The projection u∗ has exactly k ∈ [m] non-zeros if and only if

2 of z/µ ∈ Rm onto

where z(1) ≥ · · · ≥ z(m) are the components of z in non-decreasing order and z(m+1) := −∞. In this case,
u∗ is given by

k
(cid:88)

i=1

(cid:0)z(i) − z(k)

(cid:1) < µ ≤

(cid:0)z(i) − z(k+1)

(cid:1) ,

k
(cid:88)

i=1

(cid:26)

u∗
i = max

0, 1
kµ

(cid:0)zi − z(j)

(cid:1) + 1
k

(cid:27)

.

k
(cid:88)

j=1

m
(cid:88)

i=1

(cid:18) zi
µ

+ ρ

= 1 ,

(cid:19)

+

Proof. The projection u∗ satisﬁes u∗

i = (zi/µ + ρ∗)+, where ρ∗ is the unique solution of ρ in the equation

where α+ = max{0, α}. See, e.g., Held et al. [1974] for a proof of this fact. Note that z(i)/µ + ρ∗ ≤ 0
implies that z(j)/µ + ρ∗ ≤ 0 for all j ≥ i. Therefore u∗ has k non-zeros if and only if z(k)/µ + ρ∗ > 0 and
z(k+1)/µ + ρ∗ ≤ 0.

Now suppose that u∗ has exactly k non-zeros, we can then solve (9) to obtain ρ∗ = ϕk(z/µ), which is

deﬁned as

ϕk

(cid:19)

(cid:18) z
µ

:=

−

1
k

1
k

k
(cid:88)

i=1

z(i)
µ

.

Plugging in the value of ρ∗ in z(k)/µ + ρ∗ > 0 gives µ > (cid:80)k
(cid:1).
gives µ ≤ (cid:80)k

(cid:0)z(i) − z(k+1)

i=1

i=1

(cid:0)z(i) − z(k)

(cid:1). Likewise, z(k+1)/µ + ρ∗ ≤ 0

Conversely assume (8) and let (cid:98)ρ = ϕk(z/µ). Eq. (8) can be written as z(k)/µ+(cid:98)ρ > 0 and z(k+1)/µ+(cid:98)ρ ≤
0. Furthermore, we verify that (cid:98)ρ satisﬁes Eq. (9), and so (cid:98)ρ = ρ∗ is its unique root. It follows, therefore, that
the sparsity of u∗ is k.

7

(8)

(9)

(10)

Thus, the projection of z/µ onto the simplex picks out some number Kz/µ of the largest entries of z/µ
- we refer to this as the sparsity of proj∆m−1(z/µ). This fact motivates the top-K strategy: given µ > 0, ﬁx
an integer K a priori and consider as surrogates for hµ(cid:96)2

and ∇hµ(cid:96)2

respectively

2

2

hµ,K(z) := max

u∈∆K−1

(cid:8)(cid:10)z[K], u(cid:11) − µ(cid:96)2

2(u)(cid:9) ,

and,

(cid:101)∇hµ,K(z) := ΩK(z)(cid:62) proj∆K−1

(cid:19)

(cid:18) z[K]
µ

,

where z[K] denotes the vector composed of the K largest entries of z and ΩK : Rm → {0, 1}K×m deﬁnes
their extraction, i.e., ΩK(z) = (e(cid:62)
)(cid:62) ∈ {0, 1}K×m where j1, · · · , jK satisfy zj1 ≥ · · · ≥ zjK
j1
such that z[K] = ΩK(z)z . A surrogate of the (cid:96)2

2 smoothing is then given by

, . . . , e(cid:62)
jK

fµ,K := hµ,K ◦ g ,

and,

(cid:101)∇fµ,K(w) := ∇g(w)(cid:62) (cid:101)∇hµ,K(g(w)) .

(11)

Exactness of Top-K Strategy We say that the top-K strategy is exact at z for µ > 0 when it recovers
(z) = (cid:101)∇hµ,K(z). The next
the ﬁrst order information of hµ(cid:96)2
proposition outlines when this is the case. Note that if the top-K strategy is exact at z for a smoothing
parameter µ > 0 then it will be exact at z for any µ(cid:48) < µ.

(z) = hµ,K(z) and ∇hµ(cid:96)2

, i.e. when hµ(cid:96)2

2

2

2

Proposition 4. The top-K strategy is exact at z for µ > 0 if

µ ≤

(cid:0)z(i) − z(K+1)

(cid:1) .

K
(cid:88)

i=1

(12)

Moreover, for any ﬁxed z ∈ Rm such that the vector z[K+1] = ΩK+1(z)z has at least two unique elements,
the top-K strategy is exact at z for all µ satisfying 0 < µ ≤ z(1) − z(K+1).

Proof. First, we note that the top-K strategy is exact when the sparsity Kz/µ of the projection proj∆m−1(z/µ)
satisﬁes Kz/µ ≤ K. From Prop. 3, the condition that Kz/µ ∈ {1, 2, · · · , K} happens when

µ ∈

K
(cid:91)

(cid:32) k

(cid:88)

k=1

i=1

(cid:0)z(i) − z(k)

(cid:1) ,

(cid:0)z(i) − z(k+1)

(cid:1)

k
(cid:88)

i=1

(cid:35)

(cid:32)

=

0,

K
(cid:88)

i=1

(cid:35)

(cid:0)z(i) − z(K+1)

(cid:1)

,

since the intervals in the union are contiguous. This establishes (12).

The only case when (12) cannot hold for any value of µ > 0 is when the right hand size of (12) is zero.
In the opposite case when z[K+1] has at least two unique components, or equivalently, z(1) − z(K+1) > 0,
the condition 0 < µ ≤ z(1) − z(K+1) implies (12).

If the top-K strategy is exact at g(w) for µ, then

fµ,K(w) = fµ(cid:96)2

(w)

2

and

(cid:101)∇fµ,K(w) = ∇fµ(cid:96)2

(w) ,

2

where the latter follows from the chain rule. When used instead of (cid:96)2
2 smoothing in the algorithms presented
in Sec. 5, the top-K strategy provides a computationally efﬁcient heuristic to smooth the structural hinge
loss. Though we do not have theoretical guarantees using this surrogate, experiments presented in Sec. 7
show its efﬁciency and its robustness to the choice of K.

8

This section studies ﬁrst order oracles used in standard and smoothed structured prediction. We ﬁrst describe
the parameterization of the score functions through graphical models.

3 Inference Oracles

3.1 Score Functions

Structured prediction is deﬁned by the structure of the output y, while input x ∈ X can be arbitrary.
Each output y ∈ Y is composed of p components y1, . . . , yp that are linked through a graphical model
G = (V, E) - the nodes V = {1, · · · , p} represent the components of the output y while the edges E deﬁne
the dependencies between various components. The value of each component yv for v ∈ V represents the
state of the node v and takes values from a ﬁnite set Yv. The set of all output structures Y = Y1 × · · · × Yp
is then ﬁnite yet potentially intractably large.

The structure of the graph (i.e., its edge structure) depends on the task. For the task of sequence labeling,
the graph is a chain, while for the task of parsing, the graph is a tree. On the other hand, the graph used in
image segmentation is a grid.

For a given input x and a score function φ(·, ·; w), the value φ(x, y; w) measures the compatibility of
the output y for the input x. The essential characteristic of the score function is that it decomposes over the
nodes and edges of the graph as

φ(x, y; w) =

φv(x, yv; w) +

φv,v(cid:48)(x, yv, yv(cid:48); w) .

(13)

(cid:88)

v∈V

(cid:88)

(v,v(cid:48))∈E

For a ﬁxed w, each input x deﬁnes a speciﬁc compatibility function φ(x, · ; w). The nature of the
problem and the optimization algorithms we consider hinge upon whether φ is an afﬁne function of w or
not. The two settings studied here are the following:

Pre-deﬁned Feature Map. In this structured prediction framework, a pre-speciﬁed feature map Φ : X ×

Y → Rd is employed and the score φ is then deﬁned as the linear function

φ(x, y; w) = (cid:104)Φ(x, y), w(cid:105) =

(cid:104)Φv(x, yv), w(cid:105) +

(cid:104)Φv,v(cid:48)(x, yv, yv(cid:48)), w(cid:105) .

(14)

(cid:88)

v∈V

(cid:88)

(v,v(cid:48))∈E

Learning the Feature Map. We also consider the setting where the feature map Φ is parameterized by
w0, for example, using a neural network, and is learned from the data. The score function can then be
written as

φ(x, y; w) = (cid:104)Φ(x, y; w0), w1(cid:105)

(15)

where w = (w0, w1) and the scalar product decomposes into nodes and edges as above.

Note that we only need the decomposition of the score function over nodes and edges of the G as in Eq. (13).
In particular, while Eq. (15) is helpful to understand the use of neural networks in structured prediction, the
optimization algorithms developed in Sec. 6 apply to general nonlinear but smooth score functions.

This framework captures both generative probabilistic models such as Hidden Markov Models (HMMs)
that model the joint distribution between x and y as well as discriminative probabilistic models, such as
conditional random ﬁelds [Lafferty et al., 2001] where dependencies among the input variables x do not
need to be explicitly represented. In these cases, the log joint and conditional probabilities respectively play
the role of the score φ.

9

Example 5 (Sequence Tagging). Consider the task of sequence tagging in natural language processing
where each x = (x1, · · · , xp) ∈ X is a sequence of words and y = (y1, · · · , yp) ∈ Y is a sequence of
labels, both of length p. Common examples include part of speech tagging and named entity recognition.
Each word xv in the sequence x comes from a ﬁnite dictionary D, and each tag yv in y takes values from a
ﬁnite set Yv = Ytag. The corresponding graph is simply a linear chain.

The score function measures the compatibility of a sequence y ∈ Y for the input x ∈ X using parame-

ters w = (wunary, wpair) as, for instance,

φ(x, y; w) =

(cid:104)Φunary(xv, yv), wunary(cid:105) +

(cid:104)Φpair(yv, yv+1), wpair(cid:105) ,

p
(cid:88)

v=1

p
(cid:88)

v=0

where, using wunary ∈ R|D||Ytag| and wpair ∈ R|Ytag|2
each v ∈ [p],

as node and edge weights respectively, we deﬁne for

(cid:104)Φunary(xv, yv), wunary(cid:105) =

wunary, x,j I(x = xv) I(j = yv) .

(cid:88)

x∈D, j∈Ytag

The pairwise term (cid:104)Φpair(yv, yv+1), wpair(cid:105) is analogously deﬁned. Here, y0, yp+1 are special “start” and
“stop” symbols respectively. This can be written as a dot product of w with a pre-speciﬁed feature map as
in (14), by deﬁning

Φ(x, y) = (cid:0)

exv ⊗ eyv

(cid:1) ⊕ (cid:0)

eyv ⊗ eyv+1

(cid:1) ,

p
(cid:88)

v=1

p
(cid:88)

v=0

where exv is the unit vector (I(x = xv))x∈D ∈ R|D|, eyv is the unit vector (I(j = yv))j∈Ytag ∈ R|Ytag|, ⊗
denotes the Kronecker product between vectors and ⊕ denotes vector concatenation.

3.2

Inference Oracles

We deﬁne now inference oracles as ﬁrst order oracles in structured prediction. These are used later to
understand the information-based complexity of optimization algorithms.

3.2.1 First Order Oracles in Structured Prediction

A ﬁrst order oracle for a function f : Rd → R is a routine which, given a point w ∈ Rd, returns on output a
value f (w) and a (sub)gradient v ∈ ∂f (w), where ∂f is the Fr´echet (or regular) subdifferential [Rockafellar
and Wets, 2009, Def. 8.3]. We now deﬁne inference oracles as ﬁrst order oracles for the structural hinge
loss f and its smoothed variants fµω. Note that these deﬁnitions are independent of the graphical structure.
However, as we shall see, the graphical structure plays a crucial role in the implementation of the inference
oracles.

Deﬁnition 6. Consider an augmented score function ψ, a level of smoothing µ > 0 and the structural hinge
loss f (w) = maxy∈Y ψ(y; w). For a given w ∈ Rd,

(i)

the max oracle returns f (w) and v ∈ ∂f (w).

(ii)

the exp oracle returns f−µH (w) and ∇f−µH (w).

(iii) the top-K oracle returns fµ,K(w) and (cid:101)∇fµ,K(w) as surrogates for fµ(cid:96)2

(w) and ∇fµ(cid:96)2

(w) respec-

2

2

tively.

10

(a) Non-smooth.

(b) (cid:96)2

2 smoothing.

(c) Entropy smoothing.

Figure 1: Viterbi trellis for a chain graph with p = 4 nodes and 3 labels.

Note that the exp oracle gets its name since it can be written as an expectation over all y, as revealed by the
next lemma, which gives analytical expressions for the gradients returned by the oracles.

Lemma 7. Consider the setting of Def. 6. We have the following:

(i) For any y∗ ∈ arg maxy∈Y ψ(y; w), we have that ∇wψ(y∗; w) ∈ ∂f (w). That is, the max oracle can

be implemented by inference.

(ii) The output of the exp oracle satisﬁes ∇f−µH (w) = (cid:80)

y∈Y Pψ,µ(y; w)∇ψ(y; w), where

Pψ,µ(y; w) =

exp

(cid:16) 1

(cid:17)
µ ψ(y; w)

(cid:80)

y(cid:48)∈Y exp

µ ψ(y(cid:48); w)

(cid:16) 1

(cid:17) .

(iii) The output of the top-K oracle satisﬁes (cid:101)∇fµ,K(w) = (cid:80)K

i=1 u∗
(cid:9) is the set of K largest scoring outputs satisfying

(cid:8)y(1), · · · , y(K)

ψ,µ,i(w)∇ψ(y(i); w) , where YK =

ψ(y(1); w) ≥ · · · ≥ ψ(y(K); w) ≥ max
y∈Y\YK

ψ(y; w) ,

and u∗

ψ,µ = proj∆K−1

(cid:16)(cid:2)ψ(y(1); w), · · · , ψ(y(K); w)(cid:3)(cid:62)(cid:17)

.

Proof. Part (ii) deals with the composition of differentiable functions, and follows from the chain rule.
Part (iii) follows from the deﬁnition in Eq. (11). The proof of Part (i) follows from the chain rule for Fr´echet
subdifferentials of compositions [Rockafellar and Wets, 2009, Theorem 10.6] together with the fact that
by convexity and Danskin’s theorem [Bertsekas, 1999, Proposition B.25], the subdifferential of the max
function is given by ∂h(z) = conv{ei | i ∈ [m] such that zi = h(z)}.

Example 8. Consider the task of sequence tagging from Example 5. The inference problem (3) is a search
over all |Y| = |Ytag|p label sequences. For chain graphs, this is equivalent to searching for the shortest
path in the associated trellis, shown in Fig. 1. An efﬁcient dynamic programming approach called the
Viterbi algorithm [Viterbi, 1967] can solve this problem in space and time polynomial in p and |Ytag|. The
structural hinge loss is non-smooth because a small change in w might lead to a radical change in the best
scoring path shown in Fig. 1.

11

When smoothing f with ω = (cid:96)2

is given by a projection onto the simplex,
which picks out some number Kψ/µ of the highest scoring outputs y ∈ Y or equivalently, Kψ/µ shortest
paths in the Viterbi trellis (Fig. 1b). The top-K oracle then uses the top-K strategy to approximate fµ(cid:96)2
with
fµ,K.

2, the smoothed function fµ(cid:96)2

2

2

On the other hand, with entropy smoothing ω = −H, we get the log-sum-exp function and its gra-
dient is obtained by averaging over paths with weights such that shorter paths have a larger weight (cf.
Lemma 7(ii)). This is visualized in Fig. 1c.

3.2.2 Exp Oracles and Conditional Random Fields

Recall that a Conditional Random Field (CRF) [Lafferty et al., 2001] with augmented score function ψ and
parameters w ∈ Rd is a probabilistic model that assigns to output y ∈ Y the probability

P(y | ψ; w) = exp (ψ(y; w) − Aψ(w)) ,

(16)

where Aψ(w) is known as the log-partition function, a normalizer so that the probabilities sum to one.
Gradient-based maximum likelihood learning algorithms for CRFs require computation of the log-partition
function Aψ(w) and its gradient ∇Aψ(w). Next proposition relates the computational costs of the exp
oracle and the log-partition function.
Proposition 9. The exp oracle for an augmented score function ψ with parameters w ∈ Rd is equivalent
in hardness to computing the log-partition function Aψ(w) and its gradient ∇Aψ(w) for a conditional
random ﬁeld with augmented score function ψ.

Proof. Fix a smoothing parameter µ > 0. Consider a CRF with augmented score function ψ(cid:48)(y; w) =
y∈Y exp (cid:0)µ−1ψ(y; w)(cid:1). The
µ−1ψ(y; w). Its log-partition function Aψ(cid:48)(w) satisﬁes exp(Aψ(cid:48)(w)) = (cid:80)
claim now follows from the bijection f−µH (w) = µ Aψ(cid:48)(w) between f−µH and Aψ(cid:48).

4 Implementation of Inference Oracles

We now turn to the concrete implementation of the inference oracles. This depends crucially on the structure
of the graph G = (V, E). If the graph G is a tree, then the inference oracles can be computed exactly with
efﬁcient procedures, as we shall see in in the Sec. 4.1. When the graph G is not a tree, we study special cases
when speciﬁc discrete structure can be exploited to efﬁciently implement some of the inference oracles in
Sec. 4.2. The results of this section are summarized in Table 2.

Throughout this section, we ﬁx an input-output pair (x(i), y(i)) and consider the augmented score func-
tion ψ(y; w) = φ(x(i), y; w) + (cid:96)(y(i), y) − φ(x(i), y(i); w) it deﬁnes, where the index of the sample is
dropped by convenience. From (13) and the decomposability of the loss, we get that ψ decomposes along
nodes V and edges E of G as:

ψ(y; w) =

ψv(yv; w) +

ψv,v(cid:48)(yv, yv(cid:48); w) .

(17)

(cid:88)

v∈V

(cid:88)

(v,v(cid:48))∈E

When w is clear from the context, we denote ψ(· ; w) by ψ. Likewise for ψv and ψv,v(cid:48).

4.1

Inference Oracles in Trees

We ﬁrst consider algorithms implementing the inference algorithms in trees and examine their computational
complexity.

12

Table 2: Smooth inference oracles, algorithms and complexity. Here, p is the size of each y ∈ Y. The time
complexity is phrased in terms of the time complexity T of the max oracle.

Max oracle
Algo

Max-product

Graph cut

Graph matching

Branch and
Bound search

Algo
Top-K
max-product

BMMF

BMMF

Top-K oracle

Time

Exp oracle
Algo

Time

O(KT log K)

Sum-Product

O(T )

O(pKT )

O(KT )

Intractable

Intractable

Intractable

Top-K search

N/A

4.1.1

Implementation of Inference Oracles

Max Oracle
In tree structured graphical models, the inference problem (3), and thus the max oracle (cf.
Lemma 7(i)) can always be solved exactly in polynomial time by the max-product algorithm [Pearl, 1988],
which uses the technique of dynamic programming [Bellman, 1957]. The Viterbi algorithm (Algo. 1) for
chain graphs from Example 8 is a special case. See Algo. 7 in Appendix B for the max-product algorithm
in full generality.

Top-K Oracle The top-K oracle uses a generalization of the max-product algorithm that we name top-K
max-product algorithm. Following the work of Seroussi and Golmard [1994], it keeps track of the K-best
intermediate structures while the max-product algorithm just tracks the single best intermediate structure.
Formally, the kth largest element from a discrete set S is deﬁned as

(cid:40)

max(k)
x∈S

f (x) =

kth largest element of {f (y) | y ∈ S} k ≤ |S|
k > |S| .
−∞,

We present the algorithm in the simple case of chain structured graphical models in Algo. 2. The top-K
max-product algorithm for general trees is given in Algo. 8 in Appendix B. Note that it requires (cid:101)O(K) times
the time and space of the max oracle.

Exp oracle The relationship of the exp oracle with CRFs (Prop. 9) leads directly to Algo. 3, which is
based on marginal computations from the sum-product algorithm.

Remark 10. We note that clique trees allow the generalization of the algorithms of this section to general
graphs with cycles. However, the construction of a clique tree requires time and space exponential in the
treewidth of the graph.

Example 11. Consider the task of sequence tagging from Example 5. The Viterbi algorithm (Algo. 1)
maintains a table πv(yv), which stores the best length-v preﬁx ending in label yv. One the other hand, the
top-K Viterbi algorithm (Algo. 2) must store in π(k)
v (yv) the score of kth best length-v preﬁx that ends in
yv for each k ∈ [K]. In the vanilla Viterbi algorithm, the entry πv(yv) is updated by looking the previous
column πv−1 following (18). Compare this to update (19) of the top-K Viterbi algorithm. In this case,
the exp oracle is implemented by the forward-backward algorithm, a specialization of the sum-product
algorithm to chain graphs.

13

Algorithm 1 Max-product (Viterbi) algorithm for chain graphs

1: Input: Augmented score function ψ(·, ·; w) deﬁned on a chain graph G.
2: Set π1(y1) ← ψ1(y1) for all y1 ∈ Y1.
3: for v = 2, · · · p do
4:

For all yv ∈ Yv, set

πv(yv) ← ψv(yv) + max

{πv−1(yv−1) + ψv,v−1(yv, yv−1)} .

(18)

yv−1∈Yv−1

Assign to δv(yv) the yv−1 that attains the max above for each yv ∈ Yv.

5:
6: end for
7: Set ψ∗ ← maxyp∈Yp πp(yp) and store the maximizing assignments of yp in y∗
p.
8: for v = p − 1, · · · , 1 do
Set y∗
v ← δv+1(yv+1).
9:
10: end for
11: return ψ∗, y∗ := (y∗

1, · · · , y∗

p).

4.1.2 Complexity of Inference Oracles

The next proposition presents the correctness guarantee and complexity of each of the aforementioned algo-
rithms. Its proof has been placed in Appendix B.

Proposition 12. Consider as inputs an augmented score function ψ(·, ·; w) deﬁned on a tree structured
graph G, an integer K > 0 and a smoothing parameter µ > 0.

(i) The output (ψ∗, y∗) of the max-product algorithm (Algo. 1 for the special case when G is chain struc-
tured Algo. 7 from Appendix B in general) satisﬁes ψ∗ = ψ(y∗; w) = maxy∈Y ψ(y; w). Thus, the pair
(cid:0)ψ∗, ∇ψ(y∗; w)(cid:1) is a correct implementation of the max oracle. It requires time O(p maxv∈V |Yv|2)
and space O(p maxv∈V |Yv|).

(ii) The output {ψ(k), y(k)}K

k=1 of the top-K max-product algorithm (Algo. 2 for the special case when G
is chain structured or Algo. 8 from Appendix B in general) satisﬁes ψ(k) = ψ(y(k)) = max(k)
y∈Y ψ(y).
Thus, the top-K max-product algorithm followed by a projection onto the simplex (Algo. 6 in Ap-
pendix A) is a correct implementation of the top-K oracle. It requires time O(pK log K maxv∈V |Yv|2)
and space O(pK maxv∈V |Yv|).

(iii) Algo. 3 returns (cid:0)f−µH (w), ∇f−µH (w)(cid:1). Thus, Algo. 3 is a correct implementation of the exp oracle.

It requires time O(p maxv∈V |Yv|2) and space O(p maxv∈V |Yv|).

4.2

Inference Oracles in Loopy Graphs

For general loopy graphs with high tree-width, the inference problem (3) is NP-hard [Cooper, 1990]. In
particular cases, graph cut, matching or search algorithms can be used for exact inference in dense loopy
graphs, and therefore, to implement the max oracle as well (cf. Lemma 7(i)). In each of these cases, we ﬁnd
that the top-K oracle can be implemented, but the exp oracle is intractable. Appendix C contains a review
of the algorithms and guarantees referenced in this section.

14

Algorithm 2 Top-K max-product (top-K Viterbi) algorithm for chain graphs

1: Input: Augmented score function ψ(·, ·; w) deﬁned on chain graph G, integer K > 0.
2: For k = 1, · · · , K, set π(k)
1 (y1) ← ψ1(y1) if k = 1 and −∞ otherwise for all y1 ∈ Y1.
3: for v = 2, · · · p and k = 1, · · · , K do
4:

For all yv ∈ Yv, set

v (yv) ← ψv(yv) + max(k)
π(k)

yv−1∈Yv−1,(cid:96)∈[K]

(cid:110)

π((cid:96))
v−1(yv−1) + ψv,v−1(yv, yv−1)

(cid:111)

.

(19)

v (yv) the yv−1, (cid:96) that attain the max(k) above for each yv ∈ Yv.

yp∈Yp,k∈[K] π(k)

p (yp) and store in y(k)

p , (cid:96)(k) respectively the maxi-

Assign to δ(k)

v (yv), κ(k)

5:
6: end for
7: For k = 1, · · · , K, set ψ(k) ← max(k)

mizing assignments of yp, k.

8: for v = p − 1, · · · 1 and k = 1, · · · , K do

Set y(k)

v ← δ((cid:96)(k))

v+1

(cid:0)y(k)
v+1

(cid:1) and (cid:96)(k) ← κ((cid:96)(k))
v+1

(cid:0)y(k)
v+1

(cid:1).

9:
10: end for
11: return

(cid:110)

ψ(k), y(k) := (y(k)

1 , · · · , y(k)
p )

(cid:111)K

.

k=1

4.2.1

Inference Oracles using Max-Marginals

We now deﬁne a max-marginal, which is a constrained maximum of the augmented score ψ.

Deﬁnition 13. The max-marginal of ψ relative to a variable yv is deﬁned, for j ∈ Yv as

ψv;j(w) := max

ψ(y; w) .

y∈Y : yv=j

In cases where exact inference is tractable using graph cut or matching algorithms, it is possible to extract
max-marginals as well. This, as we shall see next, allows the implementation of the max and top-K oracles.
When the augmented score function ψ is unambiguous, i.e., no two distinct y1, y2 ∈ Y have the same
augmented score, the output y∗(w) is unique can be decoded from the max-marginals as (see Pearl [1988],
Dawid [1992] or Thm. 45 in Appendix C)

(20)

(21)

y∗
v(w) = arg max

ψv;j(w) .

j∈Yv

If one has access to an algorithm M that can compute max-marginals, the top-K oracle is also eas-
ily implemented via the Best Max-Marginal First (BMMF) algorithm of Yanover and Weiss [2004]. This
algorithm requires computations of 2K sets of max-marginals, where a set of max-marginals refers to max-
marginals for all yv in y. Therefore, the BMMF algorithm followed by a projection onto the simplex
(Algo. 6 in Appendix A) is a correct implementation of the top-K oracle at a computational cost of 2K sets
of max-marginals. The BMMF algorithm and its guarantee are recalled in Appendix C.1 for completeness.

Graph Cut and Matching Inference Kolmogorov and Zabin [2004] showed that submodular energy
functions [Lov´asz, 1983] over binary variables can be efﬁciently minimized exactly via a minimum cut
algorithm. For a class of alignment problems, e.g., Taskar et al. [2005], inference amounts to ﬁnding the
best bipartite matching. In both these cases, max-marginals can be computed exactly and efﬁciently by

15

Algorithm 3 Entropy smoothed max-product algorithm

1: Input: Augmented score function ψ(·, ·; w) deﬁned on tree structured graph G, µ > 0.
2: Compute the log-partition function and marginals using the sum-product algorithm (Algo. 9 in Ap-

pendix B)

Aψ/µ, {Pv for v ∈ V}, {Pv,v(cid:48) for (v, v(cid:48)) ∈ E} ← SUMPRODUCT

(cid:16) 1

µ ψ(· ; w), G

(cid:17)

.

3: Set f−µH (w) ← µAψ/µ and

∇f−µH (w) ←

Pv(yv)∇ψv(yv; w) +

Pv,v(cid:48)(yv, yv(cid:48))∇ψv,v(cid:48)(yv; w) .

(cid:88)

(cid:88)

v∈V

yv∈Yv

(cid:88)

(cid:88)

(cid:88)

(v,v(cid:48))∈E

yv∈Yv

yv(cid:48) ∈Yv(cid:48)

4: return f−µH (w), ∇f−µH (w).

combinatorial algorithms. This gives us a way to implement the max and top-K oracles. However, in
both settings, computing the log-partition function Aψ(w) of a CRF with score ψ is known to be #P-
complete [Jerrum and Sinclair, 1993]. Prop. 9 immediately extends this result to the exp oracle. This
discussion is summarized by the following proposition, whose proof is provided in Appendix C.4.

Proposition 14. Consider as inputs an augmented score function ψ(·, ·; w), an integer K > 0 and a smooth-
ing parameter µ > 0. Further, suppose that ψ is unambiguous, that is, ψ(y(cid:48); w) (cid:54)= ψ(y(cid:48)(cid:48); w) for all distinct
y(cid:48), y(cid:48)(cid:48) ∈ Y. Consider one of the two settings:

(A)

the output space Yv = {0, 1} for each v ∈ V, and the function −ψ is submodular (see Appendix C.2
and, in particular, (72) for the precise deﬁnition), or,

(B)

the augmented score corresponds to an alignment task where the inference problem (3) corresponds to
a maximum weight bipartite matching (see Appendix C.3 for a precise deﬁnition).

In these cases, we have the following:

(i) The max oracle can be implemented at a computational complexity of O(p) minimum cut computations

in Case (A), and in time O(p3) in Case (B).

(ii) The top-K oracle can be implemented at a computational complexity of O(pK) minimum cut compu-

tations in Case (A), and in time O(p3K) in Case (B).

(iii) The exp oracle is #P-complete in both cases.

Prop. 14 is loose in that the max oracle can be implemented with just one minimum cut computation

instead of p in in Case (A) [Kolmogorov and Zabin, 2004].

4.2.2 Branch and Bound Search

Max oracles implemented via search algorithms can often be extended to implement the top-K oracle. We
restrict our attention to best-ﬁrst branch and bound search such as the celebrated Efﬁcient Subwindow Search
[Lampert et al., 2008].

16

Branch and bound methods partition the search space into disjoint subsets, while keeping an upper
bound (cid:98)ψ : X × 2Y → R, on the maximal augmented score for each of the subsets (cid:98)Y ⊆ Y. Using a best-ﬁrst
strategy, promising parts of the search space are explored ﬁrst. Parts of the search space whose upper bound
indicates that they cannot contain the maximum do not have to be examined further.

The top-K oracle is implemented by simply continuing the search procedure until K outputs have been
produced - see Algo. 13 in Appendix C.5. Both the max oracle and the top-K oracle can degenerate to an
exhaustive search in the worst case, so we do not have sharp running time guarantees. However, we have
the following correctness guarantee.

Proposition 15. Consider an augmented score function ψ(·, ·; w), an integer K > 0 and a smoothing
parameter µ > 0. Suppose the upper bound function (cid:98)ψ(·, ·; w) : X × 2Y → R satisﬁes the following
properties:

(a) (cid:98)ψ( (cid:98)Y; w) is ﬁnite for every (cid:98)Y ⊆ Y,

(b) (cid:98)ψ( (cid:98)Y; w) ≥ maxy∈ (cid:98)Y ψ(y; w) for all (cid:98)Y ⊆ Y, and,

(c) (cid:98)ψ({y}; w) = ψ(y; w) for every y ∈ Y.

Then, we have the following:

(i) Algo. 13 with K = 1 is a correct implementation of the max oracle.

(ii) Algo. 13 followed by a projection onto the simplex (Algo. 6 in Appendix A) is a correct implementation

of the top-K oracle.

See Appendix C.5 for a proof. The discrete structure that allows inference via branch and bound search
cannot be leveraged to implement the exp oracle.

5 The Casimir Algorithm

We come back to the optimization problem (1) with f (i) deﬁned in (7). We assume in this section that the
mappings g(i) deﬁned in (6) are afﬁne. Problem (1) now reads

(cid:34)

min
w∈Rd

F (w) :=

1
n

n
(cid:88)

i=1

h(A(i)w + b(i)) +

(cid:107)w(cid:107)2
2

.

(cid:35)

λ
2

For a single input (n = 1), the problem reads

min
w∈Rd

h(Aw + b) +

λ
2

(cid:107)w(cid:107)2
2.

(22)

(23)

where h is a simple non-smooth convex function and λ ≥ 0. Nesterov [2005b,a] ﬁrst analyzed such set-
ting: while the problem suffers from its non-smoothness, fast methods can be developed by considering
smooth approximations of the objectives. We combine this idea with the Catalyst acceleration scheme [Lin
et al., 2018] to accelerate a linearly convergent smooth optimization algorithm resulting in a scheme called
Casimir.

17

5.1 Casimir: Catalyst with Smoothing

The Catalyst [Lin et al., 2018] approach minimizes regularized objectives centered around the current it-
erate. The algorithm proceeds by computing approximate proximal point steps instead of the classical
(sub)-gradient steps. A proximal point step from a point w with step-size κ−1 is deﬁned as the minimizer
of

min
z∈Rm

F (z) +

κ
2

(cid:107)z − w(cid:107)2
2,

(24)

which can also be seen as a gradient step on the Moreau envelope of F - see Lin et al. [2018] for a detailed
discussion. While solving the subproblem (24) might be as hard as the original problem we only require
an approximate solution returned by a given optimization method M. The Catalyst approach is then an
inexact accelerated proximal point algorithm that carefully mixes approximate proximal point steps with
the extrapolation scheme of Nesterov [1983]. The Casimir scheme extends this approach to non-smooth
optimization.

For the overall method to be efﬁcient, subproblems (24) must have a low complexity. That is, there must
exist an optimization algorithm M that solves them linearly. For the Casimir approach to be able to handle
non-smooth objectives, it means that we need not only to regularize the objective but also to smooth it. To
this end we deﬁne

Fµω(w) :=

hµω(A(i)w + b(i)) +

(cid:107)w(cid:107)2
2

λ
2

1
n

n
(cid:88)

i=1

as a smooth approximation of the objective F , and,

Fµω,κ(w; z) :=

hµω(A(i)w + b(i)) +

(cid:107)w(cid:107)2

2 +

(cid:107)w − z(cid:107)2
2

λ
2

κ
2

1
n

n
(cid:88)

i=1

a smooth and regularized approximation of the objective centered around a given point z ∈ Rd. While the
original Catalyst algorithm considered a ﬁxed regularization term κ, we vary κ and µ along the iterations.
This enables us to get adaptive smoothing strategies.

The overall method is presented in Algo. 4. We ﬁrst analyze in Sec. 5.2 its complexity for a generic lin-
early convergent algorithm M. Thereafter, in Sec. 5.3, we compute the total complexity with SVRG [John-
son and Zhang, 2013] as M. Before that, we specify two practical aspects of the implementation: a proper
stopping criterion (26) and a good initialization of subproblems (Line 4).

Stopping Criterion Following Lin et al. [2018], we solve subproblem k in Line 4 to a degree of relative ac-
curacy speciﬁed by δk ∈ [0, 1). In view of the (λ + κk)-strong convexity of Fµkω,κk (· ; zk−1), the functional
gap can be controlled by the norm of the gradient, precisely it can be seen that (cid:107)∇Fµkω,κk ( (cid:98)w; zk−1)(cid:107)2
2 ≤
(λ + κk)δkκk(cid:107) (cid:98)w − zk−1(cid:107)2

2 is a sufﬁcient condition for the stopping criterion (26).
A practical alternate stopping criterion proposed by Lin et al. [2018] is to ﬁx an iteration budget Tbudget
and run the inner solver M for exactly Tbudget steps. We do not have a theoretical analysis for this scheme
but ﬁnd that it works well in experiments.

Warm Start of Subproblems Rate of convergence of ﬁrst order optimization algorithms depends on the
initialization and we must warm start M at an appropriate initial point in order to obtain the best convergence
of subproblem (25) in Line 4 of Algo. 4. We advocate the use of the prox center zk−1 in iteration k as the
warm start strategy. We also experiment with other warm start strategies in Section 7.

18

Algorithm 4 The Casimir algorithm

1: Input: Smoothable objective F of the form (23) with h simple, smoothing function ω, linearly con-
vergent algorithm M, non-negative and non-increasing sequence of smoothing parameters (µk)k≥1,
positive and non-decreasing sequence of regularization parameters (κk)k≥1, non-negative sequence of
relative target accuracies (δk)k≥1 and, initial point w0, α0 ∈ (0, 1), time horizon K.

Using M with zk−1 as the starting point, ﬁnd wk ≈ arg minw∈Rd Fµkω,κk (w; zk−1) where

Fµkω,κk (w; zk−1) :=

hµkω(A(i)w + b(i)) +

(cid:107)w(cid:107)2

2 +

(cid:107)w − zk−1(cid:107)2
2

(25)

λ
2

κk
2

1
n

n
(cid:88)

i=1

Fµkω,κk (wk; zk−1) − min
w

Fµkω,κk (w; zk−1) ≤ δkκk

2 (cid:107)wk − zk−1(cid:107)2

2

k(κk+1 + λ) = (1 − αk)α2
α2

k−1(κk + λ) + αkλ.

zk = wk + βk(wk − wk−1),

βk =

αk−1(1 − αk−1)(κk + λ)
k−1(κk + λ) + αk(κk+1 + λ)

α2

.

(26)

(27)

(28)

(29)

2: Initialize: z0 = w0.
3: for k = 1 to K do
4:

such that

5:

Solve for αk ≥ 0

6:

Set

where

7: end for
8: return wK.

5.2 Convergence Analysis of Casimir

We ﬁrst state the outer loop complexity results of Algo. 4 for any generic linearly convergent algorithm M in
Sec. 5.2.1, prove it in Sec. 5.2.2. Then, we consider the complexity of each inner optimization problem (25)
in Sec. 5.2.3 based on properties of M.

5.2.1 Outer Loop Complexity Results

The following theorem states the convergence of the algorithm for general choice of parameters, where we
denote w∗ ∈ arg minw∈Rd F (w) and F ∗ = F (w∗).

Theorem 16. Consider Problem (22). Suppose δk ∈ [0, 1) for all k ≥ 1, the sequence (µk)k≥1 is non-
negative and non-increasing, and the sequence (κk)k≥1 is strictly positive and non-decreasing. Further,
suppose the smoothing function ω : dom h∗ → R satisﬁes −Dω ≤ ω(u) ≤ 0 for all u ∈ dom h∗ and that

19

0 ≥ λ/(λ + κ1). Then, the sequence (αk)k≥0 generated by Algo. 4 satisﬁes 0 < αk ≤ αk−1 < 1 for all

α2
k ≥ 1. Furthermore, the sequence (wk)k≥0 of iterates generated by Algo. 4 satisﬁes

F (wk) − F ∗ ≤

∆0 + µkDω +

(µj−1 − (1 − δj)µj) Dω ,

(30)

Ak−1
0
Bk
1

k
(cid:88)

j=1

Ak−1
j
Bk
j

where Aj
µ0 := 2µ1.

i := (cid:81)j

r=i(1 − αr), Bj

i := (cid:81)j

r=i(1 − δr), ∆0 := F (w0) − F ∗ + (κ1+λ)α2

0−λα0

2(1−α0)

(cid:107)w0 − w∗(cid:107)2

2 and

Before giving its proof, we present various parameters strategies as corollaries. Table 3 summarizes the
parameter settings and the rates obtained for each setting. Overall, the target accuracies δk are chosen such
that Bk
j is a constant and the parameters µk and κk are then carefully chosen for an almost parameter-free
algorithm with the right rate of convergence. Proofs of these corollaries are provided in Appendix D.2.

The ﬁrst corollary considers the strongly convex case (λ > 0) with constant smoothing µk = µ, as-
suming that (cid:15) is known a priori. We note that this is, up to constants, the same complexity obtained by the
original Catalyst scheme on a ﬁxed smooth approximation Fµω with µ = O((cid:15)Dω).

Corollary 17. Consider the setting of Thm. 16. Let q = λ/(λ + κ). Suppose λ > 0 and µk = µ, κk = κ,
√
for all k ≥ 1. Choose α0 =

q) . Then, we have,

q and, δk =

q/(2 −

√

√

F (wk) − F ∗ ≤

µDω + 2

1 −

(F (w0) − F ∗) .

√
√

q
q

3 −
1 −

(cid:18)

(cid:19)k

√

q
2

Next, we consider the strongly convex case where the target accuracy (cid:15) is not known in advance. We
let smoothing parameters (µk)k≥0 decrease over time to obtain an adaptive smoothing scheme that gives
progressively better surrogates of the original objective.

Corollary 18. Consider the setting of Thm. 16. Let q = λ/(λ + κ) and η = 1 −
κk = κ, for all k ≥ 1. Choose α0 =

q and, the sequences (µk)k≥1 and (δk)k≥1 as

√

√

q/2. Suppose λ > 0 and

µk = µηk/2 ,

and,

δk =

√

q
√

,

q

2 −

where µ > 0 is any constant. Then, we have,

F (wk) − F ∗ ≤ ηk/2

(cid:20)
2 (F (w0) − F ∗) +

µDω
√
q
1 −

(cid:18)

√

2 −

q +

√

(cid:19)(cid:21)

.

q
√
η

1 −

The next two corollaries consider the unregularized problem, i.e., λ = 0 with constant and adaptive smooth-
ing respectively.

Corollary 19. Consider the setting of Thm. 16. Suppose µk = µ, κk = κ, for all k ≥ 1 and λ = 0. Choose
α0 = (

5 − 1)/2 and δk = (k + 1)−2 . Then, we have,

√

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2
2

+ µDω

1 +

(cid:16)

8
(k + 2)2

(cid:17)

(cid:18)

12
k + 2

+

30
(k + 2)2

(cid:19)

.

κ
2

20

Table 3: Summary of outer iteration complexity for Algorithm 4 for different parameter settings. We use shorthand
∆F0 := F (w0) − F ∗ and ∆0 = (cid:107)w0 − w∗(cid:107)2. Absolute constants are omitted from the rates.

Cor.

λ > 0

κk

17

18

19

20

Yes

Yes

No

No

κ

κ

κ

µk

µ
√

q
2

µ

(cid:16)

µ

1 −

(cid:17)k/2

κ k

µ/k

δk
√
q
√

q

2−
√

q
√

2−
q
k−2

k−2

α0
√

√

q

q

c

c

F (wk) − F ∗
√

(cid:16)

1 −
√

q
2

(cid:16)

1 −

(cid:17)k

q
2
(cid:17)k/2 (cid:16)

∆F0 + µD
√
1−

q
∆F0 + µD
√
1−
(cid:1) + µD

(cid:17)

q

(cid:0)∆F0 + κ∆2
1
0
k2
log k
k (∆F0 + κ∆2
0 + µD)

Remark

q = λ
λ+κ

q = λ
λ+κ
√
5 − 1)/2

c = (

√

c = (

5 − 1)/2

Corollary 20. Consider the setting of Thm. 16 with λ = 0. Choose α0 = (
non-negative constants κ, µ, deﬁne sequences (κk)k≥1, (µk)k≥1, (δk)k≥1 as

√

5 − 1)/2, and for some

κk = κ k , µk =

and,

δk =

µ
k

1
(k + 1)2 .

Then, for k ≥ 2, we have,

F (wk) − F ∗ ≤

log(k + 1)
k + 1

(cid:0)2(F (w0) − F ∗) + κ(cid:107)w0 − w∗(cid:107)2

2 + 27µDω

(cid:1) .

For the ﬁrst iteration (i.e., k = 1), this bound is off by a constant factor 1/ log 2.

5.2.2 Outer Loop Convergence Analysis

We now prove Thm. 16. The proof technique largely follows that of Lin et al. [2018], with the added
challenges of accounting for smoothing and varying Moreau-Yosida regularization. We ﬁrst analyze the
sequence (αk)k≥0. The proof follows from the algebra of Eq. (27) and has been given in Appendix D.1.

Lemma 21. Given a positive, non-decreasing sequence (κk)k≥1 and λ ≥ 0, consider the sequence (αk)k≥0
deﬁned by (27), where α0 ∈ (0, 1) such that α2
0 ≥ λ/(λ + κ1). Then, we have for every k ≥ 1 that
0 < αk ≤ αk−1 and, α2

k ≥ λ/(λ + κk+1) .

We now characterize the effect of an approximate proximal point step on Fµω.
Lemma 22. Suppose (cid:98)w ∈ Rd satisﬁes Fµω,κ( (cid:98)w; z) − minw∈Rd Fµω,κ(w; z) ≤ (cid:98)(cid:15) for some (cid:98)(cid:15) > 0. Then, for
all 0 < θ < 1 and all w ∈ Rd, we have,

Fµω( (cid:98)w) +

(cid:107) (cid:98)w − z(cid:107)2

2 +

(1 − θ)(cid:107)w − (cid:98)w(cid:107)2

2 ≤ Fµω(w) +

(cid:107)w − z(cid:107)2

κ
2

2 + (cid:98)(cid:15)
θ

.

(31)

κ + λ
2

κ
2

Proof. Let (cid:98)F ∗ = minw∈Rd Fµω,κ(w; z). Let (cid:98)w∗ be the unique minimizer of Fµω,κ(· ; z). We have, from
(κ + λ)-strong convexity of Fµω,κ(· ; z),

Fµω,κ(w; z) ≥ (cid:98)F ∗ +

κ + λ
2

(cid:107)w − (cid:98)w∗(cid:107)2

2

≥ (Fµω,κ( (cid:98)w; z) − (cid:98)(cid:15)) +

(1 − θ)(cid:107)w − (cid:98)w(cid:107)2

2 −

κ + λ
2

κ + λ
2

(cid:18) 1
θ

(cid:19)

− 1

(cid:107) (cid:98)w − (cid:98)w∗(cid:107)2
2 ,

21

where we used that (cid:98)(cid:15) was sub-optimality of (cid:98)w and Lemma 51 from Appendix D.7. From (κ + λ)-strong
convexity of Fµω,κ(·; z), we have,

κ + λ
2

(cid:107) (cid:98)w − (cid:98)w∗(cid:107)2

2 ≤ Fµω,κ( (cid:98)w; z) − (cid:98)F ∗ ≤ (cid:98)(cid:15) ,

Since (1/θ − 1) is non-negative, we can plug this into the previous statement to get,

Fµω,κ(w; z) ≥ Fµω,κ( (cid:98)w; z) +

κ + λ
2

(1 − θ)(cid:107)w − (cid:98)w(cid:107)2

2 − (cid:98)(cid:15)
θ

.

Substituting the deﬁnition of Fµω,κ(· ; z) from (25) completes the proof.

We now deﬁne a few auxiliary sequences integral to the proof. Deﬁne sequences (vk)k≥0, (γk)k≥0,

(ηk)k≥0, and (rk)k≥1 as

One might recognize γk and vk from their resemblance to counterparts from the proof of Nesterov [2013].
Now, we claim some properties of these sequences.

Claim 23. For the sequences deﬁned in (32)-(37), we have,

v0 = w0

vk = wk−1 +

(wk − wk−1) , k ≥ 1 ,

1
αk−1

γ0 =

(κ1 + λ)α2
1 − α0
γk = (κk + λ)α2

ηk =

αkγk
γk+1 + αkγk

0 − λα0

,

k−1 , k ≥ 1 ,

, k ≥ 0 ,

rk = αk−1w∗ + (1 − αk−1)wk−1 , k ≥ 1 .

γk =

(κk+1 + λ)α2
1 − αk
γk+1 = (1 − αk)γk + λαk , k ≥ 0 ,

k − λαk

, k ≥ 0 ,

ηk =

αkγk
γk + αkλ

, k ≥ 0

zk = ηkvk + (1 − ηk)wk , k ≥ 0 , .

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

Proof. Eq. (38) follows from plugging in (27) in (35) for k ≥ 1, while for k = 0, it is true by deﬁnition.
Eq. (39) follows from plugging (35) in (38). Eq. (40) follows from (39) and (36). Lastly, to show (41), we
shall show instead that (41) is equivalent to the update (28) for zk. We have,

zk = ηkvk + (1 − ηk)wk

(cid:18)

(33)
= ηk

(cid:19)

(wk − wk−1)

+ (1 − ηk)wk

= wk + ηk

− 1

(wk − wk−1) .

1
αk−1

wk−1 +
(cid:18) 1

αk−1

(cid:19)

22

Now,

ηk

(cid:18) 1

αk−1

(cid:19) (36)
=

− 1

·

αkγk
γk+1 + αkγk

1 − αk−1
αk−1
αk(κk + λ)α2
k(κk+1 + λ) + αk(κk + λ)α2
α2

k−1

k−1

(35)
=

·

1 − αk−1
αk−1

(29)
= βk ,

completing the proof.

Claim 24. The sequence (rk)k≥1 from (37) satisﬁes

(cid:107)rk − zk−1(cid:107)2

2 ≤ αk−1(αk−1 − ηk−1)(cid:107)wk−1 − w∗(cid:107)2

2 + αk−1ηk−1(cid:107)vk−1 − w∗(cid:107)2
2 .

(42)

Proof. Notice that ηk
get,

(40)
= αk ·

γk

γk+αkλ ≤ αk. Hence, using convexity of the squared Euclidean norm, we

(cid:107)rk − zk−1(cid:107)2
2

(41)
= (cid:107)(αk−1 − ηk−1)(w∗ − wk−1) + ηk−1(w∗ − vk−1)(cid:107)2
2

= α2

k−1

1 −

(w∗ − wk−1) +

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:18)

(cid:19)

ηk−1
αk−1
(cid:19)
ηk−1
αk−1

(∗)
≤ α2

k−1

1 −

(cid:107)wk−1 − w∗(cid:107)2

2 + α2

k−1

(cid:107)vk−1 − w∗(cid:107)2
2

ηk−1
αk−1

ηk−1
αk−1

(cid:13)
2
(cid:13)
(w∗ − vk−1)
(cid:13)
(cid:13)
2

= αk−1(αk−1 − ηk−1)(cid:107)wk−1 − w∗(cid:107)2

2 + αk−1ηk−1(cid:107)vk−1 − w∗(cid:107)2
2 .

For all µ ≥ µ(cid:48) ≥ 0, we know from Prop. 2 that

0 ≤ Fµω(w) − Fµ(cid:48)ω(w) ≤ (µ − µ(cid:48))Dω .

We now deﬁne the sequence (Sk)k≥0 to play the role of a potential function here.

S0 = (1 − α0)(F (w0) − F (w∗)) +

α0κ1η0
2
Sk = (1 − αk)(Fµkω(wk) − Fµkω(w∗)) +

(cid:107)w0 − w∗(cid:107)2
2 ,
αkκk+1ηk
2

(cid:107)vk − w∗(cid:107)2

2 , k ≥ 1 .

(43)

(44)

We are now ready to analyze the effect of one outer loop. This lemma is the crux of the analysis.

Lemma 25. Suppose Fµkω,κk (wk; z) − minw∈Rd Fµkω,κk (w; z) ≤ (cid:15)k for some (cid:15)k > 0. The following
statement holds for all 0 < θk < 1:

Sk
1 − αk

≤ Sk−1 + (µk−1 − µk)Dω +

(cid:107)wk − zk−1(cid:107)2

2 +

(cid:107)vk − w∗(cid:107)2
2 ,

(45)

(cid:15)k
θk

−

κk
2

κk+1ηkαkθk
2(1 − αk)

where we set µ0 := 2µ1.

Proof. For ease of notation, let Fk := Fµkω, and D := Dω. By λ-strong convexity of Fµkω, we have,

Fk(rk) ≤ αk−1Fk(w∗) + (1 − αk−1)Fk(wk−1) −

λαk−1(1 − αk−1)
2

(cid:107)wk−1 − w∗(cid:107)2
2 .

(46)

23

We now invoke Lemma 22 on the function Fµkω,κk (·; zk−1) with (cid:98)(cid:15) = (cid:15)k and w = rk to get,

Fk(wk) +

(cid:107)wk − zk−1(cid:107)2

2 +

(1 − θk)(cid:107)rk − wk(cid:107)2

2 ≤ Fk(rk) +

(cid:107)rk − zk−1(cid:107)2

2 +

.

(47)

κk
2

κk + λ
2

κk
2

(cid:15)k
θk

We shall separately manipulate the left and right hand sides of (47), starting with the right hand side, which
we call R. We have, using (46) and (42),

R ≤ (1 − αk−1)Fk(wk−1) + αk−1Fk(w∗) −

+

κk
2

αk−1(αk−1 − ηk−1)(cid:107)wk−1 − w∗(cid:107)2

λαk−1(1 − αk−1)
2
κkαk−1ηk−1
2

2 +

(cid:107)wk−1 − w∗(cid:107)2
2

(cid:107)vk−1 − w∗(cid:107)2

2 +

(cid:15)k
θk

.

We notice now that

αk−1 − ηk−1

(40)
= αk−1 −

αk−1γk−1
γk + αk−1γk−1
(cid:18) γk − γk−1(1 − αk−1)
γk + αk−1γk−1

(cid:19)

= αk−1

(39)
=

(38)
=

=

α2

k−1λ
γk−1 + αk−1λ

(κk + λ)α2
λ
κk

(1 − αk−1) ,

α2

k−1λ(1 − αk−1)

k−1 − λαk−1 + (1 − αk−1)αk−1λ

and hence the terms containing (cid:107)wk−1 − w∗(cid:107)2

2 cancel out. Therefore, we get,

R ≤ (1 − αk−1)Fk(wk−1) + αk−1Fk(w∗) +

κkαk−1ηk−1
2

(cid:107)vk−1 − w∗(cid:107)2

2 +

(cid:15)k
θk

.

To move on to the left hand side, we note that

αkηk

(40)
=

α2
kγk
γk + αkλ

(35),(38)
=

kα2
α2
(κk+1+λ)α2
1−αk

k−1(κk + λ)
k−λαk

+ αkλ

=

(1 − αk)(κk + λ)α2
(κk+1 + λ)α2

k − λα2
k

k−1α2
k

= (1 − αk)α2

k−1

κk + λ
κk+1

.

Therefore,

Fk(wk) − Fk(w∗) +

κk + λ
2

k−1(cid:107)vk − w∗(cid:107)2
α2
2

(44),(50)
=

Sk
1 − αk

.

Using rk − wk

(33)
= αk−1(w∗ − vk), we simplify the left hand side of (47), which we call L, as

L = Fk(wk) − Fk(w∗) +

(cid:107)wk − zk−1(cid:107)2

2 +

(1 − θk)α2

k−1(cid:107)vk − w∗(cid:107)2
2

κk + λ
2

(51)
=

Sk
1 − αk

+ Fk(w∗) +

(cid:107)wk − zk−1(cid:107)2

2 −

κk+1αkηkθk
2(1 − αk)

(cid:107)vk − w∗(cid:107)2
2 .

κk
2
κk
2

24

(48)

(49)

(50)

(51)

(52)

In view of (49) and (52), we can simplify (47) as

Sk
1 − αk

+

κk
2

(cid:107)wk − zk−1(cid:107)2

2 −

κk+1αkηkθk
2(1 − αk)

≤ (1 − αk−1) (Fk(wk−1) − Fk(w∗)) +

(cid:107)vk − w∗(cid:107)2
2
κkαk−1ηk−1
2

(cid:107)vk−1 − w∗(cid:107)2

2 +

(cid:15)k
θk

.

We make a distinction for k ≥ 2 and k = 1 here. For k ≥ 2, the condition that µk−1 ≥ µk gives us,

Fk(wk−1) − Fk(w∗)

(43)
≤ Fk−1(wk−1) − Fk−1(w∗) + (µk−1 − µk)D .

(53)

(54)

The right hand side of (53) can now be upper bounded by

(1 − αk−1)(µk−1 − µk)D + Sk−1 +

(cid:15)k
θk

,

and noting that 1 − αk−1 ≤ 1 yields (45) for k ≥ 2.

For k = 1, we note that Sk−1(= S0) is deﬁned in terms of F (w). So we have,

F1(w0) − F1(w∗) ≤ F (w0) − F (w∗) + µ1D = F (w0) − F (w∗) + (µ0 − µ1)D ,

because we used µ0 = 2µ1. This is of the same form as (54). Therefore, (45) holds for k = 1 as well.

We now prove Thm. 16.

Proof of Thm. 16. We continue to use shorthand Fk := Fµkω, and D := Dω. We now apply Lemma 25. In
order to satisfy the supposition of Lemma 25 that wk is (cid:15)k-suboptimal, we make the choice (cid:15)k = δkκk
2 (cid:107)wk −
zk−1(cid:107)2

2 (cf. (26)). Plugging this in and setting θk = δk < 1, we get from (45),

Sk
1 − αk

−

κk+1ηkαkδk
2(1 − αk)

(cid:107)vk − w∗(cid:107)2

2 ≤ Sk−1 + (µk−1 − µk)D .

The left hand side simpliﬁes to Sk (1 − δk)/(1 − αk) + δk(Fk(wk) − Fk(w∗)). Note that Fk(wk) −
(43)
≥ F (wk) − F (w∗) − µkD ≥ −µkD. From this, noting that αk ∈ (0, 1) for all k, we get,
Fk(w∗)

Sk

(cid:19)

(cid:18) 1 − δk
1 − αk

≤ Sk−1 + δkµkD + (µk−1 − µk)D ,

or equivalently,

Sk ≤

(cid:19)

(cid:18) 1 − αk
1 − δk

(cid:19)

(cid:18) 1 − αk
1 − δk

Sk−1 +

(µk−1 − (1 − δk)µk)D .

Unrolling the recursion for Sk, we now have,

Sk ≤





k
(cid:89)

j=1

1 − αj
1 − δj



 S0 +

k
(cid:88)

k
(cid:89)





j=1

i=j

1 − αi
1 − δi



 (µj−1 − (1 − δj)µj)D .

(55)

25

Now, we need to reason about S0 and Sk to complete the proof. To this end, consider η0:

(36)
=

η0

α0γ0
γ1 + α0γ0

α0γ0

(34)
=

=

(κ1 + λ)α2

0 + α0
1−α0

α0γ0(1 − α0)

(κ1 + λ)α2

0 − λα2
0

(cid:1)

(cid:0)(κ1 + λ)α2
0 − λα0
γ0
κ1α0

= (1 − α0)

.

(56)

With this, we can expand out S0 to get

S0

(44)
= (1 − α0) (F (w0) − F (w∗)) +
γ0
2

F (w0) − F ∗ +

(56)
= (1 − α0)

(cid:16)

(cid:107)w0 − w∗(cid:107)2
2

α0κ1η0
2
(cid:107)w0 − w∗(cid:107)2
2

(cid:17)

.

Lastly, we reason about Sk for k ≥ 1 as,

(44)
≥ (1 − αk) (Fk(wk) − Fk(w∗))

(43)
≥ (1 − αk) (F (wk) − F (w∗) − µkD) .

Sk

Plugging this into the left hand side of (55) completes the proof.

5.2.3

Inner Loop Complexity

Consider a class FL,λ of functions deﬁned as

(cid:110)

FL,λ =

f : Rd → R such that f is L-smooth and λ-strongly convex

(cid:111)

.

We now formally deﬁne a linearly convergent algorithm on this class of functions.

Deﬁnition 26. A ﬁrst order algorithm M is said to be linearly convergent with parameters C : R+ × R+ →
R+ and τ : R+ × R+ → (0, 1) if the following holds: for all L ≥ λ > 0, and every f ∈ FL,λ and w0 ∈ Rd,
M started at w0 generates a sequence (wk)k≥0 that satisﬁes:

Ef (wk) − f ∗ ≤ C(L, λ) (1 − τ (L, λ))k (f (w0) − f ∗) ,

(57)

where f ∗ := minw∈Rd f (w) and the expectation is over the randomness of M.

The parameter τ determines the rate of convergence of the algorithm. For instance, batch gradient
descent is a deterministic linearly convergent algorithm with τ (L, λ)−1 = L/λ and incremental algorithms
such as SVRG and SAGA satisfy requirement (57) with τ (L, λ)−1 = c(n+ L/λ) for some universal constant
c.

The warm start strategy in step k of Algo. 4 is to initialize M at the prox center zk−1. The next
proposition, due to Lin et al. [2018, Cor. 16] bounds the expected number of iterations of M required to
ensure that wk satisﬁes (26). Its proof has been given in Appendix D.3 for completeness.

Proposition 27. Consider Fµω,κ(· ; z) deﬁned in Eq. (25), and a linearly convergent algorithm M with
parameters C, τ . Let δ ∈ [0, 1). Suppose Fµω is Lµω-smooth and λ-strongly convex. Then the expected
number of iterations E[ (cid:98)T ] of M when started at z in order to obtain (cid:98)w ∈ Rd that satisﬁes

Fµω,κ( (cid:98)w; z) − min
w

Fµω,κ(w; z) ≤ δκ

2 (cid:107)w − z(cid:107)2

2

26

Table 4: Summary of global complexity of Casimir-SVRG, i.e., Algorithm 4 with SVRG as the inner solver for various
parameter settings. We show E[N ], the expected total number of SVRG iterations required to obtain an accuracy (cid:15), up
to constants and factors logarithmic in problem parameters. We denote ∆F0 := F (w0) − F ∗ and ∆0 = (cid:107)w0 − w∗(cid:107)2.
Constants D, A are short for Dω, Aω (see (58)).

Prop.

λ > 0

µk

κk

δk
(cid:113) λ(cid:15)n
AD

c(cid:48)

(cid:15)/D AD/(cid:15)n − λ

λ

29

30

31

32

Yes

Yes

No

No

µck

(cid:15)/D

µ/k

AD/(cid:15)n

1/k2

n

κ0 k

1/k2

E[N ]
(cid:113) ADn
λ(cid:15)

n +

n + A
λ(cid:15)
(cid:113) ∆F0

∆F0+µD
µ

√

(cid:15) +
(cid:16)

ADn∆0
(cid:15)
(cid:17)

(cid:98)∆0
(cid:15)

n + A
µκ0

Remark

ﬁx (cid:15) in advance

c, c(cid:48) < 1 are universal constants

ﬁx (cid:15) in advance

(cid:98)∆0 = ∆F0 + κ0

2 ∆2

0 + µD

is upper bounded by

E[ (cid:98)T ] ≤

1
τ (Lµω + κ, λ + κ)

log

(cid:18) 8C(Lµω + κ, λ + κ)
τ (Lµω + κ, λ + κ)

·

Lµω + κ
κδ

(cid:19)

+ 1 .

5.3 Casimir with SVRG

We now choose SVRG [Johnson and Zhang, 2013] to be the linearly convergent algorithm M, resulting
in an algorithm called Casimir-SVRG. The rest of this section analyzes the total iteration complexity of
Casimir-SVRG to solve Problem (22). The proofs of the results from this section are calculations stemming
from combining the outer loop complexity from Cor. 17 to 20 with the inner loop complexity from Prop. 27,
and are relegated to Appendix D.4. Table 4 summarizes the results of this section.

Recall that if ω is 1-strongly convex with respect to (cid:107) · (cid:107)α, then hµω(Aw + b) is Lµω-smooth with
2,α/µ. Therefore, the complexity of solving problem (22) will depend

respect to (cid:107) · (cid:107)2, where Lµω = (cid:107)A(cid:107)2
on

Aω := max
i=1,··· ,n

(cid:107)A(i)(cid:107)2

2,α .

(58)

Remark 28. We have that (cid:107)A(cid:107)2,2 = (cid:107)A(cid:107)2 is the spectral norm of A and (cid:107)A(cid:107)2,1 = maxj (cid:107)aj(cid:107)2 is the
largest row norm, where aj is the jth row of A. Moreover, we have that (cid:107)A(cid:107)2,2 ≥ (cid:107)A(cid:107)2,1.

We start with the strongly convex case with constant smoothing.

Proposition 29. Consider the setting of Thm. 16 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as the inner
solver with parameters: µk = µ = (cid:15)/10Dω, κk = k chosen as

√

√

√

q = λ/(λ + κ), α0 =
F (w) − F (w∗) ≤ (cid:15) is bounded in expectation as
(cid:32)

q, and δ =

q/(2 −

q). Then, the number of iterations N to obtain w such that

κ =

(cid:40) A

µn − λ , if A
λ , otherwise

µn > 4λ

,

E[N ] ≤ (cid:101)O

n +

(cid:114)

(cid:33)

.

AωDωn
λ(cid:15)

27

Here, we note that κ was chosen to minimize the total complexity (cf. Lin et al. [2018]). This bound is
known to be tight, up to logarithmic factors [Woodworth and Srebro, 2016]. Next, we turn to the strongly
convex case with decreasing smoothing.

Proposition 30. Consider the setting of Thm. 16. Suppose λ > 0 and κk = κ, for all k ≥ 1 and that α0,
(µk)k≥1 and (δk)k≥1 are chosen as in Cor. 18, with q = λ/(λ+κ) and η = 1−
q/2. If we run Algo. 4 with
SVRG as the inner solver with these parameters, the number of iterations N of SVRG required to obtain w
such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

√

(cid:18)

E[N ] ≤ (cid:101)O

n +

(cid:18)

Aω
µ(λ + κ)(cid:15)

F (w0) − F ∗ +

µDω
√
1 −

q

(cid:19)(cid:19)

.

Unlike the previous case, there is no obvious choice of κ, such as to minimize the global complexity. Notice
that we do not get the accelerated rate of Prop. 29. We now turn to the case when λ = 0 and µk = µ for all
k.

Proposition 31. Consider the setting of Thm. 16 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as the inner
5 − 1)/2, δk = 1/(k + 1)2, and κk = κ =
solver with parameters: µk = µ = (cid:15)/20Dω, α0 = (
Aω/µ(n + 1). Then, the number of iterations N to get a point w such that F (w) − F ∗ ≤ (cid:15) is bounded in
expectation as

√

(cid:32)

(cid:114)

E[N ] ≤ (cid:101)O

n

F (w0) − F ∗
(cid:15)

(cid:112)

+

AωDωn

(cid:107)w0 − w∗(cid:107)2
(cid:15)

(cid:33)

.

This rate is tight up to log factors [Woodworth and Srebro, 2016]. Lastly, we consider the non-strongly
convex case (λ = 0) together with decreasing smoothing. As with Prop. 30, we do not obtain an accelerated
rate here.

Proposition 32. Consider the setting of Thm. 16. Suppose λ = 0 and that α0, (µk)k≥1,(κk)k≥1 and (δk)k≥1
are chosen as in Cor. 20. If we run Algo. 4 with SVRG as the inner solver with these parameters, the number
of iterations N of SVRG required to obtain w such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

E[N ] ≤ (cid:101)O

(cid:0)F (w0) − F ∗ + κ(cid:107)w0 − w∗(cid:107)2

2 + µD(cid:1)

(cid:18) 1
(cid:15)

(cid:18)

n +

(cid:19)(cid:19)

.

Aω
µκ

6 Extension to Non-Convex Optimization

Let us now turn to the optimization problem (1) in full generality where the mappings g(i) deﬁned in (6) are
not constrained to be afﬁne:

(cid:34)

min
w∈Rd

F (w) :=

1
n

n
(cid:88)

i=1

h(g(i)(w)) +

(cid:107)w(cid:107)2
2

,

λ
2

(cid:35)

(59)

where h is a simple, non-smooth, convex function, and each g(i) is a continuously differentiable nonlinear
map and λ ≥ 0.

We describe the prox-linear algorithm in Sec. 6.1, followed by the convergence guarantee in Sec. 6.2

and the total complexity of using Casimir-SVRG together with the prox-linear algorithm in Sec. 6.3.

28

Algorithm 5 (Inexact) Prox-linear algorithm: outer loop

1: Input: Smoothable objective F of the form (59) with h simple, step length η, tolerances ((cid:15)k)k≥1, initial

point w0, non-smooth convex optimization algorithm, M, time horizon K

2: for k = 1 to K do
3:

Using M with wk−1 as the starting point, ﬁnd

(cid:20)

(cid:98)wk ≈ arg min

w

Fη(w; wk−1) :=

1
n

n
(cid:88)

i=1

h(cid:0)g(i)(wk−1) + ∇g(i)(wk−1)(w − wk−1)(cid:1)

+

(cid:107)w(cid:107)2

2 +

(cid:107)w − wk−1(cid:107)2
2 ,

λ
2

1
2η

(cid:21)

(61)

(62)

such that

4:
5: end for
6: return wK.

Fη( (cid:98)wk; wk−1) − min
w∈Rd

Fη(w; wk−1) ≤ (cid:15)k .

Set wk = (cid:98)wk if F ( (cid:98)wk) ≤ F (wk−1), else set wk = wk−1.

6.1 The Prox-Linear Algorithm

The exact prox-linear algorithm of Burke [1985] generalizes the proximal gradient algorithm (see e.g.,
Nesterov [2013]) to compositions of convex functions with smooth mappings such as (59). When given a
function f = h ◦ g, the prox-linear algorithm deﬁnes a local convex approximation f (· ; wk) about some
point w ∈ Rd by linearizing the smooth map g as f (w; wk) := h(g(wk) + ∇g(wk)(w − wk)) . With this,
it builds a convex model F (· ; wk) of F about wk as

F (w; wk) :=

h(g(i)(wk) + ∇g(i)(wk)(w − wk)) +

(cid:107)w(cid:107)2
2 .

λ
2

Given a step length η > 0, each iteration of the exact prox-linear algorithm then minimizes the local convex
model plus a proximal term as

wk+1 = arg min

Fη(w; wk) := F (w; wk) +

(60)

(cid:21)

(cid:107)w − wk(cid:107)2
2

.

1
2η

Following Drusvyatskiy and Paquette [2018], we consider an inexact prox-linear algorithm, which ap-
proximately solves (60) using an iterative algorithm. In particular, since the function to be minimized in
(60) is precisely of the form (23), we employ the fast convex solvers developed in the previous section as
subroutines. Concretely, the prox-linear outer loop is displayed in Algo. 5. We now delve into details about
the algorithm and convergence guarantees.

6.1.1

Inexactness Criterion

As in Section 5, we must be prudent in choosing when to terminate the inner optimization (Line 3 of Algo. 5).
Function value suboptimality is used as the inexactness criterion here. In particular, for some speciﬁed

1
n

n
(cid:88)

i=1

(cid:20)

w∈Rd

29

tolerance (cid:15)k > 0, iteration k of the prox-linear algorithm accepts a solution (cid:98)w that satisﬁes Fη( (cid:98)wk; wk−1) −
minw Fη(w; wk−1) ≤ (cid:15)k.

Implementation In view of the (λ + η−1)-strong convexity of Fη(· ; wk−1), it sufﬁces to ensure that
(λ + η−1)(cid:107)v(cid:107)2

2 ≤ (cid:15)k for a subgradient v ∈ ∂Fη( (cid:98)wk; wk−1).

Fixed Iteration Budget As in the convex case, we consider as a practical alternative a ﬁxed iteration
budget Tbudget and optimize Fη(· ; wk) for exactly Tbudget iterations of M. Again, we do not have a
theoretical analysis for this scheme but ﬁnd it to be effective in practice.

6.1.2 Warm Start of Subproblems

As in the convex case, we advocate the use of the prox center wk−1 to warm start the inner optimization
problem in iteration k (Line 3 of Algo. 5).

6.2 Convergence analysis of the prox-linear algorithm

We now state the assumptions and the convergence guarantee of the prox-linear algorithm.

6.2.1 Assumptions

For the prox-linear algorithm to work, the only requirement is that we minimize an upper model. The
assumption below makes this concrete.

Assumption 33. The map g(i) is continuously differentiable everywhere for each i ∈ [n]. Moreover, there
exists a constant L > 0 such that for all w, w(cid:48) ∈ Rd and i ∈ [n], it holds that

h(cid:0)g(i)(w(cid:48))(cid:1) ≤ h(cid:0)g(i)(w) + ∇g(i)(w)(w(cid:48) − w)(cid:1) +

L
2

(cid:107)w(cid:48) − w(cid:107)2
2 .

When h is G-Lipschitz and each g(i) is (cid:101)L-smooth, both with respect to (cid:107) · (cid:107)2, then Assumption 33 holds
with L = G(cid:101)L [Drusvyatskiy and Paquette, 2018]. In the case of structured prediction, Assumption 33 holds
when the augmented score ψ as a function of w is L-smooth. The next lemma makes this precise and its
proof is in Appendix D.5.

Lemma 34. Consider the structural hinge loss f (w) = maxy∈Y ψ(y; w) = h ◦ g(w) where h, g are as
deﬁned in (6). If the mapping w (cid:55)→ ψ(y; w) is L-smooth with respect to (cid:107) · (cid:107)2 for all y ∈ Y, then it holds
for all w, z ∈ Rd that

|h(g(w + z)) − h(g(w) + ∇g(w)z)| ≤

6.2.2 Convergence Guarantee

Convergence is measured via the norm of the prox-gradient (cid:37)η(·), also known as the gradient mapping,
deﬁned as

(cid:37)η(w) =

w − arg min

Fη(z; w)

.

(63)

L
2

(cid:107)z(cid:107)2
2 .

(cid:33)

(cid:32)

1
η

z∈Rd

30

The measure of stationarity (cid:107)(cid:37)η(w)(cid:107) turns out to be related to the norm of the gradient of the Moreau
envelope of F under certain conditions - see Drusvyatskiy and Paquette [2018, Section 4] for a discussion.
In particular, a point w with small (cid:107)(cid:37)η(w)(cid:107) means that w is close to w(cid:48) = arg minz∈Rd Fη(z; w), which
is nearly stationary for F .

The prox-linear outer loop shown in Algo. 5 has the following convergence guarantee [Drusvyatskiy and

Paquette, 2018, Thm. 5.2].

Theorem 35. Consider F of the form (59) that satisﬁes Assumption 33, a step length 0 < η ≤ 1/L and a
non-negative sequence ((cid:15)k)k≥1. With these inputs, Algo. 5 produces a sequence (wk)k≥0 that satisﬁes

min
k=0,··· ,K−1

(cid:107)(cid:37)η(wk)(cid:107)2

2 ≤

F (w0) − F ∗ +

(cid:32)

2
ηK

(cid:33)

(cid:15)k

,

K
(cid:88)

k=1

where F ∗ = inf w∈Rd F (w). In addition, we have that the sequence (F (wk))k≥0 is non-increasing.

Remark 36. Algo. 5 accepts an update only if it improves the function value (Line 4). A variant of Algo. 5
which always accepts the update has a guarantee identical to that of Thm. 35, but the sequence (F (wk))k≥0
would not guaranteed to be non-increasing.

6.3 Prox-Linear with Casimir-SVRG

We now analyze the total complexity of minimizing the ﬁnite sum problem (59) with Casimir-SVRG to
approximately solve the subproblems of Algo. 5.

For the algorithm to converge, the map w (cid:55)→ g(i)(wk) + ∇g(i)(wk)(w − wk) must be Lipschitz for

each i and each iterate wk. To be precise, we assume that

Aω := max
i=1,··· ,n

sup
w∈Rd

(cid:107)∇g(i)(w)(cid:107)2

2,α

(64)

is ﬁnite, where ω, the smoothing function, is 1-strongly convex with respect to (cid:107) · (cid:107)α. When g(i) is the linear
map w (cid:55)→ A(i)w, this reduces to (58).

We choose the tolerance (cid:15)k to decrease as 1/k. When using the Casimir-SVRG algorithm with constant
smoothing (Prop. 29) as the inner solver, this method effectively smooths the kth prox-linear subproblem as
1/k. We have the following rate of convergence for this method, which is proved in Appendix D.6.

Proposition 37. Consider the setting of Thm. 35. Suppose the sequence ((cid:15)k)k≥1 satisﬁes (cid:15)k = (cid:15)0/k for
some (cid:15)0 > 0 and that the subproblem of Line 3 of Algo. 5 is solved using Casimir-SVRG with the settings
of Prop. 29. Then, total number of SVRG iterations N required to produce a w such that (cid:107)(cid:37)η(w)(cid:107)2 ≤ (cid:15) is
bounded as

E[N ] ≤ (cid:101)O





n
η(cid:15)2 (F (w0) − F ∗ + (cid:15)0) +

(cid:113)

AωDωn(cid:15)−1
0
η(cid:15)3



(F (w0) − F ∗ + (cid:15)0)3/2

 .

Remark 38. When an estimate or an upper bound B on F (w0) − F ∗, one could set (cid:15)0 = O(B). This is
true, for instance, in the structured prediction task where F ∗ ≥ 0 whenever the task loss (cid:96) is non-negative
(cf. (4)).

31

7 Experiments

In this section, we study the experimental behavior of the proposed algorithms on two structured prediction
tasks, namely named entity recognition and visual object localization. Recall that given training examples
{(x(i), y(i))}n

i=1, we wish to solve the problem:

(cid:34)

min
w∈Rd

F (w) :=

(cid:107)w(cid:107)2

2 +

λ
2

1
n

n
(cid:88)

i=1

(cid:110)

max
y(cid:48)∈Y(x(i))

φ(x(i), y(cid:48); w) + (cid:96)(y(i), y(cid:48))

− φ(x(i), y(i); w)

.

(cid:111)

(cid:35)

Note that we now allow the output space Y(x) to depend on the instance x - the analysis from the previous
sections applies to this setting as well. In all the plots, the shaded region represents one standard deviation
over ten random runs.

We compare the performance of various optimization algorithms based on the number of calls to a
smooth inference oracle. Moreover, following literature for algorithms based on SVRG [Schmidt et al.,
2017, Lin et al., 2018], we exclude the cost of computing the full gradients.

The results must be interpreted keeping in mind that the running time of all inference oracles is not
the same. These choices were motivated by the following reasons, which may not be appropriate in all
contexts. The ultimate yardstick to benchmark the performance of optimization algorithms is wall clock
time. However, this depends heavily on implementation, system and ambient system conditions. With
regards to the differing running times of different oracles, we ﬁnd that a small value of K, e.g., 5 sufﬁces,
so that our highly optimized implementations of the top-K oracle incurs negligible running time penalties
over the max oracle. Moreover, the computations of the batch gradient have been neglected as they are
embarrassingly parallel.

The outline of the rest of this section is as follows. First, we describe the datasets and task description
in Sec. 7.1, followed by methods compared in Sec. 7.2 and their hyperparameter settings in Sec. 7.3. Lastly,
Sec. 7.4 presents the experimental studies.

7.1 Dataset and Task Description

For each of the tasks, we specify below the following: (a) the dataset {(x(i), y(i))}n
i=1, (b) the output
structure Y, (c) the loss function (cid:96), (d) the score function φ(x, y; w), (e) implementation of inference
oracles, and lastly, (f) the evaluation metric used to assess the quality of predictions.

7.1.1 CoNLL 2003: Named Entity Recognition

Named entities are phrases that contain the names of persons, organization, locations, etc, and the task is
to predict the label (tag) of each entity. Named entity recognition can be formulated as a sequence tagging
problem where the set Ytag of individual tags is of size 7.

Each datapoint x is a sequence of words x = (x1, · · · , xp), and the label y = (y1, · · · , yp) ∈ Y(x) is a

sequence of the same length, where each yi ∈ Ytag is a tag.

Loss Function The loss function is the Hamming Loss (cid:96)(y, y(cid:48)) = (cid:80)
i

I(yi (cid:54)= y(cid:48)

i).

Score Function We use a chain graph to represent this task. In other words, the observation-label depen-
dencies are encoded as a Markov chain of order 1 to enable efﬁcient inference using the Viterbi algorithm.
We only consider the case of linear score φ(x, y; w) = (cid:104)w, Φ(x, y)(cid:105) for this task. The feature map Φ here

32

is very similar to that given in Example 5. Following Tkachenko and Simanovsky [2012], we use local con-
text Ψi(x) around ith word xi of x. In particular, deﬁne Ψi(x) = exi−2 ⊗ · · · ⊗ exi+2, where ⊗ denotes the
Kronecker product between column vectors, and exi denotes a one hot encoding of word xi, concatenated
with the one hot encoding of its the part of speech tag and syntactic chunk tag which are provided with the
input. Now, we can deﬁne the feature map Φ as

Φ(x, y) =

Ψv(x) ⊗ eyv

⊕

eyv ⊗ eyv+1

,

(cid:35)

(cid:34) p

(cid:88)

i=0

(cid:35)

(cid:34) p

(cid:88)

v=1

where ey ∈ R|Ytag| is a one hot-encoding of y ∈ Ytag, and ⊕ denotes vector concatenation.

Inference We use the Viterbi algorithm as the max oracle (Algo. 1) and top-K Viterbi algorithm (Algo. 2)
for the top-K oracle.

Dataset The dataset used was CoNLL 2003 [Tjong Kim Sang and De Meulder, 2003], which contains
about ∼ 20K sentences.

Evaluation Metric We follow the ofﬁcial CoNLL metric: the F1 measure excluding the ‘O’ tags. In
addition, we report the objective function value measured on the training set (“train loss”).

Other Implementation Details The sparse feature vectors obtained above are hashed onto 216 − 1 dimen-
sions for efﬁciency.

7.1.2 PASCAL VOC 2007: Visual Object Localization

Given an image and an object of interest, the task is to localize the object in the given image, i.e., determine
the best bounding box around the object. A related, but harder task is object detection, which requires
identifying and localizing any number of objects of interest, if any, in the image. Here, we restrict ourselves
to pure localization with a single instance of each object. Given an image x ∈ X of size n1 × n2, the label
y ∈ Y(x) is a bounding box, where Y(x) is the set of all bounding boxes in an image of size n1 × n2. Note
that |Y(x)| = O(n2

1n2

2).

Loss Function The PASCAL IoU metric [Everingham et al., 2010] is used to measure the quality of
localization. Given bounding boxes y, y(cid:48), the IoU is deﬁned as the ratio of the intersection of the bounding
boxes to the union:

IoU(y, y(cid:48)) =

Area(y ∩ y(cid:48))
Area(y ∪ y(cid:48))

.

We then use the 1 − IoU loss deﬁned as (cid:96)(y, y(cid:48)) = 1 − IoU(y, y(cid:48)).

Score Function The formulation we use is based on the popular R-CNN approach [Girshick et al., 2014].
linear score and non-linear score φ, both of which are based on the following
We consider two cases:
deﬁnition of the feature map Φ(x, y).

• Consider a patch x|y of image x cropped to box y, and rescale it to 64 × 64. Call this Π(x|y).

33

• Consider a convolutional neural network known as AlexNet [Krizhevsky et al., 2012] pre-trained on
ImageNet [Russakovsky et al., 2015] and pass Π(x|y) through it. Take the output of conv4, the
penultimate convolutional layer as the feature map Φ(x, y). It is of size 3 × 3 × 256.

In the case of linear score functions, we take φ(x, y; w) = (cid:104)w, Φ(x, y)(cid:105). In the case of non-linear score
functions, we deﬁne the score φ as the the result of a convolution composed with a non-linearity and followed
by a linear map. Concretely, for θ ∈ RH×W ×C1 and w ∈ RC1×C2 let the map θ (cid:55)→ θ (cid:63) w ∈ RH×W ×C2
denote a two dimensional convolution with stride 1 and kernel size 1, and σ : R → R denote the exponential
linear unit, deﬁned respectively as

[θ (cid:63) w]ij = w(cid:62)[θ]ij

and σ(x) = x I(x ≥ 0) + (exp(x) − 1) I(x < 0) ,

where [θ]ij ∈ RC1 is such that its lth entry is θijl and likewise for [θ (cid:63) w]ij. We overload notation to
let σ : Rd → Rd denote the exponential linear unit applied element-wise. Notice that σ is smooth. The
non-linear score function φ is now deﬁned, with w1 ∈ R256×16, w2 ∈ R16×3×3 and w = (w1, w2), as,

φ(x, y; w) = (cid:104)σ(Φ(x, y) (cid:63) w1), w2(cid:105) .

Inference For a given input image x, we follow the R-CNN approach [Girshick et al., 2014] and use
selective search [Van de Sande et al., 2011] to prune the search space. In particular, for an image x, we use
the selective search implementation provided by OpenCV [Bradski, 2000] and take the top 1000 candidates
returned to be the set (cid:98)Y(x), which we use as a proxy for Y(x). The max oracle and the top-K oracle are
then implemented as exhaustive searches over this reduced set (cid:98)Y(x).

Dataset We use the PASCAL VOC 2007 dataset [Everingham et al., 2010], which contains ∼ 5K an-
notated consumer (real world) images shared on the photo-sharing site Flickr from 20 different object cat-
egories. For each class, we consider all images with only a single occurrence of the object, and train an
independent model for each class.

Evaluation Metric We keep track of two metrics. The ﬁrst is the localization accuracy, also known as
CorLoc (for correct localization), following Deselaers et al. [2010]. A bounding box with IoU > 0.5 with the
ground truth is considered correct and the localization accuracy is the fraction of images labeled correctly.
The second metric is average precision (AP), which requires a conﬁdence score for each prediction. We use
φ(x, y(cid:48); w) as the conﬁdence score of y(cid:48). As previously, we also plot the objective function value measured
on the training examples.

Other Implementation Details For a given input-output pair (x, y) in the dataset, we instead use (x, (cid:98)y)
as a training example, where (cid:98)y = arg maxy(cid:48)∈ (cid:98)Y(x) IoU(y, y(cid:48)) is the element of (cid:98)Y(x) which overlaps the
most with the true output y.

7.2 Methods Compared

The experiments compare various convex stochastic and incremental optimization methods for structured
prediction.

34

• SGD: Stochastic subgradient method with a learning rate γt = γ0/(1+(cid:98)t/t0(cid:99)), where η0, t0 are tuning
parameters. Note that this scheme of learning rates does not have a theoretical analysis. However,
the averaged iterate wt = 2/(t2 + t) (cid:80)t
τ =1 τ wτ obtained from the related scheme γt = 1/(λt) was
shown to have a convergence rate of O((λ(cid:15))−1) [Shalev-Shwartz et al., 2011, Lacoste-Julien et al.,
2012]. It works on the non-smooth formulation directly.

• BCFW: The block coordinate Frank-Wolfe algorithm of Lacoste-Julien et al. [2013]. We use the
version that was found to work best in practice, namely, one that uses the weighted averaged iterate
wt = 2/(t2 + t) (cid:80)t
τ =1 τ wτ (called bcfw-wavg by the authors) with optimal tuning of learning
rates. This algorithm also works on the non-smooth formulation and does not require any tuning.

• SVRG: The SVRG algorithm proposed by Johnson and Zhang [2013], with each epoch making one
pass through the dataset and using the averaged iterate to compute the full gradient and restart the next
epoch. This algorithm requires smoothing.

• Casimir-SVRG-const: Algo. 4 with SVRG as the inner optimization algorithm. The parameters µk
and κk as chosen in Prop. 29, where µ and κ are hyperparameters. This algorithm requires smoothing.

• Casimir-SVRG-adapt: Algo. 4 with SVRG as the inner optimization algorithm. The parameters µk
and κk as chosen in Prop. 30, where µ and κ are hyperparameters. This algorithm requires smoothing.

On the other hand, for non-convex structured prediction, we only have two methods:

• SGD: The stochastic subgradient method [Davis and Drusvyatskiy, 2018], which we call as SGD. This
√
algorithm works directly on the non-smooth formulation. We try learning rates γt = γ0, γt = γ0/
t
and γt = γ0/t, where γ0 is found by grid search in each of these cases. We use the names SGD-const,
SGD-t−1/2 and SGD-t−1 respectively for these variants. We note that SGD-t−1 does not have any
theoretical analysis in the non-convex case.

• PL-Casimir-SVRG: Algo. 5 with Casimir-SVRG-const as the inner solver using the settings of

Prop. 37. This algorithm requires smoothing the inner subproblem.

7.3 Hyperparameters and Variants

Smoothing In light of the discussion of Sec. 4, we use the (cid:96)2
strategy for efﬁcient computation. We then have Dω = 1/2.

2 smoother ω(u) = (cid:107)u(cid:107)2

2/2 and use the top-K

Regularization The regularization coefﬁcient λ is chosen as c/n, where c is varied in {0.01, 0.1, 1, 10}.

Choice of K The experiments use K = 5 for named entity recognition where the performance of the
top-K oracle is K times slower, and K = 10 for visual object localization, where the running time of the
top-K oracle is independent of K. We also present results for other values of K in Fig. 5d and ﬁnd that the
performance of the tested algorithms is robust to the value of K.

35

Tuning Criteria Some algorithms require tuning one or more hyperparameters such as the learning rate.
We use grid search to ﬁnd the best choice of the hyperparameters using the following criteria: For the named
entity recognition experiments, the train function value and the validation F1 metric were only weakly
correlated. For instance, the 3 best learning rates in the grid in terms of F1 score, the best F1 score attained
the worst train function value and vice versa. Therefore, we choose the value of the tuning parameter that
attained the best objective function value within 1% of the best validation F1 score in order to measure the
optimization performance while still remaining relevant to the named entity recognition task. For the visual
object localization task, a wide range of hyperparameter values achieved nearly equal performance in terms
of the best CorLoc over the given time horizon, so we choose the value of the hyperparameter that achieves
the best objective function value within a given iteration budget.

7.3.1 Hyperparameters for Convex Optimization

This corresponds to the setting of Section 5.

Learning Rate The algorithms SVRG and Casimir-SVRG-adapt require tuning of a learning rate, while
SGD requires η0, t0 and Casimir-SVRG-const requires tuning of the Lipschitz constant L of ∇Fµω, which
determines the learning rate γ = 1/(L + λ + κ). Therefore, tuning the Lipschitz parameter is similar
to tuning the learning rate. For both the learning rate and Lipschitz parameter, we use grid search on a
logarithmic grid, with consecutive entries chosen a factor of two apart.

Choice of κ For Casimir-SVRG-const, with the Lipschitz constant in hand, the parameter κ is chosen to
minimize the overall complexity as in Prop. 29. For Casimir-SVRG-adapt, we use κ = λ.

Stopping Criteria Following the discussion of Sec. 5, we use an iteration budget of Tbudget = n.

Warm Start The warm start criterion determines the starting iterate of an epoch of the inner optimization
algorithm. Recall that we solve the following subproblem using SVRG for the kth iterate (cf. (25)):

wk ≈ arg min

Fµkω,κk (wk; zk−1) .

w∈Rd

Here, we consider the following warm start strategy to choose the initial iterate (cid:98)w0 for this subproblem:

• Prox-center: (cid:98)w0 = zk−1.

In addition, we also try out the following warm start strategies of Lin et al. [2018]:

• Extrapolation: (cid:98)w0 = wk−1 + c(zk−1 − zk−2) where c = κ
• Prev-iterate: (cid:98)w0 = wk−1.

κ+λ .

We use the Prox-center strategy unless mentioned otherwise.

Level of Smoothing and Decay Strategy For SVRG and Casimir-SVRG-const with constant smoothing,
we try various values of the smoothing parameter in a logarithmic grid. On the other hand, Casimir-SVRG-
adapt is more robust to the choice of the smoothing parameter (Fig. 5a). We use the defaults of µ = 2 for
named entity recognition and µ = 10 for visual object localization.

36

Figure 2: Comparison of convex optimization algorithms for the task of Named Entity Recognition on
CoNLL 2003.

7.3.2 Hyperparameters for Non-Convex Optimization

This corresponds to the setting of Section 6.

Prox-Linear Learning Rate η We perform grid search in powers of 10 to ﬁnd the best prox-linear learning
rate η. We ﬁnd that the performance of the algorithm is robust to the choice of η (Fig. 7a).

Stopping Criteria We used a ﬁxed budget of 5 iterations of Casimir-SVRG-const. In Fig. 7b, we experi-
ment with different iteration budgets.

In order to solve the kth prox-linear subproblem with Casimir-
Level of Smoothing and Decay Strategy
SVRG-const, we must specify the level of smoothing µk. We experiment with two schemes, (a) constant
smoothing µk = µ, and (b) adaptive smoothing µk = µ/k. Here, µ is a tuning parameters, and the adaptive
smoothing scheme is designed based on Prop. 37 and Remark 38. We use the adaptive smoothing strategy
as a default, but compare the two in Fig. 6.

Gradient Lipschitz Parameter for Inner Optimization The inner optimization algorithm Casimir-SVRG-
const still requires a hyperparameter Lk to serve as an estimate to the Lipschitz parameter of the gradient
∇Fη,µkω(· ; wk). We set this parameter as follows, based on the smoothing strategy: (a) Lk = L0 with the
constant smoothing strategy, and (b) Lk = k L0 with the adaptive smoothing strategy (cf. Prop. 2). We note
that the latter choice has the effect of decaying the learning rate as 1/k in the kth outer iteration.

37

7.4 Experimental study of different methods

Convex Optimization For the named entity recognition task, Fig. 2 plots the performance of various
methods on CoNLL 2003. On the other hand, Fig. 3 presents plots for various classes of PASCAL VOC
2007 for visual object localization.

The plots reveal that smoothing-based methods converge faster in terms of training error while achieving
a competitive performance in terms of the performance metric on a held-out set. Furthermore, BCFW and
SGD make twice as many actual passes as SVRG based algorithms.

Non-Convex Optimization Fig. 4 plots the performance of various algorithms on the task of visual object
localization on PASCAL VOC.

7.5 Experimental Study of Effect of Hyperparameters: Convex Optimization

We now study the effects of various hyperparameter choices.

Effect of Smoothing Fig. 5a plots the effect of the level of smoothing for Casimir-SVRG-const and
Casimir-SVRG-adapt. The plots reveal that, in general, small values of the smoothing parameter lead to
better optimization performance for Casimir-SVRG-const. Casimir-SVRG-adapt is robust to the choice of
µ. Fig. 5b shows how the smooth optimization algorithms work when used heuristically on the non-smooth
problem.

Effect of Warm Start Strategies Fig. 5c plots different warm start strategies for Casimir-SVRG-const and
Casimir-SVRG-adapt. We ﬁnd that Casimir-SVRG-adapt is robust to the choice of the warm start strategy
while Casimir-SVRG-const is not. For the latter, we observe that Extrapolation is less stable (i.e., tends
to diverge more) than Prox-center, which is in turn less stable than Prev-iterate, which always
works (cf. Fig. 5c). However, when they do work, Extrapolation and Prox-center provide greater
acceleration than Prev-iterate. We use Prox-center as the default choice to trade-off between
acceleration and applicability.

Effect of K Fig. 5d illustrates the robustness of the method to choice of K: we observe that the results are
all within one standard deviation of each other.

7.6 Experimental Study of Effect of Hyperparameters: Non-Convex Optimization

We now study the effect of various hyperparameters for the non-convex optimization algorithms. All of
these comparisons have been made for λ = 1/n.

Effect of Smoothing Fig. 6a compares the adaptive and constant smoothing strategies. Fig. 6b and Fig. 6c
compare the effect of the level of smoothing on the the both of these. As previously, the adaptive smoothing
strategy is more robust to the choice of the smoothing parameter.

Effect of Prox-Linear Learning Rate η Fig. 7a shows the robustness of the proposed method to the
choice of η.

38

Effect of Iteration Budget Fig. 7b also shows the robustness of the proposed method to the choice of
iteration budget of the inner solver, Casimir-SVRG-const.

Effect of Warm Start of the Inner Solver Fig. 7c studies the effect of the warm start strategy used within
the inner solver Casimir-SVRG-const in each inner prox-linear iteration. The results are similar to those
obtained in the convex case, with Prox-center choice being the best compromise between acceleration
and compatibility.

8 Future Directions

We introduced a general notion of smooth inference oracles in the context of black-box ﬁrst-order opti-
mization. This allows us to set the scene to extend the scope of fast incremental optimization algorithms to
structured prediction problems owing to a careful blend of a smoothing strategy and an acceleration scheme.
We illustrated the potential of our framework by proposing a new incremental optimization algorithm to
train structural support vector machines both enjoying worst-case complexity bounds and demonstrating
competitive performance on two real-world problems. This work paves also the way to faster incremental
primal optimization algorithms for deep structured prediction models.

There are several potential venues for future work. When there is no discrete structure that admits
efﬁcient inference algorithms, it could be beneﬁcial to not treat inference as a black-box numerical proce-
dure [Meshi et al., 2010, Hazan and Urtasun, 2010, Hazan et al., 2016]. Instance-level improved algorithms
along the lines of Hazan et al. [2016] could also be interesting to explore.

Acknowledgments This work was supported by NSF Award CCF-1740551, the Washington Research
Foundation for innovation in Data-intensive Discovery, and the program “Learning in Machines and Brains”
of CIFAR.

39

Figure 3: Comparison of convex optimization algorithms for the task of visual object localization on PAS-
CAL VOC 2007 for λ = 10/n. Plots for all other classes are in Appendix E.

40

Figure 4: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n. Plots for all other classes are in Appendix E.

41

(a) Effect of level of smoothing.

(b) Effect of smoothing: use of smooth optimization with smoothing (labeled “smooth”) versus the
heuristic use of these algorithms without smoothing (labeled “non-smooth”) for λ = 0.01/n.

(c) Effect of warm start strategies for λ = 0.01/n (ﬁrst row) and λ = 1/n (second row).

(d) Effect of K in the top-K oracle (λ = 0.01/n).

Figure 5: Effect of hyperparameters for the task of Named Entity Recognition on CoNLL 2003. C-SVRG
stands for Casimir-SVRG in these plots.

42

(a) Comparison of adaptive and constant smoothing strategies.

(b) Effect of µ of the adaptive smoothing strategy.

(c) Effect of µ of the constant smoothing strategy.

Figure 6: Effect of smoothing on PL-Casimir-SVRG for the task of visual object localization on PASCAL
VOC 2007.

43

(a) Effect of the hyperparameter η.

(b) Effect of the iteration budget of the inner solver.

44

(c) Effect of the warm start strategy of the inner Casimir-SVRG-const algorithm.

Figure 7: Effect of hyperparameters on PL-Casimir-SVRG for the task of visual object localization on
PASCAL VOC 2007.

References

Z. Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. Journal of Machine

Learning Research, 18:221:1–221:51, 2017.

Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden Markov Support Vector Machines. In International

Conference on Machine Learning, pages 3–10, 2003.

D. Batra. An efﬁcient message-passing algorithm for the M -best MAP problem. In Conference on Uncer-

tainty in Artiﬁcial Intelligence, pages 121–130, 2012.

D. Batra, P. Yadollahpour, A. Guzm´an-Rivera, and G. Shakhnarovich. Diverse M -best Solutions in Markov

Random Fields. In European Conference on Computer Vision, pages 1–16, 2012.

A. Beck and M. Teboulle. Smoothing and ﬁrst order methods: A uniﬁed framework. SIAM Journal on

Optimization, 22(2):557–580, 2012.

D. Belanger and A. McCallum. Structured prediction energy networks.

In International Conference on

Machine Learning, pages 983–992, 2016.

R. Bellman. Dynamic Programming. Courier Dover Publications, 1957.

Y. Bengio, Y. LeCun, C. Nohl, and C. Burges. LeRec: A NN/HMM Hybrid for On-Line Handwriting

Recognition. Neural Computation, 7(6):1289–1303, 1995.

D. P. Bertsekas. Dynamic programming and optimal control, volume 1. Athena scientiﬁc Belmont, MA,

1995.

D. P. Bertsekas. Nonlinear programming. Athena Scientiﬁc Belmont, 1999.

L. Bottou and P. Gallinari. A Framework for the Cooperation of Learning Algorithms.

In Advances in

Neural Information Processing Systems, pages 781–788, 1990.

L. Bottou, Y. Bengio, and Y. LeCun. Global Training of Document Processing Systems Using Graph
In Conference on Computer Vision and Pattern Recognition, pages 489–494,

Transformer Networks.
1997.

G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000.

J. V. Burke. Descent methods for composite nondifferentiable optimization problems. Mathematical Pro-

gramming, 33(3):260–279, 1985.

C. Chen, V. Kolmogorov, Y. Zhu, D. N. Metaxas, and C. H. Lampert. Computing the M Most Probable
Modes of a Graphical Model. In International Conference on Artiﬁcial Intelligence and Statistics, pages
161–169, 2013.

Y.-Q. Cheng, V. Wu, R. Collins, A. R. Hanson, and E. M. Riseman. Maximum-weight bipartite matching
technique and its application in image feature matching. In Visual Communications and Image Process-
ing, volume 2727, pages 453–463, 1996.

45

M. Collins, A. Globerson, T. Koo, X. Carreras, and P. L. Bartlett. Exponentiated gradient algorithms for
conditional random ﬁelds and max-margin markov networks. Journal of Machine Learning Research, 9
(Aug):1775–1822, 2008.

R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. P. Kuksa. Natural language process-

ing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537, 2011.

G. F. Cooper. The computational complexity of probabilistic inference using bayesian belief networks.

Artiﬁcial Intelligence, 42(2-3):393–405, 1990.

B. Cox, A. Juditsky, and A. Nemirovski. Dual subgradient algorithms for large-scale nonsmooth learning

problems. Mathematical Programming, 148(1-2):143–180, 2014.

K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines.

Journal of Machine Learning Research, 2(Dec):265–292, 2001.

H. Daum´e III and D. Marcu. Learning as search optimization: approximate large margin methods for

structured prediction. In International Conference on Machine Learning, pages 169–176, 2005.

D. Davis and D. Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. arXiv

preprint arXiv:1803.06523, 2018.

and Computing, 2(1):25–36, 1992.

A. P. Dawid. Applications of a general propagation algorithm for probabilistic expert systems. Statistics

A. Defazio. A simple practical accelerated method for ﬁnite sums.

In Advances in Neural Information

Processing Systems, pages 676–684, 2016.

A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for
non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages
1646–1654, 2014.

T. Deselaers, B. Alexe, and V. Ferrari. Localizing objects while learning their appearance. In European

Conference on Computer Vision, pages 452–466, 2010.

D. Drusvyatskiy and C. Paquette. Efﬁciency of minimizing compositions of convex functions and smooth

maps. Mathematical Programming, Jul 2018.

J. C. Duchi, D. Tarlow, G. Elidan, and D. Koller. Using Combinatorial Optimization within Max-Product

Belief Propagation. In Advances in Neural Information Processing Systems, pages 369–376, 2006.

M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The Pascal Visual Object Classes

(VOC) challenge. International Journal of Computer Vision, 88(2):303–338, 2010.

N. Flerova, R. Marinescu, and R. Dechter. Searching for the M Best Solutions in Graphical Models. Journal

of Artiﬁcial Intelligence Research, 55:889–952, 2016.

M. Fromer and A. Globerson. An LP view of the M -best MAP problem. In Advances in Neural Information

Processing Systems, pages 567–575, 2009.

46

R. Frostig, R. Ge, S. Kakade, and A. Sidford. Un-regularizing: approximate proximal point and faster
stochastic algorithms for empirical risk minimization. In International Conference on Machine Learning,
pages 2540–2548, 2015.

R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Conference on Computer Vision and Pattern Recognition, pages 580–587,
2014.

D. M. Greig, B. T. Porteous, and A. H. Seheult. Exact maximum a posteriori estimation for binary images.

Journal of the Royal Statistical Society. Series B (Methodological), pages 271–279, 1989.

T. Hazan and R. Urtasun. A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Struc-

tured Prediction. In Advances in Neural Information Processing Systems, pages 838–846, 2010.

T. Hazan, A. G. Schwing, and R. Urtasun. Blending Learning and Inference in Conditional Random Fields.

Journal of Machine Learning Research, 17:237:1–237:25, 2016.

L. He, K. Lee, M. Lewis, and L. Zettlemoyer. Deep Semantic Role Labeling: What Works and What’s Next.

In Annual Meeting of the Association for Computational Linguistics, pages 473–483, 2017.

N. He and Z. Harchaoui. Semi-Proximal Mirror-Prox for Nonsmooth Composite Minimization. In Advances

in Neural Information Processing Systems, pages 3411–3419, 2015.

M. Held, P. Wolfe, and H. P. Crowder. Validation of subgradient optimization. Mathematical Programming,

6(1):62–88, Dec 1974.

T. Hofmann, A. Lucchi, S. Lacoste-Julien, and B. McWilliams. Variance reduced stochastic gradient descent

with neighbors. In Advances in Neural Information Processing Systems, pages 2305–2313, 2015.

H. Ishikawa and D. Geiger. Segmentation by grouping junctions. In Conference on Computer Vision and

Pattern Recognition, pages 125–131, 1998.

M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for the Ising model. SIAM Journal

on computing, 22(5):1087–1116, 1993.

T. Joachims, T. Finley, and C.-N. J. Yu. Cutting-plane training of structural SVMs. Machine Learning, 77

(1):27–59, 2009.

J. K. Johnson. Convex relaxation methods for graphical models: Lagrangian and maximum entropy ap-

proaches. PhD thesis, Massachusetts Institute of Technology, Cambridge, MA, USA, 2008.

R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In

Advances in Neural Information Processing Systems, pages 315–323, 2013.

V. Jojic, S. Gould, and D. Koller. Accelerated dual decomposition for MAP inference. In International

Conference on Machine Learning, pages 503–510, 2010.

D. Jurafsky, J. H. Martin, P. Norvig, and S. Russell. Speech and Language Processing. Pearson Education,

2014. ISBN 9780133252934.

ISBN 978-0-262-01319-2.

D. Koller and N. Friedman. Probabilistic Graphical Models - Principles and Techniques. MIT Press, 2009.

47

V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? IEEE Transactions

on Pattern Analysis and Machine Intelligence, 26(2):147–159, 2004.

A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet classiﬁcation with deep convolutional neural

networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012.

S. Lacoste-Julien, M. Schmidt, and F. Bach. A simpler approach to obtaining an O(1/t) convergence rate

for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002, 2012.

S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Block-Coordinate Frank-Wolfe Optimization for

Structural SVMs. In International Conference on Machine Learning, pages 53–61, 2013.

J. Lafferty, A. McCallum, and F. C. Pereira. Conditional Random Fields: Probabilistic Models for Segment-
In International Conference on Machine Learning, pages 282–289,

ing and Labeling Sequence Data.
2001.

C. H. Lampert, M. B. Blaschko, and T. Hofmann. Beyond sliding windows: Object localization by efﬁcient

subwindow search. In Conference on Computer Vision and Pattern Recognition, pages 1–8, 2008.

N. Le Roux, M. W. Schmidt, and F. R. Bach. A Stochastic Gradient Method with an Exponential Conver-
gence Rate for Strongly-Convex Optimization with Finite Training Sets. In Advances in Neural Informa-
tion Processing Systems, pages 2672–2680, 2012.

M. Lewis and M. Steedman. A* CCG parsing with a supertag-factored model. In Conference on Empirical

Methods in Natural Language Processing, pages 990–1000, 2014.

H. Lin, J. Mairal, and Z. Harchaoui. A universal catalyst for ﬁrst-order optimization. In Advances in Neural

Information Processing Systems, pages 3384–3392, 2015.

H. Lin, J. Mairal, and Z. Harchaoui. Catalyst Acceleration for First-order Convex Optimization: from

Theory to Practice. Journal of Machine Learning Research, 18(212):1–54, 2018.

L. Lov´asz. Submodular functions and convexity. In Mathematical Programming The State of the Art, pages

235–257. Springer, 1983.

J. Mairal.

Incremental majorization-minimization optimization with application to large-scale machine

learning. SIAM Journal on Optimization, 25(2):829–855, 2015.

A. F. T. Martins and R. F. Astudillo. From Softmax to Sparsemax: A Sparse Model of Attention and
Multi-Label Classiﬁcation. In International Conference on Machine Learning, pages 1614–1623, 2016.

R. J. McEliece, D. J. C. MacKay, and J. Cheng. Turbo Decoding as an Instance of Pearl’s ”Belief Propaga-

tion” Algorithm. IEEE Journal on Selected Areas in Communications, 16(2):140–152, 1998.

A. Mensch and M. Blondel. Differentiable dynamic programming for structured prediction and attention.

In International Conference on Machine Learning, pages 3459–3468, 2018.

O. Meshi, D. Sontag, T. S. Jaakkola, and A. Globerson. Learning Efﬁciently with Approximate Inference

via Dual Losses. In International Conference on Machine Learning, pages 783–790, 2010.

O. Meshi, T. S. Jaakkola, and A. Globerson. Convergence Rate Analysis of MAP Coordinate Minimization

Algorithms. In Advances in Neural Information Processing Systems, pages 3023–3031, 2012.

48

K. P. Murphy, Y. Weiss, and M. I. Jordan. Loopy belief propagation for approximate inference: An empirical

study. In Conference on Uncertainty in Artiﬁcial Intelligence, pages 467–475, 1999.

Y. Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2). In Soviet

Mathematics Doklady, volume 27, pages 372–376, 1983.

Y. Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on Optimization,

Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103(1):127–152,

Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science &

16(1):235–249, 2005a.

2005b.

Business Media, 2013.

V. Niculae, A. F. Martins, M. Blondel, and C. Cardie. SparseMAP: Differentiable Sparse Structured Infer-

ence. In International Conference on Machine Learning, pages 3796–3805, 2018.

D. Nilsson. An efﬁcient algorithm for ﬁnding the M most probable conﬁgurations in probabilistic expert

systems. Statistics and Computing, 8(2):159–173, 1998.

A. Osokin, J.-B. Alayrac, I. Lukasewitz, P. Dokania, and S. Lacoste-Julien. Minding the gaps for block
Frank-Wolfe optimization of structured SVMs. In International Conference on Machine Learning, pages
593–602, 2016.

B. Palaniappan and F. Bach. Stochastic variance reduction methods for saddle-point problems. In Advances

in Neural Information Processing Systems, pages 1408–1416, 2016.

C. Paquette, H. Lin, D. Drusvyatskiy, J. Mairal, and Z. Harchaoui. Catalyst for gradient-based nonconvex
optimization. In International Conference on Artiﬁcial Intelligence and Statistics, pages 613–622, 2018.

J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann,

N. D. Ratliff, J. A. Bagnell, and M. Zinkevich. (Approximate) Subgradient Methods for Structured Predic-

tion. In International Conference on Artiﬁcial Intelligence and Statistics, pages 380–387, 2007.

R. T. Rockafellar and R. J.-B. Wets. Variational analysis, volume 317. Springer Science & Business Media,

1988.

2009.

O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bern-
stein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International
Journal of Computer Vision, 115(3):211–252, 2015.

B. Savchynskyy, J. H. Kappes, S. Schmidt, and C. Schn¨orr. A study of Nesterov’s scheme for Lagrangian
decomposition and MAP labeling. In Conference on Computer Vision and Pattern Recognition, pages
1817–1823, 2011.

M. I. Schlesinger. Syntactic analysis of two-dimensional visual signals in noisy conditions. Kibernetika, 4

(113-130):1, 1976.

49

M. Schmidt, R. Babanezhad, M. Ahmed, A. Defazio, A. Clifton, and A. Sarkar. Non-uniform stochastic
average gradient method for training conditional random ﬁelds. In International Conference on Artiﬁcial
Intelligence and Statistics, pages 819–828, 2015.

M. Schmidt, N. Le Roux, and F. Bach. Minimizing ﬁnite sums with the stochastic average gradient. Math-

ematical Programming, 162(1-2):83–112, 2017.

A. Schrijver. Combinatorial Optimization - Polyhedra and Efﬁciency. Springer, 2003.

B. Seroussi and J. Golmard. An algorithm directly ﬁnding the K most probable conﬁgurations in Bayesian

networks. International Journal of Approximate Reasoning, 11(3):205 – 233, 1994.

S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss minimiza-

tion. Journal of Machine Learning Research, 14(Feb):567–599, 2013.

S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized

loss minimization. In International Conference on Machine Learning, pages 64–72, 2014.

S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-gradient solver for

SVM. Mathematical programming, 127(1):3–30, 2011.

H. O. Song, R. B. Girshick, S. Jegelka, J. Mairal, Z. Harchaoui, and T. Darrell. On learning to localize
objects with minimal supervision. In International Conference on Machine Learning, pages 1611–1619,
2014.

B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In Advances in Neural Information

Processing Systems, pages 25–32, 2004.

B. Taskar, S. Lacoste-Julien, and D. Klein. A discriminative matching approach to word alignment.

In
Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing, pages 73–80, 2005.

B. Taskar, S. Lacoste-Julien, and M. I. Jordan. Structured prediction, dual extragradient and Bregman

projections. Journal of Machine Learning Research, 7(Jul):1627–1653, 2006.

C. H. Teo, S. Vishwanathan, A. Smola, and Q. V. Le. Bundle methods for regularized risk minimization.

Journal of Machine Learning Research, 1(55), 2009.

E. F. Tjong Kim Sang and F. De Meulder.

Introduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Conference on Natural Language Learning, pages 142–147,
2003.

M. Tkachenko and A. Simanovsky. Named entity recognition: Exploring features. In Empirical Methods in

Natural Language Processing, pages 118–127, 2012.

I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdepen-
dent and structured output spaces. In International Conference on Machine Learning, page 104, 2004.

K. E. Van de Sande, J. R. Uijlings, T. Gevers, and A. W. Smeulders. Segmentation as selective search for

object recognition. In International Conference on Computer Vision, pages 1879–1886, 2011.

50

A. J. Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.

IEEE Trans. Information Theory, 13(2):260–269, 1967. doi: 10.1109/TIT.1967.1054010.

M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference.

Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008.

M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. MAP estimation via agreement on trees: message-
passing and linear programming. IEEE transactions on information theory, 51(11):3697–3717, 2005.

B. E. Woodworth and N. Srebro. Tight complexity bounds for optimizing composite objectives. In Advances

in Neural Information Processing Systems, pages 3639–3647, 2016.

C. Yanover and Y. Weiss. Finding the M most probable conﬁgurations using loopy belief propagation. In

Advances in Neural Information Processing Systems, pages 289–296, 2004.

X. Zhang, A. Saha, and S. Vishwanathan. Accelerated training of max-margin markov networks with ker-

nels. Theoretical Computer Science, 519:88–102, 2014.

51

A Smoothing

We ﬁrst prove an extension of Lemma 4.2 of Beck and Teboulle [2012], which proves the following state-
ment for the special case of µ2 = 0. Recall that we deﬁned hµω ≡ h when µ = 0.

Proposition 39. Consider the setting of Def. 1. For µ1 ≥ µ2 ≥ 0, we have for every z ∈ Rm that

(µ1 − µ2)

inf
u∈dom h∗

ω(u) ≤ hµ2ω(z) − hµ1ω(z) ≤ (µ1 − µ2)

sup
u∈dom h∗

ω(u) .

Proof. We successively deduce,

hµ1ω(z) = sup

{(cid:104)u, z(cid:105) − h∗(u) − µ1ω(u)}

u∈dom h∗

= sup

u∈dom h∗

≥ sup

u∈dom h∗

(cid:26)

{(cid:104)u, z(cid:105) − h∗(u) − µ2ω(u) − (µ1 − µ2)ω(u)}

(cid:104)u, z(cid:105) − h∗(u) − µ2ω(u) + inf

u(cid:48)∈dom h∗

(cid:27)
(cid:8)−(µ1 − µ2)ω(u(cid:48))}(cid:9)

= hµ2ω(z) − (µ1 − µ2)

sup
u(cid:48)∈dom h∗

ω(u(cid:48)) ,

since µ1 − µ2 ≥ 0. The other side follows using instead that

−(µ1 − µ2)ω(u) ≤ sup

(cid:8)−(µ1 − µ2)ω(u(cid:48))}(cid:9) .

u(cid:48)∈dom h∗

Next, we recall the following equivalent deﬁnition of a matrix norm deﬁned in Eq. (2).

(cid:107)A(cid:107)β,α = sup
y(cid:54)=0

(cid:107)A(cid:62)y(cid:107)∗
β
(cid:107)y(cid:107)α

= sup
x(cid:54)=0

(cid:107)Ax(cid:107)∗
α
(cid:107)y(cid:107)β

= (cid:107)A(cid:62)(cid:107)α,β .

(65)

Now, we consider the smoothness of a composition of a smooth function with an afﬁne map.

Lemma 40. Suppose h : Rm → R is L-smooth with respect to (cid:107) · (cid:107)∗
b ∈ Rm, we have that the map Rd (cid:51) w (cid:55)→ h(Aw + b) is (cid:0)L(cid:107)A(cid:62)(cid:107)2

α. Then, for any A ∈ Rm×d and
(cid:1)-smooth with respect to (cid:107) · (cid:107)β.

α,β

Proof. Fix A ∈ Rm×d, b ∈ Rm and deﬁne f : Rd → R as f (w) = h(Aw + b). By the chain rule, we have
that ∇f (w) = A(cid:62)∇h(Aw + b). Using smoothness of h, we successively deduce,

(cid:107)∇f (w1) − ∇f (w2)(cid:107)∗

β = (cid:107)A(cid:62)(∇h(Aw1 + b) − ∇h(Aw2 + b))(cid:107)∗
β

(65)
≤ (cid:107)A(cid:62)(cid:107)α,β(cid:107)∇h(Aw1 + b) − ∇h(Aw2 + b)(cid:107)α
≤ (cid:107)A(cid:62)(cid:107)α,β L(cid:107)A(w1 − w2)(cid:107)∗
α
(65)
≤ L(cid:107)A(cid:62)(cid:107)2

α,β(cid:107)w1 − w2(cid:107)β .

Shown in Algo. 6 is the procedure to compute the outputs of the top-K oracle from the K best scoring

outputs obtained, for instance, from the top-K max-product algorithm.

52

Algorithm 6 Top-K oracle from top-K outputs
1: Input: Augmented score function ψ, w ∈ Rd, µ > 0 YK = {y1, · · · , yK} such that yk =

max(k)

y∈Y ψ(y; w).

2: Populate z ∈ RK so that zk = 1
µ ψ(yk; w).
3: Compute u∗ = arg minu∈∆K−1 (cid:107)u − z(cid:107)2
4: return s = (cid:80)K
k=1 u∗

k ψ(yk; w) and v = (cid:80)K

2 by a projection on the simplex.
k ∇wψ(yk; w).

k=1 u∗

Algorithm 7 Standard max-product algorithm

1: Input: Augmented score function ψ(·, ·; w) deﬁned on tree structured graph G with root r ∈ V.
2: Initialize: Let V be a list of nodes from V\{r} arranged in increasing order of height.
3: for v in V do
4:

Set mv(yρ(v)) ← maxyv∈Yv
Yρ(v).
Assign to δv(yρ(v)) a maximizing assignment of yv from above for each yρ(v) ∈ Yρ(v).

(cid:110)
ψv(yv) + ψv,ρ(v)(yv, yρ(v)) + (cid:80)

v(cid:48)∈C(v) mv(cid:48)(yv)

(cid:111)

for each yρ(v) ∈

(cid:110)
ψr(yr) + (cid:80)

v(cid:48)∈C(r) mv(cid:48)(yr)

(cid:111)
.

(cid:110)
ψr(yr) + (cid:80)

v(cid:48)∈C(r) mv(cid:48)(yr)

(cid:111)
.

5:
6: end for
7: ψ∗ ← maxyr∈Yr
8: y∗
r ← arg maxyr∈Yr
9: for v in reverse(V ) do
v = δv(y∗
y∗
10:
11: end for
12: return ψ∗, y∗ = (y∗

ρ(v)).

1, · · · , y∗

p).

B Smooth Inference in Trees

A graph G is a tree if it is connected, directed and each node has at most one incoming edge. It has one root
r ∈ V with no incoming edge. An undirected graph with no loops can be converted to a tree by ﬁxing an
arbitrary root and directing all edges way from the root. We say that G is a chain if it is a tree with root p
where all edges are of the form (v + 1, v). For a node v in a tree G, we denote by ρ(v) and C(v) respectively
the parent of v and the children of v in the tree.

Recall ﬁrst that the height of a node in a rooted tree is the number of edges on the longest directed
path from the node to a leaf where each edge is directed away from the root. We ﬁrst review the standard
max-product algorithm for maximum a posteriori (MAP) inference [Dawid, 1992] - Algo. 7. It runs in time
O(p maxv∈V |Yv|2) and requires space O(p maxv∈V |Yv|).

B.1 Proof of Correctness of Top-K Max-Product

We now consider the top-K max-product algorithm, shown in full generality in Algo. 8. The following
proposition proves its correctness.

Proposition 41. Consider as inputs to Algo. 8 an augmented score function ψ(·, ·; w) deﬁned on tree
structured graph G, and an integer K > 0. Then, the outputs of Algo. 8 satisfy ψ(k) = ψ(y(k)) =
max(k)
Proof. For a node v ∈ V, let τ (v) denote the sub-tree of G rooted at v. Let yτ (v) denote (cid:0)yv(cid:48) for v(cid:48) ∈ τ (v)(cid:1).

y∈Y ψ(y). Moreover, Algo. 8 runs in time O(pK log K maxv∈V |Yv|2) and uses space O(pK maxv∈V |Yv|).

53

Algorithm 8 Top-K max-product algorithm

1: Input: Augmented score function ψ(·, ·; w) deﬁned on tree structured graph G with root r ∈ V, and

integer K > 0.

2: Initialize: Let V be a list of nodes from V\{r} arranged in increasing order of height.
3: for v in V and k = 1, · · · , K do
4:

if v is a leaf then

m(k)

v (yρ(v)) ← max(k)

yv∈Yv

(cid:8)ψv(yv) + ψv,ρ(v)(yv, yρ(v))(cid:9) for each yρ(v) ∈ Yρ(v).

else

Assign for each yρ(v) ∈ Yρ(v),

m(k)

v (yρ(v)) ← max(k)

(cid:40)

ψv(yv) + ψv,ρ(v)(yv, yρ(v))
v(cid:48)∈C(v) m(lv(cid:48) )

+ (cid:80)

(yv)

v(cid:48)

yv ∈ Yv and
lv(cid:48) ∈ [K] for v(cid:48) ∈ C(v)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)

.

(66)

v(cid:48) (yρ(v)) for v(cid:48) ∈ C(v) store the maximizing assignment of yv and l(cid:48)

v from

5:

6:

7:

8:

Let δ(k)
v (yρ(v)) and κ(k)
above for each yρ(v) ∈ Yρ(v).

end if
9:
10: end for
11: For k = 1, · · · , K, set

ψ(k) ← max(k)

ψr(yr) +

(cid:26)

(cid:88)

m(lv(cid:48) )
v(cid:48)

(yr)

v(cid:48)∈C(r)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

yr ∈ Yr and lv(cid:48) ∈ [K] for v(cid:48) ∈ C(r)

(cid:27)

and assign maximizing assignments of yr, lv(cid:48) above respectively to y(k)

r

and l(k)
v(cid:48)

for v(cid:48) ∈ C(r).

12: for v in reverse(V ) and k = 1, · · · , K do
v )

(cid:0)y(k)
ρ(v)
(cid:0)y(k)
ρ(v)

(cid:1).
(cid:1) for all v(cid:48) ∈ C(v).

13:

Set y(k)
Set l(k)

v

v ← δ(l(k)
v(cid:48) = κ(l(k)
(cid:110)

v )

v(cid:48)

14:
15: end for
16: return

ψ(k), y(k) := (y(k)

1 , · · · , y(k)
p )

(cid:111)K

.

k=1

54

(67)

(68)

(69)

Deﬁne ψτ (v) as follows: if v is a leaf, yτ (v) = (yv) and ψτ (v)(yτ (v)) := ψv(yv). For a non-leaf v, deﬁne
recursively

ψτ (v)(yτ (v)) := ψv(yv) +

(cid:2)ψv,v(cid:48)(yv, yv(cid:48)) + ψτ (v(cid:48))(yτ (v(cid:48)))(cid:3) .

(cid:88)

v(cid:48)∈C(v)

We will need some identities about choosing the kth largest element from a ﬁnite collection. For ﬁnite sets
S1, · · · , Sn and functions fj : Sj → R, h : S1 × S2 → R, we have,












max(k)
u1∈S1,··· ,un∈Sn

n
(cid:88)



j=1

fj(uj)

= max(k)
l1,··· ,ln∈[k]



max(lj )
uj ∈Sj

fj(uj)

,



n
(cid:88)



j=1

(cid:26)

max(k)
u1∈S1,u2∈S2

{f1(u1) + h(u1, u2)} = max(k)
u1∈S1,l∈[k]

f1(u1) + max(l)

h(u1, u2)

.

u2∈S2

(cid:27)

The identities above state that for a sum to take its kth largest value, each component of the sum must take
one of its k largest values. Indeed, if one of the components of the sum took its lth largest value for l > k,
replacing it with any of the k largest values cannot decrease the value of the sum. Eq. (69) is a generalized
version of Bellman’s principle of optimality (see Bellman [1957, Chap. III.3.] or Bertsekas [1995, Vol. I,
Chap. 1]).

For the rest of the proof, yτ (v)\yv is used as shorthand for {yv(cid:48) | v(cid:48) ∈ τ (v)\{v}}. Moreover, maxyτ (v)
represents maximization over yτ (v) ∈ ×v(cid:48)∈τ (v) Yv(cid:48). Likewise for maxyτ (v)\yv . Now, we shall show by
induction that for all v ∈ V, yv ∈ Yv and k = 1, · · · , K,

max(k)
yτ (v)\yv

ψτ (v)(yτ (v)) = ψv(yv) + max(k)

(cid:26) (cid:88)

v(cid:48)∈C(v)

m(lv(cid:48) )
v(cid:48)

(cid:12)
(cid:12)
lv(cid:48) ∈ [K] for v(cid:48) ∈ C(v)
(yv(cid:48))
(cid:12)
(cid:12)

(cid:27)

.

(70)

The induction is based on the height of a node. The statement is clearly true for a leaf v since C(v) = ∅.
Suppose (70) holds for all nodes of height ≤ h. For a node v of height h + 1, we observe that τ (v)\v can
be partitioned into {τ (v(cid:48)) for v(cid:48) ∈ C(v)} to get,

max(k)
yτ (v)\yv

ψτ (v)(yτ (v)) − ψv(yv)

(67)
= max(k)
yτ (v)\yv

(cid:26) (cid:88)

v(cid:48)∈C(v)

ψv,v(cid:48)(yv, yv(cid:48)) + ψτ (v(cid:48))(yτ (v(cid:48)))

(cid:27)

(cid:27)

{ψv,v(cid:48)(yv, yv(cid:48)) + ψτ (v(cid:48))(yτ (v(cid:48)))}

lv(cid:48) ∈ [K] for v(cid:48) ∈ C(v)

.

(71)

(68)
= max(k)

(cid:26) (cid:88)

v(cid:48)∈C(v)

max(lv(cid:48) )
yτ (v(cid:48))
(cid:124)

(cid:123)(cid:122)
=:Tv(cid:48) (yv)

Let us analyze the term in the underbrace, Tv(cid:48)(yv). We successively deduce, with the argument l in the
maximization below taking values in {1, · · · , K},

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:125)

(cid:27)

Tv(cid:48)(yv)

(69)
= max(lv(cid:48) )
yv(cid:48) ,l

ψv,v(cid:48)(yv, yv(cid:48)) + max(l)
yτ (v(cid:48))\yv(cid:48)

ψτ (v(cid:48))(yτ (v(cid:48)))

ψv(cid:48)(yv(cid:48)) + ψv,v(cid:48)(yv, yv(cid:48))+

(cid:27)
(yv(cid:48)) | lv(cid:48)(cid:48) ∈ [K] for v(cid:48)(cid:48) ∈ C(v(cid:48))(cid:9)

max(l) (cid:8) (cid:80)

v(cid:48)(cid:48)
(cid:26)ψv(cid:48)(yv(cid:48)) + ψv(cid:48),v(yv(cid:48), yv)
v(cid:48)(cid:48)∈C(v(cid:48)) m(lv(cid:48)(cid:48) )
(yv(cid:48))

v(cid:48)(cid:48)∈C(v(cid:48)) m(lv(cid:48)(cid:48) )
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ (cid:80)

v(cid:48)(cid:48)

yv(cid:48) ∈ Yv(cid:48) and
lv(cid:48)(cid:48) ∈ [K] for v(cid:48)(cid:48) ∈ C(v)

(cid:27)

(cid:26)

(cid:26)

(70)
= max(lv(cid:48) )
yv(cid:48) ,l

(69)
= max(lv(cid:48) )

(66)
= m(lv(cid:48) )
v(cid:48)

(yv) .

55

Here, the penultimate step followed from applying in reverse the identity (69) with u1, u2 being by yv(cid:48), {lv(cid:48)(cid:48) for v(cid:48)(cid:48) ∈
C(v(cid:48))} respectively, and f1 and h respectively being ψv(cid:48)(yv(cid:48)) + ψv(cid:48),v(yv(cid:48), yv) and (cid:80)
(yv(cid:48)). Plug-
ging this into (71) completes the induction argument. To complete the proof, we repeat the same argument
over the root as follows. We note that τ (r) is the entire tree G. Therefore, yτ (r) = y and ψτ (r) = ψ. We
now apply the identity (69) with u1 and u2 being yr and yτ (r)\r respectively and f1 ≡ 0 to get

v(cid:48)(cid:48) m(lv(cid:48)(cid:48) )
v(cid:48)(cid:48)

max(k)
y∈Y

ψ(y)

(69)
= max(k)
yr,l

max(l)
y\yr

ψ(y)

= max(k)
yr,l

max(l)
yτ (r)\yr

ψτ (r)(yτ (r))

(cid:27)

(cid:40)

(cid:41)

(cid:26)

(cid:26)

(cid:26)

(70)
= max(k)
yr,l

(69)
= max(k)

= ψ(k) ,

ψr(yr) + max(l) (cid:8) (cid:80)

v∈C(r) m(lv)

v

(yr) | lv ∈ [K] for v ∈ C(r)(cid:9)

(cid:27)

ψr(yr)+
v∈C(r) m(lv)

v

(cid:80)

(yr)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

yr ∈ Yr and
lv ∈ [K] for v ∈ C(r)

(cid:27)

where the last equality follows from Line 11 of Algo. 8.

The algorithm requires storage of m(k)

v , an array of size maxv∈V |Yv| for each k = 1, · · · , K, and v ∈ V.
The backpointers δ, κ are of the same size. This adds up to a total storage of O(pK maxv|Yv|). To bound
the running time, consider Line 7 of Algo. 8. For a ﬁxed v(cid:48) ∈ C(v), the computation

max(k)
yv,lv(cid:48)

(cid:110)
ψv(yv) + ψv,ρ(v)(yv, yρ(v)) + m(lv(cid:48) )

(yv)

(cid:111)

v(cid:48)

for k = 1, · · · , K takes time O(K log K maxv|Yv|). This operation is repeated for each yv ∈ Yv and once
for every (v, v(cid:48)) ∈ E. Since |E| = p − 1, the total running time is O(pK log K maxv|Yv|2).

B.2 Proof of Correctness of Entropy Smoothing of Max-Product

Next, we consider entropy smoothing.

Proposition 42. Given an augmented score function ψ(·, ·; w) deﬁned on tree structured graph G and
µ > 0 as input, Algo. 3 correctly computes f−µH (w) and ∇f−µH (w). Furthermore, Algo. 3 runs in
time O(p maxv∈V |Yv|2) and requires space O(p maxv∈V |Yv|).

Proof. The correctness of the function value f−µH follows from the bijection f−µH (w) = µ Aψ/µ(w) (cf.
Prop. 9), where Thm. 43 shows correctness of Aψ/µ. To show the correctness of the gradient, deﬁne the
probability distribution Pψ,µ as the probability distribution from Lemma 7(ii) and Pψ,µ,v, Pψ,µ,v,v(cid:48) as its
node and edge marginal probabilities respectively as

Pψ,µ(y; w) =

exp

(cid:16) 1

(cid:17)
µ ψ(y; w)

(cid:16) 1

y(cid:48)∈Y exp

µ ψ(y(cid:48); w)

(cid:17) ,

Pψ,µ,v(yv; w) =

Pψ,µ(y; w)

for yv ∈ Yv, v ∈ V , and,

Pψ,µ,v,v(cid:48)(yv, yv(cid:48); w) =

Pψ,µ(y; w)

for yv ∈ Yv, yv(cid:48) ∈ Yv(cid:48), (v, v(cid:48)) ∈ E .

(cid:80)

(cid:88)

y∈Y :
yv=yv
(cid:88)

y∈Y:
yv=yv,
yv(cid:48) =yv(cid:48)

56

Thm. 43 again shows that Algo. 9 correctly produces marginals Pψ,µ,v and Pψ,µ,v,v(cid:48). We now start with
Lemma 7(ii) and invoke (17) to get

∇f−µH (w) =

Pψ,µ(y; w)∇ψ(y; w)

(cid:88)

y∈Y

(cid:88)

y∈Y
(cid:88)

=

=

=

(cid:88)

v∈V
(cid:88)

y∈Y
(cid:88)

(cid:88)





(cid:88)

v∈V

Pψ,µ(y; w)

∇ψv(yv; w) +

∇ψv,v(cid:48)(yv, yv(cid:48); w)

 ,



Pψ,µ(y; w)∇ψv(yv; w) +

Pψ,µ(y; w)∇ψv,v(cid:48)(yv, yv(cid:48); w)

(cid:88)

(v,v(cid:48))∈E
(cid:88)

(cid:88)

(v,v(cid:48))∈E

y∈Y

Pψ,µ(y; w)∇ψv(yv; w)

v∈V

yv∈Yv

y∈Y : yv=yv
(cid:88)
(cid:88)

+

(cid:88)

(cid:88)

(v,v(cid:48))∈E

yv∈Yv

yv(cid:48) ∈Yv(cid:48)

y∈Y :

yv=yv
yv(cid:48) =yv(cid:48)

Pψ,µ,v(yv; w)∇ψv(yv; w)

(cid:88)

(cid:88)

=

v∈V

yv∈Yv

(cid:88)

(cid:88)

(cid:88)

+

(v,v(cid:48))∈E

yv∈Yv

yv(cid:48) ∈Yv(cid:48)

Pψ,µ,v,v(cid:48)(yv, yv(cid:48); w)∇ψv,v(cid:48)(yv, yv(cid:48); w) .

Pψ,µ(y; w)∇ψv,v(cid:48)(yv, yv(cid:48); w)

Here, the penultimate equality followed from breaking the sum over y ∈ Y into an outer sum that sums over
every yv ∈ Yv and an inner sum over y ∈ Y : yv = yv, and likewise for the edges. The last equality above
followed from the deﬁnitions of the marginals. Therefore, Line 3 of Algo. 3 correctly computes the gradient.
The storage complexity of the algorithm is O(p maxv|Yv|) provided that the edge marginals Pψ,µ,v,v(cid:48) are
computed on the ﬂy as needed. The time overhead of Algo. 3 after Algo. 9 is O(p maxv|Yv|2), by noting
that each edge marginal can be computed in constant time (Remark 44).

Given below is the guarantee of the sum-product algorithm (Algo. 9). See, for instance, Koller and

Friedman [2009, Ch. 10] for a proof.

Theorem 43. Consider an augmented score function ψ deﬁned over a tree structured graphical model G.
Then, the output of Algo. 9 satisﬁes

Pv(yv) =

exp(ψ(y) − A)

for all yv ∈ Yv, v ∈ V, and,

Pv,v(cid:48)(yv, yv(cid:48)) =

exp(ψ(y) − A)

for all yv ∈ Yv, yv(cid:48) ∈ Yv(cid:48), (v, v(cid:48)) ∈ E.

A = log

exp(ψ(y)) ,

(cid:88)

y∈Y
(cid:88)

y∈Y : yv=yv
(cid:88)

y∈Y :

yv=yv,
yv(cid:48) =yv(cid:48)

Furthermore, Algo. 9 runs in time O(p maxv∈V |Yv|2) and requires an intermediate storage of O(p maxv∈V |Yv|).

57

Algorithm 9 Sum-product algorithm

1: Procedure: SUMPRODUCT
2: Input: Augmented score function ψ deﬁned on tree structured graph G with root r ∈ V.
3: Notation: Let N (v) = C(v) ∪ {ρ(v)} denote all the neighbors of v ∈ V if the orientation of the edges

were ignored.

4: Initialize: Let V be a list of nodes from V arranged in increasing order of height.
5: for v in V \{r} do
6:

Set for each yρ(v) ∈ Yρ(v):

mv→ρ(v)(yρ(v)) ←

(cid:88)

yv∈Yv


exp (cid:0)ψv(yv) + ψv,ρ(v)(yv, yρ(v))(cid:1) (cid:89)

mv(cid:48)→v(yv)

 .

v(cid:48)∈C(v)

exp (ψr(yr)) (cid:81)

(cid:105)
v(cid:48)∈C(r) mv(cid:48)→r(yr)

.

(cid:104)

7: end for
8: A ← log (cid:80)
9: for v in reverse(V ) do
for v(cid:48) ∈ C(v) do
10:

yr∈Yr

11:

Set for each yv(cid:48) ∈ Yv(cid:48):

mv→v(cid:48)(yv(cid:48)) =

(cid:88)

yv∈Yv


exp (cid:0)ψv(yv) + ψv(cid:48),v(yv(cid:48), yv)(cid:1) (cid:89)

mv(cid:48)(cid:48)→v(yv)

 .

v(cid:48)(cid:48)∈N (v)\{v(cid:48)}

end for

12:
13: end for
14: for v in V do
15:
16: end for
17: for (v, v(cid:48)) in E do
18:

For every pair (yv, yv(cid:48)) ∈ Yv × Yv(cid:48), set

Set Pv(yv) ← exp (ψv(yv) − A) (cid:81)

v(cid:48)(cid:48)∈N (v) mv(cid:48)(cid:48)→v(yv) for every yv ∈ Yv.

Pv,v(cid:48)(yv, yv(cid:48)) ← exp (cid:0)ψv(yv) + ψv(cid:48)(yv(cid:48)) + ψv,v(cid:48)(yv, yv(cid:48)) − A(cid:1)

(cid:89)

mv(cid:48)(cid:48)→v(yv)

mv(cid:48)(cid:48)→v(cid:48)(yv(cid:48)) .

(cid:89)

v(cid:48)(cid:48)∈N (v)\{v(cid:48)}

v(cid:48)(cid:48)∈N (v(cid:48))\{v}

19: end for
20: return A, {Pv for v ∈ V}, {Pv,v(cid:48) for (v, v(cid:48)) ∈ E}.





58

Remark 44. Line 18 of Algo. 9 can be implemented in constant time by reusing the node marginals Pv and
messages mv→v(cid:48), mv(cid:48)→v as

Pv,v(cid:48)(yv, yv(cid:48)) =

Pv(yv)Pv(cid:48)(yv(cid:48)) exp(ψv,v(cid:48)(yv, yv(cid:48)) + A)
mv(cid:48)→v(yv)mv→v(cid:48)(yv(cid:48))

.

C Inference Oracles in Loopy Graphs

This section presents the missing details and recalls from literature the relevant algorithms and results re-
quired in Sec. 4.2. First, we review the BMMF algorithm of Yanover and Weiss [2004], followed by graph
cut inference and graph matching inference.

We now recall and prove the correctness of the decoding scheme (21) for completeness. The result is

due to Pearl [1988], Dawid [1992].

Theorem 45. Consider an unambiguous augmented score function ψ , that is, ψ(y(cid:48); w) (cid:54)= ψ(y(cid:48)(cid:48); w)
for all distinct y(cid:48), y(cid:48)(cid:48) ∈ Y. Then, the result (cid:98)y of the decoding (cid:98)yv = arg maxj∈Yv ψv;j satisﬁes (cid:98)y =
arg maxy∈Y ψ(y).

Proof. Suppose for the sake of contradiction that (cid:98)y (cid:54)= y∗ := arg maxy∈Y ψ(y). Let v ∈ V be such that
yv = j and y∗
v = j(cid:48) where j (cid:54)= j(cid:48). By the fact that y∗ has the highest augmented score and unambiguity, we
get that

max
y∈Y,yv=j(cid:48)

ψ(y) = ψ(y∗) > ψ((cid:98)y) = max

y∈Y,yv=j

ψ(y) ,

which contradicts the deﬁnition of (cid:98)yv.

C.1 Review of Best Max-Marginal First

If one has access to an algorithm M that can compute max-marginals, the top-K oracle is easily imple-
mented via the Best Max Marginal First (BMMF) algorithm of Yanover and Weiss [2004], which is recalled
in Algo. 10. This algorithm requires computations of two sets of max-marginals per iteration, where a set
of max-marginals refers to max-marginals for all variables yv in y.

Details The algorithm runs by maintaining a partitioning of the search space Y and a table ϕ(k)(v, j)
that stores the best score in partition k (deﬁned by constraints C(k)) subject to the additional constraint that
yv = j. In iteration k, the algorithm looks at the k − 1 existing partitions and picks the best partition
sk (Line 9). This partition is further divided into two parts: the max-marginals in the promising partition
(corresponding to yvk = jk) are computed (Line 11) and decoded (Line 12) to yield kth best scoring y(k).
The scores of the less promising partition are updated via a second round of max-marginal computations
(Line 14).

Guarantee The following theorem shows that Algo. 10 provably implements the top-K oracle as long
as the max-marginals can be computed exactly under the assumption of unambiguity. With approximate
max-marginals however, Algo. 10 comes with no guarantees.

59

Algorithm 10 Best Max Marginal First (BMMF)

1: Input: Augmented score function ψ, parameters w, non-negative integer K, algorithm M to compute

max-marginals of ψ.

2: Initialization: C(1) = ∅ and U (2) = ∅.
3: for v ∈ [p] do
4:

5:
6: end for
7: for k = 2, · · · , K do

For j ∈ Yv, set ϕ(1)(v; j) = max{ψ(y; w) | y ∈ Y s.t. yv = j} using M.
Set y(1)

v = arg maxj∈Yv ϕ(1)(v, j).

8:

9:

10:

11:

12:

13:

14:

(cid:110)
(v, j, s) ∈ [p] × Yv × [k − 1] (cid:12)

Deﬁne search space S (k) =
Find indices (vk, jk, sk) = arg max(v,j,s)∈S(k) ϕ(s)(v, j) and set constraints C(k) = C(sk) ∪ {yvk =
jk}.
for v ∈ [p] do

(cid:54)= j, and (v, j, s) /∈ U (t)(cid:111)
.

(cid:12) y(s)

v

For each j ∈ Yv, use M to set ϕ(k)(v, j) = max (cid:8)ψ(y; w) | y ∈ Y s.t. constraints C(k) hold and yv = j(cid:9).

Set y(k)

v = arg maxj∈Yv ϕ(k)(v, j).

end for
Update U (k+1) = U (k) ∪ {(vk, jk, sk)} and C(sk) = C(sk) ∪ {yvk (cid:54)= jk} and the max-marginal table
ϕ(sk)(v, j) = maxy∈Y,C(sk ),yv=j ψ(y; w) using M.

15: end for
16: return (cid:8)(cid:0)ψ(y(k); w), y(k)(cid:1)(cid:9)K

k=1.

Theorem 46 (Yanover and Weiss [2004]). Suppose the score function ψ is unambiguous, that is, ψ(y(cid:48); w) (cid:54)=
ψ(y(cid:48)(cid:48); w) for all distinct y(cid:48), y(cid:48)(cid:48) ∈ Y. Given an algorithm M that can compute the max-marginals of ψ
exactly, Algo. 10 makes at most 2K calls to M and its output satisﬁes ψ(yk; w) = max(k)
y∈Y ψ(y; w).
Thus, the BMMF algorithm followed by a projection onto the simplex (Algo. 6 in Appendix A) is a correct
implementation of the top-K oracle. It makes 2K calls to M.

Constrained Max-Marginals The algorithm requires computation of max-marginals subject to constraints
of the form yv ∈ Yv for some set Yv ⊆ Yv. This is accomplished by redeﬁning for a constraint yv ∈ Yv:

(cid:40)

ψ(y) =

if yv ∈ Yv
ψ(y),
−∞, otherwise

.

C.2 Max-Marginals Using Graph Cuts

This section recalls a simple procedure to compute max-marginals using graph cuts. Such a construction

was used, for instance, by Kolmogorov and Zabin [2004].

Notation In the literature on graph cut inference, it is customary to work with the energy function, which
is deﬁned as the negative of the augmented score −ψ. For this section, we also assume that the labels are
binary, i.e., Yv = {0, 1} for each v ∈ [p]. Recall the decomposition (17) of the augmented score function

60

Algorithm 11 Max-marginal computation via Graph Cuts
1: Input: Augmented score function ψ(·, ·; w) with Y = {0, 1}p, constraints C of the form yv = b for

b ∈ {0, 1}.

Add to E(cid:48) the (edge, cost) pairs (s → yv, θv;0) and (yv → t, θv;1).

2: Using artiﬁcial source s and sink t, set V (cid:48) = V ∪ {s, t} and E(cid:48) = ∅.
3: for v ∈ [p] do
4:
5: end for
6: for v, v(cid:48) ∈ R such that v < v(cid:48) do
7:

Add to E(cid:48) the (edge, cost) pairs (s → yv, θvv(cid:48);00), (yv(cid:48) → t, θvv(cid:48);11), (yv → yv(cid:48), θvv(cid:48);10), (yv(cid:48) →
yv, θvv(cid:48);01 − θvv(cid:48);00 − θvv(cid:48);11).

Add to E(cid:48) the edge yv → t if b = 0 or edge s → yv if b = 1 with cost +∞.

8: end for
9: for constraint yv = b in C do
10:
11: end for
12: Create graph G(cid:48) = (V (cid:48), E(cid:48)), where parallel edges are merged by adding weights.
13: Compute minimum cost s, t-cut of G(cid:48). Let C be its cost.
14: Create (cid:98)y ∈ {0, 1}p as follows: for each v ∈ V, set (cid:98)yv = 0 if the edge s → v is cut. Else (cid:98)yv = 1.
15: return −C, (cid:98)y.

over nodes and edges. Deﬁne a reparameterization

θv;z(w) = −ψv(z; w) for v ∈ V, z ∈ {0, 1}
if (v, v(cid:48)) ∈ E
θvv(cid:48);z,z(cid:48)(w) = −ψv,v(cid:48)(z, z(cid:48); w) ,

for (v, v(cid:48)) ∈ E, (z, z(cid:48)) ∈ {0, 1}2 .

We then get

−ψ(y) =

θv;z I(yv = z) +

θvv(cid:48);zz(cid:48) I(yv = z) I(yv(cid:48) = z(cid:48)) I((v, v(cid:48)) ∈ E) ,

p
(cid:88)

(cid:88)

v=1

z∈{0,1}

p
(cid:88)

p
(cid:88)

(cid:88)

v=1

v(cid:48)=i+1

z,z(cid:48)∈{0,1}

where we dropped the dependence on w for simplicity. We require the energies to be submodular, i.e., for
every v, v(cid:48) ∈ [p], we have that

θvv(cid:48);00 + θvv(cid:48);11 ≤ θvv(cid:48);01 + θvv(cid:48);10 .

(72)

Also, assume without loss of generality that θv;z, θvv(cid:48);zz(cid:48) are non-negative [Kolmogorov and Zabin, 2004].

Algorithm and Correctness Algo. 11 shows how to compute the max-marginal relative to a single vari-
able yv. The next theorem shows its correctness.

Theorem 47 (Kolmogorov and Zabin [2004]). Given a binary pairwise graphical model with augmented
score function ψ which satisﬁes (72), and a set of constraints C, Algo. 11 returns maxy∈YC ψ(y; w), where
YC denotes the subset of Y that satisﬁes constraints C. Moreover, Algo. 11 requires one maximum ﬂow
computation.

61

C.3 Max-Marginals Using Graph Matchings

The alignment problem that we consider in this section is as follows: given two sets V, V (cid:48), both of equal size
(for simplicity), and a weight function ϕ : V × V (cid:48) → R, the task is to ﬁnd a map σ : V → V (cid:48) so that each
v ∈ V is mapped to a unique z ∈ V (cid:48) and the total weight (cid:80)
v∈V ϕ(v, σ(v)) is maximized. For example, V
and V (cid:48) might represent two natural language sentences and this task is to align the two sentences.

Graphical Model This problem is framed as a graphical model as follows. Suppose V and V (cid:48) are of size
p. Deﬁne y = (y1, · · · , yp) so that yv denotes σ(v). The graph G = (V, E) is constructed as the fully
connected graph over V = {1, · · · , p}. The range Yv of each yv is simply V (cid:48) in the unconstrained case.
Note that when considering constrained max-marginal computations, Yv might be subset of V (cid:48). The score
function ψ is deﬁned as node and edge potentials as in Eq. (17). Again, we suppress dependence of ψ on w
for simplicity. Deﬁne unary and pairwise scores as

ψv(yv) = ϕ(v, yv)

and ψv,v(cid:48)(yv, yv(cid:48)) =

(cid:40)

0, if yv (cid:54)= yv(cid:48)
−∞, otherwise

.

Max Oracle The max oracle with ψ deﬁned as above, or equivalently, the inference problem (3) (cf.
Lemma 7(i)) can be cast as a maximum weight bipartite matching, see e.g., Taskar et al. [2005]. Deﬁne a
fully connected bipartite graph G = (V ∪ V (cid:48), E) with partitions V, V (cid:48), and directed edges from each v ∈ V
to each vertex z ∈ V (cid:48) with weight ϕ(v, z). The maximum weight bipartite matching in this graph G gives
the mapping σ, and thus implements the max oracle. It can be written as the following linear program:

max
{θv,z for (v,z)∈E}

(cid:88)

ϕ(v, z)θv,z ,

s.t.

(v,z)∈E
0 ≤ θv,z ≤ 1 ∀(v, z) ∈ V × V (cid:48)
(cid:88)

θv,z ≤ 1 ∀z ∈ V (cid:48)

v∈V
(cid:88)

z∈V (cid:48)

θv,z ≤ 1 ∀v ∈ V .

Max-Marginal For the graphical model deﬁned above, the max-marginal ψ¯v;¯z is the constrained maxi-
mum weight matching in the graph G deﬁned above subject to the constraint that ¯v is mapped to ¯z. The
linear program above can be modiﬁed to include the constraint θ¯v,¯z = 1:

max
{θv,z for (v,z)∈E}

(cid:88)

ϕ(v, z)θv,z ,

s.t.

(v,z)∈E
0 ≤ θv,z ≤ 1 ∀(v, z) ∈ V × V (cid:48)
(cid:88)

θv,z ≤ 1 ∀z ∈ V (cid:48)

(73)

θv,z ≤ 1 ∀v ∈ V

v∈V
(cid:88)

z∈V (cid:48)
θ¯v,¯z = 1 .

62

Algorithm 12 Max marginal computation via Graph matchings
1: Input: Directed bipartite graph G = (V ∪ V (cid:48), E), weights ϕ : V × V (cid:48) → R.
2: Find a maximum weight bipartite matching σ∗ in the graph G. Let the maximum weight be ψ∗.
3: Deﬁne a weighted residual bipartite graph (cid:98)G = (V ∪ V (cid:48), (cid:98)E), where the set (cid:98)E is populated as follows:
for (v, z) ∈ E, add an edge (v, z) to (cid:98)E with weight 1 − I(σ∗(v) = z), add (z, v) to (cid:98)E with weights
− I(σ∗(v) = z).

4: Find the maximum weight path from every vertex z ∈ V (cid:48) to every vertex v ∈ V and denote this by

∆(z, v).

5: Assign the max-marginals ψv;z = ψ∗ + I(σ∗(v) (cid:54)= z) (∆(z, v) + ϕ(v, z)) for all (v, z) ∈ V × V (cid:48).
6: return Max-marginals ψv;z for all (v, z) ∈ V × V (cid:48).

Algorithm to Compute Max-Marginals Algo. 12, which shows how to compute max-marginals is due
to Duchi et al. [2006]. Its running time complexity is as follows: the initial maximum weight matching
computation takes O(p3) via computation of a maximum ﬂow [Schrijver, 2003, Ch. 10]. Line 4 of Algo. 12
can be performed by the all-pairs shortest paths algorithm [Schrijver, 2003, Ch. 8.4] in time O(p3). Its
correctness is shown by the following theorem:

Theorem 48 (Duchi et al. [2006]). Given a directed bipartite graph G and weights ϕ : V × V (cid:48) → R, the
output ψv;z from Algo. 12 are valid max-marginals, i.e., ψv;z coincides with the optimal value of the linear
program (73). Moreover, Algo. 12 runs in time O(p3) where p = |V | = |V (cid:48)|.

C.4 Proof of Proposition 14

Proposition 14. Consider as inputs an augmented score function ψ(·, ·; w), an integer K > 0 and a smooth-
ing parameter µ > 0. Further, suppose that ψ is unambiguous, that is, ψ(y(cid:48); w) (cid:54)= ψ(y(cid:48)(cid:48); w) for all distinct
y(cid:48), y(cid:48)(cid:48) ∈ Y. Consider one of the two settings:

(A)

the output space Yv = {0, 1} for each v ∈ V, and the function −ψ is submodular (see Appendix C.2
and, in particular, (72) for the precise deﬁnition), or,

(B)

the augmented score corresponds to an alignment task where the inference problem (3) corresponds to
a maximum weight bipartite matching (see Appendix C.3 for a precise deﬁnition).

In these cases, we have the following:

(i) The max oracle can be implemented at a computational complexity of O(p) minimum cut computations

in Case (A), and in time O(p3) in Case (B).

(ii) The top-K oracle can be implemented at a computational complexity of O(pK) minimum cut compu-

tations in Case (A), and in time O(p3K) in Case (B).

(iii) The exp oracle is #P-complete in both cases.

Proof. A set of max-marginals can be computed by an algorithm M deﬁned as follows:

• In Case (A), invoke Algo. 11 a total of 2p times, with yv = 0, and yv = 1 for each v ∈ V. This takes

a total of 2p min-cut computations.

63

Algorithm 13 Top-K best-ﬁrst branch and bound search

1: Input: Augmented score function ψ(·, ·; w), integer K > 0, search space Y, upper bound (cid:98)ψ, split

2: Initialization: Initialize priority queue with single entry Y with priority (cid:98)ψ(Y; w), and solution set S as

strategy.

the empty list.
3: while |S| < K do
4:

Pop (cid:98)Y from the priority queue.
if (cid:98)Y = {(cid:98)y} is a singleton then
Append ((cid:98)y, ψ((cid:98)y; w)) to S.

else

5:

6:

7:

8:

9:

end if

10:
11: end while
12: return S.

Y1, Y2 ← split( (cid:98)Y).
Add Y1 with priority (cid:98)ψ(Y1; w) and Y2 with priority (cid:98)ψ(Y2; w) to the priority queue.

• In Case (B), M is simply Algo. 12, which takes time O(p3).

The max oracle can then be implmented by the decoding in Eq. (21), whose correctness is guaranteed by
Thm. 45. The top-K oracle is implemented by invoking the BMMF algorithm with M deﬁned above,
followed by a projection onto the simplex (Algo. 6 in Appendix A) and its correctness is guaranteed by
Thm. 46. Lastly, the result of exp oracle follows from Jerrum and Sinclair [1993, Thm. 15] in conjunction
with Prop. 9.

C.5

Inference using branch and bound search

Algo. 13 with the input K = 1 is the standard best-ﬁrst branch and bound search algorithm. Effectively, the
top-K oracle is implemented by simply continuing the search procedure until K outputs have been produced
- compare Algo. 13 with inputs K = 1 and K > 1. We now prove the correctness guarantee.

Proposition 15. Consider an augmented score function ψ(·, ·, w), an integer K > 0 and a smoothing
parameter µ > 0. Suppose the upper bound function (cid:98)ψ(·, ·; w) : X × 2Y → R satisﬁes the following
properties:

(a) (cid:98)ψ( (cid:98)Y; w) is ﬁnite for every (cid:98)Y ⊆ Y,

(b) (cid:98)ψ( (cid:98)Y; w) ≥ maxy∈ (cid:98)Y ψ(y; w) for all (cid:98)Y ⊆ Y, and,

(c) (cid:98)ψ({y}; w) = ψ(y; w) for every y ∈ Y.

Then, we have the following:

(i) Algo. 13 with K = 1 is a valid implementation of the max oracle.

(ii) Algo. 13 followed by a projection onto the simplex (Algo. 6 in Appendix A) is a valid implementation of

the top-K oracle.

64

Proof. Suppose at some point during the execution of the algorithm, we have a (cid:98)Y = {(cid:98)y} on Line 5 and that
|S| = k for some 0 ≤ k < K. From the properties of the quality upper bound (cid:98)ψ, and using the fact that {(cid:98)y}
had the highest priority in the priority queue (denoted by (∗)), we get,

ψ((cid:98)y; w) = (cid:98)ψ({(cid:98)y}; w)
(∗)
≥ max
Y ∈P
≥ max
Y ∈P

max
y∈Y

(cid:98)ψ(Y ; w)

ψ(y; w)

(#)
= max
y∈Y−S

ψ(y; w) ,

where the equality (#) followed from the fact that any y ∈ Y exits the priority queue only if it is added to
S. This shows that if a (cid:98)y is added to S, it has a score that is no less than that of any y ∈ Y − S. In other
words, Algo. 13 returns the top-K highest scoring y’s.

D The Casimir Algorithm and Non-Convex Extensions: Missing Proofs

This appendix contains missing proofs from Sections 5 and 6. Throughout, we shall assume that ω is ﬁxed
and drop the subscript in Aω, Dω. Moreover, an unqualiﬁed norm (cid:107)·(cid:107) refers to the Euclidean norm (cid:107) · (cid:107)2.

D.1 Behavior of the Sequence (αk)k≥0

Lemma 21. Given a positive, non-decreasing sequence (κk)k≥1 and λ ≥ 0, consider the sequence (αk)k≥0
deﬁned by (27), where α0 ∈ (0, 1) such that α2
0 ≥ λ/(λ + κ1). Then, we have for every k ≥ 1 that
0 < αk ≤ αk−1 and, α2

k ≥ λ/(λ + κk+1) .

Proof. It is clear that (27) always has a positive root, so the update is well deﬁned. Deﬁne sequences
(ck)k≥1, (dk)k≥0 as

ck =

λ + κk
λ + κk+1

,

and dk =

λ
λ + κk+1

.

Therefore, we have that ckdk−1 = dk, 0 < ck ≤ 1 and 0 ≤ dk < 1. With these in hand, the rule for αk can
be written as

−(ckα2

k−1 − dk) +

k−1 − dk)2 + 4ckα2

k−1

αk =

.

(74)

(cid:113)

(ckα2
2

We show by induction that that dk ≤ α2
satisﬁes the hypothesis for some k ≥ 1. Noting that α2
that

k < 1. The base case holds by assumption. Suppose that αk−1
k−1 − dk ≥ 0, we get

k−1 ≥ dk−1 is equivalent to ckα2

(cid:113)

(ckα2

k−1 − dk)2 + 4ckα2

k−1 ≤

(cid:113)

(ckα2

k−1 − dk)2 + 4ckα2
√
ckαk−1 .

k−1 − dk + 2

= ckα2

k−1 + 2(ckα2

k−1 − dk)(2

ckαk−1)

√

(75)

65

We now conclude from (74) and (75) that
−(ckα2

k−1 − dk) + (ckα2
k−1 − dk + 2
2

√

ckαk−1)

αk ≤

√

=

ckαk−1 ≤ αk−1 < 1 ,

(76)

since ck ≤ 1 and αk−1 < 1. To show the other side, we expand out (74) and apply (75) again to get

α2

k − dk =

(ckα2

(ckα2

=

1
2
1
2
1
2
= (ckα2

≥

k−1 − dk)2 + (ckα2
(cid:16)

2 + (ckα2

k−1 − dk)
k−1 − dk) (cid:0)2 + (ckα2
k−1 − dk)(1 −

√

(ckα2

ckαk−1) ≥ 0 .

k−1 − dk) −

(cid:113)

k−1 − dk)

(ckα2

1
2

(ckα2
(cid:113)

k−1 − dk)2 + 4ckα2
(cid:17)

k−1

k−1 − dk) −

(ckα2

k−1 − dk) − (ckα2

k−1 − dk + 2

k−1 − dk)2 + 4ckα2
√

k−1

ckαk−1)(cid:1)

The fact that (αk)k≥0 is a non-increasing sequence follows from (76).

D.2 Proofs of Corollaries to Theorem 16

We rewrite (30) from Theorem 16 as follows:




F (wk) − F ∗ ≤



k
(cid:89)

j=1

1 − αj−1
1 − δj

(cid:16)



+

1
1 − αk











k
(cid:89)

j=1

1 − αj
1 − δj

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2(cid:17)

+ µkDω

γ0
2







k
(cid:88)

k
(cid:89)

j=2

i=j

1 − αi
1 − δi

 (1 + δ1)µ1Dω +

 (µj−1 − (1 − δj)µj) Dω

 ,

(77)



Next, we have proofs of Corollaries 17 to 20.
Corollary 17. Consider the setting of Thm. 16. Let q = λ
k ≥ 1. Choose α0 =
q . Then, we have,
q and, δk =

√

q
√

2−

√

λ+κ . Suppose λ > 0 and µk = µ, κk = κ, for all

F (wk) − F ∗ ≤

√
q
√
q

3 −
1 −

(cid:18)

µD + 2

1 −

(cid:19)k

√

q
2

√

√

(F (w0) − F ∗) .

Proof. Notice that when α0 =
all k, j, 1−αk
1−δj
conditions as

= 1 −

q, we have, αk =

q for all k. Moreover, for our choice of δk, we get, for
√
q
2 . Under this choice of α0, we have, γ0 = λ. So, we get the dependence on initial

∆0 = F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2 ≤ 2(F (w0) − F ∗) ,

λ
2

by λ-strong convexity of F . The last term of (77) is now,



(cid:18)








(cid:124)


µD
√

1 −

q

1 −

√

q
2
(cid:123)(cid:122)
≤1

(cid:19)k−1

√

q
2

(cid:124)

+

(cid:125)

k
(cid:88)

(cid:18)

j=2

1 −

(cid:123)(cid:122)
(∗)
≤ 1

(cid:19)k−j

√

q
2

≤

2µD
√

1 −

,

q










(cid:125)


66

where (∗) holds since

k
(cid:88)

(cid:18)

1 −

j=2

√

q
2

(cid:19)k−j

∞
(cid:88)

(cid:18)

≤

1 −

j=0

(cid:19)j

√

q
2

=

2
√
q

.

Corollary 18. Consider the setting of Thm. 16. Let q = λ
all k ≥ 1. Choose α0 =

q and, the sequences (µk)k≥1 and (δk)k≥1 as

λ+κ , η = 1 −

√

√
q
2 . Suppose λ > 0 and κk = κ, for

µk = µηk/2 ,

and,

δk =

√

q
√

,

q

2 −

where µ > 0 is any constant. Then, we have,

F (wk) − F ∗ ≤ ηk/2

(cid:20)
2 (F (w0) − F ∗) +

µDω
√
q
1 −

(cid:18)

√

2 −

q +

√

(cid:19)(cid:21)

.

q
√
η

1 −

√

q for each k, and 1−δ

Proof. As previously in Corollary 17, notice that under the speciﬁc parameter choices here, we have, γ0 = λ,
√
q
αk =
2 = η. By λ-strong convexity of F and the fact that γ0 = λ, the
contribution of w0 can be upper bounded by 2(F (w0) − F ∗). Now, we plugging these into (77) and
collecting the terms dependent on δk separately, we get,

1−α = 1 −

F (wk) − F ∗ ≤ 2ηk(F (w0) − F ∗)
(cid:124)
(cid:125)

(cid:123)(cid:122)
=:T1

+ µkD
(cid:124)(cid:123)(cid:122)(cid:125)
=:T2

+

1
√
1 −

q









+

ηkµ1D
(cid:124) (cid:123)(cid:122) (cid:125)
=:T3

k
(cid:88)

j=2
(cid:124)

ηk−j+1(µj−1 − µj)D

+

ηk−j+1µjδjD

.

(78)

(cid:123)(cid:122)
=:T4

(cid:125)

(cid:123)(cid:122)
=:T5








(cid:125)

We shall consider each of these terms. Since ηk ≤ ηk/2, we get T1 ≤ 2ηk/2(F (w0) − F ∗) and T3 =
ηkµ1D ≤ ηkµD ≤ ηk/2µD. Moreover, T2 = µkD = ηk/2µD. Next, using 1 −

η ≤ 1 − η =

√

√
q
2 ,

k
(cid:88)

j=1
(cid:124)

√

T4 =

ηk−j+1(µj−1 − µj)D =

ηk−j+1µη(j−1)/2(1 −

η)D

≤

µD

ηk− j−1

2 =

µDη(k+1)/2

ηj/2 ≤

µD

k−2
(cid:88)

j=0

√

q
2

η(k+1)/2
√
η
1 −

k
(cid:88)

j=2

√

q
2

Similarly, using δj =

q/2η, we have,

T5 =

ηk−j+1µηj/2D

=

µD

ηk−j/2 ≤

µD

√

q
2η

√

q
2

k
(cid:88)

j=1

√
q
2

ηk/2
√

1 −

η

.

Plugging these into (78) completes the proof.

67

k
(cid:88)

j=2

µD

ηk/2
√

1 −

η

.

≤

√

k
(cid:88)

j=2
√

q
2
√

q
2

k
(cid:88)

j=1

Corollary 19. Consider the setting of Thm. 16. Suppose µk = µ, κk = κ, for all k ≥ 1 and λ = 0. Choose
α0 =

and δk = 1

(1+k)2 . Then, we have,

5−1
2

√

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2
2

+ µDω

1 +

κ
2

(cid:17)

(cid:18)

12
k + 2

+

30
(k + 2)2

(cid:19)

.

(cid:16)

8
(k + 2)2

Proof. Firstly, note that γ0 = κ α2
0
1−α0

= κ. Now, deﬁne

Ak =

(1 − αi), and, Bk =

(1 − δi) .

k
(cid:89)

i=0

k
(cid:89)

i=1

We have,

Therefore,

k
(cid:89)

(cid:18)

1 −

Bk =

i=1

1
(i + 1)2

(cid:19)

=

k
(cid:89)

i=1

i(i + 2)
(i + 1)2 =

1
2

+

1
2(k + 1)

.

(79)

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:16)

Ak−1
Bk

+

µD
1 − α0





k
(cid:89)

j=1

1 − αj−1
1 − δk

γ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)


+ µD

 (1 + δ1) + µD

≤

(cid:16)

Ak−1
Bk

F (w0) − F ∗ +

γ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)
(cid:125)

+µD

(cid:124)

+

Ak−1
Bk

5
4 µD
1 − α0
(cid:123)(cid:122)
(cid:124)
=:T2

(cid:125)

(cid:123)(cid:122)
=:T1

+ µD

(cid:124)

k
(cid:88)

j=2

Ak−1/Aj−2
Bk/Bj−1

(cid:123)(cid:122)
=:T3

.

δj
1 − αj−1
(cid:125)

k
(cid:88)

k
(cid:89)





j=2

i=j





1 − αi−1
1 − δi

δj
1 − αj−1

From Lemma 52, which analyzes the evolution of (αk) and (Ak), we get that
αk ≤ 2

k+3 for k ≥ 0. Since Bk ≥ 1
2 ,

2

(k+2)2 ≤ Ak−1 ≤ 4

(k+2)2 and

T1 ≤

(cid:16)

8
(k + 2)2

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2(cid:17)

.

γ0
2

Moreover, since α0 ≤ 2/3,

Lastly, we have,

T2 ≤

30
(k + 2)2 .

T3 ≤

k
(cid:88)

j=2

4
(k + 2)2 ×

(j + 1)2
2

× 2

(cid:19)

(cid:18) 1
2

+

1
2j

×

1
(j + 1)2 ×

1
1 − 2/j+2

≤ 2

2
(k + 2)2

k
(cid:88)

j=2

j + 2
j

4

≤

(k + 2)2 (k − 1 + 2 log k) ≤

12
k + 2

,

where we have used the simpliﬁcations (cid:80)k

j=2 1/k ≤ log k and k − 1 + 2 log k ≤ 3k.

68

Corollary 20. Consider the setting of Thm. 16 with λ = 0. Choose α0 =
constants κ, µ, deﬁne sequences (κk)k≥1, (µk)k≥1, (δk)k≥1 as

√

5−1
2

, and for some non-negative

κk = κ k , µk =

and,

δk =

µ
k

1
(k + 1)2 .

Then, for k ≥ 2, we have,

F (wk) − F ∗ ≤

log(k + 1)
k + 1

(cid:0)2(F (w0) − F ∗) + κ(cid:107)w0 − w∗(cid:107)2

2 + 27µDω

(cid:1) .

(80)

For the ﬁrst iteration (i.e., k = 1), this bound is off by a constant factor 1/ log 2.

Proof. Notice that γ0 = κ1

= κ. As in Corollary 19, deﬁne

α2
0
1−α0

From Lemma 53 and (79) respectively, we have for k ≥ 1,

Ak =

(1 − αi) ,

and, Bk =

(1 − δi) .

k
(cid:89)

i=0

1 − 1√
2
k + 1

≤ Ak ≤

and,

≤ Bk ≤ 1 .

1
k + 2

,

k
(cid:89)

i=1

1
2

Now, invoking Theorem 16, we get,

F (wk) − F ∗ ≤

F (w0) − F ∗ +

(cid:16)

Ak−1
Bk

(cid:124)

k
(cid:88)

j=2
(cid:124)

Ak−1/Aj−1
Bk/Bj−1

(cid:123)(cid:122)
=:T4

γ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)
(cid:125)

+

+ µkD
(cid:124)(cid:123)(cid:122)(cid:125)
=:T2

1
1 − α0
(cid:124)

Ak−1
Bk

(cid:123)(cid:122)
=:T3

(cid:123)(cid:122)
=:T1

µ1D(1 + δ1)

+

(cid:125)

(µj−1 − µj)D

+

k
(cid:88)

j=2
(cid:124)

(cid:125)

Ak−1/Aj−1
Bk/Bj−1

δjµjD

.

(cid:123)(cid:122)
=:T5

(cid:125)

(81)

We shall bound each of these terms as follows.

T1 =

(cid:16)

Ak−1
Bk

F (w0) − F ∗ +

F (w0) − F ∗ +

(cid:107)w0 − w∗(cid:107)2(cid:17)

,

κ0
2

(cid:107)w0 − w∗(cid:107)2(cid:17)

=

γ0
2

T2 = µkD =

µD
k

(cid:16)

2
k + 1

,

≤

2µD
k + 1
2
k + 1

T3 =

1
1 − α0
where we used the fact that α0 ≤ 2/3. Next, using (cid:80)k
1 + log(k − 1), we get,

Ak−1
Bk

µ1D(1 + δ1) ≤ 3 ×

× µ ×

15
2
j=2 1/j ≤ 1 + (cid:82) k−1
j=2 1/(j − 1) = 1 + (cid:80)k−1

µD
k + 1

D =

5
4

,

1

dx/x =

T4 =

k
(cid:88)

j=2
√

2
k + 1

·

√

≤ 2

2(

2 + 1)µD

(cid:19)

(cid:18) µ

−

µ
j

j
1 − 1√
j − 1
2
(cid:18) 1 + log(k + 1)
k + 1

(cid:19)

.

√

√

D = 2

2(

2 + 1)

µD
k + 1

k
(cid:88)

j=2

1
j − 1

69

Moreover, from (cid:80)k

j=2 1/(j + 1)2 ≤ (cid:82) k+1

2

dx/x2 ≤ 1/2, it follows that

T5 =

k
(cid:88)

j=2

2
k + 1

·

j
1 − 1√
2

µ
j

·

1

√
(j + 1)2 D = 2

√

2(

2 + 1)

µD
k + 1

k
(cid:88)

j=2

1
(j + 1)2 ≤

√

√

2(

2 + 1)

µD
k + 1

.

Plugging these back into (81), we get

F (wk) − F ∗ ≤

2
k + 1
µD
k + 1

(cid:16)

F (wk) − F ∗ +
(cid:18)

√

κ
2

(cid:107)w0 − w∗(cid:107)2(cid:17)
(cid:19)
√
√

+

√

2 +

+

2(1 +

2)

+ 2

2(1 +

2)µD

15
2

1 + log(k + 1)
k + 1

.

To complete the proof, note that log(k + 1) ≥ 1 for k ≥ 2 and numerically verify that the coefﬁcient of µD
is smaller than 27.

D.3

Inner Loop Complexity Analysis for Casimir

Before proving Prop. 27, the following lemmas will be helpful. First, we present a lemma from Lin et al.
[2018, Lemma 11] about the expected number of iterations a randomized linearly convergent ﬁrst order
methods requires to achieve a certain target accuracy.
Lemma 49. Let M be a linearly convergent algorithm and f ∈ FL,λ. Deﬁne f ∗ = minw∈Rd f (w). Given
a starting point w0 and a target accuracy (cid:15), let (wk)k≥0 be the sequence of iterates generated by M. Deﬁne
T ((cid:15)) = inf {k ≥ 0 | f (wk) − f ∗ ≤ (cid:15)} . We then have,

E[T ((cid:15))] ≤

1
τ (L, λ)

log

(cid:18) 2C(L, λ)
τ (L, λ)(cid:15)

(cid:19)

(f (w0) − f ∗)

+ 1 .

(82)

This next lemma is due to Lin et al. [2018, Lemma 14, Prop. 15].

Lemma 50. Consider Fµω,κ(· ; z) deﬁned in Eq. (25) and let δ ∈ [0, 1). Let (cid:98)F ∗ = minw∈Rd Fµω,κ(w; z)
and (cid:98)w∗ = arg minw∈Rd Fµω,κ(w; z). Further let Fµω(· ; z) be Lµω-smooth. We then have the following:
Lµω + κ
2

Fµω,κ(z; z) − (cid:98)F ∗ ≤

(cid:107)z − (cid:98)w∗(cid:107)2
2 ,

and,

Fµω,κ( (cid:98)w; z) − (cid:98)F ∗ ≤

(cid:107)z − (cid:98)w∗(cid:107)2

2 =⇒ Fµω,κ( (cid:98)w; z) − (cid:98)F ∗ ≤

(cid:107) (cid:98)w − z(cid:107)2
2 .

δκ
2

δκ
8

We now restate and prove Prop. 27.

Proposition 27. Consider Fµω,κ(· ; z) deﬁned in Eq. (25), and a linearly convergent algorithm M with
parameters C, τ . Let δ ∈ [0, 1). Suppose Fµω is Lµω-smooth and λ-strongly convex. Then the expected
number of iterations E[ (cid:98)T ] of M when started at z in order to obtain (cid:98)w ∈ Rd that satisﬁes

Fµω,κ( (cid:98)w; z) − min
w

Fµω,κ(w; z) ≤ δκ

2 (cid:107)w − z(cid:107)2

2

(83)

is upper bounded by

E[ (cid:98)T ] ≤

1
τ (Lµω + κ, λ + κ)

log

(cid:18) 8C(Lµω + κ, λ + κ)
τ (Lµω + κ, λ + κ)

·

Lµω + κ
κδ

(cid:19)

+ 1 .

Proof. In order to invoke Lemma 49, we must appropriately set (cid:15) for (cid:98)w to satisfy (83) and then bound the
ratio (Fµω,κ(z; z) − (cid:98)F ∗)/(cid:15). Firstly, Lemma 50 tells us that choosing (cid:15) = δkκk
2 guarantees that
the (cid:98)w so obtained satisﬁes (83), where (cid:98)w∗ := arg minw∈Rd Fµω,κ(w; z), Therefore, (Fµω,κ(z; z) − (cid:98)F ∗)/(cid:15)
is bounded from above by 4(Lµω + κ)/κδ.

8 (cid:107)zk−1 − (cid:98)w∗(cid:107)2

70

D.4

Information Based Complexity of Casimir-SVRG

Presented below are the proofs of Propositions 29 to 32 from Section 5.3. We use the following values of
C, τ , see e.g., Hofmann et al. [2015].

τ (L, λ) =

8 L

1
λ + n
(cid:32)

L
λ

1
λ + n(cid:1)
(cid:33)

.

≥

8 (cid:0) L
n L
λ
λ + n

8 L

C(L, λ) =

1 +

κ =

(cid:40) A

µn − λ , if A
λ , otherwise

µn > 4λ

,

(cid:32)

(cid:114)

E[N ] ≤ (cid:101)O

n +

(cid:33)

.

AωDωn
λ(cid:15)

K ≤

log

2
√
q

(cid:18) 2∆F0

(cid:19)

(cid:15) − cqµD

,

Proposition 29. Consider the setting of Thm. 16 with λ > 0 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as
the inner solver with parameters: µk = µ = (cid:15)/10Dω, κk = k chosen as

q = λ/(λ + κ), α0 =
q/(2 −
F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

q, and δ =

√

√

√

q). Then, the number of iterations N to obtain w such that

Proof. We use shorthand A := Aω, D := Dω, Lµ = λ + A/µ and ∆F0 = F (w0) − F ∗. Let C, τ be the
linear convergence parameters of SVRG. From Cor. 17, the number of outer iterations K required to obtain
F (wK) − F ∗ ≤ (cid:15) is

where cq = (3 −
√
δk =

q/(2 −

√

√

q),

√

q)/(1 −

q). From Prop. 27, the number Tk of inner iterations for inner loop k is, from

E[Tk] ≤

1
τ (Lµ + κ, λ + κ)
2
τ (Lµ + κ, λ + κ)

log

log

(cid:18) 8C(Lµ + κ, λ + κ)
τ (Lµ + κ, λ + κ)
(cid:18) 8C(Lµ + κ, λ + κ)
τ (Lµ + κ, λ + κ)

·

·

Lµ + κ
κ
Lµ + κ
κ

≤

2 −
√

2 −
√

·

·

√

q
√

q

(cid:19)

q

+ 1

(cid:19)

q

.

Let the total number N of iterations of SVRG to obtain an iterate w that satisﬁes F (w) − F ∗ ≤ (cid:15). Next,
we upper bound E[N ] ≤ (cid:80)K
i=1

E[Tk] as

E[N ] ≤

√

4
qτ (Lµ + κ, λ + κ)

log

(cid:18) 8C(Lµ + κ, λ + κ)
τ (Lµ + κ, λ + κ)

Lµ + κ
κ

2 −
√

q

log

(cid:18) 2(F (w0) − F ∗)
(cid:15) − cqµD

(cid:19)

.

√

(cid:19)

q

(84)

Next, we shall plug in C, τ for SVRG in two different cases:

• Case 1: A > 4µλn, in which case κ + λ = A/(µn) and q < 1/4.

71

• Case 2: A ≤ 4µλn, in which case, κ = λ and q = 1/2.

We ﬁrst consider the term outside the logarithm. It is, up to constants,

(cid:18)

n +

1
√
q

A
µ(λ + κ)

(cid:19)

(cid:114)

= n

λ + κ
λ

+

A
µ(cid:112)λ(λ + κ)

.

For Case 1, plug in κ + λ = A/(µn) so this term evaluates to (cid:112)ADn/(λ(cid:15)). For Case 2, we use the fact
that A ≤ 4µλn so that this term can be upper bounded by,

(cid:32)(cid:114)

n

λ + κ
λ

+ 4

(cid:114) λ

(cid:33)

λ + κ

√

= 3

2n ,

since we chose κ = λ. It remains to consider the logarithmic terms. Noting that κ ≥ λ always, it follows
that the ﬁrst log term of (84) is clearly logarithmic in the problem parameters.

As for the second logarithmic term, we must evaluate cq. For Case 1, we have that q < 1/4 so that
cq < 5 and cqµD < (cid:15)/2. For Case 2, we get that q = 1/2 and cq < 8 so that cqµD < 4(cid:15)/5. Thus, the
second log term of (84) is also logarithmic in problem parameters.

Proposition 30. Consider the setting of Thm. 16. Suppose λ > 0 and κk = κ, for all k ≥ 1 and that α0,
q/2. If we run Algo. 4 with
(µk)k≥1 and (δk)k≥1 are chosen as in Cor. 18, with q = λ/(λ+κ) and η = 1−
SVRG as the inner solver with these parameters, the number of iterations N of SVRG required to obtain w
such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

√

(cid:18)

E[N ] ≤ (cid:101)O

n +

(cid:18)

Aω
µ(λ + κ)(cid:15)

F (w0) − F ∗ +

µDω
√
1 −

q

(cid:19)(cid:19)

.

Proof. We continue to use shorthand A := Aω, D := Dω. First, let us consider the minimum number of
outer iterations K required to achieve F (wK) − F ∗ ≤ (cid:15). From Cor. 18, if we have η−K/2∆0 ≤ (cid:15), or,

K ≥ Kmin :=

log (∆0/(cid:15))
η(cid:1) .
log (cid:0)1/

√

For this smallest value, we have,

µKmin = µηKmin/2 =

µ(cid:15)
∆0

.

(85)

Let C, τ be the linear convergence parameters of SVRG, and deﬁne Lk := λ + A/µk for each k ≥ 1.
Further, let T (cid:48) be such that

T (cid:48) ≥

max
k∈{1,··· ,Kmin}

log

(cid:18)
8

C(Lk + κ, λ + κ)
τ (Lk + κ, λ + κ)

Lk + κ
κδ

(cid:19)

.

72

Then, the total complexity is, from Prop. 27, (ignoring absolute constants)

E[N ] ≤

n +

λ + κ + A
µk
λ + κ

(cid:33)

T (cid:48)

(cid:32)

Kmin(cid:88)

k=1

Kmin(cid:88)

(cid:18)

k=1
(cid:32)

(cid:32)

(cid:32)

=

n + 1 +

(cid:19)

η−k/2

T (cid:48)

A/µ
λ + κ

=

Kmin(n + 1) +

≤

Kmin(n + 1) +

A/µ
λ + κ

A/µ
λ + κ

Kmin(cid:88)

(cid:33)

η−k/2

T (cid:48)

k=1
η−Kmin/2
1 − η1/2

(cid:33)

T (cid:48)

=

(n + 1)

log (cid:0) ∆0
(cid:15)
log(1/√

(cid:1)

η)

+

A/µ
λ + κ

1
√
1 −

η

∆0
(cid:15)

(cid:33)

T (cid:48) .

(86)

It remains to bound T (cid:48). Here, we use λ + A
T (cid:48) is logarithmic in ∆0/(cid:15), n, AD, µ, κ, λ−1.

µ ≤ Lk ≤ λ + A
µK

for all k ≤ K together with (85) to note that

Proposition 31. Consider the setting of Thm. 16 and ﬁx (cid:15) > 0. If we run Algo. 4 with SVRG as the inner
√
, δk = 1/(k + 1)2, and κk = κ = Aω/µ(n + 1).
solver with parameters: µk = µ = (cid:15)/20Dω, α0 =
Then, the number of iterations N to get a point w such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

5−1
2

(cid:32)

(cid:114)

E[N ] ≤ (cid:101)O

n

F (w0) − F ∗
(cid:15)

(cid:112)

+

AωDωn

(cid:107)w0 − w∗(cid:107)2
(cid:15)

(cid:33)

.

Proof. We use shorthand A := Aω, D := Dω, Lµ = A/µ and ∆F0 = F (w0) − F ∗ + κ
2 (cid:107)w0 − w∗(cid:107)2.
Further, let C, τ be the linear convergence parameters of SVRG. In Cor. 19, the fact that K ≥ 1 allows us to
bound the contribution of the smoothing as 10µD. So, we get that the number of outer iterations K required
to get F (wK) − F ∗ ≤ (cid:15) can be bounded as

K + 1 ≤

(cid:115)

8∆F0
(cid:15) − 10µD

.

Moreover, from our choice δk = 1/(k + 1)2, the number of inner iterations Tk for inner loop k is, from
Prop. 27,

E[Tk] ≤

1
τ (Lµ + κ, κ)
2
τ (Lµ + κ, κ)

log

log

(cid:18) 8C(Lµ + κ, κ)
τ (Lµ + κ, κ)
(cid:18) 8C(Lµ + κ, κ)
τ (Lµ + κ, κ)

·

·

Lµ + κ
κ
Lµ + κ
κ

≤

(cid:19)

· (k + 1)2

+ 1

·

8∆F0
(cid:15) − 10µD

(cid:19)

.

Next, we consider the total number N of iterations of SVRG to obtain an iterate w such that F (w) −

F ∗ ≤ (cid:15). Using the fact that E[N ] ≤ (cid:80)K
i=1

E[Tk], we bound it as

E[N ] ≤

1
τ (Lµ + κ, κ)

8∆F0
(cid:15) − 10µD

log

(cid:18) 64C(Lµ + κ, κ)
τ (Lµ + κ, κ)

Lµ + κ
κ

∆F0
(cid:15) − 10µD

(cid:19)

.

(cid:115)

(87)

73

Now, we plug into (87) the values of C, τ for SVRG. Note that κ = Lµ/(n + 1). So we have,

1
τ (Lµ + κ, κ)

= 8

(cid:18) Lµ + κ
κ

(cid:19)

+ n

= 16(n + 1) , and,

C(Lµ + κ, κ) =

Lµ + κ
κ

(cid:32)

1 +

(cid:33)

n Lµ+κ
κ
8 L+κ
κ + n

≤ (n + 2) (cid:0)1 + n
8

(cid:1) .

It now remains to assign µ = (cid:15)/(20D) and plug C, τ from above into (87), noting that κ = 20AD/((cid:15)(n + 1)).

Proposition 32. Consider the setting of Thm. 16. Suppose λ = 0 and that α0, (µk)k≥1,(κk)k≥1 and (δk)k≥1
are chosen as in Cor. 20. If we run Algo. 4 with SVRG as the inner solver with these parameters, the number
of iterations N of SVRG required to obtain w such that F (w) − F ∗ ≤ (cid:15) is bounded in expectation as

E[N ] ≤ (cid:101)O

(cid:0)F (w0) − F ∗ + κ(cid:107)w0 − w∗(cid:107)2

2 + µD(cid:1)

(cid:18) 1
(cid:15)

(cid:18)

n +

(cid:19)(cid:19)

.

Aω
µκ

Proof. Deﬁne short hand A := Aω, D := Dω and

∆0 := 2(F (w0) − F ∗) + κ(cid:107)w0 − w∗(cid:107)2 + 27µD .

From Cor. 20, the number of iterations K required to obtain F (wK) − F ∗ ≤ log(K+1)
Lemma 54),

K+1 ∆0 ≤ (cid:15) is (see

K + 1 =

2∆0
(cid:15)

log

2∆0
(cid:15)

.

Let C, τ be such that SVRG is linearly convergent with parameters C, τ , and deﬁne Lk := A/µk for each
k ≥ 1. Further, let T (cid:48) be such that

T (cid:48) ≥ max

k∈{1,··· ,K}

(cid:18)
8

log

C(Lk + κ, κ)
τ (Lk + κ, κ)

Lk + κ
κδk

(cid:19)

.

Clearly, T (cid:48) is logarithmic in K, n, AD, µ, κ. From Prop. 27, the minimum total complexity is (ignoring

absolute constants)

E[N ] =

K
(cid:88)

(cid:18)

n +

A/µk + κk
κk

(cid:19)

T (cid:48)

n + 1 +

(cid:19)

T (cid:48)

A
µkκk

=

=

(cid:18)

(cid:18)

k=1
K
(cid:88)

k=1
K
(cid:88)

k=1
(cid:18)

n + 1 +

(cid:19)

T (cid:48)

A
µκ
(cid:19)

≤

n + 1 +

KT (cid:48) ,

A
µκ

and plugging in K from (89) completes the proof.

74

(88)

(89)

(90)

D.5 Prox-Linear Convergence Analysis

We ﬁrst prove Lemma 34 that speciﬁes the assumption required by the prox-linear in the case of structured
prediction.
Lemma 34. Consider the structural hinge loss f (w) = maxy∈Y ψ(y; w) = h ◦ g(w) where h, g are as
deﬁned in (6). If the mapping w (cid:55)→ ψ(y; w) is L-smooth with respect to (cid:107) · (cid:107)2 for all y ∈ Y, then it holds
for all w, z ∈ Rd that

|h(g(w + z)) − h(g(w) + ∇g(w)z)| ≤

L
2

(cid:107)z(cid:107)2
2 .

Proof. For any A ∈ Rm×d and w ∈ Rd, and (cid:107)A(cid:107)2,1 deﬁned in (2), notice that

(cid:107)Aw(cid:107)∞ ≤ (cid:107)A(cid:107)2,1(cid:107)w(cid:107)2 .

(91)

Now using the fact that max function h satisﬁes |h(u(cid:48)) − h(u)| ≤ (cid:107)u(cid:48) − u(cid:107)∞ and the fundamental theorem
of calculus (∗), we deduce

|h(g(w + z)) − h(g(w) + ∇g(w)z)| ≤ (cid:107)g(w + z) − (g(w) + ∇g(w)z) (cid:107)∞
(cid:13)
(∗)
(cid:13)
≤
(cid:13)
(cid:13)∞

(∇g(w + tz) − ∇g(w))z dt

(cid:90) 1

(cid:107)∇g(w + tz) − ∇g(w)(cid:107)2,1(cid:107)z(cid:107)2 dt .

(92)

Note that the deﬁnition (2) can equivalently be stated as (cid:107)A(cid:107)2,1 = max(cid:107)u(cid:107)1≤1 (cid:107)A(cid:62)u(cid:107)2. Given u ∈ Rm,
we index its entries uy by y ∈ Y. Then, the matrix norm in (92) can be simpliﬁed as

(cid:107)∇g(w + tz) − ∇g(w)(cid:107)2,1 = max
(cid:107)u(cid:107)1≤1

uy(∇ψ(y; w + tz) − ∇ψ(y; w))

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

|uy|(cid:107)∇ψ(y; w + tz) − ∇ψ(y; w)(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
0
(cid:90) 1

0

(91)
≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

y∈Y
(cid:88)

y∈Y

≤ max
(cid:107)u(cid:107)1≤1

≤ Lt(cid:107)z(cid:107)2 ,

from the L-smoothness of ψ. Plugging this back into (92) completes the proof. The bound on the smothing
approximation holds similarly by noticing that if h is 1-Lipschitz then hµω too since ∇hµω(u) ∈ dom h∗
for any u ∈ dom h.

D.6

Information Based Complexity of the Prox-Linear Algorithm with Casimir-SVRG

Proposition 37. Consider the setting of Thm. 35. Suppose the sequence {(cid:15)k}k≥1 satisﬁes (cid:15)k = (cid:15)0/k for
some (cid:15)0 > 0 and that the subproblem of Line 3 of Algo. 5 is solved using Casimir-SVRG with the settings
of Prop. 29. Then, total number of SVRG iterations N required to produce a w such that (cid:107)(cid:37)η(w)(cid:107)2 ≤ (cid:15) is
bounded as

E[N ] ≤ (cid:101)O





n
η(cid:15)2 (F (w0) − F ∗ + (cid:15)0) +

(cid:113)

AωDωn(cid:15)−1
0
η(cid:15)3



(F (w0) − F ∗ + (cid:15)0)3/2

 .

75

Proof. First note that (cid:80)K
k=1 k−1 ≤ 4(cid:15)0 log K for K ≥ 2. Let ∆F0 := F (w0) − F ∗ and use
shorthand A, D for Aω, Dω respectively. From Thm. 35, the number K of prox-linear iterations required to
ﬁnd a w such that (cid:107)(cid:37)η(w)(cid:107)2 ≤ (cid:15) must satisfy

k=1 (cid:15)k ≤ (cid:15)0

(cid:80)K

For this, it sufﬁces to have (see e.g., Lemma 54)

2
ηK

(∆F0 + 4(cid:15)0 log K) ≤ (cid:15) .

K ≥

4(∆F0 + 4(cid:15)0)
η(cid:15)2

log

(cid:18) 4(∆F0 + 4(cid:15)0)
η(cid:15)2

(cid:19)

.

Before we can invoke Prop. 29, we need to bound the dependence of each inner loop on its warm start:
Fη(wk−1; wk−1) − Fη(w∗
k; wk−1) in terms of problem parameters, where w∗
k = arg minw Fη(w; wk−1)
is the exact result of an exact prox-linear step. We note that Fη(wk−1; wk−1) = F (wk−1) ≤ F (w0), by
Line 4 of Algo. 5. Moreover, from η ≤ 1/L and Asmp. 33, we have,

Fη(w∗

k; wk−1) =

h(cid:0)g(i)(wk−1) + ∇g(i)(wk−1)(w∗

k − wk−1)(cid:1) +

(cid:107)w∗

k(cid:107)2

2 +

(cid:107)w∗

k − wk−1(cid:107)2
2

h(cid:0)g(i)(wk−1) + ∇g(i)(wk−1)(w∗

k − wk−1)(cid:1) +

(cid:107)w∗

k(cid:107)2

2 +

(cid:107)w∗

k − wk−1(cid:107)2
2

λ
2

λ
2

1
2η

L
2

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

1
n

1
n

1
n

≥

≥

h(cid:0)g(i)(w∗

k)(cid:1) +

(cid:107)w∗

k(cid:107)2
2

λ
2

i=1
= F (w∗

k) ≥ F ∗ .

Thus, we bound Fη(wk−1; wk−1) − Fη(w∗
k; wk−1) ≤ ∆F0. We now invoke Prop. 29 and collect all
constants and terms logarithmic in n, (cid:15)−1, (cid:15)−1
0 , ∆F0, η−1, AωDω in T , T (cid:48), T (cid:48)(cid:48). We note that all terms in the
logarithm in Prop. 29 are logarithmic in the problem parameters here. Letting Nk be the number of SVRG
iterations required for iteration k, we get,

E[N ] =

E[Nk] ≤

(cid:32)

K
(cid:88)

n +

(cid:33)

T

(cid:114) ηADn
(cid:15)k
(cid:33)(cid:35)

√

k

T

k=1
(cid:114) ηADn
(cid:15)0
(cid:114) ηADn
(cid:15)0

(cid:32) K
(cid:88)

k=1
(cid:35)

K3/2

T (cid:48)

K
(cid:88)

k=1
(cid:34)

≤

nK +

≤

nK +

(cid:34)

(cid:34)

(cid:34)

≤

=

n
η(cid:15)2 (∆F0 + (cid:15)0) +

(cid:114) ηADn
(cid:15)0

(cid:18) ∆F0 + (cid:15)0
η(cid:15)2

(cid:19)3/2(cid:35)

T (cid:48)(cid:48)

n
η(cid:15)2 (∆F0 + (cid:15)0) +

ADn
η(cid:15)3

(∆F0 + (cid:15)0)3/2
√
(cid:15)0

(cid:35)

T (cid:48)(cid:48) .

√

76

D.7 Some Helper Lemmas

The ﬁrst lemma is a property of the squared Euclidean norm from Lin et al. [2018, Lemma 5], which we
restate here.

Lemma 51. For any vectors, w, z, r ∈ Rd, we have, for any θ > 0,

(cid:107)w − z(cid:107)2 ≥ (1 − θ)(cid:107)w − r(cid:107)2 +

1 −

(cid:107)r − z(cid:107)2 .

(cid:18)

(cid:19)

1
θ

The next lemmas consider rates of the sequences (αk) and (Ak) under different recursions.

Lemma 52. Deﬁne a sequence (αk)k≥0 as

Then this sequence satisﬁes

Moreover, Ak := (cid:81)k

j=0(1 − αk) satisﬁes

√

5 − 1
α0 =
2
k = (1 − αk)α2
α2

k−1 .

√

2
k + 3

≤ αk ≤

2
k + 3

.

2

(k + 3)2 ≤ Ak ≤

4
(k + 3)2 .

Proof. Notice that α0 satisﬁes α2
Hence, we can deﬁne a sequence (bk)k≥0 such that bk := 1/αk. It satisﬁes the recurrence, b2
for k ≥ 1, or in other words, bk = 1
2

0 = 1 − α0. Further, it is clear from deﬁnition that αk ∈ (0, 1) ∀k ≥ 0.
k−1

. Form this we get,

k − bk = b2

1 + 4b2

1 +

k−1

(cid:113)

(cid:16)

(cid:17)

bk ≥ bk−1 +

≥ b0 +

≥

+

k
2

3
2

k
2

.

1
2

since b0 =

. This gives us the upper bound on αk. Moreover, unrolling the recursion,

√

5+1
2

k = (1 − αk)α2
α2

k−1 = Ak

α2
0
1 − α0

= Ak .

(93)

Since αk ≤ 2/(k + 3), (93) yields the upper bound on Ak. The upper bound on αk again gives us,

Ak ≥

k
(cid:89)

(cid:18)

1 −

i=0

(cid:19)

2
i + 3

=

2
(k + 2)(k + 3)

≥

2
(k + 3)2 ,

to get the lower bound on Ak. Invoking (93) again to obtain the lower bound on αk completes the proof.

The next lemma considers the evolution of the sequences (αk) and (Ak) with a different recursion.

77

Lemma 53. Consider a sequence (αk)k≥0 deﬁned by α0 =

, and αk+1 as the non-negative root of

Further, deﬁne

Then, we have for all k ≥ 0,

√

5−1
2

α2
k
1 − αk

= α2

k−1

k
k + 1

.

Ak =

(1 − αi) .

k
(cid:89)

i=0

1
k + 1

(cid:18)

1 −

(cid:19)

1
√
2

≤ Ak ≤

1
k + 2

.

Proof. Deﬁne a sequence (bk)k≥0 such that bk = 1/αk, for each k. This is well-deﬁned because αk (cid:54)=
0, which may be veriﬁed by induction. This sequence satisﬁes the recursion for k ≥ 1: bk(bk − 1) =
(cid:0) k+1
k

(cid:1) bk−1. From this recursion, we get,

(cid:32)

(cid:115)

bk =

1 +

1 + 4b2

k−1

(cid:19)(cid:33)

(cid:18) k + 1
k

1
2

1
2

1
2
√

≥

≥

+ bk−1
(cid:32)

(cid:114)

1 +

(cid:114)

k + 1
k

k + 1
k

k + 1
2

k + 1

√

=

(∗)
≥

(cid:16)

√

(cid:16)√

1/

2 + · · · + 1/

k + 1

+ b0

k + 1

+ · · · +

+ b0

k + 1

(cid:114)

(cid:33)

k + 1
2

√

√

(cid:17)
2

(cid:17)

√

√

√

√

(cid:16)√

k + 2 + b0 −
√

=

k + 1

k + 2 + b0 −

k + 1 ≥ (cid:82) k+2

2

dx√
x = 2(

√

k + 2 −

2) . Since

√

(cid:17)
2

,

√

where (∗) followed from noting that 1/

2 + · · · + 1/

b0 = 1/α0 =

2, we have, for k ≥ 1,

√

5+1
2 >

√

αk ≤

√

1

√

k + 1(

k + 2 + b0 −

2)

√

≤

√

1
√
k + 1

.

k + 2

This relation also clearly holds for k = 0. Next, we claim that

Ak = (k + 1)α2

k ≤

√

k + 1
√

(

k + 1

k + 2)2

=

1
k + 2

.

Indeed, this is true because

For the lower bound, we have,

k = (1 − αk)α2
α2

k−1

k
k + 1

= Ak

α2
0
1 − α0

1
k + 1

=

Ak
k + 1

.

Ak =

(1 − αi) ≥

1 −

√

k
(cid:89)

(cid:18)

i=0

1
√
i + 1

i + 2

(cid:19)

(cid:18)

≥

1 −

(cid:19) k
(cid:89)

(cid:18)

1 −

1
√
2

(cid:19)

=

1
i + 1

1 − 1√
2
k + 1

.

i=1

k
(cid:89)

i=0

(94)

(95)

(96)

(97)

78

Lemma 54. Fix some (cid:15) > 0. If k ≥ 2

(cid:15) log 2

(cid:15) , then we have that log k

k ≤ (cid:15).

Proof. We have, since log x ≤ x for x > 0,

log k
k

≤

log 2

(cid:15) + log log 2
(cid:15) log 2

2

(cid:15)

(cid:15)

(cid:32)

(cid:15)
2

=

1 +

(cid:33)

log log 2
(cid:15)
log 2
(cid:15)

≤ (cid:15) .

E Experiments: Extended Evaluation

Given here are plots for all missing classes of PASCAL VOC 2007. Figures 8 to 10 contain the extension of
Figure 3 while Figures 11 to 13 contain the extension of Figure 4 to all classes.

79

Figure 8: Comparison of convex optimization algorithms for the task of visual object localization on PAS-
CAL VOC 2007 for λ = 10/n for all other classes (1/3).

80

Figure 9: Comparison of convex optimization algorithms for the task of visual object localization on PAS-
CAL VOC 2007 for λ = 10/n for all other classes (2/3).

81

Figure 10: Comparison of convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 10/n for all other classes (3/3).

82

Figure 11: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n for all other classes (1/3).

83

Figure 12: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n for all other classes (2/3).

84

Figure 13: Comparison of non-convex optimization algorithms for the task of visual object localization on
PASCAL VOC 2007 for λ = 1/n for all other classes (3/3).

85

