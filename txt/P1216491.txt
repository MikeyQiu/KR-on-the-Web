Loss Functions for Top-k Error: Analysis and Insights

Maksim Lapin,1 Matthias Hein2 and Bernt Schiele1
1Max Planck Institute for Informatics, Saarbrücken, Germany
2Saarland University, Saarbrücken, Germany

6
1
0
2
 
r
p
A
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
8
4
0
0
.
2
1
5
1
:
v
i
X
r
a

Abstract

In order to push the performance on realistic computer
vision tasks, the number of classes in modern benchmark
datasets has signiﬁcantly increased in recent years. This in-
crease in the number of classes comes along with increased
ambiguity between the class labels, raising the question if
top-1 error is the right performance measure. In this pa-
per, we provide an extensive comparison and evaluation of
established multiclass methods comparing their top-k per-
formance both from a practical as well as from a theoret-
ical perspective. Moreover, we introduce novel top-k loss
functions as modiﬁcations of the softmax and the multiclass
SVM losses and provide efﬁcient optimization schemes for
them. In the experiments, we compare on various datasets
all of the proposed and established methods for top-k error
optimization. An interesting insight of this paper is that the
softmax loss yields competitive top-k performance for all
k simultaneously. For a speciﬁc top-k error, our new top-
k losses lead typically to further improvements while being
faster to train than the softmax.

1. Introduction

The number of classes is rapidly growing in modern
computer vision benchmarks [47, 62]. Typically, this also
leads to ambiguity in the labels as classes start to overlap.
Even for humans, the error rates in top-1 performance are
often quite high (≈ 30% on SUN 397 [60]). While previous
research focuses on minimizing the top-1 error, we address
top-k error optimization in this paper. We are interested in
two cases: a) achieving small top-k error for all reasonably
small k; and b) minimization of a speciﬁc top-k error.

While it is argued in [2] that the one-versus-all (OVA)
SVM scheme performs on par in top-1 and top-5 accuracy
with the other SVM variations based on ranking losses,
we have recently shown in [28] that minimization of the
top-k hinge loss leads to improvements in top-k perfor-
mance compared to OVA SVM, multiclass SVM, and other
ranking-based formulations.
In this paper, we study top-
k error optimization from a wider perspective. On the

one hand, we compare OVA schemes and direct multi-
class losses in extensive experiments, and on the other,
we present theoretical discussion regarding their calibra-
tion for the top-k error. Based on these insights, we sug-
gest 4 new families of loss functions for the top-k error.
Two are smoothed versions of the top-k hinge losses [28],
and the other two are top-k versions of the softmax loss.
We discuss their advantages and disadvantages, and for the
convex losses provide an efﬁcient implementation based on
stochastic dual coordinate ascent (SDCA) [48].

We evaluate a battery of loss functions on 11 datasets
of different tasks ranging from text classiﬁcation to large
scale vision benchmarks, including ﬁne-grained and scene
classiﬁcation. We systematically optimize and report results
separately for each top-k accuracy. One interesting message
that we would like to highlight is that the softmax loss is
able to optimize all top-k error measures simultaneously.
This is in contrast to multiclass SVM and is also reﬂected
in our experiments. Finally, we show that our new top-k
variants of smooth multiclass SVM and the softmax loss
can further improve top-k performance for a speciﬁc k.

Related work. Top-k optimization has recently received
revived attention with the advent of large scale problems
[20, 28, 30, 31]. The top-k error in multiclass classiﬁca-
tion, which promotes good ranking of class labels for each
example, is closely related to the precision@k metric in in-
formation retrieval, which counts the fraction of positive in-
stances among the top-k ranked examples. In essence, both
approaches enforce a desirable ranking of items [28].

The classic approaches optimize pairwise ranking with
SVMstruct [25, 53], RankNet [11], and LaRank [7]. An
alternative direction was proposed by Usunier et al. [54],
who described a general family of convex loss functions for
ranking and classiﬁcation. One of the loss functions that
we consider (top-k SVMβ [28]) also falls into that fam-
ily. Weston et al. [59] then introduced Wsabie, which opti-
mizes an approximation of a ranking-based loss from [54].
A Bayesian approach was suggested by [51].

Recent works focus on the top of the ranked list [1, 9,
39, 46], scalability to large datasets [20, 28, 30], explore
transductive learning [31] and prediction of tuples [45].

1

Method

Name

Loss function

Conjugate SDCA update Top-k calibrated Convex

One-vs-all (OVA) SVM

OVA logistic regression

Multiclass SVM

SVMOVA
LROVA
SVMMulti
LRMulti
top-k SVMα Top-k hinge (α)
top-k SVMβ Top-k hinge (β)
top-k SVMα
top-k SVMβ

γ Smooth top-k hinge (α) ∗
γ Smooth top-k hinge (β) ∗

Softmax (maximum entropy)

top-k Ent

top-k Enttr

Top-k entropy ∗
Truncated top-k entropy ∗

max{0, 1 − a}
log(1 + e−a)
max (cid:8)0, (a + c)π1
log (cid:0) (cid:80)
max (cid:8)0, 1
(cid:80)k

(cid:9)
j∈Y exp(aj)(cid:1)
(cid:80)k
j=1(a + c)πj
j=1 max (cid:8)0, (a + c)πj
Eq. (12) w/ ∆α
k
Eq. (12) w/ ∆β
k
Prop. 9

1
k

k

(cid:9)

(cid:9)

[48]

[48]

[28, 48]

Prop. 8

[28, 48]

Prop. 12

[28]

[28]

Prop. 7

Prop. 11

Eq. (14)

Prop. 12

no1 (Prop. 1)
yes (Prop. 2)

no (Prop. 3)

yes (Prop. 4)

open

question

for k > 1

yes

Eq. (22)

-

-

yes (Prop. 10)

no

Note that SVMMulti ≡ top-1 SVMα ≡ top-1 SVMβ and LRMulti ≡ top-1 Ent ≡ top-1 Enttr.
We let a (cid:44) yf (x) (binary one-vs-all); a (cid:44) (fj(x) − fy(x))j∈Y , c (cid:44) 1 − ey (multiclass); π : aπ1 ≥ . . . ≥ aπm .

Table 1: Overview of the methods we consider and our contributions. ∗Novel loss. 1But smoothed one is (Prop. 5).

Contributions. We study the problem of top-k error op-
timization on a diverse range of learning tasks. We con-
sider existing methods as well as propose 4 novel loss func-
tions for minimizing the top-k error. A brief overview of
the methods is given in Table 1. For the proposed convex
top-k losses, we develop an efﬁcient optimization scheme
based on SDCA1, which can also be used for training with
the softmax loss. All methods are evaluated empirically in
terms of the top-k error and, whenever possible, in terms
of classiﬁcation calibration. We discover that the softmax
loss and the proposed smooth top-1 SVM are astonishingly
competitive in all top-k errors. Further small improvements
can be obtained with the new top-k losses.

2. Loss Functions for Top-k Error

We consider multiclass problems with m classes where
i=1 consists of n examples xi ∈ Rd
the training set (xi, yi)n
along with the corresponding labels yi ∈ Y (cid:44) {1, . . . , m}.
We use π and τ to denote a permutation of (indexes) Y. Un-
less stated otherwise, aπ reorders components of a vector
a ∈ Rm in descending order, i.e. aπ1 ≥ aπ2 ≥ . . . ≥ aπm .
While we consider linear classiﬁers in our experiments, all
loss functions below are formulated in the general setting
where a function f : X → Rm is learned and predic-
tion at test time is done via arg maxy∈Y fy(x), resp. the
top-k predictions. For the linear case, all predictors fy
have the form fy(x) = (cid:104)wy, x(cid:105). Let W ∈ Rd×m be the
stacked weight matrix, L : Y × Rm → R be a convex
loss function, and λ > 0 be a regularization parameter.
We consider the following multiclass optimization problem
minW

i=1 L(yi, W (cid:62)xi) + λ (cid:107)W (cid:107)2
F .

(cid:80)n

1
n

1 Code available at: https://github.com/mlapin/libsdca

(cid:75)

We use the Iverson bracket notation

, deﬁned as
= 1 if P is true, 0 otherwise; and introduce a short-
P
(cid:74)
hand py(x) (cid:44) Pr(Y = y | X = x). We generalize the
standard zero-one error and allow k guesses instead of one.
Formally, the top-k zero-one loss (top-k error) is

(cid:74)

(cid:75)

P

errk(y, f (x)) (cid:44)

fπk (x) > fy(x)

(1)

(cid:74)
Note that for k = 1 we recover the standard zero-one error.
Top-k accuracy is deﬁned as 1 minus the top-k error.

.
(cid:75)

2.1. Bayes Optimality and Top-k Calibration

In this section, we establish the best achievable top-k er-
ror, determine when a classiﬁer achieves it, and deﬁne a
notion of top-k calibration.

Lemma 1. The Bayes optimal top-k error at x is

min
g∈Rm

EY |X [errk(Y, g) | X = x] = 1 − (cid:80)k

j=1 pτj (x),

where pτ1(x) ≥ pτ2 (x) ≥ . . . ≥ pτm(x). A classiﬁer f is
top-k Bayes optimal at x if and only if

(cid:8)y | fy(x) ≥ fπk (x)(cid:9) ⊂ (cid:8)y | py(x) ≥ pτk (x)(cid:9),

where fπ1(x) ≥ fπ2(x) ≥ . . . ≥ fπm(x).
Proof. Let g ∈ Rm and π be a permutation such that gπ1 ≥
gπ2 ≥ . . . ≥ gπm. The expected top-k error at x is

EY |X [errk(Y, g) | X = x] = (cid:80)
= (cid:80)
gπk > gπy
y∈Y
(cid:74)
(cid:75)
= 1 − (cid:80)k
j=1 pπj (x).

gπk > gy
(cid:75)
(cid:74)
pπy (x) = (cid:80)m
j=k+1 pπj (x)

y∈Y

py(x)

The error is minimal when (cid:80)k
j=1 pπj (x) is maximal, which
corresponds to taking the k largest conditional probabilities
(cid:80)k
j=1 pτj (x) and yields the Bayes optimal top-k error at x.
Since the relative order within {pτj (x)}k
j=1 is irrelevant
for the top-k error, any classiﬁer f (x), for which the sets
{π1, . . . , πk} and {τ1, . . . , τk} coincide, is Bayes optimal.
Note that we assumed w.l.o.g. that there is a clear cut
pτk (x) > pτk+1(x) between the k most likely classes and
the rest. In general, ties can be resolved arbitrarily as long
as we can guarantee that the k largest components of f (x)
correspond to the classes (indexes) that yield the maximal
sum (cid:80)k
j=1 pπj (x) and lead to top-k Bayes optimality.

Optimization of the zero-one loss (and, by extension, the
top-k error) leads to hard combinatorial problems. Instead,
a standard approach is to use a convex surrogate loss which
upper bounds the zero-one error. Under mild conditions on
the loss function [3, 52], the optimal classiﬁer w.r.t. the sur-
rogate yields a Bayes optimal solution for the zero-one loss.
Such loss is called classiﬁcation calibrated, which is known
in statistical learning theory as a necessary condition for a
classiﬁer to be universally Bayes consistent [3]. We intro-
duce now the notion of calibration for the top-k error.
Deﬁnition 1. A loss function L : Y × Rm → R (or a re-
duction scheme) is called top-k calibrated if for all possible
data generating measures on Rd × Y and all x ∈ Rd

arg ming∈Rm EY |X [L(Y, g) | X = x]
⊆ arg ming∈Rm EY |X [errk(Y, g) | X = x].
If a loss is not top-k calibrated, it implies that even in the
limit of inﬁnite data, one does not obtain a classiﬁer with
the Bayes optimal top-k error from Lemma 1.

2.2. OVA and Direct Multiclass Approaches

The standard multiclass problem is often solved using
the one-vs-all (OVA) reduction into a set of m binary clas-
siﬁcation problems. Every class is trained versus the rest
which yields m classiﬁers {fy}y∈Y .

Typically, the binary classiﬁcation problems are formu-
lated with a convex margin-based loss function L(yf (x)),
where L : R → R and y = ±1. We consider in this paper:

L(yf (x)) = max{0, 1 − yf (x)},
L(yf (x)) = log(1 + e−yf (x)).

(2)

(3)

The hinge (2) and logistic (3) losses correspond to the SVM
and logistic regression respectively. We now show when
the OVA schemes are top-k calibrated, not only for k = 1
(standard multiclass loss) but for all k simultaneously.

Lemma 2. The OVA reduction is top-k calibrated for any
1 ≤ k ≤ m if the Bayes optimal function of the convex
margin-based loss L(yf (x)) is a strictly monotonically in-
creasing function of Pr(Y = 1 | X = x).

Proof. For every class y ∈ Y, the Bayes optimal classiﬁer
for the corresponding binary problem has the form

fy(x) = g(cid:0) Pr(Y = y | X = x)(cid:1),

where g is a strictly monotonically increasing function.
The ranking of fy corresponds to the ranking of Pr(Y =
y | X = x) and hence the OVA reduction is top-k calibrated
for any k = 1, . . . , m.

Next, we check if the one-vs-all schemes employing

hinge and logistic regression losses are top-k calibrated.

Proposition 1. OVA SVM is not top-k calibrated.

Proof. First, we show that the Bayes optimal function for
the binary hinge loss is

f ∗(x) = 2
(cid:74)

Pr(Y = 1 | X = x) > 1
2

− 1.

(cid:75)

We decompose the expected loss as

EX,Y [L(Y, f (X))] = EX [EY |X [L(Y, f (x)) | X = x]].

Thus, one can compute the Bayes optimal classiﬁer f ∗
pointwise by solving

arg min
α∈R

EY |X [L(Y, α) | X = x],

for every x ∈ Rd, which leads to the following problem

arg min
α∈R

max{0, 1 − α}p1(x) + max{0, 1 + α}p−1(x),

where py(x) (cid:44) Pr(Y = y | X = x). It is obvious that the
optimal α∗ is contained in [−1, 1]. We get

arg min
−1≤α≤1

(1 − α)p1(x) + (1 + α)p−1(x).

The minimum is attained at the boundary and we get

f ∗(x) =

(cid:40)

+1
−1

if p1(x) > 1
2 ,
if p1(x) ≤ 1
2 .

Therefore, the Bayes optimal classiﬁer for the hinge loss is
not a strictly monotonically increasing function of p1(x).

To show that OVA hinge is not top-k calibrated, we con-
struct an example problem with 3 classes and p1(x) = 0.4,
p2(x) = p3(x) = 0.3. Note that for every class y = 1, 2, 3,
the Bayes optimal binary classiﬁer is −1, hence the pre-
dicted ranking of labels is arbitrary and may not produce
the Bayes optimal top-k error.

In contrast, logistic regression is top-k calibrated.

Proposition 2. OVA logistic regression is top-k calibrated.

Proof. First, we show that the Bayes optimal function for
the binary logistic loss is

Proof. First, we derive the Bayes optimal function.

Let y ∈ arg maxj∈Y pj(x). Given any c ∈ R, a Bayes

optimal function f ∗ : Rd → Rm for the loss (4) is

f ∗(x) = log

(cid:16) px(1)

1 − px(1)

(cid:17)

.

As above, the pointwise optimization problem is

(cid:40)

f ∗
y (x) =

c + 1
c

if maxj∈Y pj(x) ≥ 1
2 ,
otherwise,

f ∗
j (x) = c, j ∈ Y \ {y}.

arg min
α∈R

log(1+exp(−α))p1(x)+log(1+exp(α))p−1(x).

The logistic loss is known to be convex and differentiable
and thus the optimum can be computed via

Let g = f (x) ∈ Rm, then

EY |X [L(Y, g) | X] =

(cid:88)

l∈Y

(cid:8)

max
j∈Y

j (cid:54)= l
(cid:74)

(cid:75)

+ gj − gl

(cid:9)pl(x).

− exp(−α)
1 + exp(−α)

p1(x) +

exp(α)
1 + exp(α)

p−1(x) = 0.

Suppose that the maximum of (gj)j∈Y is not unique. In this
case, we have

> 0,

∀x ∈ (0, 1).

As the loss only depends on the gap gy −gl, we can optimize
this with βl = gy − gl.

Re-writing the ﬁrst fraction we get

−1
1 + exp(α)

p1(x) +

exp(α)
1 + exp(α)

p−1(x) = 0,

which can be solved as α∗ = log
formula for the Bayes optimal classiﬁer stated above.

and leads to the

We check now that the function φ : (0, 1) → R deﬁned

as φ(x) = log( x

1−x ) is strictly monotonically increasing.

(cid:16) p1(x)
p−1(x)

(cid:17)

φ(cid:48)(x) =

1 − x
x
1 − x
x

(cid:0)

(cid:1)

+

1
1 − x
1
(1 − x)2 =

x
(1 − x)2
1
x(1 − x)

=

The derivative is strictly positive on (0, 1), which implies
that φ is strictly monotonically increasing. The logistic loss,
therefore, fulﬁlls the conditions of Lemma 2 and is top-k
calibrated for any 1 ≤ k ≤ m.

An alternative to the OVA scheme with binary losses is to
use a multiclass loss L : Y ×Rm → R directly. We consider
two generalizations of the hinge and logistic losses below:

L(y, f (x)) = max
j∈Y

j (cid:54)= y

(cid:8)
(cid:74)
(cid:16) (cid:80)

+ fj(x) − fy(x)(cid:9),
(cid:17)

(cid:75)

j∈Y exp(fj(x) − fy(x))

.

(4)

(5)

L(y, f (x)) = log

Both the multiclass hinge loss (4) of Crammer & Singer
[15] and the softmax loss (5) are popular losses for mul-
ticlass problems. The latter is also known as the cross-
entropy or multiclass logistic loss and is often used as the
last layer in deep architectures [6, 26, 50]. The multiclass
hinge loss has been shown to be competitive in large-scale
image classiﬁcation [2], however, it is known to be not cal-
ibrated [52] for the top-1 error. Next, we show that it is not
top-k calibrated for any k.

Proposition 3. Multiclass SVM is not top-k calibrated.

(cid:8)

max
j∈Y

j (cid:54)= l
(cid:74)

(cid:75)

+ gj − gl

(cid:9) ≥ 1, ∀ l ∈ Y

(cid:74)

j (cid:54)= l

is always active. The best possible loss
as the term
is obtained by setting gj = c for all j ∈ Y, which yields an
expected loss of 1. On the other hand, if the maximum is
unique and is achieved by gy, then

(cid:75)

+ gj − gl

(cid:9)

(cid:8)

max
j∈Y

j (cid:54)= l
(cid:74)
(cid:40)

=

(cid:75)
1 + gy − gl
max (cid:8)0, maxj(cid:54)=y{1 + gj − gy}(cid:9)

if l (cid:54)= y,
if l = y.

EY |X [L(Y, g) | X = x]

=

(1 + gy − gl)pl(x)

(cid:88)

l(cid:54)=y

=

=

(cid:88)

l(cid:54)=y
(cid:88)

l(cid:54)=y

+ max (cid:8)0, max

{1 + gl − gy}(cid:9)py(x)

l(cid:54)=y

(1 + βl)pl(x) + max (cid:8)0, max

{1 − βl}(cid:9)py(x)

l(cid:54)=y

(1 + βl)pl(x) + max{0, 1 − min
l(cid:54)=y

βl}py(x).

As only the minimal βl enters the last term, the optimum is
achieved if all βl are equal for l (cid:54)= y (otherwise it is possible
to reduce the ﬁrst term without affecting the last term). Let
α (cid:44) βl for all l (cid:54)= y. The problem becomes

(1 + α)pl(x) + max{0, 1 − α}py(x)

min
α≥0

(cid:88)

l(cid:54)=y

≡ min
0≤α≤1

α(1 − 2py(x))

Let p (cid:44) py(x) = Pr(Y = y | X = x). The solution is
(cid:40)

α∗ =

0
1

if p < 1
2 ,
if p ≥ 1
2 ,

and the associated risk is

EY |X [L(Y, g) | X = x] =

(cid:40)
1
2(1 − p)

if p < 1
2 ,
if p ≥ 1
2 .

for j ∈ Y. We note that the critical point is not unique
as multiplication g → κg leaves the equation invariant for
any κ > 0. One can verify that egj = αpj(x) satisﬁes the
equations for any α > 0. This yields a solution

If p < 1
all j ∈ Y and any c ∈ R. Otherwise, p ≥ 1

2 , then the Bayes optimal classiﬁer f ∗
2 and

j (x) = c for

(cid:40)

f ∗
y (x) =

log(αpy(x))
−∞

if py(x) > 0,
otherwise,

(cid:40)

f ∗
j (x) =

c + 1
c

if j = y,
if j ∈ Y \ {y}.

Moreover, we have that the Bayes risk at x is

EY |X [L(Y, f ∗(x)) | X = x] = min{1, 2(1 − p)} ≤ 1.

It follows, that the multiclass hinge loss is not (top-1)
classiﬁcation calibrated at any x where maxy∈Y py(x) < 1
2
as its Bayes optimal classiﬁer reduces to a constant. More-
over, even if py(x) ≥ 1
2 for some y, the loss is not top-k
calibrated for k ≥ 2 as the predicted order of the remaining
classes need not be optimal.

Again, a contrast between the hinge and logistic losses.

Proposition 4. The softmax loss is top-k calibrated.

Proof. The multiclass logistic loss is (top-1) calibrated for
the zero-one error in the following sense. If

f ∗(x) ∈ arg min

EY |X [L(Y, g) | X = x],

g∈Rm

then for some α > 0 and all y ∈ Y

(cid:40)

f ∗
y (x) =

log(α py(x))
−∞

if py(x) > 0,
otherwise,

which implies

arg max
y∈Y

f ∗
y (x) = arg max

Pr(Y = y | X = x).

y∈Y

We now prove this result and show that it also generalizes
to top-k calibration for k > 1. Using the identity
L(y, g) = log (cid:0) (cid:80)

j∈Y egj −gy (cid:1) = log (cid:0) (cid:80)
j∈Y egj (cid:1) − gy
y∈Y py(x) = 1, we write for a g ∈ Rm

and the fact that (cid:80)

EY |X [L(Y, g) | X = x]

(cid:88)

L(y, g)py(x) = log (cid:0) (cid:88)

egy (cid:1) −

=

(cid:88)

gypx(y).

y∈Y

y∈Y

y∈Y

for any ﬁxed α > 0. We note that f ∗
y is a strictly mono-
tonically increasing function of the conditional class prob-
abilities. Therefore, it preserves the ranking of py(x) and
implies that f ∗ is top-k calibrated for any 1 ≤ k ≤ m.

The implicit reason for top-k calibration of the OVA
schemes and the softmax loss is that one can estimate the
probabilities py(x) from the Bayes optimal classiﬁer. Loss
functions which allow this are called proper. We refer to
[41] and references therein for a detailed discussion.

We have established that the OVA logistic regression and
the softmax loss are top-k calibrated for any k, so why
should we be interested in deﬁning new loss functions for
the top-k error? The reason is that calibration is an asymp-
totic property as the Bayes optimal functions are obtained
pointwise. The picture changes if we use linear classiﬁers,
since they obviously cannot be minimized independently at
each point. Indeed, most of the Bayes optimal classiﬁers
cannot be realized by linear functions.

In particular, convexity of the softmax and multiclass
hinge losses leads to phenomena where errk(y, f (x)) = 0,
but L(y, f (x)) (cid:29) 0. This happens if fπ1(x) (cid:29) fy(x) ≥
fπk (x) and adds a bias when working with “rigid” function
classes such as linear ones. The loss functions which we
introduce in the following are modiﬁcations of the above
losses with the goal of alleviating that phenomenon.

2.3. Smooth Top-k Hinge Loss

Recently, we introduced two top-k versions of the mul-
ticlass hinge loss (4) in [28], where the second version is
based on the family of ranking losses introduced earlier by
[54]. We use our notation from [28] for direct comparison
and refer to the ﬁrst version as α and the second one as β.
Let c = 1 − ey, where 1 is the all ones vector, ey is the y-th
basis vector, and let a ∈ Rm be deﬁned componentwise as
aj (cid:44) (cid:104)wj, x(cid:105) − (cid:104)wy, x(cid:105). The two top-k hinge losses are

L(a) = max (cid:8)0, 1
L(a) = 1
k

j=1(a + c)πj
j=1max (cid:8)0, (a + c)πj

(cid:80)k

(cid:80)k

k

(cid:9) (cid:0)top-k SVMα(cid:1), (6)
(cid:9) (cid:0)top-k SVMβ(cid:1), (7)

As the loss is convex and differentiable, we get the global
optimum by computing a critical point. We have

∂
∂gj

EY |X [L(Y, g) | X = x] =

− pj(x) = 0

egj
y∈Y egy

(cid:80)

where (a)πj is the j-th largest component of a.
It was
shown in [28] that (6) is a tighter upper bound on the top-k
error than (7), however, both losses performed similarly in
our experiments. In the following, we simply refer to them
as the top-k hinge or the top-k SVM loss.

Both losses reduce to the multiclass hinge loss (4) for
k = 1. Therefore, they are unlikely to be top-k calibrated,
even though we can currently neither prove nor disprove
this for k > 1. The multiclass hinge loss is not calibrated
as it is non-smooth and does not allow to estimate the class
conditional probabilities py(x). Our new family of smooth
top-k hinge losses is based on the Moreau-Yosida regular-
ization [5, 34]. This technique has been used in [48] to
smooth the binary hinge loss (2). Interestingly, smooth bi-
nary hinge loss fulﬁlls the conditions of Lemma 2 and leads
to a top-k calibrated OVA scheme. The hope is that the
smooth top-k hinge loss becomes top-k calibrated as well.
Smoothing works by adding a quadratic term to the
conjugate function2, which then becomes strongly convex.
Smoothness of the loss, among other things, typically leads
to much faster optimization as we discuss in Section 3.

Proposition 5. OVA smooth hinge is top-k calibrated.

Proof. In order to derive the smooth hinge loss, we ﬁrst
compute the conjugate of the standard binary hinge loss,

L(α) = max{0, 1 − α},
L∗(β) = sup
α∈R
(cid:40)

(cid:8)αβ − max{0, 1 − α}(cid:9)

=

if − 1 ≤ β ≤ 0,

β
∞ otherwise.

Case 0 < γ ≤ 1. Consider the case 1 − γ ≤ α ≤ 1,

α − 1
γ

p + (1 − p) = 0 =⇒ α∗ = 1 − γ

1 − p
p

.

This case corresponds to p ≥ 1
2 , which follows from the
constraint α∗ ≥ 1 − γ. Next, consider γ − 1 ≤ α ≤ 1 − γ,

−p + (1 − p) = 1 − 2p (cid:54)= 0,

unless p = 1
Finally, consider −1 ≤ α ≤ γ − 1 ≤ 1 − γ. Then

2 , which is already captured by the ﬁrst case.

−p −

−α − 1
γ

(1 − p) = 0 =⇒ α∗ = −1 + γ

p
1 − p

,

where we have −1 ≤ α∗ ≤ γ − 1 if p ≤ 1
Bayes optimal classiﬁer for 0 < γ ≤ 1 as follows:

2 . We obtain the

f ∗(x) =

(cid:40)

1 − γ 1−p
p
−1 + γ p
1−p

if p ≥ 1
2 ,
if p < 1
2 .

Note that while f ∗(x) is not a continuous function of p =
p1(x) for γ < 1, it is still a strictly monotonically increasing
function of p for any 0 < γ ≤ 1.

Case γ > 1. First, consider γ − 1 ≤ α ≤ 1,

α − 1
γ

p + (1 − p) = 0 =⇒ α∗ = 1 − γ

1 − p
p

.

(8)

From α∗ ≥ γ − 1, we get the condition p ≥ γ
consider 1 − γ ≤ α ≤ γ − 1,

2 . Next,

The smoothed conjugate is

L∗

γ(β) = L∗(β) +

β2.

γ
2

The corresponding primal smooth hinge loss is given by

Lγ(α) = sup

(cid:8)αβ − β − γ

2 β2(cid:9)

−1≤β≤0



1 − α − γ
2
(α−1)2
2γ



0,

=

if α < 1 − γ,

if 1 − γ ≤ α ≤ 1,
if α > 1.

(9)

Lγ(α) is convex and differentiable with the derivative

L(cid:48)

γ(α) =






−1
α−1
γ
0,

if α < 1 − γ,
if 1 − γ ≤ α ≤ 1,
if α > 1.

We compute the Bayes optimal classiﬁer pointwise.

f ∗(x) = arg min

L(α)p1(x) + L(−α)p−1(x).

α∈R

Let p (cid:44) p1(x), the optimal α∗ is found by solving

L(cid:48)(α)p − L(cid:48)(−α)(1 − p) = 0.

2 The convex conjugate of f is f ∗(x∗) = supx{(cid:104)x∗, x(cid:105) − f (x)}.

α − 1
γ

p −

−α − 1
γ

(1 − p) = 0 =⇒ α∗ = 2p − 1,

which is in the range [1 − γ, γ − 1] if 1 − γ
Finally, consider −1 ≤ α ≤ 1 − γ,

2 ≤ p ≤ γ
2 .

−p −

−α − 1
γ

(1 − p) = 0 =⇒ α∗ = −1 + γ

p
1 − p

,

where we have −1 ≤ α∗ ≤ 1 − γ if p ≤ 1 − γ
the Bayes optimal classiﬁer for γ > 1 is

2 . Overall,






f ∗(x) =

if p ≥ γ
2 ,
2 ≤ p ≤ γ
if 1 − γ
2 ,
if p < 1 − γ
2 .

1 − γ 1−p
p
2p − 1
−1 + γ p
1−p
Note that f ∗ is again a strictly monotonically increasing
function of p = p1(x). Therefore, for any γ > 0, the
one-vs-all scheme with the smooth hinge loss (9) is top-k
calibrated for all 1 ≤ k ≤ m by Lemma 2.

Next, we introduce the multiclass smooth top-k hinge
losses, which extend the top-k hinge losses (6) and (7). We
deﬁne the top-k simplex (α and β) of radius r as

∆α
∆β

k (r) (cid:44) (cid:8)x | (cid:104)1, x(cid:105) ≤ r, 0 ≤ xi ≤ 1
k (r) (cid:44) (cid:8)x | (cid:104)1, x(cid:105) ≤ r, 0 ≤ xi ≤ 1
(cid:44) ∆β

k (1) and ∆β

(cid:44) ∆α

k (cid:104)1, x(cid:105) , ∀i(cid:9),
k r, ∀i(cid:9).
k (1).

k

We also let ∆α
k

(10)

(11)

Proposition 6 ([28]). The convex conjugates of (6) and (7)
are respectively L∗(b) = − (cid:104)c, b(cid:105), if b ∈ ∆α
k , +∞ other-
wise; and L∗(b) = − (cid:104)c, b(cid:105), if b ∈ ∆β

k , +∞ otherwise.

Smoothing applied to the top-k hinge loss (6) yields the
following smooth top-k hinge loss (α). Smoothing of (7)
is done similarly, but the set ∆α
k (r).

k (r) is replaced with ∆β

Proposition 7. Let γ > 0 be the smoothing parameter. The
smooth top-k hinge loss (α) and its conjugate are

Lγ(a) = 1
γ
γ(b) = γ
L∗

2 (cid:104)p, p(cid:105) (cid:1),
(cid:0) (cid:104)a + c, p(cid:105) − 1
2 (cid:104)b, b(cid:105) − (cid:104)c, b(cid:105) , if b ∈ ∆α

k , +∞ o/w,

(12)

(13)

where p = proj∆α
(a + c) on ∆α

k (γ)(a + c) is the Euclidean projection of

k (γ). Moreover, Lγ(a) is 1/γ-smooth.

Proof. We take the convex conjugate of the top-k hinge
loss, which was derived in [28, Proposition 2],

(cid:40)

L∗(b) =

− (cid:104)c, b(cid:105)
+∞

k (1),

if b ∈ ∆α
otherwise,

and add the regularizer γ
2 (cid:104)b, b(cid:105) to obtain the γ-strongly con-
vex conjugate loss L∗
γ(b) as stated in the proposition. As
mentioned above [21] (see also [48, Lemma 2]), the primal
smooth top-k hinge loss Lγ(a), obtained as the convex con-
jugate of L∗
γ(b), is 1/γ-smooth. We now obtain a formula to
compute it based on the Euclidean projection onto the top-k
simplex. By deﬁnition,

Lγ(a) = sup
b∈Rm

{(cid:104)a, b(cid:105) − L∗

γ(b)}

= max
b∈∆α

k (1)

(cid:110) γ

= − min
b∈∆α

k (1)

= − 1

γ min
b∈∆α

k (1)

= − 1

γ min
b
γ ∈∆α

k (1)

(cid:110)

(cid:104)a, b(cid:105) − γ

(cid:111)

2 (cid:104)b, b(cid:105) + (cid:104)c, b(cid:105)
(cid:111)

2 (cid:104)b, b(cid:105) − (cid:104)a + c, b(cid:105)
(cid:110) 1

2 (cid:104)γb, γb(cid:105) − (cid:104)a + c, γb(cid:105)
(cid:111)
(cid:110) 1
.

2 (cid:104)b, b(cid:105) − (cid:104)a + c, b(cid:105)

(cid:111)

For the constraint b

γ ∈ ∆α

k (1), we have

(cid:104)1, b/γ(cid:105) ≤ 1,

(cid:104)1, b(cid:105) ≤ γ,

0 ≤ bi/γ ≤ 1
0 ≤ bi ≤ 1

k (cid:104)1, b/γ(cid:105) ⇐⇒
k (cid:104)1, b(cid:105) ⇐⇒ b ∈ ∆α

k (γ).

The ﬁnal expression follows from the fact that

(cid:8) 1
2 (cid:104)b, b(cid:105) − (cid:104)a + c, b(cid:105) (cid:9)

arg min
b∈∆α
k (γ)

≡ arg min
b∈∆α
k (γ)

(cid:107)(a + c) − b(cid:107)2 ≡ proj∆α

k (γ)(a + c).

There is no analytic expression for (12) and evaluation
requires computing a projection onto the top-k simplex
∆α
k (γ), which can be done in O(m log m) time as shown in
[28]. The non-analytic nature of smooth top-k hinge losses
currently prevents us from proving their top-k calibration.

2.4. Top-k Entropy Loss

As shown in § 4 on synthetic data, top-1 and top-2 er-
ror optimization, when limited to linear classiﬁers, lead to
completely different solutions. The softmax loss, primar-
ily aiming at top-1 performance, produces a solution that
is reasonably good in top-1 error, but is far from what can
be achieved in top-2 error. That reasoning motivated us to
adapt the softmax loss to top-k error optimization. Inspired
by the conjugate of the top-k hinge loss, we introduce in
this section the top-k entropy loss.

Recall that the conjugate functions of multiclass SVM
[15] and the top-k SVM [28] differ only in their effective
domain3 while the conjugate function is the same. Instead
of the standard simplex, the conjugate of the top-k hinge
loss is deﬁned on a subset, the top-k simplex.

This suggests a way to construct novel losses with spe-
ciﬁc properties by taking the conjugate of an existing loss
function, and modifying its essential domain in a way that
enforces the desired properties. The motivation for doing
so comes from the interpretation of the dual variables as
forces with which every training example pushes the deci-
sion surface in the direction given by the ground truth la-
bel. The absolute value of the dual variables determines the
magnitude of these forces and the optimal values are often
attained at the boundary of the feasible set (which coincides
with the essential domain of the loss). Therefore, by reduc-
ing the feasible set we can limit the maximal contribution
of a given training example.

We begin with the conjugate of the softmax loss. Let

a\y be obtained by removing the y-th coordinate from a.

Proposition 8. The convex conjugate of (5) is

L∗(v) =

j(cid:54)=y vj log vj + (1 + vy) log(1 + vy),
if (cid:104)1, v(cid:105) = 0 and v\y ∈ ∆,

(14)

(cid:80)






+∞ otherwise,

where ∆ (cid:44) (cid:8)x | (cid:104)1, x(cid:105) ≤ 1, 0 ≤ xj ≤ 1, ∀j(cid:9).
Proof. We provide a derivation for the convex conjugate
of the softmax loss which was already given in [32, Ap-
pendix D.2.3] without a proof. We also highlight the con-
straint (cid:104)1, v(cid:105) = 0 which can be easily missed when com-
puting the conjugate and is re-stated explicitly in Lemma 3.
Let u (cid:44) f (x) ∈ Rm. The softmax loss on example x is
(cid:17)

(cid:17)

(cid:16) (cid:88)

(cid:16) (cid:88)

L(u) = log

exp(uj − uy)

= log

exp(u(cid:48)
j)

,

j∈Y

j∈Y

3 The effective domain of f is dom f = {x ∈ X | f (x) < +∞}.

where we let u(cid:48) (cid:44) Hyu and Hy (cid:44) I − 1e(cid:62)

y . Let

= (cid:80)

j(cid:54)=y vj log(vj) − log(1 + Z)(1 + vy).

φ(u) (cid:44) log

(cid:16) (cid:80)

j∈Y exp(uj)

(cid:17)

,

Summing vj and using the deﬁnition of Z,

then L(u) = φ(Hyu) and the convex conjugate is computed
similar to [28, Lemma 2] as follows.

L∗(v) = sup{ (cid:104)u, v(cid:105) − L(u) | u ∈ Rm}

= sup{ (cid:104)u, v(cid:105) − φ(Hyu) | u ∈ Rm}
= sup{(cid:104)u(cid:107), v(cid:105) + (cid:104)u⊥, v(cid:105) − φ(Hyu⊥) |
u(cid:107) ∈ Ker Hy, u⊥ ∈ Ker⊥ Hy},

where Ker Hy = {u | Hyu = 0} = {t1 | t ∈ R} and
Ker⊥ Hy = {u | (cid:104)1, u(cid:105) = 0}.
It follows that L∗(v)
can only be ﬁnite if (cid:104)u(cid:107), v(cid:105) = 0, which implies v ∈
Ker⊥ Hy ⇐⇒ (cid:104)1, v(cid:105) = 0. Let H †
y be the Moore-Penrose
pseudoinverse of Hy. For a v ∈ Ker⊥ Hy, we write

L∗(v) = sup{(cid:104)H †

yHyu⊥, v(cid:105) − φ(Hyu⊥) | u⊥}

= sup{(cid:104)z, (H †

y)(cid:62)v(cid:105) − φ(z) | z ∈ Im Hy},

exp(uj)/(cid:0)1 +

exp(uj)(cid:1) = Z/(1 + Z).

(cid:88)

j(cid:54)=y

(cid:88)

j(cid:54)=y

vj =

(cid:88)

j(cid:54)=y

Therefore,

1 + Z = 1/(cid:0)1 − (cid:80)

j(cid:54)=y vj

(cid:1) = 1/(1 + vy),

which ﬁnally yields

L∗(v) = (cid:80)

j(cid:54)=y vj log(vj) + log(1 + vy)(1 + vy),

if (cid:104)1, v(cid:105) = 0 and v\y ∈ ∆ as stated in the proposition.

The conjugate of the top-k entropy loss is obtained by
replacing ∆ in (14) with ∆α
k . A β version could be obtained
using the ∆β
k instead, which defer to future work. There is
no closed-form solution for the primal top-k entropy loss
for k > 1, but we can evaluate it as follows.

where Im Hy = {Hyu | u ∈ Rm} = {u | uy = 0}. Using
rank-1 update of the pseudoinverse [37, § 3.2.7], we have

Proposition 9. Let uj (cid:44) fj(x) − fy(x) for all j ∈ Y. The
top-k entropy loss is deﬁned as

(H †

y)(cid:62) = I − eye(cid:62)

y −

(1 − ey)1(cid:62),

1
m

which together with (cid:104)1, v(cid:105) = 0 implies

(H †

y)(cid:62)v = v − vyey.

Therefore,

L∗(v) = sup{(cid:104)u, v − vyey(cid:105) − φ(u) | uy = 0}

(cid:110)

= sup

(cid:104)u\y, v\y(cid:105) − log

1 +

(cid:16)

exp(uj)

(cid:17)(cid:111)
.

(cid:88)

j(cid:54)=y

The function inside sup is concave and differentiable, hence
the global optimum is at the critical point [10]. Setting the
partial derivatives to zero yields

vj = exp(uj)/(cid:0)1 + (cid:80)

j(cid:54)=y exp(uj)(cid:1)

for j (cid:54)= y, from which we conclude, similar to [48, § 5.1],
that (cid:104)1, v(cid:105) ≤ 1 and 0 ≤ vj ≤ 1 for all j (cid:54)= y, i.e. v\y ∈ ∆.
Let Z (cid:44) (cid:80)
j(cid:54)=y exp(uj), we have at the optimum

uj = log(vj) + log(1 + Z),
Since (cid:104)1, v(cid:105) = 0, we also have that vy = − (cid:80)

∀j (cid:54)= y.

j(cid:54)=y vj, hence

L(u) = max(cid:8)(cid:104)u\y, x(cid:105) − (1 − s) log(1 − s)

− (cid:104)x, log x(cid:105) | x ∈ ∆α

k , (cid:104)1, x(cid:105) = s(cid:9).

(15)

Moreover, we recover the softmax loss (5) if k = 1.

Proof. The convex conjugate of the top-k entropy loss is

L∗(v) (cid:44)

j(cid:54)=y vj log vj + (1 + vy) log(1 + vy),
if (cid:104)1, v(cid:105) = 0 and v\y ∈ ∆α
k ,

+∞ otherwise,

(cid:80)






where the setting is the same as in Proposition 8. The (pri-
mal) top-k entropy loss is deﬁned as the convex conjugate
of the L∗(v) above. We have

L(u) = sup{ (cid:104)u, v(cid:105) − L∗(v) | v ∈ Rm}

= sup{ (cid:104)u, v(cid:105) −

vj log vj − (1 + vy) log(1 + vy)

(cid:88)

j(cid:54)=y
| (cid:104)1, v(cid:105) = 0, v\y ∈ ∆α
k }

= sup{(cid:104)u\y, v\y(cid:105) − uy

(cid:88)

vj −

vj log vj

(cid:88)

j(cid:54)=y

− (1 −

vj) log(1 −

vj) | v\y ∈ ∆α

k }.

(cid:88)

j(cid:54)=y

j(cid:54)=y
(cid:88)

j(cid:54)=y

L∗(v) = (cid:80)
= (cid:80)

j(cid:54)=y ujvj − log(1 + Z)

j(cid:54)=y vj log(vj) + log(1 + Z)

(cid:16) (cid:88)

(cid:17)

vj − 1

j(cid:54)=y

Note that uy = 0, and hence the corresponding term van-
ishes. Finally, we let x (cid:44) v\y and s (cid:44) (cid:80)
j(cid:54)=y vj = (cid:104)1, x(cid:105)
and obtain (15).

Next, we discuss how this problem can be solved and
show that it reduces to the softmax loss for k = 1. Let
a (cid:44) u\y and consider an equivalent problem below.
L(u) = − min (cid:8) (cid:104)x, log x(cid:105) + (1 − s) log(1 − s)
k , (cid:104)1, x(cid:105) = s(cid:9).

− (cid:104)a, x(cid:105) | x ∈ ∆α

(16)

The Lagrangian for (16) is

L = (cid:104)x, log x(cid:105) + (1 − s) log(1 − s) − (cid:104)a, x(cid:105)

+ t((cid:104)1, x(cid:105) − s) + λ(s − 1) − (cid:104)µ, x(cid:105) + (cid:10)ν, x − s

k 1(cid:11) ,
where t ∈ R and λ, µ, ν ≥ 0 are the dual variables. Com-
puting the partial derivatives of L w.r.t. xj and s, and setting
them to zero, we obtain

log xj = aj − 1 − t + µj − νj,
k (cid:104)1, ν(cid:105) + λ.

log(1 − s) = −1 − t − 1

∀j

Note that xj = 0 and s = 1 cannot satisfy the above con-
ditions for any choice of the dual variables in R. Therefore,
xj > 0 and s < 1, which implies µj = 0 and λ = 0. The
only constraint that might be active is xj ≤ s
k . Note, how-
ever, that in view of xj > 0 it can only be active if either
k > 1 or we have a one dimensional problem. We consider
the case when this constraint is active below.
Consider xj’s for which 0 < xj < s

k holds at the opti-
mum. The complementary slackness conditions imply that
the corresponding µj = νj = 0. Let p (cid:44) (cid:104)1, ν(cid:105) and re-
deﬁne t as t ← 1 + t. We obtain the simpliﬁed equations

log xj = aj − t,
log(1 − s) = −t − p
k .

Taking into account the minus in front of the min in (16)
and the deﬁnition of a, we ﬁnally recover the softmax loss

L(y, f (x)) = log (cid:0)1 + (cid:80)

j(cid:54)=y exp(fj(x) − fy(x))(cid:1).

The non-analytic nature of the loss for k > 1 does not
allow us to check if it is top-k calibrated. We now show
how this problem can be solved efﬁciently.

How to solve (15). We continue the derivation started in
the proof of Propostion 9. First, we write the system that
follows directly from the KKT [10] optimality conditions.

xj = min{exp(aj − t), s
k },
νj = max{0, aj − t − log( s

∀j,

k )},

∀j,

1 − s = exp(−t − p
s = (cid:104)1, x(cid:105) ,

k ),
p = (cid:104)1, ν(cid:105) .

(17)

Next, we deﬁne the two index sets U and M as follows

U (cid:44) {j | xj = s

k },

M (cid:44) {j | xj < s

k }.

Note that the set U contains at most k indexes correspond-
ing to the largest components of aj. Now, we proceed with
ﬁnding a t that solves (17). Let ρ (cid:44) |U |
k . We eliminate p as
k )(cid:1) =⇒

aj − |U | (cid:0)t + log( s

νj =

p =

(cid:88)

(cid:88)

p

k = 1

k

aj − ρ(cid:0)t + log( s

k )(cid:1).

U

j
(cid:88)

U

Let Z (cid:44) (cid:80)

M exp aj, we write for s

(cid:88)

s =

xj =

(cid:88)

j

U

(cid:88)

M

exp(aj − t)

= ρs + exp(−t)

exp aj = ρs + exp(−t)Z.

s
k +

(cid:88)

M

If k = 1, then 0 < xj < s for all j in a multiclass problem
as discussed above, hence also p = 0. We have

xj = eaj −t,

1 − s = e−t,

where t ∈ R is to be found. Plugging that into the objective,

We conclude that

j(aj − t)eaj −t − te−t − (cid:80)

(cid:80)
= e−t(cid:104) (cid:80)
= −te−t(cid:2)1 + (cid:80)
= −t(cid:2)1 − s + s(cid:3) = −t.

j ajeaj −t
j ajeaj
j eaj (cid:3) = −t(cid:2)e−t + (cid:80)

j(aj − t)eaj − t − (cid:80)

(cid:105)

j eaj −t(cid:3)

(1 − ρ)s = exp(−t)Z =⇒

t = log Z − log (cid:0)(1 − ρ)s(cid:1).

Let α (cid:44) 1
k

(cid:80)

U aj. We further write

log(1 − s) = − t − p
k

To compute t, we note that

(cid:80)

j eaj −t = (cid:104)1, x(cid:105) = s = 1 − e−t,

from which we conclude

= − t − α + ρ(cid:0)t + log( s
k )(cid:1)
=ρ log( s
k ) − (1 − ρ)t − α
=ρ log( s
k ) − α

− (1 − ρ)(cid:2) log Z − log (cid:0)(1 − ρ)s(cid:1)(cid:3),

1 = (cid:0)1 +

(cid:88)

eaj (cid:1)e−t =⇒ −t = − log(1 + (cid:80)

j eaj ).

j

which yields the following equation for s

log(1 − s) − ρ(log s − log k) + α

+ (1 − ρ)(cid:2) log Z − log(1 − ρ) − log s(cid:3) = 0.

Therefore,

We ﬁnally get

log(1 − s) − log s + ρ log k + α

+ (1 − ρ) log Z − (1 − ρ) log(1 − ρ) = 0,

log

(cid:19)

(cid:18) 1 − s
s

= log

(cid:18) (1 − ρ)(1−ρ) exp(−α)
kρZ (1−ρ)

(cid:19)

.

s = 1/(1 + Q),
Q (cid:44) (1 − ρ)(1−ρ)/(kρZ (1−ρ)eα).

(18)

We note that: a) Q is readily computable once the sets U
and M are ﬁxed; and b) Q = 1/Z if k = 1 since ρ = α = 0
in that case. This yields the formula for t as

t = log Z + log(1 + Q) − log(1 − ρ).

(19)

As a sanity check, we note that we again recover the soft-
max loss for k = 1, since t = log Z + log(1 + 1/Z) =
log(1 + Z) = log(1 + (cid:80)
j exp aj).
To verify that the computed s and t are compatible with

the choice of the sets U and M , we check if this holds:

exp(aj − t) ≥ s
k ,
exp(aj − t) ≤ s
k ,

∀j ∈ U,

∀j ∈ M,

which is equivalent to

max
M

aj ≤ log( s

k ) + t ≤ min
U

aj.

(20)

Computation of the top-k entropy loss (15). The above
derivation suggests a simple and efﬁcient algorithm to com-
pute s and t that solve the KKT system (17) and, therefore,
the original problem (15).

1. Initialization: U ← {}, M ← {1, . . . , m}.

2. Z ← (cid:80)

M exp aj, α ← 1
k

(cid:80)

U aj, ρ ← |U |
k .

3. Compute s and t using (18) and (19).

4. If (20) holds, stop; otherwise, j ← arg maxM aj.

5. U ← U ∪ {j}, M ← M \ {j} and go to step 2.

Note that the algorithm terminates after at most k iterations
since |U | ≤ k. The overall complexity is therefore O(km).
To compute the actual loss (15), we note that if U is
empty, i.e. there were no violated constraints, then the top-k
entropy loss coincides with the softmax loss and is directly
given by t. Otherwise, we have

(cid:104)a, x(cid:105) − (cid:104)x, log x(cid:105) − (1 − s) log(1 − s)

aj

s
k +

(cid:88)

M

aj exp(aj − t) −

(cid:88)

U

s

k log( s
k )

(aj − t) exp(aj − t) − (1 − s) log(1 − s)

(cid:88)

U
(cid:88)

=

−

M

= αs − ρs log( s
= αs − ρs log( s

k ) + t exp(−t)Z − (1 − s) log(1 − s)
k ) + (1 − ρ)st − (1 − s) log(1 − s).

Therefore, the top-k entropy loss is readily computed once
the optimal s and t are found.

2.5. Truncated Top-k Entropy Loss

A major limitation of the softmax loss for top-k error
optimization is that it cannot ignore the highest scoring pre-
dictions, which yields a high loss even if the top-k error is
zero. This can be seen by rewriting (5) as

L(y, f (x)) = log (cid:0)1 +

exp(fj(x) − fy(x))(cid:1).

(21)

(cid:88)

j(cid:54)=y

If there is only a single j such that fj(x) − fy(x) (cid:29) 0, then
L(y, f (x)) (cid:29) 0 even though err2(y, f (x)) = 0.

This problem is is also present in all top-k hinge losses
considered above and is an inherent limitation due to their
convexity. The origin of the problem is the fact that ranking
based losses [54] are based on functions such as

φ(f (x)) = 1
m

(cid:80)

j∈Y αjfπj (x) − fy(x).

The function φ is convex if the sequence (αj) is monotoni-
cally non-increasing [10]. This implies that convex ranking
based losses have to put more weight on the highest scoring
classiﬁers, while we would like to put less weight on them.
To that end, we drop the ﬁrst (k − 1) highest scoring predic-
tions from the sum in (21), sacriﬁcing convexity of the loss,
and deﬁne the truncated top-k entropy loss as follows

L(y, f (x)) = log (cid:0)1 +

exp(fj(x) − fy(x))(cid:1),

(22)

(cid:88)

j∈Jy

where Jy are the indexes corresponding to the (m − k)
smallest components of (fj(x))j(cid:54)=y. This loss can be seen
as a smooth version of the top-k error (1), as it is small
whenever the top-k error is zero. Below, we show that this
loss is top-k calibrated.

Proposition 10. The truncated top-k entropy loss is top-s
calibrated for any k ≤ s ≤ m.

Proof. Given a g ∈ Rm, let π be a permutation such that
gπ1 ≥ gπ2 ≥ . . . ≥ gπm . Then, we have

(cid:40)

Jy =

{πk+1, . . . , πm}
{πk, . . . , πm} \ {y}

if y ∈ {π1, . . . , πk−1},
if y ∈ {πk, . . . , πm}.

Therefore, the expected loss at x can be written as

3. Optimization Method

EY |X [L(Y, g) | X = x] = (cid:80)
r=1 log (cid:0)1 + (cid:80)m
r=k log (cid:0) (cid:80)m

= (cid:80)k−1
+ (cid:80)m

y∈Y L(y, g) py(x)
j=k+1 egπj −gπr (cid:1) pπr (x)

j=k egπj −gπr (cid:1) pπr (x).

Note that the sum inside the logarithm does not depend on
gπr for r < k. Therefore, a Bayes optimal classiﬁer will
have gπr = +∞ for all r < k as then the ﬁrst sum vanishes.

Let p (cid:44) (py(x))y∈Y and q (cid:44) (L(y, g))y∈Y , then

In this section, we brieﬂy discuss how the proposed
smooth top-k hinge losses and the top-k entropy loss can be
optimized efﬁciently within the SDCA framework of [48].
The primal and dual problems. Let X ∈ Rd×n be
the matrix of training examples xi ∈ Rd, K = X (cid:62)X the
corresponding Gram matrix, W ∈ Rd×m the matrix of pri-
mal variables, A ∈ Rm×n the matrix of dual variables, and
λ > 0 the regularization parameter. The primal and Fenchel
dual [8] objective functions are given as

qπ1 = . . . = qπk−1 = 0 ≤ qπk ≤ . . . ≤ qπm

P (W ) = +

L (cid:0)yi, W (cid:62)xi

(cid:1) +

tr (cid:0)W (cid:62)W (cid:1) ,

and we can re-write the expected loss as

EY |X [L(Y, g) | X = x] = (cid:104)p, q(cid:105) = (cid:104)pπ, qπ(cid:105) ≥ (cid:104)pτ , qπ(cid:105) ,

where pτ1 ≥ pτ2 ≥ . . . ≥ pτm and we used the rearrange-
ment inequality. Therefore, the expected loss is minimized
when π and τ coincide (up to a permutation of the ﬁrst k −1
elements, since they correspond to zero loss).

We can also derive a Bayes optimal classiﬁer following

the proof of Proposition 4. We have

EY |X [L(Y, g) | X = x]

= (cid:80)m
= (cid:80)m

r=k log (cid:0) (cid:80)m
(cid:16)
log (cid:0) (cid:80)m

j=k egτj −gτr (cid:1) pτr (x)
(cid:17)
j=k egτj (cid:1) − gτr

r=k

pτr (x).

A critical point is found by setting partial derivatives to zero
for all y ∈ {τk, . . . , τm}, which leads to

egy
j=k egτj

(cid:80)m

(cid:80)m

r=k pτr (x) = py(x).

We let gy = −∞ if py(x) = 0, and obtain ﬁnally

g∗
τj

=






+∞
log (cid:0)αpτj (x)(cid:1)
−∞

if j < k,
if j ≥ k and pτj (x) > 0,
if j ≥ k and pτj (x) = 0,

as a Bayes optimal classiﬁer for any α > 0.

Note that g∗ preserves the ranking of py(x) for all y in

{τk, . . . , τm}, hence, it is top-s calibrated for all s ≥ k.

As the loss (22) is nonconvex, we use solutions obtained
with the softmax loss (5) as initial points and optimize them
further via gradient descent. However, the resulting opti-
mization problem seems to be “mildly nonconvex” as the
same-quality solutions are obtained from different initial-
In Section 4, we show a synthetic experiment,
izations.
where the advantage of discarding the highest scoring clas-
siﬁer in the loss becomes apparent.

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

λ
2

λ
2

D(A) = −

L∗ (yi, −λnai) −

tr (cid:0)AKA(cid:62)(cid:1) ,

(23)

where L∗ is the convex conjugate of L. SDCA proceeds by
randomly picking a variable ai (which in our case is a vec-
tor of dual variables over all m classes for a sample xi) and
modifying it to achieve maximal increase in the dual objec-
tive D(A). It turns out that this update step is equivalent
to a proximal problem, which can be seen as a regularized
projection onto the essential domain of L∗.

The convex conjugate. An important ingredient in the
SDCA framework is the convex conjugate L∗. We show
that for all multiclass loss functions that we consider the fact
that they depend on the differences fj(x) − fy(x) enforces
a certain constraint on the conjugate function.

Lemma 3. Let Hy = I − 1e(cid:62)
Φ∗(v) = +∞ unless (cid:104)1, v(cid:105) = 0.

y and let Φ(u) = φ(Hyu).

Proof. The proof follows directly from [28, Lemma 2] and
was already reproduced in the proof of Proposition 8 for the
softmax loss. We have formulated this simpliﬁed lemma
since [28, Lemma 2] additionally required y-compatibility
to show that if (cid:104)1, v(cid:105) = 0, then Φ∗(v) = φ∗(v − vyey),
which does not hold e.g. for the softmax loss.

j(cid:54)=yi

Lemma 3 tells us that we need to enforce (cid:104)1, ai(cid:105) = 0 at
all times, which translates into ayi = − (cid:80)
aj. The up-
date steps are performed on the (m − 1)-dimensional vector
obtained by removing the coordinate ayi.
The update step for top-k SVMα

γ . Let a\y be obtained
by removing the y-th coordinate from vector a. We show
that performing an update step for the smooth top-k hinge
loss is equivalent to projecting a certain vector b, computed
from the prediction scores W (cid:62)xi, onto the essential do-
main of L∗, the top-k simplex, with an added regularization
ρ (cid:104)1, x(cid:105)2, which biases the solution to be orthogonal to 1.

Proposition 11. Let L and L∗ in (23) be respectively the
top-k SVMα
γ loss and its conjugate as in Proposition 7.

The update maxai{D(A) | (cid:104)1, ai(cid:105) = 0} is equivalent with
the change of variables x ↔ −a\yi

to solving

i

min
x

{(cid:107)x − b(cid:107)2 + ρ (cid:104)1, x(cid:105)2 | x ∈ ∆α

k ( 1

λn )},

(24)

(cid:0)q\yi + (1 − qyi)1(cid:1),
where b =
q = W (cid:62)xi − (cid:104)xi, xi(cid:105) ai, and ρ = (cid:104)xi,xi(cid:105)

1
(cid:104)xi,xi(cid:105)+γnλ

(cid:104)xi,xi(cid:105)+γnλ .

Proof. We follow the proof of [28, Proposition 4]. We
choose i ∈ {1, . . . , n} and, having all other variables ﬁxed,
update ai to maximize

− 1

n L∗ (yi, −λnai) − λ

2 tr (cid:0)AKA(cid:62)(cid:1) .

For the nonsmooth top-k hinge loss, it was shown [28] that

L∗ (yi, −λnai) = (cid:104)c, λn(ai − ayi,ieyi)(cid:105)

if −λn(ai − ayi,ieyi) ∈ ∆α
the smoothed loss, we add regularization and obtain

k and +∞ otherwise. Now, for

− 1
n

(cid:16) γ
(cid:17)
2 (cid:107)−λn(ai − ayi,ieyi)(cid:107)2 + (cid:104)c, λn(ai − ayi,ieyi)(cid:105)

with −λn(ai − ayi,ieyi) ∈ ∆α
(cid:104)1, ai(cid:105) = 0, one can simplify it to

k . Using c = 1 − eyi and

−

γnλ2
2

(cid:13)
(cid:13)a\yi

i

(cid:13)
2
(cid:13)

+ λayi,i,

and the feasibility constraint can be re-written as

−a\yi

i ∈ ∆α

λn ),
For the regularization term tr (cid:0)AKA(cid:62)(cid:1), we have

ayi,i = (cid:104)1, −a\yi

k ( 1

i

(cid:105).

We let q = (cid:80)

j(cid:54)=i Kijaj = AKi − Kiiai and x = −a\yi

i

:

(cid:104)ai, ai(cid:105) = (cid:104)1, x(cid:105)2 + (cid:104)x, x(cid:105) ,
(cid:104)q, ai(cid:105) = qyi (cid:104)1, x(cid:105) − (cid:104)q\yi, x(cid:105).

Now, we plug everything together and multiply with −2/λ.

min
k ( 1

λn )

x∈∆α

γnλ (cid:107)x(cid:107)2 − 2 (cid:104)1, x(cid:105) + 2(cid:0)qyi (cid:104)1, x(cid:105) − (cid:104)q\yi, x(cid:105)(cid:1)

+ Kii

(cid:0) (cid:104)1, x(cid:105)2 + (cid:104)x, x(cid:105) (cid:1).

Collecting the corresponding terms ﬁnishes the proof.

Note that setting γ = 0, we recover the update step for
the non-smooth top-k hinge loss [28]. It turns out that we
can employ their projection procedure for solving (24) with
only a minor modiﬁcation of b and ρ.

k in (24) instead of ∆α

The update step for the top-k SVMβ

γ loss is derived sim-
ilarly using the set ∆β
k . The resulting
projection problem is a biased continuous quadratic knap-
sack problem, which is discussed in the supplement of [28].
Smooth top-k hinge losses converge signiﬁcantly faster
than their nonsmooth variants as we show in the scaling ex-
periments below. This can be explained by the theoretical
results of [48] on the convergence rate of SDCA. They also
had similar observations for the smoothed binary hinge loss.
The update step for top-k Ent. We now discuss the op-
timization of the proposed top-k entropy loss in the SDCA
framework. Note that the top-k entropy loss reduces to the
softmax loss for k = 1. Thus, our SDCA approach can
be used for gradient-free optimization of the softmax loss
without having to tune step sizes or learning rates.

Proposition 12. Let L in (23) be the top-k Ent loss (15)
and L∗ be its convex conjugate as in (14) with ∆ replaced
by ∆α
k . The update maxai{D(A) | (cid:104)1, ai(cid:105) = 0} is equiva-
lent with the change of variables x ↔ −λna\yi
to solving

i

min
x∈∆α
k

α

2 ((cid:104)x, x(cid:105) + (cid:104)1, x(cid:105)2) − (cid:104)b, x(cid:105) +

(cid:104)x, log x(cid:105) + (1 − (cid:104)1, x(cid:105)) log(1 − (cid:104)1, x(cid:105))

(25)

where α = (cid:104)xi,xi(cid:105)

λn , b = q\yi −qyi1, q = W (cid:62)xi−(cid:104)xi, xi(cid:105) ai.

Proof. Let v (cid:44) −λnai and y = yi. Using Proposition 8,

L∗(y, v) = (cid:80)

j(cid:54)=y vj log vj + (1 + vy) log(1 + vy),

where (cid:104)1, v(cid:105) = 0 and v\y ∈ ∆α
It follows that s = (cid:104)1, x(cid:105) and from tr (cid:0)AKA(cid:62)(cid:1) we get

k . Let x (cid:44) v\y and s (cid:44) −vy.

where q = (cid:80)
we plug everything together as in Proposition 11.

j(cid:54)=i Kijaj = AKi − Kiiai as before. Finally,

Note that this optimization problem is similar to (24), but
is more difﬁcult to solve due to the presence of logarithms
in the objective. We propose to tackle this problem using
the Lambert W function introduced below.

Lambert W function. The Lambert W function is de-
ﬁned to be the inverse of the function w (cid:55)→ wew and is
widely used in many ﬁelds [14, 18, 55]. Taking logarithms
on both sides of the deﬁning equation z = W eW , we obtain
log z = W (z) + log W (z). Therefore, if we are given an
equation of the form x + log x = t for some t ∈ R, we
can directly “solve” it in closed-form as x = W (et). The
crux of the problem is that the function V (t) (cid:44) W (et) is
transcendental [18] just like the logarithm and the exponent.
There exist highly optimized implementations for the latter
and we argue that the same can be done for the Lambert W

tr (cid:0)AKA(cid:62)(cid:1) = Kii (cid:104)ai, ai(cid:105) + 2

Kij (cid:104)ai, aj(cid:105) + const.

Kii((cid:104)x, x(cid:105) + s2)/(λn)2 − 2(cid:10)q\y − qy1, x(cid:11)/(λn),

(cid:88)

j(cid:54)=i

and λ = 0. We re-write the above as

αxi + log(αxi) = ai − 1 − t + log α − νi,
α(1 − s) + log (cid:0)α(1 − s)(cid:1) = α − 1 − t + log α − (cid:104)1,ν(cid:105)
k .

Note that these equations correspond to the Lambert W
function of the exponent, i.e. V (t) = W (et) discussed
above. Let p (cid:44) (cid:104)1, ν(cid:105) and re-deﬁne t ← 1 + t − log α.

αxi = W (cid:0) exp(ai − t − νi)(cid:1),
k )(cid:1).

α(1 − s) = W (cid:0) exp(α − t − p

Finally, we obtain the following system:

xi = min{ 1

α V (ai − t), s

∀i,

k },
∀i,

αxi = V (ai − t − νi),
k ),

α(1 − s) = V (α − t − p
s = (cid:104)1, x(cid:105) ,

p = (cid:104)1, ν(cid:105) .

Note that V (t) is a strictly monotonically increasing func-
tion, therefore, it is invertible and we can write

ai − t − νi = V −1(αxi),
α − t − p

k = V −1(cid:0)α(1 − s)(cid:1).

Next, we deﬁned the sets U and M as before and write

s = (cid:104)1, x(cid:105) = (cid:80)
p = (cid:104)1, ν(cid:105) = (cid:80)

s

U

M

k + (cid:80)

1
α V (ai − t),
U ai − |U | (cid:0)t + V −1( αs
(cid:80)

U ai, we get

k )(cid:1).

Let ρ (cid:44) |U |

k as before and A (cid:44) 1

k

(cid:80)

(1 − ρ)s = 1
α
p

M V (ai − t),
k = A − ρ(cid:0)t + V −1( αs
Finally, we eliminate p and obtain a system in two variables,

k )(cid:1).

α(1 − ρ)s − (cid:80)
(1 − ρ)t + V −1(cid:0)α(1 − s)(cid:1) − ρV −1( αs

M V (ai − t) = 0,

k ) + A − α = 0,

which can be solved using the Newton’s method [36].
Moreover, when U is empty, the system above simpliﬁes
into a single equation in one variable t

V (α − t) + (cid:80)

M V (ai − t) = α,

(a) V (t) ≈ et for t (cid:28) 0.

(b) V (t) ≈ t − log t for t (cid:29) 0.

Figure 1: Behavior of the Lambert W function of the expo-
nent (V (t) = W (et)). (a) Log scale plot with t ∈ (−10, 0).
(b) Linear scale plot with t ∈ (0, 10).

function. In fact, there is already some work on this topic
[18, 55], which we also employ in our implementation.

To develop intuition concerning the Lambert W function
of the exponent, we now brieﬂy discuss how the function
V (t) = W (et) behaves for different values of t. An illus-
tration is provided in Figure 1. One can see directly from
the equation x + log x = t that the behavior of x = V (t)
changes dramatically depending on whether t is a large pos-
itive or a large negative number. In the ﬁrst case, the linear
part dominates the logarithm and the function is approxi-
mately linear; a better approximation is x(t) ≈ t − log t,
when t (cid:29) 1. In the second case, the function behaves like
an exponent et. To see this, we write x = ete−x and note
that e−x ≈ 1 when t (cid:28) 0, therefore, x(t) ≈ et, if t (cid:28) 0.
We use these approximations as initial points for a 5-th or-
der Householder method [22], which was also used in [18].
A single iteration is already sufﬁcient to get full float pre-
cision and at most two iterations are needed for double.

How to solve (25). We present a similar derivation as
was already done for the problem (15) above. The main dif-
ference is that we now encounter the Lambert W function
in the optimality conditions. We re-write the problem as

min (cid:8) α

2 ((cid:104)x, x(cid:105) + s2) − (cid:104)a, x(cid:105) + (cid:104)x, log x(cid:105)
+ (1 − s) log(1 − s) | s = (cid:104)1, x(cid:105) , x ∈ ∆α
k

(cid:9).

The Lagrangian is given by

L = α

2 ((cid:104)x, x(cid:105) + s2) − (cid:104)a, x(cid:105) + (cid:104)x, log x(cid:105)
+ (1 − s) log(1 − s) + t((cid:104)1, x(cid:105) − s)
+ λ(s − 1) − (cid:104)µ, x(cid:105) + (cid:10)ν, x − s
k 1(cid:11) ,

where t ∈ R, λ, µ, ν ≥ 0 are the dual variables. Computing
partial derivatives of L w.r.t. xi and s, and setting them to
zero, we obtain

αxi + log xi = ai − 1 − t + µi − νi,

α(1 − s) + log(1 − s) = α − 1 − t − λ − 1

∀i,
k (cid:104)1, ν(cid:105) ,

∀i.

Note that xi > 0 and s < 1 as before, which implies µi = 0

which can be solved efﬁciently using the Householder’s
method [22]. As both methods require derivatives of V (t),
we note that ∂tV (t) = V (t)/(1 + V (t)) [14]. Therefore,
V (ai − t) is only computed once for each ai − t and then
re-used to also compute the derivatives.

The efﬁciency of this approach crucially depends on fast
computation of V (t). Our implementation was able to scale
the training procedure to large datasets as we show next.

(a) Relative duality gap vs. time

(b) Top-1 accuracy vs. time

Figure 2: SDCA convergence with LRMulti, SVMMulti,
and top-1 SVMα

1 objectives on ILSVRC 2012.

Figure 3: Convergence rate
of SDCA (ours) and the
SPAMS toolbox [33].

Runtime. We compare
the wall-clock runtime of
the top-1 multiclass SVM
[28] (SVMMulti) with our
smooth multiclass SVM
(smooth SVMMulti)
and
the softmax loss (LRMulti)
objectives in Figure 2. We
plot the relative duality gap
(P (W ) − D(A))/P (W )
and the validation accuracy
versus time for the best per-
forming models on ILSVRC 2012. We obtain substantial
improvement of the convergence rate for smooth top-1
SVM compared to the non-smooth baseline. Moreover,
top-1 accuracy saturates after a few passes over the training
data, which justiﬁes the use of a fairly loose stopping
criterion (we used 10−3). For LRMulti, the cost of each
epoch is signiﬁcantly higher compared to the top-1 SVMs,
which is due to the difﬁculty of solving (25). This suggests
that one can use the smooth top-1 SVMα
1 and obtain
competitive performance (see § 5) at a lower training cost.
We also compare our implementation LRMulti (SDCA)
with the SPAMS optimization toolbox [33], denoted
LRMulti (SPAMS), which provides an efﬁcient implemen-
tation of FISTA [4]. We note that the rate of convergence
of SDCA is competitive with FISTA for (cid:15) ≥ 10−4 and is
noticeably better for (cid:15) < 10−4. We conclude that our ap-
proach is competitive with the state-of-the-art, and faster
computation of V (t) would lead to a further speedup.

Gradient-based optimization. Finally, we note that the
proposed smooth top-k hinge and the truncated top-k en-
tropy losses are easily amenable to gradient-based optimiza-
tion, in particular, for training deep architectures (see § 5).
The computation of the gradient of (22) is straightforward,
while for the smooth top-k hinge loss (12) we have

∇Lγ(a) = 1

γ proj∆α

k (γ)(a + c),

which follows from the fact that Lγ(a) can be written as
2γ ((cid:107)x(cid:107)2 − (cid:107)x − p(cid:107)2) for x = a + c and p = proj∆α
1
k (γ)(x),

(a) top-1 SVM1 test accuracy
(top-1 / top-2): 65.7% / 81.3%

(b) top-2 Enttr test accuracy
(top-1 / top-2): 29.4%, 96.1%

Figure 4: Synthetic data on the unit circle in R2 (inside
black circle) and visualization of top-1 and top-2 predic-
tions (outside black circle). (a) Smooth top-1 SVM1 opti-
mizes top-1 error which impedes its top-2 error. (b) Trunc.
top-2 entropy loss ignores top-1 scores and optimizes di-
rectly top-2 errors leading to a much better top-2 result.

and a known result from convex analysis [8, § 3, Ex. 12.d]
which postulates that ∇x

2 (cid:107)x − PC(x)(cid:107)2 = x − PC(x).

1

4. Synthetic Example

In this section, we demonstrate in a synthetic experiment
that our proposed top-2 losses outperform the top-1 losses
when one aims at optimal top-2 performance. The dataset
with three classes is shown in the inner circle of Figure 4.

Sampling. First, we generate samples in [0, 7] which is
subdivided into 5 segments. All segments have unit length,
except for the 4-th segment which has length 3. We sample
uniformly at random in each of the 5 segments according to
the following class-conditional probabilities: (0, 1, .4, .3, 0)
for class 1, (1, 0, .1, .7, 0) for class 2, and (0, 0, .5, 0, 1) for
class 3. Finally, the data is rescaled to [0, 1] and mapped
onto the unit circle.

Circle (synthetic)

Method

Top-1 Top-2 Method

Top-1

Top-2

SVMOVA
54.3 85.8
LROVA
54.7 81.7
SVMMulti 58.9 89.3
LRMulti
54.7 81.7

65.7

top-1 SVM1
top-2 SVM0/1 54.4 / 54.5 87.1 / 87.0
top-2 Ent
top-2 Enttr

87.6
96.1

54.6
58.4

83.9

Table 2: Top-k accuracy (%) on synthetic data. Left: Base-
lines methods. Right: Top-k SVM (nonsmooth / smooth)
and top-k softmax losses (convex and nonconvex).

Samples of different classes are plotted next to each other
for better visibility as there is signiﬁcant class overlap. We
visualize top-1/2 predictions with two colored circles (out-
side the black circle). We sample 200/200/200K points for
training/validation/testing and tune the C = 1
λn parameter
in the range 2−18 to 218. Results are in Table 2.

State-of-the-art

ALOI
93 ± 1.2 [44]

Letter
97.98 [23] (RBF kernel)

News 20
86.9 [42]

Caltech 101 Silhouettes
62.1
[51]

79.6

83.4

Method

Top-1 Top-3 Top-5 Top-10 Top-1 Top-3 Top-5 Top-10 Top-1 Top-3 Top-5 Top-10 Top-1 Top-3 Top-5 Top-10

SVMOVA
LROVA

SVMMulti
LRMulti

82.4
86.1

90.0
89.8

89.2
top-3 SVM
top-5 SVM
87.3
top-10 SVM 85.0

89.5
93.0

95.1
95.7

95.5
95.6
95.5

91.5
94.8

96.7
97.1

97.2
97.4
97.3

top-1 SVM1
top-3 SVM1
top-5 SVM1
top-10 SVM1

96.7
90.6 95.5
95.7
89.6
97.3
95.7 97.5
87.6
97.4
95.6
85.2

top-3 Ent
top-5 Ent
top-10 Ent

89.0
87.9
86.0

95.8
95.8
95.6

97.2
97.2
97.3

top-3 Enttr
top-5 Enttr
top-10 Enttr

89.3 95.9 97.3
97.3
95.7
87.9
97.1
94.8
85.2

93.7
96.6

98.1
98.4

98.4
98.6
98.7

98.2
98.4
98.6
98.7

98.4
98.4
98.5

98.5
98.6
98.5

63.0
68.1

76.5
75.3

82.0
86.1

89.2
90.3

88.1
90.6

93.1
94.3

74.0
94.4
91.0
70.8 91.5 95.1
96.0
88.9
61.6

93.6
76.8 89.9
74.1
94.5
90.9
70.8 91.5 95.2
95.9
89.1
61.7

73.0
69.7
65.0

63.6
50.3
46.5

90.8
94.9
95.1
90.9
89.7 96.2

91.1
87.7
80.9

95.6
96.1
93.7

94.6
96.2

97.7
98.0

97.8
98.4
99.6

97.6
97.9
98.6
99.7

98.5
98.8
99.6

98.8
99.4
99.6

84.3
84.9

85.4
84.5

85.1
84.3
82.7

95.4
96.3

94.9
96.4

96.6
96.7
96.5

85.6 96.3
96.6
85.1
96.7
84.5
96.5
82.9

97.9
97.8

97.2
98.1

98.2
98.4
98.4

98.0
98.4
98.4
98.4

84.7
98.3
96.6
84.3 96.8 98.6
98.5
96.4
82.7

83.4
83.2
82.9

96.4
96.0
95.7

98.3
98.2
97.9

99.5
99.3

99.1
99.5

99.3
99.3
99.3

99.3
99.4
99.4
99.5

99.4
99.4
99.4

99.4
99.4
99.4

61.8
63.2

76.5
80.4

80.8
84.4

62.8
82.0
77.8
63.2 81.2 85.1

63.4
63.3
63.0

79.7
80.0
80.5

63.9 80.3
80.1
63.3
80.5
63.3
80.5
63.1

83.6
84.3
84.6

84.0
84.0
84.5
84.8

81.1
85.0
85.2
80.9
80.8 85.4

63.3
63.2
62.5

60.7
58.3
51.9

86.6
89.4

86.9
89.7

88.3
88.7
89.1

89.0
89.2
89.1
89.1

89.9
89.9
90.1

Indoor 67
82.0 [58]

CUB
62.8 [13] / 76.37 [61]

Flowers
86.8 [40]

81.1
79.8
78.4

85.2
85.2
84.6

90.2
90.2
90.2

FMD
77.4 [13] / 82.4 [13]

Method

Top-1 Top-3 Top-5 Top-10 Top-1 Top-3 Top-5 Top-10 Top-1 Top-3 Top-5 Top-10 Top-1 Top-3 Top-5

top-3 SVM
81.6
79.9
top-5 SVM
top-10 SVM 78.4

State-of-the-art

SVMOVA
LROVA

SVMMulti
LRMulti

top-1 SVM1
top-3 SVM1
top-5 SVM1
top-10 SVM1

top-3 Ent
top-5 Ent
top-10 Ent

top-3 Enttr
top-5 Enttr
top-10 Enttr

81.9
82.0

82.5
82.4

82.6
81.6
80.4
78.3

81.4
80.3
79.2

79.8
76.4
72.6

94.3
94.9

95.4
95.2

95.1
95.0
95.1

95.2
95.1
95.1
95.1

95.4
95.0
95.1

95.0
94.3
92.8

96.5
97.2

97.3
98.0

97.7
97.7
97.4

97.6
97.8
97.8
97.5

97.6
97.7
97.6

97.5
97.3
97.1

98.0
98.7

99.1
99.1

99.0
99.0
99.0

99.0
99.0
99.1
99.0

99.2
99.0
99.0

99.1
99.0
98.9

60.6
62.3

61.0
62.3

61.3
60.9
59.6

61.9
61.9
61.3
59.8

62.5
62.0
61.2

62.0
61.4
59.7

77.1
80.5

79.2
81.7

80.4
81.2
81.3

80.2
81.1
81.3
81.4

81.8
81.9
81.6

81.4
81.2
80.7

83.4
87.4

85.7
87.9

86.3
87.2
87.7

86.9
86.6
87.4
87.8

87.9
88.1
88.2

87.6
87.7
87.2

89.9
93.5

92.3
93.9

92.5
92.9
93.4

93.1
93.2
92.9
93.4

93.9
93.8
93.8

93.4
93.7
93.4

82.0
82.6

82.5
82.9

81.9
81.7
80.5

83.0
82.5
82.0
80.6

82.5
82.1
80.9

82.1
81.4
77.9

94.3
94.8

94.8
95.1

95.0
95.1
95.1

95.1
95.2
95.1
95.1

95.3
95.1
95.0

95.2
95.0
94.3

96.8
97.6

96.4
97.8

96.1
97.8
97.7

97.6
97.7
97.8
97.7

97.8
97.9
97.7

97.6
97.7
97.3

77.4
79.6

77.6
79.0

78.8
78.4

78.6
79.0
79.4

92.4
94.2

93.8
94.6

94.6
94.4

93.8
94.4
94.4

96.4
98.2

97.2
97.8

97.8
97.6

98.0
98.0
97.6

79.8
79.4

94.8
94.4

98.0
98.0

78.4
77.2

95.4 98.2
97.8
94.0

SUN 397 (10 splits)
66.9 [58]

Places 205 (val)

ILSVRC 2012 (val)

60.6

88.5

[58]

76.3

93.2

[50]

Method

Top-1

Top-3

Top-5

Top-10

Top-1

Top-3

Top-5

Top-10

Top-1

Top-3

Top-5

Top-10

65.8 ± 0.1
67.5 ± 0.1

85.1 ± 0.2
87.7 ± 0.2

90.8 ± 0.1
92.9 ± 0.1

95.3 ± 0.1
96.8 ± 0.1

66.5 ± 0.2
66.3 ± 0.2
64.8 ± 0.3

67.4 ± 0.2
67.0 ± 0.2
66.5 ± 0.2
64.9 ± 0.3

67.2 ± 0.2
66.6 ± 0.3
65.2 ± 0.3

86.5 ± 0.1
87.0 ± 0.2
87.2 ± 0.2

86.8 ± 0.1
87.0 ± 0.1
87.2 ± 0.1
87.3 ± 0.2

91.8 ± 0.1
92.2 ± 0.2
92.6 ± 0.1

92.0 ± 0.1
92.2 ± 0.1
92.4 ± 0.2
92.6 ± 0.2

95.9 ± 0.1
96.3 ± 0.1
96.6 ± 0.1

96.1 ± 0.1
96.2 ± 0.0
96.3 ± 0.0
96.6 ± 0.1

87.7 ± 0.2
87.7 ± 0.2
87.4 ± 0.1

92.9 ± 0.1
92.9 ± 0.1
92.8 ± 0.1

96.8 ± 0.1
96.8 ± 0.1
96.8 ± 0.1

58.4
59.0

58.6
58.4
58.0

59.2
58.9
58.5
58.0

58.7
58.1
57.0

78.7
80.6

80.3
80.5
80.4

80.5
80.5
80.5
80.4

80.6
80.4
80.0

89.9
94.3

93.3
94.0
94.3

93.8
93.9
94.1
94.3

94.2
94.2
94.1

68.3
67.2

68.2
67.8
67.0

68.7
68.2
67.9
67.1

66.8
66.5
65.8

82.9
83.2

84.0
84.1
83.8

83.9
84.1
84.1
83.8

83.1
83.0
82.8

87.0
87.7

88.1
88.2
88.3

88.0
88.2
88.4
88.3

87.8
87.7
87.6

91.1
92.2

92.1
92.4
92.6

92.1
92.3
92.5
92.6

92.2
92.2
92.1

State-of-the-art

SVMMulti
LRMulti

top-3 SVM
top-5 SVM
top-10 SVM

top-1 SVM1
top-3 SVM1
top-5 SVM1
top-10 SVM1

top-3 Ent
top-5 Ent
top-10 Ent

91.7
92.2

92.2
92.4

92.2
92.4
91.9

92.4
92.3
92.5
91.9

92.0
92.2
92.1

92.2
92.0
91.1

84.7
87.6

87.3
87.4
87.4

87.3
87.6
87.5
87.5

87.6
87.4
87.2

Table 3: Top-k accuracy (%) on various datasets. The ﬁrst line is a reference to the state-of-the-art on each dataset and reports
top-1 accuracy except when the numbers are aligned with Top-k. We compare the one-vs-all and multiclass baselines with
the top-k SVMα [28] as well as the proposed smooth top-k SVMα

γ , top-k Ent, and the nonconvex top-k Enttr.

In each column we provide the results for the model
that optimizes the corresponding top-k accuracy, which is
in general different for top-1 and top-2. First, we note that
all top-1 baselines perform similar in top-1 performance,
except for SVMMulti and top-1 SVM1 which show better
results. Next, we see that our top-2 losses improve the top-
2 accuracy and the improvement is most signiﬁcant for the
nonconvex top-2 Enttr loss, which is close to the optimal
solution for this dataset. This is because top-2 Enttr is a
tight bound on the top-2 error and ignores top-1 errors in
the loss. Unfortunately, similar signiﬁcant improvements
were not observed on the real-world data sets that we tried.

5. Experimental Results

The goal of this section is to provide an extensive empir-
ical evaluation of the top-k performance of different losses
in multiclass classiﬁcation. To this end, we evaluate the
loss functions introduced in § 2 on 11 datasets (500 to 2.4M
training examples, 10 to 1000 classes), from various prob-
lem domains (vision and non-vision; ﬁne-grained, scene
and general object classiﬁcation). The detailed statistics of
the datasets is given in Table 4.

Dataset

m

n

d

Dataset

m

n

d

1K 54K 128

ALOI [44]
Caltech 101 Sil [51] 101 4100 784 Letter [23]
CUB [57]
Flowers [35]
FMD [49]
ILSVRC 2012 [47]

67
4K
5354
26 10.5K 16
202 5994 4K News 20 [27]
20 15.9K 16K
102 2040 4K Places 205 [62] 205 2.4M 4K
10
397 19.9K 4K
500
1K 1.3M 4K

4K SUN 397 [60]

Indoor 67 [38]

Table 4: Statistics of the datasets used in the experiments
(m – # classes, n – # training examples, d – # features).

Please refer to Table 1 for an overview of the methods
and our naming convention. A broad selection of results
is also reported at the end of the paper. As other ranking
based losses did not perform well in [28], we do no further
comparison here.

Solvers. We use LibLinear [17] for the one-vs-all
baselines SVMOVA and LROVA; and our code from [28] for
top-k SVM. We extended the latter to support the smooth
top-k SVMγ and top-k Ent. The multiclass loss base-
lines SVMMulti and LRMulti correspond respectively to
top-1 SVM and top-1 Ent. For the nonconvex top-k Enttr,
we use the LRMulti solution as an initial point and perform
gradient descent with line search. We cross-validate hyper-
parameters in the range 10−5 to 103, extending it when the
optimal value is at the boundary.

Features. For ALOI, Letter, and News20 datasets, we
use the features provided by the LibSVM [12] datasets. For
ALOI, we randomly split the data into equally sized train-
ing and test sets preserving class distributions. The Letter
dataset comes with a separate validation set, which we used

for model selection only. For News20, we use PCA to re-
duce dimensionality of sparse features from 62060 to 15478
preserving all non-singular PCA components4.

For Caltech101 Silhouettes, we use the features and the

train/val/test splits provided by [51].

For CUB, Flowers, FMD, and ILSVRC 2012, we use
MatConvNet [56] to extract the outputs of the last fully
connected layer of the imagenet-vgg-verydeep-16
model which is pre-trained on ImageNet [16] and achieves
state-of-the-art results in image classiﬁcation [50].

For Indoor 67, SUN 397, and Places 205, we use the
Places205-VGGNet-16 model by [58] which is pre-
trained on Places 205 [62] and outperforms the ImageNet
pre-trained model on scene classiﬁcation tasks [58]. Fur-
ther results can be found at the end of the paper. In all cases
we obtain a similar behavior in terms of the ranking of the
considered losses as discussed below.

Discussion. The experimental results are given in Ta-
ble 3. There are several interesting observations that one
can make. While the OVA schemes perform quite similar to
the multiclass approaches (logistic OVA vs. softmax, hinge
OVA vs. multiclass SVM), which conﬁrms earlier obser-
vations in [2, 43], the OVA schemes performed worse on
ALOI and Letter. Therefore it seems safe to recommend to
use multiclass losses instead of the OVA schemes.

Comparing the softmax vs. multiclass SVM losses, we
see that there is no clear winner in top-1 performance, but
softmax consistently outperforms multiclass SVM in top-k
performance for k > 1. This might be due to the strong
property of softmax being top-k calibrated for all k. Please
note that this trend is uniform across all datasets, in par-
ticular, also for the ones where the features are not com-
ing from a convnet. Both the smooth top-k hinge and the
top-k entropy losses perform slightly better than softmax if
one compares the corresponding top-k errors. However, the
good performance of the truncated top-k loss on synthetic
data does not transfer to the real world datasets. This might
be due to a relatively high dimension of the feature spaces,
but requires further investigation.

Fine-tuning experiments. We also performed a number
of ﬁne-tuning experiments where the original network was
trained further for 1-3 epochs with the smooth top-k hinge
and the truncated top-k entropy losses5. The motivation was
to see if the full end-to-end training would be more ben-
eﬁcial compared to training just the classiﬁer. Results are
reported in Table 5. First, we note that the setting is now
there is no feature extraction step with
slightly different:
the MatConvNet and there is a non-regularized bias term
in Caffe [24]. Next, we see that the top-k speciﬁc losses
are able to improve the performance compared to the refer-
ence model, and that the top-5 SVM1 loss achieves the best

4 The top-k SVM solvers that we used were designed for dense inputs.
5 Code: https://github.com/mlapin/caffe/tree/topk

Method

LRMulti

top-3 SVM1 (FT)
top-5 SVM1 (FT)

top-3 Enttr (FT)
top-5 Enttr (FT)

LRMulti (FT)

Places 205 (val)
Top-3

Top-1

Top-5

Top-10

59.97

81.39

88.17

94.59

60.73
60.88

60.51
60.48

82.09
82.18

81.86
81.66

88.58
88.78

88.69
88.66

94.56
94.75

94.78
94.80

60.73

82.07

88.71

94.82

ILSVRC 2012 (val)

Method

LRMulti

Top-1

Top-3

Top-5

Top-10

68.60

84.29

88.66

92.83

top-3 SVM1 (FT)
top-5 SVM1 (FT)

top-3 Enttr (FT)
top-5 Enttr (FT)

71.66
71.60

71.41
71.20

86.63
86.67

86.80
86.57

90.55
90.56

90.77
90.75

94.17
94.23

94.35
94.38

LRMulti (FT)

72.11

87.08

90.88

94.38

Table 5: Top-k accuracy (%), as reported by Caffe [24], on
large scale datasets after ﬁne-tuning (FT) for approximately
one epoch on Places and 3 epochs on ILSVRC. The ﬁrst line
(LRMulti) is the reference performance w/o ﬁne-tuning.

top-1..5 performance on Places 205. However, in this set of
experiments, we also observed similar improvements when
ﬁne-tuning with the standard softmax loss, which achieves
best performance on ILSVRC 2012.

We conclude that a safe choice for multiclass problems
seems to be the softmax loss as it yields competitive re-
sults in all top-k errors. An interesting alternative is the
smooth top-k hinge loss which is faster to train (see Sec-
tion 3) and achieves competitive performance. If one wants
to optimize directly for a top-k error (at the cost of a higher
top-1 error), then further improvements are possible using
either the smooth top-k SVM or the top-k entropy losses.

6. Conclusion

We have done an extensive experimental study of top-k
performance optimization. We observed that the softmax
loss and the smooth top-1 hinge loss are competitive across
all top-k errors and should be considered the primary candi-
dates in practice. Our new top-k loss functions can further
improve these results slightly, especially if one is targeting a
particular top-k error as the performance measure. Finally,
we would like to highlight our new optimization scheme
based on SDCA for the top-k entropy loss which also in-
cludes the softmax loss and is of an independent interest.

References

[1] S. Agarwal. The inﬁnite push: A new support vector ranking
algorithm that directly optimizes accuracy at the absolute top
of the list. In SDM, pages 839–850, 2011. 1

[2] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid.
Good practice in large-scale learning for image classiﬁca-
tion. PAMI, 36(3):507–520, 2014. 1, 4, 16

[3] P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convex-
ity, classiﬁcation and risk bounds. Journal of the American
Statistical Association, 101:138–156, 2006. 3

[4] A. Beck and M. Teboulle. A fast iterative shrinkage-
thresholding algorithm for linear inverse problems. SIAM
Journal on Imaging Sciences, 2(1):183–202, 2009. 14
[5] A. Beck and M. Teboulle. Smoothing and ﬁrst order meth-
ods, a uniﬁed framework. SIAM Journal on Optimization,
22:557–580, 2012. 6

[6] Y. Bengio. Learning deep architectures for AI. Foundations
and Trends in Machine Learning, 2(1):1–127, 2009. 4
[7] A. Bordes, L. Bottou, P. Gallinari, and J. Weston. Solving
multiclass support vector machines with LaRank. In ICML,
pages 89–96, 2007. 1

[8] J. M. Borwein and A. S. Lewis. Convex Analysis and Non-
linear Optimization: Theory and Examples. Cms Books in
Mathematics Series. Springer Verlag, 2000. 11, 14

[9] S. Boyd, C. Cortes, M. Mohri, and A. Radovanovic. Accu-

racy at the top. In NIPS, pages 953–961, 2012. 1

[10] S. Boyd and L. Vandenberghe. Convex Optimization. Cam-

bridge University Press, 2004. 8, 9, 10

[11] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. Learning to rank using gra-
dient descent. In ICML, pages 89–96, 2005. 1

[12] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support
vector machines. ACM Transactions on Intelligent Systems
and Technology, 2:1–27, 2011. 16

[13] M. Cimpoi, S. Maji, and A. Vedaldi. Deep ﬁlter banks for

texture recognition and segmentation. In CVPR, 2015. 15

[14] R. M. Corless, G. H. Gonnet, D. E. Hare, D. J. Jeffrey, and
D. E. Knuth. On the lambert W function. Advances in Com-
putational Mathematics, 5(1):329–359, 1996. 12, 13
[15] K. Crammer and Y. Singer. On the algorithmic implemen-
tation of multiclass kernel-based vector machines. JMLR,
2:265–292, 2001. 4, 7

[16] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, pages 248–255, 2009. 16

[17] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J.
Lin. LIBLINEAR: A library for large linear classiﬁcation.
JMLR, 9:1871–1874, 2008. 16

[18] T. Fukushima. Precise and fast computation of Lambert W-
functions without transcendental function evaluations. Jour-
nal of Computational and Applied Mathematics, 244:77 –
89, 2013. 12, 13

[19] P. Gehler and S. Nowozin. On feature combination for mul-
ticlass object classiﬁcation. In ICCV, pages 221–228, 2009.
23, 24

[20] M. R. Gupta, S. Bengio, and J. Weston. Training highly mul-

ticlass classiﬁers. JMLR, 15:1461–1492, 2014. 1

[21] J.-B. Hiriart-Urruty and C. Lemaréchal. Fundamentals of

Convex Analysis. Springer, Berlin, 2001. 7

[22] A. S. Householder. The Numerical Treatment of a Single

Nonlinear Equation. McGraw-Hill, 1970. 13

[23] C.-W. Hsu and C.-J. Lin. A comparison of methods for multi-
class support vector machines. Neural Networks, 13(2):415–
425, 2002. 15, 16

[24] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014. 16, 17

[25] T. Joachims. A support vector method for multivariate per-
formance measures. In ICML, pages 377–384, 2005. 1
[26] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet clas-
siﬁcation with deep convolutional neural networks. In NIPS,
pages 1106–1114, 2012. 4

[27] K. Lang. Newsweeder: Learning to ﬁlter netnews. In ICML,

pages 331–339, 1995. 16

[28] M. Lapin, M. Hein, and B. Schiele. Top-k multiclass SVM.
In NIPS, 2015. 1, 2, 5, 7, 8, 11, 12, 14, 15, 16, 28, 32, 34, 36
[29] M. Lapin, B. Schiele, and M. Hein. Scalable multitask rep-
resentation learning for scene classiﬁcation. In CVPR, 2014.
31

[30] N. Li, R. Jin, and Z.-H. Zhou. Top rank optimization in linear

time. In NIPS, pages 1502–1510, 2014. 1

[31] L. Liu, T. G. Dietterich, N. Li, and Z. Zhou. Transductive op-
timization of top k precision. CoRR, abs/1510.05976, 2015.
1

[32] J. Mairal. Sparse Coding for Machine Learning, Image Pro-
cessing and Computer Vision. PhD thesis, Ecole Normale
Superieure de Cachan, 2010. 7

[33] J. Mairal, R. Jenatton, F. R. Bach, and G. R. Obozinski. Net-
work ﬂow algorithms for structured sparsity. In NIPS, pages
1558–1566, 2010. 14

[34] Y. Nesterov. Smooth minimization of non-smooth functions.
Mathematical Programming, 103(1):127–152, 2005. 6
[35] M.-E. Nilsback and A. Zisserman. Automated ﬂower clas-
siﬁcation over a large number of classes. In ICVGIP, pages
722–729, 2008. 16

[36] J. Nocedal and S. J. Wright. Numerical Optimization.

Springer Science+ Business Media, 2006. 13

[37] K. B. Petersen, M. S. Pedersen, et al. The matrix cookbook.
Technical University of Denmark, 450:7–15, 2008. 8
[38] A. Quattoni and A. Torralba. Recognizing indoor scenes. In

CVPR, 2009. 16

[39] A. Rakotomamonjy. Sparse support vector inﬁnite push. In

ICML, pages 1335–1342. ACM, 2012. 1

[40] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carls-
son. CNN features off-the-shelf: an astounding baseline for
recognition. In CVPRW, DeepVision workshop, 2014. 15
[41] M. Reid and B. Williamson. Composite binary losses. JMLR,

11:2387–2422, 2010. 5

[42] J. D. Rennie. Improving multi-class text classiﬁcation with
naive bayes. Technical report, Massachusetts Institute of
Technology, 2001. 15

[43] R. Rifkin and A. Klautau. In defense of one-vs-all classiﬁ-

cation. JMLR, 5:101–141, 2004. 16

[44] A. Rocha and S. Klein Goldenstein. Multiclass from bi-
nary: Expanding one-versus-all, one-versus-one and ecoc-
based approaches. Neural Networks and Learning Systems,
IEEE Transactions on, 25(2):289–302, 2014. 15, 16

[45] S. Ross, J. Zhou, Y. Yue, D. Dey, and D. Bagnell. Learn-
ing policies for contextual submodular prediction. In ICML,
pages 1364–1372, 2013. 1

[46] C. Rudin. The p-norm push: A simple convex ranking algo-
rithm that concentrates at the top of the list. JMLR, 10:2233–
2271, 2009. 1

[47] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge, 2014. 1, 16

[48] S. Shalev-Shwartz and T. Zhang. Accelerated proximal
stochastic dual coordinate ascent for regularized loss mini-
mization. Mathematical Programming, pages 1–41, 2014. 1,
2, 6, 7, 8, 11, 12

[49] L. Sharan, R. Rosenholtz, and E. Adelson. Material percep-
tion: What can you see in a brief glance? Journal of Vision,
9(8):784–784, 2009. 16

[50] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 4, 15, 16

[51] K. Swersky, B. J. Frey, D. Tarlow, R. S. Zemel, and R. P.
Adams. Probabilistic n-choose-k models for classiﬁcation
and ranking. In NIPS, pages 3050–3058, 2012. 1, 15, 16
[52] A. Tewari and P. Bartlett. On the consistency of multiclass
classiﬁcation methods. JMLR, 8:1007–1025, 2007. 3, 4
[53] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun.
Large margin methods for structured and interdependent out-
put variables. JMLR, pages 1453–1484, 2005. 1

[54] N. Usunier, D. Buffoni, and P. Gallinari. Ranking with
In ICML, pages

ordered weighted pairwise classiﬁcation.
1057–1064, 2009. 1, 5, 10

[55] D. Veberiˇc. Lambert W function for applications in physics.
Computer Physics Communications, 183(12):2622–2628,
2012. 12, 13

[56] A. Vedaldi and K. Lenc. Matconvnet – convolutional neural
networks for matlab. In Proceeding of the ACM Int. Conf. on
Multimedia, 2015. 16

[57] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The Caltech-UCSD Birds-200-2011 dataset. Technical re-
port, California Institute of Technology, 2011. 16

[58] L. Wang, S. Guo, W. Huang, and Y. Qiao. Places205-vggnet
models for scene recognition. CoRR, abs/1508.01667, 2015.
15, 16

[59] J. Weston, S. Bengio, and N. Usunier. Wsabie: scaling up
to large vocabulary image annotation. IJCAI, pages 2764–
2770, 2011. 1

[60] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba.
SUN database: Large-scale scene recognition from abbey to
zoo. In CVPR, 2010. 1, 16

[61] N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Part-

based rcnn for ﬁne-grained detection. In ECCV, 2014. 15

[62] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using places
database. In NIPS, 2014. 1, 16

Method

SVMOVA

LROVA

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

82.4

87.4

89.5

90.7

91.5

92.1

92.6

93.1

93.4

93.7

86.1

91.1

93.0

94.1

94.8

95.4

95.8

96.1

96.4

96.6

ALOI

95.1
95.5
95.5
95.6
95.6
95.5

95.5
95.6
95.7
95.7
95.7
95.6

95.1
95.2
95.4
95.4
95.5
95.7

95.5
95.5
95.6
95.6
95.7
95.7

95.7
95.8
95.8
95.8
95.8
95.6

95.9
95.9
95.8
95.7
94.8

96.0
96.4
96.7
96.8
96.9
96.5

96.2
96.3
96.7
96.9
96.9
96.6

96.0
96.0
96.0
96.1
96.2
96.5

96.2
96.2
96.3
96.3
96.3
96.6

96.5
96.6
96.6
96.7
96.7
96.7

96.7
96.7
96.7
96.7
96.3

93.4
94.0
94.2
94.3
94.1
93.0

94.2
94.3
94.4
94.4
94.3
93.1

93.4
93.9
94.1
94.2
94.3
94.2

94.2
94.2
94.3
94.4
94.4
94.3

94.2
94.2
94.3
94.2
94.2
93.2

94.4
94.3
94.0
93.7
92.4

90.6
90.3
89.6
88.7
87.6
85.2

90.0
90.2
90.2
90.1
90.0
89.5

90.6
90.6
90.4
90.3
90.2
89.5

89.8
89.4
89.0
88.5
87.9
86.0

89.8
89.3
88.7
87.9
85.2

top-1 SVMα / SVMMulti 90.0
top-2 SVMα
90.0
top-3 SVMα
89.2
top-4 SVMα
88.4
top-5 SVMα
87.3
top-10 SVMα
85.0

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

96.7
97.0
97.2
97.4
97.4
97.3

96.7
96.8
97.3
97.4
97.5
97.4

96.7
96.7
96.5
96.6
96.7
96.9

96.7
96.7
96.7
96.8
96.8
97.0

97.1
97.1
97.2
97.2
97.2
97.3

97.2
97.3
97.3
97.3
97.1

97.1
97.4
97.6
97.8
97.8
97.8

97.0
97.1
97.6
97.8
97.8
97.8

97.1
97.1
96.9
97.0
97.1
97.3

97.0
97.0
97.1
97.1
97.2
97.4

97.5
97.5
97.5
97.6
97.6
97.7

97.6
97.7
97.7
97.7
97.6

97.5
97.7
97.8
98.0
98.0
98.2

97.6
97.7
97.9
98.0
98.1
98.2

97.5
97.5
97.5
97.3
97.3
97.5

97.6
97.3
97.4
97.4
97.5
97.6

97.8
97.8
97.8
97.8
97.9
98.0

97.9
98.0
98.0
98.0
98.0

97.7
97.9
98.1
98.2
98.3
98.4

97.9
98.0
98.1
98.2
98.3
98.4

97.7
97.7
97.8
97.5
97.5
98.0

97.9
97.9
97.6
97.6
97.7
97.8

98.0
98.0
98.0
98.1
98.1
98.2

98.1
98.2
98.2
98.2
98.2

97.9
98.1
98.2
98.4
98.4
98.6

98.1
98.2
98.3
98.4
98.4
98.6

97.9
97.9
98.0
98.0
97.7
97.9

98.1
98.1
98.1
97.8
97.8
98.1

98.2
98.2
98.2
98.3
98.3
98.4

98.3
98.3
98.4
98.4
98.4

98.1
98.3
98.4
98.5
98.6
98.7

98.2
98.3
98.4
98.5
98.6
98.7

98.1
98.1
98.1
98.2
98.2
98.3

98.2
98.2
98.2
97.9
98.0
98.2

98.4
98.4
98.4
98.4
98.4
98.5

98.5
98.5
98.5
98.6
98.5

Table 6: Comparison of different methods in top-k accuracy (%).

Letter

Method

SVMOVA

LROVA

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

63.0

75.7

82.0

85.7

88.1

89.9

91.4

92.8

93.9

94.6

68.1

81.1

86.1

88.4

90.6

92.2

93.4

94.6

95.3

96.2

top-1 SVMα / SVMMulti 76.5
top-2 SVMα
76.1
top-3 SVMα
74.0
top-4 SVMα
71.4
top-5 SVMα
70.8
top-10 SVMα
61.6

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

85.5
86.9
87.0
86.4
85.9
82.5

86.0
87.0
87.1
86.7
86.2
82.9

85.5
86.4
86.9
86.9
86.9
85.9

86.0
86.6
86.9
87.0
87.0
86.1

86.5
86.5
86.4
86.1
85.6
82.5

86.3
83.9
80.3
76.4
67.9

89.2
90.1
91.0
91.4
91.5
88.9

89.9
90.3
90.9
91.4
91.5
89.1

89.2
90.2
90.6
90.9
91.0
90.8

89.9
90.2
90.6
90.8
91.1
90.8

90.1
90.6
90.8
91.0
90.9
89.7

90.7
91.1
90.0
87.7
80.9

91.5
92.2
93.0
93.5
93.9
93.6

92.1
92.5
93.2
93.2
93.8
93.6

91.5
92.1
92.7
93.1
93.4
93.5

92.1
92.4
92.8
93.1
93.4
93.6

93.1
93.2
93.3
93.6
93.7
93.6

93.7
93.9
93.9
93.7
88.9

93.1
93.3
94.4
94.8
95.1
96.0

93.6
94.0
94.5
94.7
95.2
95.9

93.1
93.8
94.2
94.6
94.9
95.3

93.6
93.8
94.2
94.6
94.9
95.2

94.5
94.7
94.9
95.3
95.1
96.2

95.1
95.6
96.1
96.1
93.7

94.3
94.8
95.4
95.7
96.2
97.6

94.9
94.9
95.6
95.7
96.3
97.5

94.3
94.8
95.3
95.5
95.7
96.2

94.9
95.0
95.3
95.5
95.8
96.3

95.7
95.7
95.9
96.3
96.7
97.7

96.1
96.7
97.2
97.5
96.3

95.5
96.0
96.2
96.6
96.9
98.3

95.8
96.0
96.4
96.7
97.0
98.3

95.5
95.9
95.8
96.3
96.5
97.2

95.8
96.0
96.1
96.3
96.6
97.1

96.5
96.5
96.8
97.1
97.3
98.4

97.0
97.5
98.0
98.3
97.7

96.5
96.5
96.7
97.2
97.5
98.9

96.3
96.6
96.9
97.3
97.6
98.9

96.5
96.7
96.7
96.9
97.2
97.8

96.3
96.8
96.6
97.0
97.2
97.9

97.4
97.5
97.6
97.7
97.9
98.9

97.7
98.1
98.5
98.7
98.7

97.0
97.2
97.3
97.6
98.1
99.2

97.0
97.3
97.3
97.8
98.2
99.3

97.0
97.3
97.4
97.5
97.8
98.3

97.0
97.4
97.3
97.8
97.8
98.3

98.1
98.0
98.2
98.3
98.6
99.3

98.1
98.5
98.9
99.2
99.3

97.7
97.7
97.8
98.2
98.4
99.6

97.6
97.7
97.9
98.0
98.6
99.7

97.7
97.8
97.9
98.1
98.2
98.8

97.6
97.8
97.9
98.1
98.2
98.8

98.1
98.4
98.5
98.6
98.8
99.6

98.5
98.8
99.3
99.4
99.6

76.8
76.2
74.1
72.1
70.8
61.7

76.5
76.5
75.6
74.9
74.5
72.9

76.8
76.5
75.6
75.1
74.6
72.9

75.2
74.6
73.0
71.9
69.7
65.0

70.1
63.6
58.7
50.3
46.5

Table 7: Comparison of different methods in top-k accuracy (%).

News 20

Method

SVMOVA

LROVA

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

84.3

93.0

95.4

97.0

97.9

98.5

98.8

99.0

99.3

99.5

84.9

93.1

96.3

97.2

97.8

98.3

98.7

98.8

99.0

99.3

top-1 SVMα / SVMMulti 85.4
top-2 SVMα
85.3
top-3 SVMα
85.1
top-4 SVMα
85.0
top-5 SVMα
84.3
top-10 SVMα
82.7

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

92.7
94.3
94.8
94.5
94.2
93.3

94.5
94.6
94.7
94.4
94.4
93.5

92.7
94.3
94.5
94.8
94.8
94.6

94.5
94.5
94.5
94.6
94.7
94.7

94.2
94.4
94.6
94.5
94.3
93.3

93.9
93.6
93.5
93.3
92.7

94.9
96.4
96.6
96.7
96.7
96.5

96.3
96.5
96.6
96.7
96.7
96.5

94.9
96.1
96.5
96.6
96.7
96.7

96.3
96.3
96.5
96.6
96.6
96.7

96.4
96.5
96.6
96.7
96.8
96.4

96.6
96.4
96.2
96.0
95.7

96.4
97.2
97.7
97.7
97.8
97.7

97.3
97.6
97.8
97.8
97.9
97.8

96.4
97.4
97.5
97.7
97.7
98.0

97.3
97.4
97.6
97.6
97.6
97.9

97.6
97.7
97.8
97.9
97.8
97.8

97.6
97.7
97.6
97.7
97.0

97.2
97.9
98.2
98.3
98.4
98.4

98.0
98.1
98.4
98.4
98.4
98.4

97.2
97.8
98.1
98.1
98.3
98.5

98.0
98.0
98.1
98.2
98.3
98.5

98.1
98.3
98.3
98.5
98.6
98.5

98.4
98.3
98.3
98.2
97.9

97.6
98.4
98.6
98.6
98.7
98.6

98.5
98.6
98.7
98.7
98.8
98.7

97.6
97.8
98.4
98.4
98.7
98.7

98.5
98.4
98.5
98.6
98.7
98.7

98.5
98.6
98.7
98.7
98.8
98.7

98.7
98.6
98.6
98.5
98.4

98.2
98.5
99.0
98.9
99.0
99.0

98.8
98.9
99.0
99.0
99.0
99.0

98.2
98.2
98.7
98.7
99.0
99.0

98.8
98.8
98.8
98.9
99.0
99.0

99.0
98.9
98.9
99.0
99.0
98.9

98.9
98.9
98.9
98.9
98.8

98.5
98.9
99.2
99.1
99.2
99.2

99.0
99.1
99.1
99.1
99.1
99.2

98.5
98.6
99.0
99.0
99.1
99.2

99.0
99.0
99.0
99.1
99.2
99.1

99.1
99.2
99.2
99.2
99.1
99.1

99.1
99.2
99.1
99.1
99.0

98.7
99.0
99.3
99.3
99.3
99.3

98.9
99.3
99.3
99.3
99.3
99.3

98.7
98.9
99.2
99.2
99.3
99.3

98.9
99.2
99.3
99.0
99.3
99.3

99.3
99.3
99.3
99.3
99.3
99.3

99.3
99.2
99.2
99.2
99.2

99.1
99.1
99.3
99.4
99.3
99.3

99.3
99.3
99.4
99.4
99.4
99.5

99.1
99.1
99.2
99.2
99.4
99.3

99.3
99.3
99.3
99.4
99.3
99.3

99.5
99.3
99.4
99.4
99.4
99.4

99.4
99.4
99.3
99.4
99.4

85.6
85.2
85.1
84.9
84.5
82.9

85.4
85.7
86.0
85.9
85.4
84.9

85.6
85.8
85.7
85.6
85.6
84.9

84.5
84.7
84.7
84.5
84.3
82.7

84.2
83.4
83.3
83.2
82.9

Table 8: Comparison of different methods in top-k accuracy (%).

Method

SVMOVA

LROVA

top-1 SVMα / SVMMulti 62.8
top-2 SVMα
63.1
top-3 SVMα
63.4
top-4 SVMα
63.2
top-5 SVMα
63.3
top-10 SVMα
63.0

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

Caltech 101 Silhouettes

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

61.8

73.1

76.5

78.5

80.8

82.5

83.6

84.6

85.6

86.6

63.2

77.1

80.4

82.6

84.4

85.7

87.0

87.9

88.6

89.4

74.6
76.2
76.7
76.6
76.8
77.3

76.9
76.9
77.2
77.0
77.1
77.2

74.6
76.2
76.6
76.9
77.0
77.1

76.9
77.1
77.0
77.0
77.2
77.3

77.8
77.5
77.5
77.5
77.5
77.3

77.5
76.9
75.9
75.4
72.1

77.8
79.0
79.7
79.8
80.0
80.5

80.3
80.5
80.1
80.3
80.5
80.5

77.8
79.3
79.7
80.1
80.4
80.5

80.3
80.4
80.4
80.2
80.5
80.6

81.2
81.1
81.1
81.0
80.9
80.8

81.3
81.1
80.4
79.8
78.4

80.0
81.0
81.5
82.4
82.7
82.7

82.4
82.3
82.3
82.4
82.7
82.8

80.0
81.1
81.4
82.0
82.6
83.0

82.4
82.3
82.4
82.6
82.4
83.2

83.4
83.3
83.3
83.2
83.1
83.4

83.5
83.7
83.1
82.7
82.0

82.0
82.7
83.6
84.0
84.3
84.6

84.0
84.1
84.0
84.3
84.5
84.8

82.0
82.6
83.4
83.5
84.2
84.9

84.0
84.1
84.2
84.4
84.4
85.0

85.1
85.0
85.0
85.0
85.2
85.4

85.5
85.2
85.1
85.2
84.6

83.3
84.4
85.1
85.3
85.6
85.9

85.4
85.3
85.7
85.5
85.9
86.0

83.3
84.3
84.9
85.0
85.3
86.2

85.4
85.5
85.4
85.7
85.9
86.2

86.5
86.5
86.6
86.5
86.4
86.6

86.9
87.0
86.9
86.9
86.2

84.4
85.5
85.8
86.0
86.5
86.8

86.4
86.5
86.5
86.6
86.7
87.0

84.4
85.3
85.7
85.9
86.3
87.3

86.4
86.5
86.6
86.7
86.9
87.1

87.3
87.4
87.4
87.5
87.5
87.9

87.8
88.2
88.2
88.0
87.5

85.0
86.2
86.6
86.9
87.5
88.0

87.2
87.4
87.3
87.3
87.6
88.1

85.0
86.3
86.6
87.0
87.4
87.9

87.2
87.3
87.3
87.4
87.6
88.0

88.7
88.6
88.7
88.6
88.5
88.5

88.7
88.8
88.9
88.8
88.5

85.9
87.0
87.6
87.8
88.1
88.8

88.4
88.4
88.0
88.2
88.5
88.5

85.9
87.0
87.4
87.8
88.1
88.8

88.4
88.4
88.4
88.4
88.4
88.8

89.3
89.4
89.2
89.3
89.3
89.3

89.3
89.8
89.7
89.6
89.4

86.9
87.6
88.3
88.6
88.7
89.1

89.0
89.0
89.2
89.2
89.1
89.1

86.9
87.9
88.0
88.7
89.0
89.4

89.0
89.0
89.1
89.0
89.1
89.5

89.7
89.8
89.9
89.9
89.9
90.1

90.0
90.2
90.5
90.2
90.2

63.9
63.8
63.3
63.1
63.3
63.1

62.8
63.5
63.9
63.9
63.6
64.0

63.9
63.9
64.0
63.7
63.7
64.2

63.2
63.3
63.3
63.2
63.2
62.5

62.7
60.7
60.0
58.3
51.9

Table 9: Comparison of different methods in top-k accuracy (%).

Caltech 101 (average of 39 kernels from [19], 5 splits)

Method

Top-1

Top-2

Top-3

Top-4

Top-5

Top-6

Top-7

Top-8

Top-9

Top-10

top-1 SVMα / SVMMulti 73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
top-2 SVMα
73.2 ± 0.9 81.7 ± 0.7 85.5 ± 0.9 87.8 ± 0.7 89.6 ± 0.6 90.9 ± 0.5 91.9 ± 0.4 92.7 ± 0.3 93.3 ± 0.4 93.9 ± 0.5
top-3 SVMα
73.0 ± 1.0 81.6 ± 0.7 85.4 ± 0.9 87.8 ± 0.6 89.6 ± 0.6 90.9 ± 0.4 91.8 ± 0.3 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.5
top-4 SVMα
72.8 ± 1.0 81.4 ± 0.7 85.4 ± 0.9 87.9 ± 0.6 89.6 ± 0.5 90.9 ± 0.4 91.8 ± 0.3 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
top-5 SVMα
72.6 ± 1.0 81.2 ± 0.7 85.2 ± 0.8 87.7 ± 0.6 89.5 ± 0.6 90.8 ± 0.5 91.7 ± 0.4 92.6 ± 0.4 93.2 ± 0.5 93.7 ± 0.5
top-10 SVMα
71.0 ± 0.8 80.2 ± 1.0 84.2 ± 0.7 87.0 ± 0.8 88.8 ± 0.7 90.1 ± 0.5 91.3 ± 0.4 92.2 ± 0.5 92.9 ± 0.4 93.5 ± 0.4

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
73.2 ± 0.9 81.7 ± 0.7 85.5 ± 0.9 87.8 ± 0.7 89.6 ± 0.6 90.9 ± 0.5 91.9 ± 0.4 92.7 ± 0.3 93.3 ± 0.4 93.9 ± 0.5
73.0 ± 1.0 81.6 ± 0.7 85.4 ± 0.8 87.8 ± 0.6 89.6 ± 0.6 90.9 ± 0.4 91.8 ± 0.3 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
72.8 ± 1.0 81.4 ± 0.7 85.4 ± 0.9 87.9 ± 0.6 89.6 ± 0.5 90.9 ± 0.4 91.8 ± 0.3 92.7 ± 0.4 93.3 ± 0.5 93.8 ± 0.4
72.5 ± 1.0 81.2 ± 0.7 85.2 ± 0.8 87.7 ± 0.6 89.5 ± 0.6 90.8 ± 0.4 91.7 ± 0.4 92.6 ± 0.4 93.2 ± 0.5 93.8 ± 0.4
71.0 ± 0.8 80.2 ± 1.0 84.2 ± 0.7 87.0 ± 0.8 88.8 ± 0.7 90.1 ± 0.5 91.3 ± 0.4 92.2 ± 0.5 92.9 ± 0.4 93.5 ± 0.4

top-1 SVMβ / SVMMulti 73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
top-2 SVMβ
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
top-3 SVMβ
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
top-4 SVMβ
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
top-5 SVMβ
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
top-10 SVMβ
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.9 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.5

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4

72.7 ± 0.8 80.9 ± 0.9 84.9 ± 0.9 87.4 ± 0.7 89.1 ± 0.7 90.5 ± 0.6 91.6 ± 0.5 92.4 ± 0.3 93.0 ± 0.5 93.6 ± 0.5
72.6 ± 0.9 80.9 ± 0.8 85.0 ± 0.9 87.5 ± 0.8 89.2 ± 0.5 90.6 ± 0.5 91.6 ± 0.5 92.4 ± 0.4 93.1 ± 0.4 93.6 ± 0.4
72.5 ± 0.9 80.8 ± 0.8 85.0 ± 0.9 87.3 ± 0.7 89.2 ± 0.6 90.5 ± 0.5 91.6 ± 0.4 92.4 ± 0.4 93.1 ± 0.4 93.6 ± 0.5
72.2 ± 1.0 80.7 ± 0.8 84.8 ± 0.9 87.3 ± 0.7 89.0 ± 0.7 90.5 ± 0.5 91.5 ± 0.5 92.4 ± 0.3 93.1 ± 0.5 93.6 ± 0.5
72.0 ± 0.8 80.5 ± 0.9 84.7 ± 0.8 87.2 ± 0.7 89.0 ± 0.6 90.4 ± 0.5 91.5 ± 0.3 92.3 ± 0.2 93.0 ± 0.4 93.6 ± 0.4
70.2 ± 0.9 79.8 ± 1.1 83.5 ± 0.6 86.6 ± 0.5 88.4 ± 0.6 89.7 ± 0.8 90.8 ± 0.7 91.9 ± 0.4 92.7 ± 0.4 93.0 ± 0.5

Table 10: Comparison of different methods in top-k accuracy (%).

Caltech 256 (average of 39 kernels from [19])

Method

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

top-1 SVMα / SVMMulti 47.1
top-2 SVMα
47.1
top-3 SVMα
47.0
top-4 SVMα
46.9
top-5 SVMα
46.8
top-10 SVMα
45.4

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

47.1
47.1
47.0
46.9
46.8
45.4

47.1
47.2
47.2
47.2
47.2
47.2

47.1
47.1
47.1
47.1
47.1
47.2

46.2
46.2
46.1
46.1
46.0
45.0

56.3
56.3
56.3
56.2
56.1
55.3

56.3
56.3
56.2
56.2
56.1
55.3

56.3
56.3
56.3
56.3
56.3
56.3

56.3
56.3
56.3
56.3
56.3
56.3

55.6
55.7
55.8
55.5
55.4
54.6

61.4
61.4
61.5
61.4
61.2
60.7

61.4
61.4
61.5
61.4
61.3
60.8

61.4
61.4
61.4
61.4
61.4
61.4

61.4
61.4
61.4
61.4
61.4
61.4

60.9
60.9
61.0
60.9
60.7
59.9

64.7
64.6
64.6
64.5
64.4
64.1

64.5
64.4
64.5
64.5
64.3
64.1

64.7
64.7
64.7
64.7
64.7
64.7

64.5
64.5
64.5
64.5
64.5
64.5

64.1
64.1
64.2
64.2
64.1
63.7

67.3
67.2
67.3
67.3
67.1
66.7

67.1
67.0
67.3
67.3
67.0
66.6

67.3
67.2
67.2
67.2
67.2
67.2

67.1
67.1
67.1
67.1
67.1
67.1

66.7
66.7
66.8
66.7
66.7
66.2

69.3
69.4
69.4
69.3
69.3
69.0

69.3
69.3
69.4
69.3
69.4
68.9

69.3
69.4
69.4
69.4
69.3
69.3

69.3
69.3
69.3
69.3
69.3
69.3

69.1
69.0
69.0
68.9
68.8
68.4

71.5
71.3
71.3
71.3
71.2
71.0

71.5
71.3
71.3
71.1
71.1
71.0

71.5
71.4
71.4
71.4
71.4
71.5

71.5
71.5
71.5
71.5
71.5
71.5

71.0
71.0
70.9
70.9
70.7
70.4

72.6
72.6
72.6
72.8
72.8
72.6

72.8
72.7
72.7
72.8
72.9
72.6

72.6
72.6
72.6
72.6
72.6
72.7

72.8
72.8
72.7
72.7
72.7
72.7

72.5
72.0
72.5
71.5
72.7
72.3

74.1
74.1
74.1
74.1
74.2
73.9

74.1
74.0
74.1
74.2
74.2
74.0

74.1
74.1
74.1
74.1
74.1
74.1

74.1
74.1
74.1
74.1
74.1
74.1

73.7
73.3
73.7
73.9
73.2
73.5

75.2
75.2
75.1
75.2
75.1
75.2

75.2
75.2
75.1
75.2
75.2
75.2

75.2
75.2
75.2
75.2
75.2
75.2

75.2
75.2
75.2
75.2
75.2
75.2

74.9
74.5
74.8
74.8
74.2
74.7

Table 11: Comparison of different methods in top-k accuracy (%).

CUB

Method

SVMOVA

LROVA

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

60.6

71.4

77.1

80.7

83.4

85.2

86.6

87.9

89.0

89.9

62.3

74.8

80.5

84.6

87.4

89.4

90.7

91.9

92.8

93.5

top-1 SVMα / SVMMulti 61.0
top-2 SVMα
61.2
top-3 SVMα
61.3
top-4 SVMα
61.1
top-5 SVMα
60.9
top-10 SVMα
59.6

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

73.3
73.7
74.9
75.1
74.7
73.9

74.3
74.7
75.1
75.1
75.0
73.9

73.3
73.4
73.7
73.9
74.4
74.9

74.3
74.3
74.3
74.4
74.4
74.9

75.2
75.4
75.6
75.6
75.4
74.7

74.8
75.0
74.5
74.8
73.2

79.2
79.9
80.4
81.1
81.2
81.3

80.2
80.5
81.1
81.2
81.3
81.4

79.2
79.2
79.8
79.7
79.8
80.8

80.2
80.2
80.2
80.2
80.4
80.9

81.7
81.6
81.8
81.4
81.9
81.6

81.2
81.4
81.1
81.2
80.7

82.8
83.5
83.9
84.6
84.7
85.1

84.0
84.0
84.2
84.7
85.0
85.2

82.8
83.1
83.5
83.8
83.8
84.4

84.0
84.0
83.9
83.9
83.9
84.3

85.2
85.2
85.4
85.4
85.4
85.3

84.7
84.9
84.9
85.0
84.7

85.7
85.9
86.3
86.7
87.2
87.7

86.9
86.9
86.6
87.1
87.4
87.8

85.7
86.0
86.4
86.6
86.4
86.8

86.9
86.9
86.7
86.8
86.7
86.8

87.9
87.9
87.9
87.8
88.1
88.2

87.6
87.6
87.7
87.7
87.2

87.8
88.2
88.1
88.7
89.0
89.6

88.6
88.6
88.6
89.0
89.2
89.7

87.8
88.1
88.2
88.3
88.5
88.8

88.6
88.6
88.6
88.5
88.7
88.8

89.6
89.7
89.6
89.6
90.0
90.0

89.4
89.4
89.6
89.7
89.4

89.5
89.9
89.9
89.8
90.4
90.7

90.1
90.1
90.2
90.3
90.6
90.7

89.5
89.8
89.9
89.8
90.1
90.1

90.1
90.2
90.2
90.2
90.1
90.3

91.3
91.3
91.2
91.1
91.2
91.3

90.9
90.8
91.0
91.1
90.8

90.7
91.0
91.3
90.9
91.0
91.7

91.4
91.3
91.4
91.4
91.2
91.8

90.7
91.1
91.2
90.9
91.1
91.3

91.4
91.3
91.4
91.3
91.5
91.3

92.5
92.5
92.4
92.4
92.4
92.4

91.9
91.9
92.0
92.0
91.9

91.6
91.9
91.9
91.9
92.1
92.7

92.3
92.2
92.2
92.3
92.2
92.7

91.6
91.9
91.9
91.9
91.6
92.2

92.3
92.2
92.2
92.3
92.4
92.1

93.2
93.2
93.2
93.2
93.2
93.3

92.7
93.0
93.0
92.9
92.8

92.3
92.6
92.5
92.8
92.9
93.4

93.1
93.0
93.2
93.0
92.9
93.4

92.3
92.6
92.5
92.8
92.4
93.1

93.1
93.1
93.2
93.1
93.0
92.9

93.9
93.9
93.9
93.9
93.8
93.8

93.4
93.4
93.6
93.7
93.4

61.9
62.0
61.9
61.7
61.3
59.8

61.0
61.0
61.3
61.5
61.8
62.1

61.9
61.9
62.0
61.9
61.9
62.4

62.3
62.4
62.5
62.3
62.0
61.2

61.9
62.0
61.8
61.4
59.7

Table 12: Comparison of different methods in top-k accuracy (%).

Flowers

Method

SVMOVA

LROVA

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

82.0

89.2

91.7

93.2

94.3

95.2

95.9

95.8

96.2

96.8

82.6

89.4

92.2

93.9

94.8

95.8

96.4

96.9

97.3

97.6

top-1 SVMα / SVMMulti 82.5
top-2 SVMα
82.3
top-3 SVMα
81.9
top-4 SVMα
81.8
top-5 SVMα
81.7
top-10 SVMα
80.5

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

89.5
89.5
89.3
89.3
89.1
88.8

89.8
89.6
89.7
89.3
89.3
88.8

89.5
89.6
89.8
89.5
89.7
89.7

89.8
89.8
89.8
89.7
89.9
89.8

89.7
89.7
89.5
89.5
89.4
88.9

89.4
89.2
88.8
89.0
87.8

92.2
92.3
92.2
92.3
92.4
91.9

92.4
92.4
92.3
92.4
92.5
91.9

92.2
92.2
92.1
92.1
92.0
92.3

92.4
92.4
92.4
92.4
92.4
92.4

92.4
92.4
92.0
92.4
92.2
92.1

92.3
92.2
92.3
92.0
91.1

93.8
93.9
93.8
94.0
94.1
93.7

94.0
94.0
94.1
94.1
94.1
93.7

93.8
93.7
93.7
93.7
93.7
93.9

94.0
94.0
94.0
94.0
93.9
94.0

94.0
94.0
94.1
94.1
94.1
93.9

93.8
94.2
94.1
93.8
93.0

94.8
95.0
95.0
95.0
95.1
95.1

95.1
95.2
95.2
95.2
95.1
95.1

94.8
94.9
94.8
94.8
94.9
95.0

95.1
95.1
95.1
95.0
95.1
95.1

95.1
95.3
95.3
95.3
95.1
95.0

95.0
95.2
95.0
95.0
94.3

95.6
95.7
95.8
95.9
95.8
95.9

95.9
95.9
95.8
96.0
95.9
95.9

95.6
95.6
95.7
95.6
95.6
95.6

95.9
95.9
95.9
96.0
95.9
96.0

96.0
96.0
96.1
96.0
95.9
95.9

96.0
96.0
95.9
95.7
95.2

96.2
96.5
96.4
96.6
96.6
96.5

96.4
96.5
96.5
96.6
96.6
96.6

96.2
96.2
96.2
96.2
96.2
96.2

96.4
96.4
96.5
96.5
96.5
96.2

96.6
96.6
96.6
96.7
96.6
96.5

96.5
96.7
96.6
96.2
96.0

95.5
95.6
97.0
97.0
97.1
97.1

96.7
96.9
97.1
97.1
97.1
97.0

95.5
95.6
96.8
96.7
96.7
96.7

96.7
96.7
96.7
96.9
96.7
96.7

97.1
97.1
97.1
97.1
97.1
97.0

97.2
97.0
97.2
97.0
96.6

96.0
95.9
96.0
95.6
97.4
97.4

97.3
97.3
97.3
97.5
97.5
97.4

96.0
95.9
95.8
95.6
95.8
97.2

97.3
97.3
97.4
97.3
97.4
97.3

97.4
97.5
97.4
97.5
97.5
97.4

97.4
97.4
97.5
97.3
96.9

96.4
96.2
96.1
97.8
97.8
97.7

97.6
97.6
97.7
97.7
97.8
97.7

96.4
96.3
96.1
96.0
96.2
97.5

97.6
97.6
97.6
97.6
97.6
97.6

97.8
97.8
97.8
97.8
97.9
97.7

97.8
97.6
97.7
97.7
97.3

83.0
82.6
82.5
82.3
82.0
80.6

82.5
82.5
82.4
82.4
82.5
82.7

83.0
83.0
83.0
82.9
83.0
82.7

82.9
82.6
82.5
82.2
82.1
80.9

82.1
82.1
81.9
81.4
77.9

Table 13: Comparison of different methods in top-k accuracy (%).

FMD

Method

SVMOVA

LROVA

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

77.4

87.2

92.4

94.4

96.4

97.8

99.0

99.8 100.0 100.0

79.6

90.2

94.2

96.6

98.2

98.8

99.2 100.0 99.8

100.0

top-1 SVMα / SVMMulti 77.6
top-2 SVMα
78.2
top-3 SVMα
78.8
top-4 SVMα
78.2
top-5 SVMα
78.4

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1

top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr

88.8
89.2
89.2
89.4
89.2

89.4
90.2
89.6
89.4
89.2

88.8
89.6
89.6
90.0
90.2

89.4
89.4
89.8
90.2
90.2

90.6
89.6
89.4
90.2
90.4

89.4
89.6
89.4
89.4

93.8
94.0
94.6
94.6
94.4

93.8
93.8
94.4
94.6
94.4

93.8
93.6
93.6
93.8
94.8

93.8
93.8
94.0
94.2
94.6

94.6
94.6
94.8
94.8
94.4

94.4
95.4
94.8
94.0

95.4
95.8
96.4
96.8
96.8

96.0
96.2
96.2
96.6
96.8

95.4
95.4
95.4
95.6
95.8

96.0
96.0
96.2
96.2
96.0

96.6
97.6
97.4
97.0
97.2

96.6
97.4
96.6
96.4

97.2
97.4
97.8
98.0
97.6

98.0
97.6
98.0
97.8
97.6

97.2
97.4
97.8
97.8
97.4

98.0
98.0
98.2
97.8
97.6

97.8
98.0
98.0
97.8
98.0

98.2
98.2
98.0
97.8

98.4 100.0 100.0 100.0 100.0
99.8 100.0 100.0 100.0
98.6
99.8 100.0 100.0
99.4
98.8
99.6 100.0 100.0
99.4
98.8
100.0
99.8
99.6
99.2
98.6

99.0
99.0
99.0
98.8
98.8

99.4
99.2
99.2
99.2
99.2

99.6 100.0 100.0
100.0
99.8
99.6
100.0
99.8
99.6
100.0
99.8
99.6
100.0
99.8
99.2

98.4 100.0 100.0 100.0 100.0
99.8 100.0 100.0 100.0
98.6
99.8 100.0 100.0
99.2
98.8
99.2
98.8
99.6 100.0 100.0
99.6 100.0 100.0 100.0
98.8

99.0
99.0
99.0
98.8
98.8

98.8
98.8
98.8
98.8
98.8

99.0
98.6
98.8
98.8

99.6 100.0 100.0
99.4
99.6 100.0 100.0
99.4
99.6 100.0 100.0
99.2
99.4
99.6 100.0 100.0
99.4 100.0 100.0 100.0

99.2 100.0 99.8
99.2 100.0 99.8
99.8
99.2
99.2
99.8
99.2
99.0
99.8
99.2
99.2

100.0
100.0
100.0
100.0
100.0

99.2 100.0 100.0 100.0
99.2 100.0 100.0 100.0
99.4
99.8 100.0 100.0
99.2 100.0 100.0 100.0

78.6
78.4
79.0
79.2
79.4

77.6
79.0
79.8
80.4
80.2

78.6
78.6
79.6
79.4
80.0

79.0
79.4
79.8
79.2
79.4

79.0
78.4
77.8
77.2

Table 14: Comparison of different methods in top-k accuracy (%).

Indoor 67 (AlexNet trained on Places 205, FC7 output, provided by [28])

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

Method

SVMOVA

LROVA

top-1 SVMα / SVMMulti 74.0
top-2 SVMα
73.1
top-3 SVMα
71.6
top-4 SVMα
71.4
top-5 SVMα
70.7
top-10 SVMα
70.0

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

71.7

80.1

85.9

90.2

92.6

94.3

94.9

95.7

96.4

96.9

73.1

84.5

89.6

91.8

93.3

94.3

95.3

96.1

96.6

97.0

85.2
85.7
86.3
85.7
85.7
85.4

86.0
85.9
86.1
86.2
86.2
85.2

85.2
85.9
86.3
86.2
85.6
85.3

86.0
85.9
86.3
86.2
85.9
85.5

86.0
85.9
86.0
86.0
85.7
84.8

85.3
83.9
82.4
82.8
82.6

89.3
90.4
91.1
90.7
90.4
90.0

90.7
90.5
90.8
90.7
90.2
89.9

89.3
89.8
90.6
90.8
90.7
90.4

90.7
90.8
90.7
90.7
90.8
90.7

91.2
91.3
91.0
91.2
90.6
90.1

90.4
89.8
89.3
89.6
87.9

91.9
92.2
93.2
93.3
93.2
93.1

92.8
93.0
93.1
93.1
92.8
93.1

91.9
92.2
92.8
92.7
93.0
93.4

92.8
92.8
92.7
92.5
92.7
93.4

93.8
93.7
93.6
93.4
93.0
92.5

92.8
92.6
92.2
92.8
91.9

93.4
94.5
94.7
94.8
94.7
94.6

94.5
94.3
94.6
94.6
94.7
94.6

93.4
94.1
94.4
94.5
94.5
94.4

94.5
94.5
94.5
94.3
94.4
94.6

94.9
94.8
94.9
94.8
94.7
94.9

94.6
94.5
94.2
94.3
93.8

94.9
95.1
95.7
95.7
95.5
95.6

95.9
95.8
95.4
95.5
95.4
95.7

94.9
95.1
95.9
95.8
95.7
95.6

95.9
95.7
95.6
95.7
95.6
95.6

95.7
95.9
95.7
95.7
95.8
95.9

95.6
95.1
95.4
95.3
95.3

95.6
96.2
96.4
96.2
96.1
96.2

96.1
96.3
96.3
96.4
96.3
96.0

95.6
95.7
96.1
96.2
96.2
96.2

96.1
96.2
96.3
96.3
96.2
96.3

96.6
96.6
96.6
96.6
96.5
96.4

96.5
96.3
96.2
95.9
95.9

95.8
96.6
96.6
96.6
96.9
97.1

96.8
96.6
96.6
96.7
97.0
97.0

95.8
96.4
96.3
96.6
96.8
97.2

96.8
96.6
96.6
96.7
97.0
97.1

97.1
97.0
97.2
97.2
97.1
97.1

97.1
97.2
97.0
96.7
96.8

96.4
97.0
97.1
97.2
97.5
97.5

97.1
97.1
97.3
97.4
97.5
97.4

96.4
97.0
96.9
97.1
97.4
97.5

97.1
97.1
97.3
97.4
97.5
97.5

97.5
97.5
97.6
97.5
97.5
97.4

97.6
97.5
97.4
97.4
97.0

96.9
97.3
97.2
97.8
97.9
97.5

97.4
97.4
97.7
97.7
97.8
97.7

96.9
97.3
97.2
97.7
97.6
97.8

97.4
97.5
97.5
97.5
97.8
97.8

97.8
97.8
97.7
97.6
97.6
97.5

97.8
97.5
97.5
97.3
97.3

74.0
72.7
72.2
71.3
71.0
70.3

74.0
74.0
73.0
73.1
72.6
71.9

74.0
73.9
73.7
73.0
72.7
72.2

72.5
72.1
71.4
71.3
71.0
69.6

68.2
68.3
68.2
67.1
66.4

Table 15: Comparison of different methods in top-k accuracy (%).

Method

SVMOVA

LROVA

Indoor 67 (VGG-16 trained on Places 205, FC6 output)

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

80.4

89.9

94.3

95.3

96.0

96.9

97.3

97.7

97.9

98.3

82.5

92.6

95.1

96.3

97.5

97.5

98.3

98.6

98.4

98.7

top-1 SVMα / SVMMulti 82.6
top-2 SVMα
83.1
top-3 SVMα
83.0
top-4 SVMα
82.9
top-5 SVMα
82.5
top-10 SVMα
80.4

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

91.4
92.3
92.5
92.5
92.3
92.2

92.5
93.1
93.0
92.5
92.5
92.1

91.4
92.1
92.5
92.6
92.8
92.8

92.5
92.8
92.9
92.7
92.8
92.9

92.9
93.2
93.3
92.8
92.5
92.5

92.0
92.0
91.9
91.9
90.9

95.0
95.4
95.4
95.6
95.7
95.8

95.1
95.2
95.4
95.7
95.7
95.9

95.0
95.2
95.1
95.2
95.4
95.7

95.1
95.1
95.1
95.4
95.4
95.5

95.4
95.7
95.7
95.7
95.8
95.7

95.5
95.7
95.2
95.1
94.6

96.1
96.8
97.5
97.0
97.0
96.9

97.1
97.1
97.2
97.2
97.0
96.9

96.1
96.6
96.9
97.1
97.1
97.1

97.1
97.0
97.0
97.1
97.0
97.0

97.1
97.2
97.1
96.9
96.9
96.9

96.9
96.9
97.0
96.5
96.2

96.0
97.7
97.8
97.8
97.7
97.7

97.8
97.8
97.8
97.8
97.7
97.5

96.0
97.5
97.9
97.8
97.8
97.7

97.8
97.8
97.8
97.8
97.8
97.7

97.2
97.2
97.5
97.6
97.5
97.6

97.6
97.6
97.7
97.5
97.5

97.9
98.0
98.3
98.2
98.1
98.4

98.1
98.1
98.2
98.2
98.3
98.1

97.9
98.0
98.3
98.4
98.3
98.4

98.1
98.2
98.4
98.4
98.4
98.4

98.1
98.1
98.1
98.2
98.4
98.4

98.1
98.4
98.4
98.1
98.3

97.5
98.3
98.5
98.7
98.5
98.8

98.5
98.4
98.5
98.7
98.7
98.7

97.5
98.4
98.4
98.6
98.6
98.7

98.5
98.5
98.5
98.6
98.7
98.7

98.7
98.7
98.8
98.7
98.7
98.7

98.4
98.6
98.6
98.6
98.6

97.8
98.7
98.7
98.7
98.7
99.0

98.3
98.8
98.8
98.7
98.7
99.0

97.8
98.7
98.7
98.8
98.8
99.0

98.3
98.7
98.8
98.8
98.8
98.9

99.1
99.0
98.9
98.9
99.0
99.0

98.7
98.7
98.7
98.9
98.8

98.3
98.7
98.7
98.8
98.8
99.1

98.4
98.4
98.8
98.8
99.0
99.2

98.3
98.7
98.4
98.8
98.8
99.0

98.4
98.4
98.8
98.8
98.8
99.0

99.1
99.0
99.0
99.0
99.0
99.1

98.8
99.0
98.7
99.1
99.0

98.9
98.5
98.9
99.0
99.0
99.2

99.0
99.0
98.9
99.0
99.1
99.3

98.9
98.5
98.7
98.8
98.9
99.2

99.0
99.0
99.0
99.0
99.0
99.3

99.1
99.2
99.1
99.2
99.2
99.2

99.0
99.1
99.1
99.0
99.2

83.1
83.0
82.9
82.5
82.1
80.1

82.6
83.2
82.9
82.5
82.2
81.9

83.1
82.9
82.8
82.2
82.3
81.6

82.4
82.4
81.9
81.9
81.9
81.2

81.0
81.2
81.1
80.9
78.7

Table 16: Comparison of different methods in top-k accuracy (%).

Method

SVMOVA

LROVA

Indoor 67 (VGG-16 trained on Places 205, FC7 output)

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

81.9

91.0

94.3

95.7

96.5

97.2

97.4

97.7

97.8

98.0

82.0

91.6

94.9

96.3

97.2

97.8

98.1

98.3

98.6

98.7

top-1 SVMα / SVMMulti 82.5
top-2 SVMα
82.0
top-3 SVMα
81.6
top-4 SVMα
80.8
top-5 SVMα
79.9
top-10 SVMα
78.4

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

91.7
91.3
91.4
91.3
91.3
91.0

91.6
91.6
91.5
91.3
91.3
91.0

91.7
91.3
91.4
91.6
91.3
91.8

91.6
91.5
91.4
91.3
91.6
91.7

91.4
91.9
91.5
91.6
91.3
91.0

90.8
90.9
90.1
90.2
88.7

95.4
95.1
95.1
95.2
95.0
95.1

95.2
95.0
95.1
95.1
95.1
95.1

95.4
95.1
95.0
95.2
94.9
95.2

95.2
95.1
95.1
95.0
95.1
95.3

95.2
95.1
95.4
95.4
95.0
95.1

94.4
95.0
94.9
94.3
92.8

96.9
96.6
96.8
96.9
96.5
96.6

96.9
96.7
96.6
96.8
96.6
96.6

96.9
96.9
97.1
96.9
96.9
96.7

96.9
96.9
96.8
97.0
96.7
96.8

97.2
96.9
96.7
96.6
96.6
96.7

96.3
96.2
96.3
96.0
95.7

97.3
97.6
97.7
97.7
97.7
97.4

97.6
97.8
97.8
97.8
97.8
97.5

97.3
97.7
97.7
97.5
97.6
97.3

97.6
97.7
97.6
97.6
97.5
97.2

98.0
97.8
97.6
97.7
97.7
97.6

97.0
97.5
97.4
97.3
97.1

97.8
97.8
98.2
98.6
98.4
98.3

98.1
98.0
98.3
98.4
98.4
98.4

97.8
97.8
98.1
98.1
98.2
98.2

98.1
98.2
98.3
98.3
98.3
98.2

98.4
98.4
98.3
98.3
98.2
98.3

97.9
98.1
98.1
97.8
97.6

97.6
98.0
98.6
98.6
98.6
98.5

98.4
97.9
98.7
98.6
98.6
98.5

97.6
98.3
98.2
98.4
98.4
98.7

98.4
97.9
98.6
98.5
98.5
98.6

98.7
98.7
98.7
98.7
98.7
98.7

98.4
98.5
98.5
98.4
98.3

98.5
98.6
98.7
98.7
98.8
98.8

98.6
98.1
98.7
98.7
98.8
98.8

98.5
98.5
98.6
98.7
98.7
98.9

98.6
98.1
98.1
98.1
98.7
98.8

98.8
98.7
98.8
98.8
98.8
98.7

98.7
98.7
98.9
98.6
98.6

98.8
98.7
98.8
98.9
98.8
99.0

98.7
98.9
98.7
99.0
98.8
99.0

98.8
98.7
98.7
98.9
98.9
99.0

98.7
98.8
98.4
98.9
98.9
99.0

99.0
99.0
99.0
99.0
99.0
99.0

98.9
99.0
99.0
98.7
98.7

99.1
99.0
99.0
99.1
99.0
99.0

99.0
98.7
99.0
99.3
99.1
99.0

99.1
98.7
99.0
98.6
99.0
99.1

99.0
98.7
98.7
98.7
98.8
99.2

99.1
99.3
99.2
99.1
99.0
99.0

99.0
99.1
99.1
99.0
98.9

82.6
82.5
81.6
81.0
80.4
78.3

82.5
82.5
82.1
82.2
82.2
81.7

82.6
82.7
82.5
82.6
82.5
81.6

82.3
81.9
81.4
80.8
80.3
79.2

79.6
79.8
78.7
76.4
72.6

Table 17: Comparison of different methods in top-k accuracy (%).

SUN 397 (Fisher Vector kernels provided by [29])

Method

Top-1

Top-2

Top-3

Top-4

Top-5

Top-6

Top-7

Top-8

Top-9

Top-10

top-1 SVMα / SVMMulti 48.9 ± 0.3 60.6 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-2 SVMα
48.9 ± 0.3 60.5 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-3 SVMα
48.9 ± 0.3 60.5 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.3 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-4 SVMα
48.8 ± 0.3 60.5 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.6 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-5 SVMα
48.7 ± 0.3 60.5 ± 0.3 66.8 ± 0.2 70.9 ± 0.3 73.9 ± 0.3 76.2 ± 0.2 78.1 ± 0.2 79.6 ± 0.2 80.9 ± 0.2 82.1 ± 0.2
top-10 SVMα
48.3 ± 0.4 60.4 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.1 ± 0.2 76.5 ± 0.2 78.4 ± 0.2 80.0 ± 0.2 81.3 ± 0.2 82.5 ± 0.2

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.9 ± 0.3 60.6 ± 0.2 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.8 ± 0.3 60.6 ± 0.2 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.2 78.4 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.7 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.4 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.3 ± 0.4 60.4 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.1 ± 0.2 76.5 ± 0.2 78.5 ± 0.2 80.0 ± 0.2 81.4 ± 0.2 82.6 ± 0.2

top-1 SVMβ / SVMMulti 48.9 ± 0.3 60.6 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-2 SVMβ
48.9 ± 0.3 60.6 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-3 SVMβ
48.9 ± 0.3 60.6 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-4 SVMβ
48.9 ± 0.3 60.6 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-5 SVMβ
48.9 ± 0.3 60.6 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-10 SVMβ
48.9 ± 0.3 60.6 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.3 ± 0.2 78.2 ± 0.2 79.8 ± 0.2 81.1 ± 0.2 82.3 ± 0.2

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.4 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.0 ± 0.3 74.2 ± 0.2 76.5 ± 0.2 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2

48.5 ± 0.3 60.5 ± 0.2 66.9 ± 0.3 71.2 ± 0.3 74.3 ± 0.3 76.7 ± 0.3 78.6 ± 0.2 80.2 ± 0.2 81.6 ± 0.2 82.7 ± 0.2
48.5 ± 0.3 60.5 ± 0.2 66.9 ± 0.3 71.2 ± 0.3 74.4 ± 0.3 76.7 ± 0.3 78.6 ± 0.2 80.2 ± 0.2 81.6 ± 0.2 82.7 ± 0.2
48.5 ± 0.3 60.5 ± 0.2 66.9 ± 0.3 71.2 ± 0.3 74.4 ± 0.3 76.7 ± 0.3 78.6 ± 0.2 80.2 ± 0.2 81.6 ± 0.2 82.8 ± 0.2
48.5 ± 0.3 60.5 ± 0.2 66.9 ± 0.3 71.2 ± 0.3 74.3 ± 0.3 76.7 ± 0.3 78.6 ± 0.2 80.2 ± 0.2 81.6 ± 0.2 82.8 ± 0.2
48.4 ± 0.3 60.5 ± 0.2 66.9 ± 0.3 71.2 ± 0.3 74.3 ± 0.3 76.7 ± 0.3 78.6 ± 0.2 80.2 ± 0.3 81.6 ± 0.2 82.8 ± 0.2
48.0 ± 0.4 60.3 ± 0.3 66.8 ± 0.3 71.1 ± 0.3 74.3 ± 0.3 76.7 ± 0.2 78.7 ± 0.2 80.2 ± 0.2 81.6 ± 0.2 82.8 ± 0.2

Table 18: Comparison of different methods in top-k accuracy (%).

Method

SVMOVA

LROVA

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

SUN 397 (AlexNet trained on Places 205, FC7 output, provided by [28])

Top-1

Top-2

Top-3

Top-4

Top-5

Top-6

Top-7

Top-8

Top-9

Top-10

44.1 ± 0.4 60.8 ± 0.3 69.9 ± 0.2 75.8 ± 0.1 79.8 ± 0.1 82.8 ± 0.1 85.1 ± 0.2 86.9 ± 0.2 88.3 ± 0.2 89.6 ± 0.2

53.9 ± 0.2 69.2 ± 0.2 76.5 ± 0.2 80.9 ± 0.2 84.0 ± 0.3 86.2 ± 0.2 87.9 ± 0.2 89.2 ± 0.2 90.3 ± 0.2 91.2 ± 0.2

top-1 SVMα / SVMMulti 58.2 ± 0.2 71.7 ± 0.2 78.2 ± 0.1 82.3 ± 0.2 85.0 ± 0.2 87.1 ± 0.2 88.8 ± 0.2 90.0 ± 0.2 91.0 ± 0.2 91.9 ± 0.2
top-2 SVMα
58.8 ± 0.2 72.7 ± 0.2 79.3 ± 0.2 83.3 ± 0.2 85.9 ± 0.2 87.8 ± 0.2 89.2 ± 0.2 90.3 ± 0.2 91.3 ± 0.2 92.2 ± 0.2
top-3 SVMα
59.0 ± 0.1 73.2 ± 0.2 79.9 ± 0.2 83.8 ± 0.2 86.5 ± 0.2 88.3 ± 0.2 89.7 ± 0.2 90.9 ± 0.2 91.8 ± 0.2 92.6 ± 0.2
top-4 SVMα
58.9 ± 0.1 73.5 ± 0.2 80.3 ± 0.2 84.2 ± 0.2 86.8 ± 0.2 88.6 ± 0.2 90.0 ± 0.2 91.1 ± 0.2 92.0 ± 0.2 92.8 ± 0.2
top-5 SVMα
58.9 ± 0.1 73.7 ± 0.2 80.5 ± 0.2 84.4 ± 0.3 87.0 ± 0.2 88.8 ± 0.2 90.2 ± 0.2 91.3 ± 0.2 92.2 ± 0.2 93.0 ± 0.2
top-10 SVMα
58.0 ± 0.2 73.6 ± 0.1 80.8 ± 0.1 84.8 ± 0.2 87.4 ± 0.2 89.3 ± 0.2 90.7 ± 0.2 91.8 ± 0.2 92.7 ± 0.2 93.4 ± 0.2

59.7 ± 0.1 73.8 ± 0.1 80.3 ± 0.2 84.2 ± 0.2 86.8 ± 0.2 88.6 ± 0.2 90.0 ± 0.2 91.1 ± 0.2 92.0 ± 0.2 92.8 ± 0.2
59.6 ± 0.1 73.8 ± 0.1 80.4 ± 0.2 84.3 ± 0.2 86.8 ± 0.2 88.6 ± 0.3 90.1 ± 0.2 91.2 ± 0.2 92.1 ± 0.2 92.9 ± 0.2
59.4 ± 0.1 73.8 ± 0.1 80.4 ± 0.2 84.4 ± 0.2 86.9 ± 0.3 88.7 ± 0.3 90.1 ± 0.2 91.3 ± 0.2 92.2 ± 0.2 92.9 ± 0.2
59.2 ± 0.1 73.9 ± 0.1 80.6 ± 0.2 84.5 ± 0.2 87.1 ± 0.2 88.8 ± 0.2 90.2 ± 0.2 91.4 ± 0.2 92.3 ± 0.2 93.0 ± 0.2
59.1 ± 0.1 74.0 ± 0.2 80.7 ± 0.2 84.6 ± 0.3 87.2 ± 0.2 89.0 ± 0.2 90.4 ± 0.2 91.5 ± 0.2 92.4 ± 0.2 93.1 ± 0.2
58.1 ± 0.2 73.7 ± 0.2 80.9 ± 0.2 84.9 ± 0.2 87.5 ± 0.2 89.4 ± 0.2 90.7 ± 0.2 91.8 ± 0.2 92.7 ± 0.1 93.4 ± 0.1

top-1 SVMβ / SVMMulti 58.2 ± 0.2 71.7 ± 0.2 78.2 ± 0.1 82.3 ± 0.2 85.0 ± 0.2 87.1 ± 0.2 88.8 ± 0.2 90.0 ± 0.2 91.0 ± 0.2 91.9 ± 0.2
top-2 SVMβ
58.8 ± 0.2 72.7 ± 0.2 79.3 ± 0.2 83.2 ± 0.2 85.9 ± 0.2 87.7 ± 0.2 89.0 ± 0.2 90.3 ± 0.2 91.3 ± 0.2 92.2 ± 0.2
top-3 SVMβ
59.1 ± 0.2 73.2 ± 0.2 79.8 ± 0.2 83.8 ± 0.2 86.4 ± 0.2 88.2 ± 0.2 89.6 ± 0.2 90.8 ± 0.2 91.7 ± 0.2 92.5 ± 0.2
top-4 SVMβ
59.2 ± 0.1 73.6 ± 0.2 80.2 ± 0.2 84.2 ± 0.2 86.7 ± 0.2 88.5 ± 0.2 89.9 ± 0.2 91.1 ± 0.2 92.0 ± 0.2 92.7 ± 0.2
top-5 SVMβ
59.3 ± 0.2 73.8 ± 0.2 80.4 ± 0.3 84.4 ± 0.3 87.0 ± 0.3 88.7 ± 0.3 90.1 ± 0.2 91.2 ± 0.2 92.1 ± 0.2 92.9 ± 0.2
top-10 SVMβ
59.3 ± 0.1 74.1 ± 0.2 80.9 ± 0.2 84.9 ± 0.2 87.5 ± 0.2 89.3 ± 0.2 90.7 ± 0.2 91.7 ± 0.2 92.6 ± 0.2 93.4 ± 0.2

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

59.7 ± 0.1 73.8 ± 0.1 80.3 ± 0.2 84.2 ± 0.2 86.8 ± 0.2 88.6 ± 0.2 90.0 ± 0.2 91.1 ± 0.2 92.0 ± 0.2 92.8 ± 0.2
59.7 ± 0.1 73.8 ± 0.2 80.3 ± 0.2 84.3 ± 0.2 86.8 ± 0.2 88.6 ± 0.3 90.0 ± 0.2 91.2 ± 0.2 92.1 ± 0.2 92.9 ± 0.2
59.6 ± 0.1 73.8 ± 0.1 80.4 ± 0.3 84.3 ± 0.2 86.9 ± 0.3 88.7 ± 0.3 90.1 ± 0.2 91.2 ± 0.2 92.1 ± 0.2 92.9 ± 0.2
59.6 ± 0.2 73.9 ± 0.2 80.5 ± 0.2 84.4 ± 0.2 87.0 ± 0.2 88.8 ± 0.3 90.2 ± 0.2 91.3 ± 0.2 92.2 ± 0.2 93.0 ± 0.2
59.5 ± 0.1 74.1 ± 0.1 80.7 ± 0.2 84.6 ± 0.2 87.1 ± 0.2 88.9 ± 0.3 90.3 ± 0.2 91.4 ± 0.2 92.3 ± 0.2 93.1 ± 0.2
59.3 ± 0.1 74.2 ± 0.2 81.0 ± 0.2 85.0 ± 0.2 87.5 ± 0.2 89.3 ± 0.2 90.7 ± 0.2 91.8 ± 0.2 92.7 ± 0.2 93.4 ± 0.2

59.5 ± 0.2 74.2 ± 0.2 81.1 ± 0.2 85.1 ± 0.2 87.7 ± 0.2 89.6 ± 0.2 91.0 ± 0.1 92.1 ± 0.2 93.0 ± 0.2 93.7 ± 0.2
59.5 ± 0.2 74.3 ± 0.2 81.1 ± 0.2 85.1 ± 0.2 87.7 ± 0.2 89.6 ± 0.2 91.0 ± 0.2 92.1 ± 0.2 93.0 ± 0.2 93.7 ± 0.2
59.4 ± 0.1 74.3 ± 0.2 81.2 ± 0.2 85.2 ± 0.2 87.8 ± 0.2 89.6 ± 0.2 91.0 ± 0.2 92.1 ± 0.2 93.0 ± 0.2 93.7 ± 0.2
59.2 ± 0.2 74.3 ± 0.2 81.2 ± 0.2 85.2 ± 0.2 87.8 ± 0.2 89.7 ± 0.2 91.1 ± 0.2 92.2 ± 0.2 93.0 ± 0.2 93.7 ± 0.2
58.9 ± 0.1 74.3 ± 0.2 81.2 ± 0.2 85.2 ± 0.2 87.8 ± 0.2 89.7 ± 0.2 91.0 ± 0.2 92.1 ± 0.1 93.0 ± 0.2 93.7 ± 0.2
58.0 ± 0.2 73.7 ± 0.2 81.0 ± 0.2 85.1 ± 0.2 87.8 ± 0.2 89.7 ± 0.2 91.0 ± 0.2 92.2 ± 0.1 93.1 ± 0.2 93.8 ± 0.1

Table 19: Comparison of different methods in top-k accuracy (%).

Method

SVMOVA

LROVA

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

SUN 397 (VGG-16 trained on Places 205, FC7 output)

Top-1

Top-2

Top-3

Top-4

Top-5

Top-6

Top-7

Top-8

Top-9

Top-10

65.4 ± 0.2 77.6 ± 0.1 83.8 ± 0.2 87.2 ± 0.1 89.6 ± 0.1 91.3 ± 0.1 92.6 ± 0.2 93.6 ± 0.2 94.3 ± 0.2 94.9 ± 0.1

67.6 ± 0.1 81.5 ± 0.2 87.2 ± 0.2 90.4 ± 0.2 92.4 ± 0.1 93.7 ± 0.1 94.7 ± 0.1 95.4 ± 0.1 96.0 ± 0.1 96.4 ± 0.1

top-1 SVMα / SVMMulti 65.8 ± 0.1 79.0 ± 0.2 85.1 ± 0.2 88.4 ± 0.2 90.8 ± 0.1 92.3 ± 0.1 93.3 ± 0.1 94.2 ± 0.1 94.8 ± 0.1 95.3 ± 0.1
top-2 SVMα
66.4 ± 0.2 80.2 ± 0.2 86.1 ± 0.1 89.4 ± 0.1 91.5 ± 0.1 92.9 ± 0.2 93.8 ± 0.2 94.6 ± 0.2 95.3 ± 0.1 95.7 ± 0.1
top-3 SVMα
66.5 ± 0.2 80.6 ± 0.2 86.5 ± 0.1 89.7 ± 0.2 91.8 ± 0.1 93.2 ± 0.1 94.2 ± 0.1 95.0 ± 0.1 95.3 ± 0.1 95.9 ± 0.1
top-4 SVMα
66.4 ± 0.2 80.8 ± 0.2 86.8 ± 0.1 90.0 ± 0.2 92.1 ± 0.2 93.4 ± 0.1 94.4 ± 0.1 95.1 ± 0.1 95.7 ± 0.1 96.2 ± 0.1
top-5 SVMα
66.3 ± 0.2 81.0 ± 0.2 87.0 ± 0.2 90.2 ± 0.1 92.2 ± 0.2 93.6 ± 0.1 94.5 ± 0.1 95.2 ± 0.1 95.8 ± 0.1 96.3 ± 0.1
top-10 SVMα
64.8 ± 0.3 80.9 ± 0.1 87.2 ± 0.2 90.5 ± 0.2 92.6 ± 0.1 93.9 ± 0.1 94.9 ± 0.1 95.6 ± 0.1 96.2 ± 0.1 96.6 ± 0.1

67.4 ± 0.2 81.1 ± 0.2 86.8 ± 0.1 90.0 ± 0.1 92.0 ± 0.1 93.4 ± 0.1 94.3 ± 0.1 95.1 ± 0.1 95.7 ± 0.1 96.1 ± 0.1
67.2 ± 0.2 81.1 ± 0.2 86.9 ± 0.2 90.1 ± 0.2 92.1 ± 0.1 93.4 ± 0.1 94.4 ± 0.1 95.1 ± 0.1 95.7 ± 0.1 96.2 ± 0.1
67.0 ± 0.2 81.2 ± 0.2 87.0 ± 0.1 90.2 ± 0.2 92.2 ± 0.1 93.5 ± 0.1 94.5 ± 0.1 95.2 ± 0.1 95.7 ± 0.1 96.2 ± 0.0
66.8 ± 0.2 81.2 ± 0.2 87.1 ± 0.1 90.3 ± 0.2 92.3 ± 0.2 93.6 ± 0.1 94.5 ± 0.1 95.2 ± 0.1 95.8 ± 0.1 96.3 ± 0.0
66.5 ± 0.2 81.3 ± 0.2 87.2 ± 0.1 90.4 ± 0.2 92.4 ± 0.2 93.7 ± 0.1 94.6 ± 0.1 95.3 ± 0.1 95.9 ± 0.1 96.3 ± 0.0
64.9 ± 0.3 80.9 ± 0.1 87.3 ± 0.2 90.6 ± 0.2 92.6 ± 0.2 94.0 ± 0.1 94.9 ± 0.1 95.6 ± 0.1 96.2 ± 0.1 96.6 ± 0.1

top-1 SVMβ / SVMMulti 65.8 ± 0.1 79.0 ± 0.2 85.1 ± 0.2 88.4 ± 0.2 90.8 ± 0.1 92.3 ± 0.1 93.3 ± 0.1 94.2 ± 0.1 94.8 ± 0.1 95.3 ± 0.1
top-2 SVMβ
66.4 ± 0.2 80.1 ± 0.1 86.0 ± 0.2 89.3 ± 0.2 91.4 ± 0.1 92.7 ± 0.2 93.8 ± 0.1 94.6 ± 0.2 95.3 ± 0.1 95.8 ± 0.1
top-3 SVMβ
66.8 ± 0.2 80.7 ± 0.2 86.5 ± 0.1 89.7 ± 0.2 91.7 ± 0.1 93.1 ± 0.2 94.2 ± 0.1 94.7 ± 0.1 95.3 ± 0.1 95.9 ± 0.1
top-4 SVMβ
66.9 ± 0.2 80.9 ± 0.2 86.8 ± 0.1 90.0 ± 0.2 92.0 ± 0.2 93.4 ± 0.1 94.4 ± 0.1 95.1 ± 0.1 95.7 ± 0.1 96.1 ± 0.1
top-5 SVMβ
67.0 ± 0.2 81.2 ± 0.2 87.0 ± 0.1 90.2 ± 0.2 92.2 ± 0.1 93.6 ± 0.1 94.5 ± 0.1 95.2 ± 0.1 95.8 ± 0.1 96.2 ± 0.1
top-10 SVMβ
66.9 ± 0.2 81.5 ± 0.2 87.4 ± 0.2 90.7 ± 0.1 92.6 ± 0.1 93.9 ± 0.1 94.8 ± 0.1 95.5 ± 0.1 96.1 ± 0.1 96.5 ± 0.0

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

67.4 ± 0.2 81.1 ± 0.2 86.8 ± 0.1 90.0 ± 0.1 92.0 ± 0.1 93.4 ± 0.1 94.3 ± 0.1 95.1 ± 0.1 95.7 ± 0.1 96.1 ± 0.1
67.3 ± 0.2 81.1 ± 0.2 86.8 ± 0.1 90.0 ± 0.2 92.0 ± 0.1 93.4 ± 0.1 94.4 ± 0.1 95.1 ± 0.1 95.7 ± 0.1 96.2 ± 0.1
67.3 ± 0.2 81.2 ± 0.2 86.9 ± 0.1 90.1 ± 0.1 92.1 ± 0.1 93.4 ± 0.1 94.4 ± 0.1 95.1 ± 0.1 95.7 ± 0.1 96.2 ± 0.1
67.2 ± 0.2 81.3 ± 0.2 87.0 ± 0.1 90.2 ± 0.2 92.2 ± 0.1 93.5 ± 0.1 94.5 ± 0.1 95.2 ± 0.1 95.8 ± 0.1 96.2 ± 0.1
67.2 ± 0.2 81.4 ± 0.2 87.2 ± 0.2 90.3 ± 0.1 92.3 ± 0.1 93.6 ± 0.1 94.6 ± 0.1 95.3 ± 0.1 95.9 ± 0.1 96.3 ± 0.1
66.9 ± 0.2 81.5 ± 0.2 87.5 ± 0.2 90.7 ± 0.1 92.6 ± 0.1 93.9 ± 0.1 94.9 ± 0.1 95.5 ± 0.1 96.1 ± 0.1 96.5 ± 0.1

67.5 ± 0.1 81.7 ± 0.2 87.7 ± 0.2 90.9 ± 0.2 92.9 ± 0.1 94.2 ± 0.1 95.1 ± 0.1 95.8 ± 0.1 96.4 ± 0.1 96.8 ± 0.1
67.4 ± 0.2 81.8 ± 0.2 87.7 ± 0.2 90.9 ± 0.1 92.9 ± 0.1 94.2 ± 0.1 95.1 ± 0.1 95.8 ± 0.1 96.4 ± 0.1 96.8 ± 0.1
67.2 ± 0.2 81.8 ± 0.2 87.7 ± 0.2 90.9 ± 0.1 92.9 ± 0.1 94.2 ± 0.1 95.1 ± 0.1 95.8 ± 0.1 96.4 ± 0.1 96.8 ± 0.1
66.9 ± 0.2 81.7 ± 0.2 87.7 ± 0.2 91.0 ± 0.1 92.9 ± 0.1 94.2 ± 0.1 95.2 ± 0.1 95.9 ± 0.1 96.4 ± 0.1 96.8 ± 0.1
66.6 ± 0.3 81.6 ± 0.2 87.7 ± 0.2 91.0 ± 0.1 92.9 ± 0.1 94.2 ± 0.1 95.2 ± 0.1 95.9 ± 0.1 96.4 ± 0.1 96.8 ± 0.1
65.2 ± 0.3 81.0 ± 0.2 87.4 ± 0.1 90.8 ± 0.2 92.8 ± 0.1 94.2 ± 0.1 95.2 ± 0.1 95.9 ± 0.1 96.4 ± 0.1 96.8 ± 0.1

Table 20: Comparison of different methods in top-k accuracy (%).

Places 205 (AlexNet trained on Places 205, FC7 output, provided by [28])

Method

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

top-1 SVMα / SVMMulti 50.6
top-2 SVMα
51.1
top-3 SVMα
51.3
top-4 SVMα
51.2
top-5 SVMα
50.8
top-10 SVMα
50.1

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

51.8
51.5
51.5
51.3
50.9
50.2

50.6
51.0
51.3
51.4
51.3
50.9

51.8
51.7
51.5
51.5
51.3
51.0

51.1
51.0
50.9
50.7
50.3
48.9

64.5
65.7
66.2
66.3
66.2
65.8

66.4
66.4
66.4
66.4
66.2
65.8

64.5
65.6
66.0
66.2
66.3
66.1

66.4
66.4
66.3
66.4
66.4
66.1

66.1
66.1
66.1
66.0
65.8
64.9

71.4
73.1
73.2
73.5
73.7
73.4

73.5
73.5
73.5
73.7
73.6
73.4

71.4
72.7
73.4
73.6
73.7
73.5

73.5
73.5
73.7
73.7
73.7
73.5

73.5
73.4
73.4
73.3
73.3
72.7

75.5
77.5
77.9
78.1
78.2
78.3

78.1
78.1
78.1
78.1
78.2
78.3

75.5
77.4
77.9
78.0
78.3
78.4

78.1
78.0
78.2
78.3
78.3
78.3

78.1
78.1
78.1
78.0
77.8
77.5

78.5
80.7
81.3
81.4
81.4
81.6

81.4
81.4
81.4
81.5
81.5
81.7

78.5
80.6
81.3
81.4
81.4
81.7

81.4
81.4
81.4
81.5
81.4
81.7

81.5
81.5
81.5
81.5
81.3
81.0

80.7
83.1
83.6
83.7
83.9
84.0

83.9
83.8
83.8
83.8
83.9
84.0

80.7
82.9
83.6
83.8
83.8
84.0

83.9
83.9
83.8
83.8
83.8
84.0

84.1
84.0
83.9
83.9
83.9
83.7

82.5
84.9
85.6
85.7
85.8
86.0

85.7
85.7
85.7
85.9
85.9
86.0

82.5
84.9
85.6
85.7
85.8
86.0

85.7
85.7
85.8
85.8
85.8
86.0

85.9
85.8
85.8
85.7
85.7
85.6

84.0
86.3
87.1
87.3
87.5
87.6

87.4
87.3
87.4
87.5
87.5
87.6

84.0
86.1
87.1
87.3
87.5
87.6

87.4
87.4
87.4
87.4
87.5
87.6

87.6
87.6
87.5
87.5
87.3
87.2

85.1
87.5
88.3
88.7
88.9
89.0

88.7
88.6
88.7
88.8
88.9
89.0

85.1
87.4
88.3
88.7
88.8
89.0

88.7
88.7
88.8
88.8
88.8
89.0

88.9
88.9
88.9
88.9
88.8
88.7

86.2
88.4
89.4
89.7
90.0
90.1

89.8
89.7
89.8
89.9
90.0
90.2

86.2
88.4
89.3
89.8
89.9
90.2

89.8
89.8
89.8
89.9
90.0
90.2

90.0
90.0
89.9
89.9
89.9
89.8

Table 21: Comparison of different methods in top-k accuracy (%).

Places 205 (VGG-16 trained on Places 205, FC7 output)

Method

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

top-1 SVMα / SVMMulti 58.4
top-2 SVMα
58.6
top-3 SVMα
58.6
top-4 SVMα
58.6
top-5 SVMα
58.4
top-10 SVMα
58.0

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

72.5
73.4
73.7
73.8
73.8
73.2

74.2
73.9
74.0
73.8
73.8
73.2

72.5
73.6
73.9
73.9
74.0
74.0

74.2
74.2
74.1
74.0
74.0
74.0

73.9
73.8
73.8
73.6
73.5
72.8

73.4
72.8
71.6
71.2
68.4

78.7
80.1
80.3
80.5
80.5
80.4

80.5
80.4
80.5
80.5
80.5
80.4

78.7
80.0
80.4
80.6
80.6
80.7

80.5
80.5
80.6
80.7
80.7
80.7

80.6
80.6
80.6
80.5
80.4
80.0

80.1
80.0
79.2
79.0
76.9

82.3
84.1
84.5
84.6
84.5
84.6

84.6
84.6
84.6
84.6
84.5
84.5

82.3
83.9
84.5
84.6
84.7
84.8

84.6
84.6
84.7
84.7
84.7
84.8

84.8
84.7
84.8
84.6
84.5
84.2

84.2
84.1
83.6
83.5
82.2

84.7
86.6
87.3
87.4
87.4
87.4

87.3
87.5
87.6
87.4
87.5
87.5

84.7
86.4
87.2
87.4
87.5
87.6

87.3
87.4
87.5
87.5
87.5
87.6

87.6
87.6
87.6
87.5
87.4
87.2

87.2
87.1
86.9
86.9
85.8

86.4
88.5
89.3
89.5
89.5
89.6

89.6
89.6
89.6
89.6
89.5
89.6

86.4
88.3
89.2
89.6
89.6
89.7

89.6
89.6
89.7
89.7
89.7
89.7

89.7
89.6
89.6
89.6
89.6
89.3

89.4
89.3
89.2
89.1
88.3

87.5
89.9
90.8
91.0
91.1
91.2

91.1
91.1
91.1
91.1
91.2
91.3

87.5
89.6
90.7
91.0
91.0
91.3

91.1
91.0
91.1
91.1
91.1
91.3

91.3
91.2
91.2
91.2
91.2
91.0

90.8
90.9
90.8
90.8
90.2

88.4
90.8
91.8
92.1
92.3
92.5

92.2
92.2
92.3
92.3
92.3
92.5

88.4
90.6
91.7
92.1
92.2
92.4

92.2
92.2
92.2
92.3
92.3
92.4

92.5
92.4
92.5
92.4
92.4
92.3

92.0
92.2
92.0
92.0
91.7

89.2
91.6
92.6
93.0
93.2
93.5

93.2
93.1
93.2
93.2
93.2
93.5

89.2
91.4
92.6
93.0
93.2
93.5

93.2
93.1
93.1
93.2
93.3
93.5

93.5
93.5
93.5
93.4
93.4
93.4

93.2
93.3
93.0
93.1
92.9

89.9
92.2
93.3
93.8
94.0
94.3

93.8
93.7
93.9
94.1
94.1
94.3

89.9
92.0
93.2
93.7
94.0
94.2

93.8
93.7
93.8
94.0
94.2
94.3

94.3
94.3
94.2
94.2
94.2
94.1

94.0
94.2
94.1
94.0
93.8

59.2
59.0
58.9
58.7
58.5
58.0

58.4
58.6
58.8
58.8
58.9
58.7

59.2
59.0
59.0
58.9
58.9
58.7

59.0
58.9
58.7
58.5
58.1
57.0

57.7
56.8
55.2
54.2
51.1

Table 22: Comparison of different methods in top-k accuracy (%).

ILSVRC 2012 (AlexNet trained on ImageNet, FC7 output, provided by [28])

Method

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

top-1 SVMα / SVMMulti 56.6
top-2 SVMα
56.6
top-3 SVMα
56.6
top-4 SVMα
56.5
top-5 SVMα
56.5
top-10 SVMα
55.9

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

57.1
56.7
56.6
56.6
56.5
55.9

56.6
56.9
57.0
57.0
57.1
56.9

57.1
57.1
54.6
57.1
57.1
56.9

55.8
55.6
55.5
55.4
55.2
54.8

67.3
68.1
68.3
68.4
68.4
68.2

68.3
68.4
68.4
68.5
68.5
68.2

67.3
68.0
68.3
68.4
68.4
68.4

68.3
68.3
66.1
68.5
68.5
68.5

67.4
67.4
67.4
67.3
67.2
66.9

72.4
73.2
73.6
73.8
73.8
73.8

73.5
73.6
73.8
73.9
73.9
73.8

72.4
73.2
73.5
73.6
73.7
73.9

73.5
73.6
71.5
73.8
73.8
73.9

73.1
73.0
73.0
73.0
72.9
72.7

75.4
76.4
76.8
77.1
77.2
77.3

76.7
76.8
77.0
77.1
77.3
77.4

75.4
76.2
76.7
76.9
76.9
77.3

76.7
76.7
74.7
77.0
77.0
77.3

76.6
76.5
76.5
76.5
76.5
76.4

77.7
78.6
79.0
79.3
79.4
79.8

78.9
79.0
79.2
79.4
79.5
79.7

77.7
78.5
78.9
79.1
79.3
79.5

78.9
78.9
77.1
79.2
79.3
79.6

79.0
79.0
78.9
78.9
78.9
78.8

79.3
80.3
80.7
81.1
81.1
81.4

80.6
80.8
80.9
81.1
81.2
81.4

79.3
80.2
80.6
80.8
81.0
81.2

80.6
80.6
78.7
80.9
81.0
81.2

80.8
80.8
80.7
80.7
80.7
80.6

80.8
81.7
82.1
82.4
82.5
82.8

82.0
82.1
82.4
82.5
82.6
82.8

80.8
81.6
82.0
82.2
82.4
82.7

82.0
82.0
80.2
82.3
82.5
82.7

82.2
82.2
82.2
82.1
82.1
82.1

82.0
82.8
83.2
83.5
83.7
84.0

83.1
83.2
83.5
83.7
83.7
84.0

82.0
82.7
83.1
83.4
83.5
83.8

83.1
83.2
81.3
83.5
83.6
83.8

83.4
83.4
83.4
83.4
83.4
83.3

82.9
83.7
84.2
84.5
84.6
85.0

84.1
84.2
84.4
84.6
84.6
85.0

82.9
83.6
84.0
84.3
84.5
84.8

84.1
84.2
82.3
84.5
84.6
84.8

84.4
84.4
84.4
84.4
84.3
84.3

83.7
84.6
85.0
85.3
85.4
85.8

84.9
85.0
85.2
85.3
85.5
85.8

83.7
84.4
84.8
85.1
85.2
85.6

84.9
84.9
83.1
85.2
85.3
85.7

85.3
85.2
85.2
85.2
85.2
85.2

Table 23: Comparison of different methods in top-k accuracy (%).

ILSVRC 2012 (VGG-16 trained on ImageNet, FC7 output)

Method

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

top-1 SVMα / SVMMulti 68.3
top-2 SVMα
68.3
top-3 SVMα
68.2
top-4 SVMα
68.0
top-5 SVMα
67.8
top-10 SVMα
67.0

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

78.6
79.3
79.5
79.6
79.5
79.0

79.5
79.6
79.6
79.7
79.6
79.1

78.6
79.2
79.5
79.5
79.6
79.5

79.5
79.5
79.5
79.6
79.6
79.5

78.5
78.4
78.4
78.3
78.2
77.8

78.1
77.8
77.7
77.1
76.7

82.9
83.7
84.0
84.1
84.1
83.8

83.9
84.0
84.1
84.2
84.1
83.8

82.9
83.6
83.9
84.0
84.1
84.0

83.9
84.0
84.1
84.1
84.1
84.1

83.2
83.2
83.1
83.1
83.0
82.8

83.0
82.8
82.7
82.3
82.0

85.4
86.3
86.5
86.6
86.6
86.5

86.4
86.5
86.6
86.7
86.6
86.5

85.4
86.1
86.4
86.6
86.6
86.6

86.4
86.5
86.6
86.6
86.6
86.6

85.9
85.9
85.9
85.8
85.8
85.7

85.7
85.7
85.6
85.2
85.0

87.0
87.8
88.1
88.3
88.2
88.3

88.0
88.1
88.2
88.4
88.4
88.3

87.0
87.6
88.0
88.2
88.2
88.3

88.0
88.1
88.1
88.3
88.3
88.3

87.7
87.8
87.8
87.8
87.7
87.6

87.6
87.5
87.5
87.3
87.0

88.2
89.0
89.3
89.5
89.5
89.6

89.3
89.3
89.4
89.6
89.6
89.6

88.2
88.9
89.2
89.4
89.5
89.6

89.3
89.3
89.4
89.5
89.5
89.6

89.1
89.1
89.1
89.1
89.1
89.0

89.0
89.0
88.9
88.7
88.5

89.2
89.9
90.2
90.4
90.5
90.6

90.2
90.2
90.3
90.5
90.5
90.6

89.2
89.8
90.1
90.3
90.4
90.6

90.2
90.2
90.3
90.4
90.4
90.6

90.1
90.1
90.1
90.1
90.1
90.0

90.0
89.9
89.9
89.8
89.6

89.9
90.6
91.0
91.2
91.2
91.4

90.9
91.0
91.1
91.2
91.3
91.4

89.9
90.6
90.8
91.0
91.1
91.3

90.9
91.0
91.0
91.1
91.2
91.4

91.0
91.0
91.0
91.0
91.0
90.9

90.8
90.8
90.8
90.6
90.5

90.6
91.3
91.6
91.8
91.9
92.1

91.6
91.7
91.8
91.8
92.0
92.1

90.6
91.2
91.5
91.6
91.7
92.0

91.6
91.6
91.6
91.7
91.8
92.0

91.7
91.7
91.7
91.7
91.6
91.6

91.6
91.5
91.5
91.3
91.2

91.1
91.8
92.1
92.3
92.4
92.6

92.1
92.2
92.3
92.3
92.5
92.6

91.1
91.7
91.9
92.1
92.2
92.5

92.1
92.1
92.2
92.2
92.3
92.5

92.2
92.2
92.2
92.2
92.2
92.1

92.1
92.1
92.1
91.9
91.8

68.7
68.5
68.2
68.0
67.9
67.1

68.3
68.6
68.5
68.4
68.4
68.0

68.7
68.7
68.6
68.5
68.4
68.0

67.2
67.1
66.8
66.7
66.5
65.8

66.6
65.9
66.0
65.0
64.6

Table 24: Comparison of different methods in top-k accuracy (%).

Loss Functions for Top-k Error: Analysis and Insights

Maksim Lapin,1 Matthias Hein2 and Bernt Schiele1
1Max Planck Institute for Informatics, Saarbrücken, Germany
2Saarland University, Saarbrücken, Germany

6
1
0
2
 
r
p
A
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
8
4
0
0
.
2
1
5
1
:
v
i
X
r
a

Abstract

In order to push the performance on realistic computer
vision tasks, the number of classes in modern benchmark
datasets has signiﬁcantly increased in recent years. This in-
crease in the number of classes comes along with increased
ambiguity between the class labels, raising the question if
top-1 error is the right performance measure. In this pa-
per, we provide an extensive comparison and evaluation of
established multiclass methods comparing their top-k per-
formance both from a practical as well as from a theoret-
ical perspective. Moreover, we introduce novel top-k loss
functions as modiﬁcations of the softmax and the multiclass
SVM losses and provide efﬁcient optimization schemes for
them. In the experiments, we compare on various datasets
all of the proposed and established methods for top-k error
optimization. An interesting insight of this paper is that the
softmax loss yields competitive top-k performance for all
k simultaneously. For a speciﬁc top-k error, our new top-
k losses lead typically to further improvements while being
faster to train than the softmax.

1. Introduction

The number of classes is rapidly growing in modern
computer vision benchmarks [47, 62]. Typically, this also
leads to ambiguity in the labels as classes start to overlap.
Even for humans, the error rates in top-1 performance are
often quite high (≈ 30% on SUN 397 [60]). While previous
research focuses on minimizing the top-1 error, we address
top-k error optimization in this paper. We are interested in
two cases: a) achieving small top-k error for all reasonably
small k; and b) minimization of a speciﬁc top-k error.

While it is argued in [2] that the one-versus-all (OVA)
SVM scheme performs on par in top-1 and top-5 accuracy
with the other SVM variations based on ranking losses,
we have recently shown in [28] that minimization of the
top-k hinge loss leads to improvements in top-k perfor-
mance compared to OVA SVM, multiclass SVM, and other
ranking-based formulations.
In this paper, we study top-
k error optimization from a wider perspective. On the

one hand, we compare OVA schemes and direct multi-
class losses in extensive experiments, and on the other,
we present theoretical discussion regarding their calibra-
tion for the top-k error. Based on these insights, we sug-
gest 4 new families of loss functions for the top-k error.
Two are smoothed versions of the top-k hinge losses [28],
and the other two are top-k versions of the softmax loss.
We discuss their advantages and disadvantages, and for the
convex losses provide an efﬁcient implementation based on
stochastic dual coordinate ascent (SDCA) [48].

We evaluate a battery of loss functions on 11 datasets
of different tasks ranging from text classiﬁcation to large
scale vision benchmarks, including ﬁne-grained and scene
classiﬁcation. We systematically optimize and report results
separately for each top-k accuracy. One interesting message
that we would like to highlight is that the softmax loss is
able to optimize all top-k error measures simultaneously.
This is in contrast to multiclass SVM and is also reﬂected
in our experiments. Finally, we show that our new top-k
variants of smooth multiclass SVM and the softmax loss
can further improve top-k performance for a speciﬁc k.

Related work. Top-k optimization has recently received
revived attention with the advent of large scale problems
[20, 28, 30, 31]. The top-k error in multiclass classiﬁca-
tion, which promotes good ranking of class labels for each
example, is closely related to the precision@k metric in in-
formation retrieval, which counts the fraction of positive in-
stances among the top-k ranked examples. In essence, both
approaches enforce a desirable ranking of items [28].

The classic approaches optimize pairwise ranking with
SVMstruct [25, 53], RankNet [11], and LaRank [7]. An
alternative direction was proposed by Usunier et al. [54],
who described a general family of convex loss functions for
ranking and classiﬁcation. One of the loss functions that
we consider (top-k SVMβ [28]) also falls into that fam-
ily. Weston et al. [59] then introduced Wsabie, which opti-
mizes an approximation of a ranking-based loss from [54].
A Bayesian approach was suggested by [51].

Recent works focus on the top of the ranked list [1, 9,
39, 46], scalability to large datasets [20, 28, 30], explore
transductive learning [31] and prediction of tuples [45].

1

Method

Name

Loss function

Conjugate SDCA update Top-k calibrated Convex

One-vs-all (OVA) SVM

OVA logistic regression

Multiclass SVM

SVMOVA
LROVA
SVMMulti
LRMulti
top-k SVMα Top-k hinge (α)
top-k SVMβ Top-k hinge (β)
top-k SVMα
top-k SVMβ

γ Smooth top-k hinge (α) ∗
γ Smooth top-k hinge (β) ∗

Softmax (maximum entropy)

top-k Ent

top-k Enttr

Top-k entropy ∗
Truncated top-k entropy ∗

max{0, 1 − a}
log(1 + e−a)
max (cid:8)0, (a + c)π1
log (cid:0) (cid:80)
max (cid:8)0, 1
(cid:80)k

(cid:9)
j∈Y exp(aj)(cid:1)
(cid:80)k
j=1(a + c)πj
j=1 max (cid:8)0, (a + c)πj
Eq. (12) w/ ∆α
k
Eq. (12) w/ ∆β
k
Prop. 9

1
k

k

(cid:9)

(cid:9)

[48]

[48]

[28, 48]

Prop. 8

[28, 48]

Prop. 12

[28]

[28]

Prop. 7

Prop. 11

Eq. (14)

Prop. 12

no1 (Prop. 1)
yes (Prop. 2)

no (Prop. 3)

yes (Prop. 4)

open

question

for k > 1

yes

Eq. (22)

-

-

yes (Prop. 10)

no

Note that SVMMulti ≡ top-1 SVMα ≡ top-1 SVMβ and LRMulti ≡ top-1 Ent ≡ top-1 Enttr.
We let a (cid:44) yf (x) (binary one-vs-all); a (cid:44) (fj(x) − fy(x))j∈Y , c (cid:44) 1 − ey (multiclass); π : aπ1 ≥ . . . ≥ aπm .

Table 1: Overview of the methods we consider and our contributions. ∗Novel loss. 1But smoothed one is (Prop. 5).

Contributions. We study the problem of top-k error op-
timization on a diverse range of learning tasks. We con-
sider existing methods as well as propose 4 novel loss func-
tions for minimizing the top-k error. A brief overview of
the methods is given in Table 1. For the proposed convex
top-k losses, we develop an efﬁcient optimization scheme
based on SDCA1, which can also be used for training with
the softmax loss. All methods are evaluated empirically in
terms of the top-k error and, whenever possible, in terms
of classiﬁcation calibration. We discover that the softmax
loss and the proposed smooth top-1 SVM are astonishingly
competitive in all top-k errors. Further small improvements
can be obtained with the new top-k losses.

2. Loss Functions for Top-k Error

We consider multiclass problems with m classes where
i=1 consists of n examples xi ∈ Rd
the training set (xi, yi)n
along with the corresponding labels yi ∈ Y (cid:44) {1, . . . , m}.
We use π and τ to denote a permutation of (indexes) Y. Un-
less stated otherwise, aπ reorders components of a vector
a ∈ Rm in descending order, i.e. aπ1 ≥ aπ2 ≥ . . . ≥ aπm .
While we consider linear classiﬁers in our experiments, all
loss functions below are formulated in the general setting
where a function f : X → Rm is learned and predic-
tion at test time is done via arg maxy∈Y fy(x), resp. the
top-k predictions. For the linear case, all predictors fy
have the form fy(x) = (cid:104)wy, x(cid:105). Let W ∈ Rd×m be the
stacked weight matrix, L : Y × Rm → R be a convex
loss function, and λ > 0 be a regularization parameter.
We consider the following multiclass optimization problem
minW

i=1 L(yi, W (cid:62)xi) + λ (cid:107)W (cid:107)2
F .

(cid:80)n

1
n

1 Code available at: https://github.com/mlapin/libsdca

(cid:75)

We use the Iverson bracket notation

, deﬁned as
= 1 if P is true, 0 otherwise; and introduce a short-
P
(cid:74)
hand py(x) (cid:44) Pr(Y = y | X = x). We generalize the
standard zero-one error and allow k guesses instead of one.
Formally, the top-k zero-one loss (top-k error) is

(cid:74)

(cid:75)

P

errk(y, f (x)) (cid:44)

fπk (x) > fy(x)

(1)

(cid:74)
Note that for k = 1 we recover the standard zero-one error.
Top-k accuracy is deﬁned as 1 minus the top-k error.

.
(cid:75)

2.1. Bayes Optimality and Top-k Calibration

In this section, we establish the best achievable top-k er-
ror, determine when a classiﬁer achieves it, and deﬁne a
notion of top-k calibration.

Lemma 1. The Bayes optimal top-k error at x is

min
g∈Rm

EY |X [errk(Y, g) | X = x] = 1 − (cid:80)k

j=1 pτj (x),

where pτ1(x) ≥ pτ2 (x) ≥ . . . ≥ pτm(x). A classiﬁer f is
top-k Bayes optimal at x if and only if

(cid:8)y | fy(x) ≥ fπk (x)(cid:9) ⊂ (cid:8)y | py(x) ≥ pτk (x)(cid:9),

where fπ1(x) ≥ fπ2(x) ≥ . . . ≥ fπm(x).
Proof. Let g ∈ Rm and π be a permutation such that gπ1 ≥
gπ2 ≥ . . . ≥ gπm. The expected top-k error at x is

EY |X [errk(Y, g) | X = x] = (cid:80)
= (cid:80)
gπk > gπy
y∈Y
(cid:74)
(cid:75)
= 1 − (cid:80)k
j=1 pπj (x).

gπk > gy
(cid:75)
(cid:74)
pπy (x) = (cid:80)m
j=k+1 pπj (x)

y∈Y

py(x)

The error is minimal when (cid:80)k
j=1 pπj (x) is maximal, which
corresponds to taking the k largest conditional probabilities
(cid:80)k
j=1 pτj (x) and yields the Bayes optimal top-k error at x.
Since the relative order within {pτj (x)}k
j=1 is irrelevant
for the top-k error, any classiﬁer f (x), for which the sets
{π1, . . . , πk} and {τ1, . . . , τk} coincide, is Bayes optimal.
Note that we assumed w.l.o.g. that there is a clear cut
pτk (x) > pτk+1(x) between the k most likely classes and
the rest. In general, ties can be resolved arbitrarily as long
as we can guarantee that the k largest components of f (x)
correspond to the classes (indexes) that yield the maximal
sum (cid:80)k
j=1 pπj (x) and lead to top-k Bayes optimality.

Optimization of the zero-one loss (and, by extension, the
top-k error) leads to hard combinatorial problems. Instead,
a standard approach is to use a convex surrogate loss which
upper bounds the zero-one error. Under mild conditions on
the loss function [3, 52], the optimal classiﬁer w.r.t. the sur-
rogate yields a Bayes optimal solution for the zero-one loss.
Such loss is called classiﬁcation calibrated, which is known
in statistical learning theory as a necessary condition for a
classiﬁer to be universally Bayes consistent [3]. We intro-
duce now the notion of calibration for the top-k error.
Deﬁnition 1. A loss function L : Y × Rm → R (or a re-
duction scheme) is called top-k calibrated if for all possible
data generating measures on Rd × Y and all x ∈ Rd

arg ming∈Rm EY |X [L(Y, g) | X = x]
⊆ arg ming∈Rm EY |X [errk(Y, g) | X = x].
If a loss is not top-k calibrated, it implies that even in the
limit of inﬁnite data, one does not obtain a classiﬁer with
the Bayes optimal top-k error from Lemma 1.

2.2. OVA and Direct Multiclass Approaches

The standard multiclass problem is often solved using
the one-vs-all (OVA) reduction into a set of m binary clas-
siﬁcation problems. Every class is trained versus the rest
which yields m classiﬁers {fy}y∈Y .

Typically, the binary classiﬁcation problems are formu-
lated with a convex margin-based loss function L(yf (x)),
where L : R → R and y = ±1. We consider in this paper:

L(yf (x)) = max{0, 1 − yf (x)},
L(yf (x)) = log(1 + e−yf (x)).

(2)

(3)

The hinge (2) and logistic (3) losses correspond to the SVM
and logistic regression respectively. We now show when
the OVA schemes are top-k calibrated, not only for k = 1
(standard multiclass loss) but for all k simultaneously.

Lemma 2. The OVA reduction is top-k calibrated for any
1 ≤ k ≤ m if the Bayes optimal function of the convex
margin-based loss L(yf (x)) is a strictly monotonically in-
creasing function of Pr(Y = 1 | X = x).

Proof. For every class y ∈ Y, the Bayes optimal classiﬁer
for the corresponding binary problem has the form

fy(x) = g(cid:0) Pr(Y = y | X = x)(cid:1),

where g is a strictly monotonically increasing function.
The ranking of fy corresponds to the ranking of Pr(Y =
y | X = x) and hence the OVA reduction is top-k calibrated
for any k = 1, . . . , m.

Next, we check if the one-vs-all schemes employing

hinge and logistic regression losses are top-k calibrated.

Proposition 1. OVA SVM is not top-k calibrated.

Proof. First, we show that the Bayes optimal function for
the binary hinge loss is

f ∗(x) = 2
(cid:74)

Pr(Y = 1 | X = x) > 1
2

− 1.

(cid:75)

We decompose the expected loss as

EX,Y [L(Y, f (X))] = EX [EY |X [L(Y, f (x)) | X = x]].

Thus, one can compute the Bayes optimal classiﬁer f ∗
pointwise by solving

arg min
α∈R

EY |X [L(Y, α) | X = x],

for every x ∈ Rd, which leads to the following problem

arg min
α∈R

max{0, 1 − α}p1(x) + max{0, 1 + α}p−1(x),

where py(x) (cid:44) Pr(Y = y | X = x). It is obvious that the
optimal α∗ is contained in [−1, 1]. We get

arg min
−1≤α≤1

(1 − α)p1(x) + (1 + α)p−1(x).

The minimum is attained at the boundary and we get

f ∗(x) =

(cid:40)

+1
−1

if p1(x) > 1
2 ,
if p1(x) ≤ 1
2 .

Therefore, the Bayes optimal classiﬁer for the hinge loss is
not a strictly monotonically increasing function of p1(x).

To show that OVA hinge is not top-k calibrated, we con-
struct an example problem with 3 classes and p1(x) = 0.4,
p2(x) = p3(x) = 0.3. Note that for every class y = 1, 2, 3,
the Bayes optimal binary classiﬁer is −1, hence the pre-
dicted ranking of labels is arbitrary and may not produce
the Bayes optimal top-k error.

In contrast, logistic regression is top-k calibrated.

Proposition 2. OVA logistic regression is top-k calibrated.

Proof. First, we show that the Bayes optimal function for
the binary logistic loss is

Proof. First, we derive the Bayes optimal function.

Let y ∈ arg maxj∈Y pj(x). Given any c ∈ R, a Bayes

optimal function f ∗ : Rd → Rm for the loss (4) is

f ∗(x) = log

(cid:16) px(1)

1 − px(1)

(cid:17)

.

As above, the pointwise optimization problem is

(cid:40)

f ∗
y (x) =

c + 1
c

if maxj∈Y pj(x) ≥ 1
2 ,
otherwise,

f ∗
j (x) = c, j ∈ Y \ {y}.

arg min
α∈R

log(1+exp(−α))p1(x)+log(1+exp(α))p−1(x).

The logistic loss is known to be convex and differentiable
and thus the optimum can be computed via

Let g = f (x) ∈ Rm, then

EY |X [L(Y, g) | X] =

(cid:88)

l∈Y

(cid:8)

max
j∈Y

j (cid:54)= l
(cid:74)

(cid:75)

+ gj − gl

(cid:9)pl(x).

− exp(−α)
1 + exp(−α)

p1(x) +

exp(α)
1 + exp(α)

p−1(x) = 0.

Suppose that the maximum of (gj)j∈Y is not unique. In this
case, we have

> 0,

∀x ∈ (0, 1).

As the loss only depends on the gap gy −gl, we can optimize
this with βl = gy − gl.

Re-writing the ﬁrst fraction we get

−1
1 + exp(α)

p1(x) +

exp(α)
1 + exp(α)

p−1(x) = 0,

which can be solved as α∗ = log
formula for the Bayes optimal classiﬁer stated above.

and leads to the

We check now that the function φ : (0, 1) → R deﬁned

as φ(x) = log( x

1−x ) is strictly monotonically increasing.

(cid:16) p1(x)
p−1(x)

(cid:17)

φ(cid:48)(x) =

1 − x
x
1 − x
x

(cid:0)

(cid:1)

+

1
1 − x
1
(1 − x)2 =

x
(1 − x)2
1
x(1 − x)

=

The derivative is strictly positive on (0, 1), which implies
that φ is strictly monotonically increasing. The logistic loss,
therefore, fulﬁlls the conditions of Lemma 2 and is top-k
calibrated for any 1 ≤ k ≤ m.

An alternative to the OVA scheme with binary losses is to
use a multiclass loss L : Y ×Rm → R directly. We consider
two generalizations of the hinge and logistic losses below:

L(y, f (x)) = max
j∈Y

j (cid:54)= y

(cid:8)
(cid:74)
(cid:16) (cid:80)

+ fj(x) − fy(x)(cid:9),
(cid:17)

(cid:75)

j∈Y exp(fj(x) − fy(x))

.

(4)

(5)

L(y, f (x)) = log

Both the multiclass hinge loss (4) of Crammer & Singer
[15] and the softmax loss (5) are popular losses for mul-
ticlass problems. The latter is also known as the cross-
entropy or multiclass logistic loss and is often used as the
last layer in deep architectures [6, 26, 50]. The multiclass
hinge loss has been shown to be competitive in large-scale
image classiﬁcation [2], however, it is known to be not cal-
ibrated [52] for the top-1 error. Next, we show that it is not
top-k calibrated for any k.

Proposition 3. Multiclass SVM is not top-k calibrated.

(cid:8)

max
j∈Y

j (cid:54)= l
(cid:74)

(cid:75)

+ gj − gl

(cid:9) ≥ 1, ∀ l ∈ Y

(cid:74)

j (cid:54)= l

is always active. The best possible loss
as the term
is obtained by setting gj = c for all j ∈ Y, which yields an
expected loss of 1. On the other hand, if the maximum is
unique and is achieved by gy, then

(cid:75)

+ gj − gl

(cid:9)

(cid:8)

max
j∈Y

j (cid:54)= l
(cid:74)
(cid:40)

=

(cid:75)
1 + gy − gl
max (cid:8)0, maxj(cid:54)=y{1 + gj − gy}(cid:9)

if l (cid:54)= y,
if l = y.

EY |X [L(Y, g) | X = x]

=

(1 + gy − gl)pl(x)

(cid:88)

l(cid:54)=y

=

=

(cid:88)

l(cid:54)=y
(cid:88)

l(cid:54)=y

+ max (cid:8)0, max

{1 + gl − gy}(cid:9)py(x)

l(cid:54)=y

(1 + βl)pl(x) + max (cid:8)0, max

{1 − βl}(cid:9)py(x)

l(cid:54)=y

(1 + βl)pl(x) + max{0, 1 − min
l(cid:54)=y

βl}py(x).

As only the minimal βl enters the last term, the optimum is
achieved if all βl are equal for l (cid:54)= y (otherwise it is possible
to reduce the ﬁrst term without affecting the last term). Let
α (cid:44) βl for all l (cid:54)= y. The problem becomes

(1 + α)pl(x) + max{0, 1 − α}py(x)

min
α≥0

(cid:88)

l(cid:54)=y

≡ min
0≤α≤1

α(1 − 2py(x))

Let p (cid:44) py(x) = Pr(Y = y | X = x). The solution is
(cid:40)

α∗ =

0
1

if p < 1
2 ,
if p ≥ 1
2 ,

and the associated risk is

EY |X [L(Y, g) | X = x] =

(cid:40)
1
2(1 − p)

if p < 1
2 ,
if p ≥ 1
2 .

for j ∈ Y. We note that the critical point is not unique
as multiplication g → κg leaves the equation invariant for
any κ > 0. One can verify that egj = αpj(x) satisﬁes the
equations for any α > 0. This yields a solution

If p < 1
all j ∈ Y and any c ∈ R. Otherwise, p ≥ 1

2 , then the Bayes optimal classiﬁer f ∗
2 and

j (x) = c for

(cid:40)

f ∗
y (x) =

log(αpy(x))
−∞

if py(x) > 0,
otherwise,

(cid:40)

f ∗
j (x) =

c + 1
c

if j = y,
if j ∈ Y \ {y}.

Moreover, we have that the Bayes risk at x is

EY |X [L(Y, f ∗(x)) | X = x] = min{1, 2(1 − p)} ≤ 1.

It follows, that the multiclass hinge loss is not (top-1)
classiﬁcation calibrated at any x where maxy∈Y py(x) < 1
2
as its Bayes optimal classiﬁer reduces to a constant. More-
over, even if py(x) ≥ 1
2 for some y, the loss is not top-k
calibrated for k ≥ 2 as the predicted order of the remaining
classes need not be optimal.

Again, a contrast between the hinge and logistic losses.

Proposition 4. The softmax loss is top-k calibrated.

Proof. The multiclass logistic loss is (top-1) calibrated for
the zero-one error in the following sense. If

f ∗(x) ∈ arg min

EY |X [L(Y, g) | X = x],

g∈Rm

then for some α > 0 and all y ∈ Y

(cid:40)

f ∗
y (x) =

log(α py(x))
−∞

if py(x) > 0,
otherwise,

which implies

arg max
y∈Y

f ∗
y (x) = arg max

Pr(Y = y | X = x).

y∈Y

We now prove this result and show that it also generalizes
to top-k calibration for k > 1. Using the identity
L(y, g) = log (cid:0) (cid:80)

j∈Y egj −gy (cid:1) = log (cid:0) (cid:80)
j∈Y egj (cid:1) − gy
y∈Y py(x) = 1, we write for a g ∈ Rm

and the fact that (cid:80)

EY |X [L(Y, g) | X = x]

(cid:88)

L(y, g)py(x) = log (cid:0) (cid:88)

egy (cid:1) −

=

(cid:88)

gypx(y).

y∈Y

y∈Y

y∈Y

for any ﬁxed α > 0. We note that f ∗
y is a strictly mono-
tonically increasing function of the conditional class prob-
abilities. Therefore, it preserves the ranking of py(x) and
implies that f ∗ is top-k calibrated for any 1 ≤ k ≤ m.

The implicit reason for top-k calibration of the OVA
schemes and the softmax loss is that one can estimate the
probabilities py(x) from the Bayes optimal classiﬁer. Loss
functions which allow this are called proper. We refer to
[41] and references therein for a detailed discussion.

We have established that the OVA logistic regression and
the softmax loss are top-k calibrated for any k, so why
should we be interested in deﬁning new loss functions for
the top-k error? The reason is that calibration is an asymp-
totic property as the Bayes optimal functions are obtained
pointwise. The picture changes if we use linear classiﬁers,
since they obviously cannot be minimized independently at
each point. Indeed, most of the Bayes optimal classiﬁers
cannot be realized by linear functions.

In particular, convexity of the softmax and multiclass
hinge losses leads to phenomena where errk(y, f (x)) = 0,
but L(y, f (x)) (cid:29) 0. This happens if fπ1(x) (cid:29) fy(x) ≥
fπk (x) and adds a bias when working with “rigid” function
classes such as linear ones. The loss functions which we
introduce in the following are modiﬁcations of the above
losses with the goal of alleviating that phenomenon.

2.3. Smooth Top-k Hinge Loss

Recently, we introduced two top-k versions of the mul-
ticlass hinge loss (4) in [28], where the second version is
based on the family of ranking losses introduced earlier by
[54]. We use our notation from [28] for direct comparison
and refer to the ﬁrst version as α and the second one as β.
Let c = 1 − ey, where 1 is the all ones vector, ey is the y-th
basis vector, and let a ∈ Rm be deﬁned componentwise as
aj (cid:44) (cid:104)wj, x(cid:105) − (cid:104)wy, x(cid:105). The two top-k hinge losses are

L(a) = max (cid:8)0, 1
L(a) = 1
k

j=1(a + c)πj
j=1max (cid:8)0, (a + c)πj

(cid:80)k

(cid:80)k

k

(cid:9) (cid:0)top-k SVMα(cid:1), (6)
(cid:9) (cid:0)top-k SVMβ(cid:1), (7)

As the loss is convex and differentiable, we get the global
optimum by computing a critical point. We have

∂
∂gj

EY |X [L(Y, g) | X = x] =

− pj(x) = 0

egj
y∈Y egy

(cid:80)

where (a)πj is the j-th largest component of a.
It was
shown in [28] that (6) is a tighter upper bound on the top-k
error than (7), however, both losses performed similarly in
our experiments. In the following, we simply refer to them
as the top-k hinge or the top-k SVM loss.

Both losses reduce to the multiclass hinge loss (4) for
k = 1. Therefore, they are unlikely to be top-k calibrated,
even though we can currently neither prove nor disprove
this for k > 1. The multiclass hinge loss is not calibrated
as it is non-smooth and does not allow to estimate the class
conditional probabilities py(x). Our new family of smooth
top-k hinge losses is based on the Moreau-Yosida regular-
ization [5, 34]. This technique has been used in [48] to
smooth the binary hinge loss (2). Interestingly, smooth bi-
nary hinge loss fulﬁlls the conditions of Lemma 2 and leads
to a top-k calibrated OVA scheme. The hope is that the
smooth top-k hinge loss becomes top-k calibrated as well.
Smoothing works by adding a quadratic term to the
conjugate function2, which then becomes strongly convex.
Smoothness of the loss, among other things, typically leads
to much faster optimization as we discuss in Section 3.

Proposition 5. OVA smooth hinge is top-k calibrated.

Proof. In order to derive the smooth hinge loss, we ﬁrst
compute the conjugate of the standard binary hinge loss,

L(α) = max{0, 1 − α},
L∗(β) = sup
α∈R
(cid:40)

(cid:8)αβ − max{0, 1 − α}(cid:9)

=

if − 1 ≤ β ≤ 0,

β
∞ otherwise.

Case 0 < γ ≤ 1. Consider the case 1 − γ ≤ α ≤ 1,

α − 1
γ

p + (1 − p) = 0 =⇒ α∗ = 1 − γ

1 − p
p

.

This case corresponds to p ≥ 1
2 , which follows from the
constraint α∗ ≥ 1 − γ. Next, consider γ − 1 ≤ α ≤ 1 − γ,

−p + (1 − p) = 1 − 2p (cid:54)= 0,

unless p = 1
Finally, consider −1 ≤ α ≤ γ − 1 ≤ 1 − γ. Then

2 , which is already captured by the ﬁrst case.

−p −

−α − 1
γ

(1 − p) = 0 =⇒ α∗ = −1 + γ

p
1 − p

,

where we have −1 ≤ α∗ ≤ γ − 1 if p ≤ 1
Bayes optimal classiﬁer for 0 < γ ≤ 1 as follows:

2 . We obtain the

f ∗(x) =

(cid:40)

1 − γ 1−p
p
−1 + γ p
1−p

if p ≥ 1
2 ,
if p < 1
2 .

Note that while f ∗(x) is not a continuous function of p =
p1(x) for γ < 1, it is still a strictly monotonically increasing
function of p for any 0 < γ ≤ 1.

Case γ > 1. First, consider γ − 1 ≤ α ≤ 1,

α − 1
γ

p + (1 − p) = 0 =⇒ α∗ = 1 − γ

1 − p
p

.

(8)

From α∗ ≥ γ − 1, we get the condition p ≥ γ
consider 1 − γ ≤ α ≤ γ − 1,

2 . Next,

The smoothed conjugate is

L∗

γ(β) = L∗(β) +

β2.

γ
2

The corresponding primal smooth hinge loss is given by

Lγ(α) = sup

(cid:8)αβ − β − γ

2 β2(cid:9)

−1≤β≤0



1 − α − γ
2
(α−1)2
2γ



0,

=

if α < 1 − γ,

if 1 − γ ≤ α ≤ 1,
if α > 1.

(9)

Lγ(α) is convex and differentiable with the derivative

L(cid:48)

γ(α) =






−1
α−1
γ
0,

if α < 1 − γ,
if 1 − γ ≤ α ≤ 1,
if α > 1.

We compute the Bayes optimal classiﬁer pointwise.

f ∗(x) = arg min

L(α)p1(x) + L(−α)p−1(x).

α∈R

Let p (cid:44) p1(x), the optimal α∗ is found by solving

L(cid:48)(α)p − L(cid:48)(−α)(1 − p) = 0.

2 The convex conjugate of f is f ∗(x∗) = supx{(cid:104)x∗, x(cid:105) − f (x)}.

α − 1
γ

p −

−α − 1
γ

(1 − p) = 0 =⇒ α∗ = 2p − 1,

which is in the range [1 − γ, γ − 1] if 1 − γ
Finally, consider −1 ≤ α ≤ 1 − γ,

2 ≤ p ≤ γ
2 .

−p −

−α − 1
γ

(1 − p) = 0 =⇒ α∗ = −1 + γ

p
1 − p

,

where we have −1 ≤ α∗ ≤ 1 − γ if p ≤ 1 − γ
the Bayes optimal classiﬁer for γ > 1 is

2 . Overall,






f ∗(x) =

if p ≥ γ
2 ,
2 ≤ p ≤ γ
if 1 − γ
2 ,
if p < 1 − γ
2 .

1 − γ 1−p
p
2p − 1
−1 + γ p
1−p
Note that f ∗ is again a strictly monotonically increasing
function of p = p1(x). Therefore, for any γ > 0, the
one-vs-all scheme with the smooth hinge loss (9) is top-k
calibrated for all 1 ≤ k ≤ m by Lemma 2.

Next, we introduce the multiclass smooth top-k hinge
losses, which extend the top-k hinge losses (6) and (7). We
deﬁne the top-k simplex (α and β) of radius r as

∆α
∆β

k (r) (cid:44) (cid:8)x | (cid:104)1, x(cid:105) ≤ r, 0 ≤ xi ≤ 1
k (r) (cid:44) (cid:8)x | (cid:104)1, x(cid:105) ≤ r, 0 ≤ xi ≤ 1
(cid:44) ∆β

k (1) and ∆β

(cid:44) ∆α

k (cid:104)1, x(cid:105) , ∀i(cid:9),
k r, ∀i(cid:9).
k (1).

k

We also let ∆α
k

(10)

(11)

Proposition 6 ([28]). The convex conjugates of (6) and (7)
are respectively L∗(b) = − (cid:104)c, b(cid:105), if b ∈ ∆α
k , +∞ other-
wise; and L∗(b) = − (cid:104)c, b(cid:105), if b ∈ ∆β

k , +∞ otherwise.

Smoothing applied to the top-k hinge loss (6) yields the
following smooth top-k hinge loss (α). Smoothing of (7)
is done similarly, but the set ∆α
k (r).

k (r) is replaced with ∆β

Proposition 7. Let γ > 0 be the smoothing parameter. The
smooth top-k hinge loss (α) and its conjugate are

Lγ(a) = 1
γ
γ(b) = γ
L∗

2 (cid:104)p, p(cid:105) (cid:1),
(cid:0) (cid:104)a + c, p(cid:105) − 1
2 (cid:104)b, b(cid:105) − (cid:104)c, b(cid:105) , if b ∈ ∆α

k , +∞ o/w,

(12)

(13)

where p = proj∆α
(a + c) on ∆α

k (γ)(a + c) is the Euclidean projection of

k (γ). Moreover, Lγ(a) is 1/γ-smooth.

Proof. We take the convex conjugate of the top-k hinge
loss, which was derived in [28, Proposition 2],

(cid:40)

L∗(b) =

− (cid:104)c, b(cid:105)
+∞

k (1),

if b ∈ ∆α
otherwise,

and add the regularizer γ
2 (cid:104)b, b(cid:105) to obtain the γ-strongly con-
vex conjugate loss L∗
γ(b) as stated in the proposition. As
mentioned above [21] (see also [48, Lemma 2]), the primal
smooth top-k hinge loss Lγ(a), obtained as the convex con-
jugate of L∗
γ(b), is 1/γ-smooth. We now obtain a formula to
compute it based on the Euclidean projection onto the top-k
simplex. By deﬁnition,

Lγ(a) = sup
b∈Rm

{(cid:104)a, b(cid:105) − L∗

γ(b)}

= max
b∈∆α

k (1)

(cid:110) γ

= − min
b∈∆α

k (1)

= − 1

γ min
b∈∆α

k (1)

= − 1

γ min
b
γ ∈∆α

k (1)

(cid:110)

(cid:104)a, b(cid:105) − γ

(cid:111)

2 (cid:104)b, b(cid:105) + (cid:104)c, b(cid:105)
(cid:111)

2 (cid:104)b, b(cid:105) − (cid:104)a + c, b(cid:105)
(cid:110) 1

2 (cid:104)γb, γb(cid:105) − (cid:104)a + c, γb(cid:105)
(cid:111)
(cid:110) 1
.

2 (cid:104)b, b(cid:105) − (cid:104)a + c, b(cid:105)

(cid:111)

For the constraint b

γ ∈ ∆α

k (1), we have

(cid:104)1, b/γ(cid:105) ≤ 1,

(cid:104)1, b(cid:105) ≤ γ,

0 ≤ bi/γ ≤ 1
0 ≤ bi ≤ 1

k (cid:104)1, b/γ(cid:105) ⇐⇒
k (cid:104)1, b(cid:105) ⇐⇒ b ∈ ∆α

k (γ).

The ﬁnal expression follows from the fact that

(cid:8) 1
2 (cid:104)b, b(cid:105) − (cid:104)a + c, b(cid:105) (cid:9)

arg min
b∈∆α
k (γ)

≡ arg min
b∈∆α
k (γ)

(cid:107)(a + c) − b(cid:107)2 ≡ proj∆α

k (γ)(a + c).

There is no analytic expression for (12) and evaluation
requires computing a projection onto the top-k simplex
∆α
k (γ), which can be done in O(m log m) time as shown in
[28]. The non-analytic nature of smooth top-k hinge losses
currently prevents us from proving their top-k calibration.

2.4. Top-k Entropy Loss

As shown in § 4 on synthetic data, top-1 and top-2 er-
ror optimization, when limited to linear classiﬁers, lead to
completely different solutions. The softmax loss, primar-
ily aiming at top-1 performance, produces a solution that
is reasonably good in top-1 error, but is far from what can
be achieved in top-2 error. That reasoning motivated us to
adapt the softmax loss to top-k error optimization. Inspired
by the conjugate of the top-k hinge loss, we introduce in
this section the top-k entropy loss.

Recall that the conjugate functions of multiclass SVM
[15] and the top-k SVM [28] differ only in their effective
domain3 while the conjugate function is the same. Instead
of the standard simplex, the conjugate of the top-k hinge
loss is deﬁned on a subset, the top-k simplex.

This suggests a way to construct novel losses with spe-
ciﬁc properties by taking the conjugate of an existing loss
function, and modifying its essential domain in a way that
enforces the desired properties. The motivation for doing
so comes from the interpretation of the dual variables as
forces with which every training example pushes the deci-
sion surface in the direction given by the ground truth la-
bel. The absolute value of the dual variables determines the
magnitude of these forces and the optimal values are often
attained at the boundary of the feasible set (which coincides
with the essential domain of the loss). Therefore, by reduc-
ing the feasible set we can limit the maximal contribution
of a given training example.

We begin with the conjugate of the softmax loss. Let

a\y be obtained by removing the y-th coordinate from a.

Proposition 8. The convex conjugate of (5) is

L∗(v) =

j(cid:54)=y vj log vj + (1 + vy) log(1 + vy),
if (cid:104)1, v(cid:105) = 0 and v\y ∈ ∆,

(14)

(cid:80)






+∞ otherwise,

where ∆ (cid:44) (cid:8)x | (cid:104)1, x(cid:105) ≤ 1, 0 ≤ xj ≤ 1, ∀j(cid:9).
Proof. We provide a derivation for the convex conjugate
of the softmax loss which was already given in [32, Ap-
pendix D.2.3] without a proof. We also highlight the con-
straint (cid:104)1, v(cid:105) = 0 which can be easily missed when com-
puting the conjugate and is re-stated explicitly in Lemma 3.
Let u (cid:44) f (x) ∈ Rm. The softmax loss on example x is
(cid:17)

(cid:17)

(cid:16) (cid:88)

(cid:16) (cid:88)

L(u) = log

exp(uj − uy)

= log

exp(u(cid:48)
j)

,

j∈Y

j∈Y

3 The effective domain of f is dom f = {x ∈ X | f (x) < +∞}.

where we let u(cid:48) (cid:44) Hyu and Hy (cid:44) I − 1e(cid:62)

y . Let

= (cid:80)

j(cid:54)=y vj log(vj) − log(1 + Z)(1 + vy).

φ(u) (cid:44) log

(cid:16) (cid:80)

j∈Y exp(uj)

(cid:17)

,

Summing vj and using the deﬁnition of Z,

then L(u) = φ(Hyu) and the convex conjugate is computed
similar to [28, Lemma 2] as follows.

L∗(v) = sup{ (cid:104)u, v(cid:105) − L(u) | u ∈ Rm}

= sup{ (cid:104)u, v(cid:105) − φ(Hyu) | u ∈ Rm}
= sup{(cid:104)u(cid:107), v(cid:105) + (cid:104)u⊥, v(cid:105) − φ(Hyu⊥) |
u(cid:107) ∈ Ker Hy, u⊥ ∈ Ker⊥ Hy},

where Ker Hy = {u | Hyu = 0} = {t1 | t ∈ R} and
Ker⊥ Hy = {u | (cid:104)1, u(cid:105) = 0}.
It follows that L∗(v)
can only be ﬁnite if (cid:104)u(cid:107), v(cid:105) = 0, which implies v ∈
Ker⊥ Hy ⇐⇒ (cid:104)1, v(cid:105) = 0. Let H †
y be the Moore-Penrose
pseudoinverse of Hy. For a v ∈ Ker⊥ Hy, we write

L∗(v) = sup{(cid:104)H †

yHyu⊥, v(cid:105) − φ(Hyu⊥) | u⊥}

= sup{(cid:104)z, (H †

y)(cid:62)v(cid:105) − φ(z) | z ∈ Im Hy},

exp(uj)/(cid:0)1 +

exp(uj)(cid:1) = Z/(1 + Z).

(cid:88)

j(cid:54)=y

(cid:88)

j(cid:54)=y

vj =

(cid:88)

j(cid:54)=y

Therefore,

1 + Z = 1/(cid:0)1 − (cid:80)

j(cid:54)=y vj

(cid:1) = 1/(1 + vy),

which ﬁnally yields

L∗(v) = (cid:80)

j(cid:54)=y vj log(vj) + log(1 + vy)(1 + vy),

if (cid:104)1, v(cid:105) = 0 and v\y ∈ ∆ as stated in the proposition.

The conjugate of the top-k entropy loss is obtained by
replacing ∆ in (14) with ∆α
k . A β version could be obtained
using the ∆β
k instead, which defer to future work. There is
no closed-form solution for the primal top-k entropy loss
for k > 1, but we can evaluate it as follows.

where Im Hy = {Hyu | u ∈ Rm} = {u | uy = 0}. Using
rank-1 update of the pseudoinverse [37, § 3.2.7], we have

Proposition 9. Let uj (cid:44) fj(x) − fy(x) for all j ∈ Y. The
top-k entropy loss is deﬁned as

(H †

y)(cid:62) = I − eye(cid:62)

y −

(1 − ey)1(cid:62),

1
m

which together with (cid:104)1, v(cid:105) = 0 implies

(H †

y)(cid:62)v = v − vyey.

Therefore,

L∗(v) = sup{(cid:104)u, v − vyey(cid:105) − φ(u) | uy = 0}

(cid:110)

= sup

(cid:104)u\y, v\y(cid:105) − log

1 +

(cid:16)

exp(uj)

(cid:17)(cid:111)
.

(cid:88)

j(cid:54)=y

The function inside sup is concave and differentiable, hence
the global optimum is at the critical point [10]. Setting the
partial derivatives to zero yields

vj = exp(uj)/(cid:0)1 + (cid:80)

j(cid:54)=y exp(uj)(cid:1)

for j (cid:54)= y, from which we conclude, similar to [48, § 5.1],
that (cid:104)1, v(cid:105) ≤ 1 and 0 ≤ vj ≤ 1 for all j (cid:54)= y, i.e. v\y ∈ ∆.
Let Z (cid:44) (cid:80)
j(cid:54)=y exp(uj), we have at the optimum

uj = log(vj) + log(1 + Z),
Since (cid:104)1, v(cid:105) = 0, we also have that vy = − (cid:80)

∀j (cid:54)= y.

j(cid:54)=y vj, hence

L(u) = max(cid:8)(cid:104)u\y, x(cid:105) − (1 − s) log(1 − s)

− (cid:104)x, log x(cid:105) | x ∈ ∆α

k , (cid:104)1, x(cid:105) = s(cid:9).

(15)

Moreover, we recover the softmax loss (5) if k = 1.

Proof. The convex conjugate of the top-k entropy loss is

L∗(v) (cid:44)

j(cid:54)=y vj log vj + (1 + vy) log(1 + vy),
if (cid:104)1, v(cid:105) = 0 and v\y ∈ ∆α
k ,

+∞ otherwise,

(cid:80)






where the setting is the same as in Proposition 8. The (pri-
mal) top-k entropy loss is deﬁned as the convex conjugate
of the L∗(v) above. We have

L(u) = sup{ (cid:104)u, v(cid:105) − L∗(v) | v ∈ Rm}

= sup{ (cid:104)u, v(cid:105) −

vj log vj − (1 + vy) log(1 + vy)

(cid:88)

j(cid:54)=y
| (cid:104)1, v(cid:105) = 0, v\y ∈ ∆α
k }

= sup{(cid:104)u\y, v\y(cid:105) − uy

(cid:88)

vj −

vj log vj

(cid:88)

j(cid:54)=y

− (1 −

vj) log(1 −

vj) | v\y ∈ ∆α

k }.

(cid:88)

j(cid:54)=y

j(cid:54)=y
(cid:88)

j(cid:54)=y

L∗(v) = (cid:80)
= (cid:80)

j(cid:54)=y ujvj − log(1 + Z)

j(cid:54)=y vj log(vj) + log(1 + Z)

(cid:16) (cid:88)

(cid:17)

vj − 1

j(cid:54)=y

Note that uy = 0, and hence the corresponding term van-
ishes. Finally, we let x (cid:44) v\y and s (cid:44) (cid:80)
j(cid:54)=y vj = (cid:104)1, x(cid:105)
and obtain (15).

Next, we discuss how this problem can be solved and
show that it reduces to the softmax loss for k = 1. Let
a (cid:44) u\y and consider an equivalent problem below.
L(u) = − min (cid:8) (cid:104)x, log x(cid:105) + (1 − s) log(1 − s)
k , (cid:104)1, x(cid:105) = s(cid:9).

− (cid:104)a, x(cid:105) | x ∈ ∆α

(16)

The Lagrangian for (16) is

L = (cid:104)x, log x(cid:105) + (1 − s) log(1 − s) − (cid:104)a, x(cid:105)

+ t((cid:104)1, x(cid:105) − s) + λ(s − 1) − (cid:104)µ, x(cid:105) + (cid:10)ν, x − s

k 1(cid:11) ,
where t ∈ R and λ, µ, ν ≥ 0 are the dual variables. Com-
puting the partial derivatives of L w.r.t. xj and s, and setting
them to zero, we obtain

log xj = aj − 1 − t + µj − νj,
k (cid:104)1, ν(cid:105) + λ.

log(1 − s) = −1 − t − 1

∀j

Note that xj = 0 and s = 1 cannot satisfy the above con-
ditions for any choice of the dual variables in R. Therefore,
xj > 0 and s < 1, which implies µj = 0 and λ = 0. The
only constraint that might be active is xj ≤ s
k . Note, how-
ever, that in view of xj > 0 it can only be active if either
k > 1 or we have a one dimensional problem. We consider
the case when this constraint is active below.
Consider xj’s for which 0 < xj < s

k holds at the opti-
mum. The complementary slackness conditions imply that
the corresponding µj = νj = 0. Let p (cid:44) (cid:104)1, ν(cid:105) and re-
deﬁne t as t ← 1 + t. We obtain the simpliﬁed equations

log xj = aj − t,
log(1 − s) = −t − p
k .

Taking into account the minus in front of the min in (16)
and the deﬁnition of a, we ﬁnally recover the softmax loss

L(y, f (x)) = log (cid:0)1 + (cid:80)

j(cid:54)=y exp(fj(x) − fy(x))(cid:1).

The non-analytic nature of the loss for k > 1 does not
allow us to check if it is top-k calibrated. We now show
how this problem can be solved efﬁciently.

How to solve (15). We continue the derivation started in
the proof of Propostion 9. First, we write the system that
follows directly from the KKT [10] optimality conditions.

xj = min{exp(aj − t), s
k },
νj = max{0, aj − t − log( s

∀j,

k )},

∀j,

1 − s = exp(−t − p
s = (cid:104)1, x(cid:105) ,

k ),
p = (cid:104)1, ν(cid:105) .

(17)

Next, we deﬁne the two index sets U and M as follows

U (cid:44) {j | xj = s

k },

M (cid:44) {j | xj < s

k }.

Note that the set U contains at most k indexes correspond-
ing to the largest components of aj. Now, we proceed with
ﬁnding a t that solves (17). Let ρ (cid:44) |U |
k . We eliminate p as
k )(cid:1) =⇒

aj − |U | (cid:0)t + log( s

νj =

p =

(cid:88)

(cid:88)

p

k = 1

k

aj − ρ(cid:0)t + log( s

k )(cid:1).

U

j
(cid:88)

U

Let Z (cid:44) (cid:80)

M exp aj, we write for s

(cid:88)

s =

xj =

(cid:88)

j

U

(cid:88)

M

exp(aj − t)

= ρs + exp(−t)

exp aj = ρs + exp(−t)Z.

s
k +

(cid:88)

M

If k = 1, then 0 < xj < s for all j in a multiclass problem
as discussed above, hence also p = 0. We have

xj = eaj −t,

1 − s = e−t,

where t ∈ R is to be found. Plugging that into the objective,

We conclude that

j(aj − t)eaj −t − te−t − (cid:80)

(cid:80)
= e−t(cid:104) (cid:80)
= −te−t(cid:2)1 + (cid:80)
= −t(cid:2)1 − s + s(cid:3) = −t.

j ajeaj −t
j ajeaj
j eaj (cid:3) = −t(cid:2)e−t + (cid:80)

j(aj − t)eaj − t − (cid:80)

(cid:105)

j eaj −t(cid:3)

(1 − ρ)s = exp(−t)Z =⇒

t = log Z − log (cid:0)(1 − ρ)s(cid:1).

Let α (cid:44) 1
k

(cid:80)

U aj. We further write

log(1 − s) = − t − p
k

To compute t, we note that

(cid:80)

j eaj −t = (cid:104)1, x(cid:105) = s = 1 − e−t,

from which we conclude

= − t − α + ρ(cid:0)t + log( s
k )(cid:1)
=ρ log( s
k ) − (1 − ρ)t − α
=ρ log( s
k ) − α

− (1 − ρ)(cid:2) log Z − log (cid:0)(1 − ρ)s(cid:1)(cid:3),

1 = (cid:0)1 +

(cid:88)

eaj (cid:1)e−t =⇒ −t = − log(1 + (cid:80)

j eaj ).

j

which yields the following equation for s

log(1 − s) − ρ(log s − log k) + α

+ (1 − ρ)(cid:2) log Z − log(1 − ρ) − log s(cid:3) = 0.

Therefore,

We ﬁnally get

log(1 − s) − log s + ρ log k + α

+ (1 − ρ) log Z − (1 − ρ) log(1 − ρ) = 0,

log

(cid:19)

(cid:18) 1 − s
s

= log

(cid:18) (1 − ρ)(1−ρ) exp(−α)
kρZ (1−ρ)

(cid:19)

.

s = 1/(1 + Q),
Q (cid:44) (1 − ρ)(1−ρ)/(kρZ (1−ρ)eα).

(18)

We note that: a) Q is readily computable once the sets U
and M are ﬁxed; and b) Q = 1/Z if k = 1 since ρ = α = 0
in that case. This yields the formula for t as

t = log Z + log(1 + Q) − log(1 − ρ).

(19)

As a sanity check, we note that we again recover the soft-
max loss for k = 1, since t = log Z + log(1 + 1/Z) =
log(1 + Z) = log(1 + (cid:80)
j exp aj).
To verify that the computed s and t are compatible with

the choice of the sets U and M , we check if this holds:

exp(aj − t) ≥ s
k ,
exp(aj − t) ≤ s
k ,

∀j ∈ U,

∀j ∈ M,

which is equivalent to

max
M

aj ≤ log( s

k ) + t ≤ min
U

aj.

(20)

Computation of the top-k entropy loss (15). The above
derivation suggests a simple and efﬁcient algorithm to com-
pute s and t that solve the KKT system (17) and, therefore,
the original problem (15).

1. Initialization: U ← {}, M ← {1, . . . , m}.

2. Z ← (cid:80)

M exp aj, α ← 1
k

(cid:80)

U aj, ρ ← |U |
k .

3. Compute s and t using (18) and (19).

4. If (20) holds, stop; otherwise, j ← arg maxM aj.

5. U ← U ∪ {j}, M ← M \ {j} and go to step 2.

Note that the algorithm terminates after at most k iterations
since |U | ≤ k. The overall complexity is therefore O(km).
To compute the actual loss (15), we note that if U is
empty, i.e. there were no violated constraints, then the top-k
entropy loss coincides with the softmax loss and is directly
given by t. Otherwise, we have

(cid:104)a, x(cid:105) − (cid:104)x, log x(cid:105) − (1 − s) log(1 − s)

aj

s
k +

(cid:88)

M

aj exp(aj − t) −

(cid:88)

U

s

k log( s
k )

(aj − t) exp(aj − t) − (1 − s) log(1 − s)

(cid:88)

U
(cid:88)

=

−

M

= αs − ρs log( s
= αs − ρs log( s

k ) + t exp(−t)Z − (1 − s) log(1 − s)
k ) + (1 − ρ)st − (1 − s) log(1 − s).

Therefore, the top-k entropy loss is readily computed once
the optimal s and t are found.

2.5. Truncated Top-k Entropy Loss

A major limitation of the softmax loss for top-k error
optimization is that it cannot ignore the highest scoring pre-
dictions, which yields a high loss even if the top-k error is
zero. This can be seen by rewriting (5) as

L(y, f (x)) = log (cid:0)1 +

exp(fj(x) − fy(x))(cid:1).

(21)

(cid:88)

j(cid:54)=y

If there is only a single j such that fj(x) − fy(x) (cid:29) 0, then
L(y, f (x)) (cid:29) 0 even though err2(y, f (x)) = 0.

This problem is is also present in all top-k hinge losses
considered above and is an inherent limitation due to their
convexity. The origin of the problem is the fact that ranking
based losses [54] are based on functions such as

φ(f (x)) = 1
m

(cid:80)

j∈Y αjfπj (x) − fy(x).

The function φ is convex if the sequence (αj) is monotoni-
cally non-increasing [10]. This implies that convex ranking
based losses have to put more weight on the highest scoring
classiﬁers, while we would like to put less weight on them.
To that end, we drop the ﬁrst (k − 1) highest scoring predic-
tions from the sum in (21), sacriﬁcing convexity of the loss,
and deﬁne the truncated top-k entropy loss as follows

L(y, f (x)) = log (cid:0)1 +

exp(fj(x) − fy(x))(cid:1),

(22)

(cid:88)

j∈Jy

where Jy are the indexes corresponding to the (m − k)
smallest components of (fj(x))j(cid:54)=y. This loss can be seen
as a smooth version of the top-k error (1), as it is small
whenever the top-k error is zero. Below, we show that this
loss is top-k calibrated.

Proposition 10. The truncated top-k entropy loss is top-s
calibrated for any k ≤ s ≤ m.

Proof. Given a g ∈ Rm, let π be a permutation such that
gπ1 ≥ gπ2 ≥ . . . ≥ gπm . Then, we have

(cid:40)

Jy =

{πk+1, . . . , πm}
{πk, . . . , πm} \ {y}

if y ∈ {π1, . . . , πk−1},
if y ∈ {πk, . . . , πm}.

Therefore, the expected loss at x can be written as

3. Optimization Method

EY |X [L(Y, g) | X = x] = (cid:80)
r=1 log (cid:0)1 + (cid:80)m
r=k log (cid:0) (cid:80)m

= (cid:80)k−1
+ (cid:80)m

y∈Y L(y, g) py(x)
j=k+1 egπj −gπr (cid:1) pπr (x)

j=k egπj −gπr (cid:1) pπr (x).

Note that the sum inside the logarithm does not depend on
gπr for r < k. Therefore, a Bayes optimal classiﬁer will
have gπr = +∞ for all r < k as then the ﬁrst sum vanishes.

Let p (cid:44) (py(x))y∈Y and q (cid:44) (L(y, g))y∈Y , then

In this section, we brieﬂy discuss how the proposed
smooth top-k hinge losses and the top-k entropy loss can be
optimized efﬁciently within the SDCA framework of [48].
The primal and dual problems. Let X ∈ Rd×n be
the matrix of training examples xi ∈ Rd, K = X (cid:62)X the
corresponding Gram matrix, W ∈ Rd×m the matrix of pri-
mal variables, A ∈ Rm×n the matrix of dual variables, and
λ > 0 the regularization parameter. The primal and Fenchel
dual [8] objective functions are given as

qπ1 = . . . = qπk−1 = 0 ≤ qπk ≤ . . . ≤ qπm

P (W ) = +

L (cid:0)yi, W (cid:62)xi

(cid:1) +

tr (cid:0)W (cid:62)W (cid:1) ,

and we can re-write the expected loss as

EY |X [L(Y, g) | X = x] = (cid:104)p, q(cid:105) = (cid:104)pπ, qπ(cid:105) ≥ (cid:104)pτ , qπ(cid:105) ,

where pτ1 ≥ pτ2 ≥ . . . ≥ pτm and we used the rearrange-
ment inequality. Therefore, the expected loss is minimized
when π and τ coincide (up to a permutation of the ﬁrst k −1
elements, since they correspond to zero loss).

We can also derive a Bayes optimal classiﬁer following

the proof of Proposition 4. We have

EY |X [L(Y, g) | X = x]

= (cid:80)m
= (cid:80)m

r=k log (cid:0) (cid:80)m
(cid:16)
log (cid:0) (cid:80)m

j=k egτj −gτr (cid:1) pτr (x)
(cid:17)
j=k egτj (cid:1) − gτr

r=k

pτr (x).

A critical point is found by setting partial derivatives to zero
for all y ∈ {τk, . . . , τm}, which leads to

egy
j=k egτj

(cid:80)m

(cid:80)m

r=k pτr (x) = py(x).

We let gy = −∞ if py(x) = 0, and obtain ﬁnally

g∗
τj

=






+∞
log (cid:0)αpτj (x)(cid:1)
−∞

if j < k,
if j ≥ k and pτj (x) > 0,
if j ≥ k and pτj (x) = 0,

as a Bayes optimal classiﬁer for any α > 0.

Note that g∗ preserves the ranking of py(x) for all y in

{τk, . . . , τm}, hence, it is top-s calibrated for all s ≥ k.

As the loss (22) is nonconvex, we use solutions obtained
with the softmax loss (5) as initial points and optimize them
further via gradient descent. However, the resulting opti-
mization problem seems to be “mildly nonconvex” as the
same-quality solutions are obtained from different initial-
In Section 4, we show a synthetic experiment,
izations.
where the advantage of discarding the highest scoring clas-
siﬁer in the loss becomes apparent.

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

λ
2

λ
2

D(A) = −

L∗ (yi, −λnai) −

tr (cid:0)AKA(cid:62)(cid:1) ,

(23)

where L∗ is the convex conjugate of L. SDCA proceeds by
randomly picking a variable ai (which in our case is a vec-
tor of dual variables over all m classes for a sample xi) and
modifying it to achieve maximal increase in the dual objec-
tive D(A). It turns out that this update step is equivalent
to a proximal problem, which can be seen as a regularized
projection onto the essential domain of L∗.

The convex conjugate. An important ingredient in the
SDCA framework is the convex conjugate L∗. We show
that for all multiclass loss functions that we consider the fact
that they depend on the differences fj(x) − fy(x) enforces
a certain constraint on the conjugate function.

Lemma 3. Let Hy = I − 1e(cid:62)
Φ∗(v) = +∞ unless (cid:104)1, v(cid:105) = 0.

y and let Φ(u) = φ(Hyu).

Proof. The proof follows directly from [28, Lemma 2] and
was already reproduced in the proof of Proposition 8 for the
softmax loss. We have formulated this simpliﬁed lemma
since [28, Lemma 2] additionally required y-compatibility
to show that if (cid:104)1, v(cid:105) = 0, then Φ∗(v) = φ∗(v − vyey),
which does not hold e.g. for the softmax loss.

j(cid:54)=yi

Lemma 3 tells us that we need to enforce (cid:104)1, ai(cid:105) = 0 at
all times, which translates into ayi = − (cid:80)
aj. The up-
date steps are performed on the (m − 1)-dimensional vector
obtained by removing the coordinate ayi.
The update step for top-k SVMα

γ . Let a\y be obtained
by removing the y-th coordinate from vector a. We show
that performing an update step for the smooth top-k hinge
loss is equivalent to projecting a certain vector b, computed
from the prediction scores W (cid:62)xi, onto the essential do-
main of L∗, the top-k simplex, with an added regularization
ρ (cid:104)1, x(cid:105)2, which biases the solution to be orthogonal to 1.

Proposition 11. Let L and L∗ in (23) be respectively the
top-k SVMα
γ loss and its conjugate as in Proposition 7.

The update maxai{D(A) | (cid:104)1, ai(cid:105) = 0} is equivalent with
the change of variables x ↔ −a\yi

to solving

i

min
x

{(cid:107)x − b(cid:107)2 + ρ (cid:104)1, x(cid:105)2 | x ∈ ∆α

k ( 1

λn )},

(24)

(cid:0)q\yi + (1 − qyi)1(cid:1),
where b =
q = W (cid:62)xi − (cid:104)xi, xi(cid:105) ai, and ρ = (cid:104)xi,xi(cid:105)

1
(cid:104)xi,xi(cid:105)+γnλ

(cid:104)xi,xi(cid:105)+γnλ .

Proof. We follow the proof of [28, Proposition 4]. We
choose i ∈ {1, . . . , n} and, having all other variables ﬁxed,
update ai to maximize

− 1

n L∗ (yi, −λnai) − λ

2 tr (cid:0)AKA(cid:62)(cid:1) .

For the nonsmooth top-k hinge loss, it was shown [28] that

L∗ (yi, −λnai) = (cid:104)c, λn(ai − ayi,ieyi)(cid:105)

if −λn(ai − ayi,ieyi) ∈ ∆α
the smoothed loss, we add regularization and obtain

k and +∞ otherwise. Now, for

− 1
n

(cid:16) γ
(cid:17)
2 (cid:107)−λn(ai − ayi,ieyi)(cid:107)2 + (cid:104)c, λn(ai − ayi,ieyi)(cid:105)

with −λn(ai − ayi,ieyi) ∈ ∆α
(cid:104)1, ai(cid:105) = 0, one can simplify it to

k . Using c = 1 − eyi and

−

γnλ2
2

(cid:13)
(cid:13)a\yi

i

(cid:13)
2
(cid:13)

+ λayi,i,

and the feasibility constraint can be re-written as

−a\yi

i ∈ ∆α

λn ),
For the regularization term tr (cid:0)AKA(cid:62)(cid:1), we have

ayi,i = (cid:104)1, −a\yi

k ( 1

i

(cid:105).

We let q = (cid:80)

j(cid:54)=i Kijaj = AKi − Kiiai and x = −a\yi

i

:

(cid:104)ai, ai(cid:105) = (cid:104)1, x(cid:105)2 + (cid:104)x, x(cid:105) ,
(cid:104)q, ai(cid:105) = qyi (cid:104)1, x(cid:105) − (cid:104)q\yi, x(cid:105).

Now, we plug everything together and multiply with −2/λ.

min
k ( 1

λn )

x∈∆α

γnλ (cid:107)x(cid:107)2 − 2 (cid:104)1, x(cid:105) + 2(cid:0)qyi (cid:104)1, x(cid:105) − (cid:104)q\yi, x(cid:105)(cid:1)

+ Kii

(cid:0) (cid:104)1, x(cid:105)2 + (cid:104)x, x(cid:105) (cid:1).

Collecting the corresponding terms ﬁnishes the proof.

Note that setting γ = 0, we recover the update step for
the non-smooth top-k hinge loss [28]. It turns out that we
can employ their projection procedure for solving (24) with
only a minor modiﬁcation of b and ρ.

k in (24) instead of ∆α

The update step for the top-k SVMβ

γ loss is derived sim-
ilarly using the set ∆β
k . The resulting
projection problem is a biased continuous quadratic knap-
sack problem, which is discussed in the supplement of [28].
Smooth top-k hinge losses converge signiﬁcantly faster
than their nonsmooth variants as we show in the scaling ex-
periments below. This can be explained by the theoretical
results of [48] on the convergence rate of SDCA. They also
had similar observations for the smoothed binary hinge loss.
The update step for top-k Ent. We now discuss the op-
timization of the proposed top-k entropy loss in the SDCA
framework. Note that the top-k entropy loss reduces to the
softmax loss for k = 1. Thus, our SDCA approach can
be used for gradient-free optimization of the softmax loss
without having to tune step sizes or learning rates.

Proposition 12. Let L in (23) be the top-k Ent loss (15)
and L∗ be its convex conjugate as in (14) with ∆ replaced
by ∆α
k . The update maxai{D(A) | (cid:104)1, ai(cid:105) = 0} is equiva-
lent with the change of variables x ↔ −λna\yi
to solving

i

min
x∈∆α
k

α

2 ((cid:104)x, x(cid:105) + (cid:104)1, x(cid:105)2) − (cid:104)b, x(cid:105) +

(cid:104)x, log x(cid:105) + (1 − (cid:104)1, x(cid:105)) log(1 − (cid:104)1, x(cid:105))

(25)

where α = (cid:104)xi,xi(cid:105)

λn , b = q\yi −qyi1, q = W (cid:62)xi−(cid:104)xi, xi(cid:105) ai.

Proof. Let v (cid:44) −λnai and y = yi. Using Proposition 8,

L∗(y, v) = (cid:80)

j(cid:54)=y vj log vj + (1 + vy) log(1 + vy),

where (cid:104)1, v(cid:105) = 0 and v\y ∈ ∆α
It follows that s = (cid:104)1, x(cid:105) and from tr (cid:0)AKA(cid:62)(cid:1) we get

k . Let x (cid:44) v\y and s (cid:44) −vy.

where q = (cid:80)
we plug everything together as in Proposition 11.

j(cid:54)=i Kijaj = AKi − Kiiai as before. Finally,

Note that this optimization problem is similar to (24), but
is more difﬁcult to solve due to the presence of logarithms
in the objective. We propose to tackle this problem using
the Lambert W function introduced below.

Lambert W function. The Lambert W function is de-
ﬁned to be the inverse of the function w (cid:55)→ wew and is
widely used in many ﬁelds [14, 18, 55]. Taking logarithms
on both sides of the deﬁning equation z = W eW , we obtain
log z = W (z) + log W (z). Therefore, if we are given an
equation of the form x + log x = t for some t ∈ R, we
can directly “solve” it in closed-form as x = W (et). The
crux of the problem is that the function V (t) (cid:44) W (et) is
transcendental [18] just like the logarithm and the exponent.
There exist highly optimized implementations for the latter
and we argue that the same can be done for the Lambert W

tr (cid:0)AKA(cid:62)(cid:1) = Kii (cid:104)ai, ai(cid:105) + 2

Kij (cid:104)ai, aj(cid:105) + const.

Kii((cid:104)x, x(cid:105) + s2)/(λn)2 − 2(cid:10)q\y − qy1, x(cid:11)/(λn),

(cid:88)

j(cid:54)=i

and λ = 0. We re-write the above as

αxi + log(αxi) = ai − 1 − t + log α − νi,
α(1 − s) + log (cid:0)α(1 − s)(cid:1) = α − 1 − t + log α − (cid:104)1,ν(cid:105)
k .

Note that these equations correspond to the Lambert W
function of the exponent, i.e. V (t) = W (et) discussed
above. Let p (cid:44) (cid:104)1, ν(cid:105) and re-deﬁne t ← 1 + t − log α.

αxi = W (cid:0) exp(ai − t − νi)(cid:1),
k )(cid:1).

α(1 − s) = W (cid:0) exp(α − t − p

Finally, we obtain the following system:

xi = min{ 1

α V (ai − t), s

∀i,

k },
∀i,

αxi = V (ai − t − νi),
k ),

α(1 − s) = V (α − t − p
s = (cid:104)1, x(cid:105) ,

p = (cid:104)1, ν(cid:105) .

Note that V (t) is a strictly monotonically increasing func-
tion, therefore, it is invertible and we can write

ai − t − νi = V −1(αxi),
α − t − p

k = V −1(cid:0)α(1 − s)(cid:1).

Next, we deﬁned the sets U and M as before and write

s = (cid:104)1, x(cid:105) = (cid:80)
p = (cid:104)1, ν(cid:105) = (cid:80)

s

U

M

k + (cid:80)

1
α V (ai − t),
U ai − |U | (cid:0)t + V −1( αs
(cid:80)

U ai, we get

k )(cid:1).

Let ρ (cid:44) |U |

k as before and A (cid:44) 1

k

(cid:80)

(1 − ρ)s = 1
α
p

M V (ai − t),
k = A − ρ(cid:0)t + V −1( αs
Finally, we eliminate p and obtain a system in two variables,

k )(cid:1).

α(1 − ρ)s − (cid:80)
(1 − ρ)t + V −1(cid:0)α(1 − s)(cid:1) − ρV −1( αs

M V (ai − t) = 0,

k ) + A − α = 0,

which can be solved using the Newton’s method [36].
Moreover, when U is empty, the system above simpliﬁes
into a single equation in one variable t

V (α − t) + (cid:80)

M V (ai − t) = α,

(a) V (t) ≈ et for t (cid:28) 0.

(b) V (t) ≈ t − log t for t (cid:29) 0.

Figure 1: Behavior of the Lambert W function of the expo-
nent (V (t) = W (et)). (a) Log scale plot with t ∈ (−10, 0).
(b) Linear scale plot with t ∈ (0, 10).

function. In fact, there is already some work on this topic
[18, 55], which we also employ in our implementation.

To develop intuition concerning the Lambert W function
of the exponent, we now brieﬂy discuss how the function
V (t) = W (et) behaves for different values of t. An illus-
tration is provided in Figure 1. One can see directly from
the equation x + log x = t that the behavior of x = V (t)
changes dramatically depending on whether t is a large pos-
itive or a large negative number. In the ﬁrst case, the linear
part dominates the logarithm and the function is approxi-
mately linear; a better approximation is x(t) ≈ t − log t,
when t (cid:29) 1. In the second case, the function behaves like
an exponent et. To see this, we write x = ete−x and note
that e−x ≈ 1 when t (cid:28) 0, therefore, x(t) ≈ et, if t (cid:28) 0.
We use these approximations as initial points for a 5-th or-
der Householder method [22], which was also used in [18].
A single iteration is already sufﬁcient to get full float pre-
cision and at most two iterations are needed for double.

How to solve (25). We present a similar derivation as
was already done for the problem (15) above. The main dif-
ference is that we now encounter the Lambert W function
in the optimality conditions. We re-write the problem as

min (cid:8) α

2 ((cid:104)x, x(cid:105) + s2) − (cid:104)a, x(cid:105) + (cid:104)x, log x(cid:105)
+ (1 − s) log(1 − s) | s = (cid:104)1, x(cid:105) , x ∈ ∆α
k

(cid:9).

The Lagrangian is given by

L = α

2 ((cid:104)x, x(cid:105) + s2) − (cid:104)a, x(cid:105) + (cid:104)x, log x(cid:105)
+ (1 − s) log(1 − s) + t((cid:104)1, x(cid:105) − s)
+ λ(s − 1) − (cid:104)µ, x(cid:105) + (cid:10)ν, x − s
k 1(cid:11) ,

where t ∈ R, λ, µ, ν ≥ 0 are the dual variables. Computing
partial derivatives of L w.r.t. xi and s, and setting them to
zero, we obtain

αxi + log xi = ai − 1 − t + µi − νi,

α(1 − s) + log(1 − s) = α − 1 − t − λ − 1

∀i,
k (cid:104)1, ν(cid:105) ,

∀i.

Note that xi > 0 and s < 1 as before, which implies µi = 0

which can be solved efﬁciently using the Householder’s
method [22]. As both methods require derivatives of V (t),
we note that ∂tV (t) = V (t)/(1 + V (t)) [14]. Therefore,
V (ai − t) is only computed once for each ai − t and then
re-used to also compute the derivatives.

The efﬁciency of this approach crucially depends on fast
computation of V (t). Our implementation was able to scale
the training procedure to large datasets as we show next.

(a) Relative duality gap vs. time

(b) Top-1 accuracy vs. time

Figure 2: SDCA convergence with LRMulti, SVMMulti,
and top-1 SVMα

1 objectives on ILSVRC 2012.

Figure 3: Convergence rate
of SDCA (ours) and the
SPAMS toolbox [33].

Runtime. We compare
the wall-clock runtime of
the top-1 multiclass SVM
[28] (SVMMulti) with our
smooth multiclass SVM
(smooth SVMMulti)
and
the softmax loss (LRMulti)
objectives in Figure 2. We
plot the relative duality gap
(P (W ) − D(A))/P (W )
and the validation accuracy
versus time for the best per-
forming models on ILSVRC 2012. We obtain substantial
improvement of the convergence rate for smooth top-1
SVM compared to the non-smooth baseline. Moreover,
top-1 accuracy saturates after a few passes over the training
data, which justiﬁes the use of a fairly loose stopping
criterion (we used 10−3). For LRMulti, the cost of each
epoch is signiﬁcantly higher compared to the top-1 SVMs,
which is due to the difﬁculty of solving (25). This suggests
that one can use the smooth top-1 SVMα
1 and obtain
competitive performance (see § 5) at a lower training cost.
We also compare our implementation LRMulti (SDCA)
with the SPAMS optimization toolbox [33], denoted
LRMulti (SPAMS), which provides an efﬁcient implemen-
tation of FISTA [4]. We note that the rate of convergence
of SDCA is competitive with FISTA for (cid:15) ≥ 10−4 and is
noticeably better for (cid:15) < 10−4. We conclude that our ap-
proach is competitive with the state-of-the-art, and faster
computation of V (t) would lead to a further speedup.

Gradient-based optimization. Finally, we note that the
proposed smooth top-k hinge and the truncated top-k en-
tropy losses are easily amenable to gradient-based optimiza-
tion, in particular, for training deep architectures (see § 5).
The computation of the gradient of (22) is straightforward,
while for the smooth top-k hinge loss (12) we have

∇Lγ(a) = 1

γ proj∆α

k (γ)(a + c),

which follows from the fact that Lγ(a) can be written as
2γ ((cid:107)x(cid:107)2 − (cid:107)x − p(cid:107)2) for x = a + c and p = proj∆α
1
k (γ)(x),

(a) top-1 SVM1 test accuracy
(top-1 / top-2): 65.7% / 81.3%

(b) top-2 Enttr test accuracy
(top-1 / top-2): 29.4%, 96.1%

Figure 4: Synthetic data on the unit circle in R2 (inside
black circle) and visualization of top-1 and top-2 predic-
tions (outside black circle). (a) Smooth top-1 SVM1 opti-
mizes top-1 error which impedes its top-2 error. (b) Trunc.
top-2 entropy loss ignores top-1 scores and optimizes di-
rectly top-2 errors leading to a much better top-2 result.

and a known result from convex analysis [8, § 3, Ex. 12.d]
which postulates that ∇x

2 (cid:107)x − PC(x)(cid:107)2 = x − PC(x).

1

4. Synthetic Example

In this section, we demonstrate in a synthetic experiment
that our proposed top-2 losses outperform the top-1 losses
when one aims at optimal top-2 performance. The dataset
with three classes is shown in the inner circle of Figure 4.

Sampling. First, we generate samples in [0, 7] which is
subdivided into 5 segments. All segments have unit length,
except for the 4-th segment which has length 3. We sample
uniformly at random in each of the 5 segments according to
the following class-conditional probabilities: (0, 1, .4, .3, 0)
for class 1, (1, 0, .1, .7, 0) for class 2, and (0, 0, .5, 0, 1) for
class 3. Finally, the data is rescaled to [0, 1] and mapped
onto the unit circle.

Circle (synthetic)

Method

Top-1 Top-2 Method

Top-1

Top-2

SVMOVA
54.3 85.8
LROVA
54.7 81.7
SVMMulti 58.9 89.3
LRMulti
54.7 81.7

65.7

top-1 SVM1
top-2 SVM0/1 54.4 / 54.5 87.1 / 87.0
top-2 Ent
top-2 Enttr

87.6
96.1

54.6
58.4

83.9

Table 2: Top-k accuracy (%) on synthetic data. Left: Base-
lines methods. Right: Top-k SVM (nonsmooth / smooth)
and top-k softmax losses (convex and nonconvex).

Samples of different classes are plotted next to each other
for better visibility as there is signiﬁcant class overlap. We
visualize top-1/2 predictions with two colored circles (out-
side the black circle). We sample 200/200/200K points for
training/validation/testing and tune the C = 1
λn parameter
in the range 2−18 to 218. Results are in Table 2.

State-of-the-art

ALOI
93 ± 1.2 [44]

Letter
97.98 [23] (RBF kernel)

News 20
86.9 [42]

Caltech 101 Silhouettes
62.1
[51]

79.6

83.4

Method

Top-1 Top-3 Top-5 Top-10 Top-1 Top-3 Top-5 Top-10 Top-1 Top-3 Top-5 Top-10 Top-1 Top-3 Top-5 Top-10

SVMOVA
LROVA

SVMMulti
LRMulti

82.4
86.1

90.0
89.8

89.2
top-3 SVM
top-5 SVM
87.3
top-10 SVM 85.0

89.5
93.0

95.1
95.7

95.5
95.6
95.5

91.5
94.8

96.7
97.1

97.2
97.4
97.3

top-1 SVM1
top-3 SVM1
top-5 SVM1
top-10 SVM1

96.7
90.6 95.5
95.7
89.6
97.3
95.7 97.5
87.6
97.4
95.6
85.2

top-3 Ent
top-5 Ent
top-10 Ent

89.0
87.9
86.0

95.8
95.8
95.6

97.2
97.2
97.3

top-3 Enttr
top-5 Enttr
top-10 Enttr

89.3 95.9 97.3
97.3
95.7
87.9
97.1
94.8
85.2

93.7
96.6

98.1
98.4

98.4
98.6
98.7

98.2
98.4
98.6
98.7

98.4
98.4
98.5

98.5
98.6
98.5

63.0
68.1

76.5
75.3

82.0
86.1

89.2
90.3

88.1
90.6

93.1
94.3

74.0
94.4
91.0
70.8 91.5 95.1
96.0
88.9
61.6

93.6
76.8 89.9
74.1
94.5
90.9
70.8 91.5 95.2
95.9
89.1
61.7

73.0
69.7
65.0

63.6
50.3
46.5

90.8
94.9
95.1
90.9
89.7 96.2

91.1
87.7
80.9

95.6
96.1
93.7

94.6
96.2

97.7
98.0

97.8
98.4
99.6

97.6
97.9
98.6
99.7

98.5
98.8
99.6

98.8
99.4
99.6

84.3
84.9

85.4
84.5

85.1
84.3
82.7

95.4
96.3

94.9
96.4

96.6
96.7
96.5

85.6 96.3
96.6
85.1
96.7
84.5
96.5
82.9

97.9
97.8

97.2
98.1

98.2
98.4
98.4

98.0
98.4
98.4
98.4

84.7
98.3
96.6
84.3 96.8 98.6
98.5
96.4
82.7

83.4
83.2
82.9

96.4
96.0
95.7

98.3
98.2
97.9

99.5
99.3

99.1
99.5

99.3
99.3
99.3

99.3
99.4
99.4
99.5

99.4
99.4
99.4

99.4
99.4
99.4

61.8
63.2

76.5
80.4

80.8
84.4

62.8
82.0
77.8
63.2 81.2 85.1

63.4
63.3
63.0

79.7
80.0
80.5

63.9 80.3
80.1
63.3
80.5
63.3
80.5
63.1

83.6
84.3
84.6

84.0
84.0
84.5
84.8

81.1
85.0
85.2
80.9
80.8 85.4

63.3
63.2
62.5

60.7
58.3
51.9

86.6
89.4

86.9
89.7

88.3
88.7
89.1

89.0
89.2
89.1
89.1

89.9
89.9
90.1

Indoor 67
82.0 [58]

CUB
62.8 [13] / 76.37 [61]

Flowers
86.8 [40]

81.1
79.8
78.4

85.2
85.2
84.6

90.2
90.2
90.2

FMD
77.4 [13] / 82.4 [13]

Method

Top-1 Top-3 Top-5 Top-10 Top-1 Top-3 Top-5 Top-10 Top-1 Top-3 Top-5 Top-10 Top-1 Top-3 Top-5

top-3 SVM
81.6
79.9
top-5 SVM
top-10 SVM 78.4

State-of-the-art

SVMOVA
LROVA

SVMMulti
LRMulti

top-1 SVM1
top-3 SVM1
top-5 SVM1
top-10 SVM1

top-3 Ent
top-5 Ent
top-10 Ent

top-3 Enttr
top-5 Enttr
top-10 Enttr

81.9
82.0

82.5
82.4

82.6
81.6
80.4
78.3

81.4
80.3
79.2

79.8
76.4
72.6

94.3
94.9

95.4
95.2

95.1
95.0
95.1

95.2
95.1
95.1
95.1

95.4
95.0
95.1

95.0
94.3
92.8

96.5
97.2

97.3
98.0

97.7
97.7
97.4

97.6
97.8
97.8
97.5

97.6
97.7
97.6

97.5
97.3
97.1

98.0
98.7

99.1
99.1

99.0
99.0
99.0

99.0
99.0
99.1
99.0

99.2
99.0
99.0

99.1
99.0
98.9

60.6
62.3

61.0
62.3

61.3
60.9
59.6

61.9
61.9
61.3
59.8

62.5
62.0
61.2

62.0
61.4
59.7

77.1
80.5

79.2
81.7

80.4
81.2
81.3

80.2
81.1
81.3
81.4

81.8
81.9
81.6

81.4
81.2
80.7

83.4
87.4

85.7
87.9

86.3
87.2
87.7

86.9
86.6
87.4
87.8

87.9
88.1
88.2

87.6
87.7
87.2

89.9
93.5

92.3
93.9

92.5
92.9
93.4

93.1
93.2
92.9
93.4

93.9
93.8
93.8

93.4
93.7
93.4

82.0
82.6

82.5
82.9

81.9
81.7
80.5

83.0
82.5
82.0
80.6

82.5
82.1
80.9

82.1
81.4
77.9

94.3
94.8

94.8
95.1

95.0
95.1
95.1

95.1
95.2
95.1
95.1

95.3
95.1
95.0

95.2
95.0
94.3

96.8
97.6

96.4
97.8

96.1
97.8
97.7

97.6
97.7
97.8
97.7

97.8
97.9
97.7

97.6
97.7
97.3

77.4
79.6

77.6
79.0

78.8
78.4

78.6
79.0
79.4

92.4
94.2

93.8
94.6

94.6
94.4

93.8
94.4
94.4

96.4
98.2

97.2
97.8

97.8
97.6

98.0
98.0
97.6

79.8
79.4

94.8
94.4

98.0
98.0

78.4
77.2

95.4 98.2
97.8
94.0

SUN 397 (10 splits)
66.9 [58]

Places 205 (val)

ILSVRC 2012 (val)

60.6

88.5

[58]

76.3

93.2

[50]

Method

Top-1

Top-3

Top-5

Top-10

Top-1

Top-3

Top-5

Top-10

Top-1

Top-3

Top-5

Top-10

65.8 ± 0.1
67.5 ± 0.1

85.1 ± 0.2
87.7 ± 0.2

90.8 ± 0.1
92.9 ± 0.1

95.3 ± 0.1
96.8 ± 0.1

66.5 ± 0.2
66.3 ± 0.2
64.8 ± 0.3

67.4 ± 0.2
67.0 ± 0.2
66.5 ± 0.2
64.9 ± 0.3

67.2 ± 0.2
66.6 ± 0.3
65.2 ± 0.3

86.5 ± 0.1
87.0 ± 0.2
87.2 ± 0.2

86.8 ± 0.1
87.0 ± 0.1
87.2 ± 0.1
87.3 ± 0.2

91.8 ± 0.1
92.2 ± 0.2
92.6 ± 0.1

92.0 ± 0.1
92.2 ± 0.1
92.4 ± 0.2
92.6 ± 0.2

95.9 ± 0.1
96.3 ± 0.1
96.6 ± 0.1

96.1 ± 0.1
96.2 ± 0.0
96.3 ± 0.0
96.6 ± 0.1

87.7 ± 0.2
87.7 ± 0.2
87.4 ± 0.1

92.9 ± 0.1
92.9 ± 0.1
92.8 ± 0.1

96.8 ± 0.1
96.8 ± 0.1
96.8 ± 0.1

58.4
59.0

58.6
58.4
58.0

59.2
58.9
58.5
58.0

58.7
58.1
57.0

78.7
80.6

80.3
80.5
80.4

80.5
80.5
80.5
80.4

80.6
80.4
80.0

89.9
94.3

93.3
94.0
94.3

93.8
93.9
94.1
94.3

94.2
94.2
94.1

68.3
67.2

68.2
67.8
67.0

68.7
68.2
67.9
67.1

66.8
66.5
65.8

82.9
83.2

84.0
84.1
83.8

83.9
84.1
84.1
83.8

83.1
83.0
82.8

87.0
87.7

88.1
88.2
88.3

88.0
88.2
88.4
88.3

87.8
87.7
87.6

91.1
92.2

92.1
92.4
92.6

92.1
92.3
92.5
92.6

92.2
92.2
92.1

State-of-the-art

SVMMulti
LRMulti

top-3 SVM
top-5 SVM
top-10 SVM

top-1 SVM1
top-3 SVM1
top-5 SVM1
top-10 SVM1

top-3 Ent
top-5 Ent
top-10 Ent

91.7
92.2

92.2
92.4

92.2
92.4
91.9

92.4
92.3
92.5
91.9

92.0
92.2
92.1

92.2
92.0
91.1

84.7
87.6

87.3
87.4
87.4

87.3
87.6
87.5
87.5

87.6
87.4
87.2

Table 3: Top-k accuracy (%) on various datasets. The ﬁrst line is a reference to the state-of-the-art on each dataset and reports
top-1 accuracy except when the numbers are aligned with Top-k. We compare the one-vs-all and multiclass baselines with
the top-k SVMα [28] as well as the proposed smooth top-k SVMα

γ , top-k Ent, and the nonconvex top-k Enttr.

In each column we provide the results for the model
that optimizes the corresponding top-k accuracy, which is
in general different for top-1 and top-2. First, we note that
all top-1 baselines perform similar in top-1 performance,
except for SVMMulti and top-1 SVM1 which show better
results. Next, we see that our top-2 losses improve the top-
2 accuracy and the improvement is most signiﬁcant for the
nonconvex top-2 Enttr loss, which is close to the optimal
solution for this dataset. This is because top-2 Enttr is a
tight bound on the top-2 error and ignores top-1 errors in
the loss. Unfortunately, similar signiﬁcant improvements
were not observed on the real-world data sets that we tried.

5. Experimental Results

The goal of this section is to provide an extensive empir-
ical evaluation of the top-k performance of different losses
in multiclass classiﬁcation. To this end, we evaluate the
loss functions introduced in § 2 on 11 datasets (500 to 2.4M
training examples, 10 to 1000 classes), from various prob-
lem domains (vision and non-vision; ﬁne-grained, scene
and general object classiﬁcation). The detailed statistics of
the datasets is given in Table 4.

Dataset

m

n

d

Dataset

m

n

d

1K 54K 128

ALOI [44]
Caltech 101 Sil [51] 101 4100 784 Letter [23]
CUB [57]
Flowers [35]
FMD [49]
ILSVRC 2012 [47]

67
4K
5354
26 10.5K 16
202 5994 4K News 20 [27]
20 15.9K 16K
102 2040 4K Places 205 [62] 205 2.4M 4K
10
397 19.9K 4K
500
1K 1.3M 4K

4K SUN 397 [60]

Indoor 67 [38]

Table 4: Statistics of the datasets used in the experiments
(m – # classes, n – # training examples, d – # features).

Please refer to Table 1 for an overview of the methods
and our naming convention. A broad selection of results
is also reported at the end of the paper. As other ranking
based losses did not perform well in [28], we do no further
comparison here.

Solvers. We use LibLinear [17] for the one-vs-all
baselines SVMOVA and LROVA; and our code from [28] for
top-k SVM. We extended the latter to support the smooth
top-k SVMγ and top-k Ent. The multiclass loss base-
lines SVMMulti and LRMulti correspond respectively to
top-1 SVM and top-1 Ent. For the nonconvex top-k Enttr,
we use the LRMulti solution as an initial point and perform
gradient descent with line search. We cross-validate hyper-
parameters in the range 10−5 to 103, extending it when the
optimal value is at the boundary.

Features. For ALOI, Letter, and News20 datasets, we
use the features provided by the LibSVM [12] datasets. For
ALOI, we randomly split the data into equally sized train-
ing and test sets preserving class distributions. The Letter
dataset comes with a separate validation set, which we used

for model selection only. For News20, we use PCA to re-
duce dimensionality of sparse features from 62060 to 15478
preserving all non-singular PCA components4.

For Caltech101 Silhouettes, we use the features and the

train/val/test splits provided by [51].

For CUB, Flowers, FMD, and ILSVRC 2012, we use
MatConvNet [56] to extract the outputs of the last fully
connected layer of the imagenet-vgg-verydeep-16
model which is pre-trained on ImageNet [16] and achieves
state-of-the-art results in image classiﬁcation [50].

For Indoor 67, SUN 397, and Places 205, we use the
Places205-VGGNet-16 model by [58] which is pre-
trained on Places 205 [62] and outperforms the ImageNet
pre-trained model on scene classiﬁcation tasks [58]. Fur-
ther results can be found at the end of the paper. In all cases
we obtain a similar behavior in terms of the ranking of the
considered losses as discussed below.

Discussion. The experimental results are given in Ta-
ble 3. There are several interesting observations that one
can make. While the OVA schemes perform quite similar to
the multiclass approaches (logistic OVA vs. softmax, hinge
OVA vs. multiclass SVM), which conﬁrms earlier obser-
vations in [2, 43], the OVA schemes performed worse on
ALOI and Letter. Therefore it seems safe to recommend to
use multiclass losses instead of the OVA schemes.

Comparing the softmax vs. multiclass SVM losses, we
see that there is no clear winner in top-1 performance, but
softmax consistently outperforms multiclass SVM in top-k
performance for k > 1. This might be due to the strong
property of softmax being top-k calibrated for all k. Please
note that this trend is uniform across all datasets, in par-
ticular, also for the ones where the features are not com-
ing from a convnet. Both the smooth top-k hinge and the
top-k entropy losses perform slightly better than softmax if
one compares the corresponding top-k errors. However, the
good performance of the truncated top-k loss on synthetic
data does not transfer to the real world datasets. This might
be due to a relatively high dimension of the feature spaces,
but requires further investigation.

Fine-tuning experiments. We also performed a number
of ﬁne-tuning experiments where the original network was
trained further for 1-3 epochs with the smooth top-k hinge
and the truncated top-k entropy losses5. The motivation was
to see if the full end-to-end training would be more ben-
eﬁcial compared to training just the classiﬁer. Results are
reported in Table 5. First, we note that the setting is now
there is no feature extraction step with
slightly different:
the MatConvNet and there is a non-regularized bias term
in Caffe [24]. Next, we see that the top-k speciﬁc losses
are able to improve the performance compared to the refer-
ence model, and that the top-5 SVM1 loss achieves the best

4 The top-k SVM solvers that we used were designed for dense inputs.
5 Code: https://github.com/mlapin/caffe/tree/topk

Method

LRMulti

top-3 SVM1 (FT)
top-5 SVM1 (FT)

top-3 Enttr (FT)
top-5 Enttr (FT)

LRMulti (FT)

Places 205 (val)
Top-3

Top-1

Top-5

Top-10

59.97

81.39

88.17

94.59

60.73
60.88

60.51
60.48

82.09
82.18

81.86
81.66

88.58
88.78

88.69
88.66

94.56
94.75

94.78
94.80

60.73

82.07

88.71

94.82

ILSVRC 2012 (val)

Method

LRMulti

Top-1

Top-3

Top-5

Top-10

68.60

84.29

88.66

92.83

top-3 SVM1 (FT)
top-5 SVM1 (FT)

top-3 Enttr (FT)
top-5 Enttr (FT)

71.66
71.60

71.41
71.20

86.63
86.67

86.80
86.57

90.55
90.56

90.77
90.75

94.17
94.23

94.35
94.38

LRMulti (FT)

72.11

87.08

90.88

94.38

Table 5: Top-k accuracy (%), as reported by Caffe [24], on
large scale datasets after ﬁne-tuning (FT) for approximately
one epoch on Places and 3 epochs on ILSVRC. The ﬁrst line
(LRMulti) is the reference performance w/o ﬁne-tuning.

top-1..5 performance on Places 205. However, in this set of
experiments, we also observed similar improvements when
ﬁne-tuning with the standard softmax loss, which achieves
best performance on ILSVRC 2012.

We conclude that a safe choice for multiclass problems
seems to be the softmax loss as it yields competitive re-
sults in all top-k errors. An interesting alternative is the
smooth top-k hinge loss which is faster to train (see Sec-
tion 3) and achieves competitive performance. If one wants
to optimize directly for a top-k error (at the cost of a higher
top-1 error), then further improvements are possible using
either the smooth top-k SVM or the top-k entropy losses.

6. Conclusion

We have done an extensive experimental study of top-k
performance optimization. We observed that the softmax
loss and the smooth top-1 hinge loss are competitive across
all top-k errors and should be considered the primary candi-
dates in practice. Our new top-k loss functions can further
improve these results slightly, especially if one is targeting a
particular top-k error as the performance measure. Finally,
we would like to highlight our new optimization scheme
based on SDCA for the top-k entropy loss which also in-
cludes the softmax loss and is of an independent interest.

References

[1] S. Agarwal. The inﬁnite push: A new support vector ranking
algorithm that directly optimizes accuracy at the absolute top
of the list. In SDM, pages 839–850, 2011. 1

[2] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid.
Good practice in large-scale learning for image classiﬁca-
tion. PAMI, 36(3):507–520, 2014. 1, 4, 16

[3] P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convex-
ity, classiﬁcation and risk bounds. Journal of the American
Statistical Association, 101:138–156, 2006. 3

[4] A. Beck and M. Teboulle. A fast iterative shrinkage-
thresholding algorithm for linear inverse problems. SIAM
Journal on Imaging Sciences, 2(1):183–202, 2009. 14
[5] A. Beck and M. Teboulle. Smoothing and ﬁrst order meth-
ods, a uniﬁed framework. SIAM Journal on Optimization,
22:557–580, 2012. 6

[6] Y. Bengio. Learning deep architectures for AI. Foundations
and Trends in Machine Learning, 2(1):1–127, 2009. 4
[7] A. Bordes, L. Bottou, P. Gallinari, and J. Weston. Solving
multiclass support vector machines with LaRank. In ICML,
pages 89–96, 2007. 1

[8] J. M. Borwein and A. S. Lewis. Convex Analysis and Non-
linear Optimization: Theory and Examples. Cms Books in
Mathematics Series. Springer Verlag, 2000. 11, 14

[9] S. Boyd, C. Cortes, M. Mohri, and A. Radovanovic. Accu-

racy at the top. In NIPS, pages 953–961, 2012. 1

[10] S. Boyd and L. Vandenberghe. Convex Optimization. Cam-

bridge University Press, 2004. 8, 9, 10

[11] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. Learning to rank using gra-
dient descent. In ICML, pages 89–96, 2005. 1

[12] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support
vector machines. ACM Transactions on Intelligent Systems
and Technology, 2:1–27, 2011. 16

[13] M. Cimpoi, S. Maji, and A. Vedaldi. Deep ﬁlter banks for

texture recognition and segmentation. In CVPR, 2015. 15

[14] R. M. Corless, G. H. Gonnet, D. E. Hare, D. J. Jeffrey, and
D. E. Knuth. On the lambert W function. Advances in Com-
putational Mathematics, 5(1):329–359, 1996. 12, 13
[15] K. Crammer and Y. Singer. On the algorithmic implemen-
tation of multiclass kernel-based vector machines. JMLR,
2:265–292, 2001. 4, 7

[16] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, pages 248–255, 2009. 16

[17] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J.
Lin. LIBLINEAR: A library for large linear classiﬁcation.
JMLR, 9:1871–1874, 2008. 16

[18] T. Fukushima. Precise and fast computation of Lambert W-
functions without transcendental function evaluations. Jour-
nal of Computational and Applied Mathematics, 244:77 –
89, 2013. 12, 13

[19] P. Gehler and S. Nowozin. On feature combination for mul-
ticlass object classiﬁcation. In ICCV, pages 221–228, 2009.
23, 24

[20] M. R. Gupta, S. Bengio, and J. Weston. Training highly mul-

ticlass classiﬁers. JMLR, 15:1461–1492, 2014. 1

[21] J.-B. Hiriart-Urruty and C. Lemaréchal. Fundamentals of

Convex Analysis. Springer, Berlin, 2001. 7

[22] A. S. Householder. The Numerical Treatment of a Single

Nonlinear Equation. McGraw-Hill, 1970. 13

[23] C.-W. Hsu and C.-J. Lin. A comparison of methods for multi-
class support vector machines. Neural Networks, 13(2):415–
425, 2002. 15, 16

[24] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014. 16, 17

[25] T. Joachims. A support vector method for multivariate per-
formance measures. In ICML, pages 377–384, 2005. 1
[26] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet clas-
siﬁcation with deep convolutional neural networks. In NIPS,
pages 1106–1114, 2012. 4

[27] K. Lang. Newsweeder: Learning to ﬁlter netnews. In ICML,

pages 331–339, 1995. 16

[28] M. Lapin, M. Hein, and B. Schiele. Top-k multiclass SVM.
In NIPS, 2015. 1, 2, 5, 7, 8, 11, 12, 14, 15, 16, 28, 32, 34, 36
[29] M. Lapin, B. Schiele, and M. Hein. Scalable multitask rep-
resentation learning for scene classiﬁcation. In CVPR, 2014.
31

[30] N. Li, R. Jin, and Z.-H. Zhou. Top rank optimization in linear

time. In NIPS, pages 1502–1510, 2014. 1

[31] L. Liu, T. G. Dietterich, N. Li, and Z. Zhou. Transductive op-
timization of top k precision. CoRR, abs/1510.05976, 2015.
1

[32] J. Mairal. Sparse Coding for Machine Learning, Image Pro-
cessing and Computer Vision. PhD thesis, Ecole Normale
Superieure de Cachan, 2010. 7

[33] J. Mairal, R. Jenatton, F. R. Bach, and G. R. Obozinski. Net-
work ﬂow algorithms for structured sparsity. In NIPS, pages
1558–1566, 2010. 14

[34] Y. Nesterov. Smooth minimization of non-smooth functions.
Mathematical Programming, 103(1):127–152, 2005. 6
[35] M.-E. Nilsback and A. Zisserman. Automated ﬂower clas-
siﬁcation over a large number of classes. In ICVGIP, pages
722–729, 2008. 16

[36] J. Nocedal and S. J. Wright. Numerical Optimization.

Springer Science+ Business Media, 2006. 13

[37] K. B. Petersen, M. S. Pedersen, et al. The matrix cookbook.
Technical University of Denmark, 450:7–15, 2008. 8
[38] A. Quattoni and A. Torralba. Recognizing indoor scenes. In

CVPR, 2009. 16

[39] A. Rakotomamonjy. Sparse support vector inﬁnite push. In

ICML, pages 1335–1342. ACM, 2012. 1

[40] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carls-
son. CNN features off-the-shelf: an astounding baseline for
recognition. In CVPRW, DeepVision workshop, 2014. 15
[41] M. Reid and B. Williamson. Composite binary losses. JMLR,

11:2387–2422, 2010. 5

[42] J. D. Rennie. Improving multi-class text classiﬁcation with
naive bayes. Technical report, Massachusetts Institute of
Technology, 2001. 15

[43] R. Rifkin and A. Klautau. In defense of one-vs-all classiﬁ-

cation. JMLR, 5:101–141, 2004. 16

[44] A. Rocha and S. Klein Goldenstein. Multiclass from bi-
nary: Expanding one-versus-all, one-versus-one and ecoc-
based approaches. Neural Networks and Learning Systems,
IEEE Transactions on, 25(2):289–302, 2014. 15, 16

[45] S. Ross, J. Zhou, Y. Yue, D. Dey, and D. Bagnell. Learn-
ing policies for contextual submodular prediction. In ICML,
pages 1364–1372, 2013. 1

[46] C. Rudin. The p-norm push: A simple convex ranking algo-
rithm that concentrates at the top of the list. JMLR, 10:2233–
2271, 2009. 1

[47] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge, 2014. 1, 16

[48] S. Shalev-Shwartz and T. Zhang. Accelerated proximal
stochastic dual coordinate ascent for regularized loss mini-
mization. Mathematical Programming, pages 1–41, 2014. 1,
2, 6, 7, 8, 11, 12

[49] L. Sharan, R. Rosenholtz, and E. Adelson. Material percep-
tion: What can you see in a brief glance? Journal of Vision,
9(8):784–784, 2009. 16

[50] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 4, 15, 16

[51] K. Swersky, B. J. Frey, D. Tarlow, R. S. Zemel, and R. P.
Adams. Probabilistic n-choose-k models for classiﬁcation
and ranking. In NIPS, pages 3050–3058, 2012. 1, 15, 16
[52] A. Tewari and P. Bartlett. On the consistency of multiclass
classiﬁcation methods. JMLR, 8:1007–1025, 2007. 3, 4
[53] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun.
Large margin methods for structured and interdependent out-
put variables. JMLR, pages 1453–1484, 2005. 1

[54] N. Usunier, D. Buffoni, and P. Gallinari. Ranking with
In ICML, pages

ordered weighted pairwise classiﬁcation.
1057–1064, 2009. 1, 5, 10

[55] D. Veberiˇc. Lambert W function for applications in physics.
Computer Physics Communications, 183(12):2622–2628,
2012. 12, 13

[56] A. Vedaldi and K. Lenc. Matconvnet – convolutional neural
networks for matlab. In Proceeding of the ACM Int. Conf. on
Multimedia, 2015. 16

[57] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The Caltech-UCSD Birds-200-2011 dataset. Technical re-
port, California Institute of Technology, 2011. 16

[58] L. Wang, S. Guo, W. Huang, and Y. Qiao. Places205-vggnet
models for scene recognition. CoRR, abs/1508.01667, 2015.
15, 16

[59] J. Weston, S. Bengio, and N. Usunier. Wsabie: scaling up
to large vocabulary image annotation. IJCAI, pages 2764–
2770, 2011. 1

[60] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba.
SUN database: Large-scale scene recognition from abbey to
zoo. In CVPR, 2010. 1, 16

[61] N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Part-

based rcnn for ﬁne-grained detection. In ECCV, 2014. 15

[62] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using places
database. In NIPS, 2014. 1, 16

ALOI

Method

SVMOVA

LROVA

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

82.4

87.4

89.5

90.7

91.5

92.1

92.6

93.1

93.4

93.7

86.1

91.1

93.0

94.1

94.8

95.4

95.8

96.1

96.4

96.6

top-1 SVMα / SVMMulti 90.0
top-2 SVMα
90.0
top-3 SVMα
89.2
top-4 SVMα
88.4
top-5 SVMα
87.3
top-10 SVMα
85.0

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

93.4
94.0
94.2
94.3
94.1
93.0

94.2
94.3
94.4
94.4
94.3
93.1

93.4
93.9
94.1
94.2
94.3
94.2

94.2
94.2
94.3
94.4
94.4
94.3

94.2
94.2
94.3
94.2
94.2
93.2

94.4
94.3
94.0
93.7
92.4

95.1
95.5
95.5
95.6
95.6
95.5

95.5
95.6
95.7
95.7
95.7
95.6

95.1
95.2
95.4
95.4
95.5
95.7

95.5
95.5
95.6
95.6
95.7
95.7

95.7
95.8
95.8
95.8
95.8
95.6

95.9
95.9
95.8
95.7
94.8

96.0
96.4
96.7
96.8
96.9
96.5

96.2
96.3
96.7
96.9
96.9
96.6

96.0
96.0
96.0
96.1
96.2
96.5

96.2
96.2
96.3
96.3
96.3
96.6

96.5
96.6
96.6
96.7
96.7
96.7

96.7
96.7
96.7
96.7
96.3

96.7
97.0
97.2
97.4
97.4
97.3

96.7
96.8
97.3
97.4
97.5
97.4

96.7
96.7
96.5
96.6
96.7
96.9

96.7
96.7
96.7
96.8
96.8
97.0

97.1
97.1
97.2
97.2
97.2
97.3

97.2
97.3
97.3
97.3
97.1

97.1
97.4
97.6
97.8
97.8
97.8

97.0
97.1
97.6
97.8
97.8
97.8

97.1
97.1
96.9
97.0
97.1
97.3

97.0
97.0
97.1
97.1
97.2
97.4

97.5
97.5
97.5
97.6
97.6
97.7

97.6
97.7
97.7
97.7
97.6

97.5
97.7
97.8
98.0
98.0
98.2

97.6
97.7
97.9
98.0
98.1
98.2

97.5
97.5
97.5
97.3
97.3
97.5

97.6
97.3
97.4
97.4
97.5
97.6

97.8
97.8
97.8
97.8
97.9
98.0

97.9
98.0
98.0
98.0
98.0

97.7
97.9
98.1
98.2
98.3
98.4

97.9
98.0
98.1
98.2
98.3
98.4

97.7
97.7
97.8
97.5
97.5
98.0

97.9
97.9
97.6
97.6
97.7
97.8

98.0
98.0
98.0
98.1
98.1
98.2

98.1
98.2
98.2
98.2
98.2

97.9
98.1
98.2
98.4
98.4
98.6

98.1
98.2
98.3
98.4
98.4
98.6

97.9
97.9
98.0
98.0
97.7
97.9

98.1
98.1
98.1
97.8
97.8
98.1

98.2
98.2
98.2
98.3
98.3
98.4

98.3
98.3
98.4
98.4
98.4

98.1
98.3
98.4
98.5
98.6
98.7

98.2
98.3
98.4
98.5
98.6
98.7

98.1
98.1
98.1
98.2
98.2
98.3

98.2
98.2
98.2
97.9
98.0
98.2

98.4
98.4
98.4
98.4
98.4
98.5

98.5
98.5
98.5
98.6
98.5

90.6
90.3
89.6
88.7
87.6
85.2

90.0
90.2
90.2
90.1
90.0
89.5

90.6
90.6
90.4
90.3
90.2
89.5

89.8
89.4
89.0
88.5
87.9
86.0

89.8
89.3
88.7
87.9
85.2

Table 6: Comparison of different methods in top-k accuracy (%).

Letter

Method

SVMOVA

LROVA

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

63.0

75.7

82.0

85.7

88.1

89.9

91.4

92.8

93.9

94.6

68.1

81.1

86.1

88.4

90.6

92.2

93.4

94.6

95.3

96.2

top-1 SVMα / SVMMulti 76.5
top-2 SVMα
76.1
top-3 SVMα
74.0
top-4 SVMα
71.4
top-5 SVMα
70.8
top-10 SVMα
61.6

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

85.5
86.9
87.0
86.4
85.9
82.5

86.0
87.0
87.1
86.7
86.2
82.9

85.5
86.4
86.9
86.9
86.9
85.9

86.0
86.6
86.9
87.0
87.0
86.1

86.5
86.5
86.4
86.1
85.6
82.5

86.3
83.9
80.3
76.4
67.9

89.2
90.1
91.0
91.4
91.5
88.9

89.9
90.3
90.9
91.4
91.5
89.1

89.2
90.2
90.6
90.9
91.0
90.8

89.9
90.2
90.6
90.8
91.1
90.8

90.1
90.6
90.8
91.0
90.9
89.7

90.7
91.1
90.0
87.7
80.9

91.5
92.2
93.0
93.5
93.9
93.6

92.1
92.5
93.2
93.2
93.8
93.6

91.5
92.1
92.7
93.1
93.4
93.5

92.1
92.4
92.8
93.1
93.4
93.6

93.1
93.2
93.3
93.6
93.7
93.6

93.7
93.9
93.9
93.7
88.9

93.1
93.3
94.4
94.8
95.1
96.0

93.6
94.0
94.5
94.7
95.2
95.9

93.1
93.8
94.2
94.6
94.9
95.3

93.6
93.8
94.2
94.6
94.9
95.2

94.5
94.7
94.9
95.3
95.1
96.2

95.1
95.6
96.1
96.1
93.7

94.3
94.8
95.4
95.7
96.2
97.6

94.9
94.9
95.6
95.7
96.3
97.5

94.3
94.8
95.3
95.5
95.7
96.2

94.9
95.0
95.3
95.5
95.8
96.3

95.7
95.7
95.9
96.3
96.7
97.7

96.1
96.7
97.2
97.5
96.3

95.5
96.0
96.2
96.6
96.9
98.3

95.8
96.0
96.4
96.7
97.0
98.3

95.5
95.9
95.8
96.3
96.5
97.2

95.8
96.0
96.1
96.3
96.6
97.1

96.5
96.5
96.8
97.1
97.3
98.4

97.0
97.5
98.0
98.3
97.7

96.5
96.5
96.7
97.2
97.5
98.9

96.3
96.6
96.9
97.3
97.6
98.9

96.5
96.7
96.7
96.9
97.2
97.8

96.3
96.8
96.6
97.0
97.2
97.9

97.4
97.5
97.6
97.7
97.9
98.9

97.7
98.1
98.5
98.7
98.7

97.0
97.2
97.3
97.6
98.1
99.2

97.0
97.3
97.3
97.8
98.2
99.3

97.0
97.3
97.4
97.5
97.8
98.3

97.0
97.4
97.3
97.8
97.8
98.3

98.1
98.0
98.2
98.3
98.6
99.3

98.1
98.5
98.9
99.2
99.3

97.7
97.7
97.8
98.2
98.4
99.6

97.6
97.7
97.9
98.0
98.6
99.7

97.7
97.8
97.9
98.1
98.2
98.8

97.6
97.8
97.9
98.1
98.2
98.8

98.1
98.4
98.5
98.6
98.8
99.6

98.5
98.8
99.3
99.4
99.6

76.8
76.2
74.1
72.1
70.8
61.7

76.5
76.5
75.6
74.9
74.5
72.9

76.8
76.5
75.6
75.1
74.6
72.9

75.2
74.6
73.0
71.9
69.7
65.0

70.1
63.6
58.7
50.3
46.5

Table 7: Comparison of different methods in top-k accuracy (%).

Method

SVMOVA

LROVA

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

84.3

93.0

95.4

97.0

97.9

98.5

98.8

99.0

99.3

99.5

84.9

93.1

96.3

97.2

97.8

98.3

98.7

98.8

99.0

99.3

News 20

94.9
96.4
96.6
96.7
96.7
96.5

96.3
96.5
96.6
96.7
96.7
96.5

94.9
96.1
96.5
96.6
96.7
96.7

96.3
96.3
96.5
96.6
96.6
96.7

96.4
96.5
96.6
96.7
96.8
96.4

96.6
96.4
96.2
96.0
95.7

96.4
97.2
97.7
97.7
97.8
97.7

97.3
97.6
97.8
97.8
97.9
97.8

96.4
97.4
97.5
97.7
97.7
98.0

97.3
97.4
97.6
97.6
97.6
97.9

97.6
97.7
97.8
97.9
97.8
97.8

97.6
97.7
97.6
97.7
97.0

92.7
94.3
94.8
94.5
94.2
93.3

94.5
94.6
94.7
94.4
94.4
93.5

92.7
94.3
94.5
94.8
94.8
94.6

94.5
94.5
94.5
94.6
94.7
94.7

94.2
94.4
94.6
94.5
94.3
93.3

93.9
93.6
93.5
93.3
92.7

85.6
85.2
85.1
84.9
84.5
82.9

85.4
85.7
86.0
85.9
85.4
84.9

85.6
85.8
85.7
85.6
85.6
84.9

84.5
84.7
84.7
84.5
84.3
82.7

84.2
83.4
83.3
83.2
82.9

top-1 SVMα / SVMMulti 85.4
top-2 SVMα
85.3
top-3 SVMα
85.1
top-4 SVMα
85.0
top-5 SVMα
84.3
top-10 SVMα
82.7

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

97.2
97.9
98.2
98.3
98.4
98.4

98.0
98.1
98.4
98.4
98.4
98.4

97.2
97.8
98.1
98.1
98.3
98.5

98.0
98.0
98.1
98.2
98.3
98.5

98.1
98.3
98.3
98.5
98.6
98.5

98.4
98.3
98.3
98.2
97.9

97.6
98.4
98.6
98.6
98.7
98.6

98.5
98.6
98.7
98.7
98.8
98.7

97.6
97.8
98.4
98.4
98.7
98.7

98.5
98.4
98.5
98.6
98.7
98.7

98.5
98.6
98.7
98.7
98.8
98.7

98.7
98.6
98.6
98.5
98.4

98.2
98.5
99.0
98.9
99.0
99.0

98.8
98.9
99.0
99.0
99.0
99.0

98.2
98.2
98.7
98.7
99.0
99.0

98.8
98.8
98.8
98.9
99.0
99.0

99.0
98.9
98.9
99.0
99.0
98.9

98.9
98.9
98.9
98.9
98.8

98.5
98.9
99.2
99.1
99.2
99.2

99.0
99.1
99.1
99.1
99.1
99.2

98.5
98.6
99.0
99.0
99.1
99.2

99.0
99.0
99.0
99.1
99.2
99.1

99.1
99.2
99.2
99.2
99.1
99.1

99.1
99.2
99.1
99.1
99.0

98.7
99.0
99.3
99.3
99.3
99.3

98.9
99.3
99.3
99.3
99.3
99.3

98.7
98.9
99.2
99.2
99.3
99.3

98.9
99.2
99.3
99.0
99.3
99.3

99.3
99.3
99.3
99.3
99.3
99.3

99.3
99.2
99.2
99.2
99.2

99.1
99.1
99.3
99.4
99.3
99.3

99.3
99.3
99.4
99.4
99.4
99.5

99.1
99.1
99.2
99.2
99.4
99.3

99.3
99.3
99.3
99.4
99.3
99.3

99.5
99.3
99.4
99.4
99.4
99.4

99.4
99.4
99.3
99.4
99.4

Table 8: Comparison of different methods in top-k accuracy (%).

Method

SVMOVA

LROVA

top-1 SVMα / SVMMulti 62.8
top-2 SVMα
63.1
top-3 SVMα
63.4
top-4 SVMα
63.2
top-5 SVMα
63.3
top-10 SVMα
63.0

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

Caltech 101 Silhouettes

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

61.8

73.1

76.5

78.5

80.8

82.5

83.6

84.6

85.6

86.6

63.2

77.1

80.4

82.6

84.4

85.7

87.0

87.9

88.6

89.4

74.6
76.2
76.7
76.6
76.8
77.3

76.9
76.9
77.2
77.0
77.1
77.2

74.6
76.2
76.6
76.9
77.0
77.1

76.9
77.1
77.0
77.0
77.2
77.3

77.8
77.5
77.5
77.5
77.5
77.3

77.5
76.9
75.9
75.4
72.1

77.8
79.0
79.7
79.8
80.0
80.5

80.3
80.5
80.1
80.3
80.5
80.5

77.8
79.3
79.7
80.1
80.4
80.5

80.3
80.4
80.4
80.2
80.5
80.6

81.2
81.1
81.1
81.0
80.9
80.8

81.3
81.1
80.4
79.8
78.4

80.0
81.0
81.5
82.4
82.7
82.7

82.4
82.3
82.3
82.4
82.7
82.8

80.0
81.1
81.4
82.0
82.6
83.0

82.4
82.3
82.4
82.6
82.4
83.2

83.4
83.3
83.3
83.2
83.1
83.4

83.5
83.7
83.1
82.7
82.0

82.0
82.7
83.6
84.0
84.3
84.6

84.0
84.1
84.0
84.3
84.5
84.8

82.0
82.6
83.4
83.5
84.2
84.9

84.0
84.1
84.2
84.4
84.4
85.0

85.1
85.0
85.0
85.0
85.2
85.4

85.5
85.2
85.1
85.2
84.6

83.3
84.4
85.1
85.3
85.6
85.9

85.4
85.3
85.7
85.5
85.9
86.0

83.3
84.3
84.9
85.0
85.3
86.2

85.4
85.5
85.4
85.7
85.9
86.2

86.5
86.5
86.6
86.5
86.4
86.6

86.9
87.0
86.9
86.9
86.2

84.4
85.5
85.8
86.0
86.5
86.8

86.4
86.5
86.5
86.6
86.7
87.0

84.4
85.3
85.7
85.9
86.3
87.3

86.4
86.5
86.6
86.7
86.9
87.1

87.3
87.4
87.4
87.5
87.5
87.9

87.8
88.2
88.2
88.0
87.5

85.0
86.2
86.6
86.9
87.5
88.0

87.2
87.4
87.3
87.3
87.6
88.1

85.0
86.3
86.6
87.0
87.4
87.9

87.2
87.3
87.3
87.4
87.6
88.0

88.7
88.6
88.7
88.6
88.5
88.5

88.7
88.8
88.9
88.8
88.5

85.9
87.0
87.6
87.8
88.1
88.8

88.4
88.4
88.0
88.2
88.5
88.5

85.9
87.0
87.4
87.8
88.1
88.8

88.4
88.4
88.4
88.4
88.4
88.8

89.3
89.4
89.2
89.3
89.3
89.3

89.3
89.8
89.7
89.6
89.4

86.9
87.6
88.3
88.6
88.7
89.1

89.0
89.0
89.2
89.2
89.1
89.1

86.9
87.9
88.0
88.7
89.0
89.4

89.0
89.0
89.1
89.0
89.1
89.5

89.7
89.8
89.9
89.9
89.9
90.1

90.0
90.2
90.5
90.2
90.2

63.9
63.8
63.3
63.1
63.3
63.1

62.8
63.5
63.9
63.9
63.6
64.0

63.9
63.9
64.0
63.7
63.7
64.2

63.2
63.3
63.3
63.2
63.2
62.5

62.7
60.7
60.0
58.3
51.9

Table 9: Comparison of different methods in top-k accuracy (%).

Caltech 101 (average of 39 kernels from [19], 5 splits)

Method

Top-1

Top-2

Top-3

Top-4

Top-5

Top-6

Top-7

Top-8

Top-9

Top-10

top-1 SVMα / SVMMulti 73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
top-2 SVMα
73.2 ± 0.9 81.7 ± 0.7 85.5 ± 0.9 87.8 ± 0.7 89.6 ± 0.6 90.9 ± 0.5 91.9 ± 0.4 92.7 ± 0.3 93.3 ± 0.4 93.9 ± 0.5
top-3 SVMα
73.0 ± 1.0 81.6 ± 0.7 85.4 ± 0.9 87.8 ± 0.6 89.6 ± 0.6 90.9 ± 0.4 91.8 ± 0.3 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.5
top-4 SVMα
72.8 ± 1.0 81.4 ± 0.7 85.4 ± 0.9 87.9 ± 0.6 89.6 ± 0.5 90.9 ± 0.4 91.8 ± 0.3 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
top-5 SVMα
72.6 ± 1.0 81.2 ± 0.7 85.2 ± 0.8 87.7 ± 0.6 89.5 ± 0.6 90.8 ± 0.5 91.7 ± 0.4 92.6 ± 0.4 93.2 ± 0.5 93.7 ± 0.5
top-10 SVMα
71.0 ± 0.8 80.2 ± 1.0 84.2 ± 0.7 87.0 ± 0.8 88.8 ± 0.7 90.1 ± 0.5 91.3 ± 0.4 92.2 ± 0.5 92.9 ± 0.4 93.5 ± 0.4

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
73.2 ± 0.9 81.7 ± 0.7 85.5 ± 0.9 87.8 ± 0.7 89.6 ± 0.6 90.9 ± 0.5 91.9 ± 0.4 92.7 ± 0.3 93.3 ± 0.4 93.9 ± 0.5
73.0 ± 1.0 81.6 ± 0.7 85.4 ± 0.8 87.8 ± 0.6 89.6 ± 0.6 90.9 ± 0.4 91.8 ± 0.3 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
72.8 ± 1.0 81.4 ± 0.7 85.4 ± 0.9 87.9 ± 0.6 89.6 ± 0.5 90.9 ± 0.4 91.8 ± 0.3 92.7 ± 0.4 93.3 ± 0.5 93.8 ± 0.4
72.5 ± 1.0 81.2 ± 0.7 85.2 ± 0.8 87.7 ± 0.6 89.5 ± 0.6 90.8 ± 0.4 91.7 ± 0.4 92.6 ± 0.4 93.2 ± 0.5 93.8 ± 0.4
71.0 ± 0.8 80.2 ± 1.0 84.2 ± 0.7 87.0 ± 0.8 88.8 ± 0.7 90.1 ± 0.5 91.3 ± 0.4 92.2 ± 0.5 92.9 ± 0.4 93.5 ± 0.4

top-1 SVMβ / SVMMulti 73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
top-2 SVMβ
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
top-3 SVMβ
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
top-4 SVMβ
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
top-5 SVMβ
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
top-10 SVMβ
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.9 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.5

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4
73.2 ± 1.0 81.6 ± 0.8 85.5 ± 0.8 87.8 ± 0.7 89.5 ± 0.6 90.8 ± 0.5 91.8 ± 0.4 92.7 ± 0.4 93.3 ± 0.4 93.8 ± 0.4

72.7 ± 0.8 80.9 ± 0.9 84.9 ± 0.9 87.4 ± 0.7 89.1 ± 0.7 90.5 ± 0.6 91.6 ± 0.5 92.4 ± 0.3 93.0 ± 0.5 93.6 ± 0.5
72.6 ± 0.9 80.9 ± 0.8 85.0 ± 0.9 87.5 ± 0.8 89.2 ± 0.5 90.6 ± 0.5 91.6 ± 0.5 92.4 ± 0.4 93.1 ± 0.4 93.6 ± 0.4
72.5 ± 0.9 80.8 ± 0.8 85.0 ± 0.9 87.3 ± 0.7 89.2 ± 0.6 90.5 ± 0.5 91.6 ± 0.4 92.4 ± 0.4 93.1 ± 0.4 93.6 ± 0.5
72.2 ± 1.0 80.7 ± 0.8 84.8 ± 0.9 87.3 ± 0.7 89.0 ± 0.7 90.5 ± 0.5 91.5 ± 0.5 92.4 ± 0.3 93.1 ± 0.5 93.6 ± 0.5
72.0 ± 0.8 80.5 ± 0.9 84.7 ± 0.8 87.2 ± 0.7 89.0 ± 0.6 90.4 ± 0.5 91.5 ± 0.3 92.3 ± 0.2 93.0 ± 0.4 93.6 ± 0.4
70.2 ± 0.9 79.8 ± 1.1 83.5 ± 0.6 86.6 ± 0.5 88.4 ± 0.6 89.7 ± 0.8 90.8 ± 0.7 91.9 ± 0.4 92.7 ± 0.4 93.0 ± 0.5

Table 10: Comparison of different methods in top-k accuracy (%).

Caltech 256 (average of 39 kernels from [19])

Method

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

top-1 SVMα / SVMMulti 47.1
top-2 SVMα
47.1
top-3 SVMα
47.0
top-4 SVMα
46.9
top-5 SVMα
46.8
top-10 SVMα
45.4

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

47.1
47.1
47.0
46.9
46.8
45.4

47.1
47.2
47.2
47.2
47.2
47.2

47.1
47.1
47.1
47.1
47.1
47.2

46.2
46.2
46.1
46.1
46.0
45.0

56.3
56.3
56.3
56.2
56.1
55.3

56.3
56.3
56.2
56.2
56.1
55.3

56.3
56.3
56.3
56.3
56.3
56.3

56.3
56.3
56.3
56.3
56.3
56.3

55.6
55.7
55.8
55.5
55.4
54.6

61.4
61.4
61.5
61.4
61.2
60.7

61.4
61.4
61.5
61.4
61.3
60.8

61.4
61.4
61.4
61.4
61.4
61.4

61.4
61.4
61.4
61.4
61.4
61.4

60.9
60.9
61.0
60.9
60.7
59.9

64.7
64.6
64.6
64.5
64.4
64.1

64.5
64.4
64.5
64.5
64.3
64.1

64.7
64.7
64.7
64.7
64.7
64.7

64.5
64.5
64.5
64.5
64.5
64.5

64.1
64.1
64.2
64.2
64.1
63.7

67.3
67.2
67.3
67.3
67.1
66.7

67.1
67.0
67.3
67.3
67.0
66.6

67.3
67.2
67.2
67.2
67.2
67.2

67.1
67.1
67.1
67.1
67.1
67.1

66.7
66.7
66.8
66.7
66.7
66.2

69.3
69.4
69.4
69.3
69.3
69.0

69.3
69.3
69.4
69.3
69.4
68.9

69.3
69.4
69.4
69.4
69.3
69.3

69.3
69.3
69.3
69.3
69.3
69.3

69.1
69.0
69.0
68.9
68.8
68.4

71.5
71.3
71.3
71.3
71.2
71.0

71.5
71.3
71.3
71.1
71.1
71.0

71.5
71.4
71.4
71.4
71.4
71.5

71.5
71.5
71.5
71.5
71.5
71.5

71.0
71.0
70.9
70.9
70.7
70.4

72.6
72.6
72.6
72.8
72.8
72.6

72.8
72.7
72.7
72.8
72.9
72.6

72.6
72.6
72.6
72.6
72.6
72.7

72.8
72.8
72.7
72.7
72.7
72.7

72.5
72.0
72.5
71.5
72.7
72.3

74.1
74.1
74.1
74.1
74.2
73.9

74.1
74.0
74.1
74.2
74.2
74.0

74.1
74.1
74.1
74.1
74.1
74.1

74.1
74.1
74.1
74.1
74.1
74.1

73.7
73.3
73.7
73.9
73.2
73.5

75.2
75.2
75.1
75.2
75.1
75.2

75.2
75.2
75.1
75.2
75.2
75.2

75.2
75.2
75.2
75.2
75.2
75.2

75.2
75.2
75.2
75.2
75.2
75.2

74.9
74.5
74.8
74.8
74.2
74.7

Table 11: Comparison of different methods in top-k accuracy (%).

Method

SVMOVA

LROVA

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

60.6

71.4

77.1

80.7

83.4

85.2

86.6

87.9

89.0

89.9

62.3

74.8

80.5

84.6

87.4

89.4

90.7

91.9

92.8

93.5

CUB

79.2
79.9
80.4
81.1
81.2
81.3

80.2
80.5
81.1
81.2
81.3
81.4

79.2
79.2
79.8
79.7
79.8
80.8

80.2
80.2
80.2
80.2
80.4
80.9

81.7
81.6
81.8
81.4
81.9
81.6

81.2
81.4
81.1
81.2
80.7

82.8
83.5
83.9
84.6
84.7
85.1

84.0
84.0
84.2
84.7
85.0
85.2

82.8
83.1
83.5
83.8
83.8
84.4

84.0
84.0
83.9
83.9
83.9
84.3

85.2
85.2
85.4
85.4
85.4
85.3

84.7
84.9
84.9
85.0
84.7

73.3
73.7
74.9
75.1
74.7
73.9

74.3
74.7
75.1
75.1
75.0
73.9

73.3
73.4
73.7
73.9
74.4
74.9

74.3
74.3
74.3
74.4
74.4
74.9

75.2
75.4
75.6
75.6
75.4
74.7

74.8
75.0
74.5
74.8
73.2

61.9
62.0
61.9
61.7
61.3
59.8

61.0
61.0
61.3
61.5
61.8
62.1

61.9
61.9
62.0
61.9
61.9
62.4

62.3
62.4
62.5
62.3
62.0
61.2

61.9
62.0
61.8
61.4
59.7

top-1 SVMα / SVMMulti 61.0
top-2 SVMα
61.2
top-3 SVMα
61.3
top-4 SVMα
61.1
top-5 SVMα
60.9
top-10 SVMα
59.6

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

85.7
85.9
86.3
86.7
87.2
87.7

86.9
86.9
86.6
87.1
87.4
87.8

85.7
86.0
86.4
86.6
86.4
86.8

86.9
86.9
86.7
86.8
86.7
86.8

87.9
87.9
87.9
87.8
88.1
88.2

87.6
87.6
87.7
87.7
87.2

87.8
88.2
88.1
88.7
89.0
89.6

88.6
88.6
88.6
89.0
89.2
89.7

87.8
88.1
88.2
88.3
88.5
88.8

88.6
88.6
88.6
88.5
88.7
88.8

89.6
89.7
89.6
89.6
90.0
90.0

89.4
89.4
89.6
89.7
89.4

89.5
89.9
89.9
89.8
90.4
90.7

90.1
90.1
90.2
90.3
90.6
90.7

89.5
89.8
89.9
89.8
90.1
90.1

90.1
90.2
90.2
90.2
90.1
90.3

91.3
91.3
91.2
91.1
91.2
91.3

90.9
90.8
91.0
91.1
90.8

90.7
91.0
91.3
90.9
91.0
91.7

91.4
91.3
91.4
91.4
91.2
91.8

90.7
91.1
91.2
90.9
91.1
91.3

91.4
91.3
91.4
91.3
91.5
91.3

92.5
92.5
92.4
92.4
92.4
92.4

91.9
91.9
92.0
92.0
91.9

91.6
91.9
91.9
91.9
92.1
92.7

92.3
92.2
92.2
92.3
92.2
92.7

91.6
91.9
91.9
91.9
91.6
92.2

92.3
92.2
92.2
92.3
92.4
92.1

93.2
93.2
93.2
93.2
93.2
93.3

92.7
93.0
93.0
92.9
92.8

92.3
92.6
92.5
92.8
92.9
93.4

93.1
93.0
93.2
93.0
92.9
93.4

92.3
92.6
92.5
92.8
92.4
93.1

93.1
93.1
93.2
93.1
93.0
92.9

93.9
93.9
93.9
93.9
93.8
93.8

93.4
93.4
93.6
93.7
93.4

Table 12: Comparison of different methods in top-k accuracy (%).

Flowers

Method

SVMOVA

LROVA

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

82.0

89.2

91.7

93.2

94.3

95.2

95.9

95.8

96.2

96.8

82.6

89.4

92.2

93.9

94.8

95.8

96.4

96.9

97.3

97.6

top-1 SVMα / SVMMulti 82.5
top-2 SVMα
82.3
top-3 SVMα
81.9
top-4 SVMα
81.8
top-5 SVMα
81.7
top-10 SVMα
80.5

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

89.5
89.5
89.3
89.3
89.1
88.8

89.8
89.6
89.7
89.3
89.3
88.8

89.5
89.6
89.8
89.5
89.7
89.7

89.8
89.8
89.8
89.7
89.9
89.8

89.7
89.7
89.5
89.5
89.4
88.9

89.4
89.2
88.8
89.0
87.8

92.2
92.3
92.2
92.3
92.4
91.9

92.4
92.4
92.3
92.4
92.5
91.9

92.2
92.2
92.1
92.1
92.0
92.3

92.4
92.4
92.4
92.4
92.4
92.4

92.4
92.4
92.0
92.4
92.2
92.1

92.3
92.2
92.3
92.0
91.1

93.8
93.9
93.8
94.0
94.1
93.7

94.0
94.0
94.1
94.1
94.1
93.7

93.8
93.7
93.7
93.7
93.7
93.9

94.0
94.0
94.0
94.0
93.9
94.0

94.0
94.0
94.1
94.1
94.1
93.9

93.8
94.2
94.1
93.8
93.0

94.8
95.0
95.0
95.0
95.1
95.1

95.1
95.2
95.2
95.2
95.1
95.1

94.8
94.9
94.8
94.8
94.9
95.0

95.1
95.1
95.1
95.0
95.1
95.1

95.1
95.3
95.3
95.3
95.1
95.0

95.0
95.2
95.0
95.0
94.3

95.6
95.7
95.8
95.9
95.8
95.9

95.9
95.9
95.8
96.0
95.9
95.9

95.6
95.6
95.7
95.6
95.6
95.6

95.9
95.9
95.9
96.0
95.9
96.0

96.0
96.0
96.1
96.0
95.9
95.9

96.0
96.0
95.9
95.7
95.2

96.2
96.5
96.4
96.6
96.6
96.5

96.4
96.5
96.5
96.6
96.6
96.6

96.2
96.2
96.2
96.2
96.2
96.2

96.4
96.4
96.5
96.5
96.5
96.2

96.6
96.6
96.6
96.7
96.6
96.5

96.5
96.7
96.6
96.2
96.0

95.5
95.6
97.0
97.0
97.1
97.1

96.7
96.9
97.1
97.1
97.1
97.0

95.5
95.6
96.8
96.7
96.7
96.7

96.7
96.7
96.7
96.9
96.7
96.7

97.1
97.1
97.1
97.1
97.1
97.0

97.2
97.0
97.2
97.0
96.6

96.0
95.9
96.0
95.6
97.4
97.4

97.3
97.3
97.3
97.5
97.5
97.4

96.0
95.9
95.8
95.6
95.8
97.2

97.3
97.3
97.4
97.3
97.4
97.3

97.4
97.5
97.4
97.5
97.5
97.4

97.4
97.4
97.5
97.3
96.9

96.4
96.2
96.1
97.8
97.8
97.7

97.6
97.6
97.7
97.7
97.8
97.7

96.4
96.3
96.1
96.0
96.2
97.5

97.6
97.6
97.6
97.6
97.6
97.6

97.8
97.8
97.8
97.8
97.9
97.7

97.8
97.6
97.7
97.7
97.3

83.0
82.6
82.5
82.3
82.0
80.6

82.5
82.5
82.4
82.4
82.5
82.7

83.0
83.0
83.0
82.9
83.0
82.7

82.9
82.6
82.5
82.2
82.1
80.9

82.1
82.1
81.9
81.4
77.9

Table 13: Comparison of different methods in top-k accuracy (%).

FMD

Method

SVMOVA

LROVA

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

77.4

87.2

92.4

94.4

96.4

97.8

99.0

99.8 100.0 100.0

79.6

90.2

94.2

96.6

98.2

98.8

99.2 100.0 99.8

100.0

top-1 SVMα / SVMMulti 77.6
top-2 SVMα
78.2
top-3 SVMα
78.8
top-4 SVMα
78.2
top-5 SVMα
78.4

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1

top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr

88.8
89.2
89.2
89.4
89.2

89.4
90.2
89.6
89.4
89.2

88.8
89.6
89.6
90.0
90.2

89.4
89.4
89.8
90.2
90.2

90.6
89.6
89.4
90.2
90.4

89.4
89.6
89.4
89.4

93.8
94.0
94.6
94.6
94.4

93.8
93.8
94.4
94.6
94.4

93.8
93.6
93.6
93.8
94.8

93.8
93.8
94.0
94.2
94.6

94.6
94.6
94.8
94.8
94.4

94.4
95.4
94.8
94.0

95.4
95.8
96.4
96.8
96.8

96.0
96.2
96.2
96.6
96.8

95.4
95.4
95.4
95.6
95.8

96.0
96.0
96.2
96.2
96.0

96.6
97.6
97.4
97.0
97.2

96.6
97.4
96.6
96.4

97.2
97.4
97.8
98.0
97.6

98.0
97.6
98.0
97.8
97.6

97.2
97.4
97.8
97.8
97.4

98.0
98.0
98.2
97.8
97.6

97.8
98.0
98.0
97.8
98.0

98.2
98.2
98.0
97.8

98.4 100.0 100.0 100.0 100.0
99.8 100.0 100.0 100.0
98.6
99.8 100.0 100.0
99.4
98.8
99.6 100.0 100.0
99.4
98.8
100.0
99.8
99.6
99.2
98.6

99.0
99.0
99.0
98.8
98.8

99.4
99.2
99.2
99.2
99.2

99.6 100.0 100.0
100.0
99.8
99.6
100.0
99.8
99.6
100.0
99.8
99.6
100.0
99.8
99.2

98.4 100.0 100.0 100.0 100.0
99.8 100.0 100.0 100.0
98.6
99.8 100.0 100.0
99.2
98.8
99.2
98.8
99.6 100.0 100.0
99.6 100.0 100.0 100.0
98.8

99.0
99.0
99.0
98.8
98.8

98.8
98.8
98.8
98.8
98.8

99.0
98.6
98.8
98.8

99.6 100.0 100.0
99.4
99.6 100.0 100.0
99.4
99.6 100.0 100.0
99.2
99.4
99.6 100.0 100.0
99.4 100.0 100.0 100.0

99.2 100.0 99.8
99.2 100.0 99.8
99.8
99.2
99.2
99.8
99.2
99.0
99.8
99.2
99.2

100.0
100.0
100.0
100.0
100.0

99.2 100.0 100.0 100.0
99.2 100.0 100.0 100.0
99.4
99.8 100.0 100.0
99.2 100.0 100.0 100.0

78.6
78.4
79.0
79.2
79.4

77.6
79.0
79.8
80.4
80.2

78.6
78.6
79.6
79.4
80.0

79.0
79.4
79.8
79.2
79.4

79.0
78.4
77.8
77.2

Table 14: Comparison of different methods in top-k accuracy (%).

Indoor 67 (AlexNet trained on Places 205, FC7 output, provided by [28])

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

Method

SVMOVA

LROVA

top-1 SVMα / SVMMulti 74.0
top-2 SVMα
73.1
top-3 SVMα
71.6
top-4 SVMα
71.4
top-5 SVMα
70.7
top-10 SVMα
70.0

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

71.7

80.1

85.9

90.2

92.6

94.3

94.9

95.7

96.4

96.9

73.1

84.5

89.6

91.8

93.3

94.3

95.3

96.1

96.6

97.0

85.2
85.7
86.3
85.7
85.7
85.4

86.0
85.9
86.1
86.2
86.2
85.2

85.2
85.9
86.3
86.2
85.6
85.3

86.0
85.9
86.3
86.2
85.9
85.5

86.0
85.9
86.0
86.0
85.7
84.8

85.3
83.9
82.4
82.8
82.6

89.3
90.4
91.1
90.7
90.4
90.0

90.7
90.5
90.8
90.7
90.2
89.9

89.3
89.8
90.6
90.8
90.7
90.4

90.7
90.8
90.7
90.7
90.8
90.7

91.2
91.3
91.0
91.2
90.6
90.1

90.4
89.8
89.3
89.6
87.9

91.9
92.2
93.2
93.3
93.2
93.1

92.8
93.0
93.1
93.1
92.8
93.1

91.9
92.2
92.8
92.7
93.0
93.4

92.8
92.8
92.7
92.5
92.7
93.4

93.8
93.7
93.6
93.4
93.0
92.5

92.8
92.6
92.2
92.8
91.9

93.4
94.5
94.7
94.8
94.7
94.6

94.5
94.3
94.6
94.6
94.7
94.6

93.4
94.1
94.4
94.5
94.5
94.4

94.5
94.5
94.5
94.3
94.4
94.6

94.9
94.8
94.9
94.8
94.7
94.9

94.6
94.5
94.2
94.3
93.8

94.9
95.1
95.7
95.7
95.5
95.6

95.9
95.8
95.4
95.5
95.4
95.7

94.9
95.1
95.9
95.8
95.7
95.6

95.9
95.7
95.6
95.7
95.6
95.6

95.7
95.9
95.7
95.7
95.8
95.9

95.6
95.1
95.4
95.3
95.3

95.6
96.2
96.4
96.2
96.1
96.2

96.1
96.3
96.3
96.4
96.3
96.0

95.6
95.7
96.1
96.2
96.2
96.2

96.1
96.2
96.3
96.3
96.2
96.3

96.6
96.6
96.6
96.6
96.5
96.4

96.5
96.3
96.2
95.9
95.9

95.8
96.6
96.6
96.6
96.9
97.1

96.8
96.6
96.6
96.7
97.0
97.0

95.8
96.4
96.3
96.6
96.8
97.2

96.8
96.6
96.6
96.7
97.0
97.1

97.1
97.0
97.2
97.2
97.1
97.1

97.1
97.2
97.0
96.7
96.8

96.4
97.0
97.1
97.2
97.5
97.5

97.1
97.1
97.3
97.4
97.5
97.4

96.4
97.0
96.9
97.1
97.4
97.5

97.1
97.1
97.3
97.4
97.5
97.5

97.5
97.5
97.6
97.5
97.5
97.4

97.6
97.5
97.4
97.4
97.0

96.9
97.3
97.2
97.8
97.9
97.5

97.4
97.4
97.7
97.7
97.8
97.7

96.9
97.3
97.2
97.7
97.6
97.8

97.4
97.5
97.5
97.5
97.8
97.8

97.8
97.8
97.7
97.6
97.6
97.5

97.8
97.5
97.5
97.3
97.3

74.0
72.7
72.2
71.3
71.0
70.3

74.0
74.0
73.0
73.1
72.6
71.9

74.0
73.9
73.7
73.0
72.7
72.2

72.5
72.1
71.4
71.3
71.0
69.6

68.2
68.3
68.2
67.1
66.4

Table 15: Comparison of different methods in top-k accuracy (%).

Method

SVMOVA

LROVA

Indoor 67 (VGG-16 trained on Places 205, FC6 output)

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

80.4

89.9

94.3

95.3

96.0

96.9

97.3

97.7

97.9

98.3

82.5

92.6

95.1

96.3

97.5

97.5

98.3

98.6

98.4

98.7

top-1 SVMα / SVMMulti 82.6
top-2 SVMα
83.1
top-3 SVMα
83.0
top-4 SVMα
82.9
top-5 SVMα
82.5
top-10 SVMα
80.4

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

91.4
92.3
92.5
92.5
92.3
92.2

92.5
93.1
93.0
92.5
92.5
92.1

91.4
92.1
92.5
92.6
92.8
92.8

92.5
92.8
92.9
92.7
92.8
92.9

92.9
93.2
93.3
92.8
92.5
92.5

92.0
92.0
91.9
91.9
90.9

95.0
95.4
95.4
95.6
95.7
95.8

95.1
95.2
95.4
95.7
95.7
95.9

95.0
95.2
95.1
95.2
95.4
95.7

95.1
95.1
95.1
95.4
95.4
95.5

95.4
95.7
95.7
95.7
95.8
95.7

95.5
95.7
95.2
95.1
94.6

96.1
96.8
97.5
97.0
97.0
96.9

97.1
97.1
97.2
97.2
97.0
96.9

96.1
96.6
96.9
97.1
97.1
97.1

97.1
97.0
97.0
97.1
97.0
97.0

97.1
97.2
97.1
96.9
96.9
96.9

96.9
96.9
97.0
96.5
96.2

96.0
97.7
97.8
97.8
97.7
97.7

97.8
97.8
97.8
97.8
97.7
97.5

96.0
97.5
97.9
97.8
97.8
97.7

97.8
97.8
97.8
97.8
97.8
97.7

97.2
97.2
97.5
97.6
97.5
97.6

97.6
97.6
97.7
97.5
97.5

97.9
98.0
98.3
98.2
98.1
98.4

98.1
98.1
98.2
98.2
98.3
98.1

97.9
98.0
98.3
98.4
98.3
98.4

98.1
98.2
98.4
98.4
98.4
98.4

98.1
98.1
98.1
98.2
98.4
98.4

98.1
98.4
98.4
98.1
98.3

97.5
98.3
98.5
98.7
98.5
98.8

98.5
98.4
98.5
98.7
98.7
98.7

97.5
98.4
98.4
98.6
98.6
98.7

98.5
98.5
98.5
98.6
98.7
98.7

98.7
98.7
98.8
98.7
98.7
98.7

98.4
98.6
98.6
98.6
98.6

97.8
98.7
98.7
98.7
98.7
99.0

98.3
98.8
98.8
98.7
98.7
99.0

97.8
98.7
98.7
98.8
98.8
99.0

98.3
98.7
98.8
98.8
98.8
98.9

99.1
99.0
98.9
98.9
99.0
99.0

98.7
98.7
98.7
98.9
98.8

98.3
98.7
98.7
98.8
98.8
99.1

98.4
98.4
98.8
98.8
99.0
99.2

98.3
98.7
98.4
98.8
98.8
99.0

98.4
98.4
98.8
98.8
98.8
99.0

99.1
99.0
99.0
99.0
99.0
99.1

98.8
99.0
98.7
99.1
99.0

98.9
98.5
98.9
99.0
99.0
99.2

99.0
99.0
98.9
99.0
99.1
99.3

98.9
98.5
98.7
98.8
98.9
99.2

99.0
99.0
99.0
99.0
99.0
99.3

99.1
99.2
99.1
99.2
99.2
99.2

99.0
99.1
99.1
99.0
99.2

83.1
83.0
82.9
82.5
82.1
80.1

82.6
83.2
82.9
82.5
82.2
81.9

83.1
82.9
82.8
82.2
82.3
81.6

82.4
82.4
81.9
81.9
81.9
81.2

81.0
81.2
81.1
80.9
78.7

Table 16: Comparison of different methods in top-k accuracy (%).

Method

SVMOVA

LROVA

Indoor 67 (VGG-16 trained on Places 205, FC7 output)

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

81.9

91.0

94.3

95.7

96.5

97.2

97.4

97.7

97.8

98.0

82.0

91.6

94.9

96.3

97.2

97.8

98.1

98.3

98.6

98.7

top-1 SVMα / SVMMulti 82.5
top-2 SVMα
82.0
top-3 SVMα
81.6
top-4 SVMα
80.8
top-5 SVMα
79.9
top-10 SVMα
78.4

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

91.7
91.3
91.4
91.3
91.3
91.0

91.6
91.6
91.5
91.3
91.3
91.0

91.7
91.3
91.4
91.6
91.3
91.8

91.6
91.5
91.4
91.3
91.6
91.7

91.4
91.9
91.5
91.6
91.3
91.0

90.8
90.9
90.1
90.2
88.7

95.4
95.1
95.1
95.2
95.0
95.1

95.2
95.0
95.1
95.1
95.1
95.1

95.4
95.1
95.0
95.2
94.9
95.2

95.2
95.1
95.1
95.0
95.1
95.3

95.2
95.1
95.4
95.4
95.0
95.1

94.4
95.0
94.9
94.3
92.8

96.9
96.6
96.8
96.9
96.5
96.6

96.9
96.7
96.6
96.8
96.6
96.6

96.9
96.9
97.1
96.9
96.9
96.7

96.9
96.9
96.8
97.0
96.7
96.8

97.2
96.9
96.7
96.6
96.6
96.7

96.3
96.2
96.3
96.0
95.7

97.3
97.6
97.7
97.7
97.7
97.4

97.6
97.8
97.8
97.8
97.8
97.5

97.3
97.7
97.7
97.5
97.6
97.3

97.6
97.7
97.6
97.6
97.5
97.2

98.0
97.8
97.6
97.7
97.7
97.6

97.0
97.5
97.4
97.3
97.1

97.8
97.8
98.2
98.6
98.4
98.3

98.1
98.0
98.3
98.4
98.4
98.4

97.8
97.8
98.1
98.1
98.2
98.2

98.1
98.2
98.3
98.3
98.3
98.2

98.4
98.4
98.3
98.3
98.2
98.3

97.9
98.1
98.1
97.8
97.6

97.6
98.0
98.6
98.6
98.6
98.5

98.4
97.9
98.7
98.6
98.6
98.5

97.6
98.3
98.2
98.4
98.4
98.7

98.4
97.9
98.6
98.5
98.5
98.6

98.7
98.7
98.7
98.7
98.7
98.7

98.4
98.5
98.5
98.4
98.3

98.5
98.6
98.7
98.7
98.8
98.8

98.6
98.1
98.7
98.7
98.8
98.8

98.5
98.5
98.6
98.7
98.7
98.9

98.6
98.1
98.1
98.1
98.7
98.8

98.8
98.7
98.8
98.8
98.8
98.7

98.7
98.7
98.9
98.6
98.6

98.8
98.7
98.8
98.9
98.8
99.0

98.7
98.9
98.7
99.0
98.8
99.0

98.8
98.7
98.7
98.9
98.9
99.0

98.7
98.8
98.4
98.9
98.9
99.0

99.0
99.0
99.0
99.0
99.0
99.0

98.9
99.0
99.0
98.7
98.7

99.1
99.0
99.0
99.1
99.0
99.0

99.0
98.7
99.0
99.3
99.1
99.0

99.1
98.7
99.0
98.6
99.0
99.1

99.0
98.7
98.7
98.7
98.8
99.2

99.1
99.3
99.2
99.1
99.0
99.0

99.0
99.1
99.1
99.0
98.9

82.6
82.5
81.6
81.0
80.4
78.3

82.5
82.5
82.1
82.2
82.2
81.7

82.6
82.7
82.5
82.6
82.5
81.6

82.3
81.9
81.4
80.8
80.3
79.2

79.6
79.8
78.7
76.4
72.6

Table 17: Comparison of different methods in top-k accuracy (%).

SUN 397 (Fisher Vector kernels provided by [29])

Method

Top-1

Top-2

Top-3

Top-4

Top-5

Top-6

Top-7

Top-8

Top-9

Top-10

top-1 SVMα / SVMMulti 48.9 ± 0.3 60.6 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-2 SVMα
48.9 ± 0.3 60.5 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-3 SVMα
48.9 ± 0.3 60.5 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.3 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-4 SVMα
48.8 ± 0.3 60.5 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.6 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-5 SVMα
48.7 ± 0.3 60.5 ± 0.3 66.8 ± 0.2 70.9 ± 0.3 73.9 ± 0.3 76.2 ± 0.2 78.1 ± 0.2 79.6 ± 0.2 80.9 ± 0.2 82.1 ± 0.2
top-10 SVMα
48.3 ± 0.4 60.4 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.1 ± 0.2 76.5 ± 0.2 78.4 ± 0.2 80.0 ± 0.2 81.3 ± 0.2 82.5 ± 0.2

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.9 ± 0.3 60.6 ± 0.2 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.8 ± 0.3 60.6 ± 0.2 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.2 78.4 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.7 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.4 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.3 ± 0.4 60.4 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.1 ± 0.2 76.5 ± 0.2 78.5 ± 0.2 80.0 ± 0.2 81.4 ± 0.2 82.6 ± 0.2

top-1 SVMβ / SVMMulti 48.9 ± 0.3 60.6 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-2 SVMβ
48.9 ± 0.3 60.6 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-3 SVMβ
48.9 ± 0.3 60.6 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-4 SVMβ
48.9 ± 0.3 60.6 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-5 SVMβ
48.9 ± 0.3 60.6 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.1 ± 0.2 78.0 ± 0.2 79.5 ± 0.2 80.9 ± 0.2 82.0 ± 0.2
top-10 SVMβ
48.9 ± 0.3 60.6 ± 0.3 66.8 ± 0.2 70.8 ± 0.3 73.8 ± 0.2 76.3 ± 0.2 78.2 ± 0.2 79.8 ± 0.2 81.1 ± 0.2 82.3 ± 0.2

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.4 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.1 ± 0.3 74.2 ± 0.2 76.5 ± 0.3 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2
48.9 ± 0.3 60.6 ± 0.3 66.9 ± 0.2 71.0 ± 0.3 74.2 ± 0.2 76.5 ± 0.2 78.5 ± 0.2 80.1 ± 0.2 81.4 ± 0.2 82.6 ± 0.2

48.5 ± 0.3 60.5 ± 0.2 66.9 ± 0.3 71.2 ± 0.3 74.3 ± 0.3 76.7 ± 0.3 78.6 ± 0.2 80.2 ± 0.2 81.6 ± 0.2 82.7 ± 0.2
48.5 ± 0.3 60.5 ± 0.2 66.9 ± 0.3 71.2 ± 0.3 74.4 ± 0.3 76.7 ± 0.3 78.6 ± 0.2 80.2 ± 0.2 81.6 ± 0.2 82.7 ± 0.2
48.5 ± 0.3 60.5 ± 0.2 66.9 ± 0.3 71.2 ± 0.3 74.4 ± 0.3 76.7 ± 0.3 78.6 ± 0.2 80.2 ± 0.2 81.6 ± 0.2 82.8 ± 0.2
48.5 ± 0.3 60.5 ± 0.2 66.9 ± 0.3 71.2 ± 0.3 74.3 ± 0.3 76.7 ± 0.3 78.6 ± 0.2 80.2 ± 0.2 81.6 ± 0.2 82.8 ± 0.2
48.4 ± 0.3 60.5 ± 0.2 66.9 ± 0.3 71.2 ± 0.3 74.3 ± 0.3 76.7 ± 0.3 78.6 ± 0.2 80.2 ± 0.3 81.6 ± 0.2 82.8 ± 0.2
48.0 ± 0.4 60.3 ± 0.3 66.8 ± 0.3 71.1 ± 0.3 74.3 ± 0.3 76.7 ± 0.2 78.7 ± 0.2 80.2 ± 0.2 81.6 ± 0.2 82.8 ± 0.2

Table 18: Comparison of different methods in top-k accuracy (%).

Method

SVMOVA

LROVA

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

SUN 397 (AlexNet trained on Places 205, FC7 output, provided by [28])

Top-1

Top-2

Top-3

Top-4

Top-5

Top-6

Top-7

Top-8

Top-9

Top-10

44.1 ± 0.4 60.8 ± 0.3 69.9 ± 0.2 75.8 ± 0.1 79.8 ± 0.1 82.8 ± 0.1 85.1 ± 0.2 86.9 ± 0.2 88.3 ± 0.2 89.6 ± 0.2

53.9 ± 0.2 69.2 ± 0.2 76.5 ± 0.2 80.9 ± 0.2 84.0 ± 0.3 86.2 ± 0.2 87.9 ± 0.2 89.2 ± 0.2 90.3 ± 0.2 91.2 ± 0.2

top-1 SVMα / SVMMulti 58.2 ± 0.2 71.7 ± 0.2 78.2 ± 0.1 82.3 ± 0.2 85.0 ± 0.2 87.1 ± 0.2 88.8 ± 0.2 90.0 ± 0.2 91.0 ± 0.2 91.9 ± 0.2
top-2 SVMα
58.8 ± 0.2 72.7 ± 0.2 79.3 ± 0.2 83.3 ± 0.2 85.9 ± 0.2 87.8 ± 0.2 89.2 ± 0.2 90.3 ± 0.2 91.3 ± 0.2 92.2 ± 0.2
top-3 SVMα
59.0 ± 0.1 73.2 ± 0.2 79.9 ± 0.2 83.8 ± 0.2 86.5 ± 0.2 88.3 ± 0.2 89.7 ± 0.2 90.9 ± 0.2 91.8 ± 0.2 92.6 ± 0.2
top-4 SVMα
58.9 ± 0.1 73.5 ± 0.2 80.3 ± 0.2 84.2 ± 0.2 86.8 ± 0.2 88.6 ± 0.2 90.0 ± 0.2 91.1 ± 0.2 92.0 ± 0.2 92.8 ± 0.2
top-5 SVMα
58.9 ± 0.1 73.7 ± 0.2 80.5 ± 0.2 84.4 ± 0.3 87.0 ± 0.2 88.8 ± 0.2 90.2 ± 0.2 91.3 ± 0.2 92.2 ± 0.2 93.0 ± 0.2
top-10 SVMα
58.0 ± 0.2 73.6 ± 0.1 80.8 ± 0.1 84.8 ± 0.2 87.4 ± 0.2 89.3 ± 0.2 90.7 ± 0.2 91.8 ± 0.2 92.7 ± 0.2 93.4 ± 0.2

59.7 ± 0.1 73.8 ± 0.1 80.3 ± 0.2 84.2 ± 0.2 86.8 ± 0.2 88.6 ± 0.2 90.0 ± 0.2 91.1 ± 0.2 92.0 ± 0.2 92.8 ± 0.2
59.6 ± 0.1 73.8 ± 0.1 80.4 ± 0.2 84.3 ± 0.2 86.8 ± 0.2 88.6 ± 0.3 90.1 ± 0.2 91.2 ± 0.2 92.1 ± 0.2 92.9 ± 0.2
59.4 ± 0.1 73.8 ± 0.1 80.4 ± 0.2 84.4 ± 0.2 86.9 ± 0.3 88.7 ± 0.3 90.1 ± 0.2 91.3 ± 0.2 92.2 ± 0.2 92.9 ± 0.2
59.2 ± 0.1 73.9 ± 0.1 80.6 ± 0.2 84.5 ± 0.2 87.1 ± 0.2 88.8 ± 0.2 90.2 ± 0.2 91.4 ± 0.2 92.3 ± 0.2 93.0 ± 0.2
59.1 ± 0.1 74.0 ± 0.2 80.7 ± 0.2 84.6 ± 0.3 87.2 ± 0.2 89.0 ± 0.2 90.4 ± 0.2 91.5 ± 0.2 92.4 ± 0.2 93.1 ± 0.2
58.1 ± 0.2 73.7 ± 0.2 80.9 ± 0.2 84.9 ± 0.2 87.5 ± 0.2 89.4 ± 0.2 90.7 ± 0.2 91.8 ± 0.2 92.7 ± 0.1 93.4 ± 0.1

top-1 SVMβ / SVMMulti 58.2 ± 0.2 71.7 ± 0.2 78.2 ± 0.1 82.3 ± 0.2 85.0 ± 0.2 87.1 ± 0.2 88.8 ± 0.2 90.0 ± 0.2 91.0 ± 0.2 91.9 ± 0.2
top-2 SVMβ
58.8 ± 0.2 72.7 ± 0.2 79.3 ± 0.2 83.2 ± 0.2 85.9 ± 0.2 87.7 ± 0.2 89.0 ± 0.2 90.3 ± 0.2 91.3 ± 0.2 92.2 ± 0.2
top-3 SVMβ
59.1 ± 0.2 73.2 ± 0.2 79.8 ± 0.2 83.8 ± 0.2 86.4 ± 0.2 88.2 ± 0.2 89.6 ± 0.2 90.8 ± 0.2 91.7 ± 0.2 92.5 ± 0.2
top-4 SVMβ
59.2 ± 0.1 73.6 ± 0.2 80.2 ± 0.2 84.2 ± 0.2 86.7 ± 0.2 88.5 ± 0.2 89.9 ± 0.2 91.1 ± 0.2 92.0 ± 0.2 92.7 ± 0.2
top-5 SVMβ
59.3 ± 0.2 73.8 ± 0.2 80.4 ± 0.3 84.4 ± 0.3 87.0 ± 0.3 88.7 ± 0.3 90.1 ± 0.2 91.2 ± 0.2 92.1 ± 0.2 92.9 ± 0.2
top-10 SVMβ
59.3 ± 0.1 74.1 ± 0.2 80.9 ± 0.2 84.9 ± 0.2 87.5 ± 0.2 89.3 ± 0.2 90.7 ± 0.2 91.7 ± 0.2 92.6 ± 0.2 93.4 ± 0.2

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

59.7 ± 0.1 73.8 ± 0.1 80.3 ± 0.2 84.2 ± 0.2 86.8 ± 0.2 88.6 ± 0.2 90.0 ± 0.2 91.1 ± 0.2 92.0 ± 0.2 92.8 ± 0.2
59.7 ± 0.1 73.8 ± 0.2 80.3 ± 0.2 84.3 ± 0.2 86.8 ± 0.2 88.6 ± 0.3 90.0 ± 0.2 91.2 ± 0.2 92.1 ± 0.2 92.9 ± 0.2
59.6 ± 0.1 73.8 ± 0.1 80.4 ± 0.3 84.3 ± 0.2 86.9 ± 0.3 88.7 ± 0.3 90.1 ± 0.2 91.2 ± 0.2 92.1 ± 0.2 92.9 ± 0.2
59.6 ± 0.2 73.9 ± 0.2 80.5 ± 0.2 84.4 ± 0.2 87.0 ± 0.2 88.8 ± 0.3 90.2 ± 0.2 91.3 ± 0.2 92.2 ± 0.2 93.0 ± 0.2
59.5 ± 0.1 74.1 ± 0.1 80.7 ± 0.2 84.6 ± 0.2 87.1 ± 0.2 88.9 ± 0.3 90.3 ± 0.2 91.4 ± 0.2 92.3 ± 0.2 93.1 ± 0.2
59.3 ± 0.1 74.2 ± 0.2 81.0 ± 0.2 85.0 ± 0.2 87.5 ± 0.2 89.3 ± 0.2 90.7 ± 0.2 91.8 ± 0.2 92.7 ± 0.2 93.4 ± 0.2

59.5 ± 0.2 74.2 ± 0.2 81.1 ± 0.2 85.1 ± 0.2 87.7 ± 0.2 89.6 ± 0.2 91.0 ± 0.1 92.1 ± 0.2 93.0 ± 0.2 93.7 ± 0.2
59.5 ± 0.2 74.3 ± 0.2 81.1 ± 0.2 85.1 ± 0.2 87.7 ± 0.2 89.6 ± 0.2 91.0 ± 0.2 92.1 ± 0.2 93.0 ± 0.2 93.7 ± 0.2
59.4 ± 0.1 74.3 ± 0.2 81.2 ± 0.2 85.2 ± 0.2 87.8 ± 0.2 89.6 ± 0.2 91.0 ± 0.2 92.1 ± 0.2 93.0 ± 0.2 93.7 ± 0.2
59.2 ± 0.2 74.3 ± 0.2 81.2 ± 0.2 85.2 ± 0.2 87.8 ± 0.2 89.7 ± 0.2 91.1 ± 0.2 92.2 ± 0.2 93.0 ± 0.2 93.7 ± 0.2
58.9 ± 0.1 74.3 ± 0.2 81.2 ± 0.2 85.2 ± 0.2 87.8 ± 0.2 89.7 ± 0.2 91.0 ± 0.2 92.1 ± 0.1 93.0 ± 0.2 93.7 ± 0.2
58.0 ± 0.2 73.7 ± 0.2 81.0 ± 0.2 85.1 ± 0.2 87.8 ± 0.2 89.7 ± 0.2 91.0 ± 0.2 92.2 ± 0.1 93.1 ± 0.2 93.8 ± 0.1

Table 19: Comparison of different methods in top-k accuracy (%).

Method

SVMOVA

LROVA

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1

SUN 397 (VGG-16 trained on Places 205, FC7 output)

Top-1

Top-2

Top-3

Top-4

Top-5

Top-6

Top-7

Top-8

Top-9

Top-10

65.4 ± 0.2 77.6 ± 0.1 83.8 ± 0.2 87.2 ± 0.1 89.6 ± 0.1 91.3 ± 0.1 92.6 ± 0.2 93.6 ± 0.2 94.3 ± 0.2 94.9 ± 0.1

67.6 ± 0.1 81.5 ± 0.2 87.2 ± 0.2 90.4 ± 0.2 92.4 ± 0.1 93.7 ± 0.1 94.7 ± 0.1 95.4 ± 0.1 96.0 ± 0.1 96.4 ± 0.1

top-1 SVMα / SVMMulti 65.8 ± 0.1 79.0 ± 0.2 85.1 ± 0.2 88.4 ± 0.2 90.8 ± 0.1 92.3 ± 0.1 93.3 ± 0.1 94.2 ± 0.1 94.8 ± 0.1 95.3 ± 0.1
top-2 SVMα
66.4 ± 0.2 80.2 ± 0.2 86.1 ± 0.1 89.4 ± 0.1 91.5 ± 0.1 92.9 ± 0.2 93.8 ± 0.2 94.6 ± 0.2 95.3 ± 0.1 95.7 ± 0.1
top-3 SVMα
66.5 ± 0.2 80.6 ± 0.2 86.5 ± 0.1 89.7 ± 0.2 91.8 ± 0.1 93.2 ± 0.1 94.2 ± 0.1 95.0 ± 0.1 95.3 ± 0.1 95.9 ± 0.1
top-4 SVMα
66.4 ± 0.2 80.8 ± 0.2 86.8 ± 0.1 90.0 ± 0.2 92.1 ± 0.2 93.4 ± 0.1 94.4 ± 0.1 95.1 ± 0.1 95.7 ± 0.1 96.2 ± 0.1
top-5 SVMα
66.3 ± 0.2 81.0 ± 0.2 87.0 ± 0.2 90.2 ± 0.1 92.2 ± 0.2 93.6 ± 0.1 94.5 ± 0.1 95.2 ± 0.1 95.8 ± 0.1 96.3 ± 0.1
top-10 SVMα
64.8 ± 0.3 80.9 ± 0.1 87.2 ± 0.2 90.5 ± 0.2 92.6 ± 0.1 93.9 ± 0.1 94.9 ± 0.1 95.6 ± 0.1 96.2 ± 0.1 96.6 ± 0.1

67.4 ± 0.2 81.1 ± 0.2 86.8 ± 0.1 90.0 ± 0.1 92.0 ± 0.1 93.4 ± 0.1 94.3 ± 0.1 95.1 ± 0.1 95.7 ± 0.1 96.1 ± 0.1
67.2 ± 0.2 81.1 ± 0.2 86.9 ± 0.2 90.1 ± 0.2 92.1 ± 0.1 93.4 ± 0.1 94.4 ± 0.1 95.1 ± 0.1 95.7 ± 0.1 96.2 ± 0.1
67.0 ± 0.2 81.2 ± 0.2 87.0 ± 0.1 90.2 ± 0.2 92.2 ± 0.1 93.5 ± 0.1 94.5 ± 0.1 95.2 ± 0.1 95.7 ± 0.1 96.2 ± 0.0
66.8 ± 0.2 81.2 ± 0.2 87.1 ± 0.1 90.3 ± 0.2 92.3 ± 0.2 93.6 ± 0.1 94.5 ± 0.1 95.2 ± 0.1 95.8 ± 0.1 96.3 ± 0.0
66.5 ± 0.2 81.3 ± 0.2 87.2 ± 0.1 90.4 ± 0.2 92.4 ± 0.2 93.7 ± 0.1 94.6 ± 0.1 95.3 ± 0.1 95.9 ± 0.1 96.3 ± 0.0
64.9 ± 0.3 80.9 ± 0.1 87.3 ± 0.2 90.6 ± 0.2 92.6 ± 0.2 94.0 ± 0.1 94.9 ± 0.1 95.6 ± 0.1 96.2 ± 0.1 96.6 ± 0.1

top-1 SVMβ / SVMMulti 65.8 ± 0.1 79.0 ± 0.2 85.1 ± 0.2 88.4 ± 0.2 90.8 ± 0.1 92.3 ± 0.1 93.3 ± 0.1 94.2 ± 0.1 94.8 ± 0.1 95.3 ± 0.1
top-2 SVMβ
66.4 ± 0.2 80.1 ± 0.1 86.0 ± 0.2 89.3 ± 0.2 91.4 ± 0.1 92.7 ± 0.2 93.8 ± 0.1 94.6 ± 0.2 95.3 ± 0.1 95.8 ± 0.1
top-3 SVMβ
66.8 ± 0.2 80.7 ± 0.2 86.5 ± 0.1 89.7 ± 0.2 91.7 ± 0.1 93.1 ± 0.2 94.2 ± 0.1 94.7 ± 0.1 95.3 ± 0.1 95.9 ± 0.1
top-4 SVMβ
66.9 ± 0.2 80.9 ± 0.2 86.8 ± 0.1 90.0 ± 0.2 92.0 ± 0.2 93.4 ± 0.1 94.4 ± 0.1 95.1 ± 0.1 95.7 ± 0.1 96.1 ± 0.1
top-5 SVMβ
67.0 ± 0.2 81.2 ± 0.2 87.0 ± 0.1 90.2 ± 0.2 92.2 ± 0.1 93.6 ± 0.1 94.5 ± 0.1 95.2 ± 0.1 95.8 ± 0.1 96.2 ± 0.1
top-10 SVMβ
66.9 ± 0.2 81.5 ± 0.2 87.4 ± 0.2 90.7 ± 0.1 92.6 ± 0.1 93.9 ± 0.1 94.8 ± 0.1 95.5 ± 0.1 96.1 ± 0.1 96.5 ± 0.0

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

67.4 ± 0.2 81.1 ± 0.2 86.8 ± 0.1 90.0 ± 0.1 92.0 ± 0.1 93.4 ± 0.1 94.3 ± 0.1 95.1 ± 0.1 95.7 ± 0.1 96.1 ± 0.1
67.3 ± 0.2 81.1 ± 0.2 86.8 ± 0.1 90.0 ± 0.2 92.0 ± 0.1 93.4 ± 0.1 94.4 ± 0.1 95.1 ± 0.1 95.7 ± 0.1 96.2 ± 0.1
67.3 ± 0.2 81.2 ± 0.2 86.9 ± 0.1 90.1 ± 0.1 92.1 ± 0.1 93.4 ± 0.1 94.4 ± 0.1 95.1 ± 0.1 95.7 ± 0.1 96.2 ± 0.1
67.2 ± 0.2 81.3 ± 0.2 87.0 ± 0.1 90.2 ± 0.2 92.2 ± 0.1 93.5 ± 0.1 94.5 ± 0.1 95.2 ± 0.1 95.8 ± 0.1 96.2 ± 0.1
67.2 ± 0.2 81.4 ± 0.2 87.2 ± 0.2 90.3 ± 0.1 92.3 ± 0.1 93.6 ± 0.1 94.6 ± 0.1 95.3 ± 0.1 95.9 ± 0.1 96.3 ± 0.1
66.9 ± 0.2 81.5 ± 0.2 87.5 ± 0.2 90.7 ± 0.1 92.6 ± 0.1 93.9 ± 0.1 94.9 ± 0.1 95.5 ± 0.1 96.1 ± 0.1 96.5 ± 0.1

67.5 ± 0.1 81.7 ± 0.2 87.7 ± 0.2 90.9 ± 0.2 92.9 ± 0.1 94.2 ± 0.1 95.1 ± 0.1 95.8 ± 0.1 96.4 ± 0.1 96.8 ± 0.1
67.4 ± 0.2 81.8 ± 0.2 87.7 ± 0.2 90.9 ± 0.1 92.9 ± 0.1 94.2 ± 0.1 95.1 ± 0.1 95.8 ± 0.1 96.4 ± 0.1 96.8 ± 0.1
67.2 ± 0.2 81.8 ± 0.2 87.7 ± 0.2 90.9 ± 0.1 92.9 ± 0.1 94.2 ± 0.1 95.1 ± 0.1 95.8 ± 0.1 96.4 ± 0.1 96.8 ± 0.1
66.9 ± 0.2 81.7 ± 0.2 87.7 ± 0.2 91.0 ± 0.1 92.9 ± 0.1 94.2 ± 0.1 95.2 ± 0.1 95.9 ± 0.1 96.4 ± 0.1 96.8 ± 0.1
66.6 ± 0.3 81.6 ± 0.2 87.7 ± 0.2 91.0 ± 0.1 92.9 ± 0.1 94.2 ± 0.1 95.2 ± 0.1 95.9 ± 0.1 96.4 ± 0.1 96.8 ± 0.1
65.2 ± 0.3 81.0 ± 0.2 87.4 ± 0.1 90.8 ± 0.2 92.8 ± 0.1 94.2 ± 0.1 95.2 ± 0.1 95.9 ± 0.1 96.4 ± 0.1 96.8 ± 0.1

Table 20: Comparison of different methods in top-k accuracy (%).

Places 205 (AlexNet trained on Places 205, FC7 output, provided by [28])

Method

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

top-1 SVMα / SVMMulti 50.6
top-2 SVMα
51.1
top-3 SVMα
51.3
top-4 SVMα
51.2
top-5 SVMα
50.8
top-10 SVMα
50.1

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

51.8
51.5
51.5
51.3
50.9
50.2

50.6
51.0
51.3
51.4
51.3
50.9

51.8
51.7
51.5
51.5
51.3
51.0

51.1
51.0
50.9
50.7
50.3
48.9

64.5
65.7
66.2
66.3
66.2
65.8

66.4
66.4
66.4
66.4
66.2
65.8

64.5
65.6
66.0
66.2
66.3
66.1

66.4
66.4
66.3
66.4
66.4
66.1

66.1
66.1
66.1
66.0
65.8
64.9

71.4
73.1
73.2
73.5
73.7
73.4

73.5
73.5
73.5
73.7
73.6
73.4

71.4
72.7
73.4
73.6
73.7
73.5

73.5
73.5
73.7
73.7
73.7
73.5

73.5
73.4
73.4
73.3
73.3
72.7

75.5
77.5
77.9
78.1
78.2
78.3

78.1
78.1
78.1
78.1
78.2
78.3

75.5
77.4
77.9
78.0
78.3
78.4

78.1
78.0
78.2
78.3
78.3
78.3

78.1
78.1
78.1
78.0
77.8
77.5

78.5
80.7
81.3
81.4
81.4
81.6

81.4
81.4
81.4
81.5
81.5
81.7

78.5
80.6
81.3
81.4
81.4
81.7

81.4
81.4
81.4
81.5
81.4
81.7

81.5
81.5
81.5
81.5
81.3
81.0

80.7
83.1
83.6
83.7
83.9
84.0

83.9
83.8
83.8
83.8
83.9
84.0

80.7
82.9
83.6
83.8
83.8
84.0

83.9
83.9
83.8
83.8
83.8
84.0

84.1
84.0
83.9
83.9
83.9
83.7

82.5
84.9
85.6
85.7
85.8
86.0

85.7
85.7
85.7
85.9
85.9
86.0

82.5
84.9
85.6
85.7
85.8
86.0

85.7
85.7
85.8
85.8
85.8
86.0

85.9
85.8
85.8
85.7
85.7
85.6

84.0
86.3
87.1
87.3
87.5
87.6

87.4
87.3
87.4
87.5
87.5
87.6

84.0
86.1
87.1
87.3
87.5
87.6

87.4
87.4
87.4
87.4
87.5
87.6

87.6
87.6
87.5
87.5
87.3
87.2

85.1
87.5
88.3
88.7
88.9
89.0

88.7
88.6
88.7
88.8
88.9
89.0

85.1
87.4
88.3
88.7
88.8
89.0

88.7
88.7
88.8
88.8
88.8
89.0

88.9
88.9
88.9
88.9
88.8
88.7

86.2
88.4
89.4
89.7
90.0
90.1

89.8
89.7
89.8
89.9
90.0
90.2

86.2
88.4
89.3
89.8
89.9
90.2

89.8
89.8
89.8
89.9
90.0
90.2

90.0
90.0
89.9
89.9
89.9
89.8

Table 21: Comparison of different methods in top-k accuracy (%).

Places 205 (VGG-16 trained on Places 205, FC7 output)

Method

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

top-1 SVMα / SVMMulti 58.4
top-2 SVMα
58.6
top-3 SVMα
58.6
top-4 SVMα
58.6
top-5 SVMα
58.4
top-10 SVMα
58.0

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

72.5
73.4
73.7
73.8
73.8
73.2

74.2
73.9
74.0
73.8
73.8
73.2

72.5
73.6
73.9
73.9
74.0
74.0

74.2
74.2
74.1
74.0
74.0
74.0

73.9
73.8
73.8
73.6
73.5
72.8

73.4
72.8
71.6
71.2
68.4

78.7
80.1
80.3
80.5
80.5
80.4

80.5
80.4
80.5
80.5
80.5
80.4

78.7
80.0
80.4
80.6
80.6
80.7

80.5
80.5
80.6
80.7
80.7
80.7

80.6
80.6
80.6
80.5
80.4
80.0

80.1
80.0
79.2
79.0
76.9

82.3
84.1
84.5
84.6
84.5
84.6

84.6
84.6
84.6
84.6
84.5
84.5

82.3
83.9
84.5
84.6
84.7
84.8

84.6
84.6
84.7
84.7
84.7
84.8

84.8
84.7
84.8
84.6
84.5
84.2

84.2
84.1
83.6
83.5
82.2

84.7
86.6
87.3
87.4
87.4
87.4

87.3
87.5
87.6
87.4
87.5
87.5

84.7
86.4
87.2
87.4
87.5
87.6

87.3
87.4
87.5
87.5
87.5
87.6

87.6
87.6
87.6
87.5
87.4
87.2

87.2
87.1
86.9
86.9
85.8

86.4
88.5
89.3
89.5
89.5
89.6

89.6
89.6
89.6
89.6
89.5
89.6

86.4
88.3
89.2
89.6
89.6
89.7

89.6
89.6
89.7
89.7
89.7
89.7

89.7
89.6
89.6
89.6
89.6
89.3

89.4
89.3
89.2
89.1
88.3

87.5
89.9
90.8
91.0
91.1
91.2

91.1
91.1
91.1
91.1
91.2
91.3

87.5
89.6
90.7
91.0
91.0
91.3

91.1
91.0
91.1
91.1
91.1
91.3

91.3
91.2
91.2
91.2
91.2
91.0

90.8
90.9
90.8
90.8
90.2

88.4
90.8
91.8
92.1
92.3
92.5

92.2
92.2
92.3
92.3
92.3
92.5

88.4
90.6
91.7
92.1
92.2
92.4

92.2
92.2
92.2
92.3
92.3
92.4

92.5
92.4
92.5
92.4
92.4
92.3

92.0
92.2
92.0
92.0
91.7

89.2
91.6
92.6
93.0
93.2
93.5

93.2
93.1
93.2
93.2
93.2
93.5

89.2
91.4
92.6
93.0
93.2
93.5

93.2
93.1
93.1
93.2
93.3
93.5

93.5
93.5
93.5
93.4
93.4
93.4

93.2
93.3
93.0
93.1
92.9

89.9
92.2
93.3
93.8
94.0
94.3

93.8
93.7
93.9
94.1
94.1
94.3

89.9
92.0
93.2
93.7
94.0
94.2

93.8
93.7
93.8
94.0
94.2
94.3

94.3
94.3
94.2
94.2
94.2
94.1

94.0
94.2
94.1
94.0
93.8

59.2
59.0
58.9
58.7
58.5
58.0

58.4
58.6
58.8
58.8
58.9
58.7

59.2
59.0
59.0
58.9
58.9
58.7

59.0
58.9
58.7
58.5
58.1
57.0

57.7
56.8
55.2
54.2
51.1

Table 22: Comparison of different methods in top-k accuracy (%).

ILSVRC 2012 (AlexNet trained on ImageNet, FC7 output, provided by [28])

Method

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

top-1 SVMα / SVMMulti 56.6
top-2 SVMα
56.6
top-3 SVMα
56.6
top-4 SVMα
56.5
top-5 SVMα
56.5
top-10 SVMα
55.9

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

57.1
56.7
56.6
56.6
56.5
55.9

56.6
56.9
57.0
57.0
57.1
56.9

57.1
57.1
54.6
57.1
57.1
56.9

55.8
55.6
55.5
55.4
55.2
54.8

67.3
68.1
68.3
68.4
68.4
68.2

68.3
68.4
68.4
68.5
68.5
68.2

67.3
68.0
68.3
68.4
68.4
68.4

68.3
68.3
66.1
68.5
68.5
68.5

67.4
67.4
67.4
67.3
67.2
66.9

72.4
73.2
73.6
73.8
73.8
73.8

73.5
73.6
73.8
73.9
73.9
73.8

72.4
73.2
73.5
73.6
73.7
73.9

73.5
73.6
71.5
73.8
73.8
73.9

73.1
73.0
73.0
73.0
72.9
72.7

75.4
76.4
76.8
77.1
77.2
77.3

76.7
76.8
77.0
77.1
77.3
77.4

75.4
76.2
76.7
76.9
76.9
77.3

76.7
76.7
74.7
77.0
77.0
77.3

76.6
76.5
76.5
76.5
76.5
76.4

77.7
78.6
79.0
79.3
79.4
79.8

78.9
79.0
79.2
79.4
79.5
79.7

77.7
78.5
78.9
79.1
79.3
79.5

78.9
78.9
77.1
79.2
79.3
79.6

79.0
79.0
78.9
78.9
78.9
78.8

79.3
80.3
80.7
81.1
81.1
81.4

80.6
80.8
80.9
81.1
81.2
81.4

79.3
80.2
80.6
80.8
81.0
81.2

80.6
80.6
78.7
80.9
81.0
81.2

80.8
80.8
80.7
80.7
80.7
80.6

80.8
81.7
82.1
82.4
82.5
82.8

82.0
82.1
82.4
82.5
82.6
82.8

80.8
81.6
82.0
82.2
82.4
82.7

82.0
82.0
80.2
82.3
82.5
82.7

82.2
82.2
82.2
82.1
82.1
82.1

82.0
82.8
83.2
83.5
83.7
84.0

83.1
83.2
83.5
83.7
83.7
84.0

82.0
82.7
83.1
83.4
83.5
83.8

83.1
83.2
81.3
83.5
83.6
83.8

83.4
83.4
83.4
83.4
83.4
83.3

82.9
83.7
84.2
84.5
84.6
85.0

84.1
84.2
84.4
84.6
84.6
85.0

82.9
83.6
84.0
84.3
84.5
84.8

84.1
84.2
82.3
84.5
84.6
84.8

84.4
84.4
84.4
84.4
84.3
84.3

83.7
84.6
85.0
85.3
85.4
85.8

84.9
85.0
85.2
85.3
85.5
85.8

83.7
84.4
84.8
85.1
85.2
85.6

84.9
84.9
83.1
85.2
85.3
85.7

85.3
85.2
85.2
85.2
85.2
85.2

Table 23: Comparison of different methods in top-k accuracy (%).

ILSVRC 2012 (VGG-16 trained on ImageNet, FC7 output)

Method

Top-1 Top-2 Top-3 Top-4 Top-5 Top-6 Top-7 Top-8 Top-9 Top-10

top-1 SVMα / SVMMulti 68.3
top-2 SVMα
68.3
top-3 SVMα
68.2
top-4 SVMα
68.0
top-5 SVMα
67.8
top-10 SVMα
67.0

top-1 SVMα
1
top-2 SVMα
1
top-3 SVMα
1
top-4 SVMα
1
top-5 SVMα
1
top-10 SVMα
1
top-1 SVMβ / SVMMulti
top-2 SVMβ
top-3 SVMβ
top-4 SVMβ
top-5 SVMβ
top-10 SVMβ

top-1 SVMβ
1
top-2 SVMβ
1
top-3 SVMβ
1
top-4 SVMβ
1
top-5 SVMβ
1
top-10 SVMβ
1

top-1 Ent / LRMulti
top-2 Ent
top-3 Ent
top-4 Ent
top-5 Ent
top-10 Ent

top-2 Enttr
top-3 Enttr
top-4 Enttr
top-5 Enttr
top-10 Enttr

78.6
79.3
79.5
79.6
79.5
79.0

79.5
79.6
79.6
79.7
79.6
79.1

78.6
79.2
79.5
79.5
79.6
79.5

79.5
79.5
79.5
79.6
79.6
79.5

78.5
78.4
78.4
78.3
78.2
77.8

78.1
77.8
77.7
77.1
76.7

82.9
83.7
84.0
84.1
84.1
83.8

83.9
84.0
84.1
84.2
84.1
83.8

82.9
83.6
83.9
84.0
84.1
84.0

83.9
84.0
84.1
84.1
84.1
84.1

83.2
83.2
83.1
83.1
83.0
82.8

83.0
82.8
82.7
82.3
82.0

85.4
86.3
86.5
86.6
86.6
86.5

86.4
86.5
86.6
86.7
86.6
86.5

85.4
86.1
86.4
86.6
86.6
86.6

86.4
86.5
86.6
86.6
86.6
86.6

85.9
85.9
85.9
85.8
85.8
85.7

85.7
85.7
85.6
85.2
85.0

87.0
87.8
88.1
88.3
88.2
88.3

88.0
88.1
88.2
88.4
88.4
88.3

87.0
87.6
88.0
88.2
88.2
88.3

88.0
88.1
88.1
88.3
88.3
88.3

87.7
87.8
87.8
87.8
87.7
87.6

87.6
87.5
87.5
87.3
87.0

88.2
89.0
89.3
89.5
89.5
89.6

89.3
89.3
89.4
89.6
89.6
89.6

88.2
88.9
89.2
89.4
89.5
89.6

89.3
89.3
89.4
89.5
89.5
89.6

89.1
89.1
89.1
89.1
89.1
89.0

89.0
89.0
88.9
88.7
88.5

89.2
89.9
90.2
90.4
90.5
90.6

90.2
90.2
90.3
90.5
90.5
90.6

89.2
89.8
90.1
90.3
90.4
90.6

90.2
90.2
90.3
90.4
90.4
90.6

90.1
90.1
90.1
90.1
90.1
90.0

90.0
89.9
89.9
89.8
89.6

89.9
90.6
91.0
91.2
91.2
91.4

90.9
91.0
91.1
91.2
91.3
91.4

89.9
90.6
90.8
91.0
91.1
91.3

90.9
91.0
91.0
91.1
91.2
91.4

91.0
91.0
91.0
91.0
91.0
90.9

90.8
90.8
90.8
90.6
90.5

90.6
91.3
91.6
91.8
91.9
92.1

91.6
91.7
91.8
91.8
92.0
92.1

90.6
91.2
91.5
91.6
91.7
92.0

91.6
91.6
91.6
91.7
91.8
92.0

91.7
91.7
91.7
91.7
91.6
91.6

91.6
91.5
91.5
91.3
91.2

91.1
91.8
92.1
92.3
92.4
92.6

92.1
92.2
92.3
92.3
92.5
92.6

91.1
91.7
91.9
92.1
92.2
92.5

92.1
92.1
92.2
92.2
92.3
92.5

92.2
92.2
92.2
92.2
92.2
92.1

92.1
92.1
92.1
91.9
91.8

68.7
68.5
68.2
68.0
67.9
67.1

68.3
68.6
68.5
68.4
68.4
68.0

68.7
68.7
68.6
68.5
68.4
68.0

67.2
67.1
66.8
66.7
66.5
65.8

66.6
65.9
66.0
65.0
64.6

Table 24: Comparison of different methods in top-k accuracy (%).

