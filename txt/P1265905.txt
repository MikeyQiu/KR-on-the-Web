Learning a Deep Embedding Model for Zero-Shot Learning

Li Zhang

Tao Xiang

Shaogang Gong

Queen Mary University of London
{david.lizhang, t.xiang, s.gong}@qmul.ac.uk

9
1
0
2
 
l
u
J
 
9
1
 
 
]

V
C
.
s
c
[
 
 
4
v
8
8
0
5
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

Zero-shot learning (ZSL) models rely on learning a joint
embedding space where both textual/semantic description
of object classes and visual representation of object images
can be projected to for nearest neighbour search. Despite
the success of deep neural networks that learn an end-to-
end model between text and images in other vision problems
such as image captioning, very few deep ZSL model exists
and they show little advantage over ZSL models that utilise
deep feature representations but do not learn an end-to-end
embedding.
In this paper we argue that the key to make
deep ZSL models succeed is to choose the right embedding
space. Instead of embedding into a semantic space or an
intermediate space, we propose to use the visual space as
the embedding space. This is because that in this space,
the subsequent nearest neighbour search would suffer much
less from the hubness problem and thus become more effec-
tive. This model design also provides a natural mechanism
for multiple semantic modalities (e.g., attributes and sen-
tence descriptions) to be fused and optimised jointly in an
end-to-end manner. Extensive experiments on four bench-
marks show that our model signiﬁcantly outperforms the ex-
isting models. Code is available at: https://github.
com/lzrobots/DeepEmbeddingModel_ZSL

1. Introduction

A recent trend in developing visual recognition models is
to scale up the number of object categories. However, most
existing recognition models are based on supervised learn-
ing and require a large amount (at least 100s) of training
samples to be collected and annotated for each object class
to capture its intra-class appearance variations [7]. This
severely limits their scalability – collecting daily objects
such as chair is easier, but many other categories are rare
(e.g., a newly identiﬁed specie of beetle on a remote paciﬁc
island). None of these models can work with few or even
no training samples for a given class. In contrast, humans
are very good at recognising objects without seeing any vi-
sual samples, i.e., zero-shot learning (ZSL). For example, a

child would have no problem recognising a zebra if she has
seen horses before and also read elsewhere that a zebra is
a horse but with black-and-white stripes on it. Inspired by
humans’ ZSL ability, recently there is a surge of interest in
machine ZSL [3, 54, 25, 1, 40, 46, 11, 34, 12, 15, 27, 52,
37, 5, 14, 4, 6, 55, 56].

A zero-shot learning method relies on the existence of
a labelled training set of seen classes and the knowledge
about how an unseen class is semantically related to the
seen classes. Seen and unseen classes are usually related
in a high dimensional vector space, called semantic space,
where the knowledge from seen classes can be transferred
to unseen classes. The semantic spaces used by most early
works are based on semantic attributes [9, 10, 35]. Given
a deﬁned attribute ontology, each class name can be repre-
sented by an attribute vector and termed as a class prototype.
More recently, semantic word vector space [46, 11] and sen-
tence descriptions/captions [37] have started to gain popu-
larity. With the former, the class names are projected into a
word vector space so that different classes can be compared,
whilst with the latter, a neural language model is required to
provide a vector representation of the description.

With the semantic space and a visual feature representa-
tion of image content, ZSL is typically solved in two steps:
(1) A joint embedding space is learned where both the se-
mantic vectors (prototypes) and the visual feature vectors
can be projected to; and (2) nearest neighbour (NN) search
is performed in this embedding space to match the pro-
jection of an image feature vector against that of an un-
seen class prototype. Most state-of-the-arts ZSL models
[12, 14, 3, 4, 40, 54, 25] use deep CNN features for vi-
sual feature representation; the features are extracted with
pretrained CNN models. They differ mainly in how to learn
the embedding space given the features. They are thus not
end-to-end deep learning models.

In this paper, we focus on end-to-end learning of a deep
embedding based ZSL model which offers a number of
advantages. First, end-to-end optimisation can potentially
lead to learning a better embedding space. For example,
if sentence descriptions are used as the input to a neural
language model such as recurrent neural networks (RNNs)

for computing a semantic space, both the neural language
model and the CNN visual feature representation learning
model can be jointly optimised in an end-to-end fashion.
Second, a neural network based joint embedding model
offers the ﬂexibility for addressing various transfer learn-
ing problems such as multi-task learning and multi-domain
learning [52]. Third, when multiple semantic spaces are
available, this model can provide a natural mechanism for
fusing the multiple modalities. However, despite all these
intrinsic advantages, in practice, the few existing end-to-end
deep models for ZSL in the literature [27, 11, 46, 52, 37]
fail to demonstrate these advantages and yield only weaker
or merely comparable performances on benchmarks when
compared to non-deep learning alternatives.

We argue that the key to the success of a deep embed-
ding model for ZSL is the choice of the embedding space.
Existing models, regardless whether they are deep or non-
deep, choose either the semantic space [25, 14, 46, 11] or an
intermediate embedding space [27, 3, 40, 12] as the embed-
ding space. However, since the embedding space is of high
dimension and NN search is to be performed there, the hub-
ness problem is inevitable [36], that is, a few unseen class
prototypes will become the NNs of many data points, i.e.,
hubs. Using the semantic space as the embedding space
means that the visual feature vectors need to be projected
into the semantic space which will shrink the variance of the
projected data points and thus aggravate the hubness prob-
lem [36, 8].

In this work, we propose a novel Deep neural network
based Mmbedding Model (DEM) for ZSL which differs
from existing models in that: (1) To alleviate the hubness
problem, we use the output visual feature space of a CNN
subnet as the embedding space. The resulting projection di-
rection is from a semantic space, e.g., attribute or word vec-
tor, to a visual feature space. Such a direction is opposite to
the one adopted by most existing models. We provide a the-
oretical analysis and some intuitive visualisations to explain
why this would help us counter the hubness problem. (2) A
simple yet effective multi-modality fusion method is devel-
oped in our neural network model which is ﬂexible and im-
portantly enables end-to-end learning of the semantic space
representation.

The contributions of this work are as follows: (i) A novel
deep embedding model for ZSL has been formulated which
differs from existing models in the selection of embedding
space. (ii) A multi-modality fusion method is further de-
veloped to combine different semantic representations and
to enable end-to-end learning of the representations. Exten-
sive experiments carried out on four benchmarks including
AwA [25], CUB [49] and large scale ILSVRC 2010 and
ILSVRC 2012 [7] show that our model beats all the state-
of-the-art models presented to date, often by a large margin.

2. Related Work

Semantic space
Existing ZSL methods differ in what se-
mantic spaces are used: typically either attribute [9, 10, 35,
47], word vector [46, 11], or text description [37, 53]. It
has been shown that an attribute space is often more ef-
fective than a word vector space [3, 54, 25, 40]. This is
hardly surprising as additional attribute annotations are re-
quired for each class. Similarly, state-of-the-art results on
ﬁne-grained recognition tasks have been achieved in [37]
using image sentence descriptions to construct the semantic
space. Again, the good performance is obtained at the price
of more manual annotation: 10 sentence descriptions need
to be collected for each image, which is even more expen-
sive than attribute annotation. This is why the word vector
semantic space is still attractive: it is ‘free’ and is the only
choice for large scale recognition with many unseen classes
[14]. In this work, all three semantic spaces are considered.
Fusing multiple semantic spaces
Multiple semantic
spaces are often complementary to each other; fusing them
thus can potentially lead to improvements in recognition
performance. Score-level fusion is perhaps the simplest
strategy [15]. More sophisticated multi-view embedding
models have been proposed. Akata et al. [3] learn a joint
embedding semantic space between attribute, text and hier-
archical relationship which relies heavily on hyperparame-
ter search. Multi-view canonical correlation analysis (CCA)
has also been employed [12] to explore different modali-
ties of testing data in a transductive way. Differing from
these models, our neural network based model has an em-
bedding layer to fuse different semantic spaces and connect
the fused representation with the rest of the visual-semantic
embedding network for end-to-end learning. Unlike [12], it
is inductive and does not require to access the whole test set
at once.
Embedding model
Existing methods also differ in the
visual-semantic embedding model used. They can be cate-
gorised into two groups: (1) The ﬁrst group learns a map-
ping function by regression from the visual feature space to
the semantic space with pre-computed features [25, 14] or
deep neural network regression [46, 11]. For these embed-
ding models, the semantic space is the embedding space.
(2) The second group of models implicitly learn the rela-
tionship between the visual and semantic space through a
common intermediate space, again either with a neural net-
work formulation [27, 52] or without [27, 3, 40, 12]. The
embedding space is thus neither the visual feature space,
nor the semantic space. We show in this work that using
the visual feature space as the embedding space is intrinsi-
cally advantageous due to its ability to alleviate the hubness
problem.
Deep ZSL model All recent ZSL models use deep CNN
features as inputs to their embedding model. However, few
are deep end-to-end models. Existing deep neural network

based ZSL works [11, 46, 27, 52, 37] differ in whether they
use the semantic space or an intermediate space as the em-
bedding space, as mentioned above. They also use different
losses. Some of them use margin-based losses [11, 52, 37].
Socher et al [46] choose a euclidean distance loss. Ba et al
[27] takes a dot product between the embedded visual fea-
ture and semantic vectors and consider three training losses,
including a binary cross entropy loss, hinge loss and Eu-
clidean distance loss. In our model, we ﬁnd that the least
square loss between the two embedded vectors is very effec-
tive and offers an easy theoretical justiﬁcation as for why it
copes with the hubness problem better. The work in [37]
differs from the other models in that it integrates a neu-
ral language model into its neural network for end-to-end
learning of the embedding space as well as the language
model.
In additional to the ability of jointly learning the
neural language model and embedding model, our model
is capable of fusing text description with other semantic
spaces and achieves better performance than [37].
The hubness problem The phenomenon of the presence
of ‘universal’ neighbours, or hubs, in a high-dimensional
space for nearest neighbour search was ﬁrst studied by
Radovanovic et al. [29]. They show that hubness is an inher-
ent property of data distributions in a high-dimensional vec-
tor space, and a speciﬁc aspect of the curse of dimension-
ality. A couple of recent studies [8, 44] noted that regres-
sion based zero-shot learning methods suffer from the hub-
ness problem and proposed solutions to mitigate the hub-
ness problem. Among them, the method in [8] relies on
the modelling of the global distribution of test unseen data
ranks w.r.t. each class prototypes to ease the hubness prob-
lem. It is thus transductive. In contrast, the method in [44] is
inductive: It argued that least square regularised projection
functions make the hubness problem worse and proposed
to perform reverse regression, i.e., embedding class proto-
types into the visual feature space. Our model also uses the
visual feature space as the embedding space but achieve so
by using an end-to-end deep neural network which yields
far superior performance on ZSL.

3. Methodology

3.1. Problem deﬁnition

i , tu

Assume a labelled training set of N training samples is
given as Dtr = {(Ii, yu
i ), i = 1, . . . , N }, with associ-
ated class label set Ttr, where Ii is the i-th training image,
i ∈ RL×1 is its corresponding L-dimensional semantic
yu
representation vector, tu
i ∈ Ttr is the u-th training class la-
bel for the i-th training image. Given a new test image Ij,
the goal of ZSL is to predict a class label tv
j ∈ Tte, where tv
j
is the v-th test class label for the j-th test instance. We have
Ttr ∩ Tte = ∅, i.e., the training (seen) classes and test (un-
seen) classes are disjoint. Note that each class label tu or tv

is associated with a pre-deﬁned semantic space representa-
tion yu or yv (e.g. attribute vector), referred to as semantic
class prototypes. For the training set, yu
is given because
i
each training image Ii is labelled by a semantic representa-
tion vector representing its corresponding class label tu
j .

3.2. Model architecture

The architecture of our model is shown in Fig. 1. It has
two branches. One branch is the visual encoding branch,
which consists of a CNN subnet that takes an image Ii as
input and outputs a D-dimensional feature vector φ(Ii) ∈
RD×1. This D-dimensional visual feature space will be
used as the embedding space where both the image con-
tent and the semantic representation of the class that the
image belongs to will be embedded. The semantic em-
bedding is achieved by the other branch which is a seman-
tic encoding subnet. Speciﬁcally, it takes a L-dimensional
semantic representation vector of the corresponding class
yu
i as input, and after going through two fully connected
(FC) linear + Rectiﬁed Linear Unit (ReLU) layers outputs
a D-dimensional semantic embedding vector. Each of the
FC layer has a l2 parameter regularisation loss. The two
branches are linked together by a least square embedding
loss which aims to minimise the discrepancy between the
visual feature φ(Ii) and its class representation embedding
vector in the visual feature space. With the three losses, our
objective function is as follows:

L(W1, W2) =

||φ(Ii) − f1(W2f1(W1yu

i ))||2

1
N

N
(cid:88)

i=1

+λ(||W1||2 + ||W2||2)

(1)

where W1 ∈ RL×M are the weights to be learned in the
ﬁrst FC layer and W2 ∈ RM ×D for the second FC layer.
λ is the hyperparameter weighting the strengths of the two
parameter regularisation losses against the embedding loss.
We set f1((cid:5)) to be the Rectiﬁed Linear Unit (ReLU) which
introduces nonlinearity in the encoding subnet [24].

After that, the classiﬁcation of the test image Ij in the
visual feature space can be achieved by simply calculating
its distance to the embed prototypes:

v = arg min

D(φ(Ij), f1(W2f1(W1yv)))

(2)

v

where D is a distance function, and yv is the semantic space
vector of the v-th test class prototype.

3.3. Multiple semantic space fusion

As shown in Fig. 1, we can consider the semantic rep-
resentation and the ﬁrst FC and ReLU layer together as a

Figure 1: Illustration of the network architecture of our deep embedding model. The detailed architecture of the semantic
representation unit in the left branch (semantic encoding subnet) is given in (a), (b) and (c) which correspond to the single
modality (semantic space) case, the multiple (two) modality case, and the case where one of the modalities is text description.
For the case in (c), the semantic representation itself is a neural network (RNN) which is learned end-to-end with the rest of
the network.

semantic representation unit. When there is only one se-
mantic space considered, it is illustrated in Fig. 1(a). How-
ever, when more than one semantic spaces are used, e.g., we
want to fuse attribute vector with word vector for semantic
representation of classes, the structure of the semantic rep-
resentation unit is changed slightly, as shown in Fig. 1(b).

More speciﬁcally, we map different semantic representa-
tion vectors to a multi-modal fusion layer/space where they
are added. The output of the semantic representation unit
thus becomes:

1

· yu2

f2(W(1)
· yu1
1
i ∈ RL1×1 and yu2

i + W(2)
i ),
(3)
where yu1
i ∈ RL2×1 denote two differ-
ent semantic space representations (e.g., attribute and word
vector), “+” denotes element-wise sum, W(1)
1 ∈ RL1×M
and W(2)
1 ∈ RL2×M are the weights which will be learned.
f2((cid:5)) is the element-wise scaled hyperbolic tangent func-
tion [26]:

f2(x) = 1.7159 · tanh(

x).

(4)

2
3

This activation function forces the gradient into the most
non-linear value range and leads to a faster training process
than the basic hyperbolic tangent function.

3.4. Bidirectional LSTM encoder for description

The structure of the semantic representation unit needs
to be changed again, when text description is avalialbe for
each training image (see Fig. 1(c)). In this work, we use
a recurrent neural network (RNN) to encode the content of
a text description (a variable length sentence) into a ﬁxed-
length semantic vector. Speciﬁcally, given a text descrip-
tion of T words, x = (x1, . . . , xT ) we use a Bidirectional

RNN model [42] to encode them. For the RNN cell, the
Long-Shot Term Memory (LSTM) [19] units are used as
the recurrent units. The LSTM is a special kind of RNN,
which introduces the concept of gating to control the mes-
sage passing between different times steps. In this way, it
could potentially model long term dependencies. Following
[17], the model has two types of states to keep track of the
historical records: a cell state c and a hidden state h. For a
particular time step t, they are computed by integrating the
current inputs xt and previous state (ct−1, ht−1). During
the integrating, three types of gates are used to control the
messaging passing: an input gate it, a forget gate ft and an
output gate ot.

We omit the formulation of the bidirectional LSTM here
and refer the readers to [17, 16] for details. With the bidirec-
tional LSTM model, we use the ﬁnal output as our encoded
semantic feature vector to represent the text description:

f (W−→
h

·

−→
h + W←−
h

←−
h ),

·

(5)

−→
h denote the forward ﬁnal hidden state,

←−
h denote
where
the backward ﬁnal hidden state. f ((cid:5)) = f1((cid:5)) if text descrip-
tion is used only for semantic space unit, and f ((cid:5)) = f2((cid:5))
if other semantic space need to be fused (Sec. 3.3). W−→
h
and W←−

h are the weights which will be learned.

In the testing stage, we ﬁrst extract text encoding from
test descriptions and then average them per-class to form the
test prototypes as in [37]. Note that since our ZSL model
is a neural network, it is possible now to learn the RNN
encoding subnet using the training data together with the
rest of the network in an end-to-end fashion.

||A(cid:62)(AA(cid:62) + λI)−1A||2 =

≤ 1.

(8)

3.6. Relationship to other deep ZSL models

σ2
σ2 + λ

3.5. The hubness problem

How does our model deal with the hubness problem?
First we show that our objective function is closely related
to that of the ridge regression formulation. In particular, if
we use the matrix form and write the outputs of the semantic
representation unit as A and the outputs of the CNN visual
feature encoder as B, and ignore the ReLU unit for now,
our training objective becomes

L(W) = ||B − WA||2

F + λ||W||2
F ,

(6)

which is basically ridge regression.
It is well known
that ridge regression has a closed-form solution W =
BA(cid:62)(AA(cid:62) + λI)−1. Thus we have:

||WA||2 = ||BA(cid:62)(AA(cid:62) + λI)−1A||2
≤ ||B||2||A(cid:62)(AA(cid:62) + λI)−1A||2

(7)

It can be further shown that

Where σ is the largest singular value of A. So we have
||WA||2 ≤ ||B||2. This means the mapped source data
||WA||2 are likely to be closer to the origin of the space
than the target data ||B||2, with a smaller variance.

(a) S → V

(b) V → S

Figure 2: Illustration of the effects of different embedding
directions on the hubness problem. S: semantic space, and
V: visual feature space. Better viewed in colour.

Why does this matter in the context of ZSL? Figure 2
gives an intuitive explanation. Speciﬁcally, assuming the
feature distribution is uniform in the visual feature space,
Fig. 2(a) shows that if the projected class prototypes are
slightly shrunk towards the origin, it would not change how
hubness problem arises – in other words, it at least does not
make the hubness issue worse. However, if the mapping di-
rection were to be reversed, that is, we use the semantic vec-
tor space as the embedding space and project the visual fea-
ture vectors φ(I) into the space, the training objective is still
ridge regression-like, so the projected visual feature repre-
sentation vectors will be shrunk towards the origin as shown

in Fig. 2(b). Then there is an adverse effect: the semantic
vectors which are closer to the origin are more likely to be-
come hubs, i.e. nearest neighbours to many projected visual
feature representation vectors. This is conﬁrmed by our ex-
periments (see Sec. 4) which show that using which space
as the embedding space makes a big difference in terms of
the degree/seriousness of the resultant hubness problem and
therefore the ZSL performance.
Measure of hubness To measure the degree of hubness
in a nearest neighbour search problem, the skewness of the
(empirical) Nk distribution is used, following [36, 44]. The
Nk distribution is the distribution of the number Nk(i) of
times each prototype i is found in the top k of the rank-
ing for test samples (i.e. their k-nearest neighbour), and its
skewness is deﬁned as follows:

(Nkskewness) =

(cid:80)l

i=1(Nk(i) − E[Nk])3/l
V ar[Nk] 3

2

,

(9)

where l is the total number of test prototypes. A large skew-
ness value indicates the emergence of more hubs.

Let’s now compare the proposed model with the related
end-to-end neural network based models: DeViSE [11],
Socher et al. [46], MTMDL [52], and Ba et al. [27]. Their
model structures fall into two groups. In the ﬁrst group (see
Fig. 3(a)), DeViSE [11] and Socher et al. [46] map the CNN
visual feature vector to a semantic space by a hinge ranking
loss or least square loss.
In contrast, MTMDL [52] and
Ba et al. [27] fuse visual space and semantic space to a
common intermediate space and then use a hinge ranking
loss or a binary cross entropy loss (see Fig. 3(b)). For both
groups, the learned embedding model will make the vari-
ance of WA to be smaller than that of B, which would thus
make the hubness problem worse. In summary, the hubness
will persist regardless what embedding model is adopted, as
long as NN search is conducted in a high dimensional space.
Our model does not worsen it, whist other deep models do,
which leads to the performance difference as demonstrated
in our experiments.

4. Experiments

4.1. Dataset and settings

We follow two ZSL settings: the old setting and the new
GBU setting provided by [51] for training/test splits. Un-
der the old setting, adopted by most existing ZSL works
before [51], some of the test classes also appear in the Im-
ageNet 1K classes, which have been used to pretrain the
image embedding network, thus violating the zero-shot as-
In contrast, the new GBU setting ensures that
sumption.
none of the test classes of the datasets appear in the Im-
ageNet 1K classes. Under both settings, the test set can

Model setting and training Unless otherwise speciﬁed,
We use the Inception-V2 [48, 21] as the CNN subnet in the
old and conventional setting, and ResNet101 [18] for the
GBU and generalised setting, taking the top pooling units
as image embedding with dimension D = 1024 and 2048
respectively. The CNN subnet is pre-trained on ILSVRC
2012 1K classiﬁcation without ﬁne-tuning, the same as the
recent deep ZSL works [27, 37]. For fair comparison with
DeViSE [11], ConSE [34] and AMP [15] on ILSVRC 2010,
we also use the Alexnet [24] architecture and pretrain it
from scratch using the 800 training classes. All input im-
ages are resized to 224 × 224. Fully connected layers of
our model are initialised with random weights for all of our
experiments. Adam [22] is used to optimise our model with
a learning rate of 0.0001 and a minibatch size of 64. The
model is implemented based on Tensorﬂow.
Parameter setting
In the semantic encoding branch of
our network, the output size of the ﬁrst FC layer M is
set to 300 and 700 for AwA and CUB respectively when
a single semantic space is used (see Fig. 1(a)). Speciﬁ-
cally, we use one FC layer for ImageNet in our experiments.
For multiple semantic space fusion, the multi-modal fusion
layer output size is set to 900 (see Fig. 1(b)). When the
semantic representation was encoded from descriptions for
the CUB dataset, a bidirectional LSTM encoding subnet is
employed (see Fig. 1(c)). We use the BasicLSTMCell
in Tensorﬂow as our RNN cell and employ ReLU as acti-
vation function. We set the input sequence length to 30;
longer text inputs are cut off at this point and shorter ones
are zero-padded. The word embedding size and the number
of LSTM unit are both 512. Note that with this LSTM sub-
net, RMSprop is used in the place of Adam to optimise the
whole network with a learning rate of 0.0001, a minibatch
size of 64 and gradient clipped at 5. The loss weighting
factor λ in Eq. (1) is searched by ﬁve-fold cross-validation.
Speciﬁcally, 20% of the seen classes in the training set are
used to form a validation set.

4.2. Experiments on small scale datasets

Competitors Numerous existing works reported results
on AwA and CUB these two relatively small-scale datasets
under old setting. Among them, only the most competitive
ones are selected for comparison due to space constraint.
The selected 13 can be categorised into the non-deep model
group and the deep model group. All the non-deep mod-
els use ImageNet pretrained CNN to extract visual features.
They differ in which CNN model is used: FO indicates that
overfeat [43] is used; FG for GoogLeNet [48]; and FV for
VGG net [45]. The second group are all neural network
based with a CNN subnet. For fair comparison, we imple-
ment the models in [11, 46, 52, 27] on AwA and CUB with
Inception-V2 as the CNN subnet as in our model and [37].
The compared methods also differ in the semantic spaces

(a) [11, 46]

(b) [52, 27]

Figure 3: The architectures of existing deep ZSL models
fall into two groups: (a) learning projection function ω from
visual feature space to semantic space; (b) learning an inter-
mediate space as embedding space.

comprise only the unseen class samples (conventional test
set setting) or a mixture of seen and unseen class samples.
The latter, termed generalised zero-shot learning (GZSL), is
more realistic in practice.
Four benchmarks are selected for the old set-
Datasets
ting: AwA (Animals with Attributes) [25] consists of
30,745 images of 50 classes. It has a ﬁxed split for eval-
uation with 40 training classes and 10 test classes. CUB
(CUB-200-2011) [49] contains 11,788 images of 200 bird
species. We use the same split as in [3] with 150 classes
for training and 50 disjoint classes for testing. ImageNet
(ILSVRC) 2010 1K [41] consists of 1,000 categories and
more than 1.2 million images. We use the same train-
ing/test split as [30, 11] which gives 800 classes for train-
ImageNet (ILSVRC)
ing and 200 classes for testing.
2012/2010: for this dataset, we use the same setting as [14],
that is, ILSVRC 2012 1K is used as the training seen
classes, while 360 classes in ILSVRC 2010 which do not
appear in ILSVRC 2012 are used as the test unseen classes.
Three datasets [51] are selected for GBU setting: AwA1,
AwA2 and CUB. The newly released AwA2 [51] consists of
37,322 images of 50 classes which is an extension of AwA
while AwA1 is same as AwA but under the GBU setting.
Semantic space For AwA, we use the continuous 85-
dimension class-level attributes provided in [25], which
have been used by all recent works. For the word vector
space, we use the 1,000 dimension word vectors provided
in [12, 13]. For CUB, continuous 312-dimension class-level
attributes and 10 descriptions per image provided in [37] are
used. For ILSVRC 2010 and ILSVRC 2012, we trained a
skip-gram language model [31, 32] on a corpus of 4.6M
Wikipedia documents to extract 1, 000D word vectors for
each class.

used. Attributes (A) are used by all methods; some also use
word vector (W) either as an alternative to attributes, or in
conjunction with attributes (A+W). For CUB, recently the
instance-level sentence descriptions (D) are used [37]. Note
that only inductive methods are considered. Some recent
methods [56, 12, 13] are tranductive in that they use all
test data at once for model training, which gives them a big
unfair advantage.
Comparative results on AwA under old setting
From
Table 1 we can make the following observations: (1) Our
model DEM achieves the best results either with attribute
or word vector. When both semantic spaces are used, our
result is further improved to 88.1%, which is 7.6% higher
(2) The perfor-
than the best result reported so far [55].
mance gap between our model to the existing neural net-
work based models are particularly striking.
In fact, the
four models [11, 46, 52, 27] achieve weaker results than
most of the compared non-deep models that use deep fea-
tures only and do not perform end-to-end training. This ver-
ify our claim that selecting the appropriate visual-semantic
embedding space is critical for the deep embedding models
to work. (3) As expected, the word vector space is less in-
formative than the attribute space (86.7% vs. 78.8%) even
though our word vector space alone result already beats all
published results except for one [55]. Nevertheless, fusing
the two spaces still brings some improvement (1.4%).
Comparative results on CUB under old setting
Table
1 shows that on the ﬁne-grained dataset CUB, our model
also achieves the best result.
In particular, with attribute
only, our result of 58.3% is 3.8% higher than the strongest
competitor [5]. The best result reported so far, however,
was obtained by the neural network based DS-SJE [37] at
56.8% using sentence descriptions. It is worth pointing out
that this result was obtained using a word-CNN-RNN neu-
ral language model, whilst our model uses a bidirectional
LSTM subnet, which is easier to train end-to-end with the
rest of the network. When the same LSTM based neural
language model is used, DS-SJE reports a lower accuracy
of 53.0%. Further more, with attribute only, the result of
DS-SJE (50.4%) is much lower than ours. This is signiﬁ-
cant because annotating attributes for ﬁne-grained classes is
probably just about manageable; but annotating 10 descrip-
tions for each images is unlikely to scale to large number of
classes. It is also evident that fusing attribute with descrip-
tions leads to further improvement.
Comparative results under the GBU setting We fol-
low the evaluation setting of [51]. We compare our model
with 13 alternative ZSL models in Table 2. We can see
that on AwA1, AwA2 and aPY, the proposed model DEM
is particularly strong under the more realistic GZSL setting
measured using the harmonic mean (H) metric. In particu-
lar, DEM achieves state-of-the-art performance on AwA1,
AwA2 and SUN under conventional setting with 68.4%,

Model

AMP [15]
SJE [3]
SJE [3]
ESZSL [40]
SSE-ReLU [54]
JLSE [55]
SS-Voc [14]
SynC-struct [5]
SEC-ML [4]

DeViSE [11]
Socher et al. [46]
MTMDL [52]
Ba et al. [27]
DS-SJE [37]

DEM

DEM

F

FO
FG
FG
FG
FV
FV
FO
FG
FV

NG
NG
NG
NG
NG

NG

NG

SS

A+W
A
A+W
A
A
A
A/W
A
A

A/W
A/W
A/W
A/W
A/D

AwA

CUB

66.0
66.7
73.9
76.3
76.3
80.5
78.3/68.9
72.9
77.3

56.7/50.4
60.8/50.3
63.7/55.3
69.3/58.7
-

-
50.1
51.7
47.2
30.4
42.1
-
54.5
43.3

33.5
39.6
32.3
34.0
50.4/56.8

A/W(D)

86.7/78.8

58.3/53.5

A+W(D)

88.1

59.0

Table 1: Zero-shot classiﬁcation accuracy (%) comparison
on AwA and CUB (hit@1 accuracy over all samples) un-
der the old and conventional setting. SS: semantic space;
A: attribute space; W: semantic word vector space; D: sen-
tence description (only available for CUB). F: how the vi-
sual feature space is computed; For non-deep models: FO
if overfeat [43] is used; FG for GoogLeNet [48]; and FV
for VGG net [45]. For neural network based methods,
all use Inception-V2 (GoogLeNet with batch normalisa-
tion) [48, 21] as the CNN subnet, indicated as NG.

67.1% and 61.9%, outperforming alternatives by big mar-
gins.

4.3. Experiments on ImageNet

Comparative results on ILSVRC 2010
Compared to
AwA and CUB, far fewer works report results on the large-
scale ImageNet ZSL tasks. We compare our model against
8 alternatives on ILSVRC 2010 in Table 3, where we use
hit@5 rather than hit@1 accuracy as in the small dataset
experiments. Note that existing works follow two set-
tings. Some of them [33, 20] use existing CNN model
(e.g. VGG/GoogLeNet) pretrained from ILSVRC 2012 1K
classes to initialise their model or extract deep visual fea-
ture. Comparing to these two methods under the same set-
ting, our model gives 60.7%, which beats the nearest rival
PDDM [20] by over 12%. For comparing with the other 6
methods, we follow their setting and pretrain our CNN sub-
net from scratch with Alexnet [24] architecture using the
800 training classes for fair comparison. The results show
that again, signiﬁcant improvement has been obtained with
our model.
Comparative results on ILSVRC 2012/2010
Even
fewer published results on this dataset are available. Table 4
shows that our model clearly outperform the state-of-the-art

H

2
.
7

8
.
1

6
.
1
1

3
.
3
1

0
.
4

9
.
0
2

8
.
9
1

5
.
9
1

8
.
5
1

3
.
6
2

4
.
3
1

8
.
1
1

-

1
.
5
2

8
.
7
3

9
.
9
3

0
.
8
2

4
.
6
3

4
.
7
2

5
.
0
3

8
.
8
2

9
.
7
2

1
.
3
3

3
.
3
4

0
.
8
1

-

u

2
.
4

0
.
1

8
.
6

7
.
8

1
.
2

9
.
6
1

7
.
4
1

7
.
4
1

0
.
1
1

8
.
1
2

9
.
7

8
.
8

-

s

L
S
Z
G

N
U
S

L
S
Z

1
T

9
.
9
3

4
.
9
1

8
.
8
3

9
.
9
3

5
.
1
5

5
.
6
5

7
.
3
5

3
.
5
5

5
.
4
5

1
.
8
5

3
.
6
5

3
.
0
4

-

H

0
.
9

4
.
0
1

0
.
0

0
.
9
1

4
.
0

2
.
9

9
.
6

2
.
0

6
.
4

7
.
8

3
.
3
1

9
.
0

-

3
.
8
7

6
.
5
6

2
.
1
9

2
.
4
7

9
.
8
7

9
.
6
7

7
.
5
5

0
.
3
7

1
.
0
7

7
.
3
7

3
.
6
6

9
.
0
8

-

u

8
.
4

7
.
5

0
.
0

2
.
0

9
.
4

7
.
3

1
.
0

4
.
2

6
.
4

4
.
7

4
.
0

-

9
.
0
1

L
S
Z
G

s

Y
P
a

L
S
Z

1
T

8
.
3
3

6
.
6
3

9
.
6
2

0
.
8
2

0
.
4
3

8
.
9
3

9
.
2
3

2
.
5
3

3
.
8
3

7
.
9
3

9
.
3
2

3
.
8

-

6
.
5
2

3
.
4
3

5
.
0
2

9
.
1
6

4
.
9
1

1
.
5
7

1
.
1
1

0
.
5
3

H

3
.
3

4
.
0

1
.
3

7
.
8

4
.
4
1

8
.
2
3

6
.
3
3

0
.
4
2

0
.
1
2

4
.
4
3

8
.
9
1

6
.
3
1

0
.
7
4

2
.
9
2

9
.
7
6

8
.
2
7

2
.
2
7

1
.
0
6

9
.
6
4

0
.
3
5

2
.
9
5

3
.
7
5

8
.
3
6

8
.
2
6

9
.
0
7

0
.
4
5

1
.
1
6

9
.
7
5

u

7
.
1

2
.
0

6
.
1

7
.
4

5
.
8

8
.
3
2

5
.
3
2

2
.
5
1

6
.
2
1

7
.
3
2

5
.
1
1

8
.
7

1
.
8
3

6
.
9
1

L
S
Z
G

s

B
U
C

L
S
Z

1
T

0
.
0
4

0
.
4
2

3
.
4
3

6
.
4
3

9
.
3
4

0
.
2
5

9
.
3
5

3
.
9
4

9
.
3
5

9
.
4
5

6
.
5
5

3
.
3
3

6
.
5
5

7
.
1
5

H

0
.
0

8
.
1

0
.
1

9
.
5
1

8
.
4
1

8
.
7
2

4
.
4
1

0
.
0
2

0
.
1
1

9
.
3
2

0
.
8
1

2
.
2

3
.
5
4

1
.
5
4

7
.
4
8

6
.
7
8

6
.
0
9

0
.
9
8

5
.
2
8

7
.
4
7

9
.
3
7

3
.
7
7

8
.
7
7

8
.
1
8

5
.
0
9

2
.
2
8

4
.
3
9

4
.
6
8

u

0
.
0

9
.
0

5
.
0

7
.
8

1
.
8

1
.
7
1

0
.
8

5
.
1
1

9
.
5

0
.
4
1

0
.
0
1

1
.
1

0
.
0
3

5
.
0
3

L
S
Z
G

s

2
A
w
A

L
S
Z

1
T

1
.
6
4

9
.
5
3

5
.
4
4

9
.
7
3

0
.
1
6

7
.
9
5

9
.
1
6

8
.
5
5

6
.
8
5

5
.
2
6

6
.
6
4

1
.
4
5

2
.
4
6

1
.
7
6

H

0
.
0

1
.
4

8
.
0

3
.
5
1

9
.
2
1

4

.
2
2

6

.
9
1

3
.
3
1

1
.
2
1

5

.
7
2

2
.
6
1

5
.
3

7

.
6
4

3

.
7
4

7
.
8
8

2
.
8
7

6
.
8
8

9
.
6
8

5
.
0
8

7
.
8
6

6
.
4
7

7
.
1
7

6
.
5
7

1
.
6
7

3
.
7
8

1
.
7
7

3
.
1
9

7
.
4
8

u

0
.
0

1
.
2

4
.
0

4
.
8

0
.
7

4
.
3
1

3
.
1
1

3
.
7

6
.
6

8
.
6
1

9
.
8

8
.
1

4
.
1
3

8
.
2
3

L
S
Z
G

s

1
A
w
A

L
S
Z

1
T

1
.
4
4

9
.
5
3

6
.
5
4

5
.
9
3

1
.
0
6

2
.
4
5

6
.
5
6

1
.
5
5

2
.
8
5

9
.
9
5

0
.
4
5

0
.
3
5

2
.
8
6

4
.
8
6

]
4
3
[
E
S
n
o
C

]
6
4
[
T
M
C

]
4
5
[
E
S
S

]
1
1
[
E
S
i
V
e
D

]
3
[
E
J
S

]
0
5
[

M
E
T
A
L

]
0
4
[
L
S
Z
S
E

]
5
2
[
P
A
D

]
5
2
[
P
A

I

]
5
[

C
N
Y
S

]
3
2
[
E
A
S

]
2
[
E
L
A

l
e
d
o
M

:
2
e
l
b
a
T

M
E
D

]
7
4
[

t
e
N
n
o
i
t
a
l
e
R

n
o
1
T
=
u
g
n
i
s
u
d
e
r
u
s
a
e
m
s
i

t
i

,

L
S
Z
G

r
e
d
n
u
d
n
a

,
)

%

(
y
c
a
r
u
c
c
a

)
1
T

(
1
-
p
o
T
e
g
a
r
e
v
a

s
s
a
l
c
-
r
e
p
g
n
i
s
u
d
e
t
a
u
l
a
v
e

s
i

e
c
n
a
m
r
o
f
r
e
p
e
h
t

,
g
n
i
t
t
e
s
L
S
Z

t
a
h
t

r
e
d
n
U

.
s
t
e
s
a
t
a
d
r
u
o
f
n
o
s
t
l
u
s
e
r

e
v
i
t
a
r
a
p
m
o
C

.
n
a
e
m
c
i
n
o
m
r
a
h
=
H
d
n
a

,
s
e
s
s
a
l
c

n
e
e
s

n
o

1
T
=
s

,
s
e
s
s
a
l
c

n
e
e
s
n
u

Model

hit@5

ConSE [34]
DeViSE [11]
Mensink et al. [30]
Rohrbach [39]
PST [38]
AMP [15]

Ours

DEM

Gaussian Embedding [33]
PDDM [20]

28.5
31.8
35.7
34.8
34.0
41.0

46.7

45.7
48.2

60.7

Table 3: Comparative results (%) on ILSVRC 2010 (hit@1
accuracy over all samples) under the old and conventional
setting.

alternatives by a large margin.

Model

hit@1

hit@5

ConSE [34]
DeViSE [11]
AMP [15]
SS-Voc [14]

DEM

7.8
5.2
6.1
9.5

11.0

15.5
12.8
13.1
16.8

25.7

Table 4: Comparative results (%) on ILSVRC 2012/2010
(hit@1 accuracy over all samples) under the old and con-
ventional setting.

4.4. Further analysis

Importance of embedding space selection We argued
that the key for an effective deep embedding model is the
use of the CNN output visual feature space rather than the
semantic space as the embedding space. In this experiment,
we modify our model in Fig. 1 by moving the two FC lay-
ers from the semantic embedding branch to the CNN feature
extraction branch so that the embedding space now becomes
the semantic space (attributes are used). Table 5 shows that
by mapping the visual features to the semantic embedding
space, the performance on AwA drops by 26.1% on AwA,
highlighting the importance of selecting the right embed-
ding space. We also hypothesised that using the CNN visual
feature space as the embedding layer would lead to less hub-
ness problem. To verify that we measure the hubness using
the skewness score (see Sec. 3.5). Table 6 shows clearly that
the hubness problem is much more severe when the wrong
embedding space is selected. We also plot the data distri-
bution of the 10 unseen classes of AwA together with the
prototypes. Figure 4 suggests that with the visual feature
space as the embedding space, the 10 classes form com-

(a) S → V

(b) V → S

Figure 4: Visualisation of the distribution of the 10 unseen class images in the two embedding spaces on AwA using t-
SNE [28]. Different classes as well as their corresponding class prototypes (in squares) are shown in different colours. Better
viewed in colour.

pact clusters and are near to their corresponding prototypes,
whilst in the semantic space, the data distributions of differ-
ent classes are much less separated and a few prototypes are
clearly hubs causing miss-classiﬁcation.

Model

AwA

CUB

Linear regression (V → S)
Linear regression (S → V)
DEM

54.0
74.8
86.7

40.7
45.7
58.3

Loss

Visual → Semantic

Semantic → Visual

Least square loss
Hinge loss

60.6
57.7

86.7
72.8

Table 7: Zero-shot classiﬁcation accuracy (%) comparison
with linear regression on AwA and CUB.

Table 5: Effects of selecting different embedding space and
different loss functions on zero-shot classiﬁcation accuracy
(%) on AwA.

N1 skewness

Visual → Semantic
Semantic → Visual

AwA

0.4162
-0.4834

CUB

8.2697
2.2594

5. Conclusion

Table 6: N1 skewness score on AwA and CUB with differ-
ent embedding space.

Neural network formulation Can we apply the idea of
using visual feature space as embedding space to other mod-
els? To answer this, we consider a very simple model based
on linear ridge regression which maps from the CNN fea-
ture space to the attribute semantic space or vice versa. In
Table 7, we can see that even for such a simple model, very
impressive results are obtained with the right choice of em-
bedding space. The results also show that with our neural
network based model, much better performance can be ob-
tained due to the introduced nonlinearity and its ability to
learn end-to-end.
Choices of the loss function As reviewed in Sec. 2, most
existing ZSL models use either margin based losses or bi-
nary cross entropy loss to learn the embedding model. In

this work, least square loss is used. Table 5 shows that
when the semantic space is used as the embedding space,
a slightly inferior result is obtained using a hinge ranking
loss in place of least square loss in our model. However,
least square loss is clearly better when the visual feature
space is the embedding space.

We have proposed a novel deep embedding model for
zero-shot learning. The model differs from existing ZSL
model in that it uses the CNN output feature space as the
embedding space. We hypothesise that this embedding
space would lead to less hubness problem compared to the
alternative selections of embedding space. Further more,
the proposed model offers the ﬂexible of utilising multiple
semantic spaces and is capable of end-to-end learning when
the semantic space itself is computed using a neural net-
work. Extensive experiments show that our model achieves
state-of-the-art performance on a number of benchmark
datasets and validate the hypothesis that selecting the cor-
rect embedding space is the key for achieving the excellent
performance.

Acknowledgement

This work was funded in part by the European FP7

Project SUNNY (grant agreement no. 313243).

References

[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-
embedding for attribute-based classiﬁcation. In CVPR, 2013.
1

[2] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-

embedding for image classiﬁcation. TPAMI, 2016. 8

[3] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele. Eval-
uation of output embeddings for ﬁne-grained image classiﬁ-
cation. In CVPR, 2015. 1, 2, 6, 7, 8

[4] M. Bucher, S. Herbin, and F. Jurie. Improving semantic em-
bedding consistency by metric learning for zero-shot classif-
ﬁcation. In ECCV, 2016. 1, 7

[5] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha. Synthe-
sized classiﬁers for zero-shot learning. In CVPR, 2016. 1, 7,
8

[6] W.-L. Chao, S. Changpinyo, B. Gong, and F. Sha. An empir-
ical study and analysis of generalized zero-shot learning for
object recognition in the wild. In ECCV, 2016. 1

[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 1, 2

[8] G. Dinu, A. Lazaridou, and M. Baroni. Improving zero-shot
learning by mitigating the hubness problem. In ICLR work-
shop, 2014. 2, 3

[9] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing

objects by their attributes. In CVPR, 2009. 1, 2

[10] V. Ferrari and A. Zisserman. Learning visual attributes. In

NIPS, 2007. 1, 2

[11] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean,
T. Mikolov, et al. Devise: A deep visual-semantic embed-
ding model. In NIPS, 2013. 1, 2, 3, 5, 6, 7, 8

[12] Y. Fu, T. M. Hospedales, T. Xiang, Z. Fu, and S. Gong.
Transductive multi-view embedding for zero-shot recogni-
tion and annotation. In ECCV, 2014. 1, 2, 6, 7

[13] Y. Fu, T. M. Hospedales, T. Xiang, and S. Gong. Transduc-
tive multi-view zero-shot learning. PAMI, 2015. 6, 7
[14] Y. Fu and L. Sigal. Semi-supervised vocabulary-informed

learning. In CVPR, 2016. 1, 2, 6, 7, 8

[15] Z. Fu, T. Xiang, E. Kodirov, and S. Gong. Zero-shot object
recognition by semantic manifold distance. In CVPR, 2015.
1, 2, 6, 7, 8

[16] A. Graves, N. Jaitly, and A.-r. Mohamed. Hybrid speech
recognition with deep bidirectional lstm. In ASRU, 2013. 4
[17] A. Graves, A. Mohamed, and G. Hinton. Speech recognition

with deep recurrent neural networks. In ICASSP, 2013. 4

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 6

[19] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 1997. 4

[20] C. Huang, C. C. Loy, and X. Tang. Local similarity-aware

deep feature embedding. In NIPS, 2016. 7, 8

[21] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 6, 7

[22] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. In ICLR, 2015. 6

[23] E. Kodirov, T. Xiang, and S. Gong. Semantic autoencoder

for zero-shot learning. In CVPR, 2017. 8
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 3, 6, 7

Imagenet
In

[25] C. H. Lampert, H. Nickisch, and S. Harmeling. Attribute-
based classiﬁcation for zero-shot visual object categoriza-
tion. PAMI, 2014. 1, 2, 6, 8

[26] Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. Ef-
In Neural networks: Tricks of the trade,

ﬁcient backprop.
2012. 4

[27] J. Lei Ba, K. Swersky, S. Fidler, and R. Salakhutdinov. Pre-
dicting deep zero-shot convolutional neural networks using
textual descriptions. In ICCV, 2015. 1, 2, 3, 5, 6, 7

[28] L. v. d. Maaten and G. Hinton. Visualizing data using t-SNE.

JMLR, 2008. 9

[29] B. Marco, L. Angeliki, and D. Georgiana. Hubness and
pollution: Delving into cross-space mapping for zero-shot
learning. In ACL, 2015. 3

[30] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric
learning for large scale image classiﬁcation: Generalizing to
new classes at near-zero cost. In ECCV, 2012. 6, 8

[31] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
estimation of word representations in vector space. In arXiv
preprint arXiv:1301.3781, 2013. 6

[32] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and phrases
and their compositionality. In NIPS, 2013. 6

[33] T. Mukherjee and T. Hospedales. Gaussian visual-linguistic
embedding for zero-shot recognition. In EMNLP, 2016. 7, 8
[34] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens,
A. Frome, G. S. Corrado, and J. Dean. Zero-shot learning
by convex combination of semantic embeddings. In ICLR,
2014. 1, 6, 8

[35] D. Parikh and K. Grauman. Relative attributes.

In ICCV,

2011. 1, 2

[36] M. Radovanovi´c, A. Nanopoulos, and M. Ivanovi´c. Hubs in
space: Popular nearest neighbors in high-dimensional data.
JMLR, 2010. 2, 5

[37] S. Reed, Z. Akata, B. Schiele, and H. Lee. Learning deep
representations of ﬁne-grained visual descriptions. In CVPR,
2016. 1, 2, 3, 4, 6, 7

[38] M. Rohrbach, S. Ebert, and B. Schiele. Transfer learning in

a transductive setting. In NIPS, 2013. 8

[39] M. Rohrbach, M. Stark, and B. Schiele. Evaluating knowl-
edge transfer and zero-shot learning in a large-scale setting.
In CVPR, 2011. 8

[40] B. Romera-Paredes and P. Torr. An embarrassingly simple
approach to zero-shot learning. In ICML, 2015. 1, 2, 7, 8
[41] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. IJCV, 2015. 6

[42] M. Schuster and K. K. Paliwal. Bidirectional recurrent neural
networks. IEEE Transactions on Signal Processing, 1997. 4
[43] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,
and Y. LeCun. Overfeat: Integrated recognition, localization

and detection using convolutional networks. arXiv preprint
arXiv:1312.6229, 2013. 6, 7

[44] Y. Shigeto, I. Suzuki, K. Hara, M. Shimbo, and Y. Mat-
sumoto. Ridge regression, hubness, and zero-shot learning.
In ECML/PKDD, 2015. 3, 5

[45] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 6, 7

[46] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-shot
learning through cross-modal transfer. In NIPS, 2013. 1, 2,
3, 5, 6, 7, 8

[47] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M.
Hospedales. Learning to compare: Relation network for few-
shot learning. In CVPR, 2018. 2, 8

[48] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 6, 7
[49] C. Wah, S. Branson, P. Perona, and S. Belongie. Multiclass
recognition and part localization with humans in the loop. In
ICCV, 2011. 2, 6

[50] Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, and
B. Schiele. Latent embeddings for zero-shot classiﬁcation.
In CVPR, 2016. 8

[51] Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata. Zero-
shot learning-a comprehensive evaluation of the good, the
bad and the ugly. arXiv preprint arXiv:1707.00600, 2017. 5,
6, 7

[52] Y. Yang and T. M. Hospedales. A uniﬁed perspective on
multi-domain and multi-task learning. In ICLR, 2015. 1, 2,
3, 5, 6, 7

[53] L. Zhang, F. Sung, F. Liu, T. Xiang, S. Gong, Y. Yang, and
T. M. Hospedales. Actor-critic sequence training for image
captioning. In NeurIPS Workshop on Visually-Grounded In-
teraction and Language, 2017. 2

[54] Z. Zhang and V. Saligrama. Zero-shot learning via semantic

similarity embedding. In ICCV, 2015. 1, 2, 7, 8

[55] Z. Zhang and V. Saligrama. Zero-shot learning via joint la-

tent similarity embedding. In CVPR, 2016. 1, 7

[56] Z. Zhang and V. Saligrama. Zero-shot recognition via struc-

tured prediction. In ECCV, 2016. 1, 7

Learning a Deep Embedding Model for Zero-Shot Learning

Li Zhang

Tao Xiang

Shaogang Gong

Queen Mary University of London
{david.lizhang, t.xiang, s.gong}@qmul.ac.uk

9
1
0
2
 
l
u
J
 
9
1
 
 
]

V
C
.
s
c
[
 
 
4
v
8
8
0
5
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

Zero-shot learning (ZSL) models rely on learning a joint
embedding space where both textual/semantic description
of object classes and visual representation of object images
can be projected to for nearest neighbour search. Despite
the success of deep neural networks that learn an end-to-
end model between text and images in other vision problems
such as image captioning, very few deep ZSL model exists
and they show little advantage over ZSL models that utilise
deep feature representations but do not learn an end-to-end
embedding.
In this paper we argue that the key to make
deep ZSL models succeed is to choose the right embedding
space. Instead of embedding into a semantic space or an
intermediate space, we propose to use the visual space as
the embedding space. This is because that in this space,
the subsequent nearest neighbour search would suffer much
less from the hubness problem and thus become more effec-
tive. This model design also provides a natural mechanism
for multiple semantic modalities (e.g., attributes and sen-
tence descriptions) to be fused and optimised jointly in an
end-to-end manner. Extensive experiments on four bench-
marks show that our model signiﬁcantly outperforms the ex-
isting models. Code is available at: https://github.
com/lzrobots/DeepEmbeddingModel_ZSL

1. Introduction

A recent trend in developing visual recognition models is
to scale up the number of object categories. However, most
existing recognition models are based on supervised learn-
ing and require a large amount (at least 100s) of training
samples to be collected and annotated for each object class
to capture its intra-class appearance variations [7]. This
severely limits their scalability – collecting daily objects
such as chair is easier, but many other categories are rare
(e.g., a newly identiﬁed specie of beetle on a remote paciﬁc
island). None of these models can work with few or even
no training samples for a given class. In contrast, humans
are very good at recognising objects without seeing any vi-
sual samples, i.e., zero-shot learning (ZSL). For example, a

child would have no problem recognising a zebra if she has
seen horses before and also read elsewhere that a zebra is
a horse but with black-and-white stripes on it. Inspired by
humans’ ZSL ability, recently there is a surge of interest in
machine ZSL [3, 54, 25, 1, 40, 46, 11, 34, 12, 15, 27, 52,
37, 5, 14, 4, 6, 55, 56].

A zero-shot learning method relies on the existence of
a labelled training set of seen classes and the knowledge
about how an unseen class is semantically related to the
seen classes. Seen and unseen classes are usually related
in a high dimensional vector space, called semantic space,
where the knowledge from seen classes can be transferred
to unseen classes. The semantic spaces used by most early
works are based on semantic attributes [9, 10, 35]. Given
a deﬁned attribute ontology, each class name can be repre-
sented by an attribute vector and termed as a class prototype.
More recently, semantic word vector space [46, 11] and sen-
tence descriptions/captions [37] have started to gain popu-
larity. With the former, the class names are projected into a
word vector space so that different classes can be compared,
whilst with the latter, a neural language model is required to
provide a vector representation of the description.

With the semantic space and a visual feature representa-
tion of image content, ZSL is typically solved in two steps:
(1) A joint embedding space is learned where both the se-
mantic vectors (prototypes) and the visual feature vectors
can be projected to; and (2) nearest neighbour (NN) search
is performed in this embedding space to match the pro-
jection of an image feature vector against that of an un-
seen class prototype. Most state-of-the-arts ZSL models
[12, 14, 3, 4, 40, 54, 25] use deep CNN features for vi-
sual feature representation; the features are extracted with
pretrained CNN models. They differ mainly in how to learn
the embedding space given the features. They are thus not
end-to-end deep learning models.

In this paper, we focus on end-to-end learning of a deep
embedding based ZSL model which offers a number of
advantages. First, end-to-end optimisation can potentially
lead to learning a better embedding space. For example,
if sentence descriptions are used as the input to a neural
language model such as recurrent neural networks (RNNs)

for computing a semantic space, both the neural language
model and the CNN visual feature representation learning
model can be jointly optimised in an end-to-end fashion.
Second, a neural network based joint embedding model
offers the ﬂexibility for addressing various transfer learn-
ing problems such as multi-task learning and multi-domain
learning [52]. Third, when multiple semantic spaces are
available, this model can provide a natural mechanism for
fusing the multiple modalities. However, despite all these
intrinsic advantages, in practice, the few existing end-to-end
deep models for ZSL in the literature [27, 11, 46, 52, 37]
fail to demonstrate these advantages and yield only weaker
or merely comparable performances on benchmarks when
compared to non-deep learning alternatives.

We argue that the key to the success of a deep embed-
ding model for ZSL is the choice of the embedding space.
Existing models, regardless whether they are deep or non-
deep, choose either the semantic space [25, 14, 46, 11] or an
intermediate embedding space [27, 3, 40, 12] as the embed-
ding space. However, since the embedding space is of high
dimension and NN search is to be performed there, the hub-
ness problem is inevitable [36], that is, a few unseen class
prototypes will become the NNs of many data points, i.e.,
hubs. Using the semantic space as the embedding space
means that the visual feature vectors need to be projected
into the semantic space which will shrink the variance of the
projected data points and thus aggravate the hubness prob-
lem [36, 8].

In this work, we propose a novel Deep neural network
based Mmbedding Model (DEM) for ZSL which differs
from existing models in that: (1) To alleviate the hubness
problem, we use the output visual feature space of a CNN
subnet as the embedding space. The resulting projection di-
rection is from a semantic space, e.g., attribute or word vec-
tor, to a visual feature space. Such a direction is opposite to
the one adopted by most existing models. We provide a the-
oretical analysis and some intuitive visualisations to explain
why this would help us counter the hubness problem. (2) A
simple yet effective multi-modality fusion method is devel-
oped in our neural network model which is ﬂexible and im-
portantly enables end-to-end learning of the semantic space
representation.

The contributions of this work are as follows: (i) A novel
deep embedding model for ZSL has been formulated which
differs from existing models in the selection of embedding
space. (ii) A multi-modality fusion method is further de-
veloped to combine different semantic representations and
to enable end-to-end learning of the representations. Exten-
sive experiments carried out on four benchmarks including
AwA [25], CUB [49] and large scale ILSVRC 2010 and
ILSVRC 2012 [7] show that our model beats all the state-
of-the-art models presented to date, often by a large margin.

2. Related Work

Semantic space
Existing ZSL methods differ in what se-
mantic spaces are used: typically either attribute [9, 10, 35,
47], word vector [46, 11], or text description [37, 53]. It
has been shown that an attribute space is often more ef-
fective than a word vector space [3, 54, 25, 40]. This is
hardly surprising as additional attribute annotations are re-
quired for each class. Similarly, state-of-the-art results on
ﬁne-grained recognition tasks have been achieved in [37]
using image sentence descriptions to construct the semantic
space. Again, the good performance is obtained at the price
of more manual annotation: 10 sentence descriptions need
to be collected for each image, which is even more expen-
sive than attribute annotation. This is why the word vector
semantic space is still attractive: it is ‘free’ and is the only
choice for large scale recognition with many unseen classes
[14]. In this work, all three semantic spaces are considered.
Fusing multiple semantic spaces
Multiple semantic
spaces are often complementary to each other; fusing them
thus can potentially lead to improvements in recognition
performance. Score-level fusion is perhaps the simplest
strategy [15]. More sophisticated multi-view embedding
models have been proposed. Akata et al. [3] learn a joint
embedding semantic space between attribute, text and hier-
archical relationship which relies heavily on hyperparame-
ter search. Multi-view canonical correlation analysis (CCA)
has also been employed [12] to explore different modali-
ties of testing data in a transductive way. Differing from
these models, our neural network based model has an em-
bedding layer to fuse different semantic spaces and connect
the fused representation with the rest of the visual-semantic
embedding network for end-to-end learning. Unlike [12], it
is inductive and does not require to access the whole test set
at once.
Embedding model
Existing methods also differ in the
visual-semantic embedding model used. They can be cate-
gorised into two groups: (1) The ﬁrst group learns a map-
ping function by regression from the visual feature space to
the semantic space with pre-computed features [25, 14] or
deep neural network regression [46, 11]. For these embed-
ding models, the semantic space is the embedding space.
(2) The second group of models implicitly learn the rela-
tionship between the visual and semantic space through a
common intermediate space, again either with a neural net-
work formulation [27, 52] or without [27, 3, 40, 12]. The
embedding space is thus neither the visual feature space,
nor the semantic space. We show in this work that using
the visual feature space as the embedding space is intrinsi-
cally advantageous due to its ability to alleviate the hubness
problem.
Deep ZSL model All recent ZSL models use deep CNN
features as inputs to their embedding model. However, few
are deep end-to-end models. Existing deep neural network

based ZSL works [11, 46, 27, 52, 37] differ in whether they
use the semantic space or an intermediate space as the em-
bedding space, as mentioned above. They also use different
losses. Some of them use margin-based losses [11, 52, 37].
Socher et al [46] choose a euclidean distance loss. Ba et al
[27] takes a dot product between the embedded visual fea-
ture and semantic vectors and consider three training losses,
including a binary cross entropy loss, hinge loss and Eu-
clidean distance loss. In our model, we ﬁnd that the least
square loss between the two embedded vectors is very effec-
tive and offers an easy theoretical justiﬁcation as for why it
copes with the hubness problem better. The work in [37]
differs from the other models in that it integrates a neu-
ral language model into its neural network for end-to-end
learning of the embedding space as well as the language
model.
In additional to the ability of jointly learning the
neural language model and embedding model, our model
is capable of fusing text description with other semantic
spaces and achieves better performance than [37].
The hubness problem The phenomenon of the presence
of ‘universal’ neighbours, or hubs, in a high-dimensional
space for nearest neighbour search was ﬁrst studied by
Radovanovic et al. [29]. They show that hubness is an inher-
ent property of data distributions in a high-dimensional vec-
tor space, and a speciﬁc aspect of the curse of dimension-
ality. A couple of recent studies [8, 44] noted that regres-
sion based zero-shot learning methods suffer from the hub-
ness problem and proposed solutions to mitigate the hub-
ness problem. Among them, the method in [8] relies on
the modelling of the global distribution of test unseen data
ranks w.r.t. each class prototypes to ease the hubness prob-
lem. It is thus transductive. In contrast, the method in [44] is
inductive: It argued that least square regularised projection
functions make the hubness problem worse and proposed
to perform reverse regression, i.e., embedding class proto-
types into the visual feature space. Our model also uses the
visual feature space as the embedding space but achieve so
by using an end-to-end deep neural network which yields
far superior performance on ZSL.

3. Methodology

3.1. Problem deﬁnition

i , tu

Assume a labelled training set of N training samples is
given as Dtr = {(Ii, yu
i ), i = 1, . . . , N }, with associ-
ated class label set Ttr, where Ii is the i-th training image,
i ∈ RL×1 is its corresponding L-dimensional semantic
yu
representation vector, tu
i ∈ Ttr is the u-th training class la-
bel for the i-th training image. Given a new test image Ij,
the goal of ZSL is to predict a class label tv
j ∈ Tte, where tv
j
is the v-th test class label for the j-th test instance. We have
Ttr ∩ Tte = ∅, i.e., the training (seen) classes and test (un-
seen) classes are disjoint. Note that each class label tu or tv

is associated with a pre-deﬁned semantic space representa-
tion yu or yv (e.g. attribute vector), referred to as semantic
class prototypes. For the training set, yu
is given because
i
each training image Ii is labelled by a semantic representa-
tion vector representing its corresponding class label tu
j .

3.2. Model architecture

The architecture of our model is shown in Fig. 1. It has
two branches. One branch is the visual encoding branch,
which consists of a CNN subnet that takes an image Ii as
input and outputs a D-dimensional feature vector φ(Ii) ∈
RD×1. This D-dimensional visual feature space will be
used as the embedding space where both the image con-
tent and the semantic representation of the class that the
image belongs to will be embedded. The semantic em-
bedding is achieved by the other branch which is a seman-
tic encoding subnet. Speciﬁcally, it takes a L-dimensional
semantic representation vector of the corresponding class
yu
i as input, and after going through two fully connected
(FC) linear + Rectiﬁed Linear Unit (ReLU) layers outputs
a D-dimensional semantic embedding vector. Each of the
FC layer has a l2 parameter regularisation loss. The two
branches are linked together by a least square embedding
loss which aims to minimise the discrepancy between the
visual feature φ(Ii) and its class representation embedding
vector in the visual feature space. With the three losses, our
objective function is as follows:

L(W1, W2) =

||φ(Ii) − f1(W2f1(W1yu

i ))||2

1
N

N
(cid:88)

i=1

+λ(||W1||2 + ||W2||2)

(1)

where W1 ∈ RL×M are the weights to be learned in the
ﬁrst FC layer and W2 ∈ RM ×D for the second FC layer.
λ is the hyperparameter weighting the strengths of the two
parameter regularisation losses against the embedding loss.
We set f1((cid:5)) to be the Rectiﬁed Linear Unit (ReLU) which
introduces nonlinearity in the encoding subnet [24].

After that, the classiﬁcation of the test image Ij in the
visual feature space can be achieved by simply calculating
its distance to the embed prototypes:

v = arg min

D(φ(Ij), f1(W2f1(W1yv)))

(2)

v

where D is a distance function, and yv is the semantic space
vector of the v-th test class prototype.

3.3. Multiple semantic space fusion

As shown in Fig. 1, we can consider the semantic rep-
resentation and the ﬁrst FC and ReLU layer together as a

Figure 1: Illustration of the network architecture of our deep embedding model. The detailed architecture of the semantic
representation unit in the left branch (semantic encoding subnet) is given in (a), (b) and (c) which correspond to the single
modality (semantic space) case, the multiple (two) modality case, and the case where one of the modalities is text description.
For the case in (c), the semantic representation itself is a neural network (RNN) which is learned end-to-end with the rest of
the network.

semantic representation unit. When there is only one se-
mantic space considered, it is illustrated in Fig. 1(a). How-
ever, when more than one semantic spaces are used, e.g., we
want to fuse attribute vector with word vector for semantic
representation of classes, the structure of the semantic rep-
resentation unit is changed slightly, as shown in Fig. 1(b).

More speciﬁcally, we map different semantic representa-
tion vectors to a multi-modal fusion layer/space where they
are added. The output of the semantic representation unit
thus becomes:

1

· yu2

f2(W(1)
· yu1
1
i ∈ RL1×1 and yu2

i + W(2)
i ),
(3)
where yu1
i ∈ RL2×1 denote two differ-
ent semantic space representations (e.g., attribute and word
vector), “+” denotes element-wise sum, W(1)
1 ∈ RL1×M
and W(2)
1 ∈ RL2×M are the weights which will be learned.
f2((cid:5)) is the element-wise scaled hyperbolic tangent func-
tion [26]:

f2(x) = 1.7159 · tanh(

x).

(4)

2
3

This activation function forces the gradient into the most
non-linear value range and leads to a faster training process
than the basic hyperbolic tangent function.

3.4. Bidirectional LSTM encoder for description

The structure of the semantic representation unit needs
to be changed again, when text description is avalialbe for
each training image (see Fig. 1(c)). In this work, we use
a recurrent neural network (RNN) to encode the content of
a text description (a variable length sentence) into a ﬁxed-
length semantic vector. Speciﬁcally, given a text descrip-
tion of T words, x = (x1, . . . , xT ) we use a Bidirectional

RNN model [42] to encode them. For the RNN cell, the
Long-Shot Term Memory (LSTM) [19] units are used as
the recurrent units. The LSTM is a special kind of RNN,
which introduces the concept of gating to control the mes-
sage passing between different times steps. In this way, it
could potentially model long term dependencies. Following
[17], the model has two types of states to keep track of the
historical records: a cell state c and a hidden state h. For a
particular time step t, they are computed by integrating the
current inputs xt and previous state (ct−1, ht−1). During
the integrating, three types of gates are used to control the
messaging passing: an input gate it, a forget gate ft and an
output gate ot.

We omit the formulation of the bidirectional LSTM here
and refer the readers to [17, 16] for details. With the bidirec-
tional LSTM model, we use the ﬁnal output as our encoded
semantic feature vector to represent the text description:

f (W−→
h

·

−→
h + W←−
h

←−
h ),

·

(5)

−→
h denote the forward ﬁnal hidden state,

←−
h denote
where
the backward ﬁnal hidden state. f ((cid:5)) = f1((cid:5)) if text descrip-
tion is used only for semantic space unit, and f ((cid:5)) = f2((cid:5))
if other semantic space need to be fused (Sec. 3.3). W−→
h
and W←−

h are the weights which will be learned.

In the testing stage, we ﬁrst extract text encoding from
test descriptions and then average them per-class to form the
test prototypes as in [37]. Note that since our ZSL model
is a neural network, it is possible now to learn the RNN
encoding subnet using the training data together with the
rest of the network in an end-to-end fashion.

||A(cid:62)(AA(cid:62) + λI)−1A||2 =

≤ 1.

(8)

3.6. Relationship to other deep ZSL models

σ2
σ2 + λ

3.5. The hubness problem

How does our model deal with the hubness problem?
First we show that our objective function is closely related
to that of the ridge regression formulation. In particular, if
we use the matrix form and write the outputs of the semantic
representation unit as A and the outputs of the CNN visual
feature encoder as B, and ignore the ReLU unit for now,
our training objective becomes

L(W) = ||B − WA||2

F + λ||W||2
F ,

(6)

which is basically ridge regression.
It is well known
that ridge regression has a closed-form solution W =
BA(cid:62)(AA(cid:62) + λI)−1. Thus we have:

||WA||2 = ||BA(cid:62)(AA(cid:62) + λI)−1A||2
≤ ||B||2||A(cid:62)(AA(cid:62) + λI)−1A||2

(7)

It can be further shown that

Where σ is the largest singular value of A. So we have
||WA||2 ≤ ||B||2. This means the mapped source data
||WA||2 are likely to be closer to the origin of the space
than the target data ||B||2, with a smaller variance.

(a) S → V

(b) V → S

Figure 2: Illustration of the effects of different embedding
directions on the hubness problem. S: semantic space, and
V: visual feature space. Better viewed in colour.

Why does this matter in the context of ZSL? Figure 2
gives an intuitive explanation. Speciﬁcally, assuming the
feature distribution is uniform in the visual feature space,
Fig. 2(a) shows that if the projected class prototypes are
slightly shrunk towards the origin, it would not change how
hubness problem arises – in other words, it at least does not
make the hubness issue worse. However, if the mapping di-
rection were to be reversed, that is, we use the semantic vec-
tor space as the embedding space and project the visual fea-
ture vectors φ(I) into the space, the training objective is still
ridge regression-like, so the projected visual feature repre-
sentation vectors will be shrunk towards the origin as shown

in Fig. 2(b). Then there is an adverse effect: the semantic
vectors which are closer to the origin are more likely to be-
come hubs, i.e. nearest neighbours to many projected visual
feature representation vectors. This is conﬁrmed by our ex-
periments (see Sec. 4) which show that using which space
as the embedding space makes a big difference in terms of
the degree/seriousness of the resultant hubness problem and
therefore the ZSL performance.
Measure of hubness To measure the degree of hubness
in a nearest neighbour search problem, the skewness of the
(empirical) Nk distribution is used, following [36, 44]. The
Nk distribution is the distribution of the number Nk(i) of
times each prototype i is found in the top k of the rank-
ing for test samples (i.e. their k-nearest neighbour), and its
skewness is deﬁned as follows:

(Nkskewness) =

(cid:80)l

i=1(Nk(i) − E[Nk])3/l
V ar[Nk] 3

2

,

(9)

where l is the total number of test prototypes. A large skew-
ness value indicates the emergence of more hubs.

Let’s now compare the proposed model with the related
end-to-end neural network based models: DeViSE [11],
Socher et al. [46], MTMDL [52], and Ba et al. [27]. Their
model structures fall into two groups. In the ﬁrst group (see
Fig. 3(a)), DeViSE [11] and Socher et al. [46] map the CNN
visual feature vector to a semantic space by a hinge ranking
loss or least square loss.
In contrast, MTMDL [52] and
Ba et al. [27] fuse visual space and semantic space to a
common intermediate space and then use a hinge ranking
loss or a binary cross entropy loss (see Fig. 3(b)). For both
groups, the learned embedding model will make the vari-
ance of WA to be smaller than that of B, which would thus
make the hubness problem worse. In summary, the hubness
will persist regardless what embedding model is adopted, as
long as NN search is conducted in a high dimensional space.
Our model does not worsen it, whist other deep models do,
which leads to the performance difference as demonstrated
in our experiments.

4. Experiments

4.1. Dataset and settings

We follow two ZSL settings: the old setting and the new
GBU setting provided by [51] for training/test splits. Un-
der the old setting, adopted by most existing ZSL works
before [51], some of the test classes also appear in the Im-
ageNet 1K classes, which have been used to pretrain the
image embedding network, thus violating the zero-shot as-
In contrast, the new GBU setting ensures that
sumption.
none of the test classes of the datasets appear in the Im-
ageNet 1K classes. Under both settings, the test set can

Model setting and training Unless otherwise speciﬁed,
We use the Inception-V2 [48, 21] as the CNN subnet in the
old and conventional setting, and ResNet101 [18] for the
GBU and generalised setting, taking the top pooling units
as image embedding with dimension D = 1024 and 2048
respectively. The CNN subnet is pre-trained on ILSVRC
2012 1K classiﬁcation without ﬁne-tuning, the same as the
recent deep ZSL works [27, 37]. For fair comparison with
DeViSE [11], ConSE [34] and AMP [15] on ILSVRC 2010,
we also use the Alexnet [24] architecture and pretrain it
from scratch using the 800 training classes. All input im-
ages are resized to 224 × 224. Fully connected layers of
our model are initialised with random weights for all of our
experiments. Adam [22] is used to optimise our model with
a learning rate of 0.0001 and a minibatch size of 64. The
model is implemented based on Tensorﬂow.
Parameter setting
In the semantic encoding branch of
our network, the output size of the ﬁrst FC layer M is
set to 300 and 700 for AwA and CUB respectively when
a single semantic space is used (see Fig. 1(a)). Speciﬁ-
cally, we use one FC layer for ImageNet in our experiments.
For multiple semantic space fusion, the multi-modal fusion
layer output size is set to 900 (see Fig. 1(b)). When the
semantic representation was encoded from descriptions for
the CUB dataset, a bidirectional LSTM encoding subnet is
employed (see Fig. 1(c)). We use the BasicLSTMCell
in Tensorﬂow as our RNN cell and employ ReLU as acti-
vation function. We set the input sequence length to 30;
longer text inputs are cut off at this point and shorter ones
are zero-padded. The word embedding size and the number
of LSTM unit are both 512. Note that with this LSTM sub-
net, RMSprop is used in the place of Adam to optimise the
whole network with a learning rate of 0.0001, a minibatch
size of 64 and gradient clipped at 5. The loss weighting
factor λ in Eq. (1) is searched by ﬁve-fold cross-validation.
Speciﬁcally, 20% of the seen classes in the training set are
used to form a validation set.

4.2. Experiments on small scale datasets

Competitors Numerous existing works reported results
on AwA and CUB these two relatively small-scale datasets
under old setting. Among them, only the most competitive
ones are selected for comparison due to space constraint.
The selected 13 can be categorised into the non-deep model
group and the deep model group. All the non-deep mod-
els use ImageNet pretrained CNN to extract visual features.
They differ in which CNN model is used: FO indicates that
overfeat [43] is used; FG for GoogLeNet [48]; and FV for
VGG net [45]. The second group are all neural network
based with a CNN subnet. For fair comparison, we imple-
ment the models in [11, 46, 52, 27] on AwA and CUB with
Inception-V2 as the CNN subnet as in our model and [37].
The compared methods also differ in the semantic spaces

(a) [11, 46]

(b) [52, 27]

Figure 3: The architectures of existing deep ZSL models
fall into two groups: (a) learning projection function ω from
visual feature space to semantic space; (b) learning an inter-
mediate space as embedding space.

comprise only the unseen class samples (conventional test
set setting) or a mixture of seen and unseen class samples.
The latter, termed generalised zero-shot learning (GZSL), is
more realistic in practice.
Four benchmarks are selected for the old set-
Datasets
ting: AwA (Animals with Attributes) [25] consists of
30,745 images of 50 classes. It has a ﬁxed split for eval-
uation with 40 training classes and 10 test classes. CUB
(CUB-200-2011) [49] contains 11,788 images of 200 bird
species. We use the same split as in [3] with 150 classes
for training and 50 disjoint classes for testing. ImageNet
(ILSVRC) 2010 1K [41] consists of 1,000 categories and
more than 1.2 million images. We use the same train-
ing/test split as [30, 11] which gives 800 classes for train-
ImageNet (ILSVRC)
ing and 200 classes for testing.
2012/2010: for this dataset, we use the same setting as [14],
that is, ILSVRC 2012 1K is used as the training seen
classes, while 360 classes in ILSVRC 2010 which do not
appear in ILSVRC 2012 are used as the test unseen classes.
Three datasets [51] are selected for GBU setting: AwA1,
AwA2 and CUB. The newly released AwA2 [51] consists of
37,322 images of 50 classes which is an extension of AwA
while AwA1 is same as AwA but under the GBU setting.
Semantic space For AwA, we use the continuous 85-
dimension class-level attributes provided in [25], which
have been used by all recent works. For the word vector
space, we use the 1,000 dimension word vectors provided
in [12, 13]. For CUB, continuous 312-dimension class-level
attributes and 10 descriptions per image provided in [37] are
used. For ILSVRC 2010 and ILSVRC 2012, we trained a
skip-gram language model [31, 32] on a corpus of 4.6M
Wikipedia documents to extract 1, 000D word vectors for
each class.

used. Attributes (A) are used by all methods; some also use
word vector (W) either as an alternative to attributes, or in
conjunction with attributes (A+W). For CUB, recently the
instance-level sentence descriptions (D) are used [37]. Note
that only inductive methods are considered. Some recent
methods [56, 12, 13] are tranductive in that they use all
test data at once for model training, which gives them a big
unfair advantage.
Comparative results on AwA under old setting
From
Table 1 we can make the following observations: (1) Our
model DEM achieves the best results either with attribute
or word vector. When both semantic spaces are used, our
result is further improved to 88.1%, which is 7.6% higher
(2) The perfor-
than the best result reported so far [55].
mance gap between our model to the existing neural net-
work based models are particularly striking.
In fact, the
four models [11, 46, 52, 27] achieve weaker results than
most of the compared non-deep models that use deep fea-
tures only and do not perform end-to-end training. This ver-
ify our claim that selecting the appropriate visual-semantic
embedding space is critical for the deep embedding models
to work. (3) As expected, the word vector space is less in-
formative than the attribute space (86.7% vs. 78.8%) even
though our word vector space alone result already beats all
published results except for one [55]. Nevertheless, fusing
the two spaces still brings some improvement (1.4%).
Comparative results on CUB under old setting
Table
1 shows that on the ﬁne-grained dataset CUB, our model
also achieves the best result.
In particular, with attribute
only, our result of 58.3% is 3.8% higher than the strongest
competitor [5]. The best result reported so far, however,
was obtained by the neural network based DS-SJE [37] at
56.8% using sentence descriptions. It is worth pointing out
that this result was obtained using a word-CNN-RNN neu-
ral language model, whilst our model uses a bidirectional
LSTM subnet, which is easier to train end-to-end with the
rest of the network. When the same LSTM based neural
language model is used, DS-SJE reports a lower accuracy
of 53.0%. Further more, with attribute only, the result of
DS-SJE (50.4%) is much lower than ours. This is signiﬁ-
cant because annotating attributes for ﬁne-grained classes is
probably just about manageable; but annotating 10 descrip-
tions for each images is unlikely to scale to large number of
classes. It is also evident that fusing attribute with descrip-
tions leads to further improvement.
Comparative results under the GBU setting We fol-
low the evaluation setting of [51]. We compare our model
with 13 alternative ZSL models in Table 2. We can see
that on AwA1, AwA2 and aPY, the proposed model DEM
is particularly strong under the more realistic GZSL setting
measured using the harmonic mean (H) metric. In particu-
lar, DEM achieves state-of-the-art performance on AwA1,
AwA2 and SUN under conventional setting with 68.4%,

Model

AMP [15]
SJE [3]
SJE [3]
ESZSL [40]
SSE-ReLU [54]
JLSE [55]
SS-Voc [14]
SynC-struct [5]
SEC-ML [4]

DeViSE [11]
Socher et al. [46]
MTMDL [52]
Ba et al. [27]
DS-SJE [37]

DEM

DEM

F

FO
FG
FG
FG
FV
FV
FO
FG
FV

NG
NG
NG
NG
NG

NG

NG

SS

A+W
A
A+W
A
A
A
A/W
A
A

A/W
A/W
A/W
A/W
A/D

AwA

CUB

66.0
66.7
73.9
76.3
76.3
80.5
78.3/68.9
72.9
77.3

56.7/50.4
60.8/50.3
63.7/55.3
69.3/58.7
-

-
50.1
51.7
47.2
30.4
42.1
-
54.5
43.3

33.5
39.6
32.3
34.0
50.4/56.8

A/W(D)

86.7/78.8

58.3/53.5

A+W(D)

88.1

59.0

Table 1: Zero-shot classiﬁcation accuracy (%) comparison
on AwA and CUB (hit@1 accuracy over all samples) un-
der the old and conventional setting. SS: semantic space;
A: attribute space; W: semantic word vector space; D: sen-
tence description (only available for CUB). F: how the vi-
sual feature space is computed; For non-deep models: FO
if overfeat [43] is used; FG for GoogLeNet [48]; and FV
for VGG net [45]. For neural network based methods,
all use Inception-V2 (GoogLeNet with batch normalisa-
tion) [48, 21] as the CNN subnet, indicated as NG.

67.1% and 61.9%, outperforming alternatives by big mar-
gins.

4.3. Experiments on ImageNet

Comparative results on ILSVRC 2010
Compared to
AwA and CUB, far fewer works report results on the large-
scale ImageNet ZSL tasks. We compare our model against
8 alternatives on ILSVRC 2010 in Table 3, where we use
hit@5 rather than hit@1 accuracy as in the small dataset
experiments. Note that existing works follow two set-
tings. Some of them [33, 20] use existing CNN model
(e.g. VGG/GoogLeNet) pretrained from ILSVRC 2012 1K
classes to initialise their model or extract deep visual fea-
ture. Comparing to these two methods under the same set-
ting, our model gives 60.7%, which beats the nearest rival
PDDM [20] by over 12%. For comparing with the other 6
methods, we follow their setting and pretrain our CNN sub-
net from scratch with Alexnet [24] architecture using the
800 training classes for fair comparison. The results show
that again, signiﬁcant improvement has been obtained with
our model.
Comparative results on ILSVRC 2012/2010
Even
fewer published results on this dataset are available. Table 4
shows that our model clearly outperform the state-of-the-art

H

2
.
7

8
.
1

6
.
1
1

3
.
3
1

0
.
4

9
.
0
2

8
.
9
1

5
.
9
1

8
.
5
1

3
.
6
2

4
.
3
1

8
.
1
1

-

1
.
5
2

8
.
7
3

9
.
9
3

0
.
8
2

4
.
6
3

4
.
7
2

5
.
0
3

8
.
8
2

9
.
7
2

1
.
3
3

3
.
3
4

0
.
8
1

-

u

2
.
4

0
.
1

8
.
6

7
.
8

1
.
2

9
.
6
1

7
.
4
1

7
.
4
1

0
.
1
1

8
.
1
2

9
.
7

8
.
8

-

s

L
S
Z
G

N
U
S

L
S
Z

1
T

9
.
9
3

4
.
9
1

8
.
8
3

9
.
9
3

5
.
1
5

5
.
6
5

7
.
3
5

3
.
5
5

5
.
4
5

1
.
8
5

3
.
6
5

3
.
0
4

-

H

0
.
9

4
.
0
1

0
.
0

0
.
9
1

4
.
0

2
.
9

9
.
6

2
.
0

6
.
4

7
.
8

3
.
3
1

9
.
0

-

3
.
8
7

6
.
5
6

2
.
1
9

2
.
4
7

9
.
8
7

9
.
6
7

7
.
5
5

0
.
3
7

1
.
0
7

7
.
3
7

3
.
6
6

9
.
0
8

-

u

8
.
4

7
.
5

0
.
0

2
.
0

9
.
4

7
.
3

1
.
0

4
.
2

6
.
4

4
.
7

4
.
0

-

9
.
0
1

L
S
Z
G

s

Y
P
a

L
S
Z

1
T

8
.
3
3

6
.
6
3

9
.
6
2

0
.
8
2

0
.
4
3

8
.
9
3

9
.
2
3

2
.
5
3

3
.
8
3

7
.
9
3

9
.
3
2

3
.
8

-

6
.
5
2

3
.
4
3

5
.
0
2

9
.
1
6

4
.
9
1

1
.
5
7

1
.
1
1

0
.
5
3

H

3
.
3

4
.
0

1
.
3

7
.
8

4
.
4
1

8
.
2
3

6
.
3
3

0
.
4
2

0
.
1
2

4
.
4
3

8
.
9
1

6
.
3
1

0
.
7
4

2
.
9
2

9
.
7
6

8
.
2
7

2
.
2
7

1
.
0
6

9
.
6
4

0
.
3
5

2
.
9
5

3
.
7
5

8
.
3
6

8
.
2
6

9
.
0
7

0
.
4
5

1
.
1
6

9
.
7
5

u

7
.
1

2
.
0

6
.
1

7
.
4

5
.
8

8
.
3
2

5
.
3
2

2
.
5
1

6
.
2
1

7
.
3
2

5
.
1
1

8
.
7

1
.
8
3

6
.
9
1

L
S
Z
G

s

B
U
C

L
S
Z

1
T

0
.
0
4

0
.
4
2

3
.
4
3

6
.
4
3

9
.
3
4

0
.
2
5

9
.
3
5

3
.
9
4

9
.
3
5

9
.
4
5

6
.
5
5

3
.
3
3

6
.
5
5

7
.
1
5

H

0
.
0

8
.
1

0
.
1

9
.
5
1

8
.
4
1

8
.
7
2

4
.
4
1

0
.
0
2

0
.
1
1

9
.
3
2

0
.
8
1

2
.
2

3
.
5
4

1
.
5
4

7
.
4
8

6
.
7
8

6
.
0
9

0
.
9
8

5
.
2
8

7
.
4
7

9
.
3
7

3
.
7
7

8
.
7
7

8
.
1
8

5
.
0
9

2
.
2
8

4
.
3
9

4
.
6
8

u

0
.
0

9
.
0

5
.
0

7
.
8

1
.
8

1
.
7
1

0
.
8

5
.
1
1

9
.
5

0
.
4
1

0
.
0
1

1
.
1

0
.
0
3

5
.
0
3

L
S
Z
G

s

2
A
w
A

L
S
Z

1
T

1
.
6
4

9
.
5
3

5
.
4
4

9
.
7
3

0
.
1
6

7
.
9
5

9
.
1
6

8
.
5
5

6
.
8
5

5
.
2
6

6
.
6
4

1
.
4
5

2
.
4
6

1
.
7
6

H

0
.
0

1
.
4

8
.
0

3
.
5
1

9
.
2
1

4

.
2
2

6

.
9
1

3
.
3
1

1
.
2
1

5

.
7
2

2
.
6
1

5
.
3

7

.
6
4

3

.
7
4

7
.
8
8

2
.
8
7

6
.
8
8

9
.
6
8

5
.
0
8

7
.
8
6

6
.
4
7

7
.
1
7

6
.
5
7

1
.
6
7

3
.
7
8

1
.
7
7

3
.
1
9

7
.
4
8

u

0
.
0

1
.
2

4
.
0

4
.
8

0
.
7

4
.
3
1

3
.
1
1

3
.
7

6
.
6

8
.
6
1

9
.
8

8
.
1

4
.
1
3

8
.
2
3

L
S
Z
G

s

1
A
w
A

L
S
Z

1
T

1
.
4
4

9
.
5
3

6
.
5
4

5
.
9
3

1
.
0
6

2
.
4
5

6
.
5
6

1
.
5
5

2
.
8
5

9
.
9
5

0
.
4
5

0
.
3
5

2
.
8
6

4
.
8
6

]
4
3
[
E
S
n
o
C

]
6
4
[
T
M
C

]
4
5
[
E
S
S

]
1
1
[
E
S
i
V
e
D

]
3
[
E
J
S

]
0
5
[

M
E
T
A
L

]
0
4
[
L
S
Z
S
E

]
5
2
[
P
A
D

]
5
2
[
P
A

I

]
5
[

C
N
Y
S

]
3
2
[
E
A
S

]
2
[
E
L
A

l
e
d
o
M

:
2
e
l
b
a
T

M
E
D

]
7
4
[

t
e
N
n
o
i
t
a
l
e
R

n
o
1
T
=
u
g
n
i
s
u
d
e
r
u
s
a
e
m
s
i

t
i

,

L
S
Z
G

r
e
d
n
u
d
n
a

,
)

%

(
y
c
a
r
u
c
c
a

)
1
T

(
1
-
p
o
T
e
g
a
r
e
v
a

s
s
a
l
c
-
r
e
p
g
n
i
s
u
d
e
t
a
u
l
a
v
e

s
i

e
c
n
a
m
r
o
f
r
e
p
e
h
t

,
g
n
i
t
t
e
s
L
S
Z

t
a
h
t

r
e
d
n
U

.
s
t
e
s
a
t
a
d
r
u
o
f
n
o
s
t
l
u
s
e
r

e
v
i
t
a
r
a
p
m
o
C

.
n
a
e
m
c
i
n
o
m
r
a
h
=
H
d
n
a

,
s
e
s
s
a
l
c

n
e
e
s

n
o

1
T
=
s

,
s
e
s
s
a
l
c

n
e
e
s
n
u

Model

hit@5

ConSE [34]
DeViSE [11]
Mensink et al. [30]
Rohrbach [39]
PST [38]
AMP [15]

Ours

DEM

Gaussian Embedding [33]
PDDM [20]

28.5
31.8
35.7
34.8
34.0
41.0

46.7

45.7
48.2

60.7

Table 3: Comparative results (%) on ILSVRC 2010 (hit@1
accuracy over all samples) under the old and conventional
setting.

alternatives by a large margin.

Model

hit@1

hit@5

ConSE [34]
DeViSE [11]
AMP [15]
SS-Voc [14]

DEM

7.8
5.2
6.1
9.5

11.0

15.5
12.8
13.1
16.8

25.7

Table 4: Comparative results (%) on ILSVRC 2012/2010
(hit@1 accuracy over all samples) under the old and con-
ventional setting.

4.4. Further analysis

Importance of embedding space selection We argued
that the key for an effective deep embedding model is the
use of the CNN output visual feature space rather than the
semantic space as the embedding space. In this experiment,
we modify our model in Fig. 1 by moving the two FC lay-
ers from the semantic embedding branch to the CNN feature
extraction branch so that the embedding space now becomes
the semantic space (attributes are used). Table 5 shows that
by mapping the visual features to the semantic embedding
space, the performance on AwA drops by 26.1% on AwA,
highlighting the importance of selecting the right embed-
ding space. We also hypothesised that using the CNN visual
feature space as the embedding layer would lead to less hub-
ness problem. To verify that we measure the hubness using
the skewness score (see Sec. 3.5). Table 6 shows clearly that
the hubness problem is much more severe when the wrong
embedding space is selected. We also plot the data distri-
bution of the 10 unseen classes of AwA together with the
prototypes. Figure 4 suggests that with the visual feature
space as the embedding space, the 10 classes form com-

(a) S → V

(b) V → S

Figure 4: Visualisation of the distribution of the 10 unseen class images in the two embedding spaces on AwA using t-
SNE [28]. Different classes as well as their corresponding class prototypes (in squares) are shown in different colours. Better
viewed in colour.

pact clusters and are near to their corresponding prototypes,
whilst in the semantic space, the data distributions of differ-
ent classes are much less separated and a few prototypes are
clearly hubs causing miss-classiﬁcation.

Model

AwA

CUB

Linear regression (V → S)
Linear regression (S → V)
DEM

54.0
74.8
86.7

40.7
45.7
58.3

Loss

Visual → Semantic

Semantic → Visual

Least square loss
Hinge loss

60.6
57.7

86.7
72.8

Table 7: Zero-shot classiﬁcation accuracy (%) comparison
with linear regression on AwA and CUB.

Table 5: Effects of selecting different embedding space and
different loss functions on zero-shot classiﬁcation accuracy
(%) on AwA.

N1 skewness

Visual → Semantic
Semantic → Visual

AwA

0.4162
-0.4834

CUB

8.2697
2.2594

5. Conclusion

Table 6: N1 skewness score on AwA and CUB with differ-
ent embedding space.

Neural network formulation Can we apply the idea of
using visual feature space as embedding space to other mod-
els? To answer this, we consider a very simple model based
on linear ridge regression which maps from the CNN fea-
ture space to the attribute semantic space or vice versa. In
Table 7, we can see that even for such a simple model, very
impressive results are obtained with the right choice of em-
bedding space. The results also show that with our neural
network based model, much better performance can be ob-
tained due to the introduced nonlinearity and its ability to
learn end-to-end.
Choices of the loss function As reviewed in Sec. 2, most
existing ZSL models use either margin based losses or bi-
nary cross entropy loss to learn the embedding model. In

this work, least square loss is used. Table 5 shows that
when the semantic space is used as the embedding space,
a slightly inferior result is obtained using a hinge ranking
loss in place of least square loss in our model. However,
least square loss is clearly better when the visual feature
space is the embedding space.

We have proposed a novel deep embedding model for
zero-shot learning. The model differs from existing ZSL
model in that it uses the CNN output feature space as the
embedding space. We hypothesise that this embedding
space would lead to less hubness problem compared to the
alternative selections of embedding space. Further more,
the proposed model offers the ﬂexible of utilising multiple
semantic spaces and is capable of end-to-end learning when
the semantic space itself is computed using a neural net-
work. Extensive experiments show that our model achieves
state-of-the-art performance on a number of benchmark
datasets and validate the hypothesis that selecting the cor-
rect embedding space is the key for achieving the excellent
performance.

Acknowledgement

This work was funded in part by the European FP7

Project SUNNY (grant agreement no. 313243).

References

[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-
embedding for attribute-based classiﬁcation. In CVPR, 2013.
1

[2] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-

embedding for image classiﬁcation. TPAMI, 2016. 8

[3] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele. Eval-
uation of output embeddings for ﬁne-grained image classiﬁ-
cation. In CVPR, 2015. 1, 2, 6, 7, 8

[4] M. Bucher, S. Herbin, and F. Jurie. Improving semantic em-
bedding consistency by metric learning for zero-shot classif-
ﬁcation. In ECCV, 2016. 1, 7

[5] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha. Synthe-
sized classiﬁers for zero-shot learning. In CVPR, 2016. 1, 7,
8

[6] W.-L. Chao, S. Changpinyo, B. Gong, and F. Sha. An empir-
ical study and analysis of generalized zero-shot learning for
object recognition in the wild. In ECCV, 2016. 1

[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 1, 2

[8] G. Dinu, A. Lazaridou, and M. Baroni. Improving zero-shot
learning by mitigating the hubness problem. In ICLR work-
shop, 2014. 2, 3

[9] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing

objects by their attributes. In CVPR, 2009. 1, 2

[10] V. Ferrari and A. Zisserman. Learning visual attributes. In

NIPS, 2007. 1, 2

[11] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean,
T. Mikolov, et al. Devise: A deep visual-semantic embed-
ding model. In NIPS, 2013. 1, 2, 3, 5, 6, 7, 8

[12] Y. Fu, T. M. Hospedales, T. Xiang, Z. Fu, and S. Gong.
Transductive multi-view embedding for zero-shot recogni-
tion and annotation. In ECCV, 2014. 1, 2, 6, 7

[13] Y. Fu, T. M. Hospedales, T. Xiang, and S. Gong. Transduc-
tive multi-view zero-shot learning. PAMI, 2015. 6, 7
[14] Y. Fu and L. Sigal. Semi-supervised vocabulary-informed

learning. In CVPR, 2016. 1, 2, 6, 7, 8

[15] Z. Fu, T. Xiang, E. Kodirov, and S. Gong. Zero-shot object
recognition by semantic manifold distance. In CVPR, 2015.
1, 2, 6, 7, 8

[16] A. Graves, N. Jaitly, and A.-r. Mohamed. Hybrid speech
recognition with deep bidirectional lstm. In ASRU, 2013. 4
[17] A. Graves, A. Mohamed, and G. Hinton. Speech recognition

with deep recurrent neural networks. In ICASSP, 2013. 4

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 6

[19] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 1997. 4

[20] C. Huang, C. C. Loy, and X. Tang. Local similarity-aware

deep feature embedding. In NIPS, 2016. 7, 8

[21] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 6, 7

[22] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. In ICLR, 2015. 6

[23] E. Kodirov, T. Xiang, and S. Gong. Semantic autoencoder

for zero-shot learning. In CVPR, 2017. 8
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 3, 6, 7

Imagenet
In

[25] C. H. Lampert, H. Nickisch, and S. Harmeling. Attribute-
based classiﬁcation for zero-shot visual object categoriza-
tion. PAMI, 2014. 1, 2, 6, 8

[26] Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. Ef-
In Neural networks: Tricks of the trade,

ﬁcient backprop.
2012. 4

[27] J. Lei Ba, K. Swersky, S. Fidler, and R. Salakhutdinov. Pre-
dicting deep zero-shot convolutional neural networks using
textual descriptions. In ICCV, 2015. 1, 2, 3, 5, 6, 7

[28] L. v. d. Maaten and G. Hinton. Visualizing data using t-SNE.

JMLR, 2008. 9

[29] B. Marco, L. Angeliki, and D. Georgiana. Hubness and
pollution: Delving into cross-space mapping for zero-shot
learning. In ACL, 2015. 3

[30] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric
learning for large scale image classiﬁcation: Generalizing to
new classes at near-zero cost. In ECCV, 2012. 6, 8

[31] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
estimation of word representations in vector space. In arXiv
preprint arXiv:1301.3781, 2013. 6

[32] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and phrases
and their compositionality. In NIPS, 2013. 6

[33] T. Mukherjee and T. Hospedales. Gaussian visual-linguistic
embedding for zero-shot recognition. In EMNLP, 2016. 7, 8
[34] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens,
A. Frome, G. S. Corrado, and J. Dean. Zero-shot learning
by convex combination of semantic embeddings. In ICLR,
2014. 1, 6, 8

[35] D. Parikh and K. Grauman. Relative attributes.

In ICCV,

2011. 1, 2

[36] M. Radovanovi´c, A. Nanopoulos, and M. Ivanovi´c. Hubs in
space: Popular nearest neighbors in high-dimensional data.
JMLR, 2010. 2, 5

[37] S. Reed, Z. Akata, B. Schiele, and H. Lee. Learning deep
representations of ﬁne-grained visual descriptions. In CVPR,
2016. 1, 2, 3, 4, 6, 7

[38] M. Rohrbach, S. Ebert, and B. Schiele. Transfer learning in

a transductive setting. In NIPS, 2013. 8

[39] M. Rohrbach, M. Stark, and B. Schiele. Evaluating knowl-
edge transfer and zero-shot learning in a large-scale setting.
In CVPR, 2011. 8

[40] B. Romera-Paredes and P. Torr. An embarrassingly simple
approach to zero-shot learning. In ICML, 2015. 1, 2, 7, 8
[41] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. IJCV, 2015. 6

[42] M. Schuster and K. K. Paliwal. Bidirectional recurrent neural
networks. IEEE Transactions on Signal Processing, 1997. 4
[43] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,
and Y. LeCun. Overfeat: Integrated recognition, localization

and detection using convolutional networks. arXiv preprint
arXiv:1312.6229, 2013. 6, 7

[44] Y. Shigeto, I. Suzuki, K. Hara, M. Shimbo, and Y. Mat-
sumoto. Ridge regression, hubness, and zero-shot learning.
In ECML/PKDD, 2015. 3, 5

[45] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 6, 7

[46] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-shot
learning through cross-modal transfer. In NIPS, 2013. 1, 2,
3, 5, 6, 7, 8

[47] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M.
Hospedales. Learning to compare: Relation network for few-
shot learning. In CVPR, 2018. 2, 8

[48] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 6, 7
[49] C. Wah, S. Branson, P. Perona, and S. Belongie. Multiclass
recognition and part localization with humans in the loop. In
ICCV, 2011. 2, 6

[50] Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, and
B. Schiele. Latent embeddings for zero-shot classiﬁcation.
In CVPR, 2016. 8

[51] Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata. Zero-
shot learning-a comprehensive evaluation of the good, the
bad and the ugly. arXiv preprint arXiv:1707.00600, 2017. 5,
6, 7

[52] Y. Yang and T. M. Hospedales. A uniﬁed perspective on
multi-domain and multi-task learning. In ICLR, 2015. 1, 2,
3, 5, 6, 7

[53] L. Zhang, F. Sung, F. Liu, T. Xiang, S. Gong, Y. Yang, and
T. M. Hospedales. Actor-critic sequence training for image
captioning. In NeurIPS Workshop on Visually-Grounded In-
teraction and Language, 2017. 2

[54] Z. Zhang and V. Saligrama. Zero-shot learning via semantic

similarity embedding. In ICCV, 2015. 1, 2, 7, 8

[55] Z. Zhang and V. Saligrama. Zero-shot learning via joint la-

tent similarity embedding. In CVPR, 2016. 1, 7

[56] Z. Zhang and V. Saligrama. Zero-shot recognition via struc-

tured prediction. In ECCV, 2016. 1, 7

