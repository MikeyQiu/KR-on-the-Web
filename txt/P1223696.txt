The World of Fast Moving Objects

Denys Rozumnyi1,3

Jan Kotˇera2

Filip ˇSroubek2

Luk´aˇs Novotn´y1

Jiˇr´ı Matas1

1CMP, Czech Technical University in Prague

2UTIA, Czech Academy of Sciences

3SGN, Tampere University of Technology

6
1
0
2
 
v
o
N
 
3
2
 
 
]

V
C
.
s
c
[
 
 
1
v
9
8
8
7
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

The notion of a Fast Moving Object (FMO), i.e. an ob-
ject that moves over a distance exceeding its size within the
exposure time, is introduced. FMOs may, and typically do,
rotate with high angular speed. FMOs are very common in
sports videos, but are not rare elsewhere. In a single frame,
such objects are often barely visible and appear as semi-
transparent streaks.

A method for the detection and tracking of FMOs is pro-
posed. The method consists of three distinct algorithms,
which form an efﬁcient localization pipeline that operates
successfully in a broad range of conditions. We show that
it is possible to recover the appearance of the object and its
axis of rotation, despite its blurred appearance. The pro-
posed method is evaluated on a new annotated dataset. The
results show that existing trackers are inadequate for the
problem of FMO localization and a new approach is re-
quired. Two applications of localization, temporal super-
resolution and highlighting, are presented.

1. Introduction

Object tracking has received enormous attention by the
computer vision community. Methods based on various
principles have been proposed and several surveys have
been compiled [2, 3, 11]. Standard benchmarks, some
comprising hundreds of videos, such as ALOV [22], VOT
[15, 16] and OTB [27] are available. Yet none of them in-
clude objects that are moving so fast that they appear as
streaks much larger than their size. This is a surprising
omission considering the fact that such objects commonly
appear in diverse real-world situations, in which sports play
undoubtedly a prominent role; see examples in Fig. 11.

To develop algorithms for detection and tracking of fast
moving objects, we had to collect and annotate a new
dataset. The substantial difference of the FMO dataset and
the standard ones was conﬁrmed by ex-post analysis of
inter-frame motion statistics. The most common overlap

1Fast moving objects are often poorly visible and for improved under-

standing, the reader is referred to videos in the supplementary material.

Figure 1: Fast moving objects appear as semi-transparent
streaks larger than their size. Examples (left-to-right, top-
to-bottom) from table tennis, archery, softball, tennis, hail-
storm and ﬁreworks.

of ground truth bounding boxes in two consecutive frames
is zero for the FMO set while it is close to one for ALOV,
OTB and VOT [22, 27, 15]. The speed of the tracked object
projected to image coordinates, measured as the distance
of object centers in two consecutive frames, is on average
ten times higher in the new dataset, see Fig. 2. Given the
difference in the properties of the sequences, it is not sur-
prising that state-of-the-art trackers designed for the classi-
cal problem do not perform well on the FMO dataset. The
two “worlds” are so different that on almost all sequences
the classical state-of-the-art methods fail completely, their
output bounding boxes achieving a 50% overlap with the
ground truth in zero frames, see Tab. 3.

In the paper, we propose an efﬁcient method for FMO
localization and a method for estimation of their extrinsic –
the trajectory and the axis and angular velocity of rotation,
and intrinsic properties – the size and color of the object.
In speciﬁc cases we can go further and estimate the full ap-

1

since the latent sharp map is a binary image. The same idea
applied to rotating objects was proposed in [21]. An inter-
esting variation was proposed in [7], where linear motion
blur is estimated locally using a relation similar to optical
ﬂow. The main drawback of these methods is that an accu-
rate estimation of the transparency map using alpha matting
algorithms [18] is necessary.

Methods exploiting the fact

that autocorrelation in-
creases in the direction of blur were proposed to deal with
objects moving over static backgrounds [5, 14]. Similarly
[19, 23], autocorrelation was considered for motion detec-
tion of the whole scene due to camera motion. However, all
these methods require a relatively large neighborhood to es-
timate blur parameters, which means that they are not suit-
able for small moving objects. Simultaneously dealing with
rotation of objects has not been considered in the literature
so far.

3. Problem deﬁnition

FMOs are objects that move over a large distance com-
pared to their size during the exposure time of a single
frame, and possibly also rotate along an arbitrary axis with
an unknown angular speed. For simplicity, we assume a
single object F moving over a static background B; an ex-
tension to multiple objects is relatively straightforward. To
get close to the static background state, camera motion is
assumed to be compensated by video stabilization.
Let a recorded video sequence consist of

frames
I1(x), . . . In(x), where x ∈ R2 is a pixel coordinate. Frame
It is formed as

It(x) = (1 − [HtM ](x))B(x) + [HtF ](x) ,

(1)

where M is the indicator function of F .
In general, the
operator Ht models the blur caused by object motion and
rotation, and performs the 3D→2D projection of the object
representation F onto the image plane. This operator de-
pends mainly on three parameters, {Pt, at, φt}, which are
the FMO trajectory (path), and the axis and angle of ro-
tation, respectively. The [HtM ](x) function corresponds
to the object visibility map (alpha matte, relative duration
of object presence during exposure) and appears in (1) to
merge the blurred object and the partially visible back-
ground.

The object trajectory Pt can be represented in the im-
age plane as a path (set of pixels) along which the object
moves during the frame exposure. In the case of no rotation
or when F is homogeneous, i.e. the surface is uniform and
thus rotation is not perceivable, Ht simpliﬁes to a convolu-
tion in the image plane, i.e. [HtF ](x) = 1
|Pt| [Pt ∗ F ](x),
where |Pt| is the path length – F can then be viewed as a
2D image.

Finding all the intrinsic and extrinsic properties of arbi-
trary FMOs means estimating both F and Ht, which is, at

object speed [pxl]

intersection over union

Figure 2: The FMO dataset includes motions that are an or-
der of magnitude faster than three standard datasets - ALOV,
VOT, OTB [22, 15, 27]. Normalized histograms of pro-
jected object speeds (left) and intersection over union IoU
of bounding boxes (right) between adjacent frames.

pearance model of the object. Properties like the rotation
axis, angular velocity and object appearance require precise
modeling of the image formation (acquisition) process. The
proposed method thus proceeds by solving a blind space-
variant deconvolution problem with occlusion.

Detection,

tracking and appearance reconstruction of
FMOs allows performing tasks with applications in diverse
areas. We show, for instance, the ability to synthesize real-
istic videos with higher frame rates, i.e. to perform tempo-
ral super-resolution. The extracted properties of the FMO,
such as trajectory, rotation angle and velocity have applica-
tion, e.g. in sports analytics.

The rest of the paper is organized as follows: Related
work is discussed in Section 2. Section 3 deﬁnes the main
concepts arising in the problem. Section 4 explains in detail
the proposed method for FMO localization. The estimation
of intrinsic and extrinsic properties formulated as an opti-
mization problem is presented in Section 5. In Section 6,
the FMO annotated dataset of 16 videos is introduced. Last,
the method is evaluated and its different applications are
demonstrated in Section 7.

2. Related Work

Tracking is a key problem in video processing. A range
of methods has been proposed based on diverse principles,
such as correlation [4, 9, 8], feature point tracking [24],
mean-shift [6, 25], and tracking-by-detection [29, 12]. The
literature sometimes refer to fast moving objects, but only
the case with no signiﬁcant blur is considered, e.g. [28, 17].
Object blur is a cue for object motion, since the blur
size and shape encode information about motion. However,
classical tracking methods suffer from blur, yet FMOs con-
sist predominantly of blur. Most motion deblurring meth-
ods assume that the degradation can be modeled locally by
a linear motion. One category of methods works with oc-
clusion and considers the object’s blurred transparency map
[13]. Blind deconvolution of the transparency map is easier,

this moment, an intractable task. To alleviate this problem,
some prior knowledge of F is necessary. In our case, the
prior is in the form of object shape. Since in most sport
videos the FMOs are spheres (balls), we continue our the-
oretical analysis focusing on spherical objects, although, as
we further demonstrate, the proposed localization method
can also successfully handle objects of signiﬁcantly differ-
ent shapes.

We propose methods for two tasks: (i) efﬁcient and reli-
able FMO localization, i.e. detection and tracking, and (ii)
reconstruction of the FMO appearance, and the axis and an-
gle of the object rotation, which requires the precise output
of (i). For tracking, we use a simpliﬁed version of (1) and
approximate the FMO by a homogeneous circle determined
by two parameters: color µ and radius r. The tracker out-
put (trajectory Pt and radius r) is then used to initialize the
precise estimation of appearance using the full model (1).

4. Localization of FMOs

The proposed FMO localization pipeline consists of
three algorithms that differ in their prerequisites and speed.
First, the pipeline runs the fastest algorithm and terminates
if a fast moving object is localized; otherwise, it proceeds
to run the remaining two more complex and general al-
gorithms. This strategy produces an efﬁcient localization
method that operates successfully in a broader range of con-
ditions than either of the three algorithms alone. We call
them detector, re-detector, and tracker, and their basic prop-
erties are outlined in Tab. 1.

The ﬁrst algorithm, detector, discovers previously un-
seen FMOs and establishes their properties. It requires suf-
ﬁcient contrast between the object and the background, an
unoccluded view in three consecutive frames, and no inter-
ference with other moving objects. Then the FMO can be
tracked by either of the other two algorithms. The second
algorithm, re-detector, is applied in a region predicted by
the FMO trajectory in the previous frames. It handles the
problems of partial occlusions and object-background ap-
pearance similarity while being as fast as the detector. Fi-
nally, the tracker searches for the object by synthesizing its
appearance with the background at the predicted locations.
All three algorithms require a static background or a
registration of consecutive frames. To this end, we apply
video stabilization by estimating the afﬁne transformation
between frames using RANSAC [10] by matching FREAK
descriptors [1] of FAST features [20].

The detector also updates the FMO model properties re-
quired by the re-detector and tracker, namely FMO’s color µ
and radius r. For increased stability, the new value of any of
these parameters is a weighted average of the detected value
and the previous value using a forgetting function proposed
in [26]. For each video sequence we also need to determine
the so called exposure fraction ε, which is the ratio of expo-

Detector

IN

It−1, It, It+1

Redetector
It−1, It, It+1
µ, r, Pt−1, ε

Tracker
It, ε
µ, r, Pt−1

OUT

µ, r, Pt

Pt

Pt

ASM

high contrast,
fast movement,
no contact with
moving objects,
no occlusion,

high contrast,
fast movement,
model

linear traj.,
model

Table 1: Inputs, outputs and assumptions of each algorithm.
Image frame at time t is denoted by It. Symbols µ and r
are used for FMO intrinsics – mean color and radius. FMO
trajectory in It is marked by Pt, camera exposure fraction
by ε.

sure period and time difference between consecutive frames
(e.g. 25fps video with 1/50s exposure has ε = 0.5). This
can be done from any two subsequent FMO detections and
we use average over multiple observations.

We need three consecutive video frames to localize the
FMO in the second of the three frames, which causes a con-
stant delay of one frame in real-time processing, but this
does not present any obstacle for practical use.

4.1. Detector

The detector is the only generic algorithm for FMO lo-
calization that requires no input, except for three consecu-
tive image frames It−1, It, It+1. First we compute differ-
ential images ∆+ = |It − It−1|, ∆0 = |It+1 − It−1|, and
∆− = |It − It+1|. These are binarized (denoted by su-
perscript b) by thresholding, and the resulting images are
combined by a boolean operation to a single binary image

∆ = ∆b

+ ∧ ∆b

− ∧ ¬∆b
0.

(2)

This image contains all objects, which are present in the
frame It, but not in the frames It−1 and It+1 (i.e. moving
objects in It).

The second step is to identify all objects which can be
explained by the FMO motion model. We calculate the tra-
jectory Pt and radius r for each FMO candidate and deter-
mine if it satisﬁes the motion model. For each connected
component C in ∆, we compute the distance transform to
get the minimal distance d(x) for each inner pixel x ∈ C
to a pixel on its component’s contour. Then the maxi-
mum of such distances for each component is its radius,
r = max d(x), x ∈ C. Next, we determine the trajec-
tory by morphologically thinning the pixels x that satisfy
d(x) > ψr, where the threshold ψ is set to 0.7. Now we
decide whether the object satisﬁes the FMO motion model
by verifying two conditions: (i) the trajectory Pt must be a

It−1

It−1

It

It

It+1

It+1

∆+

∆+

∆0

∆0

∆−

∆−

∆b

∆b

+

+

It−1

It−1

∆+

∆+

∆b

+

∆b

+

∆b
∆b
0
0
It
It

(b) Re-detection

(a) Detection

(c) Tracking

Figure 3: (a) FMO detection, (b) redetection where detection failed because FMO is not a single connected component,
(c) tracking where both algorithms failed due to imprecise ∆. Top row: cropped It with Pt−1 (blue) and Pt (green) with
contours. Bottom row: binary differential image ∆.

single connected stroke, and (ii) the area a covered by the
component C must correspond to the area ˆa expected ac-
cording to the motion model, that is ˆa = 2r |Pt| + πr2.
ˆa − 1(cid:12)
We say that the areas correspond, if (cid:12)
(cid:12) a
(cid:12) < γ, where
γ is a chosen threshold 0.2. All components which satisfy
these two conditions are then marked as FMOs. The whole
algorithm is pictorially described in Fig. 4.

It−1

∆+

∆b
+

∆
∆0

4.2. Re-detector

∆
∆0

The re-detector requires the knowledge of the FMO, but
allows one FMO occurrence to be composed of several con-
nected components in ∆ (e.g. the FMO passes in front of
background with similar color). Fig. 3 shows an example,
where the re-detector ﬁnds an FMO missed by the detector.
The re-detector operates on a rectangular window of
a binary differential image ∆ in (2), restricted to the lo-
cal neighborhood of the previous FMO localization. Let
Pt−1 be the trajectory from the previous frame It−1, then
the re-detector works in the square neighborhood with side
4 1
ε |Pt−1| and centered on the position of the previous lo-
calization. Note that 1
ε |Pt−1|, where ε is the exposure frac-
tion, is the full trajectory length between It−1 and It. For
each connected component in this region, the trajectory Pt
and radius r are computed in the same way as in the detec-
tion algorithm. The mean color µ is obtained by averaging
all pixel values on the trajectory. In this region, connected
components with model parameters (µ, r) are selected if the
Euclidean distance in RGB (cid:107)µ − µ0(cid:107)2 and the normalized
difference |r − r0| /r0 are below prescribed thresholds 0.3
and 0.4, respectively. Here, the previous FMO parameters
are denoted by (µ0, r0).

4.3. Tracker

The ﬁnal attempt to ﬁnd the FMO after both the detector
and re-detector have failed is the tracker, which uses image
synthesis. The tracker is based on the simpliﬁed formation
model (1) by assuming an object F with color µ and radius r
moving along a linear trajectory Pt. The indicator function
M is then a ball of radius r, and given the trajectory Pt, the

∆b
0

∆b
0

It+1

∆−

∆b
−

It

∆0

∆b
0

∆

Figure 4: Detection of FMOs. Three differential images
of three consecutive frames are binarized, segmented by
boolean operation, and connected components are checked
if they satisfy the FMO model. The two detected FMOs
on this frame are the ball and the white stripe on player’s
t-shirt. However, only the ball passed the check and was
marked as an FMO.

alpha value [HtM ](x) from (1) is

A(x|Pt) =

[Pt ∗ M ](x) =

Pt(x − z)dz.

1
|Pt|

(cid:90)

1
|Pt|

|z|≤r

(3)
For linear trajectories this integral can be solved analyti-
cally. Let D(x, Pt) denote the distance function from x to
the trajectory Pt, then A is

A(x|Pt) ≈

(cid:112)max ((r2 − D2(x, Pt)), 0).

(4)

2
|Pt|

∆b

∆b

−

−

It+1

It+1

∆−

∆−

∆b

−

∆b

−

∆

∆

(1)

(2)

(3)

Figure 5: Tracking steps. (1) detection of orientation, (2)
detection of starting point, (3) detection of ending point.
Previous detection is in blue. Green cross denotes the min-
imizer, red crosses the initial guess. All sampled points
(gray) are scaled by their cost (6) (the darker the higher
cost).

This approximation is inaccurate only in the neighborhood
of the starting and ending point of the trajectory, and for
FMOs this area is small compared to the central section of
the trajectory. Using the above relation, It in (1) can be
written in a simpler form as

ˆIt(x|Pt) = (1 − A(x|Pt))B(x) + µA(x|Pt).

(5)

The tracker now looks for the trajectory Pt that best ex-
plains the frame It using the approximation ˆIt. This is
equivalent to solving

Pt = arg min
Pt

(cid:107) ˆIt(·|Pt) − It(cid:107)2.

(6)

As in the other two algorithms, instead of the background
B we can use one of the previous frames It−1 or It−2, since
a proper FMO should not occupy the same region in several
consecutive frames, and thus the previous frame can locally
serve as the background.

A linear trajectory Pt is given by its starting point st, ori-
entation βt and length |Pt| (equivalently ending point et).
We minimize (6) over these parameters by a coordinate de-
scent search.

First, we ﬁnd the best orientation. We extrapolate the
starting point linearly from the previous detection and as-
sume that the length remains the same, st = et−1 +
ε − 1(cid:1) |Pt−1|uβt and |Pt| = |Pt−1|, where uβ =
(cid:0) 1
(cos(β), sin(β)) is a unit vector with orientation β. Next
we sample the space of βt’s that differ from βt−1 by up to
15◦ and choose the one that minimizes the cost (6).

The minimization w.r.t. st and |Pt| is done in a similar
manner. For st, we sample points in the 1
2 |Pt−1| neigh-
borhood of the extrapolated st from the previous detection,
and for |Pt| we again use the range |Pt−1| ± 50%. The three
minimization stages are illustrated in Fig. 5.

5. Estimation of appearance

Let us consider a video frame It acquired according to
(1) and the object trajectory Pt and size r as determined

by the FMO detector. The objective is to estimate the ap-
pearance F , which is essentially a (modiﬁed) blind image
deblurring task. One has to ﬁrst estimate the blur-and-
projection operator H, and then solve the non-blind deblur-
ring task for F . As mentioned in Sec. 3, to make the esti-
mation of H tractable, we focus on ball-like objects moving
(approximately) parallel to the camera while undergoing ar-
bitrary 3D rotation. As in the FMO tracking, if the object
rotation is negligible or unperceivable, the H operator is
fully determined by the object trajectory and we can pro-
ceed directly to the non-blind estimation of F . Let us ﬁrst
focus on the estimation of F and then on the problem of
obtaining H.

Let F denote some representation of the object appear-
ance – in the absence of rotation, this can be directly the
image of the object projected in the video frame, and when
3D rotation is present we use the spherical parametrization
to capture the whole surface. Following the model (1) we
solve the problem

min
F

(cid:107)(1 − [HM ])B + [HF ] − I(cid:107)1 + α(cid:107)DF (cid:107)1,

(7)

where D is the derivative operator (gradient magnitude) and
α is the weighting parameter proportional to the level of
noise in I. The L1-norm while increasing robustness leads
to nonlinear equations. We therefore apply the method of it-
eratively re-weighted least squares to convert the optimiza-
tion problem to a linear system and use conjugate gradients
to solve it. For object sizes in the FMO dataset (r < 100
pixels) this can be done in less than a second.

In the case of object rotation, the blur operator H en-
codes the object pose (orientation in space) as well as loca-
tion in each fractional moment during the camera exposure.
Trajectory aside, this is fully determined by the object’s an-
gular velocity, which we assume constant throughout the
exposure. Angular velocity (in 3D) is given by three pa-
rameters (two for axis orientation, one for velocity). The
functional in (7) is non-convex w.r.t the angular velocity
parameters. However, we can solve it with an exhaustive
search since the parametric space is not that large. We thus
construct H for each point in the discretized space of pos-
sible angular velocities, estimate F , and then measure the
error given by the functional in (7). The parametrization
which gives the lowest error is our solution.

In Fig. 8 we illustrate the result of FMO deblurring in the
form of temporal super-resolution. The left side (a) shows
a frame captured by a conventional video camera (25fps),
which contains a volleyball that is severely motion blurred.
On the right side (b), the top row shows several frames cap-
tured by a high-speed video camera (250fps) spanning ap-
proximately the same time frame – the volleyball ﬂies from
left to right while rotating clockwise. In the bottom row of
(b) we show the result of FMO deblurring, computed solely
from the single frame in (a), at times corresponding to the

Figure 6: The FMO dataset – one example image per sequence. Red polygons delineate ground truth regions with fast moving
objects. For clearer visualization two frames do not show annotations because their area consists only of several pixels. The
sequences are sorted in natural reading order from left to right and top to bottom as in Tab. 2.

high-speed frames above. The restoration is on par with the
high-speed ground-truth; it signiﬁcantly enhances the video
information content merely by post-processing. For com-
parison, we also display the calculated rotation axis and the
one estimated from the high-speed video. Both are close
to each other; compare the blue cross and red circle in (b).
Note that for a human observer it is impossible to determine
the ball rotation from blurred images while the proposed al-
gorithm with the temporal super-resolution output provides
this insight. Another appearance estimation example is in
Fig. 9, where we use the simpliﬁed model of pure transla-
tion motion for the table-tennis ball (top) and frisbee (bot-
tom).

6. Dataset

The FMO dataset contains videos of various activities in-
volving fast moving objects, such as ping pong, tennis, fris-
bee, volleyball, badminton, squash, darts, arrows, softball,
as well as others. Acquisition of the videos differ: some are
taken from a tripod with mostly static backgrounds, some
have severe camera motions and dynamic backgrounds,
some FMOs are nearly homogeneous, while some have col-
ored texture. All the sequences are annotated with ground-
truth locations of the object (even in cases when the object

Figure 7: FMO detection and tracking. Each blue re-
gion refers to the object trajectory and contour in previous
frames.

of interest does not strictly satisfy the notion of FMO).

None of the public tracking datasets contain objects
moving fast enough to be considered FMOs – with sig-
niﬁcant blur and large frame-to-frame displacement. We
analyzed three of the most widely used tracking datasets,
ALOV [22], VOT [15], and OTB [27] and compared them
with the proposed method in terms of the motion of the ob-
ject of interest. For example, in the conventional datasets,

n
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

Sequence name
volleyball
volleyball passing
darts
darts window
softball
archery
tennis serve side
tennis serve back
tennis court
hockey
squash
frisbee
blue ball
ping pong tampere
ping pong side
ping pong top
Average

#
50
66
75
50
96
119
68
156
128
350
250
100
53
120
445
350
–

Pr.
100.0
21.8
100.0
25.0
66.7
0.0
100.0
28.6
0.0
100.0
0.0
100.0
100.0
100.0
12.1
92.6
59.2

Rc.
45.5
10.4
26.5
50.0
15.4
0.0
58.8
5.9
0.0
16.1
0.0
100.0
52.4
88.7
7.3
87.8
35.5

F-sc.
62.5
14.1
41.7
33.3
25.0
0.0
74.1
9.8
0.0
27.7
0.0
100.0
68.8
94.0
9.1
90.1
40.6

Table 2: Performance of the proposed method on the FMO
dataset. We report precision, recall and F-score. The num-
ber of frames is indicated by #.

the object frame-to-frame displacement is below 10 pixels
in 91% of cases, while in the FMO dataset the displace-
ment is uniformly spread between 0 and 150 pixels. Sim-
ilarly, the intersection over union (IoU) of bounding boxes
between adjacent frames is above 0.5 in 94% of times for
the conventional datasets, whereas the proposed dataset has
zero intersection nearly every time. Fig. 2 summarizes these
ﬁndings.

An overview of the FMO dataset is in Fig. 6, showing
some of the included activities and the ground-truth anno-
tations. The dataset and annotations will be made publicly
available.

7. Evaluation

The proposed localization pipeline was evaluated on
the FMO dataset. The performance criteria are preci-
sion TP/(TP + FP), recall TP/(TP + FN) and F-score
2TP/(2TP + FN + FP), where TP, FP, FN is the number
of true positives, false positive and false negatives, respec-
tively. A true positive detection has an intersection over
union (IoU) with the ground truth polygon greater than 0.5
and an IoU larger than other detections. The second condi-
tion ensures that multiple detections of the same object gen-
erates only one TP. False negatives are FMOs in the ground
truth with no associated FP detection.

Quantitative results for individual video sequences are
listed in Tab. 2. All results were achieved for the same set
of parameters in the localization pipeline as discussed in
Sec. 4. Performance varies widely, ranging from a F-score

d
o
h
t
e

M

]
5
2
[
S
M
S
A

80
12
3
0
0
5
7
5
0
0
0
65
30
0
1
0
17

]
9
[
T
S
S
D

0
6
0
0
0
5
0
0
0
0
0
0
0
0
0
0
1

]
9
2
[
M
E
E
M

50
95
6
0
0
5
0
0
3
0
0
6
0
0
0
0
1

]
8
[
F
C
D
R
S

0
88
0
0
0
5
0
0
3
0
0
6
0
0
0
0
1

]
2
1
[
K
C
U
R
T
S
10
8
0
0
0
0
6
3
0
0
0
0
25
0
0
1
3

d
e
s
o
p
o
r
P

46
10
27
50
15
0
59
6
0
16
0
100
52
89
7
88
36

Sq. name

volleyball
volleyball passing
darts
darts window
softball
archery
tennis serve side
tennis serve back
tennis court
hockey
squash
frisbee
blue ball
ping pong tampere
ping pong side
ping pong top
Average

Table 3: Performance of baseline methods on the FMO
dataset. Percentage of frames with FMOs present where
tracking was successful (IoU > 0.5).

of 0% (complete failure) for the archery, tennis court, and
squash sequences, to 100% (complete success) for the fris-
bee sequences. The sequences with the best results contain
objects with prominent FMO characteristics, i.e. a large
motion against a contrasting background. False negatives
occur in three types of situations: (i) the object motion is
too small (archery, volleyball), (ii) the object itself is too
small (tennis court, squash), and (iii) the background is too
similar to the object color (e.g., table tennis net, white edge
of the table). Problem (i) can be addressed by combining
the FMO detector with a state-of-the-art “slow” short-term
tracker. False positives usually occur when local move-
ments of larger objects, such as players’ body parts, can be
partially explained by the FMO model, or due to imprecise
camera stabilization. Note that none of the test sequences
contain multiple FMOs in a single frame, but the algorithm
is not constrained to detect a ﬁxed number of objects. The
detection results are included in the supplementary material.
Some examples are shown in Fig. 7.

Next, we compare the results of the FMO localization
pipeline to those of several standard state-of-the-art track-
ers, namely ASMS [25], DSST [9], SRDCF [8], MEEM
[29], and STRUCK [12]. For a fair comparison, only frames
containing exactly one FMO were included. Since these
trackers always output exactly one detection per frame and
the proposed method can return any number of detections,

a)

b)

Figure 8: Reconstruction of an FMO blurred by motion and rotation. a) Input video frame. b) Top row: actual frames from
a high-speed camera (250fps). Bottom row: frames at corresponding times reconstructed from a single frame of a regular
camera (25fps), i.e. 10x temporal super-resolution. The top left image shows the rotation axis position estimated from the
blurred frame (blue cross) and from the highspeed video (red circle).

including none, the proposed method would have an advan-
tage on the full set of frames. The results are presented in
Tab. 3 in terms of the percentage of frames with a successful
detection. Some of the standard trackers performed reason-
ably well on the volleyball sequences, where the motions
are relatively slow, but overall results are very poor. The
proposed method performs signiﬁcantly better. This is ex-
plainable because the compared methods were not designed
for scenarios involving FMOs, but it highlights the need for
a specialized FMO tracker.

Besides FMO localization, the proposed model and es-
timator enable several applications which may be useful in
processing videos containing FMOs. In Sec. 5 on appear-
ance estimation, we suggested the task of temporal super-
resolution, which increases the video frame-rate by ﬁlling
out the gap between existing frames and artiﬁcially de-
creases the exposure period of existing frames. The naive
approach is the interpolation of adjacent frames, which is
inadequate for videos containing FMOs. A more precise ap-
proach requires moving objects to be localized, deblurred,
and their motions modeled, which the proposed method ac-
complishes (see Sec. 5), so that new frames can be synthe-
sized at the desired frame-rate. Figs. 8 and 9 show example
results of the temporal super-resolution.

Another popular use case is highlighting FMO in sport
videos. Due to the extreme blur, FMOs are often hard
to localize, even for humans, despite having the context
provided by perfect semantic scene understanding. Sim-
ple highlighting, like recoloring or scaling, enhances the
viewer’s experience. Fig. 9 top-right demonstrates temporal
super-resolution with highlighting.

Acknowledgments. The authors were supported by the Technology
Agency of the Czech Republic project TE01020415 V3C, the MSMT
LL1303 ERC-CZ project and the Grant Agency of the Czech Republic
under project GA13-29225S.

Figure 9: Temporal super-resolution using plain interpola-
tion (left) and the appearance estimation model (right). The
top right image shows the possibility of FMO highlighting.

8. Conclusions

Fast moving objects are a common phenomenon in real-
life videos, especially sports. We proposed a generic, i.e.
not requiring prior knowledge of appearance, algorithm for
their fast localization and tracking and a blind deblurring
algorithm for estimation of their appearance. We created
a new dataset consisting of 16 sports videos with ground-
truth annotations. Tracking FMOs is considerably different
from standard object tracking targeted by state-of-the-art
algorithms and thus requires a specialized approach. The
proposed method is the ﬁrst attempt in this direction and
outperforms baseline methods by a wide margin. The es-
timated FMO appearance could support applications useful
in sports analytics, such as realistic increase of video frame-
rate (temporal super-resolution), artiﬁcial object highlight-
ing, visualization of rotational axis and measurement of
speed and angular velocity.

References

[1] A. Alahi, R. Ortiz, and P. Vandergheynst.

Freak: Fast
retina keypoint. In Computer Vision and Pattern Recognition
(CVPR), 2012 IEEE Conference on, pages 510–517, June
2012. 3

[2] S. Avidan. Ensemble tracking. IEEE Trans. Pattern Anal.

Mach. Intell., 29(2):261–271, Feb. 2007. 1

[3] B. Babenko, M. H. Yang, and S. Belongie. Robust ob-
IEEE
ject tracking with online multiple instance learning.
Transactions on Pattern Analysis and Machine Intelligence,
33(8):1619–1632, Aug 2011. 1

[4] T. A. Biresaw, A. Cavallaro, and C. S. Regazzoni.
Correlation-based self-correcting tracking. Neurocomput.,
152(C):345–358, Mar. 2015. 2

[5] A. Chakrabarti, T. Zickler, and W. T. Freeman. Analyz-
In Proc. IEEE Conf. Computer
ing spatially-varying blur.
Vision and Pattern Recognition (CVPR), pages 2512–2519,
San Francisco, CA, USA, June 2010. 2

[6] D. Comaniciu, V. Ramesh, and P. Meer. Kernel-based ob-
IEEE Trans. Pattern Anal. Mach. Intell.,

ject tracking.
25(5):564–575, May 2003. 2

[7] S. Dai and Y. Wu. Motion from blur. In Computer Vision and
Pattern Recognition, 2008. CVPR 2008. IEEE Conference
on, pages 1 –8, June 2008. 2

[8] M. Danelljan, G. Hager, F. Shahbaz Khan, and M. Felsberg.
Learning spatially regularized correlation ﬁlters for visual
tracking. In Proceedings of the IEEE International Confer-
ence on Computer Vision, pages 4310–4318, 2015. 2, 7
[9] M. Danelljan, G. Hauml;ger, F. Shahbaz Khan, and M. Fels-
berg. Accurate scale estimation for robust visual tracking.
In Proceedings of the British Machine Vision Conference.
BMVA Press, 2014. 2, 7

[10] M. A. Fischler and R. C. Bolles. Random sample consen-
sus: A paradigm for model ﬁtting with applications to im-
age analysis and automated cartography. Commun. ACM,
24(6):381–395, June 1981. 3

[11] M. Godec, P. M. Roth, and H. Bischof. Hough-based track-
ing of non-rigid objects. Comput. Vis. Image Underst.,
117(10):1245–1256, Oct. 2013. 1

[12] S. Hare, S. Golodetz, A. Saffari, V. Vineet, M. M. Cheng,
S. L. Hicks, and P. H. S. Torr. Struck: Structured output
tracking with kernels. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 38(10):2096–2109, Oct 2016.
2, 7

[13] J. Jia. Single image motion deblurring using transparency.
In Computer Vision and Pattern Recognition (CVPR), 2007
IEEE Conference on, pages 1–8, 2007. 2

[14] T. H. Kim and K. M. Lee. Segmentation-free dynamic scene
In Computer Vision and Pattern Recognition
deblurring.
(CVPR), 2014 IEEE Conference on, pages 2766–2773, 2014.
2

[15] M. Kristan, J. Matas, A. Leonardis, M. Felsberg, L. Ce-
hovin, G. Fernandez, T. Vojir, G. Hager, G. Nebehay, and
R. Pﬂugfelder. The visual object tracking vot2015 challenge
results. In The IEEE International Conference on Computer
Vision (ICCV) Workshops, December 2015. 1, 2, 6

[16] M. Kristan, J. Matas, A. Leonardis, T. Vojir, R. Pﬂugfelder,
G. Fernandez, G. Nebehay, F. Porikli, and L. ˇCehovin. A
novel performance evaluation methodology for single-target
trackers, Jan 2016. 1

[17] A. V. Kruglov and V. N. Kruglov. Tracking of fast moving
objects in real time. Pattern Recognition and Image Analysis,
26(3):582–586, 2016. 2

[18] A. Levin, D. Lischinski, and Y. Weiss. A closed-form so-
lution to natural image matting. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 30(2):228–242, Feb.
2008. 2

[19] J. Oliveira, M. Figueiredo, and J. Bioucas-Dias. Parametric
blur estimation for blind restoration of natural images: Lin-
ear motion and out-of-focus. IEEE Transactions on Image
Processing, 23(1):466–477, 2014. 2

[20] E. Rosten and T. Drummond. Machine Learning for High-
Speed Corner Detection, pages 430–443. Springer Berlin
Heidelberg, Berlin, Heidelberg, 2006. 3

[21] Q. Shan, W. Xiong, and J. Jia. Rotational motion deblurring
In Proc. IEEE 11th
of a rigid object from a single image.
International Conference on Computer Vision ICCV 2007,
pages 1–8, Oct. 2007. 2

[22] A. W. M. Smeulders, D. M. Chu, R. Cucchiara, S. Calderara,
A. Dehghan, and M. Shah. Visual tracking: An experimental
survey. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 36(7):1442–1468, July 2014. 1, 2, 6

[23] J. Sun, W. Cao, Z. Xu, and J. Ponce. Learning a convolu-
tional neural network for non-uniform motion blur removal.
In Computer Vision and Pattern Recognition (CVPR), 2015
IEEE Conference on, pages 769–777, 2015. 2

[24] C. Tomasi and T. Kanade. Detection and tracking of point
features. School of Computer Science, Carnegie Mellon
Univ. Pittsburgh, 1991. 2

[25] T. Vojir, J. Noskova, and J. Matas. Robust Scale-Adaptive
Mean-Shift for Tracking, pages 652–663. Springer Berlin
Heidelberg, Berlin, Heidelberg, 2013. 2, 7

[26] K. G. White. Forgetting functions. Animal Learning & Be-

havior, 29(3):193–207, 2001. 3

[27] Y. Wu, J. Lim, and M.-H. Yang. Online object tracking: A
In IEEE Conference on Computer Vision and

benchmark.
Pattern Recognition (CVPR), 2013. 1, 2, 6

[28] M. A. Zaveri, S. N. Merchant, and U. B. Desai. Small and
fast moving object detection and tracking in sports video se-
In Multimedia and Expo, 2004. ICME ’04. 2004
quences.
IEEE International Conference on, volume 3, pages 1539–
1542 Vol.3, June 2004. 2

[29] J. Zhang, S. Ma, and S. Sclaroff. MEEM: Robust Track-
ing via Multiple Experts Using Entropy Minimization, pages
188–203. Springer International Publishing, Cham, 2014. 2,
7

