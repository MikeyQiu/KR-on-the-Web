Temporally Identity-Aware SSD with
Attentional LSTM

Xingyu Chen, Junzhi Yu, Senior Member, IEEE, and Zhengxing Wu

1

0
2
0
2
 
r
a

M
 
5
2
 
 
]

V
C
.
s
c
[
 
 
4
v
7
9
1
0
0
.
3
0
8
1
:
v
i
X
r
a

Abstract—Temporal object detection has attracted signiﬁcant
attention, but most popular detection methods cannot leverage
rich temporal information in videos. Very recently, many algo-
rithms have been developed for video detection task, yet very
few approaches can achieve real-time online object detection
in videos. In this paper, based on attention mechanism and
convolutional long short-term memory (ConvLSTM), we propose
a temporal single-shot detector (TSSD) for real-world detection.
Distinct from previous methods, we take aim at temporally
integrating pyramidal feature hierarchy using ConvLSTM, and
design a novel structure including a low-level temporal unit as
well as a high-level one (LH-TU) for multi-scale feature maps.
Moreover, we develop a creative temporal analysis unit, namely,
attentional ConvLSTM (AC-LSTM), in which a temporal atten-
tion mechanism is specially tailored for background suppression
and scale suppression while a ConvLSTM integrates attention-
aware features across time. An association loss and a multi-
step training are designed for temporal coherence. Besides, an
online tubelet analysis (OTA) is exploited for identiﬁcation. Our
framework is evaluated on ImageNet VID dataset and 2DMOT15
dataset. Extensive comparisons on the detection and tracking
capability validate the superiority of the proposed approach.
Consequently, the developed TSSD-OTA achieves a fast speed
and an overall competitive performance in terms of detection
and tracking. Finally, a real-world maneuver is conducted for
underwater object grasping. The source code is publicly available
at https://github.com/SeanChenxy/TSSD-OTA.

Index Terms—Object detection, tracking-by-detection, video

processing, sequential learning.

I. INTRODUCTION

T AKING the advantage of convolutional neural network

(CNN), existing detection methods commonly fall into
two categories, i.e., one-stage and two-stage detectors. The
former is represented by RCNN family [1]–[4] and RFCN
[5], all of which detect objects based on region proposal.
On the other hand, YOLO [6], SSD [7], RetinaNet [8],
and etc., detect objects in a one-step fashion with a single-
shot network. In particular, making use of CNN’s features

Manuscript received May 8, 2018; revised August 27, 2018 and November
23, 2018; accepted January 16, 2019. This work was supported in part by
the National Natural Science Foundation of China under Grant 61725305,
Grant 61633004, Grant 61633020, Grant 61633017, and Grant 61603388;
and in part by the Beijing Natural Science Foundation under Grant 4161002.
(Corresponding author: Junzhi Yu.)

X. Chen and Z. Wu are with the State Key Laboratory of Management and
Control for Complex Systems, Institute of Automation, Chinese Academy
of Sciences, Beijing 100190, China and University of Chinese Academy of
Sciences, Beijing 100049, China (e-mail: chenxingyu2015@ia.ac.cn; zhengx-
ing.wu@ia.ac.cn).

J. Yu is with the State Key Laboratory of Management and Control for
Complex Systems, Institute of Automation, Chinese Academy of Sciences,
Beijing 100190, China, and also with the Beijing Innovation Center for
Engineering Science and Advanced Technology, Peking University, Beijing
100871, China (e-mail: junzhi.yu@ia.ac.cn).

more effectively, SSD is one of the ﬁrst methods that adopt
the pyramidal feature hierarchy for detection. However, most
works have largely focused on detecting in static images,
ignoring temporal consistency in videos. Thus, it is imperative
to develop an approach to integrate spatial features with
temporal information. In addition, because of its relatively
fast detection speed, the one-stage detector is more suitable
for real-world applications.

Recurrent neural network (RNN) has seen great success
in some sequence processing tasks [9], [10]. Typically, long
short-term memory (LSTM) is proposed for longer sequence
learning [11]. For spatiotemporal visual features, Shi et al.
developed a convolutional LSTM (ConvLSTM) to associate
LSTM with spatial structure [12]. However, as for detec-
tion, selecting discriminative features for ConvLSTM is a
pivotal step, because only a small part of visual features
can devote themselves to detecting. Fortunately, attention is
an exciting idea which imitates human’s cognitive patterns,
promoting CNN to concern on essential
information. For
example, Mnih et al. proposed a recurrent attention model
to ﬁnd the most suitable local feature for image classiﬁcation
[14]. Yet, very few literature has reported an attention-based
temporal module for image-sequence detection.

As a closely relevant ﬁeld, object tracking requires the initial
position to be known as a prior knowledge [35]. Moreover,
detection and tracking are moving towards unity in recent
years, where a detector and a tracker tend to be cascaded in
most up-to-date approaches [21], [44]. This solution generally
raises model complexity and computational cost. Hence, it is
worthwhile to exploit a tracker-like detector for their in-depth
combination and time efﬁciency. Tracking-by-detection is a
popular idea, advocating that a detector should output tracker-
like results, where the tracking component is actually designed
for data association [22], [23], [43], [45]. Nevertheless, the
detector and tracking component are usually independent in
existing tracking-by-detection framework, so it is essential to
jointly investigate their performance.

As shown in Fig. 1, our approach is able to detect objects
across time and link them frame by frame using individual
identities, and we call this kind of detection approach identity-
aware detector. Aiming at detecting objects in videos, we
propose a temporal detection model based on SSD, namely,
temporal single-shot detector (TSSD). To integrate features
across time, ConvLSTMs are employed for temporal infor-
mation. Due to the pyramidal feature hierarchy for multi-
scale detection, SSD always generates a large body of visual
features with multi-scale semantic information, thus we design
a new structure including a low-level temporal unit as well

2

II. RELATED WORK

A. Advancement of Detection Framework for Videos

At the beginning, static detection and post-proposing meth-
ods are combined to counteract video detection task [15]–
[17]. They statically detect in each video frame, then com-
prehensively deal with multi-frame results. Kang et al. de-
veloped detection methods based on tubelet, which is deﬁned
as temporally propagative bounding boxes in video snippet
[15], [16]. Their method TCNN contains still-image object
detection, multi-context suppression, motion guided propaga-
tion, and temporal tubelet re-scoring. Taking inspiration from
the non-maximum suppression (NMS), Han et al. proposed
an SeqNMS to suppress temporally discontinuous bounding
boxes [17]. However, these solutions come with two major
drawbacks. The complex post-processing could affect time
efﬁciency, and they do not improve the performance of the
detector itself.

Faster RCNN uses region proposal network for object
localization [3], so some approaches for video detection try to
enhance the effectiveness of RPN with temporal information
[18]–[20], [26], [37]. Galteri et al. designed a closed-loop RPN
to merge new proposals with previous detection results. This
method effectively reduces the number of invalid regions, but it
may also make the proposed regions excessively concentrated.
Kang et al. developed tubelet proposal networks (TPN) to gen-
erate tubelets rather than bounding boxes. Then, an encoder-
decoder LSTM is used for classiﬁcation. The TPN integrates
temporal information, but it requires the future messages. Such
methods are extended from two-stage detectors, resulting in
relatively low computational efﬁciency.

B. Detection and Tracking

Feichtenhofer et al. associated an RFCN detector with a
correlation-ﬁlter-based tracker [35] to detect objects in videos,
called D&T [21]. Thanks to the tracking method, it achieves
a high recall rate, but this cascaded system could seriously
increase the model complexity, and impair the inference speed.
In terms of tracking-by-detection, Xiang et al. converted
the tracking task to decision making, and their policy relies
on tracking states [43]. H. Kim and C. Kim combined a
detector, a forward tracker, and a backward tracker for tracing
multiple objects in video sequences, and the detector was
also used to reﬁne tracking results [44]. Ning et al. proposed
an ROLO based on YOLO and LSTM for tracking [22].
The YOLO is responsible for static detection, and the visual
features as well as positions of high-score objects will be
fed to LSTM for temporally modeling. Lu et al. proposed
association LSTM (ALSTM) to temporally analyze relations
of high-score objects, and an association loss was designed for
identiﬁcation [23]. Pragmatically, Bewley et al. developed an
SORT with Kalman ﬁlter [47] and Hungarian method [48] for
real-time tracking [45], and the tracking component achieves a
speed of 260 frames per second (FPS). Nevertheless, in most
of these methods, the visual features in detector have not been
efﬁciently used yet.

Fig. 1. The main idea of TSSD-OTA. We aim to temporally detect objects in
videos, and generate tracking-like results with low computational costs. The
TSSD is a temporal detector, and the OTA is designed for identiﬁcation.

as a high-level one (LH-TU) for their temporal propagation.
Furthermore, as for multi-scale feature maps, only a small
part of visual features are related to objects. Thereby, attention
mechanism is adopted for background suppression and scale
suppression, then we propose an attentional ConvLSTM (AC-
LSTM) module. Subsequently, an association objective and
a multi-step method are developed for sequence training.
Ultimately, an online tubelet analysis (OTA) is carried out
for identiﬁcation. As a consequence, the TSSD-OTA achieves
considerably improved detection and tracking performance for
consecutive vision in terms of both accuracy and speed. To the
best of our knowledge, only few temporal one-stage detectors
have been reported. Moreover, a real-time, online, and identity-
aware detector is absent in existing detection frameworks. The
contributions made in this paper are summarized as follows:
• We design an LH-TU structure to effectively propagate
pyramidal feature hierarchy across time. Moreover, we
propose an AC-LSTM module as a temporal analysis
unit, in which a temporal attention mechanism is designed
for background suppression and scale suppression. Then,
the corresponding training methods are developed for the
TSSD.

• For identiﬁcation, an OTA algorithm is exploited with the
low-level AC-LSTM, which links detected objects frame
by frame in a fairly fast speed.

• We achieve a considerably improved results on ImageNet
VID dataset and 2DMOT15 dataset in terms of detection
and tracking.

The remainder of this paper is organized as follows. Sec-
tion II presents the related works. Thereafter, our approach
including TSSD and OTA is elaborated in Section III. Next,
Section IV provides the experimental results and discussion.
Finally, conclusion and future work are summarized in Sec-
tion V.

3

Fig. 2. The schematic illustration of TSSD-OTA. The low-level features share an AC-LSTM and high-level features do so, namely, LH-TU. Next, the hidden
states of ConvLSTM will be used for multi-box regression and classiﬁcation. Eventually, based on multi-scale attention maps, online tubelet analysis is
conducted for identiﬁcation. Although we show a two-frame detection process here, the TSSD-OTA learns from all previous frames, and generates current
hidden states using current pyramidal features as well as all previous memories. Moreover, the valid memory length is under the control of AC-LSTM’s forget
gate.

C. RNN-Based Detector

Very recently, up-to-date approaches have associated the
detector with RNN. Liu and Zhu reported a mobile video
detection method based on SSD and LSTM, called LSTM-
SSD [27]. Moreover, they also designed a Bottleneck-LSTM
structure to reduce computational costs. As a result, LSTM-
SSD reached a real-time inference speed of up to 15 FPS on
a mobile CPU. Xiao and Lee developed a spatial-temporal
memory module (STMM) with ConvGRU [13] for temporal
information propagation [26]. In particular, “MatchTrans” was
proposed to suppress the redundant memory.

III. APPROACH

In this section, we ﬁrstly present the proposed architecture,
including the LH-TU and the AC-LSTM. Then, we describe
how to train the network in detail. Finally, the methodologies
of OTA algorithm will be briefed.

A. Architecture

Extending form SSD with VGG-16 [24] as the backbone,
we build a temporal architecture, where fc6, fc7 in original
VGG-16 are converted to convolutional layers, namely, Conv6,
Conv7. Referring to Fig. 2, the proposed TSSD is based on
forward CNN and RNN that generate pyramidal features for
detection. Convolutional detection head is designed for multi-
class classiﬁcation and regression (shown in green). In this
process, convolution operations are leveraged to predict objects
information with multi-scale visual features, then a ﬁxed num-
ber of bounding boxes and the category-discriminative scores
indicating the presence different classes of objects on those
boxes, followed the NMS to generate the ﬁnal results. The
spatial resolution of the input image is 300 × 300. Conv4 3,
Conv7, Conv8 2, Conv9 2, Conv10 2, Conv11 2 are em-
ployed as pyramidal features, whose size are 38 × 38 × 512,

19 × 19 × 512, 10 × 10 × 512, 5 × 5 × 256, 3 × 3 × 256, and
1 × 1 × 256, respectively. As for sequence learning, the TSSD
is equipped with multi-scale feature-integration structures, i.e.
the LH-TU and the AC-LSTM. The LH-TU takes aim at
propagation of pyramidal feature hierarchy, whereas the AC-
LSTM aims to effectively produce temporal memory without
useless information.

1) LH-TU: We use the same two structures to integrate
the pyramidal feature hierarchy temporally, called LH-TU.
There are pyramidal features for six-scale semantic infor-
mation in adopted SSD model, and their feature sizes are
diverse from each other. In original SSD framework, there are
512, 1024, 512, 256, 256, 256 channels in feature maps from
low-level to high-level. Creatively, we divide the multi-scale
feature maps into two categories according to their hierarchical
relation and channel sizes, i.e., low-level features and high-
level features. That is, on one hand, we group the multi-scale
feature maps according to the order of different convolutional
layers. It is known that convolution operation extracts visual
features gradually, and shadow features contain more image
details while the high-level features cover more semantic in-
formation. Thus, the LH-TU is in favor of the learning process.
On the other hand, the low-level and high-level features should
share their respective temporal unit, but a ConvLSTM unit
cannot process channel-size-variable features, so in the manner
of LH-TU, we only change Conv7’s channel size to 512 to
remain original SSD structure as much as possible. Therefore,
as illustrated in Fig. 2, we treat the ﬁrst three feature maps
as low-level features (shown in red), whereas the last three
maps are considered as high-level features (shown in gold).
The channel sizes of low-level features are uniﬁed as 512
to share a temporal unit while that of high-level features
remain as 256. Additionally, the low-level features cover more
details, whereas the high-level features contain more semantic
information. Correspondingly, the LH-TU including a low-

4

As shown in Fig. 3, the AC-LSTM is designed with CNN
and RNN. Current feature map (x) and previous hidden state
(ht−1) serve as the input of the attention module. After a
three-layer convolution, a one-channel attention map (a) is
generated containing pixel-wise positions for object-aware
features, which will be used to select useful features in the AC-
LSTM and identify objects in the OTA. It should be mentioned
that, instead of binary selection (1 for attention area and 0
otherwise), each element in the attention map is continuous in
[0, 1], so as to describe object’s saliency mass more effectively.
For feature selection, each channel of current feature map
multiplies this attention map pixel-by-pixel, and the attention-
aware feature (a ◦ x) can be obtained. The attention-aware
feature and previous hidden state are concatenated as the input
of the ConvLSTM. Different from traditional LSTM, gates (i,
f , o) and incoming information (c) will be computed with
convolution operation [12]. Subsequently, controlled by gates,
the temporal memory (s) will be updated, and current hidden
state is generated for multi-box regression and classiﬁcation.
During this operation, a ◦ x, i, f, o, c, s, h are in the same size.
In addition, we also use dropout regularization [39] for the
attention-aware feature during training. Apparently, temporal
information transmission is conducted twice in temporal at-
tention and ConvLSTM.

Note that

the attention mechanism and input gate play
different roles, although both of them can be aware of useful
features. For background suppression and scale suppression,
the attention mechanism works for spatial location in each 2D
map, whereas the input gate can deal with the 3D feature along
the channel to preserve discriminative data.

B. Training

We design a multi-task objective to train the TSSD, in-
cluding a localization loss Lloc, a conﬁdence loss Lconf , an
attention loss Latt, and an association loss Lasso,

L =

(αLloc + βLconf ) + γLatt + ξLasso,

(2)

1
M

where M is the number of matched boxes, and α, β, γ, ξ are
trade-off parameters. Lloc and Lconf are deﬁned in accordance
with SSD [7]. Then, we train the TSSD in three steps.

1) Attention Loss: The generation of attention maps is
the
supervised using cross entropy. At ﬁrst, we construct
ground truth attention map Ag, in which elements in ground
truth boxes equal to 1 and others are 0. There are six feature
maps for multi-box prediction, which generate multi-scale
attention maps Apsc . Therefore, each Apsc is ﬁrstly uniﬁed
to the same resolution as the input image through bilinear
upsampling operation, followed by the generation of Aup
psc .
Each upsampled attention map can generate a scale-related
attention loss with cross entropy, and we add up 6-scale losses
as the ﬁnal attention loss. Then, Latt can be given as

Latt =

µ(−Aup
psc

log(Ag) − (1 − Aup
psc

) log(1 − Ag)), (3)

6
(cid:88)

sc=1

where µ averages all elements in a matrix.

Fig. 3.
Implementation detail of AC-LSTM. “c” denotes concatenation;
“Chw-x”, “Elw-x” represent channel-wise and element-wise multiplication;
“+” is element-wise summation. The generated hidden state is used for
detection while the attention map is employed by both AC-LSTM and OTA.

level temporal unit and a high-level one is designed for them.
2) AC-LSTM: In object detection task, most features are
related to background. Moreover, feature maps in different
scales contribute differently to the detection. Therefore,
it
is inefﬁcient when a ConvLSTM handles background or
aforementioned small-contributed multi-scale feature maps.
For example, if an object’s size is too small, its detection
will be contributed by Conv4 3, in which features associated
with the small object are far less than that for background.
Moreover, all the higher-level feature maps can be considered
useless, which should be suppressed to avoid the false positive.
To that end, we propose an AC-LSTM for background sup-
pression and scale suppression, in which a temporal attention
mechanism selects object-aware features for a ConvLSTM,
and in turn, the ConvLSTM provides the attention mechanism
with temporal information to improve attention accuracy. As
a temporal analysis unit, the AC-LSTM can be formulated as:

at = sigmoid(Wa ∗ [x, ht−1])
it = sigmoid(Wi ∗ [at ◦ x, ht−1] + bi)
ft = sigmoid(Wf ∗ [at ◦ x, ht−1] + bf )
ot = sigmoid(Wo ∗ [at ◦ x, ht−1] + bo)
ct = sigmoid(Wc ∗ [at ◦ x, ht−1] + bc)
st = (ft (cid:12) st−1) + (it (cid:12) ct)
ht = ot (cid:12) tanh(st),

(1)

[·, ·]

is concatenation; (cid:12) is
where ∗ denotes convolution;
element-wise multiplication; and ◦ represents that a one-
channel map multiplies with each channel in a multi-channel
feature map. At time step t, at, ht, it, ft, ot, ct, st are attention
map, hidden state, input gate, forget gate, output gate, LSTM’s
incoming information and memory, respectively.

2) Association Loss: Pixel-level changes could signiﬁcantly
impact the detection results, so an object in video always
encounters large conﬁdence ﬂuctuations with a static detection
method (studied in [16]). Thus, towards temporal consistency
of videos, an association loss should be developed for se-
quence training. To that end, we encourage the TSSD to
generate similar global classiﬁcation results for consecutive
frames. We ﬁrstly compute top k high conﬁdence scores per
class after NMS,
then sum them up to generate a class-
discriminative score list (sl). The score list should remain
small ﬂuctuation in consecutive frames. Thereby, the Lasso
can be obtained,

Lasso = (

slt − slave)/seq len,

(4)

seq len
(cid:88)

t=1

where slt is the score list at time step t; slave denotes the mean
score list among sl1:t−1; and seq len represents the sequence
length. It should be remarked that our proposed association
loss works in a self-supervision manner. That is, there is no
incoming ground truth label when computing Lasso.

3) Multi-Step Training: We ﬁrst train an SSD model fol-
lowing [7]. In the next step, the TSSD is trained based on well-
trained SSD. We freeze the weights in the network except for
AC-LSTM and detection head. In particular, the ConvLSTM is
trained with RMSProp [36] while the rest of TSSD is trained
using SGD optimizer with an initial learning rate of 10−4 and
a decay rate of 0.1 for 40 epochs. The learning rate drops at
the 30th epoch. On the other hand, the TSSD should be trained
with a sequence of frames, but the frame rates of videos are
inconstant. Moreover, the motion speed of objects in videos
varies considerably. For better generalization, it should not
be trained frame by frame. Instead, we only choose seq len
frames in a video for backpropagation in an iteration. The
seq len frames are chosen uniformly based on the start frame
sf and skip sp, namely, random skip sampling (RSS), i.e.,

sp = R[1, v/seq len]

sf = R[1, v − seq len × sp + 1],

(5)

where v is the total number of frames in a video, and
R[min, max] represents the operation of selecting an inte-
ger randomly between min and max. Finally, the uniform
seq len frames are chosen with sf as the start frame and
sp as the skip. In this paper, seq len = 8. At this step, the
association loss Lasso is not involved.

Thirdly, the full objective including Lasso is used to ﬁne
tune parameters for 10 epoches. At this step, the learning rate
is 10−5, and sp = 1. The hyper parameters α = 1, β = 1, γ =
0.5, ξ = 2, δ = 3 are selected based on model performance.

C. Inference

At inference phase, the LH-TU and the AC-LSTM integrate
features across time, generating temporally-aware hidden state
for regression and classiﬁcation. Finally, we apply the NMS
with jaccard overlap (IoU) of 0.45 (for ImageNet VID) or 0.3
(for 2DMOT15) per class and retain the top 200 (for ImageNet
VID) or 400 (for 2DMOT15) high conﬁdent detections per
image to produce the ﬁnal detections.

5

D. Online Tubelet Analysis

For online tracking-by-detection task, it is expected that
the TSSD is endowed with the ability to identify objects.
Tubelet in videos has been studied by [16], [20], [37], each
of which has a unique identity (ID). In the scope of detection,
[16] generates tubelets by tracking, whereas [20] and [37]
use tubelet proposal for batch-mode detection. However, these
methods are either computationally expensive or not online.
Thereby, we attempt
to exploit a real-time online object
association method based on tubelet for tracking-by-detection.
Features in a detector are not suitable for identiﬁcation,
because they always contain within-class-similar information.
Fortunately, there are category-independent features in our
framework, i.e., attention maps. Instead of being discriminative
for each class, the attention maps describe the object’s saliency
mass at different visual scales. Thereby, the attention space is
employed for fast data association. This idea is straightforward
and computationally tractability, where the key intuition is that
the saliency mass of objects are diverse from each other, and
the attention mechanism is able to capture this subtle distinc-
tiveness. The low-level AC-LSTM is employed for this task,
because low-level features cover more detailed information. In
short, attention maps have three merits for identiﬁcation:

• We no longer need extra instance-related training.
• They are instance-aware features that describe objects’

saliency mass.

Algorithm 1 Online Tubelet Analysis

if tubs[cls] is not empty then

for obj ∈ detection result do

obj = 0

Smax
for tub ∈ tubs[cls] do

1: for cls ∈ classes do
2:
3:
4:
5:
6:
7:

Sobj,tub =(7)
if Sobj,tub > Smax
obj
Smax
obj = Sobj,tub
candidate = tub

then

end if
end for
if Smax

obj > T then
obj = objcandidate[id]

end if
end for
for tub ∈ tubs[cls] do

end if
end for

end if
for obj ∈ {obj−1} do

if obj[conf ] > G then
obj = objnew id

end if
end for
update tubs[cls]

25:
26:
27:
28: end for

8:
9:
10:
11:
12:

13:
14:
15:
16:
17:
18:

19:
20:
21:
22:
23:
24:

if len(objtub[id])> 1 then

preserve only one with maximal S

6

• Low computational costs are produced. That is, when
the spatial size is sampled as 7 × 7 for each object, the
length of an attention vector is 147, whereas it increases
to 75267 if low-level features or corresponding hidden
states are employed without any other operation.

Hence, the attention similarity asij between two objects i

and j and the can be formulated as a cosine distance,

2) 2DMOT15: The TSSD-OTA is an identity-aware detec-
tor, so 2DMOT15 dataset [41] is employed to evaluate tracking
performance. This is a multi-object tracking consisting of 11
training sequences. Since the annotations are available for
the training set only, we split 5 videos as the validation set
following [23], [43]. In addition, another 3 video sequences
in MOT17Det [42] dataset are employed for training.

asi,j =

avi · avj
||avi||||avj||

,

(6)

B. Runtime Performance

where av denotes the 147-dimension attention vector ﬂattened
by bilinear-sampled multi-scale attention maps.

Moreover, identiﬁcation in a video can leverage more tem-
poral coherence, so we also employ IoU o in the OTA. Due
to multiple objects in a tubelet, the similarity Sobj,tub between
an object obj and a tubelet tub can be given as

Our methods are implemented under the PyTorch frame-
work. The experiments are carried out on a workstation with
an Intel 2.20 GHz Xeon(R) E5-2630 CPU, NVIDIA TITAN-
X GPUs, 64 GB of RAM, CUDA 8.0, and cuDNN v6. The
inference time is described in Table I, and we achieve a beyond
real-time speed for temporal detection or tracking.

asobj,tub = ((cid:80)

k∈tub asobj,k)/tub len

oobj,tub = IoU(obj, tub[0])

(7)

Sobj,tub = exp(oobj,tub) × asobj,tub,

where tub len is current length of the tubelet, and tub[0]
denotes the most recent object.

Deﬁne G, T to denote the tubelet generation score, match
threshold, respectively. Suppose that each detected object is
described as obj[conf, loc, av], and each class-distinct tubelets
set is denoted as tubs[cls], each tubelet in which is given as
tub[id, objs], the OTA algorithm is presented in Algorithm 1,
where objid denotes an object with an identify id, and id = −1
represents an object without an identify. len(·) computes the
number of elements. Note that the maximal existence time
after disappearance and maximal tubelet length are restricted
when updating tubelets.

IV. EXPERIMENTS AND DISCUSSION

A. Dataset

1) ImageNet VID: We evaluate the TSSD on ImageNet VID
dataset [25], which is the biggest dataset for temporal object
detection now. The task requires algorithms to detect 30-class
targets in consecutive frames. There are 4000 videos in the
training set, containing 1181113 frames. On the other hand,
the validation set compasses 555 videos, including 176126
frames. We measure performance as mean average precision
(mAP) over the 30 classes on the validation set following [3],
[7]. In addition, ImageNet DET dataset is employed as training
assistance. The 30 categories in VID dataset are a subset of
the 200 categories in the DET dataset. Therefore, following
[15], [16], [20], [21], we train the SSD with VID and DET
(only using the data from the 30 VID classes). In reality, there
are millions of frames in VID training set, so it is hard to
train a network directly using them. Additionally, the data for
each category are imbalanced, because there are long videos
(contain more than 1000 frames) and short videos (contain
only a dozen frames). Thus, following [21], we sample at most
2000 images per class from DET, and select 10 frames in each
VID video for SSD training at the ﬁrst step. In the second and
third training steps, all the VID training videos are adopted.

TABLE I
FPS LIST ON EMPLOYED DATASETS BY THE PROPOSED METHODS.

Method
SSD
TSSD
TSSD-OTA

VID (FPS)
∼ 45
∼ 27
∼ 21

2DMOT15 (FPS)
∼ 77
∼ 30
∼ 27

C. Ablation Study on ImageNet VID

1) LH-TU: Our proposed LH-TU is effective in the fol-
lowing aspects. Firstly, redundant parameters are avoided. For
example, the original SSD contains 2.6 M parameters, and
SSD with LH-TU has 4.9 M parameters. However, if six
ConvLSTMs are employed for each feature map, the parameter
size will dramatically increase to 15.5 M, which leads to
unstable training. Secondly, as reported in [23], Conv4 3 and
Conv11 2 make relatively less contribution to detection. That
is, there are a small amount of data for oversized or tiny-size
objects. Thus, the highest-level and lowest-level ConvLSTMs
can hardly be well
if six-scale ConvLSTMs are
employed. We ﬁnd that the mAP increases by 0.92% when
LH-TU is adopted using two ConvLSTMs as temporal units
due to the feature integration brought by ConvLSTMs.

trained,

2) AC-LSTM: At ﬁrst, we qualitatively analyze the interac-
tion of the attention mechanism and ConvLSTM. As shown
in Fig. 4, the comparison of temporal and traditional attention
mechanism are presented. Note that the traditional attention
only uses current feature map as the input. As for presented
heat maps in Fig. 4, crimson means a higher probability of
being a target, whereas the mazarine indicates background
features. Moreover, multi-scale attention maps are generated
in the TSSD, and the righter maps are generated with higher-
level features. For the ease of observation, all maps in Figs. 4
and 5 have been uniﬁed to the same spatial resolution as the
input image through a bilinear upsampling operation.

We choose two challenging scenes for this test. The airplane
frames include small objects, and the bird frames contain rich
stripes, both of which are difﬁcult for attention operation.
As delineated in Fig. 4(a), (b), (e), (f), the original attention
is,
method is not able to handle these two scenes. That
although the targets are focused roughly, the background and

7

Fig. 4. Effect of the ConvLSTM for attention mechanism. There are two video
snippets containing small objects (airplane) or wild environment (bird). The
traditional attention and temporal attention mechanism are used to generate
multi-scale attention maps, in which crimson denotes higher level of concern
while mazarine represents something neglected. (a)–(b) attention maps for
airplanes generated by traditional module; (c)–(d) attention maps for airplanes
generated by temporal module; (e)–(f) attention maps for bird generated by
traditional module; (g)–(h) attention maps for bird generated by temporal
attention method; In above 4-pair maps, the former is for the ﬁrst frame while
the latter is with respect to the 20th frames. Each line in (a)–(h) is multi-scale
attention maps. From left to right, they are generated with Conv4 3, Conv7,
Conv8 2, Conv9 2, Conv10 2, Conv11 2, respectively.

small-contributed multi-scale feature maps are not suppressed
effectively. Moreover, there is no improvement across time
series. On the contrary, as illustrated in Fig. 4(c), (d), (g), (h),
the proposed attention mechanism performs better. In short,
our method not only localizes the targets more accurately,
but also suppresses the background more efﬁciently. Further,
our method is effective for scale suppression. For instance,
small objects in airplane frames are described by Conv4 3. As
delineated in Fig. 4(d), the attention maps for Conv4 3,Conv7
localize airplanes, and all the last four maps are “cold”. That
is, when it comes to larger scale, our attention mechanism
cannot ﬁnd any target, so the whole of feature map has
been suppressed. In addition, the performance of proposed
approach improves along with the accumulation of temporal
information. For example, in Fig. 4(g), (h), the attention map
for Conv4 3 can hardly ﬁnd the bird in the ﬁrst frame, but
the bird’s proﬁle is focused without overmuch background
in the 20th frame. Moreover, if attention maps for the ﬁrst
frame are compared, a conclusion can be drawn that
the
temporal attention method is better even though the temporal
information has not generated. The reason is that the temporal
attention can be trained more effectively.

On the other hand, there are beneﬁts of the AC-LSTM over
traditional ConvLSTM. Referring to Fig. 5, the ConvLSTM’s
memories for Conv4 3, Conv 7 are visualized. Small-scale

Fig. 5. Effect of the temporal attention for ConvLSTM. The ConvLSTM’s
memory (s) is visualized (by computing the L2 norm across feature channels
at each spatial location to get a saliency map). (a) original ConvLSTM’s
memory for Conv4 3; (b) ConvLSTM’s memory for Conv4 3 in AC-LSTM;
(c) original ConvLSTM’s memory for Conv7; (d) ConvLSTM’s memory for
Conv7 in AC-LSTM;.

TABLE II
EFFECTIVENESS OF VARIOUS DESIGNS. ALL MODEL ARE TRAINED AND
VALIDATED ON IMAGENET VID DATASET.

Component
Association loss?
The 3rd training stage?
RSS in the 2nd stage?
AC-LSTM?
LH-TU?
mAP(%)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

TSSD

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

65.43 65.13 64.76 64.63 63.95 63.03

memory deals with image details, but the ConvLSTM cannot
generate valid memory because it is confused by mussy fea-
tures, whereas the AC-LSTM is able to memorize the essential
information of the car without background (see Fig. 5(a–b)).
In consideration of the scale of the car, Conv7 is more crucial
to detecting it. As delineated in Fig. 5(c), the ConvLSTM
for Conv7 has a tendency to learn trivial representations that
just memorizes the inputs. Moreover, this memorization also
involves the background. Thus, not all these information is
useful for future detection, and they may incur inaccuracies.
On the contrary, the AC-LSTM produces more clear memory
with pivotal features (see Fig. 5(d)). Nevertheless, we cannot
draw a conclusion about the best temporal learning length.
Thus, the AC-LSTM learns from all previous frames, and
under the control of its forget gate, the length of the valid
memory depends on sequential learning in a speciﬁc scenario.
Owing to the above reasons, the improvement brought by AC-
LSTM is evident, i.e., the mAP rises by 1.60% based on SSD.
3) Multi-step Training: The aforementioned analysis does
not involve the third training step and RSS. As shown in
Table II, although it only brings 0.14% mAP improvement
in the second training stage, the RSS makes it possible to
conduct the third training step. Without the association loss,

TABLE III
COMPARISON OF THE TSSD AND SEVERAL PRIOR AND CONTEMPORARY APPROACHES.

Backbone

one-stage? Optical ﬂow?

Tracking? Attention? RNN? Real time?

ID? mAP

Components

Performances

Method

ofﬂine methods
STMN [26]
TPN [20]
FGFA [28]
HPVD [40]
STSN [38]
Object-link [37]

online methods
Closed-loop [18]
Seq-NMS [17]
LSTM-SSD [27]
TCNN [16]
D&T [21]
TSSD(-OTA)

VGG-16
GoogLeNet [32]
ResNet-101 [33]
ResNet-101
ResNet-101
ResNet-101

VGG-M [29]
VGG-16
MobileNet [34]
DeepID+Craft [30], [31]
ResNet-101
VGG-16

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

8

55.6
68.4
76.3
78.6
78.7
80.6

50.0
52.2
54.4
61.5
78.7
65.4

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

time efﬁciency to some extent. In terms of the backbone, some
methods based on the ResNet-101 can achieve higher mAP,
but this deeper backbone is not suited to our task. Referring
to [49], when it deals with small-size images in SSD, the
ResNet-101 has an ignorable advantage in accuracy. The main
reason is that two-stage approaches usually leverage large
input size (e.g., 1000 × 600), so deeper backbone can learn
these richer visual features more effectively. However, our
one-stage detector uses 300 × 300 input image and relatively
insufﬁcient visual features for high inference speed, so the
advantage of ResNet-101 is negligible. On the other hand,
some approaches process video sequences in a batch mode
(i.e., ofﬂine method), where future frames are also utilized
for current frame detection. Hence, such non-causal methods
are prohibitive for real-world applications. Because of its real-
time and online characteristic, the TSSD is able to detect
objects temporally for real-world applications. According to
the authors’ knowledge, our method has the following merits:
• The TSSD is the ﬁrst temporal one-stage detector achiev-
ing above 65% mAP with such small input image and
VGG-16 as a backbone.

• The TSSD-OTA is a uniﬁed framework, where a real-time
online detector is capable of detecting and identifying
objects. This framework is absent in the existing video
detection approaches.

5) Qualitative Results: We show some qualitative results on
the ImageNet VID validation set in Fig. 7. We only display
the detected bounding boxes with > 0.5 conﬁdence score.
Different colors of the bounding boxes indicate different object
identity. The proposed TSSD works better with precision and
temporal stability.

D. Tracking Performance on 2DMOT15 Dataset

We employ the 2DMOT15 dataset to jointly investigate the
TSSD-OTA, whose metrics are designed for tracking. The
MOTA considers the comprehensive tracking performance,
and the MOTP measures the tightness of the tracking results
and ground truth. the FP, PN denote the total number of false

Fig. 6. Detection performance vs. θ. This ﬁgure shows model performance
as a function of the conﬁdence threshold θ in the association loss.

we obtain 65.13% as an mAP after the ﬁnal step, but Lasso
can make further improvement.

The association loss is adopted in the third training step,
where sp = 1 assuring the training data is highly associated. In
consideration of the employed NMS, there are three parameter
settings for computing the association loss, i.e., conﬁdence
threshold θ, IoU threshold, and top k retained boxes. Because
the number of retained boxes is relatively
of the NMS,
insensitive, and the IoU threshold could be consistent with
that in the inference phase. Thus, the major implication of
Lasso is θ, which indicates what kind of objects should be
involved. As depicted in Fig. 6, the mAP generally decreases
as θ increasing. That is, overmuch invalid boxes are considered
when θ = 0.01, whereas the Lasso gradually loses efﬁcacy as
the decrease of involved objects. Hence, taking into account
almost all positive samples, Lasso is the most effective when
θ = 0.1. Consequently, the multi-step training brings 0.8%
mAP improvement, and the TSSD achieves 65.43% in mAP.
4) Comparison with Other Architectures: We also compare
the TSSD against several prior and contemporary approaches.
As shown in Table III, their components and performances
have been summarized. Most methods are based on a two-
stage detector with RPN for region proposal, and few ap-
proaches successfully adopt attention or LSTM for temporal
coherence. Additionally, tracking employed in TCNN [16] and
D&T [21] is a good idea for enhancing recall rate, but it affects

9

Fig. 7. Demonstration results on the ImageNet VID validation dataset. The proposed TSSD(-OTA) can handle a variety of scenes with multiple objects more
accurately. In addition, different from traditional detectors, our approach has the capability of identifying. The detection results are shown as “ID:class:conf ”.
ID=-1 means the identity is not generated.

positives and false negatives, respectively. For each trajectory,
if more than 80% of positions are successfully tracked, the
MT increases by 1. On the other hand, if more than 80% of
positions are lost, the ML increases by 1. Finally, the IDS
counts ID switch times.

1) Parameter Analysis: There are three parameters in the
OTA, i.e., match threshold T , tubelet generation score G, and
tubelet length tub len. To track each detected object, G should
equal to detector’s conﬁdence threshold conf , and T , tub len
will impact the tracking performance. We let conf = G =
0.3, tub len = 10 to investigate the effect of T . The change
of MOTA and IDS is employed to represent the variation of
tracking performance here. Referring to Fig. 8(a), the MOTA
and IDS are strongly inﬂuenced by T , and T = 1.0 is optimal
as for our validation set. That is, if T is too high, the number
of the failed match will increase. On the other hand, lower T

causes the false match. Both failed and false match can weaken
tracking performance. However, as shown in Fig. 8(a), failed
match has a greater impact, because S max
prevents the OTA
from overmuch false match. as illustrated in Fig. 8(b), the
IDS also ﬁrst decreases and then increases as tub len rising
(T = 1.0). If tub len = 1, the match process only relies on
the most recent object, so it is unreliable for some emergencies
(e.g., occlusion). However, an oversized tub len would retain
remote information, which may result in inaccurate.

obj

We also unveil the effectiveness of attention maps for IDS
with the parameters of conf = G = 0.3, tub len = 10, T =
1.0. Note that the attention map for Conv11 2, whose size
is 1 × 1, is not qualiﬁed for identiﬁcation, so it is not been
involved in this test. Referring to Table IV, the IDS is equal
to 550 when S is computed with IoU alone (T = 0.5, S =
o). The validity of attention maps is evident, and IDS drops

(a)

(b)

10

any objective related to identity or association. The proposed
association loss is also not adopted in this experiment, and we
will explain the reason in Section IV-E.

Then, employing the TSSD as the detector, we compare the
OTA and SORT [45] comprehensively. As shown in Table V,
we achieve better indexes on MOTA, MT, ML, FN, and IDS.
That is, the OTA generates better tracking performance while
keeping quite high processing speed.

3) Qualitative Results: As schematically illustrated in
Fig. 9, the proposed TSSD-OTA is able to track pedestrians
in a variety of scenarios, and we demonstrate some typical
results. In the ﬁrst line of Fig. 9, #56 is in a small size
at the beginning, then it becomes larger with the change of
visual perspective. As a result, the TSSD-OTA can adapt to
this continuous scale change. #149 in the second line shuttles
in the crowd, and it undergoes illumination changes. Still, this
challenging target is tracked well by the TSSD-OTA. There
are chaotic small objects in the third line of Fig. 9, and they
occlude each other. Nevertheless, our method works well in
terms of #0, #3, #4, #23, #30, and etc. Unfortunately, there
are some failed cases appear in this scenario, e.g., ID switches
(#15→#39, #26→#23, #34→#30), false negatives (#4,#49).

We employ ImageNet VID and 2DMOT15 datasets for
experiments, which are signiﬁcantly different as for video
amount (4000 v.s. 8), class number (30 v.s. 1), and metrics
(mAP v.s. MOTA and etc.). In addition, 2DMOT15 contains
vast small objects while VID covers multifarious scenarios.
Hence, we discuss the following topics in detail.

1) RSS: As shown in Table II, there are only 0.14% mAP
increase after RSS is adopted. However, the reduced data
amount highlights the need for it. For example, the TSSD
cannot be well trained without RSS on the 2DMOT15 dataset,
resulting in that the MOTA drops 1.4 after the third training
step. Obviously, the phenomenon is caused by the diversity
of training data. That is, due to a variety of scenarios in VID
dataset, the data diversity can be preserved without the RSS.
Unfortunately, this trick cannot be neglected when training
2DMOT15 dataset, since there are only 8 employed videos.
Since our association loss requires continuous sampling, it is
not adopted during 2DMOT15 training.

2) Association Loss: It is not the ﬁrst time that an associa-
tion loss is designed. For instance, Lu et al. also exploited one
to train an LSTM [23]. As opposed to previous design, our
association loss focuses on global classiﬁcation rather than
each associated object pair, forgoing the need to introduce
identity ground truth labels to the training process. It
is
necessary because detection datasets usually do not involve
the identity label, or collecting ID-labelled data is a costly
work. Although the OTA can generate objects’ identities, it
is not suitable for computing association loss. That is, the
OTA also cannot be supervised for lack of ID labels, so its
errors are directly uncorrectable during training. Furthermore,
the OTA is closely dependent on TSSD’s results, and it cannot
perform well if the TSSD is not well trained. If association
loss is computed with the OTA, some match errors could occur,
making the training process unstable.

Fig. 8. Tracking performance vs. match threshold T and tubelet length
tub len.

TABLE IV
EFFECTIVENESS OF ATTENTION MAPS FOR IDS. THE CHECKMARK
INDICATES THAT CORRESPONDING ATTENTION MAP IS EMPLOYED FOR S.

Conv4 3

Conv7

Conv8 2

Conv9 2

Conv10 2

E. Discussion

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

IDS
550
236
240
238
244
251
236
235
236
237

about 60% as a result. In addition, the effectiveness gradually
declines as visual scale increasing, since high-level features
usually contain more within-class-similar information. Further,
we also combine multi-scale attention maps to compute S, and
the best result is obtained when attention maps in low-level
AC-LSTM are employed.

2) Tracking Results: Under the conditions of tub len =
10, T = 1.0, conf = G = 0.3, we use the TSSD-OTA to
track pedestrians in 2DMOT15 dataset. Referring to Table V,
the FPS is described in the form of overall speed / tracking
component speed. A conclusion can be drawn based on the
comparison with prior and contemporary approaches that the
proposed TSSD-OTA can run at frame-rates beyond real time
while maintaining comparable tracking performance. Concern-
ing the MT and ML, the FasterRCNN-SORT performs better
because the two-stage detector works well on recall rate. In
case of MOTA, the SSD-ALSTM is better owing to their
LSTM-based tracking component. However, their solutions
come with a major drawback, i.e., high time cost. There are
two main reasons causing the proposed TSSD-OTA cannot be
equal to the compared methods on some criteria. i) There are
vast small objects in the 2DMOT15 dataset, but our detector
with such small input image is not adept at dealing them;
is optimized without
ii) Unlike most methods, our model

11

TABLE V
TRACKING PERFORMANCE ON THE 2DMOT15 VALIDATION SET.

MOTA ↑ MOTP ↑ MT ↑ ML ↓

Method
ACF-MDP [43], [46]
SSD-ALSTM [23]
FasterRCNN-SORT [45]
TSSD-SORT
TSSD-OTA(propsed)

26.7
38.6
34.0
25.6
29.2

FP ↓
12.0% 51.7% 3290
14.9% 46.8%
788
20.5% 32.1% 3311
798
48.9%
6.4%
12.4% 34.6% 1482

FN ↓
13491
13253
11660
16121
14624

IDS ↓
133
154
274
260
235

FPS ↑
∼< 1/1
∼ 9/12
∼ 5/220
∼ 28/420
∼ 27/270

73.6
74.2
73.3
72.4
71.9

Fig. 9. Demonstration results on the 2DMOT15 validation dataset. The TSSD-OTA can track pedestrians in a variety of multi-target scenes. The identities
for each object are denoted on boxes, and some representative results are demonstrated in red, green, blue, or orange.

3) Trade-off Between Accuracy and Speed: In our experi-
ments, the average inference time of SSD, SSD+ConvLSTM,
SSD+AC-LSTM are 0.022 s, 0.026 s, and 0.037 s, respec-
tively. Thus, the attention module has a bigger impact on
the detection speed because it conducts a 3-layer convolution
(ConvLSTM only needs a 1-layer convolution). Thus, sim-
plifying the attention module could be in favor of inference
speed, but obviously, the attention performance could also
decline. Besides, special designs for ConvLSTM can increase
the speed as well, e.g., Bottleneck-LSTM [27]. The method for
integrating temporal information is an essential step towards
high detection accuracy. On one hand, single-object tracking
[21] is more effective than LSTM. (Note that multi-object
tracking or tracking-by-detection methods, like OTA, cannot
be leveraged for this purpose because it needs detection results
at every time steps.) However, single-object tracking methods
usually have adverse effects on inference speed. On the other
hand, it is well known that SSD’s performance comes with
a series of data augmentation, e.g., SSD randomly expands
and crops the original training images with additional random
photometric distortion [7]. However, most existing data aug-
mentation methods do not suitable for sequential learning (i.e.,
they would change the spatiotemporal relation), so the second

and third training steps do not include any data augmentation.
Thus, exploiting a sequential data augmentation during train-
ing could further improve the detection accuracy. By and large,
the TSSD achieves a reasonable trade-off between accuracy
and speed.

4) OTA: As for the OTA, there have been some similar
methods, e.g., [50], [51] analyzed spatiotemporal action based
on tubelets. These approaches and the OTA have a similar
purpose, i.e., linking detected boxes to existing tubelets for
data association. However, we have two key advantages: 1)
They ﬁrst select “candidates” using IoU threshold. On the
contrary, we treat the IoU threshold as a weight that impacts
the similarity score (see (7)), so the OTA could be more
computationally efﬁcient; 2) Match score in existing methods
is usually based on the mean of the conﬁdence scores of the
tube’s member detection boxes. These conﬁdence scores could
suffer from unstable ﬂuctuations because pixel-level changes
could signiﬁcantly impact the conﬁdence score. Besides, the
method in [50], [51] only deals with a small number of
objects, but dozens of objects with similar conﬁdence scores
should be considered for multi-object tracking task. Thus, the
method proposed by [50], [51] is not discriminative enough
for tracking numerous objects. Conversely, we use attention

12

In the future, we plan to further improve the inference speed
of the TSSD-OTA. Besides, the TSSD-OTA will be used for
robotic visual navigation under dynamic environments.

REFERENCES

[1] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature
hierarchies for accurate object detection and semantic segmentation,”
in Proc. IEEE Conf. Comput. Vis. Pattern Recognition, Columbus, U.S.,
Jun. 2014, pp. 580–587.

[2] R. Girshick. “Fast R-CNN,” in Proc. IEEE Int. Conf. Comput. Vis.,

Santiago, Chile, Dec. 2015, pp. 1440–1448.

[3] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-
time object detection with region proposal networks,” in Proc. Adv.
Neural Info. Process Syst., Montreal, Canada, Dec. 2015, pp. 91–99.
[4] K. He, G. Gkioxari, P. Dollar, and R. Girshick. “Mask R-CNN,” in Proc.
IEEE Int. Conf. Comput. Vis., Venice, Italy, Oct. 2017, pp. 2961–2969.
[5] J. Dai, Y. Li, K. He, and J. Sun, “R-FCN: Object detection via region-
based fully convolutional networks,” in Proc. Adv. Neural Info. Process
Syst., Barcelona, Spain, Dec. 2016, pp. 379–387.

[6] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look
once: Uniﬁed, real-time object detection,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognition, Las Vegas, U.S., Jun. 2016, pp. 779–788.
[7] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Y. Fu, and
A. C. Berg, “SSD: single shot multibox detector,” in Proc. Eur. Conf.
Comput. Vis., Amsterdam, Netherlands, Oct. 2016, pp. 21–37.

[8] T. Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, “Focal loss for
dense object detection,” in Proc. IEEE Int. Conf. Comput. Vis., Venice,
Italy, Oct. 2017, pp. 2980–2988.

[9] Y. Zhang, M. Pezeshki, P. Brakel, S. Zhang, C. L. Y. Bengio, and
A. Courville, “Towards end-to-end speech recognition with deep convo-
lutional neural networks,” arXiv:1701.02720, 2017.

[10] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
with neural networks,” in Proc. Adv. Neural Info. Process Syst., Mon-
treal, Canada, Dec. 2014, pp. 3104–3112.

[11] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

Comput., vol. 9, no. 8, pp. 1735–1780, 1997.

[12] X. Shi, Z. Chen, H. Wang, D. Yeung, W. Wong, and W. Woo, “Convo-
lutional LSTM network: A machine learning approach for precipitation
nowcasting,” in Proc. Adv. Neural Info. Process Syst., Montreal, Canada,
Dec. 2015, pp. 802–810.

[13] S. Valipour, M. Siam, M. Jagersand, and N. Ray, “Recurrent fully con-
volutional networks for video segmentation,” arXiv:1606.00487, 2016.
[14] V. Mnih, N. Heess, and A. Graves, “Recurrent models of visual
attention,” in Proc. Adv. Neural Info. Process Syst., Montreal, Canada,
Dec. 2014, pp. 2204–2212.

[15] K. Kang, W. Ouyang, H. Li, and X. Wang, “Object detection from
video tubelets with convolutional neural networks,” in Proc. IEEE Conf.
Comput. Vis. Pattern Recognition, Las Vegas, U.S., Jun. 2016, pp. 817–
825.

[16] K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang, Z. Wang,
R. Wang, X. Wang, and W. Ouyang, “T-CNN: Tubelets with convolu-
tional neural networks for object detection from videos”, IEEE Trans.
Circuits Syst. Video Technol., DOI: 10.1109/TCSVT.2017.2736553.
[17] W. Han, P. Khorrami, T. L. Paine, P. Ramachandran, M. Babaeizadeh,
H. Shi, J. Li, S. Yan, and T. S. Huang, “Seq-NMS for video object
detection,” arXiv:1602.08465, 2016.

[18] L. Galteri, L. Seidenari, M. Bertini, and A. Del Bimbo, “Spatio-temporal
closed-loop object detection,” IEEE Trans. Image Process., vol. 26, no.
3, pp. 1253–1263, 2017.

[19] S. Tripathi, S. Belongie, Y. Hwang, and T. Nguyen, “Detecting tempo-
rally consistent objects in videos through object class label propagation,”
in Proc. IEEE Winter Conf. Appl. Comput. Vis., New York, U.S., Mar.
2016, pp. 1–9.

[20] K. Kang H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,
“Object detection in videos with tubelet proposal networks,” in Proc.
IEEE Conf. Comput. Vis. Pattern Recognition, Hawaii, U.S., Jul, 2017,
pp. 727–735.

[21] C. Feichtenhofer, A. Pinz, and A. Zisserman, “Detect to track and track
to detect,” in Proc. IEEE Int. Conf. Comput. Vis., Venice, Italy, Oct.
2017, pp. 3038–3046.

[22] G. Ning, Z. Zhang, C. Huang, X. Ren, H. Wang, C. Cai, and Z. He,
“Spatially supervised recurrent convolutional neural networks for visual
object tracking,” in Proc. IEEE Int. Symp. Circuits Syst., Baltimore,
USA, May 2017, pp. 1–4.

Fig. 10. Undersea object grasping and amount statistics. Sea-urchins, scallops,
and sea-cucumbers are demonstrated in red, blue and orange. Objects’
identities are denoted on the top of boxes. Besides counting the number of
objects, the TSSD-OTA can locate each target individual for grasping.

vector from a temporal detector to describe objects’ saliency
mass, and the match between boxes and tubelets is based on
the similarity of the attention vector. Thus, the OTA is in favor
of feature-level match.

F. A Real-World Application

As shown in Fig. 10, we conducted practical experiments on
the seabed using a remotely operated vehicle (ROV). Equipped
with a camera as visual guidance, the ROV is 0.68 m in length,
0.57 m in width, 0.39 m in height, and 50 kg in weight. The
test venue is located in Zhangzidao, Dalian, China, where the
water depth is approximately 10 m. For humans, managing
seaﬂoor products (e.g., sea-urchins, scallops, sea-cucumbers,
and etc.) is very difﬁcult. Moreover, their collection has been a
big problem. To that end, we utilize the ROV for this laborious
and dangerous job instead of humans. Further, the TSSD-OTA
is of crucial importance in this maneuver. On one hand, our
proposed method can accurately count the amount for each
category of sea-products when the ROV travels straightly.
On the other hand, distinguished from traditional detection
approaches, the TSSD-OTA is capable of locating each tar-
get individual for grasping. The full video demonstration is
available at https://youtu.be/v3LG-O-abWI.

V. CONCLUSION AND FUTURE WORK

This paper has aimed at temporally detecting and identifying
objects in real time for real-world applications. A creative
TSSD is proposed. Differing from existing video detection
methods, the TSSD is a temporal one-stage detector, and it
can perform well in terms of both detection precision and
inference speed. To efﬁciently integrate pyramidal feature
hierarchy, an LH-TU is proposed, in which the high-level
features and low-level features share their respective temporal
units. Furthermore, we design an AC-LSTM as a temporal
analysis unit, where the temporal attention mechanism is
responsible for background suppression and scale suppression.
A novel association loss function and multi-step training are
also designed for sequence learning. In addition, the OTA
algorithm equips the TSSD with the ability of identiﬁcation
with low computational costs. As a result, the TSSD-OTA sees
considerably enhanced detection precision, tracking perfor-
mance, and inference speed. Finally, our proposed approaches
have been employed for real-world applications.

13

[50] G. Singh, S. Saha, M. Sapienza, P. Torr, and F. Cuzzolin, “Online real-
time multiple spatiotemporal action localisation and prediction,” in Proc.
IEEE Int. Conf. Comput. Vis., Venice, Italy, Oct. 2017, pp. 3657–3666.
[51] V. Kalogeiton, P. Weinzaepfel, V. Ferrari, and C. Schmid, “Action tubelet
detector for spatio-temporal action localization,” in Proc. IEEE Int. Conf.
Comput. Vis., Venice, Italy, Oct. 2017, pp. 4405–4413.

[23] Y. Lu, C. Lu, and C. K. Tang, “Online video object detection using
association LSTM, in Proc. IEEE Int. Conf. Comput. Vis., Venice, Italy,
Oct. 2017, pp. 2344–2352.

[24] K. Simonyan and A. Zisserman, “Very deep convolutional networks for

large-scale image recognition,” arXiv:1409.1556, 2014.

[25] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and F. Li,
“ImageNet large scale visual recognition challenge,” Int. J. Comput. Vis.,
vol. 115, no. 3, pp. 211–252, 2015.

[26] F. Xiao and Y. J. Lee, “Spatial-temporal memory networks for video

object detection,” arXiv:1712.06317, 2017.

[27] M. Liu and M. Zhu, “Mobile video object detection with temporally-
aware feature maps,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog-
nition, Salt Lack City, USA, Jun. 2018, pp. 5686–5695.

[28] X. Zhu, Y. Wang, J. Dai, L. Yuan, and Y. Wei, “Flow-guided feature
aggregation for video object detection,” arXiv:1703.10025, 2017.
[29] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of
the devil in the details: delving deep into convolutional nets,” in Proc.
Brit. Mach. Vis. Conf., Nottingham, U.K., Sep. 2014, pp. 1–12.
[30] W. Ouyang, P. Luo, X. Zeng, S. Qiu, Y. Tian, H. Li, S. Yang, Z. Wang,
Y. Xiong, and C. Qian, “Deepid-net: Multi-stage and deformable deep
convolutional neural networks for object detection,” arXiv:1409.3505,
2014.

[31] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Craft objects from images,” in
Proc. IEEE Conf. Comput. Vis. Pattern Recognition, Las Vegas, U.S.,
Jun. 2016, pp. 6043–6051.

[32] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich. “Going deeper with convolutions,” in
Proc. IEEE Conf. Comput. Vis. Pattern Recognition, Boston, U.S., Jun.
2015, pp. 1–9.

[33] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognition, Las
Vegas, U.S., Jun. 2016, pp. 770–778.

[34] A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand,
M. Andreetto, and H. Adam. “Mobilenets: Efﬁcient convolutional neural
networks for mobile vision applications,” arXiv:1704.04861, 2017.
[35] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui, “Visual
object tracking using adaptive correlation ﬁlters,” in Proc. IEEE Conf.
Comput. Vis. Pattern Recognition, San Francisco, U.S., Jun. 2010, pp.
2544–2550.

[36] T. Tieleman and G. Hinton, “Lecture 6.5-rmsprop: Divide the gradient
by a running average of its recent magnitude,” COURSERA: Neural
Networks for Machine Learning, vol. 4, no. 2, pp. 26–31, 2012.
[37] P. Tang, C. Wang, X. Wang, W. Liu, W. Zeng, and J. Wang, “Ob-
linking,”

ject detection in videos by short and long range object
arXiv:1801.09823, 2018.

[38] G. Bertasius, L. Torresani, and J. Shi, “Object detection in video with

spatiotemporal sampling networks,” arXiv:1803.05549, 2018.

[39] N. Srivastava, G. E. Hinton, A. Krizhevsky,

I. Sutskever, and
R. Salakhutdinov, “Dropout: A simple way to prevent neural networks
from overﬁtting,” J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929–1958,
2014.

[40] X. Zhu, J. Dai, L. Yuan, and Y. Wei, “Towards high performance video

object detection,” arXiv:1711.11577, 2017.

[41] L. Leal-Taixe, A. Milan,

I. Reid, S. Roth, and K. Schindler,
“Motchallenge 2015: Towards a benchmark for multi-target tracking,”
arXiv:1504.01942, 2015.

[42] A. Milan, L. Leal-Taixe, I. Reid, S. Roth, and K. Schindler, “MOT16:
A benchmark for multi-object tracking,” arXiv:1603.00831, 2016.
[43] Y. Xiang, A. Alahi, and S. Savarese, “Learning to track: Online multi-
object tracking by decision making,” in Proc. IEEE Int. Conf. Comput.
Vis., Santiago, Chile, Dec. 2015, pp. 4705–4713.

[44] H. U. Kim and C. S. Kim, “CDT: Cooperative detection and tracking for
tracing multiple objects in video sequences. in Proc. Eur. Conf. Comput.
Vis., Amsterdam, Netherlands, Oct. 2016, pp. 851–867.

[45] A. Bewley, Z. Ge, L. Ott, F. Ramos, and B. Upcroft, “Simple online
and realtime tracking,” in IEEE Int. Conf. Image Process., Phoneix,
U.S., Sep. 2016, pp. 3464–3468.

[46] P. Dollar, R. Appel, S. Belongie, and P. Perona, ”Fast feature pyramids
for object detection,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 36,
no. 8, pp. 1532–1545, 2014.

[47] R. Kalman, “A new approach to linear ﬁltering and prediction problems,”

J. Basic Eng., vol. 82, no. Series D, pp. 35–45, 1960.

[48] H. W. Kuhn, “The hungarian method for the assignment problem,” Naval

Res Logistics Quarterly, vol. 2, pp. 83–97, 1955.

[49] C. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, “DSSD: Deconvo-

lutional single shot detector,” arXiv:1701.06659, 2017.

