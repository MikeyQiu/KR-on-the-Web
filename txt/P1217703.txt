SQL-to-Text Generation with Graph-to-Sequence Model

Kun Xu1∗, Lingfei Wu2, Zhiguo Wang2, Yansong Feng3, Vadim Sheinin2
1Tencent AI Lab
2IBM Research
3Peking University, Beijing, China
{syxu828,zgw.tomorrow}@gmail.com, lwu@email.wm.edu
fengyansong@pku.edu.cn, vadims@us.ibm.com

9
1
0
2
 
b
e
F
 
2
1
 
 
]
L
C
.
s
c
[
 
 
2
v
5
5
2
5
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

Previous work approaches the SQL-to-text
generation task using vanilla Seq2Seq mod-
els, which may not fully capture the inherent
graph-structured information in SQL query. In
this paper, we ﬁrst introduce a strategy to rep-
resent the SQL query as a directed graph and
then employ a graph-to-sequence model to en-
code the global structure information into node
embeddings. This model can effectively learn
the correlation between the SQL query pattern
and its interpretation. Experimental results
on the WikiSQL dataset and Stackoverﬂow
dataset show that our model signiﬁcantly out-
performs the Seq2Seq and Tree2Seq baselines,
achieving the state-of-the-art performance.

1

Introduction

The goal of the SQL-to-text task is to automati-
cally generate human-like descriptions interpret-
ing the meaning of a given structured query lan-
guage (SQL) query (Figure 1 gives an example).
This task is critical to the natural language inter-
face to a database since it helps non-expert users
to understand the esoteric SQL queries that are
used to retrieve the answers through the question-
answering process (Simitsis and Ioannidis, 2009)
using varous text embeddings techniques (Kim,
2014; Arora et al., 2017; Wu et al., 2018a).

Earlier attempts for SQL-to-text task are rule-
based and template-based (Koutrika et al., 2010;
Ngonga Ngomo et al., 2013). Despite requiring
intensive human efforts to design temples or rules,
these approaches still tend to generate rigid and
stylized language that lacks the natural text of the
human language. To address this, Iyer et al. (2016)
proposes a sequence-to-sequence (Seq2Seq) net-
work to model the SQL query and natural lan-
guage jointly. However, since the SQL is designed

∗ Work done when the author was at IBM Research.

Figure 1: An example of SQL query and its interpretation.

to express graph-structured query intent, the se-
quence encoder may need an elaborate design to
fully capture the global structure information. In-
tuitively, varous graph encoding techniques base
on deep neural network (Kipf and Welling, 2016;
Hamilton et al., 2017; Song et al., 2018) or based
on Graph Kernels (Vishwanathan et al., 2010; Wu
et al., 2018b), whose goal is to learn the node-level
or graph-level representations for a given graph,
are more proper to tackle this problem.

In this paper, we ﬁrst introduce a strategy to
represent the SQL query as a directed graph (see
§2) and further make full use of a novel graph-
to-sequence (Graph2Seq) model (Xu et al., 2018)
that encodes this graph-structured SQL query, and
then decodes its interpretation (see §3). On the en-
coder side, we extend the graph encoding work of
Hamilton et al. (2017) by encoding the edge direc-
tion information into the node embedding. Our en-
coder learns the representation of each node by ag-
gregating information from its K-hop neighbors.
Different from Hamilton et al. (2017) which ne-
glects the edge direction, we classify the neighbors
of a node according to the edge direction, say v,
into two classes, i.e., forward nodes (v directs to)
and backward nodes (direct to v). We apply two
distinct aggregators to aggregate the information
of these two types of nodes, resulting two repre-
sentations. The node embedding of v is the con-
catenation of these two representations. Given the
learned node embeddings, we further introduce a
pooling-based and an aggregation-based method
to generate the graph embedding.

On the decoder side, we develop an RNN-based
decoder which takes the graph vector representa-
tion as the initial hidden state to generate the se-
quences while employing an attention mechanism
over all node embeddings. Experimental results
show that our model achieves the state-of-the-art
performance on the WikiSQL dataset and Stack-
overﬂow dataset. Our code and data is available at
https://github.com/IBM/SQL-to-Text.

Figure 2: The graph representation of the SQL query in Fig-
ure 1.

2 Graph Representation of SQL Query

3 Graph-to-sequence Model

Representing the SQL query as a graph instead
of a sequence could better preserve the inherent
structure information in the query. An example is
illustrated in the blue dashed frame in Figure 2.
One can see that representing them as a graph in-
stead of a sequence could help the model to bet-
ter learn the correlation between this graph pattern
and the interpretation “...both X and Y higher than
Z...”. This observation motivates us to represent
the SQL query as a graph. In particular, we use
the following method to transform the SQL query
to a graph:1

SELECT Clause. For the SELECT clause such
as “SELECT company”, we ﬁrst create a node as-
signed with text attribute select. This SELECT
node connects with column nodes whose text at-
tributes are the selected column names such as
company. For SQL queries that contain aggrega-
tion functions such as count or max, we add one
aggregation node which is connected with column
nodes. Similarly, their text attributes are the ag-
gregation function names.

WHERE Clause. The WHERE clause usually
contains more than one condition. For each condi-
tion, we use the same process as for the SELECT
clause to create nodes. For example, in Figure 2,
we create node assets and >val0 for the ﬁrst con-
dition, the node sales and >val0 for the second
condition. We then integrate the constraint nodes
that have the same text attribute (e.g., >val0 in
Figure 2). For a logical operator such as AND, OR
and NOT, we create a node that connects with all
column nodes that the operator works on. Finally,
these logical operator nodes connect with the SE-
LECT node.

1This method could be simply extended to cope with more
general SQL queries that have complex syntaxes such as
JOIN and ORDER BY.

Based on the constructed graphs for the SQL
queries, we make full use of a novel graph-to-
sequence model (Xu et al., 2018), which consists
of a graph encoder to learn the embedding for the
graph-structured SQL query, and a sequence de-
coder with attention mechanism to generate sen-
tences. Conceptually, the graph encoder generates
the node embedding for each node by accumu-
lating information from its K-hop neighbors, and
produces a graph embedding for the entire graph
by abstracting all node embeddings. Our decoder
takes the graph embedding as the initial hidden
state and calculates the attention over all node em-
beddings on the encoder side to generate natural
language interpretations.

Node Embedding. Given the graph G = (V, E),
since the text attribute of a node may include a list
of words, we ﬁrst use a Long Short Term Memory
(LSTM) to generate the feature vector av for all
nodes ∀v ∈ V from v’s text attribute. We use these
feature vectors as initial node embeddings. Then,
our model incorporates information from a node’s
neighbors within K hop into its representation by
repeating the following process K times:

(cid:96)({hk−1

h0
v(cid:96) = av, h0
hk
N(cid:96)(v) = Mk
v(cid:96) = σ(Wk · CONCAT(hk−1
hk
hk
N(cid:97)(v) = Mk
v(cid:97) = σ(Wk · CONCAT(hk−1
hk

(cid:96)v = av, ∀v ∈ V
u(cid:96) , ∀u ∈ N(cid:96)(v)})
v(cid:96) , hk
N(cid:96)(v)))
u(cid:97) , ∀u ∈ N(cid:97)(v)})
v(cid:97) , hk
N(cid:97)(v)))

(cid:97)({hk−1

(1)

(2)

(3)

(4)

(5)

where k ∈ {1, ..., K} is the iteration index, N is
the neighborhood function2, hk
v(cid:97)) is node v’s
forward (backward) representation which aggre-
gates the information of nodes in N(cid:96)(v) (N(cid:97)(v)),

v(cid:96) (hk

2N(cid:96)(v) returns the nodes that v directs to and N(cid:97)(v) re-

turns the nodes that direct to v.

(cid:96) and Mk

Mk
(cid:97) are the forward and backward aggre-
gator functions, Wk denotes weight matrices, σ is
a non-linearity function.

For example, for node v ∈ V, we ﬁrst aggre-
gate the forward representations of its immediate
neighbors {hk−1
u(cid:96) , ∀u ∈ N(cid:96)(v)} into a single vec-
tor hk
N(cid:96)(v) (equation 2). Note that this aggrega-
tion step only uses the representations generated
at previous iteration and its initial representation is
av. Then we concatenate v’s current forward rep-
resentation hk−1
v(cid:96) with the newly generated neigh-
borhood vector hk
N(cid:96)(v). This concatenated vector
is fed into a fully connected layer with nonlinear
activation function σ, which updates the forward
representation of v to be used at the next itera-
tion (equation 3). Next, we update the backward
representation of v in the similar fashion (equa-
tion 4∼5). Finally, the concatenation of the for-
ward and backward representation at last itera-
tion K, is used as the resulting representation of
v. Since the neighbor information from different
hops may have the different impact on the node
embedding, we learn a distinct aggregator function
at each step. This aggregator feeds each neigh-
bor’s vector to a fully-connected neural network
and an element-wise max-pooling operation is ap-
plied to capture different aspects of the neighbor
set.

Graph Embedding. Most existing works of
graph convolution neural networks focus more on
node embeddings rather than graph embeddings
(GE) since their focus is on the node-wise clas-
siﬁcation task. However, graph embeddings that
convey the entire graph information are essential
to the downstream decoder, which is crucial to our
task. For this purpose, we propose two ways to
generate graph embeddings, namely, the Pooling-
based and Node-based methods.

Pooling-based GE. This method feeds the ob-
tained node embeddings into a fully-connected
neural network and applies the element-wise max-
pooling operation on all node embeddings.
In
experiments, we did not observe signiﬁcant per-
formance improvement using min-pooling and
average-pooling.

Node-based GE. Following (Scarselli et al.,
2009), this method adds a super node vs that is
connected to all other nodes by a special type of
edge. The embedding of vs, which is treated as
graph embedding, is produced using node embed-
ding generation algorithm mentioned above.

Sequence Decoding. The decoder is an RNN
which predicts the next token yi given all the pre-
vious words y<i = y1, ..., yi−1, the RNN hid-
den state si for time-step i and the context vector
ci that captures the attention of the encoder side.
In particular, the context vector ci depends on a
set of node representations (h1,...,hV ) to which
the encoder maps the input graph. The context
vector ci is dynamically computed using attention
mechanism over the node representations. Our
model is jointly trained to maximize the condi-
tional log-probability of the correct description
given a source graph with respect to the parame-
ters θ of the model:

θ∗ = arg max

log p(yn

t |yn

<t, xn)

N
(cid:88)

Tn(cid:88)

θ

n=1

t=1

where (xn, yn) is the n-th SQL-interpretation pair
in the training set, and Tn is the length of the n-th
target sentence yn. In the inference phase, we use
the beam search algorithm with beam size = 5.

4 Experiments

We evaluate our model on two datasets, WikiSQL
(Zhong et al., 2017) and Stackoverﬂow (Iyer et al.,
2016). WikiSQL consists of a corpus of 87,726
hand-annotated SQL query and natural language
question pairs. These SQL queries are further
split into training (61,297 examples), development
(9,145 examples) and test sets (17,284 examples).
StackOverﬂow consists of 32,337 SQL query and
natural language question pairs, and we use the
same train/development/test split as (Iyer et al.,
2016). We use the BLEU-4 score (Papineni et al.,
2002) as our automatic evaluation metric and also
perform a human study. For human evaluation,
we randomly sampled 1,000 predicted results and
asked three native English speakers to rate each in-
terpretation against both the correctness conform-
ing to the input SQL and grammaticality on a
scale between 1 and 5. We compare some vari-
ants of our model against the template, Seq2Seq,
and Tree2Seq baselines.

Graph2Seq-PGE. This method uses

the
Pooling method for generating Graph Embedding.
Graph2Seq-NGE. This method uses the Node

based Graph Embedding.

Template. We implement a template-based
method which ﬁrst maps each element of a SQL
query to an utterance and then uses simple rules
to assemble these utterances. For example, we

BLEU-4 Grammar.

Correct.

Template
Seq2Seq
Seq2Seq + Copy
Tree2Seq
Graph2Seq-PGE
Graph2Seq-NGE
(Iyer et al., 2016)
Graph2Seq-PGE
Graph2Seq-NGE

15.71
20.91
24.12
26.67
38.97
34.28
18.4
23.3
21.9

1.50
2.54
2.65
2.70
3.81
3.26
3.16
3.23
2.97

-
62.1%
64.5%
66.8%
79.2%
75.3%
64.2%
70.2%
65.1%

SQL Query & Interpretations
1. COUNT Player WHERE starter = val0 AND touchdowns

= val1 AND position = val2

S: How many players played in position val2
G: number of players with starter val0 and get touchdowns
val1 for val2
2. SELECT Tires WHERE engine = val0 AND chassis =

val1 AND team = val2

S: which tire has engine val0 and chassis val1 and val2
G: which tire does val2 run with val0 engine and val1 chassis

Table 1: Results on the WikiSQL (above) and Stackoverﬂow
(below).

Table 2: Example of SQL queries and predicted interpreta-
tions where S and G denotes Seq2Seq and Graph2Seq mod-
els, respectively.

map SELECT to which, WHERE to where, > to
more than. This method translates the SQL query
of Figure 1 to which company where assets more
than val0 and sales more than val0 and industry
less than or equal to val1 and proﬁts equals val2.
Seq2Seq. We choose two Seq2Seq models
as our baselines. The ﬁrst one is the attention-
based Seq2Seq model proposed by Bahdanau et al.
(2014), and the second one additionally introduces
the copy mechanism in the decoder side (Gu et al.,
2016). To evaluate these models, we employ
a template to convert the SQL query into a se-
quence: “SELECT + <aggregation function> + <Split
Symbol> + <selected column> + WHERE + <condition0>
+ <Split Symbol> + <condition1> + ... ”.

Tree2Seq. We also choose a tree-to-sequence
model proposed by (Eriguchi et al., 2016) as our
baseline. We use the SQL Parser tool3 to convert a
SQL query into the tree structure4 which is fed to
the Tree2Seq model.

Our proposed models are trained using the
Adam optimizer (Kingma and Ba, 2014), with
mini-batch size 30. Our hyper-parameters are set
based on performance on the validation set. The
learning rate is set to 0.001. We apply the dropout
strategy (Srivastava et al., 2014) with the ratio of
0.5 at the decoder layer to avoid overﬁtting. Gra-
dients are clipped when their norm is bigger than
20. We initialize word embeddings using GloVe
word vectors from Pennington et al. (2014), and
the word embedding dimension is 300. For the
graph encoder, the hop size K is set to 6, the non-
linearity function σ is implemented as ReLU (Glo-
rot et al., 2011), the parameters of weight matrices
Wk are randomly initialized. The decoder has one
layer, and its hidden state size is 300.

3http://www.sqlparser.com
4See Appendix for details.

Results and Discussion Table 1 summarizes the
results of our models and baselines. Although
the template-based method achieves decent BLEU
scores, its grammaticality score is substantially
worse than other baselines. We can see that on
both two datasets, our Graph2Seq models per-
form signiﬁcantly better than the Seq2Seq and
Tree2Seq baselines. One possible reason is that
in our graph encoder, the node embedding retains
the information of neighbor nodes within K hops.
However, in the tree encoder, the node embed-
ding only aggregates the information of descen-
dants while losing the knowledge of ancestors.
The pooling-based graph embedding is found to
be more useful than the node-based graph em-
bedding because Graph2Seq-NGE adds a nonex-
istent node into the graph, which introduces the
noisy information in calculating the embeddings
of other nodes. We also conducted an experiment
that treats the SQL query graph as an undirected
graph and found the performance degrades.

By manually analyzing the cases in which the
Graph2Seq model performs better than Seq2Seq,
we ﬁnd the Graph2Seq model is better at inter-
preting two classes of queries: (1) the complicated
queries that have more than two conditions (Query
1); (2) the queries whose columns have implicit
relationships (Query 2). Table 2 lists some such
SQL queries and their interpretations. One possi-
ble reason is that the Graph2Seq model can better
learn the correlation between the graph pattern and
natural language by utilizing the global structure
information.

We ﬁnd the hop size has a signiﬁcant impact on
our model since it determines how many neigh-
bor nodes to be considered during the node em-
bedding generation. As the hop size increasing,
the performance is found to be signiﬁcantly im-
proved. However, after the hop size reaches 6,

increasing the hop size can not boost the perfor-
mance on WikiSQL anymore. By analyzing the
most complicated queries (around 6.2%) in Wik-
iSQL, we ﬁnd there are average six hops between
a node and its most distant neighbor. This result
indicates that the selected hop size should guar-
antee each node can receive the information from
others nodes in the graph.

5 Conclusions

Previous work approaches the SQL-to-text task
using an Seq2Seq model which does not fully cap-
ture the global structure information of the SQL
query. To address this, we proposed a Graph2Seq
model which includes a graph encoder, an atten-
tion based sequence decoder. Experimental results
show that our model signiﬁcantly outperforms the
Seq2Seq and Tree2Seq models on the WikiSQL
and Stackoverﬂow datasets.

Appendix

Figure 3: Tree representation of the SQL query.

We apply the SQL Parser tool5 to convert
an SQL query to a tree whose structure is il-
lustrated in Figure 3. More speciﬁcally,
the
root has two child nodes, namely Select List and
Where Clause. The child nodes of Select List rep-
resent the selected columns in the SQL query. The
Where Clause has the logical operators occurred
in the SQL query as its children. The children of a
logical operator node are the conditions on which
this operator works.

References

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings. In ICLR.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Neural machine translation by

Bengio. 2014.

5http://www.sqlparser.com

jointly learning to align and translate.
abs/1409.0473.

CoRR,

Akiko Eriguchi, Kazuma Hashimoto, and Yoshi-
masa Tsuruoka. 2016.
Tree-to-sequence atten-
tional neural machine translation. arXiv preprint
arXiv:1603.06075.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectiﬁer neural networks.
In
Proceedings of the Fourteenth International Confer-
ence on Artiﬁcial Intelligence and Statistics, AIS-
TATS 2011, Fort Lauderdale, USA, April 11-13,
2011, pages 315–323.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Incorporating copying mechanism in
arXiv preprint

Li. 2016.
sequence-to-sequence learning.
arXiv:1603.06393.

William L. Hamilton, Zhitao Ying, and Jure Leskovec.
2017.
Inductive representation learning on large
graphs. In Advances in Neural Information Process-
ing Systems 30: Annual Conference on Neural In-
formation Processing Systems 2017, 4-9 December
2017, Long Beach, CA, USA, pages 1025–1035.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and
Luke Zettlemoyer. 2016. Summarizing source code
In Proceedings of
using a neural attention model.
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
volume 1, pages 2073–2083.

Yoon Kim. 2014.

works for sentence classiﬁcation.
arXiv:1408.5882.

Convolutional neural net-
arXiv preprint

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Thomas N Kipf and Max Welling. 2016.

Semi-
supervised classiﬁcation with graph convolutional
networks. arXiv preprint arXiv:1609.02907.

Georgia Koutrika, Alkis Simitsis, and Yannis E Ioan-
nidis. 2010. Explaining structured queries in natural
language. In Data Engineering (ICDE), 2010 IEEE
26th International Conference on, pages 333–344.
IEEE.

Axel-Cyrille Ngonga Ngomo, Lorenz B¨uhmann,
Christina Unger, Jens Lehmann, and Daniel Gerber.
2013. Sorry, i don’t speak sparql: translating sparql
queries into natural language. In Proceedings of the
22nd international conference on World Wide Web,
pages 977–988. ACM.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, July 6-12, 2002, Philadelphia,
PA, USA., pages 311–318.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
Hagenbuchner, and Gabriele Monfardini. 2009. The
graph neural network model. IEEE Transactions on
Neural Networks, 20(1):61–80.

Alkis

Simitsis

and Yannis

Dbmss should talk back too.
arXiv:0909.1786.

Ioannidis.

2009.
arXiv preprint

Linfeng Song, Yue Zhang, Zhiguo Wang,

and
Daniel Gildea. 2018.
A graph-to-sequence
model for amr-to-text generation. arXiv preprint
arXiv:1805.02473.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: a simple way to prevent neural
Journal of Machine
networks from overﬁtting.
Learning Research, 15(1):1929–1958.

S Vichy N Vishwanathan, Nicol N Schraudolph, Risi
Kondor, and Karsten M Borgwardt. 2010. Graph
Journal of Machine Learning Research,
kernels.
11(Apr):1201–1242.

Lingfei Wu, Ian E.H. Yen, Kun Xu, Fangli Xu, Avinash
Balakrishnan, Pin-Yu Chen, Pradeep Ravikumar,
and Michael J. Witbrock. 2018a. Word mover’s em-
bedding: From word2vec to document embedding.
In EMNLP.

Lingfei Wu, Ian En-Hsu Yen, Fangli Xu, Pradeep
Ravikuma, and Michael Witbrock. 2018b. D2ke:
arXiv
From distance to kernel and embedding.
preprint arXiv:1802.04956.

Kun Xu, Lingfei Wu, Zhiguo Wang, and Vadim
Sheinin. 2018. Graph2seq: Graph to sequence
learning with attention-based neural networks.
arXiv preprint arXiv:1804.00823.

Victor Zhong, Caiming Xiong, and Richard Socher.
Seq2sql: Generating structured queries
2017.
from natural language using reinforcement learning.
CoRR, abs/1709.00103.

SQL-to-Text Generation with Graph-to-Sequence Model

Kun Xu1∗, Lingfei Wu2, Zhiguo Wang2, Yansong Feng3, Vadim Sheinin2
1Tencent AI Lab
2IBM Research
3Peking University, Beijing, China
{syxu828,zgw.tomorrow}@gmail.com, lwu@email.wm.edu
fengyansong@pku.edu.cn, vadims@us.ibm.com

9
1
0
2
 
b
e
F
 
2
1
 
 
]
L
C
.
s
c
[
 
 
2
v
5
5
2
5
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

Previous work approaches the SQL-to-text
generation task using vanilla Seq2Seq mod-
els, which may not fully capture the inherent
graph-structured information in SQL query. In
this paper, we ﬁrst introduce a strategy to rep-
resent the SQL query as a directed graph and
then employ a graph-to-sequence model to en-
code the global structure information into node
embeddings. This model can effectively learn
the correlation between the SQL query pattern
and its interpretation. Experimental results
on the WikiSQL dataset and Stackoverﬂow
dataset show that our model signiﬁcantly out-
performs the Seq2Seq and Tree2Seq baselines,
achieving the state-of-the-art performance.

1

Introduction

The goal of the SQL-to-text task is to automati-
cally generate human-like descriptions interpret-
ing the meaning of a given structured query lan-
guage (SQL) query (Figure 1 gives an example).
This task is critical to the natural language inter-
face to a database since it helps non-expert users
to understand the esoteric SQL queries that are
used to retrieve the answers through the question-
answering process (Simitsis and Ioannidis, 2009)
using varous text embeddings techniques (Kim,
2014; Arora et al., 2017; Wu et al., 2018a).

Earlier attempts for SQL-to-text task are rule-
based and template-based (Koutrika et al., 2010;
Ngonga Ngomo et al., 2013). Despite requiring
intensive human efforts to design temples or rules,
these approaches still tend to generate rigid and
stylized language that lacks the natural text of the
human language. To address this, Iyer et al. (2016)
proposes a sequence-to-sequence (Seq2Seq) net-
work to model the SQL query and natural lan-
guage jointly. However, since the SQL is designed

∗ Work done when the author was at IBM Research.

Figure 1: An example of SQL query and its interpretation.

to express graph-structured query intent, the se-
quence encoder may need an elaborate design to
fully capture the global structure information. In-
tuitively, varous graph encoding techniques base
on deep neural network (Kipf and Welling, 2016;
Hamilton et al., 2017; Song et al., 2018) or based
on Graph Kernels (Vishwanathan et al., 2010; Wu
et al., 2018b), whose goal is to learn the node-level
or graph-level representations for a given graph,
are more proper to tackle this problem.

In this paper, we ﬁrst introduce a strategy to
represent the SQL query as a directed graph (see
§2) and further make full use of a novel graph-
to-sequence (Graph2Seq) model (Xu et al., 2018)
that encodes this graph-structured SQL query, and
then decodes its interpretation (see §3). On the en-
coder side, we extend the graph encoding work of
Hamilton et al. (2017) by encoding the edge direc-
tion information into the node embedding. Our en-
coder learns the representation of each node by ag-
gregating information from its K-hop neighbors.
Different from Hamilton et al. (2017) which ne-
glects the edge direction, we classify the neighbors
of a node according to the edge direction, say v,
into two classes, i.e., forward nodes (v directs to)
and backward nodes (direct to v). We apply two
distinct aggregators to aggregate the information
of these two types of nodes, resulting two repre-
sentations. The node embedding of v is the con-
catenation of these two representations. Given the
learned node embeddings, we further introduce a
pooling-based and an aggregation-based method
to generate the graph embedding.

On the decoder side, we develop an RNN-based
decoder which takes the graph vector representa-
tion as the initial hidden state to generate the se-
quences while employing an attention mechanism
over all node embeddings. Experimental results
show that our model achieves the state-of-the-art
performance on the WikiSQL dataset and Stack-
overﬂow dataset. Our code and data is available at
https://github.com/IBM/SQL-to-Text.

Figure 2: The graph representation of the SQL query in Fig-
ure 1.

2 Graph Representation of SQL Query

3 Graph-to-sequence Model

Representing the SQL query as a graph instead
of a sequence could better preserve the inherent
structure information in the query. An example is
illustrated in the blue dashed frame in Figure 2.
One can see that representing them as a graph in-
stead of a sequence could help the model to bet-
ter learn the correlation between this graph pattern
and the interpretation “...both X and Y higher than
Z...”. This observation motivates us to represent
the SQL query as a graph. In particular, we use
the following method to transform the SQL query
to a graph:1

SELECT Clause. For the SELECT clause such
as “SELECT company”, we ﬁrst create a node as-
signed with text attribute select. This SELECT
node connects with column nodes whose text at-
tributes are the selected column names such as
company. For SQL queries that contain aggrega-
tion functions such as count or max, we add one
aggregation node which is connected with column
nodes. Similarly, their text attributes are the ag-
gregation function names.

WHERE Clause. The WHERE clause usually
contains more than one condition. For each condi-
tion, we use the same process as for the SELECT
clause to create nodes. For example, in Figure 2,
we create node assets and >val0 for the ﬁrst con-
dition, the node sales and >val0 for the second
condition. We then integrate the constraint nodes
that have the same text attribute (e.g., >val0 in
Figure 2). For a logical operator such as AND, OR
and NOT, we create a node that connects with all
column nodes that the operator works on. Finally,
these logical operator nodes connect with the SE-
LECT node.

1This method could be simply extended to cope with more
general SQL queries that have complex syntaxes such as
JOIN and ORDER BY.

Based on the constructed graphs for the SQL
queries, we make full use of a novel graph-to-
sequence model (Xu et al., 2018), which consists
of a graph encoder to learn the embedding for the
graph-structured SQL query, and a sequence de-
coder with attention mechanism to generate sen-
tences. Conceptually, the graph encoder generates
the node embedding for each node by accumu-
lating information from its K-hop neighbors, and
produces a graph embedding for the entire graph
by abstracting all node embeddings. Our decoder
takes the graph embedding as the initial hidden
state and calculates the attention over all node em-
beddings on the encoder side to generate natural
language interpretations.

Node Embedding. Given the graph G = (V, E),
since the text attribute of a node may include a list
of words, we ﬁrst use a Long Short Term Memory
(LSTM) to generate the feature vector av for all
nodes ∀v ∈ V from v’s text attribute. We use these
feature vectors as initial node embeddings. Then,
our model incorporates information from a node’s
neighbors within K hop into its representation by
repeating the following process K times:

(cid:96)({hk−1

h0
v(cid:96) = av, h0
hk
N(cid:96)(v) = Mk
v(cid:96) = σ(Wk · CONCAT(hk−1
hk
hk
N(cid:97)(v) = Mk
v(cid:97) = σ(Wk · CONCAT(hk−1
hk

(cid:96)v = av, ∀v ∈ V
u(cid:96) , ∀u ∈ N(cid:96)(v)})
v(cid:96) , hk
N(cid:96)(v)))
u(cid:97) , ∀u ∈ N(cid:97)(v)})
v(cid:97) , hk
N(cid:97)(v)))

(cid:97)({hk−1

(1)

(2)

(3)

(4)

(5)

where k ∈ {1, ..., K} is the iteration index, N is
the neighborhood function2, hk
v(cid:97)) is node v’s
forward (backward) representation which aggre-
gates the information of nodes in N(cid:96)(v) (N(cid:97)(v)),

v(cid:96) (hk

2N(cid:96)(v) returns the nodes that v directs to and N(cid:97)(v) re-

turns the nodes that direct to v.

(cid:96) and Mk

Mk
(cid:97) are the forward and backward aggre-
gator functions, Wk denotes weight matrices, σ is
a non-linearity function.

For example, for node v ∈ V, we ﬁrst aggre-
gate the forward representations of its immediate
neighbors {hk−1
u(cid:96) , ∀u ∈ N(cid:96)(v)} into a single vec-
tor hk
N(cid:96)(v) (equation 2). Note that this aggrega-
tion step only uses the representations generated
at previous iteration and its initial representation is
av. Then we concatenate v’s current forward rep-
resentation hk−1
v(cid:96) with the newly generated neigh-
borhood vector hk
N(cid:96)(v). This concatenated vector
is fed into a fully connected layer with nonlinear
activation function σ, which updates the forward
representation of v to be used at the next itera-
tion (equation 3). Next, we update the backward
representation of v in the similar fashion (equa-
tion 4∼5). Finally, the concatenation of the for-
ward and backward representation at last itera-
tion K, is used as the resulting representation of
v. Since the neighbor information from different
hops may have the different impact on the node
embedding, we learn a distinct aggregator function
at each step. This aggregator feeds each neigh-
bor’s vector to a fully-connected neural network
and an element-wise max-pooling operation is ap-
plied to capture different aspects of the neighbor
set.

Graph Embedding. Most existing works of
graph convolution neural networks focus more on
node embeddings rather than graph embeddings
(GE) since their focus is on the node-wise clas-
siﬁcation task. However, graph embeddings that
convey the entire graph information are essential
to the downstream decoder, which is crucial to our
task. For this purpose, we propose two ways to
generate graph embeddings, namely, the Pooling-
based and Node-based methods.

Pooling-based GE. This method feeds the ob-
tained node embeddings into a fully-connected
neural network and applies the element-wise max-
pooling operation on all node embeddings.
In
experiments, we did not observe signiﬁcant per-
formance improvement using min-pooling and
average-pooling.

Node-based GE. Following (Scarselli et al.,
2009), this method adds a super node vs that is
connected to all other nodes by a special type of
edge. The embedding of vs, which is treated as
graph embedding, is produced using node embed-
ding generation algorithm mentioned above.

Sequence Decoding. The decoder is an RNN
which predicts the next token yi given all the pre-
vious words y<i = y1, ..., yi−1, the RNN hid-
den state si for time-step i and the context vector
ci that captures the attention of the encoder side.
In particular, the context vector ci depends on a
set of node representations (h1,...,hV ) to which
the encoder maps the input graph. The context
vector ci is dynamically computed using attention
mechanism over the node representations. Our
model is jointly trained to maximize the condi-
tional log-probability of the correct description
given a source graph with respect to the parame-
ters θ of the model:

θ∗ = arg max

log p(yn

t |yn

<t, xn)

N
(cid:88)

Tn(cid:88)

θ

n=1

t=1

where (xn, yn) is the n-th SQL-interpretation pair
in the training set, and Tn is the length of the n-th
target sentence yn. In the inference phase, we use
the beam search algorithm with beam size = 5.

4 Experiments

We evaluate our model on two datasets, WikiSQL
(Zhong et al., 2017) and Stackoverﬂow (Iyer et al.,
2016). WikiSQL consists of a corpus of 87,726
hand-annotated SQL query and natural language
question pairs. These SQL queries are further
split into training (61,297 examples), development
(9,145 examples) and test sets (17,284 examples).
StackOverﬂow consists of 32,337 SQL query and
natural language question pairs, and we use the
same train/development/test split as (Iyer et al.,
2016). We use the BLEU-4 score (Papineni et al.,
2002) as our automatic evaluation metric and also
perform a human study. For human evaluation,
we randomly sampled 1,000 predicted results and
asked three native English speakers to rate each in-
terpretation against both the correctness conform-
ing to the input SQL and grammaticality on a
scale between 1 and 5. We compare some vari-
ants of our model against the template, Seq2Seq,
and Tree2Seq baselines.

Graph2Seq-PGE. This method uses

the
Pooling method for generating Graph Embedding.
Graph2Seq-NGE. This method uses the Node

based Graph Embedding.

Template. We implement a template-based
method which ﬁrst maps each element of a SQL
query to an utterance and then uses simple rules
to assemble these utterances. For example, we

BLEU-4 Grammar.

Correct.

Template
Seq2Seq
Seq2Seq + Copy
Tree2Seq
Graph2Seq-PGE
Graph2Seq-NGE
(Iyer et al., 2016)
Graph2Seq-PGE
Graph2Seq-NGE

15.71
20.91
24.12
26.67
38.97
34.28
18.4
23.3
21.9

1.50
2.54
2.65
2.70
3.81
3.26
3.16
3.23
2.97

-
62.1%
64.5%
66.8%
79.2%
75.3%
64.2%
70.2%
65.1%

SQL Query & Interpretations
1. COUNT Player WHERE starter = val0 AND touchdowns

= val1 AND position = val2

S: How many players played in position val2
G: number of players with starter val0 and get touchdowns
val1 for val2
2. SELECT Tires WHERE engine = val0 AND chassis =

val1 AND team = val2

S: which tire has engine val0 and chassis val1 and val2
G: which tire does val2 run with val0 engine and val1 chassis

Table 1: Results on the WikiSQL (above) and Stackoverﬂow
(below).

Table 2: Example of SQL queries and predicted interpreta-
tions where S and G denotes Seq2Seq and Graph2Seq mod-
els, respectively.

map SELECT to which, WHERE to where, > to
more than. This method translates the SQL query
of Figure 1 to which company where assets more
than val0 and sales more than val0 and industry
less than or equal to val1 and proﬁts equals val2.
Seq2Seq. We choose two Seq2Seq models
as our baselines. The ﬁrst one is the attention-
based Seq2Seq model proposed by Bahdanau et al.
(2014), and the second one additionally introduces
the copy mechanism in the decoder side (Gu et al.,
2016). To evaluate these models, we employ
a template to convert the SQL query into a se-
quence: “SELECT + <aggregation function> + <Split
Symbol> + <selected column> + WHERE + <condition0>
+ <Split Symbol> + <condition1> + ... ”.

Tree2Seq. We also choose a tree-to-sequence
model proposed by (Eriguchi et al., 2016) as our
baseline. We use the SQL Parser tool3 to convert a
SQL query into the tree structure4 which is fed to
the Tree2Seq model.

Our proposed models are trained using the
Adam optimizer (Kingma and Ba, 2014), with
mini-batch size 30. Our hyper-parameters are set
based on performance on the validation set. The
learning rate is set to 0.001. We apply the dropout
strategy (Srivastava et al., 2014) with the ratio of
0.5 at the decoder layer to avoid overﬁtting. Gra-
dients are clipped when their norm is bigger than
20. We initialize word embeddings using GloVe
word vectors from Pennington et al. (2014), and
the word embedding dimension is 300. For the
graph encoder, the hop size K is set to 6, the non-
linearity function σ is implemented as ReLU (Glo-
rot et al., 2011), the parameters of weight matrices
Wk are randomly initialized. The decoder has one
layer, and its hidden state size is 300.

3http://www.sqlparser.com
4See Appendix for details.

Results and Discussion Table 1 summarizes the
results of our models and baselines. Although
the template-based method achieves decent BLEU
scores, its grammaticality score is substantially
worse than other baselines. We can see that on
both two datasets, our Graph2Seq models per-
form signiﬁcantly better than the Seq2Seq and
Tree2Seq baselines. One possible reason is that
in our graph encoder, the node embedding retains
the information of neighbor nodes within K hops.
However, in the tree encoder, the node embed-
ding only aggregates the information of descen-
dants while losing the knowledge of ancestors.
The pooling-based graph embedding is found to
be more useful than the node-based graph em-
bedding because Graph2Seq-NGE adds a nonex-
istent node into the graph, which introduces the
noisy information in calculating the embeddings
of other nodes. We also conducted an experiment
that treats the SQL query graph as an undirected
graph and found the performance degrades.

By manually analyzing the cases in which the
Graph2Seq model performs better than Seq2Seq,
we ﬁnd the Graph2Seq model is better at inter-
preting two classes of queries: (1) the complicated
queries that have more than two conditions (Query
1); (2) the queries whose columns have implicit
relationships (Query 2). Table 2 lists some such
SQL queries and their interpretations. One possi-
ble reason is that the Graph2Seq model can better
learn the correlation between the graph pattern and
natural language by utilizing the global structure
information.

We ﬁnd the hop size has a signiﬁcant impact on
our model since it determines how many neigh-
bor nodes to be considered during the node em-
bedding generation. As the hop size increasing,
the performance is found to be signiﬁcantly im-
proved. However, after the hop size reaches 6,

increasing the hop size can not boost the perfor-
mance on WikiSQL anymore. By analyzing the
most complicated queries (around 6.2%) in Wik-
iSQL, we ﬁnd there are average six hops between
a node and its most distant neighbor. This result
indicates that the selected hop size should guar-
antee each node can receive the information from
others nodes in the graph.

5 Conclusions

Previous work approaches the SQL-to-text task
using an Seq2Seq model which does not fully cap-
ture the global structure information of the SQL
query. To address this, we proposed a Graph2Seq
model which includes a graph encoder, an atten-
tion based sequence decoder. Experimental results
show that our model signiﬁcantly outperforms the
Seq2Seq and Tree2Seq models on the WikiSQL
and Stackoverﬂow datasets.

Appendix

Figure 3: Tree representation of the SQL query.

We apply the SQL Parser tool5 to convert
an SQL query to a tree whose structure is il-
lustrated in Figure 3. More speciﬁcally,
the
root has two child nodes, namely Select List and
Where Clause. The child nodes of Select List rep-
resent the selected columns in the SQL query. The
Where Clause has the logical operators occurred
in the SQL query as its children. The children of a
logical operator node are the conditions on which
this operator works.

References

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings. In ICLR.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Neural machine translation by

Bengio. 2014.

5http://www.sqlparser.com

jointly learning to align and translate.
abs/1409.0473.

CoRR,

Akiko Eriguchi, Kazuma Hashimoto, and Yoshi-
masa Tsuruoka. 2016.
Tree-to-sequence atten-
tional neural machine translation. arXiv preprint
arXiv:1603.06075.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectiﬁer neural networks.
In
Proceedings of the Fourteenth International Confer-
ence on Artiﬁcial Intelligence and Statistics, AIS-
TATS 2011, Fort Lauderdale, USA, April 11-13,
2011, pages 315–323.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Incorporating copying mechanism in
arXiv preprint

Li. 2016.
sequence-to-sequence learning.
arXiv:1603.06393.

William L. Hamilton, Zhitao Ying, and Jure Leskovec.
2017.
Inductive representation learning on large
graphs. In Advances in Neural Information Process-
ing Systems 30: Annual Conference on Neural In-
formation Processing Systems 2017, 4-9 December
2017, Long Beach, CA, USA, pages 1025–1035.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and
Luke Zettlemoyer. 2016. Summarizing source code
In Proceedings of
using a neural attention model.
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
volume 1, pages 2073–2083.

Yoon Kim. 2014.

works for sentence classiﬁcation.
arXiv:1408.5882.

Convolutional neural net-
arXiv preprint

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Thomas N Kipf and Max Welling. 2016.

Semi-
supervised classiﬁcation with graph convolutional
networks. arXiv preprint arXiv:1609.02907.

Georgia Koutrika, Alkis Simitsis, and Yannis E Ioan-
nidis. 2010. Explaining structured queries in natural
language. In Data Engineering (ICDE), 2010 IEEE
26th International Conference on, pages 333–344.
IEEE.

Axel-Cyrille Ngonga Ngomo, Lorenz B¨uhmann,
Christina Unger, Jens Lehmann, and Daniel Gerber.
2013. Sorry, i don’t speak sparql: translating sparql
queries into natural language. In Proceedings of the
22nd international conference on World Wide Web,
pages 977–988. ACM.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, July 6-12, 2002, Philadelphia,
PA, USA., pages 311–318.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
Hagenbuchner, and Gabriele Monfardini. 2009. The
graph neural network model. IEEE Transactions on
Neural Networks, 20(1):61–80.

Alkis

Simitsis

and Yannis

Dbmss should talk back too.
arXiv:0909.1786.

Ioannidis.

2009.
arXiv preprint

Linfeng Song, Yue Zhang, Zhiguo Wang,

and
Daniel Gildea. 2018.
A graph-to-sequence
model for amr-to-text generation. arXiv preprint
arXiv:1805.02473.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: a simple way to prevent neural
Journal of Machine
networks from overﬁtting.
Learning Research, 15(1):1929–1958.

S Vichy N Vishwanathan, Nicol N Schraudolph, Risi
Kondor, and Karsten M Borgwardt. 2010. Graph
Journal of Machine Learning Research,
kernels.
11(Apr):1201–1242.

Lingfei Wu, Ian E.H. Yen, Kun Xu, Fangli Xu, Avinash
Balakrishnan, Pin-Yu Chen, Pradeep Ravikumar,
and Michael J. Witbrock. 2018a. Word mover’s em-
bedding: From word2vec to document embedding.
In EMNLP.

Lingfei Wu, Ian En-Hsu Yen, Fangli Xu, Pradeep
Ravikuma, and Michael Witbrock. 2018b. D2ke:
arXiv
From distance to kernel and embedding.
preprint arXiv:1802.04956.

Kun Xu, Lingfei Wu, Zhiguo Wang, and Vadim
Sheinin. 2018. Graph2seq: Graph to sequence
learning with attention-based neural networks.
arXiv preprint arXiv:1804.00823.

Victor Zhong, Caiming Xiong, and Richard Socher.
Seq2sql: Generating structured queries
2017.
from natural language using reinforcement learning.
CoRR, abs/1709.00103.

