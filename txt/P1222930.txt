A step towards procedural terrain generation with GANs

Christopher Beckham 1 Christopher Pal 1

7
1
0
2
 
l
u
J
 
1
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
3
8
3
3
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

Procedural terrain generation for video games
has been traditionally been done with smartly de-
signed but handcrafted algorithms that generate
heightmaps. We propose a ﬁrst step toward the
learning and synthesis of these using recent ad-
vances in deep generative modelling with openly
available satellite imagery from NASA.

1. Introduction

Procedural generation in video games is the algorithmic
generation of content intended to increase replay value
through interleaving the gameplay with elements of unpre-
dictability. This is in contrast to the more traditional, ‘hand-
crafty’ generation of content, which is generally of higher
quality but with the added expense of labour. A prominent
game whose premise is almost entirely based on procedu-
ral terrain generation is Minecraft, a game where the player
can explore a vast open world whose terrain is based en-
tirely on voxels (‘volumetric pixels’), allowing the player to
manipulate the terrain (i.e. dig tunnels, build walls) and ex-
plore interesting landscapes (i.e. beaches, jungles, caves).

So far, terrains have been procedurally generated through
a host of algorithms designed to mimic real-life terrain.
Some prominent examples of this include Perlin noise (Per-
lin, 1985) and diamond square (Fournier et al., 1982), in
which a greyscale image is generated from a noise source
(with intensities proportional to heights above sea level),
which, when rendered in 3D as a mesh, produces a terrain.
While these methods are quite fast, they generate terrains
which are quite simple in nature. Software such as L3DT
employ sophisticated algorithms which let the user control
what kind of terrain they desire, (e.g. mountains, lakes, val-
leys), and while these can produce very impressive terrains
1, it would still seem like an exciting endeavour to leverage
the power of generative networks in deep learning (such as

1Montr´eal

Institute of Learning Algorithms, Qu´ebec,
Canada. Correspondence to: Christopher Beckham <christo-
pher.beckham@polymtl.ca>.

Submitted to VGML workshop at ICML 2017.

1See

http://www.bundysoft.com/docs/doku.

php?id=l3dt:algorithms

the GAN (Goodfellow et al., 2014)) to learn algorithms to
automatically generate terrain, without the need to manu-
ally write algorithms to do so.

In this paper, we leverage extremely high-resolution ter-
rain and heightmap data provided by the NASA ‘Visible
Earth’ project2 in conjunction with generative adversarial
networks (GANs) to create a two-stage pipeline in which
heightmaps can be randomly generated as well as a tex-
ture map that is inferred from the heightmap. Concretely,
we synthesise 512px height and texture maps using ran-
dom 512px crops from the original NASA images (of size
21600px x 10800px), as seen in Figure 1.

1.1. Formulation

Suppose z(cid:48) ∼ p(z) is a k-dimensional sample we draw
from the prior distribution, x(cid:48) = Gh(z(cid:48)) the heightmap
which is generated from z(cid:48), and y(cid:48) = Gt(x(cid:48)) is the tex-
ture generated from the corresponding heightmap. We can
think of this process as being comprised of two GANs:
the ‘DCGAN’ (Radford et al., 2015) which generates the
heightmap from noise, and ‘pix2pix’ (Isola et al., 2016),
which (informally) refers to conditional GANs for image-
to-image translation. If we denote the DCGAN generator
and discriminator as Gh(·) and Dh(·) respectively (where
the ‘h’ in the subscript denotes ‘heightmap’), then we can
formulate the training objective as:

(cid:96)(Dh(Gh(z(cid:48))), 1)

(cid:96)(Dh(x), 1) + (cid:96)(Dh(Gh(z(cid:48))), 0),

(1)

min
Gh
min
Dh

where (cid:96) is a GAN-speciﬁc loss, e.g. binary cross-entropy
for the regular GAN formulation, and squared error for LS-
GAN (Mao et al., 2016). We can write similar equations for
the pix2pix GAN, where we now have Gt(·) and Dt(·, ·)
instead (where ‘t’ denotes ‘texture’), and instead of x and
x(cid:48) we have y (ground truth texture) and y(cid:48) (generated tex-
ture), respectively. Note that the discriminator in this case,
Dt, actually takes two arguments: either a real heightmap
/ real texture pair (x, y), or a real heightmap / generated
texture pair (x, y(cid:48)). Also note that for the pix2pix part of
the network, we can also employ some form of pixel-wise
reconstruction loss to prevent the generator from dropping

2https://visibleearth.nasa.gov/

A step towards procedural terrain generation with GANs

(a) World heightmap

(b) World texture map

Figure 1. Heightmap and texture map (21600px x 10800px) of the earth provided by the NASA Visible Earth project. Both maps provide
a spatial resolution of 1 square km per pixel.

modes. Therefore, we can write the training objectives for
pix2pix as such:

(cid:96)(Dt(x, Gt(x(cid:48))), 1) + λd(y, Gt(x(cid:48)))

(cid:96)(Dt(x, y), 1) + (cid:96)(Dt(x, Gt(x(cid:48))), 0),

(2)

min
Gt
min
Dt

textures by the pix2pix GAN Gt. We can see that the
pix2pix GAN has created textures that roughly ‘match’
their corresponding heightmaps. For example, regions of
relatively higher elevation in the heightmap correspond to
different textures. Interestingly, parts of some of the tex-
tures are completely white; this appears to be a side-effect
of not training the DCGAN and pix2pix GANs jointly.

where d(·, ·) can be some distance function such as L1 or
L2 loss, and λ is a hyperparameter denoting the relative
strength of the reconstruction loss. We set λ = 100 and use
L1.

3. Conclusion

2. Experiments and Results

As a ﬁrst step, we train a DCGAN which maps samples
from the prior z(cid:48) ∼ p(z) to a generated heightmap x(cid:48) =
Gh(z(cid:48)) of size 512px. While we experienced some issues
with training stability we were able to generate heightmaps
that were somewhat faithful to the original images. We
generate two of these and illustrate a linear interpolation
between the two, which is shown in Figure 2. While the
interpolation is shown purely for illustrative purposes, one
could imagine that if the DCGAN successfully learned rep-
resentations of the different landscapes (e.g. mountains,
valleys, desert, jungle) then one could ﬁnd their latent rep-
resentations – through a bidirectional GAN like BiGAN
(Donahue et al., 2016) or ALI (Dumoulin et al., 2016) –
then interpolate between them and decode to control the
resulting heightmap.

Apart from the aforementioned stability issues, the gener-
ated heightmaps can sometimes exhibit small-scaled arti-
facts, which can be seen for the ﬁrst generated heightmap
(top-left corner). While experimenting with deeper archi-
tectures and/or skip connections could mitigate this, one
easy trick is to apply a slight blur to the ﬁnal images via a
Gaussian kernel convolution. This can serve to smooth out
any weird artifacts generated by the DCGAN.

Figure 3 shows the a variety of heightmaps generated with
the DCGAN Gh and their corresponding translations to

In this work we have achieved a reasonable ﬁrst step toward
procedural generation of terrain based on real-world data.
The most obvious next step would be to jointly train the
DCGAN and pix2pix GANs.

A neat addition to this idea would be the addition of a seg-
mentation pipeline to classify different parts of the terrain,
e.g. biomes. This effectively serves as a layer of metadata
that can be leveraged to add interesting detail in the terrain.
For example, if the segmentation identiﬁes a certain region
in the generated terrain as ‘jungle’, the 3D game engine (or
renderer) can automatically populate that region with trees
and plants.
(This is called a ‘splatmap’ in the computer
graphics literature.)

The two-stage GAN framework that we have described
here can have many applications in procedural generation
outside of terrain modelling. For example, one can imag-
ine the same scheme being applied to synthesise 3D meshes
which are then textured (e.g. faces). These kinds of possi-
bilities serve to not only promote richer entertainment ex-
periences, but to also provide useful tools to aid content
producers (e.g. 3D artists) in their work.

4. Acknowledgements

The authors would like to thank the developers of Theano
(Theano Development Team, 2016), Lasagne (Dieleman
et al., 2015), and Keras (Chollet et al., 2015). This work
is partially funded by Imagia, Inc. under a MITACS Accel-
erate scholarship.

A step towards procedural terrain generation with GANs

Figure 2. A linear interpolation of two heightmaps (top-left and bottom-right corner).

Figure 3. Heightmaps generated by the Gh and their corresponding textures predicted by Gt. The texture GAN Gt seems to think that
high elevations are snow, despite the fact that we are generating deserts – what is this madness?! We will just pretend they are salt ﬂats
for now.

A step towards procedural terrain generation with GANs

(a) Heightmap

(b) Texture map

Figure 4. Left: randomly generated heightmap, right: the corresponding texture. Both images are 512px, which corresponds to 512
square km.

Figure 5. A rendering of one of the generated heightmap in Figure 4(a) in the Unity 3D game engine. A very minor Gaussian blur (of
radius 0.4px) was used to smooth out artifacts prior to rendering.

References

5. Supplementary material

A step towards procedural terrain generation with GANs

Chollet, Franc¸ois et al. Keras. https://github.com/

fchollet/keras, 2015.

Dieleman, Sander, Schlter, Jan, Raffel, Colin, Olson, Eben,
Snderby, Sren Kaae, Nouri, Daniel, and et al. Lasagne:
First release., August 2015. URL http://dx.doi.
org/10.5281/zenodo.27878.

Donahue, Jeff, Kr¨ahenb¨uhl, Philipp, and Darrell, Trevor.
Adversarial feature learning. CoRR, abs/1605.09782,
URL http://arxiv.org/abs/1605.
2016.
09782.

Dumoulin, Vincent, Belghazi, Ishmael, Poole, Ben, Lamb,
Alex, Arjovsky, Martin, Mastropietro, Olivier, and
Courville, Aaron. Adversarially learned inference. arXiv
preprint arXiv:1606.00704, 2016.

Fournier, Alain, Fussell, Don, and Carpenter, Loren. Com-
puter rendering of stochastic models. Commun. ACM,
25(6):371–384, June 1982.
doi:
10.1145/358523.358553. URL http://doi.acm.
org/10.1145/358523.358553.

ISSN 0001-0782.

Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,
Bing, Warde-Farley, David, Ozair, Sherjil, Courville,
Aaron, and Bengio, Yoshua. Generative adversarial nets.
In Advances in neural information processing systems,
pp. 2672–2680, 2014.

Isola, Phillip, Zhu, Jun-Yan, Zhou, Tinghui, and Efros,
Alexei A. Image-to-image translation with conditional
adversarial networks. CoRR, abs/1611.07004, 2016.
URL http://arxiv.org/abs/1611.07004.

Mao, Xudong, Li, Qing, Xie, Haoran, Lau, Raymond
Y. K., and Wang, Zhen. Multi-class generative ad-
versarial networks with the L2 loss function. CoRR,
abs/1611.04076, 2016. URL http://arxiv.org/
abs/1611.04076.

Perlin, Ken. An image synthesizer. SIGGRAPH Com-
put. Graph., 19(3):287–296, July 1985.
ISSN 0097-
8930. doi: 10.1145/325165.325247. URL http://
doi.acm.org/10.1145/325165.325247.

Radford, Alec, Metz, Luke, and Chintala, Soumith.
Unsupervised representation learning with deep con-
CoRR,
volutional generative adversarial networks.
abs/1511.06434, 2015. URL http://arxiv.org/
abs/1511.06434.

Theano Development Team. Theano: A Python framework
for fast computation of mathematical expressions. arXiv
e-prints, abs/1605.02688, May 2016. URL http://
arxiv.org/abs/1605.02688.

In this section we provide some extra information that we
were unable to ﬁt into the main section due to page restric-
tions.

5.1. Dataset

The dataset was prepared as follows. First, we downloaded
a high-res heightmap and texture map of the earth, as can be
found here3 and here4. We slide a 512px window through
both images simultaneously, and only retain (heightmap,
texture) pairs where the heightmap’s colour composition
is less than 90% black; this so that we do not feed the
GAN data that is too ‘trivial’ to generate. Note that at this
point, textures in the collection can correspond to various
biomes such as jungle, desert, and arctic, and in theory,
any particular heightmap could correspond to any of these
biomes (which can confuse the pix2pix GAN during train-
ing). To address this, we choose a ‘reference texture’ with
our biome of interest (in our case, desert), and compute
the Euclidean distance between this texture and all other
textures in the collection. From this, we choose the top M
pairs that have the smallest distances with the reference tex-
ture, so that the ﬁnal collection only contains pairs whose
biome of interest is desert.

5.2. Architectures

Images detailing the precise architectures used for the
GANs have been added in the .zip ﬁle from which the
source of this LATEX can be found.

5.3. Training

We use the LSGAN formulation as this made training more
stable. Therefore, the (cid:96)(·, ·) in Equations 1 and 2 are binary
cross-entropy, and the output activations of both discrimi-
nators Dh and Dt are linear instead of sigmoid.

We train both GANs using RMSProp with initial learning
rates 1e−4.

3https://eoimages.gsfc.nasa.gov/images/

imagerecords/74000/74218/world.200412.
3x21600x10800.jpg

4https://eoimages.gsfc.nasa.gov/images/
imagerecords/73000/73934/gebco_08_rev_elev_
21600x10800.png

