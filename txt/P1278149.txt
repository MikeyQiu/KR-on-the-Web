Bayesian optimisation for fast approximate inference in
state-space models with intractable likelihoods

Johan Dahlin, Mattias Villani and Thomas B. Sch¨on∗

June 14, 2017

Abstract

We consider the problem of approximate Bayesian parameter inference in non-linear state-space mod-

els with intractable likelihoods. Sequential Monte Carlo with approximate Bayesian computations

(smc-abc) is one approach to approximate the likelihood in this type of models. However, such ap-

proximations can be noisy and computationally costly which hinders eﬃcient implementations using

standard methods based on optimisation and Monte Carlo. We propose a computationally eﬃcient

novel method based on the combination of Gaussian process optimisation and smc-abc to create a

Laplace approximation of the intractable posterior. We exemplify the proposed algorithm for infer-

ence in stochastic volatility models with both synthetic and real-world data as well as for estimating

the Value-at-Risk for two portfolios using a copula model. We document speed-ups of between one

and two orders of magnitude compared to state-of-the-art algorithms for posterior inference.

Keywords: α-stable distributions, approximate Bayesian computations, Bayesian inference, Gaus-

sian process optimisation, sequential Monte Carlo

7
1
0
2
 
n
u
J
 
3
1
 
 
]

O
C

.
t
a
t
s
[
 
 
3
v
5
7
9
6
0
.
6
0
5
1
:
v
i
X
r
a

∗E-mail to corresponding author: liu@johandahlin.com. JD and MV are with the Department of Computer and
Information Science, Link¨oping University. TS is with the Department of Information Technology, Uppsala University,
Sweden. This work was mainly carried out while JD worked at the Division of Automatic Control, Department of Electrical
Engineering, Link¨oping University.

1

1

Introduction

Dynamical modelling of time series data is an essential part of many scientiﬁc ﬁelds including statistics

[Durbin and Koopman, 2012], econometrics [McNeil et al., 2010] and engineering [Ljung, 1999]. A popular

dynamical model is the state-space model (ssm) which can be expressed by

x0 ∼ µθ(x0),

xt|xt−1 ∼ fθ(xt|xt−1),

yt|xt ∼ gθ(yt|xt),

(1)

where θ ∈ Θ ⊆ Rp denotes unknown static parameters. Here, xt ∈ X ⊆ Rnx and yt ∈ Y ⊆ Rny denotes

the latent state and the observations at time t ∈ {0, 1, . . . , T }, respectively. The distribution of the initial

state, the state dynamics and the observations are modelled by using the known probability densities µ,

fθ and gtheta, respectively.

In this paper, we are interested in estimating the unknown parameters θ in ssms using a Bayesian

approach. This amounts to computing the parameter posterior distribution given by

p(θ|y1:T ) =

(cid:90)

p(θ)pθ(y1:T )

,

p(θ(cid:48))pθ(cid:48)(y1:T ) dθ(cid:48)

Θ

(2)

where p(θ) and pθ(y1:T ) (cid:44) p(y1, y2, . . . , yT |θ) denote the parameter prior distribution and the likelihood,

respectively. For an ssm, we cannot compute the posterior in closed-form due to that the likelihood

pθ(y1:T ) depends on the unknown latent states x0:T . Fortunately, it is possible to obtain unbiased

estimates of the likelihood via so-called sequential Monte Carlo (smc; Doucet and Johansen, 2011) or

particle ﬁltering algorithms.

However for some models of interest, it is not possible to make use of smc due to that gθ(yt|xt) lacks

an analytical closed-form expression, is deﬁned recursively or is computationally prohibitive to evaluate.

We refer to this class of models as ssms with intractable likelihoods. One example is when the α-stable

distribution [Nolan, 2003] is used as gθ in (1) to model heavy-tailed noise in the observations. This

type of modelling has recently been advocated by Stoyanov et al. [2010] among others to capture the

behaviour of ﬁnancial indices and stock prices, which often exhibit so-called jumps.

One approach to obtain a biased estimate of the intractable likelihood is to make use of approximate

Bayesian computations (abc; Marin et al., 2012) in combination with smc [Jasra et al., 2012]. The

parameters θ can then be estimated using standard inference algorithms. However, the estimator obtained

by smc-abc often suﬀers from a large variance and is computationally expensive to evaluate. This usually

results in long run-times (days) of the complete inference algorithm, which is prohibitive in practical

applications.

In this paper, we propose a computationally eﬃcient algorithm for Bayes- ian inference in ssms with

intractable likelihoods. The proposed algorithm is referred to as gsa and is a combination of Gaussian

2

process optimisation (gpo; Brochu et al., 2010) and smc-abc. The aim of gsa is to construct a Laplace

approximation to approximate (2). The eﬃciency of the proposed algorithm stems from that gpo requires

substantially less posterior evaluations and is more robust to noise compared with other optimisation

algorithms. This is mainly due to that gpo operates by constructing a surrogate function that mimics

(2) in analogue with Wood [2010]. The resulting surrogate is smooth and computationally cheap to

evaluate, which enables the use of standard optimisation methods to extract a Laplace approximation of

the surrogate mimicking the true posterior.

The main contribution of this paper is to introduce, develop and numerically study the gsa algo-

rithm. We compare the proposed algorithm to particle Metropolis-Hastings (pmh; Andrieu et al., 2010,

Dahlin and Sch¨on, 2015) and spsa [Spall, 1998, Ehrlich et al., 2015] for inference in ssms using both

synthetic and real-world data.

In Bayesian inference, pmh is seen as the gold standard for posterior

approximations and spsa is known as an eﬃcient and scalable gradient-free optimisation algorithm. The

numerical comparisons indicate that gsa can: (i) provide good posterior approximations, (ii) reduce the

computational time by between one and two orders of magnitude compared with pmh, (iii) exhibit good

robustness to the abc approximation and noise in the estimates. Furthermore, we demonstrate how to

make use of the proposed algorithm in estimating the risk in ﬁnancial portfolios.

Related work to the proposed algorithm is presented in e.g. Dahlin and Lindsten [2014], Gutmann

and Corander [2016] and Meeds and Welling [2014]. In the ﬁrst two works, the authors aim to obtain a

maximum likelihood estimate and map estimate using gpo, respectively. In the present work, we would

like to approximate the entire posterior and not only the parameter that maximises the value of the

likelihood or posterior. Moreover, compared with Meeds and Welling [2014], the uncertainty encoded

into the surrogate function is utilized to determine the next point in which to sample the log-posterior.

We continue with Section 2, where an overview of the proposed algorithm and its components are

presented. Sections 3 and 4 discuss the details of these components and the resulting algorithm is

presented in Section 5. We conclude the paper with an extensive numerical evaluation in Section 6, and

some remarks and future work in Section 7.

2 An intuitive overview of GSA

Our aim is to ﬁnd Laplace approximation of the parameter posterior (2),

(cid:18)

(cid:98)p(θ|y1:T ) = N

θ; (cid:98)θMAP,

(cid:104)

(cid:12)
−∇2 log p(θ|y1:T )
(cid:12)
(cid:12)θ=(cid:98)θMAP
(cid:124)
(cid:125)

(cid:123)(cid:122)
(cid:44)J ((cid:98)θMAP)

(cid:105)−1(cid:19)
,

(3)

3

Figure 1: Samples from the log-posterior (left) of (16) with respect to µ. The distribution (center)
and qq-plot(right) of 1, 000 log-posterior estimates of (16). The purple lines indicate the best Gaussian
approximation.

where N (θ; µ, Σ) denotes the Gaussian distribution with mean µ and covariance matrix Σ. Here,

J ((cid:98)θMAP) denotes the estimate of the Hessian of the log-posterior evaluated at the posterior mode,

(cid:98)θMAP = argmax

log p(θ|y1:T ),

θ∈Θ

(4)

where y1:T denotes the recorded observations. This can be seen as a Gaussian approximation around the

mode of the posterior motivated by the Bernstein-von Mises theorem, which states that the posterior

concentrates to a Gaussian distribution centred at the true parameters with the inverse expected infor-

mation matrix as its covariance when T → ∞. Note that, even if this is an asymptotic results it can

provide reasonable approximations using a ﬁnite number of samples as discussed by Panov and Spokoiny

[2015].

We encounter two main problems when constructing the Laplace approximation: (i) the optimisation

problem in (4) is diﬃcult to solve eﬃciently and (ii) J ((cid:98)θMAP) is typically diﬃcult to estimate with

good accuracy. The ﬁrst problem is due to the high variance in and computational cost of the posterior

estimates. An example of this problem is presented in the left part of Figure 1. Here, we present the

log-posterior estimates obtained by smc-abc over a grid of µ in (16). The high variance typically results

in a slow convergence of parameter inference algorithms such as spsa and pmh. It is also diﬃcult to speed

up the convergence by using gradient information as this requires running computationally expensive

particle smoothing algorithms.

Instead, we propose to circumvent these problems by optimising a smooth surrogate function that

mimics the log-posterior distribution similar to Wood [2010]. We can then obtain a Laplace approxima-

tion using standard optimisation methods as the surrogate function is smooth and cheap to evaluate.

The surrogate function is obtained by gpo algorithm, which is a speciﬁc instance of Bayesian opti-

misation [Brochu et al., 2010]. The surrogate function is sequentially updated using samples from the

4

log-posterior. In this paper, we make use of the predictive distribution of a Gaussian process (gp) as

the surrogate function. Hence, we can encode certain prior knowledge regarding the smoothness of the

log-posterior into the gp prior, which reduces the number of samples required to explore the log-posterior.

The resulting gsa algorithm iterates three steps. At the kth iteration:

(i) compute an approximation of the log-posterior at the parameter θk denoted ξk = log (cid:98)p(θk|y1:T )

(ii) construct a surrogate function by a gp predictive posterior using the observed data {θk, ξk} =

using smc-abc.

{θj, ξj}k

j=1.

(iii) evaluate the acquisition rule to determine θk+1.

The gpo algorithm then returns an estimate of the posterior and its uncertainty. There are two major

advantages of gps for estimating the posterior density compared with e.g., using splines. Firstly, the

uncertainty quantiﬁcation can be used to develop so-called acquisition rules that explore areas with large

uncertainty and exploits the information about the possible location of the mode of the posterior. This

typically results in a rapid convergence of the algorithm, which limits the number of posterior samples

required and hence also decreases the computational cost. Secondly, the gp can handle noisy function

evaluations in a natural manner.

3 Estimating log p(θ|y1:T )

In this section, we discuss how to estimate the log-posterior that is required for carrying out Step (i) of

gsa. This is done by making use of the (bootstrap) particle ﬁlter from which we can obtain an estimate

of the marginal ﬁltering distribution pθ(xt|y1:t) by

pN
θ (dxt|y1:t) =

w(i)

t δx(i)

t

(dxt),

N
(cid:88)

i=1

(5)

where x(i)
t

and w(i)

t denotes the particle i at time t and its normalised weight, respectively. Here, δx

denotes the Dirac measure placed at x. As we shall see, the particle system {w(i)
t

, x(i)

t }N

i=1 can also be

used to obtain estimates of the log-posterior.

3.1 Particle ﬁltering with ABC

The main problem with applying the particle ﬁlter is that it assumes that we can evaluate gθ(yt|xt)

point-wise.

In the current setting, this is not possible and instead we circumvent the evaluation of

gθ(yt|xt) by using abc. This amounts to augmenting the posterior with an auxiliary variable ˇy1:T , which

is data simulated from gθ(yt|xt) for t = 1, . . . , T . The fundamental assumption of abc is that data ˇyt

5

generated from gθ(yt|xt) should be similar to the observed data yt if θ is properly selected. The resulting

augmented posterior can be expressed as

p(cid:15)(θ, ˇy1:T |y1:T ) =

(cid:90)

(cid:16)

ˇy1:T
(cid:16)

ˇy1:T

(cid:17)

(cid:16)

ρ

ˇy1:T ; y1:T , (cid:15)
(cid:16)

ˇy1:T ; y1:T , (cid:15)

(cid:17)

ρ

(cid:17)

(cid:17)

,

dθ(cid:48)

p(θ)pθ

p(θ(cid:48))pθ(cid:48)

Θ

where, ρ(µ, (cid:15)) denotes some density with mean µ and tolerance parameter (cid:15). This density is used to

compute the distance between the simulated observations ˇyt and the true observations yt. A common

choice is the Gaussian density ρ(µ, (cid:15)) = N (µ, (cid:15)2). Finally, we assume that the following marginalisation

property holds

(cid:90)

p(cid:15)(θ|y1:T ) =

p(cid:15)(θ, ˇy1:T |y1:T ) dˇy1:T ,

when the tolerance parameter is small enough and where p(cid:15)(θ|y1:T ) denotes the posterior of the perturbed

model. Hence, we recover a perturbed version of the posterior when T → ∞ and (cid:15) is small enough.

To make use of this in the particle ﬁlter, we reformulate the ssm in (1) using abc as in Jasra et al.

[2012]. The observed data y1:T is perturbed by

ˇyt = ψ(yt) + zt,

zt ∼ ρ(0, (cid:15)),

(6)

where ψ denotes some suitable one-to-one transformation. Moreover, we assume that it is possible to

simulate from the model using a transformation of random variables. This corresponds to that we can

write ˇyt = τθ(ˇxt), where τθ denotes a function of ˇx(cid:62)

t = (x(cid:62)

t , v(cid:62)

t ) and vt ∼ νθ(vt|xt) for some probability

distribution νθ. This is a useful construction as it is often possible to generate samples from complicated

distributions but not to evaluate them point-wise. See A for how to select τθ and νθ to generate α-stable

random variables.

From these two steps, we can rewrite the ssm (1) as

ˇxt|ˇxt−1 ∼ Ξθ(ˇxt|ˇxt−1) = νθ(vt|xt)fθ(xt|xt−1),

ˇyt|ˇxt ∼ hθ,(cid:15)(ˇyt|ˇxt) = ρ

ˇyt; ψ(τθ(ˇxt)), (cid:15)

(cid:16)

(cid:17)

.

(7a)

(7b)

We can now apply a particle ﬁlter as in Algorithm 1 for this new model, which does not require us to

evaluate the gθ(yt|xt) point-wise but only that we can simulate ˇyt. In this paper, we leave the choice

of (cid:15) to the user, which can be done using e.g. pilot runs on simulated data. However, it is possible to

adapt (cid:15) on-the-ﬂy using the approaches discussed by e.g. Del Moral et al. [2012] or Calvet and Czellar

[2015]. However in our experience, these methods require a larger value of N than un-adapted smc-abc

6

Algorithm 1 Estimate the log-posterior by particle ﬁltering
Inputs: ˇy1:T (perturbed data), ssm (7), N ∈ N (no. particles), ρ(cid:15) (density for abc approximation) with
(cid:15) ∈ R+ (tolerance par.).
Outputs: log (cid:98)pN (θ|ˇy1:T ) (est. of log-posterior).
Note: all operations are carried out over i, j = 1, . . . , N .

0 ∼ µθ(x0)νθ(v0|x0) and set w(i)

1: Sample ˇx(i)
2: for t = 1 to T do
3: Apply systematic resampling to obtain the ancestor index a(i)
t

0 = 1/N .

from a categorical distribution with

(cid:16)

P

(cid:17)

a(i)
t = j

= w(j)
t−1.

4:

Propagate particles using ˇx(i)

t ∼ Ξθ
Compute particle weights by W (i)

5:
6: end for
7: Compute log (cid:98)pN (θ|ˇy1:T ) by (8).

(cid:17)

(cid:16)

ˇxt|ˇxa(i)
t
t−1
(cid:16)

t = hθ,(cid:15)

ˇyt|ˇx(i)
t

and set ˇx(i)

0:t =
t = W (i)
and w(i)

(cid:110)
ˇxa(i)
0:t−1, ˇx(i)
t
(cid:104)(cid:80)N

t

t

(cid:17)

(cid:111)
.

j=1 W (j)

t

(cid:105)−1

.

to provide log-posterior estimates with a reasonable bias. Therefore, we decided to ﬁx (cid:15) in this paper to

obtain computationally eﬃcient algorithms.

3.2 The estimator and its statistical properties

An estimator for the log-posterior of the perturbed model (7) is given by

log (cid:98)pN (θ|y1:T ) = log (cid:98)pN

θ (y1:T ) + log p(θ)

=

log

T
(cid:88)

t=1

(cid:41)

W (i)
t

(cid:40) N
(cid:88)

i=1

− T log N + log p(θ),

(8)

which makes use of the unnormalised weights generated by Algorithm 1. It is known from Pitt et al. [2012]

that the estimator (8) is biased for a ﬁnite number of particles but it is consistent and asymptotically

Gaussian. Speciﬁcally, we have that the error in the log-posterior estimate fulﬁls a clt given by

√

(cid:20)

N

log p(θ|y1:T ) − log (cid:98)pN (θ|y1:T ) +

(cid:21)

γ2(θ)
2N

(cid:16)

d−→ N

0, γ2(θ)

(cid:17)

,

(9)

when N → ∞ and for some unknown variance γ(θ). As a result, we have an expression for the bias

of the estimator given by −γ2(θ)/2N for a ﬁnite number of particles. However, we see that the error

is approximately Gaussian for this type of model in the ﬁnite sample case by the experimental data

presented in the center and right parts of Figure 1.

The log-posterior estimator in (8) is consistent with respect to the perturbed model (7) but not the

true model (1). Dean and Singh [2011] show that the perturbation results in a bias in the parameter

estimates (w.r.t. the unperturbed model) that decrease proportional to O((cid:15)2) under some regularity

assumptions. Furthermore, the asymptotic Gaussianity of the estimator and a Bernstein-von Mises-type

theorem holds for a small enough (cid:15). This is an important fact to motivate the Laplace approximation as

7

discussed in Section 2 and we investigate these properties empirically in Section 6.1.

4 Constructing the surrogate of log p(θ|y1:T )

In this section, we brieﬂy discuss Steps (ii) and (iii) of gsa, where we construct a surrogate function to

mimic the log-posterior. The interested reader is referred to Brochu et al. [2010] for more details.

4.1 Gaussian process prior

gps [Rasmussen and Williams, 2006] are an instance of so-called Bayesian non-parametric models and

can be interpreted as a generalisation of the multivariate Gaussian distribution to an inﬁnite dimensional

setting. A realisation drawn from a gp can therefore be seen as an inﬁnite vector of real values (a function

over the real space Rp). To construct the surrogate function, we assume a priori that the log-posterior

is distributed according to

log p(θ|y1:T ) ∼ GP(cid:0)m(θ), κ(θ, θ(cid:48))(cid:1).

(10)

Note that this does not correspond to an assumption that the log-posterior of the parameters in the ssm

is Gaussian. Here, GP(m, κ) denotes a gp with mean function m and covariance function κ deﬁned by

(cid:104)
m(θ) = E

(cid:105)
log p(θ|y1:T )

,

κ(θ, θ(cid:48)) = E

log p(θ|y1:T ) − m(θ)

log p(θ(cid:48)|y1:T ) − m(θ(cid:48))

(cid:104)(cid:16)

(cid:17)(cid:16)

(cid:17)(cid:105)

.

The mean function speciﬁes the expected value of the process and the covariance function speciﬁes the

dependence between any pair of points on the log-posterior function. The covariance function depends

on a set of hyperparameters, such as the length scale that controls the dependence, see Rasmussen and

Williams [2006] for details. From the clt in (9) and Figure 1, we know that the error in the log-posterior

is approximately Gaussian,

ξk = log (cid:98)pN (θk|y1:T ) ≈ log p(θk|y1:T ) + σξzk,

zk ∼ N (0, 1),

(11)

8

where σ2

ξ denotes some unknown variance estimated in a later stage of the algorithm. Consequently, we

have that the predictive posterior for any test point θ(cid:63) ∈ Θ is given by

log p(θ(cid:63)|y1:T )|Dk ∼ GP

µ(θ(cid:63)|Dk), σ2(θ(cid:63)|Dk) + σ2
ξ

(cid:16)

(cid:17)
,

µ(θ(cid:63)|Dk) = m(θ(cid:63))

σ2(θ(cid:63)|Dk) = κ(θ(cid:63), θ(cid:63))

+ κ(θ(cid:63), θk)

(cid:104)
κ(θk, θk) + σ2

ξ Ik×k

(cid:105)−1(cid:110)

ξk − m(θ(cid:63))

(cid:111)
,

− κ(θ(cid:63), θk)

(cid:104)
κ(θk, θk) + σ2

ξ Ik×k

(cid:105)−1

κ(θk, θ(cid:63)),

(12a)

(12b)

(12c)

where we have introduced the notation Dk = {θk, ξk} for the information available at iteration k. The

surrogate function of the log-posterior is then given by (12). The major cost in computing the predictive

posterior is incurred by the matrix inversion which is proportional to O(k3). Hence, sparse formulations

of the gp can be useful to decrease the computation cost for large K, see Rasmussen and Williams [2006].

4.2 Acquisition function

The surrogate function given by the gp predictive distribution gives us the estimate of the log-posterior

and its uncertainty. As previously mentioned, this is useful information for creating an acquisition rule

AQ(θ(cid:63)|Dk) to balance exploration and exploitation. We can then determine the next point in which to

sample the log-posterior by

θk+1 = argmax
θ(cid:63)∈ΘGPO

AQ(θ(cid:63)|Dk),

where ΘGPO denotes a search space deﬁned by the user. In this paper, we make use of the expected

improvement (ei) due to its general good performance in numerical evaluations [Lizotte, 2008]. To

derive the ei rule, consider the predicted improvement deﬁned as

PI(θ(cid:63)) = max

0, log p(θ(cid:63)|y1:T ) − µmax − ζ

∀θ(cid:63) ∈ ΘGPO,

(13)

(cid:110)

(cid:111)
,

where µmax denotes the maximum value of µ(θ) for the sampled points θ ∈ θk. Here, we introduce ζ as

a parameter that controls the exploitation/exploration behaviour as in Lizotte [2008]. Hence for ζ = 0,

we have that PI(θ(cid:63)) is the diﬀerence between the posterior and the maximum value it assumes in the set

of sampled points. Therefore, it is positive for points where the log-posterior is larger than the current

peak and zero for all other points. The ei rule is obtained by computing the expected value of (13) with

9

respect to (12). This results in the acquisition rule given by

(cid:104)
σ(θ(cid:63)|Dk)

Z(θ(cid:63))Φ(cid:0)Z(θ(cid:63))(cid:1) + φ(cid:0)Z(θ(cid:63))(cid:1)(cid:105)(cid:27)

+ ˇzk,

(14)

(cid:26)

θk+1 =

argmax
θ(cid:63)∈ΘGPO

(cid:104)
Z(θ(cid:63)) = σ−1(θ(cid:63)|Dk)

µ(θ(cid:63)|Dk) − µmax − ζ

(cid:105)

,

where φ and Φ denotes the density and distribution function of the standard Gaussian distribution,

respectively. Here, we jitter to the solution of the optimisation problem by adding Gaussian noise

ˇzk ∼ N (0, Σ) with covariance Σ. In practice, this improves the exploration and increases the accuracy of

the obtained parameter estimates. Jittering is also advocated by Bull [2011] and Gutmann and Corander

[2016] to increase the convergence rate of gpo.

The optimisation in (14) is possibly non-convex but it is cheap to evaluate the objective function as it

only amounts to evaluating the gp predictive posterior in one point. We make use of the the gradient-free

dividing rectangles (direct; Jones et al., 1993) to solve (14) over ΘGPO, which is determined from the

support of the prior distribution p(θ).

5 The GSA algorithm

gsa (Algorithm 2) is obtained by combining smc-abc (Algorithm 1) to approximate the log-posterior

point-wise and gpo to create a surrogate function that mimics the log-posterior around its mode. In this

section, we discuss some user choices and convergence results for the algorithm. See A for the details of

the implementation employed in this paper.

5.1 Initialisation and convergence criteria

We initialise Algorithm 2 at Line 1 to ﬁnd some suitable hyperparameters for the gp prior. The hyper-

parameters are estimated using L initial samples from the log-posterior obtained by Latin hypercube

sampling. We then execute Algorithm 1 for each of the sampled parameters {θ(cid:63)

1, θ(cid:63)

2, . . . , θ(cid:63)

L} to obtain

D(cid:63)

L by the analogue of Line 4 in Algorithm 2. After the initialisation of the algorithm, we can update

the hyperparameters with Line 5 at every iteration or at some pre-deﬁned interval. Estimating the

hyperparameters is computationally costly and it is therefore recommended to re-estimate them only at

some ﬁxed interval. The algorithm is usually executed for some pre-deﬁned number of iterations K or

until the ei is smaller than some ∆EI > 0, i.e. until k satisﬁes EI(θk|D) < ∆EI.

5.2 Extracting the Laplace approximation

From Algorithm 2, we obtain the predictive posterior mean function µ(θ(cid:63)|D), which hopefully is an

accurate surrogate for the log-posterior. We then proceed to extract the map estimate (cid:98)θMAP deﬁned

10

Algorithm 2 Find a Laplace approximation of log p(θ|y1:T ) using GSA
Inputs: Algorithm 1, p(θ) (parameter prior), m(θ) (mean function), κ(θ, θ(cid:48)) (covariance function), θ1
(initial parameter), Σ (jittering covariance) and ΘGPO (optimisation bounds).
Output: (cid:98)θMAP (est. of the parameter) and (cid:98)J ((cid:98)θMAP) (est. of posterior covariance).

1: Estimate the hyperparameters of the gp prior by using some initial data D(cid:63)
L.
2: Initialise the parameter to θ1 and set k = 1.
3: while convergence criteria is not satisﬁed do
4:

Estimate ξk = log (cid:98)p(θk|y1:T ) by Algorithm 1 and set Dk = {D(cid:63)
(if required ) Update the hyperparameters of the gp prior using Dk.
Construct the gp surrogate log p(θ(cid:63)|y1:T )|Dk using (12).
Compute µmax = argmaxθ∈θk
Compute θk+1 by (14) using optimisation over ΘGPO.
Set k = k + 1.

5:
6:
7:
8:
9:
10: end while
11: Compute the map estimate (cid:98)θ by optimising µ(θ|Dk) using optimisation over ΘGPO.
12: Extract the Hessian estimate J ((cid:98)θMAP) using e.g. ﬁnite-diﬀerences on µ(θ|Dk).

L, θk, ξk}.

µ(θ|Dk).

by (4). As the surrogate is smooth and cheap to evaluate, we can carry out the optimisation using

standard methods such as the direct algorithm to ﬁnd the mode. The estimate of the Hessian of the

log-posterior J ((cid:98)θMAP) can be computed by a ﬁnite-diﬀerence scheme, by analytically computing the

Hessian of µ(θ(cid:63)|D) when possible or by using a quasi-Newton algorithm to solve (4).

5.3 Convergence properties

There are only a limited number of results regarding the convergence properties of the gpo algorithm in

the literature. Most of the properties have been studied numerically by benchmarking the gpo algorithm

against alternatives on a large number of optimisation problems. However, some theoretical results are

discussed by Bull [2011] and Vazquez and Bect [2010]. They conclude that gpo using the ei rule samples

the log-posterior densely if it is continuous with respect to the gp prior. Also, gpo achieves an optimal
convergence rate of the order O(cid:0)(K log K)−5/p(log K)1/2(cid:1) for the Mat´ern 5/2 covariance function, where

K and p denote the number of samples and parameters to infer, respectively.

6 Numerical illustrations and applications

In this section, we provide four illustrations of the properties and advantages of the proposed algorithm.

The implementation details are collected in A and the source code with data is available for download

at GitHub: https://github.com/compops/gpo-smc-abc/.

11

6.1 Stochastic volatility with Gaussian log-returns

Consider the stochastic volatility model with Gaussian log-returns (gsv),

x0 ∼ N

x0; µ,

σ2
v
(cid:0)1 − φ2(cid:1)

(cid:33)

,

xt+1 ∼ N

xt+1; µ + φ(xt − µ), σ2
v

(cid:17)

,

yt ∼ N

yt; 0, exp(xt)

(cid:17)

,

(cid:32)

(cid:16)

(cid:16)

(15a)

(15b)

(15c)

with parameters θ = {µ, φ, σv}. Here, the latent log-volatility is assumed to follow a mean-reverting

random walk with mean µ ∈ R, persistence φ ∈ (−1, 1) and standard deviation of the increments

σv ∈ R+. We generate a single synthetic data set from this model with T = 500 observations, parameters

θ(cid:63) = {0.20, 0.96, 0.15} and initial state x0 = 0.

This model is interesting since it allows us to compare the gsa algorithm to an algorithm that makes

use of a standard particle ﬁlter to estimate the log-posterior. This is possible as we can evaluate gθ(yt|xt)

in closed-form for (15). We refer to this version of the algorithm as gs and it corresponds to the gsa

algorithm with tolerance parameter (cid:15) = 0.

In the left part of Figure 2, we present the posterior estimates from gs (solid curve) and pmh

(histogram). We begin by observing a good ﬁt of the Laplace approximation from gs to the histogram

approximation obtained by pmh; both the location and the spread of the posterior approximations are

similar. Using gs results in a speed-up of about 30 times (15, 000/500 ≈ 30) compared with pmh, where

the latter is seen as the gold standard in computational Bayesian inference The main computational cost

for both algorithms is incurred by running the particle ﬁlter (one run per iteration). The overhead for

the proposed algorithm (estimating hyperparameters in the gp and computing the predictive posterior)

is negligible compared with the computational cost of running a particle ﬁlter.

We now continue with analysing the Laplace approximations obtained by gsa. In the right side of

Figure 2, we present the posterior approximations obtained by varying the tolerance parameter (cid:15) between

0.1 and 0.5 to study the bias and robustness of the approximation. Darker shades of grey indicate a

larger tolerance parameter. For (cid:15) = 0.1 and 0.2, we see that the approximations are rather poor with a

signiﬁcant bias and bad ﬁt to the spread. However for the other choices of (cid:15), the approximations from

gsa converges quickly to be similar to gs (solid curve). From these results, we conclude that there is a

bias in the posterior approximation when (cid:15) is too small and the approximation tends to grow wider as (cid:15)

increases. Hence, a bias-variance trade-oﬀ is introduced into the posterior approximation depending on

(cid:15).

We continue by comparing gs to spsa, where the latter is a gradient-free alternative with good

convergence properties and performance in many applications. spsa operates by constructing a ﬁnite-

12

Figure 2: Marginal parameter posteriors for the synthetic data in the gsv model. Left: Solid curves
indicate the Laplace approximations of the posterior using gs for µ (green), φ (orange) and σv (purple).
The histograms represent the exact posteriors estimated using pmh and the dark grey (left) curves
indicate the prior distributions. Right: Laplace approximations (shaded areas) from gs for the three
parameters. The grey curves (right) indicate the Laplace approximations obtained by gsa using ﬁve
diﬀerent values of the tolerance parameter (cid:15) in the abc approximation.

13

Figure 3: The trace of the map estimate for µ (green), φ (orange) and σv (purple) from gs (solid) and
spsa (solid-cicle) as a function of the number of log-posterior samples. The ﬁrst L = 50 samples of
gs are used to estimate the hyperparameters. Both algorithms are run for a total of 700 log-posterior
samples. Dashed lines indicate the posterior means from pmh.

diﬀerence approximation of the gradient at each iteration after which it takes a step in the gradient

direction. Note that spsa requires two log-posterior estimates at each iteration compared with only one

sample in the gs. Another possible drawback with spsa is that it only provides the map estimate and

no quantiﬁcation of the posterior uncertainty.

In Figure 3, we compare the map parameter estimates of two algorithms as a function of the number

of log-posterior estimates. The ﬁrst L = 50 samples of the log-posterior are used to estimate the

hyperparameters of the gp prior. After this initial phase, gs converges quickly to reasonable values of

the parameters using about half the number of posterior samples. This results in a speed-up of factor 2

when using the proposed algorithm compared with spsa.

6.2 Stochastic volatility model α-stable log-returns

In the upper part of Figure 4, we present the log-returns of future contracts on coﬀee during the period

between June, 2013 and December, 2014. The data seem to indicate the presence of jumps around the

ﬁrst half of 2014. This is common in ﬁnancial data and can be modelled in a number of diﬀerent ways.

To this end, we consider the stochastic volatility model with α-stable log-returns (αsv) given by

x0 ∼ N

x0; µ,

σ2
v
(cid:0)1 − φ2(cid:1)

(cid:33)

,

xt+1 ∼ N

xt+1; µ + φ(xt − µ), σ2
v

(cid:17)

,

yt ∼ A

yt; α, exp(xt)

(cid:17)

,

(cid:32)

(cid:16)

(cid:16)

(16a)

(16b)

(16c)

14

Figure 4: Upper: log-returns (green) of coﬀee futures and the estimate of the log-volatility (dark grey).
The shaded area indicates the 95% conﬁdence region for the log-returns according to the αsv model.
Middle and lower: marginal parameter posteriors in the αsv model estimated by gsa (solid curves) using
10 independent runs and pmh (histogram) for µ (green), φ (orange), σv (purple) and α (magenta). Dark
grey curves indicate the prior distributions.

15

Algorithm 3 Copula modelling using αsv as the marginal models
Stage 1 (repeated for each asset i)
1: Run Algorithm 2 to obtain the log-volatility estimate (cid:98)x1:T,i and the parameter estimate (cid:98)θMAP,i of (16). Compute the

ﬁltered log-returns (cid:98)et,i by

(cid:98)et,i = exp

−

(cid:18)

(cid:19)

yt,

1
2 (cid:98)xt,i

for t = 1, . . . , T.

2: Estimate the cumulative distribution function (cdf) denoted by (cid:98)Gi from {(cid:98)et,i}T

t=1. Compute the probability transfor-

mation of the residuals by

(17)

(18)

Stage 2
4: Infer the parameters of the copula to model the dependency between {{(cid:98)ut,i}T

t=1}30

i=1.

(cid:98)ut,i = (cid:98)Gi((cid:98)et,i),

for t = 1, . . . , T.

with parameters θ = {µ, φ, σv, α} and A(α, γ) denoting a zero-mean symmetric α-stable distribution with

stability parameter α ∈ (0, 2) and scale γ ∈ R+. The stability parameter determines the tail behaviour

of the distribution, see Nolan [2003] for a discussion of the α-stable distribution and its properties.

The likelihood is in general intractable for this model and therefore approximations such as the particle

ﬁltering using abc are required.

In the middle and lower part of Figure 4, we compare the posterior approximations obtained with

gsa (solid curves) with 10 independent runs and pmh (histograms). We see that the mixing of pmh

is quite poor for this model as the histograms are peaky. This is a common problem as Markov chains

tends to get stuck if the log-posterior estimates are noisy. However, the posterior estimates overlap and

seems to give reasonable parameter values for each run of gsa. The main diﬀerence is in the estimate

of α, which could be the result of the abc approximation or problems with observability. Finally, the

estimate the log-volatility (black) seems reasonable when compared to the log-returns.

6.3 Computing VaR for a portfolio of oil futures

We follow Charpentier [2015] to construct a copula model to capture the dependency structure between

prices of oil future contracts. The data considered is presented in Figure 5 and consists of weekly

log-returns between January 10, 1997 and June 4, 2010 of Brent (produced in the North Sea), Dubai

(produced in the Persian Gulf) and Maya (produced in the Gulf of Mexico) oil. We partition the data

set into two parts and make use of the ﬁrst 465 data points for estimating the αsv and copula models.

The remaining 233 data points are kept for validating the model by backtesting.

We adopt the commonly used two-stage approach to copula modelling outlined in Algorithm 3, where

marginal models are ﬁrst ﬁtted separately to each of the log-return series, and then combined using a

copula to model the dependency structure [Joe, 2005]. We make use of the procedure in Section 6.2 to

estimate the parameters of an αsv model for each type of oil. The ﬁltered residuals (17) are assumed to

be independent and identically distributed as A((cid:98)αi, 1) by (16) if x1:T is known. Stage 1 is carried out
independently for each asset and is therefore straight-forward to implemented in a parallel manner. The

16

Figure 5: Left: weekly log-returns of Brent (green), Dubai (orange) and Maya (purple) oil between
January 10, 1997 and June 4, 2010. The shaded area indicates the 95% conﬁdence region for the log-
returns according to the αsv model. The dark grey curves indicate estimates of the log-volatility. Right:
the corresponding transformed ﬁltered residuals (cid:98)ut,i.

17

Figure 6: Estimated values of VaR0.99(et) for an equally weighted portfolio of the three oil futures using
the gsv (magenta) and αsv (green) model with the Student-t copula. The dashed line indicates the
division of estimation and validation data.

use of the proposed algorithm decreases the computational time for this stage from hours or days for

each asset to about half an hour compared with pmh.

We follow McNeil et al. [2010, p. 231-231] to model the dependency in the residuals. This amounts to

applying a probability transform via the empirical cdf on the residuals into the 3-dimensional hypercube,

see right part of Figure 5. The transformed residuals {(cid:98)u1:T,1, . . . , (cid:98)u1:T,d} are then combined by a Student’s
t-copula function to ﬁnd a model for the joint distribution. The degrees of freedom and the correlation

matrix in the Student’s t-copula are estimated using map and matching of moments via Kendall’s τ ,

respectively.

Finally, we make use of the copula models and their margins to estimate the var for each type of oil.

The var at conﬁdence ¯α ∈ (0, 1) is deﬁned by

VaR ¯α(et) = inf

− et ∈ R : G(−et) ≥ ¯α

(cid:110)

(cid:111)
,

i.e. the smallest loss −et such that probability that the loss (the negative log-return) exceeds −et is

no larger then (1 − ¯α). We adopt a Monte Carlo approach to estimate VaR ¯α(et) by: (i) simulating

from the copula, (ii) obtain simulated ﬁltered residuals by applying a quantile transform based on the

empirical cdf, (iii) computing the resulting log-returns by the estimated volatility and the inverse of

(17). We repeat this 100, 000 times for each asset and then compute the empirical quantile corresponding

to VaR0.99(et) for an equally weighted portfolio. However, more advanced portfolio weighting schemes

can be easily implemented.

The resulting var-estimate is presented Figure 6, where we compared the αsv model with a gsv

model (15) estimated using gs in a similar manner. We note that the estimates from the two models

are quite diﬀerent especially at the end of the data series. Backtesting on the validation data gives 0

violations for both models and the expected number of violations is 2.3. The gsa algorithm requires

18

Figure 7: The log-returns (grey dots) and the estimated values of VaR0.99(et) for an equally weighted
portfolio of stocks using the gsv (magenta) and αsv (green) model with the Student’s t-copula. The
dashed line indicates the division of estimation and validation data.

around 30 − 60 minutes to infer the model for each of the three assets, where the inference is straight-

forward to run on parallel architecture. The corresponding computational time required by the pmh

algorithm would be in the order of 18 − 24 hours.

6.4 Computing VAR for a portfolio of stocks

We oﬀer a ﬁnal numerical example to illustrate that the proposed method can be applied to large port-

folios as well. The data that we consider is the 30 industrial portfolio provided by Kenneth French. The

portfolio consists of monthly log-returns of 30 diﬀerent industrial sectors during the period September,

1926 to December, 2015. Again, we partition the data into an estimation set and a validation set with

716 and 358 data points, respectively.

We adopt the same procedure as in Section 6.3 and the results are presented in Figure 7. The

conclusions are similar with both var estimates being quite similar. Backtesting gives 0 and 1 violations

for the αsv and gsv models, respectively. The expected number of violations is 3.5. The computational

speed-up with gsa compared with pmh is similar to in Section 6.3 as the number of observations per

asset are similar.

7 Conclusions

We have proposed gsa, an algorithm to approximate the posterior distribution in ssms with intractable

likelihoods. The illustrations provided in Section 6 indicate that the proposed algorithm is quite accurate

and exhibits a substantial decrease in the computational cost. We obtain similar posterior estimates to

pmh with a speed-up of one or two orders of magnitude, which reduces computational time from tens of

hours to tens of minutes. Moreover, gsa seems to be quite robust to noise in the log-posterior estimates,

19

which typically results in that the pmh algorithm gets stuck at times and that the spsa algorithm

converges slowly. Overall, this shows that the proposed algorithm is an eﬃcient inference method that

makes it possible for practitioners to use models with intractable likelihoods, such as copula models with

α-stable margins, in applied work.

Future work includes: (i) adopting a sparse representation of the gp, (ii) developing new acqusi-

tion functions, (iii) making use of better tailored covariance functions and (iv) incorperating adaptive

approaches for choosing the tolerance level (cid:15). Some ideas for sequential and sparse representations of

gps are discussed by Huber [2014] and Bijl et al. [2015].

It would also be interesting to design new

acquisition functions to obtain good estimates of the Hessian of the log-posterior or higher order mo-

ments at the same time as the map estimate. This could improve the Laplace approximation of the

parameter posterior and open up for alternative posterior approximations such as using the skewed Stu-

dent’s t-distribution. Moreover, an interesting approach for tailored gp priors would be to make use of

non-stationary covariance functions [Paciorek and Schervish, 2004]. This would capture the fact that

the log-posterior often falls oﬀ rapidly in some parts of the parameter space and is almost ﬂat in other

parts.

Finally, it would be beneﬁcial to include some adaptive approach to select (cid:15) in the smc-abc algorithm

to decrease the number of choices for the user. As previously discussed, we initially implemented the

adaptive algorithms proposed in Del Moral et al. [2012] and Calvet and Czellar [2015]. However, we ran

into problems when using them to approximate the log-posterior values using moderate values of N . The

estimates exhibited a large bias that resulted in biased estimates of θ using pmh. As a result, we choose

not to adapt (cid:15) to be able to keep N much smaller. It is therefore interesting to explore other adaptive

schemes that provide good estimates of the log-posterior using a moderate value of N .

Acknowledgements

This work was supported by: Probabilistic modeling of dynamical systems (Contract number: 621-2013-

5524) and cadics, a Linnaeus Center, both funded by the Swedish Research Council. The simulations

were performed on resources provided by the Swedish National Infrastructure for Computing (snic) at

Link¨oping University, Sweden. J. Dahlin would like to thank Fredrik Lindsten, Neil Lawrence and Carl-

Henrik Ek for fruitful discussions. Thanks also to Joerg M. Gablonsky, Abraham Lee, Per A. Brodtkorb

and the GPy team for making their Python implementations available.

A Implementation details

gpo algorithm: We make use of the GPy package [The GPy authors, 2014] for calculating the gp

predictive posterior and estimating the gp prior hyperparameters.

In this paper, we assume a zero

20

prior mean function m(θ) = 0 and a combination of the bias and the Mat´ern 5/2 covariance functions.

This choice corresponds to a prior for the log-posterior with some non-zero mean and two continuous

derivatives. These are reasonable assumptions as this kind of smoothness is assumed in the Laplace

approximation. We estimate the hyperparameters using empirical Bayes (eb), i.e. by optimising the

marginal likelihood with respect to λ. More advanced schemes that marginalise over the hyperparameters

using slice sampling [Murray et al., 2010] or smc [Svensson et al., 2015] can be used within sga as well.

For the acquisition rule in (14), we use ζ = 0.01 and Σ = 0.01Ip. We initialise the gpo algorithm using

L = 50 samples obtained using Latin hypercube sampling with the implementation written by Abraham

Lee available at https://pypi.python.org/pypi/pyDOE. The optimisation problems in Lines 8 and

11 in Algorithm 2 are solved using the direct implementation written by Joerg M. Gablonsky, avail-

able from https://pypi.python.org/pypi/DIRECT/. Finally for Line 12, we make use of the Python

implementation by Per A. Brodtkorb available at https://pypi.python.org/pypi/Numdifftools.

Section 6.1: We use N = 2, 000 particles in gs and N = 2, 000 particles with the Gaussian density

with tolerance level (cid:15) = 0.20 and ψ(x) = x in gsa to produce the results in Figure 2. We run the gpo

algorithms for K = 450 iterations after the initialisation and re-estimate the hyperparameters of the gp

prior every 25th iteration. The search space for the gpo algorithm ΘGPO is given by µ ∈ (0, 1), φ ∈ (0, 1)

and σv ∈ (0.01, 1). We use the following prior densities

p(µ) ∼ N (µ; 0, 0.22),

p(φ) ∼ T N (−1,1)(φ; 0.9, 0.052),

p(σv) ∼ G(σv; 2, 20),

where T N (a,b)(·) denotes a truncated Gaussian distribution on [a, b], G(a, b) denotes the Gamma distri-

bution with mean a/b.

For pmh, we make use of the smc-abc algorithm with N = 2, 000 particles to estimate the log-

posterior. We initialise pmh in θ0 = {0.10, 0.95, 0.12} and run the algorithm for M = 15, 000 iterations

(discarding the ﬁrst 5, 000 as burn-in). The parameter proposal is selected as

q(θ(cid:48)|θk) = N (θ(cid:48); θk, Σq),

Σq =

· 10−4 · diag(137, 7, 38),

2.5622
3

which results from an asymptotic rule-of-thumb, see Dahlin and Sch¨on [2015], with an estimate of the

posterior covariance from a pilot run.

For spsa, we make use of N = 2, 000 particles and follow Spall [1998] to select the hyperparameters

as a = 0.001, c = 0.30, A = 35, α = 0.602 and γ = 0.101 using pilot runs.

Section 6.2: The real-world data is computed as yt = 100[log(st) − log(st−1)] , where st denotes

the price of a future contract on coﬀee1. We follow Yıldırım et al. [2014] and apply ψ(x) = arctan(x)

to stabilise the variance of the likelihood (and gradient estimate). A two step approach is applied to

1Data available at: https://www.quandl.com/CHRIS/ICE_KC2.

21

sample from the zero-mean symmetric α-stable distribution A(α, γ). First, we sample v(1)

t ∼ Exp(1) and

v(2)
t ∼ U(−π/2, π/2). Then, we obtain a sample (when α (cid:54)= 1) by applying the transformation

ˇyt = γ

sin (cid:0)αv(2)

(cid:1)

t

(cid:34)

(cid:2) cos(v(2)

)(cid:3)1/α

t

cos (cid:2)(α − 1)v(2)
v(1)
t

t

(cid:35) 1−α

α

(cid:3)

.

See Nolan [2003] for more on the generation of α-stable random numbers.

We use N = 2, 000 particles with the Gaussian density with tolerance level (cid:15) = 0.10 in smc-abc

to estimate the log-posterior. We run the gpo algorithm using the same settings as before but add

α ∈ (1.2, 2) to the search space and p(α) ∼ B(α/2; 20, 2) to the prior distributions, where B(a, b) denotes

the Beta distribution. We initialise pmh in θ0 = {0.22, 0.93, 0.25, 1.55} and the parameter proposal is

selected using a pilot run as

q(θ(cid:48)|θk) = N (θ(cid:48); θk, Σq),

Σq =

· 10−3 · diag(26, 1, 9, 11).

2.5622
4

Section 6.3 and 6.4: Most of the settings are the same for the oil2 and stock3 portfolio examples.

We use N = 5, 000 particles in the smc and smc-abc algorithms and keep the remaining settings as

Sections 6.1 and 6.2. For the stock portfolio example, we change the search space of µ to (0, 4) as the

weekly log-returns in this data set can be much larger than the daily log-returns in the other data sets.

The map estimate of the degrees of freedom in the copula is obtained by a quasi-Newton solver using a

uniform prior.

References

C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo methods. Journal of the

Royal Statistical Society: Series B (Statistical Methodology), 72(3):269–342, 2010.

H. Bijl, J-W. van Wingerden, T. B. Sch¨on, and M. Verhaegen. Online sparse Gaussian process regres-

sion using FITC and PITC approximations. In Proceedings of the 17th IFAC Symposium on System

Identiﬁcation (SYSID), pages 703–708, Beijing, China, October 2015.

E. Brochu, V. M. Cora, and N. De Freitas. A tutorial on Bayesian optimization of expensive cost

functions, with application to active user modeling and hierarchical reinforcement learning. Pre-print,

2010. arXiv:1012.2599v1.

Research, 12:2879–2904, 2011.

A. D. Bull. Convergence rates of eﬃcient global optimization algorithms. Journal of Machine Learning

2Data available at: http://freakonometrics.free.fr/oil.xls.
3Data available at: http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html.

22

L. E. Calvet and V. Czellar. Accurate methods for approximate Bayesian computation ﬁltering. Journal

of Financial Econometrics, 13(4):798–838, 2015.

A. Charpentier. Pr´evision avec des copules en ﬁnance. Technical report, May 2015. URL https:

//hal.archives-ouvertes.fr/hal-01151233.

J. Dahlin and F. Lindsten. Particle ﬁlter-based Gaussian process optimisation for parameter inference.

In Proceedings of the 19th IFAC World Congress, Cape Town, South Africa, August 2014.

J. Dahlin and T. B. Sch¨on. Getting started with particle Metropolis-Hastings for inference in nonlinear

models. Pre-print:, 2015. arXiv:1511.01707v4.

T. A. Dean and S. S. Singh. Asymptotic behaviour of approximate Bayesian estimators. Pre-print:,

2011. arXiv:1105.3655v1.

P. Del Moral, A. Doucet, and A. Jasra. An adaptive sequential Monte Carlo method for approximate

Bayesian computation. Statistics and Computing, 22(5):1009–1020, 2012.

A. Doucet and A. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years later.

In

D. Crisan and B. Rozovsky, editors, The Oxford Handbook of Nonlinear Filtering. Oxford University

J. Durbin and S. J. Koopman. Time series analysis by state space methods. Oxford University Press, 2

Press, 2011.

edition, 2012.

E. Ehrlich, A. Jasra, and N. Kantas. Gradient free parameter estimation for hidden Markov models with

intractable likelihoods. Methodology and Computing in Applied Probability, 17(2):315–349, 2015.

M. U. Gutmann and J. Corander. Bayesian optimization for likelihood-free inference of simulator-based

statistical models. Journal of Machine Learning Research, 17(125):1–47, 2016.

M. F. Huber. Recursive Gaussian process: On-line regression and learning. Pattern Recognition Letters,

45:85–91, 2014.

A. Jasra, S. S. Singh, J. S. Martin, and E. McCoy. Filtering via approximate Bayesian computation.

Statistics and Computing, 22(6):1223–1237, 2012.

H. Joe. Asymptotic eﬃciency of the two-stage estimation method for copula-based models. Journal of

Multivariate Analysis, 94(2):401–419, 2005.

D. R. Jones, C. D. Perttunen, and B. E. Stuckman. Lipschitzian optimization without the Lipschitz

constant. Journal of Optimization Theory and Applications, 79(1):157–181, 1993.

D. J. Lizotte. Practical Bayesian optimization. PhD thesis, University of Alberta, 2008.

23

L. Ljung. System identiﬁcation: theory for the user. Prentice Hall, 1999.

J-M. Marin, P. Pudlo, C. P. Robert, and R. J. Ryder. Approximate Bayesian computational methods.

Statistics and Computing, 22(6):1167–1180, 2012.

A. J. McNeil, R. Frey, and P. Embrechts. Quantitative risk management: concepts, techniques, and tools.

Princeton University Press, 2010.

E. Meeds and M. Welling. GPS-ABC: Gaussian process surrogate approximate Bayesian computation.

In Proceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), Quebec City,

Canada, July 2014.

2010.

I. Murray, R. Adams, and D. MacKay. Elliptical slice sampling. In Proceedings of the 13th International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 541–548, Sardinia, Italy, May

J. Nolan. Stable distributions: models for heavy-tailed data. Birkhauser, 2003.

C. J. Paciorek and M. J. Schervish. Nonstationary covariance functions for Gaussian process regression.

In Proceedings of the 2004 Conference on Neural Information Processing Systems (NIPS), pages 273–

280, Vancouver, Canada, December 2004.

Bayesian Analysis, 10(3):665–710, 2015.

M. Panov and V. Spokoiny. Finite sample Bernstein-von-Mises theorem for semiparametric problems.

M. K. Pitt, R. S. Silva, P. Giordani, and R. Kohn. On some properties of Markov chain Monte Carlo

simulation methods based on the particle ﬁlter. Journal of Econometrics, 171(2):134–151, 2012.

C. E. Rasmussen and C. K. I. Williams. Gaussian processes for machine learning. MIT Press, 2006.

J. C. Spall.

Implementation of the simultaneous perturbation algorithm for stochastic optimization.

IEEE Transactions on Aerospace and Electronic Systems, 34(3):817–823, 1998.

S. V. Stoyanov, B. Racheva-Iotova, S. T. Rachev, and F. J. Fabozzi. Stochastic models for risk estimation

in volatile markets: a survey. Annals of Operations Research, 176(1):293–309, 2010.

A. Svensson, J. Dahlin, and T. B. Sch¨on. Marginalizing Gaussian process hyperparameters using sequen-

tial Monte Carlo. In Proceedings of the 6th IEEE International Workshop on Computational Advances

in Multi-Sensor Adaptive Processing (CAMSAP), Cancun, Mexico, December 2015.

The GPy authors. GPy: A Gaussian process framework in Python. http://github.com/SheffieldML/

GPy, 2014.

24

E. Vazquez and J. Bect. Convergence properties of the expected improvement algorithm with ﬁxed mean

and covariance functions. Journal of Statistical Planning and inference, 140(11):3088–3095, 2010.

S. N. Wood. Statistical inference for noisy nonlinear ecological dynamic systems. Nature Letters, 466:

1102–1104, 2010.

24(3):846–865, 2014.

S. Yıldırım, S. S. Singh, T. Dean, and A. Jasra. Parameter estimation in hidden Markov models with in-

tractable likelihoods using sequential Monte Carlo. Journal of Computational and Graphical Statistics,

25

