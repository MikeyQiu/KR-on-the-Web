Controlling Decoding for More Abstractive Summaries
with Copy-Based Networks

Noah Weber∗1, Leena Shekhar∗1, Niranjan Balasubramanian1, Kyunghyun Cho2
1Stony Brook University, NY
{nwweber, lshekhar, niranjan}@cs.stonybrook.edu
2New York University, NY
Kyunghyun.cho@nyu.edu

Abstract

Attention-based neural abstractive summariza-
tion systems equipped with copy mechanisms
have shown promising results. Despite this
success, it has been noticed that such a system
generates a summary by mostly, if not entirely,
copying over phrases, sentences, and some-
times multiple consecutive sentences from an
input paragraph, effectively performing ex-
tractive summarization. In this paper, we ver-
ify this behavior using the latest neural ab-
stractive summarization system - a pointer-
generator network (See et al., 2017). We pro-
pose a simple baseline method that allows us
to control the amount of copying without re-
training. Experiments indicate that the method
provides a strong baseline for abstractive sys-
tems looking to obtain high ROUGE scores
while minimizing overlap with the source arti-
cle, substantially reducing the n-gram overlap
with the original article while keeping within 2
points of the original model’s ROUGE score.

1

Introduction

Automatic abstractive summarization has seen a
renewed interest
in recent years (Rush et al.,
2015; Nallapati et al., 2016b; See et al., 2017)
building on attention-based encoder-decoder mod-
els originally proposed for neural machine trans-
lation (Bahdanau et al., 2014).
Recent ap-
proaches rely on encoder-decoder formulations
augmented with copy-mechanisms to produce ab-
stractive summaries. The encoder-decoder allows
the model to generate new words that are not part
of the input article, while the copy-mechanism
allows the model to copy over important details
from the input even if these symbols are rare in
the training corpus overall. See et al. (2017) and
Paulus et al. (2017) use a pointer-generator model
which produces a summary using an interpolation

∗*These authors contributed equally to this work.

of generation and copying probabilities. The inter-
polation is controlled by a mixture coefﬁcient that
is predicted by the model at each time step.

Even though the pointer-generator mechanism,
in theory, enables the model to interpolate be-
tween extractive (copying) and abstractive (gener-
ating) modes, in practice the extractive mode dom-
inates. See et al. (2017) for instance reported that
“at test time, [the conditional distribution is] heav-
ily skewed towards copying”. This is also evident
from the examples from the state-of-the-art system
presented in Table 3 of Paulus et al. (2017).

We carefully conﬁrm this behavior using the
neural abstractive summarization system by See
et al. (2017). We consider the n-gram overlap
between the input paragraph and generated sum-
mary and observe extremely high overlaps across
varying n’s (from 2 up to 25). When the cover-
age penalty, which was found to improve the sum-
marization quality in See et al. (2017), was intro-
duced, these overlaps further increased. On the
other hand, ground-truth summaries have almost
no overlaps. This clearly suggests that the neu-
ral abstractive summarization system largely per-
forms extractive summarization.

We introduce a simple modiﬁcation to beam
search to promote abstractive modes during de-
coding. In particular, for each hypothesis we track
the mixture coefﬁcients that are used to combine
the copying and generating probabilities at each
time step. As See et al. (2017) report, during
test time, the mixture coefﬁcients are often sig-
niﬁcantly low, which predominantly favors copy-
ing. To counter this effect, we introduce an addi-
tional term to the beam score, which penalizes a
hypothesis whose average mixture coefﬁcient de-
viates from a expected mixture, predeﬁned target.
By setting the target appropriately, it allows us
to control the level of abstractiveness at the de-
coding time. We empirically conﬁrm that we can

8
1
0
2
 
r
a

M
 
0
2
 
 
]
L
C
.
s
c
[
 
 
2
v
8
3
0
7
0
.
3
0
8
1
:
v
i
X
r
a

control the abstractiveness while largely maintain-
ing the quality of summary, measured in terms of
ROUGE (Lin, 2004) and METEOR (Lavie and
Denkowski, 2009), without having to retrain a
system. The relative simplicity and performance
of the method makes it a strong baseline for fu-
ture abstractive summarization systems looking to
solve the copying problem during training.

2 Neural Abstractive Summarization
and Copy-Controlled Decoding

2.1 Neural Abstractive Summarization

In this paper, we use the pointer-generator net-
work, proposed by See et al. (2017), as a target
abstractive summarization system. The pointer-
generator network is an extension of an earlier
neural attention model for abstractive sentence
summarization by Rush et al. (2015) that incorpo-
rates the copy mechanism by (Gu et al., 2016a).
Since our focus is on the copying behavior, we
summarize the decoder component of the pointer-
generator network here. We refer the readers to
See et al. (2017) for other details.

At each time step t, the decoder computes three
quantities: (i) a copy distribution over the source
symbols in the input pcopy, (ii) a generating distri-
bution pgen deﬁned over a predeﬁned vocabulary
(all the symbols in the training data), and (iii) a
mixture co-efﬁcient, mt, that is used to combine
the copy and generating distributions.

The decoder computes the copy distribution
at time t via the attention weights αi’s deﬁned
over the encoded representations hi’s of the cor-
responding source symbols xi’s.
Since these
weights are non-negative and sum to one, we can
treat them as the output probability distribution de-
ﬁned over the source symbols, which we refer to
as the copy distribution pcopy. Then, the decoder
computes the generating distribution pgen over the
entire training vocabulary based on the context
vector, h∗
i=1 αihi, and the decoder’s state
st. These two distributions are then mixed based
on a coefﬁcient mt ∈ [0, 1], which is also com-
puted based on the context vector h∗
t , the decoder’s
hidden state st and the previously decoded symbol
ˆyt−1. The ﬁnal output distribution is given by

t = (cid:80)|X|

p(w) =mtpgen(w) + (1 − mt)

Ixi=wαi,

where I is an indicator function. We omitted the
conditioning variables ˆy<t and X for brevity.

|X|
(cid:88)

i=1

The mixture coefﬁcient mt indicates the degree
to which the decoder generates the next symbol
from its own vocabulary. When mt is close to 1,
the decoder is free to generate any symbol from
the vocabulary regardless of its existence in the in-
put. On the other hand, when mt is close to 0, the
decoder ignores pgen and copies over one of the
input symbols to the output using pcopy.

Mismatch between training and decoding
See et al. (2017) observed that the statistics of
the mixture coefﬁcient mt differs signiﬁcantly be-
tween training and testing. During training, the av-
erage of the mixture coefﬁcients was found to con-
verge to around 0.53, while it was much smaller,
close to 0.17, during testing when summaries gen-
erated from the trained model (i.e. without teacher
forcing). Furthermore, most of the generated n-
grams turn out to be exact copies from the input
paragraph with this model. Our analysis also cor-
roborates this observation.

2.2 Copy-Controlled Decoding

With a conditional neural language model, such as
the neural abstractive summarization system here,
we often use beam search to ﬁnd a target sequence.
At each time step, beam search collects top-K pre-
ﬁxes according to a scoring function deﬁned as

s(y≤t, X) = log p(y≤t|X) =

log p(y(cid:48)

t|y<t(cid:48), X).

t
(cid:88)

t(cid:48)=1

In order to bridge between training and decod-
ing in terms of the amount of copying, we propose
a new scoring function:

t
(cid:88)

t(cid:48)=1

s(y≤t, X) =

log p(yt(cid:48)|y<t, X)

(1)

− ηt max(0, m∗ − ¯mt(cid:48)),

a

is

ηt

target
penalty

time-varying
(cid:80)t(cid:48)

coefﬁcient,
strength,

where m∗
is
and
a
¯mt(cid:48) = 1
t(cid:48)(cid:48)=1 mt(cid:48)(cid:48). We use the following
t(cid:48)
scheduling of ηt
to ensure the diversity of
hypotheses
in the early stage of decoding:
ηt = t · η0, although other scheduling strategies
should explored in the future.

The penalty term in Eq. (1) allows us to softly
eliminate any hypothesis whose average mixture
coefﬁcients thus far has been too far away from
the intended ratio. The target average may be
selected via validation or determined manually.

y
t
l
a
n
e
p

e
g
a
r
e
v
o
c
-

)
a
(

y
t
l
a
n
e
p

e
g
a
r
e
v
o
c
+
)
b
(

Figure 1: The n-gram overlap (%) (a) with and (b)
without the coverage penalty. We observe reduction
with the proposed copy-controlled decoding.

Std.

35.77
15.28
32.54

15.03
16.33

C-C

34.10
14.02
31.15

13.73
14.96

+coverage
C-C

Std.

38.82
16.81
35.71

16.73
18.14

36.22
14.69
33.60

14.57
15.84

ROUGE-1
ROUGE-2
ROUGE-L

METEOR
METEOR(cid:63)

# of tokens

56.10

50.47

59.87

50.30

Table 1: The summary quality measured in ROUGE-
X and METEOR. Standard: Standard abstractive beam
search for decoding. C-C: copy-controlled decoding.
METEOR(cid:63): METEOR +stem +synonym +paraphrase.

Later in the experiments, we present both quantita-
tive and qualitative impacts of the proposed copy-
controlled decoding procedure.

Related Work Although decoding algorithms
for conditional neural language models have re-
ceived relatively little interest, there have been a
few papers that have aimed at improving exist-
ing decoding algorithms. One line of such re-
search has been on augmenting the score func-
tion of beam search. For instance, Li et al. (2016)
proposed a diversity promoting term, and shortly
after, Li et al. (2017) generalized this notion by
learning such a term. Another line of research
looked at replacing manually-designed decoding
algorithms with trainable ones based on a recur-
rent network (Gu et al., 2016b, 2017a). The pro-
posed copy-controlled decoding falls in the ﬁrst
category and is applicable to any neural genera-
tion model that partly relies on the copy mech-
anism, such as neural machine translation (Gul-
cehre et al., 2016; Gu et al., 2017b) and data-to-
document generation (Wiseman et al., 2017).

3 Experiments

We use the pretrained pointer-generator network,
trained on CNN/DailyMail data (Hermann et al.,
2015; Nallapati et al., 2016a), provided by See
et al. (2017).

The pretrained network is provided together
with the code based on which we implement the
proposed copy-controlled decoding. It should be
noted that our work is not strictly comparable
to the abstractive work done by Nallapati et al.
(2016b) as the latter was trained and evaluated on
the anonymized dataset and used pretrained word
embeddings.

3.1 Quantitative Analysis

Controlling mt We ﬁrst test whether the pro-
posed scoring function in Eq. (1) does indeed al-
low us to control the mixture coefﬁcient. When
forced to generate summaries of a randomly drawn
subset of 500 validation examples, the pointer-
generator network with the original scoring func-
tion resulted in the average mixture coefﬁcients of
0.24 and 0.26 respectively with and without the
coverage penalty. With the target mixture coef-
ﬁcient m∗ set to 0.4 and the penalty coefﬁcient
η = 0.5, we observed that the average mixture co-
efﬁcient increased to 0.29 and 0.33, respectively,
with and without the coverage penalty. While
the mixture coefﬁcients increased on average, the
ROUGE-1 scores roughly stayed at the same level,
going from 27.49 and 28.87 to 27.63 and 28.86
(w/ and w/o the coverage penalty). We observed
the similar trends with other evaluation metrics.
Based on this observation, we use m∗ = 0.4 and
η = 0.5 with the full test set from here on.

Abstractiveness We then investigate the overlap
between the input paragraph and summary to mea-
sure the novelty in the generated summary. We
count the number of n-grams in a summary and
those that also occur in the original article and look
at the ratio (%). To draw a clear picture, we do so
for a wide range from n = 2 up to n = 25. We
report the number of n-gram overlaps in Fig. 1.

The ﬁrst observation we make is that there is al-
most no overlap between the input paragraph and
its reference summary. There are a few overlap-
ping bi-grams, ≤ 2 on average. On the other hand,
the summaries generated by the pointer-generator
network exhibit signiﬁcantly more n-gram over-
lap. For instance, over 20% 25-grams are found

Article: a federal judge has ordered the defense department to release photos that allegedly show detainees being abused in
detention centers in iraq and afghanistan during the bush administration . the photos wo n’t be made public right away . in an
order issued friday , u.s. district judge alvin k. hellerstein of the southern district of new york granted the government 60 days
to appeal . the aclu sued the defense department in 2003 to have the photos made public . it ’s not clear how many photos are
involved or where the pictures were taken (...)
Standard + coverage: u.s. district
judge

southern district of new york granted

alvin k. hellerstein of

the

the

government

60

days

to

appeal

.

the

aclu

sued

the

defense

department

in

2003

to

have

the

photos made

public

.

it

’s

not

clear

how many

photos

are

involved

or where

the

pictures were

taken .
C-C + coverage: federal

judge orders defense department

to release photos of detainees being abused in

afghanistan

and

iraq

.

photos may

have

been made

public

right

away

,

judge

says

.

aclu

says

photos

are

“

the best

evidence of what

took place

in the military ’s detention centers

”

Article: a pennsylvania community is pulling together to search for an eighth-grade student who has been missing since
wednesday . the search has drawn hundreds of volunteers on foot and online . the parents of cayman naib , 13 , have been
communicating through the facebook group “ ﬁnd cayman ” since a day after his disappearance , according to close friend
david binswanger . newtown police say cayman was last seen wearing a gray down winter jacket , black ski pants and hiking
boots . he could be in the radnor-wayne area , roughly 20 miles from philadelphia (...)
Standard + coverage:

search has drawn hundreds of volunteers on foot

the parents of

and online

the

.

cayman naib , 13 , have been communicating through the

facebook group “ ﬁnd cayman ”

since

a

C-C + coverage:

cayman

naib

,

13

,

has

been missing

since wednesday

.

the

search

has

drawn

hundreds of volunteers on foot

and online

. he

could be

in the

radnor-wayne

area

,

roughly 20

day after his disappearance

.

miles

from philadelphia

.

Table 2: Sample summaries generated by the vanilla beam search and the proposed copy-controlled decoding.
Colors indicate the strengths of the mixture coefﬁcients. Darker shade of blue indicates high mixture coefﬁcient
value i.e., more generation, and lighter color indicates low value i.e. more copying.

exactly as they are in the input paragraph on aver-
age. Furthermore, the overlap increases when the
coverage penalty is used in decoding, suggesting
that its success may not be only due to the removal
of repeated sentences but also due to even more
aggressive copying of phrases/sentences from the
input paragraph. We observe that the proposed
copy-controlled decoding algorithm effectively re-
duces the n-gram overlap. We observe the signif-
icant reduction of overlaps especially when n is
large. For instance, when the coverage penalty
was used, the 25-gram overlap decreased from
28.72% to 3.93%, which is quite signiﬁcant.

Summary Quality Along with signiﬁcant re-
duction in the overlap between the generated sum-
mary and input, we also observed slight degrada-
tion in the summary quality measured in terms
of ROUGE and METEOR scores, as shown in
The small drop in these scores is
Table 1.
however not discouraging, because the proposed
copy-controlled (C-C) decoding generally gener-
ates slightly shorter summaries, while the recall-
based ROUGE (or METEOR) score prefers longer
summaries. This suggests the effectiveness of our
approach, since ROUGE (or METEOR) does not

take into account the length of a hypothesis but
only considers the recall rate of n-grams. Finally,
we note that the ROUGE and METEOR scores of
the summaries generated by the pointer generator
network, released by See et al. (2017) is lower
than what they have reported. We believe this nei-
ther contradicts nor discounts our contribution, as
the proposed decoding works on top of any pre-
trained summarizer which relies on beam search
during inference.

3.2 Qualitative Analysis

We illustrate the beneﬁts of the copy-controlled
decoding with examples in Table 2. The heatmap
of
the mixture co-efﬁcients show that copy-
controlled generates more often (shown by darker
shade) than the standard decoding model. Stan-
dard decoding is fully extractive, copying over
full sentences from the input. Whereas, copy-
It gen-
controlled decoding is more abstractive.
erates new words ”orders” and ”says” that are
not part of the input. Also, it turns out that fa-
voring higher mixture co-efﬁcients improves the
In both exam-
ability to condense information.
ples, controlled-decoding condenses information
present in two different sentences to generate a

single output sentence.

We conjecture that an occasionally high mix-
ture coefﬁcient, encouraged by the proposed copy-
controlled decoding, disrupts the sequence of copy
operations, enabling the attention mechanism to
jump to another part of the input paragraph. This
leads to a more compact summary that compresses
information from multiple sentences distributed
across the input paragraph. We leave more in-
depth analysis for future work.

4 Conclusion

In this paper, we conﬁrmed that a recently pop-
ular neural abstractive summarization approach
largely performs extractive summarization when
equipped with the copy mechanism. To address
this, we proposed a copy-controlled decoding pro-
cedure that introduces a penalty term to the scor-
ing function used during beam search and empir-
ically validated its effectiveness. Our proposed
mechanism currently only modiﬁes the decoding
behavior. A future direction would be to investi-
gate ways to enforce abstractiveness of a summary
during training.

5 Acknowledgement

We would like to thank the authors of See et al.
(2017) for their publicly available, well docu-
mented code. Noah Weber and Niranjan Balasub-
ramanian were supported in part by the National
Science Foundation under Grant IIS-1617969.
Kyunghyun Cho was partly supported by Samsung
Advanced Institute of Technology (Next Genera-
tion Deep Learning: from pattern recognition to
AI) and Samsung Electronics (Improving Deep
Learning using Latent Structure).

References

Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Vic-
tor OK Li. 2016b. Learning to translate in real-
time with neural machine translation. arXiv preprint
arXiv:1610.00388 .

Jiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-
Search engine guided non-
arXiv

tor OK Li. 2017b.
parametric neural machine translation.
preprint arXiv:1705.07267 .

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallap-
ati, Bowen Zhou, and Yoshua Bengio. 2016.
arXiv preprint
Pointing the unknown words.
arXiv:1603.08148 .

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems. pages 1693–
1701.

Alon Lavie and Michael J Denkowski. 2009. The
meteor metric for automatic evaluation of machine
translation. Machine translation 23(2):105–115.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. A sim-
ple, fast diverse decoding algorithm for neural gen-
eration. arXiv preprint arXiv:1611.08562 .

Jiwei Li, Will Monroe, and Dan Jurafsky. 2017. Learn-
ing to decode for future success. arXiv preprint
arXiv:1701.06549 .

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Text summariza-
tion branches out: Proceedings of the ACL-04 work-
shop. Barcelona, Spain, volume 8.

Ramesh Nallapati, Feifei Zhai, and Bowen Zhou.
2016a. Summarunner: A recurrent neural network
based sequence model for extractive summarization
of documents. arXiv preprint arXiv:1611.04230 .

Ramesh Nallapati, Bowen Zhou, C´ıcero Nogueira dos
Santos, aglar G¨ulcehre, and Bing Xiang. 2016b.
Abstractive text summarization using sequence-to-
sequence rnns and beyond. In CoNLL.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304 .

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473 .

Alexander M Rush, Sumit Chopra, and Jason We-
A neural attention model for ab-
arXiv preprint

ston. 2015.
stractive sentence summarization.
arXiv:1509.00685 .

Jiatao Gu, Kyunghyun Cho, and Victor OK Li. 2017a.
Trainable greedy decoding for neural machine trans-
lation. arXiv preprint arXiv:1702.02429 .

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Incorporating copying mechanism in
arXiv preprint

Li. 2016a.
sequence-to-sequence learning.
arXiv:1603.06393 .

Abigail See, Peter J Liu, and Christopher D Man-
to the point: Summarization
arXiv preprint

ning. 2017. Get
with pointer-generator networks.
arXiv:1704.04368 .

Sam Wiseman, Stuart M Shieber, and Alexander M
Rush. 2017. Challenges in data-to-document gen-
eration. arXiv preprint arXiv:1707.08052 .

