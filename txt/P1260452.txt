8
1
0
2
 
c
e
D
 
1
1
 
 
]

V
C
.
s
c
[
 
 
2
v
9
1
9
2
0
.
6
0
8
1
:
v
i
X
r
a

Non-Local Recurrent Network for Image Restoration

Ding Liu1, Bihan Wen1, Yuchen Fan1, Chen Change Loy2, Thomas S. Huang1
1University of Illinois at Urbana-Champaign 2Nanyang Technological University
{dingliu2, bwen3, yuchenf4, t-huang1}@illinois.edu ccloy@ntu.edu.sg

Abstract

Many classic methods have shown non-local self-similarity in natural images
to be an effective prior for image restoration. However, it remains unclear and
challenging to make use of this intrinsic property via deep networks.
In this
paper, we propose a non-local recurrent network (NLRN) as the ﬁrst attempt to
incorporate non-local operations into a recurrent neural network (RNN) for image
restoration. The main contributions of this work are: (1) Unlike existing methods
that measure self-similarity in an isolated manner, the proposed non-local module
can be ﬂexibly integrated into existing deep networks for end-to-end training to
capture deep feature correlation between each location and its neighborhood. (2)
We fully employ the RNN structure for its parameter efﬁciency and allow deep
feature correlation to be propagated along adjacent recurrent states. This new design
boosts robustness against inaccurate correlation estimation due to severely degraded
images. (3) We show that it is essential to maintain a conﬁned neighborhood for
computing deep feature correlation given degraded images. This is in contrast to
existing practice [43] that deploys the whole image. Extensive experiments on both
image denoising and super-resolution tasks are conducted. Thanks to the recurrent
non-local operations and correlation propagation, the proposed NLRN achieves
superior results to state-of-the-art methods with many fewer parameters. The code
is available at https://github.com/Ding-Liu/NLRN.

1

Introduction

Image restoration is an ill-posed inverse problem that aims at estimating the underlying image from its
degraded measurements. Depending on the type of degradation, image restoration can be categorized
into different sub-problems, e.g., image denoising and image super-resolution (SR). The key to
successful restoration typically relies on the design of an effective regularizer based on image priors.
Both local and non-local image priors have been extensively exploited in the past. Considering
image denoising as an example, local image properties such as Gaussian ﬁltering and total variation
based methods [32] are widely used in early studies. Later on, the notion of self-similarity in natural
images draws more attention and it has been exploited by non-local-based methods, e.g., non-local
means [2], collaborative ﬁltering [8], joint sparsity [28], and low-rank modeling [16]. These non-local
methods are shown to be effective in capturing the correlation among non-local patches to improve
the restoration quality.

While non-local self-similarity has been extensively studied in the literature, approaches for capturing
this intrinsic property with deep networks are little explored. Recent convolutional neural networks
(CNNs) for image restoration [10, 21, 29, 51] achieve impressive performance over conventional
approaches but do not explicitly use self-similarity properties in images. To rectify this weakness, a
few studies [24, 31] apply block matching to patches before feeding them into CNNs. Nevertheless,
the block matching step is isolated and thus not jointly trained with image restoration networks.

In this paper, we present the ﬁrst attempt to incorporate non-local operations in CNN for image
restoration, and propose a non-local recurrent network (NLRN) as an efﬁcient yet effective network

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

with non-local module. First, we design a non-local module to produce reliable feature correlation
for self-similarity measurement given severely degraded images, which can be ﬂexibly integrated
into existing deep networks while embracing the beneﬁt of end-to-end learning. For high parameter
efﬁciency without compromising restoration quality, we deploy a recurrent neural network (RNN)
framework similar to [22, 37, 38] such that operations with shared weights are applied recursively.
Second, we carefully study the behavior of non-local operation in deep feature space and ﬁnd that
limiting the neighborhood of correlation computation improves its robustness to degraded images. The
conﬁned neighborhood helps concentrate the computation on relevant features in the spatial vicinity
and disregard noisy features, which is in line with conventional image restoration approaches [8, 16].
In addition, we allow message passing of non-local operations between adjacent recurrent states of
RNN. Such inter-state ﬂow of feature correlation facilitates more robust correlation estimation. By
combining the non-local operation with typical convolutions, our NLRN can effectively capture and
employ both local and non-local image properties for image restoration.

It is noteworthy that recent work has adopted similar ideas on video classiﬁcation [43]. However,
our method signiﬁcantly differs from it in the following aspects. For each location, we measure the
feature correlation of each location only in its neighborhood, rather than throughout the whole image
as in [43]. In our experiments, we show that deep features useful for computing non-local priors
are more likely to reside in neighboring regions. A larger neighborhood (the whole image as one
extreme) can lead to inaccurate correlation estimation over degraded measurements. In addition, our
method fully exploits the advantage of RNN architecture - the correlation information is propagated
among adjacent recurrent states to increase the robustness of correlation estimation to degradations of
various degrees. Moreover, our non-local module is ﬂexible to handle inputs of various sizes, while
the module in [43] handles inputs of ﬁxed sizes only.

We introduce NLRN by ﬁrst relating our proposed model to other classic and existing non-local
image restoration approaches in a uniﬁed framework. We thoroughly analyze the non-local module
and recurrent architecture in our NLRN via extensive ablation studies. We provide a comprehensive
comparison with recent competitors, in which our NLRN achieves state-of-the-art performance
in image denoising and SR over several benchmark datasets, demonstrating the superiority of the
non-local operation with recurrent architecture for image restoration.

2 Related Work

Image self-similarity as an important image characteristic has been used in a number of non-local-
based image restoration approaches. The early works include bilateral ﬁltering [40] and non-local
means [2] for image denoising. Recent approaches exploit image self-similarity by imposing spar-
sity [28, 46]. Alternatively, similar image patches are modeled with low-rankness [16], or by
collaborative Wiener ﬁltering [8, 49]. Neighborhood embedding is a common approach for image
SR [5, 39], in which each image patch is approximated by multiple similar patches in a manifold.
Self-example based image SR approaches [15, 13] exploit the local self-similarity assumption, and
extract LR-HR exemplar pairs merely from the low-resolution image across different scales to predict
the high-resolution image. Similar ideas are adopted for image deblurring [9].

Deep neural networks have been prevalent for image restoration. The pioneering works include a
multilayer perceptron for image denoising [3] and a three-layer CNN for image SR [10]. Deconvolu-
tion is adopted to save computation cost and accelerate inference speed [36, 11]. Very deep CNNs are
designed to boost SR accuracy in [21, 23, 25]. Dense connections among various residual blocks are
included in [41]. Similarly CNN based methods are developed for image denoising in [29, 51, 52, 27].
Block matching as a preprocessing step is cascaded with CNNs for image denoising [24, 31]. Be-
sides CNNs, RNNs have also been applied for image restoration while enjoying the high parameter
efﬁciency [22, 37, 38].

In addition to image restoration, feature correlations are widely exploited along with neural networks
in many other areas, including graphical models [53, 4, 18], relational reasoning [33], machine
translation [14, 42] and so on. We do not elaborate on them here due to the limitation of space.

3 Non-Local Operations for Image Restoration

In this section, we ﬁrst present a uniﬁed framework of non-local operations used for image restoration
methods, e.g., collaborative ﬁltering [8], non-local means [2], and low-rank modeling [16], and we
discuss the relations between them. We then present the proposed non-local operation module.

2

3.1 A General Framework

In general, a non-local operation takes a multi-channel input X ∈ RN ×m as the image feature, and
generates output feature Z ∈ RN ×k. Here N and m denote the number of image pixels and data
channels, respectively. We propose a general framework with the following formulation:

Z = diag{δ(X)}−1 Φ(X) G(X) .
(1)
Here, Φ(X) ∈ RN ×N is the non-local correlation matrix, and G(X) ∈ RN ×k is the multi-channel
non-local transform. Each row vector X i denotes the local features in location i. Φ(X)j
i represents
the relationship between the X i and X j, and each row vector G(X)j is the embedding of X j.1 The
diagonal matrix diag{δ(X)} ∈ RN ×N normalizes the output at each i-th pixel with normalization
factor δi(X).

3.2 Classic Methods

The proposed framework works with various classic non-local methods for image restoration, includ-
ing methods based on low-rankness [16], collaborative ﬁltering [8], joint sparsity [28], as well as
non-local mean ﬁltering [2].

Block matching (BM) is a commonly used approach for exploiting non-local image structures
in conventional methods [16, 8, 28]. A q × q spatial neighborhood is set to be centered at each
location i, and X i reduces to the image patch centered at i. BM selects the Ki most similar patches
(Ki (cid:28) q2) from this neighborhood, which are used jointly to restore X i. Under the proposed
non-local framework, these methods can be represented as

Zi =

(cid:88)

1
δi(X)

j∈Ci

Φ(X)j

i G(X)j , ∀i .

(2)

j∈Ci

Φ(X)j

Here δi(X) = (cid:80)
i and Ci denotes the set of indices of the Ki selected patches. Thus, each
row Φ(X)i has only Ki non-zero entries. The embedding G(X) and the non-zero elements vary for
non-local methods based on different models. For example, in WNNM [16], (cid:80)
i G(X)j
corresponds to the projection of X i onto the group-speciﬁc subspace as a function of the selected
patches. Speciﬁcally, the subspace for calculating Zi is spanned by the eigenvectors U i of X T
X Ci .
Ci
Thus Zi = X CiU idiag{σ}U T
i , where diag{σ} is obtained by applying the shrinkage function
associated with the weighted nuclear norm [16] to the eigenvalues of X T
X Ci. We show the
Ci
generalization about more classic non-local image restoration methods in Section 7.

Φ(X)j

j∈Ci

Except for the hard block matching, other methods, e.g., the non-local means algorithm [2], apply
soft block matching by calculating the correlation between the reference patch and each patch in
the neighborhood. Each element Φ(X)j
i =
φ(X i, X j), where φ( · ) is determined by the distance metric. In [2], weighted Euclidean distance
with Gaussian kernel is applied as the metric, such that φ(X i, X j) = exp{− (cid:107)X i − X j(cid:107)2
2,a /h2}.
Besides, identity mapping is directly used as the embedding in [2], i.e., G(X)j = X j. In this case,
the non-local framework in (1) reduces to

i is determined only by each {X i, X j} pair, so Φ(X)j

Zi =

(cid:88)

1
δi(X)

j∈Si
exp{− (cid:107)X i − X j(cid:107)2

exp{−

(cid:107)X i − X j(cid:107)2
2,a
h2

}X j , ∀i,

(3)

j∈Si

where δi(X) = (cid:80)
2,a /h2} and Si is the set of indices in the neighborhood
of X i. Note that both a and h are constants, denoting the standard deviation of Gaussian kernel, and
the degree of ﬁltering, respectively [2]. It is noteworthy that the cardinality of Si for soft BM is much
larger than that of Ci for hard BM, which gives more ﬂexibility of using feature correlations between
neighboring locations.

The conventional non-local methods suffer from the drawback that parameters are either ﬁxed [2], or
obtained by suboptimal approaches [8, 28, 16], e.g., the parameters of WNNM are learned based on
the low-rankness assumption, which is suboptimal as the ultimate objective is to minimize the image
reconstruction error.

1In our analysis, if A is a matrix, Ai, Aj, and Aj

i denote its i-th row, j-th column, and the element at the

i-th row and j-th column, respectively.

3

(4)
(5)

(6)

Figure 1: An illustration of our non-local module working on a single location. The white tensor denotes
the deep feature representation of an entire image. The red ﬁber is the features of this location and the blue
tensor denotes the features in its neighborhood. θ, ψ and g are implemented by 1 × 1 convolution followed by
reshaping operations.

3.3 The Proposed Non-Local Module

Based on the general non-local framework in (1), we propose another soft block matching approach
and apply the Euclidean distance with linearly embedded Gaussian kernel [43] as the distance metric.
The linear embeddings are deﬁned as follows:

Φ(X)j

i = φ(X i, X j) = exp{θ(X i)ψ(X j)T } , ∀i, j ,

θ(X i) = X iW θ, ψ(X i) = X iW ψ, G(X)i = X iW g , ∀i .

The embedding transforms W θ, W φ, and W g are all learnable and have the shape of m × l, m ×
l, m × m, respectively. Thus, the proposed non-local operation can be written as

Zi =

(cid:88)

1
δi(X)

j∈Si

exp {X iW θW T

ψ X T

j } X iW g , ∀i ,

j∈Si

φ(X i, X j). Similar to [2], to obtain Zi, we evaluate the correlation between

where δi(X) = (cid:80)
X i and each X j in the neighborhood Si. More choices of φ(X i, X j) are discussed in Section 5.
The proposed non-local operation can be implemented by common differentiable operations, and thus
can be jointly learned when incorporated into a neural network. We wrap it as a non-local module
by adding a skip connection, as shown in Figure 1, since the skip connection enables us to insert a
non-local module into any pre-trained model, while maintaining its initial behavior by initializing
W g as zero. Such a module introduces only a limited number of parameters since θ, ψ and g are
1 × 1 convolutions and m = 128, l = 64 in practice. The output of this module on each location only
depends on its q × q neighborhood, so this operation can work on inputs of various sizes.

Relation to Other Methods: Recent works have combined non-local BM and neural networks
for image restoration [31, 24, 43]. Lefkimmiatis [24] proposed to ﬁrst apply BM to noisy image
patches. The hard BM results are used to group patch features, and a CNN conducts a trainable
collaborative ﬁltering over the matched patches. Qiao et al. [31] combined similar non-local BM
with TNRD networks [7] for image denoising. However, as conventional methods [8, 28, 16], these
works [24, 31] conduct hard BM directly over degraded input patches, which may be inaccurate over
severely degraded images. In contrast, our proposed non-local operation as soft BM is applied on
learned deep feature representations that are more robust to degradation. Furthermore, the matching
results in [24] are isolated from the neural network, similar to the conventional approaches, whereas
the proposed non-local module is trained jointly with the entire network in an end-to-end manner.

Wang et al. [43] used similar approaches to add non-local operations into neural networks for high-
level vision tasks. However, unlike our approach, Wang et al. [43] calculated feature correlations
throughout the whole image. which is equivalent to enlarging the neighborhood to the entire image in
our approach. We empirically show that increasing the neighborhood size does not always improve
image restoration performance, due to the inaccuracy of correlation estimation over degraded input
images. Hence it is imperative to choose a neighborhood of a proper size to achieve best performance
for image restoration. In addition, the non-local operation in [43] can only handle input images of
ﬁxed size, while our module in (6) is ﬂexible to various image sizes. Finally, our non-local module,
when incorporated into an RNN framework, allows the ﬂow of correlation information between
adjacent states to enhance robustness against inaccurate correlation estimation. This is a new unique
formulation to deal with degraded images. More details are provided next.

4

Figure 2: An illustration of the transition function
frecurrent in the proposed NLRN.

Figure 3: The operations for a single location i in the
non-local module used in NLRN.

4 Non-Local Recurrent Network
In this section, we describe the RNN architecture that incorporates the non-local module to form
our NLRN. We adopt the common formulation of an RNN, which consists of a set of states, namely,
input state, output state and recurrent state, as well as transition functions among the states. The
input, output, and recurrent states are represented as x, y and s respectively. At each time step t,
an RNN receives an input xt, and the recurrent state and the output state of the RNN are updated
recursively as follows:

st = finput(xt) + frecurrent(st−1),

yt = foutput(st),

(7)

where finput, foutput, and frecurrent are reused at every time step. In our NLRN, we set the following:

• s0 is a function of the input image I.
• xt = 0, ∀t ∈ {1, . . . , T }, and finput(0) = 0.
• The output state yt is calculated only at the time T as the ﬁnal output.

We add an identity path from the very ﬁrst state which helps gradient backpropagation during
training [37], and a residual path of the deep feature correlation between each location and its
corr}, and st = frecurrent(st−1, s0), ∀t ∈
neighborhood from the previous state. Hence, st = {st
{1, . . . , T }, where st
corr is the collection of deep feature
correlation. For the transition function frecurrent, a non-local module is ﬁrst adopted and is followed
by two convolutional layers, before the feature s0 is added from the identity path. The weights in the
non-local module are shared across recurrent states just as convolutional layers, so our NLRN still
keeps high parameter efﬁciency as a whole. An illustration is displayed in Figure 2.

feat denotes the feature map in time t and st

feat, st

It is noteworthy that inside the non-local module, the feature correlation for location i from the
previous state, st−1
corr,i, is added to the estimated feature correlation in the current state before the
softmax normalization, which enables the propagation of correlation information between adjacent
states for more robust correlation estimation. The details can be found in Figure 3. The initial
state s0 is set as the feature after a convolutional layer on the input image. foutput is represented
by another single convolutional layer. All layers have 128 ﬁlters with 3 × 3 kernel size except for
the non-local module. Batch normalization and ReLU activation function are performed ahead of
each convolutional layer following [19]. We adopt residual learning and the output of NLRN is the
residual image ˆI = foutput(sT ) when NLRN is unfolded T times. During training, the objective is to
2 || ˆI + I − ˜I||2, where ˜I denotes the ground truth image.
minimize the mean square error L( ˆI, ˜I) = 1
Relation to Other RNN Methods: Although RNNs have been adopted for image restoration before,
our NLRN is the ﬁrst to incorporate non-local operations into an RNN framework with correlation
propagation. DRCN [22] recursively applies a single convolutional layer to the input feature map
multiple times without the identity path from the ﬁrst state. DRRN [37] applies both the identity path
and the residual path in each state, but without non-local operations, and thus there is no correlation
information ﬂow across adjacent states. MemNet [38] builds dense connections among several types
of memory blocks, and weights are shared in the same type of memory blocks but are different across
various types. Compared with MemNet, our NLRN has an efﬁcient yet effective RNN structure with
shallower effective depth and fewer parameters, but obtains better restoration performance, which is
shown in Section 5 in detail.

5 Experiments

Dataset: For image denoising, we adopt two different settings to fairly and comprehensively compare
with recent deep learning based methods [29, 24, 51, 38]: (1) As in [7, 51, 24], we choose as the

5

training set the combination of 200 images from the train set and 200 images from the test set in the
Berkeley Segmentation Dataset (BSD) [30], and test on two popular benchmarks: Set12 and Set68
with σ = 15, 25, 50 following [51]. (2) As in [29, 38], we use as the training set the combination of
200 images from the train set and 100 images from the val set in BSD, and test on Set14 and the
BSD test set of 200 images with σ = 30, 50, 70 following [29, 38]. In addition, we evaluate our
NLRN on the Urban100 dataset [20], which contains abundant structural patterns and textures, to
further demonstrate the capability of using image self-similarity of our NLRN. The training set and
test set are strictly disjoint and all the images are converted to gray-scale in each experiment setup.
For image SR, we follow [21, 37, 38] and use a training set of 291 images where 91 images are
proposed in [48] and other 200 are from the BSD train set. We adopt four benchmark sets: Set5 [1],
Set14 [50], BSD100 [30] and Urban100 [20] for testing with three upscaling factors: ×2, ×3 and
×4. The low-resolution images are synthesized by bicubic downsampling.

Training Settings: We randomly sample patches whose size equals the neighborhood of non-local
operation from images during training. We use ﬂipping, rotation and scaling for augmenting training
data. For image denoising, we add independent and identically distributed Gaussian noise with zero
mean to the original image as the noisy input during training. We train a different model for each
noise level. For image SR, only the luminance channel of images is super-resolved, and the other two
color channels are upscaled by bicubic interpolation, following [21, 22, 37]. Moreover, the training
images for all three upscaling factors: ×2, ×3 and ×4 are upscaled by bicubic interpolation into the
desired spatial size and are combined into one training set. We use this set to train one single model
for all these three upscaling factors as in [21, 37, 38].

We use Adam optimizer to minimize the loss function. We set the initial learning rate as 1e-3 and
reduce it by half ﬁve times during training. We use Xavier initialization for the weights. We clip
the gradient at the norm of 0.5 to prevent the gradient explosion which is shown to empirically
accelerate training convergence, and we adopt 16 as the minibatch size during training. Training a
model takes about 3 days with a Titan Xp GPU. For non-local module, we use circular padding for
the neighborhood outside input patches. For convolution, we pad the boundaries of feature maps with
zeros to preserve the spatial size of feature maps.

5.1 Model Analysis

In this section, we analyze our model in the following aspects. First, we conduct the ablation study of
using different distance metrics in the non-local module. Table 1 compares instantiations including
Euclidean distance, dot product, embedded dot product, Gaussian, symmetric embedded Gaussian
and embedded Gaussian when used in NLRN of 12 unfolded steps. Embedded Gaussian achieves the
best performance and is adopted in the following experiments.

We compare the NLRN with its variants in terms of PSNR in Table 2. We have a few observations.
First, the same model with untied weights performs worse than its weight-sharing counter-part. We
speculate that the model with untied weights is prone to model over-ﬁtting and suffers much slower
training convergence, both of which undermine its performance. To investigate the function of non-
local modules, we implement a baseline RNN with the same parameter number of NLRN, and ﬁnd it
is worse than NLRN by about 0.2 dB, showing the advantage of using non-local image properties for
image restoration. Besides, we implement NLRNs where non-local module is used in every other
state or every three states, and observe that if the frequency of using non-local modules in NLRN
is reduced, the performance decreases accordingly. We show the beneﬁt of propagating correlation
information among adjacent states by comparing with the counter-part in terms of restoration accuracy.
To further analyze the non-local module, we visualize the feature correlation maps for non-local
operations in Figure 4. It can be seen that as the number of recurrent states increases, the locations

Table 1: Image denoising comparison of our proposed model
with various distance metrics on Set12 with noise level of 25.

Distance metric
Euclidean distance
Dot product
Embedded dot product
Gaussian
Symmetric embedded Gaussian
Embedded Gaussian

φ(X i, X j)

exp{− (cid:107)X i − X j(cid:107)2

2 /h2}

X iX T
j
θ(X i)ψ(X j)T
exp{X iX T
j }
exp{θ(X i)θ(X j)T }
exp{θ(X i)ψ(X j)T }

PSNR
30.74
30.68
30.75
30.69
30.76
30.80

Table 2: Image denoising comparison of our
NLRN with its variants on Set12 with noise
level of 25.

Model
NLRN w/o parameter sharing
RNN with same parameter no.
Non-local module in every other state
Non-local module in every 3 states
NLRN w/o propagating correlations
NLRN

PSNR
30.65
30.61
30.76
30.72
30.78
30.80

6

Figure 4: Examples of correlation maps of non-local operations for
image denoising. Noisy patch/ground truth patch: the neighborhood of
the red center pixel used in non-local operations. (1)-(6): the correlation
map for recurrent state 1-6 from NLRN with unrolling length of 6.

Figure 5: Neighborhood size vs.
image denoising performance of
our proposed model on Set12 with
noise level of 25.

Max effective depth
Parameter sharing
Parameter no.
Multi-view testing
Training images
PSNR

DnCNN
17
No
554k
No
400
27.18

RED MemNet
30
No
4,131k
Yes
300
27.33

80
Yes
667k
No
300
27.38

NLRN
38
Yes
330k
No
300
27.60

No
400
27.64

Yes
300
27.66

Table 3: Image denoising comparison of our proposed model with state-
of-the-art network models on Set12 with noise level of 50. Model com-
plexities are also compared.

Figure 6: Unrolling length vs.
image denoising performance of
our proposed model on Set12 with
noise level of 25.

with similar features progressively show higher correlations in the map, which demonstrates the
effectiveness of the non-local module for exploiting image self-similarity.

Figure 5 investigates the inﬂuence of the neighborhood size in the non-local module on image
denoising results. The performance peaks at q = 45. This shows that limiting the neighborhood
helps concentrate the correlation calculation on relevant features in the spatial vicinity and enhance
correlation estimation. Therefore, it is necessary to choose a proper neighborhood size (rather than
the whole image) for image restoration. We select q = 45 for the rest of this paper unless stated
otherwise.

The unrolling length T determines the maximum effective depth (i.e., maximum number of convolu-
tional layers) of NLRN. The inﬂuence of the unrolling length on image denoising results is shown in
Figure 6. The performance increases as the unrolling length rises, but gets saturated after T = 12.
Given the tradeoff between restoration accuracy and inference time, we adopt T = 12 for NLRN in
all the experiments.

5.2 Comparisons with State-of-the-Art Methods

We compare our proposed model with a number of recent competitors for image denoising and
image SR, respectively. PSNR and SSIM [44] are adopted for measuring quantitative restoration
performance.

Image Denoising: For a fair comparison with other methods based on deep networks, we train our
model under two settings: (1) We use the training data as in TNRD [7], DnCNN [51] and NLNet [24],
and the result is shown in Table 4. We cite the result of NLNet in the original paper [24], since no
public code or model is available. (2) We use the training data as in RED [29] and MemNet [38], and
the result is shown in Table 5. We note that RED uses multi-view testing [45] to boost the restoration
accuracy, i.e., RED processes each test image as well as its rotated and ﬂipped versions, and all
the outputs are then averaged to form the ﬁnal denoised image. Accordingly, we perform the same
procedure for NLRN and ﬁnd its performance, termed as NLRN-MV, is consistently improved. In
addition, we include recent non-deep-learning based methods: BM3D [8] and WNNM [16] in our
comparison. We do not list other methods [54, 3, 47, 6, 52] whose average performances are worse
than DnCNN or MemNet. Our NLRN signiﬁcantly outperforms all the competitors on Urban100 and
yields the best results across almost all the noise levels and datasets.

To further show the advantage of the network design of NLRN, we compare different versions of
NLRN with several state-of-the-art network models, i.e., DnCNN, RED and MemNet in Table 3.
NLRN uses the fewest parameters but outperforms all the competitors. Speciﬁcally, NLRN beneﬁts

7

Table 4: Benchmark image denoising results. Training and testing protocols are followed as in [51]. Average
PSNR/SSIM for various noise levels on Set12, BSD68 and Urban100. The best performance is in bold.

Dataset

Set12

BSD68

Urban100

Noise
15
25
50
15
25
50
15
25
50

BM3D
32.37/0.8952
29.97/0.8504
26.72/0.7676
31.07/0.8717
28.57/0.8013
25.62/0.6864
32.35/0.9220
29.70/0.8777
25.95/0.7791

WNNM
32.70/0.8982
30.28/0.8557
27.05/0.7775
31.37/0.8766
28.83/0.8087
25.87/0.6982
32.97/0.9271
30.39/0.8885
26.83/0.8047

TNRD
32.50/0.8958
30.06/0.8512
26.81/0.7680
31.42/0.8769
28.92/0.8093
25.97/0.6994
31.86/0.9031
29.25/0.8473
25.88/0.7563

NLNet
-/-
-/-
-/-
31.52/-
29.03/-
26.07/-
-/-
-/-
-/-

DnCNN
32.86/0.9031
30.44/0.8622
27.18/0.7829
31.73/0.8907
29.23/0.8278
26.23/0.7189
32.68/0.9255
29.97/0.8797
26.28/0.7874

NLRN
33.16/0.9070
30.80/0.8689
27.64/0.7980
31.88/0.8932
29.41/0.8331
26.47/0.7298
33.45/0.9354
30.94/0.9018
27.49/0.8279

Table 5: Benchmark image denoising results. Training and testing protocols are followed as in [38]. Average
PSNR/SSIM for various noise levels on 14 images, BSD200 and Urban100. Red is the best and blue is the
second best performance.

Dataset

14 images

BSD200

Urban100

Noise
30
50
70
30
50
70
30
50
70

BM3D
28.49/0.8204
26.08/0.7427
24.65/0.6882
27.31/0.7755
25.06/0.6831
23.82/0.6240
28.75/0.8567
25.95/0.7791
24.27/0.7163

WNNM
28.74/0.8273
26.32/0.7517
24.80/0.6975
27.48/0.7807
25.26/0.6928
23.95/0.6346
29.47/0.8697
26.83/0.8047
25.11/0.7501

RED
29.17/0.8423
26.81/0.7733
25.31/0.7206
27.95/0.8056
25.75/0.7167
24.37/0.6551
29.12/0.8674
26.44/0.7977
24.75/0.7415

MemNet
29.22/0.8444
26.91/0.7775
25.43/0.7260
28.04/0.8053
25.86/0.7202
24.53/0.6608
29.10/0.8631
26.65/0.8030
25.01/0.7496

NLRN
29.37/0.8460
27.00/0.7777
25.49/0.7255
28.15/0.8423
25.93/0.7214
24.58/0.6614
29.94/0.8830
27.38/0.8241
25.66/0.7707

NLRN-MV
29.41/0.8472
27.05/0.7791
25.54/0.7273
28.20/0.8436
25.97/0.8429
24.62/0.6634
29.99/0.8842
27.43/0.8256
25.71/0.7724

Table 6: Benchmark SISR results. Average PSNR/SSIM for scale factor ×2, ×3 and ×4 on datasets Set5, Set14,
BSD100 and Urban100. The best performance is in bold.

Dataset

Set5

Set14

BSD100

Urban100

Scale
×2
×3
×4
×2
×3
×4
×2
×3
×4
×2
×3
×4

SRCNN
36.66/0.9542
32.75/0.9090
30.48/0.8628
32.45/0.9067
29.30/0.8215
27.50/0.7513
31.36/0.8879
28.41/0.7863
26.90/0.7101
29.50/0.8946
26.24/0.7989
24.52/0.7221

VDSR
37.53/0.9587
33.66/0.9213
31.35/0.8838
33.03/0.9124
29.77/0.8314
28.01/0.7674
31.90/0.8960
28.82/0.7976
27.29/0.7251
30.76/0.9140
27.14/0.8279
25.18/0.7524

DRCN
37.63/0.9588
33.82/0.9226
31.53/0.8854
33.04/0.9118
29.76/0.8311
28.02/0.7670
31.85/0.8942
28.80/0.7963
27.23/0.7233
30.75/0.9133
27.15/0.8276
25.14/0.7510

LapSRN
37.52/0.959
33.82/0.923
31.54/0.885
33.08/0.913
29.79/0.832
28.19/0.772
31.80/0.895
28.82/0.797
27.32/0.728
30.41/0.910
27.07/0.827
25.21/0.756

DRRN
37.74/0.9591
34.03/0.9244
31.68/0.8888
33.23/0.9136
29.96/0.8349
28.21/0.7721
32.05/0.8973
28.95/0.8004
27.38/0.7284
31.23/0.9188
27.53/0.8378
25.44/0.7638

MemNet
37.78/0.9597
34.09/0.9248
31.74/0.8893
33.28/0.9142
30.00/0.8350
28.26/0.7723
32.08/0.8978
28.96/0.8001
27.40/0.7281
31.31/0.9195
27.56/0.8376
25.50/0.7630

NLRN
38.00/0.9603
34.27/0.9266
31.92/0.8916
33.46/0.9159
30.16/0.8374
28.36/0.7745
32.19/0.8992
29.06/0.8026
27.48/0.7306
31.81/0.9249
27.93/0.8453
25.79/0.7729

from inherent parameter sharing and uses only less than 1/10 parameters of RED. Compared with the
RNN competitor, MemNet, NLRN uses only half of parameters and much shallower depth to obtain
better performance, which shows the superiority of our non-local recurrent architecture.

Image Super-Resolution: We compare our model with several recent SISR approaches, including
SRCNN [10], VDSR [21], DRCN [22], LapSRN [23], DRRN [37] and MemNet [38] in Table 6. We
crop pixels near image borders before calculating PSNR and SSIM as in [10, 35, 21, 22]. We do
not list other methods [20, 35, 26, 36, 17] since their performances are worse than that of DRRN or
MemNet. Besides, we do not include SRDenseNet [41] and EDSR [25] in the comparison because
the number of parameters in these two network models is over two orders of magnitude larger than
that of our NLRN and their training datasets are signiﬁcantly larger than ours. It can be seen that
NLRN yields the best result across all the upscaling factors and datasets. Visual results are provided
in Section 7.

6 Conclusion

We have presented a new and effective recurrent network that incorporates non-local operations for
image restoration. The proposed non-local module can be trained end-to-end with the recurrent
network. We have studied the importance of computing reliable feature correlations within a conﬁned
neighorhood against the whole image, and have shown the beneﬁts of passing feature correlation
messages between adjacent recurrent stages. Comprehensive evaluations over benchmarks for image
denoising and super-resolution demonstrate the superiority of NLRN over existing methods.

8

7 Appendix

7.1 Extension of the General Framework to Other Classic Non-Local Methods

Besides the extension to WNNM and non-local means, which are discussed in Section 3.1, we show
the proposed non-local framework (1) can be extended to collaborative ﬁltering methods, e.g., BM3D
algorithm [8], as well as joint sparsity based methods, e.g., LSSC algorithm [28]. We follow the same
notations in Section 3.1. Both BM3D and LSSC apply block matching (BM) ﬁrst before processing,
and form N groups of similar patches into data matrices. The index set of the matched patches for
the i-th reference patch is denoted as Ci. The group of matched patches for the i-th reference patch is
denoted as X Ci.
Similar to WNNM [16], BM3D [8] also applies BM ﬁrst to group similar patches based on their
Euclidean distances. The matched patches are then processed via Wiener ﬁltering [8], and the
denoised results of the i-th group of patches are

ZCi = τ −1(diag(ω)τ (X Ci)).

(8)

Here τ (·) and τ −1(·) denote the forward and backward Wiener ﬁltering applied to the groups of
matched patches, respectively. The diagonal matrix diag(ω) is formed by the empirical Wiener
coefﬁcients ω. BM3D applies data pre-cleaning, using discrete cosine transform (DCT), to estimate
the original patch, and calculate the estimate of ω [8]. Since calculating ZCi in (8) involves only
linear ﬁltering, it can also be generalized using the proposed non-local framework as (2). Unlike the
extension to WNNM, here (cid:80)
i G(X)j corresponds to the denoised results via Wiener
ﬁltering as shown in (8), of the i-th group of matched patches.

Φ(X)j

j∈Ci

Different from BM3D and WNNM, LSSC learns a common dictionary D for all image patches, and
imposes joint sparsity [28] on each data matrix of matched patches X Ci, so that the correlation of
the matched patches are exploited by enforcing the same support of their sparse codes. Thus, the
joint sparse coding in LSSC [28] becomes

ˆAi = argminAi (cid:107)Ai(cid:107)0,∞ s.t.

− DAi

≤ (cid:15) |Ci| , ∀i ,

(9)

(cid:13)
(cid:13)X T
(cid:13)
Ci

(cid:13)
2
(cid:13)
(cid:13)

F

where the (0, ∞) “norm” (cid:107)·(cid:107)0,∞ counts the number of non-zero columns of each sparse code matrix
Ai [28], and |Ci| is the cardinality of Ci. The coefﬁcient (cid:15) is a constant, which is used to upper bound
the sparse modeling errors. In general, the solution to (9) is NP-hard. To simplify the discussion, we
assume the dictionary to be unitary (which reduces the sparse coding problem to the transform-model
sparse coding [46]), i.e., DT D = I and D ∈ Rk×k. Thus there exists a corresponding shrinkage
function η(·) for imposing joint sparsity on the sparse codes [28, 34], such that the denoised estimates
T
of the i-th patch group can be obtained as ZCi = ˆA
i DT = η( X Ci D ) DT . Though joint sparse
coding projects all data onto a union of subspaces [28, 12, 46] which is a non-linear operation in
general, each data matrix X Ci is projected onto one particular subspace spanned by the selected
atoms corresponding to the non-zero columns in ˆAi, which is locally linear. For the i-th group of
patches, such a subspace projection corresponds to (cid:80)
i G(X)j in the proposed general
framework.

Φ(X)j

j∈Ci

7.2 Visual Results

We show the visual comparison of our NLRN and several competing methods: BM3D [8],
WNNM [16], and MemNet [38] for image denoising in Figure 7. Our method can recover more
details from the noisy measurement. The visual comparison of our NLRN and several recent methods:
DRCN [22], LapSRN [23], DRRN [37], and MemNet [38] for image super-resolution is displayed in
Figure 8. Our method is able to reconstruct sharper edges and produce fewer artifacts especially in
the regions of repetitive patterns.

9

Noisy (18.75/0.3232)

BM3D (29.13/0.8261)

WNNM (29.30/0.8334)

Ground truth

MemNet (29.18/0.8223)

NLRN (29.53/0.8369)

Ground truth (PSNR/SSIM)

Noisy (19.49/0.5099)

BM3D (28.95/0.9062)

WNNM (30.44/0.9260)

Ground truth

MemNet (28.71/0.8906)

NLRN (30.52/0.9267)

Ground truth (PSNR/SSIM)

Noisy (19.39/0.4540)

BM3D (27.20/0.8775)

WNNM (28.86/0.8913)

Ground truth

MemNet (27.55/0.8807)

NLRN (29.04/0.9044)

Ground truth (PSNR/SSIM)

Noisy (19.06/0.3005)

BM3D (29.61/0.8304)

WNNM (30.59/0.8543)

Ground truth

MemNet (30.21/0.8517)

NLRN (31.17/0.8727)

Ground truth (PSNR/SSIM)

Noisy (18.88/0.3479)

BM3D (28.82/0.9051)

WNNM (28.38/0.9189)

Ground truth

MemNet (28.31/0.9112)

NLRN (28.42/0.9308)

Ground truth (PSNR/SSIM)

Figure 7: Qualitative comparison of image denoising results with noise level of 30. The zoom-in region in the
red bounding box is shown on the right. From top to bottom: 1) the image barbara. 2) image 004 in Urban100.
3) image 019 in Urban100. 4) image 033 in Urban100. 5) image 046 in Urban100.

10

DRCN (26.82/0.9329)

LapSRN (26.52/0.9316)

DRRN (27.52/0.9434)

Ground truth HR

MemNet (27.78/0.9451)

NLRN (28.46/0.9513)

HR (PSNR/SSIM)

DRCN (20.95/0.7716)

LapSRN (20.90/0.7722)

DRRN (21.37/0.7874)

Ground truth HR

MemNet (21.35/0.7877)

NLRN (21.92/0.8014)

HR (PSNR/SSIM)

DRCN (30.18/0.8306)

LapSRN (30.29/0.8388)

DRRN (30.18/0.8306)

Ground truth HR

MemNet (29.25/0.8347)

NLRN (31.19/0.8598)

HR (PSNR/SSIM)

DRCN (20.71/0.7466)

LapSRN (20.86/0.7524)

DRRN (20.92/0.7666)

Ground truth HR

MemNet (21.06/0.7716)

NLRN (21.41/0.7866)

HR (PSNR/SSIM)

DRCN (23.99/0.6940)

LapSRN (24.49/0.7247)

DRRN (25.14/0.7469)

Ground truth HR

MemNet (25.19/0.7519)

NLRN (25.97/0.7882)

HR (PSNR/SSIM)

Figure 8: Qualitative comparison of image super-resolution results with ×4 upscaling. The zoom-in region in
the red bounding box is shown on the right. From top to bottom: 1) image 005 in Urban100. 2) image 019 in
Urban100. 3) image 044 in Urban100. 4) image 062 in Urban100. 5) image 099 in Urban100.

References

[1] M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. Alberi-Morel. Low-complexity single-image super-

resolution based on nonnegative neighbor embedding. 2012.

11

[2] A. Buades, B. Coll, and J.-M. Morel. A non-local algorithm for image denoising. In CVPR, 2005.
[3] H. C. Burger, C. J. Schuler, and S. Harmeling. Image denoising: Can plain neural networks compete with

[4] S. Chandra, N. Usunier, and I. Kokkinos. Dense and low-rank gaussian crfs using deep embeddings. In

[5] H. Chang, D.-Y. Yeung, and Y. Xiong. Super-resolution through neighbor embedding. In CVPR, 2004.
[6] F. Chen, L. Zhang, and H. Yu. External patch prior guided internal clustering for image denoising. In

bm3d? In CVPR, 2012.

ICCV, 2017.

ICCV, 2015.

[7] Y. Chen and T. Pock. Trainable nonlinear reaction diffusion: A ﬂexible framework for fast and effective

[8] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-d transform-domain

[9] A. Danielyan, V. Katkovnik, and K. Egiazarian. Bm3d frames and variational image deblurring. TIP,

image restoration. IEEE TPAMI, 2017.

collaborative ﬁltering. IEEE TIP, 2007.

21(4):1715–1728, 2012.

[10] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep convolutional network for image super-resolution.

[11] C. Dong, C. C. Loy, and X. Tang. Accelerating the super-resolution convolutional neural network. In

In ECCV, 2014.

ECCV, 2016.

IEEE TIP, 2006.

Graphics (TOG), 2011.

learning. In ICML, 2017.

[12] M. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned dictionaries.

[13] G. Freedman and R. Fattal. Image and video upscaling from local self-examples. ACM Transactions on

[14] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to sequence

[15] D. Glasner, S. Bagon, and M. Irani. Super-resolution from a single image. In ICCV, 2009.
[16] S. Gu, L. Zhang, W. Zuo, and X. Feng. Weighted nuclear norm minimization with application to image

denoising. In CVPR, pages 2862–2869, 2014.

[17] W. Han, S. Chang, D. Liu, M. Yu, M. Witbrock, and T. S. Huang. Image super-resolution via dual-state

[18] A. W. Harley, K. G. Derpanis, and I. Kokkinos. Segmentation-aware convolutional networks using local

recurrent networks. In CVPR, June 2018.

attention masks. In ICCV, 2017.

[19] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, 2016.
[20] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-resolution from transformed self-exemplars. In

[21] J. Kim, J. Kwon Lee, and K. Mu Lee. Accurate image super-resolution using very deep convolutional

CVPR, 2015.

networks. In CVPR, 2016.

In CVPR, 2016.

super-resolution. In CVPR, 2017.

[22] J. Kim, J. Kwon Lee, and K. Mu Lee. Deeply-recursive convolutional network for image super-resolution.

[23] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang. Deep laplacian pyramid networks for fast and accurate

[24] S. Lefkimmiatis. Non-local color image denoising with convolutional neural networks. In CVPR, 2017.
[25] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced deep residual networks for single image

super-resolution. In CVPR Workshops, 2017.

[26] D. Liu, Z. Wang, B. Wen, J. Yang, W. Han, and T. S. Huang. Robust single image super-resolution via

deep networks with sparse prior. TIP, 25(7):3194–3207, 2016.

[27] D. Liu, B. Wen, X. Liu, Z. Wang, and T. S. Huang. When image denoising meets high-level vision tasks:

A deep learning approach. In IJCAI, 2018.

[28] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Non-local sparse models for image restoration.

In ICCV, 2009.

[29] X. Mao, C. Shen, and Y.-B. Yang. Image restoration using very deep convolutional encoder-decoder

networks with symmetric skip connections. In NIPS, 2016.

[30] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its

application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, 2001.

[31] P. Qiao, Y. Dou, W. Feng, R. Li, and Y. Chen. Learning non-local image diffusion for image denoising. In

ACM on Multimedia Conference, 2017.

[32] L. I. Rudin and S. Osher. Total variation based image restoration with free local constraints. In ICIP, 1994.
[33] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lillicrap. A simple

neural network module for relational reasoning. In NIPS, 2017.

[34] U. Schmidt and S. Roth. Shrinkage ﬁelds for effective image restoration. In CVPR, 2014.
[35] S. Schulter, C. Leistner, and H. Bischof. Fast and accurate image upscaling with super-resolution forests.

In CVPR, 2015.

12

[36] W. Shi, J. Caballero, F. Huszár, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-time
single image and video super-resolution using an efﬁcient sub-pixel convolutional neural network. In
CVPR, 2016.

[37] Y. Tai, J. Yang, and X. Liu. Image super-resolution via deep recursive residual network. In CVPR, 2017.
[38] Y. Tai, J. Yang, X. Liu, and C. Xu. Memnet: A persistent memory network for image restoration. In ICCV,

[39] R. Timofte, V. De, and L. Van Gool. Anchored neighborhood regression for fast example-based super-

resolution. In ICCV, 2013.

[40] C. Tomasi and R. Manduchi. Bilateral ﬁltering for gray and color images. In ICCV, 1998.
[41] T. Tong, G. Li, X. Liu, and Q. Gao. Image super-resolution using dense skip connections. In ICCV, 2017.
[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.

Attention is all you need. In NIPS, 2017.

[43] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. arXiv preprint arXiv:1711.07971,

[44] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility

[45] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang. Deep networks for image super-resolution with sparse

to structural similarity. IEEE TIP, 2004.

prior. In ICCV, 2015.

[46] B. Wen, S. Ravishankar, and Y. Bresler. Structured overcomplete sparsifying transform learning with

convergence guarantees and applications. IJCV, 2015.

[47] J. Xu, L. Zhang, W. Zuo, D. Zhang, and X. Feng. Patch group based nonlocal self-similarity prior learning

for image denoising. In ICCV, 2015.

[48] J. Yang, J. Wright, T. S. Huang, and Y. Ma. Image super-resolution via sparse representation. IEEE TIP,

[49] R. Yin, T. Gao, Y. M. Lu, and I. Daubechies. A tale of two bases: Local-nonlocal regularization on image

patches with convolution framelets. SIAM Journal on Imaging Sciences, 10(2):711–750, 2017.

[50] R. Zeyde, M. Elad, and M. Protter. On single image scale-up using sparse-representations. In International

[51] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Beyond a gaussian denoiser: Residual learning of

conference on curves and surfaces, 2010.

deep cnn for image denoising. IEEE TIP, 2017.

[52] K. Zhang, W. Zuo, S. Gu, and L. Zhang. Learning deep cnn denoiser prior for image restoration. In CVPR,

[53] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and P. H. Torr. Conditional

random ﬁelds as recurrent neural networks. In ICCV, 2015.

[54] D. Zoran and Y. Weiss. From learning models of natural image patches to whole image restoration. In

ICCV, 2011.

2017.

2017.

2010.

2017.

13

8
1
0
2
 
c
e
D
 
1
1
 
 
]

V
C
.
s
c
[
 
 
2
v
9
1
9
2
0
.
6
0
8
1
:
v
i
X
r
a

Non-Local Recurrent Network for Image Restoration

Ding Liu1, Bihan Wen1, Yuchen Fan1, Chen Change Loy2, Thomas S. Huang1
1University of Illinois at Urbana-Champaign 2Nanyang Technological University
{dingliu2, bwen3, yuchenf4, t-huang1}@illinois.edu ccloy@ntu.edu.sg

Abstract

Many classic methods have shown non-local self-similarity in natural images
to be an effective prior for image restoration. However, it remains unclear and
challenging to make use of this intrinsic property via deep networks.
In this
paper, we propose a non-local recurrent network (NLRN) as the ﬁrst attempt to
incorporate non-local operations into a recurrent neural network (RNN) for image
restoration. The main contributions of this work are: (1) Unlike existing methods
that measure self-similarity in an isolated manner, the proposed non-local module
can be ﬂexibly integrated into existing deep networks for end-to-end training to
capture deep feature correlation between each location and its neighborhood. (2)
We fully employ the RNN structure for its parameter efﬁciency and allow deep
feature correlation to be propagated along adjacent recurrent states. This new design
boosts robustness against inaccurate correlation estimation due to severely degraded
images. (3) We show that it is essential to maintain a conﬁned neighborhood for
computing deep feature correlation given degraded images. This is in contrast to
existing practice [43] that deploys the whole image. Extensive experiments on both
image denoising and super-resolution tasks are conducted. Thanks to the recurrent
non-local operations and correlation propagation, the proposed NLRN achieves
superior results to state-of-the-art methods with many fewer parameters. The code
is available at https://github.com/Ding-Liu/NLRN.

1

Introduction

Image restoration is an ill-posed inverse problem that aims at estimating the underlying image from its
degraded measurements. Depending on the type of degradation, image restoration can be categorized
into different sub-problems, e.g., image denoising and image super-resolution (SR). The key to
successful restoration typically relies on the design of an effective regularizer based on image priors.
Both local and non-local image priors have been extensively exploited in the past. Considering
image denoising as an example, local image properties such as Gaussian ﬁltering and total variation
based methods [32] are widely used in early studies. Later on, the notion of self-similarity in natural
images draws more attention and it has been exploited by non-local-based methods, e.g., non-local
means [2], collaborative ﬁltering [8], joint sparsity [28], and low-rank modeling [16]. These non-local
methods are shown to be effective in capturing the correlation among non-local patches to improve
the restoration quality.

While non-local self-similarity has been extensively studied in the literature, approaches for capturing
this intrinsic property with deep networks are little explored. Recent convolutional neural networks
(CNNs) for image restoration [10, 21, 29, 51] achieve impressive performance over conventional
approaches but do not explicitly use self-similarity properties in images. To rectify this weakness, a
few studies [24, 31] apply block matching to patches before feeding them into CNNs. Nevertheless,
the block matching step is isolated and thus not jointly trained with image restoration networks.

In this paper, we present the ﬁrst attempt to incorporate non-local operations in CNN for image
restoration, and propose a non-local recurrent network (NLRN) as an efﬁcient yet effective network

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

with non-local module. First, we design a non-local module to produce reliable feature correlation
for self-similarity measurement given severely degraded images, which can be ﬂexibly integrated
into existing deep networks while embracing the beneﬁt of end-to-end learning. For high parameter
efﬁciency without compromising restoration quality, we deploy a recurrent neural network (RNN)
framework similar to [22, 37, 38] such that operations with shared weights are applied recursively.
Second, we carefully study the behavior of non-local operation in deep feature space and ﬁnd that
limiting the neighborhood of correlation computation improves its robustness to degraded images. The
conﬁned neighborhood helps concentrate the computation on relevant features in the spatial vicinity
and disregard noisy features, which is in line with conventional image restoration approaches [8, 16].
In addition, we allow message passing of non-local operations between adjacent recurrent states of
RNN. Such inter-state ﬂow of feature correlation facilitates more robust correlation estimation. By
combining the non-local operation with typical convolutions, our NLRN can effectively capture and
employ both local and non-local image properties for image restoration.

It is noteworthy that recent work has adopted similar ideas on video classiﬁcation [43]. However,
our method signiﬁcantly differs from it in the following aspects. For each location, we measure the
feature correlation of each location only in its neighborhood, rather than throughout the whole image
as in [43]. In our experiments, we show that deep features useful for computing non-local priors
are more likely to reside in neighboring regions. A larger neighborhood (the whole image as one
extreme) can lead to inaccurate correlation estimation over degraded measurements. In addition, our
method fully exploits the advantage of RNN architecture - the correlation information is propagated
among adjacent recurrent states to increase the robustness of correlation estimation to degradations of
various degrees. Moreover, our non-local module is ﬂexible to handle inputs of various sizes, while
the module in [43] handles inputs of ﬁxed sizes only.

We introduce NLRN by ﬁrst relating our proposed model to other classic and existing non-local
image restoration approaches in a uniﬁed framework. We thoroughly analyze the non-local module
and recurrent architecture in our NLRN via extensive ablation studies. We provide a comprehensive
comparison with recent competitors, in which our NLRN achieves state-of-the-art performance
in image denoising and SR over several benchmark datasets, demonstrating the superiority of the
non-local operation with recurrent architecture for image restoration.

2 Related Work

Image self-similarity as an important image characteristic has been used in a number of non-local-
based image restoration approaches. The early works include bilateral ﬁltering [40] and non-local
means [2] for image denoising. Recent approaches exploit image self-similarity by imposing spar-
sity [28, 46]. Alternatively, similar image patches are modeled with low-rankness [16], or by
collaborative Wiener ﬁltering [8, 49]. Neighborhood embedding is a common approach for image
SR [5, 39], in which each image patch is approximated by multiple similar patches in a manifold.
Self-example based image SR approaches [15, 13] exploit the local self-similarity assumption, and
extract LR-HR exemplar pairs merely from the low-resolution image across different scales to predict
the high-resolution image. Similar ideas are adopted for image deblurring [9].

Deep neural networks have been prevalent for image restoration. The pioneering works include a
multilayer perceptron for image denoising [3] and a three-layer CNN for image SR [10]. Deconvolu-
tion is adopted to save computation cost and accelerate inference speed [36, 11]. Very deep CNNs are
designed to boost SR accuracy in [21, 23, 25]. Dense connections among various residual blocks are
included in [41]. Similarly CNN based methods are developed for image denoising in [29, 51, 52, 27].
Block matching as a preprocessing step is cascaded with CNNs for image denoising [24, 31]. Be-
sides CNNs, RNNs have also been applied for image restoration while enjoying the high parameter
efﬁciency [22, 37, 38].

In addition to image restoration, feature correlations are widely exploited along with neural networks
in many other areas, including graphical models [53, 4, 18], relational reasoning [33], machine
translation [14, 42] and so on. We do not elaborate on them here due to the limitation of space.

3 Non-Local Operations for Image Restoration

In this section, we ﬁrst present a uniﬁed framework of non-local operations used for image restoration
methods, e.g., collaborative ﬁltering [8], non-local means [2], and low-rank modeling [16], and we
discuss the relations between them. We then present the proposed non-local operation module.

2

3.1 A General Framework

In general, a non-local operation takes a multi-channel input X ∈ RN ×m as the image feature, and
generates output feature Z ∈ RN ×k. Here N and m denote the number of image pixels and data
channels, respectively. We propose a general framework with the following formulation:

Z = diag{δ(X)}−1 Φ(X) G(X) .
(1)
Here, Φ(X) ∈ RN ×N is the non-local correlation matrix, and G(X) ∈ RN ×k is the multi-channel
non-local transform. Each row vector X i denotes the local features in location i. Φ(X)j
i represents
the relationship between the X i and X j, and each row vector G(X)j is the embedding of X j.1 The
diagonal matrix diag{δ(X)} ∈ RN ×N normalizes the output at each i-th pixel with normalization
factor δi(X).

3.2 Classic Methods

The proposed framework works with various classic non-local methods for image restoration, includ-
ing methods based on low-rankness [16], collaborative ﬁltering [8], joint sparsity [28], as well as
non-local mean ﬁltering [2].

Block matching (BM) is a commonly used approach for exploiting non-local image structures
in conventional methods [16, 8, 28]. A q × q spatial neighborhood is set to be centered at each
location i, and X i reduces to the image patch centered at i. BM selects the Ki most similar patches
(Ki (cid:28) q2) from this neighborhood, which are used jointly to restore X i. Under the proposed
non-local framework, these methods can be represented as

Zi =

(cid:88)

1
δi(X)

j∈Ci

Φ(X)j

i G(X)j , ∀i .

(2)

j∈Ci

Φ(X)j

Here δi(X) = (cid:80)
i and Ci denotes the set of indices of the Ki selected patches. Thus, each
row Φ(X)i has only Ki non-zero entries. The embedding G(X) and the non-zero elements vary for
non-local methods based on different models. For example, in WNNM [16], (cid:80)
i G(X)j
corresponds to the projection of X i onto the group-speciﬁc subspace as a function of the selected
patches. Speciﬁcally, the subspace for calculating Zi is spanned by the eigenvectors U i of X T
X Ci .
Ci
Thus Zi = X CiU idiag{σ}U T
i , where diag{σ} is obtained by applying the shrinkage function
associated with the weighted nuclear norm [16] to the eigenvalues of X T
X Ci. We show the
Ci
generalization about more classic non-local image restoration methods in Section 7.

Φ(X)j

j∈Ci

Except for the hard block matching, other methods, e.g., the non-local means algorithm [2], apply
soft block matching by calculating the correlation between the reference patch and each patch in
the neighborhood. Each element Φ(X)j
i =
φ(X i, X j), where φ( · ) is determined by the distance metric. In [2], weighted Euclidean distance
with Gaussian kernel is applied as the metric, such that φ(X i, X j) = exp{− (cid:107)X i − X j(cid:107)2
2,a /h2}.
Besides, identity mapping is directly used as the embedding in [2], i.e., G(X)j = X j. In this case,
the non-local framework in (1) reduces to

i is determined only by each {X i, X j} pair, so Φ(X)j

Zi =

(cid:88)

1
δi(X)

j∈Si
exp{− (cid:107)X i − X j(cid:107)2

exp{−

(cid:107)X i − X j(cid:107)2
2,a
h2

}X j , ∀i,

(3)

j∈Si

where δi(X) = (cid:80)
2,a /h2} and Si is the set of indices in the neighborhood
of X i. Note that both a and h are constants, denoting the standard deviation of Gaussian kernel, and
the degree of ﬁltering, respectively [2]. It is noteworthy that the cardinality of Si for soft BM is much
larger than that of Ci for hard BM, which gives more ﬂexibility of using feature correlations between
neighboring locations.

The conventional non-local methods suffer from the drawback that parameters are either ﬁxed [2], or
obtained by suboptimal approaches [8, 28, 16], e.g., the parameters of WNNM are learned based on
the low-rankness assumption, which is suboptimal as the ultimate objective is to minimize the image
reconstruction error.

1In our analysis, if A is a matrix, Ai, Aj, and Aj

i denote its i-th row, j-th column, and the element at the

i-th row and j-th column, respectively.

3

(4)
(5)

(6)

Figure 1: An illustration of our non-local module working on a single location. The white tensor denotes
the deep feature representation of an entire image. The red ﬁber is the features of this location and the blue
tensor denotes the features in its neighborhood. θ, ψ and g are implemented by 1 × 1 convolution followed by
reshaping operations.

3.3 The Proposed Non-Local Module

Based on the general non-local framework in (1), we propose another soft block matching approach
and apply the Euclidean distance with linearly embedded Gaussian kernel [43] as the distance metric.
The linear embeddings are deﬁned as follows:

Φ(X)j

i = φ(X i, X j) = exp{θ(X i)ψ(X j)T } , ∀i, j ,

θ(X i) = X iW θ, ψ(X i) = X iW ψ, G(X)i = X iW g , ∀i .

The embedding transforms W θ, W φ, and W g are all learnable and have the shape of m × l, m ×
l, m × m, respectively. Thus, the proposed non-local operation can be written as

Zi =

(cid:88)

1
δi(X)

j∈Si

exp {X iW θW T

ψ X T

j } X iW g , ∀i ,

j∈Si

φ(X i, X j). Similar to [2], to obtain Zi, we evaluate the correlation between

where δi(X) = (cid:80)
X i and each X j in the neighborhood Si. More choices of φ(X i, X j) are discussed in Section 5.
The proposed non-local operation can be implemented by common differentiable operations, and thus
can be jointly learned when incorporated into a neural network. We wrap it as a non-local module
by adding a skip connection, as shown in Figure 1, since the skip connection enables us to insert a
non-local module into any pre-trained model, while maintaining its initial behavior by initializing
W g as zero. Such a module introduces only a limited number of parameters since θ, ψ and g are
1 × 1 convolutions and m = 128, l = 64 in practice. The output of this module on each location only
depends on its q × q neighborhood, so this operation can work on inputs of various sizes.

Relation to Other Methods: Recent works have combined non-local BM and neural networks
for image restoration [31, 24, 43]. Lefkimmiatis [24] proposed to ﬁrst apply BM to noisy image
patches. The hard BM results are used to group patch features, and a CNN conducts a trainable
collaborative ﬁltering over the matched patches. Qiao et al. [31] combined similar non-local BM
with TNRD networks [7] for image denoising. However, as conventional methods [8, 28, 16], these
works [24, 31] conduct hard BM directly over degraded input patches, which may be inaccurate over
severely degraded images. In contrast, our proposed non-local operation as soft BM is applied on
learned deep feature representations that are more robust to degradation. Furthermore, the matching
results in [24] are isolated from the neural network, similar to the conventional approaches, whereas
the proposed non-local module is trained jointly with the entire network in an end-to-end manner.

Wang et al. [43] used similar approaches to add non-local operations into neural networks for high-
level vision tasks. However, unlike our approach, Wang et al. [43] calculated feature correlations
throughout the whole image. which is equivalent to enlarging the neighborhood to the entire image in
our approach. We empirically show that increasing the neighborhood size does not always improve
image restoration performance, due to the inaccuracy of correlation estimation over degraded input
images. Hence it is imperative to choose a neighborhood of a proper size to achieve best performance
for image restoration. In addition, the non-local operation in [43] can only handle input images of
ﬁxed size, while our module in (6) is ﬂexible to various image sizes. Finally, our non-local module,
when incorporated into an RNN framework, allows the ﬂow of correlation information between
adjacent states to enhance robustness against inaccurate correlation estimation. This is a new unique
formulation to deal with degraded images. More details are provided next.

4

Figure 2: An illustration of the transition function
frecurrent in the proposed NLRN.

Figure 3: The operations for a single location i in the
non-local module used in NLRN.

4 Non-Local Recurrent Network
In this section, we describe the RNN architecture that incorporates the non-local module to form
our NLRN. We adopt the common formulation of an RNN, which consists of a set of states, namely,
input state, output state and recurrent state, as well as transition functions among the states. The
input, output, and recurrent states are represented as x, y and s respectively. At each time step t,
an RNN receives an input xt, and the recurrent state and the output state of the RNN are updated
recursively as follows:

st = finput(xt) + frecurrent(st−1),

yt = foutput(st),

(7)

where finput, foutput, and frecurrent are reused at every time step. In our NLRN, we set the following:

• s0 is a function of the input image I.
• xt = 0, ∀t ∈ {1, . . . , T }, and finput(0) = 0.
• The output state yt is calculated only at the time T as the ﬁnal output.

We add an identity path from the very ﬁrst state which helps gradient backpropagation during
training [37], and a residual path of the deep feature correlation between each location and its
corr}, and st = frecurrent(st−1, s0), ∀t ∈
neighborhood from the previous state. Hence, st = {st
{1, . . . , T }, where st
corr is the collection of deep feature
correlation. For the transition function frecurrent, a non-local module is ﬁrst adopted and is followed
by two convolutional layers, before the feature s0 is added from the identity path. The weights in the
non-local module are shared across recurrent states just as convolutional layers, so our NLRN still
keeps high parameter efﬁciency as a whole. An illustration is displayed in Figure 2.

feat denotes the feature map in time t and st

feat, st

It is noteworthy that inside the non-local module, the feature correlation for location i from the
previous state, st−1
corr,i, is added to the estimated feature correlation in the current state before the
softmax normalization, which enables the propagation of correlation information between adjacent
states for more robust correlation estimation. The details can be found in Figure 3. The initial
state s0 is set as the feature after a convolutional layer on the input image. foutput is represented
by another single convolutional layer. All layers have 128 ﬁlters with 3 × 3 kernel size except for
the non-local module. Batch normalization and ReLU activation function are performed ahead of
each convolutional layer following [19]. We adopt residual learning and the output of NLRN is the
residual image ˆI = foutput(sT ) when NLRN is unfolded T times. During training, the objective is to
2 || ˆI + I − ˜I||2, where ˜I denotes the ground truth image.
minimize the mean square error L( ˆI, ˜I) = 1
Relation to Other RNN Methods: Although RNNs have been adopted for image restoration before,
our NLRN is the ﬁrst to incorporate non-local operations into an RNN framework with correlation
propagation. DRCN [22] recursively applies a single convolutional layer to the input feature map
multiple times without the identity path from the ﬁrst state. DRRN [37] applies both the identity path
and the residual path in each state, but without non-local operations, and thus there is no correlation
information ﬂow across adjacent states. MemNet [38] builds dense connections among several types
of memory blocks, and weights are shared in the same type of memory blocks but are different across
various types. Compared with MemNet, our NLRN has an efﬁcient yet effective RNN structure with
shallower effective depth and fewer parameters, but obtains better restoration performance, which is
shown in Section 5 in detail.

5 Experiments

Dataset: For image denoising, we adopt two different settings to fairly and comprehensively compare
with recent deep learning based methods [29, 24, 51, 38]: (1) As in [7, 51, 24], we choose as the

5

training set the combination of 200 images from the train set and 200 images from the test set in the
Berkeley Segmentation Dataset (BSD) [30], and test on two popular benchmarks: Set12 and Set68
with σ = 15, 25, 50 following [51]. (2) As in [29, 38], we use as the training set the combination of
200 images from the train set and 100 images from the val set in BSD, and test on Set14 and the
BSD test set of 200 images with σ = 30, 50, 70 following [29, 38]. In addition, we evaluate our
NLRN on the Urban100 dataset [20], which contains abundant structural patterns and textures, to
further demonstrate the capability of using image self-similarity of our NLRN. The training set and
test set are strictly disjoint and all the images are converted to gray-scale in each experiment setup.
For image SR, we follow [21, 37, 38] and use a training set of 291 images where 91 images are
proposed in [48] and other 200 are from the BSD train set. We adopt four benchmark sets: Set5 [1],
Set14 [50], BSD100 [30] and Urban100 [20] for testing with three upscaling factors: ×2, ×3 and
×4. The low-resolution images are synthesized by bicubic downsampling.

Training Settings: We randomly sample patches whose size equals the neighborhood of non-local
operation from images during training. We use ﬂipping, rotation and scaling for augmenting training
data. For image denoising, we add independent and identically distributed Gaussian noise with zero
mean to the original image as the noisy input during training. We train a different model for each
noise level. For image SR, only the luminance channel of images is super-resolved, and the other two
color channels are upscaled by bicubic interpolation, following [21, 22, 37]. Moreover, the training
images for all three upscaling factors: ×2, ×3 and ×4 are upscaled by bicubic interpolation into the
desired spatial size and are combined into one training set. We use this set to train one single model
for all these three upscaling factors as in [21, 37, 38].

We use Adam optimizer to minimize the loss function. We set the initial learning rate as 1e-3 and
reduce it by half ﬁve times during training. We use Xavier initialization for the weights. We clip
the gradient at the norm of 0.5 to prevent the gradient explosion which is shown to empirically
accelerate training convergence, and we adopt 16 as the minibatch size during training. Training a
model takes about 3 days with a Titan Xp GPU. For non-local module, we use circular padding for
the neighborhood outside input patches. For convolution, we pad the boundaries of feature maps with
zeros to preserve the spatial size of feature maps.

5.1 Model Analysis

In this section, we analyze our model in the following aspects. First, we conduct the ablation study of
using different distance metrics in the non-local module. Table 1 compares instantiations including
Euclidean distance, dot product, embedded dot product, Gaussian, symmetric embedded Gaussian
and embedded Gaussian when used in NLRN of 12 unfolded steps. Embedded Gaussian achieves the
best performance and is adopted in the following experiments.

We compare the NLRN with its variants in terms of PSNR in Table 2. We have a few observations.
First, the same model with untied weights performs worse than its weight-sharing counter-part. We
speculate that the model with untied weights is prone to model over-ﬁtting and suffers much slower
training convergence, both of which undermine its performance. To investigate the function of non-
local modules, we implement a baseline RNN with the same parameter number of NLRN, and ﬁnd it
is worse than NLRN by about 0.2 dB, showing the advantage of using non-local image properties for
image restoration. Besides, we implement NLRNs where non-local module is used in every other
state or every three states, and observe that if the frequency of using non-local modules in NLRN
is reduced, the performance decreases accordingly. We show the beneﬁt of propagating correlation
information among adjacent states by comparing with the counter-part in terms of restoration accuracy.
To further analyze the non-local module, we visualize the feature correlation maps for non-local
operations in Figure 4. It can be seen that as the number of recurrent states increases, the locations

Table 1: Image denoising comparison of our proposed model
with various distance metrics on Set12 with noise level of 25.

Distance metric
Euclidean distance
Dot product
Embedded dot product
Gaussian
Symmetric embedded Gaussian
Embedded Gaussian

φ(X i, X j)

exp{− (cid:107)X i − X j(cid:107)2

2 /h2}

X iX T
j
θ(X i)ψ(X j)T
exp{X iX T
j }
exp{θ(X i)θ(X j)T }
exp{θ(X i)ψ(X j)T }

PSNR
30.74
30.68
30.75
30.69
30.76
30.80

Table 2: Image denoising comparison of our
NLRN with its variants on Set12 with noise
level of 25.

Model
NLRN w/o parameter sharing
RNN with same parameter no.
Non-local module in every other state
Non-local module in every 3 states
NLRN w/o propagating correlations
NLRN

PSNR
30.65
30.61
30.76
30.72
30.78
30.80

6

Figure 4: Examples of correlation maps of non-local operations for
image denoising. Noisy patch/ground truth patch: the neighborhood of
the red center pixel used in non-local operations. (1)-(6): the correlation
map for recurrent state 1-6 from NLRN with unrolling length of 6.

Figure 5: Neighborhood size vs.
image denoising performance of
our proposed model on Set12 with
noise level of 25.

Max effective depth
Parameter sharing
Parameter no.
Multi-view testing
Training images
PSNR

DnCNN
17
No
554k
No
400
27.18

RED MemNet
30
No
4,131k
Yes
300
27.33

80
Yes
667k
No
300
27.38

NLRN
38
Yes
330k
No
300
27.60

No
400
27.64

Yes
300
27.66

Table 3: Image denoising comparison of our proposed model with state-
of-the-art network models on Set12 with noise level of 50. Model com-
plexities are also compared.

Figure 6: Unrolling length vs.
image denoising performance of
our proposed model on Set12 with
noise level of 25.

with similar features progressively show higher correlations in the map, which demonstrates the
effectiveness of the non-local module for exploiting image self-similarity.

Figure 5 investigates the inﬂuence of the neighborhood size in the non-local module on image
denoising results. The performance peaks at q = 45. This shows that limiting the neighborhood
helps concentrate the correlation calculation on relevant features in the spatial vicinity and enhance
correlation estimation. Therefore, it is necessary to choose a proper neighborhood size (rather than
the whole image) for image restoration. We select q = 45 for the rest of this paper unless stated
otherwise.

The unrolling length T determines the maximum effective depth (i.e., maximum number of convolu-
tional layers) of NLRN. The inﬂuence of the unrolling length on image denoising results is shown in
Figure 6. The performance increases as the unrolling length rises, but gets saturated after T = 12.
Given the tradeoff between restoration accuracy and inference time, we adopt T = 12 for NLRN in
all the experiments.

5.2 Comparisons with State-of-the-Art Methods

We compare our proposed model with a number of recent competitors for image denoising and
image SR, respectively. PSNR and SSIM [44] are adopted for measuring quantitative restoration
performance.

Image Denoising: For a fair comparison with other methods based on deep networks, we train our
model under two settings: (1) We use the training data as in TNRD [7], DnCNN [51] and NLNet [24],
and the result is shown in Table 4. We cite the result of NLNet in the original paper [24], since no
public code or model is available. (2) We use the training data as in RED [29] and MemNet [38], and
the result is shown in Table 5. We note that RED uses multi-view testing [45] to boost the restoration
accuracy, i.e., RED processes each test image as well as its rotated and ﬂipped versions, and all
the outputs are then averaged to form the ﬁnal denoised image. Accordingly, we perform the same
procedure for NLRN and ﬁnd its performance, termed as NLRN-MV, is consistently improved. In
addition, we include recent non-deep-learning based methods: BM3D [8] and WNNM [16] in our
comparison. We do not list other methods [54, 3, 47, 6, 52] whose average performances are worse
than DnCNN or MemNet. Our NLRN signiﬁcantly outperforms all the competitors on Urban100 and
yields the best results across almost all the noise levels and datasets.

To further show the advantage of the network design of NLRN, we compare different versions of
NLRN with several state-of-the-art network models, i.e., DnCNN, RED and MemNet in Table 3.
NLRN uses the fewest parameters but outperforms all the competitors. Speciﬁcally, NLRN beneﬁts

7

Table 4: Benchmark image denoising results. Training and testing protocols are followed as in [51]. Average
PSNR/SSIM for various noise levels on Set12, BSD68 and Urban100. The best performance is in bold.

Dataset

Set12

BSD68

Urban100

Noise
15
25
50
15
25
50
15
25
50

BM3D
32.37/0.8952
29.97/0.8504
26.72/0.7676
31.07/0.8717
28.57/0.8013
25.62/0.6864
32.35/0.9220
29.70/0.8777
25.95/0.7791

WNNM
32.70/0.8982
30.28/0.8557
27.05/0.7775
31.37/0.8766
28.83/0.8087
25.87/0.6982
32.97/0.9271
30.39/0.8885
26.83/0.8047

TNRD
32.50/0.8958
30.06/0.8512
26.81/0.7680
31.42/0.8769
28.92/0.8093
25.97/0.6994
31.86/0.9031
29.25/0.8473
25.88/0.7563

NLNet
-/-
-/-
-/-
31.52/-
29.03/-
26.07/-
-/-
-/-
-/-

DnCNN
32.86/0.9031
30.44/0.8622
27.18/0.7829
31.73/0.8907
29.23/0.8278
26.23/0.7189
32.68/0.9255
29.97/0.8797
26.28/0.7874

NLRN
33.16/0.9070
30.80/0.8689
27.64/0.7980
31.88/0.8932
29.41/0.8331
26.47/0.7298
33.45/0.9354
30.94/0.9018
27.49/0.8279

Table 5: Benchmark image denoising results. Training and testing protocols are followed as in [38]. Average
PSNR/SSIM for various noise levels on 14 images, BSD200 and Urban100. Red is the best and blue is the
second best performance.

Dataset

14 images

BSD200

Urban100

Noise
30
50
70
30
50
70
30
50
70

BM3D
28.49/0.8204
26.08/0.7427
24.65/0.6882
27.31/0.7755
25.06/0.6831
23.82/0.6240
28.75/0.8567
25.95/0.7791
24.27/0.7163

WNNM
28.74/0.8273
26.32/0.7517
24.80/0.6975
27.48/0.7807
25.26/0.6928
23.95/0.6346
29.47/0.8697
26.83/0.8047
25.11/0.7501

RED
29.17/0.8423
26.81/0.7733
25.31/0.7206
27.95/0.8056
25.75/0.7167
24.37/0.6551
29.12/0.8674
26.44/0.7977
24.75/0.7415

MemNet
29.22/0.8444
26.91/0.7775
25.43/0.7260
28.04/0.8053
25.86/0.7202
24.53/0.6608
29.10/0.8631
26.65/0.8030
25.01/0.7496

NLRN
29.37/0.8460
27.00/0.7777
25.49/0.7255
28.15/0.8423
25.93/0.7214
24.58/0.6614
29.94/0.8830
27.38/0.8241
25.66/0.7707

NLRN-MV
29.41/0.8472
27.05/0.7791
25.54/0.7273
28.20/0.8436
25.97/0.8429
24.62/0.6634
29.99/0.8842
27.43/0.8256
25.71/0.7724

Table 6: Benchmark SISR results. Average PSNR/SSIM for scale factor ×2, ×3 and ×4 on datasets Set5, Set14,
BSD100 and Urban100. The best performance is in bold.

Dataset

Set5

Set14

BSD100

Urban100

Scale
×2
×3
×4
×2
×3
×4
×2
×3
×4
×2
×3
×4

SRCNN
36.66/0.9542
32.75/0.9090
30.48/0.8628
32.45/0.9067
29.30/0.8215
27.50/0.7513
31.36/0.8879
28.41/0.7863
26.90/0.7101
29.50/0.8946
26.24/0.7989
24.52/0.7221

VDSR
37.53/0.9587
33.66/0.9213
31.35/0.8838
33.03/0.9124
29.77/0.8314
28.01/0.7674
31.90/0.8960
28.82/0.7976
27.29/0.7251
30.76/0.9140
27.14/0.8279
25.18/0.7524

DRCN
37.63/0.9588
33.82/0.9226
31.53/0.8854
33.04/0.9118
29.76/0.8311
28.02/0.7670
31.85/0.8942
28.80/0.7963
27.23/0.7233
30.75/0.9133
27.15/0.8276
25.14/0.7510

LapSRN
37.52/0.959
33.82/0.923
31.54/0.885
33.08/0.913
29.79/0.832
28.19/0.772
31.80/0.895
28.82/0.797
27.32/0.728
30.41/0.910
27.07/0.827
25.21/0.756

DRRN
37.74/0.9591
34.03/0.9244
31.68/0.8888
33.23/0.9136
29.96/0.8349
28.21/0.7721
32.05/0.8973
28.95/0.8004
27.38/0.7284
31.23/0.9188
27.53/0.8378
25.44/0.7638

MemNet
37.78/0.9597
34.09/0.9248
31.74/0.8893
33.28/0.9142
30.00/0.8350
28.26/0.7723
32.08/0.8978
28.96/0.8001
27.40/0.7281
31.31/0.9195
27.56/0.8376
25.50/0.7630

NLRN
38.00/0.9603
34.27/0.9266
31.92/0.8916
33.46/0.9159
30.16/0.8374
28.36/0.7745
32.19/0.8992
29.06/0.8026
27.48/0.7306
31.81/0.9249
27.93/0.8453
25.79/0.7729

from inherent parameter sharing and uses only less than 1/10 parameters of RED. Compared with the
RNN competitor, MemNet, NLRN uses only half of parameters and much shallower depth to obtain
better performance, which shows the superiority of our non-local recurrent architecture.

Image Super-Resolution: We compare our model with several recent SISR approaches, including
SRCNN [10], VDSR [21], DRCN [22], LapSRN [23], DRRN [37] and MemNet [38] in Table 6. We
crop pixels near image borders before calculating PSNR and SSIM as in [10, 35, 21, 22]. We do
not list other methods [20, 35, 26, 36, 17] since their performances are worse than that of DRRN or
MemNet. Besides, we do not include SRDenseNet [41] and EDSR [25] in the comparison because
the number of parameters in these two network models is over two orders of magnitude larger than
that of our NLRN and their training datasets are signiﬁcantly larger than ours. It can be seen that
NLRN yields the best result across all the upscaling factors and datasets. Visual results are provided
in Section 7.

6 Conclusion

We have presented a new and effective recurrent network that incorporates non-local operations for
image restoration. The proposed non-local module can be trained end-to-end with the recurrent
network. We have studied the importance of computing reliable feature correlations within a conﬁned
neighorhood against the whole image, and have shown the beneﬁts of passing feature correlation
messages between adjacent recurrent stages. Comprehensive evaluations over benchmarks for image
denoising and super-resolution demonstrate the superiority of NLRN over existing methods.

8

7 Appendix

7.1 Extension of the General Framework to Other Classic Non-Local Methods

Besides the extension to WNNM and non-local means, which are discussed in Section 3.1, we show
the proposed non-local framework (1) can be extended to collaborative ﬁltering methods, e.g., BM3D
algorithm [8], as well as joint sparsity based methods, e.g., LSSC algorithm [28]. We follow the same
notations in Section 3.1. Both BM3D and LSSC apply block matching (BM) ﬁrst before processing,
and form N groups of similar patches into data matrices. The index set of the matched patches for
the i-th reference patch is denoted as Ci. The group of matched patches for the i-th reference patch is
denoted as X Ci.
Similar to WNNM [16], BM3D [8] also applies BM ﬁrst to group similar patches based on their
Euclidean distances. The matched patches are then processed via Wiener ﬁltering [8], and the
denoised results of the i-th group of patches are

ZCi = τ −1(diag(ω)τ (X Ci)).

(8)

Here τ (·) and τ −1(·) denote the forward and backward Wiener ﬁltering applied to the groups of
matched patches, respectively. The diagonal matrix diag(ω) is formed by the empirical Wiener
coefﬁcients ω. BM3D applies data pre-cleaning, using discrete cosine transform (DCT), to estimate
the original patch, and calculate the estimate of ω [8]. Since calculating ZCi in (8) involves only
linear ﬁltering, it can also be generalized using the proposed non-local framework as (2). Unlike the
extension to WNNM, here (cid:80)
i G(X)j corresponds to the denoised results via Wiener
ﬁltering as shown in (8), of the i-th group of matched patches.

Φ(X)j

j∈Ci

Different from BM3D and WNNM, LSSC learns a common dictionary D for all image patches, and
imposes joint sparsity [28] on each data matrix of matched patches X Ci, so that the correlation of
the matched patches are exploited by enforcing the same support of their sparse codes. Thus, the
joint sparse coding in LSSC [28] becomes

ˆAi = argminAi (cid:107)Ai(cid:107)0,∞ s.t.

− DAi

≤ (cid:15) |Ci| , ∀i ,

(9)

(cid:13)
(cid:13)X T
(cid:13)
Ci

(cid:13)
2
(cid:13)
(cid:13)

F

where the (0, ∞) “norm” (cid:107)·(cid:107)0,∞ counts the number of non-zero columns of each sparse code matrix
Ai [28], and |Ci| is the cardinality of Ci. The coefﬁcient (cid:15) is a constant, which is used to upper bound
the sparse modeling errors. In general, the solution to (9) is NP-hard. To simplify the discussion, we
assume the dictionary to be unitary (which reduces the sparse coding problem to the transform-model
sparse coding [46]), i.e., DT D = I and D ∈ Rk×k. Thus there exists a corresponding shrinkage
function η(·) for imposing joint sparsity on the sparse codes [28, 34], such that the denoised estimates
T
of the i-th patch group can be obtained as ZCi = ˆA
i DT = η( X Ci D ) DT . Though joint sparse
coding projects all data onto a union of subspaces [28, 12, 46] which is a non-linear operation in
general, each data matrix X Ci is projected onto one particular subspace spanned by the selected
atoms corresponding to the non-zero columns in ˆAi, which is locally linear. For the i-th group of
patches, such a subspace projection corresponds to (cid:80)
i G(X)j in the proposed general
framework.

Φ(X)j

j∈Ci

7.2 Visual Results

We show the visual comparison of our NLRN and several competing methods: BM3D [8],
WNNM [16], and MemNet [38] for image denoising in Figure 7. Our method can recover more
details from the noisy measurement. The visual comparison of our NLRN and several recent methods:
DRCN [22], LapSRN [23], DRRN [37], and MemNet [38] for image super-resolution is displayed in
Figure 8. Our method is able to reconstruct sharper edges and produce fewer artifacts especially in
the regions of repetitive patterns.

9

Noisy (18.75/0.3232)

BM3D (29.13/0.8261)

WNNM (29.30/0.8334)

Ground truth

MemNet (29.18/0.8223)

NLRN (29.53/0.8369)

Ground truth (PSNR/SSIM)

Noisy (19.49/0.5099)

BM3D (28.95/0.9062)

WNNM (30.44/0.9260)

Ground truth

MemNet (28.71/0.8906)

NLRN (30.52/0.9267)

Ground truth (PSNR/SSIM)

Noisy (19.39/0.4540)

BM3D (27.20/0.8775)

WNNM (28.86/0.8913)

Ground truth

MemNet (27.55/0.8807)

NLRN (29.04/0.9044)

Ground truth (PSNR/SSIM)

Noisy (19.06/0.3005)

BM3D (29.61/0.8304)

WNNM (30.59/0.8543)

Ground truth

MemNet (30.21/0.8517)

NLRN (31.17/0.8727)

Ground truth (PSNR/SSIM)

Noisy (18.88/0.3479)

BM3D (28.82/0.9051)

WNNM (28.38/0.9189)

Ground truth

MemNet (28.31/0.9112)

NLRN (28.42/0.9308)

Ground truth (PSNR/SSIM)

Figure 7: Qualitative comparison of image denoising results with noise level of 30. The zoom-in region in the
red bounding box is shown on the right. From top to bottom: 1) the image barbara. 2) image 004 in Urban100.
3) image 019 in Urban100. 4) image 033 in Urban100. 5) image 046 in Urban100.

10

DRCN (26.82/0.9329)

LapSRN (26.52/0.9316)

DRRN (27.52/0.9434)

Ground truth HR

MemNet (27.78/0.9451)

NLRN (28.46/0.9513)

HR (PSNR/SSIM)

DRCN (20.95/0.7716)

LapSRN (20.90/0.7722)

DRRN (21.37/0.7874)

Ground truth HR

MemNet (21.35/0.7877)

NLRN (21.92/0.8014)

HR (PSNR/SSIM)

DRCN (30.18/0.8306)

LapSRN (30.29/0.8388)

DRRN (30.18/0.8306)

Ground truth HR

MemNet (29.25/0.8347)

NLRN (31.19/0.8598)

HR (PSNR/SSIM)

DRCN (20.71/0.7466)

LapSRN (20.86/0.7524)

DRRN (20.92/0.7666)

Ground truth HR

MemNet (21.06/0.7716)

NLRN (21.41/0.7866)

HR (PSNR/SSIM)

DRCN (23.99/0.6940)

LapSRN (24.49/0.7247)

DRRN (25.14/0.7469)

Ground truth HR

MemNet (25.19/0.7519)

NLRN (25.97/0.7882)

HR (PSNR/SSIM)

Figure 8: Qualitative comparison of image super-resolution results with ×4 upscaling. The zoom-in region in
the red bounding box is shown on the right. From top to bottom: 1) image 005 in Urban100. 2) image 019 in
Urban100. 3) image 044 in Urban100. 4) image 062 in Urban100. 5) image 099 in Urban100.

References

[1] M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. Alberi-Morel. Low-complexity single-image super-

resolution based on nonnegative neighbor embedding. 2012.

11

[2] A. Buades, B. Coll, and J.-M. Morel. A non-local algorithm for image denoising. In CVPR, 2005.
[3] H. C. Burger, C. J. Schuler, and S. Harmeling. Image denoising: Can plain neural networks compete with

[4] S. Chandra, N. Usunier, and I. Kokkinos. Dense and low-rank gaussian crfs using deep embeddings. In

[5] H. Chang, D.-Y. Yeung, and Y. Xiong. Super-resolution through neighbor embedding. In CVPR, 2004.
[6] F. Chen, L. Zhang, and H. Yu. External patch prior guided internal clustering for image denoising. In

bm3d? In CVPR, 2012.

ICCV, 2017.

ICCV, 2015.

[7] Y. Chen and T. Pock. Trainable nonlinear reaction diffusion: A ﬂexible framework for fast and effective

[8] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-d transform-domain

[9] A. Danielyan, V. Katkovnik, and K. Egiazarian. Bm3d frames and variational image deblurring. TIP,

image restoration. IEEE TPAMI, 2017.

collaborative ﬁltering. IEEE TIP, 2007.

21(4):1715–1728, 2012.

[10] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep convolutional network for image super-resolution.

[11] C. Dong, C. C. Loy, and X. Tang. Accelerating the super-resolution convolutional neural network. In

In ECCV, 2014.

ECCV, 2016.

IEEE TIP, 2006.

Graphics (TOG), 2011.

learning. In ICML, 2017.

[12] M. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned dictionaries.

[13] G. Freedman and R. Fattal. Image and video upscaling from local self-examples. ACM Transactions on

[14] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to sequence

[15] D. Glasner, S. Bagon, and M. Irani. Super-resolution from a single image. In ICCV, 2009.
[16] S. Gu, L. Zhang, W. Zuo, and X. Feng. Weighted nuclear norm minimization with application to image

denoising. In CVPR, pages 2862–2869, 2014.

[17] W. Han, S. Chang, D. Liu, M. Yu, M. Witbrock, and T. S. Huang. Image super-resolution via dual-state

[18] A. W. Harley, K. G. Derpanis, and I. Kokkinos. Segmentation-aware convolutional networks using local

recurrent networks. In CVPR, June 2018.

attention masks. In ICCV, 2017.

[19] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, 2016.
[20] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-resolution from transformed self-exemplars. In

[21] J. Kim, J. Kwon Lee, and K. Mu Lee. Accurate image super-resolution using very deep convolutional

CVPR, 2015.

networks. In CVPR, 2016.

In CVPR, 2016.

super-resolution. In CVPR, 2017.

[22] J. Kim, J. Kwon Lee, and K. Mu Lee. Deeply-recursive convolutional network for image super-resolution.

[23] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang. Deep laplacian pyramid networks for fast and accurate

[24] S. Lefkimmiatis. Non-local color image denoising with convolutional neural networks. In CVPR, 2017.
[25] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced deep residual networks for single image

super-resolution. In CVPR Workshops, 2017.

[26] D. Liu, Z. Wang, B. Wen, J. Yang, W. Han, and T. S. Huang. Robust single image super-resolution via

deep networks with sparse prior. TIP, 25(7):3194–3207, 2016.

[27] D. Liu, B. Wen, X. Liu, Z. Wang, and T. S. Huang. When image denoising meets high-level vision tasks:

A deep learning approach. In IJCAI, 2018.

[28] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Non-local sparse models for image restoration.

In ICCV, 2009.

[29] X. Mao, C. Shen, and Y.-B. Yang. Image restoration using very deep convolutional encoder-decoder

networks with symmetric skip connections. In NIPS, 2016.

[30] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its

application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, 2001.

[31] P. Qiao, Y. Dou, W. Feng, R. Li, and Y. Chen. Learning non-local image diffusion for image denoising. In

ACM on Multimedia Conference, 2017.

[32] L. I. Rudin and S. Osher. Total variation based image restoration with free local constraints. In ICIP, 1994.
[33] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lillicrap. A simple

neural network module for relational reasoning. In NIPS, 2017.

[34] U. Schmidt and S. Roth. Shrinkage ﬁelds for effective image restoration. In CVPR, 2014.
[35] S. Schulter, C. Leistner, and H. Bischof. Fast and accurate image upscaling with super-resolution forests.

In CVPR, 2015.

12

[36] W. Shi, J. Caballero, F. Huszár, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-time
single image and video super-resolution using an efﬁcient sub-pixel convolutional neural network. In
CVPR, 2016.

[37] Y. Tai, J. Yang, and X. Liu. Image super-resolution via deep recursive residual network. In CVPR, 2017.
[38] Y. Tai, J. Yang, X. Liu, and C. Xu. Memnet: A persistent memory network for image restoration. In ICCV,

[39] R. Timofte, V. De, and L. Van Gool. Anchored neighborhood regression for fast example-based super-

resolution. In ICCV, 2013.

[40] C. Tomasi and R. Manduchi. Bilateral ﬁltering for gray and color images. In ICCV, 1998.
[41] T. Tong, G. Li, X. Liu, and Q. Gao. Image super-resolution using dense skip connections. In ICCV, 2017.
[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.

Attention is all you need. In NIPS, 2017.

[43] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. arXiv preprint arXiv:1711.07971,

[44] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility

[45] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang. Deep networks for image super-resolution with sparse

to structural similarity. IEEE TIP, 2004.

prior. In ICCV, 2015.

[46] B. Wen, S. Ravishankar, and Y. Bresler. Structured overcomplete sparsifying transform learning with

convergence guarantees and applications. IJCV, 2015.

[47] J. Xu, L. Zhang, W. Zuo, D. Zhang, and X. Feng. Patch group based nonlocal self-similarity prior learning

for image denoising. In ICCV, 2015.

[48] J. Yang, J. Wright, T. S. Huang, and Y. Ma. Image super-resolution via sparse representation. IEEE TIP,

[49] R. Yin, T. Gao, Y. M. Lu, and I. Daubechies. A tale of two bases: Local-nonlocal regularization on image

patches with convolution framelets. SIAM Journal on Imaging Sciences, 10(2):711–750, 2017.

[50] R. Zeyde, M. Elad, and M. Protter. On single image scale-up using sparse-representations. In International

[51] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Beyond a gaussian denoiser: Residual learning of

conference on curves and surfaces, 2010.

deep cnn for image denoising. IEEE TIP, 2017.

[52] K. Zhang, W. Zuo, S. Gu, and L. Zhang. Learning deep cnn denoiser prior for image restoration. In CVPR,

[53] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and P. H. Torr. Conditional

random ﬁelds as recurrent neural networks. In ICCV, 2015.

[54] D. Zoran and Y. Weiss. From learning models of natural image patches to whole image restoration. In

ICCV, 2011.

2017.

2017.

2010.

2017.

13

8
1
0
2
 
c
e
D
 
1
1
 
 
]

V
C
.
s
c
[
 
 
2
v
9
1
9
2
0
.
6
0
8
1
:
v
i
X
r
a

Non-Local Recurrent Network for Image Restoration

Ding Liu1, Bihan Wen1, Yuchen Fan1, Chen Change Loy2, Thomas S. Huang1
1University of Illinois at Urbana-Champaign 2Nanyang Technological University
{dingliu2, bwen3, yuchenf4, t-huang1}@illinois.edu ccloy@ntu.edu.sg

Abstract

Many classic methods have shown non-local self-similarity in natural images
to be an effective prior for image restoration. However, it remains unclear and
challenging to make use of this intrinsic property via deep networks.
In this
paper, we propose a non-local recurrent network (NLRN) as the ﬁrst attempt to
incorporate non-local operations into a recurrent neural network (RNN) for image
restoration. The main contributions of this work are: (1) Unlike existing methods
that measure self-similarity in an isolated manner, the proposed non-local module
can be ﬂexibly integrated into existing deep networks for end-to-end training to
capture deep feature correlation between each location and its neighborhood. (2)
We fully employ the RNN structure for its parameter efﬁciency and allow deep
feature correlation to be propagated along adjacent recurrent states. This new design
boosts robustness against inaccurate correlation estimation due to severely degraded
images. (3) We show that it is essential to maintain a conﬁned neighborhood for
computing deep feature correlation given degraded images. This is in contrast to
existing practice [43] that deploys the whole image. Extensive experiments on both
image denoising and super-resolution tasks are conducted. Thanks to the recurrent
non-local operations and correlation propagation, the proposed NLRN achieves
superior results to state-of-the-art methods with many fewer parameters. The code
is available at https://github.com/Ding-Liu/NLRN.

1

Introduction

Image restoration is an ill-posed inverse problem that aims at estimating the underlying image from its
degraded measurements. Depending on the type of degradation, image restoration can be categorized
into different sub-problems, e.g., image denoising and image super-resolution (SR). The key to
successful restoration typically relies on the design of an effective regularizer based on image priors.
Both local and non-local image priors have been extensively exploited in the past. Considering
image denoising as an example, local image properties such as Gaussian ﬁltering and total variation
based methods [32] are widely used in early studies. Later on, the notion of self-similarity in natural
images draws more attention and it has been exploited by non-local-based methods, e.g., non-local
means [2], collaborative ﬁltering [8], joint sparsity [28], and low-rank modeling [16]. These non-local
methods are shown to be effective in capturing the correlation among non-local patches to improve
the restoration quality.

While non-local self-similarity has been extensively studied in the literature, approaches for capturing
this intrinsic property with deep networks are little explored. Recent convolutional neural networks
(CNNs) for image restoration [10, 21, 29, 51] achieve impressive performance over conventional
approaches but do not explicitly use self-similarity properties in images. To rectify this weakness, a
few studies [24, 31] apply block matching to patches before feeding them into CNNs. Nevertheless,
the block matching step is isolated and thus not jointly trained with image restoration networks.

In this paper, we present the ﬁrst attempt to incorporate non-local operations in CNN for image
restoration, and propose a non-local recurrent network (NLRN) as an efﬁcient yet effective network

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

with non-local module. First, we design a non-local module to produce reliable feature correlation
for self-similarity measurement given severely degraded images, which can be ﬂexibly integrated
into existing deep networks while embracing the beneﬁt of end-to-end learning. For high parameter
efﬁciency without compromising restoration quality, we deploy a recurrent neural network (RNN)
framework similar to [22, 37, 38] such that operations with shared weights are applied recursively.
Second, we carefully study the behavior of non-local operation in deep feature space and ﬁnd that
limiting the neighborhood of correlation computation improves its robustness to degraded images. The
conﬁned neighborhood helps concentrate the computation on relevant features in the spatial vicinity
and disregard noisy features, which is in line with conventional image restoration approaches [8, 16].
In addition, we allow message passing of non-local operations between adjacent recurrent states of
RNN. Such inter-state ﬂow of feature correlation facilitates more robust correlation estimation. By
combining the non-local operation with typical convolutions, our NLRN can effectively capture and
employ both local and non-local image properties for image restoration.

It is noteworthy that recent work has adopted similar ideas on video classiﬁcation [43]. However,
our method signiﬁcantly differs from it in the following aspects. For each location, we measure the
feature correlation of each location only in its neighborhood, rather than throughout the whole image
as in [43]. In our experiments, we show that deep features useful for computing non-local priors
are more likely to reside in neighboring regions. A larger neighborhood (the whole image as one
extreme) can lead to inaccurate correlation estimation over degraded measurements. In addition, our
method fully exploits the advantage of RNN architecture - the correlation information is propagated
among adjacent recurrent states to increase the robustness of correlation estimation to degradations of
various degrees. Moreover, our non-local module is ﬂexible to handle inputs of various sizes, while
the module in [43] handles inputs of ﬁxed sizes only.

We introduce NLRN by ﬁrst relating our proposed model to other classic and existing non-local
image restoration approaches in a uniﬁed framework. We thoroughly analyze the non-local module
and recurrent architecture in our NLRN via extensive ablation studies. We provide a comprehensive
comparison with recent competitors, in which our NLRN achieves state-of-the-art performance
in image denoising and SR over several benchmark datasets, demonstrating the superiority of the
non-local operation with recurrent architecture for image restoration.

2 Related Work

Image self-similarity as an important image characteristic has been used in a number of non-local-
based image restoration approaches. The early works include bilateral ﬁltering [40] and non-local
means [2] for image denoising. Recent approaches exploit image self-similarity by imposing spar-
sity [28, 46]. Alternatively, similar image patches are modeled with low-rankness [16], or by
collaborative Wiener ﬁltering [8, 49]. Neighborhood embedding is a common approach for image
SR [5, 39], in which each image patch is approximated by multiple similar patches in a manifold.
Self-example based image SR approaches [15, 13] exploit the local self-similarity assumption, and
extract LR-HR exemplar pairs merely from the low-resolution image across different scales to predict
the high-resolution image. Similar ideas are adopted for image deblurring [9].

Deep neural networks have been prevalent for image restoration. The pioneering works include a
multilayer perceptron for image denoising [3] and a three-layer CNN for image SR [10]. Deconvolu-
tion is adopted to save computation cost and accelerate inference speed [36, 11]. Very deep CNNs are
designed to boost SR accuracy in [21, 23, 25]. Dense connections among various residual blocks are
included in [41]. Similarly CNN based methods are developed for image denoising in [29, 51, 52, 27].
Block matching as a preprocessing step is cascaded with CNNs for image denoising [24, 31]. Be-
sides CNNs, RNNs have also been applied for image restoration while enjoying the high parameter
efﬁciency [22, 37, 38].

In addition to image restoration, feature correlations are widely exploited along with neural networks
in many other areas, including graphical models [53, 4, 18], relational reasoning [33], machine
translation [14, 42] and so on. We do not elaborate on them here due to the limitation of space.

3 Non-Local Operations for Image Restoration

In this section, we ﬁrst present a uniﬁed framework of non-local operations used for image restoration
methods, e.g., collaborative ﬁltering [8], non-local means [2], and low-rank modeling [16], and we
discuss the relations between them. We then present the proposed non-local operation module.

2

3.1 A General Framework

In general, a non-local operation takes a multi-channel input X ∈ RN ×m as the image feature, and
generates output feature Z ∈ RN ×k. Here N and m denote the number of image pixels and data
channels, respectively. We propose a general framework with the following formulation:

Z = diag{δ(X)}−1 Φ(X) G(X) .
(1)
Here, Φ(X) ∈ RN ×N is the non-local correlation matrix, and G(X) ∈ RN ×k is the multi-channel
non-local transform. Each row vector X i denotes the local features in location i. Φ(X)j
i represents
the relationship between the X i and X j, and each row vector G(X)j is the embedding of X j.1 The
diagonal matrix diag{δ(X)} ∈ RN ×N normalizes the output at each i-th pixel with normalization
factor δi(X).

3.2 Classic Methods

The proposed framework works with various classic non-local methods for image restoration, includ-
ing methods based on low-rankness [16], collaborative ﬁltering [8], joint sparsity [28], as well as
non-local mean ﬁltering [2].

Block matching (BM) is a commonly used approach for exploiting non-local image structures
in conventional methods [16, 8, 28]. A q × q spatial neighborhood is set to be centered at each
location i, and X i reduces to the image patch centered at i. BM selects the Ki most similar patches
(Ki (cid:28) q2) from this neighborhood, which are used jointly to restore X i. Under the proposed
non-local framework, these methods can be represented as

Zi =

(cid:88)

1
δi(X)

j∈Ci

Φ(X)j

i G(X)j , ∀i .

(2)

j∈Ci

Φ(X)j

Here δi(X) = (cid:80)
i and Ci denotes the set of indices of the Ki selected patches. Thus, each
row Φ(X)i has only Ki non-zero entries. The embedding G(X) and the non-zero elements vary for
non-local methods based on different models. For example, in WNNM [16], (cid:80)
i G(X)j
corresponds to the projection of X i onto the group-speciﬁc subspace as a function of the selected
patches. Speciﬁcally, the subspace for calculating Zi is spanned by the eigenvectors U i of X T
X Ci .
Ci
Thus Zi = X CiU idiag{σ}U T
i , where diag{σ} is obtained by applying the shrinkage function
associated with the weighted nuclear norm [16] to the eigenvalues of X T
X Ci. We show the
Ci
generalization about more classic non-local image restoration methods in Section 7.

Φ(X)j

j∈Ci

Except for the hard block matching, other methods, e.g., the non-local means algorithm [2], apply
soft block matching by calculating the correlation between the reference patch and each patch in
the neighborhood. Each element Φ(X)j
i =
φ(X i, X j), where φ( · ) is determined by the distance metric. In [2], weighted Euclidean distance
with Gaussian kernel is applied as the metric, such that φ(X i, X j) = exp{− (cid:107)X i − X j(cid:107)2
2,a /h2}.
Besides, identity mapping is directly used as the embedding in [2], i.e., G(X)j = X j. In this case,
the non-local framework in (1) reduces to

i is determined only by each {X i, X j} pair, so Φ(X)j

Zi =

(cid:88)

1
δi(X)

j∈Si
exp{− (cid:107)X i − X j(cid:107)2

exp{−

(cid:107)X i − X j(cid:107)2
2,a
h2

}X j , ∀i,

(3)

j∈Si

where δi(X) = (cid:80)
2,a /h2} and Si is the set of indices in the neighborhood
of X i. Note that both a and h are constants, denoting the standard deviation of Gaussian kernel, and
the degree of ﬁltering, respectively [2]. It is noteworthy that the cardinality of Si for soft BM is much
larger than that of Ci for hard BM, which gives more ﬂexibility of using feature correlations between
neighboring locations.

The conventional non-local methods suffer from the drawback that parameters are either ﬁxed [2], or
obtained by suboptimal approaches [8, 28, 16], e.g., the parameters of WNNM are learned based on
the low-rankness assumption, which is suboptimal as the ultimate objective is to minimize the image
reconstruction error.

1In our analysis, if A is a matrix, Ai, Aj, and Aj

i denote its i-th row, j-th column, and the element at the

i-th row and j-th column, respectively.

3

(4)
(5)

(6)

Figure 1: An illustration of our non-local module working on a single location. The white tensor denotes
the deep feature representation of an entire image. The red ﬁber is the features of this location and the blue
tensor denotes the features in its neighborhood. θ, ψ and g are implemented by 1 × 1 convolution followed by
reshaping operations.

3.3 The Proposed Non-Local Module

Based on the general non-local framework in (1), we propose another soft block matching approach
and apply the Euclidean distance with linearly embedded Gaussian kernel [43] as the distance metric.
The linear embeddings are deﬁned as follows:

Φ(X)j

i = φ(X i, X j) = exp{θ(X i)ψ(X j)T } , ∀i, j ,

θ(X i) = X iW θ, ψ(X i) = X iW ψ, G(X)i = X iW g , ∀i .

The embedding transforms W θ, W φ, and W g are all learnable and have the shape of m × l, m ×
l, m × m, respectively. Thus, the proposed non-local operation can be written as

Zi =

(cid:88)

1
δi(X)

j∈Si

exp {X iW θW T

ψ X T

j } X iW g , ∀i ,

j∈Si

φ(X i, X j). Similar to [2], to obtain Zi, we evaluate the correlation between

where δi(X) = (cid:80)
X i and each X j in the neighborhood Si. More choices of φ(X i, X j) are discussed in Section 5.
The proposed non-local operation can be implemented by common differentiable operations, and thus
can be jointly learned when incorporated into a neural network. We wrap it as a non-local module
by adding a skip connection, as shown in Figure 1, since the skip connection enables us to insert a
non-local module into any pre-trained model, while maintaining its initial behavior by initializing
W g as zero. Such a module introduces only a limited number of parameters since θ, ψ and g are
1 × 1 convolutions and m = 128, l = 64 in practice. The output of this module on each location only
depends on its q × q neighborhood, so this operation can work on inputs of various sizes.

Relation to Other Methods: Recent works have combined non-local BM and neural networks
for image restoration [31, 24, 43]. Lefkimmiatis [24] proposed to ﬁrst apply BM to noisy image
patches. The hard BM results are used to group patch features, and a CNN conducts a trainable
collaborative ﬁltering over the matched patches. Qiao et al. [31] combined similar non-local BM
with TNRD networks [7] for image denoising. However, as conventional methods [8, 28, 16], these
works [24, 31] conduct hard BM directly over degraded input patches, which may be inaccurate over
severely degraded images. In contrast, our proposed non-local operation as soft BM is applied on
learned deep feature representations that are more robust to degradation. Furthermore, the matching
results in [24] are isolated from the neural network, similar to the conventional approaches, whereas
the proposed non-local module is trained jointly with the entire network in an end-to-end manner.

Wang et al. [43] used similar approaches to add non-local operations into neural networks for high-
level vision tasks. However, unlike our approach, Wang et al. [43] calculated feature correlations
throughout the whole image. which is equivalent to enlarging the neighborhood to the entire image in
our approach. We empirically show that increasing the neighborhood size does not always improve
image restoration performance, due to the inaccuracy of correlation estimation over degraded input
images. Hence it is imperative to choose a neighborhood of a proper size to achieve best performance
for image restoration. In addition, the non-local operation in [43] can only handle input images of
ﬁxed size, while our module in (6) is ﬂexible to various image sizes. Finally, our non-local module,
when incorporated into an RNN framework, allows the ﬂow of correlation information between
adjacent states to enhance robustness against inaccurate correlation estimation. This is a new unique
formulation to deal with degraded images. More details are provided next.

4

Figure 2: An illustration of the transition function
frecurrent in the proposed NLRN.

Figure 3: The operations for a single location i in the
non-local module used in NLRN.

4 Non-Local Recurrent Network
In this section, we describe the RNN architecture that incorporates the non-local module to form
our NLRN. We adopt the common formulation of an RNN, which consists of a set of states, namely,
input state, output state and recurrent state, as well as transition functions among the states. The
input, output, and recurrent states are represented as x, y and s respectively. At each time step t,
an RNN receives an input xt, and the recurrent state and the output state of the RNN are updated
recursively as follows:

st = finput(xt) + frecurrent(st−1),

yt = foutput(st),

(7)

where finput, foutput, and frecurrent are reused at every time step. In our NLRN, we set the following:

• s0 is a function of the input image I.
• xt = 0, ∀t ∈ {1, . . . , T }, and finput(0) = 0.
• The output state yt is calculated only at the time T as the ﬁnal output.

We add an identity path from the very ﬁrst state which helps gradient backpropagation during
training [37], and a residual path of the deep feature correlation between each location and its
corr}, and st = frecurrent(st−1, s0), ∀t ∈
neighborhood from the previous state. Hence, st = {st
{1, . . . , T }, where st
corr is the collection of deep feature
correlation. For the transition function frecurrent, a non-local module is ﬁrst adopted and is followed
by two convolutional layers, before the feature s0 is added from the identity path. The weights in the
non-local module are shared across recurrent states just as convolutional layers, so our NLRN still
keeps high parameter efﬁciency as a whole. An illustration is displayed in Figure 2.

feat denotes the feature map in time t and st

feat, st

It is noteworthy that inside the non-local module, the feature correlation for location i from the
previous state, st−1
corr,i, is added to the estimated feature correlation in the current state before the
softmax normalization, which enables the propagation of correlation information between adjacent
states for more robust correlation estimation. The details can be found in Figure 3. The initial
state s0 is set as the feature after a convolutional layer on the input image. foutput is represented
by another single convolutional layer. All layers have 128 ﬁlters with 3 × 3 kernel size except for
the non-local module. Batch normalization and ReLU activation function are performed ahead of
each convolutional layer following [19]. We adopt residual learning and the output of NLRN is the
residual image ˆI = foutput(sT ) when NLRN is unfolded T times. During training, the objective is to
2 || ˆI + I − ˜I||2, where ˜I denotes the ground truth image.
minimize the mean square error L( ˆI, ˜I) = 1
Relation to Other RNN Methods: Although RNNs have been adopted for image restoration before,
our NLRN is the ﬁrst to incorporate non-local operations into an RNN framework with correlation
propagation. DRCN [22] recursively applies a single convolutional layer to the input feature map
multiple times without the identity path from the ﬁrst state. DRRN [37] applies both the identity path
and the residual path in each state, but without non-local operations, and thus there is no correlation
information ﬂow across adjacent states. MemNet [38] builds dense connections among several types
of memory blocks, and weights are shared in the same type of memory blocks but are different across
various types. Compared with MemNet, our NLRN has an efﬁcient yet effective RNN structure with
shallower effective depth and fewer parameters, but obtains better restoration performance, which is
shown in Section 5 in detail.

5 Experiments

Dataset: For image denoising, we adopt two different settings to fairly and comprehensively compare
with recent deep learning based methods [29, 24, 51, 38]: (1) As in [7, 51, 24], we choose as the

5

training set the combination of 200 images from the train set and 200 images from the test set in the
Berkeley Segmentation Dataset (BSD) [30], and test on two popular benchmarks: Set12 and Set68
with σ = 15, 25, 50 following [51]. (2) As in [29, 38], we use as the training set the combination of
200 images from the train set and 100 images from the val set in BSD, and test on Set14 and the
BSD test set of 200 images with σ = 30, 50, 70 following [29, 38]. In addition, we evaluate our
NLRN on the Urban100 dataset [20], which contains abundant structural patterns and textures, to
further demonstrate the capability of using image self-similarity of our NLRN. The training set and
test set are strictly disjoint and all the images are converted to gray-scale in each experiment setup.
For image SR, we follow [21, 37, 38] and use a training set of 291 images where 91 images are
proposed in [48] and other 200 are from the BSD train set. We adopt four benchmark sets: Set5 [1],
Set14 [50], BSD100 [30] and Urban100 [20] for testing with three upscaling factors: ×2, ×3 and
×4. The low-resolution images are synthesized by bicubic downsampling.

Training Settings: We randomly sample patches whose size equals the neighborhood of non-local
operation from images during training. We use ﬂipping, rotation and scaling for augmenting training
data. For image denoising, we add independent and identically distributed Gaussian noise with zero
mean to the original image as the noisy input during training. We train a different model for each
noise level. For image SR, only the luminance channel of images is super-resolved, and the other two
color channels are upscaled by bicubic interpolation, following [21, 22, 37]. Moreover, the training
images for all three upscaling factors: ×2, ×3 and ×4 are upscaled by bicubic interpolation into the
desired spatial size and are combined into one training set. We use this set to train one single model
for all these three upscaling factors as in [21, 37, 38].

We use Adam optimizer to minimize the loss function. We set the initial learning rate as 1e-3 and
reduce it by half ﬁve times during training. We use Xavier initialization for the weights. We clip
the gradient at the norm of 0.5 to prevent the gradient explosion which is shown to empirically
accelerate training convergence, and we adopt 16 as the minibatch size during training. Training a
model takes about 3 days with a Titan Xp GPU. For non-local module, we use circular padding for
the neighborhood outside input patches. For convolution, we pad the boundaries of feature maps with
zeros to preserve the spatial size of feature maps.

5.1 Model Analysis

In this section, we analyze our model in the following aspects. First, we conduct the ablation study of
using different distance metrics in the non-local module. Table 1 compares instantiations including
Euclidean distance, dot product, embedded dot product, Gaussian, symmetric embedded Gaussian
and embedded Gaussian when used in NLRN of 12 unfolded steps. Embedded Gaussian achieves the
best performance and is adopted in the following experiments.

We compare the NLRN with its variants in terms of PSNR in Table 2. We have a few observations.
First, the same model with untied weights performs worse than its weight-sharing counter-part. We
speculate that the model with untied weights is prone to model over-ﬁtting and suffers much slower
training convergence, both of which undermine its performance. To investigate the function of non-
local modules, we implement a baseline RNN with the same parameter number of NLRN, and ﬁnd it
is worse than NLRN by about 0.2 dB, showing the advantage of using non-local image properties for
image restoration. Besides, we implement NLRNs where non-local module is used in every other
state or every three states, and observe that if the frequency of using non-local modules in NLRN
is reduced, the performance decreases accordingly. We show the beneﬁt of propagating correlation
information among adjacent states by comparing with the counter-part in terms of restoration accuracy.
To further analyze the non-local module, we visualize the feature correlation maps for non-local
operations in Figure 4. It can be seen that as the number of recurrent states increases, the locations

Table 1: Image denoising comparison of our proposed model
with various distance metrics on Set12 with noise level of 25.

Distance metric
Euclidean distance
Dot product
Embedded dot product
Gaussian
Symmetric embedded Gaussian
Embedded Gaussian

φ(X i, X j)

exp{− (cid:107)X i − X j(cid:107)2

2 /h2}

X iX T
j
θ(X i)ψ(X j)T
exp{X iX T
j }
exp{θ(X i)θ(X j)T }
exp{θ(X i)ψ(X j)T }

PSNR
30.74
30.68
30.75
30.69
30.76
30.80

Table 2: Image denoising comparison of our
NLRN with its variants on Set12 with noise
level of 25.

Model
NLRN w/o parameter sharing
RNN with same parameter no.
Non-local module in every other state
Non-local module in every 3 states
NLRN w/o propagating correlations
NLRN

PSNR
30.65
30.61
30.76
30.72
30.78
30.80

6

Figure 4: Examples of correlation maps of non-local operations for
image denoising. Noisy patch/ground truth patch: the neighborhood of
the red center pixel used in non-local operations. (1)-(6): the correlation
map for recurrent state 1-6 from NLRN with unrolling length of 6.

Figure 5: Neighborhood size vs.
image denoising performance of
our proposed model on Set12 with
noise level of 25.

Max effective depth
Parameter sharing
Parameter no.
Multi-view testing
Training images
PSNR

DnCNN
17
No
554k
No
400
27.18

RED MemNet
30
No
4,131k
Yes
300
27.33

80
Yes
667k
No
300
27.38

NLRN
38
Yes
330k
No
300
27.60

No
400
27.64

Yes
300
27.66

Table 3: Image denoising comparison of our proposed model with state-
of-the-art network models on Set12 with noise level of 50. Model com-
plexities are also compared.

Figure 6: Unrolling length vs.
image denoising performance of
our proposed model on Set12 with
noise level of 25.

with similar features progressively show higher correlations in the map, which demonstrates the
effectiveness of the non-local module for exploiting image self-similarity.

Figure 5 investigates the inﬂuence of the neighborhood size in the non-local module on image
denoising results. The performance peaks at q = 45. This shows that limiting the neighborhood
helps concentrate the correlation calculation on relevant features in the spatial vicinity and enhance
correlation estimation. Therefore, it is necessary to choose a proper neighborhood size (rather than
the whole image) for image restoration. We select q = 45 for the rest of this paper unless stated
otherwise.

The unrolling length T determines the maximum effective depth (i.e., maximum number of convolu-
tional layers) of NLRN. The inﬂuence of the unrolling length on image denoising results is shown in
Figure 6. The performance increases as the unrolling length rises, but gets saturated after T = 12.
Given the tradeoff between restoration accuracy and inference time, we adopt T = 12 for NLRN in
all the experiments.

5.2 Comparisons with State-of-the-Art Methods

We compare our proposed model with a number of recent competitors for image denoising and
image SR, respectively. PSNR and SSIM [44] are adopted for measuring quantitative restoration
performance.

Image Denoising: For a fair comparison with other methods based on deep networks, we train our
model under two settings: (1) We use the training data as in TNRD [7], DnCNN [51] and NLNet [24],
and the result is shown in Table 4. We cite the result of NLNet in the original paper [24], since no
public code or model is available. (2) We use the training data as in RED [29] and MemNet [38], and
the result is shown in Table 5. We note that RED uses multi-view testing [45] to boost the restoration
accuracy, i.e., RED processes each test image as well as its rotated and ﬂipped versions, and all
the outputs are then averaged to form the ﬁnal denoised image. Accordingly, we perform the same
procedure for NLRN and ﬁnd its performance, termed as NLRN-MV, is consistently improved. In
addition, we include recent non-deep-learning based methods: BM3D [8] and WNNM [16] in our
comparison. We do not list other methods [54, 3, 47, 6, 52] whose average performances are worse
than DnCNN or MemNet. Our NLRN signiﬁcantly outperforms all the competitors on Urban100 and
yields the best results across almost all the noise levels and datasets.

To further show the advantage of the network design of NLRN, we compare different versions of
NLRN with several state-of-the-art network models, i.e., DnCNN, RED and MemNet in Table 3.
NLRN uses the fewest parameters but outperforms all the competitors. Speciﬁcally, NLRN beneﬁts

7

Table 4: Benchmark image denoising results. Training and testing protocols are followed as in [51]. Average
PSNR/SSIM for various noise levels on Set12, BSD68 and Urban100. The best performance is in bold.

Dataset

Set12

BSD68

Urban100

Noise
15
25
50
15
25
50
15
25
50

BM3D
32.37/0.8952
29.97/0.8504
26.72/0.7676
31.07/0.8717
28.57/0.8013
25.62/0.6864
32.35/0.9220
29.70/0.8777
25.95/0.7791

WNNM
32.70/0.8982
30.28/0.8557
27.05/0.7775
31.37/0.8766
28.83/0.8087
25.87/0.6982
32.97/0.9271
30.39/0.8885
26.83/0.8047

TNRD
32.50/0.8958
30.06/0.8512
26.81/0.7680
31.42/0.8769
28.92/0.8093
25.97/0.6994
31.86/0.9031
29.25/0.8473
25.88/0.7563

NLNet
-/-
-/-
-/-
31.52/-
29.03/-
26.07/-
-/-
-/-
-/-

DnCNN
32.86/0.9031
30.44/0.8622
27.18/0.7829
31.73/0.8907
29.23/0.8278
26.23/0.7189
32.68/0.9255
29.97/0.8797
26.28/0.7874

NLRN
33.16/0.9070
30.80/0.8689
27.64/0.7980
31.88/0.8932
29.41/0.8331
26.47/0.7298
33.45/0.9354
30.94/0.9018
27.49/0.8279

Table 5: Benchmark image denoising results. Training and testing protocols are followed as in [38]. Average
PSNR/SSIM for various noise levels on 14 images, BSD200 and Urban100. Red is the best and blue is the
second best performance.

Dataset

14 images

BSD200

Urban100

Noise
30
50
70
30
50
70
30
50
70

BM3D
28.49/0.8204
26.08/0.7427
24.65/0.6882
27.31/0.7755
25.06/0.6831
23.82/0.6240
28.75/0.8567
25.95/0.7791
24.27/0.7163

WNNM
28.74/0.8273
26.32/0.7517
24.80/0.6975
27.48/0.7807
25.26/0.6928
23.95/0.6346
29.47/0.8697
26.83/0.8047
25.11/0.7501

RED
29.17/0.8423
26.81/0.7733
25.31/0.7206
27.95/0.8056
25.75/0.7167
24.37/0.6551
29.12/0.8674
26.44/0.7977
24.75/0.7415

MemNet
29.22/0.8444
26.91/0.7775
25.43/0.7260
28.04/0.8053
25.86/0.7202
24.53/0.6608
29.10/0.8631
26.65/0.8030
25.01/0.7496

NLRN
29.37/0.8460
27.00/0.7777
25.49/0.7255
28.15/0.8423
25.93/0.7214
24.58/0.6614
29.94/0.8830
27.38/0.8241
25.66/0.7707

NLRN-MV
29.41/0.8472
27.05/0.7791
25.54/0.7273
28.20/0.8436
25.97/0.8429
24.62/0.6634
29.99/0.8842
27.43/0.8256
25.71/0.7724

Table 6: Benchmark SISR results. Average PSNR/SSIM for scale factor ×2, ×3 and ×4 on datasets Set5, Set14,
BSD100 and Urban100. The best performance is in bold.

Dataset

Set5

Set14

BSD100

Urban100

Scale
×2
×3
×4
×2
×3
×4
×2
×3
×4
×2
×3
×4

SRCNN
36.66/0.9542
32.75/0.9090
30.48/0.8628
32.45/0.9067
29.30/0.8215
27.50/0.7513
31.36/0.8879
28.41/0.7863
26.90/0.7101
29.50/0.8946
26.24/0.7989
24.52/0.7221

VDSR
37.53/0.9587
33.66/0.9213
31.35/0.8838
33.03/0.9124
29.77/0.8314
28.01/0.7674
31.90/0.8960
28.82/0.7976
27.29/0.7251
30.76/0.9140
27.14/0.8279
25.18/0.7524

DRCN
37.63/0.9588
33.82/0.9226
31.53/0.8854
33.04/0.9118
29.76/0.8311
28.02/0.7670
31.85/0.8942
28.80/0.7963
27.23/0.7233
30.75/0.9133
27.15/0.8276
25.14/0.7510

LapSRN
37.52/0.959
33.82/0.923
31.54/0.885
33.08/0.913
29.79/0.832
28.19/0.772
31.80/0.895
28.82/0.797
27.32/0.728
30.41/0.910
27.07/0.827
25.21/0.756

DRRN
37.74/0.9591
34.03/0.9244
31.68/0.8888
33.23/0.9136
29.96/0.8349
28.21/0.7721
32.05/0.8973
28.95/0.8004
27.38/0.7284
31.23/0.9188
27.53/0.8378
25.44/0.7638

MemNet
37.78/0.9597
34.09/0.9248
31.74/0.8893
33.28/0.9142
30.00/0.8350
28.26/0.7723
32.08/0.8978
28.96/0.8001
27.40/0.7281
31.31/0.9195
27.56/0.8376
25.50/0.7630

NLRN
38.00/0.9603
34.27/0.9266
31.92/0.8916
33.46/0.9159
30.16/0.8374
28.36/0.7745
32.19/0.8992
29.06/0.8026
27.48/0.7306
31.81/0.9249
27.93/0.8453
25.79/0.7729

from inherent parameter sharing and uses only less than 1/10 parameters of RED. Compared with the
RNN competitor, MemNet, NLRN uses only half of parameters and much shallower depth to obtain
better performance, which shows the superiority of our non-local recurrent architecture.

Image Super-Resolution: We compare our model with several recent SISR approaches, including
SRCNN [10], VDSR [21], DRCN [22], LapSRN [23], DRRN [37] and MemNet [38] in Table 6. We
crop pixels near image borders before calculating PSNR and SSIM as in [10, 35, 21, 22]. We do
not list other methods [20, 35, 26, 36, 17] since their performances are worse than that of DRRN or
MemNet. Besides, we do not include SRDenseNet [41] and EDSR [25] in the comparison because
the number of parameters in these two network models is over two orders of magnitude larger than
that of our NLRN and their training datasets are signiﬁcantly larger than ours. It can be seen that
NLRN yields the best result across all the upscaling factors and datasets. Visual results are provided
in Section 7.

6 Conclusion

We have presented a new and effective recurrent network that incorporates non-local operations for
image restoration. The proposed non-local module can be trained end-to-end with the recurrent
network. We have studied the importance of computing reliable feature correlations within a conﬁned
neighorhood against the whole image, and have shown the beneﬁts of passing feature correlation
messages between adjacent recurrent stages. Comprehensive evaluations over benchmarks for image
denoising and super-resolution demonstrate the superiority of NLRN over existing methods.

8

7 Appendix

7.1 Extension of the General Framework to Other Classic Non-Local Methods

Besides the extension to WNNM and non-local means, which are discussed in Section 3.1, we show
the proposed non-local framework (1) can be extended to collaborative ﬁltering methods, e.g., BM3D
algorithm [8], as well as joint sparsity based methods, e.g., LSSC algorithm [28]. We follow the same
notations in Section 3.1. Both BM3D and LSSC apply block matching (BM) ﬁrst before processing,
and form N groups of similar patches into data matrices. The index set of the matched patches for
the i-th reference patch is denoted as Ci. The group of matched patches for the i-th reference patch is
denoted as X Ci.
Similar to WNNM [16], BM3D [8] also applies BM ﬁrst to group similar patches based on their
Euclidean distances. The matched patches are then processed via Wiener ﬁltering [8], and the
denoised results of the i-th group of patches are

ZCi = τ −1(diag(ω)τ (X Ci)).

(8)

Here τ (·) and τ −1(·) denote the forward and backward Wiener ﬁltering applied to the groups of
matched patches, respectively. The diagonal matrix diag(ω) is formed by the empirical Wiener
coefﬁcients ω. BM3D applies data pre-cleaning, using discrete cosine transform (DCT), to estimate
the original patch, and calculate the estimate of ω [8]. Since calculating ZCi in (8) involves only
linear ﬁltering, it can also be generalized using the proposed non-local framework as (2). Unlike the
extension to WNNM, here (cid:80)
i G(X)j corresponds to the denoised results via Wiener
ﬁltering as shown in (8), of the i-th group of matched patches.

Φ(X)j

j∈Ci

Different from BM3D and WNNM, LSSC learns a common dictionary D for all image patches, and
imposes joint sparsity [28] on each data matrix of matched patches X Ci, so that the correlation of
the matched patches are exploited by enforcing the same support of their sparse codes. Thus, the
joint sparse coding in LSSC [28] becomes

ˆAi = argminAi (cid:107)Ai(cid:107)0,∞ s.t.

− DAi

≤ (cid:15) |Ci| , ∀i ,

(9)

(cid:13)
(cid:13)X T
(cid:13)
Ci

(cid:13)
2
(cid:13)
(cid:13)

F

where the (0, ∞) “norm” (cid:107)·(cid:107)0,∞ counts the number of non-zero columns of each sparse code matrix
Ai [28], and |Ci| is the cardinality of Ci. The coefﬁcient (cid:15) is a constant, which is used to upper bound
the sparse modeling errors. In general, the solution to (9) is NP-hard. To simplify the discussion, we
assume the dictionary to be unitary (which reduces the sparse coding problem to the transform-model
sparse coding [46]), i.e., DT D = I and D ∈ Rk×k. Thus there exists a corresponding shrinkage
function η(·) for imposing joint sparsity on the sparse codes [28, 34], such that the denoised estimates
T
of the i-th patch group can be obtained as ZCi = ˆA
i DT = η( X Ci D ) DT . Though joint sparse
coding projects all data onto a union of subspaces [28, 12, 46] which is a non-linear operation in
general, each data matrix X Ci is projected onto one particular subspace spanned by the selected
atoms corresponding to the non-zero columns in ˆAi, which is locally linear. For the i-th group of
patches, such a subspace projection corresponds to (cid:80)
i G(X)j in the proposed general
framework.

Φ(X)j

j∈Ci

7.2 Visual Results

We show the visual comparison of our NLRN and several competing methods: BM3D [8],
WNNM [16], and MemNet [38] for image denoising in Figure 7. Our method can recover more
details from the noisy measurement. The visual comparison of our NLRN and several recent methods:
DRCN [22], LapSRN [23], DRRN [37], and MemNet [38] for image super-resolution is displayed in
Figure 8. Our method is able to reconstruct sharper edges and produce fewer artifacts especially in
the regions of repetitive patterns.

9

Noisy (18.75/0.3232)

BM3D (29.13/0.8261)

WNNM (29.30/0.8334)

Ground truth

MemNet (29.18/0.8223)

NLRN (29.53/0.8369)

Ground truth (PSNR/SSIM)

Noisy (19.49/0.5099)

BM3D (28.95/0.9062)

WNNM (30.44/0.9260)

Ground truth

MemNet (28.71/0.8906)

NLRN (30.52/0.9267)

Ground truth (PSNR/SSIM)

Noisy (19.39/0.4540)

BM3D (27.20/0.8775)

WNNM (28.86/0.8913)

Ground truth

MemNet (27.55/0.8807)

NLRN (29.04/0.9044)

Ground truth (PSNR/SSIM)

Noisy (19.06/0.3005)

BM3D (29.61/0.8304)

WNNM (30.59/0.8543)

Ground truth

MemNet (30.21/0.8517)

NLRN (31.17/0.8727)

Ground truth (PSNR/SSIM)

Noisy (18.88/0.3479)

BM3D (28.82/0.9051)

WNNM (28.38/0.9189)

Ground truth

MemNet (28.31/0.9112)

NLRN (28.42/0.9308)

Ground truth (PSNR/SSIM)

Figure 7: Qualitative comparison of image denoising results with noise level of 30. The zoom-in region in the
red bounding box is shown on the right. From top to bottom: 1) the image barbara. 2) image 004 in Urban100.
3) image 019 in Urban100. 4) image 033 in Urban100. 5) image 046 in Urban100.

10

DRCN (26.82/0.9329)

LapSRN (26.52/0.9316)

DRRN (27.52/0.9434)

Ground truth HR

MemNet (27.78/0.9451)

NLRN (28.46/0.9513)

HR (PSNR/SSIM)

DRCN (20.95/0.7716)

LapSRN (20.90/0.7722)

DRRN (21.37/0.7874)

Ground truth HR

MemNet (21.35/0.7877)

NLRN (21.92/0.8014)

HR (PSNR/SSIM)

DRCN (30.18/0.8306)

LapSRN (30.29/0.8388)

DRRN (30.18/0.8306)

Ground truth HR

MemNet (29.25/0.8347)

NLRN (31.19/0.8598)

HR (PSNR/SSIM)

DRCN (20.71/0.7466)

LapSRN (20.86/0.7524)

DRRN (20.92/0.7666)

Ground truth HR

MemNet (21.06/0.7716)

NLRN (21.41/0.7866)

HR (PSNR/SSIM)

DRCN (23.99/0.6940)

LapSRN (24.49/0.7247)

DRRN (25.14/0.7469)

Ground truth HR

MemNet (25.19/0.7519)

NLRN (25.97/0.7882)

HR (PSNR/SSIM)

Figure 8: Qualitative comparison of image super-resolution results with ×4 upscaling. The zoom-in region in
the red bounding box is shown on the right. From top to bottom: 1) image 005 in Urban100. 2) image 019 in
Urban100. 3) image 044 in Urban100. 4) image 062 in Urban100. 5) image 099 in Urban100.

References

[1] M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. Alberi-Morel. Low-complexity single-image super-

resolution based on nonnegative neighbor embedding. 2012.

11

[2] A. Buades, B. Coll, and J.-M. Morel. A non-local algorithm for image denoising. In CVPR, 2005.
[3] H. C. Burger, C. J. Schuler, and S. Harmeling. Image denoising: Can plain neural networks compete with

[4] S. Chandra, N. Usunier, and I. Kokkinos. Dense and low-rank gaussian crfs using deep embeddings. In

[5] H. Chang, D.-Y. Yeung, and Y. Xiong. Super-resolution through neighbor embedding. In CVPR, 2004.
[6] F. Chen, L. Zhang, and H. Yu. External patch prior guided internal clustering for image denoising. In

bm3d? In CVPR, 2012.

ICCV, 2017.

ICCV, 2015.

[7] Y. Chen and T. Pock. Trainable nonlinear reaction diffusion: A ﬂexible framework for fast and effective

[8] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-d transform-domain

[9] A. Danielyan, V. Katkovnik, and K. Egiazarian. Bm3d frames and variational image deblurring. TIP,

image restoration. IEEE TPAMI, 2017.

collaborative ﬁltering. IEEE TIP, 2007.

21(4):1715–1728, 2012.

[10] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep convolutional network for image super-resolution.

[11] C. Dong, C. C. Loy, and X. Tang. Accelerating the super-resolution convolutional neural network. In

In ECCV, 2014.

ECCV, 2016.

IEEE TIP, 2006.

Graphics (TOG), 2011.

learning. In ICML, 2017.

[12] M. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned dictionaries.

[13] G. Freedman and R. Fattal. Image and video upscaling from local self-examples. ACM Transactions on

[14] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to sequence

[15] D. Glasner, S. Bagon, and M. Irani. Super-resolution from a single image. In ICCV, 2009.
[16] S. Gu, L. Zhang, W. Zuo, and X. Feng. Weighted nuclear norm minimization with application to image

denoising. In CVPR, pages 2862–2869, 2014.

[17] W. Han, S. Chang, D. Liu, M. Yu, M. Witbrock, and T. S. Huang. Image super-resolution via dual-state

[18] A. W. Harley, K. G. Derpanis, and I. Kokkinos. Segmentation-aware convolutional networks using local

recurrent networks. In CVPR, June 2018.

attention masks. In ICCV, 2017.

[19] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, 2016.
[20] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-resolution from transformed self-exemplars. In

[21] J. Kim, J. Kwon Lee, and K. Mu Lee. Accurate image super-resolution using very deep convolutional

CVPR, 2015.

networks. In CVPR, 2016.

In CVPR, 2016.

super-resolution. In CVPR, 2017.

[22] J. Kim, J. Kwon Lee, and K. Mu Lee. Deeply-recursive convolutional network for image super-resolution.

[23] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang. Deep laplacian pyramid networks for fast and accurate

[24] S. Lefkimmiatis. Non-local color image denoising with convolutional neural networks. In CVPR, 2017.
[25] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced deep residual networks for single image

super-resolution. In CVPR Workshops, 2017.

[26] D. Liu, Z. Wang, B. Wen, J. Yang, W. Han, and T. S. Huang. Robust single image super-resolution via

deep networks with sparse prior. TIP, 25(7):3194–3207, 2016.

[27] D. Liu, B. Wen, X. Liu, Z. Wang, and T. S. Huang. When image denoising meets high-level vision tasks:

A deep learning approach. In IJCAI, 2018.

[28] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Non-local sparse models for image restoration.

In ICCV, 2009.

[29] X. Mao, C. Shen, and Y.-B. Yang. Image restoration using very deep convolutional encoder-decoder

networks with symmetric skip connections. In NIPS, 2016.

[30] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its

application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, 2001.

[31] P. Qiao, Y. Dou, W. Feng, R. Li, and Y. Chen. Learning non-local image diffusion for image denoising. In

ACM on Multimedia Conference, 2017.

[32] L. I. Rudin and S. Osher. Total variation based image restoration with free local constraints. In ICIP, 1994.
[33] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lillicrap. A simple

neural network module for relational reasoning. In NIPS, 2017.

[34] U. Schmidt and S. Roth. Shrinkage ﬁelds for effective image restoration. In CVPR, 2014.
[35] S. Schulter, C. Leistner, and H. Bischof. Fast and accurate image upscaling with super-resolution forests.

In CVPR, 2015.

12

[36] W. Shi, J. Caballero, F. Huszár, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-time
single image and video super-resolution using an efﬁcient sub-pixel convolutional neural network. In
CVPR, 2016.

[37] Y. Tai, J. Yang, and X. Liu. Image super-resolution via deep recursive residual network. In CVPR, 2017.
[38] Y. Tai, J. Yang, X. Liu, and C. Xu. Memnet: A persistent memory network for image restoration. In ICCV,

[39] R. Timofte, V. De, and L. Van Gool. Anchored neighborhood regression for fast example-based super-

resolution. In ICCV, 2013.

[40] C. Tomasi and R. Manduchi. Bilateral ﬁltering for gray and color images. In ICCV, 1998.
[41] T. Tong, G. Li, X. Liu, and Q. Gao. Image super-resolution using dense skip connections. In ICCV, 2017.
[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.

Attention is all you need. In NIPS, 2017.

[43] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. arXiv preprint arXiv:1711.07971,

[44] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility

[45] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang. Deep networks for image super-resolution with sparse

to structural similarity. IEEE TIP, 2004.

prior. In ICCV, 2015.

[46] B. Wen, S. Ravishankar, and Y. Bresler. Structured overcomplete sparsifying transform learning with

convergence guarantees and applications. IJCV, 2015.

[47] J. Xu, L. Zhang, W. Zuo, D. Zhang, and X. Feng. Patch group based nonlocal self-similarity prior learning

for image denoising. In ICCV, 2015.

[48] J. Yang, J. Wright, T. S. Huang, and Y. Ma. Image super-resolution via sparse representation. IEEE TIP,

[49] R. Yin, T. Gao, Y. M. Lu, and I. Daubechies. A tale of two bases: Local-nonlocal regularization on image

patches with convolution framelets. SIAM Journal on Imaging Sciences, 10(2):711–750, 2017.

[50] R. Zeyde, M. Elad, and M. Protter. On single image scale-up using sparse-representations. In International

[51] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Beyond a gaussian denoiser: Residual learning of

conference on curves and surfaces, 2010.

deep cnn for image denoising. IEEE TIP, 2017.

[52] K. Zhang, W. Zuo, S. Gu, and L. Zhang. Learning deep cnn denoiser prior for image restoration. In CVPR,

[53] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and P. H. Torr. Conditional

random ﬁelds as recurrent neural networks. In ICCV, 2015.

[54] D. Zoran and Y. Weiss. From learning models of natural image patches to whole image restoration. In

ICCV, 2011.

2017.

2017.

2010.

2017.

13

8
1
0
2
 
c
e
D
 
1
1
 
 
]

V
C
.
s
c
[
 
 
2
v
9
1
9
2
0
.
6
0
8
1
:
v
i
X
r
a

Non-Local Recurrent Network for Image Restoration

Ding Liu1, Bihan Wen1, Yuchen Fan1, Chen Change Loy2, Thomas S. Huang1
1University of Illinois at Urbana-Champaign 2Nanyang Technological University
{dingliu2, bwen3, yuchenf4, t-huang1}@illinois.edu ccloy@ntu.edu.sg

Abstract

Many classic methods have shown non-local self-similarity in natural images
to be an effective prior for image restoration. However, it remains unclear and
challenging to make use of this intrinsic property via deep networks.
In this
paper, we propose a non-local recurrent network (NLRN) as the ﬁrst attempt to
incorporate non-local operations into a recurrent neural network (RNN) for image
restoration. The main contributions of this work are: (1) Unlike existing methods
that measure self-similarity in an isolated manner, the proposed non-local module
can be ﬂexibly integrated into existing deep networks for end-to-end training to
capture deep feature correlation between each location and its neighborhood. (2)
We fully employ the RNN structure for its parameter efﬁciency and allow deep
feature correlation to be propagated along adjacent recurrent states. This new design
boosts robustness against inaccurate correlation estimation due to severely degraded
images. (3) We show that it is essential to maintain a conﬁned neighborhood for
computing deep feature correlation given degraded images. This is in contrast to
existing practice [43] that deploys the whole image. Extensive experiments on both
image denoising and super-resolution tasks are conducted. Thanks to the recurrent
non-local operations and correlation propagation, the proposed NLRN achieves
superior results to state-of-the-art methods with many fewer parameters. The code
is available at https://github.com/Ding-Liu/NLRN.

1

Introduction

Image restoration is an ill-posed inverse problem that aims at estimating the underlying image from its
degraded measurements. Depending on the type of degradation, image restoration can be categorized
into different sub-problems, e.g., image denoising and image super-resolution (SR). The key to
successful restoration typically relies on the design of an effective regularizer based on image priors.
Both local and non-local image priors have been extensively exploited in the past. Considering
image denoising as an example, local image properties such as Gaussian ﬁltering and total variation
based methods [32] are widely used in early studies. Later on, the notion of self-similarity in natural
images draws more attention and it has been exploited by non-local-based methods, e.g., non-local
means [2], collaborative ﬁltering [8], joint sparsity [28], and low-rank modeling [16]. These non-local
methods are shown to be effective in capturing the correlation among non-local patches to improve
the restoration quality.

While non-local self-similarity has been extensively studied in the literature, approaches for capturing
this intrinsic property with deep networks are little explored. Recent convolutional neural networks
(CNNs) for image restoration [10, 21, 29, 51] achieve impressive performance over conventional
approaches but do not explicitly use self-similarity properties in images. To rectify this weakness, a
few studies [24, 31] apply block matching to patches before feeding them into CNNs. Nevertheless,
the block matching step is isolated and thus not jointly trained with image restoration networks.

In this paper, we present the ﬁrst attempt to incorporate non-local operations in CNN for image
restoration, and propose a non-local recurrent network (NLRN) as an efﬁcient yet effective network

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

with non-local module. First, we design a non-local module to produce reliable feature correlation
for self-similarity measurement given severely degraded images, which can be ﬂexibly integrated
into existing deep networks while embracing the beneﬁt of end-to-end learning. For high parameter
efﬁciency without compromising restoration quality, we deploy a recurrent neural network (RNN)
framework similar to [22, 37, 38] such that operations with shared weights are applied recursively.
Second, we carefully study the behavior of non-local operation in deep feature space and ﬁnd that
limiting the neighborhood of correlation computation improves its robustness to degraded images. The
conﬁned neighborhood helps concentrate the computation on relevant features in the spatial vicinity
and disregard noisy features, which is in line with conventional image restoration approaches [8, 16].
In addition, we allow message passing of non-local operations between adjacent recurrent states of
RNN. Such inter-state ﬂow of feature correlation facilitates more robust correlation estimation. By
combining the non-local operation with typical convolutions, our NLRN can effectively capture and
employ both local and non-local image properties for image restoration.

It is noteworthy that recent work has adopted similar ideas on video classiﬁcation [43]. However,
our method signiﬁcantly differs from it in the following aspects. For each location, we measure the
feature correlation of each location only in its neighborhood, rather than throughout the whole image
as in [43]. In our experiments, we show that deep features useful for computing non-local priors
are more likely to reside in neighboring regions. A larger neighborhood (the whole image as one
extreme) can lead to inaccurate correlation estimation over degraded measurements. In addition, our
method fully exploits the advantage of RNN architecture - the correlation information is propagated
among adjacent recurrent states to increase the robustness of correlation estimation to degradations of
various degrees. Moreover, our non-local module is ﬂexible to handle inputs of various sizes, while
the module in [43] handles inputs of ﬁxed sizes only.

We introduce NLRN by ﬁrst relating our proposed model to other classic and existing non-local
image restoration approaches in a uniﬁed framework. We thoroughly analyze the non-local module
and recurrent architecture in our NLRN via extensive ablation studies. We provide a comprehensive
comparison with recent competitors, in which our NLRN achieves state-of-the-art performance
in image denoising and SR over several benchmark datasets, demonstrating the superiority of the
non-local operation with recurrent architecture for image restoration.

2 Related Work

Image self-similarity as an important image characteristic has been used in a number of non-local-
based image restoration approaches. The early works include bilateral ﬁltering [40] and non-local
means [2] for image denoising. Recent approaches exploit image self-similarity by imposing spar-
sity [28, 46]. Alternatively, similar image patches are modeled with low-rankness [16], or by
collaborative Wiener ﬁltering [8, 49]. Neighborhood embedding is a common approach for image
SR [5, 39], in which each image patch is approximated by multiple similar patches in a manifold.
Self-example based image SR approaches [15, 13] exploit the local self-similarity assumption, and
extract LR-HR exemplar pairs merely from the low-resolution image across different scales to predict
the high-resolution image. Similar ideas are adopted for image deblurring [9].

Deep neural networks have been prevalent for image restoration. The pioneering works include a
multilayer perceptron for image denoising [3] and a three-layer CNN for image SR [10]. Deconvolu-
tion is adopted to save computation cost and accelerate inference speed [36, 11]. Very deep CNNs are
designed to boost SR accuracy in [21, 23, 25]. Dense connections among various residual blocks are
included in [41]. Similarly CNN based methods are developed for image denoising in [29, 51, 52, 27].
Block matching as a preprocessing step is cascaded with CNNs for image denoising [24, 31]. Be-
sides CNNs, RNNs have also been applied for image restoration while enjoying the high parameter
efﬁciency [22, 37, 38].

In addition to image restoration, feature correlations are widely exploited along with neural networks
in many other areas, including graphical models [53, 4, 18], relational reasoning [33], machine
translation [14, 42] and so on. We do not elaborate on them here due to the limitation of space.

3 Non-Local Operations for Image Restoration

In this section, we ﬁrst present a uniﬁed framework of non-local operations used for image restoration
methods, e.g., collaborative ﬁltering [8], non-local means [2], and low-rank modeling [16], and we
discuss the relations between them. We then present the proposed non-local operation module.

2

3.1 A General Framework

In general, a non-local operation takes a multi-channel input X ∈ RN ×m as the image feature, and
generates output feature Z ∈ RN ×k. Here N and m denote the number of image pixels and data
channels, respectively. We propose a general framework with the following formulation:

Z = diag{δ(X)}−1 Φ(X) G(X) .
(1)
Here, Φ(X) ∈ RN ×N is the non-local correlation matrix, and G(X) ∈ RN ×k is the multi-channel
non-local transform. Each row vector X i denotes the local features in location i. Φ(X)j
i represents
the relationship between the X i and X j, and each row vector G(X)j is the embedding of X j.1 The
diagonal matrix diag{δ(X)} ∈ RN ×N normalizes the output at each i-th pixel with normalization
factor δi(X).

3.2 Classic Methods

The proposed framework works with various classic non-local methods for image restoration, includ-
ing methods based on low-rankness [16], collaborative ﬁltering [8], joint sparsity [28], as well as
non-local mean ﬁltering [2].

Block matching (BM) is a commonly used approach for exploiting non-local image structures
in conventional methods [16, 8, 28]. A q × q spatial neighborhood is set to be centered at each
location i, and X i reduces to the image patch centered at i. BM selects the Ki most similar patches
(Ki (cid:28) q2) from this neighborhood, which are used jointly to restore X i. Under the proposed
non-local framework, these methods can be represented as

Zi =

(cid:88)

1
δi(X)

j∈Ci

Φ(X)j

i G(X)j , ∀i .

(2)

j∈Ci

Φ(X)j

Here δi(X) = (cid:80)
i and Ci denotes the set of indices of the Ki selected patches. Thus, each
row Φ(X)i has only Ki non-zero entries. The embedding G(X) and the non-zero elements vary for
non-local methods based on different models. For example, in WNNM [16], (cid:80)
i G(X)j
corresponds to the projection of X i onto the group-speciﬁc subspace as a function of the selected
patches. Speciﬁcally, the subspace for calculating Zi is spanned by the eigenvectors U i of X T
X Ci .
Ci
Thus Zi = X CiU idiag{σ}U T
i , where diag{σ} is obtained by applying the shrinkage function
associated with the weighted nuclear norm [16] to the eigenvalues of X T
X Ci. We show the
Ci
generalization about more classic non-local image restoration methods in Section 7.

Φ(X)j

j∈Ci

Except for the hard block matching, other methods, e.g., the non-local means algorithm [2], apply
soft block matching by calculating the correlation between the reference patch and each patch in
the neighborhood. Each element Φ(X)j
i =
φ(X i, X j), where φ( · ) is determined by the distance metric. In [2], weighted Euclidean distance
with Gaussian kernel is applied as the metric, such that φ(X i, X j) = exp{− (cid:107)X i − X j(cid:107)2
2,a /h2}.
Besides, identity mapping is directly used as the embedding in [2], i.e., G(X)j = X j. In this case,
the non-local framework in (1) reduces to

i is determined only by each {X i, X j} pair, so Φ(X)j

Zi =

(cid:88)

1
δi(X)

j∈Si
exp{− (cid:107)X i − X j(cid:107)2

exp{−

(cid:107)X i − X j(cid:107)2
2,a
h2

}X j , ∀i,

(3)

j∈Si

where δi(X) = (cid:80)
2,a /h2} and Si is the set of indices in the neighborhood
of X i. Note that both a and h are constants, denoting the standard deviation of Gaussian kernel, and
the degree of ﬁltering, respectively [2]. It is noteworthy that the cardinality of Si for soft BM is much
larger than that of Ci for hard BM, which gives more ﬂexibility of using feature correlations between
neighboring locations.

The conventional non-local methods suffer from the drawback that parameters are either ﬁxed [2], or
obtained by suboptimal approaches [8, 28, 16], e.g., the parameters of WNNM are learned based on
the low-rankness assumption, which is suboptimal as the ultimate objective is to minimize the image
reconstruction error.

1In our analysis, if A is a matrix, Ai, Aj, and Aj

i denote its i-th row, j-th column, and the element at the

i-th row and j-th column, respectively.

3

(4)
(5)

(6)

Figure 1: An illustration of our non-local module working on a single location. The white tensor denotes
the deep feature representation of an entire image. The red ﬁber is the features of this location and the blue
tensor denotes the features in its neighborhood. θ, ψ and g are implemented by 1 × 1 convolution followed by
reshaping operations.

3.3 The Proposed Non-Local Module

Based on the general non-local framework in (1), we propose another soft block matching approach
and apply the Euclidean distance with linearly embedded Gaussian kernel [43] as the distance metric.
The linear embeddings are deﬁned as follows:

Φ(X)j

i = φ(X i, X j) = exp{θ(X i)ψ(X j)T } , ∀i, j ,

θ(X i) = X iW θ, ψ(X i) = X iW ψ, G(X)i = X iW g , ∀i .

The embedding transforms W θ, W φ, and W g are all learnable and have the shape of m × l, m ×
l, m × m, respectively. Thus, the proposed non-local operation can be written as

Zi =

(cid:88)

1
δi(X)

j∈Si

exp {X iW θW T

ψ X T

j } X iW g , ∀i ,

j∈Si

φ(X i, X j). Similar to [2], to obtain Zi, we evaluate the correlation between

where δi(X) = (cid:80)
X i and each X j in the neighborhood Si. More choices of φ(X i, X j) are discussed in Section 5.
The proposed non-local operation can be implemented by common differentiable operations, and thus
can be jointly learned when incorporated into a neural network. We wrap it as a non-local module
by adding a skip connection, as shown in Figure 1, since the skip connection enables us to insert a
non-local module into any pre-trained model, while maintaining its initial behavior by initializing
W g as zero. Such a module introduces only a limited number of parameters since θ, ψ and g are
1 × 1 convolutions and m = 128, l = 64 in practice. The output of this module on each location only
depends on its q × q neighborhood, so this operation can work on inputs of various sizes.

Relation to Other Methods: Recent works have combined non-local BM and neural networks
for image restoration [31, 24, 43]. Lefkimmiatis [24] proposed to ﬁrst apply BM to noisy image
patches. The hard BM results are used to group patch features, and a CNN conducts a trainable
collaborative ﬁltering over the matched patches. Qiao et al. [31] combined similar non-local BM
with TNRD networks [7] for image denoising. However, as conventional methods [8, 28, 16], these
works [24, 31] conduct hard BM directly over degraded input patches, which may be inaccurate over
severely degraded images. In contrast, our proposed non-local operation as soft BM is applied on
learned deep feature representations that are more robust to degradation. Furthermore, the matching
results in [24] are isolated from the neural network, similar to the conventional approaches, whereas
the proposed non-local module is trained jointly with the entire network in an end-to-end manner.

Wang et al. [43] used similar approaches to add non-local operations into neural networks for high-
level vision tasks. However, unlike our approach, Wang et al. [43] calculated feature correlations
throughout the whole image. which is equivalent to enlarging the neighborhood to the entire image in
our approach. We empirically show that increasing the neighborhood size does not always improve
image restoration performance, due to the inaccuracy of correlation estimation over degraded input
images. Hence it is imperative to choose a neighborhood of a proper size to achieve best performance
for image restoration. In addition, the non-local operation in [43] can only handle input images of
ﬁxed size, while our module in (6) is ﬂexible to various image sizes. Finally, our non-local module,
when incorporated into an RNN framework, allows the ﬂow of correlation information between
adjacent states to enhance robustness against inaccurate correlation estimation. This is a new unique
formulation to deal with degraded images. More details are provided next.

4

Figure 2: An illustration of the transition function
frecurrent in the proposed NLRN.

Figure 3: The operations for a single location i in the
non-local module used in NLRN.

4 Non-Local Recurrent Network
In this section, we describe the RNN architecture that incorporates the non-local module to form
our NLRN. We adopt the common formulation of an RNN, which consists of a set of states, namely,
input state, output state and recurrent state, as well as transition functions among the states. The
input, output, and recurrent states are represented as x, y and s respectively. At each time step t,
an RNN receives an input xt, and the recurrent state and the output state of the RNN are updated
recursively as follows:

st = finput(xt) + frecurrent(st−1),

yt = foutput(st),

(7)

where finput, foutput, and frecurrent are reused at every time step. In our NLRN, we set the following:

• s0 is a function of the input image I.
• xt = 0, ∀t ∈ {1, . . . , T }, and finput(0) = 0.
• The output state yt is calculated only at the time T as the ﬁnal output.

We add an identity path from the very ﬁrst state which helps gradient backpropagation during
training [37], and a residual path of the deep feature correlation between each location and its
corr}, and st = frecurrent(st−1, s0), ∀t ∈
neighborhood from the previous state. Hence, st = {st
{1, . . . , T }, where st
corr is the collection of deep feature
correlation. For the transition function frecurrent, a non-local module is ﬁrst adopted and is followed
by two convolutional layers, before the feature s0 is added from the identity path. The weights in the
non-local module are shared across recurrent states just as convolutional layers, so our NLRN still
keeps high parameter efﬁciency as a whole. An illustration is displayed in Figure 2.

feat denotes the feature map in time t and st

feat, st

It is noteworthy that inside the non-local module, the feature correlation for location i from the
previous state, st−1
corr,i, is added to the estimated feature correlation in the current state before the
softmax normalization, which enables the propagation of correlation information between adjacent
states for more robust correlation estimation. The details can be found in Figure 3. The initial
state s0 is set as the feature after a convolutional layer on the input image. foutput is represented
by another single convolutional layer. All layers have 128 ﬁlters with 3 × 3 kernel size except for
the non-local module. Batch normalization and ReLU activation function are performed ahead of
each convolutional layer following [19]. We adopt residual learning and the output of NLRN is the
residual image ˆI = foutput(sT ) when NLRN is unfolded T times. During training, the objective is to
2 || ˆI + I − ˜I||2, where ˜I denotes the ground truth image.
minimize the mean square error L( ˆI, ˜I) = 1
Relation to Other RNN Methods: Although RNNs have been adopted for image restoration before,
our NLRN is the ﬁrst to incorporate non-local operations into an RNN framework with correlation
propagation. DRCN [22] recursively applies a single convolutional layer to the input feature map
multiple times without the identity path from the ﬁrst state. DRRN [37] applies both the identity path
and the residual path in each state, but without non-local operations, and thus there is no correlation
information ﬂow across adjacent states. MemNet [38] builds dense connections among several types
of memory blocks, and weights are shared in the same type of memory blocks but are different across
various types. Compared with MemNet, our NLRN has an efﬁcient yet effective RNN structure with
shallower effective depth and fewer parameters, but obtains better restoration performance, which is
shown in Section 5 in detail.

5 Experiments

Dataset: For image denoising, we adopt two different settings to fairly and comprehensively compare
with recent deep learning based methods [29, 24, 51, 38]: (1) As in [7, 51, 24], we choose as the

5

training set the combination of 200 images from the train set and 200 images from the test set in the
Berkeley Segmentation Dataset (BSD) [30], and test on two popular benchmarks: Set12 and Set68
with σ = 15, 25, 50 following [51]. (2) As in [29, 38], we use as the training set the combination of
200 images from the train set and 100 images from the val set in BSD, and test on Set14 and the
BSD test set of 200 images with σ = 30, 50, 70 following [29, 38]. In addition, we evaluate our
NLRN on the Urban100 dataset [20], which contains abundant structural patterns and textures, to
further demonstrate the capability of using image self-similarity of our NLRN. The training set and
test set are strictly disjoint and all the images are converted to gray-scale in each experiment setup.
For image SR, we follow [21, 37, 38] and use a training set of 291 images where 91 images are
proposed in [48] and other 200 are from the BSD train set. We adopt four benchmark sets: Set5 [1],
Set14 [50], BSD100 [30] and Urban100 [20] for testing with three upscaling factors: ×2, ×3 and
×4. The low-resolution images are synthesized by bicubic downsampling.

Training Settings: We randomly sample patches whose size equals the neighborhood of non-local
operation from images during training. We use ﬂipping, rotation and scaling for augmenting training
data. For image denoising, we add independent and identically distributed Gaussian noise with zero
mean to the original image as the noisy input during training. We train a different model for each
noise level. For image SR, only the luminance channel of images is super-resolved, and the other two
color channels are upscaled by bicubic interpolation, following [21, 22, 37]. Moreover, the training
images for all three upscaling factors: ×2, ×3 and ×4 are upscaled by bicubic interpolation into the
desired spatial size and are combined into one training set. We use this set to train one single model
for all these three upscaling factors as in [21, 37, 38].

We use Adam optimizer to minimize the loss function. We set the initial learning rate as 1e-3 and
reduce it by half ﬁve times during training. We use Xavier initialization for the weights. We clip
the gradient at the norm of 0.5 to prevent the gradient explosion which is shown to empirically
accelerate training convergence, and we adopt 16 as the minibatch size during training. Training a
model takes about 3 days with a Titan Xp GPU. For non-local module, we use circular padding for
the neighborhood outside input patches. For convolution, we pad the boundaries of feature maps with
zeros to preserve the spatial size of feature maps.

5.1 Model Analysis

In this section, we analyze our model in the following aspects. First, we conduct the ablation study of
using different distance metrics in the non-local module. Table 1 compares instantiations including
Euclidean distance, dot product, embedded dot product, Gaussian, symmetric embedded Gaussian
and embedded Gaussian when used in NLRN of 12 unfolded steps. Embedded Gaussian achieves the
best performance and is adopted in the following experiments.

We compare the NLRN with its variants in terms of PSNR in Table 2. We have a few observations.
First, the same model with untied weights performs worse than its weight-sharing counter-part. We
speculate that the model with untied weights is prone to model over-ﬁtting and suffers much slower
training convergence, both of which undermine its performance. To investigate the function of non-
local modules, we implement a baseline RNN with the same parameter number of NLRN, and ﬁnd it
is worse than NLRN by about 0.2 dB, showing the advantage of using non-local image properties for
image restoration. Besides, we implement NLRNs where non-local module is used in every other
state or every three states, and observe that if the frequency of using non-local modules in NLRN
is reduced, the performance decreases accordingly. We show the beneﬁt of propagating correlation
information among adjacent states by comparing with the counter-part in terms of restoration accuracy.
To further analyze the non-local module, we visualize the feature correlation maps for non-local
operations in Figure 4. It can be seen that as the number of recurrent states increases, the locations

Table 1: Image denoising comparison of our proposed model
with various distance metrics on Set12 with noise level of 25.

Distance metric
Euclidean distance
Dot product
Embedded dot product
Gaussian
Symmetric embedded Gaussian
Embedded Gaussian

φ(X i, X j)

exp{− (cid:107)X i − X j(cid:107)2

2 /h2}

X iX T
j
θ(X i)ψ(X j)T
exp{X iX T
j }
exp{θ(X i)θ(X j)T }
exp{θ(X i)ψ(X j)T }

PSNR
30.74
30.68
30.75
30.69
30.76
30.80

Table 2: Image denoising comparison of our
NLRN with its variants on Set12 with noise
level of 25.

Model
NLRN w/o parameter sharing
RNN with same parameter no.
Non-local module in every other state
Non-local module in every 3 states
NLRN w/o propagating correlations
NLRN

PSNR
30.65
30.61
30.76
30.72
30.78
30.80

6

Figure 4: Examples of correlation maps of non-local operations for
image denoising. Noisy patch/ground truth patch: the neighborhood of
the red center pixel used in non-local operations. (1)-(6): the correlation
map for recurrent state 1-6 from NLRN with unrolling length of 6.

Figure 5: Neighborhood size vs.
image denoising performance of
our proposed model on Set12 with
noise level of 25.

Max effective depth
Parameter sharing
Parameter no.
Multi-view testing
Training images
PSNR

DnCNN
17
No
554k
No
400
27.18

RED MemNet
30
No
4,131k
Yes
300
27.33

80
Yes
667k
No
300
27.38

NLRN
38
Yes
330k
No
300
27.60

No
400
27.64

Yes
300
27.66

Table 3: Image denoising comparison of our proposed model with state-
of-the-art network models on Set12 with noise level of 50. Model com-
plexities are also compared.

Figure 6: Unrolling length vs.
image denoising performance of
our proposed model on Set12 with
noise level of 25.

with similar features progressively show higher correlations in the map, which demonstrates the
effectiveness of the non-local module for exploiting image self-similarity.

Figure 5 investigates the inﬂuence of the neighborhood size in the non-local module on image
denoising results. The performance peaks at q = 45. This shows that limiting the neighborhood
helps concentrate the correlation calculation on relevant features in the spatial vicinity and enhance
correlation estimation. Therefore, it is necessary to choose a proper neighborhood size (rather than
the whole image) for image restoration. We select q = 45 for the rest of this paper unless stated
otherwise.

The unrolling length T determines the maximum effective depth (i.e., maximum number of convolu-
tional layers) of NLRN. The inﬂuence of the unrolling length on image denoising results is shown in
Figure 6. The performance increases as the unrolling length rises, but gets saturated after T = 12.
Given the tradeoff between restoration accuracy and inference time, we adopt T = 12 for NLRN in
all the experiments.

5.2 Comparisons with State-of-the-Art Methods

We compare our proposed model with a number of recent competitors for image denoising and
image SR, respectively. PSNR and SSIM [44] are adopted for measuring quantitative restoration
performance.

Image Denoising: For a fair comparison with other methods based on deep networks, we train our
model under two settings: (1) We use the training data as in TNRD [7], DnCNN [51] and NLNet [24],
and the result is shown in Table 4. We cite the result of NLNet in the original paper [24], since no
public code or model is available. (2) We use the training data as in RED [29] and MemNet [38], and
the result is shown in Table 5. We note that RED uses multi-view testing [45] to boost the restoration
accuracy, i.e., RED processes each test image as well as its rotated and ﬂipped versions, and all
the outputs are then averaged to form the ﬁnal denoised image. Accordingly, we perform the same
procedure for NLRN and ﬁnd its performance, termed as NLRN-MV, is consistently improved. In
addition, we include recent non-deep-learning based methods: BM3D [8] and WNNM [16] in our
comparison. We do not list other methods [54, 3, 47, 6, 52] whose average performances are worse
than DnCNN or MemNet. Our NLRN signiﬁcantly outperforms all the competitors on Urban100 and
yields the best results across almost all the noise levels and datasets.

To further show the advantage of the network design of NLRN, we compare different versions of
NLRN with several state-of-the-art network models, i.e., DnCNN, RED and MemNet in Table 3.
NLRN uses the fewest parameters but outperforms all the competitors. Speciﬁcally, NLRN beneﬁts

7

Table 4: Benchmark image denoising results. Training and testing protocols are followed as in [51]. Average
PSNR/SSIM for various noise levels on Set12, BSD68 and Urban100. The best performance is in bold.

Dataset

Set12

BSD68

Urban100

Noise
15
25
50
15
25
50
15
25
50

BM3D
32.37/0.8952
29.97/0.8504
26.72/0.7676
31.07/0.8717
28.57/0.8013
25.62/0.6864
32.35/0.9220
29.70/0.8777
25.95/0.7791

WNNM
32.70/0.8982
30.28/0.8557
27.05/0.7775
31.37/0.8766
28.83/0.8087
25.87/0.6982
32.97/0.9271
30.39/0.8885
26.83/0.8047

TNRD
32.50/0.8958
30.06/0.8512
26.81/0.7680
31.42/0.8769
28.92/0.8093
25.97/0.6994
31.86/0.9031
29.25/0.8473
25.88/0.7563

NLNet
-/-
-/-
-/-
31.52/-
29.03/-
26.07/-
-/-
-/-
-/-

DnCNN
32.86/0.9031
30.44/0.8622
27.18/0.7829
31.73/0.8907
29.23/0.8278
26.23/0.7189
32.68/0.9255
29.97/0.8797
26.28/0.7874

NLRN
33.16/0.9070
30.80/0.8689
27.64/0.7980
31.88/0.8932
29.41/0.8331
26.47/0.7298
33.45/0.9354
30.94/0.9018
27.49/0.8279

Table 5: Benchmark image denoising results. Training and testing protocols are followed as in [38]. Average
PSNR/SSIM for various noise levels on 14 images, BSD200 and Urban100. Red is the best and blue is the
second best performance.

Dataset

14 images

BSD200

Urban100

Noise
30
50
70
30
50
70
30
50
70

BM3D
28.49/0.8204
26.08/0.7427
24.65/0.6882
27.31/0.7755
25.06/0.6831
23.82/0.6240
28.75/0.8567
25.95/0.7791
24.27/0.7163

WNNM
28.74/0.8273
26.32/0.7517
24.80/0.6975
27.48/0.7807
25.26/0.6928
23.95/0.6346
29.47/0.8697
26.83/0.8047
25.11/0.7501

RED
29.17/0.8423
26.81/0.7733
25.31/0.7206
27.95/0.8056
25.75/0.7167
24.37/0.6551
29.12/0.8674
26.44/0.7977
24.75/0.7415

MemNet
29.22/0.8444
26.91/0.7775
25.43/0.7260
28.04/0.8053
25.86/0.7202
24.53/0.6608
29.10/0.8631
26.65/0.8030
25.01/0.7496

NLRN
29.37/0.8460
27.00/0.7777
25.49/0.7255
28.15/0.8423
25.93/0.7214
24.58/0.6614
29.94/0.8830
27.38/0.8241
25.66/0.7707

NLRN-MV
29.41/0.8472
27.05/0.7791
25.54/0.7273
28.20/0.8436
25.97/0.8429
24.62/0.6634
29.99/0.8842
27.43/0.8256
25.71/0.7724

Table 6: Benchmark SISR results. Average PSNR/SSIM for scale factor ×2, ×3 and ×4 on datasets Set5, Set14,
BSD100 and Urban100. The best performance is in bold.

Dataset

Set5

Set14

BSD100

Urban100

Scale
×2
×3
×4
×2
×3
×4
×2
×3
×4
×2
×3
×4

SRCNN
36.66/0.9542
32.75/0.9090
30.48/0.8628
32.45/0.9067
29.30/0.8215
27.50/0.7513
31.36/0.8879
28.41/0.7863
26.90/0.7101
29.50/0.8946
26.24/0.7989
24.52/0.7221

VDSR
37.53/0.9587
33.66/0.9213
31.35/0.8838
33.03/0.9124
29.77/0.8314
28.01/0.7674
31.90/0.8960
28.82/0.7976
27.29/0.7251
30.76/0.9140
27.14/0.8279
25.18/0.7524

DRCN
37.63/0.9588
33.82/0.9226
31.53/0.8854
33.04/0.9118
29.76/0.8311
28.02/0.7670
31.85/0.8942
28.80/0.7963
27.23/0.7233
30.75/0.9133
27.15/0.8276
25.14/0.7510

LapSRN
37.52/0.959
33.82/0.923
31.54/0.885
33.08/0.913
29.79/0.832
28.19/0.772
31.80/0.895
28.82/0.797
27.32/0.728
30.41/0.910
27.07/0.827
25.21/0.756

DRRN
37.74/0.9591
34.03/0.9244
31.68/0.8888
33.23/0.9136
29.96/0.8349
28.21/0.7721
32.05/0.8973
28.95/0.8004
27.38/0.7284
31.23/0.9188
27.53/0.8378
25.44/0.7638

MemNet
37.78/0.9597
34.09/0.9248
31.74/0.8893
33.28/0.9142
30.00/0.8350
28.26/0.7723
32.08/0.8978
28.96/0.8001
27.40/0.7281
31.31/0.9195
27.56/0.8376
25.50/0.7630

NLRN
38.00/0.9603
34.27/0.9266
31.92/0.8916
33.46/0.9159
30.16/0.8374
28.36/0.7745
32.19/0.8992
29.06/0.8026
27.48/0.7306
31.81/0.9249
27.93/0.8453
25.79/0.7729

from inherent parameter sharing and uses only less than 1/10 parameters of RED. Compared with the
RNN competitor, MemNet, NLRN uses only half of parameters and much shallower depth to obtain
better performance, which shows the superiority of our non-local recurrent architecture.

Image Super-Resolution: We compare our model with several recent SISR approaches, including
SRCNN [10], VDSR [21], DRCN [22], LapSRN [23], DRRN [37] and MemNet [38] in Table 6. We
crop pixels near image borders before calculating PSNR and SSIM as in [10, 35, 21, 22]. We do
not list other methods [20, 35, 26, 36, 17] since their performances are worse than that of DRRN or
MemNet. Besides, we do not include SRDenseNet [41] and EDSR [25] in the comparison because
the number of parameters in these two network models is over two orders of magnitude larger than
that of our NLRN and their training datasets are signiﬁcantly larger than ours. It can be seen that
NLRN yields the best result across all the upscaling factors and datasets. Visual results are provided
in Section 7.

6 Conclusion

We have presented a new and effective recurrent network that incorporates non-local operations for
image restoration. The proposed non-local module can be trained end-to-end with the recurrent
network. We have studied the importance of computing reliable feature correlations within a conﬁned
neighorhood against the whole image, and have shown the beneﬁts of passing feature correlation
messages between adjacent recurrent stages. Comprehensive evaluations over benchmarks for image
denoising and super-resolution demonstrate the superiority of NLRN over existing methods.

8

7 Appendix

7.1 Extension of the General Framework to Other Classic Non-Local Methods

Besides the extension to WNNM and non-local means, which are discussed in Section 3.1, we show
the proposed non-local framework (1) can be extended to collaborative ﬁltering methods, e.g., BM3D
algorithm [8], as well as joint sparsity based methods, e.g., LSSC algorithm [28]. We follow the same
notations in Section 3.1. Both BM3D and LSSC apply block matching (BM) ﬁrst before processing,
and form N groups of similar patches into data matrices. The index set of the matched patches for
the i-th reference patch is denoted as Ci. The group of matched patches for the i-th reference patch is
denoted as X Ci.
Similar to WNNM [16], BM3D [8] also applies BM ﬁrst to group similar patches based on their
Euclidean distances. The matched patches are then processed via Wiener ﬁltering [8], and the
denoised results of the i-th group of patches are

ZCi = τ −1(diag(ω)τ (X Ci)).

(8)

Here τ (·) and τ −1(·) denote the forward and backward Wiener ﬁltering applied to the groups of
matched patches, respectively. The diagonal matrix diag(ω) is formed by the empirical Wiener
coefﬁcients ω. BM3D applies data pre-cleaning, using discrete cosine transform (DCT), to estimate
the original patch, and calculate the estimate of ω [8]. Since calculating ZCi in (8) involves only
linear ﬁltering, it can also be generalized using the proposed non-local framework as (2). Unlike the
extension to WNNM, here (cid:80)
i G(X)j corresponds to the denoised results via Wiener
ﬁltering as shown in (8), of the i-th group of matched patches.

Φ(X)j

j∈Ci

Different from BM3D and WNNM, LSSC learns a common dictionary D for all image patches, and
imposes joint sparsity [28] on each data matrix of matched patches X Ci, so that the correlation of
the matched patches are exploited by enforcing the same support of their sparse codes. Thus, the
joint sparse coding in LSSC [28] becomes

ˆAi = argminAi (cid:107)Ai(cid:107)0,∞ s.t.

− DAi

≤ (cid:15) |Ci| , ∀i ,

(9)

(cid:13)
(cid:13)X T
(cid:13)
Ci

(cid:13)
2
(cid:13)
(cid:13)

F

where the (0, ∞) “norm” (cid:107)·(cid:107)0,∞ counts the number of non-zero columns of each sparse code matrix
Ai [28], and |Ci| is the cardinality of Ci. The coefﬁcient (cid:15) is a constant, which is used to upper bound
the sparse modeling errors. In general, the solution to (9) is NP-hard. To simplify the discussion, we
assume the dictionary to be unitary (which reduces the sparse coding problem to the transform-model
sparse coding [46]), i.e., DT D = I and D ∈ Rk×k. Thus there exists a corresponding shrinkage
function η(·) for imposing joint sparsity on the sparse codes [28, 34], such that the denoised estimates
T
of the i-th patch group can be obtained as ZCi = ˆA
i DT = η( X Ci D ) DT . Though joint sparse
coding projects all data onto a union of subspaces [28, 12, 46] which is a non-linear operation in
general, each data matrix X Ci is projected onto one particular subspace spanned by the selected
atoms corresponding to the non-zero columns in ˆAi, which is locally linear. For the i-th group of
patches, such a subspace projection corresponds to (cid:80)
i G(X)j in the proposed general
framework.

Φ(X)j

j∈Ci

7.2 Visual Results

We show the visual comparison of our NLRN and several competing methods: BM3D [8],
WNNM [16], and MemNet [38] for image denoising in Figure 7. Our method can recover more
details from the noisy measurement. The visual comparison of our NLRN and several recent methods:
DRCN [22], LapSRN [23], DRRN [37], and MemNet [38] for image super-resolution is displayed in
Figure 8. Our method is able to reconstruct sharper edges and produce fewer artifacts especially in
the regions of repetitive patterns.

9

Noisy (18.75/0.3232)

BM3D (29.13/0.8261)

WNNM (29.30/0.8334)

Ground truth

MemNet (29.18/0.8223)

NLRN (29.53/0.8369)

Ground truth (PSNR/SSIM)

Noisy (19.49/0.5099)

BM3D (28.95/0.9062)

WNNM (30.44/0.9260)

Ground truth

MemNet (28.71/0.8906)

NLRN (30.52/0.9267)

Ground truth (PSNR/SSIM)

Noisy (19.39/0.4540)

BM3D (27.20/0.8775)

WNNM (28.86/0.8913)

Ground truth

MemNet (27.55/0.8807)

NLRN (29.04/0.9044)

Ground truth (PSNR/SSIM)

Noisy (19.06/0.3005)

BM3D (29.61/0.8304)

WNNM (30.59/0.8543)

Ground truth

MemNet (30.21/0.8517)

NLRN (31.17/0.8727)

Ground truth (PSNR/SSIM)

Noisy (18.88/0.3479)

BM3D (28.82/0.9051)

WNNM (28.38/0.9189)

Ground truth

MemNet (28.31/0.9112)

NLRN (28.42/0.9308)

Ground truth (PSNR/SSIM)

Figure 7: Qualitative comparison of image denoising results with noise level of 30. The zoom-in region in the
red bounding box is shown on the right. From top to bottom: 1) the image barbara. 2) image 004 in Urban100.
3) image 019 in Urban100. 4) image 033 in Urban100. 5) image 046 in Urban100.

10

DRCN (26.82/0.9329)

LapSRN (26.52/0.9316)

DRRN (27.52/0.9434)

Ground truth HR

MemNet (27.78/0.9451)

NLRN (28.46/0.9513)

HR (PSNR/SSIM)

DRCN (20.95/0.7716)

LapSRN (20.90/0.7722)

DRRN (21.37/0.7874)

Ground truth HR

MemNet (21.35/0.7877)

NLRN (21.92/0.8014)

HR (PSNR/SSIM)

DRCN (30.18/0.8306)

LapSRN (30.29/0.8388)

DRRN (30.18/0.8306)

Ground truth HR

MemNet (29.25/0.8347)

NLRN (31.19/0.8598)

HR (PSNR/SSIM)

DRCN (20.71/0.7466)

LapSRN (20.86/0.7524)

DRRN (20.92/0.7666)

Ground truth HR

MemNet (21.06/0.7716)

NLRN (21.41/0.7866)

HR (PSNR/SSIM)

DRCN (23.99/0.6940)

LapSRN (24.49/0.7247)

DRRN (25.14/0.7469)

Ground truth HR

MemNet (25.19/0.7519)

NLRN (25.97/0.7882)

HR (PSNR/SSIM)

Figure 8: Qualitative comparison of image super-resolution results with ×4 upscaling. The zoom-in region in
the red bounding box is shown on the right. From top to bottom: 1) image 005 in Urban100. 2) image 019 in
Urban100. 3) image 044 in Urban100. 4) image 062 in Urban100. 5) image 099 in Urban100.

References

[1] M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. Alberi-Morel. Low-complexity single-image super-

resolution based on nonnegative neighbor embedding. 2012.

11

[2] A. Buades, B. Coll, and J.-M. Morel. A non-local algorithm for image denoising. In CVPR, 2005.
[3] H. C. Burger, C. J. Schuler, and S. Harmeling. Image denoising: Can plain neural networks compete with

[4] S. Chandra, N. Usunier, and I. Kokkinos. Dense and low-rank gaussian crfs using deep embeddings. In

[5] H. Chang, D.-Y. Yeung, and Y. Xiong. Super-resolution through neighbor embedding. In CVPR, 2004.
[6] F. Chen, L. Zhang, and H. Yu. External patch prior guided internal clustering for image denoising. In

bm3d? In CVPR, 2012.

ICCV, 2017.

ICCV, 2015.

[7] Y. Chen and T. Pock. Trainable nonlinear reaction diffusion: A ﬂexible framework for fast and effective

[8] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-d transform-domain

[9] A. Danielyan, V. Katkovnik, and K. Egiazarian. Bm3d frames and variational image deblurring. TIP,

image restoration. IEEE TPAMI, 2017.

collaborative ﬁltering. IEEE TIP, 2007.

21(4):1715–1728, 2012.

[10] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep convolutional network for image super-resolution.

[11] C. Dong, C. C. Loy, and X. Tang. Accelerating the super-resolution convolutional neural network. In

In ECCV, 2014.

ECCV, 2016.

IEEE TIP, 2006.

Graphics (TOG), 2011.

learning. In ICML, 2017.

[12] M. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned dictionaries.

[13] G. Freedman and R. Fattal. Image and video upscaling from local self-examples. ACM Transactions on

[14] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to sequence

[15] D. Glasner, S. Bagon, and M. Irani. Super-resolution from a single image. In ICCV, 2009.
[16] S. Gu, L. Zhang, W. Zuo, and X. Feng. Weighted nuclear norm minimization with application to image

denoising. In CVPR, pages 2862–2869, 2014.

[17] W. Han, S. Chang, D. Liu, M. Yu, M. Witbrock, and T. S. Huang. Image super-resolution via dual-state

[18] A. W. Harley, K. G. Derpanis, and I. Kokkinos. Segmentation-aware convolutional networks using local

recurrent networks. In CVPR, June 2018.

attention masks. In ICCV, 2017.

[19] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, 2016.
[20] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-resolution from transformed self-exemplars. In

[21] J. Kim, J. Kwon Lee, and K. Mu Lee. Accurate image super-resolution using very deep convolutional

CVPR, 2015.

networks. In CVPR, 2016.

In CVPR, 2016.

super-resolution. In CVPR, 2017.

[22] J. Kim, J. Kwon Lee, and K. Mu Lee. Deeply-recursive convolutional network for image super-resolution.

[23] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang. Deep laplacian pyramid networks for fast and accurate

[24] S. Lefkimmiatis. Non-local color image denoising with convolutional neural networks. In CVPR, 2017.
[25] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced deep residual networks for single image

super-resolution. In CVPR Workshops, 2017.

[26] D. Liu, Z. Wang, B. Wen, J. Yang, W. Han, and T. S. Huang. Robust single image super-resolution via

deep networks with sparse prior. TIP, 25(7):3194–3207, 2016.

[27] D. Liu, B. Wen, X. Liu, Z. Wang, and T. S. Huang. When image denoising meets high-level vision tasks:

A deep learning approach. In IJCAI, 2018.

[28] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Non-local sparse models for image restoration.

In ICCV, 2009.

[29] X. Mao, C. Shen, and Y.-B. Yang. Image restoration using very deep convolutional encoder-decoder

networks with symmetric skip connections. In NIPS, 2016.

[30] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its

application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, 2001.

[31] P. Qiao, Y. Dou, W. Feng, R. Li, and Y. Chen. Learning non-local image diffusion for image denoising. In

ACM on Multimedia Conference, 2017.

[32] L. I. Rudin and S. Osher. Total variation based image restoration with free local constraints. In ICIP, 1994.
[33] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lillicrap. A simple

neural network module for relational reasoning. In NIPS, 2017.

[34] U. Schmidt and S. Roth. Shrinkage ﬁelds for effective image restoration. In CVPR, 2014.
[35] S. Schulter, C. Leistner, and H. Bischof. Fast and accurate image upscaling with super-resolution forests.

In CVPR, 2015.

12

[36] W. Shi, J. Caballero, F. Huszár, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-time
single image and video super-resolution using an efﬁcient sub-pixel convolutional neural network. In
CVPR, 2016.

[37] Y. Tai, J. Yang, and X. Liu. Image super-resolution via deep recursive residual network. In CVPR, 2017.
[38] Y. Tai, J. Yang, X. Liu, and C. Xu. Memnet: A persistent memory network for image restoration. In ICCV,

[39] R. Timofte, V. De, and L. Van Gool. Anchored neighborhood regression for fast example-based super-

resolution. In ICCV, 2013.

[40] C. Tomasi and R. Manduchi. Bilateral ﬁltering for gray and color images. In ICCV, 1998.
[41] T. Tong, G. Li, X. Liu, and Q. Gao. Image super-resolution using dense skip connections. In ICCV, 2017.
[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.

Attention is all you need. In NIPS, 2017.

[43] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. arXiv preprint arXiv:1711.07971,

[44] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility

[45] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang. Deep networks for image super-resolution with sparse

to structural similarity. IEEE TIP, 2004.

prior. In ICCV, 2015.

[46] B. Wen, S. Ravishankar, and Y. Bresler. Structured overcomplete sparsifying transform learning with

convergence guarantees and applications. IJCV, 2015.

[47] J. Xu, L. Zhang, W. Zuo, D. Zhang, and X. Feng. Patch group based nonlocal self-similarity prior learning

for image denoising. In ICCV, 2015.

[48] J. Yang, J. Wright, T. S. Huang, and Y. Ma. Image super-resolution via sparse representation. IEEE TIP,

[49] R. Yin, T. Gao, Y. M. Lu, and I. Daubechies. A tale of two bases: Local-nonlocal regularization on image

patches with convolution framelets. SIAM Journal on Imaging Sciences, 10(2):711–750, 2017.

[50] R. Zeyde, M. Elad, and M. Protter. On single image scale-up using sparse-representations. In International

[51] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Beyond a gaussian denoiser: Residual learning of

conference on curves and surfaces, 2010.

deep cnn for image denoising. IEEE TIP, 2017.

[52] K. Zhang, W. Zuo, S. Gu, and L. Zhang. Learning deep cnn denoiser prior for image restoration. In CVPR,

[53] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and P. H. Torr. Conditional

random ﬁelds as recurrent neural networks. In ICCV, 2015.

[54] D. Zoran and Y. Weiss. From learning models of natural image patches to whole image restoration. In

ICCV, 2011.

2017.

2017.

2010.

2017.

13

