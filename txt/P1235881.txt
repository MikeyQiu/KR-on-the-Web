VILD: Variational Imitation Learning
with Diverse-quality Demonstrations

Voot Tangkaratt1, Bo Han1, Mohammad Emtiyaz Khan1, and Masashi Sugiyama1,2

1RIKEN AIP, Japan
2The University of Tokyo, Japan

9
1
0
2
 
p
e
S
 
5
1
 
 
]

G
L
.
s
c
[
 
 
1
v
9
6
7
6
0
.
9
0
9
1
:
v
i
X
r
a

Abstract

The goal of imitation learning (IL) is to learn a good policy from high-quality demonstrations.
However, the quality of demonstrations in reality can be diverse, since it is easier and cheaper to
collect demonstrations from a mix of experts and amateurs. IL in such situations can be challenging,
especially when the level of demonstrators’ expertise is unknown. We propose a new IL method called
variational imitation learning with diverse-quality demonstrations (VILD), where we explicitly model
the level of demonstrators’ expertise with a probabilistic graphical model and estimate it along with
a reward function. We show that a naive approach to estimation is not suitable to large state and
action spaces, and ﬁx its issues by using a variational approach which can be easily implemented using
existing reinforcement learning methods. Experiments on continuous-control benchmarks demonstrate
that VILD outperforms state-of-the-art methods. Our work enables scalable and data-eﬃcient IL
under more realistic settings than before.

1

Introduction

The goal of sequential decision making is to learn a policy that makes good decisions (Puterman, 1994).
As an important branch of sequential decision making, imitation learning (IL) (Russell, 1998; Schaal,
1999) aims to learn such a policy from demonstrations (i.e., sequences of decisions) collected from experts.
However, high-quality demonstrations can be diﬃcult to obtain in reality, since such experts may not
always be available and sometimes are too costly (Osa et al., 2018). This is especially true when the
quality of decisions depends on speciﬁc domain-knowledge not typically available to amateurs; e.g., in
applications such as robot control (Osa et al., 2018), autonomous driving (Silver et al., 2012), and the
game of Go (Silver et al., 2016).

In practice, demonstrations are often diverse in quality, since it is cheaper to collect them from
mixed demonstrators, containing both experts and amateurs (Audiﬀren et al., 2015). Unfortunately, IL
in such settings tends to perform poorly since low-quality demonstrations often negatively aﬀect the
performance (Shiarlis et al., 2016; Lee et al., 2016). For example, demonstrations for robotics can be
cheaply collected via a robot simulation (Mandlekar et al., 2018), but demonstrations from amateurs
who are not familiar with the robot may cause damages to the robot which is catastrophic in the real-
world (Shiarlis et al., 2016). Similarly, demonstrations for autonomous driving can be collected from
drivers in public roads (Fridman et al., 2017), but these low-quality demonstrations may also cause traﬃc
accidents..

When the level of demonstrators’ expertise is known, multi-modal IL (MM-IL) may be used to learn a
good policy with diverse-quality demonstrations (Li et al., 2017; Hausman et al., 2017; Wang et al., 2017).
More speciﬁcally, MM-IL aims to learn a multi-modal policy where each mode of the policy represents
the decision making of each demonstrator. When knowing the level of demonstrators’ expertise, good
policies can be obtained by selecting modes that correspond to the decision making of high-expertise
demonstrators. However, in reality it is diﬃcult to truly determine the level of expertise beforehand.
Without knowing the level of demonstrators’ expertise, it is diﬃcult to distinguish the decision making of
experts and amateurs, and thus learning a good policy is quite challenging.

To overcome the issue of MM-IL, existing works have proposed to estimate the quality of each
demonstration using additional information from experts (Audiﬀren et al., 2015; Wu et al., 2019; Brown

Contacts: voot.tangkaratt@riken.jp; bo.han@riken.jp

1

et al., 2019). Speciﬁcally, Audiﬀren et al. (2015) proposed a method that infers the quality using similarities
between diverse-quality demonstrations and high-quality demonstrations, where the latter are collected in
a small number from experts. In contrast, Wu et al. (2019) proposed to estimate the quality using a small
number of demonstrations with conﬁdence scores. The value of these scores are proportion to the quality
and are given by an expert. Similarly, the quality can be estimated using demonstrations that are ranked
according to their relative quality by an expert (Brown et al., 2019). These methods rely on additional
information from experts, namely high-quality demonstrations, conﬁdence scores, and ranking. In practice,
these pieces of information can be scarce or noisy, which leads to the poor performance of these methods.
In this paper, we consider a novel but realistic setting of IL where only diverse-quality demonstrations
are available, while the level of demonstrators’ expertise and additional information from experts are
fully absent. To tackle this challenging setting, we propose a new method called variational imitation
learning with diverse-quality demonstrations (VILD). The central idea of VILD is to model the level of
expertise via a probabilistic graphical model, and learn it along with a reward function that represents an
intention of expert’s decision making. To scale up our model for large state and action spaces, we leverage
the variational approach (Jordan et al., 1999), which can be implemented using reinforcement learning
(RL) (Sutton & Barto, 1998). To further improve data-eﬃciency when learning the reward function,
we utilize importance sampling to re-weight a sampling distribution according to the estimated level
of expertise. Experiments on continuous-control benchmarks demonstrate that VILD is robust against
diverse-quality demonstrations and outperforms existing methods signiﬁcantly. Empirical results also
show that VILD is a scalable and data-eﬃcient method for realistic settings of IL.

2 Related Work

In this section, we ﬁrstly discuss a related area of supervised learning with diverse-quality data. Then, we
discuss existing IL methods that use the variational approach.

Supervised learning with diverse-quality data.
In supervised learning, diverse-quality data has
been studied extensively under the setting of classiﬁcation with noisy label (Angluin & Laird, 1988). This
classiﬁcation setting assumes that human labelers may assign incorrect class labels to training inputs.
With such labelers, the obtained dataset consists of high-quality data with correct labels and low-quality
data with incorrect labels. To handle this challenging setting, many methods were proposed (Raykar
et al., 2010; Natarajan et al., 2013; Han et al., 2018). The most related methods to ours are probabilistic
modeling methods, which aim to infer correct labels and the level of labeler’s expertise (Raykar et al.,
2010; Khetan et al., 2018). Speciﬁcally, Raykar et al. (2010) proposed a method based on a two-coin
model which enables estimating the correct labels and level of expertise. Recently, Khetan et al. (2018)
proposed a method based on weighted loss functions, where the weight is determined by the estimated
labels and level of expertise.

Methods for supervised learning with diverse-quality data may be used to learn a policy in our setting.
However, they tend to perform poorly due to the issue of compounding error (Ross & Bagnell, 2010).
Speciﬁcally, supervised learning methods generally assume that data distributions during training and
testing are identical. However, data distributions during training and testing are diﬀerent in IL, since
data distributions depend on policies (Ng & Russell, 2000). A discrepancy of data distributions causes
compounding errors during testing, where prediction errors increase further in future predictions. Due to
the issue of compounding error, supervised-learning-based methods often perform poorly in IL (Ross &
Bagnell, 2010). The issue becomes even worse with diverse-quality demonstrations, since data distributions
of diﬀerent demonstrators tend to be highly diﬀerent. For these reasons, methods for supervised learning
with diverse-quality data is not suitable for IL.

Variational approach in IL. The variational approach (Jordan et al., 1999) has been previously
utilized in IL to perform MM-IL and reduce over-ﬁtting. Speciﬁcally, MM-IL aims to learn a multi-modal
policy from diverse demonstrations collected by many experts (Li et al., 2017), where each mode of the
policy represents decision making of each expert1. A multi-modal policy is commonly represented by a
context-dependent policy, where each context represents each mode of the policy. The variational approach
has been used to learn a distribution of such contexts, i.e., by learning a variational auto-encoder (Wang
et al., 2017) and by maximizing a variational lower-bound of mutual information (Li et al., 2017; Hausman

1We emphasize that diverse demonstrations are diﬀerent from diverse-quality demonstrations. Diverse demonstrations
are collected by experts who execute equally good policies, while diverse-quality demonstrations are collected by mixed
demonstrators; The former consists of demonstrations that are equally high-quality but diverse in behavior, while the latter
consists of demonstrations that are diverse in both quality and behavior.

2

et al., 2017). Meanwhile, variational information bottleneck (VIB) (Alemi et al., 2017) has been used
to reduce over-ﬁtting in IL (Peng et al., 2019). Speciﬁcally, VIB aims to compress information ﬂow by
minimizing a variational bound of mutual information. This compression ﬁlters irrelevant signals, which
leads to less over-ﬁtting. Unlike these existing works, we utilize the variational approach to aid computing
integrals in large state-action spaces, and do not use a variational auto-encoder or a variational bound of
mutual information.

3

IL from Diverse-quality Demonstrations and its Challenge

Before delving into our main contribution, we ﬁrst give the minimum background about RL and IL. Then,
we formulate a new setting of IL with diverse-quality demonstrations, discuss its challenge, and reveal the
deﬁciency of existing methods.

Reinforcement learning. Reinforcement learning (RL) (Sutton & Barto, 1998) aims to learn an
optimal policy of a sequential decision making problem, which is often mathematically formulated as a
Markov decision process (MDP) (Puterman, 1994). We consider a ﬁnite-horizon MDP with continuous
state and action spaces deﬁned by a tuple M “ pS, A, pps1|s, aq, p1ps1q, rps, aqq with a state st P S Ď Rds,
an action at P A Ď Rda, an initial state density p1ps1q, a transition probability density ppst`1|st, atq,
and a reward function r : S ˆ A ÞÑ R, where the subscript t P t1, . . . , T u denotes the time step. A
sequence of states and actions, ps1:T , a1:T q, is called a trajectory. A decision making of an agent is
determined by a policy function πpat|stq, which is a conditional probability density of action given
state. RL seeks for an optimal policy π‹pat|stq which maximizes the expected cumulative reward, i.e.,
t“1rpst, atqs, where pπps1:T , a1:T q “ p1ps1qΠT
π‹ “ argmaxπ
t“1ppst`1|st, atqπpat|stq is a
trajectory probability density induced by π. RL has shown great successes recently, especially when
combined with deep neural networks (Mnih et al., 2015; Silver et al., 2017). However, a major limitation
of RL is that it relies on the reward function which may be unavailable in practice (Russell, 1998).

Epπps1:T ,a1:T qrΣT

Imitation learning. To address the above limitation of RL, imitation learning (IL) was proposed (Schaal,
1999; Ng & Russell, 2000). Without using the reward function, IL aims to learn the optimal policy from
demonstrations that encode information about the optimal policy. A common assumption in most IL
methods is that, demonstrations are collected by K ě 1 demonstrators who execute actions at drawn
from π‹pat|stq for every states st. A graphic model describing this data collection process is depicted in
Figure 1(a), where a random variable k P t1, . . . , Ku denotes each demonstrator’s identiﬁcation number
and ppkq denotes the probability of collecting a demonstration from the k-th demonstrator. Under this
assumption, demonstrations tps1:T , a1:T , kqnuN
n“1 (i.e., observed random variables in Figure 1(a)) are
called expert demonstrations and are regarded to be drawn independently from a probability density
p‹ps1:T , a1:T qppkq “ ppkqp1ps1qΠT
t“1ppst`1|st, atqπ‹pat|stq. We note that the variable k does not aﬀect the
trajectory density p‹ps1:T , a1:T q and can be omitted. In this paper, we assume a common assumption
that p1ps1q and ppst`1|st, atq are unknown but we can sample states from them.

IL has shown great successes in benchmark settings (Ho & Ermon, 2016; Fu et al., 2018; Peng et al.,
2019). However, practical applications of IL in the real-world is relatively few (Schroecker et al., 2019). One
of the main reasons is that most IL methods aim to learn with expert demonstrations. In practice, such
demonstrations are often too costly to obtain due to a limited number of experts, and even when we obtain
them, the number of demonstrations is often too few to accurately learn the optimal policy (Audiﬀren
et al., 2015; Wu et al., 2019; Brown et al., 2019).

New setting: Diverse-quality demonstrations. To improve practicality, we consider a new problem
called IL with diverse-quality demonstrations, where demonstrations are collected from demonstrators with
diﬀerent level of expertise. Compared to expert demosntrations, diverse-quality demonstrations can be
collected more cheaply, e.g., via crowdsourcing (Mandlekar et al., 2018). The graphical model in Figure 1(b)
depicts the process of collecting such demonstrations from K ą 1 demonstrators. Formally, we select
the k-th demonstrator for demonstrations according to a probability distribution ppkq. After selecting k,
for each time step t, the k-th demonstrator observes state st and samples action at using the optimal
policy π‹pat|stq. However, the demonstrator may not execute at in the MDP if this demonstrator is not
expertised. Instead, he/she may sample an action ut P A with another probability density pput|st, at, kq
and execute it. Then, the next state st`1 is observed with a probability density ppst`1|st, utq, and the
demonstrator continues making decision until time step T . We repeat this process for N times to collect
diverse-quality demonstrations Dd “ tps1:T , u1:T , kqnuN
n“1. These demonstrations are regarded to be drawn

3

s1

a1

u1

k

ż

A

s2

a2

¨ ¨ ¨

. . .

sT

aT

s1

a1

k

¨ ¨ ¨

. . .

s2

a2

u2

sT

aT

uT

N

N

Tź

t“1

(a) Expert demonstrations.

(b) Diverse-quality demonstrations.

Figure 1: Graphical models describe expert demonstrations and diverse-quality demonstrations. Shaded
and unshaded nodes indicate observed and unobserved random variables, respectively. Plate notations
indicate that the sampling process is repeated for N times. st P S is a state with transition densities
ppst`1|st, atq, at P A is an action with density π‹pat|stq, ut P A is a noisy action with density pput|st, at, kq,
and k P t1, . . . , Ku is an identiﬁcation number with distribution ppkq.

independently from a probability density

pdps1:T , u1:T |kqppkq “ ppkqpps1q

p1pst`1|st, utq

π‹pat|stqpput|st, at, kqdat.

(1)

We refer to pput|st, at, kq as a noisy policy of the k-th demonstrator, since it is used to execute a noisy
action ut. Our goal is to learn the optimal policy π‹ using diverse-quality demonstrations Dd.

The deﬁciency of existing methods. We conjecture that existing IL methods are not suitable to learn
with diverse-quality demonstrations according to pd. Speciﬁcally, these methods always treat observed
demonstrations as if they were drawn from p‹. By comparing p‹ and pd, we can see that existing methods
ş
would learn πput|stq such that πput|stq « ΣK
A π‹pat|stqpput|st, at, kqdat. In other words, they
learn a policy that averages over decisions of all demonstrators. This would be problematic when amateurs
are present, as averaged decisions of all demonstrators would be highly diﬀerent from those of all experts.
Worse yet, state distributions of amateurs and experts tend to be highly diﬀerent, which often leads to
unstable learning. For these reasons, we believe that existing methods tend to learn a policy that achieves
average performances and are not suitable for handling the setting of diverse-quality demonstrations.

k“1ppkq

4 VILD: A Robust Method for Diverse-quality Demonstrations

This section describes VILD, namely a robust method for tackling the challenge from diverse-quality
demonstrations. Speciﬁcally, we build a probabilistic model that explicitly describes the level of demonstra-
tors’ expertise and a reward function (Section 4.1), and estimate its parameters by a variational approach
(Section 4.2), which can be implemented by using RL (Section 4.3). We also improve data-eﬃciency by
using importance sampling (Section 4.4). Mathematical derivations are provided in Appendix A.

4.1 Model describing diverse-quality demonstrations
This section presents a model which enables estimating the level of demonstrators’ expertise. We ﬁrst
describe a naive model, whose parameters can be estimated trivially via supervised learning, but suﬀers
from the issue of compounding error. Then, we describe our proposed model, which avoids the issue of
the naive model by learning a reward function.

Naive model. Based on pd, one of the simplest models to handle diverse-quality demonstrations is
pθ,ωps1:T , u1:T , kq “ ppkqpps1qΠT
A πθpat|stqpωput|st, at, kqdat, where θ and ω are real-
valued parameter vectors. These parameters can be learned by e.g., minimizing the Kullback-Leibler (KL)
divergence from the data distribution to the model: minθ,ω KLppdps1:T , u1:T |kqppkq||pθ,ωps1:T , u1:T , kqq.
This naive model can be regarded as a regression-extension of the two-coin model proposed by Raykar
et al. (2010) for classiﬁcation with noisy label. As discussed previously in Section 2, such a model suﬀers
from the issue of compounding error and is not suitable for our IL setting.

t“1ppst`1|st, utq

ş

4

Proposed model. To avoid the issue of compounding error, our method utilizes the inverse RL (IRL)
approach (Ng & Russell, 2000), where we aim to learn a reward function from diverse-quality demonstra-
tions2. IL problems can be solved by a combination of IRL and RL, where we learn a reward function
by IRL and then learn a policy from the reward function by RL. This combination avoids the issue
of compounding error, since the policy is learned by RL which generalizes to states not presented in
demonstrations.

Speciﬁcally, our proposed model is based on a model of maximum entropy IRL (MaxEnt-IRL) (Ziebart
et al., 2010). Brieﬂy speaking, MaxEnt-IRL learns a reward function from expert demonstrations by
using a model pφps1:T , a1:T q 9 pps1qΠT
t“1p1pst`1|st, atq expprφpst, atqq. Based on this model, we propose
to learn the reward function and the level of expertise by a model

Tź

t“1

ż

A

pφ,ωps1:T , u1:T , kq “ ppkqp1ps1q

ppst`1|st, utq

exp prφpst, atqq pωput|st, at, kqdat{Zφ,ω,

(2)

where φ and ω are parameters of the model and Zφ,ω is the normalization term. By comparing the
proposed model pφ,ωps1:T , u1:T , kq to the data distribution pd, the reward parameter φ should be learned
so that the cumulative rewards is proportion to a probability density of actions given by the optimal
policy, i.e., exppΣT
t“1π‹pat|stq. In other words, the cumulative rewards are large for
trajectories induced by the optimal policy π‹. Therefore, π‹ can be learned by maximizing the cumulative
rewards. Meanwhile, the density pωput|st, at, kq is learned to estimate the noisy policy pput|st, at, kq. In
the remainder, we refer to ω as an expertise parameter.

t“1rφpst, atqq 9 ΠT

to

solving

To learn the parameters of this model, we propose to minimize the KL divergence from
By
the data distribution to the model: minφ,ω KLppdps1:T , u1:T |kqppkq||pφ,ωps1:T , u1:T , kqq.
equiv-
rearranging terms and ignoring constant
is
f pφ, ωq “
alent
Epdps1:T ,u1:T |kqppkqrΣT
A expprφpst, atqqpωput|st, at, kqdatqs and gpφ, ωq “ log Zφ,ω. To solve this
optimization, we need to compute the integrals over both state space S and action space A. Computing
these integrals is feasible for small state and action spaces, but is infeasible for large state and action
spaces. To scale up our model to MDPs with large state and action spaces, we leverage a variational
approach in the followings.

an optimization problem maxφ,ω f pφ, ωq ´ gpφ, ωq, where
ş
t“1 logp

terms, minimizing this KL divergence

4.2 Variational approach for parameter estimation
The central idea of the variational approach is to lower-bound an integral by the Jensen inequality and
a variational distribution (Jordan et al., 1999). The main beneﬁt of the variational approach is that
the integral can be indirectly computed via the lower-bound, given an optimal variational distribution.
However, ﬁnding the optimal distribution often requires solving a sub-optimization problem.

Before we proceed, notice that f pφ, ωq´gpφ, ωq is not a joint concave function of the integrals, and this
prohibits using the Jensen inequality. However, we can use the Jensen inequality to separately lower-bound
f pφ, ωq and gpφ, ωq, since they are concave functions of their corresponding integrals. Speciﬁcally, let
lφ,ωpst, at, ut, kq “ rφpst, atq ` log pωput|st, at, kq. By using a variational distribution qψpat|st, ut, kq with
parameter ψ, we obtain an inequality f pφ, ωq ě Fpφ, ω, ψq, where

ﬀ

Fpφ, ω, ψq “ Epdps1:T ,u1:T |kqppkq

Eqψpat|st,ut,kq rlφ,ωpst, at, ut, kqs ` Htpqψq

,

(3)

«

Tÿ

t“1

and Htpqψq “ ´Eqψpat|st,ut,kq rlog qψpat|st, ut, kqs.
It is trivial to verify that the equality f pφ, ωq “
(Murphy, 2013), where the maximizer ψ‹ of the lower-bound yields
maxψ Fpφ, ω, ψq holds
qψ‹pat|st, ut, kq 9 expplφ,ωpst, at, ut, kqq. Therefore, the function f pφ, ωq can be substituted by
maxψ Fpφ, ω, ψq. Meanwhile, by using a variational distribution qθpat, ut|st, kq with parameter θ, we
obtain an inequality gpφ, ωq ě Gpφ, ω, θq, where
«

ﬀ

Gpφ, ω, θq “ Erqθ ps1:T ,u1:T ,a1:T ,kq

lφ,ωpst, at, ut, kq ´ log qθpat, ut|st, kq

,

(4)

and rqθps1:T , u1:T , a1:T , kq “ ppkqp1ps1qΠT
t“1ppst`1|st, utqqθpat, ut|st, kq. The lower-bound G resembles the
maximum entropy RL (MaxEnt-RL) (Ziebart et al., 2010). By using the optimality results of MaxEnt-
RL (Levine, 2018), we have an equality gpφ, ωq “ maxθ Gpφ, ω, θq. Therefore, the function gpφ, ωq can
be substituted by maxθ Gpφ, ω, θq.

2We emphasize that IRL (Ng & Russell, 2000) is diﬀerent from RL, since RL learns an optimal policy from a known

reward function.

Tÿ

t“1

5

By using these lower-bounds, we have that maxφ,ω f pφ, ωq ´ gpφ, ωq “ maxφ,ω,ψ Fpφ, ω, ψq ´
maxθ Gpφ, ω, θq “ maxφ,ω,ψ minθ Fpφ, ω, ψq ´ Gpφ, ω, θq. Solving the max-min problem is often feasible
even for large state and action spaces, since Fpφ, ω, ψq and Gpφ, ω, θq are deﬁned as an expectation and
can be optimized straightforwardly. Nevertheless, in practice, we represent the variational distributions by
parameterized functions, and solve the sub-optimization (w.r.t. ψ and θ) by stochastic optimization meth-
ods. However, in this scenario, the equalities f pφ, ωq “ maxψ Fpφ, ω, ψq and gpφ, ωq “ maxθ Gpφ, ω, θq
may not hold for two reasons. First, the optimal variational distributions may not be in the space of our
parameterized functions. Second, stochastic optimization methods may yield local solutions. Nonetheless,
when the variational distributions are represented by deep neural networks, the obtained variational
distributions are often reasonably accurate and the equalities approximately hold (Ranganath et al., 2014).

4.3 Model speciﬁcation
In practice, we are required to specify models for qθpat, ut|st, kq and pωput|st, at, kq. We propose to
use qθpat, ut|st, kq “ qθpat|stqN put|at, Σq and pωput|st, at, kq “ N put|at, Cωpkqq. As shown below, the
choice for qθpat, ut|st, kq enables us to solve the sub-optimization w.r.t. θ by using RL with reward function
rφ. Meanwhile, the choice for pωput|st, at, kq incorporates our prior knowledge that the noisy policy tends
to Gaussian, which is a reasonable assumption for actual human motor behavior (van Beers et al., 2004).
Under these model speciﬁcations, solving maxφ,ω,ψ minθ Fpφ, ω, ψq ´ Gpφ, ω, θq is equivalent to solving
maxφ,ω,ψ minθ Hpφ, ω, ψ, θq, where

”ř
T
t“1

Hpφ, ω, ψ, θq “ Epdps1:T ,u1:T |kqppkq
”ř
T
t“1rφpst, atq´log qθpat|stq

Eqψpat|st,ut,kq

´ Erqθ ps1:T ,a1:T q
ş

”
rφpst, atq´ 1
ı

2 }ut ´ at}2
“

Eppkq

TrpC´1

ı

C´1

ı
`Htpqψq
‰
ω pkqΣq

ω pkq

.

`

T
2

(5)

t“1

Here, rqθps1:T , a1:T q “ p1ps1qΠT
R ppst`1|st, at ` (cid:15)tqN p(cid:15)t|0, Σqd(cid:15)tqθpat|stq is a noisy trajectory density
induced by a policy qθpat|stq, where N p(cid:15)t|0, Σq can be regarded as an approximation of the noisy policy
in Figure 1(b). Minimizing H w.r.t. θ resembles solving a MaxEnt-RL problem with a reward function
rφpst, atq, except that trajectories are collected according to the noisy trajectory density. In other words,
this minimization problem can be solved using RL, and qθpat|stq can be regarded as an approximation
of the optimal policy. The hyper-parameter Σ determines the quality of this approximation: smaller
value of Σ gives a better approximation. Therefore, by choosing a reasonably small value of Σ, solving
the max-min problem yields a reward function rφpat|stq and a policy qθpat|stq. This policy imitates the
optimal policy, which is the goal of IL.

We note that the model assumption for pω incorporates our prior knowledge about the noisy policy
pput|st, at, kq. Namely, pωput|st, at, kq “ N put|at, Cωpkqq assumes that the noisy policy tends to Gaussian,
where the covariance Cωpkq gives an estimated expertise of the k-th demonstrator: High-expertise
demonstrators have small Cωpkq and vice-versa for low-expertise demonstrators. VILD is not restricted
to this choice. Diﬀerent choices of pω incorporate diﬀerent prior knowledge. For example, we may use a
Laplace distribution to incorporate a prior knowledge about demonstrators who tend to execute outlier
actions (Murphy, 2013). In such a case, the squared error in H is simply replaced by the absolute error
(see Appendix A.3).

It should be mentioned that qψpat|st, ut, kq is a maximum-entropy probability density which maximizes
the immediate reward at time t and minimizes the weighted squared error between ut and at. The trade-
oﬀ between the reward and squared-error is determined by the covariance Cωpkq. Speciﬁcally, for
demonstrators with a small Cωpkq (i.e., high-expertise demonstrators), the squared error has a large
magnitude and qψ tends to minimize the squared error. Meanwhile, for demonstrators with a large value
of Cωpkq (i.e., low-expertise demonstrators), the squared error has a small magnitude and qψ tends to
maximize the immediate reward.

In practice, we include a regularization term Lpωq “ T Eppkqrlog |C´1

ω pkq|s{2, to penalize large covari-
ance. Without this regularization, the covariance can be overly large which makes learning degenerate.
We note that H already includes such a penalty via the trace term: EppkqrTrpC´1
ω pkqΣqs. However, the
strength of this penalty tends to be too small, since we choose Σ to be small.

Importance sampling for reward learning

4.4
To
we
∇φtEpdps1:T ,u1:T |kqppkqrΣT

the
importance

improve
use

convergence
sampling

rate
(IS).

Speciﬁcally,
Eqψpat|st,ut,kqrrφpst, atqss ´ Erqθ ps1:T ,a1:T qrΣT

by

analyzing

t“1

of VILD when updating

the

reward parameter φ,
gradient ∇φH “
t“1rφpst, atqsu, we can see that the

the

6

Algorithm 1 VILD: Variational Imitation Learning with Diverse-quality demonstrations
1: Input: Diverse-quality demonstrations Dd “ tps1:T , u1:T , kqnuN
2: while Not converge do
3:
4:
5:
6:

Sample at „ qθpat|stq and (cid:15)t „ N p(cid:15)t|0, Σq.
Execute at ` (cid:15)t in environment and observe next state s1
Include pst, at, s1
tq into the replay buﬀer B. Set t Ð t ` 1.

while |B| ă B with batch size B do

t|st, at ` (cid:15)tq.

t „ pps1

n“1 and a replay buﬀer B “ ∅.

Ź Collect samples from rqθps1:T , a1:T q

7:

8:
9:
10:

Update qψ by an estimate of ∇ψHpφ, ω, ψ, θq.
Update pω by an estimate of ∇ωHpφ, ω, ψ, θq ` ∇ωLpωq.
Update rφ by an estimate of ∇φHISpφ, ω, ψ, θq.
Update qθ by an RL method (e.g., TRPO or SAC) with reward function rφ.

reward function is updated to maximize expected cumulative rewards obtained by demonstrators and qψ,
while minimizing expected cumulative rewards obtained by qθ. However, low-quality demonstrations often
have low reward values. For this reason, stochastic gradients estimated by these demonstrations tend to
be uninformative, which leads to slow convergence and poor data-eﬃciency.

To avoid estimating such uninformative gradients, we use IS to estimate gradients using high-quality
demonstrations which are sampled with high probability. Brieﬂy, IS is a technique for estimating an
expectation over a distribution by using samples from a diﬀerent distribution (Robert & Casella, 2005).
For VILD, we propose to sample k from a distribution ˜ppkq “ zk{ΣK
ω pkqq}1.
This distribution assigns high probabilities to demonstrators with high estimated level of expertise. With
this distribution, the estimated gradients tend to be more informative which leads to a faster convergence.
To reduce a sampling bias, we use a truncated importance weight: wpkq “ minpppkq{˜ppkq, 1q (Ion-
ides, 2008). The distribution ˜ppkq and the importance weight wpkq lead to an IS gradient: ∇φHIS “
∇φtEpdps1:T ,u1:T |kq ˜ppkqrwpkqΣT
Eqψpat|st,ut,kqrrφpst, atqss ´ Erqθ ps1:T ,a1:T qrΣT
t“1rφpst, atqsu. Computing the
importance weight requires ppkq, which can be estimated accurately since k is a discrete random variable.
For simplicity, we assume that ppkq is a uniform distribution. A pseudo-code of VILD with IS is given in
Algorithm 1 and more details of our implementation are given in Appendix B.

k1“1zk1, where zk “ }vecpC´1

t“1

5 Experiments
In this section, we experimentally evaluate the performance of VILD (with and without IS) in Mujoco
tasks from OpenAI Gym (Brockman et al., 2016). Performance is evaluated using cumulative ground-truth
rewards along trajectories (i.e., higher is better), which is computed using 10 test trajectories generated
by learned policies (i.e., qθpat|stq). We repeat experiments for 5 trials with diﬀerent random seeds and
report the mean and standard error.

Baselines & data generation. We compare VILD against GAIL (Ho & Ermon, 2016), AIRL (Fu
et al., 2018), VAIL (Peng et al., 2019), MaxEnt-IRL (Ziebart et al., 2010), and InfoGAIL (Li et al., 2017).
These are online IL methods which collect transition samples to learn policies. We use trust-region policy
optimization (TRPO) (Schulman et al., 2015) to update policies, except for the Humanoid task where we
use soft actor-critic (SAC) (Haarnoja et al., 2018). To generate demonstrations from π‹ (pre-trained by
TRPO) according to Figure 1(b), we use two types of noisy policy pput|at, st, kq: Gaussian noisy policy:
N put|at, σ2
kIq and time-signal-dependent (TSD) noisy policy: N put|at, diagpbkptq ˆ }at}1qq, where bkptq
is sampled from a noise process. We use 10 demonstrators with diﬀerent σk and noise processes for bkptq.
Notice that for TSD, the noise variance depends on time and magnitude of actions. This characteristic of
TSD has been observed in human motor control (van Beers et al., 2004). More details of data generation
are given in Appendix C.

Results against online IL methods. Figure 2 shows learning curves of VILD and existing methods
against the number of transition samples in HalfCheetah and Ant3, whereas Table 1 reports the performance
achieved in the last 100 update iterations. We can see that VILD with IS outperforms existing methods
in terms of both data-eﬃciency and ﬁnal performance, i.e., VILD with IS learns better policies using less
numbers of transition samples. VILD without IS tends to outperform existing methods in terms of the
ﬁnal performance. However, it is less data-eﬃcient when compared to VILD with IS, except on Humanoid
with the Gaussian noisy policy, where VILD without IS performs better than VILD with IS in terms of
the ﬁnal performance. We conjecture that this is because IS slightly biases gradient estimation, which

3Learning curves of other tasks are given in Appendix D.

7

(a) Performan when demonstrations are generated using
Gaussian noisy policy.

(b) Performan when demonstrations are generated using
TSD noisy policy.

Figure 2: Performance averaged over 5 trials in terms of the mean and standard error. Demonstrations
are generated by 10 demonstrators using (a) Gaussian and (b) TSD noisy policies. Horizontal dotted lines
indicate performance of k “ 1, 3, 5, 7, 10 demonstrators. IS denotes importance sampling.

may have a negative eﬀect on the performance. Nonetheless, the overall good performance of VILD with
IS suggests that it is an eﬀective method to handle diverse-quality demonstrations.

On the contrary, existing methods perform poorly overall. We found that InfoGAIL, which learns
a context-dependent policy, can achieve good performance when the policy is conditioned on speciﬁc
contexts. However, its performance is quite poor on average when using contexts sampled from a (uniform)
prior distribution. These results supports our conjecture that existing methods are not suitable for
diverse-quality demonstrations when the level of demonstrators’ expertise in unknown.

It can be seen that VILD without IS performs better for the Gaussian noisy policy when compared to
the TSD noisy policy. This is because the model of VILD is correctly speciﬁed for the Gaussian noisy
policy, but the model is incorrectly speciﬁed for the TSD noisy policy; misspeciﬁed model indeed leads to
the reduction in performance. Nonetheless, VILD with IS still perform well for both types of noisy policy.
This is perhaps because negative eﬀects of a misspeciﬁed model is not too severe for learning expertise
parameters, which are required to compute rppkq.

We also conduct the following evaluations. Due to space limitation, ﬁgures are given in Appendix D.

Results against oﬄine IL methods. We compare VILD against oﬄine IL methods based on super-
vised learning, namely behavior cloning (BC) (Pomerleau, 1988), Co-Teaching which is based on a noisy
label learning method (Han et al., 2018), and BC from diverse-quality demonstrations (BC-D) which opti-
mizes the naive model described in Section 4.1. Results in Figure 5 show that these methods perform worse
than VILD overall; BC performs the worst since it severely suﬀers from both the compounding error and
low-quality demonstrations. BC-D and Co-teaching are quite robust against low-quality demonstrations,
but they perform poorly due to the issue of compounding error.

Accuracy of estimated expertise parameter. To evaluate accuracy of estimated expertise parameter,
we compare the ground-truth value of σk under the Gaussian noisy policy against the learned covariance
Cωpkq. Figure 6 shows that VILD learns an accurate ranking of demonstrators’ expertise. The values
of these parameters are also quite accurate compared to the ground-truth, except for demonstrators
with low-levels of expertise. A reason for this phenomena is that low-quality demonstrations are highly
dissimilar, which makes learning the expertise more challenging.

6 Conclusion and Future Work

In this paper, we explored a practical setting of IL where demonstrations have diverse-quality. We showed
the deﬁciency of existing methods, and proposed a robust method called VILD which learns both the
reward function and the level of demonstrators’ expertise by using the variational approach. Empirical
results demonstrated that our work enables scalable and data-eﬃcient IL under this practical setting. In
future, we will explore other approaches to eﬃciently estimate parameters of the proposed model except
the variational approach.

8

Table 1: Performance in the last 100 iterations in terms of the mean and standard error of cumulative
rewards (higher is better). (G) denotes Gaussian noisy policy and (TSD) denotes time-signal-dependent
noisy policy. Boldfaces indicate best and comparable methods according to t-test with p-value 0.01. The
performance of VAIL is similar to that of GAIL and is omitted.

Task
HalfCheetah (G)
HalfCheetah (TSD)
Ant (G)
Ant (TSD)
Walker2d (G)
Walker2d (TSD)
Humanoid (G)
Humanoid (TSD)

VILD (IS) VILD (w/o IS)
1848 (429)
4559 (43)
1159 (594)
4394 (136)
3719 (65)
1426 (81)
1072 (134)
3396 (64)
3470 (300)
2132 (64)
1244 (132)
3115 (130)
4840 (56)
3781 (557)
3610 (448)
4600 (97)

AIRL
341 (177)
-304 (51)
1417 (184)
1357 (59)
1534 (99)
578 (47)
4274 (93)
4212 (121)

GAIL MaxEnt-IRL
1192 (245)
177 (132)
731 (93)
775 (135)
1795 (172)
752 (112)
3038 (731)
4132 (651)

551 (23)
318 (134)
209 (30)
97 (161)
1410 (115)
834 (84)
284 (24)
203 (31)

InfoGAIL
1244 (210)
2664 (779)
675 (36)
1076 (140)
1668 (82)
1041 (36)
4047 (653)
3962 (635)

References

ISSN 0885-6125.

Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, , and Kevin Murphy. Deep variational information

bottleneck. In International Conference on Learning Representations (ICLR), 2017.

Dana Angluin and Philip Laird. Learning from noisy examples. Machine Learning, 2(4):343–370, 1988.

Devansh Arpit, Stanislaw K. Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S.
Kanwal, Tegan Maharaj, Asja Fischer, Aaron C. Courville, Yoshua Bengio, and Simon Lacoste-Julien. A
closer look at memorization in deep networks. In ICML, volume 70 of Proceedings of Machine Learning
Research, pp. 233–242. PMLR, 2017.

Julien Audiﬀren, Michal Valko, Alessandro Lazaric, and Mohammad Ghavamzadeh. Maximum entropy

semi-supervised inverse reinforcement learning. In IJCAI, pp. 3315–3321. AAAI Press, 2015.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and

Wojciech Zaremba. OpenAI Gym. CoRR, abs/1606.01540, 2016.

Daniel S. Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond suboptimal
In Proceedings of the 36th

demonstrations via inverse reinforcement learning from observations.
International Conference on Machine Learning, ICML, pp. 783–792, 2019.

Chelsea Finn, Paul F. Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative
adversarial networks, inverse reinforcement learning, and energy-based models. CoRR, abs/1611.03852,
2016a. URL http://arxiv.org/abs/1611.03852.

Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via
policy optimization. In Proceedings of the 33nd International Conference on Machine Learning, pp.
49–58, 2016b. URL http://jmlr.org/proceedings/papers/v48/finn16.html.

Lex Fridman, Daniel E. Brown, Michael Glazer, William Angell, Spencer Dodd, Benedikt Jenik, Jack
Terwilliger, Julia Kindelsberger, Li Ding, Sean Seaman, Hillary Abraham, Alea Mehler, Andrew
Sipperley, Anthony Pettinato, Bobbie Seppelt, Linda Angell, Bruce Mehler, and Bryan Reimer. MIT
autonomous vehicle technology study: Large-scale deep learning based analysis of driver behavior and
interaction with automation. CoRR, abs/1711.06976, 2017.

Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement

learning. 2018.

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in Neural Information
Processing Systems 27, pp. 2672–2680, 2014.

Shixiang (Shane) Gu, Timothy Lillicrap, Richard E Turner, Zoubin Ghahramani, Bernhard Schölkopf,
and Sergey Levine. Interpolated policy gradient: Merging on-policy and oﬀ-policy gradient estimation
In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
for deep reinforcement learning.

9

S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp.
3846–3855. Curran Associates, Inc., 2017.

Ishaan Gulrajani, Faruk Ahmed, Martín Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Improved
Training of Wasserstein GANs. In Advances in Neural Information Processing Systems 30, pp. 5769–5779,
2017.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Oﬀ-policy maximum
entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International
Conference on Machine Learning, ICML, pp. 1856–1865, 2018.

Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W. Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
Advances in Neural Information Processing Systems 31, pp. 8536–8546, 2018.

Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gaurav S. Sukhatme, and Joseph J. Lim. Multi-modal
imitation learning from unstructured demonstrations using generative adversarial nets. In Advances in
Neural Information Processing Systems 30, pp. 1235–1245, 2017.

Jonathan Ho and Stefano Ermon. Generative Adversarial Imitation Learning. In Advances in Neural

Information Processing Systems 29, pp. 4565–4573, 2016.

Matthew D. Hoﬀman and David M. Blei. Stochastic structured variational inference. In Proceedings of

the Eighteenth International Conference on Artiﬁcial Intelligence and Statistics, AISTATS, 2015.

Edward L Ionides. Truncated importance sampling. Journal of Computational and Graphical Statistics,

17(2):295–311, 2008.

E. T. Jaynes. Information theory and statistical mechanics. Physical Review, 106, 1957.

Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. An introduction to
variational methods for graphical models. Machine Learning, 37(2):183–233, November 1999. ISSN
0885-6125.

Ashish Khetan, Zachary C. Lipton, and Animashree Anandkumar. Learning from noisy singly-labeled

data. In 6th International Conference on Learning Representations ICLR, 2018.

Kyungjae Lee, Sungjoon Choi, and Songhwai Oh. Inverse reinforcement learning with leveraged gaussian
In IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS, pp.

processes.
3907–3912, 2016.

Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. CoRR,

abs/1805.00909, 2018. URL http://arxiv.org/abs/1805.00909.

Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end Training of Deep Visuomotor
Policies. Journal of Machine Learning Research, 17(1):1334–1373, January 2016. ISSN 1532-4435.

Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from visual
demonstrations. In Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, pp. 3815–3825, 2017.

Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Booher, Max Spero, Albert Tung, Julian Gao, John
Emmons, Anchit Gupta, Emre Orbay, Silvio Savarese, and Li Fei-Fei. ROBOTURK: A crowdsourcing
platform for robotic skill learning through imitation. In CoRL, volume 87 of Proceedings of Machine
Learning Research, pp. 879–893. PMLR, 2018.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare,
Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie,
Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and
Demis Hassabis. Human-Level Control Through Deep Reinforcement Learning. Nature, 518(7540):
529–533, February 2015. ISSN 00280836.

Kevin P. Murphy. Machine learning : a probabilistic perspective. MIT Press, Cambridge, Mass. [u.a.],

2013.

10

Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy

labels, 2013. URL http://papers.nips.cc/paper/5073-learning-with-noisy-labels.pdf.

Andrew Y. Ng and Stuart J. Russell. Algorithms for Inverse Reinforcement Learning. In Proceedings of

the 17th International Conference on Machine Learning, pp. 663–670, 2000.

Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J. Andrew Bagnell, Pieter Abbeel, and Jan Peters. An
algorithmic perspective on imitation learning. Foundations and Trends in Robotics, 7(1-2):1–179, 2018.

Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine. Variational discriminator
bottleneck: Improving imitation learning, inverse RL, and GANs by constraining information ﬂow. In
International Conference on Learning Representations (ICLR), 2019.

Dean Pomerleau. ALVINN: an autonomous land vehicle in a neural network. In Advances in Neural
Information Processing Systems 1, [NIPS Conference, Denver, Colorado, USA, 1988], pp. 305–313,
1988.

Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley

& Sons, Inc., New York, NY, USA, 1st edition, 1994. ISBN 0-471-61977-9.

Rajesh Ranganath, Sean Gerrish, and David M. Blei. Black box variational inference. In Proceedings
of the Seventeenth International Conference on Artiﬁcial Intelligence and Statistics, AISTATS, pp.
814–822, 2014.

Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni,
and Linda Moy. Learning from crowds. Journal of Machine Learning Research, 11:1297–1322, 2010.

Christian P. Robert and George Casella. Monte Carlo Statistical Methods. Springer-Verlag, Berlin,

Heidelberg, 2005. ISBN 0387212396.

Stephane Ross and Drew Bagnell. Eﬃcient reductions for imitation learning. In Yee Whye Teh and
Mike Titterington (eds.), Proceedings of the 13th International Conference on Artiﬁcial Intelligence and
Statistics, AISTATS, volume 9 of Proceedings of Machine Learning Research, pp. 661–668, Chia Laguna
Resort, Sardinia, Italy, 13–15 May 2010. PMLR.

Stuart Russell. Learning agents for uncertain environments (extended abstract). In Proceedings of the
Eleventh Annual Conference on Computational Learning Theory, COLT’ 98, pp. 101–103. ACM, 1998.
ISBN 1-58113-057-0.

Stefan Schaal. Is imitation learning the route to humanoid robots? 3(6):233–242, 1999. clmc.

Yannick Schroecker, Mel Vecerik, and Jon Scholz. Generative predecessor models for sample-eﬃcient
In International Conference on Learning Representations, 2019. URL https:

imitation learning.
//openreview.net/forum?id=SkeVsiAcYm.

John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust Region Policy
Optimization. In Proceedings of the 32nd International Conference on Machine Learning, July 6-11,
2015, Lille, France, 2015.

Kyriacos Shiarlis, João V. Messias, and Shimon Whiteson. Inverse Reinforcement Learning from Failure.
In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pp.
1060–1068, 2016.

David Silver, J. Andrew Bagnell, and Anthony Stentz. Learning autonomous driving styles and maneu-
vers from expert demonstration. In Experimental Robotics - The 13th International Symposium on
Experimental Robotics, ISER, pp. 371–386, 2012.

David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach,
Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the Game of Go with Deep Neural
Networks and Tree Search. Nature, 529(7587):484–489, 2016.

11

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui,
Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the Game of
Go Without Human Knowledge. Nature, 550(7676):354–359, October 2017. ISSN 00280836.

Richard S. Sutton and Andrew G. Barto. Reinforcement Learning - an Introduction. Adaptive computation

and machine learning. MIT Press, 1998.

Umar Syed, Michael H. Bowling, and Robert E. Schapire. Apprenticeship learning using linear programming.
In Proceedings of the 25th International Conference on Machine Learning, pp. 1032–1039, 2008. doi:
10.1145/1390156.1390286.

Istvan Szita and Csaba Szepesvári. Model-based reinforcement learning with nearly tight exploration
complexity bounds. In Proceedings of the 27th International Conference on Machine Learning ICML,
pp. 1031–1038, 2010.

G. E. Uhlenbeck and L. S. Ornstein. On the theory of the brownian motion. Physical Revview, 36:823–841,

1930. doi: 10.1103/PhysRev.36.823.

Robert J. van Beers, Patrick Haggard, and Daniel M. Wolpert. The role of execution noise in movement
variability. Journal of Neurophysiology, 91(2):1050–1063, 2004. doi: 10.1152/jn.00652.2003. URL
https://doi.org/10.1152/jn.00652.2003. PMID: 14561687.

Ziyu Wang, Josh Merel, Scott E. Reed, Nando de Freitas, Gregory Wayne, and Nicolas Heess. Robust
imitation of diverse behaviors. In Advances in Neural Information Processing Systems 30, pp. 5320–5329,
2017.

Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi Sugiyama. Imitation
learning from imperfect demonstration. In Proceedings of the 36th International Conference on Machine
Learning, ICML, 2019.

Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey. Modeling Interaction via the Principle of
Maximum Causal Entropy. In Proceedings of the 27th International Conference on Machine Learning,
June 21-24, 2010, Haifa, Israel, 2010.

This section derives the lower-bounds of f pφ, ωq and gpφ, ωq presented in the paper. We also derive the
objective function Hpφ, ω, ψ, θq of VILD.

A Derivations

A.1 Lower-bound of f

”ř

rφpst, atq ` log pωput|st, at, kq,
lφ,ωpst, at, ut, kq
“
ı
T
, where ftpφ, ωq “ log
t“1 ftpφ, ωq

Let
“
ş
Epdps1:T ,u1:T |kqppkq
A exp plφ,ωpst, at, ut, kqq dat. By using a
variational distribution qψpat|st, ut, kq with parameter ψ, we can bound ftpφ, ωq from below by using the
Jensen inequality as follows:

f pφ, ωq

have

that

we

ˆż

A

ftpφ, ωq “ log

exp plφ,ωpst, at, ut, kqq

ż

A

ˆ

˙

qψpat|st, ut, kq
qψpat|st, ut, kq

dat

ě

qψpat|st, ut, kq log

exp plφ,ωpst, at, ut, kqq

“ Eqψpat|st,ut,kq rlφ,ωpst, at, ut, kq ´ log qψpat|st, ut, kqs
“ Ftpφ, ω, ψq.

1
qψpat|st, ut, kq

˙

dat

Then, by using the linearity of expectation, we obtain the lower-bound of f pφ, ωq as follows:

f pφ, ωq ě Epdps1:T ,u1:T |kqppkq

“ Epdps1:T ,u1:T |kqppkq
“ Fpφ, ω, ψq.

ı
”ř
T
t“1Ftpφ, ω, ψq
”ř
T
t“1

ı
Eqψpat|st,ut,kq rlφ,ωpst, at, ut, kq ´ log qψpat|st, ut, kqs

(6)

(7)

12

To verify that f pφ, ωq “ maxψ Fpφ, ω, ψq, we maximize Ftpφ, ω, ψq w.r.t. qψ under the constraint
A qψpat|st, ut, kqdat “ 1. By setting

that qψ is a valid probability density, i.e., qψpat|st, ut, kq ą 0 and
the derivative of Ftpφ, ω, ψq w.r.t. qψ to zero, we obtain

ş

qψpat|st, ut, kq “ exp plφ,ωpst, at, ut, kq ´ 1q
exp plφ,ωpst, at, ut, kqq
A exp plφ,ωpst, at, ut, kqq dat
ş
A qψpat|st, ut, kqdat “ 1. To show that this is indeed the

“

ş

,

where the last line follows from the constraint
maximizer, we substitute qψ‹ pat|st, ut, kq “ expplpst,at,ut,kqq
ş
A expplpst,at,ut,kqqdat

into Ftpφ, ω, ψq:

Ftpφ, ω, ψ‹q “ Eq‹

ψpat|st,ut,kq rlφ,ωpst, at, ut, kq ´ log qψ‹pat|st, ut, kqs
ˆż

˙

“ log

exp plφ,ωpst, at, ut, kqq dat

.

A

This equality veriﬁes that ftpφ, ωq “ maxψ Ftpφ, ω, ψq. Finally, by using the linearity of expectation, we
have that f pφ, ωq “ maxψ Fpφ, ω, ψq.

A.2 Lower-bound of g

Next, we derive the lower-bound of gpφ, ωq presented in the paper. We ﬁrst derive a trivial lower-bound
using a general variational distribution over trajectories and reveal its issues. Then, we derive a lower-
bound stated presented in the paper by using a structured variational distribution. Recall that the
function gpφ, ωq “ log Zφ,ω is

gpφ, ωq “ log

ppkq

¨ ¨ ¨

p1ps1q

ppst`1|st, utq exp plpst, at, ut, kqq ds1:T du1:T da1:T

¨

˚
˝

Kÿ

k“1

ż

ż

pSˆAˆAqT

Tź

t“1

˛

‹
‚.

Lower-bound via a variational distribution A lower-bound of g can be obtained by using a
variational distribution sqβps1:T , u1:T , a1:T , kq with parameter β. We note that this variational distribution
allows any dependency between the random variables s1:T , u1:T , a1:T , and k. By using this distribution,
we have a lower-bound

gpφ, ωq “ log

ppkq

¨ ¨ ¨

p1ps1q

ppst`1|st, utq exp plφ,ωpst, at, ut, kqq

˜

Kÿ

ż

ż

Tź

t“1

k“1

pSˆAˆAqT
sqβps1:T , u1:T , a1:T , kq
sqβps1:T , u1:T , a1:T , kq

ˆ

«

¸

ds1:T du1:T da1:T

Tÿ

t“1

ﬀ

´ log sqβps1:T , u1:T , a1:T , kq

sGpφ, ω, βq.

“

ě Esqβps1:T ,u1:T ,a1:T ,kq

log ppkqp1ps1q `

tlog ppst`1|st, utq ` lφ,ωpst, at, ut, kqu

The main issue of using this lower-bound is that, sGpφ, ω, βq can be computed or approximated only when
we have an access to the transition probability ppst`1|st, utq. In many practical tasks, the transition
probability is unknown and needs to be approximated. However, approximating the transition probability
for large state and action spaces is known to be highly challenging (Szita & Szepesvári, 2010). For these
reasons, this lower-bound is not suitable for our method.

Lower-bound via a structured variational distribution To avoid the above issue, we use the
structure variational approach (Hoﬀman & Blei, 2015), where the key idea is to pre-deﬁne conditional
dependenc to ease computation. Speciﬁcally, we use a variational distribution qθpat, ut|st, kq with

(8)

13

parameter θ and deﬁne dependencies between states according to the transition probability of MDPs.
With this variational distribution, we lower-bound g as follows:

gpφ, ωq “ log

ppkq

¨ ¨ ¨

p1ps1q

ppst`1|st, utq exp plφ,ωpst, at, ut, kqq

˜

Kÿ

ż

ż

k“1

pSˆAˆAqT

ˆ

qθpat, ut|st, kq
qθpat, ut|st, kq
«

Tź

t“1

¸

ds1:T du1:T da1:T

ě Erqθ ps1:T ,u1:T ,a1:T ,kq

lφ,ωpst, at, ut, kq ´ log qθpat, ut|st, kq

Tÿ

t“1

“ Gpφ, ω, θq,

ﬀ

where rqθps1:T , u1:T , a1:T , kq “ ppkqp1ps1qΠT
t“1ppst`1|st, utqqθpat, ut|st, kq. The optimal variational distri-
bution qθ‹ pat, ut|st, kq can be founded by maximizing Gpφ, ω, θq w.r.t. qθ. Solving this maximization
problem is identical to solving a maximum entropy RL (MaxEnt-RL) problem (Ziebart et al., 2010) for
an MDP deﬁned by a tuple M “ pS ˆ N`, A ˆ A, pps1, |s, uqIk“k1, p1ps1qppk1q, lφ,ωps, a, u, kqq. Speciﬁ-
cally, this MDP is deﬁned with a state variable pst, ktq P S ˆ N, an action variable pat, utq P A ˆ A, a
transition probability density ppst`1, |st, utqIkt“kt`1 , an initial state density p1ps1qppk1q, and a reward
function lφ,ωpst, at, ut, kq. Here, Ia“b is the indicator function which equals to 1 if a “ b and 0 other-
wise. By adopting the optimality results of MaxEnt-RL (Ziebart et al., 2010; Levine, 2018), we have
gpφ, ωq “ maxθ Gpφ, ω, θq, where the optimal variational distribution is

The functions Q and V are soft-value functions deﬁned as

qθ‹ pat, ut|st, kq “ exppQpst, k, at, utq ´ V pst, kqq.

Qpst, k, at, utq “ lφ,ωpst, at, ut, kq ` Eppst`1|st,utq rV pst`1, kqs ,

V pst, kq “ log

exp pQpst, k, at, utqq datdut.

ĳ

AˆA

(9)

(10)

(11)

(12)

A.3 Objective function H of VILD

This section derives the objective function Hpφ, ω, ψ, θq from Fpφ, ω, ψq ´ Gpφ, ω, θq. Specﬁcally, we
substitute the models pωput|st, at, kq “ N put|at, Cωpkqq and qθpat, ut|st, kq “ qθpat|stqN put|at, Σq. We
also give an example when using a Laplace distribution for pωput|st, at, kq instead of the Gaussian
distribution.

First, we substitute qθpat, ut|st, kq “ qθpat|stqN put|at, Σq into G:

ﬀ

Gpφ, ω, θq “ Erqθ ps1:T ,u1:T ,a1:T ,kq

lφ,ωpst, at, ut, kq ´ log N put|at, Σq ´ log qθpat|stq
ﬀ

“ Erqθ ps1:T ,u1:T ,a1:T ,kq

lφ,ωpst, at, ut, kq `

}ut ´ at}2

Σ´1 ´ log qθpat|stq

` c1,

1
2

«

Tÿ

«

t“1
Tÿ

t“1

where c1 is a constant corresponding to the log-normalization term of the Gaussian distribution. Next, by
using the re-parameterization trick, we rewrite rqθps1:T , u1:T , a1:T , kq as

rqθps1:T , u1:T , a1:T , kq “ ppkqp1ps1q

p1pst`1|st, at ` Σ1{2(cid:15)tqN p(cid:15)t|0, Iqqθpat|stq,

where we use ut “ at ` Σ1{2(cid:15)t with (cid:15)t „ N p(cid:15)t|0, Iq. With this, the expectation of ΣT
rqθps1:T , u1:T , a1:T , kq can be written as

t“1}ut ´ at}2

Σ´1 over

Erqθ ps1:T ,u1:T ,a1:T ,kq

}ut ´ at}2

Σ´1

“ Erqθ ps1:T ,u1:T ,a1:T ,kq

}at ` Σ1{2(cid:15)t ´ at}2

Σ´1

«

Tÿ

t“1

ﬀ

ﬀ

«

Tÿ

«

t“1
Tÿ

t“1

“ Erqθ ps1:T ,u1:T ,a1:T ,kq

}Σ1{2(cid:15)t}2

Σ´1

Tź

t“1

ﬀ

“ T da,

14

ﬀ

ﬀ

ﬀ

«

Tÿ

t“1

«

Tÿ

t“1

«

Tÿ

t“1

which is a constant. Then, the quantity G can be expressed as

Gpφ, ω, θq “ Erqθ ps1:T ,u1:T ,a1:T ,kq

lφ,ωpst, at, ut, kq ´ log qθpat|stq

` c1 ` T da.

By ignoring the constant, the optimization problem maxφ,ω,ψ minθ Fpφ, ω, ψq ´ Gpφ, ω, θq is equivalent
to

ﬀ

max
φ,ω,ψ

min
θ

Epdps1:T ,u1:T ,kq

Eqψpat|st,ut,kq rlφ,ωpst, at, ut, kq ´ log qψpat|st, ut, kqs

´ Erqθ ps1:T ,u1:T ,a1:T ,kq

lφ,ωpst, at, ut, kq ´ log qθpat|stq

.

(13)

Our next step is to substitute pωput|st, at, kq by our choice of model. First, let us consider a Gaussian
distribution pωput|st, at, kq “ N put|at, Cωpst, kqq, where the covariance depends on state. With this
model, the second term in Eq. (13) is given by
«

ﬀ

Erqθ ps1:T ,u1:T ,a1:T ,kq

lφ,ωpst, at, ut, kq ´ log qθpat|stq

Tÿ

t“1
«

Tÿ

«

t“1
Tÿ

t“1

as follows:
«

Tÿ

t“1

«

Tÿ

t“1

“ Erqθ ps1:T ,u1:T ,a1:T ,kq

rφpst, atq ` log N put|at, Cωpst, kqq ´ log qθpat|stq

“ Erqθ ps1:T ,u1:T ,a1:T ,kq

rφpst, atq ´

}ut ´ at}2

ω pst,kq ´
C´1

log |Cωpst, kq| ´ log qθpat|stq

` c2,

1
2

where c2 “ ´ da
t“1}ut ´ at}2
ΣT

2 log 2π is a constant. By using the reparameterization trick, we write the expectation of
C´1
ω pst,kq

1
2

ﬀ

ﬀ

ﬀ

Erqθ ps1:T ,u1:T ,a1:T ,kq

}ut ´ at}2

C´1

ω pst,kq

“ Erqθ ps1:T ,u1:T ,a1:T ,kq

}at ` Σ1{2(cid:15)t ´ at}2
ﬀ

C´1

ω pst,kq

“ Erqθ ps1:T ,u1:T ,a1:T ,kq

}Σ1{2(cid:15)t}2

C´1

ω pst,kq

.

«

Tÿ

«

t“1
Tÿ

t“1

Using this equality, the second term in Eq. (13) is given by

Erqθ ps1:T ,u1:T ,a1:T ,kq

rφpst, atq ´ log qθpat|stq ´

´
}Σ1{2(cid:15)t}2

¯
ω pst,kq ` log |Cωpst, kq|
C´1

1
2

ﬀ

.

(14)

Maximizing this quantity w.r.t. θ has an implication as follows: qθpat|stq is maximum entropy policy
which maximizes expected cumulative rewards while avoiding states that are diﬃcult for demonstrators.
Speciﬁcally, a large value of Eppkq rlog |Cωpst, kq|s indicates that demonstrators have a low level of expertise
for state st on average, given by our estimated covariance. In other words, this state is diﬃcult to accurately
execute optimal actions for all demonstrators on averages. Since the policy qθpat|stq should minimize
Eppkq rlog |Cωpst, kq|s, the policy should avoid states that are diﬃcult for demonstrators. We expect that
this property may improve exploration-exploitation trade-oﬀ. Still, such a property is not in the scope of
this paper, and we leave it for future work.

In this paper, we assume that the covariance does not depend on state: Cωpst, kq “ Cωpkq. This

model enables us to simplify Eq. (14) as follows:

Erqθ ps1:T ,u1:T ,a1:T ,kq

rφpst, atq ´ log qθpat|stq ´

ω pkq ` log |Cωpkq|
C´1

“ Erqθ ps1:T ,u1:T ,a1:T ,kq

rφpst, atq ´ log qθpat|stq

´

EppkqN p(cid:15)|0,Iq

”
}Σ1{2(cid:15)}2

ı
ω pkq ` log |Cωpkq|
C´1

´
}Σ1{2(cid:15)t}2

1
2
ﬀ

T
2

ﬀ

¯

“ Erqθ ps1:T ,a1:T q

rφpst, atq ´ log qθpat|stq

´

“

TrpC´1

‰
ω pkqΣq ` log |Cωpkq|

,

Eppkq

«

Tÿ

t“1
«

Tÿ

t“1

«

Tÿ

t“1

ﬀ

T
2

15

where rqθps1:T , a1:T q “ p1ps1q
from the quadratic form identity: EN p(cid:15)t|0,Iq
pωput|st, at, kq “ N put|at, Cωpkqq into the ﬁrst term of Eq. (13).

”
}Σ1{2(cid:15)t}2

ppst`1|st, at ` (cid:15)tqN p(cid:15)t|0, Σqd(cid:15)tqθpat|stq. The last line follows
ω pkqΣq. Next, we substitute

“ TrpC´1

ω pkq

C´1

ı

ş

ś
T
t“1

ﬀ

Epdps1:T ,u1:T ,kq

Eqψpat|st,ut,kq rlφ,ωpst, at, ut, kq ´ log qψpat|st, ut, kqs

«

Tÿ

t“1
«

Tÿ

“ Epdps1:T ,u1:T ,kq

Eqψpat|st,ut,kq
ﬀ
ı
´ log qψpat|st, ut, kq

t“1

´ T da log 2π{2.

”
rφpst, atq ´

1
2

}ut ´ at}2

ω pkq ´
C´1

log |Cωpkq|

1
2

Lastly, by ignoring constants, Eq. (13) is equivalent to maxφ,ω,ψ,θ Hpφ, ω, ψ, θq, where

Hpφ, ω, ψ, θq “ Epdps1:T ,u1:T ,kq

Eqψpat|st,ut,kq

}ut ´ at}2

ω pkq ´ log qψpat|st, ut, kq
C´1

´ Erqθ ps1:T ,a1:T q

rφpst, atq ´ log qθpat|stq

`

“
TrpC´1

‰
ω pkqΣq

.

Eppkq

„

rφpst, atq ´
ﬀ

1
2

T
2

This concludes the derivation of VILD.

As mentioned, other distributions beside the Gaussian distribution can be used for pω. For instance, let
us consider a multivariate-independent Laplace distribution: pωput|st, at, kq “ Πda
}1q,
d“1
where a division of vector by vector denotes element-wise division. The Laplace distribution has heavier
tails when compared to the Gaussian distribution, which makes the Laplace distribution more suitable
for modeling demonstrators who tend to execute outlier actions. By using the Laplace distribution for
pωput|st, at, kq, we obtain an objective

expp´} ut´at

1
2cpdq
k

ck

HLap. “ Epdps1:T ,u1:T ,kq

Eqψpat|st,ut,kq

´ Erqθ ps1:T ,a1:T q

rφpst, atq ´ log qθpat|stq

`

„

rφpst, atq ´
ﬀ

ﬀ

(cid:13)
(cid:13)
(cid:13)1

(cid:13)
(cid:13)
(cid:13)

ut ´ at
ck
?
T
?

2
π

´ log qψpat|st, ut, kq

”
TrpC´1

ı
ω pkqΣ1{2q

.

Eppkq

We cann see that diﬀerences between HLap and H are the absolute error and scaling of the trace term.

«

Tÿ

«

t“1
Tÿ

t“1

«

Tÿ

«

t“1
Tÿ

t“1

(15)

ﬀ

B Implementation details

We implement VILD using the PyTorch deep learning framework. For all function approximators, we
use neural networks with 2 hidden-layers of 100 tanh units, except for the Humanoid task where we use
neural networks with 2 hidden-layers of 100 relu units. We optimize parameters φ, ω, and ψ by Adam
with step-size 3 ˆ 10´4, β1 “ 0.9, β2 “ 0.999 and mini-batch size 256. To optimize the policy parameter
θ, we use trust-region policy optimization (TRPO) (Schulman et al., 2015) with batch size 1000, except
on the Humanoid task where we use soft actor-critic (SAC) (Haarnoja et al., 2018) with mini-batch
size 256; TRPO is an on-policy RL method that uses only trajectories collected by the current policy,
while SAC is an oﬀ-policy RL method that use trajectories collected by previous policies. On-policy
methods are generally more stable than oﬀ-policy methods, while oﬀ-policy methods are generally more
data-eﬃcient (Gu et al., 2017). We use SAC for Humanoid mainly due to its high data-eﬃciency. When
SAC is used, we also use trajectories collected by previous policies to approximate the expectation over
the trajectory density ˜qθps1:T , a1:T q.

k“1 with ck P Rda

For the distribution pωput|st, at, kq “ N put, at, Cωpkqq, we use diagonal covariances Cωpkq “ diagpckq,
where ω “ tckuK
` are parameter vectors to be learned. For the distribution qψpat|st, ut, kq,
we use a Gaussian distribution with diagonal covariance, where the mean and logarithm of the standard
deviation are the outputs of neural networks. Since k is a discrete variable, we represent qψpat|st, ut, kq
by neural networks that have K output heads and take input vectors pst, utq; The k-th output head
corresponds to (the mean and log-standard-deviation of) qψpat|st, ut, kq. We also pre-train the mean
function of qψpat|st, ut, kq, by performing least-squares regression for 1000 gradient steps with target

16

value ut. This pre-training is done to obtain reasonable initial predictions. For the policy qθpat|stq, we
use a Gaussian policy with diagonal covariance, where the mean and logarithm of the standard deviation
are outputs of neural networks. We use Σ “ 10´8I in experiments.

To control exploration-exploitation trade-oﬀ, we use an entropy coeﬃcient α “ 0.0001 in TRPO. In
SAC, we tune the value of α by optimization, as described in the SAC paper. Note that including α in
1
VILD is equivalent to rescaling quantities in the model by α, i.e., expprφpst, atq{αq and ppωput|st, at, kqq
α .
A discount factor 0 ă γ ă 1 may be included similarly, and we use γ “ 0.99 in experiments.

For all methods, we regularize the reward/discriminator function by the gradient penalty (Gulrajani
et al., 2017) with coeﬃcient 10, since it was previously shown to improve performance of generative
adversarial learning methods. For methods that learn a reward function, namely VILD, AIRL, and
MaxEnt-IRL, we apply a sigmoid function to the output of reward function to control the bounds of
reward function. We found that without controlling the bounds, reward values can be highly negative in
the early stage of learning, which makes learning the policy by RL very challenging. A possible explanation
is that, in MDPs with large state and action spaces, distribution of demonstrations and distribution of
agent’s trajectories are not overlapped in the early stage of learning. In such a scenario, it is trivial to
learn a reward function which tends to positive-inﬁnity values for demonstrations and negative-inﬁnity
values for agent’s trajectories. While the gradient penalty regularizer slightly remedies this issue, we found
that the regularizer alone is insuﬃcient to prevent this scenario.

C Experimental Details

In this section, we describe experimental settings and data generation. We also give brief reviews of
methods compared against VILD in the experiments.

C.1 Settings and data generation

We evaluate VILD on four continuous control tasks from OpenAI gym platform (Brockman et al., 2016)
with the Mujoco physics simulator: HalfCheetah, Ant, Walker2d, and Humanoid. To obtain the optimal
policy for generating demonstrations, we use the ground-truth reward function of each task to pre-train
π‹ with TRPO. We generate diverse-quality demonstrations by using K “ 10 demonstrators according to
the graphical model in Figure 1(b). We consider two types of the noisy policy pput|st, at, kq: a Gaussian
noisy policy and a time-signal-dependent (TSD) noisy policy.

Gaussian noisy policy. We use a Gaussian noisy policy N put|at, σ2
kIq with a constant covariance. The
value of σk for each of the 10 demonstrators is 0.01, 0.05, 0.1, 0.25, 0.4, 0.6, 0.7, 0.8, 0.9 and 1.0, respectively.
Note that our model assumption on pω corresponds to this Gaussian noisy policy. Table 2 shows the
performance of demonstrators (in terms of cumulative ground-truth rewards) with this Gaussian noisy
policy.

TSD noisy policy. To make learning more challenging, we gen-
erate demonstrations by simulating characteristics of human mo-
tor control (van Beers et al., 2004), where actuator noises are
proportion to the magnitude of actuators, and noise’s strength
increases with execution time (van Beers et al., 2004). Speciﬁ-
cally, we generate demonstrations using a Gaussian distribution
N put|at, diagpbkptq ˆ }at}1{daqq, where the covariance is propor-
tion to the magnitude of action and depends on time step. We
call this policy time-signal-dependent (TSD) noisy policy. Here,
bkptq is a sample of a noise process whose noise variance increases
over time, as shown in Figure 3. We obtain this noise process
for the k-th demonstrator by reversing Ornstein–Uhlenbeck (OU)
processes with parameters θ “ 0.15 and σ “ σk (Uhlenbeck
& Ornstein, 1930)4. The value of σk for each demonstrator is
0.01, 0.05, 0.1, 0.25, 0.4, 0.6, 0.7, 0.8, 0.9, and 1.0, respectively. Table 3 shows the performance of demon-
strators with this TSD noisy policy. Learning from demonstrations generated by TSD is challenging; The

Figure 3: Samples bkptq drawn from
noise processes used for the TSD
noisy policy.

4OU process is commonly used to generate time-correlated noises, where the noise variance decays towards zero. We

reserve this process along the time axis, so that the noise variance grows over time.

17

Table 2: Performance of the optimal policy and
demonstrators with the Gaussian noisy policy.

Table 3: Performance of the optimal policy and
demonstrators with the TSD noisy policy.

Cheetah Ant Walker Humanoid

Cheetah Ant Walker Humanoid

σk
(π‹)
0.01
0.05
0.01
0.25
0.40
0.6
0.7
0.8
0.9
1.0

4624
4311
3978
4019
1853
1090
567
267
-45
-399
-177

4349
3985
3861
3514
536
227
-73
-208
-979
-328
-203

4963
4434
3486
4651
4362
467
523
332
283
255
249

5093
4315
5140
5189
3628
5220
2593
1744
735
538
361

σk
(π‹)
0.01
0.05
0.01
0.25
0.40
0.6
0.7
0.8
0.9
1.0

4624
4362
4015
3741
1301
-203
-230
-249
-416
-389
-424

4349
3758
3623
3368
873
231
-51
-37
-567
-751
-269

4963
4695
4528
2362
644
302
29
24
14
7
4

5093
5130
5099
5195
1675
610
249
221
191
178
169

Gaussian model of pω cannot perfectly model the TSD noisy policy, since the ground-truth variance is a
function of actions and time steps.

C.2 Comparison methods

Here, we brieﬂy review methods compared against VILD in our experiments. We ﬁrstly review online IL
methods, which learn a policy by RL and require additional transition samples from MDPs.

MaxEnt-IRL. Maximum entropy IRL (MaxEnt-IRL) (Ziebart et al., 2010) is a well-known IRL method.
The original derivation of the method is based on the maximum entropy principle (Jaynes, 1957) and
uses a linear-in-parameter reward function: rφpst, atq “ φJbpst, atq with a basis function b. Here,
we consider an alternative derivation which is applicable to non-linear reward function (Finn et al.,
2016b,a). Brieﬂy speaking, MaxEnt-IRL learns a reward parameter by minimizing a KL divergence from a
data distribution p‹ps1:T , a1:T q to a model pφps1:T , a1:T q “ 1
t“1ppst`1|st, atq expprφpst, atq{αq,
Zφ
where Zφ is the normalization term. Minimizing this KL divergence is equivalent to solving
‰
maxφ Ep‹ps1:T ,a1:T q
´ log Zφ. To compute log Zφ, we can use the variational approaches
t“1rφpst, atq
as done in VILD, which leads to a max-min problem

p1ps1qΠT

ΣT

“

max
φ

min
θ

Ep‹ps1:T ,a1:T q

”ř

ı
T
t“1rφpst, atq

´ Eqθ ps1:T ,a1:T q

”ř

ı
T
t“1rφpst, atq ´ α log qθpat|stq

,

where qθps1:T , a1:T q “ p1ps1qΠT
reward function and is the solution of IL.

t“1ppst`1|st, atqqθpat|stq. The policy qθpat|stq maximizes the learned

As we mentioned, the proposed model in VILD is based on the model of MaxEnt-IRL. By comparing
the max-min problem of MaxEnt-IRL and the max-min problem of VILD, we can see that the main
diﬀerence are the variational distribution qψ and the noisy policy model pω. If we assume that qψ and
pω are Dirac delta functions: qψpat|st, ut, kq “ δat“ut and pωput|at, st, kq “ δut“at, then the max-min
problem of VILD reduces to the max-min problem of MaxEnt-IRL. In other words, if we assume that all
demonstrators execute the same optimal policy and have an equal level of expertise, then VILD reduces
to MaxEnt-IRL.

GAIL. Generative adversarial IL (GAIL) (Ho & Ermon, 2016) is an IL method that perform occupancy
measure matching (Syed et al., 2008) via generative adversarial networks (GAN) (Goodfellow et al.,
2014). Speciﬁcally, GAIL ﬁnds a parameterized policy πθ such that the occupancy measure ρπθ ps, aq
of πθ is similar to the occupancy measure ρπ‹ps, aq of π‹. To measure the similarity, GAIL uses the
Jensen-Shannon divergence, which is estimated and minimized by the following generative-adversarial
training objective:

min
θ

max
φ

Eρπ‹ rlog Dφps, aqs ` Eρπθ

rlogp1 ´ Dφps, aqq ` α log πθpat|stqs ,

where Dφps, aq “ dφps,aq
RL with a reward function ´ logp1 ´ Dφps, aqq.

dφps,aq`1 is called a discriminator. The minimization problem w.r.t. θ is achieved using

18

AIRL. Adversarial IRL (AIRL) (Fu et al., 2018) was proposed to overcome a limitation of GAIL regarding
reward function: GAIL does not learn the expert reward function, since GAIL has Dφps, aq “ 0.5 at
the saddle point for every states and actions. To overcome this limitation while taking advantage of
generative-adversarial training, AIRL learns a reward function by solving

max
φ

Ep‹ps1:T ,a1:T q

ı
”ř
T
t“1 log Dφps, aq

”ř

ı

` Eqθ ps1:T ,a1:T q

T
t“1 logp1 ´ Dφps, aqq

,

rφps,aq

where Dφps, aq “
rφps,aq`qθ pa|sq . The policy qθpat|stq is learned by RL with a reward function rφpst, atq.
Fu et al. (2018) showed that the gradient of this objective w.r.t. φ is equivalent to the gradient of
MaxEnt-IRL w.r.t. φ. The authors also proposed an approach to disentangle reward function, which leads
to a better performance in transfer learning settings. Nonetheless, this disentangle approach is general
and can be applied to other IRL methods, including MaxEnt-IRL and VILD. We do not evaluate AIRL
with disentangle reward function.

We note that, based on the relation between MaxEnt-IRL and VILD, we can extend VILD to use
a training procedure of AIRL. Speciﬁcally, by using the same derivation from MaxEnt-IRL to AIRL
by Fu et al. (2018), we can derive a variant of VILD which learns a reward parameter by solving
Eqψpat|st,ut,kqrlog Dφps, aqss ` Erqθ ps1:T ,a1:T qrΣT
maxφ Epdps1:T ,u1:T |kqppkqrΣT
t“1 logp1 ´ Dφps, aqqs. We do
t“1
not evaluate this variant of VILD in our experiment.

VAIL. Variational adversarial imitation learning (VAIL) (Peng et al., 2019) improves upon GAIL by
using variational information bottleneck (VIB) (Alemi et al., 2017). VIB aims to compress information ﬂow
by minimizing a variational bound of mutual information. This compression ﬁlters irrelevant signals, which
leads to less over-ﬁtting. To achieve this in GAIL, VAIL learns the discriminator Dφ by an optimization
problem

min
φ,E

max
βě0

Eρπ‹

“

‰
EEpz|s,aq r´ log Dφpzqs

` Eρπθ

“

‰
EEpz|s,aq r´ logp1 ´ Dφpzqqs

` βE

pρπ‹ `ρπθ q{2 rKLpEpz|s, aq|ppzqq ´ Ics ,

where z is an encode vector, Epz|s, aq is an encoder, ppzq is a prior distribution of z, Ic is the target value
of mutual information, and β ą 0 is a Lagrange multiplier. With this discriminator, the policy πθpat|stq
is learned by RL with a reward function ´ logp1 ´ DφpEEpz|s,aq rzsqq.

It might be expected that the compression can make VAIL robust against diverse-quality demonstrations,
since irrelevant signals in low-quality demonstrations are ﬁltered out via the encoder. However, we ﬁnd
that this is not the case, and VAIL does not improve much upon GAIL in our experiments. This is perhaps
because VAIL compress information from both demonstrators and agent’s trajectories. Meanwhile in our
setting, irrelevant signals are generated only by demonstrators. Therefore, the information bottleneck
may also ﬁlter out relevant signals in agent’s trajectories by chance, which lead to poor performances.

InfoGAIL.
Information maximizing GAIL (InfoGAIL) (Li et al., 2017) is an extension of GAIL for
learning a multi-modal policy in MM-IL. The key idea of InfoGAIL is to introduce a context variable z
to the GAIL formulation and learn a context-dependent policy πθpa|s, zq, where each context represents
each mode of the multi-modal policy. To ensure that the context is not ignored during learning, InfoGAIL
regularizes GAIL’s objective so that a mutual information between contexts and state-action variables is
maximized. This mutual information is indirectly maximized via maximizing a variational lower-bound of
mutual information. By doing so, InfoGAIL solves a min-max problem

min
θ,Q

max
φ

Eρπ‹ rlog Dφps, aqs ` Eρπθ

rlogp1 ´ Dφps, aqq ` α log πθpa|s, zqs ` λLpπθ, Qq,

where Lpπθ, Qq “ Eppzqπθ pa|s,zq rlog Qpz|s, aq ´ log ppzqs is a lower-bound of mutual information, Qpz|s, aq
is an encoder neural network, and ppzq is a prior distribution of contexts. In our experiment, the number
of context z is set to be the number of demonstrators K. As discussed in Section 1, if we know the level
of demonstrators’ expertise, then we can choose contexts that correspond to high-expertise demonstrator.
In other words, we can hand-craft the prior ppzq so that a probability of contexts is proportion to the
level of demonstrators’ expertise. For fair comparison in experiments, we do not use the oracle knowledge
about the level of demonstrators’ expertise, and set ppzq to be a uniform distribution.

Next, we review oﬄine IL methods. These methods learn a policy based on supervised learning and

do not require additional transition samples from MDPs.

19

BC. Behavior cloning (BC) (Pomerleau, 1988) is perhaps the simplest IL method. BC treats an IL
problem as a supervised learning problem and ignores dependency between states distributions and policy.
For continuous action space, BC solves a least-square regression problem to learn a parameter θ of a
deterministic policy πθpstq:

”ř

ı

min
θ

Ep‹ps1:T ,a1:T q

T

t“1}at ´ πθpstq}2

2

.

BC-D. BC with Diverse-quality demonstrations (BC-D) is a simple extension of BC for handling
diverse-quality demonstrations. This method is based on the naive model in Section 4.1, and we consider it
mainly for evaluation purpose. BC-D uses supervised learning to learn a policy parameter θ and expertise
ş
parameter ω of a model pθ,ωps1:T , u1:T , kq “ ppkqpps1qΣT
A πθpat|stqpωput|st, at, kqdat.
To learn the parameters, we minimize the KL divergence from data distribution to the model. By using
the variational approach to handle integration over the action space, BC-D solves an optimization problem

t“1ppst`1|st, utq

max
θ,ω,ν

Epdps1:T ,u1:T |kqppkq

”ř
T
t“1

Eqν pat|st,ut,kq

”
log πθ pat|stqpωput|st,at,kq

qν pat|st,ut,kq

ıı

,

where qν pat|st, ut, kq is a variational distribution with parameters ν. We note that the model
pθ,ωps1:T , u1:T , kq of BC-D can be regarded as a regression-extension of the two-coin model proposed
by Raykar et al. (2010) for classiﬁcation with noisy labels.

Co-teaching. Co-teaching (Han et al., 2018) is the state-of-the-art method to perform classiﬁcation with
noisy labels. This method trains two neural networks such that mini-batch samples are exchanged under
a small loss criteria. We extend this method to learn a policy by least-square regression. Speciﬁcally, let
πθ1pstq and πθ2pstq be two neural networks presenting policies, and ∇θLpθ, Bq “ ∇θΣps,aqPB}a ´ πθpsq}2
2
be gradients of a least-square loss estimated by using a mini-batch B. The parameters θ1 and θ2 are
updated by gradient iterates:

θ1 Ð θ1 ´ η∇θ1 Lpθ1, Bθ2q,

θ2 Ð θ2 ´ η∇θ2 Lpθ2, Bθ1q.

The mini-batch Bθ2 for updating θ1 is obtained such that Bθ2 incurs small loss when using prediction
from πθ2 , i.e., Bθ2 “ argminB1 Lpθ2, B1q. Similarly, the mini-batch Bθ1 for updating θ2 is obtained such
that Bθ1 incurs small loss when using prediction from πθ1 . For evaluating the performance, we use the
ﬁrst policy network: πθ1 .

D More experimental results
Results against online IL methods. Figure 4 shows the learning curves of VILD and existing online
IL methods against the number of transition samples. It can be seen that for both types of noisy policy,
VILD with and without IS outperform existing methods overall, in terms of both ﬁnal performance and
data-eﬃciency.

Results against oﬄine IL methods. Figure 5 shows learning curves of oﬄine IL methods, namely
BC, BC-D, and Co-teaching. For comparison, the ﬁgure also shows the ﬁnal performance of VILD with
and without IS, according to Table 1. We can see that these oﬄine methods do not perform well, especially
on the high-dimensional Humanoid task. The poor performance of these methods is due to the issues
of compounding error and low-quality demonstrations. Speciﬁcally, BC performs the worst, since it
suﬀers from both issues. Still, BC may learn well in the early stage of learning, but its performance
sharply degrades, as seen in Ant and Walker2d. This phenomena can be explained as an empirical eﬀect
of memorization in deep neural networks (Arpit et al., 2017). Namely, deep neural networks learn to
remember samples with simple patterns ﬁrst (i.e., high-quality demonstrations from experts), but as
learning progresses the networks overﬁt to samples with diﬃcult patterns (i.e., low-quality demonstrations
from amateurs). Co-teaching is the-state-of-the-art method to avoid this eﬀect, and we can see that it
performs overall better than BC. Meanwhile, BC-D, which learns the policy and level of demonstrators’
expertise, also performs better than BC and is comparable to Co-teaching. However, due to the presence
of compounding error, the performance of Co-teaching and BC-D is still worse than VILD with IS.

20

Accuracy of estimated expertise parameter. Figure 6 shows the estimated parameters ω “ tckuK
k“1
of N put|at, diagpckqq and the ground-truth variance tσ2
k“1 of the Gaussian noisy policy N put|at, σ2
kIq.
The results show that VILD learns an accurate ranking of the variance compared to the ground-truth. The
values of these parameters are also quite accurate compared to the ground truth, except for demonstrators
with low-levels of expertise. A possible reason for this phenomena is that low-quality demonstrations are
highly dissimilar, which makes learning the expertise more challenging. We can also see that the diﬀerence
between the parameters of VILD with IS and VILD without IS is small and negligible.

kuK

21

(a) Performan of online IL methods when demonstrations are generated using Gaussian noisy policy.

(b) Performan of online IL methods when demonstrations are generated using TSD noisy policy.

Figure 4: Performance averaged over 5 trials of online IL methods against the number of transition
samples. Horizontal dotted lines indicate performance of k “ 1, 3, 5, 7, 10 demonstrators.

(a) Performan of oﬄine IL methods when demonstrations are generated using Gaussian noisy policy.

(b) Performan of oﬄine IL methods when demonstrations are generated using TSD noisy policy.

Figure 5: Performance averaged over 5 trials of oﬄine IL methods against the number of gradient update
steps. For VILD with and without IS, we report the ﬁnal performance in Table 1.

Figure 6: Expertise parameters ω “ tckuK
Gaussian noisy policy. For VILD, we report the value of }ck}1{da.

k“1 learned by VILD and the ground-truth tσ2

kuK

k“1 for the

22

