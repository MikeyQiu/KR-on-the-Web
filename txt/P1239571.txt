7
1
0
2
 
t
c
O
 
9
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
2
5
9
2
0
.
0
1
7
1
:
v
i
X
r
a

Conic Scan-and-Cover algorithms for
nonparametric topic modeling

Mikhail Yurochkin
Department of Statistics
University of Michigan
moonfolk@umich.edu

Aritra Guha
Department of Statistics
University of Michigan
aritra@umich.edu

XuanLong Nguyen
Department of Statistics
University of Michigan
xuanlong@umich.edu

Abstract

We propose new algorithms for topic modeling when the number of topics is
unknown. Our approach relies on an analysis of the concentration of mass and
angular geometry of the topic simplex, a convex polytope constructed by taking
the convex hull of vertices representing the latent topics. Our algorithms are shown
in practice to have accuracy comparable to a Gibbs sampler in terms of topic
estimation, which requires the number of topics be given. Moreover, they are one
of the fastest among several state of the art parametric techniques.1 Statistical
consistency of our estimator is established under some conditions.

1

Introduction

A well-known challenge associated with topic modeling inference can be succinctly summed up
by the statement that sampling based approaches may be accurate but computationally very slow,
e.g., Pritchard et al. (2000); Grifﬁths & Steyvers (2004), while the variational inference approaches
are faster but their estimates may be inaccurate, e.g., Blei et al. (2003); Hoffman et al. (2013). For
nonparametric topic inference, i.e., when the number of topics is a priori unknown, the problem
becomes more acute. The Hierarchical Dirichlet Process model (Teh et al., 2006) is an elegant
Bayesian nonparametric approach which allows for the number of topics to grow with data size, but
its sampling based inference is much more inefﬁcient compared to the parametric counterpart. As
pointed out by Yurochkin & Nguyen (2016), the root of the inefﬁciency can be traced to the need for
approximating the posterior distributions of the latent variables representing the topic labels — these
are not geometrically intrinsic as any permutation of the labels yields the same likelihood.

A promising approach in addressing the aforementioned challenges is to take a convex geometric
perspective, where topic learning and inference may be formulated as a convex geometric problem: the
observed documents correspond to points randomly drawn from a topic polytope, a convex set whose
vertices represent the topics to be inferred. This perspective has been adopted to establish posterior
contraction behavior of the topic polytope in both theory and practice (Nguyen, 2015; Tang et al.,
2014). A method for topic estimation that exploits convex geometry, the Geometric Dirichlet Means
(GDM) algorithm, was proposed by Yurochkin & Nguyen (2016), which demonstrates attractive
behaviors both in terms of running time and estimation accuracy. In this paper we shall continue to
amplify this viewpoint to address nonparametric topic modeling, a setting in which the number of
topics is unknown, as is the distribution inside the topic polytope (in some situations).

We will propose algorithms for topic estimation by explicitly accounting for the concentration of
mass and angular geometry of the topic polytope, typically a simplex in topic modeling applications.
The geometric intuition is fairly clear: each vertex of the topic simplex can be identiﬁed by a ray
emanating from its center (to be deﬁned formally), while the concentration of mass can be quantiﬁed

1Code is available at https://github.com/moonfolk/Geometric-Topic-Modeling.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

for the cones hinging on the apex positioned at the center. Such cones can be rotated around the
center to scan for high density regions inside the topic simplex — under mild conditions such cones
can be constructed efﬁciently to recover both the number of vertices and their estimates.

We also mention another fruitful approach, which casts topic estimation as a matrix factorization
problem (Deerwester et al., 1990; Xu et al., 2003; Anandkumar et al., 2012; Arora et al., 2012). A
notable recent algorithm coming from the matrix factorization perspective is RecoverKL (Arora et al.,
2012), which solves non-negative matrix factorization (NMF) efﬁciently under assumptions on the
existence of so-called anchor words. RecoverKL remains to be a parametric technique — we will
extend it to a nonparametric setting and show that the anchor word assumption appears to limit the
number of topics one can efﬁciently learn.

Our paper is organized as follows. In Section 2 we discuss recent developments in geometric topic
modeling and introduce our approach; Sections 3 and 4 deliver the contributions outlined above;
Section 5 demonstrates experimental performance; we conclude with a discussion in Section 6.

2 Geometric topic modeling

RV

Background and related work In this section we present the convex geometry of the Latent
Dirichlet Allocation (LDA) model of Blei et al. (2003), along with related theoretical and algorithmic
results that motivate our work. Let V be vocabulary size and ∆V −1 be the corresponding vocabulary
probability simplex. Sample K topics (i.e., distributions on words) βk ∼ DirV (η), k = 1, . . . , K,
+. Next, sample M document-word probabilities pm residing in the topic simplex
where η
B := Conv(β1, . . . , βK) (cf. Nguyen (2015)), by ﬁrst generating their barycentric coordinates
(i.e., topic proportions) θm ∼ DirK(α) and then setting pm := (cid:80)
k βkθmk for m = 1, . . . , M and
+ . Finally, word counts of the m-th document can be sampled wm ∼ Mult(pm, Nm), where
α
∈
N is the number of words in document m. The above model is equivalent to the LDA when
Nm
individual words to topic label assignments are marginalized out.

RK

∈

∈

Nguyen (2015) established posterior contraction rates of the topic simplex, provided that αk
k
and either number of topics K is known or topics are sufﬁciently separated in terms of the Euclidean
distance. Yurochkin & Nguyen (2016) devised an estimate for B, taken to be a ﬁxed unknown
quantity, by formulating a geometric objective function, which is minimized when topic simplex B
is close to the normalized documents ¯wm := wm/Nm. They showed that the estimation of topic
proportions θm given B simply reduces to taking barycentric coordinates of the projection of ¯wm
onto B. To estimate B given K, they proposed a Geometric Dirichlet Means (GDM) algorithm,
which operated by performing a k-means clustering on the normalized documents, followed by a
geometric correction for the cluster centroids. The resulting algorithm is remarkably fast and accurate,
supporting the potential of the geometric approach. The GDM is not applicable when K is unknown,
but it provides a motivation which our approach is built on.

≤

∀

1

The Conic Scan-and-Cover approach To enable the inference of B when K is not known, we
need to investigate the concentration of mass inside the topic simplex. It sufﬁces to focus on two
types of geometric objects: cones and spheres, which provide the basis for a complete coverage of the
simplex. To gain intuition of our procedure, which we call Conic Scan-and-Cover (CoSAC) approach,
imagine someone standing at a center point of a triangular dark room trying to ﬁgure out all corners
with a portable ﬂashlight, which can produce a cone of light. A room corner can be identiﬁed with
the direction of the farthest visible data objects. Once a corner is found, one can turn the ﬂashlight to
another direction to scan for the next ones. See Fig. 1a, where red denotes the scanned area. To make
sure that all corners are detected, the cones of light have to be open to an appropriate range of angles
so that enough data objects can be captured and removed from the room. To make sure no false
corners are declared, we also need a suitable stopping criterion, by relying only on data points that lie
beyond a certain spherical radius, see Fig. 1b. Hence, we need to be able to gauge the concentration
of mass for suitable cones and spherical balls in ∆V −1. This is the subject of the next section.

3 Geometric estimation of the topic simplex

We start by representing B in terms of its convex and angular geometry. First, B is centered at a point
denoted by Cp. The centered probability simplex is denoted by ∆V −1
.

∆V −1

x+Cp

RV

:=

0

x
{

∈

|

∈

}

2

t!

(a) An incomplete coverage using
3 cones (containing red points).

(b) Complete coverage using
3 cones (red) and a ball (yellow).

(c) Cap Λc(v1) and cone Sω(v1).

Figure 1: Complete coverage of topic simplex by cones and a spherical ball for K = 3, V = 3.

∆V −1
0

Cp

Then, write bk := βk
−
m = 1, . . . , M . Note that re-centering leaves corresponding barycentric coordinates θm
unchanged. Moreover, the extreme points of centered topic simplex ˜B := Conv
RV and corresponding radii Rk
now be represented by their directions vk
bk = Rkvk for any k = 1, . . . , K.

for k = 1, . . . , K and ˜pm := pm

b1, . . . , bK
{
∈

for
∆K−1
∈
can
R+ such that

Cp

−

∈

∈

∈

}

∆V −1
0

3.1 Coverage of the topic simplex

The ﬁrst step toward formulating a CoSAC approach is to show how ˜B can be covered with
exactly K cones and one spherical ball positioned at Cp. A cone is deﬁned as set
ω(v) :=
, where we employ the angular distance (a.k.a. cosine distance)
dcos(v, p) < ω
p
{
}
|
cos(v, p) and cos(v, p) is the cosine of angle ∠(v, p) formed by vectors v and p.
dcos(v, p) := 1

∆V −1
0

∈

S

−

The Conical coverage

exactly K cones, that is,

It is possible to choose ω so that the topic simplex can be covered with
K
(cid:83)
˜B. Moreover, each cone contains exactly one vertex. Suppose
k=1 S

ω(vk)

⊇

that Cp is the incenter of the topic simplex ˜B, with r being the inradius. The incenter and inradius
correspond to the maximum volume sphere contained in ˜B. Let ai,k denote the distance between
the i-th and k-th vertex of ˜B, with amin
amax for all i, k, and Rmax, Rmin such that
Rmin
2
(cid:107)
Proposition 1. For simplex ˜B and ω
max

r/Rmax and ω2 =
ω(v) around any vertex direction

k = 1, . . . , K. Then we can establish the following.

(ω1, ω2), where ω1 = 1

, the cone

Rk :=

Rmax

max), max

max)/(2R2

ai,k

(1

bk

≤

≤

−

≤

≤

∈

∀

(cid:107)

i,k=1,...,K

cos(bi, bk)
}

−

(a2
{

S

v of ˜B contains exactly one vertex. Moreover, complete coverage holds:

K
(cid:83)
k=1 S

ω(vk)

˜B.

⊇

(cid:16)

We say there is an angular separation if cos(bi, bk)
0 for any i, k = 1, . . . , K (i.e., the angles for
(cid:17)
all pairs are at least π/2), then ω
. Thus, under angular separation, the range ω
∅
that allows for full coverage is nonempty independently of K. Our result is in agreement with that of
Nguyen (2015), whose result suggested that topic simplex B can be consistently estimated without
knowing K, provided there is a minimum edge length amin > 0. The notion of angular separation
leads naturally to the Conic Scan-and-Cover algorithm. Before getting there, we show a series of
results allowing us to further extend the range of admissible ω.

r
Rmax

≤
=

, 1

−

∈

1

The inclusion of a spherical ball centered at Cp allows us to expand substantially the range of ω
for which conical coverage continues to hold. In particular, we can reduce the lower bound on ω in
Proposition 1, since we only need to cover the regions near the vertices of ˜B with cones using the
following proposition. Fig. 1b provides an illustration.
Proposition 2. Let B(Cp,

> 0; ω1, ω2 given in Prop. 1, and

) =

RV

Cp

2

˜p

∈

˜p
{
−
Rk sin2(bi, bk)

|(cid:107)

,
≤ R}

(cid:107)

R
(cid:115)

+ cos(bi, bk)

1

R





(cid:26)

min
i,k

R

R2

k sin2(bi, bj)

−

2
R



(cid:27)

 , 1

,

(1)

ω3 := 1

min

−

3

then we have

K
(cid:83)
k=1 S

ω(vk)

B(Cp,

∪

)
R

⊇

˜B whenever ω

(min
{

∈

ω1, ω3

, ω2).
}

Notice that as
Rmax, the admissible
→
range for ω in Prop. 2 results in a substantial strengthening from Prop. 1. It is worth noting that the
above two geometric propositions do not require any distributional properties inside the simplex.

Rmax , the value of ω3

0. Hence if

Rmin

R →

R ≤

≈

In practice complete coverage may fail if ω and

Coverage leftovers
are chosen outside of
corresponding ranges suggested by the previous two propositions. In that case, it is useful to note that
leftover regions will have a very low mass. Next we quantify the mass inside a cone that does contain
a vertex, which allows us to reject a cone that has low mass, therefore not containing a vertex in it.
Proposition 3. The cone Sω(v1) whose axis is a topic direction v1 has mass

R

ω(v1)) > P(Λc(b1)) =
P(
S
(cid:80)
c
((cid:80)

i(cid:54)=1 αi(1
−
i(cid:54)=1 αi)Γ(α1)Γ((cid:80)

c)α1 Γ((cid:80)K

i=1 αi)
i(cid:54)=1 αi)

1

(cid:80)

−

(1

θ1)
(cid:80)

(cid:82) 1
1−c θα1−1
(cid:82) 1
0 θα1−1
(1
−
c (cid:80)K
(cid:20)
i=1 αi
(cid:80)
i(cid:54)=1 αi + 1

1 +

θ1)

1

i(cid:54)=1 αi−1dθ1
i(cid:54)=1 αi−1dθ1

=

+

c2((cid:80)K
((cid:80)

i=1 αi)((cid:80)K
i(cid:54)=1 αi + 1)((cid:80)

i=1 αi + 1)
i(cid:54)=1 αi + 2)

(cid:21)
,

+

· · ·

(2)

where Λc(b1) is the simplicial cap of
S
the corresponding base of ˜B and cutting adjacent edges of ˜B in the ratio c : (1

ω(v1) which is composed of vertex b1 and a base parallel to

c).

See Fig. 1c for an illustration for the simplicial cap described in the proposition. Given the lower
bound for the mass around a cone containing a vertex, we have arrived at the following guarantee.
Proposition 4. For λ

P(Λcλ(bk)) and let ωλ be such that

(0, 1), let cλ be such that λ = min

∈
(cid:32)(cid:32)
2

(cid:115)
1

cλ =

(cid:33)

r2
R2

max

−

k

−

(sin(d) cot(arccos(1

ωλ)) + cos(d))

,

(3)

−

(cid:33)−1

where angle d

min
i,k

≤

∠(bk, bk

−

bi). Then, as long as

ω

∈

≥

S

the bound P(

ω(vk))

λ holds for all k = 1, . . . , K.

3.2 CoSAC: Conic Scan-and-Cover algorithm

(cid:18)

ωλ, max

(cid:18) a2
2R2

max

max

, max
i,k=1,...,K

(1

−

cos(bi, bk)

,

(cid:19)(cid:19)

(4)

Having laid out the geometric foundations, we are ready to present the Conic Scan-and-Cover
(CoSAC) algorithm, which is a scanning procedure for detecting the presence of simplicial vertices
based on data drawn randomly from the simplex. The idea is simple: iteratively pick the farthest point
from the center estimate ˆCp := 1
ω(v) for some suitably
M
chosen ω, and remove all the data residing in this cone. Repeat until there is no data point left.

m pm, say v, then construct a cone

(cid:80)

S

(cid:107)

1, . . . , M
}
{
2 and update A := A
\ S

be the index set of the initially unseen data, then set v :=
ω(v). The parameter ω needs to be sufﬁciently large to ensure

Speciﬁcally, let A =
˜pm
argmax
˜pm:m∈A (cid:107)
that the farthest point is a good estimate of a true vertex, and that the scan will be completed in exactly
K iterations; ω needs to be not too large, so that
ω(v) does not contain more than one vertex. The
S
existence of such ω is guaranteed by Prop. 1. In particular, for an equilateral ˜B, the condition of the
Prop. 1 is satisﬁed as long as ω

1, 1 + 1/(K

1/√K

1)).

(1

∈

−

−

−

In our setting, K is unknown. A smaller ω would be a more robust choice, and accordingly the set A
will likely remain non-empty after K iterations. See the illustration of Fig. 1a, where the blue regions
correspond to A after K = 3 iterations of the scan. As a result, we proceed by adopting a stopping
, which allows us
criteria based on Prop. 2: the procedure is stopped as soon as
∀
to complete the scan in K iterations (as in Fig. 1b for K = 3).

2 <
(cid:107)

˜pm
(cid:107)

m

R

A

∈

The CoSAC algorithm is formally presented by Algorithm 1. Its running is illustrated in Fig. 2,
where we show iterations 1, 26, 29, 30 of the algorithm by plotting norms of the centered documents

4

S

ω(v) against cosine distance to the chosen direction of a topic. Iteration
in the active set A and cone
30 (right) satisﬁes stopping criteria and therefore CoSAC recovered correct K = 30. Note that this
type of visual representation can be useful in practice to verify choices of ω and
. The following
theorem establishes the consistency of the CoSAC procedure.
Theorem 1. Suppose
β1, . . . , βK
pm := (cid:80)
k βkθmk for m = 1, . . . , M and α
ˆβ1, . . . , ˆβ ˆK}
{
(cid:32)(cid:40)
P

are the true topics, incenter Cp is given, θm ∼ DirK(α) and
+ . Let ˆK be the estimated number of topics,
as in Prop. 2. Then

be the output of Algorithm 1 trained with ω and

(cid:15) > 0,

RK

R

R

(cid:41)

(cid:33)

∈

∀

}

{

1, . . . , ˆK

∈ {

}

∪ {

K

= ˆK

}

→

0 as M

.
→ ∞

min
j∈{1,..., ˆK}(cid:107)

βi

−

ˆβj

(cid:107)

> (cid:15) , for any i

Remark We found the choices ω = 0.6 and
}
(cid:107)
practice and agreeing with our theoretical results. From Prop. 3 it follows that choosing
R
K−1 ( c
K
1−c )1−1/K
length is equivalent to choosing ω resulting in an edge cut ratio c such that 1
1/2, then c
ω

≥
2K )K/(K−1), which, for any equilateral topic simplex B, is satisﬁed by setting

( K−1
(0.3, 1), provided that K

to be robust in
as median

2000 based on the Eq. (3).

to be median of

2, . . . ,
(cid:107)

˜pM

{(cid:107)

˜p1

R

−

≤

(cid:107)

2

∈

≤

4 Document Conic Scan-and-Cover algorithm

In the topic modeling problem, pm for m = 1, . . . , M are not given. Instead, under the bag-of-words
assumption, we are given the frequencies of words in documents w1, . . . , wM which provide a point
and length of
estimate ¯wm := wm/Nm for the pm. Clearly, if number of documents M
→ ∞
m, we can use Algorithm 1 with the plug-in estimates ¯wm in place of pm,
documents Nm
(cid:80) ¯wm. In practice, M and Nm are
since ¯wm
ﬁnite, some of which may take relatively small values. Taking the topic direction to be the farthest
, may no
point in the topic simplex, i.e., v = argmax
˜wm

pm. Moreover, Cp will be estimated by ˆCp := 1
M

→ ∞ ∀

ˆCp

→

∆V −1
0

2, where ˜wm := ¯wm
(cid:107)

−

∈

˜wm:m∈A (cid:107)

longer yield a robust estimate, because the variance of this topic direction estimator can be quite high
(in Proposition 5 we show that it is upper bounded with (1

1/V )/Nm).

To obtain improved estimates, we propose a technique that we call “mean-shifting”. Instead of taking
the farthest point in the simplex, this technique is designed to shift the estimate of a topic to a high
density region, where true topics are likely to be found. Precisely, given a (current) cone
ω(v), we
cos( ˜wm, v)). In other words,
re-position the cone by updating v := argmin

˜wm

(cid:80)

S

v

m∈Sω(v) (cid:107)

2(1
(cid:107)

−

we re-position the cone by centering it around the mean direction of the cone weighted by the norms
of the data points inside, which is simply given by v
ω(v)). This results in
reduced variance of the topic direction estimate, due to the averaging over data residing in the cone.

m∈Sω(v) ˜wm/ card(
S

(cid:80)

∝

−

The mean-shifting technique may be slightly modiﬁed and taken as a local update for a subsequent
optimization which cycles through the entire set of documents and iteratively updates the cones. The
optimization is with respect to the following weighted spherical k-means objective:

min
(cid:107)vk(cid:107)2=1,k=1,...K

˜wm
(cid:107)

2(1
(cid:107)

−

cos(vk, ˜wm)),

(5)

K
(cid:88)

(cid:88)

k=1

m∈Sk(vk)

Sk(vk) =

{
1, . . . , M

dcos(vk, ˜pm) < dcos(vl, ˜pi)
m
|
(this is different from

where cones Sk(vk) =
K
(cid:70)
k=1
optimization is to use full data for estimation of topic directions, hence further reducing the variance
due to short documents. The connection between objective function (5) and topic simplex estimation
is given in the Supplement. Finally, obtain topic norms Rk along the directions vk using maximum
projection: Rk := max

. Our entire procedure is summarized in Algorithm 2.

ω(vk)). The rationale of spherical k-means

yield a disjoint data partition

vk, ˜wm

= k

l
∀

S

}

{

}

m:m∈Sk(vk)(cid:104)

(cid:105)

Remark In Step 9 of the algorithm, cone
ω(v)) <
λM , for some small constant λ, is discarded because this is likely an outlier region that does not actu-
k,
ally contain a true vertex. The choice of λ is governed by results of Prop. 4. For small αk = 1/K,

ω(v) with a very low cardinality, i.e., card(

S

S

∀

5

P(Λc)

λ

≤

≈

c(K−1)/K
(K−1)(1−c) and for an equilateral ˜B we can choose d such that cos(d) =

ging these values into Eq. (3) leads to c =

(cid:18)(cid:16)

(cid:113)
2

1

(cid:17) (cid:18)(cid:113) K−1
2K (

1
K2

−

1−ω

√1−(1−ω)2 ) +

(cid:113) K+1

2K . Plug-
(cid:19)(cid:19)−1

(cid:113) K+1
2K

.

Now, plugging in ω = 0.6 we obtain λ
K to get a sense of λ, we now make a conservative choice λ = 0.001, so that (K)−1 > λ
As a result, a topic is rejected if the corresponding cone contains less than 0.1% of the data.

K −1 for large K. Our approximations were based on large
K < 1000.

≤

∀

Finding anchor words using Conic Scan-and-Cover Another approach to reduce the noise is
to consider the problem from a different viewpoint, where Algorithm 1 will prove itself useful.
RecoverKL by Arora et al. (2012) can identify topics with diminishing errors (in number of documents
M ), provided that topics contain anchor words. The problem of ﬁnding anchor words geometrically
reduces to identifying rows of the word-to-word co-occurrence matrix that form a simplex containing
other rows of the same matrix (cf. Arora et al. (2012) for details). An advantage of this approach
no matter the
is that noise in the word-to-word co-occurrence matrix goes to zero as M
document lengths, hence we can use Algorithm 1 with "documents" being rows of the word-to-word
co-occurrence matrix to learn anchor words nonparametrically and then run RecoverKL to obtain
topic estimates. We will call this procedure cscRecoverKL.

→ ∞

Algorithm 1 Conic Scan-and-Cover (CoSAC)
Input: document generating distributions p1, . . . , pM ,

angle threshold ω, norm threshold

R

(cid:80)

m pm {ﬁnd center};

Output: topics β1, . . . , βk
1: ˆCp = 1
M
{initialize active set};
1, . . . , M
2: A1 =
}
3: while
˜pm
Ak :
m
(cid:107)
˜pm
vk = argmax
4:
˜pm:m∈Ak (cid:107)

2 >
(cid:107)
R
2 {ﬁnd topic}
(cid:107)

{
∃

do

∈

ω(vk) =
5:
S
6: Ak = Ak
βk = vk + ˆCp, k = k + 1 {compute topic}
7:
8: end while

m : dcos(˜pm, vk) < ω
{
\ S

ω(vk) {update active set}

}

˜pm := pm

ˆCp for m = 1, . . . , M {center the data}

−

k = 1 {initialize topic count}

{ﬁnd cone of near documents}

Figure 2: Iterations 1, 26, 29, 30 of the Algorithm 1. Red are the documents in the cone
are the documents in the active set Ak+1 for next iteration. Yellow are documents
˜pm
(cid:107)

ω(vk); blue
S
.
2 <
(cid:107)

R

5 Experimental results

5.1 Simulation experiments

In the simulation studies we shall compare CoSAC (Algorithm 2) and cscRecoverKL based on
Algorithm 1 both of which don’t have access to the true K, versus popular parametric topic modeling
approaches (trained with true K): Stochastic Variational Inference (SVI), Collapsed Gibbs sampler,
RecoverKL and GDM (more details in the Supplement). The comparisons are done on the basis of
minimum-matching Euclidean distance, which quantiﬁes distance between topic simplices (Tang
et al., 2014), and running times (perplexity scores comparison is given in Fig. 7). Lastly we will
demonstrate the ability of CoSAC to recover correct number of topics for a varying K.

6

ˆCp for m = 1, . . . , M {center the data}

−

k = 1 {initialize topic count}

Algorithm 2 CoSAC for documents
Input: normalized documents ¯w1, . . . , ¯wM ,
angle threshold ω, norm threshold

m ¯wm {ﬁnd center};

(cid:80)

Output: topics β1, . . . , βk
1: ˆCp = 1
M
1, . . . , M
2: A1 =
}
3: while
Ak :
m
vk = argmax
4:

{
∃

{initialize active set};
˜wm
(cid:107)
˜wm
˜wm:m∈Ak (cid:107)

2 >
(cid:107)
2 {initialize direction}
(cid:107)
while vk not converged do {mean-shifting}

do

R

∈

˜wm := ¯wm

, outlier threshold λ

R

{ﬁnd cone of near documents}

}

ω(vk)) {update direction}

m∈Sω(vk) ˜wm/ card(
S

m : dcos( ˜wm, vk) < ω
{

5:
ω(vk) =
6:
vk = (cid:80)
S
7:
end while
8:
9: Ak = Ak
if card(
10: end while
11: v1, . . . , vk = weighted spherical k-means (v1, . . . , vk, ˜w1, . . . , ˜wM )
12: for l in
1, . . . , k
{
13: Rl := max

{ﬁnd topic length along direction vl}

ω(vk) {update active set}

\ S
ω(vk)) > λM

vl, ˜wm

do

S

}

then k = k + 1 {record topic direction}

m:m∈Sl(vl)(cid:104)

(cid:105)

βl = Rlvl + ˆCp {compute topic}

14:
15: end for

Figure 3: Minimum matching Euclidean distance for (a) varying corpora size, (b) varying length of
documents; (c) Running times for varying corpora size; (d) Estimation of number of topics.

Figure 4: Gibbs sampler convergence analysis for (a) Minimum matching Euclidean distance for
corpora sizes 1000 and 5000; (b) Perplexity for corpora sizes 1000 and 5000; (c) Perplexity for
NYTimes data.

Estimation of the LDA topics First we evaluate the ability of CoSAC and cscRecoverKL to
estimate topics β1, . . . , βK, ﬁxing K = 15. Fig. 3(a) shows performance for the case of fewer
[100, 10000] but longer Nm = 500 documents (e.g. scientiﬁc articles, novels, legal documents).
M
CoSAC demonstrates performance comparable in accuracy to Gibbs sampler and GDM.

∈

[25, 300] documents (e.g. news
Next we consider larger corpora M = 30000 of shorter Nm
articles, social media posts). Fig. 3(b) shows that this scenario is harder and CoSAC matches the
performance of Gibbs sampler for Nm
75. Indeed across both experiments CoSAC only made
mistakes in terms of K for the case of Nm = 25, when it was underestimating on average by 4 topics

≥

∈

7

and for Nm = 50 when it was off by around 1, which explains the earlier observation. Experiments
with varying V and α are given in the Supplement.

It is worth noting that cscRecoverKL appears to be strictly better than its predecessor. This suggests
that our procedure for selection of anchor words is more accurate in addition to being nonparametric.

Running time A notable advantage of the CoSAC algorithm is its speed. In Fig. 3(c) we see
that Gibbs, SVI, GDM and CoSAC all have linear complexity growth in M , but the slopes are very
different and approximately are INm for SVI and Gibbs (where I is the number of iterations which
has to be large enough for convergence), number of k-means iterations to converge for GDM and is
of order K for the CoSAC procedure making it the fastest algorithm of all under consideration.

Next we compare CoSAC to per iteration quality of the Gibbs sampler trained with 500 iterations for
M = 1000 and M = 5000. Fig. 4(b) shows that Gibbs sampler, when true K is given, can achieve
good perplexity score as fast as CoSAC and outperforms it as training continues, although Fig. 4(a)
suggests that much longer training time is needed for Gibbs sampler to achieve good topic estimates
and small estimation variance.

Estimating number of topics Model selection in the LDA context is a quite challenging task and,
to the best of our knowledge, there is no "go to" procedure. One of the possible approaches is based
on reﬁtting LDA with multiple choices of K and using Bayes Factor for model selection (Grifﬁths &
Steyvers, 2004). Another option is to adopt the Hierarchical Dirichlet Process (HDP) model, but we
should understand that it is not a procedure to estimate K of the LDA model, but rather a particular
prior on the number of topics, that assumes K to grow with the data. A more recent suggestion is to
slightly modify LDA and use Bayes moment matching (Hsu & Poupart, 2016), but, as can be seen
from Figure 2 of their paper, estimation variance is high and the method is not very accurate (we
tried it with true K = 15 and it took above 1 hour to ﬁt and found 35 topics). Next we compare
Bayes factor model selection versus CoSAC and cscRecoverKL for K
[5, 50]. Fig. 3(d) shows that
CoSAC consistently recovers exact number of topics in a wide range.

∈

We also observe that cscRecoverKL does not estimate K well (underestimates) in the higher range.
This is expected because cscRecoverKL ﬁnds the number of anchor words, not topics. The former
is decreasing when later is increasing. Attempting to ﬁt RecoverKL with more topics than there
are anchor words might lead to deteriorating performance and our modiﬁcation can address this
limitation of the RecoverKL method.

5.2 Real data analysis

In this section we demonstrate CoSAC algorithm for topic modeling on one of the standard bag
of words datasets — NYTimes news articles. After preprocessing we obtained M
130, 000
documents over V = 5320 words. Bayes factor for the LDA selected the smallest model among
[80, 195], while CoSAC selected 159 topics. We think that disagreement between the two
K
procedures is attributed to the misspeciﬁcation of the LDA model when real data is in play, which
affects Bayes factor, while CoSAC is largely based on the geometry of the topic simplex.

≈

∈

The results are summarized in Table 1 — CoSAC found 159 topics in less than 20min; cscRecoverKL
estimated the number of anchor words in the data to be 27 leading to fewer topics. Fig. 4(c) compares
CoSAC perplexity score to per iteration test perplexity of the LDA (1000 iterations) and HDP (100
iterations) Gibbs samplers. Text ﬁles with top 20 words of all topics are available on GitHub. We
note that CoSAC procedure recovered meaningful topics, contextually similar to LDA and HDP (e.g.
elections, terrorist attacks, Enron scandal, etc.) and also recovered more speciﬁc topics about Mike
Tyson, boxing and case of Timothy McVeigh which were present among HDP topics, but not LDA
ones. We conclude that CoSAC is a practical procedure for topic modeling on large scale corpora
able to ﬁnd meaningful topics in a short amount of time.

6 Discussion

We have analyzed the problem of estimating topic simplex without assuming number of vertices
(i.e., topics) to be known. We showed that it is possible to cover topic simplex using two types of
geometric shapes, cones and a sphere, leading to a class of Conic Scan-and-Cover algorithms. We

8

Table 1: Modeling topics of NYTimes articles

K Perplexity

Coherence

Time

cscRecoverKL
HDP Gibbs
LDA Gibbs
CoSAC

221

27
5
±
80
159

1477
1520

2603
1.6
±
1.5
±
1568

442
300

−
−

-238
1.7
±
0.7
±
-322

37 min
35 hours
5.3 hours
19 min

then proposed several geometric correction techniques to account for the noisy data. Our procedure is
accurate in recovering the true number of topics, while remaining practical due to its computational
speed. We think that angular geometric approach might allow for fast and elegant solutions to other
clustering problems, although as of now it does not immediately offer a unifying problem solving
framework like MCMC or variational inference. An interesting direction in a geometric framework is
related to building models based on geometric quantities such as distances and angles.

Acknowledgments

This research is supported in part by grants NSF CAREER DMS-1351362, NSF CNS-1409303, a
research gift from Adobe Research and a Margaret and Herman Sokol Faculty Award.

A Proofs of main theorems

We start by reminding the reader of our geometric setup. First, topic simplex B := Conv(β1, . . . , βK)
is centered at a point denoted by Cp. Let ∆V −1
— centered
probability simplex. Then, write bk := βk
−
∆V −1
0
∆K−1 unchanged. Moreover, the extreme points of centered topic simplex ˜B := Conv
can now be represented by their directions vk
bk = Rkvk for any k = 1, . . . , K.

Cp
∈
for m = 1, . . . , M . Note that re-centering leaves corresponding barycentric coordinates θm
∈
b1, . . . , bK
{
}
R+ such that

∈
for k = 1, . . . , K and ˜pm := pm

RV and corresponding radii Rk

RV : x + Cp

:=
x
{
∆V −1
0

∆V −1

0
Cp

−

∈

∈

∈

∈

}

A.1 Coverage of the topic simplex

Suppose that Cp is the incenter of the topic simplex ˜B, with r being the inradius. Recall that the
incenter and inradius correspond to the maximum volume sphere inside ˜B. Let ai,k denote the
distance between the ith and kth vertex of ˜B, with amin
amax for all i, k, and Rmax, Rmin
such that Rmin
Rmax
bk
(cid:107)
Proposition 1. For simplex ˜B and ω
max

r/Rmax and ω2 =
ω(v) around any vertex direction

(ω1, ω2), where ω1 = 1

≤
k = 1, . . . , K

, the cone

Rk :=

max), max

max)/(2R2

ai,k

(1

−

≤

≤

≤

∈

∀

(cid:107)

2

i,k=1,...,K

cos(bi, bk)
}

−

(a2
{

S

v of ˜B contains exactly one vertex. Moreover, complete coverage holds:

K
(cid:83)
k=1 S

ω(vk)

˜B.

⊇

min

Proof. Let ω0 = a2
ω(vk) does not contain
any other vertices. This can be explained as follows. Fix k, and choose i
= k. Deﬁne
1, . . . , K
φi,k as the angle at Cp made by the side connecting the vertex i and vertex k. Then from the cosine
law for triangles, we have

, for any ω
}

. Then, for any k

1, . . . , K

S
∈ {

∈ {

ω0,

2R2

} (cid:54)

≤

max

Now, for any φ

min
i,k
than vertex k, for any k. Now φ1 = min
i,k

φi,k, with ωφ = 1

≤

cos(φi,k) =

R2

i + R2

k −
2RiRk
cos(φ), the cone

a2
i,k

.

S

−
φi,k satisﬁes

ωφ(vk) does not cover any vertex other

1

cos(φ1)

−

a2
min
2R2
max −

≤

(Rmax

Rmin)2

−

2RmaxRmin ≤

a2
min
2R2

max

.

9

C

∠(bi, bk)

Rk

arccos(1

−

ωR)
B

A

R

R

Figure 5: C : kth vertex point, A : point where the adjacent side to the vertex has been cut off by the
sphere, Rk: distance to kth vertex from incenter,

: radius of sphere, B : incenter

from which we obtain the upper bound for ω. For the lower bound, consider for vertex k,

cone connecting the incenter to facial incenters of facets containing vertex k. Then

(vk) the
˜B.

S
(vk)

K
(cid:83)
k=1 S

⊇
cos(φ2), with φ2 satisfying cos(φ2)
≤
a2
min is needed to ensure

max ≤

Now for each k,

(vk)

ω2 (vk), where ω2 = 1

. From this we get the lower bound. The restriction 2R2

⊆ S

−

min
k∈{1,...,K}

r
Rk

that the set (cid:8)ω : 1

S

−

(

r
Rmax

)

ω

≤

≤

( a2
2R2

max

max

)(cid:9) is non-empty.

Proposition 2. Let B(Cp,

) =

˜p

{

∈

R

RV

˜p

|(cid:107)

−

Cp

2
(cid:107)

,
≤ R}

> 0; ω1, ω2 given in Prop. 1, and

ω3 := 1

min

−





(cid:26)

min
i,k

Rk sin2(bi, bk)

+ cos(bi, bk)

1

R

R2

k sin2(bi, bj)

−

2
R



(cid:27)

 , 1

,

(6)

R
(cid:115)

then we have

K
(cid:83)
k=1 S

ω(vk)

B(Cp,

∪

)
R

⊇

˜B whenever ω

(min
{

∈

ω1, ω3

, ω2).
}

ωi,k) be the angle formed by the line joining the kth vertex to the
Proof. Let φi,k = arccos(1
incenter Cp and the radial vector from incenter to the point where the sphere cuts the edge connecting
i and k (segment AB on Fig. 5). From the sine law for a triangle we have

−

cos(φi,k) + cot(bi, bk) sin(φi,k)

= 0.

Rk

R

−

(7)

(cid:19)

Solving for φi,k we have cos(φi,k) =

Rk sin2(bi,bk)
R
since we must choose the largest such φ over all i and k, the bound follows immediately. Notice

k sin2(bi,bk)
R2

+ cos(bi, bk)

. Now,

−

R2

(cid:113)
1

(cid:18)

(cid:18)

Rk sin2(bi,bk)
R

+ cos(bi, bk)

1

(cid:113)

(cid:19)

R2

k sin2(bi,bk)
R2

−

→

1, whereas

increases the lower bound in this limiting scenario is dominated by

that as

r
Rmax

Rmax, the value of

R →
< 1 strictly. Thus, as

(cid:18)

Rk sin2(bi,bk)
R

R
+ cos(bi, bk)

(cid:113)

1

min
i,k

−

bound from Proposition 1.

(cid:19)

R2

k sin2(bi,bk)
R2

1

−

, thereby obtaining an improvement in the

10

Proposition 3. The cone Sω(v1) whose axis is a topic direction v1 has mass

ω(v1)) > P(Λc(b1)) =
P(
S
(cid:80)
c
((cid:80)

i(cid:54)=1 αi(1
−
i(cid:54)=1 αi)Γ(α1)Γ((cid:80)

c)α1 Γ((cid:80)K

i=1 αi)
i(cid:54)=1 αi)

1

(cid:80)

−

(1

θ1)
(cid:80)

(cid:82) 1
1−c θα1−1
(cid:82) 1
0 θα1−1
(1
−
c (cid:80)K
(cid:20)
i=1 αi
(cid:80)
i(cid:54)=1 αi + 1

1 +

θ1)

1

i(cid:54)=1 αi−1dθ1
i(cid:54)=1 αi−1dθ1

=

+

c2((cid:80)K
((cid:80)

i=1 αi)((cid:80)K
i(cid:54)=1 αi + 1)((cid:80)

i=1 αi + 1)
i(cid:54)=1 αi + 2)

(cid:21)
,

+

· · ·

(8)

where Λc(b1) is the simplicial cap of
S
the corresponding base of ˜B and cutting adjacent edges of ˜B in the ratio c : (1

ω(v1) which is composed of vertex b1 and a base parallel to

c).

The truncated beta probability calculations in Proposition 3 can be found in Olver et al. (2010).
P(Λcλ(bk)) and let ωλ be such that
Proposition 4. For λ

(0, 1), let cλ be such that λ = min

∈

k

(cid:32)(cid:32)
2

(cid:115)
1

cλ =

(cid:33)

r2
R2

max

−

(sin(d) cot(arccos(1

ωλ)) + cos(d))

,

(9)

−

−

(cid:33)−1

where angle d

min
i,k

≤

∠(bk, bk

−

bi). Then, as long as

(cid:18)

ω

∈

ωλ, max

(cid:18) a2
2R2

max

max

, max
i,k=1,...,K

(1

−

cos(bi, bk)

,

(cid:19)(cid:19)

the bound P(

ω(vk))

S

≥

λ holds for all k = 1, . . . , K.

Proof. Consider Figure 5, with length of AC = ai,kc, where c is the proportion in which the cone
cuts AC, the edge joining vertex i and vertex k. Now, from the sine law of a triangle,

Rk
ai,kc

= sin(bi, bk) cot φi,k + cos(bi, bk)

where φi,k is as deﬁned in the proof of Proposition 2. Now ai,k
φλ = cos ωλ satisﬁes

Rk ≤

2(√R2

max−r2)

Rmax

. The choice of

cλ

≥

(cid:113)
2

1

1

−

r2

R2

max

min
i,k

1
sin(bi, bk) cot φλ + cos(bi, bk)

∠(bi, bk), for all i, k, the function sin(bi, bk) cot φλ +
π
therefore proves the theorem. Since, φλ
2 −
cos(bi, bk) is increasing as the angle between bi and bk increases, as can be checked for maxima by
the ﬁrst derivative rule. Using the cosine law,

≤

(10)

(11)

(12)

(13)

cos(bi, bk) = −

k + a2
i,k

R2

i + R2
2ai,kRk

.

Minimizing this quantity with respect to i and k we get the result.

A.2 Consistency of the Conic Scan-and-Cover algorithm

Under the LDA setup (as presented in Section 2), recall that ai,k is the length of the edge connecting
the ith and kth vertex, i.e.,
, (cid:15)) denote an
−
(cid:15)-ball in (cid:96)2-norm. Then the following result states that with high probability there exists a document
in a neighborhood of every vertex.
Lemma 1. Let pm := (cid:80)
max
k(cid:54)=i

k βkθmk for m = 1, . . . , M as before. Then for any i and any 0 < (cid:15) <

2 is the (cid:96)2 norm. Let B(
·

2 = ai,k, where
(cid:107)

βi
(cid:107)

ai,k,

(cid:107) · (cid:107)

βk

P(pm /
∈

B(βi, (cid:15))

m

∀

∈ {

1, . . . , M

)
}

≤





(cid:82)

0

1−((cid:15)/max
k(cid:54)=i

ai,k)

(cid:82) 1
0 θαi−1

i

(cid:80)

j(cid:54)=i αj −1dθi

(1

θi)

θi)
−
(cid:80)
j(cid:54)=1 αj −1dθi

θαi−1
i
(1

−

M





. (14)

11

Since

(cid:32)

(cid:82)

0

1−((cid:15)/max
k(cid:54)=i

ai,k )

(cid:82) 1
0 θαi−1

i

θαi−1
i
(1−θi)

(1−θi)
(cid:80)

j(cid:54)=1 αj −1dθi

(cid:80)

j(cid:54)=i αj −1dθi

(cid:33)

< 1, for all i because Beta distribution is absolutely

continuous in (0, 1), the bound on the right hand side goes to 0 as M

.
→ ∞

be the topics identiﬁed by Conic Scan-and-Cover algorithm, with labels permuted
being the true topics. Then

}

ˆβ1, . . . , ˆβK
{

Let
according to the minimum matching distance criteria, with
the following result shows the consistency of the identiﬁed topics.
Theorem 1. Suppose
pm := (cid:80)
k βkθmk for m = 1, . . . , M and α
Scan-and-Cover algorithm trained with ω and

β1, . . . , βK
{

}

β1, . . . , βK
{

}

are the true topics, incenter Cp is given, θm ∼ DirK(α) and
be the output of the Conic

∈
R

RK
+ . Let
as in Proposition 2. Then
(cid:33)

ˆβ1, . . . , ˆβ ˆK}
{
(cid:41)

∀

(cid:15) > 0,

min
j∈{1,..., ˆK}(cid:107)

βi

−

ˆβj

(cid:107)

> (cid:15) , for any i

1, . . . , ˆK

∈ {

}

∪ {

K

= ˆK

}

→

0 as M

.
→ ∞

(cid:32)(cid:40)

P

Proof. From the description of the Conic Scan-and-Cover algorithm it sufﬁces to prove that for the
suitable choice of ω,
<
p1, . . . , pm
. But this probability expression is bounded from below by
(cid:15)
1

). The conclusion now follows from Lemma 1.

as in Proposition 2 there holds P(

R
)
→
}
B(βi, (cid:15))

1 as M
m

1, . . . , M

such that

βi
(cid:107)

∈ {

xi

xi

−

∃

(cid:107)

}

i
∈ {
(cid:80)K
i=1

→ ∞
∈ {

∀

}

1, . . . , K
P(pm /
∈

∀
−

A.3 Variance argument for multinomial setup

In the topic modeling problem we are not given pm for m = 1, . . . , M . Under the bag-of-words
assumption we have access to the frequencies of words in documents w1, . . . , wM which provide
a point estimate ¯wm := wm/Nm for the pm. The following proposition establishes a bound on the
variation of ¯wm from pm.
Proposition 5.

E[

¯wm
(cid:107)

−

pm

2
2]
(cid:107)

≤

1

−

(1/V )
Nm

.

(15)

Proof. By iterated expectation identity,

E[
¯wm
(cid:107)

−

pm

(cid:20)
2] = E
2
(cid:107)

E

(cid:20) V

(cid:88)

i=1

¯wmi
(cid:107)

−

pmi

pm

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2
2

(cid:107)

(cid:21)(cid:21)

(cid:20) V

(cid:88)

pmi(1

(cid:21)

pmi)

= E

−
Nm

i=1 p2

mi]

1

=

−

i=1

E[(cid:80)V
Nm

1

−

(1/V )
Nm

.

≤

The second equality follows because conditioned on pm, each wmi
inequality follows from Cauchy-Schwartz Inequality.

∼

Bin(Nm, pmi). The last

B Spherical k-means for topic modeling

We aim to clarify the role of Step 11 of the document Conic Scan-and-Cover algorithm, a geometric
correction technique based on weighted spherical k-means optimization.

B.1 Topic directions as solutions to weighted spherical k-means

Let centered document norms rm :=
2 for m = 1, . . . , M and αk(v) := cos(bk, v), cosine of
(cid:107)
the angle between direction v and k-th topic. The weighted spherical k-means objective takes the
form

˜pm
(cid:107)

min
(cid:107)vk(cid:107)2=1,k=1,...K

rm(1

cos(vk, ˜pm)),

−

(16)

K
(cid:88)

(cid:88)

k=1

m∈Sk(vk)

12

where Sk(vk) :=

m
{

|

cos(vk, ˜pm)) > cos(vl, ˜pm)
K
(cid:88)

rm cos(vk, ˜pm) =

vk, ˜pm
(cid:104)

(cid:105)

=

θmi

vk, bi
(cid:104)

(cid:105)

=

i=1

K
(cid:88)

i=1

l
∀

= k)

. Next observe that:
}

θmiRiαi(vk),

(17)

so our objective 16 becomes:

max
(cid:107)vk(cid:107)2=1,k=1,...K

K
(cid:88)

(cid:88)

K
(cid:88)

k=1

m∈Sk(vk)

i=1

θmiRiαi(vk).

(18)

= i, which implies that topic simplex
Now, if R1 = . . . = RK and αi(bk) = αi(bl)
Sk(bk) iff
is equilateral, we see that cluster boundaries of topic directions are given by m
= k. Observe that the corresponding partition is deﬁned by the geometric medians
θmk > θml
of topic simplex, which in turn partitions it into equal volume parts. Then, assuming that the topic
simplex B is symmetric, combined with the symmetricity of the Dirichlet distribution of θm-s, it
follows that bk is the centroid of Sk(bk) for k = 1, . . . , K.

k, l

∈

∀

∀

l

B.2 Role of the spherical k-means in CoSAC algorithm for documents

The result of Section B.1 shows that weighted spherical k-means with Lloyd type updates (Lloyd,
1982) will converge to the directions of the true topics if it is initialized in their respective neigh-
borhoods and equilaterality of B and symmetricity of Dirichlet for document topic proportions is
satisﬁed.

Recall that goal of the Conic Scan-and-Cover is to ﬁnd the number of topics and their directions, while
Mean Shifting was used to address the noise in the data. We proceed to compare weighted spherical
k-means by itself (with 500 iterations, which makes it slower than CoSAC) versus document Conic
Scan-and-Cover with only Mean Shifting and the full document Conic Scan-and-Cover algorithm
to see the effect of the spherical k-means post-processing step. Results in Fig. 6 are for the same
[25, 300] but corpora is large
scenarios as in Section 5 – that is when either documents are short Nm
M = 30000 or when documents are longer Nm = 500 and corpora is smaller M
[100, 10000].
We see that spherical k-means by itself does not succeed, whereas when used as a postprocessing step
for CoSAC it allows for a slight improvement when documents are short. This is because it operates
on the full data partition when taking averages for direction estimation, while Mean Shifting only
ω(v). Using more data is important for noise reduction
has access to the data in its respective cone
when documents are short as suggested by our analysis.

∈

∈

S

Figure 6: Minimum matching Euclidean distance for (a) varying corpora size, (b) varying length of
documents. Perplexity for (c) varying corpora sizes, (d) varying length of documents.

C Additional experiments

C.1 Perplexity comparison

In this section we present perplexity scores comparison for the experiments of Section 5. For
simulation experiments we used V = 2000, symmetric α = η = 0.1. To compute held-out perplexity
for the CoSAC we employed projection based estimates for topic proportions θm from Yurochkin &
Nguyen (2016), which led to a slightly worse perplexity scores for CoSAC and GDM in comparison
to Gibbs sampler. However, CoSAC (except for Nm = 25, when it slightly underestimates K) shows
competitive performance without requiring K as an input. We note that as before cscRecoverKL
outperforms RecoverKL in all cases.

13

C.2 Varying vocabulary size V

Our next experiment investigates the inﬂuence of vocabulary size V . We set Nm = 500, M = 5000,
K = 15, symmetric α = η = 0.1 and varied V from 2000 to 15000. We discovered that ω = 0.6 is
too small for V > 10000, meaning that CoSAC algorithm does not ﬁnd enough documents in the
corresponding cones and keeps discarding without recording topics (per Step 9 of Algorithm 2). This
can be explained by the fact that vectors tending to be far apart in high dimensions and relatively (to
V > 10000) small values of corpora size M and document lengths Nm. On the other hand, setting
ω = 0.75 worked well for all values of V in this experiment. Results are reported in Fig. 7(c), (d)
and Fig. 8(d). Document CoSAC with ω = 0.75 recovered true K = 15 for all values of V and
showed better recovery than GDM and Gibbs sampler in terms of minimum matching distance, while
Gibbs sampler had slightly better perplexity for higher values of V . It is worth reminding that unlike
CoSAC, both GDM and the Gibbs sampling based method requires the number of topics K be given.

C.3

Impact of α

Recall that, per the LDA model, topic proportions θ ∼ DirK(α). Cases with α > 1 were previously
shown (Nguyen, 2015) to exhibit slower convergence rates of the LDA’s posterior estimation (via
Gibbs sampler, for instance). Geometrically, large α implies that documents are more likely concen-
trated near the center of the topic simplex, leaving fewer documents near the vertices; this entails that
geometric inference is more challenging. In our choices for parameters ω,
, λ we relied on small
values of α as a more practical scenario. Speciﬁcally, we considered ω = 0.8 for this experiment
to achieve full coverage of the topic simplex. In our previous experiments we set α = 0.1. Now,
we consider a larger range, α
[0.01, 1.5], to gauge its impact more fully. Results are reported in
Fig. 8(a), (b) and (c). For smaller values of α CoSAC is demonstrated to be the best algorithm of all
under consideration. As α increases, CoSAC can still recover correct K with high accuracy, although
the quality of topic estimates deteriorates faster than for Gibbs sampler and GDM. We think that
further work on estimation procedures for topic radii Rks (recall that topics are estimated as direction
and length along this direction bk = Rkvk) might address this issue. In this work we considered
maximum projection (Step 13 of Algorithm 2) to estimate Rks, which might not be as accurate when
documents are mostly near the center of the topic simplex (i.e., for higher α).

R

∈

Figure 7: Perplexity for (a) varying corpora size, (b) varying length of documents, (c) varying
vocabulary size; (d) Minimum matching Euclidean distance for varying vocabulary size.

Figure 8: Varying α (a) Minimum matching Euclidean distance, (b) Perplexity, (c) Estimation of
number of topics; (d) Estimation of number of topics for varying vocabulary size.

14

D Implementation details

In this section we give details about the implementations of the algorithms used in simulation studies
and real data. We implemented Conic Scan-and-Cover (CoSAC) algorithm in Python with the
help of Scipy (Jones et al., 2001–) sparse matrix modules. Geometric Dirichlet Means (GDM)
(Yurochkin & Nguyen, 2016) was implemented with the help of Scikit-learn (Pedregosa et al.,
2011) k-means implementation (with 10 restarts to avoid local minima of k-means) combined
with a geometric correction technique. Codes for CoSAC and GDM are available at https:
//github.com/moonfolk/Geometric-Topic-Modeling. For RecoverKL (Arora et al.,
2012) we applied code from one of the coauthors. To implement cscRecoverKL we used our CoSAC
implementation (Algorithm 1 with outlier threshold λ as in Algorithm 2) to ﬁnd anchor words and
then recovery routine from the aforementioned code. For the Gibbs sampling (Grifﬁths & Steyvers,
2004) we used an lda package in Python that utilizes Cython to achieve C speed. Gibbs sampler
was trained with α = 0.1, η = 0.01 and 500 iterations in simulations studies and α = 0.1, η = 0.1
and 1000 iterations in the NYTimes articles2 analysis. For the SVI (Hoffman et al., 2013) we used
Gensim implementation ( ˇReh˚uˇrek & Sojka, 2010) with automatic hyperparameters estimation, 50
iterations and 10 passes. Finally for HDP (Teh et al., 2006) we used C++ implementation with
default hyperparameter settings and 100 iterations. For all experiments (except large vocabulary
sizes and bigger α), per discussions in Sections 3.2 and 4, parameters of the CoSAC were set to
ω = 0.6, n = 0.001M and
as median of the centered and normalized document norms. Spherical
R
k-means post-processing step was run for 30 iterations. For cscRecoverKL we set ω = 0.4, λ = 0.015
(λ = 0.005 for real data) and
as corresponding median of the norms. Note that cscRecoverKL
takes word-to-word co-occurrence matrix as input, therefore sample size is V and "documents" are
rows of this matrix. Exploring distributional properties of the simplex spanned by the anchor words
is outside of the scope of this work, therefore parameter choices were made empirically based on the
visual analysis illustrated by Fig. 2. All simulated results are reported after 20 repetitions of the data
generation for each scenario and NYTimes results for LDA and HDP are reported over 10 reﬁts of
the corresponding Gibbs samplers.

R

2https://archive.ics.uci.edu/ml/datasets/bag+of+words

15

References

Allocation. NIPS, 2012.

Anandkumar, A., Foster, D. P., Hsu, D., Kakade, S. M., and Liu, Y. A spectral algorithm for Latent Dirichlet

Arora, S., Ge, R., Halpern, Y., Mimno, D., Moitra, A., Sontag, D., Wu, Y., and Zhu, M. A practical algorithm for

topic modeling with provable guarantees. arXiv preprint arXiv:1212.4777, 2012.

Blei, D. M., Ng, A. Y., and Jordan, M. I. Latent Dirichlet Allocation. J. Mach. Learn. Res., 3:993–1022, March

2003.

Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. Indexing by latent semantic

analysis. Journal of the American Society for Information Science, 41(6):391, Sep 01 1990.

Grifﬁths, Thomas L and Steyvers, Mark. Finding scientiﬁc topics. PNAS, 101(suppl. 1):5228–5235, 2004.

Hoffman, Ma. D., Blei, D. M., Wang, C., and Paisley, J. Stochastic variational inference. J. Mach. Learn. Res.,

14(1):1303–1347, May 2013.

Hsu, Wei-Shou and Poupart, Pascal. Online bayesian moment matching for topic modeling with unknown

number of topics. In Advances In Neural Information Processing Systems, pp. 4529–4537, 2016.

Jones, Eric, Oliphant, Travis, Peterson, Pearu, et al. SciPy: Open source scientiﬁc tools for Python, 2001–. URL

http://www.scipy.org/.

Lloyd, S. Least squares quantization in PCM. Information Theory, IEEE Transactions on, 28(2):129–137, Mar

Nguyen, XuanLong. Posterior contraction of the population polytope in ﬁnite admixture models. Bernoulli, 21

1982.

(1):618–646, 02 2015.

Olver, Frank W. J., Lozier, Daniel M., Boisvert, Ronald F., and Clark, Charles W. NIST handbook of mathematical

functions, cambridge university press, 2010. URL http://dlmf.nist.gov/8.17.

Pedregosa, Fabian, Varoquaux, Gaël, Gramfort, Alexandre, Michel, Vincent, Thirion, Bertrand, Grisel, Olivier,
Blondel, Mathieu, Prettenhofer, Peter, Weiss, Ron, Dubourg, Vincent, et al. Scikit-learn: Machine learning in
python. Journal of Machine Learning Research, 12(Oct):2825–2830, 2011.

Pritchard, Jonathan K, Stephens, Matthew, and Donnelly, Peter. Inference of population structure using multilocus

genotype data. Genetics, 155(2):945–959, 2000.

ˇReh˚uˇrek, Radim and Sojka, Petr. Software Framework for Topic Modelling with Large Corpora. In Proceedings
of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pp. 45–50, Valletta, Malta, May 2010.
ELRA. http://is.muni.cz/publication/884893/en.

Tang, Jian, Meng, Zhaoshi, Nguyen, Xuanlong, Mei, Qiaozhu, and Zhang, Ming. Understanding the limiting
In Proceedings of The 31st International

factors of topic modeling via posterior contraction analysis.
Conference on Machine Learning, pp. 190–198. ACM, 2014.

Teh, Y. W., Jordan, M. I., Beal, M. J., and Blei, D. M. Hierarchical dirichlet processes. Journal of the american

statistical association, 101(476), 2006.

Xu, Wei, Liu, Xin, and Gong, Yihong. Document clustering based on non-negative matrix factorization. In
Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in
Informaion Retrieval, SIGIR ’03, pp. 267–273. ACM, 2003.

Yurochkin, Mikhail and Nguyen, XuanLong. Geometric dirichlet means algorithm for topic inference. In

Advances in Neural Information Processing Systems, pp. 2505–2513, 2016.

16

7
1
0
2
 
t
c
O
 
9
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
2
5
9
2
0
.
0
1
7
1
:
v
i
X
r
a

Conic Scan-and-Cover algorithms for
nonparametric topic modeling

Mikhail Yurochkin
Department of Statistics
University of Michigan
moonfolk@umich.edu

Aritra Guha
Department of Statistics
University of Michigan
aritra@umich.edu

XuanLong Nguyen
Department of Statistics
University of Michigan
xuanlong@umich.edu

Abstract

We propose new algorithms for topic modeling when the number of topics is
unknown. Our approach relies on an analysis of the concentration of mass and
angular geometry of the topic simplex, a convex polytope constructed by taking
the convex hull of vertices representing the latent topics. Our algorithms are shown
in practice to have accuracy comparable to a Gibbs sampler in terms of topic
estimation, which requires the number of topics be given. Moreover, they are one
of the fastest among several state of the art parametric techniques.1 Statistical
consistency of our estimator is established under some conditions.

1

Introduction

A well-known challenge associated with topic modeling inference can be succinctly summed up
by the statement that sampling based approaches may be accurate but computationally very slow,
e.g., Pritchard et al. (2000); Grifﬁths & Steyvers (2004), while the variational inference approaches
are faster but their estimates may be inaccurate, e.g., Blei et al. (2003); Hoffman et al. (2013). For
nonparametric topic inference, i.e., when the number of topics is a priori unknown, the problem
becomes more acute. The Hierarchical Dirichlet Process model (Teh et al., 2006) is an elegant
Bayesian nonparametric approach which allows for the number of topics to grow with data size, but
its sampling based inference is much more inefﬁcient compared to the parametric counterpart. As
pointed out by Yurochkin & Nguyen (2016), the root of the inefﬁciency can be traced to the need for
approximating the posterior distributions of the latent variables representing the topic labels — these
are not geometrically intrinsic as any permutation of the labels yields the same likelihood.

A promising approach in addressing the aforementioned challenges is to take a convex geometric
perspective, where topic learning and inference may be formulated as a convex geometric problem: the
observed documents correspond to points randomly drawn from a topic polytope, a convex set whose
vertices represent the topics to be inferred. This perspective has been adopted to establish posterior
contraction behavior of the topic polytope in both theory and practice (Nguyen, 2015; Tang et al.,
2014). A method for topic estimation that exploits convex geometry, the Geometric Dirichlet Means
(GDM) algorithm, was proposed by Yurochkin & Nguyen (2016), which demonstrates attractive
behaviors both in terms of running time and estimation accuracy. In this paper we shall continue to
amplify this viewpoint to address nonparametric topic modeling, a setting in which the number of
topics is unknown, as is the distribution inside the topic polytope (in some situations).

We will propose algorithms for topic estimation by explicitly accounting for the concentration of
mass and angular geometry of the topic polytope, typically a simplex in topic modeling applications.
The geometric intuition is fairly clear: each vertex of the topic simplex can be identiﬁed by a ray
emanating from its center (to be deﬁned formally), while the concentration of mass can be quantiﬁed

1Code is available at https://github.com/moonfolk/Geometric-Topic-Modeling.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

for the cones hinging on the apex positioned at the center. Such cones can be rotated around the
center to scan for high density regions inside the topic simplex — under mild conditions such cones
can be constructed efﬁciently to recover both the number of vertices and their estimates.

We also mention another fruitful approach, which casts topic estimation as a matrix factorization
problem (Deerwester et al., 1990; Xu et al., 2003; Anandkumar et al., 2012; Arora et al., 2012). A
notable recent algorithm coming from the matrix factorization perspective is RecoverKL (Arora et al.,
2012), which solves non-negative matrix factorization (NMF) efﬁciently under assumptions on the
existence of so-called anchor words. RecoverKL remains to be a parametric technique — we will
extend it to a nonparametric setting and show that the anchor word assumption appears to limit the
number of topics one can efﬁciently learn.

Our paper is organized as follows. In Section 2 we discuss recent developments in geometric topic
modeling and introduce our approach; Sections 3 and 4 deliver the contributions outlined above;
Section 5 demonstrates experimental performance; we conclude with a discussion in Section 6.

2 Geometric topic modeling

RV

Background and related work In this section we present the convex geometry of the Latent
Dirichlet Allocation (LDA) model of Blei et al. (2003), along with related theoretical and algorithmic
results that motivate our work. Let V be vocabulary size and ∆V −1 be the corresponding vocabulary
probability simplex. Sample K topics (i.e., distributions on words) βk ∼ DirV (η), k = 1, . . . , K,
+. Next, sample M document-word probabilities pm residing in the topic simplex
where η
B := Conv(β1, . . . , βK) (cf. Nguyen (2015)), by ﬁrst generating their barycentric coordinates
(i.e., topic proportions) θm ∼ DirK(α) and then setting pm := (cid:80)
k βkθmk for m = 1, . . . , M and
+ . Finally, word counts of the m-th document can be sampled wm ∼ Mult(pm, Nm), where
α
∈
N is the number of words in document m. The above model is equivalent to the LDA when
Nm
individual words to topic label assignments are marginalized out.

RK

∈

∈

Nguyen (2015) established posterior contraction rates of the topic simplex, provided that αk
k
and either number of topics K is known or topics are sufﬁciently separated in terms of the Euclidean
distance. Yurochkin & Nguyen (2016) devised an estimate for B, taken to be a ﬁxed unknown
quantity, by formulating a geometric objective function, which is minimized when topic simplex B
is close to the normalized documents ¯wm := wm/Nm. They showed that the estimation of topic
proportions θm given B simply reduces to taking barycentric coordinates of the projection of ¯wm
onto B. To estimate B given K, they proposed a Geometric Dirichlet Means (GDM) algorithm,
which operated by performing a k-means clustering on the normalized documents, followed by a
geometric correction for the cluster centroids. The resulting algorithm is remarkably fast and accurate,
supporting the potential of the geometric approach. The GDM is not applicable when K is unknown,
but it provides a motivation which our approach is built on.

≤

∀

1

The Conic Scan-and-Cover approach To enable the inference of B when K is not known, we
need to investigate the concentration of mass inside the topic simplex. It sufﬁces to focus on two
types of geometric objects: cones and spheres, which provide the basis for a complete coverage of the
simplex. To gain intuition of our procedure, which we call Conic Scan-and-Cover (CoSAC) approach,
imagine someone standing at a center point of a triangular dark room trying to ﬁgure out all corners
with a portable ﬂashlight, which can produce a cone of light. A room corner can be identiﬁed with
the direction of the farthest visible data objects. Once a corner is found, one can turn the ﬂashlight to
another direction to scan for the next ones. See Fig. 1a, where red denotes the scanned area. To make
sure that all corners are detected, the cones of light have to be open to an appropriate range of angles
so that enough data objects can be captured and removed from the room. To make sure no false
corners are declared, we also need a suitable stopping criterion, by relying only on data points that lie
beyond a certain spherical radius, see Fig. 1b. Hence, we need to be able to gauge the concentration
of mass for suitable cones and spherical balls in ∆V −1. This is the subject of the next section.

3 Geometric estimation of the topic simplex

We start by representing B in terms of its convex and angular geometry. First, B is centered at a point
denoted by Cp. The centered probability simplex is denoted by ∆V −1
.

∆V −1

x+Cp

RV

:=

0

x
{

∈

|

∈

}

2

t!

(a) An incomplete coverage using
3 cones (containing red points).

(b) Complete coverage using
3 cones (red) and a ball (yellow).

(c) Cap Λc(v1) and cone Sω(v1).

Figure 1: Complete coverage of topic simplex by cones and a spherical ball for K = 3, V = 3.

∆V −1
0

Cp

Then, write bk := βk
−
m = 1, . . . , M . Note that re-centering leaves corresponding barycentric coordinates θm
unchanged. Moreover, the extreme points of centered topic simplex ˜B := Conv
RV and corresponding radii Rk
now be represented by their directions vk
bk = Rkvk for any k = 1, . . . , K.

for k = 1, . . . , K and ˜pm := pm

b1, . . . , bK
{
∈

for
∆K−1
∈
can
R+ such that

Cp

−

∈

∈

∈

}

∆V −1
0

3.1 Coverage of the topic simplex

The ﬁrst step toward formulating a CoSAC approach is to show how ˜B can be covered with
exactly K cones and one spherical ball positioned at Cp. A cone is deﬁned as set
ω(v) :=
, where we employ the angular distance (a.k.a. cosine distance)
dcos(v, p) < ω
p
{
}
|
cos(v, p) and cos(v, p) is the cosine of angle ∠(v, p) formed by vectors v and p.
dcos(v, p) := 1

∆V −1
0

∈

S

−

The Conical coverage

exactly K cones, that is,

It is possible to choose ω so that the topic simplex can be covered with
K
(cid:83)
˜B. Moreover, each cone contains exactly one vertex. Suppose
k=1 S

ω(vk)

⊇

that Cp is the incenter of the topic simplex ˜B, with r being the inradius. The incenter and inradius
correspond to the maximum volume sphere contained in ˜B. Let ai,k denote the distance between
the i-th and k-th vertex of ˜B, with amin
amax for all i, k, and Rmax, Rmin such that
Rmin
2
(cid:107)
Proposition 1. For simplex ˜B and ω
max

r/Rmax and ω2 =
ω(v) around any vertex direction

k = 1, . . . , K. Then we can establish the following.

(ω1, ω2), where ω1 = 1

, the cone

Rk :=

Rmax

max), max

max)/(2R2

ai,k

(1

bk

−

≤

≤

≤

≤

∈

∀

(cid:107)

i,k=1,...,K

cos(bi, bk)
}

−

(a2
{

S

v of ˜B contains exactly one vertex. Moreover, complete coverage holds:

K
(cid:83)
k=1 S

ω(vk)

˜B.

⊇

(cid:16)

We say there is an angular separation if cos(bi, bk)
0 for any i, k = 1, . . . , K (i.e., the angles for
(cid:17)
all pairs are at least π/2), then ω
. Thus, under angular separation, the range ω
∅
that allows for full coverage is nonempty independently of K. Our result is in agreement with that of
Nguyen (2015), whose result suggested that topic simplex B can be consistently estimated without
knowing K, provided there is a minimum edge length amin > 0. The notion of angular separation
leads naturally to the Conic Scan-and-Cover algorithm. Before getting there, we show a series of
results allowing us to further extend the range of admissible ω.

r
Rmax

≤
=

, 1

−

∈

1

The inclusion of a spherical ball centered at Cp allows us to expand substantially the range of ω
for which conical coverage continues to hold. In particular, we can reduce the lower bound on ω in
Proposition 1, since we only need to cover the regions near the vertices of ˜B with cones using the
following proposition. Fig. 1b provides an illustration.
Proposition 2. Let B(Cp,

> 0; ω1, ω2 given in Prop. 1, and

) =

RV

Cp

2

˜p

∈

˜p
{
−
Rk sin2(bi, bk)

|(cid:107)

,
≤ R}

(cid:107)

R
(cid:115)

+ cos(bi, bk)

1

R





(cid:26)

min
i,k

R

R2

k sin2(bi, bj)

−

2
R



(cid:27)

 , 1

,

(1)

ω3 := 1

min

−

3

then we have

K
(cid:83)
k=1 S

ω(vk)

B(Cp,

∪

)
R

⊇

˜B whenever ω

(min
{

∈

ω1, ω3

, ω2).
}

Notice that as
Rmax, the admissible
→
range for ω in Prop. 2 results in a substantial strengthening from Prop. 1. It is worth noting that the
above two geometric propositions do not require any distributional properties inside the simplex.

Rmax , the value of ω3

0. Hence if

Rmin

R →

R ≤

≈

In practice complete coverage may fail if ω and

Coverage leftovers
are chosen outside of
corresponding ranges suggested by the previous two propositions. In that case, it is useful to note that
leftover regions will have a very low mass. Next we quantify the mass inside a cone that does contain
a vertex, which allows us to reject a cone that has low mass, therefore not containing a vertex in it.
Proposition 3. The cone Sω(v1) whose axis is a topic direction v1 has mass

R

ω(v1)) > P(Λc(b1)) =
P(
S
(cid:80)
c
((cid:80)

i(cid:54)=1 αi(1
−
i(cid:54)=1 αi)Γ(α1)Γ((cid:80)

c)α1 Γ((cid:80)K

i=1 αi)
i(cid:54)=1 αi)

1

(cid:80)

−

(1

θ1)
(cid:80)

(cid:82) 1
1−c θα1−1
(cid:82) 1
0 θα1−1
(1
−
c (cid:80)K
(cid:20)
i=1 αi
(cid:80)
i(cid:54)=1 αi + 1

1 +

θ1)

1

i(cid:54)=1 αi−1dθ1
i(cid:54)=1 αi−1dθ1

=

+

c2((cid:80)K
((cid:80)

i=1 αi)((cid:80)K
i(cid:54)=1 αi + 1)((cid:80)

i=1 αi + 1)
i(cid:54)=1 αi + 2)

(cid:21)
,

+

· · ·

(2)

where Λc(b1) is the simplicial cap of
S
the corresponding base of ˜B and cutting adjacent edges of ˜B in the ratio c : (1

ω(v1) which is composed of vertex b1 and a base parallel to

c).

See Fig. 1c for an illustration for the simplicial cap described in the proposition. Given the lower
bound for the mass around a cone containing a vertex, we have arrived at the following guarantee.
Proposition 4. For λ

P(Λcλ(bk)) and let ωλ be such that

(0, 1), let cλ be such that λ = min

∈
(cid:32)(cid:32)
2

(cid:115)
1

cλ =

(cid:33)

r2
R2

max

−

k

−

(sin(d) cot(arccos(1

ωλ)) + cos(d))

,

(3)

−

(cid:33)−1

where angle d

min
i,k

≤

∠(bk, bk

−

bi). Then, as long as

ω

∈

≥

S

the bound P(

ω(vk))

λ holds for all k = 1, . . . , K.

3.2 CoSAC: Conic Scan-and-Cover algorithm

(cid:18)

ωλ, max

(cid:18) a2
2R2

max

max

, max
i,k=1,...,K

(1

−

cos(bi, bk)

,

(cid:19)(cid:19)

(4)

Having laid out the geometric foundations, we are ready to present the Conic Scan-and-Cover
(CoSAC) algorithm, which is a scanning procedure for detecting the presence of simplicial vertices
based on data drawn randomly from the simplex. The idea is simple: iteratively pick the farthest point
from the center estimate ˆCp := 1
ω(v) for some suitably
M
chosen ω, and remove all the data residing in this cone. Repeat until there is no data point left.

m pm, say v, then construct a cone

(cid:80)

S

(cid:107)

1, . . . , M
}
{
2 and update A := A
\ S

be the index set of the initially unseen data, then set v :=
ω(v). The parameter ω needs to be sufﬁciently large to ensure

Speciﬁcally, let A =
˜pm
argmax
˜pm:m∈A (cid:107)
that the farthest point is a good estimate of a true vertex, and that the scan will be completed in exactly
K iterations; ω needs to be not too large, so that
ω(v) does not contain more than one vertex. The
S
existence of such ω is guaranteed by Prop. 1. In particular, for an equilateral ˜B, the condition of the
Prop. 1 is satisﬁed as long as ω

1, 1 + 1/(K

1/√K

1)).

(1

∈

−

−

−

In our setting, K is unknown. A smaller ω would be a more robust choice, and accordingly the set A
will likely remain non-empty after K iterations. See the illustration of Fig. 1a, where the blue regions
correspond to A after K = 3 iterations of the scan. As a result, we proceed by adopting a stopping
, which allows us
criteria based on Prop. 2: the procedure is stopped as soon as
∀
to complete the scan in K iterations (as in Fig. 1b for K = 3).

2 <
(cid:107)

˜pm
(cid:107)

m

R

A

∈

The CoSAC algorithm is formally presented by Algorithm 1. Its running is illustrated in Fig. 2,
where we show iterations 1, 26, 29, 30 of the algorithm by plotting norms of the centered documents

4

S

ω(v) against cosine distance to the chosen direction of a topic. Iteration
in the active set A and cone
30 (right) satisﬁes stopping criteria and therefore CoSAC recovered correct K = 30. Note that this
type of visual representation can be useful in practice to verify choices of ω and
. The following
theorem establishes the consistency of the CoSAC procedure.
Theorem 1. Suppose
β1, . . . , βK
pm := (cid:80)
k βkθmk for m = 1, . . . , M and α
ˆβ1, . . . , ˆβ ˆK}
{
(cid:32)(cid:40)
P

are the true topics, incenter Cp is given, θm ∼ DirK(α) and
+ . Let ˆK be the estimated number of topics,
as in Prop. 2. Then

be the output of Algorithm 1 trained with ω and

(cid:15) > 0,

RK

R

R

(cid:41)

(cid:33)

∈

∀

{

}

1, . . . , ˆK

∈ {

}

∪ {

K

= ˆK

}

→

0 as M

.
→ ∞

min
j∈{1,..., ˆK}(cid:107)

βi

−

ˆβj

(cid:107)

> (cid:15) , for any i

Remark We found the choices ω = 0.6 and
}
(cid:107)
practice and agreeing with our theoretical results. From Prop. 3 it follows that choosing
R
K−1 ( c
K
1−c )1−1/K
length is equivalent to choosing ω resulting in an edge cut ratio c such that 1
1/2, then c
ω

≥
2K )K/(K−1), which, for any equilateral topic simplex B, is satisﬁed by setting

( K−1
(0.3, 1), provided that K

to be robust in
as median

2000 based on the Eq. (3).

to be median of

2, . . . ,
(cid:107)

˜pM

{(cid:107)

˜p1

R

−

≤

(cid:107)

2

∈

≤

4 Document Conic Scan-and-Cover algorithm

In the topic modeling problem, pm for m = 1, . . . , M are not given. Instead, under the bag-of-words
assumption, we are given the frequencies of words in documents w1, . . . , wM which provide a point
and length of
estimate ¯wm := wm/Nm for the pm. Clearly, if number of documents M
→ ∞
m, we can use Algorithm 1 with the plug-in estimates ¯wm in place of pm,
documents Nm
(cid:80) ¯wm. In practice, M and Nm are
since ¯wm
ﬁnite, some of which may take relatively small values. Taking the topic direction to be the farthest
, may no
point in the topic simplex, i.e., v = argmax
˜wm

pm. Moreover, Cp will be estimated by ˆCp := 1
M

→ ∞ ∀

ˆCp

→

∆V −1
0

2, where ˜wm := ¯wm
(cid:107)

−

∈

˜wm:m∈A (cid:107)

longer yield a robust estimate, because the variance of this topic direction estimator can be quite high
(in Proposition 5 we show that it is upper bounded with (1

1/V )/Nm).

To obtain improved estimates, we propose a technique that we call “mean-shifting”. Instead of taking
the farthest point in the simplex, this technique is designed to shift the estimate of a topic to a high
density region, where true topics are likely to be found. Precisely, given a (current) cone
ω(v), we
cos( ˜wm, v)). In other words,
re-position the cone by updating v := argmin

˜wm

(cid:80)

S

v

m∈Sω(v) (cid:107)

2(1
(cid:107)

−

we re-position the cone by centering it around the mean direction of the cone weighted by the norms
of the data points inside, which is simply given by v
ω(v)). This results in
reduced variance of the topic direction estimate, due to the averaging over data residing in the cone.

m∈Sω(v) ˜wm/ card(
S

(cid:80)

∝

−

The mean-shifting technique may be slightly modiﬁed and taken as a local update for a subsequent
optimization which cycles through the entire set of documents and iteratively updates the cones. The
optimization is with respect to the following weighted spherical k-means objective:

min
(cid:107)vk(cid:107)2=1,k=1,...K

˜wm
(cid:107)

2(1
(cid:107)

−

cos(vk, ˜wm)),

(5)

K
(cid:88)

(cid:88)

k=1

m∈Sk(vk)

Sk(vk) =

{
1, . . . , M

dcos(vk, ˜pm) < dcos(vl, ˜pi)
m
|
(this is different from

where cones Sk(vk) =
K
(cid:70)
k=1
optimization is to use full data for estimation of topic directions, hence further reducing the variance
due to short documents. The connection between objective function (5) and topic simplex estimation
is given in the Supplement. Finally, obtain topic norms Rk along the directions vk using maximum
projection: Rk := max

. Our entire procedure is summarized in Algorithm 2.

ω(vk)). The rationale of spherical k-means

yield a disjoint data partition

vk, ˜wm

= k

l
∀

S

{

}

}

m:m∈Sk(vk)(cid:104)

(cid:105)

Remark In Step 9 of the algorithm, cone
ω(v)) <
λM , for some small constant λ, is discarded because this is likely an outlier region that does not actu-
k,
ally contain a true vertex. The choice of λ is governed by results of Prop. 4. For small αk = 1/K,

ω(v) with a very low cardinality, i.e., card(

S

S

∀

5

P(Λc)

λ

≤

≈

c(K−1)/K
(K−1)(1−c) and for an equilateral ˜B we can choose d such that cos(d) =

ging these values into Eq. (3) leads to c =

(cid:18)(cid:16)

(cid:113)
2

1

(cid:17) (cid:18)(cid:113) K−1
2K (

1
K2

−

1−ω

√1−(1−ω)2 ) +

(cid:113) K+1

2K . Plug-
(cid:19)(cid:19)−1

(cid:113) K+1
2K

.

Now, plugging in ω = 0.6 we obtain λ
K to get a sense of λ, we now make a conservative choice λ = 0.001, so that (K)−1 > λ
As a result, a topic is rejected if the corresponding cone contains less than 0.1% of the data.

K −1 for large K. Our approximations were based on large
K < 1000.

≤

∀

Finding anchor words using Conic Scan-and-Cover Another approach to reduce the noise is
to consider the problem from a different viewpoint, where Algorithm 1 will prove itself useful.
RecoverKL by Arora et al. (2012) can identify topics with diminishing errors (in number of documents
M ), provided that topics contain anchor words. The problem of ﬁnding anchor words geometrically
reduces to identifying rows of the word-to-word co-occurrence matrix that form a simplex containing
other rows of the same matrix (cf. Arora et al. (2012) for details). An advantage of this approach
no matter the
is that noise in the word-to-word co-occurrence matrix goes to zero as M
document lengths, hence we can use Algorithm 1 with "documents" being rows of the word-to-word
co-occurrence matrix to learn anchor words nonparametrically and then run RecoverKL to obtain
topic estimates. We will call this procedure cscRecoverKL.

→ ∞

Algorithm 1 Conic Scan-and-Cover (CoSAC)
Input: document generating distributions p1, . . . , pM ,

angle threshold ω, norm threshold

R

(cid:80)

m pm {ﬁnd center};

Output: topics β1, . . . , βk
1: ˆCp = 1
M
{initialize active set};
1, . . . , M
2: A1 =
}
3: while
˜pm
Ak :
m
(cid:107)
˜pm
vk = argmax
4:
˜pm:m∈Ak (cid:107)

2 >
(cid:107)
R
2 {ﬁnd topic}
(cid:107)

{
∃

do

∈

ω(vk) =
5:
S
6: Ak = Ak
βk = vk + ˆCp, k = k + 1 {compute topic}
7:
8: end while

m : dcos(˜pm, vk) < ω
{
\ S

ω(vk) {update active set}

}

˜pm := pm

ˆCp for m = 1, . . . , M {center the data}

−

k = 1 {initialize topic count}

{ﬁnd cone of near documents}

Figure 2: Iterations 1, 26, 29, 30 of the Algorithm 1. Red are the documents in the cone
are the documents in the active set Ak+1 for next iteration. Yellow are documents
˜pm
(cid:107)

ω(vk); blue
S
.
2 <
(cid:107)

R

5 Experimental results

5.1 Simulation experiments

In the simulation studies we shall compare CoSAC (Algorithm 2) and cscRecoverKL based on
Algorithm 1 both of which don’t have access to the true K, versus popular parametric topic modeling
approaches (trained with true K): Stochastic Variational Inference (SVI), Collapsed Gibbs sampler,
RecoverKL and GDM (more details in the Supplement). The comparisons are done on the basis of
minimum-matching Euclidean distance, which quantiﬁes distance between topic simplices (Tang
et al., 2014), and running times (perplexity scores comparison is given in Fig. 7). Lastly we will
demonstrate the ability of CoSAC to recover correct number of topics for a varying K.

6

ˆCp for m = 1, . . . , M {center the data}

−

k = 1 {initialize topic count}

Algorithm 2 CoSAC for documents
Input: normalized documents ¯w1, . . . , ¯wM ,
angle threshold ω, norm threshold

m ¯wm {ﬁnd center};

(cid:80)

Output: topics β1, . . . , βk
1: ˆCp = 1
M
1, . . . , M
2: A1 =
}
3: while
Ak :
m
vk = argmax
4:

{
∃

∈

{initialize active set};
˜wm
(cid:107)
˜wm
˜wm:m∈Ak (cid:107)

2 >
(cid:107)
2 {initialize direction}
(cid:107)
while vk not converged do {mean-shifting}

do

R

˜wm := ¯wm

, outlier threshold λ

R

{ﬁnd cone of near documents}

}

ω(vk)) {update direction}

m∈Sω(vk) ˜wm/ card(
S

m : dcos( ˜wm, vk) < ω
{

5:
ω(vk) =
6:
vk = (cid:80)
S
7:
end while
8:
9: Ak = Ak
if card(
10: end while
11: v1, . . . , vk = weighted spherical k-means (v1, . . . , vk, ˜w1, . . . , ˜wM )
12: for l in
1, . . . , k
{
13: Rl := max

{ﬁnd topic length along direction vl}

ω(vk) {update active set}

\ S
ω(vk)) > λM

vl, ˜wm

do

S

}

then k = k + 1 {record topic direction}

m:m∈Sl(vl)(cid:104)

(cid:105)

βl = Rlvl + ˆCp {compute topic}

14:
15: end for

Figure 3: Minimum matching Euclidean distance for (a) varying corpora size, (b) varying length of
documents; (c) Running times for varying corpora size; (d) Estimation of number of topics.

Figure 4: Gibbs sampler convergence analysis for (a) Minimum matching Euclidean distance for
corpora sizes 1000 and 5000; (b) Perplexity for corpora sizes 1000 and 5000; (c) Perplexity for
NYTimes data.

Estimation of the LDA topics First we evaluate the ability of CoSAC and cscRecoverKL to
estimate topics β1, . . . , βK, ﬁxing K = 15. Fig. 3(a) shows performance for the case of fewer
[100, 10000] but longer Nm = 500 documents (e.g. scientiﬁc articles, novels, legal documents).
M
CoSAC demonstrates performance comparable in accuracy to Gibbs sampler and GDM.

∈

[25, 300] documents (e.g. news
Next we consider larger corpora M = 30000 of shorter Nm
articles, social media posts). Fig. 3(b) shows that this scenario is harder and CoSAC matches the
performance of Gibbs sampler for Nm
75. Indeed across both experiments CoSAC only made
mistakes in terms of K for the case of Nm = 25, when it was underestimating on average by 4 topics

≥

∈

7

and for Nm = 50 when it was off by around 1, which explains the earlier observation. Experiments
with varying V and α are given in the Supplement.

It is worth noting that cscRecoverKL appears to be strictly better than its predecessor. This suggests
that our procedure for selection of anchor words is more accurate in addition to being nonparametric.

Running time A notable advantage of the CoSAC algorithm is its speed. In Fig. 3(c) we see
that Gibbs, SVI, GDM and CoSAC all have linear complexity growth in M , but the slopes are very
different and approximately are INm for SVI and Gibbs (where I is the number of iterations which
has to be large enough for convergence), number of k-means iterations to converge for GDM and is
of order K for the CoSAC procedure making it the fastest algorithm of all under consideration.

Next we compare CoSAC to per iteration quality of the Gibbs sampler trained with 500 iterations for
M = 1000 and M = 5000. Fig. 4(b) shows that Gibbs sampler, when true K is given, can achieve
good perplexity score as fast as CoSAC and outperforms it as training continues, although Fig. 4(a)
suggests that much longer training time is needed for Gibbs sampler to achieve good topic estimates
and small estimation variance.

Estimating number of topics Model selection in the LDA context is a quite challenging task and,
to the best of our knowledge, there is no "go to" procedure. One of the possible approaches is based
on reﬁtting LDA with multiple choices of K and using Bayes Factor for model selection (Grifﬁths &
Steyvers, 2004). Another option is to adopt the Hierarchical Dirichlet Process (HDP) model, but we
should understand that it is not a procedure to estimate K of the LDA model, but rather a particular
prior on the number of topics, that assumes K to grow with the data. A more recent suggestion is to
slightly modify LDA and use Bayes moment matching (Hsu & Poupart, 2016), but, as can be seen
from Figure 2 of their paper, estimation variance is high and the method is not very accurate (we
tried it with true K = 15 and it took above 1 hour to ﬁt and found 35 topics). Next we compare
Bayes factor model selection versus CoSAC and cscRecoverKL for K
[5, 50]. Fig. 3(d) shows that
CoSAC consistently recovers exact number of topics in a wide range.

∈

We also observe that cscRecoverKL does not estimate K well (underestimates) in the higher range.
This is expected because cscRecoverKL ﬁnds the number of anchor words, not topics. The former
is decreasing when later is increasing. Attempting to ﬁt RecoverKL with more topics than there
are anchor words might lead to deteriorating performance and our modiﬁcation can address this
limitation of the RecoverKL method.

5.2 Real data analysis

In this section we demonstrate CoSAC algorithm for topic modeling on one of the standard bag
of words datasets — NYTimes news articles. After preprocessing we obtained M
130, 000
documents over V = 5320 words. Bayes factor for the LDA selected the smallest model among
[80, 195], while CoSAC selected 159 topics. We think that disagreement between the two
K
procedures is attributed to the misspeciﬁcation of the LDA model when real data is in play, which
affects Bayes factor, while CoSAC is largely based on the geometry of the topic simplex.

≈

∈

The results are summarized in Table 1 — CoSAC found 159 topics in less than 20min; cscRecoverKL
estimated the number of anchor words in the data to be 27 leading to fewer topics. Fig. 4(c) compares
CoSAC perplexity score to per iteration test perplexity of the LDA (1000 iterations) and HDP (100
iterations) Gibbs samplers. Text ﬁles with top 20 words of all topics are available on GitHub. We
note that CoSAC procedure recovered meaningful topics, contextually similar to LDA and HDP (e.g.
elections, terrorist attacks, Enron scandal, etc.) and also recovered more speciﬁc topics about Mike
Tyson, boxing and case of Timothy McVeigh which were present among HDP topics, but not LDA
ones. We conclude that CoSAC is a practical procedure for topic modeling on large scale corpora
able to ﬁnd meaningful topics in a short amount of time.

6 Discussion

We have analyzed the problem of estimating topic simplex without assuming number of vertices
(i.e., topics) to be known. We showed that it is possible to cover topic simplex using two types of
geometric shapes, cones and a sphere, leading to a class of Conic Scan-and-Cover algorithms. We

8

Table 1: Modeling topics of NYTimes articles

K Perplexity

Coherence

Time

cscRecoverKL
HDP Gibbs
LDA Gibbs
CoSAC

221

27
5
±
80
159

1477
1520

2603
1.6
±
1.5
±
1568

442
300

−
−

-238
1.7
±
0.7
±
-322

37 min
35 hours
5.3 hours
19 min

then proposed several geometric correction techniques to account for the noisy data. Our procedure is
accurate in recovering the true number of topics, while remaining practical due to its computational
speed. We think that angular geometric approach might allow for fast and elegant solutions to other
clustering problems, although as of now it does not immediately offer a unifying problem solving
framework like MCMC or variational inference. An interesting direction in a geometric framework is
related to building models based on geometric quantities such as distances and angles.

Acknowledgments

This research is supported in part by grants NSF CAREER DMS-1351362, NSF CNS-1409303, a
research gift from Adobe Research and a Margaret and Herman Sokol Faculty Award.

A Proofs of main theorems

We start by reminding the reader of our geometric setup. First, topic simplex B := Conv(β1, . . . , βK)
is centered at a point denoted by Cp. Let ∆V −1
— centered
probability simplex. Then, write bk := βk
−
∆V −1
0
∆K−1 unchanged. Moreover, the extreme points of centered topic simplex ˜B := Conv
can now be represented by their directions vk
bk = Rkvk for any k = 1, . . . , K.

Cp
∈
for m = 1, . . . , M . Note that re-centering leaves corresponding barycentric coordinates θm
∈
b1, . . . , bK
{
}
R+ such that

∈
for k = 1, . . . , K and ˜pm := pm

RV and corresponding radii Rk

RV : x + Cp

:=
x
{
∆V −1
0

∆V −1

0
Cp

−

∈

∈

∈

∈

}

A.1 Coverage of the topic simplex

Suppose that Cp is the incenter of the topic simplex ˜B, with r being the inradius. Recall that the
incenter and inradius correspond to the maximum volume sphere inside ˜B. Let ai,k denote the
distance between the ith and kth vertex of ˜B, with amin
amax for all i, k, and Rmax, Rmin
such that Rmin
Rmax
bk
(cid:107)
Proposition 1. For simplex ˜B and ω
max

r/Rmax and ω2 =
ω(v) around any vertex direction

(ω1, ω2), where ω1 = 1

≤
k = 1, . . . , K

, the cone

Rk :=

max), max

max)/(2R2

ai,k

(1

≤

−

≤

≤

∈

∀

(cid:107)

2

i,k=1,...,K

cos(bi, bk)
}

−

(a2
{

S

v of ˜B contains exactly one vertex. Moreover, complete coverage holds:

K
(cid:83)
k=1 S

ω(vk)

˜B.

⊇

min

Proof. Let ω0 = a2
ω(vk) does not contain
any other vertices. This can be explained as follows. Fix k, and choose i
= k. Deﬁne
1, . . . , K
φi,k as the angle at Cp made by the side connecting the vertex i and vertex k. Then from the cosine
law for triangles, we have

, for any ω
}

. Then, for any k

1, . . . , K

S
∈ {

∈ {

ω0,

2R2

} (cid:54)

≤

max

Now, for any φ

min
i,k
than vertex k, for any k. Now φ1 = min
i,k

φi,k, with ωφ = 1

≤

cos(φi,k) =

R2

i + R2

k −
2RiRk
cos(φ), the cone

a2
i,k

.

S

−
φi,k satisﬁes

ωφ(vk) does not cover any vertex other

1

cos(φ1)

−

a2
min
2R2
max −

≤

(Rmax

Rmin)2

−

2RmaxRmin ≤

a2
min
2R2

max

.

9

C

∠(bi, bk)

Rk

arccos(1

−

ωR)
B

A

R

R

Figure 5: C : kth vertex point, A : point where the adjacent side to the vertex has been cut off by the
sphere, Rk: distance to kth vertex from incenter,

: radius of sphere, B : incenter

from which we obtain the upper bound for ω. For the lower bound, consider for vertex k,

cone connecting the incenter to facial incenters of facets containing vertex k. Then

(vk) the
˜B.

S
(vk)

K
(cid:83)
k=1 S

⊇
cos(φ2), with φ2 satisfying cos(φ2)
≤
a2
min is needed to ensure

max ≤

Now for each k,

(vk)

ω2 (vk), where ω2 = 1

. From this we get the lower bound. The restriction 2R2

⊆ S

−

min
k∈{1,...,K}

r
Rk

that the set (cid:8)ω : 1

S

−

(

r
Rmax

)

ω

≤

≤

( a2
2R2

max

max

)(cid:9) is non-empty.

Proposition 2. Let B(Cp,

) =

˜p

{

∈

R

RV

˜p

|(cid:107)

−

Cp

2
(cid:107)

,
≤ R}

> 0; ω1, ω2 given in Prop. 1, and

ω3 := 1

min

−





(cid:26)

min
i,k

Rk sin2(bi, bk)

+ cos(bi, bk)

1

R

R2

k sin2(bi, bj)

−

2
R



(cid:27)

 , 1

,

(6)

R
(cid:115)

then we have

K
(cid:83)
k=1 S

ω(vk)

B(Cp,

∪

)
R

⊇

˜B whenever ω

(min
{

∈

ω1, ω3

, ω2).
}

ωi,k) be the angle formed by the line joining the kth vertex to the
Proof. Let φi,k = arccos(1
incenter Cp and the radial vector from incenter to the point where the sphere cuts the edge connecting
i and k (segment AB on Fig. 5). From the sine law for a triangle we have

−

cos(φi,k) + cot(bi, bk) sin(φi,k)

= 0.

Rk

R

−

(7)

(cid:19)

Solving for φi,k we have cos(φi,k) =

Rk sin2(bi,bk)
R
since we must choose the largest such φ over all i and k, the bound follows immediately. Notice

k sin2(bi,bk)
R2

+ cos(bi, bk)

. Now,

−

R2

(cid:113)
1

(cid:18)

(cid:18)

Rk sin2(bi,bk)
R

+ cos(bi, bk)

1

(cid:113)

(cid:19)

R2

k sin2(bi,bk)
R2

−

→

1, whereas

increases the lower bound in this limiting scenario is dominated by

that as

r
Rmax

Rmax, the value of

R →
< 1 strictly. Thus, as

(cid:18)

Rk sin2(bi,bk)
R

R
+ cos(bi, bk)

(cid:113)

1

min
i,k

−

bound from Proposition 1.

(cid:19)

R2

k sin2(bi,bk)
R2

1

−

, thereby obtaining an improvement in the

10

Proposition 3. The cone Sω(v1) whose axis is a topic direction v1 has mass

ω(v1)) > P(Λc(b1)) =
P(
S
(cid:80)
c
((cid:80)

i(cid:54)=1 αi(1
−
i(cid:54)=1 αi)Γ(α1)Γ((cid:80)

c)α1 Γ((cid:80)K

i=1 αi)
i(cid:54)=1 αi)

1

(cid:80)

−

(1

θ1)
(cid:80)

(cid:82) 1
1−c θα1−1
(cid:82) 1
0 θα1−1
(1
−
c (cid:80)K
(cid:20)
i=1 αi
(cid:80)
i(cid:54)=1 αi + 1

1 +

θ1)

1

i(cid:54)=1 αi−1dθ1
i(cid:54)=1 αi−1dθ1

=

+

c2((cid:80)K
((cid:80)

i=1 αi)((cid:80)K
i(cid:54)=1 αi + 1)((cid:80)

i=1 αi + 1)
i(cid:54)=1 αi + 2)

(cid:21)
,

+

· · ·

(8)

where Λc(b1) is the simplicial cap of
S
the corresponding base of ˜B and cutting adjacent edges of ˜B in the ratio c : (1

ω(v1) which is composed of vertex b1 and a base parallel to

c).

The truncated beta probability calculations in Proposition 3 can be found in Olver et al. (2010).
P(Λcλ(bk)) and let ωλ be such that
Proposition 4. For λ

(0, 1), let cλ be such that λ = min

∈

k

(cid:32)(cid:32)
2

(cid:115)
1

cλ =

(cid:33)

r2
R2

max

−

(sin(d) cot(arccos(1

ωλ)) + cos(d))

,

(9)

−

−

(cid:33)−1

where angle d

min
i,k

≤

∠(bk, bk

−

bi). Then, as long as

(cid:18)

ω

∈

ωλ, max

(cid:18) a2
2R2

max

max

, max
i,k=1,...,K

(1

−

cos(bi, bk)

,

(cid:19)(cid:19)

the bound P(

ω(vk))

S

≥

λ holds for all k = 1, . . . , K.

Proof. Consider Figure 5, with length of AC = ai,kc, where c is the proportion in which the cone
cuts AC, the edge joining vertex i and vertex k. Now, from the sine law of a triangle,

Rk
ai,kc

= sin(bi, bk) cot φi,k + cos(bi, bk)

where φi,k is as deﬁned in the proof of Proposition 2. Now ai,k
φλ = cos ωλ satisﬁes

Rk ≤

2(√R2

max−r2)

Rmax

. The choice of

cλ

≥

(cid:113)
2

1

1

−

r2

R2

max

min
i,k

1
sin(bi, bk) cot φλ + cos(bi, bk)

∠(bi, bk), for all i, k, the function sin(bi, bk) cot φλ +
π
therefore proves the theorem. Since, φλ
2 −
cos(bi, bk) is increasing as the angle between bi and bk increases, as can be checked for maxima by
the ﬁrst derivative rule. Using the cosine law,

≤

(10)

(11)

(12)

(13)

cos(bi, bk) = −

k + a2
i,k

R2

i + R2
2ai,kRk

.

Minimizing this quantity with respect to i and k we get the result.

A.2 Consistency of the Conic Scan-and-Cover algorithm

Under the LDA setup (as presented in Section 2), recall that ai,k is the length of the edge connecting
the ith and kth vertex, i.e.,
, (cid:15)) denote an
−
(cid:15)-ball in (cid:96)2-norm. Then the following result states that with high probability there exists a document
in a neighborhood of every vertex.
Lemma 1. Let pm := (cid:80)
max
k(cid:54)=i

k βkθmk for m = 1, . . . , M as before. Then for any i and any 0 < (cid:15) <

2 is the (cid:96)2 norm. Let B(
·

2 = ai,k, where
(cid:107)

βi
(cid:107)

ai,k,

(cid:107) · (cid:107)

βk

P(pm /
∈

B(βi, (cid:15))

m

∀

∈ {

1, . . . , M

)
}

≤





(cid:82)

0

1−((cid:15)/max
k(cid:54)=i

ai,k)

(cid:82) 1
0 θαi−1

i

(cid:80)

j(cid:54)=i αj −1dθi

(1

θi)

θi)
−
(cid:80)
j(cid:54)=1 αj −1dθi

θαi−1
i
(1

−

M





. (14)

11

Since

(cid:32)

(cid:82)

0

1−((cid:15)/max
k(cid:54)=i

ai,k )

(cid:82) 1
0 θαi−1

i

θαi−1
i
(1−θi)

(1−θi)
(cid:80)

j(cid:54)=1 αj −1dθi

(cid:80)

j(cid:54)=i αj −1dθi

(cid:33)

< 1, for all i because Beta distribution is absolutely

continuous in (0, 1), the bound on the right hand side goes to 0 as M

.
→ ∞

be the topics identiﬁed by Conic Scan-and-Cover algorithm, with labels permuted
being the true topics. Then

}

ˆβ1, . . . , ˆβK
{

Let
according to the minimum matching distance criteria, with
the following result shows the consistency of the identiﬁed topics.
Theorem 1. Suppose
pm := (cid:80)
k βkθmk for m = 1, . . . , M and α
Scan-and-Cover algorithm trained with ω and

β1, . . . , βK
{

}

β1, . . . , βK
{

}

are the true topics, incenter Cp is given, θm ∼ DirK(α) and
be the output of the Conic

∈
R

RK
+ . Let
as in Proposition 2. Then
(cid:33)

ˆβ1, . . . , ˆβ ˆK}
{
(cid:41)

∀

(cid:15) > 0,

min
j∈{1,..., ˆK}(cid:107)

βi

−

ˆβj

(cid:107)

> (cid:15) , for any i

1, . . . , ˆK

∈ {

}

∪ {

K

= ˆK

}

→

0 as M

.
→ ∞

(cid:32)(cid:40)

P

Proof. From the description of the Conic Scan-and-Cover algorithm it sufﬁces to prove that for the
suitable choice of ω,
<
p1, . . . , pm
. But this probability expression is bounded from below by
(cid:15)
1

). The conclusion now follows from Lemma 1.

as in Proposition 2 there holds P(

R
)
→
}
B(βi, (cid:15))

1 as M
m

1, . . . , M

such that

βi
(cid:107)

∈ {

xi

xi

−

∃

}

(cid:107)

i
∈ {
(cid:80)K
i=1

→ ∞
∈ {

∀

}

1, . . . , K
P(pm /
∈

∀
−

A.3 Variance argument for multinomial setup

In the topic modeling problem we are not given pm for m = 1, . . . , M . Under the bag-of-words
assumption we have access to the frequencies of words in documents w1, . . . , wM which provide
a point estimate ¯wm := wm/Nm for the pm. The following proposition establishes a bound on the
variation of ¯wm from pm.
Proposition 5.

E[

¯wm
(cid:107)

−

pm

2
2]
(cid:107)

≤

1

−

(1/V )
Nm

.

(15)

Proof. By iterated expectation identity,

E[
¯wm
(cid:107)

−

pm

(cid:20)
2] = E
2
(cid:107)

E

(cid:20) V

(cid:88)

i=1

¯wmi
(cid:107)

−

pmi

pm

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2
2

(cid:107)

(cid:21)(cid:21)

(cid:20) V

(cid:88)

pmi(1

(cid:21)

pmi)

= E

−
Nm

i=1 p2

mi]

1

=

−

i=1

E[(cid:80)V
Nm

1

−

(1/V )
Nm

.

≤

The second equality follows because conditioned on pm, each wmi
inequality follows from Cauchy-Schwartz Inequality.

∼

Bin(Nm, pmi). The last

B Spherical k-means for topic modeling

We aim to clarify the role of Step 11 of the document Conic Scan-and-Cover algorithm, a geometric
correction technique based on weighted spherical k-means optimization.

B.1 Topic directions as solutions to weighted spherical k-means

Let centered document norms rm :=
2 for m = 1, . . . , M and αk(v) := cos(bk, v), cosine of
(cid:107)
the angle between direction v and k-th topic. The weighted spherical k-means objective takes the
form

˜pm
(cid:107)

min
(cid:107)vk(cid:107)2=1,k=1,...K

rm(1

cos(vk, ˜pm)),

−

(16)

K
(cid:88)

(cid:88)

k=1

m∈Sk(vk)

12

where Sk(vk) :=

m
{

|

cos(vk, ˜pm)) > cos(vl, ˜pm)
K
(cid:88)

rm cos(vk, ˜pm) =

vk, ˜pm
(cid:104)

(cid:105)

=

θmi

vk, bi
(cid:104)

(cid:105)

=

i=1

K
(cid:88)

i=1

l
∀

= k)

. Next observe that:
}

θmiRiαi(vk),

(17)

so our objective 16 becomes:

max
(cid:107)vk(cid:107)2=1,k=1,...K

K
(cid:88)

(cid:88)

K
(cid:88)

k=1

m∈Sk(vk)

i=1

θmiRiαi(vk).

(18)

= i, which implies that topic simplex
Now, if R1 = . . . = RK and αi(bk) = αi(bl)
Sk(bk) iff
is equilateral, we see that cluster boundaries of topic directions are given by m
= k. Observe that the corresponding partition is deﬁned by the geometric medians
θmk > θml
of topic simplex, which in turn partitions it into equal volume parts. Then, assuming that the topic
simplex B is symmetric, combined with the symmetricity of the Dirichlet distribution of θm-s, it
follows that bk is the centroid of Sk(bk) for k = 1, . . . , K.

k, l

∈

∀

∀

l

B.2 Role of the spherical k-means in CoSAC algorithm for documents

The result of Section B.1 shows that weighted spherical k-means with Lloyd type updates (Lloyd,
1982) will converge to the directions of the true topics if it is initialized in their respective neigh-
borhoods and equilaterality of B and symmetricity of Dirichlet for document topic proportions is
satisﬁed.

Recall that goal of the Conic Scan-and-Cover is to ﬁnd the number of topics and their directions, while
Mean Shifting was used to address the noise in the data. We proceed to compare weighted spherical
k-means by itself (with 500 iterations, which makes it slower than CoSAC) versus document Conic
Scan-and-Cover with only Mean Shifting and the full document Conic Scan-and-Cover algorithm
to see the effect of the spherical k-means post-processing step. Results in Fig. 6 are for the same
[25, 300] but corpora is large
scenarios as in Section 5 – that is when either documents are short Nm
M = 30000 or when documents are longer Nm = 500 and corpora is smaller M
[100, 10000].
We see that spherical k-means by itself does not succeed, whereas when used as a postprocessing step
for CoSAC it allows for a slight improvement when documents are short. This is because it operates
on the full data partition when taking averages for direction estimation, while Mean Shifting only
ω(v). Using more data is important for noise reduction
has access to the data in its respective cone
when documents are short as suggested by our analysis.

∈

∈

S

Figure 6: Minimum matching Euclidean distance for (a) varying corpora size, (b) varying length of
documents. Perplexity for (c) varying corpora sizes, (d) varying length of documents.

C Additional experiments

C.1 Perplexity comparison

In this section we present perplexity scores comparison for the experiments of Section 5. For
simulation experiments we used V = 2000, symmetric α = η = 0.1. To compute held-out perplexity
for the CoSAC we employed projection based estimates for topic proportions θm from Yurochkin &
Nguyen (2016), which led to a slightly worse perplexity scores for CoSAC and GDM in comparison
to Gibbs sampler. However, CoSAC (except for Nm = 25, when it slightly underestimates K) shows
competitive performance without requiring K as an input. We note that as before cscRecoverKL
outperforms RecoverKL in all cases.

13

C.2 Varying vocabulary size V

Our next experiment investigates the inﬂuence of vocabulary size V . We set Nm = 500, M = 5000,
K = 15, symmetric α = η = 0.1 and varied V from 2000 to 15000. We discovered that ω = 0.6 is
too small for V > 10000, meaning that CoSAC algorithm does not ﬁnd enough documents in the
corresponding cones and keeps discarding without recording topics (per Step 9 of Algorithm 2). This
can be explained by the fact that vectors tending to be far apart in high dimensions and relatively (to
V > 10000) small values of corpora size M and document lengths Nm. On the other hand, setting
ω = 0.75 worked well for all values of V in this experiment. Results are reported in Fig. 7(c), (d)
and Fig. 8(d). Document CoSAC with ω = 0.75 recovered true K = 15 for all values of V and
showed better recovery than GDM and Gibbs sampler in terms of minimum matching distance, while
Gibbs sampler had slightly better perplexity for higher values of V . It is worth reminding that unlike
CoSAC, both GDM and the Gibbs sampling based method requires the number of topics K be given.

C.3

Impact of α

Recall that, per the LDA model, topic proportions θ ∼ DirK(α). Cases with α > 1 were previously
shown (Nguyen, 2015) to exhibit slower convergence rates of the LDA’s posterior estimation (via
Gibbs sampler, for instance). Geometrically, large α implies that documents are more likely concen-
trated near the center of the topic simplex, leaving fewer documents near the vertices; this entails that
geometric inference is more challenging. In our choices for parameters ω,
, λ we relied on small
values of α as a more practical scenario. Speciﬁcally, we considered ω = 0.8 for this experiment
to achieve full coverage of the topic simplex. In our previous experiments we set α = 0.1. Now,
we consider a larger range, α
[0.01, 1.5], to gauge its impact more fully. Results are reported in
Fig. 8(a), (b) and (c). For smaller values of α CoSAC is demonstrated to be the best algorithm of all
under consideration. As α increases, CoSAC can still recover correct K with high accuracy, although
the quality of topic estimates deteriorates faster than for Gibbs sampler and GDM. We think that
further work on estimation procedures for topic radii Rks (recall that topics are estimated as direction
and length along this direction bk = Rkvk) might address this issue. In this work we considered
maximum projection (Step 13 of Algorithm 2) to estimate Rks, which might not be as accurate when
documents are mostly near the center of the topic simplex (i.e., for higher α).

R

∈

Figure 7: Perplexity for (a) varying corpora size, (b) varying length of documents, (c) varying
vocabulary size; (d) Minimum matching Euclidean distance for varying vocabulary size.

Figure 8: Varying α (a) Minimum matching Euclidean distance, (b) Perplexity, (c) Estimation of
number of topics; (d) Estimation of number of topics for varying vocabulary size.

14

D Implementation details

In this section we give details about the implementations of the algorithms used in simulation studies
and real data. We implemented Conic Scan-and-Cover (CoSAC) algorithm in Python with the
help of Scipy (Jones et al., 2001–) sparse matrix modules. Geometric Dirichlet Means (GDM)
(Yurochkin & Nguyen, 2016) was implemented with the help of Scikit-learn (Pedregosa et al.,
2011) k-means implementation (with 10 restarts to avoid local minima of k-means) combined
with a geometric correction technique. Codes for CoSAC and GDM are available at https:
//github.com/moonfolk/Geometric-Topic-Modeling. For RecoverKL (Arora et al.,
2012) we applied code from one of the coauthors. To implement cscRecoverKL we used our CoSAC
implementation (Algorithm 1 with outlier threshold λ as in Algorithm 2) to ﬁnd anchor words and
then recovery routine from the aforementioned code. For the Gibbs sampling (Grifﬁths & Steyvers,
2004) we used an lda package in Python that utilizes Cython to achieve C speed. Gibbs sampler
was trained with α = 0.1, η = 0.01 and 500 iterations in simulations studies and α = 0.1, η = 0.1
and 1000 iterations in the NYTimes articles2 analysis. For the SVI (Hoffman et al., 2013) we used
Gensim implementation ( ˇReh˚uˇrek & Sojka, 2010) with automatic hyperparameters estimation, 50
iterations and 10 passes. Finally for HDP (Teh et al., 2006) we used C++ implementation with
default hyperparameter settings and 100 iterations. For all experiments (except large vocabulary
sizes and bigger α), per discussions in Sections 3.2 and 4, parameters of the CoSAC were set to
ω = 0.6, n = 0.001M and
as median of the centered and normalized document norms. Spherical
R
k-means post-processing step was run for 30 iterations. For cscRecoverKL we set ω = 0.4, λ = 0.015
(λ = 0.005 for real data) and
as corresponding median of the norms. Note that cscRecoverKL
takes word-to-word co-occurrence matrix as input, therefore sample size is V and "documents" are
rows of this matrix. Exploring distributional properties of the simplex spanned by the anchor words
is outside of the scope of this work, therefore parameter choices were made empirically based on the
visual analysis illustrated by Fig. 2. All simulated results are reported after 20 repetitions of the data
generation for each scenario and NYTimes results for LDA and HDP are reported over 10 reﬁts of
the corresponding Gibbs samplers.

R

2https://archive.ics.uci.edu/ml/datasets/bag+of+words

15

References

Allocation. NIPS, 2012.

Anandkumar, A., Foster, D. P., Hsu, D., Kakade, S. M., and Liu, Y. A spectral algorithm for Latent Dirichlet

Arora, S., Ge, R., Halpern, Y., Mimno, D., Moitra, A., Sontag, D., Wu, Y., and Zhu, M. A practical algorithm for

topic modeling with provable guarantees. arXiv preprint arXiv:1212.4777, 2012.

Blei, D. M., Ng, A. Y., and Jordan, M. I. Latent Dirichlet Allocation. J. Mach. Learn. Res., 3:993–1022, March

2003.

Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. Indexing by latent semantic

analysis. Journal of the American Society for Information Science, 41(6):391, Sep 01 1990.

Grifﬁths, Thomas L and Steyvers, Mark. Finding scientiﬁc topics. PNAS, 101(suppl. 1):5228–5235, 2004.

Hoffman, Ma. D., Blei, D. M., Wang, C., and Paisley, J. Stochastic variational inference. J. Mach. Learn. Res.,

14(1):1303–1347, May 2013.

Hsu, Wei-Shou and Poupart, Pascal. Online bayesian moment matching for topic modeling with unknown

number of topics. In Advances In Neural Information Processing Systems, pp. 4529–4537, 2016.

Jones, Eric, Oliphant, Travis, Peterson, Pearu, et al. SciPy: Open source scientiﬁc tools for Python, 2001–. URL

http://www.scipy.org/.

Lloyd, S. Least squares quantization in PCM. Information Theory, IEEE Transactions on, 28(2):129–137, Mar

Nguyen, XuanLong. Posterior contraction of the population polytope in ﬁnite admixture models. Bernoulli, 21

1982.

(1):618–646, 02 2015.

Olver, Frank W. J., Lozier, Daniel M., Boisvert, Ronald F., and Clark, Charles W. NIST handbook of mathematical

functions, cambridge university press, 2010. URL http://dlmf.nist.gov/8.17.

Pedregosa, Fabian, Varoquaux, Gaël, Gramfort, Alexandre, Michel, Vincent, Thirion, Bertrand, Grisel, Olivier,
Blondel, Mathieu, Prettenhofer, Peter, Weiss, Ron, Dubourg, Vincent, et al. Scikit-learn: Machine learning in
python. Journal of Machine Learning Research, 12(Oct):2825–2830, 2011.

Pritchard, Jonathan K, Stephens, Matthew, and Donnelly, Peter. Inference of population structure using multilocus

genotype data. Genetics, 155(2):945–959, 2000.

ˇReh˚uˇrek, Radim and Sojka, Petr. Software Framework for Topic Modelling with Large Corpora. In Proceedings
of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pp. 45–50, Valletta, Malta, May 2010.
ELRA. http://is.muni.cz/publication/884893/en.

Tang, Jian, Meng, Zhaoshi, Nguyen, Xuanlong, Mei, Qiaozhu, and Zhang, Ming. Understanding the limiting
In Proceedings of The 31st International

factors of topic modeling via posterior contraction analysis.
Conference on Machine Learning, pp. 190–198. ACM, 2014.

Teh, Y. W., Jordan, M. I., Beal, M. J., and Blei, D. M. Hierarchical dirichlet processes. Journal of the american

statistical association, 101(476), 2006.

Xu, Wei, Liu, Xin, and Gong, Yihong. Document clustering based on non-negative matrix factorization. In
Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in
Informaion Retrieval, SIGIR ’03, pp. 267–273. ACM, 2003.

Yurochkin, Mikhail and Nguyen, XuanLong. Geometric dirichlet means algorithm for topic inference. In

Advances in Neural Information Processing Systems, pp. 2505–2513, 2016.

16

