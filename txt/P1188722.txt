8
1
0
2
 
n
a
J
 
2
1
 
 
]

G
L
.
s
c
[
 
 
6
v
0
0
5
8
0
.
6
0
7
1
:
v
i
X
r
a

GANs Trained by a Two Time-Scale Update Rule
Converge to a Local Nash Equilibrium

Martin Heusel

Hubert Ramsauer

Thomas Unterthiner

Bernhard Nessler

Sepp Hochreiter

LIT AI Lab & Institute of Bioinformatics,
Johannes Kepler University Linz
A-4040 Linz, Austria
{mhe,ramsauer,unterthiner,nessler,hochreit}@bioinf.jku.at

Abstract

Generative Adversarial Networks (GANs) excel at creating realistic images with
complex models for which maximum likelihood is infeasible. However, the con-
vergence of GAN training has still not been proved. We propose a two time-scale
update rule (TTUR) for training GANs with stochastic gradient descent on ar-
bitrary GAN loss functions. TTUR has an individual learning rate for both the
discriminator and the generator. Using the theory of stochastic approximation, we
prove that the TTUR converges under mild assumptions to a stationary local Nash
equilibrium. The convergence carries over to the popular Adam optimization, for
which we prove that it follows the dynamics of a heavy ball with friction and thus
prefers ﬂat minima in the objective landscape. For the evaluation of the perfor-
mance of GANs at image generation, we introduce the ‘Fréchet Inception Distance”
(FID) which captures the similarity of generated images to real ones better than
the Inception Score. In experiments, TTUR improves learning for DCGANs and
Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN train-
ing on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word
Benchmark.

Introduction

Generative adversarial networks (GANs) [18] have achieved outstanding results in generating realistic
images [51, 36, 27, 3, 6] and producing text [23]. GANs can learn complex generative models for
which maximum likelihood or a variational approximations are infeasible. Instead of the likelihood,
a discriminator network serves as objective for the generative model, that is, the generator. GAN
learning is a game between the generator, which constructs synthetic data from random variables,
and the discriminator, which separates synthetic data from real world data. The generator’s goal is
to construct data in such a way that the discriminator cannot tell them apart from real world data.
Thus, the discriminator tries to minimize the synthetic-real discrimination error while the generator
tries to maximize this error. Since training GANs is a game and its solution is a Nash equilibrium,
gradient descent may fail to converge [53, 18, 20]. Only local Nash equilibria are found, because
gradient descent is a local optimization method. If there exists a local neighborhood around a point
in parameter space where neither the generator nor the discriminator can unilaterally decrease their
respective losses, then we call this point a local Nash equilibrium.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

Figure 1: Left: Original vs. TTUR GAN training on CelebA. Right: Figure from Zhang 2007 [61]
which shows the distance of the parameter from the optimum for a one time-scale update of a 4 node
network ﬂow problem. When the upper bounds on the errors (α, β) are small, the iterates oscillate
and repeatedly return to a neighborhood of the optimal solution (see also Appendix Section A2.3).
However, when the upper bounds on the errors are large, the iterates typically diverge.

To characterize the convergence properties of training general GANs is still an open challenge [19, 20].
For special GAN variants, convergence can be proved under certain assumptions [39, 22, 56]. A
prerequisit for many convergence proofs is local stability [35] which was shown for GANs by
Nagarajan and Kolter [46] for a min-max GAN setting. However, Nagarajan and Kolter require for
their proof either rather strong and unrealistic assumptions or a restriction to a linear discriminator.
Recent convergence proofs for GANs hold for expectations over training samples or for the number
of examples going to inﬁnity [37, 45, 40, 4], thus do not consider mini-batch learning which leads to
a stochastic gradient [57, 25, 42, 38].

Recently actor-critic learning has been analyzed using stochastic approximation. Prasad et al. [50]
showed that a two time-scale update rule ensures that training reaches a stationary local Nash
equilibrium if the critic learns faster than the actor. Convergence was proved via an ordinary
differential equation (ODE), whose stable limit points coincide with stationary local Nash equilibria.
We follow the same approach. We prove that GANs converge to a local Nash equilibrium when trained
by a two time-scale update rule (TTUR), i.e., when discriminator and generator have separate learning
rates. This also leads to better results in experiments. The main premise is that the discriminator
converges to a local minimum when the generator is ﬁxed. If the generator changes slowly enough,
then the discriminator still converges, since the generator perturbations are small. Besides ensuring
convergence, the performance may also improve since the discriminator must ﬁrst learn new patterns
before they are transferred to the generator. In contrast, a generator which is overly fast, drives the
discriminator steadily into new regions without capturing its gathered information. In recent GAN
implementations, the discriminator often learned faster than the generator. A new objective slowed
down the generator to prevent it from overtraining on the current discriminator [53]. The Wasserstein
GAN algorithm uses more update steps for the discriminator than for the generator [3]. We compare
TTUR and standard GAN training. Fig. 1 shows at the left panel a stochastic gradient example on
CelebA for original GAN training (orig), which often leads to oscillations, and the TTUR. On the
right panel an example of a 4 node network ﬂow problem of Zhang et al. [61] is shown. The distance
between the actual parameter and its optimum for an one time-scale update rule is shown across
iterates. When the upper bounds on the errors are small, the iterates return to a neighborhood of the
optimal solution, while for large errors the iterates may diverge (see also Appendix Section A2.3).

Our novel contributions in this paper are:

The two time-scale update rule for GANs,

•

•

•

•

•

We proof that GANs trained with TTUR converge to a stationary local Nash equilibrium,

The description of Adam as heavy ball with friction and the resulting second order differential
equation,

The convergence of GANs trained with TTUR and Adam to a stationary local Nash equilib-
rium,

We introduce the “Fréchet Inception Distance” (FID) to evaluate GANs, which is more
consistent than the Inception Score.

2

Two Time-Scale Update Rule for GANs

LG. The loss functions

We consider a discriminator D(.; w) with parameter vector w and a generator G(.; θ) with parameter
vector θ. Learning is based on a stochastic gradient ˜g(θ, w) of the discriminator’s loss function
LD
and a stochastic gradient ˜h(θ, w) of the generator’s loss function
LD and
LG can be the original as introduced in Goodfellow et al. [18], its improved versions [20], or recently
proposed losses for GANs like the Wasserstein GAN [3]. Our setting is not restricted to min-max
GANs, but is valid for all other, more general GANs for which the discriminator’s loss function
LD
θ, w
is not necessarily related to the generator’s loss function
are stochastic, since they use mini-batches of m real world samples x(i), 1 (cid:54) i (cid:54) m and m synthetic
(cid:0)
(cid:1)
samples z(i), 1 (cid:54) i (cid:54) m which are randomly chosen. If the true gradients are g(θ, w) =
∇wLD and
∇θLG, then we can deﬁne ˜g(θ, w) = g(θ, w) +M (w) and ˜h(θ, w) = h(θ, w) +M (θ)
h(θ, w) =
with random variables M (w) and M (θ). Thus, the gradients ˜g
are stochastic
approximations to the true gradients. Consequently, we analyze convergence of GANs by two
time-scale stochastic approximations algorithms. For a two time-scale update rule (TTUR), we use
the learning rates b(n) and a(n) for the discriminator and the generator update, respectively:

LG. The gradients ˜g

and ˜h

and ˜h

θ, w

θ, w

θ, w

(cid:0)

(cid:1)

(cid:1)

(cid:0)

(cid:0)

(cid:1)

wn+1 = wn + b(n)

g

θn, wn

+ M (w)

n

, θn+1 = θn + a(n)

h

θn, wn

+ M (θ)

n

.

(1)

For more details on the following convergence proof and its assumptions see Appendix Section A2.1.
To prove convergence of GANs learned by TTUR, we make the following assumptions (The actual
assumption is ended by (cid:74), the following text are just comments and explanations):

(cid:16)

(cid:0)

(cid:1)

(cid:17)

(cid:16)

(cid:0)

(cid:1)

(cid:17)

(A1) The gradients h and g are Lipschitz. (cid:74) Consequently, networks with Lipschitz smooth
activation functions like ELUs (α = 1) [13] fulﬁll the assumption but not ReLU networks.

,

∞

∞

(A2)

n b(n) =

n a(n) =

(cid:80)
w.r.t.

n a2(n) <
(A3) The stochastic gradient errors
(cid:80)
the increasing σ-ﬁeld
(θ)
n
| F

n b2(n) <
are martingale difference sequences
, l (cid:54) n), n (cid:62) 0 with
(cid:54) B2, where B1 and B2 are positive
2
E
deterministic constants.(cid:74) The original Assumption (A3) from Borkar 1997 follows from
Lemma 2 in [7] (see also [52]). The assumption is fulﬁlled in the Robbins-Monro setting,
where mini-batches are randomly sampled and the gradients are bounded.

,
M (θ)
(cid:80)
n
{
Fn = σ(θl, wl, M (θ)

,
∞
M (w)
(cid:80)
n
}
{

(cid:54) B1 and E

, a(n) = o(b(n))(cid:74)

M (θ)
n
(cid:107)

(w)
n
| F

, M (w)
l

M (w)
n

(cid:107)
(cid:104)

2
(cid:107)

and

∞

}

(cid:107)

(cid:105)

(cid:104)

(cid:105)

l

(A4) For each θ, the ODE ˙w(t) = g

θ, w(t)

(cid:1)

(cid:0)

(cid:0)

θ(t), λ(θ(t))

has a local asymptotically stable attractor
λ(θ) within a domain of attraction Gθ such that λ is Lipschitz. The ODE ˙θ(t) =
(cid:1)
h
has a local asymptotically stable attractor θ∗ within a domain of
attraction.(cid:74) The discriminator must converge to a minimum for ﬁxed generator param-
eters and the generator, in turn, must converge to a minimum for this ﬁxed discriminator
minimum. Borkar 1997 required unique global asymptotically stable equilibria [9]. The
assumption of global attractors was relaxed to local attractors via Assumption (A6) and
Theorem 2.7 in Karmakar & Bhatnagar [28]. See for more details Assumption (A6) in the
Appendix Section A2.1.3. Here, the GAN objectives may serve as Lyapunov functions.
These assumptions of locally stable ODEs can be ensured by an additional weight decay term
in the loss function which increases the eigenvalues of the Hessian. Therefore, problems
with a region-wise constant discriminator that has zero second order derivatives are avoided.
For further discussion see Appendix Section A2 (C3).

(A5) supn (cid:107)

θn(cid:107)
decay term.

<

and supn (cid:107)

wn(cid:107)

<

∞

∞

.(cid:74) Typically ensured by the objective or a weight

The next theorem has been proved in the seminal paper of Borkar 1997 [9].
Theorem 1 (Borkar). If the assumptions are satisﬁed, then the updates Eq. (1) converge to
(θ∗, λ(θ∗)) a.s.

The solution (θ∗, λ(θ∗)) is a stationary local Nash equilibrium [50], since θ∗ as well as λ(θ∗) are
θ∗, λ(θ∗)
local asymptotically stable attractors with g
= 0. An alternative
approach to the proof of convergence using the Poisson equation for ensuring a solution to the fast

= 0 and h

θ∗, λ(θ∗)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

3

update rule can be found in the Appendix Section A2.1.2. This approach assumes a linear update
function in the fast update rule which, however, can be a linear approximation to a nonlinear gradient
[30, 32]. For the rate of convergence see Appendix Section A2.2, where Section A2.2.1 focuses on
linear and Section A2.2.2 on non-linear updates. For equal time-scales it can only be proven that
the updates revisit an environment of the solution inﬁnitely often, which, however, can be very large
[61, 14]. For more details on the analysis of equal time-scales see Appendix Section A2.3. The main
idea of the proof of Borkar [9] is to use (T, δ) perturbed ODEs according to Hirsch 1989 [24] (see
also Appendix Section C of Bhatnagar, Prasad, & Prashanth 2013 [8]). The proof relies on the fact
that there eventually is a time point when the perturbation of the slow update rule is small enough
(given by δ) to allow the fast update rule to converge. For experiments with TTUR, we aim at ﬁnding
learning rates such that the slow update is small enough to allow the fast to converge. Typically,
the slow update is the generator and the fast update the discriminator. We have to adjust the two
learning rates such that the generator does not affect discriminator learning in a undesired way and
perturb it too much. However, even a larger learning rate for the generator than for the discriminator
may ensure that the discriminator has low perturbations. Learning rates cannot be translated directly
into perturbation since the perturbation of the discriminator by the generator is different from the
perturbation of the generator by the discriminator.

Adam Follows an HBF ODE and Ensures TTUR Convergence

In our experiments, we aim at using Adam stochastic approximation to avoid mode collapsing. GANs
suffer from “mode collapsing” where large masses of probability are mapped onto a few modes
that cover only small regions. While these regions represent meaningful samples, the variety of the
real world data is lost and only few prototype samples are
generated. Different methods have been proposed to avoid
mode collapsing [11, 43]. We obviate mode collapsing by
using Adam stochastic approximation [29]. Adam can be
described as Heavy Ball with Friction (HBF) (see below),
since it averages over past gradients. This averaging cor-
responds to a velocity that makes the generator resistant
to getting pushed into small regions. Adam as an HBF
method typically overshoots small local minima that cor-
respond to mode collapse and can ﬁnd ﬂat minima which
generalize well [26]. Fig. 2 depicts the dynamics of HBF,
where the ball settles at a ﬂat minimum. Next, we analyze
whether GANs trained with TTUR converge when using
Adam. For more details see Appendix Section A3.

Figure 2: Heavy Ball with Friction, where the
ball with mass overshoots the local minimum
θ+ and settles at the ﬂat minimum θ∗.

We recapitulate the Adam update rule at step n, with learning rate a, exponential averaging factors β1
for the ﬁrst and β2 for the second moment of the gradient

f (θn

1):

∇

−

(2)

gn ←− ∇
mn ←−
vn ←−
θn ←−

f (θn
(β1/(1
(β2/(1
θn

−

1)
βn
1 )) mn
1 + ((1
−
βn
2 )) vn
1 + ((1
−
a mn/(√vn + (cid:15)) ,

β1)/(1
−
β2)/(1
−

βn
1 )) gn
−
βn
2 )) gn (cid:12)
−

gn

τ for τ

−
−
1 −
−
, the square root √., and the
where following operations are meant componentwise: the product
division / in the last line. Instead of learning rate a, we introduce the damping coefﬁcient a(n) with
(0, 1]. Adam has parameters β1 for averaging the gradient and β2 parametrized
a(n) = an−
by a positive α for averaging the squared gradient. These parameters can be considered as deﬁning a
memory for Adam. To characterize β1 and β2 in the following, we deﬁne the exponential memory
n
r(n) = r and the polynomial memory r(n) = r/
l=1 a(l) for some positive constant r. The next
theorem describes Adam by a differential equation, which in turn allows to apply the idea of (T, δ)
(cid:80)
perturbed ODEs to TTUR. Consequently, learning GANs with TTUR and Adam converges.
Theorem 2. If Adam is used with β1 = 1
f
as the full gradient of the lower bounded, continuously differentiable objective f , then for stationary
second moments of the gradient, Adam follows the differential equation for Heavy Ball with Friction

αa(n + 1)r(n) and with

a(n + 1)r(n), β2 = 1

∇

(cid:12)

−

−

∈

4

(HBF):

Adam converges for gradients

∇

¨θt + a(t) ˙θt +
f that are L-Lipschitz.

∇

f (θt) = 0 .

(3)

Proof. Gadat et al. derived a discrete and stochastic version of Polyak’s Heavy Ball method [49], the
Heavy Ball with Friction (HBF) [17]:
θn+1 = θn −
mn+1 =
−

a(n + 1) mn ,
a(n + 1) r(n)

mn + a(n + 1) r(n)

f (θn) + Mn+1

(4)

∇

1

.

(cid:0)

(cid:1)

(cid:0)

These update rules are the ﬁrst moment update rules of Adam [29]. The HBF can be formulated as the
differential equation Eq. (3) [17]. Gadat et al. showed that the update rules Eq. (4) converge for loss
functions f with at most quadratic grow and stated that convergence can be proofed for
f that are
∇
L-Lipschitz [17]. Convergence has been proved for continuously differentiable f that is quasiconvex
f that is L-Lipschitz
(Theorem 3 in Goudou & Munier [21]). Convergence has been proved for
and bounded from below (Theorem 3.1 in Attouch et al. [5]). Adam normalizes the average mn by
gn]. mn is componentwise divided by
the second moments vn of of the gradient gn: vn = E [gn (cid:12)
the square root of the components of vn. We assume that the second moments of gn are stationary,
gn]. In this case the normalization can be considered as additional noise since the
i.e., v = E [gn (cid:12)
normalization factor randomly deviates from its mean. In the HBF interpretation the normalization
by √v corresponds to introducing gravitation. We obtain

∇

(cid:1)

n

1
1

β2
βn
2

n

1
1

β2
βn
2

l

βn
2

l

βn
2

−

−

−

(5)

v) .

(cid:88)l=1

(cid:88)l=1

−
−

v =

vn =

gl (cid:12)

gl −

(gl (cid:12)

gl , ∆vn = vn −

−
−
For a stationary second moment v and β2 = 1
αa(n + 1)r(n), we have ∆vn ∝
a(n + 1)r(n). We
use a componentwise linear approximation to Adam’s second moment normalization 1/√v + ∆vn ≈
∆vn + O(∆2vn), where all operations are meant componentwise. If
1/√v
−
we set M (v)
mn/√v + a(n +
∆vn)/(2v
1)r(n)M (v)
v] = 0. For a stationary second moment v,

(cid:12)
= 0, since E [gl (cid:12)
is a martingale difference sequence with a bounded second moment.
the random variable
in update rules Eq. (4). The factor 1/√v can
Mn+1}
Therefore
n+1}
{
be componentwise incorporated into the gradient g which corresponds to rescaling the parameters
without changing the minimum.

√v))
(cid:12)
(mn (cid:12)
M (v)
n+1
M (v)
(cid:104)
n
{
can be subsumed into

√va(n + 1)r(n)), then mn/√vn ≈

(1/(2v
(cid:12)
n+1 =
−
n+1 and E

M (v)
{

gl −

(cid:105)
}

2+f (θ(t))
According to Attouch et al. [5] the energy, that is, a Lyapunov function, is E(t) = 1/2
|
and ˙E(t) =
2 < 0. Since Adam can be expressed as differential equation and has a
|
Lyapunov function, the idea of (T, δ) perturbed ODEs [9, 24, 10] carries over to Adam. Therefore
the convergence of Adam with TTUR can be proved via two time-scale stochastic approximation
analysis like in Borkar [9] for stationary second moments of the gradient.

˙θ(t)
|

−

a

˙θ(t)
|

In the Appendix we further discuss the convergence of two time-scale stochastic approximation
algorithms with additive noise, linear update functions depending on Markov chains, nonlinear
update functions, and updates depending on controlled Markov processes. Futhermore, the Appendix
presents work on the rate of convergence for both linear and nonlinear update rules using similar
techniques as the local stability analysis of Nagarajan and Kolter [46]. Finally, we elaborate more on
equal time-scale updates, which are investigated for saddle point problems and actor-critic learning.

Experiments

Performance Measure. Before presenting the experiments, we introduce a quality measure for
models learned by GANs. The objective of generative learning is that the model produces data which
matches the observed data. Therefore, each distance between the probability of observing real world
data pw(.) and the probability of generating model data p(.) can serve as performance measure for
generative models. However, deﬁning appropriate performance measures for generative models

5

Figure 3: FID is evaluated for upper left: Gaussian noise, upper middle: Gaussian blur, upper
right: implanted black rectangles, lower left: swirled images, lower middle: salt and pepper noise,
and lower right: CelebA dataset contaminated by ImageNet images. The disturbance level rises
from zero and increases to the highest level. The FID captures the disturbance level very well by
monotonically increasing.

is difﬁcult [55]. The best known measure is the likelihood, which can be estimated by annealed
importance sampling [59]. However, the likelihood heavily depends on the noise assumptions for
the real data and can be dominated by single samples [55]. Other approaches like density estimates
have drawbacks, too [55]. A well-performing approach to measure the performance of GANs is the
“Inception Score” which correlates with human judgment [53]. Generated samples are fed into an
inception model that was trained on ImageNet. Images with meaningful objects are supposed to
have low label (output) entropy, that is, they belong to few object classes. On the other hand, the
entropy across images should be high, that is, the variance over the images should be large. Drawback
of the Inception Score is that the statistics of real world samples are not used and compared to the
statistics of synthetic samples. Next, we improve the Inception Score. The equality p(.) = pw(.)
pw(.)f (x)dx for a basis f (.)
holds except for a non-measurable set if and only if
spanning the function space in which p(.) and pw(.) live. These equalities of expectations are used
to describe distributions by moments or cumulants, where f (x) are polynomials of the data x. We
generalize these polynomials by replacing x by the coding layer of an inception model in order to
obtain vision-relevant features. For practical reasons we only consider the ﬁrst two polynomials, that
is, the ﬁrst two moments: mean and covariance. The Gaussian is the maximum entropy distribution
for given mean and covariance, therefore we assume the coding units to follow a multidimensional
Gaussian. The difference of two Gaussians (synthetic and real-world images) is measured by the
Fréchet distance [16] also known as Wasserstein-2 distance [58]. We call the Fréchet distance d(., .)
between the Gaussian with mean (m, C) obtained from p(.) and the Gaussian with mean (mw, Cw)
obtained from pw(.) the “Fréchet Inception Distance” (FID), which is given by [15]:

p(.)f (x)dx =

(cid:82)

(cid:82)

m
(cid:107)

2
2 + Tr

d2((m, C), (mw, Cw)) =

mw(cid:107)
Next we show that the FID is consistent with increasing disturbances and human judgment. Fig. 3
evaluates the FID for Gaussian noise, Gaussian blur, implanted black rectangles, swirled images,
salt and pepper noise, and CelebA dataset contaminated by ImageNet images. The FID captures the
disturbance level very well. In the experiments we used the FID to evaluate the performance of GANs.
For more details and a comparison between FID and Inception Score see Appendix Section A1,
where we show that FID is more consistent with the noise level than the Inception Score.

C + Cw −

CCw

(6)

−

2

(cid:0)

(cid:0)

(cid:1)

(cid:1)

.

1/2

Model Selection and Evaluation. We compare the two time-scale update rule (TTUR) for GANs
with the original GAN training to see whether TTUR improves the convergence speed and per-
formance of GANs. We have selected Adam stochastic optimization to reduce the risk of mode
collapsing. The advantage of Adam has been conﬁrmed by MNIST experiments, where Adam indeed

6

considerably reduced the cases for which we observed mode collapsing. Although TTUR ensures
that the discriminator converges during learning, practicable learning rates must be found for each
experiment. We face a trade-off since the learning rates should be small enough (e.g. for the generator)
to ensure convergence but at the same time should be large enough to allow fast learning. For each of
the experiments, the learning rates have been optimized to be large while still ensuring stable training
which is indicated by a decreasing FID or Jensen-Shannon-divergence (JSD). We further ﬁxed the
time point for stopping training to the update step when the FID or Jensen-Shannon-divergence of
the best models was no longer decreasing. For some models, we observed that the FID diverges
or starts to increase at a certain time point. An example of this behaviour is shown in Fig. 5. The
performance of generative models is evaluated via the Fréchet Inception Distance (FID) introduced
above. For the One Billion Word experiment, the normalized JSD served as performance measure.
For computing the FID, we propagated all images from the training dataset through the pretrained
Inception-v3 model following the computation of the Inception Score [53], however, we use the last
pooling layer as coding layer. For this coding layer, we calculated the mean mw and the covariance
matrix Cw. Thus, we approximate the ﬁrst and second central moment of the function given by
the Inception coding layer under the real world distribution. To approximate these moments for the
model distribution, we generate 50,000 images, propagate them through the Inception-v3 model, and
then compute the mean m and the covariance matrix C. For computational efﬁciency, we evaluate
the FID every 1,000 DCGAN mini-batch updates, every 5,000 WGAN-GP outer iterations for the
image experiments, and every 100 outer iterations for the WGAN-GP language model. For the one
time-scale updates a WGAN-GP outer iteration for the image model consists of ﬁve discriminator
mini-batches and ten discriminator mini-batches for the language model, where we follow the original
implementation. For TTUR however, the discriminator is updated only once per iteration. We repeat
the training for each single time-scale (orig) and TTUR learning rate eight times for the image
datasets and ten times for the language benchmark. Additionally to the mean FID training progress
we show the minimum and maximum FID over all runs at each evaluation time-step. For more details,
implementations and further results see Appendix Section A4 and A6.

Simple Toy Data. We ﬁrst want to demonstrate the difference between a single time-scale update
rule and TTUR on a simple toy min/max problem where a saddle point should be found. The
y2) in Fig. 4 (left) has a saddle point at (x, y) = (0, 0) and
objective f (x, y) = (1 + x2)(100
measures the distance of the parameter vector (x, y) to
fulﬁlls assumption A4. The norm
the saddle point. We update (x, y) by gradient descent in x and gradient ascent in y using additive
Gaussian noise in order to simulate a stochastic update. The updates should converge to the saddle
point (x, y) = (0, 0) with objective value f (0, 0) = 100 and the norm 0. In Fig. 4 (right), the ﬁrst
two rows show one time-scale update rules. The large learning rate in the ﬁrst row diverges and has
large ﬂuctuations. The smaller learning rate in the second row converges but slower than the TTUR in
the third row which has slow x-updates. TTUR with slow y-updates in the fourth row also converges
but slower.

−
(x, y)
(cid:107)
(cid:107)

Figure 4: Left: Plot of the objective with a saddle point at (0, 0). Right: Training progress with
equal learning rates of 0.01 (ﬁrst row) and 0.001 (second row)) for x and y, TTUR with a learning
rate of 0.0001 for x vs. 0.01 for y (third row) and a larger learning rate of 0.01 for x vs. 0.0001 for y
(fourth row). The columns show the function values (left), norms (middle), and (x, y) (right). TTUR
(third row) clearly converges faster than with equal time-scale updates and directly moves to the
saddle point as shown by the norm and in the (x, y)-plot.

DCGAN on Image Data. We test TTUR for the deep convolutional GAN (DCGAN) [51] at the
CelebA, CIFAR-10, SVHN and LSUN Bedrooms dataset. Fig. 5 shows the FID during learning

7

Figure 5: Mean FID (solid line) surrounded by a shaded area bounded by the maximum and the
minimum over 8 runs for DCGAN on CelebA, CIFAR-10, SVHN, and LSUN Bedrooms. TTUR
learning rates are given for the discriminator b and generator a as: “TTUR b a”. Top Left: CelebA.
Top Right: CIFAR-10, starting at mini-batch update 10k for better visualisation. Bottom Left:
SVHN. Bottom Right: LSUN Bedrooms. Training with TTUR (red) is more stable, has much lower
variance, and leads to a better FID.

with the original learning method (orig) and with TTUR. The original training method is faster at
the beginning, but TTUR eventually achieves better performance. DCGAN trained TTUR reaches
constantly a lower FID than the original method and for CelebA and LSUN Bedrooms all one
time-scale runs diverge. For DCGAN the learning rate of the generator is larger then that of the
discriminator, which, however, does not contradict the TTUR theory (see the Appendix Section A5).
In Table 1 we report the best FID with TTUR and one time-scale training for optimized number of
updates and learning rates. TTUR constantly outperforms standard training and is more stable.

WGAN-GP on Image Data. We used the WGAN-GP image model [23] to test TTUR with the
CIFAR-10 and LSUN Bedrooms datasets. In contrast to the original code where the discriminator is
trained ﬁve times for each generator update, TTUR updates the discriminator only once, therefore
we align the training progress with wall-clock time. The learning rate for the original training was
optimized to be large but leads to stable learning. TTUR can use a higher learning rate for the
discriminator since TTUR stabilizes learning. Fig. 6 shows the FID during learning with the original
learning method and with TTUR. Table 1 shows the best FID with TTUR and one time-scale training
for optimized number of iterations and learning rates. Again TTUR reaches lower FIDs than one
time-scale training.

Figure 6: Mean FID (solid line) surrounded by a shaded area bounded by the maximum and the
minimum over 8 runs for WGAN-GP on CelebA, CIFAR-10, SVHN, and LSUN Bedrooms. TTUR
learning rates are given for the discriminator b and generator a as: “TTUR b a”. Left: CIFAR-10,
starting at minute 20. Right: LSUN Bedrooms. Training with TTUR (red) has much lower variance
and leads to a better FID.

8

Figure 7: Performance of WGAN-GP models trained with the original (orig) and our TTUR method
on the One Billion Word benchmark. The performance is measured by the normalized Jensen-
Shannon-divergence based on 4-gram (left) and 6-gram (right) statistics averaged (solid line) and
surrounded by a shaded area bounded by the maximum and the minimum over 10 runs, aligned to
wall-clock time and starting at minute 150. TTUR learning (red) clearly outperforms the original one
time-scale learning.

WGAN-GP on Language Data. Finally the One Billion Word Benchmark [12] serves to evaluate
TTUR on WGAN-GP. The character-level generative language model is a 1D convolutional neural
network (CNN) which maps a latent vector to a sequence of one-hot character vectors of dimension
32 given by the maximum of a softmax output. The discriminator is also a 1D CNN applied to
sequences of one-hot vectors of 32 characters. Since the FID criterium only works for images, we
measured the performance by the Jensen-Shannon-divergence (JSD) between the model and the
real world distribution as has been done previously [23]. In contrast to the original code where the
critic is trained ten times for each generator update, TTUR updates the discriminator only once,
therefore we align the training progress with wall-clock time. The learning rate for the original
training was optimized to be large but leads to stable learning. TTUR can use a higher learning rate
for the discriminator since TTUR stabilizes learning. We report for the 4 and 6-gram word evaluation
the normalized mean JSD for ten runs for original training and TTUR training in Fig. 7. In Table 1
we report the best JSD at an optimal time-step where TTUR outperforms the standard training for
both measures. The improvement of TTUR on the 6-gram statistics over original training shows that
TTUR enables to learn to generate more subtle pseudo-words which better resembles real words.

Table 1: The performance of DCGAN and WGAN-GP trained with the original one time-scale
update rule and with TTUR on CelebA, CIFAR-10, SVHN, LSUN Bedrooms and the One Billion
Word Benchmark. During training we compare the performance with respect to the FID and JSD for
optimized number of updates. TTUR exhibits consistently a better FID and a better JSD.

DCGAN Image
method
dataset
CelebA
TTUR
CIFAR-10 TTUR
TTUR
SVHN
LSUN
TTUR
WGAN-GP Image
method
dataset
CIFAR-10 TTUR
LSUN
TTUR
WGAN-GP Language
n-gram
4-gram
6-gram

method
TTUR
TTUR

b, a
1e-5, 5e-4
1e-4, 5e-4
1e-5, 1e-4
1e-5, 1e-4

updates
225k
75k
165k
340k

FID method
12.5
36.9
12.5
57.5

orig
orig
orig
orig

b, a
3e-4, 1e-4
3e-4, 1e-4

time(m)
700
1900

FID method
24.8
9.5

orig
orig

b, a
3e-4, 1e-4
3e-4, 1e-4

time(m)
1150
1120

JSD method
0.35
0.74

orig
orig

b = a
5e-4
1e-4
5e-5
5e-5

b = a
1e-4
1e-4

b = a
1e-4
1e-4

updates
70k
100k
185k
70k

time(m)
800
2010

time(m)
1040
1070

FID
21.4
37.7
21.4
70.4

FID
29.3
20.5

JSD
0.38
0.77

9

Conclusion

For learning GANs, we have introduced the two time-scale update rule (TTUR), which we have
proved to converge to a stationary local Nash equilibrium. Then we described Adam stochastic
optimization as a heavy ball with friction (HBF) dynamics, which shows that Adam converges and
that Adam tends to ﬁnd ﬂat minima while avoiding small local minima. A second order differential
equation describes the learning dynamics of Adam as an HBF system. Via this differential equation,
the convergence of GANs trained with TTUR to a stationary local Nash equilibrium can be extended
to Adam. Finally, to evaluate GANs, we introduced the ‘Fréchet Inception Distance” (FID) which
captures the similarity of generated images to real ones better than the Inception Score. In experiments
we have compared GANs trained with TTUR to conventional GAN training with a one time-scale
update rule on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.
TTUR outperforms conventional GAN training consistently in all experiments.

Acknowledgment

This work was supported by NVIDIA Corporation, Bayer AG with Research Agreement 09/2017,
Zalando SE with Research Agreement 01/2016, Audi.JKU Deep Learning Center, Audi Electronic
Venture GmbH, IWT research grant IWT150865 (Exaptation), H2020 project grant 671555 (ExCAPE)
and FWF grant P 28660-N31.

The references are provided after Section A6.

References

Appendix

Contents

. .

A1 Fréchet Inception Distance (FID)
A2 Two Time-Scale Stochastic Approximation Algorithms . . . . . . . . . . . . . . . . . .
A2.1 Convergence of Two Time-Scale Stochastic Approximation Algorithms . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A2.1.1 Additive Noise .
A2.1.2 Linear Update, Additive Noise, and Markov Chain . . . . . . . . . . . . .
A2.1.3 Additive Noise and Controlled Markov Processes . . . . . . . . . . . . . .
A2.2 Rate of Convergence of Two Time-Scale Stochastic Approximation Algorithms . .
A2.2.1 Linear Update Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A2.2.2 Nonlinear Update Rules . . . . . . . . . . . . . . . . . . . . . . . . . . .
A2.3 Equal Time-Scale Stochastic Approximation Algorithms . . . . . . . . . . . . . .
A2.3.1 Equal Time-Scale for Saddle Point Iterates . . . . . . . . . . . . . . . . .
A2.3.2 Equal Time Step for Actor-Critic Method . . . . . . . . . . . . . . . . . .
A3 ADAM Optimization as Stochastic Heavy Ball with Friction . . . . . . . . . . . . . . .
A4 Experiments: Additional Information . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A5 Discriminator vs. Generator Learning Rate . . . . . . . . . . . . . . . . . . . . . . . .
A6 Used Software, Datasets, Pretrained Models, and Implementations . . . . . . . . . . . .
.
List of Figures .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
List of Tables .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
16
16
16
18
20
23
23
25
27
27
28
30
32
32
33
33
34
34
38
38

A4.1 WGAN-GP on Image Data.
A4.2 WGAN-GP on the One Billion Word Benchmark.
.
A4.3 BEGAN .

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

.

.

.

10

A1 Fréchet Inception Distance (FID)

(cid:82)

p(.)f (x)dx =

We improve the Inception score for comparing the results of GANs [53]. The Inception score has the
disadvantage that it does not use the statistics of real world samples and compare it to the statistics
of synthetic samples. Let p(.) be the distribution of model samples and pw(.) the distribution of
the samples from real world. The equality p(.) = pw(.) holds except for a non-measurable set if
pw(.)f (x)dx for a basis f (.) spanning the function space in which
and only if
p(.) and pw(.) live. These equalities of expectations are used to describe distributions by moments
or cumulants, where f (x) are polynomials of the data x. We replacing x by the coding layer of an
Inception model in order to obtain vision-relevant features and consider polynomials of the coding
unit functions. For practical reasons we only consider the ﬁrst two polynomials, that is, the ﬁrst two
moments: mean and covariance. The Gaussian is the maximum entropy distribution for given mean
and covariance, therefore we assume the coding units to follow a multidimensional Gaussian. The
difference of two Gaussians is measured by the Fréchet distance [16] also known as Wasserstein-2
distance [58]. The Fréchet distance d(., .) between the Gaussian with mean and covariance (m, C)
obtained from p(.) and the Gaussian (mw, Cw) obtained from pw(.) is called the “Fréchet Inception
Distance” (FID), which is given by [15]:

(cid:82)

d2((m, C), (mw, Cw)) =

mw(cid:107)
Next we show that the FID is consistent with increasing disturbances and human judgment on the
CelebA dataset. We computed the (mw, Cw) on all CelebA images, while for computing (m, C)
we used 50,000 randomly selected samples. We considered following disturbances of the image X:

C + Cw −

2
2 + Tr

m
(cid:107)

CCw

(7)

−

2

(cid:0)

(cid:0)

(cid:1)

(cid:1)

.

1/2

1. Gaussian noise: We constructed a matrix N with Gaussian noise scaled to [0, 255]. The
α)X + αN for α
noisy image is computed as (1
. The larger α is,
}
the larger is the noise added to the image, the larger is the disturbance of the image.

0, 0.25, 0.5, 0.75

∈ {

−

2. Gaussian blur: The image is convolved with a Gaussian kernel with standard deviation
. The larger α is, the larger is the disturbance of the image, that is, the more
0, 1, 2, 4
}

α
the image is smoothed.

∈ {

3. Black rectangles: To an image ﬁve black rectangles are are added at randomly chosen
locations. The rectangles cover parts of the image. The size of the rectangles is αimagesize
. The larger α is, the larger is the disturbance of the image, that
with α
}
is, the more of the image is covered by black rectangles.

0, 0.25, 0.5, 0.75

∈ {

4. Swirl: Parts of the image are transformed as a spiral, that is, as a swirl (whirlpool effect).
Consider the coordinate (x, y) in the noisy (swirled) image for which we want to ﬁnd the
color. Towards this end we need the reverse mapping for the swirl transformation which
gives the location which is mapped to (x, y). We ﬁrst compute polar coordinates relative
x0)) and the radius
to a center (x0, y0) given by the angle θ = arctan((y
5r/(ln 2ρ).
r =
Here α is a parameter for the amount of swirl and ρ indicates the swirl extent in pixels. The
original coordinates, where the color for (x, y) can be found, are xorg = x0 + r cos(θ(cid:48))
and yorg = y0 + r sin(θ(cid:48)). We set (x0, y0) to the center of the image and ρ = 25. The
disturbance level is given by the amount of swirl α
. The larger α is, the larger
∈ {
is the disturbance of the image via the amount of swirl.

y0)2. We transform them according to θ(cid:48) = θ + αe−

0, 1, 2, 4
}

x0)2 + (y

y0)/(x

(cid:112)

(x

−

−

−

−

5. Salt and pepper noise: Some pixels of the image are set to black or white, where black is
chosen with 50% probability (same for white). Pixels are randomly chosen for being ﬂipped
to white or black, where the ratio of pixel ﬂipped to white or black is given by the noise
level α
. The larger α is, the larger is the noise added to the image via
}
ﬂipping pixels to white or black, the larger is the disturbance level.

0, 0.1, 0.2, 0.3

∈ {

6. ImageNet contamination: From each of the 1,000 ImageNet classes, 5 images are randomly
chosen, which gives 5,000 ImageNet images. The images are ensured to be RGB and to
have a minimal size of 256x256. A percentage of α
of the CelebA
images has been replaced by ImageNet images. α = 0 means all images are from CelebA,
α = 0.25 means that 75% of the images are from CelebA and 25% from ImageNet etc.
The larger α is, the larger is the disturbance of the CelebA dataset by contaminating it by
ImageNet images. The larger the disturbance level is, the more the dataset deviates from the
reference real world dataset.

0, 0.25, 0.5, 0.75

∈ {

}

11

We compare the Inception Score [53] with the FID. The Inception Score with m samples and K
classes is

m

K

exp

1
m

i=1
(cid:88)

(cid:88)k=1

(cid:0)

p(yk |

Xi) log

Xi)

p(yk |

p(yk)

.

(cid:1)

The FID is a distance, while the Inception Score is a score. To compare FID and Inception Score,
we transform the Inception Score to a distance, which we call “Inception Distance” (IND). This
transformation to a distance is possible since the Inception Score has a maximal value. For zero
p(yk) = 0. We can bound the
probability p(yk |
log-term by

Xi) = 0, we set the value p(yk |

Xi) log p(yk|

Xi)

p(yk |

log

Xi)

p(yk)

(cid:54) log

1
1/m

= log m .

Using this bound, we obtain an upper bound on the Inception Score:

p(yk |

Xi) log

Xi)

p(yk |

p(yk)

(cid:1)

exp

1
m

(cid:0)
(cid:54) exp

m

K

i=1
(cid:88)

(cid:88)k=1
1
m

log m

m

K

1
m

i=1
(cid:88)
m

(cid:88)k=1
1

i=1
(cid:88)

(cid:1)

(cid:0)

(cid:0)

= exp

log m

= m .

p(yk |

Xi)

(cid:1)

The upper bound is tight and achieved if m (cid:54) K and every sample is from a different class and
the sample is classiﬁed correctly with probability 1. The IND is computed “IND = m - Inception
Score”, therefore the IND is zero for a perfect subset of the ImageNet with m < K samples, where
each sample stems from a different class. Therefore both distances should increase with increasing
disturbance level. In Figure A8 we present the evaluation for each kind of disturbance. The larger the
disturbance level is, the larger the FID and IND should be. In Figure A9, A10, A11, and A11 we
show examples of images generated with DCGAN trained on CelebA with FIDs 500, 300, 133, 100,
45, 13, and FID 3 achieved with WGAN-GP on CelebA.

12

(8)

(9)

(10)

(11)

(12)

Figure A8: Left: FID and right: Inception Score are evaluated for ﬁrst row: Gaussian noise, second
row: Gaussian blur, third row: implanted black rectangles, fourth row: swirled images, ﬁfth row.
salt and pepper noise, and sixth row: the CelebA dataset contaminated by ImageNet images. Left is
the smallest disturbance level of zero, which increases to the highest level at right. The FID captures
the disturbance level very well by monotonically increasing whereas the Inception Score ﬂuctuates,
stays ﬂat or even, in the worst case, decreases.

13

Figure A9: Samples generated from DCGAN trained on CelebA with different FIDs. Left: FID 500
and Right: FID 300.

Figure A10: Samples generated from DCGAN trained on CelebA with different FIDs. Left: FID 133
and Right: FID 100.

14

Figure A11: Samples generated from DCGAN trained on CelebA with different FIDs. Left: FID 45
and Right: FID 13.

Figure A12: Samples generated from WGAN-GP trained on CelebA with a FID of 3.

15

A2 Two Time-Scale Stochastic Approximation Algorithms

Stochastic approximation algorithms are iterative procedures to ﬁnd a root or a stationary point
(minimum, maximum, saddle point) of a function when only noisy observations of its values or
its derivatives are provided. Two time-scale stochastic approximation algorithms are two coupled
iterations with different step sizes. For proving convergence of these interwoven iterates it is assumed
that one step size is considerably smaller than the other. The slower iterate (the one with smaller step
size) is assumed to be slow enough to allow the fast iterate converge while being perturbed by the the
slower. The perturbations of the slow should be small enough to ensure convergence of the faster.
The iterates map at time step n (cid:62) 0 the fast variable wn ∈
their new values:

Rk and the slow variable θn ∈

Rm to

θn+1 = θn + a(n)

h

θn, wn, Z(θ)
n

+ M (θ)

n

,

wn+1 = wn + b(n)

(cid:16)

(cid:0)
g

(cid:1)
θn, wn, Z(w)

n

(cid:17)
+ M (w)

n

.

(cid:16)

(cid:0)

(cid:1)

(cid:17)

(13)

(14)

The iterates use

•

•

•

•

•

•

•

•

M (w)

Z(θ)

Z(w)

h(.)

g(.)

∈

∈

Rm: mapping for the slow iterate Eq. (13),
Rk: mapping for the fast iterate Eq. (14),

a(n): step size for the slow iterate Eq. (13),

b(n): step size for the fast iterate Eq. (14),
M (θ)

n : additive random Markov process for the slow iterate Eq. (13),

n : additive random Markov process for the fast iterate Eq. (14),

n : random Markov process for the slow iterate Eq. (13),

n : random Markov process for the fast iterate Eq. (14).

A2.1 Convergence of Two Time-Scale Stochastic Approximation Algorithms

A2.1.1 Additive Noise

The ﬁrst result is from Borkar 1997 [9] which was generalized in Konda and Borkar 1999 [31].
Borkar considered the iterates:

θn+1 = θn + a(n)

h

θn, wn

+ M (θ)

n

wn+1 = wn + b(n)

(cid:16)

(cid:0)
g

(cid:1)
θn, wn

(cid:17)
+ M (w)

n

(cid:16)

(cid:0)

(cid:1)

,

.

(cid:17)

(cid:55)→

Assumptions. We make the following assumptions:

(A1) Assumptions on the update functions: The functions h : Rk+m

Rm and g : Rk+m

Rk

are Lipschitz.

(A2) Assumptions on the learning rates:

(15)

(16)

(cid:55)→

(17)

(18)

(19)

n
(cid:88)

a(n) =

b(n) =

,

,

∞

∞

n
(cid:88)
a(n) = o(b(n)) ,

a2(n) <

b2(n) <

,

,

∞

∞

n
(cid:88)

n
(cid:88)

16

(A3) Assumptions on the noise: For the increasing σ-ﬁeld

Fn = σ(θl, wl, M (θ)
the sequences of random variables (M (θ)
n ,

l

, l (cid:54) n), n (cid:62) 0 ,

, M (w)
l
Fn) and (M (w)
n ,
n <

a.s.

Fn) satisfy

a(n) M (θ)

n
(cid:88)

b(n) M (w)

n <

a.s. .

∞

∞

n
(cid:88)
(A4) Assumption on the existence of a solution of the fast iterate: For each θ

Rm, the ODE

∈

˙w(t) = g

θ, w(t)

has a unique global asymptotically stable equilibrium λ(θ) such that λ : Rm
Lipschitz.

(cid:0)

(cid:1)

(cid:55)→

(A5) Assumption on the existence of a solution of the slow iterate: The ODE

˙θ(t) = h

θ(t), λ(θ(t))

has a unique global asymptotically stable equilibrium θ∗.
(cid:1)

(cid:0)

(A6) Assumption of bounded iterates:

(20)

(21)

(22)

Rk is

(23)

(24)

(25)

sup

n (cid:107)

sup

n (cid:107)

θn(cid:107)
wn(cid:107)

<

<

,

.

∞

∞

Convergence Theorem The next theorem is from Borkar 1997 [9].
Theorem 3 (Borkar). If the assumptions are satisﬁed, then the iterates Eq. (15) and Eq. (16) converge
to (θ∗, λ(θ∗)) a.s.

Comments

difference sequence w.r.t

(C1) According to Lemma 2 in [7] Assumption (A3) is fulﬁlled if
Fn with
E

(cid:54) B1

2
M (θ)
n (cid:107)
(cid:107)

(θ)
n
| F

and

M (w)
n

{

}

(cid:104)
is a martingale difference sequence w.r.t

(cid:105)

Fn with
(cid:54) B2 ,

E

2
M (w)
n (cid:107)
(cid:107)

(w)
n
| F

(cid:104)

(cid:105)

where B1 and B2 are positive deterministic constants.

M (θ)
n
{

}

is a martingale

(C2) Assumption (A3) holds for mini-batch learning which is the most frequent case of stochastic
i=1 f (xi, θ)), 1 (cid:54) i (cid:54) N and the mini-
∇θ( 1
gradient. The batch gradient is Gn :=
i=1 f (xui, θ)), 1 (cid:54) ui (cid:54) N , where the
∇θ( 1
batch gradient for batch size s is hn :=
indexes ui are randomly and uniformly chosen. For the noise M (θ)
Gn we have
n := hn −
E[M (θ)
Gn = 0. Since the indexes are chosen without knowing
past events, we have a martingale difference sequence. For bounded gradients we have
bounded

Gn = Gn −

n ] = E[hn]

M (θ)
n

(cid:80)
(cid:80)

2.

−

N

N

s

s

(C3) We address assumption (A4) with weight decay in two ways: (I) Weight decay avoids
problems with a discriminator that is region-wise constant and, therefore, does not have a
locally stable generator. If the generator is perfect, then the discriminator is 0.5 everywhere.
For generator with mode collapse, (i) the discriminator is 1 in regions without generator
examples, (ii) 0 in regions with generator examples only, (iii) is equal to the local ratio

(cid:107)

(cid:107)

17

of real world examples for regions with generator and real world examples. Since the
discriminator is locally constant, the generator has gradient zero and cannot improve. Also
the discriminator cannot improve, since it has minimal error given the current generator.
However, without weight decay the Nash Equilibrium is not stable since the second order
derivatives are zero, too. (II) Weight decay avoids that the generator is driven to inﬁnity
with unbounded weights. For example a linear discriminator can supply a gradient for the
generator outside each bounded region.

(C4) The main result used in the proof of the theorem relies on work on perturbations of ODEs

(C5) Konda and Borkar 1999 [31] generalized the convergence proof to distributed asynchronous

according to Hirsch 1989 [24].

update rules.

(C6) Tadi´c relaxed the assumptions for showing convergence [54]. In particular the noise as-
sumptions (Assumptions A2 in [54]) do not have to be martingale difference sequences
and are more general than in [9]. In another result the assumption of bounded iterates is
not necessary if other assumptions are ensured [54]. Finally, Tadi´c considers the case of
non-additive noise [54]. Tadi´c does not provide proofs for his results. We were not able
to ﬁnd such proofs even in other publications of Tadi´c.

A2.1.2 Linear Update, Additive Noise, and Markov Chain

In contrast to the previous subsection, we assume that an additional Markov chain inﬂuences the
iterates [30, 32]. The Markov chain allows applications in reinforcement learning, in particular in
actor-critic setting where the Markov chain is used to model the environment. The slow iterate is the
actor update while the fast iterate is the critic update. For reinforcement learning both the actor and
the critic observe the environment which is driven by the actor actions. The environment observations
are assumed to be a Markov chain. The Markov chain can include eligibility traces which are modeled
as explicit states in order to keep the Markov assumption.

The Markov chain is the sequence of observations of the environment which progresses via transition
probabilities. The transitions are not affected by the critic but by the actor.

Konda et al. considered the iterates [30, 32]:

θn+1 = θn + a(n) Hn ,

wn+1 = wn + b(n)

g

Z(w)

n ; θn

+ G

Z(w)

n ; θn

wn + M (w)

n wn

.

Hn is a random process that drives the changes of θn. We assume that Hn is a slow enough process.
(cid:0)
Rk and the matrix
We have a linear update rule for the fast iterate using the vector function g(.)
function G(.)

Rk

k.

∈

(cid:1)

(cid:0)

(cid:1)

(cid:16)

(cid:17)

×

∈

Assumptions. We make the following assumptions:

(A1) Assumptions on the Markov process, that is, the transition kernel: The stochastic process
takes values in a Polish (complete, separable, metric) space Z with the Borel σ-ﬁeld

Z(w)
n

Fn = σ(θl, wl, Z(w)
| Fn) = P(Z(w)
A
We deﬁne for every measurable function f

For every measurable set A
P(Z(w)

n+1 ∈

n+1 ∈

⊂

l

|

, Hl, l (cid:54) n), n (cid:62) 0 .

Z and the parametrized transition kernel P(.; θn) we have:
n ; θn) = P(Z(w)
(28)

n , A; θn) .

Z(w)

A

(A2) Assumptions on the learning rates:

Pθf (z) :=

P(z, d ¯z; θn) f ( ¯z) .

b2(n) <

,

∞

b(n) =

,

n
(cid:88)

n (cid:18)
(cid:88)

a(n)
b(n)

(cid:19)

n
(cid:88)

<

,

∞

(cid:90)

∞

d

18

(26)

(27)

(29)

(30)

for some d > 0.

(A3) Assumptions on the noise: The sequence M (w)

n

is a k

k-matrix valued

×

Fn-martingale

difference with bounded moments:

We assume slowly changing θ, therefore the random process Hn satisﬁes

E

M (w)
n

= 0 ,

(cid:104)
E

sup
n

| Fn
d

M (w)
n

(cid:105)

(cid:21)

<

,

d > 0 .

∞

∀

(cid:20)(cid:13)
(cid:13)
(cid:13)

(cid:104)

(cid:13)
(cid:13)
(cid:13)

d

(cid:105)

E

sup
n

Hn(cid:107)
(cid:107)

<

,

d > 0 .

∞

∀

(A4) Assumption on the existence of a solution of the fast iterate: We assume the existence of a
Rm, there exist functions
k that satisfy the

solution to the Poisson equation for the fast iterate. For each θ
Rk, ¯G(θ)
¯g(θ)
Poisson equations:

∈
Rk, and ˆG(z; θ) : Z

k, ˆg(z; θ) : Z

Rk

Rk

→

→

∈

∈

×

×

ˆg(z; θ) = g(z; θ)
ˆG(z; θ) = G(z; θ)

¯g(θ) + (Pθ ˆg(.; θ))(z) ,
¯G(θ) + (Pθ ˆG(.; θ))(z) .

−

−

(A5) Assumptions on the update functions and solutions to the Poisson equation:
(a) Boundedness of solutions: For some constant C and for all θ:

max

max

{(cid:107)

¯g(θ)
¯G(θ)

{(cid:107)

(cid:107)}

(cid:107)}

(cid:54) C ,
(cid:54) C .

(b) Boundedness in expectation: All moments are bounded. For any d > 0, there exists

Cd > 0 such that

E

sup
n

ˆg(Z(w)

(cid:54) Cd ,

(cid:21)

E

sup
n

(cid:20)(cid:13)
(cid:13)
g(Z(w)
(cid:13)
(cid:20)(cid:13)
(cid:13)
ˆG(Z(w)
(cid:13)
(cid:20)(cid:13)
(cid:13)
G(Z(w)
(cid:13)
(cid:20)(cid:13)
(cid:13)
(c) Lipschitz continuity of solutions: For some constant C > 0 and for all θ, ¯θ
(cid:13)

(cid:54) Cd ,

(cid:54) Cd .

(cid:54) Cd ,

sup
n

sup
n

E

E

(cid:21)

(cid:21)

(cid:21)

Rm:

∈

d

n ; θ)
(cid:13)
d
(cid:13)
n ; θ)
(cid:13)
(cid:13)
d
(cid:13)
n ; θ)
(cid:13)
(cid:13)
d
(cid:13)
n ; θ)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:54) C
(cid:54) C

(d) Lipschitz continuity in expectation: There exists a positive measurable function C(.)

on Z such that

¯g(θ)
¯G(θ)
(cid:13)
(cid:13)

−

−

¯g( ¯θ)
¯G( ¯θ)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

θ
(cid:107)
θ
(cid:107)

−

−

¯θ
(cid:107)
¯θ
(cid:107)

,

.

E

sup
n

(cid:104)

C(Z(w)

n )d

<

,

d > 0 .

∞

∀

(cid:105)

Function C(.) gives the Lipschitz constant for every z:

(Pθ ˆg(.; θ))(z)
(Pθ ˆG(.; θ))(z)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

−

(P ¯θ ˆg(.; ¯θ))(z)
ˆG(.; ¯θ))(z)

(P ¯θ

(cid:54) C(z)

(cid:54) C(z)

θ
(cid:107)
θ
(cid:107)

−

−

¯θ
(cid:107)
¯θ
(cid:107)

,

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(e) Uniform positive deﬁniteness: There exists some α > 0 such that for all w

Rm:

θ

∈

Rk and

∈

(47)

wT ¯G(θ) w (cid:62) α

2 .

w
(cid:107)

(cid:107)

19

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

Convergence Theorem. We report Theorem 3.2 (see also Theorem 7 in [32]) and Theorem 3.13
from [30]:
Theorem 4 (Konda & Tsitsiklis). If the assumptions are satisﬁed, then for the iterates Eq. (26) and
Eq. (27) holds:

lim
n
→∞
lim
n
→∞

¯G(θn) wn −
(cid:13)
wn −
(cid:13)
(cid:13)
(cid:13)

¯G−

¯g(θn)

= 0 a.s. ,

1(θn) ¯g(θn)
(cid:13)
(cid:13)

= 0 .

(cid:13)
(cid:13)

(48)

(49)

Comments.

(C1) The proofs only use the boundedness of the moments of Hn [30, 32], therefore Hn may
depend on wn. In his PhD thesis [30], Vijaymohan Konda used this framework for the
actor-critic learning, where Hn drives the updates of the actor parameters θn. However, the
actor updates are based on the current parameters wn of the critic.

(C2) The random process Z(w)

n

can affect Hn as long as boundedness is ensured.
Z(w)

(C3) Nonlinear update rule. g

Z(w)
wn can be viewed as a linear approxi-
mation of a nonlinear update rule. The nonlinear case has been considered in [30] where
additional approximation errors due to linearization were addressed. These errors are treated
in the given framework [30].

n ; θn

n ; θn

+ G

(cid:0)

(cid:1)

(cid:0)

(cid:1)

A2.1.3 Additive Noise and Controlled Markov Processes

The most general iterates use nonlinear update functions g and h, have additive noise, and have
controlled Markov processes [28].

θn+1 = θn + a(n)

h

θn, wn, Z(θ)
n

+ M (θ)

n

,

wn+1 = wn + b(n)

(cid:16)

(cid:0)
g

(cid:1)
θn, wn, Z(w)

n

(cid:17)
+ M (w)

n

.

(cid:16)

(cid:0)

(cid:1)

(cid:17)

(50)

(51)

Required Deﬁnitions. Marchaud Map: A set-valued map h : Rl
Marchaud map if it satisﬁes the following properties:

→ {

subsets of Rk} is called a

(i) For each θ
(ii) (point-wise boundedness) For each θ

Rl, h(θ) is convex and compact.

Rl,

∈

∈

sup

h(θ) (cid:107)
∈

w

(cid:107)

w

< K (1 +

θ

(cid:107)

) for some K > 0.
(cid:107)

(iii) h is an upper-semicontinuous map.

1 (in Rl) and
θn}n
We say that h is upper-semicontinuous, if given sequences
1
{
≥
≥
(in Rk) with θn →
h(θn), n
h(θ). In other words, the
1, y
∈
Rk.
, is closed in Rl
graph of h,
×

(x, y) : y

yn}n
{

≥

(cid:9)
is Marchaud, then the differential inclusion (DI)

θ, yn →
y and yn ∈
Rl
∈
∈
subsets of Rm

h(x), x

}

If the set-valued map H : Rm
given by

(cid:8)

→ {

˙θ(t)

H(θ(t))

∈

(52)

is guaranteed to have at least one solution that is absolutely continuous. If Θ is an absolutely
continuous map satisfying Eq. (52) then we say that Θ

Σ.

⊆

Invariant Set: M
with Θ(0) = θ. In other words, Θ
Internally Chain Transitive Set: M
and for every θ, y
∈
solutions to the differential inclusion ˙θ(t)

Rm is invariant if for every θ
0.
Σ with Θ(t)
Rm is said to be internally chain transitive if M is compact
M , (cid:15) > 0 and T > 0 we have the following: There exist Φ1, . . . , Φn that are n
M and

M there exists a trajectory, Θ, entirely in M
M , for all t

h(θ(t)), a sequence θ1(= θ), . . . , θn+1(= y)

∈
⊂

∈
∈

≥

∈

⊂

∈

20

n real numbers t1, t2, . . . , tn greater than T such that: Φi
(cid:15)-neighborhood of θ and Φi
is called an ((cid:15), T ) chain in M from θ to y.

[0,ti](θi)

M for 1

⊂

≤

≤

i

N (cid:15)(θi+1) where N (cid:15)(θ) is the open
ti(θi)
n. The sequence (θ1(= θ), . . . , θn+1(= y))

∈

Assumptions. We make the following assumptions [28]:

Z(w)
(A1) Assumptions on the controlled Markov processes: The controlled Markov process
n
}
Z(θ)
takes values in a compact metric space S(w). The controlled Markov process
n
}
takes values in a compact metric space S(θ). Both processes are controlled by the iterate
Z(w)
θn}
is additionally controlled by a random
and
sequences
n
{
{
Z(θ)
A(w)
taking values in a compact metric space U (w) and
process
is additionally
n
n
{
{
}
taking values in a compact metric space U (θ). The
controlled by a random process
Z(θ)
n
{
}
P(Z(θ)

, θl, wl, l (cid:54) n) =

. Furthermore

dynamics is

wn}
{

p(θ)(dz

A(θ)
n

B(θ)

{
{

n , A(θ)

, A(θ)
l

}

}

{

}

n+1 ∈

Z(θ)
l
|

Z(θ)
|

n , θn, wn), n (cid:62) 0 ,
(53)

(cid:90)B(θ)

for B(θ) Borel in S(θ). The

dynamics is

Z(w)
n

}

{
, A(w)
l

P(Z(w)

n+1 ∈

B(w)

Z(w)
l
|

for B(w) Borel in S(w).

, θl, wl, l (cid:54) n) =

p(w)(dz

Z(w)

n , A(w)

n , θn, wn), n (cid:62) 0 ,

(cid:90)B(w)

|

(54)

(A2) Assumptions on the update functions: h : Rm+k

Rm is jointly continuous as
well as Lipschitz in its ﬁrst two arguments uniformly w.r.t. the third. The latter condition
means that
z(θ)
∀

h(θ(cid:48), w(cid:48), z(θ))
(cid:107)

h(θ, w, z(θ))
(cid:107)

(cid:54) L(θ) (
(cid:107)

S(θ) :

w
(cid:107)

S(θ)

w(cid:48)

→

θ(cid:48)

−

+

−

−

×

∈

θ

(cid:107)

) .
(cid:107)
(55)

Note that the Lipschitz constant L(θ) does not depend on z(θ).
g : Rk+m
uniformly w.r.t. the third. The latter condition means that

S(w)

→

×

Rk is jointly continuous as well as Lipschitz in its ﬁrst two arguments

z(w)
∀

∈

S(w) :

g(θ, w, z(w))
(cid:107)

−

g(θ(cid:48), w(cid:48), z(w))
(cid:107)

(cid:54) L(w) (

θ
(cid:107)

−

θ(cid:48)

+

(cid:107)

w
(cid:107)

−

w(cid:48)

) .
(cid:107)
(56)

Note that the Lipschitz constant L(w) does not depend on z(w).

(A3) Assumptions on the additive noise:

M (θ)
M (w)
and
n
n
}
}
{
{
2 +
wn(cid:107)
θn(cid:107)
with second moments bounded by K(1 +
(cid:107)
(cid:107)
martingale difference sequence w.r.t. increasing σ-ﬁelds
Fn = σ(θl, wl, M (θ)

, M (w)
l

, Z(w)
l

, Z(θ)
l

l

satisfying

are martingale difference sequence
is a

2). More precisely,

M (θ)
n

{

}

, l (cid:54) n), n (cid:62) 0 ,

(cid:54) K (1 +

2 +

θn(cid:107)

(cid:107)

2) ,

wn(cid:107)
(cid:107)

E

M (θ)

2

n+1(cid:107)

(cid:107)
(cid:105)
(cid:104)
for n (cid:62) 0 and a given constant K > 0.
M (w)
n
{

}

| Fn

is a martingale difference sequence w.r.t. increasing σ-ﬁelds

Fn = σ(θl, wl, M (θ)

l

, M (w)
l

, Z(θ)
l

, Z(w)
l

, l (cid:54) n), n (cid:62) 0 ,

satisfying

E

M (w)
(cid:107)

n+1(cid:107)

2

| Fn

(cid:54) K (1 +

2 +

θn(cid:107)

(cid:107)

2) ,

wn(cid:107)

(cid:107)

(cid:105)
(cid:104)
for n (cid:62) 0 and a given constant K > 0.

21

(57)

(58)

(59)

(60)

(A4) Assumptions on the learning rates:

n
(cid:88)

a(n) =

b(n) =

,

,

∞

∞

n
(cid:88)
a(n) = o(b(n)) ,

a2(n) <

b2(n) <

,

,

∞

∞

n
(cid:88)

n
(cid:88)

Furthermore, a(n), b(n), n (cid:62) 0 are non-increasing.

(A5) Assumptions on the controlled Markov processes, that is, the transition kernels: The state-

action map

S(θ)

U (θ)

×

×

Rm+k

(cid:51)

and the state-action map

S(w)

U (w)

×

×

Rm+k

(cid:51)

are continuous.

(z(θ), a(θ), θ, w)

p(θ)(dy

z(θ), a(θ), θ, w)

(64)

(z(w), a(w), θ, w)

p(w)(dy

z(w), a(w), θ, w)

(65)

→

→

|

|

(A6) Assumptions on the existence of a solution:

We consider occupation measures which give for the controlled Markov process the prob-
U for given θ and a
ability or density to observe a particular state-action pair from S
given control policy π. We denote by D(w)(θ, w) the set of all ergodic occupation measures
for the prescribed θ and w on state-action space S(w)
U (θ) for the controlled Markov
process Z(w) with policy π(w). Analogously we denote, by D(θ)(θ, w) the set of all ergodic
occupation measures for the prescribed θ and w on state-action space S(θ)
U (θ) for the
controlled Markov process Z(θ) with policy π(θ). Deﬁne

×

×

×

˜g(θ, w, ν) =

g(θ, w, z) ν(dz, U (w))

(cid:90)

for ν a measure on S(w)

U (w) and the Marchaud map

×

ˆg(θ, w) =

˜g(θ, w, ν) : ν

{

D(w)(θ, w)
}

.

∈

We assume that the set D(w)(θ, w) is singleton, that is, ˆg(θ, w) contains a single function
and we use the same notation for the set and its single element. If the set is not a singleton, the
assumption of a solution can be expressed by the differential inclusion ˙w(t)
ˆg(θ, w(t))
[28].
θ

Rm, the ODE

∈

∀

∈

˙w(t) = ˆg(θ, w(t))

→

has an asymptotically stable equilibrium λ(θ) with domain of attraction Gθ where λ :
Rk is a Lipschitz map with constant K. Moreover, the function V : G
Rm
)
→
∞
is continuously differentiable where V (θ, .) is the Lyapunov function for λ(θ) and G =
(θ, w) : w
(θ, λ(θ)) :
{
{
Rm
θ

becomes an asymptotically stable set of the coupled ODE

. This extra condition is needed so that the set

Gθ, θ

Rm

[0,

∈

∈

}

∈

}

(A7) Assumption of bounded iterates:

˙w(t) = ˆg(θ(t), w(t))
˙θ(t) = 0 .

sup

n (cid:107)

sup

n (cid:107)

θn(cid:107)
wn(cid:107)

<

<

∞

∞

a.s. ,

a.s.

22

(61)

(62)

(63)

(66)

(67)

(68)

(69)

(70)

(71)

(72)

Convergence Theorem. The following theorem is from Karmakar & Bhatnagar [28]:
Theorem 5 (Karmakar & Bhatnagar). Under above assumptions if for all θ
1,

Rm, with probability
belongs to a compact subset Qθ (depending on the sample point) of Gθ “eventually”, then
(73)

A0 (θ∗, λ(θ∗)) a.s.

wn}

as n

∈

θ∗

{

,

∈

→ ∪
which is almost everywhere an internally chain transitive set of the

→ ∞

(θn, wn)
¯θ(s) : s (cid:62) t
}

where A0 =
∩t(cid:62)0{
differential inclusion

˙θ(t)

ˆh(θ(t)),

∈

(74)

where ˆh(θ) =

˜h(θ, λ(θ), ν) : ν
{

∈

D(w)(θ, λ(θ))

.
}

Comments.

(C1) This framework allows to show convergence for gradient descent methods beyond stochastic
gradient like for the ADAM procedure where current learning parameters are memorized
and updated. The random processes Z(w) and Z(θ) may track the current learning status for
the fast and slow iterate, respectively.

(C2) Stochastic regularization like dropout is covered via the random processes A(w) and A(θ).

A2.2 Rate of Convergence of Two Time-Scale Stochastic Approximation Algorithms

A2.2.1 Linear Update Rules

First we consider linear iterates according to the PhD thesis of Konda [30] and Konda & Tsitsiklis
[33].

θn+1 = θn + a(n)

wn+1 = wn + b(n)

a1 −
a2 −

A11 θn −
A21 θn −

A12 wn + M (θ)

n

(cid:17)
A22 wn + M (w)

n

(cid:16)

(cid:16)

,

.

(cid:17)

Assumptions. We make the following assumptions:

(A1) The random variables (M (θ)

n , M (w)
other. The have zero mean: E[M (θ)

n ), n = 0, 1, . . ., are independent of w0, θ0 and of each
n ] = 0 and E[M (w)

n ] = 0. The covariance is

E

M (θ)

n (M (θ)

n )T

= Γ11 ,

(cid:104)
M (θ)

n (M (w)

n )T

E

(cid:104)
M (w)
n

E

(M (w)

n )T

= Γ12 = ΓT

21 ,

= Γ22 .

(cid:105)

(cid:105)

(cid:105)

a(n) =

b(n) =

,

,

∞

∞

lim
n
→∞

lim
n
→∞

a(n) = 0 ,

b(n) = 0 ,

(cid:104)

n
(cid:88)

(A2) The learning rates are deterministic, positive, nondecreasing and satisfy with (cid:15) (cid:54) 0:

n
(cid:88)
a(n)
b(n) →
We often consider the case (cid:15) = 0.
(A3) Convergence of the iterates: We deﬁne

(cid:15) .

∆ := A11 −

1
A12A−

22 A21 .

A matrix is Hurwitz if the real part of each eigenvalue is strictly negative. We assume that
the matrices

∆ are Hurwitz.

A22 and

−

−

23

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

(85)

(86)

(87)
(88)

(89)

(90)

(91)

(92)

(96)

(97)

(98)

(99)

(A4) Convergence rate remains simple:

(a) There exists a constant ¯a (cid:54) 0 such that
(a(n + 1)−

1

a(n)−

1) = ¯a .

lim
n

lim
n

−

−

1
(b(n + 1)−

b(n)−

1) = 0 .

(b) If (cid:15) = 0, then

(c) The matrix

is Hurwitz.

∆

−

−

(cid:16)

¯a
2

I

(cid:17)

Rate of Convergence Theorem. The next theorem is taken from Konda [30] and Konda & Tsitsik-
lis [33].

Let θ∗

Rm and w∗

Rk be the unique solution to the system of linear equations

∈

∈

For each n, let

A11 θn + A12 wn = a1 ,
A21 θn + A22 wn = a2 .

ˆθn = θn −
ˆwn = wn −
1
Σn
11 = θ−
n E

θ∗ ,

1

22 (a2 −
A−
ˆθn ˆθT
,
n

Σn

12 =

Σn
21

(cid:104)
T

(cid:105)
1
= θ−
n E

A21 θn) ,

,

ˆθn ˆwT
n
(cid:104)

(cid:105)

(cid:0)

Σn

ˆwn ˆwT
n

1
22 = w−
(cid:1)
n E
Σn
11 Σn
(cid:2)
12
21 Σn
Σn
22(cid:19)
Theorem 6 (Konda & Tsitsiklis). Under above assumptions and when the constant (cid:15) is sufﬁciently
small, the limit matrices
Σ((cid:15))

Σn =

(94)

(93)

Σn

Σn

Σn

(cid:18)

(cid:3)
.

(95)

12 , Σ((cid:15))

11 , Σ((cid:15))

22 .

,

12 = lim
n

22 = lim
n

11 = lim
n
exist. Furthermore, the matrix

Σ(0) =

Σ(0)
Σ(0)

11 Σ(0)
21 Σ(0)

12
22 (cid:33)

(cid:32)

is the unique solution to the following system of equations

¯a Σ(0)

11 + A12 Σ(0)

21 + Σ(0)

12 AT

12 = Γ11 ,

11 + Σ(0)

∆ Σ(0)
A12 Σ(0)
A22 Σ(0)

11 ∆T
12 AT
22 AT

22 + Σ(0)
22 + Σ(0)

−
22 = Γ12 ,

22 = Γ22 .

Finally,

lim
0
(cid:15)
↓

Σ((cid:15))

11 = Σ(0)
11 ,

Σ((cid:15))

12 = Σ(0)
12 ,

Σ((cid:15))

22 = Σ(0)
22 .

(100)

lim
0
(cid:15)
↓

lim
0
(cid:15)
↓

The next theorems shows that the asymptotic covariance matrix of a(n)−
a(n)−

1/2 ¯θn, where ¯θn evolves according to the single time-scale stochastic iteration:
a1 −
(cid:16)
A21 ¯θn −

¯θn+1 = ¯θn + a(n)
0 = a2 −

A11 ¯θn −
A22 ¯wn + M (w)

A12 ¯wn + M (θ)

(cid:17)

n

n

.

,

1/2θn is the same as that of

(101)

(102)

The next theorem combines Theorem 2.8 of Konda & Tsitsiklis and Theorem 4.1 of Konda &
Tsitsiklis:

24

Theorem 7 (Konda & Tsitsiklis 2nd). Under above assumptions

If the assumptions hold with (cid:15) = 0, then a(n)−

Σ(0)

11 = lim
n

a(n)−

1 E

¯θn ¯θT
n

.

(103)

(cid:3)
1/2 ˆθn converges in distribution to

(cid:2)

(0, Σ(0)

11 ).

N

Comments.

(C1) In his PhD thesis [30] Konda extended the analysis to the nonlinear case. Konda makes a

linearization of the nonlinear function h and g with

A11 =

, A12 =

, A21 =

, A22 =

(104)

∂h
∂θ

−

∂h
∂w

−

∂g
∂θ

−

∂g
∂w

.

−

There are additional errors due to linearization which have to be considered. However, only
a sketch of a proof is provided but not a complete proof.

(C2) Theorem 4.1 of Konda & Tsitsiklis is important to generalize to the nonlinear case.
(C3) The convergence rate is governed by A22 for the fast and ∆ for the slow iterate. ∆ in turn
is affected by the interaction effects captured by A21 and A12 together with the inverse of
A22.

A2.2.2 Nonlinear Update Rules

The rate of convergence for nonlinear update rules according to Mokkadem & Pelletier is considered
[44].

The iterates are

θn+1 = θn + a(n)

h

θn, wn

+ Z(θ)

n + M (θ)
n

,

wn+1 = wn + b(n)

(cid:16)

(cid:0)
g

(cid:1)
θn, wn

+ Z(w)

(cid:17)
n + M (w)

n

.

with the increasing σ-ﬁelds

(cid:16)

(cid:0)

(cid:1)

(cid:17)

Fn = σ(θl, wl, M (θ)

l

, M (w)
l

, Z(θ)
l

, Z(w)
l

, l (cid:54) n), n (cid:62) 0 .

The terms Z(θ)
of the nonlinear functions to their linear approximation.

n and Z(w)

n

can be used to address the error through linearization, that is, the difference

Assumptions. We make the following assumptions:

(A1) Convergence is ensured:

θn = θ∗ a.s. ,

wn = w∗ a.s. .

lim
n
→∞
lim
n
→∞

(A2) Linear approximation and Hurwitz:
There exists a neighborhood

of (θ∗, w∗) such that, for all (θ, w)

h
g

θ, w
θ, w
(cid:0)
(cid:0)
We deﬁne

(cid:18)

(cid:19)

(cid:1)
(cid:1)

=

(cid:18)

U
A11 A12
A21 A22

θ
w

θ∗
w∗

(cid:19)

−
−

(cid:19) (cid:18)

+ O

∈ U
θ∗
w∗

θ
w

−
−

2

.

(cid:33)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:32)(cid:13)
(cid:13)
(cid:13)
(cid:13)

∆ := A11 −

1
A12A−

22 A21 .

A matrix is Hurwitz if the real part of each eigenvalue is strictly negative. We assume that
the matrices A22 and ∆ are Hurwitz.

25

(105)

(106)

(107)

(108)

(109)

(110)

(111)

(A3) Assumptions on the learning rates:

b(n) = b0 n−
(113)
where a0 > 0 and b0 > 0 and 1/2 < β < α (cid:54) 1. If α = 1, then a0 > 1/(2emin) with emin
as the absolute value of the largest eigenvalue of ∆ (the eigenvalue closest to 0).

a(n) = a0 n−

α

β ,

(A4) Assumptions on the noise and error:

(a) martingale difference sequences:

E

E

M (θ)

n+1 | Fn

(cid:104)
M (w)

n+1 | Fn

(cid:104)

(cid:105)

(cid:105)

= 0 a.s. ,

= 0 a.s. .

(b) existing second moments:

E

lim
n
→∞

(cid:34)(cid:32)

M (θ)
n+1
M (w)
n+1(cid:33)

(cid:16)

(M (θ)

n+1)T

(M (w)

n+1)T

= Γ =

| Fn

(cid:35)

(cid:17)

Γ11 Γ12
Γ21 Γ22

(cid:18)

a.s.

(cid:19)

(116)

(c) bounded moments:

There exist l > 2/β such that

E

sup
n

E

sup
n

l
n+1(cid:107)
l
n+1(cid:107)

M (θ)
(cid:107)
(cid:104)
M (w)
(cid:107)
(cid:104)

| Fn

| Fn

(cid:105)

(cid:105)

<

<

∞

∞

a.s. ,

a.s.

(d) bounded error:

with

Z(θ)
n = r(θ)
n = r(w)
Z(w)

n + O
n + O
(cid:0)

θ

(cid:107)

θ
(cid:107)

−

−

θ∗

2 +
(cid:107)
2 +
θ∗
(cid:107)

w

w

(cid:107)

(cid:107)

w∗

2
(cid:107)
w∗

2
(cid:1)

(cid:107)

,

,

−

−

r(θ)
n (cid:107)

(cid:107)

+

(cid:0)
r(w)
n (cid:107)

(cid:107)

= o(

a(n)) a.s.

(cid:112)

(cid:1)

Rate of Convergence Theorem. We report a theorem and a proposition from Mokkadem & Pel-
letier [44]. However, ﬁrst we have to deﬁne the covariance matrices Σθ and Σw which govern the
rate of convergence.

First we deﬁne

Γθ := lim
→∞

n

E

M (θ)

n+1 −

A12 A−

1

22 M (w)

n+1

M (θ)

n+1 −

A12 A−

1

22 M (w)

n+1

T

(cid:17)

=

| Fn

(cid:21)

(122)

Γ11 + A12 A−

22 Γ22 (A−

1
22 )T AT

12 −
We now deﬁne the asymptotic covariance matrices Σθ and Σw:

12 −

Γ12(A−

A12 A−

22 Γ21 .

1

(cid:17) (cid:16)
1
22 )T AT

Σθ =

∞

exp

∆ +

I

t

Γθ exp

∆T +

I

t

dt ,

(123)

(cid:18)(cid:18)

(cid:19)

(cid:19)

(cid:18)(cid:18)

(cid:19)

(cid:19)

1a=1
2 a0

1a=1
2 a0

Σw =

exp (A22 t) Γ22 exp (A22 t) dt .

(cid:20)(cid:16)
1

∞

0
(cid:90)

0
(cid:90)

Σθ and Σw are solutions of the Lyapunov equations:

1a=1
2 a0

∆ +

(cid:18)

(cid:19)

I

Σθ + Σθ

∆T +

I

=

Γθ ,

(cid:18)

(cid:19)
A22 Σw + Σw AT
22 =

−

−

Γ22 .

1a=1
2 a0

26

(112)

(114)

(115)

(117)

(118)

(119)

(120)

(121)

(124)

(125)

(126)

Theorem 8 (Mokkadem & Pelletier: Joint weak convergence). Under above assumptions:

θ∗)
w∗)
Theorem 9 (Mokkadem & Pelletier: Strong convergence). Under above assumptions:

Σθ
0
0 Σw

a(n)−
b(n)−

1 (θ
1 (w

D
−→ N

−
−

(cid:19)(cid:19)

0 ,

(cid:18)

(cid:18)

(cid:19)

.

(cid:18) (cid:112)
(cid:112)

θ
(cid:107)

θ∗

(cid:107)

−

= O

a(n) log

a(l)

a.s. ,





(cid:118)
(cid:117)
(cid:117)
(cid:116)





(cid:118)
(cid:117)
(cid:117)
(cid:116)

n

(cid:32)

(cid:88)l=1
n

(cid:32)

(cid:88)l=1

(cid:33)


(cid:33)


w

(cid:107)

−

(cid:107)

w∗

= O

b(n) log

b(l)

a.s.

Comments.

(C1) Besides the learning steps a(n) and b(n), the convergence rate is governed by A22 for
the fast and ∆ for the slow iterate. ∆ in turn is affected by interaction effects which are
captured by A21 and A12 together with the inverse of A22.

A2.3 Equal Time-Scale Stochastic Approximation Algorithms

In this subsection we consider the case when the learning rates have equal time-scale.

A2.3.1 Equal Time-Scale for Saddle Point Iterates

If equal time-scales assumed then the iterates revisit inﬁnite often an environment of the solution
[61]. In Zhang 2007, the functions of the iterates are the derivatives of a Lagrangian with respect to
the dual and primal variables [61]. The iterates are

θn+1 = θn + a(n)

h

θn, wn

+ Z(θ)

n + M (θ)
n

,

wn+1 = wn + a(n)

(cid:16)

(cid:0)
g

(cid:1)
θn, wn

+ Z(w)

(cid:17)
n + M (w)

n

.

with the increasing σ-ﬁelds

Fn = σ(θl, wl, M (θ)

l

, Z(w)
l

, l (cid:54) n), n (cid:62) 0 .

The terms Z(θ)

n and Z(w)

n

subsum biased estimation errors.

(cid:16)

(cid:0)
, M (w)
l

(cid:1)
, Z(θ)
l

(cid:17)

Assumptions. We make the following assumptions:

(A1) Assumptions on update function: h and g are continuous, differentiable, and bounded. The

Jacobians

∂g
∂w

and

∂h
∂θ

are Hurwitz. A matrix is Hurwitz if the real part of each eigenvalue is strictly negative. This
assumptions corresponds to the assumption in [61] that the Lagrangian is concave in w and
convex in θ.

(A2) Assumptions on noise:

and

M (θ)
n
{
Fn. Furthermore they are mutually independent.
Bounded second moment:

M (w)
n
{

}

}

are a martingale difference sequences w.r.t. the increasing σ-ﬁelds

(127)

(128)

(129)

(130)

(131)

(132)

(133)

(134)

(135)

M (θ)
(cid:107)
M (w)
(cid:107)

2
n+1(cid:107)
2
n+1(cid:107)

E

E

(cid:104)

(cid:104)

| Fn

| Fn

(cid:105)

(cid:105)

<

<

∞

∞

a.s. ,

a.s. .

27

(A3) Assumptions on the learning rate:

(A4) Assumption on the biased error:

Boundedness:

a(n) > 0

,

a(n)

0

,

a(n) =

,

→

n
(cid:88)

∞

n
(cid:88)

a2(n) <

.

∞

(136)

sup

lim
n

sup

lim
n

Z(θ)
n (cid:107)
(cid:107)
Z(w)
n (cid:107)

(cid:107)

(cid:54) α(θ) a.s.

(cid:54) α(w) a.s.

(137)

(138)

Theorem. Deﬁne the “contraction region” Aη as follows:

Aη =

(θ, w) : α(θ) (cid:62) η
{

h(θ, w)
(cid:107)

(cid:107)

or α(w) (cid:62) η

g(θ, w)

(cid:107)

, 0 (cid:54) η < 1
}
(cid:107)

.

(139)

Theorem 10 (Zhang). Under above assumptions the iterates return to Aη inﬁnitely often with
probability one (a.s.).

Comments.

(C1) The proof of the theorem in [61] does not use the saddle point condition and not the fact

that the functions of the iterates are derivatives of the same function.

(C2) For the unbiased case, Zhang showed in Theorem 3.1 of [61] that the iterates converge.
However, he used the saddle point condition of the Lagrangian. He considered iterates
with functions that are the derivatives of a Lagrangian with respect to the dual and primal
variables [61].

A2.3.2 Equal Time Step for Actor-Critic Method

If equal time-scales assumed then the iterates revisit inﬁnite often an environment of the solution of
DiCastro & Meir [14]. The iterates of DiCastro & Meir are derived for actor-critic learning.

To present the actor-critic update iterates, we have to deﬁne some functions and terms. µ(u
the policy function parametrized by θ
chain given by P(y
each state x the agent receives a reward r(x).

x, θ) is
. A Markov
x, u) gives the next observation y using the observation x and the action u. In

Rm with observations x

and actions u

∈ X

∈ U

∈

|

|

The average reward per stage is for the recurrent state x∗:

˜η(θ) = lim
→∞

T

E

(cid:34)

r(xn)

x0 = x∗, θ

.

|

(cid:35)

1
T

T

1

−

n=0
(cid:88)

The estimate of ˜η is denoted by η.

The differential value function is

˜h(x, θ) = E

(r(xn)

˜η(θ))

x0 = x, θ

.

−

|

(cid:35)

T

1

−

(cid:34)

n=0
(cid:88)

˜d(x, y, θ) = r(x)

˜η(θ) + ˜h(y, θ)

˜h(x, θ) .

−

−

The temporal difference is

The estimate of ˜d is denoted by d.
The likelihood ratio derivative Ψ

Rm is

∈
Ψ(x, u, θ) = ∇θµ(u
µ(u
|

x, θ)

|
x, θ)

.

28

(140)

(141)

(142)

(143)

(144)

(145)

(146)

(147)

(148)

(149)

(150)

(151)
(152)
(153)
(154)

(155)

The value function ˜h is approximated by

where φ(x)

Rk. We deﬁne Φ

∈

h(x, w) = φ(x)T w ,

k

R|X |×
∈
φ1(x1)
φ1(x2)
...
φ1(x

|X |

φ2(x1)
φ2(x2)
...
) φ2(x

)

|X |

Φ = 





. . . φk(x1)
. . . φk(x2)

...
. . . φk(x

)

|X |







and

For TD(λ) we have an eligibility trace:

h(w) = Φ w .

en = λ en

1 + φ(xn) .

−

We deﬁne the approximation error with optimal parameter w∗(θ):

−
where π(θ) is an projection operator into the span of Φw. We bound this error by

Rk (cid:107)

−

Φ w

(cid:107)π(θ) =

˜h(θ)
(cid:107)

Φ w∗(θ)

(cid:107)π(θ) ,

(cid:15)app(θ) = inf
∈

w

˜h(θ)

(cid:15)app = sup
Rk

θ

(cid:15)app(θ) .

∈
We denoted by ˜η, ˜d, and ˜h the exact functions and used for their approximation η, d, and h,
respectively. We have learning rate adjustments Γη and Γw for the critic.

The update rules are:
Critic:

ηn+1 = ηn + a(n) Γη (r(xn)

ηn) ,

−

h(x, wn) = φ(x)T wn ,

d(xn, xn+1, wn) = r(xn)

ηn + h(xn+1, wn)

h(xn, wn) ,

en = λ en

−
1 + φ(xn) ,
wn+1 = wn + a(n) Γw d(xn, xn+1, wn) en .

−

−

Actor:

θn+1 = θn + a(n) Ψ(xn, un, θn) d(xn, xn+1, wn) .

Assumptions. We make the following assumptions:

(A1) Assumption on rewards:

The rewards

r(x)
{
(A2) Assumption on the Markov chain:

x
}

∈X

are uniformly bounded by a ﬁnite constant Br.

Each Markov chain for each θ is aperiodic, recurrent, and irreducible.

(A3) Assumptions on the policy function:

The conditional probability function µ(u
exist positive constants, Bµ1 and Bµ2, such that for all x
1 (cid:54) l1, l2 (cid:54) m we have
∂µ(u

∂2µ(u

x, θ)

|

x, θ)

x, θ) is twice differentiable. Moreover, there
Rm and
, u

, θ

∈ X

∈ U

∈

|
∂θl

, and θ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
, u
∈ U

(cid:54) Bµ2 .

(156)

(cid:54) Bµ1 ,

|
∂θl1 ∂θl2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
Rm, there exists a positive constant BΨ, such that
∈
Ψ(x, u, θ)
(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:107)2 (cid:54) BΨ <

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∞

,

(157)

For all x

∈ X

(A4) Assumption on the likelihood ratio derivative:

where

.
(cid:107)

(cid:107)2 is the Euclidean L2 norm.

29

(A5) Assumptions on the approximation space given by Φ:

The columns of the matrix Φ are independent, that is, the form a basis of dimension k. The
φl(cid:107)2 (cid:54) 1
norms of the columns vectors of the matrix Φ are bounded above by 1, that is,
(cid:107)
for 1 (cid:54) l (cid:54) k.

(A6) Assumptions on the learning rate:

a(n) =

,

∞

a2(n) <

.

∞

n
(cid:88)

n
(cid:88)

(158)

Theorem. The algorithm converged if
where the updates are zero. We assume that
point.

∇θ ˜η(θ) = 0, since the actor reached a stationary point
(cid:107)∇θ ˜η(θ)
hints at how close we are to the convergence
(cid:107)

The next theorem from DiCastro & Meir [14] implies that the trajectory visits a neighborhood of
a local maximum inﬁnitely often. Although it may leave the local vicinity of the maximum, it is
guaranteed to return to it inﬁnitely often.
Theorem 11 (DiCastro & Meir). Deﬁne

B

˜η =

∇

B∆td1
Γw

+

B∆td2
Γη

+ B∆td3 (cid:15)app ,

(159)

where B∆td1, B∆td2, and B∆td3 are ﬁnite constants depending on the Markov decision process and
the agent parameters.

Under above assumptions

lim
t
→∞
The trajectory visits a neighborhood of a local maximum inﬁnitely often.

(cid:107)∇θ ˜η(θt)
(cid:107)

inf

˜η .

∇

(cid:54) B

(160)

Comments.

maximum.

(C1) The larger the critic learning rates Γw and Γη are, the smaller is the region around the local

(C2) The results are in agreement with those of Zhang 2007 [61].
(C3) Even if the results are derived for a special actor-critic setting, they carry over to a more

general setting of the iterates.

A3 ADAM Optimization as Stochastic Heavy Ball with Friction

The Nesterov Accelerated Gradient Descent (NAGD) [47] has raised considerable interest due to its
numerical simplicity and its low complexity. Previous to NAGD and its derived methods there was
Polyak’s Heavy Ball method [49]. The idea of the Heavy Ball is a ball that evolves over the graph of
a function f with damping (due to friction) and acceleration. Therefore, this second-order dynamical
system can be described by the ODE for the Heavy Ball with Friction (HBF) [17]:
¨θt + a(t) ˙θt +

f (θt) = 0 ,
∇
where a(n) is the damping coefﬁcient with a(n) = a
nβ for β
integro-differential equation

∈

(161)
(0, 1]. This ODE is equivalent to the

˙θt =

1
k(t)

−

t

h(s)

f (θs)ds ,

∇

(162)

0
(cid:90)
where k and h are two memory functions related to a(t). For polynomially memoried HBF we have
k(t) = tα+1 and h(t) = (α + 1)tα for some positive α, and for exponentially memoried HBF we
have k(t) = λ exp(λ t) and h(t) = exp(λ t). For the sum of the learning rates, we obtain
ln(n) + γ + 1
n1−β
β
1

for β = 1
for β < 1

2n + O

a(l) = a

(163)

1
n2

n

,

(cid:40)

(cid:0)

(cid:1)

(cid:88)l=1

−

30

(164)

(165)

(166)

(167)

(168)

(169)

(170)

where γ = 0.5772156649 is the Euler-Mascheroni constant.

Gadat et al. derived a discrete and stochastic version of the HBF [17]:

θn+1 = θn −
mn+1 = mn + a(n + 1) r(n)

a(n + 1) mn

where

f (θn)

mn

+ a(n + 1) r(n) Mn+1 ,

∇

(cid:0)

−

(cid:1)

r(n) =

r

(cid:80)n

r
l=1 a(l)

(cid:40)

for exponentially memoried HBF
for polynomially memoried HBF

.

This recursion can be rewritten as
θn+1 = θn −
mn+1 =
−

a(n + 1) mn
a(n + 1) r(n)

1

∇
The recursion Eq. (166) is the ﬁrst moment update of ADAM [29].

(cid:1)

(cid:0)

(cid:0)

(cid:1)

mn + a(n + 1) r(n)

f (θn) + Mn+1

.

For the term r(n)a(n) we obtain for the polynomial memory the approximations

r(n) a(n)

r

≈

1
n log n
1

β

(cid:40)

−
n

for β = 1
for β < 1

,

Gadat et al. showed that the recursion Eq. (164) converges for functions with at most quadratic grow
[17]. The authors mention that convergence can be proofed for functions f that are L-smooth, that is,
the gradient is L-Lipschitz.

Kingma et al. [29] state in Theorem 4.1 convergence of ADAM while assuming that β1, the ﬁrst
moment running average coefﬁcient, decays exponentially. Furthermore they assume that β2
< 1
1
√β2
and the learning rate αt decays with αt = α
√t .

ADAM divides mn of the recursion Eq. (166) by the bias-corrected second raw moment estimate.
Since the bias-corrected second raw moment estimate changes slowly, we consider it as an error.

1

√v + ∆v ≈

1
√v −

1
2 v √v

∆v + O(∆v2) .

ADAM assumes the second moment E

g2

to be stationary with its approximation vn:

(cid:2)
vn =

(cid:3)
1
1

−
−

β2
βn
2

n

(cid:88)l=1

βn
2

l

−

g2
l .

∆nvn = vn −

vn

1 =

−

n

(cid:88)l=1
β2)

β2
βn
2

1
1

−
−
β2 (1
1

−
βn
2

−

βn
2

l
−

g2
l −

n

1

−

1

1

−

β2
βn
2

−

1

−

βn
2

l

1

−

−

g2
l

(171)

(cid:88)l=1
β2
βn
2

−

−

1

1

1

−

n

1

−

(cid:88)l=1

βn
2

l
−

−

1

g2
l

=

=

=

1
1

1
1

1
1

−
−

−
−

−
−

β2
βn
2

β2
βn

β2
βn

g2
n +

βn
2

l

1

−

−

g2
l −

g2
n +

β2 −

2 (cid:32)

g2
n −

2 (cid:32)

1

(cid:0)

1

−

β2
βn
2

−

1

−

1

1

−

βn
2
βn
2

−

1

−

n

1

−

βn
2

l

1

−

−

g2
l

(cid:33)

(cid:88)l=1
1

−

(cid:1)
βn
2

−

l

g2
l

.

(cid:33)

n

1

−

(cid:88)l=1

n

1

−

(cid:88)l=1

31

Therefore

E [∆nvn] = E [vn −
β2
βn
2

1
1

=

−
−

vn

1] =

−

1
1

−
−

β2
βn

2 (cid:32)

E

g2

E

g2

−

(cid:0)

(cid:2)

(cid:3)

(cid:2)

(cid:3)(cid:1)

E

g2

(cid:3)

(cid:2)
= 0 .

1

−

β2
βn
2

−

1

−

−

1

n

1

−

(cid:88)l=1

βn
2

l

1

−

−

E

g2

(172)

(cid:33)
(cid:3)

(cid:2)

We are interested in the difference of actual stochastic vn to the true stationary v:

∆vn = vn −

v =

1
1

−
−

β2
βn
2

n

(cid:88)l=1

βn
2

l

−

g2
l −

v

.

(cid:0)

(cid:1)

(173)

αa(n + 1)r(n), we have ∆vn ∝

1/(2v√v)∆vn + O(∆2vn). If we set M (v)
mn/√v + a(n + 1)r(n)M (v)
n+1 and E

For a stationary second moment of mn and β2 = 1
a(n +
1)r(n). We use a linear approximation to ADAM’s second moment normalization 1/√v + ∆vn ≈
(mn∆vn)/(2v√va(n + 1)r(n)), then
1/√v
−
g2
mn/√vn ≈
= 0. For a
l −
stationary second moment of mn,
M (v)
second moment. Therefore
{
factor 1/√v can be incorporated into a(n + 1) and r(n).

is a martingale difference sequence with a bounded
in update rules Eq. (166). The

M (v)
n
{
can be subsumed into
n+1}

(cid:105)
Mn+1}

= 0, since E

−
M (v)
n+1

n+1 =

−

}

{

v

(cid:104)

(cid:3)

(cid:2)

A4 Experiments: Additional Information

A4.1 WGAN-GP on Image Data.

Table A2: The performance of WGAN-GP trained with the original procedure and with TTUR on
CIFAR-10 and LSUN Bedrooms. We compare the performance with respect to the FID at the optimal
number of iterations during training and wall-clock time in minutes.

dataset

method

b, a

iter

time(m)

FID method

b = a

time(m)

CIFAR-10 TTUR
TTUR
LSUN

3e-4, 1e-4
3e-4, 1e-4

168k
80k

700
1900

24.8
9.5

orig
orig

1e-4
1e-4

iter

53k
23k

800
2010

FID

29.3
20.5

32

A4.2 WGAN-GP on the One Billion Word Benchmark.

Table A3: Samples generated by WGAN-GP trained on fhe One Billion Word benchmark with TTUR
(left) the original method (right).

Dry Hall Sitning tven the concer
There are court phinchs hasffort
He scores a supponied foutver il
Bartfol reportings ane the depor
Seu hid , it ’s watter ’s remold
Later fasted the store the inste
Indiwezal deducated belenseous K
Starfers on Rbama ’s all is lead
Inverdick oper , caldawho ’s non
She said , five by theically rec
RichI , Learly said remain .‘‘‘‘
Reforded live for they were like
The plane was git finally fuels
The skip lifely will neek by the
SEW McHardy Berfect was luadingu
But I pol rated Franclezt is the

No say that tent Franstal at Bra
Caulh Paphionars tven got corfle
Resumaly , braaky facting he at
On toipe also houd , aid of sole
When Barrysels commono toprel to
The Moster suprr tent Elay diccu
The new vebators are demases to
Many ’s lore wockerssaow 2 2 ) A
Andly , has le wordd Uold steali
But be the firmoters is no 200 s
Jermueciored a noval wan ’t mar
Onles that his boud-park , the g
ISLUN , The crather wilh a them
Fow 22o2 surgeedeto , theirestra
Make Sebages of intarmamates , a
Gullla " has cautaria Thoug ly t

Table A4: The performance of WGAN-GP trained with the original procedure and with TTUR on the
One Billion Word Benchmark. We compare the performance with respect to the JSD at the optimal
number of iterations and wall-clock time in minutes during training. WGAN-GP trained with TTUR
exhibits consistently a better FID.

n-gram method

b, a

iter

time(m)

JSD method

b = a

time(m)

4-gram TTUR
6-gram TTUR

3e-4, 1e-4
3e-4, 1e-4

98k
100k

1150
1120

0.35
0.74

orig
orig

1e-4
1e-4

iter

33k
32k

JSD

0.38
0.77

1040
1070

A4.3 BEGAN

The Boundary Equilibrium GAN (BEGAN) [6] maintains an equilibrium between the discriminator
and generator loss (cf. Section 3.3 in [6])

which, in turn, also leads to a ﬁxed relation between the two gradients, therefore, a two time-scale
update is not ensured by solely adjusting the learning rates. Indeed, for stable learning rates, we see
no differences in the learning progress between orig and TTUR as depicted in Figure A13.

E[

(G(z))] = γE[

(x)]

L

L

(174)

Figure A13: Mean, maximum and minimum FID over eight runs for BEGAN training on CelebA
and LSUN Bedrooms. TTUR learning rates are given as pairs (b, a) of discriminator learning rate
b and generator learning rate a: “TTUR b a”. Left: CelebA, starting at mini-batch 10k for better
visualisation. Right: LSUN Bedrooms. Orig and TTUR behave similar. For BEGAN we cannot
ensure TTUR by adjusting learning rates.

33

A5 Discriminator vs. Generator Learning Rate

The convergence proof for learning GANs with TTUR assumes that the generator learning rate will
eventually become small enough to ensure convergence of the discriminator learning. At some time
point, the perturbations of the discriminator updates by updates of the generator parameters are
sufﬁcient small to assure that the discriminator converges. Crucial for discriminator convergence is
the magnitude of the perturbations which the generator induces into the discriminator updates. These
perturbations are not only determined by the generator learning rate but also by its loss function,
current value of the loss function, optimization method, size of the error signals that reach the
generator (vanishing or exploding gradient), complexity of generator’s learning task, architecture of
the generator, regularization, and others. Consequently, the size of generator learning rate does not
solely determine how large the perturbations of the discriminator updates are but serve to modulate
them. Thus, the generator learning rate may be much larger than the discriminator learning rate
without inducing large perturbation into the discriminator learning.

Even the learning dynamics of the generator is different from the learning dynamics of the discrimi-
nator, though they both have the same learning rate. Figure A14 shows the loss of the generator and
the discriminator for an experiment with DCGAN on CelebA, where the learning rate was 0.0005
for both the discriminator and the generator. However, the discriminator loss is decreasing while
the generator loss is increasing. This example shows that the learning rate neither determines the
perturbations nor the progress in learning for two coupled update rules. The choice of the learning
rate for the generator should be independent from choice for the discriminator. Also the search ranges
of discriminator and generator learning rates should be independent from each other, but adjusted to
the corresponding architecture, task, etc.

Figure A14: The respective losses of the discriminator and the generator show the different learning
dynamics of the two networks.

A6 Used Software, Datasets, Pretrained Models, and Implementations

We used the following datasets to evaluate GANs: The Large-scale CelebFaces Attributes (CelebA)
dataset, aligned and cropped [41], the training dataset of the bedrooms category of the large scale
image database (LSUN) [60], the CIFAR-10 training dataset [34], the Street View House Numbers
training dataset (SVHN) [48], and the One Billion Word Benchmark [12].

All experiments rely on the respective reference implementations for the corresponding GAN model.
The software framework for our experiments was Tensorﬂow 1.3 [1, 2] and Python 3.6. We used
following software, datasets and pretrained models:

•

BEGAN in Tensorﬂow, https://github.com/carpedm20/BEGAN-tensorflow, Fixed
random seeds removed. Accessed: 2017-05-30

34

•

•

•

•

•

DCGAN in Tensorﬂow, https://github.com/carpedm20/DCGAN-tensorflow, Fixed
random seeds removed. Accessed: 2017-04-03
Improved Training of Wasserstein GANs, image model, https://github.com/igul222/
improved_wgan_training/blob/master/gan_64x64.py, Accessed: 2017-06-12
language model, https://github.com/
Improved Training of Wasserstein GANs,
igul222/improved_wgan_training/blob/master/gan_language.py, Accessed:
2017-06-12
Inception-v3
imagenet/inception-2015-12-05.tgz, Accessed: 2017-05-02

http://download.tensorflow.org/models/image/

pretrained,

Implementations are available at

https://github.com/bioinf-jku/TTUR

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis,
J. Dean, M. Devin, S. Ghemawat, I. J. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Józefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. G.
Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. A. Tucker,
V. Vanhoucke, V. Vasudevan, F. B. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke,
Y. Yu, and X. Zheng. Tensorﬂow: Large-scale machine learning on heterogeneous distributed
systems. arXiv e-prints, arXiv:1603.04467, 2016.

[2] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving,
M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker,
V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng. Tensorﬂow: A system for large-
In 12th USENIX Symposium on Operating Systems Design and
scale machine learning.
Implementation (OSDI 16), pages 265–283, 2016.

[3] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein GAN. arXiv e-prints, arXiv:1701.07875,

2017.

[4] S. Arora, R. Ge, Y. Liang, T. Ma, and Y. Zhang. Generalization and equilibrium in generative
In D. Precup and Y. W. Teh, editors, Proceedings of the 34th
adversarial nets (GANs).
International Conference on Machine Learning, Proceedings of Machine Learning Research,
vol. 70, pages 224–232, 2017.

[5] H. Attouch, X. Goudou, and P. Redont. The heavy ball with friction method, I. the continu-
ous dynamical system: Global exploration of the local minima of a real-valued function by
asymptotic analysis of a dissipative dynamical system. Communications in Contemporary
Mathematics, 2(1):1–34, 2000.

[6] D. Berthelot, T. Schumm, and L. Metz. BEGAN: Boundary equilibrium generative adversarial

networks. arXiv e-prints, arXiv:1703.10717, 2017.

[7] D. P. Bertsekas and J. N. Tsitsiklis. Gradient convergence in gradient methods with errors.

SIAM Journal on Optimization, 10(3):627–642, 2000.

[8] S. Bhatnagar, H. L. Prasad, and L. A. Prashanth. Stochastic Recursive Algorithms for Optimiza-
tion. Lecture Notes in Control and Information Sciences. Springer-Verlag London, 2013.
[9] V. S. Borkar. Stochastic approximation with two time scales. Systems & Control Letters,

29(5):291–294, 1997.

[10] V. S. Borkar and S. P. Meyn. The O.D.E. method for convergence of stochastic approximation
and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447–469, 2000.
[11] T. Che, Y. Li, A. P. Jacob, Y. Bengio, and W. Li. Mode regularized generative adversarial
networks. In Proceedings of the International Conference on Learning Representations (ICLR),
2017. arXiv:1612.02136.

[12] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson. One billion
word benchmark for measuring progress in statistical language modeling. arXiv e-prints,
arXiv:1312.3005, 2013.

35

[13] D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by
exponential linear units (ELUs). In Proceedings of the International Conference on Learning
Representations (ICLR), 2016. arXiv:1511.07289.

[14] D. DiCastro and R. Meir. A convergent online single time scale actor critic algorithm. J. Mach.

Learn. Res., 11:367–410, 2010.

[15] D. C. Dowson and B. V. Landau. The Fréchet distance between multivariate normal distributions.

Journal of Multivariate Analysis, 12:450–455, 1982.

[16] M. Fréchet. Sur la distance de deux lois de probabilité. C. R. Acad. Sci. Paris, 244:689–692,

[17] S. Gadat, F. Panloup, and S. Saadane. Stochastic heavy ball. arXiv e-prints, arXiv:1609.04228,

1957.

2016.

[18] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems
27, pages 2672–2680, 2014.

[19] I. J. Goodfellow. On distinguishability criteria for estimating generative models. In Workshop
at the International Conference on Learning Representations (ICLR), 2015. arXiv:1412.6515.

[20] I. J. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. arXiv e-prints,

arXiv:1701.00160, 2017.

[21] X. Goudou and J. Munier. The gradient and heavy ball with friction dynamical systems: the

quasiconvex case. Mathematical Programming, 116(1):173–191, 2009.

[22] P. Grnarova, K. Y. Levy, A. Lucchi, T. Hofmann, and A. Krause. An online learning approach

to generative adversarial networks. arXiv e-prints, arXiv:1706.03269, 2017.

[23] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of
Wasserstein GANs. arXiv e-prints, arXiv:1704.00028, 2017. Advances in Neural Information
Processing Systems 31 (NIPS 2017).

[24] M. W. Hirsch. Convergent activation dynamics in continuous time networks. Neural Networks,

2(5):331–349, 1989.

[25] R. D. Hjelm, A. P. Jacob, T. Che, K. Cho, and Y. Bengio. Boundary-seeking generative

adversarial networks. arXiv e-prints, arXiv:1702.08431, 2017.

[26] S. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.

[27] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional
adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2017. arXiv:1611.07004.

[28] P. Karmakar and S. Bhatnagar. Two time-scale stochastic approximation with controlled Markov
noise and off-policy temporal-difference learning. Mathematics of Operations Research, 2017.

[29] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In Proceedings of the
International Conference on Learning Representations (ICLR)), 2015. arXiv:1412.6980.

[30] V. R. Konda. Actor-Critic Algorithms. PhD thesis, Department of Electrical Engineering and

Computer Science, Massachusetts Institute of Technology, 2002.

[31] V. R. Konda and V. S. Borkar. Actor-critic-type learning algorithms for Markov decision

processes. SIAM J. Control Optim., 38(1):94–123, 1999.

[32] V. R. Konda and J. N. Tsitsiklis. Linear stochastic approximation driven by slowly varying

Markov chains. Systems & Control Letters, 50(2):95–102, 2003.

[33] V. R. Konda and J. N. Tsitsiklis. Convergence rate of linear two-time-scale stochastic approxi-

mation. The Annals of Applied Probability, 14(2):796–819, 2004.

[34] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classiﬁcation with deep convolutional
neural networks. In Proceedings of the 25th International Conference on Neural Information
Processing Systems, pages 1097–1105, 2012.

[35] H. J. Kushner and G. G. Yin. Stochastic Approximation Algorithms and Recursive Algorithms

and Applications. Springer-Verlag New York, second edition, 2003.

36

[36] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. P. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi.
Photo-realistic single image super-resolution using a generative adversarial network. arXiv
e-prints, arXiv:1609.04802, 2016.

[37] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Póczos. MMD GAN: Towards deeper
understanding of moment matching network. In Advances in Neural Information Processing
Systems 31 (NIPS 2017), 2017. arXiv:1705.08584.

[38] J. Li, A. Madry, J. Peebles, and L. Schmidt. Towards understanding the dynamics of generative

adversarial networks. arXiv e-prints, arXiv:1706.09884, 2017.

[39] J. H. Lim and J. C. Ye. Geometric GAN. arXiv e-prints, arXiv:1705.02894, 2017.

[40] S. Liu, O. Bousquet, and K. Chaudhuri. Approximation and convergence properties of generative
adversarial learning. In Advances in Neural Information Processing Systems 31 (NIPS 2017),
2017. arXiv:1705.08991.

[41] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings

of International Conference on Computer Vision (ICCV), 2015.

[42] L. M. Mescheder, S. Nowozin, and A. Geiger. The numerics of GANs. In Advances in Neural

Information Processing Systems 31 (NIPS 2017), 2017. arXiv:1705.10461.

[43] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks.
In Proceedings of the International Conference on Learning Representations (ICLR), 2017.
arXiv:1611.02163.

[44] A. Mokkadem and M. Pelletier. Convergence rate and averaging of nonlinear two-time-scale
stochastic approximation algorithms. The Annals of Applied Probability, 16(3):1671–1702,
2006.

[45] Y. Mroueh and T. Sercu. Fisher GAN. In Advances in Neural Information Processing Systems

31 (NIPS 2017), 2017. arXiv:1705.09675.

[46] V. Nagarajan and J. Z. Kolter. Gradient descent GAN optimization is locally stable. arXiv
e-prints, arXiv:1706.04156, 2017. Advances in Neural Information Processing Systems 31
(NIPS 2017).

[47] Y. Nesterov. A method of solving a convex programming problem with convergence rate

o(1/k2). Soviet Mathematics Doklady, 27:372–376, 1983.

[48] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in nat-
ural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and
Unsupervised Feature Learning 2011, 2011.

[49] B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR

Computational Mathematics and Mathematical Physics, 4(5):1–17, 1964.

[50] H. L. Prasad, L. A. Prashanth, and S. Bhatnagar. Two-timescale algorithms for learning Nash
equilibria in general-sum stochastic games. In Proceedings of the 2015 International Conference
on Autonomous Agents and Multiagent Systems (AAMAS ’15), pages 1371–1379, 2015.

[51] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convo-
lutional generative adversarial networks. In Proceedings of the International Conference on
Learning Representations (ICLR), 2016. arXiv:1511.06434.

[52] A. Ramaswamy and S. Bhatnagar. Stochastic recursive inclusion in two timescales with an

application to the lagrangian dual problem. Stochastics, 88(8):1173–1187, 2016.

[53] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved
techniques for training GANs. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 2234–2242,
2016.

[54] V. B. Tadi´c. Almost sure convergence of two time-scale stochastic approximation algorithms.
In Proceedings of the 2004 American Control Conference, volume 4, pages 3802–3807, 2004.

[55] L. Theis, A. van den Oord, and M. Bethge. A note on the evaluation of generative models.
In Proceedings of the International Conference on Learning Representations (ICLR), 2016.
arXiv:1511.01844.

37

[56] I. Tolstikhin, S. Gelly, O. Bousquet, C.-J. Simon-Gabriel, and B. Schölkopf. AdaGAN: Boosting
generative models. arXiv e-prints, arXiv:1701.02386, 2017. Advances in Neural Information
Processing Systems 31 (NIPS 2017).

[57] R. Wang, A. Cully, H. J. Chang, and Y. Demiris. MAGAN: margin adaptation for generative

adversarial networks. arXiv e-prints, arXiv:1704.03817, 2017.

[58] L. N. Wasserstein. Markov processes over denumerable products of spaces describing large

systems of automata. Probl. Inform. Transmission, 5:47–52, 1969.

[59] Y. Wu, Y. Burda, R. Salakhutdinov, and R. B. Grosse. On the quantitative analysis of decoder-
based generative models. In Proceedings of the International Conference on Learning Repre-
sentations (ICLR), 2017. arXiv:1611.04273.

[60] F. Yu, Y. Zhang, S. Song, A. Seff, and J. Xiao. LSUN: construction of a large-scale image
dataset using deep learning with humans in the loop. arXiv e-prints, arXiv:1506.03365, 2015.
[61] J. Zhang, D. Zheng, and M. Chiang. The impact of stochastic noisy feedback on distributed
network utility maximization. In IEEE INFOCOM 2007 - 26th IEEE International Conference
on Computer Communications, pages 222–230, 2007.

List of Figures

Oscillation in GAN training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Heavy Ball with Friction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
. . . . . . . . . . . . . . . . . . . . . . .
FID evaluated for different disturbances
3
. . . . . . . . . . . . . . . . . .
TTUR and single time-scale update with toy data.
4
. . . . . .
FID for DCGAN on CelebA, CIFAR-10, SVHN, and LSUN Bedrooms.
5
FID for WGAN-GP trained on CIFAR-10 and LSUN Bedrooms.
. . . . . . . . . .
6
Performance of WGAN-GP on One Billion Word. . . . . . . . . . . . . . . . . . .
7
A8 FID and Inception Score Comparison . . . . . . . . . . . . . . . . . . . . . . . . .
A9 CelebA Samples with FID 500 and 300 . . . . . . . . . . . . . . . . . . . . . . . .
A10 CelebA Samples with FID 133 and 100 . . . . . . . . . . . . . . . . . . . . . . . .
A11 CelebA Samples with FID 45 and 13 . . . . . . . . . . . . . . . . . . . . . . . . .
A12 CelebA Samples with FID 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A13 FID for BEGAN trained on CelebA and LSUN Bedrooms.
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
A14 Learning dynamics of two networks.

List of Tables

1 Results DCGAN and WGAN-GP . . . . . . . . . . . . . . . . . . . . . . . . . . .
A2 Results WGAN-GP on Image Data
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
A3 Samples of the One Billion Word benchmark generated by WGAN-GP.
A4 Results WGAN-GP on One Billion Word . . . . . . . . . . . . . . . . . . . . . . .

2
4
6
7
8
8
9
13
14
14
15
15
33
34

9
32
33
33

38

8
1
0
2
 
n
a
J
 
2
1
 
 
]

G
L
.
s
c
[
 
 
6
v
0
0
5
8
0
.
6
0
7
1
:
v
i
X
r
a

GANs Trained by a Two Time-Scale Update Rule
Converge to a Local Nash Equilibrium

Martin Heusel

Hubert Ramsauer

Thomas Unterthiner

Bernhard Nessler

Sepp Hochreiter

LIT AI Lab & Institute of Bioinformatics,
Johannes Kepler University Linz
A-4040 Linz, Austria
{mhe,ramsauer,unterthiner,nessler,hochreit}@bioinf.jku.at

Abstract

Generative Adversarial Networks (GANs) excel at creating realistic images with
complex models for which maximum likelihood is infeasible. However, the con-
vergence of GAN training has still not been proved. We propose a two time-scale
update rule (TTUR) for training GANs with stochastic gradient descent on ar-
bitrary GAN loss functions. TTUR has an individual learning rate for both the
discriminator and the generator. Using the theory of stochastic approximation, we
prove that the TTUR converges under mild assumptions to a stationary local Nash
equilibrium. The convergence carries over to the popular Adam optimization, for
which we prove that it follows the dynamics of a heavy ball with friction and thus
prefers ﬂat minima in the objective landscape. For the evaluation of the perfor-
mance of GANs at image generation, we introduce the ‘Fréchet Inception Distance”
(FID) which captures the similarity of generated images to real ones better than
the Inception Score. In experiments, TTUR improves learning for DCGANs and
Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN train-
ing on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word
Benchmark.

Introduction

Generative adversarial networks (GANs) [18] have achieved outstanding results in generating realistic
images [51, 36, 27, 3, 6] and producing text [23]. GANs can learn complex generative models for
which maximum likelihood or a variational approximations are infeasible. Instead of the likelihood,
a discriminator network serves as objective for the generative model, that is, the generator. GAN
learning is a game between the generator, which constructs synthetic data from random variables,
and the discriminator, which separates synthetic data from real world data. The generator’s goal is
to construct data in such a way that the discriminator cannot tell them apart from real world data.
Thus, the discriminator tries to minimize the synthetic-real discrimination error while the generator
tries to maximize this error. Since training GANs is a game and its solution is a Nash equilibrium,
gradient descent may fail to converge [53, 18, 20]. Only local Nash equilibria are found, because
gradient descent is a local optimization method. If there exists a local neighborhood around a point
in parameter space where neither the generator nor the discriminator can unilaterally decrease their
respective losses, then we call this point a local Nash equilibrium.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

Figure 1: Left: Original vs. TTUR GAN training on CelebA. Right: Figure from Zhang 2007 [61]
which shows the distance of the parameter from the optimum for a one time-scale update of a 4 node
network ﬂow problem. When the upper bounds on the errors (α, β) are small, the iterates oscillate
and repeatedly return to a neighborhood of the optimal solution (see also Appendix Section A2.3).
However, when the upper bounds on the errors are large, the iterates typically diverge.

To characterize the convergence properties of training general GANs is still an open challenge [19, 20].
For special GAN variants, convergence can be proved under certain assumptions [39, 22, 56]. A
prerequisit for many convergence proofs is local stability [35] which was shown for GANs by
Nagarajan and Kolter [46] for a min-max GAN setting. However, Nagarajan and Kolter require for
their proof either rather strong and unrealistic assumptions or a restriction to a linear discriminator.
Recent convergence proofs for GANs hold for expectations over training samples or for the number
of examples going to inﬁnity [37, 45, 40, 4], thus do not consider mini-batch learning which leads to
a stochastic gradient [57, 25, 42, 38].

Recently actor-critic learning has been analyzed using stochastic approximation. Prasad et al. [50]
showed that a two time-scale update rule ensures that training reaches a stationary local Nash
equilibrium if the critic learns faster than the actor. Convergence was proved via an ordinary
differential equation (ODE), whose stable limit points coincide with stationary local Nash equilibria.
We follow the same approach. We prove that GANs converge to a local Nash equilibrium when trained
by a two time-scale update rule (TTUR), i.e., when discriminator and generator have separate learning
rates. This also leads to better results in experiments. The main premise is that the discriminator
converges to a local minimum when the generator is ﬁxed. If the generator changes slowly enough,
then the discriminator still converges, since the generator perturbations are small. Besides ensuring
convergence, the performance may also improve since the discriminator must ﬁrst learn new patterns
before they are transferred to the generator. In contrast, a generator which is overly fast, drives the
discriminator steadily into new regions without capturing its gathered information. In recent GAN
implementations, the discriminator often learned faster than the generator. A new objective slowed
down the generator to prevent it from overtraining on the current discriminator [53]. The Wasserstein
GAN algorithm uses more update steps for the discriminator than for the generator [3]. We compare
TTUR and standard GAN training. Fig. 1 shows at the left panel a stochastic gradient example on
CelebA for original GAN training (orig), which often leads to oscillations, and the TTUR. On the
right panel an example of a 4 node network ﬂow problem of Zhang et al. [61] is shown. The distance
between the actual parameter and its optimum for an one time-scale update rule is shown across
iterates. When the upper bounds on the errors are small, the iterates return to a neighborhood of the
optimal solution, while for large errors the iterates may diverge (see also Appendix Section A2.3).

Our novel contributions in this paper are:

The two time-scale update rule for GANs,

•

•

•

•

•

We proof that GANs trained with TTUR converge to a stationary local Nash equilibrium,

The description of Adam as heavy ball with friction and the resulting second order differential
equation,

The convergence of GANs trained with TTUR and Adam to a stationary local Nash equilib-
rium,

We introduce the “Fréchet Inception Distance” (FID) to evaluate GANs, which is more
consistent than the Inception Score.

2

Two Time-Scale Update Rule for GANs

LG. The loss functions

We consider a discriminator D(.; w) with parameter vector w and a generator G(.; θ) with parameter
vector θ. Learning is based on a stochastic gradient ˜g(θ, w) of the discriminator’s loss function
LD
and a stochastic gradient ˜h(θ, w) of the generator’s loss function
LD and
LG can be the original as introduced in Goodfellow et al. [18], its improved versions [20], or recently
proposed losses for GANs like the Wasserstein GAN [3]. Our setting is not restricted to min-max
GANs, but is valid for all other, more general GANs for which the discriminator’s loss function
LD
θ, w
is not necessarily related to the generator’s loss function
are stochastic, since they use mini-batches of m real world samples x(i), 1 (cid:54) i (cid:54) m and m synthetic
(cid:0)
(cid:1)
samples z(i), 1 (cid:54) i (cid:54) m which are randomly chosen. If the true gradients are g(θ, w) =
∇wLD and
∇θLG, then we can deﬁne ˜g(θ, w) = g(θ, w) +M (w) and ˜h(θ, w) = h(θ, w) +M (θ)
h(θ, w) =
with random variables M (w) and M (θ). Thus, the gradients ˜g
are stochastic
approximations to the true gradients. Consequently, we analyze convergence of GANs by two
time-scale stochastic approximations algorithms. For a two time-scale update rule (TTUR), we use
the learning rates b(n) and a(n) for the discriminator and the generator update, respectively:

LG. The gradients ˜g

and ˜h

and ˜h

θ, w

θ, w

θ, w

(cid:1)

(cid:0)

(cid:0)

(cid:1)

(cid:1)

(cid:0)

wn+1 = wn + b(n)

g

θn, wn

+ M (w)

n

, θn+1 = θn + a(n)

h

θn, wn

+ M (θ)

n

.

(1)

For more details on the following convergence proof and its assumptions see Appendix Section A2.1.
To prove convergence of GANs learned by TTUR, we make the following assumptions (The actual
assumption is ended by (cid:74), the following text are just comments and explanations):

(cid:16)

(cid:0)

(cid:1)

(cid:17)

(cid:16)

(cid:0)

(cid:1)

(cid:17)

(A1) The gradients h and g are Lipschitz. (cid:74) Consequently, networks with Lipschitz smooth
activation functions like ELUs (α = 1) [13] fulﬁll the assumption but not ReLU networks.

,

∞

∞

(A2)

n b(n) =

n a(n) =

(cid:80)
w.r.t.

n a2(n) <
(A3) The stochastic gradient errors
(cid:80)
the increasing σ-ﬁeld
(θ)
n
| F

n b2(n) <
are martingale difference sequences
, l (cid:54) n), n (cid:62) 0 with
(cid:54) B2, where B1 and B2 are positive
2
E
deterministic constants.(cid:74) The original Assumption (A3) from Borkar 1997 follows from
Lemma 2 in [7] (see also [52]). The assumption is fulﬁlled in the Robbins-Monro setting,
where mini-batches are randomly sampled and the gradients are bounded.

,
M (θ)
(cid:80)
n
{
Fn = σ(θl, wl, M (θ)

,
∞
M (w)
(cid:80)
n
}
{

(cid:54) B1 and E

, a(n) = o(b(n))(cid:74)

M (θ)
n
(cid:107)

(w)
n
| F

, M (w)
l

M (w)
n

(cid:107)
(cid:104)

2
(cid:107)

and

∞

}

(cid:107)

(cid:104)

(cid:105)

(cid:105)

l

(A4) For each θ, the ODE ˙w(t) = g

θ, w(t)

(cid:0)

(cid:1)

(cid:0)

θ(t), λ(θ(t))

has a local asymptotically stable attractor
λ(θ) within a domain of attraction Gθ such that λ is Lipschitz. The ODE ˙θ(t) =
(cid:1)
h
has a local asymptotically stable attractor θ∗ within a domain of
attraction.(cid:74) The discriminator must converge to a minimum for ﬁxed generator param-
eters and the generator, in turn, must converge to a minimum for this ﬁxed discriminator
minimum. Borkar 1997 required unique global asymptotically stable equilibria [9]. The
assumption of global attractors was relaxed to local attractors via Assumption (A6) and
Theorem 2.7 in Karmakar & Bhatnagar [28]. See for more details Assumption (A6) in the
Appendix Section A2.1.3. Here, the GAN objectives may serve as Lyapunov functions.
These assumptions of locally stable ODEs can be ensured by an additional weight decay term
in the loss function which increases the eigenvalues of the Hessian. Therefore, problems
with a region-wise constant discriminator that has zero second order derivatives are avoided.
For further discussion see Appendix Section A2 (C3).

(A5) supn (cid:107)

θn(cid:107)
decay term.

<

and supn (cid:107)

wn(cid:107)

<

∞

∞

.(cid:74) Typically ensured by the objective or a weight

The next theorem has been proved in the seminal paper of Borkar 1997 [9].
Theorem 1 (Borkar). If the assumptions are satisﬁed, then the updates Eq. (1) converge to
(θ∗, λ(θ∗)) a.s.

The solution (θ∗, λ(θ∗)) is a stationary local Nash equilibrium [50], since θ∗ as well as λ(θ∗) are
θ∗, λ(θ∗)
local asymptotically stable attractors with g
= 0. An alternative
approach to the proof of convergence using the Poisson equation for ensuring a solution to the fast

= 0 and h

θ∗, λ(θ∗)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

3

update rule can be found in the Appendix Section A2.1.2. This approach assumes a linear update
function in the fast update rule which, however, can be a linear approximation to a nonlinear gradient
[30, 32]. For the rate of convergence see Appendix Section A2.2, where Section A2.2.1 focuses on
linear and Section A2.2.2 on non-linear updates. For equal time-scales it can only be proven that
the updates revisit an environment of the solution inﬁnitely often, which, however, can be very large
[61, 14]. For more details on the analysis of equal time-scales see Appendix Section A2.3. The main
idea of the proof of Borkar [9] is to use (T, δ) perturbed ODEs according to Hirsch 1989 [24] (see
also Appendix Section C of Bhatnagar, Prasad, & Prashanth 2013 [8]). The proof relies on the fact
that there eventually is a time point when the perturbation of the slow update rule is small enough
(given by δ) to allow the fast update rule to converge. For experiments with TTUR, we aim at ﬁnding
learning rates such that the slow update is small enough to allow the fast to converge. Typically,
the slow update is the generator and the fast update the discriminator. We have to adjust the two
learning rates such that the generator does not affect discriminator learning in a undesired way and
perturb it too much. However, even a larger learning rate for the generator than for the discriminator
may ensure that the discriminator has low perturbations. Learning rates cannot be translated directly
into perturbation since the perturbation of the discriminator by the generator is different from the
perturbation of the generator by the discriminator.

Adam Follows an HBF ODE and Ensures TTUR Convergence

In our experiments, we aim at using Adam stochastic approximation to avoid mode collapsing. GANs
suffer from “mode collapsing” where large masses of probability are mapped onto a few modes
that cover only small regions. While these regions represent meaningful samples, the variety of the
real world data is lost and only few prototype samples are
generated. Different methods have been proposed to avoid
mode collapsing [11, 43]. We obviate mode collapsing by
using Adam stochastic approximation [29]. Adam can be
described as Heavy Ball with Friction (HBF) (see below),
since it averages over past gradients. This averaging cor-
responds to a velocity that makes the generator resistant
to getting pushed into small regions. Adam as an HBF
method typically overshoots small local minima that cor-
respond to mode collapse and can ﬁnd ﬂat minima which
generalize well [26]. Fig. 2 depicts the dynamics of HBF,
where the ball settles at a ﬂat minimum. Next, we analyze
whether GANs trained with TTUR converge when using
Adam. For more details see Appendix Section A3.

Figure 2: Heavy Ball with Friction, where the
ball with mass overshoots the local minimum
θ+ and settles at the ﬂat minimum θ∗.

We recapitulate the Adam update rule at step n, with learning rate a, exponential averaging factors β1
for the ﬁrst and β2 for the second moment of the gradient

f (θn

1):

∇

−

(2)

gn ←− ∇
mn ←−
vn ←−
θn ←−

f (θn
(β1/(1
(β2/(1
θn

−

1)
βn
1 )) mn
1 + ((1
−
βn
2 )) vn
1 + ((1
−
a mn/(√vn + (cid:15)) ,

β1)/(1
−
β2)/(1
−

βn
1 )) gn
−
βn
2 )) gn (cid:12)
−

gn

τ for τ

−
−
1 −
−
, the square root √., and the
where following operations are meant componentwise: the product
division / in the last line. Instead of learning rate a, we introduce the damping coefﬁcient a(n) with
(0, 1]. Adam has parameters β1 for averaging the gradient and β2 parametrized
a(n) = an−
by a positive α for averaging the squared gradient. These parameters can be considered as deﬁning a
memory for Adam. To characterize β1 and β2 in the following, we deﬁne the exponential memory
n
r(n) = r and the polynomial memory r(n) = r/
l=1 a(l) for some positive constant r. The next
theorem describes Adam by a differential equation, which in turn allows to apply the idea of (T, δ)
(cid:80)
perturbed ODEs to TTUR. Consequently, learning GANs with TTUR and Adam converges.
Theorem 2. If Adam is used with β1 = 1
f
as the full gradient of the lower bounded, continuously differentiable objective f , then for stationary
second moments of the gradient, Adam follows the differential equation for Heavy Ball with Friction

αa(n + 1)r(n) and with

a(n + 1)r(n), β2 = 1

∇

(cid:12)

−

−

∈

4

(HBF):

Adam converges for gradients

∇

¨θt + a(t) ˙θt +
f that are L-Lipschitz.

∇

f (θt) = 0 .

(3)

Proof. Gadat et al. derived a discrete and stochastic version of Polyak’s Heavy Ball method [49], the
Heavy Ball with Friction (HBF) [17]:
θn+1 = θn −
mn+1 =
−

a(n + 1) mn ,
a(n + 1) r(n)

mn + a(n + 1) r(n)

f (θn) + Mn+1

(4)

∇

1

.

(cid:0)

(cid:0)

(cid:1)

These update rules are the ﬁrst moment update rules of Adam [29]. The HBF can be formulated as the
differential equation Eq. (3) [17]. Gadat et al. showed that the update rules Eq. (4) converge for loss
functions f with at most quadratic grow and stated that convergence can be proofed for
f that are
∇
L-Lipschitz [17]. Convergence has been proved for continuously differentiable f that is quasiconvex
f that is L-Lipschitz
(Theorem 3 in Goudou & Munier [21]). Convergence has been proved for
and bounded from below (Theorem 3.1 in Attouch et al. [5]). Adam normalizes the average mn by
gn]. mn is componentwise divided by
the second moments vn of of the gradient gn: vn = E [gn (cid:12)
the square root of the components of vn. We assume that the second moments of gn are stationary,
gn]. In this case the normalization can be considered as additional noise since the
i.e., v = E [gn (cid:12)
normalization factor randomly deviates from its mean. In the HBF interpretation the normalization
by √v corresponds to introducing gravitation. We obtain

∇

(cid:1)

n

1
1

β2
βn
2

n

1
1

β2
βn
2

l

βn
2

l

βn
2

−

−

−

(5)

v) .

(cid:88)l=1

(cid:88)l=1

−
−

v =

vn =

gl (cid:12)

gl −

(gl (cid:12)

gl , ∆vn = vn −

−
−
For a stationary second moment v and β2 = 1
αa(n + 1)r(n), we have ∆vn ∝
a(n + 1)r(n). We
use a componentwise linear approximation to Adam’s second moment normalization 1/√v + ∆vn ≈
∆vn + O(∆2vn), where all operations are meant componentwise. If
1/√v
−
we set M (v)
mn/√v + a(n +
∆vn)/(2v
1)r(n)M (v)
v] = 0. For a stationary second moment v,

(cid:12)
= 0, since E [gl (cid:12)
is a martingale difference sequence with a bounded second moment.
the random variable
in update rules Eq. (4). The factor 1/√v can
Mn+1}
Therefore
n+1}
{
be componentwise incorporated into the gradient g which corresponds to rescaling the parameters
without changing the minimum.

√v))
(cid:12)
(mn (cid:12)
M (v)
n+1
M (v)
(cid:104)
n
{
can be subsumed into

√va(n + 1)r(n)), then mn/√vn ≈

(1/(2v
(cid:12)
n+1 =
−
n+1 and E

M (v)
{

gl −

(cid:105)
}

2+f (θ(t))
According to Attouch et al. [5] the energy, that is, a Lyapunov function, is E(t) = 1/2
|
and ˙E(t) =
2 < 0. Since Adam can be expressed as differential equation and has a
|
Lyapunov function, the idea of (T, δ) perturbed ODEs [9, 24, 10] carries over to Adam. Therefore
the convergence of Adam with TTUR can be proved via two time-scale stochastic approximation
analysis like in Borkar [9] for stationary second moments of the gradient.

˙θ(t)
|

−

a

˙θ(t)
|

In the Appendix we further discuss the convergence of two time-scale stochastic approximation
algorithms with additive noise, linear update functions depending on Markov chains, nonlinear
update functions, and updates depending on controlled Markov processes. Futhermore, the Appendix
presents work on the rate of convergence for both linear and nonlinear update rules using similar
techniques as the local stability analysis of Nagarajan and Kolter [46]. Finally, we elaborate more on
equal time-scale updates, which are investigated for saddle point problems and actor-critic learning.

Experiments

Performance Measure. Before presenting the experiments, we introduce a quality measure for
models learned by GANs. The objective of generative learning is that the model produces data which
matches the observed data. Therefore, each distance between the probability of observing real world
data pw(.) and the probability of generating model data p(.) can serve as performance measure for
generative models. However, deﬁning appropriate performance measures for generative models

5

Figure 3: FID is evaluated for upper left: Gaussian noise, upper middle: Gaussian blur, upper
right: implanted black rectangles, lower left: swirled images, lower middle: salt and pepper noise,
and lower right: CelebA dataset contaminated by ImageNet images. The disturbance level rises
from zero and increases to the highest level. The FID captures the disturbance level very well by
monotonically increasing.

is difﬁcult [55]. The best known measure is the likelihood, which can be estimated by annealed
importance sampling [59]. However, the likelihood heavily depends on the noise assumptions for
the real data and can be dominated by single samples [55]. Other approaches like density estimates
have drawbacks, too [55]. A well-performing approach to measure the performance of GANs is the
“Inception Score” which correlates with human judgment [53]. Generated samples are fed into an
inception model that was trained on ImageNet. Images with meaningful objects are supposed to
have low label (output) entropy, that is, they belong to few object classes. On the other hand, the
entropy across images should be high, that is, the variance over the images should be large. Drawback
of the Inception Score is that the statistics of real world samples are not used and compared to the
statistics of synthetic samples. Next, we improve the Inception Score. The equality p(.) = pw(.)
pw(.)f (x)dx for a basis f (.)
holds except for a non-measurable set if and only if
spanning the function space in which p(.) and pw(.) live. These equalities of expectations are used
to describe distributions by moments or cumulants, where f (x) are polynomials of the data x. We
generalize these polynomials by replacing x by the coding layer of an inception model in order to
obtain vision-relevant features. For practical reasons we only consider the ﬁrst two polynomials, that
is, the ﬁrst two moments: mean and covariance. The Gaussian is the maximum entropy distribution
for given mean and covariance, therefore we assume the coding units to follow a multidimensional
Gaussian. The difference of two Gaussians (synthetic and real-world images) is measured by the
Fréchet distance [16] also known as Wasserstein-2 distance [58]. We call the Fréchet distance d(., .)
between the Gaussian with mean (m, C) obtained from p(.) and the Gaussian with mean (mw, Cw)
obtained from pw(.) the “Fréchet Inception Distance” (FID), which is given by [15]:

p(.)f (x)dx =

(cid:82)

(cid:82)

m
(cid:107)

2
2 + Tr

d2((m, C), (mw, Cw)) =

mw(cid:107)
Next we show that the FID is consistent with increasing disturbances and human judgment. Fig. 3
evaluates the FID for Gaussian noise, Gaussian blur, implanted black rectangles, swirled images,
salt and pepper noise, and CelebA dataset contaminated by ImageNet images. The FID captures the
disturbance level very well. In the experiments we used the FID to evaluate the performance of GANs.
For more details and a comparison between FID and Inception Score see Appendix Section A1,
where we show that FID is more consistent with the noise level than the Inception Score.

C + Cw −

CCw

(6)

−

2

(cid:1)

(cid:0)

(cid:0)

(cid:1)

.

1/2

Model Selection and Evaluation. We compare the two time-scale update rule (TTUR) for GANs
with the original GAN training to see whether TTUR improves the convergence speed and per-
formance of GANs. We have selected Adam stochastic optimization to reduce the risk of mode
collapsing. The advantage of Adam has been conﬁrmed by MNIST experiments, where Adam indeed

6

considerably reduced the cases for which we observed mode collapsing. Although TTUR ensures
that the discriminator converges during learning, practicable learning rates must be found for each
experiment. We face a trade-off since the learning rates should be small enough (e.g. for the generator)
to ensure convergence but at the same time should be large enough to allow fast learning. For each of
the experiments, the learning rates have been optimized to be large while still ensuring stable training
which is indicated by a decreasing FID or Jensen-Shannon-divergence (JSD). We further ﬁxed the
time point for stopping training to the update step when the FID or Jensen-Shannon-divergence of
the best models was no longer decreasing. For some models, we observed that the FID diverges
or starts to increase at a certain time point. An example of this behaviour is shown in Fig. 5. The
performance of generative models is evaluated via the Fréchet Inception Distance (FID) introduced
above. For the One Billion Word experiment, the normalized JSD served as performance measure.
For computing the FID, we propagated all images from the training dataset through the pretrained
Inception-v3 model following the computation of the Inception Score [53], however, we use the last
pooling layer as coding layer. For this coding layer, we calculated the mean mw and the covariance
matrix Cw. Thus, we approximate the ﬁrst and second central moment of the function given by
the Inception coding layer under the real world distribution. To approximate these moments for the
model distribution, we generate 50,000 images, propagate them through the Inception-v3 model, and
then compute the mean m and the covariance matrix C. For computational efﬁciency, we evaluate
the FID every 1,000 DCGAN mini-batch updates, every 5,000 WGAN-GP outer iterations for the
image experiments, and every 100 outer iterations for the WGAN-GP language model. For the one
time-scale updates a WGAN-GP outer iteration for the image model consists of ﬁve discriminator
mini-batches and ten discriminator mini-batches for the language model, where we follow the original
implementation. For TTUR however, the discriminator is updated only once per iteration. We repeat
the training for each single time-scale (orig) and TTUR learning rate eight times for the image
datasets and ten times for the language benchmark. Additionally to the mean FID training progress
we show the minimum and maximum FID over all runs at each evaluation time-step. For more details,
implementations and further results see Appendix Section A4 and A6.

Simple Toy Data. We ﬁrst want to demonstrate the difference between a single time-scale update
rule and TTUR on a simple toy min/max problem where a saddle point should be found. The
y2) in Fig. 4 (left) has a saddle point at (x, y) = (0, 0) and
objective f (x, y) = (1 + x2)(100
measures the distance of the parameter vector (x, y) to
fulﬁlls assumption A4. The norm
the saddle point. We update (x, y) by gradient descent in x and gradient ascent in y using additive
Gaussian noise in order to simulate a stochastic update. The updates should converge to the saddle
point (x, y) = (0, 0) with objective value f (0, 0) = 100 and the norm 0. In Fig. 4 (right), the ﬁrst
two rows show one time-scale update rules. The large learning rate in the ﬁrst row diverges and has
large ﬂuctuations. The smaller learning rate in the second row converges but slower than the TTUR in
the third row which has slow x-updates. TTUR with slow y-updates in the fourth row also converges
but slower.

−
(x, y)
(cid:107)
(cid:107)

Figure 4: Left: Plot of the objective with a saddle point at (0, 0). Right: Training progress with
equal learning rates of 0.01 (ﬁrst row) and 0.001 (second row)) for x and y, TTUR with a learning
rate of 0.0001 for x vs. 0.01 for y (third row) and a larger learning rate of 0.01 for x vs. 0.0001 for y
(fourth row). The columns show the function values (left), norms (middle), and (x, y) (right). TTUR
(third row) clearly converges faster than with equal time-scale updates and directly moves to the
saddle point as shown by the norm and in the (x, y)-plot.

DCGAN on Image Data. We test TTUR for the deep convolutional GAN (DCGAN) [51] at the
CelebA, CIFAR-10, SVHN and LSUN Bedrooms dataset. Fig. 5 shows the FID during learning

7

Figure 5: Mean FID (solid line) surrounded by a shaded area bounded by the maximum and the
minimum over 8 runs for DCGAN on CelebA, CIFAR-10, SVHN, and LSUN Bedrooms. TTUR
learning rates are given for the discriminator b and generator a as: “TTUR b a”. Top Left: CelebA.
Top Right: CIFAR-10, starting at mini-batch update 10k for better visualisation. Bottom Left:
SVHN. Bottom Right: LSUN Bedrooms. Training with TTUR (red) is more stable, has much lower
variance, and leads to a better FID.

with the original learning method (orig) and with TTUR. The original training method is faster at
the beginning, but TTUR eventually achieves better performance. DCGAN trained TTUR reaches
constantly a lower FID than the original method and for CelebA and LSUN Bedrooms all one
time-scale runs diverge. For DCGAN the learning rate of the generator is larger then that of the
discriminator, which, however, does not contradict the TTUR theory (see the Appendix Section A5).
In Table 1 we report the best FID with TTUR and one time-scale training for optimized number of
updates and learning rates. TTUR constantly outperforms standard training and is more stable.

WGAN-GP on Image Data. We used the WGAN-GP image model [23] to test TTUR with the
CIFAR-10 and LSUN Bedrooms datasets. In contrast to the original code where the discriminator is
trained ﬁve times for each generator update, TTUR updates the discriminator only once, therefore
we align the training progress with wall-clock time. The learning rate for the original training was
optimized to be large but leads to stable learning. TTUR can use a higher learning rate for the
discriminator since TTUR stabilizes learning. Fig. 6 shows the FID during learning with the original
learning method and with TTUR. Table 1 shows the best FID with TTUR and one time-scale training
for optimized number of iterations and learning rates. Again TTUR reaches lower FIDs than one
time-scale training.

Figure 6: Mean FID (solid line) surrounded by a shaded area bounded by the maximum and the
minimum over 8 runs for WGAN-GP on CelebA, CIFAR-10, SVHN, and LSUN Bedrooms. TTUR
learning rates are given for the discriminator b and generator a as: “TTUR b a”. Left: CIFAR-10,
starting at minute 20. Right: LSUN Bedrooms. Training with TTUR (red) has much lower variance
and leads to a better FID.

8

Figure 7: Performance of WGAN-GP models trained with the original (orig) and our TTUR method
on the One Billion Word benchmark. The performance is measured by the normalized Jensen-
Shannon-divergence based on 4-gram (left) and 6-gram (right) statistics averaged (solid line) and
surrounded by a shaded area bounded by the maximum and the minimum over 10 runs, aligned to
wall-clock time and starting at minute 150. TTUR learning (red) clearly outperforms the original one
time-scale learning.

WGAN-GP on Language Data. Finally the One Billion Word Benchmark [12] serves to evaluate
TTUR on WGAN-GP. The character-level generative language model is a 1D convolutional neural
network (CNN) which maps a latent vector to a sequence of one-hot character vectors of dimension
32 given by the maximum of a softmax output. The discriminator is also a 1D CNN applied to
sequences of one-hot vectors of 32 characters. Since the FID criterium only works for images, we
measured the performance by the Jensen-Shannon-divergence (JSD) between the model and the
real world distribution as has been done previously [23]. In contrast to the original code where the
critic is trained ten times for each generator update, TTUR updates the discriminator only once,
therefore we align the training progress with wall-clock time. The learning rate for the original
training was optimized to be large but leads to stable learning. TTUR can use a higher learning rate
for the discriminator since TTUR stabilizes learning. We report for the 4 and 6-gram word evaluation
the normalized mean JSD for ten runs for original training and TTUR training in Fig. 7. In Table 1
we report the best JSD at an optimal time-step where TTUR outperforms the standard training for
both measures. The improvement of TTUR on the 6-gram statistics over original training shows that
TTUR enables to learn to generate more subtle pseudo-words which better resembles real words.

Table 1: The performance of DCGAN and WGAN-GP trained with the original one time-scale
update rule and with TTUR on CelebA, CIFAR-10, SVHN, LSUN Bedrooms and the One Billion
Word Benchmark. During training we compare the performance with respect to the FID and JSD for
optimized number of updates. TTUR exhibits consistently a better FID and a better JSD.

DCGAN Image
method
dataset
CelebA
TTUR
CIFAR-10 TTUR
TTUR
SVHN
LSUN
TTUR
WGAN-GP Image
method
dataset
CIFAR-10 TTUR
LSUN
TTUR
WGAN-GP Language
n-gram
4-gram
6-gram

method
TTUR
TTUR

b, a
1e-5, 5e-4
1e-4, 5e-4
1e-5, 1e-4
1e-5, 1e-4

updates
225k
75k
165k
340k

FID method
12.5
36.9
12.5
57.5

orig
orig
orig
orig

b, a
3e-4, 1e-4
3e-4, 1e-4

time(m)
700
1900

FID method
24.8
9.5

orig
orig

b, a
3e-4, 1e-4
3e-4, 1e-4

time(m)
1150
1120

JSD method
0.35
0.74

orig
orig

b = a
5e-4
1e-4
5e-5
5e-5

b = a
1e-4
1e-4

b = a
1e-4
1e-4

updates
70k
100k
185k
70k

time(m)
800
2010

time(m)
1040
1070

FID
21.4
37.7
21.4
70.4

FID
29.3
20.5

JSD
0.38
0.77

9

Conclusion

For learning GANs, we have introduced the two time-scale update rule (TTUR), which we have
proved to converge to a stationary local Nash equilibrium. Then we described Adam stochastic
optimization as a heavy ball with friction (HBF) dynamics, which shows that Adam converges and
that Adam tends to ﬁnd ﬂat minima while avoiding small local minima. A second order differential
equation describes the learning dynamics of Adam as an HBF system. Via this differential equation,
the convergence of GANs trained with TTUR to a stationary local Nash equilibrium can be extended
to Adam. Finally, to evaluate GANs, we introduced the ‘Fréchet Inception Distance” (FID) which
captures the similarity of generated images to real ones better than the Inception Score. In experiments
we have compared GANs trained with TTUR to conventional GAN training with a one time-scale
update rule on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.
TTUR outperforms conventional GAN training consistently in all experiments.

Acknowledgment

This work was supported by NVIDIA Corporation, Bayer AG with Research Agreement 09/2017,
Zalando SE with Research Agreement 01/2016, Audi.JKU Deep Learning Center, Audi Electronic
Venture GmbH, IWT research grant IWT150865 (Exaptation), H2020 project grant 671555 (ExCAPE)
and FWF grant P 28660-N31.

The references are provided after Section A6.

References

Appendix

Contents

. .

A1 Fréchet Inception Distance (FID)
A2 Two Time-Scale Stochastic Approximation Algorithms . . . . . . . . . . . . . . . . . .
A2.1 Convergence of Two Time-Scale Stochastic Approximation Algorithms . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A2.1.1 Additive Noise .
A2.1.2 Linear Update, Additive Noise, and Markov Chain . . . . . . . . . . . . .
A2.1.3 Additive Noise and Controlled Markov Processes . . . . . . . . . . . . . .
A2.2 Rate of Convergence of Two Time-Scale Stochastic Approximation Algorithms . .
A2.2.1 Linear Update Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A2.2.2 Nonlinear Update Rules . . . . . . . . . . . . . . . . . . . . . . . . . . .
A2.3 Equal Time-Scale Stochastic Approximation Algorithms . . . . . . . . . . . . . .
A2.3.1 Equal Time-Scale for Saddle Point Iterates . . . . . . . . . . . . . . . . .
A2.3.2 Equal Time Step for Actor-Critic Method . . . . . . . . . . . . . . . . . .
A3 ADAM Optimization as Stochastic Heavy Ball with Friction . . . . . . . . . . . . . . .
A4 Experiments: Additional Information . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A5 Discriminator vs. Generator Learning Rate . . . . . . . . . . . . . . . . . . . . . . . .
A6 Used Software, Datasets, Pretrained Models, and Implementations . . . . . . . . . . . .
.
List of Figures .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
List of Tables .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
16
16
16
18
20
23
23
25
27
27
28
30
32
32
33
33
34
34
38
38

A4.1 WGAN-GP on Image Data.
A4.2 WGAN-GP on the One Billion Word Benchmark.
.
A4.3 BEGAN .

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

.

.

.

10

A1 Fréchet Inception Distance (FID)

(cid:82)

p(.)f (x)dx =

We improve the Inception score for comparing the results of GANs [53]. The Inception score has the
disadvantage that it does not use the statistics of real world samples and compare it to the statistics
of synthetic samples. Let p(.) be the distribution of model samples and pw(.) the distribution of
the samples from real world. The equality p(.) = pw(.) holds except for a non-measurable set if
pw(.)f (x)dx for a basis f (.) spanning the function space in which
and only if
p(.) and pw(.) live. These equalities of expectations are used to describe distributions by moments
or cumulants, where f (x) are polynomials of the data x. We replacing x by the coding layer of an
Inception model in order to obtain vision-relevant features and consider polynomials of the coding
unit functions. For practical reasons we only consider the ﬁrst two polynomials, that is, the ﬁrst two
moments: mean and covariance. The Gaussian is the maximum entropy distribution for given mean
and covariance, therefore we assume the coding units to follow a multidimensional Gaussian. The
difference of two Gaussians is measured by the Fréchet distance [16] also known as Wasserstein-2
distance [58]. The Fréchet distance d(., .) between the Gaussian with mean and covariance (m, C)
obtained from p(.) and the Gaussian (mw, Cw) obtained from pw(.) is called the “Fréchet Inception
Distance” (FID), which is given by [15]:

(cid:82)

d2((m, C), (mw, Cw)) =

mw(cid:107)
Next we show that the FID is consistent with increasing disturbances and human judgment on the
CelebA dataset. We computed the (mw, Cw) on all CelebA images, while for computing (m, C)
we used 50,000 randomly selected samples. We considered following disturbances of the image X:

C + Cw −

2
2 + Tr

m
(cid:107)

CCw

(7)

−

2

(cid:0)

(cid:0)

(cid:1)

(cid:1)

.

1/2

1. Gaussian noise: We constructed a matrix N with Gaussian noise scaled to [0, 255]. The
α)X + αN for α
noisy image is computed as (1
. The larger α is,
}
the larger is the noise added to the image, the larger is the disturbance of the image.

0, 0.25, 0.5, 0.75

∈ {

−

2. Gaussian blur: The image is convolved with a Gaussian kernel with standard deviation
. The larger α is, the larger is the disturbance of the image, that is, the more
0, 1, 2, 4
}

α
the image is smoothed.

∈ {

3. Black rectangles: To an image ﬁve black rectangles are are added at randomly chosen
locations. The rectangles cover parts of the image. The size of the rectangles is αimagesize
. The larger α is, the larger is the disturbance of the image, that
with α
}
is, the more of the image is covered by black rectangles.

0, 0.25, 0.5, 0.75

∈ {

4. Swirl: Parts of the image are transformed as a spiral, that is, as a swirl (whirlpool effect).
Consider the coordinate (x, y) in the noisy (swirled) image for which we want to ﬁnd the
color. Towards this end we need the reverse mapping for the swirl transformation which
gives the location which is mapped to (x, y). We ﬁrst compute polar coordinates relative
x0)) and the radius
to a center (x0, y0) given by the angle θ = arctan((y
5r/(ln 2ρ).
r =
Here α is a parameter for the amount of swirl and ρ indicates the swirl extent in pixels. The
original coordinates, where the color for (x, y) can be found, are xorg = x0 + r cos(θ(cid:48))
and yorg = y0 + r sin(θ(cid:48)). We set (x0, y0) to the center of the image and ρ = 25. The
disturbance level is given by the amount of swirl α
. The larger α is, the larger
∈ {
is the disturbance of the image via the amount of swirl.

y0)2. We transform them according to θ(cid:48) = θ + αe−

0, 1, 2, 4
}

x0)2 + (y

y0)/(x

(cid:112)

(x

−

−

−

−

5. Salt and pepper noise: Some pixels of the image are set to black or white, where black is
chosen with 50% probability (same for white). Pixels are randomly chosen for being ﬂipped
to white or black, where the ratio of pixel ﬂipped to white or black is given by the noise
level α
. The larger α is, the larger is the noise added to the image via
}
ﬂipping pixels to white or black, the larger is the disturbance level.

0, 0.1, 0.2, 0.3

∈ {

6. ImageNet contamination: From each of the 1,000 ImageNet classes, 5 images are randomly
chosen, which gives 5,000 ImageNet images. The images are ensured to be RGB and to
have a minimal size of 256x256. A percentage of α
of the CelebA
images has been replaced by ImageNet images. α = 0 means all images are from CelebA,
α = 0.25 means that 75% of the images are from CelebA and 25% from ImageNet etc.
The larger α is, the larger is the disturbance of the CelebA dataset by contaminating it by
ImageNet images. The larger the disturbance level is, the more the dataset deviates from the
reference real world dataset.

0, 0.25, 0.5, 0.75

∈ {

}

11

We compare the Inception Score [53] with the FID. The Inception Score with m samples and K
classes is

m

K

exp

1
m

i=1
(cid:88)

(cid:88)k=1

(cid:0)

p(yk |

Xi) log

Xi)

p(yk |

p(yk)

.

(cid:1)

The FID is a distance, while the Inception Score is a score. To compare FID and Inception Score,
we transform the Inception Score to a distance, which we call “Inception Distance” (IND). This
transformation to a distance is possible since the Inception Score has a maximal value. For zero
p(yk) = 0. We can bound the
probability p(yk |
log-term by

Xi) = 0, we set the value p(yk |

Xi) log p(yk|

Xi)

p(yk |

log

Xi)

p(yk)

(cid:54) log

1
1/m

= log m .

Using this bound, we obtain an upper bound on the Inception Score:

p(yk |

Xi) log

Xi)

p(yk |

p(yk)

(cid:1)

exp

1
m

(cid:0)
(cid:54) exp

m

K

i=1
(cid:88)

(cid:88)k=1
1
m

log m

m

K

1
m

i=1
(cid:88)
m

(cid:88)k=1
1

i=1
(cid:88)

(cid:1)

(cid:0)

(cid:0)

= exp

log m

= m .

p(yk |

Xi)

(cid:1)

The upper bound is tight and achieved if m (cid:54) K and every sample is from a different class and
the sample is classiﬁed correctly with probability 1. The IND is computed “IND = m - Inception
Score”, therefore the IND is zero for a perfect subset of the ImageNet with m < K samples, where
each sample stems from a different class. Therefore both distances should increase with increasing
disturbance level. In Figure A8 we present the evaluation for each kind of disturbance. The larger the
disturbance level is, the larger the FID and IND should be. In Figure A9, A10, A11, and A11 we
show examples of images generated with DCGAN trained on CelebA with FIDs 500, 300, 133, 100,
45, 13, and FID 3 achieved with WGAN-GP on CelebA.

12

(8)

(9)

(10)

(11)

(12)

Figure A8: Left: FID and right: Inception Score are evaluated for ﬁrst row: Gaussian noise, second
row: Gaussian blur, third row: implanted black rectangles, fourth row: swirled images, ﬁfth row.
salt and pepper noise, and sixth row: the CelebA dataset contaminated by ImageNet images. Left is
the smallest disturbance level of zero, which increases to the highest level at right. The FID captures
the disturbance level very well by monotonically increasing whereas the Inception Score ﬂuctuates,
stays ﬂat or even, in the worst case, decreases.

13

Figure A9: Samples generated from DCGAN trained on CelebA with different FIDs. Left: FID 500
and Right: FID 300.

Figure A10: Samples generated from DCGAN trained on CelebA with different FIDs. Left: FID 133
and Right: FID 100.

14

Figure A11: Samples generated from DCGAN trained on CelebA with different FIDs. Left: FID 45
and Right: FID 13.

Figure A12: Samples generated from WGAN-GP trained on CelebA with a FID of 3.

15

A2 Two Time-Scale Stochastic Approximation Algorithms

Stochastic approximation algorithms are iterative procedures to ﬁnd a root or a stationary point
(minimum, maximum, saddle point) of a function when only noisy observations of its values or
its derivatives are provided. Two time-scale stochastic approximation algorithms are two coupled
iterations with different step sizes. For proving convergence of these interwoven iterates it is assumed
that one step size is considerably smaller than the other. The slower iterate (the one with smaller step
size) is assumed to be slow enough to allow the fast iterate converge while being perturbed by the the
slower. The perturbations of the slow should be small enough to ensure convergence of the faster.
The iterates map at time step n (cid:62) 0 the fast variable wn ∈
their new values:

Rk and the slow variable θn ∈

Rm to

θn+1 = θn + a(n)

h

θn, wn, Z(θ)
n

+ M (θ)

n

,

wn+1 = wn + b(n)

(cid:16)

(cid:0)
g

(cid:1)
θn, wn, Z(w)

n

(cid:17)
+ M (w)

n

.

(cid:16)

(cid:0)

(cid:1)

(cid:17)

(13)

(14)

The iterates use

•

•

•

•

•

•

•

•

M (w)

Z(θ)

Z(w)

h(.)

g(.)

∈

∈

Rm: mapping for the slow iterate Eq. (13),
Rk: mapping for the fast iterate Eq. (14),

a(n): step size for the slow iterate Eq. (13),

b(n): step size for the fast iterate Eq. (14),
M (θ)

n : additive random Markov process for the slow iterate Eq. (13),

n : additive random Markov process for the fast iterate Eq. (14),

n : random Markov process for the slow iterate Eq. (13),

n : random Markov process for the fast iterate Eq. (14).

A2.1 Convergence of Two Time-Scale Stochastic Approximation Algorithms

A2.1.1 Additive Noise

The ﬁrst result is from Borkar 1997 [9] which was generalized in Konda and Borkar 1999 [31].
Borkar considered the iterates:

θn+1 = θn + a(n)

h

θn, wn

+ M (θ)

n

wn+1 = wn + b(n)

(cid:16)

(cid:0)
g

(cid:1)
θn, wn

(cid:17)
+ M (w)

n

(cid:16)

(cid:0)

(cid:1)

,

.

(cid:17)

(cid:55)→

Assumptions. We make the following assumptions:

(A1) Assumptions on the update functions: The functions h : Rk+m

Rm and g : Rk+m

Rk

are Lipschitz.

(A2) Assumptions on the learning rates:

(15)

(16)

(cid:55)→

(17)

(18)

(19)

n
(cid:88)

a(n) =

b(n) =

,

,

∞

∞

n
(cid:88)
a(n) = o(b(n)) ,

a2(n) <

b2(n) <

,

,

∞

∞

n
(cid:88)

n
(cid:88)

16

(A3) Assumptions on the noise: For the increasing σ-ﬁeld

Fn = σ(θl, wl, M (θ)
the sequences of random variables (M (θ)
n ,

l

, l (cid:54) n), n (cid:62) 0 ,

, M (w)
l
Fn) and (M (w)
n ,
n <

a.s.

Fn) satisfy

a(n) M (θ)

n
(cid:88)

b(n) M (w)

n <

a.s. .

∞

∞

n
(cid:88)
(A4) Assumption on the existence of a solution of the fast iterate: For each θ

Rm, the ODE

∈

˙w(t) = g

θ, w(t)

has a unique global asymptotically stable equilibrium λ(θ) such that λ : Rm
Lipschitz.

(cid:0)

(cid:1)

(cid:55)→

(A5) Assumption on the existence of a solution of the slow iterate: The ODE

˙θ(t) = h

θ(t), λ(θ(t))

has a unique global asymptotically stable equilibrium θ∗.
(cid:1)

(cid:0)

(A6) Assumption of bounded iterates:

(20)

(21)

(22)

Rk is

(23)

(24)

(25)

sup

n (cid:107)

sup

n (cid:107)

θn(cid:107)
wn(cid:107)

<

<

,

.

∞

∞

Convergence Theorem The next theorem is from Borkar 1997 [9].
Theorem 3 (Borkar). If the assumptions are satisﬁed, then the iterates Eq. (15) and Eq. (16) converge
to (θ∗, λ(θ∗)) a.s.

Comments

difference sequence w.r.t

(C1) According to Lemma 2 in [7] Assumption (A3) is fulﬁlled if
Fn with
E

(cid:54) B1

2
M (θ)
n (cid:107)
(cid:107)

(θ)
n
| F

and

M (w)
n

{

}

(cid:104)
is a martingale difference sequence w.r.t

(cid:105)

Fn with
(cid:54) B2 ,

E

2
M (w)
n (cid:107)
(cid:107)

(w)
n
| F

(cid:104)

(cid:105)

where B1 and B2 are positive deterministic constants.

M (θ)
n
{

}

is a martingale

(C2) Assumption (A3) holds for mini-batch learning which is the most frequent case of stochastic
i=1 f (xi, θ)), 1 (cid:54) i (cid:54) N and the mini-
∇θ( 1
gradient. The batch gradient is Gn :=
i=1 f (xui, θ)), 1 (cid:54) ui (cid:54) N , where the
∇θ( 1
batch gradient for batch size s is hn :=
indexes ui are randomly and uniformly chosen. For the noise M (θ)
Gn we have
n := hn −
E[M (θ)
Gn = 0. Since the indexes are chosen without knowing
past events, we have a martingale difference sequence. For bounded gradients we have
bounded

Gn = Gn −

n ] = E[hn]

M (θ)
n

(cid:80)
(cid:80)

2.

−

N

N

s

s

(C3) We address assumption (A4) with weight decay in two ways: (I) Weight decay avoids
problems with a discriminator that is region-wise constant and, therefore, does not have a
locally stable generator. If the generator is perfect, then the discriminator is 0.5 everywhere.
For generator with mode collapse, (i) the discriminator is 1 in regions without generator
examples, (ii) 0 in regions with generator examples only, (iii) is equal to the local ratio

(cid:107)

(cid:107)

17

of real world examples for regions with generator and real world examples. Since the
discriminator is locally constant, the generator has gradient zero and cannot improve. Also
the discriminator cannot improve, since it has minimal error given the current generator.
However, without weight decay the Nash Equilibrium is not stable since the second order
derivatives are zero, too. (II) Weight decay avoids that the generator is driven to inﬁnity
with unbounded weights. For example a linear discriminator can supply a gradient for the
generator outside each bounded region.

(C4) The main result used in the proof of the theorem relies on work on perturbations of ODEs

(C5) Konda and Borkar 1999 [31] generalized the convergence proof to distributed asynchronous

according to Hirsch 1989 [24].

update rules.

(C6) Tadi´c relaxed the assumptions for showing convergence [54]. In particular the noise as-
sumptions (Assumptions A2 in [54]) do not have to be martingale difference sequences
and are more general than in [9]. In another result the assumption of bounded iterates is
not necessary if other assumptions are ensured [54]. Finally, Tadi´c considers the case of
non-additive noise [54]. Tadi´c does not provide proofs for his results. We were not able
to ﬁnd such proofs even in other publications of Tadi´c.

A2.1.2 Linear Update, Additive Noise, and Markov Chain

In contrast to the previous subsection, we assume that an additional Markov chain inﬂuences the
iterates [30, 32]. The Markov chain allows applications in reinforcement learning, in particular in
actor-critic setting where the Markov chain is used to model the environment. The slow iterate is the
actor update while the fast iterate is the critic update. For reinforcement learning both the actor and
the critic observe the environment which is driven by the actor actions. The environment observations
are assumed to be a Markov chain. The Markov chain can include eligibility traces which are modeled
as explicit states in order to keep the Markov assumption.

The Markov chain is the sequence of observations of the environment which progresses via transition
probabilities. The transitions are not affected by the critic but by the actor.

Konda et al. considered the iterates [30, 32]:

θn+1 = θn + a(n) Hn ,

wn+1 = wn + b(n)

g

Z(w)

n ; θn

+ G

Z(w)

n ; θn

wn + M (w)

n wn

.

Hn is a random process that drives the changes of θn. We assume that Hn is a slow enough process.
(cid:0)
Rk and the matrix
We have a linear update rule for the fast iterate using the vector function g(.)
function G(.)

Rk

k.

∈

(cid:1)

(cid:0)

(cid:1)

(cid:16)

(cid:17)

×

∈

Assumptions. We make the following assumptions:

(A1) Assumptions on the Markov process, that is, the transition kernel: The stochastic process
takes values in a Polish (complete, separable, metric) space Z with the Borel σ-ﬁeld

Z(w)
n

Fn = σ(θl, wl, Z(w)
| Fn) = P(Z(w)
A
We deﬁne for every measurable function f

For every measurable set A
P(Z(w)

n+1 ∈

n+1 ∈

⊂

l

|

, Hl, l (cid:54) n), n (cid:62) 0 .

Z and the parametrized transition kernel P(.; θn) we have:
n ; θn) = P(Z(w)
(28)

n , A; θn) .

Z(w)

A

(A2) Assumptions on the learning rates:

Pθf (z) :=

P(z, d ¯z; θn) f ( ¯z) .

b2(n) <

,

∞

b(n) =

,

n
(cid:88)

n (cid:18)
(cid:88)

a(n)
b(n)

(cid:19)

n
(cid:88)

<

,

∞

(cid:90)

∞

d

18

(26)

(27)

(29)

(30)

for some d > 0.

(A3) Assumptions on the noise: The sequence M (w)

n

is a k

k-matrix valued

×

Fn-martingale

difference with bounded moments:

We assume slowly changing θ, therefore the random process Hn satisﬁes

E

M (w)
n

= 0 ,

(cid:104)
E

sup
n

| Fn
d

M (w)
n

(cid:105)

(cid:21)

<

,

d > 0 .

∞

∀

(cid:20)(cid:13)
(cid:13)
(cid:13)

(cid:104)

(cid:13)
(cid:13)
(cid:13)

d

(cid:105)

E

sup
n

Hn(cid:107)
(cid:107)

<

,

d > 0 .

∞

∀

(A4) Assumption on the existence of a solution of the fast iterate: We assume the existence of a
Rm, there exist functions
k that satisfy the

solution to the Poisson equation for the fast iterate. For each θ
Rk, ¯G(θ)
¯g(θ)
Poisson equations:

∈
Rk, and ˆG(z; θ) : Z

k, ˆg(z; θ) : Z

Rk

Rk

→

→

∈

∈

×

×

ˆg(z; θ) = g(z; θ)
ˆG(z; θ) = G(z; θ)

¯g(θ) + (Pθ ˆg(.; θ))(z) ,
¯G(θ) + (Pθ ˆG(.; θ))(z) .

−

−

(A5) Assumptions on the update functions and solutions to the Poisson equation:
(a) Boundedness of solutions: For some constant C and for all θ:

max

max

{(cid:107)

¯g(θ)
¯G(θ)

{(cid:107)

(cid:107)}

(cid:107)}

(cid:54) C ,
(cid:54) C .

(b) Boundedness in expectation: All moments are bounded. For any d > 0, there exists

Cd > 0 such that

E

sup
n

ˆg(Z(w)

(cid:54) Cd ,

(cid:21)

E

sup
n

(cid:20)(cid:13)
(cid:13)
g(Z(w)
(cid:13)
(cid:20)(cid:13)
(cid:13)
ˆG(Z(w)
(cid:13)
(cid:20)(cid:13)
(cid:13)
G(Z(w)
(cid:13)
(cid:20)(cid:13)
(cid:13)
(c) Lipschitz continuity of solutions: For some constant C > 0 and for all θ, ¯θ
(cid:13)

(cid:54) Cd ,

(cid:54) Cd ,

(cid:54) Cd .

sup
n

sup
n

E

E

(cid:21)

(cid:21)

(cid:21)

Rm:

∈

d

n ; θ)
(cid:13)
d
(cid:13)
n ; θ)
(cid:13)
(cid:13)
d
(cid:13)
n ; θ)
(cid:13)
(cid:13)
d
(cid:13)
n ; θ)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:54) C
(cid:54) C

(d) Lipschitz continuity in expectation: There exists a positive measurable function C(.)

on Z such that

¯g(θ)
¯G(θ)
(cid:13)
(cid:13)

−

−

¯g( ¯θ)
¯G( ¯θ)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

θ
(cid:107)
θ
(cid:107)

−

−

¯θ
(cid:107)
¯θ
(cid:107)

,

.

E

sup
n

(cid:104)

C(Z(w)

n )d

<

,

d > 0 .

∞

∀

(cid:105)

Function C(.) gives the Lipschitz constant for every z:

(Pθ ˆg(.; θ))(z)
(Pθ ˆG(.; θ))(z)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

−

(P ¯θ ˆg(.; ¯θ))(z)
ˆG(.; ¯θ))(z)

(P ¯θ

(cid:54) C(z)

(cid:54) C(z)

θ
(cid:107)
θ
(cid:107)

−

−

¯θ
(cid:107)
¯θ
(cid:107)

,

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(e) Uniform positive deﬁniteness: There exists some α > 0 such that for all w

Rm:

θ

∈

Rk and

∈

(47)

wT ¯G(θ) w (cid:62) α

2 .

w
(cid:107)

(cid:107)

19

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

Convergence Theorem. We report Theorem 3.2 (see also Theorem 7 in [32]) and Theorem 3.13
from [30]:
Theorem 4 (Konda & Tsitsiklis). If the assumptions are satisﬁed, then for the iterates Eq. (26) and
Eq. (27) holds:

lim
n
→∞
lim
n
→∞

¯G(θn) wn −
(cid:13)
wn −
(cid:13)
(cid:13)
(cid:13)

¯G−

¯g(θn)

= 0 a.s. ,

1(θn) ¯g(θn)
(cid:13)
(cid:13)

= 0 .

(cid:13)
(cid:13)

(48)

(49)

Comments.

(C1) The proofs only use the boundedness of the moments of Hn [30, 32], therefore Hn may
depend on wn. In his PhD thesis [30], Vijaymohan Konda used this framework for the
actor-critic learning, where Hn drives the updates of the actor parameters θn. However, the
actor updates are based on the current parameters wn of the critic.

(C2) The random process Z(w)

n

can affect Hn as long as boundedness is ensured.
Z(w)

(C3) Nonlinear update rule. g

Z(w)
wn can be viewed as a linear approxi-
mation of a nonlinear update rule. The nonlinear case has been considered in [30] where
additional approximation errors due to linearization were addressed. These errors are treated
in the given framework [30].

n ; θn

n ; θn

+ G

(cid:0)

(cid:1)

(cid:0)

(cid:1)

A2.1.3 Additive Noise and Controlled Markov Processes

The most general iterates use nonlinear update functions g and h, have additive noise, and have
controlled Markov processes [28].

θn+1 = θn + a(n)

h

θn, wn, Z(θ)
n

+ M (θ)

n

,

wn+1 = wn + b(n)

(cid:16)

(cid:0)
g

(cid:1)
θn, wn, Z(w)

n

(cid:17)
+ M (w)

n

.

(cid:16)

(cid:0)

(cid:1)

(cid:17)

(50)

(51)

Required Deﬁnitions. Marchaud Map: A set-valued map h : Rl
Marchaud map if it satisﬁes the following properties:

→ {

subsets of Rk} is called a

(i) For each θ
(ii) (point-wise boundedness) For each θ

Rl, h(θ) is convex and compact.

Rl,

∈

∈

sup

h(θ) (cid:107)
∈

w

(cid:107)

w

< K (1 +

θ

(cid:107)

) for some K > 0.
(cid:107)

(iii) h is an upper-semicontinuous map.

1 (in Rl) and
θn}n
We say that h is upper-semicontinuous, if given sequences
1
{
≥
≥
(in Rk) with θn →
h(θn), n
h(θ). In other words, the
1, y
∈
Rk.
, is closed in Rl
graph of h,
×

(x, y) : y

yn}n
{

h(x), x

≥

(cid:9)
is Marchaud, then the differential inclusion (DI)

θ, yn →
y and yn ∈
Rl
∈
∈
subsets of Rm

}

If the set-valued map H : Rm
given by

(cid:8)

→ {

˙θ(t)

H(θ(t))

∈

(52)

is guaranteed to have at least one solution that is absolutely continuous. If Θ is an absolutely
continuous map satisfying Eq. (52) then we say that Θ

Σ.

⊆

Invariant Set: M
with Θ(0) = θ. In other words, Θ
Internally Chain Transitive Set: M
and for every θ, y
∈
solutions to the differential inclusion ˙θ(t)

Rm is invariant if for every θ
0.
Σ with Θ(t)
Rm is said to be internally chain transitive if M is compact
M , (cid:15) > 0 and T > 0 we have the following: There exist Φ1, . . . , Φn that are n
M and

M there exists a trajectory, Θ, entirely in M
M , for all t

h(θ(t)), a sequence θ1(= θ), . . . , θn+1(= y)

∈
⊂

∈
∈

≥

∈

⊂

∈

20

n real numbers t1, t2, . . . , tn greater than T such that: Φi
(cid:15)-neighborhood of θ and Φi
is called an ((cid:15), T ) chain in M from θ to y.

[0,ti](θi)

M for 1

⊂

≤

≤

i

N (cid:15)(θi+1) where N (cid:15)(θ) is the open
ti(θi)
n. The sequence (θ1(= θ), . . . , θn+1(= y))

∈

Assumptions. We make the following assumptions [28]:

Z(w)
(A1) Assumptions on the controlled Markov processes: The controlled Markov process
n
}
Z(θ)
takes values in a compact metric space S(w). The controlled Markov process
n
}
takes values in a compact metric space S(θ). Both processes are controlled by the iterate
Z(w)
θn}
is additionally controlled by a random
and
sequences
n
{
{
Z(θ)
A(w)
taking values in a compact metric space U (w) and
process
is additionally
n
n
{
{
}
taking values in a compact metric space U (θ). The
controlled by a random process
Z(θ)
n
{
}
P(Z(θ)

, θl, wl, l (cid:54) n) =

. Furthermore

dynamics is

wn}
{

p(θ)(dz

A(θ)
n

B(θ)

{
{

n , A(θ)

, A(θ)
l

}

}

{

}

n+1 ∈

Z(θ)
l
|

Z(θ)
|

n , θn, wn), n (cid:62) 0 ,
(53)

(cid:90)B(θ)

for B(θ) Borel in S(θ). The

dynamics is

Z(w)
n

}

{
, A(w)
l

P(Z(w)

n+1 ∈

B(w)

Z(w)
l
|

for B(w) Borel in S(w).

, θl, wl, l (cid:54) n) =

p(w)(dz

Z(w)

n , A(w)

n , θn, wn), n (cid:62) 0 ,

(cid:90)B(w)

|

(54)

(A2) Assumptions on the update functions: h : Rm+k

Rm is jointly continuous as
well as Lipschitz in its ﬁrst two arguments uniformly w.r.t. the third. The latter condition
means that
z(θ)
∀

h(θ(cid:48), w(cid:48), z(θ))
(cid:107)

h(θ, w, z(θ))
(cid:107)

(cid:54) L(θ) (
(cid:107)

S(θ) :

w
(cid:107)

S(θ)

w(cid:48)

→

θ(cid:48)

×

−

−

−

+

∈

θ

(cid:107)

) .
(cid:107)
(55)

Note that the Lipschitz constant L(θ) does not depend on z(θ).
g : Rk+m
uniformly w.r.t. the third. The latter condition means that

S(w)

→

×

Rk is jointly continuous as well as Lipschitz in its ﬁrst two arguments

z(w)
∀

∈

S(w) :

g(θ, w, z(w))
(cid:107)

−

g(θ(cid:48), w(cid:48), z(w))
(cid:107)

(cid:54) L(w) (

θ
(cid:107)

−

θ(cid:48)

+

(cid:107)

w
(cid:107)

−

w(cid:48)

) .
(cid:107)
(56)

Note that the Lipschitz constant L(w) does not depend on z(w).

(A3) Assumptions on the additive noise:

M (θ)
M (w)
and
n
n
}
}
{
{
2 +
wn(cid:107)
θn(cid:107)
with second moments bounded by K(1 +
(cid:107)
(cid:107)
martingale difference sequence w.r.t. increasing σ-ﬁelds
Fn = σ(θl, wl, M (θ)

, M (w)
l

, Z(w)
l

, Z(θ)
l

l

satisfying

are martingale difference sequence
is a

2). More precisely,

M (θ)
n

{

}

, l (cid:54) n), n (cid:62) 0 ,

(cid:54) K (1 +

2 +

θn(cid:107)

(cid:107)

2) ,

wn(cid:107)
(cid:107)

E

M (θ)

2

n+1(cid:107)

(cid:107)
(cid:105)
(cid:104)
for n (cid:62) 0 and a given constant K > 0.
M (w)
n
{

}

| Fn

is a martingale difference sequence w.r.t. increasing σ-ﬁelds

Fn = σ(θl, wl, M (θ)

l

, M (w)
l

, Z(θ)
l

, Z(w)
l

, l (cid:54) n), n (cid:62) 0 ,

satisfying

E

M (w)
(cid:107)

n+1(cid:107)

2

| Fn

(cid:54) K (1 +

2 +

θn(cid:107)

(cid:107)

2) ,

wn(cid:107)

(cid:107)

(cid:105)
(cid:104)
for n (cid:62) 0 and a given constant K > 0.

21

(57)

(58)

(59)

(60)

(A4) Assumptions on the learning rates:

n
(cid:88)

a(n) =

b(n) =

,

,

∞

∞

n
(cid:88)
a(n) = o(b(n)) ,

a2(n) <

b2(n) <

,

,

∞

∞

n
(cid:88)

n
(cid:88)

Furthermore, a(n), b(n), n (cid:62) 0 are non-increasing.

(A5) Assumptions on the controlled Markov processes, that is, the transition kernels: The state-

action map

S(θ)

U (θ)

×

×

Rm+k

(cid:51)

and the state-action map

S(w)

U (w)

×

×

Rm+k

(cid:51)

are continuous.

(z(θ), a(θ), θ, w)

p(θ)(dy

z(θ), a(θ), θ, w)

(64)

(z(w), a(w), θ, w)

p(w)(dy

z(w), a(w), θ, w)

(65)

→

→

|

|

(A6) Assumptions on the existence of a solution:

We consider occupation measures which give for the controlled Markov process the prob-
U for given θ and a
ability or density to observe a particular state-action pair from S
given control policy π. We denote by D(w)(θ, w) the set of all ergodic occupation measures
for the prescribed θ and w on state-action space S(w)
U (θ) for the controlled Markov
process Z(w) with policy π(w). Analogously we denote, by D(θ)(θ, w) the set of all ergodic
occupation measures for the prescribed θ and w on state-action space S(θ)
U (θ) for the
controlled Markov process Z(θ) with policy π(θ). Deﬁne

×

×

×

˜g(θ, w, ν) =

g(θ, w, z) ν(dz, U (w))

(cid:90)

for ν a measure on S(w)

U (w) and the Marchaud map

×

ˆg(θ, w) =

˜g(θ, w, ν) : ν

{

D(w)(θ, w)
}

.

∈

We assume that the set D(w)(θ, w) is singleton, that is, ˆg(θ, w) contains a single function
and we use the same notation for the set and its single element. If the set is not a singleton, the
assumption of a solution can be expressed by the differential inclusion ˙w(t)
ˆg(θ, w(t))
[28].
θ

Rm, the ODE

∈

∀

∈

˙w(t) = ˆg(θ, w(t))

→

has an asymptotically stable equilibrium λ(θ) with domain of attraction Gθ where λ :
Rk is a Lipschitz map with constant K. Moreover, the function V : G
Rm
)
→
∞
is continuously differentiable where V (θ, .) is the Lyapunov function for λ(θ) and G =
(θ, w) : w
(θ, λ(θ)) :
{
{
Rm
θ

becomes an asymptotically stable set of the coupled ODE

. This extra condition is needed so that the set

Gθ, θ

Rm

[0,

∈

∈

}

∈

}

(A7) Assumption of bounded iterates:

˙w(t) = ˆg(θ(t), w(t))
˙θ(t) = 0 .

sup

n (cid:107)

sup

n (cid:107)

θn(cid:107)
wn(cid:107)

<

<

∞

∞

a.s. ,

a.s.

22

(61)

(62)

(63)

(66)

(67)

(68)

(69)

(70)

(71)

(72)

Convergence Theorem. The following theorem is from Karmakar & Bhatnagar [28]:
Theorem 5 (Karmakar & Bhatnagar). Under above assumptions if for all θ
1,

Rm, with probability
belongs to a compact subset Qθ (depending on the sample point) of Gθ “eventually”, then
(73)

A0 (θ∗, λ(θ∗)) a.s.

wn}

as n

∈

θ∗

{

,

∈

→ ∪
which is almost everywhere an internally chain transitive set of the

→ ∞

(θn, wn)
¯θ(s) : s (cid:62) t
}

where A0 =
∩t(cid:62)0{
differential inclusion

˙θ(t)

ˆh(θ(t)),

∈

(74)

where ˆh(θ) =

˜h(θ, λ(θ), ν) : ν
{

∈

D(w)(θ, λ(θ))

.
}

Comments.

(C1) This framework allows to show convergence for gradient descent methods beyond stochastic
gradient like for the ADAM procedure where current learning parameters are memorized
and updated. The random processes Z(w) and Z(θ) may track the current learning status for
the fast and slow iterate, respectively.

(C2) Stochastic regularization like dropout is covered via the random processes A(w) and A(θ).

A2.2 Rate of Convergence of Two Time-Scale Stochastic Approximation Algorithms

A2.2.1 Linear Update Rules

First we consider linear iterates according to the PhD thesis of Konda [30] and Konda & Tsitsiklis
[33].

θn+1 = θn + a(n)

wn+1 = wn + b(n)

a1 −
a2 −

A11 θn −
A21 θn −

A12 wn + M (θ)

n

(cid:17)
A22 wn + M (w)

n

(cid:16)

(cid:16)

,

.

(cid:17)

Assumptions. We make the following assumptions:

(A1) The random variables (M (θ)

n , M (w)
other. The have zero mean: E[M (θ)

n ), n = 0, 1, . . ., are independent of w0, θ0 and of each
n ] = 0 and E[M (w)

n ] = 0. The covariance is

E

M (θ)

n (M (θ)

n )T

= Γ11 ,

(cid:104)
M (θ)

n (M (w)

n )T

E

(cid:104)
M (w)
n

E

(M (w)

n )T

= Γ12 = ΓT

21 ,

= Γ22 .

(cid:105)

(cid:105)

(cid:105)

a(n) =

b(n) =

,

,

∞

∞

lim
n
→∞

lim
n
→∞

a(n) = 0 ,

b(n) = 0 ,

(cid:104)

n
(cid:88)

(A2) The learning rates are deterministic, positive, nondecreasing and satisfy with (cid:15) (cid:54) 0:

n
(cid:88)
a(n)
b(n) →
We often consider the case (cid:15) = 0.
(A3) Convergence of the iterates: We deﬁne

(cid:15) .

∆ := A11 −

1
A12A−

22 A21 .

A matrix is Hurwitz if the real part of each eigenvalue is strictly negative. We assume that
the matrices

∆ are Hurwitz.

A22 and

−

−

23

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

(85)

(86)

(87)
(88)

(89)

(90)

(91)

(92)

(96)

(97)

(98)

(99)

(A4) Convergence rate remains simple:

(a) There exists a constant ¯a (cid:54) 0 such that
(a(n + 1)−

1

a(n)−

1) = ¯a .

lim
n

lim
n

−

−

1
(b(n + 1)−

b(n)−

1) = 0 .

(b) If (cid:15) = 0, then

(c) The matrix

is Hurwitz.

∆

−

−

(cid:16)

¯a
2

I

(cid:17)

Rate of Convergence Theorem. The next theorem is taken from Konda [30] and Konda & Tsitsik-
lis [33].

Let θ∗

Rm and w∗

Rk be the unique solution to the system of linear equations

∈

∈

For each n, let

A11 θn + A12 wn = a1 ,
A21 θn + A22 wn = a2 .

ˆθn = θn −
ˆwn = wn −
1
Σn
11 = θ−
n E

θ∗ ,

1

22 (a2 −
A−
ˆθn ˆθT
,
n

Σn

12 =

Σn
21

(cid:104)
T

(cid:105)
1
= θ−
n E

A21 θn) ,

,

ˆθn ˆwT
n
(cid:104)

(cid:105)

(cid:0)

Σn

ˆwn ˆwT
n

1
22 = w−
(cid:1)
n E
Σn
11 Σn
(cid:2)
12
21 Σn
Σn
22(cid:19)
Theorem 6 (Konda & Tsitsiklis). Under above assumptions and when the constant (cid:15) is sufﬁciently
small, the limit matrices
Σ((cid:15))

Σn =

(94)

(93)

Σn

Σn

Σn

(cid:18)

(cid:3)
.

(95)

12 , Σ((cid:15))

11 , Σ((cid:15))

22 .

,

12 = lim
n

22 = lim
n

11 = lim
n
exist. Furthermore, the matrix

Σ(0) =

Σ(0)
Σ(0)

11 Σ(0)
21 Σ(0)

12
22 (cid:33)

(cid:32)

is the unique solution to the following system of equations

¯a Σ(0)

11 + A12 Σ(0)

21 + Σ(0)

12 AT

12 = Γ11 ,

11 + Σ(0)

∆ Σ(0)
A12 Σ(0)
A22 Σ(0)

11 ∆T
12 AT
22 AT

22 + Σ(0)
22 + Σ(0)

−
22 = Γ12 ,

22 = Γ22 .

Finally,

lim
0
(cid:15)
↓

Σ((cid:15))

11 = Σ(0)
11 ,

Σ((cid:15))

12 = Σ(0)
12 ,

Σ((cid:15))

22 = Σ(0)
22 .

(100)

lim
0
(cid:15)
↓

lim
0
(cid:15)
↓

The next theorems shows that the asymptotic covariance matrix of a(n)−
a(n)−

1/2 ¯θn, where ¯θn evolves according to the single time-scale stochastic iteration:
a1 −
(cid:16)
A21 ¯θn −

¯θn+1 = ¯θn + a(n)
0 = a2 −

A11 ¯θn −
A22 ¯wn + M (w)

A12 ¯wn + M (θ)

(cid:17)

n

n

,

.

1/2θn is the same as that of

(101)

(102)

The next theorem combines Theorem 2.8 of Konda & Tsitsiklis and Theorem 4.1 of Konda &
Tsitsiklis:

24

Theorem 7 (Konda & Tsitsiklis 2nd). Under above assumptions

If the assumptions hold with (cid:15) = 0, then a(n)−

Σ(0)

11 = lim
n

a(n)−

1 E

¯θn ¯θT
n

.

(103)

(cid:3)
1/2 ˆθn converges in distribution to

(cid:2)

(0, Σ(0)

11 ).

N

Comments.

(C1) In his PhD thesis [30] Konda extended the analysis to the nonlinear case. Konda makes a

linearization of the nonlinear function h and g with

A11 =

, A12 =

, A21 =

, A22 =

(104)

∂h
∂θ

−

∂h
∂w

−

∂g
∂θ

−

∂g
∂w

.

−

There are additional errors due to linearization which have to be considered. However, only
a sketch of a proof is provided but not a complete proof.

(C2) Theorem 4.1 of Konda & Tsitsiklis is important to generalize to the nonlinear case.
(C3) The convergence rate is governed by A22 for the fast and ∆ for the slow iterate. ∆ in turn
is affected by the interaction effects captured by A21 and A12 together with the inverse of
A22.

A2.2.2 Nonlinear Update Rules

The rate of convergence for nonlinear update rules according to Mokkadem & Pelletier is considered
[44].

The iterates are

θn+1 = θn + a(n)

h

θn, wn

+ Z(θ)

n + M (θ)
n

,

wn+1 = wn + b(n)

(cid:16)

(cid:0)
g

(cid:1)
θn, wn

+ Z(w)

(cid:17)
n + M (w)

n

.

with the increasing σ-ﬁelds

(cid:16)

(cid:0)

(cid:1)

(cid:17)

Fn = σ(θl, wl, M (θ)

l

, M (w)
l

, Z(θ)
l

, Z(w)
l

, l (cid:54) n), n (cid:62) 0 .

The terms Z(θ)
of the nonlinear functions to their linear approximation.

n and Z(w)

n

can be used to address the error through linearization, that is, the difference

Assumptions. We make the following assumptions:

(A1) Convergence is ensured:

θn = θ∗ a.s. ,

wn = w∗ a.s. .

lim
n
→∞
lim
n
→∞

(A2) Linear approximation and Hurwitz:
There exists a neighborhood

of (θ∗, w∗) such that, for all (θ, w)

h
g

θ, w
θ, w
(cid:0)
(cid:0)
We deﬁne

(cid:18)

(cid:19)

(cid:1)
(cid:1)

=

(cid:18)

U
A11 A12
A21 A22

θ
w

θ∗
w∗

(cid:19)

−
−

(cid:19) (cid:18)

+ O

∈ U
θ∗
w∗

θ
w

−
−

2

.

(cid:33)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:32)(cid:13)
(cid:13)
(cid:13)
(cid:13)

∆ := A11 −

1
A12A−

22 A21 .

A matrix is Hurwitz if the real part of each eigenvalue is strictly negative. We assume that
the matrices A22 and ∆ are Hurwitz.

25

(105)

(106)

(107)

(108)

(109)

(110)

(111)

(A3) Assumptions on the learning rates:

b(n) = b0 n−
(113)
where a0 > 0 and b0 > 0 and 1/2 < β < α (cid:54) 1. If α = 1, then a0 > 1/(2emin) with emin
as the absolute value of the largest eigenvalue of ∆ (the eigenvalue closest to 0).

a(n) = a0 n−

α

β ,

(A4) Assumptions on the noise and error:

(a) martingale difference sequences:

E

E

M (θ)

n+1 | Fn

(cid:104)
M (w)

n+1 | Fn

(cid:104)

(cid:105)

(cid:105)

= 0 a.s. ,

= 0 a.s. .

(b) existing second moments:

E

lim
n
→∞

(cid:34)(cid:32)

M (θ)
n+1
M (w)
n+1(cid:33)

(cid:16)

(M (θ)

n+1)T

(M (w)

n+1)T

= Γ =

| Fn

(cid:35)

(cid:17)

Γ11 Γ12
Γ21 Γ22

(cid:18)

a.s.

(cid:19)

(116)

(c) bounded moments:

There exist l > 2/β such that

E

sup
n

E

sup
n

l
n+1(cid:107)
l
n+1(cid:107)

M (θ)
(cid:107)
(cid:104)
M (w)
(cid:107)
(cid:104)

| Fn

| Fn

(cid:105)

(cid:105)

<

<

∞

∞

a.s. ,

a.s.

(d) bounded error:

with

Z(θ)
n = r(θ)
n = r(w)
Z(w)

n + O
n + O
(cid:0)

θ

(cid:107)

θ
(cid:107)

−

−

θ∗

2 +
(cid:107)
2 +
θ∗
(cid:107)

w

w

(cid:107)

(cid:107)

w∗

2
(cid:107)
w∗

2
(cid:1)

(cid:107)

,

,

−

−

r(θ)
n (cid:107)

(cid:107)

+

(cid:0)
r(w)
n (cid:107)

(cid:107)

= o(

a(n)) a.s.

(cid:112)

(cid:1)

Rate of Convergence Theorem. We report a theorem and a proposition from Mokkadem & Pel-
letier [44]. However, ﬁrst we have to deﬁne the covariance matrices Σθ and Σw which govern the
rate of convergence.

First we deﬁne

Γθ := lim
→∞

n

E

M (θ)

n+1 −

A12 A−

1

22 M (w)

n+1

M (θ)

n+1 −

A12 A−

1

22 M (w)

n+1

T

(cid:17)

=

| Fn

(cid:21)

(122)

Γ11 + A12 A−

22 Γ22 (A−

1
22 )T AT

12 −
We now deﬁne the asymptotic covariance matrices Σθ and Σw:

12 −

Γ12(A−

A12 A−

22 Γ21 .

1

(cid:17) (cid:16)
1
22 )T AT

Σθ =

∞

exp

∆ +

I

t

Γθ exp

∆T +

I

t

dt ,

(123)

(cid:18)(cid:18)

(cid:19)

(cid:19)

(cid:18)(cid:18)

(cid:19)

(cid:19)

1a=1
2 a0

1a=1
2 a0

Σw =

exp (A22 t) Γ22 exp (A22 t) dt .

(cid:20)(cid:16)
1

∞

0
(cid:90)

0
(cid:90)

Σθ and Σw are solutions of the Lyapunov equations:

1a=1
2 a0

∆ +

(cid:18)

(cid:19)

I

Σθ + Σθ

∆T +

I

=

Γθ ,

(cid:18)

(cid:19)
A22 Σw + Σw AT
22 =

−

−

Γ22 .

1a=1
2 a0

26

(112)

(114)

(115)

(117)

(118)

(119)

(120)

(121)

(124)

(125)

(126)

Theorem 8 (Mokkadem & Pelletier: Joint weak convergence). Under above assumptions:

θ∗)
w∗)
Theorem 9 (Mokkadem & Pelletier: Strong convergence). Under above assumptions:

Σθ
0
0 Σw

a(n)−
b(n)−

1 (θ
1 (w

D
−→ N

−
−

(cid:19)(cid:19)

0 ,

(cid:19)

(cid:18)

(cid:18)

.

(cid:18) (cid:112)
(cid:112)

θ
(cid:107)

θ∗

(cid:107)

−

= O

a(n) log

a(l)

a.s. ,





(cid:118)
(cid:117)
(cid:117)
(cid:116)





(cid:118)
(cid:117)
(cid:117)
(cid:116)

n

(cid:32)

(cid:88)l=1
n

(cid:32)

(cid:88)l=1

(cid:33)


(cid:33)


w

(cid:107)

−

(cid:107)

w∗

= O

b(n) log

b(l)

a.s.

Comments.

(C1) Besides the learning steps a(n) and b(n), the convergence rate is governed by A22 for
the fast and ∆ for the slow iterate. ∆ in turn is affected by interaction effects which are
captured by A21 and A12 together with the inverse of A22.

A2.3 Equal Time-Scale Stochastic Approximation Algorithms

In this subsection we consider the case when the learning rates have equal time-scale.

A2.3.1 Equal Time-Scale for Saddle Point Iterates

If equal time-scales assumed then the iterates revisit inﬁnite often an environment of the solution
[61]. In Zhang 2007, the functions of the iterates are the derivatives of a Lagrangian with respect to
the dual and primal variables [61]. The iterates are

θn+1 = θn + a(n)

h

θn, wn

+ Z(θ)

n + M (θ)
n

,

wn+1 = wn + a(n)

(cid:16)

(cid:0)
g

(cid:1)
θn, wn

+ Z(w)

(cid:17)
n + M (w)

n

.

with the increasing σ-ﬁelds

Fn = σ(θl, wl, M (θ)

l

, Z(w)
l

, l (cid:54) n), n (cid:62) 0 .

The terms Z(θ)

n and Z(w)

n

subsum biased estimation errors.

(cid:16)

(cid:0)
, M (w)
l

(cid:1)
, Z(θ)
l

(cid:17)

Assumptions. We make the following assumptions:

(A1) Assumptions on update function: h and g are continuous, differentiable, and bounded. The

Jacobians

∂g
∂w

and

∂h
∂θ

are Hurwitz. A matrix is Hurwitz if the real part of each eigenvalue is strictly negative. This
assumptions corresponds to the assumption in [61] that the Lagrangian is concave in w and
convex in θ.

(A2) Assumptions on noise:

and

M (θ)
n
{
Fn. Furthermore they are mutually independent.
Bounded second moment:

M (w)
n
{

}

}

are a martingale difference sequences w.r.t. the increasing σ-ﬁelds

(127)

(128)

(129)

(130)

(131)

(132)

(133)

(134)

(135)

M (θ)
(cid:107)
M (w)
(cid:107)

2
n+1(cid:107)
2
n+1(cid:107)

E

E

(cid:104)

(cid:104)

| Fn

| Fn

(cid:105)

(cid:105)

<

<

∞

∞

a.s. ,

a.s. .

27

(A3) Assumptions on the learning rate:

(A4) Assumption on the biased error:

Boundedness:

a(n) > 0

,

a(n)

0

,

a(n) =

,

→

n
(cid:88)

∞

n
(cid:88)

a2(n) <

.

∞

(136)

sup

lim
n

sup

lim
n

Z(θ)
n (cid:107)
(cid:107)
Z(w)
n (cid:107)

(cid:107)

(cid:54) α(θ) a.s.

(cid:54) α(w) a.s.

(137)

(138)

Theorem. Deﬁne the “contraction region” Aη as follows:

Aη =

(θ, w) : α(θ) (cid:62) η
{

h(θ, w)
(cid:107)

(cid:107)

or α(w) (cid:62) η

g(θ, w)

(cid:107)

, 0 (cid:54) η < 1
}
(cid:107)

.

(139)

Theorem 10 (Zhang). Under above assumptions the iterates return to Aη inﬁnitely often with
probability one (a.s.).

Comments.

(C1) The proof of the theorem in [61] does not use the saddle point condition and not the fact

that the functions of the iterates are derivatives of the same function.

(C2) For the unbiased case, Zhang showed in Theorem 3.1 of [61] that the iterates converge.
However, he used the saddle point condition of the Lagrangian. He considered iterates
with functions that are the derivatives of a Lagrangian with respect to the dual and primal
variables [61].

A2.3.2 Equal Time Step for Actor-Critic Method

If equal time-scales assumed then the iterates revisit inﬁnite often an environment of the solution of
DiCastro & Meir [14]. The iterates of DiCastro & Meir are derived for actor-critic learning.

To present the actor-critic update iterates, we have to deﬁne some functions and terms. µ(u
the policy function parametrized by θ
chain given by P(y
each state x the agent receives a reward r(x).

x, θ) is
. A Markov
x, u) gives the next observation y using the observation x and the action u. In

Rm with observations x

and actions u

∈ X

∈ U

∈

|

|

The average reward per stage is for the recurrent state x∗:

˜η(θ) = lim
→∞

T

E

(cid:34)

r(xn)

x0 = x∗, θ

.

|

(cid:35)

1
T

T

1

−

n=0
(cid:88)

The estimate of ˜η is denoted by η.

The differential value function is

˜h(x, θ) = E

(r(xn)

˜η(θ))

x0 = x, θ

.

−

|

(cid:35)

T

1

−

(cid:34)

n=0
(cid:88)

˜d(x, y, θ) = r(x)

˜η(θ) + ˜h(y, θ)

˜h(x, θ) .

−

−

The temporal difference is

The estimate of ˜d is denoted by d.
The likelihood ratio derivative Ψ

Rm is

∈
Ψ(x, u, θ) = ∇θµ(u
µ(u
|

x, θ)

|
x, θ)

.

28

(140)

(141)

(142)

(143)

(144)

(145)

(146)

(147)

(148)

(149)

(150)

(151)
(152)
(153)
(154)

(155)

The value function ˜h is approximated by

where φ(x)

Rk. We deﬁne Φ

∈

h(x, w) = φ(x)T w ,

k

R|X |×
∈
φ1(x1)
φ1(x2)
...
φ1(x

|X |

φ2(x1)
φ2(x2)
...
) φ2(x

)

|X |

Φ = 





. . . φk(x1)
. . . φk(x2)

...
. . . φk(x

)

|X |







and

For TD(λ) we have an eligibility trace:

h(w) = Φ w .

en = λ en

1 + φ(xn) .

−

We deﬁne the approximation error with optimal parameter w∗(θ):

−
where π(θ) is an projection operator into the span of Φw. We bound this error by

Rk (cid:107)

−

Φ w

(cid:107)π(θ) =

˜h(θ)
(cid:107)

Φ w∗(θ)

(cid:107)π(θ) ,

(cid:15)app(θ) = inf
∈

w

˜h(θ)

(cid:15)app = sup
Rk

θ

(cid:15)app(θ) .

∈
We denoted by ˜η, ˜d, and ˜h the exact functions and used for their approximation η, d, and h,
respectively. We have learning rate adjustments Γη and Γw for the critic.

The update rules are:
Critic:

ηn+1 = ηn + a(n) Γη (r(xn)

ηn) ,

−

h(x, wn) = φ(x)T wn ,

d(xn, xn+1, wn) = r(xn)

ηn + h(xn+1, wn)

h(xn, wn) ,

en = λ en

−
1 + φ(xn) ,
wn+1 = wn + a(n) Γw d(xn, xn+1, wn) en .

−

−

Actor:

θn+1 = θn + a(n) Ψ(xn, un, θn) d(xn, xn+1, wn) .

Assumptions. We make the following assumptions:

(A1) Assumption on rewards:

The rewards

r(x)
{
(A2) Assumption on the Markov chain:

x
}

∈X

are uniformly bounded by a ﬁnite constant Br.

Each Markov chain for each θ is aperiodic, recurrent, and irreducible.

(A3) Assumptions on the policy function:

The conditional probability function µ(u
exist positive constants, Bµ1 and Bµ2, such that for all x
1 (cid:54) l1, l2 (cid:54) m we have
∂µ(u

∂2µ(u

x, θ)

|

x, θ)

x, θ) is twice differentiable. Moreover, there
Rm and
, u

, θ

∈ X

∈ U

∈

|
∂θl

, and θ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
, u
∈ U

(cid:54) Bµ2 .

(156)

(cid:54) Bµ1 ,

|
∂θl1 ∂θl2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
Rm, there exists a positive constant BΨ, such that
∈
Ψ(x, u, θ)
(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:107)2 (cid:54) BΨ <

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∞

,

(157)

For all x

∈ X

(A4) Assumption on the likelihood ratio derivative:

where

.
(cid:107)

(cid:107)2 is the Euclidean L2 norm.

29

(A5) Assumptions on the approximation space given by Φ:

The columns of the matrix Φ are independent, that is, the form a basis of dimension k. The
φl(cid:107)2 (cid:54) 1
norms of the columns vectors of the matrix Φ are bounded above by 1, that is,
(cid:107)
for 1 (cid:54) l (cid:54) k.

(A6) Assumptions on the learning rate:

a(n) =

,

∞

a2(n) <

.

∞

n
(cid:88)

n
(cid:88)

(158)

Theorem. The algorithm converged if
where the updates are zero. We assume that
point.

∇θ ˜η(θ) = 0, since the actor reached a stationary point
(cid:107)∇θ ˜η(θ)
hints at how close we are to the convergence
(cid:107)

The next theorem from DiCastro & Meir [14] implies that the trajectory visits a neighborhood of
a local maximum inﬁnitely often. Although it may leave the local vicinity of the maximum, it is
guaranteed to return to it inﬁnitely often.
Theorem 11 (DiCastro & Meir). Deﬁne

B

˜η =

∇

B∆td1
Γw

+

B∆td2
Γη

+ B∆td3 (cid:15)app ,

(159)

where B∆td1, B∆td2, and B∆td3 are ﬁnite constants depending on the Markov decision process and
the agent parameters.

Under above assumptions

lim
t
→∞
The trajectory visits a neighborhood of a local maximum inﬁnitely often.

(cid:107)∇θ ˜η(θt)
(cid:107)

inf

˜η .

∇

(cid:54) B

(160)

Comments.

maximum.

(C1) The larger the critic learning rates Γw and Γη are, the smaller is the region around the local

(C2) The results are in agreement with those of Zhang 2007 [61].
(C3) Even if the results are derived for a special actor-critic setting, they carry over to a more

general setting of the iterates.

A3 ADAM Optimization as Stochastic Heavy Ball with Friction

The Nesterov Accelerated Gradient Descent (NAGD) [47] has raised considerable interest due to its
numerical simplicity and its low complexity. Previous to NAGD and its derived methods there was
Polyak’s Heavy Ball method [49]. The idea of the Heavy Ball is a ball that evolves over the graph of
a function f with damping (due to friction) and acceleration. Therefore, this second-order dynamical
system can be described by the ODE for the Heavy Ball with Friction (HBF) [17]:
¨θt + a(t) ˙θt +

f (θt) = 0 ,
∇
where a(n) is the damping coefﬁcient with a(n) = a
nβ for β
integro-differential equation

∈

(161)
(0, 1]. This ODE is equivalent to the

˙θt =

1
k(t)

−

t

h(s)

f (θs)ds ,

∇

(162)

0
(cid:90)
where k and h are two memory functions related to a(t). For polynomially memoried HBF we have
k(t) = tα+1 and h(t) = (α + 1)tα for some positive α, and for exponentially memoried HBF we
have k(t) = λ exp(λ t) and h(t) = exp(λ t). For the sum of the learning rates, we obtain
ln(n) + γ + 1
n1−β
β
1

for β = 1
for β < 1

2n + O

a(l) = a

(163)

1
n2

n

,

(cid:40)

(cid:0)

(cid:1)

(cid:88)l=1

−

30

(164)

(165)

(166)

(167)

(168)

(169)

(170)

where γ = 0.5772156649 is the Euler-Mascheroni constant.

Gadat et al. derived a discrete and stochastic version of the HBF [17]:

θn+1 = θn −
mn+1 = mn + a(n + 1) r(n)

a(n + 1) mn

where

f (θn)

mn

+ a(n + 1) r(n) Mn+1 ,

∇

(cid:0)

−

(cid:1)

r(n) =

r

(cid:80)n

r
l=1 a(l)

(cid:40)

for exponentially memoried HBF
for polynomially memoried HBF

.

This recursion can be rewritten as
θn+1 = θn −
mn+1 =
−

a(n + 1) mn
a(n + 1) r(n)

1

∇
The recursion Eq. (166) is the ﬁrst moment update of ADAM [29].

(cid:0)

(cid:1)

(cid:0)

(cid:1)

mn + a(n + 1) r(n)

f (θn) + Mn+1

.

For the term r(n)a(n) we obtain for the polynomial memory the approximations

r(n) a(n)

r

≈

1
n log n
1

β

(cid:40)

−
n

for β = 1
for β < 1

,

Gadat et al. showed that the recursion Eq. (164) converges for functions with at most quadratic grow
[17]. The authors mention that convergence can be proofed for functions f that are L-smooth, that is,
the gradient is L-Lipschitz.

Kingma et al. [29] state in Theorem 4.1 convergence of ADAM while assuming that β1, the ﬁrst
moment running average coefﬁcient, decays exponentially. Furthermore they assume that β2
< 1
1
√β2
and the learning rate αt decays with αt = α
√t .

ADAM divides mn of the recursion Eq. (166) by the bias-corrected second raw moment estimate.
Since the bias-corrected second raw moment estimate changes slowly, we consider it as an error.

1

√v + ∆v ≈

1
√v −

1
2 v √v

∆v + O(∆v2) .

ADAM assumes the second moment E

g2

to be stationary with its approximation vn:

(cid:2)
vn =

(cid:3)
1
1

−
−

β2
βn
2

n

(cid:88)l=1

βn
2

l

−

g2
l .

∆nvn = vn −

vn

1 =

−

n

(cid:88)l=1
β2)

β2
βn
2

1
1

−
−
β2 (1
1

−
βn
2

−

βn
2

l
−

g2
l −

n

1

−

1

1

−

β2
βn
2

−

1

−

βn
2

l

1

−

−

g2
l

(171)

(cid:88)l=1
β2
βn
2

−

−

1

1

1

−

n

1

−

(cid:88)l=1

βn
2

l
−

−

1

g2
l

=

=

=

1
1

1
1

1
1

−
−

−
−

−
−

β2
βn
2

β2
βn

β2
βn

g2
n +

βn
2

l

1

−

−

g2
l −

g2
n +

β2 −

2 (cid:32)

g2
n −

2 (cid:32)

1

(cid:0)

1

−

β2
βn
2

−

1

−

1

1

−

βn
2
βn
2

−

1

−

n

1

−

βn
2

l

1

−

−

g2
l

(cid:33)

(cid:88)l=1
1

−

(cid:1)
βn
2

−

l

g2
l

.

(cid:33)

n

1

−

(cid:88)l=1

n

1

−

(cid:88)l=1

31

Therefore

E [∆nvn] = E [vn −
β2
βn
2

1
1

=

−
−

vn

1] =

−

1
1

−
−

β2
βn

2 (cid:32)

E

g2

E

g2

−

(cid:0)

(cid:2)

(cid:3)

(cid:2)

(cid:3)(cid:1)

E

g2

(cid:3)

(cid:2)
= 0 .

1

−

β2
βn
2

−

1

−

−

1

n

1

−

(cid:88)l=1

βn
2

l

1

−

−

E

g2

(172)

(cid:33)
(cid:3)

(cid:2)

We are interested in the difference of actual stochastic vn to the true stationary v:

∆vn = vn −

v =

1
1

−
−

β2
βn
2

n

(cid:88)l=1

βn
2

l

−

g2
l −

v

.

(cid:0)

(cid:1)

(173)

αa(n + 1)r(n), we have ∆vn ∝

1/(2v√v)∆vn + O(∆2vn). If we set M (v)
mn/√v + a(n + 1)r(n)M (v)
n+1 and E

For a stationary second moment of mn and β2 = 1
a(n +
1)r(n). We use a linear approximation to ADAM’s second moment normalization 1/√v + ∆vn ≈
(mn∆vn)/(2v√va(n + 1)r(n)), then
1/√v
−
g2
mn/√vn ≈
= 0. For a
l −
stationary second moment of mn,
M (v)
second moment. Therefore
{
factor 1/√v can be incorporated into a(n + 1) and r(n).

is a martingale difference sequence with a bounded
in update rules Eq. (166). The

M (v)
n
{
can be subsumed into
n+1}

(cid:105)
Mn+1}

= 0, since E

−
M (v)
n+1

n+1 =

−

}

{

v

(cid:104)

(cid:3)

(cid:2)

A4 Experiments: Additional Information

A4.1 WGAN-GP on Image Data.

Table A2: The performance of WGAN-GP trained with the original procedure and with TTUR on
CIFAR-10 and LSUN Bedrooms. We compare the performance with respect to the FID at the optimal
number of iterations during training and wall-clock time in minutes.

dataset

method

b, a

iter

time(m)

FID method

b = a

time(m)

CIFAR-10 TTUR
TTUR
LSUN

3e-4, 1e-4
3e-4, 1e-4

168k
80k

700
1900

24.8
9.5

orig
orig

1e-4
1e-4

iter

53k
23k

800
2010

FID

29.3
20.5

32

A4.2 WGAN-GP on the One Billion Word Benchmark.

Table A3: Samples generated by WGAN-GP trained on fhe One Billion Word benchmark with TTUR
(left) the original method (right).

Dry Hall Sitning tven the concer
There are court phinchs hasffort
He scores a supponied foutver il
Bartfol reportings ane the depor
Seu hid , it ’s watter ’s remold
Later fasted the store the inste
Indiwezal deducated belenseous K
Starfers on Rbama ’s all is lead
Inverdick oper , caldawho ’s non
She said , five by theically rec
RichI , Learly said remain .‘‘‘‘
Reforded live for they were like
The plane was git finally fuels
The skip lifely will neek by the
SEW McHardy Berfect was luadingu
But I pol rated Franclezt is the

No say that tent Franstal at Bra
Caulh Paphionars tven got corfle
Resumaly , braaky facting he at
On toipe also houd , aid of sole
When Barrysels commono toprel to
The Moster suprr tent Elay diccu
The new vebators are demases to
Many ’s lore wockerssaow 2 2 ) A
Andly , has le wordd Uold steali
But be the firmoters is no 200 s
Jermueciored a noval wan ’t mar
Onles that his boud-park , the g
ISLUN , The crather wilh a them
Fow 22o2 surgeedeto , theirestra
Make Sebages of intarmamates , a
Gullla " has cautaria Thoug ly t

Table A4: The performance of WGAN-GP trained with the original procedure and with TTUR on the
One Billion Word Benchmark. We compare the performance with respect to the JSD at the optimal
number of iterations and wall-clock time in minutes during training. WGAN-GP trained with TTUR
exhibits consistently a better FID.

n-gram method

b, a

iter

time(m)

JSD method

b = a

time(m)

4-gram TTUR
6-gram TTUR

3e-4, 1e-4
3e-4, 1e-4

98k
100k

1150
1120

0.35
0.74

orig
orig

1e-4
1e-4

iter

33k
32k

JSD

0.38
0.77

1040
1070

A4.3 BEGAN

The Boundary Equilibrium GAN (BEGAN) [6] maintains an equilibrium between the discriminator
and generator loss (cf. Section 3.3 in [6])

which, in turn, also leads to a ﬁxed relation between the two gradients, therefore, a two time-scale
update is not ensured by solely adjusting the learning rates. Indeed, for stable learning rates, we see
no differences in the learning progress between orig and TTUR as depicted in Figure A13.

E[

(G(z))] = γE[

(x)]

L

L

(174)

Figure A13: Mean, maximum and minimum FID over eight runs for BEGAN training on CelebA
and LSUN Bedrooms. TTUR learning rates are given as pairs (b, a) of discriminator learning rate
b and generator learning rate a: “TTUR b a”. Left: CelebA, starting at mini-batch 10k for better
visualisation. Right: LSUN Bedrooms. Orig and TTUR behave similar. For BEGAN we cannot
ensure TTUR by adjusting learning rates.

33

A5 Discriminator vs. Generator Learning Rate

The convergence proof for learning GANs with TTUR assumes that the generator learning rate will
eventually become small enough to ensure convergence of the discriminator learning. At some time
point, the perturbations of the discriminator updates by updates of the generator parameters are
sufﬁcient small to assure that the discriminator converges. Crucial for discriminator convergence is
the magnitude of the perturbations which the generator induces into the discriminator updates. These
perturbations are not only determined by the generator learning rate but also by its loss function,
current value of the loss function, optimization method, size of the error signals that reach the
generator (vanishing or exploding gradient), complexity of generator’s learning task, architecture of
the generator, regularization, and others. Consequently, the size of generator learning rate does not
solely determine how large the perturbations of the discriminator updates are but serve to modulate
them. Thus, the generator learning rate may be much larger than the discriminator learning rate
without inducing large perturbation into the discriminator learning.

Even the learning dynamics of the generator is different from the learning dynamics of the discrimi-
nator, though they both have the same learning rate. Figure A14 shows the loss of the generator and
the discriminator for an experiment with DCGAN on CelebA, where the learning rate was 0.0005
for both the discriminator and the generator. However, the discriminator loss is decreasing while
the generator loss is increasing. This example shows that the learning rate neither determines the
perturbations nor the progress in learning for two coupled update rules. The choice of the learning
rate for the generator should be independent from choice for the discriminator. Also the search ranges
of discriminator and generator learning rates should be independent from each other, but adjusted to
the corresponding architecture, task, etc.

Figure A14: The respective losses of the discriminator and the generator show the different learning
dynamics of the two networks.

A6 Used Software, Datasets, Pretrained Models, and Implementations

We used the following datasets to evaluate GANs: The Large-scale CelebFaces Attributes (CelebA)
dataset, aligned and cropped [41], the training dataset of the bedrooms category of the large scale
image database (LSUN) [60], the CIFAR-10 training dataset [34], the Street View House Numbers
training dataset (SVHN) [48], and the One Billion Word Benchmark [12].

All experiments rely on the respective reference implementations for the corresponding GAN model.
The software framework for our experiments was Tensorﬂow 1.3 [1, 2] and Python 3.6. We used
following software, datasets and pretrained models:

•

BEGAN in Tensorﬂow, https://github.com/carpedm20/BEGAN-tensorflow, Fixed
random seeds removed. Accessed: 2017-05-30

34

•

•

•

•

•

DCGAN in Tensorﬂow, https://github.com/carpedm20/DCGAN-tensorflow, Fixed
random seeds removed. Accessed: 2017-04-03
Improved Training of Wasserstein GANs, image model, https://github.com/igul222/
improved_wgan_training/blob/master/gan_64x64.py, Accessed: 2017-06-12
language model, https://github.com/
Improved Training of Wasserstein GANs,
igul222/improved_wgan_training/blob/master/gan_language.py, Accessed:
2017-06-12
Inception-v3
imagenet/inception-2015-12-05.tgz, Accessed: 2017-05-02

http://download.tensorflow.org/models/image/

pretrained,

Implementations are available at

https://github.com/bioinf-jku/TTUR

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis,
J. Dean, M. Devin, S. Ghemawat, I. J. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Józefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. G.
Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. A. Tucker,
V. Vanhoucke, V. Vasudevan, F. B. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke,
Y. Yu, and X. Zheng. Tensorﬂow: Large-scale machine learning on heterogeneous distributed
systems. arXiv e-prints, arXiv:1603.04467, 2016.

[2] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving,
M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker,
V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng. Tensorﬂow: A system for large-
In 12th USENIX Symposium on Operating Systems Design and
scale machine learning.
Implementation (OSDI 16), pages 265–283, 2016.

[3] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein GAN. arXiv e-prints, arXiv:1701.07875,

2017.

[4] S. Arora, R. Ge, Y. Liang, T. Ma, and Y. Zhang. Generalization and equilibrium in generative
In D. Precup and Y. W. Teh, editors, Proceedings of the 34th
adversarial nets (GANs).
International Conference on Machine Learning, Proceedings of Machine Learning Research,
vol. 70, pages 224–232, 2017.

[5] H. Attouch, X. Goudou, and P. Redont. The heavy ball with friction method, I. the continu-
ous dynamical system: Global exploration of the local minima of a real-valued function by
asymptotic analysis of a dissipative dynamical system. Communications in Contemporary
Mathematics, 2(1):1–34, 2000.

[6] D. Berthelot, T. Schumm, and L. Metz. BEGAN: Boundary equilibrium generative adversarial

networks. arXiv e-prints, arXiv:1703.10717, 2017.

[7] D. P. Bertsekas and J. N. Tsitsiklis. Gradient convergence in gradient methods with errors.

SIAM Journal on Optimization, 10(3):627–642, 2000.

[8] S. Bhatnagar, H. L. Prasad, and L. A. Prashanth. Stochastic Recursive Algorithms for Optimiza-
tion. Lecture Notes in Control and Information Sciences. Springer-Verlag London, 2013.
[9] V. S. Borkar. Stochastic approximation with two time scales. Systems & Control Letters,

29(5):291–294, 1997.

[10] V. S. Borkar and S. P. Meyn. The O.D.E. method for convergence of stochastic approximation
and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447–469, 2000.
[11] T. Che, Y. Li, A. P. Jacob, Y. Bengio, and W. Li. Mode regularized generative adversarial
networks. In Proceedings of the International Conference on Learning Representations (ICLR),
2017. arXiv:1612.02136.

[12] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson. One billion
word benchmark for measuring progress in statistical language modeling. arXiv e-prints,
arXiv:1312.3005, 2013.

35

[13] D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by
exponential linear units (ELUs). In Proceedings of the International Conference on Learning
Representations (ICLR), 2016. arXiv:1511.07289.

[14] D. DiCastro and R. Meir. A convergent online single time scale actor critic algorithm. J. Mach.

Learn. Res., 11:367–410, 2010.

[15] D. C. Dowson and B. V. Landau. The Fréchet distance between multivariate normal distributions.

Journal of Multivariate Analysis, 12:450–455, 1982.

[16] M. Fréchet. Sur la distance de deux lois de probabilité. C. R. Acad. Sci. Paris, 244:689–692,

[17] S. Gadat, F. Panloup, and S. Saadane. Stochastic heavy ball. arXiv e-prints, arXiv:1609.04228,

1957.

2016.

[18] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems
27, pages 2672–2680, 2014.

[19] I. J. Goodfellow. On distinguishability criteria for estimating generative models. In Workshop
at the International Conference on Learning Representations (ICLR), 2015. arXiv:1412.6515.

[20] I. J. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. arXiv e-prints,

arXiv:1701.00160, 2017.

[21] X. Goudou and J. Munier. The gradient and heavy ball with friction dynamical systems: the

quasiconvex case. Mathematical Programming, 116(1):173–191, 2009.

[22] P. Grnarova, K. Y. Levy, A. Lucchi, T. Hofmann, and A. Krause. An online learning approach

to generative adversarial networks. arXiv e-prints, arXiv:1706.03269, 2017.

[23] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of
Wasserstein GANs. arXiv e-prints, arXiv:1704.00028, 2017. Advances in Neural Information
Processing Systems 31 (NIPS 2017).

[24] M. W. Hirsch. Convergent activation dynamics in continuous time networks. Neural Networks,

2(5):331–349, 1989.

[25] R. D. Hjelm, A. P. Jacob, T. Che, K. Cho, and Y. Bengio. Boundary-seeking generative

adversarial networks. arXiv e-prints, arXiv:1702.08431, 2017.

[26] S. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.

[27] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional
adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2017. arXiv:1611.07004.

[28] P. Karmakar and S. Bhatnagar. Two time-scale stochastic approximation with controlled Markov
noise and off-policy temporal-difference learning. Mathematics of Operations Research, 2017.

[29] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In Proceedings of the
International Conference on Learning Representations (ICLR)), 2015. arXiv:1412.6980.

[30] V. R. Konda. Actor-Critic Algorithms. PhD thesis, Department of Electrical Engineering and

Computer Science, Massachusetts Institute of Technology, 2002.

[31] V. R. Konda and V. S. Borkar. Actor-critic-type learning algorithms for Markov decision

processes. SIAM J. Control Optim., 38(1):94–123, 1999.

[32] V. R. Konda and J. N. Tsitsiklis. Linear stochastic approximation driven by slowly varying

Markov chains. Systems & Control Letters, 50(2):95–102, 2003.

[33] V. R. Konda and J. N. Tsitsiklis. Convergence rate of linear two-time-scale stochastic approxi-

mation. The Annals of Applied Probability, 14(2):796–819, 2004.

[34] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classiﬁcation with deep convolutional
neural networks. In Proceedings of the 25th International Conference on Neural Information
Processing Systems, pages 1097–1105, 2012.

[35] H. J. Kushner and G. G. Yin. Stochastic Approximation Algorithms and Recursive Algorithms

and Applications. Springer-Verlag New York, second edition, 2003.

36

[36] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. P. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi.
Photo-realistic single image super-resolution using a generative adversarial network. arXiv
e-prints, arXiv:1609.04802, 2016.

[37] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Póczos. MMD GAN: Towards deeper
understanding of moment matching network. In Advances in Neural Information Processing
Systems 31 (NIPS 2017), 2017. arXiv:1705.08584.

[38] J. Li, A. Madry, J. Peebles, and L. Schmidt. Towards understanding the dynamics of generative

adversarial networks. arXiv e-prints, arXiv:1706.09884, 2017.

[39] J. H. Lim and J. C. Ye. Geometric GAN. arXiv e-prints, arXiv:1705.02894, 2017.

[40] S. Liu, O. Bousquet, and K. Chaudhuri. Approximation and convergence properties of generative
adversarial learning. In Advances in Neural Information Processing Systems 31 (NIPS 2017),
2017. arXiv:1705.08991.

[41] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings

of International Conference on Computer Vision (ICCV), 2015.

[42] L. M. Mescheder, S. Nowozin, and A. Geiger. The numerics of GANs. In Advances in Neural

Information Processing Systems 31 (NIPS 2017), 2017. arXiv:1705.10461.

[43] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks.
In Proceedings of the International Conference on Learning Representations (ICLR), 2017.
arXiv:1611.02163.

[44] A. Mokkadem and M. Pelletier. Convergence rate and averaging of nonlinear two-time-scale
stochastic approximation algorithms. The Annals of Applied Probability, 16(3):1671–1702,
2006.

[45] Y. Mroueh and T. Sercu. Fisher GAN. In Advances in Neural Information Processing Systems

31 (NIPS 2017), 2017. arXiv:1705.09675.

[46] V. Nagarajan and J. Z. Kolter. Gradient descent GAN optimization is locally stable. arXiv
e-prints, arXiv:1706.04156, 2017. Advances in Neural Information Processing Systems 31
(NIPS 2017).

[47] Y. Nesterov. A method of solving a convex programming problem with convergence rate

o(1/k2). Soviet Mathematics Doklady, 27:372–376, 1983.

[48] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in nat-
ural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and
Unsupervised Feature Learning 2011, 2011.

[49] B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR

Computational Mathematics and Mathematical Physics, 4(5):1–17, 1964.

[50] H. L. Prasad, L. A. Prashanth, and S. Bhatnagar. Two-timescale algorithms for learning Nash
equilibria in general-sum stochastic games. In Proceedings of the 2015 International Conference
on Autonomous Agents and Multiagent Systems (AAMAS ’15), pages 1371–1379, 2015.

[51] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convo-
lutional generative adversarial networks. In Proceedings of the International Conference on
Learning Representations (ICLR), 2016. arXiv:1511.06434.

[52] A. Ramaswamy and S. Bhatnagar. Stochastic recursive inclusion in two timescales with an

application to the lagrangian dual problem. Stochastics, 88(8):1173–1187, 2016.

[53] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved
techniques for training GANs. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 2234–2242,
2016.

[54] V. B. Tadi´c. Almost sure convergence of two time-scale stochastic approximation algorithms.
In Proceedings of the 2004 American Control Conference, volume 4, pages 3802–3807, 2004.

[55] L. Theis, A. van den Oord, and M. Bethge. A note on the evaluation of generative models.
In Proceedings of the International Conference on Learning Representations (ICLR), 2016.
arXiv:1511.01844.

37

[56] I. Tolstikhin, S. Gelly, O. Bousquet, C.-J. Simon-Gabriel, and B. Schölkopf. AdaGAN: Boosting
generative models. arXiv e-prints, arXiv:1701.02386, 2017. Advances in Neural Information
Processing Systems 31 (NIPS 2017).

[57] R. Wang, A. Cully, H. J. Chang, and Y. Demiris. MAGAN: margin adaptation for generative

adversarial networks. arXiv e-prints, arXiv:1704.03817, 2017.

[58] L. N. Wasserstein. Markov processes over denumerable products of spaces describing large

systems of automata. Probl. Inform. Transmission, 5:47–52, 1969.

[59] Y. Wu, Y. Burda, R. Salakhutdinov, and R. B. Grosse. On the quantitative analysis of decoder-
based generative models. In Proceedings of the International Conference on Learning Repre-
sentations (ICLR), 2017. arXiv:1611.04273.

[60] F. Yu, Y. Zhang, S. Song, A. Seff, and J. Xiao. LSUN: construction of a large-scale image
dataset using deep learning with humans in the loop. arXiv e-prints, arXiv:1506.03365, 2015.
[61] J. Zhang, D. Zheng, and M. Chiang. The impact of stochastic noisy feedback on distributed
network utility maximization. In IEEE INFOCOM 2007 - 26th IEEE International Conference
on Computer Communications, pages 222–230, 2007.

List of Figures

Oscillation in GAN training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Heavy Ball with Friction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
. . . . . . . . . . . . . . . . . . . . . . .
FID evaluated for different disturbances
3
. . . . . . . . . . . . . . . . . .
TTUR and single time-scale update with toy data.
4
. . . . . .
FID for DCGAN on CelebA, CIFAR-10, SVHN, and LSUN Bedrooms.
5
FID for WGAN-GP trained on CIFAR-10 and LSUN Bedrooms.
. . . . . . . . . .
6
Performance of WGAN-GP on One Billion Word. . . . . . . . . . . . . . . . . . .
7
A8 FID and Inception Score Comparison . . . . . . . . . . . . . . . . . . . . . . . . .
A9 CelebA Samples with FID 500 and 300 . . . . . . . . . . . . . . . . . . . . . . . .
A10 CelebA Samples with FID 133 and 100 . . . . . . . . . . . . . . . . . . . . . . . .
A11 CelebA Samples with FID 45 and 13 . . . . . . . . . . . . . . . . . . . . . . . . .
A12 CelebA Samples with FID 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A13 FID for BEGAN trained on CelebA and LSUN Bedrooms.
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
A14 Learning dynamics of two networks.

List of Tables

1 Results DCGAN and WGAN-GP . . . . . . . . . . . . . . . . . . . . . . . . . . .
A2 Results WGAN-GP on Image Data
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
A3 Samples of the One Billion Word benchmark generated by WGAN-GP.
A4 Results WGAN-GP on One Billion Word . . . . . . . . . . . . . . . . . . . . . . .

2
4
6
7
8
8
9
13
14
14
15
15
33
34

9
32
33
33

38

8
1
0
2
 
n
a
J
 
2
1
 
 
]

G
L
.
s
c
[
 
 
6
v
0
0
5
8
0
.
6
0
7
1
:
v
i
X
r
a

GANs Trained by a Two Time-Scale Update Rule
Converge to a Local Nash Equilibrium

Martin Heusel

Hubert Ramsauer

Thomas Unterthiner

Bernhard Nessler

Sepp Hochreiter

LIT AI Lab & Institute of Bioinformatics,
Johannes Kepler University Linz
A-4040 Linz, Austria
{mhe,ramsauer,unterthiner,nessler,hochreit}@bioinf.jku.at

Abstract

Generative Adversarial Networks (GANs) excel at creating realistic images with
complex models for which maximum likelihood is infeasible. However, the con-
vergence of GAN training has still not been proved. We propose a two time-scale
update rule (TTUR) for training GANs with stochastic gradient descent on ar-
bitrary GAN loss functions. TTUR has an individual learning rate for both the
discriminator and the generator. Using the theory of stochastic approximation, we
prove that the TTUR converges under mild assumptions to a stationary local Nash
equilibrium. The convergence carries over to the popular Adam optimization, for
which we prove that it follows the dynamics of a heavy ball with friction and thus
prefers ﬂat minima in the objective landscape. For the evaluation of the perfor-
mance of GANs at image generation, we introduce the ‘Fréchet Inception Distance”
(FID) which captures the similarity of generated images to real ones better than
the Inception Score. In experiments, TTUR improves learning for DCGANs and
Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN train-
ing on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word
Benchmark.

Introduction

Generative adversarial networks (GANs) [18] have achieved outstanding results in generating realistic
images [51, 36, 27, 3, 6] and producing text [23]. GANs can learn complex generative models for
which maximum likelihood or a variational approximations are infeasible. Instead of the likelihood,
a discriminator network serves as objective for the generative model, that is, the generator. GAN
learning is a game between the generator, which constructs synthetic data from random variables,
and the discriminator, which separates synthetic data from real world data. The generator’s goal is
to construct data in such a way that the discriminator cannot tell them apart from real world data.
Thus, the discriminator tries to minimize the synthetic-real discrimination error while the generator
tries to maximize this error. Since training GANs is a game and its solution is a Nash equilibrium,
gradient descent may fail to converge [53, 18, 20]. Only local Nash equilibria are found, because
gradient descent is a local optimization method. If there exists a local neighborhood around a point
in parameter space where neither the generator nor the discriminator can unilaterally decrease their
respective losses, then we call this point a local Nash equilibrium.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

Figure 1: Left: Original vs. TTUR GAN training on CelebA. Right: Figure from Zhang 2007 [61]
which shows the distance of the parameter from the optimum for a one time-scale update of a 4 node
network ﬂow problem. When the upper bounds on the errors (α, β) are small, the iterates oscillate
and repeatedly return to a neighborhood of the optimal solution (see also Appendix Section A2.3).
However, when the upper bounds on the errors are large, the iterates typically diverge.

To characterize the convergence properties of training general GANs is still an open challenge [19, 20].
For special GAN variants, convergence can be proved under certain assumptions [39, 22, 56]. A
prerequisit for many convergence proofs is local stability [35] which was shown for GANs by
Nagarajan and Kolter [46] for a min-max GAN setting. However, Nagarajan and Kolter require for
their proof either rather strong and unrealistic assumptions or a restriction to a linear discriminator.
Recent convergence proofs for GANs hold for expectations over training samples or for the number
of examples going to inﬁnity [37, 45, 40, 4], thus do not consider mini-batch learning which leads to
a stochastic gradient [57, 25, 42, 38].

Recently actor-critic learning has been analyzed using stochastic approximation. Prasad et al. [50]
showed that a two time-scale update rule ensures that training reaches a stationary local Nash
equilibrium if the critic learns faster than the actor. Convergence was proved via an ordinary
differential equation (ODE), whose stable limit points coincide with stationary local Nash equilibria.
We follow the same approach. We prove that GANs converge to a local Nash equilibrium when trained
by a two time-scale update rule (TTUR), i.e., when discriminator and generator have separate learning
rates. This also leads to better results in experiments. The main premise is that the discriminator
converges to a local minimum when the generator is ﬁxed. If the generator changes slowly enough,
then the discriminator still converges, since the generator perturbations are small. Besides ensuring
convergence, the performance may also improve since the discriminator must ﬁrst learn new patterns
before they are transferred to the generator. In contrast, a generator which is overly fast, drives the
discriminator steadily into new regions without capturing its gathered information. In recent GAN
implementations, the discriminator often learned faster than the generator. A new objective slowed
down the generator to prevent it from overtraining on the current discriminator [53]. The Wasserstein
GAN algorithm uses more update steps for the discriminator than for the generator [3]. We compare
TTUR and standard GAN training. Fig. 1 shows at the left panel a stochastic gradient example on
CelebA for original GAN training (orig), which often leads to oscillations, and the TTUR. On the
right panel an example of a 4 node network ﬂow problem of Zhang et al. [61] is shown. The distance
between the actual parameter and its optimum for an one time-scale update rule is shown across
iterates. When the upper bounds on the errors are small, the iterates return to a neighborhood of the
optimal solution, while for large errors the iterates may diverge (see also Appendix Section A2.3).

Our novel contributions in this paper are:

The two time-scale update rule for GANs,

•

•

•

•

•

We proof that GANs trained with TTUR converge to a stationary local Nash equilibrium,

The description of Adam as heavy ball with friction and the resulting second order differential
equation,

The convergence of GANs trained with TTUR and Adam to a stationary local Nash equilib-
rium,

We introduce the “Fréchet Inception Distance” (FID) to evaluate GANs, which is more
consistent than the Inception Score.

2

Two Time-Scale Update Rule for GANs

LG. The loss functions

We consider a discriminator D(.; w) with parameter vector w and a generator G(.; θ) with parameter
vector θ. Learning is based on a stochastic gradient ˜g(θ, w) of the discriminator’s loss function
LD
and a stochastic gradient ˜h(θ, w) of the generator’s loss function
LD and
LG can be the original as introduced in Goodfellow et al. [18], its improved versions [20], or recently
proposed losses for GANs like the Wasserstein GAN [3]. Our setting is not restricted to min-max
GANs, but is valid for all other, more general GANs for which the discriminator’s loss function
LD
θ, w
is not necessarily related to the generator’s loss function
are stochastic, since they use mini-batches of m real world samples x(i), 1 (cid:54) i (cid:54) m and m synthetic
(cid:0)
(cid:1)
samples z(i), 1 (cid:54) i (cid:54) m which are randomly chosen. If the true gradients are g(θ, w) =
∇wLD and
∇θLG, then we can deﬁne ˜g(θ, w) = g(θ, w) +M (w) and ˜h(θ, w) = h(θ, w) +M (θ)
h(θ, w) =
with random variables M (w) and M (θ). Thus, the gradients ˜g
are stochastic
approximations to the true gradients. Consequently, we analyze convergence of GANs by two
time-scale stochastic approximations algorithms. For a two time-scale update rule (TTUR), we use
the learning rates b(n) and a(n) for the discriminator and the generator update, respectively:

LG. The gradients ˜g

and ˜h

and ˜h

θ, w

θ, w

θ, w

(cid:0)

(cid:1)

(cid:1)

(cid:0)

(cid:0)

(cid:1)

wn+1 = wn + b(n)

g

θn, wn

+ M (w)

n

, θn+1 = θn + a(n)

h

θn, wn

+ M (θ)

n

.

(1)

For more details on the following convergence proof and its assumptions see Appendix Section A2.1.
To prove convergence of GANs learned by TTUR, we make the following assumptions (The actual
assumption is ended by (cid:74), the following text are just comments and explanations):

(cid:16)

(cid:0)

(cid:1)

(cid:17)

(cid:16)

(cid:0)

(cid:1)

(cid:17)

(A1) The gradients h and g are Lipschitz. (cid:74) Consequently, networks with Lipschitz smooth
activation functions like ELUs (α = 1) [13] fulﬁll the assumption but not ReLU networks.

,

∞

∞

(A2)

n b(n) =

n a(n) =

(cid:80)
w.r.t.

n a2(n) <
(A3) The stochastic gradient errors
(cid:80)
the increasing σ-ﬁeld
(θ)
n
| F

n b2(n) <
are martingale difference sequences
, l (cid:54) n), n (cid:62) 0 with
(cid:54) B2, where B1 and B2 are positive
2
E
deterministic constants.(cid:74) The original Assumption (A3) from Borkar 1997 follows from
Lemma 2 in [7] (see also [52]). The assumption is fulﬁlled in the Robbins-Monro setting,
where mini-batches are randomly sampled and the gradients are bounded.

,
M (θ)
(cid:80)
n
{
Fn = σ(θl, wl, M (θ)

,
∞
M (w)
(cid:80)
n
}
{

(cid:54) B1 and E

, a(n) = o(b(n))(cid:74)

M (θ)
n
(cid:107)

(w)
n
| F

, M (w)
l

M (w)
n

(cid:107)
(cid:104)

2
(cid:107)

and

∞

(cid:107)

}

(cid:104)

(cid:105)

(cid:105)

l

(A4) For each θ, the ODE ˙w(t) = g

θ, w(t)

(cid:1)

(cid:0)

(cid:0)

θ(t), λ(θ(t))

has a local asymptotically stable attractor
λ(θ) within a domain of attraction Gθ such that λ is Lipschitz. The ODE ˙θ(t) =
(cid:1)
h
has a local asymptotically stable attractor θ∗ within a domain of
attraction.(cid:74) The discriminator must converge to a minimum for ﬁxed generator param-
eters and the generator, in turn, must converge to a minimum for this ﬁxed discriminator
minimum. Borkar 1997 required unique global asymptotically stable equilibria [9]. The
assumption of global attractors was relaxed to local attractors via Assumption (A6) and
Theorem 2.7 in Karmakar & Bhatnagar [28]. See for more details Assumption (A6) in the
Appendix Section A2.1.3. Here, the GAN objectives may serve as Lyapunov functions.
These assumptions of locally stable ODEs can be ensured by an additional weight decay term
in the loss function which increases the eigenvalues of the Hessian. Therefore, problems
with a region-wise constant discriminator that has zero second order derivatives are avoided.
For further discussion see Appendix Section A2 (C3).

(A5) supn (cid:107)

θn(cid:107)
decay term.

<

and supn (cid:107)

wn(cid:107)

<

∞

∞

.(cid:74) Typically ensured by the objective or a weight

The next theorem has been proved in the seminal paper of Borkar 1997 [9].
Theorem 1 (Borkar). If the assumptions are satisﬁed, then the updates Eq. (1) converge to
(θ∗, λ(θ∗)) a.s.

The solution (θ∗, λ(θ∗)) is a stationary local Nash equilibrium [50], since θ∗ as well as λ(θ∗) are
θ∗, λ(θ∗)
local asymptotically stable attractors with g
= 0. An alternative
approach to the proof of convergence using the Poisson equation for ensuring a solution to the fast

= 0 and h

θ∗, λ(θ∗)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

3

update rule can be found in the Appendix Section A2.1.2. This approach assumes a linear update
function in the fast update rule which, however, can be a linear approximation to a nonlinear gradient
[30, 32]. For the rate of convergence see Appendix Section A2.2, where Section A2.2.1 focuses on
linear and Section A2.2.2 on non-linear updates. For equal time-scales it can only be proven that
the updates revisit an environment of the solution inﬁnitely often, which, however, can be very large
[61, 14]. For more details on the analysis of equal time-scales see Appendix Section A2.3. The main
idea of the proof of Borkar [9] is to use (T, δ) perturbed ODEs according to Hirsch 1989 [24] (see
also Appendix Section C of Bhatnagar, Prasad, & Prashanth 2013 [8]). The proof relies on the fact
that there eventually is a time point when the perturbation of the slow update rule is small enough
(given by δ) to allow the fast update rule to converge. For experiments with TTUR, we aim at ﬁnding
learning rates such that the slow update is small enough to allow the fast to converge. Typically,
the slow update is the generator and the fast update the discriminator. We have to adjust the two
learning rates such that the generator does not affect discriminator learning in a undesired way and
perturb it too much. However, even a larger learning rate for the generator than for the discriminator
may ensure that the discriminator has low perturbations. Learning rates cannot be translated directly
into perturbation since the perturbation of the discriminator by the generator is different from the
perturbation of the generator by the discriminator.

Adam Follows an HBF ODE and Ensures TTUR Convergence

In our experiments, we aim at using Adam stochastic approximation to avoid mode collapsing. GANs
suffer from “mode collapsing” where large masses of probability are mapped onto a few modes
that cover only small regions. While these regions represent meaningful samples, the variety of the
real world data is lost and only few prototype samples are
generated. Different methods have been proposed to avoid
mode collapsing [11, 43]. We obviate mode collapsing by
using Adam stochastic approximation [29]. Adam can be
described as Heavy Ball with Friction (HBF) (see below),
since it averages over past gradients. This averaging cor-
responds to a velocity that makes the generator resistant
to getting pushed into small regions. Adam as an HBF
method typically overshoots small local minima that cor-
respond to mode collapse and can ﬁnd ﬂat minima which
generalize well [26]. Fig. 2 depicts the dynamics of HBF,
where the ball settles at a ﬂat minimum. Next, we analyze
whether GANs trained with TTUR converge when using
Adam. For more details see Appendix Section A3.

Figure 2: Heavy Ball with Friction, where the
ball with mass overshoots the local minimum
θ+ and settles at the ﬂat minimum θ∗.

We recapitulate the Adam update rule at step n, with learning rate a, exponential averaging factors β1
for the ﬁrst and β2 for the second moment of the gradient

f (θn

1):

∇

−

(2)

gn ←− ∇
mn ←−
vn ←−
θn ←−

f (θn
(β1/(1
(β2/(1
θn

−

1)
βn
1 )) mn
1 + ((1
−
βn
2 )) vn
1 + ((1
−
a mn/(√vn + (cid:15)) ,

β1)/(1
−
β2)/(1
−

βn
1 )) gn
−
βn
2 )) gn (cid:12)
−

gn

τ for τ

−
−
1 −
−
, the square root √., and the
where following operations are meant componentwise: the product
division / in the last line. Instead of learning rate a, we introduce the damping coefﬁcient a(n) with
(0, 1]. Adam has parameters β1 for averaging the gradient and β2 parametrized
a(n) = an−
by a positive α for averaging the squared gradient. These parameters can be considered as deﬁning a
memory for Adam. To characterize β1 and β2 in the following, we deﬁne the exponential memory
n
r(n) = r and the polynomial memory r(n) = r/
l=1 a(l) for some positive constant r. The next
theorem describes Adam by a differential equation, which in turn allows to apply the idea of (T, δ)
(cid:80)
perturbed ODEs to TTUR. Consequently, learning GANs with TTUR and Adam converges.
Theorem 2. If Adam is used with β1 = 1
f
as the full gradient of the lower bounded, continuously differentiable objective f , then for stationary
second moments of the gradient, Adam follows the differential equation for Heavy Ball with Friction

αa(n + 1)r(n) and with

a(n + 1)r(n), β2 = 1

∇

(cid:12)

−

−

∈

4

(HBF):

Adam converges for gradients

∇

¨θt + a(t) ˙θt +
f that are L-Lipschitz.

∇

f (θt) = 0 .

(3)

Proof. Gadat et al. derived a discrete and stochastic version of Polyak’s Heavy Ball method [49], the
Heavy Ball with Friction (HBF) [17]:
θn+1 = θn −
mn+1 =
−

a(n + 1) mn ,
a(n + 1) r(n)

mn + a(n + 1) r(n)

f (θn) + Mn+1

(4)

∇

1

.

(cid:0)

(cid:1)

(cid:0)

These update rules are the ﬁrst moment update rules of Adam [29]. The HBF can be formulated as the
differential equation Eq. (3) [17]. Gadat et al. showed that the update rules Eq. (4) converge for loss
functions f with at most quadratic grow and stated that convergence can be proofed for
f that are
∇
L-Lipschitz [17]. Convergence has been proved for continuously differentiable f that is quasiconvex
f that is L-Lipschitz
(Theorem 3 in Goudou & Munier [21]). Convergence has been proved for
and bounded from below (Theorem 3.1 in Attouch et al. [5]). Adam normalizes the average mn by
gn]. mn is componentwise divided by
the second moments vn of of the gradient gn: vn = E [gn (cid:12)
the square root of the components of vn. We assume that the second moments of gn are stationary,
gn]. In this case the normalization can be considered as additional noise since the
i.e., v = E [gn (cid:12)
normalization factor randomly deviates from its mean. In the HBF interpretation the normalization
by √v corresponds to introducing gravitation. We obtain

∇

(cid:1)

n

1
1

β2
βn
2

n

1
1

β2
βn
2

l

βn
2

l

βn
2

−

−

−

(5)

v) .

(cid:88)l=1

(cid:88)l=1

−
−

v =

vn =

gl (cid:12)

gl −

(gl (cid:12)

gl , ∆vn = vn −

−
−
For a stationary second moment v and β2 = 1
αa(n + 1)r(n), we have ∆vn ∝
a(n + 1)r(n). We
use a componentwise linear approximation to Adam’s second moment normalization 1/√v + ∆vn ≈
∆vn + O(∆2vn), where all operations are meant componentwise. If
1/√v
−
we set M (v)
mn/√v + a(n +
∆vn)/(2v
1)r(n)M (v)
v] = 0. For a stationary second moment v,

(cid:12)
= 0, since E [gl (cid:12)
is a martingale difference sequence with a bounded second moment.
the random variable
in update rules Eq. (4). The factor 1/√v can
Mn+1}
Therefore
n+1}
{
be componentwise incorporated into the gradient g which corresponds to rescaling the parameters
without changing the minimum.

√v))
(cid:12)
(mn (cid:12)
M (v)
n+1
M (v)
(cid:104)
n
{
can be subsumed into

√va(n + 1)r(n)), then mn/√vn ≈

(1/(2v
(cid:12)
n+1 =
−
n+1 and E

M (v)
{

gl −

(cid:105)
}

2+f (θ(t))
According to Attouch et al. [5] the energy, that is, a Lyapunov function, is E(t) = 1/2
|
and ˙E(t) =
2 < 0. Since Adam can be expressed as differential equation and has a
|
Lyapunov function, the idea of (T, δ) perturbed ODEs [9, 24, 10] carries over to Adam. Therefore
the convergence of Adam with TTUR can be proved via two time-scale stochastic approximation
analysis like in Borkar [9] for stationary second moments of the gradient.

˙θ(t)
|

−

a

˙θ(t)
|

In the Appendix we further discuss the convergence of two time-scale stochastic approximation
algorithms with additive noise, linear update functions depending on Markov chains, nonlinear
update functions, and updates depending on controlled Markov processes. Futhermore, the Appendix
presents work on the rate of convergence for both linear and nonlinear update rules using similar
techniques as the local stability analysis of Nagarajan and Kolter [46]. Finally, we elaborate more on
equal time-scale updates, which are investigated for saddle point problems and actor-critic learning.

Experiments

Performance Measure. Before presenting the experiments, we introduce a quality measure for
models learned by GANs. The objective of generative learning is that the model produces data which
matches the observed data. Therefore, each distance between the probability of observing real world
data pw(.) and the probability of generating model data p(.) can serve as performance measure for
generative models. However, deﬁning appropriate performance measures for generative models

5

Figure 3: FID is evaluated for upper left: Gaussian noise, upper middle: Gaussian blur, upper
right: implanted black rectangles, lower left: swirled images, lower middle: salt and pepper noise,
and lower right: CelebA dataset contaminated by ImageNet images. The disturbance level rises
from zero and increases to the highest level. The FID captures the disturbance level very well by
monotonically increasing.

is difﬁcult [55]. The best known measure is the likelihood, which can be estimated by annealed
importance sampling [59]. However, the likelihood heavily depends on the noise assumptions for
the real data and can be dominated by single samples [55]. Other approaches like density estimates
have drawbacks, too [55]. A well-performing approach to measure the performance of GANs is the
“Inception Score” which correlates with human judgment [53]. Generated samples are fed into an
inception model that was trained on ImageNet. Images with meaningful objects are supposed to
have low label (output) entropy, that is, they belong to few object classes. On the other hand, the
entropy across images should be high, that is, the variance over the images should be large. Drawback
of the Inception Score is that the statistics of real world samples are not used and compared to the
statistics of synthetic samples. Next, we improve the Inception Score. The equality p(.) = pw(.)
pw(.)f (x)dx for a basis f (.)
holds except for a non-measurable set if and only if
spanning the function space in which p(.) and pw(.) live. These equalities of expectations are used
to describe distributions by moments or cumulants, where f (x) are polynomials of the data x. We
generalize these polynomials by replacing x by the coding layer of an inception model in order to
obtain vision-relevant features. For practical reasons we only consider the ﬁrst two polynomials, that
is, the ﬁrst two moments: mean and covariance. The Gaussian is the maximum entropy distribution
for given mean and covariance, therefore we assume the coding units to follow a multidimensional
Gaussian. The difference of two Gaussians (synthetic and real-world images) is measured by the
Fréchet distance [16] also known as Wasserstein-2 distance [58]. We call the Fréchet distance d(., .)
between the Gaussian with mean (m, C) obtained from p(.) and the Gaussian with mean (mw, Cw)
obtained from pw(.) the “Fréchet Inception Distance” (FID), which is given by [15]:

p(.)f (x)dx =

(cid:82)

(cid:82)

m
(cid:107)

2
2 + Tr

d2((m, C), (mw, Cw)) =

mw(cid:107)
Next we show that the FID is consistent with increasing disturbances and human judgment. Fig. 3
evaluates the FID for Gaussian noise, Gaussian blur, implanted black rectangles, swirled images,
salt and pepper noise, and CelebA dataset contaminated by ImageNet images. The FID captures the
disturbance level very well. In the experiments we used the FID to evaluate the performance of GANs.
For more details and a comparison between FID and Inception Score see Appendix Section A1,
where we show that FID is more consistent with the noise level than the Inception Score.

C + Cw −

CCw

(6)

−

2

(cid:1)

(cid:0)

(cid:0)

(cid:1)

.

1/2

Model Selection and Evaluation. We compare the two time-scale update rule (TTUR) for GANs
with the original GAN training to see whether TTUR improves the convergence speed and per-
formance of GANs. We have selected Adam stochastic optimization to reduce the risk of mode
collapsing. The advantage of Adam has been conﬁrmed by MNIST experiments, where Adam indeed

6

considerably reduced the cases for which we observed mode collapsing. Although TTUR ensures
that the discriminator converges during learning, practicable learning rates must be found for each
experiment. We face a trade-off since the learning rates should be small enough (e.g. for the generator)
to ensure convergence but at the same time should be large enough to allow fast learning. For each of
the experiments, the learning rates have been optimized to be large while still ensuring stable training
which is indicated by a decreasing FID or Jensen-Shannon-divergence (JSD). We further ﬁxed the
time point for stopping training to the update step when the FID or Jensen-Shannon-divergence of
the best models was no longer decreasing. For some models, we observed that the FID diverges
or starts to increase at a certain time point. An example of this behaviour is shown in Fig. 5. The
performance of generative models is evaluated via the Fréchet Inception Distance (FID) introduced
above. For the One Billion Word experiment, the normalized JSD served as performance measure.
For computing the FID, we propagated all images from the training dataset through the pretrained
Inception-v3 model following the computation of the Inception Score [53], however, we use the last
pooling layer as coding layer. For this coding layer, we calculated the mean mw and the covariance
matrix Cw. Thus, we approximate the ﬁrst and second central moment of the function given by
the Inception coding layer under the real world distribution. To approximate these moments for the
model distribution, we generate 50,000 images, propagate them through the Inception-v3 model, and
then compute the mean m and the covariance matrix C. For computational efﬁciency, we evaluate
the FID every 1,000 DCGAN mini-batch updates, every 5,000 WGAN-GP outer iterations for the
image experiments, and every 100 outer iterations for the WGAN-GP language model. For the one
time-scale updates a WGAN-GP outer iteration for the image model consists of ﬁve discriminator
mini-batches and ten discriminator mini-batches for the language model, where we follow the original
implementation. For TTUR however, the discriminator is updated only once per iteration. We repeat
the training for each single time-scale (orig) and TTUR learning rate eight times for the image
datasets and ten times for the language benchmark. Additionally to the mean FID training progress
we show the minimum and maximum FID over all runs at each evaluation time-step. For more details,
implementations and further results see Appendix Section A4 and A6.

Simple Toy Data. We ﬁrst want to demonstrate the difference between a single time-scale update
rule and TTUR on a simple toy min/max problem where a saddle point should be found. The
y2) in Fig. 4 (left) has a saddle point at (x, y) = (0, 0) and
objective f (x, y) = (1 + x2)(100
measures the distance of the parameter vector (x, y) to
fulﬁlls assumption A4. The norm
the saddle point. We update (x, y) by gradient descent in x and gradient ascent in y using additive
Gaussian noise in order to simulate a stochastic update. The updates should converge to the saddle
point (x, y) = (0, 0) with objective value f (0, 0) = 100 and the norm 0. In Fig. 4 (right), the ﬁrst
two rows show one time-scale update rules. The large learning rate in the ﬁrst row diverges and has
large ﬂuctuations. The smaller learning rate in the second row converges but slower than the TTUR in
the third row which has slow x-updates. TTUR with slow y-updates in the fourth row also converges
but slower.

−
(x, y)
(cid:107)
(cid:107)

Figure 4: Left: Plot of the objective with a saddle point at (0, 0). Right: Training progress with
equal learning rates of 0.01 (ﬁrst row) and 0.001 (second row)) for x and y, TTUR with a learning
rate of 0.0001 for x vs. 0.01 for y (third row) and a larger learning rate of 0.01 for x vs. 0.0001 for y
(fourth row). The columns show the function values (left), norms (middle), and (x, y) (right). TTUR
(third row) clearly converges faster than with equal time-scale updates and directly moves to the
saddle point as shown by the norm and in the (x, y)-plot.

DCGAN on Image Data. We test TTUR for the deep convolutional GAN (DCGAN) [51] at the
CelebA, CIFAR-10, SVHN and LSUN Bedrooms dataset. Fig. 5 shows the FID during learning

7

Figure 5: Mean FID (solid line) surrounded by a shaded area bounded by the maximum and the
minimum over 8 runs for DCGAN on CelebA, CIFAR-10, SVHN, and LSUN Bedrooms. TTUR
learning rates are given for the discriminator b and generator a as: “TTUR b a”. Top Left: CelebA.
Top Right: CIFAR-10, starting at mini-batch update 10k for better visualisation. Bottom Left:
SVHN. Bottom Right: LSUN Bedrooms. Training with TTUR (red) is more stable, has much lower
variance, and leads to a better FID.

with the original learning method (orig) and with TTUR. The original training method is faster at
the beginning, but TTUR eventually achieves better performance. DCGAN trained TTUR reaches
constantly a lower FID than the original method and for CelebA and LSUN Bedrooms all one
time-scale runs diverge. For DCGAN the learning rate of the generator is larger then that of the
discriminator, which, however, does not contradict the TTUR theory (see the Appendix Section A5).
In Table 1 we report the best FID with TTUR and one time-scale training for optimized number of
updates and learning rates. TTUR constantly outperforms standard training and is more stable.

WGAN-GP on Image Data. We used the WGAN-GP image model [23] to test TTUR with the
CIFAR-10 and LSUN Bedrooms datasets. In contrast to the original code where the discriminator is
trained ﬁve times for each generator update, TTUR updates the discriminator only once, therefore
we align the training progress with wall-clock time. The learning rate for the original training was
optimized to be large but leads to stable learning. TTUR can use a higher learning rate for the
discriminator since TTUR stabilizes learning. Fig. 6 shows the FID during learning with the original
learning method and with TTUR. Table 1 shows the best FID with TTUR and one time-scale training
for optimized number of iterations and learning rates. Again TTUR reaches lower FIDs than one
time-scale training.

Figure 6: Mean FID (solid line) surrounded by a shaded area bounded by the maximum and the
minimum over 8 runs for WGAN-GP on CelebA, CIFAR-10, SVHN, and LSUN Bedrooms. TTUR
learning rates are given for the discriminator b and generator a as: “TTUR b a”. Left: CIFAR-10,
starting at minute 20. Right: LSUN Bedrooms. Training with TTUR (red) has much lower variance
and leads to a better FID.

8

Figure 7: Performance of WGAN-GP models trained with the original (orig) and our TTUR method
on the One Billion Word benchmark. The performance is measured by the normalized Jensen-
Shannon-divergence based on 4-gram (left) and 6-gram (right) statistics averaged (solid line) and
surrounded by a shaded area bounded by the maximum and the minimum over 10 runs, aligned to
wall-clock time and starting at minute 150. TTUR learning (red) clearly outperforms the original one
time-scale learning.

WGAN-GP on Language Data. Finally the One Billion Word Benchmark [12] serves to evaluate
TTUR on WGAN-GP. The character-level generative language model is a 1D convolutional neural
network (CNN) which maps a latent vector to a sequence of one-hot character vectors of dimension
32 given by the maximum of a softmax output. The discriminator is also a 1D CNN applied to
sequences of one-hot vectors of 32 characters. Since the FID criterium only works for images, we
measured the performance by the Jensen-Shannon-divergence (JSD) between the model and the
real world distribution as has been done previously [23]. In contrast to the original code where the
critic is trained ten times for each generator update, TTUR updates the discriminator only once,
therefore we align the training progress with wall-clock time. The learning rate for the original
training was optimized to be large but leads to stable learning. TTUR can use a higher learning rate
for the discriminator since TTUR stabilizes learning. We report for the 4 and 6-gram word evaluation
the normalized mean JSD for ten runs for original training and TTUR training in Fig. 7. In Table 1
we report the best JSD at an optimal time-step where TTUR outperforms the standard training for
both measures. The improvement of TTUR on the 6-gram statistics over original training shows that
TTUR enables to learn to generate more subtle pseudo-words which better resembles real words.

Table 1: The performance of DCGAN and WGAN-GP trained with the original one time-scale
update rule and with TTUR on CelebA, CIFAR-10, SVHN, LSUN Bedrooms and the One Billion
Word Benchmark. During training we compare the performance with respect to the FID and JSD for
optimized number of updates. TTUR exhibits consistently a better FID and a better JSD.

DCGAN Image
method
dataset
CelebA
TTUR
CIFAR-10 TTUR
TTUR
SVHN
LSUN
TTUR
WGAN-GP Image
method
dataset
CIFAR-10 TTUR
LSUN
TTUR
WGAN-GP Language
n-gram
4-gram
6-gram

method
TTUR
TTUR

b, a
1e-5, 5e-4
1e-4, 5e-4
1e-5, 1e-4
1e-5, 1e-4

updates
225k
75k
165k
340k

FID method
12.5
36.9
12.5
57.5

orig
orig
orig
orig

b, a
3e-4, 1e-4
3e-4, 1e-4

time(m)
700
1900

FID method
24.8
9.5

orig
orig

b, a
3e-4, 1e-4
3e-4, 1e-4

time(m)
1150
1120

JSD method
0.35
0.74

orig
orig

b = a
5e-4
1e-4
5e-5
5e-5

b = a
1e-4
1e-4

b = a
1e-4
1e-4

updates
70k
100k
185k
70k

time(m)
800
2010

time(m)
1040
1070

FID
21.4
37.7
21.4
70.4

FID
29.3
20.5

JSD
0.38
0.77

9

Conclusion

For learning GANs, we have introduced the two time-scale update rule (TTUR), which we have
proved to converge to a stationary local Nash equilibrium. Then we described Adam stochastic
optimization as a heavy ball with friction (HBF) dynamics, which shows that Adam converges and
that Adam tends to ﬁnd ﬂat minima while avoiding small local minima. A second order differential
equation describes the learning dynamics of Adam as an HBF system. Via this differential equation,
the convergence of GANs trained with TTUR to a stationary local Nash equilibrium can be extended
to Adam. Finally, to evaluate GANs, we introduced the ‘Fréchet Inception Distance” (FID) which
captures the similarity of generated images to real ones better than the Inception Score. In experiments
we have compared GANs trained with TTUR to conventional GAN training with a one time-scale
update rule on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.
TTUR outperforms conventional GAN training consistently in all experiments.

Acknowledgment

This work was supported by NVIDIA Corporation, Bayer AG with Research Agreement 09/2017,
Zalando SE with Research Agreement 01/2016, Audi.JKU Deep Learning Center, Audi Electronic
Venture GmbH, IWT research grant IWT150865 (Exaptation), H2020 project grant 671555 (ExCAPE)
and FWF grant P 28660-N31.

The references are provided after Section A6.

References

Appendix

Contents

. .

A1 Fréchet Inception Distance (FID)
A2 Two Time-Scale Stochastic Approximation Algorithms . . . . . . . . . . . . . . . . . .
A2.1 Convergence of Two Time-Scale Stochastic Approximation Algorithms . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A2.1.1 Additive Noise .
A2.1.2 Linear Update, Additive Noise, and Markov Chain . . . . . . . . . . . . .
A2.1.3 Additive Noise and Controlled Markov Processes . . . . . . . . . . . . . .
A2.2 Rate of Convergence of Two Time-Scale Stochastic Approximation Algorithms . .
A2.2.1 Linear Update Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A2.2.2 Nonlinear Update Rules . . . . . . . . . . . . . . . . . . . . . . . . . . .
A2.3 Equal Time-Scale Stochastic Approximation Algorithms . . . . . . . . . . . . . .
A2.3.1 Equal Time-Scale for Saddle Point Iterates . . . . . . . . . . . . . . . . .
A2.3.2 Equal Time Step for Actor-Critic Method . . . . . . . . . . . . . . . . . .
A3 ADAM Optimization as Stochastic Heavy Ball with Friction . . . . . . . . . . . . . . .
A4 Experiments: Additional Information . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A5 Discriminator vs. Generator Learning Rate . . . . . . . . . . . . . . . . . . . . . . . .
A6 Used Software, Datasets, Pretrained Models, and Implementations . . . . . . . . . . . .
.
List of Figures .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
List of Tables .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
16
16
16
18
20
23
23
25
27
27
28
30
32
32
33
33
34
34
38
38

A4.1 WGAN-GP on Image Data.
A4.2 WGAN-GP on the One Billion Word Benchmark.
.
A4.3 BEGAN .

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

.

.

.

10

A1 Fréchet Inception Distance (FID)

(cid:82)

p(.)f (x)dx =

We improve the Inception score for comparing the results of GANs [53]. The Inception score has the
disadvantage that it does not use the statistics of real world samples and compare it to the statistics
of synthetic samples. Let p(.) be the distribution of model samples and pw(.) the distribution of
the samples from real world. The equality p(.) = pw(.) holds except for a non-measurable set if
pw(.)f (x)dx for a basis f (.) spanning the function space in which
and only if
p(.) and pw(.) live. These equalities of expectations are used to describe distributions by moments
or cumulants, where f (x) are polynomials of the data x. We replacing x by the coding layer of an
Inception model in order to obtain vision-relevant features and consider polynomials of the coding
unit functions. For practical reasons we only consider the ﬁrst two polynomials, that is, the ﬁrst two
moments: mean and covariance. The Gaussian is the maximum entropy distribution for given mean
and covariance, therefore we assume the coding units to follow a multidimensional Gaussian. The
difference of two Gaussians is measured by the Fréchet distance [16] also known as Wasserstein-2
distance [58]. The Fréchet distance d(., .) between the Gaussian with mean and covariance (m, C)
obtained from p(.) and the Gaussian (mw, Cw) obtained from pw(.) is called the “Fréchet Inception
Distance” (FID), which is given by [15]:

(cid:82)

d2((m, C), (mw, Cw)) =

mw(cid:107)
Next we show that the FID is consistent with increasing disturbances and human judgment on the
CelebA dataset. We computed the (mw, Cw) on all CelebA images, while for computing (m, C)
we used 50,000 randomly selected samples. We considered following disturbances of the image X:

C + Cw −

2
2 + Tr

m
(cid:107)

CCw

(7)

−

2

(cid:0)

(cid:1)

(cid:0)

(cid:1)

.

1/2

1. Gaussian noise: We constructed a matrix N with Gaussian noise scaled to [0, 255]. The
α)X + αN for α
noisy image is computed as (1
. The larger α is,
}
the larger is the noise added to the image, the larger is the disturbance of the image.

0, 0.25, 0.5, 0.75

∈ {

−

2. Gaussian blur: The image is convolved with a Gaussian kernel with standard deviation
. The larger α is, the larger is the disturbance of the image, that is, the more
0, 1, 2, 4
}

α
the image is smoothed.

∈ {

3. Black rectangles: To an image ﬁve black rectangles are are added at randomly chosen
locations. The rectangles cover parts of the image. The size of the rectangles is αimagesize
. The larger α is, the larger is the disturbance of the image, that
with α
}
is, the more of the image is covered by black rectangles.

0, 0.25, 0.5, 0.75

∈ {

4. Swirl: Parts of the image are transformed as a spiral, that is, as a swirl (whirlpool effect).
Consider the coordinate (x, y) in the noisy (swirled) image for which we want to ﬁnd the
color. Towards this end we need the reverse mapping for the swirl transformation which
gives the location which is mapped to (x, y). We ﬁrst compute polar coordinates relative
x0)) and the radius
to a center (x0, y0) given by the angle θ = arctan((y
5r/(ln 2ρ).
r =
Here α is a parameter for the amount of swirl and ρ indicates the swirl extent in pixels. The
original coordinates, where the color for (x, y) can be found, are xorg = x0 + r cos(θ(cid:48))
and yorg = y0 + r sin(θ(cid:48)). We set (x0, y0) to the center of the image and ρ = 25. The
disturbance level is given by the amount of swirl α
. The larger α is, the larger
∈ {
is the disturbance of the image via the amount of swirl.

y0)2. We transform them according to θ(cid:48) = θ + αe−

0, 1, 2, 4
}

x0)2 + (y

y0)/(x

(cid:112)

(x

−

−

−

−

5. Salt and pepper noise: Some pixels of the image are set to black or white, where black is
chosen with 50% probability (same for white). Pixels are randomly chosen for being ﬂipped
to white or black, where the ratio of pixel ﬂipped to white or black is given by the noise
level α
. The larger α is, the larger is the noise added to the image via
}
ﬂipping pixels to white or black, the larger is the disturbance level.

0, 0.1, 0.2, 0.3

∈ {

6. ImageNet contamination: From each of the 1,000 ImageNet classes, 5 images are randomly
chosen, which gives 5,000 ImageNet images. The images are ensured to be RGB and to
have a minimal size of 256x256. A percentage of α
of the CelebA
images has been replaced by ImageNet images. α = 0 means all images are from CelebA,
α = 0.25 means that 75% of the images are from CelebA and 25% from ImageNet etc.
The larger α is, the larger is the disturbance of the CelebA dataset by contaminating it by
ImageNet images. The larger the disturbance level is, the more the dataset deviates from the
reference real world dataset.

0, 0.25, 0.5, 0.75

∈ {

}

11

We compare the Inception Score [53] with the FID. The Inception Score with m samples and K
classes is

m

K

exp

1
m

i=1
(cid:88)

(cid:88)k=1

(cid:0)

p(yk |

Xi) log

Xi)

p(yk |

p(yk)

.

(cid:1)

The FID is a distance, while the Inception Score is a score. To compare FID and Inception Score,
we transform the Inception Score to a distance, which we call “Inception Distance” (IND). This
transformation to a distance is possible since the Inception Score has a maximal value. For zero
p(yk) = 0. We can bound the
probability p(yk |
log-term by

Xi) = 0, we set the value p(yk |

Xi) log p(yk|

Xi)

p(yk |

log

Xi)

p(yk)

(cid:54) log

1
1/m

= log m .

Using this bound, we obtain an upper bound on the Inception Score:

p(yk |

Xi) log

Xi)

p(yk |

p(yk)

(cid:1)

exp

1
m

(cid:0)
(cid:54) exp

m

K

i=1
(cid:88)

(cid:88)k=1
1
m

log m

m

K

1
m

i=1
(cid:88)
m

(cid:88)k=1
1

i=1
(cid:88)

(cid:1)

(cid:0)

(cid:0)

= exp

log m

= m .

p(yk |

Xi)

(cid:1)

The upper bound is tight and achieved if m (cid:54) K and every sample is from a different class and
the sample is classiﬁed correctly with probability 1. The IND is computed “IND = m - Inception
Score”, therefore the IND is zero for a perfect subset of the ImageNet with m < K samples, where
each sample stems from a different class. Therefore both distances should increase with increasing
disturbance level. In Figure A8 we present the evaluation for each kind of disturbance. The larger the
disturbance level is, the larger the FID and IND should be. In Figure A9, A10, A11, and A11 we
show examples of images generated with DCGAN trained on CelebA with FIDs 500, 300, 133, 100,
45, 13, and FID 3 achieved with WGAN-GP on CelebA.

12

(8)

(9)

(10)

(11)

(12)

Figure A8: Left: FID and right: Inception Score are evaluated for ﬁrst row: Gaussian noise, second
row: Gaussian blur, third row: implanted black rectangles, fourth row: swirled images, ﬁfth row.
salt and pepper noise, and sixth row: the CelebA dataset contaminated by ImageNet images. Left is
the smallest disturbance level of zero, which increases to the highest level at right. The FID captures
the disturbance level very well by monotonically increasing whereas the Inception Score ﬂuctuates,
stays ﬂat or even, in the worst case, decreases.

13

Figure A9: Samples generated from DCGAN trained on CelebA with different FIDs. Left: FID 500
and Right: FID 300.

Figure A10: Samples generated from DCGAN trained on CelebA with different FIDs. Left: FID 133
and Right: FID 100.

14

Figure A11: Samples generated from DCGAN trained on CelebA with different FIDs. Left: FID 45
and Right: FID 13.

Figure A12: Samples generated from WGAN-GP trained on CelebA with a FID of 3.

15

A2 Two Time-Scale Stochastic Approximation Algorithms

Stochastic approximation algorithms are iterative procedures to ﬁnd a root or a stationary point
(minimum, maximum, saddle point) of a function when only noisy observations of its values or
its derivatives are provided. Two time-scale stochastic approximation algorithms are two coupled
iterations with different step sizes. For proving convergence of these interwoven iterates it is assumed
that one step size is considerably smaller than the other. The slower iterate (the one with smaller step
size) is assumed to be slow enough to allow the fast iterate converge while being perturbed by the the
slower. The perturbations of the slow should be small enough to ensure convergence of the faster.
The iterates map at time step n (cid:62) 0 the fast variable wn ∈
their new values:

Rk and the slow variable θn ∈

Rm to

θn+1 = θn + a(n)

h

θn, wn, Z(θ)
n

+ M (θ)

n

,

wn+1 = wn + b(n)

(cid:16)

(cid:0)
g

(cid:1)
θn, wn, Z(w)

n

(cid:17)
+ M (w)

n

.

(cid:16)

(cid:0)

(cid:1)

(cid:17)

(13)

(14)

The iterates use

•

•

•

•

•

•

•

•

M (w)

Z(θ)

Z(w)

h(.)

g(.)

∈

∈

Rm: mapping for the slow iterate Eq. (13),
Rk: mapping for the fast iterate Eq. (14),

a(n): step size for the slow iterate Eq. (13),

b(n): step size for the fast iterate Eq. (14),
M (θ)

n : additive random Markov process for the slow iterate Eq. (13),

n : additive random Markov process for the fast iterate Eq. (14),

n : random Markov process for the slow iterate Eq. (13),

n : random Markov process for the fast iterate Eq. (14).

A2.1 Convergence of Two Time-Scale Stochastic Approximation Algorithms

A2.1.1 Additive Noise

The ﬁrst result is from Borkar 1997 [9] which was generalized in Konda and Borkar 1999 [31].
Borkar considered the iterates:

θn+1 = θn + a(n)

h

θn, wn

+ M (θ)

n

wn+1 = wn + b(n)

(cid:16)

(cid:0)
g

(cid:1)
θn, wn

(cid:17)
+ M (w)

n

(cid:16)

(cid:0)

(cid:1)

,

.

(cid:17)

(cid:55)→

Assumptions. We make the following assumptions:

(A1) Assumptions on the update functions: The functions h : Rk+m

Rm and g : Rk+m

Rk

are Lipschitz.

(A2) Assumptions on the learning rates:

(15)

(16)

(cid:55)→

(17)

(18)

(19)

n
(cid:88)

a(n) =

b(n) =

,

,

∞

∞

n
(cid:88)
a(n) = o(b(n)) ,

a2(n) <

b2(n) <

,

,

∞

∞

n
(cid:88)

n
(cid:88)

16

(A3) Assumptions on the noise: For the increasing σ-ﬁeld

Fn = σ(θl, wl, M (θ)
the sequences of random variables (M (θ)
n ,

l

, l (cid:54) n), n (cid:62) 0 ,

, M (w)
l
Fn) and (M (w)
n ,
n <

a.s.

Fn) satisfy

a(n) M (θ)

n
(cid:88)

b(n) M (w)

n <

a.s. .

∞

∞

n
(cid:88)
(A4) Assumption on the existence of a solution of the fast iterate: For each θ

Rm, the ODE

∈

˙w(t) = g

θ, w(t)

has a unique global asymptotically stable equilibrium λ(θ) such that λ : Rm
Lipschitz.

(cid:0)

(cid:1)

(cid:55)→

(A5) Assumption on the existence of a solution of the slow iterate: The ODE

˙θ(t) = h

θ(t), λ(θ(t))

has a unique global asymptotically stable equilibrium θ∗.
(cid:1)

(cid:0)

(A6) Assumption of bounded iterates:

(20)

(21)

(22)

Rk is

(23)

(24)

(25)

sup

n (cid:107)

sup

n (cid:107)

θn(cid:107)
wn(cid:107)

<

<

,

.

∞

∞

Convergence Theorem The next theorem is from Borkar 1997 [9].
Theorem 3 (Borkar). If the assumptions are satisﬁed, then the iterates Eq. (15) and Eq. (16) converge
to (θ∗, λ(θ∗)) a.s.

Comments

difference sequence w.r.t

(C1) According to Lemma 2 in [7] Assumption (A3) is fulﬁlled if
Fn with
E

(cid:54) B1

2
M (θ)
n (cid:107)
(cid:107)

(θ)
n
| F

and

M (w)
n

{

}

(cid:104)
is a martingale difference sequence w.r.t

(cid:105)

Fn with
(cid:54) B2 ,

E

2
M (w)
n (cid:107)
(cid:107)

(w)
n
| F

(cid:104)

(cid:105)

where B1 and B2 are positive deterministic constants.

M (θ)
n
{

}

is a martingale

(C2) Assumption (A3) holds for mini-batch learning which is the most frequent case of stochastic
i=1 f (xi, θ)), 1 (cid:54) i (cid:54) N and the mini-
∇θ( 1
gradient. The batch gradient is Gn :=
i=1 f (xui, θ)), 1 (cid:54) ui (cid:54) N , where the
∇θ( 1
batch gradient for batch size s is hn :=
indexes ui are randomly and uniformly chosen. For the noise M (θ)
Gn we have
n := hn −
E[M (θ)
Gn = 0. Since the indexes are chosen without knowing
past events, we have a martingale difference sequence. For bounded gradients we have
bounded

Gn = Gn −

n ] = E[hn]

M (θ)
n

(cid:80)
(cid:80)

2.

−

N

N

s

s

(C3) We address assumption (A4) with weight decay in two ways: (I) Weight decay avoids
problems with a discriminator that is region-wise constant and, therefore, does not have a
locally stable generator. If the generator is perfect, then the discriminator is 0.5 everywhere.
For generator with mode collapse, (i) the discriminator is 1 in regions without generator
examples, (ii) 0 in regions with generator examples only, (iii) is equal to the local ratio

(cid:107)

(cid:107)

17

of real world examples for regions with generator and real world examples. Since the
discriminator is locally constant, the generator has gradient zero and cannot improve. Also
the discriminator cannot improve, since it has minimal error given the current generator.
However, without weight decay the Nash Equilibrium is not stable since the second order
derivatives are zero, too. (II) Weight decay avoids that the generator is driven to inﬁnity
with unbounded weights. For example a linear discriminator can supply a gradient for the
generator outside each bounded region.

(C4) The main result used in the proof of the theorem relies on work on perturbations of ODEs

(C5) Konda and Borkar 1999 [31] generalized the convergence proof to distributed asynchronous

according to Hirsch 1989 [24].

update rules.

(C6) Tadi´c relaxed the assumptions for showing convergence [54]. In particular the noise as-
sumptions (Assumptions A2 in [54]) do not have to be martingale difference sequences
and are more general than in [9]. In another result the assumption of bounded iterates is
not necessary if other assumptions are ensured [54]. Finally, Tadi´c considers the case of
non-additive noise [54]. Tadi´c does not provide proofs for his results. We were not able
to ﬁnd such proofs even in other publications of Tadi´c.

A2.1.2 Linear Update, Additive Noise, and Markov Chain

In contrast to the previous subsection, we assume that an additional Markov chain inﬂuences the
iterates [30, 32]. The Markov chain allows applications in reinforcement learning, in particular in
actor-critic setting where the Markov chain is used to model the environment. The slow iterate is the
actor update while the fast iterate is the critic update. For reinforcement learning both the actor and
the critic observe the environment which is driven by the actor actions. The environment observations
are assumed to be a Markov chain. The Markov chain can include eligibility traces which are modeled
as explicit states in order to keep the Markov assumption.

The Markov chain is the sequence of observations of the environment which progresses via transition
probabilities. The transitions are not affected by the critic but by the actor.

Konda et al. considered the iterates [30, 32]:

θn+1 = θn + a(n) Hn ,

wn+1 = wn + b(n)

g

Z(w)

n ; θn

+ G

Z(w)

n ; θn

wn + M (w)

n wn

.

Hn is a random process that drives the changes of θn. We assume that Hn is a slow enough process.
(cid:0)
Rk and the matrix
We have a linear update rule for the fast iterate using the vector function g(.)
function G(.)

Rk

k.

∈

(cid:1)

(cid:0)

(cid:1)

(cid:17)

(cid:16)

×

∈

Assumptions. We make the following assumptions:

(A1) Assumptions on the Markov process, that is, the transition kernel: The stochastic process
takes values in a Polish (complete, separable, metric) space Z with the Borel σ-ﬁeld

Z(w)
n

Fn = σ(θl, wl, Z(w)
| Fn) = P(Z(w)
A
We deﬁne for every measurable function f

For every measurable set A
P(Z(w)

n+1 ∈

n+1 ∈

⊂

l

|

, Hl, l (cid:54) n), n (cid:62) 0 .

Z and the parametrized transition kernel P(.; θn) we have:
n ; θn) = P(Z(w)
(28)

n , A; θn) .

Z(w)

A

(A2) Assumptions on the learning rates:

Pθf (z) :=

P(z, d ¯z; θn) f ( ¯z) .

b2(n) <

,

∞

b(n) =

,

n
(cid:88)

n (cid:18)
(cid:88)

a(n)
b(n)

(cid:19)

n
(cid:88)

<

,

∞

(cid:90)

∞

d

18

(26)

(27)

(29)

(30)

for some d > 0.

(A3) Assumptions on the noise: The sequence M (w)

n

is a k

k-matrix valued

×

Fn-martingale

difference with bounded moments:

We assume slowly changing θ, therefore the random process Hn satisﬁes

E

M (w)
n

= 0 ,

(cid:104)
E

sup
n

| Fn
d

M (w)
n

(cid:105)

(cid:21)

<

,

d > 0 .

∞

∀

(cid:20)(cid:13)
(cid:13)
(cid:13)

(cid:104)

(cid:13)
(cid:13)
(cid:13)

d

(cid:105)

E

sup
n

Hn(cid:107)
(cid:107)

<

,

d > 0 .

∞

∀

(A4) Assumption on the existence of a solution of the fast iterate: We assume the existence of a
Rm, there exist functions
k that satisfy the

solution to the Poisson equation for the fast iterate. For each θ
Rk, ¯G(θ)
¯g(θ)
Poisson equations:

∈
Rk, and ˆG(z; θ) : Z

k, ˆg(z; θ) : Z

Rk

Rk

→

→

∈

∈

×

×

ˆg(z; θ) = g(z; θ)
ˆG(z; θ) = G(z; θ)

¯g(θ) + (Pθ ˆg(.; θ))(z) ,
¯G(θ) + (Pθ ˆG(.; θ))(z) .

−

−

(A5) Assumptions on the update functions and solutions to the Poisson equation:
(a) Boundedness of solutions: For some constant C and for all θ:

max

max

{(cid:107)

¯g(θ)
¯G(θ)

{(cid:107)

(cid:107)}

(cid:107)}

(cid:54) C ,
(cid:54) C .

(b) Boundedness in expectation: All moments are bounded. For any d > 0, there exists

Cd > 0 such that

E

sup
n

ˆg(Z(w)

(cid:54) Cd ,

(cid:21)

E

sup
n

(cid:20)(cid:13)
(cid:13)
g(Z(w)
(cid:13)
(cid:20)(cid:13)
(cid:13)
ˆG(Z(w)
(cid:13)
(cid:20)(cid:13)
(cid:13)
G(Z(w)
(cid:13)
(cid:20)(cid:13)
(cid:13)
(c) Lipschitz continuity of solutions: For some constant C > 0 and for all θ, ¯θ
(cid:13)

(cid:54) Cd ,

(cid:54) Cd .

(cid:54) Cd ,

sup
n

sup
n

E

E

(cid:21)

(cid:21)

(cid:21)

Rm:

∈

d

n ; θ)
(cid:13)
d
(cid:13)
n ; θ)
(cid:13)
(cid:13)
d
(cid:13)
n ; θ)
(cid:13)
(cid:13)
d
(cid:13)
n ; θ)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:54) C
(cid:54) C

(d) Lipschitz continuity in expectation: There exists a positive measurable function C(.)

on Z such that

¯g(θ)
¯G(θ)
(cid:13)
(cid:13)

−

−

¯g( ¯θ)
¯G( ¯θ)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

θ
(cid:107)
θ
(cid:107)

−

−

¯θ
(cid:107)
¯θ
(cid:107)

,

.

E

sup
n

(cid:104)

C(Z(w)

n )d

<

,

d > 0 .

∞

∀

(cid:105)

Function C(.) gives the Lipschitz constant for every z:

(Pθ ˆg(.; θ))(z)
(Pθ ˆG(.; θ))(z)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

−

(P ¯θ ˆg(.; ¯θ))(z)
ˆG(.; ¯θ))(z)

(P ¯θ

(cid:54) C(z)

(cid:54) C(z)

θ
(cid:107)
θ
(cid:107)

−

−

¯θ
(cid:107)
¯θ
(cid:107)

,

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(e) Uniform positive deﬁniteness: There exists some α > 0 such that for all w

Rm:

θ

∈

Rk and

∈

(47)

wT ¯G(θ) w (cid:62) α

2 .

w
(cid:107)

(cid:107)

19

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

Convergence Theorem. We report Theorem 3.2 (see also Theorem 7 in [32]) and Theorem 3.13
from [30]:
Theorem 4 (Konda & Tsitsiklis). If the assumptions are satisﬁed, then for the iterates Eq. (26) and
Eq. (27) holds:

lim
n
→∞
lim
n
→∞

¯G(θn) wn −
(cid:13)
wn −
(cid:13)
(cid:13)
(cid:13)

¯G−

¯g(θn)

= 0 a.s. ,

1(θn) ¯g(θn)
(cid:13)
(cid:13)

= 0 .

(cid:13)
(cid:13)

(48)

(49)

Comments.

(C1) The proofs only use the boundedness of the moments of Hn [30, 32], therefore Hn may
depend on wn. In his PhD thesis [30], Vijaymohan Konda used this framework for the
actor-critic learning, where Hn drives the updates of the actor parameters θn. However, the
actor updates are based on the current parameters wn of the critic.

(C2) The random process Z(w)

n

can affect Hn as long as boundedness is ensured.
Z(w)

(C3) Nonlinear update rule. g

Z(w)
wn can be viewed as a linear approxi-
mation of a nonlinear update rule. The nonlinear case has been considered in [30] where
additional approximation errors due to linearization were addressed. These errors are treated
in the given framework [30].

n ; θn

n ; θn

+ G

(cid:1)

(cid:1)

(cid:0)

(cid:0)

A2.1.3 Additive Noise and Controlled Markov Processes

The most general iterates use nonlinear update functions g and h, have additive noise, and have
controlled Markov processes [28].

θn+1 = θn + a(n)

h

θn, wn, Z(θ)
n

+ M (θ)

n

,

wn+1 = wn + b(n)

(cid:16)

(cid:0)
g

(cid:1)
θn, wn, Z(w)

n

(cid:17)
+ M (w)

n

.

(cid:16)

(cid:0)

(cid:1)

(cid:17)

(50)

(51)

Required Deﬁnitions. Marchaud Map: A set-valued map h : Rl
Marchaud map if it satisﬁes the following properties:

→ {

subsets of Rk} is called a

(i) For each θ
(ii) (point-wise boundedness) For each θ

Rl, h(θ) is convex and compact.

Rl,

∈

∈

sup

h(θ) (cid:107)
∈

w

(cid:107)

w

< K (1 +

θ

(cid:107)

) for some K > 0.
(cid:107)

(iii) h is an upper-semicontinuous map.

1 (in Rl) and
θn}n
We say that h is upper-semicontinuous, if given sequences
1
{
≥
≥
(in Rk) with θn →
h(θn), n
h(θ). In other words, the
1, y
∈
Rk.
, is closed in Rl
graph of h,
×

(x, y) : y

yn}n
{

≥

(cid:9)
is Marchaud, then the differential inclusion (DI)

θ, yn →
y and yn ∈
Rl
∈
∈
subsets of Rm

h(x), x

}

If the set-valued map H : Rm
given by

(cid:8)

→ {

˙θ(t)

H(θ(t))

∈

(52)

is guaranteed to have at least one solution that is absolutely continuous. If Θ is an absolutely
continuous map satisfying Eq. (52) then we say that Θ

Σ.

⊆

Invariant Set: M
with Θ(0) = θ. In other words, Θ
Internally Chain Transitive Set: M
and for every θ, y
∈
solutions to the differential inclusion ˙θ(t)

Rm is invariant if for every θ
0.
Σ with Θ(t)
Rm is said to be internally chain transitive if M is compact
M , (cid:15) > 0 and T > 0 we have the following: There exist Φ1, . . . , Φn that are n
M and

M there exists a trajectory, Θ, entirely in M
M , for all t

h(θ(t)), a sequence θ1(= θ), . . . , θn+1(= y)

∈
⊂

∈
∈

≥

∈

⊂

∈

20

n real numbers t1, t2, . . . , tn greater than T such that: Φi
(cid:15)-neighborhood of θ and Φi
is called an ((cid:15), T ) chain in M from θ to y.

[0,ti](θi)

M for 1

⊂

≤

≤

i

N (cid:15)(θi+1) where N (cid:15)(θ) is the open
ti(θi)
n. The sequence (θ1(= θ), . . . , θn+1(= y))

∈

Assumptions. We make the following assumptions [28]:

Z(w)
(A1) Assumptions on the controlled Markov processes: The controlled Markov process
n
}
Z(θ)
takes values in a compact metric space S(w). The controlled Markov process
n
}
takes values in a compact metric space S(θ). Both processes are controlled by the iterate
Z(w)
θn}
is additionally controlled by a random
and
sequences
n
{
{
Z(θ)
A(w)
taking values in a compact metric space U (w) and
process
is additionally
n
n
{
{
}
taking values in a compact metric space U (θ). The
controlled by a random process
Z(θ)
n
{
}
P(Z(θ)

, θl, wl, l (cid:54) n) =

. Furthermore

dynamics is

wn}
{

p(θ)(dz

A(θ)
n

B(θ)

{
{

n , A(θ)

, A(θ)
l

}

}

}

{

n+1 ∈

Z(θ)
l
|

Z(θ)
|

n , θn, wn), n (cid:62) 0 ,
(53)

(cid:90)B(θ)

for B(θ) Borel in S(θ). The

dynamics is

Z(w)
n

}

{
, A(w)
l

P(Z(w)

n+1 ∈

B(w)

Z(w)
l
|

for B(w) Borel in S(w).

, θl, wl, l (cid:54) n) =

p(w)(dz

Z(w)

n , A(w)

n , θn, wn), n (cid:62) 0 ,

(cid:90)B(w)

|

(54)

(A2) Assumptions on the update functions: h : Rm+k

Rm is jointly continuous as
well as Lipschitz in its ﬁrst two arguments uniformly w.r.t. the third. The latter condition
means that
z(θ)
∀

h(θ(cid:48), w(cid:48), z(θ))
(cid:107)

h(θ, w, z(θ))
(cid:107)

(cid:54) L(θ) (
(cid:107)

S(θ) :

w
(cid:107)

S(θ)

w(cid:48)

→

θ(cid:48)

×

−

−

−

+

∈

θ

(cid:107)

) .
(cid:107)
(55)

Note that the Lipschitz constant L(θ) does not depend on z(θ).
g : Rk+m
uniformly w.r.t. the third. The latter condition means that

S(w)

→

×

Rk is jointly continuous as well as Lipschitz in its ﬁrst two arguments

z(w)
∀

∈

S(w) :

g(θ, w, z(w))
(cid:107)

−

g(θ(cid:48), w(cid:48), z(w))
(cid:107)

(cid:54) L(w) (

θ
(cid:107)

−

θ(cid:48)

+

(cid:107)

w
(cid:107)

−

w(cid:48)

) .
(cid:107)
(56)

Note that the Lipschitz constant L(w) does not depend on z(w).

(A3) Assumptions on the additive noise:

M (θ)
M (w)
and
n
n
}
}
{
{
2 +
wn(cid:107)
θn(cid:107)
with second moments bounded by K(1 +
(cid:107)
(cid:107)
martingale difference sequence w.r.t. increasing σ-ﬁelds
Fn = σ(θl, wl, M (θ)

, M (w)
l

, Z(w)
l

, Z(θ)
l

l

satisfying

are martingale difference sequence
is a

2). More precisely,

M (θ)
n

{

}

, l (cid:54) n), n (cid:62) 0 ,

(cid:54) K (1 +

2 +

θn(cid:107)

(cid:107)

2) ,

wn(cid:107)
(cid:107)

E

M (θ)

2

n+1(cid:107)

(cid:107)
(cid:105)
(cid:104)
for n (cid:62) 0 and a given constant K > 0.
M (w)
n
{

}

| Fn

is a martingale difference sequence w.r.t. increasing σ-ﬁelds

Fn = σ(θl, wl, M (θ)

l

, M (w)
l

, Z(θ)
l

, Z(w)
l

, l (cid:54) n), n (cid:62) 0 ,

satisfying

E

M (w)
(cid:107)

n+1(cid:107)

2

| Fn

(cid:54) K (1 +

2 +

θn(cid:107)

(cid:107)

2) ,

wn(cid:107)

(cid:107)

(cid:105)
(cid:104)
for n (cid:62) 0 and a given constant K > 0.

21

(57)

(58)

(59)

(60)

(A4) Assumptions on the learning rates:

n
(cid:88)

a(n) =

b(n) =

,

,

∞

∞

n
(cid:88)
a(n) = o(b(n)) ,

a2(n) <

b2(n) <

,

,

∞

∞

n
(cid:88)

n
(cid:88)

Furthermore, a(n), b(n), n (cid:62) 0 are non-increasing.

(A5) Assumptions on the controlled Markov processes, that is, the transition kernels: The state-

action map

S(θ)

U (θ)

×

×

Rm+k

(cid:51)

and the state-action map

S(w)

U (w)

×

×

Rm+k

(cid:51)

are continuous.

(z(θ), a(θ), θ, w)

p(θ)(dy

z(θ), a(θ), θ, w)

(64)

(z(w), a(w), θ, w)

p(w)(dy

z(w), a(w), θ, w)

(65)

→

→

|

|

(A6) Assumptions on the existence of a solution:

We consider occupation measures which give for the controlled Markov process the prob-
U for given θ and a
ability or density to observe a particular state-action pair from S
given control policy π. We denote by D(w)(θ, w) the set of all ergodic occupation measures
for the prescribed θ and w on state-action space S(w)
U (θ) for the controlled Markov
process Z(w) with policy π(w). Analogously we denote, by D(θ)(θ, w) the set of all ergodic
occupation measures for the prescribed θ and w on state-action space S(θ)
U (θ) for the
controlled Markov process Z(θ) with policy π(θ). Deﬁne

×

×

×

˜g(θ, w, ν) =

g(θ, w, z) ν(dz, U (w))

(cid:90)

for ν a measure on S(w)

U (w) and the Marchaud map

×

ˆg(θ, w) =

˜g(θ, w, ν) : ν

{

D(w)(θ, w)
}

.

∈

We assume that the set D(w)(θ, w) is singleton, that is, ˆg(θ, w) contains a single function
and we use the same notation for the set and its single element. If the set is not a singleton, the
assumption of a solution can be expressed by the differential inclusion ˙w(t)
ˆg(θ, w(t))
[28].
θ

Rm, the ODE

∈

∀

∈

˙w(t) = ˆg(θ, w(t))

→

has an asymptotically stable equilibrium λ(θ) with domain of attraction Gθ where λ :
Rk is a Lipschitz map with constant K. Moreover, the function V : G
Rm
)
→
∞
is continuously differentiable where V (θ, .) is the Lyapunov function for λ(θ) and G =
(θ, w) : w
(θ, λ(θ)) :
{
{
Rm
θ

becomes an asymptotically stable set of the coupled ODE

. This extra condition is needed so that the set

Gθ, θ

Rm

[0,

∈

∈

}

∈

}

(A7) Assumption of bounded iterates:

˙w(t) = ˆg(θ(t), w(t))
˙θ(t) = 0 .

sup

n (cid:107)

sup

n (cid:107)

θn(cid:107)
wn(cid:107)

<

<

∞

∞

a.s. ,

a.s.

22

(61)

(62)

(63)

(66)

(67)

(68)

(69)

(70)

(71)

(72)

Convergence Theorem. The following theorem is from Karmakar & Bhatnagar [28]:
Theorem 5 (Karmakar & Bhatnagar). Under above assumptions if for all θ
1,

Rm, with probability
belongs to a compact subset Qθ (depending on the sample point) of Gθ “eventually”, then
(73)

A0 (θ∗, λ(θ∗)) a.s.

wn}

as n

∈

θ∗

{

,

∈

→ ∪
which is almost everywhere an internally chain transitive set of the

→ ∞

(θn, wn)
¯θ(s) : s (cid:62) t
}

where A0 =
∩t(cid:62)0{
differential inclusion

˙θ(t)

ˆh(θ(t)),

∈

(74)

where ˆh(θ) =

˜h(θ, λ(θ), ν) : ν
{

∈

D(w)(θ, λ(θ))

.
}

Comments.

(C1) This framework allows to show convergence for gradient descent methods beyond stochastic
gradient like for the ADAM procedure where current learning parameters are memorized
and updated. The random processes Z(w) and Z(θ) may track the current learning status for
the fast and slow iterate, respectively.

(C2) Stochastic regularization like dropout is covered via the random processes A(w) and A(θ).

A2.2 Rate of Convergence of Two Time-Scale Stochastic Approximation Algorithms

A2.2.1 Linear Update Rules

First we consider linear iterates according to the PhD thesis of Konda [30] and Konda & Tsitsiklis
[33].

θn+1 = θn + a(n)

wn+1 = wn + b(n)

a1 −
a2 −

A11 θn −
A21 θn −

A12 wn + M (θ)

n

(cid:17)
A22 wn + M (w)

n

(cid:16)

(cid:16)

,

.

(cid:17)

Assumptions. We make the following assumptions:

(A1) The random variables (M (θ)

n , M (w)
other. The have zero mean: E[M (θ)

n ), n = 0, 1, . . ., are independent of w0, θ0 and of each
n ] = 0 and E[M (w)

n ] = 0. The covariance is

E

M (θ)

n (M (θ)

n )T

= Γ11 ,

(cid:104)
M (θ)

n (M (w)

n )T

E

(cid:104)
M (w)
n

E

(M (w)

n )T

= Γ12 = ΓT

21 ,

= Γ22 .

(cid:105)

(cid:105)

(cid:105)

a(n) =

b(n) =

,

,

∞

∞

lim
n
→∞

lim
n
→∞

a(n) = 0 ,

b(n) = 0 ,

(cid:104)

n
(cid:88)

(A2) The learning rates are deterministic, positive, nondecreasing and satisfy with (cid:15) (cid:54) 0:

n
(cid:88)
a(n)
b(n) →
We often consider the case (cid:15) = 0.
(A3) Convergence of the iterates: We deﬁne

(cid:15) .

∆ := A11 −

1
A12A−

22 A21 .

A matrix is Hurwitz if the real part of each eigenvalue is strictly negative. We assume that
the matrices

∆ are Hurwitz.

A22 and

−

−

23

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

(85)

(86)

(87)
(88)

(89)

(90)

(91)

(92)

(96)

(97)

(98)

(99)

(A4) Convergence rate remains simple:

(a) There exists a constant ¯a (cid:54) 0 such that
(a(n + 1)−

1

a(n)−

1) = ¯a .

lim
n

lim
n

−

−

1
(b(n + 1)−

b(n)−

1) = 0 .

(b) If (cid:15) = 0, then

(c) The matrix

is Hurwitz.

∆

−

−

(cid:16)

¯a
2

I

(cid:17)

Rate of Convergence Theorem. The next theorem is taken from Konda [30] and Konda & Tsitsik-
lis [33].

Let θ∗

Rm and w∗

Rk be the unique solution to the system of linear equations

∈

∈

For each n, let

A11 θn + A12 wn = a1 ,
A21 θn + A22 wn = a2 .

ˆθn = θn −
ˆwn = wn −
1
Σn
11 = θ−
n E

θ∗ ,

1

22 (a2 −
A−
ˆθn ˆθT
,
n

Σn

12 =

Σn
21

(cid:104)
T

(cid:105)
1
= θ−
n E

A21 θn) ,

,

ˆθn ˆwT
n
(cid:104)

(cid:105)

(cid:0)

Σn

ˆwn ˆwT
n

1
22 = w−
(cid:1)
n E
Σn
11 Σn
(cid:2)
12
21 Σn
Σn
22(cid:19)
Theorem 6 (Konda & Tsitsiklis). Under above assumptions and when the constant (cid:15) is sufﬁciently
small, the limit matrices
Σ((cid:15))

Σn =

(93)

(94)

Σn

Σn

Σn

(cid:18)

(cid:3)
.

(95)

12 , Σ((cid:15))

11 , Σ((cid:15))

22 .

,

12 = lim
n

22 = lim
n

11 = lim
n
exist. Furthermore, the matrix

Σ(0) =

Σ(0)
Σ(0)

11 Σ(0)
21 Σ(0)

12
22 (cid:33)

(cid:32)

is the unique solution to the following system of equations

¯a Σ(0)

11 + A12 Σ(0)

21 + Σ(0)

12 AT

12 = Γ11 ,

11 + Σ(0)

∆ Σ(0)
A12 Σ(0)
A22 Σ(0)

11 ∆T
12 AT
22 AT

22 + Σ(0)
22 + Σ(0)

−
22 = Γ12 ,

22 = Γ22 .

Finally,

lim
0
(cid:15)
↓

Σ((cid:15))

11 = Σ(0)
11 ,

Σ((cid:15))

12 = Σ(0)
12 ,

Σ((cid:15))

22 = Σ(0)
22 .

(100)

lim
0
(cid:15)
↓

lim
0
(cid:15)
↓

The next theorems shows that the asymptotic covariance matrix of a(n)−
a(n)−

1/2 ¯θn, where ¯θn evolves according to the single time-scale stochastic iteration:
a1 −
(cid:16)
A21 ¯θn −

¯θn+1 = ¯θn + a(n)
0 = a2 −

A11 ¯θn −
A22 ¯wn + M (w)

A12 ¯wn + M (θ)

(cid:17)

n

n

.

,

1/2θn is the same as that of

(101)

(102)

The next theorem combines Theorem 2.8 of Konda & Tsitsiklis and Theorem 4.1 of Konda &
Tsitsiklis:

24

Theorem 7 (Konda & Tsitsiklis 2nd). Under above assumptions

If the assumptions hold with (cid:15) = 0, then a(n)−

Σ(0)

11 = lim
n

a(n)−

1 E

¯θn ¯θT
n

.

(103)

(cid:3)
1/2 ˆθn converges in distribution to

(cid:2)

(0, Σ(0)

11 ).

N

Comments.

(C1) In his PhD thesis [30] Konda extended the analysis to the nonlinear case. Konda makes a

linearization of the nonlinear function h and g with

A11 =

, A12 =

, A21 =

, A22 =

(104)

∂h
∂θ

−

∂h
∂w

−

∂g
∂θ

−

∂g
∂w

.

−

There are additional errors due to linearization which have to be considered. However, only
a sketch of a proof is provided but not a complete proof.

(C2) Theorem 4.1 of Konda & Tsitsiklis is important to generalize to the nonlinear case.
(C3) The convergence rate is governed by A22 for the fast and ∆ for the slow iterate. ∆ in turn
is affected by the interaction effects captured by A21 and A12 together with the inverse of
A22.

A2.2.2 Nonlinear Update Rules

The rate of convergence for nonlinear update rules according to Mokkadem & Pelletier is considered
[44].

The iterates are

θn+1 = θn + a(n)

h

θn, wn

+ Z(θ)

n + M (θ)
n

,

wn+1 = wn + b(n)

(cid:16)

(cid:0)
g

(cid:1)
θn, wn

+ Z(w)

(cid:17)
n + M (w)

n

.

with the increasing σ-ﬁelds

(cid:16)

(cid:0)

(cid:1)

(cid:17)

Fn = σ(θl, wl, M (θ)

l

, M (w)
l

, Z(θ)
l

, Z(w)
l

, l (cid:54) n), n (cid:62) 0 .

The terms Z(θ)
of the nonlinear functions to their linear approximation.

n and Z(w)

n

can be used to address the error through linearization, that is, the difference

Assumptions. We make the following assumptions:

(A1) Convergence is ensured:

θn = θ∗ a.s. ,

wn = w∗ a.s. .

lim
n
→∞
lim
n
→∞

(A2) Linear approximation and Hurwitz:
There exists a neighborhood

of (θ∗, w∗) such that, for all (θ, w)

h
g

θ, w
θ, w
(cid:0)
(cid:0)
We deﬁne

(cid:18)

(cid:19)

(cid:1)
(cid:1)

=

(cid:18)

U
A11 A12
A21 A22

θ
w

θ∗
w∗

(cid:19)

−
−

(cid:19) (cid:18)

+ O

∈ U
θ∗
w∗

θ
w

−
−

2

.

(cid:33)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:32)(cid:13)
(cid:13)
(cid:13)
(cid:13)

∆ := A11 −

1
A12A−

22 A21 .

A matrix is Hurwitz if the real part of each eigenvalue is strictly negative. We assume that
the matrices A22 and ∆ are Hurwitz.

25

(105)

(106)

(107)

(108)

(109)

(110)

(111)

(A3) Assumptions on the learning rates:

b(n) = b0 n−
(113)
where a0 > 0 and b0 > 0 and 1/2 < β < α (cid:54) 1. If α = 1, then a0 > 1/(2emin) with emin
as the absolute value of the largest eigenvalue of ∆ (the eigenvalue closest to 0).

a(n) = a0 n−

α

β ,

(A4) Assumptions on the noise and error:

(a) martingale difference sequences:

E

E

M (θ)

n+1 | Fn

(cid:104)
M (w)

n+1 | Fn

(cid:104)

(cid:105)

(cid:105)

= 0 a.s. ,

= 0 a.s. .

(b) existing second moments:

E

lim
n
→∞

(cid:34)(cid:32)

M (θ)
n+1
M (w)
n+1(cid:33)

(cid:16)

(M (θ)

n+1)T

(M (w)

n+1)T

= Γ =

| Fn

(cid:35)

(cid:17)

Γ11 Γ12
Γ21 Γ22

(cid:18)

a.s.

(cid:19)

(116)

(c) bounded moments:

There exist l > 2/β such that

E

sup
n

E

sup
n

l
n+1(cid:107)
l
n+1(cid:107)

M (θ)
(cid:107)
(cid:104)
M (w)
(cid:107)
(cid:104)

| Fn

| Fn

(cid:105)

(cid:105)

<

<

∞

∞

a.s. ,

a.s.

(d) bounded error:

with

Z(θ)
n = r(θ)
n = r(w)
Z(w)

n + O
n + O
(cid:0)

θ

(cid:107)

θ
(cid:107)

−

−

θ∗

2 +
(cid:107)
2 +
θ∗
(cid:107)

w

w

(cid:107)

(cid:107)

w∗

2
(cid:107)
w∗

2
(cid:1)

(cid:107)

,

,

−

−

r(θ)
n (cid:107)

(cid:107)

+

(cid:0)
r(w)
n (cid:107)

(cid:107)

= o(

a(n)) a.s.

(cid:112)

(cid:1)

Rate of Convergence Theorem. We report a theorem and a proposition from Mokkadem & Pel-
letier [44]. However, ﬁrst we have to deﬁne the covariance matrices Σθ and Σw which govern the
rate of convergence.

First we deﬁne

Γθ := lim
→∞

n

E

M (θ)

n+1 −

A12 A−

1

22 M (w)

n+1

M (θ)

n+1 −

A12 A−

1

22 M (w)

n+1

T

(cid:17)

=

| Fn

(cid:21)

(122)

Γ11 + A12 A−

22 Γ22 (A−

1
22 )T AT

12 −
We now deﬁne the asymptotic covariance matrices Σθ and Σw:

12 −

Γ12(A−

A12 A−

22 Γ21 .

1

(cid:17) (cid:16)
1
22 )T AT

Σθ =

∞

exp

∆ +

I

t

Γθ exp

∆T +

I

t

dt ,

(123)

(cid:18)(cid:18)

(cid:19)

(cid:19)

(cid:18)(cid:18)

(cid:19)

(cid:19)

1a=1
2 a0

1a=1
2 a0

Σw =

exp (A22 t) Γ22 exp (A22 t) dt .

(cid:20)(cid:16)
1

∞

0
(cid:90)

0
(cid:90)

Σθ and Σw are solutions of the Lyapunov equations:

1a=1
2 a0

∆ +

(cid:18)

(cid:19)

I

Σθ + Σθ

∆T +

I

=

Γθ ,

(cid:18)

(cid:19)
A22 Σw + Σw AT
22 =

−

−

Γ22 .

1a=1
2 a0

26

(112)

(114)

(115)

(117)

(118)

(119)

(120)

(121)

(124)

(125)

(126)

Theorem 8 (Mokkadem & Pelletier: Joint weak convergence). Under above assumptions:

θ∗)
w∗)
Theorem 9 (Mokkadem & Pelletier: Strong convergence). Under above assumptions:

Σθ
0
0 Σw

a(n)−
b(n)−

1 (θ
1 (w

D
−→ N

−
−

(cid:19)(cid:19)

0 ,

(cid:18)

(cid:19)

(cid:18)

.

(cid:18) (cid:112)
(cid:112)

θ
(cid:107)

θ∗

(cid:107)

−

= O

a(n) log

a(l)

a.s. ,





(cid:118)
(cid:117)
(cid:117)
(cid:116)





(cid:118)
(cid:117)
(cid:117)
(cid:116)

n

(cid:32)

(cid:88)l=1
n

(cid:32)

(cid:88)l=1

(cid:33)


(cid:33)


w

(cid:107)

−

(cid:107)

w∗

= O

b(n) log

b(l)

a.s.

Comments.

(C1) Besides the learning steps a(n) and b(n), the convergence rate is governed by A22 for
the fast and ∆ for the slow iterate. ∆ in turn is affected by interaction effects which are
captured by A21 and A12 together with the inverse of A22.

A2.3 Equal Time-Scale Stochastic Approximation Algorithms

In this subsection we consider the case when the learning rates have equal time-scale.

A2.3.1 Equal Time-Scale for Saddle Point Iterates

If equal time-scales assumed then the iterates revisit inﬁnite often an environment of the solution
[61]. In Zhang 2007, the functions of the iterates are the derivatives of a Lagrangian with respect to
the dual and primal variables [61]. The iterates are

θn+1 = θn + a(n)

h

θn, wn

+ Z(θ)

n + M (θ)
n

,

wn+1 = wn + a(n)

(cid:16)

(cid:0)
g

(cid:1)
θn, wn

+ Z(w)

(cid:17)
n + M (w)

n

.

with the increasing σ-ﬁelds

Fn = σ(θl, wl, M (θ)

l

, Z(w)
l

, l (cid:54) n), n (cid:62) 0 .

The terms Z(θ)

n and Z(w)

n

subsum biased estimation errors.

(cid:16)

(cid:0)
, M (w)
l

(cid:1)
, Z(θ)
l

(cid:17)

Assumptions. We make the following assumptions:

(A1) Assumptions on update function: h and g are continuous, differentiable, and bounded. The

Jacobians

∂g
∂w

and

∂h
∂θ

are Hurwitz. A matrix is Hurwitz if the real part of each eigenvalue is strictly negative. This
assumptions corresponds to the assumption in [61] that the Lagrangian is concave in w and
convex in θ.

(A2) Assumptions on noise:

and

M (θ)
n
{
Fn. Furthermore they are mutually independent.
Bounded second moment:

M (w)
n
{

}

}

are a martingale difference sequences w.r.t. the increasing σ-ﬁelds

(127)

(128)

(129)

(130)

(131)

(132)

(133)

(134)

(135)

M (θ)
(cid:107)
M (w)
(cid:107)

2
n+1(cid:107)
2
n+1(cid:107)

E

E

(cid:104)

(cid:104)

| Fn

| Fn

(cid:105)

(cid:105)

<

<

∞

∞

a.s. ,

a.s. .

27

(A3) Assumptions on the learning rate:

(A4) Assumption on the biased error:

Boundedness:

a(n) > 0

,

a(n)

0

,

a(n) =

,

→

n
(cid:88)

∞

n
(cid:88)

a2(n) <

.

∞

(136)

sup

lim
n

sup

lim
n

Z(θ)
n (cid:107)
(cid:107)
Z(w)
n (cid:107)

(cid:107)

(cid:54) α(θ) a.s.

(cid:54) α(w) a.s.

(137)

(138)

Theorem. Deﬁne the “contraction region” Aη as follows:

Aη =

(θ, w) : α(θ) (cid:62) η
{

h(θ, w)
(cid:107)

(cid:107)

or α(w) (cid:62) η

g(θ, w)

(cid:107)

, 0 (cid:54) η < 1
}
(cid:107)

.

(139)

Theorem 10 (Zhang). Under above assumptions the iterates return to Aη inﬁnitely often with
probability one (a.s.).

Comments.

(C1) The proof of the theorem in [61] does not use the saddle point condition and not the fact

that the functions of the iterates are derivatives of the same function.

(C2) For the unbiased case, Zhang showed in Theorem 3.1 of [61] that the iterates converge.
However, he used the saddle point condition of the Lagrangian. He considered iterates
with functions that are the derivatives of a Lagrangian with respect to the dual and primal
variables [61].

A2.3.2 Equal Time Step for Actor-Critic Method

If equal time-scales assumed then the iterates revisit inﬁnite often an environment of the solution of
DiCastro & Meir [14]. The iterates of DiCastro & Meir are derived for actor-critic learning.

To present the actor-critic update iterates, we have to deﬁne some functions and terms. µ(u
the policy function parametrized by θ
chain given by P(y
each state x the agent receives a reward r(x).

x, θ) is
. A Markov
x, u) gives the next observation y using the observation x and the action u. In

Rm with observations x

and actions u

∈ X

∈ U

∈

|

|

The average reward per stage is for the recurrent state x∗:

˜η(θ) = lim
→∞

T

E

(cid:34)

r(xn)

x0 = x∗, θ

.

|

(cid:35)

1
T

T

1

−

n=0
(cid:88)

The estimate of ˜η is denoted by η.

The differential value function is

˜h(x, θ) = E

(r(xn)

˜η(θ))

x0 = x, θ

.

−

|

(cid:35)

T

1

−

(cid:34)

n=0
(cid:88)

˜d(x, y, θ) = r(x)

˜η(θ) + ˜h(y, θ)

˜h(x, θ) .

−

−

The temporal difference is

The estimate of ˜d is denoted by d.
The likelihood ratio derivative Ψ

Rm is

∈
Ψ(x, u, θ) = ∇θµ(u
µ(u
|

x, θ)

|
x, θ)

.

28

(140)

(141)

(142)

(143)

(144)

(145)

(146)

(147)

(148)

(149)

(150)

(151)
(152)
(153)
(154)

(155)

The value function ˜h is approximated by

where φ(x)

Rk. We deﬁne Φ

∈

h(x, w) = φ(x)T w ,

k

R|X |×
∈
φ1(x1)
φ1(x2)
...
φ1(x

|X |

φ2(x1)
φ2(x2)
...
) φ2(x

)

|X |

Φ = 





. . . φk(x1)
. . . φk(x2)

...
. . . φk(x

)

|X |







and

For TD(λ) we have an eligibility trace:

h(w) = Φ w .

en = λ en

1 + φ(xn) .

−

We deﬁne the approximation error with optimal parameter w∗(θ):

−
where π(θ) is an projection operator into the span of Φw. We bound this error by

Rk (cid:107)

−

Φ w

(cid:107)π(θ) =

˜h(θ)
(cid:107)

Φ w∗(θ)

(cid:107)π(θ) ,

(cid:15)app(θ) = inf
∈

w

˜h(θ)

(cid:15)app = sup
Rk

θ

(cid:15)app(θ) .

∈
We denoted by ˜η, ˜d, and ˜h the exact functions and used for their approximation η, d, and h,
respectively. We have learning rate adjustments Γη and Γw for the critic.

The update rules are:
Critic:

ηn+1 = ηn + a(n) Γη (r(xn)

ηn) ,

−

h(x, wn) = φ(x)T wn ,

d(xn, xn+1, wn) = r(xn)

ηn + h(xn+1, wn)

h(xn, wn) ,

en = λ en

−
1 + φ(xn) ,
wn+1 = wn + a(n) Γw d(xn, xn+1, wn) en .

−

−

Actor:

θn+1 = θn + a(n) Ψ(xn, un, θn) d(xn, xn+1, wn) .

Assumptions. We make the following assumptions:

(A1) Assumption on rewards:

The rewards

r(x)
{
(A2) Assumption on the Markov chain:

x
}

∈X

are uniformly bounded by a ﬁnite constant Br.

Each Markov chain for each θ is aperiodic, recurrent, and irreducible.

(A3) Assumptions on the policy function:

The conditional probability function µ(u
exist positive constants, Bµ1 and Bµ2, such that for all x
1 (cid:54) l1, l2 (cid:54) m we have
∂µ(u

∂2µ(u

x, θ)

|

x, θ)

x, θ) is twice differentiable. Moreover, there
Rm and
, u

, θ

∈ X

∈ U

∈

|
∂θl

, and θ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
, u
∈ U

(cid:54) Bµ2 .

(156)

(cid:54) Bµ1 ,

|
∂θl1 ∂θl2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
Rm, there exists a positive constant BΨ, such that
∈
Ψ(x, u, θ)
(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:107)2 (cid:54) BΨ <

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∞

,

(157)

For all x

∈ X

(A4) Assumption on the likelihood ratio derivative:

where

.
(cid:107)

(cid:107)2 is the Euclidean L2 norm.

29

(A5) Assumptions on the approximation space given by Φ:

The columns of the matrix Φ are independent, that is, the form a basis of dimension k. The
φl(cid:107)2 (cid:54) 1
norms of the columns vectors of the matrix Φ are bounded above by 1, that is,
(cid:107)
for 1 (cid:54) l (cid:54) k.

(A6) Assumptions on the learning rate:

a(n) =

,

∞

a2(n) <

.

∞

n
(cid:88)

n
(cid:88)

(158)

Theorem. The algorithm converged if
where the updates are zero. We assume that
point.

∇θ ˜η(θ) = 0, since the actor reached a stationary point
(cid:107)∇θ ˜η(θ)
hints at how close we are to the convergence
(cid:107)

The next theorem from DiCastro & Meir [14] implies that the trajectory visits a neighborhood of
a local maximum inﬁnitely often. Although it may leave the local vicinity of the maximum, it is
guaranteed to return to it inﬁnitely often.
Theorem 11 (DiCastro & Meir). Deﬁne

B

˜η =

∇

B∆td1
Γw

+

B∆td2
Γη

+ B∆td3 (cid:15)app ,

(159)

where B∆td1, B∆td2, and B∆td3 are ﬁnite constants depending on the Markov decision process and
the agent parameters.

Under above assumptions

lim
t
→∞
The trajectory visits a neighborhood of a local maximum inﬁnitely often.

(cid:107)∇θ ˜η(θt)
(cid:107)

inf

˜η .

∇

(cid:54) B

(160)

Comments.

maximum.

(C1) The larger the critic learning rates Γw and Γη are, the smaller is the region around the local

(C2) The results are in agreement with those of Zhang 2007 [61].
(C3) Even if the results are derived for a special actor-critic setting, they carry over to a more

general setting of the iterates.

A3 ADAM Optimization as Stochastic Heavy Ball with Friction

The Nesterov Accelerated Gradient Descent (NAGD) [47] has raised considerable interest due to its
numerical simplicity and its low complexity. Previous to NAGD and its derived methods there was
Polyak’s Heavy Ball method [49]. The idea of the Heavy Ball is a ball that evolves over the graph of
a function f with damping (due to friction) and acceleration. Therefore, this second-order dynamical
system can be described by the ODE for the Heavy Ball with Friction (HBF) [17]:
¨θt + a(t) ˙θt +

f (θt) = 0 ,
∇
where a(n) is the damping coefﬁcient with a(n) = a
nβ for β
integro-differential equation

∈

(161)
(0, 1]. This ODE is equivalent to the

˙θt =

1
k(t)

−

t

h(s)

f (θs)ds ,

∇

(162)

0
(cid:90)
where k and h are two memory functions related to a(t). For polynomially memoried HBF we have
k(t) = tα+1 and h(t) = (α + 1)tα for some positive α, and for exponentially memoried HBF we
have k(t) = λ exp(λ t) and h(t) = exp(λ t). For the sum of the learning rates, we obtain
ln(n) + γ + 1
n1−β
β
1

for β = 1
for β < 1

2n + O

a(l) = a

(163)

1
n2

n

,

(cid:40)

(cid:0)

(cid:1)

(cid:88)l=1

−

30

(164)

(165)

(166)

(167)

(168)

(169)

(170)

where γ = 0.5772156649 is the Euler-Mascheroni constant.

Gadat et al. derived a discrete and stochastic version of the HBF [17]:

θn+1 = θn −
mn+1 = mn + a(n + 1) r(n)

a(n + 1) mn

where

f (θn)

mn

+ a(n + 1) r(n) Mn+1 ,

∇

(cid:0)

−

(cid:1)

r(n) =

r

(cid:80)n

r
l=1 a(l)

(cid:40)

for exponentially memoried HBF
for polynomially memoried HBF

.

This recursion can be rewritten as
θn+1 = θn −
mn+1 =
−

a(n + 1) mn
a(n + 1) r(n)

1

∇
The recursion Eq. (166) is the ﬁrst moment update of ADAM [29].

(cid:0)

(cid:1)

(cid:0)

(cid:1)

mn + a(n + 1) r(n)

f (θn) + Mn+1

.

For the term r(n)a(n) we obtain for the polynomial memory the approximations

r(n) a(n)

r

≈

1
n log n
1

β

(cid:40)

−
n

for β = 1
for β < 1

,

Gadat et al. showed that the recursion Eq. (164) converges for functions with at most quadratic grow
[17]. The authors mention that convergence can be proofed for functions f that are L-smooth, that is,
the gradient is L-Lipschitz.

Kingma et al. [29] state in Theorem 4.1 convergence of ADAM while assuming that β1, the ﬁrst
moment running average coefﬁcient, decays exponentially. Furthermore they assume that β2
< 1
1
√β2
and the learning rate αt decays with αt = α
√t .

ADAM divides mn of the recursion Eq. (166) by the bias-corrected second raw moment estimate.
Since the bias-corrected second raw moment estimate changes slowly, we consider it as an error.

1

√v + ∆v ≈

1
√v −

1
2 v √v

∆v + O(∆v2) .

ADAM assumes the second moment E

g2

to be stationary with its approximation vn:

(cid:2)
vn =

(cid:3)
1
1

−
−

β2
βn
2

n

(cid:88)l=1

βn
2

l

−

g2
l .

∆nvn = vn −

vn

1 =

−

n

(cid:88)l=1
β2)

β2
βn
2

1
1

−
−
β2 (1
1

−
βn
2

−

βn
2

l
−

g2
l −

n

1

−

1

1

−

β2
βn
2

−

1

−

βn
2

l

1

−

−

g2
l

(171)

(cid:88)l=1
β2
βn
2

−

−

1

1

1

−

n

1

−

(cid:88)l=1

βn
2

l
−

−

1

g2
l

=

=

=

1
1

1
1

1
1

−
−

−
−

−
−

β2
βn
2

β2
βn

β2
βn

g2
n +

βn
2

l

1

−

−

g2
l −

g2
n +

β2 −

2 (cid:32)

g2
n −

2 (cid:32)

1

(cid:0)

1

−

β2
βn
2

−

1

−

1

1

−

βn
2
βn
2

−

1

−

n

1

−

βn
2

l

1

−

−

g2
l

(cid:33)

(cid:88)l=1
1

−

(cid:1)
βn
2

−

l

g2
l

.

(cid:33)

n

1

−

(cid:88)l=1

n

1

−

(cid:88)l=1

31

Therefore

E [∆nvn] = E [vn −
β2
βn
2

1
1

=

−
−

vn

1] =

−

1
1

−
−

β2
βn

2 (cid:32)

E

g2

E

g2

−

(cid:0)

(cid:2)

(cid:3)

(cid:2)

(cid:3)(cid:1)

E

g2

(cid:3)

(cid:2)
= 0 .

1

−

β2
βn
2

−

1

−

−

1

n

1

−

(cid:88)l=1

βn
2

l

1

−

−

E

g2

(172)

(cid:33)
(cid:3)

(cid:2)

We are interested in the difference of actual stochastic vn to the true stationary v:

∆vn = vn −

v =

1
1

−
−

β2
βn
2

n

(cid:88)l=1

βn
2

l

−

g2
l −

v

.

(cid:0)

(cid:1)

(173)

αa(n + 1)r(n), we have ∆vn ∝

1/(2v√v)∆vn + O(∆2vn). If we set M (v)
mn/√v + a(n + 1)r(n)M (v)
n+1 and E

For a stationary second moment of mn and β2 = 1
a(n +
1)r(n). We use a linear approximation to ADAM’s second moment normalization 1/√v + ∆vn ≈
(mn∆vn)/(2v√va(n + 1)r(n)), then
1/√v
−
g2
mn/√vn ≈
= 0. For a
l −
stationary second moment of mn,
M (v)
second moment. Therefore
{
factor 1/√v can be incorporated into a(n + 1) and r(n).

is a martingale difference sequence with a bounded
in update rules Eq. (166). The

M (v)
n
{
can be subsumed into
n+1}

(cid:105)
Mn+1}

= 0, since E

−
M (v)
n+1

n+1 =

−

}

{

v

(cid:104)

(cid:2)

(cid:3)

A4 Experiments: Additional Information

A4.1 WGAN-GP on Image Data.

Table A2: The performance of WGAN-GP trained with the original procedure and with TTUR on
CIFAR-10 and LSUN Bedrooms. We compare the performance with respect to the FID at the optimal
number of iterations during training and wall-clock time in minutes.

dataset

method

b, a

iter

time(m)

FID method

b = a

time(m)

CIFAR-10 TTUR
TTUR
LSUN

3e-4, 1e-4
3e-4, 1e-4

168k
80k

700
1900

24.8
9.5

orig
orig

1e-4
1e-4

iter

53k
23k

800
2010

FID

29.3
20.5

32

A4.2 WGAN-GP on the One Billion Word Benchmark.

Table A3: Samples generated by WGAN-GP trained on fhe One Billion Word benchmark with TTUR
(left) the original method (right).

Dry Hall Sitning tven the concer
There are court phinchs hasffort
He scores a supponied foutver il
Bartfol reportings ane the depor
Seu hid , it ’s watter ’s remold
Later fasted the store the inste
Indiwezal deducated belenseous K
Starfers on Rbama ’s all is lead
Inverdick oper , caldawho ’s non
She said , five by theically rec
RichI , Learly said remain .‘‘‘‘
Reforded live for they were like
The plane was git finally fuels
The skip lifely will neek by the
SEW McHardy Berfect was luadingu
But I pol rated Franclezt is the

No say that tent Franstal at Bra
Caulh Paphionars tven got corfle
Resumaly , braaky facting he at
On toipe also houd , aid of sole
When Barrysels commono toprel to
The Moster suprr tent Elay diccu
The new vebators are demases to
Many ’s lore wockerssaow 2 2 ) A
Andly , has le wordd Uold steali
But be the firmoters is no 200 s
Jermueciored a noval wan ’t mar
Onles that his boud-park , the g
ISLUN , The crather wilh a them
Fow 22o2 surgeedeto , theirestra
Make Sebages of intarmamates , a
Gullla " has cautaria Thoug ly t

Table A4: The performance of WGAN-GP trained with the original procedure and with TTUR on the
One Billion Word Benchmark. We compare the performance with respect to the JSD at the optimal
number of iterations and wall-clock time in minutes during training. WGAN-GP trained with TTUR
exhibits consistently a better FID.

n-gram method

b, a

iter

time(m)

JSD method

b = a

time(m)

4-gram TTUR
6-gram TTUR

3e-4, 1e-4
3e-4, 1e-4

98k
100k

1150
1120

0.35
0.74

orig
orig

1e-4
1e-4

iter

33k
32k

1040
1070

JSD

0.38
0.77

A4.3 BEGAN

The Boundary Equilibrium GAN (BEGAN) [6] maintains an equilibrium between the discriminator
and generator loss (cf. Section 3.3 in [6])

which, in turn, also leads to a ﬁxed relation between the two gradients, therefore, a two time-scale
update is not ensured by solely adjusting the learning rates. Indeed, for stable learning rates, we see
no differences in the learning progress between orig and TTUR as depicted in Figure A13.

E[

(G(z))] = γE[

(x)]

L

L

(174)

Figure A13: Mean, maximum and minimum FID over eight runs for BEGAN training on CelebA
and LSUN Bedrooms. TTUR learning rates are given as pairs (b, a) of discriminator learning rate
b and generator learning rate a: “TTUR b a”. Left: CelebA, starting at mini-batch 10k for better
visualisation. Right: LSUN Bedrooms. Orig and TTUR behave similar. For BEGAN we cannot
ensure TTUR by adjusting learning rates.

33

A5 Discriminator vs. Generator Learning Rate

The convergence proof for learning GANs with TTUR assumes that the generator learning rate will
eventually become small enough to ensure convergence of the discriminator learning. At some time
point, the perturbations of the discriminator updates by updates of the generator parameters are
sufﬁcient small to assure that the discriminator converges. Crucial for discriminator convergence is
the magnitude of the perturbations which the generator induces into the discriminator updates. These
perturbations are not only determined by the generator learning rate but also by its loss function,
current value of the loss function, optimization method, size of the error signals that reach the
generator (vanishing or exploding gradient), complexity of generator’s learning task, architecture of
the generator, regularization, and others. Consequently, the size of generator learning rate does not
solely determine how large the perturbations of the discriminator updates are but serve to modulate
them. Thus, the generator learning rate may be much larger than the discriminator learning rate
without inducing large perturbation into the discriminator learning.

Even the learning dynamics of the generator is different from the learning dynamics of the discrimi-
nator, though they both have the same learning rate. Figure A14 shows the loss of the generator and
the discriminator for an experiment with DCGAN on CelebA, where the learning rate was 0.0005
for both the discriminator and the generator. However, the discriminator loss is decreasing while
the generator loss is increasing. This example shows that the learning rate neither determines the
perturbations nor the progress in learning for two coupled update rules. The choice of the learning
rate for the generator should be independent from choice for the discriminator. Also the search ranges
of discriminator and generator learning rates should be independent from each other, but adjusted to
the corresponding architecture, task, etc.

Figure A14: The respective losses of the discriminator and the generator show the different learning
dynamics of the two networks.

A6 Used Software, Datasets, Pretrained Models, and Implementations

We used the following datasets to evaluate GANs: The Large-scale CelebFaces Attributes (CelebA)
dataset, aligned and cropped [41], the training dataset of the bedrooms category of the large scale
image database (LSUN) [60], the CIFAR-10 training dataset [34], the Street View House Numbers
training dataset (SVHN) [48], and the One Billion Word Benchmark [12].

All experiments rely on the respective reference implementations for the corresponding GAN model.
The software framework for our experiments was Tensorﬂow 1.3 [1, 2] and Python 3.6. We used
following software, datasets and pretrained models:

•

BEGAN in Tensorﬂow, https://github.com/carpedm20/BEGAN-tensorflow, Fixed
random seeds removed. Accessed: 2017-05-30

34

•

•

•

•

•

DCGAN in Tensorﬂow, https://github.com/carpedm20/DCGAN-tensorflow, Fixed
random seeds removed. Accessed: 2017-04-03
Improved Training of Wasserstein GANs, image model, https://github.com/igul222/
improved_wgan_training/blob/master/gan_64x64.py, Accessed: 2017-06-12
language model, https://github.com/
Improved Training of Wasserstein GANs,
igul222/improved_wgan_training/blob/master/gan_language.py, Accessed:
2017-06-12
Inception-v3
imagenet/inception-2015-12-05.tgz, Accessed: 2017-05-02

http://download.tensorflow.org/models/image/

pretrained,

Implementations are available at

https://github.com/bioinf-jku/TTUR

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis,
J. Dean, M. Devin, S. Ghemawat, I. J. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Józefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. G.
Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. A. Tucker,
V. Vanhoucke, V. Vasudevan, F. B. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke,
Y. Yu, and X. Zheng. Tensorﬂow: Large-scale machine learning on heterogeneous distributed
systems. arXiv e-prints, arXiv:1603.04467, 2016.

[2] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving,
M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker,
V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng. Tensorﬂow: A system for large-
In 12th USENIX Symposium on Operating Systems Design and
scale machine learning.
Implementation (OSDI 16), pages 265–283, 2016.

[3] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein GAN. arXiv e-prints, arXiv:1701.07875,

2017.

[4] S. Arora, R. Ge, Y. Liang, T. Ma, and Y. Zhang. Generalization and equilibrium in generative
In D. Precup and Y. W. Teh, editors, Proceedings of the 34th
adversarial nets (GANs).
International Conference on Machine Learning, Proceedings of Machine Learning Research,
vol. 70, pages 224–232, 2017.

[5] H. Attouch, X. Goudou, and P. Redont. The heavy ball with friction method, I. the continu-
ous dynamical system: Global exploration of the local minima of a real-valued function by
asymptotic analysis of a dissipative dynamical system. Communications in Contemporary
Mathematics, 2(1):1–34, 2000.

[6] D. Berthelot, T. Schumm, and L. Metz. BEGAN: Boundary equilibrium generative adversarial

networks. arXiv e-prints, arXiv:1703.10717, 2017.

[7] D. P. Bertsekas and J. N. Tsitsiklis. Gradient convergence in gradient methods with errors.

SIAM Journal on Optimization, 10(3):627–642, 2000.

[8] S. Bhatnagar, H. L. Prasad, and L. A. Prashanth. Stochastic Recursive Algorithms for Optimiza-
tion. Lecture Notes in Control and Information Sciences. Springer-Verlag London, 2013.
[9] V. S. Borkar. Stochastic approximation with two time scales. Systems & Control Letters,

29(5):291–294, 1997.

[10] V. S. Borkar and S. P. Meyn. The O.D.E. method for convergence of stochastic approximation
and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447–469, 2000.
[11] T. Che, Y. Li, A. P. Jacob, Y. Bengio, and W. Li. Mode regularized generative adversarial
networks. In Proceedings of the International Conference on Learning Representations (ICLR),
2017. arXiv:1612.02136.

[12] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson. One billion
word benchmark for measuring progress in statistical language modeling. arXiv e-prints,
arXiv:1312.3005, 2013.

35

[13] D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by
exponential linear units (ELUs). In Proceedings of the International Conference on Learning
Representations (ICLR), 2016. arXiv:1511.07289.

[14] D. DiCastro and R. Meir. A convergent online single time scale actor critic algorithm. J. Mach.

Learn. Res., 11:367–410, 2010.

[15] D. C. Dowson and B. V. Landau. The Fréchet distance between multivariate normal distributions.

Journal of Multivariate Analysis, 12:450–455, 1982.

[16] M. Fréchet. Sur la distance de deux lois de probabilité. C. R. Acad. Sci. Paris, 244:689–692,

[17] S. Gadat, F. Panloup, and S. Saadane. Stochastic heavy ball. arXiv e-prints, arXiv:1609.04228,

1957.

2016.

[18] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems
27, pages 2672–2680, 2014.

[19] I. J. Goodfellow. On distinguishability criteria for estimating generative models. In Workshop
at the International Conference on Learning Representations (ICLR), 2015. arXiv:1412.6515.

[20] I. J. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. arXiv e-prints,

arXiv:1701.00160, 2017.

[21] X. Goudou and J. Munier. The gradient and heavy ball with friction dynamical systems: the

quasiconvex case. Mathematical Programming, 116(1):173–191, 2009.

[22] P. Grnarova, K. Y. Levy, A. Lucchi, T. Hofmann, and A. Krause. An online learning approach

to generative adversarial networks. arXiv e-prints, arXiv:1706.03269, 2017.

[23] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of
Wasserstein GANs. arXiv e-prints, arXiv:1704.00028, 2017. Advances in Neural Information
Processing Systems 31 (NIPS 2017).

[24] M. W. Hirsch. Convergent activation dynamics in continuous time networks. Neural Networks,

2(5):331–349, 1989.

[25] R. D. Hjelm, A. P. Jacob, T. Che, K. Cho, and Y. Bengio. Boundary-seeking generative

adversarial networks. arXiv e-prints, arXiv:1702.08431, 2017.

[26] S. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.

[27] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional
adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2017. arXiv:1611.07004.

[28] P. Karmakar and S. Bhatnagar. Two time-scale stochastic approximation with controlled Markov
noise and off-policy temporal-difference learning. Mathematics of Operations Research, 2017.

[29] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In Proceedings of the
International Conference on Learning Representations (ICLR)), 2015. arXiv:1412.6980.

[30] V. R. Konda. Actor-Critic Algorithms. PhD thesis, Department of Electrical Engineering and

Computer Science, Massachusetts Institute of Technology, 2002.

[31] V. R. Konda and V. S. Borkar. Actor-critic-type learning algorithms for Markov decision

processes. SIAM J. Control Optim., 38(1):94–123, 1999.

[32] V. R. Konda and J. N. Tsitsiklis. Linear stochastic approximation driven by slowly varying

Markov chains. Systems & Control Letters, 50(2):95–102, 2003.

[33] V. R. Konda and J. N. Tsitsiklis. Convergence rate of linear two-time-scale stochastic approxi-

mation. The Annals of Applied Probability, 14(2):796–819, 2004.

[34] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classiﬁcation with deep convolutional
neural networks. In Proceedings of the 25th International Conference on Neural Information
Processing Systems, pages 1097–1105, 2012.

[35] H. J. Kushner and G. G. Yin. Stochastic Approximation Algorithms and Recursive Algorithms

and Applications. Springer-Verlag New York, second edition, 2003.

36

[36] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. P. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi.
Photo-realistic single image super-resolution using a generative adversarial network. arXiv
e-prints, arXiv:1609.04802, 2016.

[37] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Póczos. MMD GAN: Towards deeper
understanding of moment matching network. In Advances in Neural Information Processing
Systems 31 (NIPS 2017), 2017. arXiv:1705.08584.

[38] J. Li, A. Madry, J. Peebles, and L. Schmidt. Towards understanding the dynamics of generative

adversarial networks. arXiv e-prints, arXiv:1706.09884, 2017.

[39] J. H. Lim and J. C. Ye. Geometric GAN. arXiv e-prints, arXiv:1705.02894, 2017.

[40] S. Liu, O. Bousquet, and K. Chaudhuri. Approximation and convergence properties of generative
adversarial learning. In Advances in Neural Information Processing Systems 31 (NIPS 2017),
2017. arXiv:1705.08991.

[41] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings

of International Conference on Computer Vision (ICCV), 2015.

[42] L. M. Mescheder, S. Nowozin, and A. Geiger. The numerics of GANs. In Advances in Neural

Information Processing Systems 31 (NIPS 2017), 2017. arXiv:1705.10461.

[43] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks.
In Proceedings of the International Conference on Learning Representations (ICLR), 2017.
arXiv:1611.02163.

[44] A. Mokkadem and M. Pelletier. Convergence rate and averaging of nonlinear two-time-scale
stochastic approximation algorithms. The Annals of Applied Probability, 16(3):1671–1702,
2006.

[45] Y. Mroueh and T. Sercu. Fisher GAN. In Advances in Neural Information Processing Systems

31 (NIPS 2017), 2017. arXiv:1705.09675.

[46] V. Nagarajan and J. Z. Kolter. Gradient descent GAN optimization is locally stable. arXiv
e-prints, arXiv:1706.04156, 2017. Advances in Neural Information Processing Systems 31
(NIPS 2017).

[47] Y. Nesterov. A method of solving a convex programming problem with convergence rate

o(1/k2). Soviet Mathematics Doklady, 27:372–376, 1983.

[48] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in nat-
ural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and
Unsupervised Feature Learning 2011, 2011.

[49] B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR

Computational Mathematics and Mathematical Physics, 4(5):1–17, 1964.

[50] H. L. Prasad, L. A. Prashanth, and S. Bhatnagar. Two-timescale algorithms for learning Nash
equilibria in general-sum stochastic games. In Proceedings of the 2015 International Conference
on Autonomous Agents and Multiagent Systems (AAMAS ’15), pages 1371–1379, 2015.

[51] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convo-
lutional generative adversarial networks. In Proceedings of the International Conference on
Learning Representations (ICLR), 2016. arXiv:1511.06434.

[52] A. Ramaswamy and S. Bhatnagar. Stochastic recursive inclusion in two timescales with an

application to the lagrangian dual problem. Stochastics, 88(8):1173–1187, 2016.

[53] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved
techniques for training GANs. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 2234–2242,
2016.

[54] V. B. Tadi´c. Almost sure convergence of two time-scale stochastic approximation algorithms.
In Proceedings of the 2004 American Control Conference, volume 4, pages 3802–3807, 2004.

[55] L. Theis, A. van den Oord, and M. Bethge. A note on the evaluation of generative models.
In Proceedings of the International Conference on Learning Representations (ICLR), 2016.
arXiv:1511.01844.

37

[56] I. Tolstikhin, S. Gelly, O. Bousquet, C.-J. Simon-Gabriel, and B. Schölkopf. AdaGAN: Boosting
generative models. arXiv e-prints, arXiv:1701.02386, 2017. Advances in Neural Information
Processing Systems 31 (NIPS 2017).

[57] R. Wang, A. Cully, H. J. Chang, and Y. Demiris. MAGAN: margin adaptation for generative

adversarial networks. arXiv e-prints, arXiv:1704.03817, 2017.

[58] L. N. Wasserstein. Markov processes over denumerable products of spaces describing large

systems of automata. Probl. Inform. Transmission, 5:47–52, 1969.

[59] Y. Wu, Y. Burda, R. Salakhutdinov, and R. B. Grosse. On the quantitative analysis of decoder-
based generative models. In Proceedings of the International Conference on Learning Repre-
sentations (ICLR), 2017. arXiv:1611.04273.

[60] F. Yu, Y. Zhang, S. Song, A. Seff, and J. Xiao. LSUN: construction of a large-scale image
dataset using deep learning with humans in the loop. arXiv e-prints, arXiv:1506.03365, 2015.
[61] J. Zhang, D. Zheng, and M. Chiang. The impact of stochastic noisy feedback on distributed
network utility maximization. In IEEE INFOCOM 2007 - 26th IEEE International Conference
on Computer Communications, pages 222–230, 2007.

List of Figures

Oscillation in GAN training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Heavy Ball with Friction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
. . . . . . . . . . . . . . . . . . . . . . .
FID evaluated for different disturbances
3
. . . . . . . . . . . . . . . . . .
TTUR and single time-scale update with toy data.
4
. . . . . .
FID for DCGAN on CelebA, CIFAR-10, SVHN, and LSUN Bedrooms.
5
FID for WGAN-GP trained on CIFAR-10 and LSUN Bedrooms.
. . . . . . . . . .
6
Performance of WGAN-GP on One Billion Word. . . . . . . . . . . . . . . . . . .
7
A8 FID and Inception Score Comparison . . . . . . . . . . . . . . . . . . . . . . . . .
A9 CelebA Samples with FID 500 and 300 . . . . . . . . . . . . . . . . . . . . . . . .
A10 CelebA Samples with FID 133 and 100 . . . . . . . . . . . . . . . . . . . . . . . .
A11 CelebA Samples with FID 45 and 13 . . . . . . . . . . . . . . . . . . . . . . . . .
A12 CelebA Samples with FID 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A13 FID for BEGAN trained on CelebA and LSUN Bedrooms.
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
A14 Learning dynamics of two networks.

List of Tables

1 Results DCGAN and WGAN-GP . . . . . . . . . . . . . . . . . . . . . . . . . . .
A2 Results WGAN-GP on Image Data
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
A3 Samples of the One Billion Word benchmark generated by WGAN-GP.
A4 Results WGAN-GP on One Billion Word . . . . . . . . . . . . . . . . . . . . . . .

2
4
6
7
8
8
9
13
14
14
15
15
33
34

9
32
33
33

38

