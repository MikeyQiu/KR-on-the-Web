Billion-scale similarity search with GPUs

Jeff Johnson
Facebook AI Research
New York

Matthijs Douze
Facebook AI Research
Paris

Herv ´e J ´egou
Facebook AI Research
Paris

7
1
0
2
 
b
e
F
 
8
2
 
 
]

V
C
.
s
c
[
 
 
1
v
4
3
7
8
0
.
2
0
7
1
:
v
i
X
r
a

ABSTRACT
Similarity search ﬁnds application in specialized database
systems handling complex data such as images or videos,
which are typically represented by high-dimensional features
and require speciﬁc indexing structures. This paper tackles
the problem of better utilizing GPUs for this task. While
GPUs excel at data-parallel tasks, prior approaches are bot-
tlenecked by algorithms that expose less parallelism, such as
k-min selection, or make poor use of the memory hierarchy.
We propose a design for k-selection that operates at up
to 55% of theoretical peak performance, enabling a nearest
neighbor implementation that is 8.5× faster than prior GPU
state of the art. We apply it in diﬀerent similarity search
scenarios, by proposing optimized design for brute-force, ap-
proximate and compressed-domain search based on product
quantization. In all these setups, we outperform the state of
the art by large margins. Our implementation enables the
construction of a high accuracy k-NN graph on 95 million
images from the Yfcc100M dataset in 35 minutes, and of
a graph connecting 1 billion vectors in less than 12 hours
on 4 Maxwell Titan X GPUs. We have open-sourced our
approach1 for the sake of comparison and reproducibility.

1.

INTRODUCTION

Images and videos constitute a new massive source of data
for indexing and search. Extensive metadata for this con-
tent is often not available. Search and interpretation of this
and other human-generated content, like text, is diﬃcult and
important. A variety of machine learning and deep learn-
ing algorithms are being used to interpret and classify these
complex, real-world entities. Popular examples include the
text representation known as word2vec [32], representations
of images by convolutional neural networks [39, 19], and im-
age descriptors for instance search [20]. Such representations
or embeddings are usually real-valued, high-dimensional vec-
tors of 50 to 1000+ dimensions. Many of these vector repre-
sentations can only eﬀectively be produced on GPU systems,

1https://github.com/facebookresearch/faiss

as the underlying processes either have high arithmetic com-
plexity and/or high data bandwidth demands [28], or cannot
be eﬀectively partitioned without failing due to communi-
cation overhead or representation quality [38]. Once pro-
duced, their manipulation is itself arithmetically intensive.
However, how to utilize GPU assets is not straightforward.
More generally, how to exploit new heterogeneous architec-
tures is a key subject for the database community [9].

In this context, searching by numerical similarity rather
than via structured relations is more suitable. This could be
to ﬁnd the most similar content to a picture, or to ﬁnd the
vectors that have the highest response to a linear classiﬁer
on all vectors of a collection.

One of the most expensive operations to be performed on
large collections is to compute a k-NN graph. It is a directed
graph where each vector of the database is a node and each
edge connects a node to its k nearest neighbors. This is
our ﬂagship application. Note, state of the art methods like
NN-Descent [15] have a large memory overhead on top of
the dataset itself and cannot readily scale to the billion-sized
databases we consider.

Such applications must deal with the curse of dimension-
ality [46], rendering both exhaustive search or exact index-
ing for non-exhaustive search impractical on billion-scale
databases. This is why there is a large body of work on
approximate search and/or graph construction. To handle
huge datasets that do not ﬁt in RAM, several approaches
employ an internal compressed representation of the vec-
tors using an encoding. This is especially convenient for
memory-limited devices like GPUs. It turns out that accept-
ing a minimal accuracy loss results in orders of magnitude
of compression [21]. The most popular vector compression
methods can be classiﬁed into either binary codes [18, 22],
or quantization methods [25, 37]. Both have the desirable
property that searching neighbors does not require recon-
structing the vectors.

Our paper focuses on methods based on product quanti-
zation (PQ) codes, as these were shown to be more eﬀective
than binary codes [34]. In addition, binary codes incur im-
portant overheads for non-exhaustive search methods [35].
Several improvements were proposed after the original prod-
uct quantization proposal known as IVFADC [25]; most are
diﬃcult to implement eﬃciently on GPU. For instance, the
inverted multi-index [4], useful for high-speed/low-quality
operating points, depends on a complicated “multi-sequence”
algorithm. The optimized product quantization or OPQ [17]
is a linear transformation on the input vectors that improves
the accuracy of the product quantization; it can be applied

1

as a pre-processing. The SIMD-optimized IVFADC imple-
mentation from [2] operates only with sub-optimal parame-
ters (few coarse quantization centroids). Many other meth-
ods, like LOPQ and the Polysemous codes [27, 16] are too
complex to be implemented eﬃciently on GPUs.

There are many implementations of similarity search on
GPUs, but mostly with binary codes [36], small datasets [44],
or exhaustive search [14, 40, 41]. To the best of our knowl-
edge, only the work by Wieschollek et al. [47] appears suit-
able for billion-scale datasets with quantization codes. This
is the prior state of the art on GPUs, which we compare
against in Section 6.4.

This paper makes the following contributions:

• a GPU k-selection algorithm, operating in fast register
memory and ﬂexible enough to be fusable with other
kernels, for which we provide a complexity analysis;

• a near-optimal algorithmic layout for exact and ap-

proximate k-nearest neighbor search on GPU;

• a range of experiments that show that these improve-
ments outperform previous art by a large margin on
mid- to large-scale nearest-neighbor search tasks, in
single or multi-GPU conﬁgurations.

The paper is organized as follows. Section 2 introduces
the context and notation. Section 3 reviews GPU archi-
tecture and discusses problems appearing when using it for
similarity search. Section 4 introduces one of our main con-
tributions, i.e., our k-selection method for GPUs, while Sec-
tion 5 provides details regarding the algorithm computation
layout. Finally, Section 6 provides extensive experiments for
our approach, compares it to the state of the art, and shows
concrete use cases for image collections.

2. PROBLEM STATEMENT

We are concerned with similarity search in vector collec-
tions. Given the query vector x ∈ Rd and the collection2
[yi]i=0:(cid:96) (yi ∈ Rd), we search:

L = k-argmini=0:(cid:96)(cid:107)x − yi(cid:107)2,

(1)

i.e., we search the k nearest neighbors of x in terms of L2
distance. The L2 distance is used most often, as it is op-
timized by design when learning several embeddings (e.g.,
[20]), due to its attractive linear algebra properties.

The lowest distances are collected by k-selection. For an
array [ai]i=0:(cid:96), k-selection ﬁnds the k lowest valued elements
[asi ]i=0:k, asi ≤ asi+1 , along with the indices [si]i=0:k, 0 ≤
si < (cid:96), of those elements from the input array. The ai will be
32-bit ﬂoating point values; the si are 32- or 64-bit integers.
Other comparators are sometimes desired; e.g., for cosine
similarity we search for highest values. The order between
equivalent keys asi = asj is not speciﬁed.

Batching. Typically, searches are performed in batches
of nq query vectors [xj]j=0:nq (xj ∈ Rd) in parallel, which
allows for more ﬂexibility when executing on multiple CPU
threads or on GPU. Batching for k-selection entails selecting
nq × k elements and indices from nq separate arrays, where
each array is of a potentially diﬀerent length (cid:96)i ≥ k.

2To avoid clutter in 0-based indexing, we use the array no-
tation 0 : (cid:96) to denote the range {0, ..., (cid:96) − 1} inclusive.

Exact search. The exact solution computes the full pair-
2]j=0:nq,i=0:(cid:96) ∈ Rnq×(cid:96).
wise distance matrix D = [(cid:107)xj − yi(cid:107)2
In practice, we use the decomposition

(cid:107)xj − yi(cid:107)2

2 = (cid:107)xj(cid:107)2 + (cid:107)yi(cid:107)2 − 2(cid:104)xj, yi(cid:105).

(2)

The two ﬁrst terms can be precomputed in one pass over
the matrices X and Y whose rows are the [xj] and [yi]. The
bottleneck is to evaluate (cid:104)xj, yi(cid:105), equivalent to the matrix
multiplication XY (cid:62). The k-nearest neighbors for each of
the nq queries are k-selected along each row of D.

Compressed-domain search. From now on, we focus on
approximate nearest-neighbor search. We consider, in par-
ticular, the IVFADC indexing structure [25]. The IVFADC
index relies on two levels of quantization, and the database
vectors are encoded. The database vector y is approximated
as:

y ≈ q(y) = q1(y) + q2(y − q1(y))
(3)
where q1 : Rd → C1 ⊂ Rd and q2 : Rd → C2 ⊂ Rd are quan-
tizers; i.e., functions that output an element from a ﬁnite
set. Since the sets are ﬁnite, q(y) is encoded as the index of
q1(y) and that of q2(y − q1(y)). The ﬁrst-level quantizer is a
coarse quantizer and the second level ﬁne quantizer encodes
the residual vector after the ﬁrst level.

The Asymmetric Distance Computation (ADC) search

method returns an approximate result:

LADC = k-argmini=0:(cid:96)(cid:107)x − q(yi)(cid:107)2.

(4)

For IVFADC the search is not exhaustive. Vectors for
which the distance is computed are pre-selected depending
on the ﬁrst-level quantizer q1:

LIVF = τ -argminc∈C1

(cid:107)x − c(cid:107)2.

(5)

The multi-probe parameter τ is the number of coarse-level
centroids we consider. The quantizer operates a nearest-
neighbor search with exact distances, in the set of reproduc-
tion values. Then, the IVFADC search computes

LIVFADC =

k-argmin
i=0:(cid:96) s.t. q1(yi)∈LIVF

(cid:107)x − q(yi)(cid:107)2.

(6)

Hence, IVFADC relies on the same distance estimations as
the two-step quantization of ADC, but computes them only
on a subset of vectors.

The corresponding data structure, the inverted ﬁle, groups
the vectors yi into |C1| inverted lists I1, ..., I|C1| with homo-
geneous q1(yi). Therefore, the most memory-intensive op-
eration is computing LIVFADC, and boils down to linearly
scanning τ inverted lists.

The quantizers. The quantizers q1 and q2 have diﬀerent
properties. q1 needs to have a relatively low number of repro-
duction values so that the number of inverted lists does not
(cid:96), trained via k-means.
explode. We typically use |C1| ≈
For q2, we can aﬀord to spend more memory for a more ex-
tensive representation. The ID of the vector (a 4- or 8-byte
integer) is also stored in the inverted lists, so it makes no
sense to have shorter codes than that; i.e., log2 |C2| > 4 × 8.

√

Product quantizer. We use a product quantizer [25] for q2,
which provides a large number of reproduction values with-
out increasing the processing cost. It interprets the vector y
as b sub-vectors y = [y0...yb−1], where b is an even divisor of

2

the dimension d. Each sub-vector is quantized with its own
quantizer, yielding the tuple (q0(y0), ..., qb−1(yb−1)). The
sub-quantizers typically have 256 reproduction values, to ﬁt
in one byte. The quantization value of the product quantizer
is then q2(y) = q0(y0) + 256 × q1(y1) + ... + 256b−1 × qb−1,
which from a storage point of view is just the concatena-
tion of the bytes produced by each sub-quantizer. Thus, the
product quantizer generates b-byte codes with |C2| = 256b
reproduction values. The k-means dictionaries of the quan-
tizers are small and quantization is computationally cheap.

3. GPU: OVERVIEW AND K-SELECTION
This section reviews salient details of Nvidia’s general-
purpose GPU architecture and programming model [30]. We
then focus on one of the less GPU-compliant parts involved
in similarity search, namely the k-selection, and discuss the
literature and challenges.

3.1 Architecture

GPU lanes and warps. The Nvidia GPU is a general-
purpose computer that executes instruction streams using
a 32-wide vector of CUDA threads (the warp); individual
threads in the warp are referred to as lanes, with a lane
ID from 0 – 31. Despite the “thread” terminology, the best
analogy to modern vectorized multicore CPUs is that each
warp is a separate CPU hardware thread, as the warp shares
an instruction counter. Warp lanes taking diﬀerent execu-
tion paths results in warp divergence, reducing performance.
Each lane has up to 255 32-bit registers in a shared register
ﬁle. The CPU analogy is that there are up to 255 vector
registers of width 32, with warp lanes as SIMD vector lanes.

Collections of warps. A user-conﬁgurable collection of 1
to 32 warps comprises a block or a co-operative thread ar-
ray (CTA). Each block has a high speed shared memory, up
to 48 KiB in size. Individual CUDA threads have a block-
relative ID, called a thread id, which can be used to parti-
tion and assign work. Each block is run on a single core of
the GPU called a streaming multiprocessor (SM). Each SM
has functional units, including ALUs, memory load/store
units, and various special instruction units. A GPU hides
execution latencies by having many operations in ﬂight on
warps across all SMs. Each individual warp lane instruction
throughput is low and latency is high, but the aggregate
arithmetic throughput of all SMs together is 5 – 10× higher
than typical CPUs.

Grids and kernels. Blocks are organized in a grid of blocks
in a kernel. Each block is assigned a grid relative ID. The
kernel is the unit of work (instruction stream with argu-
ments) scheduled by the host CPU for the GPU to execute.
After a block runs through to completion, new blocks can
be scheduled. Blocks from diﬀerent kernels can run concur-
rently. Ordering between kernels is controllable via ordering
primitives such as streams and events.

Resources and occupancy. The number of blocks execut-
ing concurrently depends upon shared memory and register
resources used by each block. Per-CUDA thread register us-
age is determined at compilation time, while shared memory
usage can be chosen at runtime. This usage aﬀects occu-
pancy on the GPU. If a block demands all 48 KiB of shared
memory for its private usage, or 128 registers per thread as

opposed to 32, then only 1 – 2 other blocks can run concur-
rently on the same SM, resulting in low occupancy. Under
high occupancy more blocks will be present across all SMs,
allowing more work to be in ﬂight at once.

Memory types. Diﬀerent blocks and kernels communicate
through global memory, typically 4 – 32 GB in size, with 5 –
10× higher bandwidth than CPU main memory. Shared
memory is analogous to CPU L1 cache in terms of speed.
GPU register ﬁle memory is the highest bandwidth memory.
In order to maintain the high number of instructions in ﬂight
on a GPU, a vast register ﬁle is also required: 14 MB in the
latest Pascal P100, in contrast with a few tens of KB on
CPU. A ratio of 250 : 6.25 : 1 for register to shared to global
memory aggregate cross-sectional bandwidth is typical on
GPU, yielding 10 – 100s of TB/s for the register ﬁle [10].

3.2 GPU register ﬁle usage

Structured register data. Shared and register memory
usage involves eﬃciency tradeoﬀs; they lower occupancy but
can increase overall performance by retaining a larger work-
ing set in a faster memory. Making heavy use of register-
resident data at the expense of occupancy or instead of
shared memory is often proﬁtable [43].

As the GPU register ﬁle is very large, storing structured
data (not just temporary operands) is useful. A single lane
can use its (scalar) registers to solve a local task, but with
limited parallelism and storage.
Instead, lanes in a GPU
warp can instead exchange register data using the warp shuf-
ﬂe instruction, enabling warp-wide parallelism and storage.

Lane-stride register array. A common pattern to achieve
this is a lane-stride register array. That is, given elements
[ai]i=0:(cid:96), each successive value is held in a register by neigh-
boring lanes. The array is stored in (cid:96)/32 registers per lane,
with (cid:96) a multiple of 32. Lane j stores {aj, a32+j, ..., a(cid:96)−32+j},
while register r holds {a32r, a32r+1, ..., a32r+31}.

For manipulating the [ai], the register in which ai is stored
(i.e., (cid:98)i/32(cid:99)) and (cid:96) must be known at assembly time, while
the lane (i.e., i mod 32) can be runtime knowledge. A wide
variety of access patterns (shift, any-to-any) are provided;
we use the butterﬂy permutation [29] extensively.

3.3 k-selection on CPU versus GPU

k-selection algorithms, often for arbitrarily large (cid:96) and
k, can be translated to a GPU, including radix selection
and bucket selection [1], probabilistic selection [33], quick-
select [14], and truncated sorts [40]. Their performance is
dominated by multiple passes over the input in global mem-
ory. Sometimes for similarity search, the input distances are
computed on-the-ﬂy or stored only in small blocks, not in
their entirety. The full, explicit array might be too large to
ﬁt into any memory, and its size could be unknown at the
start of the processing, rendering algorithms that require
multiple passes impractical. They suﬀer from other issues
as well. Quickselect requires partitioning on a storage of
size O((cid:96)), a data-dependent memory movement. This can
result in excessive memory transactions, or requiring parallel
preﬁx sums to determine write oﬀsets, with synchronization
overhead. Radix selection has no partitioning but multiple
passes are still required.

Heap parallelism. In similarity search applications, one
is usually interested only in a small number of results, k <

3

1000 or so. In this regime, selection via max-heap is a typi-
cal choice on the CPU, but heaps do not expose much data
parallelism (due to serial tree update) and cannot saturate
SIMD execution units. The ad-heap [31] takes better advan-
tage of parallelism available in heterogeneous systems, but
still attempts to partition serial and parallel work between
appropriate execution units. Despite the serial nature of
heap update, for small k the CPU can maintain all of its
state in the L1 cache with little eﬀort, and L1 cache latency
and bandwidth remains a limiting factor. Other similarity
search components, like PQ code manipulation, tend to have
greater impact on CPU performance [2].

GPU heaps. Heaps can be similarly implemented on a
GPU [7]. However, a straightforward GPU heap implemen-
tation suﬀers from high warp divergence and irregular, data-
dependent memory movement, since the path taken for each
inserted element depends upon other values in the heap.

GPU parallel priority queues [24] improve over the serial
heap update by allowing multiple concurrent updates, but
they require a potential number of small sorts for each insert
and data-dependent memory movement. Moreover, it uses
multiple synchronization barriers through kernel launches in
diﬀerent streams, plus the additional latency of successive
kernel launches and coordination with the CPU host.

Other more novel GPU algorithms are available for small
k, namely the selection algorithm in the fgknn library [41].
This is a complex algorithm that may suﬀer from too many
synchronization points, greater kernel launch overhead, us-
age of slower memories, excessive use of hierarchy, partition-
ing and buﬀering. However, we take inspiration from this
particular algorithm through the use of parallel merges as
seen in their merge queue structure.

4. FAST K-SELECTION ON THE GPU

For any CPU or GPU algorithm, either memory or arith-
metic throughput should be the limiting factor as per the
rooﬂine performance model [48]. For input from global mem-
ory, k-selection cannot run faster than the time required to
scan the input once at peak memory bandwidth. We aim to
get as close to this limit as possible. Thus, we wish to per-
form a single pass over the input data (from global memory
or produced on-the-ﬂy, perhaps fused with a kernel that is
generating the data).

We want to keep intermediate state in the fastest memory:
the register ﬁle. The major disadvantage of register memory
is that the indexing into the register ﬁle must be known at
assembly time, which is a strong constraint on the algorithm.

4.1

In-register sorting

We use an in-register sorting primitive as a building block.
Sorting networks are commonly used on SIMD architec-
tures [13], as they exploit vector parallelism. They are eas-
ily implemented on the GPU, and we build sorting networks
with lane-stride register arrays.

We use a variant of Batcher’s bitonic sorting network [8],
which is a set of parallel merges on an array of size 2k. Each
merge takes s arrays of length t (s and t a power of 2) to s/2
arrays of length 2t, using log2(t) parallel steps. A bitonic
sort applies this merge recursively: to sort an array of length
(cid:96), merge (cid:96) arrays of length 1 to (cid:96)/2 arrays of length 2, to (cid:96)/4
arrays of length 4, successively to 1 sorted array of length (cid:96),
leading to 1

2 (log2((cid:96))2 + log2((cid:96))) parallel merge steps.

Algorithm 1 Odd-size merging network

function merge-odd([Li]i=0:(cid:96)L , [Ri]i=0:(cid:96)R )
parallel for i ← 0 : min((cid:96)L, (cid:96)R) do

(cid:46) inverted 1st stage; inputs are already sorted

compare-swap(L(cid:96)L−i−1, Ri)

end for
parallel do

(cid:46) If (cid:96)L = (cid:96)R and a power-of-2, these are equivalent
merge-odd-continue([Li]i=0:(cid:96)L , left)
merge-odd-continue([Ri]i=0:(cid:96)R , right)

end do
end function
function merge-odd-continue([xi]i=0:(cid:96), p)

if (cid:96) > 1 then

h ← 2(cid:100)log2 (cid:96)(cid:101)−1
parallel for i ← 0 : (cid:96) − h do

(cid:46) largest power-of-2 < (cid:96)

(cid:46) Implemented with warp shuﬄe butterﬂy

compare-swap(xi, xi+h)

end for
parallel do

if p = left then

(cid:46) left side recursion

merge-odd-continue([xi]i=0:(cid:96)−h, left)
merge-odd-continue([xi]i=(cid:96)−h:(cid:96), right)

else

(cid:46) right side recursion

merge-odd-continue([xi]i=0:h, left)
merge-odd-continue([xi]i=h:(cid:96), right)

end if

end do

end if
end function

Odd-size merging and sorting networks. If some input
data is already sorted, we can modify the network to avoid
merging steps. We may also not have a full power-of-2 set of
data, in which case we can eﬃciently shortcut to deal with
the smaller size.

Algorithm 1 is an odd-sized merging network that merges
already sorted left and right arrays, each of arbitrary length.
While the bitonic network merges bitonic sequences, we start
with monotonic sequences: sequences sorted monotonically.
A bitonic merge is made monotonic by reversing the ﬁrst
comparator stage.

The odd size algorithm is derived by considering arrays to
be padded to the next highest power-of-2 size with dummy

Figure 1: Odd-size network merging arrays of sizes
5 and 3. Bullets indicate parallel compare/swap.
Dashed lines are elided elements or comparisons.

4

Figure 2: Overview of WarpSelect. The input val-
ues stream in on the left, and the warp queue on the
right holds the output result.

elements that are never swapped (the merge is monotonic)
and are already properly positioned; any comparisons with
dummy elements are elided. A left array is considered to
be padded with dummy elements at the start; a right ar-
ray has them at the end. A merge of two sorted arrays
of length (cid:96)L and (cid:96)R to a sorted array of (cid:96)L + (cid:96)R requires
(cid:100)log2(max((cid:96)L, (cid:96)R))(cid:101) + 1 parallel steps. Figure 1 shows Algo-
rithm 1’s merging network for arrays of size 5 and 3, with 4
parallel steps.

The compare-swap is implemented using warp shuﬄes on
a lane-stride register array. Swaps with a stride a multiple
of 32 occur directly within a lane as the lane holds both
elements locally. Swaps of stride ≤ 16 or a non-multiple of
32 occur with warp shuﬄes. In practice, used array lengths
are multiples of 32 as they are held in lane-stride arrays.

Algorithm 2 Odd-size sorting network

function sort-odd([xi]i=0:(cid:96))

if (cid:96) > 1 then

parallel do

sort-odd([xi]i=0:(cid:98)(cid:96)/2(cid:99))
sort-odd([xi]i=(cid:98)(cid:96)/2(cid:99):(cid:96))

end do
merge-odd([xi]i=0:(cid:98)(cid:96)/2(cid:99), [xi]i=(cid:98)(cid:96)/2(cid:99):(cid:96))

end if
end function

Algorithm 2 extends the merge to a full sort. Assuming no
2 ((cid:100)log2((cid:96))(cid:101)2 +(cid:100)log2((cid:96))(cid:101))

structure present in the input data, 1
parallel steps are required for sorting data of length (cid:96).

4.2 WarpSelect

Our k-selection implementation, WarpSelect, maintains
state entirely in registers, requires only a single pass over
data and avoids cross-warp synchronization. It uses merge-
odd and sort-odd as primitives. Since the register ﬁle pro-
vides much more storage than shared memory, it supports
k ≤ 1024. Each warp is dedicated to k-selection to a single
one of the n arrays [ai]. If n is large enough, a single warp
per each [ai] will result in full GPU occupancy. Large (cid:96) per
warp is handled by recursive decomposition, if (cid:96) is known in
advance.

Overview. Our approach (Algorithm 3 and Figure 2) oper-
ates on values, with associated indices carried along (omit-
ted from the description for simplicity). It selects the k least
values that come from global memory, or from intermediate
value registers if fused into another kernel providing the val-
ues. Let [ai]i=0:(cid:96) be the sequence provided for selection.

The elements (on the left of Figure 2) are processed in
groups of 32, the warp size. Lane j is responsible for pro-
cessing {aj, a32+j, ...}; thus, if the elements come from global
memory, the reads are contiguous and coalesced into a min-
imal number of memory transactions.

i ≥ T j

Data structures. Each lane j maintains a small queue
of t elements in registers, called the thread queues [T j
i ]i=0:t,
ordered from largest to smallest (T j
i+1). The choice of
t is made relative to k, see Section 4.3. The thread queue is
a ﬁrst-level ﬁlter for new values coming in. If a new a32i+j
is greater than the largest key currently in the queue, T j
0 , it
is guaranteed that it won’t be in the k smallest ﬁnal results.
The warp shares a lane-stride register array of k smallest
seen elements, [Wi]i=0:k, called the warp queue. It is ordered
from smallest to largest (Wi ≤ Wi+1); if the requested k is
not a multiple of 32, we round it up. This is a second level
data structure that will be used to maintain all of the k
smallest warp-wide seen values. The thread and warp queues
are initialized to maximum sentinel values, e.g., +∞.

Update. The three invariants maintained are:

• all per-lane T j

0 are not in the min-k

• all per-lane T j

0 are greater than all warp queue keys

Wi

• all ai seen so far in the min-k are contained in either
i ]i=0:t,j=0:32), or in the

some lane’s thread queue ([T j
warp queue.

Lane j receives a new a32i+j and attempts to insert it into
0 , then the new pair is by

its thread queue. If a32i+j > T j
deﬁnition not in the k minimum, and can be rejected.

Otherwise, it is inserted into its proper sorted position
in the thread queue, thus ejecting the old T j
0 . All lanes
complete doing this with their new received pair and their
thread queue, but it is now possible that the second invariant
have been violated. Using the warp ballot instruction, we
determine if any lane has violated the second invariant. If
not, we are free to continue processing new elements.

Restoring the invariants.
If any lane has its invariant
violated, then the warp uses odd-merge to merge and sort
the thread and warp queues together. The new warp queue

Algorithm 3 WarpSelect pseudocode for lane j

function WarpSelect(a)

if a < T j

0 then

insert a into our [T j

i ]i=0:t

end if
if warp-ballot(T j

0 < Wk−1) then

(cid:46) Reinterpret thread queues as lane-stride array

[αi]i=0:32t ← cast([T j

i ]i=0:t,j=0:32)

(cid:46) concatenate and sort thread queues

sort-odd([αi]i=0:32t)
merge-odd([Wi]i=0:k, [αi]i=0:32t)

(cid:46) Reinterpret lane-stride array as thread queues
i ]i=0:t,j=0:32 ← cast([αi]i=0:32t)

[T j
reverse-array([Ti]i=0:t)

(cid:46) Back in thread queue order, invariant restored

end if
end function

5

will be the min-k elements across the merged, sorted queues,
and the new thread queues will be the remainder, from min-
(k + 1) to min-(k + 32t + 1). This restores the invariants and
we are free to continue processing subsequent elements.

Since the thread and warp queues are already sorted, we
merge the sorted warp queue of length k with 32 sorted
arrays of length t. Supporting odd-sized merges is important
because Batcher’s formulation would require that 32t = k
and is a power-of-2; thus if k = 1024, t must be 32. We
found that the optimal t is way smaller (see below).

Using odd-merge to merge the 32 already sorted thread
queues would require a struct-of-arrays to array-of-structs
transposition in registers across the warp, since the t succes-
sive sorted values are held in diﬀerent registers in the same
lane rather than a lane-stride array. This is possible [12],
but would use a comparable number of warp shuﬄes, so we
just reinterpret the thread queue registers as an (unsorted)
lane-stride array and sort from scratch. Signiﬁcant speedup
is realizable by using odd-merge for the merge of the ag-
gregate sorted thread queues with the warp queue.

Handling the remainder. If there are remainder elements
because (cid:96) is not a multiple of 32, those are inserted into the
thread queues for the lanes that have them, after which we
proceed to the output stage.

Output. A ﬁnal sort and merge is made of the thread and
warp queues, after which the warp queue holds all min-k
values.

4.3 Complexity and parameter selection

For each incoming group of 32 elements, WarpSelect
can perform 1, 2 or 3 constant-time operations, all happen-
ing in warp-wide parallel time:

1. read 32 elements, compare to all thread queue heads

T j
0 , cost C1, happens N1 times;

2. if ∃j ∈ {0, ..., 31}, a32n+j < T j

0 , perform insertion sort
on those speciﬁc thread queues, cost C2 = O(t), hap-
pens N2 times;

3. if ∃j, T j

0 < Wk−1, sort and merge queues, cost C3 =
O(t log(32t)2 + k log(max(k, 32t))), happens N3 times.

Thus, the total cost is N1C1 + N2C2 + N3C3. N1 = (cid:96)/32,
and on random data drawn independently, N2 = O(k log((cid:96)))
and N3 = O(k log((cid:96))/t), see the Appendix for a full deriva-
tion. Hence, the trade-oﬀ is to balance a cost in N2C2 and
one in N3C3. The practical choice for t given k and (cid:96) was
made by experiment on a variety of k-NN data. For k ≤ 32,
we use t = 2, k ≤ 128 uses t = 3, k ≤ 256 uses t = 4, and
k ≤ 1024 uses t = 8, all irrespective of (cid:96).

5. COMPUTATION LAYOUT

This section explains how IVFADC, one of the indexing
methods originally built upon product quantization [25], is
implemented eﬃciently. Details on distance computations
and articulation with k-selection are the key to understand-
ing why this method can outperform more recent GPU-
compliant approximate nearest neighbor strategies [47].

5.1 Exact search

We brieﬂy come back to the exhaustive search method,
often referred to as exact brute-force. It is interesting on its

own for exact nearest neighbor search in small datasets. It
is also a component of many indexes in the literature. In
our case, we use it for the IVFADC coarse quantizer q1.

As stated in Section 2, the distance computation boils
down to a matrix multiplication. We use optimized GEMM
routines in the cuBLAS library to calculate the −2(cid:104)xj, yi(cid:105)
term for L2 distance, resulting in a partial distance matrix
D(cid:48). To complete the distance calculation, we use a fused
k-selection kernel that adds the (cid:107)yi(cid:107)2 term to each entry of
the distance matrix and immediately submits the value to
k-selection in registers. The (cid:107)xj(cid:107)2 term need not be taken
into account before k-selection. Kernel fusion thus allows
for only 2 passes (GEMM write, k-select read) over D(cid:48), com-
pared to other implementations that may require 3 or more.
Row-wise k-selection is likely not fusable with a well-tuned
GEMM kernel, or would result in lower overall eﬃciency.

As D(cid:48) does not ﬁt in GPU memory for realistic problem
sizes, the problem is tiled over the batch of queries, with
tq ≤ nq queries being run in a single tile. Each of the (cid:100)nq/tq(cid:101)
tiles are independent problems, but we run two in parallel
on diﬀerent streams to better occupy the GPU, so the eﬀec-
tive memory requirement of D is O(2(cid:96)tq). The computation
can similarly be tiled over (cid:96). For very large input coming
from the CPU, we support buﬀering with pinned memory
to overlap CPU to GPU copy with GPU compute.

5.2

IVFADC indexing

PQ lookup tables. At its core, the IVFADC requires com-
puting the distance from a vector to a set of product quanti-
zation reproduction values. By developing Equation (6) for
a database vector y, we obtain:

(cid:107)x − q(y)(cid:107)2

2 = (cid:107)x − q1(y) − q2(y − q1(y))(cid:107)2
2.

(7)

If we decompose the residual vectors left after q1 as:

y − q1(y) = [ (cid:101)y1 · · · (cid:101)yb] and

x − q1(y) = [(cid:102)x1 · · · (cid:101)xb]

(8)

(9)

then the distance is rewritten as:

(cid:107)x − q(y)(cid:107)2

2 = (cid:107)(cid:102)x1 − q1( (cid:101)y1)(cid:107)2

2 + ... + (cid:107) (cid:101)xb − qb( (cid:101)yb)(cid:107)2
2.

(10)

Each quantizer q1, ..., qb has 256 reproduction values, so
when x and q1(y) are known all distances can be precom-
puted and stored in tables T1, ..., Tb each of size 256 [25].
Computing the sum (10) consists of b look-ups and addi-
tions. Comparing the cost to compute n distances:

• Explicit computation: n × d mutiply-adds;

• With lookup tables: 256 × d multiply-adds and n × b

lookup-adds.

This is the key to the eﬃciency of the product quantizer.
In our GPU implementation, b is any multiple of 4 up to
64. The codes are stored as sequential groups of b bytes per
vector within lists.

IVFADC lookup tables. When scanning over the ele-
ments of the inverted list IL (where by deﬁnition q1(y) is
constant), the look-up table method can be applied, as the
query x and q1(y) are known.

6

Moreover, the computation of the tables T1 . . . Tb is fur-
2 in Equation

ther optimized [5]. The expression of (cid:107)x−q(y)(cid:107)2
(7) can be decomposed as:

(cid:107)q2(...)(cid:107)2
(cid:124)

2 + 2(cid:104)q1(y), q2(...)(cid:105)
(cid:125)

(cid:123)(cid:122)
term 1

+ (cid:107)x − q1(y)(cid:107)2
2
(cid:123)(cid:122)
(cid:125)
term 2

(cid:124)

−2 (cid:104)x, q2(...)(cid:105)
(cid:125)

(cid:123)(cid:122)
term 3

(cid:124)

.

(11)
The objective is to minimize inner loop computations.
The computations we can do in advance and store in lookup
tables are as follows:

• Term 1 is independent of the query. It can be precom-
puted from the quantizers, and stored in a table T of
size |C1| × 256 × b;

• Term 2 is the distance to q1’s reproduction value. It is

thus a by-product of the ﬁrst-level quantizer q1;

• Term 3 can be computed independently of the inverted
list. Its computation costs d × 256 multiply-adds.

This decomposition is used to produce the lookup tables
T1 . . . Tb used during the scan of the inverted list. For a
single query, computing the τ × b tables from scratch costs
τ × d × 256 multiply-adds, while this decomposition costs
256×d multiply-adds and τ ×b×256 additions. On the GPU,
the memory usage of T can be prohibitive, so we enable the
decomposition only when memory is a not a concern.

5.3 GPU implementation

Algorithm 4 summarizes the process as one would im-
plement it on a CPU. The inverted lists are stored as two
separate arrays, for PQ codes and associated IDs. IDs are
resolved only if k-selection determines k-nearest member-
ship. This lookup yields a few sparse memory reads in a
large array, thus the IDs can optionally be stored on CPU
for tiny performance cost.

List scanning. A kernel is responsible for scanning the τ
closest inverted lists for each query, and calculating the per-
vector pair distances using the lookup tables Ti. The Ti are
stored in shared memory: up to nq ×τ ×maxi |Ii|×b lookups
are required for a query set (trillions of accesses in practice),
and are random access. This limits b to at most 48 (32-
bit ﬂoating point) or 96 (16-bit ﬂoating point) with current
architectures. In case we do not use the decomposition of
Equation (11), the Ti are calculated by a separate kernel
before scanning.

Multi-pass kernels. Each nq × τ pairs of query against
inverted list can be processed independently. At one ex-
treme, a block is dedicated to each of these, resulting in up
to nq × τ × maxi |Ii| partial results being written back to
global memory, which is then k-selected to nq × k ﬁnal re-
sults. This yields high parallelism but can exceed available
GPU global memory; as with exact search, we choose a tile
size tq ≤ nq to reduce memory consumption, bounding its
complexity by O(2tqτ maxi |Ii|) with multi-streaming.

A single warp could be dedicated to k-selection of each
tq set of lists, which could result in low parallelism. We
introduce a two-pass k-selection, reducing tq × τ × maxi |Ii|
to tq × f × k partial results for some subdivision factor f .
This is reduced again via k-selection to the ﬁnal tq×k results.

Fused kernel. As with exact search, we experimented with
a kernel that dedicates a single block to scanning all τ lists

7

for a single query, with k-selection fused with distance com-
putation. This is possible as WarpSelect does not ﬁght for
the shared memory resource which is severely limited. This
reduces global memory write-back, since almost all interme-
diate results can be eliminated. However, unlike k-selection
overhead for exact computation, a signiﬁcant portion of the
runtime is the gather from the Ti in shared memory and lin-
ear scanning of the Ii from global memory; the write-back is
not a dominant contributor. Timing for the fused kernel is
improved by at most 15%, and for some problem sizes would
be subject to lower parallelism and worse performance with-
out subsequent decomposition. Therefore, and for reasons
of implementation simplicity, we do not use this layout.

Algorithm 4 IVFPQ batch search routine

function ivfpq-search([x1, ..., xnq ], I1, ..., I|C1|)

for i ← 0 : nq do (cid:46) batch quantization of Section 5.1

IVF ← τ -argminc∈C1

Li
end for
for i ← 0 : nq do

(cid:107)x − c(cid:107)2

L ← []
Compute term 3 (see Section 5.2)
for L in Li

IVF do

Compute distance tables T1, ..., Tb
for j in IL do

(cid:46) distance table

(cid:46) τ loops

(cid:46) distance estimation, Equation (10)

d ← (cid:107)xi − q(yj)(cid:107)2
2
Append (d, L, j) to L

end for

end for
Ri ← k-select smallest distances d from L

end for
return R

end function

5.4 Multi-GPU parallelism

Modern servers can support several GPUs. We employ

this capability for both compute power and memory.

Replication. If an index instance ﬁts in the memory of a
single GPU, it can be replicated across R diﬀerent GPUs. To
query nq vectors, each replica handles a fraction nq/R of the
queries, joining the results back together on a single GPU
or in CPU memory. Replication has near linear speedup,
except for a potential loss in eﬃciency for small nq.

Sharding. If an index instance does not ﬁt in the memory
of a single GPU, an index can be sharded across S diﬀer-
ent GPUs. For adding (cid:96) vectors, each shard receives (cid:96)/S of
the vectors, and for query, each shard handles the full query
set nq, joining the partial results (an additional round of k-
selection is still required) on a single GPU or in CPU mem-
ory. For a given index size (cid:96), sharding will yield a speedup
(sharding has a query of nq against (cid:96)/S versus replication
with a query of nq/R against (cid:96)), but is usually less than
pure replication due to ﬁxed overhead and cost of subse-
quent k-selection.

Replication and sharding can be used together (S shards,
each with R replicas for S × R GPUs in total). Sharding or
replication are both fairly trivial, and the same principle can
be used to distribute an index across multiple machines.

method
BIDMach [11]
Ours
Ours

# GPUs
1
1
4

# centroids
4096
256
735 s
320 s
316 s
140 s
100 s
84 s

Table 1: MNIST8m k-means performance

6.2 k-means clustering

The exact search method with k = 1 can be used by a k-
means clustering method in the assignment stage, to assign
nq training vectors to |C1| centroids. Despite the fact that
it does not use the IVFADC and k = 1 selection is trivial (a
parallel reduction is used for the k = 1 case, not WarpSe-
lect), k-means is a good benchmark for the clustering used
to train the quantizer q1.

We apply the algorithm on MNIST8m images. The 8.1M
images are graylevel digits in 28x28 pixels, linearized to vec-
tors of 784-d. We compare this k-means implementation to
the GPU k-means of BIDMach [11], which was shown to be
more eﬃcient than several distributed k-means implemen-
tations that require dozens of machines3. Both algorithms
were run for 20 iterations. Table 1 shows that our imple-
mentation is more than 2× faster, although both are built
upon cuBLAS. Our implementation receives some beneﬁt
from the k-selection fusion into L2 distance computation.
For multi-GPU execution via replicas, the speedup is close
to linear for large enough problems (3.16× for 4 GPUs with
4096 centroids). Note that this benchmark is somewhat un-
realistic, as one would typically sub-sample the dataset ran-
domly when so few centroids are requested.

Large scale. We can also compare to [3], an approximate
CPU method that clusters 108 128-d vectors to 85k cen-
troids. Their clustering method runs in 46 minutes, but re-
quires 56 minutes (at least) of pre-processing to encode the
vectors. Our method performs exact k-means on 4 GPUs in
52 minutes without any pre-processing.

6.3 Exact nearest neighbor search

We consider a classical dataset used to evaluate nearest
neighbor search: Sift1M [25].
Its characteristic sizes are
(cid:96) = 106, d = 128, nq = 104. Computing the partial distance
matrix D(cid:48) costs nq × (cid:96) × d = 1.28 Tﬂop, which runs in less
than one second on current GPUs. Figure 4 shows the cost
of the distance computations against the cost of our tiling
of the GEMM for the −2 (cid:104)xj, yi(cid:105) term of Equation 2 and
the peak possible k-selection performance on the distance
matrix of size nq ×(cid:96), which additionally accounts for reading
the tiled result matrix D(cid:48) at peak memory bandwidth.

In addition to our method from Section 5, we include
times from the two GPU libraries evaluated for k-selection
performance in Section 6.1. We make several observations:

• for k-selection, the naive algorithm that sorts the full
result array for each query using thrust::sort_by_key
is more than 10× slower than the comparison methods;

• L2 distance and k-selection cost is dominant for all but
our method, which has 85 % of the peak possible
performance, assuming GEMM usage and our tiling

3BIDMach numbers from https://github.com/BIDData/
BIDMach/wiki/Benchmarks#KMeans

8

Figure 3: Runtimes for diﬀerent k-selection meth-
ods, as a function of array length (cid:96). Simultaneous
arrays processed are nq = 10000. k = 100 for full lines,
k = 1000 for dashed lines.

6. EXPERIMENTS & APPLICATIONS

This section compares our GPU k-selection and nearest-
neighbor approach to existing libraries. Unless stated other-
wise, experiments are carried out on a 2×2.8GHz Intel Xeon
E5-2680v2 with 4 Maxwell Titan X GPUs on CUDA 8.0.

6.1 k-selection performance

We compare against two other GPU small k-selection im-
plementations: the row-based Merge Queue with Buﬀered
Search and Hierarchical Partition extracted from the fgknn
library of Tang et al. [41] and Truncated Bitonic Sort (TBiS )
from Sismanis et al. [40]. Both were extracted from their re-
spective exact search libraries.

We evaluate k-selection for k = 100 and 1000 of each row
from a row-major matrix nq × (cid:96) of random 32-bit ﬂoating
point values on a single Titan X. The batch size nq is ﬁxed
at 10000, and the array lengths (cid:96) vary from 1000 to 128000.
Inputs and outputs to the problem remain resident in GPU
memory, with the output being of size nq × k, with corre-
sponding indices. Thus, the input problem sizes range from
40 MB ((cid:96) = 1000) to 5.12 GB ((cid:96) = 128k). TBiS requires large
auxiliary storage, and is limited to (cid:96) ≤ 48000 in our tests.

Figure 3 shows our relative performance against TBiS and
fgknn. It also includes the peak possible performance given
by the memory bandwidth limit of the Titan X. The rela-
tive performance of WarpSelect over fgknn increases for
larger k; even TBiS starts to outperform fgknn for larger (cid:96)
at k = 1000. We look especially at the largest (cid:96) = 128000.
WarpSelect is 1.62× faster at k = 100, 2.01× at k = 1000.
Performance against peak possible drops oﬀ for all imple-
mentations at larger k. WarpSelect operates at 55% of
peak at k = 100 but only 16% of peak at k = 1000. This
is due to additional overhead assocated with bigger thread
queues and merge/sort networks for large k.

Diﬀerences from fgknn. WarpSelect is inﬂuenced by
fgknn, but has several improvements: all state is maintained
in registers (no shared memory), no inter-warp synchroniza-
tion or buﬀering is used, no “hierarchical partition”, the k-
selection can be fused into other kernels, and it uses odd-size
networks for eﬃcient merging and sorting.

Figure 4: Exact search k-NN time for the SIFT1M
dataset with varying k on 1 Titan X GPU.

of the partial distance matrix D(cid:48) on top of GEMM is
close to optimal. The cuBLAS GEMM itself has low
eﬃciency for small reduction sizes (d = 128);

• Our fused L2/k-selection kernel is important. Our
same exact algorithm without fusion (requiring an ad-
ditional pass through D(cid:48)) is at least 25% slower.

Eﬃcient k-selection is even more important in situations
where approximate methods are used to compute distances,
because the relative cost of k-selection with respect to dis-
tance computation increases.

6.4 Billion-scale approximate search

There are few studies on GPU-based approximate nearest-
neighbor search on large datasets ((cid:96) (cid:29) 106). We report a
few comparison points here on index search, using standard
datasets and evaluation protocol in this ﬁeld.

SIFT1M. For the sake of completeness, we ﬁrst compare
our GPU search speed on Sift1M with the implementation
of Wieschollek et al. [47]. They obtain a nearest neighbor re-
call at 1 (fraction of queries where the true nearest neighbor
is in the top 1 result) of R@1 = 0.51, and R@100 = 0.86 in
0.02 ms per query on a Titan X. For the same time budget,
our implementation obtains R@1 = 0.80 and R@100 = 0.95.

SIFT1B. We compare again with Wieschollek et al., on the
Sift1B dataset [26] of 1 billion SIFT image features at nq =
104. We compare the search performance in terms of same
memory usage for similar accuracy (more accurate methods
may involve greater search time or memory usage). On a
single GPU, with m = 8 bytes per vector, R@10 = 0.376 in
17.7 µs per query vector, versus their reported R@10 = 0.35
in 150 µs per query vector. Thus, our implementation is
more accurate at a speed 8.5× faster.

DEEP1B. We also experimented on the Deep1B dataset [6]
of (cid:96)=1 billion CNN representations for images at nq = 104.
The paper that introduces the dataset reports CPU results
(1 thread): R@1 = 0.45 in 20 ms search time per vector. We
use a PQ encoding of m = 20, with d = 80 via OPQ [17],
and |C1| = 218, which uses a comparable dataset storage as
the original paper (20 GB). This requires multiple GPUs as
it is too large for a single GPU’s global memory, so we con-
sider 4 GPUs with S = 2, R = 2. We obtain a R@1 = 0.4517
in 0.0133 ms per vector. While the hardware platforms are

Figure 5: Speed/accuracy trade-oﬀ of brute-force
10-NN graph construction for the YFCC100M and
DEEP1B datasets.

diﬀerent, it shows that making searches on GPUs is a game-
changer in terms of speed achievable on a single machine.

6.5 The k-NN graph

An example usage of our similarity search method is to
construct a k-nearest neighbor graph of a dataset via brute
force (all vectors queried against the entire index).

Experimental setup. We evaluate the trade-oﬀ between
speed, precision and memory on two datasets: 95 million
images from the Yfcc100M dataset [42] and Deep1B. For
Yfcc100M, we compute CNN descriptors as the one-before-
last layer of a ResNet [23], reduced to d = 128 with PCA.

The evaluation measures the trade-oﬀ between:

• Speed: How much time it takes to build the IVFADC
index from scratch and construct the whole k-NN graph
(k = 10) by searching nearest neighbors for all vectors
in the dataset. Thus, this is an end-to-end test that
includes indexing as well as search time;

• Quality: We sample 10,000 images for which we com-
pute the exact nearest neighbors. Our accuracy mea-
sure is the fraction of 10 found nearest neighbors that
are within the ground-truth 10 nearest neighbors.

For Yfcc100M, we use a coarse quantizer (216 centroids),
and consider m = 16, 32 and 64 byte PQ encodings for each
vector. For Deep1B, we pre-process the vectors to d = 120
via OPQ, use |C1| = 218 and consider m = 20, 40. For a
given encoding, we vary τ from 1 to 256, to obtain trade-
oﬀs between eﬃciency and quality, as seen in Figure 5.

9

Figure 6: Path in the k-NN graph of 95 million images from YFCC100M. The ﬁrst and the last image are
given; the algorithm computes the smoothest path between them.

Discussion. For Yfcc100M we used S = 1, R = 4. An
accuracy of more than 0.8 is obtained in 35 minutes. For
Deep1B, a lower-quality graph can be built in 6 hours,
with higher quality in about half a day. We also experi-
mented with more GPUs by doubling the replica set, us-
ing 8 Maxwell M40s (the M40 is roughly equivalent in per-
formance to the Titan X). Performance is improved sub-
linearly (∼ 1.6× for m = 20, ∼ 1.7× for m = 40).

For comparison, the largest k-NN graph construction we
are aware of used a dataset comprising 36.5 million 384-
d vectors, which took a cluster of 128 CPU servers 108.7
hours of compute [45], using NN-Descent [15]. Note that
NN-Descent could also build or reﬁne the k-NN graph for
the datasets we consider, but it has a large memory over-
head over the graph storage, which is already 80 GB for
Deep1B. Moreover it requires random access across all vec-
tors (384 GB for Deep1B).

The largest GPU k-NN graph construction we found is a
brute-force construction using exact search with GEMM, of
a dataset of 20 million 15,000-d vectors, which took a cluster
of 32 Tesla C2050 GPUs 10 days [14]. Assuming computa-
tion scales with GEMM cost for the distance matrix, this
approach for Deep1B would take an impractical 200 days
of computation time on their cluster.

6.6 Using the k-NN graph

When a k-NN graph has been constructed for an image
dataset, we can ﬁnd paths in the graph between any two
images, provided there is a single connected component (this
is the case). For example, we can search the shortest path
between two images of ﬂowers, by propagating neighbors
from a starting image to a destination image. Denoting by
S and D the source and destination images, and dij the
distance between nodes, we search the path P = {p1, ..., pn}
with p1 = S and pn = D such that

min
P

max
i=1..n

dpipi+1 ,

(12)

i.e., we want to favor smooth transitions. An example re-
sult is shown in Figure 6 from Yfcc100M4. It was ob-
tained after 20 seconds of propagation in a k-NN graph with
k = 15 neighbors. Since there are many ﬂower images in the
dataset, the transitions are smooth.

4The mapping from vectors to images is not available for
Deep1B

7. CONCLUSION

The arithmetic throughput and memory bandwidth of
GPUs are well into the teraﬂops and hundreds of gigabytes
per second. However,
implementing algorithms that ap-
proach these performance levels is complex and counter-
intuitive. In this paper, we presented the algorithmic struc-
ture of similarity search methods that achieves near-optimal
performance on GPUs.

This work enables applications that needed complex ap-
proximate algorithms before. For example, the approaches
presented here make it possible to do exact k-means cluster-
ing or to compute the k-NN graph with simple brute-force
approaches in less time than a CPU (or a cluster of them)
would take to do this approximately.

GPU hardware is now very common on scientiﬁc work-
stations, due to their popularity for machine learning algo-
rithms. We believe that our work further demonstrates their
interest for database applications. Along with this work, we
are publishing a carefully engineered implementation of this
paper’s algorithms, so that these GPUs can now also be used
for eﬃcient similarity search.

8. REFERENCES
[1] T. Alabi, J. D. Blanchard, B. Gordon, and R. Steinbach.
Fast k-selection algorithms for graphics processing units.
ACM Journal of Experimental Algorithmics,
17:4.2:4.1–4.2:4.29, October 2012.

[2] F. Andr´e, A.-M. Kermarrec, and N. L. Scouarnec. Cache

locality is not enough: High-performance nearest neighbor
search with product quantization fast scan. In Proc.
International Conference on Very Large DataBases, pages
288–299, 2015.

[3] Y. Avrithis, Y. Kalantidis, E. Anagnostopoulos, and I. Z.
Emiris. Web-scale image clustering revisited. In Proc.
International Conference on Computer Vision, pages
1502–1510, 2015.

[4] A. Babenko and V. Lempitsky. The inverted multi-index.
In Proc. IEEE Conference on Computer Vision and
Pattern Recognition, pages 3069–3076, June 2012.

[5] A. Babenko and V. Lempitsky. Improving bilayer product

quantization for billion-scale approximate nearest neighbors
in high dimensions. arXiv preprint arXiv:1404.1831, 2014.

[6] A. Babenko and V. Lempitsky. Eﬃcient indexing of

billion-scale datasets of deep descriptors. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition,
pages 2055–2063, June 2016.

[7] R. Barrientos, J. G´omez, C. Tenllado, M. Prieto, and

M. Marin. knn query processing in metric spaces using
GPUs. In International European Conference on Parallel
and Distributed Computing, volume 6852 of Lecture Notes

10

in Computer Science, pages 380–392, Bordeaux, France,
September 2011. Springer.

[8] K. E. Batcher. Sorting networks and their applications. In
Proc. Spring Joint Computer Conference, AFIPS ’68
(Spring), pages 307–314, New York, NY, USA, 1968. ACM.

[9] P. Boncz, W. Lehner, and T. Neumann. Special issue:
Modern hardware. The VLDB Journal, 25(5):623–624,
2016.

[10] J. Canny, D. L. W. Hall, and D. Klein. A multi-teraﬂop
constituency parser using GPUs. In Proc. Empirical
Methods on Natural Language Processing, pages 1898–1907.
ACL, 2013.

[11] J. Canny and H. Zhao. Bidmach: Large-scale learning with
zero memory allocation. In BigLearn workshop, NIPS,
2013.

[12] B. Catanzaro, A. Keller, and M. Garland. A decomposition

for in-place matrix transposition. In Proc. ACM
Symposium on Principles and Practice of Parallel
Programming, PPoPP ’14, pages 193–206, 2014.

[13] J. Chhugani, A. D. Nguyen, V. W. Lee, W. Macy,
M. Hagog, Y.-K. Chen, A. Baransi, S. Kumar, and
P. Dubey. Eﬃcient implementation of sorting on multi-core
simd cpu architecture. Proc. VLDB Endow.,
1(2):1313–1324, August 2008.

[14] A. Dashti. Eﬃcient computation of k-nearest neighbor

graphs for large high-dimensional data sets on gpu clusters.
Master’s thesis, University of Wisconsin Milwaukee, August
2013.

[15] W. Dong, M. Charikar, and K. Li. Eﬃcient k-nearest

neighbor graph construction for generic similarity measures.
In WWW: Proceeding of the International Conference on
World Wide Web, pages 577–586, March 2011.

[16] M. Douze, H. J´egou, and F. Perronnin. Polysemous codes.
In Proc. European Conference on Computer Vision, pages
785–801. Springer, October 2016.

[17] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product

quantization. IEEE Trans. PAMI, 36(4):744–755, 2014.

and Signal Processing, pages 861–864, May 2011.

[27] Y. Kalantidis and Y. Avrithis. Locally optimized product
quantization for approximate nearest neighbor search. In
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pages 2329–2336, June 2014.

[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classiﬁcation with deep convolutional neural networks. In
Advances in Neural Information Processing Systems, pages
1097–1105, 2012.

[29] F. T. Leighton. Introduction to Parallel Algorithms and
Architectures: Array, Trees, Hypercubes. Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA, 1992.

[30] E. Lindholm, J. Nickolls, S. Oberman, and J. Montrym.
NVIDIA Tesla: a uniﬁed graphics and computing
architecture. IEEE Micro, 28(2):39–55, March 2008.
[31] W. Liu and B. Vinter. Ad-heap: An eﬃcient heap data

structure for asymmetric multicore processors. In Proc. of
Workshop on General Purpose Processing Using GPUs,
pages 54:54–54:63. ACM, 2014.

[32] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and

J. Dean. Distributed representations of words and phrases
and their compositionality. In Advances in Neural
Information Processing Systems, pages 3111–3119, 2013.
[33] L. Monroe, J. Wendelberger, and S. Michalak. Randomized

selection on the GPU. In Proc. ACM Symposium on High
Performance Graphics, pages 89–98, 2011.

[34] M. Norouzi and D. Fleet. Cartesian k-means. In Proc.

IEEE Conference on Computer Vision and Pattern
Recognition, pages 3017–3024, June 2013.

[35] M. Norouzi, A. Punjani, and D. J. Fleet. Fast search in

Hamming space with multi-index hashing. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition,
pages 3108–3115, 2012.

[36] J. Pan and D. Manocha. Fast GPU-based locality sensitive
hashing for k-nearest neighbor computation. In Proc. ACM
International Conference on Advances in Geographic
Information Systems, pages 211–220, 2011.

[18] Y. Gong and S. Lazebnik. Iterative quantization: A

[37] L. Paulev´e, H. J´egou, and L. Amsaleg. Locality sensitive

procrustean approach to learning binary codes. In Proc.
IEEE Conference on Computer Vision and Pattern
Recognition, pages 817–824, June 2011.

[19] Y. Gong, L. Wang, R. Guo, and S. Lazebnik. Multi-scale

orderless pooling of deep convolutional activation features.
In Proc. European Conference on Computer Vision, pages
392–407, 2014.

hashing: a comparison of hash function types and querying
mechanisms. Pattern recognition letters, 31(11):1348–1358,
August 2010.

[38] O. Shamir. Fundamental limits of online and distributed
algorithms for statistical learning and estimation. In
Advances in Neural Information Processing Systems, pages
163–171, 2014.

[20] A. Gordo, J. Almazan, J. Revaud, and D. Larlus. Deep

[39] A. Sharif Razavian, H. Azizpour, J. Sullivan, and

image retrieval: Learning global representations for image
search. In Proc. European Conference on Computer Vision,
pages 241–257, 2016.

S. Carlsson. CNN features oﬀ-the-shelf: an astounding
baseline for recognition. In CVPR workshops, pages
512–519, 2014.

[21] S. Han, H. Mao, and W. J. Dally. Deep compression:

[40] N. Sismanis, N. Pitsianis, and X. Sun. Parallel search of

Compressing deep neural networks with pruning, trained
quantization and huﬀman coding. arXiv preprint
arXiv:1510.00149, 2015.

k-nearest neighbors with synchronous operations. In IEEE
High Performance Extreme Computing Conference, pages
1–6, 2012.

[22] K. He, F. Wen, and J. Sun. K-means hashing: An

[41] X. Tang, Z. Huang, D. M. Eyers, S. Mills, and M. Guo.

aﬃnity-preserving quantization method for learning binary
compact codes. In Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pages 2938–2945, June
2013.

[23] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual

learning for image recognition. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition, pages
770–778, June 2016.

[24] X. He, D. Agarwal, and S. K. Prasad. Design and

implementation of a parallel priority queue on many-core
architectures. IEEE International Conference on High
Performance Computing, pages 1–10, 2012.

[25] H. J´egou, M. Douze, and C. Schmid. Product quantization

for nearest neighbor search. IEEE Trans. PAMI,
33(1):117–128, January 2011.

[26] H. J´egou, R. Tavenard, M. Douze, and L. Amsaleg.
Searching in one billion vectors: re-rank with source
coding. In International Conference on Acoustics, Speech,

Eﬃcient selection algorithm for fast k-nn search on GPUs.
In IEEE International Parallel & Distributed Processing
Symposium, pages 397–406, 2015.

[42] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde,

K. Ni, D. Poland, D. Borth, and L.-J. Li. YFCC100M: The
new data in multimedia research. Communications of the
ACM, 59(2):64–73, January 2016.

[43] V. Volkov and J. W. Demmel. Benchmarking GPUs to tune

dense linear algebra. In Proc. ACM/IEEE Conference on
Supercomputing, pages 31:1–31:11, 2008.

[44] A. Wakatani and A. Murakami. GPGPU implementation of
nearest neighbor search with product quantization. In
IEEE International Symposium on Parallel and Distributed
Processing with Applications, pages 248–253, 2014.
[45] T. Warashina, K. Aoyama, H. Sawada, and T. Hattori.

Eﬃcient k-nearest neighbor graph construction using
mapreduce for large-scale data sets. IEICE Transactions,

11

97-D(12):3142–3154, 2014.

[46] R. Weber, H.-J. Schek, and S. Blott. A quantitative
analysis and performance study for similarity-search
methods in high-dimensional spaces. In Proc. International
Conference on Very Large DataBases, pages 194–205, 1998.

[47] P. Wieschollek, O. Wang, A. Sorkine-Hornung, and

H. P. A. Lensch. Eﬃcient large-scale approximate nearest
neighbor search on the GPU. In Proc. IEEE Conference on
Computer Vision and Pattern Recognition, pages
2027–2035, June 2016.

[48] S. Williams, A. Waterman, and D. Patterson. Rooﬂine: An

insightful visual performance model for multicore
architectures. Communications of the ACM, 52(4):65–76,
April 2009.

Appendix: Complexity analysis of WarpSelect
We derive the average number of times updates are triggered
in WarpSelect, for use in Section 4.3.

Let the input to k-selection be a sequence {a1, a2, ..., a(cid:96)}
(1-based indexing), a randomly chosen permutation of a set
of distinct elements. Elements are read sequentially in c
groups of size w (the warp; in our case, w = 32); assume (cid:96)
is a multiple of w, so c = (cid:96)/w. Recall that t is the thread
queue length. We call elements prior to or at position n
in the min-k seen so far the successive min-k (at n). The
likelihood that an is in the successive min-k at n is:

The last case is the probability of: there is a (cid:96) − 1 se-
quence with m − 1 successive min-k elements preceding us,
and the current element is in the successive min-k, or the
current element is not in the successive min-k, m ones are
before us. We can then develop a recurrence relationship for
π((cid:96), k, t, 1). Note that

δ((cid:96), b, k, t) :=

γ((cid:96), m, k)

(17)

min((bt+max(0,t−1)),(cid:96))
(cid:88)

m=bt

for b where 0 ≤ bt ≤ (cid:96) is the fraction of all sequences of
length (cid:96) that will force b sorts of data by winning the thread
queue ballot, as there have to be bt to (bt + max(0, t − 1))
elements in the successive min-k for these sorts to happen (as
the min-k elements will overﬂow the thread queues). There
are at most (cid:98)(cid:96)/t(cid:99) won ballots that can occur, as it takes t
separate sequential current min-k seen elements to win the
ballot. π((cid:96), k, t, 1) is thus the expectation of this over all
possible b:

π((cid:96), k, t, 1) =

b · δ((cid:96), b, k, t).

(18)

(cid:98)(cid:96)/t(cid:99)
(cid:88)

b=1

α(n, k) :=

(cid:40)

1
k/n

if n ≤ k
if n > k

(13)

This can be computed by dynamic programming. Analyti-
cally, note that for t = 1, k = 1, π((cid:96), 1, 1, 1) is the harmonic
number H(cid:96) = 1 + 1
(cid:96) , which converges to ln((cid:96)) + γ
(the Euler-Mascheroni constant γ) as (cid:96) → ∞.

3 + ... + 1

2 + 1

as each an, n > k has a k/n chance as all permutations are
equally likely, and all elements in the ﬁrst k qualify.

Counting the insertion sorts. In a given lane, an inser-
tion sort is triggered if the incoming value is in the successive
min-k + t values, but the lane has “seen” only wc0 + (c − c0)
values, where c0 is the previous won warp ballot. The prob-
ability of this happening is:

α(wc0 + (c − c0), k + t) ≈

for c > k.

(14)

k + t
wc

The approximation considers that the thread queue has seen
all the wc values, not just those assigned to its lane. The
probability of any lane triggering an insertion sort is then:

(cid:18)

1 −

1 −

(cid:19)w

k + t
wc

≈

k + t
c

.

(15)

Here the approximation is a ﬁrst-order Taylor expansion.
Summing up the probabilities over c gives an expected num-
ber of insertions of N2 ≈ (k + t) log(c) = O(k log((cid:96)/w)).

Counting full sorts. We seek N3 = π((cid:96), k, t, w), the ex-
pected number of full sorts required for WarpSelect.

Single lane. For now, we assume w = 1, so c = (cid:96). Let
γ((cid:96), m, k) be the probability that in an sequence {a1, ..., a(cid:96)},
exactly m of the elements as encountered by a sequential
scanner (w = 1) are in the successive min-k. Given m, there
(cid:1) places where these successive min-k elements can
are (cid:0) (cid:96)
occur. It is given by a recurrence relation:

m

γ((cid:96), m, k) :=






1
0
0
(γ((cid:96) − 1, m − 1, k) · α((cid:96), k)+
γ((cid:96) − 1, m, k) · (1 − α((cid:96), k)))

(cid:96) = 0 and m = 0
(cid:96) = 0 and m > 0
(cid:96) > 0 and m = 0

otherwise.

(16)

12

For t = 1, k > 1, (cid:96) > k, π((cid:96), k, 1, 1) = k + k(H(cid:96) − Hk)
or O(k log((cid:96))), as the ﬁrst k elements are in the successive
k+2 +...+ k
(cid:96) .
min-k, and the expectation for the rest is

k+1 + k

k

For t > 1, k > 1, (cid:96) > k, note that there are some number
D, k ≤ D ≤ (cid:96) of successive min-k determinations D made
for each possible {a1, ..., a(cid:96)}. The number of won ballots for
each case is by deﬁnition (cid:98)D/t(cid:99), as the thread queue must
ﬁll up t times. Thus, π((cid:96), k, t, 1) = O(k log((cid:96))/t).

Multiple lanes. The w > 1 case is complicated by the
fact that there are joint probabilities to consider (if more
than one of the w workers triggers a sort for a given group,
only one sort takes place). However, the likelihood can be
bounded. Let π(cid:48)((cid:96), k, t, w) be the expected won ballots as-
suming no mutual interference between the w workers for
winning ballots (i.e., we win b ballots if there are b ≤ w
workers that independently win a ballot at a single step),
but with the shared min-k set after each sort from the joint
sequence. Assume that k ≥ w. Then:

π(cid:48)((cid:96), k, 1, w) ≤ w

(cid:25)

(cid:32) (cid:24) k
w

+

(cid:100)(cid:96)/w(cid:101)−(cid:100)k/w(cid:101)
(cid:88)

(cid:33)

k
w((cid:100)k/w(cid:101) + i)

i=1
≤ wπ((cid:100)(cid:96)/w(cid:101), k, 1, 1) = O(wk log((cid:96)/w))

(19)
where the likelihood of the w workers seeing a successive
min-k element has an upper bound of that of the ﬁrst worker
at each step. As before, the number of won ballots is scaled
by t, so π(cid:48)((cid:96), k, t, w) = O(wk log((cid:96)/w)/t). Mutual interfer-
ence can only reduce the number of ballots, so we obtain the
same upper bound for π((cid:96), k, t, w).

Billion-scale similarity search with GPUs

Jeff Johnson
Facebook AI Research
New York

Matthijs Douze
Facebook AI Research
Paris

Herv ´e J ´egou
Facebook AI Research
Paris

7
1
0
2
 
b
e
F
 
8
2
 
 
]

V
C
.
s
c
[
 
 
1
v
4
3
7
8
0
.
2
0
7
1
:
v
i
X
r
a

ABSTRACT
Similarity search ﬁnds application in specialized database
systems handling complex data such as images or videos,
which are typically represented by high-dimensional features
and require speciﬁc indexing structures. This paper tackles
the problem of better utilizing GPUs for this task. While
GPUs excel at data-parallel tasks, prior approaches are bot-
tlenecked by algorithms that expose less parallelism, such as
k-min selection, or make poor use of the memory hierarchy.
We propose a design for k-selection that operates at up
to 55% of theoretical peak performance, enabling a nearest
neighbor implementation that is 8.5× faster than prior GPU
state of the art. We apply it in diﬀerent similarity search
scenarios, by proposing optimized design for brute-force, ap-
proximate and compressed-domain search based on product
quantization. In all these setups, we outperform the state of
the art by large margins. Our implementation enables the
construction of a high accuracy k-NN graph on 95 million
images from the Yfcc100M dataset in 35 minutes, and of
a graph connecting 1 billion vectors in less than 12 hours
on 4 Maxwell Titan X GPUs. We have open-sourced our
approach1 for the sake of comparison and reproducibility.

1.

INTRODUCTION

Images and videos constitute a new massive source of data
for indexing and search. Extensive metadata for this con-
tent is often not available. Search and interpretation of this
and other human-generated content, like text, is diﬃcult and
important. A variety of machine learning and deep learn-
ing algorithms are being used to interpret and classify these
complex, real-world entities. Popular examples include the
text representation known as word2vec [32], representations
of images by convolutional neural networks [39, 19], and im-
age descriptors for instance search [20]. Such representations
or embeddings are usually real-valued, high-dimensional vec-
tors of 50 to 1000+ dimensions. Many of these vector repre-
sentations can only eﬀectively be produced on GPU systems,

1https://github.com/facebookresearch/faiss

as the underlying processes either have high arithmetic com-
plexity and/or high data bandwidth demands [28], or cannot
be eﬀectively partitioned without failing due to communi-
cation overhead or representation quality [38]. Once pro-
duced, their manipulation is itself arithmetically intensive.
However, how to utilize GPU assets is not straightforward.
More generally, how to exploit new heterogeneous architec-
tures is a key subject for the database community [9].

In this context, searching by numerical similarity rather
than via structured relations is more suitable. This could be
to ﬁnd the most similar content to a picture, or to ﬁnd the
vectors that have the highest response to a linear classiﬁer
on all vectors of a collection.

One of the most expensive operations to be performed on
large collections is to compute a k-NN graph. It is a directed
graph where each vector of the database is a node and each
edge connects a node to its k nearest neighbors. This is
our ﬂagship application. Note, state of the art methods like
NN-Descent [15] have a large memory overhead on top of
the dataset itself and cannot readily scale to the billion-sized
databases we consider.

Such applications must deal with the curse of dimension-
ality [46], rendering both exhaustive search or exact index-
ing for non-exhaustive search impractical on billion-scale
databases. This is why there is a large body of work on
approximate search and/or graph construction. To handle
huge datasets that do not ﬁt in RAM, several approaches
employ an internal compressed representation of the vec-
tors using an encoding. This is especially convenient for
memory-limited devices like GPUs. It turns out that accept-
ing a minimal accuracy loss results in orders of magnitude
of compression [21]. The most popular vector compression
methods can be classiﬁed into either binary codes [18, 22],
or quantization methods [25, 37]. Both have the desirable
property that searching neighbors does not require recon-
structing the vectors.

Our paper focuses on methods based on product quanti-
zation (PQ) codes, as these were shown to be more eﬀective
than binary codes [34]. In addition, binary codes incur im-
portant overheads for non-exhaustive search methods [35].
Several improvements were proposed after the original prod-
uct quantization proposal known as IVFADC [25]; most are
diﬃcult to implement eﬃciently on GPU. For instance, the
inverted multi-index [4], useful for high-speed/low-quality
operating points, depends on a complicated “multi-sequence”
algorithm. The optimized product quantization or OPQ [17]
is a linear transformation on the input vectors that improves
the accuracy of the product quantization; it can be applied

1

as a pre-processing. The SIMD-optimized IVFADC imple-
mentation from [2] operates only with sub-optimal parame-
ters (few coarse quantization centroids). Many other meth-
ods, like LOPQ and the Polysemous codes [27, 16] are too
complex to be implemented eﬃciently on GPUs.

There are many implementations of similarity search on
GPUs, but mostly with binary codes [36], small datasets [44],
or exhaustive search [14, 40, 41]. To the best of our knowl-
edge, only the work by Wieschollek et al. [47] appears suit-
able for billion-scale datasets with quantization codes. This
is the prior state of the art on GPUs, which we compare
against in Section 6.4.

This paper makes the following contributions:

• a GPU k-selection algorithm, operating in fast register
memory and ﬂexible enough to be fusable with other
kernels, for which we provide a complexity analysis;

• a near-optimal algorithmic layout for exact and ap-

proximate k-nearest neighbor search on GPU;

• a range of experiments that show that these improve-
ments outperform previous art by a large margin on
mid- to large-scale nearest-neighbor search tasks, in
single or multi-GPU conﬁgurations.

The paper is organized as follows. Section 2 introduces
the context and notation. Section 3 reviews GPU archi-
tecture and discusses problems appearing when using it for
similarity search. Section 4 introduces one of our main con-
tributions, i.e., our k-selection method for GPUs, while Sec-
tion 5 provides details regarding the algorithm computation
layout. Finally, Section 6 provides extensive experiments for
our approach, compares it to the state of the art, and shows
concrete use cases for image collections.

2. PROBLEM STATEMENT

We are concerned with similarity search in vector collec-
tions. Given the query vector x ∈ Rd and the collection2
[yi]i=0:(cid:96) (yi ∈ Rd), we search:

L = k-argmini=0:(cid:96)(cid:107)x − yi(cid:107)2,

(1)

i.e., we search the k nearest neighbors of x in terms of L2
distance. The L2 distance is used most often, as it is op-
timized by design when learning several embeddings (e.g.,
[20]), due to its attractive linear algebra properties.

The lowest distances are collected by k-selection. For an
array [ai]i=0:(cid:96), k-selection ﬁnds the k lowest valued elements
[asi ]i=0:k, asi ≤ asi+1 , along with the indices [si]i=0:k, 0 ≤
si < (cid:96), of those elements from the input array. The ai will be
32-bit ﬂoating point values; the si are 32- or 64-bit integers.
Other comparators are sometimes desired; e.g., for cosine
similarity we search for highest values. The order between
equivalent keys asi = asj is not speciﬁed.

Batching. Typically, searches are performed in batches
of nq query vectors [xj]j=0:nq (xj ∈ Rd) in parallel, which
allows for more ﬂexibility when executing on multiple CPU
threads or on GPU. Batching for k-selection entails selecting
nq × k elements and indices from nq separate arrays, where
each array is of a potentially diﬀerent length (cid:96)i ≥ k.

2To avoid clutter in 0-based indexing, we use the array no-
tation 0 : (cid:96) to denote the range {0, ..., (cid:96) − 1} inclusive.

Exact search. The exact solution computes the full pair-
2]j=0:nq,i=0:(cid:96) ∈ Rnq×(cid:96).
wise distance matrix D = [(cid:107)xj − yi(cid:107)2
In practice, we use the decomposition

(cid:107)xj − yi(cid:107)2

2 = (cid:107)xj(cid:107)2 + (cid:107)yi(cid:107)2 − 2(cid:104)xj, yi(cid:105).

(2)

The two ﬁrst terms can be precomputed in one pass over
the matrices X and Y whose rows are the [xj] and [yi]. The
bottleneck is to evaluate (cid:104)xj, yi(cid:105), equivalent to the matrix
multiplication XY (cid:62). The k-nearest neighbors for each of
the nq queries are k-selected along each row of D.

Compressed-domain search. From now on, we focus on
approximate nearest-neighbor search. We consider, in par-
ticular, the IVFADC indexing structure [25]. The IVFADC
index relies on two levels of quantization, and the database
vectors are encoded. The database vector y is approximated
as:

y ≈ q(y) = q1(y) + q2(y − q1(y))
(3)
where q1 : Rd → C1 ⊂ Rd and q2 : Rd → C2 ⊂ Rd are quan-
tizers; i.e., functions that output an element from a ﬁnite
set. Since the sets are ﬁnite, q(y) is encoded as the index of
q1(y) and that of q2(y − q1(y)). The ﬁrst-level quantizer is a
coarse quantizer and the second level ﬁne quantizer encodes
the residual vector after the ﬁrst level.

The Asymmetric Distance Computation (ADC) search

method returns an approximate result:

LADC = k-argmini=0:(cid:96)(cid:107)x − q(yi)(cid:107)2.

(4)

For IVFADC the search is not exhaustive. Vectors for
which the distance is computed are pre-selected depending
on the ﬁrst-level quantizer q1:

LIVF = τ -argminc∈C1

(cid:107)x − c(cid:107)2.

(5)

The multi-probe parameter τ is the number of coarse-level
centroids we consider. The quantizer operates a nearest-
neighbor search with exact distances, in the set of reproduc-
tion values. Then, the IVFADC search computes

LIVFADC =

k-argmin
i=0:(cid:96) s.t. q1(yi)∈LIVF

(cid:107)x − q(yi)(cid:107)2.

(6)

Hence, IVFADC relies on the same distance estimations as
the two-step quantization of ADC, but computes them only
on a subset of vectors.

The corresponding data structure, the inverted ﬁle, groups
the vectors yi into |C1| inverted lists I1, ..., I|C1| with homo-
geneous q1(yi). Therefore, the most memory-intensive op-
eration is computing LIVFADC, and boils down to linearly
scanning τ inverted lists.

The quantizers. The quantizers q1 and q2 have diﬀerent
properties. q1 needs to have a relatively low number of repro-
duction values so that the number of inverted lists does not
(cid:96), trained via k-means.
explode. We typically use |C1| ≈
For q2, we can aﬀord to spend more memory for a more ex-
tensive representation. The ID of the vector (a 4- or 8-byte
integer) is also stored in the inverted lists, so it makes no
sense to have shorter codes than that; i.e., log2 |C2| > 4 × 8.

√

Product quantizer. We use a product quantizer [25] for q2,
which provides a large number of reproduction values with-
out increasing the processing cost. It interprets the vector y
as b sub-vectors y = [y0...yb−1], where b is an even divisor of

2

the dimension d. Each sub-vector is quantized with its own
quantizer, yielding the tuple (q0(y0), ..., qb−1(yb−1)). The
sub-quantizers typically have 256 reproduction values, to ﬁt
in one byte. The quantization value of the product quantizer
is then q2(y) = q0(y0) + 256 × q1(y1) + ... + 256b−1 × qb−1,
which from a storage point of view is just the concatena-
tion of the bytes produced by each sub-quantizer. Thus, the
product quantizer generates b-byte codes with |C2| = 256b
reproduction values. The k-means dictionaries of the quan-
tizers are small and quantization is computationally cheap.

3. GPU: OVERVIEW AND K-SELECTION
This section reviews salient details of Nvidia’s general-
purpose GPU architecture and programming model [30]. We
then focus on one of the less GPU-compliant parts involved
in similarity search, namely the k-selection, and discuss the
literature and challenges.

3.1 Architecture

GPU lanes and warps. The Nvidia GPU is a general-
purpose computer that executes instruction streams using
a 32-wide vector of CUDA threads (the warp); individual
threads in the warp are referred to as lanes, with a lane
ID from 0 – 31. Despite the “thread” terminology, the best
analogy to modern vectorized multicore CPUs is that each
warp is a separate CPU hardware thread, as the warp shares
an instruction counter. Warp lanes taking diﬀerent execu-
tion paths results in warp divergence, reducing performance.
Each lane has up to 255 32-bit registers in a shared register
ﬁle. The CPU analogy is that there are up to 255 vector
registers of width 32, with warp lanes as SIMD vector lanes.

Collections of warps. A user-conﬁgurable collection of 1
to 32 warps comprises a block or a co-operative thread ar-
ray (CTA). Each block has a high speed shared memory, up
to 48 KiB in size. Individual CUDA threads have a block-
relative ID, called a thread id, which can be used to parti-
tion and assign work. Each block is run on a single core of
the GPU called a streaming multiprocessor (SM). Each SM
has functional units, including ALUs, memory load/store
units, and various special instruction units. A GPU hides
execution latencies by having many operations in ﬂight on
warps across all SMs. Each individual warp lane instruction
throughput is low and latency is high, but the aggregate
arithmetic throughput of all SMs together is 5 – 10× higher
than typical CPUs.

Grids and kernels. Blocks are organized in a grid of blocks
in a kernel. Each block is assigned a grid relative ID. The
kernel is the unit of work (instruction stream with argu-
ments) scheduled by the host CPU for the GPU to execute.
After a block runs through to completion, new blocks can
be scheduled. Blocks from diﬀerent kernels can run concur-
rently. Ordering between kernels is controllable via ordering
primitives such as streams and events.

Resources and occupancy. The number of blocks execut-
ing concurrently depends upon shared memory and register
resources used by each block. Per-CUDA thread register us-
age is determined at compilation time, while shared memory
usage can be chosen at runtime. This usage aﬀects occu-
pancy on the GPU. If a block demands all 48 KiB of shared
memory for its private usage, or 128 registers per thread as

opposed to 32, then only 1 – 2 other blocks can run concur-
rently on the same SM, resulting in low occupancy. Under
high occupancy more blocks will be present across all SMs,
allowing more work to be in ﬂight at once.

Memory types. Diﬀerent blocks and kernels communicate
through global memory, typically 4 – 32 GB in size, with 5 –
10× higher bandwidth than CPU main memory. Shared
memory is analogous to CPU L1 cache in terms of speed.
GPU register ﬁle memory is the highest bandwidth memory.
In order to maintain the high number of instructions in ﬂight
on a GPU, a vast register ﬁle is also required: 14 MB in the
latest Pascal P100, in contrast with a few tens of KB on
CPU. A ratio of 250 : 6.25 : 1 for register to shared to global
memory aggregate cross-sectional bandwidth is typical on
GPU, yielding 10 – 100s of TB/s for the register ﬁle [10].

3.2 GPU register ﬁle usage

Structured register data. Shared and register memory
usage involves eﬃciency tradeoﬀs; they lower occupancy but
can increase overall performance by retaining a larger work-
ing set in a faster memory. Making heavy use of register-
resident data at the expense of occupancy or instead of
shared memory is often proﬁtable [43].

As the GPU register ﬁle is very large, storing structured
data (not just temporary operands) is useful. A single lane
can use its (scalar) registers to solve a local task, but with
limited parallelism and storage.
Instead, lanes in a GPU
warp can instead exchange register data using the warp shuf-
ﬂe instruction, enabling warp-wide parallelism and storage.

Lane-stride register array. A common pattern to achieve
this is a lane-stride register array. That is, given elements
[ai]i=0:(cid:96), each successive value is held in a register by neigh-
boring lanes. The array is stored in (cid:96)/32 registers per lane,
with (cid:96) a multiple of 32. Lane j stores {aj, a32+j, ..., a(cid:96)−32+j},
while register r holds {a32r, a32r+1, ..., a32r+31}.

For manipulating the [ai], the register in which ai is stored
(i.e., (cid:98)i/32(cid:99)) and (cid:96) must be known at assembly time, while
the lane (i.e., i mod 32) can be runtime knowledge. A wide
variety of access patterns (shift, any-to-any) are provided;
we use the butterﬂy permutation [29] extensively.

3.3 k-selection on CPU versus GPU

k-selection algorithms, often for arbitrarily large (cid:96) and
k, can be translated to a GPU, including radix selection
and bucket selection [1], probabilistic selection [33], quick-
select [14], and truncated sorts [40]. Their performance is
dominated by multiple passes over the input in global mem-
ory. Sometimes for similarity search, the input distances are
computed on-the-ﬂy or stored only in small blocks, not in
their entirety. The full, explicit array might be too large to
ﬁt into any memory, and its size could be unknown at the
start of the processing, rendering algorithms that require
multiple passes impractical. They suﬀer from other issues
as well. Quickselect requires partitioning on a storage of
size O((cid:96)), a data-dependent memory movement. This can
result in excessive memory transactions, or requiring parallel
preﬁx sums to determine write oﬀsets, with synchronization
overhead. Radix selection has no partitioning but multiple
passes are still required.

Heap parallelism. In similarity search applications, one
is usually interested only in a small number of results, k <

3

1000 or so. In this regime, selection via max-heap is a typi-
cal choice on the CPU, but heaps do not expose much data
parallelism (due to serial tree update) and cannot saturate
SIMD execution units. The ad-heap [31] takes better advan-
tage of parallelism available in heterogeneous systems, but
still attempts to partition serial and parallel work between
appropriate execution units. Despite the serial nature of
heap update, for small k the CPU can maintain all of its
state in the L1 cache with little eﬀort, and L1 cache latency
and bandwidth remains a limiting factor. Other similarity
search components, like PQ code manipulation, tend to have
greater impact on CPU performance [2].

GPU heaps. Heaps can be similarly implemented on a
GPU [7]. However, a straightforward GPU heap implemen-
tation suﬀers from high warp divergence and irregular, data-
dependent memory movement, since the path taken for each
inserted element depends upon other values in the heap.

GPU parallel priority queues [24] improve over the serial
heap update by allowing multiple concurrent updates, but
they require a potential number of small sorts for each insert
and data-dependent memory movement. Moreover, it uses
multiple synchronization barriers through kernel launches in
diﬀerent streams, plus the additional latency of successive
kernel launches and coordination with the CPU host.

Other more novel GPU algorithms are available for small
k, namely the selection algorithm in the fgknn library [41].
This is a complex algorithm that may suﬀer from too many
synchronization points, greater kernel launch overhead, us-
age of slower memories, excessive use of hierarchy, partition-
ing and buﬀering. However, we take inspiration from this
particular algorithm through the use of parallel merges as
seen in their merge queue structure.

4. FAST K-SELECTION ON THE GPU

For any CPU or GPU algorithm, either memory or arith-
metic throughput should be the limiting factor as per the
rooﬂine performance model [48]. For input from global mem-
ory, k-selection cannot run faster than the time required to
scan the input once at peak memory bandwidth. We aim to
get as close to this limit as possible. Thus, we wish to per-
form a single pass over the input data (from global memory
or produced on-the-ﬂy, perhaps fused with a kernel that is
generating the data).

We want to keep intermediate state in the fastest memory:
the register ﬁle. The major disadvantage of register memory
is that the indexing into the register ﬁle must be known at
assembly time, which is a strong constraint on the algorithm.

4.1

In-register sorting

We use an in-register sorting primitive as a building block.
Sorting networks are commonly used on SIMD architec-
tures [13], as they exploit vector parallelism. They are eas-
ily implemented on the GPU, and we build sorting networks
with lane-stride register arrays.

We use a variant of Batcher’s bitonic sorting network [8],
which is a set of parallel merges on an array of size 2k. Each
merge takes s arrays of length t (s and t a power of 2) to s/2
arrays of length 2t, using log2(t) parallel steps. A bitonic
sort applies this merge recursively: to sort an array of length
(cid:96), merge (cid:96) arrays of length 1 to (cid:96)/2 arrays of length 2, to (cid:96)/4
arrays of length 4, successively to 1 sorted array of length (cid:96),
leading to 1

2 (log2((cid:96))2 + log2((cid:96))) parallel merge steps.

Algorithm 1 Odd-size merging network

function merge-odd([Li]i=0:(cid:96)L , [Ri]i=0:(cid:96)R )
parallel for i ← 0 : min((cid:96)L, (cid:96)R) do

(cid:46) inverted 1st stage; inputs are already sorted

compare-swap(L(cid:96)L−i−1, Ri)

end for
parallel do

(cid:46) If (cid:96)L = (cid:96)R and a power-of-2, these are equivalent
merge-odd-continue([Li]i=0:(cid:96)L , left)
merge-odd-continue([Ri]i=0:(cid:96)R , right)

end do
end function
function merge-odd-continue([xi]i=0:(cid:96), p)

if (cid:96) > 1 then

h ← 2(cid:100)log2 (cid:96)(cid:101)−1
parallel for i ← 0 : (cid:96) − h do

(cid:46) largest power-of-2 < (cid:96)

(cid:46) Implemented with warp shuﬄe butterﬂy

compare-swap(xi, xi+h)

end for
parallel do

if p = left then

(cid:46) left side recursion

merge-odd-continue([xi]i=0:(cid:96)−h, left)
merge-odd-continue([xi]i=(cid:96)−h:(cid:96), right)

else

(cid:46) right side recursion

merge-odd-continue([xi]i=0:h, left)
merge-odd-continue([xi]i=h:(cid:96), right)

end if

end do

end if
end function

Odd-size merging and sorting networks. If some input
data is already sorted, we can modify the network to avoid
merging steps. We may also not have a full power-of-2 set of
data, in which case we can eﬃciently shortcut to deal with
the smaller size.

Algorithm 1 is an odd-sized merging network that merges
already sorted left and right arrays, each of arbitrary length.
While the bitonic network merges bitonic sequences, we start
with monotonic sequences: sequences sorted monotonically.
A bitonic merge is made monotonic by reversing the ﬁrst
comparator stage.

The odd size algorithm is derived by considering arrays to
be padded to the next highest power-of-2 size with dummy

Figure 1: Odd-size network merging arrays of sizes
5 and 3. Bullets indicate parallel compare/swap.
Dashed lines are elided elements or comparisons.

4

Figure 2: Overview of WarpSelect. The input val-
ues stream in on the left, and the warp queue on the
right holds the output result.

elements that are never swapped (the merge is monotonic)
and are already properly positioned; any comparisons with
dummy elements are elided. A left array is considered to
be padded with dummy elements at the start; a right ar-
ray has them at the end. A merge of two sorted arrays
of length (cid:96)L and (cid:96)R to a sorted array of (cid:96)L + (cid:96)R requires
(cid:100)log2(max((cid:96)L, (cid:96)R))(cid:101) + 1 parallel steps. Figure 1 shows Algo-
rithm 1’s merging network for arrays of size 5 and 3, with 4
parallel steps.

The compare-swap is implemented using warp shuﬄes on
a lane-stride register array. Swaps with a stride a multiple
of 32 occur directly within a lane as the lane holds both
elements locally. Swaps of stride ≤ 16 or a non-multiple of
32 occur with warp shuﬄes. In practice, used array lengths
are multiples of 32 as they are held in lane-stride arrays.

Algorithm 2 Odd-size sorting network

function sort-odd([xi]i=0:(cid:96))

if (cid:96) > 1 then

parallel do

sort-odd([xi]i=0:(cid:98)(cid:96)/2(cid:99))
sort-odd([xi]i=(cid:98)(cid:96)/2(cid:99):(cid:96))

end do
merge-odd([xi]i=0:(cid:98)(cid:96)/2(cid:99), [xi]i=(cid:98)(cid:96)/2(cid:99):(cid:96))

end if
end function

Algorithm 2 extends the merge to a full sort. Assuming no
2 ((cid:100)log2((cid:96))(cid:101)2 +(cid:100)log2((cid:96))(cid:101))

structure present in the input data, 1
parallel steps are required for sorting data of length (cid:96).

4.2 WarpSelect

Our k-selection implementation, WarpSelect, maintains
state entirely in registers, requires only a single pass over
data and avoids cross-warp synchronization. It uses merge-
odd and sort-odd as primitives. Since the register ﬁle pro-
vides much more storage than shared memory, it supports
k ≤ 1024. Each warp is dedicated to k-selection to a single
one of the n arrays [ai]. If n is large enough, a single warp
per each [ai] will result in full GPU occupancy. Large (cid:96) per
warp is handled by recursive decomposition, if (cid:96) is known in
advance.

Overview. Our approach (Algorithm 3 and Figure 2) oper-
ates on values, with associated indices carried along (omit-
ted from the description for simplicity). It selects the k least
values that come from global memory, or from intermediate
value registers if fused into another kernel providing the val-
ues. Let [ai]i=0:(cid:96) be the sequence provided for selection.

The elements (on the left of Figure 2) are processed in
groups of 32, the warp size. Lane j is responsible for pro-
cessing {aj, a32+j, ...}; thus, if the elements come from global
memory, the reads are contiguous and coalesced into a min-
imal number of memory transactions.

i ≥ T j

Data structures. Each lane j maintains a small queue
of t elements in registers, called the thread queues [T j
i ]i=0:t,
ordered from largest to smallest (T j
i+1). The choice of
t is made relative to k, see Section 4.3. The thread queue is
a ﬁrst-level ﬁlter for new values coming in. If a new a32i+j
is greater than the largest key currently in the queue, T j
0 , it
is guaranteed that it won’t be in the k smallest ﬁnal results.
The warp shares a lane-stride register array of k smallest
seen elements, [Wi]i=0:k, called the warp queue. It is ordered
from smallest to largest (Wi ≤ Wi+1); if the requested k is
not a multiple of 32, we round it up. This is a second level
data structure that will be used to maintain all of the k
smallest warp-wide seen values. The thread and warp queues
are initialized to maximum sentinel values, e.g., +∞.

Update. The three invariants maintained are:

• all per-lane T j

0 are not in the min-k

• all per-lane T j

0 are greater than all warp queue keys

Wi

• all ai seen so far in the min-k are contained in either
i ]i=0:t,j=0:32), or in the

some lane’s thread queue ([T j
warp queue.

Lane j receives a new a32i+j and attempts to insert it into
0 , then the new pair is by

its thread queue. If a32i+j > T j
deﬁnition not in the k minimum, and can be rejected.

Otherwise, it is inserted into its proper sorted position
in the thread queue, thus ejecting the old T j
0 . All lanes
complete doing this with their new received pair and their
thread queue, but it is now possible that the second invariant
have been violated. Using the warp ballot instruction, we
determine if any lane has violated the second invariant. If
not, we are free to continue processing new elements.

Restoring the invariants.
If any lane has its invariant
violated, then the warp uses odd-merge to merge and sort
the thread and warp queues together. The new warp queue

Algorithm 3 WarpSelect pseudocode for lane j

function WarpSelect(a)

if a < T j

0 then

insert a into our [T j

i ]i=0:t

end if
if warp-ballot(T j

0 < Wk−1) then

(cid:46) Reinterpret thread queues as lane-stride array

[αi]i=0:32t ← cast([T j

i ]i=0:t,j=0:32)

(cid:46) concatenate and sort thread queues

sort-odd([αi]i=0:32t)
merge-odd([Wi]i=0:k, [αi]i=0:32t)

(cid:46) Reinterpret lane-stride array as thread queues
i ]i=0:t,j=0:32 ← cast([αi]i=0:32t)

[T j
reverse-array([Ti]i=0:t)

(cid:46) Back in thread queue order, invariant restored

end if
end function

5

will be the min-k elements across the merged, sorted queues,
and the new thread queues will be the remainder, from min-
(k + 1) to min-(k + 32t + 1). This restores the invariants and
we are free to continue processing subsequent elements.

Since the thread and warp queues are already sorted, we
merge the sorted warp queue of length k with 32 sorted
arrays of length t. Supporting odd-sized merges is important
because Batcher’s formulation would require that 32t = k
and is a power-of-2; thus if k = 1024, t must be 32. We
found that the optimal t is way smaller (see below).

Using odd-merge to merge the 32 already sorted thread
queues would require a struct-of-arrays to array-of-structs
transposition in registers across the warp, since the t succes-
sive sorted values are held in diﬀerent registers in the same
lane rather than a lane-stride array. This is possible [12],
but would use a comparable number of warp shuﬄes, so we
just reinterpret the thread queue registers as an (unsorted)
lane-stride array and sort from scratch. Signiﬁcant speedup
is realizable by using odd-merge for the merge of the ag-
gregate sorted thread queues with the warp queue.

Handling the remainder. If there are remainder elements
because (cid:96) is not a multiple of 32, those are inserted into the
thread queues for the lanes that have them, after which we
proceed to the output stage.

Output. A ﬁnal sort and merge is made of the thread and
warp queues, after which the warp queue holds all min-k
values.

4.3 Complexity and parameter selection

For each incoming group of 32 elements, WarpSelect
can perform 1, 2 or 3 constant-time operations, all happen-
ing in warp-wide parallel time:

1. read 32 elements, compare to all thread queue heads

T j
0 , cost C1, happens N1 times;

2. if ∃j ∈ {0, ..., 31}, a32n+j < T j

0 , perform insertion sort
on those speciﬁc thread queues, cost C2 = O(t), hap-
pens N2 times;

3. if ∃j, T j

0 < Wk−1, sort and merge queues, cost C3 =
O(t log(32t)2 + k log(max(k, 32t))), happens N3 times.

Thus, the total cost is N1C1 + N2C2 + N3C3. N1 = (cid:96)/32,
and on random data drawn independently, N2 = O(k log((cid:96)))
and N3 = O(k log((cid:96))/t), see the Appendix for a full deriva-
tion. Hence, the trade-oﬀ is to balance a cost in N2C2 and
one in N3C3. The practical choice for t given k and (cid:96) was
made by experiment on a variety of k-NN data. For k ≤ 32,
we use t = 2, k ≤ 128 uses t = 3, k ≤ 256 uses t = 4, and
k ≤ 1024 uses t = 8, all irrespective of (cid:96).

5. COMPUTATION LAYOUT

This section explains how IVFADC, one of the indexing
methods originally built upon product quantization [25], is
implemented eﬃciently. Details on distance computations
and articulation with k-selection are the key to understand-
ing why this method can outperform more recent GPU-
compliant approximate nearest neighbor strategies [47].

5.1 Exact search

We brieﬂy come back to the exhaustive search method,
often referred to as exact brute-force. It is interesting on its

own for exact nearest neighbor search in small datasets. It
is also a component of many indexes in the literature. In
our case, we use it for the IVFADC coarse quantizer q1.

As stated in Section 2, the distance computation boils
down to a matrix multiplication. We use optimized GEMM
routines in the cuBLAS library to calculate the −2(cid:104)xj, yi(cid:105)
term for L2 distance, resulting in a partial distance matrix
D(cid:48). To complete the distance calculation, we use a fused
k-selection kernel that adds the (cid:107)yi(cid:107)2 term to each entry of
the distance matrix and immediately submits the value to
k-selection in registers. The (cid:107)xj(cid:107)2 term need not be taken
into account before k-selection. Kernel fusion thus allows
for only 2 passes (GEMM write, k-select read) over D(cid:48), com-
pared to other implementations that may require 3 or more.
Row-wise k-selection is likely not fusable with a well-tuned
GEMM kernel, or would result in lower overall eﬃciency.

As D(cid:48) does not ﬁt in GPU memory for realistic problem
sizes, the problem is tiled over the batch of queries, with
tq ≤ nq queries being run in a single tile. Each of the (cid:100)nq/tq(cid:101)
tiles are independent problems, but we run two in parallel
on diﬀerent streams to better occupy the GPU, so the eﬀec-
tive memory requirement of D is O(2(cid:96)tq). The computation
can similarly be tiled over (cid:96). For very large input coming
from the CPU, we support buﬀering with pinned memory
to overlap CPU to GPU copy with GPU compute.

5.2

IVFADC indexing

PQ lookup tables. At its core, the IVFADC requires com-
puting the distance from a vector to a set of product quanti-
zation reproduction values. By developing Equation (6) for
a database vector y, we obtain:

(cid:107)x − q(y)(cid:107)2

2 = (cid:107)x − q1(y) − q2(y − q1(y))(cid:107)2
2.

(7)

If we decompose the residual vectors left after q1 as:

y − q1(y) = [ (cid:101)y1 · · · (cid:101)yb] and

x − q1(y) = [(cid:102)x1 · · · (cid:101)xb]

(8)

(9)

then the distance is rewritten as:

(cid:107)x − q(y)(cid:107)2

2 = (cid:107)(cid:102)x1 − q1( (cid:101)y1)(cid:107)2

2 + ... + (cid:107) (cid:101)xb − qb( (cid:101)yb)(cid:107)2
2.

(10)

Each quantizer q1, ..., qb has 256 reproduction values, so
when x and q1(y) are known all distances can be precom-
puted and stored in tables T1, ..., Tb each of size 256 [25].
Computing the sum (10) consists of b look-ups and addi-
tions. Comparing the cost to compute n distances:

• Explicit computation: n × d mutiply-adds;

• With lookup tables: 256 × d multiply-adds and n × b

lookup-adds.

This is the key to the eﬃciency of the product quantizer.
In our GPU implementation, b is any multiple of 4 up to
64. The codes are stored as sequential groups of b bytes per
vector within lists.

IVFADC lookup tables. When scanning over the ele-
ments of the inverted list IL (where by deﬁnition q1(y) is
constant), the look-up table method can be applied, as the
query x and q1(y) are known.

6

Moreover, the computation of the tables T1 . . . Tb is fur-
2 in Equation

ther optimized [5]. The expression of (cid:107)x−q(y)(cid:107)2
(7) can be decomposed as:

(cid:107)q2(...)(cid:107)2
(cid:124)

2 + 2(cid:104)q1(y), q2(...)(cid:105)
(cid:125)

(cid:123)(cid:122)
term 1

+ (cid:107)x − q1(y)(cid:107)2
2
(cid:123)(cid:122)
(cid:125)
term 2

(cid:124)

−2 (cid:104)x, q2(...)(cid:105)
(cid:125)

(cid:123)(cid:122)
term 3

(cid:124)

.

(11)
The objective is to minimize inner loop computations.
The computations we can do in advance and store in lookup
tables are as follows:

• Term 1 is independent of the query. It can be precom-
puted from the quantizers, and stored in a table T of
size |C1| × 256 × b;

• Term 2 is the distance to q1’s reproduction value. It is

thus a by-product of the ﬁrst-level quantizer q1;

• Term 3 can be computed independently of the inverted
list. Its computation costs d × 256 multiply-adds.

This decomposition is used to produce the lookup tables
T1 . . . Tb used during the scan of the inverted list. For a
single query, computing the τ × b tables from scratch costs
τ × d × 256 multiply-adds, while this decomposition costs
256×d multiply-adds and τ ×b×256 additions. On the GPU,
the memory usage of T can be prohibitive, so we enable the
decomposition only when memory is a not a concern.

5.3 GPU implementation

Algorithm 4 summarizes the process as one would im-
plement it on a CPU. The inverted lists are stored as two
separate arrays, for PQ codes and associated IDs. IDs are
resolved only if k-selection determines k-nearest member-
ship. This lookup yields a few sparse memory reads in a
large array, thus the IDs can optionally be stored on CPU
for tiny performance cost.

List scanning. A kernel is responsible for scanning the τ
closest inverted lists for each query, and calculating the per-
vector pair distances using the lookup tables Ti. The Ti are
stored in shared memory: up to nq ×τ ×maxi |Ii|×b lookups
are required for a query set (trillions of accesses in practice),
and are random access. This limits b to at most 48 (32-
bit ﬂoating point) or 96 (16-bit ﬂoating point) with current
architectures. In case we do not use the decomposition of
Equation (11), the Ti are calculated by a separate kernel
before scanning.

Multi-pass kernels. Each nq × τ pairs of query against
inverted list can be processed independently. At one ex-
treme, a block is dedicated to each of these, resulting in up
to nq × τ × maxi |Ii| partial results being written back to
global memory, which is then k-selected to nq × k ﬁnal re-
sults. This yields high parallelism but can exceed available
GPU global memory; as with exact search, we choose a tile
size tq ≤ nq to reduce memory consumption, bounding its
complexity by O(2tqτ maxi |Ii|) with multi-streaming.

A single warp could be dedicated to k-selection of each
tq set of lists, which could result in low parallelism. We
introduce a two-pass k-selection, reducing tq × τ × maxi |Ii|
to tq × f × k partial results for some subdivision factor f .
This is reduced again via k-selection to the ﬁnal tq×k results.

Fused kernel. As with exact search, we experimented with
a kernel that dedicates a single block to scanning all τ lists

7

for a single query, with k-selection fused with distance com-
putation. This is possible as WarpSelect does not ﬁght for
the shared memory resource which is severely limited. This
reduces global memory write-back, since almost all interme-
diate results can be eliminated. However, unlike k-selection
overhead for exact computation, a signiﬁcant portion of the
runtime is the gather from the Ti in shared memory and lin-
ear scanning of the Ii from global memory; the write-back is
not a dominant contributor. Timing for the fused kernel is
improved by at most 15%, and for some problem sizes would
be subject to lower parallelism and worse performance with-
out subsequent decomposition. Therefore, and for reasons
of implementation simplicity, we do not use this layout.

Algorithm 4 IVFPQ batch search routine

function ivfpq-search([x1, ..., xnq ], I1, ..., I|C1|)

for i ← 0 : nq do (cid:46) batch quantization of Section 5.1

IVF ← τ -argminc∈C1

Li
end for
for i ← 0 : nq do

(cid:107)x − c(cid:107)2

L ← []
Compute term 3 (see Section 5.2)
for L in Li

IVF do

Compute distance tables T1, ..., Tb
for j in IL do

(cid:46) distance table

(cid:46) τ loops

(cid:46) distance estimation, Equation (10)

d ← (cid:107)xi − q(yj)(cid:107)2
2
Append (d, L, j) to L

end for

end for
Ri ← k-select smallest distances d from L

end for
return R

end function

5.4 Multi-GPU parallelism

Modern servers can support several GPUs. We employ

this capability for both compute power and memory.

Replication. If an index instance ﬁts in the memory of a
single GPU, it can be replicated across R diﬀerent GPUs. To
query nq vectors, each replica handles a fraction nq/R of the
queries, joining the results back together on a single GPU
or in CPU memory. Replication has near linear speedup,
except for a potential loss in eﬃciency for small nq.

Sharding. If an index instance does not ﬁt in the memory
of a single GPU, an index can be sharded across S diﬀer-
ent GPUs. For adding (cid:96) vectors, each shard receives (cid:96)/S of
the vectors, and for query, each shard handles the full query
set nq, joining the partial results (an additional round of k-
selection is still required) on a single GPU or in CPU mem-
ory. For a given index size (cid:96), sharding will yield a speedup
(sharding has a query of nq against (cid:96)/S versus replication
with a query of nq/R against (cid:96)), but is usually less than
pure replication due to ﬁxed overhead and cost of subse-
quent k-selection.

Replication and sharding can be used together (S shards,
each with R replicas for S × R GPUs in total). Sharding or
replication are both fairly trivial, and the same principle can
be used to distribute an index across multiple machines.

method
BIDMach [11]
Ours
Ours

# GPUs
1
1
4

# centroids
4096
256
735 s
320 s
316 s
140 s
100 s
84 s

Table 1: MNIST8m k-means performance

6.2 k-means clustering

The exact search method with k = 1 can be used by a k-
means clustering method in the assignment stage, to assign
nq training vectors to |C1| centroids. Despite the fact that
it does not use the IVFADC and k = 1 selection is trivial (a
parallel reduction is used for the k = 1 case, not WarpSe-
lect), k-means is a good benchmark for the clustering used
to train the quantizer q1.

We apply the algorithm on MNIST8m images. The 8.1M
images are graylevel digits in 28x28 pixels, linearized to vec-
tors of 784-d. We compare this k-means implementation to
the GPU k-means of BIDMach [11], which was shown to be
more eﬃcient than several distributed k-means implemen-
tations that require dozens of machines3. Both algorithms
were run for 20 iterations. Table 1 shows that our imple-
mentation is more than 2× faster, although both are built
upon cuBLAS. Our implementation receives some beneﬁt
from the k-selection fusion into L2 distance computation.
For multi-GPU execution via replicas, the speedup is close
to linear for large enough problems (3.16× for 4 GPUs with
4096 centroids). Note that this benchmark is somewhat un-
realistic, as one would typically sub-sample the dataset ran-
domly when so few centroids are requested.

Large scale. We can also compare to [3], an approximate
CPU method that clusters 108 128-d vectors to 85k cen-
troids. Their clustering method runs in 46 minutes, but re-
quires 56 minutes (at least) of pre-processing to encode the
vectors. Our method performs exact k-means on 4 GPUs in
52 minutes without any pre-processing.

6.3 Exact nearest neighbor search

We consider a classical dataset used to evaluate nearest
neighbor search: Sift1M [25].
Its characteristic sizes are
(cid:96) = 106, d = 128, nq = 104. Computing the partial distance
matrix D(cid:48) costs nq × (cid:96) × d = 1.28 Tﬂop, which runs in less
than one second on current GPUs. Figure 4 shows the cost
of the distance computations against the cost of our tiling
of the GEMM for the −2 (cid:104)xj, yi(cid:105) term of Equation 2 and
the peak possible k-selection performance on the distance
matrix of size nq ×(cid:96), which additionally accounts for reading
the tiled result matrix D(cid:48) at peak memory bandwidth.

In addition to our method from Section 5, we include
times from the two GPU libraries evaluated for k-selection
performance in Section 6.1. We make several observations:

• for k-selection, the naive algorithm that sorts the full
result array for each query using thrust::sort_by_key
is more than 10× slower than the comparison methods;

• L2 distance and k-selection cost is dominant for all but
our method, which has 85 % of the peak possible
performance, assuming GEMM usage and our tiling

3BIDMach numbers from https://github.com/BIDData/
BIDMach/wiki/Benchmarks#KMeans

8

Figure 3: Runtimes for diﬀerent k-selection meth-
ods, as a function of array length (cid:96). Simultaneous
arrays processed are nq = 10000. k = 100 for full lines,
k = 1000 for dashed lines.

6. EXPERIMENTS & APPLICATIONS

This section compares our GPU k-selection and nearest-
neighbor approach to existing libraries. Unless stated other-
wise, experiments are carried out on a 2×2.8GHz Intel Xeon
E5-2680v2 with 4 Maxwell Titan X GPUs on CUDA 8.0.

6.1 k-selection performance

We compare against two other GPU small k-selection im-
plementations: the row-based Merge Queue with Buﬀered
Search and Hierarchical Partition extracted from the fgknn
library of Tang et al. [41] and Truncated Bitonic Sort (TBiS )
from Sismanis et al. [40]. Both were extracted from their re-
spective exact search libraries.

We evaluate k-selection for k = 100 and 1000 of each row
from a row-major matrix nq × (cid:96) of random 32-bit ﬂoating
point values on a single Titan X. The batch size nq is ﬁxed
at 10000, and the array lengths (cid:96) vary from 1000 to 128000.
Inputs and outputs to the problem remain resident in GPU
memory, with the output being of size nq × k, with corre-
sponding indices. Thus, the input problem sizes range from
40 MB ((cid:96) = 1000) to 5.12 GB ((cid:96) = 128k). TBiS requires large
auxiliary storage, and is limited to (cid:96) ≤ 48000 in our tests.

Figure 3 shows our relative performance against TBiS and
fgknn. It also includes the peak possible performance given
by the memory bandwidth limit of the Titan X. The rela-
tive performance of WarpSelect over fgknn increases for
larger k; even TBiS starts to outperform fgknn for larger (cid:96)
at k = 1000. We look especially at the largest (cid:96) = 128000.
WarpSelect is 1.62× faster at k = 100, 2.01× at k = 1000.
Performance against peak possible drops oﬀ for all imple-
mentations at larger k. WarpSelect operates at 55% of
peak at k = 100 but only 16% of peak at k = 1000. This
is due to additional overhead assocated with bigger thread
queues and merge/sort networks for large k.

Diﬀerences from fgknn. WarpSelect is inﬂuenced by
fgknn, but has several improvements: all state is maintained
in registers (no shared memory), no inter-warp synchroniza-
tion or buﬀering is used, no “hierarchical partition”, the k-
selection can be fused into other kernels, and it uses odd-size
networks for eﬃcient merging and sorting.

Figure 4: Exact search k-NN time for the SIFT1M
dataset with varying k on 1 Titan X GPU.

of the partial distance matrix D(cid:48) on top of GEMM is
close to optimal. The cuBLAS GEMM itself has low
eﬃciency for small reduction sizes (d = 128);

• Our fused L2/k-selection kernel is important. Our
same exact algorithm without fusion (requiring an ad-
ditional pass through D(cid:48)) is at least 25% slower.

Eﬃcient k-selection is even more important in situations
where approximate methods are used to compute distances,
because the relative cost of k-selection with respect to dis-
tance computation increases.

6.4 Billion-scale approximate search

There are few studies on GPU-based approximate nearest-
neighbor search on large datasets ((cid:96) (cid:29) 106). We report a
few comparison points here on index search, using standard
datasets and evaluation protocol in this ﬁeld.

SIFT1M. For the sake of completeness, we ﬁrst compare
our GPU search speed on Sift1M with the implementation
of Wieschollek et al. [47]. They obtain a nearest neighbor re-
call at 1 (fraction of queries where the true nearest neighbor
is in the top 1 result) of R@1 = 0.51, and R@100 = 0.86 in
0.02 ms per query on a Titan X. For the same time budget,
our implementation obtains R@1 = 0.80 and R@100 = 0.95.

SIFT1B. We compare again with Wieschollek et al., on the
Sift1B dataset [26] of 1 billion SIFT image features at nq =
104. We compare the search performance in terms of same
memory usage for similar accuracy (more accurate methods
may involve greater search time or memory usage). On a
single GPU, with m = 8 bytes per vector, R@10 = 0.376 in
17.7 µs per query vector, versus their reported R@10 = 0.35
in 150 µs per query vector. Thus, our implementation is
more accurate at a speed 8.5× faster.

DEEP1B. We also experimented on the Deep1B dataset [6]
of (cid:96)=1 billion CNN representations for images at nq = 104.
The paper that introduces the dataset reports CPU results
(1 thread): R@1 = 0.45 in 20 ms search time per vector. We
use a PQ encoding of m = 20, with d = 80 via OPQ [17],
and |C1| = 218, which uses a comparable dataset storage as
the original paper (20 GB). This requires multiple GPUs as
it is too large for a single GPU’s global memory, so we con-
sider 4 GPUs with S = 2, R = 2. We obtain a R@1 = 0.4517
in 0.0133 ms per vector. While the hardware platforms are

Figure 5: Speed/accuracy trade-oﬀ of brute-force
10-NN graph construction for the YFCC100M and
DEEP1B datasets.

diﬀerent, it shows that making searches on GPUs is a game-
changer in terms of speed achievable on a single machine.

6.5 The k-NN graph

An example usage of our similarity search method is to
construct a k-nearest neighbor graph of a dataset via brute
force (all vectors queried against the entire index).

Experimental setup. We evaluate the trade-oﬀ between
speed, precision and memory on two datasets: 95 million
images from the Yfcc100M dataset [42] and Deep1B. For
Yfcc100M, we compute CNN descriptors as the one-before-
last layer of a ResNet [23], reduced to d = 128 with PCA.

The evaluation measures the trade-oﬀ between:

• Speed: How much time it takes to build the IVFADC
index from scratch and construct the whole k-NN graph
(k = 10) by searching nearest neighbors for all vectors
in the dataset. Thus, this is an end-to-end test that
includes indexing as well as search time;

• Quality: We sample 10,000 images for which we com-
pute the exact nearest neighbors. Our accuracy mea-
sure is the fraction of 10 found nearest neighbors that
are within the ground-truth 10 nearest neighbors.

For Yfcc100M, we use a coarse quantizer (216 centroids),
and consider m = 16, 32 and 64 byte PQ encodings for each
vector. For Deep1B, we pre-process the vectors to d = 120
via OPQ, use |C1| = 218 and consider m = 20, 40. For a
given encoding, we vary τ from 1 to 256, to obtain trade-
oﬀs between eﬃciency and quality, as seen in Figure 5.

9

Figure 6: Path in the k-NN graph of 95 million images from YFCC100M. The ﬁrst and the last image are
given; the algorithm computes the smoothest path between them.

Discussion. For Yfcc100M we used S = 1, R = 4. An
accuracy of more than 0.8 is obtained in 35 minutes. For
Deep1B, a lower-quality graph can be built in 6 hours,
with higher quality in about half a day. We also experi-
mented with more GPUs by doubling the replica set, us-
ing 8 Maxwell M40s (the M40 is roughly equivalent in per-
formance to the Titan X). Performance is improved sub-
linearly (∼ 1.6× for m = 20, ∼ 1.7× for m = 40).

For comparison, the largest k-NN graph construction we
are aware of used a dataset comprising 36.5 million 384-
d vectors, which took a cluster of 128 CPU servers 108.7
hours of compute [45], using NN-Descent [15]. Note that
NN-Descent could also build or reﬁne the k-NN graph for
the datasets we consider, but it has a large memory over-
head over the graph storage, which is already 80 GB for
Deep1B. Moreover it requires random access across all vec-
tors (384 GB for Deep1B).

The largest GPU k-NN graph construction we found is a
brute-force construction using exact search with GEMM, of
a dataset of 20 million 15,000-d vectors, which took a cluster
of 32 Tesla C2050 GPUs 10 days [14]. Assuming computa-
tion scales with GEMM cost for the distance matrix, this
approach for Deep1B would take an impractical 200 days
of computation time on their cluster.

6.6 Using the k-NN graph

When a k-NN graph has been constructed for an image
dataset, we can ﬁnd paths in the graph between any two
images, provided there is a single connected component (this
is the case). For example, we can search the shortest path
between two images of ﬂowers, by propagating neighbors
from a starting image to a destination image. Denoting by
S and D the source and destination images, and dij the
distance between nodes, we search the path P = {p1, ..., pn}
with p1 = S and pn = D such that

min
P

max
i=1..n

dpipi+1 ,

(12)

i.e., we want to favor smooth transitions. An example re-
sult is shown in Figure 6 from Yfcc100M4. It was ob-
tained after 20 seconds of propagation in a k-NN graph with
k = 15 neighbors. Since there are many ﬂower images in the
dataset, the transitions are smooth.

4The mapping from vectors to images is not available for
Deep1B

7. CONCLUSION

The arithmetic throughput and memory bandwidth of
GPUs are well into the teraﬂops and hundreds of gigabytes
per second. However,
implementing algorithms that ap-
proach these performance levels is complex and counter-
intuitive. In this paper, we presented the algorithmic struc-
ture of similarity search methods that achieves near-optimal
performance on GPUs.

This work enables applications that needed complex ap-
proximate algorithms before. For example, the approaches
presented here make it possible to do exact k-means cluster-
ing or to compute the k-NN graph with simple brute-force
approaches in less time than a CPU (or a cluster of them)
would take to do this approximately.

GPU hardware is now very common on scientiﬁc work-
stations, due to their popularity for machine learning algo-
rithms. We believe that our work further demonstrates their
interest for database applications. Along with this work, we
are publishing a carefully engineered implementation of this
paper’s algorithms, so that these GPUs can now also be used
for eﬃcient similarity search.

8. REFERENCES
[1] T. Alabi, J. D. Blanchard, B. Gordon, and R. Steinbach.
Fast k-selection algorithms for graphics processing units.
ACM Journal of Experimental Algorithmics,
17:4.2:4.1–4.2:4.29, October 2012.

[2] F. Andr´e, A.-M. Kermarrec, and N. L. Scouarnec. Cache

locality is not enough: High-performance nearest neighbor
search with product quantization fast scan. In Proc.
International Conference on Very Large DataBases, pages
288–299, 2015.

[3] Y. Avrithis, Y. Kalantidis, E. Anagnostopoulos, and I. Z.
Emiris. Web-scale image clustering revisited. In Proc.
International Conference on Computer Vision, pages
1502–1510, 2015.

[4] A. Babenko and V. Lempitsky. The inverted multi-index.
In Proc. IEEE Conference on Computer Vision and
Pattern Recognition, pages 3069–3076, June 2012.

[5] A. Babenko and V. Lempitsky. Improving bilayer product

quantization for billion-scale approximate nearest neighbors
in high dimensions. arXiv preprint arXiv:1404.1831, 2014.

[6] A. Babenko and V. Lempitsky. Eﬃcient indexing of

billion-scale datasets of deep descriptors. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition,
pages 2055–2063, June 2016.

[7] R. Barrientos, J. G´omez, C. Tenllado, M. Prieto, and

M. Marin. knn query processing in metric spaces using
GPUs. In International European Conference on Parallel
and Distributed Computing, volume 6852 of Lecture Notes

10

in Computer Science, pages 380–392, Bordeaux, France,
September 2011. Springer.

[8] K. E. Batcher. Sorting networks and their applications. In
Proc. Spring Joint Computer Conference, AFIPS ’68
(Spring), pages 307–314, New York, NY, USA, 1968. ACM.

[9] P. Boncz, W. Lehner, and T. Neumann. Special issue:
Modern hardware. The VLDB Journal, 25(5):623–624,
2016.

[10] J. Canny, D. L. W. Hall, and D. Klein. A multi-teraﬂop
constituency parser using GPUs. In Proc. Empirical
Methods on Natural Language Processing, pages 1898–1907.
ACL, 2013.

[11] J. Canny and H. Zhao. Bidmach: Large-scale learning with
zero memory allocation. In BigLearn workshop, NIPS,
2013.

[12] B. Catanzaro, A. Keller, and M. Garland. A decomposition

for in-place matrix transposition. In Proc. ACM
Symposium on Principles and Practice of Parallel
Programming, PPoPP ’14, pages 193–206, 2014.

[13] J. Chhugani, A. D. Nguyen, V. W. Lee, W. Macy,
M. Hagog, Y.-K. Chen, A. Baransi, S. Kumar, and
P. Dubey. Eﬃcient implementation of sorting on multi-core
simd cpu architecture. Proc. VLDB Endow.,
1(2):1313–1324, August 2008.

[14] A. Dashti. Eﬃcient computation of k-nearest neighbor

graphs for large high-dimensional data sets on gpu clusters.
Master’s thesis, University of Wisconsin Milwaukee, August
2013.

[15] W. Dong, M. Charikar, and K. Li. Eﬃcient k-nearest

neighbor graph construction for generic similarity measures.
In WWW: Proceeding of the International Conference on
World Wide Web, pages 577–586, March 2011.

[16] M. Douze, H. J´egou, and F. Perronnin. Polysemous codes.
In Proc. European Conference on Computer Vision, pages
785–801. Springer, October 2016.

[17] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product

quantization. IEEE Trans. PAMI, 36(4):744–755, 2014.

and Signal Processing, pages 861–864, May 2011.

[27] Y. Kalantidis and Y. Avrithis. Locally optimized product
quantization for approximate nearest neighbor search. In
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pages 2329–2336, June 2014.

[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classiﬁcation with deep convolutional neural networks. In
Advances in Neural Information Processing Systems, pages
1097–1105, 2012.

[29] F. T. Leighton. Introduction to Parallel Algorithms and
Architectures: Array, Trees, Hypercubes. Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA, 1992.

[30] E. Lindholm, J. Nickolls, S. Oberman, and J. Montrym.
NVIDIA Tesla: a uniﬁed graphics and computing
architecture. IEEE Micro, 28(2):39–55, March 2008.
[31] W. Liu and B. Vinter. Ad-heap: An eﬃcient heap data

structure for asymmetric multicore processors. In Proc. of
Workshop on General Purpose Processing Using GPUs,
pages 54:54–54:63. ACM, 2014.

[32] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and

J. Dean. Distributed representations of words and phrases
and their compositionality. In Advances in Neural
Information Processing Systems, pages 3111–3119, 2013.
[33] L. Monroe, J. Wendelberger, and S. Michalak. Randomized

selection on the GPU. In Proc. ACM Symposium on High
Performance Graphics, pages 89–98, 2011.

[34] M. Norouzi and D. Fleet. Cartesian k-means. In Proc.

IEEE Conference on Computer Vision and Pattern
Recognition, pages 3017–3024, June 2013.

[35] M. Norouzi, A. Punjani, and D. J. Fleet. Fast search in

Hamming space with multi-index hashing. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition,
pages 3108–3115, 2012.

[36] J. Pan and D. Manocha. Fast GPU-based locality sensitive
hashing for k-nearest neighbor computation. In Proc. ACM
International Conference on Advances in Geographic
Information Systems, pages 211–220, 2011.

[18] Y. Gong and S. Lazebnik. Iterative quantization: A

[37] L. Paulev´e, H. J´egou, and L. Amsaleg. Locality sensitive

procrustean approach to learning binary codes. In Proc.
IEEE Conference on Computer Vision and Pattern
Recognition, pages 817–824, June 2011.

[19] Y. Gong, L. Wang, R. Guo, and S. Lazebnik. Multi-scale

orderless pooling of deep convolutional activation features.
In Proc. European Conference on Computer Vision, pages
392–407, 2014.

hashing: a comparison of hash function types and querying
mechanisms. Pattern recognition letters, 31(11):1348–1358,
August 2010.

[38] O. Shamir. Fundamental limits of online and distributed
algorithms for statistical learning and estimation. In
Advances in Neural Information Processing Systems, pages
163–171, 2014.

[20] A. Gordo, J. Almazan, J. Revaud, and D. Larlus. Deep

[39] A. Sharif Razavian, H. Azizpour, J. Sullivan, and

image retrieval: Learning global representations for image
search. In Proc. European Conference on Computer Vision,
pages 241–257, 2016.

S. Carlsson. CNN features oﬀ-the-shelf: an astounding
baseline for recognition. In CVPR workshops, pages
512–519, 2014.

[21] S. Han, H. Mao, and W. J. Dally. Deep compression:

[40] N. Sismanis, N. Pitsianis, and X. Sun. Parallel search of

Compressing deep neural networks with pruning, trained
quantization and huﬀman coding. arXiv preprint
arXiv:1510.00149, 2015.

k-nearest neighbors with synchronous operations. In IEEE
High Performance Extreme Computing Conference, pages
1–6, 2012.

[22] K. He, F. Wen, and J. Sun. K-means hashing: An

[41] X. Tang, Z. Huang, D. M. Eyers, S. Mills, and M. Guo.

aﬃnity-preserving quantization method for learning binary
compact codes. In Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pages 2938–2945, June
2013.

[23] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual

learning for image recognition. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition, pages
770–778, June 2016.

[24] X. He, D. Agarwal, and S. K. Prasad. Design and

implementation of a parallel priority queue on many-core
architectures. IEEE International Conference on High
Performance Computing, pages 1–10, 2012.

[25] H. J´egou, M. Douze, and C. Schmid. Product quantization

for nearest neighbor search. IEEE Trans. PAMI,
33(1):117–128, January 2011.

[26] H. J´egou, R. Tavenard, M. Douze, and L. Amsaleg.
Searching in one billion vectors: re-rank with source
coding. In International Conference on Acoustics, Speech,

Eﬃcient selection algorithm for fast k-nn search on GPUs.
In IEEE International Parallel & Distributed Processing
Symposium, pages 397–406, 2015.

[42] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde,

K. Ni, D. Poland, D. Borth, and L.-J. Li. YFCC100M: The
new data in multimedia research. Communications of the
ACM, 59(2):64–73, January 2016.

[43] V. Volkov and J. W. Demmel. Benchmarking GPUs to tune

dense linear algebra. In Proc. ACM/IEEE Conference on
Supercomputing, pages 31:1–31:11, 2008.

[44] A. Wakatani and A. Murakami. GPGPU implementation of
nearest neighbor search with product quantization. In
IEEE International Symposium on Parallel and Distributed
Processing with Applications, pages 248–253, 2014.
[45] T. Warashina, K. Aoyama, H. Sawada, and T. Hattori.

Eﬃcient k-nearest neighbor graph construction using
mapreduce for large-scale data sets. IEICE Transactions,

11

97-D(12):3142–3154, 2014.

[46] R. Weber, H.-J. Schek, and S. Blott. A quantitative
analysis and performance study for similarity-search
methods in high-dimensional spaces. In Proc. International
Conference on Very Large DataBases, pages 194–205, 1998.

[47] P. Wieschollek, O. Wang, A. Sorkine-Hornung, and

H. P. A. Lensch. Eﬃcient large-scale approximate nearest
neighbor search on the GPU. In Proc. IEEE Conference on
Computer Vision and Pattern Recognition, pages
2027–2035, June 2016.

[48] S. Williams, A. Waterman, and D. Patterson. Rooﬂine: An

insightful visual performance model for multicore
architectures. Communications of the ACM, 52(4):65–76,
April 2009.

Appendix: Complexity analysis of WarpSelect
We derive the average number of times updates are triggered
in WarpSelect, for use in Section 4.3.

Let the input to k-selection be a sequence {a1, a2, ..., a(cid:96)}
(1-based indexing), a randomly chosen permutation of a set
of distinct elements. Elements are read sequentially in c
groups of size w (the warp; in our case, w = 32); assume (cid:96)
is a multiple of w, so c = (cid:96)/w. Recall that t is the thread
queue length. We call elements prior to or at position n
in the min-k seen so far the successive min-k (at n). The
likelihood that an is in the successive min-k at n is:

The last case is the probability of: there is a (cid:96) − 1 se-
quence with m − 1 successive min-k elements preceding us,
and the current element is in the successive min-k, or the
current element is not in the successive min-k, m ones are
before us. We can then develop a recurrence relationship for
π((cid:96), k, t, 1). Note that

δ((cid:96), b, k, t) :=

γ((cid:96), m, k)

(17)

min((bt+max(0,t−1)),(cid:96))
(cid:88)

m=bt

for b where 0 ≤ bt ≤ (cid:96) is the fraction of all sequences of
length (cid:96) that will force b sorts of data by winning the thread
queue ballot, as there have to be bt to (bt + max(0, t − 1))
elements in the successive min-k for these sorts to happen (as
the min-k elements will overﬂow the thread queues). There
are at most (cid:98)(cid:96)/t(cid:99) won ballots that can occur, as it takes t
separate sequential current min-k seen elements to win the
ballot. π((cid:96), k, t, 1) is thus the expectation of this over all
possible b:

π((cid:96), k, t, 1) =

b · δ((cid:96), b, k, t).

(18)

(cid:98)(cid:96)/t(cid:99)
(cid:88)

b=1

α(n, k) :=

(cid:40)

1
k/n

if n ≤ k
if n > k

(13)

This can be computed by dynamic programming. Analyti-
cally, note that for t = 1, k = 1, π((cid:96), 1, 1, 1) is the harmonic
number H(cid:96) = 1 + 1
(cid:96) , which converges to ln((cid:96)) + γ
(the Euler-Mascheroni constant γ) as (cid:96) → ∞.

3 + ... + 1

2 + 1

as each an, n > k has a k/n chance as all permutations are
equally likely, and all elements in the ﬁrst k qualify.

Counting the insertion sorts. In a given lane, an inser-
tion sort is triggered if the incoming value is in the successive
min-k + t values, but the lane has “seen” only wc0 + (c − c0)
values, where c0 is the previous won warp ballot. The prob-
ability of this happening is:

α(wc0 + (c − c0), k + t) ≈

for c > k.

(14)

k + t
wc

The approximation considers that the thread queue has seen
all the wc values, not just those assigned to its lane. The
probability of any lane triggering an insertion sort is then:

(cid:18)

1 −

1 −

(cid:19)w

k + t
wc

≈

k + t
c

.

(15)

Here the approximation is a ﬁrst-order Taylor expansion.
Summing up the probabilities over c gives an expected num-
ber of insertions of N2 ≈ (k + t) log(c) = O(k log((cid:96)/w)).

Counting full sorts. We seek N3 = π((cid:96), k, t, w), the ex-
pected number of full sorts required for WarpSelect.

Single lane. For now, we assume w = 1, so c = (cid:96). Let
γ((cid:96), m, k) be the probability that in an sequence {a1, ..., a(cid:96)},
exactly m of the elements as encountered by a sequential
scanner (w = 1) are in the successive min-k. Given m, there
(cid:1) places where these successive min-k elements can
are (cid:0) (cid:96)
occur. It is given by a recurrence relation:

m

γ((cid:96), m, k) :=






1
0
0
(γ((cid:96) − 1, m − 1, k) · α((cid:96), k)+
γ((cid:96) − 1, m, k) · (1 − α((cid:96), k)))

(cid:96) = 0 and m = 0
(cid:96) = 0 and m > 0
(cid:96) > 0 and m = 0

otherwise.

(16)

12

For t = 1, k > 1, (cid:96) > k, π((cid:96), k, 1, 1) = k + k(H(cid:96) − Hk)
or O(k log((cid:96))), as the ﬁrst k elements are in the successive
k+2 +...+ k
(cid:96) .
min-k, and the expectation for the rest is

k+1 + k

k

For t > 1, k > 1, (cid:96) > k, note that there are some number
D, k ≤ D ≤ (cid:96) of successive min-k determinations D made
for each possible {a1, ..., a(cid:96)}. The number of won ballots for
each case is by deﬁnition (cid:98)D/t(cid:99), as the thread queue must
ﬁll up t times. Thus, π((cid:96), k, t, 1) = O(k log((cid:96))/t).

Multiple lanes. The w > 1 case is complicated by the
fact that there are joint probabilities to consider (if more
than one of the w workers triggers a sort for a given group,
only one sort takes place). However, the likelihood can be
bounded. Let π(cid:48)((cid:96), k, t, w) be the expected won ballots as-
suming no mutual interference between the w workers for
winning ballots (i.e., we win b ballots if there are b ≤ w
workers that independently win a ballot at a single step),
but with the shared min-k set after each sort from the joint
sequence. Assume that k ≥ w. Then:

π(cid:48)((cid:96), k, 1, w) ≤ w

(cid:25)

(cid:32) (cid:24) k
w

+

(cid:100)(cid:96)/w(cid:101)−(cid:100)k/w(cid:101)
(cid:88)

(cid:33)

k
w((cid:100)k/w(cid:101) + i)

i=1
≤ wπ((cid:100)(cid:96)/w(cid:101), k, 1, 1) = O(wk log((cid:96)/w))

(19)
where the likelihood of the w workers seeing a successive
min-k element has an upper bound of that of the ﬁrst worker
at each step. As before, the number of won ballots is scaled
by t, so π(cid:48)((cid:96), k, t, w) = O(wk log((cid:96)/w)/t). Mutual interfer-
ence can only reduce the number of ballots, so we obtain the
same upper bound for π((cid:96), k, t, w).

