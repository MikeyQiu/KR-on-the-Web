9
1
0
2
 
l
u
J
 
1
1
 
 
]

V
C
.
s
c
[
 
 
2
v
8
5
3
0
0
.
6
0
9
1
:
v
i
X
r
a

Data Augmentation for Object Detection via Progressive and Selective
Instance-Switching

Hao Wang1, Qilong Wang2, Fan Yang1, Weiqi Zhang 1, Wangmeng Zuo1,∗
1Harbin Institute of Technology, China, 2Tianjin University, China
ddsywh@yeah.net, qlwang@tju.edu.cn, buddhisant@outlook.com

amote530@163.com, wmzuo@hit.edu.cn

Abstract

Collection of massive well-annotated samples is effec-
tive in improving object detection performance but is ex-
tremely laborious and costly.
Instead of data collection
and annotation, the recently proposed Cut-Paste methods
[12, 15] show the potential to augment training dataset by
cutting foreground objects and pasting them on proper new
backgrounds. However, existing Cut-Paste methods cannot
guarantee synthetic images always precisely model visual
context, and all of them require external datasets. To han-
dle above issues, this paper proposes a simple yet effective
instance-switching (IS) strategy, which generates new train-
ing data by switching instances of same class from different
images. Our IS naturally preserves contextual coherence
in the original images while requiring no external dataset.
For guiding our IS to obtain better object performance, we
explore issues of instance imbalance and class importance
in datasets, which frequently occur and bring adverse ef-
fect on detection performance. To this end, we propose a
novel Progressive and Selective Instance-Switching (PSIS)
method to augment training data for object detection. The
proposed PSIS enhances instance balance by combining se-
lective re-sampling with a class-balanced loss, and consid-
ers class importance by progressively augmenting training
dataset guided by detection performance. The experiments
are conducted on the challenging MS COCO benchmark,
and results demonstrate our PSIS brings clear improvement
over various state-of-the-art detectors (e.g., Faster R-CNN,
FPN, Mask R-CNN and SNIPER), showing the superiority
and generality of our PSIS. Code and models are available
at: https://github.com/Hwang64/PSIS.

1. Introduction

Object detection as one of core issues in computer vision
community has been covered with many applications, e.g.,

∗Corresponding author.

face recognition [37], robotic grasping [30] and human in-
teraction [19]. In recent years, object detection greatly ben-
eﬁts from the rapid development of deep convolutional neu-
ral networks (CNNs) [25, 27, 42, 44], whose performance
is heavily dependent on a mass of labeled training images.
However, a large-scale well-annotated dataset for object de-
tection is much more laborious and costly, compared with
annotation of the whole images for classiﬁcation. One po-
tential way to efﬁciently acquire more training images is
augmentation of the existing datasets or generation of syn-
thetic images in an effective way.

In terms of data augmentation for object detection, tradi-
tional methods perform geometrical transformations (e.g.,
horizontal ﬂipping [39], multi-scale strategy [43], patch
crop [36] and random erasing [50]) on the original images to
vary their spatial structures. However, these methods hardly
change the visual content and context of objects, bringing
little increase in the diversity of training dataset. Recently,
some researches [12, 15, 18, 21, 32] show generation of new
synthetic images in Cut-Paste manner is a potential way
to augment detection datasets. Speciﬁcally, given the cut-
ting foreground objects, these methods paste them on some
background images, where the pasted positions are esti-
mated by considering 3D scene geometry [21], in a random
manner [15], by predicting support surfaces [18] or by mod-
eling visual context [12, 32]. Meanwhile, above Cut-Paste
methods [12, 32] show appropriate visual context plays a
key role in data augmentation for object detection. How-
ever, it is very difﬁcult for the existing Cut-Paste methods to
precisely model visual context for the whole time, although
some prediction mechanisms are carefully designed. Fur-
thermore, they require external datasets for collecting fore-
ground/background images or both of them.

To overcome above issues, this paper proposes a simple
yet efﬁcient instance-switching (IS) strategy. Speciﬁcally,
given a pair of training images containing instances of same
class, our IS method generates a new pair of training images
by switching the instances of same class by taking shape
and scale information of instances into account. As illus-

1

verse effect on detection performance. However, few ob-
ject detection method is concerned with these issues and
the simple IS also lacks the ability to handle issue of in-
stance imbalance and consider class importance in datasets.
To this end, we explore a progressive and selective scheme
for our IS strategy to generate synthetic images for ob-
ject detection while training detectors with a class-balanced
loss. The resulted method is called Progressive and Se-
lective Instance-Switching (PSIS), which enhances instance
balance by selectively performing IS (i.e., inversely pro-
portional to the original instance frequency) for adjusting
distribution of instances [3, 4, 22] and combining with a
class-balanced loss [8]. To consider class importance, PSIS
augments training dataset in a progressive manner, which is
guided by detection performance (i.e., increase of times of
IS for classes with lowest APs). The experiments are con-
ducted on MS COCO dataset [35] to evaluate out PSIS.

The contributions of this paper are summarized as fol-
lows:
(1) We propose a simple yet efﬁcient instance-
switching (IS) strategy to augment training data for object
detection. The proposed IS can increase the diversity of
samples, while preserving contextual coherence in the orig-
inal images and requiring no external datasets. (2) We pro-
pose a novel Progressive and Selective Instance-Switching
(PSIS) method to guide our IS strategy can enhance in-
stance balance and consider class importance, which are
ubiquity but few detection methods pay attention to. Our
PSIS can further improve detection performance. (3) We
thoroughly evaluate the proposed PSIS on challenging MS
COCO benchmark, and experimental results show our PSIS
is superior and complementary to existing data augmenta-
tion methods and brings clear improvement over various
state-of-the-art detectors (e.g., Faster R-CNN [39], Mask R-
CNN [23] and SNIPER [43]), while our PSIS has the poten-
tial to improve performance of instance segmentation [23].

2. Related Work

Our work focuses on data augmentation for object de-
tection. Albeit many data augmentation methods are pro-
posed for deep CNNs in image classiﬁcation task [9, 10,
28, 45, 49], it is not clear whether they are suitable for
object detection or not. The traditional data augmentation
for object detection performs geometrical transformations
on images [17, 23, 36, 39], and some works use random
occlusion mask to dropout some part of images [50] or
learn occlusion mask on convolutional feature maps [47]
for generating hard positive samples. An alternative solu-
tion is to generate new synthetic images in Cut-Paste man-
ner [12, 15, 18, 21, 32]. Compared with above methods, our
method can better increase the diversity of samples than tra-
ditional ones, and model more precise visual context than
Cut-Paste methods in an efﬁcient way. Generative Adver-
sarial Networks (GANs) [1, 6, 20, 46] have shown promis-

Figure 1. Some analysis based on Faster R-CNN [39] with ResNet-
101 [25] on MS COCO [35]. (a) Comparing a pair of the original
images with a pair of synthetic ones generated by our instance-
switching (IS) strategy. Clearly, the synthetic images naturally
preserve contextual coherence in the original ones. Meanwhile,
the detector can correctly detect objects in the original images,
but miss the switched instances (i.e., plant, denoted by red dashed
boxes) in synthetic images. (b) shows distribution of instances in
each class and (c) gives average precision (AP) of each class.

trated in Figure 1 (a), our IS strategy can always preserve
contextual coherence in the original images. Meanwhile,
a detector (i.e., Faster R-CNN [39] with ResNet-101 [25])
trained on original dataset can correctly detect objects in
the original images, but miss the switched instances (i.e.,
plant, denoted by red dashed boxes) in synthetic images, in-
dicating IS has ability to increase the diversity of samples.
In addition, our IS clearly requires no external datasets, and
naturally keeps more coherence in visual context, compared
with methods those paste the cutting foreground objects on
a new background [12, 15, 21].

The attendant problem is what criteria do we follow to
perform IS? We ﬁrst make analysis using Faster R-CNN
with ResNet-101 on the most widely used MS COCO [35].
Figure 1 (b) and (c) show distribution of instances in each
class and average precision (AP) of each class, respectively.
We can clearly see that distribution of instances is long-
tailed and detection performance of each class is quite dif-
ferent. Many works demonstrate data imbalance brings side
effect on image classiﬁcation [7, 8, 11, 26, 29, 48]. Mean-
while, detection difﬁculties of different classes greatly vary,
and distribution of instances and detection performance of
each class do not always behave in the same way. Above is-
sues of instance imbalance and class importance frequently
occur in real-world applications and inevitably bring ad-

Figure 2. Overview of instance-switching strategy for image generation. We ﬁrst select a candidate set Ωc
for each class based on shape and scale of instance. For a quadruple {I c
rescaling and cut-paste. Finally, Gaussian blurring is used to smooth the boundary artifacts. Thus, synthetic images { ˆI c

cad, we switch the instances ic
A, ˆI c

cad from all training data Ωc
all
A and ic
B through
B} are generated.

B} in Ωc

B, ic

A, ic

A,I c

ing ability for image synthesis. However, these methods are
unable to automatically annotate bounding boxes of objects,
so that they cannot be directly used for object detection. The
recently proposed compositional GAN [2] introduces a self-
consistent composition-decomposition network to generate
images by combining two instances with explicitly learning
possible interactions, but it is incapable of generating im-
ages with complex context. Remea et al. [38] use a copy-
and-paste GAN to generate images for weakly segmentation
tasks. However, synthetic images generated by GANs usu-
ally suffer from a clear domain gap with the real images,
bringing the side effect for subsequent applications [41].

The way of our PSIS method to enhance instance balance
is related to works those handle the issue of class imbal-
ance in image classiﬁcation [4, 8, 11, 22, 26, 48]. Among
them, methods [3, 4, 22] balance distribution of classes by
re-sampling for those containing less training samples. An-
other kind of methods perform sample re-weighting by in-
troducing some cost-sensitive losses [8, 26, 29, 40], which
can alleviate the over-ﬁtting caused by duplicated sam-
ples in re-sampling process. Wang et al. [48] propose an
on-line method to adaptively fuse the sampling strategy
with loss learning using two level curriculum schedulers.
But differently, our PSIS focuses on balance of instances
rather than images, as one image may contain multiple in-
stances. Moreover, we combine re-sampling with sample
re-weighting for better enhancing instance balance. Addi-
tionally, Focal loss [34] is proposed to handle issue of class
imbalance in one-stage detectors, but it only focuses on two
classes (i.e., foreground and background). Our PSIS em-
ploys a progressive augmentation process, which is related
to [7]. The latter proposes a Drop and Pick training policy
to select samples for reducing training computation in im-
age classiﬁcation task. In contrast, our PSIS progressively
increases number of instances for classes with lowest APs.

3. Proposed Method

In this section, we introduce the proposed Progres-
sive and Selective Instance-Switching (PSIS) method. We
ﬁrst describe our Instance-Switching (IS) strategy for syn-
thetic images generation. Then, selective re-sampling and
class-balanced loss are introduced to enhance instance bal-
ance. Additionally, we perform IS in a progressive man-
ner for considering class importance. Finally, we show the
overview of our PSIS for object detection.

3.1. Instance-Switching for Image Generation

Previous works [12, 32] show appropriate visual context
plays a key role in synthetic images for object detection. To
precisely model visual context of synthetic images while
increasing the diversity of training samples, this paper pro-
poses an instance-switching (IS) strategy for synthetic im-
age generation. Figure 2 illustrates the core idea of our IS
for image generation. Let Ωc
all be the training set of class
c (e.g., teddy bear) involving N images. To guarantee vi-
sual context of synthetic images as coherence as possible,
we deﬁne a candidate set (indicted by Ωc
cad) based on seg-
mentation masks of instances1.

Speciﬁcally, our Ωc
A, ic
A,I c

{I c
B, ic
lowing conditions:

B}, where I c

cad consists of a set of quadruples
B satisfy the fol-

A and ic

A, I c

B, ic

Ωc
A, I c
cad : {I c
s.t., fshape(ic

B}, I c
A, ic

B ∈ Ωc

A, I c
A ∈ I c
B) < ε, ρ1 < fscale(ic

all, ic

A, ic
A, ic

B ∈ I c
B) < ρ2,

B, (1)

A and I c

A and ic
all; ic
where I c
B are in-
stances of label c in images I c
B, respectively. fshape
and fscale are two functions matching shapes and scales of

B are two images in Ωc
A and I c

1In this paper, our experiments are conducted on MS COCO
dataset [35], where segmentation mask of each instance can be reliably
obtained using COCO API. Note that all existing Cut-Paste based data
augmentation methods for object detection employ segmented objects or
masks [12, 15, 18, 21].

A and mc

A and ic
A and ic

B. Let mc
instances ic
B be masks (binary im-
ages) of ic
B, we align them and rescale them to the
same size, then the normalized masks are denoted as (cid:98)mc
A
and (cid:98)mc
B. Accordingly, the function fshape can be computed
based on sum square difference (ssd) [31, 51], i.e.,

fshape(ic

A, ic

B) =

ssd( (cid:98)mc
max(area( (cid:98)mc

A, (cid:98)mc
B)
A), area( (cid:98)mc

B))

,

(2)

where area(Q) indicates area of Q, counted by the num-
ber of one in Q. max is a maximum function. Then, the
function fscale has

fscale(ic

A, ic

B) = area(mc

B)/area(mc

A).

(3)

B, ic

B}, where instances ic

According to Eqns. (1), (2) and (3), Ωc
A,I c

cad consists of a
set of quadruples {I c
A, ic
A and
ic
B have similar shapes (i.e., shape differences are less than
threshold ε) and are in controlled scaling ratios (i.e., scal-
ing ratios range from ρ1 to ρ2). Given the candidate set
cad, we can generate new training images { ˆI c
Ωc
B} by
A and ic
switching instances ic
B through rescaling and cut-
paste. Finally, we follow [15] to employ Gaussian blurring
for smoothing the boundary artifacts. Once the quadruple
A,I c
{I c
B} completes IS, we will remove them from
Ωc
cad to avoid duplicated sampling. Clearly, our IS method
can preserve contextual coherence in the original images
and requires no external datasets.

A, ˆI c

B, ic

A, ic

3.2. Enhancement of Instance Balance

The data in many real-world applications are long-tailed
distributions. As shown in Figure 1, the most widely used
object detection dataset [35] also suffers from extreme im-
balance distribution of instances, bringing the challenge for
detection methods. To alleviate this issue, we propose to ex-
ploit a selective re-sampling strategy for balancing distribu-
tion of instances and a class-balanced loss [8] for avoiding
over-ﬁtting of re-sampling.

3.2.1 Selective Re-sampling Strategy

To obtain an instance-balanced training dataset, a natural
scheme is to select the same number of quadruples in each
Ωc
cad, and perform IS to generate synthetic images. The re-
sulted dataset is dubbed by Ωequ, and the selected set of
quadruples in c-th class is indicated by Ωc
equ. Comparing
Figure 1 (b) with Figure 3 (a), distribution of instances in
Ωequ is more balanced than one in original training dataset
Ωori. However, one image usually contains different in-
stances of various numbers, so distribution of instances in
the equally sampled dataset Ωequ is still far away from a
uniform one. Hence, we propose a selective re-sampling
strategy to further adjust distribution of instances in Ωequ.

Figure 3. Distributions of instances in equally sampled dataset
Ωequ (a) and adjustment dataset Ωuni (b). Clearly, our re-
sampling method makes distribution of instances more uniform,
leading to better detection performance (refer to Section 4.1.2).

Let small and large classes indicate the classes with
fewer and more instances, respectively. Our selective re-
sampling strategy is developed based on a straightforward
rule: (1) On the one hand, we pick more images for small
classes, where each picked image should contain instances
of small classes as many as possible while involving in-
stances of large classes as few as possible; (2) On the other
hand, we drop images for large classes, where the dropped
images have opposite situation to the picked ones.
Intu-
itively, if we persistently carry out above drop-pick process,
distribution of instances in Ωequ will tend to be uniform.

In our case, we drop or pick the image I if it satisﬁes the

following conditions:

Mc − MIc
Mequ − MI

<

Mc
Mequ

<

Mc + MIc
Mequ + MI

(4)

where Mc and Mequ are numbers of instances in Ωc
equ and
Ωequ, respectively; MIc and MI indicate number of in-
stances of c-th class and number of all instances in I, re-
spectively. Furthermore, it is difﬁcult to directly handle all
classes as number of classes increases. So we carry out
above drop-pick within two stages.
In the ﬁrst stage, we
handle maximum and minimum classes, and the drop-pick
process stops when percentage of instances in minimum
and maximum classes are 1.5 times and half of original
ones. Once drop-pick process stops, maximum and min-
imum classes will be changed, so we iteratively carry out
this process 20 times. In the second stage, we extend above
process to all classes. After our selective re-sampling strat-
egy, as shown in Figure 3 (b), distribution of instances in
Ωequ is adjusted to be approximately uniform, which is in-
dicated by Ωuni.

3.2.2 Class-Balanced Loss

Above selective re-sampling strategy can generate a syn-
thetic training dataset where distribution of instances is ap-
proximately uniform, but it is still very difﬁcult to make

the whole training datasets (i.e., Ωuni + Ωori) obey a uni-
form distribution. Moreover, over-sampling easily gener-
ates the duplicated images, leading to over-ﬁtting [8]. The
similar phenomenon is also found in our experiments (re-
fer to Section 4.1.2 for details). To this end, we exploit
the recently proposed class-balanced loss [8] to re-weight
instances, which reformulates the softmax loss by introduc-
ing a weight that is inversely proportional to the number
of instances in each class. Given an instance with label c
and the prediction vector z = [z1, z2, . . . , zC](cid:62), where C
is the number of classes. The class-balanced loss can be
computed as:

(cid:96)CB(z, c) = −

γ
1 − (1 − γ)nc

log(

exp(zc)
j=1 exp(zj)

)

(cid:80)C

(5)

where nc is the number of instances belonging to the class
of c, and 0 < γ (cid:54) 1 is a regularized constant to control
the effect of number of instances on ﬁnal prediction.
In
particular, Eqn. (5) will degenerate to the standard softmax
loss if γ = 1, and γ → 0 will enlarge the effect of number
of instances, which is evaluated in Section 4.1.2.

3.3. Progressive Instance-Switching

In above section, we introduce two strategies to enhance
instance balance. As shown in Figure 1 (b) and (c), detec-
tion performance (i.e., AP) of each class is quite different,
while AP of each class does not always show positive cor-
relation with distribution of instances. Above observations
encourage us to pay more attention on these classes with
lowest APs for further improving detection performance.
To this end, we introduce a progressive instance-switching
for data augmentation, which generates more training im-
ages for the classes with lowest APs in a self-supervised
learning manner.

Speciﬁcally, we ﬁrst train a detector using a predeﬁned
training dataset Ωtrain. After T training epochs, we evalu-
ate the current detector on the validation set and pick up K
classes with the lowest APs. In order to control the number
of augmented images for avoiding over-ﬁtting and breaking
of balance, we perform selective instance-switching (Sec-
tion 3.2.1) to generate training images for c-th class (c is
one of K classes of lowest APs) based on the proportion θc:

θc = p +

∗ Rank(c), 1 (cid:54) c (cid:54) K,

(6)

p
K

where K and p are number of classes and image percent-
age to be augmented, which are decided by cross valida-
tion. Rank(c) indicates rank of AP of c-th class sorted in
a descending order, e.g, Rank(c) = K if c-th class has the
lowest AP. The new generated dataset is denoted by Ωaug,
given it we go on training the current detector using union
set of Ωtrain and Ωaug. According to Eqn. (6), our progres-
sive instance-switching method can augment more training

images for the classes with lower APs in a controlled range,
which takes class importance into consideration.

Algorithm 1 PSIS for Object Detection
Input: Original training dataset Ωori, number of training

epochs Ttotal;

Output: Augmented dataset ΩP SIS, object detector D;

1: Generate a uniform augmented dataset Ωuni (Sec-

tion 3.2.1);

2: Ωtrain ← Ωori ∪ Ωuni;
3: for t ∈ [1, . . . , Ttotal] do
4:
5:
6:

Train detector Dt with loss (cid:96)CB (5) on Ωtrain;
if mod (t, T ) = 0 then

Generate a new dataset Ωaug based on Dt (Sec-

Ωtrain ← Ωtrain ∪ Ωaug;

tion 3.3);

end if

7:
8:
9: end for
10: ΩP SIS ← Ωtrain;

3.4. PSIS for Object Detection

So far, we have introduced our proposed progressive
and selective instance-switching (PSIS) method. Finally,
we show how to apply our PSIS for object detection, which
is summarized in Algorithm 1. Speciﬁcally, we ﬁrst gener-
ate a uniform augmented dataset Ωuni based on the original
training dataset Ωori as descried in Section 3.2.1. Then we
combine Ωori with Ωuni as initial training dataset Ωtrain,
and employ Ωtrain to train the detector D with the class
balance loss (5). After T training epochs, we augment the
Ωtrain using the current detector Dt following the rules in
Section 3.3, and continue training of detector on the union
set of Ωtrain and Ωaug. Finally, our PSIS algorithm outputs
a object detector D and an augmented dataset ΩP SIS. As
shown in our experiments, the augmented dataset ΩP SIS
can be ﬂexibly adopted to other detectors or tasks.

4. Experiments

In this section, we evaluate our PSIS method on the chal-
lenging MS COCO 2017 [35], which is the most widely
used benchmark for object detection, containing about 118k
training, 5k validation and 40k testing images collected
from 80 classes. We ﬁrst make ablation studies using Faster
R-CNN [39] with ResNet-101 [25]. For determining candi-
date sets for our instance-switching, we ﬁx the parameters
ε, ρ1 and ρ2 in Eqn. (1) to 0.3, 1/3 and 3, respectively. We
train Faster R-CNN (ResNet-101) within 14 epochs (i.e.,
Ttotal = 14 in Algorithm 1) and set T to 6. The initial
learning rate is set to 10−3, and decreases with a factor of
10 after 11 training epochs. The horizontal ﬂipping is used
for data augmentation. After generating augmented dataset

Table 1. Results (%) of Faster R-CNN (ResNet-101) on validation set of MS COCO 2017 under different training conﬁgurations.
Avg.Recall, Area:

Avg.Precision, Area:

Avg.Precision, IOU:

Training sets

Detector

Ωori
Ωori + Ωequ
Ωori + Ωuni
Ωori + Ωuni + (cid:96)CB
ΩP SIS + (cid:96)CB

Faster R-CNN
(ResNet-101)
[39]

0.5:0.95
27.3
28.4
28.7
29.0
29.7

0.50
48.6
49.1
49.3
49.7
50.6

0.75
27.5
29.1
29.8
30.2
30.5

Small Med.
30.4
32.1
32.6
33.0
33.4

8.8
8.9
9.2
9.4
10.3

Large
43.1
44.8
45.2
46.0
48.0

Avg.Recall, #Det:
100
10
1
39.3
38.5
25.8
40.5
39.4
26.7
41.0
39.7
27.2
41.5
40.1
27.4
41.6
40.5
27.6

Small Med.
44.5
16.3
45.8
16.8
46.3
17.1
47.0
17.5
47.6
18.6

Large
60.2
62.0
62.7
63.2
64.6

(ΩP SIS) by our PSIS and Faster R-CNN, we directly adopt
ΩP SIS to four state-of-the-art detectors (i.e., FPN [33],
Mask R-CNN [23], SNIPER [43] and BlitzNet [14]) for
comparing with the related works. Finally, we verify the
generalization ability of ΩP SIS on instance segmentation
task. For all evaluated detectors, Faster R-CNN, SNIPER
and BlitzNet are implemented using the source codes re-
leased by the respective authors. We employ the publicly
available toolkit [5] to implement FPN [33] and Mask R-
CNN [23]. All programs run on a server equipped with four
NVIDIA GTX 1080Ti GPUs. Following the standard eval-
uation metric (e.g., mean Average Precision (mAP) under
IoU = [0.5 : 0.95]), we report results on validation set
and test-dev 2017 evaluation server for ablation studies and
comparison, respectively.

4.1. Ablation studies

In this subsection, we assess the effects of different com-
ponents on our PSIS, including instance-switching strategy,
instance balance enhancement and progressive instance-
switching. To this end, we train Faster R-CNN [39] on var-
ious training datasets under the exactly same experimental
settings, and report the results on the validation set.

4.1.1 Instance-switching Strategy

Using our instance-switching strategy in the equally
sampling manner, we can generate a synthetic dataset Ωequ,
which shares the same size (i.e., 118k) with the original
training dataset Ωori. As compared in the top two rows
of Table 1, combination of Ωori with Ωequ can achieve
consistent improvements over single Ωori under all evalu-
ation metrics.
In particular, Faster R-CNN (ResNet-101)
trained with Ωori + Ωequ outperforms one with Ωori by
1.1% in terms of mAP under IoU = [0.5 : 0.95]. This
clear improvement veriﬁes the effectiveness of instance-
switching strategy as data augmentation for object detec-
tion, and we owe the improvement to increase of diver-
sity and keep of visual contextual coherence inherent in our
instance-switching strategy.

To further verify effect of our instance-switching (IS)
strategy, we make a statistical analysis. Speciﬁcally, we ran-
domly pick 2K images with swappable instances (i.e., Ω2K
and Ω2K
v ) from training (Ωt) and validation (Ωv) sets of MS
COCO 2017, respectively. Then, we perform 1K times IS

t

Figure 4. Results of loss (cid:96)CB with various values of γ by training
Faster R-CNN on Ωori + Ωuni. Note that γ = 1 is the classical
softmax loss.

t

t

t

v

and Ω2K

and Ω2K

are indicted by ΩIS

and Ω2K
on Ω2K
v . The image sets containing 2K switched
and ΩIS
instances of Ω2K
v ,
t
respectively. As listed in Table 3, Faster R-CNN trained on
Ωt misses 76 and 117 more instances on ΩIS
than
t
those on Ω2K
v , respectively. In contrast, Faster R-
(cid:83) ΩIS
CNN trained on Ωt
can reduce number of the missed
t
instances, speciﬁcally on ΩIS
v . The result indicates
our IS can change the context of switched instances, thereby
increasing the diversity of samples and leading the missing
of pre-trained detector for switched instances, and it can be
well suppressed by training detector on Ωt

and ΩIS
v

and ΩIS

(cid:83) ΩIS
t

.

t

4.1.2

Instance Balance Enhancement

Our PSIS introduces two strategies to enhance instance
balance, including selective re-sampling strategy and class-
balanced loss. As shown in Figure 3 (b), our selective re-
sampling strategy is helpful to generate an approximately
uniform training dataset Ωuni, which has the same size (i.e.,
118k) with Ωori and Ωequ. As listed in Table 1, Ωuni brings
1.4% and 0.3% gains over Ωori and Ωequ in terms of mAP
under IoU = [0.5 : 0.95], respectively. Combining with
class-balanced loss (cid:96)CB, detection performance can further
be improved to 29.0%. Through introducing instance bal-
ance enhancement into our instance-switching strategy, we
obtain 0.6% improvement over Ωequ that performs instance-
switching in a non-uniform manner, indicating instance bal-
ance is an important property of dataset for better detec-
tion. Furthermore, we evaluate the effect of parameter γ
on class-balanced loss (cid:96)CB. In general, γ → 0 indicates
re-weighting instances in much larger inverse proportion
to instance number. As illustrated in Figure 4, γ = 10−3
achieves the best result, outperforming γ = 1 (i.e., the clas-
sical softmax loss) by 0.3%. The smaller γ (γ < 10−3)

Table 2. Results (%) of three state-of-the-art detectors (i.e., FPN [33], Mask R-CNN [23] and SNIPER [43]) under various augmentation
methods. ∗ indicates no horizontal ﬂipping, and ×2 means two times training epochs, which is regarded as training-time augmentation [24].

Training sets

Detectors

Avg.Precision, IOU:

Avg.Precision, Area:

Avg.Recall, Area:

Ω∗
ori
Ωori
Ω∗
P SIS
ΩP SIS
Ωori × 2
ΩP SIS × 2
Ωori
ΩP SIS
Ωori × 2
ΩP SIS × 2
Ωori
ΩP SIS

FPN
[33]

Mask R-CNN
[23]

SNIPER
[43]

0.5:0.95
38.1
38.6
38.7
39.8
39.4
40.2
39.4
40.7
40.4
41.2
43.4
44.2

0.50
59.1
60.4
59.7
61.0
60.7
61.1
61.0
61.8
61.6
62.5
62.8
63.5

0.75
41.3
41.6
41.8
43.4
43.0
44.2
43.3
44.5
44.2
45.4
48.8
49.3

Small Med.
42.0
20.7
42.8
22.3
43.0
21.6
44.2
22.7
43.6
22.1
45.7
22.3
43.7
23.1
45.2
23.4
44.8
22.3
46.0
23.7
45.2
27.4
46.2
29.3

Large
51.1
50.0
51.7
52.1
52.1
51.6
51.3
53.0
52.9
53.6
56.2
57.1

Avg.Recall, #Det:
100
10
1
51.5
49.3
31.6
53.2
50.6
31.8
52.3
50.0
32.0
53.6
51.1
32.6
53.4
51.0
32.5
53.6
51.2
32.6
54.3
51.5
32.3
55.4
52.8
33.3
54.5
52.0
33.1
55.5
52.9
33.4
N/A
N/A
N/A
65.9
60.1
35.0

Small Med.
55.7
31.1
57.7
34.5
56.4
32.3
59.0
34.8
57.6
33.6
58.9
33.6
58.7
34.9
59.7
35.5
58.8
34.7
60.0
36.2
N/A
N/A
70.4
50.4

Large
66.7
66.8
67.6
68.5
68.6
68.8
68.5
70.3
69.5
70.3
N/A
78.0

Table 3. Statistical analysis of missed instances associated with
2K switched ones. Numbers in brackets indicate increased or de-
creased missed instances.

Trained on Ωt

Trained on Ωt

(cid:83) ΩIS
t

Ω2K
t
146
133 (-13)

ΩIS
t
222 (+76)
142 (-80)

Ω2K
v
614
607 (-7)

ΩIS
v
731 (+117)
634 (-97)

mented. Figure 5 shows the results of ΩP SIS using pro-
gressive instance-switching with various K and p. From it
we can see that ΩP SIS with K = 30, p = 3 obtains the
best performance. Furthermore, generation of training im-
ages for more classes with larger proportions leads perfor-
mance decline. It may be account for breaking of instance
balance and over-ﬁtting caused by over-sampling. Finally,
our ΩP SIS involves about 283k training images in total, in-
cluding Ωori of 118k, Ωuni of 118k and Ωaug of 47k.

4.2. Apply ΩP SIS to State-of-the-art Detectors

In above subsection, we have generated an augmented
dataset ΩP SIS based on Faster R-CNN, Then, we directly
employ this dataset to train four state-of-the-art detectors
(i.e., FPN [33], Mask R-CNN [23], BlitzNet [14] and
SNIPER [43]), and report results on test server for com-
paring with other augmentation methods.

4.2.1 PSIS for FPN

We ﬁrstly adopt our ΩP SIS to FPN [33] with RoI Align-
ment layer [23] under Faster R-CNN framework [39]. We
employ ResNet-101 as backbone model, and train FPN fol-
lowing the same settings in [5]. Speciﬁcally, we train FPN
on Ωori within 12 epochs, and set the initial learning rate
to 10−2, which is decreased with a factor of 10 after 8 and
11 epochs. Due to ΩP SIS containing more images, FPN
is trained on ΩP SIS with 18 epochs and learning rate is
decreased after 13 and 16 epochs. Here we compare with
two widely used data augmentation methods, i.e., horizon-
tal ﬂipping and training-time augmentation [24].

The comparison results are given in Table 2, from it we
can see that our PSIS and horizontal ﬂipping bring 0.6%
and 0.5% gains over non-augmented one, respectively. We
further combine our PSIS with horizontal ﬂipping, which
achieves 39.8% mAP under IoU = [0.5 : 0.95], out-
performing horizontal ﬂipping and non-augmented one by
1.2% and 1.7%, respectively. We owe these gains to in-
crease of sample diversity inherent in our PSIS. Besides, we

Figure 5. Results of ΩP SIS using progressive instance-switching
with various K and p.

leads performance decline. Therefore, we set γ = 10−3 in
the following experiments.

4.1.3 Progressive Instance-switching

To take the class importance into account, we propose a
progressive instance-switching method for data augmenta-
tion. As listed in the bottom of Table 1, our progressive
manner (ΩP SIS) brings 0.7% gain in terms of mAP un-
der IoU = [0.5 : 0.95], demonstrating consideration of
class importance is helpful to augment data for object de-
tection. Note that our PSIS can achieve in total 2.4% im-
provement over the original dataset Ωori under a strong de-
tector (e.g., Faster R-CNN + ResNet-101). In addition, our
progressive instance-switching (6) involves two parameters,
i.e., class number K and image percentage p to be aug-

also compare with training-time augmentation method (i.e.,
2× training epochs). Note that our PSIS without training-
time augmentation is superior to training-time augmenta-
tion method by 0.4%. By performing training-time augmen-
tation, our PSIS further achieves 0.8% improvement. Above
results clearly demonstrate our PSIS is superior and com-
plementary to horizontal ﬂipping and training-time aug-
mentation methods.

4.2.2 PSIS for Mask R-CNN

We evaluate our PSIS using Mask R-CNN [23], which
also exploits mask information to improve object detec-
tion accuracy while achieving state-of-the-art performance.
Here, we employ ResNet-101 as backbone model, and train
Mask R-CNN using [5] and employing the same settings
with FPN. We compare our PSIS with two augmentation
manners, i.e., segmentation mask and training-time aug-
mentation. The results are compared in Table 2, where we
can see that Mask R-CNN trained with our ΩP SIS consis-
tently outperforms one with the original dataset Ωori under
all evaluation metrics, although Mask R-CNN exploits in-
stance masks as well. In particular, our PSIS brings 1.3%
gains under IoU = [0.5 : 0.95], demonstrating the com-
plementarity between mask branch of Mask R-CNN and
our PSIS. For training-time augmentation, ΩP SIS × 1 is
better than Ωori with 2 times training epochs. Combining
with training-time augmentation, PSIS is further improved
by 0.5% . Above results show again our PSIS is superior
and complementary to training-time augmentation method.

4.2.3 PSIS for SNIPER

SNIPER [43] is a recently proposed high-performance
detector, which handles object detection in an efﬁcient
multi-scale manner. To verify the effectiveness of our PSIS
under multi-scale training strategy, we employ ResNet-101
as backbone model and train SNIPER on Ωori and ΩP SIS
following the experimental settings in [43] except batch-
size and training epochs. Due to the limited computing
resources, we use half of batch-size and double training
epochs in the original settings, and we do not use negative
chip mining for [43]. As compared in Table 2, SNIPER
trained on our ΩP SIS achieves 0.8% gain over one with
the original training dataset, showing the proposed PSIS is
complementary to multi-scale strategy and it can be ﬂexibly
adopted to multi-scale training/testing detectors.

4.2.4 PSIS for BlitzNet

Recently, Dvornik et al. propose a context-based
data augmentation (Context-DA) method for object detec-
tion [13], where a real-time single-shot detector (dubbed by
BlitzNet [14]) is used to evaluate performance of Context-
DA. In comparison to Context-DA [13], we also adopt
our PSIS to BlitzNet. Following the settings in [13], we
train BlitzNet using ResNet-50 as backbone model, while

set batch-size and training epochs to half of and 2 times
original ones. Note that the original BlitzNet runs on MS
COCO 2014, so we rebuild our ΩP SIS based on the train-
ing set of MS COCO 2014. The results are compared in
Table 7. Clearly, both Context-DA and our PSIS improve
the original dataset Ωori. Meanwhile, our PSIS outperforms
Context-DA by 2.8% under IoU = [0.5 : 0.95]. We owe
this improvement to that our PSIS can preserve contextual
coherence in the original images and beneﬁt from more ap-
propriate visual context.

4.3. Longer Training Time on Ωori

As described in Section 4.2, our PSIS exploits more
training epochs.
To evaluate its effect, we also train
FPN [33] and Mask R-CNN [23] on the original set Ωori
longer time. Speciﬁcally, we train the detectors within 18
(same with ΩP SIS) and 44 (considering effects of both
more data and longer training) epochs on Ωori, which are
indicted by Ω†
ori, respectively. The results are
given in Table 4, where our PSIS (ΩP SIS) outperforms both
ori and Ω‡
Ω†
ori,
indicating more training epochs lead over-ﬁtting on Ωori.
Our PSIS avoids over-ﬁtting by increasing diversity of sam-
ples and beneﬁts from longer training-time.

ori brings little gains over Ω†

ori. Moreover, Ω‡

ori and Ω‡

Due to limited computing resources, we use half of
batch-size and double training epochs of original settings
when training SNIPER and BlitzNet on ΩP SIS. To assess
its effect, we report the results of SNIPER and BlitzNet
trained on Ωori with the same settings in Table 10 and Ta-
ble 11. They obtain 43.2% and 27.2% at IoU=[0.5:0.95],
respectively. For SNIPER, our PSIS (44.2%) can achieve
1% gain. Meanwhile, our PSIS (30.8%) can achieve 3.6%
gains for BlitzNet.

4.4. Generalization to Instance Segmentation

We verify the generalization ability of our PSIS on in-
stance segmentation task of MS COCO 2017. To this end,
we train Mask R-CNN [23] (ResNet-101) following the ex-
actly same settings in Section 4.2.2. As shown in Table 5,
our PSIS can improve the original training dataset over
0.8% and 0.5% under IoU = [0.5 : 0.95] within ×1 and ×2
training epochs, respectively. PSIS further achieves 0.4%
gain in training-time augmentation manner. As for Mask R-
CNN, it exploits instance masks to simultaneously perform
detection and segmentation through a multi-task loss for
obtaining better performance. Different from it, our PSIS
employs instance masks to augment training data, and the
results clearly show our PSIS offers a new and complemen-
tary way to use instance masks for improving both detection
and segmentation performance. Besides, above results indi-
cate our PSIS is independent on pre-deﬁned detector, and
can generalize well to various detectors and tasks.

Table 4. Results (%) of FPN and Mask R-CNN using different training sets and epochs on MS COCO. Ω†
within 18 and 44 epochs.

ori and Ω‡

ori: training on Ωori

Training sets

Detectors

Avg.Precision, IOU:

Avg.Precision, Area:

Avg.Recall, Area:

ori

Ω†
Ω‡
ori
ΩP SIS
Ω†
Ω‡
ori
ΩP SIS

ori

FPN
[33]

Mask R-CNN
[23]

Training sets

Method

Ωori
ΩP SIS
Ωori × 2
ΩP SIS × 2

Mask R-CNN
[23]

0.5:0.95
39.0
38.9
39.8
40.2
40.3
40.7

0.5:0.95
35.9
36.7
36.6
37.1

0.50
60.2
60.3
61.0
61.2
61.3
61.8

0.50
57.7
58.4
58.2
58.8

0.75
42.6
42.6
43.4
44.4
44.3
44.5

0.75
38.4
39.4
39.2
39.9

Small Med.
43.2
22.5
43.3
22.6
44.2
22.7
44.1
23.2
23.7
44.3
45.2
23.4

Large
50.7
50.0
52.1
51.8
51.9
53.0

Avg.Recall, #Det:
100
10
1
51.5
54.1
32.4
54.2
51.5
32.3
32.6
53.6
51.1
54.9
52.3
33.0
54.9
52.3
33.0
55.4
52.8
33.3

Small Med.
34.8
35.1
34.8
35.5
35.4
35.5

58.4
58.4
59.0
59.2
59.1
59.7

Large
68.6
68.3
68.5
69.2
69.2
70.3

Small Med.
39.7
19.2
40.6
19.0
40.3
18.5
41.2
19.3

Large
49.7
50.2
50.4
50.8

Avg.Recall, #Det:
100
10
1
49.6
47.3
30.5
50.3
48.2
31.0
49.7
47.7
31.0
50.4
47.7
31.1

Small Med.
53.8
29.7
54.4
29.8
53.5
29.5
54.5
30.2

Large
65.8
66.9
66.6
67.0

Table 5. Results (%) of Mask R-CNN training with different datasets on instance segmentation task of MS COCO 2017.
Avg.Recall, Area:

Avg.Precision, Area:

Avg.Precision, IOU:

Table 6. Results (%) of pre-trained Faster R-CNN using MS COCO on PASCAL VOC 2007 test set. Ωcoco−
Ωcoco

P SIS, respectively, as latter involve many irrelevant classes.

ori and Ωcoco

ori

and Ωcoco−

P SIS outperforms

Training sets
Ωcoco
ori
Ωcoco
P SIS
Ωcoco−
ori
Ωcoco−
P SIS

aero

81.4

82.2

82.1

83.9

bike

80.6

82.4

82.0

84.0

bird

81.1

83.9

83.3

85.0

boat

66.0

67.8

67.4

69.3

bottle

73.8

74.3

74.3

76.2

bus

85.1

85.8

87.2

88.9

car

81.6

82.2

82.9

84.0

cat

87.1

87.9

88.0

89.2

chair

64.4

65.1

64.1

67.2

cow

86.0

87.5

87.8

88.9

table

59.9

61.8

62.1

64.0

dog

85.6

85.7

86.5

86.6

horse

mbike

person

plant

sheep

87.7

89.3

89.1

90.6

85.3

86.8

87.5

88.3

85.1

86.1

85.2

87.4

49.6

52.5

52.4

54.0

84.2

85.0

85.0

86.1

sofa

73.7

75.3

76.2

76.9

train

87.8

88.5

87.3

89.6

tv

80.5

81.2

80.1

82.3

mAP

78.3

79.6

79.5

81.1

Table 7. Results (%) of BlitzNet [14] training with different
datasets on MS COCO 2014.

Training set

Ωori
Context-DA [13]
ΩP SIS

Avg.Precision, IOU:

Avg.Precision, Area:

0.5:0.95
27.3
28.0
30.8

0.50
46.0
46.7
50.0

0.75
28.1
28.9
32.2

Small Med.
26.8
10.7
27.8
10.7
31.0
12.6

Large
46.0
47.0
50.2

For Faster R-CNN, our PSIS obtains 1.2% gains over the
original training set ΩF
ori. For BlitzNet, PSIS outperforms
ΩB
DA) by 2.2% and 0.9%, re-
spectively. These improvements clearly show our PSIS has
good generalization ability to different datasets.

ori and Context-DA [13] (ΩB

4.5. Generalization to Small-scale PASCAL VOC

5. Conclusion

ori /Ωcoco

We further verify the generalization ability of PSIS by
adopting Faster R-CNN [39] trained with MS COCO to
test set of PASCAL VOC 2007 [16] without any ﬁne-
tuning. Here, we train Faster R-CNN on full MS COCO
P SIS) or a subset of MS COCO (Ωcoco−
(Ωcoco
P SIS ).
The latter only contains 20 classes those share with ones
of PASCAL VOC. As shown in Table 6, our PSIS respec-
tively obtains 1.3% and 1.6% gains in term of mAP, com-
paring with ones trained with the original datasets Ωcoco
ori
and Ωcoco−
. Above results demonstrate the improvement
achieved by PSIS on MS COCO can be generalized to other
dataset.

/Ωcoco−

ori

ori

Furthermore, we directly adopt PSIS to PASCAL
VOC [16]. Following the same experimental settings in
Context-DA [13], we use PASCAL VOC 2012 training set
that equips with segmentation annotations (including 1464
images) for training and test set of PASCAL VOC 2007 for
testing. Meanwhile, Faster R-CNN [39] and BlitzNet [14]
are used for evaluation. The results are compared in Table 8.

In this paper, we proposed a simple yet effective data
augmentation for object detection, whose core is a pro-
gressive and selective instance-switching (PSIS) method for
synthetic image generation. The proposed PSIS as data
augmentation for object detection beneﬁts several merits,
i.e., increase of diversity of samples, keep of contextual co-
herence in the original images, no requirement of external
datasets, and consideration of instance balance and class
importance. Experimental results demonstrate the effective-
ness of our PSIS against the existing data augmentation, in-
cluding horizontal ﬂipping and training time augmentation
for FPN, segmentation masks and training time augmen-
tation for Mask R-CNN, multi-scale training strategy for
SNIPER, and Context-DA for BlitzNet. The improvement
on both object detection and instance segmentation tasks
suggest our proposed PSIS has the potential to improve the
performance of other applications (i.e., keypoint detection),
which will be investigated in future work.

Table 8. Results (%) of different methods on PASCAL VOC 2007 test set. ΩF
and ΩP SIS, respectively. ΩB
DA and ΩB

ori and ΩF
P SIS indicate training BlitzNet [14] on Ωori, ΩDA and ΩP SIS, respectively.

P SIS indicate training Faster R-CNN [39] on Ωori

ori, ΩB

ori

P SIS

Training sets
ΩF
ΩF
ΩB
ori
ΩB
ΩB

DA

P SIS

aero

60.9

63.2

63.6

66.8

67.6

bike

75.8

76.4

73.3

75.3

76.4

bird

65.8

67.0

63.2

65.9

66.3

boat

48.4

50.6

57.0

57.2

57.7

bottle

57.0

57.8

31.5

33.1

33.5

bus

74.1

74.3

76.0

75.0

75.5

car

70.2

71.0

71.5

72.4

73.9

cat

83.0

83.3

79.9

79.6

80.8

chair

40.6

43.3

40.0

40.6

42.6

cow

74.7

76.5

71.6

73.9

74.5

table

56.3

56.8

61.4

63.7

65.5

dog

80.1

82.5

74.6

77.1

77.0

horse

mbike

person

plant

sheep

77.7

78.0

80.9

81.4

81.4

69.8

69.9

70.4

71.8

71.6

67.6

68.3

67.9

68.1

69.3

35.0

35.5

36.5

37.9

40.2

66.9

68.8

64.9

67.6

68.4

sofa

58.0

58.2

63.0

64.7

65.1

train

65.5

67.2

79.3

81.2

81.7

tv

57.2

59.9

64.7

65.5

66.3

mAP

64.2

65.4

64.6

65.9

66.8

References

[1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gener-

ative adversarial networks. In ICML, 2017. 2

[2] S. Azadi, D. Pathak, S. Ebrahimi, and T. Darrell. Com-
positional GAN: Learning conditional image composition.
arXiv:1807.07560, 2018. 3

[3] M. Buda, A. Maki, and M. A. Mazurowski. A systematic
study of the class imbalance problem in convolutional neural
networks. Neural Networks, 106:249–259, 2018. 2, 3
[4] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P.
SMOTE: synthetic minority over-sampling

Kegelmeyer.
technique. JAIR, 16:321–357, 2002. 2, 3

[5] K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun,
W. Feng, Z. Liu, J. Shi, W. Ouyang, C. C. Loy, and D. Lin.
mmdetection. https://github.com/open-mmlab/
mmdetection, 2018. 6, 7, 8

[6] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever,
and P. Abbeel. InfoGAN: Interpretable representation learn-
ing by information maximizing generative adversarial nets.
In NIPS, 2016. 2

[7] B. Cheng, Y. Wei, H. Shi, S. Chang, J. Xiong, and T. S.
Huang. Revisiting pre-training: An efﬁcient training method
for image classiﬁcation. arXiv:1811.09347, 2018. 2, 3
[8] Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie.
Class-balanced loss based on effective number of samples.
arXiv:1901.05555, 2019. 2, 3, 4, 5

[9] T. DeVries and G. W. Taylor. Dataset augmentation in fea-

ture space. arXiv:1702.05538, 2017. 2

[10] T. DeVries and G. W. Taylor.

Improved regular-
ization of convolutional neural networks with cutout.
arXiv:1708.04552, 2017. 2

[11] Q. Dong, S. Gong, and X. Zhu. Imbalanced deep learning
IEEE T-PAMI,

by minority class incremental rectiﬁcation.
2018. 2, 3

[12] N. Dvornik, J. Mairal, and C. Schmid. Modeling visual con-
text is key to augmenting object detection datasets. In ECCV,
2018. 1, 2, 3

[13] N. Dvornik, J. Mairal, and C. Schmid. On the importance of
visual context for data augmentation in scene understanding.
arXiv:1809.02492, 2018. 8, 9, 12

[14] N. Dvornik, K. Shmelkov, J. Mairal, and C. Schmid.
Blitznet: A real-time deep network for scene understanding.
In ICCV, 2017. 6, 7, 8, 9, 10, 12, 14

[15] D. Dwibedi, I. Misra, and M. Hebert. Cut, paste and learn:
Surprisingly easy synthesis for instance detection. In ICCV,
2017. 1, 2, 3, 4

[16] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes (voc) chal-
lenge. IJCV, 2010. 9

[17] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg. DSSD:
arXiv:1701.06659,

Deconvolutional single shot detector.
2017. 2

[18] G. Georgakis, A. Mousavian, A. C. Berg, and J. Kosecka.
Synthesizing training data for object detection in indoor
scenes. arXiv:1702.07836, 2017. 1, 2, 3

[19] G. Gkioxari, R. Girshick, P. Doll´ar, and K. He. Detecting
and recognizing human-object interactions. In CVPR, 2018.
1

[20] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In NIPS, 2014. 2

[21] A. Gupta, A. Vedaldi, and A. Zisserman. Synthetic data for

text localisation in natural images. In CVPR, 2016. 1, 2, 3

[22] H. He and E. A. Garcia. Learning from imbalanced data.

IEEE T-K&DE, (9):1263–1284, 2008. 2, 3

[23] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask R-

CNN. In ICCV, 2017. 2, 6, 7, 8, 9

[24] K. He, G. Gkioxari, P. Doll´ar, and R. B. Girshick. Mask

R-CNN. IEEE T-PAMI, 2018. 7

[25] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 2, 5, 12

[26] C. Huang, Y. Li, C. Change Loy, and X. Tang. Learning deep
representation for imbalanced classiﬁcation. In CVPR, 2016.
2, 3

[27] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger.
Densely connected convolutional networks. In CVPR, 2017.
1

[28] H. Inoue. Data augmentation by pairing samples for images

classiﬁcation. arXiv:1801.02929, 2018. 2

[29] S. H. Khan, M. Hayat, M. Bennamoun, F. A. Sohel, and
R. Togneri. Cost-sensitive learning of deep feature represen-
tations from imbalanced data. IEEE T-NNLS, 29(8), 2018. 2,
3

[30] S. Kumra and C. Kanan. Robotic grasp detection using deep

convolutional neural networks. In IROS, 2017. 1

[31] J.-F. Lalonde and A. A. Efros. Using color compatibility for

assessing image realism. In ICCV, 2007. 4

[32] D. Lee, S. Liu, J. Gu, M.-Y. Liu, M.-H. Yang, and J. Kautz.
Context-aware synthesis and placement of object instances.
In NeurIPS, 2018. 1, 2, 3

[33] T.-Y. Lin, P. Doll´ar, R. B. Girshick, K. He, B. Hariharan, and
S. J. Belongie. Feature pyramid networks for object detec-
tion. In CVPR, 2017. 6, 7, 8, 9

[34] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar. Focal

loss for dense object detection. In ICCV, 2017. 3

[35] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft COCO: Com-
mon objects in context. In ECCV, 2014. 2, 3, 4, 5

[36] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.

Fu, and A. C. Berg. SSD: Single shot multibox detector. In
ECCV, 2016. 1, 2

[37] R. Ranjan, V. M. Patel, and R. Chellappa. Hyperface: A deep
multi-task learning framework for face detection, landmark
localization, pose estimation, and gender recognition. IEEE
T-PAMI, 41(1), 2019. 1

[38] T. Remez, J. Huang, and M. Brown. Learning to segment via

cut-and-paste. In ECCV, 2018. 3

[39] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-
wards real-time object detection with region proposal net-
works. In NIPS, 2015. 1, 2, 5, 6, 7, 9, 10, 12, 13

[40] N. Saraﬁanos, X. Xu, and I. A. Kakadiaris. Deep imbalanced
attribute classiﬁcation using visual attention aggregation. In
ECCV, 2018. 3

[41] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang,
and R. Webb. Learning from simulated and unsupervised
images through adversarial training. In CVPR, 2017. 3

[42] K. Simonyan and A. Zisserman.

Very deep con-
large-scale image recognition.

volutional networks for
arXiv:1409.1556, 2014. 1

[43] B. Singh, M. Najibi, and L. S. Davis. SNIPER: Efﬁcient

multi-scale training. In NeurIPS, 2018. 1, 2, 6, 7, 8, 12, 14

[44] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI, 2017. 1

[45] R. Takahashi, T. Matsubara, and K. Uehara. Data augmen-
tation using random image cropping and patching for deep
cnns. arXiv:1811.09030, 2018. 2

[46] A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu.

Pixel recurrent neural networks. In ICML, 2016. 2

[47] X. Wang, A. Shrivastava, and A. Gupta. A-Fast-RCNN: Hard
positive generation via adversary for object detection.
In
CVPR, 2017. 2

[48] Y. Wang, W. Gan, W. Wu, and J. Yan.

Dynamic
imbalanced data classiﬁcation.

curriculum learning for
arXiv:1901.06783, 2019. 2, 3

[49] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz.
ICLR, 2017.

mixup: Beyond empirical risk minimization.
2

[50] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang. Random

erasing data augmentation. arXiv:1708.04896, 2017. 1, 2

[51] J.-Y. Zhu, P. Krahenbuhl, E. Shechtman, and A. A. Efros.
Learning a discriminative model for the perception of real-
ism in composite images. In ICCV, 2015. 4

6. Supplementary Material

In this supplementary ﬁle, we provide more qualitative
and quantitative analysis for our proposed PSIS. Speciﬁ-
cally, we further verify the effect of progressive instance-
switching, and provide more examples of synthetic images
generated by our IS strategy. Moreover, we evaluate the
effect of backbone models with various depths. Finally,
we assess the effects of training settings (i.e., batch-size,
epochs and negative chip mining) on BlitzNet [14] and
SNIPER [43].

6.1. Illustration of Examples of Synthetic Images

A,I c

A, ic

cad, our IS strategy switches the instances ic

Switching and Annotation for Attaching Instances.
Given a quadruple {I c
B, ic
B} in the candidate set
Ωc
A, ic
B to obtain
a new synthetic image pair { ˆI c
B}. However, as shown
in Figure 6, some instances (e.g., person ic∗
∗ ) may be at-
tached on a switched instance (e.g., bed ic
B). To preserve
contextual coherence, our IS also switches and annotates
the attaching instances during generation of the synthetic
images.

A, ˆI c

Comparison with Context-DA [13]. We also compare
the synthetic images generated by Context-DA and our
instance-switching strategy. The synthetic images gener-
ated by Context-DA are copied from the original paper [13],
and we pick the synthetic images generated by our instance-
switching strategy, sharing the similar scenes with ones of
Context-DA. The image examples are illustrated in Fig-
ure 7, where we can see that our instance-switching strat-
egy can better preserve contextual coherence in the original
images in comparison to Context-DA. Meanwhile, the syn-
thetic images generated by our instance-switching strategy
have better visual authenticity.

Examples of Synthetic Images Generated by Our IS
Here we show some examples of synthetic images gener-
ated by our IS strategy. As illustrated in Figure 10, the
new (switched) instances are denoted in red boxes, and our
instance-switching strategy can clearly preserve contextual
coherence in the original images.

6.2. Further Analysis of Gains for Each Class

Progressive
Instance-Switching. Our
progressive
instance-switching mainly focuses on the classes with
lowest APs. To further verify its effect, we conduct exper-
iments using Faster R-CNN (ResNet-101) on MS COCO
dataset, where the settings follow Section 4.1. Figure 8
(a) and (b) show AP and gain ratios of each class before
and after progressive instance-switching, respectively. The
gain ratios are computed by (APf −APb)/APb, where APb

and APf are AP of each class before and after progressive
instance-switching. The results clearly demonstrate our
progressive instance-switching brings much larger gain
ratios for the classes with lowest APs. Meanwhile, it does
not bring the side effect on the classes with highest APs,
and even slightly improves their performance.

Progressive and Selective Instance-Switching. To ver-
ify the effect of PSIS, we further analysis the gain AP for
each class brought by PSIS using Faster R-CNN (ResNet-
101) on MS COCO dataset. Figure 9 (a) and (b) show AP of
each class on Ωori (27.3%) and gains of each class brought
by ΩP SIS (29.7%), respectively. The gains are computed
by APP SIS−APori, where APP SIS and APori are AP of
each class for ΩP SIS and Ωori. The results clearly indicate
our PSIS brings improvement for each class.

6.3. Effect of Backbone Models with Various Depths

In this part, we evaluate the effect of backbone models
on our PSIS method. To this end, we conduct experiments
using Faster R-CNN [39] with three networks of differ-
ent depths, including ResNet-50 [25], ResNet-101 [25] and
ResNet-152 [25]. Faster R-CNN with different backbone
models are trained on two datasets (i.e., Ωori and ΩP SIS),
and results are reported on the validation set for comparison.
We employ the same settings in Section 4.1 (ResNet-101
as backbone) to train Faster R-CNN with ResNet-50 and
ResNet-152. The results are summarized in Table 9, where
our ΩP SIS achieves 1.5%, 2.4% and 2.3% gains in terms
of IoU = [0.5 : 0.95] over the original Ωori under ResNet-
50, ResNet-101 and ResNet-152, respectively. Above re-
sults demonstrate that our PSIS can improve detection per-
formance under backbone models with various depths. It
is worth mentioning that Faster R-CNN with ResNet-50
trained on our ΩP SIS is superior to one with ResNet-101
trained on Ωori. Meanwhile, Faster R-CNN + ResNet-101
+ ΩP SIS outperforms Faster R-CNN + ResNet-152 + Ωori.
They show again the effectiveness of our PSIS under back-
bone models with various depths.

6.4. Effects of Training Settings on BlitzNet and

SNIPER

Half of Batch-size and 2 Times Epochs. Due to the lim-
ited computing resources, we use half of batch-size and
double training epochs in the original settings for train-
ing both BlitzNet [14] and SNIPER [43]. Here we eval-
uate the effect of such training settings on BlitzNet and
SNIPER. To this end, we use our training settings (i.e.,
half of batch-size and double training epochs) to re-train
BlitzNet and SNIPER on the original training sets of MS
COCO 2014 and MS COCO 2017, respectively. The cor-
responding method is indicted by Ω†
ori. As compared in
Tables 10 and 11, we can see that Ω†
ori and Ωori achieve

Table 9. Results (%) of Faster R-CNN on validation set of MS COCO 2017 under different depth networks (e.g., ResNet-50, ResNet-101
and ResNet-152).

Training sets

Detector

Ωori
ΩP SIS
Ωori
ΩP SIS
Ωori
ΩP SIS

Faster R-CNN
(ResNet-50)[39]
Faster R-CNN
(ResNet-101)[39]
Faster R-CNN
(ResNet-152)[39]

Avg.Precision, IOU:

Avg.Precision, Area:

0.5:0.95
26.2
27.7
27.3
29.7
28.0
30.3

0.50
47.4
49.1
48.6
50.6
49.0
51.7

0.75
26.8
28.7
27.5
30.5
28.8
31.4

Small Med.
29.7
31.5
30.4
33.4
31.3
33.8

8.3
9.2
8.8
10.3
9.2
10.5

Large
41.8
44.2
43.1
48.0
44.9
48.5

Avg.Recall, #Det:
100
1
10
39.0
37.9
25.5
40.5
39.3
26.5
39.3
38.5
25.8
41.6
40.5
27.6
40.7
39.7
26.6
42.2
41.0
27.8

Avg.Recall, Area:

Small Med.
45.0
16.0
46.7
17.7
44.5
16.3
47.6
18.6
46.9
18.0
48.5
18.9

Large
59.5
61.6
60.2
64.6
62.9
64.8

Figure 6. Switching and annotation for attaching instances.

Figure 7. Comparison of synthetic images generated by (a) Context-DA and (b) our instance-switching strategy. The new instances are
denoted in red boxes. Compared with Context-DA, our instance-switching strategy can better preserve contextual coherence in the original
images and has better visual authenticity.

Figure 8. Results of Faster R-CNN (ResNet-101) on MS COCO dataset. (a) shows AP of each class before progressive instance-switching
in descending order; (b) gives gain ratios of each class brought by progressive instance-switching.

Table 10. Results (%) of SNIPER (ResNet-101) with different training conﬁgurations on test-dev set of MS COCO 2017. The results of
Ωori are copied from [43].

Training sets

Detector

Ω†
ori
Ωori
ΩP SIS
Ω†
ori
Ωori
ΩP SIS

SNIPER
(ResNet-101)[43]
W/o neg
SNIPER
(ResNet-101)[43]
with neg

Avg.Precision, IOU:

Avg.Precision, Area:

0.5:0.95
43.2
43.4
44.2
46.4
46.5
47.1

0.50
62.5
62.8
63.5
67.2
67.5
68.5

0.75
48.7
48.8
49.3
52.3
52.2
52.8

Small Med.
45.6
28.3
45.2
27.4
46.2
29.3
49.5
30.7
49.4
30.0
50.2
32.1

Large
56.7
56.2
57.1
58.8
58.4
59.0

Avg.Recall, #Det:
100
1
10
65.3
59.4
34.7
N/A
N/A
N/A
65.9
60.1
35.0
68.2
62.1
36.9
N/A
N/A
N/A
69.0
63.2
37.4

Avg.Recall, Area:

Small Med.
70.0
49.5
N/A
N/A
70.4
50.4
73.3
52.1
N/A
N/A
73.9
53.1

Large
77.1
N/A
78.0
82.0
N/A
82.6

Figure 9. Results of Faster R-CNN (ResNet-101) on MS COCO dataset. (a) shows AP of each class on Ωori.; (b) gives gains of each class
brought by ΩP SIS compared with Ωori.

very comparable results under all evaluation metrics, clearly
showing our modiﬁed training settings have little effect on
performance of BlitzNet and SNIPER. Therefore, we can
owe the improvement of ΩP SIS over Ωori to the effective-
ness of our PSIS method.

Negative Chip Mining for SNIPER. Besides, we imple-
ment SNIPER [43] on our ΩP SIS with negative chip min-
ing, which is very time-consuming due to our limited com-
puting resources. As listed in Table 10, SNIPER with our
training settings (i.e., half of batch-size and double training
epochs) trained on the original dataset Ωori achieves very
comparable results with the ones reported in the original pa-
per [43]. When negative chip mining is employed, SNIPER
trained on our ΩP SIS respectively achieves 0.7% and 0.6%
gain over one with Ω†
ori and Ωori, achieving state-of-the-
art performance on MS COCO 2017 dataset. Above results
demonstrate our proposed PSIS is also complementary to
negative chip mining strategy.

Table 11. Results (%) of BlitzNet (ResNet-50) with different train-
ing conﬁgurations on MS COCO 2014. The results of Ωori are
copied from [14].

Training set

Ω†
ori
Ωori
ΩP SIS

Avg.Precision, IOU:

Avg.Precision, Area:

0.5:0.95
27.2
27.3
30.8

0.50
45.8
46.0
50.0

0.75
27.8
28.1
32.2

Small Med.
26.9
10.2
26.8
10.7
31.0
12.6

Large
44.9
46.0
50.2

Figure 10. Some examples of synthetic images generated by our IS. The new instances are denoted in red boxes.

