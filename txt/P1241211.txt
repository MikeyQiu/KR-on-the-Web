ArduCode: Predictive Framework for Automation Engineering

Arquimedes Canedo1* Palash Goyal2∗ Di Huang2∗ Amit Pandey1
1Siemens Corporate Technology
2USC Information Sciences Institute

9
1
0
2
 
p
e
S
 
1
1
 
 
]
E
S
.
s
c
[
 
 
2
v
3
0
5
4
0
.
9
0
9
1
:
v
i
X
r
a

Abstract

Automation engineering is the task of integrating, via soft-
ware, various sensors, actuators, and controls for automating
a real-world process. Today, automation engineering is sup-
ported by a suite of software tools including integrated devel-
opment environments (IDE), hardware conﬁgurators, compil-
ers, and runtimes. These tools focus on the automation code
itself, but leave the automation engineer unassisted in their
decision making. This can lead to increased time for software
development because of imperfections in decision making
leading to multiple iterations between software and hardware.
To address this, this paper deﬁnes multiple challenges often
faced in automation engineering and propose solutions using
machine learning to assist engineers tackle such challenges.
We show that machine learning can be leveraged to assist the
automation engineer in classifying automation, ﬁnding simi-
lar code snippets, and reasoning about the hardware selection
of sensors and actuators. We validate our architecture on two
real datasets consisting of 2,927 Arduino projects, and 683
Programmable Logic Controller (PLC) projects. Our results
show that paragraph embedding techniques can be utilized to
classify automation using code snippets with precision close
to human annotation, giving an F1-score of 72%. Further, we
show that such embedding techniques can help us ﬁnd similar
code snippets with high accuracy. Finally, we use autoencoder
models for hardware recommendation and achieve a p@3 of
0.79 and p@5 of 0.95.

1

Introduction

Industrial automation is undergoing a technological revolu-
tion of smart production enabled by recent breakthroughs
in intelligent robotics, sensors, big data, advanced materials,
edge supercomputing, internet of things, cyber-physical sys-
tems, and artiﬁcial intelligence. These systems are currently
being integrated by software into factories, power grids,
transportation systems, buildings, homes, and consumer de-
vices. The lifecycle of industrial automation systems is di-
vided into two phases: engineering and runtime. Engineer-
ing refers to all activities that occur before the system is in
operation. These engineering activities include hardware se-
lection, hardware conﬁguration, automation code develop-
ment, testing, and simulation. Runtime, on the other hand,

*These authors contributed equally to this work.

Copyright © 2020, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All
rights reserved.

refers to all activities that occur during the system’s oper-
ation. These runtime activities include control, signal pro-
cessing, monitoring, prognostics, etc.

Applications of AI in industrial automation have been fo-
cused mainly on the runtime phase due to the availability of
large volumes of data from sensors. For example, time se-
ries forecasting algorithms have been very successful in sig-
nal processing. Planning and constraint satisfaction are used
in controls and control code generation. Anomaly detection
algorithms are becoming very popular in cyber-attack mon-
itoring. Probabilistic graphical models and neural networks
for prognostics and health management of complex cyber-
physical systems such as wind and gas turbines.

The use of AI in the engineering phase, on the other hand,
has remained relatively unexplored. There may be several
reasons for this. First, engineering data is very scarce be-
cause of its proprietary nature. Second, the duration of the
engineering phase is short compared to the runtime phase;
some industrial automation systems are in operation for
more than 30 years. Therefore, the engineering phase is of-
ten considered less important than the runtime phase. Third,
acquiring human intent and knowledge is difﬁcult. Cap-
turing engineering know-how in expert systems has been
shown to be time consuming and expensive.

There are three tasks that can greatly beneﬁt the automa-

tion engineering of the future:

1. Code classiﬁcation. As production demands change
rapidly, there is a need to efﬁciently integrate new func-
tionality into production. Code classiﬁcation can help or-
ganize existing and new code into functional libraries with
code functions for different categories: signal processing,
signal generation, robot motion control, etc.

2. Semantic code search. Frequent reconﬁgurations of the
production system demand a much higher degree of code
reusability. Semantic code search can help the produc-
tivity of engineers by allowing them to ﬁnd functionally
equivalent code. Similar code can inform their decision
making when writing software to automate hardware they
never experienced before.

3. Hardware recommendation. Automation engineering is
the task of integrating various hardware components in
software to achieve a production goal. Therefore, select-
ing good hardware conﬁgurations is a critical activity.

Hardware recommendation can assist the engineers with
hardware conﬁguration auto-completion. That is, given a
partial hardware conﬁguration, predict the full hardware
conﬁguration.

1.1 Methodology and Contributions
This paper introduces the use of machine learning methods
in AES to address the three tasks listed above. First, we
demonstrate code classiﬁcation on two real AES datasets.
We learn representation of AES code via document embed-
ding methods, using different artifacts such as function calls,
includes, comments, tags, and the code itself. Then we train
classiﬁers on code embeddings to categorize code projects.
Our results show that our approach captures code struc-
ture and it is comparable to human annotation prediction
performance. Second, using the resulting code embeddings,
we demonstrate a semantic code search capability for AES
code capable of ﬁnding functionally equivalent fragments of
code. Third, we develop a hardware recommendation sys-
tem to auto-complete partial hardware conﬁgurations. Our
results show a 3× higher precision than the baselines. The
original contributions of this paper are as follows:

• The introduction of three AES tasks where AI has a
big impact potential: code classiﬁcation, semantic code
search, and hardware recommendation.

• An unsupervised learning AES code embedding approach
based on bag of words suitable for code classiﬁcation and
semantic code search.

• The comparison of two hardware recommendation ap-
proaches using Bayesian Newtorks and Autoencoders.

• The evaluation of our AI models in two real AES
datasets consisting of 2,927 Arduino projects, and 683
Programmable Logic Controller (PLC) programs.

• The ArduCode reference implementation in Python, and
datasets for advancing the AI research in automation con-
sisting of: (i) AES source code and meta-data; (ii) an ex-
pert evaluation of code structural and syntactic similarity
for 50 code snippets; (iii) a manually curated silver stan-
dard for hardware recommendation systems with two lev-
els of granularity.

2 Related Work
To the best of our knowledge, we are the ﬁrst to investigate
the use of machine learning in AES. Recently, however, the
use of machine learning to support general purpose software
engineering is becoming an active area of research.

Broadly, recent advances in code learning can be divided
into two categories (Chen and Monperrus 2019): (i) lan-
guage speciﬁc models, and (ii) language independent mod-
els. Language speciﬁc models use knowledge of the lan-
guages used in the code to generate low-dimensional repre-
sentations. For example, code2vec (Alon et al. 2019) con-
structs abstract syntax tree from the code for Java lan-
guage for the purpose of predicting a method’s name from
its content. It deconstructs the tree into several paths and
learns code embedding by aggregating the representations

of these paths. func2vec (DeFreez, Thakur, and Rubio-
Gonz´alez 2018) uses control ﬂow graphs to generate em-
beddings of functions in C language. They utilize such rep-
resentations to detect function clones. Similarly, Deepre-
pair (White et al. 2019) use a combination of word2vec on
tokens and recursive encoder on abstract syntax tree for Java
token embedding. They use the representation to automati-
cally repair programs with bugs. Several other works, such
as DeepFix (Gupta et al. 2017), use language speciﬁc code
learning to identify bugs and programing errors in codes.
DeepTyper (Hellendoorn et al. 2018) uses recurrent neural
networks to perform type inference in dynamically typed
languages such as Javascript and Python.

Language independent models focus on syntactic repre-
sentation learning. For example, (Harer et al. 2018) utilize
word2vec directly on tokens from code to learn their rep-
resentations. They show that their model can help predict
software vulnerabilities. (Chen and Monperrus 2018) uti-
lize a similar approach for the task of automated program
repair. (Allamanis et al. 2015) introduce a syntactic model
based on logbilinear contexts to generate new method names
using these embeddings. Such models which do not use lan-
guage syntax to learn code representations are less widely
used compared to language speciﬁc models and often do not
perform as well. However, in this paper, we show that our
proposed language independent model achieves high accu-
racy in automation engineering tasks.

3 ArduCode: Automation Engineering

Software Learning

In this section, we introduce our predictive framework for
three key tasks in AES (see Figure 1): (i) code classiﬁca-
tion, (ii) semantic code search, and (iii) hardware recom-
mendation.

Figure 1: AES code learning architecture.

3.1 Data Collection
To validate our approach, we collected two real datasets rep-
resentative of AES.

Arduino Code We collected the source code and textual
metadata from 2,927 Arduino projects from the Arduino
Project Hub 1. The textual metadata consists of the project’s

1 https://create.arduino.cc/projecthub

category, title, abstract, tags, description, and hardware con-
ﬁguration. Each project is labeled by one category. In total,
there are 12 categories as shown in Figure 2. We use these
categories as the labels to predict in the code classiﬁcation
task. Makers are well known for helping and fostering col-
laboration in the DIY community. The documentation asso-
ciated to the Arduino projects is extensive. Therefore, the
project’s title, abstract, tags, and description metadata pro-
vide a upper bound baseline for label classiﬁcation using
human annotations.

The hardware conﬁguration is a list of components re-
quired to build the project. In the 2,927 projects, there are
6,500 unique components. After manual inspection, we ob-
served that different authors name the same component dif-
ferently; e.g., “resistor 10k” and “resistor 10k ohm”. To
clean the data, we manually curated the hardware conﬁgu-
ration lists and renamed the 6,500 components according to
their functionality. We deﬁned functional levels of abstrac-
tion for the hardware. The level-1 functionality consists of 9
categories: Actuators, Arduino, Communications, Electron-
ics, Human Machine Interface, Materials, Memory, Power,
Sensors. The level-2 functionality further reﬁnes the level-1
into a total of 45 categories (e.g., Actuators.{acoustic, air,
ﬂow, motor}).

Figure 2: Hand-curated Level-1 Arduino Project Categories.

PLC Code The OSCAT library http://www.oscat.
de/ is the largest publicly available library of PLC pro-
grams. The OSCAT-LIB is vendor independent and it pro-
vides reusable code functions in different categories such
as signal processing (SIGPRO), geometry calculations (GE-
OMETRY), and string manipulation (STRINGS). These cat-
egories are extracted from the comment’s section of each ﬁle
marked by the line “FAMILY: X”, where “X” is the cate-
gory associated to that function. This line is eliminated from
the dataset during training. In total, the OSCAT-LIB Basic
version 3.21 contains 683 functions and 28 category labels.
The OSCAT-LIB does not contain hardware conﬁguration,
and therefore it is only suitable for the tasks of code classi-
ﬁcation and semantic code search. All the code is written in
SCL language.

3.2 Code Classiﬁcation
Given a code snippet, the task of code classiﬁcation is to pre-
dict its label. Our pipeline consists of four steps: preprocess-

ing, feature selection, code embeddings, and classiﬁcation.

Preprocessing We preprocess the automation projects and
code snippets to expose the various features shown in Ta-
ble 1. The Arduino dataset contains more features than the
PLC dataset. Therefore, not all features are available in the
PLC dataset. For example, PLC code does not have includes,
and project data such as tags, title, descriptions, and compo-
nents is not available.

Arduino PLC Description

Feature
Includes
Functions
Comments
Tokens
Code
LOC
Tags
Title
Descriptions
Labels
Components

–

–
–
–

–

C/C++ includes
Function names
Comments in code
All code tokens
Code keywords
Lines of code
Project tags
Project title
Project descriptions
Labels to predict
Hardware conﬁguration

Table 1: Features exposed by preprocessing.

Feature Selection The purpose of feature selection is to
provide the ArduCode framework with a feature space ex-
ploration mechanism to compare the performance of differ-
ent code representations in the task of code classiﬁcation and
semantic code search. The quality of the code embeddings is
expected to vary according to the provided features. There-
fore, the feature selection generates different experiments
by combining different sets of features. For example, code
can be represented by different combinations of includes,
functions, comments, tokens, keywords. Code documenta-
tion can be represented by combinations of tags, titles, and
descriptions. Or both code representations and code docu-
mentation features can be combined.

Code Embeddings The next step is to embed the tex-
tual representations generated by the feature selection. We
compare the performance of the embeddings generated by
gensim doc2vec (Le and Mikolov 2014) with the embed-
dings generated by the term frequencyinverse document fre-
quency (tf-idf). The doc2vec’s hyperparameters of interest
are the embedding dimension, and the training algorithm
(distributed memory and distributed bag of words). We run
all our experiments with negative sampling of 5.

Classiﬁcation The ﬁnal step is to train a supervised model
for code classiﬁcation using the code embeddings as the in-
put samples, and the code labels as the target values. We
compare the performance of logistic regression and random
forest classiﬁers using the F1-score metric.

Results First, we established the lower and upper bounds
for code label classiﬁcation. The lower bound is given by
training the code label classiﬁer using random embeddings.
The upper bound is given by training the code label classiﬁer

using human annotations. The Arduino dataset provides hu-
man annotations in the form of tags and descriptions that can
be combined in three conﬁgurations: tags, descriptions, and
descriptions+tags. We ﬁrst embed these three conﬁgurations
using tf-idf and doc2vec, and compare the label classiﬁca-
tion performance using the F1-score. As shown in Figure 3
doc2vec yields a better performance than tf-idf. The embed-
ding dimension for doc2vec was set to 50, and the tf-idf
models generated embedding dimensions of 1,469 for tags,
66,310 for descriptions, and 66,634 for descriptions+tags.
Intuitively, the descriptions+tags conﬁguration provides the
upper bound F1-score of 0.8213.

Figure 3: Human annotation prediction performance.

We establish the lower bound by generating 50-
dimensional random embeddings and predicting the labels
using the LR classiﬁer. Figure 4 shows that with both tf-idf
and doc2vec the lower bound is 0.3538. After establishing
the upper and lower bounds, we use different code features
to predict labels. Figure 4 shows that embedding includes
and functions provide a slightly better performance than the
random baseline due to the very limited amount of informa-
tion contained in these: 1.82 includes and 4.70 functions on
average. Other code features improve the classiﬁcation accu-
racy signiﬁcantly. For example, tokens and code are similar
representations and give a similar F1-score of 0.63 and 0.67.
These results also show that comments contain valuable in-
formation that can be used to predict the code label with a
score of 0.67. Embedding code+comments and code+titles
yield the highest F1-scores of 0.71. These results show that
the prediction performance with code feature embeddings is
comparable to human annotation embeddings with improve-
ments of 2.03× and 2.32× over the random baseline, respec-
tively.

Figure 4: Code label prediction using code.

Since the PLC dataset does not have any human annota-
tion features, we can only compare the performance of code
feature embeddings (against the random baseline. The F1-
score for the code embeddings is 0.9024 and for the random
baseline is 0.2878; a 3.13× improvement. Compared to the
Arduino dataset, the PLC dataset has less samples (683 vs
2,927), more category labels (28 vs 12), and less lines of
code per ﬁle on average (55 vs 177). These are three factors
that inﬂuence the higher prediction accuracy of ArduCode
on the PLC dataset.

3.3 Semantic Code Search
Given a code snippet, the goal of semantic code search is
to ﬁnd similar programs. For automation engineering, simi-
larity is deﬁned in terms of syntax and functionality. Syntax
similarity helps engineers ﬁnd useful functions in a given
context, and functional similarity informs engineers on how
other automation solutions have been engineered.

Doc2vec attempts to bring similar documents close to
each other in the embedding space. For a given code embed-
ding of a code snippet, the nearest neighbors are expected
to be similar and are used as the basis of our semantic code
search. While this approach is intuitive for syntactically sim-
ilar documents, it is unclear whether functional structure is
captured in the embeddings.

Results To validate the quality of our code embeddings,
we randomly sampled 50 Arduino code snippets, and tasked
a group of 6 software engineers to score the similarity of
each code snippet to its top-3 nearest neighbors. For every
code snippet pair, two similarity ratings for code syntax and
code structure are given. A rating of 1 represents similarity,
and a rating of 0 represents the lack of similarity. Code syn-
tax refers to the use of similar variables and function names.
Code structure refers to the use of similar code arrangements
such as if-then-else and for loops. In addition, every rating
has an associated expert’s conﬁdence score from 1 (lowest
conﬁdence) to 5 (highest conﬁdence) that represent the ex-
pert’s self-assurance during the evaluation. During the ex-
pert evaluation we eliminated 5 out of 50 samples where one
of the top-3 nearest neighbors was either an empty ﬁle, or
it contained code in a different programming language. We
found three code snippets written in Python and Javascript.
Table 2 shows the average code syntax and code struc-
ture similarity scores given by experts. We only report the
high conﬁdence ratings (avg. conﬁdence >= 4.5) in or-
der to eliminate the inﬂuence of uncertain answers. We also
measure the experts’ agreement via the Fleiss Kappa. These
results show that the similarity scores for both syntax and
structure are high for the top-1 neighbors (0.68 and 0.61
respectively) but reduce signiﬁcantly (under 0.50) for the
top-2 and top-3 neighbors. The experts are in substantial
agreement (0.61 <= κ <= 0.80) in their syntax similarity
scores, and in moderate agreement (0.41 <= κ <= 0.60 in
their structure similarity scores. While these results conﬁrm
that doc2vec code embeddings capture syntactic similarity,
they also show that some structure similarity is captured in
the top-1 neighbor.

To further gain insight into our experiment, we selected

Nearest neighbors

Similarity Top 1 (κ)
Syntax
Structure

0.61 (0.75)
0.68 (0.53)

Top 2 (κ)
0.48 (0.70)
0.40 (0.44)

Top 3 (κ)
0.32 (0.61)
0.33 (0.66)

perform the same task of creating a heatmap for a sensor
value using LEDs. While there are some syntactic similari-
ties, or semantic code search is also able to capture semantic
and functional similarities.

Table 2: Average code structure and code syntax similarity
and Fleiss Kappa values for high conﬁdence raters.

four similar and three not similar code snippets, and mea-
sured the cosine similarity of their embeddings as shown in
Table 3. The selected code snippets have a strong agreement
among the experts, and a high conﬁdence in the similarity
and lack of similarity across the top-3 nearest neighbors.
These results conﬁrm that the code snippets considered very
similar by the experts are close to each other in the embed-
ding space. On the other hand, code snippets considered not
similar are far apart in the embedding space.

Nearest neighbors

Top 1

Top 2

Top 3

Similar code snippets

#2696
#547
#2815
#54

0.8768
0.8719
0.9465
0.8513

0.7527
0.8705
0.9445
0.8056

0.7642
0.8506
0.9126
0.7815

Not Similar code snippets

#4512
#4345
#1730

0.5967
0.5415
0.5970

0.5497
0.4175
0.5035

0.5643
0.5192
0.5511

Table 3: Code embedding cosine similarity for similar and
not similar code snippets.

Figure 5 shows two similar Arduino code snippets (#54
and #2689) produced by ArduCode. There are similarities
between these two programs at different levels. First, all
Arduino programs are required to have the setup() and
loop() functions to initialize the program, and to spec-
ify the control logic executed on every cycle. Syntacti-
cally, the two programs use the same standard functions:
pinMode() to conﬁgure the Arduino board pins where the
hardware connects as inputs or outputs; analogRead()
to read an analog value from a pin; Serial.print() to
print an ASCII character via the serial port; delay() to
pause the program for the amount of time (in ms) speciﬁed
by the parameter; and analogWrite() to write an analog
value to a pin. Semantically, the two programs read sensor
values (only 1 value in #54 and 3 values in #2689), scale
the sensor value to a range (from 300-1024 to 0-255 using
map() in #54 and to (x+100)/4 in #2689), print the scaled
sensor value via the serial port, write the analog value to a
LED (a single LED in #54 and three LEDs in #2689), and
pause the program (10ms in #54 and 100ms in #2689). Note
that the order in which these operations are scheduled is dif-
ferent in the two programs. Functionally, the two programs

Figure 5: Arduino semantic code search result.

3.4 Hardware Recommendation

Given a partial list of hardware components, the task of the
hardware recommendation is to give a prediction of other
hardware components typically used in combination with
the partial list. The hand curated silver standard described
above is used to learn the joint probability distribution of
the hardware components. The hardware recommendation
task is then to compute the conditional probability of miss-
ing hardware components given a partial list of components.
We compare two approaches for the hardware recom-
mendation task. Our baseline consists of the predictions
given on random hardware conﬁgurations. First, we learn
a Bayesian network where the random variables are the
hardware component categories. We use the Pomegranate
Python package to learn the structure of the Bayesian net-
work and ﬁt the model with 70% of the hardware conﬁgu-
ration data. The Bayesian network for level-1 components
consists of 9 nodes and the one for level-2 components con-
sist of 45 nodes. However, we were only able to ﬁt the level-
1 Bayesian network as the initialization of the Bayesian net-
work takes exponential time with the number of variables.
Typically, this cannot be done with more than two dozen
variables due to the super-exponential time complexity with
respect to the number of variables, and the level-2 hardware
conﬁguration consists of 45 variables. To overcome this lim-
itation, we use an autoencoder implemented in Keras where
the encoder learns a lower dimensional representation of the
hardware conﬁguration data, and the decoder learns to re-
construct the original input from the lower dimensional rep-

resentation. To avoid overﬁtting, we use L1 and L2 regular-
izers.

Results
In hardware recommendation, we are interested
in recommending the top-k hardware components. There-
fore, we evaluate our models in terms of precision@k.
P recision@k is the portion of recommended hardware
components in the top-k set that are relevant. For each
hardware conﬁguration in the test data, we leave one hard-
ware component out, and measure its precision@k. Table 4
shows the results for the random baseline, the Bayesian net-
work, and the autoencoder. As expected, the performance
of the random baseline improves linearly from p@1 = 0.1,
p@3 = 0.32, and p@5 = 0.54 to p@9 = 1 for the level-1
hardware predictions. The Bayesian Network also improves
linearly from p@1 = 0.32, p@3 = 0.59, and p@5 = 0.79.
The autoencoder provides both the best performance and the
best improvements from p@1 = 0.36, p@3 = 0.79, and
p@5 = 0.95. Note that the autoencoder’s p@3 is the same
performance as the Bayesian Network’s p@5, 0.79. Further-
more, the autoencoder achieves > 0.95 precision at p@5,
and the Bayesian network at p@8.

p@k Random Bayesian Autoencoder

Baseline Network
0.10
0.32
0.54
1.00

0.32
0.59
0.79
1.00

p@1
p@3
p@5
p@9

0.36
0.79
0.95
1.00

Table 4: p@k results for level-1 hardware predictions.

Learning a Bayesian Network for level-2 hardware com-
ponents is computationally unfeasible. Therefore, we rely
on an autoencoder to accomplish this task and the p@k re-
sults are reported in Table 5. The overall p@k performance
of the autoencoder for level-2 is comparatively lower than
for level-1. The reason is that the level-2 hardware conﬁgu-
ration is sparser than level-1. On average, level-2 conﬁgura-
tions have 4/40 components and level-1 conﬁgurations have
4/12 components. However, the improvement over the ran-
dom baseline is of 10× for p@1, 5× for p@3, 4× for p@5,
and 3× for p@10.

p@k
p@1
p@3
p@5
p@10

Random Baseline Autoencoder
0.02
0.06
0.11
0.21

0.21
0.34
0.45
0.69

Table 5: p@k results for level-2 hardware predictions.

Acknowledgements

We thank Evan Patterson, Jade Master, Georg Muenzel, and
Gustavo Quiros for their valuable input and discussions.

4 Conclusion
In this paper, we introduced and studied three automation
engineering predictive tasks. First, we showed that our code
classiﬁcation approach based on doc2vec code embeddings
and logistic regression achieves an F1-scores of 72% and
90% on two real datasets. Second, a group of 6 experts val-
idated the semantic code search task by assessing the syn-
tax and structure similarity of 50 code snippets. Third, we
demonstrated a p@3 of 79% and p@5 of 95% for the hard-
ware recommendation task using an autoencoder.

Future research directions are as follows. Evaluate Ar-
duCode’s doc2vec approach against recent approaches such
as code2vec that are likely to better capture code struc-
ture and improve the code classiﬁcation and semantic code
search tasks. In addition, ArduCode’s hardware recommen-
dation is limited to hardware components. This task would
be even more useful if it incorporated software elements
such as library or API recommendations. One promising
idea is to model software elements as random variables
in the Bayesian Network and use expert know-how to de-
ﬁne their conditional probabilities and combine this expert
knowledge with data.

References
[Allamanis et al. 2015] Allamanis, M.; Barr, E. T.; Bird, C.;
and Sutton, C. 2015. Suggesting accurate method and class
names. In Proceedings of the 2015 10th Joint Meeting on
Foundations of Software Engineering, 38–49. ACM.
[Alon et al. 2019] Alon, U.; Zilberstein, M.; Levy, O.; and
Yahav, E. 2019. code2vec: Learning distributed represen-
tations of code. Proceedings of the ACM on Programming
Languages 3(POPL):40.
[Chen and Monperrus 2018] Chen, Z., and Monperrus, M.
2018. The remarkable role of similarity in redundancy-based
program repair. arXiv preprint arXiv:1811.05703.
[Chen and Monperrus 2019] Chen, Z., and Monperrus, M.
2019. A literature study of embeddings on source code.
arXiv preprint arXiv:1904.03061.
[DeFreez, Thakur, and Rubio-Gonz´alez 2018] DeFreez, D.;
Thakur, A. V.; and Rubio-Gonz´alez, C. 2018. Path-based
function embedding and its application to speciﬁcation min-
ing. arXiv preprint arXiv:1802.07779.
[Gupta et al. 2017] Gupta, R.; Pal, S.; Kanade, A.; and She-
vade, S. 2017. Deepﬁx: Fixing common c language errors
by deep learning. In AAAI Conference on Artiﬁcial Intelli-
gence, 1345–1351. AAAI.
[Harer et al. 2018] Harer, J. A.; Kim, L. Y.; Russell, R. L.;
Ozdemir, O.; Kosta, L. R.; Rangamani, A.; Hamilton, L. H.;
Centeno, G. I.; Key, J. R.; Ellingwood, P. M.; et al. 2018.
Automated software vulnerability detection with machine
learning. arXiv preprint arXiv:1803.04497.
[Hellendoorn et al. 2018] Hellendoorn, V. J.; Bird, C.; Barr,
E. T.; and Allamanis, M. 2018. Deep learning type infer-
ence. In ACM Joint European Software Engineering Con-
ference and Symposium on the Foundations of Software En-
gineering, 1345–1351.

[Le and Mikolov 2014] Le, Q., and Mikolov, T. 2014. Dis-
tributed representations of sentences and documents. In In-
ternational conference on machine learning, 1188–1196.
[White et al. 2019] White, M.; Tufano, M.; Martinez, M.;
Monperrus, M.; and Poshyvanyk, D. 2019. Sorting and
transforming program repair ingredients via deep learning
In 2019 IEEE 26th International Con-
code similarities.
ference on Software Analysis, Evolution and Reengineering
(SANER), 479–490. IEEE.

