9
1
0
2
 
t
c
O
 
1
 
 
]

G
L
.
s
c
[
 
 
1
v
8
2
5
0
0
.
0
1
9
1
:
v
i
X
r
a

Augmenting learning using symmetry in a
biologically-inspired domain

Shruti Mishra1,2, Abbas Abdolmaleki1, Arthur Guez1, Piotr Trochim1, Doina Precup1,3
1 DeepMind
2 Harvard University
3 McGill University

Abstract

Invariances to translation, rotation and other spatial transformations are a hallmark
of the laws of motion, and have widespread use in the natural sciences to reduce
the dimensionality of systems of equations e.g. [13, 3]. In supervised learning,
such as in image classiﬁcation tasks, rotation, translation and scale invariances
are used to augment training datasets, such as in [7]. In this work, we use data
augmentation in a similar way, exploiting symmetry in the quadruped domain of
the DeepMind control suite [12] to add to the trajectories experienced by the actor
in the actor-critic algorithm of Abdolmaleki et al. [1]. In a data-limited regime,
the agent using a set of experiences augmented through symmetry is able to learn
faster. Our approach can be used to inject knowledge of invariances in the domain
and task to augment learning in robots, and more generally, to speed up learning in
realistic robotics applications.

1

Introduction

Reinforcement learning (RL) agents are able to perform locomotion tasks using continuous actions
and observations in animal-like domains such as the planar walker, half-cheetah, and humanoid [8, 6].
While the agents learn policies that successfully maximise a cumulative return, the behaviours of
agents in such domains often appear idiosyncratic to humans in at least two ways. Most notably, the
locomotion behaviour in animals is associated with a natural rhythm and stereotypy, such as in the
case of a walking dog, galloping horse, or the tripod gait of an ant. In the absence of this spatial order
and temporal rhythm in simulated agents, the behaviours of the learned policies can look unnatural.

Idiosyncratic behaviour can arise in simulated agents because the physical simulation is incorrect, the
task does not capture the tasks executed by embodied agents, or the constraints on a simulated agent
are different from those on an embodied agent. The physical simulation itself is an inaccurate and
incomplete model of the physical world – it allows perfect ﬂoors, instantaneous, noise-free actuation,
and a windless environment. Embodied agents necessarily have behaviour policies that are robust to
deviations from any precise environment crafted for simulation. Additionally, the mechanical and
biological constraints that restrict the behaviour of an embodied agent, a robot or an animal, are
different from those experienced by simulated agents. Embodied agents are also necessarily affected
by notions of mechanical stress and fatigue. For example, an embodied walker would be less likely
to drag a leg, due to the notion of wear, or ﬂail around its arms, due to considerations of energy and
efﬁciency.

In this work, we leverage order in the spatial structure of the domain. Speciﬁcally, the vast majority
of animals have at least one plane of external symmetry, which is reﬂected in their stereotyped gaits
when they are walking or running. Similarly, for simulated bodies with nearly identical limbs, if the
agent knows how to move one limb relative to the body in a particular way, it can do the same for
the other similar limbs. We use symmetry as a natural inductive bias in order to reduce the space of

shrutimishra@g.harvard.edu

behaviour policies that can be achieved by agents, as well as to explore if this approach can boost the
learning process.

2 Related work

We try to use invariances in the physical domain/task to augment learning. Invariances to scale,
orientation, translation and relative motion are common to the natural world and a hallmark of
the dynamics of tangible objects and of continuous ﬁelds. The use of such invariances to reduce
the dimensionality of the equations governing a physical process is thus ubiquitous in the natural
sciences [3, 13]. In machine learning, supervised classiﬁcation tasks use rotation, symmetry and
scale invariances to augment training data to get improved performance [2, 11, 7]. In RL, symmetry
and rotation invariances have been used to augment data in AlphaGo [10]. In robotics and continuous
control applications, some ways to improve the real-world suitability and sample efﬁciency of RL
agents include curriculum learning [4], learning from motion-capture data [9], and using constraint-
based objective functions [5]. Our work follows the same theme of making the policies learned by
RL agents more realistic, and aiming to make the learning process efﬁcient.

3 Domain and tasks

To illustrate the use of symmetry, we can use
any animal-like domain with a saggital plane of
symmetry, which is a plane of symmetry com-
mon to several animals. We use the quadruped
domain from Tassa et al. (2018) [12]. The body
of the quadruped comprises a torso with four
legs. Each leg is identical, with three hinge
joints, as shown in Figure 1 (left). The joints
are controlled by torque actuators. We consider
the move tasks in Tassa et al. (2018) [12], in
which positive reward is given for an upright
orientation relative to the horizontal plane and a
forward velocity of the torso, relative to a speci-
ﬁed desired velocity, in the frame of reference of
the torso. Within the move tasks, the walk task
and run task are speciﬁed by different values of
desired velocity. Details of the reward function
are given in Tassa et al. (2018) [12]. The ob-
servations comprise joint angles and velocities,
forces and torques on the joints, the state of the actuators, and the torso velocity.

Figure 1: Left: Quadruped domain, showing the
three joints on one of the legs (light grey), and
the sagittal plane in green. Right: Top view of
a quadruped, showing a ﬁctitious adopted pose
in terms of leg orientations (blue), and its corre-
sponding pose (pink), mirrored about the axis of
symmetry. The co-ordinate axes correspond to the
local frame of reference of the torso.

This is illustrated in Figure 1 (right), by means of a pose with the legs shown in blue, and its
corresponding mirrored pose, with the legs shown in pink. In the move tasks, for every observation-
action pair that the agent encounters, there is a corresponding mirrored observation-action pair that
can be obtained by reﬂection about the plane of symmetry. The mirroring operation for observations
and actions are essentially matrix multiplications, and are obtained cheaply from the observations.
The mirrored corresponding observation-action pair will result in the same forward displacement,
and therefore the same reward. Therefore, for every trajectory executed by the agent, there is a
corresponding mirrored trajectory that results in the same sequence of rewards.

4 Symmetric policy

A symmetric gait such as a human walk is characterised by a phase difference between the legs, and
an antisymmetric gait such as that of a galloping horse is characterised by an in-phase synchrony
between the left and right halves of the body. When we use the term symmetric policy, we mean
that the probability π (amirrored, smirrored) = π(a, s), where smirrored and amirrored are obtained by
reﬂecting the state s and action a, respectively, about a plane of symmetry. By this deﬁnition, the
biological symmetric and antisymmetric gaits are both executed by a symmetric policy.

2

for i = 1,...,N do
ai ∼ πk(a|sj)
Qij = ˆQ(sj, ai)
qij ∝ exp(Qij/temperature parameter))

Sample states with batch of size N from replay buffer
Step 1: sample based policy (weights)
q(ai|sj) = qij, computed as:
for j = 1,...,K do

Algorithm 1 MPO with Mirrored Data
1: given batch-size (K), number of actions (N), Q-function ˆQ, old-policy πk and replay-buffer
2: initialize πθ from the parameters of π(k)
3: repeat
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20: until Fixed number of steps
21: return π(k+1)

end for
Calculate mirrored states and mirrored actions
Step 2: update parametric policy
Given the data-set {sj, (ai, qij)i=1...N }j=1...K and {smirrored
Update the Policy by taking gradient of following weighted maximum likelihood objective
maxθ((cid:80)K
(subject to additional (KL) regularization)

i qij log πθ(ai|sj) + (cid:80)K

i qij log πθ(amirrored

, qij)i=1...N }j=1...K

, (amirrored
i

|smirrored

end for

(cid:80)N

(cid:80)N

))

j

j

j

j

i

5 Proposed algorithm

We use MPO, an actor-critic algorithm described in Abdolmaleki et al. [1]. To encourage the agent
to take symmetries into account, we augment the batch of experiences generated by the actor and
stored in the replay buffer, by computing a set of corresponding mirrored trajectories for use by the
learner of the MPO algorithm [1]. The MPO algorithm has two steps, policy evaluation and policy
improvement. In this section, we summarise the algorithm and specify the modiﬁcations required to
use data from mirrored trajectories, with a pseudocode of our approach given in algorithm 1.

5.1 Policy Evaluation

We employ a 1-step temporal difference (TD) learning, ﬁtting a parametric Q-function Qπ
parameters φ by minimizing the squared (TD) error

φ(s, a) with

(cid:16)

minφ

rt + γQπ(k−1)

φ(cid:48)

(st+1, at+1) − Qπ(k)

φ

(st, at)

(cid:17)2

,

where at+1 ∼ π(k−1)(a|st+1) and rt = r(st, at), which we optimize via gradient descent. We let
φ(cid:48) be the parameters of a target network that is held constant for 250 steps (and then copied from
the optimized parameters φ). Using the fact that for a symmetric policy, Q (smirrored, amirrored) =
Q(s, a), we also ﬁt the parametric Q-function for Q (smirrored, amirrored),

(cid:16)

minφ

rt + γQπ(k−1)

φ(cid:48)

(st+1, at+1) − Qπ(k)

φ

(cid:0)smirrored

t

, amirrored

t

(cid:1)(cid:17)2

.

5.2 Policy Improvement

Step 1: We construct a non-parametric improved policy q. This is done by maximizing ¯J(s, q) =
Eq(a|s)[ ˆQ(s, a)] for states s drawn from a replay buffer R while ensuring that the solution stays close
to the current policy πk; i.e. Es∼R[KL(q(a|s), πk(a|s))] < (cid:15), as detailed in Abdolmaleki et al. [1].
Step 2: We ﬁt a new parametric policy to samples from q(a|s) by solving the optimization problem
Eµ(s)[KL(q(a|s)(cid:107)π(a|s)], where πk+1 is the new and improved policy, which
πk+1 = argminπ
employs additional regularization [1]. To learn about/from mirrored data, we repeat steps 1 and 2 for
mirrored states and mirrored actions calculated from original data.

3

Figure 2: Learning curves for 10 seeds of the walk (top row) and run (bottom row) tasks, for batch
sizes 256 (left column) and 512 (right column). The plot shows the cumulative reward as a function
of gradient steps. The normal training and augmented training are in blue and pink, respectively. The
thick line shows the mean.

6 Experiments

We refer to training with the MPO algorithm in [1] as “normal training” and training with the
additional experience from the mirrored data, as described in Section 3, as “augmented training”.
For the normal and augmented training conditions, we use the same hyper-parameters as in [1], with
one main modiﬁcation: we slowed down the actor to get a rate of trajectory generation of 1000 s of
environment steps every 30 s of real time, to bring it closer to the data-generation rate of real robots.
The learner thus operates in a data-limited regime. The resulting cumulative reward as a function
of gradient steps is shown in Figure 2. For each of the training conditions, the augmented training
condition achieves better performance, typically in fewer episodes. This preliminary result suggests
that knowledge of symmetry, enforced through data augmentation in the policy optimisation step, is a
useful bias for the agent to shape its policy.

7 Discussion

We presented an approach for incorporating symmetry in the environment in an actor-critic architec-
ture by augmenting the experiences generated by the actor. A different approach to inform an agent
of symmetry in its domain could, for instance, compress the state representation, explicitly specifying
the relationship between an observation-action pair and its mirrored version. This involves changing
the underlying Markov decision process so that transitions are between “canonical" observations. In
general, this means that transitions do not arise from a physical process and may require modiﬁcations
to the algorithm to ensure that no undesirable bias has been introduced. A promising extension of
this work would be to move towards a policy that explicitly encodes the symmetry using hierarchy,
with similar legs sharing their lower-level policy with the corresponding actuators of other legs, and
using experiences from the other legs to update their policies. Using symmetric policies leads to a
change in the space of policies that can be achieved and therefore qualitative change in gait. We
plan to investigate this further. In general, this work can also be useful for transfer learning to tasks

4

which are not symmetric; an agent may ﬁrst learn to walk using a knowledge of symmetry to be
data-efﬁcient, and then learn to navigate an external non-uniform landscape.

We would like to thank Ankit Anand, Eser Aygün, Tom Erez, Philippe Hamel, Yuval Tassa and Daniel
Toyama.

8 Acknowledgements

References

[1] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and
Martin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920,
2018.

[2] Henry S Baird. Document image defect models. In Structured Document Image Analysis, pages

546–556. Springer, 1992.

[3] Grigory Isaakovich Barenblatt. Scaling, self-similarity, and intermediate asymptotics: di-
mensional analysis and intermediate asymptotics, volume 14. Cambridge University Press,
1996.

[4] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.
In Proceedings of the 26th annual international conference on machine learning, pages 41–48.
ACM, 2009.

[5] Steven Bohez, Abbas Abdolmaleki, Michael Neunert, Jonas Buchli, Nicolas Heess, and Raia
Hadsell. Value constrained model-free continuous control. arXiv preprint arXiv:1902.04623,
2019.

[6] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan,
Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms
and applications. arXiv preprint arXiv:1812.05905, 2018.

[7] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems, pages
1097–1105, 2012.

[8] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval
Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning.
International Conference on Learning Representations, 2016.

[9] Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne,
Yee Whye Teh, and Nicolas Heess. Neural probabilistic motor primitives for humanoid control.
arXiv preprint arXiv:1811.11711, 2018.

[10] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al.
Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484,
2016.

[11] Patrice Y Simard, David Steinkraus, John C Platt, et al. Best practices for convolutional neural

networks applied to visual document analysis. In Icdar, volume 3, 2003.

[12] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David
Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin
Riedmiller. DeepMind control suite. https://arxiv.org/abs/1801.00690, January 2018.

[13] Kenneth G Wilson. Renormalization group and critical phenomena. i. renormalization group

and the kadanoff scaling picture. Physical review B, 4(9):3174, 1971.

5

