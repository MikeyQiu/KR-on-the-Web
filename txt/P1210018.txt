Scalable Hyperparameter Optimization
with Products of Gaussian Process Experts

Nicolas Schilling, Martin Wistuba and Lars Schmidt-Thieme

Information Systems and Machine Learning Lab
Universitätsplatz 1, 31141 Hildesheim, Germany
{schilling,wistuba,schmidt-thieme}@ismll.uni-hildesheim.de

Abstract. In machine learning, hyperparameter optimization is a chal-
lenging but necessary task that is usually approached in a computation-
ally expensive manner such as grid-search. Out of this reason, surrogate
based black-box optimization techniques such as sequential model-based
optimization have been proposed which allow for a faster hyperparameter
optimization. Recent research proposes to also integrate hyperparameter
performances on past data sets to allow for a faster and more eﬃcient
hyperparameter optimization.
In this paper, we use products of Gaussian process experts as surrogate
models for hyperparameter optimization. Naturally, Gaussian processes
are a decent choice as they oﬀer good prediction accuracy as well as es-
timations about their uncertainty. Additionally, their hyperparameters
can be tuned very eﬀectively. However, in the light of large meta data
sets, learning a single Gaussian process is not feasible as it involves in-
version of a large kernel matrix. This directly limits their usefulness for
hyperparameter optimization if large scale hyperparameter performances
on past data sets are given.
By using products of Gaussian process experts the scalability issues can
be circumvened, however, this usually comes with the price of having
less predictive accuracy. In our experiments, we show empirically that
products of experts nevertheless perform very well compared to a variety
of published surrogate models. Thus, we propose a surrogate model that
performs as well as the current state of the art, is scalable to large scale
meta knowledge, does not include hyperparameters itself and ﬁnally is
even very easy to parallelize.

Keywords: Hyperparameter Optimization, Sequential Model-Based Op-
timization, Product of Experts

1

Introduction

In recent years, machine learning and data mining has been gaining more and
more attention by showing very good prediction performance in areas such as
recommender systems, pattern, speech and visual object recognition and many
more. The lift in prediction performance is usually due to the development of
more complex models as we see for example in the area of deep learning. However,

2

developing more complex models usually has drawbacks, which is the increas-
ing time that is spent for learning the model plus the increasing dimensionality
of the hyperparameter space of the associated model. By hyperparameters we
denote parameters of a model that can not explicitly be learned from the data
by a well-deﬁned optimization criterion such as the minimization of a regular-
ized loss functional. These hyperparameters can be continuous, the reader might
consider a positive learning rate of a gradient descent optimization approach,
or a regularization constant of a Tikhonov regularization term. However, by hy-
perparameters we also consider discrete choices, such as the dimensionality of
a low-rank factorization or the number of nodes and layers in a deep feedfor-
ward neural network. Additionally, hyperparameters can also be categorical, for
instance the choice of kernel function in a support vector machine, or even the
choice of loss function to optimize within the optimization criterion. Finally, even
model choice as well as preprocessing of the data can be understood as hyperpa-
rameters of a general learner. What all of these parameters have in common is
that they cannot be optimized in a straightforward fashion, but usually their cor-
rect setting renders methods from producing weak predictions to state-of-the-art
predictions. Due to this impact, practicioners that do not know the underlying
techniques very well usually have a hard time optimizing hyperparameters and
therefore rely on either choosing standard hyperparameters or on performing a
grid-search, which tries many hyperparameters and in the end chooses the one
that performs best. In this way, a lot of unnecessary computations are created.

Out of this reason, recent research proposes to use black-box optimization
techniques such as sequential model-based optimization (SMBO) to allow for
a more directed search in the hyperparameter space. Essentially, SMBO treats
the hyperparameter conﬁguration as input for a black box function and uses a
surrogate model to learn on a few observed performances to then predict the
performance of any arbitrary hyperparameter conﬁguration. The predicted per-
formance as well as the uncertainty of the surrogate model are then used within
the context of an acquisition function to ﬁnally predict a hyperparameter con-
ﬁguration that likely performs better, while keeping a good balance between
exploitation and exploration. On the one hand, exploitation is attained when-
ever the acquisition function chooses hyperparameter conﬁgurations that are
very close to already observed well-performing conﬁgurations and therefore the
surrogate model is quite certain about its estimation. On the other hand, ex-
ploration is met if the acquisition function chooses conﬁgurations that are very
distant to all observed conﬁgurations, i.e. explores new areas of the hyperparam-
eter space, where the surrogate model is quite uncertain about its prediction.
Given that usually only a few initial observations are present and the amount
of overall queries for hyperparameter conﬁgurations is limited, a decent tradeoﬀ
between both exploration and exploitation is desired.

More recent work is inspired by the area of meta learning, where the goal
is to transfer knowledge for parameters of a given model from having learned
this model already on other data sets [4]. Thus, these methods propose to also
take into account the knowledge of hyperparameter performances on diﬀerent

3

(past) data sets, where hyperparameter opimization has already been done. This
is quite intuitive, as every experienced practitioner, who has already learned a
model many times on diﬀerent data sets probably comes up with better hyperpa-
rameter conﬁgurations for the target data set to test initially. In many works, the
surrogate model is then learned on the hyperparameter performances of past data
sets and therefore has a better knowledge of well-performing hyperparameters
to choose. In order for the surrogate model to not confuse performances of the
same hyperparameter conﬁguration on diﬀerent data sets, the meta knowledge
is usually augmented by additional meta features that describe characteristics
of a data set.

Many surrogate models have been proposed, but one of the simplest surro-
gates is probably a Gaussian process (GP), as it is relatively simple to learn,
delivers good predictions and furthermore, due to its probabilistic nature, allows
for a direct estimation of uncertainties, which is a key ingredient for SMBO.
Another advantage of using Gaussian processes compared to other surrogate
models is that they are basically hyperparameter free, as all the parameters that
we have to specify for the kernel can be learned by optimizing their marginal
log likelihood. However, Gaussian processes have one huge drawback which lies
in their scalability. In order to learn a Gaussian process, the kernel matrix com-
puted over all observed instances has to be inverted which is an operation with
cubic expense in the number of observations. Thus, if we seek to include meta
knowledge of many past data sets into the training data of the Gaussian pro-
cess, learning the Gaussian process might even take more time and memory
than learning the model we seek to optimize the hyperparameters for, which
then renders a Gaussian process infeasible, despite its advantages.

In this paper, we propose to use a product of Gaussian process experts as
surrogate model, where basically an independent GP is learned for all the obser-
vation of one past data set and in the end all the predictions of the individual
experts are assembled to predict hyperparameter performances of the target data
set. Following this approach, our work has four main contributions:

(cid:73) We learn a product of GP experts, which allows for the inclusion of a large

amount of meta information,

(cid:73) by using GPs as base surrogate model, we employ surrogates that are very

easy and fast to learn, and do not require much memory
(cid:73) additionally, by using GPs, we do not introduce additional

surrogate-hyperparameters in opposition to many state of the art methods,
(cid:73) ﬁnally, we show empirically that products of GP experts perform very com-
petitively for hyperparameter optimization against a variety of published
competitors, as well as make both the implementation and the meta data
publicly available.

2 Related Work

As already mentioned, in the recent years there has been a growing interest
in research regarding hyperparameter optimization. Random search has been

4

proposed as an alternative to grid-search and works well in cases of low eﬀective
dimensionality, where a subspace of the hyperparameter space does not inﬂuence
the results as much as the remaining hyperparameter dimensions [3].

In the context of SMBO, many diﬀerent surrogate models have been pro-
posed in a variety of papers. At ﬁrst, an independent Gaussian process [17] was
used. We denote it as independent as it does not learn across data [20]. Secondly,
random forests have been proposed as surrogates and inherit the ability to work
well with non numerical as well as hierarchical hyperparameters [13]. Regard-
ing hyperparameter optimization using meta knowledge, a stacking of a GP on
top of a ranking SVM was proposed [2], as well as a Gaussian process with a
multi task kernel in two closely related works [26] [21]. Furthermore, a mixture
of a multilayer perceptron and a factorization machine has been employed as
surrogate model [18], which automatically learns data set representations and
therefore does not necessarily need meta features.

A diﬀerent aspect of using meta knowledge is conducted through learning an
initialization of well-performing hyperparameters. The ﬁrst work in this context
is [8] where the initial hyperparameters are chosen based on data sets that are
closest with respect to the Euclidean distance evaluated on the meta features of
the respective data sets. This intuition has been extended by [24] which uses a
diﬀerentiable plug in estimator to compute initial hyperparameters. Finally, [25]
employs a static sequence of hyperparameters that is learned using meta knowl-
edge and does not need a surrogate model at all, however, it has the drawback
that it needs meta information over diﬀerent data sets evaluated on the same
hyperparameter grids.

There is a plethora of other approaches that are either model speciﬁc [1] or
use genetic algorithms [15] [7], or do both in conjunction [9]. As these approaches
are not embedded in the context of SMBO, we will leave them out of further
discussions.

Since we are seeking to employ product of experts models in the framework
of SMBO-based hyperparameter optimization, we also review the related work
in this ﬁeld as well as various techniques to speed up Gaussian process learning.
Initially, product of experts models have been proposed by [11] alongside with a
learning algorithm [12] to train the parameters of such a model. The generalized
product of experts [5] introduces additional weighting factors within the product
in order to reduce the overconﬁdence of the product of experts in unknown areas.
Another model that also estimates a joint probability density given by a set of
experts is the Bayesian committee machine [22], which includes the prior in its
predictions. Finally, the work by [6] combines both the idea of the generalized
POE with its weighting factors with the Bayesian committee machine. We do
want to highlight that all of this work is not speciﬁcally tailored to Gaussian
processes, however, [6] argues that using products of experts is an easy way to
make Gaussian processes more scalable to larger training data sets.

Additionally, many eﬀorts have been made by the means of sparse GPs,
namely Gaussian processes learned on subsets of the original training data such
as [19] which employs kd-trees for subsampling. There are many more works in

5

this area such as [23], [10] or [16], however, as we want to make use of the rich
meta information of hyperparameter performance on other data sets using only
a subset of the meta information seems counterintuitive. Due to this reason,
we do not intend to use sparse GPs as surrogates for Bayesian hyperparameter
optimization.

3 Background

In this section we ﬁrst review hyperparameter optimization and sequential model-
based optimization in general, secondly, we discuss Gaussian processes shortly
and lastly we give a review of product of experts models which we ultimately
seek to employ as surrogate models.

3.1 Problem Setting

Let D denote by the space of all data sets, following the notation by [3], we denote
a learning algorithm for a ﬁxed model class M by a mapping A : Λ × D −→ M.
Thus, an algorithm A is essentially a mapping from a given hyperparameter
conﬁguration and training data to a model which is learned by minimizing a loss
functional. In many cases, the hyperparameter space Λ is the cartesian product
of lower dimensional spaces. Now we can deﬁne the problem of hyperparameter
optimization as choosing the hyperparameter conﬁguration λ(cid:63) which minimizes
the loss of a learned model on given validation data:

λ(cid:63) := arg min

L(A(λ, Dtrain), Dval) =: arg min

b(λ, D) .

(1)

λ∈Λ

λ∈Λ

Please note that we use the short b as notation for the process of learning a model
on training data with given hyperparameters and evaluating it on validation
data. Clearly, b is the black box function that we seek to optimize using Bayesian
optimization.

3.2 Sequential Model-Based Optimization

The SMBO framework is depicted in Algorithm 1. It starts by learning a sur-
rogate model denoted by Ψ such that Ψ ≈ b on a set of given hyperparameter
performances which are stored in the observation history H. Secondly, the surro-
gate model will be used to predict the hyperparameter performance of unknown
hyperparameters, these predictions as well as the uncertainties will be forwarded
to the acquisition function a, which then picks a hyperparameter conﬁguration
to test. The most commonly used acquisition function is Expected Improvement
(EI) and can be computed analytically if one assumes the probability of im-
provement to be Gaussian [14]. Having chosen a candidate conﬁguration, b will
be evaluated for the proposed hyperparameter conﬁguration, the result will be
fed into the observation history and the process is repeated for T many times
until ﬁnally a best hyperparameter conﬁguration λbest is found. Additionally, the

6

Algorithm 1 Sequential Model-based Optimization Across Data Sets
Input: Hyperparameter space Λ, observation history H, target data set D, number of
iterations T , acquisition function a, surrogate model Ψ , initial best hyperparameter
conﬁguration λbest.

Output: Best hyperparameter conﬁguration λbest for D
1: for t = 1 to T do
Fit Ψ to H
2:
λnew = arg max
3:

a (Ψ (λ, D), H)

λ∈Λ

Evaluate b (λnew, D)
if b(λnew, D) < b(λbest, D) then

4:
5:
6:
7: H = H ∪ (λnew, b (λnew, D))
8: return λbest

λbest = λnew

surrogate model’s feature vector is usually augmented by meta features, which
are descriptive features of a data set, to allow the surrogate model to distinguish
between diﬀerent data sets.

3.3 Gaussian Processes

We introduce Gaussian processes as we use them as base models in a product of
experts. Given is a regression problem of the form

y(x) = f (x) + (cid:15) ,

(2)

where we assume i.i.d. noise (cid:15) ∼ N (0, σ2). A Gaussian process assumes that
for a given set of input variables X = (x1, ..., xN ) with associated labels y =
(y1, ..., yN ) the labels are multivariate Gaussian distributed y ∼ N (0, K), where
K is a covariance matrix that is deﬁned through a positive semideﬁnite kernel
function k(x, x(cid:48)). A very common choice for k is the squared exponential kernel

k(x, x(cid:48)) = exp

+ σ2δ(x = x(cid:48)) ,

(3)

(cid:18) −(cid:107)x − x(cid:48)(cid:107)2
2σ2
l

(cid:19)

where θ = (σl, σ) are denoted as the hyperparameters and the δ function returns
1 if its predicate is true and 0 otherwise. Given a set of known observations, the
conditional distribution of a label f(cid:63) given its input x(cid:63) is Gaussian distributed
with mean and covariance

(cid:63) K −1y
µ(f(cid:63)) = k(cid:62)
σ2(f(cid:63)) = k(cid:63)(cid:63) − k(cid:62)

(cid:63) K −1k(cid:63) ,

(4)

(5)

where k(cid:63) = (k(x1, x(cid:63)), ..., k(xn, x(cid:63))) is the vector of kernel evaluations of the new
input x(cid:63) to all observed inputs and k(cid:63)(cid:63) = k(x(cid:63), x(cid:63)) is the prior covariance of f(cid:63).
As we can see, using a GP for predictions requires inverting the kernel matrix of
size N , which is an operation of O(N 3) and thus becomes infeasible for data sets

7

with many instances. Recalling that our primary goal was to include large scale
meta knowledge of past hyperparameter performances, this boundary might be
reached very soon, which would force us to throw valuable data away or rely
on other surrogate models. However, solving the linear system of equations for
inversion of K can be reduced to O(N 2) by using a Cholesky decomposition of
the kernel matrix [17].

The kernel hyperparameters can be learned by maximizing their marginal
log likelihood using standard optimization techniques such as gradient ascent.
Again, for optimizing the kernel hyperparameters, we have to invert the kernel
matrix as well as compute its determinant, which also both scale cubically in
the dimension of K. As gradient ascent might use several iterations to converge
to a useful θ, this inversion becomes even more the bottle neck with respect to
both computational speed as well as memory usage.

3.4

(Generalized) Product of Experts (POE)

In order to scale Gaussian processes to a large training data set (i.e. observation
history) we will use product of experts models [11], of which several variants
have been proposed. Within a product of GP experts, a set of M invididual
Gaussian processes are learned on M disjoint subsets of the training data, so let
us decompose our training data as

X = (X (1), ..., X (M ))

y = (y(1), ..., y(M )) ,

(6)

such that the individual subsets of instances and labels are disjoint. Then, follow-
ing the independence assumption, the marginal joint likelihood factorizes into a
product of single likelihoods

p(y | X, θ) =

(cid:16)

y(i) | X (i), θ(i)(cid:17)

.

pi

(7)

Thus, in order to learn the individual experts, we only need to invert kernel
matrices of the size of roughly N/M , thus learning the individual experts can be
done in O(N 3/M 3) which is a reasonable reduction for a suﬃciently large enough
M . In this way, we also learn M many diﬀerent sets of kernel hyperparameters.
As the M experts have been learned, we can compute the marginal likelihood
by multiplying all individual likelihoods. The generalized product of experts [5]
introduces additional weighting factors βi such that:

p(y | X, θ) =

(cid:16)

y(i) | X (i), θ(i)(cid:17)

.

βi

pi

(8)

M
(cid:89)

i=1

M
(cid:89)

i=1

Naturally, if all βi = 1, we arrive at the initial formulation of equation 7. Comput-
ing the product of the individual likelihoods yields a density that is proportional

8

to a Gaussian with following mean and precision:

µpoe(f(cid:63)) = (σpoe(f(cid:63)))2

βiσ−2
i

(f(cid:63))µi(f(cid:63))

M
(cid:88)

i=1

(σpoe(f(cid:63)))−2 =

βiσ−2
i

(f(cid:63))

M
(cid:88)

i=1

(9)

(10)

i

Essentially, by replacing σ−2
(f(cid:63)) = τi(f(cid:63)) with the precision, we see that the
mean predicted by the product of experts is a sum of means, weighted by the
product of the individual βi and the precision τi, which is then divided by the
total sum of weighting factors. Usually, the βi are set such that (cid:80)
i βi = 1,
surprisingly, this already works quite well for βi ≡ 1/M . This does not change
the mean as the multiplication with the precision cancels out the eﬀect, however,
the precisions eﬀectively get weighed down and decrease the overconﬁdence of
the initial product of experts without any weights.

3.5 Product of Experts in SMBO

Having introduced the product of experts models, their implementation for hy-
perparameter optimization in the SMBO framework seems straightforward. How-
ever, a few questions still remain unanswered. At ﬁrst, we split the meta knowl-
edge into all the instances belonging to hyperparameter performances of one
data set. In this way, each expert will be learned on the meta information of
one distinct data set. If this would still be too large for a GP to learn, we could
further subdivide them into smaller subsets.

Secondly, the question of how the information on the target data set will be
incorporated into the surrogate model remains. We seek for two alternatives, in
the ﬁrst one we simply add the information of new points on the target data set
to all the experts in the ensemble. Doing this, we eﬀectively train all experts to
be expert for two data sets, the initial one they have been trained on plus the
target data set. In our implementation, we then follow the intution of all weights
summing up to one, thus we set all βi = 1/M .

As an independent Gaussian process that is learned without any meta knowl-
edge already behaves reasonably well as a surrogate model, we also tried another
alternative. We still feed the target data set information into all experts learned
in the ensemble but additionally create a new GP that carries only the informa-
tion of the target data set and is weighed much higher than the individual ex-
perts. Speciﬁcally, we use βi = 1/2M for the individual experts and βM +1 = 1/2
for the GP learned on only the target data set responses. In this way, we use the
meta information as well as the strength of an independent Gaussian process.

Additionally, we seek to scale the hyperparameter performances observed in
the meta data as well as the hyperparameter performances of the target data
set. This is due to the fact that the range of b naturally depends on the data
set. Consider for example a classiﬁcation problem where b models the misclassi-
ﬁcation rate of a classiﬁer for some test data. Naturally, for some data sets, very

9

low misclassiﬁcation rates might be achieved in contrast to other data sets that
are simply harder to classify. A POE might then be biased towards choosing
hyperparameter conﬁgurations that produce good results on simple data sets,
which is something we want to prevent. In order to do so, we scale the labels of
the meta data to become standard Gaussian distributed, for the target data set,
we do this on-the-ﬂy every time we see a new response of b as was also proposed
by [26].

4 Experiments

To evaluate the proposed surrogate models for hyperparameter optimization, we
conduct hyperparameter optimization within the SMBO framework including a
variety of published baselines. The experiments are performed on two meta data
sets that we have created ourselves.

4.1 Meta Data Set Creation

We have created two meta data sets for the task of classiﬁcation using two
distinct classiﬁers, namely being a support vector machine (SVM) and AdaBoost.
These meta data sets consists of a complete grid search for both classiﬁers on
50 classiﬁcation data sets that we have taken from the UCI repository1. If splits
were already given, we merged them into one complete data set, shuﬄed the
resulting data set and then took 80% of the data for training and the remaining
20% for testing. The AdaBoost meta data set was created by running AdaBoost2
with hyperparameters I ∈ {2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000}
and M ∈ {2, 3, 4, 5, 7, 10, 15, 20, 30}. This yields 108 meta instances per data set
and therefore the overall meta data set contains 5400 instances.

The second meta data set was created by running an SVM3 on all of the data
sets, with four hyperparameters. The ﬁrst one resembles the choice of kernel and
is categorical between a linear, a polynomial and an RBF kernel, thus intro-
duces three binary hyperparameters. The second hyperparameter is the tradeoﬀ
parameter, usually denoted as C, the third and fourth hyperparameter are the
degree d of the polynomial kernel and the width γ of the RBF kernel. If the ker-
nel hyperparameters are not used, i.e. the polynomial degree for an RBF kernel,
we set them to a constant value of zero. As for the AdaBoost meta data set, we
computed the misclassiﬁcation rates using grid-search, where C was chosen from
the set {2−5, . . . , 26}, the polynomial degree d was chosen from {2, . . . , 10} and
γ was chosen from {0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 20, 50, 100, 1000}.
This results in 288 runs per data set, and therefore the overall meta data set
contains up to 14, 400 instances.

Finally, we also added meta features to the meta data set, to allow the sur-
rogate models to distinguish between the same hyperparameter conﬁgurations

1 http://archive.ics.uci.edu/ml/index.html
2 http://www.multiboost.org
3 http://svmlight.joachims.org

10

evaluated on diﬀerent data sets. A list of all employed meta features can be seen
in Table 1. For our experiments, all features in the meta data set, namely the
computed meta features as well as the hyperparameter conﬁgurations have been
scaled to values in [0, 1].

Table 1. List of all meta-features used.

Number of Classes
Number of Instances
Log Number of Instances
Number of Features
Log Number of Features
Data Set Dimensionality
Log Data Set Dimensionality
Inverse Data Set Dimensionality

Log Inverse Data Set Dimensionality
Class Cross Entropy
Class Probability Min
Class Probability Max
Class Probability Mean
Class Probability Standard Deviation
Kurtosis Min
Kurtosis Max

Kurtosis Mean
Kurtosis Standard Deviation
Skewness Min
Skewness Max
Skewness Mean
Skewness Standard Deviation

4.2 Competing Surrogate Models

Random Search (RANDOM). This is a surrogate that simply picks a random
point out of the grid.

Random Forests (RF). Sequential Model-based Algorithm Conﬁguration [13]
employs a random forest as surrogate model and computes uncertainties using
the learned ensemble by estimating empirical means and standard deviations.

Independent Gaussian Process (IGP). An independent Gaussian process with
SE-ARD kernel that is only learned on the observations on the target data set,
this was proposed by [20].

Surrogate-based Collaborative Tuning (SCOT). This surrogate model is eﬀec-
tively a stacking of an SVMRANK and a Gaussian process and was proposed
by [2]. The ranking SVM learns how to rank hyperparameter conﬁgurations
across data sets, uncertainties are estimated by stacking a GP on the ranked
output.

Full Gaussian Process (FGP). A Gaussian process with SE-ARD kernel that
is learned on the whole meta data set. This is basically the model we seek to
approximate by learning a product of experts.

Gaussian Process with MKL (MKLGP). This surrogate was proposed by [26]
and learns basically a full GP over the whole meta data set using a combination
of an SE-ARD kernel and a kernel function that models the distances between
data sets based on meta features.

Factorized Multilayer Perceptron (FMLP). The surrogate model that was pro-
posed by [18], which learns a multilayer perceptron and factorizes the weights in
the ﬁrst layer in order to learn latent data set and hyperparameter representa-
tions. Uncertainties are estimated by learning an ensemble of FMLPs.

11

Product of Gaussian Process Experts (POGPE). This surrogate model learns
a product of GP experts as described in Section 3.4. Each expert employs an
SE-ARD kernel. Information of the target data set is distributed to all experts,
which are all weighted equally.

Single Gaussian Process Expert (SGPE). This surrogate model also learns a
product of GP experts as described in Section 3.4, however, also learns an inde-
pendent GP for the target data set only and weighs the target GP as much as
the whole set of experts.

4.3 Experimental Setup

Our experiments are performed in a leave-one-out fashion, meaning that we
train the surrogate model on 49 data sets and use the meta knowledge to start
SMBO on the remaining test data set. To cancel out random eﬀects, we ran
all experiments for a total of 100 times and averaged the results in the end.
In total, each SMBO run was allowed to test T = 70 diﬀerent hyperparameter
conﬁgurations on the test data. As acquisition function we employed the pop-
ular expected improvement, which is by now the most widely used acquisition
function in hyperparameter optimization using the SMBO framework.

As evaluation metric, we use the average rank, where, for each target data
set, we rank all competing surrogate models based on the best misclassiﬁcation
rate they have found so far. Ties are being solved by granting the average rank,
i.e. if one surrogate models ﬁnd the misclassiﬁcation rates 0.2, another two ﬁnd
0.25 and a third one ﬁnds only 0.5, we would rank the surrogates with 1, 2.5,
2.5 and 4. As we run the experiments for 50 diﬀerent target data sets, we report
the average of all average ranks.

The implementations were largely done by ourselves, except for SMAC and

SCOT, where we used MLTK4 for the former and the implementation by
Joachims5 for the ranking SVM used in SCOT. All hyperparameters of the GP
based models have been automatically tuned by maximizing their marginal likeli-
hood, for FMLP we used the setting proposed by the authors. For SMAC, SCOT
and MKLGP we used leave-one-out cross validation to tune the hyperparame-
ters. For all GP-based models, we implemented the Cholesky decomposition to
speed up the inversion of kernel matrices. In order to facilitate reproducibility
of our experimental results, we make the program code as well as the employed
meta data sets publicly available on Github6.

4.4 Performance in SMBO

The average rank among all competing methods can be seen in Figure 1, where
the left plot shows the average rank on AdaBoost versus the number of trials

4 http://www.cs.cornell.edu/~yinlou/projects/mltk/
5 http://svmlight.joachims.org/
6 https://github.com/nicoschilling/ECML2016

12

conducted, and the right one shows the results for the SVM meta data set. First
of all, we see that for both meta data sets the random baseline shows the worst
performance as is expected. Surprisingly, for both meta data sets, POGPE and
SGPE ﬁnd the best hyperparameter among the competitors in the ﬁrst trial.
During the SMBO procedure, both the full Gaussian process as well as MKLGP
perform better on the AdaBoost data set, however, we observe that POGPE
performs better on the SVM data set which is quite a surprise. POGPE, despite
its good starting point, is being outperformed by FMLP on the SVM data set
in the ﬁrst 15 trials, which then degrades in performance and performs worse
than both full GP approaches. Comparing both of these with each other, we
see that they perform almost equally, however, MKLGP tends to have worse
starting points than a simple full GP. For both meta data sets, we see the lift of
including meta knowledge through comparison with the independent GP, which
performs reasonably on AdaBoost but degrades on SVM. This observation leads
us to the conclusion that optimizing the hyperparameters of AdaBoost seems
In contrast to POGPE, SGPE does not seem to
an easier task than on SVM.

Fig. 1. Average Rank of all competing methods. The left plot shows results for Ad-
aBoost, the right plot shows results for the SVM meta data set.

perform that well, maybe the tradeoﬀ between product of experts and single
GP has to be adjusted for each trial, however, this would introduce another
hyperparameter for the surrogate model, which we do not seek to do.

Overall, we conclude that POGPE, FMLP, FGP and MKLGP are among
the best performing surrogate models, so for these models we also computed the
average distance to the minimum in terms of b. For each data set, we scale all
values of b (in this case accuracies) to be in [0, 1]. Then, again for each data set,

13

Fig. 2. Average Distance to Minimum of the best performing methods. The left plot
shows results for AdaBoost, the right plot shows results for the SVM meta data set.

we compute the distance of the best hyperparameter response so far to the best
on the overall grid. This value is then averaged to become the average distance
to the minimum, which gives an idea of how a surrogate model makes use of the
responses it gets on the target data set. The results can be seen in Figure 2, where
the left plot shows the results for AdaBoost and the right for SVM. Overall, we
see the same behaviour as we have seen in average rank, however, FMLP seems to
be a little bit better here. For AdaBoost, it achieves the lowest average distance,
this is due to FMLP winning severely against its competitors on the data set
sonar-scale, where in the average rank, this win does not count that much. In
conclusion, we see that POGPE works really well on both evaluatuion metrics,
especially when we consider its simplicity in the light of POGPE actually being
an approximation of a full GP.

4.5 Runtime Experiments

In order to demonstrate the scalability of using POGPE, we have also conducted
a runtime experiment. We have measured the runtime of the most competitive
methods, namely being POGPE, FMLP, and both of the full Gaussian process
approaches FGP and MKLGP. Experiments were conducted on a Xeon E5-
2670v2 with 2.50GHz clock speed and 64 GB of RAM, where we again performed
a total of 70 trials of an SMBO run on the SVM meta data set. To account for
measurement noise, we repeated all experiments 10 times.

The results can be seen in Figure 3, where the left plot shows the cumulative
runtime in seconds without the initial training time of the surrogate in opposition
to the right plot which includes it. We plot both results as the training of the

14

Fig. 3. Runtime comparison among the most competitive surrogate models. The left
plot shows the cumulative runtime in seconds.

surrogate model can be performed in an oﬄine fashion while waiting for new
data. As we do not take into account the learning of the actual model, i.e. the
evaluation of b, both plots can be understood as the total overhead time of
running SMBO instead of using default hyperparameters. We can observe that
POGPE consumes drastically less time than all its competitors, simply due to
the fact that we have to invert much smaller kernel matrices. By excluding the
potentially oﬄine training time, a full GP is faster than FMLP, however, if the
full GP needs to be trained ﬁrst an FMLP is faster but gets overtaken with
respect to computation time if only enough trials are performed. In both plots,
MKLGP requires the most computation time. Considering that these diﬀerences
will be bigger if we use more meta information, we conclude that POGPE is very
fast while performing also very well in the SMBO procedure.

5 Conclusions

In this paper, we proposed to choose POE models as surrogate models for hyper-
parameter tuning, speciﬁcally we chose to employ Gaussian processes because of
their fairly easy implementation as well as their predictive performance in the
ﬁeld of Bayesian optimizazion. We do acknowledge that POGPE is not the best
model in all experiments, but is quite competitive which is a surprise due to its
simplicity and its approximative nature. In the very ﬁrst trial both POGPE and
SGPE (as they start out the same) on average pick the best hyperparameter con-
ﬁguration compared to all competitor methods, which shows how eﬃcient usage
of the meta data can simply be made by learning a product of experts on each
invidual data set and querying the committee. Moreover, the other competitive

15

surrogate models such as FMLP and MKLGP introduce additional hyperpa-
rameters for the surrogate model that need to be optimized. For FMLP, tuning
of the network architecture such as number of layers and number of nodes per
layer as well as setting correct learning rates is demanded. MKLGP requires
tuning of the number of neighboring data sets and the tradeoﬀ term between
both employed kernels. In comparison, a simple product of GP experts does not
require any hyperparameter tuning, as the GP parameters can be learned by
maximizing their marginal likelihood quite eﬀectively.

As we have seen in the results, POGPE can also be trained much faster than
the other competitive surrogate models. In the light of big data we will probably
have access to also growing meta data sets that we can employ for hyperpa-
rameter optimization, which makes scalable use of the meta data a necessity.
Moreover, POE models are easy to parallelize which allows easy usage in dis-
tributed scenarios. Out of all these reasons we see them as a very reasonable
choice to pick as surrogate models for hyperparameter optimization including
large scale meta data.

Acknowledgments The authors gratefully acknowledge the co-funding of their
work by the German Research Foundation (DFG) under grant SCHM 2583/6-1.

References

1. Adankon, M.M., Cheriet, M.: Model selection for the LS-SVM. Application to

handwriting recognition. Pattern Recognition 42(12), 3264–3270 (2009)

2. Bardenet, R., Brendel, M., Kegl, B., Sebag, M.: Collaborative hyperparameter
tuning. In: Dasgupta, S., Mcallester, D. (eds.) Proceedings of the 30th Interna-
tional Conference on Machine Learning (ICML-13). vol. 28, pp. 199–207. JMLR
Workshop and Conference Proceedings (May 2013)

3. Bergstra, J., Bengio, Y.: Random search for hyper-parameter optimization. J.

Mach. Learn. Res. 13, 281–305 (Feb 2012)

4. Brazdil, P., Carrier, C.G., Soares, C., Vilalta, R.: Metalearning: Applications to

data mining. Springer Science & Business Media (2008)

5. Cao, Y., Fleet, D.J.: Generalized product of experts for automatic and principled
fusion of gaussian process predictions. arXiv preprint arXiv:1410.7827 (2014)
6. Deisenroth, M.P., Ng, J.W.: Distributed gaussian processes. In: International Con-

ference on Machine Learning (ICML). vol. 2, p. 5 (2015)

7. Escalante, H.J., Montes, M., Sucar, L.E.: Particle swarm model selection. The

Journal of Machine Learning Research 10, 405–440 (2009)

8. Feurer, M., Springenberg, J.T., Hutter, F.: Initializing bayesian hyperparameter
optimization via meta-learning. Proceedings of the Twenty-Ninth AAAI Confer-
ence on Artiﬁcial Intelligence (2015)

9. Guo, X.C., Yang, J.H., Wu, C.G., Wang, C.Y., Liang, Y.C.: A novel ls-svms hyper-
parameter selection based on particle swarm optimization. Neurocomput. 71(16-
18), 3211–3215 (Oct 2008)

10. Hensman, J., Fusi, N., Lawrence, N.D.: Gaussian processes for big data. arXiv

preprint arXiv:1309.6835 (2013)

16

11. Hinton, G.E.: Products of experts. In: Artiﬁcial Neural Networks, 1999. ICANN
99. Ninth International Conference on (Conf. Publ. No. 470). vol. 1, pp. 1–6. IET
(1999)

12. Hinton, G.E.: Training products of experts by minimizing contrastive divergence.

Neural computation 14(8), 1771–1800 (2002)

13. Hutter, F., Hoos, H.H., Leyton-Brown, K.: Sequential model-based optimization
for general algorithm conﬁguration. In: Proceedings of the 5th International Con-
ference on Learning and Intelligent Optimization. pp. 507–523. LION’05, Springer-
Verlag, Berlin, Heidelberg (2011)

14. Jones, D.R., Schonlau, M., Welch, W.J.: Eﬃcient global optimization of expensive

black-box functions. J. of Global Optimization 13(4), 455–492 (Dec 1998)

15. Koch, P., Bischl, B., Flasch, O., Bartz-Beielstein, T., Weihs, C., Konen, W.: Tuning
and evolution of support vector kernels. Evolutionary Intelligence 5(3), 153–170
(2012)

16. Quinonero-Candela, J., Rasmussen, C.E.: A unifying view of sparse approximate
gaussian process regression. The Journal of Machine Learning Research 6, 1939–
1959 (2005)

17. Rasmussen, C.E., Williams, C.K.I.: Gaussian Processes for Machine Learning

(Adaptive Computation and Machine Learning). The MIT Press (2005)

18. Schilling, N., Wistuba, M., Drumond, L., Schmidt-Thieme, L.: Hyperparameter
optimization with factorized multilayer perceptrons. In: Machine Learning and
Knowledge Discovery in Databases, pp. 87–103. Springer (2015)

19. Shen, Y., Ng, A., Seeger, M.: Fast gaussian process regression using kd-trees. In:
Proceedings of the 19th Annual Conference on Neural Information Processing Sys-
tems. No. EPFL-CONF-161316 (2006)

20. Snoek, J., Larochelle, H., Adams, R.P.: Practical bayesian optimization of machine
learning algorithms. In: Pereira, F., Burges, C., Bottou, L., Weinberger, K. (eds.)
Advances in Neural Information Processing Systems 25, pp. 2951–2959. Curran
Associates, Inc. (2012)

21. Swersky, K., Snoek, J., Adams, R.P.: Multi-task bayesian optimization. In: Burges,
C., Bottou, L., Welling, M., Ghahramani, Z., Weinberger, K. (eds.) Advances in
Neural Information Processing Systems 26, pp. 2004–2012. Curran Associates, Inc.
(2013)

22. Tresp, V.: A bayesian committee machine. Neural Computation 12(11), 2719–2741

(2000)

23. Williams, C., Seeger, M.: Using the nyström method to speed up kernel machines.
In: Proceedings of the 14th Annual Conference on Neural Information Processing
Systems. pp. 682–688. No. EPFL-CONF-161322 (2001)

24. Wistuba, M., Schilling, N., Schmidt-Thieme, L.: Learning hyperparameter opti-
mization initializations. In: Data Science and Advanced Analytics (DSAA), 2015.
36678 2015. IEEE International Conference on. pp. 1–10. IEEE (2015)

25. Wistuba, M., Schilling, N., Schmidt-Thieme, L.: Sequential model-free hyperpa-
rameter tuning. In: Data Mining (ICDM), 2015 IEEE International Conference
on. pp. 1033–1038. IEEE (2015)

26. Yogatama, D., Mann, G.: Eﬃcient transfer learning method for automatic hy-
perparameter tuning. In: International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS 2014) (2014)

Scalable Hyperparameter Optimization
with Products of Gaussian Process Experts

Nicolas Schilling, Martin Wistuba and Lars Schmidt-Thieme

Information Systems and Machine Learning Lab
Universitätsplatz 1, 31141 Hildesheim, Germany
{schilling,wistuba,schmidt-thieme}@ismll.uni-hildesheim.de

Abstract. In machine learning, hyperparameter optimization is a chal-
lenging but necessary task that is usually approached in a computation-
ally expensive manner such as grid-search. Out of this reason, surrogate
based black-box optimization techniques such as sequential model-based
optimization have been proposed which allow for a faster hyperparameter
optimization. Recent research proposes to also integrate hyperparameter
performances on past data sets to allow for a faster and more eﬃcient
hyperparameter optimization.
In this paper, we use products of Gaussian process experts as surrogate
models for hyperparameter optimization. Naturally, Gaussian processes
are a decent choice as they oﬀer good prediction accuracy as well as es-
timations about their uncertainty. Additionally, their hyperparameters
can be tuned very eﬀectively. However, in the light of large meta data
sets, learning a single Gaussian process is not feasible as it involves in-
version of a large kernel matrix. This directly limits their usefulness for
hyperparameter optimization if large scale hyperparameter performances
on past data sets are given.
By using products of Gaussian process experts the scalability issues can
be circumvened, however, this usually comes with the price of having
less predictive accuracy. In our experiments, we show empirically that
products of experts nevertheless perform very well compared to a variety
of published surrogate models. Thus, we propose a surrogate model that
performs as well as the current state of the art, is scalable to large scale
meta knowledge, does not include hyperparameters itself and ﬁnally is
even very easy to parallelize.

Keywords: Hyperparameter Optimization, Sequential Model-Based Op-
timization, Product of Experts

1

Introduction

In recent years, machine learning and data mining has been gaining more and
more attention by showing very good prediction performance in areas such as
recommender systems, pattern, speech and visual object recognition and many
more. The lift in prediction performance is usually due to the development of
more complex models as we see for example in the area of deep learning. However,

2

developing more complex models usually has drawbacks, which is the increas-
ing time that is spent for learning the model plus the increasing dimensionality
of the hyperparameter space of the associated model. By hyperparameters we
denote parameters of a model that can not explicitly be learned from the data
by a well-deﬁned optimization criterion such as the minimization of a regular-
ized loss functional. These hyperparameters can be continuous, the reader might
consider a positive learning rate of a gradient descent optimization approach,
or a regularization constant of a Tikhonov regularization term. However, by hy-
perparameters we also consider discrete choices, such as the dimensionality of
a low-rank factorization or the number of nodes and layers in a deep feedfor-
ward neural network. Additionally, hyperparameters can also be categorical, for
instance the choice of kernel function in a support vector machine, or even the
choice of loss function to optimize within the optimization criterion. Finally, even
model choice as well as preprocessing of the data can be understood as hyperpa-
rameters of a general learner. What all of these parameters have in common is
that they cannot be optimized in a straightforward fashion, but usually their cor-
rect setting renders methods from producing weak predictions to state-of-the-art
predictions. Due to this impact, practicioners that do not know the underlying
techniques very well usually have a hard time optimizing hyperparameters and
therefore rely on either choosing standard hyperparameters or on performing a
grid-search, which tries many hyperparameters and in the end chooses the one
that performs best. In this way, a lot of unnecessary computations are created.

Out of this reason, recent research proposes to use black-box optimization
techniques such as sequential model-based optimization (SMBO) to allow for
a more directed search in the hyperparameter space. Essentially, SMBO treats
the hyperparameter conﬁguration as input for a black box function and uses a
surrogate model to learn on a few observed performances to then predict the
performance of any arbitrary hyperparameter conﬁguration. The predicted per-
formance as well as the uncertainty of the surrogate model are then used within
the context of an acquisition function to ﬁnally predict a hyperparameter con-
ﬁguration that likely performs better, while keeping a good balance between
exploitation and exploration. On the one hand, exploitation is attained when-
ever the acquisition function chooses hyperparameter conﬁgurations that are
very close to already observed well-performing conﬁgurations and therefore the
surrogate model is quite certain about its estimation. On the other hand, ex-
ploration is met if the acquisition function chooses conﬁgurations that are very
distant to all observed conﬁgurations, i.e. explores new areas of the hyperparam-
eter space, where the surrogate model is quite uncertain about its prediction.
Given that usually only a few initial observations are present and the amount
of overall queries for hyperparameter conﬁgurations is limited, a decent tradeoﬀ
between both exploration and exploitation is desired.

More recent work is inspired by the area of meta learning, where the goal
is to transfer knowledge for parameters of a given model from having learned
this model already on other data sets [4]. Thus, these methods propose to also
take into account the knowledge of hyperparameter performances on diﬀerent

3

(past) data sets, where hyperparameter opimization has already been done. This
is quite intuitive, as every experienced practitioner, who has already learned a
model many times on diﬀerent data sets probably comes up with better hyperpa-
rameter conﬁgurations for the target data set to test initially. In many works, the
surrogate model is then learned on the hyperparameter performances of past data
sets and therefore has a better knowledge of well-performing hyperparameters
to choose. In order for the surrogate model to not confuse performances of the
same hyperparameter conﬁguration on diﬀerent data sets, the meta knowledge
is usually augmented by additional meta features that describe characteristics
of a data set.

Many surrogate models have been proposed, but one of the simplest surro-
gates is probably a Gaussian process (GP), as it is relatively simple to learn,
delivers good predictions and furthermore, due to its probabilistic nature, allows
for a direct estimation of uncertainties, which is a key ingredient for SMBO.
Another advantage of using Gaussian processes compared to other surrogate
models is that they are basically hyperparameter free, as all the parameters that
we have to specify for the kernel can be learned by optimizing their marginal
log likelihood. However, Gaussian processes have one huge drawback which lies
in their scalability. In order to learn a Gaussian process, the kernel matrix com-
puted over all observed instances has to be inverted which is an operation with
cubic expense in the number of observations. Thus, if we seek to include meta
knowledge of many past data sets into the training data of the Gaussian pro-
cess, learning the Gaussian process might even take more time and memory
than learning the model we seek to optimize the hyperparameters for, which
then renders a Gaussian process infeasible, despite its advantages.

In this paper, we propose to use a product of Gaussian process experts as
surrogate model, where basically an independent GP is learned for all the obser-
vation of one past data set and in the end all the predictions of the individual
experts are assembled to predict hyperparameter performances of the target data
set. Following this approach, our work has four main contributions:

(cid:73) We learn a product of GP experts, which allows for the inclusion of a large

amount of meta information,

(cid:73) by using GPs as base surrogate model, we employ surrogates that are very

easy and fast to learn, and do not require much memory
(cid:73) additionally, by using GPs, we do not introduce additional

surrogate-hyperparameters in opposition to many state of the art methods,
(cid:73) ﬁnally, we show empirically that products of GP experts perform very com-
petitively for hyperparameter optimization against a variety of published
competitors, as well as make both the implementation and the meta data
publicly available.

2 Related Work

As already mentioned, in the recent years there has been a growing interest
in research regarding hyperparameter optimization. Random search has been

4

proposed as an alternative to grid-search and works well in cases of low eﬀective
dimensionality, where a subspace of the hyperparameter space does not inﬂuence
the results as much as the remaining hyperparameter dimensions [3].

In the context of SMBO, many diﬀerent surrogate models have been pro-
posed in a variety of papers. At ﬁrst, an independent Gaussian process [17] was
used. We denote it as independent as it does not learn across data [20]. Secondly,
random forests have been proposed as surrogates and inherit the ability to work
well with non numerical as well as hierarchical hyperparameters [13]. Regard-
ing hyperparameter optimization using meta knowledge, a stacking of a GP on
top of a ranking SVM was proposed [2], as well as a Gaussian process with a
multi task kernel in two closely related works [26] [21]. Furthermore, a mixture
of a multilayer perceptron and a factorization machine has been employed as
surrogate model [18], which automatically learns data set representations and
therefore does not necessarily need meta features.

A diﬀerent aspect of using meta knowledge is conducted through learning an
initialization of well-performing hyperparameters. The ﬁrst work in this context
is [8] where the initial hyperparameters are chosen based on data sets that are
closest with respect to the Euclidean distance evaluated on the meta features of
the respective data sets. This intuition has been extended by [24] which uses a
diﬀerentiable plug in estimator to compute initial hyperparameters. Finally, [25]
employs a static sequence of hyperparameters that is learned using meta knowl-
edge and does not need a surrogate model at all, however, it has the drawback
that it needs meta information over diﬀerent data sets evaluated on the same
hyperparameter grids.

There is a plethora of other approaches that are either model speciﬁc [1] or
use genetic algorithms [15] [7], or do both in conjunction [9]. As these approaches
are not embedded in the context of SMBO, we will leave them out of further
discussions.

Since we are seeking to employ product of experts models in the framework
of SMBO-based hyperparameter optimization, we also review the related work
in this ﬁeld as well as various techniques to speed up Gaussian process learning.
Initially, product of experts models have been proposed by [11] alongside with a
learning algorithm [12] to train the parameters of such a model. The generalized
product of experts [5] introduces additional weighting factors within the product
in order to reduce the overconﬁdence of the product of experts in unknown areas.
Another model that also estimates a joint probability density given by a set of
experts is the Bayesian committee machine [22], which includes the prior in its
predictions. Finally, the work by [6] combines both the idea of the generalized
POE with its weighting factors with the Bayesian committee machine. We do
want to highlight that all of this work is not speciﬁcally tailored to Gaussian
processes, however, [6] argues that using products of experts is an easy way to
make Gaussian processes more scalable to larger training data sets.

Additionally, many eﬀorts have been made by the means of sparse GPs,
namely Gaussian processes learned on subsets of the original training data such
as [19] which employs kd-trees for subsampling. There are many more works in

5

this area such as [23], [10] or [16], however, as we want to make use of the rich
meta information of hyperparameter performance on other data sets using only
a subset of the meta information seems counterintuitive. Due to this reason,
we do not intend to use sparse GPs as surrogates for Bayesian hyperparameter
optimization.

3 Background

In this section we ﬁrst review hyperparameter optimization and sequential model-
based optimization in general, secondly, we discuss Gaussian processes shortly
and lastly we give a review of product of experts models which we ultimately
seek to employ as surrogate models.

3.1 Problem Setting

Let D denote by the space of all data sets, following the notation by [3], we denote
a learning algorithm for a ﬁxed model class M by a mapping A : Λ × D −→ M.
Thus, an algorithm A is essentially a mapping from a given hyperparameter
conﬁguration and training data to a model which is learned by minimizing a loss
functional. In many cases, the hyperparameter space Λ is the cartesian product
of lower dimensional spaces. Now we can deﬁne the problem of hyperparameter
optimization as choosing the hyperparameter conﬁguration λ(cid:63) which minimizes
the loss of a learned model on given validation data:

λ(cid:63) := arg min

L(A(λ, Dtrain), Dval) =: arg min

b(λ, D) .

(1)

λ∈Λ

λ∈Λ

Please note that we use the short b as notation for the process of learning a model
on training data with given hyperparameters and evaluating it on validation
data. Clearly, b is the black box function that we seek to optimize using Bayesian
optimization.

3.2 Sequential Model-Based Optimization

The SMBO framework is depicted in Algorithm 1. It starts by learning a sur-
rogate model denoted by Ψ such that Ψ ≈ b on a set of given hyperparameter
performances which are stored in the observation history H. Secondly, the surro-
gate model will be used to predict the hyperparameter performance of unknown
hyperparameters, these predictions as well as the uncertainties will be forwarded
to the acquisition function a, which then picks a hyperparameter conﬁguration
to test. The most commonly used acquisition function is Expected Improvement
(EI) and can be computed analytically if one assumes the probability of im-
provement to be Gaussian [14]. Having chosen a candidate conﬁguration, b will
be evaluated for the proposed hyperparameter conﬁguration, the result will be
fed into the observation history and the process is repeated for T many times
until ﬁnally a best hyperparameter conﬁguration λbest is found. Additionally, the

6

Algorithm 1 Sequential Model-based Optimization Across Data Sets
Input: Hyperparameter space Λ, observation history H, target data set D, number of
iterations T , acquisition function a, surrogate model Ψ , initial best hyperparameter
conﬁguration λbest.

Output: Best hyperparameter conﬁguration λbest for D
1: for t = 1 to T do
Fit Ψ to H
2:
λnew = arg max
3:

a (Ψ (λ, D), H)

λ∈Λ

Evaluate b (λnew, D)
if b(λnew, D) < b(λbest, D) then

4:
5:
6:
7: H = H ∪ (λnew, b (λnew, D))
8: return λbest

λbest = λnew

surrogate model’s feature vector is usually augmented by meta features, which
are descriptive features of a data set, to allow the surrogate model to distinguish
between diﬀerent data sets.

3.3 Gaussian Processes

We introduce Gaussian processes as we use them as base models in a product of
experts. Given is a regression problem of the form

y(x) = f (x) + (cid:15) ,

(2)

where we assume i.i.d. noise (cid:15) ∼ N (0, σ2). A Gaussian process assumes that
for a given set of input variables X = (x1, ..., xN ) with associated labels y =
(y1, ..., yN ) the labels are multivariate Gaussian distributed y ∼ N (0, K), where
K is a covariance matrix that is deﬁned through a positive semideﬁnite kernel
function k(x, x(cid:48)). A very common choice for k is the squared exponential kernel

k(x, x(cid:48)) = exp

+ σ2δ(x = x(cid:48)) ,

(3)

(cid:18) −(cid:107)x − x(cid:48)(cid:107)2
2σ2
l

(cid:19)

where θ = (σl, σ) are denoted as the hyperparameters and the δ function returns
1 if its predicate is true and 0 otherwise. Given a set of known observations, the
conditional distribution of a label f(cid:63) given its input x(cid:63) is Gaussian distributed
with mean and covariance

(cid:63) K −1y
µ(f(cid:63)) = k(cid:62)
σ2(f(cid:63)) = k(cid:63)(cid:63) − k(cid:62)

(cid:63) K −1k(cid:63) ,

(4)

(5)

where k(cid:63) = (k(x1, x(cid:63)), ..., k(xn, x(cid:63))) is the vector of kernel evaluations of the new
input x(cid:63) to all observed inputs and k(cid:63)(cid:63) = k(x(cid:63), x(cid:63)) is the prior covariance of f(cid:63).
As we can see, using a GP for predictions requires inverting the kernel matrix of
size N , which is an operation of O(N 3) and thus becomes infeasible for data sets

7

with many instances. Recalling that our primary goal was to include large scale
meta knowledge of past hyperparameter performances, this boundary might be
reached very soon, which would force us to throw valuable data away or rely
on other surrogate models. However, solving the linear system of equations for
inversion of K can be reduced to O(N 2) by using a Cholesky decomposition of
the kernel matrix [17].

The kernel hyperparameters can be learned by maximizing their marginal
log likelihood using standard optimization techniques such as gradient ascent.
Again, for optimizing the kernel hyperparameters, we have to invert the kernel
matrix as well as compute its determinant, which also both scale cubically in
the dimension of K. As gradient ascent might use several iterations to converge
to a useful θ, this inversion becomes even more the bottle neck with respect to
both computational speed as well as memory usage.

3.4

(Generalized) Product of Experts (POE)

In order to scale Gaussian processes to a large training data set (i.e. observation
history) we will use product of experts models [11], of which several variants
have been proposed. Within a product of GP experts, a set of M invididual
Gaussian processes are learned on M disjoint subsets of the training data, so let
us decompose our training data as

X = (X (1), ..., X (M ))

y = (y(1), ..., y(M )) ,

(6)

such that the individual subsets of instances and labels are disjoint. Then, follow-
ing the independence assumption, the marginal joint likelihood factorizes into a
product of single likelihoods

p(y | X, θ) =

(cid:16)

y(i) | X (i), θ(i)(cid:17)

.

pi

(7)

Thus, in order to learn the individual experts, we only need to invert kernel
matrices of the size of roughly N/M , thus learning the individual experts can be
done in O(N 3/M 3) which is a reasonable reduction for a suﬃciently large enough
M . In this way, we also learn M many diﬀerent sets of kernel hyperparameters.
As the M experts have been learned, we can compute the marginal likelihood
by multiplying all individual likelihoods. The generalized product of experts [5]
introduces additional weighting factors βi such that:

p(y | X, θ) =

(cid:16)

y(i) | X (i), θ(i)(cid:17)

.

βi

pi

(8)

M
(cid:89)

i=1

M
(cid:89)

i=1

Naturally, if all βi = 1, we arrive at the initial formulation of equation 7. Comput-
ing the product of the individual likelihoods yields a density that is proportional

8

to a Gaussian with following mean and precision:

µpoe(f(cid:63)) = (σpoe(f(cid:63)))2

βiσ−2
i

(f(cid:63))µi(f(cid:63))

M
(cid:88)

i=1

(σpoe(f(cid:63)))−2 =

βiσ−2
i

(f(cid:63))

M
(cid:88)

i=1

(9)

(10)

i

Essentially, by replacing σ−2
(f(cid:63)) = τi(f(cid:63)) with the precision, we see that the
mean predicted by the product of experts is a sum of means, weighted by the
product of the individual βi and the precision τi, which is then divided by the
total sum of weighting factors. Usually, the βi are set such that (cid:80)
i βi = 1,
surprisingly, this already works quite well for βi ≡ 1/M . This does not change
the mean as the multiplication with the precision cancels out the eﬀect, however,
the precisions eﬀectively get weighed down and decrease the overconﬁdence of
the initial product of experts without any weights.

3.5 Product of Experts in SMBO

Having introduced the product of experts models, their implementation for hy-
perparameter optimization in the SMBO framework seems straightforward. How-
ever, a few questions still remain unanswered. At ﬁrst, we split the meta knowl-
edge into all the instances belonging to hyperparameter performances of one
data set. In this way, each expert will be learned on the meta information of
one distinct data set. If this would still be too large for a GP to learn, we could
further subdivide them into smaller subsets.

Secondly, the question of how the information on the target data set will be
incorporated into the surrogate model remains. We seek for two alternatives, in
the ﬁrst one we simply add the information of new points on the target data set
to all the experts in the ensemble. Doing this, we eﬀectively train all experts to
be expert for two data sets, the initial one they have been trained on plus the
target data set. In our implementation, we then follow the intution of all weights
summing up to one, thus we set all βi = 1/M .

As an independent Gaussian process that is learned without any meta knowl-
edge already behaves reasonably well as a surrogate model, we also tried another
alternative. We still feed the target data set information into all experts learned
in the ensemble but additionally create a new GP that carries only the informa-
tion of the target data set and is weighed much higher than the individual ex-
perts. Speciﬁcally, we use βi = 1/2M for the individual experts and βM +1 = 1/2
for the GP learned on only the target data set responses. In this way, we use the
meta information as well as the strength of an independent Gaussian process.

Additionally, we seek to scale the hyperparameter performances observed in
the meta data as well as the hyperparameter performances of the target data
set. This is due to the fact that the range of b naturally depends on the data
set. Consider for example a classiﬁcation problem where b models the misclassi-
ﬁcation rate of a classiﬁer for some test data. Naturally, for some data sets, very

9

low misclassiﬁcation rates might be achieved in contrast to other data sets that
are simply harder to classify. A POE might then be biased towards choosing
hyperparameter conﬁgurations that produce good results on simple data sets,
which is something we want to prevent. In order to do so, we scale the labels of
the meta data to become standard Gaussian distributed, for the target data set,
we do this on-the-ﬂy every time we see a new response of b as was also proposed
by [26].

4 Experiments

To evaluate the proposed surrogate models for hyperparameter optimization, we
conduct hyperparameter optimization within the SMBO framework including a
variety of published baselines. The experiments are performed on two meta data
sets that we have created ourselves.

4.1 Meta Data Set Creation

We have created two meta data sets for the task of classiﬁcation using two
distinct classiﬁers, namely being a support vector machine (SVM) and AdaBoost.
These meta data sets consists of a complete grid search for both classiﬁers on
50 classiﬁcation data sets that we have taken from the UCI repository1. If splits
were already given, we merged them into one complete data set, shuﬄed the
resulting data set and then took 80% of the data for training and the remaining
20% for testing. The AdaBoost meta data set was created by running AdaBoost2
with hyperparameters I ∈ {2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000}
and M ∈ {2, 3, 4, 5, 7, 10, 15, 20, 30}. This yields 108 meta instances per data set
and therefore the overall meta data set contains 5400 instances.

The second meta data set was created by running an SVM3 on all of the data
sets, with four hyperparameters. The ﬁrst one resembles the choice of kernel and
is categorical between a linear, a polynomial and an RBF kernel, thus intro-
duces three binary hyperparameters. The second hyperparameter is the tradeoﬀ
parameter, usually denoted as C, the third and fourth hyperparameter are the
degree d of the polynomial kernel and the width γ of the RBF kernel. If the ker-
nel hyperparameters are not used, i.e. the polynomial degree for an RBF kernel,
we set them to a constant value of zero. As for the AdaBoost meta data set, we
computed the misclassiﬁcation rates using grid-search, where C was chosen from
the set {2−5, . . . , 26}, the polynomial degree d was chosen from {2, . . . , 10} and
γ was chosen from {0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 20, 50, 100, 1000}.
This results in 288 runs per data set, and therefore the overall meta data set
contains up to 14, 400 instances.

Finally, we also added meta features to the meta data set, to allow the sur-
rogate models to distinguish between the same hyperparameter conﬁgurations

1 http://archive.ics.uci.edu/ml/index.html
2 http://www.multiboost.org
3 http://svmlight.joachims.org

10

evaluated on diﬀerent data sets. A list of all employed meta features can be seen
in Table 1. For our experiments, all features in the meta data set, namely the
computed meta features as well as the hyperparameter conﬁgurations have been
scaled to values in [0, 1].

Table 1. List of all meta-features used.

Number of Classes
Number of Instances
Log Number of Instances
Number of Features
Log Number of Features
Data Set Dimensionality
Log Data Set Dimensionality
Inverse Data Set Dimensionality

Log Inverse Data Set Dimensionality
Class Cross Entropy
Class Probability Min
Class Probability Max
Class Probability Mean
Class Probability Standard Deviation
Kurtosis Min
Kurtosis Max

Kurtosis Mean
Kurtosis Standard Deviation
Skewness Min
Skewness Max
Skewness Mean
Skewness Standard Deviation

4.2 Competing Surrogate Models

Random Search (RANDOM). This is a surrogate that simply picks a random
point out of the grid.

Random Forests (RF). Sequential Model-based Algorithm Conﬁguration [13]
employs a random forest as surrogate model and computes uncertainties using
the learned ensemble by estimating empirical means and standard deviations.

Independent Gaussian Process (IGP). An independent Gaussian process with
SE-ARD kernel that is only learned on the observations on the target data set,
this was proposed by [20].

Surrogate-based Collaborative Tuning (SCOT). This surrogate model is eﬀec-
tively a stacking of an SVMRANK and a Gaussian process and was proposed
by [2]. The ranking SVM learns how to rank hyperparameter conﬁgurations
across data sets, uncertainties are estimated by stacking a GP on the ranked
output.

Full Gaussian Process (FGP). A Gaussian process with SE-ARD kernel that
is learned on the whole meta data set. This is basically the model we seek to
approximate by learning a product of experts.

Gaussian Process with MKL (MKLGP). This surrogate was proposed by [26]
and learns basically a full GP over the whole meta data set using a combination
of an SE-ARD kernel and a kernel function that models the distances between
data sets based on meta features.

Factorized Multilayer Perceptron (FMLP). The surrogate model that was pro-
posed by [18], which learns a multilayer perceptron and factorizes the weights in
the ﬁrst layer in order to learn latent data set and hyperparameter representa-
tions. Uncertainties are estimated by learning an ensemble of FMLPs.

11

Product of Gaussian Process Experts (POGPE). This surrogate model learns
a product of GP experts as described in Section 3.4. Each expert employs an
SE-ARD kernel. Information of the target data set is distributed to all experts,
which are all weighted equally.

Single Gaussian Process Expert (SGPE). This surrogate model also learns a
product of GP experts as described in Section 3.4, however, also learns an inde-
pendent GP for the target data set only and weighs the target GP as much as
the whole set of experts.

4.3 Experimental Setup

Our experiments are performed in a leave-one-out fashion, meaning that we
train the surrogate model on 49 data sets and use the meta knowledge to start
SMBO on the remaining test data set. To cancel out random eﬀects, we ran
all experiments for a total of 100 times and averaged the results in the end.
In total, each SMBO run was allowed to test T = 70 diﬀerent hyperparameter
conﬁgurations on the test data. As acquisition function we employed the pop-
ular expected improvement, which is by now the most widely used acquisition
function in hyperparameter optimization using the SMBO framework.

As evaluation metric, we use the average rank, where, for each target data
set, we rank all competing surrogate models based on the best misclassiﬁcation
rate they have found so far. Ties are being solved by granting the average rank,
i.e. if one surrogate models ﬁnd the misclassiﬁcation rates 0.2, another two ﬁnd
0.25 and a third one ﬁnds only 0.5, we would rank the surrogates with 1, 2.5,
2.5 and 4. As we run the experiments for 50 diﬀerent target data sets, we report
the average of all average ranks.

The implementations were largely done by ourselves, except for SMAC and

SCOT, where we used MLTK4 for the former and the implementation by
Joachims5 for the ranking SVM used in SCOT. All hyperparameters of the GP
based models have been automatically tuned by maximizing their marginal likeli-
hood, for FMLP we used the setting proposed by the authors. For SMAC, SCOT
and MKLGP we used leave-one-out cross validation to tune the hyperparame-
ters. For all GP-based models, we implemented the Cholesky decomposition to
speed up the inversion of kernel matrices. In order to facilitate reproducibility
of our experimental results, we make the program code as well as the employed
meta data sets publicly available on Github6.

4.4 Performance in SMBO

The average rank among all competing methods can be seen in Figure 1, where
the left plot shows the average rank on AdaBoost versus the number of trials

4 http://www.cs.cornell.edu/~yinlou/projects/mltk/
5 http://svmlight.joachims.org/
6 https://github.com/nicoschilling/ECML2016

12

conducted, and the right one shows the results for the SVM meta data set. First
of all, we see that for both meta data sets the random baseline shows the worst
performance as is expected. Surprisingly, for both meta data sets, POGPE and
SGPE ﬁnd the best hyperparameter among the competitors in the ﬁrst trial.
During the SMBO procedure, both the full Gaussian process as well as MKLGP
perform better on the AdaBoost data set, however, we observe that POGPE
performs better on the SVM data set which is quite a surprise. POGPE, despite
its good starting point, is being outperformed by FMLP on the SVM data set
in the ﬁrst 15 trials, which then degrades in performance and performs worse
than both full GP approaches. Comparing both of these with each other, we
see that they perform almost equally, however, MKLGP tends to have worse
starting points than a simple full GP. For both meta data sets, we see the lift of
including meta knowledge through comparison with the independent GP, which
performs reasonably on AdaBoost but degrades on SVM. This observation leads
us to the conclusion that optimizing the hyperparameters of AdaBoost seems
In contrast to POGPE, SGPE does not seem to
an easier task than on SVM.

Fig. 1. Average Rank of all competing methods. The left plot shows results for Ad-
aBoost, the right plot shows results for the SVM meta data set.

perform that well, maybe the tradeoﬀ between product of experts and single
GP has to be adjusted for each trial, however, this would introduce another
hyperparameter for the surrogate model, which we do not seek to do.

Overall, we conclude that POGPE, FMLP, FGP and MKLGP are among
the best performing surrogate models, so for these models we also computed the
average distance to the minimum in terms of b. For each data set, we scale all
values of b (in this case accuracies) to be in [0, 1]. Then, again for each data set,

13

Fig. 2. Average Distance to Minimum of the best performing methods. The left plot
shows results for AdaBoost, the right plot shows results for the SVM meta data set.

we compute the distance of the best hyperparameter response so far to the best
on the overall grid. This value is then averaged to become the average distance
to the minimum, which gives an idea of how a surrogate model makes use of the
responses it gets on the target data set. The results can be seen in Figure 2, where
the left plot shows the results for AdaBoost and the right for SVM. Overall, we
see the same behaviour as we have seen in average rank, however, FMLP seems to
be a little bit better here. For AdaBoost, it achieves the lowest average distance,
this is due to FMLP winning severely against its competitors on the data set
sonar-scale, where in the average rank, this win does not count that much. In
conclusion, we see that POGPE works really well on both evaluatuion metrics,
especially when we consider its simplicity in the light of POGPE actually being
an approximation of a full GP.

4.5 Runtime Experiments

In order to demonstrate the scalability of using POGPE, we have also conducted
a runtime experiment. We have measured the runtime of the most competitive
methods, namely being POGPE, FMLP, and both of the full Gaussian process
approaches FGP and MKLGP. Experiments were conducted on a Xeon E5-
2670v2 with 2.50GHz clock speed and 64 GB of RAM, where we again performed
a total of 70 trials of an SMBO run on the SVM meta data set. To account for
measurement noise, we repeated all experiments 10 times.

The results can be seen in Figure 3, where the left plot shows the cumulative
runtime in seconds without the initial training time of the surrogate in opposition
to the right plot which includes it. We plot both results as the training of the

14

Fig. 3. Runtime comparison among the most competitive surrogate models. The left
plot shows the cumulative runtime in seconds.

surrogate model can be performed in an oﬄine fashion while waiting for new
data. As we do not take into account the learning of the actual model, i.e. the
evaluation of b, both plots can be understood as the total overhead time of
running SMBO instead of using default hyperparameters. We can observe that
POGPE consumes drastically less time than all its competitors, simply due to
the fact that we have to invert much smaller kernel matrices. By excluding the
potentially oﬄine training time, a full GP is faster than FMLP, however, if the
full GP needs to be trained ﬁrst an FMLP is faster but gets overtaken with
respect to computation time if only enough trials are performed. In both plots,
MKLGP requires the most computation time. Considering that these diﬀerences
will be bigger if we use more meta information, we conclude that POGPE is very
fast while performing also very well in the SMBO procedure.

5 Conclusions

In this paper, we proposed to choose POE models as surrogate models for hyper-
parameter tuning, speciﬁcally we chose to employ Gaussian processes because of
their fairly easy implementation as well as their predictive performance in the
ﬁeld of Bayesian optimizazion. We do acknowledge that POGPE is not the best
model in all experiments, but is quite competitive which is a surprise due to its
simplicity and its approximative nature. In the very ﬁrst trial both POGPE and
SGPE (as they start out the same) on average pick the best hyperparameter con-
ﬁguration compared to all competitor methods, which shows how eﬃcient usage
of the meta data can simply be made by learning a product of experts on each
invidual data set and querying the committee. Moreover, the other competitive

15

surrogate models such as FMLP and MKLGP introduce additional hyperpa-
rameters for the surrogate model that need to be optimized. For FMLP, tuning
of the network architecture such as number of layers and number of nodes per
layer as well as setting correct learning rates is demanded. MKLGP requires
tuning of the number of neighboring data sets and the tradeoﬀ term between
both employed kernels. In comparison, a simple product of GP experts does not
require any hyperparameter tuning, as the GP parameters can be learned by
maximizing their marginal likelihood quite eﬀectively.

As we have seen in the results, POGPE can also be trained much faster than
the other competitive surrogate models. In the light of big data we will probably
have access to also growing meta data sets that we can employ for hyperpa-
rameter optimization, which makes scalable use of the meta data a necessity.
Moreover, POE models are easy to parallelize which allows easy usage in dis-
tributed scenarios. Out of all these reasons we see them as a very reasonable
choice to pick as surrogate models for hyperparameter optimization including
large scale meta data.

Acknowledgments The authors gratefully acknowledge the co-funding of their
work by the German Research Foundation (DFG) under grant SCHM 2583/6-1.

References

1. Adankon, M.M., Cheriet, M.: Model selection for the LS-SVM. Application to

handwriting recognition. Pattern Recognition 42(12), 3264–3270 (2009)

2. Bardenet, R., Brendel, M., Kegl, B., Sebag, M.: Collaborative hyperparameter
tuning. In: Dasgupta, S., Mcallester, D. (eds.) Proceedings of the 30th Interna-
tional Conference on Machine Learning (ICML-13). vol. 28, pp. 199–207. JMLR
Workshop and Conference Proceedings (May 2013)

3. Bergstra, J., Bengio, Y.: Random search for hyper-parameter optimization. J.

Mach. Learn. Res. 13, 281–305 (Feb 2012)

4. Brazdil, P., Carrier, C.G., Soares, C., Vilalta, R.: Metalearning: Applications to

data mining. Springer Science & Business Media (2008)

5. Cao, Y., Fleet, D.J.: Generalized product of experts for automatic and principled
fusion of gaussian process predictions. arXiv preprint arXiv:1410.7827 (2014)
6. Deisenroth, M.P., Ng, J.W.: Distributed gaussian processes. In: International Con-

ference on Machine Learning (ICML). vol. 2, p. 5 (2015)

7. Escalante, H.J., Montes, M., Sucar, L.E.: Particle swarm model selection. The

Journal of Machine Learning Research 10, 405–440 (2009)

8. Feurer, M., Springenberg, J.T., Hutter, F.: Initializing bayesian hyperparameter
optimization via meta-learning. Proceedings of the Twenty-Ninth AAAI Confer-
ence on Artiﬁcial Intelligence (2015)

9. Guo, X.C., Yang, J.H., Wu, C.G., Wang, C.Y., Liang, Y.C.: A novel ls-svms hyper-
parameter selection based on particle swarm optimization. Neurocomput. 71(16-
18), 3211–3215 (Oct 2008)

10. Hensman, J., Fusi, N., Lawrence, N.D.: Gaussian processes for big data. arXiv

preprint arXiv:1309.6835 (2013)

16

11. Hinton, G.E.: Products of experts. In: Artiﬁcial Neural Networks, 1999. ICANN
99. Ninth International Conference on (Conf. Publ. No. 470). vol. 1, pp. 1–6. IET
(1999)

12. Hinton, G.E.: Training products of experts by minimizing contrastive divergence.

Neural computation 14(8), 1771–1800 (2002)

13. Hutter, F., Hoos, H.H., Leyton-Brown, K.: Sequential model-based optimization
for general algorithm conﬁguration. In: Proceedings of the 5th International Con-
ference on Learning and Intelligent Optimization. pp. 507–523. LION’05, Springer-
Verlag, Berlin, Heidelberg (2011)

14. Jones, D.R., Schonlau, M., Welch, W.J.: Eﬃcient global optimization of expensive

black-box functions. J. of Global Optimization 13(4), 455–492 (Dec 1998)

15. Koch, P., Bischl, B., Flasch, O., Bartz-Beielstein, T., Weihs, C., Konen, W.: Tuning
and evolution of support vector kernels. Evolutionary Intelligence 5(3), 153–170
(2012)

16. Quinonero-Candela, J., Rasmussen, C.E.: A unifying view of sparse approximate
gaussian process regression. The Journal of Machine Learning Research 6, 1939–
1959 (2005)

17. Rasmussen, C.E., Williams, C.K.I.: Gaussian Processes for Machine Learning

(Adaptive Computation and Machine Learning). The MIT Press (2005)

18. Schilling, N., Wistuba, M., Drumond, L., Schmidt-Thieme, L.: Hyperparameter
optimization with factorized multilayer perceptrons. In: Machine Learning and
Knowledge Discovery in Databases, pp. 87–103. Springer (2015)

19. Shen, Y., Ng, A., Seeger, M.: Fast gaussian process regression using kd-trees. In:
Proceedings of the 19th Annual Conference on Neural Information Processing Sys-
tems. No. EPFL-CONF-161316 (2006)

20. Snoek, J., Larochelle, H., Adams, R.P.: Practical bayesian optimization of machine
learning algorithms. In: Pereira, F., Burges, C., Bottou, L., Weinberger, K. (eds.)
Advances in Neural Information Processing Systems 25, pp. 2951–2959. Curran
Associates, Inc. (2012)

21. Swersky, K., Snoek, J., Adams, R.P.: Multi-task bayesian optimization. In: Burges,
C., Bottou, L., Welling, M., Ghahramani, Z., Weinberger, K. (eds.) Advances in
Neural Information Processing Systems 26, pp. 2004–2012. Curran Associates, Inc.
(2013)

22. Tresp, V.: A bayesian committee machine. Neural Computation 12(11), 2719–2741

(2000)

23. Williams, C., Seeger, M.: Using the nyström method to speed up kernel machines.
In: Proceedings of the 14th Annual Conference on Neural Information Processing
Systems. pp. 682–688. No. EPFL-CONF-161322 (2001)

24. Wistuba, M., Schilling, N., Schmidt-Thieme, L.: Learning hyperparameter opti-
mization initializations. In: Data Science and Advanced Analytics (DSAA), 2015.
36678 2015. IEEE International Conference on. pp. 1–10. IEEE (2015)

25. Wistuba, M., Schilling, N., Schmidt-Thieme, L.: Sequential model-free hyperpa-
rameter tuning. In: Data Mining (ICDM), 2015 IEEE International Conference
on. pp. 1033–1038. IEEE (2015)

26. Yogatama, D., Mann, G.: Eﬃcient transfer learning method for automatic hy-
perparameter tuning. In: International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS 2014) (2014)

