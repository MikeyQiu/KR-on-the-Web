Deep RTS: A Game Environment for
Deep Reinforcement Learning in
Real-Time Strategy Games

Per-Arne Andersen
Department of ICT
University of Agder
Grimstad, Norway
per.andersen@uia.no

Morten Goodwin
Department of ICT
University of Agder
Grimstad, Norway
morten.goodwin@uia.no

Ole-Christoffer Granmo
Department of ICT
University of Agder
Grimstad, Norway
ole.granmo@uia.no

8
1
0
2
 
g
u
A
 
5
1
 
 
]
I

A
.
s
c
[
 
 
1
v
2
3
0
5
0
.
8
0
8
1
:
v
i
X
r
a

Abstract—Reinforcement learning (RL) is an area of research
that has blossomed tremendously in recent years and has shown
remarkable potential for artiﬁcial intelligence based opponents
in computer games. This success is primarily due to the vast
capabilities of convolutional neural networks, that can extract
useful features from noisy and complex data. Games are excellent
tools to test and push the boundaries of novel RL algorithms
because they give valuable insight into how well an algorithm
can perform in isolated environments without the real-life con-
sequences. Real-time strategy games (RTS) is a genre that has
tremendous complexity and challenges the player in short and
long-term planning. There is much research that focuses on
applied RL in RTS games, and novel advances are therefore
anticipated in the not too distant future. However, there are to
date few environments for testing RTS AIs. Environments in the
literature are often either overly simplistic, such as microRTS, or
complex and without the possibility for accelerated learning on
consumer hardware like StarCraft II. This paper introduces the
Deep RTS game environment for testing cutting-edge artiﬁcial
intelligence algorithms for RTS games. Deep RTS is a high-
performance RTS game made speciﬁcally for artiﬁcial intelligence
research. It supports accelerated learning, meaning that it can
learn at a magnitude of 50 000 times faster compared to existing
RTS games. Deep RTS has a ﬂexible conﬁguration, enabling
research in several different RTS scenarios, including partially
observable state-spaces and map complexity. We show that Deep
RTS lives up to our promises by comparing its performance
with microRTS, ELF, and StarCraft II on high-end consumer
hardware. Using Deep RTS, we show that a Deep Q-Network
agent beats random-play agents over 70% of the time. Deep
RTS is publicly available at https://github.com/cair/DeepRTS.

Index Terms—real-time strategy game, deep reinforcement

learning, deep q-learning

I. INTRODUCTION

Despite many advances in Artiﬁcial Intelligence (AI) for
games, no universal Reinforcement learning (RL) algorithm
can be applied to complex game environments without ex-
tensive data manipulation or customization. This includes
traditional Real-time strategy games (RTS) such as WarCraft
III, StarCraft II, and Age of Empires. RL has recently been
applied to simpler game environments such as those found
in the Arcade Learning Environment [1](ALE) and board
games [2] but has not successfully been applied to more

advanced games. Further, existing game environments that
target AI research are either overly simplistic such as ALE
or complex such as StarCraft II.

RL has in recent years had tremendous progress in learning
how to control agents from high-dimensional sensory inputs
like images. In simple environments, this has been proven to
work well [3], but are still an issue for complex environments
with large state and action spaces [4]. The distinction between
simple and complex tasks in RL often lies in how easy it is
to design a reward model that encourages the algorithm to
improve its policy without ending in local optima [5]. For
simple tasks, the reward function can be described by only a
few parameters, while in more demanding tasks, the algorithm
struggles to determine what the reward signal is trying to
accomplish [6]. For this reason, the reward function is in liter-
ature often a constant or single-valued variable for most time-
steps, where only the ﬁnal time-step determines a negative or
positive reward [7]–[9]. In this paper we introduce Deep RTS,
a new game environment targeted deep reinforcement learning
(DRL) research. Deep RTS is an RTS simulator inspired by
the famous StarCraft II video game by Blizzard Entertainment.
This paper is structured as follows. First, Section II and
Section III thoroughly outlines previous work and central
achievements using game environments for RL research. Next,
Section IV introduces the Deep RTS game environment.
Section V presents the Deep RTS performance, a compari-
son between well-established game environments and Deep
RTS, and experimental results using Deep Q-Network as an
agent in Deep RTS. Subsequently, Section VI concludes the
contribution of this paper and outlines a roadmap for future
work.

II. RELATED GAME ENVIRONMENTS

There exist several exciting game environments in the
literature that focus on state-of-the-art research in AI algo-
rithms. Few game environments target the RTS-genre. One
the reason may be because these environments are by nature
challenging to solve, and there are few ways to ﬁt results
with preprocessing tricks. It is, however, essential to include

RTS as part of the active research of deep reinforcement
learning algorithms as they feature long-term planning. This
section outlines a thorough literature review of existing game
platforms and environments and is summarized in Table I.

to allow research on complex systems where only a fraction
of the state-space is available [18]. In literature, TorchCraft
has been used for deep learning research [26], [27]. There is
also a dataset that provides data from over 65,000 StarCraft
replays [28].

TABLE I
SELECTED GAME ENVIRONMENTS THAT IS ACTIVELY USED IN
REINFORCEMENT LEARNING RESEARCH

E. Malmo Platform

Platform

ALE
Malmo Platform
ViZDoom
DeepMind Lab
OpenAI Gym
OpenAI Universe
Stratagus
microRTS
TorchCraft
ELF
SC2LE
Deep RTS

RTS
No
No
No
No
No
No
Yes
Yes
Yes
Yes
Yes
Yes

Complex1
No
Yes
Yes
Yes
No
Yes
Yes
No
Yes
Yes
Yes
Yes

Year
2012
2016
2016
2016
2016
2016
2005
2013
2016
2017
2017
2018

Solved
Yes
No
No
No
No
No
No
No
No
No
No
No

Source
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[8]
-

A. Stratagus

Stratagus is an open source game engine that can be used
to create RTS-themed games. Wargus, a clone of Warcraft II,
and Stargus, a clone of StarCraft I are examples of games
implemented in the Stratagus game engine. Stratagus is not
an engine that targets machine learning explicitly, but several
researchers have performed experiments in case-based reason-
ing [20], [21] and q-learning [22] using Wargus. Stratagus is
still actively updated by contributions from the community.

B. Arcade Learning Environment

Bellemare et al. provided in 2012 the arcade learning
environment that enabled researchers to conduct cutting-edge
research in general deep learning [10]. The package provided
hundreds of Atari 2600 environments that in 2013 allowed
Minh et al. to do a breakthrough using Deep Q-Learning and
A3C. The platform has been a critical component in several
advances in RL research. [1], [3], [23]

C. microRTS

microRTS is a simple RTS game, designed to conduct AI
research. The idea behind microRTS is to strip away the
computational heavy game logic to increase the performance
and to enable researchers to test theoretical concepts quickly
[17]. The microRTS game logic is deterministic, and include
options for full and partially-observable state-spaces. The
primary ﬁeld of research in microRTS is game-tree search
techniques such as variations of Monte-Carlo tree search and
minimax [17], [24], [25].

D. TorchCraft

In 2016, a research group developed TorchCraft, a bridge
that enables research in the game StarCraft. TorchCraft intends
to provide the reinforcement learning community with a way

1A Complex environment has an enormous state-space, with reward signals

that are difﬁcult to correlate to an action.

The Malmo project is a platform built atop of the popular
game Minecraft. This game is set in a 3D environment where
the object is to survive in a world of dangers. The paper The
Malmo Platform for Artiﬁcial Intelligence Experimentation by
Johnson et al. claims that the platform has all characteristics
qualifying it to be a platform for general artiﬁcial intelligence
research. [11]

F. ViZDoom

ViZDoom is a platform for research in visual reinforcement
learning. With the paper ViZDoom: A Doom-based AI Re-
search Platform for Visual Reinforcement Learning Kempka
et al. illustrated that an RL agent could successfully learn
to play the game Doom, a ﬁrst-person shooter game, with
behavior similar to humans. [29]

G. DeepMind Lab

With the paper DeepMind Lab, Beattie et al. released a
platform for 3D navigation and puzzle solving tasks. The
primary purpose of DeepMind Lab is to act as a platform
for DRL research. [13]

H. OpenAI Gym

In 2016, Brockman et al. from OpenAI released GYM
which they referred to as ”a toolkit for developing and com-
paring reinforcement learning algorithms”. GYM provides
various types of environments from following technologies:
Algorithmic tasks, Atari 2600, Board games, Box2d physics
engine, MuJoCo physics engine, and Text-based environments.
OpenAI also hosts a website where researchers can submit
their performance for comparison between algorithms. GYM
is open-source and encourages researchers to add support for
their environments. [14]

I. OpenAI Universe

OpenAI recently released a new learning platform called
Universe. This environment further adds support for environ-
ments running inside VNC. It also supports running Flash
games and browser applications. However, despite OpenAI’s
open-source policy, they do not allow researchers to add new
environments to the repository. This limits the possibilities of
running any environment. The OpenAI Universe is, however, a
signiﬁcant learning platform as it also has support for desktop
games like Grand Theft Auto IV, which allow for research in
autonomous driving [30].

J. ELF

The Extensive Lightweight Flexible (ELF) research plat-
form was recently present at NIPS with the paper ELF: An
Extensive, Lightweight and Flexible Research Platform for
Real-time Strategy Games. This paper focuses on RTS game
research and is the ﬁrst platform ofﬁcially targeting these types
of games. [19]

K. StarCraft II Learning Environment

SC2LE (StarCraft II Learning Environment) is an API
wrapper that facilitates access to the StarCraft II game-state
using languages such as Python. The purpose is to enable
reinforcement learning and machine learning algorithms to be
used as AI for the game players. StarCraft II is a complex
environment that requires short and long-term planning. It is
difﬁcult to observe a correlation between actions and rewards
due to the imperfect state information and delayed rewards,
making StarCraft II one of the hardest challenges so far in AI
research.

III. REINFORCEMENT LEARNING IN GAMES

Although there are several open-source game environments
suited for reinforcement learning, few of them are part of a
success story. One of the reasons for this is that current state-
of-the-art algorithms are seemingly unstable [30], and have
difﬁculties to converge towards optimal policy in environments
with multi-reward objectives [31]. This section exhibits the
most signiﬁcant achievements using reinforcement learning in
games.

A. TD-Gammon

TD-Gammon is an algorithm capable of reaching an expert
level of play in the board game Backgammon [7], [32]. The
algorithm was developed by Gerald Tesauro in 1992 at IBM’s
Thomas J. Watson Research Center. TD-Gammon consists of a
three-layer artiﬁcial neural network (ANN) and is trained using
a reinforcement learning technique called TD-Lambda. TD-
Lambda is a temporal difference learning algorithm invented
by Richard S. Sutton [33]. The ANN iterates over all possible
moves the player can perform and estimates the reward for
that particular move. The action that yields the highest reward
is then selected. TD-Gammon is the ﬁrst algorithm to utilize
self-play methods to improve the ANN parameters.

was generalized to work for Chess and Shogi (Japanese Chess)
only using 24 hours to reach a superhuman level of play [2].

C. DeepStack

DeepStack is an algorithm that can perform an expert
level play in Texas Hold’em poker. This algorithm uses
tree-search in conjunction with neural networks to perform
sensible actions in the game [35]. DeepStack is a general-
purpose algorithm that aims to solve problems with imperfect
information. The DeepStack algorithm is open-source and
available at https://github.com/lifrordi/DeepStack-Leduc.

D. Dota 2

DOTA 2 is a complex player versus player game where
the player controls a hero unit. The game objective is to
defeat the enemy heroes and destroy their base. In August
2017, OpenAI invented a reinforcement learning based AI that
defeated professional players in one versus one games. The
training was done by only using self-play, and the algorithm
learned how to exploit game mechanics to perform well within
the environment. DOTA 2 is used actively in research where
the next goal is to train the AI to play in a team-game based
environment.

IV. THE DEEP RTS LEARNING ENVIRONMENT

There is a need for new RTS game environments targeting
reinforcement learning research. Few game environments have
a complexity suited for current state-of-the-art research, and
there is a lack of ﬂexibility the existing solutions.

The Deep RTS game environment enables research at dif-
ferent difﬁculty levels in planning, reasoning, and control. The
inspiration behind this contribution is microRTS and StarCraft
II, where the goal is to create an environment that features
challenges between the two. The simplest conﬁgurations of
Deep RTS are deterministic and non-durative. Actions in the
non-durative conﬁguration are directly applied to the envi-
ronment within the next few game frames. This makes the
correlation between action and reward easier to observe. The
durative conﬁguration complicates the state-space signiﬁcantly
because it then becomes a temporal problem that requires
long-term planning. Deep RTS supports the OpenAI Gym
abstraction through the Python API and is a promising tool
for reinforcement learning research.

B. AlphaGO

A. Game Objective

In late 2015, AlphaGO became the ﬁrst algorithm to win
against a human professional Go player. AlphaGO is a re-
inforcement learning framework that uses Monte-Carlo tree
search and two deep neural networks for value and policy
estimation [9]. Value refers to the expected future reward from
a state assuming that the agent plays perfectly. The policy
network attempts to learn which action is best in any given
board conﬁguration. The earliest versions of AlphaGO used
training data from previous games played by human profes-
sionals. In the most recent version, AlphaGO Zero, only self-
play is used to train the AI [34]. In a recent update, AlphaGO

The objective of the Deep RTS challenge is to build a
base consisting of a town-hall, and then strive to expand the
base using gathered resources to gain the military upper hand.
Military units are used to conduct attacks where the primary
goal is to demolish the base of the opponent. Players start with
a worker unit. The primary objective of the worker units is
to expand the base offensive, defensive and to gather natural
resources found throughout the game world. Buildings can
further spawn additional units that strengthen the offensive
capabilities of the player. For a player to reach the terminal
state, all opponent units must be destroyed.

A regular RTS game can be represented in three stages:
early-game, mid-game and late-game. Early-game is the gath-
ering and base expansion stage. The mid-game focuses on the
military and economic superiority, while the late-game stage
is usually a deathmatch between the players until the game
ends.

adjustments of how many ticks equals a second. For each
iteration of the game-loop, the tick counter is incremented,
and the tick-timers are evaluated. By using tick-timers, the
game-state resembles how the StarCraft II game mechanics
function while lowering the tick-timer value better resembles
microRTS.

TABLE II
AN OVERVIEW OF AVAILABLE SCENARIOS FOUND IN THE DEEP RTS
GAME ENVIRONMENT

Scenario Name
10x10-2-FFA
15x15-2-FFA
21x21-2-FFA
31x31-2-FFA
31x31-4-FFA
31x31-6-FFA
solo-score
solo-resources
solo-army

Description
2-Player game
2-Player game
2-Player game
2-Player game
4-Player game
6-Player game
Score Accumulation
Resource Harvesting
Army Accumulation

Game Length
600-900 ticks
900-1300 ticks
2000-3000 ticks
6000-9000 ticks
8000-11k ticks
15k-20k ticks
1200 ticks
600 ticks
1200 ticks

Map Size
10x10
15x15
21x21
31x31
31x31
31x31
10x10
10x10
10x10

Because Deep RTS targets a various range of reinforcement
learning tasks,
there are game scenarios such as resource
gathering tasks, military tasks, and defensive tasks that nar-
rows the complexity of a full RTS game. Table II shows
nine scenarios currently implemented in the Deep RTS game
environment. The ﬁrst six scenarios are regular RTS games
with the possibility of having 6 active players in a free-for-all
setting. The solo-score scenario features an environment where
the objective is to only generate as much score as possible in
shortest amount of time. solo-resources is a game mode that
focuses on resource gathering. The agent must ﬁnd a balance
between base expansion and resource gathering to optimally
gather as many resources as possible. solo-army is a scenario
where the primary goal is to expand the military forces quickly
and launch an attack on an idle enemy. The Deep RTS game
environment enables researchers to create custom scenarios via
a ﬂexible conﬁguration interface.

B. Game Mechanics

TABLE III
CENTRAL CONFIGURATION FLAGS FOR THE DEEP RTS GAME ENGINE

Conﬁg Name
instant town hall
instant building
instant walking
harvest forever
auto attack
durative

Type
Bool
Bool
Bool
Bool
Bool
Bool

Description
Spawn Town-Hall at game start.
Non-durative Build Mode.
Non-durative Walk Mode.
Harvest resources automatically.
Automatic retaliation when being attacked.
Enable durative mode.

The game mechanics of the Deep RTS are ﬂexible and can
be adjusted before a game starts. Table III shows a list of
conﬁgurations currently available. An important design choice
is to allow actions to affect the environment without any
temporal delay. All actions are bound to a tick-timer that
defaults to 10, that is, it takes 10 ticks for a unit to move one
tile, 10 ticks for a unit to attack once, and 300 ticks to build
buildings. The tick-timer also includes a multiplier that enables

Fig. 1. Unit state evaluation based on actions and current state

All game entities (Units and Buildings) have a state-machine
that determine its current state. Figure 1 illustrates a portion of
the logic that is evaluated through the state-machine. Entities
in the Spawning state transitioning to the Idle state
start
when the entity spawn process is complete. The Idle state
can be considered the default state of all entities and is only
transitioned from when the player interacts with the entity.
This implementation enables researchers to modify the state-
transitions to produce alternative game logic.

TABLE IV
THE AVAILABLE ECONOMIC RESOURCES AND LIMITS AVAILABLE TO
PLAYERS IN DEEP RTS

Player Resources

Property
Range

Lumber Gold
0 - 106

0 - 106

Oil
0 - 106

Food
0 - 6000

Units
0 - 2000

Table IV shows the available resources and unit limits in
the Deep RTS game environment. There are primarily three
resources, gold, lumber, and oil that are available for workers
to harvest. The value range is practically limited to the number
of resources that exist on the game map. The food limit and
the unit limit ensures that the player does not produce units
excessively.

C. Graphics

The Deep RTS game engine features two graphical interface
modes in addition to the headless mode that is used by default.

D. Action-space deﬁnition

The action-space of the Deep RTS game environment is
separated into two abstract levels. The ﬁrst level is actions
that directly impact the environment, for instance, right-click,
left-click, move-left, and select-unit. The next layer of abstrac-
tion is actions that combine actions from the previous layer,
typically select-unit → right-click → right-click → move-left.
The beneﬁt of this abstraction is that algorithms can focus
on speciﬁc areas within the game-state, and enable to build
hierarchical models that each specialize in tasks (planning).
The Deep RTS initially features 16 different actions in the
ﬁrst layer and 6 actions in the last abstraction layer, but it is
trivial to add additional actions.

E. Summary

This section presents some of the central parts what the
Deep RTS game environment features for reinforcement learn-
ing research. It
is designed to measure the performance
of algorithms accurately having a standardized API through
OpenAI Gym, which is widely used in the reinforcement
learning community.

V. EXPERIMENTS

A. Performance considerations in Deep RTS

The goal of Deep RTS is to simulate RTS scenarios with
ultra high-performance accurately. The performance is mea-
sured by how fast the game engine updates the game-state, and
how quickly the game-state can be represented as an image.
Some experiments suggest that it is beneﬁcial to render game
graphics on the CPU instead of the GPU. Because the GPU has
a separate memory, there is a severe bottleneck when copying
the screen buffer from the GPU to the CPU.

Figure 4a shows the correlation between the frame-rate and
size of the game map. Observing the data, it is clear that
the map-size has O(n) penalty to the frame-rate performance.
It is vital to preserve this linearity, and optimally have the
constant performance of O(1) per game update. Figure 4
extends this benchmark by testing the impact a unit has on
the game performance, averaging 1 000 games for all map-
sizes. The data indicates that entities have an exponential
impact on the frame-rate performance. The reason for this
is primarily the jump-point-search algorithm used for unit
path-ﬁnding. The path-ﬁnding algorithm can be disabled using
custom conﬁgurations.

The Deep RTS game environment

is high-performance,
with few elements that signiﬁcantly reduce the frame-rate
performance. While some mechanics, namely path-ﬁnding is
a signiﬁcant portion of the update-loop it can be deactivated
by conﬁgurations to optimize the performance further.

B. Comparing Deep RTS to existing learning environments

There is a substantial difference between the performance
in games targeted research and those aimed towards gaming.
Table V shows that the frame-rate difference ranges from
60 to 7 000 000 for selected environments. A high frame-
rate is essential because some exploration algorithms often

Fig. 2. Overview of a battle in the fully-observable Deep RTS state-space
using the C++ graphical user interface

Fig. 3.

Illustration of how the raw state is represented using 3-D matrices

The primary graphical interface relies on Python while the
second is implemented in C++. The Python version is not
interactive and can only render the raw game-state as an image.
By using software rendering, the capture process of images is
signiﬁcantly faster because the copy between GPU and CPU
is slow. The C++ implementation, seen in Figure 2 is fully
interactive, enabling manual play of Deep RTS. Figure 3 shows
how the raw game-state is represented as a 3-D matrix in
headless mode. Deep learning methods often favor raw game-
state data instead of image representation as sensory input.
This is because raw data is often more concrete with clear
patterns.

(a) Correlation between FPS (Y-axis) and Map-sizes (X-
axis)

(b) Correlation between FPS and Number of Units

Fig. 4. FPS Performance in Deep RTS

TABLE V
COMPARISON OF THE FPS FOR SELECTED ENVIRONMENTS. THE DEEP
RTS BENCHMARKS ARE PERFORMED USING MINIMUM AND MAXIMUM
CONFIGURATIONS

Environment

ALE
Malmo Platform
ViZDoom
DeepMind Lab
OpenAI Gym
OpenAI Universe
Stratagus
microRTS
TorchCraft
ELF
SC2LE
Deep RTS

Frame per second
6,500
60-144
8,300
1,000
60
60
60-144
11,500
2,500
36,000
60-144
24,000, 7,000,000

Source
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[8]
-

Fig. 5. Overview of the Deep Q-Network architecture used in the experiments.
Inspired by the work seen in [1]

require a quick assessment of future states through forward-
search. Table V shows that microRTS, ELF, and Deep RTS
are superior in performance compared to other game environ-
ments. Deep RTS is measured using the largest available map
(Table II) having a unit limit of 20 per player. This yields the
performance of 24 000 updates-per-second. The Deep RTS
game engine can also render the game state with up to 7 000
000 updates-per-second using the minimal conﬁguration. This
is a tremendous improvement on previous work and could
enable algorithms with a limited time budget to do deeper
tree-searches.

C. Using Deep Q-Learning in Deep RTS

(cid:48)

r the rewards,and s
the next state. Instead, a non-linear
function approximation can be used to approximate Q(s, a; θ).
This is called Deep-Q Learning. θ describes the tunable
parameters (weights) for the approximation function. Artiﬁcial
neural networks are used as an approximation function for the
Q-Table but at the cost of stability [3]. Using artiﬁcial neural
networks is much like compression found in JPEG images. The
compression is lossy, and some information is lost during the
compression. Deep Q-Learning is thus unstable, since values
may be incorrectly encoded during training [36].

This paper presents experimental results using the Deep
Q-Learning architecture from [3], [37]. Figure 5 shows the
network model, and ﬁgure 6 illustrates the averaged training
loss of 100 agents. The agent uses gray-scale image game-
state representations with an additional convolutional layer to
decrease the training time, but can also achieve comparable
results after approximately 800 episodes of training with the
exact architecture from [3]2. The graph shows that the agent
quickly learns the correlation between game-state, action and
the reward function. The loss quickly stabilizes at a relatively
low value, but
is likely that very small optimizations
in the parameters have a signiﬁcant impact on the agent’s
performance.

it

Figure 7a shows the win-rate against an AI with a random-
play strategy. The agent quickly learns how to perform better
than random behavior, and achieves 70 % win-rate at episode 1
250. Figure 7b illustrates the same agent playing against a rule-
based strategy. The graph shows that the Deep Q-Network can
achieve an average of 50 % win-rate over a 1 000 games. This
strategy is considered an easy to moderate player, where its
strategy is to expand the base towards the opponent and build
a military force after approximately 600 seconds. Figure 2
shows how the rule-based player (blue) expands the base to
gain the upper hand.

The experimental results presented in this paper show that
the Deep RTS game environment can be used to train deep

At the most basic level, Q-Learning utilizes a table for
) pairs, where s is the states, a is the actions,

storing (s, a, r, s

(cid:48)

size of 16

2Each episode contains approximately 1 000 epochs of training with a batch

REFERENCES

[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves,

I. Antonoglou,
D. Wierstra, and M. Riedmiller, “Playing Atari with Deep Reinforcement
Learning,” arxiv preprint arXiv:1312.5602, dec 2013.
[Online].
Available: http://arxiv.org/abs/1312.5602

[2] W. Gaetz, S. K. Kessler, T. P. Roberts, J. I. Berman, T. J. Levy,
M. Hsia, D. Humpl, E. S. Schwartz, S. Amaral, B. Chang, and L. S.
Levin, “Massive cortical reorganization is reversible following bilateral
transplants of the hands: evidence from the ﬁrst successful bilateral
pediatric hand transplant patient,” Annals of Clinical and Translational
Neurology, vol. 5, no. 1, pp. 92–97, dec 2018. [Online]. Available:
https://arxiv.org/abs/1712.01815

[3] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu,

J. Veness,
M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland,
G. Ostrovski, S. Petersen, C. Beattie, A. Sadik,
I. Antonoglou,
H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis,
“Human-level control through deep reinforcement learning,” Nature,
vol. 518, no. 7540, pp. 529–533,
[Online]. Available:
http://www.nature.com/articles/nature14236

feb 2015.

[4] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino,
M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, D. Kumaran,
and R. Hadsell, “Learning to Navigate in Complex Environments,”
arXiv preprint arXiv:1611.03673, nov 2016.
[Online]. Available:
http://arxiv.org/abs/1611.03673

[5] M. A. Yasin, W. A. Al-Ashwal, A. M. Shire, S. A. Hamzah, and
K. N. Ramli, “Tri-band planar inverted F-antenna (PIFA) for GSM bands
and bluetooth applications,” ARPN Journal of Engineering and Applied
Sciences, vol. 10, no. 19, pp. 8740–8744, 2015.

[6] G. Konidaris and A. Barto, “Autonomous shaping,” Proceedings of the
23rd international conference on Machine learning - ICML ’06, pp.
489–496, 2006. [Online]. Available: http://portal.acm.org/citation.cfm?
doid=1143844.1143906

[7] G. Tesauro, “TD-Gammon, a Self-Teaching Backgammon Program,
Achieves Master-Level Play,” Neural Computation, vol. 6, no. 2, pp.
215–219, 1994. [Online]. Available: http://www.mitpressjournals.org/
doi/10.1162/neco.1994.6.2.215

[8] O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets,
M. Yeo, A. Makhzani, H. K¨uttler, J. Agapiou, J. Schrittwieser, J. Quan,
S. Gaffney, S. Petersen, K. Simonyan, T. Schaul, H. van Hasselt,
D. Silver, T. Lillicrap, K. Calderone, P. Keet, A. Brunasso, D. Lawrence,
A. Ekermo, J. Repp, and R. Tsing, “StarCraft II: A New Challenge for
Reinforcement Learning,” Proceedings, The Thirteenth AAAI Conference
on Artiﬁcial Intelligence and Interactive Digital Entertainment, pp.
86–92, 2017. [Online]. Available: http://arxiv.org/abs/1708.04782
[9] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den
Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanc-
tot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever,
T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis,
“Mastering the game of Go with deep neural networks and tree search,”
Nature, vol. 529, no. 7587, pp. 484–489, 2016.

[10] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, “The
arcade learning environment: An evaluation platform for general
agents,” IJCAI International Joint Conference on Artiﬁcial Intelligence,
vol. 2015-Janua, pp. 4148–4152, 2015.
[Online]. Available: http:
//arxiv.org/abs/1207.4708

[11] M. Johnson, K. Hofmann, T. Hutton, and D. Bignell, “The malmo
platform for artiﬁcial intelligence experimentation,” IJCAI International
Joint Conference on Artiﬁcial Intelligence, vol. 2016-Janua, pp. 4246–
4247, 2016. [Online]. Available: https://www.ijcai.org/Proceedings/16/
Papers/643.pdf

[12] E. Perot, M. Jaritz, M. Toromanoff, and R. D. Charette, “End-to-End
Driving in a Realistic Racing Game with Deep Reinforcement
Learning,” IEEE Computer Society Conference on Computer Vision
and Pattern Recognition Workshops, vol. 2017-July, pp. 474–475, may
2017. [Online]. Available: http://arxiv.org/abs/1605.02097

[13] C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright,
H. K¨uttler, A. Lefrancq, S. Green, V. Vald´es, A. Sadik, J. Schrittwieser,
K. Anderson, S. York, M. Cant, A. Cain, A. Bolton, S. Gaffney,
H. King, D. Hassabis, S. Legg, and S. Petersen, “DeepMind Lab,”
arXiv preprint arXiv:1612.03801, dec 2016.
[Online]. Available:
http://arxiv.org/abs/1612.03801

[14] G.

Brockman, V.

Cheung,

L.

Pettersson,

J. Schulman,

J. Tang,

and W. Zaremba,

J.

Schneider,
“OpenAI Gym,”

Fig. 6. Training loss of the Deep Q-Network. Each episode consists of
approximately 1 000 epochs.

reinforcement learning algorithms. The Deep Q-Network does
not achieve super-human expertise but performs similarly to
a player of easy to moderate skill level, which is a good step
towards a high-level AI.

VI. CONCLUSION AND FUTURE WORK
This paper is a contribution towards the continuation of
research into deep reinforcement learning for RTS games. The
paper summarizes previous work and outlines the few but
essential success stories in reinforcement learning. The Deep
RTS game environment is a high-performance RTS simulator
that enables rapid research and testing of novel reinforcement
learning techniques. It successfully ﬁlls the gap between the
vital game simulator microRTS, and StarCraft II, which is the
ultimate goal for reinforcement learning research for the RTS
game genre.

The hope is that Deep RTS can bring insightful results to
the complex problems of RTS [17] and that it can be a useful
tool in future research.

Although the Deep RTS game environment is ready for
use, several improvements can be applied to the environment.
The following items are scheduled for implementation in the
continuation of Deep RTS:

• Enable LUA developers to use Deep RTS through LUA

• Implement a generic interface for custom graphics ren-

bindings.

dering.

• Implement duplex WebSockets and ZeroMQ to enable

any language to interact with Deep RTS

• Implement alternative path-ﬁnding algorithms to increase

performance for some scenarios

• Add possibility for memory-based fog-of-war to better

mimic StarCraft II

VII. ACKNOWLEDGEMENTS
We would like to thank Santiago Ontanon at the University
of Drexel for his excellent work on microRTS. microRTS has
to us been a valuable tool for benchmarks in the reinforcement
learning domain and continues to be the goto environment for
research in tree-search algorithms.

(a) DQN vs Random-play AI in the 15x15-2-FFA sce-
nario

(b) DQN vs Rule-based AI in the 15x15-2-FFA scenario

Fig. 7. Performance comparison of agents using Deep Q-Network, random-play, and rule-based strategies

arXiv preprint arXiv:1606.01540,
http://arxiv.org/abs/1606.01540

jun 2016.

[Online]. Available:

[15] OpenAI,

“OpenAI Universe,” 2017.

[Online]. Available: https:

//universe.openai.com/

“Stratagus: An open-source game

[16] M. Ponsen, S. Lee-Urban, H. Mu˜noz-Avila, D. Aha,
engine

and
M. Molineaux,
for
research in real-time strategy games,” Reasoning, Representation,
and Learning in Computer Games, no. Code 5515, p. 78,
https://pdfs.semanticscholar.org/d005/
2005.
87160d3b37c70f238ebd92c71454479e829e.pdf

[Online]. Available:

[17] S. Ontanon, “The combinatorial multi-armed bandit problem and its
application to real-time strategy games,” in Ninth Artiﬁcial Intelligence
and Interactive Digital . . . , 2013, pp. 58–64. [Online]. Available: http:
//www.aaai.org/ocs/index.php/AIIDE/AIIDE13/paper/viewPaper/7377

[18] G. Synnaeve, N. Nardelli, A. Auvolat, S. Chintala, T. Lacroix,
Z. Lin, F. Richoux,
a Library
and N. Usunier,
for Machine Learning Research on Real-Time Strategy Games,”
arxiv preprint arXiv:1611.00625, nov 2016.
[Online]. Available:
http://arxiv.org/abs/1611.00625

“TorchCraft:

[19] Y. Tian, Q. Gong, W. Shang, Y. Wu,

and C. L. Zitnick,
“ELF: An Extensive, Lightweight and Flexible Research Platform
for Real-time Strategy Games,” Advances in Neural
Information
Processing Systems, pp. 2656–2666,
jul 2017. [Online]. Available:
http://arxiv.org/abs/1707.01067

[20] S. Onta˜n´on, K. Mishra, N. Sugandh, and A. Ram, “Learning from
demonstration and case-based planning for real-time strategy games,”
in Studies in Fuzziness and Soft Computing. Berlin, Heidelberg:
Springer Berlin Heidelberg, 2008, vol. 226, pp. 293–310. [Online].
Available: http://link.springer.com/10.1007/978-3-540-77465-5{\ }15
[21] I. Fathy, M. Aref, O. Enayet, and A. Al-Ogail, “Intelligent online
case-based planning agent model for real-time strategy games,” in
Proceedings of the 2010 10th International Conference on Intelligent
Systems Design and Applications, ISDA’10.
IEEE, nov 2010, pp. 445–
450. [Online]. Available: http://ieeexplore.ieee.org/document/5687225/
“CLASSQ-L: A Q-Learning
Algorithm for Adversarial Real-Time Strategy Games,” pp. 8–13,
[Online]. Available: http://www.aaai.org/ocs/index.php/AIIDE/
2012.
AIIDE12/paper/viewFile/5515/5734

and H. Mu˜noz-Avila,

[22] U.

Jaidee

[23] B. Lindstr¨om,

I. Selbing, T. Molapour, and A. Olsson, “Racial
Bias Shapes Social Reinforcement Learning,” Psychological Science,
vol. 25, no. 3, pp. 711–719,
[Online]. Available:
http://arxiv.org/abs/1602.01783

feb 2014.

[24] N. A. Barriga, M. Stanescu, and M. Buro, “Game Tree Search Based on
Non-Deterministic Action Scripts in Real-Time Strategy Games,” IEEE
Transactions on Computational Intelligence and AI in Games, pp. 1–1,
2017. [Online]. Available: http://ieeexplore.ieee.org/document/7954767/
[25] A. Shleyfman, A. Komenda, and C. Domshlak, “On combinatorial
actions and CMABs with linear side information,” in Frontiers in
Artiﬁcial Intelligence and Applications, vol. 263, 2014, pp. 825–830.

[26] D. Churchill, Z. Lin, and G. Synnaeve, “An Analysis of Model-Based
Heuristic Search Techniques for StarCraft Combat Scenarios,” pp.

8–14, 22017. [Online]. Available: https://aaai.org/ocs/index.php/AIIDE/
AIIDE17/paper/view/15916

[27] P. Peng, Y. Wen, Y. Yang, Q. Yuan, Z. Tang, H. Long, and J. Wang,
“Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-
level Coordination in Learning to Play StarCraft Combat Games,”
arxiv preprint arXiv:1703.10069, mar 2017.
[Online]. Available:
http://arxiv.org/abs/1703.10069

[28] Z. Lin, J. Gehring, V. Khalidov, and G. Synnaeve, “STARDATA:
A StarCraft AI Research Dataset,” Proceedings, The Thirteenth
AAAI Conference on Artiﬁcial Intelligence and Interactive Digital
Entertainment, aug 2017. [Online]. Available: http://arxiv.org/abs/1708.
02139

[29] E. Perot, M. Jaritz, M. Toromanoff, and R. D. Charette, “End-to-End
Driving in a Realistic Racing Game with Deep Reinforcement
Learning,” IEEE Computer Society Conference on Computer Vision
and Pattern Recognition Workshops, vol. 2017-July, pp. 474–475, may
2017. [Online]. Available: https://arxiv.org/abs/1605.02097

[30] Y. Li, “Deep Reinforcement Learning: An Overview,” arXiv preprint
arXiv:1701.07274, pp. 1–30, 2017. [Online]. Available: http://arxiv.org/
abs/1701.07274

[31] T. Mannucci, E. J. van Kampen, C. de Visser, and Q. Chu, “Safe
Exploration Algorithms for Reinforcement Learning Controllers,” IEEE
Transactions on Neural Networks and Learning Systems, vol. 16,
pp. 1437–1480, 2017. [Online]. Available: http://www.jmlr.org/papers/
volume16/garcia15a/garcia15a.pdf

[32] G. Tesauro,

“Temporal difference

learning and TD-Gammon,”
Communications of the ACM, vol. 38, no. 3, pp. 58–68, 1995. [Online].
Available: http://portal.acm.org/citation.cfm?doid=203330.203343
[33] R. S. Sutton and A. G. Barto, “Chapter 12: Introductions,” Acta
Physiologica Scandinavica, vol. 48, no. Mowrer 1960, pp. 57–63, 1960.
[34] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap,
F. Hui, L. Sifre, G. Van Den Driessche, T. Graepel, and D. Hassabis,
“Mastering the game of Go without human knowledge,” Nature, vol.
550, no. 7676, pp. 354–359, 2017.

[35] M. Moravˇc´ık, M. Schmid, N. Burch, V. Lis´y, D. Morrill, N. Bard,
T. Davis, K. Waugh, M. Johanson, and M. Bowling, “DeepStack:
Expert-level artiﬁcial intelligence in heads-up no-limit poker,” Science,
vol. 356, no. 6337, pp. 508–513, jan 2017. [Online]. Available: http:
//arxiv.org/abs/1701.01724http://dx.doi.org/10.1126/science.aam6960
[36] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” arXiv preprint arXiv:1509.02971, vol. abs/1509.0, sep 2015.
[Online]. Available: http://arxiv.org/abs/1509.02971

[37] W. Chen, M. Zhang, Y. Zhang, and X. Duan, “Exploiting meta
features for dependency parsing and part-of-speech tagging,” Artiﬁcial
Intelligence, vol. 230, pp. 173–191, sep 2016. [Online]. Available:
http://arxiv.org/abs/1509.06461

