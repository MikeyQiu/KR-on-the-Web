On the Compactness, Eﬃciency, and
Representation of 3D Convolutional Networks:
Brain Parcellation as a Pretext Task

Wenqi Li, Guotai Wang, Lucas Fidon, Sebastien Ourselin, M. Jorge Cardoso,
and Tom Vercauteren

Translational Imaging Group, Centre for Medical Image Computing (CMIC),
University College London, London, UK
Wellcome/EPSRC Centre for Surgical and Interventional Science, University College
London, London, UK

Abstract. Deep convolutional neural networks are powerful tools for
learning visual representations from images. However, designing eﬃcient
deep architectures to analyse volumetric medical images remains chal-
lenging. This work investigates eﬃcient and ﬂexible elements of modern
convolutional networks such as dilated convolution and residual connec-
tion. With these essential building blocks, we propose a high-resolution,
compact convolutional network for volumetric image segmentation. To
illustrate its eﬃciency of learning 3D representation from large-scale im-
age data, the proposed network is validated with the challenging task
of parcellating 155 neuroanatomical structures from brain MR images.
Our experiments show that the proposed network architecture compares
favourably with state-of-the-art volumetric segmentation networks while
being an order of magnitude more compact. We consider the brain par-
cellation task as a pretext task for volumetric image segmentation; our
trained network potentially provides a good starting point for transfer
learning. Additionally, we show the feasibility of voxel-level uncertainty
estimation using a sampling approximation through dropout.

1

Introduction

Convolutional neural networks (CNNs) have been shown to be powerful tools for
learning visual representations from images. They often consist of multiple layers
of non-linear functions with a large number of trainable parameters. Hierarchical
features can be obtained by training the CNNs discriminatively.

In the medical image computing domain, recent years have seen a growing
number of applications using CNNs. Although there have been recent advances
in tailoring CNNs to analyse volumetric images, most of the work to date studies
image representations in 2D. While volumetric representations are more informa-
tive, the number of voxels scales cubically with the size of the region of interest.
This raises challenges of learning more complex visual patterns as well as higher
computational burden compared to the 2D cases. While developing compact and

7
1
0
2
 
l
u
J
 
6
 
 
]

V
C
.
s
c
[
 
 
1
v
2
9
9
1
0
.
7
0
7
1
:
v
i
X
r
a

2

Li et al.

eﬀective 3D network architectures is of signiﬁcant interest, designing 3D CNNs
remains a challenging problem.

The goal of this paper is to design a high-resolution and compact network
architecture for the segmentation of ﬁne structures in volumetric images. For
this purpose, we study the simple and ﬂexible elements of modern convolutional
networks, such as dilated convolution and residual connection. Most of the ex-
isting network architectures follow a fully convolutional downsample-upsample
pathway [11,4,15,3,16,13]. Low-level features with high spatial resolutions are
ﬁrst downsampled for higher-level feature abstraction; then the feature maps
are upsampled to achieve high-resolution segmentation. In contrast to these, we
propose a novel 3D architecture that incorporates high spatial resolution feature
maps throughout the layers, and can be trained with a wide range of receptive
ﬁelds. We validate our network with the challenging task of automated brain
parcellation into 155 structures from T1-weighted MR images. We show that
the proposed network, with twenty times fewer parameters, achieves competi-
tive segmentation performance compared with state-of-the-art architectures.

A well-designed network could be trained with a large-scale dataset and en-
ables transfer learning to other image recognition tasks [9]. In the ﬁeld of com-
puter vision, the well-known AlexNet and VGG net were trained on the Ima-
geNet dataset. They provide general-purpose image representations that can be
adapted for a wide range of computer vision problems. Given the large amount
of data and the complex visual patterns of the brain parcellation problem, we
consider it as a pretext task. Our trained network is the ﬁrst step towards a
general-purpose volumetric image representation. It potentially provides an ini-
tial model for transfer learning of other volumetric image segmentation tasks.

The uncertainty of the segmentation is also important for indicating the
conﬁdence and reliability of one algorithm [5,18,19]. The high uncertainty of la-
belling can be a sign of an unreliable classiﬁcation. In this work, we demonstrate
the feasibility of voxel-level uncertainty estimation using Monte Carlo samples
of the proposed network with dropout at test time. Compared to the existing
volumetric segmentation networks, our compact network has fewer parameter in-
teractions, and thus potentially achieves better uncertainty estimates with fewer
samples.

2 On the elements of 3D convolutional networks

Convolutions and dilated convolutions. To maintain a relatively low num-
ber of parameters, we choose to use small 3D convolutional kernels with only 33
parameters for all convolutions. This is about the smallest kernel that can rep-
resent 3D features in all directions with respect to the central voxel. Although a
convolutional kernel with 5 × 5 × 5 voxels gives the same receptive ﬁeld as stack-
ing two layers of 3 × 3 × 3-voxel convolution, the latter has approximately 57%
fewer parameters. Using smaller kernels implicitly imposes more regularisation
on the network while achieving the same receptive ﬁeld.

3D Convolutional Networks for Brain Parcellation

3

To further enlarge the receptive ﬁeld to capture large image contexts, most
of the existing volumetric segmentation networks downsample the intermediate
feature maps. This signiﬁcantly reduces the spatial resolution. For example, 3D
U-net [3] heavily employs 2×2×2-voxel max pooling with strides of two voxels in
each dimension. Each max pooling reduces the feature responses of the previous
layer to only 1/8 of its spatial resolution. Upsampling layers, such as decon-
volutions, are often used subsequently to partially recover the high resolution
of the input. However, adding deconvolution layers also introduces additional
computational costs.

Recently, Chen et al. [2] used dilated convolutions with upsampled kernels
for semantic image segmentation. The advantages of dilated convolutions are
that the features can be computed with a high spatial resolution, and the size of
the receptive ﬁeld can be enlarged arbitrarily. Dilated convolutions can be used
to produce accurate dense predictions and detailed segmentation maps along
object boundaries.

In contrast to the downsample-upsample pathway, we propose to adopt di-
lated convolutions for volumetric image segmentation. More speciﬁcally, the con-
volutional kernels are upsampled with a dilation factor r. For M -channels of
input feature maps I, the output feature channel O generated with dilated con-
volutions are:

Ox,y,z =

wi,j,k,mI(x+ir),(y+jr),(z+kr),m ;

(1)

M −1
(cid:88)

2
(cid:88)

2
(cid:88)

2
(cid:88)

m=0

i=0

j=0

k=0

where the index tuple (x, y, z) runs through every spatial location in the volumes;
the kernels w consist of 33 × M trainable parameters. The dilated convolution in
Eq. (1) has the same number of trainable parameters as the standard 3 × 3 × 3
convolution. It preserves the spatial resolution and provides a (2r + 1)3-voxel
receptive ﬁeld. Setting r to 1 reduces the dilated convolution to the standard
3 × 3 × 3 convolution. In practice, we implement 3D dilated convolutions with
a split-and-merge strategy [2] to beneﬁt from the existing GPU convolution
routines.

Residual connections. Residual connections
were ﬁrst introduced and later reﬁned by He et
al. [7,8] for the eﬀective training of deep networks.
The key idea of residual connection is to create
identity mapping connections to bypass the pa-
rameterised layers in a network. The input of a
residual block is directly merged to the output
by addition. The residual connections have been
shown to make information propagation smooth
and improve the training speed [7].

Fig. 1. A block with residual
connections.

More speciﬁcally, let the input to the p-th layer of a residual block as xp, the
output of the block xp+1 has the form: xp+1 = xp + F (xp, wp); where F (xp, wp)

4

Li et al.

denotes the path with non-linear functions in the block (shown in Fig. 1). If we
stack the residual blocks, the last layer output xl can be expressed as: xl = xp +
(cid:80)l−1
i=p F (xi, wi). The residual connections enables direct information propagation
from any residual block to another in both forward pass and back-propagation.

Eﬀective receptive ﬁeld. One interpretation of the residual network is that
they behave like ensembles of relatively shallow networks. The unravelled view of
the residual connections proposed by Veit et al. [20] suggests that the networks
with n residual blocks have a collection of 2n unique paths.

Without residual connections, the receptive ﬁeld of a network is generally
considered ﬁxed. However, when training with n residual blocks, the networks
utilise 2n diﬀerent paths and therefore features can be learned with a large range
of diﬀerent receptive ﬁelds. For example, the proposed network with 9 residual
blocks (see Section 3) has a maximum receptive ﬁeld of 87 × 87 × 87 voxels.
Following the unravel view of the residual network, it consists of 29 unique paths.
Fig. 2 shows the distribution of the receptive ﬁeld of these paths. The receptive
ﬁelds range from 3 × 3 × 3 to 87 × 87 × 87, following a binomial distribution.

This diﬀers from the existing 3D networks.
For example, Deepmedic [11] model operates
at two paths, with a ﬁxed receptive ﬁeld 17 ×
17 × 17 and 42 × 42 × 42 respectively. 3D U-
net [3] has a relatively large receptive ﬁeld of
about 88 × 88 × 88 voxels. However, there are
only eight unique paths and receptive ﬁelds.

Fig. 2. Histogram of the receptive
ﬁelds.

Intuitively, given that the receptive ﬁeld
of a deep convolutional network is relatively
large, the segmentation maps will suﬀer from
distortions due to the border eﬀects of convo-
lution. That is, the segmentation results near the border of the output volume
are less accurate due to the lack of input supporting window. We conduct ex-
periments and demonstrate that the proposed networks generate only a small
distortion near the borders (See Section 4). This suggests training the network
with residual connections reduces the eﬀective receptive ﬁeld. The width of the
distorted border is much smaller than the maximum receptive ﬁeld. This phe-
nomenon was also recently analysed by Luo et al. [14]. In practice, at test time
we pad each input volume with a border of zeros and discard the same amount
of border in the segmentation output.

Loss function. The last layer of the network is a softmax function that gives
scores over all labels for each voxel. Typically, the end-to-end training proce-
dure minimises the cross entropy loss function using an N -voxel image volume
{xn}N
n=1 where

n=1 and the training data of C-class segmentation map {yn}N

3D Convolutional Networks for Brain Parcellation

5

yn ∈ {1, . . . , C} is:

L({xn}, {yn}) = −

δ(yn = c) log Fc(xn),

(2)

1
N

N
(cid:88)

C
(cid:88)

n=1

c=1

where δ corresponds to the Dirac delta function, Fc(xn) is the softmax classiﬁca-
tion score of xn over the c-th class. However, when the training data are severely
unbalanced (which is typical in medical image segmentation problems), this for-
mulation leads to a strongly biased estimation towards the majority class. In-
stead of directly re-weighting each voxel by class frequencies, Milletari et al. [16]
propose a solution by maximising the mean Dice coeﬃcient directly, i.e.,

D({xn}, {yn}) =

1
C

C
(cid:88)

c=1

2 (cid:80)N

n=1 δ(yn = c)Fc(xn)

(cid:80)N

n=1[δ(yn = c)]2 + (cid:80)N

n=1[Fc(xn)]2

.

(3)

We employ this formulation to handle the issue of training data imbalance.

Uncertainty estimation using dropout. Gal and Ghahramani demonstrated
that the deep network trained with dropout can be cast as a Bayesian approxi-
mation of the Gaussian process [5]. Given a set of training data and their labels
{X, Y}, training a network F (· , W) with dropout has the eﬀect of approxi-
mating the posterior distribution p(W|{X, Y}) by minimising the Kullback-
Leibler divergence term, i.e. KL(q(W)||p(W|{X, Y})); where q(W) is an ap-
proximating distribution over the weight matrices W with their elements ran-
domly set to zero according to Bernoulli random variables. After training the
network, the predictive distribution of test data ˆx can be expressed as q(ˆy|ˆx) =
(cid:82) F (ˆx, W)q(W)dW. The prediction can be approximated using Monte Carlo
samples of the trained network: ˆy = 1
m=1 is a
M
set of M samples from q(W). The uncertainty of the prediction can be estimated
using the sample variance of the M samples.

m=1 F (ˆx, Wm), where {Wm}M

(cid:80)M

With this theoretical insight, we are able to estimate the uncertainty of the
segmentation map at the voxel level. We extend the segmentation network with
a 1 × 1 × 1 convolutional layer before the last convolutional layer. The extended
network is trained with a dropout ratio of 0.5 applied to the newly inserted
layer. At test time, we sample the network N times using dropout. The ﬁnal
segmentation is obtained by majority voting. The percentage of samples which
disagrees with the voting results is calculated at each voxel as the uncertainty
estimates.

3 The network architecture and its implementation

3.1 The proposed architecture

Our network consists of 20 layers of convolutions. In the ﬁrst seven convolu-
tional layers, we adopt 3 × 3 × 3-voxel convolutions. These layers are designed

6

Li et al.

Fig. 3. The proposed network architecture for volumetric image segmentation. The
network mainly utilises dilated convolutions and residual connections to make an end-
to-end mapping from image volume to a voxel-level dense segmentation. To incorporate
features at multiple scales, the dilation factor of the dilated convolutions is gradually
increased when the layer goes deeper. The residual blocks with identity mapping enable
the direct fusion of features from diﬀerent scales. The spatial resolution of the input
volume is maintained throughout the network.

to capture low-level image features such as edges and corners. In the subsequent
convolutional layers, the kernels are dilated by a factor of two or four. These
deeper layers with dilated kernels encode mid- and high-level image features.

Residual connections are employed to group every two convolutional lay-
ers. Within each residual block, each convolutional layer is associated with
an element-wise rectiﬁed linear unit (ReLU) layer and a batch normalisation
layer [10]. The ReLU, batch normalisation, and convolutional layers are arranged
in the pre-activation order [8].

The network can be trained end-to-end. In the training stage, the inputs
to our network are 96 × 96 × 96-voxel images. The ﬁnal softmax layer gives
classiﬁcation scores over the class labels for each of the 96 × 96 × 96 voxels. The
architecture is illustrated in Fig. 3.

3.2 Implementation details

In the training stage, the pre-processing step involved input data standardisa-
tion and augmentation at both image- and subvolume-level. At image-level, we
adopted the histogram-based scale standardisation method [17] to normalised
the intensity histograms. As a data augmentation at image-level, randomisation
was introduced in the normalisation process by randomly choosing a threshold
of foreground between the volume minimum and mean intensity (at test time,
the mean intensity of the test volume was used as the threshold). Each image
was further normalised to have zero mean and unit standard deviation. Augmen-
tations on the randomly sampled 96 × 96 × 96 subvolumes were employed on the
ﬂy. These included rotation with a random angle in the range of [−10◦, 10◦] for

3D Convolutional Networks for Brain Parcellation

7

each of the three orthogonal planes and spatial rescaling with a random scaling
factor in the range of [0.9, 1.1].

All the parameters in the convolutional layers were initialised according to
He et al. [6]. The scaling and shifting parameters in the batch normalisation
layers were initialised to 1 and 0 respectively. The networks were trained with
two Nvidia K80 GPUs. At each training iteration, each GPU processed one in-
put volume; the average gradients computed over these two training volumes
were used as the gradients update. To make a fair comparison, we employed the
Adam optimisation method [12] for all the methods with ﬁxed hyper-parameters.
The learning rate lr was set to 0.01, the step size hyper-parameter β1 was 0.9
and β2 was 0.999 in all cases, except V-Net for which we chose the largest lr
that the training algorithm converges (lr = 0.0001). The models were trained
until we observed a plateau in performance on the validation set. We do not
employ additional spatial smoothing function (such as conditional random ﬁeld)
as a post-processing step. Instead of aiming for better segmentation results by
adding post-processing steps, we focused on the dense segmentation maps gen-
erated by the networks. As we consider brain parcellation as a pretext task,
networks without explicit spatial smoothing are potentially more reusable. We
implemented all the methods (including a re-implementation of Deepmedic [11],
V-net [16], and 3D U-net [3] architecture) with Tensorﬂow1.

4 Experiments and results

Data. To demonstrate the feasibility of learning complex 3D image representa-
tions from large-scale data, the proposed network is learning a highly granular
segmentation of 543 T1-weighted MR images of healthy controls from the ADNI
dataset. The average number of voxels of each volume is about 182 × 244 × 246.
The average voxel size is approximately 1.18mm × 1.05mm × 1.05mm. All vol-
umes are bias-corrected and reoriented to a standard Right-Anterior-Superior
orientation. The bronze standard parcellation of 155 brain structures and 5 non-
brain outer tissues are obtained using the GIF framework [1]. Fig. 5(left) shows
the label distribution of the dataset. We randomly choose 443, 50, and 50 vol-
umes for training, test, and validation respectively.

Overall evaluation. In this section, we compare the proposed high-resolution
compact network architecture (illustrated in Fig. 3; denoted as HC-default) with
three variants: (1) the HC-default conﬁguration without the residual connec-
tions, trained with cross-entropy loss function (NoRes-entropy); (2) the HC-
default conﬁguration without residual connections, trained with Dice loss func-
tion (NoRes-dice); and (3) the HC-default conﬁguration trained with an addi-
tional dropout layer, and makes predictions with a majority voting of 10 Monte
Carlo samples (HC-dropout). For the dropout variant, our dropout layer em-
ployed before the last convolutional layer consists of 80 kernels.

1 The source code is available at https://github.com/gift-surg/HighRes3DNet

8

Li et al.

(1)

(2)

(3)

(4)

(5)

Fig. 4. Visualisations of segmentation results. (1) slices from a test image volume,
segmentation maps and false prediction maps generated by HC-dropout (2, 3), and 3D
U-net-dice (4, 5).

Table 1. Comparison of diﬀerent 3D convolutional network architectures.

Architecture

Multi-layer fusion Num. param. Loss type DCS (%) STD (%)

HC-default
HC-dropout
NoRes-entropy
NoRes-dice
Deepmedic[11]-dice Two pathways
3D U-net[3]-dice
V-net[16]

Residual
Residual
N/A
N/A

Feature forwarding
Feature forwarding

0.81M
0.82M
0.81M
0.81M
0.68M
19.08M
62.63M

Dice loss
Dice loss
Cross entr.
Dice loss
Dice loss
Dice loss
Dice loss

82.05
84.34
39.36
75.47
78.74
80.18
74.58

2.96
1.89
1.13
2.97
1.72
6.18
1.86

Additionally, three state-of-the-art volumetric segmentation networks are
evaluated. These include 3D U-net [3], V-net [16], and Deepmedic [11]. The last
layer of each network architecture is replaced with a 160-way softmax classiﬁer.
We observe that training these networks with the cross entropy loss function
(Eq. 2) leads to poor segmentation results. Since the cross-entropy loss function
treats all training voxels equally, the network may have diﬃculties in learning
representations related to the minority classes. Training with the Dice loss func-
tion alleviates this issue by implicitly re-weighting the voxels. Thus we train all
networks using the Dice loss function for a fair comparison.

We use the mean Dice Coeﬃcient Similarity (DCS) as the performance mea-
sure. Table 1 and Fig. 5(right) compare the performance on the test set. In
terms of our network variants, the results show that the use of Dice loss function
largely improves the segmentation performance. This suggests that the Dice loss

3D Convolutional Networks for Brain Parcellation

9

Fig. 5. Left: label distribution of the dataset; right: comparison of diﬀerent network
architectures.

Fig. 6. Segmentation performance against a set of key structures.

function can handle the severely unbalanced segmentation problem well. The
results also suggest that introducing the residual connections improved the seg-
mentation performance measured in mean DCS. This indicates that the residual
connections are important elements of the proposed network. By adopting the
dropout method, the DCS can be further improved by 2% in DCS.

With a relatively small number of parameters, our HC-default and HC-
dropout outperform the competing methods in terms of mean DCS. This sug-
gests that our network is more eﬀective for the brain parcellation problem. Note
that V-net has a similar architecture to 3D U-net and has more parameters, but
does not employ the batch normalisation technique. The lower DCS produced by
V-net suggests that batch normalisation is important for training the networks
for brain parcellation.

10

Li et al.

In Fig. 6, we show that the dropout variant achieves better segmentation
results for all the key structures. Fig. 4 presents an example of the segmentation
results of the proposed network and 3D U-net-Dice.

Receptive ﬁeld and border eﬀects. We further compare the segmentation
performance of a trained network by discarding the borders in each dimension of
the segmentation map. That is, given a d × d × d-voxel input, at border size 1 we
only preserve the (d−2)3-voxel output volume centred within the predicted map.
Fig. 7 plots the DCS and standard er-
rors of segmentation according to the
size of the segmentation borders in
each dimension. The results show that
the distorted border is around 17 vox-
els in each dimension. The border ef-
fects do not severely decrease the seg-
mentation performance. In practice,
we pad the volume images with 16 ze-
ros in each dimension, and remove the
same amount of borders in the seg-
mentation output.

Fig. 7. Empirical analysis of the segmen-
tation borders. Voxels near to the volume
borders are classiﬁed less accurately.

The eﬀect of number of samples
in uncertainty estimations. This section investigates the number of Monte
Carlo samples and the segmentation performance of the proposed network.
Fig. 8(a) suggests that using 10 samples is enough to achieve good segmen-
tation. Further increasing the number of samples has relatively small eﬀects on
the DCS. Fig. 8(b) plots the voxel-wise segmentation accuracy computed using
only the voxels with an uncertainty less than a threshold. The voxel-wise ac-
curacy is high when the threshold is small. This indicates that the uncertainty

(a)

(b)

Fig. 8. Evaluation of dropout sampling. (a) The segmentation performance against the
number of Monte Carlo samples. (b) voxel-level segmentation accuracy by thresholding
the uncertainties. The shaded area represents the standard errors.

3D Convolutional Networks for Brain Parcellation

11

Fig. 9. Voxel-level segmentation uncertainty estimations. Top row: uncertainty map
generated with 100 Monte Carlo samples using dropout. Bottom row: uncertainty map
thresholded at 0.1.

estimation reﬂects the conﬁdence of the network. Fig. 9 shows an uncertainty
map generated by the proposed network. The uncertainties near the boundaries
of diﬀerent structures are relatively higher than the other regions.

Currently, our method takes about 60 seconds to predict a typical volume
with 192 × 256 × 256 voxels. To achieve better segmentation results and measure
uncertainty, 10 Monte Carlo samples of our dropout model are required. The
entire process takes slightly more than 10 minutes in total. However, during
the Monte Carlo sampling at test time, only the dropout layer and the ﬁnal
prediction layer are randomised. To further reduce the computational time, the
future software could reuse the features extracted from the layers before dropout,
resulting in only a marginal increase in runtime when compared to a single
prediction.

5 Conclusion

In this paper, we propose a high-resolution, 3D convolutional network archi-
tecture that incorporates large volumetric context using dilated convolutions
and residual connections. Our network is conceptually simpler and more com-
pact than the state-of-the-art volumetric segmentation networks. We validate the
proposed network using the challenging task of brain parcellation in MR images.
We show that the segmentation performance of our network compares favourably
with the competing methods. Additionally, we demonstrate that Monte Carlo
sampling of dropout technique can be used to generate voxel-level uncertainty es-

12

Li et al.

timation for our brain parcellation network. Moreover, we consider the brain par-
cellation task as a pretext task for volumetric image segmentation. Our trained
network potentially provides a good starting point for transfer learning of other
segmentation tasks.

In the future, we will extensively test the generalisation ability of the network
to brain MR scans obtained with various scanning protocols from diﬀerent data
centres. Furthermore, we note that the uncertainty estimations are not proba-
bilities. We will investigate the calibration of the uncertainty scores to provide
reliable probability estimations.

Acknowledgements. This work was supported through an Innovative Engi-
neering for Health award by the Wellcome Trust [WT101957, 203145Z/16/Z],
Engineering and Physical Sciences Research Council (EPSRC) [NS/A000027/1,
NS/A000050/1], the National Institute for Health Research University College
London Hospitals Biomedical Research Centre (NIHR BRC UCLH/UCL High
Impact Initiative), UCL EPSRC CDT Scholarship Award [EP/L016478/1], a
UCL Overseas Research Scholarship, a UCL Graduate Research Scholarship,
and the Health Innovation Challenge Fund [HICF-T4-275, WT 97914], a paral-
lel funding partnership between the Department of Health and Wellcome Trust.
The authors would also like to acknowledge that the work presented here made
use of Emerald, a GPU-accelerated High Performance Computer, made available
by the Science & Engineering South Consortium operated in partnership with
the STFC Rutherford-Appleton Laboratory.

References

1. Cardoso, M.J., Modat, M., Wolz, R., Melbourne, A., Cash, D., Rueckert, D.,
Ourselin, S.: Geodesic information ﬂows: Spatially-variant graphs and their appli-
cation to segmentation and fusion. IEEE Transactions on Medical Imaging 34(9)
(2015)

2. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: DeepLab:
Semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected CRFs. arXiv:1606.00915 (2016)

3. C¸ i¸cek, ¨O., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O.: 3D U-
Net: Learning dense volumetric segmentation from sparse annotation. In: MICCAI
(2016)

4. Dou, Q., Chen, H., Yu, L., Zhao, L., Qin, J., Wang, D., Mok, V.C., Shi, L., Heng,
P.A.: Automatic detection of cerebral microbleeds from MR images via 3D convo-
lutional neural networks. IEEE Transactions on Medical Imaging 35(5) (2016)
5. Gal, Y., Ghahramani, Z.: Dropout as a Bayesian approximation: Representing

model uncertainty in deep learning. In: ICML (2016)

6. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectiﬁers: Surpassing human-

level performance on imagenet classiﬁcation. In: ICCV (2015)

7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

8. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.

In: CVPR (2016)

In: ECCV (2016)

3D Convolutional Networks for Brain Parcellation

13

9. Huh, M., Agrawal, P., Efros, A.A.: What makes ImageNet good for transfer learn-

ing? arXiv:1608.08614 (2016)

10. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML (2015)

11. Kamnitsas, K., Ledig, C., Newcombe, V.F., Simpson, J.P., Kane, A.D., Menon,
D.K., Rueckert, D., Glocker, B.: Eﬃcient multi-scale 3D CNN with fully connected
CRF for accurate brain lesion segmentation. Medical Image Analysis 36 (2017)
12. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. arXiv:1412.6980

(2014)

13. Kleesiek, J., Urban, G., Hubert, A., Schwarz, D., Maier-Hein, K., Bendszus, M.,
Biller, A.: Deep MRI brain extraction: A 3D convolutional neural network for skull
stripping. NeuroImage 129 (2016)

14. Luo, W., Li, Y., Urtasun, R., Zemel, R.: Understanding the eﬀective receptive ﬁeld

in deep convolutional neural networks. In: NIPS (2016)

15. Merkow, J., Kriegman, D., Marsden, A., Tu, Z.: Dense volume-to-volume vascular

boundary detection. In: MICCAI (2016)

16. Milletari, F., Navab, N., Ahmadi, S.A.: V-Net: Fully convolutional neural networks
for volumetric medical image segmentation. In: International Conference on 3D
Vision (2016)

17. Ny´ul, L.G., Udupa, J.K., Zhang, X.: New variants of a method of MRI scale stan-

dardization. IEEE Transactions on Medical Imaging 19(2) (2000)

18. Sankaran, S., Grady, L.J., Taylor, C.A.: Real-time sensitivity analysis of blood ﬂow

simulations to lumen segmentation uncertainty. In: MICCAI (2014)

19. Shi, W., Zhuang, X., Wolz, R., Simon, D., Tung, K., Wang, H., Ourselin, S., Ed-
wards, P., Razavi, R., Rueckert, D.: A multi-image graph cut approach for cardiac
image segmentation and uncertainty estimation. In: International Workshop on
Statistical Atlases and Computational Models of the Heart (2011)

20. Veit, A., Wilber, M., Belongie, S.: Residual networks are exponential ensembles of

relatively shallow networks. In: NIPS (2016)

On the Compactness, Eﬃciency, and
Representation of 3D Convolutional Networks:
Brain Parcellation as a Pretext Task

Wenqi Li, Guotai Wang, Lucas Fidon, Sebastien Ourselin, M. Jorge Cardoso,
and Tom Vercauteren

Translational Imaging Group, Centre for Medical Image Computing (CMIC),
University College London, London, UK
Wellcome/EPSRC Centre for Surgical and Interventional Science, University College
London, London, UK

Abstract. Deep convolutional neural networks are powerful tools for
learning visual representations from images. However, designing eﬃcient
deep architectures to analyse volumetric medical images remains chal-
lenging. This work investigates eﬃcient and ﬂexible elements of modern
convolutional networks such as dilated convolution and residual connec-
tion. With these essential building blocks, we propose a high-resolution,
compact convolutional network for volumetric image segmentation. To
illustrate its eﬃciency of learning 3D representation from large-scale im-
age data, the proposed network is validated with the challenging task
of parcellating 155 neuroanatomical structures from brain MR images.
Our experiments show that the proposed network architecture compares
favourably with state-of-the-art volumetric segmentation networks while
being an order of magnitude more compact. We consider the brain par-
cellation task as a pretext task for volumetric image segmentation; our
trained network potentially provides a good starting point for transfer
learning. Additionally, we show the feasibility of voxel-level uncertainty
estimation using a sampling approximation through dropout.

1

Introduction

Convolutional neural networks (CNNs) have been shown to be powerful tools for
learning visual representations from images. They often consist of multiple layers
of non-linear functions with a large number of trainable parameters. Hierarchical
features can be obtained by training the CNNs discriminatively.

In the medical image computing domain, recent years have seen a growing
number of applications using CNNs. Although there have been recent advances
in tailoring CNNs to analyse volumetric images, most of the work to date studies
image representations in 2D. While volumetric representations are more informa-
tive, the number of voxels scales cubically with the size of the region of interest.
This raises challenges of learning more complex visual patterns as well as higher
computational burden compared to the 2D cases. While developing compact and

7
1
0
2
 
l
u
J
 
6
 
 
]

V
C
.
s
c
[
 
 
1
v
2
9
9
1
0
.
7
0
7
1
:
v
i
X
r
a

2

Li et al.

eﬀective 3D network architectures is of signiﬁcant interest, designing 3D CNNs
remains a challenging problem.

The goal of this paper is to design a high-resolution and compact network
architecture for the segmentation of ﬁne structures in volumetric images. For
this purpose, we study the simple and ﬂexible elements of modern convolutional
networks, such as dilated convolution and residual connection. Most of the ex-
isting network architectures follow a fully convolutional downsample-upsample
pathway [11,4,15,3,16,13]. Low-level features with high spatial resolutions are
ﬁrst downsampled for higher-level feature abstraction; then the feature maps
are upsampled to achieve high-resolution segmentation. In contrast to these, we
propose a novel 3D architecture that incorporates high spatial resolution feature
maps throughout the layers, and can be trained with a wide range of receptive
ﬁelds. We validate our network with the challenging task of automated brain
parcellation into 155 structures from T1-weighted MR images. We show that
the proposed network, with twenty times fewer parameters, achieves competi-
tive segmentation performance compared with state-of-the-art architectures.

A well-designed network could be trained with a large-scale dataset and en-
ables transfer learning to other image recognition tasks [9]. In the ﬁeld of com-
puter vision, the well-known AlexNet and VGG net were trained on the Ima-
geNet dataset. They provide general-purpose image representations that can be
adapted for a wide range of computer vision problems. Given the large amount
of data and the complex visual patterns of the brain parcellation problem, we
consider it as a pretext task. Our trained network is the ﬁrst step towards a
general-purpose volumetric image representation. It potentially provides an ini-
tial model for transfer learning of other volumetric image segmentation tasks.

The uncertainty of the segmentation is also important for indicating the
conﬁdence and reliability of one algorithm [5,18,19]. The high uncertainty of la-
belling can be a sign of an unreliable classiﬁcation. In this work, we demonstrate
the feasibility of voxel-level uncertainty estimation using Monte Carlo samples
of the proposed network with dropout at test time. Compared to the existing
volumetric segmentation networks, our compact network has fewer parameter in-
teractions, and thus potentially achieves better uncertainty estimates with fewer
samples.

2 On the elements of 3D convolutional networks

Convolutions and dilated convolutions. To maintain a relatively low num-
ber of parameters, we choose to use small 3D convolutional kernels with only 33
parameters for all convolutions. This is about the smallest kernel that can rep-
resent 3D features in all directions with respect to the central voxel. Although a
convolutional kernel with 5 × 5 × 5 voxels gives the same receptive ﬁeld as stack-
ing two layers of 3 × 3 × 3-voxel convolution, the latter has approximately 57%
fewer parameters. Using smaller kernels implicitly imposes more regularisation
on the network while achieving the same receptive ﬁeld.

3D Convolutional Networks for Brain Parcellation

3

To further enlarge the receptive ﬁeld to capture large image contexts, most
of the existing volumetric segmentation networks downsample the intermediate
feature maps. This signiﬁcantly reduces the spatial resolution. For example, 3D
U-net [3] heavily employs 2×2×2-voxel max pooling with strides of two voxels in
each dimension. Each max pooling reduces the feature responses of the previous
layer to only 1/8 of its spatial resolution. Upsampling layers, such as decon-
volutions, are often used subsequently to partially recover the high resolution
of the input. However, adding deconvolution layers also introduces additional
computational costs.

Recently, Chen et al. [2] used dilated convolutions with upsampled kernels
for semantic image segmentation. The advantages of dilated convolutions are
that the features can be computed with a high spatial resolution, and the size of
the receptive ﬁeld can be enlarged arbitrarily. Dilated convolutions can be used
to produce accurate dense predictions and detailed segmentation maps along
object boundaries.

In contrast to the downsample-upsample pathway, we propose to adopt di-
lated convolutions for volumetric image segmentation. More speciﬁcally, the con-
volutional kernels are upsampled with a dilation factor r. For M -channels of
input feature maps I, the output feature channel O generated with dilated con-
volutions are:

Ox,y,z =

wi,j,k,mI(x+ir),(y+jr),(z+kr),m ;

(1)

M −1
(cid:88)

2
(cid:88)

2
(cid:88)

2
(cid:88)

m=0

i=0

j=0

k=0

where the index tuple (x, y, z) runs through every spatial location in the volumes;
the kernels w consist of 33 × M trainable parameters. The dilated convolution in
Eq. (1) has the same number of trainable parameters as the standard 3 × 3 × 3
convolution. It preserves the spatial resolution and provides a (2r + 1)3-voxel
receptive ﬁeld. Setting r to 1 reduces the dilated convolution to the standard
3 × 3 × 3 convolution. In practice, we implement 3D dilated convolutions with
a split-and-merge strategy [2] to beneﬁt from the existing GPU convolution
routines.

Residual connections. Residual connections
were ﬁrst introduced and later reﬁned by He et
al. [7,8] for the eﬀective training of deep networks.
The key idea of residual connection is to create
identity mapping connections to bypass the pa-
rameterised layers in a network. The input of a
residual block is directly merged to the output
by addition. The residual connections have been
shown to make information propagation smooth
and improve the training speed [7].

Fig. 1. A block with residual
connections.

More speciﬁcally, let the input to the p-th layer of a residual block as xp, the
output of the block xp+1 has the form: xp+1 = xp + F (xp, wp); where F (xp, wp)

4

Li et al.

denotes the path with non-linear functions in the block (shown in Fig. 1). If we
stack the residual blocks, the last layer output xl can be expressed as: xl = xp +
(cid:80)l−1
i=p F (xi, wi). The residual connections enables direct information propagation
from any residual block to another in both forward pass and back-propagation.

Eﬀective receptive ﬁeld. One interpretation of the residual network is that
they behave like ensembles of relatively shallow networks. The unravelled view of
the residual connections proposed by Veit et al. [20] suggests that the networks
with n residual blocks have a collection of 2n unique paths.

Without residual connections, the receptive ﬁeld of a network is generally
considered ﬁxed. However, when training with n residual blocks, the networks
utilise 2n diﬀerent paths and therefore features can be learned with a large range
of diﬀerent receptive ﬁelds. For example, the proposed network with 9 residual
blocks (see Section 3) has a maximum receptive ﬁeld of 87 × 87 × 87 voxels.
Following the unravel view of the residual network, it consists of 29 unique paths.
Fig. 2 shows the distribution of the receptive ﬁeld of these paths. The receptive
ﬁelds range from 3 × 3 × 3 to 87 × 87 × 87, following a binomial distribution.

This diﬀers from the existing 3D networks.
For example, Deepmedic [11] model operates
at two paths, with a ﬁxed receptive ﬁeld 17 ×
17 × 17 and 42 × 42 × 42 respectively. 3D U-
net [3] has a relatively large receptive ﬁeld of
about 88 × 88 × 88 voxels. However, there are
only eight unique paths and receptive ﬁelds.

Fig. 2. Histogram of the receptive
ﬁelds.

Intuitively, given that the receptive ﬁeld
of a deep convolutional network is relatively
large, the segmentation maps will suﬀer from
distortions due to the border eﬀects of convo-
lution. That is, the segmentation results near the border of the output volume
are less accurate due to the lack of input supporting window. We conduct ex-
periments and demonstrate that the proposed networks generate only a small
distortion near the borders (See Section 4). This suggests training the network
with residual connections reduces the eﬀective receptive ﬁeld. The width of the
distorted border is much smaller than the maximum receptive ﬁeld. This phe-
nomenon was also recently analysed by Luo et al. [14]. In practice, at test time
we pad each input volume with a border of zeros and discard the same amount
of border in the segmentation output.

Loss function. The last layer of the network is a softmax function that gives
scores over all labels for each voxel. Typically, the end-to-end training proce-
dure minimises the cross entropy loss function using an N -voxel image volume
{xn}N
n=1 where

n=1 and the training data of C-class segmentation map {yn}N

3D Convolutional Networks for Brain Parcellation

5

yn ∈ {1, . . . , C} is:

L({xn}, {yn}) = −

δ(yn = c) log Fc(xn),

(2)

1
N

N
(cid:88)

C
(cid:88)

n=1

c=1

where δ corresponds to the Dirac delta function, Fc(xn) is the softmax classiﬁca-
tion score of xn over the c-th class. However, when the training data are severely
unbalanced (which is typical in medical image segmentation problems), this for-
mulation leads to a strongly biased estimation towards the majority class. In-
stead of directly re-weighting each voxel by class frequencies, Milletari et al. [16]
propose a solution by maximising the mean Dice coeﬃcient directly, i.e.,

D({xn}, {yn}) =

1
C

C
(cid:88)

c=1

2 (cid:80)N

n=1 δ(yn = c)Fc(xn)

(cid:80)N

n=1[δ(yn = c)]2 + (cid:80)N

n=1[Fc(xn)]2

.

(3)

We employ this formulation to handle the issue of training data imbalance.

Uncertainty estimation using dropout. Gal and Ghahramani demonstrated
that the deep network trained with dropout can be cast as a Bayesian approxi-
mation of the Gaussian process [5]. Given a set of training data and their labels
{X, Y}, training a network F (· , W) with dropout has the eﬀect of approxi-
mating the posterior distribution p(W|{X, Y}) by minimising the Kullback-
Leibler divergence term, i.e. KL(q(W)||p(W|{X, Y})); where q(W) is an ap-
proximating distribution over the weight matrices W with their elements ran-
domly set to zero according to Bernoulli random variables. After training the
network, the predictive distribution of test data ˆx can be expressed as q(ˆy|ˆx) =
(cid:82) F (ˆx, W)q(W)dW. The prediction can be approximated using Monte Carlo
samples of the trained network: ˆy = 1
m=1 is a
M
set of M samples from q(W). The uncertainty of the prediction can be estimated
using the sample variance of the M samples.

m=1 F (ˆx, Wm), where {Wm}M

(cid:80)M

With this theoretical insight, we are able to estimate the uncertainty of the
segmentation map at the voxel level. We extend the segmentation network with
a 1 × 1 × 1 convolutional layer before the last convolutional layer. The extended
network is trained with a dropout ratio of 0.5 applied to the newly inserted
layer. At test time, we sample the network N times using dropout. The ﬁnal
segmentation is obtained by majority voting. The percentage of samples which
disagrees with the voting results is calculated at each voxel as the uncertainty
estimates.

3 The network architecture and its implementation

3.1 The proposed architecture

Our network consists of 20 layers of convolutions. In the ﬁrst seven convolu-
tional layers, we adopt 3 × 3 × 3-voxel convolutions. These layers are designed

6

Li et al.

Fig. 3. The proposed network architecture for volumetric image segmentation. The
network mainly utilises dilated convolutions and residual connections to make an end-
to-end mapping from image volume to a voxel-level dense segmentation. To incorporate
features at multiple scales, the dilation factor of the dilated convolutions is gradually
increased when the layer goes deeper. The residual blocks with identity mapping enable
the direct fusion of features from diﬀerent scales. The spatial resolution of the input
volume is maintained throughout the network.

to capture low-level image features such as edges and corners. In the subsequent
convolutional layers, the kernels are dilated by a factor of two or four. These
deeper layers with dilated kernels encode mid- and high-level image features.

Residual connections are employed to group every two convolutional lay-
ers. Within each residual block, each convolutional layer is associated with
an element-wise rectiﬁed linear unit (ReLU) layer and a batch normalisation
layer [10]. The ReLU, batch normalisation, and convolutional layers are arranged
in the pre-activation order [8].

The network can be trained end-to-end. In the training stage, the inputs
to our network are 96 × 96 × 96-voxel images. The ﬁnal softmax layer gives
classiﬁcation scores over the class labels for each of the 96 × 96 × 96 voxels. The
architecture is illustrated in Fig. 3.

3.2 Implementation details

In the training stage, the pre-processing step involved input data standardisa-
tion and augmentation at both image- and subvolume-level. At image-level, we
adopted the histogram-based scale standardisation method [17] to normalised
the intensity histograms. As a data augmentation at image-level, randomisation
was introduced in the normalisation process by randomly choosing a threshold
of foreground between the volume minimum and mean intensity (at test time,
the mean intensity of the test volume was used as the threshold). Each image
was further normalised to have zero mean and unit standard deviation. Augmen-
tations on the randomly sampled 96 × 96 × 96 subvolumes were employed on the
ﬂy. These included rotation with a random angle in the range of [−10◦, 10◦] for

3D Convolutional Networks for Brain Parcellation

7

each of the three orthogonal planes and spatial rescaling with a random scaling
factor in the range of [0.9, 1.1].

All the parameters in the convolutional layers were initialised according to
He et al. [6]. The scaling and shifting parameters in the batch normalisation
layers were initialised to 1 and 0 respectively. The networks were trained with
two Nvidia K80 GPUs. At each training iteration, each GPU processed one in-
put volume; the average gradients computed over these two training volumes
were used as the gradients update. To make a fair comparison, we employed the
Adam optimisation method [12] for all the methods with ﬁxed hyper-parameters.
The learning rate lr was set to 0.01, the step size hyper-parameter β1 was 0.9
and β2 was 0.999 in all cases, except V-Net for which we chose the largest lr
that the training algorithm converges (lr = 0.0001). The models were trained
until we observed a plateau in performance on the validation set. We do not
employ additional spatial smoothing function (such as conditional random ﬁeld)
as a post-processing step. Instead of aiming for better segmentation results by
adding post-processing steps, we focused on the dense segmentation maps gen-
erated by the networks. As we consider brain parcellation as a pretext task,
networks without explicit spatial smoothing are potentially more reusable. We
implemented all the methods (including a re-implementation of Deepmedic [11],
V-net [16], and 3D U-net [3] architecture) with Tensorﬂow1.

4 Experiments and results

Data. To demonstrate the feasibility of learning complex 3D image representa-
tions from large-scale data, the proposed network is learning a highly granular
segmentation of 543 T1-weighted MR images of healthy controls from the ADNI
dataset. The average number of voxels of each volume is about 182 × 244 × 246.
The average voxel size is approximately 1.18mm × 1.05mm × 1.05mm. All vol-
umes are bias-corrected and reoriented to a standard Right-Anterior-Superior
orientation. The bronze standard parcellation of 155 brain structures and 5 non-
brain outer tissues are obtained using the GIF framework [1]. Fig. 5(left) shows
the label distribution of the dataset. We randomly choose 443, 50, and 50 vol-
umes for training, test, and validation respectively.

Overall evaluation. In this section, we compare the proposed high-resolution
compact network architecture (illustrated in Fig. 3; denoted as HC-default) with
three variants: (1) the HC-default conﬁguration without the residual connec-
tions, trained with cross-entropy loss function (NoRes-entropy); (2) the HC-
default conﬁguration without residual connections, trained with Dice loss func-
tion (NoRes-dice); and (3) the HC-default conﬁguration trained with an addi-
tional dropout layer, and makes predictions with a majority voting of 10 Monte
Carlo samples (HC-dropout). For the dropout variant, our dropout layer em-
ployed before the last convolutional layer consists of 80 kernels.

1 The source code is available at https://github.com/gift-surg/HighRes3DNet

8

Li et al.

(1)

(2)

(3)

(4)

(5)

Fig. 4. Visualisations of segmentation results. (1) slices from a test image volume,
segmentation maps and false prediction maps generated by HC-dropout (2, 3), and 3D
U-net-dice (4, 5).

Table 1. Comparison of diﬀerent 3D convolutional network architectures.

Architecture

Multi-layer fusion Num. param. Loss type DCS (%) STD (%)

HC-default
HC-dropout
NoRes-entropy
NoRes-dice
Deepmedic[11]-dice Two pathways
3D U-net[3]-dice
V-net[16]

Residual
Residual
N/A
N/A

Feature forwarding
Feature forwarding

0.81M
0.82M
0.81M
0.81M
0.68M
19.08M
62.63M

Dice loss
Dice loss
Cross entr.
Dice loss
Dice loss
Dice loss
Dice loss

82.05
84.34
39.36
75.47
78.74
80.18
74.58

2.96
1.89
1.13
2.97
1.72
6.18
1.86

Additionally, three state-of-the-art volumetric segmentation networks are
evaluated. These include 3D U-net [3], V-net [16], and Deepmedic [11]. The last
layer of each network architecture is replaced with a 160-way softmax classiﬁer.
We observe that training these networks with the cross entropy loss function
(Eq. 2) leads to poor segmentation results. Since the cross-entropy loss function
treats all training voxels equally, the network may have diﬃculties in learning
representations related to the minority classes. Training with the Dice loss func-
tion alleviates this issue by implicitly re-weighting the voxels. Thus we train all
networks using the Dice loss function for a fair comparison.

We use the mean Dice Coeﬃcient Similarity (DCS) as the performance mea-
sure. Table 1 and Fig. 5(right) compare the performance on the test set. In
terms of our network variants, the results show that the use of Dice loss function
largely improves the segmentation performance. This suggests that the Dice loss

3D Convolutional Networks for Brain Parcellation

9

Fig. 5. Left: label distribution of the dataset; right: comparison of diﬀerent network
architectures.

Fig. 6. Segmentation performance against a set of key structures.

function can handle the severely unbalanced segmentation problem well. The
results also suggest that introducing the residual connections improved the seg-
mentation performance measured in mean DCS. This indicates that the residual
connections are important elements of the proposed network. By adopting the
dropout method, the DCS can be further improved by 2% in DCS.

With a relatively small number of parameters, our HC-default and HC-
dropout outperform the competing methods in terms of mean DCS. This sug-
gests that our network is more eﬀective for the brain parcellation problem. Note
that V-net has a similar architecture to 3D U-net and has more parameters, but
does not employ the batch normalisation technique. The lower DCS produced by
V-net suggests that batch normalisation is important for training the networks
for brain parcellation.

10

Li et al.

In Fig. 6, we show that the dropout variant achieves better segmentation
results for all the key structures. Fig. 4 presents an example of the segmentation
results of the proposed network and 3D U-net-Dice.

Receptive ﬁeld and border eﬀects. We further compare the segmentation
performance of a trained network by discarding the borders in each dimension of
the segmentation map. That is, given a d × d × d-voxel input, at border size 1 we
only preserve the (d−2)3-voxel output volume centred within the predicted map.
Fig. 7 plots the DCS and standard er-
rors of segmentation according to the
size of the segmentation borders in
each dimension. The results show that
the distorted border is around 17 vox-
els in each dimension. The border ef-
fects do not severely decrease the seg-
mentation performance. In practice,
we pad the volume images with 16 ze-
ros in each dimension, and remove the
same amount of borders in the seg-
mentation output.

Fig. 7. Empirical analysis of the segmen-
tation borders. Voxels near to the volume
borders are classiﬁed less accurately.

The eﬀect of number of samples
in uncertainty estimations. This section investigates the number of Monte
Carlo samples and the segmentation performance of the proposed network.
Fig. 8(a) suggests that using 10 samples is enough to achieve good segmen-
tation. Further increasing the number of samples has relatively small eﬀects on
the DCS. Fig. 8(b) plots the voxel-wise segmentation accuracy computed using
only the voxels with an uncertainty less than a threshold. The voxel-wise ac-
curacy is high when the threshold is small. This indicates that the uncertainty

(a)

(b)

Fig. 8. Evaluation of dropout sampling. (a) The segmentation performance against the
number of Monte Carlo samples. (b) voxel-level segmentation accuracy by thresholding
the uncertainties. The shaded area represents the standard errors.

3D Convolutional Networks for Brain Parcellation

11

Fig. 9. Voxel-level segmentation uncertainty estimations. Top row: uncertainty map
generated with 100 Monte Carlo samples using dropout. Bottom row: uncertainty map
thresholded at 0.1.

estimation reﬂects the conﬁdence of the network. Fig. 9 shows an uncertainty
map generated by the proposed network. The uncertainties near the boundaries
of diﬀerent structures are relatively higher than the other regions.

Currently, our method takes about 60 seconds to predict a typical volume
with 192 × 256 × 256 voxels. To achieve better segmentation results and measure
uncertainty, 10 Monte Carlo samples of our dropout model are required. The
entire process takes slightly more than 10 minutes in total. However, during
the Monte Carlo sampling at test time, only the dropout layer and the ﬁnal
prediction layer are randomised. To further reduce the computational time, the
future software could reuse the features extracted from the layers before dropout,
resulting in only a marginal increase in runtime when compared to a single
prediction.

5 Conclusion

In this paper, we propose a high-resolution, 3D convolutional network archi-
tecture that incorporates large volumetric context using dilated convolutions
and residual connections. Our network is conceptually simpler and more com-
pact than the state-of-the-art volumetric segmentation networks. We validate the
proposed network using the challenging task of brain parcellation in MR images.
We show that the segmentation performance of our network compares favourably
with the competing methods. Additionally, we demonstrate that Monte Carlo
sampling of dropout technique can be used to generate voxel-level uncertainty es-

12

Li et al.

timation for our brain parcellation network. Moreover, we consider the brain par-
cellation task as a pretext task for volumetric image segmentation. Our trained
network potentially provides a good starting point for transfer learning of other
segmentation tasks.

In the future, we will extensively test the generalisation ability of the network
to brain MR scans obtained with various scanning protocols from diﬀerent data
centres. Furthermore, we note that the uncertainty estimations are not proba-
bilities. We will investigate the calibration of the uncertainty scores to provide
reliable probability estimations.

Acknowledgements. This work was supported through an Innovative Engi-
neering for Health award by the Wellcome Trust [WT101957, 203145Z/16/Z],
Engineering and Physical Sciences Research Council (EPSRC) [NS/A000027/1,
NS/A000050/1], the National Institute for Health Research University College
London Hospitals Biomedical Research Centre (NIHR BRC UCLH/UCL High
Impact Initiative), UCL EPSRC CDT Scholarship Award [EP/L016478/1], a
UCL Overseas Research Scholarship, a UCL Graduate Research Scholarship,
and the Health Innovation Challenge Fund [HICF-T4-275, WT 97914], a paral-
lel funding partnership between the Department of Health and Wellcome Trust.
The authors would also like to acknowledge that the work presented here made
use of Emerald, a GPU-accelerated High Performance Computer, made available
by the Science & Engineering South Consortium operated in partnership with
the STFC Rutherford-Appleton Laboratory.

References

1. Cardoso, M.J., Modat, M., Wolz, R., Melbourne, A., Cash, D., Rueckert, D.,
Ourselin, S.: Geodesic information ﬂows: Spatially-variant graphs and their appli-
cation to segmentation and fusion. IEEE Transactions on Medical Imaging 34(9)
(2015)

2. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: DeepLab:
Semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected CRFs. arXiv:1606.00915 (2016)

3. C¸ i¸cek, ¨O., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O.: 3D U-
Net: Learning dense volumetric segmentation from sparse annotation. In: MICCAI
(2016)

4. Dou, Q., Chen, H., Yu, L., Zhao, L., Qin, J., Wang, D., Mok, V.C., Shi, L., Heng,
P.A.: Automatic detection of cerebral microbleeds from MR images via 3D convo-
lutional neural networks. IEEE Transactions on Medical Imaging 35(5) (2016)
5. Gal, Y., Ghahramani, Z.: Dropout as a Bayesian approximation: Representing

model uncertainty in deep learning. In: ICML (2016)

6. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectiﬁers: Surpassing human-

level performance on imagenet classiﬁcation. In: ICCV (2015)

7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

8. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.

In: CVPR (2016)

In: ECCV (2016)

3D Convolutional Networks for Brain Parcellation

13

9. Huh, M., Agrawal, P., Efros, A.A.: What makes ImageNet good for transfer learn-

ing? arXiv:1608.08614 (2016)

10. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML (2015)

11. Kamnitsas, K., Ledig, C., Newcombe, V.F., Simpson, J.P., Kane, A.D., Menon,
D.K., Rueckert, D., Glocker, B.: Eﬃcient multi-scale 3D CNN with fully connected
CRF for accurate brain lesion segmentation. Medical Image Analysis 36 (2017)
12. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. arXiv:1412.6980

(2014)

13. Kleesiek, J., Urban, G., Hubert, A., Schwarz, D., Maier-Hein, K., Bendszus, M.,
Biller, A.: Deep MRI brain extraction: A 3D convolutional neural network for skull
stripping. NeuroImage 129 (2016)

14. Luo, W., Li, Y., Urtasun, R., Zemel, R.: Understanding the eﬀective receptive ﬁeld

in deep convolutional neural networks. In: NIPS (2016)

15. Merkow, J., Kriegman, D., Marsden, A., Tu, Z.: Dense volume-to-volume vascular

boundary detection. In: MICCAI (2016)

16. Milletari, F., Navab, N., Ahmadi, S.A.: V-Net: Fully convolutional neural networks
for volumetric medical image segmentation. In: International Conference on 3D
Vision (2016)

17. Ny´ul, L.G., Udupa, J.K., Zhang, X.: New variants of a method of MRI scale stan-

dardization. IEEE Transactions on Medical Imaging 19(2) (2000)

18. Sankaran, S., Grady, L.J., Taylor, C.A.: Real-time sensitivity analysis of blood ﬂow

simulations to lumen segmentation uncertainty. In: MICCAI (2014)

19. Shi, W., Zhuang, X., Wolz, R., Simon, D., Tung, K., Wang, H., Ourselin, S., Ed-
wards, P., Razavi, R., Rueckert, D.: A multi-image graph cut approach for cardiac
image segmentation and uncertainty estimation. In: International Workshop on
Statistical Atlases and Computational Models of the Heart (2011)

20. Veit, A., Wilber, M., Belongie, S.: Residual networks are exponential ensembles of

relatively shallow networks. In: NIPS (2016)

On the Compactness, Eﬃciency, and
Representation of 3D Convolutional Networks:
Brain Parcellation as a Pretext Task

Wenqi Li, Guotai Wang, Lucas Fidon, Sebastien Ourselin, M. Jorge Cardoso,
and Tom Vercauteren

Translational Imaging Group, Centre for Medical Image Computing (CMIC),
University College London, London, UK
Wellcome/EPSRC Centre for Surgical and Interventional Science, University College
London, London, UK

Abstract. Deep convolutional neural networks are powerful tools for
learning visual representations from images. However, designing eﬃcient
deep architectures to analyse volumetric medical images remains chal-
lenging. This work investigates eﬃcient and ﬂexible elements of modern
convolutional networks such as dilated convolution and residual connec-
tion. With these essential building blocks, we propose a high-resolution,
compact convolutional network for volumetric image segmentation. To
illustrate its eﬃciency of learning 3D representation from large-scale im-
age data, the proposed network is validated with the challenging task
of parcellating 155 neuroanatomical structures from brain MR images.
Our experiments show that the proposed network architecture compares
favourably with state-of-the-art volumetric segmentation networks while
being an order of magnitude more compact. We consider the brain par-
cellation task as a pretext task for volumetric image segmentation; our
trained network potentially provides a good starting point for transfer
learning. Additionally, we show the feasibility of voxel-level uncertainty
estimation using a sampling approximation through dropout.

1

Introduction

Convolutional neural networks (CNNs) have been shown to be powerful tools for
learning visual representations from images. They often consist of multiple layers
of non-linear functions with a large number of trainable parameters. Hierarchical
features can be obtained by training the CNNs discriminatively.

In the medical image computing domain, recent years have seen a growing
number of applications using CNNs. Although there have been recent advances
in tailoring CNNs to analyse volumetric images, most of the work to date studies
image representations in 2D. While volumetric representations are more informa-
tive, the number of voxels scales cubically with the size of the region of interest.
This raises challenges of learning more complex visual patterns as well as higher
computational burden compared to the 2D cases. While developing compact and

7
1
0
2
 
l
u
J
 
6
 
 
]

V
C
.
s
c
[
 
 
1
v
2
9
9
1
0
.
7
0
7
1
:
v
i
X
r
a

2

Li et al.

eﬀective 3D network architectures is of signiﬁcant interest, designing 3D CNNs
remains a challenging problem.

The goal of this paper is to design a high-resolution and compact network
architecture for the segmentation of ﬁne structures in volumetric images. For
this purpose, we study the simple and ﬂexible elements of modern convolutional
networks, such as dilated convolution and residual connection. Most of the ex-
isting network architectures follow a fully convolutional downsample-upsample
pathway [11,4,15,3,16,13]. Low-level features with high spatial resolutions are
ﬁrst downsampled for higher-level feature abstraction; then the feature maps
are upsampled to achieve high-resolution segmentation. In contrast to these, we
propose a novel 3D architecture that incorporates high spatial resolution feature
maps throughout the layers, and can be trained with a wide range of receptive
ﬁelds. We validate our network with the challenging task of automated brain
parcellation into 155 structures from T1-weighted MR images. We show that
the proposed network, with twenty times fewer parameters, achieves competi-
tive segmentation performance compared with state-of-the-art architectures.

A well-designed network could be trained with a large-scale dataset and en-
ables transfer learning to other image recognition tasks [9]. In the ﬁeld of com-
puter vision, the well-known AlexNet and VGG net were trained on the Ima-
geNet dataset. They provide general-purpose image representations that can be
adapted for a wide range of computer vision problems. Given the large amount
of data and the complex visual patterns of the brain parcellation problem, we
consider it as a pretext task. Our trained network is the ﬁrst step towards a
general-purpose volumetric image representation. It potentially provides an ini-
tial model for transfer learning of other volumetric image segmentation tasks.

The uncertainty of the segmentation is also important for indicating the
conﬁdence and reliability of one algorithm [5,18,19]. The high uncertainty of la-
belling can be a sign of an unreliable classiﬁcation. In this work, we demonstrate
the feasibility of voxel-level uncertainty estimation using Monte Carlo samples
of the proposed network with dropout at test time. Compared to the existing
volumetric segmentation networks, our compact network has fewer parameter in-
teractions, and thus potentially achieves better uncertainty estimates with fewer
samples.

2 On the elements of 3D convolutional networks

Convolutions and dilated convolutions. To maintain a relatively low num-
ber of parameters, we choose to use small 3D convolutional kernels with only 33
parameters for all convolutions. This is about the smallest kernel that can rep-
resent 3D features in all directions with respect to the central voxel. Although a
convolutional kernel with 5 × 5 × 5 voxels gives the same receptive ﬁeld as stack-
ing two layers of 3 × 3 × 3-voxel convolution, the latter has approximately 57%
fewer parameters. Using smaller kernels implicitly imposes more regularisation
on the network while achieving the same receptive ﬁeld.

3D Convolutional Networks for Brain Parcellation

3

To further enlarge the receptive ﬁeld to capture large image contexts, most
of the existing volumetric segmentation networks downsample the intermediate
feature maps. This signiﬁcantly reduces the spatial resolution. For example, 3D
U-net [3] heavily employs 2×2×2-voxel max pooling with strides of two voxels in
each dimension. Each max pooling reduces the feature responses of the previous
layer to only 1/8 of its spatial resolution. Upsampling layers, such as decon-
volutions, are often used subsequently to partially recover the high resolution
of the input. However, adding deconvolution layers also introduces additional
computational costs.

Recently, Chen et al. [2] used dilated convolutions with upsampled kernels
for semantic image segmentation. The advantages of dilated convolutions are
that the features can be computed with a high spatial resolution, and the size of
the receptive ﬁeld can be enlarged arbitrarily. Dilated convolutions can be used
to produce accurate dense predictions and detailed segmentation maps along
object boundaries.

In contrast to the downsample-upsample pathway, we propose to adopt di-
lated convolutions for volumetric image segmentation. More speciﬁcally, the con-
volutional kernels are upsampled with a dilation factor r. For M -channels of
input feature maps I, the output feature channel O generated with dilated con-
volutions are:

Ox,y,z =

wi,j,k,mI(x+ir),(y+jr),(z+kr),m ;

(1)

M −1
(cid:88)

2
(cid:88)

2
(cid:88)

2
(cid:88)

m=0

i=0

j=0

k=0

where the index tuple (x, y, z) runs through every spatial location in the volumes;
the kernels w consist of 33 × M trainable parameters. The dilated convolution in
Eq. (1) has the same number of trainable parameters as the standard 3 × 3 × 3
convolution. It preserves the spatial resolution and provides a (2r + 1)3-voxel
receptive ﬁeld. Setting r to 1 reduces the dilated convolution to the standard
3 × 3 × 3 convolution. In practice, we implement 3D dilated convolutions with
a split-and-merge strategy [2] to beneﬁt from the existing GPU convolution
routines.

Residual connections. Residual connections
were ﬁrst introduced and later reﬁned by He et
al. [7,8] for the eﬀective training of deep networks.
The key idea of residual connection is to create
identity mapping connections to bypass the pa-
rameterised layers in a network. The input of a
residual block is directly merged to the output
by addition. The residual connections have been
shown to make information propagation smooth
and improve the training speed [7].

Fig. 1. A block with residual
connections.

More speciﬁcally, let the input to the p-th layer of a residual block as xp, the
output of the block xp+1 has the form: xp+1 = xp + F (xp, wp); where F (xp, wp)

4

Li et al.

denotes the path with non-linear functions in the block (shown in Fig. 1). If we
stack the residual blocks, the last layer output xl can be expressed as: xl = xp +
(cid:80)l−1
i=p F (xi, wi). The residual connections enables direct information propagation
from any residual block to another in both forward pass and back-propagation.

Eﬀective receptive ﬁeld. One interpretation of the residual network is that
they behave like ensembles of relatively shallow networks. The unravelled view of
the residual connections proposed by Veit et al. [20] suggests that the networks
with n residual blocks have a collection of 2n unique paths.

Without residual connections, the receptive ﬁeld of a network is generally
considered ﬁxed. However, when training with n residual blocks, the networks
utilise 2n diﬀerent paths and therefore features can be learned with a large range
of diﬀerent receptive ﬁelds. For example, the proposed network with 9 residual
blocks (see Section 3) has a maximum receptive ﬁeld of 87 × 87 × 87 voxels.
Following the unravel view of the residual network, it consists of 29 unique paths.
Fig. 2 shows the distribution of the receptive ﬁeld of these paths. The receptive
ﬁelds range from 3 × 3 × 3 to 87 × 87 × 87, following a binomial distribution.

This diﬀers from the existing 3D networks.
For example, Deepmedic [11] model operates
at two paths, with a ﬁxed receptive ﬁeld 17 ×
17 × 17 and 42 × 42 × 42 respectively. 3D U-
net [3] has a relatively large receptive ﬁeld of
about 88 × 88 × 88 voxels. However, there are
only eight unique paths and receptive ﬁelds.

Fig. 2. Histogram of the receptive
ﬁelds.

Intuitively, given that the receptive ﬁeld
of a deep convolutional network is relatively
large, the segmentation maps will suﬀer from
distortions due to the border eﬀects of convo-
lution. That is, the segmentation results near the border of the output volume
are less accurate due to the lack of input supporting window. We conduct ex-
periments and demonstrate that the proposed networks generate only a small
distortion near the borders (See Section 4). This suggests training the network
with residual connections reduces the eﬀective receptive ﬁeld. The width of the
distorted border is much smaller than the maximum receptive ﬁeld. This phe-
nomenon was also recently analysed by Luo et al. [14]. In practice, at test time
we pad each input volume with a border of zeros and discard the same amount
of border in the segmentation output.

Loss function. The last layer of the network is a softmax function that gives
scores over all labels for each voxel. Typically, the end-to-end training proce-
dure minimises the cross entropy loss function using an N -voxel image volume
{xn}N
n=1 where

n=1 and the training data of C-class segmentation map {yn}N

3D Convolutional Networks for Brain Parcellation

5

yn ∈ {1, . . . , C} is:

L({xn}, {yn}) = −

δ(yn = c) log Fc(xn),

(2)

1
N

N
(cid:88)

C
(cid:88)

n=1

c=1

where δ corresponds to the Dirac delta function, Fc(xn) is the softmax classiﬁca-
tion score of xn over the c-th class. However, when the training data are severely
unbalanced (which is typical in medical image segmentation problems), this for-
mulation leads to a strongly biased estimation towards the majority class. In-
stead of directly re-weighting each voxel by class frequencies, Milletari et al. [16]
propose a solution by maximising the mean Dice coeﬃcient directly, i.e.,

D({xn}, {yn}) =

1
C

C
(cid:88)

c=1

2 (cid:80)N

n=1 δ(yn = c)Fc(xn)

(cid:80)N

n=1[δ(yn = c)]2 + (cid:80)N

n=1[Fc(xn)]2

.

(3)

We employ this formulation to handle the issue of training data imbalance.

Uncertainty estimation using dropout. Gal and Ghahramani demonstrated
that the deep network trained with dropout can be cast as a Bayesian approxi-
mation of the Gaussian process [5]. Given a set of training data and their labels
{X, Y}, training a network F (· , W) with dropout has the eﬀect of approxi-
mating the posterior distribution p(W|{X, Y}) by minimising the Kullback-
Leibler divergence term, i.e. KL(q(W)||p(W|{X, Y})); where q(W) is an ap-
proximating distribution over the weight matrices W with their elements ran-
domly set to zero according to Bernoulli random variables. After training the
network, the predictive distribution of test data ˆx can be expressed as q(ˆy|ˆx) =
(cid:82) F (ˆx, W)q(W)dW. The prediction can be approximated using Monte Carlo
samples of the trained network: ˆy = 1
m=1 is a
M
set of M samples from q(W). The uncertainty of the prediction can be estimated
using the sample variance of the M samples.

m=1 F (ˆx, Wm), where {Wm}M

(cid:80)M

With this theoretical insight, we are able to estimate the uncertainty of the
segmentation map at the voxel level. We extend the segmentation network with
a 1 × 1 × 1 convolutional layer before the last convolutional layer. The extended
network is trained with a dropout ratio of 0.5 applied to the newly inserted
layer. At test time, we sample the network N times using dropout. The ﬁnal
segmentation is obtained by majority voting. The percentage of samples which
disagrees with the voting results is calculated at each voxel as the uncertainty
estimates.

3 The network architecture and its implementation

3.1 The proposed architecture

Our network consists of 20 layers of convolutions. In the ﬁrst seven convolu-
tional layers, we adopt 3 × 3 × 3-voxel convolutions. These layers are designed

6

Li et al.

Fig. 3. The proposed network architecture for volumetric image segmentation. The
network mainly utilises dilated convolutions and residual connections to make an end-
to-end mapping from image volume to a voxel-level dense segmentation. To incorporate
features at multiple scales, the dilation factor of the dilated convolutions is gradually
increased when the layer goes deeper. The residual blocks with identity mapping enable
the direct fusion of features from diﬀerent scales. The spatial resolution of the input
volume is maintained throughout the network.

to capture low-level image features such as edges and corners. In the subsequent
convolutional layers, the kernels are dilated by a factor of two or four. These
deeper layers with dilated kernels encode mid- and high-level image features.

Residual connections are employed to group every two convolutional lay-
ers. Within each residual block, each convolutional layer is associated with
an element-wise rectiﬁed linear unit (ReLU) layer and a batch normalisation
layer [10]. The ReLU, batch normalisation, and convolutional layers are arranged
in the pre-activation order [8].

The network can be trained end-to-end. In the training stage, the inputs
to our network are 96 × 96 × 96-voxel images. The ﬁnal softmax layer gives
classiﬁcation scores over the class labels for each of the 96 × 96 × 96 voxels. The
architecture is illustrated in Fig. 3.

3.2 Implementation details

In the training stage, the pre-processing step involved input data standardisa-
tion and augmentation at both image- and subvolume-level. At image-level, we
adopted the histogram-based scale standardisation method [17] to normalised
the intensity histograms. As a data augmentation at image-level, randomisation
was introduced in the normalisation process by randomly choosing a threshold
of foreground between the volume minimum and mean intensity (at test time,
the mean intensity of the test volume was used as the threshold). Each image
was further normalised to have zero mean and unit standard deviation. Augmen-
tations on the randomly sampled 96 × 96 × 96 subvolumes were employed on the
ﬂy. These included rotation with a random angle in the range of [−10◦, 10◦] for

3D Convolutional Networks for Brain Parcellation

7

each of the three orthogonal planes and spatial rescaling with a random scaling
factor in the range of [0.9, 1.1].

All the parameters in the convolutional layers were initialised according to
He et al. [6]. The scaling and shifting parameters in the batch normalisation
layers were initialised to 1 and 0 respectively. The networks were trained with
two Nvidia K80 GPUs. At each training iteration, each GPU processed one in-
put volume; the average gradients computed over these two training volumes
were used as the gradients update. To make a fair comparison, we employed the
Adam optimisation method [12] for all the methods with ﬁxed hyper-parameters.
The learning rate lr was set to 0.01, the step size hyper-parameter β1 was 0.9
and β2 was 0.999 in all cases, except V-Net for which we chose the largest lr
that the training algorithm converges (lr = 0.0001). The models were trained
until we observed a plateau in performance on the validation set. We do not
employ additional spatial smoothing function (such as conditional random ﬁeld)
as a post-processing step. Instead of aiming for better segmentation results by
adding post-processing steps, we focused on the dense segmentation maps gen-
erated by the networks. As we consider brain parcellation as a pretext task,
networks without explicit spatial smoothing are potentially more reusable. We
implemented all the methods (including a re-implementation of Deepmedic [11],
V-net [16], and 3D U-net [3] architecture) with Tensorﬂow1.

4 Experiments and results

Data. To demonstrate the feasibility of learning complex 3D image representa-
tions from large-scale data, the proposed network is learning a highly granular
segmentation of 543 T1-weighted MR images of healthy controls from the ADNI
dataset. The average number of voxels of each volume is about 182 × 244 × 246.
The average voxel size is approximately 1.18mm × 1.05mm × 1.05mm. All vol-
umes are bias-corrected and reoriented to a standard Right-Anterior-Superior
orientation. The bronze standard parcellation of 155 brain structures and 5 non-
brain outer tissues are obtained using the GIF framework [1]. Fig. 5(left) shows
the label distribution of the dataset. We randomly choose 443, 50, and 50 vol-
umes for training, test, and validation respectively.

Overall evaluation. In this section, we compare the proposed high-resolution
compact network architecture (illustrated in Fig. 3; denoted as HC-default) with
three variants: (1) the HC-default conﬁguration without the residual connec-
tions, trained with cross-entropy loss function (NoRes-entropy); (2) the HC-
default conﬁguration without residual connections, trained with Dice loss func-
tion (NoRes-dice); and (3) the HC-default conﬁguration trained with an addi-
tional dropout layer, and makes predictions with a majority voting of 10 Monte
Carlo samples (HC-dropout). For the dropout variant, our dropout layer em-
ployed before the last convolutional layer consists of 80 kernels.

1 The source code is available at https://github.com/gift-surg/HighRes3DNet

8

Li et al.

(1)

(2)

(3)

(4)

(5)

Fig. 4. Visualisations of segmentation results. (1) slices from a test image volume,
segmentation maps and false prediction maps generated by HC-dropout (2, 3), and 3D
U-net-dice (4, 5).

Table 1. Comparison of diﬀerent 3D convolutional network architectures.

Architecture

Multi-layer fusion Num. param. Loss type DCS (%) STD (%)

HC-default
HC-dropout
NoRes-entropy
NoRes-dice
Deepmedic[11]-dice Two pathways
3D U-net[3]-dice
V-net[16]

Residual
Residual
N/A
N/A

Feature forwarding
Feature forwarding

0.81M
0.82M
0.81M
0.81M
0.68M
19.08M
62.63M

Dice loss
Dice loss
Cross entr.
Dice loss
Dice loss
Dice loss
Dice loss

82.05
84.34
39.36
75.47
78.74
80.18
74.58

2.96
1.89
1.13
2.97
1.72
6.18
1.86

Additionally, three state-of-the-art volumetric segmentation networks are
evaluated. These include 3D U-net [3], V-net [16], and Deepmedic [11]. The last
layer of each network architecture is replaced with a 160-way softmax classiﬁer.
We observe that training these networks with the cross entropy loss function
(Eq. 2) leads to poor segmentation results. Since the cross-entropy loss function
treats all training voxels equally, the network may have diﬃculties in learning
representations related to the minority classes. Training with the Dice loss func-
tion alleviates this issue by implicitly re-weighting the voxels. Thus we train all
networks using the Dice loss function for a fair comparison.

We use the mean Dice Coeﬃcient Similarity (DCS) as the performance mea-
sure. Table 1 and Fig. 5(right) compare the performance on the test set. In
terms of our network variants, the results show that the use of Dice loss function
largely improves the segmentation performance. This suggests that the Dice loss

3D Convolutional Networks for Brain Parcellation

9

Fig. 5. Left: label distribution of the dataset; right: comparison of diﬀerent network
architectures.

Fig. 6. Segmentation performance against a set of key structures.

function can handle the severely unbalanced segmentation problem well. The
results also suggest that introducing the residual connections improved the seg-
mentation performance measured in mean DCS. This indicates that the residual
connections are important elements of the proposed network. By adopting the
dropout method, the DCS can be further improved by 2% in DCS.

With a relatively small number of parameters, our HC-default and HC-
dropout outperform the competing methods in terms of mean DCS. This sug-
gests that our network is more eﬀective for the brain parcellation problem. Note
that V-net has a similar architecture to 3D U-net and has more parameters, but
does not employ the batch normalisation technique. The lower DCS produced by
V-net suggests that batch normalisation is important for training the networks
for brain parcellation.

10

Li et al.

In Fig. 6, we show that the dropout variant achieves better segmentation
results for all the key structures. Fig. 4 presents an example of the segmentation
results of the proposed network and 3D U-net-Dice.

Receptive ﬁeld and border eﬀects. We further compare the segmentation
performance of a trained network by discarding the borders in each dimension of
the segmentation map. That is, given a d × d × d-voxel input, at border size 1 we
only preserve the (d−2)3-voxel output volume centred within the predicted map.
Fig. 7 plots the DCS and standard er-
rors of segmentation according to the
size of the segmentation borders in
each dimension. The results show that
the distorted border is around 17 vox-
els in each dimension. The border ef-
fects do not severely decrease the seg-
mentation performance. In practice,
we pad the volume images with 16 ze-
ros in each dimension, and remove the
same amount of borders in the seg-
mentation output.

Fig. 7. Empirical analysis of the segmen-
tation borders. Voxels near to the volume
borders are classiﬁed less accurately.

The eﬀect of number of samples
in uncertainty estimations. This section investigates the number of Monte
Carlo samples and the segmentation performance of the proposed network.
Fig. 8(a) suggests that using 10 samples is enough to achieve good segmen-
tation. Further increasing the number of samples has relatively small eﬀects on
the DCS. Fig. 8(b) plots the voxel-wise segmentation accuracy computed using
only the voxels with an uncertainty less than a threshold. The voxel-wise ac-
curacy is high when the threshold is small. This indicates that the uncertainty

(a)

(b)

Fig. 8. Evaluation of dropout sampling. (a) The segmentation performance against the
number of Monte Carlo samples. (b) voxel-level segmentation accuracy by thresholding
the uncertainties. The shaded area represents the standard errors.

3D Convolutional Networks for Brain Parcellation

11

Fig. 9. Voxel-level segmentation uncertainty estimations. Top row: uncertainty map
generated with 100 Monte Carlo samples using dropout. Bottom row: uncertainty map
thresholded at 0.1.

estimation reﬂects the conﬁdence of the network. Fig. 9 shows an uncertainty
map generated by the proposed network. The uncertainties near the boundaries
of diﬀerent structures are relatively higher than the other regions.

Currently, our method takes about 60 seconds to predict a typical volume
with 192 × 256 × 256 voxels. To achieve better segmentation results and measure
uncertainty, 10 Monte Carlo samples of our dropout model are required. The
entire process takes slightly more than 10 minutes in total. However, during
the Monte Carlo sampling at test time, only the dropout layer and the ﬁnal
prediction layer are randomised. To further reduce the computational time, the
future software could reuse the features extracted from the layers before dropout,
resulting in only a marginal increase in runtime when compared to a single
prediction.

5 Conclusion

In this paper, we propose a high-resolution, 3D convolutional network archi-
tecture that incorporates large volumetric context using dilated convolutions
and residual connections. Our network is conceptually simpler and more com-
pact than the state-of-the-art volumetric segmentation networks. We validate the
proposed network using the challenging task of brain parcellation in MR images.
We show that the segmentation performance of our network compares favourably
with the competing methods. Additionally, we demonstrate that Monte Carlo
sampling of dropout technique can be used to generate voxel-level uncertainty es-

12

Li et al.

timation for our brain parcellation network. Moreover, we consider the brain par-
cellation task as a pretext task for volumetric image segmentation. Our trained
network potentially provides a good starting point for transfer learning of other
segmentation tasks.

In the future, we will extensively test the generalisation ability of the network
to brain MR scans obtained with various scanning protocols from diﬀerent data
centres. Furthermore, we note that the uncertainty estimations are not proba-
bilities. We will investigate the calibration of the uncertainty scores to provide
reliable probability estimations.

Acknowledgements. This work was supported through an Innovative Engi-
neering for Health award by the Wellcome Trust [WT101957, 203145Z/16/Z],
Engineering and Physical Sciences Research Council (EPSRC) [NS/A000027/1,
NS/A000050/1], the National Institute for Health Research University College
London Hospitals Biomedical Research Centre (NIHR BRC UCLH/UCL High
Impact Initiative), UCL EPSRC CDT Scholarship Award [EP/L016478/1], a
UCL Overseas Research Scholarship, a UCL Graduate Research Scholarship,
and the Health Innovation Challenge Fund [HICF-T4-275, WT 97914], a paral-
lel funding partnership between the Department of Health and Wellcome Trust.
The authors would also like to acknowledge that the work presented here made
use of Emerald, a GPU-accelerated High Performance Computer, made available
by the Science & Engineering South Consortium operated in partnership with
the STFC Rutherford-Appleton Laboratory.

References

1. Cardoso, M.J., Modat, M., Wolz, R., Melbourne, A., Cash, D., Rueckert, D.,
Ourselin, S.: Geodesic information ﬂows: Spatially-variant graphs and their appli-
cation to segmentation and fusion. IEEE Transactions on Medical Imaging 34(9)
(2015)

2. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: DeepLab:
Semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected CRFs. arXiv:1606.00915 (2016)

3. C¸ i¸cek, ¨O., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O.: 3D U-
Net: Learning dense volumetric segmentation from sparse annotation. In: MICCAI
(2016)

4. Dou, Q., Chen, H., Yu, L., Zhao, L., Qin, J., Wang, D., Mok, V.C., Shi, L., Heng,
P.A.: Automatic detection of cerebral microbleeds from MR images via 3D convo-
lutional neural networks. IEEE Transactions on Medical Imaging 35(5) (2016)
5. Gal, Y., Ghahramani, Z.: Dropout as a Bayesian approximation: Representing

model uncertainty in deep learning. In: ICML (2016)

6. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectiﬁers: Surpassing human-

level performance on imagenet classiﬁcation. In: ICCV (2015)

7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

8. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.

In: CVPR (2016)

In: ECCV (2016)

3D Convolutional Networks for Brain Parcellation

13

9. Huh, M., Agrawal, P., Efros, A.A.: What makes ImageNet good for transfer learn-

ing? arXiv:1608.08614 (2016)

10. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML (2015)

11. Kamnitsas, K., Ledig, C., Newcombe, V.F., Simpson, J.P., Kane, A.D., Menon,
D.K., Rueckert, D., Glocker, B.: Eﬃcient multi-scale 3D CNN with fully connected
CRF for accurate brain lesion segmentation. Medical Image Analysis 36 (2017)
12. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. arXiv:1412.6980

(2014)

13. Kleesiek, J., Urban, G., Hubert, A., Schwarz, D., Maier-Hein, K., Bendszus, M.,
Biller, A.: Deep MRI brain extraction: A 3D convolutional neural network for skull
stripping. NeuroImage 129 (2016)

14. Luo, W., Li, Y., Urtasun, R., Zemel, R.: Understanding the eﬀective receptive ﬁeld

in deep convolutional neural networks. In: NIPS (2016)

15. Merkow, J., Kriegman, D., Marsden, A., Tu, Z.: Dense volume-to-volume vascular

boundary detection. In: MICCAI (2016)

16. Milletari, F., Navab, N., Ahmadi, S.A.: V-Net: Fully convolutional neural networks
for volumetric medical image segmentation. In: International Conference on 3D
Vision (2016)

17. Ny´ul, L.G., Udupa, J.K., Zhang, X.: New variants of a method of MRI scale stan-

dardization. IEEE Transactions on Medical Imaging 19(2) (2000)

18. Sankaran, S., Grady, L.J., Taylor, C.A.: Real-time sensitivity analysis of blood ﬂow

simulations to lumen segmentation uncertainty. In: MICCAI (2014)

19. Shi, W., Zhuang, X., Wolz, R., Simon, D., Tung, K., Wang, H., Ourselin, S., Ed-
wards, P., Razavi, R., Rueckert, D.: A multi-image graph cut approach for cardiac
image segmentation and uncertainty estimation. In: International Workshop on
Statistical Atlases and Computational Models of the Heart (2011)

20. Veit, A., Wilber, M., Belongie, S.: Residual networks are exponential ensembles of

relatively shallow networks. In: NIPS (2016)

On the Compactness, Eﬃciency, and
Representation of 3D Convolutional Networks:
Brain Parcellation as a Pretext Task

Wenqi Li, Guotai Wang, Lucas Fidon, Sebastien Ourselin, M. Jorge Cardoso,
and Tom Vercauteren

Translational Imaging Group, Centre for Medical Image Computing (CMIC),
University College London, London, UK
Wellcome/EPSRC Centre for Surgical and Interventional Science, University College
London, London, UK

Abstract. Deep convolutional neural networks are powerful tools for
learning visual representations from images. However, designing eﬃcient
deep architectures to analyse volumetric medical images remains chal-
lenging. This work investigates eﬃcient and ﬂexible elements of modern
convolutional networks such as dilated convolution and residual connec-
tion. With these essential building blocks, we propose a high-resolution,
compact convolutional network for volumetric image segmentation. To
illustrate its eﬃciency of learning 3D representation from large-scale im-
age data, the proposed network is validated with the challenging task
of parcellating 155 neuroanatomical structures from brain MR images.
Our experiments show that the proposed network architecture compares
favourably with state-of-the-art volumetric segmentation networks while
being an order of magnitude more compact. We consider the brain par-
cellation task as a pretext task for volumetric image segmentation; our
trained network potentially provides a good starting point for transfer
learning. Additionally, we show the feasibility of voxel-level uncertainty
estimation using a sampling approximation through dropout.

1

Introduction

Convolutional neural networks (CNNs) have been shown to be powerful tools for
learning visual representations from images. They often consist of multiple layers
of non-linear functions with a large number of trainable parameters. Hierarchical
features can be obtained by training the CNNs discriminatively.

In the medical image computing domain, recent years have seen a growing
number of applications using CNNs. Although there have been recent advances
in tailoring CNNs to analyse volumetric images, most of the work to date studies
image representations in 2D. While volumetric representations are more informa-
tive, the number of voxels scales cubically with the size of the region of interest.
This raises challenges of learning more complex visual patterns as well as higher
computational burden compared to the 2D cases. While developing compact and

7
1
0
2
 
l
u
J
 
6
 
 
]

V
C
.
s
c
[
 
 
1
v
2
9
9
1
0
.
7
0
7
1
:
v
i
X
r
a

2

Li et al.

eﬀective 3D network architectures is of signiﬁcant interest, designing 3D CNNs
remains a challenging problem.

The goal of this paper is to design a high-resolution and compact network
architecture for the segmentation of ﬁne structures in volumetric images. For
this purpose, we study the simple and ﬂexible elements of modern convolutional
networks, such as dilated convolution and residual connection. Most of the ex-
isting network architectures follow a fully convolutional downsample-upsample
pathway [11,4,15,3,16,13]. Low-level features with high spatial resolutions are
ﬁrst downsampled for higher-level feature abstraction; then the feature maps
are upsampled to achieve high-resolution segmentation. In contrast to these, we
propose a novel 3D architecture that incorporates high spatial resolution feature
maps throughout the layers, and can be trained with a wide range of receptive
ﬁelds. We validate our network with the challenging task of automated brain
parcellation into 155 structures from T1-weighted MR images. We show that
the proposed network, with twenty times fewer parameters, achieves competi-
tive segmentation performance compared with state-of-the-art architectures.

A well-designed network could be trained with a large-scale dataset and en-
ables transfer learning to other image recognition tasks [9]. In the ﬁeld of com-
puter vision, the well-known AlexNet and VGG net were trained on the Ima-
geNet dataset. They provide general-purpose image representations that can be
adapted for a wide range of computer vision problems. Given the large amount
of data and the complex visual patterns of the brain parcellation problem, we
consider it as a pretext task. Our trained network is the ﬁrst step towards a
general-purpose volumetric image representation. It potentially provides an ini-
tial model for transfer learning of other volumetric image segmentation tasks.

The uncertainty of the segmentation is also important for indicating the
conﬁdence and reliability of one algorithm [5,18,19]. The high uncertainty of la-
belling can be a sign of an unreliable classiﬁcation. In this work, we demonstrate
the feasibility of voxel-level uncertainty estimation using Monte Carlo samples
of the proposed network with dropout at test time. Compared to the existing
volumetric segmentation networks, our compact network has fewer parameter in-
teractions, and thus potentially achieves better uncertainty estimates with fewer
samples.

2 On the elements of 3D convolutional networks

Convolutions and dilated convolutions. To maintain a relatively low num-
ber of parameters, we choose to use small 3D convolutional kernels with only 33
parameters for all convolutions. This is about the smallest kernel that can rep-
resent 3D features in all directions with respect to the central voxel. Although a
convolutional kernel with 5 × 5 × 5 voxels gives the same receptive ﬁeld as stack-
ing two layers of 3 × 3 × 3-voxel convolution, the latter has approximately 57%
fewer parameters. Using smaller kernels implicitly imposes more regularisation
on the network while achieving the same receptive ﬁeld.

3D Convolutional Networks for Brain Parcellation

3

To further enlarge the receptive ﬁeld to capture large image contexts, most
of the existing volumetric segmentation networks downsample the intermediate
feature maps. This signiﬁcantly reduces the spatial resolution. For example, 3D
U-net [3] heavily employs 2×2×2-voxel max pooling with strides of two voxels in
each dimension. Each max pooling reduces the feature responses of the previous
layer to only 1/8 of its spatial resolution. Upsampling layers, such as decon-
volutions, are often used subsequently to partially recover the high resolution
of the input. However, adding deconvolution layers also introduces additional
computational costs.

Recently, Chen et al. [2] used dilated convolutions with upsampled kernels
for semantic image segmentation. The advantages of dilated convolutions are
that the features can be computed with a high spatial resolution, and the size of
the receptive ﬁeld can be enlarged arbitrarily. Dilated convolutions can be used
to produce accurate dense predictions and detailed segmentation maps along
object boundaries.

In contrast to the downsample-upsample pathway, we propose to adopt di-
lated convolutions for volumetric image segmentation. More speciﬁcally, the con-
volutional kernels are upsampled with a dilation factor r. For M -channels of
input feature maps I, the output feature channel O generated with dilated con-
volutions are:

Ox,y,z =

wi,j,k,mI(x+ir),(y+jr),(z+kr),m ;

(1)

M −1
(cid:88)

2
(cid:88)

2
(cid:88)

2
(cid:88)

m=0

i=0

j=0

k=0

where the index tuple (x, y, z) runs through every spatial location in the volumes;
the kernels w consist of 33 × M trainable parameters. The dilated convolution in
Eq. (1) has the same number of trainable parameters as the standard 3 × 3 × 3
convolution. It preserves the spatial resolution and provides a (2r + 1)3-voxel
receptive ﬁeld. Setting r to 1 reduces the dilated convolution to the standard
3 × 3 × 3 convolution. In practice, we implement 3D dilated convolutions with
a split-and-merge strategy [2] to beneﬁt from the existing GPU convolution
routines.

Residual connections. Residual connections
were ﬁrst introduced and later reﬁned by He et
al. [7,8] for the eﬀective training of deep networks.
The key idea of residual connection is to create
identity mapping connections to bypass the pa-
rameterised layers in a network. The input of a
residual block is directly merged to the output
by addition. The residual connections have been
shown to make information propagation smooth
and improve the training speed [7].

Fig. 1. A block with residual
connections.

More speciﬁcally, let the input to the p-th layer of a residual block as xp, the
output of the block xp+1 has the form: xp+1 = xp + F (xp, wp); where F (xp, wp)

4

Li et al.

denotes the path with non-linear functions in the block (shown in Fig. 1). If we
stack the residual blocks, the last layer output xl can be expressed as: xl = xp +
(cid:80)l−1
i=p F (xi, wi). The residual connections enables direct information propagation
from any residual block to another in both forward pass and back-propagation.

Eﬀective receptive ﬁeld. One interpretation of the residual network is that
they behave like ensembles of relatively shallow networks. The unravelled view of
the residual connections proposed by Veit et al. [20] suggests that the networks
with n residual blocks have a collection of 2n unique paths.

Without residual connections, the receptive ﬁeld of a network is generally
considered ﬁxed. However, when training with n residual blocks, the networks
utilise 2n diﬀerent paths and therefore features can be learned with a large range
of diﬀerent receptive ﬁelds. For example, the proposed network with 9 residual
blocks (see Section 3) has a maximum receptive ﬁeld of 87 × 87 × 87 voxels.
Following the unravel view of the residual network, it consists of 29 unique paths.
Fig. 2 shows the distribution of the receptive ﬁeld of these paths. The receptive
ﬁelds range from 3 × 3 × 3 to 87 × 87 × 87, following a binomial distribution.

This diﬀers from the existing 3D networks.
For example, Deepmedic [11] model operates
at two paths, with a ﬁxed receptive ﬁeld 17 ×
17 × 17 and 42 × 42 × 42 respectively. 3D U-
net [3] has a relatively large receptive ﬁeld of
about 88 × 88 × 88 voxels. However, there are
only eight unique paths and receptive ﬁelds.

Fig. 2. Histogram of the receptive
ﬁelds.

Intuitively, given that the receptive ﬁeld
of a deep convolutional network is relatively
large, the segmentation maps will suﬀer from
distortions due to the border eﬀects of convo-
lution. That is, the segmentation results near the border of the output volume
are less accurate due to the lack of input supporting window. We conduct ex-
periments and demonstrate that the proposed networks generate only a small
distortion near the borders (See Section 4). This suggests training the network
with residual connections reduces the eﬀective receptive ﬁeld. The width of the
distorted border is much smaller than the maximum receptive ﬁeld. This phe-
nomenon was also recently analysed by Luo et al. [14]. In practice, at test time
we pad each input volume with a border of zeros and discard the same amount
of border in the segmentation output.

Loss function. The last layer of the network is a softmax function that gives
scores over all labels for each voxel. Typically, the end-to-end training proce-
dure minimises the cross entropy loss function using an N -voxel image volume
{xn}N
n=1 where

n=1 and the training data of C-class segmentation map {yn}N

3D Convolutional Networks for Brain Parcellation

5

yn ∈ {1, . . . , C} is:

L({xn}, {yn}) = −

δ(yn = c) log Fc(xn),

(2)

1
N

N
(cid:88)

C
(cid:88)

n=1

c=1

where δ corresponds to the Dirac delta function, Fc(xn) is the softmax classiﬁca-
tion score of xn over the c-th class. However, when the training data are severely
unbalanced (which is typical in medical image segmentation problems), this for-
mulation leads to a strongly biased estimation towards the majority class. In-
stead of directly re-weighting each voxel by class frequencies, Milletari et al. [16]
propose a solution by maximising the mean Dice coeﬃcient directly, i.e.,

D({xn}, {yn}) =

1
C

C
(cid:88)

c=1

2 (cid:80)N

n=1 δ(yn = c)Fc(xn)

(cid:80)N

n=1[δ(yn = c)]2 + (cid:80)N

n=1[Fc(xn)]2

.

(3)

We employ this formulation to handle the issue of training data imbalance.

Uncertainty estimation using dropout. Gal and Ghahramani demonstrated
that the deep network trained with dropout can be cast as a Bayesian approxi-
mation of the Gaussian process [5]. Given a set of training data and their labels
{X, Y}, training a network F (· , W) with dropout has the eﬀect of approxi-
mating the posterior distribution p(W|{X, Y}) by minimising the Kullback-
Leibler divergence term, i.e. KL(q(W)||p(W|{X, Y})); where q(W) is an ap-
proximating distribution over the weight matrices W with their elements ran-
domly set to zero according to Bernoulli random variables. After training the
network, the predictive distribution of test data ˆx can be expressed as q(ˆy|ˆx) =
(cid:82) F (ˆx, W)q(W)dW. The prediction can be approximated using Monte Carlo
samples of the trained network: ˆy = 1
m=1 is a
M
set of M samples from q(W). The uncertainty of the prediction can be estimated
using the sample variance of the M samples.

m=1 F (ˆx, Wm), where {Wm}M

(cid:80)M

With this theoretical insight, we are able to estimate the uncertainty of the
segmentation map at the voxel level. We extend the segmentation network with
a 1 × 1 × 1 convolutional layer before the last convolutional layer. The extended
network is trained with a dropout ratio of 0.5 applied to the newly inserted
layer. At test time, we sample the network N times using dropout. The ﬁnal
segmentation is obtained by majority voting. The percentage of samples which
disagrees with the voting results is calculated at each voxel as the uncertainty
estimates.

3 The network architecture and its implementation

3.1 The proposed architecture

Our network consists of 20 layers of convolutions. In the ﬁrst seven convolu-
tional layers, we adopt 3 × 3 × 3-voxel convolutions. These layers are designed

6

Li et al.

Fig. 3. The proposed network architecture for volumetric image segmentation. The
network mainly utilises dilated convolutions and residual connections to make an end-
to-end mapping from image volume to a voxel-level dense segmentation. To incorporate
features at multiple scales, the dilation factor of the dilated convolutions is gradually
increased when the layer goes deeper. The residual blocks with identity mapping enable
the direct fusion of features from diﬀerent scales. The spatial resolution of the input
volume is maintained throughout the network.

to capture low-level image features such as edges and corners. In the subsequent
convolutional layers, the kernels are dilated by a factor of two or four. These
deeper layers with dilated kernels encode mid- and high-level image features.

Residual connections are employed to group every two convolutional lay-
ers. Within each residual block, each convolutional layer is associated with
an element-wise rectiﬁed linear unit (ReLU) layer and a batch normalisation
layer [10]. The ReLU, batch normalisation, and convolutional layers are arranged
in the pre-activation order [8].

The network can be trained end-to-end. In the training stage, the inputs
to our network are 96 × 96 × 96-voxel images. The ﬁnal softmax layer gives
classiﬁcation scores over the class labels for each of the 96 × 96 × 96 voxels. The
architecture is illustrated in Fig. 3.

3.2 Implementation details

In the training stage, the pre-processing step involved input data standardisa-
tion and augmentation at both image- and subvolume-level. At image-level, we
adopted the histogram-based scale standardisation method [17] to normalised
the intensity histograms. As a data augmentation at image-level, randomisation
was introduced in the normalisation process by randomly choosing a threshold
of foreground between the volume minimum and mean intensity (at test time,
the mean intensity of the test volume was used as the threshold). Each image
was further normalised to have zero mean and unit standard deviation. Augmen-
tations on the randomly sampled 96 × 96 × 96 subvolumes were employed on the
ﬂy. These included rotation with a random angle in the range of [−10◦, 10◦] for

3D Convolutional Networks for Brain Parcellation

7

each of the three orthogonal planes and spatial rescaling with a random scaling
factor in the range of [0.9, 1.1].

All the parameters in the convolutional layers were initialised according to
He et al. [6]. The scaling and shifting parameters in the batch normalisation
layers were initialised to 1 and 0 respectively. The networks were trained with
two Nvidia K80 GPUs. At each training iteration, each GPU processed one in-
put volume; the average gradients computed over these two training volumes
were used as the gradients update. To make a fair comparison, we employed the
Adam optimisation method [12] for all the methods with ﬁxed hyper-parameters.
The learning rate lr was set to 0.01, the step size hyper-parameter β1 was 0.9
and β2 was 0.999 in all cases, except V-Net for which we chose the largest lr
that the training algorithm converges (lr = 0.0001). The models were trained
until we observed a plateau in performance on the validation set. We do not
employ additional spatial smoothing function (such as conditional random ﬁeld)
as a post-processing step. Instead of aiming for better segmentation results by
adding post-processing steps, we focused on the dense segmentation maps gen-
erated by the networks. As we consider brain parcellation as a pretext task,
networks without explicit spatial smoothing are potentially more reusable. We
implemented all the methods (including a re-implementation of Deepmedic [11],
V-net [16], and 3D U-net [3] architecture) with Tensorﬂow1.

4 Experiments and results

Data. To demonstrate the feasibility of learning complex 3D image representa-
tions from large-scale data, the proposed network is learning a highly granular
segmentation of 543 T1-weighted MR images of healthy controls from the ADNI
dataset. The average number of voxels of each volume is about 182 × 244 × 246.
The average voxel size is approximately 1.18mm × 1.05mm × 1.05mm. All vol-
umes are bias-corrected and reoriented to a standard Right-Anterior-Superior
orientation. The bronze standard parcellation of 155 brain structures and 5 non-
brain outer tissues are obtained using the GIF framework [1]. Fig. 5(left) shows
the label distribution of the dataset. We randomly choose 443, 50, and 50 vol-
umes for training, test, and validation respectively.

Overall evaluation. In this section, we compare the proposed high-resolution
compact network architecture (illustrated in Fig. 3; denoted as HC-default) with
three variants: (1) the HC-default conﬁguration without the residual connec-
tions, trained with cross-entropy loss function (NoRes-entropy); (2) the HC-
default conﬁguration without residual connections, trained with Dice loss func-
tion (NoRes-dice); and (3) the HC-default conﬁguration trained with an addi-
tional dropout layer, and makes predictions with a majority voting of 10 Monte
Carlo samples (HC-dropout). For the dropout variant, our dropout layer em-
ployed before the last convolutional layer consists of 80 kernels.

1 The source code is available at https://github.com/gift-surg/HighRes3DNet

8

Li et al.

(1)

(2)

(3)

(4)

(5)

Fig. 4. Visualisations of segmentation results. (1) slices from a test image volume,
segmentation maps and false prediction maps generated by HC-dropout (2, 3), and 3D
U-net-dice (4, 5).

Table 1. Comparison of diﬀerent 3D convolutional network architectures.

Architecture

Multi-layer fusion Num. param. Loss type DCS (%) STD (%)

HC-default
HC-dropout
NoRes-entropy
NoRes-dice
Deepmedic[11]-dice Two pathways
3D U-net[3]-dice
V-net[16]

Residual
Residual
N/A
N/A

Feature forwarding
Feature forwarding

0.81M
0.82M
0.81M
0.81M
0.68M
19.08M
62.63M

Dice loss
Dice loss
Cross entr.
Dice loss
Dice loss
Dice loss
Dice loss

82.05
84.34
39.36
75.47
78.74
80.18
74.58

2.96
1.89
1.13
2.97
1.72
6.18
1.86

Additionally, three state-of-the-art volumetric segmentation networks are
evaluated. These include 3D U-net [3], V-net [16], and Deepmedic [11]. The last
layer of each network architecture is replaced with a 160-way softmax classiﬁer.
We observe that training these networks with the cross entropy loss function
(Eq. 2) leads to poor segmentation results. Since the cross-entropy loss function
treats all training voxels equally, the network may have diﬃculties in learning
representations related to the minority classes. Training with the Dice loss func-
tion alleviates this issue by implicitly re-weighting the voxels. Thus we train all
networks using the Dice loss function for a fair comparison.

We use the mean Dice Coeﬃcient Similarity (DCS) as the performance mea-
sure. Table 1 and Fig. 5(right) compare the performance on the test set. In
terms of our network variants, the results show that the use of Dice loss function
largely improves the segmentation performance. This suggests that the Dice loss

3D Convolutional Networks for Brain Parcellation

9

Fig. 5. Left: label distribution of the dataset; right: comparison of diﬀerent network
architectures.

Fig. 6. Segmentation performance against a set of key structures.

function can handle the severely unbalanced segmentation problem well. The
results also suggest that introducing the residual connections improved the seg-
mentation performance measured in mean DCS. This indicates that the residual
connections are important elements of the proposed network. By adopting the
dropout method, the DCS can be further improved by 2% in DCS.

With a relatively small number of parameters, our HC-default and HC-
dropout outperform the competing methods in terms of mean DCS. This sug-
gests that our network is more eﬀective for the brain parcellation problem. Note
that V-net has a similar architecture to 3D U-net and has more parameters, but
does not employ the batch normalisation technique. The lower DCS produced by
V-net suggests that batch normalisation is important for training the networks
for brain parcellation.

10

Li et al.

In Fig. 6, we show that the dropout variant achieves better segmentation
results for all the key structures. Fig. 4 presents an example of the segmentation
results of the proposed network and 3D U-net-Dice.

Receptive ﬁeld and border eﬀects. We further compare the segmentation
performance of a trained network by discarding the borders in each dimension of
the segmentation map. That is, given a d × d × d-voxel input, at border size 1 we
only preserve the (d−2)3-voxel output volume centred within the predicted map.
Fig. 7 plots the DCS and standard er-
rors of segmentation according to the
size of the segmentation borders in
each dimension. The results show that
the distorted border is around 17 vox-
els in each dimension. The border ef-
fects do not severely decrease the seg-
mentation performance. In practice,
we pad the volume images with 16 ze-
ros in each dimension, and remove the
same amount of borders in the seg-
mentation output.

Fig. 7. Empirical analysis of the segmen-
tation borders. Voxels near to the volume
borders are classiﬁed less accurately.

The eﬀect of number of samples
in uncertainty estimations. This section investigates the number of Monte
Carlo samples and the segmentation performance of the proposed network.
Fig. 8(a) suggests that using 10 samples is enough to achieve good segmen-
tation. Further increasing the number of samples has relatively small eﬀects on
the DCS. Fig. 8(b) plots the voxel-wise segmentation accuracy computed using
only the voxels with an uncertainty less than a threshold. The voxel-wise ac-
curacy is high when the threshold is small. This indicates that the uncertainty

(a)

(b)

Fig. 8. Evaluation of dropout sampling. (a) The segmentation performance against the
number of Monte Carlo samples. (b) voxel-level segmentation accuracy by thresholding
the uncertainties. The shaded area represents the standard errors.

3D Convolutional Networks for Brain Parcellation

11

Fig. 9. Voxel-level segmentation uncertainty estimations. Top row: uncertainty map
generated with 100 Monte Carlo samples using dropout. Bottom row: uncertainty map
thresholded at 0.1.

estimation reﬂects the conﬁdence of the network. Fig. 9 shows an uncertainty
map generated by the proposed network. The uncertainties near the boundaries
of diﬀerent structures are relatively higher than the other regions.

Currently, our method takes about 60 seconds to predict a typical volume
with 192 × 256 × 256 voxels. To achieve better segmentation results and measure
uncertainty, 10 Monte Carlo samples of our dropout model are required. The
entire process takes slightly more than 10 minutes in total. However, during
the Monte Carlo sampling at test time, only the dropout layer and the ﬁnal
prediction layer are randomised. To further reduce the computational time, the
future software could reuse the features extracted from the layers before dropout,
resulting in only a marginal increase in runtime when compared to a single
prediction.

5 Conclusion

In this paper, we propose a high-resolution, 3D convolutional network archi-
tecture that incorporates large volumetric context using dilated convolutions
and residual connections. Our network is conceptually simpler and more com-
pact than the state-of-the-art volumetric segmentation networks. We validate the
proposed network using the challenging task of brain parcellation in MR images.
We show that the segmentation performance of our network compares favourably
with the competing methods. Additionally, we demonstrate that Monte Carlo
sampling of dropout technique can be used to generate voxel-level uncertainty es-

12

Li et al.

timation for our brain parcellation network. Moreover, we consider the brain par-
cellation task as a pretext task for volumetric image segmentation. Our trained
network potentially provides a good starting point for transfer learning of other
segmentation tasks.

In the future, we will extensively test the generalisation ability of the network
to brain MR scans obtained with various scanning protocols from diﬀerent data
centres. Furthermore, we note that the uncertainty estimations are not proba-
bilities. We will investigate the calibration of the uncertainty scores to provide
reliable probability estimations.

Acknowledgements. This work was supported through an Innovative Engi-
neering for Health award by the Wellcome Trust [WT101957, 203145Z/16/Z],
Engineering and Physical Sciences Research Council (EPSRC) [NS/A000027/1,
NS/A000050/1], the National Institute for Health Research University College
London Hospitals Biomedical Research Centre (NIHR BRC UCLH/UCL High
Impact Initiative), UCL EPSRC CDT Scholarship Award [EP/L016478/1], a
UCL Overseas Research Scholarship, a UCL Graduate Research Scholarship,
and the Health Innovation Challenge Fund [HICF-T4-275, WT 97914], a paral-
lel funding partnership between the Department of Health and Wellcome Trust.
The authors would also like to acknowledge that the work presented here made
use of Emerald, a GPU-accelerated High Performance Computer, made available
by the Science & Engineering South Consortium operated in partnership with
the STFC Rutherford-Appleton Laboratory.

References

1. Cardoso, M.J., Modat, M., Wolz, R., Melbourne, A., Cash, D., Rueckert, D.,
Ourselin, S.: Geodesic information ﬂows: Spatially-variant graphs and their appli-
cation to segmentation and fusion. IEEE Transactions on Medical Imaging 34(9)
(2015)

2. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: DeepLab:
Semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected CRFs. arXiv:1606.00915 (2016)

3. C¸ i¸cek, ¨O., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O.: 3D U-
Net: Learning dense volumetric segmentation from sparse annotation. In: MICCAI
(2016)

4. Dou, Q., Chen, H., Yu, L., Zhao, L., Qin, J., Wang, D., Mok, V.C., Shi, L., Heng,
P.A.: Automatic detection of cerebral microbleeds from MR images via 3D convo-
lutional neural networks. IEEE Transactions on Medical Imaging 35(5) (2016)
5. Gal, Y., Ghahramani, Z.: Dropout as a Bayesian approximation: Representing

model uncertainty in deep learning. In: ICML (2016)

6. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectiﬁers: Surpassing human-

level performance on imagenet classiﬁcation. In: ICCV (2015)

7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

8. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.

In: CVPR (2016)

In: ECCV (2016)

3D Convolutional Networks for Brain Parcellation

13

9. Huh, M., Agrawal, P., Efros, A.A.: What makes ImageNet good for transfer learn-

ing? arXiv:1608.08614 (2016)

10. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML (2015)

11. Kamnitsas, K., Ledig, C., Newcombe, V.F., Simpson, J.P., Kane, A.D., Menon,
D.K., Rueckert, D., Glocker, B.: Eﬃcient multi-scale 3D CNN with fully connected
CRF for accurate brain lesion segmentation. Medical Image Analysis 36 (2017)
12. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. arXiv:1412.6980

(2014)

13. Kleesiek, J., Urban, G., Hubert, A., Schwarz, D., Maier-Hein, K., Bendszus, M.,
Biller, A.: Deep MRI brain extraction: A 3D convolutional neural network for skull
stripping. NeuroImage 129 (2016)

14. Luo, W., Li, Y., Urtasun, R., Zemel, R.: Understanding the eﬀective receptive ﬁeld

in deep convolutional neural networks. In: NIPS (2016)

15. Merkow, J., Kriegman, D., Marsden, A., Tu, Z.: Dense volume-to-volume vascular

boundary detection. In: MICCAI (2016)

16. Milletari, F., Navab, N., Ahmadi, S.A.: V-Net: Fully convolutional neural networks
for volumetric medical image segmentation. In: International Conference on 3D
Vision (2016)

17. Ny´ul, L.G., Udupa, J.K., Zhang, X.: New variants of a method of MRI scale stan-

dardization. IEEE Transactions on Medical Imaging 19(2) (2000)

18. Sankaran, S., Grady, L.J., Taylor, C.A.: Real-time sensitivity analysis of blood ﬂow

simulations to lumen segmentation uncertainty. In: MICCAI (2014)

19. Shi, W., Zhuang, X., Wolz, R., Simon, D., Tung, K., Wang, H., Ourselin, S., Ed-
wards, P., Razavi, R., Rueckert, D.: A multi-image graph cut approach for cardiac
image segmentation and uncertainty estimation. In: International Workshop on
Statistical Atlases and Computational Models of the Heart (2011)

20. Veit, A., Wilber, M., Belongie, S.: Residual networks are exponential ensembles of

relatively shallow networks. In: NIPS (2016)

