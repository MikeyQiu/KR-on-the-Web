 

 

 

Fusing Multiple Multiband Images 

Reza Arablouei and Frank de Hoog 

1 

Abstractâ€”We  consider  the  problem  of  fusing  an  arbitrary 
number  of  multiband,  i.e.,  panchromatic,  multispectral,  or 
hyperspectral,  images  belonging  to  the  same  scene.  We  use  the 
well-known forward observation and linear mixture models with 
Gaussian  perturbations  to  formulate  the  maximum-likelihood 
estimator of the endmember abundance matrix of the fused image. 
We calculate the Fisher information matrix for this estimator and 
examine the conditions for the uniqueness of the estimator. We use 
a vector total-variation penalty term together with nonnegativity 
and  sum-to-one  constraints  on  the  endmember  abundances  to 
regularize  the  derived  maximum-likelihood  estimation  problem. 
The regularization facilitates exploiting the prior knowledge that 
natural images are mostly composed of piecewise smooth regions 
with  limited  abrupt  changes,  i.e.,  edges,  as  well  as  coping  with 
potential  ill-posedness  of  the  fusion  problem.  We  solve  the 
resultant  convex  optimization  problem  using  the  alternating 
direction  method  of  multipliers.  We  utilize 
the  circular 
convolution  theorem  in  conjunction  with  the  fast  Fourier 
transform  to  alleviate  the  computational  complexity  of  the 
proposed  algorithm.  Experiments  with  multiband 
images 
constructed  from  real  hyperspectral  images  reveal  the  superior 
performance  of  the  proposed  algorithm  in  comparison  with  the 
state-of-the-art  algorithms,  which  need  to  be  used  in  tandem  to 
fuse more than two multiband images.ï€  
 

Index  Termsâ€”multiband  image  fusion;  alternating  direction 
method of multipliers; total variation; Cramer-Rao lower bound; 
maximum likelihood; linear mixture model. 
 

I. INTRODUCTION 

T 

HE  WEALTH  of  spectroscopic  information  provided  by 
images  containing  hundreds  or  even 
hyperspectral 
thousands  of  contiguous  bands  can  immensely  benefit  many 
remote sensing and computer vision applications, such as object 
recognition [2], change detection [3], material classification [4], 
in 
and  spectral  unmixing  [5],  commonly  encountered 
environmental  monitoring,  resource  location,  weather  or 
natural  disaster  forecasting,  etc.  Therefore,  finely-resolved 
hyperspectral  images  are in  great  demand  [6]-[10].  However, 
limitations in light intensity as well as efficiency of the current 
sensors  impose  an  inexorable  trade-off  between  the  spatial 
resolution,  spectral  sensitivity,  and  the  signal-to-noise  ratio 
(SNR)  of  existing  spectral  imagers  [11].  As  a  results,  typical 
spectral imaging systems can capture multiband images of high 
spatial  resolution  at  a  small  number  of  spectral  bands  or 
multiband  images  of  high  spectral  resolution  with  a  reduced 
spatial  resolution.  For  example,  imaging  devices  onboard 

ï€ A conference precursor of this work has appeared in the Proceedings of the 
IEEE International Conference on Acoustics, Speech and Signal Processing, 
Calgary, Canada, April 2018 [1]. 

R.  Arablouei  and  F.  de  Hoog  are  with  the  Commonwealth  Scientific  and 
Industrial Research Organisation, Pullenvale QLD 4069 and Acton ACT 2601, 
Australia (email: reza.arablouei@csiro.au, frank.dehoog@csiro.au). 
 

IKONOS 

satellites1  provide 

Pleiades  or 
single-band 
panchromatic  images  with  spatial  resolutions  of  less  than  a 
meter  and  multispectral  images  with  a  few  bands  and  spatial 
resolutions  of  a  few  meters  while  NASAâ€™s  Airborne 
Visible/Infrared  Imaging  Spectrometer  (AVIRIS)2  provides 
hyperspectral  images  with  more  than  two  hundred  bands  but 
with a spatial resolution of several ten meters. 

One  way  to  surmount  the  abovementioned  technological 
limitation of acquiring high-resolution hyperspectral images is 
to  capture  multiple multiband  images  of  the  same  scene  with 
practical  spatial  and  spectral  resolutions,  then  fuse  them 
together  in  a  synergistic  manner.  Fusing  multiband  images 
combines  their  complementary  information  obtained  through 
multiple  sensors  that  may  have  different  spatial  and  spectral 
resolutions and cover different spectral ranges. 

Initial multiband image fusion algorithms were developed to 
fuse a panchromatic image with a multispectral image and the 
associated  inverse  problem  was  dubbed  pansharpening  [12]-
[15]. Most of the pansharpening algorithms are based on either 
of  the  two  popular  pansharpening  strategies:  component 
substitution (CS) and multiresolution analysis (MRA). The CS-
based  algorithms  substitute  a  component  of  the  multispectral 
image  obtained  through  a  suitable  transformation  by  the 
panchromatic  image.  The  MRA-based  algorithm  inject  the 
spatial  detail  of  the  panchromatic  image  obtained  by  a 
multiscale  decomposition  into  the  multispectral  image.  There 
also exist hybrid methods that use both CS and MRA. Some of 
the algorithms originally proposed for pansharpening have been 
successfully  extended  to  be  used  for  fusing  a  panchromatic 
image  with  a  hyperspectral  image,  a  problem  that  is  called 
hyperspectral pansharpening [15]. 

Recently,  significant  research  effort  has  been  expended  to 
solve  the  problem  of  fusing  a  multispectral  image  with  a 
hyperspectral one. This inverse problem is essentially different 
from  the  pansharpening  and  hyperspectral  pansharpening 
problems  since  a  multispectral  image has multiple  bands  that 
are  intricately  related  to  the  bands  of  its  corresponding 
hyperspectral  image.  Unlike  a  panchromatic  image  that 
contains  only  one  band  of  reflectance  data  usually  covering 
parts  of  the  visible  and  near-infrared  spectral  ranges,  a 
multispectral  image  contains  multiple  bands  each  covering  a 
smaller  spectral  range,  some  being  in  the  shortwave-infrared 
the  pansharpening 
(SWIR)  region.  Therefore,  extending 
techniques so that they can be used to inject the spatial details 
of  a  multispectral  image  into  a  hyperspectral  image  is  not 
straightforward. Nonetheless, an effort towards this end has led 

1 http://www.satimagingcorp.com/satellite-sensors/ 
2 http://aviris.jpl.nasa.gov/data/free_data.html 

 

to  the  development  of  a  framework  called  hypersharpening, 
which  is  based  on  adapting  the  MRA-based  pansharpening 
methods to multispectral-hyperspectral image fusion. The main 
idea  is  to  synthesize  a  high-spatial-resolution  image  for  each 
band  of  the  hyperspectral  image  by  linearly  combining  the 
bands of the multispectral image using linear regression [16]. 

In some works on multispectral-hyperspectral image fusion, 
it is assumed that each pixel on the hyperspectral image, which 
has  a  lower  spatial  resolution  than  the  target  image,  is  the 
average of the pixels of the same area on the target image [17]-
[21]. Clearly, the size of this area depends on the downsampling 
ratio.  Based  on  this  pixel-aggregation  assumption,  one  can 
divide  the  problem  of  fusing  two  multiband  images  into 
subproblems  dealing  with  smaller  blocks  and  hence 
significantly  decrease  the  complexity  of  the  overall  process. 
However,  it  is  more  realistic  to  allow  the  area  on  the  target 
image  corresponding  to  a  pixel  of  the hyperspectral  image to 
span as many pixels as determined by the point-spread function 
of 
induces  spatial  blurring.  The 
downsampling  ratio  generally  depends  on  the  physical  and 
optical  characteristics  of  a  sensor  and  is  usually  fixed. 
Therefore, spatial blurring and downsampling can be expressed 
as two separate linear operations. The spectral degradation of a 
panchromatic or multispectral image with respect to the target 
image  can  also  be  modeled  as  a  linear  transformation. 
Articulating  the  spatial  and  spectral  degradations  in  terms  of 
linear  operations  forms  a  realistic  and  convenient  forward 
observation model to relate the observed multiband images to 
the target image. 

the  sensor,  which 

the  spectra  of 

Hyperspectral image data is generally known to have a low-
rank  structure  and  reside  in  a  subspace  that  usually  has  a 
dimension much smaller than the number of the spectral bands 
[5],  [22]-[24].  This  is  mainly  due  to  correlations  among  the 
spectral bands and the fact that the spectrum of each pixel can 
often be represented as a linear combination of a relatively few 
spectral signatures. These signatures, called endmembers, may 
be 
the  scene. 
linearly 
Consequently,  a  hyperspectral 
decomposed into its constituent endmembers and the fractional 
abundances  of  the  endmembers  for  each  pixel.  This  linear 
decomposition 
the 
corresponding  data  model  is  called  the  linear  mixture  model. 
Other  linear  decompositions  that  can  be  used  to  reduce  the 
dimensionality of a hyperspectral image in the spectral domain 
are  dictionary-learning-based  sparse 
representation  and 
principle-component analysis. 

the  material  present  at 

spectral  unmixing  and 

image  can  be 

is  called 

Many recent works on multiband image fusion, which mostly 
deal  with  fusing  a  multispectral  image  with  a  hyperspectral 
image of the same scene, employ the abovementioned forward 
observation model and a form of linear spectral decomposition. 
They mostly extract the endmembers or the spectral dictionary 
from  the  hyperspectral  image.  Some  of  the  works  use  the 
extracted  endmember  or  dictionary  matrix  to  reconstruct  the 
multispectral  image  via  sparse  regression  and  calculate  the 
endmember abundances or the representation coefficients [25]. 
Others  cast 
fusion  problem  as 
reconstructing a high-spatial-resolution hyperspectral datacube 
from  two  datacubes  degraded  according  to  the  mentioned 
forward observation model. When the number of spectral bands 
in  the  multispectral  image  is  smaller  than  the  number  of 

the  multiband 

image 

2 

endmembers  or  dictionary  atoms,  the  linear  inverse  problem 
associated with the multispectral-hyperspectral fusion problem 
is  ill-posed  and  needs  be  regularized  to  have  a  meaningful 
solution.  Any  prior knowledge  about  the  target  image  can be 
used  for  regularization.  Natural  images  are  known  to  mostly 
consist  of  smooth  segments  with  few  abrupt  changes 
corresponding  to  the  edges  and  object  boundaries  [26]-[28]. 
Therefore,  penalizing  the  total-variation  [29]-[31]  and  sparse 
(low-rank)  representation  in  the  spatial  domain  [32]-[35]  are 
two  popular  approaches  to  regularizing  the  multiband  image 
fusion  problems.  Some  algorithms,  developed  within  the 
framework  of  the  Bayesian  estimation,  incorporate  the  prior 
knowledge  or  conjecture  about  the  probability  distribution  of 
the target image into the fusion problem [36]-[38]. The work of 
[39]  obviates  the  need  for  regularization  by  dividing  the 
observed  multiband  images  into  small  spatial  patches  for 
spectral  unmixing  and  fusion  under  the  assumption  that  the 
target image is locally low-rank. 

When the endmembers or dictionary atoms are induced from 
an  observed  hyperspectral  image,  the  problem  of  fusing  the 
hyperspectral image with a multispectral image boils  down to 
estimating  the  endmember  abundances  or  representation 
coefficients of the target image, a problem that is often tractable 
(due  to  being  a  convex  optimization  problem)  and  has  a 
manageable  size  and  complexity.  The  estimate  of  the  target 
induced 
image 
endmembers/dictionary 
estimated 
abundances/coefficients.  It  is also  possible  to  jointly  estimate 
the  endmembers/dictionary  and  the  abundances/coefficients 
from  the  available  multiband  data.  This  joint  estimation 
problem  is  usually  formulated  as  a  non-convex  optimization 
problem  of  nonnegative  matrix  factorization,  which  can  be 
solved approximately using block coordinate-descent iterations 
[40]-[43]. 

then  obtained  by  mixing 
the 

and 

the 

is 

To the best of our knowledge, all existing multiband image 
fusion  algorithms  are  designed  to  fuse  a  pair  of  multiband 
images  with  complementary  spatial  and  spectral  resolutions. 
Therefore,  fusing  more  than  two  multiband  images  using  the 
existing  algorithms  can  only  be  realized  by  performing  a 
hierarchical procedure that combines multiple fusion processes 
possibly implemented via different algorithms as, for example, 
in [44] and [45]. In addition, there are potentially various ways 
to  arrange  the  pairings  and  often  it  is  not  possible  to  know 
beforehand  which  way  will  provide  the  best  overall  fusion 
result.  For  instance,  in  order  to  fuse  a  panchromatic,  a 
multispectral,  and  a  hyperspectral  image  of  a  scene,  one  can 
first fuse the panchromatic and multispectral images, then fuse 
the  resultant  pansharpened  multispectral  image  with  the 
hyperspectral  image.  Another  way  would  be  to  first  fuse  the 
multispectral  and  hyperspectral  images,  then  pansharpen  the 
resultant  hyperspectral  image  with  the  panchromatic  image. 
Apart from the said ambiguity of choice, such combined pair-
wise fusions can be slow and inaccurate since they may require 
several  runs  of  different  algorithms  and  may  suffer  from 
the 
propagation  and  accumulation  of  errors.  Therefore, 
increasing 
images  with 
complementary  characteristics  captured  by  modern  spectral 
imaging devices has brought about the demand for efficient and 
accurate fusion techniques that can handle multiple multiband 
images simultaneously. 

of  multiband 

availability 

 

In 

this  paper,  we  propose  an  algorithm 

that  can 
simultaneously fuse an arbitrary number of multiband images. 
We utilize the forward observation and linear mixture models 
to effectively model the data and reduce the dimensionality of 
the  problem.  Assuming  matrix  normal  distribution  for  the 
observation noise, we derive the likelihood function as well as 
the  Fisher  information  matrix  (FIM)  associated  with  the 
problem of recovering the endmember abundance matrix of the 
target image from the observations. We study the properties of 
the FIM and the conditions for existence of a unique maximum-
likelihood  estimate  and  the  associated  Cramer-Rao  lower 
bound.  We  regularize  the  problem  of  maximum-likelihood 
estimation  of  the  endmember  abundances  by  adding  a  vector 
total-variation  penalty 
function  and 
constraining  the  abundances  to  be nonnegative  and add  up to 
one for each pixel. The total-variation penalty serves two major 
purposes. First, it helps us cope with the likely ill-posedness of 
the maximum-likelihood estimation problem. Second, it allows 
us  to  take  into  account  the  spatial  characteristics  of  natural 
images  that  is  they  mostly  consist  of  piecewise  plane regions 
with  few  sharp  variations.  Regularization  with a  vector  total-
variation  penalty  can  effectively  advocate  this  desired  feature 
by  promoting  sparsity  in  the  image  gradient,  i.e.,  local 
differences between adjacent pixels, while encourages the local 
differences to be spatially aligned across different bands [28]. 
The  nonnegativity  and  sum-to-one  constraints  on 
the 
endmember  abundances  ensure  that  the  abundances  have 
practical  values.  They  also  implicitly  promote  sparsity  in  the 
estimated endmember abundances. 

the  cost 

term 

to 

We  solve  the  resultant  constrained  optimization  problem 
using the alternating direction method of multipliers (ADMM) 
[46]-[51].  Simulation  results  indicate  that  the  proposed 
algorithm outperforms several combinations of the state-of-the-
art algorithms, which need be cascaded to carry out fusion of 
multiple (more than two) multiband images. 

II. DATA MODEL 

A. Forward observation model 

Let us denote the target multiband image by ğ— âˆˆ â„ğ¿Ã—ğ‘ where 
ğ¿ is the number of spectral bands and ğ‘ is the number of pixels 
in the image. We wish to recover ğ— from ğ¾ observed multiband 
images  ğ˜ğ‘˜ âˆˆ â„ğ¿ğ‘˜Ã—ğ‘ğ‘˜,  ğ‘˜ = 1, â€¦ , ğ¾, 
that  are  spatially  or 
spectrally downgraded and degraded versions of ğ—. We assume 
that these multiband images are geometrically co-registered and 
are related to ğ— via the following forward observation model 

ğ˜ğ‘˜ = ğ‘ğ‘˜ğ—ğğ‘˜ğ’ğ‘˜ + ğğ‘˜ 

(1) 

 

where 

2  with  ğ·ğ‘˜  being  the  spatial 

ğ¿ğ‘˜ â‰¤ ğ¿  and  ğ‘ğ‘˜ = ğ‘/ğ·ğ‘˜
downsampling ratio of the ğ‘˜th image; 
ğ‘ğ‘˜ âˆˆ â„ğ¿ğ‘˜Ã—ğ‘  is  the  spectral  response  of  the  sensor 
producing ğ˜ğ‘˜; 
ğğ‘˜ âˆˆ â„ğ‘Ã—ğ‘ is a band-independent spatial blurring matrix 
that represents a two-dimensional convolution with a blur 
kernel  corresponding  to  the  point-spread  function  of  the 
sensor producing ğ˜ğ‘˜; 
ğ’ğ‘˜ âˆˆ â„ğ‘Ã—ğ‘ğ‘˜  is  a  sparse  matrix  with  ğ‘ğ‘˜  ones  and  zeros 
elsewhere  that  implements  a  two-dimensional  uniform 

 

 

3 

downsampling of ratio ğ·ğ‘˜ on both spatial dimensions and 
satisfies ğ’ğ‘˜
ğğ‘˜ âˆˆ â„ğ¿ğ‘˜Ã—ğ‘ğ‘˜  is  an  additive  perturbation  representing  the 
noise or error associated with the observation of ğ˜ğ‘˜. 

âŠ¤ğ’ğ‘˜ = ğˆğ‘; 

We  assume  that  the  perturbations  ğğ‘˜,  ğ‘˜ = 1, â€¦ , ğ¾,  are 
independent of each other and have matrix normal distributions 
expressed by 

 

ğğ‘˜ âˆ¼ â„³ğ’©ğ¿ğ‘˜Ã—ğ‘ğ‘˜

(ğŸğ¿ğ‘˜Ã—ğ‘ğ‘˜, ğšºğ‘˜, ğˆğ‘ğ‘˜

) 

(2) 

where ğŸğ¿ğ‘˜Ã—ğ‘ğ‘˜  is  the ğ¿ğ‘˜ Ã— ğ‘ğ‘˜ zero matrix, ğˆğ‘ğ‘˜  is the ğ‘ğ‘˜ Ã— ğ‘ğ‘˜ 
identity  matrix,  and  ğšºğ‘˜ âˆˆ â„ğ¿ğ‘˜Ã—ğ¿ğ‘˜  is  a  diagonal  matrix  that 
represents the correlation among rows of ğğ‘˜, which correspond 
to different spectral bands. Note that we consider the column-
covariance  matrices 
the 
perturbations are independent and identically-distributed in the 
spatial  domain.  However,  by  considering  diagonal  row-
covariance  matrices,  we  assume  that  the  perturbations  are 
independent in the spectral domain but may have non-identical 
variances at different bands. 

identity  assuming 

to  be 

that 

Note that ğ˜ğ‘˜, ğ‘˜ = 1, â€¦ , ğ¾, in (1) contain the corrected (pre-
processed) spectral values, not the raw measurements produced 
by  the  spectral  imagers.  The  pre-processing  usually  involves 
several  steps  including  radiometric  calibration,  geometric 
[52].  The 
correction,  and  atmospheric  compensation 
radiometric  calibration  is  generally  performed  to  obtain 
radiance  values  at  the  sensor.  The  reflected  sunlight  passing 
through  the  atmosphere  is  partially  absorbed  and  scattered 
through  a  complex  interaction  between  the  light  and  various 
parts  of  the  atmosphere.  The  atmospheric  compensation 
counters  these  effects  and  converts  the  radiance  values  into 
ground-leaving  radiance  or  surface  reflectance  values.  To 
obtain  accurate  reflectance  values,  one  additionally  has  to 
account  for  the  effects  of  the  viewing  geometry  and  sunâ€™s 
position as well as the surfaces structural and optical properties 
[6].  This  pre-processing  is  particularly  important  when  the 
multiband  images  to  be  fused  are  acquired  via  different 
instruments,  from  different  viewpoints,  or  at  different  times. 
After  the  pre-processing,  the  images  should  also  be  co-
registered. 

B. Linear mixture model 

Under some mild assumptions, multiband images of natural 
scenes can be suitably described by a linear mixture model [5]. 
Specifically, the spectrum of each pixel can often be written as 
a linear mixture of a few archetypal spectral signatures known 
as endmembers. The number of endmembers, denoted by ğ‘€, is 
usually  much  smaller  than  the  spectral  dimension  of  a 
hyperspectral  image,  i.e,  ğ‘€ â‰ª ğ¿.  Therefore,  if  we  arrange  ğ‘€ 
endmembers corresponding to ğ— as columns of the matrix ğ„ âˆˆ
â„ğ¿Ã—ğ‘€, we can factorize ğ— as 

ğ— = ğ„ğ€ + ğ 

(3) 

where ğ€ âˆˆ â„ğ‘€Ã—ğ‘ is the matrix of endmember abundances and 
ğ âˆˆ â„ğ¿Ã—ğ‘  is  a  perturbation  matrix  that  accounts  for  any 
possible  inaccuracy  or  mismatch  in  the  linear  mixture  mode. 
We assume that ğ is independent of ğğ‘˜, ğ‘˜ = 1, â€¦ , ğ¾, and has a 
matrix normal distribution as 

ğ âˆ¼ â„³ğ’©ğ¿Ã—ğ‘(ğŸğ¿Ã—ğ‘, ğšº, ğˆğ‘) 

(4) 

 

where ğšº âˆˆ â„ğ¿Ã—ğ¿ is its row-covariance matrix. Every column of 
ğ€  contains the  fractional abundances  of  the  endmembers  at  a 
pixel.  The  fractional  abundances  are  nonnegative  and  often 
assumed to add up to one for each pixel. 

The linear mixture model stated above has been widely used 
in  various  contexts  and  applications  concerning  multiband, 
particularly hyperspectral, images. Its popularity can mostly be 
attributed to its intuitiveness as well as relative simplicity and 
ease  of  implementation.  However,  there  are  a  few  caveats 
regarding this model that should be kept in mind. First, ğ— in (3) 
corresponds to a matrix of corrected (pre-processed) values, not 
raw ones that would typically be captured by a spectral imager 
of the same spatial and spectral resolutions. However, whether 
these  values  are radiance  or reflectance  has no impact  on  the 
validity  of  the  model,  though  it  certainly  matters  for  further 
processing  of  the  data.  Second,  the  model  (3)  does  not 
necessarily require each endmember to be the spectral signature 
of only one (pure) material. An endmember may be composed 
of the spectral signatures of multiple materials or may be seen 
as the spectral signature of a composite material made of several 
constituent  materials.  Additionally,  depending  on 
the 
application,  the  endmembers  may  be  purposely  defined  in 
particular  subjective  ways.  Third,  in  practice,  an  endmember 
may have slightly different spectral manifestations at different 
parts  of  a  scene  due  to  variable  illumination,  environmental, 
atmospheric, or temporal conditions. This so-called endmember 
variability [53] along with possible nonlinearities in the actual 
underlying mixing process [54] may introduce inaccuracies or 
inconsistencies in the linear mixture model and consequently in 
the endmember extraction or spectral unmixing techniques that 
rely  on  this  model.  Moreover,  the  sum-to-one  assumption  on 
the abundances of each pixel may not always hold, especially, 
when the linear mixture model is not able to account for every 
material in a pixel possibly because of the effects of endmember 
variability or nonlinear mixing. 

C. Fusion model 

Substituting (3) into (1) gives 

 

ğ˜ğ‘˜ = ğ‘ğ‘˜ğ„ğ€ğğ‘˜ğ’ğ‘˜ + ğÌŒğ‘˜ 

where the aggregate perturbation of the ğ‘˜th image is 

its  abundance  matrix  ğ€  from 

ğÌŒğ‘˜ = ğğ‘˜ + ğ‘ğ‘˜ğğğ‘˜ğ’ğ‘˜. 
Instead of estimating the target multiband image ğ— directly, we 
consider  estimating 
the 
observations  ğ˜ğ‘˜,  ğ‘˜ = 1, . . ğ¾,  given  the  endmember  matrix  ğ„. 
We  can  then  obtain  an  estimate  of  the  target  image  by 
multiplying the estimated abundance matrix by the endmember 
matrix.  This  way,  we  reduce  the  dimensionality  of  the  fusion 
problem  and  consequently 
the  associated  computational 
burden. In addition, by estimating ğ€ first, we attain an unmixed 
fused image obviating the need to perform additional unmixing, 
if  demanded  by  any  application  utilizing  the  fused  image. 
However,  this  approach  requires  the  prior  knowledge  of  the 
endmember  matrix  ğ„.  The  columns  of  this  matrix  can  be 
selected from a library of known spectral signatures, such as the 
U.S.  Geological  Survey  digital  spectral  library3,  or  extracted 

 
3 http://speclab.cr.usgs.gov/spectral.lib06/ 

4 

from the observed multiband images that have the appropriate 
spectral dimension. 

III. PROBLEM 

A. Maximum-likelihood estimation 

In  order  to  facilitate  our  analysis,  we  define  the  following 

vectorized variables 

ğ²ğ‘˜ = vec{ğ˜ğ‘˜} âˆˆ â„ğ¿ğ‘˜ğ‘ğ‘˜Ã—1 
ğš = vec{ğ€} âˆˆ â„ğ‘€ğ‘Ã—1 
ğ©ğ‘˜ = vec{ğÌŒğ‘˜} âˆˆ â„ğ¿ğ‘˜ğ‘ğ‘˜Ã—1 
where  vec{âˆ™}  is  the  vectorization  operator  that  stacks  the 
columns of its matrix argument on top of each other. Applying 
vec{âˆ™} to both sides of (5) while using the property vec{ğ€ğğ‚} =
(ğ‚âŠ¤â¨‚ğ€)vec{ğ} gives 

ğ²ğ‘˜ = (ğ’ğ‘˜

âŠ¤ğğ‘˜

âŠ¤â¨‚ğ‘ğ‘˜ğ„)ğš + ğ©ğ‘˜ 

(6) 

where â¨‚ denotes the Kronecker product. 

Since ğğ‘˜ and ğ have independent matrix normal distributions 
[see  (2)  and  (4)],  ğ©ğ‘˜  has  a  multivariate  normal  distribution 
expressed as 

ğ©ğ‘˜ ~ ğ’©ğ¿ğ‘˜ğ‘ğ‘˜

(ğŸğ¿ğ‘˜ğ‘ğ‘˜, ğˆğ‘ğ‘˜â¨‚ğšºğ‘˜ + ğ’ğ‘˜

âŠ¤ğğ‘˜

âŠ¤ğğ‘˜ğ’ğ‘˜â¨‚ğ‘ğ‘˜ğšºğ‘ğ‘˜

âŠ¤) 

where ğŸğ¿ğ‘˜ğ‘ğ‘˜  stands  for the ğ¿ğ‘˜ğ‘ğ‘˜ Ã— 1  vector  of  zeroes.  Using 
the approximation ğ’ğ‘˜

âŠ¤ğğ‘˜ğ’ğ‘˜ â‰ˆ ğœ‰ğ‘˜ğˆğ‘ğ‘˜ with ğœ‰ğ‘˜ > 0, we get 

âŠ¤ğğ‘˜

ğ©ğ‘˜ ~ ğ’©ğ¿ğ‘˜ğ‘ğ‘˜

(ğŸğ¿ğ‘˜ğ‘ğ‘˜, ğˆğ‘ğ‘˜â¨‚ğš²ğ‘˜) 

(7) 

 

 

where 

âŠ¤. 
ğš²ğ‘˜ = ğšºğ‘˜ + ğœ‰ğ‘˜ğ‘ğ‘˜ğšºğ‘ğ‘˜

In view of (6) and (7), we have 

ğ²ğ‘˜ ~ ğ’©ğ¿ğ‘˜ğ‘ğ‘˜

([ğ’ğ‘˜

âŠ¤ğğ‘˜

âŠ¤â¨‚ğ‘ğ‘˜ğ„]ğš, ğˆğ‘ğ‘˜â¨‚ğš²ğ‘˜). 

Hence, the probability density function of ğ²ğ‘˜ parametrized over 
the unknown ğš can be written as 

(5) 

1
âˆ’
2 
(ğ²ğ‘˜; ğš) = |2ğœ‹ğˆğ‘ğ‘˜â¨‚ğš²ğ‘˜|

   ğ‘“ğ²ğ‘˜
                           Ã— exp {âˆ’

1
2

âŠ¤ğğ‘˜
[ğ²ğ‘˜ âˆ’ (ğ’ğ‘˜
âˆ’1

âŠ¤â¨‚ğ‘ğ‘˜ğ„)ğš]âŠ¤ 

                                       (ğˆğ‘ğ‘˜â¨‚ğš²ğ‘˜)

[ğ²ğ‘˜ âˆ’ (ğ’ğ‘˜

âŠ¤ğğ‘˜

âŠ¤â¨‚ğ‘ğ‘˜ğ„)ğš]}. 

Since  the  perturbations  ğ©ğ‘˜,  ğ‘˜ = 1, â€¦ , ğ¾,  are  independent  of 
each  other,  the  joint  probability  density  function  of  the 
observations is written as 

ğ‘“ğ²1,â€¦,ğ²ğ¾

(ğ²1, â€¦ , ğ²ğ¾; ğš) = âˆ ğ‘“ğ²ğ‘˜

(ğ²ğ‘˜; ğš)

 

ğ¾

ğ‘˜=1
ğ¾

ğ‘˜=1

âˆ’
= âˆ |2ğœ‹ğˆğ‘ğ‘˜â¨‚ğš²ğ‘˜|

1
2

 

Ã— exp {âˆ’

1
2

ğ¾
âˆ‘â€–(ğˆğ‘ğ‘˜â¨‚ğš²ğ‘˜
ğ‘˜=1

âˆ’1/2)[ğ²ğ‘˜ âˆ’ (ğ’ğ‘˜

âŠ¤ğğ‘˜

2
âŠ¤â¨‚ğ‘ğ‘˜ğ„)ğš]â€–

} 

and the log-likelihood function of ğš given the observed data as 

 

ğ‘™(ğš|ğ²1, â€¦ , ğ²ğ¾) = ln ğ‘“ğ²1,â€¦,ğ²ğ¾

(ğ²1, â€¦ , ğ²ğ¾; ğš) 

= âˆ’

ln (âˆ |2ğœ‹ğˆğ‘ğ‘˜â¨‚ğš²ğ‘˜|

) 

ğ¾

ğ‘˜=1

1
2
ğ¾
âˆ‘â€–(ğˆğ‘ğ‘˜â¨‚ğš²ğ‘˜
ğ‘˜=1

1
2

                         âˆ’

âˆ’1 2â„ )[ğ²ğ‘˜ âˆ’ (ğ’ğ‘˜

âŠ¤ğğ‘˜

2
âŠ¤â¨‚ğ‘ğ‘˜ğ„)ğš]â€–

. 

Accordingly, the maximum-likelihood estimate of ğš is found by 
solving the following optimization problem 

   ğšÌ‚ = argmax

 ğ‘™(ğš|ğ²1, â€¦ , ğ²ğ¾) 

ğš

= argmin

ğš

1
 
2

ğ¾
âˆ‘â€–(ğˆğ‘ğ‘˜â¨‚ğš²ğ‘˜
ğ‘˜=1

âˆ’1/2)[ğ²ğ‘˜ âˆ’ (ğ’ğ‘˜

âŠ¤ğğ‘˜

2
âŠ¤â¨‚ğ‘ğ‘˜ğ„)ğš]â€–

. 

(8) 

This problem can be stated in terms of ğ€ = vecâˆ’1{ğš} as 

 

ğ€Ì‚ = argmin

ğ€

1
 
2

ğ¾
âˆ‘â€–ğš²ğ‘˜
ğ‘˜=1

âˆ’1 2â„ (ğ˜ğ‘˜ âˆ’ ğ‘ğ‘˜ğ„ğ€ğğ‘˜ğ’ğ‘˜)â€–

(9) 

2

F

. 

The  Fisher  information  matrix  (FIM)  of  the  maximum-

likelihood estimator ğšÌ‚ in (8) is calculated as 

ğ“• = âˆ’E[ğ“—ğ‘™(ğš)] 
where  ğ“—ğ‘™(ğš)  denotes  the  Hessian,  i.e.,  the  Jacobian  of  the 
gradient,  of  the  log-likelihood  function  ğ‘™(ğš|ğ²1, â€¦ , ğ²ğ¾).  The 
entry on the ğ‘–th row and the ğ‘—th column of ğ“—ğ‘™(ğš) is computed 
as 

ğœ•2
ğœ•ğ‘ğ‘–ğœ•ğ‘ğ‘—

ğ‘™(ğš|ğ²1, â€¦ , ğ²ğ¾) 

ğ¾

ğ‘˜=1

ğ¾

ğ‘˜=1

ğ¾

ğ‘˜=1

where ğ‘ğ‘– and ğ‘ğ‘— denote the ğ‘–th and ğ‘—th entries of ğš, respectively. 
Accordingly, we can show that 

ğ“• = âˆ‘(ğğ‘˜ğ’ğ‘˜ğ’ğ‘˜

âŠ¤ğğ‘˜

âŠ¤â¨‚ğ„âŠ¤ğ‘ğ‘˜

âŠ¤ğš²ğ‘˜

âˆ’1ğ‘ğ‘˜ğ„)
. 

If ğ“• is invertible, the optimization problem (8) has a unique 

solution given by 

ğšÌ‚ = [âˆ‘(ğğ‘˜ğ’ğ‘˜ğ’ğ‘˜

âŠ¤ğğ‘˜

âŠ¤â¨‚ğ„âŠ¤ğ‘ğ‘˜

âŠ¤ğš²ğ‘˜

âˆ’1ğ‘ğ‘˜ğ„)
]

 

âˆ’1

          Ã— âˆ‘(ğğ‘˜ğ’ğ‘˜â¨‚ğ„âŠ¤ğ‘ğ‘˜

âŠ¤ğš²ğ‘˜

âˆ’1)ğ²ğ‘˜

 

âŠ¤ğğ‘˜

âŠ¤ğš²ğ‘˜

and the Cramer-Rao lower bound for the estimator ğšÌ‚, which is 
a lower bound on the covariance of ğšÌ‚, is the inverse of ğ“•. The 
FIM  ğ“•  is  guaranteed  to  be  invertible  when,  for  at  least  one 
âŠ¤â¨‚ğ„âŠ¤ğ‘ğ‘˜
image, the matrix ğğ‘˜ğ’ğ‘˜ğ’ğ‘˜

âˆ’1ğ‘ğ‘˜ğ„ is full-rank. 

The matrix ğ’ğ‘˜ğ’ğ‘˜

âŠ¤ has a rank of ğ‘ğ‘˜ hence for ğ·ğ‘˜ > 1 is rank-
deficient. The blurring matrix ğğ‘˜ does not change the rank of 
âˆ’1 
the matrix that it multiplies from the right. In addition, as ğš²ğ‘˜
âˆ’1ğ‘ğ‘˜ğ„ has a full rank of ğ‘€ when the rows 
is full-rank, ğ„âŠ¤ğ‘ğ‘˜
of  ğ‘ğ‘˜ğ„  are  at  least  as  many  as  its  columns,  i.e.,  ğ¿ğ‘˜ â‰¥ ğ‘€. 
Therefore, ğ€ and consequently ğ— is guaranteed to be uniquely 
identifiable  given  ğ˜ğ‘˜,  ğ‘˜ = 1, â€¦ , ğ¾,  only  when  at  least  one 
observed image, say the ğ‘th image, has full spatial resolution, 

âŠ¤ğš²ğ‘˜

5 

i.e., ğ‘ğ‘ = ğ‘, with the number of its spectral bands being equal 
to or larger than the number of endmembers, i.e., ğ¿ğ‘ â‰¥ ğ‘€, so 
âˆ’1ğ‘ğ‘ğ„ is 
that, at  least  for  the ğ‘th  image, ğğ‘ğ’ğ‘ğ’ğ‘
full-rank. 

âŠ¤â¨‚ğ„âŠ¤ğ‘ğ‘

âŠ¤ğğ‘

âŠ¤ğš²ğ‘

it 

is 

to  satisfy 

In  practice, 

rarely  possible 

the 
abovementioned requirements as  multiband images  with high 
spectral resolution are generally spatially downsampled and the 
number of bands of the ones with full spatial resolution, such as 
panchromatic  or  multispectral  images,  is  often  less  than  the 
number  of  endmembers.  Hence,  the  inverse  problem  of 
recovering ğ€ from ğ˜ğ‘˜, ğ‘˜ = 1, â€¦ , ğ¾, is usually ill-posed or ill-
conditioned. Thus, some prior knowledge need be injected into 
the  estimation  process  to  produce  a  unique  and  reliable 
estimate.  The  prior  knowledge  is  intended  to  partially 
compensate  for  the  information  lost  in  spectral  and  spatial 
downsampling and usually stems from experimental evidence 
or common facts that may induce certain analytical properties 
or constraints. The prior information is commonly incorporated 
into the problem in the form of imposed constraints or additive 
regularization terms. Examples of prior knowledge about ğ€ that 
are regularly used in the literature are nonnegativity and sum-
to-one  constraints,  matrix  normal  distribution  with  known  or 
estimated parameters [36], sparse representation with a learned 
or known dictionary or basis [32], and minimal total variation 
[29]. 

B. Regularization 

To  develop  an  algorithm  for  effective  fusion  of  multiple 
images  with  arbitrary  spatial  and  spectral 
multiband 
resolutions,  we  employ  two  mechanisms  to  regularize  the 
maximum-likelihood cost function in (9). 

As  the  first  regularization  mechanism,  we  impose  a 
constraint on ğ€ such that its entries are nonnegative and sum to 
one  in  all  columns.  We  express  this  constraint  as  ğ€ â‰¥ 0  and 
âŠ¤ where ğ€ â‰¥ 0 means all the entries of ğ€ are greater 
âŠ¤ ğ€ = ğŸğ‘
ğŸğ‘€
than or equal to zero. As the second regularization mechanism, 
we add an isotropic vector total-variation penalty term, denoted 
by â€–âˆ‡ğ€â€–2,1, to the cost function. Here, â€–âˆ™â€–2,1 is the â„“2,1-norm 
operator that returns the sum of â„“2-norms of all the columns of 
its matrix argument. In addition, we define 

âˆ‡ğ€ = [

] âˆˆ â„2ğ‘€Ã—ğ‘ 

ğ€ğƒâ„
ğ€ğƒğ‘£

where ğƒâ„ and ğƒğ‘£ are discrete differential matrix operators that, 
respectively,  yield  the  horizontal  and  vertical  first-order 
backward differences (gradients) of the row-vectorized image 
that they multiply from the right. Consequently, we formulate 
our regularized optimization problem for estimating ğ€ as 

min
ğ€

 

1
 
2

ğ¾
âˆ‘â€–ğš²ğ‘˜
ğ‘˜=1

âˆ’1 2â„ (ğ˜ğ‘˜ âˆ’ ğ‘ğ‘˜ğ„ğ€ğğ‘˜ğ’ğ‘˜)â€–

+ Î±â€–âˆ‡ğ€â€–2,1 

2

F

âŠ¤  
âŠ¤ ğ€ = ğŸğ‘
where Î± â‰¥ 0 is the regularization parameter. 

subject to: ğ€ â‰¥ 0 and ğŸğ‘€

 

 

(10) 

The nonnegativity  and  sum-to-one  constraints  on ğ€,  which 
force the columns of ğ€ to reside on the unit (ğ‘€ âˆ’ 1)-simplex, 
are naturally expected and help find a solution that is physically 
plausible. In addition, they implicitly induce sparseness in the 
solution. The total-variation penalty promotes solutions with a 

 

sparse  gradient,  a  property  that  is  known  to  be  possessed  by 
images  of  most  natural  scenes  as  they  are  usually  made  of 
piecewise  homogeneous  regions  with  few  sudden  changes  at 
object boundaries or edges. Note that the subspace spanned by 
the  endmembers  is  the  one  that  the  target  image  ğ—  lives  in. 
Therefore,  through  the  total-variation  regularization  of  the 
abundance matrix ğ€, we regularize ğ— indirectly. 

IV. ALGORITHM 

Defining the set of values for ğ€ that satisfy the nonnegativity 

and sum-to-one constraints as 

 

ğ’® = {ğ€|ğ€ â‰¥ 0, ğŸğ‘€

âŠ¤ ğ€ = ğŸğ‘

âŠ¤ } 

(11) 

and making use of the indicator function ğš¤ğ’®(ğ€) defined as 

ğš¤ğ’®(ğ€) = {

0

ğ€ âˆˆ ğ’®
+âˆ ğ€ âˆ‰ ğ’®,

 

âˆ’1 2â„ (ğ˜ğ‘˜ âˆ’ ğ‘ğ‘˜ğ„ğ€ğğ‘˜ğ’ğ‘˜)â€–

+ Î±â€–âˆ‡ğ€â€–2,1 + ğš¤ğ’®(ğ€). 

2

F

we rewrite (10) as 

1
 
2

ğ¾
âˆ‘â€–ğš²ğ‘˜
ğ‘˜=1

min
ğ€

A. Iterations 

(ğ‘›âˆ’1), â€¦ , ğ”ğ¾
(ğ‘›âˆ’1), â€¦ , ğ…ğ¾

(ğ‘›âˆ’1), ğ•(ğ‘›âˆ’1), ğ–(ğ‘›âˆ’1), 
(ğ‘›âˆ’1), ğ†(ğ‘›âˆ’1), ğ‡(ğ‘›âˆ’1))  (15) 

ğ€(ğ‘›) = argmin

 â„’(ğ€, ğ”1

ğ€
                                       ğ…1
(ğ‘›), â€¦ , ğ”ğ¾
{ğ”1
           = argmin
ğ”1,â€¦,ğ”ğ¾,ğ•,ğ–
                                          ğ…1

(ğ‘›), ğ•(ğ‘›), ğ–(ğ‘›)} 

â„’(ğ€(ğ‘›), ğ”1, â€¦ , ğ”ğ¾, ğ•, ğ–, 
(ğ‘›âˆ’1), â€¦ , ğ…ğ¾

(ğ‘›âˆ’1), ğ†(ğ‘›âˆ’1), ğ‡(ğ‘›âˆ’1)) 

 
(16) 

(ğ‘›) = ğ…ğ‘˜

(ğ‘›âˆ’1) âˆ’ (ğ€(ğ‘›)ğğ‘˜ âˆ’ ğ”ğ‘˜

  ğ…ğ‘˜
 ğ†(ğ‘›) = ğ†(ğ‘›âˆ’1) âˆ’ (âˆ‡ğ€(ğ‘›) âˆ’ ğ•(ğ‘›)) 
 ğ‡(ğ‘›) = ğ‡(ğ‘›âˆ’1) âˆ’ (ğ€(ğ‘›) âˆ’ ğ–(ğ‘›)) 

(ğ‘›)),   ğ‘˜ = 1, â€¦ , ğ¾ 

where superscript (ğ‘›) denotes the value of an iterate at iteration 
number  ğ‘› â‰¥ 0.  We  repeat  the  iterations  until  convergence  is 
reached up to a maximum allowed number of iterations. 

Since we define the auxiliary variables independent of each 
other, the minimization of the augmented Lagrangian function 
(14)  with  respect  to  the  auxiliary  variables  can  be  realized 
separately. Thus, (16) is equivalent to 

(12) 

(ğ‘›) = argmin

   ğ”ğ‘˜

ğ”ğ‘˜

âˆ’1 2â„ (ğ˜ğ‘˜ âˆ’ ğ‘ğ‘˜ğ„ğ”ğ‘˜ğ’ğ‘˜)â€–

2

F

â€–ğ€(ğ‘›)ğğ‘˜ âˆ’ ğ”ğ‘˜ âˆ’ ğ…ğ‘˜

2
(ğ‘›âˆ’1)â€–
F

, ğ‘˜ = 1, â€¦ , ğ¾ 

We  use  the  alternating  direction  method  of  multipliers 
(ADMM),  also  known  as  the  split-Bregman method, to  solve 
the convex but non-smooth optimization problem of (12). We 
split  the  problem  to  smaller  and  more  manageable  pieces  by 
defining the auxiliary variables, ğ”ğ‘˜ âˆˆ â„ğ‘€Ã—ğ‘, ğ‘˜ = 1, â€¦ , ğ¾, ğ• âˆˆ
â„2ğ‘€Ã—ğ‘, and ğ– âˆˆ â„ğ‘€Ã—ğ‘, and changing (12) into 

min
ğ€,ğ”1,â€¦,ğ”ğ¾,ğ•,ğ–

1
 
2

ğ¾
âˆ‘â€–ğš²ğ‘˜
ğ‘˜=1

+ ğš¤ğ’®(ğ–) 

âˆ’1 2â„ (ğ˜ğ‘˜ âˆ’ ğ‘ğ‘˜ğ„ğ”ğ‘˜ğ’ğ‘˜)â€–

+ Î±â€–ğ•â€–2,1

2

F

subject to: ğ”ğ‘˜ = ğ€ğğ‘˜, ğ• = âˆ‡ğ€, ğ– = ğ€. 

(13) 

Then, we write the augmented Lagrangian function associated 
with (13) as 
 

 

 ğ•(ğ‘›) = argmin

 Î±â€–ğ•â€–2,1 +

2
â€–âˆ‡ğ€(ğ‘›) âˆ’ ğ• âˆ’ ğ†(ğ‘›âˆ’1)â€–
F

 

ğ–(ğ‘›) = argmin

 ğš¤ğ’®(ğ–) +

2
â€–ğ€(ğ‘›) âˆ’ ğ– âˆ’ ğ‡(ğ‘›âˆ’1)â€–
F

. 

B. Solutions of subproblems 

Considering (14), (15) can be written as 

ğ€(ğ‘›) = argmin

ğ€

2
(ğ‘›âˆ’1)â€–
F

(ğ‘›âˆ’1) âˆ’ ğ…ğ‘˜

ğ¾
 âˆ‘â€–ğ€ğğ‘˜ âˆ’ ğ”ğ‘˜
ğ‘˜=1
2
+ â€–âˆ‡ğ€ âˆ’ ğ•(ğ‘›âˆ’1) âˆ’ ğ†(ğ‘›âˆ’1)â€–
F
2
+ â€–ğ€ âˆ’ ğ–(ğ‘›âˆ’1) âˆ’ ğ‡(ğ‘›âˆ’1)â€–
 . 
F

           =

âˆ’1 2â„ (ğ˜ğ‘˜ âˆ’ ğ‘ğ‘˜ğ„ğ”ğ‘˜ğ’ğ‘˜)â€–

+ Î±â€–ğ•â€–2,1 + ğš¤ğ’®(ğ–) 

2

F

â„’(ğ€, ğ”1, â€¦ , ğ”ğ¾, ğ•, ğ–, ğ…1, â€¦ , ğ…ğ¾, ğ†, ğ‡) 

ğ¾
âˆ‘â€–ğš²ğ‘˜
ğ‘˜=1
ğ¾
2
âˆ‘â€–ğ€ğğ‘˜ âˆ’ ğ”ğ‘˜ âˆ’ ğ…ğ‘˜â€–F
ğ‘˜=1
2 
â€–ğ€ âˆ’ ğ– âˆ’ ğ‡â€–F

1
2

ğœ‡
2
ğœ‡
2

           +

           +

+

2 
â€–âˆ‡ğ€ âˆ’ ğ• âˆ’ ğ†â€–F

ğœ‡
2

where  ğ…ğ‘˜ âˆˆ â„ğ‘€Ã—ğ‘,  ğ‘˜ = 1, â€¦ , ğ¾,  ğ† âˆˆ â„2ğ‘€Ã—ğ‘,  and  ğ‡ âˆˆ â„ğ‘€Ã—ğ‘ 
are  the  scaled  Lagrange  multipliers  and  ğœ‡ â‰¥ 0  is  the  penalty 
parameter. 

Using the ADMM, we minimize the augmented Lagrangian 
function in an iterative fashion. At each iteration, we alternate 
the minimization with respect to the main unknown variable ğ€ 
and the auxiliary variables; then, we update the scaled Lagrange 
multipliers. Hence, we compute the iterates as 

Calculating the gradient of the cost function in (20) with respect 
to ğ€ and setting it to zero gives 

ğ€(ğ‘›) = [âˆ‘(ğ”ğ‘˜

(ğ‘›âˆ’1) + ğ…ğ‘˜

(ğ‘›âˆ’1))ğğ‘˜

âŠ¤

  + ğ1

(ğ‘›âˆ’1)ğƒâ„

âŠ¤

+ ğ2

(ğ‘›âˆ’1)ğƒğ‘£

âŠ¤ + ğ–(ğ‘›âˆ’1) + ğ‡(ğ‘›âˆ’1)] 

âˆ’1

(14) 

âŠ¤
               Ã— (âˆ‘ ğğ‘˜ğğ‘˜

+ ğƒâ„ğƒâ„

âŠ¤ + ğƒğ‘£ğƒğ‘£

âŠ¤ + ğˆğ‘)

 

(21) 

where,  for the  convenience  of  presentation,  we  define  ğ1
and ğ2

(ğ‘›âˆ’1) as 

(ğ‘›âˆ’1) 

ğ¾

ğ‘˜=1

ğ¾

ğ‘˜=1

(ğ‘›âˆ’1)

[

ğ1
ğ2

(ğ‘›âˆ’1)] = ğ•(ğ‘›âˆ’1) + ğ†(ğ‘›âˆ’1). 

1
 
2

+

â€–ğš²ğ‘˜
ğœ‡
2

ğ•

ğ–

ğœ‡
2

ğœ‡
2

6 

(17) 

(18) 

(19) 

(20) 

 

 

 

 

 

 

 

 

To make the computation of ğ€(ğ‘›) in (21) more efficient, we 
assume that  the  two-dimensional  convolutions represented by 
ğğ‘˜,  ğ‘˜ = 1, â€¦ , ğ¾,  are  cyclic.  In  addition,  we  assume  that  the 
differential  matrix  operators  ğƒâ„  and  ğƒğ‘£  apply  with  periodic 
âŠ¤, and ğƒğ‘£
âŠ¤ 
boundaries. Consequently, multiplications by ğğ‘˜
âŠ¤
âŠ¤ + ğˆğ‘)âˆ’1  can  be 
as  well  as  by  (âˆ‘
+ ğƒâ„ğƒâ„
ğğ‘˜ğğ‘˜
performed through the use of the fast Fourier transform (FFT) 
algorithm and the circular convolution theorem. This theorem 
states that the Fourier transform of a circular convolution is the 
pointwise  product  of  the  Fourier  transforms,  i.e.,  a  circular 
convolution can be expressed as the inverse Fourier transform 
of the product of the individual spectra [55]. 

âŠ¤ + ğƒğ‘£ğƒğ‘£

âŠ¤, ğƒâ„

ğ¾
ğ‘˜=1

Equating the gradient of the cost function in (17) with respect 

to ğ”ğ‘˜ to zero results in 

âŠ¤ğš²ğ‘˜

       ğ„âŠ¤ğ‘ğ‘˜
                              = ğ„âŠ¤ğ‘ğ‘˜

âˆ’1ğ‘ğ‘˜ğ„ğ”ğ‘˜

(ğ‘›)ğ’ğ‘˜ğ’ğ‘˜
âŠ¤ğš²ğ‘˜

âŠ¤ + ğœ‡ğ”ğ‘˜
âˆ’1ğ˜ğ‘˜ğ’ğ‘˜

(ğ‘›) 
âŠ¤ + ğœ‡(ğ€(ğ‘›)ğğ‘˜ âˆ’ ğ…ğ‘˜

 

(ğ‘›âˆ’1)). 

(22) 

âŠ¤ and its complement ğˆğ‘ âˆ’ ğŒğ‘˜ yields 

Multiplying  both  sides  of  (22)  from the right  by  the masking 
matrix ğŒğ‘˜ = ğ’ğ‘˜ğ’ğ‘˜
(ğ‘›)ğŒğ‘˜ = (ğ„âŠ¤ğ‘ğ‘˜
  ğ”ğ‘˜
                  Ã— [ğ„âŠ¤ğ‘ğ‘˜

âˆ’1ğ‘ğ‘˜ğ„ + ğœ‡ğˆğ‘)âˆ’1 
âˆ’1ğ˜ğ‘˜ğ’ğ‘˜

âŠ¤ + ğœ‡(ğ€(ğ‘›)ğğ‘˜ âˆ’ ğ…ğ‘˜

(ğ‘›âˆ’1))ğŒğ‘˜] 

âŠ¤ğš²ğ‘˜
âŠ¤ğš²ğ‘˜

(23) 

 

 

where 

and 

 

(ğ‘›)(ğˆğ‘ âˆ’ ğŒğ‘˜) = (ğ€(ğ‘›)ğğ‘˜ âˆ’ ğ…ğ‘˜
(ğ‘›âˆ’1))(ğˆğ‘ âˆ’ ğŒğ‘˜), 
ğ”ğ‘˜
âŠ¤ğ’ğ‘˜ = ğˆğ‘  and  ğŒğ‘˜  is 
respectively.  Note  that  we  have  ğ’ğ‘˜
idempotent, i.e., ğŒğ‘˜ğŒğ‘˜ = ğŒğ‘˜. Summing both sides of (23) and 
(24) gives the solution of (17) for ğ‘˜ = 1, â€¦ , ğ¾ as 

(24) 

(ğ‘›) = ğ”ğ‘˜
ğ”ğ‘˜

(ğ‘›)ğŒğ‘˜ + ğ”ğ‘˜
âŠ¤ğš²ğ‘˜
âŠ¤ğš²ğ‘˜

(ğ‘›)(ğˆğ‘ âˆ’ ğŒğ‘˜) 
âˆ’1ğ‘ğ‘˜ğ„ + ğœ‡ğˆğ‘)âˆ’1 
âˆ’1ğ˜ğ‘˜ğ’ğ‘˜

= (ğ„âŠ¤ğ‘ğ‘˜
         Ã— [ğ„âŠ¤ğ‘ğ‘˜
         +(ğ€(ğ‘›)ğğ‘˜ âˆ’ ğ…ğ‘˜

(ğ‘›âˆ’1))(ğˆğ‘ âˆ’ ğŒğ‘˜). 
âˆ’1ğ‘ğ‘˜ğ„ + ğœ‡ğˆğ‘)âˆ’1 and ğ„âŠ¤ğ‘ğ‘˜

âŠ¤ + ğœ‡(ğ€(ğ‘›)ğğ‘˜ âˆ’ ğ…ğ‘˜

(ğ‘›âˆ’1))ğŒğ‘˜] 

The terms (ğ„âŠ¤ğ‘ğ‘˜
change during the iterations and can be precomputed. 

âˆ’1ğ˜ğ‘˜ğ’ğ‘˜

âŠ¤ğš²ğ‘˜

âŠ¤ğš²ğ‘˜

âŠ¤ do not 

The  subproblem  (18)  can  be  decomposed  pixelwise  and  its 
solution is linked to the so-called Moreau proximity operator of 
the  â„“2,1-norm  given  by  column-wise  vector-soft-thresholding 
[56], [57]. If we define 

ğ™(ğ‘›) = âˆ‡ğ€(ğ‘›) âˆ’ ğ†(ğ‘›âˆ’1), 

the ğ‘—th column of ğ•(ğ‘›), denoted by ğ¯ğ‘—
ğ‘—th column of ğ™(ğ‘›), denoted by ğ³ğ‘—

(ğ‘›), as 

(ğ‘›), is given in terms of the 

(ğ‘›) =
ğ¯ğ‘—

max {â€–ğ³ğ‘—

(ğ‘›)â€–
2
(ğ‘›)â€–

â€–ğ³ğ‘—

2

âˆ’

Î±
ğœ‡ , 0}

(ğ‘›). 
ğ³ğ‘—

The solution of (19) is the value of the proximity operator of 
the indicator function ğš¤ğ’®(ğ–) at the point ğ€(ğ‘›) âˆ’ ğ‡(ğ‘›âˆ’1), which 
is  the  projection  of  ğ€(ğ‘›) âˆ’ ğ‡(ğ‘›âˆ’1)  onto  the  set  ğ’®  defined  by 
(11). Therefore, we have 

7 

ğ–(ğ‘›) = argmin

ğ–âˆˆğ’®

2
 â€–ğ€(ğ‘›) âˆ’ ğ‡(ğ‘›âˆ’1) âˆ’ ğ–â€–
F

 

= Î ğ’®{ğ€(ğ‘›) âˆ’ ğ‡(ğ‘›âˆ’1)} 

where Î ğ’®{âˆ™} denotes the projection onto ğ’®. We implement this 
projection  onto  the  unit  (ğ‘€ âˆ’ 1)-simplex  employing  the 
algorithm proposed in [58]. 

We present a summary of the proposed algorithm in Table I. 

C. Convergence 
By defining 

and 

ğ“¤ = [ğ”1, â‹¯ , ğ”ğ¾, ğ•, ğ–]âŠ¤ 

ğ“’ = [ğ1, â‹¯ , ğğ¾, ğƒâ„, ğƒğ‘£, ğˆğ‘]âŠ¤, 

(13) can be expressed as 

min
ğ€

 ğ‘“(ğ“¤) 

subject to ğ“¤ = ğ“’ğ€âŠ¤ 

(25) 

ğ‘“(ğ“¤) =

âˆ’1 2â„ (ğ˜ğ‘˜ âˆ’ ğ‘ğ‘˜ğ„ğ”ğ‘˜ğ’ğ‘˜)â€–

+ Î±â€–ğ•â€–2,1 + ğš¤ğ’®(ğ–). 

2

F

1
2

ğ¾
âˆ‘â€–ğš²ğ‘˜
ğ‘˜=1

The function ğ‘“(ğ“¤) is closed, proper, and convex as it is a sum 
of closed, proper, and convex functions and ğ“’ has full column 
rank.  Therefore,  according  to  [47,  Theorem  8],  if  (25)  has  a 
solution,  the  proposed  algorithm  converges  to  this  solution, 
regardless of the initial values as long as the penalty parameter 
ğœ‡ is positive. If no solution exists, at least one of ğ€(ğ‘›) and ğ“¤(ğ‘›) 
will diverge. 

V. SIMULATIONS 

images,  viz.  a  panchromatic 

To  examine  the  performance  of  the  proposed  algorithm  in 
comparison with the state-of-the-art, we simulate the fusion of 
image,  a 
three  multiband 
multispectral image, and a hyperspectral image. To this end, we 
adopt the popular practice known as the Waldâ€™s protocol [78], 
which is to use a reference image with high spatial and spectral 
resolutions  to  generate  the  lower-resolution  images  that  are 
fused  and  evaluate  the  fusion  performance  by  comparing  the 
fused image with the reference image. 

We  obtain  the  reference  images  of  our  experiments  by 
cropping  five  publicly  available  hyperspectral  images  to  the 
spatial resolutions  given  in  Table  II. These  images  are  called 
Botswana4, Indian Pines [59], Washington DC Mall5, Moffett 
Field6, and Kennedy Space Center4. The Botswana image has 
been  captured  by  the  Hyperion  sensor  aboard  the  Earth 
Observing 1 (EO-1) satellite, the Washington DC Mall image 
by 
the  airborne-mounted  Hyperspectral  Digital  Imagery 
Collection  Experiment  (HYDICE),  and  the  Indian  Pines, 
Moffett Filed, and Kennedy Space Center images by the NASA 
Airborne  Visible/Infrared  Imaging  Spectrometer  (AVIRIS) 
instrument. All images cover the visible near-infrared (VNIR) 
ranges  with 
infrared 
and 

short-wavelength 

(SWIR) 

 
4 http://www.ehu.eus/ccwintco/?title=Hyperspectral_Remote_Sensing_Scenes 
5 https://engineering.purdue.edu/~biehl/MultiSpec/hyperspectral.html 

6 http://aviris.jpl.nasa.gov/data/free_data.html 

 

uncalibrated, noisy, and water-absorption bands removed. The 
spectral resolution of each image is also given in Table II. The 
data as well as the MATLAB code used to produce the results 
of this paper can be found at [60]. 

We  generate 

three  multiband 

images  (panchromatic, 
multispectral,  and  hyperspectral)  using  each  reference  image. 
We obtain the hyperspectral images by applying a rotationally-
symmetric 2D Gaussian blur filter with a kernel size of 13 Ã— 13 
and  a  standard  deviation  of  2.12  to  each  reference  image 
followed  by  downsampling  with  a  ratio  of  4  in  both  spatial 
dimensions for all bands. For the multispectral images, we use 
a Gaussian blur filter with a kernel size of 7 Ã— 7 and a standard 
deviation of 1.06 and downsampling with a ratio of 2 in both 
spatial  dimensions  for  all  bands  of  each  reference  image. 
Afterwards,  we  downgrade  the resultant  images  spectrally  by 
applying the spectral responses of the Landsat 8 multispectral 
sensor.  This  sensor  has  eight  multispectral  bands  and  one 
panchromatic band. Fig. 1 depicts the spectral responses of all 
the bands of this sensor7. We create the panchromatic images 
from the reference images using the panchromatic band of the 
Landsat  8  sensor  without  applying  any  spatial  blurring  or 
downsampling.  We  add  zero-mean  Gaussian  white  noise  to 
each band of the produced multiband images such that the band-
specific  signal-to-noise  ratio  (SNR) 
is  30  dB  for  the 
multispectral  and  hyperspectral  images  and  40  dB  for  the 
panchromatic  image.  Note  that  we  have  selected  the  standard 
deviations of the abovementioned 2D Gaussian blur filters such 
that  the  normalized  magnitude  of  the  modulation  transfer 
function  (MTF)  of  both  filters  is  approximately  0.25  at  the 
Nyquist frequency in both spatial dimensions [77] as shown in 
Fig. 2. 

for 

algorithms 

pansharpening, 

The current multiband image fusion algorithms published in 
the literature are designed to fuse two images at a time. In order 
to compare the performance of the proposed algorithm with the 
state-of-the-art,  we  consider  fusing the  abovementioned  three 
multiband images in three different ways, which we refer to as 
Pan + HS, Pan + (MS + HS), and (Pan + MS) + HS, using the 
hyperspectral 
existing 
pansharpening, and hyperspectral-multispectral fusion. In Pan 
+ HS, we only fuse the panchromatic and hyperspectral images. 
In Pan + (MS + HS), and (Pan + MS) + HS, we fuse the given 
images in two cascading stages. In Pan + (MS + HS), first, we 
fuse the multispectral and hyperspectral images. Then, we fuse 
the resultant hyperspectral image with the panchromatic image. 
We use the same algorithm at both stages, albeit with different 
parameter  values.  In  (Pan  +  MS)  +  HS,  we  first  fuse  the 
panchromatic image with the multispectral one. Then, we fuse 
the  pansharpened  multispectral  image  with  the  hyperspectral 
image.  We  use  two  different  algorithms  at  each  of  the  two 
stages resulting in four combined solutions. 

For  pansharpening,  which  is  the  fusion  of  a  panchromatic 
image with a multispectral one, we use two algorithms called 
the  band-dependent  spatial  detail  (BDSD)  [61]  and  the 
modulation-transfer-function  generalized  Laplacian  pyramid 
with  high-pass  modulation  (MTF-GLP-HPM)  [62]-[64].  The 
BDSD algorithm belongs to the class of component substitution 
methods  and  the  MTF-GLP-HPM  algorithm  falls  into  the 

 
7 http://landsat.gsfc.nasa.gov/landsat-8/ 

8 

category  of  multiresolution  analysis.  In  [12],  where  several 
pansharpening  algorithms  are  studied,  it  is  shown  that  the 
BDSD  and  MTF-GLP-HPM  algorithms  exhibit  the  best 
performance among all the considered ones. 

For  fusing  a  panchromatic  or  multispectral  image  with  a 
hyperspectral image,  we  use  two  algorithms  proposed  in  [29] 
and  [65],  [66],  which  are  called  HySure  and  R-FUSE-TV, 
respectively.  These  algorithms  are  based  on  total-variation 
regularization  and  are  among  the  best  performing  and  most 
efficient  hyperspectral  pansharpening  and  multispectral-
hyperspectral fusion algorithms currently available [15], [67]. 
We use three performance metrics for assessing the quality 
of a fused image with respect to its reference image. The metrics 
are  the  relative  dimensionless  global  error  in  synthesis 
(ERGAS)8  [68],  spectral  angle mapper  (SAM)  [69],  and ğ‘„2ğ‘› 
[70]. The metric ğ‘„2ğ‘› is a generalization of the universal image 
quality index (UIQI) proposed in [71] and an extension of the 
ğ‘„4 index [72] to hyperspectral images based on hypercomplex 
numbers. 

We  extract  the  endmembers  (columns  of  ğ„)  from  each 
hyperspectral  image  using  the  vertex  component  analysis 
(VCA)  algorithm  [73].  The  VCA  is  a  fast  unsupervised 
unmixing  algorithm  that  assumes  the  endmembers  as  the 
vertices  of  a  simplex  encompassing  the  hyperspectral  data 
cloud. We utilize the SUnSAL algorithm [74] together with the 
extracted endmembers to unmix each hyperspectral image and 
obtain  its  abundance  matrix.  Then,  we  upscale  the  resulting 
matrix  by  a  factor  of  four  and  apply  two-dimensional  spline 
interpolation on each of its rows (abundance bands) to generate 
the initial estimate for the abundance matrix ğ€(0). We initialize 
the proposed algorithm as well as the HySure and R-FUSE-TV 
algorithms by this matrix. 

To  make  our  comparisons  fair,  we  tune  the  values  of  the 
parameters in the HySure and R-FUSE-TV algorithms to yield 
the best possible performance in all experiments. In addition, in 
order to use the BDSD and MTF-GLP-HPM algorithms to their 
best potential, we provide these algorithms with the true point-
spread  function, i.e., the  blurring  kernel,  used to  generate  the 
multispectral images. 

Apart  from  the  number  of  endmembers,  which  can  be 
estimated using, for example, the HySime algorithm [23], the 
proposed  algorithm  has  two  tunable  parameters,  the  total-
variation  regularization  parameter  Î±  and  the  ADMM  penalty 
parameter  ğœ‡.  The  automatic  tuning  of  the  values  of  these 
parameters is an interesting and challenging subject. There are 
a  number  of  strategies  that  can  be  employed  such  as  those 
proposed in [75] and [76]. We found through experimentations 
that although the value of ğœ‡ impacts the convergence speed of 
the  proposed  algorithm,  as  long as it  is  within  an  appropriate 
range,  it  has  little  influence  on  the  accuracy  of  the  proposed 
algorithm.  Therefore,  we  set  it  to  ğœ‡ = 1.5 Ã— 103  in  all 
experiments.  The  value  of  Î±  affects  the  performance  of  the 
proposed algorithm in subtle ways as shown in Fig. 3 where we 
plot the performance metrics, ERGAS, SAM, and ğ‘„2ğ‘›, against 
Î±  for  the  Botswana  and  Washington  DC  Mall  images.  The 
results in Fig. 3 suggest that, for different values of Î±, there is a 
the  performance  metrics,  specifically, 
trade-off  between 

8 The original  phrase in French is: erreur relative globale adimensionnelle  de 
synthÃ¨se. 

 

ERGAS and ğ‘„2ğ‘› on one side and SAM on the other. Therefore, 
we  tune  the  value  of  Î±  for  each  experiment  only  roughly  to 
obtain  a  reasonable  set  of  values  for  all  three  performance 
metrics. We give the values of Î± used in the proposed algorithm 
in Table II. 

In Table III, we give the values of the performance metrics 
to  assess  the  quality  of  the  images  fused  using  the  proposed 
algorithm  and  the  considered  benchmarks.  We  provide  the 
performance metrics for the case of considering only the bands 
within the spectrum of the panchromatic image as well as the 
case  of  considering  all  bands,  i.e.,  the  entire  spectrum  of  the 
reference image. We also give the time taken by each algorithm 
to produce the fused images9. According to the results in Table 
III,  the  proposed  algorithm  significantly  outperforms  the 
considered  benchmarks.  It  is  also  evident  from  the  required 
processing  times  that  the  computational  (time)  complexity  of 
the proposed algorithm is lower than those of its contenders. 

In Figs. 3 and 4, we plot the sorted per-pixel normalized root 
mean-square error (NRMSE) values of the proposed algorithm 
and the best performing algorithms from each of the Pan + HS, 
Pan  +  (MS  +  HS),  and  (Pan  +  MS)  +  HS  categories.  Fig.  4 
corresponds to the case of considering only the spectrum of the 
panchromatic image  and  Fig.  5  to the  case  of  considering  the 
entire spectrum. We define the per-pixel NRMSE as 

â€–ğ±ğ‘— âˆ’ ğ±Ì‚ğ‘—â€–
2
â€–ğ±ğ‘—â€–
2

 
 

where ğ±ğ‘— and ğ±Ì‚ğ‘— are the ğ‘—th column of the reference image ğ— and 
the fused image ğ—Ì‚, respectively. We sort the NRMSE values in 
the ascending order. 

the 

panchromatic,  multispectral, 

In Fig. 5, we show RGB renderings of the reference images 
and 
together  with 
hyperspectral  images  generated  from  them  and  used  for  the 
fusion. We also show the fused images yielded by the proposed 
algorithm  and  Pan  +  (MS  +  HS)  fusion  using  the  HySure 
algorithm,  which  generally  performs  better  than  the  other 
considered benchmarks. The multispectral images are depicted 
using their red, green, and blue bands. The RGB representations 
of the hyperspectral images are rendered through transforming 
the  spectral  data  to  the  CIE  XYZ  color  space  and  then 
transforming  the  XYZ  values to  the  sRGB  color  space.  From 
visual inspection  of  the reference and  fused  images  shown in 
Fig.  6,  it  is  observed  that  the  images  fused  by  the  proposed 
algorithm  match  their  corresponding  reference  images  better 
than the ones produced by the Pan + (MS + HS) fusion using 
the HySure algorithm do. 

VI. CONCLUSION 

We  proposed  a  new  image  fusion  algorithm  that  can 
simultaneously fuse multiple multiband images. We utilized the 
well-known forward observation model together with the linear 
mixture  model  to  cast  the  fusion  problem  as  a  reduced-
dimension  linear  inverse  problem.  We  used  a  vector  total-
variation  penalty  as  well  as  nonnegativity  and  sum-to-one 
constraints  on  the  endmember  abundances  to  regularize  the 
 
9 We used MATLAB with a 2.9GHz Core-i7 CPU and 24GB of DDR3 RAM 
and  ran  each  of  the  proposed,  HySure,  and  R-FUSE-TV  algorithms  for  200 
iterations as they always converged sufficiently after this number of iterations. 

9 

associated  maximum-likelihood  estimation  problem.  The 
regularization  encourages  the  estimated  fused  image  to  have 
low  rank  with  a  sparse  representation  in  the  spectral  domain 
while  preserving  the  edges  and  discontinuities  in  the  spatial 
domain.  We  solved  the  regularized  problem  using  the 
alternating  direction  method  of  multipliers.  We  demonstrated 
the advantages  of  the  proposed  algorithm in  comparison  with 
the state-of-the-art via experiments with five real hyperspectral 
images that were done following the Waldâ€™s protocol. 

REFERENCES 

[1]  R. Arablouei, â€œFusion of multiple multiband images with complementary 
spatial  and  spectral  resolutions,â€  Int.  Conf.  Acoust.,  Speech  Signal 
Process., Calgary, Canada, Apr. 2018. 

[2]  A.  Mohammadzadeh,  A. Tavakoli, and  M. J.  V. Zoej, â€œRoad  extraction 
based on  fuzzy logic and mathematical  morphology  from  pansharpened 
IKONOS images,â€ Photogramm. Rec., vol. 21, no. 113, pp. 44â€“60, Feb. 
2006. 

[3]  C. Souza, Jr, L. Firestone, L. M. Silva, and D. Roberts, â€œMapping forest 
degradation in the Eastern amazon from SPOT 4 through spectral mixture 
models,â€ Remote Sens. Environ., vol. 87, no. 4, pp. 494â€“506, 2003. 
[4]  G. A. Licciardi, A. Villa, M. M. Khan, and J. Chanussot, â€œImage fusion 
and spectral unmixing of hyperspectral images for spatial improvement of 
classification  maps,â€  in  Proc.  IEEE  Int.  Conf.  Geoscience  Remote 
Sensing, 2012, pp. 7290â€“729. 
J. M. Bioucas-Dias, A. Plaza, N. Dobigeon, M. Parente, Q. Du, P. Gader, 
and  J.  Chanussot,  â€œHyperspectral  unmixing  overview:  Geometrical, 
statistical,  and  sparse  regression-based  approaches,â€  IEEE  J.  Select. 
Topics Appl. Earth Observ. Remote Sens., vol. 5, no. 2, pp. 354â€“379, Apr. 
2012. 
J. M. Bioucas-Dias et al., â€œHyperspectral remote sensing data analysis and 
future challenges,â€ IEEE Geosci. Remote Sens. Mag., vol. 1, no. 2, pp. 6â€“
36, Jun. 2013. 

[5] 

[6] 

[7]  G. A. Shaw and H.-H. K. Burke, â€œSpectral imaging for remote sensing,â€ 

[8] 

Lincoln Lab. J., vol. 14, no. 1, pp. 3â€“28, 2003. 
J. Greer, â€œSparse demixing of hyperspectral images,â€ IEEE Trans. Image 
Process., vol. 21, no. 1, pp. 219â€“228, Jan. 2012. 

[9]  R. Arablouei and F. de Hoog, â€œHyperspectral image recovery via hybrid 
regularization,â€ IEEE Trans. Image Process., vol. 25, no. 12, pp. 5649â€“
5663, Dec. 2016. 

[10]  R.  Arablouei,  â€œSpectral  unmixing  with  perturbed  endmembers,â€  IEEE 
Trans. Geosci. Remote Sens., submitted for publication, 2018; available 
online at https://osf.io/wgkxd/. 

[11]  R. Arablouei, E. Goan, S. Gensemer, and B. Kusy, â€œFast and robust push-
broom hyperspectral imaging via DMD-based scanning,â€ in Proc. SPIE 
9948, Novel Optical Systems Design and Optimization XIX, San Diego, 
CA, USA, Aug./Sep. 2016, id. 99480A. 

[12]  G.  Vivone  et  al.,  â€œA  critical  comparison  among  pansharpening 
algorithms,â€ IEEE Trans. Geosci. Remote Sens., vol. 53, no. 5, pp. 2565â€“
2586, May 2015. 

[13]  L.  Alparone,  L.  Wald,  J.  Chanussot,  C.  Thomas,  P.  Gamba,  and  L.  M. 
Bruce, â€œComparison of pansharpening algorithms: Outcome of the 2006 
GRS-S data-fusion contest,â€ IEEE Trans. Geosci. Remote Sens., vol. 45, 
no. 10, pp. 3012â€“3021, Oct. 2007. 

[14]  B. Aiazzi, L. Alparone, S. Baronti, A. Garzelli, and M. Selva, â€œ25 years 
of pansharpening: A critical review and new developments,â€ in Signal and 
Image  Processing  for  Remote  Sensing,  C.  H.  Chen,  Ed.,  2nd  ed.  Boca 
Raton, FL, USA: CRC Press, 2011, ch. 28, pp. 533â€“548. 

[15]  L. Loncan et al., â€œHyperspectral pansharpening: A review,â€ IEEE Geosci. 

Remote Sens. Mag., vol. 3, no. 3, pp. 27â€“46, Sep. 2015. 

[16]  M.  Selva, B.  Aiazzi, F. Butera, L. Chiarantini, and  S. Baronti, â€œHyper-
sharpening:  A  first approach  on  SIM-GA data,â€ IEEE J. Select. Topics 
Appl.  Earth  Observ.  Remote  Sens.,  vol.  8,  no.  6,  pp.  3008â€“3024,  Jun. 
2015. 

[17]  M. T. Eismann and R. C. Hardie, â€œApplication of the stochastic mixing 
model  to  hyperspectral  resolution  enhancement,â€  IEEE  Trans.  Geosci. 
Remote Sens., vol. 42, no. 9, pp. 1924â€“1933, Sep. 2004. 

 

[18]  B.  Huang,  H.  Song,  H.  Cui,  J.  Peng,  and  Z.  Xu,  â€œSpatial  and  spectral 
image  fusion  using  sparse  matrix  factorization,â€  IEEE  Trans.  Geosci. 
Remote Sens., vol. 52, no. 3, pp. 1693â€“1704, Mar. 2014. 

[19]  Y.  Zhang,  S.  De  Backer,  and  P.  Scheunders,  â€œNoise-resistant  wavelet-
based Bayesian fusion of multispectral and hyperspectral images,â€ IEEE 
Trans. Geosci. Remote Sens., vol. 47, no. 11, pp. 3834â€“3843, Nov. 2009. 
[20]  R. Kawakami, J. Wright, Y.-W. Tai, Y. Matsushita, M. Ben-Ezra, and K. 
Ikeuchi, 
via  matrix 
factorization,â€  in  Proc.  IEEE  Conf.  Comput.  Vis.  Pattern  Recognit. 
(CVPR), Jun. 2011, pp. 2329â€“2336. 

â€œHigh-resolution 

hyperspectral 

imaging 

[21]  E. Wycoff, T.-H. Chan, K. Jia, W.-K. Ma, and Y. Ma, â€œA non-negative 
sparse promoting algorithm for high resolution hyperspectral imaging,â€ in 
Proc.  IEEE  Int.  Conf.  Acoustics,  Speech  Signal  Process.,  Vancouver, 
Canada, May 2013, pp. 1409â€“1413. 

[22]  K. Cawse-Nicholson, S. Damelin, A. Robin, and M. Sears, â€œDetermining 
the  intrinsic  dimension  of  a  hyperspectral  image  using  random  matrix 
theory,â€ IEEE Trans. Image Process., vol. 22, no. 4, pp. 1301â€“1310, Apr. 
2013. 

[23]  J.  Bioucas-Dias  and 

subspace 
identification,â€  IEEE  Trans.  Geosci.  Remote  Sens.,  vol.  46,  no.  8,  pp. 
2435â€“2445, Aug. 2008. 

J.  Nascimento,  â€œHyperspectral 

[24]  D. Landgrebe, â€œHyperspectral image data analysis,â€ IEEE Signal Process. 

Mag., vol. 19, no. 1, pp. 17â€“28, Jan. 2002. 

[25]  R. Zurita-Milla, J. G. P. W. Clevers, and M. E. Schaepman, â€œUnmixing-
based  landsat  TM  and  MERIS  FR  data  fusion,â€  IEEE  Geosci.  Remote 
Sens. Lett., vol. 5, no. 3, pp. 453â€“457, Jul. 2008. 

[26]  L. Rudin, S. Osher, and E. Fatemi, â€œNonlinear total variation based noise 
removal algorithms,â€ Physica D: Nonlinear Phenomena, vol. 60, pp. 259â€“
268, Nov. 1992. 

[27]  A. Beck and M. Teboulle, â€œFast gradient-based algorithms for constrained 
total  variation  image  denoising and deblurring  problems,â€  IEEE  Trans. 
Image Process., vol. 18, no. 11, pp. 2419â€“2434, Nov. 2009. 

[28]  X.  Bresson  and  T.  Chan,  â€œFast  dual  minimization  of  the  vectorial  total 
variation  norm  and  applications  to  color  image  processing,â€  Inverse 
Problems and Imaging, vol. 2, no. 4, pp. 455â€“484, Nov. 2008. 

[29]  M. SimÃµes, J. Bioucas-Dias, L. B. Almeida, and J. Chanussot, â€œA convex 
formulation for hyperspectral image superresolution via subspace-based 
regularization,â€  IEEE  Trans.  Geosci.  Remote  Sens.,  vol.  53,  no.  6,  pp. 
3373â€“3388, Jun. 2015. 

[30]  X.  He,  L.  Condat,  J.  Bioucas-Dias,  J.  Chanussot,  and  J.  Xia,  â€œA  new 
pansharpening method based on spatial and spectral sparsity priors,â€ IEEE 
Trans. Image Process., vol. 23, no. 9, pp. 4160â€“4174, Sep. 2014. 

[31]  F. Palsson, J. R. Sveinsson, and M. O. Ulfarsson, â€œA new pansharpening 
algorithm based on total variation,â€ IEEE Geosci. Remote Sens. Lett., vol. 
11, no. 1, pp. 318-322, Jan. 2014. 

[32]  Q. Wei, J. Bioucas-Dias, N. Dobigeon, and J. Tourneret, â€œHyperspectral 
and multispectral image fusion based on a sparse representation,â€  IEEE 
Trans. Geosci. Remote Sens., vol. 53, no. 7, pp. 3658â€“3668, Jul. 2015. 

[33]  W. Dong et al., â€œHyperspectral image super-resolution via non-negative 
structured  sparse representation,â€  IEEE Trans.  Image Process., vol. 25, 
no. 5, pp. 2337â€“2352, May 2016. 

[34]  C.  Grohnfeldt,  X.  X.  Zhu,  and  R.  Bamler,  â€œJointly  sparse  fusion  of 
hyperspectral  and  multispectral  imagery,â€  in  Proc.  IEEE  Int.  Geosci. 
Remote Sens. Symp., Melbourne, Australia, Jul. 2013, pp. 4090â€“4093. 
[35]  N. Akhtar, F. Shafait, and A. Mian, â€œBayesian sparse representation for 
hyperspectral  image  super  resolution,â€  in  Proc.  IEEE  Conf.  Comput. 
Vision  Pattern  Recognition,  Boston,  MA,  USA,  Jun.  2015,  pp.  3631â€“
3640. 

[36]  Q. Wei, N. Dobigeon, and J.-Y. Tourneret, â€œBayesian fusion of multiband 
images,â€  IEEE  J.  Sel.  Topics  Signal  Process.,  vol.  9,  no.  6,  pp.  1117â€“
1127, Sep. 2015. 

[37]  R.  C.  Hardie,  M.  T.  Eismann,  and  G.  L.  Wilson,  â€œMAP  estimation  for 
hyperspectral image resolution enhancement using an auxiliary sensor,â€ 
IEEE Trans. Image Process., vol. 13, no. 9, pp. 1174â€“1184, Sep. 2004. 

[38]  Y.  Zhang,  A.  Duijster,  and  P.  Scheunders,  â€œA  Bayesian  restoration 
approach for hyperspectral images,â€ IEEE Trans. Geosci. Remote Sens., 
vol. 50, no. 9, pp. 3453â€“3462, Sep. 2012. 

[39]  M. A. Veganzones, M. SimÃµes, G. Licciardi, N. Yokoya, J. M. Bioucas- 
Dias,  and  J.  Chanussot,  â€œHyperspectral  super-resolution  of  locally  low 
rank images from complementary multisource data,â€ IEEE Trans. Image 
Process., vol. 25, no. 1, pp. 274â€“288, Jan. 2016. 

[40]  O.  Berne,  A.  Helens,  P.  Pilleri,  and  C.  Joblin,  â€œNon-negative  matrix 
factorization pansharpening of hyperspectral data: An application to mid-
infrared  astronomy,â€  in  Proc.  IEEE  GRSS  Workshop  Hyperspectral 

10 

Image  Signal  Process.:  Evolution  in  Remote  Sens.,  Reykjavik,  Iceland, 
Jun. 2010, pp. 1â€“4. 

[41]  N.  Yokoya,  T.  Yairi,  and  A.  Iwasaki,  â€œCoupled  nonnegative  matrix 
factorization unmixing  for  hyperspectral and multispectral data  fusion,â€ 
IEEE Trans. Geosci. Remote Sens., vol. 50, no. 2, pp. 528â€“537, Feb. 2012. 
[42]  C.  Lanaras,  E.  Baltsavias,  and  K.  Schindler,  â€œHyperspectral 
superresolution  by  coupled  spectral  unmixing,â€  in  Proc.  IEEE  ICCV, 
Santiago, Chile, Dec. 2015, pp. 3586â€“3594. 

[43]  Q. Wei, J. Bioucas-Dias, N. Dobigeon, J.-Y. Tourneret, M. Chen, and S. 
Godsill,  â€œMultiband  image  fusion  based  on  spectral  unmixing,â€  IEEE 
Trans. Geosci. Remote Sens., vol. 54, no. 12, pp. 7236- 7249, Dec. 2016. 
[44]  C.  Kwan,  B.  Budavari,  A.  C.  Bovik,  and  G.  Marchisio,  â€œBlind  quality 
assessment of fused WorldView-3 images by using the combinations of 
pansharpening  and  hypersharpening  paradigms,â€  IEEE  Geosci.  Remote 
Sens. Lett., vol. 14, no. 10, pp. 1835â€“1839, Oct. 2017. 

[45]  N. Yokoya, T. Yairi, and A. Iwasaki, â€œHyperspectral, multispectral, and 
panchromatic  data  fusion  based  on  coupled  non-negative  matrix 
factorization,â€  in  Proc.  IEEE  GRSS  Workshop  Hyperspectral  Image 
Signal Process.: Evolution in Remote Sens., Lisbon, Portugal, Jun. 2011, 
pp. 1â€“4. 

[46]  M.  Afonso,  J.  M.  Bioucas-Dias,  and  M.  Figueiredo,  â€œAn  augmented 
Lagrangian  approach  to  the  constrained  optimization  formulation  of 
imaging inverse problems,â€ IEEE Trans. Image Process., vol. 20, no. 3, 
pp. 681â€“95, Mar. 2011. 

[47]  J.  Eckstein  and  D.  P.  Bertsekas,  â€œOn  the  Douglasâ€“Rachford  splitting 
method  and  the  proximal  point  algorithm  for  maximal  monotone 
operators,â€ Math. Program., vol. 55, nos. 1â€“3, pp. 293â€“318, 1992. 
[48]  D. Gabay and B. Mercier, â€œA dual algorithm for the solution of nonlinear 
variational  problems  via  finite-element  approximation,â€  Comput.  Math. 
Appl., vol. 2, no. 1, pp. 17â€“40, 1976. 

[49]  R. Glowinski and  A. Marroco, â€œSur lâ€™approximation, par  Ã©lÃ©ments  finis 
dâ€™ordre  un,  et  la  rÃ©solution,  par  pÃ©nalisation-dualitÃ©  dâ€™une  classe  de 
problÃ¨mes  de  Dirichlet  non  linÃ©aires,â€  Revue  franÃ§aise  dâ€™automatique, 
informatique, recherche opÃ©rationnelle. Analyse numÃ©rique, vol. 9, no. 2, 
pp. 41â€“76, 1975. 

[50]  S.  Boyd,  N.  Parikh,  E.  Chu,  B.  Peleato,  and  J.  Eckstein,  â€œDistributed 
optimization and statistical learning via the alternating direction method 
of multipliers,â€ Foundation and Trends in Machine Learning, vol. 3, no. 
1, pp. 1â€“122, 2011. 

[51]  E.  Esser,  â€œApplications  of  Lagrangian-based  alternating  direction 
methods  and  connections  to  split-Bregman,â€  Center  Comput.  Appl. 
Math., Univ. California, Los Angeles, Tech. Rep. 09-31, 2009. 

[52]  B.-C. Gao,  M. J.  Montes, C.  O. Davis, and  A.  F. Goetz, â€œAtmospheric 
correction algorithms  for  hyperspectral remote  sensing  data of land and 
ocean,â€ Remote Sensing Environ., vol. 113, pp. S17â€“S24, Sept. 2009. 
[53]  A. Zare and K. C. Ho, â€œEndmember variability in hyperspectral analysis,â€ 
IEEE Signal Process. Mag., vol. 31, no. 1, pp. 95â€“104, Jan. 2014. 
[54]  N.  Dobigeon,  J.-Y.  Tourneret,  C.  Richard,  J.  C.  M.  Bermudez,  S. 
McLaughlin,  and  A.  O.  Hero,  â€œNonlinear  unmixing  of  hyperspectral 
images: Models and algorithms,â€ IEEE Signal Process. Mag., vol. 31, no. 
1, pp. 89â€“94, Jan. 2014. 

[55]  T. G. Stockham, Jr., â€œHigh-speed convolution and correlation,â€ in  Proc. 
ACM Spring Joint Computer Conf., New York, NY, USA, Apr. 1966, pp. 
229-233. 

[56]  D.  Donoho  and  I.  Johnstone,  â€œAdapting  to  unknown  smoothness  via 
wavelet shrinkage,â€ Journal of the American Statistical Association, vol. 
90, no. 432, pp. 1200â€“1224, Dec. 1995. 

[57]  P.  Combettes  and  J.-C.  Pesquet,  â€œProximal  splitting  methods  in  signal 
processing,â€ in Fixed-Point Algorithms for Inverse Problems in Science 
and Engineering, New York, NY, USA: Springer-Verlag, 2011, pp. 185â€“
212. 

[58]  L.  Condat,  â€œFast  projection  onto  the  simplex  and  the  â„“1  ball,â€ 
Mathematical Programming, Series A, vol. 158, pp. 575â€“585, 2016. 
[59]  M.  Baumgardner,  L.  Biehl,  and  D.  Landgrebe,  â€œ220  band  AVIRIS 
hyperspectral  image  data  set:  June  12,  1992  Indian  pine  test  site  3,â€ 
Purdue University Research Repository, 2015. doi:10.4231/R7RX991C 

[60]  https://github.com/Reza219/Multiple-multiband-image-fusion 
[61]  A.  Garzelli,  F.  Nencini,  and  L.  Capobianco,  â€œOptimal  MMSE  pan 
sharpening  of  very  high  resolution  multispectral  images,â€  IEEE  Trans. 
Geosci. Remote Sens., vol. 46, no. 1, pp. 228â€“236, Jan. 2008. 

[62]  B.  Aiazzi,  L.  Alparone,  S.  Baronti,  A.  Garzelli,  and  M.  Selva,  â€œMTF-
tailored  multiscale  fusion  of  high-resolution  MS  and  Pan  imagery,â€ 
Photogrammetric  Engineering  and  Remote  Sensing,  vol.  72,  no.  5,  pp. 
591â€“596, May 2006. 

11 

 

 

[63]  J. Lee and C. Lee, â€œFast and  efficient panchromatic  sharpening,â€  IEEE 
Trans. Geosci. Remote Sens., vol. 48, no. 1, pp. 155â€“163, Jan. 2010. 
[64]  B. Aiazzi, L. Alparone, S. Baronti, A. Garzelli, and M. Selva, â€œAn MTF-
based  spectral  distortion  minimizing  model  for  pan-sharpening  of  very 
high  resolution  multispectral  images  of  urban  areas,â€  in  Proc.  2nd 
GRSS/ISPRS Joint Workshop Remote Sens. Data Fusion URBAN Areas, 
Berlin, Germany, May 2003, pp. 90â€“94. 

[65]  Q. Wei, N. Dobigeon, J.-Y. Tourneret, J. M. Bioucas-Dias, and S. Godsill, 
â€œR-FUSE:  Robust  fast  fusion  of  multiband  images  based  on  solving  a 
Sylvester equation,â€ IEEE Signal Process. Lett., vol. 23, no. 11, pp. 1632-
1636, Nov. 2016. 

[66]  Q.  Wei,  N.  Dobigeon,  and  J.-Y.  Tourneret,  â€œFast  fusion  of  multi-band 
images  based  on  solving  a  Sylvester  equation,â€  IEEE  Trans.  Image 
Process., vol. 24, no. 11, pp. 4109â€“4121, Nov. 2015. 

[67]  N.  Yokoya,  C.  Grohnfeldt,  and  J.  Chanussot,  â€œHyperspectral  and 
multispectral data fusion: A comparative review of the recent literature,â€ 
IEEE Geosci. Remote Sens. Mag., vol. 5, no. 2, pp. 29â€“56, Jun. 2017. 
[68]  L. Wald, â€œQuality of high resolution synthesised images: Is there a simple 
criterion?â€ in Proc. Int. Conf. Fusion Earth Data, Nice, France, Jan. 2000, 
pp. 99â€“103. 

[69]  F.  A.  Kruse  et  al.,  â€œThe  spectral  image  processing  system  (SIPS): 
Interactive  visualization  and  analysis  of  imaging  spectrometer  data,â€ 
Remote Sens. Environ., vol. 44, no. 2â€“3, pp. 145â€“163, Mayâ€“June 1993. 

[70]  A.  Garzelli  and  F.  Nencini,  â€œHypercomplex  quality  assessment  of 
multi/hyperspectral images,â€ IEEE Geosci. Remote Sens. Lett., vol. 6, no. 
4, pp. 662â€“665, Oct. 2009. 

 

 

[71]  Z. Wang and A. C. Bovik, â€œA universal image quality index,â€ IEEE Signal 

Process. Lett., vol. 9, no. 3, pp. 81â€“84, Mar. 2002. 

[72]  L.  Alparone,  S. Baronti,  A. Garzelli, and F. Nencini, â€œA  global  quality 
measurement  of  pan-sharpened  multispectral  imagery,â€  IEEE  Geosci. 
Remote Sens. Lett., vol. 1, no. 4, pp. 313â€“317, Oct. 2004. 

[73]  J. Nascimento and J. Bioucas-Dias, â€œVertex component analysis: A fast 
algorithm  to  unmix  hyperspectral  data,â€  IEEE  Trans.  Geosci.  Remote 
Sens., vol. 43, no. 4, pp. 898â€“910, Apr. 2005. 

[74]  J. Bioucas-Dias and M. Figueiredo, â€œAlternating direction algorithms for 
constrained sparse regression: Application to hyperspectral unmixing,â€ in 
Proc.  IEEE  GRSS  Workshop  Hyperspectral  Image  Signal  Process.: 
Evolution in Remote Sens., Reykjavik, Iceland, Jun. 2010, vol. 1, pp. 1â€“4. 
[75]  D.  Donoho  and  I.  Johnstone,  â€œAdapting  to  unknown  smoothness  via 
wavelet  shrinkage,â€  J.  Amer.  Stat.  Assoc.,  vol.  90,  no.  432,  pp.  1200â€“
1224, Dec. 1995. 

[76]  G. Golub,  M. Heath, and G.  Wahba, â€œGeneralized  cross-validation as a 
method for choosing a good ridge parameter,â€ Technometrics, vol. 21, no. 
2, pp. 215â€“223, May 1979. 

[77]  L. Alparone, S. Baronti, B. Aiazzi, and A. Garzelli, â€œSpatial methods for 
multispectral  pansharpening:  Multiresolution  analysis  demystified,â€ 
IEEE Trans. Geosci. Remote Sens., vol. 54, no. 5, pp. 2563â€“2576, May 
2016. 

[78]  L.  Wald, T. Ranchin, and M.  Mangolini, â€œFusion of  satellite images of 
different  spatial  resolutions:  Assessing  the  quality  of  resulting  image,â€ 
IEEE Trans. Geosci. Remote Sens., vol. 43, pp. 1391â€“1402, 2005. 

 

12 

Table I 
THE PROPOSED ALGORITHM 

initialize 
     ğ„ â† VCA(ğ˜ğ‘™)   % if ğ„ is not known and ğ˜ğ‘™ has full spectral resolution 
     ğ€(0) â†  upscale and interpolate the output of SUnSAL(ğ˜ğ‘™, ğ„) 
     for ğ‘˜ = 1, â€¦ , ğ¾ 
(0) = ğ€(0) 
          ğ”ğ‘˜
(ğ‘›) = ğŸğ‘€Ã—ğ‘ 
          ğ…ğ‘˜
     ğ•(0) = ğ€(0), ğ–(0) = ğ€(0) 
     ğ†(0) = ğŸğ‘€Ã—ğ‘, ğ‡(0) = ğŸğ‘€Ã—ğ‘ 
for ğ‘› = 1,2, â€¦   % until a convergence criterion is met or a given maximum number of iterations is reached 

     ğ€(ğ‘›) = [âˆ‘(ğ”ğ‘˜

(ğ‘›âˆ’1) + ğ…ğ‘˜

(ğ‘›âˆ’1))ğğ‘˜

âŠ¤

  + ğ1

(ğ‘›âˆ’1)ğƒâ„

âŠ¤ + ğ2

(ğ‘›âˆ’1)ğƒğ‘£

âŠ¤
âŠ¤ + ğ–(ğ‘›âˆ’1) + ğ‡(ğ‘›âˆ’1)] (âˆ‘ ğğ‘˜ğğ‘˜

+ ğƒâ„ğƒâ„

âŠ¤ + ğƒğ‘£ğƒğ‘£

âŠ¤ + ğˆğ‘)

 

ğ¾

ğ‘˜=1

âˆ’1

âˆ’1ğ‘ğ‘˜ğ„ + ğœ‡ğˆğ‘)âˆ’1[ğ„âŠ¤ğ‘ğ‘˜

âŠ¤ğš²ğ‘˜

âˆ’1ğ˜ğ‘˜ğ’ğ‘˜

âŠ¤ + ğœ‡(ğ€(ğ‘›)ğğ‘˜ âˆ’ ğ…ğ‘˜

(ğ‘›âˆ’1))ğŒğ‘˜] + (ğ€(ğ‘›)ğğ‘˜ âˆ’ ğ…ğ‘˜

(ğ‘›âˆ’1))(ğˆğ‘ âˆ’ ğŒğ‘˜) 

(ğ‘›âˆ’1)
(ğ‘›âˆ’1)] = ğ•(ğ‘›âˆ’1) + ğ†(ğ‘›âˆ’1) 

     [

ğ1
ğ2

ğ¾

ğ‘˜=1

     for ğ‘˜ = 1, â€¦ , ğ¾ 
(ğ‘›) = (ğ„âŠ¤ğ‘ğ‘˜
          ğ”ğ‘˜
     ğ™(ğ‘›) = âˆ‡ğ€(ğ‘›) âˆ’ ğ†(ğ‘›âˆ’1) 
     for ğ‘— = 1, â€¦ , ğ‘ 

âŠ¤ğš²ğ‘˜

          ğ¯ğ‘—

(ğ‘›) =

max {â€–ğ³ğ‘—

âˆ’

(ğ‘›)â€–
2
(ğ‘›)â€–
2

â€–ğ³ğ‘—

Î±
ğœ‡

, 0}

(ğ‘›) 
ğ³ğ‘—

     ğ–(ğ‘›) = Î ğ’®{ğ€(ğ‘›) âˆ’ ğ‡(ğ‘›âˆ’1)} 
     for ğ‘˜ = 1, â€¦ , ğ¾ 

(ğ‘›)) 

(ğ‘›) = ğ…ğ‘˜

(ğ‘›âˆ’1) âˆ’ (ğ€(ğ‘›)ğğ‘˜ âˆ’ ğ”ğ‘˜
          ğ…ğ‘˜
     ğ†(ğ‘›) = ğ†(ğ‘›âˆ’1) âˆ’ (âˆ‡ğ€(ğ‘›) âˆ’ ğ•(ğ‘›)) 
     ğ‡(ğ‘›) = ğ‡(ğ‘›âˆ’1) âˆ’ (ğ€(ğ‘›) âˆ’ ğ–(ğ‘›)) 
calculate the fused image 
     ğ—Ì‚ = ğ„ğ€(ğ‘›) 

 
Table II 
THE SPATIAL AND SPECTRAL DIMENSIONS OF THE CONSIDERED REFERENCE HYPERSPECTRAL IMAGES AND THE VALUE OF THE REGULARIZATION PARAMETER 
USED IN THE PROPOSED ALGORITHM WITH EACH IMAGE 

image 

Botswana 
Indian Pines 
Washington DC Mall 
Moffett Field 
Kennedy Space Center 

no. of rows 
400 
400 
400 
480 
500 

no. of columns 
240 
400 
300 
320 
400 

no. of bands 
145 
200 
191 
176 
176 

Î± 
5 
7 
5 
22 
28 

 

 

13 

Fig. 1.  The spectral responses of the Landsat 8 multispectral and panchromatic sensors. 

 

Fig. 2.  The modulation transfer function (normalized spatial-frequency response) of the used 2D Gaussian blur filters in both spatial dimensions. The solid curve 
corresponds to the filter used to generate the multispectral images and the dashed curve corresponds to the filter used to generate the hyperspectral images. 
 
 

 

Fig. 3.  The values of the performance metrics versus the regularization parameter Î± for the experiments with Botswana and Washington DC Mall images. The 
left ğ‘¦-axis corresponds to ERGAS and SAM and the right ğ‘¦-axis to ğ‘„2ğ‘›. 

 

 

Washington DC Mall 

Botswana 

 

 

 
 

 
 

Table III 
THE VALUES OF THE PERFORMANCE METRICS FOR ASSESSING THE FUSION QUALITY AS WELL AS THE RUNTIMES OF THE CONSIDERED ALGORITHMS FOR 
EXPERIMENTS WITH DIFFERENT IMAGES 

14 

fusion 

algorithm(s) 

Pan + MS + HS 

Pan + HS 

Pan + (MS + HS) 

(Pan + MS) + HS 

proposed 
HySure 
R-FUSE-TV 
HySure 
R-FUSE-TV 
BDSD & HySure 
BDSD & R-FUSE-TV 
MTF-GLP-HPM & HySure 
MTF-GLP-HPM & R-FUSE-TV 

spectrum of panchromatic image 
SAM (Â°) 
ERGAS 
1.355 
0.900 
1.975 
1.273 
1.974 
1.272 
1.721 
1.256 
1.734 
1.265 
1.971 
1.393 
1.977 
1.392 
2.120 
1.441 
2.124 
1.440 

ğ‘„2ğ‘› 
0.980 
0.967 
0.967 
0.962 
0.961 
0.955 
0.956 
0.957 
0.957 

entire spectrum 
SAM (Â°) 
1.575 
2.435 
2.436 
2.101 
2.113 
2.359 
2.365 
2.442 
2.446 

ERGAS 
1.637 
1.839 
1.840 
1.992 
2.002 
2.458 
2.461 
2.181 
2.185 

ğ‘„2ğ‘› 
0.956 
0.946 
0.946 
0.937 
0.937 
0.912 
0.912 
0.931 
0.931 

Botswana 

Indian Pines 

fusion 

algorithm(s) 

Pan + MS + HS 

Pan + HS 

Pan + (MS + HS) 

(Pan + MS) + HS 

proposed 
HySure 
R-FUSE-TV 
HySure 
R-FUSE-TV 
BDSD & HySure 
BDSD & R-FUSE-TV 
MTF-GLP-HPM & HySure 
MTF-GLP-HPM & R-FUSE-TV 

spectrum of panchromatic image 
SAM (Â°) 
ERGAS 
0.293 
0.304 
0.547 
0.420 
0.555 
0.425 
0.641 
0.656 
0.642 
0.695 
0.517 
0.538 
0.520 
0.539 
0.563 
0.566 
0.567 
0.567 

ğ‘„2ğ‘› 
0.990 
0.986 
0.986 
0.961 
0.953 
0.972 
0.972 
0.972 
0.972 

entire spectrum 
SAM (Â°) 
0.761 
1.108 
1.113 
1.117 
1.120 
1.183 
1.182 
1.268 
1.270 

ERGAS 
0.500 
0.813 
0.813 
0.834 
0.875 
0.803 
0.794 
0.959 
0.947 

ğ‘„2ğ‘› 
0.969 
0.632 
0.632 
0.594 
0.573 
0.670 
0.674 
0.626 
0.628 

Washington DC Mall 

fusion 

algorithm(s) 

Pan + MS + HS 

Pan + HS 

Pan + (MS + HS) 

(Pan + MS) + HS 

proposed 
HySure 
R-FUSE-TV 
HySure 
R-FUSE-TV 
BDSD & HySure 
BDSD & R-FUSE-TV 
MTF-GLP-HPM & HySure 
MTF-GLP-HPM & R-FUSE-TV 

spectrum of panchromatic image 
SAM (Â°) 
ERGAS 
1.116 
0.731 
2.047 
1.171 
2.042 
1.171 
1.718 
0.937 
1.738 
1.204 
2.039 
1.114 
2.060 
1.104 
1.870 
1.308 
1.884 
1.298 

ğ‘„2ğ‘› 
0.997 
0.992 
0.992 
0.994 
0.991 
0.992 
0.992 
0.991 
0.991 

entire spectrum 
SAM (Â°) 
2.795 
4.539 
4.537 
3.592 
3.664 
5.048 
5.033 
5.147 
5.114 

ERGAS 
2.484 
3.822 
3.832 
3.233 
3.270 
4.174 
4.251 
4.380 
4.440 

ğ‘„2ğ‘› 
0.970 
0.930 
0.930 
0.949 
0.947 
0.918 
0.916 
0.911 
0.910 

Moffett Field 

fusion 

algorithm(s) 

Pan + MS + HS 

Pan + HS 

Pan + (MS + HS) 

(Pan + MS) + HS 

proposed 
HySure 
R-FUSE-TV 
HySure 
R-FUSE-TV 
BDSD & HySure 
BDSD & R-FUSE-TV 
MTF-GLP-HPM & HySure 
MTF-GLP-HPM & R-FUSE-TV 

spectrum of panchromatic image 
SAM (Â°) 
ERGAS 
0.786 
0.572 
1.151 
0.902 
1.152 
0.914 
1.004 
0.826 
1.014 
0.964 
1.135 
1.061 
1.134 
1.058 
1.122 
1.396 
1.123 
1.396 

ğ‘„2ğ‘› 
0.992 
0.985 
0.984 
0.986 
0.977 
0.980 
0.980 
0.968 
0.969 

entire spectrum 
SAM (Â°) 
3.148 
4.233 
4.210 
3.603 
3.670 
4.065 
4.039 
4.384 
4.360 

ERGAS 
4.232 
6.507 
6.416 
5.078 
5.100 
5.325 
5.244 
5.924 
5.835 

ğ‘„2ğ‘› 
0.885 
0.823 
0.827 
0.868 
0.845 
0.829 
0.834 
0.824 
0.830 

Kennedy Space Center 

fusion 

algorithm(s) 

Pan + MS + HS 

Pan + HS 

Pan + (MS + HS) 

(Pan + MS) + HS 

proposed 
HySure 
R-FUSE-TV 
HySure 
R-FUSE-TV 
BDSD & HySure 
BDSD & R-FUSE-TV 
MTF-GLP-HPM & HySure 
MTF-GLP-HPM & R-FUSE-TV 

 

spectrum of panchromatic image 
SAM (Â°) 
ERGAS 
1.628 
1.024 
2.426 
1.451 
2.496 
1.518 
2.203 
1.462 
2.343 
1.875 
2.594 
1.738 
2.547 
1.691 
3.250 
6.801 
3.264 
8.143 

ğ‘„2ğ‘› 
0.984 
0.979 
0.974 
0.967 
0.939 
0.949 
0.953 
0.912 
0.914 

entire spectrum 
SAM (Â°) 
3.211 
3.995 
3.795 
3.546 
4.155 
4.824 
4.584 
5.183 
5.197 

ERGAS 
2.468 
3.544 
3.680 
2.851 
2.986 
3.727 
3.534 
9.532 
11.130 

ğ‘„2ğ‘› 
0.909 
0.890 
0.886 
0.909 
0.878 
0.850 
0.865 
0.805 
0.816 

time (s) 

47.01 
61.20 
61.17 
78.28 
79.44 
62.58 
62.10 
62.78 
62.20 

time (s) 

80.21 
106.75 
106.47 
134.79 
134.32 
108.33 
107.34 
108.48 
107.51 

time (s) 

59.52 
79.02 
78.38 
99.74 
100.53 
79.68 
78.41 
79.28 
78.13 

time (s) 

77.37 
107.73 
106.20 
134.78 
135.20 
108.91 
106.12 
108.98 
106.28 

time (s) 

99.94 
138.16 
134.97 
172.18 
172.25 
138.66 
135.74 
138.60 
135.58 

 

 

15 

 

 

 

Botswana 

Indian Pines 

Washington DC Mall 

Moffett Field 

 

 

 

 

 

Kennedy Space Center 

Fig. 4.  The sorted per-pixel NRMSE of different algorithms measured only on the spectrum of the panchromatic image in experiments with different images. 

 

 

16 

 

 

 

Botswana 

Indian Pines 

Washington DC Mall 

Moffett Field 

 

 

 

 

 

Kennedy Space Center 

Fig. 5.  The sorted per-pixel NRMSE of different algorithms measured on the entire spectrum in experiments with different images. 

 

 

 

Botswana 

 

 

 

 

 

 

Indian Pines 

 

 

Washington DC Mall 

Moffett Field 

 

 

 

 

Kennedy Space Center 

 

 

 

 

 

 

hyper-
spectral 

 

 

 

 

 

 

 

 

panchromatic 

multispectral 

reference 

proposed 

Pan + (MS + HS) by HySure 

Fig. 6.  The panchromatic, multispectral, and hyperspectral images that are fused together, the reference hyperspectral image, and the fused images produced by 
the proposed algorithm and the Pan + (MS + HS) method using the HySure algorithm. 

17 

 

 

 

 

