Identiﬁcation In Missing Data Models Represented By
Directed Acyclic Graphs

9
1
0
2
 
n
u
J
 
9
2

 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
4
2
0
0
.
7
0
9
1
:
v
i
X
r
a

Rohit Bhattacharya†∗, Razieh Nabi†∗, Ilya Shpitser†, James M. Robins‡
† Department of Computer Science, Johns Hopkins University, Baltimore, MD
‡ Department of Epidemiology, Harvard T. H. Chan School of Public Health, Boston, MA
∗ Equal contribution
(rbhattacharya@, rnabi@, ilyas@cs.)jhu.edu, robins@hsph.harvard.edu

Abstract

Missing data is a pervasive problem in data
analyses,
resulting in datasets that contain
censored realizations of a target distribution.
Many approaches to inference on the target
distribution using censored observed data, rely
on missing data models represented as a factor-
ization with respect to a directed acyclic graph.
In this paper we consider the identiﬁability of
the target distribution within this class of mod-
els, and show that the most general identiﬁca-
tion strategies proposed so far retain a signiﬁ-
cant gap in that they fail to identify a wide class
of identiﬁable distributions. To address this
gap, we propose a new algorithm that signif-
icantly generalizes the types of manipulations
used in the ID algorithm [14, 16], developed
in the context of causal inference, in order to
obtain identiﬁcation.

1 INTRODUCTION

Missing data is ubiquitous in applied data analyses re-
sulting in target distributions that are systematically cen-
sored by a missingness process. A common modeling
approach assumes data entries are censored in a way that
does not depend on the underlying missing data, known
as the missing completely at random (MCAR) model,
or only depends on observed values in the data, known
as the missing at random (MAR) model. These sim-
ple models are insufﬁcient however, in problems where
missingness status may depend on underlying values that
are themselves censored. This type of missingness is
known as missing not at random (MNAR) [9, 10, 17].

While the underlying target distribution is often not iden-
tiﬁed from observed data under MNAR, there exist iden-
tiﬁed MNAR models. These include the permutation

model [9], the discrete choice model [15], the no self-
censoring model [11, 12], the block-sequential MAR
model [18], and others. Restrictions deﬁning many, but
not all, of these models may be represented by a factor-
ization of the full data law (consisting of both the target
distribution and the missingness process) with respect to
a directed acyclic graph (DAG).

The problem of identiﬁcation of the target distribution
from the observed distribution in missing data DAG
models bears many similarities to the problem of identi-
ﬁcation of interventional distributions from the observed
distribution in causal DAG models with hidden variables.
This observation prompted recent work [3, 4, 13] on
adapting identiﬁcation methods from causal inference to
identifying target distributions in missing data models.

In this paper we show that the most general currently
known methods for identiﬁcation in missing data DAG
models retain a signiﬁcant gap, in the sense that they fail
to identify the target distribution in many models where
it is identiﬁed. We show that methods used to obtain
a complete characterization of identiﬁcation of interven-
tional distributions, via the ID algorithm [14, 16], or their
simple generalizations [3, 4, 13], are insufﬁcient on their
own for obtaining a similar characterization for missing
data problems. We describe, via a set of examples, that
in order to be complete, an identiﬁcation algorithm for
missing data must recursively simplify the problem by
removing sets of variables, rather than single variables,
and these must be removed according to a partial order,
rather than a total order. Furthermore, the algorithm must
be able to handle subproblems where selection bias or
hidden variables, or both, are present even if these com-
plications are missing in the original problem. We de-
velop a new general algorithm that exploits these obser-
vations and signiﬁcantly narrows the identiﬁability gap
in existing methods. Finally, we show that in certain
classes of missing data DAG models, our algorithm takes
on a particularly simple formulation to identify the target
distribution.

Our paper is organized as follows. In section 2, we in-
troduce the necessary preliminaries from the graphical
In section 3 we introduce
causal inference literature.
missing data models represented by DAGs.
In section
4, we illustrate, via examples, that existing identiﬁcation
strategies based on simple generalizations of causal in-
ference methods are not sufﬁcient for identiﬁcation in
general, and describe generalizations needed for identiﬁ-
cation in these examples. In section 5, we give a general
identiﬁcation algorithm which incorporates techniques
needed to obtain identiﬁcation in the examples we de-
scribe. Section 6 contains our conclusions. We defer
longer proofs to the supplement in the interests of space.

2 PRELIMINARIES

Many techniques useful for identiﬁcation in missing data
contexts were ﬁrst derived in causal inference. Causal in-
ference is concerned with expressing counterfactual dis-
tributions, obtained after the intervention operation, from
the observed data distribution, using constraints embed-
ded in a causal model, often represented by a DAG.
A DAG is a graph G with a vertex set V connected by di-
rected edges such that there are no directed cycles in the
graph. A statistical model of a DAG G is the set of distri-
butions p(V) such that p(V) =
QV ∈V p(V | paG(V )),
where paG(V ) are the set of parents of V in G. Causal
models of a DAG are also sets of distributions, but on
counterfactual random variables. Given Y ∈ V and
A ⊆ V \ {Y }, a counterfactual variable, or potential
outcome, written as Y (a), represents the value of Y in
a hypothetical situation where A were set to values a
by an intervention operation [6]. Given a set Y, deﬁne
Y(a) ≡ {Y}(a) ≡ {Y (a) | Y ∈ Y}. The distribution
p(Y(a)) is sometimes written as p(Y|do(a)) [6].
A causal parameter is said to be identiﬁed in a causal
model if it is a function of the observed data distribu-
tion p(V). Otherwise the parameter is said to be non-
identiﬁed. In all causal models of a DAG G that are typi-
cally used, all interventional distributions p({V\A}(a))
are identiﬁed by the g-formula [8]:

p({V \ A}(a)) =

p(V | paG (V ))

A=a .

(1)

Y
V ∈V\A

(cid:12)
(cid:12)

If a causal model contains hidden variables, only data
on the observed marginal distribution is available.
In
this case, not every interventional distribution is identi-
ﬁed, and identiﬁcation theory becomes more complex.
A general algorithm for identiﬁcation of causal effects
in this setting was given in [16], and proven complete in
[14, 1]. Here, we describe a simple reformulation of this
algorithm as a truncated nested factorization analogous
to the g-formula, phrased in terms of kernels and mixed
graphs recursively deﬁned via a ﬁxing operator [7]. As

we will see, many of the techniques developed for iden-
tiﬁcation in the presence of hidden variables will need to
be employed (and generalized) for missing data, even if
no variables are completely hidden.

We describe acyclic directed mixed graphs (ADMGs)
obtained from a hidden variable DAG by a latent projec-
tion operation in section 2.1, and a nested factorization
associated with these ADMGs in section 2.2. This fac-
torization is formulated in terms of conditional ADMGs
and kernels (described in section 2.2.1), via the ﬁxing op-
erator (described in section 2.2.2). The truncated nested
factorization that yields all identiﬁable functions for in-
terventional distributions is described in section 2.3.

As a prelude to the rest of the paper, we introduce the
following notation for some standard genealogic sets of
a graph G with a set of vertices V: parents paG(V ) ≡
{U ∈ V|U → V }, children chG(V ) ≡ {U ∈ V|V →
U }, descendants deG(V ) ≡ {U ∈ V|V → · · · → U },
ancestors anG(V ) ≡ {U ∈ V|U → · · · → V }, and
non-descendants ndG(V ) ≡ V \ deG(V ). A district D
is deﬁned as the maximal set of vertices that are pair-
wise connected by a bidirected path (a path containing
only ↔ edges). We denote the district of V as disG(V ),
and the set of all districts in G as D(G). By conven-
tion, for any V , disG(V ) ∩ de(V ) ∩ anG(V ) = {V }.
Finally, the Markov blanket mbG(V ) ≡ disG(V ) ∪
paG(disG(V )) is deﬁned as the set that gives rise to the
following independence relation through m-separation:
V ⊥⊥ ndG(V ) \ mbG(V )| mbG(V ) [7]. The above def-
initions apply disjunctively to sets of variables S ⊂ V;
e.g. paG(S) = ∪S∈S paG(S).

2.1 LATENT PROJECTION ADMGS

Given a DAG G(V ∪ H), where V are observed and H
are hidden variables, a latent projection G(V) is the fol-
lowing ADMG with a vertex set V. An edge A → B
exists in G(V) if there exists a directed path from A to B
in G(V ∪ H) with all intermediate vertices in H. Sim-
ilarly, an edge A ↔ B exists in G(V) if there exists a
path without consecutive edges → ◦ ← from A to B
with the ﬁrst edge on the path of the form A ← and the
last edge on the path of the form → B, and all interme-
diate vertices on the path in H. Latent projections de-
ﬁne an inﬁnite class of hidden variable DAGs that share
identiﬁcation theory. Thus, identiﬁcation algorithms are
typically deﬁned on latent projections for simplicity.

2.2 NESTED FACTORIZATION

The nested factorization of p(V) with respect to an
ADMG G(V) is deﬁned on kernel objects derived from
p(V) and conditional ADMGs derived from G(V). The

derivations are via a ﬁxing operation, which can be
causally interpreted as a single application of the g-
formula on a single variable (to either a graph or a kernel)
to obtain another graph or another kernel.

2.2.1 Conditional Graphs And Kernels

A conditional acyclic directed mixed graph (CADMG)
G(V, W) is an ADMG in which the nodes are par-
titioned into W, representing ﬁxed variables, and V,
representing random variables. Only outgoing directed
edges may be adjacent to variables in W.
A kernel qV(V|W) is a mapping from values in W to
normalized densities over V [2].
In other words, ker-
nels act like conditional distributions in the sense that
v∈V qV(v|w) = 1, ∀w ∈ W. Conditioning and
P
marginalization in kernels are deﬁned in the usual way.
V\A q(V|W) and
For A ⊆ V, we deﬁne q(A|W) ≡
q(V \ A|A, W) ≡ q(V|W)/q(A|W).

P

2.2.2 Fixability And Fixing

qV(V|W)

A variable V ∈ V in a CADMG G is ﬁxable if deG(V ) ∩
disG(V ) = {V }. In other words, V is ﬁxable if paths
V ↔ · · · ↔ U and V → · · · → U do not both exist in
G for any U ∈ V \ {V }. Given a CADMG G(V, W)
and V ∈ V ﬁxable in G, the ﬁxing operator φV (G)
yields a new CADMG G′(V \ {V }, W ∪ {V }), where
all edges with arrowheads into V are removed, and all
other edges in G are kept. Similarly, given a CADMG
G(V, W), a kernel qV(V|W), and V ∈ V ﬁxable in
G, the ﬁxing operator φV (qV; G) yields a new kernel
V\{V }(V \ {V }|W ∪{V }) ≡
q′
qV(V | ndG (V ),W) . Fixing
is a probabilistic operation in which we divide a kernel
by a conditional kernel. In some cases this operates as a
conditioning operation, in other cases as a marginaliza-
tion operation, and in yet other cases, as neither, depend-
ing on the structure of the kernel being divided.
For a set S ⊆ V in a CADMG G, if all vertices in S can
be ordered into a sequence σS = hS1, S2, . . . i such that
S1 is ﬁxable in G, S2 in φS1 (G), etc., S is said to be ﬁx-
able in G, V \ S is said to be reachable in G, and σS is
said to be valid. A reachable set C is said to be intrin-
sic if GC has a single district, where GC is the induced
subgraph where we keep all vertices in C and edges
whose endpoints are in C. We will deﬁne φσS (G) and
φσS (qV; G) via the usual function composition to yield
operators that ﬁx all elements in S in the order given by
σS.
The distribution p(V) is said to obey the nested fac-
torization for an ADMG G if there exists a set of ker-
such that
nels
for every ﬁxable S, and any valid σS, φσS (p(V); G) =

C | paG(C)
(cid:1)
(cid:0)

| C is intrinsic in G

qC

(cid:8)

(cid:9)

D∈D(φσS (G)) qD(D| paGS(D)). All valid ﬁxing se-
Q
quences for S yield the same CADMG G(V \ S, S), and
if p(V) obeys the nested factorization for G, all valid ﬁx-
ing sequences for S yield the same kernel. As a result,
for any valid sequence σ for S, we will redeﬁne the op-
erator φσ, for both graphs and kernels, to be φS. In ad-
dition, it can be shown that the above kernel set is char-
C | paG(C)
acterized as:
=
(cid:0)
(cid:1)
φV\C(p (V); G) | C is intrinsic in G
[7]. Thus, we
(cid:8)
can re-express the above nested factorization as stat-
ing that for any ﬁxable set S, we have φS(p(V); G) =

| C is intrinsic in G

qC

(cid:8)

(cid:9)

(cid:9)

D∈D(φS(G)) φV\D(p(V); G).

Q
An important result in [7] states that if p(V ∪ H) obeys
the factorization for a DAG G with vertex set V ∪ H,
then p(V) obeys the nested factorization for the latent
projection ADMG G(V).

2.3 IDENTIFICATION AS A TRUNCATED

NESTED FACTORIZATION

For any disjoint subsets Y, A of V in a latent projec-
tion G(V) representing a causal DAG G(V ∪ H), deﬁne
Y∗ ≡ anG(V)V\A (Y). Then p(Y(a)) is identiﬁed from
p(V) in G if and only if every set D ∈ D(G(V)Y∗ ) is
intrinsic. If identiﬁcation holds, we have:

p(Y(a)) =

φV\D(p(V); G(V))|A=a.

X
Y∗\Y

Y
D∈D(G(V)Y∗ )

In other words, p(Y(a)) is identiﬁed if and only if it can
be expressed as a factorization, where every piece corre-
sponds to a kernel associated with a set intrinsic in G(V).
Moreover, no term in this factorization contains elements
of A as random variables, just as was the case in (1). The
above provides a concise formulation of the ID algorithm
[16, 14] in terms of the nested Markov model which con-
tains the causal model of the observed distribution.
If Y = {Y }, and A = {paG(Y )}, then the above trun-
cated factorization has a simpler form:

p(Y (a)) = φV\{Y }(p(V); G)|A=a.

In words, to identify the interventional distribution of Y
where all parents (direct causes) A of Y are set to val-
ues a, we must ﬁnd a total ordering on variables other
than Y (V \ {Y }) that forms a valid ﬁxing sequence.
If such an ordering exists, the identifying functional is
found from p(V) by applying the ﬁxing operator to each
variable in succession, in accordance with this ordering.
Fig. 1 shows the identiﬁcation of the functional p(Y (a))
following a total ordering of ﬁxing M, B, A.

Before generalizing these tools to the identiﬁcation of
missing data models, we ﬁrst introduce the representa-
tion of these models using DAGs.

B

M

A

Y

B

m

A

Y

(a) p(V; G)

(b) φ{M}(p(V; G))

= p(Y, A|M, B) p(B)

b

m

A

Y

b

m

a

Y

(c) φ{M,B}(p(V; G)

(d) φ{M,B,A}(p(V; G))

=

PB p(Y, A|M, B) p(B)

= PB p(Y,A=a|M,B) p(B)
PB p(A=a|M,B) p(B)

Figure 1: Identiﬁcation of p(Y (a)) by following a total
order of valid ﬁxing operations.

3 MISSING DATA MODELS OF A DAG

1 , . . . , X (1)

Missing data models are sets of full data laws (dis-
tributions) p(X(1), O, R) composed of the target laws
p(X(1), O), and the nuisance laws p(R|X(1), O) deﬁn-
ing the missingness processes. The target law is over a
set X(1) ≡ {X (1)
k } of random variables that
are potentially missing, and a set O ≡ {O1, . . . , Om}
of random variables that are always observed. The nui-
sance law deﬁnes the behavior of missingness indicators
R ≡ {R1, . . . , Rk} given values of missing and ob-
∈ X(1)
served variables. Each missing variable X (1)
has a corresponding observed proxy variable Xi, deﬁned
as Xi ≡ X (1)
if Ri = 1, and deﬁned as Xi ≡ “?” if
Ri = 0 (this is the missing data analogue of the consis-
tency property in causal inference). As a result, the ob-
served data law in missing data problems is p(R, O, X),
while some function of the target law p(X(1), O), as its
name implies, is the target of inference. The goal in miss-
ing data problems is to estimate the latter from the for-
mer. By chain rule of probability,

i

i

p(X(1), O) =

p(X, O, R = 1)
p(R = 1|X(1), O)

.

(2)

In other words, p(X(1), O) is identiﬁed from the ob-
served data law p(R, O, X) if and only if p(R =
1|X(1), O) is. In general, p(X(1)) is not identiﬁed from
the observed data law, unless sufﬁcient restrictions are
placed on the full data law deﬁning the missing data
model.

Many popular missing data models may be represented
as a factorization of the full data law with respect to
a DAG [4]. These include the permutation model,
the monotone MAR model, the block sequential MAR
model, and certain submodels of the no self-censoring
model [9, 12, 18].
Given a set of full data laws p(X(1), O, R), a DAG G
with the following properties may be used to represent a
missing data model: G has a vertex set X(1), O, R, X;
for each Xi ∈ X, paG(Xi) = {Ri, X (1)
i }; for each Ri ∈

R, deG(Ri) ∩ (X(1) ∪ O) = ∅. Given a DAG G with the
above properties, a missing data model associated with
G is the set of distributions p(X(1), O, R) that can be
written as

p(Xi|Ri, X (1)

i

)

Y
Xi∈X

Y
V ∈X(1)∪O∪R

p(V | paG(V )),

(3)

where the set of factors of the form p(Xi|Ri, X (1)
) are
deterministic to remain consistent with the deﬁnition of
Xi. Note that by standard results on DAG models, con-
ditional independences in p(X(1), O, R) may be read off
from G by the d-separation criterion [5].

i

4 EXAMPLES OF IDENTIFIED

MODELS

In this section, we describe a set of examples of missing
data models that factorize as in (3) for different DAGs,
where the target law is identiﬁed. We start with simpler
examples where sequential ﬁxing techniques from causal
inference sufﬁce to obtain identiﬁcation, then move on
to describe more complex examples where existing al-
gorithms in the literature sufﬁce, and ﬁnally proceed to
examples where no published method known to us ob-
tains identiﬁcation, illustrating an identiﬁability gap in
existing methods. In these examples, we show how iden-
tiﬁcation may be obtained by appropriately generaliz-
ing existing techniques.
In these discussions, we con-
centrate on obtaining identiﬁcation of the nuisance law
p(R|X(1), O) evaluated at R = 1, as this sufﬁces to
identify the target law p(X(1), O) by (2). In the course
of describing these examples, we will obtain intermedi-
ate graphs and kernels. In these graphs, lower case let-
ters (e.g. v) indicates the variable V is evaluated at v (for
Ri, ri = 1). A square vertex indicates V had been ﬁxed.
Drawing the vertex normally with lower case indicates
V was conditioned on (creating selection bias in the sub-
problem). For brevity, we use 1Ri to denote {Ri = 1}.

is

We ﬁrst consider the block-sequential MAR model
[18], shown in Fig. 2 for three variables. The tar-
law is identiﬁed by applying the (valid) ﬁxing
get
sequence hR1, R2, R3i via the operator φ to G and
p(R, X). We proceed as follows. p(R1| paG(R1)) =
identiﬁed immedi-
p(R1| ndG(R1)) = p(R1)
ately. Applying the ﬁxing operator φR1 yields the
graph G1 ≡ φR1 (G) shown in Fig. 2(b), and cor-
responding kernel q1(X (1)
1 , X2, X3, R2, R3|1R1) ≡
p(X1, X2, X3, R2, R3, 1R1)/p(1R1 ) where X (1)
is now
in the new subproblem rep-
observed.
resented by G1 and q1, p(R2| paG(R2))|R=1 =
q1(R2|X (1)
1 , 1R1 ) is identiﬁed. Applying the ﬁxing op-
erator φR2 to G1 and q1 yields G2 ≡ φR2 (G1) shown

Thus,

1

R3

X3

X (1)
1

X (1)
2

X (1)
3

X (1)
2

X (1)
3

X1

X2

X (1)
3

X (1)
1

X (1)
2

X (1)
3

R1

R2

R3

R2

R3

r1

r2

R1

R2

R3

X1

r1

X1

X3

X2

X3

X2

(a) G

(b) G1 ≡ φR1 (G)

(c) G2 ≡ φR2 (G1)

X1

X3

X2

(d)

Figure 2:
{R1, R2, R3} in sequence. (d) is an MNAR model that is identiﬁable by ﬁxing all Rs in parallel.

(a), (b), (c) are intermediate graphs obtained in identiﬁcation of a block-sequential model by ﬁxing

1 , X (1)

1 , X (1)

1 , X (1)

1 , X (1)

1 , X2, X3, R2, R3|1R1 )/q1(R2|X (1)

in Fig. 2(c), and q2(X (1)
2 , X3, R3|1R1,R2 ) =
q1(X (1)
1 , 1R1).
Fi-
nally, in the new subproblem represented by G2 and
q2, p(R3| paG(R3))|R=1 = q2(R3|X (1)
2 , 1R1,R2 )
to
is identiﬁed. Applying the ﬁxing operator φR3
G2 and q2 yields q3(X (1)
2 , X (1)
3 |1R1,R2,R3) =
p(X (1)
2 , X (1)
3 ). The identifying functional for the
target law only involves monotone cases (cases where
Ri = 0 implies Ri+1 = 0) just as would be the case un-
der the monotone MAR model, although this model does
not assume monotonicity and is not MAR. In this sim-
ple example, identiﬁcation may be achieved purely by
causal inference methods, by treating variables in R as
treatments, and ﬁnding a valid ﬁxing sequence on them.
In this example, each Ri in the sequence is ﬁxable given
that the previous variables are ﬁxable, since all parents
of each Ri become observed at the time it is ﬁxed.

2 , X (1)

Following a total order to ﬁx is not always sufﬁcient to
identify the target law, as noted in [4, 3, 13]. Consider
the model represented by DAG in Fig. 2(d). For any
Ri in this model, say R1, we have, by d-separation, that
p(R1| paG(R1)) = p(R1|X (1)
3 , 1R2,R3), which is
identiﬁed. However, if we were to ﬁx R1 in p(X, R),
we would obtain a kernel q1(X (1)
1 , X2, X3, 1R2,R3|1R1 )
where selection bias on R2 and R3 is introduced. The
fact that q1 is not available at all levels of R2 and R3
prevents us from sequentially obtaining p(Ri| paG(Ri)),
for Ri = R2, R3, due to our inability to sum out those
variables from q1.

The model in Fig. 2(d) allows identiﬁcation of the tar-
get law in another way, however. This follows from the
fact that p(Ri| paG(Ri)) is identiﬁed for each Ri by ex-
ploiting conditional independences in p(X, R) displayed
by Fig. 2(d). Since p(R|X(1)) =
3
i=1 p(Ri| paG(Ri)),
Q
the nuisance law is identiﬁed, which means the target law
is also identiﬁed, as long as we ﬁx R1, R2, R3 in paral-
lel (as in (2)) rather than sequentially. In other words,
the model is identiﬁed, but no total order on ﬁxing op-

erations sufﬁces for identiﬁcation. A general algorithm
that aimed to ﬁx indicators in R in parallel, while po-
tentially exploiting causal inference ﬁxing operations to
identify each p(Ri| paG(Ri)) was proposed in [13]. Our
subsequent examples show that this algorithm is insufﬁ-
cient to obtain identiﬁcation of the target law in general,
and thus is incomplete.

2

3 , 1R3) or p(R2|X (1)

Since R2 is a child
Consider the DAG in Fig. 3.
of R3 and X (1)
is a parent of R3, we cannot obtain
p(R3| paG(R3)) = p(R3|X (1)
2 ) by d-separation in any
kernel (including the original distribution) where R2 is
not ﬁxed. Thus, any total order on ﬁxing operations
of elements in R must start with R1 or R2. Fixing
either of these variables entails dividing p(X, R) by
some factor p(Ri| paG(Ri)), which is identiﬁed as either
p(R1|X (1)
1 , 1R1 ). This division en-
tails inducing selection bias on the subsequent kernel q1
for a variable not yet ﬁxed (either R3 or R1). Thus, no to-
tal order on ﬁxing operations works to identify the target
law in this model. At the same time, attempting to ﬁx all
R variables in parallel would fail as well, since we can-
not identify p(R3|X (1)
2 ) either in the original distribution
or any kernel obtained by standard causal inference oper-
ations described in [13]. In particular, in any such kernel
or distribution R3 remains dependent on R2 given X (1)
2 .
However, the target law in this model is identiﬁed by fol-
lowing a partial order ≺ of ﬁxing operations.
In this
partial order, R1 is incompatble with R2, and R2 ≺
R3. This results in an identiﬁcation strategy where we
ﬁx each variable only given that variables earlier than
it in the partial order are ﬁxed. That is, distributions
3 ) = p(R1|X3, 1R3) and p(R2|X (1)
p(R1|X (1)
1 , R3) =
p(R2|X1, 1R1 , R3) are obtained directly in the orig-
The dis-
inal distribution without ﬁxing anything.
tribution p(R3| paG(R3)), on the other hand,
is ob-
tained in the kernel q1(X1, X (1)
2 , X3, 1R1, R3|1R2 ) =
p(X, R)/p(R2|X1, 1R1, R3) after R2 (the variable ear-
lier than R3 in the partial order) is ﬁxed. The graph cor-

X (1)
1

X (1)
2

X (1)
3

X1

X2

X (1)
3

X (1)
1

X (1)
2

X (1)
3

U1

X (1)
2

X (1)
3

R1

R2

R3

r1

r2

R1

R2

R3

R1

R2

R3

X1

X3

X2

(a) G

(b) φR2 (G)

X1

X3

X1

X2

X3

X2

(a) G

(b) G(V ∪ U1 \ X (1)
1 )

R3

X3

Figure 3: (a) A DAG where Rs are ﬁxed according to a
partial order. (b) The CADMG obtained by ﬁxing R2.

2

responding to this kernel is shown in Fig. 3(b). Note
that in this graph X (1)
is observed, and there is selection
bias on R1. However, it easily follows by d-separation
that R3 is independent of R1. It can thus be shown that
p(R3|X (1)
2 , 1R2) even if q1 is only avail-
able at value R1 = 1. Since all p(Ri| paG(Ri)) are iden-
tiﬁed, so is the target law in this model, by (2).

2 ) = q1(R3|X (1)

the model

in Fig. 4.

Here,
Next, we consider
1 , X (1)
p(R2|X (1)
3 , R1) = p(R2|X1, X3, 1R1,R3 ) and
p(R3|X (1)
2 , R1) = p(R3|X2, 1R2 , R1) are identiﬁed im-
mediately. However, p(R1|X (1)
2 ) poses a problem. In or-
der to identify this distribution, we either require that R1
is conditionally independent of R2, possibly after some
ﬁxing operations, or we are able to render X (1)
observ-
able by ﬁxing R2 in some way. Neither seems to be
In particular, ﬁxing
possible in the problem as stated.
R2 via dividing by p(R2|X (1)
3 , R1) will necessar-
ily induce selection bias on R1, which will prevent iden-
tiﬁcation of p(R1|X (1)

2 ) in the resulting kernel.

1 , X (1)

2

1

However, we can circumvent the difﬁculty by treat-
ing X (1)
as an unobserved variable U1, and attempt-
ing the problem in the resulting (hidden variable) DAG
shown in Fig. 4(b), and its latent projection ADMG
˜G shown in Fig. 4(c), where U1 is “projected out.”
In the resulting problem, we can ﬁx variables accord-
ing to a partial order ≺ where R2 and R3 are in-
Thus,
compatible, R2 ≺ R1, and R3 ≺ R1.
we are able to ﬁx R2 and R3 in parallel by divid-
ing by p(R2| mb ˜G(R2)) = p(R2|X1, R1, X (1)
3 , 1R3)
and p(R3|R1, X (1)
2 ) = p(R3|R1, X2, 1R2 ), leading to
a kernel ˜q1(X1, X (1)
3 , R1|1R2,R3), and the graph
φ≺R1 ( ˜G) shown in Fig. 4(d), where notation φ≺R1
means “ﬁx all necessary elements that occur earlier than
R1 in the partial order, in a way consistent with that par-
tial order.” In this example, this means ﬁxing R2 and
R3 in parallel. We will describe how ﬁxing operates
given general ﬁxing schedules given by a partial order
later in the paper. In the kernel ˜q1 the parent of R1 is

2 , X (1)

X (1)
2

X (1)
3

X2

X3

R1

R2

R3

r2

r3

X1

X3

X2

(c) ˜G

(d) φ≺R1( ˜G)

R1

X1

Figure 4: A DAG where selection bias on R1 is avoidable
by following a partial order ﬁxing schedule on an ADMG
induced by latent projecting out X (1)
1 .

observed data, meaning that p(R1|X (1)
2 ) is identiﬁed as
˜q1(R1|X2, 1R2,R3). This implies the target law is identi-
ﬁed in this model.

In general, to identify p(Ri| paG(Ri)), we may need to
use separate partial ﬁxing orders on different sets of vari-
ables for different Ri ∈ R. In addition, the fact that ﬁx-
ing introduces selection bias sometimes results in having
to divide by a kernel where a set of variables are random,
something that was never necessary in causal inference
problems. In general, for a given Ri, the goal of a ﬁxing
schedule is to arrive at a kernel where an independence
exists allowing us to identify p(Ri| paG(Ri)), even if
some elements of paG(Ri) are in X(1) in the original
problem. This ﬁxing must be given by a partial order,
and sometimes on sets of variables. In addition, some
elements of X(1) must be treated as hidden variables.
These complications are necessary in general to avoid
creating selection bias in subproblems, and ultimately to
identify the nuisance law. The following example is a
good illustration.

graph in Fig.

5(a).
are
empty,
distributions

the
the ﬁxing schedules
their
obtain

For R1
Consider
and
and R3,
as
we
immediately
p(R1|X (1)
2 , X (1)
4 , R2, R3) = p(R1|X2, X4, R3, 1R2,R4 )
and p(R3|X (1)
4 , R2) = p(R3|X4, 1R4, R2).
For
R2, the partial order is R3 ≺ R1 in a graph where
we treat X (1)
as a hidden variable U2. This yields
p(R2|X (1)
1 , R4, 1R1,R3), where
q2(X (1)
to

1 , R4) = q2(R2|X (1)

3 , X4, R2, 1R4 |1R1,R3)

1 , X2, X (1)

is equal

2

q1 (X1 ,X2,X

,X4 ,R1,R2,1

q1 (1

R1

|X2 ,X3,X4 ,R2,1

|1

R4
R3 ,R4

)

R3
)

(1)
3

, and

X (1)
1

X (1)
2

X (1)
3

X (1)
4

X (1)
1

X (1)
3

X (1)
1

O3

X (1)
2

O3

r4

R1

R2

R3

R4

R1

R2

R3

R4

X1

X2

X3

X4

X1

X2

X3

X4

(a)

(b)

Figure 5: (a) A DAG where the ﬁxing operator must be
performed on a set of vertices. (b) A latent projection of
a subproblem used for identiﬁcation of p(R4|X (1)

1 ).

q1(X1, X2, X (1)

3 , X4, R1, R2, 1R4 |1R3 ) =

p(X, R1, R2, 1R3,R4 )
p(1R3 |R2, X4, 1R4 )

.

1

In order to obtain the propensity score for R4 we must
either render X (1)
observable through ﬁxing R1 or per-
form valid ﬁxing operations until we obtain a kernel in
which R4 is conditionally independent of R1 given its
parent X (1)
1 . However, there exists no partial order on el-
ements of R. All partial orders on elements in R induce
selection bias on variables higher in the order, preventing
the identiﬁcation of the required distribution for R4. For
example, choosing a partial ﬁxing order of R1 ≺ R3,
where we treat X (1)
and X (1)
as hidden variables re-
4
sults in selection bias on R3 as soon as we ﬁx R1. Other
partial orders fail similarly. However, the following ap-
proach is possible in the graph in which we treat X (1)
and X (1)

as hidden variables.

2

2

4

R1 and R3 lie in the same district in the resulting latent
projection ADMG, shown in Fig. 5(b). Moreover, the
set {R1, R3} is closed under descendants in the district
in Fig. 5(b). As a result, R1 and R3 can essentially be
viewed as a single vertex from the point of view of ﬁxing.
Indeed we may choose a partial order {R1, R3} ≺ R2,
where we ﬁx R1 and R3 as a set. The ﬁxing operation
on the set is possible since p(1R1,R3| mb(R1, R3)) =
p(1R1,R3|R2, R4, X2, X (1)
3 , X4) is a function of ob-
served data law, p(X, R). Speciﬁcally, it is equal to
p(1R3 |R2, R4, X2, X4)p(1R1 |R2, R4, X2, X3, X4, 1R3 ),
where
(R3 ⊥⊥ X (1)

d-separation
3 |R2, R4, X2, X4). We then obtain
1 ,X (1)

3 ,X4,R4|1R1,R2,R3 )

equality

q2(X (1)

holds

the

by

P

p(R4|X (1)

1 ) =

X

(1)
3

,X4

P

X

(1)
3

where q2(.|1R\R4) =
and q1(.|1R1,R3) =

,X4 ,R4

q2(X (1)

q1(X (1)

1 ,X (1)
1 ,X2,X (1)
q1(R2|X (1)
p(X,R2,R4,1R1,R3 )
p(1R1,R3 |R2,R4,X2,X (1)

3 ,X4,R4|1R1,R2,R3 )
3 ,X4,R2,R4|1R1,R3 )
1 ,R4,1R1,R3 )

,

3 ,X4)

.

,

Our ﬁnal example demonstrates that in order to iden-
tify the target law, we may potentially need to ﬁx vari-

X (1)
4

(a)

O3

x4

R2

X1

R2

X1

R4

X4

R1

X2

R1

X2

R4

X4

O3

X4

(c)

o3

x4

R2

X1

R2

R1

X2

r1

X2

R2

X1

R2

X1

(b)

o3

x4

R1

X2

R1

X2

r4

X (1)
1

r4

X1

r4

(d)

(e)

(f)

Figure 6: A DAG where variables besides Rs are re-
quired to be ﬁxed.

ables outside R, including variables in X(1) that become
observed after ﬁxing or conditioning on some elements
of R. Fig. 6(a) contains a generalization of the model
considered in [13], where O3 is fully observed. In this
model, distributions for R4 and R1 are identiﬁed imme-
diately, while identiﬁcation of R2 requires a partial or-
der R4 ≺ X (1)
4 ≺ O3 ≺ R1 in the graph where we
2 , X (1)
1 , X (1)
treat X (1)
as latent variables (with the latent
projection ADMG shown in Fig. 6(b)) until they are ren-
dered observed by ﬁxing the corresponding missingness
indicators. To illustrate ﬁxing operations according to
this order, the intermediate graphs that arise are shown
in Fig. 6(c),(d),(e),(f).

4

5 A NEW IDENTIFICATION

ALGORITHM

In order to identify the target law in examples dis-
cussed in the previous section, we had to consider situa-
tions where some variables were viewed as hidden, and
marginalized out, and others were conditioned on, intro-
ducing selection bias. In addition, ﬁxing operations were
performed according to a partial, rather than a total, or-
der as was the case in causal inference problems. Finally,
we sometimes ﬁxed sets of variables jointly, rather than
individual variables. We now introduce relevant deﬁni-
tions that allow us to formulate a general identiﬁcation
algorithm that takes advantage of all these techniques.
Let V be a set of random variables (and corresponding
vertices) consisting of observed variables O, R, X, miss-

ing variables X(1), and selected variables S. Let W be
a set of ﬁxed observed variables. The following def-
initions apply to a latent projection G(V \ X(1)
U , W),
for some X(1)
U ⊆ X(1), and a corresponding kernel
q(V \ X(1)
q(V|W). Graph G can be
viewed as a latent variable CADMG for q where X(1)
U
are latent. Such CADMGs represent intermediate sub-
problems in our identiﬁcation algorithm.

U |W) ≡

PX(1)

U

For Z ⊆ DZ ∈ D(G), let RZ = {Rj|X (1)
∈ Z ∪
mbG(Z), Rj 6∈ Z}, and mbG(Z) ≡ (DZ∪paG(DZ))\Z.
We say Z is ﬁxable in G(V \ X(1)

U , W) if

j

(i) deG(Z) ∩ DZ ⊆ Z,
(ii) S ∩ Z = ∅,

(iii) Z ⊥⊥ (S ∪ RZ) \ mbG(Z)| mbG(Z).

In words, these conditions apply to some Z that is a
subset of its own district (which is trivial when the
set Z is a singleton). The conditions,
in the listed
order, require that Z is closed under descendants in
the district, should not contain any selected variables,
and should be independent of both selected variables
S and the missingness indicators RZ of the corre-
sponding counterfactal parents given the Markov blan-
ket of Z, respectively. Consider the graph in Fig. 5(b)
where S = ∅ and let Z = {R1, R3}. Z is ﬁx-
able since Z ⊆ DZ = {R1, R3, X2, X4}, deG(Z) =
{R1, R3, X1, X3} ∩ DZ = {R1, R3} is closed, and both
S and RZ are empty sets.

Z spanning multiple elements in D(G) is said to be
A set
ﬁxable if it can be partitioned into a set Z of elements Z,
e
such that each Z is a subset of a single district in D(G)
and is ﬁxable.
Given an ordering ≺ on vertices V ∪ W topological in
G and

Z ﬁxable in G, deﬁne φ
e

Z(q; G) as
e
U ∪ RZ), RZ = 1|W)
q(Z| mbG (Z; anG (DZ) ∩ {(cid:22) Z})), RZ)|(R∩Z)∪RZ =1

q(V \ (X(1)

, (4)

Q
Z∈Z

Q
Z∈Z

where mbG(V ; S) ≡ mbGS (V ) and {(cid:22) Z} is the set of
all elements earlier than Z in the order ≺ (this includes
Z itself).
Given a set Z ⊆ R ∪ O ∪ X(1), and an equivalence
relation ∼, let Z/∼ be the partition of Z into equivalence
classes according to ∼. Deﬁne a ﬁxing schedule for Z/∼
to be a partial order ⊳ on Z/∼. For each Z ∈ Z/∼, deﬁne
Z} to be the set of elements in Z/∼ earlier than
Z in
{E
Z. Deﬁne E
Z} \
Z} ≡ {E
the order ⊳, and {⊳
Z and ⊳
e
e
Z
e
e
Z}, respectively.
Z} and {⊳
e
e
e
to be restrictions of ⊳ to {E
Z and ⊳
Both restrictions, E
Z, are also partial orders.
e
e
e
e

e

We inductively deﬁne a valid ﬁxing schedule (a sched-
ule where ﬁxing operations can be successfully imple-
mented), along with the ﬁxing operator on valid sched-
ules. The ﬁxing operator will implement ﬁxing as in (4)
Z within an intermediate problem represented by a
on
CADMG where some X(1)
Z ⊆ X(1) will become ob-
e
Z, with X(1) \ X(1)
served after ﬁxing
Z treated as latent
e
variables, and a kernel associated with this CADMG de-
e
ﬁned on the observed subset of variables. We also deﬁne
X(1)
{E

S
Z is valid for {⊳
e
Y of {⊳
e

Z}
e
Z} in G if for every ⊳-largest
We say ⊳
Z}, E
Y}. If ⊳
e
element
Z is
e
Z}, we deﬁne φ⊳
e
e
valid for {⊳
Z (G) to be a new CADMG
Z) obtained from
Z, W ∪
G(V \
e
e
Z∈{⊳
Z∈{⊳
G(V, W) by:

Y is valid for {E
e

X(1)
Z .

Z∈{E

Z}
e

Z}
e

Z}
e

S

S

≡

• Removing

all

edges with

arrowheads

into

Z,

S

Z∈{⊳

Z}
e
• Marking any {X (1)
Z}} as observed,
e

{⊳

j

|X (1)

j ∈ Z ∪ mbφ⊳Z (G)(Z), Z ∈

• Marking any {RZ ∩ V|Z ∈ {⊳

Z
as selected to value 1, where RZ is deﬁned with
respect to φ⊳Z (G)

Z}} \
e

Z∈{⊳

Z}
e

S

• Treating elements of X(1) \ X(1)

Z as hidden vari-
e

ables.

We say E
and

Z is valid for {E
e

Z} if ⊳
e
Z (G). If E

Z is valid for {⊳
e
Z is valid, we deﬁne
e

e

Z}
e

Z is ﬁxable in φ⊳
e

φE

Z (q; G) ≡ φ

e

Z (cid:16)φ⊳

e

e

Z (q; G); φ⊳

Z (G)(cid:17) ,

e

(5)

where φ⊳

Z(q; G) ≡ q(V|W)
Z} q
e
inductively as the denominator of (4) for
φ⊳

Y∈{⊳
f

Y (q; G).

Q

Y
f

e

, and q

Y are deﬁned
e
Y, φ⊳
e

Y (G) and

f

f

We have the following claims.
Proposition 1. Given a DAG G(X(1), R, O, X),
the
distribution p(Ri| paG(Ri))|paG(Ri)∩R=1 is identiﬁable
from p(R, O, X) if there exists

(i) Z ⊆ X(1) ∪ R ∪ O,

(ii) an equivalence relation ∼ on Z such that {Ri} ∈

Z/∼,

Z such that X(1)

(iii) a set of elements X(1)
e
Z ∈ Z/∼,
X(1) for each
e
(iv) X(1) ∩ paG(Ri) ⊆ (Z \ {Ri}) ∪ X(1)

{⊳

Z}
e

{Ri},

⊆ X(1)

Z ⊆
e

(v) and a valid ﬁxing schedule ⊳ for Z/∼ in G such that
Z ⊳ {Ri}.
e

Z ∈ Z/∼,
e

for each

Moreover, p(Ri| paG(Ri))|paG(Ri)∩R=1 is equal
to
q{Ri}, deﬁned inductively as the denominator of (4) for
{Ri}, φ⊳{Ri} (G) and φ⊳{Ri} (p; G), and evaluated at
paG(Ri) ∩ R = 1.

Proposition 1 implies that p(Ri| paG(Ri)) is identiﬁed if
we can ﬁnd a set of variables that can be ﬁxed according
to a partial order (possibly through set ﬁxing) within sub-
problems where certain variables are hidden. At the end
of the ﬁxing schedule, we require that Ri itself is ﬁxable
given its Markov blanket in the original DAG. We en-
courage the reader to view the example provided in Ap-
pendix B, for a demonstration of valid ﬁxing schedules
that may be chosen by Proposition 1.
Corollary 1. Given a DAG G(X(1), R, O, X), the target
law p(X(1), O) is identiﬁed if p(Ri| paG(Ri)) is identi-
ﬁed via Proposition 1 for every Ri ∈ R.

Proof. Follows by Proposition 1 and (2).

In addition, in special classes of models, the full law,
rather than just the target law is identiﬁed.
Proposition 2. Given a DAG G(X(1), R, O, X), the full
law p(R, X(1), O) is identiﬁable from p(R, O, X) if for
every Ri ∈ R, all conditions in Proposition 1 (i-v) are
met, and also for each
Z does not con-
e
tain any elements in {X (1)
|Rj ∈ paG(Ri)}. More-
over, p(Ri| paG(Ri)) is equal to q{Ri}, deﬁned induc-
tively as the denominator of (4) for {Ri}, φ⊳{Ri} (G) and
φ⊳{Ri} (p; G), and

Z ∈ Z/∼, X(1)
e
j

p(R, X(1), O) = (cid:16) Y

qRi (cid:17) ×

Ri∈R

p(R = 1, O, X)
Ri∈R qRi (cid:17) |R=1

(cid:16)Q

Proof. Under conditions (i-v) in Proposition 1, we
are guaranteed to identify the target law and obtain
p(Ri| paG(Ri)) where some Rj ∈ paG(Ri) may be eval-
uated at Rj = 1. Under the additional restriction stated
above, all Rj ∈ paG(Ri) can be evaluated at all lev-
els.

Proposition 2 always fails if a special collider structure
X (1)
j → Ri ← Rj, which we call the colluder, exists in
G. The following Lemma implies that colluders always
imply the full law is not identiﬁed.
Lemma 1. In a DAG G(X(1), R, O, X), if there exists
Ri, Rj ∈ R such that {Rj, X (1)
j } ∈ paG(Ri), then
p(Ri| paG(Ri))|Rj =0 is not identiﬁed. Hence, the full
law p(X(1), R) is not identiﬁed.

Proof. Follows by providing two different full laws that
agree on the observed law on a DAG with 2 counterfac-
tual random variables (Appendix C). This result holds for
an arbitrary DAG representing a missing data model that
contains the colluder structure mentioned above.

Propositions 1 and 2 do not address a computationally
efﬁcient search procedure for a valid ﬁxing schedule ⊳
that permit identiﬁcation of p(Ri| paG(Ri)) for a partic-
ular Ri ∈ R. Nevertheless, the following Lemma shows
how to easily obtain identiﬁcation of the target law in a
restricted class of missing data DAGs.
Lemma 2. Consider a DAG G(X(1), R, O, X) such that
for every Ri ∈ R, {Rj|X (1)
j ∈ paG(Ri)} ∩ anG(Ri) =
Then for every Ri ∈ R, a ﬁxing schedule ⊳
∅.
for {{Rj}|Rj ∈ GR∩deG (Ri)} given by the partial or-
der induced by the ancestrality relation on GR∩deG (Ri)
is valid in G(X(1), R, O, X), by taking each X(1)
Z =
e
Z ∈ {E{Ri}}. Thus the target
e

X(1)
Z}
S
e
law is identiﬁed.

Z , for every

Z∈{E

6 DISCUSSION AND CONCLUSION

In this paper we addressed the signiﬁcant gap present
in identiﬁcation theory for missing data models rep-
resentable as DAGs. We showed, by examples, that
straightforward application of identiﬁcation machinery
in causal inference with hidden variables do not sufﬁce
for identiﬁcation in missing data, and discussed the gen-
eralizations required to make it suitable for this task.
These generalizations included ﬁxing (possibly sets of)
variables on a partial order and avoiding selection bias
by introducing hidden variables into the problem though
they were not present in the initial problem statement.
Proposition 1 gives a characterization of how to utilize
these generalized procedures to obtain identiﬁcation of
the target law, while Proposition 2 gives a similar charac-
terization for the full law. While neither of these propo-
sitions alluded to a computationally efﬁcient algorithm
to obtain identiﬁcation in general, Lemma 2 provides
such a procedure for a special class of missing data mod-
els where the partial order of ﬁxing operations required
for each R is easy to determine. Providing a compu-
tationally efﬁcient search procedure for identiﬁcation in
all DAG models of missing data, and questions regarding
the completeness of our proposed algorithm are left for
future work.

Acknowledgements

This project is sponsored in part by the National Insti-
tutes of Health grant R01 AI127271-01 A1 and the Of-
ﬁce of Naval Research grant N00014-18-1-2760.

[14] Ilya Shpitser and Judea Pearl.

Identiﬁcation of
joint interventional distributions in recursive semi-
In Proceedings of the
Markovian causal models.
Twenty-First National Conference on Artiﬁcial In-
telligence (AAAI-06). AAAI Press, 2006.

[15] Eric J. Tchetgen Tchetgen, Linbo Wang, and
BaoLuo Sun. Discrete choice models for non-
monotone nonignorable missing data:
Identiﬁca-
tion and inference. Statistica Sinica, 28(4):2069–
2088, 2018.

[16] Jin Tian and Judea Pearl. A general identiﬁcation
condition for causal effects. In Eighteenth National
Conference on Artiﬁcial Intelligence, pages 567–
573, 2002.

[17] Anastasios Tsiatis.

Semiparametric Theory and
Missing Data. Springer-Verlag New York, 1st edi-
tion edition, 2006.

[18] Yan Zhou, Roderick J. A. Little, and John D.
Kalbﬂeisch. Block-conditional missing at ran-
dom models for missing data. Statistical Science,
25(4):517–532, 2010.

References

[1] Yimin Huang and Marco Valtorta. Pearl’s calcu-
In Twenty Sec-
lus of interventions is complete.
ond Conference On Uncertainty in Artiﬁcial Intel-
ligence, 2006.

[2] Steffan L. Lauritzen. Graphical Models. Oxford,

U.K.: Clarendon, 1996.

[3] Karthika Mohan and Judea Pearl. Graphical models
for recovering probabilistic and causal queries from
missing data. In Advances in Neural Information
Processing Systems, pages 1520–1528. 2014.

[4] Karthika Mohan, Judea Pearl, and Jin Tian. Graph-
ical models for inference with missing data. In Ad-
vances in Neural Information Processing Systems,
pages 1277–1285, 2013.

[5] Judea Pearl. Probabilistic Reasoning in Intelligent
Systems. Morgan and Kaufmann, San Mateo, 1988.

[6] Judea Pearl. Causality: Models, Reasoning, and
Inference. Cambridge University Press, 2 edition,
2009.

[7] Thomas S. Richardson, Robin J. Evans, James M.
Nested Markov
Robins, and Ilya Shpitser.
properties for acyclic directed mixed graphs.
arXiv:1701.06686v2, 2017. Working paper.

[8] James M. Robins. A new approach to causal in-
ference in mortality studies with sustained expo-
sure periods – application to control of the healthy
worker survivor effect. Mathematical Modeling,
7:1393–1512, 1986.

[9] James M. Robins. Non-response models for the
analysis of non-monotone non-ignorable missing
data. Statistics in Medicine, 16:21–37, 1997.

[10] D. B. Rubin. Causal inference and missing data
(with discussion). Biometrika, 63:581–592, 1976.

[11] Mauricio Sadinle and Jerome P. Reiter.

Item-
wise conditionally independent nonresponse mod-
elling for incomplete multivariate data. Biometrika,
104(1):207–220, 2017.

[12] Ilya Shpitser. Consistent estimation of functions of
data missing non-monotonically and not at random.
In Advances in Neural Information Processing Sys-
tems, pages 3144–3152, 2016.

[13] Ilya Shpitser, Karthika Mohan, and Judea Pearl.
Missing data as a causal and probabilistic prob-
lem. In Proceedings of the Thirty First Conference
on Uncertainty in Artiﬁcial Intelligence (UAI-15),
pages 802–811. AUAI Press, 2015.

7 APPENDIX

A. Proofs

Proposition 1 Given a DAG G(X(1), R, O, X), the
distribution p(Ri| paG(Ri))|paG(Ri)∩R=1 is identiﬁable
from p(R, O, X) if there exists

(i) Z ⊆ X(1) ∪ R ∪ O,

Z/∼,

(ii) an equivalence relation ∼ on Z such that {Ri} ∈

and CADMG

Z such that X(1)

(iii) a set of elements X(1)
e
Z ∈ Z/∼,
X(1) for each
e
(iv) X(1) ∩ paG(Ri) ⊆ (Z \ {Ri}) ∪ X(1)

{⊳

{Ri},

⊆ X(1)

Z ⊆
e

Z}
e

(v) and a valid ﬁxing schedule ⊳ for Z/∼ in G such that
Z ⊳ {Ri}.
e

Z ∈ Z/∼,
e

for each

Moreover, p(Ri| paG(Ri))|paG(Ri)∩R=1 is equal
to
q{Ri}, deﬁned inductively as the denominator of (4) for
{Ri}, φ⊳{Ri} (G) and φ⊳{Ri} (p; G), and evaluated at
paG(Ri) ∩ R = 1.

Proof. We ﬁrst outline the essential argument made in
this proof. We will reformulate the process of ﬁxing ac-
cording to a partial order in a missing data problem as
a problem of ordinary ﬁxing based on a total order in a
causal inference problem where, previously missing vari-
ables are in fact observed. If we are able to show this, we
can invoke results from [7], that guarantee that we obtain
the desired conditional for each Ri.
Z ∈ Z/∼, and deﬁne X(1)
e
k ∈ X(1)
Z} ≡ {Rk|X (1)
{E
e
and R
Z}.
e

and R
X(1)
{⊳

}, and similarly for

Consider

X(1)
Z ,

Z∈{E

Z}
e

Z}
e

Z}
e

S

≡

{E

{E

{⊳

Z}
e

Z}
e

Z} in G(R, O, X(1), X)), where X(1)
{⊳
e

Z} con-
We ﬁrst note that any total ordering ≺ on {⊳
sistent with ⊳ yields a valid ﬁxing sequence on sets
e
, R, O, X
in {⊳
are observed. The total ordering ≺ can be reﬁned to oper-
Z is ﬁxed as single-
ate on single variables where each set
tons following a topological total order where variables
e
Z would be ﬁxed ﬁrst. Such a total
with no children in
order is also valid and follows from the validity of ⊳ and
e
the fact that at each step of the ﬁxing operation in the
total order, the Markov blanket of each Z contains only
observed variables; hence no selection bias is induced on
Z}.
any singleton variables {≻
e

We now show, by induction on the structure of the partial
order ⊳, that for a particular

Z ∈ Z/∼, q
e

Z is equal to
e

˜q(Z| mb ˜G(Z; an ˜G (DZ)∩ ≺ ˜G {Z}, RZ)|(R∩Z)∪RZ=1,

Y
Z∈Z

Y
Z∈Z

(6)

obtained from a kernel

˜q ≡ φ{⊳

Z}(p(R, O, X(1)

{⊳

e

Z}
e

, X); G),

˜G ≡ φ{⊳

Z}(G(R, O, X(1)

{⊳

e

Z}
e

, X)),

where X(1)
{⊳

Z}
e

, R, O, X are observed.

For any ⊳-smallest

Z,
e
its Markov blanket;
served results in the same kernel as q

Z is independent of R
{E
therefore treating X(1)
e
{E

Z}
e

Z} given
e
as ob-

Z.
e

We now show that the above is also true for any
Z/∼. Assume the inductive hypothesis holds for all
{⊳
Z by applying
e

Z}. Since ⊳ is valid, we obtain q
e
φE
Z (q; G) ≡

Z ∈
Y ∈
e
e

e

φ

Z(cid:16)
e

p(O, X, R \ R

{⊳

Z}, R
e
Z} q
Y∈{⊳
e

Y
e

Q e

Z} = 1)

{⊳

e

; φ⊳

Z (G)(cid:17),

e

(7)

where q
φ

Y are deﬁned by the inductive hypothesis, and
e
Z is deﬁned via
e

q(V \ ((X(1) \ X(1)
{⊳
q(Z| mb ˜G(Z; an ˜G(DZ)∩ ≺ ˜G (Z)), RZ)|(R∩Z)∪RZ=1

) ∪ RZ), RZ = 1|W)

Z}
e

,

QZ∈Z QZ∈Z

where

(8)

(9)

q(V\(X(1)\X(1)
{⊳

Z}
e

)|W) ≡

p(O, X, R \ R

{⊳

Z}, R
e
Z} q
Y∈{⊳
e

Y
e

Q e

Z} = 1)

{⊳

e

.

Consider the equivalent functional in the model where
we observe X(1)
{⊳

Z}
e
q†(V \ ((X(1) \ X(1)
{⊳
q†(Z| mb ˜G(Z; an ˜G(DZ)∩ ≺ ˜G (Z)), RZ)|(R∩Z)∪RZ=1

) ∪ RZ), RZ = 1|W)

Z}
e

,

QZ∈Z QZ∈Z

where
q†(V\(X(1) \ X(1)
{⊳
p(O, X, X(1)
{⊳

Z}
e

)|W) ≡

Z}
e

, R \

R
Z},
{⊳
e
e
Z} q
Y∈{⊳
e

Y
e

R
e

Q e

Z} = 1)

{⊳

e

,

Z} is deﬁned as the subset of R

{⊳

Z} that is ﬁxed
e

R
and
{⊳
e
e
Z}.
in {⊳
e

The only difference between (8) and (9) for the purposes
of the denominator is the variables in R
Z}.
e
But the denominator is independent of these variables,
by assumption. Thus, it follows that ﬁxing on a valid
partial order with missing data and ﬁxing on a total order
consistent with this partial order, as in causal inference,
yield equivalent kernels.

Z} \
e

R
e

{⊳

{⊳

The conclusion follows by Lemma 55 in [7].

ﬁxed prior to Rj by the proposed partial order. Thus, it
follows that S ∩ {Rj} = ∅.

Finally, following the partial order, and under the as-
sumption stated in the lemma, R{Rj} ⊆ {⊳Rj}. We
have also proved that S ⊆ ndφ⊳Rj
(G)(Rj). Therefore,
Rj ⊥⊥ (S ∪ R{Rj}) \ mbφ⊳Rj
(G)(Rj).

(G)(Rj)| mbφ⊳Rj

Z is ﬁxable, the proposed partial order ⊳ for
Since each
each Ri is valid. Therefore, all ﬁve conditions in Propo-
e
sition 1 are satisﬁed concluding the target law is ID.

B. An example to illustrate the algorithm

We walk the reader through identiﬁcation of the target
law for the missing data DAG shown in Fig. 7(a) in order
to demonstrate the full generality of our missing ID algo-
rithm. As a reminder, the target law is identiﬁed by (2)
if we are able to identify p(Ri| paG(Ri))|R=1 for each
Ri ∈ R. The identiﬁcation of these conditional densities
are shown in equations (i) through (viii). For a clearer
presentation of this example, we switch to one-column
format.

Lemma 2 Consider a DAG G(X(1), R, O, X) such that
for every Ri ∈ R, {Rj|X (1)
j ∈ paG(Ri)} ∩ anG(Ri) =
Then for every Ri ∈ R, a ﬁxing schedule ⊳
∅.
for {{Rj}|Rj ∈ GR∩deG (Ri)} given by the partial or-
der induced by the ancestrality relation on GR∩deG (Ri)
is valid in G(X(1), R, O, X), by taking each X(1)
Z =
e
Z ∈ {E{Ri}}. Thus the target
e

X(1)
Z}
S
e
law is identiﬁed.

Z , for every

Z∈{E

Proof. In order to prove that the target law is identiﬁed,
we demonstrate that conditions (i-v) in Proposition 1 are
satisﬁed for each Ri.

Conditions (i) and (ii) are trivially satisﬁed as we choose
to ﬁx Z ⊆ R, and we choose no equivalence relation,
thus Z/∼ consists of singleton sets of Rs. Condition (iii)
is also trivial as each X(1)
Z is a union of the corresponding
e
sets X(1)
Y earlier in the partial order. In the pro-
posed order we never ﬁx elements in X(1), and propose
e
to keep elements in X(1) ∩ paG(Rj) for every Rj ∈ Z.
In particular, this also includes Ri, satisfying condition
(iv).

Y , for
e

Finally, we show that the proposed schedule ⊳ is valid
Z ∈ Z/∼ is ﬁxable. There are 3
by showing that each
Z to be ﬁxable as mentioned
e
conditions for an element
in section 5. We go through each of these conditions and
e
demonstrate each
Z (G)
where ⊳ is the proposed ﬁxing schedule above.

Z in Z/∼ is a valid ﬁxing in φ⊳
e

e

Rj = X(1), φ⊳Rj

(G) is a CDAG. Thus, D(φ⊳Rj

Z is a singleton Rj ∈ Z/∼
In the proposed schedule each
that we are trying to ﬁx in a graph φ⊳Rj (G). Since
e
X(1)
(G))
is just sets of singleton vertices. In particular, DRj =
{Rj}. Further, by deﬁnition of the schedule, it must be
that deφ⊳Rj
(G)(Rj ) = {Rj}. This satisﬁes condition (i).
For condition (ii), we note that S ⊆ ndφ⊳Rj
(G)(Rj) else,
S contains some Rk ∈ deG(Rj ) which should have been

X (1)
8

X (1)
7

X (1)
6

X (1)
5

X (1)
1

X (1)
2

X (1)
3

X (1)
4

R2

R8

R7

R6

R5

R1

R2

R3

R4

R1

R1

R8

{R1, R3}

R8

(a) G

(b)

(c)

(d)

R5 R6

R5 R6 R3

R6 R7

R5

R7

Figure 7: (a) A complex missing data DAG used to illustrate the general techniques used in our algorithm (b-e) The
corresponding ﬁxing schedules of Rs.

We start with {R3, R5, R6, R7}. The ﬁxing schedules for these are empty and we obtain the following immediately
from the original distribution.

R4

R2

R6

(e)

(i) p(R3| pa(R3)) = p(R3|R2, X (1)

4 ) = p(R3|R2, X4, 1R4 ),

(ii) p(R5| pa(R5)) = p(R5|R1, X (1)

6 ) = p(R5|R1, X6, 1R6 ),

(iii) p(R6| pa(R6)) = p(R6|R1, R8, X (1)

5 , X (1)

7 ) = p(R6|R1, R8, X5, X7, 1R5,R7),

(iv) p(R7| pa(R7)) = p(R7|R8, X (1)

6 ) = p(R7|R8, X6, 1R6 ).

For R1, we choose Z = {R1, R5, R6}, and no equivalence relations. Thus, Z/∼ = {{R1}, {R5}, {R6}}. The ﬁxing
schedule ⊳ is a partial order shown in Fig. 7(b) where R5 and R6 are incompatible, and R5 ≺ R1, R6 ≺ R1. Starting
with the original G in Fig. 7(a), ﬁxing R5 and R6 in parallel yields the following kernel.

qr1(X \ {X5, X6}, X (1)

5 , X (1)

6 , R \ {R5, R6}|1R5,R6 ) =

p(R5|R1, X (1)

p(X, R = 1)
6 ) p(R6|R1, R8, X (1)

5 , X (1)

7 )|R=1

,

(10)

where the propensity scores in the denominator are identiﬁed using (ii) and (iii). The CADMG corresponding to this
ﬁxing operation is shown in Fig. 8(a).

(v) p(R1| pa(R1))|R=1 = p(R1|R2, R3, X (1)

2 , X (1)

4 , X (1)

5 , X (1)

6 )|R=1

= qr1 (R1|R2, R3, X (1)
2 , X (1)
= qr1 (R1|R3, X2, X (1)
4 , X5, X6, 1R2,R5,R6)|R=1
= qr1 (R1|R3, X2, X4, X5, X6, 1R2,R4,R5,R6)|R=1

4 , X5, X6, 1R5,R6 )|R=1

(by d-sep)

where the last term can be obtained using kernel operations (conditioning+marginalization) on qr1(.|.) deﬁned in (10).

A similar procedure is applicable to R8, where Z/∼ = {{R8}, {R7}, {R6}}; Fig. 7(d). Starting with the original G in
Fig. 7(a), ﬁxing R6 and R7 in parallel yields the following kernel.

qr8(X \ {X6, X7}, X (1)

6 , X (1)

7 , R \ {R6, R7}|1R6,R7 ) =

p(R6|R1, R8, X (1)

7 ) p(R7|R8, X (1)

6 )|R=1

p(X, R = 1)
5 , X (1)

,

(11)

X (1)
8

X7

X6

X5

X (1)
1

X (1)
2

X (1)
3

X (1)
4

X (1)
8

X7

X6

X5

X (1)
1

X (1)
2

X (1)
3

X (1)
4

R8

r7

r6

r5

R1

R2

R3

R4

R8

r7

r6

r5

R1

R2

R3

R4

(a) φ{R5,R6}(G)

(b) φ{R6,R7}(G)

Figure 8: (a) Graph corresponding to the kernel obtained in (10) (b) Graph corresponding to the kernel obtained in
(11).

where the propensity scores in the denominator are identiﬁed using (iii) and (iv). The CADMG corresponding to this
ﬁxing operation is shown in Fig. 8(b).

(vi) p(R8| pa(R8))|R=1 = p(R8|R4, X (1)

6 , X (1)

7 )|R=1

= qr8 (R8|R4, X (1)
= qr8 (R8|R4, X6, X7, 1R6,R7)|R=1

7 , 1R6,R7)|R=1

6 , X (1)

where the last term can be obtained using kernel operations (conditioning+marginalization) on qr8(.|.) deﬁned in (11).

Thus, Z/∼ =
For R2, we choose Z = {R1, R2, R3, R5, R6},
{{R1}, {R2}, {R3}, {R5}, {R6}}. The ﬁxing schedule ⊳ is a partial order where R3, R5, R6 are incompatible and
R5, R6 ≺ R1 ≺ R2 and R3 ≺ R2 as shown in Fig. 7(c). In addition, the portion of the ﬁxing schedule involving R1,
R5, and R6 is executed in a latent projection ADMG where we treat X (1)
as being hidden as shown in Fig. 9(a), while
the portion of the ﬁxing schedule involving R3 is executed in the original graph, Fig. 7(a).

and no equivalence relations.

2

(vii) p(R2|R4, X (1)

1 ) = qr2(R2|R4, X (1)

1 , 1R1,R3),

(12)

where qr2 corresponds to the kernel obtained by following the partial order of ﬁxing R3 and R1, separately. That is,

qr2(.|1R1,R3) =

r2 (R1|R2, R3, X2, X5, X6, X (1)
q1

8 , 1R5,R6 ) p(R3|R2, X (1)
4 )

p(X, R = 1)
3 , X (1)

.

(13)

The propensity score for R3 is obtained from (i) and q1
graph where X (1)

is treated as hidden, as shown in Figures 9(a) and (b). That is,

2

r2 is the kernel obtained by ﬁxing R5 and R6 in parallel in a

r2(X \ {X5, X6}, X (1)
q1

5 , X (1)

6 , R \ {R5, R6}|1R5,R6 ) =

p(R5|R1, X (1)

p(X, R = 1)
6 ) p(R6|R1, R8, X (1)

5 , X (1)

7 )|R=1

.

The propensity scores in the denominator above are identiﬁed using (ii) and (iii). For clarity, the CADMGs corre-
sponding to ﬁxing R1 and R3 are illustrated in Figures 9(c) and (d).

for R4, we choose Z = {R} and equivalence relation R1 ∼ R3.

Thus, Z/∼ =
Finally,
{{R1, R3}, {R2}, {R4}, {R5}, {R6}, {R7}, {R8}}. The ﬁxing schedule ⊳ is a partial order where R5, R6 ≺
{R1, R3} ≺ R2 ≺ R4 and R6, R7 ≺ R8 ≺ R4 as shown in Fig. 7(e). In addition, the portion of the ﬁxing schedule
involving R5, R6, {R1, R3}, and R2 is executed in a latent projection ADMG where we treat X (1)
as hidden
variables, shown in Fig. 10(b), while the portion of the ﬁxing schedule involving R6, R7, and R8 is executed in the
original graph, Fig. 7(a).

and X (1)

2

4

(viii) p(R4|X (1)

1 ) = qr4 (R4|X (1)

1 , 1R2,R8),

(14)

X (1)
8

X (1)
7

X (1)
6

X (1)
5

X (1)
1

X (1)
3

X (1)
4

X (1)
8

X7

X6

X5

X (1)
1

X (1)
3

X (1)
4

R8

R7

R6

R5

R1

R2

R3

R4

R8

r7

r6

r5

R1

R2

R3

R4

X2
(a) G1 ≡ G(V \ X (1)
2 )

X2
(b) G2 ≡ φ{R5,R6}(G1)

X8

X7

X6

X5

X1

X (1)
3

X (1)
8

X (1)
7

X (1)
6

X (1)
5

X (1)
1

X (1)
2

X3

X4

r8

r7

r6

r5

r1

R2

R3

R8

R7

R6

R5

R1

R2

r3

r4

X4

r4

X2
(c) G3 ≡ φR1(G2)

(d) φR3(G)

Figure 9: Execution of the ﬁxing schedule to obtain the propensity score for R1 (a) Latent projection ADMG obtained
by projecting out X (1)

(b) Fixing R5 and R6 in G1 (c) Fixing R1 in G2 (d) Fixing R3 in the original graph.

2

where qr4 corresponds to the kernel obtained by following the partial order of ﬁxing R2 and R8, separately. That is,

qr4(.|1R2,R8) =

p(X, R = 1)

r4 (R2|R4, X2) q2
q1

r4(R8|R4, X6, X7)

.

(15)

q1
r4 is the kernel obtained by ﬁxing the set {R1, R3} in graph G2 shown in Fig. 10(c). That is,

r4(.|1R1,R3,R5,R6) =
q1

r4 (.|1R5,R6 )
q3

r4 (R1, R3|R2, R4, X2, X (1)
q3

3 , X4)
r4 (.|1R5,R6 )
q3
r4 (R1|R2, R4, X2, X3, X4, 1R3 ) q3
q3

=

r4(R3|R2, R4, X2, X4)

q3
r4

is the kernel obtained by ﬁxing R5 and R6 in parallel in the graph G1 shown in Fig. 10(b). That is,

r4(.|1R5,R6) =
q3

p(R5|R1, X (1)

p(X, R = 1)
6 ) p(R6|R1, R8, X (1)

5 , X (1)

7 )|R=1

The propensity scores in the denominator above are identiﬁed using (ii) and (iii).

Finally, q2
r4

is the kernel obtained by ﬁxing R6 and R7 in parallel in the original graph G, shown in Fig. 7(a). That is,

r4(.|1R6,R7) =
q2

p(R6|R1, R8, X (1)

7 ) p(R7|R8, X (1)

6 )|R=1

p(X, R = 1)
5 , X (1)

The propensity scores in the denominator above are identiﬁed using (iii) and (iv). For clarity, the CADMG corre-
sponding to ﬁxing R8 is illustrated in Figures 10(a).

.

.

X4

R4

X4

X3

r3

R2

X2

X8

X7

X6

X5

X (1)
1

X (1)
2

X (1)
3

X (1)
4

X (1)
8

X (1)
7

X (1)
6

X (1)
5

X (1)
1

X (1)
3

r8

r7

r6

r5

R1

R2

R3

R4

R8

R7

R6

R5

R1

R2

R3

R4

(a) φ{R6,R7,R8}(G)

(b) G1 ≡ G(V \ {X (1)

X2
2 , X (1)

4 })

X (1)
8

X7

X6

X5

X (1)
1

X (1)
3

X8

X7

X6

X5

X1

R8

r7

r6

r5

R1

R2

R3

R4

r8

r7

r6

r5

r1

X2
(c) G2 ≡ φ{R5,R6}(G1)

X4

(d) G3 ≡ φ{{R1,R3}}(G2)

Figure 10: Execution of the ﬁxing schedule to obtain the propensity score for R4 (a) CADMG obtained by following
the schedule to get the propensity score for R8 (b) Latent projection ADMG obtained by projecting out X (1)
and X (1)
(c) Fixing R5 and R6 in G1 (d) Fixing R1 in G2.

2

4

C. Table for Lemma 1

X (1)
1

X (1)
2

R1

R2

X1

X2

R1
0
1

p(R1)
a
1 − a

X (1)
1
0
1

p(X (1)
1 )
b
1 − b

X (1)
2
0
1

p(X (1)
2 )
c
1 − c

R2 R1 X (1)
0
0
0
1
1
0
1
1
0
0
0
1
1
0
1
1

1
0
0
0
0
1
1
1
1

p(R2|R1, X (1)
1 )
d
1 − d
e
1 − e
f
1 − f
g
1 − g

R1 R2 X (1)

X1 X2

p(Observed Law)

0

0

1

0

0

1

1

1

1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1

X (1)
2
0
0
1
1
0
0
1
1
0
0
1
1
0
0
1
1

p(Full Law)
abcd
a(1 − b)cf
ab(1 − c)d
a(1 − b)(1 − c)f
(1 − a)ebc
(1 − a)g(1 − b)c
(1 − a)eb(1 − c)
(1 − a)g(1 − b)(1 − c)
abc(1 − d)
a(1 − b)c(1 − f )
ab(1 − c)(1 − d)
a(1 − b)(1 − c)(1 − f )
(1 − a)(1 − e)bc
(1 − a)(1 − g)(1 − b)c
(1 − a)(1 − e)b(1 − c)
(1 − a)(1 − g)(1 − b)(1 − c)

?

0

1

?

0
1
0
1

?

?

0

1

0
0
0
1

ahdb + f (1 − b))i

(1 − a)eb

(1 − a)g(1 − b)

ach1 − (cid:16)db + f (1 − b)(cid:17)i

a(1 − c)h1 − (cid:16)db + f (1 − b)(cid:17)i
(1 − a)(1 − e)bc
(1 − a)(1 − g)(1 − b)c
(1 − a)(1 − e)b(1 − c)
(1 − a)(1 − g)(1 − b)(1 − c)

Any pair of {d, f } would lead to different full laws. However, as long as db + f (1 − b) stays constant, the observe law
would agree across all different full laws (which include inﬁnitely many models). This is a general characterization of
non-identiﬁable models with two binary random variables.

