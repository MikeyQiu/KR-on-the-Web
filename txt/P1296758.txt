Dynamic Compositional Neural Networks over Tree Structure

Pengfei Liu Xipeng Qiu∗ Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China
{pﬂiu14,xpqiu,xjhuang}@fudan.edu.cn

7
1
0
2
 
y
a
M
 
1
1
 
 
]
L
C
.
s
c
[
 
 
1
v
3
5
1
4
0
.
5
0
7
1
:
v
i
X
r
a

Abstract

Tree-structured neural networks have proven to
be effective in learning semantic representations
by exploiting syntactic information. In spite of
their success, most existing models suffer from
the underﬁtting problem: they recursively use the
same shared compositional function throughout the
whole compositional process and lack expressive
power due to inability to capture the richness of
compositionality. In this paper, we address this is-
sue by introducing the dynamic compositional neu-
ral networks over tree structure (DC-TreeNN), in
which the compositional function is dynamically
generated by a meta network. The role of meta-
network is to capture the metaknowledge across the
different compositional rules and formulate them.
Experimental results on two typical tasks show the
effectiveness of the proposed models.

1 Introduction
Learning the distributed representation for long spans of text
from its constituents has been a key step for various natu-
ral language processing (NLP) tasks, such as text classiﬁca-
tion [Zhao et al., 2015; Liu et al., 2015], semantic matching
[Liu et al., 2016a; Liu et al., 2016b], and machine translation
[Cho et al., 2014]. Existing deep learning approaches take
a compositional function with different forms to compose
word vectors recursively until obtaining a sentential represen-
tation. Typically, these compositional functions involve re-
current neural networks [Hochreiter and Schmidhuber, 1997;
Sutskever et al., 2014], convolutional neural networks [Col-
lobert et al., 2011; Kalchbrenner et al., 2014], and tree-
structured neural networks [Tai et al., 2015; Zhu et al., 2015].
Among these methods, tree-structured neural networks
(Tree-NNs) show theirs superior performance in many NLP
tasks [Socher et al., 2012; Irsoy and Cardie, 2014]. Following
the syntactic tree structure, Tree-NNs assign a ﬁxed-length
vector to each word at the leaves of the tree, and combine
word and phrase pairs recursively to create intermediate node
vectors, eventually obtaining one ﬁnal vector to represent the
whole sentence.

∗Corresponding author.

However, these models have a major limitation in their
inability to fully capture the richness of compositionality
[Socher et al., 2013a]. The same parameters are used for all
kinds of semantic compositions, even though the composi-
tions have different characteristics in nature. For example, the
composition of the adjective and the noun differs signiﬁcantly
from the composition of the verb and the noun. Moreover,
many semantic phenomena, such as semantic idiomaticity or
transparency, call for more powerful compositional mecha-
nisms [Pylkk¨anen and McElree, 2006]. Therefore, Tree-NNs
suffer from the underﬁtting problem.

To alleviate this problem, some researchers propose to use
multiple compositional functions, which are arranged before-
hand according to some partition criterion [Socher et al.,
2012; Socher et al., 2013a; Dong et al., 2014]. Intuitively,
using different parameters for different types of compositions
has the potential to greatly reduce underﬁtting. Socher et al.
[2013a] deﬁned different compositional functions in terms of
syntactic categories, and a suitable compositional function is
selected based on the syntactic categories. Dong et al. [2014]
introduced multiple compositional functions and during com-
positional phase, a proper one is selected based on the input
information. Although these models accomplished their mis-
sion to a certain extent, they still suffer from the following
three challenges. First, the predeﬁned compositional func-
tions cannot cover all the compositional rules; Second, they
require more learnable parameters, suffering from the prob-
lem of overﬁtting; Third, it is difﬁcult to determine a universal
criterion for semantic composition based solely on syntactic
categories.

In this paper, we propose dynamic compositional neural
networks over tree structure, in which a meta network is
used to generate the context-speciﬁc parameters of a dynamic
compositional network. Speciﬁcally, we construct our mod-
els based on two kinds of tree-structured neural networks: re-
cursive neural network (Tree-RecNN) [Socher et al., 2012]
and tree-structure long short-term memory neural network
(Tree-LSTM) [Tai et al., 2015]. Our work is inspired by re-
cent work on dynamic parameter prediction [De Brabandere
et al., 2016; Bertinetto et al., 2016; Ha et al., 2016]. The meta
network is used to extract the shared meta-knowledge across
different compositional rules and to dynamically generate the
context-speciﬁc compositional function. Thus, the composi-
tional function of our models varies with positions, contexts

and samples. The dynamic compositional network then ap-
plies those context-speciﬁc parameters to the current input
information. Both meta and dynamic networks are differen-
tiable such that the overall networks can be trained in an end-
to-end fashion. Additional, to reduce the complexity of the
whole networks, we deﬁne the dynamic weight matrix in a
manner simulating low-rank matrix decomposition.

We evaluate our models on two typical tasks: text classi-
ﬁcation and text semantic matching. The results show that
our models are more expressive due to their learning to learn
nature, yet without increasing the number of model’s parame-
ters. Moreover, we ﬁnd certain composition operations can be
learned implicitly by meta TreeNN, such as the composition
of noun phrases and verb phrases.

The contributions of the paper can be summed up as fol-

lows.

1. We provide a new perspective on how to compose the
individual word of a sentence. Instead of directly us-
ing a learnable parameterized compositional function,
we introduce a meta neural network, which can generate
a compositional network to dynamically compose con-
stituents over tree structure.

2. Experimental results show that the proposed architecture
is more expressive due to its learning to learn nature, yet
without increasing the number of model’s parameters.

3. We present an elaborate qualitative analysis, giving an
intuitive understanding on how our model works from
semantic and syntactic perspectives.

2 Tree-Structured Neural Network
In this section, we brieﬂy describe the tree-structured neural
networks.

The idea of tree-structured neural networks for natural lan-
guage processing (NLP) is to train a deep learning model
with a grammatical tree structure [Pollack, 1990] that can be
applied to phrases and sentences. At every node in the tree,
the contexts of the left and right children are combined by a
compositional function. The parameters of the compositional
function are shared across all nodes in the tree. The layer
computed at the top node gives a representation for the whole
sentence.

2.1 Vanilla Recursive Neural Network
The simplest member of tree-structured NN is the vanilla re-
cursive neural network [Socher et al., 2013a], in which the
representation of parent is calculated by weighted linear com-
bination of the child vectors.

Formally, given a binary constituency tree T induced by
a sentence, each non-leaf node corresponds to a phrase. We
refer to hj ∈ Rd as the hidden state of each node j, and let hl
j,
hr
j denote the left and right child representations respectively.

hj = tanh

W

(cid:18)

(cid:21)

(cid:20)hl
j
hr
j

(cid:19)

+ b

,

(1)

where W ∈ Rd×2d is a learnable compositional matrix, b is
the bias vector.

Figure 1: Illustration of Vanilla Tree Structure Network. NP
and VP represent noun and verb phrases respectively. θ de-
notes the shared parameters of compositional function.

2.2 Tree LSTM
Tree LSTM [Tai et al., 2015] is a generalization of LSTMs
to tree-structured network topologies. In this model, the com-
positional function is an LSTM unit, and the hidden state hj
of each node can be computed as follows: we refer to hj and
cj as the hidden state and memory cell of each node j. The
transition equations of node j are as follows:








˜cj
oj
ij
f l
j
f r
j







xj
hl
j
hr
j





=


















W

tanh
σ
σ
σ
σ
cj = ˜cj (cid:12) ij + cl
j (cid:12) f l
hj = oj (cid:12) tanh (cj) ,





 + b

 ,

j + cr

j (cid:12) f r
j ,

(2)

(3)

(4)
where xj ∈ Rd denotes the input vector and is non-zero if and
only if it is a leaf node. The superscript l and r represent the
left child and right child respectively. σ represents the logistic
sigmoid function and (cid:12) denotes element-wise multiplication.
W ∈ R5d×3d and b ∈ R5d are learnable parameters.

3 Dynamic Compositional Neural Network
In the above two tree-structured NNs, the compositional func-
tion is shared across all nodes in the tree, which results in
underﬁtting since the semantic compositions have great di-
versities. To address this problem, we propose two dynamic
compositional neural networks over tree structure, which dy-
namically generate different parameters for different types of
compositions. Figure 2 shows an illustration of the dynamic
compositional neural network, consisting of two components:
(1) meta network and (2) basic network with dynamic param-
eters.

Speciﬁcally, we propose two meta networks to generate
the context-speciﬁc compositional functions for RecNN and
TreeLSTM respectively.

3.1 Meta Network for RecNN
For RecNN, we replace the static parameters W and b in
Eq.(1) with the dynamic parameters W(zj) and b(zj), which
are generated by a meta network. The meta network is a
smaller RecNN, and the hidden state ˆhj ∈ Rm of node j
in meta network is deﬁned as

ˆhj = tanh (WmHj + bm) ,

(5)

(12)

(13)

(14)

(15)

(16)

zj = Wz

ˆhj

j ⊕ hr

j ∈ R2m+2d; Wm ∈
where Hj = hl
R5m×(3d+2m) and bm ∈ Rm are parameters of meta TreeL-
STM; Wz ∈ Rz×m is a scale matrix.

j ⊕ ˆhr

j ⊕ ˆhl

The dynamic parameters W(zj) and b(zj) of basic TreeL-

STM are computed by:

W(zj) =

Wg, Wi, Wf l

b(zj) =

bg, bi, bf l

, bf r

(cid:104)

(cid:104)

, Wo(cid:105)

, Wf r
, bo(cid:105)

,

where for ∗ ∈ {c, o, i, f l, f r},

W∗(zj) =

b∗(zj) =

(cid:35)

,

(cid:34)P ∗
x D(zt)Q∗
x
l D(zt)Q∗
P ∗
l
r D(zt)Q∗
P ∗
r
(cid:34)B∗
(cid:35)
xzt
B∗
l zt
B∗
r zt

,

where P ∈ R5d×z, Q ∈ Rz×3d, B ∈ R5d×z, and D(zt) ∈
Rz×z is the diagonal matrix of z.

With a small z and m, our dynamic TreeLSTM needs
a similar amount of parameters compared to the standard
TreeLSTM.

4 Application of Dynamic Compositional

Neural Networks

In this section, we describe two speciﬁc models to show the
applications of dynamic compositional neural networks for
two typical tasks in NLP.

4.1 Text Classiﬁcation
The purpose of text classiﬁcation is that, given a sentence x,
the model should predict labels ˆy from a pre-deﬁned label set
Y. From the description in the previous section, we can com-
pute the distributed representation hj of the phrase at node j
of a tree:

hj = DC-TreeNN(xj, hl

j, hr

j , θ)

(17)

After this recursive process, the hidden state hR at the root
node is used as the sentential representation, which then fol-
lowed by a softmax classiﬁer to predict the probability distri-
bution over classes.

ˆy = softmax(WthR + bt)

(18)

where ˆy is prediction probabilities, Wt and bt are the param-
eters of the classiﬁer.

(9)

(10)

(11)

4.2 Text Semantic Matching
Among many natural language processing (NLP) tasks, a
common problem is modelling the relevance of a pair of texts.
In this section, we show how to effectively use the dynamic
compositional neural networks to model the semantic rela-
tionship between two sentences.

Figure 2: Dynamic Compositional Neural Network. θ de-
notes the context-dependent parameters generated by meta
TreeNN.

zj = Wz

ˆhj

j ⊕ hr

j ∈ R2m+2d, Wm ∈
where Hj = hl
Rm×(2d+2m) and bm ∈ Rm are parameters of meta RecNN;
Wz ∈ Rz×m is a scale matrix.

j ⊕ ˆhr

j ⊕ ˆhl

To reduce the number of the parameters, we deﬁne the dy-
namic parameters with a low-rank factorized representation
of the weights, analogous to the Singular Value Decomposi-
tion. The dynamic parameters W(zj) and b(zj) of the basic
RecNN are computed by:

W(zj) =

b(zj) =

(cid:21)

(cid:20) PlD(zj)Ql
PrD(zj)Qr
(cid:21)
(cid:20)Blzj
Brzj

(6)

(7)

(8)

where P ∈ Rd×z, Q ∈ Rz×d, and D(zt) ∈ Rz×z is the
diagonal matrix of z.

Thus, our dynamic RecNN needs (6dz + mz) parameters,
while the vanilla RecNN has (2d2 + d) parameters. With a
small z and m, our dynamic RecNN needs less parameters
than the vanilla RecNN. For example, if we set d = 100 and
z = m = 20, our model needs 12, 400 parameters while the
vanilla model needs 20, 100 parameters.

3.2 Meta Network for TreeLSTM
Likewise, we also use a smaller meta network to generate the
static parameters W and b in Eq.(9) with the dynamic pa-
rameters W(zj) and b(zj). The meta network is a smaller
TreeLSTM, and the hidden state ˆhj ∈ Rm of node j in meta
network is deﬁned as








(cid:18)

=











Wm







tanh
σ
σ
σ
σ

ˆgj
ˆoj


ˆij


ˆf l

j
ˆf r
j
j (cid:12) ˆf l
ˆcj = ˆgj (cid:12)ˆij + ˆgl
ˆhj = ˆoj (cid:12) tanh (ˆcj) ,

(cid:21)

(cid:20) xj
Hj

(cid:19)

+ bm

,

j + ˆgr

j (cid:12) ˆf r
j ,

Hyper-Param.

IE MR SST

SUBJ QC

SICK

d
e
m=z

100
100
40

100
100
30

100
100
30

100
100
20

100
100
40

50(30)
50(50)
40(20)

Table 1: Hyper-parameters for our models on all tasks. m,
d denote the size of hidden state in meta and basic TreeNN
respectively. e and z represent the size of embedding vector
x and controlling vector z. The settings of our two proposed
models on all datasets are the same except SICK: the numbers
inside and outside parentheses correspond to DC-RecNN and
DC-TreeLSTM respectively.

Dataset

Train Dev.

Test

Class Lavg

|V|

MR
SST
SUBJ
IE
QC

9596
6920
9000
2221
5452

-
872
-
-
-

1066
1821
1000
300
500

2
2
2
3
6

22
18
21
16
10

21K
15K
21K
7.5K
9.4K

Table 2: Statistics of the ﬁve mainstream datasets for text clas-
siﬁcation. Lavg denotes the average length of documents; |V|
denotes the size of vocabulary.

• RNTN [Socher et al., 2013b]: The RNTN is a recur-
sive neural network with neural tensor layer, which can
model strong interactions between two constituents.
• MV-RecNN [Socher et al., 2012]: The MV-RecNN is to
represent every word and longer phrase in a parse tree as
both a vector and a matrix in order to model rich com-
positionality.

• TreeLSTM [Tai et al., 2015]: Recursive neural network

with Long Short-Term Memory unit.

5.3 Text Classiﬁcation
We evaluate our models on ﬁve different datasets. The de-
tailed statistics about the ﬁve datasets are listed in Table 2.
Each dataset is brieﬂy described as follows.

• SST The movie reviews with two classes (negative, pos-
itive) in the Stanford Sentiment Treebank [Socher et al.,
2013b].

• MR The movie reviews with two classes [Pang and Lee,

2005].

• QC The TREC questions dataset involves six different

question types. [Li and Roth, 2002].

• SUBJ Subjectivity dataset where the goal is to classify
each instance (snippet) as being subjective or objective.
[Pang and Lee, 2004]

• IE Idiom enhanced sentiment classiﬁcation. [Williams
et al., 2015]. Each sentence contains at least one idiom.

Results As shown in Table 3, DC-TreeLSTM consistently
outperforms RecNN, MV-RecNN, RNTN, and TreeLSTM
by a large margin while achieving comparable results to the
CNN and using much fewer parameters.(The number of pa-
rameters in our models is approximately 10K while in CNN

Figure 3: Illustration of DC-TreeNN Matching Network.

As shown in Figure 3, given two sentences xa and xb, the
representation of each sentence hR can be computed by one
basic TreeNN.
h(a)
R = DC-TreeNN(xR, hl
h(b)
R = DC-TreeNN(xR, hl

R, θa)
R, θb)

R, hr
R, hr

(20)

(19)

where R denotes the root node of a tree. θa and θb are gen-
erated by a shared meta TreeNN. Then, the representation of
each sentence will be fed into a multi-layer perceptron to ob-
tain a uniﬁed representation for the ﬁnal relationship classiﬁ-
cation.

The sample-speciﬁc but shared meta TreeNN ensures that,
on the one hand we can dynamically model the diversity of
semantic compositionality, on the other hand we can capture
the general rules across different samples.

5 Experiment
To make a comprehensive evaluation, we assess our model on
ﬁve text classiﬁcation tasks and a semantic matching task.

5.1 Training and Hyperparameters
Loss Function Given a sentence (or sentence pair) and its
label, the output of a neural network is the probabilities of the
different classes. The parameters of the network are trained to
minimise the cross-entropy of the predicted and true label dis-
tributions. To minimize the objective, we use stochastic gra-
dient descent with the diagonal variant of AdaGrad [Duchi et
al., 2011].

Initialization and Hyperparameters The word embed-
dings for all of the models are initialized with GloVe vec-
tors [Pennington et al., 2014]. The other parameters are ini-
tialized by randomly sampling from uniform distribution in
[−0.1, 0.1].

The ﬁnal hyper-parameters are as follows. The initial learn-
ing rate is 0.1. The regularization weight of the parameters is
1E−5 and the others are listed as Table 1.

For all the sentences in the datasets, we parse them with
constituency parser [Klein and Manning, 2003] to obtain the
trees for our models and some competitor models.

5.2 Competitor Methods

• RecNN [Socher et al., 2012]: Recursive neural network

with standard compositional function.

SUBJ

QC

will discuss later) therefore can more accurately understand
sentences.

Model

NBOW
DCNN
CNN-multichannel

RecNN
MV-RecNN
RNTN
TreeLSTM

DC-RecNN
DC-TreeLSTM

IE

54.6
-
-

52.0
54.8
55.7
56.0

58.2
60.2

MR

77.2
-
81.5

76.4
76.8
75.8
78.7

80.2
81.7

SST

80.5
86.8
88.1

82.4
82.9
85.4
86.9

86.1
87.8

91.3
-
93.4

90.6
90.9
92.1
91.0

93.5
93.7

88.2
93.0
93.6

88.8
89.2
88.9
91.6

91.2
93.8

Table 3: Accuracies of our models on ﬁve datasets against
state-of-the-art neural models. DCNN: Dynamic Convolu-
tional Neural Network [Kalchbrenner et al., 2014; Denil et
al., 2014]. CNN-multichannel: Convolutional Neural Net-
work [Kim, 2014].

Hidden Train Test

Model

NBOW
LSTM

RecNN
MV-RecNN
RNTN
TreeLSTM

30
100

30
50
50
50

DC-RecNN
30
DC-TreeLSTM 50

96.6
100.0

95.4
95.9
97.8
95.9

96.5
98.5

73.4
71.3

74.9
75.5
76.9
77.5

77.9
80.2

Table 4: Evaluation results of our models on the SICK train
and test sets.

the number of parameters is about 400K). Compared with
RecNN, DC-RecNN performs better, indicating the effective-
ness of the dynamic compositional function. Additionally,
both DC-RecNN and DC-TreeLSTM achieve substantial im-
provement on IE dataset, which covers the richness of com-
positionality (idiomaticity). We attribute the success on IE to
its power in modeling more complicated compositionality.

5.4 Text Semantic Matching
We choose the dataset of Sentences Involving Composi-
tional Knowledge (SICK), which is proposed by Marelli
et al. [2014] aiming at evaluation of compositional dis-
tributional semantic models. The dataset consists of 9927
sentence pairs in a 4500/500/4927 train/dev/test split, in
which each sentence pairs are pre-deﬁned into three labels:
“entailment”,“contradiction” and “neutral”.

Results Our results are summarized in Table 4, where the
performance of NBOW, LSTM, RecNN, and RNTN are re-
ported by [Bowman et al., 2015; Bowman et al., 2014].
For fair comparison, we train our models with the same
setting. We can see both DC-RecNN and DC-TreeLSTM
outperform competitor models, in which DC-RecNN (DC-
TreeLSTM) achieves 3% (2.7%) improvements than RecNN
(TreeLSTM). We think this breakthrough is basically at-
tributed to the dynamic compositional mechanism, which en-
ables our models to capture various syntactic patterns (As we

5.5 Discussion and Qualitative Analysis
In our models, the latent vector z controls the process of
predicting network’s parameters and its dimensionality de-
termines the number of model’s parameters. Next, we will
investigate how the controlling vector z inﬂuences the perfor-
mance of our models.

Impact of the Dimensionality Figure 4 shows the ac-
curacies of DC-RecNN across the different dimensions of
[5, 10, . . . , 50] for the controlling vector z on ﬁve datasets.
We get the following ﬁndings:

• For all ﬁve datasets, the model can achieve considerable
performances even when the size of vector z is reduced
to 5. Particularly, for the dataset QC, the model obtains
87.0% accuracy with a pretty small meta Tree-RecNN1,
suggesting a smaller meta network can be used for gen-
erating a more powerful compositional function to effec-
tively model sentence.

• When dealing with the dataset with more labels, larger
vector size leads to a better performance. For example,
the performance on IE and QC datasets reaches the max-
imum when the size of z equals 40, while for the other
three datasets MR, SST and SUBJ, the model obtains the
best performance with the value of 30, 30 and 20 respec-
tively.

Understanding the Neuron’s Behaviours As described in
previous sections, we know the compositional function is
changed cross child nodes over a tree, which is controlled
by a latent vector z. To get an intuitive understanding of how
the controlling vector z works, we design an experiment to
examine the neuron’s behaviours of z on each node. More
concretely, we refer to zjk as the activation of the k-neuron
at node j, where j ∈ {1, . . . , N } and k ∈ {1, . . . , z}. Then
we randomly sample some sentences on the development set
from the datasets we used. By visualizing the latent vector
zj and analyzing the maximum activation, we can ﬁnd what
kinds of patterns the current neuron focuses on.

Table 5 illustrates multiple interpretable neurons and some
representative words or phrases which can activate these neu-
rons. We can observe that:

• For some simple tasks such as text classiﬁcation,
meta network will integrate useful semantic information
into the the generation process of compositional func-
tion. These semantic bias before composition are task-
speciﬁc.
For example, the 21-st neuron is more sensitive to emo-
tional terms, which can be understood as a sentinel,
telling the basic neural network that an informative
phrase is coming, more attention should be paid in the

1With the same parameters, the RecNN obtain 74% accuracy in

our implementation

(a) IE

(b) MR

(c) SST

(d) SUBJ

(e) QC

Figure 4: Performances of DC-RecNN with the different sizes of latent vector z on ﬁve development datasets: IE, MR, SST,
SUBJ, and QC. Y-axis represents the accuracy(%), and X-axis represents dimensionality of z.

Type

Neurons

Examples

Explanations

Semantic

Lexical
Phrasal

Noun Phrase

Syntactic

Verb Phrase

Prep. Phrase

17-th
21-st

45-th
27-th
11-th
13-th

fun, glad, terriﬁc, wonderful, refreshing
pick holes, see red, in stitches, split hairs

Words related to sentiment
Phrases related to sentiment

blond boy, pink shirt, green grass, black dog
waking up, take off, pulling up, driving down
slicing a potato, playing guitar, chopping butter Verb-object phrases
on a track, in rocky area, on a stage, over water

Phrases related to places

Containing modiﬁers related to color
Phrases constructed by light-verb

Table 5: Multiple interpretable neurons and the words/phrases captured by these neurons. The last column gives the explanations
of corresponding neuron’s behaviours.

process of composition. Figure 5-(a) shows a visualiza-
tion. We can see in this sentence, the neuron has real-
ized that this idiomatic collocation “in stitches” is
a key pattern, which is crucial for the ﬁnal sentiment pre-
diction.

• For more complicated tasks such as semantic match-
ing, a well-grounded understanding of the syntactic
structure is crucial. In this context, we ﬁnd that a
meta network could capture some syntactic informa-
tion. For example, the 27-th neuron monitors phrases
constructed by light-verb. As shown in Figure 5-(b),
the verb phrase “taking off” has been attended for
forthcoming compositional operation, which is more
useful for judging the semantic relation between the
sentence pair “An airplane is taking off/A
plane is landing”.

6 Related Work
One thread of related work is the exploration of different
kinds of compositional function over tree structures. Socher
et al. [2012] proposed the recursive neural network with stan-
dard compositional function. After that, some extensions are
introduced to enhance the expressive power of compositional
function, such as MV-RecNN [Socher et al., 2013b], SU-
RNN [Socher et al., 2013a], RNTN [Socher et al., 2013b],
while these models suffer from the problem of hard-coded
compositional operations and overﬁtting.

Another thread of work is the idea of using one network
to direct the learning of another network [De Brabandere et
al., 2016]. Naik and Mammone [1992] introduce a meta neu-
ral network to provide another network with a step size and
a direction vector, which is helpful for parameter optimiza-
tion. De Brabandere et al. [2016] propose the dynamic ﬁlter

(a) The behaviour of 21-st neuron for sentence “She
had everyone in stitches”

(b) The behaviour of 27-th neuron for sentence “An
airplane is taking off”

Figure 5: The two heat maps describe the behaviours of neu-
rons z21 and z27 from DC-TreeNN.

network to implicitly learn a variety of ﬁltering operations.
Bertinetto et al. [2016] introduce a learnet for one-shot learn-
ing, which can predict the parameters of a second network
given a single exemplar. Ha et al. [2016] propose the model
hypernetwork, which uses a small network to generate the
weights for a larger network.

Different from these models, we employ the idea of param-

eter generation to address the limitation of weight-sharing or
partially sharing paradigm of tree-based compositional mod-
els.

7 Conclusion

In this work, we introduce a meta neural network, which can
generate a compositional network to dynamically compose
constituents over tree structure. The parameters of composi-
tional function vary from position to position and from sam-
ple to sample, allowing for more sophisticated operations on
the input.

To evaluate our models, we choose two typical NLP tasks
involving six datasets. The qualitative and quantitative exper-
iment results demonstrate the effectiveness of our models.

Acknowledgments

We would like to thank the anonymous reviewers for their
valuable comments and thank Kaiyu Qian, Jiachen Xu, Ji-
fan Chen for useful discussions. This work was partially
funded by National Natural Science Foundation of China
(No. 61532011 and 61672162), Shanghai Municipal Science
and Technology Commission (No. 16JC1420401).

References

[Bertinetto et al., 2016] Luca Bertinetto, Jo˜ao F Henriques,
Jack Valmadre, Philip Torr, and Andrea Vedaldi. Learn-
ing feed-forward one-shot learners. In Advances in NIPS,
pages 523–531, 2016.

[Bowman et al., 2014] Samuel R Bowman, Christopher
Potts, and Christopher D Manning. Recursive neural
arXiv preprint
networks can learn logical semantics.
arXiv:1406.1827, 2014.

[Bowman et al., 2015] Samuel R. Bowman, Gabor Angeli,
Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on EMNLP, 2015.

[Cho et al., 2014] Kyunghyun Cho, Bart van Merrienboer,
Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using
rnn encoder-decoder for statistical machine translation. In
Proceedings of EMNLP, 2014.

[Dong et al., 2014] Li Dong, Furu Wei, Chuanqi Tan, Duyu
Tang, Ming Zhou, and Ke Xu. Adaptive recursive neural
network for target-dependent twitter sentiment classiﬁca-
tion. In ACL (2), pages 49–54, 2014.

[Duchi et al., 2011] John Duchi, Elad Hazan, and Yoram
Singer. Adaptive subgradient methods for online learning
and stochastic optimization. The JMLR, 12:2121–2159,
2011.

[Ha et al., 2016] David Ha, Andrew Dai, and Quoc V Le.
Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.

[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and
J¨urgen Schmidhuber. Long short-term memory. Neural
computation, 9(8):1735–1780, 1997.

[Irsoy and Cardie, 2014] Ozan Irsoy and Claire Cardie. Deep
recursive neural networks for compositionality in lan-
guage. In Advances in NIPS, pages 2096–2104, 2014.

[Kalchbrenner et al., 2014] Nal Kalchbrenner,

Edward
Grefenstette, and Phil Blunsom. A convolutional neural
network for modelling sentences. In Proceedings of ACL,
2014.

[Kim, 2014] Yoon Kim. Convolutional neural networks for
sentence classiﬁcation. arXiv preprint arXiv:1408.5882,
2014.

[Klein and Manning, 2003] Dan Klein and Christopher D
Manning. Accurate unlexicalized parsing. In Proceedings
of the 41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430, 2003.

[Li and Roth, 2002] Xin Li and Dan Roth. Learning ques-
tion classiﬁers. In Proceedings of the 19th International
Conference on Computational Linguistics, pages 556–562,
2002.

[Liu et al., 2015] PengFei Liu, Xipeng Qiu, Xinchi Chen,
Shiyu Wu, and Xuanjing Huang. Multi-timescale long
short-term memory neural network for modelling sen-
tences and documents. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing,
2015.

[Liu et al., 2016a] Pengfe Liu, Xipeng Qiu, Jifan Chen, and
Xuanjing Huang. Deep fusion LSTMs for text semantic
matching. In Proceedings of ACL, 2016.

[Collobert et al., 2011] Ronan Collobert,

Jason Weston,
L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and
Pavel Kuksa. Natural language processing (almost) from
scratch. The JMLR, 12:2493–2537, 2011.

[Liu et al., 2016b] Pengfei Liu, Xipeng Qiu, Yaqian Zhou,
Jifan Chen, and Xuanjing Huang. Modelling interaction
of sentence pair with coupled-lstms. In Proceedings of the
2016 Conference on EMNLP, November 2016.

[De Brabandere et al., 2016] Bert De Brabandere, Xu Jia,
Tinne Tuytelaars, and Luc Van Gool. Dynamic ﬁlter net-
works. In NIPS, 2016.

[Denil et al., 2014] Misha Denil, Alban Demiraj, Nal Kalch-
brenner, Phil Blunsom, and Nando de Freitas. Mod-
elling, visualising and summarising documents with a
arXiv preprint
single convolutional neural network.
arXiv:1406.3830, 2014.

[Marelli et al., 2014] Marco Marelli, Luisa Bentivogli,
Marco Baroni, Raffaella Bernardi, Stefano Menini, and
Roberto Zamparelli. Semeval-2014 task 1: Evaluation of
compositional distributional. SemEval-2014, 2014.

[Naik and Mammone, 1992] Devang K Naik and RJ Mam-
In
mone. Meta-neural networks that learn by learning.
Neural Networks, 1992. IJCNN., volume 1, pages 437–
442. IEEE, 1992.

[Pang and Lee, 2004] Bo Pang and Lillian Lee. A sentimen-
tal education: Sentiment analysis using subjectivity sum-
In Proceedings of
marization based on minimum cuts.
ACL, 2004.

[Pang and Lee, 2005] Bo Pang and Lillian Lee. Seeing stars:
Exploiting class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the ACL,
pages 115–124, 2005.

[Pennington et al., 2014] Jeffrey

Richard
Socher, and Christopher D Manning. Glove: Global
the
vectors for word representation.
EMNLP, 12:1532–1543, 2014.

Proceedings of

Pennington,

[Pollack, 1990] Jordan B Pollack. Recursive distributed rep-
resentations. Artiﬁcial Intelligence, 46(1):77–105, 1990.
[Pylkk¨anen and McElree, 2006] Liina Pylkk¨anen and Brian
McElree. The syntax-semantics interface: On-line compo-
sition of sentence meaning. Handbook of psycholinguis-
tics, 2:537–577, 2006.

[Socher et al., 2012] Richard Socher, Brody Huval, Christo-
pher D Manning, and Andrew Y Ng. Semantic composi-
tionality through recursive matrix-vector spaces. In Pro-
ceedings of EMNLP, pages 1201–1211, 2012.

[Socher et al., 2013a] Richard Socher, John Bauer, Christo-
pher D Manning, and Andrew Y Ng. Parsing with compo-
sitional vector grammars. In Proceedings of ACL, 2013.
[Socher et al., 2013b] Richard Socher, Alex Perelygin,
Jean Y Wu, Jason Chuang, Christopher D Manning,
Andrew Y Ng, and Christopher Potts. Recursive deep
models for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP, 2013.

[Sutskever et al., 2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. Sequence to sequence learning with neural
networks. In Advances in NIPS, pages 3104–3112, 2014.
[Tai et al., 2015] Kai Sheng Tai, Richard Socher, and
Christopher D Manning.
Improved semantic representa-
tions from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075, 2015.

[Williams et al., 2015] Lowri Williams, Christian Bannister,
Michael Arribas-Ayllon, Alun Preece, and Irena Spasi´c.
The role of idioms in sentiment analysis. Expert Systems
with Applications, 42(21):7375–7385, 2015.

[Zhao et al., 2015] Han Zhao, Zhengdong Lu, and Pascal
Poupart. Self-adaptive hierarchical sentence model. arXiv
preprint arXiv:1504.05070, 2015.

[Zhu et al., 2015] Xiao-Dan Zhu, Parinaz Sobhani, and
Hongyu Guo. Long short-term memory over recursive
structures. In ICML, pages 1604–1612, 2015.

