Mutual Information Neural Estimation

8
1
0
2
 
n
u
J
 
7
 
 
]

G
L
.
s
c
[
 
 
4
v
2
6
0
4
0
.
1
0
8
1
:
v
i
X
r
a

Mohamed Ishmael Belghazi 1 Aristide Baratin 1 2 Sai Rajeswar 1 Sherjil Ozair 1 Yoshua Bengio 1 3 4
Aaron Courville 1 3 R Devon Hjelm 1 4

Abstract
We argue that the estimation of mutual informa-
tion between high dimensional continuous ran-
dom variables can be achieved by gradient descent
over neural networks. We present a Mutual Infor-
mation Neural Estimator (MINE) that is linearly
scalable in dimensionality as well as in sample
size, trainable through back-prop, and strongly
consistent. We present a handful of applications
on which MINE can be used to minimize or max-
imize mutual information. We apply MINE to im-
prove adversarially trained generative models. We
also use MINE to implement the Information Bot-
tleneck, applying it to supervised classiﬁcation;
our results demonstrate substantial improvement
in ﬂexibility and performance in these settings.

1. Introduction

Mutual information is a fundamental quantity for measuring
the relationship between random variables. In data science
it has found applications in a wide range of domains and
tasks, including biomedical sciences (Maes et al., 1997),
blind source separation (BSS, e.g., independent component
analysis, Hyv¨arinen et al., 2004), information bottleneck (IB,
Tishby et al., 2000), feature selection (Kwak & Choi, 2002;
Peng et al., 2005), and causality (Butte & Kohane, 2000).

Put simply, mutual information quantiﬁes the dependence
of two random variables X and Z. It has the form,

I(X; Z) =

(cid:90)

X ×Z

log

dPXZ
dPX ⊗ PZ

dPXZ,

(1)

where PXZ is the joint probability distribution, and PX =
Z dPXZ and PZ = (cid:82)
(cid:82)
X dPXZ are the marginals. In con-

1Montr´eal

Institute for Learning Algorithms (MILA),
2Department of Mathematics and
University of Montr´eal
Statistics, McGill University 3Canadian Institute for Ad-
vanced Research (CIFAR) 4The Institute for Data Valorization
(IVADO). Correspondence to: Mohamed Ishmael Belghazi <ish-
mael.belghazi@gmail.com>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

trast to correlation, mutual information captures non-linear
statistical dependencies between variables, and thus can act
as a measure of true dependence (Kinney & Atwal, 2014).

Despite being a pivotal quantity across data science, mutual
information has historically been difﬁcult to compute (Panin-
ski, 2003). Exact computation is only tractable for discrete
variables (as the sum can be computed exactly), or for a
limited family of problems where the probability distribu-
tions are known. For more general problems, this is not
possible. Common approaches are non-parametric (e.g.,
binning, likelihood-ratio estimators based on support vector
machines, non-parametric kernel-density estimators; see,
Fraser & Swinney, 1986; Darbellay & Vajda, 1999; Suzuki
et al., 2008; Kwak & Choi, 2002; Moon et al., 1995; Kraskov
et al., 2004), or rely on approximate gaussianity of data
distribution (e.g., Edgeworth expansion, Van Hulle, 2005).
Unfortunately, these estimators typically do not scale well
with sample size or dimension (Gao et al., 2014), and thus
cannot be said to be general-purpose. Other recent works
include Kandasamy et al. (2017); Singh & Pczos (2016);
Moon et al. (2017).

In order to achieve a general-purpose estimator, we rely
on the well-known characterization of the mutual informa-
tion as the Kullback-Leibler (KL-) divergence (Kullback,
1997) between the joint distribution and the product of the
marginals (i.e., I(X; Z) = DKL(PXZ || PX ⊗ PZ)). Re-
cent work uses a dual formulation to cast the estimation of
f -divergences (including the KL-divergence, see Nguyen
et al., 2010) as part of an adversarial game between com-
peting deep neural networks (Nowozin et al., 2016). This
approach is at the cornerstone of generative adversarial net-
works (GANs, Goodfellow et al., 2014), which train a
generative model without any explicit assumptions about
the underlying distribution of the data.

In this paper we demonstrate that exploiting dual optimiza-
tion to estimate divergences goes beyond the minimax ob-
jective as formalized in GANs. We leverage this strategy
to offer a general-purpose parametric neural estimator of
mutual information based on dual representations of the
KL-divergence (Ruderman et al., 2012), which we show
is valuable in settings that do not necessarily involve an
adversarial game. Our estimator is scalable, ﬂexible, and
completely trainable via back-propagation. The contribu-

tions of this paper are as follows:

2.2. Dual representations of the KL-divergence.

Mutual Information Neural Estimation

• We introduce the Mutual Information Neural Estimator
(MINE), which is scalable, ﬂexible, and completely
trainable via back-prop, as well as provide a thorough
theoretical analysis.

• We show that the utility of this estimator transcends
the minimax objective as formalized in GANs, such
that it can be used in mutual information estimation,
maximization, and minimization.

• We apply MINE to palliate mode-dropping in GANs
and to improve reconstructions and inference in Ad-
versarially Learned Inference (ALI, Dumoulin et al.,
2016) on large scale datasets.

• We use MINE to apply the Information Bottleneck
method (Tishby et al., 2000) in a continuous setting,
and show that this approach outperforms variational
bottleneck methods (Alemi et al., 2016).

2. Background

2.1. Mutual Information

Mutual information is a Shannon entropy-based measure of
dependence between random variables. The mutual infor-
mation between X and Z can be understood as the decrease
of the uncertainty in X given Z:

I(X; Z) := H(X) − H(X | Z),

(2)

where H is the Shannon entropy, and H(X | Z) is the con-
ditional entropy of Z given X. As stated in Eqn. 1 and the
discussion above, the mutual information is equivalent to
the Kullback-Leibler (KL-) divergence between the joint,
PXZ, and the product of the marginals PX ⊗ PZ:

I(X, Z) = DKL(PXZ || PX ⊗ PZ),

where DKL is deﬁned as1,

DKL(P || Q) := EP

log

(cid:20)

(cid:21)

.

dP
dQ

(3)

(4)

whenever P is absolutely continuous with respect to Q2.

The intuitive meaning of Eqn. 3 is clear: the larger the di-
vergence between the joint and the product of the marginals,
the stronger the dependence between X and Z. This di-
vergence, hence the mutual information, vanishes for fully
independent variables.

1Although the discussion is more general, we can think of P
and Q as being distributions on some compact domain Ω ⊂ Rd,
with density p and q respect the Lebesgue measure λ, so that
DKL = (cid:82) p log p

q dλ.

2and inﬁnity otherwise.

A key technical ingredient of MINE are dual representa-
tions of the KL-divergence. We will primarily work with
the Donsker-Varadhan representation (Donsker & Varadhan,
1983), which results in a tighter estimator; but will also con-
sider the dual f -divergence representation (Keziou, 2003;
Nguyen et al., 2010; Nowozin et al., 2016).

The Donsker-Varadhan representation. The following
theorem gives a representation of the KL-divergence
(Donsker & Varadhan, 1983):

Theorem 1 (Donsker-Varadhan representation). The KL
divergence admits the following dual representation:

DKL(P || Q) = sup
T :Ω→R

EP[T ] − log(EQ[eT ]),

(5)

where the supremum is taken over all functions T such that
the two expectations are ﬁnite.

Proof. See the Supplementary Material.

A straightforward consequence of Theorem 1 is as follows.
Let F be any class of functions T : Ω → R satisfying the
integrability constraints of the theorem. We then have the
lower-bound3:

DKL(P || Q) ≥ sup
T ∈F

EP[T ] − log(EQ[eT ]).

(6)

Note also that the bound is tight for optimal functions T ∗
that relate the distributions to the Gibbs density as,

dP =

eT ∗

dQ, where Z = EQ[eT ∗

].

(7)

1
Z

The f -divergence representation.
It is worthwhile to
compare the Donsker-Varadhan representation to the f -
divergence representation proposed in Nguyen et al. (2010);
Nowozin et al. (2016), which leads to the following bound:

DKL(P || Q) ≥ sup
T ∈F

EP[T ] − EQ[eT −1].

(8)

Although the bounds in Eqns. 6 and 8 are tight for sufﬁ-
ciently large families F, the Donsker-Varadhan bound is
stronger in the sense that, for any ﬁxed T , the right hand side
of Eqn. 6 is larger4 than the right hand side of Eqn. 8. We
refer to the work by Ruderman et al. (2012) for a derivation
of both representations in Eqns. 6 and 8 from the unifying
perspective of Fenchel duality. In Section 3 we discuss
versions of MINE based on these two representations, and
numerical comparisons are performed in Section 4.

3The bound in Eqn. 6 is known as the compression lemma in

the PAC-Bayes literature (Banerjee, 2006).

4To see this, just apply the identity x ≥ e log x with x =

EQ[eT ].

3. The Mutual Information Neural Estimator

Algorithm 1 MINE

Mutual Information Neural Estimation

In this section we formulate the framework of the Mutual
Information Neural Estimator (MINE). We deﬁne MINE
and present a theoretical analysis of its consistency and
convergence properties.

3.1. Method

Using both Eqn. 3 for the mutual information and the dual
representation of the KL-divergence, the idea is to choose F
to be the family of functions Tθ : X × Z → R parametrized
by a deep neural network with parameters θ ∈ Θ. We call
this network the statistics network. We exploit the bound:

I(X; Z) ≥ IΘ(X, Z),

(9)

where IΘ(X, Z) is the neural information measure deﬁned
as

IΘ(X, Z) = sup
θ∈Θ

EPXZ [Tθ] − log(EPX ⊗PZ [eTθ ]).

(10)

The expectations in Eqn. 10 are estimated using empirical
samples5 from PXZ and PX ⊗ PZ or by shufﬂing the sam-
ples from the joint distribution along the batch axis. The
objective can be maximized by gradient ascent.

It should be noted that Eqn. 10 actually deﬁnes a new class
information measures, The expressive power of neural net-
work insures that they can approximate the mutual informa-
tion with arbitrary accuracy.
In what follows, given a distribution P, we denote by ˆP(n)
as the empirical distribution associated to n i.i.d. samples.

Deﬁnition 3.1 (Mutual Information Neural Estimator
(MINE)). Let F = {Tθ}θ∈Θ be the set of functions
parametrized by a neural network. MINE is deﬁned as,

θ ← initialize network parameters
repeat

Draw b minibatch samples from the joint distribution:
(x(1), z(1)), . . . , (x(b), z(b)) ∼ PXZ
Draw n samples from the Z marginal distribution:
¯z(1), . . . , ¯z(b) ∼ PZ
Evaluate the lower-bound:

b

(cid:80)b

(cid:80)b

V(θ) ← 1
b

i=1 Tθ(x(i), z(i)) − log( 1

i=1 eTθ (x(i), ¯z(i)))
Evaluate bias corrected gradients (e.g., moving aver-
age):
(cid:98)G(θ) ← (cid:101)∇θV(θ)
Update the statistics network parameters:
θ ← θ + (cid:98)G(θ)
until convergence

3.2. Correcting the bias from the stochastic gradients

A naive application of stochastic gradient estimation leads
to the gradient estimate:

(cid:98)GB = EB[∇θTθ] −

EB[∇θTθ eTθ ]
EB [eTθ ]

.

(12)

where, in the second term, the expectations are over the
samples of a minibatch B, leads to a biased estimate of the
full batch gradient6.

Fortunately, the bias can be reduced by replacing the esti-
mate in the denominator by an exponential moving average.
For small learning rates, this improved MINE gradient esti-
mator can be made to have arbitrarily small bias.
We found in our experiments that this improves all-around
performance of MINE.

3.3. Theoretical properties

In this section we analyze the consistency and convergence
properties of MINE. All the proofs can be found in the
Supplementary Material.

(cid:92)I(X; Z)n = sup

E

P(n)
XZ

θ∈Θ

[Tθ] − log(E

X ⊗ˆP(n)
P(n)

Z

[eTθ ]). (11)

3.3.1. CONSISTENCY

Details on the implementation of MINE are provided in
Algorithm 1. An analogous deﬁnition and algorithm also
hold for the f -divergence formulation in Eqn. 8, which we
refer to as MINE-f . Since Eqn. 8 lower-bounds Eqn. 6, it
generally leads to a looser estimator of the mutual informa-
tion, and numerical comparisons of MINE with MINE-f
can be found in Section 4. However, in a mini-batch setting,
the SGD gradients of MINE are biased. We address this in
the next section.

MINE relies on a choice of (i) a statistics network and (ii)
n samples from the data distribution PXZ.

Deﬁnition 3.2 (Strong consistency). The estimator
(cid:92)I(X; Z)n is strongly consistent if for all (cid:15) > 0, there exists
a positive integer N and a choice of statistics network such
that:

∀n ≥ N,

|I(X, Z) − (cid:92)I(X; Z)n| ≤ (cid:15), a.e.

where the probability is over a set of samples.

5Note that samples ¯x ∼ PX and ¯z ∼ PZ from the marginals
are obtained by simply dropping x, z from samples (¯x, z) and
(x, ¯z) ∼ PXZ .

6From the optimization point of view, the f -divergence formu-
lation has the advantage of making the use of SGD with unbiased
gradients straightforward.

Mutual Information Neural Estimation

In a nutshell, the question of consistency is divided into two
problems: an approximation problem related to the size of
the family, F, and an estimation problem related to the use
of empirical measures. The ﬁrst problem is addressed by uni-
versal approximation theorems for neural networks (Hornik,
1989). For the second problem, classical consistency theo-
rems for extremum estimators apply (Van de Geer, 2000)
under mild conditions on the parameter space.

This leads to the two lemmas below. The ﬁrst lemma states
that the neural information measures IΘ(X, Z), deﬁned
in Eqn. 10, can approximate the mutual information with
arbitrary accuracy:

Lemma 1 (approximation). Let (cid:15) > 0. There exists a neural
network parametrizing functions Tθ with parameters θ in
some compact domain Θ ⊂ Rk, such that

|I(X, Z) − IΘ(X, Z)| ≤ (cid:15), a.e.

The second lemma states the almost sure convergence of
MINE to a neural information measure as the number of
samples goes to inﬁnity:

Lemma 2 (estimation). Let (cid:15) > 0. Given a family of neural
network functions Tθ with parameters θ in some bounded
domain Θ ⊂ Rk, there exists an N ∈ N, such that
| (cid:92)I(X; Z)n − IΘ(X, Z) |≤ (cid:15), a.e.

∀n ≥ N,

(13)

Combining the two lemmas with the triangular inequality,
we have,

Theorem 2. MINE is strongly consistent.

3.3.2. SAMPLE COMPLEXITY

In this section we discuss the sample complexity of our esti-
mator. Since the focus here is on the empirical estimation
problem, we assume that the mutual information is well
enough approximated by the neural information measure
IΘ(X, Z). The theorem below is a reﬁnement of Lemma 2:
it gives how many samples we need for an empirical estima-
tion of the neural information measure at a given accuracy
and with high conﬁdence.

We make the following assumptions: the functions Tθ are
M -bounded (i.e., |Tθ| ≤ M ) and L-Lipschitz with respect
to the parameters θ. The domain Θ ⊂ Rd is bounded, so
that (cid:107)θ(cid:107) ≤ K for some constant K. The theorem below
shows a sample complexity of (cid:101)O
, where d is the
dimension of the parameter space.

(cid:16) d log d
(cid:15)2

(cid:17)

Theorem 3. Given any values (cid:15), δ of the desired accuracy
and conﬁdence parameters, we have,

(cid:16)

Pr

| (cid:92)I(X; Z)n − IΘ(X, Z)| ≤ (cid:15)

(cid:17)

≥ 1 − δ,

(14)

whenever the number n of samples satisﬁes

2M 2(d log(16KL

d/(cid:15)) + 2dM + log(2/δ))

n ≥

√

(cid:15)2

.
(15)

4. Empirical comparisons

Before diving into applications, we perform some simple
empirical evaluation and comparisons of MINE. The ob-
jective is to show that MINE is effectively able to estimate
mutual information and account for non-linear dependence.

4.1. Comparing MINE to non-parametric estimation

a, X j

We compare MINE and MINE-f to the k-NN-based non-
parametric estimator found in Kraskov et al. (2004). In
our experiment, we consider multivariate Gaussian ran-
dom variables, Xa and Xb, with componentwise correla-
tion, corr(X i
b ) = δij ρ, where ρ ∈ (−1, 1) and δij is
Kronecker’s delta. As the mutual information is invariant
to continuous bijective transformations of the considered
variables, it is enough to consider standardized Gaussians
marginals. We also compare MINE (using the Donsker-
Varadhan representation in Eqn. 6) and MINE-f (based on
the f -divergence representation in Eqn. 8).

Our results are presented in Figs. 1. We observe that both
MINE and Kraskov’s estimation are virtually indistinguish-
able from the ground truth when estimating the mutual infor-
mation between bivariate Gaussians. MINE shows marked
improvement over Krakov’s when estimating the mutual
information between twenty dimensional random variables.
We also remark that MINE provides a tighter estimate of
the mutual information than MINE-f .

Figure 1. Mutual information between two multivariate Gaussians
with component-wise correlation ρ ∈ (−1, 1).

4.2. Capturing non-linear dependencies

An important property of mutual information between ran-
dom variables with relationship Y = f (X)+σ (cid:12)(cid:15), where f
is a deterministic non-linear transformation and (cid:15) is random
noise, is that it is invariant to the deterministic nonlinear
transformation, but should only depend on the amount of
noise, σ (cid:12) (cid:15). This important property, that guarantees the
quantiﬁcation dependence without bias for the relationship,

Mutual Information Neural Estimation

is called equitability (Kinney & Atwal, 2014). Our results
(Fig. 2) show that MINE captures this important property.

We propose to palliate mode collapse by maximizing the
mutual information between the samples and the code.
I(G([(cid:15), c]); c) = H(G([(cid:15), c])) − H(G([(cid:15), c]) | c). The
generator objective then becomes,

arg max
G

E[log(D(G([(cid:15), c])))] + βI(G([(cid:15), c]); c).

(17)

As the samples G([(cid:15), c]) are differentiable w.r.t. the param-
eters of G, and the statistics network being a differentiable
function, we can maximize the mutual information using
back-propagation and gradient ascent by only specifying
this additional loss term. Since the mutual information is
theoretically unbounded, we use adaptive gradient clipping
(see the Supplementary Material) to ensure that the genera-
tor receives learning signals similar in magnitude from the
discriminator and the statistics network.

Related works on mode-dropping Methods to address
mode dropping in GANs can readily be found in the litera-
ture. Salimans et al. (2016) use mini-batch discrimination.
In the same spirit, Lin et al. (2017) successfully mitigates
mode dropping in GANs by modifying the discriminator
to make decisions on multiple real or generated samples.
Ghosh et al. (2017) uses multiple generators that are encour-
aged to generate different parts of the target distribution.
Nguyen et al. (2017) uses two discriminators to minimize
the KL and reverse KL divergences between the target and
generated distributions. Che et al. (2016) learns a reconstruc-
tion distribution, then teach the generator to sample from
it, the intuition being that the reconstruction distribution is
a de-noised or smoothed version of the data distribution,
and thus easier to learn. Srivastava et al. (2017) minimizes
the reconstruction error in the latent space of bi-directional
GANs (Dumoulin et al., 2016; Donahue et al., 2016). Metz
et al. (2017) includes many steps of the discriminator’s opti-
mization as part of the generator’s objective. While Chen
et al. (2016) maximizes the mutual information between the
code and the samples, it does so by minimizing a variational
upper bound on the conditional entropy (Barber & Agakov,
2003) therefore ignoring the entropy of the samples. Chen
et al. (2016) makes no claim about mode-dropping.

Experiments: Spiral, 25-Gaussians datasets We apply
MINE to improve mode coverage when training a genera-
tive adversarial network (GAN, Goodfellow et al., 2014).
We demonstrate using Eqn. 17 on the spiral and the 25-
Gaussians datasets, comparing two models, one with β = 0
(which corresponds to the orthodox GAN as in Goodfellow
et al. (2014)) and one with β = 1.0, which corresponds to
mutual information maximization.

Our results on the spiral (Fig. 3) and the 25-Gaussians
(Fig. 4) experiments both show improved mode coverage
over the baseline with no mutual information objective. This

Figure 2. MINE is invariant to choice of deterministic nonlinear
transformation. The heatmap depicts mutual information estimated
by MINE between 2-dimensional random variables X ∼ U(−1, 1)
and Y = f (X) + σ (cid:12) (cid:15), where f (x) ∈ {x, x3, sin(x)} and
(cid:15) ∼ N (0, I).

5. Applications

In this section, we use MINE to present applications of
mutual information and compare to competing methods de-
signed to achieve the same goals. Speciﬁcally, by using
MINE to maximize the mutual information, we are able to
improve mode representation and reconstruction of genera-
tive models. Finally, by minimizing mutual information, we
are able to effectively implement the information bottleneck
in a continuous setting.

5.1. Maximizing mutual information to improve GANs

Mode collapse (Che et al., 2016; Dumoulin et al., 2016;
Donahue et al., 2016; Salimans et al., 2016; Metz et al.,
2017; Saatchi & Wilson, 2017; Nguyen et al., 2017; Lin
et al., 2017; Ghosh et al., 2017) is a common pathology of
generative adversarial networks (GANs, Goodfellow et al.,
2014), where the generator fails to produces samples with
sufﬁcient diversity (i.e., poorly represent some modes).

GANs as formulated in Goodfellow et al. (2014) consist
of two components: a discriminator, D : X → [0, 1] and
a generator, G : Z → X , where X is a domain such as
a compact subspace of Rn. Given Z ∈ Z follows some
simple prior distribution (e.g., a spherical Gaussian with
density, PZ), the goal of the generator is to match its output
distribution to a target distribution, PX (speciﬁed by the data
samples). The discriminator and generator are optimized
through the value function,

min
G

max
D

V (D, G) :=

EPX [D(X)] + EPZ [log (1 − D(G(Z))].

(16)

A natural approach to diminish mode collapse would be
regularizing the generator’s loss with the neg-entropy of the
samples. As the sample entropy is intractable, we propose
to use the mutual information as a proxy.

Following Chen et al. (2016), we write the prior as the
concatenation of noise and code variables, Z = [(cid:15), c].

Mutual Information Neural Estimation

(a) GAN

(b) GAN+MINE

GAN+MINE (Ours)

1000.0 ± 0.0

Figure 3. The generator of the GAN model without mutual in-
formation maximization after 5000 iterations suffers from mode
collapse (has poor coverage of the target dataset) compared to
GAN+MINE on the spiral experiment.

DCGAN
ALI
Unrolled GAN
VEEGAN
PacGAN

Stacked MNIST

Modes
(Max 1000)

99.0
16.0
48.7
150.0
1000.0 ± 0.0

KL

3.40
5.40
4.32
2.95
0.06 ± 1.0e−2

0.05 ± 6.9e−3

Table 1. Number of captured modes and Kullblack-Leibler diver-
gence between the training and samples distributions for DC-
GAN (Radford et al., 2015), ALI (Dumoulin et al., 2016), Un-
rolled GAN (Metz et al., 2017), VeeGAN (Srivastava et al., 2017),
PacGAN (Lin et al., 2017).

and reverse (decoder) models, respectively7.

One goal of bi-directional models is to do inference as well
as to learn a good generative model. Reconstructions are
one desirable property of a model that does both inference
and generation, but in practice ALI can lack ﬁdelity (i.e.,
reconstructs less faithfully than desired, see Li et al., 2017;
Ulyanov et al., 2017; Belghazi et al., 2018). To demonstrate
the connection to mutual information, it can be shown (see
the Supplementary Material for details) that the reconstruc-
tion error, R, is bounded by,

R ≤ DKL(q(x, z) || p(x, z)) − Iq(x, z) + Hq(z) (18)

If the joint distributions are matched, Hq(z) tends to Hp(z),
which is ﬁxed as long as the prior, p(z), is itself ﬁxed. Sub-
sequently, maximizing the mutual information minimizes
the expected reconstruction error.

Assuming that the generator is the same as with GANs in the
previous section, the objectives for training a bi-directional
adversarial model then become:

arg max
D
arg max
F,G

Eq(x,z)[log D(x, z)] + Ep(x,z)[log (1 − D(x, z))]

Eq(x,z)[log (1 − D(x, z))] + Ep(x,z)[log D(x, z)]

+ βIq(x, z).

(19)

(a) Original data

(b) GAN

(c) GAN+MINE

Figure 4. Kernel density estimate (KDE) plots for GAN+MINE
samples and GAN samples on 25 Gaussians dataset.

conﬁrms our hypothesis that maximizing mutual informa-
tion helps against mode-dropping in this simple setting.

Experiment: Stacked MNIST Following Che et al.
(2016); Metz et al. (2017); Srivastava et al. (2017); Lin
et al. (2017), we quantitatively assess MINE’s ability to
diminish mode dropping on the stacked MNIST dataset
which is constructed by stacking three randomly sampled
MNIST digits. As a consequence, stacked MNIST offers
1000 modes. Using the same architecture and training pro-
tocol as in Srivastava et al. (2017); Lin et al. (2017), we
train a GAN on the constructed dataset and use a pre-trained
classiﬁer on 26,000 samples to count the number of modes
in the samples, as well as to compute the KL divergence
between the sample and expected data distributions. Our
results in Table 1 demonstrate the effectiveness of MINE in
preventing mode collapse on Stacked MNIST.

5.2. Maximizing mutual information to improve

inference in bi-directional adversarial models

Adversarial bi-directional models were introduced in Ad-
versarially Learned Inference (ALI, Dumoulin et al., 2016)
and BiGAN (Donahue et al., 2016) and are an extension
of GANs which incorporate a reverse model, F : X → Z
jointly trained with the generator. These models formulate
the problem in terms of the value function in Eqn. 16 be-
tween two joint distributions, p(x, z) = p(z | x)p(x) and
q(x, z) = q(x | z)p(z) induced by the forward (encoder)

Related works Ulyanov et al. (2017) improves re-
constructions quality by forgoing the discriminator and
expressing the adversarial game between the encoder and
decoder. Kumar et al. (2017) augments the bi-directional
objective by considering the reconstruction and the corre-
sponding encodings as an additional fake pair. Belghazi
et al. (2018) shows that a Markovian hierarchical generator
in a bi-directional adversarial model provide a hierarchy of

7We switch to density notations for convenience throughout

this section.

Mutual Information Neural Estimation

(a) Training set

(b) DCGAN

(c) DCGAN+MINE

Figure 5. Samples from the Stacked MNIST dataset along with generated samples from DCGAN and DCGAN with MINE. While
DCGAN only shows a very limited number of modes, the inclusion of MINE generates a much better representative set of samples.

reconstructions with increasing levels of ﬁdelity (increasing
reconstruction quality). Li et al. (2017) shows that the
expected reconstruction error can be diminished by
minimizing the conditional entropy of the observables given
the latent representations. The conditional entropy being
intractable for general posterior, Li et al. (2017) proposes
to augment the generator’s loss with an adversarial cycle
consistency loss (Zhu et al., 2017) between the observables
and their reconstructions.
Experiment: ALI+MINE In this section we compare
MINE to existing bi-directional adversarial models. As the
decoder’s density is generally intractable, we use three dif-
ferent metrics to measure the ﬁdelity of the reconstructions
with respect to the samples; (i) the euclidean reconstruction
error, (ii) reconstruction accuracy, which is the proportion
of labels preserved by the reconstruction as identiﬁed by a
pre-trained classiﬁer; (iii) the Multi-scale structural sim-
ilarity metric (MS-SSIM, Wang et al., 2004) between the
observables and their reconstructions.

We train MINE on datasets of increasing order of complex-
ity: a toy dataset composed of 25-Gaussians, MNIST (Le-
Cun, 1998), and the CelebA dataset (Liu et al., 2015). Fig. 6
shows the reconstruction ability of MINE compared to ALI.
Although ALICE does perfect reconstruction (which is in its
explicit formulation), we observe signiﬁcant mode-dropping
in the sample space. MINE does a balanced job of recon-
structing along with capturing all the modes of the underly-
ing data distribution.

Next, we measure the ﬁdelity of the reconstructions over
ALI, ALICE, and MINE. Tbl. 2 compares MINE to the
existing baselines in terms of euclidean reconstruction er-
rors, reconstruction accuracy, and MS-SSIM. On MNIST,
MINE outperforms ALI in terms of reconstruction errors by
a good margin and is competitive to ALICE with respect to
reconstruction accuracy and MS-SSIM. Our results show
that MINE’s effect on reconstructions is even more dramatic
when compared to ALI and ALICE on the CelebA dataset.

5.3. Information Bottleneck

Model

Recons.
Error

Recons.
Acc.(%)

MS-SSIM

ALI
ALICE(l2)
ALICE(Adv.)
MINE

ALI
ALICE(l2)
ALICE(Adv.)
MINE

MNIST

CelebA

14.24
3.20
5.20
9.73

53.75
8.01
92.56
36.11

45.95
99.03
98.17
96.10

57.49
32.22
48.95
76.08

0.97
0.97
0.98
0.99

0.81
0.93
0.51
0.99

Table 2. Comparison of MINE with other bi-directional adversar-
ial models in terms of euclidean reconstruction error, reconstruc-
tion accuracy, and MS-SSIM on the MNIST and CelebA datasets.
MINE does a good job compared to ALI in terms of reconstruc-
tions. Though the explicit reconstruction based baselines (ALICE)
can sometimes do better than MINE in terms of reconstructions
related tasks, they consistently lag behind in MS-SSIM scores and
reconstruction accuracy on CelebA.

mation, or yielding a representation, that an input X ∈ X
contains about an output Y ∈ Y. An optimal representation
of X would capture the relevant factors and compress X
by diminishing the irrelevant parts which do not contribute
to the prediction of Y . IB was recently covered in the con-
text of deep learning (Tishby & Zaslavsky, 2015), and as
such can be seen as a process to construct an approxima-
tion of the minimally sufﬁcient statistics of the data. IB
seeks an encoder, q(Z | X), that induces the Markovian
structure X → Z → Y . This is done by minimizing the IB
Lagrangian,

L[q(Z | X)] = H(Y |Z) + βI(X, Z),

(20)

which appears as a standard cross-entropy loss augmented
with a regularizer promoting minimality of the representa-
tion (Achille & Soatto, 2017). Here we propose to estimate
the regularizer with MINE.

The Information Bottleneck (IB, Tishby et al., 2000) is an
information theoretic method for extracting relevant infor-

Related works
In the discrete setting, (Tishby et al., 2000)
uses the Blahut-Arimoto Algorithm (Arimoto, 1972), which

Mutual Information Neural Estimation

(a) ALI

(b) ALICE (l2)

(c) ALICE (A)

(d) ALI+MINE

Figure 6. Reconstructions and model sam-
ples from adversarially learned inference
(ALI) and variations intended to increase
improve reconstructions. Shown left to
right are the baseline (ALI), ALICE with
the l2 loss to minimize the reconstruction
error, ALICE with an adversarial loss, and
ALI+MINE. Top to bottom are the recon-
structions and samples from the priors. AL-
ICE with the adversarial loss has the best
reconstruction, though at the expense of
poor sample quality, where as ALI+MINE
captures all the modes of the data in sample
space.

can be understood as cyclical coordinate ascent in function
spaces. While IB is successful and popular in a discrete
setting, its application to the continuous setting was stiﬂed
by the intractability of the continuous mutual information.
Nonetheless, IB was applied in the case of jointly Gaussian
random variables in (Chechik et al., 2005).

In order to overcome the intractability of I(X; Z) in
the continuous setting, Alemi et al. (2016); Kolchinsky
et al. (2017); Chalk et al. (2016) exploit the variational
bound of Barber & Agakov (2003) to approximate the
conditional entropy in I(X; Z). These approaches differ
only on their treatment of the marginal distribution of
the bottleneck variable: Alemi et al. (2016) assumes a
standard multivariate normal marginal distribution, Chalk
et al. (2016) uses a Student-t distribution, and Kolchinsky
et al. (2017) uses non-parametric estimators. Due to their
reliance on a variational approximation, these methods
require a tractable density for the approximate posterior,
while MINE does not.
Experiment: Permutation-invariant MNIST classiﬁca-
tion Here, we demonstrate an implementation of the IB
objective on permutation invariant MNIST using MINE. We
compare to the Deep Variational Bottleneck (DVB, Alemi
et al., 2016) and use the same empirical setup. As the DVB
relies on a variational bound on the conditional entropy, it
therefore requires a tractable density. Alemi et al. (2016)
opts for a conditional Gaussian encoder z = µ(x) + σ (cid:12) (cid:15),
where (cid:15) ∼ N (0, I). As MINE does not require a tractable
density, we consider three type of encoders: (i) a Gaussian
encoder as in Alemi et al. (2016); (ii) an additive noise
encoder, z = enc(x + σ (cid:12) (cid:15)); and (iii) a propagated noise
encoder, z = enc([x, (cid:15)]). Our results can be seen in Tbl. 3,
and this shows MINE as being superior in these settings.

Model

Misclass. rate(%)

Baseline
Dropout
Conﬁdence penalty
Label Smoothing
DVB
DVB + Additive noise

MINE(Gaussian) (ours)
MINE(Propagated) (ours)
MINE(Additive) (ours)

1.38%
1.34%
1.36%
1.40%
1.13%
1.06%

1.11%
1.10%
1.01%

Table 3. Permutation Invariant MNIST misclassiﬁcation rate using
Alemi et al. (2016) experimental setup for regularization by con-
ﬁdence penalty (Pereyra et al., 2017), label smoothing (Pereyra
et al., 2017), Deep Variational Bottleneck(DVB) (Alemi et al.,
2016) and MINE. The misclassiﬁcation rate is averaged over ten
runs. In order to control for the regularizing impact of the additive
Gaussian noise in the additive conditional, we also report the re-
sults for DVB with additional additive Gaussian noise at the input.
All non-MINE results are taken from Alemi et al. (2016).

the efﬁciency of this estimator by applying it in a num-
ber of settings. First, a term of mutual information can
be introduced alleviate mode-dropping issue in generative
adversarial networks (GANs, Goodfellow et al., 2014). Mu-
tual information can also be used to improve inference and
reconstructions in adversarially-learned inference (ALI, Du-
moulin et al., 2016). Finally, we showed that our estimator
allows for tractable application of Information bottleneck
methods (Tishby et al., 2000) in a continuous setting.

6. Conclusion

We proposed a mutual information estimator, which we
called the mutual information neural estimator (MINE), that
is scalable in dimension and sample-size. We demonstrated

We would like to thank Martin Arjovsky, Caglar Gulcehre,
Marcin Moczulski, Negar Rostamzadeh, Thomas Boquet,
Ioannis Mitliagkas, Pedro Oliveira Pinheiro for helpful com-
ments, as well as Samsung and IVADO for their support.

7. Acknowledgements

Mutual Information Neural Estimation

References

Achille, A. and Soatto, S.

disentanglement in deep representations.
1706.01350v2[cs.LG], 2017.

Emergence of invariance and
arXiv preprint

Alemi, A. A., Fischer, I., Dillon, J. V., and Murphy, K. Deep varia-
tional information bottleneck. arXiv preprint arXiv:1612.00410,
2016.

Arimoto, S. An algorithm for computing the capacity of arbitrary
discrete memoryless channels. IEEE Transactions on Informa-
tion Theory, 18(1):14–20, 1972.

Banerjee, A. On baysian bounds. ICML, pp. 81–88, 2006.

Barber, D. and Agakov, F. The im algorithm: a variational ap-
proach to information maximization. In Proceedings of the 16th
International Conference on Neural Information Processing
Systems, pp. 201–208. MIT Press, 2003.

Belghazi, M. I., Rajeswar, S., Mastropietro, O., Mitrovic, J., Ros-
tamzadeh, N., and Courville, A. Hierarchical adversarially
learned inference. arXiv preprint arXiv:1802.01071, 2018.

Butte, A. J. and Kohane, I. S. Mutual information relevance
networks: functional genomic clustering using pairwise entropy
measurements. In Pac Symp Biocomput, volume 5, pp. 26,
2000.

Chalk, M., Marre, O., and Tkacik, G. Relevant sparse codes
with variational information bottleneck. In Advances in Neural
Information Processing Systems, pp. 1957–1965, 2016.

Che, T., Li, Y., Jacob, A. P., Bengio, Y., and Li, W. Mode
regularized generative adversarial networks. arXiv preprint
arXiv:1612.02136, 2016.

Chechik, G., Globerson, A., Tishby, N., and Weiss, Y. Information
bottleneck for gaussian variables. Journal of Machine Learning
Research, 6(Jan):165–188, 2005.

Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I.,
and Abbeel, P.
Infogan: Interpretable representation learn-
ing by information maximizing generative adversarial nets. In
Advances in Neural Information Processing Systems, pp. 2172–
2180, 2016.

Clevert, D., Unterthiner, T., and Hochreiter, S. Fast and accurate
deep network learning by exponential linear units (elus). CoRR,
abs/1511.07289, 2015.

Gao, S., Ver Steeg, G., and Galstyan, A. Efﬁcient estimation of
mutual information for strongly dependent variables. Arxiv
preprint arXiv:1411.2003[cs.IT], 2014.

Ghosh, A., Kulharia, V., Namboodiri, V., Torr, P. H., and Dokania,
P. K. Multi-agent diverse generative adversarial networks. arXiv
preprint arXiv:1704.02906, 2017.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-
Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative
adversarial nets. In Advances in neural information processing
systems, pp. 2672–2680, 2014.

Gy¨orﬁ, L. and van der Meulen, E. C. Density-free convergence
properties of various estimators of entropy. Computational
Statistics and Data Analysis, 5:425?436, 1987.

Hornik, K. Multilayer feedforward networks are universal approx-

imators. Neural Networks, 2:359–366, 1989.

Hyv¨arinen, A., Karhunen, J., and Oja, E. Independent component

analysis, volume 46. John Wiley & Sons, 2004.

Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. CoRR,
abs/1502.03167, 2015. URL http://arxiv.org/abs/
1502.03167.

Kandasamy, K., Krishnamurthy, A., Poczos, B., Wasserman, L.,
and Robins, J. Nonparametric von mises estimators for en-
tropies, divergences and mutual informations. NIPS, 2017.

Keziou, A. Dual representation of -divergences and applications.

336:857–862, 05 2003.

Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. CoRR, abs/1412.6980, 2014. URL http:
//arxiv.org/abs/1412.6980.

Kinney, J. B. and Atwal, G. S. Equitability, mutual information,
and the maximal information coefﬁcient. Proceedings of the
National Academy of Sciences, 111(9):3354–3359, 2014.

Kolchinsky, A., Tracey, B. D., and Wolpert, D. H. Nonlinear
information bottleneck. arXiv preprint arXiv:1705.02436, 2017.

Kraskov, A., St¨ogbauer, H., and Grassberger, P. Estimating mutual

information. Physical review E, 69(6):066138, 2004.

Kullback, S. Information theory and statistics. Courier Corpora-

Kumar, A., Sattigeri, P., and Fletcher, P. T.

Improved semi-
supervised learning with gans using manifold invariances. arXiv
preprint arXiv:1705.08850, 2017.

Kwak, N. and Choi, C.-H.

Input feature selection by mutual
information based on parzen window. IEEE transactions on
pattern analysis and machine intelligence, 24(12):1667–1671,
2002.

LeCun, Y. The mnist database of handwritten digits. http://yann.

lecun. com/exdb/mnist/, 1998.

Darbellay, G. A. and Vajda, I. Estimation of the information
IEEE
by an adaptive partitioning of the observation space.
Transactions on Information Theory, 45(4):1315–1321, 1999.

tion, 1997.

Donahue, J., Kr¨ahenb¨uhl, P., and Darrell, T. Adversarial feature

learning. arXiv preprint arXiv:1605.09782, 2016.

Donsker, M. and Varadhan, S. Asymptotic evaluation of certain
markov process expectations for large time, iv. Communications
on Pure and Applied Mathematics, 36(2):183?212, 1983.

Dumoulin, V., Belghazi, I., Poole, B., Lamb, A., Arjovsky, M.,
Mastropietro, O., and Courville, A. Adversarially learned infer-
ence. arXiv preprint arXiv:1606.00704, 2016.

Fraser, A. M. and Swinney, H. L. Independent coordinates for
strange attractors from mutual information. Physical review A,
33(2):1134, 1986.

Li, C., Liu, H., Chen, C., Pu, Y., Chen, L., Henao, R., and Carin, L.
Towards understanding adversarial learning for joint distribution
matching. arXiv preprint arXiv:1709.01215, 2017.

Mutual Information Neural Estimation

Lin, Z., Khetan, A., Fanti, G., and Oh, S. Pacgan: The power of
two samples in generative adversarial networks. arXiv preprint
arXiv:1712.04086, 2017.

Singh, S. and Pczos, B. Finite-sample analysis of ﬁxed-k near-
est neighbor density functional estimators. arXiv preprint
1606.01554, 2016.

Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face
attributes in the wild. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 3730–3738, 2015.

Srivastava, A., Valkov, L., Russell, C., Gutmann, M., and Sutton,
C. Veegan: Reducing mode collapse in gans using implicit
variational learning. arXiv preprint arXiv:1705.07761, 2017.

Maes, F., Collignon, A., Vandermeulen, D., Marchal, G., and
Suetens, P. Multimodality image registration by maximization
of mutual information. IEEE transactions on Medical Imaging,
16(2):187–198, 1997.

Suzuki, T., Sugiyama, M., Sese, J., and Kanamori, T. Approx-
imating mutual information by maximum likelihood density
ratio estimation. In New challenges for feature selection in data
mining and knowledge discovery, pp. 5–20, 2008.

Metz, L., Poole, B., Pfau, D., and Sohl-Dickstein, J. Un-
rolled generative adversarial networks. 2017. URL https:
//openreview.net/pdf?id=BydrOIcle.

Tishby, N. and Zaslavsky, N. Deep learning and the information
bottleneck principle. In Information Theory Workshop (ITW),
2015 IEEE, pp. 1–5. IEEE, 2015.

Moon, K., Sricharan, K., and Hero III, A. O. Ensemble estimation
of mutual information. arXiv preprint arXiv:1701.08083, 2017.

Tishby, N., Pereira, F. C., and Bialek, W. The information bottle-

neck method. arXiv preprint physics/0004057, 2000.

Ulyanov, D., Vedaldi, A., and Lempitsky, V. Adversarial generator-
encoder networks. arXiv preprint arXiv:1704.02304, 2017.

Van de Geer, S. Empirical Processes in M-estimation. Cambridge

University Press, 2000.

Van Hulle, M. M. Edgeworth approximation of multivariate differ-
ential entropy. Neural computation, 17(9):1903–1910, 2005.

Wang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P. Image
quality assessment: from error visibility to structural similarity.
IEEE transactions on image processing, 13:600–612, 2004.

Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired image-to-
image translation using cycle-consistent adversarial networks.
arXiv preprint arXiv:1703.10593, 2017.

Moon, Y.-I., Rajagopalan, B., and Lall, U. Estimation of mutual
information using kernel density estimators. Physical Review E,
52(3):2318, 1995.

Nguyen, T., Le, T., Vu, H., and Phung, D. Dual discriminator
generative adversarial nets. In Advances in Neural Information
Processing Systems, pp. 2667–2677, 2017.

Nguyen, X., Wainwright, M. J., and Jordan, M. I. Estimating
divergence functionals and the likelihood ratio by convex risk
minimization. IEEE Transactions on Information Theory, 56
(11):5847–5861, 2010.

Nowozin, S., Cseke, B., and Tomioka, R. f-gan: Training genera-
tive neural samplers using variational divergence minimization.
In Advances in Neural Information Processing Systems, pp.
271–279, 2016.

Paninski, L. Estimation of entropy and mutual information. Neural

computation, 15(6):1191–1253, 2003.

Peng, H., Long, F., and Ding, C. Feature selection based on mutual
information criteria of max-dependency, max-relevance, and
min-redundancy. IEEE Transactions on pattern analysis and
machine intelligence, 27(8):1226–1238, 2005.

Pereyra, G., Tucker, G., Chorowski, J., Kaiser, Ł., and Hinton, G.
Regularizing neural networks by penalizing conﬁdent output
distributions. ICLR Workshop, 2017.

Radford, A., Metz, L., and Chintala, S. Unsupervised represen-
tation learning with deep convolutional generative adversarial
networks. arXiv preprint arXiv:1511.06434, 2015.

Ruderman, A., Reid, M., Garc´ıa-Garc´ıa, D., and Petterson, J.
Tighter variational representations of f-divergences via restric-
tion to probability measures. arXiv preprint arXiv:1206.4664,
2012.

Saatchi, Y. and Wilson, A. G. Bayesian gan. In Advances in Neural

Information Processing Systems, pp. 3625–3634, 2017.

Salimans, T., Goodfellow, I. J., Zaremba, W., Cheung, V., Radford,
A., and Chen, X. Improved techniques for training gans. arXiv
preprint arXiv:1606.03498, 2016.

Shalev-Schwartz, S. and Ben-David, S. Understanding Machine
Learning - from Theory to Algorithms. Cambridge university
press, 2014.

Mutual Information Neural Estimation

In this Appendix, we provide additional experiment details and spell out the proofs omitted in the text.

8. Appendix

8.1. Experimental Details

8.1.1. ADAPTIVE CLIPPING

Here we assume we are in the context of GANs described in Sections 5.1 and 5.2, where the mutual information shows up as a regularizer
in the generator objective.

Notice that the generator is updated by two gradients. The ﬁrst gradient is that of the generator’s loss, Lg with respect to the generator’s
∂θ . The second ﬂows from the mutual information estimate to the generator, gm := − ∂ (cid:92)I(X;Z)
parameters θ, gu := ∂Lg
. If left unchecked,
because mutual information is unbounded, the latter can overwhelm the former, leading to a failure mode of the algorithm where the
generator puts all of its attention on maximizing the mutual information and ignores the adversarial game with the discriminator. We
propose to adaptively clip the gradient from the mutual information so that its Frobenius norm is at most that of the gradient from the
discriminator. Deﬁning ga to be the adapted gradient following from the statistics network to the generator, we have,

∂θ

ga = min((cid:107)gu(cid:107) , (cid:107)gm(cid:107))

gm
(cid:107)gm(cid:107)

.

(21)

Note that adaptive clipping can be considered in any situation where MINE is to be maximized.

8.1.2. GAN+MINE: SPIRAL AND 25-GAUSSIANS

In this section we state the details of experiments supporting mode dropping experiments on the spiral and 25-Gaussians dataset. For both
the datasets we use 100,000 examples sampled from the target distributions, using a standard deviation of 0.05 in the case of 25-gaussians,
and using additive noise for the spiral. The generator for the GAN consists of two fully connected layers with 500 units in each layer with
batch-normalization (Ioffe & Szegedy, 2015) and Leaky-ReLU as activation function as in Dumoulin et al. (2016). The discriminator
and statistics networks have three fully connected layers with 400 units each. We use the Adam (Kingma & Ba, 2014) optimizer with a
learning rate of 0.0001. Both GAN baseline and GAN+MINE were trained for 5, 000 iterations with a mini batch-size of 100.

8.1.3. GAN+MINE: STACKED-MNIST

Here we describe the experimental setup and architectural details of stacked-MNIST task with GAN+MINE. We compare to the exact
same experimental setup followed and reported in PacGAN(Lin et al., 2017) and VEEGAN(Srivastava et al., 2017). We use a pre-trained
classiﬁer to classify generated samples on each of the three stacked channels. Evaluation is done on 26,000 test samples as followed in the
baselines. We train GAN+MINE for 50 epochs on 128, 000 samples. Details for generator and discriminator networks are given below in
the table4 and table5. Speciﬁcally the statistics network has the same architecture as discriminator in DCGAN with ELU (Clevert et al.,
2015) as activation function for the individual layers and without batch-normalization as highlighted in Table 6. In order to condition the
statistics network on the z variable, we use linear MLPs at each layer, whose output are reshaped to the number of feature maps. The
linear MLPs output is then added as a dynamic bias.

Number of outputs Kernel size

Stride Activation function

Generator

Layer
Input z ∼ U(−1, 1)100
Fully-connected
Transposed convolution
Transposed convolution
Transposed convolution
Transposed convolution

100
2*2*512
4*4*256
7*7*128
14*14*64
28*28*3

5 ∗ 5
5 ∗ 5
5 ∗ 5
5 ∗ 5

2
2
2
2

ReLU
ReLU
ReLU
ReLU
Tanh

Table 4. Generator network for Stacked-MNIST experiment using GAN+MINE.

Mutual Information Neural Estimation

Discriminator

Layer

Number of outputs Kernel size

Stride Activation function

Input x
Convolution
Convolution
Convolution
Convolution
Fully-connected

28 ∗ 28 ∗ 3
14*14*64
7*7*128
4*4*256
2*2*512
1

5 ∗ 5
5 ∗ 5
5 ∗ 5
5 ∗ 5
1

2
2
2
2
Valid

ReLU
ReLU
ReLU
ReLU
Sigmoid

Table 5. Discriminator network for Stacked-MNIST experiment.

Statistics Network
number of outputs

kernel size

stride

activation function

Layer

Input x, z
Convolution
Convolution
Convolution
Flatten
Fully-Connected
Fully-Connected

14*14*16
7*7*32
4*4*64
-
1024
1

5 ∗ 5
5 ∗ 5
5 ∗ 5
-
1
1

ELU
ELU
ELU
-

2
2
2
-
Valid None
Valid None

Table 6. Statistics network for Stacked-MNIST experiment.

8.1.4. ALI+MINE: MNIST AND CELEBA

In this section we state the details of experimental setup and the network architectures used for the task of improving reconstructions and
representations in bidirectional adversarial models with MINE. The generator and discriminator network architectures along with the
hyper parameter setup used in these tasks are similar to the ones used in DCGAN (Radford et al., 2015).

Statistics network conditioning on the latent code was done as in the Stacked-MNIST experiments. We used Adam as the optimizer with a
learning rate of 0.0001. We trained the model for a total of 35, 000 iterations on CelebA and 50, 000 iterations on MNIST, both with a
mini batch-size of 100.

Layer

Number of outputs Kernel size

Stride Activation function

Encoder

Input [x, (cid:15)]
Convolution
Convolution
Convolution
Convolution
Fully-connected

28*28*129
14*14*64
7*7*128
4*4*256
256
128

5 ∗ 5
5 ∗ 5
5 ∗ 5
4 ∗ 4
-

2
2
2
Valid
-

ReLU
ReLU
ReLU
ReLU
None

Table 7. Encoder network for bi-directional models on MNIST. (cid:15) ∼ N128(0, I).

Layer

Number of outputs Kernel size

Stride Activation function

Decoder

Input z
Fully-connected
Transposed convolution
Transposed convolution
Transposed convolution

128
4*4*256
7*7*128
14*14*64
28*28*1

5 ∗ 5
5 ∗ 5
5 ∗ 5

2
2
2

ReLU
ReLU
ReLU
Tanh

Table 8. Decoder network for bi-directional models on MNIST. z ∼ N256(0, I)

5 ∗ 5
5 ∗ 5
5 ∗ 5
-
-
-
-

5 ∗ 5
5 ∗ 5
5 ∗ 5
-
-

5 ∗ 5
5 ∗ 5
5 ∗ 5
5 ∗ 5
4 ∗ 4
-

Mutual Information Neural Estimation

Discriminator

Layer

Number of outputs Kernel size

Stride Activation function

Input x
Convolution
Convolution
Convolution
Flatten
Concatenate z
Fully-connected
Fully-connected

28 ∗ 28 ∗ 3
14*14*64
7*7*128
4*4*256
-
-
1024
1

Layer

Input x, z
Convolution
Convolution
Convolution
Flatten
Fully-connected

14*14*64
7*7*128
4*4*256
-
1

2
2
2
-
-
-
-

2
2
2
-
-

LearkyReLU
LeakyReLU
LeakyReLU

LeakyReLU
Sigmoid

LeakyReLU
LeakyReLU
LeakyReLU
-
None

Table 9. Discriminator network for bi-directional models experiments MINE on MNIST.

Statistics Network
number of outputs

kernel size

stride

activation function

Table 10. Statistics network for bi-directional models using MINE on MNIST.

Layer

Number of outputs Kernel size

Stride Activation function

Encoder

Input [x, (cid:15)]
Convolution
Convolution
Convolution
Convolution
Convolution
Fully-connected

64*64*259
32*32*64
16*16*128
8*8*256
4*4*512
512
256

2
2
2
2
Valid
-

ReLU
ReLU
ReLU
ReLU
ReLU
None

Table 11. Encoder network for bi-directional models on CelebA. (cid:15) ∼ N256(0, I).

Layer

Number of outputs Kernel size

Stride Activation function

Decoder

Input z ∼ N256(0, I)
Fully-Connected
Transposed convolution
Transposed convolution
Transposed convolution
Transposed convolution

256
4*4*512
8*8*256
16*16*128
32*32*64
64*64*3

-
5 ∗ 5
5 ∗ 5
5 ∗ 5
5 ∗ 5

-
2
2
2
2

ReLU
ReLU
ReLU
ReLU
Tanh

Table 12. Decoder network for bi-directional model(ALI, ALICE) experiments using MINE on CelebA.

Mutual Information Neural Estimation

Discriminator

Layer

Number of outputs Kernel size

Stride Activation function

Input x
Convolution
Convolution
Convolution
Convolution
Flatten
Concatenate z
Fully-connected
Fully-connected

64 ∗ 64 ∗ 3
32*32*64
16*16*128
8*8*256
4*4*512
-
-
1024
1

Layer

Input x, z
Convolution
Convolution
Convolution
Convolution
Flatten
Fully-connected

32*32*16
16*16*32
8*8*64
4*4*128
-
1

5 ∗ 5
5 ∗ 5
5 ∗ 5
5 ∗ 5
-
-
-
-

5 ∗ 5
5 ∗ 5
5 ∗ 5
5 ∗ 5
-
-

2
2
2
2
-
-
-
-

2
2
2
2
-
-

LearkyReLU
LeakyReLU
LeakyReLU
LeakyReLU

LeakyReLU
Sigmoid

ELU
ELU
ELU
ELU
-
None

Table 13. Discriminator network for bi-directional models on CelebA.

Statistics Network
number of outputs

kernel size

stride

activation function

Table 14. Statistics network for bi-directional models on CelebA.

8.1.5. INFORMATION BOTTLENECK WITH MINE

In this section we outline the network details and hyper-parameters used for the information bottleneck task using MINE. To keep
comparison fair all hyperparameters and architectures are those outlined in Alemi et al. (2016). The statistics network is shown, a two
layer MLP with additive noise at each layer and 512 ELUs (Clevert et al., 2015) activations, is outlined in table15.

Statistics Network

Layer

number of outputs

activation function

input [x, z]
Gaussian noise(std=0.3)
dense layer
Gaussian noise(std=0.5)
dense layer
Gaussian noise(std=0.5)
dense layer

-
512
-
512
-
1

-
ELU
-
ELU
-
None

Table 15. Statistics network for Information-bottleneck experiments on MNIST.

8.2. Proofs

8.2.1. DONSKER-VARADHAN REPRESENTATION

Theorem 4 (Theorem 1 restated). The KL divergence admits the following dual representation:

where the supremum is taken over all functions T such that the two expectations are ﬁnite.

DKL(P || Q) = sup
T :Ω→R

EP[T ] − log(EQ[eT ]),

(22)

Proof. A simple proof goes as follows. For a given function T , consider the Gibbs distribution G deﬁned by dG = 1
Z = EQ[eT ]. By construction,

Z eT dQ, where

Mutual Information Neural Estimation

EP[T ] − log Z = EP

log

(cid:20)

(cid:21)

dG
dQ

∆ := DKL(P || Q) −

EP[T ] − log(EQ[eT ])

(cid:16)

(cid:17)

Let ∆ be the gap,

Using Eqn 23, we can write ∆ as a KL-divergence:

(cid:20)

∆ = EP

log

dP
dQ − log

dG
dQ

(cid:21)

= EP log

dP
dG = DKL(P || G)

The positivity of the KL-divergence gives ∆ ≥ 0. We have thus shown that for any T ,

DKL(P || Q) ≥ EP[T ] − log(EQ[eT ])

and the inequality is preserved upon taking the supremum over the right-hand side. Finally, the identity (25) also shows that this bound is
tight whenever G = P, namely for optimal functions T ∗ taking the form T ∗ = log dP

dQ + C for some constant C ∈ R.

8.2.2. CONSISTENCY PROOFS

This section presents the proofs of the Lemma and consistency theorem stated in the consistency in Section 3.3.1.
In what follows, we assume that the input space Ω = X × Z is a compact domain of Rd, and all measures are absolutely continuous with
respect to the Lebesgue measure. We will restrict to families of feedforward functions with continuous activations, with a single output
neuron, so that a given architecture deﬁnes a continuous mapping (ω, θ) → Tθ(ω) from Ω × Θ to R.
To avoid unnecessary heavy notation, we denote P = PXZ and Q = PX ⊗ PZ for the joint distribution and product of marginals, and
Pn, Qn for their empirical versions. We will use the notation ˆI(T ) for the quantity:

ˆI(T ) = EP[T ] − log(EQ[eT ])

so that IΘ(X, Z) = supθ∈Θ
Lemma 3 (Lemma 1 restated). Let η > 0. There exists a family of neural network functions Tθ with parameters θ in some compact
domain Θ ⊂ Rk, such that

ˆI(Tθ).

where

|I(X, Z) − IΘ(X, Z)| ≤ η

IΘ(X, Z) = sup
θ∈Θ

EPXZ [Tθ] − log(EPX ⊗PZ [eTθ ])

Proof. Let T ∗ = log dP

dQ . By construction, T ∗ satisﬁes:

For a function T , the (positive) gap I(X, Z) − ˆI(T ) can be written as

EP[T ∗] = I(X, Z),

EQ[eT ∗

] = 1

I(X, Z) − ˆI(T ) = EP[T ∗ − T ] + log EQ[eT ] ≤ EP[T ∗ − T ] + EQ[eT − eT ∗

]

where we used the inequality log x ≤ x − 1.

Fix η > 0. We ﬁrst consider the case where T ∗ is bounded from above by a constant M . By the universal approximation theorem (see
corollary 2.2 of Hornik (1989)8), we may choose a feedforward network function T ˆθ ≤ M such that

Since exp is Lipschitz continuous with constant eM on (−∞, M ], we have

EP|T ∗ − T ˆθ| ≤

and EQ|T ∗ − T ˆθ| ≤

e−M

η
2

EQ|eT ∗

− eT ˆθ | ≤ eM EQ|T ∗ − T ˆθ| ≤

η
2

η
2

8Speciﬁcally, the argument relies on the density of feedforward network functions in the space L1(Ω, µ) of integrable functions with

respect the measure µ = P + Q.

(23)

(24)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

Mutual Information Neural Estimation

From Equ 31 and the triangular inequality, we then obtain:

|I(X, Z) − ˆI(T ˆθ)| ≤ EP|T ∗ − T ˆθ| + EQ|eT ∗

− eT ˆθ | ≤

η
2

η
2

+

≤ η

In the general case, the idea is to partition Ω in two subset {T ∗ > M } and {T ∗ ≤ M } for a suitably chosen large value of M . For a
given subset S ⊂ Ω, we will denote by 1S its characteristic function, 1S(ω) = 1 if ω ∈ S and 0 otherwise. T ∗ is integrable with respect
to P9, and eT ∗
is integrable with respect to Q, so by the dominated convergence theorem, we may choose M so that the expectations
EP[T ∗1T ∗>M ] and EQ[eT ∗ 1T ∗>M ] are lower than η/4. Just like above, we then use the universal approximation theorem to ﬁnd a feed
forward network function T ˆθ, which we can assume without loss of generality to be upper-bounded by M , such that

EP|T ∗ − T ˆθ| ≤

and EQ|T ∗ − T ˆθ|1T ∗≤M ≤

e−M

η
2

η
4

We then write

EQ[eT ∗

− eT ˆθ ] = EQ[(eT ∗

− eT ˆθ )1T ∗≤M ] + EQ[(eT ∗

− eT ˆθ )1T ∗>M ]

≤ eM EQ[|T ∗ − T ˆθ|1T ∗≤M ] + EQ[eT ∗ 1T ∗>M ]
≤

+

η
4

η
4
η
2

≤

where the inequality in the second line arises from the convexity and positivity of exp. Eqns. 35 and 36, together with the triangular
inequality, lead to Eqn. 34, which proves the Lemma.

Lemma 4 (Lemma 2 restated). Let η > 0. Given a family F of neural network functions Tθ with parameters θ in some compact domain
Θ ⊂ Rk, there exists N ∈ N such that

∀n ≥ N, Pr

(cid:16)

| (cid:92)I(X; Z)n − IF (X, Z)| ≤ η

(cid:17)

= 1

Proof. We start by using the triangular inequality to write,
| (cid:92)I(X; Z)n − sup

ˆI(Tθ)| ≤ sup
Tθ ∈F

|EP[Tθ] − EPn [Tθ]| + sup
Tθ ∈F

Tθ ∈F

| log EQ[eTθ ] − log EQn [eTθ ]|

The continuous function (θ, ω) → Tθ(ω), deﬁned on the compact domain Θ × Ω, is bounded. So the functions Tθ are uniformly bounded
by a constant M , i.e |Tθ| ≤ M for all θ ∈ Θ. Since log is Lipschitz continuous with constant eM in the interval [e−M , eM ], we have

| log EQ[eTθ ] − log EQn [eTθ ]| ≤ eM |EQ[eTθ ] − EQn [eTθ ]|
Since Θ is compact and the feedforward network functions are continuous, the families of functions Tθ and eTθ satisfy the uniform law of
large numbers (Van de Geer, 2000). Given η > 0 we can thus choose N ∈ N such that ∀n ≥ N and with probability one,

(40)

sup
Tθ ∈F

|EP[Tθ] − EPn [Tθ] ≤

and

|EQ[eTθ ] − EQn [eTθ ]| ≤

e−M

η
2

sup
Tθ ∈F

η
2

Together with Eqns. 39 and 40, this leads to

| (cid:92)I(X; Z)n − sup

ˆI(Tθ)| ≤

Tθ ∈F

η
2

η
2

+

= η

Theorem 5 (Theorem 2 restated). MINE is strongly consistent.

Proof. Let (cid:15) > 0. We apply the two Lemmas to ﬁnd a a family of neural network function F and N ∈ N such that (28) and (38) hold
with η = (cid:15)/2. By the triangular inequality, for all n ≥ N and with probability one, we have:
|I(X, Z) − (cid:92)I(X; Z)n| ≤ |I(X, Z) − sup

ˆI(Tθ)| + | (cid:92)I(X; Z)n − IF (X, Z)| ≤ (cid:15)

(43)

Tθ ∈F

which proves consistency.

9This can be seen from the identity (Gy¨orﬁ & van der Meulen, 1987)

EP

(cid:12)
(cid:12)
log
(cid:12)
(cid:12)

dP
dQ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ DKL(P || Q) + 4(cid:112)DKL(P || Q)

(34)

(35)

(36)

(37)

(38)

(39)

(41)

(42)

Mutual Information Neural Estimation

8.2.3. SAMPLE COMPLEXITY PROOF

Theorem 6 (Theorem 3 restated). Assume that the functions Tθ in F are M -bounded (i.e., |Tθ| ≤ M ) and L-Lipschitz with respect to
the parameters θ. The domain Θ ⊂ Rd is bounded, so that (cid:107)θ(cid:107) ≤ K for some constant K. Given any values (cid:15), δ of the desired accuracy
and conﬁdence parameters, we have,

whenever the number n of samples satisﬁes

Proof. The assumptions of Lemma 2 apply, so let us begin with Eqns. 39 and 40. By the Hoeffding inequality, for all function f ,

To extend this inequality to a uniform inequality over all functions Tθ and eTθ , the standard technique is to choose a minimal cover of the
domain Θ ⊂ Rd by a ﬁnite set of small balls of radius η, Θ ⊂ ∪jBη(θj), and to use the union bound. The minimal cardinality of such
covering is bounded by the covering number Nη(Θ) of Θ, known to satisfy(Shalev-Schwartz & Ben-David, 2014)

(cid:16)

Pr

| (cid:92)I(X; Z)n − IF (X, Z)| ≤ (cid:15)

(cid:17)

≥ 1 − δ

2M 2(d log(16KL

d/(cid:15)) + 2dM + log(2/δ))

n ≥

√

(cid:15)2

(cid:16)
|EQ[f ] − EQn [f ]| >

Pr

(cid:17)

(cid:15)
4

≤ 2 exp (−

(cid:15)2n
2M 2 )

Nη(Θ) ≤

√

d

(cid:33)d

(cid:32)

2K
η

Successively applying a union bound in Eqn 46 with the set of functions {Tθj }j and {eTθj }j gives

and

(cid:18)

Pr

max
j

(cid:18)

Pr

max
j

|EQ[Tθj ] − EQn [Tθj ]| >

≤ 2Nη(Θ) exp (−

(cid:19)

(cid:15)
4

(cid:19)

(cid:15)
4

(cid:15)2n
2M 2 )

(cid:15)2n
2M 2 )

|EQ[eTθj ] − EQn [eTθj ]| >

≤ 2Nη(Θ) exp (−

We now choose the ball radius to be η = (cid:15)

8L e−2M . Solving for n the inequation,

2Nη(Θ) exp (−

(cid:15)2n
2M 2 ) ≤ δ

we deduce from Eqn 48 that, whenever Eqn 45 holds, with probability at least 1 − δ, for all θ ∈ Θ,

|EQ[Tθ] − EQn [Tθ]| ≤ |EQ[Tθ] − EQ[Tθj ]| + |EQ[Tθj ] − EQn [Tθj ]| + |EQn [Tθj ] − EQn [Tθ]|

Similarly, using Eqn 40 and 49, we obtain that with probability at least 1 − δ,

and hence using the triangular inequality,

e−2M +

+

e−2M

(cid:15)
4

(cid:15)
8

≤

≤

(cid:15)
8
(cid:15)
2

| log EQ[eTθ ] − log EQn [eTθ ]| ≤

(cid:15)
2

| (cid:92)I(X; Z)n − IF (X, Z)| ≤ (cid:15)

8.2.4. BOUND ON THE RECONSTRUCTION ERROR

Here we clarify relationship between reconstruction error and mutual information, by proving the bound in Equ 18. We begin with a
deﬁnition:

Deﬁnition 8.1 (Reconstruction Error). We consider encoder and decoder models giving conditional distributions q(z|x) and p(x|z) over
the data and latent variables. If q(x) denotes the marginal data distribution, the reconstruction error is deﬁned as

R = Ex∼q(x)Ez∼q(z|x)[− log p(x|z)]

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(54)

(a) ALI

(b) ALICE (L2)

(c) ALICE (A)

(d) MINE

Mutual Information Neural Estimation

Figure 7. Embeddings from adversarially learned inference (ALI) and variations intended to increase the mutual information. Shown left
to right are the baseline (ALI), ALICE with the L2 loss to minimize the reconstruction error, ALI with an additional adversarial loss, and
MINE.

We can rewrite the reconstruction error in terms of the joints q(x, z) = q(z|x)p(x) and p(x, z) = p(x|z)p(z). Elementary manipula-
tions give:

R = E(x,z)∼q(x,z) log

− E(x,z)∼q(x,z) log q(x, z) + Ez∼q(z) log p(z)

q(x, z)
p(x, z)

where q(z) is the aggregated posterior. The ﬁrst term is the KL-divergence DKL(q || p) ; the second term is the joint entropy Hq(x, z).
The third term can be written as

Ez∼q(z) log p(z) = −DKL(q(z) || p(z)) − Hq(z)

Finally, the identity

Hq(x, z) − Hq(z) := Hq(z|x) = Hq(z) − Iq(x, z)

yields the following expression for the reconstruction error:

R = DKL(q(x, z) || p(x, z)) − DKL(q(z) || p(z)) − Iq(x, z) + Hq(z)

Since the KL-divergence is positive, we obtain the bound:

R ≤ DKL(q(x, z) || p(x, z)) − Iq(x, z) + Hq(z)

which is tight whenever the induced marginal q(z) matches the prior distribution p(z).

8.3. Embeddings for bi-direction 25 Gaussians experiments

Here (Fig. 7) we present the embeddings for the experiments corresponding to Fig. 6.

(55)

(56)

(57)

(58)

