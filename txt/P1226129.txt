6
1
0
2
 
p
e
S
 
3
2
 
 
]
L
C
.
s
c
[
 
 
1
v
2
2
2
7
0
.
9
0
6
1
:
v
i
X
r
a

Deep Multi-Task Learning with Shared Memory

Pengfei Liu Xipeng Qiu∗ Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China
{pﬂiu14,xpqiu,xjhuang}@fudan.edu.cn

Abstract

Neural network based models have achieved
impressive results on various speciﬁc tasks.
However, in previous works, most models are
learned separately based on single-task su-
pervised objectives, which often suffer from
insufﬁcient training data. In this paper, we
propose two deep architectures which can be
trained jointly on multiple related tasks. More
speciﬁcally, we augment neural model with an
external memory, which is shared by several
tasks. Experiments on two groups of text clas-
siﬁcation tasks show that our proposed archi-
tectures can improve the performance of a task
with the help of other related tasks.

1

Introduction

Neural network based models have been shown to
achieved impressive results on various NLP tasks ri-
valing or in some cases surpassing traditional mod-
els, such as text classiﬁcation (Kalchbrenner et al.,
2014; Socher et al., 2013; Liu et al., 2015a), seman-
tic matching (Hu et al., 2014; Liu et al., 2016a),
parser (Chen and Manning, 2014) and machine
translation (Bahdanau et al., 2014).

Usually, due to the large number of parameters
these neural models need a large-scale corpus. It is
hard to train a deep neural model that generalizes
well with size-limited data, while building the large
scale resources for some NLP tasks is also a chal-
lenge. To overcome this problem, these models often
involve an unsupervised pre-training phase. The ﬁ-
nal model is ﬁne-tuned on speciﬁc task with respect

∗ Corresponding author.

to a supervised training criterion. However, most
pre-training methods are based on unsupervised ob-
jectives (Collobert et al., 2011; Turian et al., 2010;
Mikolov et al., 2013), which is effective to improve
the ﬁnal performance, but it does not directly opti-
mize the desired task.

Multi-task learning is an approach to learn multi-
ple related tasks simultaneously to signiﬁcantly im-
prove performance relative to learning each task in-
dependently. Inspired by the success of multi-task
learning (Caruana, 1997), several neural network
based models (Collobert and Weston, 2008; Liu et
al., 2015b) are proposed for NLP tasks, which uti-
lized multi-task learning to jointly learn several tasks
with the aim of mutual beneﬁt. The characteristic
of these multi-task architectures is they share some
lower layers to determine common features. After
the shared layers, the remaining layers are split into
multiple speciﬁc tasks.

In this paper, we propose two deep architectures
of sharing information among several tasks in multi-
task learning framework. All the related tasks are in-
tegrated into a single system which is trained jointly.
More speciﬁcally, inspired by Neural Turing Ma-
chine (NTM) (Graves et al., 2014) and memory
network (Sukhbaatar et al., 2015), we equip task-
speciﬁc long short-term memory (LSTM) neural
network (Hochreiter and Schmidhuber, 1997) with
an external shared memory. The external memory
has capability to store long term information and
knowledge shared by several related tasks. Different
with NTM, we use a deep fusion strategy to integrate
the information from the external memory into task-
speciﬁc LSTM, in which a fusion gate controls the

information ﬂowing ﬂexibly and enables the model
to selectively utilize the shared information.

et al., 2015), which is similar to the architecture of
(Graves, 2013) but without peep-hole connections.

We demonstrate the effectiveness of our architec-
tures on two groups of text classiﬁcation tasks. Ex-
perimental results show that jointly learning of mul-
tiple related tasks can improve the performance of
each task relative to learning them independently.

Our contributions are of three-folds:

• We proposed a generic multi-task framework,
in which different tasks can share information
by an external memory and communicate by
a reading/writing mechanism. Two proposed
models are complementary to prior multi-task
neural networks.

• Different with Neural Turing Machine and
memory network, we introduce a deep fu-
sion mechanism between internal and external
memories, which helps the LSTM units keep
them interacting closely without being con-
ﬂated.

• As a by-product, the fusion gate enables us
to better understand how the external shared
memory helps speciﬁc task.

2 Neural Memory Models for Speciﬁc Task

In this section, we brieﬂy describe LSTM model,
and then propose an external memory enhanced
LSTM with deep fusion.

2.1 Long Short-term Memory

Long short-term memory network (LSTM) (Hochre-
iter and Schmidhuber, 1997) is a type of recurrent
neural network (RNN) (Elman, 1990), and speciﬁ-
cally addresses the issue of learning long-term de-
pendencies. LSTM maintains an internal memory
cell that updates and exposes its content only when
deemed necessary.

Architecturally speaking, the memory state and
output state are explicitly separated by activation
gates (Wang and Cho, 2015). However, the limita-
tion of LSTM is that it lacks a mechanism to index
its memory while writing and reading (Danihelka et
al., 2016).

While there are numerous LSTM variants, here
we use the LSTM architecture used by (Jozefowicz

We deﬁne the LSTM units at each time step t to
be a collection of vectors in Rd: an input gate it, a
forget gate ft, an output gate ot, a memory cell ct
and a hidden state ht. d is the number of the LSTM
units. The elements of the gating vectors it, ft and
ot are in [0, 1].

The LSTM is precisely speciﬁed as follows.













˜ct
ot
it
ft

=













tanh
σ
σ
σ

(cid:18)

Wp

(cid:21)

(cid:20) xt
ht−1

(cid:19)

+ bp

,

(1)

ct = ˜ct (cid:12) it + ct−1 (cid:12) ft,
ht = ot (cid:12) tanh (ct) ,

(2)

(3)

where xt ∈ Rm is the input at the current time step;
W ∈ R4h×(d+m) and bp ∈ R4h are parameters of
afﬁne transformation; σ denotes the logistic sigmoid
function and (cid:12) denotes elementwise multiplication.
The update of each LSTM unit can be written pre-

cisely as follows:

(ht, ct) = LSTM(ht−1, ct−1, xt, θp).

(4)

Here, the function LSTM(·, ·, ·, ·) is a shorthand
for Eq. (1-3), and θp represents all the parameters
of LSTM.

2.2 Memory Enhanced LSTM

LSTM has an internal memory to keep useful in-
formation for speciﬁc task, some of which may be
beneﬁcial to other tasks. However, it is non-trivial to
share information stored in internal memory.

Recently, there are some works to augment LSTM
with an external memory, such as neural Turing
machine (Graves et al., 2014) and memory net-
work (Sukhbaatar et al., 2015), called memory en-
hanced LSTM (ME-LSTM). These models enhance
the low-capacity internal memory to have a capabil-
ity of modelling long pieces of text (Andrychowicz
and Kurach, 2016).

Inspired by these models, we introduce an ex-
ternal memory to share information among several
tasks. To better control shared information and un-
derstand how it is utilized from external memory, we
propose a deep fusion strategy for ME-LSTM.

Each scalar αt,k in attention distribution αt can be

obtained as:

αt,k = softmax(g(Mt−1,k, kt−1))

(7)

where Mt−1,k represents the k-th row memory vec-
tor, and kt−1 is a key vector emitted by LSTM.

Here g(x, y) (x ∈ RM , y ∈ RM ) is a align
function for which we consider two different alter-
natives:

g(x, y) =

(cid:40)

vT tanh(Wa[x; y])
cosine(x, y)

(8)

where v ∈ RM is a parameter vector.

In our current implementation, the similarity mea-

sure is cosine similarity.

Writing The memory can be written by two oper-
ations: erase and add.

Mt = Mt−1(1 − αteT

t ) + αtaT
t ,

(9)

where et, at ∈ RM represent erase and add vectors
respectively.

To facilitate the following statements, we re-write

the writing equation as:

Mt = fwrite(Mt−1, αt, ht).

(10)

Deep Fusion between External and Internal
Memories After we obtain the information from
external memory, we need a strategy to comprehen-
sively utilize information from both external and in-
ternal memory.

To better control signals ﬂowing from external
memory, inspired by (Wang and Cho, 2015), we pro-
pose a deep fusion strategy to keep internal and ex-
ternal memories interacting closely without being
conﬂated.

In detail, the state ht of LSTM at step t depends
on both the read vector rt from external memory,
and internal memory ct, which is computed by

ht = ot (cid:12) tanh(ct + gt (cid:12) (Wf rt)),

(11)

where Wf is parameter matrix, and gt is a fusion
gate to select information from external memory,
which is computed by

gt = σ(Wrrt + Wcct),

(12)

Figure 1: Graphical illustration of the proposed ME-
LSTM unit with deep fusion of internal and external
memories.

As shown in Figure 1, ME-LSTM consists the
original LSTM and an external memory which is
maintained by reading and writing operations. The
LSTM not only interacts with the input and output
information but accesses the external memory using
selective read and write operations.

The external memory and corresponding opera-

tions will be discussed in detail below.

External Memory The form of external memory
is deﬁned as a matrix M ∈ RK×M , where K is
the number of memory segments, and M is the
size of each segment. Besides, K and M are gener-
ally instance-independent and pre-deﬁned as hyper-
parameters.

At each step t, LSTM emits output ht and three
key vectors kt, et and at simultaneously. kt, et and
at can be computed as





kt
et
at





 =





tanh
σ
tanh

 (Wmht + bm)

(5)

where Wm and bm are parameters of afﬁne trans-
formation.

Reading The read operation is to read information
rt ∈ RM from memory Mt−1.

rt = αtMt−1,

(6)

where rt denotes the reading vector and αt ∈ RK
represents a distribution over the set of segments of
memory Mt−1, which controls the amount of infor-
mation to be read from and written to the memory.

(a) Global Memory Architecture

(b) Local-Global Hybrid Memory Architecture

Figure 2: Two architectures for modelling text with
multi-task learning.

where Wr and Wc are parameter matrices.

Finally, the update of external memory enhanced

LSTM unit can be written precisely as

(ht, Mt, ct) = ME-LSTM(ht−1,
Mt−1, ct−1, xt, θp, θq),

(13)

where θp represents all the parameters of LSTM in-
ternal structure and θq represents all the parameters
to maintain the external memory.

Overall, the external memory enables ME-LSTM
to have larger capability to store more information,
thereby increasing the ability of ME-LSTM. The
read and write operations allow ME-LSTM to cap-
ture complex sentence patterns.

3 Deep Architectures with Shared
Memory for Multi-task Learning

Most existing neural network methods are based
on supervised training objectives on a single task
(Collobert et al., 2011; Socher et al., 2013; Kalch-
brenner et al., 2014). These methods often suffer
from the limited amounts of training data. To deal
with this problem, these models often involve an
unsupervised pre-training phase. This unsupervised
pre-training is effective to improve the ﬁnal perfor-
mance, but it does not directly optimize the desired
task.

Motivated by the success of multi-task learning
(Caruana, 1997), we propose two deep architectures
with shared external memory to leverage supervised
data from many related tasks. Deep neural model is
well suited for multi-task learning since the features
learned from a task may be useful for other tasks.
Figure 2 gives an illustration of our proposed archi-
tectures.

ARC-I: Global Shared Memory
In ARC-I, the
input is modelled by a task-speciﬁc LSTM and ex-
ternal shared memory. More formally, given an input
text x, the task-speciﬁc output h(m)
of task m at step
t is deﬁned as

t

(h(m)
t

, M(s)
t

, c(m)
t
M(s)

) = ME-LSTM(h(m)
t−1,
t−1, c(m)
, θ(s)
t−1, xt, θ(m)
q ),

p

(14)

where xt represents word embeddings of word
xt; the superscript s represents the parameters are
shared across different tasks; the superscript m rep-
resents that the parameters or variables are task-
speciﬁc for task m.

Here all tasks share single global memory M(s),
meaning that all tasks can read information from
it and have the duty to write their shared or task-
speciﬁc information into the memory.

M(s)

t = fwrite(M(s)

t−1, α(s)

t

, h(m)
t

)

(15)

After calculating the task-speciﬁc representation of
text h(m)
for task m, we can predict the probability
distribution over classes.

T

ARC-II: Local-Global Hybrid Memory
In
ARC-I, all tasks share a global memory, but can
also record task-speciﬁc information besides shared
information. To address this, we allocate each task
a local task-speciﬁc external memory, which can
further write shared information to a global memory
for all tasks.

More generally, for task m, we assign each task-
speciﬁc LSTM with a local memory M(m), followed
by a global memory M(s), which is shared across
different tasks.

The read and write operations of the local and

global memory are deﬁned as

t = α(m)
r(m)

t M(m)

t

,

(16)

Movie
Reviews

5.1 Datasets

Embedding dimension 100
Hidden layer size
100
External memory size (50,20)
Initial learning rate
Regularization

0.01
0

Table 2: Hyper-parameters of our models.

Product
Reviews
100
100
(50,20)
0.1
1E−5

The used multi-task datasets are brieﬂy described as
follows. The detailed statistics are listed in Table 1.

Movie Reviews The movie reviews dataset con-
sists of four sub-datasets about movie reviews.

• SST-1 The movie reviews with ﬁve classes in
the Stanford Sentiment Treebank1 (Socher et
al., 2013).

• SST-2 The movie reviews with binary classes.
It is also from the Stanford Sentiment Tree-
bank.

• SUBJ The movie reviews with labels of sub-

jective or objective (Pang and Lee, 2004).

• IMDB The IMDB dataset2 consists of 100,000
movie reviews with binary classes (Maas et al.,
2011). One key aspect of this dataset is that
each movie review has several sentences.

Product Reviews This dataset3, constructed by
Blitzer et al. (2007), contains Amazon product re-
views from four different domains: Books, DVDs,
Electronics and Kitchen appliances. The goal in
each domain is to classify a product review as ei-
ther positive or negative. The datasets in each do-
main are partitioned randomly into training data, de-
velopment data and testing data with the proportion
of 70%, 20% and 10% respectively.

5.2 Competitor Methods for Multi-task

Learning

The multi-task frameworks proposed by previous
works are various while not all can be applied to the
tasks we focused. Nevertheless, we chose two most
related neural models for multi-task learning and im-
plement them as strong competitor methods .

• MT-CNN: This model is proposed by Collobert
and Weston (2008) with convolutional layer, in
which lookup-tables are shared partially while
other layers are task-speciﬁc.

• MT-DNN: The model is proposed by Liu et
al. (2015b) with bag-of-words input and multi-

t = fwrite(M(m)
M(m)
t−1, α(m)
t−1M(s)
t = α(s)
r(s)
t−1,
t−1, α(s)
t = fwrite(M(s)
M(s)

t

t

, h(m)
t

),

, r(m)
t

),

(17)

(18)

(19)

where the superscript s represents the parameters
are shared across different tasks; the superscript m
represents that the parameters or variables are task-
speciﬁc for task m.

In ARC-II, the local memories enhance the capac-
ity of memorizing, while global memory enables the
information ﬂowing from different tasks to interact
sufﬁciently.

4 Training

The task-speciﬁc representation h(m), emitted by
the deep muti-task architectures, is ultimately fed
into the corresponding task-speciﬁc output layers.
ˆy(m) = softmax(W(m)h(m) + b(m)),

(20)

where ˆy(m) is prediction probabilities for task m.

Given M related tasks, our global cost function is
the linear combination of cost function for all tasks.

φ =

λmL(ˆy(m), y(m))

(21)

M
(cid:88)

m=1

where λm is the weights for each task m respec-
tively.

Computational Cost Compared with vanilla
LSTM, our proposed two models do not cause much
extra computational cost while converge faster. In
our experiment, the most complicated ARC-II, costs
2 times as long as vanilla LSTM.

5 Experiment

In this section, we investigate the empirical perfor-
mances of our proposed architectures on two multi-
task datasets. Each dataset contains several related
tasks.

1http://nlp.stanford.edu/sentiment.
2http://ai.stanford.edu/˜amaas/data/

sentiment/

3https://www.cs.jhu.edu/˜mdredze/

datasets/sentiment/

layer perceptrons, in which a hidden layer is
shared.

models. For example, we can easily extend our mod-
els to incorporate the Tree-LSTM model.

5.3 Hyperparameters and Training

5.5 Multi-task Learning of Product Reviews

The networks are trained with backpropagation and
the gradient-based optimization is performed using
the Adagrad update rule (Duchi et al., 2011).

The word embeddings for all of the models are
initialized with the 100d GloVe vectors (840B token
version, (Pennington et al., 2014)) and ﬁne-tuned
during training to improve the performance. The
other parameters are initialized by randomly sam-
pling from uniform distribution in [−0.1, 0.1]. The
mini-batch size is set to 16.

For each task, we take the hyperparameters which
achieve the best performance on the development
set via an small grid search over combinations of
the initial learning rate [0.1, 0.01], l2 regularization
[0.0, 5E−5, 1E−5]. For datasets without develop-
ment set, we use 10-fold cross-validation (CV) in-
stead. The ﬁnal hyper-parameters are set as Table 2.

Table 4 shows the classiﬁcation accuracies on the
tasks of product reviews. The row of “Single Task”
shows the results of the baseline for each individ-
ual task. With the help of global shared memory
(ARC-I), the performances of these four tasks are
improved by an average of 2.9%(2.6%) compared
with LSTM(ME-LSTM). ARC-II achieves best per-
formances on three sub-tasks, and its average im-
provement is 3.7%(3.5%). Compared with MT-CNN
and MT-DNN, our models achieve a better perfor-
mance. We think the reason is that our models can
not only share lexical information but share compli-
cated patterns of sentences by reading/writing op-
erations of external memory. Furthermore, these re-
sults on product reviews are consistent with that on
movie reviews, which shows our architectures are
robust.

5.4 Multi-task Learning of Movie Reviews

5.6 Case Study

We ﬁrst compare our proposed models with the
baseline system for single task classiﬁcation. Table
3 shows the classiﬁcation accuracies on the movie
reviews dataset. The row of “Single Task” shows the
results of LSTM and ME-LSTM for each individ-
ual task. With the help of multi-task learning, the
performances of these four tasks are improved by
1.8% (ARC-I) and 2.9% (ARC-II) on average rela-
tive to LSTM. We can ﬁnd that the architecture of
local-global hybrid external memory has better per-
formances. The reason is that the global memory in
ARC-I could store some task-speciﬁc information
besides shared information, which maybe noisy to
other tasks. Moreover, both of our proposed mod-
els outperform MT-CNN and MT-DNN, which indi-
cates the effectiveness of our proposed shared mech-
anism. To give an intuitive evaluation of these re-
sults, we also list the following state-of-the-art neu-
ral models. With the help of utilizing the shared in-
formation of several related tasks, our results out-
perform most of state-of-the-art models. Although
Tree-LSTM outperforms our method on SST-1, it
needs an external parser to get the sentence topologi-
cal structure. It is worth noticing that our models are
generic and compatible with the other LSTM based

To get an intuitive understanding of what is happen-
ing when we use shared memory to predict the class
of text, we design an experiment to compare and an-
alyze the difference between our models and vanilla
LSTM, thereby demonstrating the effectiveness of
our proposed architectures.

We sample two sentences from the SST-2 valida-
tion dataset, and the changes of the predicted sen-
timent score at different time steps are shown in
Figure 3, which are obtained by vanilla LSTM and
ARC-I respectively. Additionally, both models are
bidirectional for better visualization. To get more
insights into how the shared external memory in-
ﬂuences the speciﬁc task, we plot and observe the
evolving activation of fusion gates through time,
which controls signals ﬂowing from a shared exter-
nal memory to task-speciﬁc output, to understand
the behaviour of neurons.

For the sentence “It is a cookie-cutter movie, a
cut-and-paste job.”, which has a negative sentiment,
while the standard LSTM gives a wrong predic-
tion due to not understanding the informative words
“cookie-cutter” and “cut-and-paste”.

In contrast, our model makes a correct prediction
and the reason can be inferred from the activation of

(a)

(c)

(b)

(d)

Figure 3: (a)(b) The change of the predicted sentiment score at different time steps. Y-axis represents the
sentiment score, while X-axis represents the input words in chronological order. The red horizontal line gives
a border between the positive and negative sentiments. (c)(d) Visualization of the fusion gate’s activation.

fusion gates. As shown in Figure 3-(c), we can see
clearly the neurons are activated much when they
take input as “cookie-cutter” and “cut-and-paste”,
which indicates much information in shared mem-
ory has be passed into LSTM, therefore enabling the
model to give a correct prediction.

Another case “If you were not nearly moved to
tears by a couple of scenes , you ’ve got ice water in
your veins”, a subjunctive clause introduced by “if ”,
has a positive sentiment.

As shown in Figure 3-(b,d), vanilla LSTM failed
to capture the implicit meaning behind the sentence,
while our model is sensitive to the pattern “If ... were
not ...” and has an accurate understanding of the
sentence, which indicates the shared memory mech-
anism can not only enrich the meaning of certain
words, but teach some information of sentence struc-
ture to speciﬁc task.

6 Related Work

Neural networks based multi-task learning has been
proven effective in many NLP problems (Collobert
and Weston, 2008; Glorot et al., 2011; Liu et al.,
2015b; Liu et al., 2016b). In most of these models,
the lower layers are shared across all tasks, while top
layers are task-speciﬁc.

Collobert and Weston (2008) used a shared rep-
resentation for input words and solved different tra-
ditional NLP tasks within one framework. However,

only one lookup table is shared, and the other lookup
tables and layers are task-speciﬁc.

Liu et al. (2015b) developed a multi-task DNN for
learning representations across multiple tasks. Their
multi-task DNN approach combines tasks of query
classiﬁcation and ranking for web search. But the
input of the model is bag-of-word representation,
which loses the information of word order.

More

recently,

several multi-task encoder-
decoder networks were also proposed for neural
machine translation (Dong et al., 2015; Luong et
al., 2015; Firat et al., 2016), which can make use of
cross-lingual information.

Unlike these works, in this paper we design two
neural architectures with shared memory for multi-
task learning, which can store useful information
across the tasks. Our architectures are relatively
loosely coupled, and therefore more ﬂexible to ex-
pand. With the help of shared memory, we can ob-
tain better task-speciﬁc sentence representation by
utilizing the knowledge obtained by other related
tasks.

7 Conclusion and Future Work

In this paper, we introduce two deep architectures
for multi-task learning. The difference with the pre-
vious models is the mechanisms of sharing infor-
mation among several tasks. We design an external
memory to store the knowledge shared by several re-

lated tasks. Experimental results show that our mod-
els can improve the performances of several related
tasks by exploring common features.

In addition, we also propose a deep fusion strat-
egy to integrate the information from the external
memory into task-speciﬁc LSTM with a fusion gate.
In future work, we would like to investigate the
other sharing mechanisms of neural network based
multi-task learning.

Acknowledgments

We would like to thank the anonymous reviewers for
their valuable comments. This work was partially
funded by National Natural Science Foundation of
China (No. 61532011 and 61672162), the National
High Technology Research and Development Pro-
gram of China (No. 2015AA015408).

References

Marcin Andrychowicz and Karol Kurach. 2016. Learn-
ing efﬁcient algorithms with hierarchical attentive
memory. arXiv preprint arXiv:1602.03218.

D. Bahdanau, K. Cho, and Y. Bengio. 2014. Neural ma-
chine translation by jointly learning to align and trans-
late. ArXiv e-prints, September.

John Blitzer, Mark Dredze, Fernando Pereira, et al. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classiﬁcation.
In
ACL, volume 7, pages 440–447.

Rich Caruana. 1997. Multitask learning. Machine learn-

ing, 28(1):41–75.

Danqi Chen and Christopher D Manning.

2014. A
fast and accurate dependency parser using neural net-
works. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 740–750.

Ronan Collobert and Jason Weston. 2008. A uniﬁed ar-
chitecture for natural language processing: Deep neu-
ral networks with multitask learning. In Proceedings
of ICML.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
The Journal of Machine Learning Research, 12:2493–
2537.

Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalch-
brenner, and Alex Graves. 2016. Associative long
short-term memory. CoRR, abs/1602.03032.

Misha Denil, Alban Demiraj, Nal Kalchbrenner, Phil
Blunsom, and Nando de Freitas. 2014. Modelling,

visualising and summarising documents with a sin-
arXiv preprint
gle convolutional neural network.
arXiv:1406.3830.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for multi-
ple language translation. In Proceedings of the ACL.

John Duchi, Elad Hazan, and Yoram Singer.

2011.
Adaptive subgradient methods for online learning and
The Journal of Machine
stochastic optimization.
Learning Research, 12:2121–2159.

Jeffrey L Elman. 1990. Finding structure in time. Cog-

nitive science, 14(2):179–211.

Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016.
Multi-way, multilingual neural machine translation
with a shared attention mechanism. arXiv preprint
arXiv:1601.01073.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classiﬁcation: A deep learning approach. In Proceed-
ings of the 28th International Conference on Machine
Learning (ICML-11), pages 513–520.

Alex Graves, Greg Wayne,
Neural

turing machines.

and Ivo Danihelka.
arXiv preprint

2014.
arXiv:1410.5401.

Alex Graves. 2013. Generating sequences with recurrent
neural networks. arXiv preprint arXiv:1308.0850.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9(8):1735–
1780.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen.
2014. Convolutional neural network architectures for
matching natural language sentences. In Advances in
Neural Information Processing Systems.

Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever.
2015. An empirical exploration of recurrent network
In Proceedings of The 32nd Interna-
architectures.
tional Conference on Machine Learning.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for mod-
elling sentences. In Proceedings of ACL.

Yoon Kim. 2014. Convolutional neural networks for sen-
tence classiﬁcation. arXiv preprint arXiv:1408.5882.
PengFei Liu, Xipeng Qiu, Xinchi Chen, Shiyu Wu, and
Xuanjing Huang. 2015a. Multi-timescale long short-
term memory neural network for modelling sentences
and documents. In Proceedings of the Conference on
EMNLP.

Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng,
Kevin Duh, and Ye-Yi Wang. 2015b. Representa-
tion learning using multi-task deep neural networks for
semantic classiﬁcation and information retrieval.
In
NAACL.

Pengfei Liu, Xipeng Qiu, Jifan Chen, and Xuanjing
Huang. 2016a. Deep fusion LSTMs for text seman-
tic matching. In Proceedings of Annual Meeting of the
Association for Computational Linguistics.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016b.
Recurrent neural network for text classiﬁcation with
In Proceedings of International
multi-task learning.
Joint Conference on Artiﬁcial Intelligence.

Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol
2015. Multi-task
arXiv preprint

Vinyals, and Lukasz Kaiser.
sequence to sequence learning.
arXiv:1511.06114.

Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Pro-
ceedings of the ACL, pages 142–150.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efﬁcient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In Proceedings of ACL.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning.
2014. Glove: Global vectors for word
representation. Proceedings of the Empiricial Meth-
ods in Natural Language Processing (EMNLP 2014),
12:1532–1543.

Richard Socher, Jeffrey Pennington, Eric H Huang, An-
drew Y Ng, and Christopher D Manning. 2011. Semi-
supervised recursive autoencoders for predicting sen-
timent distributions. In Proceedings of EMNLP.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositionality
In Proceed-
through recursive matrix-vector spaces.
ings of EMNLP, pages 1201–1211.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In Proceedings of EMNLP.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances in
Neural Information Processing Systems, pages 2431–
2439.

Kai Sheng Tai, Richard Socher, and Christopher D Man-
ning.
Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075.

2015.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of ACL.

Tian Wang and Kyunghyun Cho.

context
arXiv:1511.03729.

language modelling.

2015.
Larger-
arXiv preprint

Movie

Product

Dataset

SST-1
SST-2
SUBJ
IMDB
Books
DVDs

Type Train Size Dev. Size Test Size Class Avg. Length Vocabulary Size
Sen.
Sen.
Sen.
Doc.
Doc.
Doc.
Electronics Doc.
Kitchen Doc.

8544
6920
9000
25,000
1400
1400
1400
1400

2210
1821
1000
25,000
400
400
400
400

18K
15K
21K
392K
27K
29K
14K
12K

1101
872
-
-
200
200
200
200

19
18
21
294
181
197
117
98

5
2
2
2
2
2
2
2

Table 1: Statistics of two multi-task datasets. Each dataset consists of four related tasks.

Single Task

Multi-task

Model

LSTM
ME-LSTM
ARC-I
ARC-II
MT-CNN
MT-DNN

NBOW
RAE (Socher et al., 2011)
MV-RNN (Socher et al., 2012)
RNTN (Socher et al., 2013)
DCNN (Kalchbrenner et al., 2014)
CNN-multichannel (Kim, 2014)
Tree-LSTM (Tai et al., 2015)

SST-1
45.9
46.4
48.6
49.5
46.7
44.5
42.4
43.2
44.4
45.7
48.5
47.4
50.6

SST-2
85.8
85.5
87.0
87.8
86.1
84.0
80.5
82.4
82.9
85.4
86.8
88.1
86.9

SUBJ
91.6
91.0
93.8
95.0
92.2
90.1
91.3
-
-
-
-
93.2
-

IMDB
88.5
88.7
89.8
91.2
88.4
85.6
83.6
-
-
-
89.3
-
-

Avg∆
-
-
+(1.8/1.9)
+(2.9/3.0)
-
-
-
-
-
-
-
-
-

Table 3: Accuracies of our models on movie reviews tasks against state-of-the-art neural models. The last
column gives the improvements relative to LSTM and ME-LSTM respectively. NBOW: Sums up the word
vectors and applies a non-linearity followed by a softmax classiﬁcation layer. RAE: Recursive Autoencoders
with pre-trained word vectors from Wikipedia (Socher et al., 2011). MV-RNN: Matrix-Vector Recursive
Neural Network with parse trees (Socher et al., 2012). RNTN: Recursive Neural Tensor Network with
tensor-based feature function and parse trees (Socher et al., 2013). DCNN: Dynamic Convolutional Neural
Network with dynamic k-max pooling (Kalchbrenner et al., 2014; Denil et al., 2014). CNN-multichannel:
Convolutional Neural Network (Kim, 2014). Tree-LSTM: A generalization of LSTMs to tree-structured
network topologies (Tai et al., 2015).

Model

Single Task

Multi-task

LSTM

Books DVDs Electronics Kitchen
78.0
ME-LSTM 77.5
81.2
82.8
80.2
79.7

ARC-I
ARC-II
MT-CNN
MT-DNN

81.2
81.5
84.5
85.5
83.4
82.5

81.8
82.1
84.3
84.0
83.0
82.8

79.5
80.2
82.0
83.0
81.0
80.5

Avg∆
-
-
+(2.9/2.6)
+(3.7/3.5)
-
-

Table 4: Accuracies of our models on product reviews dataset. The last column gives the improvement
relative to LSTM and ME-LSTM respectively.

