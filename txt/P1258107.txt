Deep Feature Flow for Video Recognition

Xizhou Zhu1,2∗

Yuwen Xiong2∗

Jifeng Dai2

Lu Yuan2

Yichen Wei2

1University of Science and Technology of China
ezra0408@mail.ustc.edu.cn

2Microsoft Research
{v-yuxio,jifdai,luyuan,yichenw}@microsoft.com

7
1
0
2
 
n
u
J
 
5
 
 
]

V
C
.
s
c
[
 
 
2
v
5
1
7
7
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

Deep convolutional neutral networks have achieved
great success on image recognition tasks. Yet, it is non-
trivial to transfer the state-of-the-art image recognition net-
works to videos as per-frame evaluation is too slow and un-
affordable. We present deep feature ﬂow, a fast and accu-
rate framework for video recognition.
It runs the expen-
sive convolutional sub-network only on sparse key frames
and propagates their deep feature maps to other frames via
a ﬂow ﬁeld. It achieves signiﬁcant speedup as ﬂow com-
putation is relatively fast. The end-to-end training of the
whole architecture signiﬁcantly boosts the recognition ac-
curacy. Deep feature ﬂow is ﬂexible and general. It is vali-
dated on two video datasets on object detection and seman-
tic segmentation. It signiﬁcantly advances the practice of
video recognition tasks. Code is released at https://
github.com/msracver/Deep-Feature-Flow.

1. Introduction

Recent years have witnessed signiﬁcant success of deep
convolutional neutral networks (CNNs) for image recogni-
tion, e.g., image classiﬁcation [23, 40, 42, 16], semantic
segmentation [28, 4, 52], and object detection [13, 14, 12,
35, 8, 27]. With their success, the recognition tasks have
been extended from image domain to video domain, such
as semantic segmentation on Cityscapes dataset [6], and ob-
ject detection on ImageNet VID dataset [37]. Fast and ac-
curate video recognition is crucial for high-value scenarios,
e.g., autonomous driving and video surveillance. Neverthe-
less, applying existing image recognition networks on indi-
vidual video frames introduces unaffordable computational
cost for most applications.

It is widely recognized that image content varies slowly
over video frames, especially the high level semantics [46,
53, 21]. This observation has been used as means of reg-
ularization in feature learning, considering videos as unsu-

∗This work is done when Xizhou Zhu and Yuwen Xiong are interns at

Microsoft Research Asia

pervised data sources [46, 21]. Yet, such data redundancy
and continuity can also be exploited to reduce the computa-
tion cost. This aspect, however, has received little attention
for video recognition using CNNs in the literature.

Modern CNN architectures [40, 42, 16] share a com-
mon structure. Most layers are convolutional and account
for the most computation. The intermediate convolutional
feature maps have the same spatial extent of the input im-
age (usually at a smaller resolution, e.g., 16× smaller).
They preserve the spatial correspondences between the low
level image content and middle-to-high level semantic con-
cepts [48]. Such correspondence provides opportunities to
cheaply propagate the features between nearby frames by
spatial warping, similar to optical ﬂow [17].

In this work, we present deep feature ﬂow, a fast and
accurate approach for video recognition. It applies an image
recognition network on sparse key frames.
It propagates
the deep feature maps from key frames to other frames via
a ﬂow ﬁeld. As exemplifed in Figure 1, two intermediate
feature maps are responsive to “car” and “person” concepts.
They are similar on two nearby frames. After propagation,
the propagated features are similar to the original features.
Typically, the ﬂow estimation and feature propagation
are much faster than the computation of convolutional fea-
tures. Thus, the computational bottleneck is avoided and
signiﬁcant speedup is achieved. When the ﬂow ﬁeld is also
estimated by a network, the entire architecture is trained
end-to-end, with both image recognition and ﬂow networks
optimized for the recognition task. The recognition accu-
racy is signiﬁcantly boosted.

In sum, deep feature ﬂow is a fast, accurate, general,
and end-to-end framework for video recognition.
It can
adopt most state-of-the-art image recognition networks in
the video domain. Up to our knowledge, it is the ﬁrst
work to jointly train ﬂow and video recognition tasks in
a deep learning framework. Extensive experiments ver-
ify its effectiveness on video object detection and seman-
tic segmentation tasks, on recent large-scale video datasets.
Compared to per-frame evaluation, our approach achieves
unprecedented speed (up to 10× faster, real time frame
rate) with moderate accuracy loss (a few percent). The

Figure 1. Motivation of proposed deep feature ﬂow approach. Here we visualize the two ﬁlters’ feature maps on the last convolutional layer
of our ResNet-101 model (see Sec. 4 for details). The convolutional feature maps are similar on two nearby frames. They can be cheaply
propagated from the key frame to current frame via a ﬂow ﬁeld.

high performance facilitates video recognition tasks in
practice. Code is released at https://github.com/
msracver/Deep-Feature-Flow.

2. Related Work

To our best knowledge, our work is unique and there is
no previous similar work to directly compare with. Never-
theless, it is related to previous works in several aspects, as
discussed below.

Image Recognition Deep learning has been success-
ful on image recognition tasks. The network architectures
have evolved and become powerful on image classiﬁca-
tion [23, 40, 42, 15, 20, 16]. For object detection, the
region-based methods [13, 14, 12, 35, 8] have become the
dominant paradigm. For semantic segmentation, fully con-
volutional networks (FCNs) [28, 4, 52] have dominated the
ﬁeld. However, it is computationally unaffordable to di-
rectly apply such image recognition networks on all the
frames for video recognition. Our work provides an effec-
tive and efﬁcient solution.

Network Acceleration Various approaches have been
proposed to reduce the computation of networks. To name
a few, in [50, 12] matrix factorization is applied to de-
compose large network layers into multiple small layers.
In [7, 34, 18], network weights are quantized. These tech-

niques work on single images. They are generic and com-
plementary to our approach.

Optical Flow It is a fundamental task in video anal-
ysis. The topic has been studied for decades and domi-
nated by variational approaches [17, 2], which mainly ad-
dress small displacements [44]. The recent focus is on large
displacements [3], and combinatorial matching (e.g., Deep-
Flow [45], EpicFlow [36]) has been integrated into the vari-
ational approach. These approaches are all hand-crafted.

Deep learning and semantic information have been ex-
ploited for optical ﬂow recently. FlowNet [9] ﬁrstly applies
deep CNNs to directly estimate the motion and achieves
good result. The network architecture is simpliﬁed in the re-
cent Pyramid Network [33]. Other works attempt to exploit
semantic segmentation information to help optical ﬂow es-
timation [38, 1, 19], e.g., providing speciﬁc constraints on
the ﬂow according to the category of the regions.

Optical ﬂow information has been exploited to help vi-
sion tasks, such as pose estimation [32], frame predic-
tion [31], and attribute transfer [49]. This work exploits
optical ﬂow to speed up general video recognition tasks.

Exploiting Temporal Information in Video Recogni-
tion T-CNN [22] incorporates temporal and contextual in-
formation from tubelets in videos. The dense 3D CRF [24]
proposes long-range spatial-temporal regularization in se-
STFCN [10] considers a
mantic video segmentation.

spatial-temporal FCN for semantic video segmentation.
These works operate on volume data, show improved recog-
nition accuracy but greatly increase the computational cost.
By contrast, our approach seeks to reduce the computation
by exploiting temporal coherence in the videos. The net-
work still runs on single frames and is fast.

Slow Feature Analysis High level semantic concepts
usually evolve slower than the low level image appear-
ance in videos. The deep features are thus expected to
vary smoothly on consecutive video frames. This obser-
vation has been used to regularize the feature learning in
videos [46, 21, 53, 51, 41]. We conjecture that our approach
may also beneﬁt from this fact.

Clockwork Convnets [39] It is the most related work
to ours as it also disables certain layers in the network on
certain video frames and reuses the previous features. It is,
however, much simpler and less effective than our approach.
About speed up, Clockwork only saves the computation
of some layers (e.g., 1/3 or 2/3) in some frames (e.g., every
other frame). As seen later, our method saves that on most
layers (task network has only 1 layer) in most frames (e.g.,
9 out of 10 frames). Thus, our speedup ratio is much higher.
About accuracy, Clockwork does not exploit the corre-
spondence between frames and simply copies features. It
only reschedules the computation of inference in an off-
the-shelf network and does not perform ﬁne-tuning or re-
training. Its accuracy drop is pretty noticeable at even small
speed up. In Table 4 and 6 of their arxiv paper, at 77% full
runtime (thus 1.3 times faster), Mean IU drops from 31.1 to
26.4 on NYUD, from 70.0 to 64.0 on Youtube, from 65.9
to 63.3 on Pascal, and from 65.9 to 64.4 on Cityscapes. By
contrast, we re-train a two-frame network with motion con-
sidered end-to-end. The accuracy drop is small, e.g., from
71.1 to 70.0 on Cityscape while being 3 times faster (Fig-
ure 3, bottom).

About generality, Clockwork is only applicable for se-
mantic segmentation with FCN. Our approach transfers
general image recognition networks to the video domain.

3. Deep Feature Flow

Table 1 summarizes the notations used in this paper. Our

approach is brieﬂy illustrated in Figure 2.

Deep Feature Flow Inference Given an image recogni-
tion task and a feed-forward convolutional neutral network
N that outputs result for input image I as y = N (I). Our
goal is to apply the network to all video frames Ii, i =
0, ..., ∞, fast and accurately.

Following the modern CNN architectures [40, 42, 16]
and applications [28, 4, 52, 13, 14, 12, 35, 8], without loss
of generality, we decompose N into two consecutive sub-
networks. The ﬁrst sub-network Nf eat, dubbed feature net-
work, is fully convolutional and outputs a number of in-
termediate feature maps, f = Nf eat(I). The second sub-

k
i
r
l
s
Ii, Ik
yi, yk
fk
fi
Mi→k
p, q
Si→k
N
Nf eat
Ntask
F
W

key frame index
current frame index
per-frame computation cost ratio, Eq. (5)
key frame duration length
overall speedup ratio, Eq. (7)
video frames
recognition results
convolutional feature maps on key frame
propagated feature maps on current frame
2D ﬂow ﬁeld
2D location
scale ﬁeld
image recognition network
sub-network for feature extraction
sub-network for recognition result
ﬂow estimation function
feature propagation function, Eq. (3)

Table 1. Notations.

network Ntask, dubbed task network, has speciﬁc structures
for the task and performs the recognition task over the fea-
ture maps, y = Ntask(f ).

Consecutive video frames are highly similar. The simi-
larity is even stronger in the deep feature maps, which en-
code high level semantic concepts [46, 21]. We exploit the
similarity to reduce computational cost. Speciﬁcally, the
feature network Nf eat only runs on sparse key frames. The
feature maps of a non-key frame Ii are propagated from its
preceding key frame Ik.

The features in the deep convolutional layers encode the
semantic concepts and correspond to spatial locations in the
image [48]. Examples are illustrated in Figure 1. Such spa-
tial correspondence allows us to cheaply propagate the fea-
ture maps by the manner of spatial warping.

Let Mi→k be a two dimensional ﬂow ﬁeld.

It is ob-
tained by a ﬂow estimation algorithm F such as [26, 9],
Mi→k = F(Ik, Ii).
It is bi-linearly resized to the same
It
spatial resolution of the feature maps for propagation.
projects back a location p in current frame i to the location
p + δp in key frame k, where δp = Mi→k(p).

As the values δp are in general fractional, the feature

warping is implemented via bilinear interpolation

f c
i (p) =

G(q, p + δp)f c

k(q),

(1)

(cid:88)

q

where c identiﬁes a channel in the feature maps f , q enu-
merates all spatial locations in the feature maps, and G(·, ·)
denotes the bilinear interpolation kernel. Note that G is two

Algorithm 1 Deep feature ﬂow inference algorithm for
video recognition.

1: input: video frames {Ii}
2: k = 0;
3: f0 = Nf eat(I0)
4: y0 = Ntask(f0)
5: for i = 1 to ∞ do
6:
7:
8:

k = i
fk = Nf eat(Ik)
yk = Ntask(fk)

if is key f rame(i) then

else

fi = W(fk, F(Ik, Ii), S(Ik, Ii))
yi = Ntask(fi)

9:
10:
11:
12:
end if
13:
14: end for
15: output: recognition results {yi}

(cid:46) initialize key frame

(cid:46) key frame scheduler
(cid:46) update the key frame

(cid:46) use feature ﬂow
(cid:46) propagation

use a CNN to estimate the ﬂow ﬁeld and the scale ﬁeld such
that all the components can be jointly trained end-to-end for
the task.

The architecture is illustrated in Figure 2(b). Train-
ing is performed by stochastic gradient descent (SGD). In
each mini-batch, a pair of nearby video frames, {Ik, Ii}1,
0 ≤ i − k ≤ 9, are randomly sampled. In the forward pass,
feature network Nf eat is applied on Ik to obtain the feature
maps fk. Next, a ﬂow network F runs on the frames Ii, Ik to
estimate the ﬂow ﬁeld and the scale ﬁeld. When i > k, fea-
ture maps fk are propagated to fi as in Eq. (3). Otherwise,
the feature maps are identical and no propagation is done.
Finally, task network Ntask is applied on fi to produce the
result yi, which incurs a loss against the ground truth result.
The loss error gradients are back-propagated throughout to
update all the components. Note that our training accom-
modates the special case when i = k and degenerates to the
per-frame training as in Figure 2(a).

The ﬂow network is much faster than the feature net-
It is pre-trained on the
work, as will be elaborated later.
Flying Chairs dataset [9]. We then add the scale function
S as a sibling output at the end of the network, by increas-
ing the number of channels in the last convolutional layer
appropriately. The scale function is initialized to all ones
(weights and biases in the output layer are initialized as 0s
and 1s, respectively). The augmented ﬂow network is then
ﬁne-tuned as in Figure 2(b).

The feature propagation function in Eq.(3) is unconven-
tional. It is parameter free and fully differentiable. In back-
propagation, we compute the derivative of the features in fi
with respect to the features in fk, the scale ﬁeld Si→k, and
the ﬂow ﬁeld Mi→k. The ﬁrst two are easy to compute us-

1The same notations are used for consistency although there is no

longer the concept of “key frame” during training.

Figure 2. Illustration of video recognition using per-frame network
evaluation (a) and the proposed deep feature ﬂow (b).

dimensional and is separated into two one dimensional ker-
nels as

G(q, p + δp) = g(qx, px + δpx) · g(qy, py + δpy),

(2)

where g(a, b) = max(0, 1 − |a − b|).

We note that Eq. (1) is fast to compute as a few terms are

non-zero.

The spatial warping may be inaccurate due to errors in
ﬂow estimation, object occlusion, etc. To better approx-
imate the features, their amplitudes are modulated by a
“scale ﬁeld” Si→k, which is of the same spatial and chan-
nel dimensions as the feature maps. The scale ﬁeld is ob-
tained by applying a “scale function” S on the two frames,
Si→k = S(Ik, Ii).

Finally, the feature propagation function is deﬁned as

fi = W(fk, Mi→k, Si→k),

(3)

where W applies Eq.(1) for all locations and all channels
in the feature maps, and multiples the features with scales
Si→k in an element-wise way.

The proposed video recognition algorithm is called deep
feature ﬂow. It is summarized in Algorithm 1. Notice that
any ﬂow function F, such as the hand-crafted low-level ﬂow
(e.g., SIFT-Flow [26]), is readily applicable. Training the
ﬂow function is not obligate, and the scale function S is set
to ones everywhere.

Deep Feature Flow Training A ﬂow function is origi-
nally designed to obtain correspondence of low-level image
pixels.
It can be fast in inference, but may not be accu-
rate enough for the recognition task, in which the high-level
feature maps change differently, usually slower than pix-
els [21, 39]. To model such variations, we propose to also

ing the chain rule. For the last, from Eq. (1) and (3), for
each channel c and location p in current frame, we have

∂f c

i (p)
∂Mi→k(p)

= Sc

i→k(p)

(cid:88)

q

∂G(q, p + δp)
∂δp

f c
k(q).

(4)

The term ∂G(q,p+δp)
can be derived from Eq. (2). Note that
the ﬂow ﬁeld M(·) is two-dimensional and we use ∂δp to
denote ∂δpx and ∂δpy for simplicity.

∂δp

The proposed method can easily be trained on datasets
where only sparse frames are annotated, which is usually
the case due to the high labeling costs in video recogni-
tion tasks [29, 11, 6]. In this case, the per-frame training
(Figure 2(a)) can only use annotated frames, while DFF can
easily use all frames as long as frame Ii is annotated. In
other words, DFF can fully use the data even with sparse
ground truth annotation. This is potentially beneﬁcial for
many video recognition tasks.

Inference Complexity Analysis For each non-key
frame, the computational cost ratio of the proposed ap-
proach (line 11-12 in Algorithm 1) and per-frame approach
(line 8-9) is

FlowNet

FlowNet Half

FlowNet Inception

ResNet-50

ResNet-101

9.20
12.71

33.56
46.30

68.97
95.24

Table 2. The approximated complexity ratio in Eq. (6) for different
feature network Nf eat and ﬂow network F, measured by their
FLOPs. See Section 4. Note that r (cid:28) 1 and we use 1
r here for
clarify. A signiﬁcant per-frame speedup factor is obtained.

when to allocate a new key frame. In this work, we use a
simple ﬁxed key frame scheduling, that is, the key frame
duration length l is a ﬁxed constant. It is easy to implement
and tune. However, varied changes in image content may
require a varying l to provide a smooth tradeoff between
accuracy and speed.
Ideally, a new key frame should be
allocated when the image content changes drastically.

How to design effective and adaptive key frame schedul-
ing can further improve our work. Currently it is beyond
the scope of this work. Different video tasks may present
different behaviors and requirements. Learning an adaptive
key frame scheduler from data seems an attractive choice.
This is worth further exploration and left as future work.

r =

O(F) + O(S) + O(W) + O(Ntask)
O(Nf eat) + O(Ntask)

,

(5)

4. Network Architectures

where O(·) measures the function complexity.

To understand this ratio, we ﬁrstly note that the com-
plexity of Ntask is usually small. Although its split point
in N is kind of arbitrary, as veriﬁed in experiment, it is
sufﬁcient to keep only one learnable weight layer in Ntask
in our implementation (see Sec. 4). While both Nf eat
and F have considerable complexity (Section 4), we have
O(Ntask) (cid:28) O(Nf eat) and O(Ntask) (cid:28) O(F).

We also have O(W) (cid:28) O(F) and O(S) (cid:28) O(F) be-
cause W and S are very simple. Thus, the ratio in Eq. (5) is
approximated as

r ≈

O(F)
O(Nf eat)

.

It is mostly determined by the complexity ratio of ﬂow
network F and feature network Nf eat, which can be pre-
cisely measured, e.g., by their FLOPs. Table 2 shows its
typical values in our implementation.

Compared to the per-frame approach,

the overall
speedup factor in Algorithm 1 also depends on the spar-
sity of key frames. Let there be one key frame in every l
consecutive frames, the speedup factor is

s =

l
1 + (l − 1) ∗ r

.

Key Frame Scheduling As indicated in Algorithm 1
(line 6) and Eq. (7), a crucial factor for inference speed is

(6)

(7)

The proposed approach is general for different networks
and recognition tasks. Towards a solid evaluation, we adopt
the state-of-the-art architectures and important vision tasks.
Flow Network We adopt the state-of-the-art CNN based
FlowNet architecture (the “Simple” version) [9] as default.
We also designed two variants of lower complexity. The
ﬁrst one, dubbed FlowNet Half, reduces the number of con-
volutional kernels in each layer of FlowNet by half and the
complexity to 1
4 . The second one, dubbed FlowNet Incep-
tion, adopts the Inception structure [43] and reduces the
complexity to 1
8 of that of FlowNet. The architecture de-
tails are reported in Appendix A.

The three ﬂow networks are pre-trained on the synthetic
Flying Chairs dataset in [9]. The output stride is 4. The in-
put image is half-sized. The resolution of ﬂow ﬁeld is there-
fore 1
8 of the original resolution. As the feature stride of the
feature network is 16 (as described below), the ﬂow ﬁeld
and the scale ﬁeld is further down-sized by half using bi-
linear interpolation to match the resolution of feature maps.
This bilinear interpolation is realized as a parameter-free
layer in the network and also differentiated during training.
Feature Network We use ResNet models [16], speciﬁ-
cally, the ResNet-50 and ResNet-101 models pre-trained for
ImageNet classiﬁcation as default. The last 1000-way clas-
siﬁcation layer is discarded. The feature stride is reduced
from 32 to 16 to produce denser feature maps, following the
practice of DeepLab [4, 5] for semantic segmentation, and
R-FCN [8] for object detection. The ﬁrst block of the conv5

layers are modiﬁed to have a stride of 1 instead of 2. The
holing algorithm [4] is applied on all the 3×3 convolutional
kernels in conv5 to keep the ﬁeld of view (dilation=2). A
randomly initialized 3×3 convolution is appended to conv5
to reduce the feature channel dimension to 1024, where the
holing algorithm is also applied (dilation=6). The resulting
1024-dimensional feature maps are the intermediate feature
maps for the subsequent task.

Table 2 presents the complexity ratio Eq. (6) of feature

networks and ﬂow networks.

Semantic Segmentation A randomly initialized 1 × 1
convolutional layer is applied on the intermediate feature
maps to produce (C +1) score maps, where C is the number
of categories and 1 is for background category. A following
softmax layer outputs the per-pixel probabilities. Thus, the
task network only has one learnable weight layer. The over-
all network architecture is similar to DeepLab with large
ﬁeld-of-view in [5].

Object Detection We adopt

the state-of-the-art R-
FCN [8]. On the intermediate feature maps, two branches of
fully convolutional networks are applied on the ﬁrst half and
the second half 512-dimensional of the intermediate feature
maps separately, for sub-tasks of region proposal and detec-
tion, respectively.

In the region proposal branch, the RPN network [35]
is applied. We use na = 9 anchors (3 scales and 3 as-
pect ratios). Two sibling 1 × 1 convolutional layers out-
put the 2na-dimensional objectness scores and the 4na-
dimensional bounding box (bbox) regression values, re-
spectively. Non-maximum suppression (NMS) is applied to
generate 300 region proposals for each image. Intersection-
over-union (IoU) threshold 0.7 is used.

In the detection branch,

two sibling 1 × 1 convolu-
tional layers output the position-sensitive score maps and
bbox regression maps, respectively. They are of dimensions
(C + 1)k2 and 4k2, respectively, where k banks of classi-
ﬁers/regressors are employed to encode the relative position
information. See [8] for details. On the position-sensitive
score/bbox regression maps, position-sensitive ROI pool-
ing is used to obtain the per-region classiﬁcation score and
bbox regression result. No free parameters are involved in
the per-region computation. Finally, NMS is applied on the
scored and regressed region proposals to produce the detec-
tion result, with IoU threshold 0.3.

5. Experiments

Unlike image datasets, large scale video dataset is much
harder to collect and annotate. Our approach is evaluated
on the two recent datasets: Cityscapes [6] for semantic seg-
mentation, and ImageNet VID [37] for object detection.

5.1. Experiment Setup

Cityscapes It is for urban scene understanding and au-
tonomous driving. It contains snippets of street scenes col-
lected from 50 different cities, at a frame rate of 17 fps. The
train, validation, and test sets contain 2975, 500, and 1525
snippets, respectively. Each snippet has 30 frames, where
the 20th frame is annotated with pixel-level ground-truth la-
bels for semantic segmentation. There are 30 semantic cat-
egories. Following the protocol in [5], training is performed
on the train set and evaluation is performed on the validation
set. The semantic segmentation accuracy is measured by the
pixel-level mean intersection-over-union (mIoU) score.

In both training and inference, the images are resized to
have shorter sides of 1024 and 512 pixels for the feature net-
work and the ﬂow network, respectively. In SGD training,
20K iterations are performed on 8 GPUs (each GPU holds
one mini-batch, thus the effective batch size ×8), where the
learning rates are 10−3 and 10−4 for the ﬁrst 15K and the
last 5K iterations, respectively.

ImageNet VID It is for object detection in videos. The
training, validation, and test sets contain 3862, 555, and 937
fully-annotated video snippets, respectively. The frame rate
is 25 or 30 fps for most snippets. There are 30 object cate-
gories, which are a subset of the categories in the ImageNet
DET image dataset2. Following the protocols in [22, 25],
evaluation is performed on the validation set, using the stan-
dard mean average precision (mAP) metric.

In both training and inference, the images are resized to
have shorter sides of 600 pixels and 300 pixels for the fea-
ture network and the ﬂow network, respectively. In SGD
training, 60K iterations are performed on 8 GPUs, where
the learning rates are 10−3 and 10−4 for the ﬁrst 40K and
the last 20K iterations, respectively.

During training, besides the ImageNet VID train set, we
also used the ImageNet DET train set (only the same 30
category labels are used), following the protocols in [22,
25]. Each mini-batch samples images from either ImageNet
VID or ImageNet DET datasets, at 2 : 1 ratio.

5.2. Evaluation Methodology and Results

Deep feature ﬂow is ﬂexible and allows various design
choices. We evaluate their effects comprehensively in the
experiment. For clarify, we ﬁx their default values through-
out the experiments, unless speciﬁed otherwise. For feature
network Nf eat, ResNet-101 model is default. For ﬂow net-
work F, FlowNet (section 4) is default. Key-frame duration
length l is 5 for Cityscapes [6] segmentation and 10 for Im-
ageNet VID [37] detection by default, based on different
frame rate of videos in the datasets..

For each snippet we evaluate l image pairs, (k, i), k =
i − l + 1, ..., i, for each frame i with ground truth anno-

2http://www.image-net.org/challenges/LSVRC/

method

training of image recognition network N training of ﬂow network F

trained on single frames as in Fig. 2 (a)

no ﬂow network used

Frame (oracle baseline)
SFF-slow
SFF-fast
DFF
DFF ﬁx N
DFF ﬁx F
DFF separate

same as Frame

same as Frame

same as DFF

same as Frame

trained on frame pairs as in Fig. 2 (b)

init. on Flying Chairs [9], ﬁne-tuned in Fig. 2 (b)

same as Frame, then ﬁxed in Fig. 2 (b)

same as DFF

SIFT-Flow [26] (w/ best parameters), no training

SIFT-Flow [26] (w/ default parameters), no training

init. on Flying Chairs [9], then ﬁxed in Fig. 2 (b)

init. on Flying Chairs [9]

Table 3. Description of variants of deep feature ﬂow (DFF), shallow feature ﬂow (SFF), and the per-frame approach (Frame).

Cityscapes (l = 5)

ImageNet VID (l = 10)

mIoU(%)

runtime (fps) mAP(%)

runtime (fps)

Methods

Frame
SFF-slow
SFF-fast
DFF
DFF ﬁx N
DFF ﬁx F
DFF separate

71.1
67.8
67.3
69.2
68.8
67.0
66.9

1.52
0.08
0.95
5.60
5.60
5.60
5.60

73.9
70.7
69.7
73.1
72.3
68.8
67.4

4.05
0.26
3.04
20.25
20.25
20.25
20.25

Table 4. Comparison of accuracy and runtime (mostly in GPU) of
various approaches in Table 3. Note that, the runtime for SFF con-
sists of CPU runtime of SIFT-Flow and GPU runtime of Frame,
since SIFT-Flow only has CPU implementation.

tation. Time evaluation is on a workstation with NVIDIA
K40 GPU and Intel Core i7-4790 CPU.

Validation of DFF Architecture We compared DFF

with several baselines and variants, as listed in Table 3.

• Frame: train N on single frames with ground truth.

• SFF: use pre-computed large-displacement ﬂow (e.g.,
SIFT-Flow [26]). SFF-fast and SFF-slow adopt differ-
ent parameters.

• DFF: the proposed approach, N and F are trained
end-to-end. Several variants include DFF ﬁx N (ﬁx
N in training), DFF ﬁx F (ﬁx F in training), and DFF
seperate (N and F are separately trained).

Table 4 summarizes the accuracy and runtime of all ap-
proaches. We ﬁrstly note that the baseline Frame is strong
enough to serve as a reference for comparison. Our im-
plementation resembles the state-of-the-art DeepLab [5] for
semantic segmentation and R-FCN [8] for object detection.
In DeepLab [5], an mIoU score of 69.2% is reported with
DeepLab large ﬁeld-of-view model using ResNet-101 on
Cityscapes validation dataset. Our Frame baseline achieves
slightly higher 71.1%, based on the same ResNet model.

For object detection, Frame baseline has mAP 73.9% us-
ing R-FCN [8] and ResNet-101. As a reference, a com-
parable mAP score of 73.8% is reported in [22], by com-
bining CRAFT [47] and DeepID-net [30] object detectors
trained on the ImageNet data, using both VGG-16 [40] and
GoogleNet-v2 [20] models, with various tricks (multi-scale
training/testing, adding context information, model ensem-
ble). We do not adopt above tricks as they complicate the
comparison and obscure the conclusions.

SFF-fast has a reasonable runtime but accuracy is sig-
niﬁcantly decreased. SFF-slow uses the best parameters for
ﬂow estimation. It is much slower. Its accuracy is slightly
improved but still poor. This indicates that an off-the-shelf
ﬂow may be insufﬁcient.

The proposed DFF approach has the best overall perfor-
mance. Its accuracy is slightly lower than that of Frame and
it is 3.7 and 5.0 times faster for segmentation and detection,
respectively. As expected, the three variants without using
joint training have worse accuracy. Especially, the accuracy
drop by ﬁxing F is signiﬁcant. This indicates a jointing
end-to-end training (especially ﬂow) is crucial.

We also tested another variant of DFF with the scale
function S removed (Algorithm 1, Eq (3), Eq. (4)). The
accuracy drops for both segmentation and detection (less
than one percent). It shows that the scaled modulation of
features is slightly helpful.

Accuracy-Speedup Tradeoff We investigate the trade-
off by varying the ﬂow network F, the feature network
Nf eat, and key frame duration length l. Since Cityscapes
and ImageNet VID datasets have different frame rates, we
tested l = 1, 2, ..., 10 for segmentation and l = 1, 2, ..., 20
for detection.

The results are summarized in Figure 3. Overall, DFF
achieves signiﬁcant speedup with decent accuracy drop. It
smoothly trades in accuracy for speed and ﬁts different
application needs ﬂexibly. For example, in detection, it
improves 4.05 fps of ResNet-101 Frame to 41.26 fps of
ResNet-101 + FlowNet Inception. The 10× faster speed is
at the cost of moderate accuracy drop from 73.9% to 69.5%.
In segmentation, it improves 2.24 fps of ResNet-50 Frame

# layers in Ntask

Cityscapes (l=5)

ImageNet VID (l=10)

mIoU(%)

runtime (fps) mAP(%)

runtime (fps)

21
12
5
1 (default)
0

69.1
69.1
69.2
69.2
69.5

2.87
3.14
3.89
5.60
5.61

73.2
73.3
73.2
73.1
72.7

7.23
8.04
9.99
20.25
20.40

Table 5. Results of using different split points for Ntask.

to 17.48 fps of ResNet-50 FlowNet Inception, at the cost of
accuracy drop from 69.7% to 62.4%.

What ﬂow F should we use? From Figure 3, the smallest
FlowNet Inception is advantageous. It is faster than its two
counterparts at the same accuracy level, most of the times.
What feature Nf eat should we use? In high-accuracy
zone, an accurate model ResNet-101 is clearly better than
ResNet-50. In high-speed zone, the conclusions are differ-
ent on the two tasks. For detection, ResNet-101 is still ad-
vantageous. For segmentation, the performance curves in-
tersect at around 6.35 fps point. For higher speed, ResNet-
50 becomes better than ResNet-101. The seemingly dif-
ferent conclusions can be partially attributed to the differ-
ent video frame rates, the extents of dynamics on the two
datasets. The Cityscapes dataset not only has a low frame
rate 17 fps, but also more quick dynamics. It would be hard
to utilize temporal redundancy for a long propagation. To
achieve the same high speed, ResNet-101 needs a larger key
frame length l than ResNet-50. This in turn signiﬁcantly
increases the difﬁculty of learning.

Above observations provide useful recommendations for
practical applications. Yet, they are more heuristic than gen-
eral, as they are observed only on the two tasks, on limited
data. We plan to explore the design space more in the future.
Split point of Ntask Where should we split Ntask in N ?
Recall that the default Ntask keeps one layer with learning
weight (the 1 × 1 conv over 1024-d feature maps, see Sec-
tion 4). Before this is the 3 × 3 conv layer that reduces di-
mension to 1024. Before this is series of “Bottleneck” unit
in ResNet [16], each consisting of 3 layers. We back move
the split point to make different Ntasks with 5, 12, and 21
layers, respectively. The one with 5 layers adds the dimen-
sion reduction layer and one bottleneck unit (conv5c). The
one with 12 layers adds two more units (conv5a and conv5b)
at the beginning of conv5. The one with 21 layers adds three
more units in conv4. We also move the only layer in default
Ntask into Nf eat, leaving Ntask with 0 layer (with learn-
able weights). This is equivalent to directly propagate the
parameter-free score maps, in both semantic segmentation
and object detection.

Table 5 summarizes the results. Overall, the accuracy
variation is small enough to be neglected. The speed be-

Figure 3. (better viewed in color) Illustration of accuracy-speed
tradeoff under different implementation choices on ImageNet VID
detection (top) and on Cityscapes segmentation (bottom).

comes lower when Ntask has more layers. Using 0 layer
is mostly equivalent to using 1 layer, in both accuracy and
speed. We choose 1 layer as default as that leaves some tun-
able parameters after the feature propagation, which could
be more general.

Example results of the proposed method are presented
in Figure 4 and Figure 5 for video segmentation on
CityScapes and video detection on ImageNet VID, respec-
tively. More example results are available at https:
//www.youtube.com/watch?v=J0rMHE6ehGw.

6. Future Work

Several important aspects are left for further exploration.
It would be interesting to exploit how the joint learning af-
fects the ﬂow quality. We are unable to evaluate as there
lacks ground truth. Current optical ﬂow works are also lim-
ited to either synthetic data [9] or small real datasets, which
is insufﬁcient for deep learning.

Our method can further beneﬁt from improvements in
ﬂow estimation and key frame scheduling. In this paper, we
adopt FlowNet [9] mainly because there are few choices.
Designing faster and more accurate ﬂow network will cer-

Figure 4. Semantic segmentation results on Cityscapes validation dataset. The ﬁrst column corresponds to the images and the results on
the key frame (the kth frame). The following four columns correspond to the k + 1st, k + 2nd, k + 3rd and k + 4th frames, respectively.

Figure 5. Object detection results on ImageNet VID validation dataset. The ﬁrst column corresponds to the images and the results on the
key frame (the kth frame). The following four columns correspond to the k + 2nd, k + 4th, k + 6th and k + 8th frames, respectively.

layer

type

stride

# output

conv1
conv2
conv3
conv3 1
conv4
conv4 1
conv5
conv5 1
conv6
conv6 1

conv1
conv2
conv3
conv3 1
conv4
conv4 1
conv5
conv5 1
conv6
conv6 1

7x7 conv
5x5 conv
5x5 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv

7x7 conv
5x5 conv
5x5 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv

2
2
2

2

2

2

2
2
2

2

2

2

64
128
256
256
512
512
512
512
1024
1024

32
64
128
128
256
256
256
256
512
512

Table 6. The FlowNet network architecture.

layer

type

stride

# output

Table 7. The FlowNet Half network architecture.

tainly receive more attention in the future. For key frame
scheduling, a good scheduler may well signiﬁcantly im-
prove both speed and accuracy. And this problem is deﬁ-
nitely worth further exploration.

We believe this work opens many new possibilities. We

hope it will inspire more future work.

A. FlowNet Inception Architecture

The architectures of FlowNet, FlowNet Half follow that
of [9] (the “Simple” version), which are detailed in Table 6
and Table 7, respectively. The architecture of FlowNet In-
ception follows the design of the Inception structure [43],
which is detailed in Table 8.

References

[1] M. Bai, W. Luo, K. Kundu, and R. Urtasun. Exploiting se-
mantic information and deep matching for optical ﬂow. In

ECCV, 2016. 2

[2] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical ﬂow estimation based on a theory for warping.
In ECCV, 2004. 2

[3] T. Brox and J. Malik. Large displacement optical ﬂow: De-
scriptor matching in variational motion estimation. TPAMI,
2011. 2

[4] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-
volutional nets and fully connected crfs. In ICLR, 2015. 1,
2, 3, 5, 6

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. arXiv preprint, 2016. 5, 6, 7

[6] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In CVPR, 2016. 1, 5, 6

[7] M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect:
Training deep neural networks with binary weights during
propagations. In NIPS, 2015. 2

[8] J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via
region-based fully convolutional networks. In NIPS, 2016.
1, 2, 3, 5, 6, 7

[9] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
and V. Golkov. Flownet: Learning optical ﬂow with convo-
lutional networks. In ICCV, 2015. 2, 3, 4, 5, 7, 8, 11
[10] M. Fayyaz, M. H. Saffar, M. Sabokrou, M. Fathy, and
R. Klette. STFCN: spatio-temporal FCN for semantic video
segmentation. arXiv preprint, 2016. 2

[11] F. Galasso, N. Shankar Nagaraja, T. Jimenez Cardenas,
T. Brox, and B. Schiele. A uniﬁed video segmentation bench-
mark: Annotation, metrics and analysis. In ICCV, 2013. 5

[12] R. Girshick. Fast R-CNN. In ICCV, 2015. 1, 2, 3
[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014. 1, 2, 3

[14] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling
in deep convolutional networks for visual recognition.
In
ECCV, 2014. 1, 2, 3

[15] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In ICCV, 2015. 2

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 2, 3, 5, 8

[17] B. K. Horn and B. G. Schunck. Determining optical ﬂow.

Artiﬁcial intelligence, 1981. 1, 2

[18] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Y. Bengio. Quantized Neural Networks: Training Neu-
ral Networks with Low Precision Weights and Activations.
arXiv preprint, 2016. 2

[19] J. Hur and S. Roth. Joint optical ﬂow and temporally consis-
tent semantic segmentation. In ECCV CVRSUAD Workshop,
2016. 2

[20] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 2, 7

type

stride

# output

#1x1

#1x1-#3x3

#1x1-#3x3-#3x3

#pool

Inception/Reduction

layer

conv1
pool1
conv2
conv3 1
conv3 2
conv3 3
conv4 1
conv4 2
conv5 1
conv5 2
conv6 1
conv6 2

7x7 conv
3x3 max pool
Inception
3x3 conv
Inception
Inception
Reduction
Inception
Reduction
Inception
Reduction
Inception

2
2

2

2

2

2

32
32
64
128
128
128
256
256
384
384
512
512

24-32

24-32-32

48
48
32
96
48
144
64
192

32-64
48-64
112-128
112-128
96-192
96-192
192-256
192-256

8-16-16
12-16-16
28-32-32
28-32-32
36-48-48
36-48-48
48-64-64
48-64-64

64

96

128

Table 8. The FlowNet Inception network architecture, following the design of the Inception structure [43]. ”Inception/Reduction” modules
consist of four branches: 1x1 conv (#1x1), 1x1 conv-3x3 conv (#1x1-#3x3), 1x1 conv-3x3 conv-3x3 conv (#1x1-#3x3-#3x3), and 3x3 max
pooling followed by 1x1 conv (#pool, only for stride=2).

[21] D. Jayaraman and K. Grauman. Slow and steady feature
analysis: higher order temporal coherence in video.
In
CVPR, 2016. 1, 3, 4

[22] K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang,
Z. Wang, R. Wang, and X. Wang. T-cnn: Tubelets with con-
volutional neural networks for object detection from videos.
In CVPR, 2016. 2, 6, 7

[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1, 2

Imagenet
In

[24] A. Kundu, V. Vineet, and V. Koltun. Feature space optimiza-
tion for semantic video segmentation. In CVPR, 2016. 2
[25] B. Lee, E. Erdenee, S. Jin, and P. K. Rhee. Multi-class
multi-object tracking using changing point detection. arXiv
preprint, 2016. 6

[26] C. Liu, J. Yuen, A. Torralba, J. Sivic, and W. T. Freeman.
Sift ﬂow: dense correspondence across difference scenes. In
ECCV, 2008. 3, 4, 7

[27] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed.

Ssd: Single shot multibox detector. 2016. 1

[28] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, 2015. 1, 2, 3
[29] P. K. Nathan Silberman, Derek Hoiem and R. Fergus. Indoor
segmentation and support inference from rgbd images.
In
ECCV, 2012. 5

[30] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y. Tian, H. Li,
S. Yang, Z. Wang, and C.-C. Loy. Deepid-net: Deformable
deep convolutional neural networks for object detection. In
CVPR, 2015. 7

[31] V. Patraucean, A. Handa, and R. Cipolla. Spatio-temporal
arXiv

video autoencoder with differentiable memory.
preprint arXiv:1511.06309, 2015. 2

[32] T. Pﬁster, J. Charles, and A. Zisserman. Flowing convnets
for human pose estimation in videos. In ICCV, 2015. 2

[33] A. Ranjan and M. J. Black. Optical ﬂow estimation using a

spatial pyramid network. arXiv preprint, 2016. 2

[34] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. arXiv preprint, 2016. 2

[35] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-
wards real-time object detection with region proposal net-
works. In NIPS, 2015. 1, 2, 3, 6

[36] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
EpicFlow: Edge-Preserving Interpolation of Correspon-
dences for Optical Flow. In CVPR, 2015. 2

[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. IJCV, 2015. 1, 6

[38] L. Sevilla-Lara, D. Sun, V. Jampani, and M. J. Black. Optical
In
ﬂow with semantic segmentation and localized layers.
CVPR, 2016. 2

[39] E. Shelhamer, K. Rakelly, J. Hoffman, and T. Darrell. Clock-
work convnets for video semantic segmentation. In ECCV,
2016. 3, 4

[40] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
1, 2, 3, 7

[41] L. Sun, K. Jia, T.-H. Chan, Y. Fang, G. Wang, and S. Yan. Dl-
sfa: deeply-learned slow feature analysis for action recogni-
tion. In CVPR, 2014. 3

[42] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 1, 2, 3
[43] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR, 2016. 5, 11, 12

[44] J. Weickert, A. Bruhn, T. Brox, and N. Papenberg. A survey
on variational optic ﬂow methods for small displacements.
In Mathematical models for registration and applications to
medical imaging. 2006. 2

[45] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid.
DeepFlow: Large displacement optical ﬂow with deep
matching. In CVPR, 2013. 2

[46] L. Wiskott and T. J. Sejnowski. Slow feature analysis: Unsu-
pervised learning of invariances. Neural computation, 2002.
1, 3

[47] B. Yang, J. Yan, Z. Lei, and S. Z. Li. Craft objects from

images. In CVPR, 2016. 7

[48] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014. 1, 3

[49] W. Zhang, P. Srinivasan, and J. Shi. Discriminative image

warping with attribute ﬂow. In CVPR, 2011. 2

[50] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very
deep convolutional networks for classiﬁcation and detection.
TPAMI, 2015. 2

[51] Z. Zhang and D. Tao. Slow feature analysis for human action

recognition. TPAMI, 2012. 3

[52] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. Torr. Conditional random
ﬁelds as recurrent neural networks. In ICCV, 2015. 1, 2, 3

[53] W. Zou, S. Zhu, K. Yu, and A. Y. Ng. Deep learning of
invariant features via simulated ﬁxations in video. In NIPS,
2012. 1, 3

Deep Feature Flow for Video Recognition

Xizhou Zhu1,2∗

Yuwen Xiong2∗

Jifeng Dai2

Lu Yuan2

Yichen Wei2

1University of Science and Technology of China
ezra0408@mail.ustc.edu.cn

2Microsoft Research
{v-yuxio,jifdai,luyuan,yichenw}@microsoft.com

7
1
0
2
 
n
u
J
 
5
 
 
]

V
C
.
s
c
[
 
 
2
v
5
1
7
7
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

Deep convolutional neutral networks have achieved
great success on image recognition tasks. Yet, it is non-
trivial to transfer the state-of-the-art image recognition net-
works to videos as per-frame evaluation is too slow and un-
affordable. We present deep feature ﬂow, a fast and accu-
rate framework for video recognition.
It runs the expen-
sive convolutional sub-network only on sparse key frames
and propagates their deep feature maps to other frames via
a ﬂow ﬁeld. It achieves signiﬁcant speedup as ﬂow com-
putation is relatively fast. The end-to-end training of the
whole architecture signiﬁcantly boosts the recognition ac-
curacy. Deep feature ﬂow is ﬂexible and general. It is vali-
dated on two video datasets on object detection and seman-
tic segmentation. It signiﬁcantly advances the practice of
video recognition tasks. Code is released at https://
github.com/msracver/Deep-Feature-Flow.

1. Introduction

Recent years have witnessed signiﬁcant success of deep
convolutional neutral networks (CNNs) for image recogni-
tion, e.g., image classiﬁcation [23, 40, 42, 16], semantic
segmentation [28, 4, 52], and object detection [13, 14, 12,
35, 8, 27]. With their success, the recognition tasks have
been extended from image domain to video domain, such
as semantic segmentation on Cityscapes dataset [6], and ob-
ject detection on ImageNet VID dataset [37]. Fast and ac-
curate video recognition is crucial for high-value scenarios,
e.g., autonomous driving and video surveillance. Neverthe-
less, applying existing image recognition networks on indi-
vidual video frames introduces unaffordable computational
cost for most applications.

It is widely recognized that image content varies slowly
over video frames, especially the high level semantics [46,
53, 21]. This observation has been used as means of reg-
ularization in feature learning, considering videos as unsu-

∗This work is done when Xizhou Zhu and Yuwen Xiong are interns at

Microsoft Research Asia

pervised data sources [46, 21]. Yet, such data redundancy
and continuity can also be exploited to reduce the computa-
tion cost. This aspect, however, has received little attention
for video recognition using CNNs in the literature.

Modern CNN architectures [40, 42, 16] share a com-
mon structure. Most layers are convolutional and account
for the most computation. The intermediate convolutional
feature maps have the same spatial extent of the input im-
age (usually at a smaller resolution, e.g., 16× smaller).
They preserve the spatial correspondences between the low
level image content and middle-to-high level semantic con-
cepts [48]. Such correspondence provides opportunities to
cheaply propagate the features between nearby frames by
spatial warping, similar to optical ﬂow [17].

In this work, we present deep feature ﬂow, a fast and
accurate approach for video recognition. It applies an image
recognition network on sparse key frames.
It propagates
the deep feature maps from key frames to other frames via
a ﬂow ﬁeld. As exemplifed in Figure 1, two intermediate
feature maps are responsive to “car” and “person” concepts.
They are similar on two nearby frames. After propagation,
the propagated features are similar to the original features.
Typically, the ﬂow estimation and feature propagation
are much faster than the computation of convolutional fea-
tures. Thus, the computational bottleneck is avoided and
signiﬁcant speedup is achieved. When the ﬂow ﬁeld is also
estimated by a network, the entire architecture is trained
end-to-end, with both image recognition and ﬂow networks
optimized for the recognition task. The recognition accu-
racy is signiﬁcantly boosted.

In sum, deep feature ﬂow is a fast, accurate, general,
and end-to-end framework for video recognition.
It can
adopt most state-of-the-art image recognition networks in
the video domain. Up to our knowledge, it is the ﬁrst
work to jointly train ﬂow and video recognition tasks in
a deep learning framework. Extensive experiments ver-
ify its effectiveness on video object detection and seman-
tic segmentation tasks, on recent large-scale video datasets.
Compared to per-frame evaluation, our approach achieves
unprecedented speed (up to 10× faster, real time frame
rate) with moderate accuracy loss (a few percent). The

Figure 1. Motivation of proposed deep feature ﬂow approach. Here we visualize the two ﬁlters’ feature maps on the last convolutional layer
of our ResNet-101 model (see Sec. 4 for details). The convolutional feature maps are similar on two nearby frames. They can be cheaply
propagated from the key frame to current frame via a ﬂow ﬁeld.

high performance facilitates video recognition tasks in
practice. Code is released at https://github.com/
msracver/Deep-Feature-Flow.

2. Related Work

To our best knowledge, our work is unique and there is
no previous similar work to directly compare with. Never-
theless, it is related to previous works in several aspects, as
discussed below.

Image Recognition Deep learning has been success-
ful on image recognition tasks. The network architectures
have evolved and become powerful on image classiﬁca-
tion [23, 40, 42, 15, 20, 16]. For object detection, the
region-based methods [13, 14, 12, 35, 8] have become the
dominant paradigm. For semantic segmentation, fully con-
volutional networks (FCNs) [28, 4, 52] have dominated the
ﬁeld. However, it is computationally unaffordable to di-
rectly apply such image recognition networks on all the
frames for video recognition. Our work provides an effec-
tive and efﬁcient solution.

Network Acceleration Various approaches have been
proposed to reduce the computation of networks. To name
a few, in [50, 12] matrix factorization is applied to de-
compose large network layers into multiple small layers.
In [7, 34, 18], network weights are quantized. These tech-

niques work on single images. They are generic and com-
plementary to our approach.

Optical Flow It is a fundamental task in video anal-
ysis. The topic has been studied for decades and domi-
nated by variational approaches [17, 2], which mainly ad-
dress small displacements [44]. The recent focus is on large
displacements [3], and combinatorial matching (e.g., Deep-
Flow [45], EpicFlow [36]) has been integrated into the vari-
ational approach. These approaches are all hand-crafted.

Deep learning and semantic information have been ex-
ploited for optical ﬂow recently. FlowNet [9] ﬁrstly applies
deep CNNs to directly estimate the motion and achieves
good result. The network architecture is simpliﬁed in the re-
cent Pyramid Network [33]. Other works attempt to exploit
semantic segmentation information to help optical ﬂow es-
timation [38, 1, 19], e.g., providing speciﬁc constraints on
the ﬂow according to the category of the regions.

Optical ﬂow information has been exploited to help vi-
sion tasks, such as pose estimation [32], frame predic-
tion [31], and attribute transfer [49]. This work exploits
optical ﬂow to speed up general video recognition tasks.

Exploiting Temporal Information in Video Recogni-
tion T-CNN [22] incorporates temporal and contextual in-
formation from tubelets in videos. The dense 3D CRF [24]
proposes long-range spatial-temporal regularization in se-
STFCN [10] considers a
mantic video segmentation.

spatial-temporal FCN for semantic video segmentation.
These works operate on volume data, show improved recog-
nition accuracy but greatly increase the computational cost.
By contrast, our approach seeks to reduce the computation
by exploiting temporal coherence in the videos. The net-
work still runs on single frames and is fast.

Slow Feature Analysis High level semantic concepts
usually evolve slower than the low level image appear-
ance in videos. The deep features are thus expected to
vary smoothly on consecutive video frames. This obser-
vation has been used to regularize the feature learning in
videos [46, 21, 53, 51, 41]. We conjecture that our approach
may also beneﬁt from this fact.

Clockwork Convnets [39] It is the most related work
to ours as it also disables certain layers in the network on
certain video frames and reuses the previous features. It is,
however, much simpler and less effective than our approach.
About speed up, Clockwork only saves the computation
of some layers (e.g., 1/3 or 2/3) in some frames (e.g., every
other frame). As seen later, our method saves that on most
layers (task network has only 1 layer) in most frames (e.g.,
9 out of 10 frames). Thus, our speedup ratio is much higher.
About accuracy, Clockwork does not exploit the corre-
spondence between frames and simply copies features. It
only reschedules the computation of inference in an off-
the-shelf network and does not perform ﬁne-tuning or re-
training. Its accuracy drop is pretty noticeable at even small
speed up. In Table 4 and 6 of their arxiv paper, at 77% full
runtime (thus 1.3 times faster), Mean IU drops from 31.1 to
26.4 on NYUD, from 70.0 to 64.0 on Youtube, from 65.9
to 63.3 on Pascal, and from 65.9 to 64.4 on Cityscapes. By
contrast, we re-train a two-frame network with motion con-
sidered end-to-end. The accuracy drop is small, e.g., from
71.1 to 70.0 on Cityscape while being 3 times faster (Fig-
ure 3, bottom).

About generality, Clockwork is only applicable for se-
mantic segmentation with FCN. Our approach transfers
general image recognition networks to the video domain.

3. Deep Feature Flow

Table 1 summarizes the notations used in this paper. Our

approach is brieﬂy illustrated in Figure 2.

Deep Feature Flow Inference Given an image recogni-
tion task and a feed-forward convolutional neutral network
N that outputs result for input image I as y = N (I). Our
goal is to apply the network to all video frames Ii, i =
0, ..., ∞, fast and accurately.

Following the modern CNN architectures [40, 42, 16]
and applications [28, 4, 52, 13, 14, 12, 35, 8], without loss
of generality, we decompose N into two consecutive sub-
networks. The ﬁrst sub-network Nf eat, dubbed feature net-
work, is fully convolutional and outputs a number of in-
termediate feature maps, f = Nf eat(I). The second sub-

k
i
r
l
s
Ii, Ik
yi, yk
fk
fi
Mi→k
p, q
Si→k
N
Nf eat
Ntask
F
W

key frame index
current frame index
per-frame computation cost ratio, Eq. (5)
key frame duration length
overall speedup ratio, Eq. (7)
video frames
recognition results
convolutional feature maps on key frame
propagated feature maps on current frame
2D ﬂow ﬁeld
2D location
scale ﬁeld
image recognition network
sub-network for feature extraction
sub-network for recognition result
ﬂow estimation function
feature propagation function, Eq. (3)

Table 1. Notations.

network Ntask, dubbed task network, has speciﬁc structures
for the task and performs the recognition task over the fea-
ture maps, y = Ntask(f ).

Consecutive video frames are highly similar. The simi-
larity is even stronger in the deep feature maps, which en-
code high level semantic concepts [46, 21]. We exploit the
similarity to reduce computational cost. Speciﬁcally, the
feature network Nf eat only runs on sparse key frames. The
feature maps of a non-key frame Ii are propagated from its
preceding key frame Ik.

The features in the deep convolutional layers encode the
semantic concepts and correspond to spatial locations in the
image [48]. Examples are illustrated in Figure 1. Such spa-
tial correspondence allows us to cheaply propagate the fea-
ture maps by the manner of spatial warping.

Let Mi→k be a two dimensional ﬂow ﬁeld.

It is ob-
tained by a ﬂow estimation algorithm F such as [26, 9],
Mi→k = F(Ik, Ii).
It is bi-linearly resized to the same
It
spatial resolution of the feature maps for propagation.
projects back a location p in current frame i to the location
p + δp in key frame k, where δp = Mi→k(p).

As the values δp are in general fractional, the feature

warping is implemented via bilinear interpolation

f c
i (p) =

G(q, p + δp)f c

k(q),

(1)

(cid:88)

q

where c identiﬁes a channel in the feature maps f , q enu-
merates all spatial locations in the feature maps, and G(·, ·)
denotes the bilinear interpolation kernel. Note that G is two

Algorithm 1 Deep feature ﬂow inference algorithm for
video recognition.

1: input: video frames {Ii}
2: k = 0;
3: f0 = Nf eat(I0)
4: y0 = Ntask(f0)
5: for i = 1 to ∞ do
6:
7:
8:

k = i
fk = Nf eat(Ik)
yk = Ntask(fk)

if is key f rame(i) then

else

fi = W(fk, F(Ik, Ii), S(Ik, Ii))
yi = Ntask(fi)

9:
10:
11:
12:
end if
13:
14: end for
15: output: recognition results {yi}

(cid:46) initialize key frame

(cid:46) key frame scheduler
(cid:46) update the key frame

(cid:46) use feature ﬂow
(cid:46) propagation

use a CNN to estimate the ﬂow ﬁeld and the scale ﬁeld such
that all the components can be jointly trained end-to-end for
the task.

The architecture is illustrated in Figure 2(b). Train-
ing is performed by stochastic gradient descent (SGD). In
each mini-batch, a pair of nearby video frames, {Ik, Ii}1,
0 ≤ i − k ≤ 9, are randomly sampled. In the forward pass,
feature network Nf eat is applied on Ik to obtain the feature
maps fk. Next, a ﬂow network F runs on the frames Ii, Ik to
estimate the ﬂow ﬁeld and the scale ﬁeld. When i > k, fea-
ture maps fk are propagated to fi as in Eq. (3). Otherwise,
the feature maps are identical and no propagation is done.
Finally, task network Ntask is applied on fi to produce the
result yi, which incurs a loss against the ground truth result.
The loss error gradients are back-propagated throughout to
update all the components. Note that our training accom-
modates the special case when i = k and degenerates to the
per-frame training as in Figure 2(a).

The ﬂow network is much faster than the feature net-
It is pre-trained on the
work, as will be elaborated later.
Flying Chairs dataset [9]. We then add the scale function
S as a sibling output at the end of the network, by increas-
ing the number of channels in the last convolutional layer
appropriately. The scale function is initialized to all ones
(weights and biases in the output layer are initialized as 0s
and 1s, respectively). The augmented ﬂow network is then
ﬁne-tuned as in Figure 2(b).

The feature propagation function in Eq.(3) is unconven-
tional. It is parameter free and fully differentiable. In back-
propagation, we compute the derivative of the features in fi
with respect to the features in fk, the scale ﬁeld Si→k, and
the ﬂow ﬁeld Mi→k. The ﬁrst two are easy to compute us-

1The same notations are used for consistency although there is no

longer the concept of “key frame” during training.

Figure 2. Illustration of video recognition using per-frame network
evaluation (a) and the proposed deep feature ﬂow (b).

dimensional and is separated into two one dimensional ker-
nels as

G(q, p + δp) = g(qx, px + δpx) · g(qy, py + δpy),

(2)

where g(a, b) = max(0, 1 − |a − b|).

We note that Eq. (1) is fast to compute as a few terms are

non-zero.

The spatial warping may be inaccurate due to errors in
ﬂow estimation, object occlusion, etc. To better approx-
imate the features, their amplitudes are modulated by a
“scale ﬁeld” Si→k, which is of the same spatial and chan-
nel dimensions as the feature maps. The scale ﬁeld is ob-
tained by applying a “scale function” S on the two frames,
Si→k = S(Ik, Ii).

Finally, the feature propagation function is deﬁned as

fi = W(fk, Mi→k, Si→k),

(3)

where W applies Eq.(1) for all locations and all channels
in the feature maps, and multiples the features with scales
Si→k in an element-wise way.

The proposed video recognition algorithm is called deep
feature ﬂow. It is summarized in Algorithm 1. Notice that
any ﬂow function F, such as the hand-crafted low-level ﬂow
(e.g., SIFT-Flow [26]), is readily applicable. Training the
ﬂow function is not obligate, and the scale function S is set
to ones everywhere.

Deep Feature Flow Training A ﬂow function is origi-
nally designed to obtain correspondence of low-level image
pixels.
It can be fast in inference, but may not be accu-
rate enough for the recognition task, in which the high-level
feature maps change differently, usually slower than pix-
els [21, 39]. To model such variations, we propose to also

ing the chain rule. For the last, from Eq. (1) and (3), for
each channel c and location p in current frame, we have

∂f c

i (p)
∂Mi→k(p)

= Sc

i→k(p)

(cid:88)

q

∂G(q, p + δp)
∂δp

f c
k(q).

(4)

The term ∂G(q,p+δp)
can be derived from Eq. (2). Note that
the ﬂow ﬁeld M(·) is two-dimensional and we use ∂δp to
denote ∂δpx and ∂δpy for simplicity.

∂δp

The proposed method can easily be trained on datasets
where only sparse frames are annotated, which is usually
the case due to the high labeling costs in video recogni-
tion tasks [29, 11, 6]. In this case, the per-frame training
(Figure 2(a)) can only use annotated frames, while DFF can
easily use all frames as long as frame Ii is annotated. In
other words, DFF can fully use the data even with sparse
ground truth annotation. This is potentially beneﬁcial for
many video recognition tasks.

Inference Complexity Analysis For each non-key
frame, the computational cost ratio of the proposed ap-
proach (line 11-12 in Algorithm 1) and per-frame approach
(line 8-9) is

FlowNet

FlowNet Half

FlowNet Inception

ResNet-50

ResNet-101

9.20
12.71

33.56
46.30

68.97
95.24

Table 2. The approximated complexity ratio in Eq. (6) for different
feature network Nf eat and ﬂow network F, measured by their
FLOPs. See Section 4. Note that r (cid:28) 1 and we use 1
r here for
clarify. A signiﬁcant per-frame speedup factor is obtained.

when to allocate a new key frame. In this work, we use a
simple ﬁxed key frame scheduling, that is, the key frame
duration length l is a ﬁxed constant. It is easy to implement
and tune. However, varied changes in image content may
require a varying l to provide a smooth tradeoff between
accuracy and speed.
Ideally, a new key frame should be
allocated when the image content changes drastically.

How to design effective and adaptive key frame schedul-
ing can further improve our work. Currently it is beyond
the scope of this work. Different video tasks may present
different behaviors and requirements. Learning an adaptive
key frame scheduler from data seems an attractive choice.
This is worth further exploration and left as future work.

r =

O(F) + O(S) + O(W) + O(Ntask)
O(Nf eat) + O(Ntask)

,

(5)

4. Network Architectures

where O(·) measures the function complexity.

To understand this ratio, we ﬁrstly note that the com-
plexity of Ntask is usually small. Although its split point
in N is kind of arbitrary, as veriﬁed in experiment, it is
sufﬁcient to keep only one learnable weight layer in Ntask
in our implementation (see Sec. 4). While both Nf eat
and F have considerable complexity (Section 4), we have
O(Ntask) (cid:28) O(Nf eat) and O(Ntask) (cid:28) O(F).

We also have O(W) (cid:28) O(F) and O(S) (cid:28) O(F) be-
cause W and S are very simple. Thus, the ratio in Eq. (5) is
approximated as

r ≈

O(F)
O(Nf eat)

.

It is mostly determined by the complexity ratio of ﬂow
network F and feature network Nf eat, which can be pre-
cisely measured, e.g., by their FLOPs. Table 2 shows its
typical values in our implementation.

Compared to the per-frame approach,

the overall
speedup factor in Algorithm 1 also depends on the spar-
sity of key frames. Let there be one key frame in every l
consecutive frames, the speedup factor is

s =

l
1 + (l − 1) ∗ r

.

Key Frame Scheduling As indicated in Algorithm 1
(line 6) and Eq. (7), a crucial factor for inference speed is

(6)

(7)

The proposed approach is general for different networks
and recognition tasks. Towards a solid evaluation, we adopt
the state-of-the-art architectures and important vision tasks.
Flow Network We adopt the state-of-the-art CNN based
FlowNet architecture (the “Simple” version) [9] as default.
We also designed two variants of lower complexity. The
ﬁrst one, dubbed FlowNet Half, reduces the number of con-
volutional kernels in each layer of FlowNet by half and the
complexity to 1
4 . The second one, dubbed FlowNet Incep-
tion, adopts the Inception structure [43] and reduces the
complexity to 1
8 of that of FlowNet. The architecture de-
tails are reported in Appendix A.

The three ﬂow networks are pre-trained on the synthetic
Flying Chairs dataset in [9]. The output stride is 4. The in-
put image is half-sized. The resolution of ﬂow ﬁeld is there-
fore 1
8 of the original resolution. As the feature stride of the
feature network is 16 (as described below), the ﬂow ﬁeld
and the scale ﬁeld is further down-sized by half using bi-
linear interpolation to match the resolution of feature maps.
This bilinear interpolation is realized as a parameter-free
layer in the network and also differentiated during training.
Feature Network We use ResNet models [16], speciﬁ-
cally, the ResNet-50 and ResNet-101 models pre-trained for
ImageNet classiﬁcation as default. The last 1000-way clas-
siﬁcation layer is discarded. The feature stride is reduced
from 32 to 16 to produce denser feature maps, following the
practice of DeepLab [4, 5] for semantic segmentation, and
R-FCN [8] for object detection. The ﬁrst block of the conv5

layers are modiﬁed to have a stride of 1 instead of 2. The
holing algorithm [4] is applied on all the 3×3 convolutional
kernels in conv5 to keep the ﬁeld of view (dilation=2). A
randomly initialized 3×3 convolution is appended to conv5
to reduce the feature channel dimension to 1024, where the
holing algorithm is also applied (dilation=6). The resulting
1024-dimensional feature maps are the intermediate feature
maps for the subsequent task.

Table 2 presents the complexity ratio Eq. (6) of feature

networks and ﬂow networks.

Semantic Segmentation A randomly initialized 1 × 1
convolutional layer is applied on the intermediate feature
maps to produce (C +1) score maps, where C is the number
of categories and 1 is for background category. A following
softmax layer outputs the per-pixel probabilities. Thus, the
task network only has one learnable weight layer. The over-
all network architecture is similar to DeepLab with large
ﬁeld-of-view in [5].

Object Detection We adopt

the state-of-the-art R-
FCN [8]. On the intermediate feature maps, two branches of
fully convolutional networks are applied on the ﬁrst half and
the second half 512-dimensional of the intermediate feature
maps separately, for sub-tasks of region proposal and detec-
tion, respectively.

In the region proposal branch, the RPN network [35]
is applied. We use na = 9 anchors (3 scales and 3 as-
pect ratios). Two sibling 1 × 1 convolutional layers out-
put the 2na-dimensional objectness scores and the 4na-
dimensional bounding box (bbox) regression values, re-
spectively. Non-maximum suppression (NMS) is applied to
generate 300 region proposals for each image. Intersection-
over-union (IoU) threshold 0.7 is used.

In the detection branch,

two sibling 1 × 1 convolu-
tional layers output the position-sensitive score maps and
bbox regression maps, respectively. They are of dimensions
(C + 1)k2 and 4k2, respectively, where k banks of classi-
ﬁers/regressors are employed to encode the relative position
information. See [8] for details. On the position-sensitive
score/bbox regression maps, position-sensitive ROI pool-
ing is used to obtain the per-region classiﬁcation score and
bbox regression result. No free parameters are involved in
the per-region computation. Finally, NMS is applied on the
scored and regressed region proposals to produce the detec-
tion result, with IoU threshold 0.3.

5. Experiments

Unlike image datasets, large scale video dataset is much
harder to collect and annotate. Our approach is evaluated
on the two recent datasets: Cityscapes [6] for semantic seg-
mentation, and ImageNet VID [37] for object detection.

5.1. Experiment Setup

Cityscapes It is for urban scene understanding and au-
tonomous driving. It contains snippets of street scenes col-
lected from 50 different cities, at a frame rate of 17 fps. The
train, validation, and test sets contain 2975, 500, and 1525
snippets, respectively. Each snippet has 30 frames, where
the 20th frame is annotated with pixel-level ground-truth la-
bels for semantic segmentation. There are 30 semantic cat-
egories. Following the protocol in [5], training is performed
on the train set and evaluation is performed on the validation
set. The semantic segmentation accuracy is measured by the
pixel-level mean intersection-over-union (mIoU) score.

In both training and inference, the images are resized to
have shorter sides of 1024 and 512 pixels for the feature net-
work and the ﬂow network, respectively. In SGD training,
20K iterations are performed on 8 GPUs (each GPU holds
one mini-batch, thus the effective batch size ×8), where the
learning rates are 10−3 and 10−4 for the ﬁrst 15K and the
last 5K iterations, respectively.

ImageNet VID It is for object detection in videos. The
training, validation, and test sets contain 3862, 555, and 937
fully-annotated video snippets, respectively. The frame rate
is 25 or 30 fps for most snippets. There are 30 object cate-
gories, which are a subset of the categories in the ImageNet
DET image dataset2. Following the protocols in [22, 25],
evaluation is performed on the validation set, using the stan-
dard mean average precision (mAP) metric.

In both training and inference, the images are resized to
have shorter sides of 600 pixels and 300 pixels for the fea-
ture network and the ﬂow network, respectively. In SGD
training, 60K iterations are performed on 8 GPUs, where
the learning rates are 10−3 and 10−4 for the ﬁrst 40K and
the last 20K iterations, respectively.

During training, besides the ImageNet VID train set, we
also used the ImageNet DET train set (only the same 30
category labels are used), following the protocols in [22,
25]. Each mini-batch samples images from either ImageNet
VID or ImageNet DET datasets, at 2 : 1 ratio.

5.2. Evaluation Methodology and Results

Deep feature ﬂow is ﬂexible and allows various design
choices. We evaluate their effects comprehensively in the
experiment. For clarify, we ﬁx their default values through-
out the experiments, unless speciﬁed otherwise. For feature
network Nf eat, ResNet-101 model is default. For ﬂow net-
work F, FlowNet (section 4) is default. Key-frame duration
length l is 5 for Cityscapes [6] segmentation and 10 for Im-
ageNet VID [37] detection by default, based on different
frame rate of videos in the datasets..

For each snippet we evaluate l image pairs, (k, i), k =
i − l + 1, ..., i, for each frame i with ground truth anno-

2http://www.image-net.org/challenges/LSVRC/

method

training of image recognition network N training of ﬂow network F

trained on single frames as in Fig. 2 (a)

no ﬂow network used

Frame (oracle baseline)
SFF-slow
SFF-fast
DFF
DFF ﬁx N
DFF ﬁx F
DFF separate

same as Frame

same as Frame

same as DFF

same as Frame

trained on frame pairs as in Fig. 2 (b)

init. on Flying Chairs [9], ﬁne-tuned in Fig. 2 (b)

same as Frame, then ﬁxed in Fig. 2 (b)

same as DFF

SIFT-Flow [26] (w/ best parameters), no training

SIFT-Flow [26] (w/ default parameters), no training

init. on Flying Chairs [9], then ﬁxed in Fig. 2 (b)

init. on Flying Chairs [9]

Table 3. Description of variants of deep feature ﬂow (DFF), shallow feature ﬂow (SFF), and the per-frame approach (Frame).

Cityscapes (l = 5)

ImageNet VID (l = 10)

mIoU(%)

runtime (fps) mAP(%)

runtime (fps)

Methods

Frame
SFF-slow
SFF-fast
DFF
DFF ﬁx N
DFF ﬁx F
DFF separate

71.1
67.8
67.3
69.2
68.8
67.0
66.9

1.52
0.08
0.95
5.60
5.60
5.60
5.60

73.9
70.7
69.7
73.1
72.3
68.8
67.4

4.05
0.26
3.04
20.25
20.25
20.25
20.25

Table 4. Comparison of accuracy and runtime (mostly in GPU) of
various approaches in Table 3. Note that, the runtime for SFF con-
sists of CPU runtime of SIFT-Flow and GPU runtime of Frame,
since SIFT-Flow only has CPU implementation.

tation. Time evaluation is on a workstation with NVIDIA
K40 GPU and Intel Core i7-4790 CPU.

Validation of DFF Architecture We compared DFF

with several baselines and variants, as listed in Table 3.

• Frame: train N on single frames with ground truth.

• SFF: use pre-computed large-displacement ﬂow (e.g.,
SIFT-Flow [26]). SFF-fast and SFF-slow adopt differ-
ent parameters.

• DFF: the proposed approach, N and F are trained
end-to-end. Several variants include DFF ﬁx N (ﬁx
N in training), DFF ﬁx F (ﬁx F in training), and DFF
seperate (N and F are separately trained).

Table 4 summarizes the accuracy and runtime of all ap-
proaches. We ﬁrstly note that the baseline Frame is strong
enough to serve as a reference for comparison. Our im-
plementation resembles the state-of-the-art DeepLab [5] for
semantic segmentation and R-FCN [8] for object detection.
In DeepLab [5], an mIoU score of 69.2% is reported with
DeepLab large ﬁeld-of-view model using ResNet-101 on
Cityscapes validation dataset. Our Frame baseline achieves
slightly higher 71.1%, based on the same ResNet model.

For object detection, Frame baseline has mAP 73.9% us-
ing R-FCN [8] and ResNet-101. As a reference, a com-
parable mAP score of 73.8% is reported in [22], by com-
bining CRAFT [47] and DeepID-net [30] object detectors
trained on the ImageNet data, using both VGG-16 [40] and
GoogleNet-v2 [20] models, with various tricks (multi-scale
training/testing, adding context information, model ensem-
ble). We do not adopt above tricks as they complicate the
comparison and obscure the conclusions.

SFF-fast has a reasonable runtime but accuracy is sig-
niﬁcantly decreased. SFF-slow uses the best parameters for
ﬂow estimation. It is much slower. Its accuracy is slightly
improved but still poor. This indicates that an off-the-shelf
ﬂow may be insufﬁcient.

The proposed DFF approach has the best overall perfor-
mance. Its accuracy is slightly lower than that of Frame and
it is 3.7 and 5.0 times faster for segmentation and detection,
respectively. As expected, the three variants without using
joint training have worse accuracy. Especially, the accuracy
drop by ﬁxing F is signiﬁcant. This indicates a jointing
end-to-end training (especially ﬂow) is crucial.

We also tested another variant of DFF with the scale
function S removed (Algorithm 1, Eq (3), Eq. (4)). The
accuracy drops for both segmentation and detection (less
than one percent). It shows that the scaled modulation of
features is slightly helpful.

Accuracy-Speedup Tradeoff We investigate the trade-
off by varying the ﬂow network F, the feature network
Nf eat, and key frame duration length l. Since Cityscapes
and ImageNet VID datasets have different frame rates, we
tested l = 1, 2, ..., 10 for segmentation and l = 1, 2, ..., 20
for detection.

The results are summarized in Figure 3. Overall, DFF
achieves signiﬁcant speedup with decent accuracy drop. It
smoothly trades in accuracy for speed and ﬁts different
application needs ﬂexibly. For example, in detection, it
improves 4.05 fps of ResNet-101 Frame to 41.26 fps of
ResNet-101 + FlowNet Inception. The 10× faster speed is
at the cost of moderate accuracy drop from 73.9% to 69.5%.
In segmentation, it improves 2.24 fps of ResNet-50 Frame

# layers in Ntask

Cityscapes (l=5)

ImageNet VID (l=10)

mIoU(%)

runtime (fps) mAP(%)

runtime (fps)

21
12
5
1 (default)
0

69.1
69.1
69.2
69.2
69.5

2.87
3.14
3.89
5.60
5.61

73.2
73.3
73.2
73.1
72.7

7.23
8.04
9.99
20.25
20.40

Table 5. Results of using different split points for Ntask.

to 17.48 fps of ResNet-50 FlowNet Inception, at the cost of
accuracy drop from 69.7% to 62.4%.

What ﬂow F should we use? From Figure 3, the smallest
FlowNet Inception is advantageous. It is faster than its two
counterparts at the same accuracy level, most of the times.
What feature Nf eat should we use? In high-accuracy
zone, an accurate model ResNet-101 is clearly better than
ResNet-50. In high-speed zone, the conclusions are differ-
ent on the two tasks. For detection, ResNet-101 is still ad-
vantageous. For segmentation, the performance curves in-
tersect at around 6.35 fps point. For higher speed, ResNet-
50 becomes better than ResNet-101. The seemingly dif-
ferent conclusions can be partially attributed to the differ-
ent video frame rates, the extents of dynamics on the two
datasets. The Cityscapes dataset not only has a low frame
rate 17 fps, but also more quick dynamics. It would be hard
to utilize temporal redundancy for a long propagation. To
achieve the same high speed, ResNet-101 needs a larger key
frame length l than ResNet-50. This in turn signiﬁcantly
increases the difﬁculty of learning.

Above observations provide useful recommendations for
practical applications. Yet, they are more heuristic than gen-
eral, as they are observed only on the two tasks, on limited
data. We plan to explore the design space more in the future.
Split point of Ntask Where should we split Ntask in N ?
Recall that the default Ntask keeps one layer with learning
weight (the 1 × 1 conv over 1024-d feature maps, see Sec-
tion 4). Before this is the 3 × 3 conv layer that reduces di-
mension to 1024. Before this is series of “Bottleneck” unit
in ResNet [16], each consisting of 3 layers. We back move
the split point to make different Ntasks with 5, 12, and 21
layers, respectively. The one with 5 layers adds the dimen-
sion reduction layer and one bottleneck unit (conv5c). The
one with 12 layers adds two more units (conv5a and conv5b)
at the beginning of conv5. The one with 21 layers adds three
more units in conv4. We also move the only layer in default
Ntask into Nf eat, leaving Ntask with 0 layer (with learn-
able weights). This is equivalent to directly propagate the
parameter-free score maps, in both semantic segmentation
and object detection.

Table 5 summarizes the results. Overall, the accuracy
variation is small enough to be neglected. The speed be-

Figure 3. (better viewed in color) Illustration of accuracy-speed
tradeoff under different implementation choices on ImageNet VID
detection (top) and on Cityscapes segmentation (bottom).

comes lower when Ntask has more layers. Using 0 layer
is mostly equivalent to using 1 layer, in both accuracy and
speed. We choose 1 layer as default as that leaves some tun-
able parameters after the feature propagation, which could
be more general.

Example results of the proposed method are presented
in Figure 4 and Figure 5 for video segmentation on
CityScapes and video detection on ImageNet VID, respec-
tively. More example results are available at https:
//www.youtube.com/watch?v=J0rMHE6ehGw.

6. Future Work

Several important aspects are left for further exploration.
It would be interesting to exploit how the joint learning af-
fects the ﬂow quality. We are unable to evaluate as there
lacks ground truth. Current optical ﬂow works are also lim-
ited to either synthetic data [9] or small real datasets, which
is insufﬁcient for deep learning.

Our method can further beneﬁt from improvements in
ﬂow estimation and key frame scheduling. In this paper, we
adopt FlowNet [9] mainly because there are few choices.
Designing faster and more accurate ﬂow network will cer-

Figure 4. Semantic segmentation results on Cityscapes validation dataset. The ﬁrst column corresponds to the images and the results on
the key frame (the kth frame). The following four columns correspond to the k + 1st, k + 2nd, k + 3rd and k + 4th frames, respectively.

Figure 5. Object detection results on ImageNet VID validation dataset. The ﬁrst column corresponds to the images and the results on the
key frame (the kth frame). The following four columns correspond to the k + 2nd, k + 4th, k + 6th and k + 8th frames, respectively.

layer

type

stride

# output

conv1
conv2
conv3
conv3 1
conv4
conv4 1
conv5
conv5 1
conv6
conv6 1

conv1
conv2
conv3
conv3 1
conv4
conv4 1
conv5
conv5 1
conv6
conv6 1

7x7 conv
5x5 conv
5x5 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv

7x7 conv
5x5 conv
5x5 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv

2
2
2

2

2

2

2
2
2

2

2

2

64
128
256
256
512
512
512
512
1024
1024

32
64
128
128
256
256
256
256
512
512

Table 6. The FlowNet network architecture.

layer

type

stride

# output

Table 7. The FlowNet Half network architecture.

tainly receive more attention in the future. For key frame
scheduling, a good scheduler may well signiﬁcantly im-
prove both speed and accuracy. And this problem is deﬁ-
nitely worth further exploration.

We believe this work opens many new possibilities. We

hope it will inspire more future work.

A. FlowNet Inception Architecture

The architectures of FlowNet, FlowNet Half follow that
of [9] (the “Simple” version), which are detailed in Table 6
and Table 7, respectively. The architecture of FlowNet In-
ception follows the design of the Inception structure [43],
which is detailed in Table 8.

References

[1] M. Bai, W. Luo, K. Kundu, and R. Urtasun. Exploiting se-
mantic information and deep matching for optical ﬂow. In

ECCV, 2016. 2

[2] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical ﬂow estimation based on a theory for warping.
In ECCV, 2004. 2

[3] T. Brox and J. Malik. Large displacement optical ﬂow: De-
scriptor matching in variational motion estimation. TPAMI,
2011. 2

[4] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-
volutional nets and fully connected crfs. In ICLR, 2015. 1,
2, 3, 5, 6

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. arXiv preprint, 2016. 5, 6, 7

[6] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In CVPR, 2016. 1, 5, 6

[7] M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect:
Training deep neural networks with binary weights during
propagations. In NIPS, 2015. 2

[8] J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via
region-based fully convolutional networks. In NIPS, 2016.
1, 2, 3, 5, 6, 7

[9] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
and V. Golkov. Flownet: Learning optical ﬂow with convo-
lutional networks. In ICCV, 2015. 2, 3, 4, 5, 7, 8, 11
[10] M. Fayyaz, M. H. Saffar, M. Sabokrou, M. Fathy, and
R. Klette. STFCN: spatio-temporal FCN for semantic video
segmentation. arXiv preprint, 2016. 2

[11] F. Galasso, N. Shankar Nagaraja, T. Jimenez Cardenas,
T. Brox, and B. Schiele. A uniﬁed video segmentation bench-
mark: Annotation, metrics and analysis. In ICCV, 2013. 5

[12] R. Girshick. Fast R-CNN. In ICCV, 2015. 1, 2, 3
[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014. 1, 2, 3

[14] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling
in deep convolutional networks for visual recognition.
In
ECCV, 2014. 1, 2, 3

[15] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In ICCV, 2015. 2

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 2, 3, 5, 8

[17] B. K. Horn and B. G. Schunck. Determining optical ﬂow.

Artiﬁcial intelligence, 1981. 1, 2

[18] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Y. Bengio. Quantized Neural Networks: Training Neu-
ral Networks with Low Precision Weights and Activations.
arXiv preprint, 2016. 2

[19] J. Hur and S. Roth. Joint optical ﬂow and temporally consis-
tent semantic segmentation. In ECCV CVRSUAD Workshop,
2016. 2

[20] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 2, 7

type

stride

# output

#1x1

#1x1-#3x3

#1x1-#3x3-#3x3

#pool

Inception/Reduction

layer

conv1
pool1
conv2
conv3 1
conv3 2
conv3 3
conv4 1
conv4 2
conv5 1
conv5 2
conv6 1
conv6 2

7x7 conv
3x3 max pool
Inception
3x3 conv
Inception
Inception
Reduction
Inception
Reduction
Inception
Reduction
Inception

2
2

2

2

2

2

32
32
64
128
128
128
256
256
384
384
512
512

24-32

24-32-32

48
48
32
96
48
144
64
192

32-64
48-64
112-128
112-128
96-192
96-192
192-256
192-256

8-16-16
12-16-16
28-32-32
28-32-32
36-48-48
36-48-48
48-64-64
48-64-64

64

96

128

Table 8. The FlowNet Inception network architecture, following the design of the Inception structure [43]. ”Inception/Reduction” modules
consist of four branches: 1x1 conv (#1x1), 1x1 conv-3x3 conv (#1x1-#3x3), 1x1 conv-3x3 conv-3x3 conv (#1x1-#3x3-#3x3), and 3x3 max
pooling followed by 1x1 conv (#pool, only for stride=2).

[21] D. Jayaraman and K. Grauman. Slow and steady feature
analysis: higher order temporal coherence in video.
In
CVPR, 2016. 1, 3, 4

[22] K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang,
Z. Wang, R. Wang, and X. Wang. T-cnn: Tubelets with con-
volutional neural networks for object detection from videos.
In CVPR, 2016. 2, 6, 7

[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1, 2

Imagenet
In

[24] A. Kundu, V. Vineet, and V. Koltun. Feature space optimiza-
tion for semantic video segmentation. In CVPR, 2016. 2
[25] B. Lee, E. Erdenee, S. Jin, and P. K. Rhee. Multi-class
multi-object tracking using changing point detection. arXiv
preprint, 2016. 6

[26] C. Liu, J. Yuen, A. Torralba, J. Sivic, and W. T. Freeman.
Sift ﬂow: dense correspondence across difference scenes. In
ECCV, 2008. 3, 4, 7

[27] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed.

Ssd: Single shot multibox detector. 2016. 1

[28] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, 2015. 1, 2, 3
[29] P. K. Nathan Silberman, Derek Hoiem and R. Fergus. Indoor
segmentation and support inference from rgbd images.
In
ECCV, 2012. 5

[30] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y. Tian, H. Li,
S. Yang, Z. Wang, and C.-C. Loy. Deepid-net: Deformable
deep convolutional neural networks for object detection. In
CVPR, 2015. 7

[31] V. Patraucean, A. Handa, and R. Cipolla. Spatio-temporal
arXiv

video autoencoder with differentiable memory.
preprint arXiv:1511.06309, 2015. 2

[32] T. Pﬁster, J. Charles, and A. Zisserman. Flowing convnets
for human pose estimation in videos. In ICCV, 2015. 2

[33] A. Ranjan and M. J. Black. Optical ﬂow estimation using a

spatial pyramid network. arXiv preprint, 2016. 2

[34] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. arXiv preprint, 2016. 2

[35] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-
wards real-time object detection with region proposal net-
works. In NIPS, 2015. 1, 2, 3, 6

[36] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
EpicFlow: Edge-Preserving Interpolation of Correspon-
dences for Optical Flow. In CVPR, 2015. 2

[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. IJCV, 2015. 1, 6

[38] L. Sevilla-Lara, D. Sun, V. Jampani, and M. J. Black. Optical
In
ﬂow with semantic segmentation and localized layers.
CVPR, 2016. 2

[39] E. Shelhamer, K. Rakelly, J. Hoffman, and T. Darrell. Clock-
work convnets for video semantic segmentation. In ECCV,
2016. 3, 4

[40] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
1, 2, 3, 7

[41] L. Sun, K. Jia, T.-H. Chan, Y. Fang, G. Wang, and S. Yan. Dl-
sfa: deeply-learned slow feature analysis for action recogni-
tion. In CVPR, 2014. 3

[42] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 1, 2, 3
[43] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR, 2016. 5, 11, 12

[44] J. Weickert, A. Bruhn, T. Brox, and N. Papenberg. A survey
on variational optic ﬂow methods for small displacements.
In Mathematical models for registration and applications to
medical imaging. 2006. 2

[45] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid.
DeepFlow: Large displacement optical ﬂow with deep
matching. In CVPR, 2013. 2

[46] L. Wiskott and T. J. Sejnowski. Slow feature analysis: Unsu-
pervised learning of invariances. Neural computation, 2002.
1, 3

[47] B. Yang, J. Yan, Z. Lei, and S. Z. Li. Craft objects from

images. In CVPR, 2016. 7

[48] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014. 1, 3

[49] W. Zhang, P. Srinivasan, and J. Shi. Discriminative image

warping with attribute ﬂow. In CVPR, 2011. 2

[50] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very
deep convolutional networks for classiﬁcation and detection.
TPAMI, 2015. 2

[51] Z. Zhang and D. Tao. Slow feature analysis for human action

recognition. TPAMI, 2012. 3

[52] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. Torr. Conditional random
ﬁelds as recurrent neural networks. In ICCV, 2015. 1, 2, 3

[53] W. Zou, S. Zhu, K. Yu, and A. Y. Ng. Deep learning of
invariant features via simulated ﬁxations in video. In NIPS,
2012. 1, 3

Deep Feature Flow for Video Recognition

Xizhou Zhu1,2∗

Yuwen Xiong2∗

Jifeng Dai2

Lu Yuan2

Yichen Wei2

1University of Science and Technology of China
ezra0408@mail.ustc.edu.cn

2Microsoft Research
{v-yuxio,jifdai,luyuan,yichenw}@microsoft.com

7
1
0
2
 
n
u
J
 
5
 
 
]

V
C
.
s
c
[
 
 
2
v
5
1
7
7
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

Deep convolutional neutral networks have achieved
great success on image recognition tasks. Yet, it is non-
trivial to transfer the state-of-the-art image recognition net-
works to videos as per-frame evaluation is too slow and un-
affordable. We present deep feature ﬂow, a fast and accu-
rate framework for video recognition.
It runs the expen-
sive convolutional sub-network only on sparse key frames
and propagates their deep feature maps to other frames via
a ﬂow ﬁeld. It achieves signiﬁcant speedup as ﬂow com-
putation is relatively fast. The end-to-end training of the
whole architecture signiﬁcantly boosts the recognition ac-
curacy. Deep feature ﬂow is ﬂexible and general. It is vali-
dated on two video datasets on object detection and seman-
tic segmentation. It signiﬁcantly advances the practice of
video recognition tasks. Code is released at https://
github.com/msracver/Deep-Feature-Flow.

1. Introduction

Recent years have witnessed signiﬁcant success of deep
convolutional neutral networks (CNNs) for image recogni-
tion, e.g., image classiﬁcation [23, 40, 42, 16], semantic
segmentation [28, 4, 52], and object detection [13, 14, 12,
35, 8, 27]. With their success, the recognition tasks have
been extended from image domain to video domain, such
as semantic segmentation on Cityscapes dataset [6], and ob-
ject detection on ImageNet VID dataset [37]. Fast and ac-
curate video recognition is crucial for high-value scenarios,
e.g., autonomous driving and video surveillance. Neverthe-
less, applying existing image recognition networks on indi-
vidual video frames introduces unaffordable computational
cost for most applications.

It is widely recognized that image content varies slowly
over video frames, especially the high level semantics [46,
53, 21]. This observation has been used as means of reg-
ularization in feature learning, considering videos as unsu-

∗This work is done when Xizhou Zhu and Yuwen Xiong are interns at

Microsoft Research Asia

pervised data sources [46, 21]. Yet, such data redundancy
and continuity can also be exploited to reduce the computa-
tion cost. This aspect, however, has received little attention
for video recognition using CNNs in the literature.

Modern CNN architectures [40, 42, 16] share a com-
mon structure. Most layers are convolutional and account
for the most computation. The intermediate convolutional
feature maps have the same spatial extent of the input im-
age (usually at a smaller resolution, e.g., 16× smaller).
They preserve the spatial correspondences between the low
level image content and middle-to-high level semantic con-
cepts [48]. Such correspondence provides opportunities to
cheaply propagate the features between nearby frames by
spatial warping, similar to optical ﬂow [17].

In this work, we present deep feature ﬂow, a fast and
accurate approach for video recognition. It applies an image
recognition network on sparse key frames.
It propagates
the deep feature maps from key frames to other frames via
a ﬂow ﬁeld. As exemplifed in Figure 1, two intermediate
feature maps are responsive to “car” and “person” concepts.
They are similar on two nearby frames. After propagation,
the propagated features are similar to the original features.
Typically, the ﬂow estimation and feature propagation
are much faster than the computation of convolutional fea-
tures. Thus, the computational bottleneck is avoided and
signiﬁcant speedup is achieved. When the ﬂow ﬁeld is also
estimated by a network, the entire architecture is trained
end-to-end, with both image recognition and ﬂow networks
optimized for the recognition task. The recognition accu-
racy is signiﬁcantly boosted.

In sum, deep feature ﬂow is a fast, accurate, general,
and end-to-end framework for video recognition.
It can
adopt most state-of-the-art image recognition networks in
the video domain. Up to our knowledge, it is the ﬁrst
work to jointly train ﬂow and video recognition tasks in
a deep learning framework. Extensive experiments ver-
ify its effectiveness on video object detection and seman-
tic segmentation tasks, on recent large-scale video datasets.
Compared to per-frame evaluation, our approach achieves
unprecedented speed (up to 10× faster, real time frame
rate) with moderate accuracy loss (a few percent). The

Figure 1. Motivation of proposed deep feature ﬂow approach. Here we visualize the two ﬁlters’ feature maps on the last convolutional layer
of our ResNet-101 model (see Sec. 4 for details). The convolutional feature maps are similar on two nearby frames. They can be cheaply
propagated from the key frame to current frame via a ﬂow ﬁeld.

high performance facilitates video recognition tasks in
practice. Code is released at https://github.com/
msracver/Deep-Feature-Flow.

2. Related Work

To our best knowledge, our work is unique and there is
no previous similar work to directly compare with. Never-
theless, it is related to previous works in several aspects, as
discussed below.

Image Recognition Deep learning has been success-
ful on image recognition tasks. The network architectures
have evolved and become powerful on image classiﬁca-
tion [23, 40, 42, 15, 20, 16]. For object detection, the
region-based methods [13, 14, 12, 35, 8] have become the
dominant paradigm. For semantic segmentation, fully con-
volutional networks (FCNs) [28, 4, 52] have dominated the
ﬁeld. However, it is computationally unaffordable to di-
rectly apply such image recognition networks on all the
frames for video recognition. Our work provides an effec-
tive and efﬁcient solution.

Network Acceleration Various approaches have been
proposed to reduce the computation of networks. To name
a few, in [50, 12] matrix factorization is applied to de-
compose large network layers into multiple small layers.
In [7, 34, 18], network weights are quantized. These tech-

niques work on single images. They are generic and com-
plementary to our approach.

Optical Flow It is a fundamental task in video anal-
ysis. The topic has been studied for decades and domi-
nated by variational approaches [17, 2], which mainly ad-
dress small displacements [44]. The recent focus is on large
displacements [3], and combinatorial matching (e.g., Deep-
Flow [45], EpicFlow [36]) has been integrated into the vari-
ational approach. These approaches are all hand-crafted.

Deep learning and semantic information have been ex-
ploited for optical ﬂow recently. FlowNet [9] ﬁrstly applies
deep CNNs to directly estimate the motion and achieves
good result. The network architecture is simpliﬁed in the re-
cent Pyramid Network [33]. Other works attempt to exploit
semantic segmentation information to help optical ﬂow es-
timation [38, 1, 19], e.g., providing speciﬁc constraints on
the ﬂow according to the category of the regions.

Optical ﬂow information has been exploited to help vi-
sion tasks, such as pose estimation [32], frame predic-
tion [31], and attribute transfer [49]. This work exploits
optical ﬂow to speed up general video recognition tasks.

Exploiting Temporal Information in Video Recogni-
tion T-CNN [22] incorporates temporal and contextual in-
formation from tubelets in videos. The dense 3D CRF [24]
proposes long-range spatial-temporal regularization in se-
STFCN [10] considers a
mantic video segmentation.

spatial-temporal FCN for semantic video segmentation.
These works operate on volume data, show improved recog-
nition accuracy but greatly increase the computational cost.
By contrast, our approach seeks to reduce the computation
by exploiting temporal coherence in the videos. The net-
work still runs on single frames and is fast.

Slow Feature Analysis High level semantic concepts
usually evolve slower than the low level image appear-
ance in videos. The deep features are thus expected to
vary smoothly on consecutive video frames. This obser-
vation has been used to regularize the feature learning in
videos [46, 21, 53, 51, 41]. We conjecture that our approach
may also beneﬁt from this fact.

Clockwork Convnets [39] It is the most related work
to ours as it also disables certain layers in the network on
certain video frames and reuses the previous features. It is,
however, much simpler and less effective than our approach.
About speed up, Clockwork only saves the computation
of some layers (e.g., 1/3 or 2/3) in some frames (e.g., every
other frame). As seen later, our method saves that on most
layers (task network has only 1 layer) in most frames (e.g.,
9 out of 10 frames). Thus, our speedup ratio is much higher.
About accuracy, Clockwork does not exploit the corre-
spondence between frames and simply copies features. It
only reschedules the computation of inference in an off-
the-shelf network and does not perform ﬁne-tuning or re-
training. Its accuracy drop is pretty noticeable at even small
speed up. In Table 4 and 6 of their arxiv paper, at 77% full
runtime (thus 1.3 times faster), Mean IU drops from 31.1 to
26.4 on NYUD, from 70.0 to 64.0 on Youtube, from 65.9
to 63.3 on Pascal, and from 65.9 to 64.4 on Cityscapes. By
contrast, we re-train a two-frame network with motion con-
sidered end-to-end. The accuracy drop is small, e.g., from
71.1 to 70.0 on Cityscape while being 3 times faster (Fig-
ure 3, bottom).

About generality, Clockwork is only applicable for se-
mantic segmentation with FCN. Our approach transfers
general image recognition networks to the video domain.

3. Deep Feature Flow

Table 1 summarizes the notations used in this paper. Our

approach is brieﬂy illustrated in Figure 2.

Deep Feature Flow Inference Given an image recogni-
tion task and a feed-forward convolutional neutral network
N that outputs result for input image I as y = N (I). Our
goal is to apply the network to all video frames Ii, i =
0, ..., ∞, fast and accurately.

Following the modern CNN architectures [40, 42, 16]
and applications [28, 4, 52, 13, 14, 12, 35, 8], without loss
of generality, we decompose N into two consecutive sub-
networks. The ﬁrst sub-network Nf eat, dubbed feature net-
work, is fully convolutional and outputs a number of in-
termediate feature maps, f = Nf eat(I). The second sub-

k
i
r
l
s
Ii, Ik
yi, yk
fk
fi
Mi→k
p, q
Si→k
N
Nf eat
Ntask
F
W

key frame index
current frame index
per-frame computation cost ratio, Eq. (5)
key frame duration length
overall speedup ratio, Eq. (7)
video frames
recognition results
convolutional feature maps on key frame
propagated feature maps on current frame
2D ﬂow ﬁeld
2D location
scale ﬁeld
image recognition network
sub-network for feature extraction
sub-network for recognition result
ﬂow estimation function
feature propagation function, Eq. (3)

Table 1. Notations.

network Ntask, dubbed task network, has speciﬁc structures
for the task and performs the recognition task over the fea-
ture maps, y = Ntask(f ).

Consecutive video frames are highly similar. The simi-
larity is even stronger in the deep feature maps, which en-
code high level semantic concepts [46, 21]. We exploit the
similarity to reduce computational cost. Speciﬁcally, the
feature network Nf eat only runs on sparse key frames. The
feature maps of a non-key frame Ii are propagated from its
preceding key frame Ik.

The features in the deep convolutional layers encode the
semantic concepts and correspond to spatial locations in the
image [48]. Examples are illustrated in Figure 1. Such spa-
tial correspondence allows us to cheaply propagate the fea-
ture maps by the manner of spatial warping.

Let Mi→k be a two dimensional ﬂow ﬁeld.

It is ob-
tained by a ﬂow estimation algorithm F such as [26, 9],
Mi→k = F(Ik, Ii).
It is bi-linearly resized to the same
It
spatial resolution of the feature maps for propagation.
projects back a location p in current frame i to the location
p + δp in key frame k, where δp = Mi→k(p).

As the values δp are in general fractional, the feature

warping is implemented via bilinear interpolation

f c
i (p) =

G(q, p + δp)f c

k(q),

(1)

(cid:88)

q

where c identiﬁes a channel in the feature maps f , q enu-
merates all spatial locations in the feature maps, and G(·, ·)
denotes the bilinear interpolation kernel. Note that G is two

Algorithm 1 Deep feature ﬂow inference algorithm for
video recognition.

1: input: video frames {Ii}
2: k = 0;
3: f0 = Nf eat(I0)
4: y0 = Ntask(f0)
5: for i = 1 to ∞ do
6:
7:
8:

k = i
fk = Nf eat(Ik)
yk = Ntask(fk)

if is key f rame(i) then

else

fi = W(fk, F(Ik, Ii), S(Ik, Ii))
yi = Ntask(fi)

9:
10:
11:
12:
end if
13:
14: end for
15: output: recognition results {yi}

(cid:46) initialize key frame

(cid:46) key frame scheduler
(cid:46) update the key frame

(cid:46) use feature ﬂow
(cid:46) propagation

use a CNN to estimate the ﬂow ﬁeld and the scale ﬁeld such
that all the components can be jointly trained end-to-end for
the task.

The architecture is illustrated in Figure 2(b). Train-
ing is performed by stochastic gradient descent (SGD). In
each mini-batch, a pair of nearby video frames, {Ik, Ii}1,
0 ≤ i − k ≤ 9, are randomly sampled. In the forward pass,
feature network Nf eat is applied on Ik to obtain the feature
maps fk. Next, a ﬂow network F runs on the frames Ii, Ik to
estimate the ﬂow ﬁeld and the scale ﬁeld. When i > k, fea-
ture maps fk are propagated to fi as in Eq. (3). Otherwise,
the feature maps are identical and no propagation is done.
Finally, task network Ntask is applied on fi to produce the
result yi, which incurs a loss against the ground truth result.
The loss error gradients are back-propagated throughout to
update all the components. Note that our training accom-
modates the special case when i = k and degenerates to the
per-frame training as in Figure 2(a).

The ﬂow network is much faster than the feature net-
It is pre-trained on the
work, as will be elaborated later.
Flying Chairs dataset [9]. We then add the scale function
S as a sibling output at the end of the network, by increas-
ing the number of channels in the last convolutional layer
appropriately. The scale function is initialized to all ones
(weights and biases in the output layer are initialized as 0s
and 1s, respectively). The augmented ﬂow network is then
ﬁne-tuned as in Figure 2(b).

The feature propagation function in Eq.(3) is unconven-
tional. It is parameter free and fully differentiable. In back-
propagation, we compute the derivative of the features in fi
with respect to the features in fk, the scale ﬁeld Si→k, and
the ﬂow ﬁeld Mi→k. The ﬁrst two are easy to compute us-

1The same notations are used for consistency although there is no

longer the concept of “key frame” during training.

Figure 2. Illustration of video recognition using per-frame network
evaluation (a) and the proposed deep feature ﬂow (b).

dimensional and is separated into two one dimensional ker-
nels as

G(q, p + δp) = g(qx, px + δpx) · g(qy, py + δpy),

(2)

where g(a, b) = max(0, 1 − |a − b|).

We note that Eq. (1) is fast to compute as a few terms are

non-zero.

The spatial warping may be inaccurate due to errors in
ﬂow estimation, object occlusion, etc. To better approx-
imate the features, their amplitudes are modulated by a
“scale ﬁeld” Si→k, which is of the same spatial and chan-
nel dimensions as the feature maps. The scale ﬁeld is ob-
tained by applying a “scale function” S on the two frames,
Si→k = S(Ik, Ii).

Finally, the feature propagation function is deﬁned as

fi = W(fk, Mi→k, Si→k),

(3)

where W applies Eq.(1) for all locations and all channels
in the feature maps, and multiples the features with scales
Si→k in an element-wise way.

The proposed video recognition algorithm is called deep
feature ﬂow. It is summarized in Algorithm 1. Notice that
any ﬂow function F, such as the hand-crafted low-level ﬂow
(e.g., SIFT-Flow [26]), is readily applicable. Training the
ﬂow function is not obligate, and the scale function S is set
to ones everywhere.

Deep Feature Flow Training A ﬂow function is origi-
nally designed to obtain correspondence of low-level image
pixels.
It can be fast in inference, but may not be accu-
rate enough for the recognition task, in which the high-level
feature maps change differently, usually slower than pix-
els [21, 39]. To model such variations, we propose to also

ing the chain rule. For the last, from Eq. (1) and (3), for
each channel c and location p in current frame, we have

∂f c

i (p)
∂Mi→k(p)

= Sc

i→k(p)

(cid:88)

q

∂G(q, p + δp)
∂δp

f c
k(q).

(4)

The term ∂G(q,p+δp)
can be derived from Eq. (2). Note that
the ﬂow ﬁeld M(·) is two-dimensional and we use ∂δp to
denote ∂δpx and ∂δpy for simplicity.

∂δp

The proposed method can easily be trained on datasets
where only sparse frames are annotated, which is usually
the case due to the high labeling costs in video recogni-
tion tasks [29, 11, 6]. In this case, the per-frame training
(Figure 2(a)) can only use annotated frames, while DFF can
easily use all frames as long as frame Ii is annotated. In
other words, DFF can fully use the data even with sparse
ground truth annotation. This is potentially beneﬁcial for
many video recognition tasks.

Inference Complexity Analysis For each non-key
frame, the computational cost ratio of the proposed ap-
proach (line 11-12 in Algorithm 1) and per-frame approach
(line 8-9) is

FlowNet

FlowNet Half

FlowNet Inception

ResNet-50

ResNet-101

9.20
12.71

33.56
46.30

68.97
95.24

Table 2. The approximated complexity ratio in Eq. (6) for different
feature network Nf eat and ﬂow network F, measured by their
FLOPs. See Section 4. Note that r (cid:28) 1 and we use 1
r here for
clarify. A signiﬁcant per-frame speedup factor is obtained.

when to allocate a new key frame. In this work, we use a
simple ﬁxed key frame scheduling, that is, the key frame
duration length l is a ﬁxed constant. It is easy to implement
and tune. However, varied changes in image content may
require a varying l to provide a smooth tradeoff between
accuracy and speed.
Ideally, a new key frame should be
allocated when the image content changes drastically.

How to design effective and adaptive key frame schedul-
ing can further improve our work. Currently it is beyond
the scope of this work. Different video tasks may present
different behaviors and requirements. Learning an adaptive
key frame scheduler from data seems an attractive choice.
This is worth further exploration and left as future work.

r =

O(F) + O(S) + O(W) + O(Ntask)
O(Nf eat) + O(Ntask)

,

(5)

4. Network Architectures

where O(·) measures the function complexity.

To understand this ratio, we ﬁrstly note that the com-
plexity of Ntask is usually small. Although its split point
in N is kind of arbitrary, as veriﬁed in experiment, it is
sufﬁcient to keep only one learnable weight layer in Ntask
in our implementation (see Sec. 4). While both Nf eat
and F have considerable complexity (Section 4), we have
O(Ntask) (cid:28) O(Nf eat) and O(Ntask) (cid:28) O(F).

We also have O(W) (cid:28) O(F) and O(S) (cid:28) O(F) be-
cause W and S are very simple. Thus, the ratio in Eq. (5) is
approximated as

r ≈

O(F)
O(Nf eat)

.

It is mostly determined by the complexity ratio of ﬂow
network F and feature network Nf eat, which can be pre-
cisely measured, e.g., by their FLOPs. Table 2 shows its
typical values in our implementation.

Compared to the per-frame approach,

the overall
speedup factor in Algorithm 1 also depends on the spar-
sity of key frames. Let there be one key frame in every l
consecutive frames, the speedup factor is

s =

l
1 + (l − 1) ∗ r

.

Key Frame Scheduling As indicated in Algorithm 1
(line 6) and Eq. (7), a crucial factor for inference speed is

(6)

(7)

The proposed approach is general for different networks
and recognition tasks. Towards a solid evaluation, we adopt
the state-of-the-art architectures and important vision tasks.
Flow Network We adopt the state-of-the-art CNN based
FlowNet architecture (the “Simple” version) [9] as default.
We also designed two variants of lower complexity. The
ﬁrst one, dubbed FlowNet Half, reduces the number of con-
volutional kernels in each layer of FlowNet by half and the
complexity to 1
4 . The second one, dubbed FlowNet Incep-
tion, adopts the Inception structure [43] and reduces the
complexity to 1
8 of that of FlowNet. The architecture de-
tails are reported in Appendix A.

The three ﬂow networks are pre-trained on the synthetic
Flying Chairs dataset in [9]. The output stride is 4. The in-
put image is half-sized. The resolution of ﬂow ﬁeld is there-
fore 1
8 of the original resolution. As the feature stride of the
feature network is 16 (as described below), the ﬂow ﬁeld
and the scale ﬁeld is further down-sized by half using bi-
linear interpolation to match the resolution of feature maps.
This bilinear interpolation is realized as a parameter-free
layer in the network and also differentiated during training.
Feature Network We use ResNet models [16], speciﬁ-
cally, the ResNet-50 and ResNet-101 models pre-trained for
ImageNet classiﬁcation as default. The last 1000-way clas-
siﬁcation layer is discarded. The feature stride is reduced
from 32 to 16 to produce denser feature maps, following the
practice of DeepLab [4, 5] for semantic segmentation, and
R-FCN [8] for object detection. The ﬁrst block of the conv5

layers are modiﬁed to have a stride of 1 instead of 2. The
holing algorithm [4] is applied on all the 3×3 convolutional
kernels in conv5 to keep the ﬁeld of view (dilation=2). A
randomly initialized 3×3 convolution is appended to conv5
to reduce the feature channel dimension to 1024, where the
holing algorithm is also applied (dilation=6). The resulting
1024-dimensional feature maps are the intermediate feature
maps for the subsequent task.

Table 2 presents the complexity ratio Eq. (6) of feature

networks and ﬂow networks.

Semantic Segmentation A randomly initialized 1 × 1
convolutional layer is applied on the intermediate feature
maps to produce (C +1) score maps, where C is the number
of categories and 1 is for background category. A following
softmax layer outputs the per-pixel probabilities. Thus, the
task network only has one learnable weight layer. The over-
all network architecture is similar to DeepLab with large
ﬁeld-of-view in [5].

Object Detection We adopt

the state-of-the-art R-
FCN [8]. On the intermediate feature maps, two branches of
fully convolutional networks are applied on the ﬁrst half and
the second half 512-dimensional of the intermediate feature
maps separately, for sub-tasks of region proposal and detec-
tion, respectively.

In the region proposal branch, the RPN network [35]
is applied. We use na = 9 anchors (3 scales and 3 as-
pect ratios). Two sibling 1 × 1 convolutional layers out-
put the 2na-dimensional objectness scores and the 4na-
dimensional bounding box (bbox) regression values, re-
spectively. Non-maximum suppression (NMS) is applied to
generate 300 region proposals for each image. Intersection-
over-union (IoU) threshold 0.7 is used.

In the detection branch,

two sibling 1 × 1 convolu-
tional layers output the position-sensitive score maps and
bbox regression maps, respectively. They are of dimensions
(C + 1)k2 and 4k2, respectively, where k banks of classi-
ﬁers/regressors are employed to encode the relative position
information. See [8] for details. On the position-sensitive
score/bbox regression maps, position-sensitive ROI pool-
ing is used to obtain the per-region classiﬁcation score and
bbox regression result. No free parameters are involved in
the per-region computation. Finally, NMS is applied on the
scored and regressed region proposals to produce the detec-
tion result, with IoU threshold 0.3.

5. Experiments

Unlike image datasets, large scale video dataset is much
harder to collect and annotate. Our approach is evaluated
on the two recent datasets: Cityscapes [6] for semantic seg-
mentation, and ImageNet VID [37] for object detection.

5.1. Experiment Setup

Cityscapes It is for urban scene understanding and au-
tonomous driving. It contains snippets of street scenes col-
lected from 50 different cities, at a frame rate of 17 fps. The
train, validation, and test sets contain 2975, 500, and 1525
snippets, respectively. Each snippet has 30 frames, where
the 20th frame is annotated with pixel-level ground-truth la-
bels for semantic segmentation. There are 30 semantic cat-
egories. Following the protocol in [5], training is performed
on the train set and evaluation is performed on the validation
set. The semantic segmentation accuracy is measured by the
pixel-level mean intersection-over-union (mIoU) score.

In both training and inference, the images are resized to
have shorter sides of 1024 and 512 pixels for the feature net-
work and the ﬂow network, respectively. In SGD training,
20K iterations are performed on 8 GPUs (each GPU holds
one mini-batch, thus the effective batch size ×8), where the
learning rates are 10−3 and 10−4 for the ﬁrst 15K and the
last 5K iterations, respectively.

ImageNet VID It is for object detection in videos. The
training, validation, and test sets contain 3862, 555, and 937
fully-annotated video snippets, respectively. The frame rate
is 25 or 30 fps for most snippets. There are 30 object cate-
gories, which are a subset of the categories in the ImageNet
DET image dataset2. Following the protocols in [22, 25],
evaluation is performed on the validation set, using the stan-
dard mean average precision (mAP) metric.

In both training and inference, the images are resized to
have shorter sides of 600 pixels and 300 pixels for the fea-
ture network and the ﬂow network, respectively. In SGD
training, 60K iterations are performed on 8 GPUs, where
the learning rates are 10−3 and 10−4 for the ﬁrst 40K and
the last 20K iterations, respectively.

During training, besides the ImageNet VID train set, we
also used the ImageNet DET train set (only the same 30
category labels are used), following the protocols in [22,
25]. Each mini-batch samples images from either ImageNet
VID or ImageNet DET datasets, at 2 : 1 ratio.

5.2. Evaluation Methodology and Results

Deep feature ﬂow is ﬂexible and allows various design
choices. We evaluate their effects comprehensively in the
experiment. For clarify, we ﬁx their default values through-
out the experiments, unless speciﬁed otherwise. For feature
network Nf eat, ResNet-101 model is default. For ﬂow net-
work F, FlowNet (section 4) is default. Key-frame duration
length l is 5 for Cityscapes [6] segmentation and 10 for Im-
ageNet VID [37] detection by default, based on different
frame rate of videos in the datasets..

For each snippet we evaluate l image pairs, (k, i), k =
i − l + 1, ..., i, for each frame i with ground truth anno-

2http://www.image-net.org/challenges/LSVRC/

method

training of image recognition network N training of ﬂow network F

trained on single frames as in Fig. 2 (a)

no ﬂow network used

Frame (oracle baseline)
SFF-slow
SFF-fast
DFF
DFF ﬁx N
DFF ﬁx F
DFF separate

same as Frame

same as Frame

same as DFF

same as Frame

trained on frame pairs as in Fig. 2 (b)

init. on Flying Chairs [9], ﬁne-tuned in Fig. 2 (b)

same as Frame, then ﬁxed in Fig. 2 (b)

same as DFF

SIFT-Flow [26] (w/ best parameters), no training

SIFT-Flow [26] (w/ default parameters), no training

init. on Flying Chairs [9], then ﬁxed in Fig. 2 (b)

init. on Flying Chairs [9]

Table 3. Description of variants of deep feature ﬂow (DFF), shallow feature ﬂow (SFF), and the per-frame approach (Frame).

Cityscapes (l = 5)

ImageNet VID (l = 10)

mIoU(%)

runtime (fps) mAP(%)

runtime (fps)

Methods

Frame
SFF-slow
SFF-fast
DFF
DFF ﬁx N
DFF ﬁx F
DFF separate

71.1
67.8
67.3
69.2
68.8
67.0
66.9

1.52
0.08
0.95
5.60
5.60
5.60
5.60

73.9
70.7
69.7
73.1
72.3
68.8
67.4

4.05
0.26
3.04
20.25
20.25
20.25
20.25

Table 4. Comparison of accuracy and runtime (mostly in GPU) of
various approaches in Table 3. Note that, the runtime for SFF con-
sists of CPU runtime of SIFT-Flow and GPU runtime of Frame,
since SIFT-Flow only has CPU implementation.

tation. Time evaluation is on a workstation with NVIDIA
K40 GPU and Intel Core i7-4790 CPU.

Validation of DFF Architecture We compared DFF

with several baselines and variants, as listed in Table 3.

• Frame: train N on single frames with ground truth.

• SFF: use pre-computed large-displacement ﬂow (e.g.,
SIFT-Flow [26]). SFF-fast and SFF-slow adopt differ-
ent parameters.

• DFF: the proposed approach, N and F are trained
end-to-end. Several variants include DFF ﬁx N (ﬁx
N in training), DFF ﬁx F (ﬁx F in training), and DFF
seperate (N and F are separately trained).

Table 4 summarizes the accuracy and runtime of all ap-
proaches. We ﬁrstly note that the baseline Frame is strong
enough to serve as a reference for comparison. Our im-
plementation resembles the state-of-the-art DeepLab [5] for
semantic segmentation and R-FCN [8] for object detection.
In DeepLab [5], an mIoU score of 69.2% is reported with
DeepLab large ﬁeld-of-view model using ResNet-101 on
Cityscapes validation dataset. Our Frame baseline achieves
slightly higher 71.1%, based on the same ResNet model.

For object detection, Frame baseline has mAP 73.9% us-
ing R-FCN [8] and ResNet-101. As a reference, a com-
parable mAP score of 73.8% is reported in [22], by com-
bining CRAFT [47] and DeepID-net [30] object detectors
trained on the ImageNet data, using both VGG-16 [40] and
GoogleNet-v2 [20] models, with various tricks (multi-scale
training/testing, adding context information, model ensem-
ble). We do not adopt above tricks as they complicate the
comparison and obscure the conclusions.

SFF-fast has a reasonable runtime but accuracy is sig-
niﬁcantly decreased. SFF-slow uses the best parameters for
ﬂow estimation. It is much slower. Its accuracy is slightly
improved but still poor. This indicates that an off-the-shelf
ﬂow may be insufﬁcient.

The proposed DFF approach has the best overall perfor-
mance. Its accuracy is slightly lower than that of Frame and
it is 3.7 and 5.0 times faster for segmentation and detection,
respectively. As expected, the three variants without using
joint training have worse accuracy. Especially, the accuracy
drop by ﬁxing F is signiﬁcant. This indicates a jointing
end-to-end training (especially ﬂow) is crucial.

We also tested another variant of DFF with the scale
function S removed (Algorithm 1, Eq (3), Eq. (4)). The
accuracy drops for both segmentation and detection (less
than one percent). It shows that the scaled modulation of
features is slightly helpful.

Accuracy-Speedup Tradeoff We investigate the trade-
off by varying the ﬂow network F, the feature network
Nf eat, and key frame duration length l. Since Cityscapes
and ImageNet VID datasets have different frame rates, we
tested l = 1, 2, ..., 10 for segmentation and l = 1, 2, ..., 20
for detection.

The results are summarized in Figure 3. Overall, DFF
achieves signiﬁcant speedup with decent accuracy drop. It
smoothly trades in accuracy for speed and ﬁts different
application needs ﬂexibly. For example, in detection, it
improves 4.05 fps of ResNet-101 Frame to 41.26 fps of
ResNet-101 + FlowNet Inception. The 10× faster speed is
at the cost of moderate accuracy drop from 73.9% to 69.5%.
In segmentation, it improves 2.24 fps of ResNet-50 Frame

# layers in Ntask

Cityscapes (l=5)

ImageNet VID (l=10)

mIoU(%)

runtime (fps) mAP(%)

runtime (fps)

21
12
5
1 (default)
0

69.1
69.1
69.2
69.2
69.5

2.87
3.14
3.89
5.60
5.61

73.2
73.3
73.2
73.1
72.7

7.23
8.04
9.99
20.25
20.40

Table 5. Results of using different split points for Ntask.

to 17.48 fps of ResNet-50 FlowNet Inception, at the cost of
accuracy drop from 69.7% to 62.4%.

What ﬂow F should we use? From Figure 3, the smallest
FlowNet Inception is advantageous. It is faster than its two
counterparts at the same accuracy level, most of the times.
What feature Nf eat should we use? In high-accuracy
zone, an accurate model ResNet-101 is clearly better than
ResNet-50. In high-speed zone, the conclusions are differ-
ent on the two tasks. For detection, ResNet-101 is still ad-
vantageous. For segmentation, the performance curves in-
tersect at around 6.35 fps point. For higher speed, ResNet-
50 becomes better than ResNet-101. The seemingly dif-
ferent conclusions can be partially attributed to the differ-
ent video frame rates, the extents of dynamics on the two
datasets. The Cityscapes dataset not only has a low frame
rate 17 fps, but also more quick dynamics. It would be hard
to utilize temporal redundancy for a long propagation. To
achieve the same high speed, ResNet-101 needs a larger key
frame length l than ResNet-50. This in turn signiﬁcantly
increases the difﬁculty of learning.

Above observations provide useful recommendations for
practical applications. Yet, they are more heuristic than gen-
eral, as they are observed only on the two tasks, on limited
data. We plan to explore the design space more in the future.
Split point of Ntask Where should we split Ntask in N ?
Recall that the default Ntask keeps one layer with learning
weight (the 1 × 1 conv over 1024-d feature maps, see Sec-
tion 4). Before this is the 3 × 3 conv layer that reduces di-
mension to 1024. Before this is series of “Bottleneck” unit
in ResNet [16], each consisting of 3 layers. We back move
the split point to make different Ntasks with 5, 12, and 21
layers, respectively. The one with 5 layers adds the dimen-
sion reduction layer and one bottleneck unit (conv5c). The
one with 12 layers adds two more units (conv5a and conv5b)
at the beginning of conv5. The one with 21 layers adds three
more units in conv4. We also move the only layer in default
Ntask into Nf eat, leaving Ntask with 0 layer (with learn-
able weights). This is equivalent to directly propagate the
parameter-free score maps, in both semantic segmentation
and object detection.

Table 5 summarizes the results. Overall, the accuracy
variation is small enough to be neglected. The speed be-

Figure 3. (better viewed in color) Illustration of accuracy-speed
tradeoff under different implementation choices on ImageNet VID
detection (top) and on Cityscapes segmentation (bottom).

comes lower when Ntask has more layers. Using 0 layer
is mostly equivalent to using 1 layer, in both accuracy and
speed. We choose 1 layer as default as that leaves some tun-
able parameters after the feature propagation, which could
be more general.

Example results of the proposed method are presented
in Figure 4 and Figure 5 for video segmentation on
CityScapes and video detection on ImageNet VID, respec-
tively. More example results are available at https:
//www.youtube.com/watch?v=J0rMHE6ehGw.

6. Future Work

Several important aspects are left for further exploration.
It would be interesting to exploit how the joint learning af-
fects the ﬂow quality. We are unable to evaluate as there
lacks ground truth. Current optical ﬂow works are also lim-
ited to either synthetic data [9] or small real datasets, which
is insufﬁcient for deep learning.

Our method can further beneﬁt from improvements in
ﬂow estimation and key frame scheduling. In this paper, we
adopt FlowNet [9] mainly because there are few choices.
Designing faster and more accurate ﬂow network will cer-

Figure 4. Semantic segmentation results on Cityscapes validation dataset. The ﬁrst column corresponds to the images and the results on
the key frame (the kth frame). The following four columns correspond to the k + 1st, k + 2nd, k + 3rd and k + 4th frames, respectively.

Figure 5. Object detection results on ImageNet VID validation dataset. The ﬁrst column corresponds to the images and the results on the
key frame (the kth frame). The following four columns correspond to the k + 2nd, k + 4th, k + 6th and k + 8th frames, respectively.

layer

type

stride

# output

conv1
conv2
conv3
conv3 1
conv4
conv4 1
conv5
conv5 1
conv6
conv6 1

conv1
conv2
conv3
conv3 1
conv4
conv4 1
conv5
conv5 1
conv6
conv6 1

7x7 conv
5x5 conv
5x5 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv

7x7 conv
5x5 conv
5x5 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv

2
2
2

2

2

2

2
2
2

2

2

2

64
128
256
256
512
512
512
512
1024
1024

32
64
128
128
256
256
256
256
512
512

Table 6. The FlowNet network architecture.

layer

type

stride

# output

Table 7. The FlowNet Half network architecture.

tainly receive more attention in the future. For key frame
scheduling, a good scheduler may well signiﬁcantly im-
prove both speed and accuracy. And this problem is deﬁ-
nitely worth further exploration.

We believe this work opens many new possibilities. We

hope it will inspire more future work.

A. FlowNet Inception Architecture

The architectures of FlowNet, FlowNet Half follow that
of [9] (the “Simple” version), which are detailed in Table 6
and Table 7, respectively. The architecture of FlowNet In-
ception follows the design of the Inception structure [43],
which is detailed in Table 8.

References

[1] M. Bai, W. Luo, K. Kundu, and R. Urtasun. Exploiting se-
mantic information and deep matching for optical ﬂow. In

ECCV, 2016. 2

[2] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical ﬂow estimation based on a theory for warping.
In ECCV, 2004. 2

[3] T. Brox and J. Malik. Large displacement optical ﬂow: De-
scriptor matching in variational motion estimation. TPAMI,
2011. 2

[4] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-
volutional nets and fully connected crfs. In ICLR, 2015. 1,
2, 3, 5, 6

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. arXiv preprint, 2016. 5, 6, 7

[6] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In CVPR, 2016. 1, 5, 6

[7] M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect:
Training deep neural networks with binary weights during
propagations. In NIPS, 2015. 2

[8] J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via
region-based fully convolutional networks. In NIPS, 2016.
1, 2, 3, 5, 6, 7

[9] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
and V. Golkov. Flownet: Learning optical ﬂow with convo-
lutional networks. In ICCV, 2015. 2, 3, 4, 5, 7, 8, 11
[10] M. Fayyaz, M. H. Saffar, M. Sabokrou, M. Fathy, and
R. Klette. STFCN: spatio-temporal FCN for semantic video
segmentation. arXiv preprint, 2016. 2

[11] F. Galasso, N. Shankar Nagaraja, T. Jimenez Cardenas,
T. Brox, and B. Schiele. A uniﬁed video segmentation bench-
mark: Annotation, metrics and analysis. In ICCV, 2013. 5

[12] R. Girshick. Fast R-CNN. In ICCV, 2015. 1, 2, 3
[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014. 1, 2, 3

[14] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling
in deep convolutional networks for visual recognition.
In
ECCV, 2014. 1, 2, 3

[15] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In ICCV, 2015. 2

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 2, 3, 5, 8

[17] B. K. Horn and B. G. Schunck. Determining optical ﬂow.

Artiﬁcial intelligence, 1981. 1, 2

[18] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Y. Bengio. Quantized Neural Networks: Training Neu-
ral Networks with Low Precision Weights and Activations.
arXiv preprint, 2016. 2

[19] J. Hur and S. Roth. Joint optical ﬂow and temporally consis-
tent semantic segmentation. In ECCV CVRSUAD Workshop,
2016. 2

[20] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 2, 7

type

stride

# output

#1x1

#1x1-#3x3

#1x1-#3x3-#3x3

#pool

Inception/Reduction

layer

conv1
pool1
conv2
conv3 1
conv3 2
conv3 3
conv4 1
conv4 2
conv5 1
conv5 2
conv6 1
conv6 2

7x7 conv
3x3 max pool
Inception
3x3 conv
Inception
Inception
Reduction
Inception
Reduction
Inception
Reduction
Inception

2
2

2

2

2

2

32
32
64
128
128
128
256
256
384
384
512
512

24-32

24-32-32

48
48
32
96
48
144
64
192

32-64
48-64
112-128
112-128
96-192
96-192
192-256
192-256

8-16-16
12-16-16
28-32-32
28-32-32
36-48-48
36-48-48
48-64-64
48-64-64

64

96

128

Table 8. The FlowNet Inception network architecture, following the design of the Inception structure [43]. ”Inception/Reduction” modules
consist of four branches: 1x1 conv (#1x1), 1x1 conv-3x3 conv (#1x1-#3x3), 1x1 conv-3x3 conv-3x3 conv (#1x1-#3x3-#3x3), and 3x3 max
pooling followed by 1x1 conv (#pool, only for stride=2).

[21] D. Jayaraman and K. Grauman. Slow and steady feature
analysis: higher order temporal coherence in video.
In
CVPR, 2016. 1, 3, 4

[22] K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang,
Z. Wang, R. Wang, and X. Wang. T-cnn: Tubelets with con-
volutional neural networks for object detection from videos.
In CVPR, 2016. 2, 6, 7

[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1, 2

Imagenet
In

[24] A. Kundu, V. Vineet, and V. Koltun. Feature space optimiza-
tion for semantic video segmentation. In CVPR, 2016. 2
[25] B. Lee, E. Erdenee, S. Jin, and P. K. Rhee. Multi-class
multi-object tracking using changing point detection. arXiv
preprint, 2016. 6

[26] C. Liu, J. Yuen, A. Torralba, J. Sivic, and W. T. Freeman.
Sift ﬂow: dense correspondence across difference scenes. In
ECCV, 2008. 3, 4, 7

[27] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed.

Ssd: Single shot multibox detector. 2016. 1

[28] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, 2015. 1, 2, 3
[29] P. K. Nathan Silberman, Derek Hoiem and R. Fergus. Indoor
segmentation and support inference from rgbd images.
In
ECCV, 2012. 5

[30] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y. Tian, H. Li,
S. Yang, Z. Wang, and C.-C. Loy. Deepid-net: Deformable
deep convolutional neural networks for object detection. In
CVPR, 2015. 7

[31] V. Patraucean, A. Handa, and R. Cipolla. Spatio-temporal
arXiv

video autoencoder with differentiable memory.
preprint arXiv:1511.06309, 2015. 2

[32] T. Pﬁster, J. Charles, and A. Zisserman. Flowing convnets
for human pose estimation in videos. In ICCV, 2015. 2

[33] A. Ranjan and M. J. Black. Optical ﬂow estimation using a

spatial pyramid network. arXiv preprint, 2016. 2

[34] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. arXiv preprint, 2016. 2

[35] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-
wards real-time object detection with region proposal net-
works. In NIPS, 2015. 1, 2, 3, 6

[36] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
EpicFlow: Edge-Preserving Interpolation of Correspon-
dences for Optical Flow. In CVPR, 2015. 2

[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. IJCV, 2015. 1, 6

[38] L. Sevilla-Lara, D. Sun, V. Jampani, and M. J. Black. Optical
In
ﬂow with semantic segmentation and localized layers.
CVPR, 2016. 2

[39] E. Shelhamer, K. Rakelly, J. Hoffman, and T. Darrell. Clock-
work convnets for video semantic segmentation. In ECCV,
2016. 3, 4

[40] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
1, 2, 3, 7

[41] L. Sun, K. Jia, T.-H. Chan, Y. Fang, G. Wang, and S. Yan. Dl-
sfa: deeply-learned slow feature analysis for action recogni-
tion. In CVPR, 2014. 3

[42] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 1, 2, 3
[43] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR, 2016. 5, 11, 12

[44] J. Weickert, A. Bruhn, T. Brox, and N. Papenberg. A survey
on variational optic ﬂow methods for small displacements.
In Mathematical models for registration and applications to
medical imaging. 2006. 2

[45] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid.
DeepFlow: Large displacement optical ﬂow with deep
matching. In CVPR, 2013. 2

[46] L. Wiskott and T. J. Sejnowski. Slow feature analysis: Unsu-
pervised learning of invariances. Neural computation, 2002.
1, 3

[47] B. Yang, J. Yan, Z. Lei, and S. Z. Li. Craft objects from

images. In CVPR, 2016. 7

[48] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014. 1, 3

[49] W. Zhang, P. Srinivasan, and J. Shi. Discriminative image

warping with attribute ﬂow. In CVPR, 2011. 2

[50] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very
deep convolutional networks for classiﬁcation and detection.
TPAMI, 2015. 2

[51] Z. Zhang and D. Tao. Slow feature analysis for human action

recognition. TPAMI, 2012. 3

[52] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. Torr. Conditional random
ﬁelds as recurrent neural networks. In ICCV, 2015. 1, 2, 3

[53] W. Zou, S. Zhu, K. Yu, and A. Y. Ng. Deep learning of
invariant features via simulated ﬁxations in video. In NIPS,
2012. 1, 3

Deep Feature Flow for Video Recognition

Xizhou Zhu1,2∗

Yuwen Xiong2∗

Jifeng Dai2

Lu Yuan2

Yichen Wei2

1University of Science and Technology of China
ezra0408@mail.ustc.edu.cn

2Microsoft Research
{v-yuxio,jifdai,luyuan,yichenw}@microsoft.com

7
1
0
2
 
n
u
J
 
5
 
 
]

V
C
.
s
c
[
 
 
2
v
5
1
7
7
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

Deep convolutional neutral networks have achieved
great success on image recognition tasks. Yet, it is non-
trivial to transfer the state-of-the-art image recognition net-
works to videos as per-frame evaluation is too slow and un-
affordable. We present deep feature ﬂow, a fast and accu-
rate framework for video recognition.
It runs the expen-
sive convolutional sub-network only on sparse key frames
and propagates their deep feature maps to other frames via
a ﬂow ﬁeld. It achieves signiﬁcant speedup as ﬂow com-
putation is relatively fast. The end-to-end training of the
whole architecture signiﬁcantly boosts the recognition ac-
curacy. Deep feature ﬂow is ﬂexible and general. It is vali-
dated on two video datasets on object detection and seman-
tic segmentation. It signiﬁcantly advances the practice of
video recognition tasks. Code is released at https://
github.com/msracver/Deep-Feature-Flow.

1. Introduction

Recent years have witnessed signiﬁcant success of deep
convolutional neutral networks (CNNs) for image recogni-
tion, e.g., image classiﬁcation [23, 40, 42, 16], semantic
segmentation [28, 4, 52], and object detection [13, 14, 12,
35, 8, 27]. With their success, the recognition tasks have
been extended from image domain to video domain, such
as semantic segmentation on Cityscapes dataset [6], and ob-
ject detection on ImageNet VID dataset [37]. Fast and ac-
curate video recognition is crucial for high-value scenarios,
e.g., autonomous driving and video surveillance. Neverthe-
less, applying existing image recognition networks on indi-
vidual video frames introduces unaffordable computational
cost for most applications.

It is widely recognized that image content varies slowly
over video frames, especially the high level semantics [46,
53, 21]. This observation has been used as means of reg-
ularization in feature learning, considering videos as unsu-

∗This work is done when Xizhou Zhu and Yuwen Xiong are interns at

Microsoft Research Asia

pervised data sources [46, 21]. Yet, such data redundancy
and continuity can also be exploited to reduce the computa-
tion cost. This aspect, however, has received little attention
for video recognition using CNNs in the literature.

Modern CNN architectures [40, 42, 16] share a com-
mon structure. Most layers are convolutional and account
for the most computation. The intermediate convolutional
feature maps have the same spatial extent of the input im-
age (usually at a smaller resolution, e.g., 16× smaller).
They preserve the spatial correspondences between the low
level image content and middle-to-high level semantic con-
cepts [48]. Such correspondence provides opportunities to
cheaply propagate the features between nearby frames by
spatial warping, similar to optical ﬂow [17].

In this work, we present deep feature ﬂow, a fast and
accurate approach for video recognition. It applies an image
recognition network on sparse key frames.
It propagates
the deep feature maps from key frames to other frames via
a ﬂow ﬁeld. As exemplifed in Figure 1, two intermediate
feature maps are responsive to “car” and “person” concepts.
They are similar on two nearby frames. After propagation,
the propagated features are similar to the original features.
Typically, the ﬂow estimation and feature propagation
are much faster than the computation of convolutional fea-
tures. Thus, the computational bottleneck is avoided and
signiﬁcant speedup is achieved. When the ﬂow ﬁeld is also
estimated by a network, the entire architecture is trained
end-to-end, with both image recognition and ﬂow networks
optimized for the recognition task. The recognition accu-
racy is signiﬁcantly boosted.

In sum, deep feature ﬂow is a fast, accurate, general,
and end-to-end framework for video recognition.
It can
adopt most state-of-the-art image recognition networks in
the video domain. Up to our knowledge, it is the ﬁrst
work to jointly train ﬂow and video recognition tasks in
a deep learning framework. Extensive experiments ver-
ify its effectiveness on video object detection and seman-
tic segmentation tasks, on recent large-scale video datasets.
Compared to per-frame evaluation, our approach achieves
unprecedented speed (up to 10× faster, real time frame
rate) with moderate accuracy loss (a few percent). The

Figure 1. Motivation of proposed deep feature ﬂow approach. Here we visualize the two ﬁlters’ feature maps on the last convolutional layer
of our ResNet-101 model (see Sec. 4 for details). The convolutional feature maps are similar on two nearby frames. They can be cheaply
propagated from the key frame to current frame via a ﬂow ﬁeld.

high performance facilitates video recognition tasks in
practice. Code is released at https://github.com/
msracver/Deep-Feature-Flow.

2. Related Work

To our best knowledge, our work is unique and there is
no previous similar work to directly compare with. Never-
theless, it is related to previous works in several aspects, as
discussed below.

Image Recognition Deep learning has been success-
ful on image recognition tasks. The network architectures
have evolved and become powerful on image classiﬁca-
tion [23, 40, 42, 15, 20, 16]. For object detection, the
region-based methods [13, 14, 12, 35, 8] have become the
dominant paradigm. For semantic segmentation, fully con-
volutional networks (FCNs) [28, 4, 52] have dominated the
ﬁeld. However, it is computationally unaffordable to di-
rectly apply such image recognition networks on all the
frames for video recognition. Our work provides an effec-
tive and efﬁcient solution.

Network Acceleration Various approaches have been
proposed to reduce the computation of networks. To name
a few, in [50, 12] matrix factorization is applied to de-
compose large network layers into multiple small layers.
In [7, 34, 18], network weights are quantized. These tech-

niques work on single images. They are generic and com-
plementary to our approach.

Optical Flow It is a fundamental task in video anal-
ysis. The topic has been studied for decades and domi-
nated by variational approaches [17, 2], which mainly ad-
dress small displacements [44]. The recent focus is on large
displacements [3], and combinatorial matching (e.g., Deep-
Flow [45], EpicFlow [36]) has been integrated into the vari-
ational approach. These approaches are all hand-crafted.

Deep learning and semantic information have been ex-
ploited for optical ﬂow recently. FlowNet [9] ﬁrstly applies
deep CNNs to directly estimate the motion and achieves
good result. The network architecture is simpliﬁed in the re-
cent Pyramid Network [33]. Other works attempt to exploit
semantic segmentation information to help optical ﬂow es-
timation [38, 1, 19], e.g., providing speciﬁc constraints on
the ﬂow according to the category of the regions.

Optical ﬂow information has been exploited to help vi-
sion tasks, such as pose estimation [32], frame predic-
tion [31], and attribute transfer [49]. This work exploits
optical ﬂow to speed up general video recognition tasks.

Exploiting Temporal Information in Video Recogni-
tion T-CNN [22] incorporates temporal and contextual in-
formation from tubelets in videos. The dense 3D CRF [24]
proposes long-range spatial-temporal regularization in se-
STFCN [10] considers a
mantic video segmentation.

spatial-temporal FCN for semantic video segmentation.
These works operate on volume data, show improved recog-
nition accuracy but greatly increase the computational cost.
By contrast, our approach seeks to reduce the computation
by exploiting temporal coherence in the videos. The net-
work still runs on single frames and is fast.

Slow Feature Analysis High level semantic concepts
usually evolve slower than the low level image appear-
ance in videos. The deep features are thus expected to
vary smoothly on consecutive video frames. This obser-
vation has been used to regularize the feature learning in
videos [46, 21, 53, 51, 41]. We conjecture that our approach
may also beneﬁt from this fact.

Clockwork Convnets [39] It is the most related work
to ours as it also disables certain layers in the network on
certain video frames and reuses the previous features. It is,
however, much simpler and less effective than our approach.
About speed up, Clockwork only saves the computation
of some layers (e.g., 1/3 or 2/3) in some frames (e.g., every
other frame). As seen later, our method saves that on most
layers (task network has only 1 layer) in most frames (e.g.,
9 out of 10 frames). Thus, our speedup ratio is much higher.
About accuracy, Clockwork does not exploit the corre-
spondence between frames and simply copies features. It
only reschedules the computation of inference in an off-
the-shelf network and does not perform ﬁne-tuning or re-
training. Its accuracy drop is pretty noticeable at even small
speed up. In Table 4 and 6 of their arxiv paper, at 77% full
runtime (thus 1.3 times faster), Mean IU drops from 31.1 to
26.4 on NYUD, from 70.0 to 64.0 on Youtube, from 65.9
to 63.3 on Pascal, and from 65.9 to 64.4 on Cityscapes. By
contrast, we re-train a two-frame network with motion con-
sidered end-to-end. The accuracy drop is small, e.g., from
71.1 to 70.0 on Cityscape while being 3 times faster (Fig-
ure 3, bottom).

About generality, Clockwork is only applicable for se-
mantic segmentation with FCN. Our approach transfers
general image recognition networks to the video domain.

3. Deep Feature Flow

Table 1 summarizes the notations used in this paper. Our

approach is brieﬂy illustrated in Figure 2.

Deep Feature Flow Inference Given an image recogni-
tion task and a feed-forward convolutional neutral network
N that outputs result for input image I as y = N (I). Our
goal is to apply the network to all video frames Ii, i =
0, ..., ∞, fast and accurately.

Following the modern CNN architectures [40, 42, 16]
and applications [28, 4, 52, 13, 14, 12, 35, 8], without loss
of generality, we decompose N into two consecutive sub-
networks. The ﬁrst sub-network Nf eat, dubbed feature net-
work, is fully convolutional and outputs a number of in-
termediate feature maps, f = Nf eat(I). The second sub-

k
i
r
l
s
Ii, Ik
yi, yk
fk
fi
Mi→k
p, q
Si→k
N
Nf eat
Ntask
F
W

key frame index
current frame index
per-frame computation cost ratio, Eq. (5)
key frame duration length
overall speedup ratio, Eq. (7)
video frames
recognition results
convolutional feature maps on key frame
propagated feature maps on current frame
2D ﬂow ﬁeld
2D location
scale ﬁeld
image recognition network
sub-network for feature extraction
sub-network for recognition result
ﬂow estimation function
feature propagation function, Eq. (3)

Table 1. Notations.

network Ntask, dubbed task network, has speciﬁc structures
for the task and performs the recognition task over the fea-
ture maps, y = Ntask(f ).

Consecutive video frames are highly similar. The simi-
larity is even stronger in the deep feature maps, which en-
code high level semantic concepts [46, 21]. We exploit the
similarity to reduce computational cost. Speciﬁcally, the
feature network Nf eat only runs on sparse key frames. The
feature maps of a non-key frame Ii are propagated from its
preceding key frame Ik.

The features in the deep convolutional layers encode the
semantic concepts and correspond to spatial locations in the
image [48]. Examples are illustrated in Figure 1. Such spa-
tial correspondence allows us to cheaply propagate the fea-
ture maps by the manner of spatial warping.

Let Mi→k be a two dimensional ﬂow ﬁeld.

It is ob-
tained by a ﬂow estimation algorithm F such as [26, 9],
Mi→k = F(Ik, Ii).
It is bi-linearly resized to the same
It
spatial resolution of the feature maps for propagation.
projects back a location p in current frame i to the location
p + δp in key frame k, where δp = Mi→k(p).

As the values δp are in general fractional, the feature

warping is implemented via bilinear interpolation

f c
i (p) =

G(q, p + δp)f c

k(q),

(1)

(cid:88)

q

where c identiﬁes a channel in the feature maps f , q enu-
merates all spatial locations in the feature maps, and G(·, ·)
denotes the bilinear interpolation kernel. Note that G is two

Algorithm 1 Deep feature ﬂow inference algorithm for
video recognition.

1: input: video frames {Ii}
2: k = 0;
3: f0 = Nf eat(I0)
4: y0 = Ntask(f0)
5: for i = 1 to ∞ do
6:
7:
8:

k = i
fk = Nf eat(Ik)
yk = Ntask(fk)

if is key f rame(i) then

else

fi = W(fk, F(Ik, Ii), S(Ik, Ii))
yi = Ntask(fi)

9:
10:
11:
12:
end if
13:
14: end for
15: output: recognition results {yi}

(cid:46) initialize key frame

(cid:46) key frame scheduler
(cid:46) update the key frame

(cid:46) use feature ﬂow
(cid:46) propagation

use a CNN to estimate the ﬂow ﬁeld and the scale ﬁeld such
that all the components can be jointly trained end-to-end for
the task.

The architecture is illustrated in Figure 2(b). Train-
ing is performed by stochastic gradient descent (SGD). In
each mini-batch, a pair of nearby video frames, {Ik, Ii}1,
0 ≤ i − k ≤ 9, are randomly sampled. In the forward pass,
feature network Nf eat is applied on Ik to obtain the feature
maps fk. Next, a ﬂow network F runs on the frames Ii, Ik to
estimate the ﬂow ﬁeld and the scale ﬁeld. When i > k, fea-
ture maps fk are propagated to fi as in Eq. (3). Otherwise,
the feature maps are identical and no propagation is done.
Finally, task network Ntask is applied on fi to produce the
result yi, which incurs a loss against the ground truth result.
The loss error gradients are back-propagated throughout to
update all the components. Note that our training accom-
modates the special case when i = k and degenerates to the
per-frame training as in Figure 2(a).

The ﬂow network is much faster than the feature net-
It is pre-trained on the
work, as will be elaborated later.
Flying Chairs dataset [9]. We then add the scale function
S as a sibling output at the end of the network, by increas-
ing the number of channels in the last convolutional layer
appropriately. The scale function is initialized to all ones
(weights and biases in the output layer are initialized as 0s
and 1s, respectively). The augmented ﬂow network is then
ﬁne-tuned as in Figure 2(b).

The feature propagation function in Eq.(3) is unconven-
tional. It is parameter free and fully differentiable. In back-
propagation, we compute the derivative of the features in fi
with respect to the features in fk, the scale ﬁeld Si→k, and
the ﬂow ﬁeld Mi→k. The ﬁrst two are easy to compute us-

1The same notations are used for consistency although there is no

longer the concept of “key frame” during training.

Figure 2. Illustration of video recognition using per-frame network
evaluation (a) and the proposed deep feature ﬂow (b).

dimensional and is separated into two one dimensional ker-
nels as

G(q, p + δp) = g(qx, px + δpx) · g(qy, py + δpy),

(2)

where g(a, b) = max(0, 1 − |a − b|).

We note that Eq. (1) is fast to compute as a few terms are

non-zero.

The spatial warping may be inaccurate due to errors in
ﬂow estimation, object occlusion, etc. To better approx-
imate the features, their amplitudes are modulated by a
“scale ﬁeld” Si→k, which is of the same spatial and chan-
nel dimensions as the feature maps. The scale ﬁeld is ob-
tained by applying a “scale function” S on the two frames,
Si→k = S(Ik, Ii).

Finally, the feature propagation function is deﬁned as

fi = W(fk, Mi→k, Si→k),

(3)

where W applies Eq.(1) for all locations and all channels
in the feature maps, and multiples the features with scales
Si→k in an element-wise way.

The proposed video recognition algorithm is called deep
feature ﬂow. It is summarized in Algorithm 1. Notice that
any ﬂow function F, such as the hand-crafted low-level ﬂow
(e.g., SIFT-Flow [26]), is readily applicable. Training the
ﬂow function is not obligate, and the scale function S is set
to ones everywhere.

Deep Feature Flow Training A ﬂow function is origi-
nally designed to obtain correspondence of low-level image
pixels.
It can be fast in inference, but may not be accu-
rate enough for the recognition task, in which the high-level
feature maps change differently, usually slower than pix-
els [21, 39]. To model such variations, we propose to also

ing the chain rule. For the last, from Eq. (1) and (3), for
each channel c and location p in current frame, we have

∂f c

i (p)
∂Mi→k(p)

= Sc

i→k(p)

(cid:88)

q

∂G(q, p + δp)
∂δp

f c
k(q).

(4)

The term ∂G(q,p+δp)
can be derived from Eq. (2). Note that
the ﬂow ﬁeld M(·) is two-dimensional and we use ∂δp to
denote ∂δpx and ∂δpy for simplicity.

∂δp

The proposed method can easily be trained on datasets
where only sparse frames are annotated, which is usually
the case due to the high labeling costs in video recogni-
tion tasks [29, 11, 6]. In this case, the per-frame training
(Figure 2(a)) can only use annotated frames, while DFF can
easily use all frames as long as frame Ii is annotated. In
other words, DFF can fully use the data even with sparse
ground truth annotation. This is potentially beneﬁcial for
many video recognition tasks.

Inference Complexity Analysis For each non-key
frame, the computational cost ratio of the proposed ap-
proach (line 11-12 in Algorithm 1) and per-frame approach
(line 8-9) is

FlowNet

FlowNet Half

FlowNet Inception

ResNet-50

ResNet-101

9.20
12.71

33.56
46.30

68.97
95.24

Table 2. The approximated complexity ratio in Eq. (6) for different
feature network Nf eat and ﬂow network F, measured by their
FLOPs. See Section 4. Note that r (cid:28) 1 and we use 1
r here for
clarify. A signiﬁcant per-frame speedup factor is obtained.

when to allocate a new key frame. In this work, we use a
simple ﬁxed key frame scheduling, that is, the key frame
duration length l is a ﬁxed constant. It is easy to implement
and tune. However, varied changes in image content may
require a varying l to provide a smooth tradeoff between
accuracy and speed.
Ideally, a new key frame should be
allocated when the image content changes drastically.

How to design effective and adaptive key frame schedul-
ing can further improve our work. Currently it is beyond
the scope of this work. Different video tasks may present
different behaviors and requirements. Learning an adaptive
key frame scheduler from data seems an attractive choice.
This is worth further exploration and left as future work.

r =

O(F) + O(S) + O(W) + O(Ntask)
O(Nf eat) + O(Ntask)

,

(5)

4. Network Architectures

where O(·) measures the function complexity.

To understand this ratio, we ﬁrstly note that the com-
plexity of Ntask is usually small. Although its split point
in N is kind of arbitrary, as veriﬁed in experiment, it is
sufﬁcient to keep only one learnable weight layer in Ntask
in our implementation (see Sec. 4). While both Nf eat
and F have considerable complexity (Section 4), we have
O(Ntask) (cid:28) O(Nf eat) and O(Ntask) (cid:28) O(F).

We also have O(W) (cid:28) O(F) and O(S) (cid:28) O(F) be-
cause W and S are very simple. Thus, the ratio in Eq. (5) is
approximated as

r ≈

O(F)
O(Nf eat)

.

It is mostly determined by the complexity ratio of ﬂow
network F and feature network Nf eat, which can be pre-
cisely measured, e.g., by their FLOPs. Table 2 shows its
typical values in our implementation.

Compared to the per-frame approach,

the overall
speedup factor in Algorithm 1 also depends on the spar-
sity of key frames. Let there be one key frame in every l
consecutive frames, the speedup factor is

s =

l
1 + (l − 1) ∗ r

.

Key Frame Scheduling As indicated in Algorithm 1
(line 6) and Eq. (7), a crucial factor for inference speed is

(6)

(7)

The proposed approach is general for different networks
and recognition tasks. Towards a solid evaluation, we adopt
the state-of-the-art architectures and important vision tasks.
Flow Network We adopt the state-of-the-art CNN based
FlowNet architecture (the “Simple” version) [9] as default.
We also designed two variants of lower complexity. The
ﬁrst one, dubbed FlowNet Half, reduces the number of con-
volutional kernels in each layer of FlowNet by half and the
complexity to 1
4 . The second one, dubbed FlowNet Incep-
tion, adopts the Inception structure [43] and reduces the
complexity to 1
8 of that of FlowNet. The architecture de-
tails are reported in Appendix A.

The three ﬂow networks are pre-trained on the synthetic
Flying Chairs dataset in [9]. The output stride is 4. The in-
put image is half-sized. The resolution of ﬂow ﬁeld is there-
fore 1
8 of the original resolution. As the feature stride of the
feature network is 16 (as described below), the ﬂow ﬁeld
and the scale ﬁeld is further down-sized by half using bi-
linear interpolation to match the resolution of feature maps.
This bilinear interpolation is realized as a parameter-free
layer in the network and also differentiated during training.
Feature Network We use ResNet models [16], speciﬁ-
cally, the ResNet-50 and ResNet-101 models pre-trained for
ImageNet classiﬁcation as default. The last 1000-way clas-
siﬁcation layer is discarded. The feature stride is reduced
from 32 to 16 to produce denser feature maps, following the
practice of DeepLab [4, 5] for semantic segmentation, and
R-FCN [8] for object detection. The ﬁrst block of the conv5

layers are modiﬁed to have a stride of 1 instead of 2. The
holing algorithm [4] is applied on all the 3×3 convolutional
kernels in conv5 to keep the ﬁeld of view (dilation=2). A
randomly initialized 3×3 convolution is appended to conv5
to reduce the feature channel dimension to 1024, where the
holing algorithm is also applied (dilation=6). The resulting
1024-dimensional feature maps are the intermediate feature
maps for the subsequent task.

Table 2 presents the complexity ratio Eq. (6) of feature

networks and ﬂow networks.

Semantic Segmentation A randomly initialized 1 × 1
convolutional layer is applied on the intermediate feature
maps to produce (C +1) score maps, where C is the number
of categories and 1 is for background category. A following
softmax layer outputs the per-pixel probabilities. Thus, the
task network only has one learnable weight layer. The over-
all network architecture is similar to DeepLab with large
ﬁeld-of-view in [5].

Object Detection We adopt

the state-of-the-art R-
FCN [8]. On the intermediate feature maps, two branches of
fully convolutional networks are applied on the ﬁrst half and
the second half 512-dimensional of the intermediate feature
maps separately, for sub-tasks of region proposal and detec-
tion, respectively.

In the region proposal branch, the RPN network [35]
is applied. We use na = 9 anchors (3 scales and 3 as-
pect ratios). Two sibling 1 × 1 convolutional layers out-
put the 2na-dimensional objectness scores and the 4na-
dimensional bounding box (bbox) regression values, re-
spectively. Non-maximum suppression (NMS) is applied to
generate 300 region proposals for each image. Intersection-
over-union (IoU) threshold 0.7 is used.

In the detection branch,

two sibling 1 × 1 convolu-
tional layers output the position-sensitive score maps and
bbox regression maps, respectively. They are of dimensions
(C + 1)k2 and 4k2, respectively, where k banks of classi-
ﬁers/regressors are employed to encode the relative position
information. See [8] for details. On the position-sensitive
score/bbox regression maps, position-sensitive ROI pool-
ing is used to obtain the per-region classiﬁcation score and
bbox regression result. No free parameters are involved in
the per-region computation. Finally, NMS is applied on the
scored and regressed region proposals to produce the detec-
tion result, with IoU threshold 0.3.

5. Experiments

Unlike image datasets, large scale video dataset is much
harder to collect and annotate. Our approach is evaluated
on the two recent datasets: Cityscapes [6] for semantic seg-
mentation, and ImageNet VID [37] for object detection.

5.1. Experiment Setup

Cityscapes It is for urban scene understanding and au-
tonomous driving. It contains snippets of street scenes col-
lected from 50 different cities, at a frame rate of 17 fps. The
train, validation, and test sets contain 2975, 500, and 1525
snippets, respectively. Each snippet has 30 frames, where
the 20th frame is annotated with pixel-level ground-truth la-
bels for semantic segmentation. There are 30 semantic cat-
egories. Following the protocol in [5], training is performed
on the train set and evaluation is performed on the validation
set. The semantic segmentation accuracy is measured by the
pixel-level mean intersection-over-union (mIoU) score.

In both training and inference, the images are resized to
have shorter sides of 1024 and 512 pixels for the feature net-
work and the ﬂow network, respectively. In SGD training,
20K iterations are performed on 8 GPUs (each GPU holds
one mini-batch, thus the effective batch size ×8), where the
learning rates are 10−3 and 10−4 for the ﬁrst 15K and the
last 5K iterations, respectively.

ImageNet VID It is for object detection in videos. The
training, validation, and test sets contain 3862, 555, and 937
fully-annotated video snippets, respectively. The frame rate
is 25 or 30 fps for most snippets. There are 30 object cate-
gories, which are a subset of the categories in the ImageNet
DET image dataset2. Following the protocols in [22, 25],
evaluation is performed on the validation set, using the stan-
dard mean average precision (mAP) metric.

In both training and inference, the images are resized to
have shorter sides of 600 pixels and 300 pixels for the fea-
ture network and the ﬂow network, respectively. In SGD
training, 60K iterations are performed on 8 GPUs, where
the learning rates are 10−3 and 10−4 for the ﬁrst 40K and
the last 20K iterations, respectively.

During training, besides the ImageNet VID train set, we
also used the ImageNet DET train set (only the same 30
category labels are used), following the protocols in [22,
25]. Each mini-batch samples images from either ImageNet
VID or ImageNet DET datasets, at 2 : 1 ratio.

5.2. Evaluation Methodology and Results

Deep feature ﬂow is ﬂexible and allows various design
choices. We evaluate their effects comprehensively in the
experiment. For clarify, we ﬁx their default values through-
out the experiments, unless speciﬁed otherwise. For feature
network Nf eat, ResNet-101 model is default. For ﬂow net-
work F, FlowNet (section 4) is default. Key-frame duration
length l is 5 for Cityscapes [6] segmentation and 10 for Im-
ageNet VID [37] detection by default, based on different
frame rate of videos in the datasets..

For each snippet we evaluate l image pairs, (k, i), k =
i − l + 1, ..., i, for each frame i with ground truth anno-

2http://www.image-net.org/challenges/LSVRC/

method

training of image recognition network N training of ﬂow network F

trained on single frames as in Fig. 2 (a)

no ﬂow network used

Frame (oracle baseline)
SFF-slow
SFF-fast
DFF
DFF ﬁx N
DFF ﬁx F
DFF separate

same as Frame

same as Frame

same as DFF

same as Frame

trained on frame pairs as in Fig. 2 (b)

init. on Flying Chairs [9], ﬁne-tuned in Fig. 2 (b)

same as Frame, then ﬁxed in Fig. 2 (b)

same as DFF

SIFT-Flow [26] (w/ best parameters), no training

SIFT-Flow [26] (w/ default parameters), no training

init. on Flying Chairs [9], then ﬁxed in Fig. 2 (b)

init. on Flying Chairs [9]

Table 3. Description of variants of deep feature ﬂow (DFF), shallow feature ﬂow (SFF), and the per-frame approach (Frame).

Cityscapes (l = 5)

ImageNet VID (l = 10)

mIoU(%)

runtime (fps) mAP(%)

runtime (fps)

Methods

Frame
SFF-slow
SFF-fast
DFF
DFF ﬁx N
DFF ﬁx F
DFF separate

71.1
67.8
67.3
69.2
68.8
67.0
66.9

1.52
0.08
0.95
5.60
5.60
5.60
5.60

73.9
70.7
69.7
73.1
72.3
68.8
67.4

4.05
0.26
3.04
20.25
20.25
20.25
20.25

Table 4. Comparison of accuracy and runtime (mostly in GPU) of
various approaches in Table 3. Note that, the runtime for SFF con-
sists of CPU runtime of SIFT-Flow and GPU runtime of Frame,
since SIFT-Flow only has CPU implementation.

tation. Time evaluation is on a workstation with NVIDIA
K40 GPU and Intel Core i7-4790 CPU.

Validation of DFF Architecture We compared DFF

with several baselines and variants, as listed in Table 3.

• Frame: train N on single frames with ground truth.

• SFF: use pre-computed large-displacement ﬂow (e.g.,
SIFT-Flow [26]). SFF-fast and SFF-slow adopt differ-
ent parameters.

• DFF: the proposed approach, N and F are trained
end-to-end. Several variants include DFF ﬁx N (ﬁx
N in training), DFF ﬁx F (ﬁx F in training), and DFF
seperate (N and F are separately trained).

Table 4 summarizes the accuracy and runtime of all ap-
proaches. We ﬁrstly note that the baseline Frame is strong
enough to serve as a reference for comparison. Our im-
plementation resembles the state-of-the-art DeepLab [5] for
semantic segmentation and R-FCN [8] for object detection.
In DeepLab [5], an mIoU score of 69.2% is reported with
DeepLab large ﬁeld-of-view model using ResNet-101 on
Cityscapes validation dataset. Our Frame baseline achieves
slightly higher 71.1%, based on the same ResNet model.

For object detection, Frame baseline has mAP 73.9% us-
ing R-FCN [8] and ResNet-101. As a reference, a com-
parable mAP score of 73.8% is reported in [22], by com-
bining CRAFT [47] and DeepID-net [30] object detectors
trained on the ImageNet data, using both VGG-16 [40] and
GoogleNet-v2 [20] models, with various tricks (multi-scale
training/testing, adding context information, model ensem-
ble). We do not adopt above tricks as they complicate the
comparison and obscure the conclusions.

SFF-fast has a reasonable runtime but accuracy is sig-
niﬁcantly decreased. SFF-slow uses the best parameters for
ﬂow estimation. It is much slower. Its accuracy is slightly
improved but still poor. This indicates that an off-the-shelf
ﬂow may be insufﬁcient.

The proposed DFF approach has the best overall perfor-
mance. Its accuracy is slightly lower than that of Frame and
it is 3.7 and 5.0 times faster for segmentation and detection,
respectively. As expected, the three variants without using
joint training have worse accuracy. Especially, the accuracy
drop by ﬁxing F is signiﬁcant. This indicates a jointing
end-to-end training (especially ﬂow) is crucial.

We also tested another variant of DFF with the scale
function S removed (Algorithm 1, Eq (3), Eq. (4)). The
accuracy drops for both segmentation and detection (less
than one percent). It shows that the scaled modulation of
features is slightly helpful.

Accuracy-Speedup Tradeoff We investigate the trade-
off by varying the ﬂow network F, the feature network
Nf eat, and key frame duration length l. Since Cityscapes
and ImageNet VID datasets have different frame rates, we
tested l = 1, 2, ..., 10 for segmentation and l = 1, 2, ..., 20
for detection.

The results are summarized in Figure 3. Overall, DFF
achieves signiﬁcant speedup with decent accuracy drop. It
smoothly trades in accuracy for speed and ﬁts different
application needs ﬂexibly. For example, in detection, it
improves 4.05 fps of ResNet-101 Frame to 41.26 fps of
ResNet-101 + FlowNet Inception. The 10× faster speed is
at the cost of moderate accuracy drop from 73.9% to 69.5%.
In segmentation, it improves 2.24 fps of ResNet-50 Frame

# layers in Ntask

Cityscapes (l=5)

ImageNet VID (l=10)

mIoU(%)

runtime (fps) mAP(%)

runtime (fps)

21
12
5
1 (default)
0

69.1
69.1
69.2
69.2
69.5

2.87
3.14
3.89
5.60
5.61

73.2
73.3
73.2
73.1
72.7

7.23
8.04
9.99
20.25
20.40

Table 5. Results of using different split points for Ntask.

to 17.48 fps of ResNet-50 FlowNet Inception, at the cost of
accuracy drop from 69.7% to 62.4%.

What ﬂow F should we use? From Figure 3, the smallest
FlowNet Inception is advantageous. It is faster than its two
counterparts at the same accuracy level, most of the times.
What feature Nf eat should we use? In high-accuracy
zone, an accurate model ResNet-101 is clearly better than
ResNet-50. In high-speed zone, the conclusions are differ-
ent on the two tasks. For detection, ResNet-101 is still ad-
vantageous. For segmentation, the performance curves in-
tersect at around 6.35 fps point. For higher speed, ResNet-
50 becomes better than ResNet-101. The seemingly dif-
ferent conclusions can be partially attributed to the differ-
ent video frame rates, the extents of dynamics on the two
datasets. The Cityscapes dataset not only has a low frame
rate 17 fps, but also more quick dynamics. It would be hard
to utilize temporal redundancy for a long propagation. To
achieve the same high speed, ResNet-101 needs a larger key
frame length l than ResNet-50. This in turn signiﬁcantly
increases the difﬁculty of learning.

Above observations provide useful recommendations for
practical applications. Yet, they are more heuristic than gen-
eral, as they are observed only on the two tasks, on limited
data. We plan to explore the design space more in the future.
Split point of Ntask Where should we split Ntask in N ?
Recall that the default Ntask keeps one layer with learning
weight (the 1 × 1 conv over 1024-d feature maps, see Sec-
tion 4). Before this is the 3 × 3 conv layer that reduces di-
mension to 1024. Before this is series of “Bottleneck” unit
in ResNet [16], each consisting of 3 layers. We back move
the split point to make different Ntasks with 5, 12, and 21
layers, respectively. The one with 5 layers adds the dimen-
sion reduction layer and one bottleneck unit (conv5c). The
one with 12 layers adds two more units (conv5a and conv5b)
at the beginning of conv5. The one with 21 layers adds three
more units in conv4. We also move the only layer in default
Ntask into Nf eat, leaving Ntask with 0 layer (with learn-
able weights). This is equivalent to directly propagate the
parameter-free score maps, in both semantic segmentation
and object detection.

Table 5 summarizes the results. Overall, the accuracy
variation is small enough to be neglected. The speed be-

Figure 3. (better viewed in color) Illustration of accuracy-speed
tradeoff under different implementation choices on ImageNet VID
detection (top) and on Cityscapes segmentation (bottom).

comes lower when Ntask has more layers. Using 0 layer
is mostly equivalent to using 1 layer, in both accuracy and
speed. We choose 1 layer as default as that leaves some tun-
able parameters after the feature propagation, which could
be more general.

Example results of the proposed method are presented
in Figure 4 and Figure 5 for video segmentation on
CityScapes and video detection on ImageNet VID, respec-
tively. More example results are available at https:
//www.youtube.com/watch?v=J0rMHE6ehGw.

6. Future Work

Several important aspects are left for further exploration.
It would be interesting to exploit how the joint learning af-
fects the ﬂow quality. We are unable to evaluate as there
lacks ground truth. Current optical ﬂow works are also lim-
ited to either synthetic data [9] or small real datasets, which
is insufﬁcient for deep learning.

Our method can further beneﬁt from improvements in
ﬂow estimation and key frame scheduling. In this paper, we
adopt FlowNet [9] mainly because there are few choices.
Designing faster and more accurate ﬂow network will cer-

Figure 4. Semantic segmentation results on Cityscapes validation dataset. The ﬁrst column corresponds to the images and the results on
the key frame (the kth frame). The following four columns correspond to the k + 1st, k + 2nd, k + 3rd and k + 4th frames, respectively.

Figure 5. Object detection results on ImageNet VID validation dataset. The ﬁrst column corresponds to the images and the results on the
key frame (the kth frame). The following four columns correspond to the k + 2nd, k + 4th, k + 6th and k + 8th frames, respectively.

layer

type

stride

# output

conv1
conv2
conv3
conv3 1
conv4
conv4 1
conv5
conv5 1
conv6
conv6 1

conv1
conv2
conv3
conv3 1
conv4
conv4 1
conv5
conv5 1
conv6
conv6 1

7x7 conv
5x5 conv
5x5 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv

7x7 conv
5x5 conv
5x5 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv
3x3 conv

2
2
2

2

2

2

2
2
2

2

2

2

64
128
256
256
512
512
512
512
1024
1024

32
64
128
128
256
256
256
256
512
512

Table 6. The FlowNet network architecture.

layer

type

stride

# output

Table 7. The FlowNet Half network architecture.

tainly receive more attention in the future. For key frame
scheduling, a good scheduler may well signiﬁcantly im-
prove both speed and accuracy. And this problem is deﬁ-
nitely worth further exploration.

We believe this work opens many new possibilities. We

hope it will inspire more future work.

A. FlowNet Inception Architecture

The architectures of FlowNet, FlowNet Half follow that
of [9] (the “Simple” version), which are detailed in Table 6
and Table 7, respectively. The architecture of FlowNet In-
ception follows the design of the Inception structure [43],
which is detailed in Table 8.

References

[1] M. Bai, W. Luo, K. Kundu, and R. Urtasun. Exploiting se-
mantic information and deep matching for optical ﬂow. In

ECCV, 2016. 2

[2] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical ﬂow estimation based on a theory for warping.
In ECCV, 2004. 2

[3] T. Brox and J. Malik. Large displacement optical ﬂow: De-
scriptor matching in variational motion estimation. TPAMI,
2011. 2

[4] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-
volutional nets and fully connected crfs. In ICLR, 2015. 1,
2, 3, 5, 6

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. arXiv preprint, 2016. 5, 6, 7

[6] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In CVPR, 2016. 1, 5, 6

[7] M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect:
Training deep neural networks with binary weights during
propagations. In NIPS, 2015. 2

[8] J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via
region-based fully convolutional networks. In NIPS, 2016.
1, 2, 3, 5, 6, 7

[9] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
and V. Golkov. Flownet: Learning optical ﬂow with convo-
lutional networks. In ICCV, 2015. 2, 3, 4, 5, 7, 8, 11
[10] M. Fayyaz, M. H. Saffar, M. Sabokrou, M. Fathy, and
R. Klette. STFCN: spatio-temporal FCN for semantic video
segmentation. arXiv preprint, 2016. 2

[11] F. Galasso, N. Shankar Nagaraja, T. Jimenez Cardenas,
T. Brox, and B. Schiele. A uniﬁed video segmentation bench-
mark: Annotation, metrics and analysis. In ICCV, 2013. 5

[12] R. Girshick. Fast R-CNN. In ICCV, 2015. 1, 2, 3
[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014. 1, 2, 3

[14] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling
in deep convolutional networks for visual recognition.
In
ECCV, 2014. 1, 2, 3

[15] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In ICCV, 2015. 2

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 2, 3, 5, 8

[17] B. K. Horn and B. G. Schunck. Determining optical ﬂow.

Artiﬁcial intelligence, 1981. 1, 2

[18] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Y. Bengio. Quantized Neural Networks: Training Neu-
ral Networks with Low Precision Weights and Activations.
arXiv preprint, 2016. 2

[19] J. Hur and S. Roth. Joint optical ﬂow and temporally consis-
tent semantic segmentation. In ECCV CVRSUAD Workshop,
2016. 2

[20] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 2, 7

type

stride

# output

#1x1

#1x1-#3x3

#1x1-#3x3-#3x3

#pool

Inception/Reduction

layer

conv1
pool1
conv2
conv3 1
conv3 2
conv3 3
conv4 1
conv4 2
conv5 1
conv5 2
conv6 1
conv6 2

7x7 conv
3x3 max pool
Inception
3x3 conv
Inception
Inception
Reduction
Inception
Reduction
Inception
Reduction
Inception

2
2

2

2

2

2

32
32
64
128
128
128
256
256
384
384
512
512

24-32

24-32-32

48
48
32
96
48
144
64
192

32-64
48-64
112-128
112-128
96-192
96-192
192-256
192-256

8-16-16
12-16-16
28-32-32
28-32-32
36-48-48
36-48-48
48-64-64
48-64-64

64

96

128

Table 8. The FlowNet Inception network architecture, following the design of the Inception structure [43]. ”Inception/Reduction” modules
consist of four branches: 1x1 conv (#1x1), 1x1 conv-3x3 conv (#1x1-#3x3), 1x1 conv-3x3 conv-3x3 conv (#1x1-#3x3-#3x3), and 3x3 max
pooling followed by 1x1 conv (#pool, only for stride=2).

[21] D. Jayaraman and K. Grauman. Slow and steady feature
analysis: higher order temporal coherence in video.
In
CVPR, 2016. 1, 3, 4

[22] K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang,
Z. Wang, R. Wang, and X. Wang. T-cnn: Tubelets with con-
volutional neural networks for object detection from videos.
In CVPR, 2016. 2, 6, 7

[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1, 2

Imagenet
In

[24] A. Kundu, V. Vineet, and V. Koltun. Feature space optimiza-
tion for semantic video segmentation. In CVPR, 2016. 2
[25] B. Lee, E. Erdenee, S. Jin, and P. K. Rhee. Multi-class
multi-object tracking using changing point detection. arXiv
preprint, 2016. 6

[26] C. Liu, J. Yuen, A. Torralba, J. Sivic, and W. T. Freeman.
Sift ﬂow: dense correspondence across difference scenes. In
ECCV, 2008. 3, 4, 7

[27] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed.

Ssd: Single shot multibox detector. 2016. 1

[28] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, 2015. 1, 2, 3
[29] P. K. Nathan Silberman, Derek Hoiem and R. Fergus. Indoor
segmentation and support inference from rgbd images.
In
ECCV, 2012. 5

[30] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y. Tian, H. Li,
S. Yang, Z. Wang, and C.-C. Loy. Deepid-net: Deformable
deep convolutional neural networks for object detection. In
CVPR, 2015. 7

[31] V. Patraucean, A. Handa, and R. Cipolla. Spatio-temporal
arXiv

video autoencoder with differentiable memory.
preprint arXiv:1511.06309, 2015. 2

[32] T. Pﬁster, J. Charles, and A. Zisserman. Flowing convnets
for human pose estimation in videos. In ICCV, 2015. 2

[33] A. Ranjan and M. J. Black. Optical ﬂow estimation using a

spatial pyramid network. arXiv preprint, 2016. 2

[34] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. arXiv preprint, 2016. 2

[35] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-
wards real-time object detection with region proposal net-
works. In NIPS, 2015. 1, 2, 3, 6

[36] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
EpicFlow: Edge-Preserving Interpolation of Correspon-
dences for Optical Flow. In CVPR, 2015. 2

[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. IJCV, 2015. 1, 6

[38] L. Sevilla-Lara, D. Sun, V. Jampani, and M. J. Black. Optical
In
ﬂow with semantic segmentation and localized layers.
CVPR, 2016. 2

[39] E. Shelhamer, K. Rakelly, J. Hoffman, and T. Darrell. Clock-
work convnets for video semantic segmentation. In ECCV,
2016. 3, 4

[40] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
1, 2, 3, 7

[41] L. Sun, K. Jia, T.-H. Chan, Y. Fang, G. Wang, and S. Yan. Dl-
sfa: deeply-learned slow feature analysis for action recogni-
tion. In CVPR, 2014. 3

[42] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 1, 2, 3
[43] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR, 2016. 5, 11, 12

[44] J. Weickert, A. Bruhn, T. Brox, and N. Papenberg. A survey
on variational optic ﬂow methods for small displacements.
In Mathematical models for registration and applications to
medical imaging. 2006. 2

[45] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid.
DeepFlow: Large displacement optical ﬂow with deep
matching. In CVPR, 2013. 2

[46] L. Wiskott and T. J. Sejnowski. Slow feature analysis: Unsu-
pervised learning of invariances. Neural computation, 2002.
1, 3

[47] B. Yang, J. Yan, Z. Lei, and S. Z. Li. Craft objects from

images. In CVPR, 2016. 7

[48] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV, 2014. 1, 3

[49] W. Zhang, P. Srinivasan, and J. Shi. Discriminative image

warping with attribute ﬂow. In CVPR, 2011. 2

[50] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very
deep convolutional networks for classiﬁcation and detection.
TPAMI, 2015. 2

[51] Z. Zhang and D. Tao. Slow feature analysis for human action

recognition. TPAMI, 2012. 3

[52] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. Torr. Conditional random
ﬁelds as recurrent neural networks. In ICCV, 2015. 1, 2, 3

[53] W. Zou, S. Zhu, K. Yu, and A. Y. Ng. Deep learning of
invariant features via simulated ﬁxations in video. In NIPS,
2012. 1, 3

