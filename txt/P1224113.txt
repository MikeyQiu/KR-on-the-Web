Incorporating Relation Paths in Neural Relation Extraction

Wenyuan Zeng1, Yankai Lin2, Zhiyuan Liu2∗, Maosong Sun2
1Department of Physics, Tsinghua University, Beijing, China
2State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing, China

7
1
0
2
 
p
e
S
 
3
1
 
 
]
L
C
.
s
c
[
 
 
3
v
9
7
4
7
0
.
9
0
6
1
:
v
i
X
r
a

Abstract

Distantly supervised relation extraction
has been widely used to ﬁnd novel rela-
tional facts from plain text. To predict
the relation between a pair of two target
entities, existing methods solely rely on
those direct sentences containing both en-
In fact, there are also many sen-
tities.
tences containing only one of the target
entities, which also provide rich useful in-
formation but not yet employed by rela-
tion extraction. To address this issue, we
build inference chains between two target
entities via intermediate entities, and pro-
pose a path-based neural relation extrac-
tion model to encode the relational seman-
tics from both direct sentences and infer-
ence chains. Experimental results on real-
world datasets show that, our model can
make full use of those sentences contain-
ing only one target entity, and achieves
signiﬁcant and consistent improvements
on relation extraction as compared with
strong baselines. The source code of this
paper can be obtained from https://
github.com/thunlp/PathNRE.

1

Introduction

Knowledge Bases

(KBs) provide effective
structured information for real world facts and
have been used as crucial resources for several nat-
ural language processing (NLP) applications such
as Web search and question answering. Typical
KBs such as Freebase (Bollacker et al., 2008), DB-
pedia (Auer et al., 2007) and YAGO (Suchanek
et al., 2007) usually describe knowledge as multi-
relational data and represent them as triple facts.
As the real-world facts are inﬁnite and increasing

∗∗Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn).

every day, existing KBs are still far from com-
plete. Recently, petabytes of natural-language text
containing thousands of different structure types
are readily available, which is an important re-
source for automatically ﬁnding unknown rela-
tional facts. Hence, relation extraction (RE), de-
ﬁned as the task of extracting structured informa-
tion from plain text, has attracted much interest.

Most existing supervised RE systems usually
suffer from the issue that lacks sufﬁcient labelled
relation-speciﬁc training data. Manual annotation
is very time consuming and labor intensive. One
promising approach to address this limitation is
distant supervision.
(Mintz et al., 2009) gener-
ates training data automatically by aligning a KB
with plain text. They assume that if two target
entities have a relation in KB, then all sentences
that contain these two entities will express this
relation and can be regarded as a positive train-
ing instance. Since neural models have been ver-
iﬁed to be effective for classifying relations from
plain text (Socher et al., 2012; Zeng et al., 2014;
dos Santos et al., 2015), (Zeng et al., 2015; Lin
et al., 2016) incorporate neural networks method
with distant supervision relation extraction. Fur-
ther, (Ye et al., 2016) considers ﬁner-grained in-
formation, and achieves the state-of-the-art perfor-
mance.

Although existing RE systems have achieved
promising results with the help of distant super-
vision and neural models, they still suffer from a
major drawback: the models only learn from those
sentences contain both two target entities. How-
ever, those sentences containing only one of the
entities could also provide useful information and
help build inference chains. For example, if we
know that “h is the father of e” and “e is the father
of t”, we can infer that h is the grandfather of t.

In this work, as illustrated in Fig. 1, we intro-
duce a path-based neural relation extraction model

Figure 1: The architecture of our neural relation extraction model with relation paths.

with relation paths. First, we employ convolu-
tional neural networks (CNN) to embed the se-
mantics of sentences. Afterward, we build a rela-
tion path encoder, which measures the probability
of relations given an inference chain in the text.
Finally, we combine information from direct sen-
tences and relation paths to predict the relation.

We evaluate our model on a real-world dataset
for relation extraction. The experimental results
show that our model achieves signiﬁcant and con-
sistent improvements as compared with baselines.
Besides, with the help of those sentences contain-
ing one of the target entities, our model is more
robust and performs well even when the number
of noisy instances increases. To the best of our
knowledge, this is the ﬁrst effort to consider the
information of relation path in plain text for neu-
ral relation extraction.

2 Related Work

2.1 Distant Supervision

Distant supervision for RE is originally pro-
posed in (Craven et al., 1999). They focus on
extracting binary relations between proteins us-
ing a protein KB as the source of distant supervi-
sion. Afterward, (Mintz et al., 2009) aligns plain
text with Freebase, by using distant supervision
. However, most of these methods heuristically
transform distant supervision to traditional super-
vised learning, by regarding it as a single-instance
single-label problem, while in reality, one instance
could correspond with multiple labels in differ-
ent scenarios and vice versa. To alleviate the is-

sue, (Riedel et al., 2010) regards each sentence as
a training instance and allows multiple instances
to share the same label but disallows more than
one label. Further, (Hoffmann et al., 2011; Sur-
deanu et al., 2012) adopt multi-instance multi-
label learning in relation extraction. The main
drawback of these methods is that they obtain most
features directly from NLP tools with inevitable
errors, and these errors will propagate to the rela-
tion extraction system and limit the performance.

2.2 Neural Relation Extraction

Recently, deep learning (Bengio, 2009) has
been successfully applied in various areas, includ-
ing computer vision, speech recognition and so on.
Meanwhile, its effectiveness has also been veriﬁed
in many NLP tasks such as sentiment analysis (dos
Santos and Gatti, 2014), parsing (Socher et al.,
2013), summarization (Rush et al., 2015) and ma-
chine translation (Sutskever et al., 2014). With
the advances of deep learning, there are grow-
ing works that design neural networks for rela-
tion extraction. (Socher et al., 2012) uses a recur-
sive neural network in relation extraction, and (Xu
et al., 2015; Miwa and Bansal, 2016) further use
LSTM. (Zeng et al., 2014; dos Santos et al., 2015)
adopt CNN in this task, and (Zeng et al., 2015;
Lin et al., 2016) combine attention-based multi-
instance learning which shows promising results.
However, these above models merely learn from
those sentences which directly contain both two
target entities. The important information of those
relation paths hidden in the text is ignored. In this
paper, we propose a novel path-based neural RE

model to address this issue. Besides, although we
choose CNN to test the effectiveness of our model,
other neural models could also be easily adapted to
our architecture.

2.3 Relation Path Modeling

Relation paths have been taken into consider-
ation on large-scale KBs for relation inference.
Path Ranking algorithm (PRA) (Lao and Co-
hen, 2010) has been adopted for expert ﬁnding
(Lao and Cohen, 2010), information retrieval (Lao
et al., 2012), and further for relation classiﬁcation
based on KB structure (Lao et al., 2011; Gardner
et al., 2013). (Neelakantan et al., 2015; Lin et al.,
2015; Das et al., 2016; Wu et al., 2016) use recur-
rent neural networks (RNN) to represent relation
paths based on all involved relations in KBs.(Guu
et al., 2015) proposes an embedding-based com-
positional training method to connect the triple
knowledge for KB completion. Different from the
above work of modeling relation paths in KBs, our
model aims to utilize relation paths in text corpus,
and help to extract knowledge directly from plain
text.

3 Our Method

Given a pair of target entities, a set of corre-
sponding direct sentences S = {s1, s2, · · · , sn}
which contains this entity pair, and a set of rela-
tion paths P = {p1, p2, · · · , pm}, our model aims
to measure the conﬁdence of each relation for this
entity pair. In this section, we will introduce our
model in three parts: (1) Text Encoder. Given
the sentence with two corresponding target enti-
ties, we use a CNN to embed the sentence into
a semantic space, and measure the probability of
(2) Relation
each relation given this sentence.
Path Encoder. Given a relation path between the
target entities, we measure the probability of each
relation r, conditioned on the relation path.
(3)
Joint Model. We integrate the information from
both direct sentences and relation paths, then pre-
dict the conﬁdence of each relation.

3.1 Text Encoder

As shown in Fig. 2, we use a CNN to extract in-
formation from text. Given a set of sentences of an
entity pair, we ﬁrst transform each sentence s into
its distributed representation s, and then predict re-
lation using the most representative sentence via a
multi-instance learning mechanism.

Figure 2: The architecture of CNN used for text
encoder.

3.1.1

Input Vector

First, we transform the words {w1, w2, · · · , wl}
in sentence s into vectors of dimension d. For each
word wi, we use word embedding to encode its
syntactic and semantic meanings, and use position
embedding to encode its position information. We
then concatenate both word embedding and posi-
tion embedding to form the input vector of wi for
CNN. (See Figure 2.)

3.1.2 Convolution and Max-pooling Layers

When processing a sentence, it is a great chal-
lenge that important information could probably
appear in all parts of that sentence. In addition,
the length l of a sentence could also vary a lot.
Therefore, we apply CNN to encode all local fea-
tures regardless sentence length. We ﬁrst apply a
convolution layer to extract all possible local fea-
tures, and then select the most important one via
max-pooling layer.

To extract local features, the convolution layer
ﬁrst concatenates a sequence of word embeddings
within a sliding window to be vector qi of dimen-
sion k × d:

qi = w[i−k+1:i](1 ≤ i ≤ l + k − 1),

(1)

where k is the size of the window, and we
also set all out-of-index words to be zero vec-
It then multiplies qi by a convolution ma-
tors.
trix W ∈ Rdc×(k×d), where dc is the dimen-
sion of sentence embeddings. Hence, the out-
put of convolution layer could be expressed as
h = {h1, h2, · · · , hl+k−1}:

hi = Wqi + b,

(2)

where b is a bias vector. Finally, the max-pooling
layer takes a max operation, followed by a hyper-
bolic tangent activation, over the sequence of hi to
select the most important information, namely,

[s]j = tanh(max

[hi]j).

(3)

i

3.1.3 Multi-Instance Learning

Next, we apply a softmax classiﬁer upon the
sentence representation s to predict the corre-
sponding relation. We deﬁne the condition proba-
bility of relation r as follows,

p(r|θ, s) =

exp(er)
i=1 exp(ei)

,

(cid:80)nr

(4)

where ei, a component of e, measures how well
this sentence matches relation ri, and nr is the
number of relations. More speciﬁcally, e could be
calculated from:

e = Us + v,

(5)

where U ∈ Rnr×dc is the coefﬁcient matrix of
relations and v ∈ Rnr is a bias vector.

We use multi-instance learning to alleviate the
wrong-labeling issue in distant supervision, by
choosing one sentence in the set of all direct sen-
tences S = {s1, s2, · · · , sm} which corresponds
to the entity pair (h, t). Similar to (Zeng et al.,
2015), we deﬁne the score function of this entity
pair and its corresponding relation r as a max-one
setting:

E(h, r, t|S) = max

p(r|θ, si).

(6)

i

where E reﬂects the direct information we derive
from sentences. We can also set a random setting
as a baseline:

E(h, r, t|S) = p(r|θ, si),

(7)

where si is randomly selected from S.

3.2 Relation Path Encoder

We use Relation Path Encoder to embed the in-
ference information of relation paths. Relation
Path Encoder measures the probability of each re-
lation r given a relation path in the text. This will
utilize the inference chain structure to help make
predictions. More speciﬁcally, we deﬁne a path
p1 between (h, t) as {(h, e), (e, t)}, and the corre-
sponding relations are rA, rB. Each of (h, e) and
(e, t) corresponds to at least one sentence in the

text. Our model calculates the probability of rela-
tion r conditioned on p1 as follows,

p(r|rA, rB) =

exp(or)
i=1 exp(oi)

,

(cid:80)nr

(8)

where oi measures how well relation r matches
with the relation path (rA, rB).
Inspired by the
work on relation path representation learning (Lin
et al., 2015), our model ﬁrst transforms relation
r to its distributed representation, i.e. vector r ∈
RdR, and builds the path embeddings by composi-
tion of relation embeddings. Then, the similarity
oi is calculated as follows:

oi = −(cid:107)ri − (rA + rB)(cid:107)L1.

(9)

Therefore, if ri gets more similar to (rA + rB),
the conditioned predicting probability of ri will
become larger. Here, we make an implicit assump-
tion that if ri is semantically similar to relation
rB−→ t, the embedding ri will be
path pi : h
closer to the relation path embedding (rA + rB).
rB−→ t,
rA−→ e
Finally, for this relation path pi : h
we deﬁne an relation-path score function,

rA−→ e

G(h, r, t|pi) = E(h, rA, e)E(e, rB, t)p(r|rA, rB),
(10)
where E(h, rA, e) and E(e, rB, t) measure the
probabilities of relational facts (h, rA, e) and
(e, rB, t) from text, and p(r|rA, rB) measures
the probability of relation r given relation path
(rA, rB).

In reality, there are usually multiple relation
paths between two entities. Hence, we deﬁne the
inferring correlation between relation r and sev-
eral sentence paths P as,

G(h, r, t|P ) = max

G(h, r, t|pi),

(11)

i

where we use max operation to ﬁlter out those
noisy paths and select the most representative
path.

3.3

Joint Model

Given any entity pair (h, t), those sentences S
directly mentioning them and relation paths P be-
tween them, we deﬁne the global score function
with respect to a candidate relation r as,

L(h, r, t) = E(h, r, t|S) + αG(h, r, t|P ),

(12)

where E(h, r, t|S) models the correlation between
r and (h, t) calculated from direct sentences,

G(h, r, t|P ) models the inferring correlation be-
tween relation r and several sentence paths P . α
equals to (1 − E(h, r, t|S)) times a constant β.
This term serves to depict the relative weight be-
tween direct sentences and relation paths, since we
don’t need to pay much attention on extra infor-
mation when CNN has already given a conﬁdent
prediction, namely E(h, r, t|S) is large.

One of the advantages of this joint model is to
alleviate the issue of error propagation. The un-
certainty of information from Text Encoder and
Relation Path encoder is characterized by its con-
ﬁdence, and could be integrated and corrected in
this joint model step. Furthermore, since we treat
relation paths in a probabilistic way, our model
could fully utilize all relation paths, i.e. those al-
ways hold and those likely to hold.

3.4 Optimization and Implementation Details

The overall objective function is deﬁned as:

J(θ) =

log(L(h, r, t)),

(13)

(cid:88)

(h,r,t)

where the summing runs over the log loss of all
entity pairs in text and θ represents the model pa-
rameters. To solve this optimization problem, we
use mini-batch stochastic gradient descent (SGD)
to maximize our objective function. We initial-
ize WE with the results from Skip-gram model,
and initialize other parameters randomly. We also
adopt dropout (Srivastava et al., 2014) upon the
output layer of CNN.

We implement our model using C++. We train
our model on Intel(R) Xeon(R) CPU E5-2620, and
the training roughly takes half a day. The word
embedding and other parameters are updated via
back-propagation simultaneously, while the rela-
tion path structure is extracted before training and
stored afterward.

4 Dataset

Snapshot) with New York Times corpus (NYT).
There are 53 possible relationships between two
entities,
including a special relation type NA,
meaning that there is no relation between head and
tail entities. For each relational fact in a ﬁltered
Freebase dataset, a sentence from NYT would be
regarded as a mention of this relation if both the
head and tail entity appear in that sentence.

While this previous dataset has been frequently
used for evaluating relation extraction systems, we
observe some limitations of it. First, the relational
facts are extracted from a 2009 snapshot of Free-
base. Therefore, this dataset is too old to contain
many updated facts. This will underestimate the
performance of a relation extraction system, since
some real-world facts are missing from the dataset
and labeled as NA. Second, the relational facts in
this dataset are scattered, i.e.
there are not sufﬁ-
cient relation paths in this dataset, while relational
facts in real-world always have connections with
each other. Third, Freebase will no longer update
after 2016.These limitations mean that this dataset
is somewhat improper for evaluating RE systems.
Although other relation extraction datasets ex-
ist, e.g. ACE1 and (Hendrickx et al., 2009), they
are too small to train an effective neural relation
extraction model. Moreover, each relational fact
in (Hendrickx et al., 2009) only corresponds with
one sentence, which prevents it from evaluating
multi-instance relation extraction systems. Hence,
we constructed a novel relation extraction dataset
to address these issues, and will make it available
to the community.

4.2 Dataset Construction

Datasets

Riedel et.al.

Ours

Sets
Train
Valid
Test
Train
Valid
Test

# sentences
522,611
-
172,448
647,827
234,350
235,609

# entity pairs
281,270
-
96,678
266,118
121,160
121,837

# facts
18,252
-
1,950
50,031
5,609
5,756

Table 1: Statistics of datasets.

We build a novel dataset for evaluating relation
extraction task. We ﬁrst describe the most com-
monly used previous dataset and then explain the
reason and how we construct the new dataset.

4.1 Previous Datasets & Reasons for New

Dataset

Our dataset contains more updated facts and
richer structures of relations, e.g. more relations
/ relation paths, as compared to existing similar
datasets. The dataset is expected to be more simi-
lar to real-world cases, and thus be more appropri-
ate for evaluating RE systems’ performances.

We build the dataset by aligning Wikidata2 re-

A commonly used benchmark dataset for this
task was developed by (Riedel et al., 2010). This
dataset was built by aligning Freebase (Dec. 2009

1https://catalog.ldc.upenn.edu/

LDC2006T06

2https://www.wikidata.org/

lations with the New York Times Corpus (NYT).
Wikidata is a large, growing knowledge base,
which contains more than 80 million triple facts
and 20 million entities. Different from Freebase,
Wikidata is still in maintenance and could be eas-
ily accessed by APIs. We ﬁrst pick those en-
tities simultaneously appeared in both Wikidata
and Freebase, and relational facts associated with
them. Then, we ﬁltered out a subset S, reserv-
ing those facts associating with the 99 highest fre-
quency relations. This results in 4, 574, 665 triples
with 1, 045, 385 entities and 99 relations.

Next, we align those facts with NYT corpus,
following the assumption of distant supervision.
For each pair of entities appearing in our S, we
traverse the corpus and pick those sentences where
both entities appear. These sentences will be re-
garded as mentions of this fact, and labeled by
this relation type. To simulate noise in the real
world, we also add sentences corresponding to
“No Relation” entity pairs into our dataset. To
get those “No Relation” instances, we ﬁrst cre-
ate a fake knowledge base S− by randomly re-
placing the head or tail entities in triples, i.e.,
S− = {(h(cid:48), r, t)}∪{(h, r, t(cid:48))} and then align them
with NYT corpus. Finally, we randomly split all
those selected sentences into training, validation
and testing set, assuring that a relational fact could
be only mentioned by sentences in one set. The
statistics of our dataset and (Riedel et al., 2010)
are listed in Table 1.

5 Experiments

Following the previous work (Mintz et al.,
2009), we evaluate our model by extracting rela-
tional facts from the sentences in test set, and com-
pare them with those in Wikidata. We report Pre-
cision/Recall curves, Precision@N (P@N) and F1
scores for comparison in our experiments.

5.1

Initialization and Parameter Settings
In this paper, we use the word2vec tool 3
to pre-train word embeddings on NYT corpus.
We keep the words which appear more than 100
times in the corpus as vocabulary. We tune our
model on the validation set, using grid search
to determine the optimal parameters, which are
shown in boldface. We select learning rate for
SGD λ ∈ {0.1, 0.01, 0.001}, the sentence em-
bedding size dc ∈ {50, 60, · · · , 230, · · · , 300},

3https://code.google.com/p/word2vec/

the window size k ∈ {1, 2, 3, 4, 5}, and the
mini-batch size B ∈ {40, 160, 640}.
Be-
sides, we select
the relation embeddings size
dR ∈ {5, 10, · · · , 40, · · · , 60}, and the weight
∈
information from relation paths β
for
{0.01, 0.1, 0.2, 0.5, 1, · · · , 5}. For other parame-
ters which have little effect on the system perfor-
mance, we follow the settings used in (Zeng et al.,
2015): word embedding size dw is 50, position
embedding size dp is 5 and dropout rate p is 0.5.
For training, the iteration number over all training
data is 25.

5.2 Effectiveness of Incorporating Relation

Paths

5.2.1 Precision-Recall Curve Comparison

To demonstrate the effect of our approach, we
empirically compare it with other neural relation
extraction methods via held-out evaluation.
(1)
CNN+rand represents the CNN model reported
in (Zeng et al., 2014). (2) CNN+max represents
the CNN model with multi-instances learning used
(3) Path+rand/max is
in (Zeng et al., 2015).
our model with those two multi-instance settings.
We implement (1), (2) by ourselves which achieve
comparable results as reported in those papers.

Figure 3: Aggregate precision/recall curve for
CNN+rand, CNN+max, Path+rand, Path+max.

Fig. 3 shows the precision/recall curves of all
methods. From the ﬁgure, we can observe that: (1)
Our methods outperform their counterpart meth-
ods, achieving higher precision over almost en-
tire range of recall. They also enhance recall by
20% without decrease of precision. These results
prove the effectiveness of our approach. We no-
tice that the improvements of our methods over

Test Settings (Noise)
P@N (%)
CNN+rand
CNN+max
Path+rand
Path+max

75%
10% 20% 50% F1
57.5
86.7
57.2
86.0
89.4
59.3
59.6
89.0

67.0
68.5
71.7
71.5

38.9
38.3
39.9
39.8

85%
10% 20% 50% F1
55.0
84.6
56.5
85.4
58.1
88.2
59.4
89.0

37.5
37.7
39.0
39.6

66.4
67.6
70.2
71.4

95%
10% 20% 50% F1
51.4
79.9
54.8
84.4
55.6
86.0
59.1
88.6

35.2
36.6
37.0
39.1

61.8
66.0
67.2
71.0

Table 2: P@N and F1 for relation extraction in texts containing different percentage of no-relation facts.

baselines are relatively small at small recall value,
which corresponds to high predicting conﬁdence.
This phenomenon is intuitive since our joint model
could dynamically leverage the importance of di-
rect sentence and relation paths, and tends to trust
the Text Encoder when the conﬁdence is high. (2)
As the recall increases, our models exhibit larger
improvements compared with CNN in terms of
percentage. This is due to the fact that sometimes
CNNs cannot extract reliable information from di-
rect sentences, while our methods could alleviate
this issue by considering more information from
inference chains, and thus still maintain high pre-
cision.
(3) Both CNN+max and Path+rand are
variations of CNN+rand, aiming to alleviate the
problem of noisy data. We see that Path+rand out-
performs CNN+max over all range, which indi-
cates that considering path information is a better
way to solve this issue. Meanwhile, combining
paths information and max operation, Path+max,
gives the best performance. (4) Path+rand shows
a larger improvement over CNN+rand, compared
with those of Path+max and CNN+max. This fur-
thermore proves the effectiveness of considering
relation path information: CNN+rand has much
more severe problem suffering from noise, so us-
ing our method to incorporate paths information to
alleviate this issue could perform better.

5.2.2 Comparison on Long Tail Situation

CNN+rand
Path+rand
CNN+max
Path+max

Ns ≤ 1
53.9
58.4
57.8
63.6 (+5.8)

Ns ≤ 2
54.0
58.1
58.3
62.5 (+4.2)

Ns ≤ 5
52.0
56.0
56.5
60.2 (+3.7)

All
51.4
55.6
55.7
59.1 (+3.4)

Table 3: F1 score for long tail situation.

Real-world data follows long-tail distribution
(power law). In the testing set, we also observe
a fact that about 40% triple facts appear only in
single sentence, and thus a multi-instance rela-
tion extraction system, e.g. CNN+max, could only
rely on limited information and the multi-instance
mechanism will not work well. Our system, on the
contrary, can still utilize information from relation
paths in this case, and is expected to perform much

better in the long tail situation.

We evaluate the models on different parts of the
long-tail distribution. To get testing instances from
different parts of the distribution, we extract all the
triple facts appearing less than Ns sentences in the
testing set, and those sentences associating with
them. All text related to ’No Relation’ entity pair
are also reserved in order to simulate noise. We
then evaluate different models on those sampled
testing set, and report the results of F1 score in
Table 3.

From Table 3, we could observe that: (1) Incor-
porating relation paths is indeed effective in pre-
dicting relations, and our models have signiﬁcant
improvements compared with the baselines.
(2)
Path+max indeed has larger improvements over
CNN+max when Ns is small, which is consistent
with our previous expectation. Also notice that
the gap between Path+rand and CNN+rand is rel-
atively constant. This is due to the fact that both
these methods only use one random sentence, re-
gardless of how many sentences there are associ-
ating with an entity pair.

5.3 Model Robustness under Different

Percentages of Noise

In the task of relation extraction, there are lots
of noise in text which may hurt the model’s per-
formance. More speciﬁcally, “No Relation” entity
pair is a kind of noise, since “No Relation” could
actually contain many unknown relation types,
and thus might confuse the relation extraction sys-
tems. Therefore, it is important to verify the ro-
bustness of our model in the presence of massive
noise. Here, we evaluate those models in three set-
tings, with the same relational facts and different
percentages of “No Relation” sentences in the test-
ing sets. In each experiment, we extract top 20,000
predicting relational facts according to the model’s
predicting scores, and report the precision @top
10%, @top 20%, @top 50% and F1 score in Table
2.

From the table, we can see that: (1) In terms of
all evaluations, our models achieve the best perfor-

Path #1
Path #2
Test
Path #1
Path #2
Test

Relation
mother
has child
spouse
shares border with
shares border with
shares border with

Text
Rebecca gave birth to twin sons, Esau and Jacob, ...
...Isaac’s marriage to Rebecca, by whom he has two sons, Esau and jacob, ...
... Isaac and Rebecca and the female and male evil spirits ...
... in Somalia, ... soldiers and marines stationed in neighboring Djibouti ...
... Ethiopia have had the effect of making neighboring Djibouti ...
The next day, Ethiopia struck, its military pushing deep into Somalia ...

Table 4: Some representative examples of inference chians in NYT corpus. The bold is target entities.

mance as compared with other methods in all test
settings. It demonstrates the effectiveness of our
approach. (2) Even though the scores of all mod-
els drop as the increasing of noise, we ﬁnd that
Path+rand/max’s scores decrease much less than
their counterparts. This result proves the effective-
ness of taking inference chains into consideration.
Since we utilize more information to make predic-
tions, our model is more robust to the presence of
mass noise.

5.4 Effectiveness of Learned Features in

Zero-Shot Scenario

It has been proved that CNN could automat-
ically extract useful features, encoding syntactic
and semantic meaning of sentences. These fea-
tures are sometimes fed to subsequent models to
solve other tasks. In this experiment, we demon-
strate the effectiveness of the extracted features
from our model. Since CNN-based models have
already succeeded in extracting relations from sin-
gle sentences, we set our experiment in a new
scenario: predicting the relation between entities
which have not appeared in the same sentence.

A natural approach is to build a relation path
between this zero-shot entity pair. We assume that
we can make a prediction about (h, t), once we
know the information of (h, e) and (e, t). There-
fore, we build the training set by extracting all
such relation paths and their sentences from train-
ing text, and similar for testing set. To test the
effectiveness of features, we encode sentences by
CNN+rand/max, Path+rand/max respectively, and
then feed the concatenation of sentence vectors to
a logistic classiﬁer.

Feature
CNN+rand
CNN+max
Path+rand
Path+max

Accuracy
56.9
57.3
58.5
60.4

result using CNN+rand features is comparable to
the result using CNN+max features.
It shows
that using max operation to train the features does
not greatly improve the features’ behavior in this
task, even though it performs well in previous
tasks. The reason is that, both CNN+rand and
CNN+max only encode the information from a
single sentence, and they are unable to capture the
correlations between relations.
(2) Feature from
Path+rand/max shows its effectiveness over those
from other methods. It indicates that our method
is able to model the correlations between relations,
while also keeps the syntactic and semantic mean-
ing of a sentence. Therefore, the features extracted
from Path+rand/max are useful for a wider range
of applications, especially in those tasks which
need the information from relations.

5.5 Case study

Table 4 shows some representative inference
chains from the testing dataset. These examples
can not be predicted correctly by the original CNN
model, but are later corrected using our model. We
show the test instances and their correct relations,
as well as the inference chains the model uses. In
the ﬁrst example, the test sentence does not di-
rectly express the relation spouse, the proof of
this relation appears in a further context in NYT.
However, using path#1 and path#2, we could eas-
ily infer that Rebecca and Issac are spouse. The
second example doesn’t show the relation either.
But with the help of intermediate entity, Dijibouti,
our model predicts that Somalia shares the bor-
der with Ethiopia. Note that this inference chain
doesn’t always hold, but our model could capture
this uncertainty well via a softmax operation. In
general, our model can utilize common sense from
inference chains. It helps make correct predictions
even if the inference is not explicit.

Table 5: Accuracy of different models in zero-shot
situation.

From Table 5, we could observe that: (1) The

6 Conclusion and Future Work

In this paper, we propose a neural relation ex-
traction model which encodes the information of

relation paths. As compared to existing neural re-
lation extraction models, our model is able to uti-
lize the sentences which contain both two target
entities and only one target entity and is more ro-
bust for noisy data. Experimental results on real-
world datasets show that our model achieves sig-
niﬁcant and consistent improvements on relation
extraction as compared with baselines.

In the future, we will explore the following di-
rections: (1) We will explore the combination of
relation paths from both plain texts and KBs for
relation extraction. (2) We may take advantages
of probabilistic graphical model or recurrent neu-
ral network to encode more complicated correla-
tions between relation paths, e.g. multi-step rela-
tion paths, for relation extraction.

Acknowledgments

This work is supported by the 973 Program
(No. 2014CB340501), the National Natural Sci-
ence Foundation of China (NSFC No. 61572273,
61661146007), China Association for Science
and Technology (2016QNRC001), and Tsinghua
University Initiative Scientiﬁc Research Program
(20151080406).

References

S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
Springer.

Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and trends R(cid:13) in Machine Learning,
2(1):1–127.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of KDD, pages
1247–1250.

Mark Craven, Johan Kumlien, et al. 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In Proceedings of ISMB,
volume 1999, pages 77–86.

Rajarshi Das, Arvind Neelakantan, David Belanger,
and Andrew McCallum. 2016. Chains of reasoning
over entities, relations, and text using recurrent neu-
ral networks. arXiv preprint arXiv:1607.01426.

Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
Improving learning
and Tom M Mitchell. 2013.
and inference in a large knowledge-base using latent
In Proceedings of EMNLP, pages
syntactic cues.
833–838.

Kelvin Guu, John Miller, and Percy Liang. 2015.
In
Traversing knowledge graphs in vector space.
Proceedings of EMNLP, pages 318–327.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid ´O S´eaghdha, Sebastian
Pad´o, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2009.
Semeval-2010 task 8:
Multi-way classiﬁcation of semantic relations be-
In Proceedings of
tween pairs of nominals.
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions, pages 94–99.
Association for Computational Linguistics.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
In Proceedings of ACL-
of overlapping relations.
HLT, pages 541–550.

Ni Lao and William W Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine learning, 81(1):53–67.

Ni Lao, Tom Mitchell, and William W Cohen. 2011.
Random walk inference and learning in a large scale
knowledge base. In Proceedings of EMNLP, pages
529–539.

Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP-CoNLL, pages 1017–1026.

Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2015.
Modeling relation paths for representation learning
of knowledge bases. Proceedings of EMNLP.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Luan Huanbo,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. Proceedings
of ACL.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of ACL-
IJCNLP, pages 1003–1011.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
In Proceedings of ACL, pages 1105–
structures.
1116.

Arvind Neelakantan, Benjamin Roth, and Andrew Mc-
Callum. 2015. Compositional vector space models
for knowledge base inference. In 2015 AAAI Spring
Symposium Series.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of ECML-PKDD,
pages 148–163.

Alexander M Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. Proceedings of EMNLP.

Cıcero Nogueira dos Santos and Maıra Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In Proceedings of COLING.

Cıcero Nogueira dos Santos, Bing Xiang, and Bowen
Zhou. 2015. Classifying relations by ranking with
In Proceedings of
convolutional neural networks.
ACL, volume 1, pages 626–634.

Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
In Proceedings of ACL.
tional vector grammars.
Citeseer.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of EMNLP-CoNLL, pages 1201–1211.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. JMLR, 15(1):1929–1958.

Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
In Proceedings of WWW, pages 697–706.
edge.
ACM.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of EMNLP, pages 455–465.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of NIPS, pages 3104–3112.

Jiawei Wu, Ruobing Xie, Zhiyuan Liu, and Maosong
Sun. 2016. Knowledge representation via joint
learning of sequential text and knowledge graphs.
arXiv preprint arXiv:1609.07075.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015. Classifying relations via long
short term memory networks along shortest depen-
In Proceedings of EMNLP, pages
dency paths.
1785–1794.

Hai Ye, Wenhan Chao, and Zhunchen Luo. 2016.
Jointly extracting relations with class ties via effec-
tive deep ranking. arXiv preprint arXiv:1612.07602.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
In Pro-
piecewise convolutional neural networks.
ceedings of EMNLP.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classiﬁcation via con-
volutional deep neural network. In Proceedings of
COLING, pages 2335–2344.

Incorporating Relation Paths in Neural Relation Extraction

Wenyuan Zeng1, Yankai Lin2, Zhiyuan Liu2∗, Maosong Sun2
1Department of Physics, Tsinghua University, Beijing, China
2State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing, China

7
1
0
2
 
p
e
S
 
3
1
 
 
]
L
C
.
s
c
[
 
 
3
v
9
7
4
7
0
.
9
0
6
1
:
v
i
X
r
a

Abstract

Distantly supervised relation extraction
has been widely used to ﬁnd novel rela-
tional facts from plain text. To predict
the relation between a pair of two target
entities, existing methods solely rely on
those direct sentences containing both en-
In fact, there are also many sen-
tities.
tences containing only one of the target
entities, which also provide rich useful in-
formation but not yet employed by rela-
tion extraction. To address this issue, we
build inference chains between two target
entities via intermediate entities, and pro-
pose a path-based neural relation extrac-
tion model to encode the relational seman-
tics from both direct sentences and infer-
ence chains. Experimental results on real-
world datasets show that, our model can
make full use of those sentences contain-
ing only one target entity, and achieves
signiﬁcant and consistent improvements
on relation extraction as compared with
strong baselines. The source code of this
paper can be obtained from https://
github.com/thunlp/PathNRE.

1

Introduction

Knowledge Bases

(KBs) provide effective
structured information for real world facts and
have been used as crucial resources for several nat-
ural language processing (NLP) applications such
as Web search and question answering. Typical
KBs such as Freebase (Bollacker et al., 2008), DB-
pedia (Auer et al., 2007) and YAGO (Suchanek
et al., 2007) usually describe knowledge as multi-
relational data and represent them as triple facts.
As the real-world facts are inﬁnite and increasing

∗∗Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn).

every day, existing KBs are still far from com-
plete. Recently, petabytes of natural-language text
containing thousands of different structure types
are readily available, which is an important re-
source for automatically ﬁnding unknown rela-
tional facts. Hence, relation extraction (RE), de-
ﬁned as the task of extracting structured informa-
tion from plain text, has attracted much interest.

Most existing supervised RE systems usually
suffer from the issue that lacks sufﬁcient labelled
relation-speciﬁc training data. Manual annotation
is very time consuming and labor intensive. One
promising approach to address this limitation is
distant supervision.
(Mintz et al., 2009) gener-
ates training data automatically by aligning a KB
with plain text. They assume that if two target
entities have a relation in KB, then all sentences
that contain these two entities will express this
relation and can be regarded as a positive train-
ing instance. Since neural models have been ver-
iﬁed to be effective for classifying relations from
plain text (Socher et al., 2012; Zeng et al., 2014;
dos Santos et al., 2015), (Zeng et al., 2015; Lin
et al., 2016) incorporate neural networks method
with distant supervision relation extraction. Fur-
ther, (Ye et al., 2016) considers ﬁner-grained in-
formation, and achieves the state-of-the-art perfor-
mance.

Although existing RE systems have achieved
promising results with the help of distant super-
vision and neural models, they still suffer from a
major drawback: the models only learn from those
sentences contain both two target entities. How-
ever, those sentences containing only one of the
entities could also provide useful information and
help build inference chains. For example, if we
know that “h is the father of e” and “e is the father
of t”, we can infer that h is the grandfather of t.

In this work, as illustrated in Fig. 1, we intro-
duce a path-based neural relation extraction model

Figure 1: The architecture of our neural relation extraction model with relation paths.

with relation paths. First, we employ convolu-
tional neural networks (CNN) to embed the se-
mantics of sentences. Afterward, we build a rela-
tion path encoder, which measures the probability
of relations given an inference chain in the text.
Finally, we combine information from direct sen-
tences and relation paths to predict the relation.

We evaluate our model on a real-world dataset
for relation extraction. The experimental results
show that our model achieves signiﬁcant and con-
sistent improvements as compared with baselines.
Besides, with the help of those sentences contain-
ing one of the target entities, our model is more
robust and performs well even when the number
of noisy instances increases. To the best of our
knowledge, this is the ﬁrst effort to consider the
information of relation path in plain text for neu-
ral relation extraction.

2 Related Work

2.1 Distant Supervision

Distant supervision for RE is originally pro-
posed in (Craven et al., 1999). They focus on
extracting binary relations between proteins us-
ing a protein KB as the source of distant supervi-
sion. Afterward, (Mintz et al., 2009) aligns plain
text with Freebase, by using distant supervision
. However, most of these methods heuristically
transform distant supervision to traditional super-
vised learning, by regarding it as a single-instance
single-label problem, while in reality, one instance
could correspond with multiple labels in differ-
ent scenarios and vice versa. To alleviate the is-

sue, (Riedel et al., 2010) regards each sentence as
a training instance and allows multiple instances
to share the same label but disallows more than
one label. Further, (Hoffmann et al., 2011; Sur-
deanu et al., 2012) adopt multi-instance multi-
label learning in relation extraction. The main
drawback of these methods is that they obtain most
features directly from NLP tools with inevitable
errors, and these errors will propagate to the rela-
tion extraction system and limit the performance.

2.2 Neural Relation Extraction

Recently, deep learning (Bengio, 2009) has
been successfully applied in various areas, includ-
ing computer vision, speech recognition and so on.
Meanwhile, its effectiveness has also been veriﬁed
in many NLP tasks such as sentiment analysis (dos
Santos and Gatti, 2014), parsing (Socher et al.,
2013), summarization (Rush et al., 2015) and ma-
chine translation (Sutskever et al., 2014). With
the advances of deep learning, there are grow-
ing works that design neural networks for rela-
tion extraction. (Socher et al., 2012) uses a recur-
sive neural network in relation extraction, and (Xu
et al., 2015; Miwa and Bansal, 2016) further use
LSTM. (Zeng et al., 2014; dos Santos et al., 2015)
adopt CNN in this task, and (Zeng et al., 2015;
Lin et al., 2016) combine attention-based multi-
instance learning which shows promising results.
However, these above models merely learn from
those sentences which directly contain both two
target entities. The important information of those
relation paths hidden in the text is ignored. In this
paper, we propose a novel path-based neural RE

model to address this issue. Besides, although we
choose CNN to test the effectiveness of our model,
other neural models could also be easily adapted to
our architecture.

2.3 Relation Path Modeling

Relation paths have been taken into consider-
ation on large-scale KBs for relation inference.
Path Ranking algorithm (PRA) (Lao and Co-
hen, 2010) has been adopted for expert ﬁnding
(Lao and Cohen, 2010), information retrieval (Lao
et al., 2012), and further for relation classiﬁcation
based on KB structure (Lao et al., 2011; Gardner
et al., 2013). (Neelakantan et al., 2015; Lin et al.,
2015; Das et al., 2016; Wu et al., 2016) use recur-
rent neural networks (RNN) to represent relation
paths based on all involved relations in KBs.(Guu
et al., 2015) proposes an embedding-based com-
positional training method to connect the triple
knowledge for KB completion. Different from the
above work of modeling relation paths in KBs, our
model aims to utilize relation paths in text corpus,
and help to extract knowledge directly from plain
text.

3 Our Method

Given a pair of target entities, a set of corre-
sponding direct sentences S = {s1, s2, · · · , sn}
which contains this entity pair, and a set of rela-
tion paths P = {p1, p2, · · · , pm}, our model aims
to measure the conﬁdence of each relation for this
entity pair. In this section, we will introduce our
model in three parts: (1) Text Encoder. Given
the sentence with two corresponding target enti-
ties, we use a CNN to embed the sentence into
a semantic space, and measure the probability of
(2) Relation
each relation given this sentence.
Path Encoder. Given a relation path between the
target entities, we measure the probability of each
relation r, conditioned on the relation path.
(3)
Joint Model. We integrate the information from
both direct sentences and relation paths, then pre-
dict the conﬁdence of each relation.

3.1 Text Encoder

As shown in Fig. 2, we use a CNN to extract in-
formation from text. Given a set of sentences of an
entity pair, we ﬁrst transform each sentence s into
its distributed representation s, and then predict re-
lation using the most representative sentence via a
multi-instance learning mechanism.

Figure 2: The architecture of CNN used for text
encoder.

3.1.1

Input Vector

First, we transform the words {w1, w2, · · · , wl}
in sentence s into vectors of dimension d. For each
word wi, we use word embedding to encode its
syntactic and semantic meanings, and use position
embedding to encode its position information. We
then concatenate both word embedding and posi-
tion embedding to form the input vector of wi for
CNN. (See Figure 2.)

3.1.2 Convolution and Max-pooling Layers

When processing a sentence, it is a great chal-
lenge that important information could probably
appear in all parts of that sentence. In addition,
the length l of a sentence could also vary a lot.
Therefore, we apply CNN to encode all local fea-
tures regardless sentence length. We ﬁrst apply a
convolution layer to extract all possible local fea-
tures, and then select the most important one via
max-pooling layer.

To extract local features, the convolution layer
ﬁrst concatenates a sequence of word embeddings
within a sliding window to be vector qi of dimen-
sion k × d:

qi = w[i−k+1:i](1 ≤ i ≤ l + k − 1),

(1)

where k is the size of the window, and we
also set all out-of-index words to be zero vec-
It then multiplies qi by a convolution ma-
tors.
trix W ∈ Rdc×(k×d), where dc is the dimen-
sion of sentence embeddings. Hence, the out-
put of convolution layer could be expressed as
h = {h1, h2, · · · , hl+k−1}:

hi = Wqi + b,

(2)

where b is a bias vector. Finally, the max-pooling
layer takes a max operation, followed by a hyper-
bolic tangent activation, over the sequence of hi to
select the most important information, namely,

[s]j = tanh(max

[hi]j).

(3)

i

3.1.3 Multi-Instance Learning

Next, we apply a softmax classiﬁer upon the
sentence representation s to predict the corre-
sponding relation. We deﬁne the condition proba-
bility of relation r as follows,

p(r|θ, s) =

exp(er)
i=1 exp(ei)

,

(cid:80)nr

(4)

where ei, a component of e, measures how well
this sentence matches relation ri, and nr is the
number of relations. More speciﬁcally, e could be
calculated from:

e = Us + v,

(5)

where U ∈ Rnr×dc is the coefﬁcient matrix of
relations and v ∈ Rnr is a bias vector.

We use multi-instance learning to alleviate the
wrong-labeling issue in distant supervision, by
choosing one sentence in the set of all direct sen-
tences S = {s1, s2, · · · , sm} which corresponds
to the entity pair (h, t). Similar to (Zeng et al.,
2015), we deﬁne the score function of this entity
pair and its corresponding relation r as a max-one
setting:

E(h, r, t|S) = max

p(r|θ, si).

(6)

i

where E reﬂects the direct information we derive
from sentences. We can also set a random setting
as a baseline:

E(h, r, t|S) = p(r|θ, si),

(7)

where si is randomly selected from S.

3.2 Relation Path Encoder

We use Relation Path Encoder to embed the in-
ference information of relation paths. Relation
Path Encoder measures the probability of each re-
lation r given a relation path in the text. This will
utilize the inference chain structure to help make
predictions. More speciﬁcally, we deﬁne a path
p1 between (h, t) as {(h, e), (e, t)}, and the corre-
sponding relations are rA, rB. Each of (h, e) and
(e, t) corresponds to at least one sentence in the

text. Our model calculates the probability of rela-
tion r conditioned on p1 as follows,

p(r|rA, rB) =

exp(or)
i=1 exp(oi)

,

(cid:80)nr

(8)

where oi measures how well relation r matches
with the relation path (rA, rB).
Inspired by the
work on relation path representation learning (Lin
et al., 2015), our model ﬁrst transforms relation
r to its distributed representation, i.e. vector r ∈
RdR, and builds the path embeddings by composi-
tion of relation embeddings. Then, the similarity
oi is calculated as follows:

oi = −(cid:107)ri − (rA + rB)(cid:107)L1.

(9)

Therefore, if ri gets more similar to (rA + rB),
the conditioned predicting probability of ri will
become larger. Here, we make an implicit assump-
tion that if ri is semantically similar to relation
rB−→ t, the embedding ri will be
path pi : h
closer to the relation path embedding (rA + rB).
rB−→ t,
rA−→ e
Finally, for this relation path pi : h
we deﬁne an relation-path score function,

rA−→ e

G(h, r, t|pi) = E(h, rA, e)E(e, rB, t)p(r|rA, rB),
(10)
where E(h, rA, e) and E(e, rB, t) measure the
probabilities of relational facts (h, rA, e) and
(e, rB, t) from text, and p(r|rA, rB) measures
the probability of relation r given relation path
(rA, rB).

In reality, there are usually multiple relation
paths between two entities. Hence, we deﬁne the
inferring correlation between relation r and sev-
eral sentence paths P as,

G(h, r, t|P ) = max

G(h, r, t|pi),

(11)

i

where we use max operation to ﬁlter out those
noisy paths and select the most representative
path.

3.3

Joint Model

Given any entity pair (h, t), those sentences S
directly mentioning them and relation paths P be-
tween them, we deﬁne the global score function
with respect to a candidate relation r as,

L(h, r, t) = E(h, r, t|S) + αG(h, r, t|P ),

(12)

where E(h, r, t|S) models the correlation between
r and (h, t) calculated from direct sentences,

G(h, r, t|P ) models the inferring correlation be-
tween relation r and several sentence paths P . α
equals to (1 − E(h, r, t|S)) times a constant β.
This term serves to depict the relative weight be-
tween direct sentences and relation paths, since we
don’t need to pay much attention on extra infor-
mation when CNN has already given a conﬁdent
prediction, namely E(h, r, t|S) is large.

One of the advantages of this joint model is to
alleviate the issue of error propagation. The un-
certainty of information from Text Encoder and
Relation Path encoder is characterized by its con-
ﬁdence, and could be integrated and corrected in
this joint model step. Furthermore, since we treat
relation paths in a probabilistic way, our model
could fully utilize all relation paths, i.e. those al-
ways hold and those likely to hold.

3.4 Optimization and Implementation Details

The overall objective function is deﬁned as:

J(θ) =

log(L(h, r, t)),

(13)

(cid:88)

(h,r,t)

where the summing runs over the log loss of all
entity pairs in text and θ represents the model pa-
rameters. To solve this optimization problem, we
use mini-batch stochastic gradient descent (SGD)
to maximize our objective function. We initial-
ize WE with the results from Skip-gram model,
and initialize other parameters randomly. We also
adopt dropout (Srivastava et al., 2014) upon the
output layer of CNN.

We implement our model using C++. We train
our model on Intel(R) Xeon(R) CPU E5-2620, and
the training roughly takes half a day. The word
embedding and other parameters are updated via
back-propagation simultaneously, while the rela-
tion path structure is extracted before training and
stored afterward.

4 Dataset

Snapshot) with New York Times corpus (NYT).
There are 53 possible relationships between two
entities,
including a special relation type NA,
meaning that there is no relation between head and
tail entities. For each relational fact in a ﬁltered
Freebase dataset, a sentence from NYT would be
regarded as a mention of this relation if both the
head and tail entity appear in that sentence.

While this previous dataset has been frequently
used for evaluating relation extraction systems, we
observe some limitations of it. First, the relational
facts are extracted from a 2009 snapshot of Free-
base. Therefore, this dataset is too old to contain
many updated facts. This will underestimate the
performance of a relation extraction system, since
some real-world facts are missing from the dataset
and labeled as NA. Second, the relational facts in
this dataset are scattered, i.e.
there are not sufﬁ-
cient relation paths in this dataset, while relational
facts in real-world always have connections with
each other. Third, Freebase will no longer update
after 2016.These limitations mean that this dataset
is somewhat improper for evaluating RE systems.
Although other relation extraction datasets ex-
ist, e.g. ACE1 and (Hendrickx et al., 2009), they
are too small to train an effective neural relation
extraction model. Moreover, each relational fact
in (Hendrickx et al., 2009) only corresponds with
one sentence, which prevents it from evaluating
multi-instance relation extraction systems. Hence,
we constructed a novel relation extraction dataset
to address these issues, and will make it available
to the community.

4.2 Dataset Construction

Datasets

Riedel et.al.

Ours

Sets
Train
Valid
Test
Train
Valid
Test

# sentences
522,611
-
172,448
647,827
234,350
235,609

# entity pairs
281,270
-
96,678
266,118
121,160
121,837

# facts
18,252
-
1,950
50,031
5,609
5,756

Table 1: Statistics of datasets.

We build a novel dataset for evaluating relation
extraction task. We ﬁrst describe the most com-
monly used previous dataset and then explain the
reason and how we construct the new dataset.

4.1 Previous Datasets & Reasons for New

Dataset

Our dataset contains more updated facts and
richer structures of relations, e.g. more relations
/ relation paths, as compared to existing similar
datasets. The dataset is expected to be more simi-
lar to real-world cases, and thus be more appropri-
ate for evaluating RE systems’ performances.

We build the dataset by aligning Wikidata2 re-

A commonly used benchmark dataset for this
task was developed by (Riedel et al., 2010). This
dataset was built by aligning Freebase (Dec. 2009

1https://catalog.ldc.upenn.edu/

LDC2006T06

2https://www.wikidata.org/

lations with the New York Times Corpus (NYT).
Wikidata is a large, growing knowledge base,
which contains more than 80 million triple facts
and 20 million entities. Different from Freebase,
Wikidata is still in maintenance and could be eas-
ily accessed by APIs. We ﬁrst pick those en-
tities simultaneously appeared in both Wikidata
and Freebase, and relational facts associated with
them. Then, we ﬁltered out a subset S, reserv-
ing those facts associating with the 99 highest fre-
quency relations. This results in 4, 574, 665 triples
with 1, 045, 385 entities and 99 relations.

Next, we align those facts with NYT corpus,
following the assumption of distant supervision.
For each pair of entities appearing in our S, we
traverse the corpus and pick those sentences where
both entities appear. These sentences will be re-
garded as mentions of this fact, and labeled by
this relation type. To simulate noise in the real
world, we also add sentences corresponding to
“No Relation” entity pairs into our dataset. To
get those “No Relation” instances, we ﬁrst cre-
ate a fake knowledge base S− by randomly re-
placing the head or tail entities in triples, i.e.,
S− = {(h(cid:48), r, t)}∪{(h, r, t(cid:48))} and then align them
with NYT corpus. Finally, we randomly split all
those selected sentences into training, validation
and testing set, assuring that a relational fact could
be only mentioned by sentences in one set. The
statistics of our dataset and (Riedel et al., 2010)
are listed in Table 1.

5 Experiments

Following the previous work (Mintz et al.,
2009), we evaluate our model by extracting rela-
tional facts from the sentences in test set, and com-
pare them with those in Wikidata. We report Pre-
cision/Recall curves, Precision@N (P@N) and F1
scores for comparison in our experiments.

5.1

Initialization and Parameter Settings
In this paper, we use the word2vec tool 3
to pre-train word embeddings on NYT corpus.
We keep the words which appear more than 100
times in the corpus as vocabulary. We tune our
model on the validation set, using grid search
to determine the optimal parameters, which are
shown in boldface. We select learning rate for
SGD λ ∈ {0.1, 0.01, 0.001}, the sentence em-
bedding size dc ∈ {50, 60, · · · , 230, · · · , 300},

3https://code.google.com/p/word2vec/

the window size k ∈ {1, 2, 3, 4, 5}, and the
mini-batch size B ∈ {40, 160, 640}.
Be-
sides, we select
the relation embeddings size
dR ∈ {5, 10, · · · , 40, · · · , 60}, and the weight
∈
information from relation paths β
for
{0.01, 0.1, 0.2, 0.5, 1, · · · , 5}. For other parame-
ters which have little effect on the system perfor-
mance, we follow the settings used in (Zeng et al.,
2015): word embedding size dw is 50, position
embedding size dp is 5 and dropout rate p is 0.5.
For training, the iteration number over all training
data is 25.

5.2 Effectiveness of Incorporating Relation

Paths

5.2.1 Precision-Recall Curve Comparison

To demonstrate the effect of our approach, we
empirically compare it with other neural relation
extraction methods via held-out evaluation.
(1)
CNN+rand represents the CNN model reported
in (Zeng et al., 2014). (2) CNN+max represents
the CNN model with multi-instances learning used
(3) Path+rand/max is
in (Zeng et al., 2015).
our model with those two multi-instance settings.
We implement (1), (2) by ourselves which achieve
comparable results as reported in those papers.

Figure 3: Aggregate precision/recall curve for
CNN+rand, CNN+max, Path+rand, Path+max.

Fig. 3 shows the precision/recall curves of all
methods. From the ﬁgure, we can observe that: (1)
Our methods outperform their counterpart meth-
ods, achieving higher precision over almost en-
tire range of recall. They also enhance recall by
20% without decrease of precision. These results
prove the effectiveness of our approach. We no-
tice that the improvements of our methods over

Test Settings (Noise)
P@N (%)
CNN+rand
CNN+max
Path+rand
Path+max

75%
10% 20% 50% F1
57.5
86.7
57.2
86.0
89.4
59.3
59.6
89.0

67.0
68.5
71.7
71.5

38.9
38.3
39.9
39.8

85%
10% 20% 50% F1
55.0
84.6
56.5
85.4
58.1
88.2
59.4
89.0

37.5
37.7
39.0
39.6

66.4
67.6
70.2
71.4

95%
10% 20% 50% F1
51.4
79.9
54.8
84.4
55.6
86.0
59.1
88.6

35.2
36.6
37.0
39.1

61.8
66.0
67.2
71.0

Table 2: P@N and F1 for relation extraction in texts containing different percentage of no-relation facts.

baselines are relatively small at small recall value,
which corresponds to high predicting conﬁdence.
This phenomenon is intuitive since our joint model
could dynamically leverage the importance of di-
rect sentence and relation paths, and tends to trust
the Text Encoder when the conﬁdence is high. (2)
As the recall increases, our models exhibit larger
improvements compared with CNN in terms of
percentage. This is due to the fact that sometimes
CNNs cannot extract reliable information from di-
rect sentences, while our methods could alleviate
this issue by considering more information from
inference chains, and thus still maintain high pre-
cision.
(3) Both CNN+max and Path+rand are
variations of CNN+rand, aiming to alleviate the
problem of noisy data. We see that Path+rand out-
performs CNN+max over all range, which indi-
cates that considering path information is a better
way to solve this issue. Meanwhile, combining
paths information and max operation, Path+max,
gives the best performance. (4) Path+rand shows
a larger improvement over CNN+rand, compared
with those of Path+max and CNN+max. This fur-
thermore proves the effectiveness of considering
relation path information: CNN+rand has much
more severe problem suffering from noise, so us-
ing our method to incorporate paths information to
alleviate this issue could perform better.

5.2.2 Comparison on Long Tail Situation

CNN+rand
Path+rand
CNN+max
Path+max

Ns ≤ 1
53.9
58.4
57.8
63.6 (+5.8)

Ns ≤ 2
54.0
58.1
58.3
62.5 (+4.2)

Ns ≤ 5
52.0
56.0
56.5
60.2 (+3.7)

All
51.4
55.6
55.7
59.1 (+3.4)

Table 3: F1 score for long tail situation.

Real-world data follows long-tail distribution
(power law). In the testing set, we also observe
a fact that about 40% triple facts appear only in
single sentence, and thus a multi-instance rela-
tion extraction system, e.g. CNN+max, could only
rely on limited information and the multi-instance
mechanism will not work well. Our system, on the
contrary, can still utilize information from relation
paths in this case, and is expected to perform much

better in the long tail situation.

We evaluate the models on different parts of the
long-tail distribution. To get testing instances from
different parts of the distribution, we extract all the
triple facts appearing less than Ns sentences in the
testing set, and those sentences associating with
them. All text related to ’No Relation’ entity pair
are also reserved in order to simulate noise. We
then evaluate different models on those sampled
testing set, and report the results of F1 score in
Table 3.

From Table 3, we could observe that: (1) Incor-
porating relation paths is indeed effective in pre-
dicting relations, and our models have signiﬁcant
improvements compared with the baselines.
(2)
Path+max indeed has larger improvements over
CNN+max when Ns is small, which is consistent
with our previous expectation. Also notice that
the gap between Path+rand and CNN+rand is rel-
atively constant. This is due to the fact that both
these methods only use one random sentence, re-
gardless of how many sentences there are associ-
ating with an entity pair.

5.3 Model Robustness under Different

Percentages of Noise

In the task of relation extraction, there are lots
of noise in text which may hurt the model’s per-
formance. More speciﬁcally, “No Relation” entity
pair is a kind of noise, since “No Relation” could
actually contain many unknown relation types,
and thus might confuse the relation extraction sys-
tems. Therefore, it is important to verify the ro-
bustness of our model in the presence of massive
noise. Here, we evaluate those models in three set-
tings, with the same relational facts and different
percentages of “No Relation” sentences in the test-
ing sets. In each experiment, we extract top 20,000
predicting relational facts according to the model’s
predicting scores, and report the precision @top
10%, @top 20%, @top 50% and F1 score in Table
2.

From the table, we can see that: (1) In terms of
all evaluations, our models achieve the best perfor-

Path #1
Path #2
Test
Path #1
Path #2
Test

Relation
mother
has child
spouse
shares border with
shares border with
shares border with

Text
Rebecca gave birth to twin sons, Esau and Jacob, ...
...Isaac’s marriage to Rebecca, by whom he has two sons, Esau and jacob, ...
... Isaac and Rebecca and the female and male evil spirits ...
... in Somalia, ... soldiers and marines stationed in neighboring Djibouti ...
... Ethiopia have had the effect of making neighboring Djibouti ...
The next day, Ethiopia struck, its military pushing deep into Somalia ...

Table 4: Some representative examples of inference chians in NYT corpus. The bold is target entities.

mance as compared with other methods in all test
settings. It demonstrates the effectiveness of our
approach. (2) Even though the scores of all mod-
els drop as the increasing of noise, we ﬁnd that
Path+rand/max’s scores decrease much less than
their counterparts. This result proves the effective-
ness of taking inference chains into consideration.
Since we utilize more information to make predic-
tions, our model is more robust to the presence of
mass noise.

5.4 Effectiveness of Learned Features in

Zero-Shot Scenario

It has been proved that CNN could automat-
ically extract useful features, encoding syntactic
and semantic meaning of sentences. These fea-
tures are sometimes fed to subsequent models to
solve other tasks. In this experiment, we demon-
strate the effectiveness of the extracted features
from our model. Since CNN-based models have
already succeeded in extracting relations from sin-
gle sentences, we set our experiment in a new
scenario: predicting the relation between entities
which have not appeared in the same sentence.

A natural approach is to build a relation path
between this zero-shot entity pair. We assume that
we can make a prediction about (h, t), once we
know the information of (h, e) and (e, t). There-
fore, we build the training set by extracting all
such relation paths and their sentences from train-
ing text, and similar for testing set. To test the
effectiveness of features, we encode sentences by
CNN+rand/max, Path+rand/max respectively, and
then feed the concatenation of sentence vectors to
a logistic classiﬁer.

Feature
CNN+rand
CNN+max
Path+rand
Path+max

Accuracy
56.9
57.3
58.5
60.4

result using CNN+rand features is comparable to
the result using CNN+max features.
It shows
that using max operation to train the features does
not greatly improve the features’ behavior in this
task, even though it performs well in previous
tasks. The reason is that, both CNN+rand and
CNN+max only encode the information from a
single sentence, and they are unable to capture the
correlations between relations.
(2) Feature from
Path+rand/max shows its effectiveness over those
from other methods. It indicates that our method
is able to model the correlations between relations,
while also keeps the syntactic and semantic mean-
ing of a sentence. Therefore, the features extracted
from Path+rand/max are useful for a wider range
of applications, especially in those tasks which
need the information from relations.

5.5 Case study

Table 4 shows some representative inference
chains from the testing dataset. These examples
can not be predicted correctly by the original CNN
model, but are later corrected using our model. We
show the test instances and their correct relations,
as well as the inference chains the model uses. In
the ﬁrst example, the test sentence does not di-
rectly express the relation spouse, the proof of
this relation appears in a further context in NYT.
However, using path#1 and path#2, we could eas-
ily infer that Rebecca and Issac are spouse. The
second example doesn’t show the relation either.
But with the help of intermediate entity, Dijibouti,
our model predicts that Somalia shares the bor-
der with Ethiopia. Note that this inference chain
doesn’t always hold, but our model could capture
this uncertainty well via a softmax operation. In
general, our model can utilize common sense from
inference chains. It helps make correct predictions
even if the inference is not explicit.

Table 5: Accuracy of different models in zero-shot
situation.

From Table 5, we could observe that: (1) The

6 Conclusion and Future Work

In this paper, we propose a neural relation ex-
traction model which encodes the information of

relation paths. As compared to existing neural re-
lation extraction models, our model is able to uti-
lize the sentences which contain both two target
entities and only one target entity and is more ro-
bust for noisy data. Experimental results on real-
world datasets show that our model achieves sig-
niﬁcant and consistent improvements on relation
extraction as compared with baselines.

In the future, we will explore the following di-
rections: (1) We will explore the combination of
relation paths from both plain texts and KBs for
relation extraction. (2) We may take advantages
of probabilistic graphical model or recurrent neu-
ral network to encode more complicated correla-
tions between relation paths, e.g. multi-step rela-
tion paths, for relation extraction.

Acknowledgments

This work is supported by the 973 Program
(No. 2014CB340501), the National Natural Sci-
ence Foundation of China (NSFC No. 61572273,
61661146007), China Association for Science
and Technology (2016QNRC001), and Tsinghua
University Initiative Scientiﬁc Research Program
(20151080406).

References

S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
Springer.

Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and trends R(cid:13) in Machine Learning,
2(1):1–127.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of KDD, pages
1247–1250.

Mark Craven, Johan Kumlien, et al. 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In Proceedings of ISMB,
volume 1999, pages 77–86.

Rajarshi Das, Arvind Neelakantan, David Belanger,
and Andrew McCallum. 2016. Chains of reasoning
over entities, relations, and text using recurrent neu-
ral networks. arXiv preprint arXiv:1607.01426.

Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
Improving learning
and Tom M Mitchell. 2013.
and inference in a large knowledge-base using latent
In Proceedings of EMNLP, pages
syntactic cues.
833–838.

Kelvin Guu, John Miller, and Percy Liang. 2015.
In
Traversing knowledge graphs in vector space.
Proceedings of EMNLP, pages 318–327.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid ´O S´eaghdha, Sebastian
Pad´o, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2009.
Semeval-2010 task 8:
Multi-way classiﬁcation of semantic relations be-
In Proceedings of
tween pairs of nominals.
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions, pages 94–99.
Association for Computational Linguistics.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
In Proceedings of ACL-
of overlapping relations.
HLT, pages 541–550.

Ni Lao and William W Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine learning, 81(1):53–67.

Ni Lao, Tom Mitchell, and William W Cohen. 2011.
Random walk inference and learning in a large scale
knowledge base. In Proceedings of EMNLP, pages
529–539.

Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP-CoNLL, pages 1017–1026.

Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2015.
Modeling relation paths for representation learning
of knowledge bases. Proceedings of EMNLP.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Luan Huanbo,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. Proceedings
of ACL.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of ACL-
IJCNLP, pages 1003–1011.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
In Proceedings of ACL, pages 1105–
structures.
1116.

Arvind Neelakantan, Benjamin Roth, and Andrew Mc-
Callum. 2015. Compositional vector space models
for knowledge base inference. In 2015 AAAI Spring
Symposium Series.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of ECML-PKDD,
pages 148–163.

Alexander M Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. Proceedings of EMNLP.

Cıcero Nogueira dos Santos and Maıra Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In Proceedings of COLING.

Cıcero Nogueira dos Santos, Bing Xiang, and Bowen
Zhou. 2015. Classifying relations by ranking with
In Proceedings of
convolutional neural networks.
ACL, volume 1, pages 626–634.

Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
In Proceedings of ACL.
tional vector grammars.
Citeseer.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of EMNLP-CoNLL, pages 1201–1211.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. JMLR, 15(1):1929–1958.

Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
In Proceedings of WWW, pages 697–706.
edge.
ACM.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of EMNLP, pages 455–465.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of NIPS, pages 3104–3112.

Jiawei Wu, Ruobing Xie, Zhiyuan Liu, and Maosong
Sun. 2016. Knowledge representation via joint
learning of sequential text and knowledge graphs.
arXiv preprint arXiv:1609.07075.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015. Classifying relations via long
short term memory networks along shortest depen-
In Proceedings of EMNLP, pages
dency paths.
1785–1794.

Hai Ye, Wenhan Chao, and Zhunchen Luo. 2016.
Jointly extracting relations with class ties via effec-
tive deep ranking. arXiv preprint arXiv:1612.07602.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
In Pro-
piecewise convolutional neural networks.
ceedings of EMNLP.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classiﬁcation via con-
volutional deep neural network. In Proceedings of
COLING, pages 2335–2344.

