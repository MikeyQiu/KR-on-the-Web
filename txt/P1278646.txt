Whitening-Free Least-Squares
Non-Gaussian Component Analysis

Hiroaki Shiino1, Hiroaki Sasaki2, Gang Niu3, and Masashi Sugiyama4,3

1 Yahoo Japan Corporation
Kioi Tower 1-3 Kioicho, Chiyoda-ku, Tokyo 102-8282, Japan.
2 Nara Institute of Science and Technology
8916-5 Takayama-cho Ikoma, Nara 630-0192, Japan.
3 The University of Tokyo
5-1-5 Kashiwanoha, Kashiwa-shi, Chiba 277-8561, Japan.
4 RIKEN Center for Advanced Intelligence Project
1-4-1 Nihonbashi, Chuo-ku, Tokyo 103-0027, Japan.

Abstract. Non-Gaussian component analysis (NGCA) is an unsuper-
vised linear dimension reduction method that extracts low-dimensional
non-Gaussian “signals” from high-dimensional data contaminated with
Gaussian noise. NGCA can be regarded as a generalization of projec-
tion pursuit (PP) and independent component analysis (ICA) to multi-
dimensional and dependent non-Gaussian components. Indeed, seminal
approaches to NGCA are based on PP and ICA. Recently, a novel NGCA
approach called least-squares NGCA (LSNGCA) has been developed,
which gives a solution analytically through least-squares estimation of log-
density gradients and eigendecomposition. However, since pre-whitening
of data is involved in LSNGCA, it performs unreliably when the data
covariance matrix is ill-conditioned, which is often the case in high-
dimensional data analysis. In this paper, we propose a whitening-free
variant of LSNGCA and experimentally demonstrate its superiority.

Keywords: non-Gaussian component analysis, dimension reduction, un-
supervised learning

1

Introduction

Dimension reduction is a common technique in high-dimensional data analysis to
mitigate the curse of dimensionality [1]. Among various approaches to dimension
reduction, we focus on unsupervised linear dimension reduction in this paper.

It is known that the distribution of randomly projected data is close to Gaus-
sian [2]. Based on this observation, non-Gaussian component analysis (NGCA)
[3] tries to ﬁnd a subspace that contains non-Gaussian signal components so
that Gaussian noise components can be projected out. NGCA is formulated in
an elegant semi-parametric framework and non-Gaussian components can be
extracted without specifying their distributions. Mathematically, NGCA can
be regarded as a generalization of projection pursuit (PP) [4] and independent

7
1
0
2
 
y
a
M
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
9
2
0
1
0
.
3
0
6
1
:
v
i
X
r
a

2

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Table 1. NGCA methods.

MIPP

IMAK

SNGCA LSNGCA

WF-LSNGCA
(proposed)

Manual
NGIF design
Computational
eﬃciency
Pre-whitening

Need

No Need

Need

No Need No Need

Eﬃcient
(iterative)
Need

Ineﬃcient
(iterative)
Need

Ineﬃcient
(iterative)
No Need

Eﬃcient
(analytic)
Need

Eﬃcient
(analytic)
No Need

component analysis (ICA) [5] to multi-dimensional and dependent non-Gaussian
components.

The ﬁrst NGCA algorithm is called multi-index PP (MIPP). PP algorithms
such as FastICA [5] use a non-Gaussian index function (NGIF) to ﬁnd either
a super-Gaussian or sub-Gaussian component. MIPP uses a family of such
NGIFs to ﬁnd multiple non-Gaussian components and apply principal component
analysis (PCA) to extract a non-Gaussian subspace. However, MIPP requires us
to prepare appropriate NGIFs, which is not necessarily straightforward in practice.
Furthermore, MIPP requires pre-whitening of data, which can be unreliable when
the data covariance matrix is ill-conditioned.

To cope with these problems, MIPP has been extended in various ways. The
method called iterative metric adaptation for radial kernel functions (IMAK) [6]
tries to avoid the manual design of NGIFs by learning the NGIFs from data in
the form of radial kernel functions. However, this learning part is computationally
highly expensive and pre-whitening is still necessary. Sparse NGCA (SNGCA)
[7,2] tries to avoid pre-whitening by imposing an appropriate constraint so that
the solution is independent of the data covariance matrix. However, SNGCA
involves semi-deﬁnite programming which is computationally highly demanding,
and NGIFs still need to be manually designed.

Recently, a novel approach to NGCA called least-squares NGCA (LSNGCA)
has been proposed [8]. Based on the gradient of the log-density function, LSNGCA
constructs a vector that belongs to the non-Gaussian subspace from each sample.
Then the method of least-squares log-density gradients (LSLDG) [9,10] is employed
to directly estimate the log-density gradient without density estimation. Finally,
the principal subspace of the set of vectors generated from all samples is extracted
by eigendecomposition. LSNGCA is computationally eﬃcient and no manual
design of NGIFs is involved. However, it still requires pre-whitening of data.

The existing NGCA methods reviewed above are summarized in Table 1.
In this paper, we propose a novel NGCA method that is computationally eﬃ-
cient, no manual design of NGIFs is involved, and no pre-whitening is necessary.
Our proposed method is essentially an extention of LSNGCA so that the co-
variance of data is implicitly handled without explicit pre-whitening or explicit
constraints. Through experiments, we demonstrate that our proposed method,
called whitening-free LSNGCA (WF-LSNGCA), performs very well even when
the data covariance matrix is ill-conditioned.

Whitening-Free Least-Squares Non-Gaussian Component Analysis

3

2 Non-Gaussian Component Analysis

In this section, we formulate the problem of NGCA and review the MIPP and
LSNGCA methods.

2.1 Problem Formulation

Suppose that we are given a set of d-dimensional i.i.d. samples of size n, {xi|xi ∈
Rd}n

i=1, which are generated by the following model:

xi = Asi + ni,

where si ∈ Rm (m ≤ d) is an m-dimensional signal vector independently gener-
ated from an unknown non-Gaussian distribution (we assume that m is known),
ni ∈ Rd is a noise vector independently generated from a centered Gaussian dis-
tribution with an unknown covariance matrix Q, and A ∈ Rd×m is an unknown
mixing matrix of rank m. Under this data generative model, probability density
function p(x) that samples {xi}n
i=1 follow can be expressed in the following
semi-parametric form [3]:

(1)

(2)

p(x) = f (B(cid:62)x)φQ(x),

where f is an unknown smooth positive function on Rm, B ∈ Rd×m is an unknown
linear mapping, φQ is the centered Gaussian density with the covariance matrix
Q, and (cid:62) denotes the transpose. We note that decomposition (2) is not unique;
multiple combinations of B and f can give the same probability density function.
Nevertheless, the following m-dimensional subspace E, called the non-Gaussian
index space, can be determined uniquely [11]:

E = Null(B(cid:62))⊥ = Range(B),

(3)

where Null(B(cid:62)) denotes the null space of B(cid:62), ⊥ denotes the orthogonal comple-
ment, and Range(B) denotes the column space of B.

The goal of NGCA is to estimate the non-Gaussian index space E from

samples {xi}n

i=1.

2.2 Multi-Index Projection Pursuit (MIPP)

MIPP [3] is the ﬁrst algorithm of NGCA.
Let us whiten the samples {xi}n

identity:

i=1 so that their covariance matrix becomes

yi := Σ− 1
where Σ is the covariance matrix of x. In practice, Σ is replaced by the sample
covariance matrix. Then, for an NGIF h, the following vector β(h) was shown to
belong to the non-Gaussian index space E [3]:

2 xi,

β(h) := E [yh(y) − ∇yh(y)] ,

4

Whitening-Free Least-Squares Non-Gaussian Component Analysis

where ∇y denotes the diﬀerential operator w.r.t. y and E[·] denotes the expec-
tation over p(x). MIPP generates a set of such vectors from various NGIFs
{hl}L

l=1:

(cid:98)βl :=

[yihl(yi) − ∇yhl(yi)] ,

(4)

where the expectation is estimated by the sample average. Then (cid:98)βl is normalized
as

(cid:98)βl ← (cid:98)βl

(cid:107)yihl(yi) − ∇yhl(yi)(cid:107)2 − (cid:107)(cid:98)βl(cid:107)2,

(5)

1
n

n
(cid:88)

i=1

(cid:44)(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

by which (cid:107)(cid:98)βl(cid:107) is proportional to its signal-to-noise ratio. Then vectors (cid:98)βl with
their norm less than a pre-speciﬁed threshold τ > 0 are eliminated. Finally, PCA
is applied to the remaining vectors (cid:98)βl to obtain an estimate of the non-Gaussian
index space E.

The behavior of MIPP strongly depends on the choice of NGIF h. To improve
the performance, MIPP actively searches informative h as follows. First, the form
of h is restricted to h(y) = s(w(cid:62)y), where w ∈ Rd denotes a unit-norm vector
and s is a smooth real function. Then, estimated vector (cid:98)β is written as

(cid:98)β =

1
n

n
(cid:88)

i=1

(cid:0)yis(w(cid:62)yi) − s(cid:48)(w(cid:62)yi)w(cid:1) ,

where s(cid:48) is the derivative of s. This equation is actually equivalent to a single
iteration of the PP algorithm called FastICA [12]. Based on this fact, the pa-
rameter w is optimized by iteratively applying the following update rule until
convergence:

w ←

(cid:80)n
(cid:107) (cid:80)n

(cid:0)yis(w(cid:62)yi) − s(cid:48)(w(cid:62)yi)w(cid:1)
i=1
i=1 (yis(w(cid:62)yi) − s(cid:48)(w(cid:62)yi)w) (cid:107)

.

The superiority of MIPP has been investigated both theoretically and experi-
mentally [3]. However, MIPP has the weaknesses that NGIFs should be manually
designed and pre-whitening is necessary.

2.3 Least-Squares Non-Gaussian Component Analysis (LSNGCA)

LSNGCA [8] is a recently proposed NGCA algorithm that does not require
manual design of NGIFs (Table 1). Here the algorithm of LSNGCA is reviewed,
which will be used for further developing a new method in the next section.

Derivation: For whitened samples {yi}n
given in Eq.(2) can be simpliﬁed as

i=1, the semi-parametric form of NGCA

p(y) = (cid:101)f ( (cid:101)B

y)φI d(y),

(cid:62)

(6)

Whitening-Free Least-Squares Non-Gaussian Component Analysis

5

where (cid:101)f is an unknown smooth positive function on Rm and (cid:101)B ∈ Rd×m is
an unknown linear mapping. Under this simpliﬁed semi-parametric form, the
non-Gaussian index space E can be represented as

E = Σ− 1

2 Range( (cid:101)B).

Taking the logarithm and diﬀerentiating the both sides of Eq.(6) w.r.t. y

yield

∇y ln p(y) + y = (cid:101)B∇

(cid:62)

ln (cid:101)f ( (cid:101)B

y),

(cid:62)

y

(cid:101)B

(7)

where ∇

(cid:62)

y

(cid:101)B

denotes the diﬀerential operator w.r.t. (cid:101)B

y. This implies that

(cid:62)

u(y) := ∇y ln p(y) + y

belongs to the non-Gaussian index space E. Then applying eigendecomposition
to (cid:80)n
i=1 u(yi)u(yi)(cid:62) and extracting the m leading eigenvectors allow us to
recover Range( (cid:101)B). In LSNGCA, the method of least-squares log-density gradients
(LSLDG) [9,10] is used to estimate the log-density gradient ∇y ln p(y) included
in u(y), which is brieﬂy reviewed below.

LSLDG: Let ∂j denote the diﬀerential operator w.r.t. the j-th element of y.
LSLDG ﬁts a model g(j)(y) to ∂j ln p(y), the j-th element of log-density gradient
∇y ln p(y), under the squared loss:

J(g(j)) := E[(g(j)(y) − ∂j ln p(y))2] − E[(∂j ln p(y))2]
= E[g(j)(y)2] − 2E[g(j)(y)∂j ln p(y)].

(8)

The second term in Eq.(8) yields

E[g(j)(y)∂j ln p(y)] =

(cid:90)

g(j)(y)(∂j ln p(y))p(y)dy =
(cid:90)

= −

∂jg(j)(y)p(y)dy = −E[∂jg(j)(y)],

(cid:90)

g(j)(y)∂jp(y)dy

where the second-last equation follows from integration by parts under the
assumption lim|y(j)|→∞ g(j)(y)p(y) = 0. Then sample approximation yields

J(g(j)) = E[g(j)(y)2 − 2∂jg(j)(y)] ≈

[g(j)(yi)2 + 2∂jg(j)(yi)].

(9)

1
n

n
(cid:88)

i=1

As a model of the log-density gradient, LSLDG uses a linear-in-parameter form:

g(j)(y) =

θk,jψk,j(y) = θ(cid:62)

j ψj(y),

(10)

b
(cid:88)

k=1

6

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Algorithm 1 Pseudo-code of WF-LSNGCA.
input Element-wise standardized data samples: {xi}n
i=1.
1: Obtain an estimate (cid:98)v(x) of v(x) = ∇x ln p(x) − ∇2
2: Apply eigendecomposition to (cid:80)n

scribed in Section 3.2.

vectors as an orthonormal basis of non-Gaussian index space E.

i=1 (cid:98)v(xi)(cid:98)v(xi)(cid:62) and extract the m leading eigen-

x ln p(x)x by the method de-

where b denotes the number of basis functions, θj := (θ1,j, . . . , θb,j)(cid:62) is a pa-
rameter vector to be estimated, and ψj(y) := (ψ1,j(y), . . . , ψb,j(y))(cid:62) is a basis
function vector. The parameter vector θj is learned by solving the following
regularized empirical optimization problem:

(cid:98)θj = argmin

θj

(cid:104)

j (cid:98)Gjθj + 2θ(cid:62)
θ(cid:62)

j (cid:98)hj + λj(cid:107)θj(cid:107)2(cid:105)

,

where λj > 0 is the regularization parameter,

(cid:98)Gj =

ψj(yi)ψj(yi)(cid:62), (cid:98)hj =

∂jψj(yi).

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

This optimization problem can be analytically solved as

(cid:16)

(cid:17)−1

(cid:98)θj = −

(cid:98)Gj + λjI b

(cid:98)hj,

where I b is the b-by-b identity matrix. Finally, an estimator of the log-density
gradient g(j)(y) is obtained as

(cid:98)g(j)(y) = (cid:98)θ

(cid:62)
j ψj(y).

All tuning parameters such as the regularization parameter λj and parameters
included in the basis function ψk,j(y) can be systematically chosen based on
cross-validation w.r.t. Eq.(9).

3 Whitening-Free LSNGCA

In this section, we propose a novel NGCA algorithm that does not involve pre-
whitening. A pseudo-code of the proposed method, which we call whitening-free
LSNGCA (WF-LSNGCA), is summarized in Algorithm 1.

3.1 Derivation

Unlike LSNGCA which used the simpliﬁed semi-parametric form (6), we directly
use the original semi-parametric form (2) without whitening. Taking the logarithm
and diﬀerentiating the both sides of Eq.(2) w.r.t. x yield

∇x ln p(x) + Q−1x = B∇B(cid:62)x ln f (B(cid:62)x),

(11)

Whitening-Free Least-Squares Non-Gaussian Component Analysis

7

where ∇x denotes the derivative w.r.t. x and ∇B(cid:62)x denotes the derivative
w.r.t. B(cid:62)x. Further taking the derivative of Eq.(11) w.r.t. x yields

Q−1 = −∇2

x ln p(x) + B∇2

B(cid:62)x ln f (B(cid:62)x)B(cid:62),

(12)

x denotes the second derivative w.r.t. x. Substituting Eq.(12) back into

where ∇2
Eq.(11) yields

∇x ln p(x) − ∇2

x ln p(x)x = B

∇B(cid:62)x ln f (B(cid:62)x) − ∇2

(cid:16)

(cid:17)
B(cid:62)x ln f (B(cid:62)x)B(cid:62)x

.

(13)

This implies that

v(x) := ∇x ln p(x) − ∇2

x ln p(x)x

belongs to the non-Gaussian index space E. Then we apply eigendecomposition
to (cid:80)n
i=1 v(xi)v(xi)(cid:62) and extract the m leading eigenvectors as an orthonormal
basis of non-Gaussian index space E.

Now the remaining task is to approximate v(x) from data, which is discussed

below.

3.2 Estimation of v(x)

Let v(j)(x) be the j-th element of v(x):

v(j)(x) = ∂j ln p(x) − (∇x∂j ln p(x))(cid:62) x.

To estimate v(j)(x), let us ﬁt a model w(j)(x) to it under the squared loss:

R(w(j)) := E[(w(j)(x) − v(j)(x))2] − E[v(j)(x)2]

= E[w(j)(x)2] − 2E[w(j)(x)v(j)(x)]
= E[w(j)(x)2] − 2E[w(j)(x)∂j ln p(x)] + 2E[w(j)(x) (∇x∂j ln p(x))(cid:62) x].
(14)

The second term in Eq.(14) yields

E[w(j)(x)∂j ln p(x)] =

(cid:90)

w(j)(x)(∂j ln p(x))p(x)dx =
(cid:90)

= −

∂jw(j)(x)p(x)dx = −E[∂jw(j)(x)],

(cid:90)

w(j)(x)∂jp(x)dx

where the second-last equation follows from integration by parts under the
assumption lim|x(j)|→∞ w(j)(x)p(x) = 0. ∂j ln p(x) included in the third term
in Eq.(14) may be replaced with the LSLDG estimator (cid:98)g(j)(x) reviewed in
Section 2.3. Note that the LSLDG estimator is obtained with non-whitened data
x in this method. Then we have

R(w(j)) ≈ E[w(j)(x)2 + 2∂jw(j)(x) + 2w(j)(x)(∇x(cid:98)g(j)(x))(cid:62)x]

(15)

≈

1
n

n
(cid:88)

i=1

[w(j)(xi)2 + 2∂jw(j)(xi) + 2w(j)(xi)(∇x(cid:98)g(j)(xi))(cid:62)xi].

8

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Here, let us employ the following linear-in-parameter model as w(j)(x):

w(j)(x) :=

αk,jϕk,j(x) = α(cid:62)

j ϕj(x),

(16)

t
(cid:88)

k=1

where t denotes the number of basis functions, αj := (α1,j, . . . , αt,j)(cid:62) is a
parameter vector to be estimated, and ϕj(x) := (ϕ1,j(x), . . . , ϕt,j(x))(cid:62) is a basis
function vector. The parameter vector αj is learned by minimizing the following
regularized empirical optimization problem:

(cid:98)αj = argmin

αj

(cid:104)
α(cid:62)

j (cid:98)Sjαj + 2α(cid:62)

j (cid:98)tj(x) + γj(cid:107)αj(cid:107)2(cid:105)

,

where γj > 0 is the regularization parameter,

(cid:98)Sj =

ϕj(xi)ϕj(xi)(cid:62),

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:18)

(cid:98)tj =

∂jϕj(xi) + ϕj(xi)

(cid:16)

∇x(cid:98)g(j)(xi)

(cid:17)(cid:62)

(cid:19)

xi

.

This optimization problem can be analytically solved as

Finally, an estimator of v(j)(x) is obtained as

(cid:16)

(cid:98)αj = −

(cid:98)Sj + γjI b

(cid:17)−1

(cid:98)tj.

(cid:98)v(j)(x) = (cid:98)α(cid:62)

j ϕj(x).

All tuning parameters such as the regularization parameter γj and parameters
included in the basis function ϕk,j(y) can be systematically chosen based on
cross-validation w.r.t. Eq.(15).

3.3 Theoretical Analysis

Here, we investigate the convergence rate of WF-LSNGCA in a parametric
setting.

Let g∗(x) be the optimal estimate to ∇x ln p(x) given by LSLDG based on

the linear-in-parameter model g(x), and let

j = E (cid:2)ϕj(x)ϕj(x)(cid:62)(cid:3) ,
S∗

j = E
t∗

(cid:16)

(cid:20)
∂jϕj(x) + ϕj(x)

∇xg∗(j)(x)
j α(cid:62)α(cid:9) , w∗(j)(x) = α∗(cid:62)

j ϕj(x),

(cid:17)(cid:62)

(cid:21)

x

,

α∗

j = argminα

(cid:8)α(cid:62)S∗

j α + 2α(cid:62)t∗

j + γ∗
j I b) must be strictly positive deﬁnite. In fact, S∗

j + γ∗

where (S∗
be strictly positive deﬁnite, and thus γ∗
analysis.

j should already
j = 0 is also allowed in our theoretical

We have the following theorem (its proof is given in Section 3.4):

Whitening-Free Least-Squares Non-Gaussian Component Analysis

9

Theorem 1. As n → ∞, for any x,

(cid:107)(cid:98)v(x) − w∗(x)(cid:107)2 = Op

(cid:16)

n−1/2(cid:17)

,

provided that γj for all j converge in O(n−1/2) to γ∗
∞.

j , i.e., limn→∞ n1/2|γj −γ∗

j | <

Theorem 1 is based on the theory of perturbed optimizations [13,14] as well as
the convergence of LSLDG shown in [8]. It guarantees that for any x, the estimate
(cid:98)v(x) in WF-LSNGCA converges to the optimal estimate w∗(x) based on the
linear-in-parameter model w(x), and it achieves the optimal parametric conver-
gence rate Op(n−1/2). Note that Theorem 1 deals only with the estimation error,
and the approximation error is not taken into account. Indeed, approximation
errors exist in two places, from w∗(x) to v(x) in WF-LSNGCA itself and from
g∗(x) to ∇x ln p(x) in the plug-in LSLDG estimator. Since the original LSNGCA
also relies on LSLDG, it cannot avoid the approximation error introduced by
LSLDG. For this reason, the convergence of WF-LSNGCA is expected to be as
good as LSNGCA.

Theorem 1 is basically a theoretical guarantee that is similar to Part One
in the proof of Theorem 1 in [8]. Hence, based on Theorem 1, we can go along
the line of Part Two in the proof of Theorem 1 in [8] and obtain the following
corollary.

Corollary 1. For eigendecomposition, deﬁne matrices (cid:98)Γ = 1
i=1 (cid:98)v(xi)(cid:98)v(xi)(cid:62)
n
and Γ∗ = E[w∗(x)w∗(x)(cid:62)]. Given the estimated subspace (cid:98)E based on n samples
and the optimal estimated subspace E∗ based on inﬁnite data, denote by (cid:98)E ∈ Rd×m
the matrix form of an arbitrary orthonormal basis of (cid:98)E and by E∗ ∈ Rd×m that
of E∗. Deﬁne the distance between subspaces as

(cid:80)n

where (cid:107) · (cid:107)Fro stands for the Frobenius norm. Then, as n → ∞,

D( (cid:98)E, E∗) = inf

(cid:98)E,E∗ (cid:107)(cid:98)E − E∗(cid:107)Fro,

D( (cid:98)E, E∗) = Op

(cid:16)

n−1/2(cid:17)

,

provided that γj for all j converge in O(n−1/2) to γ∗
basis functions such that the ﬁrst m eigenvalues of Γ∗ are neither 0 nor +∞.

j and ϕj(x) are well-chosen

3.4 Proof of Theorem 1

Step 1. First of all, we establish the growth condition (see Deﬁnition 6.1 in [14]).
Denote the expected and empirical objective functions by

j (α) = α(cid:62)S∗

R∗
(cid:98)Rj(α) = α(cid:62) (cid:98)Sjα + 2α(cid:62)

j α + 2α(cid:62)t∗

j α(cid:62)α,
j + γ∗
(cid:98)tj + γjα(cid:62)α.

Then α∗

j = argminα R∗

j (α), (cid:98)αj = argminα (cid:98)Rj(α), and we have

10

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Lemma 1. Let (cid:15)j be the smallest eigenvalue of (S∗
second-order growth condition holds
j (α) ≥ R∗

j ) + (cid:15)j(cid:107)α − α∗

j (α∗

R∗

j (cid:107)2
2.

j + γ∗

j I b), then the following

Proof. R∗

j (α) must be strongly convex with parameter at least 2(cid:15)j. Hence,

R∗

j (α) ≥ R∗
≥ R∗

j (α∗
j (α∗

j ) + (∇R∗
j (α∗
j ) + (cid:15)j(cid:107)α − α∗

j ))(cid:62)(α − α∗
j (cid:107)2
2,

j ) + (α − α∗

j )(cid:62)(S∗

j + γ∗

j I b)(α − α∗
j )

where we used the optimality condition ∇R∗

j (α∗

j ) = 0.

Step 2. Second, we study the stability (with respect to perturbation) of R∗
at α∗

j (α)

j . Let

u = {uS ∈ S b

+, ut ∈ Rb, uγ ∈ R}

+ ⊂ Rb×b is the cone of b-by-
be a set of perturbation parameters, where S b
b symmetric positive semi-deﬁnite matrices. Deﬁne our perturbed objective
function by

Rj(α, u) = α(cid:62)(S∗

j + uS)α + 2α(cid:62)(t∗

j + ut) + (γ∗

j + uγ)α(cid:62)α.

j (α) = Rj(α, 0), and then the stability of R∗

It is clear that R∗
characterized as follows.
Lemma 2. The diﬀerence function Rj(α, u) − R∗
in α modulus

ω(u) = O((cid:107)uS(cid:107)Fro + (cid:107)ut(cid:107)2 + |uγ|)

j (α) at α∗

j can be

j (α) is Lipschitz continuous

on a suﬃciently small neighborhood of α∗
j .

Proof. The diﬀerence function is

Rj(α, u) − R∗

j (α) = α(cid:62)uSα + 2α(cid:62)ut + uγα(cid:62)α,

with a partial gradient

∂
∂α

(Rj(α, u) − R∗

j (α)) = 2uSα + 2ut + 2uγα.

Notice that due to the (cid:96)2-regularization in R∗
Now given a δ-ball of α∗
that ∀α ∈ Bδ(α∗

j , i.e., Bδ(α∗

j ),

j (α), ∃M > 0 such that (cid:107)α∗

j (cid:107)2 ≤ M .
j (cid:107)2 ≤ δ}, it is easy to see

j ) = {α | (cid:107)α − α∗

(cid:107)α(cid:107)2 ≤ (cid:107)α − α∗

j (cid:107)2 + (cid:107)α∗

j (cid:107)2 ≤ δ + M,

and consequently
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∂
∂α

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(Rj(α, u) − R∗

j (α))

≤ 2(δ + M )((cid:107)uS(cid:107)Fro + |uγ|) + 2(cid:107)ut(cid:107)2.

This says that the gradient ∂
j (α)) has a bounded norm of order
O((cid:107)uS(cid:107)Fro + (cid:107)ut(cid:107)2 + |uγ|), and proves that the diﬀerence function Rj(α, u) −
R∗
j ), with a Lipschitz constant of
the same order.

j (α) is Lipschitz continuous on the ball Bδ(α∗

∂α (Rj(α, u) − R∗

Whitening-Free Least-Squares Non-Gaussian Component Analysis

11

Step 3. Lemma 1 ensures the unperturbed objective R∗
α leaves α∗
for α around α∗
suﬀers. Based on Lemma 1, Lemma 2, and Proposition 6.1 in [14],

j (α) grows quickly when
j ; Lemma 2 ensures the perturbed objective Rj(α, u) changes slowly
j , where the slowness is compared with the perturbation u it

(cid:107) (cid:98)αj − α∗

j (cid:107)2 ≤

ω(u)
(cid:15)j

= O((cid:107)uS(cid:107)Fro + (cid:107)ut(cid:107)2 + |uγ|),

since (cid:98)αj is the exact solution to (cid:98)Rj(α) = Rj(α, u) given uS = (cid:98)Sj − S∗
(cid:98)tj − t∗

j , and uγ = γj − γ∗
j .

j , ut =

According to the central limit theorem (CLT), (cid:107)uS(cid:107)Fro = Op(n−1/2). Consider

(cid:98)tj − t∗
j :

(cid:98)tj − t∗

j =

∂jϕj(xi) − E (cid:2)∂jϕj(x)(cid:3) +

n
(cid:88)

1
n

i=1
(cid:20)
ϕj(x)

− E

(cid:16)

∇xg∗(j)(x)

(cid:17)(cid:62)

(cid:21)

x

.

1
n

n
(cid:88)

i=1

(cid:16)

ϕj(xi)

∇x(cid:98)g(j)(xi)

(cid:17)(cid:62)

xi

The ﬁrst half is clearly Op(n−1/2) due to CLT. For the second half, the estimate
(cid:98)g(j)(x) given by LSLDG converges to g∗(j)(x) for any x in Op(n−1/2) according to
Part One in the proof of Theorem 1 in [8], and ∇x(cid:98)g(j)(x) converges to ∇xg∗(j)(x)
in the same order because the basis functions in ψj(x) are all derivatives of
Gaussian functions. Consequently,

(cid:16)

ϕj(xi)

∇x(cid:98)g(j)(xi)

(cid:17)(cid:62)

xi −

ϕj(xi)

(cid:16)
∇xg∗(j)(xi)

(cid:17)(cid:62)

xi = Op(n−1/2),

1
n

n
(cid:88)

i=1

since ∇x(cid:98)g(j)(x) converges to ∇xg∗(j)(x) for any x in Op(n−1/2), and

(cid:16)

(cid:17)(cid:62)

ϕj(xi)

∇xg∗(j)(xi)

xi − E

(cid:20)
ϕj(x)

(cid:16)

(cid:17)(cid:62)

(cid:21)

∇xg∗(j)(x)

x

= Op(n−1/2)

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

due to CLT, which proves (cid:107)ut(cid:107)2 = Op(n−1/2). Furthermore, we have already
assumed that |uγ| = O(n−1/2). Hence, as n → ∞,

(cid:107) (cid:98)αj − α∗

j (cid:107)2 = Op

(cid:16)

n−1/2(cid:17)

.

Step 4. Finally, for any x, the gap of (cid:98)v(j)(x) and w∗(j)(x) is bounded by

|(cid:98)v(j)(x) − w∗(j)(x)| ≤ (cid:107) (cid:98)αj − α∗

j (cid:107)2 · (cid:107)ϕj(x)(cid:107)2,

where the Cauchy-Schwarz inequality is used. Since the basis functions in ϕj(x)
are again all derivatives of Gaussian functions, (cid:107)ϕj(x)(cid:107)2 must be bounded
uniformly, and then

|(cid:98)v(j)(x) − w∗(j)(x)| ≤ O((cid:107) (cid:98)αj − α∗

j (cid:107)2) = Op

(cid:16)

n−1/2(cid:17)

.

Applying the same argument for all j = 1, . . . , d completes the proof.

12

Whitening-Free Least-Squares Non-Gaussian Component Analysis

4 Experiments

In this section, we experimentally investigate the performance of MIPP, LSNGCA,
and WF-LSNGCA.5

4.1 Conﬁgurations of NGCA Algorithms

MIPP We use the MATLAB script which was used in the original MIPP paper
[3]6. In this script, NGIFs of the form sk
m(z) (m = 1, . . . , 1000, k = 1, . . . , 4) are
used:

m(z) = z3exp
s1

−

(cid:18)

s3
m(z) = sin(bmz),

(cid:19)

,

z2
2σ2
m
s4
m(z) = cos(bmz),

s2
m(z) = tanh (amz) ,

where σm, am, and bm are scalars chosen at the regular intervals from σm ∈ [0.5, 5],
am ∈ [0.05, 5], and bm ∈ [0.05, 4]. The cut-oﬀ threshold τ is set at 1.6 and the
number of FastICA iterations is set at 10 (see Section 2.2).

LSNGCA Following [10], the derivative of the Gaussian kernel is used as the
basis function ψk,j(y) in the linear-in-parameter model (10):

ψk,j(y) = ∂jexp

−

(cid:32)

(cid:107)y − ck(cid:107)2
2σ2
j

(cid:33)

,

where σj > 0 is the Gaussian bandwidth and ck is the Gaussian center randomly
selected from the whitened data samples {yi}n
i=1. The number of basis functions
is set at b = 100. For model selection, 5-fold cross-validation is performed
with respect to the hold-out error of Eq.(9) using 10 candidate values at the
regular intervals in logarithmic scale for Gaussian bandwidth σj ∈ [10−1, 101]
and regularization parameter λj ∈ [10−5, 101].

WF-LSNGCA Similarly to LSNGCA, the derivative of the Gaussian kernel
is used as the basis function ϕk,j(x) in the linear-in-parameter model (16) and
the number of basis functions is set as t = b = 100. For model selection, 5-fold
cross-validation is performed with respect to the hold-out error of Eq.(15) in the
same way as LSNGCA.

4.2 Artiﬁcial Datasets

Let x = (s1, s2, n3, . . . , n10)(cid:62), where s := (s1, s2)(cid:62) are the 2-dimensional non-
Gaussian signal components and n := (n3, . . . , n10)(cid:62) are the 8-dimensional
Gaussian noise components. For the non-Gaussian signal components, we consider
the following four distributions plotted in Figure 1:

5 The source code of the experiments is at https://github.com/hgeno/WFLSNGCA.
6 http://www.ms.k.u-tokyo.ac.jp/software.html

Whitening-Free Least-Squares Non-Gaussian Component Analysis

13

(a) Independent
Gaussian Mix-
ture

(b) Dependent
Super-Gaussian

(c) Dependent
Sub-Gaussian

(d) Dependent
Super- and Sub-
Gaussian

Fig. 1. Distributions of non-Gaussian components.

(a) Independent
Gaussian Mix-
ture

(b) Dependent
Super-Gaussian

(c) Dependent
Sub-Gaussian

(d) Dependent
Super- and Sub-
Gaussian

Fig. 2. Averages and standard deviations of the subspace estimation error as the function
of the condition-number controller r over 50 simulations on artiﬁcial datasets.

(a) Independent Gaussian Mixture:

p(s1, s2) ∝

i=1

2
(cid:89)

(cid:18)

(cid:18)

exp

−

(cid:19)

(si − 3)2
2

(cid:18)

+ exp

−

(si + 3)2
2

(cid:19)(cid:19)

.

(b) Dependent super-Gaussian:

p(s) ∝ exp (− (cid:107)s(cid:107)).

(c) Dependent sub-Gaussian:

p(s) is the uniform distribution on (cid:8)s ∈ R2| (cid:107)s(cid:107) ≤ 1(cid:9).

(d) Dependent super- and sub-Gaussian:

p(s1) ∝ exp (− |s1|) and p(s2) is the uniform distribution on [c, c + 1], where
c = 0 if |s1| ≤ log 2 and c = −1 otherwise.

For the Gaussian noise components, we include a certain parameter r ≥ 0,
which controls the condition number; the larger r is, the more ill-posed the data
covariance matrix is. The detail is described in Appendix A.

We generate n = 2000 samples for each case, and standardize each element
of the data before applying NGCA algorithms. The performance of NGCA
algorithms is measured by the following subspace estimation error :

ε(E, (cid:98)E) :=

(cid:107)(cid:98)ei − ΠE(cid:98)ei(cid:107)2 ,

1
2

2
(cid:88)

i=1

(17)

where E is the true non-Gaussian index space, (cid:98)E is its estimate, ΠE is the
orthogonal projection on E, and {(cid:98)ei}2

i=1 is an orthonormal basis in (cid:98)E.

The averages and the standard derivations of the subspace estimation error
over 50 runs for MIPP, LSNGCA, and WF-LSNGCA are depicted in Figure 2.

14

Whitening-Free Least-Squares Non-Gaussian Component Analysis

(a) The function of sample size.

(b) The function of data dimen-
sion.

Fig. 3. The average CPU time over 50 runs when the Gaussian mixture is used as
non-Gaussian components and the condition-number controller r = 0. The vertical axis
is in logarithmic scale.

This shows that, for all 4 cases, the error of MIPP grows rapidly as r increases.
On the other hand, LSNGCA and WF-LSNGCA perform much stably against the
change in r. However, LSNGCA performs poorly for (a). Overall, WF-LSNGCA is
shown to be much more robust against ill-conditioning than MIPP and LSNGCA.

In terms of the computation time, WF-LSNGCA is less eﬃcient than LSNGCA
and MIPP, but its computation time is still just a few times slower than LSNGCA,
as seen in Figure 3. For this reason, the computational eﬃciency of WF-LSNGCA
would still be acceptable in practice.

4.3 Benchmark Datasets

Finally, we evaluate the performance of NGCA methods using the LIBSVM binary
classiﬁcation benchmark datasets 7 [15]. From each dataset, n points are selected
as training (test) samples so that the number of positive and negative samples are
equal, and datasets are standardized in each dimension. For an m-dimensional
dataset, we append (d − m)-dimensional noise dimensions following the standard
Gaussian distribution so that all datasets have d dimensions. Then we use PCA,
MIPP, LSNGCA, and WF-LSNGCA to obtain m-dimensional expressions, and
apply the support vector machine (SVM) 8 to evaluate the test misclassiﬁcation
rate. As a baseline, we also evaluate the misclassiﬁcation rate by the raw SVM
without dimension reduction.

7 We preprocessed the LIBSVM binary classiﬁcation benchmark datasets as follows:

– vehicle: We convert original labels ‘1’ and ‘2’ to the positive label and original

labels ‘3’ and ‘4’ to the negative label.

– SUSY : We convert original label ‘0’ to the negative label.
– shuttle: We use only the data labeled as ‘1’ and ‘4’ and regard them as positive

and negative labels.

– svmguide1 : We mix the original training and test datasets.

8 We used LIBSVM with MATLAB [15].

Whitening-Free Least-Squares Non-Gaussian Component Analysis

15

The averages and standard deviations of the misclassiﬁcation rate over 50
runs for d = 50, 100 are summarized in Table 2. As can be seen in the table, the
appended Gaussian noise dimensions have negative eﬀects on each classiﬁcation
accuracy, and thus the baseline has relatively high misclassiﬁcation rates. PCA
has overall higher misclassiﬁcation rates than the baseline since a lot of valuable
information for each classiﬁcation problem is lost. Among the NGCA algorithms,
WF-LSNGCA overall compares favorably with the other methods. This means
that it can ﬁnd valuable low-dimensional expressions for each classiﬁcation
problem without harmful eﬀects of a pre-whitening procedure.

Table 2. Averages (and standard deviations in the parentheses) of the misclassiﬁcation
rates for the LIBSVM datasets over 50 runs. The best and comparable algorithms
judged by the two-sample t-test at the signiﬁcance level 5% are expressed as boldface.

Dataset
[m, n]
vehicle
[18, 200]
svmguide3
[21, 200]
svmguide1
[3, 2000]
shuttle
[9, 2000]
SUSY
[18, 2000]
ijcnn1
[22, 2000]

Dataset
[m, n]
vehicle
[18, 200]
svmguide3
[21, 200]
svmguide1
[3, 2000]
shuttle
[9, 2000]
SUSY
[18, 2000]
ijcnn1
[22, 2000]

No Dim.
Red.
0.340
(0.038)
0.342
(0.035)
0.088
(0.007)
0.031
(0.004)
0.238
(0.010)
0.102
(0.007)

d = 50

PCA MIPP

LS
NGCA
0.324
0.328
0.404
(0.044)
(0.034) (0.044)
0.326
0.348
0.341
(0.039)
(0.037) (0.041)
0.058
0.159
0.060
(0.006)
(0.012) (0.005)
0.041
0.024
0.021
(0.015)
(0.007) (0.004)
0.271
0.223
0.229
(0.012) (0.010) (0.012)
0.273
0.084
(0.012) (0.028)

0.061
(0.006)

WF-LS
NGCA
0.286
(0.038)
0.308
(0.036)
0.053
(0.008)
0.007
(0.002)
0.228
(0.010)
0.057
(0.007)

d = 100

No Dim.
Red.
0.380
(0.033)
0.363
(0.034)
0.102
(0.008)
0.038
(0.005)
0.250
(0.010)
0.145
(0.008)

PCA MIPP

LS
NGCA
0.439
(0.045)
0.427
(0.035)
0.088
(0.059)
0.209
(0.080)
0.228

WF-LS
NGCA
0.360
0.432
0.445
(0.051)
(0.033) (0.036)
0.343
0.443
0.367
(0.044)
(0.032) (0.042)
0.067
0.175
0.087
(0.020)
(0.013) (0.013)
0.017
0.065
0.069
(0.005)
(0.013) (0.019)
0.280
0.226
0.234
(0.011) (0.009) (0.011) (0.010)
0.091
0.318
0.107
(0.020)
(0.013) (0.021)

0.101
(0.010)

16

Whitening-Free Least-Squares Non-Gaussian Component Analysis

5 Conclusions

In this paper, we proposed a novel NGCA algorithm which is computationally
eﬃcient, no manual design of non-Gaussian index functions is required, and
pre-whitening is not involved. Through experiments, we demonstrated that the
eﬀectiveness of the proposed method.

References

1. V. N. Vapnik. Statistical Learning Theory, volume 1. Wiley New York, 1998.
2. E. Diederichs, A. Juditsky, A. Nemirovski, and V. Spokoiny. Sparse non Gaussian
component analysis by semideﬁnite programming. Machine learning, 91(2):211–238,
2013.

3. G. Blanchard, M. Kawanabe, M. Sugiyama, V. Spokoiny, and K. Müller. In search
of non-Gaussian components of a high-dimensional distribution. Journal of Machine
Learning Research, 7:247–282, 2006.

4. J. H. Friedman and J. W. Tukey. A projection pursuit algorithm for exploratory

data analysis. IEEE Transactions on Computers, C-23(9):881–890, 1974.

5. A. Hyvärinen and E. Oja. Independent component analysis: algorithms and appli-

cations. Neural Networks, 13(4):411–430, 2000.

6. M. Kawanabe, M. Sugiyama, G. Blanchard, and K. Müller. A new algorithm
of non-Gaussian component analysis with radial kernel functions. Annals of the
Institute of Statistical Mathematics, 59(1):57–75, 2007.

7. E. Diederichs, A. Juditsky, V. Spokoiny, and C. Schütte. Sparse non-Gaussian
component analysis. IEEE Transactions on Information Theory, 56(6):3033–3047,
2010.

8. H. Sasaki, G. Niu, and M. Sugiyama. Non-Gaussian component analysis with
log-density gradient estimation. In Proceedings of the Seventeenth International
Conference on Artiﬁcial Intelligence and Statistics, volume 38, pages 1177–1185,
2016.

9. D. D Cox. A penalty method for nonparametric estimation of the logarithmic
derivative of a density function. Annals of the Institute of Statistical Mathematics,
37(1):271–288, 1985.

10. H. Sasaki, A. Hyvärinen, and M. Sugiyama. Clustering via mode seeking by direct
estimation of the gradient of a log-density. In Machine Learning and Knowledge
Discovery in Databases, pages 19–34. Springer, 2014.

11. F. J Theis and M. Kawanabe. Uniqueness of non-Gaussian subspace analysis.
In Independent Component Analysis and Blind Signal Separation, pages 917–925.
Springer, 2006.

12. A. Hyvärinen. Fast and robust ﬁxed-point algorithms for independent component

analysis. IEEE Transactions on Neural Networks, 10(3):626–634, 1999.

13. F. Bonnans and R. Cominetti. Perturbed optimization in Banach spaces I: A general
theory based on a weak directional constraint qualiﬁcation; II: A theory based on
a strong directional qualiﬁcation condition; III: Semiinﬁnite optimization. SIAM
Journal on Control and Optimization, 34:1151–1171, 1172–1189, and 1555–1567,
1996.

14. F. Bonnans and A. Shapiro. Optimization problems with perturbations, a guided

tour. SIAM Review, 40(2):228–264, 1998.

15. C. Chang and C. Lin. LIBSVM: A library for support vector machines. ACM

Transactions on Intelligent Systems and Technology, 2(3):27, 2011.

Whitening-Free Least-Squares Non-Gaussian Component Analysis

17

Supplementary Materials to Whitening-Free Least-Squares
Non-Gaussian Component Analysis

A Details of Artiﬁcial Datasets

Here, we describe the detail of the artiﬁcial datasets used in Section 4.2. The
noise components n are generated as follows:

1. The n is sampled from the centered Gaussian distribution with covariance
matrix diag(10−2r, 10−2r+4r/7, 10−2r+8r/7, . . . , 102r), where diag(·) denotes
the diagonal matrix.

2. The sampled n is rotated as n(cid:48)(cid:48) ∈ R8by applying the following rotation

matrix R(i,j) for all i, j = 3, . . . , 10 such that i < j:

i,j = − sin(π/4),

R(i,j)
i,i = cos(π/4), R(i,j)
R(i,j)
j,i = sin(π/4), R(i,j)
k,k = 1 (k (cid:54)= i, k (cid:54)= j), R(i,j)
R(i,j)

j,j = cos(π/4),

k,l = 0 (otherwise).

3. The rotated n is normalized for each dimension.

By this construction, increasing r corresponds to increasing the condition number
of the data covariance matrix (see Figure 4). Thus, the larger r is, the more
ill-posed the data covariance matrix is.

Fig. 4. Condition number of the data covariance matrix as a function of experiment
parameter r (with non-Gaussian components generated from the Gaussian mixture).

Whitening-Free Least-Squares
Non-Gaussian Component Analysis

Hiroaki Shiino1, Hiroaki Sasaki2, Gang Niu3, and Masashi Sugiyama4,3

1 Yahoo Japan Corporation
Kioi Tower 1-3 Kioicho, Chiyoda-ku, Tokyo 102-8282, Japan.
2 Nara Institute of Science and Technology
8916-5 Takayama-cho Ikoma, Nara 630-0192, Japan.
3 The University of Tokyo
5-1-5 Kashiwanoha, Kashiwa-shi, Chiba 277-8561, Japan.
4 RIKEN Center for Advanced Intelligence Project
1-4-1 Nihonbashi, Chuo-ku, Tokyo 103-0027, Japan.

Abstract. Non-Gaussian component analysis (NGCA) is an unsuper-
vised linear dimension reduction method that extracts low-dimensional
non-Gaussian “signals” from high-dimensional data contaminated with
Gaussian noise. NGCA can be regarded as a generalization of projec-
tion pursuit (PP) and independent component analysis (ICA) to multi-
dimensional and dependent non-Gaussian components. Indeed, seminal
approaches to NGCA are based on PP and ICA. Recently, a novel NGCA
approach called least-squares NGCA (LSNGCA) has been developed,
which gives a solution analytically through least-squares estimation of log-
density gradients and eigendecomposition. However, since pre-whitening
of data is involved in LSNGCA, it performs unreliably when the data
covariance matrix is ill-conditioned, which is often the case in high-
dimensional data analysis. In this paper, we propose a whitening-free
variant of LSNGCA and experimentally demonstrate its superiority.

Keywords: non-Gaussian component analysis, dimension reduction, un-
supervised learning

1

Introduction

Dimension reduction is a common technique in high-dimensional data analysis to
mitigate the curse of dimensionality [1]. Among various approaches to dimension
reduction, we focus on unsupervised linear dimension reduction in this paper.

It is known that the distribution of randomly projected data is close to Gaus-
sian [2]. Based on this observation, non-Gaussian component analysis (NGCA)
[3] tries to ﬁnd a subspace that contains non-Gaussian signal components so
that Gaussian noise components can be projected out. NGCA is formulated in
an elegant semi-parametric framework and non-Gaussian components can be
extracted without specifying their distributions. Mathematically, NGCA can
be regarded as a generalization of projection pursuit (PP) [4] and independent

7
1
0
2
 
y
a
M
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
9
2
0
1
0
.
3
0
6
1
:
v
i
X
r
a

2

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Table 1. NGCA methods.

MIPP

IMAK

SNGCA LSNGCA

WF-LSNGCA
(proposed)

Manual
NGIF design
Computational
eﬃciency
Pre-whitening

Need

No Need

Need

No Need No Need

Eﬃcient
(iterative)
Need

Ineﬃcient
(iterative)
Need

Ineﬃcient
(iterative)
No Need

Eﬃcient
(analytic)
Need

Eﬃcient
(analytic)
No Need

component analysis (ICA) [5] to multi-dimensional and dependent non-Gaussian
components.

The ﬁrst NGCA algorithm is called multi-index PP (MIPP). PP algorithms
such as FastICA [5] use a non-Gaussian index function (NGIF) to ﬁnd either
a super-Gaussian or sub-Gaussian component. MIPP uses a family of such
NGIFs to ﬁnd multiple non-Gaussian components and apply principal component
analysis (PCA) to extract a non-Gaussian subspace. However, MIPP requires us
to prepare appropriate NGIFs, which is not necessarily straightforward in practice.
Furthermore, MIPP requires pre-whitening of data, which can be unreliable when
the data covariance matrix is ill-conditioned.

To cope with these problems, MIPP has been extended in various ways. The
method called iterative metric adaptation for radial kernel functions (IMAK) [6]
tries to avoid the manual design of NGIFs by learning the NGIFs from data in
the form of radial kernel functions. However, this learning part is computationally
highly expensive and pre-whitening is still necessary. Sparse NGCA (SNGCA)
[7,2] tries to avoid pre-whitening by imposing an appropriate constraint so that
the solution is independent of the data covariance matrix. However, SNGCA
involves semi-deﬁnite programming which is computationally highly demanding,
and NGIFs still need to be manually designed.

Recently, a novel approach to NGCA called least-squares NGCA (LSNGCA)
has been proposed [8]. Based on the gradient of the log-density function, LSNGCA
constructs a vector that belongs to the non-Gaussian subspace from each sample.
Then the method of least-squares log-density gradients (LSLDG) [9,10] is employed
to directly estimate the log-density gradient without density estimation. Finally,
the principal subspace of the set of vectors generated from all samples is extracted
by eigendecomposition. LSNGCA is computationally eﬃcient and no manual
design of NGIFs is involved. However, it still requires pre-whitening of data.

The existing NGCA methods reviewed above are summarized in Table 1.
In this paper, we propose a novel NGCA method that is computationally eﬃ-
cient, no manual design of NGIFs is involved, and no pre-whitening is necessary.
Our proposed method is essentially an extention of LSNGCA so that the co-
variance of data is implicitly handled without explicit pre-whitening or explicit
constraints. Through experiments, we demonstrate that our proposed method,
called whitening-free LSNGCA (WF-LSNGCA), performs very well even when
the data covariance matrix is ill-conditioned.

Whitening-Free Least-Squares Non-Gaussian Component Analysis

3

2 Non-Gaussian Component Analysis

In this section, we formulate the problem of NGCA and review the MIPP and
LSNGCA methods.

2.1 Problem Formulation

Suppose that we are given a set of d-dimensional i.i.d. samples of size n, {xi|xi ∈
Rd}n

i=1, which are generated by the following model:

xi = Asi + ni,

where si ∈ Rm (m ≤ d) is an m-dimensional signal vector independently gener-
ated from an unknown non-Gaussian distribution (we assume that m is known),
ni ∈ Rd is a noise vector independently generated from a centered Gaussian dis-
tribution with an unknown covariance matrix Q, and A ∈ Rd×m is an unknown
mixing matrix of rank m. Under this data generative model, probability density
function p(x) that samples {xi}n
i=1 follow can be expressed in the following
semi-parametric form [3]:

(1)

(2)

p(x) = f (B(cid:62)x)φQ(x),

where f is an unknown smooth positive function on Rm, B ∈ Rd×m is an unknown
linear mapping, φQ is the centered Gaussian density with the covariance matrix
Q, and (cid:62) denotes the transpose. We note that decomposition (2) is not unique;
multiple combinations of B and f can give the same probability density function.
Nevertheless, the following m-dimensional subspace E, called the non-Gaussian
index space, can be determined uniquely [11]:

E = Null(B(cid:62))⊥ = Range(B),

(3)

where Null(B(cid:62)) denotes the null space of B(cid:62), ⊥ denotes the orthogonal comple-
ment, and Range(B) denotes the column space of B.

The goal of NGCA is to estimate the non-Gaussian index space E from

samples {xi}n

i=1.

2.2 Multi-Index Projection Pursuit (MIPP)

MIPP [3] is the ﬁrst algorithm of NGCA.
Let us whiten the samples {xi}n

identity:

i=1 so that their covariance matrix becomes

yi := Σ− 1
where Σ is the covariance matrix of x. In practice, Σ is replaced by the sample
covariance matrix. Then, for an NGIF h, the following vector β(h) was shown to
belong to the non-Gaussian index space E [3]:

2 xi,

β(h) := E [yh(y) − ∇yh(y)] ,

4

Whitening-Free Least-Squares Non-Gaussian Component Analysis

where ∇y denotes the diﬀerential operator w.r.t. y and E[·] denotes the expec-
tation over p(x). MIPP generates a set of such vectors from various NGIFs
{hl}L

l=1:

(cid:98)βl :=

[yihl(yi) − ∇yhl(yi)] ,

(4)

where the expectation is estimated by the sample average. Then (cid:98)βl is normalized
as

(cid:98)βl ← (cid:98)βl

(cid:107)yihl(yi) − ∇yhl(yi)(cid:107)2 − (cid:107)(cid:98)βl(cid:107)2,

(5)

1
n

n
(cid:88)

i=1

(cid:44)(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

by which (cid:107)(cid:98)βl(cid:107) is proportional to its signal-to-noise ratio. Then vectors (cid:98)βl with
their norm less than a pre-speciﬁed threshold τ > 0 are eliminated. Finally, PCA
is applied to the remaining vectors (cid:98)βl to obtain an estimate of the non-Gaussian
index space E.

The behavior of MIPP strongly depends on the choice of NGIF h. To improve
the performance, MIPP actively searches informative h as follows. First, the form
of h is restricted to h(y) = s(w(cid:62)y), where w ∈ Rd denotes a unit-norm vector
and s is a smooth real function. Then, estimated vector (cid:98)β is written as

(cid:98)β =

1
n

n
(cid:88)

i=1

(cid:0)yis(w(cid:62)yi) − s(cid:48)(w(cid:62)yi)w(cid:1) ,

where s(cid:48) is the derivative of s. This equation is actually equivalent to a single
iteration of the PP algorithm called FastICA [12]. Based on this fact, the pa-
rameter w is optimized by iteratively applying the following update rule until
convergence:

w ←

(cid:80)n
(cid:107) (cid:80)n

(cid:0)yis(w(cid:62)yi) − s(cid:48)(w(cid:62)yi)w(cid:1)
i=1
i=1 (yis(w(cid:62)yi) − s(cid:48)(w(cid:62)yi)w) (cid:107)

.

The superiority of MIPP has been investigated both theoretically and experi-
mentally [3]. However, MIPP has the weaknesses that NGIFs should be manually
designed and pre-whitening is necessary.

2.3 Least-Squares Non-Gaussian Component Analysis (LSNGCA)

LSNGCA [8] is a recently proposed NGCA algorithm that does not require
manual design of NGIFs (Table 1). Here the algorithm of LSNGCA is reviewed,
which will be used for further developing a new method in the next section.

Derivation: For whitened samples {yi}n
given in Eq.(2) can be simpliﬁed as

i=1, the semi-parametric form of NGCA

p(y) = (cid:101)f ( (cid:101)B

y)φI d(y),

(cid:62)

(6)

Whitening-Free Least-Squares Non-Gaussian Component Analysis

5

where (cid:101)f is an unknown smooth positive function on Rm and (cid:101)B ∈ Rd×m is
an unknown linear mapping. Under this simpliﬁed semi-parametric form, the
non-Gaussian index space E can be represented as

E = Σ− 1

2 Range( (cid:101)B).

Taking the logarithm and diﬀerentiating the both sides of Eq.(6) w.r.t. y

yield

∇y ln p(y) + y = (cid:101)B∇

(cid:62)

ln (cid:101)f ( (cid:101)B

y),

(cid:62)

y

(cid:101)B

(7)

where ∇

(cid:62)

y

(cid:101)B

denotes the diﬀerential operator w.r.t. (cid:101)B

y. This implies that

(cid:62)

u(y) := ∇y ln p(y) + y

belongs to the non-Gaussian index space E. Then applying eigendecomposition
to (cid:80)n
i=1 u(yi)u(yi)(cid:62) and extracting the m leading eigenvectors allow us to
recover Range( (cid:101)B). In LSNGCA, the method of least-squares log-density gradients
(LSLDG) [9,10] is used to estimate the log-density gradient ∇y ln p(y) included
in u(y), which is brieﬂy reviewed below.

LSLDG: Let ∂j denote the diﬀerential operator w.r.t. the j-th element of y.
LSLDG ﬁts a model g(j)(y) to ∂j ln p(y), the j-th element of log-density gradient
∇y ln p(y), under the squared loss:

J(g(j)) := E[(g(j)(y) − ∂j ln p(y))2] − E[(∂j ln p(y))2]
= E[g(j)(y)2] − 2E[g(j)(y)∂j ln p(y)].

(8)

The second term in Eq.(8) yields

E[g(j)(y)∂j ln p(y)] =

(cid:90)

g(j)(y)(∂j ln p(y))p(y)dy =
(cid:90)

= −

∂jg(j)(y)p(y)dy = −E[∂jg(j)(y)],

(cid:90)

g(j)(y)∂jp(y)dy

where the second-last equation follows from integration by parts under the
assumption lim|y(j)|→∞ g(j)(y)p(y) = 0. Then sample approximation yields

J(g(j)) = E[g(j)(y)2 − 2∂jg(j)(y)] ≈

[g(j)(yi)2 + 2∂jg(j)(yi)].

(9)

1
n

n
(cid:88)

i=1

As a model of the log-density gradient, LSLDG uses a linear-in-parameter form:

g(j)(y) =

θk,jψk,j(y) = θ(cid:62)

j ψj(y),

(10)

b
(cid:88)

k=1

6

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Algorithm 1 Pseudo-code of WF-LSNGCA.
input Element-wise standardized data samples: {xi}n
i=1.
1: Obtain an estimate (cid:98)v(x) of v(x) = ∇x ln p(x) − ∇2
2: Apply eigendecomposition to (cid:80)n

scribed in Section 3.2.

vectors as an orthonormal basis of non-Gaussian index space E.

i=1 (cid:98)v(xi)(cid:98)v(xi)(cid:62) and extract the m leading eigen-

x ln p(x)x by the method de-

where b denotes the number of basis functions, θj := (θ1,j, . . . , θb,j)(cid:62) is a pa-
rameter vector to be estimated, and ψj(y) := (ψ1,j(y), . . . , ψb,j(y))(cid:62) is a basis
function vector. The parameter vector θj is learned by solving the following
regularized empirical optimization problem:

(cid:98)θj = argmin

θj

(cid:104)

j (cid:98)Gjθj + 2θ(cid:62)
θ(cid:62)

j (cid:98)hj + λj(cid:107)θj(cid:107)2(cid:105)

,

where λj > 0 is the regularization parameter,

(cid:98)Gj =

ψj(yi)ψj(yi)(cid:62), (cid:98)hj =

∂jψj(yi).

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

This optimization problem can be analytically solved as

(cid:16)

(cid:17)−1

(cid:98)θj = −

(cid:98)Gj + λjI b

(cid:98)hj,

where I b is the b-by-b identity matrix. Finally, an estimator of the log-density
gradient g(j)(y) is obtained as

(cid:98)g(j)(y) = (cid:98)θ

(cid:62)
j ψj(y).

All tuning parameters such as the regularization parameter λj and parameters
included in the basis function ψk,j(y) can be systematically chosen based on
cross-validation w.r.t. Eq.(9).

3 Whitening-Free LSNGCA

In this section, we propose a novel NGCA algorithm that does not involve pre-
whitening. A pseudo-code of the proposed method, which we call whitening-free
LSNGCA (WF-LSNGCA), is summarized in Algorithm 1.

3.1 Derivation

Unlike LSNGCA which used the simpliﬁed semi-parametric form (6), we directly
use the original semi-parametric form (2) without whitening. Taking the logarithm
and diﬀerentiating the both sides of Eq.(2) w.r.t. x yield

∇x ln p(x) + Q−1x = B∇B(cid:62)x ln f (B(cid:62)x),

(11)

Whitening-Free Least-Squares Non-Gaussian Component Analysis

7

where ∇x denotes the derivative w.r.t. x and ∇B(cid:62)x denotes the derivative
w.r.t. B(cid:62)x. Further taking the derivative of Eq.(11) w.r.t. x yields

Q−1 = −∇2

x ln p(x) + B∇2

B(cid:62)x ln f (B(cid:62)x)B(cid:62),

(12)

x denotes the second derivative w.r.t. x. Substituting Eq.(12) back into

where ∇2
Eq.(11) yields

∇x ln p(x) − ∇2

x ln p(x)x = B

∇B(cid:62)x ln f (B(cid:62)x) − ∇2

(cid:16)

(cid:17)
B(cid:62)x ln f (B(cid:62)x)B(cid:62)x

.

(13)

This implies that

v(x) := ∇x ln p(x) − ∇2

x ln p(x)x

belongs to the non-Gaussian index space E. Then we apply eigendecomposition
to (cid:80)n
i=1 v(xi)v(xi)(cid:62) and extract the m leading eigenvectors as an orthonormal
basis of non-Gaussian index space E.

Now the remaining task is to approximate v(x) from data, which is discussed

below.

3.2 Estimation of v(x)

Let v(j)(x) be the j-th element of v(x):

v(j)(x) = ∂j ln p(x) − (∇x∂j ln p(x))(cid:62) x.

To estimate v(j)(x), let us ﬁt a model w(j)(x) to it under the squared loss:

R(w(j)) := E[(w(j)(x) − v(j)(x))2] − E[v(j)(x)2]

= E[w(j)(x)2] − 2E[w(j)(x)v(j)(x)]
= E[w(j)(x)2] − 2E[w(j)(x)∂j ln p(x)] + 2E[w(j)(x) (∇x∂j ln p(x))(cid:62) x].
(14)

The second term in Eq.(14) yields

E[w(j)(x)∂j ln p(x)] =

(cid:90)

w(j)(x)(∂j ln p(x))p(x)dx =
(cid:90)

= −

∂jw(j)(x)p(x)dx = −E[∂jw(j)(x)],

(cid:90)

w(j)(x)∂jp(x)dx

where the second-last equation follows from integration by parts under the
assumption lim|x(j)|→∞ w(j)(x)p(x) = 0. ∂j ln p(x) included in the third term
in Eq.(14) may be replaced with the LSLDG estimator (cid:98)g(j)(x) reviewed in
Section 2.3. Note that the LSLDG estimator is obtained with non-whitened data
x in this method. Then we have

R(w(j)) ≈ E[w(j)(x)2 + 2∂jw(j)(x) + 2w(j)(x)(∇x(cid:98)g(j)(x))(cid:62)x]

(15)

≈

1
n

n
(cid:88)

i=1

[w(j)(xi)2 + 2∂jw(j)(xi) + 2w(j)(xi)(∇x(cid:98)g(j)(xi))(cid:62)xi].

8

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Here, let us employ the following linear-in-parameter model as w(j)(x):

w(j)(x) :=

αk,jϕk,j(x) = α(cid:62)

j ϕj(x),

(16)

t
(cid:88)

k=1

where t denotes the number of basis functions, αj := (α1,j, . . . , αt,j)(cid:62) is a
parameter vector to be estimated, and ϕj(x) := (ϕ1,j(x), . . . , ϕt,j(x))(cid:62) is a basis
function vector. The parameter vector αj is learned by minimizing the following
regularized empirical optimization problem:

(cid:98)αj = argmin

αj

(cid:104)
α(cid:62)

j (cid:98)Sjαj + 2α(cid:62)

j (cid:98)tj(x) + γj(cid:107)αj(cid:107)2(cid:105)

,

where γj > 0 is the regularization parameter,

(cid:98)Sj =

ϕj(xi)ϕj(xi)(cid:62),

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:18)

(cid:98)tj =

∂jϕj(xi) + ϕj(xi)

(cid:16)

∇x(cid:98)g(j)(xi)

(cid:17)(cid:62)

(cid:19)

xi

.

This optimization problem can be analytically solved as

Finally, an estimator of v(j)(x) is obtained as

(cid:16)

(cid:98)αj = −

(cid:98)Sj + γjI b

(cid:17)−1

(cid:98)tj.

(cid:98)v(j)(x) = (cid:98)α(cid:62)

j ϕj(x).

All tuning parameters such as the regularization parameter γj and parameters
included in the basis function ϕk,j(y) can be systematically chosen based on
cross-validation w.r.t. Eq.(15).

3.3 Theoretical Analysis

Here, we investigate the convergence rate of WF-LSNGCA in a parametric
setting.

Let g∗(x) be the optimal estimate to ∇x ln p(x) given by LSLDG based on

the linear-in-parameter model g(x), and let

j = E (cid:2)ϕj(x)ϕj(x)(cid:62)(cid:3) ,
S∗

j = E
t∗

(cid:16)

(cid:20)
∂jϕj(x) + ϕj(x)

∇xg∗(j)(x)
j α(cid:62)α(cid:9) , w∗(j)(x) = α∗(cid:62)

j ϕj(x),

(cid:17)(cid:62)

(cid:21)

x

,

α∗

j = argminα

(cid:8)α(cid:62)S∗

j α + 2α(cid:62)t∗

j + γ∗
j I b) must be strictly positive deﬁnite. In fact, S∗

j + γ∗

where (S∗
be strictly positive deﬁnite, and thus γ∗
analysis.

j should already
j = 0 is also allowed in our theoretical

We have the following theorem (its proof is given in Section 3.4):

Whitening-Free Least-Squares Non-Gaussian Component Analysis

9

Theorem 1. As n → ∞, for any x,

(cid:107)(cid:98)v(x) − w∗(x)(cid:107)2 = Op

(cid:16)

n−1/2(cid:17)

,

provided that γj for all j converge in O(n−1/2) to γ∗
∞.

j , i.e., limn→∞ n1/2|γj −γ∗

j | <

Theorem 1 is based on the theory of perturbed optimizations [13,14] as well as
the convergence of LSLDG shown in [8]. It guarantees that for any x, the estimate
(cid:98)v(x) in WF-LSNGCA converges to the optimal estimate w∗(x) based on the
linear-in-parameter model w(x), and it achieves the optimal parametric conver-
gence rate Op(n−1/2). Note that Theorem 1 deals only with the estimation error,
and the approximation error is not taken into account. Indeed, approximation
errors exist in two places, from w∗(x) to v(x) in WF-LSNGCA itself and from
g∗(x) to ∇x ln p(x) in the plug-in LSLDG estimator. Since the original LSNGCA
also relies on LSLDG, it cannot avoid the approximation error introduced by
LSLDG. For this reason, the convergence of WF-LSNGCA is expected to be as
good as LSNGCA.

Theorem 1 is basically a theoretical guarantee that is similar to Part One
in the proof of Theorem 1 in [8]. Hence, based on Theorem 1, we can go along
the line of Part Two in the proof of Theorem 1 in [8] and obtain the following
corollary.

Corollary 1. For eigendecomposition, deﬁne matrices (cid:98)Γ = 1
i=1 (cid:98)v(xi)(cid:98)v(xi)(cid:62)
n
and Γ∗ = E[w∗(x)w∗(x)(cid:62)]. Given the estimated subspace (cid:98)E based on n samples
and the optimal estimated subspace E∗ based on inﬁnite data, denote by (cid:98)E ∈ Rd×m
the matrix form of an arbitrary orthonormal basis of (cid:98)E and by E∗ ∈ Rd×m that
of E∗. Deﬁne the distance between subspaces as

(cid:80)n

where (cid:107) · (cid:107)Fro stands for the Frobenius norm. Then, as n → ∞,

D( (cid:98)E, E∗) = inf

(cid:98)E,E∗ (cid:107)(cid:98)E − E∗(cid:107)Fro,

D( (cid:98)E, E∗) = Op

(cid:16)

n−1/2(cid:17)

,

provided that γj for all j converge in O(n−1/2) to γ∗
basis functions such that the ﬁrst m eigenvalues of Γ∗ are neither 0 nor +∞.

j and ϕj(x) are well-chosen

3.4 Proof of Theorem 1

Step 1. First of all, we establish the growth condition (see Deﬁnition 6.1 in [14]).
Denote the expected and empirical objective functions by

j (α) = α(cid:62)S∗

R∗
(cid:98)Rj(α) = α(cid:62) (cid:98)Sjα + 2α(cid:62)

j α + 2α(cid:62)t∗

j α(cid:62)α,
j + γ∗
(cid:98)tj + γjα(cid:62)α.

Then α∗

j = argminα R∗

j (α), (cid:98)αj = argminα (cid:98)Rj(α), and we have

10

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Lemma 1. Let (cid:15)j be the smallest eigenvalue of (S∗
second-order growth condition holds
j (α) ≥ R∗

j ) + (cid:15)j(cid:107)α − α∗

j (α∗

R∗

j (cid:107)2
2.

j + γ∗

j I b), then the following

Proof. R∗

j (α) must be strongly convex with parameter at least 2(cid:15)j. Hence,

R∗

j (α) ≥ R∗
≥ R∗

j (α∗
j (α∗

j ) + (∇R∗
j (α∗
j ) + (cid:15)j(cid:107)α − α∗

j ))(cid:62)(α − α∗
j (cid:107)2
2,

j ) + (α − α∗

j )(cid:62)(S∗

j + γ∗

j I b)(α − α∗
j )

where we used the optimality condition ∇R∗

j (α∗

j ) = 0.

Step 2. Second, we study the stability (with respect to perturbation) of R∗
at α∗

j (α)

j . Let

u = {uS ∈ S b

+, ut ∈ Rb, uγ ∈ R}

+ ⊂ Rb×b is the cone of b-by-
be a set of perturbation parameters, where S b
b symmetric positive semi-deﬁnite matrices. Deﬁne our perturbed objective
function by

Rj(α, u) = α(cid:62)(S∗

j + uS)α + 2α(cid:62)(t∗

j + ut) + (γ∗

j + uγ)α(cid:62)α.

j (α) = Rj(α, 0), and then the stability of R∗

It is clear that R∗
characterized as follows.
Lemma 2. The diﬀerence function Rj(α, u) − R∗
in α modulus

ω(u) = O((cid:107)uS(cid:107)Fro + (cid:107)ut(cid:107)2 + |uγ|)

j (α) at α∗

j can be

j (α) is Lipschitz continuous

on a suﬃciently small neighborhood of α∗
j .

Proof. The diﬀerence function is

Rj(α, u) − R∗

j (α) = α(cid:62)uSα + 2α(cid:62)ut + uγα(cid:62)α,

with a partial gradient

∂
∂α

(Rj(α, u) − R∗

j (α)) = 2uSα + 2ut + 2uγα.

Notice that due to the (cid:96)2-regularization in R∗
Now given a δ-ball of α∗
that ∀α ∈ Bδ(α∗

j , i.e., Bδ(α∗

j ),

j (α), ∃M > 0 such that (cid:107)α∗

j (cid:107)2 ≤ M .
j (cid:107)2 ≤ δ}, it is easy to see

j ) = {α | (cid:107)α − α∗

(cid:107)α(cid:107)2 ≤ (cid:107)α − α∗

j (cid:107)2 + (cid:107)α∗

j (cid:107)2 ≤ δ + M,

and consequently
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∂
∂α

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(Rj(α, u) − R∗

j (α))

≤ 2(δ + M )((cid:107)uS(cid:107)Fro + |uγ|) + 2(cid:107)ut(cid:107)2.

This says that the gradient ∂
j (α)) has a bounded norm of order
O((cid:107)uS(cid:107)Fro + (cid:107)ut(cid:107)2 + |uγ|), and proves that the diﬀerence function Rj(α, u) −
R∗
j ), with a Lipschitz constant of
the same order.

j (α) is Lipschitz continuous on the ball Bδ(α∗

∂α (Rj(α, u) − R∗

Whitening-Free Least-Squares Non-Gaussian Component Analysis

11

Step 3. Lemma 1 ensures the unperturbed objective R∗
α leaves α∗
for α around α∗
suﬀers. Based on Lemma 1, Lemma 2, and Proposition 6.1 in [14],

j (α) grows quickly when
j ; Lemma 2 ensures the perturbed objective Rj(α, u) changes slowly
j , where the slowness is compared with the perturbation u it

(cid:107) (cid:98)αj − α∗

j (cid:107)2 ≤

ω(u)
(cid:15)j

= O((cid:107)uS(cid:107)Fro + (cid:107)ut(cid:107)2 + |uγ|),

since (cid:98)αj is the exact solution to (cid:98)Rj(α) = Rj(α, u) given uS = (cid:98)Sj − S∗
(cid:98)tj − t∗

j , and uγ = γj − γ∗
j .

j , ut =

According to the central limit theorem (CLT), (cid:107)uS(cid:107)Fro = Op(n−1/2). Consider

(cid:98)tj − t∗
j :

(cid:98)tj − t∗

j =

∂jϕj(xi) − E (cid:2)∂jϕj(x)(cid:3) +

n
(cid:88)

1
n

i=1
(cid:20)
ϕj(x)

− E

(cid:16)

∇xg∗(j)(x)

(cid:17)(cid:62)

(cid:21)

x

.

1
n

n
(cid:88)

i=1

(cid:16)

ϕj(xi)

∇x(cid:98)g(j)(xi)

(cid:17)(cid:62)

xi

The ﬁrst half is clearly Op(n−1/2) due to CLT. For the second half, the estimate
(cid:98)g(j)(x) given by LSLDG converges to g∗(j)(x) for any x in Op(n−1/2) according to
Part One in the proof of Theorem 1 in [8], and ∇x(cid:98)g(j)(x) converges to ∇xg∗(j)(x)
in the same order because the basis functions in ψj(x) are all derivatives of
Gaussian functions. Consequently,

(cid:16)

ϕj(xi)

∇x(cid:98)g(j)(xi)

(cid:17)(cid:62)

xi −

ϕj(xi)

(cid:16)
∇xg∗(j)(xi)

(cid:17)(cid:62)

xi = Op(n−1/2),

1
n

n
(cid:88)

i=1

since ∇x(cid:98)g(j)(x) converges to ∇xg∗(j)(x) for any x in Op(n−1/2), and

(cid:16)

(cid:17)(cid:62)

ϕj(xi)

∇xg∗(j)(xi)

xi − E

(cid:20)
ϕj(x)

(cid:16)

(cid:17)(cid:62)

(cid:21)

∇xg∗(j)(x)

x

= Op(n−1/2)

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

due to CLT, which proves (cid:107)ut(cid:107)2 = Op(n−1/2). Furthermore, we have already
assumed that |uγ| = O(n−1/2). Hence, as n → ∞,

(cid:107) (cid:98)αj − α∗

j (cid:107)2 = Op

(cid:16)

n−1/2(cid:17)

.

Step 4. Finally, for any x, the gap of (cid:98)v(j)(x) and w∗(j)(x) is bounded by

|(cid:98)v(j)(x) − w∗(j)(x)| ≤ (cid:107) (cid:98)αj − α∗

j (cid:107)2 · (cid:107)ϕj(x)(cid:107)2,

where the Cauchy-Schwarz inequality is used. Since the basis functions in ϕj(x)
are again all derivatives of Gaussian functions, (cid:107)ϕj(x)(cid:107)2 must be bounded
uniformly, and then

|(cid:98)v(j)(x) − w∗(j)(x)| ≤ O((cid:107) (cid:98)αj − α∗

j (cid:107)2) = Op

(cid:16)

n−1/2(cid:17)

.

Applying the same argument for all j = 1, . . . , d completes the proof.

12

Whitening-Free Least-Squares Non-Gaussian Component Analysis

4 Experiments

In this section, we experimentally investigate the performance of MIPP, LSNGCA,
and WF-LSNGCA.5

4.1 Conﬁgurations of NGCA Algorithms

MIPP We use the MATLAB script which was used in the original MIPP paper
[3]6. In this script, NGIFs of the form sk
m(z) (m = 1, . . . , 1000, k = 1, . . . , 4) are
used:

m(z) = z3exp
s1

−

(cid:18)

s3
m(z) = sin(bmz),

(cid:19)

,

z2
2σ2
m
s4
m(z) = cos(bmz),

s2
m(z) = tanh (amz) ,

where σm, am, and bm are scalars chosen at the regular intervals from σm ∈ [0.5, 5],
am ∈ [0.05, 5], and bm ∈ [0.05, 4]. The cut-oﬀ threshold τ is set at 1.6 and the
number of FastICA iterations is set at 10 (see Section 2.2).

LSNGCA Following [10], the derivative of the Gaussian kernel is used as the
basis function ψk,j(y) in the linear-in-parameter model (10):

ψk,j(y) = ∂jexp

−

(cid:32)

(cid:107)y − ck(cid:107)2
2σ2
j

(cid:33)

,

where σj > 0 is the Gaussian bandwidth and ck is the Gaussian center randomly
selected from the whitened data samples {yi}n
i=1. The number of basis functions
is set at b = 100. For model selection, 5-fold cross-validation is performed
with respect to the hold-out error of Eq.(9) using 10 candidate values at the
regular intervals in logarithmic scale for Gaussian bandwidth σj ∈ [10−1, 101]
and regularization parameter λj ∈ [10−5, 101].

WF-LSNGCA Similarly to LSNGCA, the derivative of the Gaussian kernel
is used as the basis function ϕk,j(x) in the linear-in-parameter model (16) and
the number of basis functions is set as t = b = 100. For model selection, 5-fold
cross-validation is performed with respect to the hold-out error of Eq.(15) in the
same way as LSNGCA.

4.2 Artiﬁcial Datasets

Let x = (s1, s2, n3, . . . , n10)(cid:62), where s := (s1, s2)(cid:62) are the 2-dimensional non-
Gaussian signal components and n := (n3, . . . , n10)(cid:62) are the 8-dimensional
Gaussian noise components. For the non-Gaussian signal components, we consider
the following four distributions plotted in Figure 1:

5 The source code of the experiments is at https://github.com/hgeno/WFLSNGCA.
6 http://www.ms.k.u-tokyo.ac.jp/software.html

Whitening-Free Least-Squares Non-Gaussian Component Analysis

13

(a) Independent
Gaussian Mix-
ture

(b) Dependent
Super-Gaussian

(c) Dependent
Sub-Gaussian

(d) Dependent
Super- and Sub-
Gaussian

Fig. 1. Distributions of non-Gaussian components.

(a) Independent
Gaussian Mix-
ture

(b) Dependent
Super-Gaussian

(c) Dependent
Sub-Gaussian

(d) Dependent
Super- and Sub-
Gaussian

Fig. 2. Averages and standard deviations of the subspace estimation error as the function
of the condition-number controller r over 50 simulations on artiﬁcial datasets.

(a) Independent Gaussian Mixture:

p(s1, s2) ∝

i=1

2
(cid:89)

(cid:18)

(cid:18)

exp

−

(cid:19)

(si − 3)2
2

(cid:18)

+ exp

−

(si + 3)2
2

(cid:19)(cid:19)

.

(b) Dependent super-Gaussian:

p(s) ∝ exp (− (cid:107)s(cid:107)).

(c) Dependent sub-Gaussian:

p(s) is the uniform distribution on (cid:8)s ∈ R2| (cid:107)s(cid:107) ≤ 1(cid:9).

(d) Dependent super- and sub-Gaussian:

p(s1) ∝ exp (− |s1|) and p(s2) is the uniform distribution on [c, c + 1], where
c = 0 if |s1| ≤ log 2 and c = −1 otherwise.

For the Gaussian noise components, we include a certain parameter r ≥ 0,
which controls the condition number; the larger r is, the more ill-posed the data
covariance matrix is. The detail is described in Appendix A.

We generate n = 2000 samples for each case, and standardize each element
of the data before applying NGCA algorithms. The performance of NGCA
algorithms is measured by the following subspace estimation error :

ε(E, (cid:98)E) :=

(cid:107)(cid:98)ei − ΠE(cid:98)ei(cid:107)2 ,

1
2

2
(cid:88)

i=1

(17)

where E is the true non-Gaussian index space, (cid:98)E is its estimate, ΠE is the
orthogonal projection on E, and {(cid:98)ei}2

i=1 is an orthonormal basis in (cid:98)E.

The averages and the standard derivations of the subspace estimation error
over 50 runs for MIPP, LSNGCA, and WF-LSNGCA are depicted in Figure 2.

14

Whitening-Free Least-Squares Non-Gaussian Component Analysis

(a) The function of sample size.

(b) The function of data dimen-
sion.

Fig. 3. The average CPU time over 50 runs when the Gaussian mixture is used as
non-Gaussian components and the condition-number controller r = 0. The vertical axis
is in logarithmic scale.

This shows that, for all 4 cases, the error of MIPP grows rapidly as r increases.
On the other hand, LSNGCA and WF-LSNGCA perform much stably against the
change in r. However, LSNGCA performs poorly for (a). Overall, WF-LSNGCA is
shown to be much more robust against ill-conditioning than MIPP and LSNGCA.

In terms of the computation time, WF-LSNGCA is less eﬃcient than LSNGCA
and MIPP, but its computation time is still just a few times slower than LSNGCA,
as seen in Figure 3. For this reason, the computational eﬃciency of WF-LSNGCA
would still be acceptable in practice.

4.3 Benchmark Datasets

Finally, we evaluate the performance of NGCA methods using the LIBSVM binary
classiﬁcation benchmark datasets 7 [15]. From each dataset, n points are selected
as training (test) samples so that the number of positive and negative samples are
equal, and datasets are standardized in each dimension. For an m-dimensional
dataset, we append (d − m)-dimensional noise dimensions following the standard
Gaussian distribution so that all datasets have d dimensions. Then we use PCA,
MIPP, LSNGCA, and WF-LSNGCA to obtain m-dimensional expressions, and
apply the support vector machine (SVM) 8 to evaluate the test misclassiﬁcation
rate. As a baseline, we also evaluate the misclassiﬁcation rate by the raw SVM
without dimension reduction.

7 We preprocessed the LIBSVM binary classiﬁcation benchmark datasets as follows:

– vehicle: We convert original labels ‘1’ and ‘2’ to the positive label and original

labels ‘3’ and ‘4’ to the negative label.

– SUSY : We convert original label ‘0’ to the negative label.
– shuttle: We use only the data labeled as ‘1’ and ‘4’ and regard them as positive

and negative labels.

– svmguide1 : We mix the original training and test datasets.

8 We used LIBSVM with MATLAB [15].

Whitening-Free Least-Squares Non-Gaussian Component Analysis

15

The averages and standard deviations of the misclassiﬁcation rate over 50
runs for d = 50, 100 are summarized in Table 2. As can be seen in the table, the
appended Gaussian noise dimensions have negative eﬀects on each classiﬁcation
accuracy, and thus the baseline has relatively high misclassiﬁcation rates. PCA
has overall higher misclassiﬁcation rates than the baseline since a lot of valuable
information for each classiﬁcation problem is lost. Among the NGCA algorithms,
WF-LSNGCA overall compares favorably with the other methods. This means
that it can ﬁnd valuable low-dimensional expressions for each classiﬁcation
problem without harmful eﬀects of a pre-whitening procedure.

Table 2. Averages (and standard deviations in the parentheses) of the misclassiﬁcation
rates for the LIBSVM datasets over 50 runs. The best and comparable algorithms
judged by the two-sample t-test at the signiﬁcance level 5% are expressed as boldface.

Dataset
[m, n]
vehicle
[18, 200]
svmguide3
[21, 200]
svmguide1
[3, 2000]
shuttle
[9, 2000]
SUSY
[18, 2000]
ijcnn1
[22, 2000]

Dataset
[m, n]
vehicle
[18, 200]
svmguide3
[21, 200]
svmguide1
[3, 2000]
shuttle
[9, 2000]
SUSY
[18, 2000]
ijcnn1
[22, 2000]

No Dim.
Red.
0.340
(0.038)
0.342
(0.035)
0.088
(0.007)
0.031
(0.004)
0.238
(0.010)
0.102
(0.007)

d = 50

PCA MIPP

LS
NGCA
0.324
0.328
0.404
(0.044)
(0.034) (0.044)
0.326
0.348
0.341
(0.039)
(0.037) (0.041)
0.058
0.159
0.060
(0.006)
(0.012) (0.005)
0.041
0.024
0.021
(0.015)
(0.007) (0.004)
0.271
0.223
0.229
(0.012) (0.010) (0.012)
0.273
0.084
(0.012) (0.028)

0.061
(0.006)

WF-LS
NGCA
0.286
(0.038)
0.308
(0.036)
0.053
(0.008)
0.007
(0.002)
0.228
(0.010)
0.057
(0.007)

d = 100

No Dim.
Red.
0.380
(0.033)
0.363
(0.034)
0.102
(0.008)
0.038
(0.005)
0.250
(0.010)
0.145
(0.008)

PCA MIPP

LS
NGCA
0.439
(0.045)
0.427
(0.035)
0.088
(0.059)
0.209
(0.080)
0.228

WF-LS
NGCA
0.360
0.432
0.445
(0.051)
(0.033) (0.036)
0.343
0.443
0.367
(0.044)
(0.032) (0.042)
0.067
0.175
0.087
(0.020)
(0.013) (0.013)
0.017
0.065
0.069
(0.005)
(0.013) (0.019)
0.280
0.226
0.234
(0.011) (0.009) (0.011) (0.010)
0.091
0.318
0.107
(0.020)
(0.013) (0.021)

0.101
(0.010)

16

Whitening-Free Least-Squares Non-Gaussian Component Analysis

5 Conclusions

In this paper, we proposed a novel NGCA algorithm which is computationally
eﬃcient, no manual design of non-Gaussian index functions is required, and
pre-whitening is not involved. Through experiments, we demonstrated that the
eﬀectiveness of the proposed method.

References

1. V. N. Vapnik. Statistical Learning Theory, volume 1. Wiley New York, 1998.
2. E. Diederichs, A. Juditsky, A. Nemirovski, and V. Spokoiny. Sparse non Gaussian
component analysis by semideﬁnite programming. Machine learning, 91(2):211–238,
2013.

3. G. Blanchard, M. Kawanabe, M. Sugiyama, V. Spokoiny, and K. Müller. In search
of non-Gaussian components of a high-dimensional distribution. Journal of Machine
Learning Research, 7:247–282, 2006.

4. J. H. Friedman and J. W. Tukey. A projection pursuit algorithm for exploratory

data analysis. IEEE Transactions on Computers, C-23(9):881–890, 1974.

5. A. Hyvärinen and E. Oja. Independent component analysis: algorithms and appli-

cations. Neural Networks, 13(4):411–430, 2000.

6. M. Kawanabe, M. Sugiyama, G. Blanchard, and K. Müller. A new algorithm
of non-Gaussian component analysis with radial kernel functions. Annals of the
Institute of Statistical Mathematics, 59(1):57–75, 2007.

7. E. Diederichs, A. Juditsky, V. Spokoiny, and C. Schütte. Sparse non-Gaussian
component analysis. IEEE Transactions on Information Theory, 56(6):3033–3047,
2010.

8. H. Sasaki, G. Niu, and M. Sugiyama. Non-Gaussian component analysis with
log-density gradient estimation. In Proceedings of the Seventeenth International
Conference on Artiﬁcial Intelligence and Statistics, volume 38, pages 1177–1185,
2016.

9. D. D Cox. A penalty method for nonparametric estimation of the logarithmic
derivative of a density function. Annals of the Institute of Statistical Mathematics,
37(1):271–288, 1985.

10. H. Sasaki, A. Hyvärinen, and M. Sugiyama. Clustering via mode seeking by direct
estimation of the gradient of a log-density. In Machine Learning and Knowledge
Discovery in Databases, pages 19–34. Springer, 2014.

11. F. J Theis and M. Kawanabe. Uniqueness of non-Gaussian subspace analysis.
In Independent Component Analysis and Blind Signal Separation, pages 917–925.
Springer, 2006.

12. A. Hyvärinen. Fast and robust ﬁxed-point algorithms for independent component

analysis. IEEE Transactions on Neural Networks, 10(3):626–634, 1999.

13. F. Bonnans and R. Cominetti. Perturbed optimization in Banach spaces I: A general
theory based on a weak directional constraint qualiﬁcation; II: A theory based on
a strong directional qualiﬁcation condition; III: Semiinﬁnite optimization. SIAM
Journal on Control and Optimization, 34:1151–1171, 1172–1189, and 1555–1567,
1996.

14. F. Bonnans and A. Shapiro. Optimization problems with perturbations, a guided

tour. SIAM Review, 40(2):228–264, 1998.

15. C. Chang and C. Lin. LIBSVM: A library for support vector machines. ACM

Transactions on Intelligent Systems and Technology, 2(3):27, 2011.

Whitening-Free Least-Squares Non-Gaussian Component Analysis

17

Supplementary Materials to Whitening-Free Least-Squares
Non-Gaussian Component Analysis

A Details of Artiﬁcial Datasets

Here, we describe the detail of the artiﬁcial datasets used in Section 4.2. The
noise components n are generated as follows:

1. The n is sampled from the centered Gaussian distribution with covariance
matrix diag(10−2r, 10−2r+4r/7, 10−2r+8r/7, . . . , 102r), where diag(·) denotes
the diagonal matrix.

2. The sampled n is rotated as n(cid:48)(cid:48) ∈ R8by applying the following rotation

matrix R(i,j) for all i, j = 3, . . . , 10 such that i < j:

i,j = − sin(π/4),

R(i,j)
i,i = cos(π/4), R(i,j)
R(i,j)
j,i = sin(π/4), R(i,j)
k,k = 1 (k (cid:54)= i, k (cid:54)= j), R(i,j)
R(i,j)

j,j = cos(π/4),

k,l = 0 (otherwise).

3. The rotated n is normalized for each dimension.

By this construction, increasing r corresponds to increasing the condition number
of the data covariance matrix (see Figure 4). Thus, the larger r is, the more
ill-posed the data covariance matrix is.

Fig. 4. Condition number of the data covariance matrix as a function of experiment
parameter r (with non-Gaussian components generated from the Gaussian mixture).

Whitening-Free Least-Squares
Non-Gaussian Component Analysis

Hiroaki Shiino1, Hiroaki Sasaki2, Gang Niu3, and Masashi Sugiyama4,3

1 Yahoo Japan Corporation
Kioi Tower 1-3 Kioicho, Chiyoda-ku, Tokyo 102-8282, Japan.
2 Nara Institute of Science and Technology
8916-5 Takayama-cho Ikoma, Nara 630-0192, Japan.
3 The University of Tokyo
5-1-5 Kashiwanoha, Kashiwa-shi, Chiba 277-8561, Japan.
4 RIKEN Center for Advanced Intelligence Project
1-4-1 Nihonbashi, Chuo-ku, Tokyo 103-0027, Japan.

Abstract. Non-Gaussian component analysis (NGCA) is an unsuper-
vised linear dimension reduction method that extracts low-dimensional
non-Gaussian “signals” from high-dimensional data contaminated with
Gaussian noise. NGCA can be regarded as a generalization of projec-
tion pursuit (PP) and independent component analysis (ICA) to multi-
dimensional and dependent non-Gaussian components. Indeed, seminal
approaches to NGCA are based on PP and ICA. Recently, a novel NGCA
approach called least-squares NGCA (LSNGCA) has been developed,
which gives a solution analytically through least-squares estimation of log-
density gradients and eigendecomposition. However, since pre-whitening
of data is involved in LSNGCA, it performs unreliably when the data
covariance matrix is ill-conditioned, which is often the case in high-
dimensional data analysis. In this paper, we propose a whitening-free
variant of LSNGCA and experimentally demonstrate its superiority.

Keywords: non-Gaussian component analysis, dimension reduction, un-
supervised learning

1

Introduction

Dimension reduction is a common technique in high-dimensional data analysis to
mitigate the curse of dimensionality [1]. Among various approaches to dimension
reduction, we focus on unsupervised linear dimension reduction in this paper.

It is known that the distribution of randomly projected data is close to Gaus-
sian [2]. Based on this observation, non-Gaussian component analysis (NGCA)
[3] tries to ﬁnd a subspace that contains non-Gaussian signal components so
that Gaussian noise components can be projected out. NGCA is formulated in
an elegant semi-parametric framework and non-Gaussian components can be
extracted without specifying their distributions. Mathematically, NGCA can
be regarded as a generalization of projection pursuit (PP) [4] and independent

7
1
0
2
 
y
a
M
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
9
2
0
1
0
.
3
0
6
1
:
v
i
X
r
a

2

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Table 1. NGCA methods.

MIPP

IMAK

SNGCA LSNGCA

WF-LSNGCA
(proposed)

Manual
NGIF design
Computational
eﬃciency
Pre-whitening

Need

No Need

Need

No Need No Need

Eﬃcient
(iterative)
Need

Ineﬃcient
(iterative)
Need

Ineﬃcient
(iterative)
No Need

Eﬃcient
(analytic)
Need

Eﬃcient
(analytic)
No Need

component analysis (ICA) [5] to multi-dimensional and dependent non-Gaussian
components.

The ﬁrst NGCA algorithm is called multi-index PP (MIPP). PP algorithms
such as FastICA [5] use a non-Gaussian index function (NGIF) to ﬁnd either
a super-Gaussian or sub-Gaussian component. MIPP uses a family of such
NGIFs to ﬁnd multiple non-Gaussian components and apply principal component
analysis (PCA) to extract a non-Gaussian subspace. However, MIPP requires us
to prepare appropriate NGIFs, which is not necessarily straightforward in practice.
Furthermore, MIPP requires pre-whitening of data, which can be unreliable when
the data covariance matrix is ill-conditioned.

To cope with these problems, MIPP has been extended in various ways. The
method called iterative metric adaptation for radial kernel functions (IMAK) [6]
tries to avoid the manual design of NGIFs by learning the NGIFs from data in
the form of radial kernel functions. However, this learning part is computationally
highly expensive and pre-whitening is still necessary. Sparse NGCA (SNGCA)
[7,2] tries to avoid pre-whitening by imposing an appropriate constraint so that
the solution is independent of the data covariance matrix. However, SNGCA
involves semi-deﬁnite programming which is computationally highly demanding,
and NGIFs still need to be manually designed.

Recently, a novel approach to NGCA called least-squares NGCA (LSNGCA)
has been proposed [8]. Based on the gradient of the log-density function, LSNGCA
constructs a vector that belongs to the non-Gaussian subspace from each sample.
Then the method of least-squares log-density gradients (LSLDG) [9,10] is employed
to directly estimate the log-density gradient without density estimation. Finally,
the principal subspace of the set of vectors generated from all samples is extracted
by eigendecomposition. LSNGCA is computationally eﬃcient and no manual
design of NGIFs is involved. However, it still requires pre-whitening of data.

The existing NGCA methods reviewed above are summarized in Table 1.
In this paper, we propose a novel NGCA method that is computationally eﬃ-
cient, no manual design of NGIFs is involved, and no pre-whitening is necessary.
Our proposed method is essentially an extention of LSNGCA so that the co-
variance of data is implicitly handled without explicit pre-whitening or explicit
constraints. Through experiments, we demonstrate that our proposed method,
called whitening-free LSNGCA (WF-LSNGCA), performs very well even when
the data covariance matrix is ill-conditioned.

Whitening-Free Least-Squares Non-Gaussian Component Analysis

3

2 Non-Gaussian Component Analysis

In this section, we formulate the problem of NGCA and review the MIPP and
LSNGCA methods.

2.1 Problem Formulation

Suppose that we are given a set of d-dimensional i.i.d. samples of size n, {xi|xi ∈
Rd}n

i=1, which are generated by the following model:

xi = Asi + ni,

where si ∈ Rm (m ≤ d) is an m-dimensional signal vector independently gener-
ated from an unknown non-Gaussian distribution (we assume that m is known),
ni ∈ Rd is a noise vector independently generated from a centered Gaussian dis-
tribution with an unknown covariance matrix Q, and A ∈ Rd×m is an unknown
mixing matrix of rank m. Under this data generative model, probability density
function p(x) that samples {xi}n
i=1 follow can be expressed in the following
semi-parametric form [3]:

(1)

(2)

p(x) = f (B(cid:62)x)φQ(x),

where f is an unknown smooth positive function on Rm, B ∈ Rd×m is an unknown
linear mapping, φQ is the centered Gaussian density with the covariance matrix
Q, and (cid:62) denotes the transpose. We note that decomposition (2) is not unique;
multiple combinations of B and f can give the same probability density function.
Nevertheless, the following m-dimensional subspace E, called the non-Gaussian
index space, can be determined uniquely [11]:

E = Null(B(cid:62))⊥ = Range(B),

(3)

where Null(B(cid:62)) denotes the null space of B(cid:62), ⊥ denotes the orthogonal comple-
ment, and Range(B) denotes the column space of B.

The goal of NGCA is to estimate the non-Gaussian index space E from

samples {xi}n

i=1.

2.2 Multi-Index Projection Pursuit (MIPP)

MIPP [3] is the ﬁrst algorithm of NGCA.
Let us whiten the samples {xi}n

identity:

i=1 so that their covariance matrix becomes

yi := Σ− 1
where Σ is the covariance matrix of x. In practice, Σ is replaced by the sample
covariance matrix. Then, for an NGIF h, the following vector β(h) was shown to
belong to the non-Gaussian index space E [3]:

2 xi,

β(h) := E [yh(y) − ∇yh(y)] ,

4

Whitening-Free Least-Squares Non-Gaussian Component Analysis

where ∇y denotes the diﬀerential operator w.r.t. y and E[·] denotes the expec-
tation over p(x). MIPP generates a set of such vectors from various NGIFs
{hl}L

l=1:

(cid:98)βl :=

[yihl(yi) − ∇yhl(yi)] ,

(4)

where the expectation is estimated by the sample average. Then (cid:98)βl is normalized
as

(cid:98)βl ← (cid:98)βl

(cid:107)yihl(yi) − ∇yhl(yi)(cid:107)2 − (cid:107)(cid:98)βl(cid:107)2,

(5)

1
n

n
(cid:88)

i=1

(cid:44)(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

by which (cid:107)(cid:98)βl(cid:107) is proportional to its signal-to-noise ratio. Then vectors (cid:98)βl with
their norm less than a pre-speciﬁed threshold τ > 0 are eliminated. Finally, PCA
is applied to the remaining vectors (cid:98)βl to obtain an estimate of the non-Gaussian
index space E.

The behavior of MIPP strongly depends on the choice of NGIF h. To improve
the performance, MIPP actively searches informative h as follows. First, the form
of h is restricted to h(y) = s(w(cid:62)y), where w ∈ Rd denotes a unit-norm vector
and s is a smooth real function. Then, estimated vector (cid:98)β is written as

(cid:98)β =

1
n

n
(cid:88)

i=1

(cid:0)yis(w(cid:62)yi) − s(cid:48)(w(cid:62)yi)w(cid:1) ,

where s(cid:48) is the derivative of s. This equation is actually equivalent to a single
iteration of the PP algorithm called FastICA [12]. Based on this fact, the pa-
rameter w is optimized by iteratively applying the following update rule until
convergence:

w ←

(cid:80)n
(cid:107) (cid:80)n

(cid:0)yis(w(cid:62)yi) − s(cid:48)(w(cid:62)yi)w(cid:1)
i=1
i=1 (yis(w(cid:62)yi) − s(cid:48)(w(cid:62)yi)w) (cid:107)

.

The superiority of MIPP has been investigated both theoretically and experi-
mentally [3]. However, MIPP has the weaknesses that NGIFs should be manually
designed and pre-whitening is necessary.

2.3 Least-Squares Non-Gaussian Component Analysis (LSNGCA)

LSNGCA [8] is a recently proposed NGCA algorithm that does not require
manual design of NGIFs (Table 1). Here the algorithm of LSNGCA is reviewed,
which will be used for further developing a new method in the next section.

Derivation: For whitened samples {yi}n
given in Eq.(2) can be simpliﬁed as

i=1, the semi-parametric form of NGCA

p(y) = (cid:101)f ( (cid:101)B

y)φI d(y),

(cid:62)

(6)

Whitening-Free Least-Squares Non-Gaussian Component Analysis

5

where (cid:101)f is an unknown smooth positive function on Rm and (cid:101)B ∈ Rd×m is
an unknown linear mapping. Under this simpliﬁed semi-parametric form, the
non-Gaussian index space E can be represented as

E = Σ− 1

2 Range( (cid:101)B).

Taking the logarithm and diﬀerentiating the both sides of Eq.(6) w.r.t. y

yield

∇y ln p(y) + y = (cid:101)B∇

(cid:62)

ln (cid:101)f ( (cid:101)B

y),

(cid:62)

y

(cid:101)B

(7)

where ∇

(cid:62)

y

(cid:101)B

denotes the diﬀerential operator w.r.t. (cid:101)B

y. This implies that

(cid:62)

u(y) := ∇y ln p(y) + y

belongs to the non-Gaussian index space E. Then applying eigendecomposition
to (cid:80)n
i=1 u(yi)u(yi)(cid:62) and extracting the m leading eigenvectors allow us to
recover Range( (cid:101)B). In LSNGCA, the method of least-squares log-density gradients
(LSLDG) [9,10] is used to estimate the log-density gradient ∇y ln p(y) included
in u(y), which is brieﬂy reviewed below.

LSLDG: Let ∂j denote the diﬀerential operator w.r.t. the j-th element of y.
LSLDG ﬁts a model g(j)(y) to ∂j ln p(y), the j-th element of log-density gradient
∇y ln p(y), under the squared loss:

J(g(j)) := E[(g(j)(y) − ∂j ln p(y))2] − E[(∂j ln p(y))2]
= E[g(j)(y)2] − 2E[g(j)(y)∂j ln p(y)].

(8)

The second term in Eq.(8) yields

E[g(j)(y)∂j ln p(y)] =

(cid:90)

g(j)(y)(∂j ln p(y))p(y)dy =
(cid:90)

= −

∂jg(j)(y)p(y)dy = −E[∂jg(j)(y)],

(cid:90)

g(j)(y)∂jp(y)dy

where the second-last equation follows from integration by parts under the
assumption lim|y(j)|→∞ g(j)(y)p(y) = 0. Then sample approximation yields

J(g(j)) = E[g(j)(y)2 − 2∂jg(j)(y)] ≈

[g(j)(yi)2 + 2∂jg(j)(yi)].

(9)

1
n

n
(cid:88)

i=1

As a model of the log-density gradient, LSLDG uses a linear-in-parameter form:

g(j)(y) =

θk,jψk,j(y) = θ(cid:62)

j ψj(y),

(10)

b
(cid:88)

k=1

6

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Algorithm 1 Pseudo-code of WF-LSNGCA.
input Element-wise standardized data samples: {xi}n
i=1.
1: Obtain an estimate (cid:98)v(x) of v(x) = ∇x ln p(x) − ∇2
2: Apply eigendecomposition to (cid:80)n

scribed in Section 3.2.

vectors as an orthonormal basis of non-Gaussian index space E.

i=1 (cid:98)v(xi)(cid:98)v(xi)(cid:62) and extract the m leading eigen-

x ln p(x)x by the method de-

where b denotes the number of basis functions, θj := (θ1,j, . . . , θb,j)(cid:62) is a pa-
rameter vector to be estimated, and ψj(y) := (ψ1,j(y), . . . , ψb,j(y))(cid:62) is a basis
function vector. The parameter vector θj is learned by solving the following
regularized empirical optimization problem:

(cid:98)θj = argmin

θj

(cid:104)

j (cid:98)Gjθj + 2θ(cid:62)
θ(cid:62)

j (cid:98)hj + λj(cid:107)θj(cid:107)2(cid:105)

,

where λj > 0 is the regularization parameter,

(cid:98)Gj =

ψj(yi)ψj(yi)(cid:62), (cid:98)hj =

∂jψj(yi).

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

This optimization problem can be analytically solved as

(cid:16)

(cid:17)−1

(cid:98)θj = −

(cid:98)Gj + λjI b

(cid:98)hj,

where I b is the b-by-b identity matrix. Finally, an estimator of the log-density
gradient g(j)(y) is obtained as

(cid:98)g(j)(y) = (cid:98)θ

(cid:62)
j ψj(y).

All tuning parameters such as the regularization parameter λj and parameters
included in the basis function ψk,j(y) can be systematically chosen based on
cross-validation w.r.t. Eq.(9).

3 Whitening-Free LSNGCA

In this section, we propose a novel NGCA algorithm that does not involve pre-
whitening. A pseudo-code of the proposed method, which we call whitening-free
LSNGCA (WF-LSNGCA), is summarized in Algorithm 1.

3.1 Derivation

Unlike LSNGCA which used the simpliﬁed semi-parametric form (6), we directly
use the original semi-parametric form (2) without whitening. Taking the logarithm
and diﬀerentiating the both sides of Eq.(2) w.r.t. x yield

∇x ln p(x) + Q−1x = B∇B(cid:62)x ln f (B(cid:62)x),

(11)

Whitening-Free Least-Squares Non-Gaussian Component Analysis

7

where ∇x denotes the derivative w.r.t. x and ∇B(cid:62)x denotes the derivative
w.r.t. B(cid:62)x. Further taking the derivative of Eq.(11) w.r.t. x yields

Q−1 = −∇2

x ln p(x) + B∇2

B(cid:62)x ln f (B(cid:62)x)B(cid:62),

(12)

x denotes the second derivative w.r.t. x. Substituting Eq.(12) back into

where ∇2
Eq.(11) yields

∇x ln p(x) − ∇2

x ln p(x)x = B

∇B(cid:62)x ln f (B(cid:62)x) − ∇2

(cid:16)

(cid:17)
B(cid:62)x ln f (B(cid:62)x)B(cid:62)x

.

(13)

This implies that

v(x) := ∇x ln p(x) − ∇2

x ln p(x)x

belongs to the non-Gaussian index space E. Then we apply eigendecomposition
to (cid:80)n
i=1 v(xi)v(xi)(cid:62) and extract the m leading eigenvectors as an orthonormal
basis of non-Gaussian index space E.

Now the remaining task is to approximate v(x) from data, which is discussed

below.

3.2 Estimation of v(x)

Let v(j)(x) be the j-th element of v(x):

v(j)(x) = ∂j ln p(x) − (∇x∂j ln p(x))(cid:62) x.

To estimate v(j)(x), let us ﬁt a model w(j)(x) to it under the squared loss:

R(w(j)) := E[(w(j)(x) − v(j)(x))2] − E[v(j)(x)2]

= E[w(j)(x)2] − 2E[w(j)(x)v(j)(x)]
= E[w(j)(x)2] − 2E[w(j)(x)∂j ln p(x)] + 2E[w(j)(x) (∇x∂j ln p(x))(cid:62) x].
(14)

The second term in Eq.(14) yields

E[w(j)(x)∂j ln p(x)] =

(cid:90)

w(j)(x)(∂j ln p(x))p(x)dx =
(cid:90)

= −

∂jw(j)(x)p(x)dx = −E[∂jw(j)(x)],

(cid:90)

w(j)(x)∂jp(x)dx

where the second-last equation follows from integration by parts under the
assumption lim|x(j)|→∞ w(j)(x)p(x) = 0. ∂j ln p(x) included in the third term
in Eq.(14) may be replaced with the LSLDG estimator (cid:98)g(j)(x) reviewed in
Section 2.3. Note that the LSLDG estimator is obtained with non-whitened data
x in this method. Then we have

R(w(j)) ≈ E[w(j)(x)2 + 2∂jw(j)(x) + 2w(j)(x)(∇x(cid:98)g(j)(x))(cid:62)x]

(15)

≈

1
n

n
(cid:88)

i=1

[w(j)(xi)2 + 2∂jw(j)(xi) + 2w(j)(xi)(∇x(cid:98)g(j)(xi))(cid:62)xi].

8

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Here, let us employ the following linear-in-parameter model as w(j)(x):

w(j)(x) :=

αk,jϕk,j(x) = α(cid:62)

j ϕj(x),

(16)

t
(cid:88)

k=1

where t denotes the number of basis functions, αj := (α1,j, . . . , αt,j)(cid:62) is a
parameter vector to be estimated, and ϕj(x) := (ϕ1,j(x), . . . , ϕt,j(x))(cid:62) is a basis
function vector. The parameter vector αj is learned by minimizing the following
regularized empirical optimization problem:

(cid:98)αj = argmin

αj

(cid:104)
α(cid:62)

j (cid:98)Sjαj + 2α(cid:62)

j (cid:98)tj(x) + γj(cid:107)αj(cid:107)2(cid:105)

,

where γj > 0 is the regularization parameter,

(cid:98)Sj =

ϕj(xi)ϕj(xi)(cid:62),

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:18)

(cid:98)tj =

∂jϕj(xi) + ϕj(xi)

(cid:16)

∇x(cid:98)g(j)(xi)

(cid:17)(cid:62)

(cid:19)

xi

.

This optimization problem can be analytically solved as

Finally, an estimator of v(j)(x) is obtained as

(cid:16)

(cid:98)αj = −

(cid:98)Sj + γjI b

(cid:17)−1

(cid:98)tj.

(cid:98)v(j)(x) = (cid:98)α(cid:62)

j ϕj(x).

All tuning parameters such as the regularization parameter γj and parameters
included in the basis function ϕk,j(y) can be systematically chosen based on
cross-validation w.r.t. Eq.(15).

3.3 Theoretical Analysis

Here, we investigate the convergence rate of WF-LSNGCA in a parametric
setting.

Let g∗(x) be the optimal estimate to ∇x ln p(x) given by LSLDG based on

the linear-in-parameter model g(x), and let

j = E (cid:2)ϕj(x)ϕj(x)(cid:62)(cid:3) ,
S∗

j = E
t∗

(cid:16)

(cid:20)
∂jϕj(x) + ϕj(x)

∇xg∗(j)(x)
j α(cid:62)α(cid:9) , w∗(j)(x) = α∗(cid:62)

j ϕj(x),

(cid:17)(cid:62)

(cid:21)

x

,

α∗

j = argminα

(cid:8)α(cid:62)S∗

j α + 2α(cid:62)t∗

j + γ∗
j I b) must be strictly positive deﬁnite. In fact, S∗

j + γ∗

where (S∗
be strictly positive deﬁnite, and thus γ∗
analysis.

j should already
j = 0 is also allowed in our theoretical

We have the following theorem (its proof is given in Section 3.4):

Whitening-Free Least-Squares Non-Gaussian Component Analysis

9

Theorem 1. As n → ∞, for any x,

(cid:107)(cid:98)v(x) − w∗(x)(cid:107)2 = Op

(cid:16)

n−1/2(cid:17)

,

provided that γj for all j converge in O(n−1/2) to γ∗
∞.

j , i.e., limn→∞ n1/2|γj −γ∗

j | <

Theorem 1 is based on the theory of perturbed optimizations [13,14] as well as
the convergence of LSLDG shown in [8]. It guarantees that for any x, the estimate
(cid:98)v(x) in WF-LSNGCA converges to the optimal estimate w∗(x) based on the
linear-in-parameter model w(x), and it achieves the optimal parametric conver-
gence rate Op(n−1/2). Note that Theorem 1 deals only with the estimation error,
and the approximation error is not taken into account. Indeed, approximation
errors exist in two places, from w∗(x) to v(x) in WF-LSNGCA itself and from
g∗(x) to ∇x ln p(x) in the plug-in LSLDG estimator. Since the original LSNGCA
also relies on LSLDG, it cannot avoid the approximation error introduced by
LSLDG. For this reason, the convergence of WF-LSNGCA is expected to be as
good as LSNGCA.

Theorem 1 is basically a theoretical guarantee that is similar to Part One
in the proof of Theorem 1 in [8]. Hence, based on Theorem 1, we can go along
the line of Part Two in the proof of Theorem 1 in [8] and obtain the following
corollary.

Corollary 1. For eigendecomposition, deﬁne matrices (cid:98)Γ = 1
i=1 (cid:98)v(xi)(cid:98)v(xi)(cid:62)
n
and Γ∗ = E[w∗(x)w∗(x)(cid:62)]. Given the estimated subspace (cid:98)E based on n samples
and the optimal estimated subspace E∗ based on inﬁnite data, denote by (cid:98)E ∈ Rd×m
the matrix form of an arbitrary orthonormal basis of (cid:98)E and by E∗ ∈ Rd×m that
of E∗. Deﬁne the distance between subspaces as

(cid:80)n

where (cid:107) · (cid:107)Fro stands for the Frobenius norm. Then, as n → ∞,

D( (cid:98)E, E∗) = inf

(cid:98)E,E∗ (cid:107)(cid:98)E − E∗(cid:107)Fro,

D( (cid:98)E, E∗) = Op

(cid:16)

n−1/2(cid:17)

,

provided that γj for all j converge in O(n−1/2) to γ∗
basis functions such that the ﬁrst m eigenvalues of Γ∗ are neither 0 nor +∞.

j and ϕj(x) are well-chosen

3.4 Proof of Theorem 1

Step 1. First of all, we establish the growth condition (see Deﬁnition 6.1 in [14]).
Denote the expected and empirical objective functions by

j (α) = α(cid:62)S∗

R∗
(cid:98)Rj(α) = α(cid:62) (cid:98)Sjα + 2α(cid:62)

j α + 2α(cid:62)t∗

j α(cid:62)α,
j + γ∗
(cid:98)tj + γjα(cid:62)α.

Then α∗

j = argminα R∗

j (α), (cid:98)αj = argminα (cid:98)Rj(α), and we have

10

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Lemma 1. Let (cid:15)j be the smallest eigenvalue of (S∗
second-order growth condition holds
j (α) ≥ R∗

j ) + (cid:15)j(cid:107)α − α∗

j (α∗

R∗

j (cid:107)2
2.

j + γ∗

j I b), then the following

Proof. R∗

j (α) must be strongly convex with parameter at least 2(cid:15)j. Hence,

R∗

j (α) ≥ R∗
≥ R∗

j (α∗
j (α∗

j ) + (∇R∗
j (α∗
j ) + (cid:15)j(cid:107)α − α∗

j ))(cid:62)(α − α∗
j (cid:107)2
2,

j ) + (α − α∗

j )(cid:62)(S∗

j + γ∗

j I b)(α − α∗
j )

where we used the optimality condition ∇R∗

j (α∗

j ) = 0.

Step 2. Second, we study the stability (with respect to perturbation) of R∗
at α∗

j (α)

j . Let

u = {uS ∈ S b

+, ut ∈ Rb, uγ ∈ R}

+ ⊂ Rb×b is the cone of b-by-
be a set of perturbation parameters, where S b
b symmetric positive semi-deﬁnite matrices. Deﬁne our perturbed objective
function by

Rj(α, u) = α(cid:62)(S∗

j + uS)α + 2α(cid:62)(t∗

j + ut) + (γ∗

j + uγ)α(cid:62)α.

j (α) = Rj(α, 0), and then the stability of R∗

It is clear that R∗
characterized as follows.
Lemma 2. The diﬀerence function Rj(α, u) − R∗
in α modulus

ω(u) = O((cid:107)uS(cid:107)Fro + (cid:107)ut(cid:107)2 + |uγ|)

j (α) at α∗

j can be

j (α) is Lipschitz continuous

on a suﬃciently small neighborhood of α∗
j .

Proof. The diﬀerence function is

Rj(α, u) − R∗

j (α) = α(cid:62)uSα + 2α(cid:62)ut + uγα(cid:62)α,

with a partial gradient

∂
∂α

(Rj(α, u) − R∗

j (α)) = 2uSα + 2ut + 2uγα.

Notice that due to the (cid:96)2-regularization in R∗
Now given a δ-ball of α∗
that ∀α ∈ Bδ(α∗

j , i.e., Bδ(α∗

j ),

j (α), ∃M > 0 such that (cid:107)α∗

j (cid:107)2 ≤ M .
j (cid:107)2 ≤ δ}, it is easy to see

j ) = {α | (cid:107)α − α∗

(cid:107)α(cid:107)2 ≤ (cid:107)α − α∗

j (cid:107)2 + (cid:107)α∗

j (cid:107)2 ≤ δ + M,

and consequently
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∂
∂α

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(Rj(α, u) − R∗

j (α))

≤ 2(δ + M )((cid:107)uS(cid:107)Fro + |uγ|) + 2(cid:107)ut(cid:107)2.

This says that the gradient ∂
j (α)) has a bounded norm of order
O((cid:107)uS(cid:107)Fro + (cid:107)ut(cid:107)2 + |uγ|), and proves that the diﬀerence function Rj(α, u) −
R∗
j ), with a Lipschitz constant of
the same order.

j (α) is Lipschitz continuous on the ball Bδ(α∗

∂α (Rj(α, u) − R∗

Whitening-Free Least-Squares Non-Gaussian Component Analysis

11

Step 3. Lemma 1 ensures the unperturbed objective R∗
α leaves α∗
for α around α∗
suﬀers. Based on Lemma 1, Lemma 2, and Proposition 6.1 in [14],

j (α) grows quickly when
j ; Lemma 2 ensures the perturbed objective Rj(α, u) changes slowly
j , where the slowness is compared with the perturbation u it

(cid:107) (cid:98)αj − α∗

j (cid:107)2 ≤

ω(u)
(cid:15)j

= O((cid:107)uS(cid:107)Fro + (cid:107)ut(cid:107)2 + |uγ|),

since (cid:98)αj is the exact solution to (cid:98)Rj(α) = Rj(α, u) given uS = (cid:98)Sj − S∗
(cid:98)tj − t∗

j , and uγ = γj − γ∗
j .

j , ut =

According to the central limit theorem (CLT), (cid:107)uS(cid:107)Fro = Op(n−1/2). Consider

(cid:98)tj − t∗
j :

(cid:98)tj − t∗

j =

∂jϕj(xi) − E (cid:2)∂jϕj(x)(cid:3) +

n
(cid:88)

1
n

i=1
(cid:20)
ϕj(x)

− E

(cid:16)

∇xg∗(j)(x)

(cid:17)(cid:62)

(cid:21)

x

.

1
n

n
(cid:88)

i=1

(cid:16)

ϕj(xi)

∇x(cid:98)g(j)(xi)

(cid:17)(cid:62)

xi

The ﬁrst half is clearly Op(n−1/2) due to CLT. For the second half, the estimate
(cid:98)g(j)(x) given by LSLDG converges to g∗(j)(x) for any x in Op(n−1/2) according to
Part One in the proof of Theorem 1 in [8], and ∇x(cid:98)g(j)(x) converges to ∇xg∗(j)(x)
in the same order because the basis functions in ψj(x) are all derivatives of
Gaussian functions. Consequently,

(cid:16)

ϕj(xi)

∇x(cid:98)g(j)(xi)

(cid:17)(cid:62)

xi −

ϕj(xi)

(cid:16)
∇xg∗(j)(xi)

(cid:17)(cid:62)

xi = Op(n−1/2),

1
n

n
(cid:88)

i=1

since ∇x(cid:98)g(j)(x) converges to ∇xg∗(j)(x) for any x in Op(n−1/2), and

(cid:16)

(cid:17)(cid:62)

ϕj(xi)

∇xg∗(j)(xi)

xi − E

(cid:20)
ϕj(x)

(cid:16)

(cid:17)(cid:62)

(cid:21)

∇xg∗(j)(x)

x

= Op(n−1/2)

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

due to CLT, which proves (cid:107)ut(cid:107)2 = Op(n−1/2). Furthermore, we have already
assumed that |uγ| = O(n−1/2). Hence, as n → ∞,

(cid:107) (cid:98)αj − α∗

j (cid:107)2 = Op

(cid:16)

n−1/2(cid:17)

.

Step 4. Finally, for any x, the gap of (cid:98)v(j)(x) and w∗(j)(x) is bounded by

|(cid:98)v(j)(x) − w∗(j)(x)| ≤ (cid:107) (cid:98)αj − α∗

j (cid:107)2 · (cid:107)ϕj(x)(cid:107)2,

where the Cauchy-Schwarz inequality is used. Since the basis functions in ϕj(x)
are again all derivatives of Gaussian functions, (cid:107)ϕj(x)(cid:107)2 must be bounded
uniformly, and then

|(cid:98)v(j)(x) − w∗(j)(x)| ≤ O((cid:107) (cid:98)αj − α∗

j (cid:107)2) = Op

(cid:16)

n−1/2(cid:17)

.

Applying the same argument for all j = 1, . . . , d completes the proof.

12

Whitening-Free Least-Squares Non-Gaussian Component Analysis

4 Experiments

In this section, we experimentally investigate the performance of MIPP, LSNGCA,
and WF-LSNGCA.5

4.1 Conﬁgurations of NGCA Algorithms

MIPP We use the MATLAB script which was used in the original MIPP paper
[3]6. In this script, NGIFs of the form sk
m(z) (m = 1, . . . , 1000, k = 1, . . . , 4) are
used:

m(z) = z3exp
s1

−

(cid:18)

s3
m(z) = sin(bmz),

(cid:19)

,

z2
2σ2
m
s4
m(z) = cos(bmz),

s2
m(z) = tanh (amz) ,

where σm, am, and bm are scalars chosen at the regular intervals from σm ∈ [0.5, 5],
am ∈ [0.05, 5], and bm ∈ [0.05, 4]. The cut-oﬀ threshold τ is set at 1.6 and the
number of FastICA iterations is set at 10 (see Section 2.2).

LSNGCA Following [10], the derivative of the Gaussian kernel is used as the
basis function ψk,j(y) in the linear-in-parameter model (10):

ψk,j(y) = ∂jexp

−

(cid:32)

(cid:107)y − ck(cid:107)2
2σ2
j

(cid:33)

,

where σj > 0 is the Gaussian bandwidth and ck is the Gaussian center randomly
selected from the whitened data samples {yi}n
i=1. The number of basis functions
is set at b = 100. For model selection, 5-fold cross-validation is performed
with respect to the hold-out error of Eq.(9) using 10 candidate values at the
regular intervals in logarithmic scale for Gaussian bandwidth σj ∈ [10−1, 101]
and regularization parameter λj ∈ [10−5, 101].

WF-LSNGCA Similarly to LSNGCA, the derivative of the Gaussian kernel
is used as the basis function ϕk,j(x) in the linear-in-parameter model (16) and
the number of basis functions is set as t = b = 100. For model selection, 5-fold
cross-validation is performed with respect to the hold-out error of Eq.(15) in the
same way as LSNGCA.

4.2 Artiﬁcial Datasets

Let x = (s1, s2, n3, . . . , n10)(cid:62), where s := (s1, s2)(cid:62) are the 2-dimensional non-
Gaussian signal components and n := (n3, . . . , n10)(cid:62) are the 8-dimensional
Gaussian noise components. For the non-Gaussian signal components, we consider
the following four distributions plotted in Figure 1:

5 The source code of the experiments is at https://github.com/hgeno/WFLSNGCA.
6 http://www.ms.k.u-tokyo.ac.jp/software.html

Whitening-Free Least-Squares Non-Gaussian Component Analysis

13

(a) Independent
Gaussian Mix-
ture

(b) Dependent
Super-Gaussian

(c) Dependent
Sub-Gaussian

(d) Dependent
Super- and Sub-
Gaussian

Fig. 1. Distributions of non-Gaussian components.

(a) Independent
Gaussian Mix-
ture

(b) Dependent
Super-Gaussian

(c) Dependent
Sub-Gaussian

(d) Dependent
Super- and Sub-
Gaussian

Fig. 2. Averages and standard deviations of the subspace estimation error as the function
of the condition-number controller r over 50 simulations on artiﬁcial datasets.

(a) Independent Gaussian Mixture:

p(s1, s2) ∝

i=1

2
(cid:89)

(cid:18)

(cid:18)

exp

−

(cid:19)

(si − 3)2
2

(cid:18)

+ exp

−

(si + 3)2
2

(cid:19)(cid:19)

.

(b) Dependent super-Gaussian:

p(s) ∝ exp (− (cid:107)s(cid:107)).

(c) Dependent sub-Gaussian:

p(s) is the uniform distribution on (cid:8)s ∈ R2| (cid:107)s(cid:107) ≤ 1(cid:9).

(d) Dependent super- and sub-Gaussian:

p(s1) ∝ exp (− |s1|) and p(s2) is the uniform distribution on [c, c + 1], where
c = 0 if |s1| ≤ log 2 and c = −1 otherwise.

For the Gaussian noise components, we include a certain parameter r ≥ 0,
which controls the condition number; the larger r is, the more ill-posed the data
covariance matrix is. The detail is described in Appendix A.

We generate n = 2000 samples for each case, and standardize each element
of the data before applying NGCA algorithms. The performance of NGCA
algorithms is measured by the following subspace estimation error :

ε(E, (cid:98)E) :=

(cid:107)(cid:98)ei − ΠE(cid:98)ei(cid:107)2 ,

1
2

2
(cid:88)

i=1

(17)

where E is the true non-Gaussian index space, (cid:98)E is its estimate, ΠE is the
orthogonal projection on E, and {(cid:98)ei}2

i=1 is an orthonormal basis in (cid:98)E.

The averages and the standard derivations of the subspace estimation error
over 50 runs for MIPP, LSNGCA, and WF-LSNGCA are depicted in Figure 2.

14

Whitening-Free Least-Squares Non-Gaussian Component Analysis

(a) The function of sample size.

(b) The function of data dimen-
sion.

Fig. 3. The average CPU time over 50 runs when the Gaussian mixture is used as
non-Gaussian components and the condition-number controller r = 0. The vertical axis
is in logarithmic scale.

This shows that, for all 4 cases, the error of MIPP grows rapidly as r increases.
On the other hand, LSNGCA and WF-LSNGCA perform much stably against the
change in r. However, LSNGCA performs poorly for (a). Overall, WF-LSNGCA is
shown to be much more robust against ill-conditioning than MIPP and LSNGCA.

In terms of the computation time, WF-LSNGCA is less eﬃcient than LSNGCA
and MIPP, but its computation time is still just a few times slower than LSNGCA,
as seen in Figure 3. For this reason, the computational eﬃciency of WF-LSNGCA
would still be acceptable in practice.

4.3 Benchmark Datasets

Finally, we evaluate the performance of NGCA methods using the LIBSVM binary
classiﬁcation benchmark datasets 7 [15]. From each dataset, n points are selected
as training (test) samples so that the number of positive and negative samples are
equal, and datasets are standardized in each dimension. For an m-dimensional
dataset, we append (d − m)-dimensional noise dimensions following the standard
Gaussian distribution so that all datasets have d dimensions. Then we use PCA,
MIPP, LSNGCA, and WF-LSNGCA to obtain m-dimensional expressions, and
apply the support vector machine (SVM) 8 to evaluate the test misclassiﬁcation
rate. As a baseline, we also evaluate the misclassiﬁcation rate by the raw SVM
without dimension reduction.

7 We preprocessed the LIBSVM binary classiﬁcation benchmark datasets as follows:

– vehicle: We convert original labels ‘1’ and ‘2’ to the positive label and original

labels ‘3’ and ‘4’ to the negative label.

– SUSY : We convert original label ‘0’ to the negative label.
– shuttle: We use only the data labeled as ‘1’ and ‘4’ and regard them as positive

and negative labels.

– svmguide1 : We mix the original training and test datasets.

8 We used LIBSVM with MATLAB [15].

Whitening-Free Least-Squares Non-Gaussian Component Analysis

15

The averages and standard deviations of the misclassiﬁcation rate over 50
runs for d = 50, 100 are summarized in Table 2. As can be seen in the table, the
appended Gaussian noise dimensions have negative eﬀects on each classiﬁcation
accuracy, and thus the baseline has relatively high misclassiﬁcation rates. PCA
has overall higher misclassiﬁcation rates than the baseline since a lot of valuable
information for each classiﬁcation problem is lost. Among the NGCA algorithms,
WF-LSNGCA overall compares favorably with the other methods. This means
that it can ﬁnd valuable low-dimensional expressions for each classiﬁcation
problem without harmful eﬀects of a pre-whitening procedure.

Table 2. Averages (and standard deviations in the parentheses) of the misclassiﬁcation
rates for the LIBSVM datasets over 50 runs. The best and comparable algorithms
judged by the two-sample t-test at the signiﬁcance level 5% are expressed as boldface.

Dataset
[m, n]
vehicle
[18, 200]
svmguide3
[21, 200]
svmguide1
[3, 2000]
shuttle
[9, 2000]
SUSY
[18, 2000]
ijcnn1
[22, 2000]

Dataset
[m, n]
vehicle
[18, 200]
svmguide3
[21, 200]
svmguide1
[3, 2000]
shuttle
[9, 2000]
SUSY
[18, 2000]
ijcnn1
[22, 2000]

No Dim.
Red.
0.340
(0.038)
0.342
(0.035)
0.088
(0.007)
0.031
(0.004)
0.238
(0.010)
0.102
(0.007)

d = 50

PCA MIPP

LS
NGCA
0.324
0.328
0.404
(0.044)
(0.034) (0.044)
0.326
0.348
0.341
(0.039)
(0.037) (0.041)
0.058
0.159
0.060
(0.006)
(0.012) (0.005)
0.041
0.024
0.021
(0.015)
(0.007) (0.004)
0.271
0.223
0.229
(0.012) (0.010) (0.012)
0.273
0.084
(0.012) (0.028)

0.061
(0.006)

WF-LS
NGCA
0.286
(0.038)
0.308
(0.036)
0.053
(0.008)
0.007
(0.002)
0.228
(0.010)
0.057
(0.007)

d = 100

No Dim.
Red.
0.380
(0.033)
0.363
(0.034)
0.102
(0.008)
0.038
(0.005)
0.250
(0.010)
0.145
(0.008)

PCA MIPP

LS
NGCA
0.439
(0.045)
0.427
(0.035)
0.088
(0.059)
0.209
(0.080)
0.228

WF-LS
NGCA
0.360
0.432
0.445
(0.051)
(0.033) (0.036)
0.343
0.443
0.367
(0.044)
(0.032) (0.042)
0.067
0.175
0.087
(0.020)
(0.013) (0.013)
0.017
0.065
0.069
(0.005)
(0.013) (0.019)
0.280
0.226
0.234
(0.011) (0.009) (0.011) (0.010)
0.091
0.318
0.107
(0.020)
(0.013) (0.021)

0.101
(0.010)

16

Whitening-Free Least-Squares Non-Gaussian Component Analysis

5 Conclusions

In this paper, we proposed a novel NGCA algorithm which is computationally
eﬃcient, no manual design of non-Gaussian index functions is required, and
pre-whitening is not involved. Through experiments, we demonstrated that the
eﬀectiveness of the proposed method.

References

1. V. N. Vapnik. Statistical Learning Theory, volume 1. Wiley New York, 1998.
2. E. Diederichs, A. Juditsky, A. Nemirovski, and V. Spokoiny. Sparse non Gaussian
component analysis by semideﬁnite programming. Machine learning, 91(2):211–238,
2013.

3. G. Blanchard, M. Kawanabe, M. Sugiyama, V. Spokoiny, and K. Müller. In search
of non-Gaussian components of a high-dimensional distribution. Journal of Machine
Learning Research, 7:247–282, 2006.

4. J. H. Friedman and J. W. Tukey. A projection pursuit algorithm for exploratory

data analysis. IEEE Transactions on Computers, C-23(9):881–890, 1974.

5. A. Hyvärinen and E. Oja. Independent component analysis: algorithms and appli-

cations. Neural Networks, 13(4):411–430, 2000.

6. M. Kawanabe, M. Sugiyama, G. Blanchard, and K. Müller. A new algorithm
of non-Gaussian component analysis with radial kernel functions. Annals of the
Institute of Statistical Mathematics, 59(1):57–75, 2007.

7. E. Diederichs, A. Juditsky, V. Spokoiny, and C. Schütte. Sparse non-Gaussian
component analysis. IEEE Transactions on Information Theory, 56(6):3033–3047,
2010.

8. H. Sasaki, G. Niu, and M. Sugiyama. Non-Gaussian component analysis with
log-density gradient estimation. In Proceedings of the Seventeenth International
Conference on Artiﬁcial Intelligence and Statistics, volume 38, pages 1177–1185,
2016.

9. D. D Cox. A penalty method for nonparametric estimation of the logarithmic
derivative of a density function. Annals of the Institute of Statistical Mathematics,
37(1):271–288, 1985.

10. H. Sasaki, A. Hyvärinen, and M. Sugiyama. Clustering via mode seeking by direct
estimation of the gradient of a log-density. In Machine Learning and Knowledge
Discovery in Databases, pages 19–34. Springer, 2014.

11. F. J Theis and M. Kawanabe. Uniqueness of non-Gaussian subspace analysis.
In Independent Component Analysis and Blind Signal Separation, pages 917–925.
Springer, 2006.

12. A. Hyvärinen. Fast and robust ﬁxed-point algorithms for independent component

analysis. IEEE Transactions on Neural Networks, 10(3):626–634, 1999.

13. F. Bonnans and R. Cominetti. Perturbed optimization in Banach spaces I: A general
theory based on a weak directional constraint qualiﬁcation; II: A theory based on
a strong directional qualiﬁcation condition; III: Semiinﬁnite optimization. SIAM
Journal on Control and Optimization, 34:1151–1171, 1172–1189, and 1555–1567,
1996.

14. F. Bonnans and A. Shapiro. Optimization problems with perturbations, a guided

tour. SIAM Review, 40(2):228–264, 1998.

15. C. Chang and C. Lin. LIBSVM: A library for support vector machines. ACM

Transactions on Intelligent Systems and Technology, 2(3):27, 2011.

Whitening-Free Least-Squares Non-Gaussian Component Analysis

17

Supplementary Materials to Whitening-Free Least-Squares
Non-Gaussian Component Analysis

A Details of Artiﬁcial Datasets

Here, we describe the detail of the artiﬁcial datasets used in Section 4.2. The
noise components n are generated as follows:

1. The n is sampled from the centered Gaussian distribution with covariance
matrix diag(10−2r, 10−2r+4r/7, 10−2r+8r/7, . . . , 102r), where diag(·) denotes
the diagonal matrix.

2. The sampled n is rotated as n(cid:48)(cid:48) ∈ R8by applying the following rotation

matrix R(i,j) for all i, j = 3, . . . , 10 such that i < j:

i,j = − sin(π/4),

R(i,j)
i,i = cos(π/4), R(i,j)
R(i,j)
j,i = sin(π/4), R(i,j)
k,k = 1 (k (cid:54)= i, k (cid:54)= j), R(i,j)
R(i,j)

j,j = cos(π/4),

k,l = 0 (otherwise).

3. The rotated n is normalized for each dimension.

By this construction, increasing r corresponds to increasing the condition number
of the data covariance matrix (see Figure 4). Thus, the larger r is, the more
ill-posed the data covariance matrix is.

Fig. 4. Condition number of the data covariance matrix as a function of experiment
parameter r (with non-Gaussian components generated from the Gaussian mixture).

Whitening-Free Least-Squares
Non-Gaussian Component Analysis

Hiroaki Shiino1, Hiroaki Sasaki2, Gang Niu3, and Masashi Sugiyama4,3

1 Yahoo Japan Corporation
Kioi Tower 1-3 Kioicho, Chiyoda-ku, Tokyo 102-8282, Japan.
2 Nara Institute of Science and Technology
8916-5 Takayama-cho Ikoma, Nara 630-0192, Japan.
3 The University of Tokyo
5-1-5 Kashiwanoha, Kashiwa-shi, Chiba 277-8561, Japan.
4 RIKEN Center for Advanced Intelligence Project
1-4-1 Nihonbashi, Chuo-ku, Tokyo 103-0027, Japan.

Abstract. Non-Gaussian component analysis (NGCA) is an unsuper-
vised linear dimension reduction method that extracts low-dimensional
non-Gaussian “signals” from high-dimensional data contaminated with
Gaussian noise. NGCA can be regarded as a generalization of projec-
tion pursuit (PP) and independent component analysis (ICA) to multi-
dimensional and dependent non-Gaussian components. Indeed, seminal
approaches to NGCA are based on PP and ICA. Recently, a novel NGCA
approach called least-squares NGCA (LSNGCA) has been developed,
which gives a solution analytically through least-squares estimation of log-
density gradients and eigendecomposition. However, since pre-whitening
of data is involved in LSNGCA, it performs unreliably when the data
covariance matrix is ill-conditioned, which is often the case in high-
dimensional data analysis. In this paper, we propose a whitening-free
variant of LSNGCA and experimentally demonstrate its superiority.

Keywords: non-Gaussian component analysis, dimension reduction, un-
supervised learning

1

Introduction

Dimension reduction is a common technique in high-dimensional data analysis to
mitigate the curse of dimensionality [1]. Among various approaches to dimension
reduction, we focus on unsupervised linear dimension reduction in this paper.

It is known that the distribution of randomly projected data is close to Gaus-
sian [2]. Based on this observation, non-Gaussian component analysis (NGCA)
[3] tries to ﬁnd a subspace that contains non-Gaussian signal components so
that Gaussian noise components can be projected out. NGCA is formulated in
an elegant semi-parametric framework and non-Gaussian components can be
extracted without specifying their distributions. Mathematically, NGCA can
be regarded as a generalization of projection pursuit (PP) [4] and independent

7
1
0
2
 
y
a
M
 
4
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
9
2
0
1
0
.
3
0
6
1
:
v
i
X
r
a

2

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Table 1. NGCA methods.

MIPP

IMAK

SNGCA LSNGCA

WF-LSNGCA
(proposed)

Manual
NGIF design
Computational
eﬃciency
Pre-whitening

Need

No Need

Need

No Need No Need

Eﬃcient
(iterative)
Need

Ineﬃcient
(iterative)
Need

Ineﬃcient
(iterative)
No Need

Eﬃcient
(analytic)
Need

Eﬃcient
(analytic)
No Need

component analysis (ICA) [5] to multi-dimensional and dependent non-Gaussian
components.

The ﬁrst NGCA algorithm is called multi-index PP (MIPP). PP algorithms
such as FastICA [5] use a non-Gaussian index function (NGIF) to ﬁnd either
a super-Gaussian or sub-Gaussian component. MIPP uses a family of such
NGIFs to ﬁnd multiple non-Gaussian components and apply principal component
analysis (PCA) to extract a non-Gaussian subspace. However, MIPP requires us
to prepare appropriate NGIFs, which is not necessarily straightforward in practice.
Furthermore, MIPP requires pre-whitening of data, which can be unreliable when
the data covariance matrix is ill-conditioned.

To cope with these problems, MIPP has been extended in various ways. The
method called iterative metric adaptation for radial kernel functions (IMAK) [6]
tries to avoid the manual design of NGIFs by learning the NGIFs from data in
the form of radial kernel functions. However, this learning part is computationally
highly expensive and pre-whitening is still necessary. Sparse NGCA (SNGCA)
[7,2] tries to avoid pre-whitening by imposing an appropriate constraint so that
the solution is independent of the data covariance matrix. However, SNGCA
involves semi-deﬁnite programming which is computationally highly demanding,
and NGIFs still need to be manually designed.

Recently, a novel approach to NGCA called least-squares NGCA (LSNGCA)
has been proposed [8]. Based on the gradient of the log-density function, LSNGCA
constructs a vector that belongs to the non-Gaussian subspace from each sample.
Then the method of least-squares log-density gradients (LSLDG) [9,10] is employed
to directly estimate the log-density gradient without density estimation. Finally,
the principal subspace of the set of vectors generated from all samples is extracted
by eigendecomposition. LSNGCA is computationally eﬃcient and no manual
design of NGIFs is involved. However, it still requires pre-whitening of data.

The existing NGCA methods reviewed above are summarized in Table 1.
In this paper, we propose a novel NGCA method that is computationally eﬃ-
cient, no manual design of NGIFs is involved, and no pre-whitening is necessary.
Our proposed method is essentially an extention of LSNGCA so that the co-
variance of data is implicitly handled without explicit pre-whitening or explicit
constraints. Through experiments, we demonstrate that our proposed method,
called whitening-free LSNGCA (WF-LSNGCA), performs very well even when
the data covariance matrix is ill-conditioned.

Whitening-Free Least-Squares Non-Gaussian Component Analysis

3

2 Non-Gaussian Component Analysis

In this section, we formulate the problem of NGCA and review the MIPP and
LSNGCA methods.

2.1 Problem Formulation

Suppose that we are given a set of d-dimensional i.i.d. samples of size n, {xi|xi ∈
Rd}n

i=1, which are generated by the following model:

xi = Asi + ni,

where si ∈ Rm (m ≤ d) is an m-dimensional signal vector independently gener-
ated from an unknown non-Gaussian distribution (we assume that m is known),
ni ∈ Rd is a noise vector independently generated from a centered Gaussian dis-
tribution with an unknown covariance matrix Q, and A ∈ Rd×m is an unknown
mixing matrix of rank m. Under this data generative model, probability density
function p(x) that samples {xi}n
i=1 follow can be expressed in the following
semi-parametric form [3]:

(1)

(2)

p(x) = f (B(cid:62)x)φQ(x),

where f is an unknown smooth positive function on Rm, B ∈ Rd×m is an unknown
linear mapping, φQ is the centered Gaussian density with the covariance matrix
Q, and (cid:62) denotes the transpose. We note that decomposition (2) is not unique;
multiple combinations of B and f can give the same probability density function.
Nevertheless, the following m-dimensional subspace E, called the non-Gaussian
index space, can be determined uniquely [11]:

E = Null(B(cid:62))⊥ = Range(B),

(3)

where Null(B(cid:62)) denotes the null space of B(cid:62), ⊥ denotes the orthogonal comple-
ment, and Range(B) denotes the column space of B.

The goal of NGCA is to estimate the non-Gaussian index space E from

samples {xi}n

i=1.

2.2 Multi-Index Projection Pursuit (MIPP)

MIPP [3] is the ﬁrst algorithm of NGCA.
Let us whiten the samples {xi}n

identity:

i=1 so that their covariance matrix becomes

yi := Σ− 1
where Σ is the covariance matrix of x. In practice, Σ is replaced by the sample
covariance matrix. Then, for an NGIF h, the following vector β(h) was shown to
belong to the non-Gaussian index space E [3]:

2 xi,

β(h) := E [yh(y) − ∇yh(y)] ,

4

Whitening-Free Least-Squares Non-Gaussian Component Analysis

where ∇y denotes the diﬀerential operator w.r.t. y and E[·] denotes the expec-
tation over p(x). MIPP generates a set of such vectors from various NGIFs
{hl}L

l=1:

(cid:98)βl :=

[yihl(yi) − ∇yhl(yi)] ,

(4)

where the expectation is estimated by the sample average. Then (cid:98)βl is normalized
as

(cid:98)βl ← (cid:98)βl

(cid:107)yihl(yi) − ∇yhl(yi)(cid:107)2 − (cid:107)(cid:98)βl(cid:107)2,

(5)

1
n

n
(cid:88)

i=1

(cid:44)(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

by which (cid:107)(cid:98)βl(cid:107) is proportional to its signal-to-noise ratio. Then vectors (cid:98)βl with
their norm less than a pre-speciﬁed threshold τ > 0 are eliminated. Finally, PCA
is applied to the remaining vectors (cid:98)βl to obtain an estimate of the non-Gaussian
index space E.

The behavior of MIPP strongly depends on the choice of NGIF h. To improve
the performance, MIPP actively searches informative h as follows. First, the form
of h is restricted to h(y) = s(w(cid:62)y), where w ∈ Rd denotes a unit-norm vector
and s is a smooth real function. Then, estimated vector (cid:98)β is written as

(cid:98)β =

1
n

n
(cid:88)

i=1

(cid:0)yis(w(cid:62)yi) − s(cid:48)(w(cid:62)yi)w(cid:1) ,

where s(cid:48) is the derivative of s. This equation is actually equivalent to a single
iteration of the PP algorithm called FastICA [12]. Based on this fact, the pa-
rameter w is optimized by iteratively applying the following update rule until
convergence:

w ←

(cid:80)n
(cid:107) (cid:80)n

(cid:0)yis(w(cid:62)yi) − s(cid:48)(w(cid:62)yi)w(cid:1)
i=1
i=1 (yis(w(cid:62)yi) − s(cid:48)(w(cid:62)yi)w) (cid:107)

.

The superiority of MIPP has been investigated both theoretically and experi-
mentally [3]. However, MIPP has the weaknesses that NGIFs should be manually
designed and pre-whitening is necessary.

2.3 Least-Squares Non-Gaussian Component Analysis (LSNGCA)

LSNGCA [8] is a recently proposed NGCA algorithm that does not require
manual design of NGIFs (Table 1). Here the algorithm of LSNGCA is reviewed,
which will be used for further developing a new method in the next section.

Derivation: For whitened samples {yi}n
given in Eq.(2) can be simpliﬁed as

i=1, the semi-parametric form of NGCA

p(y) = (cid:101)f ( (cid:101)B

y)φI d(y),

(cid:62)

(6)

Whitening-Free Least-Squares Non-Gaussian Component Analysis

5

where (cid:101)f is an unknown smooth positive function on Rm and (cid:101)B ∈ Rd×m is
an unknown linear mapping. Under this simpliﬁed semi-parametric form, the
non-Gaussian index space E can be represented as

E = Σ− 1

2 Range( (cid:101)B).

Taking the logarithm and diﬀerentiating the both sides of Eq.(6) w.r.t. y

yield

∇y ln p(y) + y = (cid:101)B∇

(cid:62)

ln (cid:101)f ( (cid:101)B

y),

(cid:62)

y

(cid:101)B

(7)

where ∇

(cid:62)

y

(cid:101)B

denotes the diﬀerential operator w.r.t. (cid:101)B

y. This implies that

(cid:62)

u(y) := ∇y ln p(y) + y

belongs to the non-Gaussian index space E. Then applying eigendecomposition
to (cid:80)n
i=1 u(yi)u(yi)(cid:62) and extracting the m leading eigenvectors allow us to
recover Range( (cid:101)B). In LSNGCA, the method of least-squares log-density gradients
(LSLDG) [9,10] is used to estimate the log-density gradient ∇y ln p(y) included
in u(y), which is brieﬂy reviewed below.

LSLDG: Let ∂j denote the diﬀerential operator w.r.t. the j-th element of y.
LSLDG ﬁts a model g(j)(y) to ∂j ln p(y), the j-th element of log-density gradient
∇y ln p(y), under the squared loss:

J(g(j)) := E[(g(j)(y) − ∂j ln p(y))2] − E[(∂j ln p(y))2]
= E[g(j)(y)2] − 2E[g(j)(y)∂j ln p(y)].

(8)

The second term in Eq.(8) yields

E[g(j)(y)∂j ln p(y)] =

(cid:90)

g(j)(y)(∂j ln p(y))p(y)dy =
(cid:90)

= −

∂jg(j)(y)p(y)dy = −E[∂jg(j)(y)],

(cid:90)

g(j)(y)∂jp(y)dy

where the second-last equation follows from integration by parts under the
assumption lim|y(j)|→∞ g(j)(y)p(y) = 0. Then sample approximation yields

J(g(j)) = E[g(j)(y)2 − 2∂jg(j)(y)] ≈

[g(j)(yi)2 + 2∂jg(j)(yi)].

(9)

1
n

n
(cid:88)

i=1

As a model of the log-density gradient, LSLDG uses a linear-in-parameter form:

g(j)(y) =

θk,jψk,j(y) = θ(cid:62)

j ψj(y),

(10)

b
(cid:88)

k=1

6

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Algorithm 1 Pseudo-code of WF-LSNGCA.
input Element-wise standardized data samples: {xi}n
i=1.
1: Obtain an estimate (cid:98)v(x) of v(x) = ∇x ln p(x) − ∇2
2: Apply eigendecomposition to (cid:80)n

scribed in Section 3.2.

vectors as an orthonormal basis of non-Gaussian index space E.

i=1 (cid:98)v(xi)(cid:98)v(xi)(cid:62) and extract the m leading eigen-

x ln p(x)x by the method de-

where b denotes the number of basis functions, θj := (θ1,j, . . . , θb,j)(cid:62) is a pa-
rameter vector to be estimated, and ψj(y) := (ψ1,j(y), . . . , ψb,j(y))(cid:62) is a basis
function vector. The parameter vector θj is learned by solving the following
regularized empirical optimization problem:

(cid:98)θj = argmin

θj

(cid:104)

j (cid:98)Gjθj + 2θ(cid:62)
θ(cid:62)

j (cid:98)hj + λj(cid:107)θj(cid:107)2(cid:105)

,

where λj > 0 is the regularization parameter,

(cid:98)Gj =

ψj(yi)ψj(yi)(cid:62), (cid:98)hj =

∂jψj(yi).

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

This optimization problem can be analytically solved as

(cid:16)

(cid:17)−1

(cid:98)θj = −

(cid:98)Gj + λjI b

(cid:98)hj,

where I b is the b-by-b identity matrix. Finally, an estimator of the log-density
gradient g(j)(y) is obtained as

(cid:98)g(j)(y) = (cid:98)θ

(cid:62)
j ψj(y).

All tuning parameters such as the regularization parameter λj and parameters
included in the basis function ψk,j(y) can be systematically chosen based on
cross-validation w.r.t. Eq.(9).

3 Whitening-Free LSNGCA

In this section, we propose a novel NGCA algorithm that does not involve pre-
whitening. A pseudo-code of the proposed method, which we call whitening-free
LSNGCA (WF-LSNGCA), is summarized in Algorithm 1.

3.1 Derivation

Unlike LSNGCA which used the simpliﬁed semi-parametric form (6), we directly
use the original semi-parametric form (2) without whitening. Taking the logarithm
and diﬀerentiating the both sides of Eq.(2) w.r.t. x yield

∇x ln p(x) + Q−1x = B∇B(cid:62)x ln f (B(cid:62)x),

(11)

Whitening-Free Least-Squares Non-Gaussian Component Analysis

7

where ∇x denotes the derivative w.r.t. x and ∇B(cid:62)x denotes the derivative
w.r.t. B(cid:62)x. Further taking the derivative of Eq.(11) w.r.t. x yields

Q−1 = −∇2

x ln p(x) + B∇2

B(cid:62)x ln f (B(cid:62)x)B(cid:62),

(12)

x denotes the second derivative w.r.t. x. Substituting Eq.(12) back into

where ∇2
Eq.(11) yields

∇x ln p(x) − ∇2

x ln p(x)x = B

∇B(cid:62)x ln f (B(cid:62)x) − ∇2

(cid:16)

(cid:17)
B(cid:62)x ln f (B(cid:62)x)B(cid:62)x

.

(13)

This implies that

v(x) := ∇x ln p(x) − ∇2

x ln p(x)x

belongs to the non-Gaussian index space E. Then we apply eigendecomposition
to (cid:80)n
i=1 v(xi)v(xi)(cid:62) and extract the m leading eigenvectors as an orthonormal
basis of non-Gaussian index space E.

Now the remaining task is to approximate v(x) from data, which is discussed

below.

3.2 Estimation of v(x)

Let v(j)(x) be the j-th element of v(x):

v(j)(x) = ∂j ln p(x) − (∇x∂j ln p(x))(cid:62) x.

To estimate v(j)(x), let us ﬁt a model w(j)(x) to it under the squared loss:

R(w(j)) := E[(w(j)(x) − v(j)(x))2] − E[v(j)(x)2]

= E[w(j)(x)2] − 2E[w(j)(x)v(j)(x)]
= E[w(j)(x)2] − 2E[w(j)(x)∂j ln p(x)] + 2E[w(j)(x) (∇x∂j ln p(x))(cid:62) x].
(14)

The second term in Eq.(14) yields

E[w(j)(x)∂j ln p(x)] =

(cid:90)

w(j)(x)(∂j ln p(x))p(x)dx =
(cid:90)

= −

∂jw(j)(x)p(x)dx = −E[∂jw(j)(x)],

(cid:90)

w(j)(x)∂jp(x)dx

where the second-last equation follows from integration by parts under the
assumption lim|x(j)|→∞ w(j)(x)p(x) = 0. ∂j ln p(x) included in the third term
in Eq.(14) may be replaced with the LSLDG estimator (cid:98)g(j)(x) reviewed in
Section 2.3. Note that the LSLDG estimator is obtained with non-whitened data
x in this method. Then we have

R(w(j)) ≈ E[w(j)(x)2 + 2∂jw(j)(x) + 2w(j)(x)(∇x(cid:98)g(j)(x))(cid:62)x]

(15)

≈

1
n

n
(cid:88)

i=1

[w(j)(xi)2 + 2∂jw(j)(xi) + 2w(j)(xi)(∇x(cid:98)g(j)(xi))(cid:62)xi].

8

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Here, let us employ the following linear-in-parameter model as w(j)(x):

w(j)(x) :=

αk,jϕk,j(x) = α(cid:62)

j ϕj(x),

(16)

t
(cid:88)

k=1

where t denotes the number of basis functions, αj := (α1,j, . . . , αt,j)(cid:62) is a
parameter vector to be estimated, and ϕj(x) := (ϕ1,j(x), . . . , ϕt,j(x))(cid:62) is a basis
function vector. The parameter vector αj is learned by minimizing the following
regularized empirical optimization problem:

(cid:98)αj = argmin

αj

(cid:104)
α(cid:62)

j (cid:98)Sjαj + 2α(cid:62)

j (cid:98)tj(x) + γj(cid:107)αj(cid:107)2(cid:105)

,

where γj > 0 is the regularization parameter,

(cid:98)Sj =

ϕj(xi)ϕj(xi)(cid:62),

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:18)

(cid:98)tj =

∂jϕj(xi) + ϕj(xi)

(cid:16)

∇x(cid:98)g(j)(xi)

(cid:17)(cid:62)

(cid:19)

xi

.

This optimization problem can be analytically solved as

Finally, an estimator of v(j)(x) is obtained as

(cid:16)

(cid:98)αj = −

(cid:98)Sj + γjI b

(cid:17)−1

(cid:98)tj.

(cid:98)v(j)(x) = (cid:98)α(cid:62)

j ϕj(x).

All tuning parameters such as the regularization parameter γj and parameters
included in the basis function ϕk,j(y) can be systematically chosen based on
cross-validation w.r.t. Eq.(15).

3.3 Theoretical Analysis

Here, we investigate the convergence rate of WF-LSNGCA in a parametric
setting.

Let g∗(x) be the optimal estimate to ∇x ln p(x) given by LSLDG based on

the linear-in-parameter model g(x), and let

j = E (cid:2)ϕj(x)ϕj(x)(cid:62)(cid:3) ,
S∗

j = E
t∗

(cid:16)

(cid:20)
∂jϕj(x) + ϕj(x)

∇xg∗(j)(x)
j α(cid:62)α(cid:9) , w∗(j)(x) = α∗(cid:62)

j ϕj(x),

(cid:17)(cid:62)

(cid:21)

x

,

α∗

j = argminα

(cid:8)α(cid:62)S∗

j α + 2α(cid:62)t∗

j + γ∗
j I b) must be strictly positive deﬁnite. In fact, S∗

j + γ∗

where (S∗
be strictly positive deﬁnite, and thus γ∗
analysis.

j should already
j = 0 is also allowed in our theoretical

We have the following theorem (its proof is given in Section 3.4):

Whitening-Free Least-Squares Non-Gaussian Component Analysis

9

Theorem 1. As n → ∞, for any x,

(cid:107)(cid:98)v(x) − w∗(x)(cid:107)2 = Op

(cid:16)

n−1/2(cid:17)

,

provided that γj for all j converge in O(n−1/2) to γ∗
∞.

j , i.e., limn→∞ n1/2|γj −γ∗

j | <

Theorem 1 is based on the theory of perturbed optimizations [13,14] as well as
the convergence of LSLDG shown in [8]. It guarantees that for any x, the estimate
(cid:98)v(x) in WF-LSNGCA converges to the optimal estimate w∗(x) based on the
linear-in-parameter model w(x), and it achieves the optimal parametric conver-
gence rate Op(n−1/2). Note that Theorem 1 deals only with the estimation error,
and the approximation error is not taken into account. Indeed, approximation
errors exist in two places, from w∗(x) to v(x) in WF-LSNGCA itself and from
g∗(x) to ∇x ln p(x) in the plug-in LSLDG estimator. Since the original LSNGCA
also relies on LSLDG, it cannot avoid the approximation error introduced by
LSLDG. For this reason, the convergence of WF-LSNGCA is expected to be as
good as LSNGCA.

Theorem 1 is basically a theoretical guarantee that is similar to Part One
in the proof of Theorem 1 in [8]. Hence, based on Theorem 1, we can go along
the line of Part Two in the proof of Theorem 1 in [8] and obtain the following
corollary.

Corollary 1. For eigendecomposition, deﬁne matrices (cid:98)Γ = 1
i=1 (cid:98)v(xi)(cid:98)v(xi)(cid:62)
n
and Γ∗ = E[w∗(x)w∗(x)(cid:62)]. Given the estimated subspace (cid:98)E based on n samples
and the optimal estimated subspace E∗ based on inﬁnite data, denote by (cid:98)E ∈ Rd×m
the matrix form of an arbitrary orthonormal basis of (cid:98)E and by E∗ ∈ Rd×m that
of E∗. Deﬁne the distance between subspaces as

(cid:80)n

where (cid:107) · (cid:107)Fro stands for the Frobenius norm. Then, as n → ∞,

D( (cid:98)E, E∗) = inf

(cid:98)E,E∗ (cid:107)(cid:98)E − E∗(cid:107)Fro,

D( (cid:98)E, E∗) = Op

(cid:16)

n−1/2(cid:17)

,

provided that γj for all j converge in O(n−1/2) to γ∗
basis functions such that the ﬁrst m eigenvalues of Γ∗ are neither 0 nor +∞.

j and ϕj(x) are well-chosen

3.4 Proof of Theorem 1

Step 1. First of all, we establish the growth condition (see Deﬁnition 6.1 in [14]).
Denote the expected and empirical objective functions by

j (α) = α(cid:62)S∗

R∗
(cid:98)Rj(α) = α(cid:62) (cid:98)Sjα + 2α(cid:62)

j α + 2α(cid:62)t∗

j α(cid:62)α,
j + γ∗
(cid:98)tj + γjα(cid:62)α.

Then α∗

j = argminα R∗

j (α), (cid:98)αj = argminα (cid:98)Rj(α), and we have

10

Whitening-Free Least-Squares Non-Gaussian Component Analysis

Lemma 1. Let (cid:15)j be the smallest eigenvalue of (S∗
second-order growth condition holds
j (α) ≥ R∗

j ) + (cid:15)j(cid:107)α − α∗

j (α∗

R∗

j (cid:107)2
2.

j + γ∗

j I b), then the following

Proof. R∗

j (α) must be strongly convex with parameter at least 2(cid:15)j. Hence,

R∗

j (α) ≥ R∗
≥ R∗

j (α∗
j (α∗

j ) + (∇R∗
j (α∗
j ) + (cid:15)j(cid:107)α − α∗

j ))(cid:62)(α − α∗
j (cid:107)2
2,

j ) + (α − α∗

j )(cid:62)(S∗

j + γ∗

j I b)(α − α∗
j )

where we used the optimality condition ∇R∗

j (α∗

j ) = 0.

Step 2. Second, we study the stability (with respect to perturbation) of R∗
at α∗

j (α)

j . Let

u = {uS ∈ S b

+, ut ∈ Rb, uγ ∈ R}

+ ⊂ Rb×b is the cone of b-by-
be a set of perturbation parameters, where S b
b symmetric positive semi-deﬁnite matrices. Deﬁne our perturbed objective
function by

Rj(α, u) = α(cid:62)(S∗

j + uS)α + 2α(cid:62)(t∗

j + ut) + (γ∗

j + uγ)α(cid:62)α.

j (α) = Rj(α, 0), and then the stability of R∗

It is clear that R∗
characterized as follows.
Lemma 2. The diﬀerence function Rj(α, u) − R∗
in α modulus

ω(u) = O((cid:107)uS(cid:107)Fro + (cid:107)ut(cid:107)2 + |uγ|)

j (α) at α∗

j can be

j (α) is Lipschitz continuous

on a suﬃciently small neighborhood of α∗
j .

Proof. The diﬀerence function is

Rj(α, u) − R∗

j (α) = α(cid:62)uSα + 2α(cid:62)ut + uγα(cid:62)α,

with a partial gradient

∂
∂α

(Rj(α, u) − R∗

j (α)) = 2uSα + 2ut + 2uγα.

Notice that due to the (cid:96)2-regularization in R∗
Now given a δ-ball of α∗
that ∀α ∈ Bδ(α∗

j , i.e., Bδ(α∗

j ),

j (α), ∃M > 0 such that (cid:107)α∗

j (cid:107)2 ≤ M .
j (cid:107)2 ≤ δ}, it is easy to see

j ) = {α | (cid:107)α − α∗

(cid:107)α(cid:107)2 ≤ (cid:107)α − α∗

j (cid:107)2 + (cid:107)α∗

j (cid:107)2 ≤ δ + M,

and consequently
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∂
∂α

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(Rj(α, u) − R∗

j (α))

≤ 2(δ + M )((cid:107)uS(cid:107)Fro + |uγ|) + 2(cid:107)ut(cid:107)2.

This says that the gradient ∂
j (α)) has a bounded norm of order
O((cid:107)uS(cid:107)Fro + (cid:107)ut(cid:107)2 + |uγ|), and proves that the diﬀerence function Rj(α, u) −
R∗
j ), with a Lipschitz constant of
the same order.

j (α) is Lipschitz continuous on the ball Bδ(α∗

∂α (Rj(α, u) − R∗

Whitening-Free Least-Squares Non-Gaussian Component Analysis

11

Step 3. Lemma 1 ensures the unperturbed objective R∗
α leaves α∗
for α around α∗
suﬀers. Based on Lemma 1, Lemma 2, and Proposition 6.1 in [14],

j (α) grows quickly when
j ; Lemma 2 ensures the perturbed objective Rj(α, u) changes slowly
j , where the slowness is compared with the perturbation u it

(cid:107) (cid:98)αj − α∗

j (cid:107)2 ≤

ω(u)
(cid:15)j

= O((cid:107)uS(cid:107)Fro + (cid:107)ut(cid:107)2 + |uγ|),

since (cid:98)αj is the exact solution to (cid:98)Rj(α) = Rj(α, u) given uS = (cid:98)Sj − S∗
(cid:98)tj − t∗

j , and uγ = γj − γ∗
j .

j , ut =

According to the central limit theorem (CLT), (cid:107)uS(cid:107)Fro = Op(n−1/2). Consider

(cid:98)tj − t∗
j :

(cid:98)tj − t∗

j =

∂jϕj(xi) − E (cid:2)∂jϕj(x)(cid:3) +

n
(cid:88)

1
n

i=1
(cid:20)
ϕj(x)

− E

(cid:16)

∇xg∗(j)(x)

(cid:17)(cid:62)

(cid:21)

x

.

1
n

n
(cid:88)

i=1

(cid:16)

ϕj(xi)

∇x(cid:98)g(j)(xi)

(cid:17)(cid:62)

xi

The ﬁrst half is clearly Op(n−1/2) due to CLT. For the second half, the estimate
(cid:98)g(j)(x) given by LSLDG converges to g∗(j)(x) for any x in Op(n−1/2) according to
Part One in the proof of Theorem 1 in [8], and ∇x(cid:98)g(j)(x) converges to ∇xg∗(j)(x)
in the same order because the basis functions in ψj(x) are all derivatives of
Gaussian functions. Consequently,

(cid:16)

ϕj(xi)

∇x(cid:98)g(j)(xi)

(cid:17)(cid:62)

xi −

ϕj(xi)

(cid:16)
∇xg∗(j)(xi)

(cid:17)(cid:62)

xi = Op(n−1/2),

1
n

n
(cid:88)

i=1

since ∇x(cid:98)g(j)(x) converges to ∇xg∗(j)(x) for any x in Op(n−1/2), and

(cid:16)

(cid:17)(cid:62)

ϕj(xi)

∇xg∗(j)(xi)

xi − E

(cid:20)
ϕj(x)

(cid:16)

(cid:17)(cid:62)

(cid:21)

∇xg∗(j)(x)

x

= Op(n−1/2)

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

due to CLT, which proves (cid:107)ut(cid:107)2 = Op(n−1/2). Furthermore, we have already
assumed that |uγ| = O(n−1/2). Hence, as n → ∞,

(cid:107) (cid:98)αj − α∗

j (cid:107)2 = Op

(cid:16)

n−1/2(cid:17)

.

Step 4. Finally, for any x, the gap of (cid:98)v(j)(x) and w∗(j)(x) is bounded by

|(cid:98)v(j)(x) − w∗(j)(x)| ≤ (cid:107) (cid:98)αj − α∗

j (cid:107)2 · (cid:107)ϕj(x)(cid:107)2,

where the Cauchy-Schwarz inequality is used. Since the basis functions in ϕj(x)
are again all derivatives of Gaussian functions, (cid:107)ϕj(x)(cid:107)2 must be bounded
uniformly, and then

|(cid:98)v(j)(x) − w∗(j)(x)| ≤ O((cid:107) (cid:98)αj − α∗

j (cid:107)2) = Op

(cid:16)

n−1/2(cid:17)

.

Applying the same argument for all j = 1, . . . , d completes the proof.

12

Whitening-Free Least-Squares Non-Gaussian Component Analysis

4 Experiments

In this section, we experimentally investigate the performance of MIPP, LSNGCA,
and WF-LSNGCA.5

4.1 Conﬁgurations of NGCA Algorithms

MIPP We use the MATLAB script which was used in the original MIPP paper
[3]6. In this script, NGIFs of the form sk
m(z) (m = 1, . . . , 1000, k = 1, . . . , 4) are
used:

m(z) = z3exp
s1

−

(cid:18)

s3
m(z) = sin(bmz),

(cid:19)

,

z2
2σ2
m
s4
m(z) = cos(bmz),

s2
m(z) = tanh (amz) ,

where σm, am, and bm are scalars chosen at the regular intervals from σm ∈ [0.5, 5],
am ∈ [0.05, 5], and bm ∈ [0.05, 4]. The cut-oﬀ threshold τ is set at 1.6 and the
number of FastICA iterations is set at 10 (see Section 2.2).

LSNGCA Following [10], the derivative of the Gaussian kernel is used as the
basis function ψk,j(y) in the linear-in-parameter model (10):

ψk,j(y) = ∂jexp

−

(cid:32)

(cid:107)y − ck(cid:107)2
2σ2
j

(cid:33)

,

where σj > 0 is the Gaussian bandwidth and ck is the Gaussian center randomly
selected from the whitened data samples {yi}n
i=1. The number of basis functions
is set at b = 100. For model selection, 5-fold cross-validation is performed
with respect to the hold-out error of Eq.(9) using 10 candidate values at the
regular intervals in logarithmic scale for Gaussian bandwidth σj ∈ [10−1, 101]
and regularization parameter λj ∈ [10−5, 101].

WF-LSNGCA Similarly to LSNGCA, the derivative of the Gaussian kernel
is used as the basis function ϕk,j(x) in the linear-in-parameter model (16) and
the number of basis functions is set as t = b = 100. For model selection, 5-fold
cross-validation is performed with respect to the hold-out error of Eq.(15) in the
same way as LSNGCA.

4.2 Artiﬁcial Datasets

Let x = (s1, s2, n3, . . . , n10)(cid:62), where s := (s1, s2)(cid:62) are the 2-dimensional non-
Gaussian signal components and n := (n3, . . . , n10)(cid:62) are the 8-dimensional
Gaussian noise components. For the non-Gaussian signal components, we consider
the following four distributions plotted in Figure 1:

5 The source code of the experiments is at https://github.com/hgeno/WFLSNGCA.
6 http://www.ms.k.u-tokyo.ac.jp/software.html

Whitening-Free Least-Squares Non-Gaussian Component Analysis

13

(a) Independent
Gaussian Mix-
ture

(b) Dependent
Super-Gaussian

(c) Dependent
Sub-Gaussian

(d) Dependent
Super- and Sub-
Gaussian

Fig. 1. Distributions of non-Gaussian components.

(a) Independent
Gaussian Mix-
ture

(b) Dependent
Super-Gaussian

(c) Dependent
Sub-Gaussian

(d) Dependent
Super- and Sub-
Gaussian

Fig. 2. Averages and standard deviations of the subspace estimation error as the function
of the condition-number controller r over 50 simulations on artiﬁcial datasets.

(a) Independent Gaussian Mixture:

p(s1, s2) ∝

i=1

2
(cid:89)

(cid:18)

(cid:18)

exp

−

(cid:19)

(si − 3)2
2

(cid:18)

+ exp

−

(si + 3)2
2

(cid:19)(cid:19)

.

(b) Dependent super-Gaussian:

p(s) ∝ exp (− (cid:107)s(cid:107)).

(c) Dependent sub-Gaussian:

p(s) is the uniform distribution on (cid:8)s ∈ R2| (cid:107)s(cid:107) ≤ 1(cid:9).

(d) Dependent super- and sub-Gaussian:

p(s1) ∝ exp (− |s1|) and p(s2) is the uniform distribution on [c, c + 1], where
c = 0 if |s1| ≤ log 2 and c = −1 otherwise.

For the Gaussian noise components, we include a certain parameter r ≥ 0,
which controls the condition number; the larger r is, the more ill-posed the data
covariance matrix is. The detail is described in Appendix A.

We generate n = 2000 samples for each case, and standardize each element
of the data before applying NGCA algorithms. The performance of NGCA
algorithms is measured by the following subspace estimation error :

ε(E, (cid:98)E) :=

(cid:107)(cid:98)ei − ΠE(cid:98)ei(cid:107)2 ,

1
2

2
(cid:88)

i=1

(17)

where E is the true non-Gaussian index space, (cid:98)E is its estimate, ΠE is the
orthogonal projection on E, and {(cid:98)ei}2

i=1 is an orthonormal basis in (cid:98)E.

The averages and the standard derivations of the subspace estimation error
over 50 runs for MIPP, LSNGCA, and WF-LSNGCA are depicted in Figure 2.

14

Whitening-Free Least-Squares Non-Gaussian Component Analysis

(a) The function of sample size.

(b) The function of data dimen-
sion.

Fig. 3. The average CPU time over 50 runs when the Gaussian mixture is used as
non-Gaussian components and the condition-number controller r = 0. The vertical axis
is in logarithmic scale.

This shows that, for all 4 cases, the error of MIPP grows rapidly as r increases.
On the other hand, LSNGCA and WF-LSNGCA perform much stably against the
change in r. However, LSNGCA performs poorly for (a). Overall, WF-LSNGCA is
shown to be much more robust against ill-conditioning than MIPP and LSNGCA.

In terms of the computation time, WF-LSNGCA is less eﬃcient than LSNGCA
and MIPP, but its computation time is still just a few times slower than LSNGCA,
as seen in Figure 3. For this reason, the computational eﬃciency of WF-LSNGCA
would still be acceptable in practice.

4.3 Benchmark Datasets

Finally, we evaluate the performance of NGCA methods using the LIBSVM binary
classiﬁcation benchmark datasets 7 [15]. From each dataset, n points are selected
as training (test) samples so that the number of positive and negative samples are
equal, and datasets are standardized in each dimension. For an m-dimensional
dataset, we append (d − m)-dimensional noise dimensions following the standard
Gaussian distribution so that all datasets have d dimensions. Then we use PCA,
MIPP, LSNGCA, and WF-LSNGCA to obtain m-dimensional expressions, and
apply the support vector machine (SVM) 8 to evaluate the test misclassiﬁcation
rate. As a baseline, we also evaluate the misclassiﬁcation rate by the raw SVM
without dimension reduction.

7 We preprocessed the LIBSVM binary classiﬁcation benchmark datasets as follows:

– vehicle: We convert original labels ‘1’ and ‘2’ to the positive label and original

labels ‘3’ and ‘4’ to the negative label.

– SUSY : We convert original label ‘0’ to the negative label.
– shuttle: We use only the data labeled as ‘1’ and ‘4’ and regard them as positive

and negative labels.

– svmguide1 : We mix the original training and test datasets.

8 We used LIBSVM with MATLAB [15].

Whitening-Free Least-Squares Non-Gaussian Component Analysis

15

The averages and standard deviations of the misclassiﬁcation rate over 50
runs for d = 50, 100 are summarized in Table 2. As can be seen in the table, the
appended Gaussian noise dimensions have negative eﬀects on each classiﬁcation
accuracy, and thus the baseline has relatively high misclassiﬁcation rates. PCA
has overall higher misclassiﬁcation rates than the baseline since a lot of valuable
information for each classiﬁcation problem is lost. Among the NGCA algorithms,
WF-LSNGCA overall compares favorably with the other methods. This means
that it can ﬁnd valuable low-dimensional expressions for each classiﬁcation
problem without harmful eﬀects of a pre-whitening procedure.

Table 2. Averages (and standard deviations in the parentheses) of the misclassiﬁcation
rates for the LIBSVM datasets over 50 runs. The best and comparable algorithms
judged by the two-sample t-test at the signiﬁcance level 5% are expressed as boldface.

Dataset
[m, n]
vehicle
[18, 200]
svmguide3
[21, 200]
svmguide1
[3, 2000]
shuttle
[9, 2000]
SUSY
[18, 2000]
ijcnn1
[22, 2000]

Dataset
[m, n]
vehicle
[18, 200]
svmguide3
[21, 200]
svmguide1
[3, 2000]
shuttle
[9, 2000]
SUSY
[18, 2000]
ijcnn1
[22, 2000]

No Dim.
Red.
0.340
(0.038)
0.342
(0.035)
0.088
(0.007)
0.031
(0.004)
0.238
(0.010)
0.102
(0.007)

d = 50

PCA MIPP

LS
NGCA
0.324
0.328
0.404
(0.044)
(0.034) (0.044)
0.326
0.348
0.341
(0.039)
(0.037) (0.041)
0.058
0.159
0.060
(0.006)
(0.012) (0.005)
0.041
0.024
0.021
(0.015)
(0.007) (0.004)
0.271
0.223
0.229
(0.012) (0.010) (0.012)
0.273
0.084
(0.012) (0.028)

0.061
(0.006)

WF-LS
NGCA
0.286
(0.038)
0.308
(0.036)
0.053
(0.008)
0.007
(0.002)
0.228
(0.010)
0.057
(0.007)

d = 100

No Dim.
Red.
0.380
(0.033)
0.363
(0.034)
0.102
(0.008)
0.038
(0.005)
0.250
(0.010)
0.145
(0.008)

PCA MIPP

LS
NGCA
0.439
(0.045)
0.427
(0.035)
0.088
(0.059)
0.209
(0.080)
0.228

WF-LS
NGCA
0.360
0.432
0.445
(0.051)
(0.033) (0.036)
0.343
0.443
0.367
(0.044)
(0.032) (0.042)
0.067
0.175
0.087
(0.020)
(0.013) (0.013)
0.017
0.065
0.069
(0.005)
(0.013) (0.019)
0.280
0.226
0.234
(0.011) (0.009) (0.011) (0.010)
0.091
0.318
0.107
(0.020)
(0.013) (0.021)

0.101
(0.010)

16

Whitening-Free Least-Squares Non-Gaussian Component Analysis

5 Conclusions

In this paper, we proposed a novel NGCA algorithm which is computationally
eﬃcient, no manual design of non-Gaussian index functions is required, and
pre-whitening is not involved. Through experiments, we demonstrated that the
eﬀectiveness of the proposed method.

References

1. V. N. Vapnik. Statistical Learning Theory, volume 1. Wiley New York, 1998.
2. E. Diederichs, A. Juditsky, A. Nemirovski, and V. Spokoiny. Sparse non Gaussian
component analysis by semideﬁnite programming. Machine learning, 91(2):211–238,
2013.

3. G. Blanchard, M. Kawanabe, M. Sugiyama, V. Spokoiny, and K. Müller. In search
of non-Gaussian components of a high-dimensional distribution. Journal of Machine
Learning Research, 7:247–282, 2006.

4. J. H. Friedman and J. W. Tukey. A projection pursuit algorithm for exploratory

data analysis. IEEE Transactions on Computers, C-23(9):881–890, 1974.

5. A. Hyvärinen and E. Oja. Independent component analysis: algorithms and appli-

cations. Neural Networks, 13(4):411–430, 2000.

6. M. Kawanabe, M. Sugiyama, G. Blanchard, and K. Müller. A new algorithm
of non-Gaussian component analysis with radial kernel functions. Annals of the
Institute of Statistical Mathematics, 59(1):57–75, 2007.

7. E. Diederichs, A. Juditsky, V. Spokoiny, and C. Schütte. Sparse non-Gaussian
component analysis. IEEE Transactions on Information Theory, 56(6):3033–3047,
2010.

8. H. Sasaki, G. Niu, and M. Sugiyama. Non-Gaussian component analysis with
log-density gradient estimation. In Proceedings of the Seventeenth International
Conference on Artiﬁcial Intelligence and Statistics, volume 38, pages 1177–1185,
2016.

9. D. D Cox. A penalty method for nonparametric estimation of the logarithmic
derivative of a density function. Annals of the Institute of Statistical Mathematics,
37(1):271–288, 1985.

10. H. Sasaki, A. Hyvärinen, and M. Sugiyama. Clustering via mode seeking by direct
estimation of the gradient of a log-density. In Machine Learning and Knowledge
Discovery in Databases, pages 19–34. Springer, 2014.

11. F. J Theis and M. Kawanabe. Uniqueness of non-Gaussian subspace analysis.
In Independent Component Analysis and Blind Signal Separation, pages 917–925.
Springer, 2006.

12. A. Hyvärinen. Fast and robust ﬁxed-point algorithms for independent component

analysis. IEEE Transactions on Neural Networks, 10(3):626–634, 1999.

13. F. Bonnans and R. Cominetti. Perturbed optimization in Banach spaces I: A general
theory based on a weak directional constraint qualiﬁcation; II: A theory based on
a strong directional qualiﬁcation condition; III: Semiinﬁnite optimization. SIAM
Journal on Control and Optimization, 34:1151–1171, 1172–1189, and 1555–1567,
1996.

14. F. Bonnans and A. Shapiro. Optimization problems with perturbations, a guided

tour. SIAM Review, 40(2):228–264, 1998.

15. C. Chang and C. Lin. LIBSVM: A library for support vector machines. ACM

Transactions on Intelligent Systems and Technology, 2(3):27, 2011.

Whitening-Free Least-Squares Non-Gaussian Component Analysis

17

Supplementary Materials to Whitening-Free Least-Squares
Non-Gaussian Component Analysis

A Details of Artiﬁcial Datasets

Here, we describe the detail of the artiﬁcial datasets used in Section 4.2. The
noise components n are generated as follows:

1. The n is sampled from the centered Gaussian distribution with covariance
matrix diag(10−2r, 10−2r+4r/7, 10−2r+8r/7, . . . , 102r), where diag(·) denotes
the diagonal matrix.

2. The sampled n is rotated as n(cid:48)(cid:48) ∈ R8by applying the following rotation

matrix R(i,j) for all i, j = 3, . . . , 10 such that i < j:

i,j = − sin(π/4),

R(i,j)
i,i = cos(π/4), R(i,j)
R(i,j)
j,i = sin(π/4), R(i,j)
k,k = 1 (k (cid:54)= i, k (cid:54)= j), R(i,j)
R(i,j)

j,j = cos(π/4),

k,l = 0 (otherwise).

3. The rotated n is normalized for each dimension.

By this construction, increasing r corresponds to increasing the condition number
of the data covariance matrix (see Figure 4). Thus, the larger r is, the more
ill-posed the data covariance matrix is.

Fig. 4. Condition number of the data covariance matrix as a function of experiment
parameter r (with non-Gaussian components generated from the Gaussian mixture).

