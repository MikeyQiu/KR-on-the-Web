Fast and Robust Dynamic Hand Gesture Recognition via
Key Frames Extraction and Feature Fusion

Hao Tang1, Hong Liu2∗, Wei Xiao3, Nicu Sebe1

1Department of Information Engineering and Computer Science, University of Trento, Trento, Italy
2Key Laboratory of Machine Perception, Shenzhen Graduate School, Peking University, Beijing, China
3Lingxi Artiﬁcial Intelligence Co., Ltd, Shen Zhen, China

9
1
0
2
 
n
a
J
 
5
1
 
 
]

V
C
.
s
c
[
 
 
1
v
2
2
6
4
0
.
1
0
9
1
:
v
i
X
r
a

Abstract

Gesture recognition is a hot topic in computer vision and pattern recognition, which plays a vitally important role
in natural human-computer interface. Although great progress has been made recently, fast and robust hand gesture
recognition remains an open problem, since the existing methods have not well balanced the performance and the eﬃ-
ciency simultaneously. To bridge it, this work combines image entropy and density clustering to exploit the key frames
from hand gesture video for further feature extraction, which can improve the eﬃciency of recognition. Moreover, a
feature fusion strategy is also proposed to further improve feature representation, which elevates the performance of
recognition. To validate our approach in a “wild” environment, we also introduce two new datasets called HandGesture
and Action3D datasets. Experiments consistently demonstrate that our strategy achieves competitive results on North-
western University, Cambridge, HandGesture and Action3D hand gesture datasets. Our code and datasets will release
at https://github.com/Ha0Tang/HandGestureRecognition.

Keywords: Hand gesture recognition; Key frames extraction; Feature fusion; Fast; Robust.

1. Introduction

Gesture recognition is to recognize category labels from
an image or a video which contains gestures made by the
user. Gestures are expressive, meaningful body motions
involving physical movements of the ﬁngers, hands, arms,
head, face, or body with the intent of: conveying mean-
ingful information or interacting with the environment.

Hand gesture is one of the most expressive, natural
and common type of body language for conveying atti-
tudes and emotions in human interactions. For example,
in a television control system, hand gesture has the follow-
ing attributes: “Pause”,“Play”, “Next Channel”, “Previ-
ous Channel”, “Volume Up”, “Volume Down” and “Menu
Item”. While in a recommendation system, hand gesture
can express “Like” or “Dislike” emotions of users. Thus,
it is one of the most fundamental problems in computer
vision and pattern recognition, and has a wide range of
applications such as virtual reality systems [1], interactive
gaming platforms [2], recognizing sign language [3, 4, 5],
enabling very young children to interact with computers
[6], controlling robot [7, 8], practicing music conducting [9],
television control [10, 11], automotive interfaces [12, 13],
learning and teaching assistance [14, 15], and hand gesture
generation [16].

(cid:73)E-mail: {hao.tang, niculae.sebe}@unitn.it; hongliu@pku.edu.cn;

xiaoweithu@163.com;

∗Corresponding author.

Preprint submitted to NeuroComputing

There has been signiﬁcant progress in hand gesture
recognition, however, some key problems e.g., fast and ro-
bust are still challenging. Prior work usually puts empha-
sis on using whole data series, which always contain re-
dundant information, resulting in degraded performance.
For examples, Wang et al. [1] present a superpixel-based
hand gesture recognition system based on a novel super-
pixel earth mover’s distance metric. Ren et al. [2] focus on
building a robust part-based hand gesture recognition sys-
tem. Hikawa and Kaida [3] propose a posture recognition
system with a hybrid network. Moreover, there are many
approaches are also proposed for action or video recogni-
tion task, such as [17, 18, 19, 20, 21, 22, 23, 24, 25, 26]. Liu
and Shao [17] introduce an adaptive learning methodology
to extract spatio-temporal features, simultaneously fusing
the RGB and depth information, from RGB-D video data
for visual recognition tasks. Liu et al.
[26] propose to
combine the Salient Depth Map (SDM) and the Binary
Shape Map (BSM) for human action recognition task. Si-
monyan et al. [27] propose a two-stream ConvNet architec-
ture which incorporates spatial and temporal networks to
extract spatial and temporal features. Feichtenhofer et al.
[28] study a number of ways of fusing ConvNet towers both
spatially and temporally in order to best take advantage
of this spatio-temporal information. In sum, all these ef-
forts endeavor to decrease the computation burden in each
solo frame, while overlooking all processing schemes in the
whole frames would incur more computation burden than

January 16, 2019

a few selected representative frames, which is a fundamen-
tal way to decrease the computation burden, greatly. This
paper is devoted to bridge the gap between fast and ro-
bust hand gesture recognition, simply using solo popular
cue e.g., RGB, which ensures great potential in practical
use.

Key frames, also known as representative frames, ex-
tract the main content of a data series, which could greatly
In [29], the key
reduce the amount of processing data.
frames of the video sequence are selected by their discrim-
inative power and represented by the local motion fea-
tures detected in them and integrated from their temporal
neighbors. Carlsson and Sullivan [30] demonstrate that
speciﬁc actions can be recognized in long video sequence
by matching shape information extracted from individual
frames to stored prototypes representing key frames of the
action. However, we regard every frame in a video as a
point in the 2-D coordinate space. Since we are focus-
ing on distinguishing dynamic gesture from a data series
while not reconstructing it, we simply introduce a measure
to ﬁnd which frames are more important for distinguishing
and which are not. In consideration of information entropy
[31, 32, 33] could be a useful measurement to quantify the
information each frame contains, we introduce frame en-
tropy as an quantitative feature to describe each frame
and then map these values into a 2-D coordinate space.
How to describe this 2-D space is a hard nut to crack for
its uneven distribution. Therefore, we further propose an
integrated strategy to extract key frames using local ex-
treme points and density clustering. Local extreme points
includes the local maximum and local minimum points,
which represent the most discriminative points of frame
entropy. Shao and Ji [34] also propose a key frame extrac-
tion method based on entropy. However, the diﬀerences
between [34] and the proposed method are two-folder: (i)
The entropy in [34] is calculated on motion histograms of
each frame, while the proposed method directly calculate
on each frame. (ii) [34] simply to ﬁnd peaks in the curve of
entropy and use histogram intersection to output ﬁnal key
frames, while the proposed method ﬁrst selects the local
peaks of entropy and then use density clustering to cal-
culate the cluster centers as the ﬁnal key frames. Density
clustering [35] is the approach based on the local density of
feature points, which is able to detect local clusters, while
previous clustering approaches such as dynamic delaunay
clustering [36], k-means clustering [37], spectral clustering
[38] and graph clustering [39] cannot detect local clusters
due to the fact that they only rely on the distance between
feature points to do clustering.

In order to promote the accuracy, we also present a
novel feature fusion method that combines appearance and
motion cues. After extracting key frames, we replace the
original video sequence with the key frames sequence, which
could greatly enhance the time eﬃciency at the cost of
accuracy. This feature fusion strategy takes advantage
of both the motion and the appearance information in
the spatiotemporal activity context under the hierarchical

2

model. The experimental results show that the method
proposed is accurate and eﬀective for dynamic hand ges-
ture recognition on four datasets. To summarize, the main
contributions of this paper are:

• A novel key frames extraction method is proposed,
which improves eﬃciency of hand gesture processing.

• A feature strategy is presented in which appearance
and motion cues are fused to elevate the accuracy of
recognition.

• Experiments demonstrate that our method achieves
the balance between eﬃciency and accuracy simul-
taneously in four datasets.

2. Key Frames Extraction and Feature Fusion Strat-

egy for Hand Gesture Recognition

In this section, we will introduce the proposed key

frames extraction and feature fusion strategy.

2.1. Key Frames Extraction

Key frames extraction is the key technology for video
abstraction, which can remove the redundant information
in the video greatly. The algorithm for key frames ex-
traction will aﬀect the reconstruction of video content. If
a frame in video V can be represented by fi, where i is
(1, 2, ..., n) and n is the total number of frames in video V .
Hence, the key frames set SKeyframes is deﬁned as follows:

SKeyframes = fKeyframes (V ),

(1)

where fKeyframes denotes the key frames extraction proce-
dure.

In this paper, a method of key frames extraction based
on image entropy and density clustering is proposed, as we
can see from Figure 1. Our key frames extraction methods
are mainly divided into three steps, namely, 1) calculating
image entropy, 2) ﬁnding local extreme points and 3) exe-
cuting density cluster. The following section would expand
upon on it.

2.1.1. Image Entropy.

In this section, we try to ﬁnd a proper descriptive index
to evaluate each frame in a video, facilitating key frame ex-
traction. Informative frames could better summarize the
whole video where they reside, while how to quantify the
information each frame contains is a hard-nut to crack.
Firstly, we calculate image entropy of each frame, and
then map them into a two-dimensional coordinate space,
as shown in Figure 1(b). Entropy is a nice way of repre-
senting the impurity or unpredictability of a set of data
since it is dependent on the context in which the measure-
ment is taken. As for a single video frame, the gray-scale
color/intensity distribution of this frame can be seen as

(a) A hand gesture sequence sample from the Northwestern University hand gesture dataset, which contains 26 frames. The key frames
obtained by our method are in green boxes, which are the 2, 9, 14, 20 and 26 frames.

(b) Calculate the image entropy.

(c) Select the local peak points.

(d) Calculate ρ and δ.

(e) Select the number of clustering. In this
case, we choose 5 clusters.

(f) The ﬁnal results of clustering. The points
of 2, 5, 8, 9 and 10 are the clustering centers,
therefore, the corresponding frames (2, 9, 14,
20 and 26) are the key frames of original se-
quence.

(g) The key frames are the 2, 9, 14, 20 and 26 frames. Now we use this sequence to replace the
original sequences for the next step.

Figure 1: The framework of the proposed key frames extraction method.

3

p = {p1, p2, ..., pn}. For the image frames fi, their image
entropy can be deﬁned as:

E(fi) = −

pfi (j)logpfi (j),

(2)

(cid:88)

j

where pfi (j) denotes the probability density function of
frame fi, which could be obtained by normalizing their
histogram of gray-scale pixel intensities. Next we map the
value E(fi) to a two-dimensional coordinate space (the
E(fi) vs. i plot).

2.1.2. Local Extreme Points.

Secondly, we pick the local extreme points in the two-
dimensional coordinate space, illustrated by Figure 1(c).
Local extreme points include the local maximum points
and local minimum points. Local maximum points can be
calculated as follows:

Pmax =

(cid:40)

E(fi),
remove, else.

if E(fi) > E(fi+1) & E(fi) > E(fi−1).

(3)
Local minimum points can also be calculated by the fol-
lowing formula:

Pmin =

(cid:40)

E(fi),
remove, else.

if E(fi+1) > E(fi) & E(fi−1) > E(fi).

where i = 1, 2, ..., n. Therefore, local extreme points Pextreme
can be united by:

(4)

(5)

Pextreme = Pmax ∪ Pmin .

Local extreme points could further extract representa-
tive frames from the original video sequence. This pro-
cedure could be viewed as ﬁnding local representatives to
roughly describe the original sequence.

2.1.3. Density Clustering.

After obtaining the extreme points, as shown in Figure
1(c), we try to cluster these points into N (N is a pre-
deﬁned constant for all the datasets) categories, as, {1, 2},
{3, 4, 5, 6, 7}, {8}, {9} and {10}. The distribution of these
extreme points have the characteristics, the cluster centers
are surrounded by neighbors with a lower local density and
that they are at a relatively large distance from any points
with a higher local density.

Therefore, we adopt density clustering [35] to further
cluster these extreme points Pextreme , as shown in Figure
1(d-f). Density clustering could better catch the delicate
structure of 2-D space where extreme points reside than
traditional clustering strategies, e.g. K-means. First, we
search for a local density maximum point as a cluster cen-
ter, and then spread the cluster label from high density
to low-density points sequentially. For each data point
Pk, we compute two quantities: corresponding local den-
sity (neighborhood has a density, not the data point) ρPk

4

(7)

(8)

and its distance δPk from points of higher density. Both
these quantities depend only on the distances dPk Pl be-
tween data points, which are assumed to satisfy the trian-
gular inequality. The local density ρPk of data point Pk is
deﬁned as:

ρPk =

χ(dPk Pl − dc),

(6)

(cid:88)

Pl

where χ(x) = 1 if x < 0 and χ(x) = 0 when otherwise,
and dc is a cutoﬀ distance. Basically, ρPk is equal to the
number of points that are closer than dc to point Pk. The
algorithm is sensitive only to the relative magnitude of ρPk
in diﬀerent points, which implies that, the results of the
analysis are robust with respect to the choice of dc for large
datasets. A diﬀerent way of deﬁning ρPk as:

ρPk =

(cid:88)

e−(

dPk Pl
dc

)2

.

Pl

δPk is measured by ﬁnding the minimum distance be-
tween the point Pk and any other point with higher den-
sity:

δPk = min

(dPk Pl ),

Pl :ρPl >ρPk

which uses a Gaussian kernel to calculate. We can see
from these two kernels, cutoﬀ kernel is discrete value, while
Gaussian kernel is a continuous value, which guarantees a
smaller probability of conﬂict.

As we can see from Figure 1(d), we calculate ρ and δ us-
ing Formula (6) and (8). Then select the number of cluster-
ing center N , namely, the N largest δ values, e.g., in Figure
1(e), we select 5 cluster centers. Figure 1(f) illustrates the
ﬁnal results of clustering, in which the points of 2, 5, 8,
9 and 10 are the clustering centers, therefore, the corre-
sponding x-coordinates (the 2, 9, 14, 20 and 26 frames,
shown in ﬁgure 1(g)) are the key frames SKeyframes in the
original video V (shown in Figure 1(a)). The pipeline of
the proposed key frames extraction method is summarized
in Algorithm 1. Note that the proposed density cluster-
ing cannot handle the situation where the entropy of the
video sequence is monotone increasing or decreasing since
we need to select the local extreme points. While in our ex-
periments, we observer that there is no one video sequence
which frame entropy is monotone increasing or decreasing
all the time, it means we can always obtain the local ex-
treme points.

2.2. Feature Fusion Strategy

In view of each key frame and the relationship between
each key frame, for better representing each key frame
sequence, we not only try to describe each frame of key
frame, but also the variation between the key frames. That
is, we not only extract the most representative frames and
map them into a simple 2-D space, but also describe how
these frames move in this space. We believe this two-phase
strategy could set up a “holographic” description of each
hand gesture sequence. Hadid and Pietik¨ainen [40] also

Algorithm 1 The proposed key frames extraction
method.
Require:

The original hand gesture video V , as shown in Figure 1(a)
and the number of key frames N (N is a pre-deﬁned con-
stant for all the datasets).

Ensure:

The key frames SKeyframes in original video V , as shown in
Figure 1(g).

1: Calculate image entropy E(fi) of each frame in V using

Formula 2;

2: Map E(fi) to a two-dimensional coordinate space;
3: Find local maximum points Pmax in the two-dimensional

coordinate space using Formula (3);

4: Find local minimum points Pmin in the two-dimensional

coordinate space using Formula (4);

5: Obtain Pextreme by uniting the local maximum points Pmax

and local minimum points Pmin using Formula (5);

6: Calculate ρ for each point in Pextreme using Formula (6) or

(7);

7: Calculate δ for each point in Pextreme using Formula (8);
8: Draw decision graph like Figure 1(d);
9: Choose the N largest δ values as the clustering centers, as

10: The corresponding x-coordinate of N clustering centers are

shown in Figure 1(e);

the key frames SKeyframes .

11: return SKeyframes .

demonstrate that excellent results can be obtained com-
bining appearance feature and motion feature for dynamic
video analysis. Jain et al. [41] propose a two-stream fully
convolutional neural network which fuses together motion
and appearance in a uniﬁed framework, and substantially
improve the state-of-the-art results for segmenting unseen
[42] consider exploiting the
objects in videos. Xu et al.
appearance and motion information resided in the video
with a attention mechanism for the image question an-
swering task. For this purpose, we propose a feature fusion
strategy to capture these two phases: appearance based
approach can only be applied to each frame, which repre-
sents the diﬀerences of space merely; while, motion based
method can describe the evolution with the time. Thus
we combine appearance and motion feature for better de-
scribe image sequence. Meanwhile, to better weight these
two feature, we also introduce an eﬃcient strategy to bal-
ance them. Tang et al.
[16] also propose a feature fu-
sion method which fuses features extracted from diﬀerent
sleeves to boost the recognition performance.

Figure 2 shows the whole proposed feature fusion pro-
cedure for the obtained key frames. After extracting key
frames, we take the key frames sequence in place of the
original sequence. We begin by extracting key frames from
the original sequence, and then extract appearance and
motion features (hist1 and hist2 in Figure 2) from the
key frames sequence, respectively. For further increase the
importance of the useful feature, we add weights to appear-
ance and motion features. By feeding hist1 and hist2 to

the SVM classiﬁer separately, we obtain two classiﬁcation
accuracy R = {Ra, Rm}. Based on the assumption that
the higher the rate is, the better representation becomes,
we compute the weights as follows:

T =

R − min(R)
(100 − min(R))/10

.

(9)

Finally, considering that the weight of the lowest rate is
1, the other weights can be obtained according to a linear
relationship of their diﬀerences to that with the lowest
rate. The ﬁnal step is written as:

T 1 = round(T )

T 2 =

T × ((max(T 1) − 1))
max(T )

+ 1

(10)

W = round(T 2)

in which W = {α, β} is the weight vector corresponding
to hist1 and hist2.

There are many existing descriptors for us to extract
hist1 and hist2. In other words, the fusion strategy does
not depend on speciﬁc descriptors, which guarantees its
great potential in applications. In term of hist1, we can
use Gist [43], rootSIFT [44], HSV, SURF [45], HOG [46],
LBP [47] or its variation CLBP [48] to extract appear-
ance cue of each image. As for hist2, LBP-TOP, VLBP
[49] and SIFT 3D [50] are used to extract motion cues
from the whole key frames sequence. We also use Bag-of-
Feature (BoF) [24, 51] to represent these appearance and
motion cues. At the end of the procedure, we concatenate
weighted hist1 and hist2 to obtain the ﬁnal representa-
tion hist (as shown in Figure 2).

2.3. Hand Gesture Recognition Framework

The hand gesture recognition framework based on key
frames and feature fusion is composed of two stages, train-
ing and testing, which is summarized in Algorithm 2. In
the training stage, we ﬁrst extract key frames SKeyframes
using Algorithm 1 (step 3). Then we extract appear-
ance features from each key frame using descriptors such
as SURF, LBP, etc. After obtaining the appearance fea-
tures, we employ BoF [24, 51] to represent these features
for hist1q (step 4). Next we use LBP-TOP, VLBP or SIFT
3D to extract motion features from the whole key frames
sequence, producing the corresponding histogram hist2q
(step 5). After that, hist1q and hist2q are fed to separate
classiﬁers to obtain R = {Ra, Rm} (step 6). And then, we
calculate α and β by Formula (9) and (10) (step 7). Then
the training histogram histq is constructed from αhist1q
and βhist2q (step 8). In the end of the iteration, we ob-
tain the training representation vector hist. Then hist
and corresponding labels Llabel are fed to a SVM classiﬁer
(step 10). During the testing stage, testing hand gesture
representation is obtained in the same way as the training
stage (step 12). Thereby the trained SVM classiﬁer is used
to predict the gesture label tlabel (step 13).

5

Figure 2: The framework of the proposed feature extraction and fusion methods.

Algorithm 2 The proposed hand gesture recognition
framework.
Require:

L hand gesture videos for training, as shown in Figure 1(a),
corresponds to the gesture labels Llabel ;
Testing hand gesture video t.

Ensure:

The hand gesture label tlabel .

1: TRAINING STAGE:
2: for q = 1 to L do
3:
4:
5:

Keyframes ← Algorithm 1;

Sq
hist1q ← (SURF or GIST, etc) ∪ BoF;
hist2q ← (VLBP or LBP-TOP or SIFT 3D, etc) ∪ BoF;

6:

R = {Ra, Rm} ← training a classiﬁer using hist1q and
hist2q ;
α and β ← using by Formula (9) and (10);
histq ← {αhist1, βhist2};

7:
8:
9: end for
10: Classiﬁer ← hist ∪ Llabel ;
11: TESTING STAGE:
12: Obtain hand gesture representation histt for testing t using

the same method as the training stage;

13: Obtain tlabel by the classiﬁer after calculation;
14: return tlabel .

3. Experiments and Analysis

3.1. Datasets and Settings

To evaluate the eﬀectiveness of the proposed method,
we conduct experiments on two publicly available datasets
(Cambridge [52] and Northwestern University Hand Ges-
ture datasets [53]) and two collected datasets (HandGes-
ture and Action3D hand gesture datasets, both will be re-
leased after paper accepted). Some characteristics of these
datasets are listed in Table 1.

Cambridge Hand Gesture dataset is a commonly
used benchmark gesture data set with 900 video clips of
9 hand gesture classes deﬁned by 3 primitive hand shapes
(i.e., ﬂat, spread, V-shape) and 3 primitive motions (i.e.,
leftward, rightward, contract). For each class, it includes
100 sequences captured with 5 diﬀerent illuminations, 10
arbitrary motions and 2 subjects. Each sequence is recorded

6

in front of a ﬁxed camera having coarsely isolated gestures
in spatial and temporal dimensions.

Northwestern University Hand Gesture dataset
is a more diverse data set which contains 10 categories of
dynamic hand gestures in total: move right, move left, ro-
tate up, rotate down, move downright, move right-down,
clockwise circle, counterclockwise circle, “Z” and cross.
This dataset is performed by 15 subjects and each sub-
ject contributes 70 sequences of these ten categories with
seven postures (i.e., Fist, Fingers extended, “OK”, Index,
Side Hand, Side Index and Thumb).

These two datasets mentioned above are both with
clear backgrounds, and sequences snipped tightly around
the gestures. However, how well will this method work on
videos from “the wild” with signiﬁcant clutter, extraneous
motion, continuous running video without pre-snipping?
To validate our approach, we introduce two new datasets,
called HandGesture and Action3D datasets.

HandGesture data set consists of 132 video sequences
of 640 by 360 resolution, each of which recorded from a
diﬀerent subject (7 males and 4 females) with 12 diﬀerent
gestures (“0”-“9”, “NO” and “OK”).

We also acquired Action3D dataset which consisting
of 1620 image sequences of 6 hand gesture classes (box,
high wave, horizontal wave, curl, circle and hand up),
which are deﬁned by 2 diﬀerent hands (right and left hand)
and 5 situations (sit, stand, with a pillow, with a laptop
and with a person). Each class contains 270 image se-
quences (5 diﬀerent situations × 2 diﬀerent hands × 3
times × 9 subjects). Each sequence was recorded in front
of a ﬁxed camera having roughly isolated gestures in space
and time. All video sequences were uniformly resized into
320 × 240 in our method.

3.2. Parameter Analysis

Two parameters are involved in our framework: the
number of key frames N and the dictionary number D in
BoF. Firstly, we extract N = 3, 4, ..., 9 key frames from
the original video, respectively. And then extract SURF
features from each key frame. Every key point detected by
SURF provides a 64-D vector describing the texture of it.
Finally, we adopt BoF to represent each key frame with

Table 1: Characteristics of the datasets used in our hand gesture recognition experiments.
Dataset
Cambridge
Northwestern
HandGesture
Action3D

# categories # videos # training # validation # testing
450
550
66
810

900
1,050
132
1,620

225
250
33
405

225
250
33
405

9
10
12
6

dictionary D = 1, 2, 4, ..., 4096, respectively. The number
of training set, validation set and testing set please refer
to Table 1. We repeat all the experiments 20 times with
diﬀerent random spits of the training and testing samples
to obtain reliable results. The ﬁnal classiﬁcation accuracy
is reported as the average of each run. Figure 3 presents
the accuracy results on the four datasets. From Figure
3 (a) and (c), the accuracy ﬁrst rises to the peak when
D = 64 and then drops after reaching the peak. However,
as shown in Figure 3 (b) and (d), the accuracy reaching
the peak when D = 16. Thus, we set D = 64 on the
Cambridge and Northwestern datasets, and D = 16 on
our two proposed datasets. It is observe that the more key
frames we have, the more time will be consumed. Thus,
to balance accuracy and eﬃciency, we set N = 5 on all the
four datasets in the following experiments.

3.3. Experiment Evaluation

To evaluate the necessity and eﬃciency of the proposed
strategy, we test it in multi-aspect: (1) necessity of key
frames extraction; (2) diﬀerent kernel tricks; (3) diﬀer-
ent fusion strategies; (4) diﬀerent clustering methods; (5)
performance comparisons with the state-of-the-art; (6) ef-
ﬁciency. (1)-(4) demonstrate the rationality and validity
of the methods. (5) compares the proposed method with
others. (6) shows its eﬃciency.
(1) Comparison with Diﬀerent Key Frames Extraction Meth-
ods. We discuss whether our key frames method is nec-
essary or not here. For the Cambridge and Action3D
dataset, we only extract LBP-TOP, and then concate-
nate the three orthogonal planes of LBP-TOP. For the
Northwestern and HandGesture dataset, 200 points are
randomly selected from each video using SIFT 3D. As we
can see from Table 2, our approach outperforms the other
four methods on both accuracy and eﬃciency, thereby our
approach is not only theoretical improvement, but also has
an empirical advantage.
(2) Gaussian Kernel vs. Cutoﬀ Kernel. We also compare
the Gaussian and cutoﬀ kernel. We adopt SURF to ex-
tract feature from each key frame. Comparison results are
shown in the Table 3. As we can observe that there is small
diﬀerent between using the Gaussian and cutoﬀ kernel.
(3) Comparison with Diﬀerent Feature Fusion Strategies.
We demonstrate that combining appearance (hand pos-
ture) and motion (the way hand is moving) boosts hand
gesture recognition task here. Moreover, we also compare
diﬀerent schemes based on appearance and motion, re-
spectively. For feature fusion, we set α and β to 8 and
1, respectively. As shown in Table 4, fusion strategies

are much better than appearance or motion based meth-
ods, which demonstrates the necessity of our feature fusion
strategy. Motion-based method achieve the worst results,
which can illustrate that in our task spatial cues is more
important than the temporal cues. The spatial cues rep-
resents/extracts the diﬀerence between diﬀerent gesture
classes, also called inter-class diﬀerences. While the tem-
poral cues captures the diﬀerence among diﬀerent frames
in the same gesture sequences, also called intra-class dif-
ferences.
Inter-class diﬀerences are always greater than
intra-class diﬀerences, which means the spatial cues can
represent more discriminative feature than the temporal
cues. In our task, we observe that the diﬀerences between
diﬀerent types of gestures are much greater than the dif-
ferences between the same gesture sequence, which means
the spatial cues is more discriminative than the tempo-
ral cues. However, if hand gesture moves fast and change
hugely in one sequence, the temporal cues could be more
important.
(4) Comparison with Diﬀerent Clustering Methods. We
also compare diﬀerent clustering methods for the key frames
extraction. As shown in Table 5, we can see that density
clustering is much better than K-means, OPTICS [54] and
DBSCAN [55].
(5) Comparison with State-of-the-Arts. For the Cambridge
and Northwestern datasets, we compare our results with
the state-of-the-art methods in Tables 6 and 7. We achieve
98.23% ± 0.84% and 96.89% ± 1.08% recognition accu-
racy on the Cambridge and Northwestern dataset, both
of which exceed the other baseline methods.
(6) Eﬃciency. Finally, We investigate our approach in
terms of computation time of classifying one test video.
We run our experiment on a 3.40-GHz i7-3770 CPU with
8 GB memory. The proposed method is implemented us-
ing Matlab. Code and datasets will release after paper
accepted. As we can see from Table 8 that the time of
classifying one test sequence is 4.31s, 10.89s, 13.06s and
4.26s for the Cambridge, Northwestern, HandGesture and
Action3D datasets. We observe that the proposed key
frame extraction methods including entropy calculation
and density clustering can be ﬁnished within around 1s
on the Cambridge, Northwestern and Action3D datasets.
While for the HandGesture dataset which contains about
200 frames in a single video, it only cost about 3s per video.
We also note that the most time-consuming part is feature
extraction, and we have two solutions to improve it, (i) we
can reduce the size of images, we note that Cambridge
and Action3D only consume about 4s, while Northwest-
ern and HandGesture cost about 11s and 13s respectively.

7

(a)

(b)

Table 2: Comparison between diﬀerent key frames extraction methods on the Cambridge, Northwestern, HandGesture and Action3D datasets.

(c)

(d)

Figure 3: Parameters N and D selection on the four datasets.

Method

Original Sequence
5 evenly-spaced frames (in time)
Zhao and Elgammal [29]
Carlsson and Sullivan [30]
Ours key frames method

Method

Original Sequence
5 evenly-spaced frames (in time)
Zhao and Elgammal [29]
Carlsson and Sullivan [30]
Ours key frames method

Cambridge

Northwestern

Accuracy

Time(s)
35.26% ± 3.15% 20,803
1,189
56.13% ± 5.46%
1,432
58.14% ± 3.36%
1,631
50.57% ± 4.78%
1,152
60.78% ± 2.21%

HandGesture

Accuracy
42.54% ± 4.61%
58.32% ± 3.88%
60.45% ± 4.56%
50.78% ± 4.06%
65.18% ± 3.62%

Time(s)
8,549
1,689
1,895
2,154
1,645

Accuracy

Time(s)
21.65% ± 1.23% 108,029
58.79% ± 2.64% 26,303
61.45% ± 3.45% 27,789
51.27% ± 3.86% 29,568
64.24% ± 2.15% 25,214
Action3D

Accuracy

Time(s)
34.56% ± 2.65% 284,489
52.13% ± 2.31% 18,430
54.56% ± 1.97% 20,143
46.34% ± 2.78% 23,768
56.13% ± 1.89% 16,294

Table 3: Comparison between Gaussian kernel and cutoﬀ kernel on
the Cambridge, Northwestern, HandGesture and Action3D datasets.

Kernel
Cambridge
Northwestern
HandGesture
Action3D

Cutoﬀ Kernel

Gaussian Kernel
92.37% ± 1.67% 90.33% ± 2.78%
81.31% ± 1.49% 80.25% ± 1.86%
96.32% ± 3.35% 94.54% ± 2.78%
96.26% ± 1.39% 93.65% ± 1.23%

The reason is that the image size of Cambridge and Ac-
tion3D is 320 × 240, while the image size of Northwestern
and HandGesture is 640 × 480; (ii) We can further reduce
the time for feature extraction by using a GPU like most
deep learning methods do. Moreover, we also compare two
methods ( [29] and [59] currently achieve best recognition
results on the Cambridge and Northwestern datasets, re-
spectively.) on the time for classifying a test sequence
on the Cambridge, Northwestern, HandGesture and Ac-

8

Table 4: Comparison with diﬀerent features fusion strategies on the Cambridge, Northwestern, HandGesture and Action3D datasets. N and
D denote the number of key frames and the dictionary number.

Appearance-based
SURF
GIST
Motion-based
LBP-TOP
VLBP
SIFT 3D
Appearance + Motion
SURF + LBP-TOP
SURF + VLBP
SURF + SIFT 3D
GIST + LBP-TOP
GIST + VLBP
GIST + SIFT 3D

Dimension
N ∗ D
N ∗ D
-
177
16,386
N ∗ D
-
N ∗ D+177
N ∗ D+16,386
2 ∗ N ∗ D
N ∗ D+177
N ∗ D+16,386
2 ∗ N ∗ D

Northwestern
81.31% ± 1.49%
78.41% ± 1.91%
Northwestern
51.36% ± 2.16%
42.11% ± 3.04%
64.24% ± 2.15%
Northwestern
93.54% ± 1.36%
91.22% ± 0.95%

Action3D
Cambridge
96.26% ± 1.39%
92.37% ± 1.67%
91.23% ± 1.84%
88.15% ± 1.65%
Action3D
Cambridge
56.13% ± 1.89%
60.78% ± 2.21%
44.26% ± 4.23%
50.36% ± 3.56%
62.04% ± 2.89%
68.94% ± 4.81%
Action3D
Cambridge
98.53% ± 1.31%
95.75% ± 0.79%
92.52% ± 1.27%
97.21% ± 0.94%
98.23% ± 0.84% 96.89% ± 1.08% 99.21% ± 0.88% 98.98% ± 0.65%
93.11% ± 0.89%
91.87% ± 1.65%
92.63% ± 0.64%
90.56% ± 0.87%
94.21% ± 0.61%
93.52% ± 0.63%

HandGesture
96.32% ± 3.35%
92.64% ± 2.89%
HandGesture
60.84% ± 2.61%
49.78% ± 3.51%
65.18% ± 3.62%
HandGesture
97.25% ± 0.79%
96.82% ± 0.95%

86.26% ± 0.94%
82.87% ± 1.84%
88.54% ± 1.62%

93.56% ± 1.35%
92.88% ± 1.21%
94.16% ± 0.67%

Table 5: Comparison with diﬀerent clustering methods on the Cambridge, Northwestern, HandGesture and Action3D datasets.

Clustering Method
Cambridge
Northwestern
HandGesture
Action3D

OPTICS [54]

DBSCAN [55]
88.15% ± 1.51% 90.34% ± 1.78% 86.26% ± 2.51%
86.34% ± 2.45% 88.35% ± 1.67% 83.65% ± 1.06%
84.56% ± 1.89% 85.98% ± 1.76% 84.69% ± 1.98%
83.56% ± 1.56% 87.43% ± 1.63% 82.36% ± 1.46%

K-means

Density Clustering [35]
98.23% ± 0.84%
96.89% ± 1.08%
99.21% ± 0.88%
98.98% ± 0.65%

Table 6: Comparison with the state-of-the-art methods on the Cambridge dataset.

Cambridge
Wong and Cipolla [56]
Niebles et al. [57]
Kim et al. [52]
Kim and Cipolla [58]
Liu and Shao [59]
Lui et al. [60]
Lui and Beveridge [61]
Wong et al. [62]
Sanin et al. [63]
Baraldi et al. [64]
Zhao and Elgammal [29]
Ours

Methods
Sparse Bayesian Classiﬁer
Spatial-Temporal Words
Tensor Canonical Correlation Analysis
Canonical Correlation Analysis
Genetic Programming
High Order Singular Value Decomposition
Tangent Bundle
Probabilistic Latent Semantic Analysis
Spatio-Temporal Covariance Descriptors
Dense Trajectories + Hand Segmentation
Information Theoretic
Key Frames + Feature Fusion

Accuracy
44%
67%
82%
82%
85%
88%
91%
91.47%
93%
94%
96.22%
98.23% ± 0.84%

Table 7: Comparison between the state-of-the-art methods and our method on the Northwestern University dataset.
Methods
Genetic Programming
Motion Divergence ﬁelds
Key Frames + Feature Fusion

Northwestern
Liu and Shao [59]
Shen et al. [53]
Our method

Accuracy
96.1%
95.8%
96.89% ± 1.08%

Table 8: Computation time for classifying a test sequence on the Cambridge, Northwestern, HandGesture and Action3D datasets.

Cambridge Northwestern HandGesture Action3D

Time
Entropy Calculation
Density Clustering
Feature Extraction
SVM Classiﬁcation
Our Full Model
Liu and Shao [59]
Zhao and Elgammal [29]

0.93s
0.31s
3.07s
0.60 ms
4.31s
6.45s
5.34s

0.84s
0.34s
9.71s
0.51ms
10.89s
13.32s
11.78s

9

3.21s
0.43s
9.42s
0.46ms
13.06s
15.32s
14.98s

0.75s
0.38s
3.13s
0.65 ms
4.26s
6.43s
5.21s

tion3D datasets. We re-implement both methods with the
same running settings for fair comparison, including hard-
ware platform and programming language. The results
are shown in Table 8, and we can see that the proposed
method achieve better results than both methods.

4. Conclusion

In order to build a fast and robust gesture recognition
system, in this paper, we present a novel key frames ex-
traction method and feature fusion strategy. Considering
the speed of recognition, we propose a new key frames
extraction method based on image entropy and density
clustering, which can greatly reduce the redundant infor-
mation of original video. Moreover, we further propose an
eﬃcient feature fusion strategy which combines appear-
ance and motion cues for robust hand gesture recogni-
tion. Experimental results show that the proposed ap-
proach outperforms the state-of-the-art methods on the
Cambridge (98.23% ± 0.84%) and Northwestern (96.89% ±
1.08%) datasets. For evaluate our method on videos from
“the wild” with signiﬁcant clutter, extraneous motion and
no pre-snipping, we introduce two new datasets, namely
HandGesture and Action3D. We achieve accuracy of 99.21%±
0.88% and 98.98% ± 0.65% on the HandGesture and Ac-
tion3D datasets, respectively. From the respect of the
recognition speed, we also achieve better results than the
state-of-the-art approaches for classifying one test sequence
on the Cambridge, Northwestern, HandGesture and Ac-
tion3D datasets.

Acknowledgments

This work is partially supported by National Natural
Science Foundation of China (NSFC, U1613209), Shen-
zhen Key Laboratory for Intelligent Multimedia and Vir-
tual Reality (ZDSYS201703031405467), Scientiﬁc Research
Project of Shenzhen City (JCYJ20170306164738129).

References

References

[1] C. Wang, Z. Liu, S.-C. Chan, Superpixel-based hand gesture
recognition with kinect depth camera, IEEE TMM 17 (1) (2015)
29–39.

[2] Z. Ren, J. Yuan, J. Meng, Z. Zhang, Robust part-based hand
gesture recognition using kinect sensor, IEEE TMM 15 (5)
(2013) 1110–1120.

[3] H. Hikawa, K. Kaida, Novel fpga implementation of hand sign
recognition system with som–hebb classiﬁer, IEEE TCSVT
25 (1) (2015) 153–166.

[4] G. Marin, F. Dominio, P. Zanuttigh, Hand gesture recognition

with leap motion and kinect devices, in: ICIP, 2014.

[5] A. Kuznetsova, L. Leal-Taix´e, B. Rosenhahn, Real-time sign
language recognition using a consumer depth camera, in: IC-
CVW, 2013.

[6] Y. Yao, Y. Fu, Contour model based hand-gesture recognition
using kinect sensor, IEEE TCSVT 24 (11) (2014) 1935–1944.

10

[7] L. Prasuhn, Y. Oyamada, Y. Mochizuki, H. Ishikawa, A hog-
based hand gesture recognition system on a mobile device, in:
ICIP, 2014.

[8] P. Neto, D. Pereira, J. Norberto Pires, A. P. Moreira, Real-
time and continuous hand gesture spotting: an approach based
on artiﬁcial neural networks, in: ICRA, 2013.

[9] R. Schramm, C. Rosito Jung, E. Miranda, Dynamic time warp-
ing for music conducting gestures evaluation, IEEE TMM 17 (2)
(2014) 243–255.

[10] S. Lian, W. Hu, K. Wang, Automatic user state recognition
for hand gesture based low-cost television control system, IEEE
TCE 60 (1) (2014) 107–115.

[11] W. T. Freeman, C. Weissman, Television control by hand ges-

tures, in: AFGRW, 1995.

[12] E. Ohn-Bar, M. M. Trivedi, Hand gesture recognition in real
time for automotive interfaces: A multimodal vision-based ap-
proach and evaluations, IEEE TITS 15 (6) (2014) 2368–2377.

[13] E. Ohn-Bar, M. M. Trivedi, The power is in your hands: 3d
analysis of hand gestures in naturalistic video, in: CVPRW,
2013.

[14] S. Sathayanarayana, R. K. Satzoda, A. Carini, M. Lee, L. Sala-
manca, J. Reilly, D. Forster, M. Bartlett, G. Littlewort, Towards
automated understanding of student-tutor interactions using vi-
sual deictic gestures, in: CVPRW, 2014.

[15] S. Sathyanarayana, G. Littlewort, M. Bartlett, Hand gestures
for intelligent tutoring systems: Dataset, techniques &amp;
evaluation, in: ICCVW, 2013.

[16] H. Tang, W. Wang, D. Xu, Y. Yan, N. Sebe, Gesturegan for
hand gesture-to-gesture translation in the wild, in: ACM MM,
2018.

[17] L. Liu, L. Shao, Learning discriminative representations from

rgb-d video data., in: IJCAI, 2013.

[18] M. Yu, L. Liu, L. Shao, Structure-preserving binary representa-
tions for rgb-d action recognition, IEEE TPAMI 38 (8) (2016)
1651–1664.

[19] H. Tang, H. Liu, W. Xiao, Gender classiﬁcation using pyramid
segmentation for unconstrained back-facing video sequences, in:
ACM MM, 2015.

[20] D. Tran, L. Bourdev, R. Fergus, L. Torresani, M. Paluri, Learn-
ing spatiotemporal features with 3d convolutional networks, in:
ICCV, 2015.

[21] L. Shao, L. Liu, M. Yu, Kernelized multiview projection for
robust action recognition, Springer IJCV 118 (2) (2016) 115–
129.

[22] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang,
L. Van Gool, Temporal segment networks: Towards good prac-
tices for deep action recognition, in: ECCV, 2016.

[23] S. Ji, W. Xu, M. Yang, K. Yu, 3d convolutional neural networks
for human action recognition, IEEE TPAMI 35 (1) (2013) 221–
231.

[24] H. Liu, H. Tang, W. Xiao, Z. Guo, L. Tian, Y. Gao, Sequen-
tial bag-of-words model for human action classiﬁcation, CAAI
Transactions on Intelligence Technology 1 (2) (2016) 125–136.
[25] J. Qin, L. Liu, L. Shao, F. Shen, B. Ni, J. Chen, Y. Wang, Zero-
shot action recognition with error-correcting output codes, in:
CVPR, 2017.

[26] H. Liu, L. Tian, M. Liu, H. Tang, Sdm-bsm: A fusing depth

scheme for human action recognition, in: ICIP, 2015.

[27] K. Simonyan, A. Zisserman, Two-stream convolutional net-

works for action recognition in videos, in: NIPS, 2014.

[28] C. Feichtenhofer, A. Pinz, A. Zisserman, Convolutional two-
stream network fusion for video action recognition, in: CVPR,
2016.

[29] Z. Zhao, A. M. Elgammal, Information theoretic key frame se-

lection for action recognition., in: BMVC, 2008.

[30] S. Carlsson, J. Sullivan, Action recognition by shape matching
to key frames, in: Workshop on Models versus Exemplars in
Computer Vision, 2001.

[31] A. Brink, Using spatial information as an aid to maximum en-
tropy image threshold selection, Elsevier PRL 17 (1) (1996)
29–36.

[32] B. S. Min, D. K. Lim, S. J. Kim, J. H. Lee, A novel method of
determining parameters of clahe based on image entropy, Inter-
national Journal of Software Engineering and Its Applications
7 (5) (2013) 113–120.

[33] X. Wang, C. Chen, Ship detection for complex background sar
images based on a multiscale variance weighted image entropy
method, IEEE Geoscience and Remote Sensing Letters 14 (2)
(2017) 184–187.

[34] L. Shao, L. Ji, Motion histogram analysis based key frame ex-
in: CRV,

traction for human action/activity representation,
2009.

entation images., in: BMVC, 2005.

[57] J. C. Niebles, H. Wang, L. Fei-Fei, Unsupervised learning of
human action categories using spatial-temporal words, Springer
IJCV 79 (3) (2008) 299–318.

[58] T.-K. Kim, R. Cipolla, Canonical correlation analysis of video
volume tensors for action categorization and detection, IEEE
TPAMI 31 (8) (2009) 1415–1428.

[59] L. Liu, L. Shao, Synthesis of spatio-temporal descriptors for
dynamic hand gesture recognition using genetic programming,
in: FGW, 2013.

[60] Y. M. Lui, J. R. Beveridge, M. Kirby, Action classiﬁcation on

[35] A. Rodriguez, A. Laio, Clustering by fast search and ﬁnd of

product manifolds, in: CVPR, 2010.

density peaks, Science 344 (6191) (2014) 1492–1496.

[61] Y. M. Lui, J. R. Beveridge, Tangent bundle for human action

recognition, in: FG, 2011.

[62] S.-F. Wong, T.-K. Kim, R. Cipolla, Learning motion categories
using both semantic and structural information, in: CVPR,
2007.

[63] A. Sanin, C. Sanderson, M. T. Harandi, B. C. Lovell, Spatio-
temporal covariance descriptors for action and gesture recogni-
tion, in: WACV, 2013.

[64] L. Baraldi, F. Paci, G. Serra, L. Benini, R. Cucchiara, Gesture
recognition in ego-centric videos using dense trajectories and
hand segmentation, in: CVPRW, 2014.

[36] S. K. Kuanar, R. Panda, A. S. Chowdhury, Video key frame ex-
traction through dynamic delaunay clustering with a structural
constraint, Elsevier JVCIP 24 (7) (2013) 1212–1227.

[37] S. E. F. De Avila, A. P. B. Lopes, A. da Luz, A. de Albu-
querque Ara´ujo, Vsumm: A mechanism designed to produce
static video summaries and a novel evaluation method, Elsevier
PRL 32 (1) (2011) 56–68.

[38] R. V´aZquez-Mart´ıN, A. Bandera, Spatio-temporal

feature-
based keyframe detection from video shots using spectral clus-
tering, Elsevier PRL 34 (7) (2013) 770–779.

[39] R. Panda, S. K. Kuanar, A. S. Chowdhury, Scalable video sum-
marization using skeleton graph and random walk, in: ICPR,
2014.

[40] A. Hadid, M. Pietik¨ainen, Combining appearance and motion
for face and gender recognition from videos, Elsevier PR 42 (11)
(2009) 2818–2827.

[41] S. D. Jain, B. Xiong, K. Grauman, Fusionseg: Learning to com-
bine motion and appearance for fully automatic segmention of
generic objects in videos, in: CVPR, 2017.

[42] D. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, Y. Zhuang,
Video question answering via gradually reﬁned attention over
appearance and motion, in: ACM MM, 2017.

[43] A. Oliva, A. Torralba, Modeling the shape of the scene: A holis-
tic representation of the spatial envelope, Springer IJCV 42 (3)
(2001) 145–175.

[44] R. Arandjelovic, A. Zisserman, Three things everyone should

know to improve object retrieval, in: CVPR, 2012.

[45] H. Bay, T. Tuytelaars, L. Van Gool, Surf: Speeded up robust

[46] N. Dalal, B. Triggs, Histograms of oriented gradients for human

features, in: ECCV, 2006.

detection, in: CVPR, 2005.

[47] T. Ojala, M. Pietik¨ainen, T. M¨aenp¨a¨a, Multiresolution gray-
scale and rotation invariant texture classiﬁcation with local bi-
nary patterns, IEEE TPAMI 24 (7) (2002) 971–987.

[48] Z. Guo, L. Zhang, D. Zhang, A completed modeling of local
binary pattern operator for texture classiﬁcation, IEEE TIP
19 (6) (2010) 1657–1663.

[49] G. Zhao, M. Pietikainen, Dynamic texture recognition using
local binary patterns with an application to facial expressions,
IEEE TPAMI 29 (6) (2007) 915–928.

[50] P. Scovanner, S. Ali, M. Shah, A 3-dimensional sift descriptor
and its application to action recognition, in: ACM MM, 2007.
[51] N. H. Dardas, N. D. Georganas, Real-time hand gesture detec-
tion and recognition using bag-of-features and support vector
machine techniques, IEEE TIM 60 (11) (2011) 3592–3607.
[52] T.-K. Kim, K.-Y. K. Wong, R. Cipolla, Tensor canonical corre-
lation analysis for action classiﬁcation, in: CVPR, 2007.
[53] X. Shen, G. Hua, L. Williams, Y. Wu, Dynamic hand gesture
recognition: An exemplar-based approach from motion diver-
gence ﬁelds, Elsevier IVC 30 (3) (2012) 227–235.

[54] M. Ankerst, M. M. Breunig, H.-P. Kriegel, J. Sander, Optics:
ordering points to identify the clustering structure, ACM Sig-
mod Record 28 (2) (1999) 49–60.

[55] M. Ester, H.-P. Kriegel, J. Sander, X. Xu, et al., A density-based
algorithm for discovering clusters in large spatial databases with
noise., in: KDD, 1996.

[56] S.-F. Wong, R. Cipolla, Real-time interpretation of hand mo-
tions using a sparse bayesian classiﬁer on motion gradient ori-

11

