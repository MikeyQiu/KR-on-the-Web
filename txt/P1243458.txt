Draft version July 11, 2016
Preprint typeset using LATEX style AASTeX6 v. 1.0

FUNDAMENTAL PARAMETERS OF MAIN-SEQUENCE STARS IN AN INSTANT WITH MACHINE LEARNING

Earl P. Bellinger1,2,3, George C. Angelou1,2, Saskia Hekker1,2, Sarbani Basu4, Warrick H. Ball5, and Elisabeth
Guggenberger1,2
1 Max-Planck-Institut f¨ur Sonnensystemforschung, Justus-von-Liebig-Weg 3, 37077 G¨ottingen, Germany
2 Stellar Astrophysics Centre, Department of Physics and Astronomy, Aarhus University, Ny Munkegade 120, DK-8000 Aarhus C, Denmark
3 Institut f¨ur Informatik, Georg-August-Universit¨at G¨ottingen, Goldschmidtstrasse 7, 37077 G¨ottingen, Germany
4 Department of Astronomy, Yale University, New Haven, CT 06520, USA
5 Institut f¨ur Astrophysik G¨ottingen, Friedrich-Hund-Platz 1, 37077 G¨ottingen, Germany

ABSTRACT
Owing to the remarkable photometric precision of space observatories like Kepler, stellar and planetary
systems beyond our own are now being characterized en masse for the ﬁrst time. These characterizations
are pivotal for endeavors such as searching for Earth-like planets and solar twins, understanding the
mechanisms that govern stellar evolution, and tracing the dynamics of our Galaxy. The volume of data
that is becoming available, however, brings with it the need to process this information accurately and
rapidly. While existing methods can constrain fundamental stellar parameters such as ages, masses,
and radii from these observations, they require substantial computational eﬀorts to do so.
We develop a method based on machine learning for rapidly estimating fundamental parameters of
main-sequence solar-like stars from classical and asteroseismic observations. We ﬁrst demonstrate this
method on a hare-and-hound exercise and then apply it to the Sun, 16 Cyg A & B, and 34 planet-hosting
candidates that have been observed by the Kepler spacecraft. We ﬁnd that our estimates and their
associated uncertainties are comparable to the results of other methods, but with the additional beneﬁt
of being able to explore many more stellar parameters while using much less computation time. We
furthermore use this method to present evidence for an empirical diﬀusion-mass relation. Our method
is open source and freely available for the community to use.a
Keywords: methods: statistical — stars: abundances — stars:

fundamental parameters — stars:

low-mass — stars: oscillations — stars: solar-type

1. INTRODUCTION

In recent years, dedicated photometric space missions
have delivered dramatic improvements to time-series ob-
servations of solar-like stars. These improvements have
come not only in terms of their precision, but also in
their time span and sampling, which has thus enabled
direct measurement of dynamical stellar phenomena such
as pulsations, binarity, and activity. Detailed measure-
ments like these place strong constraints on models used
to determine the ages, masses, and chemical composi-
tions of these stars. This in turn facilitates a wide range
of applications in astrophysics, such as testing theories
of stellar evolution, characterizing extrasolar planetary
systems (e.g. Campante et al. 2015; Silva Aguirre et al.
2015), assessing galactic chemical evolution (e.g. Chiap-

a The source code for all analyses and for all ﬁgures appearing in
this manuscript can be found electronically at https://github.
com/earlbellinger/asteroseismology (Bellinger 2016).

pini et al. 2015), and performing ensemble studies of
the Galaxy (e.g. Chaplin et al. 2011; Miglio et al. 2013;
Chaplin et al. 2014).

The motivation to increase photometric quality has
in part been driven by the goal of measuring oscillation
modes in stars that are like our Sun. Asteroseismology,
the study of these oscillations, provides the opportunity
to constrain the ages of stars through accurate inferences
of their interior structures. However, stellar ages cannot
be measured directly; instead, they depend on indirect
determinations via stellar modelling.

Traditionally, to determine the age of a star, proce-
dures based on iterative optimization (hereinafter IO)
seek the stellar model that best matches the available
observations (Brown et al. 1994). Several search strate-
gies have been employed, including exploration through
a pre-computed grid of models (i.e. grid-based modelling,
hereinafter GBM; see Gai et al. 2011; Chaplin et al.
2014); or in situ optimization (hereinafter ISO) such as
genetic algorithms (Metcalfe et al. 2014), Markov-chain

6
1
0
2
 
l
u
J
 
6
 
 
]

R
S
.
h
p
-
o
r
t
s
a
[
 
 
1
v
7
3
1
2
0
.
7
0
6
1
:
v
i
X
r
a

2

Bellinger & Angelou et al.

Monte Carlo (Bazot et al. 2012), or the downhill simplex
algorithm (Paxton et al. 2013; see e.g. Silva Aguirre et al.
2015 for an extended discussion on the various methods
of dating stars). Utilizing the detailed observations from
the Kepler and CoRoT space telescopes, these proce-
dures have constrained the ages of several ﬁeld stars
to within 10% of their main-sequence lifetimes (Silva
Aguirre et al. 2015).

IO is computationally intensive in that it demands
the calculation of a large number of stellar models (see
Metcalfe et al. 2009 for a discussion). ISO requires that
new stellar tracks are calculated for each target, as they
do not know a priori all of the combinations of stellar pa-
rameter values that the optimizer will need for its search.
They furthermore converge to local minima and there-
fore need to be run multiple times from diﬀerent starting
points to attain global coverage. GBM by way of inter-
polation in a high-dimensional space, on the other hand,
is sensitive to the resolution of each parameter and thus
requires a very ﬁne grid of models to search through (see
e.g. Quirion et al. 2010, who use more than ﬁve million
models that were varied in just four initial parameters).
Additional dimensions such as eﬃciency parameters (e.g.
overshooting or mixing length parameters) signiﬁcantly
impact on the number of models needed and hence the
search times for these methods. As a consequence, these
approaches typically use, for example, a solar-calibrated
mixing length parameter or a ﬁxed amount of convec-
tive overshooting. Since these values in other stars are
unknown, keeping them ﬁxed therefore results in under-
estimations of uncertainties. This is especially important
in the case of atomic diﬀusion, which is essential when
modelling the Sun (see e.g. Basu & Antia 1994), but is
usually disabled for stars with M/M(cid:12) > 1.4 because it
leads to the unobserved consequence of a hydrogen-only
surface (Morel & Th´evenin 2002).

These concessions have been made because the relation-
ships connecting observations of stars to their internal
properties are non-linear and diﬃcult to characterize.
Here we will show that through the use of machine learn-
ing, it is possible to avoid these diﬃculties by capturing
those relations statistically and using them to construct a
regression model capable of relating observations of stars
to their structural, chemical, and evolutionary proper-
ties. The relationships can be learned using many fewer
models than IO methods require, and can be used to
process entire stellar catalogs with a cost of only seconds
per star.

To date, only about a hundred solar-like oscillators
have had their frequencies resolved, allowing each of them
be modelled in detail using costly methods based on IO.
In the forthcoming era of TESS (Ricker et al. 2015) and
PLATO (Rauer et al. 2014), however, seismic data for
many more stars will become available, and it will not

be possible to dedicate large amounts of supercomputing
time to every star. Furthermore, for many stars, it will
only be possible to resolve global asteroseismic quantities
rather than individual frequencies. Therefore, the ability
to rapidly constrain stellar parameters for large numbers
of stars by means of global oscillation analysis will be
paramount.

In this work, we consider the constrained multiple-
regression problem of inferring fundamental stellar pa-
rameters from observable quantities. We construct a
random forest of decision tree regressors to learn the
relationships connecting observable quantities of main-
sequence (MS) stars to their zero-age main-sequence
(ZAMS) histories and current-age structural and chem-
ical attributes. We validate our technique by inferring
the parameters of simulated stars in a hare-and-hound
exercise, the Sun, and the well-studied stars 16 Cyg A
and B. Finally, we conclude by applying our method on
a catalog of Kepler objects-of-interest (hereinafter KOI;
Davies et al. 2016).

We explore various model physics by considering stellar
evolutionary tracks that are varied not only in their
initial mass and chemical composition, but also in their
eﬃciency of convection, extent of convective overshooting,
and strength of gravitational settling. We compare our
results to the recent ﬁndings from GBM (Silva Aguirre
et al. 2015), ISO (Metcalfe et al. 2015), interferometry
(White et al. 2013), and asteroseismic glitch analyses
(Verma et al. 2014) and ﬁnd that we obtain similar
estimates but with orders-of-magnitude speed-ups.

2. METHOD

We seek a multiple-regression model capable of charac-
terizing observed stars. To obtain such a model, we build
a matrix of evolutionary simulations and use machine
learning to discover relationships in the stellar models
that connect observable quantities of stars to the model
quantities that we wish to predict. The matrix is struc-
tured such that each column contains a diﬀerent stellar
quantity and each row contains a diﬀerent stellar model.
We construct this matrix by extracting models along
evolutionary sequences (see Appendix A for details on
the model selection process) and summarizing them to
yield the same types of information as the stars being ob-
served. Although each star (and each stellar model) may
have a diﬀerent number of oscillation modes observed,
it is possible to condense this information into only a
few numbers by leveraging the fact that the frequencies
of these modes follow a regular pattern (for a review of
solar-like oscillations, see Chaplin & Miglio 2013). Once
the machine has processed this matrix, one can feed the
algorithm a catalogue of stellar observations and use it
to predict the fundamental parameters of those stars.

The observable information obtained from models that

Stellar Parameters in an Instant with Machine Learning

3

can be used to inform the algorithm may include, but
is not limited to, combinations of temperatures, metal-
licities, global oscillation information, surface gravities,
luminosities, and/or radii. From these, the machine can
learn how to infer stellar parameters such as ages, masses,
core hydrogen and surface helium abundances. If lumi-
nosities, surface gravities, and/or radii are not supplied,
then they may be predicted as well. In addition, the
machine can also infer evolutionary parameters such as
the initial stellar mass and initial chemical compositions
as well as the mixing length parameter, overshoot co-
eﬃcient, and diﬀusion multiplication factor needed to
reproduce observations, which are explained in detail
below.

2.1. Model Generation

We use the open-source 1D stellar evolution code Mod-
ules for Experiments in Stellar Astrophysics (MESA; Pax-
ton et al. 2011) to generate main-sequence stellar models
from solar-like evolutionary tracks varied in initial mass
M, helium Y0, metallicity Z0, mixing length parameter
αMLT, overshoot coeﬃcient αov, and diﬀusion multiplica-
tion factor D. The diﬀusion multiplication factor serves
to amplify or diminish the eﬀects of diﬀusion, where a
value of zero turns it oﬀ and a value of two doubles all
velocities. The initial conditions are varied in the ranges
M ∈ [0.7, 1.6] M(cid:12), Y0 ∈ [0.22, 0.34], Z0 ∈ [10−5, 10−1]
(varied logarithmically), αMLT ∈ [1.5, 2.5], αov ∈ [10−4, 1]
(varied logarithmically), and D ∈ [10−6, 102] (varied log-
arithmically). We put a cut-oﬀ of 10−3 and 10−5 on αov
and D, respectively, below which we consider them to be
zero and disable them. The initial parameters of each
track are chosen in a quasi-random fashion so as to pop-
ulate the initial-condition hyperspace as homogeneously
and rapidly as possible (shown in Figure 1; see Appendix
B for more details).

We use MESA version r8118 with the Helmholtz-
formulated equation of state that allows for radiation
pressure and interpolates within the 2005 update of the
OPAL EOS tables (Rogers & Nayfonov 2002). We as-
sume a Grevesse & Sauval (1998) solar composition for
our initial abundances and opacity tables. Since we
restrict our study to the main sequence, we use an eight-
isotope nuclear network consisting of 1H, 3He, 4He, 12C,
14N, 16O, 20Ne, and 24Mg. We use a step function for
overshooting and set a scaling factor f0 = αov/5 to deter-
mine the radius r0 = Hp · f0 inside the convective zone
at which convection switches to overshooting, where Hp
is the pressure scale height. The overshooting parameter
applies to all convective boundaries and is kept ﬁxed
throughout the course of a track’s evolution, so a non-
zero value does not imply that the model has a convective
core at any speciﬁc age. All pre-main-sequence (PMS)
models are calculated with a simple photospheric ap-

proximation, after which an Eddington T-τ atmosphere
is appended on at ZAMS. We call ZAMS the point at
which the nuclear luminosity of the models make up
99.9% of the total luminosity. We calculate atomic dif-
fusion with gravitation settling and without radiative
levitation on the main sequence using ﬁve diﬀusion class
representatives: 1H, 3He, 4He, 16O, and 56Fe (Burgers
1969).1 Following their most recent measurements, we
correct the defaults in MESA of the gravitational con-
stant (G = 6.67408×10−8 g−1 cm3 s−2; Mohr et al. 2015),
the gravitational mass of the Sun (M(cid:12) = 1.988475 × 1033
g = µG−1 = 1.32712440042 × 1011 km s−1 G−1, where
µ is the standard gravitational parameter; Pitjeva 2015),
and the solar radius (R(cid:12) = 6.95568 × 1010 cm; Haberre-
iter et al. 2008).

Each track is evolved from ZAMS to either an age of
τ = 16 Gyr or until terminal-age main sequence (TAMS),
which we deﬁne as having a fractional core hydrogen
abundance (Xc) below 10−3. Evolutionary tracks with
eﬃcient heavy-element settling can develop discontinu-
ities in their surface abundances if they lack suﬃcient
model resolution. We implement adaptive remeshing by
recomputing any track with abundance discontinuities
in its surface layers using ﬁner spatial and temporal res-
olutions (see Appendix C for details). Running stellar
physics codes in a batch mode like this requires care, so
we manually inspect multiple evolutionary diagnostics
to ensure that proper convergence has been achieved.

2.2. Calculation of Seismic Parameters

We use the ADIPLS pulsation package (Christensen-
Dalsgaard 2008) to compute p-mode oscillations up to
spherical degree (cid:96) = 3 below the acoustic cut-oﬀ fre-
quency. We use on average of around 4 000 points per
stellar model and therefore have adequate resolution to
calculate frequencies without remeshing. We denote any
frequency separation S as the diﬀerence between a fre-
quency ν of spherical degree (cid:96) and radial order n and
another frequency, that is:

S((cid:96)1,(cid:96)2)(n1, n2) ≡ ν(cid:96)1(n1) − ν(cid:96)2(n2).

The large frequency separation is then

∆ν(cid:96)(n) ≡ S((cid:96),(cid:96))(n, n − 1)

and the small frequency separation is

δν((cid:96),(cid:96)+2)(n) ≡ S((cid:96),(cid:96)+2)(n, n − 1).

(1)

(2)

(3)

Near-surface layers of stars are poorly-modeled, which
induces systematic frequency oﬀsets (see e.g. Rosenthal

1 The atomic number of each representative isotope is used to
calculate the diﬀusion rate of the other isotopes allocated to that
group; see Paxton et al. (2011).

4

Bellinger & Angelou et al.

Figure 1. Scatterplot matrix (lower panels) and density plots (diagonal) of evolutionary track initial conditions considered. Mass
(M), initial helium (Y0), initial metallicity (Z0), mixing length parameter (αMLT), overshoot (αov), and diﬀusion multiplication
factor (D) were varied in a quasi-random fashion to obtain a low-discrepancy grid of model tracks. Points are colored by their
initial hydrogen X0 = 1−Y0−Z0, with black being low X0 (≈ 56%) and blue being high X0 (≈ 78%). The parameter space is
densely populated with evolutionary tracks of maximally diﬀerent initial conditions.

et al. 1999). The ratios between the large and small
frequency separations (Equation 4), and also between
the large frequency separation and ﬁve-point-averaged
frequencies (Equation 5) have been shown to be less
sensitive to the surface term than the aforementioned
separations and are therefore valuable asteroseismic diag-
nostics of stellar interiors (Roxburgh & Vorontsov 2003).
They are deﬁned as

r((cid:96),(cid:96)+2)(n) ≡

r((cid:96),1−(cid:96))(n) ≡

δν((cid:96),(cid:96)+2)(n)
∆ν(1−(cid:96))(n + (cid:96))

dd((cid:96),1−(cid:96))(n)
∆ν(1−(cid:96))(n + (cid:96))

(4)

(5)

where

dd0,1(n) ≡

ν0(n − 1) − 4ν1(n − 1) + 6ν0(n)

(cid:2)

− 4ν1(n) + ν0(n + 1)

(6)

dd1,0(n) ≡ −

ν1(n − 1) − 4ν0(n) + 6ν1(n)

(cid:3)

1
8

1
8

(cid:2)

− 4ν0(n + 1) + ν1(n + 1)

.

(7)

Since the set of radial orders that are observable diﬀers
from star to star, we collect global statistics on ∆ν0,

(cid:3)

δν0,2, δν1,3, r0,2, r1,3, r0,1, and r1,0. We mimic the range
of observable frequencies in our models by weighting all
frequencies by their position in a Gaussian envelope cen-
tered at the predicted frequency of maximum oscillation
power νmax and having full-width at half-maximum of
0.88 as per the prescription given by Mosser
0.66 · νmax
et al. (2012). We then calculate the weighted median of
each variable, which we denote with angled parentheses
(e.g. (cid:104)r0,2(cid:105)). We choose the median rather than the mean
because it is a robust statistic with a high breakdown
point, meaning that it is much less sensitive to the pres-
ence of outliers (for a discussion of breakdown points,
see Hampel 1971, who attributed them to Gauss). This
approach allows us to predict the fundamental stellar
parameters of any solar-like oscillator with multiple ob-
served modes irrespective of which exact radial orders
have been detected. Illustrations of the methods used to
derive the frequency separations and ratios of a stellar
model are shown in Figure 2.

2.3. Training the Random Forest

We train a random forest regressor on our matrix of
evolutionary models to discover the relations that fa-
cilitate inference of stellar parameters from observed

Stellar Parameters in an Instant with Machine Learning

5

Figure 2. Calculation of seismic parameters for a stellar model. The large and small frequency separations ∆ν0 (top left) and
δν0,2 (top right) and frequency ratios r0,2 (bottom left) and r0,1 (bottom right) are shown as a function of frequency. The vertical
dotted line in these bottom four plots indicates νmax. Points are sized and colored proportionally to the applied weighting, with
large blue symbols indicating high weight and small red symbols indicating low weight.

quantities. A schematic representation of the topology
of our random forest regressor can be seen in Figure 3.
Random forests arise in machine learning through the
family of algorithms known as CART, i.e. Classiﬁcation
and Regression Trees. There are several good textbooks
that discuss random forests (see e.g. Hastie et al. 2005,
Chapter 15). A random forest is an ensemble regres-
sor, meaning that it is composed of many individual
components that each perform statistical regression, and
the forest subsequently averages over the results from
each component (Breiman 2001). The components of
the ensemble are decision trees, each of which learns a
set of decision rules for relating observable quantities to
stellar parameters. An ensemble approach is preferred
because using only a single decision tree that is able to
see all of the training data may result in a regressor that
has memorized the training data and is therefore unable
to generalize to as yet unseen values. This undesirable
phenomenon is known in machine learning as over-ﬁtting,
and is analogous to ﬁtting n data points using a degree
n polynomial: the ﬁt will work perfectly on the data
that was used for ﬁtting, but fail badly on any unseen
data. To avoid this, each decision tree in the forest is
given a random subset of the evolutionary models and a

random subset of the observable quantities from which to
build a set of rules relating observed quantities to stellar
parameters. This process, known as statistical bagging
(Hastie et al. 2005, Section 8.7), prevents the collection
of trees from becoming over-ﬁt to the training data, and
thus results in a regression model that is capable of gen-
eralizing the information it has learned and predicting
values for data on which it has not been trained.

2.3.1. Feature Importance

The CART algorithm uses information theory to de-
cide which rule is the best choice for inferring stellar
parameters like age and mass from the supplied informa-
tion (Hastie et al. 2005, Chapter 9). At every stage, the
rule that creates the largest decrease in mean squared
error (MSE) is crafted. A rule may be, for example, “all
models with L < 0.4 L(cid:12) have M < 1 M(cid:12).” Rules are
created until every stellar model that was supplied to
that particular tree is fully explained by a sequence of
decisions. We moreover use a variant on random forests
known as extremely randomized trees (Geurts et al. 2006),
which further randomize attribute splittings (e.g. split
on L) and the location of the cut-point (e.g. split on 0.4
L/L(cid:12)) used when creating decision rules.

The process of constructing a random forest presents

6

Bellinger & Angelou et al.

Figure 3. A schematic representation of a random forest regressor for inferring fundamental stellar parameters. Observable
quantities such as Teﬀ and [Fe/H] and global asteroseismic observables like (cid:104)∆ν(cid:105) and (cid:104)δν0,2(cid:105) are input on the left side. These
quantities are then fed through to some number of hidden decision trees, which each independently predict parameters like age
and mass. The predictions are then averaged and output on the right side. All inputs and outputs are optional. For example,
surface gravities, luminosities, and radii are not always available from observations (e.g. with the KOI stars, see Section 3.3
below). In their absence, these quantities can be predicted instead of being supplied. In this case, those nodes can be moved
over to the “prediction” side instead of being on the “observations” side. Also, in addition to potentially unobserved inputs like
stellar radii, other interesting model parameters can be predicted as well, such as core hydrogen mass fraction or surface helium
abundance.

an opportunity for not only inferring stellar parameters
from observations, but also for understanding the rela-
tionships that exist in the stellar models. Each decision
tree explicitly ranks the relative “importance” of each ob-
servable quantity for inferring stellar parameters, where
importance is deﬁned in terms of both the reduction in
MSE after deﬁning a decision rule based on that quan-
tity and the number of models that use that rule. In
machine learning, the variables that have been measured
and are supplied as inputs to the algorithm are known
as “features.” Figure 4 shows a feature importance plot,
i.e. distributions of relative importance over all of the
trees in the forest for each feature used to infer stellar
parameters. The features that are used most often to

construct decision rules are metallicity and temperature,
which are each signiﬁcantly more important features
than the rest. The importance of [Fe/H] is due to the
fact that the determinations of quantities like the Z0
and D depend nearly entirely on it (see also Angelou
& Bellinger et al. in prep.). Note that importance does
not indicate indispensability: an appreciable fraction of
decision rules being made based oﬀ of one feature does
not mean that another forest without that feature would
not perform just as well. That being said, these results
indicate that the best area to improve measurements
would be in metallicity determinations, because for stars
being predicted using this random forest, less precise
values here means exploring many more paths and hence

Stellar Parameters in an Instant with Machine Learning

7

which ordinarily perform unconstrained regression and
are therefore not prevented from predicting non-physical
quantities such as negative masses or from violating
conservation requirements.

Secondly, due to the decision rule process that is ex-
plained below, random forests are insensitive to the scale
of the data. Unless care is taken, other regression meth-
ods will artiﬁcially weight some observable quantities
like temperature as being more important than, say, lu-
minosity, solely because temperatures are written using
larger numbers (e.g. 5777 vs. 1, see for example section
11.5.3 of Hastie et al. 2005 for a discussion). Conse-
quently, solutions obtained by other methods will change
if they are run using features that are expressed using
diﬀerent units of measure. For example, other methods
will produce diﬀerent regressors if trained on luminosity
values expressed in solar units verses values expressed
in ergs, whereas random forests will not. Commonly,
this problem is mitigated in other methods by means of
variable standardization and through the use of Maha-
labonis distances (Mahalanobis 1936). However, these
transformations are arbitrary, and handling variables
naturally without rescaling is thus preferred.

Thirdly, random forests take only seconds to train,
which can be a large beneﬁt if diﬀerent stars have dif-
ferent features available. For example, some stars have
luminosity information available whereas others do not,
so a diﬀerent regressor must be trained for each. In the
extreme case, if one wanted to make predictions for stars
using all of their respectively observed frequencies, one
would need to train a new regressor for each star using
the subset of simulated frequencies that correspond to
the ones observed for that star. Ignoring the diﬃculties
of surface-term corrections and mode identiﬁcations, such
an approach would be well-handled by random forest,
suﬀering only a small hit to performance from its rela-
tively small training cost. On the other hand, it would
be infeasible to do this on a star-by-star basis with most
other routines such as deep neural networks, because
those methods can take days or even weeks to train.

And ﬁnally, as we saw in the previous section, random
forests provide the opportunity to extract insight about
the actual regression being performed by examining the
importance of each feature in making predictions.

Figure 4. Relative importance of each observable feature in
inferring fundamental stellar parameters as measured by a
random forest regressor grown from a grid of evolutionary
models. The boxes display the ﬁrst (16%) and third (84%)
quartile of feature importance over all trees, the center line
indicates the median, and the whiskers extend to the most
extreme values.

arriving at less certain predictions.

For many stars, stellar quantities such as radii, lumi-
nosities, surface gravities, and/or oscillation modes with
spherical degree (cid:96) = 3 are not available from observations.
For example, the KOI data set (see Section 3.3 below)
lacks all of this information, and the hare-and-hound
exercise data (see Section 3.1 below) lack all of these
except luminosities. We therefore must train random
forests that predict those quantities instead of using them
as features. We show the relative importance for the
remaining features that were used to train these forests
in Figure 5. When (cid:96) = 3 modes and luminosities are
omitted, eﬀective temperature jumps in importance and
ties with [Fe/H] as the most important feature.

2.3.2. Advantages of CART

2.3.3. Uncertainty

We choose random forests over any of the many other
non-linear regression routines (e.g. Gaussian processes,
symbolic regression, neural networks, support vector
regression, etc.) for several reasons. First, random forests
perform constrained regression; that is, they only make
predictions within the boundaries of the supplied training
data (see e.g. Hastie et al. 2005, Section 9.2.1). This
is in contrast to other methods like neural networks,

There are three separate sources of uncertainty in
predicting stellar parameters. The ﬁrst is the systematic
uncertainty in the physics used to model stars. These
uncertainties are unknown, however, and hence cannot be
propagated. The second is the uncertainty belonging to
the observations of the star. We propagate measurement
uncertainties σ into the predictions by perturbing all
measured quantities n = 10 000 times with normal noise

8

Bellinger & Angelou et al.

Figure 5. Box-and-whisker plots of relative importance for each observable feature in measuring fundamental stellar parameters
for the hare-and-hound exercise data (left), where luminosities are available; and the Kepler objects-of-interest (right), where
they are not. Octupole ((cid:96) = 3) modes have not been measured in any of these stars, so (cid:104)δν1,3(cid:105) and (cid:104)r1,3(cid:105) from evolutionary
modelling are not supplied to these random forests. The boxes are sorted by median importance.

having zero mean and standard deviation σ. We account
for the covariance between asteroseismic separations and
ratios by recalculating them upon each perturbation.

The ﬁnal source is regression uncertainty. Fundamen-
tally, each parameter can only be constrained to the
extent that observations are able to bear information
pertaining to that parameter. Even if observations were
error-free, there still may exist a limit to which infor-
mation gleaned from the surface may tell us about the
physical qualities and evolutionary history of a star. We
quantify those limits via cross-validation: we train the
random forest on only a subset of the simulated evolution-
ary tracks and make predictions on a held-out validation
set. We randomly hold out a diﬀerent subset of the
tracks 25 times to serve as diﬀerent validation sets and
obtain averaged accuracy scores.

We calculate accuracies using several scores. The ﬁrst

is the explained variance score Ve:

Ve = 1 −

Var{y − ˆy}
Var{y}

(8)

where y is the true value we want to predict from the
validation set (e.g. stellar mass), ˆy is the predicted value
from the random forest, and Var is the variance, i.e.
the square of the standard deviation. This score tells

us the extent to which the regressor has reduced the
variance in the parameter it is predicting. The value
ranges from negative inﬁnity, which would be obtained
by a pathologically bad predictor; to one for a perfect
predictor, which occurs if all of the values are predicted
with zero error.

The next score we consider is the residuals of each
prediction, i.e. the absolute diﬀerence between the true
value y and the predicted value ˆy. Naturally, we want
this value to be as low as possible. We also consider
the precision of the regression ˆσ by taking the standard
deviation of predictions across all of the decision trees
in the forest. Finally, we consider these scores together
by calculating the distance of the residuals in units of
precision, i.e. |ˆy − y| /ˆσ.

Figure 6 shows these accuracies as a function of the
number of evolutionary tracks used in the training of
the random forest. Since the residuals and standard
deviations of each parameter are incomparable, we nor-
malize them by dividing by the maximum value. We
also consider the number of trees in the forest and the
number of models per evolutionary track. In this work,
we use 256 trees in each forest, which we have selected
via cross-validation by choosing a number of trees that

Stellar Parameters in an Instant with Machine Learning

9

is greater than the point at which we saw that the ex-
plained variance was no longer increasing greatly; see
Appendix D for an extended discussion.

When supplied with enough stellar models, the ran-
dom forest reduces the variance in each parameter and
is able to make precise inferences. The forest has very
high predictive power for most parameters, and as a
result, essentially all of the uncertainty when predicting
quantities such as stellar radii and luminosities will stem
from observational uncertainty. However, for some model
parameters—most notably the mixing length parameter—
there is still a great deal of variance in the residuals.
Prior to the point where the regressor has been trained
on about 500 evolutionary tracks, the diﬀerences between
the true and predicted mixing lengths actually have a
greater variance than just the true mixing lengths them-
selves. Likewise, the diﬀusion multiplication factor is
diﬃcult to constrain because a star can achieve the same
present-day [Fe/H] by either having a large initial non-
hydrogen abundance and a large diﬀusion multiplication
factor, or by having the same initial [Fe/H] as present
[Fe/H] but with diﬀusion disabled. These diﬃcult-to-
constrain parameters will therefore be predicted with
substantial uncertainties regardless of the precision of
the observations.

3. RESULTS

We perform three tests of our method. We begin
with a hare-and-hound simulation exercise to show that
we can reliably recover parameters. We then move to
the Sun and the solar-like stars 16 Cyg A & B, which
have been the subjects of many investigations; and we
conclude by applying our method to 34 Kepler objects-
In each case, we train our random forest
of-interest.
regressor on the subset of data that is available for the
stars being processed. In the case of the Sun and 16
Cygni, we know very accurately their radii, luminosities,
and surface gravities. For other stars, we will predict
this information instead of supplying it.

3.1. Hare and Hound

We performed a blind hare-and-hound exercise to eval-
uate the performance of our predictor. Author S.B.
prepared twelve models varied in mass, initial chemical
composition, and mixing length parameter with only
some models having overshooting and only some mod-
els having atomic diﬀusion included. The models were
evolved without rotation using the Yale rotating stellar
evolution code (YREC; Demarque et al. 2008), which is
a diﬀerent evolution code than the one that was used
to train the random forest. Eﬀective temperatures, lu-
minosities, [Fe/H] and νmax values as well as (cid:96) = 0, 1, 2
frequencies were obtained from each model. Author
G.C.A. perturbed the “observations” of these models

according to the scheme devised by Reese et al. (2016).
Appendix E lists the true values and the perturbed ob-
servations of the hare-and-hound models. The perturbed
observations and their uncertainties were given to author
E.P.B., who used the described method to recover the
stellar parameters of these models without being given
access to the true values. Relative diﬀerences between
the true and predicted ages, masses, and radii for these
models are plotted against their true values in Figure
7. The method is able to recover the true model values
within uncertainties even when they have been perturbed
by noise. We do not compare the predicted mixing length
parameter, overshooting parameter or diﬀusion multi-
plication factor the interpretation of these parameters
depends on how they have been deﬁned and their precise
implementation.

3.2. The Sun and 16 Cygni

To ensure conﬁdence in our predictions on Kepler data,
we ﬁrst degrade the frequencies of the Sun at solar min-
imum that were obtained by the Birmingham Solar-
Oscillations Network (BiSON; Davies et al. 2014a) to
the level of information that is achievable by the space-
craft. We also degrade the Sun’s uncertainties of other
observations by applying 16 Cyg B’s uncertainties of eﬀec-
tive temperature, luminosity, surface gravity, metallicity,
νmax, radius, and radial velocity. Finally, we perturb
each value with random Gaussian noise according to its
uncertainty to reﬂect the fact that the measured value
of an uncertain observation is not per se the true value.
We use the random forest whose feature importances
were shown in Figure 4 to predict the values of the Sun;
i.e. the random forest trained on eﬀective temperatures,
metallicities, luminosities, surface gravities, radii, and
asteroseismic quantities (cid:104)∆ν0(cid:105), (cid:104)δν0,2(cid:105), (cid:104)δν1,3(cid:105), (cid:104)r0,2(cid:105),
(cid:104)r1,3(cid:105), (cid:104)r0,1(cid:105), and (cid:104)r1,0(cid:105). We show in Figure 8 the densi-
ties for the predicted mass, initial composition, mixing
length parameter, overshoot coeﬃcient, and diﬀusion
multiplication factor needed for ﬁtting an evolutionary
model to degraded data of the Sun as well as the pre-
dicted solar age, core hydrogen abundance, and surface
helium abundance. As discussed in Section 2.3.3, these
densities show the distributions resulting from running
10 000 diﬀerent noise perturbations fed through the ran-
dom forest. Relative uncertainties (cid:15) = 100 · σ/µ are also
indicated, where µ is the mean and σ is the standard de-
viation of the quantity being predicted. Our predictions
are in good agreement with the known values (see also
Table 1 and Table 2).

Several parameters show multimodality due to model
degeneracies. For example, two solutions for the initial
helium are present. This is because it covaries with
the mixing length parameter: the peak of higher Y0
corresponds to the peak of lower αMLT and vice versa.

10

Bellinger & Angelou et al.

Figure 6. Evaluations of regression accuracy. Explained variance (top left), accuracy per precision distance (top right), normalized
absolute error (bottom left), and normalized uncertainty (bottom right) for each stellar parameter as a function of the number of
evolutionary tracks used in training the random forest. These results use 64 models per track and 256 trees in the random forest.

Likewise, high values of surface helium correspond to low
values of the diﬀusion multiplication factor.

Eﬀective temperatures, surface gravities, and metallici-
ties of 16 Cyg A and B were obtained from Ram´ırez et al.
(2009); radii and luminosities from White et al. (2013);
and frequencies from Davies et al. (2015). We obtained
the radial velocity measurements of 16 Cyg A and B
from Nidever et al. (2002) and corrected frequencies for
Doppler shifting as per the prescription in Davies et al.
(2014b). We tried with and without line-of-sight correc-
tions and found that it did not aﬀect the predicted quan-
tities or their uncertainties. We use the same random
forest as we used for the degraded solar data to predict
the parameters of these stars. The initial parameters—
masses, chemical compositions, mixing lengths, diﬀusion
multiplication factors, and overshoot coeﬃcients—for 16
Cygni as predicted by machine learning are shown in
Table 1, and the predicted current parameters—age, sur-
face helium and core hydrogen abundances—are shown
in Table 2. For reference we also show the predicted solar
values from these inputs there as well. These results sup-
port the hypothesis that 16 Cyg A and B were co-natal;
i.e. they formed at the same time with the same initial
composition.

We additionally predict the radii and luminosities of 16
Cyg A and B instead of using them as features. Figure 9
shows our inferred radii, luminosities and surface helium
abundances of 16 Cyg A and B plotted along with the
values determined by interferometry (White et al. 2013)
and an asteroseismic estimate (Verma et al. 2014). Here
again we ﬁnd excellent agreement between our method
and the measured values.

Metcalfe et al. (2015) performed detailed modelling of
16 Cyg A and B using the Asteroseismic Modeling Portal
(AMP), a genetic algorithm for matching individual fre-
quencies of stars to stellar models. They calculated their
results without heavy-element diﬀusion (i.e. with helium-
only diﬀusion) and without overshooting. In order to
account for systematic uncertainties, they multiplied the
spectroscopic uncertainties of 16 Cyg A and B by an
arbitrary constant C = 3. Therefore, in order to make a
fair comparison between the results of our method and
theirs, we generate a new matrix of evolutionary models
with those same conditions and also increase the uncer-
tainties on [Fe/H] by a factor of C. In Figure 10, we
show probability densities of the predicted parameters of
16 Cyg A and B that we obtain using machine learning
in comparison with the results obtained by AMP. We

Stellar Parameters in an Instant with Machine Learning

11

ﬁnd the values and uncertainties agree well. To perform
their analysis, AMP required more than 15 000 hours of
CPU time to model 16 Cyg A and B using the world’s
10th fastest supercomputer, the Texas Advanced Com-
puting Center Stampede (TOP500 2015). Here we have
obtained comparable results in roughly one minute on
a computing cluster with 64 2.5 GHz cores using only
global asteroseismic quantities and no individual frequen-
cies. Although more computationally expensive than our
method, detailed optimization codes like AMP do have
advantages in that they are additionally able to obtain
detailed structural models of stars.

Figure 7. Relative diﬀerences between the predicted and true
values for age (top), mass (middle), and radius (bottom) as a
function of the true values in the hare-and-hound simulation
exercise.

Table 1. Means and standard deviations for predicted initial stellar parameters of the Sun (degraded data) and
16 Cyg A and B.

Name

M/M(cid:12)

Y0

Z0

αMLT

αov

D

Sun

1.00 ± 0.012

0.270 ± 0.0062

0.020 ± 0.0014

1.88 ± 0.078

0.06 ± 0.015

3.7 ± 3.18

16 Cyg A 1.08 ± 0.016

0.262 ± 0.0073

0.022 ± 0.0014

1.86 ± 0.077

0.07 ± 0.028

0.9 ± 0.76

16 Cyg B 1.03 ± 0.015

0.268 ± 0.0065

0.021 ± 0.0015

1.83 ± 0.069

0.11 ± 0.029

1.9 ± 1.57

12

Bellinger & Angelou et al.

Figure 8. Predictions from machine learning of initial (top six) and current (bottom three) stellar parameters for degraded solar
data. Labels are placed at the mean and 3σ levels. Dashed and dot-dashed lines indicate the median and quartiles, respectively.
Relative uncertainties (cid:15) are shown beside each plot. Note that the overshoot parameter applies to all convective boundaries and
is not modiﬁed over the course of evolution, so a non-zero value does not imply a convective core.

Table 2. Means and standard deviations for predicted current-
age stellar parameters of the Sun (degraded data) and 16 Cyg
A and B.

Name

τ /Gyr

Xc

Ysurf

Sun

4.6 ± 0.20

0.34 ± 0.027

0.24 ± 0.017

16 Cyg A 6.9 ± 0.40

0.06 ± 0.024

0.246 ± 0.0085

16 Cyg B 6.8 ± 0.28

0.15 ± 0.023

0.24 ± 0.017

3.3. Kepler Objects of Interest

We obtain observations and frequencies of the KOI tar-
gets from Davies et al. (2016). We use line-of-sight radial
velocity corrections when available, which was only the
case for KIC 6278762 (Latham et al. 2002), KIC 10666592
(Maldonado et al. 2013), and KIC 3632418 (Gontcharov
2006). We use the random forest whose feature impor-
tances were shown in Figure 5 to predict the fundamental
parameters of these stars; that is, the random forest that
is trained on eﬀective temperatures, metallicities, and
asteroseismic quantities (cid:104)∆ν0(cid:105), (cid:104)δν0,2(cid:105), (cid:104)r0,2(cid:105), (cid:104)r0,1(cid:105), and
(cid:104)r1,0(cid:105). The predicted initial conditions—masses, chemi-

cal compositions, mixing lengths, overshoot coeﬃcients,
and diﬀusion multiplication factors—are shown in Table
3; and the predicted current conditions—ages, core hy-
drogen abundances, surface gravities, luminosities, radii,
and surface helium abundances—are shown in Table 4.
Figure 11 shows the fundamental parameters obtained
from our method plotted against those obtained by Silva
Aguirre et al. (2015, hereinafter KAGES). We ﬁnd good
agreement across all stars.

Although still in statistical agreement, the median val-
ues of our predicted ages are systematically lower and the
median values of our predicted masses are systematically

Stellar Parameters in an Instant with Machine Learning

13

Figure 9. Probability densities for predictions of 16 Cyg A (red) and B (blue) from machine learning of radii (top left), luminosities
(top right), and surface helium abundances (bottom). Relative uncertainties (cid:15) are shown beside each plot. Predictions and 2σ
uncertainties from interferometric (“int”) measurements and asteroseismic (“ast”) estimates are shown with arrows.

higher than those predicted by KAGES. We conjecture
that these discrepancies arise from diﬀerences in input
physics. We vary the eﬃciency of diﬀusion, the extent
of convective overshooting, and the value of the mixing
length parameter to arrive at these estimates, whereas
the KAGES models are calculated using ﬁxed amounts of
diﬀusion, without overshoot, and with a solar-calibrated
mixing length. Models with overshooting, for example,
will be more evolved at the same age due to having larger
core masses. Without direct access to their models, how-
ever, the exact reason is diﬃcult to pinpoint.

We ﬁnd a signiﬁcant linear trend in the Kepler objects-
of-interest between the diﬀusion multiplication factor

and stellar mass needed to reproduce observations (P =
0.0001 from a two-sided t-test with N − 2 = 32 degrees
of freedom). Since the values of mass and diﬀusion multi-
plication factor are uncertain, we use Deming regression
to estimate the coeﬃcients of this relation without re-
gression dilution (Deming 1943). We show the diﬀusion
multiplication factors as a function of stellar mass for all
of these stars in Figure 12. We ﬁnd that the diﬀusion
multiplication factor linearly decreases with mass, i.e.

D = (8.6 ± 1.94) − (5.6 ± 1.37) · M/M(cid:12)

(9)

and that this relation explains observations better than
any constant factor (e.g. D=1 or D=0).

Table 3. Means and standard deviations for initial conditions of the KOI data set inferred via machine learning.
The values obtained from degraded solar data predicted on these quantities are shown for reference.

KIC

M/M(cid:12)

Y0

Z0

αMLT

αov

D

3425851

1.15 ± 0.053

0.28 ± 0.020

0.015 ± 0.0028

1.9 ± 0.23

0.06 ± 0.057

0.5 ± 0.92

3544595

0.91 ± 0.032

0.270 ± 0.0090

0.015 ± 0.0028

1.9 ± 0.10

0.2 ± 0.11

4.9 ± 4.38

3632418

1.39 ± 0.057

0.267 ± 0.0089

0.019 ± 0.0032

2.0 ± 0.12

0.2 ± 0.14

1.1 ± 1.01

4141376

1.03 ± 0.036

0.267 ± 0.0097

0.012 ± 0.0025

1.9 ± 0.12

0.1 ± 0.11

4.0 ± 4.09

Table 3 continued

14

Bellinger & Angelou et al.

Figure 10. Probability densities showing predictions from machine learning of fundamental stellar parameters for 16 Cyg A (red)
and B (blue) along with predictions from AMP modelling. Relative uncertainties are shown beside each plot. Predictions and 2σ
uncertainties from AMP modelling are shown with arrows.

Table 3 (continued)

KIC

M/M(cid:12)

Y0

Z0

αMLT

αov

D

4143755

0.99 ± 0.037

0.277 ± 0.0050

0.014 ± 0.0026

1.77 ± 0.033

0.37 ± 0.071

13.4 ± 5.37

4349452

1.22 ± 0.056

0.28 ± 0.012

0.020 ± 0.0043

1.9 ± 0.17

0.10 ± 0.090

7.3 ± 8.82

4914423

1.19 ± 0.048

0.274 ± 0.0097

0.026 ± 0.0046

1.8 ± 0.11

0.08 ± 0.043

2.3 ± 1.6

5094751

1.11 ± 0.038

0.274 ± 0.0082

0.018 ± 0.0030

1.8 ± 0.11

0.07 ± 0.041

2.3 ± 1.39

5866724

1.29 ± 0.065

0.28 ± 0.011

0.027 ± 0.0058

1.8 ± 0.13

0.12 ± 0.086

7.0 ± 8.38

6196457

1.31 ± 0.058

0.276 ± 0.005

0.032 ± 0.0050

1.71 ± 0.050

0.16 ± 0.055

5.7 ± 2.34

6278762

0.76 ± 0.012

0.254 ± 0.0058

0.013 ± 0.0017

2.09 ± 0.069

0.06 ± 0.028

5.3 ± 2.23

6521045

1.19 ± 0.046

0.273 ± 0.0071

0.027 ± 0.0044

1.82 ± 0.074

0.12 ± 0.036

3.2 ± 1.31

7670943

1.30 ± 0.061

0.28 ± 0.017

0.021 ± 0.0045

2.0 ± 0.23

0.06 ± 0.064

1.0 ± 2.55

8077137

1.23 ± 0.070

0.270 ± 0.0093

0.018 ± 0.0028

1.8 ± 0.14

0.2 ± 0.11

2.9 ± 2.08

8292840

1.15 ± 0.079

0.28 ± 0.010

0.016 ± 0.0049

1.8 ± 0.15

0.1 ± 0.12

11. ± 10.7

8349582

1.23 ± 0.040

0.271 ± 0.0069

0.043 ± 0.0074

1.9 ± 0.12

0.11 ± 0.060

2.5 ± 1.11

8478994

0.81 ± 0.022

0.272 ± 0.0082

0.010 ± 0.0012

1.91 ± 0.054

0.21 ± 0.068

17. ± 9.74

8494142

1.42 ± 0.058

0.27 ± 0.010

0.028 ± 0.0046

1.70 ± 0.064

0.10 ± 0.051

1.6 ± 1.65

8554498

1.39 ± 0.067

0.272 ± 0.0082

0.031 ± 0.0032

1.70 ± 0.077

0.14 ± 0.079

1.7 ± 1.17

8684730

1.44 ± 0.030

0.277 ± 0.0075

0.041 ± 0.0049

1.9 ± 0.14

0.29 ± 0.094

15.2 ± 8.81

Table 3 continued

Stellar Parameters in an Instant with Machine Learning

15

Table 3 (continued)

KIC

M/M(cid:12)

Y0

Z0

αMLT

αov

D

8866102

1.26 ± 0.069

0.28 ± 0.013

0.021 ± 0.0048

1.8 ± 0.15

0.08 ± 0.070

5. ± 7.48

9414417

1.36 ± 0.054

0.264 ± 0.0073

0.018 ± 0.0028

1.9 ± 0.13

0.2 ± 0.1

2.2 ± 1.68

9592705

1.45 ± 0.038

0.27 ± 0.010

0.029 ± 0.0038

1.72 ± 0.064

0.12 ± 0.056

0.6 ± 0.47

9955598

0.93 ± 0.028

0.27 ± 0.011

0.023 ± 0.0039

1.9 ± 0.10

0.2 ± 0.13

2.2 ± 1.76

10514430

1.13 ± 0.053

0.277 ± 0.0046

0.021 ± 0.0039

1.78 ± 0.059

0.30 ± 0.097

4.7 ± 1.77

10586004

1.31 ± 0.078

0.274 ± 0.0055

0.038 ± 0.0071

1.8 ± 0.13

0.2 ± 0.13

4.3 ± 3.99

10666592

1.50 ± 0.023

0.30 ± 0.013

0.030 ± 0.0032

1.8 ± 0.11

0.06 ± 0.043

0.2 ± 0.14

10963065

1.09 ± 0.031

0.264 ± 0.0083

0.014 ± 0.0025

1.8 ± 0.11

0.05 ± 0.027

3.1 ± 2.68

11133306

1.11 ± 0.044

0.272 ± 0.0099

0.021 ± 0.0040

1.8 ± 0.16

0.04 ± 0.033

5. ± 5.75

11295426

1.11 ± 0.033

0.27 ± 0.010

0.025 ± 0.0036

1.81 ± 0.084

0.05 ± 0.035

1.3 ± 0.87

11401755

1.15 ± 0.039

0.271 ± 0.0057

0.015 ± 0.0023

1.88 ± 0.055

0.33 ± 0.071

3.8 ± 1.81

11807274

1.32 ± 0.079

0.276 ± 0.0097

0.024 ± 0.0051

1.77 ± 0.083

0.11 ± 0.066

5.4 ± 5.61

11853905

1.22 ± 0.055

0.272 ± 0.0072

0.029 ± 0.0050

1.8 ± 0.12

0.18 ± 0.086

3.3 ± 1.85

11904151

0.93 ± 0.033

0.265 ± 0.0091

0.016 ± 0.0030

1.8 ± 0.13

0.05 ± 0.029

3.1 ± 2.09

Sun

1.00 ± 0.0093

0.266 ± 0.0035

0.018 ± 0.0011

1.81 ± 0.032

0.07 ± 0.021

2.1 ± 0.83

Table 4. Means and standard deviations for current-age conditions of the KOI data set inferred via machine learning.
The values obtained from degraded solar data predicted on these quantities are shown for reference.

KIC

τ /Gyr

Xc

log g

L/L(cid:12)

R/R(cid:12)

Ysurf

3425851

3.7 ± 0.76

0.14 ± 0.081

4.234 ± 0.0098

2.7 ± 0.16

1.36 ± 0.022

0.27 ± 0.026

3544595

6.7 ± 1.47

0.31 ± 0.078

4.46 ± 0.016

0.84 ± 0.068

0.94 ± 0.020

0.23 ± 0.023

3632418

3.0 ± 0.36

0.10 ± 0.039

4.020 ± 0.0076

5.2 ± 0.25

1.91 ± 0.031

0.24 ± 0.021

4141376

3.4 ± 0.67

0.38 ± 0.070

4.41 ± 0.011

1.42 ± 0.097

1.05 ± 0.019

0.24 ± 0.022

4143755

8.0 ± 0.80

0.07 ± 0.022

4.09 ± 0.013

2.3 ± 0.12

1.50 ± 0.029

0.17 ± 0.023

4349452

2.4 ± 0.78

0.4 ± 0.10

4.28 ± 0.012

2.5 ± 0.14

1.32 ± 0.022

0.22 ± 0.043

4914423

5.2 ± 0.58

0.06 ± 0.032

4.162 ± 0.0097

2.5 ± 0.16

1.50 ± 0.022

0.24 ± 0.023

5094751

5.3 ± 0.67

0.07 ± 0.039

4.209 ± 0.0082

2.2 ± 0.13

1.37 ± 0.017

0.23 ± 0.024

5866724

2.4 ± 0.96

0.4 ± 0.12

4.24 ± 0.017

2.7 ± 0.13

1.42 ± 0.022

0.23 ± 0.038

6196457

4.0 ± 0.73

0.18 ± 0.061

4.11 ± 0.022

3.3 ± 0.21

1.68 ± 0.041

0.24 ± 0.016

6278762

10.3 ± 0.96

0.35 ± 0.026

4.557 ± 0.0084

0.34 ± 0.022

0.761 ± 0.0061

0.19 ± 0.023

6521045

5.6 ± 0.370

0.027 ± 0.0097

4.122 ± 0.0055

2.7 ± 0.15

1.57 ± 0.025

0.22 ± 0.019

7670943

2.3 ± 0.59

0.32 ± 0.088

4.234 ± 0.0099

3.3 ± 0.23

1.44 ± 0.025

0.26 ± 0.029

8077137

4.4 ± 0.96

0.08 ± 0.052

4.08 ± 0.016

3.7 ± 0.24

1.68 ± 0.044

0.22 ± 0.031

8292840

3.4 ± 1.48

0.3 ± 0.14

4.25 ± 0.023

2.6 ± 0.20

1.34 ± 0.026

0.19 ± 0.049

8349582

6.7 ± 0.53

0.02 ± 0.012

4.16 ± 0.012

2.2 ± 0.12

1.52 ± 0.016

0.23 ± 0.015

8478994

4.6 ± 1.75

0.50 ± 0.055

4.55 ± 0.012

0.51 ± 0.036

0.79 ± 0.014

0.21 ± 0.022

8494142

2.8 ± 0.52

0.18 ± 0.067

4.06 ± 0.018

4.5 ± 0.32

1.84 ± 0.043

0.24 ± 0.029

8554498

3.7 ± 0.79

0.09 ± 0.060

4.04 ± 0.015

4.1 ± 0.20

1.86 ± 0.043

0.25 ± 0.018

8684730

3.0 ± 0.38

0.24 ± 0.065

4.06 ± 0.046

4.1 ± 0.53

1.9 ± 0.11

0.17 ± 0.040

Table 4 continued

16

Bellinger & Angelou et al.

Table 4 (continued)

KIC

τ /Gyr

Xc

log g

L/L(cid:12)

R/R(cid:12)

Ysurf

8866102

1.9 ± 0.71

0.4 ± 0.11

4.27 ± 0.014

2.8 ± 0.16

1.36 ± 0.024

0.24 ± 0.039

9414417

3.1 ± 0.31

0.09 ± 0.030

4.016 ± 0.0058

5.0 ± 0.32

1.90 ± 0.032

0.21 ± 0.026

9592705

3.0 ± 0.38

0.05 ± 0.026

3.973 ± 0.0087

5.7 ± 0.37

2.06 ± 0.035

0.26 ± 0.015

9955598

7.0 ± 0.98

0.37 ± 0.035

4.494 ± 0.0061

0.66 ± 0.041

0.90 ± 0.013

0.25 ± 0.020

10514430

6.5 ± 0.89

0.06 ± 0.022

4.08 ± 0.014

2.9 ± 0.17

1.62 ± 0.026

0.22 ± 0.021

10586004

4.9 ± 1.39

0.12 ± 0.090

4.09 ± 0.041

3.1 ± 0.27

1.71 ± 0.070

0.24 ± 0.021

10666592

2.0 ± 0.24

0.15 ± 0.036

4.020 ± 0.0066

5.7 ± 0.33

1.98 ± 0.018

0.29 ± 0.014

10963065

4.4 ± 0.58

0.16 ± 0.054

4.292 ± 0.0070

2.0 ± 0.1

1.24 ± 0.015

0.22 ± 0.029

11133306

4.1 ± 0.84

0.22 ± 0.079

4.319 ± 0.0096

1.7 ± 0.11

1.21 ± 0.019

0.22 ± 0.036

11295426

6.2 ± 0.78

0.09 ± 0.036

4.283 ± 0.0059

1.65 ± 0.095

1.26 ± 0.016

0.24 ± 0.012

11401755

5.6 ± 0.630

0.037 ± 0.0053

4.043 ± 0.0071

3.4 ± 0.19

1.69 ± 0.026

0.21 ± 0.026

11807274

2.8 ± 1.05

0.3 ± 0.11

4.17 ± 0.024

3.5 ± 0.22

1.57 ± 0.038

0.22 ± 0.035

11853905

5.7 ± 0.78

0.04 ± 0.020

4.11 ± 0.011

2.7 ± 0.16

1.62 ± 0.030

0.23 ± 0.022

11904151

9.6 ± 1.43

0.08 ± 0.037

4.348 ± 0.0097

1.09 ± 0.06

1.07 ± 0.019

0.21 ± 0.026

Sun

4.6 ± 0.16

0.36 ± 0.012

4.439 ± 0.0038

1.01 ± 0.041

1.000 ± 0.0066

0.245 ± 0.0076

4. DISCUSSION

of input or output parameters.

The amount of time it takes to make predictions for a
star using a trained random forest can be decomposed
into two parts: the amount of time it takes to calculate
perturbations to the observations of the star (see Section
2.3.3), and the amount of time it takes to make a pre-
diction on each perturbed set of observations. Hence we
have

t = n(tp + tr)

(10)

where t is the total time, n is the number of perturba-
tions, tp is the time it takes to perform a single per-
turbation, and tr is the random forest regression time.
We typically see times of tp = (7.9 ± 0.7) · 10−3 (s) and
tr = (1.8 ± 0.4) · 10−5 (s). We chose a conservative
n = 10 000 for the results presented here, which results
in a time of around a minute per star. Since each star
can be processed independently and in parallel, a com-
puting cluster could feasibly process a catalog containing
millions of objects in less than a day. Since tr (cid:28) tp, the
calculation depends almost entirely on the time it takes
to perturb the observations.2 There is also the one-time
cost of training the random forest, which takes less than
a minute and can be reused without retraining on every
star with the same information. It does need to be re-
trained if one wants to consider a diﬀerent combination

2 Our perturbation code uses an interpreted language (R), so if

needed, there is still room for speed-up.

There is a one-time cost of generating the matrix of
training data. We ran our simulation generation scheme
for a week on our computing cluster and obtained 5 325
evolutionary tracks with 64 models per track, which re-
sulted in a 123 MB matrix of stellar models. This is
at least an order of magnitude fewer models than the
amount that other methods use. Furthermore, this is
in general more tracks than is needed by our method:
we showed in Figure 6 that for most parameters—most
notably age, mass, luminosity, radius, initial metallicity,
and core hydrogen abundance—one needs only a fraction
of the models that we generated in order to obtain good
predictive accuracies. Finally, unless one wants to con-
sider a diﬀerent range of parameters or diﬀerent input
physics, this matrix would not need to be calculated
again; a random forest trained on this matrix can be
re-used for all future stars that are observed. Of course,
our method would still work if trained using a diﬀerent
matrix of models, and our grid should work with other
grid-based modelling methods.

Previously, Pulone & Scaramella (1997) developed a
neural network for predicting stellar age based on the
star’s position in the Hertzsprung-Russell diagram. More
recently, Verma et al. (2016) have worked on incorpo-
rating seismic information into that analysis as we have
done here. Our method provides several advantages over
these approaches. Firstly, the random forests that we use
perform constrained regression, meaning that the values
we predict for quantities like age and mass will always be

Stellar Parameters in an Instant with Machine Learning

17

Figure 11. Predicted surface gravities, radii, luminosities, masses, and ages of 34 Kepler objects-of-interest plotted against the
suggested KAGES values. Medians, 16% quantiles, and 84% quantiles are shown for each point. A dashed line of agreement is
shown in all panels to guide the eye.

non-negative and within the bounds of the training data,
which is not true of the neural networks-based approach
that they take. Secondly, using averaged frequency sepa-
rations allows us to make predictions without need for
concern over which radial orders were observed. Thirdly,
we have shown that our random forests are very fast to
train, and can be retrained in only seconds for stars that
are missing observational constraints such as luminosities.
In contrast, deep neural networks are computationally
intensive to train, taking days or even weeks to converge
depending on the breadth of network topologies consid-
ered in the cross-validation. Finally, our grid is varied
in six initial parameters—M, Y0, Z0, αMLT, αov, and
D, which allows our method to explore a wide range of
stellar model parameters.

5. CONCLUSIONS

Here we have considered the constrained multiple-
regression problem of inferring fundamental stellar pa-
rameters from observations. We created a grid of evo-
lutionary tracks varied in mass, chemical composition,
mixing length parameter, overshooting coeﬃcient, and
diﬀusion multiplication factor. We evolved each track in

time along the main sequence and collected observable
quantities such as eﬀective temperatures and metallicities
as well as global statistics on the modes of oscillations
from models along each evolutionary path. We used
this matrix of stellar models to train a machine learning
algorithm to be able to discern the patterns that relate
observations to fundamental stellar parameters. We then
applied this method to hare-and-hound exercise data, the
Sun, 16 Cyg A and B, and 34 planet-hosting candidates
that have been observed by Kepler and rapidly obtained
precise initial conditions and current-age values of these
stars. Remarkably, we were able to empirically deter-
mine the value of the diﬀusion multiplication factor and
hence the eﬃciency of diﬀusion required to reproduce
the observations instead of inhibiting it ad hoc. A larger
sample size will better constrain the diﬀusion multipli-
cation factor and determine what other variables are
relevant in its parametrization. This is work in progress.
The method presented here has many advantages over
existing approaches. First, random forests can be trained
and used in only seconds and hence provide substantial
speed-ups over other methods. Observations of a star

18

Bellinger & Angelou et al.

Figure 12. Logarithmic diﬀusion multiplication factor as a function of stellar mass for 34 Kepler objects-of-interest. The solid
line is the line of best ﬁt from Equation 9 and the dashed lines are the 50% conﬁdence interval around this ﬁt.

simply need to be fed through the forest—akin to plug-
ging numbers into an equation—and do not need to be
subjected to expensive iterative optimization procedures.
Secondly, random forests perform non-linear and non-
parametric regression, which means that the method can
use orders-of-magnitude fewer models for the same level
of precision, while additionally attaining a more rigorous
appraisal of uncertainties for the predicted quantities.
Thirdly, our method allows us to investigate wide ranges
and combinations of stellar parameters. And ﬁnally,
the method presented here provides the opportunity to
extract insights from the statistical regression that is
being performed, which is achieved by examining the
relationships in stellar physics that the machine learns
by analyzing simulation data. This contrasts the blind
optimization processes of other methods that provide
an answer but do not indicate the elements that were
important in doing so.

We note that the predicted quantities reﬂect a set of
choices in stellar physics. Although such biases are im-
possible to propagate, varying model parameters that
are usually kept ﬁxed—such as the mixing length param-
eter, diﬀusion multiplication factor, and overshooting
coeﬃcient—takes us a step in the right direction. Fur-
thermore, the fact that quantities such as stellar radii
and luminosities—quantities that have been measured

accurately, not just precisely—can be reproduced both
precisely and accuractely by this method, gives a degree
of conﬁdence in its eﬃcacy.

The method we have presented here is currently only
applicable to main-sequence stars. We intend to extend
this study to later stages of evolution.

The research leading to the presented results has re-
ceived funding from the European Research Council
under the European Community’s Seventh Framework
Programme (FP7/2007-2013) / ERC grant agreement
no 338251 (StellarAges). This research was undertaken
in the context of the International Max Planck Research
School for Solar System Research. S.B. acknowledges
partial support from NSF grant AST-1514676 and NASA
grant NNX13AE70G. W.H.B. acknowledges research
funding by Deutsche Forschungsgemeinschaft (DFG) un-
der grant SFB 963/1 “Astrophysical ﬂow instabilities
and turbulence” (Project A18).

Software: Analysis in this manuscript was performed
with python 3.5.1 libraries scikit-learn 0.17.1 (Pedregosa
et al. 2011), NumPy 1.10.4 (Van Der Walt et al. 2011),
and pandas 0.17.1 (McKinney et al. 2010) as well as R
3.2.3 (R Core Team 2014) and the R libraries magicaxis
1.9.4 (Robotham 2015), RColorBrewer 1.1-2 (Neuwirth

Stellar Parameters in an Instant with Machine Learning

19

2014), parallelMap 1.3 (Bischl & Lang 2015), data.table
1.9.6 (Dowle et al. 2015), lpSolve 5.6.13 (Berkelaar &
others 2015), ggplot2 2.1.0 (Wickham 2009), GGally
1.0.1 (Schloerke et al. 2014), scales 0.3.0 (Wickham 2015),
deming 1.0-1 (Therneau 2014), and matrixStats 0.50.1
(Bengtsson 2015).

constraints:

ˆS = arg min

SijCij

S

APPENDIX

A. MODEL SELECTION

To prevent statistical bias towards the evolutionary
tracks that generate the most models, i.e. the ones that
require the most careful calculations and therefore use
smaller time-steps, or those that live on the main se-
quence for a longer amount of time; we select n = 64
models from each evolutionary track such that the mod-
els are as evenly-spaced in core hydrogen abundance as
possible. We chose 64 because it is a power of two, which
thus allows us to successively omit every other model
when testing our regression routine and still maintain
regular spacings.

Starting from the original vector of length m of core
hydrogen abundances (cid:126)X, we ﬁnd the subset of length m
that is closest to the optimal spacing (cid:126)B, where

(cid:126)B ≡

XT , . . . ,

(cid:20)

(m − i) · XT + XZ
m − 1

, . . . , XZ

(A1)

(cid:21)

with XZ being the core hydrogen abundance at ZAMS
and XT being that at TAMS. To obtain the closest
possible vector to (cid:126)B from our data (cid:126)X, we solve a trans-
portation problem using integer optimization (Delmotte
2014). First we set up a cost matrix C consisting of
absolute diﬀerences between the original abundances (cid:126)X
and the ideal abundances (cid:126)B:

|B1 − X1|

|B1 − X2|

|B1 − Xn|

|B2 − X1|
...
|Bm − X1|

|B2 − X2|
...
|Bm − X2|

C ≡











. . .

. . .
. . .
. . .

|B2 − Xn|
...
|Bm − Xn|

.











(A2)
We then require that exactly m values are selected from
(cid:126)X, and that each value is selected no more than one time.
Simply selecting the closest data point to each ideally-
separated point will not work because this could result
in the same point being selected twice; and selecting the
second closest point in that situation does not remedy it
because a diﬀerent result could be obtained if the points
were processed in a diﬀerent order.

We denote the optimal solution matrix by ˆS, and ﬁnd
it by minimising the cost matrix subject to the following

subject to

Sij ≤ 1 for all i = 1 . . . N

and

Sij = 1 for all j = 1 . . . M.

(A3)

ij
(cid:88)

j
(cid:88)

i
(cid:88)

(cid:126)X that are most near to being
The indices of
equidistantly-spaced are then found by looking at which
columns of ˆS contain ones. The solution is visualized in
Figure A1.

B. INITIAL GRID STRATEGY

The initial conditions of a stellar model can be viewed
as a six-dimensional hyperrectangle with dimensions M,
Y0, Z0, αMLT, αov, and D. In order to vary all of these
parameters simultaneously and ﬁll the hyperrectangle
as quickly as possible, we construct a grid of initial
conditions following a quasi-random point generation
scheme. This is in contrast to linear or random point
generation schemes, over which it has several advantages.
A linear grid subdivides all dimensions in which ini-
tial quantities can vary into equal parts and creates a
track of models for every combination of these subdi-
visions. Although in the limit such a strategy will ﬁll
the hyperrectangle of initial conditions, it does so very
slowly. It is furthermore suboptimal in the sense that
linear grids maximize redundant information, as each
varied quantity is tried with the exact same values of
all other parameters that have been considered already.
In a high-dimensional setting, if any of the parameters
are irrelevant to the task of the computation, then the
majority of the tracks in a linear grid will not contribute
any new information.

A reﬁnement on this approach is to create a grid of
models with randomly varied initial conditions. Such a
strategy ﬁlls the space more rapidly, and furthermore
solves the problem of redundant information. However,
this approach suﬀers from a diﬀerent problem: since the
points are generated at random, they tend to “clump up”
at random as well. This results in random gaps in the
parameter space, which are obviously undesirable.

Therefore, in order to select points that do not stack,
do not clump, and also ﬁll the space as rapidly as possible,
we generate Sobol numbers (Sobol 1967) in the unit 6-
cube and map them to the parameter ranges of each
quantity that we want to vary. Sobol numbers are a
sequence of m-dimensional vectors x1 . . . xn in the unit
hypercube I m constructed such that the integral of a
real function f in that space is equivalent in the limit to

20

Bellinger & Angelou et al.

Figure A1. A visaulization of the model selection process performed on each evolutionary track in order to obtain the same
number of models from each track. The blue crosses show all of the models along the evolutionary track as they vary from ZAMS
to TAMS in core hydrogen abundance and the red crosses show the models selected from this track. The models were chosen via
linear transport such that they satisfy Equation A3. For reference, an equidistant spacing is shown with black points.

that function evaluated on those numbers, that is,

seen in Figure C3.

f = lim
n→∞

(cid:90)Im

1
n

n

i=1
(cid:88)

f (xi)

(B4)

with the sequence being chosen such that the convergence
is achieved as quickly as possible. By doing this, we both
minimize redundant information and furthermore sample
the hyperspace of possible stars as uniformly as possible.
Figure B2 visualizes the diﬀerent methods of generating
multidimensional grids: linear, random, and the quasi-
random strategy that we took. This method applied
to initial model conditions was shown in Figure 1 with
1- and 2D projection plots of the evolutionary tracks
generated for our grid.

C. ADAPTIVE REMESHING

When performing element diﬀusion calculations in
MESA, the surface abundance of each isotope is con-
sidered as an average over the outermost cells of the
model. The number of outer cells N is chosen such that
the mass of the surface is more than ten times the mass
of the (N + 1)th cell. Occasionally, this approach can
lead to a situation where surface abundances change
dramatically and discontinuously in a single time-step.
These abundance discontinuities then propagate as dis-
continuities in eﬀective temperatures, surface gravities,
and radii. An example of such a diﬃculty is shown in
Figure C3.

Instead of being a physical reality, these eﬀects arise
only when there is insuﬃcient mesh resolution in the
outermost layers of the model. We therefore seek to
detect these cases and re-run any such evolutionary track
using a ﬁner mesh resolution. We consider a track an
outlier if its surface hydrogen abundance changes by more
than 1% in a single time-step. We iteratively re-run any
track with outliers detected using a ﬁner mesh resolution,
and, if necessary, smaller time-steps, until convergence
is reached. The process and a resolved track can also be

Some tracks still do not converge without surface abun-
dance discontinuities despite the ﬁneness of the mesh
or the brevity of the time-steps, and are therefore not
included in our study. These troublesome evolutionary
tracks seem to be located only in a thin ridge of models
having suﬃciently high stellar mass (M > 1), a deﬁcit of
initial metals (Z < 0.001) and a speciﬁc ineﬃciency of
diﬀusion (D (cid:39) 0.01). A visualization of this is shown in
Figure C4.

D. EVALUATING THE REGRESSOR

In training the random forest regressor, we must deter-
mine how many evolutionary tracks N to include, how
many models M to extract from each evolutionary track,
and how many trees T to use when growing the forest.
As such it is useful to deﬁne measures of gauging the
accuracy of the random forest so that we may evaluate
it with diﬀerent combinations of these parameters.

By far the most common way of measuring the quality
of a random forest regressor is its so-called “out-of-bag”
(OOB) score (see e.g. section 3.1 of Breiman 2001). While
each tree is trained on only a subset (or “bag”) of the
stellar models, all trees are tested on all of the models
that they did not see. This provides an accuracy score
representing how well the forest will perform when pre-
dicting on observations that it has not seen yet. We can
then use the scores deﬁned in Section 2.3.3 to calculate
OOB scores.

However, such an approach to scoring is too optimistic
in this scenario. Since a tree can get models from every
simulation, predicting the parameters of a model when
the tree has been trained on one of that model’s neigh-
bors leads to an artiﬁcially inﬂated OOB score. This is
especially the case for quantities like stellar mass, which
do not change along the main sequence. A tree that has
witnessed neighbors on either side of the model being
predicted will have no error when predicting that model’s
mass, and hence the score will seem artiﬁcially better

Stellar Parameters in an Instant with Machine Learning

21

Linear

Random

Quasi-random

Figure B2. Results of diﬀerent methods for generating multidimensional grids portrayed via a unit cube projected onto a unit
square. Linear (left), random (middle), and quasi-random (right) grids are generated in three dimensions, with color depicting
the third dimension, i.e., the distance between the reader and the screen. From top to bottom, all three methods are shown with
100, 400, and 2000 points generated, respectively.

than it should be.

Therefore, we opt instead to build validation sets con-
taining entire tracks that are left out from the training
of the random forest. We omit models and tracks in pow-
ers of two so that we may roughly maintain the regular
spacing that we have established in our grid of models
(refer back to Appendices B and A for details).

We have already shown in Figure 6 these cross-
validated scores as a function of the number of evo-
lutionary tracks. Figure D5 now shows these scores as
a function of the number of models obtained from each
evolutionary track, and Figure D6 shows them as a func-
tion of the number of trees in the forest. Naturally, ˆσ
increases with the number of trees, but this is not a

mark against having more trees: this score is trivially
minimal when there is only one tree, as that tree must
agree with itself! We ﬁnd that although more is better
for all quantities, there is not much improvement after
about T = 32 and M = 16. It is also interesting to note
that the predictions do not suﬀer very much from using
only four models per track, which results in a random
forest trained on only a few thousand models.

E. HARE-AND-HOUND

Table E1 lists the true values of the hare-and-hound
exercise performed here, and Table E2 lists the perturbed
inputs that were supplied to the machine learning algo-
rithm.

Table E1. True values for the hare-and-hound exercise.

Model R/R(cid:12) M/M(cid:12)

τ

Teff

L/L(cid:12) [Fe/H]

Y0

νmax

αov

0

1

2

3

4

1.705

1.388

1.068

1.126

1.497

1.303

1.279

0.951

1.066

1.406

3.725

6297.96

2.608

5861.38

6.587

5876.25

2.242

6453.57

1.202

6506.26

4.11

2.04

1.22

1.98

3.61

0.03

0.26

0.04

0.2520

1313.67 No

0.2577

2020.34 No

0.3057

2534.29 No

-0.36

0.2678

2429.83 No

0.14

0.2629

1808.52 No

D

No

No

No

No

No

Table E1 continued

22

Bellinger & Angelou et al.

Figure C3. Three iterations of surface abundance disconti-
nuity detection and iterative remeshing for an evolutionary
track. The detected discontinuities are encircled in red. The
third iteration has no discontinuities and so this track is
considered to have converged.

Table E1 (continued)

Model R/R(cid:12) M/M(cid:12)

τ

Teff

L/L(cid:12) [Fe/H]

Y0

νmax

αov

5

6

7

8

9

10

11

1.331

0.953

1.137

1.696

0.810

1.399

1.233

1.163

0.983

1.101

1.333

0.769

1.164

1.158

4.979

6081.35

2.757

5721.37

2.205

6378.23

2.792

6382.22

9.705

5919.70

6.263

5916.71

2.176

6228.02

2.18

0.87

1.92

4.29

0.72

2.15

2.05

-0.06

-0.31

-0.07

-0.83

0.00

0.11

0.03

0.2499

1955.72 No

D

No

No

No

No

No

0.2683

3345.56 No

0.2504

2483.83 No

0.2555

1348.83 No

0.2493

3563.09 No

0.2480

1799.10 Yes Yes

0.2796

2247.53 Yes Yes

Table E1 continued

Stellar Parameters in an Instant with Machine Learning

23

Figure C4. Stellar mass as a function of diﬀusion multiplication factor colored by initial surface metallicity (left) and ﬁnal
surface metallicity (right). A ridge of missing points indicating unconverged evolutionary tracks can be seen around a diﬀusion
multiplication factor of 0.01. Beyond this ridge, tracks that were initially metal-poor end their main-sequence lives with all of
their metals drained from their surfaces.

Table E1 (continued)

Model R/R(cid:12) M/M(cid:12)

τ

Teff

L/L(cid:12) [Fe/H]

Y0

νmax

αov

D

Table E2. Supplied (perturbed) inputs for the hare-and-
hound exercise.

Model

Teff

L/L(cid:12)

[Fe/H]

νmax

0

1

2

3

4

5

6

7

8

9

10

11

6237 ± 85

4.2 ± 0.12

-0.03 ± 0.09

1398 ± 66

5806 ± 85

2.1 ± 0.06

0.16 ± 0.09

2030 ± 100

5885 ± 85

1.2 ± 0.04

-0.05 ± 0.09

2630 ± 127

6422 ± 85

2.0 ± 0.06

-0.36 ± 0.09

2480 ± 124

6526 ± 85

3.7 ± 0.11

0.14 ± 0.09

1752 ± 89

6118 ± 85

2.2 ± 0.06

0.04 ± 0.09

1890 ± 101

5741 ± 85

0.8 ± 0.03

0.06 ± 0.09

3490 ± 165

6289 ± 85

2.0 ± 0.06

-0.28 ± 0.09

2440 ± 124

6351 ± 85

4.3 ± 0.13

-0.12 ± 0.09

1294 ± 67

5998 ± 85

0.7 ± 0.02

-0.85 ± 0.09

3290 ± 179

5899 ± 85

2.2 ± 0.06

-0.03 ± 0.09

1930 ± 101

6251 ± 85

2.0 ± 0.06

0.13 ± 0.09

2360 ± 101

Table E2 continued

24

Bellinger & Angelou et al.

Figure D5. Explained variance (top left), accuracy per precision distance (top right), normalized absolute error (bottom left),
and normalized standard deviation of predictions (bottom right) for each stellar parameter as a function of the number of models
per evolutionary track.

Table E2 (continued)

Model

Teff

L/L(cid:12)

[Fe/H]

νmax

Angelou & Bellinger et al. in prep.,
Basu, S., & Antia, H. M. 1994, MNRAS, 269, 1137
Bazot, M., Bourguignon, S., & Christensen-Dalsgaard, J. 2012,

170

REFERENCES

MNRAS, 427, 1847

Bellinger, E. 2016, asteroseismology: Fundamental Parameters of
Main-Sequence Stars in an Instant with Machine Learning,
doi:10.5281/zenodo.55400

Bengtsson, H. 2015, matrixStats: Methods that Apply to Rows
and Columns of Matrices (and to Vectors), r package version
0.14.2

Berkelaar, M., & others. 2015, lpSolve: Interface to lpSolve v. 5.5
to Solve Linear/Integer Programs, r package version 5.6.12
Bischl, B., & Lang, M. 2015, parallelMap: Uniﬁed Interface to

Parallelization Back-Ends, r package version 1.3

Breiman, L. 2001, Machine learning, 45, 5
Brown, T. M., Christensen-Dalsgaard, J., Weibel-Mihalas, B., &

Gilliland, R. L. 1994, ApJ, 427, 1013

Campante, T. L., Barclay, T., Swift, J. J., et al. 2015, ApJ, 799,

Chaplin, W. J., & Miglio, A. 2013, Annual Review of Astronomy

Chaplin, W. J., Kjeldsen, H., Christensen-Dalsgaard, J., et al.

and Astrophysics, 51, 353

2011, Science, 332, 213

Chaplin, W. J., Basu, S., Huber, D., et al. 2014, ApJS, 210, 1
Chiappini, C., Minchev, I., Anders, F., et al. 2015, Astrophysics

and Space Science Proceedings, 39, 111

Christensen-Dalsgaard, J. 2008, Ap&SS, 316, 113
Davies, G. R., Broomhall, A. M., Chaplin, W. J., Elsworth, Y., &

Hale, S. J. 2014a, MNRAS, 439, 2025

Davies, G. R., Handberg, R., Miglio, A., et al. 2014b, MNRAS,

Davies, G. R., Chaplin, W. J., Farr, W. M., et al. 2015, MNRAS,

445, L94

446, 2959

Burgers, J. M. 1969, Flow equations for composite gases, Tech.

Davies, G. R., Aguirre, V. S., Bedding, T. R., et al. 2016,

rep., DTIC Document

MNRAS, 456, 2183

Stellar Parameters in an Instant with Machine Learning

25

Figure D6. Explained variance (top left), accuracy per precision distance (top right), normalized absolute error (bottom left),
and normalized model uncertainty (bottom right) for each stellar parameter as a function of the number of trees used in training
the random forest.

Delmotte, F. 2014, Sample equidistant points from a numeric

Metcalfe, T. S., Creevey, O. L., & Davies, G. R. 2015, ApJL, 811,

vector, StackOverﬂow,
http://stackoverﬂow.com/questions/23145595 (version:
2014-04-18)

Demarque, P., Guenther, D. B., Li, L. H., Mazumdar, A., &

Straka, C. W. 2008, Ap&SS, 316, 31

Deming, W. E. 1943, Statistical adjustment of data. (Wiley)
Dowle, M., Srinivasan, A., Short, T., with contributions from

R Saporta, S. L., & Antonyan, E. 2015, data.table: Extension of
Data.frame, r package version 1.9.6

L37

27

Metcalfe, T. S., Creevey, O. L., Do˘gan, G., et al. 2014, ApJS, 214,

Miglio, A., Chiappini, C., Morel, T., et al. 2013, MNRAS, 429, 423
Mohr, P. J., Newell, D. B., & Taylor, B. N. 2015, ArXiv e-prints,

arXiv:1507.07956

Morel, P., & Th´evenin, F. 2002, A&A, 390, 611
Mosser, B., Elsworth, Y., Hekker, S., et al. 2012, A&A, 537, A30
Neuwirth, E. 2014, RColorBrewer: ColorBrewer Palettes, r

Gai, N., Basu, S., Chaplin, W. J., & Elsworth, Y. 2011, ApJ, 730,

package version 1.1-2

Geurts, P., Ernst, D., & Wehenkel, L. 2006, Machine learning, 63,

Vogt, S. S. 2002, ApJS, 141, 503

Nidever, D. L., Marcy, G. W., Butler, R. P., Fischer, D. A., &

Paxton, B., Bildsten, L., Dotter, A., et al. 2011, The
Astrophysical Journal Supplement Series, 192, 3

Paxton, B., Cantiello, M., Arras, P., et al. 2013, ApJS, 208, 4
Pedregosa, F., Varoquaux, G., Gramfort, A., et al. 2011, Journal

of Machine Learning Research, 12, 2825

Pitjeva, E. 2015, Journal of Physical and Chemical Reference

Gontcharov, G. A. 2006, Astronomy Letters, 32, 759
Grevesse, N., & Sauval, A. J. 1998, SSRv, 85, 161
Haberreiter, M., Schmutz, W., & Kosovichev, A. G. 2008, ApJL,

Hampel, F. R. 1971, The Annals of Mathematical Statistics, 1887
Hastie, T., Tibshirani, R., Friedman, J., & Franklin, J. 2005, The

Data, 44, 031210

Mathematical Intelligencer, 27, 83

Pulone, L., & Scaramella, R. 1997, in Neural Nets WIRN

Latham, D. W., Stefanik, R. P., Torres, G., et al. 2002, AJ, 124,

VIETRI-96 (Springer), 231–236

Mahalanobis, P. C. 1936, Proceedings of the National Institute of

ApJ, 725, 2176

Sciences (Calcutta), 2, 49

R Core Team. 2014, R: A Language and Environment for

Maldonado, J., Villaver, E., & Eiroa, C. 2013, A&A, 554, A84
McKinney, W., et al. 2010, in Proceedings of the 9th Python in

Statistical Computing, R Foundation for Statistical Computing,
Vienna, Austria

Science Conference, Vol. 445, 51–56

Metcalfe, T. S., Creevey, O. L., & Christensen-Dalsgaard, J. 2009,

Ram´ırez, I., Mel´endez, J., & Asplund, M. 2009, A&A, 508, L17
Rauer, H., Catala, C., Aerts, C., et al. 2014, Experimental

ApJ, 699, 373

Astronomy, 38, 249

Quirion, P.-O., Christensen-Dalsgaard, J., & Arentoft, T. 2010,

63

3

675, L53

1144

26

Bellinger & Angelou et al.

Reese, D. R., Chaplin, W. J., Davies, G. R., et al. 2016, Submitted
Ricker, G. R., Winn, J. N., Vanderspek, R., et al. 2015, Journal of
Astronomical Telescopes, Instruments, and Systems, 1, 014003

Robotham, A. 2015, magicaxis: Pretty Scientiﬁc Plotting with

Therneau, T. 2014, deming: Deming, Thiel-Sen and

Passing-Bablock Regression, r package version 1.0-1

TOP500. 2015, TOP500 Supercomputer Site
Van Der Walt, S., Colbert, S. C., & Varoquaux, G. 2011,

Minor-Tick and log Minor-Tick Support, r package version 1.9.4

Computing in Science & Engineering, 13, 22

Rogers, F. J., & Nayfonov, A. 2002, ApJ, 576, 1064
Rosenthal, C. S., Christensen-Dalsgaard, J., Nordlund, ˚A., Stein,

R. F., & Trampedach, R. 1999, A&A, 351, 689

Roxburgh, I. W., & Vorontsov, S. V. 2003, A&A, 411, 215
Schloerke, B., Crowley, J., Cook, D., et al. 2014, GGally:

Verma, K., Hanasoge, S., Bhattacharya, J., Antia, H., &

Krishnamurthy, G. 2016, ArXiv e-prints, arXiv:1602.00902
Verma, K., Faria, J. P., Antia, H. M., et al. 2014, ApJ, 790, 138
White, T. R., Huber, D., Maestro, V., et al. 2013, MNRAS, 433,

1262

Extension to ggplot2., r package version 0.5.0

Wickham, H. 2009, ggplot2: elegant graphics for data analysis

Silva Aguirre, V., Davies, G. R., Basu, S., et al. 2015, MNRAS,

(Springer New York)

—. 2015, scales: Scale Functions for Visualization, r package

Sobol, I. M. 1967, USSR Computational mathematics and

version 0.3.0

452, 2127

mathematical physics, 86

Draft version July 11, 2016
Preprint typeset using LATEX style AASTeX6 v. 1.0

FUNDAMENTAL PARAMETERS OF MAIN-SEQUENCE STARS IN AN INSTANT WITH MACHINE LEARNING

Earl P. Bellinger1,2,3, George C. Angelou1,2, Saskia Hekker1,2, Sarbani Basu4, Warrick H. Ball5, and Elisabeth
Guggenberger1,2
1 Max-Planck-Institut f¨ur Sonnensystemforschung, Justus-von-Liebig-Weg 3, 37077 G¨ottingen, Germany
2 Stellar Astrophysics Centre, Department of Physics and Astronomy, Aarhus University, Ny Munkegade 120, DK-8000 Aarhus C, Denmark
3 Institut f¨ur Informatik, Georg-August-Universit¨at G¨ottingen, Goldschmidtstrasse 7, 37077 G¨ottingen, Germany
4 Department of Astronomy, Yale University, New Haven, CT 06520, USA
5 Institut f¨ur Astrophysik G¨ottingen, Friedrich-Hund-Platz 1, 37077 G¨ottingen, Germany

ABSTRACT
Owing to the remarkable photometric precision of space observatories like Kepler, stellar and planetary
systems beyond our own are now being characterized en masse for the ﬁrst time. These characterizations
are pivotal for endeavors such as searching for Earth-like planets and solar twins, understanding the
mechanisms that govern stellar evolution, and tracing the dynamics of our Galaxy. The volume of data
that is becoming available, however, brings with it the need to process this information accurately and
rapidly. While existing methods can constrain fundamental stellar parameters such as ages, masses,
and radii from these observations, they require substantial computational eﬀorts to do so.
We develop a method based on machine learning for rapidly estimating fundamental parameters of
main-sequence solar-like stars from classical and asteroseismic observations. We ﬁrst demonstrate this
method on a hare-and-hound exercise and then apply it to the Sun, 16 Cyg A & B, and 34 planet-hosting
candidates that have been observed by the Kepler spacecraft. We ﬁnd that our estimates and their
associated uncertainties are comparable to the results of other methods, but with the additional beneﬁt
of being able to explore many more stellar parameters while using much less computation time. We
furthermore use this method to present evidence for an empirical diﬀusion-mass relation. Our method
is open source and freely available for the community to use.a
Keywords: methods: statistical — stars: abundances — stars:

fundamental parameters — stars:

low-mass — stars: oscillations — stars: solar-type

1. INTRODUCTION

In recent years, dedicated photometric space missions
have delivered dramatic improvements to time-series ob-
servations of solar-like stars. These improvements have
come not only in terms of their precision, but also in
their time span and sampling, which has thus enabled
direct measurement of dynamical stellar phenomena such
as pulsations, binarity, and activity. Detailed measure-
ments like these place strong constraints on models used
to determine the ages, masses, and chemical composi-
tions of these stars. This in turn facilitates a wide range
of applications in astrophysics, such as testing theories
of stellar evolution, characterizing extrasolar planetary
systems (e.g. Campante et al. 2015; Silva Aguirre et al.
2015), assessing galactic chemical evolution (e.g. Chiap-

a The source code for all analyses and for all ﬁgures appearing in
this manuscript can be found electronically at https://github.
com/earlbellinger/asteroseismology (Bellinger 2016).

pini et al. 2015), and performing ensemble studies of
the Galaxy (e.g. Chaplin et al. 2011; Miglio et al. 2013;
Chaplin et al. 2014).

The motivation to increase photometric quality has
in part been driven by the goal of measuring oscillation
modes in stars that are like our Sun. Asteroseismology,
the study of these oscillations, provides the opportunity
to constrain the ages of stars through accurate inferences
of their interior structures. However, stellar ages cannot
be measured directly; instead, they depend on indirect
determinations via stellar modelling.

Traditionally, to determine the age of a star, proce-
dures based on iterative optimization (hereinafter IO)
seek the stellar model that best matches the available
observations (Brown et al. 1994). Several search strate-
gies have been employed, including exploration through
a pre-computed grid of models (i.e. grid-based modelling,
hereinafter GBM; see Gai et al. 2011; Chaplin et al.
2014); or in situ optimization (hereinafter ISO) such as
genetic algorithms (Metcalfe et al. 2014), Markov-chain

6
1
0
2
 
l
u
J
 
6
 
 
]

R
S
.
h
p
-
o
r
t
s
a
[
 
 
1
v
7
3
1
2
0
.
7
0
6
1
:
v
i
X
r
a

2

Bellinger & Angelou et al.

Monte Carlo (Bazot et al. 2012), or the downhill simplex
algorithm (Paxton et al. 2013; see e.g. Silva Aguirre et al.
2015 for an extended discussion on the various methods
of dating stars). Utilizing the detailed observations from
the Kepler and CoRoT space telescopes, these proce-
dures have constrained the ages of several ﬁeld stars
to within 10% of their main-sequence lifetimes (Silva
Aguirre et al. 2015).

IO is computationally intensive in that it demands
the calculation of a large number of stellar models (see
Metcalfe et al. 2009 for a discussion). ISO requires that
new stellar tracks are calculated for each target, as they
do not know a priori all of the combinations of stellar pa-
rameter values that the optimizer will need for its search.
They furthermore converge to local minima and there-
fore need to be run multiple times from diﬀerent starting
points to attain global coverage. GBM by way of inter-
polation in a high-dimensional space, on the other hand,
is sensitive to the resolution of each parameter and thus
requires a very ﬁne grid of models to search through (see
e.g. Quirion et al. 2010, who use more than ﬁve million
models that were varied in just four initial parameters).
Additional dimensions such as eﬃciency parameters (e.g.
overshooting or mixing length parameters) signiﬁcantly
impact on the number of models needed and hence the
search times for these methods. As a consequence, these
approaches typically use, for example, a solar-calibrated
mixing length parameter or a ﬁxed amount of convec-
tive overshooting. Since these values in other stars are
unknown, keeping them ﬁxed therefore results in under-
estimations of uncertainties. This is especially important
in the case of atomic diﬀusion, which is essential when
modelling the Sun (see e.g. Basu & Antia 1994), but is
usually disabled for stars with M/M(cid:12) > 1.4 because it
leads to the unobserved consequence of a hydrogen-only
surface (Morel & Th´evenin 2002).

These concessions have been made because the relation-
ships connecting observations of stars to their internal
properties are non-linear and diﬃcult to characterize.
Here we will show that through the use of machine learn-
ing, it is possible to avoid these diﬃculties by capturing
those relations statistically and using them to construct a
regression model capable of relating observations of stars
to their structural, chemical, and evolutionary proper-
ties. The relationships can be learned using many fewer
models than IO methods require, and can be used to
process entire stellar catalogs with a cost of only seconds
per star.

To date, only about a hundred solar-like oscillators
have had their frequencies resolved, allowing each of them
be modelled in detail using costly methods based on IO.
In the forthcoming era of TESS (Ricker et al. 2015) and
PLATO (Rauer et al. 2014), however, seismic data for
many more stars will become available, and it will not

be possible to dedicate large amounts of supercomputing
time to every star. Furthermore, for many stars, it will
only be possible to resolve global asteroseismic quantities
rather than individual frequencies. Therefore, the ability
to rapidly constrain stellar parameters for large numbers
of stars by means of global oscillation analysis will be
paramount.

In this work, we consider the constrained multiple-
regression problem of inferring fundamental stellar pa-
rameters from observable quantities. We construct a
random forest of decision tree regressors to learn the
relationships connecting observable quantities of main-
sequence (MS) stars to their zero-age main-sequence
(ZAMS) histories and current-age structural and chem-
ical attributes. We validate our technique by inferring
the parameters of simulated stars in a hare-and-hound
exercise, the Sun, and the well-studied stars 16 Cyg A
and B. Finally, we conclude by applying our method on
a catalog of Kepler objects-of-interest (hereinafter KOI;
Davies et al. 2016).

We explore various model physics by considering stellar
evolutionary tracks that are varied not only in their
initial mass and chemical composition, but also in their
eﬃciency of convection, extent of convective overshooting,
and strength of gravitational settling. We compare our
results to the recent ﬁndings from GBM (Silva Aguirre
et al. 2015), ISO (Metcalfe et al. 2015), interferometry
(White et al. 2013), and asteroseismic glitch analyses
(Verma et al. 2014) and ﬁnd that we obtain similar
estimates but with orders-of-magnitude speed-ups.

2. METHOD

We seek a multiple-regression model capable of charac-
terizing observed stars. To obtain such a model, we build
a matrix of evolutionary simulations and use machine
learning to discover relationships in the stellar models
that connect observable quantities of stars to the model
quantities that we wish to predict. The matrix is struc-
tured such that each column contains a diﬀerent stellar
quantity and each row contains a diﬀerent stellar model.
We construct this matrix by extracting models along
evolutionary sequences (see Appendix A for details on
the model selection process) and summarizing them to
yield the same types of information as the stars being ob-
served. Although each star (and each stellar model) may
have a diﬀerent number of oscillation modes observed,
it is possible to condense this information into only a
few numbers by leveraging the fact that the frequencies
of these modes follow a regular pattern (for a review of
solar-like oscillations, see Chaplin & Miglio 2013). Once
the machine has processed this matrix, one can feed the
algorithm a catalogue of stellar observations and use it
to predict the fundamental parameters of those stars.

The observable information obtained from models that

Stellar Parameters in an Instant with Machine Learning

3

can be used to inform the algorithm may include, but
is not limited to, combinations of temperatures, metal-
licities, global oscillation information, surface gravities,
luminosities, and/or radii. From these, the machine can
learn how to infer stellar parameters such as ages, masses,
core hydrogen and surface helium abundances. If lumi-
nosities, surface gravities, and/or radii are not supplied,
then they may be predicted as well. In addition, the
machine can also infer evolutionary parameters such as
the initial stellar mass and initial chemical compositions
as well as the mixing length parameter, overshoot co-
eﬃcient, and diﬀusion multiplication factor needed to
reproduce observations, which are explained in detail
below.

2.1. Model Generation

We use the open-source 1D stellar evolution code Mod-
ules for Experiments in Stellar Astrophysics (MESA; Pax-
ton et al. 2011) to generate main-sequence stellar models
from solar-like evolutionary tracks varied in initial mass
M, helium Y0, metallicity Z0, mixing length parameter
αMLT, overshoot coeﬃcient αov, and diﬀusion multiplica-
tion factor D. The diﬀusion multiplication factor serves
to amplify or diminish the eﬀects of diﬀusion, where a
value of zero turns it oﬀ and a value of two doubles all
velocities. The initial conditions are varied in the ranges
M ∈ [0.7, 1.6] M(cid:12), Y0 ∈ [0.22, 0.34], Z0 ∈ [10−5, 10−1]
(varied logarithmically), αMLT ∈ [1.5, 2.5], αov ∈ [10−4, 1]
(varied logarithmically), and D ∈ [10−6, 102] (varied log-
arithmically). We put a cut-oﬀ of 10−3 and 10−5 on αov
and D, respectively, below which we consider them to be
zero and disable them. The initial parameters of each
track are chosen in a quasi-random fashion so as to pop-
ulate the initial-condition hyperspace as homogeneously
and rapidly as possible (shown in Figure 1; see Appendix
B for more details).

We use MESA version r8118 with the Helmholtz-
formulated equation of state that allows for radiation
pressure and interpolates within the 2005 update of the
OPAL EOS tables (Rogers & Nayfonov 2002). We as-
sume a Grevesse & Sauval (1998) solar composition for
our initial abundances and opacity tables. Since we
restrict our study to the main sequence, we use an eight-
isotope nuclear network consisting of 1H, 3He, 4He, 12C,
14N, 16O, 20Ne, and 24Mg. We use a step function for
overshooting and set a scaling factor f0 = αov/5 to deter-
mine the radius r0 = Hp · f0 inside the convective zone
at which convection switches to overshooting, where Hp
is the pressure scale height. The overshooting parameter
applies to all convective boundaries and is kept ﬁxed
throughout the course of a track’s evolution, so a non-
zero value does not imply that the model has a convective
core at any speciﬁc age. All pre-main-sequence (PMS)
models are calculated with a simple photospheric ap-

proximation, after which an Eddington T-τ atmosphere
is appended on at ZAMS. We call ZAMS the point at
which the nuclear luminosity of the models make up
99.9% of the total luminosity. We calculate atomic dif-
fusion with gravitation settling and without radiative
levitation on the main sequence using ﬁve diﬀusion class
representatives: 1H, 3He, 4He, 16O, and 56Fe (Burgers
1969).1 Following their most recent measurements, we
correct the defaults in MESA of the gravitational con-
stant (G = 6.67408×10−8 g−1 cm3 s−2; Mohr et al. 2015),
the gravitational mass of the Sun (M(cid:12) = 1.988475 × 1033
g = µG−1 = 1.32712440042 × 1011 km s−1 G−1, where
µ is the standard gravitational parameter; Pitjeva 2015),
and the solar radius (R(cid:12) = 6.95568 × 1010 cm; Haberre-
iter et al. 2008).

Each track is evolved from ZAMS to either an age of
τ = 16 Gyr or until terminal-age main sequence (TAMS),
which we deﬁne as having a fractional core hydrogen
abundance (Xc) below 10−3. Evolutionary tracks with
eﬃcient heavy-element settling can develop discontinu-
ities in their surface abundances if they lack suﬃcient
model resolution. We implement adaptive remeshing by
recomputing any track with abundance discontinuities
in its surface layers using ﬁner spatial and temporal res-
olutions (see Appendix C for details). Running stellar
physics codes in a batch mode like this requires care, so
we manually inspect multiple evolutionary diagnostics
to ensure that proper convergence has been achieved.

2.2. Calculation of Seismic Parameters

We use the ADIPLS pulsation package (Christensen-
Dalsgaard 2008) to compute p-mode oscillations up to
spherical degree (cid:96) = 3 below the acoustic cut-oﬀ fre-
quency. We use on average of around 4 000 points per
stellar model and therefore have adequate resolution to
calculate frequencies without remeshing. We denote any
frequency separation S as the diﬀerence between a fre-
quency ν of spherical degree (cid:96) and radial order n and
another frequency, that is:

S((cid:96)1,(cid:96)2)(n1, n2) ≡ ν(cid:96)1(n1) − ν(cid:96)2(n2).

The large frequency separation is then

∆ν(cid:96)(n) ≡ S((cid:96),(cid:96))(n, n − 1)

and the small frequency separation is

δν((cid:96),(cid:96)+2)(n) ≡ S((cid:96),(cid:96)+2)(n, n − 1).

(1)

(2)

(3)

Near-surface layers of stars are poorly-modeled, which
induces systematic frequency oﬀsets (see e.g. Rosenthal

1 The atomic number of each representative isotope is used to
calculate the diﬀusion rate of the other isotopes allocated to that
group; see Paxton et al. (2011).

4

Bellinger & Angelou et al.

Figure 1. Scatterplot matrix (lower panels) and density plots (diagonal) of evolutionary track initial conditions considered. Mass
(M), initial helium (Y0), initial metallicity (Z0), mixing length parameter (αMLT), overshoot (αov), and diﬀusion multiplication
factor (D) were varied in a quasi-random fashion to obtain a low-discrepancy grid of model tracks. Points are colored by their
initial hydrogen X0 = 1−Y0−Z0, with black being low X0 (≈ 56%) and blue being high X0 (≈ 78%). The parameter space is
densely populated with evolutionary tracks of maximally diﬀerent initial conditions.

et al. 1999). The ratios between the large and small
frequency separations (Equation 4), and also between
the large frequency separation and ﬁve-point-averaged
frequencies (Equation 5) have been shown to be less
sensitive to the surface term than the aforementioned
separations and are therefore valuable asteroseismic diag-
nostics of stellar interiors (Roxburgh & Vorontsov 2003).
They are deﬁned as

r((cid:96),(cid:96)+2)(n) ≡

r((cid:96),1−(cid:96))(n) ≡

δν((cid:96),(cid:96)+2)(n)
∆ν(1−(cid:96))(n + (cid:96))

dd((cid:96),1−(cid:96))(n)
∆ν(1−(cid:96))(n + (cid:96))

(4)

(5)

where

dd0,1(n) ≡

ν0(n − 1) − 4ν1(n − 1) + 6ν0(n)

(cid:2)

− 4ν1(n) + ν0(n + 1)

(6)

dd1,0(n) ≡ −

ν1(n − 1) − 4ν0(n) + 6ν1(n)

(cid:3)

1
8

1
8

(cid:2)

− 4ν0(n + 1) + ν1(n + 1)

.

(7)

Since the set of radial orders that are observable diﬀers
from star to star, we collect global statistics on ∆ν0,

(cid:3)

δν0,2, δν1,3, r0,2, r1,3, r0,1, and r1,0. We mimic the range
of observable frequencies in our models by weighting all
frequencies by their position in a Gaussian envelope cen-
tered at the predicted frequency of maximum oscillation
power νmax and having full-width at half-maximum of
0.88 as per the prescription given by Mosser
0.66 · νmax
et al. (2012). We then calculate the weighted median of
each variable, which we denote with angled parentheses
(e.g. (cid:104)r0,2(cid:105)). We choose the median rather than the mean
because it is a robust statistic with a high breakdown
point, meaning that it is much less sensitive to the pres-
ence of outliers (for a discussion of breakdown points,
see Hampel 1971, who attributed them to Gauss). This
approach allows us to predict the fundamental stellar
parameters of any solar-like oscillator with multiple ob-
served modes irrespective of which exact radial orders
have been detected. Illustrations of the methods used to
derive the frequency separations and ratios of a stellar
model are shown in Figure 2.

2.3. Training the Random Forest

We train a random forest regressor on our matrix of
evolutionary models to discover the relations that fa-
cilitate inference of stellar parameters from observed

Stellar Parameters in an Instant with Machine Learning

5

Figure 2. Calculation of seismic parameters for a stellar model. The large and small frequency separations ∆ν0 (top left) and
δν0,2 (top right) and frequency ratios r0,2 (bottom left) and r0,1 (bottom right) are shown as a function of frequency. The vertical
dotted line in these bottom four plots indicates νmax. Points are sized and colored proportionally to the applied weighting, with
large blue symbols indicating high weight and small red symbols indicating low weight.

quantities. A schematic representation of the topology
of our random forest regressor can be seen in Figure 3.
Random forests arise in machine learning through the
family of algorithms known as CART, i.e. Classiﬁcation
and Regression Trees. There are several good textbooks
that discuss random forests (see e.g. Hastie et al. 2005,
Chapter 15). A random forest is an ensemble regres-
sor, meaning that it is composed of many individual
components that each perform statistical regression, and
the forest subsequently averages over the results from
each component (Breiman 2001). The components of
the ensemble are decision trees, each of which learns a
set of decision rules for relating observable quantities to
stellar parameters. An ensemble approach is preferred
because using only a single decision tree that is able to
see all of the training data may result in a regressor that
has memorized the training data and is therefore unable
to generalize to as yet unseen values. This undesirable
phenomenon is known in machine learning as over-ﬁtting,
and is analogous to ﬁtting n data points using a degree
n polynomial: the ﬁt will work perfectly on the data
that was used for ﬁtting, but fail badly on any unseen
data. To avoid this, each decision tree in the forest is
given a random subset of the evolutionary models and a

random subset of the observable quantities from which to
build a set of rules relating observed quantities to stellar
parameters. This process, known as statistical bagging
(Hastie et al. 2005, Section 8.7), prevents the collection
of trees from becoming over-ﬁt to the training data, and
thus results in a regression model that is capable of gen-
eralizing the information it has learned and predicting
values for data on which it has not been trained.

2.3.1. Feature Importance

The CART algorithm uses information theory to de-
cide which rule is the best choice for inferring stellar
parameters like age and mass from the supplied informa-
tion (Hastie et al. 2005, Chapter 9). At every stage, the
rule that creates the largest decrease in mean squared
error (MSE) is crafted. A rule may be, for example, “all
models with L < 0.4 L(cid:12) have M < 1 M(cid:12).” Rules are
created until every stellar model that was supplied to
that particular tree is fully explained by a sequence of
decisions. We moreover use a variant on random forests
known as extremely randomized trees (Geurts et al. 2006),
which further randomize attribute splittings (e.g. split
on L) and the location of the cut-point (e.g. split on 0.4
L/L(cid:12)) used when creating decision rules.

The process of constructing a random forest presents

6

Bellinger & Angelou et al.

Figure 3. A schematic representation of a random forest regressor for inferring fundamental stellar parameters. Observable
quantities such as Teﬀ and [Fe/H] and global asteroseismic observables like (cid:104)∆ν(cid:105) and (cid:104)δν0,2(cid:105) are input on the left side. These
quantities are then fed through to some number of hidden decision trees, which each independently predict parameters like age
and mass. The predictions are then averaged and output on the right side. All inputs and outputs are optional. For example,
surface gravities, luminosities, and radii are not always available from observations (e.g. with the KOI stars, see Section 3.3
below). In their absence, these quantities can be predicted instead of being supplied. In this case, those nodes can be moved
over to the “prediction” side instead of being on the “observations” side. Also, in addition to potentially unobserved inputs like
stellar radii, other interesting model parameters can be predicted as well, such as core hydrogen mass fraction or surface helium
abundance.

an opportunity for not only inferring stellar parameters
from observations, but also for understanding the rela-
tionships that exist in the stellar models. Each decision
tree explicitly ranks the relative “importance” of each ob-
servable quantity for inferring stellar parameters, where
importance is deﬁned in terms of both the reduction in
MSE after deﬁning a decision rule based on that quan-
tity and the number of models that use that rule. In
machine learning, the variables that have been measured
and are supplied as inputs to the algorithm are known
as “features.” Figure 4 shows a feature importance plot,
i.e. distributions of relative importance over all of the
trees in the forest for each feature used to infer stellar
parameters. The features that are used most often to

construct decision rules are metallicity and temperature,
which are each signiﬁcantly more important features
than the rest. The importance of [Fe/H] is due to the
fact that the determinations of quantities like the Z0
and D depend nearly entirely on it (see also Angelou
& Bellinger et al. in prep.). Note that importance does
not indicate indispensability: an appreciable fraction of
decision rules being made based oﬀ of one feature does
not mean that another forest without that feature would
not perform just as well. That being said, these results
indicate that the best area to improve measurements
would be in metallicity determinations, because for stars
being predicted using this random forest, less precise
values here means exploring many more paths and hence

Stellar Parameters in an Instant with Machine Learning

7

which ordinarily perform unconstrained regression and
are therefore not prevented from predicting non-physical
quantities such as negative masses or from violating
conservation requirements.

Secondly, due to the decision rule process that is ex-
plained below, random forests are insensitive to the scale
of the data. Unless care is taken, other regression meth-
ods will artiﬁcially weight some observable quantities
like temperature as being more important than, say, lu-
minosity, solely because temperatures are written using
larger numbers (e.g. 5777 vs. 1, see for example section
11.5.3 of Hastie et al. 2005 for a discussion). Conse-
quently, solutions obtained by other methods will change
if they are run using features that are expressed using
diﬀerent units of measure. For example, other methods
will produce diﬀerent regressors if trained on luminosity
values expressed in solar units verses values expressed
in ergs, whereas random forests will not. Commonly,
this problem is mitigated in other methods by means of
variable standardization and through the use of Maha-
labonis distances (Mahalanobis 1936). However, these
transformations are arbitrary, and handling variables
naturally without rescaling is thus preferred.

Thirdly, random forests take only seconds to train,
which can be a large beneﬁt if diﬀerent stars have dif-
ferent features available. For example, some stars have
luminosity information available whereas others do not,
so a diﬀerent regressor must be trained for each. In the
extreme case, if one wanted to make predictions for stars
using all of their respectively observed frequencies, one
would need to train a new regressor for each star using
the subset of simulated frequencies that correspond to
the ones observed for that star. Ignoring the diﬃculties
of surface-term corrections and mode identiﬁcations, such
an approach would be well-handled by random forest,
suﬀering only a small hit to performance from its rela-
tively small training cost. On the other hand, it would
be infeasible to do this on a star-by-star basis with most
other routines such as deep neural networks, because
those methods can take days or even weeks to train.

And ﬁnally, as we saw in the previous section, random
forests provide the opportunity to extract insight about
the actual regression being performed by examining the
importance of each feature in making predictions.

Figure 4. Relative importance of each observable feature in
inferring fundamental stellar parameters as measured by a
random forest regressor grown from a grid of evolutionary
models. The boxes display the ﬁrst (16%) and third (84%)
quartile of feature importance over all trees, the center line
indicates the median, and the whiskers extend to the most
extreme values.

arriving at less certain predictions.

For many stars, stellar quantities such as radii, lumi-
nosities, surface gravities, and/or oscillation modes with
spherical degree (cid:96) = 3 are not available from observations.
For example, the KOI data set (see Section 3.3 below)
lacks all of this information, and the hare-and-hound
exercise data (see Section 3.1 below) lack all of these
except luminosities. We therefore must train random
forests that predict those quantities instead of using them
as features. We show the relative importance for the
remaining features that were used to train these forests
in Figure 5. When (cid:96) = 3 modes and luminosities are
omitted, eﬀective temperature jumps in importance and
ties with [Fe/H] as the most important feature.

2.3.2. Advantages of CART

2.3.3. Uncertainty

We choose random forests over any of the many other
non-linear regression routines (e.g. Gaussian processes,
symbolic regression, neural networks, support vector
regression, etc.) for several reasons. First, random forests
perform constrained regression; that is, they only make
predictions within the boundaries of the supplied training
data (see e.g. Hastie et al. 2005, Section 9.2.1). This
is in contrast to other methods like neural networks,

There are three separate sources of uncertainty in
predicting stellar parameters. The ﬁrst is the systematic
uncertainty in the physics used to model stars. These
uncertainties are unknown, however, and hence cannot be
propagated. The second is the uncertainty belonging to
the observations of the star. We propagate measurement
uncertainties σ into the predictions by perturbing all
measured quantities n = 10 000 times with normal noise

8

Bellinger & Angelou et al.

Figure 5. Box-and-whisker plots of relative importance for each observable feature in measuring fundamental stellar parameters
for the hare-and-hound exercise data (left), where luminosities are available; and the Kepler objects-of-interest (right), where
they are not. Octupole ((cid:96) = 3) modes have not been measured in any of these stars, so (cid:104)δν1,3(cid:105) and (cid:104)r1,3(cid:105) from evolutionary
modelling are not supplied to these random forests. The boxes are sorted by median importance.

having zero mean and standard deviation σ. We account
for the covariance between asteroseismic separations and
ratios by recalculating them upon each perturbation.

The ﬁnal source is regression uncertainty. Fundamen-
tally, each parameter can only be constrained to the
extent that observations are able to bear information
pertaining to that parameter. Even if observations were
error-free, there still may exist a limit to which infor-
mation gleaned from the surface may tell us about the
physical qualities and evolutionary history of a star. We
quantify those limits via cross-validation: we train the
random forest on only a subset of the simulated evolution-
ary tracks and make predictions on a held-out validation
set. We randomly hold out a diﬀerent subset of the
tracks 25 times to serve as diﬀerent validation sets and
obtain averaged accuracy scores.

We calculate accuracies using several scores. The ﬁrst

is the explained variance score Ve:

Ve = 1 −

Var{y − ˆy}
Var{y}

(8)

where y is the true value we want to predict from the
validation set (e.g. stellar mass), ˆy is the predicted value
from the random forest, and Var is the variance, i.e.
the square of the standard deviation. This score tells

us the extent to which the regressor has reduced the
variance in the parameter it is predicting. The value
ranges from negative inﬁnity, which would be obtained
by a pathologically bad predictor; to one for a perfect
predictor, which occurs if all of the values are predicted
with zero error.

The next score we consider is the residuals of each
prediction, i.e. the absolute diﬀerence between the true
value y and the predicted value ˆy. Naturally, we want
this value to be as low as possible. We also consider
the precision of the regression ˆσ by taking the standard
deviation of predictions across all of the decision trees
in the forest. Finally, we consider these scores together
by calculating the distance of the residuals in units of
precision, i.e. |ˆy − y| /ˆσ.

Figure 6 shows these accuracies as a function of the
number of evolutionary tracks used in the training of
the random forest. Since the residuals and standard
deviations of each parameter are incomparable, we nor-
malize them by dividing by the maximum value. We
also consider the number of trees in the forest and the
number of models per evolutionary track. In this work,
we use 256 trees in each forest, which we have selected
via cross-validation by choosing a number of trees that

Stellar Parameters in an Instant with Machine Learning

9

is greater than the point at which we saw that the ex-
plained variance was no longer increasing greatly; see
Appendix D for an extended discussion.

When supplied with enough stellar models, the ran-
dom forest reduces the variance in each parameter and
is able to make precise inferences. The forest has very
high predictive power for most parameters, and as a
result, essentially all of the uncertainty when predicting
quantities such as stellar radii and luminosities will stem
from observational uncertainty. However, for some model
parameters—most notably the mixing length parameter—
there is still a great deal of variance in the residuals.
Prior to the point where the regressor has been trained
on about 500 evolutionary tracks, the diﬀerences between
the true and predicted mixing lengths actually have a
greater variance than just the true mixing lengths them-
selves. Likewise, the diﬀusion multiplication factor is
diﬃcult to constrain because a star can achieve the same
present-day [Fe/H] by either having a large initial non-
hydrogen abundance and a large diﬀusion multiplication
factor, or by having the same initial [Fe/H] as present
[Fe/H] but with diﬀusion disabled. These diﬃcult-to-
constrain parameters will therefore be predicted with
substantial uncertainties regardless of the precision of
the observations.

3. RESULTS

We perform three tests of our method. We begin
with a hare-and-hound simulation exercise to show that
we can reliably recover parameters. We then move to
the Sun and the solar-like stars 16 Cyg A & B, which
have been the subjects of many investigations; and we
conclude by applying our method to 34 Kepler objects-
In each case, we train our random forest
of-interest.
regressor on the subset of data that is available for the
stars being processed. In the case of the Sun and 16
Cygni, we know very accurately their radii, luminosities,
and surface gravities. For other stars, we will predict
this information instead of supplying it.

3.1. Hare and Hound

We performed a blind hare-and-hound exercise to eval-
uate the performance of our predictor. Author S.B.
prepared twelve models varied in mass, initial chemical
composition, and mixing length parameter with only
some models having overshooting and only some mod-
els having atomic diﬀusion included. The models were
evolved without rotation using the Yale rotating stellar
evolution code (YREC; Demarque et al. 2008), which is
a diﬀerent evolution code than the one that was used
to train the random forest. Eﬀective temperatures, lu-
minosities, [Fe/H] and νmax values as well as (cid:96) = 0, 1, 2
frequencies were obtained from each model. Author
G.C.A. perturbed the “observations” of these models

according to the scheme devised by Reese et al. (2016).
Appendix E lists the true values and the perturbed ob-
servations of the hare-and-hound models. The perturbed
observations and their uncertainties were given to author
E.P.B., who used the described method to recover the
stellar parameters of these models without being given
access to the true values. Relative diﬀerences between
the true and predicted ages, masses, and radii for these
models are plotted against their true values in Figure
7. The method is able to recover the true model values
within uncertainties even when they have been perturbed
by noise. We do not compare the predicted mixing length
parameter, overshooting parameter or diﬀusion multi-
plication factor the interpretation of these parameters
depends on how they have been deﬁned and their precise
implementation.

3.2. The Sun and 16 Cygni

To ensure conﬁdence in our predictions on Kepler data,
we ﬁrst degrade the frequencies of the Sun at solar min-
imum that were obtained by the Birmingham Solar-
Oscillations Network (BiSON; Davies et al. 2014a) to
the level of information that is achievable by the space-
craft. We also degrade the Sun’s uncertainties of other
observations by applying 16 Cyg B’s uncertainties of eﬀec-
tive temperature, luminosity, surface gravity, metallicity,
νmax, radius, and radial velocity. Finally, we perturb
each value with random Gaussian noise according to its
uncertainty to reﬂect the fact that the measured value
of an uncertain observation is not per se the true value.
We use the random forest whose feature importances
were shown in Figure 4 to predict the values of the Sun;
i.e. the random forest trained on eﬀective temperatures,
metallicities, luminosities, surface gravities, radii, and
asteroseismic quantities (cid:104)∆ν0(cid:105), (cid:104)δν0,2(cid:105), (cid:104)δν1,3(cid:105), (cid:104)r0,2(cid:105),
(cid:104)r1,3(cid:105), (cid:104)r0,1(cid:105), and (cid:104)r1,0(cid:105). We show in Figure 8 the densi-
ties for the predicted mass, initial composition, mixing
length parameter, overshoot coeﬃcient, and diﬀusion
multiplication factor needed for ﬁtting an evolutionary
model to degraded data of the Sun as well as the pre-
dicted solar age, core hydrogen abundance, and surface
helium abundance. As discussed in Section 2.3.3, these
densities show the distributions resulting from running
10 000 diﬀerent noise perturbations fed through the ran-
dom forest. Relative uncertainties (cid:15) = 100 · σ/µ are also
indicated, where µ is the mean and σ is the standard de-
viation of the quantity being predicted. Our predictions
are in good agreement with the known values (see also
Table 1 and Table 2).

Several parameters show multimodality due to model
degeneracies. For example, two solutions for the initial
helium are present. This is because it covaries with
the mixing length parameter: the peak of higher Y0
corresponds to the peak of lower αMLT and vice versa.

10

Bellinger & Angelou et al.

Figure 6. Evaluations of regression accuracy. Explained variance (top left), accuracy per precision distance (top right), normalized
absolute error (bottom left), and normalized uncertainty (bottom right) for each stellar parameter as a function of the number of
evolutionary tracks used in training the random forest. These results use 64 models per track and 256 trees in the random forest.

Likewise, high values of surface helium correspond to low
values of the diﬀusion multiplication factor.

Eﬀective temperatures, surface gravities, and metallici-
ties of 16 Cyg A and B were obtained from Ram´ırez et al.
(2009); radii and luminosities from White et al. (2013);
and frequencies from Davies et al. (2015). We obtained
the radial velocity measurements of 16 Cyg A and B
from Nidever et al. (2002) and corrected frequencies for
Doppler shifting as per the prescription in Davies et al.
(2014b). We tried with and without line-of-sight correc-
tions and found that it did not aﬀect the predicted quan-
tities or their uncertainties. We use the same random
forest as we used for the degraded solar data to predict
the parameters of these stars. The initial parameters—
masses, chemical compositions, mixing lengths, diﬀusion
multiplication factors, and overshoot coeﬃcients—for 16
Cygni as predicted by machine learning are shown in
Table 1, and the predicted current parameters—age, sur-
face helium and core hydrogen abundances—are shown
in Table 2. For reference we also show the predicted solar
values from these inputs there as well. These results sup-
port the hypothesis that 16 Cyg A and B were co-natal;
i.e. they formed at the same time with the same initial
composition.

We additionally predict the radii and luminosities of 16
Cyg A and B instead of using them as features. Figure 9
shows our inferred radii, luminosities and surface helium
abundances of 16 Cyg A and B plotted along with the
values determined by interferometry (White et al. 2013)
and an asteroseismic estimate (Verma et al. 2014). Here
again we ﬁnd excellent agreement between our method
and the measured values.

Metcalfe et al. (2015) performed detailed modelling of
16 Cyg A and B using the Asteroseismic Modeling Portal
(AMP), a genetic algorithm for matching individual fre-
quencies of stars to stellar models. They calculated their
results without heavy-element diﬀusion (i.e. with helium-
only diﬀusion) and without overshooting. In order to
account for systematic uncertainties, they multiplied the
spectroscopic uncertainties of 16 Cyg A and B by an
arbitrary constant C = 3. Therefore, in order to make a
fair comparison between the results of our method and
theirs, we generate a new matrix of evolutionary models
with those same conditions and also increase the uncer-
tainties on [Fe/H] by a factor of C. In Figure 10, we
show probability densities of the predicted parameters of
16 Cyg A and B that we obtain using machine learning
in comparison with the results obtained by AMP. We

Stellar Parameters in an Instant with Machine Learning

11

ﬁnd the values and uncertainties agree well. To perform
their analysis, AMP required more than 15 000 hours of
CPU time to model 16 Cyg A and B using the world’s
10th fastest supercomputer, the Texas Advanced Com-
puting Center Stampede (TOP500 2015). Here we have
obtained comparable results in roughly one minute on
a computing cluster with 64 2.5 GHz cores using only
global asteroseismic quantities and no individual frequen-
cies. Although more computationally expensive than our
method, detailed optimization codes like AMP do have
advantages in that they are additionally able to obtain
detailed structural models of stars.

Figure 7. Relative diﬀerences between the predicted and true
values for age (top), mass (middle), and radius (bottom) as a
function of the true values in the hare-and-hound simulation
exercise.

Table 1. Means and standard deviations for predicted initial stellar parameters of the Sun (degraded data) and
16 Cyg A and B.

Name

M/M(cid:12)

Y0

Z0

αMLT

αov

D

Sun

1.00 ± 0.012

0.270 ± 0.0062

0.020 ± 0.0014

1.88 ± 0.078

0.06 ± 0.015

3.7 ± 3.18

16 Cyg A 1.08 ± 0.016

0.262 ± 0.0073

0.022 ± 0.0014

1.86 ± 0.077

0.07 ± 0.028

0.9 ± 0.76

16 Cyg B 1.03 ± 0.015

0.268 ± 0.0065

0.021 ± 0.0015

1.83 ± 0.069

0.11 ± 0.029

1.9 ± 1.57

12

Bellinger & Angelou et al.

Figure 8. Predictions from machine learning of initial (top six) and current (bottom three) stellar parameters for degraded solar
data. Labels are placed at the mean and 3σ levels. Dashed and dot-dashed lines indicate the median and quartiles, respectively.
Relative uncertainties (cid:15) are shown beside each plot. Note that the overshoot parameter applies to all convective boundaries and
is not modiﬁed over the course of evolution, so a non-zero value does not imply a convective core.

Table 2. Means and standard deviations for predicted current-
age stellar parameters of the Sun (degraded data) and 16 Cyg
A and B.

Name

τ /Gyr

Xc

Ysurf

Sun

4.6 ± 0.20

0.34 ± 0.027

0.24 ± 0.017

16 Cyg A 6.9 ± 0.40

0.06 ± 0.024

0.246 ± 0.0085

16 Cyg B 6.8 ± 0.28

0.15 ± 0.023

0.24 ± 0.017

3.3. Kepler Objects of Interest

We obtain observations and frequencies of the KOI tar-
gets from Davies et al. (2016). We use line-of-sight radial
velocity corrections when available, which was only the
case for KIC 6278762 (Latham et al. 2002), KIC 10666592
(Maldonado et al. 2013), and KIC 3632418 (Gontcharov
2006). We use the random forest whose feature impor-
tances were shown in Figure 5 to predict the fundamental
parameters of these stars; that is, the random forest that
is trained on eﬀective temperatures, metallicities, and
asteroseismic quantities (cid:104)∆ν0(cid:105), (cid:104)δν0,2(cid:105), (cid:104)r0,2(cid:105), (cid:104)r0,1(cid:105), and
(cid:104)r1,0(cid:105). The predicted initial conditions—masses, chemi-

cal compositions, mixing lengths, overshoot coeﬃcients,
and diﬀusion multiplication factors—are shown in Table
3; and the predicted current conditions—ages, core hy-
drogen abundances, surface gravities, luminosities, radii,
and surface helium abundances—are shown in Table 4.
Figure 11 shows the fundamental parameters obtained
from our method plotted against those obtained by Silva
Aguirre et al. (2015, hereinafter KAGES). We ﬁnd good
agreement across all stars.

Although still in statistical agreement, the median val-
ues of our predicted ages are systematically lower and the
median values of our predicted masses are systematically

Stellar Parameters in an Instant with Machine Learning

13

Figure 9. Probability densities for predictions of 16 Cyg A (red) and B (blue) from machine learning of radii (top left), luminosities
(top right), and surface helium abundances (bottom). Relative uncertainties (cid:15) are shown beside each plot. Predictions and 2σ
uncertainties from interferometric (“int”) measurements and asteroseismic (“ast”) estimates are shown with arrows.

higher than those predicted by KAGES. We conjecture
that these discrepancies arise from diﬀerences in input
physics. We vary the eﬃciency of diﬀusion, the extent
of convective overshooting, and the value of the mixing
length parameter to arrive at these estimates, whereas
the KAGES models are calculated using ﬁxed amounts of
diﬀusion, without overshoot, and with a solar-calibrated
mixing length. Models with overshooting, for example,
will be more evolved at the same age due to having larger
core masses. Without direct access to their models, how-
ever, the exact reason is diﬃcult to pinpoint.

We ﬁnd a signiﬁcant linear trend in the Kepler objects-
of-interest between the diﬀusion multiplication factor

and stellar mass needed to reproduce observations (P =
0.0001 from a two-sided t-test with N − 2 = 32 degrees
of freedom). Since the values of mass and diﬀusion multi-
plication factor are uncertain, we use Deming regression
to estimate the coeﬃcients of this relation without re-
gression dilution (Deming 1943). We show the diﬀusion
multiplication factors as a function of stellar mass for all
of these stars in Figure 12. We ﬁnd that the diﬀusion
multiplication factor linearly decreases with mass, i.e.

D = (8.6 ± 1.94) − (5.6 ± 1.37) · M/M(cid:12)

(9)

and that this relation explains observations better than
any constant factor (e.g. D=1 or D=0).

Table 3. Means and standard deviations for initial conditions of the KOI data set inferred via machine learning.
The values obtained from degraded solar data predicted on these quantities are shown for reference.

KIC

M/M(cid:12)

Y0

Z0

αMLT

αov

D

3425851

1.15 ± 0.053

0.28 ± 0.020

0.015 ± 0.0028

1.9 ± 0.23

0.06 ± 0.057

0.5 ± 0.92

3544595

0.91 ± 0.032

0.270 ± 0.0090

0.015 ± 0.0028

1.9 ± 0.10

0.2 ± 0.11

4.9 ± 4.38

3632418

1.39 ± 0.057

0.267 ± 0.0089

0.019 ± 0.0032

2.0 ± 0.12

0.2 ± 0.14

1.1 ± 1.01

4141376

1.03 ± 0.036

0.267 ± 0.0097

0.012 ± 0.0025

1.9 ± 0.12

0.1 ± 0.11

4.0 ± 4.09

Table 3 continued

14

Bellinger & Angelou et al.

Figure 10. Probability densities showing predictions from machine learning of fundamental stellar parameters for 16 Cyg A (red)
and B (blue) along with predictions from AMP modelling. Relative uncertainties are shown beside each plot. Predictions and 2σ
uncertainties from AMP modelling are shown with arrows.

Table 3 (continued)

KIC

M/M(cid:12)

Y0

Z0

αMLT

αov

D

4143755

0.99 ± 0.037

0.277 ± 0.0050

0.014 ± 0.0026

1.77 ± 0.033

0.37 ± 0.071

13.4 ± 5.37

4349452

1.22 ± 0.056

0.28 ± 0.012

0.020 ± 0.0043

1.9 ± 0.17

0.10 ± 0.090

7.3 ± 8.82

4914423

1.19 ± 0.048

0.274 ± 0.0097

0.026 ± 0.0046

1.8 ± 0.11

0.08 ± 0.043

2.3 ± 1.6

5094751

1.11 ± 0.038

0.274 ± 0.0082

0.018 ± 0.0030

1.8 ± 0.11

0.07 ± 0.041

2.3 ± 1.39

5866724

1.29 ± 0.065

0.28 ± 0.011

0.027 ± 0.0058

1.8 ± 0.13

0.12 ± 0.086

7.0 ± 8.38

6196457

1.31 ± 0.058

0.276 ± 0.005

0.032 ± 0.0050

1.71 ± 0.050

0.16 ± 0.055

5.7 ± 2.34

6278762

0.76 ± 0.012

0.254 ± 0.0058

0.013 ± 0.0017

2.09 ± 0.069

0.06 ± 0.028

5.3 ± 2.23

6521045

1.19 ± 0.046

0.273 ± 0.0071

0.027 ± 0.0044

1.82 ± 0.074

0.12 ± 0.036

3.2 ± 1.31

7670943

1.30 ± 0.061

0.28 ± 0.017

0.021 ± 0.0045

2.0 ± 0.23

0.06 ± 0.064

1.0 ± 2.55

8077137

1.23 ± 0.070

0.270 ± 0.0093

0.018 ± 0.0028

1.8 ± 0.14

0.2 ± 0.11

2.9 ± 2.08

8292840

1.15 ± 0.079

0.28 ± 0.010

0.016 ± 0.0049

1.8 ± 0.15

0.1 ± 0.12

11. ± 10.7

8349582

1.23 ± 0.040

0.271 ± 0.0069

0.043 ± 0.0074

1.9 ± 0.12

0.11 ± 0.060

2.5 ± 1.11

8478994

0.81 ± 0.022

0.272 ± 0.0082

0.010 ± 0.0012

1.91 ± 0.054

0.21 ± 0.068

17. ± 9.74

8494142

1.42 ± 0.058

0.27 ± 0.010

0.028 ± 0.0046

1.70 ± 0.064

0.10 ± 0.051

1.6 ± 1.65

8554498

1.39 ± 0.067

0.272 ± 0.0082

0.031 ± 0.0032

1.70 ± 0.077

0.14 ± 0.079

1.7 ± 1.17

8684730

1.44 ± 0.030

0.277 ± 0.0075

0.041 ± 0.0049

1.9 ± 0.14

0.29 ± 0.094

15.2 ± 8.81

Table 3 continued

Stellar Parameters in an Instant with Machine Learning

15

Table 3 (continued)

KIC

M/M(cid:12)

Y0

Z0

αMLT

αov

D

8866102

1.26 ± 0.069

0.28 ± 0.013

0.021 ± 0.0048

1.8 ± 0.15

0.08 ± 0.070

5. ± 7.48

9414417

1.36 ± 0.054

0.264 ± 0.0073

0.018 ± 0.0028

1.9 ± 0.13

0.2 ± 0.1

2.2 ± 1.68

9592705

1.45 ± 0.038

0.27 ± 0.010

0.029 ± 0.0038

1.72 ± 0.064

0.12 ± 0.056

0.6 ± 0.47

9955598

0.93 ± 0.028

0.27 ± 0.011

0.023 ± 0.0039

1.9 ± 0.10

0.2 ± 0.13

2.2 ± 1.76

10514430

1.13 ± 0.053

0.277 ± 0.0046

0.021 ± 0.0039

1.78 ± 0.059

0.30 ± 0.097

4.7 ± 1.77

10586004

1.31 ± 0.078

0.274 ± 0.0055

0.038 ± 0.0071

1.8 ± 0.13

0.2 ± 0.13

4.3 ± 3.99

10666592

1.50 ± 0.023

0.30 ± 0.013

0.030 ± 0.0032

1.8 ± 0.11

0.06 ± 0.043

0.2 ± 0.14

10963065

1.09 ± 0.031

0.264 ± 0.0083

0.014 ± 0.0025

1.8 ± 0.11

0.05 ± 0.027

3.1 ± 2.68

11133306

1.11 ± 0.044

0.272 ± 0.0099

0.021 ± 0.0040

1.8 ± 0.16

0.04 ± 0.033

5. ± 5.75

11295426

1.11 ± 0.033

0.27 ± 0.010

0.025 ± 0.0036

1.81 ± 0.084

0.05 ± 0.035

1.3 ± 0.87

11401755

1.15 ± 0.039

0.271 ± 0.0057

0.015 ± 0.0023

1.88 ± 0.055

0.33 ± 0.071

3.8 ± 1.81

11807274

1.32 ± 0.079

0.276 ± 0.0097

0.024 ± 0.0051

1.77 ± 0.083

0.11 ± 0.066

5.4 ± 5.61

11853905

1.22 ± 0.055

0.272 ± 0.0072

0.029 ± 0.0050

1.8 ± 0.12

0.18 ± 0.086

3.3 ± 1.85

11904151

0.93 ± 0.033

0.265 ± 0.0091

0.016 ± 0.0030

1.8 ± 0.13

0.05 ± 0.029

3.1 ± 2.09

Sun

1.00 ± 0.0093

0.266 ± 0.0035

0.018 ± 0.0011

1.81 ± 0.032

0.07 ± 0.021

2.1 ± 0.83

Table 4. Means and standard deviations for current-age conditions of the KOI data set inferred via machine learning.
The values obtained from degraded solar data predicted on these quantities are shown for reference.

KIC

τ /Gyr

Xc

log g

L/L(cid:12)

R/R(cid:12)

Ysurf

3425851

3.7 ± 0.76

0.14 ± 0.081

4.234 ± 0.0098

2.7 ± 0.16

1.36 ± 0.022

0.27 ± 0.026

3544595

6.7 ± 1.47

0.31 ± 0.078

4.46 ± 0.016

0.84 ± 0.068

0.94 ± 0.020

0.23 ± 0.023

3632418

3.0 ± 0.36

0.10 ± 0.039

4.020 ± 0.0076

5.2 ± 0.25

1.91 ± 0.031

0.24 ± 0.021

4141376

3.4 ± 0.67

0.38 ± 0.070

4.41 ± 0.011

1.42 ± 0.097

1.05 ± 0.019

0.24 ± 0.022

4143755

8.0 ± 0.80

0.07 ± 0.022

4.09 ± 0.013

2.3 ± 0.12

1.50 ± 0.029

0.17 ± 0.023

4349452

2.4 ± 0.78

0.4 ± 0.10

4.28 ± 0.012

2.5 ± 0.14

1.32 ± 0.022

0.22 ± 0.043

4914423

5.2 ± 0.58

0.06 ± 0.032

4.162 ± 0.0097

2.5 ± 0.16

1.50 ± 0.022

0.24 ± 0.023

5094751

5.3 ± 0.67

0.07 ± 0.039

4.209 ± 0.0082

2.2 ± 0.13

1.37 ± 0.017

0.23 ± 0.024

5866724

2.4 ± 0.96

0.4 ± 0.12

4.24 ± 0.017

2.7 ± 0.13

1.42 ± 0.022

0.23 ± 0.038

6196457

4.0 ± 0.73

0.18 ± 0.061

4.11 ± 0.022

3.3 ± 0.21

1.68 ± 0.041

0.24 ± 0.016

6278762

10.3 ± 0.96

0.35 ± 0.026

4.557 ± 0.0084

0.34 ± 0.022

0.761 ± 0.0061

0.19 ± 0.023

6521045

5.6 ± 0.370

0.027 ± 0.0097

4.122 ± 0.0055

2.7 ± 0.15

1.57 ± 0.025

0.22 ± 0.019

7670943

2.3 ± 0.59

0.32 ± 0.088

4.234 ± 0.0099

3.3 ± 0.23

1.44 ± 0.025

0.26 ± 0.029

8077137

4.4 ± 0.96

0.08 ± 0.052

4.08 ± 0.016

3.7 ± 0.24

1.68 ± 0.044

0.22 ± 0.031

8292840

3.4 ± 1.48

0.3 ± 0.14

4.25 ± 0.023

2.6 ± 0.20

1.34 ± 0.026

0.19 ± 0.049

8349582

6.7 ± 0.53

0.02 ± 0.012

4.16 ± 0.012

2.2 ± 0.12

1.52 ± 0.016

0.23 ± 0.015

8478994

4.6 ± 1.75

0.50 ± 0.055

4.55 ± 0.012

0.51 ± 0.036

0.79 ± 0.014

0.21 ± 0.022

8494142

2.8 ± 0.52

0.18 ± 0.067

4.06 ± 0.018

4.5 ± 0.32

1.84 ± 0.043

0.24 ± 0.029

8554498

3.7 ± 0.79

0.09 ± 0.060

4.04 ± 0.015

4.1 ± 0.20

1.86 ± 0.043

0.25 ± 0.018

8684730

3.0 ± 0.38

0.24 ± 0.065

4.06 ± 0.046

4.1 ± 0.53

1.9 ± 0.11

0.17 ± 0.040

Table 4 continued

16

Bellinger & Angelou et al.

Table 4 (continued)

KIC

τ /Gyr

Xc

log g

L/L(cid:12)

R/R(cid:12)

Ysurf

8866102

1.9 ± 0.71

0.4 ± 0.11

4.27 ± 0.014

2.8 ± 0.16

1.36 ± 0.024

0.24 ± 0.039

9414417

3.1 ± 0.31

0.09 ± 0.030

4.016 ± 0.0058

5.0 ± 0.32

1.90 ± 0.032

0.21 ± 0.026

9592705

3.0 ± 0.38

0.05 ± 0.026

3.973 ± 0.0087

5.7 ± 0.37

2.06 ± 0.035

0.26 ± 0.015

9955598

7.0 ± 0.98

0.37 ± 0.035

4.494 ± 0.0061

0.66 ± 0.041

0.90 ± 0.013

0.25 ± 0.020

10514430

6.5 ± 0.89

0.06 ± 0.022

4.08 ± 0.014

2.9 ± 0.17

1.62 ± 0.026

0.22 ± 0.021

10586004

4.9 ± 1.39

0.12 ± 0.090

4.09 ± 0.041

3.1 ± 0.27

1.71 ± 0.070

0.24 ± 0.021

10666592

2.0 ± 0.24

0.15 ± 0.036

4.020 ± 0.0066

5.7 ± 0.33

1.98 ± 0.018

0.29 ± 0.014

10963065

4.4 ± 0.58

0.16 ± 0.054

4.292 ± 0.0070

2.0 ± 0.1

1.24 ± 0.015

0.22 ± 0.029

11133306

4.1 ± 0.84

0.22 ± 0.079

4.319 ± 0.0096

1.7 ± 0.11

1.21 ± 0.019

0.22 ± 0.036

11295426

6.2 ± 0.78

0.09 ± 0.036

4.283 ± 0.0059

1.65 ± 0.095

1.26 ± 0.016

0.24 ± 0.012

11401755

5.6 ± 0.630

0.037 ± 0.0053

4.043 ± 0.0071

3.4 ± 0.19

1.69 ± 0.026

0.21 ± 0.026

11807274

2.8 ± 1.05

0.3 ± 0.11

4.17 ± 0.024

3.5 ± 0.22

1.57 ± 0.038

0.22 ± 0.035

11853905

5.7 ± 0.78

0.04 ± 0.020

4.11 ± 0.011

2.7 ± 0.16

1.62 ± 0.030

0.23 ± 0.022

11904151

9.6 ± 1.43

0.08 ± 0.037

4.348 ± 0.0097

1.09 ± 0.06

1.07 ± 0.019

0.21 ± 0.026

Sun

4.6 ± 0.16

0.36 ± 0.012

4.439 ± 0.0038

1.01 ± 0.041

1.000 ± 0.0066

0.245 ± 0.0076

4. DISCUSSION

of input or output parameters.

The amount of time it takes to make predictions for a
star using a trained random forest can be decomposed
into two parts: the amount of time it takes to calculate
perturbations to the observations of the star (see Section
2.3.3), and the amount of time it takes to make a pre-
diction on each perturbed set of observations. Hence we
have

t = n(tp + tr)

(10)

where t is the total time, n is the number of perturba-
tions, tp is the time it takes to perform a single per-
turbation, and tr is the random forest regression time.
We typically see times of tp = (7.9 ± 0.7) · 10−3 (s) and
tr = (1.8 ± 0.4) · 10−5 (s). We chose a conservative
n = 10 000 for the results presented here, which results
in a time of around a minute per star. Since each star
can be processed independently and in parallel, a com-
puting cluster could feasibly process a catalog containing
millions of objects in less than a day. Since tr (cid:28) tp, the
calculation depends almost entirely on the time it takes
to perturb the observations.2 There is also the one-time
cost of training the random forest, which takes less than
a minute and can be reused without retraining on every
star with the same information. It does need to be re-
trained if one wants to consider a diﬀerent combination

2 Our perturbation code uses an interpreted language (R), so if

needed, there is still room for speed-up.

There is a one-time cost of generating the matrix of
training data. We ran our simulation generation scheme
for a week on our computing cluster and obtained 5 325
evolutionary tracks with 64 models per track, which re-
sulted in a 123 MB matrix of stellar models. This is
at least an order of magnitude fewer models than the
amount that other methods use. Furthermore, this is
in general more tracks than is needed by our method:
we showed in Figure 6 that for most parameters—most
notably age, mass, luminosity, radius, initial metallicity,
and core hydrogen abundance—one needs only a fraction
of the models that we generated in order to obtain good
predictive accuracies. Finally, unless one wants to con-
sider a diﬀerent range of parameters or diﬀerent input
physics, this matrix would not need to be calculated
again; a random forest trained on this matrix can be
re-used for all future stars that are observed. Of course,
our method would still work if trained using a diﬀerent
matrix of models, and our grid should work with other
grid-based modelling methods.

Previously, Pulone & Scaramella (1997) developed a
neural network for predicting stellar age based on the
star’s position in the Hertzsprung-Russell diagram. More
recently, Verma et al. (2016) have worked on incorpo-
rating seismic information into that analysis as we have
done here. Our method provides several advantages over
these approaches. Firstly, the random forests that we use
perform constrained regression, meaning that the values
we predict for quantities like age and mass will always be

Stellar Parameters in an Instant with Machine Learning

17

Figure 11. Predicted surface gravities, radii, luminosities, masses, and ages of 34 Kepler objects-of-interest plotted against the
suggested KAGES values. Medians, 16% quantiles, and 84% quantiles are shown for each point. A dashed line of agreement is
shown in all panels to guide the eye.

non-negative and within the bounds of the training data,
which is not true of the neural networks-based approach
that they take. Secondly, using averaged frequency sepa-
rations allows us to make predictions without need for
concern over which radial orders were observed. Thirdly,
we have shown that our random forests are very fast to
train, and can be retrained in only seconds for stars that
are missing observational constraints such as luminosities.
In contrast, deep neural networks are computationally
intensive to train, taking days or even weeks to converge
depending on the breadth of network topologies consid-
ered in the cross-validation. Finally, our grid is varied
in six initial parameters—M, Y0, Z0, αMLT, αov, and
D, which allows our method to explore a wide range of
stellar model parameters.

5. CONCLUSIONS

Here we have considered the constrained multiple-
regression problem of inferring fundamental stellar pa-
rameters from observations. We created a grid of evo-
lutionary tracks varied in mass, chemical composition,
mixing length parameter, overshooting coeﬃcient, and
diﬀusion multiplication factor. We evolved each track in

time along the main sequence and collected observable
quantities such as eﬀective temperatures and metallicities
as well as global statistics on the modes of oscillations
from models along each evolutionary path. We used
this matrix of stellar models to train a machine learning
algorithm to be able to discern the patterns that relate
observations to fundamental stellar parameters. We then
applied this method to hare-and-hound exercise data, the
Sun, 16 Cyg A and B, and 34 planet-hosting candidates
that have been observed by Kepler and rapidly obtained
precise initial conditions and current-age values of these
stars. Remarkably, we were able to empirically deter-
mine the value of the diﬀusion multiplication factor and
hence the eﬃciency of diﬀusion required to reproduce
the observations instead of inhibiting it ad hoc. A larger
sample size will better constrain the diﬀusion multipli-
cation factor and determine what other variables are
relevant in its parametrization. This is work in progress.
The method presented here has many advantages over
existing approaches. First, random forests can be trained
and used in only seconds and hence provide substantial
speed-ups over other methods. Observations of a star

18

Bellinger & Angelou et al.

Figure 12. Logarithmic diﬀusion multiplication factor as a function of stellar mass for 34 Kepler objects-of-interest. The solid
line is the line of best ﬁt from Equation 9 and the dashed lines are the 50% conﬁdence interval around this ﬁt.

simply need to be fed through the forest—akin to plug-
ging numbers into an equation—and do not need to be
subjected to expensive iterative optimization procedures.
Secondly, random forests perform non-linear and non-
parametric regression, which means that the method can
use orders-of-magnitude fewer models for the same level
of precision, while additionally attaining a more rigorous
appraisal of uncertainties for the predicted quantities.
Thirdly, our method allows us to investigate wide ranges
and combinations of stellar parameters. And ﬁnally,
the method presented here provides the opportunity to
extract insights from the statistical regression that is
being performed, which is achieved by examining the
relationships in stellar physics that the machine learns
by analyzing simulation data. This contrasts the blind
optimization processes of other methods that provide
an answer but do not indicate the elements that were
important in doing so.

We note that the predicted quantities reﬂect a set of
choices in stellar physics. Although such biases are im-
possible to propagate, varying model parameters that
are usually kept ﬁxed—such as the mixing length param-
eter, diﬀusion multiplication factor, and overshooting
coeﬃcient—takes us a step in the right direction. Fur-
thermore, the fact that quantities such as stellar radii
and luminosities—quantities that have been measured

accurately, not just precisely—can be reproduced both
precisely and accuractely by this method, gives a degree
of conﬁdence in its eﬃcacy.

The method we have presented here is currently only
applicable to main-sequence stars. We intend to extend
this study to later stages of evolution.

The research leading to the presented results has re-
ceived funding from the European Research Council
under the European Community’s Seventh Framework
Programme (FP7/2007-2013) / ERC grant agreement
no 338251 (StellarAges). This research was undertaken
in the context of the International Max Planck Research
School for Solar System Research. S.B. acknowledges
partial support from NSF grant AST-1514676 and NASA
grant NNX13AE70G. W.H.B. acknowledges research
funding by Deutsche Forschungsgemeinschaft (DFG) un-
der grant SFB 963/1 “Astrophysical ﬂow instabilities
and turbulence” (Project A18).

Software: Analysis in this manuscript was performed
with python 3.5.1 libraries scikit-learn 0.17.1 (Pedregosa
et al. 2011), NumPy 1.10.4 (Van Der Walt et al. 2011),
and pandas 0.17.1 (McKinney et al. 2010) as well as R
3.2.3 (R Core Team 2014) and the R libraries magicaxis
1.9.4 (Robotham 2015), RColorBrewer 1.1-2 (Neuwirth

Stellar Parameters in an Instant with Machine Learning

19

2014), parallelMap 1.3 (Bischl & Lang 2015), data.table
1.9.6 (Dowle et al. 2015), lpSolve 5.6.13 (Berkelaar &
others 2015), ggplot2 2.1.0 (Wickham 2009), GGally
1.0.1 (Schloerke et al. 2014), scales 0.3.0 (Wickham 2015),
deming 1.0-1 (Therneau 2014), and matrixStats 0.50.1
(Bengtsson 2015).

constraints:

ˆS = arg min

SijCij

S

APPENDIX

A. MODEL SELECTION

To prevent statistical bias towards the evolutionary
tracks that generate the most models, i.e. the ones that
require the most careful calculations and therefore use
smaller time-steps, or those that live on the main se-
quence for a longer amount of time; we select n = 64
models from each evolutionary track such that the mod-
els are as evenly-spaced in core hydrogen abundance as
possible. We chose 64 because it is a power of two, which
thus allows us to successively omit every other model
when testing our regression routine and still maintain
regular spacings.

Starting from the original vector of length m of core
hydrogen abundances (cid:126)X, we ﬁnd the subset of length m
that is closest to the optimal spacing (cid:126)B, where

(cid:126)B ≡

XT , . . . ,

(cid:20)

(m − i) · XT + XZ
m − 1

, . . . , XZ

(A1)

(cid:21)

with XZ being the core hydrogen abundance at ZAMS
and XT being that at TAMS. To obtain the closest
possible vector to (cid:126)B from our data (cid:126)X, we solve a trans-
portation problem using integer optimization (Delmotte
2014). First we set up a cost matrix C consisting of
absolute diﬀerences between the original abundances (cid:126)X
and the ideal abundances (cid:126)B:

|B1 − X1|

|B1 − X2|

|B1 − Xn|

|B2 − X1|
...
|Bm − X1|

|B2 − X2|
...
|Bm − X2|

C ≡











. . .

. . .
. . .
. . .

|B2 − Xn|
...
|Bm − Xn|

.











(A2)
We then require that exactly m values are selected from
(cid:126)X, and that each value is selected no more than one time.
Simply selecting the closest data point to each ideally-
separated point will not work because this could result
in the same point being selected twice; and selecting the
second closest point in that situation does not remedy it
because a diﬀerent result could be obtained if the points
were processed in a diﬀerent order.

We denote the optimal solution matrix by ˆS, and ﬁnd
it by minimising the cost matrix subject to the following

subject to

Sij ≤ 1 for all i = 1 . . . N

and

Sij = 1 for all j = 1 . . . M.

(A3)

ij
(cid:88)

j
(cid:88)

i
(cid:88)

(cid:126)X that are most near to being
The indices of
equidistantly-spaced are then found by looking at which
columns of ˆS contain ones. The solution is visualized in
Figure A1.

B. INITIAL GRID STRATEGY

The initial conditions of a stellar model can be viewed
as a six-dimensional hyperrectangle with dimensions M,
Y0, Z0, αMLT, αov, and D. In order to vary all of these
parameters simultaneously and ﬁll the hyperrectangle
as quickly as possible, we construct a grid of initial
conditions following a quasi-random point generation
scheme. This is in contrast to linear or random point
generation schemes, over which it has several advantages.
A linear grid subdivides all dimensions in which ini-
tial quantities can vary into equal parts and creates a
track of models for every combination of these subdi-
visions. Although in the limit such a strategy will ﬁll
the hyperrectangle of initial conditions, it does so very
slowly. It is furthermore suboptimal in the sense that
linear grids maximize redundant information, as each
varied quantity is tried with the exact same values of
all other parameters that have been considered already.
In a high-dimensional setting, if any of the parameters
are irrelevant to the task of the computation, then the
majority of the tracks in a linear grid will not contribute
any new information.

A reﬁnement on this approach is to create a grid of
models with randomly varied initial conditions. Such a
strategy ﬁlls the space more rapidly, and furthermore
solves the problem of redundant information. However,
this approach suﬀers from a diﬀerent problem: since the
points are generated at random, they tend to “clump up”
at random as well. This results in random gaps in the
parameter space, which are obviously undesirable.

Therefore, in order to select points that do not stack,
do not clump, and also ﬁll the space as rapidly as possible,
we generate Sobol numbers (Sobol 1967) in the unit 6-
cube and map them to the parameter ranges of each
quantity that we want to vary. Sobol numbers are a
sequence of m-dimensional vectors x1 . . . xn in the unit
hypercube I m constructed such that the integral of a
real function f in that space is equivalent in the limit to

20

Bellinger & Angelou et al.

Figure A1. A visaulization of the model selection process performed on each evolutionary track in order to obtain the same
number of models from each track. The blue crosses show all of the models along the evolutionary track as they vary from ZAMS
to TAMS in core hydrogen abundance and the red crosses show the models selected from this track. The models were chosen via
linear transport such that they satisfy Equation A3. For reference, an equidistant spacing is shown with black points.

that function evaluated on those numbers, that is,

seen in Figure C3.

f = lim
n→∞

(cid:90)Im

1
n

n

i=1
(cid:88)

f (xi)

(B4)

with the sequence being chosen such that the convergence
is achieved as quickly as possible. By doing this, we both
minimize redundant information and furthermore sample
the hyperspace of possible stars as uniformly as possible.
Figure B2 visualizes the diﬀerent methods of generating
multidimensional grids: linear, random, and the quasi-
random strategy that we took. This method applied
to initial model conditions was shown in Figure 1 with
1- and 2D projection plots of the evolutionary tracks
generated for our grid.

C. ADAPTIVE REMESHING

When performing element diﬀusion calculations in
MESA, the surface abundance of each isotope is con-
sidered as an average over the outermost cells of the
model. The number of outer cells N is chosen such that
the mass of the surface is more than ten times the mass
of the (N + 1)th cell. Occasionally, this approach can
lead to a situation where surface abundances change
dramatically and discontinuously in a single time-step.
These abundance discontinuities then propagate as dis-
continuities in eﬀective temperatures, surface gravities,
and radii. An example of such a diﬃculty is shown in
Figure C3.

Instead of being a physical reality, these eﬀects arise
only when there is insuﬃcient mesh resolution in the
outermost layers of the model. We therefore seek to
detect these cases and re-run any such evolutionary track
using a ﬁner mesh resolution. We consider a track an
outlier if its surface hydrogen abundance changes by more
than 1% in a single time-step. We iteratively re-run any
track with outliers detected using a ﬁner mesh resolution,
and, if necessary, smaller time-steps, until convergence
is reached. The process and a resolved track can also be

Some tracks still do not converge without surface abun-
dance discontinuities despite the ﬁneness of the mesh
or the brevity of the time-steps, and are therefore not
included in our study. These troublesome evolutionary
tracks seem to be located only in a thin ridge of models
having suﬃciently high stellar mass (M > 1), a deﬁcit of
initial metals (Z < 0.001) and a speciﬁc ineﬃciency of
diﬀusion (D (cid:39) 0.01). A visualization of this is shown in
Figure C4.

D. EVALUATING THE REGRESSOR

In training the random forest regressor, we must deter-
mine how many evolutionary tracks N to include, how
many models M to extract from each evolutionary track,
and how many trees T to use when growing the forest.
As such it is useful to deﬁne measures of gauging the
accuracy of the random forest so that we may evaluate
it with diﬀerent combinations of these parameters.

By far the most common way of measuring the quality
of a random forest regressor is its so-called “out-of-bag”
(OOB) score (see e.g. section 3.1 of Breiman 2001). While
each tree is trained on only a subset (or “bag”) of the
stellar models, all trees are tested on all of the models
that they did not see. This provides an accuracy score
representing how well the forest will perform when pre-
dicting on observations that it has not seen yet. We can
then use the scores deﬁned in Section 2.3.3 to calculate
OOB scores.

However, such an approach to scoring is too optimistic
in this scenario. Since a tree can get models from every
simulation, predicting the parameters of a model when
the tree has been trained on one of that model’s neigh-
bors leads to an artiﬁcially inﬂated OOB score. This is
especially the case for quantities like stellar mass, which
do not change along the main sequence. A tree that has
witnessed neighbors on either side of the model being
predicted will have no error when predicting that model’s
mass, and hence the score will seem artiﬁcially better

Stellar Parameters in an Instant with Machine Learning

21

Linear

Random

Quasi-random

Figure B2. Results of diﬀerent methods for generating multidimensional grids portrayed via a unit cube projected onto a unit
square. Linear (left), random (middle), and quasi-random (right) grids are generated in three dimensions, with color depicting
the third dimension, i.e., the distance between the reader and the screen. From top to bottom, all three methods are shown with
100, 400, and 2000 points generated, respectively.

than it should be.

Therefore, we opt instead to build validation sets con-
taining entire tracks that are left out from the training
of the random forest. We omit models and tracks in pow-
ers of two so that we may roughly maintain the regular
spacing that we have established in our grid of models
(refer back to Appendices B and A for details).

We have already shown in Figure 6 these cross-
validated scores as a function of the number of evo-
lutionary tracks. Figure D5 now shows these scores as
a function of the number of models obtained from each
evolutionary track, and Figure D6 shows them as a func-
tion of the number of trees in the forest. Naturally, ˆσ
increases with the number of trees, but this is not a

mark against having more trees: this score is trivially
minimal when there is only one tree, as that tree must
agree with itself! We ﬁnd that although more is better
for all quantities, there is not much improvement after
about T = 32 and M = 16. It is also interesting to note
that the predictions do not suﬀer very much from using
only four models per track, which results in a random
forest trained on only a few thousand models.

E. HARE-AND-HOUND

Table E1 lists the true values of the hare-and-hound
exercise performed here, and Table E2 lists the perturbed
inputs that were supplied to the machine learning algo-
rithm.

Table E1. True values for the hare-and-hound exercise.

Model R/R(cid:12) M/M(cid:12)

τ

Teff

L/L(cid:12) [Fe/H]

Y0

νmax

αov

0

1

2

3

4

1.705

1.388

1.068

1.126

1.497

1.303

1.279

0.951

1.066

1.406

3.725

6297.96

2.608

5861.38

6.587

5876.25

2.242

6453.57

1.202

6506.26

4.11

2.04

1.22

1.98

3.61

0.03

0.26

0.04

0.2520

1313.67 No

0.2577

2020.34 No

0.3057

2534.29 No

-0.36

0.2678

2429.83 No

0.14

0.2629

1808.52 No

D

No

No

No

No

No

Table E1 continued

22

Bellinger & Angelou et al.

Figure C3. Three iterations of surface abundance disconti-
nuity detection and iterative remeshing for an evolutionary
track. The detected discontinuities are encircled in red. The
third iteration has no discontinuities and so this track is
considered to have converged.

Table E1 (continued)

Model R/R(cid:12) M/M(cid:12)

τ

Teff

L/L(cid:12) [Fe/H]

Y0

νmax

αov

5

6

7

8

9

10

11

1.331

0.953

1.137

1.696

0.810

1.399

1.233

1.163

0.983

1.101

1.333

0.769

1.164

1.158

4.979

6081.35

2.757

5721.37

2.205

6378.23

2.792

6382.22

9.705

5919.70

6.263

5916.71

2.176

6228.02

2.18

0.87

1.92

4.29

0.72

2.15

2.05

-0.06

-0.31

-0.07

-0.83

0.00

0.11

0.03

0.2499

1955.72 No

D

No

No

No

No

No

0.2683

3345.56 No

0.2504

2483.83 No

0.2555

1348.83 No

0.2493

3563.09 No

0.2480

1799.10 Yes Yes

0.2796

2247.53 Yes Yes

Table E1 continued

Stellar Parameters in an Instant with Machine Learning

23

Figure C4. Stellar mass as a function of diﬀusion multiplication factor colored by initial surface metallicity (left) and ﬁnal
surface metallicity (right). A ridge of missing points indicating unconverged evolutionary tracks can be seen around a diﬀusion
multiplication factor of 0.01. Beyond this ridge, tracks that were initially metal-poor end their main-sequence lives with all of
their metals drained from their surfaces.

Table E1 (continued)

Model R/R(cid:12) M/M(cid:12)

τ

Teff

L/L(cid:12) [Fe/H]

Y0

νmax

αov

D

Table E2. Supplied (perturbed) inputs for the hare-and-
hound exercise.

Model

Teff

L/L(cid:12)

[Fe/H]

νmax

0

1

2

3

4

5

6

7

8

9

10

11

6237 ± 85

4.2 ± 0.12

-0.03 ± 0.09

1398 ± 66

5806 ± 85

2.1 ± 0.06

0.16 ± 0.09

2030 ± 100

5885 ± 85

1.2 ± 0.04

-0.05 ± 0.09

2630 ± 127

6422 ± 85

2.0 ± 0.06

-0.36 ± 0.09

2480 ± 124

6526 ± 85

3.7 ± 0.11

0.14 ± 0.09

1752 ± 89

6118 ± 85

2.2 ± 0.06

0.04 ± 0.09

1890 ± 101

5741 ± 85

0.8 ± 0.03

0.06 ± 0.09

3490 ± 165

6289 ± 85

2.0 ± 0.06

-0.28 ± 0.09

2440 ± 124

6351 ± 85

4.3 ± 0.13

-0.12 ± 0.09

1294 ± 67

5998 ± 85

0.7 ± 0.02

-0.85 ± 0.09

3290 ± 179

5899 ± 85

2.2 ± 0.06

-0.03 ± 0.09

1930 ± 101

6251 ± 85

2.0 ± 0.06

0.13 ± 0.09

2360 ± 101

Table E2 continued

24

Bellinger & Angelou et al.

Figure D5. Explained variance (top left), accuracy per precision distance (top right), normalized absolute error (bottom left),
and normalized standard deviation of predictions (bottom right) for each stellar parameter as a function of the number of models
per evolutionary track.

Table E2 (continued)

Model

Teff

L/L(cid:12)

[Fe/H]

νmax

Angelou & Bellinger et al. in prep.,
Basu, S., & Antia, H. M. 1994, MNRAS, 269, 1137
Bazot, M., Bourguignon, S., & Christensen-Dalsgaard, J. 2012,

170

REFERENCES

MNRAS, 427, 1847

Bellinger, E. 2016, asteroseismology: Fundamental Parameters of
Main-Sequence Stars in an Instant with Machine Learning,
doi:10.5281/zenodo.55400

Bengtsson, H. 2015, matrixStats: Methods that Apply to Rows
and Columns of Matrices (and to Vectors), r package version
0.14.2

Berkelaar, M., & others. 2015, lpSolve: Interface to lpSolve v. 5.5
to Solve Linear/Integer Programs, r package version 5.6.12
Bischl, B., & Lang, M. 2015, parallelMap: Uniﬁed Interface to

Parallelization Back-Ends, r package version 1.3

Breiman, L. 2001, Machine learning, 45, 5
Brown, T. M., Christensen-Dalsgaard, J., Weibel-Mihalas, B., &

Gilliland, R. L. 1994, ApJ, 427, 1013

Campante, T. L., Barclay, T., Swift, J. J., et al. 2015, ApJ, 799,

Chaplin, W. J., & Miglio, A. 2013, Annual Review of Astronomy

Chaplin, W. J., Kjeldsen, H., Christensen-Dalsgaard, J., et al.

and Astrophysics, 51, 353

2011, Science, 332, 213

Chaplin, W. J., Basu, S., Huber, D., et al. 2014, ApJS, 210, 1
Chiappini, C., Minchev, I., Anders, F., et al. 2015, Astrophysics

and Space Science Proceedings, 39, 111

Christensen-Dalsgaard, J. 2008, Ap&SS, 316, 113
Davies, G. R., Broomhall, A. M., Chaplin, W. J., Elsworth, Y., &

Hale, S. J. 2014a, MNRAS, 439, 2025

Davies, G. R., Handberg, R., Miglio, A., et al. 2014b, MNRAS,

Davies, G. R., Chaplin, W. J., Farr, W. M., et al. 2015, MNRAS,

445, L94

446, 2959

Burgers, J. M. 1969, Flow equations for composite gases, Tech.

Davies, G. R., Aguirre, V. S., Bedding, T. R., et al. 2016,

rep., DTIC Document

MNRAS, 456, 2183

Stellar Parameters in an Instant with Machine Learning

25

Figure D6. Explained variance (top left), accuracy per precision distance (top right), normalized absolute error (bottom left),
and normalized model uncertainty (bottom right) for each stellar parameter as a function of the number of trees used in training
the random forest.

Delmotte, F. 2014, Sample equidistant points from a numeric

Metcalfe, T. S., Creevey, O. L., & Davies, G. R. 2015, ApJL, 811,

vector, StackOverﬂow,
http://stackoverﬂow.com/questions/23145595 (version:
2014-04-18)

Demarque, P., Guenther, D. B., Li, L. H., Mazumdar, A., &

Straka, C. W. 2008, Ap&SS, 316, 31

Deming, W. E. 1943, Statistical adjustment of data. (Wiley)
Dowle, M., Srinivasan, A., Short, T., with contributions from

R Saporta, S. L., & Antonyan, E. 2015, data.table: Extension of
Data.frame, r package version 1.9.6

L37

27

Metcalfe, T. S., Creevey, O. L., Do˘gan, G., et al. 2014, ApJS, 214,

Miglio, A., Chiappini, C., Morel, T., et al. 2013, MNRAS, 429, 423
Mohr, P. J., Newell, D. B., & Taylor, B. N. 2015, ArXiv e-prints,

arXiv:1507.07956

Morel, P., & Th´evenin, F. 2002, A&A, 390, 611
Mosser, B., Elsworth, Y., Hekker, S., et al. 2012, A&A, 537, A30
Neuwirth, E. 2014, RColorBrewer: ColorBrewer Palettes, r

Gai, N., Basu, S., Chaplin, W. J., & Elsworth, Y. 2011, ApJ, 730,

package version 1.1-2

Geurts, P., Ernst, D., & Wehenkel, L. 2006, Machine learning, 63,

Vogt, S. S. 2002, ApJS, 141, 503

Nidever, D. L., Marcy, G. W., Butler, R. P., Fischer, D. A., &

Paxton, B., Bildsten, L., Dotter, A., et al. 2011, The
Astrophysical Journal Supplement Series, 192, 3

Paxton, B., Cantiello, M., Arras, P., et al. 2013, ApJS, 208, 4
Pedregosa, F., Varoquaux, G., Gramfort, A., et al. 2011, Journal

of Machine Learning Research, 12, 2825

Pitjeva, E. 2015, Journal of Physical and Chemical Reference

Gontcharov, G. A. 2006, Astronomy Letters, 32, 759
Grevesse, N., & Sauval, A. J. 1998, SSRv, 85, 161
Haberreiter, M., Schmutz, W., & Kosovichev, A. G. 2008, ApJL,

Hampel, F. R. 1971, The Annals of Mathematical Statistics, 1887
Hastie, T., Tibshirani, R., Friedman, J., & Franklin, J. 2005, The

Data, 44, 031210

Mathematical Intelligencer, 27, 83

Pulone, L., & Scaramella, R. 1997, in Neural Nets WIRN

Latham, D. W., Stefanik, R. P., Torres, G., et al. 2002, AJ, 124,

VIETRI-96 (Springer), 231–236

Mahalanobis, P. C. 1936, Proceedings of the National Institute of

ApJ, 725, 2176

Sciences (Calcutta), 2, 49

R Core Team. 2014, R: A Language and Environment for

Maldonado, J., Villaver, E., & Eiroa, C. 2013, A&A, 554, A84
McKinney, W., et al. 2010, in Proceedings of the 9th Python in

Statistical Computing, R Foundation for Statistical Computing,
Vienna, Austria

Science Conference, Vol. 445, 51–56

Metcalfe, T. S., Creevey, O. L., & Christensen-Dalsgaard, J. 2009,

Ram´ırez, I., Mel´endez, J., & Asplund, M. 2009, A&A, 508, L17
Rauer, H., Catala, C., Aerts, C., et al. 2014, Experimental

ApJ, 699, 373

Astronomy, 38, 249

Quirion, P.-O., Christensen-Dalsgaard, J., & Arentoft, T. 2010,

63

3

675, L53

1144

26

Bellinger & Angelou et al.

Reese, D. R., Chaplin, W. J., Davies, G. R., et al. 2016, Submitted
Ricker, G. R., Winn, J. N., Vanderspek, R., et al. 2015, Journal of
Astronomical Telescopes, Instruments, and Systems, 1, 014003

Robotham, A. 2015, magicaxis: Pretty Scientiﬁc Plotting with

Therneau, T. 2014, deming: Deming, Thiel-Sen and

Passing-Bablock Regression, r package version 1.0-1

TOP500. 2015, TOP500 Supercomputer Site
Van Der Walt, S., Colbert, S. C., & Varoquaux, G. 2011,

Minor-Tick and log Minor-Tick Support, r package version 1.9.4

Computing in Science & Engineering, 13, 22

Rogers, F. J., & Nayfonov, A. 2002, ApJ, 576, 1064
Rosenthal, C. S., Christensen-Dalsgaard, J., Nordlund, ˚A., Stein,

R. F., & Trampedach, R. 1999, A&A, 351, 689

Roxburgh, I. W., & Vorontsov, S. V. 2003, A&A, 411, 215
Schloerke, B., Crowley, J., Cook, D., et al. 2014, GGally:

Verma, K., Hanasoge, S., Bhattacharya, J., Antia, H., &

Krishnamurthy, G. 2016, ArXiv e-prints, arXiv:1602.00902
Verma, K., Faria, J. P., Antia, H. M., et al. 2014, ApJ, 790, 138
White, T. R., Huber, D., Maestro, V., et al. 2013, MNRAS, 433,

1262

Extension to ggplot2., r package version 0.5.0

Wickham, H. 2009, ggplot2: elegant graphics for data analysis

Silva Aguirre, V., Davies, G. R., Basu, S., et al. 2015, MNRAS,

(Springer New York)

—. 2015, scales: Scale Functions for Visualization, r package

Sobol, I. M. 1967, USSR Computational mathematics and

version 0.3.0

452, 2127

mathematical physics, 86

