Deriving Neural Architectures from Sequence and Graph Kernels

7
1
0
2
 
t
c
O
 
0
3
 
 
]
E
N
.
s
c
[
 
 
3
v
7
3
0
9
0
.
5
0
7
1
:
v
i
X
r
a

Tao Lei* 1 Wengong Jin* 1 Regina Barzilay 1 Tommi Jaakkola 1

Abstract

The design of neural architectures for structured
objects is typically guided by experimental in-
sights rather than a formal process. In this work,
we appeal to kernels over combinatorial struc-
tures, such as sequences and graphs, to derive
appropriate neural operations. We introduce a
class of deep recurrent neural operations and for-
mally characterize their associated kernel spaces.
Our recurrent modules compare the input to vir-
tual reference objects (cf. ﬁlters in CNN) via the
kernels. Similar to traditional neural operations,
these reference objects are parameterized and di-
rectly optimized in end-to-end training. We em-
pirically evaluate the proposed class of neural ar-
chitectures on standard applications such as lan-
guage modeling and molecular graph regression,
achieving state-of-the-art results across these ap-
plications.

1. Introduction

Many recent studies focus on designing novel neural ar-
chitectures for structured data such as sequences or anno-
tated graphs. For instance, LSTM (Hochreiter & Schmid-
huber, 1997), GRU (Chung et al., 2014) and other complex
recurrent units (Zoph & Le, 2016) can be easily adapted
to embed structured objects such as sentences (Tai et al.,
2015) or molecules (Li et al., 2015; Dai et al., 2016) into
vector spaces suitable for later processing by standard pre-
dictive methods. The embedding algorithms are typically
integrated into an end-to-end trainable architecture so as to
tailor the learnable embeddings directly to the task at hand.

The embedding process itself is characterized by a se-
quence operations summarized in a structure known as
the computational graph. Each node in the computational

*Equal contribution

Intelligence Laboratory.

tiﬁcial
Tao Lei <taolei@csail.mit.edu>, Wengong
gong@csail.mit.edu>.

1MIT Computer Science & Ar-
to:
Jin <wen-

Correspondence

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

graph identiﬁes the unit/mapping applied while the arcs
specify the relative arrangement/order of operations. The
process of designing such computational graphs or asso-
ciated operations for classes of objects is often guided by
insights and expertise rather than a formal process.

Recent work has substantially narrowed the gap between
desirable computational operations associated with objects
and how their representations are acquired. For example,
value iteration calculations can be folded into convolutional
architectures so as to optimize the representations to fa-
cilitate planning (Tamar et al., 2016). Similarly, inference
calculations in graphical models about latent states of vari-
ables such as atom characteristics can be directly associated
with embedding operations (Dai et al., 2016).

We appeal to kernels over combinatorial structures to de-
ﬁne the appropriate computational operations. Kernels give
rise to well-deﬁned function spaces and possess rules of
composition that guide how they can be built from simpler
ones. The comparison of objects inherent in kernels is of-
ten broken down to elementary relations such as counting
of common sub-structures as in

K(χ, χ(cid:48)) =

1[s ∈ χ]1[s ∈ χ(cid:48)]

(1)

(cid:88)

s∈S

where S is the set of possible substructures. For exam-
ple, in a string kernel (Lodhi et al., 2002), S may refer
to all possible subsequences while a graph kernel (Vish-
wanathan et al., 2010) would deal with possible paths
in the graph. Several studies have highlighted the rela-
tion between feed-forward neural architectures and ker-
nels (Hazan & Jaakkola, 2015; Zhang et al., 2016) but we
are unaware of any prior work pertaining to kernels associ-
ated with neural architectures for structured objects.

In this paper, we introduce a class of deep recurrent neural
embedding operations and formally characterize their asso-
ciated kernel spaces. The resulting kernels are parameter-
ized in the sense that the neural operations relate objects of
interest to virtual reference objects through kernels. These
reference objects are parameterized and readily optimized
for end-to-end performance.

To summarize, the proposed neural architectures, or Kernel
Neural Networks 1 , enjoy the following advantages:

1Code available at https://github.com/taolei87/icml17 knn

Deriving Neural Architectures from Sequence and Graph Kernels

• The architecture design is grounded in kernel compu-

• Our neural models remain end-to-end trainable to the

tations.

task at hand.

• Resulting architectures demonstrate state-of-the-art

performance against strong baselines.

In the following sections, we will introduce these neural
components derived from string and graph kernels, as well
as their deep versions. Due to space limitations, we defer
proofs to supplementary material.

Figure 1. An unrolled view of the derived recurrent module for
K2(). Horizontal lines denote decayed propagation from c[t − 1]
to c[t], while vertical lines represent a linear mapping Wxt that
is propagated to the internal states c[t].

2. From String Kernels to Sequence NNs

λx,i,j λy,k,l (cid:104)xi ⊗ xj, yk ⊗ yl(cid:105)

Notations We deﬁne a sequence (or a string) of tokens
i=1 where xi ∈ Rd repre-
(e.g. a sentence) as x1:L = {xi}L
sents its ith element and |x| = L denotes the length. When-
ever it is clear from the context, we will omit the subscript
and directly use x (and y) to denote a sequence. For a pair
of vectors (or matrices) u, v, we denote (cid:104)u, v(cid:105) = (cid:80)
k ukvk
as their inner product. For a kernel function Ki(·, ·) with
subscript i, we use φi(·) to denote its underlying mapping,
i.e. Ki(x, y) = (cid:104)φi(x), φi(y)(cid:105) = φi(x)(cid:62)φi(y).

String Kernel String kernel measures the similarity be-
tween two sequences by counting shared subsequences
(see Lodhi et al. (2002)). For example, let x and y be two
strings, a bi-gram string kernel K2(x, y) counts the num-
ber of bi-grams (xi, xj) and (yk, yl) such that (xi, xj) =
(yk, yl)2,

K2(x, y) =

λx,i,j λy,k,l δ(xi, yk) · δ(xj, yl)

(cid:88)

1≤i<j≤|x|
1≤k<l≤|y|

(2)
where λx,i,j, λy,k,l ∈ [0, 1) are context-dependent weights
and δ(x, y) is an indicator that returns 1 only when x = y.
The weight factors can be realized in various ways. For
instance, in temporal predictions such as language model-
ing, substrings (i.e. patterns) which appear later may have
higher impact for prediction. Thus a realization λx,i,j =
λ|x|−i−1 and λy,k,l = λ|y|−k−1 (penalizing substrings far
from the end) can be used to determine weights given a
constant decay factor λ ∈ (0, 1).

In our case, each token in the sequence is a vector (such as
one-hot encoding of a word or a feature vector). We shall
replace the exact match δ(u, v) by the inner product (cid:104)u, v(cid:105).
To this end, the kernel function (2) can be rewritten as,

(cid:88)

(cid:88)

1≤i<j≤|x|

1≤k<l≤|y|

λx,i,j λy,k,l (cid:104)xi, yk(cid:105) · (cid:104)xj, yl(cid:105)

2We deﬁne n-gram as a subsequence of original string (not

necessarily consecutive).

(cid:88)

(cid:88)

1≤i<j≤|x|
(cid:42)

1≤k<l≤|y|

=

=

(cid:88)

i<j

λ|x|−i−1 xi ⊗ xj,

λ|y|−k−1 yk ⊗ yl

(cid:88)

k<l

(cid:43)

(3)

where xi ⊗ xj ∈ Rd×d (and similarly yk ⊗ yl) is the outer-
product. In other words, the underlying mapping of kernel
K2() deﬁned above is φ2(x) = (cid:80)
1≤i<j≤|x| λ|x|−i−1xi ⊗
xj. Note we could alternatively use a partial additive scor-
ing (cid:104)xi, yk(cid:105) + (cid:104)xj, yl(cid:105), and the kernel function can be gen-
eralized to n-grams when n (cid:54)= 2. Again, we commit to one
realization in this section.

String Kernel NNs We introduce a class of recurrent mod-
ules whose internal feature states embed the computation of
string kernels. The modules project kernel mapping φ(x)
into multi-dimensional vector space (i.e. internal states of
recurrent nets). Owing to the combinatorial structure of
φ(x), such projection can be realized and factorized via
efﬁcient computation. For the example kernel discussed
above, the corresponding neural component is realized as,

c1[t] = λ · c1[t − 1] +

cj[t] = λ · cj[t − 1] +

cj−1[t − 1] (cid:12) W(j)xt

(cid:17)

(cid:17)

W(1)xt

(cid:16)

(cid:16)

h[t] = σ(cn[t]),

1 < j ≤ n

(4)

where cj[t] are the pre-activation cell states at word xt, and
h[t] is the (post-activation) hidden vector. cj[0] is initial-
ized with a zero vector. W(1), .., W(n) are weight matrices
to be learned from training examples.

The network operates like other RNNs by processing each
input token and updating the internal states. The element-
wise multiplication (cid:12) can be replaced by addition + (cor-
responding to the partial additive scoring above). As a spe-
cial case, the additive variant becomes a word-level convo-
lutional neural net (Kim, 2014) when λ = 0.3

3h[t] = σ(W(1)xt−n+1 + · · · + W(n)xt) when λ = 0.

Deriving Neural Architectures from Sequence and Graph Kernels

2.1. Single Layer as Kernel Computation

Now we state how the proposed class embeds string kernel
computation. For j ∈ {1, .., n}, let cj[t][i] be the i-th entry
of state vector cj[t], w(j)
represents the i-th row of ma-
, ..., w(j)
trix W(j). Deﬁne wi,j = {w(1)
i } as a “ref-
i
erence sequence” constructed by taking the i-th row from
each matrix W(1), .., W(j).

, w(2)
i

i

Theorem 1. Let x1:t be the preﬁx of x consisting of ﬁrst
t tokens, and Kj be the string kernel of j-gram shown in
Eq.(3). Then cj[t][i] evaluates kernel function,

cj[t][i] = Kj (x1:t, wi,j) = (cid:104)φj(x1:t), φj(wi,j)(cid:105)

for any j ∈ {1, .., n}, t ∈ {1, .., |x|}.

In other words, the network embeds sequence similarity
computation by assessing the similarity between the input
sequence x1:t and the reference sequence wi,j. This in-
terpretation is similar to that of CNNs, where each ﬁlter
is a “reference pattern” to search in the input. String ker-
nel NN further takes non-consecutive n-gram patterns into
consideration (seen from the summation over all n-grams
in Eq.(3)).

Applying Non-linear Activation In practice, a non-linear
activation function such as polynomial or sigmoid-like ac-
tivation is added to the internal states to produce the ﬁ-
nal output state h[t].
It turns out that many activations
are also functions in the reproducing kernel Hilbert space
(RKHS) of certain kernel functions (see Shalev-Shwartz
et al. (2011); Zhang et al. (2016)). When this is true, the
underlying kernel of h[t] is the composition of string ker-
nel and the kernel containing the activation. We give the
formal statements below.

Lemma 1. Let x and w be multi-dimensional vectors with
ﬁnite norm. Consider the function f (x) := σ(w(cid:62)x) with
non-linear activation σ(·). For functions such as polyno-
mials and sigmoid function, there exists kernel functions
Kσ(·, ·) and the underlying mapping φσ(·) such that f (x)
is in the reproducing kernel Hilbert space of Kσ(·, ·), i.e.,

f (x) = σ(w(cid:62)x) = (cid:104)φσ(x), ψ(w)(cid:105)

for some mapping ψ(w) constructed from w. In particular,
2−(cid:104)x,y(cid:105) for
Kσ(x, y) can be the inverse-polynomial kernel
the above activations.

1

Proposition 1. For one layer string kernel NN with non-
linear activation σ(·) discussed in Lemma 1, h[t][i] as a
function of input x belongs to the RKHS introduced by the
composition of Kσ(·, ·) and string kernel Kn(·, ·). Here a
kernel composition Kσ,n(x, y) is deﬁned with the under-
lying mapping x (cid:55)→ φσ(φn(x)), and hence Kσ,n(x, y) =
φσ(φn(x))(cid:62)φσ(φn(y)).

Proposition 1 is the corollary of Lemma 1 and Theo-
rem 1, since h[t][i] = σ(cn[t][i]) = σ(Kn(x1:t, wi,j)) =
(cid:104)φσ(φn(x1:t)), ˜wi,j(cid:105) and φσ(φn(·)) is the mapping for the
composed kernel. The same proof applies when h[t] is a
linear combination of all ci[t] since kernel functions are
closed under addition.

2.2. Deep Networks as Deep Kernel Construction

We now address the case when multiple layers of the same
module are stacked to construct deeper networks. That is,
the output states h(l)[t] of the l-th layer are fed to the (l+1)-
th layer as the input sequence. We show that layer stacking
corresponds to recursive kernel construction (i.e. (l + 1)-
th kernel is deﬁned on top of l-th kernel), which has been
proven for feed-forward networks (Zhang et al., 2016).

We ﬁrst generalize the sequence kernel deﬁnition to enable
recursive construction. Notice that the deﬁnition in Eq.(3)
uses the linear kernel (inner product) (cid:104)xi, yk(cid:105) as a “subrou-
tine” to measure the similarity between substructures (e.g.
tokens) within the sequences. We can therefore replace it
with other similarity measures introduced by other “base
In particular, let K(1)(·, ·) be the string kernel
kernels”.
(associated with a single layer). The generalized sequence
kernel K(l+1)(x, y) can be recursively deﬁned as,

λx,i,j λy,k,l K(l)

σ (x1:i, y1:k) K(l)

σ (x1:j, y1:l) =

σ (x1:i) ⊗ φ(l)
φ(l)

σ (x1:j),

σ (y1:k) ⊗ φ(l)
φ(l)

σ (y1:l)

(cid:88)

k<l

(cid:43)

where φ(l)(·) denotes the pre-activation mapping of the l-
th kernel, φ(l)
σ (·) = φσ(φ(l)(·)) denotes the underlying
(post-activation) mapping for non-linear activation σ(·),
and K(l)
σ (·, ·) is the l-th post-activation kernel. Based on
this deﬁnition, a deeper model can also be interpreted as a
kernel computation.

Theorem 2. Consider a deep string kernel NN with L
layers and activation function σ(·). Let the ﬁnal output
state h(L)[t] = σ(c(L)
n [t]) (or any linear combination of
{c(l)
i

[t]}, i = 1, .., n). For l = 1, · · · , L,

(i) c(l)

n [t][i] as a function of input x belongs to the RKHS
of kernel K(l)(·, ·);

(ii) h(l)[t][i] belongs to the RKHS of kernel K(l)

σ (·, ·).

(cid:88)

i<j
k<l
(cid:42)

(cid:88)

i<j

3. From Graph Kernels to Graph NNs

In the previous section, we encode sequence kernel com-
putation into neural modules and demonstrate possible ex-
tensions using different base kernels. The same ideas apply

Deriving Neural Architectures from Sequence and Graph Kernels

to other types of kernels and data. Speciﬁcally, we derive
neural components for graphs in this section.

Notations A graph is deﬁned as G = (V, E), with each
vertex v ∈ V associated with feature vector fv. The neigh-
bor of node v is denoted as N (v). Following previous
notations, for any kernel function K∗(·, ·) with underly-
ing mapping φ∗(·), we use K∗,σ(·, ·) to denote the post-
activation kernel induced from the composed underlying
mapping φ∗,σ = φσ(φ∗(·)).

3.1. Random Walk Kernel NNs

We start from random walk graph kernels (G¨artner et al.,
2003), which count common walks in two graphs. For-
mally, let Pn(G) be the set of walks x = x1 · · · xn, where
∀i : (xi, xi+1) ∈ E.4 Given two graphs G and G(cid:48), an n-th
order random walk graph kernel is deﬁned as:

W (G, G(cid:48)) = λn−1 (cid:88)
Kn

(cid:88)

n
(cid:89)

(cid:104)fxi , fyi (cid:105)

(5)

x∈Pn(G)

y∈Pn(G(cid:48))

i=1

where fxi ∈ Rd is the feature vector of node xi in the walk.
Now we show how to realize the above graph kernel with
a neural module. Given a graph G, the proposed neural
module is:

c1[v] = W(1)fv
(cid:88)
cj[v] = λ

cj−1[u] (cid:12) W(j)fv

(6)

hG = σ(

cn[v])

1 < j ≤ n

u∈N (v)
(cid:88)

v

where again c∗[v] is the cell state vector of node v, and hG
is the representation of graph G aggregated from node vec-
tors. hG could then be used for classiﬁcation or regression.

Now we show the proposed model embeds the random
walk kernel. To show this, construct Ln,k as a “refer-
ence walk” consisting of the row vectors {w(1)
k , · · · , w(n)
k }
from the parameter matrices. Here Ln,k = (LV , LE),
where LV = {v0, v1, · · · , vn}, LE = {(vi, vi+1)} and vi’s
feature vector is w(i)

k . We have the following theorem:

Theorem 3. For any n ≥ 1, the state value cn[v][k] (the
k-th coordinate of cn[v]) satisﬁes:

(cid:88)

v

cn[v][k] = Kn

W (G, Ln,k)

thus (cid:80)
lary, hG lies in the RKHS of kernel K n

v cn[v] lies in the RKHS of kernel Kn
W,σ().

W . As a corol-

4A single node could appear multiple times in a walk.

3.2. Uniﬁed View of Graph Kernels

The derivation of the above neural module could be ex-
tended to other classes of graph kernels, such as subtree
kernels (cf. (Ramon & G¨artner, 2003; Vishwanathan et al.,
2010)). Generally speaking, most of these kernel functions
factorize graphs into local sub-structures, i.e.

K(G, G(cid:48)) =

Kloc(v, v(cid:48))

(7)

(cid:88)

(cid:88)

v

v(cid:48)

where Kloc(v, v(cid:48)) measures the similarity between local
sub-structures centered at node v and v(cid:48).

For example, the random walk kernel K n
lently deﬁned with Kn

loc(v, v(cid:48)) =

W can be equiva-






(cid:104)fv, fv(cid:48)(cid:105)
(cid:104)fv, fv(cid:48)(cid:105) · λ (cid:80)

(cid:80)
u(cid:48)∈N (v(cid:48))

Kn−1

loc (u, u(cid:48))

u∈N (v)

if n = 1

if n > 1

Other kernels like subtree kernels could be recursively de-
ﬁned similarly. Therefore, we adopt this uniﬁed view of
graph kernels for the rest of this paper.

In addition, this deﬁnition of random walk kernel could be
further generalized and enhanced by aggregating neighbor
features non-linearly:

Kn

loc(v, v(cid:48)) = (cid:104)fv, fv(cid:48)(cid:105) ◦ λ

(cid:88)

(cid:88)

Kn−1

loc,σ(u, u(cid:48))

u∈N (v)

u(cid:48)∈N (v(cid:48))

where ◦ could be either multiplication or addition. σ(·)
denotes a non-linear activation and Kn−1
loc,σ(·, ·) denotes the
post-activation kernel when σ(·) is involved. The general-
ized kernel could be realized by modifying Eq.(6) into:

cj[v] = W(j)fv ◦ λ

σ(cj−1[u])

(8)

(cid:88)

u∈N (v)

where ◦ could be either + or (cid:12) operation.

3.3. Deep Graph Kernels and NNs

Following Section 2, we could stack multiple graph kernel
NNs to form a deep network. That is:

c(l)
1 [v] = W(l,1)h(l−1)[v]
c(l)
j [v] = W(l,j)h(l−1)[v] ◦ λ

(cid:88)

(cid:16)

(cid:17)
c(l)
j−1[u]

σ

u∈N (v)

h(l)[v] = σ(U(l)c(l)

n [v])

1 ≤ l ≤ L, 1 < j ≤ n

function is recursively deﬁned in
The local kernel
depth (term h(l)) and width (term
two dimensions:
cj). Let the pre-activation kernel in the l-th layer be
K(l)
loc (v, v(cid:48)), and the post-activation ker-
nel be K(l)
loc,σ(v, v(cid:48)). We recursively deﬁne

loc,σ(v, v(cid:48)) = K(l,n)

loc(v, v(cid:48)) = K(l,n)

Deriving Neural Architectures from Sequence and Graph Kernels

K(l,j)



loc (v, v(cid:48)) =
K(l−1)
loc,σ (v, v(cid:48))
loc,σ (v, v(cid:48)) ◦ λ (cid:80)
K(l−1)



(cid:80)
u(cid:48)∈N (v(cid:48))

K(l,j−1)

loc,σ (u, u(cid:48))

u∈N (v)

if j = 1

if j > 1

loc

Finally,

v,v(cid:48) K(L,n)

is
the graph kernel
(v, v(cid:48)). Similar to Theo-

for j = 1, · · · , n.
K(L,n)(G, G(cid:48)) = (cid:80)
rem 2, we have
Theorem 4. Consider a deep graph kernel NN with L lay-
ers and activation function σ(·). Let the ﬁnal output state
hG = (cid:80)
v h(L)[v]. For l = 1, · · · , L; j = 1, · · · , n:
(i) c(l)

j [v][i] as a function of input v and graph G belongs
to the RKHS of kernel K(l,j)
loc (·, ·);

(ii) h(l)[v][i] belongs to the RKHS of kernel K(l,n)
(iii) hG[i] belongs to the RKHS of kernel K(L,n)(·, ·).

loc,σ(·, ·).

3.4. Connection to Weisfeiler-Lehman Kernel

We derived the above deep kernel NN for the purpose of
generality. This model could be simpliﬁed by setting n =
2, without losing representational power (as non-linearity
is already involved in depth dimension). In this case, we
rewrite the network by reparametrization:

h(l)

v = σ

1 h(l−1)
v

◦ U(l)
2

(cid:88)

(cid:16)

σ

V(l)h(l−1)
u


U(l)

(cid:17)





u∈N (v)

(9)
In this section, we further show that this model could be en-
hanced by sharing weight matrices U and V across layers.
This parameter tying mechanism allows our model to em-
bed Weisfeiler-Lehman kernel (Shervashidze et al., 2011).
For clarity, we brieﬂy review basic concepts of Weisfeiler-
Lehman kernel below.

Weisfeiler-Lehman Graph Relabeling Weisfeiler-
Lehman kernel borrows concepts from the Weisfeiler-
Lehman isomorphism test for labeled graphs. The key idea
of the algorithm is to augment the node labels by the sorted
set of node labels of neighbor nodes, and compress these
augmented labels into new, short labels (Figure 2). Such
relabeling process is repeated T times. In the i-th iteration,
it generates a new labeling li(v) for all nodes v in graph G,
with initial labeling l0.

Generalized Graph Relabeling The key observation here
is that graph relabeling operation could be viewed as neigh-
bor feature aggregation. As a result, the relabeling process
naturally generalizes to the case where nodes are associated
with continuous feature vectors. In particular, let r be the
relabeling function. For a node v ∈ G:
(cid:88)

r(v) = σ(U1fv + U2

σ(Vfu))

(10)

u∈N (v)

Figure 2. Node relabeling in Weisfeiler-Lehman isomorphism
test. Figure taken from Shervashidze et al. (2011)

Note that our deﬁnition of r(v) is exactly the same as hv in
Equation 9, with ◦ being additive composition.

Weisfeiler-Lehman Kernel Let K be any graph ker-
nel (called base kernel). Given a relabeling function r,
Weisfeiler-Lehman kernel with base kernel K and depth L
is deﬁned as

L
(cid:88)

K(L)

W L(G, G(cid:48)) =

K(ri(G), ri(G(cid:48)))

(11)

i=0
where r0(G) = G and ri(G), ri(G(cid:48)) are the i-th relabeled
graph of G and G(cid:48) respectively.

Weisfeiler-Lehman Kernel NN Now with the above ker-
nel deﬁnition, and random walk kernel as the base kernel,
we propose the following recurrent module:

c(l)
0 [v] = W(l,0)h(l−1)
v
c(l)
c(l)
j−1[u] (cid:12) W(l,j)h(l−1)
j [v] = λ

(cid:88)

v

u∈N (v)

U1h(l−1)
v

h(l)

v = σ

h(l)

G =

(cid:88)

v

c(l)
n [v]



(cid:88)

+ U2

σ(Vh(l−1)
u

)


u∈N (v)

1 ≤ l ≤ L, 1 < j ≤ n

where h(0)
The ﬁnal output of this network is hG = (cid:80)L

v = fv and U1, U2, V are shared across layers.

l=1 h(l)
G .

The above recurrent module is still an instance of deep ker-
nel, even though some parameters are shared. A minor dif-
ference here is that there is an additional random walk ker-
nel NN that connects i-th layer and the output layer. But
this is just a linear combination of L deep random walk
kernels (of different depth). Therefore, as an corollary of
Theorem 4, we have:
Proposition 2. For a Weisfeiler-Lehman Kernel NN with L
iterations and random walk kernel Kn
W as base kernel, the
ﬁnal output state hG = (cid:80)
G belongs to the RKHS of
kernel K(L)
W L(·, ·).

l h(l)

Deriving Neural Architectures from Sequence and Graph Kernels

4. Adaptive Decay with Neural Gates

The sequence and graph kernel (and their neural compo-
nents) discussed so far use a constant decay value λ re-
gardless of the current input. However, this is often not the
case since the importance of the input can vary across the
context or the applications. One extension is to make use of
neural gates that adaptively control the decay factor. Here
we give two illustrative examples:

Gated String Kernel NN By replacing constant decay λ
with a sigmoid gate, we modify our single-layer sequence
module as:

λt = σ(U[xt, ht−1] + b)
(cid:16)

c1[t] = λt (cid:12) c1[t − 1] +

(cid:17)

W(1)xt

cj[t] = λt (cid:12) cj[t − 1] +

cj−1[t − 1] (cid:12) W(j)xt

(cid:16)

(cid:17)

h[t] = σ(cn[t])

1 < j ≤ n

As compared with the original string kernel, now the decay
factor λx,i,j is no longer λ|x|−i−1, but rather an adaptive
value based on current context.

Gated Random Walk Kernel NN Similarly, we could in-
troduce gates so that different walks have different weights:

λu,v = σ(U[fu, fv] + b)
c0[v] = W(0)fv
cj[v] =

(cid:88)

λu,v (cid:12) cj−1[u] (cid:12) W(j)fv

hG = σ(

cn[v])

1 < j ≤ n

u∈N (v)
(cid:88)

v

The underlying kernel of the above gated network becomes

Kn

W (G, G(cid:48)) =

(cid:88)

(cid:88)

n
(cid:89)

x∈Pn(G)

y∈Pn(G(cid:48))

i=1

λxi,yi (cid:104)fxi, fyi(cid:105)

where each path is weighted by different decay weights,
determined by network itself.

5. Related Work

Sequence Networks Considerable effort has gone into de-
signing effective networks for sequence processing. This
includes recurrent modules with the ability to carry per-
sistent memories such as LSTM (Hochreiter & Schmid-
huber, 1997) and GRU (Chung et al., 2014), as well as
non-consecutive convolutional modules (RCNNs, Lei et al.
(2015)), and others. More recently, Zoph & Le (2016) ex-
empliﬁed a reinforcement learning-based search algorithm
to further optimize the design of such recurrent architec-
tures. Our proposed neural networks offer similar state

evolution and feature aggregation functionalities but de-
rive the motivation for the operations involved from well-
established kernel computations over sequences.

Recursive neural networks are alternative architectures to
model hierarchical structures such as syntax trees and logic
forms. For instance, Socher et al. (2013) employs recur-
sive networks for sentence classiﬁcation, where each node
in the dependency tree of the sentence is transformed into
a vector representation. Tai et al. (2015) further proposed
tree-LSTM, which incorporates LSTM-style architectures
as the transformation unit. Dyer et al. (2015; 2016) re-
cently introduced a recursive neural model for transition-
based language modeling and parsing. While not speciﬁ-
cally discussed in the paper, our ideas do extend to similar
neural components for hierarchical objects (e.g. trees).

Graph Networks Most of the current graph neural archi-
tectures perform either convolutional or recurrent opera-
tions on graphs. Duvenaud et al. (2015) developed Neural
Fingerprint for chemical compounds, where each convo-
lution operation is a sum of neighbor node features, fol-
lowed by a linear transformation. Our model differs from
theirs in that our generalized kernels and networks can ag-
gregate neighboring features in a non-linear way. Other ap-
proaches, e.g., Bruna et al. (2013) and Henaff et al. (2015),
rely on graph Laplacian or Fourier transform.

For recurrent architectures, Li et al. (2015) proposed gated
graph neural networks, where neighbor features are ag-
gregated by GRU function. Dai et al. (2016) considers a
different architecture where a graph is viewed as a latent
variable graphical model. Their recurrent model is derived
from Belief Propagation-like algorithms. Our approach is
most closely related to Dai et al. (2016), in terms of neigh-
bor feature aggregation and resulting recurrent architec-
ture. Nonetheless, the focus of this paper is on providing
a framework for how such recurrent networks could be de-
rived from deep graph kernels.

Kernels and Neural Nets Our work follows recent work
demonstrating the connection between neural networks and
kernels (Cho & Saul, 2009; Hazan & Jaakkola, 2015).
For example, Zhang et al. (2016) showed that standard
feedforward neural nets belong to a larger space of recur-
sively constructed kernels (given certain activation func-
tions). Similar results have been made for convolutional
neural nets (Anselmi et al., 2015), and general computa-
tional graphs (Daniely et al., 2016). We extend prior work
to kernels and neural architectures over structured inputs,
in particular, sequences and graphs. Another difference is
how we train the model. While some prior work appeals
to convex optimization through improper learning (Zhang
et al., 2016; Heinemann et al., 2016) (since kernel space is
larger), we use the proposed networks as building blocks in
typical non-convex but ﬂexible neural network training.

Deriving Neural Architectures from Sequence and Graph Kernels

6. Experiments

The left-over question is whether the proposed class of op-
erations, despite its formal characteristics, leads to more
effective architecture exploration and hence improved per-
formance. In this section, we apply the proposed sequence
and graph modules to various tasks and empirically evalu-
ate their performance against other neural network models.
These tasks include language modeling, sentiment classiﬁ-
cation and molecule regression.

6.1. Language Modeling on PTB

Dataset and Setup We use the Penn Tree Bank (PTB) cor-
pus as the benchmark. The dataset contains about 1 million
tokens in total. We use the standard train/development/test
split of this dataset with vocabulary of size 10,000.

Model Conﬁguration Following standard practice, we use
SGD with an initial learning rate of 1.0 and decrease the
learning rate by a constant factor after a certain epoch. We
back-propagate the gradient with an unroll size of 35 and
use dropout (Hinton et al., 2012) as the regularization. Un-
less otherwise speciﬁed, we train 3-layer networks with
n = 1 and normalized adaptive decay.5 Following (Zilly
et al., 2016), we add highway connections (Srivastava et al.,
2015) within each layer:

c(l)[t] = λt (cid:12) c(l)[t − 1] + (1 − λt) (cid:12) (W(l)h(l−1)[t])
h(l)[t] = ft (cid:12) c(l)[t] + (1 − ft) (cid:12) h(l−1)[t]

where h(0)[t] = xt, λt is the gated decay factor and ft is
the transformation gate of highway connections.6

Results Table 1 compares our model with various state-of-
the-art models. Our small model with 5 million parameters
achieves a test perplexity of 73.6, already outperforming
many results achieved using much larger network. By in-
creasing the network size to 20 million, we obtain a test
perplexity of 69.2, with standard dropout. Adding varia-
tional dropout (Gal & Ghahramani, 2016) within the re-
current cells further improves the perplexity to 65.5. Fi-
nally, the model achieves 63.8 perplexity when the recur-
rence depth is increased to 4, being state-of-the-art and on
par with the results reported in (Zilly et al., 2016; Zoph &
Le, 2016). Note that Zilly et al. (2016) uses 10 neural lay-
ers and Zoph & Le (2016) adopts a complex recurrent cell
found by reinforcement learning based search. Our net-
work is architecturally much simpler.

Figure 3 analyzes several variants of our model. Word-
level CNNs are degraded cases (λ = 0) that ignore non-

5See the supplementary sections for a discussion of network

variants.

6We found non-linear activation is no longer necessary when

the highway connection is added.

Table 1: Comparison with state-of-the-art results on PTB.
|θ| denotes the number of parameters. Following recent
work (Press & Wolf, 2016), we share the input and out-
put word embedding matrix. We report the test perplexity
(PPL) of each model. Lower number is better.

Model
LSTM (large) (Zaremba et al., 2014)
Character CNN (Kim et al., 2015)
Variational LSTM (Gal & Ghahramani)
Variational LSTM (Gal & Ghahramani)
Pointer Sentinel-LSTM (Merity et al.)
Variational RHN (Zilly et al., 2016)
Neural Net Search (Zoph & Le, 2016)
Kernel NN (λ = 0.8)
Kernel NN (λ learned as parameter)
Kernel NN (gated λ)
Kernel NN (gated λ)

+ variational dropout
+ variational dropout, 4 RNN layers

|θ|
PPL
66m 78.4
19m 78.9
20m 78.6
66m 73.4
21m 70.9
23m 65.4
25m 64.0
5m 84.3
5m 76.8
5m 73.6
20m 69.2
20m 65.5
20m 63.8

Figure 3. Comparison between kernel NN variants on PTB. |θ| =
5m for all models. Hyper-parameter search is performed for each
variant.

contiguous n-gram patterns. Clearly, this variant performs
worse compared to other recurrent variants with λ > 0.
Moreover, the test perplexity improves from 84.3 to 76.8
when we train the constant decay vector as part of the
model parameters. Finally, the last two variants utilize neu-
ral gates (depending on input xt only or both input xt and
previous state h[t−1]), further improving the performance.

6.2. Sentiment Classiﬁcation

Dataset and Setup We evaluate our model on the sentence
classiﬁcation task. We use the Stanford Sentiment Tree-
bank benchmark (Socher et al., 2013). The dataset con-
sists of 11855 parsed English sentences annotated at both
the root (i.e. sentence) level and the phrase level using 5-
class ﬁne-grained labels. We use the standard split for train-
ing, development and testing. Following previous work, we
also evaluate our model on the binary classiﬁcation variant
of this benchmark, ignoring all neutral sentences.

Deriving Neural Architectures from Sequence and Graph Kernels

Table 2: Classiﬁcation accuracy on Stanford Sentiment
Treebank. Block I: recursive networks; Block II: convo-
lutional or recurrent networks; Block III: other baseline
methods. Higher number is better.

Model
RNN (Socher et al. (2011))
RNTN (Socher et al. (2013))
DRNN (Irsoy & Cardie (2014))
RLSTM (Tai et al. (2015))
DCNN (Kalchbrenner et al. (2014))
CNN-MC (Kim (2014))
Bi-LSTM (Tai et al. (2015))
LSTMN (Cheng et al. (2016))
PVEC (Le & Mikolov (2014))
DAN (Iyyer et al. (2014))
DMN (Kumar et al. (2016))
Kernel NN, λ = 0.5
Kernel NN, gated λ

Fine Binary
43.2
45.7
49.8
51.0
48.5
47.4
49.1
47.9
48.7
48.2
52.1
51.2
53.2

82.4
85.4
86.8
88.0
86.9
88.1
87.5
87.0
87.8
86.8
88.6
88.6
89.9

Following the recent work of DAN (Iyyer et al., 2015) and
RLSTM (Tai et al., 2015), we use the publicly available
300-dimensional GloVe word vectors (Pennington et al.,
2014). Unlike prior work which ﬁne tunes the word vec-
tors, we normalize the vectors (i.e. (cid:107)w(cid:107)2
2 = 1) and ﬁxed
them for simplicity.

Model Conﬁguration Our best model is a 3-layer network
with n = 2 and hidden dimension d = 200. We average
the hidden states h[t] across t, and concatenate the averaged
vectors from the 3 layers as the input of the ﬁnal softmax
layer. The model is optimized with Adam (Kingma & Ba,
2015), and dropout probability of 0.35.

Results Table 2 presents the performance of our model
and other networks. We report the best results achieved
across 5 independent runs. Our best model obtains 53.2%
and 89.9% test accuracies on ﬁne-grained and binary tasks
respectively. Our model with only a constant decay factor
also obtains quite high accuracy, outperforming other base-
line methods shown in the table.

6.3. Molecular Graph Regression

Dataset and Setup We further evaluate our graph NN
models on the Harvard Clean Energy Project benchmark,
which has been used in Dai et al. (2016); Duvenaud et al.
(2015) as their evaluation dataset. This dataset contains 2.3
million candidate molecules, with each molecule labeled
with its power conversion efﬁciency (PCE) value.

We follow exactly the same train-test split as Dai et al.
(2016), and the same re-sampling procedure on the training
data (but not the test data) to make the algorithm put more

Table 3: Experiments on Harvard Clean Energy Project.
We report Root Mean Square Error(RMSE) on test set. The
ﬁrst block lists the results reported in Dai et al. (2016) for
reference. For fair comparison, we reimplemented their
best model so that all models are trained under the same
setup. Results under our setup is reported in second block.

Model (Dai et al., 2016)
Mean Predicator
Weisfeiler-lehman Kernel, degree=3
Weisfeiler-lehman Kernel, degree=6
Embedded Mean Field
Embedded Loopy BP
Under Our Setup
Neural Fingerprint
Embedded Loopy BP
Weisfeiler Kernel NN
Weisfeiler Kernel NN, gated λ

|θ|
RMSE
2.4062
1
0.2040
1.6m
1378m 0.1367
0.1250
0.1m
0.1174
0.1m

0.26m 0.1409
0.26m 0.1065
0.26m 0.1058
0.26m 0.1043

emphasis on molecules with higher PCE values, since the
data is distributed unevenly.

We use the same feature set as in Duvenaud et al. (2015) for
atoms and bonds. Initial atom features include the atoms
element, its degree, the number of attached hydrogens, its
implicit valence, and an aromaticity indicator. The bond
feature is a concatenation of bond type indicator, whether
the bond is conjugated, and whether the bond is in a ring.

Model Conﬁguration Our model is a Weisfeiler-Lehman
NN, with 4 recurrent iterations and n = 2. All models
(including baseline) are optimized with Adam (Kingma &
Ba, 2015), with learning rate decay factor 0.9.

Results
In Table 3, we report the performance of our
model against other baseline methods. Neural Finger-
print (Duvenaud et al., 2015) is a 4-layer convolutional neu-
ral network. Convolution is applied to each atom, which
sums over its neighbors’ hidden state, followed by a lin-
ear transformation and non-linear activation. Embedded
Loopy BP (Dai et al., 2016) is a recurrent architecture, with
4 recurrent iterations. It maintains message vectors for each
atom and bond, and propagates those vectors in a message
passing fashion. Table 3 shows our model achieves state-
of-the-art against various baselines.

7. Conclusion

We proposed a class of deep recurrent neural architectures
and formally characterized its underlying computation us-
ing kernels. By linking kernel and neural operations, we
have a “template” for deriving new families of neural archi-
tectures for sequences and graphs. We hope the theoretical
view of kernel neural networks can be helpful for future
model exploration.

Deriving Neural Architectures from Sequence and Graph Kernels

Acknowledgement

We thank Prof. Le Song for sharing Harvard Clean En-
ergy Project dataset. We also thank Yu Zhang, Vikas Garg,
David Alvarez, Tianxiao Shen, Karthik Narasimhan and
the reviewers for their helpful comments. This work was
supported by the DARPA Make-It program under contract
ARO W911NF-16-2-0023.

References

Anselmi, Fabio, Rosasco, Lorenzo, Tan, Cheston, and Pog-
gio, Tomaso. Deep convolutional networks are hierarchi-
cal kernel machines. preprint arXiv:1508.01084, 2015.

Balduzzi, David and Ghifary, Muhammad. Strongly-typed
In Proceedings of 33th In-
recurrent neural networks.
ternational Conference on Machine Learning (ICML),
2016.

Bruna, Joan, Zaremba, Wojciech, Szlam, Arthur, and Le-
Cun, Yann. Spectral networks and locally connected net-
works on graphs. arXiv preprint arXiv:1312.6203, 2013.

Cheng, Jianpeng, Dong, Li, and Lapata, Mirella. Long
short-term memory networks for machine reading. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pp. 551–562, 2016.

Cho, Youngmin and Saul, Lawrence K. Kernel methods for
deep learning. In Bengio, Y., Schuurmans, D., Lafferty,
J. D., Williams, C. K. I., and Culotta, A. (eds.), Advances
in Neural Information Processing Systems 22, pp. 342–
350. 2009.

Chung, Junyoung, Gulcehre, Caglar, Cho, KyungHyun,
and Bengio, Yoshua. Empirical evaluation of gated re-
current neural networks on sequence modeling. arXiv
preprint arXiv:1412.3555, 2014.

Dai, Hanjun, Dai, Bo, and Song, Le. Discriminative em-
beddings of latent variable models for structured data.
arXiv preprint arXiv:1603.05629, 2016.

Daniely, Amit, Frostig, Roy, and Singer, Yoram. Toward
deeper understanding of neural networks: The power of
initialization and a dual view on expressivity. CoRR,
abs/1602.05897, 2016.

Duvenaud, David K, Maclaurin, Dougal,

Iparraguirre,
Jorge, Bombarell, Rafael, Hirzel, Timothy, Aspuru-
Guzik, Al´an, and Adams, Ryan P. Convolutional net-
works on graphs for learning molecular ﬁngerprints. In
Advances in neural information processing systems, pp.
2224–2232, 2015.

Dyer, Chris, Ballesteros, Miguel, Ling, Wang, Matthews,
Austin, and Smith, Noah A. Transition-based depen-
dency parsing with stack long short-term memory.
In
Proceedings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long Pa-
pers), Beijing, China, July 2015.

Dyer, Chris, Kuncoro, Adhiguna, Ballesteros, Miguel, and
Smith, Noah A. Recurrent neural network grammars. In
Proceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational Lin-
guistics, San Diego, California, June 2016.

Gal, Yarin and Ghahramani, Zoubin. A theoretically
grounded application of dropout in recurrent neural net-
In Advances in Neural Information Processing
works.
Systems 29 (NIPS), 2016.

G¨artner, Thomas, Flach, Peter, and Wrobel, Stefan. On
graph kernels: Hardness results and efﬁcient alterna-
In Learning Theory and Kernel Machines, pp.
tives.
129–143. Springer, 2003.

Greff, Klaus, Srivastava, Rupesh Kumar, Koutn´ık, Jan, Ste-
unebrink, Bas R, and Schmidhuber, J¨urgen. Lstm: A
search space odyssey. arXiv preprint arXiv:1503.04069,
2015.

Hazan, Tamir and Jaakkola, Tommi. Steps toward deep
kernel methods from inﬁnite neural networks. arXiv
preprint arXiv:1508.05133, 2015.

Heinemann, Uri, Livni, Roi, Eban, Elad, Elidan, Gal, and
Globerson, Amir. Improper deep kernels. In Proceed-
ings of the 19th International Conference on Artiﬁcial
Intelligence and Statistics, pp. 1159–1167, 2016.

Henaff, Mikael, Bruna, Joan, and LeCun, Yann. Deep
convolutional networks on graph-structured data. arXiv
preprint arXiv:1506.05163, 2015.

Hinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan R. Improving
neural networks by preventing co-adaptation of feature
detectors. arXiv preprint arXiv:1207.0580, 2012.

Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-
term memory. Neural computation, 9(8):1735–1780,
1997.

Irsoy, Ozan and Cardie, Claire. Deep recursive neural net-
works for compositionality in language. In Advances in
Neural Information Processing Systems, 2014.

Iyyer, Mohit, Boyd-Graber, Jordan, Claudino, Leonardo,
Socher, Richard, and Daum´e III, Hal. A neural net-
work for factoid question answering over paragraphs. In

Deriving Neural Architectures from Sequence and Graph Kernels

Proceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pp. 633–
644, Doha, Qatar, October 2014.

Merity, Stephen, Xiong, Caiming, Bradbury, James, and
Socher, Richard. Pointer sentinel mixture models. arXiv
preprint arXiv:1609.07843, 2016.

Iyyer, Mohit, Manjunatha, Varun, Boyd-Graber, Jordan,
and Daum´e III, Hal. Deep unordered composition rivals
syntactic methods for text classiﬁcation. In Proceedings
of the 53rd Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), 2015.

Kalchbrenner, Nal, Grefenstette, Edward, and Blunsom,
Phil. A convolutional neural network for modelling sen-
tences. In Proceedings of the 52th Annual Meeting of the
Association for Computational Linguistics, 2014.

Kim, Yoon. Convolutional neural networks for sentence
classiﬁcation. In Proceedings of the Empiricial Methods
in Natural Language Processing (EMNLP 2014), 2014.

Kim, Yoon, Jernite, Yacine, Sontag, David, and Rush,
Alexander M. Character-aware neural language mod-
els. Twenty-Ninth AAAI Conference on Artiﬁcial Intelli-
gence, 2015.

Kingma, Diederik P and Ba, Jimmy Lei. Adam: A method
for stochastic optimization. In International Conference
on Learning Representation, 2015.

Kumar, Ankit, Irsoy, Ozan, Ondruska, Peter, Iyyer, Mo-
hit, James Bradbury, Ishaan Gulrajani, Zhong, Victor,
Paulus, Romain, and Socher, Richard. Ask me any-
thing: Dynamic memory networks for natural language
processing. 2016.

Le, Quoc and Mikolov, Tomas. Distributed representations
of sentences and documents. In Proceedings of the 31st
International Conference on Machine Learning (ICML-
14), pp. 1188–1196, 2014.

Lee, Kenton, Levy, Omer, and Zettlemoyer, Luke. Recur-

rent additive networks. Preprint, 2017.

Lei, Tao, Joshi, Hrishikesh, Barzilay, Regina, Jaakkola,
Tommi, Tymoshenko, Katerina, Moschitti, Alessan-
Semi-supervised ques-
dro, and Marquez, Lluis.
tion retrieval with gated convolutions. arXiv preprint
arXiv:1512.05726, 2015.

Li, Yujia, Tarlow, Daniel, Brockschmidt, Marc, and Zemel,
Richard. Gated graph sequence neural networks. arXiv
preprint arXiv:1511.05493, 2015.

Pennington,

Jeffrey, Socher, Richard, and Manning,
Christopher D. Glove: Global vectors for word repre-
sentation. volume 12, 2014.

Press, Oﬁr and Wolf, Lior. Using the output embed-
arXiv preprint

ding to improve language models.
arXiv:1608.05859, 2016.

Ramon, Jan and G¨artner, Thomas. Expressivity versus efﬁ-
ciency of graph kernels. In First international workshop
on mining graphs, trees and sequences, pp. 65–74. Cite-
seer, 2003.

Shalev-Shwartz, Shai, Shamir, Ohad, and Sridharan,
Karthik. Learning kernel-based halfspaces with the 0-
1 loss. SIAM Journal on Computing, 40(6):1623–1646,
2011.

Shervashidze, Nino, Schweitzer, Pascal, Leeuwen, Erik
Jan van, Mehlhorn, Kurt, and Borgwardt, Karsten M.
Weisfeiler-lehman graph kernels. Journal of Machine
Learning Research, 12(Sep):2539–2561, 2011.

Socher, Richard, Pennington, Jeffrey, Huang, Eric H,
Ng, Andrew Y, and Manning, Christopher D. Semi-
supervised recursive autoencoders for predicting senti-
ment distributions. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing, pp.
151–161, 2011.

Socher, Richard, Perelygin, Alex, Wu, Jean, Chuang, Ja-
son, Manning, Christopher D., Ng, Andrew Y., and Potts,
Christopher. Recursive deep models for semantic com-
positionality over a sentiment treebank. In Proceedings
of the 2013 Conference on Empirical Methods in Natural
Language Processing, pp. 1631–1642, October 2013.

Srivastava, Rupesh K, Greff, Klaus, and Schmidhuber,
In Advances in
J¨urgen. Training very deep networks.
neural information processing systems, pp. 2377–2385,
2015.

Tai, Kai Sheng, Socher, Richard, and Manning, Christo-
Improved semantic representations from tree-
pher D.
In Pro-
structured long short-term memory networks.
ceedings of the 53th Annual Meeting of the Association
for Computational Linguistics, 2015.

Lodhi, Huma, Saunders, Craig, Shawe-Taylor, John, Cris-
tianini, Nello, and Watkins, Chris. Text classiﬁcation
using string kernels. Journal of Machine Learning Re-
search, 2(Feb):419–444, 2002.

Tamar, Aviv, Levine, Sergey, Abbeel, Pieter, Wu, Yi, and
Thomas, Garrett. Value iteration networks. In Advances
in Neural Information Processing Systems, pp. 2146–
2154, 2016.

Deriving Neural Architectures from Sequence and Graph Kernels

Vishwanathan, S Vichy N, Schraudolph, Nicol N, Kondor,
Risi, and Borgwardt, Karsten M. Graph kernels. Jour-
nal of Machine Learning Research, 11(Apr):1201–1242,
2010.

Zaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol.
Recurrent neural network regularization. arXiv preprint
arXiv:1409.2329, 2014.

Zhang, Yuchen, Lee, Jason D., and Jordan, Michael I. (cid:96)1-
regularized neural networks are improperly learnable in
In Proceedings of the 33nd Interna-
polynomial time.
tional Conference on Machine Learning, 2016.

Zilly, Julian Georg, Srivastava, Rupesh Kumar, Koutn´ık,
Jan, and Schmidhuber, J¨urgen. Recurrent Highway Net-
works. arXiv preprint arXiv:1607.03474, 2016.

Zoph, Barret and Le, Quoc V.

search with reinforcement learning.
arXiv:1611.01578, 2016.

Neural architecture
arXiv preprint

Supplementary Material

A. Examples of kernel / neural variants

Our theoretical results apply to some other variants of sequence kernels and the associated neural compo-
nents. We give some examples in the this section. Table 4 shows three network variants, corresponding
to three realizations of string kernels provided in Table 5.

Connection to LSTMs
Interestingly, many recent work has reached similar RNN architectures
through empirical exploration. For example, Greff et al. (2015) found that simplifying LSTMs, by re-
moving the input gate or coupling it with the forget gate does not signiﬁcantly change the performance.
However, the forget gate (corresponding to the decay factor λ in our notation) is crucial for performance.
This is consistent with our theoretical analysis and the empirical results in Figure 3. Moreover, Balduzzi
& Ghifary (2016) and Lee et al. (2017) both suggest that a linear additive state computation sufﬁces to
provide competitive performance compared to LSTMs: 7

c[t] = λf (cid:12) c1[t − 1] + λi (cid:12) (Wxt)
h[t] = σ(c[t])

In fact, this variant becomes an instance of the kernel NN presented in this work (with n = 1 and adaptive
gating), when λf = λt and λi = 1 − λt or 1.

7Balduzzi & Ghifary (2016) also includes the previous token , i.e. Wxt + W(cid:48)xt−1, which doesn’t affect the

discussion here.

Table 4: Example sequence NN variants. We present these equations in the context of n = 3.

(a) Multiplicative mapping, aggregation un-normalized:

c1[t] = λ · c1[t − 1] + W(1)xt

c2[t] = λ · c2[t − 1] +

c3[t] = λ · c3[t − 1] +

(cid:16)
c1[t − 1] (cid:12) W(2)xt
(cid:16)
c2[t − 1] (cid:12) W(3)xt

(cid:17)

(cid:17)

(b) Multiplicative mapping, aggregation normalized:

c1[t] = λ · c1[t − 1] + (1 − λ) · W(1)xt

c2[t] = λ · c2[t − 1] + (1 − λ) ·

c3[t] = λ · c3[t − 1] + (1 − λ) ·

(cid:16)
c1[t − 1] (cid:12) W(2)xt
(cid:16)
c2[t − 1] (cid:12) W(3)xt

(cid:17)

(cid:17)

(c) Additive mapping, aggregation normalized:

c1[t] = λ · c1[t − 1] + (1 − λ) · W(1)xt

c2[t] = λ · c2[t − 1] + (1 − λ) ·

c3[t] = λ · c3[t − 1] + (1 − λ) ·

(cid:16)
c1[t − 1] + W(2)xt
(cid:16)
c2[t − 1] + W(3)xt

(cid:17)

(cid:17)

Final activation:

h[t] = σ (c3[t])

h[t] = σ (c1[t] + c2[t] + c3[t])

(any linear combination of c∗[t])

Table 5: Examples of sequence kernel functions and associated mappings. [xi, xj] denotes the concate-
nation of two vectors.

(a) Multiplicative mapping, aggregation un-normalized:

K2(x, y) =

(cid:88)

(cid:88)

λ|x|−i−1 λ|y|−k−1 (cid:104)xi, yk(cid:105) · (cid:104)xj, yl(cid:105)

1≤i<j≤|x|

1≤k<l≤|y|

φ2(x) =

(cid:88)

λ|x|−i−1 xi ⊗ xj

1≤i<j≤|x|

(b) Multiplicative mapping, aggregation normalized:

K2(x, y) =

1
Z

(cid:88)

(cid:88)

λ|x|−i−1 λ|y|−k−1 (cid:104)xi, yk(cid:105) · (cid:104)xj, yl(cid:105)

1≤i<j≤|x|

1≤k<l≤|y|

s.t. Z =

(cid:88)

(cid:88)

λ|x|−i−1 λ|y|−k−1

1≤i<j≤|x|

1≤k<l≤|y|

φ2(x) =

1
Z (cid:48)

(cid:88)

λ|x|−i−1 xi ⊗ xj

1≤i<j≤|x|

s.t. Z (cid:48) =

(cid:88)

λ|x|−i−1

1≤i<j≤|x|

(c) Additive mapping, aggregation normalized:

K2(x, y) =

1
Z

(cid:88)

(cid:88)

1≤i<j≤|x|

1≤k<l≤|y|

λ|x|−i−1 λ|y|−k−1 ((cid:104)xi, yk(cid:105) + (cid:104)xj, yl(cid:105))

s.t. Z =

(cid:88)

(cid:88)

λ|x|−i−1 λ|y|−k−1

1≤i<j≤|x|

1≤k<l≤|y|

φ2(x) =

1
Z (cid:48)

(cid:88)

λ|x|−i−1 [xi, xj]

1≤i<j≤|x|

s.t. Z (cid:48) =

(cid:88)

λ|x|−i−1

1≤i<j≤|x|

B. Proof of Theorem 1

We ﬁrst generalize the kernel deﬁnition in Eq.(3) to the case of any n-gram. For any integer n > 0, the
underlying mapping of the n-th order string kernel is deﬁned as,

φn(x) =

(cid:88)

λ|x|−i1−n+1 xi1 ⊗ xi2 ⊗ · · · ⊗ xin

1≤i1<···<in≤|x|

We now show that String Kernel NN states cn[t] deﬁned in (4),

n = 1 :

n > 1 :

c1[t] = λ · c1[t − 1] + W(1)xt

cn[t] = λ · cn[t − 1] +

cn−1[t − 1] (cid:12) W(n)xt

(cid:16)

(cid:17)

is equivalent to summing over all n-grams within the ﬁrst t tokens x1:t = {x1, · · · , xt},

cn[t] =

(cid:88)

(cid:16)

W(1)xi1 (cid:12) · · · (cid:12) W(n)xin

· λt−i1−n+1

(cid:17)

1≤i1<i2<···<in≤t

Proof: We prove the equivalence by induction. When n = 1, for c1[t] we have,

=

(cid:88)

(cid:16)

(cid:17)

W(1)xi1

· λt−1−i1

·λ + W(1)xt

c1[t] =

(cid:88)

(cid:16)

(cid:17)

W(1)xi1

· λt−i1

1≤i1≤t





(cid:124)

1≤i1≤t−1

(cid:123)(cid:122)
c1[t−1]

= c1[t − 1] · λ + W(1)xt





(cid:125)

When n = k > 1, suppose the state iteration holds for 1, · · · , k − 1, we have,

cn[t] =

(cid:88)

(cid:16)

W(1)xi1 (cid:12) · · · (cid:12) W(n)xin

· λt−i1−n+1

(cid:17)

1≤i1<i2<···<in≤t


(cid:88)

(cid:16)

1≤i1<i2<···<in≤t−1

W(1)xi1 (cid:12) · · · (cid:12) W(n)xin

· λt−1−i1−n+1



·λ

(cid:17)

(cid:123)(cid:122)
when in<t: cn[t−1]

(cid:88)

(cid:16)

W(1)xi1 (cid:12) · · · (cid:12) W(n)xin

· λt−1−i1−n+1

(cid:17)



(cid:125)





(cid:125)

=



(cid:124)





(cid:124)

+

1≤i1<i2<···<in=t

= cn[t − 1] · λ +

cn−1[t − 1] (cid:12) W(n)xt

(cid:16)

(cid:123)(cid:122)
when in=t

(cid:17)

Now we proceed to prove Theorem 1.

Proof: From the above results, we know that for cn[t],

cn[t] =

(cid:88)

(cid:16)

W(1)xi1 (cid:12) · · · (cid:12) W(n)xin

· λt−i1−n+1

(cid:17)

1≤i1<i2<···<in≤t

In particular, the value of the i-th entry, cn[t][i], is equal to,

cn[t][i] =

(cid:88)

(cid:68)
w(1)
i

, xi1

(cid:69)

(cid:68)
w(n)
i

, xin

· λt−i1−n+1

1≤i1<i2<···<in≤t

(cid:124)
(cid:68)

xi1 ⊗···⊗xin ,w(1)

i ⊗···⊗w(n)

i

· · ·
(cid:123)(cid:122)

(cid:69)

(cid:125)
(cid:69)

λt−i1−n+1 xi1 ⊗ · · · ⊗ xin , w(1)

i ⊗ · · · ⊗ w(n)

i

(cid:43)

(cid:42)

=

(cid:88)

1≤i1<i2<···<in≤t

= (cid:104)φn(x1:t), φn(wi,n)(cid:105)

where w(k)
proof since Kn(x1:t, wi,n) = (cid:104)φn(x1:t), φn(wi,n)(cid:105) by deﬁnition.

represents the i-th row of matrix W(k) and wi,n = {w(1)

i

i

, · · · , w(n)

i }. This completes the

Remarks This theorem demonstrates that the NN state vectors cn can be seen as projecting the kernel
mapping φn(x) into a low-dimensional space using the parameterized matrices {W(i)}i = 1n. This
is similar to word embeddings and CNNs, where the input (a single word or an n-gram) is projected to
obtain a low-dimensional representation.

C. Proof of Theorem 2

We ﬁrst review necessary concepts and notations for the ease of reading. Similar to the proof in Ap-
pendix B, the generalized string kernel K(l) and K(l)
σ in Eq.(??) can be deﬁned with the underlying
mappings,

λ|x|−i1−n+1 φ(l−1)

σ

(x1:i1) ⊗ φ(l−1)

σ

(x1:i2) ⊗ · · · ⊗ φ(l−1)

σ

(x1:i1)

φ(l)(x) =

(cid:88)

1≤i1<···<in≤|x|

φ(l)
σ (x) = φσ(φ(l)(x))

where φσ() is the underlying mapping of a kernel function whose reproducing kernel Hilbert space
(RKHS) contains the non-linear activation σ() used in the String Kernel NN layer. Here K(l)() is the
“pre-activation kernel” and K(l)
σ () is the “post-activation kernel”. To show that the values of String
Kernel NN states c(l)[t] is contained in the RKHS of K(l)() and that of h(l)[t] is contained in the RKHS
of K(l)
Theorem 5. Given a deep n-gram String Kernel NN with non-linear activation σ(). Assuming σ() lies
in the RKHS of a kernel function with underlying mapping φσ(), then

σ (), we re-state the claim in the following way,

(i) c(l)[t] lies in the RKHS of kernel K(l)() in the sense that

c(l)
j [t][i] =

(cid:68)
φ(l)(x1:t), ψ(l)
i,j

(cid:69)

for any internal state c(l)
model parameters;

j [t] (1 ≤ j ≤ n) of the l-th layer, where ψ(l)

i,j is a mapping constructed from

(ii) h(l)[t] lies in the RKHS of kernel K(l)

σ () as a corollary in the sense that

h(l)[t][i] = σ(c(l)
(cid:16)(cid:68)

n [t][i])
φ(l)(x1:t), ψ(l)
i,n

(cid:69)(cid:17)

= σ
(cid:68)

=

φσ(φ(l)(x1:t)), ψσ(ψ(l)
i,n)

(cid:69)

(based on (i))

(Lemma 1)

and we denote ψσ(ψ(l)

i,n) as ψ(l)

σ,i,n for short.

Proof: We prove by induction on l. When l = 1, the proof of Theorem 1 already shows that c(1)
(cid:68)
j (x1:t), φ(1)
φ(1)
proof for the case of l = 1.

in a one-layer String Kernel NN. Simply let ψ(1)

[t][i] =
j (wi,j) completes the

i,j = φ(1)

j (wi,j)

(cid:69)

j

Suppose the lemma holds for l = 1, · · · , k, we now prove the case of l = k + 1. Similar to the proof of
Theorem 1, the value of c(k+1)

[t][i] equals to

j

c(k+1)
j

[t][i] =

(cid:88)

(cid:68)
w(1)
i

(cid:69)
, h(k)[i1]

· · ·

(cid:68)

w(j)
i

(cid:69)
, h(k)[ij]

· λt−i1−j+1

(12)

1≤i1<···<ij ≤t

where w(j)
i
(cid:68)
φ(k)
σ (x1:t), ψ(k)
σ,i,n
can rewrite h(k)[t] = M φ(k)

is the i-th row of the parameter matrix W(j) of the l-th layer. Note h(k)[t][i] =
σ,i,n}i as the row vectors.8 We then

(cid:69)
, we construct a matrix M by stacking all {ψ(k)

σ (x1:t). Plugging this into Eq (12), we get

c(k+1)
j

[t][i] =

w(1)
i

, M φ(k)

σ (x1:i1 )

(cid:69)

(cid:68)

· · ·

w(j)
i

, M φ(k)

σ (x1:ij )

· λt−i1−j+1

(cid:69)

1≤i1<···<ij ≤t

(cid:88)

(cid:88)

(cid:68)

(cid:42)

1≤i1<···<ij ≤t

1≤i1<···<ij ≤t

(cid:42)

(cid:88)

1≤i1<···<ij ≤t
(cid:124)

=

=

=

M(cid:62) w(1)
(cid:125)
(cid:123)(cid:122)
(cid:124)
ui,1

i

(cid:43)

(cid:42)

, φ(k)

σ (x1:i1)

· · ·

(cid:43)

σ (x1:ij )

· λt−i1−j+1

M(cid:62) w(j)
, φ(k)
(cid:125)
(cid:123)(cid:122)
(cid:124)
ui,j

i

(cid:88)

(cid:68)

ui,1, φ(k)

σ (x1:i1)

(cid:69)

(cid:68)
ui,j, φ(k)

σ (x1:ij )

(cid:69)

· · ·

· λt−i1−j+1

λt−i1−j+1 φ(k)

σ (xi1 ) ⊗ · · · ⊗ φ(k)

σ (xij )

, ui,1 ⊗ · · · ⊗ ui,j

(cid:43)

(cid:123)(cid:122)
φ(k+1)(x1:t)

(cid:125)

(cid:68)
φ(k+1)(x1:t), ui,1 ⊗ · · · ⊗ ui,j

(cid:69)

=

Deﬁne ψ(k+1)

i,j

= ui,1 ⊗ · · · ⊗ ui,j completes the proof for the case of l = k + 1.

8Note in practice the mappings φ(k)
σ

σ may have inﬁnite dimension because the underlying mapping
for the non-linear activation φσ() can have inﬁnite dimension. The proof still apply since the dimensions are still
countable and the vectors have ﬁnite norm (easy to show this by assuming the input xi and parameter W are
bounded.

and ψ(k)

D. Proof for Theorem 3

Recall that random walk graph kernel is deﬁned as:

Kn(G, G(cid:48)) = λn (cid:88)

(cid:88)

n
(cid:89)

(cid:104)fxi , fyi (cid:105)

x∈Pn(G)

y∈Pn(G(cid:48))

i=1

φn(G) = λn (cid:88)

fx0 ⊗ fx1 ⊗ · · · ⊗ fxn

x∈Pn(G)

and single-layer graph NN computes its states as:

cn[v] = λ

cn−1[u] (cid:12) W(n)fv

(cid:88)

u∈N (v)

= λn (cid:88)

un−1∈N (v)
(cid:88)

= λn

u=u0···un∈Pn(G,v)

(cid:88)

(cid:88)

· · ·

W(0)fu0 (cid:12) W(1)fu1 (cid:12) · · · (cid:12) W(n)fun

un−2∈N (un−1)

u0∈N (u1)

W(0)fu0 (cid:12) W(1)fu1 (cid:12) · · · (cid:12) W(n)fun

where we deﬁne Pn(G, v) be the set of walks that end with node v. For the i-th coordinate of cn[v], we
have

cn[v][i] = λn (cid:88)

(cid:89)

(cid:68)

(cid:69)

w(i)
i

, fui

(cid:88)

cn[v][i] = λn (cid:88)

(cid:68)
w(i)
i

, fui

(cid:69)

v

u∈Pn(G,v)

i
(cid:89)

u∈Pn(G)

i

= Kn(G, Ln,k)

The last step relies on the fact that Pn(Ln,k) has only one element {w(0)

, w(1)
i

, · · · , w(n)

i }.

i

E. Proof for Theorem 4

For clarity, we re-state the kernel deﬁnition and theorem in the following:

K(L,n)(G, G(cid:48)) =

(cid:88)

(cid:88)

v

v(cid:48)

K(L,n)

loc,σ (v, v(cid:48))

When l = 1:





0

When l > 1:

K(l,j)

loc (v, v(cid:48)) =





0

K(l,j)

loc (v, v(cid:48)) =

(cid:104)fv, fv(cid:48)(cid:105)
(cid:104)fv, fv(cid:48)(cid:105) + λ (cid:80)

(cid:80)
u(cid:48)∈N (v(cid:48))

K(l,j−1)

loc,σ (u, u(cid:48))

u∈N (v)

if j = 1

if j > 1

if Pj(G, v) = ∅ or Pj(G(cid:48), v(cid:48)) = ∅

K(l−1)
loc,σ (v, v(cid:48))
loc,σ (v, v(cid:48)) + λ (cid:80)
K(l−1)

(cid:80)
u(cid:48)∈N (v(cid:48))

K(l,j−1)

loc,σ (u, u(cid:48))

u∈N (v)

if j = 1

if j > 1

if Pj(G, v) = ∅ or Pj(G(cid:48), v(cid:48)) = ∅

where Pj(G, v) is the set of walks of length j starting from v. Note that we force K(l,j)
loc = 0 when there
is no valid walk starting from v or v(cid:48), so that it only considers walks of length j. We use additive version
just for illustration (multiplicative version follows the same argument).

Theorem 6. For a deep random walk kernel NN with L layers and activation function σ(·), assuming
the ﬁnal output state hG = (cid:80)
v h(l)[v], for l = 1, · · · , L; j = 1, · · · , n:

(i) c(l)

j [v][i] as a function of input v and graph G belongs to the RKHS of kernel K(l,j)

loc (·, ·) in that

c(l)
j [v][i] =

(cid:68)
G (v), ψ(l)
φ(l,j)

i,j

(cid:69)

where φ(l)
parameters.

G (v) is the mapping of node v in graph G, and ψ(l)

i,j is a mapping constructed from model

(ii) h(l)[v][i] belongs to the RKHS of kernel K(l,n)

loc,σ(·, ·) as a corollary:

h(l)[v][i] = σ

uikc(l)

n [v][k]

= σ

(cid:32)

(cid:88)

k
(cid:32)(cid:42)

(cid:68)

uik

G (v), ψ(l)
φ(l,n)

k,n

(cid:33)

(cid:69)

(cid:33)

(cid:32)

(cid:88)

k
(cid:43)(cid:33)

= σ

φ(l,n)
G (v),

uikψ(l)
k,n

(cid:88)

k

=

φσ(φ(l,n)

G (v)), ψσ

uikψ(l)
k,n

(cid:33)(cid:43)

(cid:32)

(cid:88)

k

(cid:42)

(cid:17)

We denote ψσ

(cid:16)(cid:80)

k uikψ(l)

k,n

as ψ(l)

σ,i,n, and φσ(φ(l,n)

G (v)) as φ(l)

G,σ(v) for short.

(iii) hG[i] belongs to the RKHS of kernel K(L,n)(·, ·).

Proof of (i), (ii): We prove by induction on l. When l = 1, from kernel deﬁnition, the kernel mapping is
recursively deﬁned as:

φ(1,j)
G (v) =

√

fv,

λ (cid:80)

u∈N (v)

if j = 1

(cid:35)

φσ(φ(1,j−1)
G

(u))

if j > 1

fv
(cid:34)






0

if Pj(G, v) = ∅ or Pj(G(cid:48), v(cid:48)) = ∅

j

[v][i] =

(cid:68)
G (v), ψ(1)
We prove c(1)
φ(1,j)
Our hypothesis holds by ψ(1)
i,1 = w(1,1)
Pj(G, v) = ∅, we could always set c(1)

i,j

i

(cid:69)

j

by induction on j. When j = 1, c(1)

1 [v][i] =

(cid:68)
w(1,1)
i

(cid:69)

.

, fv

. Suppose induction hypothesis holds for 1, 2, · · · , j − 1. If

[v][i] = 0 in neural network computation. Otherwise:

c(1)
j

[v][i] =

(cid:68)
w(1,j)
i

, fv

+ λ

(cid:88)

(cid:16)

(cid:17)

c(1)
j−1[u][i]

u∈N (v)
(cid:88)

(cid:16)(cid:68)

σ

σ

φ(1,j−1)
G

(u), ψ(1)

i,j−1

(cid:69)(cid:17)

(cid:69)

(cid:69)

(cid:69)

=

=

=

(cid:68)
w(1,j)
i

, fv

+ λ

(cid:68)
w(1,j)
i

, fv

+

λ

u∈N (v)
(cid:88)

√

u∈N (v)

(cid:42)

√

fv,

λ

(cid:88)

u∈N (v)

(cid:68)
φσ(φ(1,j−1)
G

(u)),

λψσ(ψ(1)

i,j−1)

(cid:69)

√



(cid:104)

φσ(φ(1,j−1)
G

(u))

 ,

w(1,j)
i

,

√

λψσ(ψ(1)

(cid:105)
i,j−1)

(cid:43)

(cid:104)
w(1,j)
i

i,j =

Let ψ(1)
,
for 1, 2, · · · , l − 1. Note that when l > 1:

λψσ(ψ(1)

(cid:105)
i,j−1)

√

concludes the base case l = 1. Suppose induction hypothesis holds

φ(l,j)
G (v) =

φσ(φ(l−1)
G
(cid:34)

(v))

φσ(φ(l−1)
G

(v)),

√

λ (cid:80)

u∈N (v)

if j = 1

(cid:35)

φσ(φ(l,j−1)
G

(u))

if j > 1






0

Now we prove c(l)

j [v][i] =

(cid:68)
G (v), ψ(l)
φ(l,j)

i,j

(cid:69)

by induction on j. When j = 1,

if Pj(G, v) = ∅ or Pj(G(cid:48), v(cid:48)) = ∅

c(l)
1 [v][i] =

(cid:69)

(cid:68)
w(l,1)
i
(cid:88)

, h(l−1)[v]
(cid:68)
G,σ (v), ψ(l−1)
φ(l−1)

σ,k,n

w(l,1)
ik

(cid:69)

φ(l,1)
G (v),

w(l,1)

ik ψ(l−1)

σ,k,n

(cid:88)

k

(cid:43)

=

=

k
(cid:42)

i,j = (cid:80)

Let ψ(l)
Same as before, we only consider the case when Pj(G, v) (cid:54)= ∅:

ik ψ(l−1)

k w(l,1)

σ,k,n completes the proof for j = 1. Now suppose this holds for 1, 2, · · · , j − 1.

c(l)
j [v][i] =

(cid:68)
w(l,j)
i

, h(l−1)[v]

+ λ

(cid:69)

(cid:88)

(cid:16)

σ

c(l)
j−1[u][i]

(cid:17)

(cid:88)

w(l,j)
ik

(cid:68)
G,σ (v), ψ(l−1)
φ(l−1)

σ,k,n

+ λ

(cid:88)

(cid:16)(cid:68)

σ

φ(l,j−1)
G

(u), ψ(l)

i,j−1

(cid:69)(cid:17)

u∈N (v)
(cid:69)

φ(l−1)
G,σ (v),

(cid:88)

w(l,j)

ik ψ(l−1)

σ,k,n

+ λ

(cid:88)

(cid:68)
φ(l,j−1)
G,σ

(u), ψσ(ψ(l)

i,j−1)

(cid:69)

φ(l−1)
G,σ (v),

(cid:88)

w(l,j)

ik ψ(l−1)

σ,k,n

φ(l,j−1)
G,σ

(u),

λψσ(ψ(l)

i,j−1)

k
(cid:42)

(cid:42)

=

=

=

=

=

k

k

(cid:42)

φ(l−1)

G,σ (v),

(cid:68)

G (v), ψ(l)
φ(l,j)

i,j

(cid:88)

u∈N (v)

√

λ

(cid:69)

u∈N (v)

(cid:43)

u∈N (v)

(cid:43)

(cid:42)√

+

λ

(cid:88)



u∈N (v)
(cid:34)

(cid:88)

k

φ(l,j−1)
G,σ

(u)

 ,

w(l,j)

ik ψ(l−1)
σ,k,n ,

λψσ(ψ(l)

i,j−1)

√

√

(cid:43)

(cid:35)(cid:43)

Let ψ(l)

i,j =

k w(l,j)

ik ψ(l−1)
σ,k,n ,

(cid:104)(cid:80)

√

λψσ(ψ(l)

(cid:105)
i,j−1)

concludes the proof.

Proof for (iii): We construct a directed chain Ln,k = (V, E) from model parameters, with nodes V =
{ln, ln−1, · · · , l0} and E = {(vi+1, vi)}. lj’s underlying mapping is ψ(L)

σ,i,j. Now we have

hG[i] =

h(L)[v][i] =

(cid:88)

v

(cid:88)

v∈G

(cid:68)
φσ(φ(L)

G (v)), ψ(L)

σ,i,n

(cid:69)

=

(cid:88)

(cid:88)

K(L,n)
loc

(v, ln) = K(L,n)(G, Ln,k)

v∈G

v∈Ln,k

Note that we are utilizing the fact that K(L,n)

loc

(v, lj) = 0 for all j (cid:54)= n (because Pn(Ln,j, lj) = ∅).

Deriving Neural Architectures from Sequence and Graph Kernels

7
1
0
2
 
t
c
O
 
0
3
 
 
]
E
N
.
s
c
[
 
 
3
v
7
3
0
9
0
.
5
0
7
1
:
v
i
X
r
a

Tao Lei* 1 Wengong Jin* 1 Regina Barzilay 1 Tommi Jaakkola 1

Abstract

The design of neural architectures for structured
objects is typically guided by experimental in-
sights rather than a formal process. In this work,
we appeal to kernels over combinatorial struc-
tures, such as sequences and graphs, to derive
appropriate neural operations. We introduce a
class of deep recurrent neural operations and for-
mally characterize their associated kernel spaces.
Our recurrent modules compare the input to vir-
tual reference objects (cf. ﬁlters in CNN) via the
kernels. Similar to traditional neural operations,
these reference objects are parameterized and di-
rectly optimized in end-to-end training. We em-
pirically evaluate the proposed class of neural ar-
chitectures on standard applications such as lan-
guage modeling and molecular graph regression,
achieving state-of-the-art results across these ap-
plications.

1. Introduction

Many recent studies focus on designing novel neural ar-
chitectures for structured data such as sequences or anno-
tated graphs. For instance, LSTM (Hochreiter & Schmid-
huber, 1997), GRU (Chung et al., 2014) and other complex
recurrent units (Zoph & Le, 2016) can be easily adapted
to embed structured objects such as sentences (Tai et al.,
2015) or molecules (Li et al., 2015; Dai et al., 2016) into
vector spaces suitable for later processing by standard pre-
dictive methods. The embedding algorithms are typically
integrated into an end-to-end trainable architecture so as to
tailor the learnable embeddings directly to the task at hand.

The embedding process itself is characterized by a se-
quence operations summarized in a structure known as
the computational graph. Each node in the computational

*Equal contribution

Intelligence Laboratory.

tiﬁcial
Tao Lei <taolei@csail.mit.edu>, Wengong
gong@csail.mit.edu>.

1MIT Computer Science & Ar-
to:
Jin <wen-

Correspondence

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

graph identiﬁes the unit/mapping applied while the arcs
specify the relative arrangement/order of operations. The
process of designing such computational graphs or asso-
ciated operations for classes of objects is often guided by
insights and expertise rather than a formal process.

Recent work has substantially narrowed the gap between
desirable computational operations associated with objects
and how their representations are acquired. For example,
value iteration calculations can be folded into convolutional
architectures so as to optimize the representations to fa-
cilitate planning (Tamar et al., 2016). Similarly, inference
calculations in graphical models about latent states of vari-
ables such as atom characteristics can be directly associated
with embedding operations (Dai et al., 2016).

We appeal to kernels over combinatorial structures to de-
ﬁne the appropriate computational operations. Kernels give
rise to well-deﬁned function spaces and possess rules of
composition that guide how they can be built from simpler
ones. The comparison of objects inherent in kernels is of-
ten broken down to elementary relations such as counting
of common sub-structures as in

K(χ, χ(cid:48)) =

1[s ∈ χ]1[s ∈ χ(cid:48)]

(1)

(cid:88)

s∈S

where S is the set of possible substructures. For exam-
ple, in a string kernel (Lodhi et al., 2002), S may refer
to all possible subsequences while a graph kernel (Vish-
wanathan et al., 2010) would deal with possible paths
in the graph. Several studies have highlighted the rela-
tion between feed-forward neural architectures and ker-
nels (Hazan & Jaakkola, 2015; Zhang et al., 2016) but we
are unaware of any prior work pertaining to kernels associ-
ated with neural architectures for structured objects.

In this paper, we introduce a class of deep recurrent neural
embedding operations and formally characterize their asso-
ciated kernel spaces. The resulting kernels are parameter-
ized in the sense that the neural operations relate objects of
interest to virtual reference objects through kernels. These
reference objects are parameterized and readily optimized
for end-to-end performance.

To summarize, the proposed neural architectures, or Kernel
Neural Networks 1 , enjoy the following advantages:

1Code available at https://github.com/taolei87/icml17 knn

Deriving Neural Architectures from Sequence and Graph Kernels

• The architecture design is grounded in kernel compu-

• Our neural models remain end-to-end trainable to the

tations.

task at hand.

• Resulting architectures demonstrate state-of-the-art

performance against strong baselines.

In the following sections, we will introduce these neural
components derived from string and graph kernels, as well
as their deep versions. Due to space limitations, we defer
proofs to supplementary material.

Figure 1. An unrolled view of the derived recurrent module for
K2(). Horizontal lines denote decayed propagation from c[t − 1]
to c[t], while vertical lines represent a linear mapping Wxt that
is propagated to the internal states c[t].

2. From String Kernels to Sequence NNs

λx,i,j λy,k,l (cid:104)xi ⊗ xj, yk ⊗ yl(cid:105)

Notations We deﬁne a sequence (or a string) of tokens
i=1 where xi ∈ Rd repre-
(e.g. a sentence) as x1:L = {xi}L
sents its ith element and |x| = L denotes the length. When-
ever it is clear from the context, we will omit the subscript
and directly use x (and y) to denote a sequence. For a pair
of vectors (or matrices) u, v, we denote (cid:104)u, v(cid:105) = (cid:80)
k ukvk
as their inner product. For a kernel function Ki(·, ·) with
subscript i, we use φi(·) to denote its underlying mapping,
i.e. Ki(x, y) = (cid:104)φi(x), φi(y)(cid:105) = φi(x)(cid:62)φi(y).

String Kernel String kernel measures the similarity be-
tween two sequences by counting shared subsequences
(see Lodhi et al. (2002)). For example, let x and y be two
strings, a bi-gram string kernel K2(x, y) counts the num-
ber of bi-grams (xi, xj) and (yk, yl) such that (xi, xj) =
(yk, yl)2,

K2(x, y) =

λx,i,j λy,k,l δ(xi, yk) · δ(xj, yl)

(cid:88)

1≤i<j≤|x|
1≤k<l≤|y|

(2)
where λx,i,j, λy,k,l ∈ [0, 1) are context-dependent weights
and δ(x, y) is an indicator that returns 1 only when x = y.
The weight factors can be realized in various ways. For
instance, in temporal predictions such as language model-
ing, substrings (i.e. patterns) which appear later may have
higher impact for prediction. Thus a realization λx,i,j =
λ|x|−i−1 and λy,k,l = λ|y|−k−1 (penalizing substrings far
from the end) can be used to determine weights given a
constant decay factor λ ∈ (0, 1).

In our case, each token in the sequence is a vector (such as
one-hot encoding of a word or a feature vector). We shall
replace the exact match δ(u, v) by the inner product (cid:104)u, v(cid:105).
To this end, the kernel function (2) can be rewritten as,

(cid:88)

(cid:88)

1≤i<j≤|x|

1≤k<l≤|y|

λx,i,j λy,k,l (cid:104)xi, yk(cid:105) · (cid:104)xj, yl(cid:105)

2We deﬁne n-gram as a subsequence of original string (not

necessarily consecutive).

(cid:88)

(cid:88)

1≤i<j≤|x|
(cid:42)

1≤k<l≤|y|

=

=

(cid:88)

i<j

λ|x|−i−1 xi ⊗ xj,

λ|y|−k−1 yk ⊗ yl

(cid:88)

k<l

(cid:43)

(3)

where xi ⊗ xj ∈ Rd×d (and similarly yk ⊗ yl) is the outer-
product. In other words, the underlying mapping of kernel
K2() deﬁned above is φ2(x) = (cid:80)
1≤i<j≤|x| λ|x|−i−1xi ⊗
xj. Note we could alternatively use a partial additive scor-
ing (cid:104)xi, yk(cid:105) + (cid:104)xj, yl(cid:105), and the kernel function can be gen-
eralized to n-grams when n (cid:54)= 2. Again, we commit to one
realization in this section.

String Kernel NNs We introduce a class of recurrent mod-
ules whose internal feature states embed the computation of
string kernels. The modules project kernel mapping φ(x)
into multi-dimensional vector space (i.e. internal states of
recurrent nets). Owing to the combinatorial structure of
φ(x), such projection can be realized and factorized via
efﬁcient computation. For the example kernel discussed
above, the corresponding neural component is realized as,

c1[t] = λ · c1[t − 1] +

cj[t] = λ · cj[t − 1] +

cj−1[t − 1] (cid:12) W(j)xt

(cid:17)

(cid:17)

W(1)xt

(cid:16)

(cid:16)

h[t] = σ(cn[t]),

1 < j ≤ n

(4)

where cj[t] are the pre-activation cell states at word xt, and
h[t] is the (post-activation) hidden vector. cj[0] is initial-
ized with a zero vector. W(1), .., W(n) are weight matrices
to be learned from training examples.

The network operates like other RNNs by processing each
input token and updating the internal states. The element-
wise multiplication (cid:12) can be replaced by addition + (cor-
responding to the partial additive scoring above). As a spe-
cial case, the additive variant becomes a word-level convo-
lutional neural net (Kim, 2014) when λ = 0.3

3h[t] = σ(W(1)xt−n+1 + · · · + W(n)xt) when λ = 0.

Deriving Neural Architectures from Sequence and Graph Kernels

2.1. Single Layer as Kernel Computation

Now we state how the proposed class embeds string kernel
computation. For j ∈ {1, .., n}, let cj[t][i] be the i-th entry
of state vector cj[t], w(j)
represents the i-th row of ma-
, ..., w(j)
trix W(j). Deﬁne wi,j = {w(1)
i } as a “ref-
i
erence sequence” constructed by taking the i-th row from
each matrix W(1), .., W(j).

, w(2)
i

i

Theorem 1. Let x1:t be the preﬁx of x consisting of ﬁrst
t tokens, and Kj be the string kernel of j-gram shown in
Eq.(3). Then cj[t][i] evaluates kernel function,

cj[t][i] = Kj (x1:t, wi,j) = (cid:104)φj(x1:t), φj(wi,j)(cid:105)

for any j ∈ {1, .., n}, t ∈ {1, .., |x|}.

In other words, the network embeds sequence similarity
computation by assessing the similarity between the input
sequence x1:t and the reference sequence wi,j. This in-
terpretation is similar to that of CNNs, where each ﬁlter
is a “reference pattern” to search in the input. String ker-
nel NN further takes non-consecutive n-gram patterns into
consideration (seen from the summation over all n-grams
in Eq.(3)).

Applying Non-linear Activation In practice, a non-linear
activation function such as polynomial or sigmoid-like ac-
tivation is added to the internal states to produce the ﬁ-
nal output state h[t].
It turns out that many activations
are also functions in the reproducing kernel Hilbert space
(RKHS) of certain kernel functions (see Shalev-Shwartz
et al. (2011); Zhang et al. (2016)). When this is true, the
underlying kernel of h[t] is the composition of string ker-
nel and the kernel containing the activation. We give the
formal statements below.

Lemma 1. Let x and w be multi-dimensional vectors with
ﬁnite norm. Consider the function f (x) := σ(w(cid:62)x) with
non-linear activation σ(·). For functions such as polyno-
mials and sigmoid function, there exists kernel functions
Kσ(·, ·) and the underlying mapping φσ(·) such that f (x)
is in the reproducing kernel Hilbert space of Kσ(·, ·), i.e.,

f (x) = σ(w(cid:62)x) = (cid:104)φσ(x), ψ(w)(cid:105)

for some mapping ψ(w) constructed from w. In particular,
2−(cid:104)x,y(cid:105) for
Kσ(x, y) can be the inverse-polynomial kernel
the above activations.

1

Proposition 1. For one layer string kernel NN with non-
linear activation σ(·) discussed in Lemma 1, h[t][i] as a
function of input x belongs to the RKHS introduced by the
composition of Kσ(·, ·) and string kernel Kn(·, ·). Here a
kernel composition Kσ,n(x, y) is deﬁned with the under-
lying mapping x (cid:55)→ φσ(φn(x)), and hence Kσ,n(x, y) =
φσ(φn(x))(cid:62)φσ(φn(y)).

Proposition 1 is the corollary of Lemma 1 and Theo-
rem 1, since h[t][i] = σ(cn[t][i]) = σ(Kn(x1:t, wi,j)) =
(cid:104)φσ(φn(x1:t)), ˜wi,j(cid:105) and φσ(φn(·)) is the mapping for the
composed kernel. The same proof applies when h[t] is a
linear combination of all ci[t] since kernel functions are
closed under addition.

2.2. Deep Networks as Deep Kernel Construction

We now address the case when multiple layers of the same
module are stacked to construct deeper networks. That is,
the output states h(l)[t] of the l-th layer are fed to the (l+1)-
th layer as the input sequence. We show that layer stacking
corresponds to recursive kernel construction (i.e. (l + 1)-
th kernel is deﬁned on top of l-th kernel), which has been
proven for feed-forward networks (Zhang et al., 2016).

We ﬁrst generalize the sequence kernel deﬁnition to enable
recursive construction. Notice that the deﬁnition in Eq.(3)
uses the linear kernel (inner product) (cid:104)xi, yk(cid:105) as a “subrou-
tine” to measure the similarity between substructures (e.g.
tokens) within the sequences. We can therefore replace it
with other similarity measures introduced by other “base
In particular, let K(1)(·, ·) be the string kernel
kernels”.
(associated with a single layer). The generalized sequence
kernel K(l+1)(x, y) can be recursively deﬁned as,

λx,i,j λy,k,l K(l)

σ (x1:i, y1:k) K(l)

σ (x1:j, y1:l) =

σ (x1:i) ⊗ φ(l)
φ(l)

σ (x1:j),

σ (y1:k) ⊗ φ(l)
φ(l)

σ (y1:l)

(cid:88)

k<l

(cid:43)

where φ(l)(·) denotes the pre-activation mapping of the l-
th kernel, φ(l)
σ (·) = φσ(φ(l)(·)) denotes the underlying
(post-activation) mapping for non-linear activation σ(·),
and K(l)
σ (·, ·) is the l-th post-activation kernel. Based on
this deﬁnition, a deeper model can also be interpreted as a
kernel computation.

Theorem 2. Consider a deep string kernel NN with L
layers and activation function σ(·). Let the ﬁnal output
state h(L)[t] = σ(c(L)
n [t]) (or any linear combination of
{c(l)
i

[t]}, i = 1, .., n). For l = 1, · · · , L,

(i) c(l)

n [t][i] as a function of input x belongs to the RKHS
of kernel K(l)(·, ·);

(ii) h(l)[t][i] belongs to the RKHS of kernel K(l)

σ (·, ·).

(cid:88)

i<j
k<l
(cid:42)

(cid:88)

i<j

3. From Graph Kernels to Graph NNs

In the previous section, we encode sequence kernel com-
putation into neural modules and demonstrate possible ex-
tensions using different base kernels. The same ideas apply

Deriving Neural Architectures from Sequence and Graph Kernels

to other types of kernels and data. Speciﬁcally, we derive
neural components for graphs in this section.

Notations A graph is deﬁned as G = (V, E), with each
vertex v ∈ V associated with feature vector fv. The neigh-
bor of node v is denoted as N (v). Following previous
notations, for any kernel function K∗(·, ·) with underly-
ing mapping φ∗(·), we use K∗,σ(·, ·) to denote the post-
activation kernel induced from the composed underlying
mapping φ∗,σ = φσ(φ∗(·)).

3.1. Random Walk Kernel NNs

We start from random walk graph kernels (G¨artner et al.,
2003), which count common walks in two graphs. For-
mally, let Pn(G) be the set of walks x = x1 · · · xn, where
∀i : (xi, xi+1) ∈ E.4 Given two graphs G and G(cid:48), an n-th
order random walk graph kernel is deﬁned as:

W (G, G(cid:48)) = λn−1 (cid:88)
Kn

(cid:88)

n
(cid:89)

(cid:104)fxi , fyi (cid:105)

(5)

x∈Pn(G)

y∈Pn(G(cid:48))

i=1

where fxi ∈ Rd is the feature vector of node xi in the walk.
Now we show how to realize the above graph kernel with
a neural module. Given a graph G, the proposed neural
module is:

c1[v] = W(1)fv
(cid:88)
cj[v] = λ

cj−1[u] (cid:12) W(j)fv

(6)

hG = σ(

cn[v])

1 < j ≤ n

u∈N (v)
(cid:88)

v

where again c∗[v] is the cell state vector of node v, and hG
is the representation of graph G aggregated from node vec-
tors. hG could then be used for classiﬁcation or regression.

Now we show the proposed model embeds the random
walk kernel. To show this, construct Ln,k as a “refer-
ence walk” consisting of the row vectors {w(1)
k , · · · , w(n)
k }
from the parameter matrices. Here Ln,k = (LV , LE),
where LV = {v0, v1, · · · , vn}, LE = {(vi, vi+1)} and vi’s
feature vector is w(i)

k . We have the following theorem:

Theorem 3. For any n ≥ 1, the state value cn[v][k] (the
k-th coordinate of cn[v]) satisﬁes:

(cid:88)

v

cn[v][k] = Kn

W (G, Ln,k)

thus (cid:80)
lary, hG lies in the RKHS of kernel K n

v cn[v] lies in the RKHS of kernel Kn
W,σ().

W . As a corol-

4A single node could appear multiple times in a walk.

3.2. Uniﬁed View of Graph Kernels

The derivation of the above neural module could be ex-
tended to other classes of graph kernels, such as subtree
kernels (cf. (Ramon & G¨artner, 2003; Vishwanathan et al.,
2010)). Generally speaking, most of these kernel functions
factorize graphs into local sub-structures, i.e.

K(G, G(cid:48)) =

Kloc(v, v(cid:48))

(7)

(cid:88)

(cid:88)

v

v(cid:48)

where Kloc(v, v(cid:48)) measures the similarity between local
sub-structures centered at node v and v(cid:48).

For example, the random walk kernel K n
lently deﬁned with Kn

loc(v, v(cid:48)) =

W can be equiva-






(cid:104)fv, fv(cid:48)(cid:105)
(cid:104)fv, fv(cid:48)(cid:105) · λ (cid:80)

(cid:80)
u(cid:48)∈N (v(cid:48))

Kn−1

loc (u, u(cid:48))

u∈N (v)

if n = 1

if n > 1

Other kernels like subtree kernels could be recursively de-
ﬁned similarly. Therefore, we adopt this uniﬁed view of
graph kernels for the rest of this paper.

In addition, this deﬁnition of random walk kernel could be
further generalized and enhanced by aggregating neighbor
features non-linearly:

Kn

loc(v, v(cid:48)) = (cid:104)fv, fv(cid:48)(cid:105) ◦ λ

(cid:88)

(cid:88)

Kn−1

loc,σ(u, u(cid:48))

u∈N (v)

u(cid:48)∈N (v(cid:48))

where ◦ could be either multiplication or addition. σ(·)
denotes a non-linear activation and Kn−1
loc,σ(·, ·) denotes the
post-activation kernel when σ(·) is involved. The general-
ized kernel could be realized by modifying Eq.(6) into:

cj[v] = W(j)fv ◦ λ

σ(cj−1[u])

(8)

(cid:88)

u∈N (v)

where ◦ could be either + or (cid:12) operation.

3.3. Deep Graph Kernels and NNs

Following Section 2, we could stack multiple graph kernel
NNs to form a deep network. That is:

c(l)
1 [v] = W(l,1)h(l−1)[v]
c(l)
j [v] = W(l,j)h(l−1)[v] ◦ λ

(cid:88)

(cid:16)

(cid:17)
c(l)
j−1[u]

σ

u∈N (v)

h(l)[v] = σ(U(l)c(l)

n [v])

1 ≤ l ≤ L, 1 < j ≤ n

function is recursively deﬁned in
The local kernel
depth (term h(l)) and width (term
two dimensions:
cj). Let the pre-activation kernel in the l-th layer be
K(l)
loc (v, v(cid:48)), and the post-activation ker-
nel be K(l)
loc,σ(v, v(cid:48)). We recursively deﬁne

loc,σ(v, v(cid:48)) = K(l,n)

loc(v, v(cid:48)) = K(l,n)

Deriving Neural Architectures from Sequence and Graph Kernels

K(l,j)



loc (v, v(cid:48)) =
K(l−1)
loc,σ (v, v(cid:48))
loc,σ (v, v(cid:48)) ◦ λ (cid:80)
K(l−1)



(cid:80)
u(cid:48)∈N (v(cid:48))

K(l,j−1)

loc,σ (u, u(cid:48))

u∈N (v)

if j = 1

if j > 1

loc

Finally,

v,v(cid:48) K(L,n)

is
the graph kernel
(v, v(cid:48)). Similar to Theo-

for j = 1, · · · , n.
K(L,n)(G, G(cid:48)) = (cid:80)
rem 2, we have
Theorem 4. Consider a deep graph kernel NN with L lay-
ers and activation function σ(·). Let the ﬁnal output state
hG = (cid:80)
v h(L)[v]. For l = 1, · · · , L; j = 1, · · · , n:
(i) c(l)

j [v][i] as a function of input v and graph G belongs
to the RKHS of kernel K(l,j)
loc (·, ·);

(ii) h(l)[v][i] belongs to the RKHS of kernel K(l,n)
(iii) hG[i] belongs to the RKHS of kernel K(L,n)(·, ·).

loc,σ(·, ·).

3.4. Connection to Weisfeiler-Lehman Kernel

We derived the above deep kernel NN for the purpose of
generality. This model could be simpliﬁed by setting n =
2, without losing representational power (as non-linearity
is already involved in depth dimension). In this case, we
rewrite the network by reparametrization:

h(l)

v = σ

1 h(l−1)
v

◦ U(l)
2

(cid:88)

(cid:16)

σ

V(l)h(l−1)
u


U(l)

(cid:17)





u∈N (v)

(9)
In this section, we further show that this model could be en-
hanced by sharing weight matrices U and V across layers.
This parameter tying mechanism allows our model to em-
bed Weisfeiler-Lehman kernel (Shervashidze et al., 2011).
For clarity, we brieﬂy review basic concepts of Weisfeiler-
Lehman kernel below.

Weisfeiler-Lehman Graph Relabeling Weisfeiler-
Lehman kernel borrows concepts from the Weisfeiler-
Lehman isomorphism test for labeled graphs. The key idea
of the algorithm is to augment the node labels by the sorted
set of node labels of neighbor nodes, and compress these
augmented labels into new, short labels (Figure 2). Such
relabeling process is repeated T times. In the i-th iteration,
it generates a new labeling li(v) for all nodes v in graph G,
with initial labeling l0.

Generalized Graph Relabeling The key observation here
is that graph relabeling operation could be viewed as neigh-
bor feature aggregation. As a result, the relabeling process
naturally generalizes to the case where nodes are associated
with continuous feature vectors. In particular, let r be the
relabeling function. For a node v ∈ G:
(cid:88)

r(v) = σ(U1fv + U2

σ(Vfu))

(10)

u∈N (v)

Figure 2. Node relabeling in Weisfeiler-Lehman isomorphism
test. Figure taken from Shervashidze et al. (2011)

Note that our deﬁnition of r(v) is exactly the same as hv in
Equation 9, with ◦ being additive composition.

Weisfeiler-Lehman Kernel Let K be any graph ker-
nel (called base kernel). Given a relabeling function r,
Weisfeiler-Lehman kernel with base kernel K and depth L
is deﬁned as

L
(cid:88)

K(L)

W L(G, G(cid:48)) =

K(ri(G), ri(G(cid:48)))

(11)

i=0
where r0(G) = G and ri(G), ri(G(cid:48)) are the i-th relabeled
graph of G and G(cid:48) respectively.

Weisfeiler-Lehman Kernel NN Now with the above ker-
nel deﬁnition, and random walk kernel as the base kernel,
we propose the following recurrent module:

c(l)
0 [v] = W(l,0)h(l−1)
v
c(l)
c(l)
j−1[u] (cid:12) W(l,j)h(l−1)
j [v] = λ

(cid:88)

v

u∈N (v)

U1h(l−1)
v

h(l)

v = σ

h(l)

G =

(cid:88)

v

c(l)
n [v]



(cid:88)

+ U2

σ(Vh(l−1)
u

)


u∈N (v)

1 ≤ l ≤ L, 1 < j ≤ n

where h(0)
The ﬁnal output of this network is hG = (cid:80)L

v = fv and U1, U2, V are shared across layers.

l=1 h(l)
G .

The above recurrent module is still an instance of deep ker-
nel, even though some parameters are shared. A minor dif-
ference here is that there is an additional random walk ker-
nel NN that connects i-th layer and the output layer. But
this is just a linear combination of L deep random walk
kernels (of different depth). Therefore, as an corollary of
Theorem 4, we have:
Proposition 2. For a Weisfeiler-Lehman Kernel NN with L
iterations and random walk kernel Kn
W as base kernel, the
ﬁnal output state hG = (cid:80)
G belongs to the RKHS of
kernel K(L)
W L(·, ·).

l h(l)

Deriving Neural Architectures from Sequence and Graph Kernels

4. Adaptive Decay with Neural Gates

The sequence and graph kernel (and their neural compo-
nents) discussed so far use a constant decay value λ re-
gardless of the current input. However, this is often not the
case since the importance of the input can vary across the
context or the applications. One extension is to make use of
neural gates that adaptively control the decay factor. Here
we give two illustrative examples:

Gated String Kernel NN By replacing constant decay λ
with a sigmoid gate, we modify our single-layer sequence
module as:

λt = σ(U[xt, ht−1] + b)
(cid:16)

c1[t] = λt (cid:12) c1[t − 1] +

(cid:17)

W(1)xt

cj[t] = λt (cid:12) cj[t − 1] +

cj−1[t − 1] (cid:12) W(j)xt

(cid:16)

(cid:17)

h[t] = σ(cn[t])

1 < j ≤ n

As compared with the original string kernel, now the decay
factor λx,i,j is no longer λ|x|−i−1, but rather an adaptive
value based on current context.

Gated Random Walk Kernel NN Similarly, we could in-
troduce gates so that different walks have different weights:

λu,v = σ(U[fu, fv] + b)
c0[v] = W(0)fv
cj[v] =

(cid:88)

λu,v (cid:12) cj−1[u] (cid:12) W(j)fv

hG = σ(

cn[v])

1 < j ≤ n

u∈N (v)
(cid:88)

v

The underlying kernel of the above gated network becomes

Kn

W (G, G(cid:48)) =

(cid:88)

(cid:88)

n
(cid:89)

x∈Pn(G)

y∈Pn(G(cid:48))

i=1

λxi,yi (cid:104)fxi, fyi(cid:105)

where each path is weighted by different decay weights,
determined by network itself.

5. Related Work

Sequence Networks Considerable effort has gone into de-
signing effective networks for sequence processing. This
includes recurrent modules with the ability to carry per-
sistent memories such as LSTM (Hochreiter & Schmid-
huber, 1997) and GRU (Chung et al., 2014), as well as
non-consecutive convolutional modules (RCNNs, Lei et al.
(2015)), and others. More recently, Zoph & Le (2016) ex-
empliﬁed a reinforcement learning-based search algorithm
to further optimize the design of such recurrent architec-
tures. Our proposed neural networks offer similar state

evolution and feature aggregation functionalities but de-
rive the motivation for the operations involved from well-
established kernel computations over sequences.

Recursive neural networks are alternative architectures to
model hierarchical structures such as syntax trees and logic
forms. For instance, Socher et al. (2013) employs recur-
sive networks for sentence classiﬁcation, where each node
in the dependency tree of the sentence is transformed into
a vector representation. Tai et al. (2015) further proposed
tree-LSTM, which incorporates LSTM-style architectures
as the transformation unit. Dyer et al. (2015; 2016) re-
cently introduced a recursive neural model for transition-
based language modeling and parsing. While not speciﬁ-
cally discussed in the paper, our ideas do extend to similar
neural components for hierarchical objects (e.g. trees).

Graph Networks Most of the current graph neural archi-
tectures perform either convolutional or recurrent opera-
tions on graphs. Duvenaud et al. (2015) developed Neural
Fingerprint for chemical compounds, where each convo-
lution operation is a sum of neighbor node features, fol-
lowed by a linear transformation. Our model differs from
theirs in that our generalized kernels and networks can ag-
gregate neighboring features in a non-linear way. Other ap-
proaches, e.g., Bruna et al. (2013) and Henaff et al. (2015),
rely on graph Laplacian or Fourier transform.

For recurrent architectures, Li et al. (2015) proposed gated
graph neural networks, where neighbor features are ag-
gregated by GRU function. Dai et al. (2016) considers a
different architecture where a graph is viewed as a latent
variable graphical model. Their recurrent model is derived
from Belief Propagation-like algorithms. Our approach is
most closely related to Dai et al. (2016), in terms of neigh-
bor feature aggregation and resulting recurrent architec-
ture. Nonetheless, the focus of this paper is on providing
a framework for how such recurrent networks could be de-
rived from deep graph kernels.

Kernels and Neural Nets Our work follows recent work
demonstrating the connection between neural networks and
kernels (Cho & Saul, 2009; Hazan & Jaakkola, 2015).
For example, Zhang et al. (2016) showed that standard
feedforward neural nets belong to a larger space of recur-
sively constructed kernels (given certain activation func-
tions). Similar results have been made for convolutional
neural nets (Anselmi et al., 2015), and general computa-
tional graphs (Daniely et al., 2016). We extend prior work
to kernels and neural architectures over structured inputs,
in particular, sequences and graphs. Another difference is
how we train the model. While some prior work appeals
to convex optimization through improper learning (Zhang
et al., 2016; Heinemann et al., 2016) (since kernel space is
larger), we use the proposed networks as building blocks in
typical non-convex but ﬂexible neural network training.

Deriving Neural Architectures from Sequence and Graph Kernels

6. Experiments

The left-over question is whether the proposed class of op-
erations, despite its formal characteristics, leads to more
effective architecture exploration and hence improved per-
formance. In this section, we apply the proposed sequence
and graph modules to various tasks and empirically evalu-
ate their performance against other neural network models.
These tasks include language modeling, sentiment classiﬁ-
cation and molecule regression.

6.1. Language Modeling on PTB

Dataset and Setup We use the Penn Tree Bank (PTB) cor-
pus as the benchmark. The dataset contains about 1 million
tokens in total. We use the standard train/development/test
split of this dataset with vocabulary of size 10,000.

Model Conﬁguration Following standard practice, we use
SGD with an initial learning rate of 1.0 and decrease the
learning rate by a constant factor after a certain epoch. We
back-propagate the gradient with an unroll size of 35 and
use dropout (Hinton et al., 2012) as the regularization. Un-
less otherwise speciﬁed, we train 3-layer networks with
n = 1 and normalized adaptive decay.5 Following (Zilly
et al., 2016), we add highway connections (Srivastava et al.,
2015) within each layer:

c(l)[t] = λt (cid:12) c(l)[t − 1] + (1 − λt) (cid:12) (W(l)h(l−1)[t])
h(l)[t] = ft (cid:12) c(l)[t] + (1 − ft) (cid:12) h(l−1)[t]

where h(0)[t] = xt, λt is the gated decay factor and ft is
the transformation gate of highway connections.6

Results Table 1 compares our model with various state-of-
the-art models. Our small model with 5 million parameters
achieves a test perplexity of 73.6, already outperforming
many results achieved using much larger network. By in-
creasing the network size to 20 million, we obtain a test
perplexity of 69.2, with standard dropout. Adding varia-
tional dropout (Gal & Ghahramani, 2016) within the re-
current cells further improves the perplexity to 65.5. Fi-
nally, the model achieves 63.8 perplexity when the recur-
rence depth is increased to 4, being state-of-the-art and on
par with the results reported in (Zilly et al., 2016; Zoph &
Le, 2016). Note that Zilly et al. (2016) uses 10 neural lay-
ers and Zoph & Le (2016) adopts a complex recurrent cell
found by reinforcement learning based search. Our net-
work is architecturally much simpler.

Figure 3 analyzes several variants of our model. Word-
level CNNs are degraded cases (λ = 0) that ignore non-

5See the supplementary sections for a discussion of network

variants.

6We found non-linear activation is no longer necessary when

the highway connection is added.

Table 1: Comparison with state-of-the-art results on PTB.
|θ| denotes the number of parameters. Following recent
work (Press & Wolf, 2016), we share the input and out-
put word embedding matrix. We report the test perplexity
(PPL) of each model. Lower number is better.

Model
LSTM (large) (Zaremba et al., 2014)
Character CNN (Kim et al., 2015)
Variational LSTM (Gal & Ghahramani)
Variational LSTM (Gal & Ghahramani)
Pointer Sentinel-LSTM (Merity et al.)
Variational RHN (Zilly et al., 2016)
Neural Net Search (Zoph & Le, 2016)
Kernel NN (λ = 0.8)
Kernel NN (λ learned as parameter)
Kernel NN (gated λ)
Kernel NN (gated λ)

+ variational dropout
+ variational dropout, 4 RNN layers

|θ|
PPL
66m 78.4
19m 78.9
20m 78.6
66m 73.4
21m 70.9
23m 65.4
25m 64.0
5m 84.3
5m 76.8
5m 73.6
20m 69.2
20m 65.5
20m 63.8

Figure 3. Comparison between kernel NN variants on PTB. |θ| =
5m for all models. Hyper-parameter search is performed for each
variant.

contiguous n-gram patterns. Clearly, this variant performs
worse compared to other recurrent variants with λ > 0.
Moreover, the test perplexity improves from 84.3 to 76.8
when we train the constant decay vector as part of the
model parameters. Finally, the last two variants utilize neu-
ral gates (depending on input xt only or both input xt and
previous state h[t−1]), further improving the performance.

6.2. Sentiment Classiﬁcation

Dataset and Setup We evaluate our model on the sentence
classiﬁcation task. We use the Stanford Sentiment Tree-
bank benchmark (Socher et al., 2013). The dataset con-
sists of 11855 parsed English sentences annotated at both
the root (i.e. sentence) level and the phrase level using 5-
class ﬁne-grained labels. We use the standard split for train-
ing, development and testing. Following previous work, we
also evaluate our model on the binary classiﬁcation variant
of this benchmark, ignoring all neutral sentences.

Deriving Neural Architectures from Sequence and Graph Kernels

Table 2: Classiﬁcation accuracy on Stanford Sentiment
Treebank. Block I: recursive networks; Block II: convo-
lutional or recurrent networks; Block III: other baseline
methods. Higher number is better.

Model
RNN (Socher et al. (2011))
RNTN (Socher et al. (2013))
DRNN (Irsoy & Cardie (2014))
RLSTM (Tai et al. (2015))
DCNN (Kalchbrenner et al. (2014))
CNN-MC (Kim (2014))
Bi-LSTM (Tai et al. (2015))
LSTMN (Cheng et al. (2016))
PVEC (Le & Mikolov (2014))
DAN (Iyyer et al. (2014))
DMN (Kumar et al. (2016))
Kernel NN, λ = 0.5
Kernel NN, gated λ

Fine Binary
43.2
45.7
49.8
51.0
48.5
47.4
49.1
47.9
48.7
48.2
52.1
51.2
53.2

82.4
85.4
86.8
88.0
86.9
88.1
87.5
87.0
87.8
86.8
88.6
88.6
89.9

Following the recent work of DAN (Iyyer et al., 2015) and
RLSTM (Tai et al., 2015), we use the publicly available
300-dimensional GloVe word vectors (Pennington et al.,
2014). Unlike prior work which ﬁne tunes the word vec-
tors, we normalize the vectors (i.e. (cid:107)w(cid:107)2
2 = 1) and ﬁxed
them for simplicity.

Model Conﬁguration Our best model is a 3-layer network
with n = 2 and hidden dimension d = 200. We average
the hidden states h[t] across t, and concatenate the averaged
vectors from the 3 layers as the input of the ﬁnal softmax
layer. The model is optimized with Adam (Kingma & Ba,
2015), and dropout probability of 0.35.

Results Table 2 presents the performance of our model
and other networks. We report the best results achieved
across 5 independent runs. Our best model obtains 53.2%
and 89.9% test accuracies on ﬁne-grained and binary tasks
respectively. Our model with only a constant decay factor
also obtains quite high accuracy, outperforming other base-
line methods shown in the table.

6.3. Molecular Graph Regression

Dataset and Setup We further evaluate our graph NN
models on the Harvard Clean Energy Project benchmark,
which has been used in Dai et al. (2016); Duvenaud et al.
(2015) as their evaluation dataset. This dataset contains 2.3
million candidate molecules, with each molecule labeled
with its power conversion efﬁciency (PCE) value.

We follow exactly the same train-test split as Dai et al.
(2016), and the same re-sampling procedure on the training
data (but not the test data) to make the algorithm put more

Table 3: Experiments on Harvard Clean Energy Project.
We report Root Mean Square Error(RMSE) on test set. The
ﬁrst block lists the results reported in Dai et al. (2016) for
reference. For fair comparison, we reimplemented their
best model so that all models are trained under the same
setup. Results under our setup is reported in second block.

Model (Dai et al., 2016)
Mean Predicator
Weisfeiler-lehman Kernel, degree=3
Weisfeiler-lehman Kernel, degree=6
Embedded Mean Field
Embedded Loopy BP
Under Our Setup
Neural Fingerprint
Embedded Loopy BP
Weisfeiler Kernel NN
Weisfeiler Kernel NN, gated λ

|θ|
RMSE
2.4062
1
0.2040
1.6m
1378m 0.1367
0.1250
0.1m
0.1174
0.1m

0.26m 0.1409
0.26m 0.1065
0.26m 0.1058
0.26m 0.1043

emphasis on molecules with higher PCE values, since the
data is distributed unevenly.

We use the same feature set as in Duvenaud et al. (2015) for
atoms and bonds. Initial atom features include the atoms
element, its degree, the number of attached hydrogens, its
implicit valence, and an aromaticity indicator. The bond
feature is a concatenation of bond type indicator, whether
the bond is conjugated, and whether the bond is in a ring.

Model Conﬁguration Our model is a Weisfeiler-Lehman
NN, with 4 recurrent iterations and n = 2. All models
(including baseline) are optimized with Adam (Kingma &
Ba, 2015), with learning rate decay factor 0.9.

Results
In Table 3, we report the performance of our
model against other baseline methods. Neural Finger-
print (Duvenaud et al., 2015) is a 4-layer convolutional neu-
ral network. Convolution is applied to each atom, which
sums over its neighbors’ hidden state, followed by a lin-
ear transformation and non-linear activation. Embedded
Loopy BP (Dai et al., 2016) is a recurrent architecture, with
4 recurrent iterations. It maintains message vectors for each
atom and bond, and propagates those vectors in a message
passing fashion. Table 3 shows our model achieves state-
of-the-art against various baselines.

7. Conclusion

We proposed a class of deep recurrent neural architectures
and formally characterized its underlying computation us-
ing kernels. By linking kernel and neural operations, we
have a “template” for deriving new families of neural archi-
tectures for sequences and graphs. We hope the theoretical
view of kernel neural networks can be helpful for future
model exploration.

Deriving Neural Architectures from Sequence and Graph Kernels

Acknowledgement

We thank Prof. Le Song for sharing Harvard Clean En-
ergy Project dataset. We also thank Yu Zhang, Vikas Garg,
David Alvarez, Tianxiao Shen, Karthik Narasimhan and
the reviewers for their helpful comments. This work was
supported by the DARPA Make-It program under contract
ARO W911NF-16-2-0023.

References

Anselmi, Fabio, Rosasco, Lorenzo, Tan, Cheston, and Pog-
gio, Tomaso. Deep convolutional networks are hierarchi-
cal kernel machines. preprint arXiv:1508.01084, 2015.

Balduzzi, David and Ghifary, Muhammad. Strongly-typed
In Proceedings of 33th In-
recurrent neural networks.
ternational Conference on Machine Learning (ICML),
2016.

Bruna, Joan, Zaremba, Wojciech, Szlam, Arthur, and Le-
Cun, Yann. Spectral networks and locally connected net-
works on graphs. arXiv preprint arXiv:1312.6203, 2013.

Cheng, Jianpeng, Dong, Li, and Lapata, Mirella. Long
short-term memory networks for machine reading. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pp. 551–562, 2016.

Cho, Youngmin and Saul, Lawrence K. Kernel methods for
deep learning. In Bengio, Y., Schuurmans, D., Lafferty,
J. D., Williams, C. K. I., and Culotta, A. (eds.), Advances
in Neural Information Processing Systems 22, pp. 342–
350. 2009.

Chung, Junyoung, Gulcehre, Caglar, Cho, KyungHyun,
and Bengio, Yoshua. Empirical evaluation of gated re-
current neural networks on sequence modeling. arXiv
preprint arXiv:1412.3555, 2014.

Dai, Hanjun, Dai, Bo, and Song, Le. Discriminative em-
beddings of latent variable models for structured data.
arXiv preprint arXiv:1603.05629, 2016.

Daniely, Amit, Frostig, Roy, and Singer, Yoram. Toward
deeper understanding of neural networks: The power of
initialization and a dual view on expressivity. CoRR,
abs/1602.05897, 2016.

Duvenaud, David K, Maclaurin, Dougal,

Iparraguirre,
Jorge, Bombarell, Rafael, Hirzel, Timothy, Aspuru-
Guzik, Al´an, and Adams, Ryan P. Convolutional net-
works on graphs for learning molecular ﬁngerprints. In
Advances in neural information processing systems, pp.
2224–2232, 2015.

Dyer, Chris, Ballesteros, Miguel, Ling, Wang, Matthews,
Austin, and Smith, Noah A. Transition-based depen-
dency parsing with stack long short-term memory.
In
Proceedings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long Pa-
pers), Beijing, China, July 2015.

Dyer, Chris, Kuncoro, Adhiguna, Ballesteros, Miguel, and
Smith, Noah A. Recurrent neural network grammars. In
Proceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational Lin-
guistics, San Diego, California, June 2016.

Gal, Yarin and Ghahramani, Zoubin. A theoretically
grounded application of dropout in recurrent neural net-
In Advances in Neural Information Processing
works.
Systems 29 (NIPS), 2016.

G¨artner, Thomas, Flach, Peter, and Wrobel, Stefan. On
graph kernels: Hardness results and efﬁcient alterna-
In Learning Theory and Kernel Machines, pp.
tives.
129–143. Springer, 2003.

Greff, Klaus, Srivastava, Rupesh Kumar, Koutn´ık, Jan, Ste-
unebrink, Bas R, and Schmidhuber, J¨urgen. Lstm: A
search space odyssey. arXiv preprint arXiv:1503.04069,
2015.

Hazan, Tamir and Jaakkola, Tommi. Steps toward deep
kernel methods from inﬁnite neural networks. arXiv
preprint arXiv:1508.05133, 2015.

Heinemann, Uri, Livni, Roi, Eban, Elad, Elidan, Gal, and
Globerson, Amir. Improper deep kernels. In Proceed-
ings of the 19th International Conference on Artiﬁcial
Intelligence and Statistics, pp. 1159–1167, 2016.

Henaff, Mikael, Bruna, Joan, and LeCun, Yann. Deep
convolutional networks on graph-structured data. arXiv
preprint arXiv:1506.05163, 2015.

Hinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan R. Improving
neural networks by preventing co-adaptation of feature
detectors. arXiv preprint arXiv:1207.0580, 2012.

Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-
term memory. Neural computation, 9(8):1735–1780,
1997.

Irsoy, Ozan and Cardie, Claire. Deep recursive neural net-
works for compositionality in language. In Advances in
Neural Information Processing Systems, 2014.

Iyyer, Mohit, Boyd-Graber, Jordan, Claudino, Leonardo,
Socher, Richard, and Daum´e III, Hal. A neural net-
work for factoid question answering over paragraphs. In

Deriving Neural Architectures from Sequence and Graph Kernels

Proceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pp. 633–
644, Doha, Qatar, October 2014.

Merity, Stephen, Xiong, Caiming, Bradbury, James, and
Socher, Richard. Pointer sentinel mixture models. arXiv
preprint arXiv:1609.07843, 2016.

Iyyer, Mohit, Manjunatha, Varun, Boyd-Graber, Jordan,
and Daum´e III, Hal. Deep unordered composition rivals
syntactic methods for text classiﬁcation. In Proceedings
of the 53rd Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), 2015.

Kalchbrenner, Nal, Grefenstette, Edward, and Blunsom,
Phil. A convolutional neural network for modelling sen-
tences. In Proceedings of the 52th Annual Meeting of the
Association for Computational Linguistics, 2014.

Kim, Yoon. Convolutional neural networks for sentence
classiﬁcation. In Proceedings of the Empiricial Methods
in Natural Language Processing (EMNLP 2014), 2014.

Kim, Yoon, Jernite, Yacine, Sontag, David, and Rush,
Alexander M. Character-aware neural language mod-
els. Twenty-Ninth AAAI Conference on Artiﬁcial Intelli-
gence, 2015.

Kingma, Diederik P and Ba, Jimmy Lei. Adam: A method
for stochastic optimization. In International Conference
on Learning Representation, 2015.

Kumar, Ankit, Irsoy, Ozan, Ondruska, Peter, Iyyer, Mo-
hit, James Bradbury, Ishaan Gulrajani, Zhong, Victor,
Paulus, Romain, and Socher, Richard. Ask me any-
thing: Dynamic memory networks for natural language
processing. 2016.

Le, Quoc and Mikolov, Tomas. Distributed representations
of sentences and documents. In Proceedings of the 31st
International Conference on Machine Learning (ICML-
14), pp. 1188–1196, 2014.

Lee, Kenton, Levy, Omer, and Zettlemoyer, Luke. Recur-

rent additive networks. Preprint, 2017.

Lei, Tao, Joshi, Hrishikesh, Barzilay, Regina, Jaakkola,
Tommi, Tymoshenko, Katerina, Moschitti, Alessan-
Semi-supervised ques-
dro, and Marquez, Lluis.
tion retrieval with gated convolutions. arXiv preprint
arXiv:1512.05726, 2015.

Li, Yujia, Tarlow, Daniel, Brockschmidt, Marc, and Zemel,
Richard. Gated graph sequence neural networks. arXiv
preprint arXiv:1511.05493, 2015.

Pennington,

Jeffrey, Socher, Richard, and Manning,
Christopher D. Glove: Global vectors for word repre-
sentation. volume 12, 2014.

Press, Oﬁr and Wolf, Lior. Using the output embed-
arXiv preprint

ding to improve language models.
arXiv:1608.05859, 2016.

Ramon, Jan and G¨artner, Thomas. Expressivity versus efﬁ-
ciency of graph kernels. In First international workshop
on mining graphs, trees and sequences, pp. 65–74. Cite-
seer, 2003.

Shalev-Shwartz, Shai, Shamir, Ohad, and Sridharan,
Karthik. Learning kernel-based halfspaces with the 0-
1 loss. SIAM Journal on Computing, 40(6):1623–1646,
2011.

Shervashidze, Nino, Schweitzer, Pascal, Leeuwen, Erik
Jan van, Mehlhorn, Kurt, and Borgwardt, Karsten M.
Weisfeiler-lehman graph kernels. Journal of Machine
Learning Research, 12(Sep):2539–2561, 2011.

Socher, Richard, Pennington, Jeffrey, Huang, Eric H,
Ng, Andrew Y, and Manning, Christopher D. Semi-
supervised recursive autoencoders for predicting senti-
ment distributions. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing, pp.
151–161, 2011.

Socher, Richard, Perelygin, Alex, Wu, Jean, Chuang, Ja-
son, Manning, Christopher D., Ng, Andrew Y., and Potts,
Christopher. Recursive deep models for semantic com-
positionality over a sentiment treebank. In Proceedings
of the 2013 Conference on Empirical Methods in Natural
Language Processing, pp. 1631–1642, October 2013.

Srivastava, Rupesh K, Greff, Klaus, and Schmidhuber,
In Advances in
J¨urgen. Training very deep networks.
neural information processing systems, pp. 2377–2385,
2015.

Tai, Kai Sheng, Socher, Richard, and Manning, Christo-
Improved semantic representations from tree-
pher D.
In Pro-
structured long short-term memory networks.
ceedings of the 53th Annual Meeting of the Association
for Computational Linguistics, 2015.

Lodhi, Huma, Saunders, Craig, Shawe-Taylor, John, Cris-
tianini, Nello, and Watkins, Chris. Text classiﬁcation
using string kernels. Journal of Machine Learning Re-
search, 2(Feb):419–444, 2002.

Tamar, Aviv, Levine, Sergey, Abbeel, Pieter, Wu, Yi, and
Thomas, Garrett. Value iteration networks. In Advances
in Neural Information Processing Systems, pp. 2146–
2154, 2016.

Deriving Neural Architectures from Sequence and Graph Kernels

Vishwanathan, S Vichy N, Schraudolph, Nicol N, Kondor,
Risi, and Borgwardt, Karsten M. Graph kernels. Jour-
nal of Machine Learning Research, 11(Apr):1201–1242,
2010.

Zaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol.
Recurrent neural network regularization. arXiv preprint
arXiv:1409.2329, 2014.

Zhang, Yuchen, Lee, Jason D., and Jordan, Michael I. (cid:96)1-
regularized neural networks are improperly learnable in
In Proceedings of the 33nd Interna-
polynomial time.
tional Conference on Machine Learning, 2016.

Zilly, Julian Georg, Srivastava, Rupesh Kumar, Koutn´ık,
Jan, and Schmidhuber, J¨urgen. Recurrent Highway Net-
works. arXiv preprint arXiv:1607.03474, 2016.

Zoph, Barret and Le, Quoc V.

search with reinforcement learning.
arXiv:1611.01578, 2016.

Neural architecture
arXiv preprint

Supplementary Material

A. Examples of kernel / neural variants

Our theoretical results apply to some other variants of sequence kernels and the associated neural compo-
nents. We give some examples in the this section. Table 4 shows three network variants, corresponding
to three realizations of string kernels provided in Table 5.

Connection to LSTMs
Interestingly, many recent work has reached similar RNN architectures
through empirical exploration. For example, Greff et al. (2015) found that simplifying LSTMs, by re-
moving the input gate or coupling it with the forget gate does not signiﬁcantly change the performance.
However, the forget gate (corresponding to the decay factor λ in our notation) is crucial for performance.
This is consistent with our theoretical analysis and the empirical results in Figure 3. Moreover, Balduzzi
& Ghifary (2016) and Lee et al. (2017) both suggest that a linear additive state computation sufﬁces to
provide competitive performance compared to LSTMs: 7

c[t] = λf (cid:12) c1[t − 1] + λi (cid:12) (Wxt)
h[t] = σ(c[t])

In fact, this variant becomes an instance of the kernel NN presented in this work (with n = 1 and adaptive
gating), when λf = λt and λi = 1 − λt or 1.

7Balduzzi & Ghifary (2016) also includes the previous token , i.e. Wxt + W(cid:48)xt−1, which doesn’t affect the

discussion here.

Table 4: Example sequence NN variants. We present these equations in the context of n = 3.

(a) Multiplicative mapping, aggregation un-normalized:

c1[t] = λ · c1[t − 1] + W(1)xt

c2[t] = λ · c2[t − 1] +

c3[t] = λ · c3[t − 1] +

(cid:16)
c1[t − 1] (cid:12) W(2)xt
(cid:16)
c2[t − 1] (cid:12) W(3)xt

(cid:17)

(cid:17)

(b) Multiplicative mapping, aggregation normalized:

c1[t] = λ · c1[t − 1] + (1 − λ) · W(1)xt

c2[t] = λ · c2[t − 1] + (1 − λ) ·

c3[t] = λ · c3[t − 1] + (1 − λ) ·

(cid:16)
c1[t − 1] (cid:12) W(2)xt
(cid:16)
c2[t − 1] (cid:12) W(3)xt

(cid:17)

(cid:17)

(c) Additive mapping, aggregation normalized:

c1[t] = λ · c1[t − 1] + (1 − λ) · W(1)xt

c2[t] = λ · c2[t − 1] + (1 − λ) ·

c3[t] = λ · c3[t − 1] + (1 − λ) ·

(cid:16)
c1[t − 1] + W(2)xt
(cid:16)
c2[t − 1] + W(3)xt

(cid:17)

(cid:17)

Final activation:

h[t] = σ (c3[t])

h[t] = σ (c1[t] + c2[t] + c3[t])

(any linear combination of c∗[t])

Table 5: Examples of sequence kernel functions and associated mappings. [xi, xj] denotes the concate-
nation of two vectors.

(a) Multiplicative mapping, aggregation un-normalized:

K2(x, y) =

(cid:88)

(cid:88)

λ|x|−i−1 λ|y|−k−1 (cid:104)xi, yk(cid:105) · (cid:104)xj, yl(cid:105)

1≤i<j≤|x|

1≤k<l≤|y|

φ2(x) =

(cid:88)

λ|x|−i−1 xi ⊗ xj

1≤i<j≤|x|

(b) Multiplicative mapping, aggregation normalized:

K2(x, y) =

1
Z

(cid:88)

(cid:88)

λ|x|−i−1 λ|y|−k−1 (cid:104)xi, yk(cid:105) · (cid:104)xj, yl(cid:105)

1≤i<j≤|x|

1≤k<l≤|y|

s.t. Z =

(cid:88)

(cid:88)

λ|x|−i−1 λ|y|−k−1

1≤i<j≤|x|

1≤k<l≤|y|

φ2(x) =

1
Z (cid:48)

(cid:88)

λ|x|−i−1 xi ⊗ xj

1≤i<j≤|x|

s.t. Z (cid:48) =

(cid:88)

λ|x|−i−1

1≤i<j≤|x|

(c) Additive mapping, aggregation normalized:

K2(x, y) =

1
Z

(cid:88)

(cid:88)

1≤i<j≤|x|

1≤k<l≤|y|

λ|x|−i−1 λ|y|−k−1 ((cid:104)xi, yk(cid:105) + (cid:104)xj, yl(cid:105))

s.t. Z =

(cid:88)

(cid:88)

λ|x|−i−1 λ|y|−k−1

1≤i<j≤|x|

1≤k<l≤|y|

φ2(x) =

1
Z (cid:48)

(cid:88)

λ|x|−i−1 [xi, xj]

1≤i<j≤|x|

s.t. Z (cid:48) =

(cid:88)

λ|x|−i−1

1≤i<j≤|x|

B. Proof of Theorem 1

We ﬁrst generalize the kernel deﬁnition in Eq.(3) to the case of any n-gram. For any integer n > 0, the
underlying mapping of the n-th order string kernel is deﬁned as,

φn(x) =

(cid:88)

λ|x|−i1−n+1 xi1 ⊗ xi2 ⊗ · · · ⊗ xin

1≤i1<···<in≤|x|

We now show that String Kernel NN states cn[t] deﬁned in (4),

n = 1 :

n > 1 :

c1[t] = λ · c1[t − 1] + W(1)xt

cn[t] = λ · cn[t − 1] +

cn−1[t − 1] (cid:12) W(n)xt

(cid:16)

(cid:17)

is equivalent to summing over all n-grams within the ﬁrst t tokens x1:t = {x1, · · · , xt},

cn[t] =

(cid:88)

(cid:16)

W(1)xi1 (cid:12) · · · (cid:12) W(n)xin

· λt−i1−n+1

(cid:17)

1≤i1<i2<···<in≤t

Proof: We prove the equivalence by induction. When n = 1, for c1[t] we have,

=

(cid:88)

(cid:16)

(cid:17)

W(1)xi1

· λt−1−i1

·λ + W(1)xt

c1[t] =

(cid:88)

(cid:16)

(cid:17)

W(1)xi1

· λt−i1

1≤i1≤t





(cid:124)

1≤i1≤t−1

(cid:123)(cid:122)
c1[t−1]

= c1[t − 1] · λ + W(1)xt





(cid:125)

When n = k > 1, suppose the state iteration holds for 1, · · · , k − 1, we have,

cn[t] =

(cid:88)

(cid:16)

W(1)xi1 (cid:12) · · · (cid:12) W(n)xin

· λt−i1−n+1

(cid:17)

1≤i1<i2<···<in≤t


(cid:88)

(cid:16)

1≤i1<i2<···<in≤t−1

W(1)xi1 (cid:12) · · · (cid:12) W(n)xin

· λt−1−i1−n+1



·λ

(cid:17)

(cid:123)(cid:122)
when in<t: cn[t−1]

(cid:88)

(cid:16)

W(1)xi1 (cid:12) · · · (cid:12) W(n)xin

· λt−1−i1−n+1

(cid:17)



(cid:125)





(cid:125)

=



(cid:124)





(cid:124)

+

1≤i1<i2<···<in=t

= cn[t − 1] · λ +

cn−1[t − 1] (cid:12) W(n)xt

(cid:16)

(cid:123)(cid:122)
when in=t

(cid:17)

Now we proceed to prove Theorem 1.

Proof: From the above results, we know that for cn[t],

cn[t] =

(cid:88)

(cid:16)

W(1)xi1 (cid:12) · · · (cid:12) W(n)xin

· λt−i1−n+1

(cid:17)

1≤i1<i2<···<in≤t

In particular, the value of the i-th entry, cn[t][i], is equal to,

cn[t][i] =

(cid:88)

(cid:68)
w(1)
i

, xi1

(cid:69)

(cid:68)
w(n)
i

, xin

· λt−i1−n+1

1≤i1<i2<···<in≤t

(cid:124)
(cid:68)

xi1 ⊗···⊗xin ,w(1)

i ⊗···⊗w(n)

i

· · ·
(cid:123)(cid:122)

(cid:69)

(cid:125)
(cid:69)

λt−i1−n+1 xi1 ⊗ · · · ⊗ xin , w(1)

i ⊗ · · · ⊗ w(n)

i

(cid:43)

(cid:42)

=

(cid:88)

1≤i1<i2<···<in≤t

= (cid:104)φn(x1:t), φn(wi,n)(cid:105)

where w(k)
proof since Kn(x1:t, wi,n) = (cid:104)φn(x1:t), φn(wi,n)(cid:105) by deﬁnition.

represents the i-th row of matrix W(k) and wi,n = {w(1)

i

i

, · · · , w(n)

i }. This completes the

Remarks This theorem demonstrates that the NN state vectors cn can be seen as projecting the kernel
mapping φn(x) into a low-dimensional space using the parameterized matrices {W(i)}i = 1n. This
is similar to word embeddings and CNNs, where the input (a single word or an n-gram) is projected to
obtain a low-dimensional representation.

C. Proof of Theorem 2

We ﬁrst review necessary concepts and notations for the ease of reading. Similar to the proof in Ap-
pendix B, the generalized string kernel K(l) and K(l)
σ in Eq.(??) can be deﬁned with the underlying
mappings,

λ|x|−i1−n+1 φ(l−1)

σ

(x1:i1) ⊗ φ(l−1)

σ

(x1:i2) ⊗ · · · ⊗ φ(l−1)

σ

(x1:i1)

φ(l)(x) =

(cid:88)

1≤i1<···<in≤|x|

φ(l)
σ (x) = φσ(φ(l)(x))

where φσ() is the underlying mapping of a kernel function whose reproducing kernel Hilbert space
(RKHS) contains the non-linear activation σ() used in the String Kernel NN layer. Here K(l)() is the
“pre-activation kernel” and K(l)
σ () is the “post-activation kernel”. To show that the values of String
Kernel NN states c(l)[t] is contained in the RKHS of K(l)() and that of h(l)[t] is contained in the RKHS
of K(l)
Theorem 5. Given a deep n-gram String Kernel NN with non-linear activation σ(). Assuming σ() lies
in the RKHS of a kernel function with underlying mapping φσ(), then

σ (), we re-state the claim in the following way,

(i) c(l)[t] lies in the RKHS of kernel K(l)() in the sense that

c(l)
j [t][i] =

(cid:68)
φ(l)(x1:t), ψ(l)
i,j

(cid:69)

for any internal state c(l)
model parameters;

j [t] (1 ≤ j ≤ n) of the l-th layer, where ψ(l)

i,j is a mapping constructed from

(ii) h(l)[t] lies in the RKHS of kernel K(l)

σ () as a corollary in the sense that

h(l)[t][i] = σ(c(l)
(cid:16)(cid:68)

n [t][i])
φ(l)(x1:t), ψ(l)
i,n

(cid:69)(cid:17)

= σ
(cid:68)

=

φσ(φ(l)(x1:t)), ψσ(ψ(l)
i,n)

(cid:69)

(based on (i))

(Lemma 1)

and we denote ψσ(ψ(l)

i,n) as ψ(l)

σ,i,n for short.

Proof: We prove by induction on l. When l = 1, the proof of Theorem 1 already shows that c(1)
(cid:68)
j (x1:t), φ(1)
φ(1)
proof for the case of l = 1.

in a one-layer String Kernel NN. Simply let ψ(1)

[t][i] =
j (wi,j) completes the

i,j = φ(1)

j (wi,j)

(cid:69)

j

Suppose the lemma holds for l = 1, · · · , k, we now prove the case of l = k + 1. Similar to the proof of
Theorem 1, the value of c(k+1)

[t][i] equals to

j

c(k+1)
j

[t][i] =

(cid:88)

(cid:68)
w(1)
i

(cid:69)
, h(k)[i1]

· · ·

(cid:68)

w(j)
i

(cid:69)
, h(k)[ij]

· λt−i1−j+1

(12)

1≤i1<···<ij ≤t

where w(j)
i
(cid:68)
φ(k)
σ (x1:t), ψ(k)
σ,i,n
can rewrite h(k)[t] = M φ(k)

is the i-th row of the parameter matrix W(j) of the l-th layer. Note h(k)[t][i] =
σ,i,n}i as the row vectors.8 We then

(cid:69)
, we construct a matrix M by stacking all {ψ(k)

σ (x1:t). Plugging this into Eq (12), we get

c(k+1)
j

[t][i] =

w(1)
i

, M φ(k)

σ (x1:i1 )

(cid:69)

(cid:68)

· · ·

w(j)
i

, M φ(k)

σ (x1:ij )

· λt−i1−j+1

(cid:69)

1≤i1<···<ij ≤t

(cid:88)

(cid:88)

(cid:68)

(cid:42)

1≤i1<···<ij ≤t

1≤i1<···<ij ≤t

(cid:42)

(cid:88)

1≤i1<···<ij ≤t
(cid:124)

=

=

=

M(cid:62) w(1)
(cid:125)
(cid:123)(cid:122)
(cid:124)
ui,1

i

(cid:43)

(cid:42)

, φ(k)

σ (x1:i1)

· · ·

(cid:43)

σ (x1:ij )

· λt−i1−j+1

M(cid:62) w(j)
, φ(k)
(cid:125)
(cid:123)(cid:122)
(cid:124)
ui,j

i

(cid:88)

(cid:68)

ui,1, φ(k)

σ (x1:i1)

(cid:69)

(cid:68)
ui,j, φ(k)

σ (x1:ij )

(cid:69)

· · ·

· λt−i1−j+1

λt−i1−j+1 φ(k)

σ (xi1 ) ⊗ · · · ⊗ φ(k)

σ (xij )

, ui,1 ⊗ · · · ⊗ ui,j

(cid:43)

(cid:123)(cid:122)
φ(k+1)(x1:t)

(cid:125)

(cid:68)
φ(k+1)(x1:t), ui,1 ⊗ · · · ⊗ ui,j

(cid:69)

=

Deﬁne ψ(k+1)

i,j

= ui,1 ⊗ · · · ⊗ ui,j completes the proof for the case of l = k + 1.

8Note in practice the mappings φ(k)
σ

σ may have inﬁnite dimension because the underlying mapping
for the non-linear activation φσ() can have inﬁnite dimension. The proof still apply since the dimensions are still
countable and the vectors have ﬁnite norm (easy to show this by assuming the input xi and parameter W are
bounded.

and ψ(k)

D. Proof for Theorem 3

Recall that random walk graph kernel is deﬁned as:

Kn(G, G(cid:48)) = λn (cid:88)

(cid:88)

n
(cid:89)

(cid:104)fxi , fyi (cid:105)

x∈Pn(G)

y∈Pn(G(cid:48))

i=1

φn(G) = λn (cid:88)

fx0 ⊗ fx1 ⊗ · · · ⊗ fxn

x∈Pn(G)

and single-layer graph NN computes its states as:

cn[v] = λ

cn−1[u] (cid:12) W(n)fv

(cid:88)

u∈N (v)

= λn (cid:88)

un−1∈N (v)
(cid:88)

= λn

u=u0···un∈Pn(G,v)

(cid:88)

(cid:88)

· · ·

W(0)fu0 (cid:12) W(1)fu1 (cid:12) · · · (cid:12) W(n)fun

un−2∈N (un−1)

u0∈N (u1)

W(0)fu0 (cid:12) W(1)fu1 (cid:12) · · · (cid:12) W(n)fun

where we deﬁne Pn(G, v) be the set of walks that end with node v. For the i-th coordinate of cn[v], we
have

cn[v][i] = λn (cid:88)

(cid:89)

(cid:68)

(cid:69)

w(i)
i

, fui

(cid:88)

cn[v][i] = λn (cid:88)

(cid:68)
w(i)
i

, fui

(cid:69)

v

u∈Pn(G,v)

i
(cid:89)

u∈Pn(G)

i

= Kn(G, Ln,k)

The last step relies on the fact that Pn(Ln,k) has only one element {w(0)

, w(1)
i

, · · · , w(n)

i }.

i

E. Proof for Theorem 4

For clarity, we re-state the kernel deﬁnition and theorem in the following:

K(L,n)(G, G(cid:48)) =

(cid:88)

(cid:88)

v

v(cid:48)

K(L,n)

loc,σ (v, v(cid:48))

When l = 1:





0

When l > 1:

K(l,j)

loc (v, v(cid:48)) =





0

K(l,j)

loc (v, v(cid:48)) =

(cid:104)fv, fv(cid:48)(cid:105)
(cid:104)fv, fv(cid:48)(cid:105) + λ (cid:80)

(cid:80)
u(cid:48)∈N (v(cid:48))

K(l,j−1)

loc,σ (u, u(cid:48))

u∈N (v)

if j = 1

if j > 1

if Pj(G, v) = ∅ or Pj(G(cid:48), v(cid:48)) = ∅

K(l−1)
loc,σ (v, v(cid:48))
loc,σ (v, v(cid:48)) + λ (cid:80)
K(l−1)

(cid:80)
u(cid:48)∈N (v(cid:48))

K(l,j−1)

loc,σ (u, u(cid:48))

u∈N (v)

if j = 1

if j > 1

if Pj(G, v) = ∅ or Pj(G(cid:48), v(cid:48)) = ∅

where Pj(G, v) is the set of walks of length j starting from v. Note that we force K(l,j)
loc = 0 when there
is no valid walk starting from v or v(cid:48), so that it only considers walks of length j. We use additive version
just for illustration (multiplicative version follows the same argument).

Theorem 6. For a deep random walk kernel NN with L layers and activation function σ(·), assuming
the ﬁnal output state hG = (cid:80)
v h(l)[v], for l = 1, · · · , L; j = 1, · · · , n:

(i) c(l)

j [v][i] as a function of input v and graph G belongs to the RKHS of kernel K(l,j)

loc (·, ·) in that

c(l)
j [v][i] =

(cid:68)
G (v), ψ(l)
φ(l,j)

i,j

(cid:69)

where φ(l)
parameters.

G (v) is the mapping of node v in graph G, and ψ(l)

i,j is a mapping constructed from model

(ii) h(l)[v][i] belongs to the RKHS of kernel K(l,n)

loc,σ(·, ·) as a corollary:

h(l)[v][i] = σ

uikc(l)

n [v][k]

= σ

(cid:32)

(cid:88)

k
(cid:32)(cid:42)

(cid:68)

uik

G (v), ψ(l)
φ(l,n)

k,n

(cid:33)

(cid:69)

(cid:33)

(cid:32)

(cid:88)

k
(cid:43)(cid:33)

= σ

φ(l,n)
G (v),

uikψ(l)
k,n

(cid:88)

k

=

φσ(φ(l,n)

G (v)), ψσ

uikψ(l)
k,n

(cid:33)(cid:43)

(cid:32)

(cid:88)

k

(cid:42)

(cid:17)

We denote ψσ

(cid:16)(cid:80)

k uikψ(l)

k,n

as ψ(l)

σ,i,n, and φσ(φ(l,n)

G (v)) as φ(l)

G,σ(v) for short.

(iii) hG[i] belongs to the RKHS of kernel K(L,n)(·, ·).

Proof of (i), (ii): We prove by induction on l. When l = 1, from kernel deﬁnition, the kernel mapping is
recursively deﬁned as:

φ(1,j)
G (v) =

√

fv,

λ (cid:80)

u∈N (v)

if j = 1

(cid:35)

φσ(φ(1,j−1)
G

(u))

if j > 1

fv
(cid:34)






0

if Pj(G, v) = ∅ or Pj(G(cid:48), v(cid:48)) = ∅

j

[v][i] =

(cid:68)
G (v), ψ(1)
We prove c(1)
φ(1,j)
Our hypothesis holds by ψ(1)
i,1 = w(1,1)
Pj(G, v) = ∅, we could always set c(1)

i,j

i

(cid:69)

j

by induction on j. When j = 1, c(1)

1 [v][i] =

(cid:68)
w(1,1)
i

(cid:69)

.

, fv

. Suppose induction hypothesis holds for 1, 2, · · · , j − 1. If

[v][i] = 0 in neural network computation. Otherwise:

c(1)
j

[v][i] =

(cid:68)
w(1,j)
i

, fv

+ λ

(cid:88)

(cid:16)

(cid:17)

c(1)
j−1[u][i]

u∈N (v)
(cid:88)

(cid:16)(cid:68)

σ

σ

φ(1,j−1)
G

(u), ψ(1)

i,j−1

(cid:69)(cid:17)

(cid:69)

(cid:69)

(cid:69)

=

=

=

(cid:68)
w(1,j)
i

, fv

+ λ

(cid:68)
w(1,j)
i

, fv

+

λ

u∈N (v)
(cid:88)

√

u∈N (v)

(cid:42)

√

fv,

λ

(cid:88)

u∈N (v)

(cid:68)
φσ(φ(1,j−1)
G

(u)),

λψσ(ψ(1)

i,j−1)

(cid:69)

√



(cid:104)

φσ(φ(1,j−1)
G

(u))

 ,

w(1,j)
i

,

√

λψσ(ψ(1)

(cid:105)
i,j−1)

(cid:43)

(cid:104)
w(1,j)
i

i,j =

Let ψ(1)
,
for 1, 2, · · · , l − 1. Note that when l > 1:

λψσ(ψ(1)

(cid:105)
i,j−1)

√

concludes the base case l = 1. Suppose induction hypothesis holds

φ(l,j)
G (v) =

φσ(φ(l−1)
G
(cid:34)

(v))

φσ(φ(l−1)
G

(v)),

√

λ (cid:80)

u∈N (v)

if j = 1

(cid:35)

φσ(φ(l,j−1)
G

(u))

if j > 1






0

Now we prove c(l)

j [v][i] =

(cid:68)
G (v), ψ(l)
φ(l,j)

i,j

(cid:69)

by induction on j. When j = 1,

if Pj(G, v) = ∅ or Pj(G(cid:48), v(cid:48)) = ∅

c(l)
1 [v][i] =

(cid:69)

(cid:68)
w(l,1)
i
(cid:88)

, h(l−1)[v]
(cid:68)
G,σ (v), ψ(l−1)
φ(l−1)

σ,k,n

w(l,1)
ik

(cid:69)

φ(l,1)
G (v),

w(l,1)

ik ψ(l−1)

σ,k,n

(cid:88)

k

(cid:43)

=

=

k
(cid:42)

i,j = (cid:80)

Let ψ(l)
Same as before, we only consider the case when Pj(G, v) (cid:54)= ∅:

ik ψ(l−1)

k w(l,1)

σ,k,n completes the proof for j = 1. Now suppose this holds for 1, 2, · · · , j − 1.

c(l)
j [v][i] =

(cid:68)
w(l,j)
i

, h(l−1)[v]

+ λ

(cid:69)

(cid:88)

(cid:16)

σ

c(l)
j−1[u][i]

(cid:17)

(cid:88)

w(l,j)
ik

(cid:68)
G,σ (v), ψ(l−1)
φ(l−1)

σ,k,n

+ λ

(cid:88)

(cid:16)(cid:68)

σ

φ(l,j−1)
G

(u), ψ(l)

i,j−1

(cid:69)(cid:17)

u∈N (v)
(cid:69)

φ(l−1)
G,σ (v),

(cid:88)

w(l,j)

ik ψ(l−1)

σ,k,n

+ λ

(cid:88)

(cid:68)
φ(l,j−1)
G,σ

(u), ψσ(ψ(l)

i,j−1)

(cid:69)

φ(l−1)
G,σ (v),

(cid:88)

w(l,j)

ik ψ(l−1)

σ,k,n

φ(l,j−1)
G,σ

(u),

λψσ(ψ(l)

i,j−1)

k
(cid:42)

(cid:42)

=

=

=

=

=

k

k

(cid:42)

φ(l−1)

G,σ (v),

(cid:68)

G (v), ψ(l)
φ(l,j)

i,j

(cid:88)

u∈N (v)

√

λ

(cid:69)

u∈N (v)

(cid:43)

u∈N (v)

(cid:43)

(cid:42)√

+

λ

(cid:88)



u∈N (v)
(cid:34)

(cid:88)

k

φ(l,j−1)
G,σ

(u)

 ,

w(l,j)

ik ψ(l−1)
σ,k,n ,

λψσ(ψ(l)

i,j−1)

√

√

(cid:43)

(cid:35)(cid:43)

Let ψ(l)

i,j =

k w(l,j)

ik ψ(l−1)
σ,k,n ,

(cid:104)(cid:80)

√

λψσ(ψ(l)

(cid:105)
i,j−1)

concludes the proof.

Proof for (iii): We construct a directed chain Ln,k = (V, E) from model parameters, with nodes V =
{ln, ln−1, · · · , l0} and E = {(vi+1, vi)}. lj’s underlying mapping is ψ(L)

σ,i,j. Now we have

hG[i] =

h(L)[v][i] =

(cid:88)

v

(cid:88)

v∈G

(cid:68)
φσ(φ(L)

G (v)), ψ(L)

σ,i,n

(cid:69)

=

(cid:88)

(cid:88)

K(L,n)
loc

(v, ln) = K(L,n)(G, Ln,k)

v∈G

v∈Ln,k

Note that we are utilizing the fact that K(L,n)

loc

(v, lj) = 0 for all j (cid:54)= n (because Pn(Ln,j, lj) = ∅).

