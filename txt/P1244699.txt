Enhancing Key-Value Memory Neural Networks
for Knowledge Based Question Answering
Kun Xu1, Yuxuan Lai2, Yansong Feng2,3, Zhiguo Wang4
1Tencent AI Lab
2ICST, Peking University
3The MOE Key Laboratory of Computational Linguistics, Peking University
4Amazon
syxu828@gmail.com
{erutan,fengyansong}@pku.edu.cn
zgw.tomorrow@gmail.com

Abstract

Traditional Key-value Memory Neural Net-
works (KV-MemNNs) are proved to be effec-
tive to support shallow reasoning over a col-
lection of documents in domain speciﬁc Ques-
tion Answering or Reading Comprehension
tasks. However, extending KV-MemNNs to
Knowledge Based Question Answering (KB-
QA) is not trivia, which should properly de-
compose a complex question into a sequence
of queries against the memory, and update
the query representations to support multi-hop
reasoning over the memory.
In this paper,
we propose a novel mechanism to enable con-
ventional KV-MemNNs models to perform in-
terpretable reasoning for complex questions.
To achieve this, we design a new query up-
dating strategy to mask previously-addressed
memory information from the query repre-
sentations, and introduce a novel STOP strat-
egy to avoid invalid or repeated memory read-
ing without strong annotation signals. This
also enables KV-MemNNs to produce struc-
tured queries and work in a semantic pars-
ing fashion. Experimental results on bench-
mark datasets show that our solution, trained
with question-answer pairs only, can provide
conventional KV-MemNNs models with better
reasoning abilities on complex questions, and
achieve state-of-art performances.

1

Introduction

Memory Neural Networks (MemNNs) [Weston et
al., 2014; Sukhbaatar et al., 2015b] are a fam-
ily of neural network models that aim to learn
how to reason with a long-term memory com-
ponent and various inference components. The
memory component serves as a knowledge base
to recall facts from the past. MemNNs have
been successfully applied in many natural lan-
guage processing applications such as question

Figure 1: The Key-Value Memory Neural Network Ar-
chitecture.

answering and reading comprehension (RC). Re-
cently, Miller et al. [2016] proposed a variant
of MemNNs, namely Key-Value Memory Neural
Networks (KV-MemNNs), which generalizes the
original MemNNs by storing facts in a key-value
structured memory. Figure 1 illustrates the basic
architecture of KV-MemNNs, which consists of
ﬁve components. The question is ﬁrst fed to the
Embedding component and the Hashing compo-
nent. The former converts the incoming question
to an internal feature representation. The Hashing
component uses the question to pre-select a list of
facts to compose the key-value memory. The Key
Addressing component takes the input question
representation and the current memory to compute
the relevance probability between the question and
each key in the memory. The Value Reading com-
ponent reads the values of all addressed memories
by taking their weighted sum using the relevance
probabilities. The obtained value representation is
then added to the query representation to change
the query focus for the next round of memory
reading. After multiple hops of reasoning over the
memories, the ﬁnal value representation is treated
as the answer representation to perform the ﬁnal
prediction over all candidate answers in the Rank-

ing component.

The KV-MemNNs have been shown to support
shallow reasoning in domain-speciﬁc knowledge
based question answering (KB-QA) tasks such as
MovieQA [Tapaswi et al., 2016]. However, when
applied to a more challenging scenario, e.g., open
domain KB-QA, the KV-MemNNs models do not
perform as well as expected, possibly due to two
reasons. First of all, the focus of conventional KV-
MemNNs is about understanding the facts in the
memory rather than properly understanding the
questions, where the latter requires incrementally
decomposing a complex natural language ques-
tion into a set of focused queries with the help of
the facts in the memory. However, in open do-
main KB-QA, questions are usually more compli-
cated, e.g., multi-relation questions such as who
does maggie grace play in taken, where more than
one entity and relation are mentioned. Secondly,
as shown in Figure 1, KV-MemNNs usually work
in an information retrieval (IR) fashion, which ﬁrst
retrieve a set of candidate answers from KB, then
rank them by computing the similarity between
the value representation and candidates, and ﬁ-
nally select the top one or a ﬁxed number of top
candidates as the answer. We can imagine that it
is not trivial for such IR-styled KV-MemNNs to
properly resolve complex constraints from natu-
ral language questions, or to handle questions with
multiple answers.

We believe that an ideal framework for open do-
main KB-QA should ﬁrst understand the natural
language questions, explicitly represent the mean-
ing, and make the answer retrieval process more
interpretable. To build such an interpretable KV-
MemNN KB-QA model, we need to deal with the
following challenges: (1) KV-MemNNs often read
the memory repeatedly since they do not know
when to stop; (2) during multiple memory read-
ings, conventional KV-MemNNs often fail to pre-
cisely update the queries for multi-relation ques-
tions; (3) strong annotations are usually required
to train an interpretable QA model, e.g., the su-
pervision for the memory selection at each hop.
To address the challenges, we propose a novel so-
lution to make conventional KV-MemNNs feasi-
ble to open domain KB-QA. In particular, we in-
troduce a ﬂexible KV-MemNN solution that can
work in both the IR and semantic parsing style
with large-scale memory. To this end, we ﬁrst
present a novel query updating method that is able

to decompose complex questions and precisely ad-
dress a relevant key at each hop. Secondly, we
introduce a new STOP strategy during memory
readings, which imports a special key STOP into
the memory and guides our model to avoid re-
peated or invalid memory readings.
In addition,
our proposed model can learn to reason over mem-
ory slots with weak supervision, e.g., question-
answer pairs only, opposing the strong supervision
that most current neural semantic parsers demand,
which incurs high labor costs. Experimental re-
sults on two benchmark datasets show that our
proposed model can not only enhance the reason-
ing capability of KV-MemNNs, but also be ﬂex-
ible enough to work as a semantic parser, with
state-of-the-art performances.

2 Related Work

There are usually two main challenges in the open
domain KB-QA task: (1) it often requires the abil-
ity to properly analyze and represent the natural
language questions against knowledge bases, es-
pecially for those involving multiple entities and
relations, which we also call as reasoning over the
KBs; (2) training such interpretable question un-
derstanding models requires considerable strong
annotations, which is expensive to obtain in prac-
tice. Existing works address these using either the
information retrieval (IR) based solutions or the
semantic paring (SP) based approaches. The IR-
based models [Yao and Van Durme, 2014; Yao,
2015; Bast and Haussmann, 2015; Bordes et al.,
2015; Dong et al., 2015; Jain, 2016; Lai et al.,
2019] tackle the KB-QA task by developing var-
ious ranking models towards the candidate an-
swers, which implicitly meet the reasoning re-
quirements during the candidate-searching step or
in designing the ranking functions.
In contrast,
the SP-based approaches [Berant et al., 2013;
Kwiatkowski et al., 2013; Berant and Liang, 2014;
Reddy et al., 2014; Yih et al., 2015; Xu et al.,
2016] explicitly represent the meaning of ques-
tions as logical forms or structured queries that
naturally support reasoning over structured KBs.

More recently, memory based reasoning solu-
tions [Weston et al., 2014; Miller et al., 2016]
[Weston et al.,
are proposed to tackle the task.
2014] proposed the Memory Neural Networks
(MemNNs), which enable the neural network
models read/write on an external memory compo-
nent, and are further extended into an End-to-End
fashion [Sukhbaatar et al., 2015a]. [Miller et al.,

2016] further proposed the Key-Value Memory
Network, which generalizes the MemNN by stor-
ing facts in a key-value structured memory. Both
of the two models could perform shallow reason-
ing over the memory, since they can ﬁnd answers
by consecutively making predictions over multiple
memory slots. Compared to the ﬂat memory slots
in MemNNs, the Key-Value design can precisely
accommodate more complex structured resources,
e.g., discriminating subjects and objects in struc-
tured KBs, thus makes KV-MemNNs more ﬂexi-
ble and better ﬁt to different applications. These
neural networks models are often trained in an
end-to-end fashion, making the models relatively
less interpretable.

Conceptually similar to our STOP strategy,
Shen et al. [2017] propose the termination gate
mechanism based on a random variable gener-
ated from the internal state for reading compre-
hension. In contrast, our model attempts to learn
a general STOP key embedding based on the in-
crementally updated query representations, which
can be learned from question-answer pairs only
and lead to more explicit reasoning interpretations
over structured KBs. This make our model poten-
tially suit more real scenarios.

Our work is also related to [Jain, 2016; Bao et
al., 2016], which are designed to support reason-
ing for multi-relation questions by exploring the
relation path and certain KB schema, e.g., CVT
nodes, in the Freebase. The former also consid-
ers previously-addressed keys during query updat-
ing, but ignores the value representations. Thus, it
still requires predeﬁned rules and threshold to ar-
tiﬁcially add intermediate value representations to
update the query. The latter also relies on a set of
predeﬁned rules to perform reasoning over Free-
base. In contrast, our model incorporates both the
key and value representations into the query rep-
resentations, and update in a more uniform way,
thus is more general and supports more reasoning
scenarios in KB-QA.

3 Our Model

For a given question x, a knowledge base KB and
the question’s answer y, we aim to learn a model
such that

F(x, KB) = ˆy → y

where ˆy is the predicted answer. In standard KV-
MemNNs, the function F can be composed of
ﬁve components, i.e., key hashing, key address-

ing, value reading, query updating and answer pre-
diction. Next, we will introduce how we design a
novel mechanism upon those components to equip
KV-MemNNs with more powerful reasoning abil-
ity. The architecture of our model is shown in Fig-
ure 2.

3.1 Key Hashing

The knowledge facts in KB are usually organized
in a triple <subject, relation, object>, such as
<Maggie Grace, fb:actor character, Kim>.
In
KV-MemNNs, these facts are stored in a key-value
structured memory, where the key k is composed
of the left-hand side entity (subject) and the re-
lation, e.g., Maggie Grace fb:actor character, and
the value v is the right-hand side entity (object),
e.g., Kim. Traditional KV-MemNNs ﬁrst pre-
select a list of candidate KB facts (k1,v1),...,(kn,vn)
to compose the key-value memory. Particularly,
one can ﬁrst detect entity mentions in the ques-
tion, and include all KB facts that contains with
one of those entities as subject into the memory. In
our experiments, we directly use the entity linking
results of Xu et al. [2016] and ﬁlter out their rela-
tions that have more than 100 objects. In practice,
we ﬁnd this strategy could effectively avoid the
memory exploding for popular entities. In order to
help the model avoid repeated or invalid memory
reading, we introduce a special key, STOP, into
the memory for all questions. The corresponding
value of the STOP key is a special symbol rep-
resented by an all-zero vector. The STOP key is
designed to tell our model that we has already ac-
cumulated sufﬁcient facts at hand to answer the
question, so there is no need to ﬁnd other knowl-
edge facts from the memory in later hops.

3.2 Key Addressing & Value Reading

Key addressing is basically a matching process,
aiming to ﬁnd the most suitable key for a given
It can be formulated as a function that
query.
computes the relevance probability pi between the
question x and each key ki:

pi = Softmax(AΦ(x) · AΦ(ki))

where Φ is a feature map of dimension D, A is a
d × D matrix. The values of memories are then
read by taking their weighted sum using the rel-
evance probabilities, and the value representation
o is returned to locate the answers or update the

Figure 2: A running example of our key-value memory network model to answer the question who does maggie
grace play in taken.

query for further memory addressing:

(cid:88)

o =

piAΦ(vi)

i

There are many methods to represent ques-
tions and memory slots, including keys and val-
ues. Here we simply use the Bag-of-Words model
to produce the representations, where we sum the
embedding of each word in the question or mem-
ory slot together to obtain their vector representa-
tions.

3.3 Query Updating

After reading the addressed memory, the initial
query representation q = AΦ(x) should be updated
so that the new evidence o collected in the current
hop can be properly considered to retrieve more
pertinent information in later steps. Traditional
KV-MemNNs simply add the initial query q and
the returned value o, then perform a linear trans-
formation to obtain the new query representation.
This updating strategy is effective in the RC task,
since the questions in RC are relatively simple,
and their main emphasis is to select proper mem-
ory values to change the focus of the query, un-
til reaching the answer. However, the questions
in open domain KB-QA tasks are more compli-
cated, usually involved with multiple relations or
constraints.

Take the question “who does maggie grace play
the expected answer

in taken” as an example,

should follow two constraints: (1) Maggie Grace
plays this answer; and (2) this answer is from
the movie Taken.
To answer this question,
the model needs to perform two hops of in-
ference consecutively,
i.e., matching two keys
Maggie Grace fb:actor..character
and Taken fb:film..character in the
memory. Conventional query updating methods,
e.g., adding q and o, may not be helpful in guiding
the model to predict the other key in later hops,
and possibly even hurt the performance, since
it may introduce unrelated information into the
new query.
in KB-QA, masking
previously-addressed keys from the query could
beneﬁt latter inference, since the model will be
able to focus on the next hop, e.g., the movie
Taken. Therefore, we take into account the query
and addressed memories at the t-th hop when
updating the query qt+1 for the next hop:

Intuitively,

qt+1 = Mt · (qt ⊕

pt
iAΦ(ki)

⊕

pt
iAΦ(vi)

)

(cid:88)

i

(cid:88)

i

(cid:124)
(cid:125)
(cid:123)(cid:122)
addressed key

(cid:124)
(cid:125)
(cid:123)(cid:122)
addressed value

where ⊕ denotes the concatenation of vectors. The
query updating step is parameterized with a differ-
ent matrix Mt on the t-th hop, which is designed
to learn a proper way to combine these three rep-
resentations.

3.4 Answer Prediction

Conventional KV-MemNNs use the value o at the
ﬁnal hop of inference to retrieve the answers, by
simply computing the similarity between o and
all candidate answers. This may be of risk for
our task. First of all, many questions in the open
domain KB-QA have multiple answers, but, KV-
MemNNs are supposed to select the candidate
with the highest similarity score as the only an-
swer. Secondly, the value representation at the ﬁ-
nal step may not fully capture the answer informa-
tion throughout the whole inference process. For
example, for multi-constraint questions, the model
may address different constraints at different hops,
which requires the model to take the value repre-
sentations in every hop into consideration in order
to produce the ﬁnal answer representation from a
global view.

We therefore propose to accumulate the value
representations of all hops to make the resulting
answer representation lean more on satisfying all
constraints. We compute the answer representa-
tion m at each hop by adding the value representa-
tions of both the current hop and the previous one:
mt+1 = ot+1 + ot, m0 = o0.

With the ﬁnal m at hand, we could follow tra-
ditional IR-based methods to use the ﬁnal Answer
Representation to ﬁnd the best match over all pos-
sible candidate values in the memory, namely the
AR approach.
Instead, we can also collect all
the best matched keys at every hop to construct
a Structured Query and execute it over the KB to
obtain all qualiﬁed answers, namely the SQ ap-
proach. Speciﬁcally, the structured query can be
constructed as: we select the keys that have the
highest relevance probabilities in every hop, re-
sulting a sequence of keys sk0,.... Starting from
sk0, we append the key ski into the ﬁnal struc-
tured query until we see the STOP key for the ﬁrst
time at the k-th hop, i.e., SQ = {sk0, ..., skk−1}.
Apparently, the SQ approach can easily output
all qualiﬁed answers through excuting the queries
over the KB, while the AR approach still has
difﬁculties in selecting multiple answers from
a ranked list. However, keep in mind that, we
do not have gold-standard structured queries for
training. As a result, we have to adopt different
strategies to ﬁnd answers in the training and test
phases. During training, after a ﬁxed number
H hops, we follow the AR approach to use the
ﬁnal mH to compute a prediction over possible

candidates, and we train the model by minimizing
the cross-entropy between the prediction and
gold-standard answers. During testing, we follow
the SQ approach to collect
the ﬁnal answers
by constructing and executing the structured
queries over the KB to obtain all answers. As
illustrated in Figure 2, for the example question
who does maggie grace play in taken, our model
[<maggie grace,
i.e.,
selects three keys,
fb:actor..character>,
<taken,
film..character>, and <STOP>]. We
combine the previous two triples to construct the
structured query, which is then executed over the
KB to get the answer. We should point out that
our model could still use the AR approach to
predict the answers just like in the training phase.
And as far as we know, our model is the ﬁrst one
that is suitable to tackle the KB-QA task in both
the information retrieval and semantic parsing
fashions.

3.5 Objective Function and Learning

Given an input question x, the network with pa-
rameters θ uses the answer representation mh
x to
perform the prediction over candidate answers at
hop h, resulting a prediction vector ah
x, where the
i-th component is respect to the probability of can-
didate answer i. We denote tx as the target dis-
tribution vector. We compute the standard cross-
entropy loss between ah
x and tx, and further deﬁne
the objective function over all training data:

L(θ) =

tx log ah

x + λ||θ||2

(cid:88)

H
(cid:88)

x

h=1

where λ is a vector of regularization parameters.
Intuitively, this loss function makes our model
generate shorter paths to reach answers from the
question. On the other hand, it encourages the
query updating method to mask the information
already addressed in previous hops for the next
query representation. This design, together with
the query updating method, are the keys to learn
the STOP strategy.

4 Experiments

We evaluate our model on two benchmark datasets
to investigate whether our enhanced KV-MemNNs
model can better perform reasoning over the mem-
ory in the open domain KB-QA task, and whether
it can make the QA procedure more interpretable.

Semantic Paring Method
Berant et al. [2013]
[Berant and Liang, 2014]
Yih et al. [2015]
Bast and Haussmann [2015]
Xu et al. [2016] (KB Only)
Bao et al. [2016]
CQU + AR (a)
KVQU + AR (b)
CQU + SQ (c)
KVQU + SQ (d)
STOP + CQU + AR (e)
STOP + CQU + SQ (f)
STOP + KVQU + AR (g)
STOP + KVQU + SQ (h)

Answer F1
35.7%
39.9%
52.5%
49.4%
47.1%
54.4%
42.0%
43.2%
39.6%
41.8%
43.3%
45.8%
48.6%
54.6%

Table 1: The performance of different models on the
test set of WebQuestions.

4.1 Settings

We use the WebQuestions dataset [Berant et al.,
2013] as our main dataset, which contains 5,810
question-answer pairs. This dataset is built on
Freebase [Bollacker et al., 2008] and all answers
are Freebase entities. We use the same training,
development and test split as [Berant et al., 2013],
containing 3000, 778 and 2032 questions, respec-
tively.

Our model is trained using the Adam optimizer
[Kingma and Ba, 2014] with mini-batch size 60.
The learning rate is set to 0.001. The complexity
of model was penalized by adding L2 regulariza-
tion to the cross entropy loss function. Gradients
are clipped when their norm is bigger than 20. The
hop size is set to 3. We initialize word embed-
dings using pre-trained word representations from
Turian et al. [2010] and the dimension of word
embedding is set to 50.

We use the average question-wise F1 as our
evaluation metric. We compare our model with
representative IR based KB-QA models, and sev-
eral state-of-the-art semantic parsing models. We
also include different variants of our model for
comparisons to shed light on the advantages of our
proposed strategies, especially our three important
building blocks - the STOP strategy, how to update
a query, and how to obtain the answers.

CQU+AR: uses

the Conventional Query
Updating method (CQU) which performs a lin-
ear transformation over the sum of the query and
value representations. This method adopts the AR
approach to predict answers where the one with
highest probability is selected as the answer.

KVQU+AR: applies the approach introduced
in this paper that additionally considers both
the Key and Value representations in the Query

Updating (KVQU). This method updates the query
representation after reading the memory values at
each hop of inference. Note that this model can
be seen as a variant of Jain [2016], which is essen-
tially an IR-based approach.

CQU+SQ: uses the CQU method to update
the query representations, and applies the SQ ap-
proach to obtain the answers.

KVQU+SQ: uses the KVQU method to update
the query representations, and adopts the SQ ap-
proach to obtain the answers.

STOP+CQU+AR: introduces the STOP key
into the memory, but still uses the conventional
query updating method and answer representa-
tions to ﬁnd the answers.

STOP+CQU+SQ: introduces the STOP key
and uses the conventional query updating method,
but uses the SQ approach to obtain the answers.

STOP+KVQU+AR: introduces the STOP key
to the memory, uses the KVQU approach to up-
date the query representations, and adopts the AR
approach to retrieve the answers.

STOP+KVQU+SQ: is our main model that in-
troduces the STOP key, applies the KVQU query
updating method, and retrieves answers using the
post-constructed structured queries.

4.2 Results and Discussion

Table 1 summarizes the performance of various
methods on the test set of WebQuestions. We
can see that our main model (STOP+KVQU+SQ)
performs the best among all its variations, and sig-
niﬁcantly outperforms the state-of-the-art meth-
ods on WebQuestions (with one-tailed t-test sig-
niﬁcance of p < 0.05). We can also see that
even the conventional KV-MemNN model (i.e.,
model (a)) could still outperform traditional se-
mantic parsing models [Berant et al., 2013; Be-
rant and Liang, 2014], showing the effective-
ness of memory networks in organizing and uti-
lizing structured data. By replacing the CQU
method with our proposed KVQU method, the
KV-MemNN model (i.e., model (b)) could further
gain an improvement of 1.2%, indicating a proper
query representation updating method is critical to
KV-MemNNs. After introducing the STOP strat-
egy, the KV-MemNN model is capable to perform
proper multi-hop reasoning over the memory, thus
outperform most existing methods by a large mar-
gin except Bao et al. [2016].

Previously, Bao et al. [2016] achieves the state-

Question

Addressed Key

who did armie hammer play in the social network

who is the governor of India 2009

what team did david beckham play for in 2011

Armie Hammer fb:award nominations..award nominee
Armie Hammer fb:award nominations..award nominee
Armie Hammer fb:award nominations..award nominee
Armie Hammer fb:actor..character
Social Network fb:ﬁlm..character
STOP
Indiana fb:governing ofﬁcials..ofﬁce holder
Indiana fb:governing ofﬁcials..ofﬁce holder
Indiana fb:governing ofﬁcials..ofﬁce holder
Indiana fb:governing ofﬁcials..ofﬁce holder
Indiana fb:governing ofﬁcials..from
STOP
David Beckham fb:loans..borrowing team
David Beckham fb:player..team
David Beckham fb:player..team
David Beckham fb:player..team
STOP
STOP

before

after

before

after

before

after

Table 2: Running examples of addressed keys and corresponding relevance probabilities before and after introduc-
ing the STOP strategy. The question with blue color is correctly answered by introducing the STOP strategy while
the red ones are still not resolved (see discussion in Session 4.8).

of-the-art performance on the WebQuestions by
explicitly addressing the multi-constraint ques-
tions. Speciﬁcally, Bao et al. [2016] designs a
multi-constraint graph for such questions based
on the Freebase schema, and introduces a set of
predeﬁned rules to solve complex representations
such as max, min, top-X and so on. On the
other hand, our model only requires the KB facts
to be stored in a Key-Value memory, independent
of certain KB schemas. Although those complex
expressions in WebQuestions could be easily cov-
ered by hand-crafted rules as many did, our model
does not work upon such rules, and we think it
is crucial to properly enhance KV-MemNNs with
such more advanced reasoning abilities, which we
leave for future work.
4.3

Impact of the STOP Key

As shown Table 1, we can see that when intro-
ducing the STOP strategy, almost all models im-
prove by around 4%, not only in the SQ setting
(e.g., from (d) to (f)), but also in the AR setting
(e.g., from (a) to (e)). This is not surprising, since
the STOP key is introduced to help our model
learn to determine when it should stop reading the
memory to avoid repeated or invalid addressing.
This apparently will lead to more accurate key ad-
dressing at each hop, and produce more accurate
structured queries. And, better key addressing can
also help to generate better value representations
at each hop, and ﬁnally better answer representa-
tion at the last hop. Keep in mind that we obtain
such improvement in the case that we do not have
gold-standard annotations for the STOP key, but

with question-answer pairs only.

To better investigate how the STOP key works,
we randomly select 200 questions from the test set
and manually analyze the structured queries gen-
erated by our model. We ﬁnd that, for 184 ques-
tions (92%) our model predicts the STOP key af-
ter one hop of inference and continuously predicts
STOP keys in later hops. For the remaining ques-
tions, our model predicts two distinct keys before
predicting the STOP key. Among these 184 ques-
tions, 178 questions (96.7%) can be resolved us-
ing exactly a one-triple query. For the remaining
6 questions, it requires two distinct triples to ﬁnd
the answers. This indicates that our model can
successfully utilize the STOP strategy to avoid re-
peated or invalid reading over the memory, at least,
for simple questions.

For the multi-relation questions which require
at least two hops of inference to ﬁnd the answers,
we evaluate our model on 326 multi-constraint
questions selected in Bao et al. [2016]. We ﬁnd
that for 283 questions (86.8%), our model per-
forms two hops of inference before predicting the
STOP key while for the remaining 13.2%, our
model only performs one hop of inference. This
demonstrates that even without strong annotations
in terms of structured queries, our model still man-
ages to recognize the multi-relation structure and
properly stops the invalid reading process. Table 2
illustrates several examples before and after using
the STOP strategy.

4.4 Query Updating

In KV-MemNNs, it is important to properly up-
date the query representation after each hop, since
the updated query will be used to address more fo-
cused information in the next hop, which is espe-
cially crucial to support multi-hop reasoning over
the memory. We experimented with two query
updating approaches, the traditional adding-based
method, CQU, and our proposed KVQU, which
additionally considers the addressed key represen-
tations in previous hops to update the query rep-
resentation. From Table 1, we can see that no
matter which techniques are used to retrieve the
ﬁnal answers, our model can always beneﬁt from
replacing the CQU with our KVQU. The main
reason may be that KVQU learns to mask al-
ready addressed information and retain those un-
touched, which leads to more accurate key ad-
dressing and more expressive answer representa-
tions. But, we also observe that by switching
from AR to SQ, STOP+KVQU+SQ achieves more
improvement than STOP+CQU+SQ. One possible
reason is that the KVQU method, together with the
STOP strategy, can help address the most relevant
keys (i.e., top#1 key) more accurately than CQU
does at each hop, which results in more accurate
structured queries.

We randomly select 50 multi-constraint ques-
tions from the test set of WebQuestions and an-
alyze the addressed keys at each hop. We ﬁnd
that for 48 questions, the model with CQU (model
(f)) repeatedly selects the same keys that have
the highest relevance probabilities at
the ﬁrst
two hops. However, when replacing CQU with
KVQU, the model addresses different keys at the
ﬁrst two hops (examples shown in Table 2). One
possible reason is that, in contrast to the CQU that
only uses the value representation of current hop
to update the query, KVQU additionally considers
the addressed keys in the current hop, and aims to
mask the already addressed information, so that in
later hops, the model will focus on the remaining
untouched part of the question.

4.5 Candidate Ranking v.s. Structured

Query

We investigate two different answer
retrieval
methods, the IR styled method AR and the se-
mantic parsing styled method SQ, in the experi-
ments. By comparing CQU+AR (a) and CQU+SQ
(c) in Table 1, we can see that replacing the answer

STOP + KVQU + SQ STOP + KVQU + AR

Hop Size
1
2
3
4
5

38.7%
45.9%
53.2%
53.1%
53.0%

35.6%
41.2%
50.4%
46.7%
44.2%

3:

Performance

Table
of
STOP+KVQU+SQ and STOP+KVQU+AR with
different hop sizes on the development
set of
WebQuestions.

(answer

F1)

retrieval method does not boost the performance,
but actually hurts. This is not surprising, because
with the vanilla CQU, the model does not know
how to stop reading the memory, thus may se-
lect repeated/invalid keys after the ﬁrst hop, which
will produce incorrect structured queries. Simi-
larly, the KVQU+SQ also suffers from the incor-
rect structured queries.

After introduce the STOP key into the mem-
ory, we ﬁnd that no matter which query updat-
ing methods are used, the models using struc-
tured queries (STOP+*+SQ) signiﬁcantly outper-
form their corresponding versions using the rank-
ing based method (STOP+*+AR). Despite of the
STOP strategy, the improvement may come from
two aspects. First, the structured queries in SQ are
built from the best key at every hop, thus have cap-
tured more global information throughout the rea-
soning procedure, while the AR method only uses
the o in the last hop to retrieve answers. Secondly,
the models with SQ can easily output multiple
qualiﬁed answers from the KB, while the meth-
ods with AR can only output the top one from a
ranked list in the memory. Moreover, from a prac-
tical perspective, the SQ methods provide more in-
terpretability than the ranking based methods.

4.6

Impact of the Hop Size

To investigate the impact of the hop size to
the model performance, we evaluate the model
STOP+KVQU+SQ and STOP+KVQU+AR with
various hop size settings on the development set
of WebQuestions. We evaluate both of the two
models since we wonder which answer retrieval
method is more sensitive to the hop size. As shown
in Table 3, the model with AR answer retrieval
method achieves the best performance with hop
size of 3. Then with the hop size increasing, the
performance drops.
In contrast, the model with
the SQ method also achieves the best performance
when the hop size is 3. When the hop size in-
creases, the performance does not drop signiﬁ-

cantly, but almost remains unchanged. We think
the reason may be that the model with the AR
method keeps predicting the keys as the hop size
increasing, which inevitably introduces noise to
the answer representations. On the other hand, the
model with the SQ method is less sensitive to the
hop size, since once it sees the STOP key, the lat-
ter predictions do not affect the resulted structured
query.

4.7 Results on QALD-4

To further examine our model under different con-
ditions, besides WebQuestions, we also evaluate
our main model on the QALD-4 dataset [Unger
et al.], which is built upon another KB, DBpe-
dia [Auer et al., 2007]. Like Freebase, DBpe-
dia stores real world facts in the triple format,
making it easier for our model to adapt to this
new KB. The QALD-4 dataset consists of three
QA datasets, namely, Multilingual QA, Biomed-
ical QA and Hybrid QA. Here we evaluate our
model on the multilingual QA dataset which con-
tains 250 English question-SPARQL pairs, where
200 questions (80%) are used for training and 50
questions for testing. These questions are more
complicated than those of WebQuestions, e.g., all
QALD-4 questions require at least two hops of in-
ference over the KB to answer. Table 4 lists the
results of our model and other participated sys-
tems [Unger et al.]. We can see that our model
can achieve the best performance on this dataset,
without importing extra rules.

4.8 Error Analysis

We analyze the errors of our main model on the
development set of WebQuestions. Around 80%
errors are caused by incorrect key addressing. The
key addressing errors are mainly due to (1) the en-
tities in the addressed keys are incorrect. Entity
linking itself is a challenging task and we do not
employ any existing entity linking tools to incor-
porate the linking conﬁdence score, which may
further improve our model; (2) the relations in
the addressed keys are incorrect, which is mainly
caused by insufﬁcient context in the question. For
example, in what is duncan bannatyne, the in-
formation from the question is quite limited for
our model to predict the correct key fb:profession.
Although the STOP strategy proves to be effec-
tive on most questions, there are still some ques-
tions where the STOP strategy does not work, e.g.,
what team did david beckham play for in 2011

Method
Xser
gAnswer
CASIA
our model

Recall
0.71
0.37
0.40
0.78

Precision
0.72
0.37
0.32
0.82

F-measure
0.72
0.37
0.36
0.81

Table 4: Results of the models on the test set of QALD.

shown in Table 2. We failed to perform reason-
ing over the time constraint 2011 due to limited
context left for 2011 after we addressed play for
in earlier hops. Also due to this same reason, our
model can not correctly answer the second ques-
tion in Table 2, who is the governor of India 2009.
Keep in mind that, we actually do not have an-
notated addressed keys at each hop to explicitly
teach our model. The remaining cases also in-
clude failures in deep analysis according to spe-
ciﬁc KBs. For example, in who is the mother of
prince michael jackson, our model only matches
the triple michael jackson fb:parents, but fail to
spot female in the next hop.

5 Conclusions

In this paper, we propose to apply the KV-
MemNNs as a semantic parsing module to ap-
proach the open-domain KB-QA task. We in-
troduce a novel STOP strategy to derive struc-
tured queries with ﬂexible number of query triples
during multi-hop memory reading, and present a
new query updating method, which considers the
already-addressed keys in previous hops as well
as the value representations. Experimental re-
sults show that the STOP strategy not only enables
multi-hop reasoning over the memory, but also
acts as the key to construct the structured queries,
which help our model achieve the state-of-the-art
performances on two benchmark datasets.

Acknowledgement

We would like to thank the anonymous review-
ers for their helpful comments and valuable sug-
gestions. We also thank Yao Fu for his con-
structive comments on the early version of the
This work is supported by National
draft.
High Technology R&D Program of China (Grant
No.2018YFC0831905), Natural Science Founda-
tion of China (Grant No. 61672057, 61672058).
For any correspondence, please contact Yansong
Feng.

References

S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary G. Ives.
Dbpedia: A nucleus for a web of open data.
In
ISWC/ASWC, 2007.

Jun-Wei Bao, Nan Duan, Zhao Yan, Ming Zhou, and
Tiejun Zhao. Constraint-based question answering
with knowledge graph. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING 2016), pages 2503–2514, 2016.

Hannah Bast and Elmar Haussmann. More accurate
question answering on freebase. In CIKM, 2015.

Jonathan Berant and Percy Liang. Semantic parsing
In Proceedings of the 52nd An-
via paraphrasing.
nual Meeting of the Association for Computational
Linguistics, ACL 2014, June 22-27, 2014, Baltimore,
MD, USA, Volume 1: Long Papers, 2014.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. Semantic parsing on freebase from question-
In Proceedings of the 2013 Confer-
answer pairs.
ence on Empirical Methods in Natural Language
Processing (EMNLP 2013), 2013.

Kurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. Freebase: a collabora-
tively created graph database for structuring human
knowledge. In SIGMOD, 2008.

Antoine Bordes, Nicolas Usunier, Sumit Chopra, and
Jason Weston. Large-scale simple question answer-
ing with memory networks. CoRR, abs/1506.02075,
2015.

Li Dong, Furu Wei, Ming Zhou, and Ke Xu. Question
answering over freebase with multi-column convo-
In Proceedings of the
lutional neural networks.
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), 2015.

Sarthak Jain. Question answering over knowledge
In Proceed-
base using factual memory networks.
ings of the Student Research Workshop, SRW@HLT-
NAACL 2016, The 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
San Diego California, USA, June 12-17, 2016, pages
109–115, 2016.

Diederik Kingma and Jimmy Ba.

Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.

Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and
Luke S. Zettlemoyer. Scaling semantic parsers with
on-the-ﬂy ontology matching. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2013), 2013.

Yuxuan Lai, Yansong Feng, Xiaohan Yu, Zheng Wang,
Lattice cnns for
Kun Xu, and Dongyan Zhao.
matching based chinese question answering. Pro-
ceedings of the 33rd AAAI Conference on Artiﬁcial
Intelligence (AAAI 2019), 2019.

Alexander Miller, Adam Fisch, Jesse Dodge, Amir-
Hossein Karimi, Antoine Bordes, and Jason We-
ston. Key-value memory networks for directly read-
ing documents. arXiv preprint arXiv:1606.03126,
2016.

Siva Reddy, Mirella Lapata, and Mark Steedman.
Large-scale semantic parsing without question-
answer pairs. Transactions of the Association of
Computational Linguistics, pages 377–392, 2014.

Yelong Shen, Po-Sen Huang, Jianfeng Gao, and
Weizhu Chen. Reasonet: Learning to stop reading in
machine comprehension. In Proceedings of the 23rd
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 1047–1055.
ACM, 2017.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. End-to-end memory networks. In
NIPS, 2015.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
End-to-end memory networks. In Advances in neu-
ral information processing systems, pages 2440–
2448, 2015.

Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen,
Antonio Torralba, Raquel Urtasun, and Sanja Fidler.
Movieqa: Understanding stories in movies through
question-answering. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2016.

Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Ben-
gio. Word representations: A simple and general
In Proceed-
method for semi-supervised learning.
ings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL 2010), pages
384–394, 2010.

Christina Unger, Corina Forascu, Vanessa L´opez,
Axel-Cyrille Ngonga Ngomo, Elena Cabrio, Philipp
Cimiano, and Sebastian Walter. Question answering
In Working Notes for
over linked data (QALD-4).
CLEF 2014 Conference, Shefﬁeld, UK, September
15-18, 2014.

Jason Weston, Sumit Chopra, and Antoine Bordes.
Memory networks. arXiv preprint arXiv:1410.3916,
2014.

Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang,
and Dongyan Zhao. Question Answering on Free-
base via Relation Extraction and Textual Evidence.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), 2016.

Xuchen Yao and Benjamin Van Durme.

Information
extraction over structured data: Question answering
In Proceedings of the 52nd Annual
with freebase.
Meeting of the Association for Computational Lin-
guistics, ACL 2014, June 22-27, 2014, Baltimore,
MD, USA, Volume 1: Long Papers, 2014.

Xuchen Yao. Lean question answering over freebase
In Proceedings of the 2015 Confer-
from scratch.
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (NAACL 2015), 2015.

Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and
Jianfeng Gao. Semantic parsing via staged query
graph generation: Question answering with knowl-
edge base. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
2015.

