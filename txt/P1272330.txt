Modeling Discourse Cohesion for Discourse Parsing via Memory Network

Yanyan Jia, Yuan Ye, Yansong Feng, Yuxuan Lai, Rui Yan and Dongyan Zhao
Institute of Computer Science and Technology, Peking University
The MOE Key Laboratory of Computational Linguistics, Peking University
{jiayanyan,pkuyeyuan,fengyansong,erutan,ruiyan,zhaody}@pku.edu.cn

Abstract

tools,

existing

approaches

dependencies
Identifying
long-span
between discourse units is crucial
to
improve discourse parsing performance.
Most
design
sophisticated features or exploit various
but achieve little
off-the-shelf
success. In this paper, we propose a new
transition-based discourse parser
that
makes use of memory networks to take
The
discourse cohesion into account.
discourse
automatically
parsing,
cohesion
especially
scenarios.
on the RST discourse
Experiments
our method
treebank
outperforms
featured based
methods, and the memory based discourse
cohesion can improve the overall parsing
performance signiﬁcantly 1.

show that
traditional

discourse
span

beneﬁts
for

captured

long

1

Introduction

Discourse parsing aims to identify the structure
and relationship between different
element
discourse units (EDUs). As a fundamental topic
in natural language processing, discourse parsing
can assist many down-stream applications such as
summarization (Louis et al., 2010), sentiment
analysis (Polanyi and van den Berg, 2011) and
question-answering (Ferrucci
al., 2010).
However, the performance of discourse parsing is
still far from perfect, especially for EDUs that are
distant to each other in the discourse. In fact, as
found in (Jia et al., 2018), the discourse parsing
performance drops quickly as the dependency
span increases. The reason may be twofold:

et

1Code for replicating our experiments is available at

https://github.com/PKUYeYuan/ACL2018 CFDP.

Firstly, as discussed in previous works (Joty
et al., 2013), it is important to address discourse
structure characteristics, e.g., through modeling
lexical chains in a discourse,
for discourse
parsing, especially in dealing with long span
scenarios. However, most existing approaches
mainly focus on studying the semantic and
syntactic aspects of EDU pairs, in a more local
view. Discourse cohesion reﬂects the syntactic or
semantic relationship between words or phrases
in a discourse, and, to some extent, can indicate
the topic changing or threads in a discourse.
situations,
cohesion includes ﬁve
Discourse
including
ellipsis,
substitution,
reference,
conjunction and lexical cohesion (Halliday and
Hasan, 1989). Here, lexical cohesion reﬂects the
semantic relationship of words, and can be
modeled as the recurrence of words, synonym
and contextual words.

thesauri

However, previous works do not well model
the discourse cohesion within the discourse
parsing task, or do not even take this issue into
account. Morris and Hirst (1991) proposes to
utilize Roget
to form lexical chains
(sequences of semantically related words that can
reﬂect the topic shifts within a discourse), which
to characterize
features
are used to extract
discourse structures.
(Joty et al., 2013) uses
lexical chain feature to model multi-sentential
these simpliﬁed cohesion
relation.
features
parsing
improve
performance, especially in long spans.

Actually,
can

already

Secondly, in modern neural network methods,
the
modeling discourse cohesion as part of
networks is not a trivial task. One can still use
off-the-shell tools to obtain lexical chains, but
these tools can not be jointly optimized with the
main neural network parser. We argue that
characterizing
implicitly
within a uniﬁed framework would be more

discourse

cohesion

2 Model overview

conventional

Our parser is an arc-eager style transition system
(Nivre, 2003) with 2 stacks and a queue as shown
in Figure 2, which is similar in spirit with (Dyer
et al., 2015; Ballesteros et al., 2015). We follow
the
in
transition-based dependency parsing, i.e., a queue
(B) of EDUs to be processed, a stack (S) to store
the partially constructed discourse trees, and a
stack (A) to represent the history of transitions
(actions combined with discourse relations).

structures

data

In our parser,

the transition actions include
Shift, Reduce, Left-arc and Right-arc. At each
step, the parser chooses to take one of the four
actions and pushes the selected transition into A.
Shift pushes the ﬁrst EDU in queue B to the top of
the stack S, while Reduce pops the top item of S.
Left-arc connects the ﬁrst EDU (head) in B to the
top EDU (dependent) in S and then pops the top
item of S, while Right-arc connects the top EDU
(head) of S to the ﬁrst EDU (dependent) in B and
then pushes B’s ﬁrst EDU to the top of S. A parse
tree can be ﬁnally constructed until B is empty
and S only contains a complete discourse tree.
For more details, please refer to (Nivre, 2003).

at

As

shown in Figure 2,

time t, we
characterize the current parsing process by
preserving the top two elements in B, top three
elements in A and the root EDU in the partially
the top of S. We ﬁrst
constructed tree at
concatenate the embeddings of the preserved
elements in each data structure to obtain the
embeddings of S, B and A. We then append the
three representations with the position2 features
(introduced in Section 2.1), respectively. We pass
them through one ReLU layer and two fully
connected layers with ReLU as their activation
functions to obtain the ﬁnal state representation pt
at time t, which will be used to determine the best
transition to take at t.

Next, we apply an afﬁne transformation to pt
and feed it to a softmax layer to get the distribution
over all possible decisions (actions combined with
discourse relations). We train our model using the
automatically generated oracle action sequences
as the gold-standard annotations, and utilize cross
entropy as the loss function. We perform greedy
search during decoding.

Figure 1: An illustration for modelling discourse
cohesion with memory network. The example
discourse includes 12 EDUs and talks about 3
different threads (food, time and trafﬁc), which are
colored by blue, gray and white, respectively.

straightforward and effective for our neural
network based parser. As shown in Figure 1, the
12 EDUs in the given discourse talk about
different topics, marked with 3 different colors,
which could be captured by a memory network
that maintains several memory slots. In discourse
parsing, such an architecture may help to cluster
topically similar or related EDUs into the same
memory slot, and each slot could be considered as
a representation that maintains a speciﬁc topic or
thread within the current discourse.
Intuitively,
we could also treat such a mechanism as a way to
the
capture
discourse, just like the lexical chain features used
in previous works, but without relying on external
tools or resources.

cohesion characteristics of

the

(Hochreiter

In this paper, we investigate how to exploit
discourse cohesion to improve discourse parsing.
Our contribution includes:
1) we design a
memory network method to capture discourse
cohesion implicitly in order to improve discourse
parsing. 2) We choose bidirectional long-short
and
(LSTM)
term memory
Schmidhuber, 1997) with an attention mechanism
to represent EDUs directly from embeddings, and
use simple position features to capture shallow
on
discourse
off-the-shelf tools or resources. Experiments on
the RST corpus show that the memory based
discourse cohesion model can help better capture
discourse structure information and lead to
signiﬁcant improvement over traditional feature
based discourse parsing methods.

structures,

without

relying

the

sequence of words in the EDU to a bi-directional
Long Short Term Memory (LSTM) with attention
ﬁnal word
mechanism and
obtain
representation by concatenating the two ﬁnal
outputs from both directions. Here, we use
pre-trained Glove (Pennington et al., 2014) as the
word embeddings. We get the POS tags from
Stanford CoreNLP toolkit (Manning et al., 2014),
and similarly, send the POS tag sequence of the
EDU to a bi-directional LSTM with attention
mechanism to obtain the ﬁnal POS representation.
For concise, we omit the bi-directional LSTM
network structure for POS in Figure 2, which is
the same as the one for word. The P osition1
feature vectors are randomly initialized and we
expect them to work as a proxy to capture the
shallow discourse structure information.

Memory Reﬁned Representation: Besides the
shallow structure features, we design a memory
network component to cluster EDUs with similar
topics to the same memory slot to alleviate the
long span issues, as illustrated in Figure 1. We
expect these memory slots can work as lexical
chains, which can maintain different
threads
within the discourse. Such a memory mechanism
has
it can perform the
clustering automatically and does not rely on
extra tools or resources to train.

the advantage that

their

to get

respectively,

Concretely, we match the representations of S
and B with
corresponding memory
their discourse
networks,
cohesion clues, which are used to improve the
original representations. Take B as an example,
we ﬁrst compute the similarity between the
representation of B (Vb) and each memory slot mi
in B’s memory. We adopt the cosine similarity as
our metric as below:

Sim[x, y] =

x · y
(cid:107)x(cid:107) · (cid:107)y(cid:107)

Then, we use this cosine similarity to produce a
normalized weight wi for each memory slot. We
introduce a strength factor λ to improve the focus.

wi =

exp(λSim[Vb, mi])
j exp(λSim[Vb, mj])

(cid:80)

Finally, we get the discourse cohesion clue of B
(denoted by BCoh) from its memory according to
the weighted sum of mi.

(1)

(2)

(3)

BCoh = (cid:88)

wimi

i

Figure 2: Our discourse parsing framework: (1)
Basic EDU representation module; (2) Memory
networks to capture the discourse cohesion so as
to obtain the reﬁned representations of S and B.
RA(Li) means that the chosen action is Right-arc
and its relation is List. SH means Shift. a1 to
an are weights for the attention mechanism of the
bidirectional LSTM.

2.1 Discourse Structures

As mentioned in previous work (Jia et al., 2018),
when the top EDUs in S and B are far from each
other in the discourse, i.e., with a long span, the
parser will be prone to making wrong decisions.
To deal with these long-span cases, one should
take discourse structures
e.g.,
extracting features from the structure of a long
discourse
characterizing
analyzing
different topics discussed in the discourse.

into account,

and

or

We,

therefore, choose two kinds of position
features to reﬂect the structure information, which
can be viewed as a shallow form of discourse
cohesion. The ﬁrst one describes the position of an
EDU alone, while the second represents the spatial
relationship between the top EDUs of S and B.
(1) P osition1:
the positions of the EDU in the
sentence, paragraph and discourse, respectively.
(2) P osition2: whether the top EDUs of S and
B are in the same sentence/paragraph or not, and
the distance between them.

3 Memory based Discourse Cohesion

Basic EDU representation:
In our model, the
in both S and B follow the same
EDUs
representation method, and we take an EDU in B
as an example as shown in Figure 2. The basic
by
representation
concatenating three components, i.e., word, POS
and P osition1. Regarding word, we feed the

an EDU is

built

for

We concatenate BCoh (the discourse cohesion clue
of B) and the original embedding of B to get the
reﬁned representation Bref ined for B. Similarly,
we concatenate SCoh and the embedding of S
to get the reﬁned representation Sref ined for S,
as shown in Figure 2.
In our experiments, each
memory contains 20 slots, which are randomly
initialized and optimized during training.

4 Evaluation and Results

Dataset: We use the RST Discourse Treebank
(Carlson et al., 2001) with the same split as in (Li
et al., 2014),
training, 30 for
i.e., 312 for
development and 38 for testing. We experiment
with two set of
the 111 types of
relations,
ﬁne-grained relations and the 19 types of
coarse-grained relations, respectively.

In

the

Evaluation Metrics:
Rhetorical
Structure Theory (RST) (Mann and Thompson,
1988), head is the core of a discourse, and a
dependent gives supporting evidence to its head
with certain relationship. We adopt unlabeled
accuracy U AS (the ratio of EDUs that correctly
identify their heads) and labeled accuracy LAS
(the ratio of EDUs that have both correct heads
and relations) as our evaluation metrics.

Baselines: We compare our method with the
following baselines and models: (1) Perceptron:
We re-implement the perceptron based arc-eager
style dependency discourse parser as mentioned
in (Jia et al., 2018) with coarse-grained relation.
The Perceptron model chooses words, POS tags,
positions and length features, totally 100 feature
templates, with the early update strategy (Collins
(2) Jia18: Jia et al. (2018)
and Roark, 2004).
implement a transition-based discourse parser
with stacked LSTM, where they choose a
two-layer LSTM to represent EDUs by encoding
four kinds of features including words, POS tags,
(3) Basic EDU
positions and length features.
representation (Basic): Our discourse parser with
the basic EDU representation method mentioned
in Section 3. (4) Memory reﬁned representation
(Reﬁned): Our full parser equipped with the
basic EDU representation method and the
to capture the discourse
memory networks
(5) MST-full
cohesion mentioned in Section 3.
a graph-based dependency
(Li et al., 2014):
discourse parser with carefully selected 6 sets of
features including words, POS tags, positions,

length, syntactic and semantic similarity features,
which achieves the state-of-art performance on
the RST Treebank.

4.1 Results

for

into

useful

taking

We list the overall discourse parsing performance
in Table 1. Here, Jia18, a stack LSTM based
the
method (Jia et al., 2018), outperforms
traditional Perceptron method, but falls behind
our Basic model with word, POS tags and
Position features.
The reason may be that
representing EDUs directly from the sequence of
word/POS embeddings could probably capture
the semantic meaning of EDUs, which is
especially
account
synonyms or paraphrases that often confuse
traditional feature-based methods. We can also
see that Basic(word+pos+position) signiﬁcantly
outperforms Basic(word+pos), as the Position
features may play a crucial role in providing
useful structural clues to our parser. Such position
information can also be considered as a shallow
to capture the discourse cohesion,
treatment
especially for long span scenarios. When using
the memory network, our Reﬁned method
achieves
the
Basic(word+pos+position)
in both UAS and
LAS. The reason may come from the ability of
the memory networks in simulating the lexical
chains within a discourse, where the memory
networks can model the discourse cohesion so as
to provide topical or structural clues to our parser.
We use SIGF V2 (Pad´o, 2006)
to perform
signiﬁcance test for the discussed models. We
ﬁnd that the Basic(word+pos+position) method
signiﬁcantly outperforms (Jia et al., 2018), and
our Reﬁned model performs signiﬁcantly better
than Basic(word+pos+position) (with p < 0.1).

performance

better

than

However, when compared with MST-full (Li
et al., 2014), our models still fall behind this
state-of-the-art method. The main reason might
be that MST-full follows a global graph-based
dependency parsing framework, where their high
order methods (in cubic time complexity) can
directly analyze the relationship between any
EDUs pairs in the discourse, while, we choose the
transition-based local method with linear time
complexity, which can only investigate the top
EDUs in S and B according to the selected
actions,
thus usually has a lower performance
than the global graph-based methods, but with a

lower (linear) time complexity. On the other
the neural network components help us
hand,
maintain much fewer features than MST-full,
which carefully selects 6 different sets of features
that are usually obtained using extra tools and
resources. And,
the neural network design is
ﬂexible enough to incorporate various clues into a
uniform framework, just like how we introduce
the memory networks as a proxy to capture
discourse cohesion.

In the RST corpus, when the distance between
two EDUs is larger,
there are usually fewer
the parsing
numbers of such EDU pairs, but
performance for those long span cases drops more
signiﬁcantly. For example, the LAS is even lower
than 5% for those dependencies that have a range
of 6 EDUs. We take a detailed look at the parsing
performance for dependencies at different lengths
(from 1 to 6 as an example) using coarse-grained
relations. As shown in Table 2, compared with
the Basic method, both UAS and LAS of the
Reﬁned method are improved signiﬁcantly in
almost all
spans, where we observe more
prominent improvement for the UAS in larger
spans such as span 5 and span 6, with about
8.70% and 6.38%, respectively.

Method

Perceptron
Jia18
Basic (word+pos)
Basic (word+pos+position)
Reﬁned (20 slots)
MST-full

UAS

0.5422
0.5852
0.5588
0.5933
0.6197
0.7331

LAS
(Fine)
0.3231
0.3286
0.367
0.3832
0.3947
0.4309

LAS
(Coarse)
0.3777
0.4037
0.3985
0.4305
0.4445
0.4851

Table 1: Overall discourse parsing performance in
the RST dataset.

span
(count)
1(1225)
2 (405)
3 (212)
4 (125)
5 (69)
6 (47)

Basic(word+pos+position)
UAS
0.7796
0.6198
0.434
0.256
0.1739
0.1064

LAS
0.618
0.4
0.2217
0.112
0.0725
0.0426

Reﬁned (20)
LAS
0.6261
0.4124
0.2642
0.128
0.1015
0.0638

UAS
0.8261
0.6025
0.4576
0.296
0.2609
0.1702

Table 2: Performance in different discourse spans.

Finally,

let us take a detailed comparison
between Reﬁned and Basic to investigate the
advantages of capturing discourse cohesion. Note
that, our Reﬁned method wins Basic in almost all
relations. Here, we discuss one typical relation
List, which often indicates
long span

a

dependency between a pair of EDUs. In the test
set of RST, the average span for List is 7.55, with
the max span of 69. Our Reﬁned can successfully
identify 55 of them, with an average span of 9.02
and the largest one of 63, while, the Basic method
can only identify 41 edges labeled with List,
which are mostly shorter cases, with an average
span of 1.32 and the largest one of 5. More
detailedly, there are 18 edges that are correctly
identiﬁed by our Reﬁned but missed by the Basic
method. The average span of those dependencies
is 25.39.
It is easy to ﬁnd that without further
considerations in discourse structures, the Basic
method has limited ability in correctly identifying
longer
those
comparisons prove again that our Reﬁned can
take better advantage of modeling discourse
cohesion, which enables our model to perform
better in long span scenarios.

dependencies.

span

And

5 Conclusions

improve

discourse

to model

In this paper, we propose to utilize memory
cohesion
networks
automatically. By doing so we could capture the
topic change or threads within a discourse, which
can further
the discourse parsing
performance, especially for long span scenarios.
Experimental
results on the RST Discourse
Treebank show that our proposed method can
characterize the discourse cohesion efﬁciently and
archive signiﬁcant improvement over traditional
feature based discourse parsing methods.

Acknowledgments

and

suggestions, which

We would like to thank our anonymous reviewers,
Bingfeng Luo, and Sujian Li for their helpful
comments
greatly
improved our work. This work is supported by
National High Technology R&D Program of
China (Grant No.2015AA015403), and Natural
Science Foundation
(Grant No.
61672057, 61672058). For any correspondence,
please contact Yansong Feng.

of China

References

2015.
by modeling

Miguel Ballesteros, Chris Dyer,
Improved

and Noah A.
transition-based
Smith.
of
instead
parsing
In Proceedings of the 2015
words with lstms.
Conference on Empirical Methods
in Natural
Language Processing, EMNLP 2015, Lisbon,

characters

Portugal, September 17-21, 2015. pages 349–359.
http://aclweb.org/anthology/D/D15/D15-1041.pdf.

Lynn Carlson, Daniel Marcu,

In Proceedings of
The

and Mary Ellen
Okurovsky. 2001.
Building a discourse-tagged
corpus in the framework of rhetorical structure
the SIGDIAL
theory.
2nd Annual Meeting
2001 Workshop,
of
Interest Group on Discourse
and Dialogue, Saturday, September 1, 2001 to
Sunday, September 2, 2001, Aalborg, Denmark.
http://aclweb.org/anthology/W/W01/W01-
1605.pdf.

the Special

Michael Collins and Brian Roark. 2004. Incremental
In
parsing with the perceptron algorithm.
Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics, 21-26
July, 2004, Barcelona, Spain.. pages 111–118.
http://aclweb.org/anthology/P/P04/P04-1015.pdf.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
In Proceedings of the 53rd Annual
term memory.
the Association for Computational
Meeting of
Linguistics
Joint
7th
and
Conference on Natural Language Processing
of
the Asian Federation of Natural Language
Processing, ACL 2015, July 26-31, 2015, Beijing,
China, Volume 1: Long Papers. pages 334–343.
http://aclweb.org/anthology/P/P15/P15-1033.pdf.

International

the

Fan,

Eric Nyberg,

Jennifer
David A. Ferrucci, Eric W. Brown,
David Gondek,
Chu-Carroll,
James
J. William
Aditya Kalyanpur, Adam Lally,
Prager,
John M.
Murdock,
and Christopher A. Welty.
Nico Schlaefer,
Building watson: An overview of the
2010.
AI Magazine 31(3):59–79.
deepqa project.
http://www.aaai.org/ojs/index.php/aimagazine/article
/view/2303.

M.A.K. Halliday and Ruqaiya Hasan. 1989. Language,
Context, and Text: Aspects of Language in a Social-
Semiotic Perspective.

Sepp

Hochreiter

and
Long
1997.
Neural
Computation
https://doi.org/10.1162/neco.1997.9.8.1735.

J¨urgen
short-term

Schmidhuber.
memory.
9(8):1735–1780.

Yanyan Jia, Yansong Feng, Yuan Ye, Chao Lv,
Chongde Shi, and Dongyan Zhao. 2018. Improved
discourse parsing with two-step neural transition-
ACM Trans. Asian & Low-
based model.
Resource Lang.
Inf. Process. 17(2):11:1–11:21.
https://doi.org/10.1145/3152537.

Shaﬁq R. Joty, Giuseppe Carenini, Raymond T.
Combining
Ng, and Yashar Mehdad. 2013.
parsing
and multi-sentential
intra-
In
for document-level discourse
Proceedings of
the 51st Annual Meeting of
the Association for Computational Linguistics,

rhetorical

analysis.

ACL 2013, 4-9 August 2013, Soﬁa, Bulgaria,
Long Papers.
Volume
486–496.
http://aclweb.org/anthology/P/P13/P13-1048.pdf.

pages

1:

Sujian Li, Liang Wang, Ziqiang Cao, and Wenjie Li.
2014.
Text-level discourse dependency parsing.
the 52nd Annual Meeting of
In Proceedings of
the Association for Computational Linguistics,
ACL 2014, June 22-27, 2014, Baltimore, MD,
USA, Volume 1:
Long Papers. pages 25–35.
http://aclweb.org/anthology/P/P14/P14-1003.pdf.

Annie Louis, Aravind K. Joshi, and Ani Nenkova.
2010. Discourse indicators for content selection
in summarization. In Proceedings of the SIGDIAL
2010 Conference, The 11th Annual Meeting of the
Special Interest Group on Discourse and Dialogue,
24-15 September 2010, Tokyo, Japan. pages 147–
156. http://www.aclweb.org/anthology/W10-4327.

William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional
theory of text organization. Text - Interdisciplinary
Journal for the Study of Discourse 8(3):243–281.

Christopher D. Manning, Mihai Surdeanu,

John
Jenny Rose Finkel, Steven Bethard,
Bauer,
The stanford
and David McClosky. 2014.
In
corenlp natural language processing toolkit.
the 52nd Annual Meeting of
Proceedings of
the Association for Computational Linguistics,
ACL 2014, June 22-27, 2014, Baltimore, MD,
USA,
55–60.
http://aclweb.org/anthology/P/P14/P14-5010.pdf.

System Demonstrations.

pages

Jane Morris and Graeme Hirst. 1991. Lexical cohesion
computed by thesaural relations as an indicator of
the structure of text. Computational Linguistics
17(1):21–48.

J Nivre. 2003. An efﬁcient algorithm for projective
In Iwpt-2003 : International

dependency parsing.
Workshop on Parsing Technology. pages 149–160.

Sebastian Pad´o. 2006.

User’s guide to sigf:
Signiﬁcance testing by approximate randomisation.

Jeffrey

2014.

Socher,

Richard

for word representation.

and
Pennington,
Glove:
Christopher D. Manning.
Global vectors
In
Proceedings of the 2014 Conference on Empirical
Language Processing,
Methods
in Natural
2014, Doha,
EMNLP 2014, October 25-29,
Special
Qatar,
Interest Group of
the ACL. pages 1532–1543.
http://aclweb.org/anthology/D/D14/D14-1162.pdf.

A meeting

SIGDAT,

of

a

Livia Polanyi and Martin van den Berg. 2011.
In Data
Discourse structure and sentiment.
Mining Workshops (ICDMW), 2011 IEEE 11th
International Conference on, Vancouver, BC,
Canada, December 11, 2011. pages 97–102.
https://doi.org/10.1109/ICDMW.2011.67.

