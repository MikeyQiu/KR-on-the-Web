9
1
0
2
 
v
o
N
 
0
2
 
 
]
L
C
.
s
c
[
 
 
1
v
3
4
7
8
0
.
1
1
9
1
:
v
i
X
r
a

SemanticZ at SemEval-2016 Task 3:
Ranking Relevant Answers in Community Question Answering
Using Semantic Similarity Based on Fine-tuned Word Embeddings

Todor Mihaylov
Research Training Group AIPHES
Institute for Computational Linguistics
Heidelberg University
mihaylov@cl.uni-heidelberg.de

Preslav Nakov
Qatar Computing Research Institute, HBKU
P.O. box 5825
Doha, Qatar
pnakov@qf.org.qa

Abstract

We describe our system for ﬁnding good an-
swers in a community forum, as deﬁned in
SemEval-2016, Task 3 on Community Ques-
tion Answering. Our approach relies on sev-
eral semantic similarity features based on ﬁne-
tuned word embeddings and topics similar-
ities.
In the main Subtask C, our primary
submission was ranked third, with a MAP of
51.68 and accuracy of 69.94. In Subtask A,
our primary submission was also third, with
MAP of 77.58 and accuracy of 73.39.

1 Introduction

Posting questions that have already been asked in a
community forum is annoying to users in the forum.
The SemEval-2016 Task 3 on Community Question
Answering1(Nakov et al., 2016) aims to solve this
real-life problem. The main subtask (Subtask C)
asks to ﬁnd an answer that already exists in the fo-
rum and will be appropriate as a response to a newly-
posted question. There is also a secondary, Subtask
A, which focuses on Question-Comment Similarity
and asks to rank the comments within a question-
comment thread based on their relevance with re-
spect to the thread’s question.

Here, we examine the performance of us-
ing different word embeddings obtained with the
Word2Vec tool (Mikolov et al., 2013), which we use
to build vectors for the questions and the answers.
We train classiﬁers using features derived from these
embeddings to solve subtasks A and C.

Our contribution is in producing good word em-
beddings based on empirical evaluation of different
conﬁgurations working in the Community Question
Answering domain; as they perform well, we make
them freely available to the research community.2

2 Related Work

This year’s SemEval-2016 Task 3 is a follow up of
SemEval-2015 Task 3 on Answer Selection in Com-
munity Question Answering (Nakov et al., 2015).
The 2015 subtask A asked to determine whether
an answer was relevant, potentially useful, or bad,
while this year this is about ranking.

such

knowledge

Here we focus on features that use seman-
embeddings,
as word
tic
various
features extracted from word embed-
dings, and topic models. Word embeddings
and word embeddings
similarities have been
used by teams in the 2015 edition of the task
(Belinkov et al., 2015;
Zamanov et al., 2015;
Tran et al., 2015; Nicosia et al., 2015). LDA topic
have also been used (Tran et al., 2015).

Many other features have been tried for the task.
For example, Tran et al. (2015) used metadata about
the question and the comment. User proﬁle statis-
tics such as number of Good, Bad and Potentially
Useful comments by a given user have been used to
model user likelihood of posting different types of
comment (Nicosia et al., 2015). Vo et al. (2015) and
Nicosia et al. (2015) used syntactic tree similarities
to compare questions to comments. The problem of
selecting relevant answers has even been approached
as a spam ﬁltering task (Vo et al., 2015).

1http://alt.qcri.org/semeval2016/task3/

2https://github.com/tbmihailov/semeval2016-task3-cqa

3 Data

In our experiments, we used annotated training, de-
velopment and testing datasets, as well as a large
unannotated dataset, all provided by the SemEval-
2016 Task 3 organizers. We further collected some
additional unannotated in-domain data from some
other sources, as explained below; ﬁnally, we used
some models pretrained on out-of-domain data.

Training, development, and testing data. For
Subtask A, there are 6,398 questions and 40,288
comments from their question-answer threads, and
for Subtask C, there are 317 original questions,
3,169 related questions, and 31,690 comments. For
both subtasks, the comments are annotated as Good,
PotentiallyUseful and Bad; for subtask A, the an-
notation is with respect to the question in whose
thread the comment appeared, while for subtask C,
it is with respect to a new question. For both sub-
tasks, a successful ranking is one that ranks all Good
comments before all PotentiallyUseful and Bad ones
(without distinguishing between the latter two).

Unannotated data. We performed experiments
with Word2Vec embeddings trained on different
unannotated data sources. We wanted to ﬁnd the best
performing embeddings and to use them in our sys-
tem. In Table 1, we list the various data sources we
used for training our Word2Vec models, and their
vocabulary size.
Qatar Living Forum is the original Qatar Living.3
unannotated data containing 189,941 questions and
1,894,456 comments. It is limited to the forums sec-
tion of the Qatar Living website.
Qatar Living Forum + Ext includes the Qatar Liv-
ing Forum dataset, i.e., the forums, but also some
other sections of Qatar Living: Jobs, Classiﬁeds,
Pages, Wiki and Events posts.
Doha News is a dataset that we built by crawl-
ing about 7,000 news publications about the life in
Doha, Qatar from the DohaNews website.4

We also used an out-of-domain, general model,
readily-pretrained using Word2Vec on Google
News,5 as provided by Mikolov et al. (2013).

3www.qatarliving.com is an online community for

everyone living in or interested in the State of Qatar.

4dohanews.co covers breaking news, politics, business,

culture and more in and around Qatar.

5code.google.com/archive/p/word2vec/

Features
Qatar Living Forum
Qatar Living Forum+Ext
Google News
Doha News

Train size Vocab
104K
126K
3M
17K

61.84M
90M
100B
1.45M

Table 1: Data used for training word embedding vectors.

Shown are training source size (word tokens) and vocabulary

size (word types).

4 Method

Below we focus our explanation on subtask A; for
subtask C, we combine the predictions for subtask
A with the Google’s reciprocal rank for the related
question (see below).

We approach subtask A as a classiﬁcation prob-
lem. For each comment, we extract variety of fea-
tures from both the question and the comment, and
we train a classiﬁer to label comments as Good or
Bad with respect to the thread question. We rank the
comments in each question according to the classi-
ﬁer’s score of being classiﬁed as Good with respect
to the question.
We ﬁrst

train several word embedding vector
models and we ﬁne-tune them using different con-
ﬁgurations. For ﬁne-tuning the parameters of the
word embeddings training conﬁguration, we setup a
simple baseline system and we evaluate it on the of-
ﬁcial MAP score. We then use the best-performing
embeddings in our further experiments. Our main
features are semantic similarity based on word em-
beddings and topics, but we also use some metadata
features.

4.1 Preprocessing

Before extracting features, we preprocessed the in-
put
text using several steps. We ﬁrst replaced
URLs in text with TOKEN URL, numbers with TO-
KEN NUM, images with TOKEN IMG, and emoti-
cons with TOKEN EMO. We then tokenized the text
by matching only continuous alphabet characters in-
cluding (underscore). Next, we lowercased the re-
sult. For the training, the development, and the
test datasets, we removed the stopwords using the
English stopwords lexicon from the NLTK toolkit
(Bird and Loper, 2004).

4.2 Features

We used several semantic vector similarity and
metadata feature groups. For the similarity measures
mentioned below, we used cosine similarity:

(1)

1 −

u.v
kuk . kvk
Semantic Word Embeddings. We used seman-
tic word embeddings obtained from Word2Vec mod-
els trained on different unannotated data sources in-
cluding the QatarLiving and DohaNews. We also
used a model pre-trained on Google News text. For
each piece of text such as comment text, question
body and question subject, we constructed the cen-
troid vector from the vectors of all words in that text
(excluding stopwords).

wi

n
P
i=1
n

centroid(w1..n) =

(2)

We built centroid vectors (2) from the question
body and the comment text. We then examined dif-
ferent Word2Vec models in terms of training source
and training conﬁguration including word vector
size, training window size, minimum word occur-
rence in the corpus, and number of skip-grams.

Semantic Vector Similarities. We used vari-
ous similarity features calculated using the centroid
word vectors on the question body, on the question
subject and on the comment text, as well as on parts
thereof:

Question to Answer similarity. We assume that
a relevant answer should have a centroid vector that
is close to that for the question. We used the ques-
tion body to comment text, and question subject to
comment text vector similarities.

Maximized similarity. We ranked each word in
the answer text to the question body centroid vector
according to their similarity and we took the average
similarity of the top N words. We took the top 1,2,3
and 5 words similarities as features. The assumption
here is that if the average similarity for the top N
most similar words is high, then the answer might
be relevant.

Aligned similarity. For each word in the question
body, we chose the most similar word from the com-
ment text and we took the average of all best word
pair similarities as suggested in (Tran et al., 2015).

Part of speech (POS) based word vector similar-
ities. We performed part of speech tagging using
the Stanford tagger (Toutanova et al., 2003), and we
took similarities between centroid vectors of words
with a speciﬁc tag from the comment text and the
centroid vector of the words with a speciﬁc tag from
the question body text. The assumption is that some
parts of speech between the question and the com-
ment might be closer than other parts of speech.

Word clusters (WC) similarity. We clustered
the word vectors from the Word2Vec vocabulary
in 1,000 clusters (with 200 words per cluster on
average) using K-Means clustering. We then cal-
culated the cluster similarity between the question
body word clusters and the answer text word clus-
ters. For all experiments, we used clusters obtained
from the Word2Vec model trained on QatarLiving
forums with vector size of 100, window size 10,
minimum words frequency of 5, and skip-gram 1.

LDA topic similarity. We performed topic
clustering
using Latent Dirichlet Allocation
(LDA) as implemented in the gensim toolkit
( ˇReh˚uˇrek and Sojka, 2010) on Train1+Train2+Dev
questions and comments. We built topic models
with 100 topics. For each word in the question body
and for the comment text, we built a bag-of-topics
with corresponding distribution, and calculated sim-
ilarity. The assumption here is that if the question
and the comment share similar topics, they are more
likely to be relevant to each other.

Metadata.

In addition to the semantic features
described above, we also used some common sense
metadata features:

Answer contains a question mark. If the com-
ment has an question mark, it may be another ques-
tion, which might indicate a bad answer.

Answer length. The assumption here is that

longer answers could bring more useful detail.

Question length. If the question is longer, it may
be more clear, which may help users give a more
relevant answer.

Question to comment length. If the question is
long and the answer is short, it may be less relevant.
The answer’s author is the same as the corre-
sponding question’s author. If the answer is posted
by the same user who posted the question and it is
relevant, why has he/she asked the question in the
ﬁrst place?

Answer rank in the thread. Earlier answers could
be posted by users who visit the forum more often,
and they may have read more similar questions and
answers. Moreover, discussion in the forum tends to
diverge from the question over time.

Question category. We took the category of the
question as a sparse binary feature vector (a feature
with a value of 1 appears if question is in the cat-
egory). The assumption here is that the question-
comment relevance might depend on the category of
the question.

4.3 Classiﬁer

For each Question+Comment pair, we extracted the
features explained above from the Question body
and the subject text ﬁelds, and from the Comment
text; we also extracted the relevant metadata. We
concatenated the extracted features in a bag of fea-
tures vector, scaling them in the 0 to 1 range, and
feeding them to a classiﬁer. In our experiments, we
used different feature conﬁgurations. We used L2-
regularized logistic regression classiﬁer as imple-
mented in Liblinear (Fan et al., 2008). For most of
our experiments, we tuned the classiﬁer with differ-
ent values of the C (cost) parameter, and we took the
one that yielded the best accuracy on 5-fold cross-
validation on the training set. We used binary clas-
siﬁcation Good vs. Bad (including both Bad and
Potentially Useful original labels). The output of the
evaluation for each test example was a label, either
Good or Bad, and the probability of being Good in
the 0 to 1 range. We then used this output proba-
bility as a relevance rank for each Comment in the
Question thread.

5 Experiments and Evaluation

As explained above, we rely mainly on semantic fea-
tures extracted from Word2Vec word embeddings.
Thus, we ran several experiments looking for the
best embeddings for the task.

Table 2 shows experiments with Word2Vec mod-
els trained on the unannotated datasets described
above. The Google News Word2Vec model comes
pretrained with vector size of 300, window 10, min-
imum word frequency of 10 and skip-gram 1. We
started with training our three Word2Vec models us-
ing the same parameters.

Dataset
Qatar Living Forum
Qatar Living Forum+Ext
Google News
Doha News

Dev2016
MAP Accuracy
0.6311
0.6269
0.6113
0.5769

0.7078
0.7131
0.6996
0.6844

Table 2: Semantic vectors trained on different unanno-

tated datasets as the only features for subtask A: training

on train2016-part1, testing on dev2016.

Test2016

Vector size MAP Accuracy
78.45
78.12
77.31
77.61
78.36
77.25
77.90
77.08
77.22
75.44
59.53
Table 3: Semantic vectors of different vector sizes, trained

800
700
600
500
400
300
200
100
50
20
Baseline

74.22
73.98
73.15
73.30
74.19
74.50
73.88
74.53
73.85
72.42
-

on Qatar Living Forum+Ext as features for subtask A (to-

gether with all other features): training on train2016-part1,

testing on test2016.

Table 2 shows results using raw word vectors as
features, together with an extra feature for question
body to comment cosine similarity. We can see that
training on Qatar Living Forum data performs best
followed by using Qatar Living Forum+Ext, Google
News, and Doha News. This is not surprising as the
ﬁrst two datasets are in-domain, while the latter two
cover more topics (as they are news) and more for-
mal language. Overall, Doha News contains topics
that largely overlap with the topics discussed in the
Qatar Living forum; yet, it uses more formal lan-
guage and contains very little conversational word
types (mostly in quotations and interviews); more-
over, being smaller in size, it covers much less vo-
cabulary. Based on these preliminary experiments
on Dev2016, we concluded that the domain-speciﬁc
word vectors trained on Qatar Living Forum were
the best for this task, and we used them further in
our experiments.

After we have selected the best dataset for training
our semantic vectors, we continued with various ex-
periments to select the best training parameters for
Word2Vec. Below we present the results of these
experiments on Test2016, but we experimented with
Dev2016 when developing our system.

In Table 3, we present experiments with different
vector sizes. We trained our classiﬁer with all fea-
tures mentioned above, extracted for the correspond-
ing word vector model. We can see that word vectors
of size 800 perform best followed by sizes 400 and
700. However, we should note that using word vec-
tors of size 800 generates more than 1,650 features
(800+800+other features), which slows down train-
ing and evaluation. Moreover, in our experiments,
we noticed that using large word vectors blurs the
impact of the other, non-vector features.

Thus, next we tried to achieve the MAP for
the 800-size vector by using better parameters for
smaller vector sizes. Table 4 shows the results,
where we used vectors of size 100 and 200. We
can see that the conﬁguration with word vector size
200, window size 5, minimum word frequency 1 and
skip-gram 3 performed best improving the 200 vec-
tors MAP by 0.31 (compared to Table 3). However,
the experiments with word vector size 100 improved
its MAP score by 0.85, which suggests that there
might be potential for improvement when using vec-
tors of smaller size. We also tried to use Doc2Vec
(Le and Mikolov, 2014) instead of Word2Vec, but
this led to noticeably lower performance.

We further experimented with Word2Vec models
trained with different conﬁgurations and different
feature groups. Tables 5 and 6 show the results for
ablation experiments using the best-performing con-
ﬁguration for Subtask A and C, respectively.

For Subtask A we achieved the best score with
semantic vectors of size 200, trained with window
size 5, minimum word frequency 1 and skip-grams
3. The best score we achived (MAP 78.52) is
slightly hbetter than the best score from Table 3
(MAP 78.45), which means that it may be a good
idea to use smaller word vectors in combination with
other features. We can see that the features that con-
tribute most (the bottom features are better) are the
raw word centroid vectors and metadata features,
followed by various similarities such as LDA topic
similarity and POS-tagged-word similarity.

Test2016

Size Window Freq Skip MAP Acc
74.25
200
73.49
200
74.01
200
74.53
200
74.19
100
73.88
200
73.94
100
74.43
100
74.25
200
74.07
100
73.73
200
73.79
100
74.53
100

78.21
78.19
78.13
78.01
77.93
77.90
77.81
77.72
77.58
77.53
77.43
77.18
77.08

1
5
5
1
1
5
1
1
1
5
1
10
5

5
5
5
5
5
10
5
10
10
5
10
10
10

3
1
3
1
1
1
3
1
1
1
3
1
1

Table 4: Exploring Word2Vec training parameters on Qatar

Living Forum+Ext: word vector size (Size), context window

(Window), minimum word frequency (Freq), and skip-grams

(Skip). Vectors used as features for subtask A (together with all

other features): training on train2016-part1, testing on test2016.

Train2016-part1 as training
Features
All − Quest. to Comment sim 78.52
78.38
All − Maximized similarity
78.29
All − Word Clusters similarity
78.22
All − WC sim & Meta cat
78.21
All − Meta categories
78.21
All
All − Meta cat & LDA sim
78.18
All − Ext POS sim & WC sim 78.10
77.97
All − Aligned similarity
77.95
All − Cat & WC & LDA sim
77.92
All − WC & LDA sims
77.92
All − Ext POS sim
77.85
All − LDA sim
77.77
All − POS sim
74.50
All − Metadata full
74.35
All − Word Vectors
77.58
Primary
77.16
Contrastive 1
75.41
Contrastive 2
59.53
Baseline (IR)

Test2016
MAP Acc
74.31
74.59
74.25
74.04
74.25
74.25
73.88
74.28
74.16
74.19
74.25
74.43
74.37
74.80
70.31
70.80
73.39
73.88
72.26
–

Table 5: Subtask A. Using all features without some feature

groups. Word2Vec is trained with word vector size 200, context

window 5, minimum word frequency 1, and skip-grams 3.

Test-2016
Train2016-part1 as training
MAP Acc
Features
53.39
69.87
All − Q to C sim
69.81
53.06
All − Meta categories
69.54
52.91
All − WC sim & Meta cat
70.06
52.84
All − WC sim & LDA sim
69.87
All − Meta cat & LDA sim
52.83
70.21
All − Ext POS sim & WC sim 52.82
69.43
52.78
All
70.10
52.76
All − Aligned similarity
69.63
52.58
All − Word Clusters similarity
69.27
52.47
All − Maximized similarity
69.51
52.44
All − Cat & WC & LDA sim
69.91
52.23
All − Exr POS sim
69.97
52.08
All − LDA sim
69.96
51.57
All − POS sim
70.13
49.57
All − Word Vectors
71.06
46.03
All − Metadata full
69.94
51.68
Primary
69.69
51.46
Contrastive 1
69.71
48.76
Contrastive 2
–
28.88
Baseline (IR)

Table 6: Subtask C. Using all features without some feature

groups. Word2Vec is trained with word vector size 100, context

window 5, minimum word frequency 1, and skip-grams 1.

For Subtask C, we achieved the best score with
vectors of size 100, trained with window size 5, min-
imum word frequency 1, and skip-grams 1. The fea-
tures that contributed most were mostly the same as
for Subtask A. One difference is the maximized sim-
ilarity features group, which now yields worse re-
sults when excluded, which indicates its importance.
Our Primary, Contrastive 1 and Contrastive 2
submissions were built with the same feature set: All
features - POS similarity & Meta Category, but were
trained with ﬁxed C=0.55 on different datasets: Pri-
mary was trained on Train2016-part1, Contrastive 1
was trained on Train2016-part1 + Train2016-part2,
and Contrastive 2 was trained on Train2016-part2.

6 Conclusion and Future Work

We have described our system for SemEval-2016,
Task 3 on Community Question Answering. Our
approach relied on several semantic similarity fea-
tures based on ﬁne-tuned word embeddings and top-
ics similarities.

In the main Subtask C, our primary submission
was ranked third, with a MAP of 51.68 and accu-
racy of 69.94. In Subtask A, our primary submission
was also third, with MAP of 77.58 and accuracy of
73.39. After the submission deadline, we improved
our MAP score to 78.52 for Subtask A, and to 53.39
for Subtask C, which would rank our system second.
In future work, we plan to use our best per-
forming word embeddings models and features
in a deep learning architecture,
in
the MTE-NN system (Guzm´an et al., 2016a;
Guzm´an et al., 2016b), which borrowed an entire
neural network framework and achitecture from
previous work on machine translation evaluation
(Guzm´an et al., 2015). We also want to incorporate
several rich knowledge sources, e.g., as in the
SUper Team system (Mihaylova et al., 2016),
including troll user
inspired by
Mihaylov et al., 2015b;
(Mihaylov et al., 2015a;
Mihaylov and Nakov, 2016), and PMI-based good-
ness polarity lexicons as in the PMI-cool system
(Balchev et al., 2016), as well as sentiment polarity
features (Nicosia et al., 2015).

features

e.g.,

as

as

We further plan to use information from entire
threads to make better predictions, as using thread-
level information for answer classiﬁcation has al-
ready been shown useful for SemEval-2015 Task
3, subtask A, e.g., by using features modeling the
thread structure and dialogue (Nicosia et al., 2015;
Barr´on-Cede˜no et al., 2015), or by applying thread-
level inference using the predictions of local classi-
ﬁers (Joty et al., 2015; Joty et al., 2016). How to use
such models efﬁciently in the ranking setup of 2016
is an interesting research question.

Finally, we would like to address subtask C in a
more solid way, making good use of the data, the
gold annotations, the features, the models, and the
predictions for subtasks A and B.

Acknowledgments. This work is partly sup-
ported by the German Research Foundation as part
of the Research Training Group “Adaptive Prepara-
tion of Information from Heterogeneous Sources”
(AIPHES) under grant No. GRK 1994/1. It is also
part of the Interactive sYstems for Answer Search
(Iyas) project, which is developed by the Arabic
Language Technologies (ALT) group at the Qatar
Computing Research Institute, HBKU, part of Qatar
Foundation in collaboration with MIT-CSAIL.

References

[Balchev et al.2016] Daniel Balchev, Yasen Kiprov, Ivan
Koychev, and Preslav Nakov.
2016. PMI-cool at
SemEval-2016 Task 3: Experiments with PMI and
goodness polarity lexicons for community question
In Proceedings of the 10th International
answering.
Workshop on Semantic Evaluation, SemEval ’16, San
Diego, California, USA.

[Barr´on-Cede˜no et al.2015] Alberto Barr´on-Cede˜no, Si-
mone Filice, Giovanni Da San Martino, Shaﬁq Joty,
Llu´ıs M`arquez, Preslav Nakov, and Alessandro Mos-
chitti. 2015. Thread-level information for comment
In
classiﬁcation in community question answering.
Proceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th Inter-
national Joint Conference on Natural Language Pro-
cessing, ACL-IJCNLP ’15, pages 687–693, Beijing,
China.

[Belinkov et al.2015] Yonatan Belinkov, Mitra Mo-
htarami, Scott Cyphers, and James Glass.
2015.
VectorSLU: A continuous word vector approach to
answer selection in community question answering
In Proceedings of the 9th International
systems.
Workshop on Semantic Evaluation, SemEval ’15,
pages 282–287, Denver, Colorado, USA.

[Bird and Loper2004] Steven Bird and Edward Loper.
In The
2004. NLTK: The natural language toolkit.
Companion Volume to the Proceedings of 42st Annual
Meeting of the Association for Computational Linguis-
tics, pages 214–217, Barcelona, Spain.

[Fan et al.2008] Rong-En Fan, Kai-Wei Chang, Cho-Jui
Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008.
LIBLINEAR: A library for large linear classiﬁcation.
Journal of Machine Learning Research, 9:1871–1874.
[Guzm´an et al.2015] Francisco Guzm´an, Shaﬁq Joty,
Llu´ıs M`arquez, and Preslav Nakov. 2015. Pairwise
In Proceed-
neural machine translation evaluation.
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Process-
ing (Volume 1: Long Papers), ACL-IJCNLP ’15, pages
805–814, Beijing, China.

[Guzm´an et al.2016a] Francisco Guzm´an, Llu´ıs M`arquez,
and Preslav Nakov. 2016a. Machine translation eval-
uation meets community question answering. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ’16, Berlin,
Germany.

[Guzm´an et al.2016b] Francisco Guzm´an, Llu´ıs M`arquez,
and Preslav Nakov. 2016b. MTE-NN at SemEval-
2016 Task 3: Can machine translation evaluation help
In Proceedings of
community question answering?

the 10th International Workshop on Semantic Evalua-
tion, SemEval ’16, San Diego, California, USA.
[Joty et al.2015] Shaﬁq Joty, Alberto Barr´on-Cede˜no,
Giovanni Da San Martino, Simone Filice, Llu´ıs
M`arquez, Alessandro Moschitti, and Preslav Nakov.
2015. Global thread-level inference for comment clas-
siﬁcation in community question answering. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’15,
pages 573–578, Lisbon, Portugal.

[Joty et al.2016] Shaﬁq Joty, Llu´ıs M`arquez, and Preslav
Nakov. 2016.
Joint learning with global inference
for comment classiﬁcation in community question an-
swering. In Proceedings of the 2016 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, NAACL-HLT ’16, San Diego, California, USA.
[Le and Mikolov2014] Quoc V. Le and Tomas Mikolov.
2014. Distributed representations of sentences and
documents. CoRR, abs/1405.4053.

[Mihaylov and Nakov2016] Todor Mihaylov and Preslav
Nakov. 2016. Hunting for troll comments in news
community forums. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguis-
tics, ACL ’16, Berlin, Germany.

[Mihaylov et al.2015a] Todor

Mihaylov,

Georgi
Finding
Georgiev, and Preslav Nakov.
opinion manipulation trolls
in news community
forums. In Proceedings of the Nineteenth Conference
on Computational Natural Language Learning,
CoNLL ’15, pages 310–314, Beijing, China.

2015a.

[Mihaylov et al.2015b] Todor Mihaylov, Ivan Koychev,
Georgi Georgiev, and Preslav Nakov. 2015b. Ex-
posing paid opinion manipulation trolls. In Proceed-
ings of the International Conference Recent Advances
in Natural Language Processing, RANLP ’15, pages
443–450, Hissar, Bulgaria.

[Mihaylova et al.2016] Tsvetomila Mihaylova,

Pepa
Gencheva, Martin Boyanov, Ivana Yovcheva, Todor
Mihaylov, Momchil Hardalov, Yasen Kiprov, Daniel
Balchev,
Ivelina
Ivan Koychev, Preslav Nakov,
Nikolova, and Galia Angelova. 2016. SUper Team at
SemEval-2016 Task 3: Building a feature-rich system
In Proceedings
for community question answering.
of
the 10th International Workshop on Semantic
Evaluation, SemEval ’16, San Diego, California,
USA.

[Mikolov et al.2013] Tomas Mikolov, Wen-tau Yih, and
Geoffrey Zweig. 2013. Linguistic regularities in con-
tinuous space word representations. In Proceedings of
the 2013 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies, NAACL-HLT ’13, pages
746–751, Atlanta, Georgia, USA.

answer validation based on lexical and distance fea-
tures. In Proceedings of the 9th International Work-
shop on Semantic Evaluation, SemEval ’15, pages
242–246, Denver, Colorado, USA.

[Nakov et al.2015] Preslav Nakov, Llu´ıs M`arquez, Walid
Magdy, Alessandro Moschitti, Jim Glass, and Bilal
Randeree. 2015. SemEval-2015 task 3: Answer se-
In Pro-
lection in community question answering.
ceedings of the 9th International Workshop on Seman-
tic Evaluation, SemEval ’15, pages 269–281, Denver,
Colorado, USA.

[Nakov et al.2016] Preslav Nakov,

Llu´ıs M`arquez,
Alessandro Moschitti, Walid Magdy, Hamdy
Mubarak, Abed Alhakim Freihat, Jim Glass, and
Bilal Randeree. 2016. SemEval-2016 task 3: Com-
In Proceedings of the
munity question answering.
10th International Workshop on Semantic Evaluation,
SemEval ’16, San Diego, California, USA.

[Nicosia et al.2015] Massimo Nicosia, Simone Filice, Al-
berto Barr´on-Cede˜no, Iman Saleh, Hamdy Mubarak,
Wei Gao, Preslav Nakov, Giovanni Da San Mar-
tino, Alessandro Moschitti, Kareem Darwish, Llu´ıs
M`arquez, Shaﬁq Joty, and Walid Magdy. 2015. QCRI:
Answer selection for community question answering -
experiments for Arabic and English. In Proceedings of
the 9th International Workshop on Semantic Evalua-
tion, SemEval ’15, pages 203–209, Denver, Colorado,
USA.

[ ˇReh˚uˇrek and Sojka2010] Radim ˇReh˚uˇrek and Petr Sojka.
2010. Software Framework for Topic Modelling with
In Proceedings of the LREC 2010
Large Corpora.
Workshop on New Challenges for NLP Frameworks,
pages 45–50, Valletta, Malta.

[Toutanova et al.2003] Kristina Toutanova, Dan Klein,
Christopher D. Manning, and Yoram Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proceedings of the 2003 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology - Volume 1, NAACL-HLT ’03,
pages 173–180, Edmonton, Canada.

[Tran et al.2015] Quan Hung Tran, Vu Tran, Tu Vu, Minh
Nguyen, and Son Bao Pham. 2015. JAIST: Combin-
ing multiple features for answer selection in commu-
In Proceedings of the 9th
nity question answering.
International Workshop on Semantic Evaluation, Se-
mEval ’15, pages 215–219, Denver, Colorado, USA.
[Vo et al.2015] Ngoc Phuoc An Vo, Simone Magnolini,
and Octavian Popescu. 2015. FBK-HLT: An applica-
tion of semantic textual similarity for answer selection
in community question answering. In Proceedings of
the 9th International Workshop on Semantic Evalua-
tion, SemEval ’15, pages 231–235, Denver, Colorado,
USA.

[Zamanov et al.2015] Ivan Zamanov, Marina Kraeva,
Nelly Hateva, Ivana Yovcheva, Ivelina Nikolova, and
Galia Angelova. 2015. Voltron: A hybrid system for

