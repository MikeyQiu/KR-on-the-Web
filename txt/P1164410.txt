9
1
0
2
 
n
a
J
 
2
1
 
 
]

V
C
.
s
c
[
 
 
1
v
2
1
9
3
0
.
1
0
9
1
:
v
i
X
r
a

Real-time Joint Object Detection and Semantic
Segmentation Network for Automated Driving

Ganesh Sistu1, Isabelle Leang2 and Senthil Yogamani1
1: Valeo Vision Systems, Ireland
2: Valeo Bobigny, France
{ganesh.sistu,isabelle.leang,senthil.yogamani}@valeo.com

Abstract

Convolutional Neural Networks (CNN) are successfully used for various visual
perception tasks including bounding box object detection, semantic segmentation,
optical ﬂow, depth estimation and visual SLAM. Generally these tasks are indepen-
dently explored and modeled. In this paper, we present a joint multi-task network
design for learning object detection and semantic segmentation simultaneously.
The main motivation is to achieve real-time performance on a low power embedded
SOC by sharing of encoder for both the tasks. We construct an efﬁcient archi-
tecture using a small ResNet10 like encoder which is shared for both decoders.
Object detection uses YOLO v2 like decoder and semantic segmentation uses
FCN8 like decoder. We evaluate the proposed network in two public datasets
(KITTI, Cityscapes) and in our private ﬁsheye camera dataset, and demonstrate
that joint network provides the same accuracy as that of separate networks. We
further optimize the network to achieve 30 fps for 1280x384 resolution image.

1

Introduction

Convolutional neural networks (CNNs) have became the standard building block for majority of visual
perception tasks in autonomous vehicles. Bounding box object detection is one of the ﬁrst successful
applications of CNN for detecting pedestrians and vehicles. Recently semantic segmentation is
getting mature [12] starting with detection of roadway objects like road, lanes, curb, etc. In spite of
rapid progress in computational power of embedded systems and trend in specialized CNN hardware
accelerators, real-time performance of semantic segmentation at high accuracy is still challenging. In
this paper, we propose a real-time joint network of semantic segmentation and object detection which
cover all the critical objects for automated driving. The rest of the paper is structured as follows.
Section 2 reviews the object detection application in automated driving and provides motivation to
solve it using a multi-task network. Section 3 details the experimental setup, discusses the proposed
architecture and experimental results. Finally, section 4 summarizes the paper and provides potential
future directions.

2 Multi-task learning in Automated Driving

Joint learning of multiple tasks falls under the sub-branch of machine learning called multitask
learning. The underlying theory behind joint learning of multiple tasks is that the networks can
perform better when trained on multiple tasks as they learn rules of the game faster by leveraging the
inter task disciplines. These networks networks are not only better at generalization but also reduces
the computational complexity making them very effective for low power embedded systems. Recent
progress has shown that CNN can be used for various tasks [6] including moving object detection
[13], depth estimation [8] and visual SLAM [9].

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

Our work is closest to the recent MultiNet [14]. We differ by focusing on a smaller network, larger
set of classes for the two tasks and more extensive experiments in three datasets.

2.1

Important Objects for Automated Driving

Figure 1: Multi-task visual perception network architecture

The popular semantic segmentation automotive datasets are CamVid [1] and the more recent
Cityscapes [3]. The latter has a size of 5000 annotation frames which is relatively small. The
algorithms trained on this dataset do not generalize well to data tested on other cities and with unseen
objects like tunnels. To compensate for that, synthetic datasets like Synthia [11] and Virtual KITTI
[4] were created. There is some literature which demonstrates that a combination produces reasonable
results in small datasets. But they are still limited for a commercial deployment of an automated
driving system. Hence there is a recent effort to build larger semantic segmentation datasets like
Mapillary Vistas dataset [10] and ApolloScape [7]. Mapillary dataset comprises of 25,000 images
with 100 classes. ApolloScape dataset comprises of 143,000 images with 50 classes.

Although semantic segmentation can be used as a uniﬁed model for detecting all the objects, there
are many issues with this approach. From the application perspective, it is essential to have instances
of objects for tracking and path planning. This can be obtained by instance segmentation but it
uses bounding box detector as the ﬁrst step and it is relatively less mature. Annotation for semantic
segmentation is time consuming, typically it can take around an hour for annotating a single image
which makes it challenging for collecting large datasets. The sample complexity of key bounding
box objects like pedestrians and vehicles is much higher compared to road or lanes. Typically legacy
datasets exist for these objects using bounding box annotation. It is also relatively easier to do corner
case mining for bounding box detectors. Thus there is a need to use both semantic segmentation
and object detection models. Additionally, the computational power of embedded system needed
for deployment is still a bottleneck. Thus we focus our development on critical object classes. The
ﬁrst important group of classes is roadway objects. They represent the static infrastructure namely
road, lane and curb. They cannot be represented by a bounding box and thus need segmentation task.
The second group of classes is the dynamic objects which are vehicles, pedestrians and cyclists. This
group is particularly important as they are vulnerable road users.

2.2 Pros and Cons of MTL

In this paper, we propose a network architecture with shared encoder which can be jointly learned.
The main advantages are increased efﬁciency, scalability to add more tasks leveraging previous
features and better generalization through inductive transfer (learning transferable features for tasks).
We address the pros/cons of shared network in more details below.

2

Pros of shared network:

• Computational efﬁciency: The simple and easy to understand intuition behind shared
features is improving computational efﬁciency. Say there are 2 classes and 2 independent
networks taking 50% of processing power individually. If there is a possibility of say 30%
sharing across the 2 networks, each network can re-use the additional 15% to do a slightly
larger network individually. There is plenty of empirical evidence to show that the initial
layers of the network are task-independent (oriented Gabor ﬁlters) and we should be able to
do some level of sharing, more the better.

• Generalization and accuracy: Ignoring computational efﬁciency, the networks which are
jointly learnt tend to generalize better and be more accurate. This is why transfer learning on
imagenet is very popular where there network learns very complex classes like differentiating
between speciﬁc species of dogs. Because the subtle differences between say two species
say Labrador or Pomeranian are learnt, they are better at detecting a simpler task of dog
detection. Another argument is that there is less possibility of over-ﬁtting for a particular
task when they are jointly learnt.

• Scalability to more tasks like ﬂow estimation, depth, correspondence, and tracking. Thus a

common CNN feature pipeline can be harmonized to be used for various tasks.

Cons of shared network:

• In case of non-shared network, the algorithms are completely independent. This could make
the dataset design, architecture design, tuning, hard negative mining, etc simpler and easier
to manage.

• Debugging a shared network (especially when it doesn’t work) is harder relatively.

Figure 2: Multi-task visual perception network output. Segmentation color coding is green for road
and pink for sidewalk

3 Proposed Algorithm and Results

3.1 Network Architecture

In this section, we report results on a baseline network design which we plan to improve upon.
We propose a jointly learnable shared encoder network architecture provided in the high level
block diagram in Figure 1. We implemented a two task network with 3 segmentation classes
(background, road, sidewalk) and 3 object classes (car, person, cyclist). To enable feasibility on
a low power embedded system, we used a small encoder namely Resnet10 which is fully shared
for the two tasks. FCN8 is used as the decoder for semantic segmentation and YOLO is used as
the decoder for object detection. Loss function for semantic segmentation is the cross entropy loss
to minimize misclassiﬁcation. For geometric functions, average precision of object localization is
used as error function in the form of squared error loss. We use a weighted sum of individual losses
L = wseg ∗ Lseg + wdet ∗ Ldet for the two tasks. In case of ﬁsheye cameras which have a large
spatially variant distortion, we implemented a lens distortion correction using a polynomial model.

3

Table 1: Comparison Study : Single task vs Multi-task

Databases

KITTI Seg

KITTI Det

Cityscapes Seg

Cityscapes Det

Our own Seg

Our own Det

Metrics
JI background
JI road
JI sidewalk
mean IOU
AP car
AP person
AP cyclist
mean AP

JI road
JI sidewalk
JI building
JI vegetation
JI sky
JI person+rider
JI car
JI bicycle
mean IOU
AP car
AP person
AP bicycle
mean AP

JI background
JI road
JI lane
JI curb
mean IOU
AP car
AP person
mean AP

STL Seg
0.9706
0.8603
0.6387
0.8232

0.9045
0.5434
0.7408
0.8085
0.7544
0.3916
0.695
0.3906
0.5971

0.9712
0.9318
0.6067
0.506
0.7702

STL Det

0.801
0.469
0.5398
0.6033

0.3691
0.1623
0.1279
0.2198

0.5556
0.3275
0.4415

MTL
0.9621
0.8046
0.5045
0.757
0.7932
0.5337
0.4928
0.6066

0.8273
0.3658
0.6363
0.6949
0.6228
0.3225
0.5237
0.2911
0.4918
0.411
0.1931
0.1898
0.2647

0.9346
0.872
0.4032
0.1594
0.606
0.564
0.3639
0.4639

MTL10
0.9663
0.8418
0.5736
0.7939
0.7746
0.518
0.5107
0.6011

0.8497
0.4223
0.6737
0.7417
0.652
0.3218
0.5918
0.4123
0.5213
0.398
0.1694
0.1422
0.2365

0.9358
0.871
0.4426
0.2554
0.6419
0.554
0.2971
0.4255

MTL100
0.9673
0.8565
0.6277
0.8172
0.7814
0.468
0.5844
0.6112

0.8815
0.4335
0.6947
0.7363
0.6873
0.3613
0.6579
0.3506
0.5555
0.3711
0.1845
0.1509
0.2355

0.9682
0.9254
0.594
0.4507
0.7527
0.5571
0.3609
0.459

3.2 Experiments

In this section, we explain the experimental setting including the datasets used, training algorithm
details, etc and discuss the results. We trained and evaluated on our internal ﬁsheye dataset comprising
of 5000 images and two publicly available datasets, KITTI [5] and Cityscapes [3]. We implemented
the different proposed multi-task architecture using Keras [2]. We used pre-trained Resnet10 encoder
weights from ImageNet, which is then ﬁne-tuned for the two tasks. The FCN8 upsampling layers
are initialized using random weights. We used ADAM optimizer as it provided faster convergence,
with a learning rate of 0.0005. Categorical cross-entropy loss and squared errors loss are used as
loss function for the optimizer. Mean class IoU (Intersection over Union) and per-class IoU were
used as accuracy metrics for semantic segmentation, mean average precision (mAP) and per-class
average precision for object detection. All input images were resized to 1280x384 because of memory
requirements needed for multiple tasks. Table 1 summarizes the obtained results for STL networks
and MTL networks on KITTI, Cityscapes and our internal ﬁsheye datasets. This is intended to provide
a baseline accuracy for incorporating more complex multi-task learning techniques. We compare a
segmentation network (STL Seg) and a detection network (STL Det) to a MTL network performing
segmentation and detection (MTL, MTL10 and MTL100). We tested 3 conﬁgurations of the MTL loss,
the ﬁrst one (MTL) uses a simple sum of the segmentation loss and detection loss (wseg = wdet = 1).
The two other conﬁgurations MTL10 and MTL100, use a weighted sum of the task losses where
the segmentation loss is weighted with a weight wseg = 10 and wseg = 100 respectively. This
compensates the difference of task loss scaling: the segmentation loss is 10-100 times higher than the
detection loss during the training. Such a weighting in MTL network improves the performance of the
segmentation task for the 3 datasets. Even if the MTL results are slighty lower than the STL results
for the segmentation task, this experiment shows that the multi task network has the capacity to learn
more by tuning correctly the parameters. Moreover, by keeping almost the same accuracy, we have a
drastic gain in memory and computational efﬁciency. We make use of several standard optimization
techniques to further improve runtime and achieve 30 fps on an automotive grade low power SOC.
Some examples are (1) Reducing number of channels in each layer, (2) Reducing number of skip
connections for memory efﬁciency and (3) Restricting segmentation decoder to image below horizon
line (only for roadway objects).

4 Conclusion

In this paper, we discussed the application of multi-task learning in an automated driving setting for
joint semantic segmentation and object detection tasks. Firstly, we motivated the need to do both tasks

4

instead of just semantic segmentation. Then we discussed the pros and cons of using a multi-task
approach. We constructed an efﬁcient joint network by careful choice of encoders and decoders and
further optimize it to achieve 30 fps on a low-power embedded system. We shared experimental
results on three datasets demonstrating the efﬁciency of joint network. In future work, we plan to
explore addition of visual perception tasks like depth estimation, ﬂow estimation and Visual SLAM.

References

[1] Gabriel J Brostow, Jamie Shotton, Julien Fauqueur, and Roberto Cipolla. Segmentation and recognition
using structure from motion point clouds. In European conference on computer vision, pages 44–57.
Springer, 2008.

[2] François Chollet et al. Keras. https://keras.io, 2015.

[3] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding.
arXiv preprint arXiv:1604.01685, 2016.

[4] Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig. Virtual worlds as proxy for multi-object
tracking analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 4340–4349, 2016.

[5] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti

dataset. The International Journal of Robotics Research, 32(11):1231–1237, 2013.

[6] Jonathan Horgan, Ciarán Hughes, John McDonald, and Senthil Yogamani. Vision-based driver assistance
systems: Survey, taxonomy and advances. In Intelligent Transportation Systems (ITSC), 2015 IEEE 18th
International Conference on, pages 2032–2039. IEEE, 2015.

[7] Xinyu Huang, Xinjing Cheng, Qichuan Geng, Binbin Cao, Dingfu Zhou, Peng Wang, Yuanqing Lin, and
Ruigang Yang. The apolloscape dataset for autonomous driving. arXiv preprint arXiv:1803.06184, 2018.

[8] Varun Ravi Kumar, Stefan Milz, Christian Witt, Martin Simon, Karl Amende, Johannes Petzold, Senthil
Yogamani, and Timo Pech. Near-ﬁeld depth estimation using monocular ﬁsheye camera: A semi-supervised
learning approach using sparse lidar data. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops, Deep Vision: Beyond Supervised learning, 2018.

[9] Stefan Milz, Georg Arbeiter, Christian Witt, Bassam Abdallah, and Senthil Yogamani. Visual slam for
automated driving: Exploring the applications of deep learning. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition Workshops, pages 247–257, 2018.

[10] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas
dataset for semantic understanding of street scenes. In Proceedings of the IEEE International Conference
on Computer Vision, pages 4990–4999, 2017.

[11] German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. The synthia
dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3234–3243, 2016.

[12] Mennatullah Siam, Sara Elkerdawy, Martin Jagersand, and Senthil Yogamani. Deep semantic segmentation
for automated driving: Taxonomy, roadmap and challenges. In Intelligent Transportation Systems (ITSC),
2017 IEEE 20th International Conference on, pages 1–8. IEEE, 2017.

[13] Mennatullah Siam, Heba Mahgoub, Mohamed Zahran, Senthil Yogamani, Martin Jagersand, and Ahmad
El-Sallab. Modnet: Motion and appearance based moving object detection network for autonomous driving.
In 2018 21st International Conference on Intelligent Transportation Systems (ITSC), pages 2859–2864.
IEEE, 2018.

[14] Marvin Teichmann, Michael Weber, Marius Zoellner, Roberto Cipolla, and Raquel Urtasun. Multinet:
Real-time joint semantic reasoning for autonomous driving. In 2018 IEEE Intelligent Vehicles Symposium
(IV), pages 1013–1020. IEEE, 2018.

5

