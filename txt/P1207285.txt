EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples

Pin-Yu Chen1∗, Yash Sharma2∗†, Huan Zhang3†, Jinfeng Yi4‡, Cho-Jui Hsieh3

1AI Foundations Lab, IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA
2The Cooper Union, New York, NY 10003, USA
3University of California, Davis, Davis, CA 95616, USA
4Tencent AI Lab, Bellevue, WA 98004, USA
pin-yu.chen@ibm.com, ysharma1126@gmail.com, ecezhang@ucdavis.edu, jinfengy@us.ibm.com, chohsieh@ucdavis.edu

8
1
0
2
 
b
e
F
 
0
1
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
4
1
1
4
0
.
9
0
7
1
:
v
i
X
r
a

Abstract

Recent studies have highlighted the vulnerability of deep neu-
ral networks (DNNs) to adversarial examples - a visually
indistinguishable adversarial image can easily be crafted to
cause a well-trained model to misclassify. Existing methods
for crafting adversarial examples are based on L2 and L∞
distortion metrics. However, despite the fact that L1 distor-
tion accounts for the total variation and encourages sparsity
in the perturbation, little has been developed for crafting L1-
based adversarial examples.
In this paper, we formulate the process of attacking DNNs via
adversarial examples as an elastic-net regularized optimiza-
tion problem. Our elastic-net attacks to DNNs (EAD) fea-
ture L1-oriented adversarial examples and include the state-
of-the-art L2 attack as a special case. Experimental results on
MNIST, CIFAR10 and ImageNet show that EAD can yield a
distinct set of adversarial examples with small L1 distortion
and attains similar attack performance to the state-of-the-art
methods in different attack scenarios. More importantly, EAD
leads to improved attack transferability and complements ad-
versarial training for DNNs, suggesting novel insights on
leveraging L1 distortion in adversarial machine learning and
security implications of DNNs.

Introduction
Deep neural networks (DNNs) achieve state-of-the-art per-
formance in various tasks in machine learning and artiﬁcial
intelligence, such as image classiﬁcation, speech recogni-
tion, machine translation and game-playing. Despite their
effectiveness, recent studies have illustrated the vulnerabil-
ity of DNNs to adversarial examples (Szegedy et al. 2013;
Goodfellow, Shlens, and Szegedy 2015). For instance, a
carefully designed perturbation to an image can lead a well-
trained DNN to misclassify. Even worse, effective adversar-
ial examples can also be made virtually indistinguishable to
human perception. For example, Figure 1 shows three adver-
sarial examples of an ostrich image crafted by our algorithm,
which are classiﬁed as “safe”, “shoe shop” and “vacuum” by
the Inception-v3 model (Szegedy et al. 2016), a state-of-the-
art image classiﬁcation model.

∗Pin-Yu Chen and Yash Sharma contribute equally to this work.
†This work was done during the internship of Yash Sharma and

Huan Zhang at IBM T. J. Watson Research Center.

‡Part of the work was done when Jinfeng Yi was at AI Founda-

tions Lab, IBM T. J. Watson Research Center.

(a) Image

(b) Adversarial examples with target class labels

Figure 1: Visual illustration of adversarial examples crafted
by EAD (Algorithm 1). The original example is an ostrich
image selected from the ImageNet dataset (Figure 1 (a)).
The adversarial examples in Figure 1 (b) are classiﬁed as
the target class labels by the Inception-v3 model.

The lack of robustness exhibited by DNNs to adversar-
ial examples has raised serious concerns for security-critical
applications, including trafﬁc sign identiﬁcation and mal-
ware detection, among others. Moreover, moving beyond
the digital space, researchers have shown that these adver-
sarial examples are still effective in the physical world at
fooling DNNs (Kurakin, Goodfellow, and Bengio 2016a;
Evtimov et al. 2017). Due to the robustness and security im-
plications, the means of crafting adversarial examples are
called attacks to DNNs. In particular, targeted attacks aim to
craft adversarial examples that are misclassiﬁed as speciﬁc
target classes, and untargeted attacks aim to craft adversarial
examples that are not classiﬁed as the original class. Trans-
fer attacks aim to craft adversarial examples that are trans-
ferable from one DNN model to another. In addition to eval-
uating the robustness of DNNs, adversarial examples can be
used to train a robust model that is resilient to adversarial
perturbations, known as adversarial training (Madry et al.
2017). They have also been used in interpreting DNNs (Koh
and Liang 2017; Dong et al. 2017).

Throughout this paper, we use adversarial examples to
attack image classiﬁers based on deep convolutional neu-
ral networks. The rationale behind crafting effective adver-
sarial examples lies in manipulating the prediction results
while ensuring similarity to the original image. Speciﬁcally,
in the literature the similarity between original and adversar-
ial examples has been measured by different distortion met-
rics. One commonly used distortion metric is the Lq norm,
where (cid:107)x(cid:107)q = ((cid:80)p
i=1 |xi|q)1/q denotes the Lq norm of a

p-dimensional vector x = [x1, . . . , xp] for any q ≥ 1. In
particular, when crafting adversarial examples, the L∞ dis-
tortion metric is used to evaluate the maximum variation
in pixel value changes (Goodfellow, Shlens, and Szegedy
2015), while the L2 distortion metric is used to improve the
visual quality (Carlini and Wagner 2017b). However, despite
the fact that the L1 norm is widely used in problems related
to image denoising and restoration (Fu et al. 2006), as well
as sparse recovery (Cand`es and Wakin 2008), L1-based ad-
versarial examples have not been rigorously explored. In the
context of adversarial examples, L1 distortion accounts for
the total variation in the perturbation and serves as a popular
convex surrogate function of the L0 metric, which measures
the number of modiﬁed pixels (i.e., sparsity) by the pertur-
bation. To bridge this gap, we propose an attack algorithm
based on elastic-net regularization, which we call elastic-
net attacks to DNNs (EAD). Elastic-net regularization is a
linear mixture of L1 and L2 penalty functions, and it has
been a standard tool for high-dimensional feature selection
problems (Zou and Hastie 2005). In the context of attacking
DNNs, EAD opens up new research directions since it gen-
eralizes the state-of-the-art attack proposed in (Carlini and
Wagner 2017b) based on L2 distortion, and is able to craft
L1-oriented adversarial examples that are more effective and
fundamentally different from existing attack methods.

To explore the utility of L1-based adversarial exam-
ples crafted by EAD, we conduct extensive experiments on
MNIST, CIFAR10 and ImageNet in different attack scenar-
ios. Compared to the state-of-the-art L2 and L∞ attacks
(Kurakin, Goodfellow, and Bengio 2016b; Carlini and Wag-
ner 2017b), EAD can attain similar attack success rate when
breaking undefended and defensively distilled DNNs (Pa-
pernot et al. 2016b). More importantly, we ﬁnd that L1 at-
tacks attain superior performance over L2 and L∞ attacks
in transfer attacks and complement adversarial training. For
the most difﬁcult dataset (MNIST), EAD results in improved
attack transferability from an undefended DNN to a defen-
sively distilled DNN, achieving nearly 99% attack success
rate. In addition, joint adversarial training with L1 and L2
based examples can further enhance the resilience of DNNs
to adversarial perturbations. These results suggest that EAD
yields a distinct, yet more effective, set of adversarial ex-
amples. Moreover, evaluating attacks based on L1 distor-
tion provides novel insights on adversarial machine learning
and security implications of DNNs, suggesting that L1 may
complement L2 and L∞ based examples toward furthering
a thorough adversarial machine learning framework.

Related Work
Here we summarize related works on attacking and defend-
ing DNNs against adversarial examples.

Attacks to DNNs
FGM and I-FGM: Let x0 and x denote the original and ad-
versarial examples, respectively, and let t denote the target
class to attack. Fast gradient methods (FGM) use the gradi-
ent ∇J of the training loss J with respect to x0 for craft-
ing adversarial examples (Goodfellow, Shlens, and Szegedy

2015). For L∞ attacks, x is crafted by

x = x0 − (cid:15) · sign(∇J(x0, t)),

(1)

where (cid:15) speciﬁes the L∞ distortion between x and x0, and
sign(∇J) takes the sign of the gradient. For L1 and L2 at-
tacks, x is crafted by

x = x0 − (cid:15)

∇J(x0, t)
(cid:107)∇J(x0, t)(cid:107)q

(2)

for q = 1, 2, where (cid:15) speciﬁes the corresponding distortion.
Iterative fast gradient methods (I-FGM) were proposed in
(Kurakin, Goodfellow, and Bengio 2016b), which iteratively
use FGM with a ﬁner distortion, followed by an (cid:15)-ball clip-
ping. Untargeted attacks using FGM and I-FGM can be im-
plemented in a similar fashion.
C&W attack: Instead of leveraging the training loss, Carlini
and Wagner designed an L2-regularized loss function based
on the logit layer representation in DNNs for crafting adver-
sarial examples (Carlini and Wagner 2017b). Its formulation
turns out to be a special case of our EAD formulation, which
will be discussed in the following section. The C&W attack
is considered to be one of the strongest attacks to DNNs, as it
can successfully break undefended and defensively distilled
DNNs and can attain remarkable attack transferability.
JSMA: Papernot et al. proposed a Jacobian-based saliency
map algorithm (JSMA) for characterizing the input-output
relation of DNNs (Papernot et al. 2016a). It can be viewed as
a greedy attack algorithm that iteratively modiﬁes the most
inﬂuential pixel for crafting adversarial examples.
DeepFool: DeepFool is an untargeted L2 attack algorithm
(Moosavi-Dezfooli, Fawzi, and Frossard 2016) based on the
theory of projection to the closest separating hyperplane in
classiﬁcation. It is also used to craft a universal perturba-
tion to mislead DNNs trained on natural images (Moosavi-
Dezfooli et al. 2016).
Black-box attacks: Crafting adversarial examples in the
black-box case is plausible if one allows querying of the tar-
get DNN. In (Papernot et al. 2017), JSMA is used to train
a substitute model for transfer attacks. In (?), an effective
black-box C&W attack is made possible using zeroth order
optimization (ZOO). In the more stringent attack scenario
where querying is prohibited, ensemble methods can be used
for transfer attacks (Liu et al. 2016).

Defenses in DNNs
Defensive distillation: Defensive distillation (Papernot et al.
2016b) defends against adversarial perturbations by using
the distillation technique in (Hinton, Vinyals, and Dean
2015) to retrain the same network with class probabilities
predicted by the original network. It also introduces the tem-
perature parameter T in the softmax layer to enhance the
robustness to adversarial perturbations.
Adversarial training: Adversarial
training can be imple-
mented in a few different ways. A standard approach is
augmenting the original training dataset with the label-
corrected adversarial examples to retrain the network. Mod-
ifying the training loss or the network architecture to in-
crease the robustness of DNNs to adversarial examples has

been proposed in (Zheng et al. 2016; Madry et al. 2017;
Tram`er et al. 2017; Zantedeschi, Nicolae, and Rawat 2017).
Detection methods: Detection methods utilize statistical
tests to differentiate adversarial from benign examples
(Feinman et al. 2017; Grosse et al. 2017; Lu, Issaranon, and
Forsyth 2017; Xu, Evans, and Qi 2017). However, 10 differ-
ent detection methods were unable to detect the C&W attack
(Carlini and Wagner 2017a).

EAD: Elastic-Net Attacks to DNNs
Preliminaries on Elastic-Net Regularization
Elastic-net regularization is a widely used technique in solv-
ing high-dimensional feature selection problems (Zou and
Hastie 2005). It can be viewed as a regularizer that lin-
early combines L1 and L2 penalty functions. In general,
elastic-net regularization is used in the following minimiza-
tion problem:

minimizez∈Z f (z) + λ1(cid:107)z(cid:107)1 + λ2(cid:107)z(cid:107)2
2,

(3)

where z is a vector of p optimization variables, Z indicates
the set of feasible solutions, f (z) denotes a loss function,
(cid:107)z(cid:107)q denotes the Lq norm of z, and λ1, λ2 ≥ 0 are the
L1 and L2 regularization parameters, respectively. The term
λ1(cid:107)z(cid:107)1+λ2(cid:107)z(cid:107)2
2 in (3) is called the elastic-net regularizer of
z. For standard regression problems, the loss function f (z)
is the mean squared error, the vector z represents the weights
(coefﬁcients) on the features, and the set Z = Rp. In partic-
ular, the elastic-net regularization in (3) degenerates to the
LASSO formulation when λ2 = 0, and becomes the ridge
regression formulation when λ1 = 0. It is shown in (Zou
and Hastie 2005) that elastic-net regularization is able to se-
lect a group of highly correlated features, which overcomes
the shortcoming of high-dimensional feature selection when
solely using the LASSO or ridge regression techniques.

EAD Formulation and Generalization
Inspired by the C&W attack (Carlini and Wagner 2017b),
we adopt the same loss function f for crafting adversarial
examples. Speciﬁcally, given an image x0 and its correct la-
bel denoted by t0, let x denote the adversarial example of
x0 with a target class t (cid:54)= t0. The loss function f (x) for
targeted attacks is deﬁned as

f (x, t) = max{max
j(cid:54)=t

[Logit(x)]j − [Logit(x)]t, −κ},

(4)

where Logit(x) = [[Logit(x)]1, . . . , [Logit(x)]K] ∈ RK
is the logit layer (the layer prior to the softmax layer) rep-
resentation of x in the considered DNN, K is the num-
ber of classes for classiﬁcation, and κ ≥ 0 is a conﬁ-
dence parameter that guarantees a constant gap between
maxj(cid:54)=t[Logit(x)]j and [Logit(x)]t.

It is worth noting that the term [Logit(x)]t is proportional
to the probability of predicting x as label t, since by the
softmax classiﬁcation rule,

Prob(Label(x) = t) =

exp([Logit(x)]t)
j=1 exp([Logit(x)]j)

.

(cid:80)K

(5)

Consequently, the loss function in (4) aims to render the la-
bel t the most probable class for x, and the parameter κ
controls the separation between t and the next most likely
prediction among all classes other than t. For untargeted at-
tacks, the loss function in (4) can be modiﬁed as

f (x) = max{[Logit(x)]t0 − max
j(cid:54)=t0

[Logit(x)]j, −κ}.

(6)

In this paper, we focus on targeted attacks since they are
more challenging than untargeted attacks. Our EAD algo-
rithm (Algorithm 1) can directly be applied to untargeted
attacks by replacing f (x, t) in (4) with f (x) in (6).

In addition to manipulating the prediction via the loss
function in (4), introducing elastic-net regularization further
encourages similarity to the original image when crafting
adversarial examples. Our formulation of elastic-net attacks
to DNNs (EAD) for crafting an adversarial example (x, t)
with respect to a labeled natural image (x0, t0) is as follows:
minimizex c · f (x, t) + β(cid:107)x − x0(cid:107)1 + (cid:107)x − x0(cid:107)2
2
subject to x ∈ [0, 1]p,

(7)
where f (x, t) is as deﬁned in (4), c, β ≥ 0 are the regular-
ization parameters of the loss function f and the L1 penalty,
respectively. The box constraint x ∈ [0, 1]p restricts x to a
properly scaled image space, which can be easily satisﬁed by
dividing each pixel value by the maximum attainable value
(e.g., 255). Upon deﬁning the perturbation of x relative to x0
as δ = x−x0, the EAD formulation in (7) aims to ﬁnd an ad-
versarial example x that will be classiﬁed as the target class
t while minimizing the distortion in δ in terms of the elastic-
net loss β(cid:107)δ(cid:107)1 + (cid:107)δ(cid:107)2
2, which is a linear combination of L1
and L2 distortion metrics between x and x0. Notably, the
formulation of the C&W attack (Carlini and Wagner 2017b)
becomes a special case of the EAD formulation in (7) when
β = 0, which disregards the L1 penalty on δ. However, the
L1 penalty is an intuitive regularizer for crafting adversarial
examples, as (cid:107)δ(cid:107)1 = (cid:80)p
i=1 |δi| represents the total varia-
tion of the perturbation, and is also a widely used surrogate
function for promoting sparsity in the perturbation. As will
be evident in the performance evaluation section, including
the L1 penalty for the perturbation indeed yields a distinct
set of adversarial examples, and it leads to improved attack
transferability and complements adversarial learning.

EAD Algorithm
When solving the EAD formulation in (7) without the L1
penalty (i.e., β = 0), Carlini and Wagner used a change-
of-variable (COV) approach via the tanh transformation on
x in order to remove the box constraint x ∈ [0, 1]p (Car-
lini and Wagner 2017b). When β > 0, we ﬁnd that the same
COV approach is not effective in solving (7), since the corre-
sponding adversarial examples are insensitive to the changes
in β (see the performance evaluation section for details).
Since the L1 penalty is a non-differentiable, yet piece-wise
linear, function, the failure of the COV approach in solving
(7) can be explained by its inefﬁciency in subgradient-based
optimization problems (Duchi and Singer 2009).

To efﬁciently solve the EAD formulation in (7) for
crafting adversarial examples, we propose to use the iter-
ative shrinkage-thresholding algorithm (ISTA) (Beck and

Teboulle 2009). ISTA can be viewed as a regular ﬁrst-
order optimization algorithm with an additional shrinkage-
thresholding step on each iteration. In particular, let g(x) =
c·f (x)+(cid:107)x−x0(cid:107)2
2 and let ∇g(x) be the numerical gradient
of g(x) computed by the DNN. At the k + 1-th iteration, the
adversarial example x(k+1) of x0 is computed by

x(k+1) = Sβ(x(k) − αk∇g(x(k))),
(8)
where αk denotes the step size at the k + 1-th iteration, and
Sβ : Rp (cid:55)→ Rp is an element-wise projected shrinkage-
thresholding function, which is deﬁned as

[Sβ(z)]i =

(cid:40) min{zi − β, 1},

x0i,
max{zi + β, 0},

if zi − x0i > β;
if |zi − x0i| ≤ β;
if zi − x0i < −β,

(9)
for any i ∈ {1, . . . , p}. If |zi − x0i| > β, it shrinks the
element zi by β and projects the resulting element to the
feasible box constraint between 0 and 1. On the other hand,
if |zi − x0i| ≤ β, it thresholds zi by setting [Sβ(z)]i = x0i.
The proof of optimality of using (8) for solving the EAD
formulation in (7) is given in the supplementary material1.
Notably, since g(x) is the attack objective function of the
C&W method (Carlini and Wagner 2017b), the ISTA oper-
ation in (8) can be viewed as a robust version of the C&W
method that shrinks a pixel value of the adversarial example
if the deviation to the original image is greater than β, and
keeps a pixel value unchanged if the deviation is less than β.
Our EAD algorithm for crafting adversarial examples is
summarized in Algorithm 1. For computational efﬁciency, a
fast ISTA (FISTA) for EAD is implemented, which yields
the optimal convergence rate for ﬁrst-order optimization
methods (Beck and Teboulle 2009). The slack vector y(k)
in Algorithm 1 incorporates the momentum in x(k) for ac-
celeration. In the experiments, we set the initial learning rate
α0 = 0.01 with a square-root decay factor in k. During the
EAD iterations, the iterate x(k) is considered as a success-
ful adversarial example of x0 if the model predicts its most
likely class to be the target class t. The ﬁnal adversarial ex-
ample x is selected from all successful examples based on
distortion metrics. In this paper we consider two decision
rules for selecting x: the least elastic-net (EN) and L1 dis-
tortions relative to x0. The inﬂuence of β, κ and the decision
rules on EAD will be investigated in the following section.

Performance Evaluation
In this section, we compare the proposed EAD with the
state-of-the-art attacks to DNNs on three image classiﬁca-
tion datasets - MNIST, CIFAR10 and ImageNet. We would
like to show that (i) EAD can attain attack performance sim-
ilar to the C&W attack in breaking undefended and defen-
sively distilled DNNs, since the C&W attack is a special
case of EAD when β = 0; (ii) Comparing to existing L1-
based FGM and I-FGM methods, the adversarial examples
using EAD can lead to signiﬁcantly lower L1 distortion and
better attack success rate; (iii) The L1-based adversarial ex-
amples crafted by EAD can achieve improved attack trans-
ferability and complement adversarial training.

1https://arxiv.org/abs/1709.04114

Algorithm 1 Elastic-Net Attacks to DNNs (EAD)

Input: original labeled image (x0, t0), target attack class
t, attack transferability parameter κ, L1 regularization pa-
rameter β, step size αk, # of iterations I
Output: adversarial example x
Initialization: x(0) = y(0) = x0
for k = 0 to I − 1 do

x(k+1) = Sβ(y(k) − αk∇g(y(k)))
y(k+1) = x(k+1) + k

k+3 (x(k+1) − x(k))

end for
Decision rule: determine x from successful examples in
{x(k)}I

k=1 (EN rule or L1 rule).

Comparative Methods
We compare EAD with the following targeted attacks, which
are the most effective methods for crafting adversarial exam-
ples in different distortion metrics.
C&W attack: The state-of-the-art L2 targeted attack pro-
posed by Carlini and Wagner (Carlini and Wagner 2017b),
which is a special case of EAD when β = 0.
FGM: The fast gradient method proposed in (Goodfellow,
Shlens, and Szegedy 2015). The FGM attacks using differ-
ent distortion metrics are denoted by FGM-L1, FGM-L2 and
FGM-L∞.
I-FGM: The iterative fast gradient method proposed in (Ku-
rakin, Goodfellow, and Bengio 2016b). The I-FGM attacks
using different distortion metrics are denoted by I-FGM-L1,
I-FGM-L2 and I-FGM-L∞.

Experiment Setup and Parameter Setting
Our experiment setup is based on Carlini and Wagner’s
framework2. For both the EAD and C&W attacks, we use the
default setting1, which implements 9 binary search steps on
the regularization parameter c (starting from 0.001) and runs
I = 1000 iterations for each step with the initial learning
rate α0 = 0.01. For ﬁnding successful adversarial examples,
we use the reference optimizer1 (ADAM) for the C&W at-
tack and implement the projected FISTA (Algorithm 1) with
the square-root decaying learning rate for EAD. Similar to
the C&W attack, the ﬁnal adversarial example of EAD is se-
lected by the least distorted example among all the success-
ful examples. The sensitivity analysis of the L1 parameter β
and the effect of the decision rule on EAD will be investi-
gated in the forthcoming paragraph. Unless speciﬁed, we set
the attack transferability parameter κ = 0 for both attacks.

We implemented FGM and I-FGM using the CleverHans
package3. The best distortion parameter (cid:15) is determined by
a ﬁne-grained grid search - for each image, the smallest (cid:15)
in the grid leading to a successful attack is reported. For
I-FGM, we perform 10 FGM iterations (the default value)
with (cid:15)-ball clipping. The distortion parameter (cid:15)(cid:48) in each
FGM iteration is set to be (cid:15)/10, which has been shown to
be an effective attack setting in (Tram`er et al. 2017). The

2https://github.com/carlini/nn robust attacks
3https://github.com/tensorﬂow/cleverhans

Table 1: Comparison of the change-of-variable (COV) approach and EAD (Algorithm 1) for solving the elastic-net formulation
in (7) on MNIST. ASR means attack success rate (%). Although these two methods attain similar attack success rates, COV is
not effective in crafting L1-based adversarial examples. Increasing β leads to less L1-distorted adversarial examples for EAD,
whereas the distortion of COV is insensitive to changes in β.

Optimization
method

COV

EAD
(EN rule)

Best case

Average case

Worst case

β

0
10−5
10−4
10−3
10−2
0
10−5
10−4
10−3
10−2

ASR L1

L2

L∞

ASR L1

L2

L∞

ASR L1

L2

100
100
100
100
100
100
100
100
100
100

13.93
13.92
13.91
13.8
12.98
14.04
13.66
12.79
9.808
7.271

1.377
1.377
1.377
1.377
1.38
1.369
1.369
1.372
1.427
1.718

0.379
0.379
0.379
0.381
0.389
0.376
0.378
0.388
0.452
0.674

100
100
100
100
100
100
100
100
100
100

22.46
22.66
23.11
22.42
22.27
22.63
22.6
20.98
17.4
13.56

1.972
1.98
2.013
1.977
2.026
1.953
1.98
1.951
2.001
2.395

0.514
0.508
0.517
0.512
0.53
0.512
0.515
0.521
0.594
0.852

99.9
99.5
100
99.9
99.5
99.8
99.9
100
100
100

32.3
32.33
32.32
32.2
31.41
31.43
30.79
29.21
25.52
20.77

2.639
2.64
2.639
2.639
2.643
2.51
2.507
2.514
2.582
3.021

L∞

0.663
0.663
0.664
0.664
0.673
0.644
0.648
0.667
0.748
0.976

range of the grid and the resolution of these two methods
are speciﬁed in the supplementary material1.

The image classiﬁers for MNIST and CIFAR10 are
trained based on the DNN models provided by Carlini and
Wagner1. The image classiﬁer for ImageNet is the Inception-
v3 model (Szegedy et al. 2016). For MNIST and CIFAR10,
1000 correctly classiﬁed images are randomly selected from
the test sets to attack an incorrect class label. For ImageNet,
100 correctly classiﬁed images and 9 incorrect classes are
randomly selected to attack. All experiments are conducted
on a machine with an Intel E5-2690 v3 CPU, 40 GB RAM
and a single NVIDIA K80 GPU. Our EAD code is publicly
available for download4.

Evaluation Metrics
Following the attack evaluation criterion in (Carlini and
Wagner 2017b), we report the attack success rate and dis-
tortion of the adversarial examples from each method. The
attack success rate (ASR) is deﬁned as the percentage of
adversarial examples that are classiﬁed as the target class
(which is different from the original class). The average L1,
L2 and L∞ distortion metrics of successful adversarial ex-
amples are also reported. In particular, the ASR and distor-
tion of the following attack settings are considered:
Best case: The least difﬁcult attack among targeted attacks
to all incorrect class labels in terms of distortion.
Average case: The targeted attack to a randomly selected
incorrect class label.
Worst case: The most difﬁcult attack among targeted at-
tacks to all incorrect class labels in terms of distortion.

Sensitivity Analysis and Decision Rule for EAD
We verify the necessity of using Algorithm 1 for solving
the elastic-net regularized attack formulation in (7) by com-
paring it to a naive change-of-variable (COV) approach. In
(Carlini and Wagner 2017b), Carlini and Wagner remove the
box constraint x ∈ [0, 1]p by replacing x with 1+tanh w
,

2

4https://github.com/ysharma1126/EAD-Attack

where w ∈ Rp and 1 ∈ Rp is a vector of ones. The de-
fault ADAM optimizer (Kingma and Ba 2014) is then used
to solve w and obtain x. We apply this COV approach to
(7) and compare with EAD on MNIST with different orders
of the L1 regularization parameter β in Table 1. Although
COV and EAD attain similar attack success rates, it is ob-
served that COV is not effective in crafting L1-based ad-
versarial examples. Increasing β leads to less L1-distorted
adversarial examples for EAD, whereas the distortion (L1,
L2 and L∞) of COV is insensitive to changes in β. Similar
insensitivity of COV on β is observed when one uses other
optimizers such as AdaGrad, RMSProp or built-in SGD in
TensorFlow. We also note that the COV approach prohibits
the use of ISTA due to the subsequent tanh term in the L1
penalty. The insensitivity of COV suggests that it is inade-
quate for elastic-net optimization, which can be explained by
its inefﬁciency in subgradient-based optimization problems
(Duchi and Singer 2009). For EAD, we also ﬁnd an interest-
ing trade-off between L1 and the other two distortion met-
rics - adversarial examples with smaller L1 distortion tend
to have larger L2 and L∞ distortions. This trade-off can be
explained by the fact that increasing β further encourages
sparsity in the perturbation, and hence results in increased
L2 and L∞ distortion. Similar results are observed on CI-
FAR10 (see supplementary material1).

In Table 1, during the attack optimization process the ﬁnal
adversarial example is selected based on the elastic-net loss
of all successful adversarial examples in {x(k)}I
k=1, which
we call the elastic-net (EN) decision rule. Alternatively, we
can select the ﬁnal adversarial example with the least L1
distortion, which we call the L1 decision rule. Figure 2 com-
pares the ASR and average-case distortion of these two de-
cision rules with different β on MNIST. Both decision rules
yield 100% ASR for a wide range of β values. For the same
β, the L1 rule gives adversarial examples with less L1 dis-
tortion than those given by the EN rule at the price of larger
L2 and L∞ distortions. Similar trends are observed on CI-
FAR10 (see supplementary material1). The complete results

Figure 2: Comparison of EN and L1 decision rules in EAD on MNIST with varying L1 regularization parameter β (average
case). Comparing to the EN rule, for the same β the L1 rule attains less L1 distortion but may incur more L2 and L∞ distortions.

Table 2: Comparison of different attacks on MNIST, CIFAR10 and ImageNet (average case). ASR means attack success rate
(%). The distortion metrics are averaged over successful examples. EAD, the C&W attack, and I-FGM-L∞ attain the least L1,
L2, and L∞ distorted adversarial examples, respectively. The complete attack results are given in the supplementary material1.

Attack method ASR L1
C&W (L2)
FGM-L1
FGM-L2
FGM-L∞
I-FGM-L1
I-FGM-L2
I-FGM-L∞
EAD (EN rule)
EAD (L1 rule)

100
39
34.6
42.5
100
100
100
100
100

22.46
53.5
39.15
127.2
32.94
30.32
71.39
17.4
14.11

MNIST
L2
1.972
4.186
3.284
6.09
2.606
2.41
3.472
2.001
2.211

L∞
0.514
0.782
0.747
0.296
0.591
0.561
0.227
0.594
0.768

CIFAR10
L2
0.392
1.48
1.157
2.373
0.502
0.489
0.68
0.502
0.613

13.62
51.97
39.5
127.81
17.53
17.12
33.3
8.18
6.066

ASR L1
100
48.8
42.8
52.3
100
100
100
100
100

L∞
0.044
0.152
0.136
0.047
0.055
0.054
0.018
0.097
0.17

ImageNet
L2
0.705
0.187
6.823
7.102
1.609
2.358
2.079
1.563
1.598

232.2
61
2338
3655
526.4
774.1
864.2
69.47
40.9

ASR L1
100
1
1
3
77
100
100
100
100

L∞
0.03
0.007
0.25
0.014
0.054
0.086
0.01
0.238
0.293

of these two rules on MNIST and CIFAR10 are given in the
supplementary material1. In the following experiments, we
will report the results of EAD with these two decision rules
and set β = 10−3, since on MNIST and CIFAR10 this β
value signiﬁcantly reduces the L1 distortion while having
comparable L2 and L∞ distortions to the case of β = 0
(i.e., without L1 regularization).

Attack Success Rate and Distortion on MNIST,
CIFAR10 and ImageNet

We compare EAD with the comparative methods in terms
of attack success rate and different distortion metrics on at-
tacking the considered DNNs trained on MNIST, CIFAR10
and ImageNet. Table 2 summarizes their average-case per-
formance. It is observed that FGM methods fail to yield suc-
cessful adversarial examples (i.e., low ASR), and the cor-
responding distortion metrics are signiﬁcantly larger than
other methods. On the other hand, the C&W attack, I-FGM
and EAD all lead to 100% attack success rate. Further-
more, EAD, the C&W method, and I-FGM-L∞ attain the
least L1, L2, and L∞ distorted adversarial examples, respec-
tively. We note that EAD signiﬁcantly outperforms the exist-
ing L1-based method (I-FGM-L1). Compared to I-FGM-L1,
EAD with the EN decision rule reduces the L1 distortion by
roughly 47% on MNIST, 53% on CIFAR10 and 87% on Im-
ageNet. We also observe that EAD with the L1 decision rule
can further reduce the L1 distortion but at the price of no-
ticeable increase in the L2 and L∞ distortion metrics.

Notably, despite having large L2 and L∞ distortion met-

Figure 3: Attack success rate (average case) of the C&W
method and EAD on MNIST and CIFAR10 with respect to
varying temperature parameter T for defensive distillation.
Both methods can successfully break defensive distillation.

rics, the adversarial examples crafted by EAD with the L1
rule can still attain 100% ASRs in all datasets, which implies
the L2 and L∞ distortion metrics are insufﬁcient for evaluat-
ing the robustness of neural networks. Moreover, the attack
results in Table 2 suggest that EAD can yield a set of distinct
adversarial examples that are fundamentally different from
L2 or L∞ based examples. Similar to the C&W method and
I-FGM, the adversarial examples from EAD are also visu-
ally indistinguishable (see supplementary material1).

Breaking Defensive Distillation

In addition to breaking undefended DNNs via adversarial
examples, here we show that EAD can also break defen-
sively distilled DNNs. Defensive distillation (Papernot et al.

Table 3: Adversarial training using the C&W attack and
EAD (L1 rule) on MNIST. ASR means attack success rate.
Incorporating L1 examples complements adversarial train-
ing and enhances attack difﬁculty in terms of distortion. The
complete results are given in the supplementary material1.

Average case

Attack
method

C&W
(L2)

EAD
(L1 rule)

Adversarial
ASR L1
training
None
100
100
EAD
C&W
100
EAD + C&W 100
100
None
EAD
100
C&W
100
EAD + C&W 100

22.46
26.11
24.97
27.32
14.11
17.04
15.49
16.83

L2
1.972
2.468
2.47
2.513
2.211
2.653
2.628
2.66

L∞
0.514
0.643
0.684
0.653
0.768
0.86
0.892
0.87

which can be explained by the fact that the ISTA operation
in (8) is a robust version of the C&W attack via shrinking
and thresholding. We also ﬁnd that setting κ too large may
mitigate the ASR of transfer attacks for both EAD and the
C&W method, as the optimizer may fail to ﬁnd an adver-
sarial example that minimizes the loss function f in (4) for
large κ. The complete attack transferability results are given
in the supplementary material1.

Complementing Adversarial Training
To further validate the difference between L1-based and L2-
based adversarial examples, we test their performance in ad-
versarial training on MNIST. We randomly select 1000 im-
ages from the training set and use the C&W attack and EAD
(L1 rule) to generate adversarial examples for all incorrect
labels, leading to 9000 adversarial examples in total for each
method. We then separately augment the original training set
with these examples to retrain the network and test its ro-
bustness on the testing set, as summarized in Table 3. For
adversarial training with any single method, although both
attacks still attain a 100% success rate in the average case,
the network is more tolerable to adversarial perturbations, as
all distortion metrics increase signiﬁcantly when compared
to the null case. We also observe that joint adversarial train-
ing with EAD and the C&W method can further increase the
L1 and L2 distortions against the C&W attack and the L2
distortion against EAD, suggesting that the L1-based exam-
ples crafted by EAD can complement adversarial training.

Conclusion
We proposed an elastic-net regularized attack framework
for crafting adversarial examples to attack deep neural net-
works. Experimental results on MNIST, CIFAR10 and Ima-
geNet show that the L1-based adversarial examples crafted
by EAD can be as successful as the state-of-the-art L2 and
L∞ attacks in breaking undefended and defensively distilled
networks. Furthermore, EAD can improve attack transfer-
ability and complement adversarial training. Our results cor-
roborate the effectiveness of EAD and shed new light on
the use of L1-based adversarial examples toward adversarial
learning and security implications of deep neural networks.

Figure 4: Attack transferability (average case) from the un-
defended network to the defensively distilled network on
MNIST by varying κ. EAD can attain nearly 99% attack
success rate (ASR) when κ = 50, whereas the top ASR of
the C&W attack is nearly 88% when κ = 40.

2016b) is a standard defense technique that retrains the net-
work with class label probabilities predicted by the original
network, soft labels, and introduces the temperature parame-
ter T in the softmax layer to enhance its robustness to adver-
sarial perturbations. Similar to the state-of-the-art attack (the
C&W method), Figure 3 shows that EAD can attain 100%
attack success rate for different values of T on MNIST and
CIFAR10. Moreover, since the C&W attack formulation is
a special case of the EAD formulation in (7) when β = 0,
successfully breaking defensive distillation using EAD sug-
gests new ways of crafting effective adversarial examples by
varying the L1 regularization parameter β. The complete at-
tack results are given in the supplementary material1.

Improved Attack Transferability
It has been shown in (Carlini and Wagner 2017b) that the
C&W attack can be made highly transferable from an unde-
fended network to a defensively distilled network by tuning
the conﬁdence parameter κ in (4). Following (Carlini and
Wagner 2017b), we adopt the same experiment setting for
attack transferability on MNIST, as MNIST is the most dif-
ﬁcult dataset to attack in terms of the average distortion per
image pixel from Table 2.

Fixing κ, adversarial examples generated from the origi-
nal (undefended) network are used to attack the defensively
distilled network with the temperature parameter T = 100
(Papernot et al. 2016b). The attack success rate (ASR) of
EAD, the C&W method and I-FGM are shown in Figure 4.
When κ = 0, all methods attain low ASR and hence do
not produce transferable adversarial examples. The ASR of
EAD and the C&W method improves when we set κ > 0,
whereas I-FGM’s ASR remains low (less than 2%) since the
attack does not have such a parameter for transferability.

Notably, EAD can attain nearly 99% ASR when κ = 50,
whereas the top ASR of the C&W method is nearly 88%
when κ = 40. This implies improved attack transferabil-
ity when using the adversarial examples crafted by EAD,

Acknowledgment Cho-Jui Hsieh and Huan Zhang ac-

knowledge the support of NSF via IIS-1719097.

References

Beck, A., and Teboulle, M. 2009. A fast iterative shrinkage-
thresholding algorithm for linear inverse problems. SIAM
journal on imaging sciences 2(1):183–202.
Cand`es, E. J., and Wakin, M. B. 2008. An introduction to
IEEE signal processing magazine
compressive sampling.
25(2):21–30.
Carlini, N., and Wagner, D. 2017a. Adversarial examples are
not easily detected: Bypassing ten detection methods. arXiv
preprint arXiv:1705.07263.
Carlini, N., and Wagner, D. 2017b. Towards evaluating the
robustness of neural networks. In IEEE Symposium on Se-
curity and Privacy (SP), 39–57.
Dong, Y.; Su, H.; Zhu, J.; and Bao, F. 2017. Towards in-
terpretable deep neural networks by leveraging adversarial
examples. arXiv preprint arXiv:1708.05493.
Duchi, J., and Singer, Y. 2009. Efﬁcient online and batch
learning using forward backward splitting. Journal of Ma-
chine Learning Research 10(Dec):2899–2934.
Evtimov, I.; Eykholt, K.; Fernandes, E.; Kohno, T.; Li, B.;
Prakash, A.; Rahmati, A.; and Song, D. 2017. Robust
physical-world attacks on machine learning models. arXiv
preprint arXiv:1707.08945.
Feinman, R.; Curtin, R. R.; Shintre, S.; and Gardner, A. B.
2017. Detecting adversarial samples from artifacts. arXiv
preprint arXiv:1703.00410.
Fu, H.; Ng, M. K.; Nikolova, M.; and Barlow, J. L. 2006. Ef-
ﬁcient minimization methods of mixed l2-l1 and l1-l1 norms
for image restoration. SIAM Journal on scientiﬁc computing
27(6):1881–1902.
Goodfellow, I. J.; Shlens, J.; and Szegedy, C. 2015. Explain-
ICLR’15; arXiv
ing and harnessing adversarial examples.
preprint arXiv:1412.6572.
Grosse, K.; Manoharan, P.; Papernot, N.; Backes, M.; and
McDaniel, P. 2017. On the (statistical) detection of adver-
sarial examples. arXiv preprint arXiv:1702.06280.
Hinton, G.; Vinyals, O.; and Dean, J.
ing the knowledge in a neural network.
arXiv:1503.02531.
Kingma, D., and Ba, J. 2014. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980.
Koh, P. W., and Liang, P. 2017. Understanding black-box
ICML; arXiv preprint
predictions via inﬂuence functions.
arXiv:1703.04730.
Kurakin, A.; Goodfellow, I.; and Bengio, S. 2016a. Ad-
versarial examples in the physical world. arXiv preprint
arXiv:1607.02533.
Kurakin, A.; Goodfellow, I.; and Bengio, S. 2016b. Adver-
ICLR’17; arXiv preprint
sarial machine learning at scale.
arXiv:1611.01236.

2015. Distill-
arXiv preprint

Liu, Y.; Chen, X.; Liu, C.; and Song, D. 2016. Delv-
ing into transferable adversarial examples and black-box at-
tacks. arXiv preprint arXiv:1611.02770.
Lu, J.; Issaranon, T.; and Forsyth, D. 2017. Safetynet: De-
tecting and rejecting adversarial examples robustly. arXiv
preprint arXiv:1704.00103.
Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and
Vladu, A. 2017. Towards deep learning models resistant
to adversarial attacks. arXiv preprint arXiv:1706.06083.
Moosavi-Dezfooli, S.-M.; Fawzi, A.; Fawzi, O.; and
Frossard, P. 2016. Universal adversarial perturbations. arXiv
preprint arXiv:1610.08401.
Moosavi-Dezfooli, S.-M.; Fawzi, A.; and Frossard, P. 2016.
Deepfool: a simple and accurate method to fool deep neural
networks. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2574–2582.
Papernot, N.; McDaniel, P.; Jha, S.; Fredrikson, M.; Celik,
Z. B.; and Swami, A. 2016a. The limitations of deep learn-
ing in adversarial settings. In IEEE European Symposium on
Security and Privacy (EuroS&P), 372–387.
Papernot, N.; McDaniel, P.; Wu, X.; Jha, S.; and Swami, A.
2016b. Distillation as a defense to adversarial perturbations
against deep neural networks. In IEEE Symposium on Secu-
rity and Privacy (SP), 582–597.
Papernot, N.; McDaniel, P.; Goodfellow, I.; Jha, S.; Celik,
Z. B.; and Swami, A. 2017. Practical black-box attacks
against machine learning. In ACM Asia Conference on Com-
puter and Communications Security, 506–519.
Parikh, N.; Boyd, S.; et al. 2014. Proximal algorithms. Foun-
dations and Trends R(cid:13) in Optimization 1(3):127–239.
Szegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan,
D.; Goodfellow, I.; and Fergus, R. 2013. Intriguing proper-
ties of neural networks. arXiv preprint arXiv:1312.6199.
Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna,
Z. 2016. Rethinking the inception architecture for computer
vision. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2818–2826.
Tram`er, F.; Kurakin, A.; Papernot, N.; Boneh, D.; and Mc-
Daniel, P. 2017. Ensemble adversarial training: Attacks and
defenses. arXiv preprint arXiv:1705.07204.
Xu, W.; Evans, D.; and Qi, Y. 2017. Feature squeezing: De-
tecting adversarial examples in deep neural networks. arXiv
preprint arXiv:1704.01155.
Zantedeschi, V.; Nicolae, M.-I.; and Rawat, A. 2017. Ef-
ﬁcient defenses against adversarial attacks. arXiv preprint
arXiv:1707.06728.
Zheng, S.; Song, Y.; Leung, T.; and Goodfellow, I. 2016.
Improving the robustness of deep neural networks via sta-
bility training. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 4480–4488.
Zou, H., and Hastie, T. 2005. Regularization and variable
selection via the elastic net. Journal of the Royal Statistical
Society: Series B (Statistical Methodology) 67(2):301–320.

Supplementary Material

Proof of Optimality of (8) for Solving EAD in (3)
Since the L1 penalty β(cid:107)x − x0(cid:107)1 in (3) is a non-
differentiable yet smooth function, we use the proximal gra-
dient method (Parikh, Boyd, and others 2014) for solving
the EAD formulation in (3). Deﬁne ΦZ (z) to be the indica-
tor function of an interval Z such that ΦZ (z) = 0 if z ∈ Z
and ΦZ (z) = ∞ if z /∈ Z. Using ΦZ (z), the EAD formula-
tion in (3) can be rewritten as

minimizex∈Rp g(x) + β(cid:107)x − x0(cid:107)1 + Φ[0,1]p (x),

(10)

where g(x) = c·f (x, t)+(cid:107)x−x0(cid:107)2
Prox(x) of β(cid:107)x − x0(cid:107)1 constrained to x ∈ [0, 1]p is

2. The proximal operator

Prox(x) = arg min
z∈Rp

(cid:107)z − x(cid:107)2

2 + β(cid:107)z − x0(cid:107)1 + Φ[0,1]p (z)

(cid:107)z − x(cid:107)2

2 + β(cid:107)z − x0(cid:107)1

1
2

1
2

= arg min

z∈[0,1]p

= Sβ(x),

(11)

(12)

(13)

where the mapping function Sβ is deﬁned in (9). Con-
sequently, using (11), the proximal gradient algorithm for
solving (7) is iterated by

x(k+1) = Prox(x(k) − αk∇g(x(k)))
= Sβ(x(k) − αk∇g(x(k))),

which completes the proof.

Grid Search for FGM and I-FGM (Table 4)
To determine the optimal distortion parameter (cid:15) for FGM
and I-FGM methods, we adopt a ﬁne grid search on (cid:15). For
each image, the best parameter is the smallest (cid:15) in the grid
leading to a successful targeted attack. If the grid search fails
to ﬁnd a successful adversarial example, the attack is consid-
ered in vain. The selected range for grid search covers the
reported distortion statistics of EAD and the C&W attack.
The resolution of the grid search for FGM is selected such
that it will generate 1000 candidates of adversarial examples
during the grid search per input image. The resolution of the
grid search for I-FGM is selected such that it will compute
gradients for 10000 times in total (i.e., 1000 FGM opera-
tions × 10 iterations) during the grid search per input im-
age, which is more than the total number of gradients (9000)
computed by EAD and the C&W attack.

Table 4: Range and resolution of grid search for ﬁnding the
optimal distortion parameter (cid:15) for FGM and I-FGM.

Grid search

Range
Method
[10−3, 1]
FGM-L∞
[1, 103]
FGM-L1
[10−2, 10]
FGM-L2
I-FGM-L∞ [10−3, 1]
I-FGM-L1
I-FGM-L2

[1, 103]
[10−2, 10]

Resolution
10−3
1
10−2
10−3
1
10−2

Comparison of COV and EAD on CIFAR10 (Table
5)

Table 5 compares the attack performance of using EAD (Al-
gorithm 1)) and the change-of-variable (COV) approach for
solving the elastic-net formulation in (7) on CIFAR10. Sim-
ilar to the MNIST results in Table 1, although COV and
EAD attain similar attack success rates, we ﬁnd that COV
is not effective in crafting L1-based adversarial examples.
Increasing β leads to less L1-distorted adversarial examples
for EAD, whereas the distortion (L1, L2 and L∞) of COV
is insensitive to changes in β. The insensitivity of COV sug-
gests that it is inadequate for elastic-net optimization, which
can be explained by its inefﬁciency in subgradient-based op-
timization problems (Duchi and Singer 2009).

Comparison of EN and L1 decision rules in EAD
on MNIST and CIFAR10 (Tables 6 and 7)

Figure 5 compares the average-case distortion of these two
decision rules with different values of β on CIFAR10. For
the same β, the L1 rule gives less L1 distorted adversarial
examples than those given by the EN rule at the price of
larger L2 and L∞ distortions. We also observe that the L1
distortion does not decrease monotonically with β. In partic-
ular, large β values (e.g., β = 5·10−2) may lead to increased
L1 distortion due to excessive shrinking and thresholding.
Table 6 and Table 7 displays the complete attack results of
these two decision rules on MNIST and CIFAR10, respec-
tively.

Complete Attack Results and Visual Illustration on
MNIST, CIFAR10 and ImageNet (Tables 8, 9 and
10 and Figures 6, 7 and 8)

Tables 8, 9 and 10 summarize the complete attack results
of all the considered attack methods on MNIST, CIFAR10
and ImageNet, respectively. EAD, the C&W attack and I-
FGM all lead to 100% attack success rate in the average
case. Among the three image classiﬁcation datasets, Ima-
geNet is the easiest one to attack due to low distortion per
image pixel, and MNIST is the most difﬁcult one to attack.
For the purpose of visual illustration, the adversarial exam-
ples of selected benign images from the test sets are dis-
played in Figures 6, 7 and 8. On CIFAR10 and ImageNet,
the adversarial examples are visually indistinguishable. On
MNIST, the I-FGM examples are blurrier than EAD and the
C&W attack.

Complete Results on Attacking Defensive
Distillation on MNIST and CIFAR10 (Tables 11
and 12)

Tables 11 and 12 display the complete attack results of EAD
and the C&W method on breaking defensive distillation with
different temperature parameter T on MNIST and CIFAR10.
Although defensive distillation is a standard defense tech-
nique for DNNs, EAD and the C&W attack can successfully
break defensive distillation with a wide range of temperature
parameters.

Table 5: Comparison of the change-of-variable (COV) approach and EAD (Algorithm 1) for solving the elastic-net formulation
in (7) on CIFAR10. ASR means attack success rate (%). Similar to the results on MNIST in Table 1, increasing β leads to less
L1-distorted adversarial examples for EAD, whereas the distortion of COV is insensitive to changes in β.

Optimization
method

COV

EAD
(EN rule)

Best case

Average case

Worst case

β

0
10−5
10−4
10−3
10−2
0
10−5
10−4
10−3
10−2

ASR L1

L2

L∞

ASR L1

L∞

L∞

ASR L1

L2

100
100
100
100
100
100
100
100
100
100

7.167
7.165
7.148
6.961
5.963
6.643
5.967
4.638
4.014
3.688

0.209
0.209
0.209
0.211
0.222
0.201
0.201
0.215
0.261
0.357

0.022
0.022
0.022
0.023
0.029
0.023
0.025
0.032
0.047
0.085

100
100
99.9
100
100
99.9
100
99.9
100
100

13.62
13.43
13.64
13.35
11.51
13.39
12.24
10.4
8.18
8.106

0.392
0.386
0.394
0.396
0.408
0.392
0.391
0.414
0.502
0.741

0.044
0.044
0.044
0.045
0.055
0.045
0.047
0.058
0.097
0.209

99.9
100
99.9
100
100
99.9
99.7
99.9
100
100

19.17
19.16
19.14
18.7
16.31
18.72
17.24
14.86
12.11
12.59

0.546
0.546
0.547
0.547
0.556
0.541
0.541
0.56
0.69
1.053

L∞

0.064
0.065
0.064
0.066
0.077
0.064
0.068
0.081
0.147
0.351

Figure 5: Comparison of EN and L1 decision rules in EAD on CIFAR10 with varying L1 regularization parameter β (average
case). Comparing to the EN rule, for the same β the L1 rule attains less L1 distortion but may incur more L2 and L∞ distortions.

can render the DNN more difﬁcult to attack in terms of in-
creased distortion metrics when compared with the null case.
Notably, in the average case, joint adversarial training using
L1 and L2 examples lead to increased L1 and L2 distortion
against the C&W attack and EAD (EN), and increased L2
distortion against EAD (L1). The results suggest that EAD
can complement adversarial training toward resilient DNNs.
We would like to point out that in our experiments, adversar-
ial training maintains comparable test accuracy. All the ad-
versarially trained DNNs in Table 14 can still attain at least
99% test accuracy on MNIST.

Complete Attack Transferability Results on
MNIST (Table 13 )

Table 13 summarizes the transfer attack results from an un-
defended DNN to a defensively distilled DNN on MNIST
using EAD, the C&W attack and I-FGM. I-FGM methods
have poor performance in attack transferability. The aver-
age attack success rate (ASR) of I-FGM is below 2%. On
the other hand, adjusting the transferability parameter κ in
EAD and the C&W attack can signiﬁcantly improve ASR.
Tested on a wide range of κ values, the top average-case
ASR for EAD is 98.6% using the EN rule and 98.1% using
the L1 rule. The top average-case ASR for the C&W attack
is 87.4%. This improvement is signiﬁcantly due to the im-
provement in the worst case, where the top worst-case ASR
for EAD is 87% using the EN rule and 85.8% using the L1
rule, while the top worst-case ASR for the C&W attack is
30.5%. The results suggest that L1-based adversarial exam-
ples have better attack transferability.

Complete Results on Adversarial Training with L1
and L2 examples (Table 14)
Table 14 displays the complete results of adversarial training
on MNIST using the L2-based adversarial examples crafted
by the C&W attack and the L1-based adversarial examples
crafted by EAD with the EN or the L1 decision rule. It can
be observed that adversarial training with any single method

Table 6: Comparison of the elastic net (EN) and L1 decision rules in EAD for selecting adversarial examples on MNIST. ASR
means attack success rate (%).

Decision
rule

β

EAD
(EN rule)

EAD
(L1 rule)

10−5
5 · 10−5
10−4
5 · 10−4
10−3
5 · 10−3
10−2
5 · 10−2
10−5
5 · 10−4
10−4
5 · 10−4
10−3
5 · 10−3
10−2
5 · 10−2

Best case

Average case

Worst case

ASR L1

L2

L∞

ASR L1

L2

L∞

ASR L1

L2

13.66
13.19
12.79
10.95
9.808
7.912
7.271
7.088
13.26
11.81
10.75
8.025
7.153
6.347
6.193
5.918

1.369
1.37
1.372
1.395
1.427
1.6
1.718
1.872
1.376
1.385
1.403
1.534
1.639
1.844
1.906
2.101

0.378
0.383
0.388
0.42
0.452
0.591
0.674
0.736
0.386
0.404
0.424
0.527
0.593
0.772
0.861
0.956

100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

22.6
21.25
20.98
19.11
17.4
14.49
13.56
13.38
21.75
20.28
18.69
15.42
14.11
11.99
11.69
11.59

1.98
1.947
1.951
1.986
2.001
2.261
2.395
2.712
1.965
1.98
1.983
2.133
2.211
2.491
2.68
2.873

0.515
0.518
0.521
0.558
0.594
0.772
0.852
0.919
0.525
0.54
0.565
0.694
0.768
0.927
0.97
0.993

99.9
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

30.79
29.82
29.21
27.04
25.52
21.64
20.77
20.19
30.48
28.8
27.49
23.62
22.05
18.01
17.29
18.67

2.507
2.512
2.514
2.544
2.582
2.835
3.021
3.471
2.521
2.527
2.539
2.646
2.747
3.218
3.381
3.603

Table 7: Comparison of the elastic net (EN) and L1 decision rules in EAD for selecting adversarial examples on CIFAR10.
ASR means attack success rate (%).

Decision
rule

β

EAD
(EN rule)

EAD
(L1 rule)

10−5
5 · 10−5
10−4
5 · 10−4
10−3
5 · 10−3
10−2
5 · 10−2
10−5
5 · 10−5
10−4
5 · 10−4
10−3
5 · 10−3
10−2
5 · 10−2

Best case

Average case

Worst case

ASR L1

L2

L∞

ASR L1

L2

L∞

ASR L1

L2

5.967
5.09
4.638
4.35
4.014
3.83
3.688
2.855
5.002
3.884
3.361
2.689
2.6
2.216
2.201
2.306

0.201
0.207
0.215
0.241
0.261
0.319
0.357
0.52
0.209
0.231
0.255
0.339
0.358
0.521
0.568
0.674

0.025
0.028
0.032
0.039
0.047
0.067
0.085
0.201
0.029
0.04
0.05
0.091
0.103
0.22
0.256
0.348

100
99.9
99.9
100
100
100
100
100
100
100
100
100
100
100
100
100

12.24
10.94
10.4
8.671
8.18
8.462
8.106
7.735
11.03
8.516
7.7
6.414
6.127
5.15
5.282
6.566

0.391
0.396
0.414
0.459
0.502
0.619
0.741
1.075
0.4
0.431
0.472
0.552
0.617
0.894
0.958
1.234

0.047
0.052
0.058
0.079
0.097
0.145
0.209
0.426
0.052
0.071
0.089
0.135
0.171
0.372
0.417
0.573

99.7
99.9
99.9
100
100
100
100
100
99.8
100
100
100
100
100
100
100

17.24
15.87
14.86
12.39
12.11
13.03
12.59
14.66
16.03
12.91
11.7
10.11
8.99
7.983
8.437
12.81

0.541
0.549
0.56
0.63
0.69
0.904
1.053
1.657
0.548
0.585
0.619
0.732
0.874
1.195
1.274
1.804

L∞

0.648
0.659
0.667
0.706
0.748
0.921
0.976
0.998
0.666
0.686
0.71
0.857
0.934
0.997
1
1

L∞

0.068
0.075
0.081
0.117
0.147
0.255
0.351
0.661
0.074
0.099
0.121
0.192
0.272
0.542
0.593
0.779

100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

Table 8: Comparison of different adversarial attacks on MNIST. ASR means attack success rate (%). N.A. means “not available”
due to zero ASR.

Attack
C&W (L2)
FGM-L1
FGM-L2
FGM-L∞
I-FGM-L1
I-FGM-L2
I-FGM-L∞
EAD (EN rule)
EAD (L1 rule)

ASR L1
100
99.9
99.4
99.9
100
100
100
100
100

13.93
26.56
25.84
83.82
18.34
17.6
49.8
9.808
7.153

Best case
L2
1.377
2.29
2.245
4.002
1.605
1.543
2.45
1.427
1.639

L∞
0.379
0.577
0.574
0.193
0.403
0.387
0.147
0.452
0.593

Average case

Worst case

ASR L1
100
39
34.6
42.5
100
100
100
100
100

22.46
53.5
39.15
127.2
32.94
30.32
71.39
17.4
14.11

L2
1.972
4.186
3.284
6.09
2.606
2.41
3.472
2.001
2.211

L∞
0.514
0.782
0.747
0.296
0.591
0.561
0.227
0.594
0.768

ASR L1
99.9
0
0
0
100
99.8
99.9
100
100

32.3
N.A.
N.A.
N.A.
53.02
47.8
95.48
25.52
22.05

L2
2.639
N.A.
N.A.
N.A.
3.979
3.597
4.604
2.582
2.747

L∞
0.663
N.A.
N.A.
N.A.
0.807
0.771
0.348
0.748
0.934

Table 9: Comparison of different adversarial attacks on CIFAR10. ASR means attack success rate (%).

Attack
C&W (L2)
FGM-L1
FGM-L2
FGM-L∞
I-FGM-L1
I-FGM-L2
I-FGM-L∞
EAD (EN rule)
EAD (L1 rule)

ASR L1
100
99.5
99.5
100
100
100
100
100
100

7.167
14.76
14.13
32.74
7.906
7.587
17.92
4.014
2.597

Best case
L2
0.209
0.434
0.421
0.595
0.232
0.223
0.35
0.261
0.359

Average case

Worst case

L∞
0.022
0.049
0.05
0.011
0.026
0.025
0.008
0.047
0.103

ASR L1
100
48.8
42.8
52.3
100
100
100
100
100

13.62
51.97
39.5
127.81
17.53
17.12
33.3
8.18
6.066

L2
0.392
1.48
1.157
2.373
0.502
0.489
0.68
0.502
0.613

L∞
0.044
0.152
0.136
0.047
0.055
0.054
0.018
0.097
0.17

ASR L1
99.9
0.7
0.7
0.6
100
100
100
100
100

19.17
157.5
107.1
246.4
29.73
28.94
48.3
12.11
8.986

L2
0.546
4.345
3.115
4.554
0.847
0.823
1.025
0.69
0.871

L∞
0.064
0.415
0.369
0.086
0.095
0.092
0.032
0.147
0.27

Table 10: Comparison of different adversarial attacks on ImageNet. ASR means attack success rate (%). N.A. means “not
available” due to zero ASR.

Method
C&W (L2)
FGM-L1
FGM-L2
FGM-L∞
I-FGM-L1
I-FGM-L2
I-FGM-L∞
EAD (EN rule)
EAD (L1 rule)

Best case
L2
0.511
0.661
2.29
45.76
0.89
0.95
1.163
1.007
1.167

157.3
193.3
752.9
21640
292.2
315.4
504.9
29.56
22.11

ASR L1
100
9
12
19
98
100
100
100
100

Average case

Worst case

L∞
0.018
0.025
0.087
0.115
0.03
0.03
0.004
0.128
0.195

ASR L1
100
1
1
3
77
100
100
100
100

232.2
61
2338
3655
526.4
774.1
864.2
69.47
40.9

L2
0.705
0.187
6.823
7.102
1.609
2.358
2.079
1.563
1.598

L∞
0.03
0.007
0.25
0.014
0.054
0.086
0.01
0.238
0.293

ASR L1
100
0
0
0
34
96
100
100
100

330.3
N.A.
N.A.
N.A.
695.5
1326
1408
160.3
100

L2
0.969
N.A.
N.A.
N.A.
2.104
4.064
3.465
2.3
2.391

L∞
0.044
N.A.
N.A.
N.A.
0.078
0.153
0.019
0.351
0.423

(a) EAD (EN rule)

(b) EAD (L1 rule)

(c) I-FGM-L1

(d) I-FGM-L2

(e) I-FGM-L∞

(f) C&W

Figure 6: Visual illustration of adversarial examples crafted by different attack methods on MNIST. For each method, the images
displayed on the diagonal are the original examples. In each row, the off-diagonal images are the corresponding adversarial
examples with columns indexing target labels (from left to right: digits 0 to 9).

(a) EAD (EN rule)

(b) EAD (L1 rule)

(c) I-FGM-L1

(d) I-FGM-L2

(e) I-FGM-L∞

(f) C&W

Figure 7: Visual illustration of adversarial examples crafted by different attack methods on CIFAR10. For each method, the im-
ages displayed on the diagonal are the original examples. In each row, the off-diagonal images are the corresponding adversarial
examples with columns indexing target labels.

Figure 8: Visual illustration of adversarial examples crafted by different attack methods on ImageNet. The original example is
the ostrich image (Figure 1 (a)). Each column represents a targeted class to attack, and each row represents an attack method.
.

Table 11: Comparison of the C&W method and EAD on attacking defensive distillation with different temperature parameter
T on MNIST. ASR means attack success rate (%).

Method

C&W
(L2)

EAD
(EN rule)

EAD
(L1 rule)

Best case
L2
1.31
1.514
1.407
1.409
1.425
1.449
1.506
1.297
1.315
1.493
1.525
1.363
1.566
1.461
1.463
1.481
1.503
1.559
1.353
1.369
1.546
1.575
1.571
1.806
1.674
1.613
1.62
1.645
1.723
1.524
1.554
1.755
1.801

14.02
16.32
16.08
16.23
16.16
16.48
16.94
15.39
15.86
16.91
16.99
9.672
11.79
11.54
11.82
11.58
12.11
12.56
11.1
11.6
12.57
12.72
6.91
8.472
8.305
8.978
8.9
9.319
9.628
8.419
8.698
9.219
9.243

ASR L1
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

T
1
10
20
30
40
50
60
70
80
90
100
1
10
20
30
40
50
60
70
80
90
100
1
10
20
30
40
50
60
70
80
90
100

L∞
0.347
0.393
0.336
0.332
0.355
0.34
0.36
0.297
0.291
0.357
0.365
0.416
0.468
0.404
0.398
0.426
0.408
0.431
0.356
0.347
0.424
0.433
0.562
0.628
0.556
0.51
0.536
0.51
0.546
0.466
0.462
0.557
0.579

Average case

Worst case

ASR L1
100
100
99.9
99.9
100
100
100
99.9
100
99.9
99.9
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

23.27
25.79
26.31
26.09
27.03
26.01
27.44
25.28
26.89
27.74
27.95
17.75
20.5
20.55
20.98
21.6
21.09
21.71
19.92
21.08
22.62
22.74
14
16.16
16.16
16.48
16.74
17.01
17.7
16.01
17.17
18.46
18.44

L2
1.938
2.183
2.111
2.083
2.164
2.111
2.247
1.961
2.062
2.256
2.3
1.954
2.224
2.133
2.134
2.226
2.161
2.256
2.001
2.076
2.312
2.335
2.149
2.428
2.391
2.417
2.475
2.401
2.477
2.193
2.283
2.529
2.535

L∞
0.507
0.539
0.489
0.468
0.501
0.486
0.512
0.453
0.46
0.508
0.518
0.58
0.617
0.566
0.546
0.583
0.56
0.588
0.518
0.519
0.587
0.596
0.758
0.798
0.764
0.749
0.778
0.744
0.752
0.674
0.677
0.744
0.759

ASR L1
99.8
99.9
99.6
99.7
100
99.9
99.7
99.8
99.9
99.8
99.8
100
100
100
100
100
99.9
100
99.9
99.7
99.9
100
100
99.9
100
100
99.9
100
100
100
100
99.9
99.9

32.96
36.18
37.59
38.05
39.06
36.74
38.74
36.58
38.83
39.77
40.11
26.42
29.59
30.97
31.92
32.48
30.52
32.47
29.99
31.91
33.38
33.82
22.41
25.38
25.84
25.72
25.66
24.59
26.38
24.31
25.97
27.9
28.44

L2
2.576
2.882
2.85
2.858
2.955
2.826
2.998
2.694
2.857
3.059
3.11
2.532
2.866
2.809
2.847
2.936
2.806
2.982
2.655
2.805
3.047
3.1
2.721
3.061
3.031
3.124
3.241
3.103
3.287
2.935
3.083
3.315
3.351

L∞
0.683
0.684
0.65
0.629
0.667
0.651
0.668
0.626
0.651
0.66
0.67
0.762
0.768
0.719
0.701
0.739
0.731
0.752
0.691
0.712
0.744
0.754
0.935
0.945
0.93
0.932
0.947
0.939
0.944
0.892
0.904
0.926
0.929

Table 12: Comparison of the C&W method and EAD on attacking defensive distillation with different temperature parameter
T on CIFAR10. ASR means attack success rate (%).

Method

C&W
(L2)

EAD
(EN rule)

EAD
(L1 rule)

Best case
L2
0.188
0.219
0.256
0.254
0.251
0.26
0.262
0.269
0.266
0.278
0.292
0.236
0.286
0.321
0.313
0.309
0.319
0.319
0.33
0.327
0.34
0.358
0.328
0.37
0.435
0.452
0.465
0.479
0.497
0.506
0.5
0.519
0.543

6.414
7.431
8.712
8.688
8.556
8.88
8.935
9.166
9.026
9.466
9.943
3.594
4.072
5.24
5.54
5.623
5.806
6.07
6.026
5.958
6.261
6.499
2.302
2.884
3.445
3.372
3.234
3.402
3.319
3.438
3.418
3.603
3.702

ASR L1
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

T
1
10
20
30
40
50
60
70
80
90
100
1
10
20
30
40
50
60
70
80
90
100
1
10
20
30
40
50
60
70
80
90
100

L∞
0.02
0.024
0.028
0.028
0.028
0.028
0.029
0.03
0.029
0.031
0.032
0.044
0.052
0.056
0.051
0.05
0.05
0.051
0.052
0.052
0.054
0.057
0.098
0.101
0.118
0.128
0.136
0.142
0.151
0.153
0.15
0.157
0.161

Average case

Worst case

ASR L1
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

12.46
15.48
18.7
19.34
18.43
19.56
19.43
19.99
19.93
21.21
21.46
7.471
9.669
11.79
12.33
12.02
13
13.06
13.85
13.74
14.07
15.02
5.595
6.857
8.943
8.802
8.537
8.965
8.647
9.344
9.202
9.654
9.839

L2
0.358
0.445
0.534
0.554
0.528
0.559
0.554
0.571
0.571
0.606
0.614
0.462
0.567
0.662
0.658
0.626
0.671
0.662
0.695
0.697
0.711
0.756
0.556
0.718
0.802
0.796
0.809
0.848
0.863
0.913
0.914
0.96
0.993

L∞
0.04
0.049
0.058
0.06
0.058
0.06
0.06
0.061
0.061
0.065
0.066
0.09
0.104
0.118
0.112
0.105
0.109
0.109
0.111
0.111
0.117
0.123
0.154
0.2
0.201
0.202
0.209
0.22
0.232
0.239
0.242
0.258
0.269

ASR L1
100
99.9
100
99.8
100
99.9
100
100
99.5
100
99.9
99.9
100
100
100
100
100
100
100
99.9
99.9
100
99.9
100
99.9
100
100
100
99.8
100
100
99.8
99.8

17.47
22.36
27.4
27.67
26.89
29.07
28.56
29.6
29.64
31.23
32.54
10.88
14.97
17.72
18.12
18.14
19.46
19.5
20.06
20.28
21.27
22.12
8.41
10.44
13.73
14.31
13.82
14.96
14.46
15.06
15.14
15.82
16.22

L2
0.498
0.635
0.776
0.785
0.761
0.822
0.809
0.838
0.839
0.884
0.921
0.638
0.782
0.932
0.94
0.915
0.979
0.964
0.997
0.999
1.046
1.084
0.776
1.037
1.164
1.124
1.093
1.17
1.174
1.214
1.227
1.287
1.351

L∞
0.058
0.073
0.087
0.089
0.086
0.091
0.091
0.093
0.092
0.099
0.103
0.136
0.154
0.175
0.175
0.167
0.176
0.173
0.177
0.174
0.189
0.192
0.237
0.321
0.322
0.304
0.297
0.317
0.325
0.331
0.337
0.361
0.379

Table 13: Comparison of attack transferability from the undefended network to the defensively distilled network (T = 100) on
MNIST with varying transferability parameter κ. ASR means attack success rate (%). N.A. means not “not available” due to
zero ASR. There is no κ parameter for I-FGM.

κ
Method
I-FGM-L1
None
I-FGM-L2
None
I-FGM-L∞ None

Best case
L2
1.604
1.537
2.311
1.103
1.491
2.033
2.491
2.665
2.719
2.829
3.012
3.255
3.553
3.892
4.284
4.703
1.197
1.547
1.916
2.263
2.519
2.694
2.838
3.009
3.248
3.526
3.843
4.193
4.562
1.431
1.807
2.154
2.481
2.718
2.897
3.023
3.186
3.406
3.665
3.957
4.293
4.66

18.39
17.77
46.38
11.13
15.58
21.94
27.65
29.71
30.12
31.17
33.27
36.13
39.86
44.2
49.37
54.97
8.373
11.45
15.36
19.18
21.98
23.92
25.52
27.42
30.23
33.61
37.59
42.01
46.7
6.392
8.914
12.16
15.39
17.73
19.71
21.1
23
25.86
29.4
33.71
38.09
42.7

ASR L1
12.2
9.8
14.7
5.4
16.6
42.2
74.2
92.9
98.7
99.8
100
100
100
100
100
100
6
18.2
39.5
69.2
89.5
98.3
99.9
100
100
100
100
100
100
6
19
40.6
70.5
90
98.6
99.8
100
100
100
100
100
100

L∞
0.418
0.395
0.145
0.338
0.424
0.525
0.603
0.639
0.664
0.69
0.727
0.772
0.818
0.868
0.907
0.937
0.426
0.515
0.59
0.651
0.692
0.724
0.748
0.778
0.814
0.857
0.899
0.934
0.961
0.628
0.728
0.773
0.809
0.83
0.851
0.862
0.882
0.904
0.931
0.95
0.971
0.985

Average case

ASR L1
19
1.6
17.25
1.3
48.3
1.7
10.16
1.1
17.35
3.4
21.97
6.5
32.54
22.6
38.34
44.4
45.41
62.9
49.63
78.1
55.56
84.2
61.25
87.4
67.82
85.2
70.87
80.6
76.77
73
82.07
67.9
4.876
0.6
13.07
2.5
16.45
8.4
22.74
19.2
28.36
37
34.14
58
40.2
76.3
45.62
87.9
52.33
95.2
57.75
98
66.22
98.6
70.66
94.4
75.59
90
6.57
0.5
9.717
3.2
13.74
7.5
18.12
19
24.15
39.4
30.33
59.3
37.38
76.9
41.13
89.3
47.54
96.3
55.16
97.6
62.01
98.1
65.79
93.6
72.49
89.6

L2
1.658
1.533
2.44
1.033
1.615
2.001
2.869
3.322
3.837
4.15
4.583
4.98
5.43
5.639
6.034
6.395
0.813
1.691
1.989
2.531
2.99
3.445
3.909
4.324
4.805
5.194
5.758
6.09
6.419
1.565
1.884
2.27
2.689
3.182
3.652
4.191
4.468
4.913
5.399
5.856
6.112
6.572

L∞
0.43
0.408
0.158
0.343
0.46
0.527
0.671
0.745
0.805
0.847
0.886
0.918
0.936
0.953
0.969
0.976
0.307
0.549
0.6
0.697
0.778
0.831
0.884
0.92
0.945
0.965
0.978
0.986
0.992
0.678
0.738
0.8
0.865
0.902
0.933
0.954
0.968
0.979
0.988
0.992
0.995
0.997

Worst case
L2
N.A.
N.A.
N.A.
N.A.
N.A.
N.A.
4.628
4.708
5.946
6.923
8.072
9.09
10.21
10.8
N.A.
N.A.
N.A.
N.A.
N.A.
3.238
3.951
4.65
5.404
6.176
6.981
7.904
8.851
9.487
10.3
N.A.
N.A.
N.A.
3.024
4.173
4.818
5.529
6.256
7.064
7.94
8.845
9.519
10.36

N.A.
N.A.
N.A.
N.A.
N.A.
N.A.
56.93
54.25
71.22
85.93
105.9
125.2
146.9
158.4
N.A.
N.A.
N.A.
N.A.
N.A.
31.18
39.91
49.12
59.9
70.93
83.19
98.51
115.7
127
140.35
N.A.
N.A.
N.A.
23.15
38.22
45.74
55.54
66.76
80.05
96.05
113.6
126.4
141.3

ASR L1
0
0
0
0
0
0
0.4
2.4
10.9
23
30.5
21
7.4
0.5
0
0
0
0
0
0.4
1.8
7.9
23.7
47.4
71.3
86.2
87
44.2
13.3
0
0
0
0.3
1.9
7.9
22.2
46.8
69.9
85.8
85.7
43.8
13

L∞
N.A.
N.A.
N.A.
N.A.
N.A.
N.A.
0.843
0.91
0.972
0.987
0.993
0.995
0.996
0.996
N.A.
N.A.
N.A.
N.A.
N.A.
0.846
0.897
0.973
0.993
0.999
1
1
1
1
1
N.A.
N.A.
N.A.
0.884
0.979
0.997
1
1
1
1
1
1
1

0
5
10
15
20
25
30
35
40
45
50
55
60
0
5
10
15
20
25
30
35
40
45
50
55
60
0
5
10
15
20
25
30
35
40
45
50
55
60

C&W
(L2)

EAD
(EN rule)

EAD
(L1 rule)

Table 14: Comparison of adversarial training using the C&W attack, EAD (L1 rule) and EAD (EN rule) on MNIST. ASR means
attack success rate.

Attack
method

C&W
(L2)

EAD
(L1)

EAD
(EN)

Adversarial
training
100
None
EAD (L1)
100
C&W (L2)
100
EAD + C&W 100
100
None
EAD (L1)
100
C&W (L2)
100
EAD + C&W 100
100
None
100
EAD (EN)
C&W (L2)
100
EAD + C&W 100

Best case

Average case

Worst case

ASR L1

L2

L∞

ASR L1

L2

L∞

ASR L1

L2

13.93
15.98
14.59
16.54
7.153
8.795
7.657
8.936
9.808
11.13
10.38
11.14

1.377
1.704
1.693
1.73
1.639
1.946
1.912
1.975
1.427
1.778
1.724
1.76

0.379
0.492
0.543
0.502
0.593
0.705
0.743
0.711
0.452
0.606
0.611
0.602

100
100
100
100
100
100
100
100
100
100
100
100

22.46
26.11
24.97
27.32
14.11
17.04
15.49
16.83
17.4
19.52
18.99
20.09

1.972
2.468
2.47
2.513
2.211
2.653
2.628
2.66
2.001
2.476
2.453
2.5

0.514
0.643
0.684
0.653
0.768
0.86
0.892
0.87
0.594
0.751
0.759
0.75

99.9
99.7
99.9
99.8
100
99.9
100
99.9
100
99.9
100
100

32.3
36.37
36.4
37.83
22.05
24.94
24.16
25.55
25.52
28.15
27.77
28.91

2.639
3.229
3.28
3.229
2.747
3.266
3.3
3.288
2.582
3.182
3.153
3.193

L∞

0.663
0.794
0.807
0.795
0.934
0.98
0.986
0.979
0.748
0.892
0.891
0.882

EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples

Pin-Yu Chen1∗, Yash Sharma2∗†, Huan Zhang3†, Jinfeng Yi4‡, Cho-Jui Hsieh3

1AI Foundations Lab, IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA
2The Cooper Union, New York, NY 10003, USA
3University of California, Davis, Davis, CA 95616, USA
4Tencent AI Lab, Bellevue, WA 98004, USA
pin-yu.chen@ibm.com, ysharma1126@gmail.com, ecezhang@ucdavis.edu, jinfengy@us.ibm.com, chohsieh@ucdavis.edu

8
1
0
2
 
b
e
F
 
0
1
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
4
1
1
4
0
.
9
0
7
1
:
v
i
X
r
a

Abstract

Recent studies have highlighted the vulnerability of deep neu-
ral networks (DNNs) to adversarial examples - a visually
indistinguishable adversarial image can easily be crafted to
cause a well-trained model to misclassify. Existing methods
for crafting adversarial examples are based on L2 and L∞
distortion metrics. However, despite the fact that L1 distor-
tion accounts for the total variation and encourages sparsity
in the perturbation, little has been developed for crafting L1-
based adversarial examples.
In this paper, we formulate the process of attacking DNNs via
adversarial examples as an elastic-net regularized optimiza-
tion problem. Our elastic-net attacks to DNNs (EAD) fea-
ture L1-oriented adversarial examples and include the state-
of-the-art L2 attack as a special case. Experimental results on
MNIST, CIFAR10 and ImageNet show that EAD can yield a
distinct set of adversarial examples with small L1 distortion
and attains similar attack performance to the state-of-the-art
methods in different attack scenarios. More importantly, EAD
leads to improved attack transferability and complements ad-
versarial training for DNNs, suggesting novel insights on
leveraging L1 distortion in adversarial machine learning and
security implications of DNNs.

Introduction
Deep neural networks (DNNs) achieve state-of-the-art per-
formance in various tasks in machine learning and artiﬁcial
intelligence, such as image classiﬁcation, speech recogni-
tion, machine translation and game-playing. Despite their
effectiveness, recent studies have illustrated the vulnerabil-
ity of DNNs to adversarial examples (Szegedy et al. 2013;
Goodfellow, Shlens, and Szegedy 2015). For instance, a
carefully designed perturbation to an image can lead a well-
trained DNN to misclassify. Even worse, effective adversar-
ial examples can also be made virtually indistinguishable to
human perception. For example, Figure 1 shows three adver-
sarial examples of an ostrich image crafted by our algorithm,
which are classiﬁed as “safe”, “shoe shop” and “vacuum” by
the Inception-v3 model (Szegedy et al. 2016), a state-of-the-
art image classiﬁcation model.

∗Pin-Yu Chen and Yash Sharma contribute equally to this work.
†This work was done during the internship of Yash Sharma and

Huan Zhang at IBM T. J. Watson Research Center.

‡Part of the work was done when Jinfeng Yi was at AI Founda-

tions Lab, IBM T. J. Watson Research Center.

(a) Image

(b) Adversarial examples with target class labels

Figure 1: Visual illustration of adversarial examples crafted
by EAD (Algorithm 1). The original example is an ostrich
image selected from the ImageNet dataset (Figure 1 (a)).
The adversarial examples in Figure 1 (b) are classiﬁed as
the target class labels by the Inception-v3 model.

The lack of robustness exhibited by DNNs to adversar-
ial examples has raised serious concerns for security-critical
applications, including trafﬁc sign identiﬁcation and mal-
ware detection, among others. Moreover, moving beyond
the digital space, researchers have shown that these adver-
sarial examples are still effective in the physical world at
fooling DNNs (Kurakin, Goodfellow, and Bengio 2016a;
Evtimov et al. 2017). Due to the robustness and security im-
plications, the means of crafting adversarial examples are
called attacks to DNNs. In particular, targeted attacks aim to
craft adversarial examples that are misclassiﬁed as speciﬁc
target classes, and untargeted attacks aim to craft adversarial
examples that are not classiﬁed as the original class. Trans-
fer attacks aim to craft adversarial examples that are trans-
ferable from one DNN model to another. In addition to eval-
uating the robustness of DNNs, adversarial examples can be
used to train a robust model that is resilient to adversarial
perturbations, known as adversarial training (Madry et al.
2017). They have also been used in interpreting DNNs (Koh
and Liang 2017; Dong et al. 2017).

Throughout this paper, we use adversarial examples to
attack image classiﬁers based on deep convolutional neu-
ral networks. The rationale behind crafting effective adver-
sarial examples lies in manipulating the prediction results
while ensuring similarity to the original image. Speciﬁcally,
in the literature the similarity between original and adversar-
ial examples has been measured by different distortion met-
rics. One commonly used distortion metric is the Lq norm,
where (cid:107)x(cid:107)q = ((cid:80)p
i=1 |xi|q)1/q denotes the Lq norm of a

p-dimensional vector x = [x1, . . . , xp] for any q ≥ 1. In
particular, when crafting adversarial examples, the L∞ dis-
tortion metric is used to evaluate the maximum variation
in pixel value changes (Goodfellow, Shlens, and Szegedy
2015), while the L2 distortion metric is used to improve the
visual quality (Carlini and Wagner 2017b). However, despite
the fact that the L1 norm is widely used in problems related
to image denoising and restoration (Fu et al. 2006), as well
as sparse recovery (Cand`es and Wakin 2008), L1-based ad-
versarial examples have not been rigorously explored. In the
context of adversarial examples, L1 distortion accounts for
the total variation in the perturbation and serves as a popular
convex surrogate function of the L0 metric, which measures
the number of modiﬁed pixels (i.e., sparsity) by the pertur-
bation. To bridge this gap, we propose an attack algorithm
based on elastic-net regularization, which we call elastic-
net attacks to DNNs (EAD). Elastic-net regularization is a
linear mixture of L1 and L2 penalty functions, and it has
been a standard tool for high-dimensional feature selection
problems (Zou and Hastie 2005). In the context of attacking
DNNs, EAD opens up new research directions since it gen-
eralizes the state-of-the-art attack proposed in (Carlini and
Wagner 2017b) based on L2 distortion, and is able to craft
L1-oriented adversarial examples that are more effective and
fundamentally different from existing attack methods.

To explore the utility of L1-based adversarial exam-
ples crafted by EAD, we conduct extensive experiments on
MNIST, CIFAR10 and ImageNet in different attack scenar-
ios. Compared to the state-of-the-art L2 and L∞ attacks
(Kurakin, Goodfellow, and Bengio 2016b; Carlini and Wag-
ner 2017b), EAD can attain similar attack success rate when
breaking undefended and defensively distilled DNNs (Pa-
pernot et al. 2016b). More importantly, we ﬁnd that L1 at-
tacks attain superior performance over L2 and L∞ attacks
in transfer attacks and complement adversarial training. For
the most difﬁcult dataset (MNIST), EAD results in improved
attack transferability from an undefended DNN to a defen-
sively distilled DNN, achieving nearly 99% attack success
rate. In addition, joint adversarial training with L1 and L2
based examples can further enhance the resilience of DNNs
to adversarial perturbations. These results suggest that EAD
yields a distinct, yet more effective, set of adversarial ex-
amples. Moreover, evaluating attacks based on L1 distor-
tion provides novel insights on adversarial machine learning
and security implications of DNNs, suggesting that L1 may
complement L2 and L∞ based examples toward furthering
a thorough adversarial machine learning framework.

Related Work
Here we summarize related works on attacking and defend-
ing DNNs against adversarial examples.

Attacks to DNNs
FGM and I-FGM: Let x0 and x denote the original and ad-
versarial examples, respectively, and let t denote the target
class to attack. Fast gradient methods (FGM) use the gradi-
ent ∇J of the training loss J with respect to x0 for craft-
ing adversarial examples (Goodfellow, Shlens, and Szegedy

2015). For L∞ attacks, x is crafted by

x = x0 − (cid:15) · sign(∇J(x0, t)),

(1)

where (cid:15) speciﬁes the L∞ distortion between x and x0, and
sign(∇J) takes the sign of the gradient. For L1 and L2 at-
tacks, x is crafted by

x = x0 − (cid:15)

∇J(x0, t)
(cid:107)∇J(x0, t)(cid:107)q

(2)

for q = 1, 2, where (cid:15) speciﬁes the corresponding distortion.
Iterative fast gradient methods (I-FGM) were proposed in
(Kurakin, Goodfellow, and Bengio 2016b), which iteratively
use FGM with a ﬁner distortion, followed by an (cid:15)-ball clip-
ping. Untargeted attacks using FGM and I-FGM can be im-
plemented in a similar fashion.
C&W attack: Instead of leveraging the training loss, Carlini
and Wagner designed an L2-regularized loss function based
on the logit layer representation in DNNs for crafting adver-
sarial examples (Carlini and Wagner 2017b). Its formulation
turns out to be a special case of our EAD formulation, which
will be discussed in the following section. The C&W attack
is considered to be one of the strongest attacks to DNNs, as it
can successfully break undefended and defensively distilled
DNNs and can attain remarkable attack transferability.
JSMA: Papernot et al. proposed a Jacobian-based saliency
map algorithm (JSMA) for characterizing the input-output
relation of DNNs (Papernot et al. 2016a). It can be viewed as
a greedy attack algorithm that iteratively modiﬁes the most
inﬂuential pixel for crafting adversarial examples.
DeepFool: DeepFool is an untargeted L2 attack algorithm
(Moosavi-Dezfooli, Fawzi, and Frossard 2016) based on the
theory of projection to the closest separating hyperplane in
classiﬁcation. It is also used to craft a universal perturba-
tion to mislead DNNs trained on natural images (Moosavi-
Dezfooli et al. 2016).
Black-box attacks: Crafting adversarial examples in the
black-box case is plausible if one allows querying of the tar-
get DNN. In (Papernot et al. 2017), JSMA is used to train
a substitute model for transfer attacks. In (?), an effective
black-box C&W attack is made possible using zeroth order
optimization (ZOO). In the more stringent attack scenario
where querying is prohibited, ensemble methods can be used
for transfer attacks (Liu et al. 2016).

Defenses in DNNs
Defensive distillation: Defensive distillation (Papernot et al.
2016b) defends against adversarial perturbations by using
the distillation technique in (Hinton, Vinyals, and Dean
2015) to retrain the same network with class probabilities
predicted by the original network. It also introduces the tem-
perature parameter T in the softmax layer to enhance the
robustness to adversarial perturbations.
Adversarial training: Adversarial
training can be imple-
mented in a few different ways. A standard approach is
augmenting the original training dataset with the label-
corrected adversarial examples to retrain the network. Mod-
ifying the training loss or the network architecture to in-
crease the robustness of DNNs to adversarial examples has

been proposed in (Zheng et al. 2016; Madry et al. 2017;
Tram`er et al. 2017; Zantedeschi, Nicolae, and Rawat 2017).
Detection methods: Detection methods utilize statistical
tests to differentiate adversarial from benign examples
(Feinman et al. 2017; Grosse et al. 2017; Lu, Issaranon, and
Forsyth 2017; Xu, Evans, and Qi 2017). However, 10 differ-
ent detection methods were unable to detect the C&W attack
(Carlini and Wagner 2017a).

EAD: Elastic-Net Attacks to DNNs
Preliminaries on Elastic-Net Regularization
Elastic-net regularization is a widely used technique in solv-
ing high-dimensional feature selection problems (Zou and
Hastie 2005). It can be viewed as a regularizer that lin-
early combines L1 and L2 penalty functions. In general,
elastic-net regularization is used in the following minimiza-
tion problem:

minimizez∈Z f (z) + λ1(cid:107)z(cid:107)1 + λ2(cid:107)z(cid:107)2
2,

(3)

where z is a vector of p optimization variables, Z indicates
the set of feasible solutions, f (z) denotes a loss function,
(cid:107)z(cid:107)q denotes the Lq norm of z, and λ1, λ2 ≥ 0 are the
L1 and L2 regularization parameters, respectively. The term
λ1(cid:107)z(cid:107)1+λ2(cid:107)z(cid:107)2
2 in (3) is called the elastic-net regularizer of
z. For standard regression problems, the loss function f (z)
is the mean squared error, the vector z represents the weights
(coefﬁcients) on the features, and the set Z = Rp. In partic-
ular, the elastic-net regularization in (3) degenerates to the
LASSO formulation when λ2 = 0, and becomes the ridge
regression formulation when λ1 = 0. It is shown in (Zou
and Hastie 2005) that elastic-net regularization is able to se-
lect a group of highly correlated features, which overcomes
the shortcoming of high-dimensional feature selection when
solely using the LASSO or ridge regression techniques.

EAD Formulation and Generalization
Inspired by the C&W attack (Carlini and Wagner 2017b),
we adopt the same loss function f for crafting adversarial
examples. Speciﬁcally, given an image x0 and its correct la-
bel denoted by t0, let x denote the adversarial example of
x0 with a target class t (cid:54)= t0. The loss function f (x) for
targeted attacks is deﬁned as

f (x, t) = max{max
j(cid:54)=t

[Logit(x)]j − [Logit(x)]t, −κ},

(4)

where Logit(x) = [[Logit(x)]1, . . . , [Logit(x)]K] ∈ RK
is the logit layer (the layer prior to the softmax layer) rep-
resentation of x in the considered DNN, K is the num-
ber of classes for classiﬁcation, and κ ≥ 0 is a conﬁ-
dence parameter that guarantees a constant gap between
maxj(cid:54)=t[Logit(x)]j and [Logit(x)]t.

It is worth noting that the term [Logit(x)]t is proportional
to the probability of predicting x as label t, since by the
softmax classiﬁcation rule,

Prob(Label(x) = t) =

exp([Logit(x)]t)
j=1 exp([Logit(x)]j)

.

(cid:80)K

(5)

Consequently, the loss function in (4) aims to render the la-
bel t the most probable class for x, and the parameter κ
controls the separation between t and the next most likely
prediction among all classes other than t. For untargeted at-
tacks, the loss function in (4) can be modiﬁed as

f (x) = max{[Logit(x)]t0 − max
j(cid:54)=t0

[Logit(x)]j, −κ}.

(6)

In this paper, we focus on targeted attacks since they are
more challenging than untargeted attacks. Our EAD algo-
rithm (Algorithm 1) can directly be applied to untargeted
attacks by replacing f (x, t) in (4) with f (x) in (6).

In addition to manipulating the prediction via the loss
function in (4), introducing elastic-net regularization further
encourages similarity to the original image when crafting
adversarial examples. Our formulation of elastic-net attacks
to DNNs (EAD) for crafting an adversarial example (x, t)
with respect to a labeled natural image (x0, t0) is as follows:
minimizex c · f (x, t) + β(cid:107)x − x0(cid:107)1 + (cid:107)x − x0(cid:107)2
2
subject to x ∈ [0, 1]p,

(7)
where f (x, t) is as deﬁned in (4), c, β ≥ 0 are the regular-
ization parameters of the loss function f and the L1 penalty,
respectively. The box constraint x ∈ [0, 1]p restricts x to a
properly scaled image space, which can be easily satisﬁed by
dividing each pixel value by the maximum attainable value
(e.g., 255). Upon deﬁning the perturbation of x relative to x0
as δ = x−x0, the EAD formulation in (7) aims to ﬁnd an ad-
versarial example x that will be classiﬁed as the target class
t while minimizing the distortion in δ in terms of the elastic-
net loss β(cid:107)δ(cid:107)1 + (cid:107)δ(cid:107)2
2, which is a linear combination of L1
and L2 distortion metrics between x and x0. Notably, the
formulation of the C&W attack (Carlini and Wagner 2017b)
becomes a special case of the EAD formulation in (7) when
β = 0, which disregards the L1 penalty on δ. However, the
L1 penalty is an intuitive regularizer for crafting adversarial
examples, as (cid:107)δ(cid:107)1 = (cid:80)p
i=1 |δi| represents the total varia-
tion of the perturbation, and is also a widely used surrogate
function for promoting sparsity in the perturbation. As will
be evident in the performance evaluation section, including
the L1 penalty for the perturbation indeed yields a distinct
set of adversarial examples, and it leads to improved attack
transferability and complements adversarial learning.

EAD Algorithm
When solving the EAD formulation in (7) without the L1
penalty (i.e., β = 0), Carlini and Wagner used a change-
of-variable (COV) approach via the tanh transformation on
x in order to remove the box constraint x ∈ [0, 1]p (Car-
lini and Wagner 2017b). When β > 0, we ﬁnd that the same
COV approach is not effective in solving (7), since the corre-
sponding adversarial examples are insensitive to the changes
in β (see the performance evaluation section for details).
Since the L1 penalty is a non-differentiable, yet piece-wise
linear, function, the failure of the COV approach in solving
(7) can be explained by its inefﬁciency in subgradient-based
optimization problems (Duchi and Singer 2009).

To efﬁciently solve the EAD formulation in (7) for
crafting adversarial examples, we propose to use the iter-
ative shrinkage-thresholding algorithm (ISTA) (Beck and

Teboulle 2009). ISTA can be viewed as a regular ﬁrst-
order optimization algorithm with an additional shrinkage-
thresholding step on each iteration. In particular, let g(x) =
c·f (x)+(cid:107)x−x0(cid:107)2
2 and let ∇g(x) be the numerical gradient
of g(x) computed by the DNN. At the k + 1-th iteration, the
adversarial example x(k+1) of x0 is computed by

x(k+1) = Sβ(x(k) − αk∇g(x(k))),
(8)
where αk denotes the step size at the k + 1-th iteration, and
Sβ : Rp (cid:55)→ Rp is an element-wise projected shrinkage-
thresholding function, which is deﬁned as

[Sβ(z)]i =

(cid:40) min{zi − β, 1},

x0i,
max{zi + β, 0},

if zi − x0i > β;
if |zi − x0i| ≤ β;
if zi − x0i < −β,

(9)
for any i ∈ {1, . . . , p}. If |zi − x0i| > β, it shrinks the
element zi by β and projects the resulting element to the
feasible box constraint between 0 and 1. On the other hand,
if |zi − x0i| ≤ β, it thresholds zi by setting [Sβ(z)]i = x0i.
The proof of optimality of using (8) for solving the EAD
formulation in (7) is given in the supplementary material1.
Notably, since g(x) is the attack objective function of the
C&W method (Carlini and Wagner 2017b), the ISTA oper-
ation in (8) can be viewed as a robust version of the C&W
method that shrinks a pixel value of the adversarial example
if the deviation to the original image is greater than β, and
keeps a pixel value unchanged if the deviation is less than β.
Our EAD algorithm for crafting adversarial examples is
summarized in Algorithm 1. For computational efﬁciency, a
fast ISTA (FISTA) for EAD is implemented, which yields
the optimal convergence rate for ﬁrst-order optimization
methods (Beck and Teboulle 2009). The slack vector y(k)
in Algorithm 1 incorporates the momentum in x(k) for ac-
celeration. In the experiments, we set the initial learning rate
α0 = 0.01 with a square-root decay factor in k. During the
EAD iterations, the iterate x(k) is considered as a success-
ful adversarial example of x0 if the model predicts its most
likely class to be the target class t. The ﬁnal adversarial ex-
ample x is selected from all successful examples based on
distortion metrics. In this paper we consider two decision
rules for selecting x: the least elastic-net (EN) and L1 dis-
tortions relative to x0. The inﬂuence of β, κ and the decision
rules on EAD will be investigated in the following section.

Performance Evaluation
In this section, we compare the proposed EAD with the
state-of-the-art attacks to DNNs on three image classiﬁca-
tion datasets - MNIST, CIFAR10 and ImageNet. We would
like to show that (i) EAD can attain attack performance sim-
ilar to the C&W attack in breaking undefended and defen-
sively distilled DNNs, since the C&W attack is a special
case of EAD when β = 0; (ii) Comparing to existing L1-
based FGM and I-FGM methods, the adversarial examples
using EAD can lead to signiﬁcantly lower L1 distortion and
better attack success rate; (iii) The L1-based adversarial ex-
amples crafted by EAD can achieve improved attack trans-
ferability and complement adversarial training.

1https://arxiv.org/abs/1709.04114

Algorithm 1 Elastic-Net Attacks to DNNs (EAD)

Input: original labeled image (x0, t0), target attack class
t, attack transferability parameter κ, L1 regularization pa-
rameter β, step size αk, # of iterations I
Output: adversarial example x
Initialization: x(0) = y(0) = x0
for k = 0 to I − 1 do

x(k+1) = Sβ(y(k) − αk∇g(y(k)))
y(k+1) = x(k+1) + k

k+3 (x(k+1) − x(k))

end for
Decision rule: determine x from successful examples in
{x(k)}I

k=1 (EN rule or L1 rule).

Comparative Methods
We compare EAD with the following targeted attacks, which
are the most effective methods for crafting adversarial exam-
ples in different distortion metrics.
C&W attack: The state-of-the-art L2 targeted attack pro-
posed by Carlini and Wagner (Carlini and Wagner 2017b),
which is a special case of EAD when β = 0.
FGM: The fast gradient method proposed in (Goodfellow,
Shlens, and Szegedy 2015). The FGM attacks using differ-
ent distortion metrics are denoted by FGM-L1, FGM-L2 and
FGM-L∞.
I-FGM: The iterative fast gradient method proposed in (Ku-
rakin, Goodfellow, and Bengio 2016b). The I-FGM attacks
using different distortion metrics are denoted by I-FGM-L1,
I-FGM-L2 and I-FGM-L∞.

Experiment Setup and Parameter Setting
Our experiment setup is based on Carlini and Wagner’s
framework2. For both the EAD and C&W attacks, we use the
default setting1, which implements 9 binary search steps on
the regularization parameter c (starting from 0.001) and runs
I = 1000 iterations for each step with the initial learning
rate α0 = 0.01. For ﬁnding successful adversarial examples,
we use the reference optimizer1 (ADAM) for the C&W at-
tack and implement the projected FISTA (Algorithm 1) with
the square-root decaying learning rate for EAD. Similar to
the C&W attack, the ﬁnal adversarial example of EAD is se-
lected by the least distorted example among all the success-
ful examples. The sensitivity analysis of the L1 parameter β
and the effect of the decision rule on EAD will be investi-
gated in the forthcoming paragraph. Unless speciﬁed, we set
the attack transferability parameter κ = 0 for both attacks.

We implemented FGM and I-FGM using the CleverHans
package3. The best distortion parameter (cid:15) is determined by
a ﬁne-grained grid search - for each image, the smallest (cid:15)
in the grid leading to a successful attack is reported. For
I-FGM, we perform 10 FGM iterations (the default value)
with (cid:15)-ball clipping. The distortion parameter (cid:15)(cid:48) in each
FGM iteration is set to be (cid:15)/10, which has been shown to
be an effective attack setting in (Tram`er et al. 2017). The

2https://github.com/carlini/nn robust attacks
3https://github.com/tensorﬂow/cleverhans

Table 1: Comparison of the change-of-variable (COV) approach and EAD (Algorithm 1) for solving the elastic-net formulation
in (7) on MNIST. ASR means attack success rate (%). Although these two methods attain similar attack success rates, COV is
not effective in crafting L1-based adversarial examples. Increasing β leads to less L1-distorted adversarial examples for EAD,
whereas the distortion of COV is insensitive to changes in β.

Optimization
method

COV

EAD
(EN rule)

Best case

Average case

Worst case

β

0
10−5
10−4
10−3
10−2
0
10−5
10−4
10−3
10−2

ASR L1

L2

L∞

ASR L1

L2

L∞

ASR L1

L2

100
100
100
100
100
100
100
100
100
100

13.93
13.92
13.91
13.8
12.98
14.04
13.66
12.79
9.808
7.271

1.377
1.377
1.377
1.377
1.38
1.369
1.369
1.372
1.427
1.718

0.379
0.379
0.379
0.381
0.389
0.376
0.378
0.388
0.452
0.674

100
100
100
100
100
100
100
100
100
100

22.46
22.66
23.11
22.42
22.27
22.63
22.6
20.98
17.4
13.56

1.972
1.98
2.013
1.977
2.026
1.953
1.98
1.951
2.001
2.395

0.514
0.508
0.517
0.512
0.53
0.512
0.515
0.521
0.594
0.852

99.9
99.5
100
99.9
99.5
99.8
99.9
100
100
100

32.3
32.33
32.32
32.2
31.41
31.43
30.79
29.21
25.52
20.77

2.639
2.64
2.639
2.639
2.643
2.51
2.507
2.514
2.582
3.021

L∞

0.663
0.663
0.664
0.664
0.673
0.644
0.648
0.667
0.748
0.976

range of the grid and the resolution of these two methods
are speciﬁed in the supplementary material1.

The image classiﬁers for MNIST and CIFAR10 are
trained based on the DNN models provided by Carlini and
Wagner1. The image classiﬁer for ImageNet is the Inception-
v3 model (Szegedy et al. 2016). For MNIST and CIFAR10,
1000 correctly classiﬁed images are randomly selected from
the test sets to attack an incorrect class label. For ImageNet,
100 correctly classiﬁed images and 9 incorrect classes are
randomly selected to attack. All experiments are conducted
on a machine with an Intel E5-2690 v3 CPU, 40 GB RAM
and a single NVIDIA K80 GPU. Our EAD code is publicly
available for download4.

Evaluation Metrics
Following the attack evaluation criterion in (Carlini and
Wagner 2017b), we report the attack success rate and dis-
tortion of the adversarial examples from each method. The
attack success rate (ASR) is deﬁned as the percentage of
adversarial examples that are classiﬁed as the target class
(which is different from the original class). The average L1,
L2 and L∞ distortion metrics of successful adversarial ex-
amples are also reported. In particular, the ASR and distor-
tion of the following attack settings are considered:
Best case: The least difﬁcult attack among targeted attacks
to all incorrect class labels in terms of distortion.
Average case: The targeted attack to a randomly selected
incorrect class label.
Worst case: The most difﬁcult attack among targeted at-
tacks to all incorrect class labels in terms of distortion.

Sensitivity Analysis and Decision Rule for EAD
We verify the necessity of using Algorithm 1 for solving
the elastic-net regularized attack formulation in (7) by com-
paring it to a naive change-of-variable (COV) approach. In
(Carlini and Wagner 2017b), Carlini and Wagner remove the
box constraint x ∈ [0, 1]p by replacing x with 1+tanh w
,

2

4https://github.com/ysharma1126/EAD-Attack

where w ∈ Rp and 1 ∈ Rp is a vector of ones. The de-
fault ADAM optimizer (Kingma and Ba 2014) is then used
to solve w and obtain x. We apply this COV approach to
(7) and compare with EAD on MNIST with different orders
of the L1 regularization parameter β in Table 1. Although
COV and EAD attain similar attack success rates, it is ob-
served that COV is not effective in crafting L1-based ad-
versarial examples. Increasing β leads to less L1-distorted
adversarial examples for EAD, whereas the distortion (L1,
L2 and L∞) of COV is insensitive to changes in β. Similar
insensitivity of COV on β is observed when one uses other
optimizers such as AdaGrad, RMSProp or built-in SGD in
TensorFlow. We also note that the COV approach prohibits
the use of ISTA due to the subsequent tanh term in the L1
penalty. The insensitivity of COV suggests that it is inade-
quate for elastic-net optimization, which can be explained by
its inefﬁciency in subgradient-based optimization problems
(Duchi and Singer 2009). For EAD, we also ﬁnd an interest-
ing trade-off between L1 and the other two distortion met-
rics - adversarial examples with smaller L1 distortion tend
to have larger L2 and L∞ distortions. This trade-off can be
explained by the fact that increasing β further encourages
sparsity in the perturbation, and hence results in increased
L2 and L∞ distortion. Similar results are observed on CI-
FAR10 (see supplementary material1).

In Table 1, during the attack optimization process the ﬁnal
adversarial example is selected based on the elastic-net loss
of all successful adversarial examples in {x(k)}I
k=1, which
we call the elastic-net (EN) decision rule. Alternatively, we
can select the ﬁnal adversarial example with the least L1
distortion, which we call the L1 decision rule. Figure 2 com-
pares the ASR and average-case distortion of these two de-
cision rules with different β on MNIST. Both decision rules
yield 100% ASR for a wide range of β values. For the same
β, the L1 rule gives adversarial examples with less L1 dis-
tortion than those given by the EN rule at the price of larger
L2 and L∞ distortions. Similar trends are observed on CI-
FAR10 (see supplementary material1). The complete results

Figure 2: Comparison of EN and L1 decision rules in EAD on MNIST with varying L1 regularization parameter β (average
case). Comparing to the EN rule, for the same β the L1 rule attains less L1 distortion but may incur more L2 and L∞ distortions.

Table 2: Comparison of different attacks on MNIST, CIFAR10 and ImageNet (average case). ASR means attack success rate
(%). The distortion metrics are averaged over successful examples. EAD, the C&W attack, and I-FGM-L∞ attain the least L1,
L2, and L∞ distorted adversarial examples, respectively. The complete attack results are given in the supplementary material1.

Attack method ASR L1
C&W (L2)
FGM-L1
FGM-L2
FGM-L∞
I-FGM-L1
I-FGM-L2
I-FGM-L∞
EAD (EN rule)
EAD (L1 rule)

100
39
34.6
42.5
100
100
100
100
100

22.46
53.5
39.15
127.2
32.94
30.32
71.39
17.4
14.11

MNIST
L2
1.972
4.186
3.284
6.09
2.606
2.41
3.472
2.001
2.211

L∞
0.514
0.782
0.747
0.296
0.591
0.561
0.227
0.594
0.768

CIFAR10
L2
0.392
1.48
1.157
2.373
0.502
0.489
0.68
0.502
0.613

13.62
51.97
39.5
127.81
17.53
17.12
33.3
8.18
6.066

ASR L1
100
48.8
42.8
52.3
100
100
100
100
100

L∞
0.044
0.152
0.136
0.047
0.055
0.054
0.018
0.097
0.17

ImageNet
L2
0.705
0.187
6.823
7.102
1.609
2.358
2.079
1.563
1.598

232.2
61
2338
3655
526.4
774.1
864.2
69.47
40.9

ASR L1
100
1
1
3
77
100
100
100
100

L∞
0.03
0.007
0.25
0.014
0.054
0.086
0.01
0.238
0.293

of these two rules on MNIST and CIFAR10 are given in the
supplementary material1. In the following experiments, we
will report the results of EAD with these two decision rules
and set β = 10−3, since on MNIST and CIFAR10 this β
value signiﬁcantly reduces the L1 distortion while having
comparable L2 and L∞ distortions to the case of β = 0
(i.e., without L1 regularization).

Attack Success Rate and Distortion on MNIST,
CIFAR10 and ImageNet

We compare EAD with the comparative methods in terms
of attack success rate and different distortion metrics on at-
tacking the considered DNNs trained on MNIST, CIFAR10
and ImageNet. Table 2 summarizes their average-case per-
formance. It is observed that FGM methods fail to yield suc-
cessful adversarial examples (i.e., low ASR), and the cor-
responding distortion metrics are signiﬁcantly larger than
other methods. On the other hand, the C&W attack, I-FGM
and EAD all lead to 100% attack success rate. Further-
more, EAD, the C&W method, and I-FGM-L∞ attain the
least L1, L2, and L∞ distorted adversarial examples, respec-
tively. We note that EAD signiﬁcantly outperforms the exist-
ing L1-based method (I-FGM-L1). Compared to I-FGM-L1,
EAD with the EN decision rule reduces the L1 distortion by
roughly 47% on MNIST, 53% on CIFAR10 and 87% on Im-
ageNet. We also observe that EAD with the L1 decision rule
can further reduce the L1 distortion but at the price of no-
ticeable increase in the L2 and L∞ distortion metrics.

Notably, despite having large L2 and L∞ distortion met-

Figure 3: Attack success rate (average case) of the C&W
method and EAD on MNIST and CIFAR10 with respect to
varying temperature parameter T for defensive distillation.
Both methods can successfully break defensive distillation.

rics, the adversarial examples crafted by EAD with the L1
rule can still attain 100% ASRs in all datasets, which implies
the L2 and L∞ distortion metrics are insufﬁcient for evaluat-
ing the robustness of neural networks. Moreover, the attack
results in Table 2 suggest that EAD can yield a set of distinct
adversarial examples that are fundamentally different from
L2 or L∞ based examples. Similar to the C&W method and
I-FGM, the adversarial examples from EAD are also visu-
ally indistinguishable (see supplementary material1).

Breaking Defensive Distillation

In addition to breaking undefended DNNs via adversarial
examples, here we show that EAD can also break defen-
sively distilled DNNs. Defensive distillation (Papernot et al.

Table 3: Adversarial training using the C&W attack and
EAD (L1 rule) on MNIST. ASR means attack success rate.
Incorporating L1 examples complements adversarial train-
ing and enhances attack difﬁculty in terms of distortion. The
complete results are given in the supplementary material1.

Average case

Attack
method

C&W
(L2)

EAD
(L1 rule)

Adversarial
ASR L1
training
None
100
100
EAD
C&W
100
EAD + C&W 100
100
None
EAD
100
C&W
100
EAD + C&W 100

22.46
26.11
24.97
27.32
14.11
17.04
15.49
16.83

L2
1.972
2.468
2.47
2.513
2.211
2.653
2.628
2.66

L∞
0.514
0.643
0.684
0.653
0.768
0.86
0.892
0.87

which can be explained by the fact that the ISTA operation
in (8) is a robust version of the C&W attack via shrinking
and thresholding. We also ﬁnd that setting κ too large may
mitigate the ASR of transfer attacks for both EAD and the
C&W method, as the optimizer may fail to ﬁnd an adver-
sarial example that minimizes the loss function f in (4) for
large κ. The complete attack transferability results are given
in the supplementary material1.

Complementing Adversarial Training
To further validate the difference between L1-based and L2-
based adversarial examples, we test their performance in ad-
versarial training on MNIST. We randomly select 1000 im-
ages from the training set and use the C&W attack and EAD
(L1 rule) to generate adversarial examples for all incorrect
labels, leading to 9000 adversarial examples in total for each
method. We then separately augment the original training set
with these examples to retrain the network and test its ro-
bustness on the testing set, as summarized in Table 3. For
adversarial training with any single method, although both
attacks still attain a 100% success rate in the average case,
the network is more tolerable to adversarial perturbations, as
all distortion metrics increase signiﬁcantly when compared
to the null case. We also observe that joint adversarial train-
ing with EAD and the C&W method can further increase the
L1 and L2 distortions against the C&W attack and the L2
distortion against EAD, suggesting that the L1-based exam-
ples crafted by EAD can complement adversarial training.

Conclusion
We proposed an elastic-net regularized attack framework
for crafting adversarial examples to attack deep neural net-
works. Experimental results on MNIST, CIFAR10 and Ima-
geNet show that the L1-based adversarial examples crafted
by EAD can be as successful as the state-of-the-art L2 and
L∞ attacks in breaking undefended and defensively distilled
networks. Furthermore, EAD can improve attack transfer-
ability and complement adversarial training. Our results cor-
roborate the effectiveness of EAD and shed new light on
the use of L1-based adversarial examples toward adversarial
learning and security implications of deep neural networks.

Figure 4: Attack transferability (average case) from the un-
defended network to the defensively distilled network on
MNIST by varying κ. EAD can attain nearly 99% attack
success rate (ASR) when κ = 50, whereas the top ASR of
the C&W attack is nearly 88% when κ = 40.

2016b) is a standard defense technique that retrains the net-
work with class label probabilities predicted by the original
network, soft labels, and introduces the temperature parame-
ter T in the softmax layer to enhance its robustness to adver-
sarial perturbations. Similar to the state-of-the-art attack (the
C&W method), Figure 3 shows that EAD can attain 100%
attack success rate for different values of T on MNIST and
CIFAR10. Moreover, since the C&W attack formulation is
a special case of the EAD formulation in (7) when β = 0,
successfully breaking defensive distillation using EAD sug-
gests new ways of crafting effective adversarial examples by
varying the L1 regularization parameter β. The complete at-
tack results are given in the supplementary material1.

Improved Attack Transferability
It has been shown in (Carlini and Wagner 2017b) that the
C&W attack can be made highly transferable from an unde-
fended network to a defensively distilled network by tuning
the conﬁdence parameter κ in (4). Following (Carlini and
Wagner 2017b), we adopt the same experiment setting for
attack transferability on MNIST, as MNIST is the most dif-
ﬁcult dataset to attack in terms of the average distortion per
image pixel from Table 2.

Fixing κ, adversarial examples generated from the origi-
nal (undefended) network are used to attack the defensively
distilled network with the temperature parameter T = 100
(Papernot et al. 2016b). The attack success rate (ASR) of
EAD, the C&W method and I-FGM are shown in Figure 4.
When κ = 0, all methods attain low ASR and hence do
not produce transferable adversarial examples. The ASR of
EAD and the C&W method improves when we set κ > 0,
whereas I-FGM’s ASR remains low (less than 2%) since the
attack does not have such a parameter for transferability.

Notably, EAD can attain nearly 99% ASR when κ = 50,
whereas the top ASR of the C&W method is nearly 88%
when κ = 40. This implies improved attack transferabil-
ity when using the adversarial examples crafted by EAD,

Acknowledgment Cho-Jui Hsieh and Huan Zhang ac-

knowledge the support of NSF via IIS-1719097.

References

Beck, A., and Teboulle, M. 2009. A fast iterative shrinkage-
thresholding algorithm for linear inverse problems. SIAM
journal on imaging sciences 2(1):183–202.
Cand`es, E. J., and Wakin, M. B. 2008. An introduction to
IEEE signal processing magazine
compressive sampling.
25(2):21–30.
Carlini, N., and Wagner, D. 2017a. Adversarial examples are
not easily detected: Bypassing ten detection methods. arXiv
preprint arXiv:1705.07263.
Carlini, N., and Wagner, D. 2017b. Towards evaluating the
robustness of neural networks. In IEEE Symposium on Se-
curity and Privacy (SP), 39–57.
Dong, Y.; Su, H.; Zhu, J.; and Bao, F. 2017. Towards in-
terpretable deep neural networks by leveraging adversarial
examples. arXiv preprint arXiv:1708.05493.
Duchi, J., and Singer, Y. 2009. Efﬁcient online and batch
learning using forward backward splitting. Journal of Ma-
chine Learning Research 10(Dec):2899–2934.
Evtimov, I.; Eykholt, K.; Fernandes, E.; Kohno, T.; Li, B.;
Prakash, A.; Rahmati, A.; and Song, D. 2017. Robust
physical-world attacks on machine learning models. arXiv
preprint arXiv:1707.08945.
Feinman, R.; Curtin, R. R.; Shintre, S.; and Gardner, A. B.
2017. Detecting adversarial samples from artifacts. arXiv
preprint arXiv:1703.00410.
Fu, H.; Ng, M. K.; Nikolova, M.; and Barlow, J. L. 2006. Ef-
ﬁcient minimization methods of mixed l2-l1 and l1-l1 norms
for image restoration. SIAM Journal on scientiﬁc computing
27(6):1881–1902.
Goodfellow, I. J.; Shlens, J.; and Szegedy, C. 2015. Explain-
ICLR’15; arXiv
ing and harnessing adversarial examples.
preprint arXiv:1412.6572.
Grosse, K.; Manoharan, P.; Papernot, N.; Backes, M.; and
McDaniel, P. 2017. On the (statistical) detection of adver-
sarial examples. arXiv preprint arXiv:1702.06280.
Hinton, G.; Vinyals, O.; and Dean, J.
ing the knowledge in a neural network.
arXiv:1503.02531.
Kingma, D., and Ba, J. 2014. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980.
Koh, P. W., and Liang, P. 2017. Understanding black-box
ICML; arXiv preprint
predictions via inﬂuence functions.
arXiv:1703.04730.
Kurakin, A.; Goodfellow, I.; and Bengio, S. 2016a. Ad-
versarial examples in the physical world. arXiv preprint
arXiv:1607.02533.
Kurakin, A.; Goodfellow, I.; and Bengio, S. 2016b. Adver-
ICLR’17; arXiv preprint
sarial machine learning at scale.
arXiv:1611.01236.

2015. Distill-
arXiv preprint

Liu, Y.; Chen, X.; Liu, C.; and Song, D. 2016. Delv-
ing into transferable adversarial examples and black-box at-
tacks. arXiv preprint arXiv:1611.02770.
Lu, J.; Issaranon, T.; and Forsyth, D. 2017. Safetynet: De-
tecting and rejecting adversarial examples robustly. arXiv
preprint arXiv:1704.00103.
Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and
Vladu, A. 2017. Towards deep learning models resistant
to adversarial attacks. arXiv preprint arXiv:1706.06083.
Moosavi-Dezfooli, S.-M.; Fawzi, A.; Fawzi, O.; and
Frossard, P. 2016. Universal adversarial perturbations. arXiv
preprint arXiv:1610.08401.
Moosavi-Dezfooli, S.-M.; Fawzi, A.; and Frossard, P. 2016.
Deepfool: a simple and accurate method to fool deep neural
networks. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2574–2582.
Papernot, N.; McDaniel, P.; Jha, S.; Fredrikson, M.; Celik,
Z. B.; and Swami, A. 2016a. The limitations of deep learn-
ing in adversarial settings. In IEEE European Symposium on
Security and Privacy (EuroS&P), 372–387.
Papernot, N.; McDaniel, P.; Wu, X.; Jha, S.; and Swami, A.
2016b. Distillation as a defense to adversarial perturbations
against deep neural networks. In IEEE Symposium on Secu-
rity and Privacy (SP), 582–597.
Papernot, N.; McDaniel, P.; Goodfellow, I.; Jha, S.; Celik,
Z. B.; and Swami, A. 2017. Practical black-box attacks
against machine learning. In ACM Asia Conference on Com-
puter and Communications Security, 506–519.
Parikh, N.; Boyd, S.; et al. 2014. Proximal algorithms. Foun-
dations and Trends R(cid:13) in Optimization 1(3):127–239.
Szegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan,
D.; Goodfellow, I.; and Fergus, R. 2013. Intriguing proper-
ties of neural networks. arXiv preprint arXiv:1312.6199.
Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna,
Z. 2016. Rethinking the inception architecture for computer
vision. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2818–2826.
Tram`er, F.; Kurakin, A.; Papernot, N.; Boneh, D.; and Mc-
Daniel, P. 2017. Ensemble adversarial training: Attacks and
defenses. arXiv preprint arXiv:1705.07204.
Xu, W.; Evans, D.; and Qi, Y. 2017. Feature squeezing: De-
tecting adversarial examples in deep neural networks. arXiv
preprint arXiv:1704.01155.
Zantedeschi, V.; Nicolae, M.-I.; and Rawat, A. 2017. Ef-
ﬁcient defenses against adversarial attacks. arXiv preprint
arXiv:1707.06728.
Zheng, S.; Song, Y.; Leung, T.; and Goodfellow, I. 2016.
Improving the robustness of deep neural networks via sta-
bility training. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 4480–4488.
Zou, H., and Hastie, T. 2005. Regularization and variable
selection via the elastic net. Journal of the Royal Statistical
Society: Series B (Statistical Methodology) 67(2):301–320.

Supplementary Material

Proof of Optimality of (8) for Solving EAD in (3)
Since the L1 penalty β(cid:107)x − x0(cid:107)1 in (3) is a non-
differentiable yet smooth function, we use the proximal gra-
dient method (Parikh, Boyd, and others 2014) for solving
the EAD formulation in (3). Deﬁne ΦZ (z) to be the indica-
tor function of an interval Z such that ΦZ (z) = 0 if z ∈ Z
and ΦZ (z) = ∞ if z /∈ Z. Using ΦZ (z), the EAD formula-
tion in (3) can be rewritten as

minimizex∈Rp g(x) + β(cid:107)x − x0(cid:107)1 + Φ[0,1]p (x),

(10)

where g(x) = c·f (x, t)+(cid:107)x−x0(cid:107)2
Prox(x) of β(cid:107)x − x0(cid:107)1 constrained to x ∈ [0, 1]p is

2. The proximal operator

Prox(x) = arg min
z∈Rp

(cid:107)z − x(cid:107)2

2 + β(cid:107)z − x0(cid:107)1 + Φ[0,1]p (z)

(cid:107)z − x(cid:107)2

2 + β(cid:107)z − x0(cid:107)1

1
2

1
2

= arg min

z∈[0,1]p

= Sβ(x),

(11)

(12)

(13)

where the mapping function Sβ is deﬁned in (9). Con-
sequently, using (11), the proximal gradient algorithm for
solving (7) is iterated by

x(k+1) = Prox(x(k) − αk∇g(x(k)))
= Sβ(x(k) − αk∇g(x(k))),

which completes the proof.

Grid Search for FGM and I-FGM (Table 4)
To determine the optimal distortion parameter (cid:15) for FGM
and I-FGM methods, we adopt a ﬁne grid search on (cid:15). For
each image, the best parameter is the smallest (cid:15) in the grid
leading to a successful targeted attack. If the grid search fails
to ﬁnd a successful adversarial example, the attack is consid-
ered in vain. The selected range for grid search covers the
reported distortion statistics of EAD and the C&W attack.
The resolution of the grid search for FGM is selected such
that it will generate 1000 candidates of adversarial examples
during the grid search per input image. The resolution of the
grid search for I-FGM is selected such that it will compute
gradients for 10000 times in total (i.e., 1000 FGM opera-
tions × 10 iterations) during the grid search per input im-
age, which is more than the total number of gradients (9000)
computed by EAD and the C&W attack.

Table 4: Range and resolution of grid search for ﬁnding the
optimal distortion parameter (cid:15) for FGM and I-FGM.

Grid search

Range
Method
[10−3, 1]
FGM-L∞
[1, 103]
FGM-L1
[10−2, 10]
FGM-L2
I-FGM-L∞ [10−3, 1]
I-FGM-L1
I-FGM-L2

[1, 103]
[10−2, 10]

Resolution
10−3
1
10−2
10−3
1
10−2

Comparison of COV and EAD on CIFAR10 (Table
5)

Table 5 compares the attack performance of using EAD (Al-
gorithm 1)) and the change-of-variable (COV) approach for
solving the elastic-net formulation in (7) on CIFAR10. Sim-
ilar to the MNIST results in Table 1, although COV and
EAD attain similar attack success rates, we ﬁnd that COV
is not effective in crafting L1-based adversarial examples.
Increasing β leads to less L1-distorted adversarial examples
for EAD, whereas the distortion (L1, L2 and L∞) of COV
is insensitive to changes in β. The insensitivity of COV sug-
gests that it is inadequate for elastic-net optimization, which
can be explained by its inefﬁciency in subgradient-based op-
timization problems (Duchi and Singer 2009).

Comparison of EN and L1 decision rules in EAD
on MNIST and CIFAR10 (Tables 6 and 7)

Figure 5 compares the average-case distortion of these two
decision rules with different values of β on CIFAR10. For
the same β, the L1 rule gives less L1 distorted adversarial
examples than those given by the EN rule at the price of
larger L2 and L∞ distortions. We also observe that the L1
distortion does not decrease monotonically with β. In partic-
ular, large β values (e.g., β = 5·10−2) may lead to increased
L1 distortion due to excessive shrinking and thresholding.
Table 6 and Table 7 displays the complete attack results of
these two decision rules on MNIST and CIFAR10, respec-
tively.

Complete Attack Results and Visual Illustration on
MNIST, CIFAR10 and ImageNet (Tables 8, 9 and
10 and Figures 6, 7 and 8)

Tables 8, 9 and 10 summarize the complete attack results
of all the considered attack methods on MNIST, CIFAR10
and ImageNet, respectively. EAD, the C&W attack and I-
FGM all lead to 100% attack success rate in the average
case. Among the three image classiﬁcation datasets, Ima-
geNet is the easiest one to attack due to low distortion per
image pixel, and MNIST is the most difﬁcult one to attack.
For the purpose of visual illustration, the adversarial exam-
ples of selected benign images from the test sets are dis-
played in Figures 6, 7 and 8. On CIFAR10 and ImageNet,
the adversarial examples are visually indistinguishable. On
MNIST, the I-FGM examples are blurrier than EAD and the
C&W attack.

Complete Results on Attacking Defensive
Distillation on MNIST and CIFAR10 (Tables 11
and 12)

Tables 11 and 12 display the complete attack results of EAD
and the C&W method on breaking defensive distillation with
different temperature parameter T on MNIST and CIFAR10.
Although defensive distillation is a standard defense tech-
nique for DNNs, EAD and the C&W attack can successfully
break defensive distillation with a wide range of temperature
parameters.

Table 5: Comparison of the change-of-variable (COV) approach and EAD (Algorithm 1) for solving the elastic-net formulation
in (7) on CIFAR10. ASR means attack success rate (%). Similar to the results on MNIST in Table 1, increasing β leads to less
L1-distorted adversarial examples for EAD, whereas the distortion of COV is insensitive to changes in β.

Optimization
method

COV

EAD
(EN rule)

Best case

Average case

Worst case

β

0
10−5
10−4
10−3
10−2
0
10−5
10−4
10−3
10−2

ASR L1

L2

L∞

ASR L1

L∞

L∞

ASR L1

L2

100
100
100
100
100
100
100
100
100
100

7.167
7.165
7.148
6.961
5.963
6.643
5.967
4.638
4.014
3.688

0.209
0.209
0.209
0.211
0.222
0.201
0.201
0.215
0.261
0.357

0.022
0.022
0.022
0.023
0.029
0.023
0.025
0.032
0.047
0.085

100
100
99.9
100
100
99.9
100
99.9
100
100

13.62
13.43
13.64
13.35
11.51
13.39
12.24
10.4
8.18
8.106

0.392
0.386
0.394
0.396
0.408
0.392
0.391
0.414
0.502
0.741

0.044
0.044
0.044
0.045
0.055
0.045
0.047
0.058
0.097
0.209

99.9
100
99.9
100
100
99.9
99.7
99.9
100
100

19.17
19.16
19.14
18.7
16.31
18.72
17.24
14.86
12.11
12.59

0.546
0.546
0.547
0.547
0.556
0.541
0.541
0.56
0.69
1.053

L∞

0.064
0.065
0.064
0.066
0.077
0.064
0.068
0.081
0.147
0.351

Figure 5: Comparison of EN and L1 decision rules in EAD on CIFAR10 with varying L1 regularization parameter β (average
case). Comparing to the EN rule, for the same β the L1 rule attains less L1 distortion but may incur more L2 and L∞ distortions.

can render the DNN more difﬁcult to attack in terms of in-
creased distortion metrics when compared with the null case.
Notably, in the average case, joint adversarial training using
L1 and L2 examples lead to increased L1 and L2 distortion
against the C&W attack and EAD (EN), and increased L2
distortion against EAD (L1). The results suggest that EAD
can complement adversarial training toward resilient DNNs.
We would like to point out that in our experiments, adversar-
ial training maintains comparable test accuracy. All the ad-
versarially trained DNNs in Table 14 can still attain at least
99% test accuracy on MNIST.

Complete Attack Transferability Results on
MNIST (Table 13 )

Table 13 summarizes the transfer attack results from an un-
defended DNN to a defensively distilled DNN on MNIST
using EAD, the C&W attack and I-FGM. I-FGM methods
have poor performance in attack transferability. The aver-
age attack success rate (ASR) of I-FGM is below 2%. On
the other hand, adjusting the transferability parameter κ in
EAD and the C&W attack can signiﬁcantly improve ASR.
Tested on a wide range of κ values, the top average-case
ASR for EAD is 98.6% using the EN rule and 98.1% using
the L1 rule. The top average-case ASR for the C&W attack
is 87.4%. This improvement is signiﬁcantly due to the im-
provement in the worst case, where the top worst-case ASR
for EAD is 87% using the EN rule and 85.8% using the L1
rule, while the top worst-case ASR for the C&W attack is
30.5%. The results suggest that L1-based adversarial exam-
ples have better attack transferability.

Complete Results on Adversarial Training with L1
and L2 examples (Table 14)
Table 14 displays the complete results of adversarial training
on MNIST using the L2-based adversarial examples crafted
by the C&W attack and the L1-based adversarial examples
crafted by EAD with the EN or the L1 decision rule. It can
be observed that adversarial training with any single method

Table 6: Comparison of the elastic net (EN) and L1 decision rules in EAD for selecting adversarial examples on MNIST. ASR
means attack success rate (%).

Decision
rule

β

EAD
(EN rule)

EAD
(L1 rule)

10−5
5 · 10−5
10−4
5 · 10−4
10−3
5 · 10−3
10−2
5 · 10−2
10−5
5 · 10−4
10−4
5 · 10−4
10−3
5 · 10−3
10−2
5 · 10−2

Best case

Average case

Worst case

ASR L1

L2

L∞

ASR L1

L2

L∞

ASR L1

L2

13.66
13.19
12.79
10.95
9.808
7.912
7.271
7.088
13.26
11.81
10.75
8.025
7.153
6.347
6.193
5.918

1.369
1.37
1.372
1.395
1.427
1.6
1.718
1.872
1.376
1.385
1.403
1.534
1.639
1.844
1.906
2.101

0.378
0.383
0.388
0.42
0.452
0.591
0.674
0.736
0.386
0.404
0.424
0.527
0.593
0.772
0.861
0.956

100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

22.6
21.25
20.98
19.11
17.4
14.49
13.56
13.38
21.75
20.28
18.69
15.42
14.11
11.99
11.69
11.59

1.98
1.947
1.951
1.986
2.001
2.261
2.395
2.712
1.965
1.98
1.983
2.133
2.211
2.491
2.68
2.873

0.515
0.518
0.521
0.558
0.594
0.772
0.852
0.919
0.525
0.54
0.565
0.694
0.768
0.927
0.97
0.993

99.9
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

30.79
29.82
29.21
27.04
25.52
21.64
20.77
20.19
30.48
28.8
27.49
23.62
22.05
18.01
17.29
18.67

2.507
2.512
2.514
2.544
2.582
2.835
3.021
3.471
2.521
2.527
2.539
2.646
2.747
3.218
3.381
3.603

Table 7: Comparison of the elastic net (EN) and L1 decision rules in EAD for selecting adversarial examples on CIFAR10.
ASR means attack success rate (%).

Decision
rule

β

EAD
(EN rule)

EAD
(L1 rule)

10−5
5 · 10−5
10−4
5 · 10−4
10−3
5 · 10−3
10−2
5 · 10−2
10−5
5 · 10−5
10−4
5 · 10−4
10−3
5 · 10−3
10−2
5 · 10−2

Best case

Average case

Worst case

ASR L1

L2

L∞

ASR L1

L2

L∞

ASR L1

L2

5.967
5.09
4.638
4.35
4.014
3.83
3.688
2.855
5.002
3.884
3.361
2.689
2.6
2.216
2.201
2.306

0.201
0.207
0.215
0.241
0.261
0.319
0.357
0.52
0.209
0.231
0.255
0.339
0.358
0.521
0.568
0.674

0.025
0.028
0.032
0.039
0.047
0.067
0.085
0.201
0.029
0.04
0.05
0.091
0.103
0.22
0.256
0.348

100
99.9
99.9
100
100
100
100
100
100
100
100
100
100
100
100
100

12.24
10.94
10.4
8.671
8.18
8.462
8.106
7.735
11.03
8.516
7.7
6.414
6.127
5.15
5.282
6.566

0.391
0.396
0.414
0.459
0.502
0.619
0.741
1.075
0.4
0.431
0.472
0.552
0.617
0.894
0.958
1.234

0.047
0.052
0.058
0.079
0.097
0.145
0.209
0.426
0.052
0.071
0.089
0.135
0.171
0.372
0.417
0.573

99.7
99.9
99.9
100
100
100
100
100
99.8
100
100
100
100
100
100
100

17.24
15.87
14.86
12.39
12.11
13.03
12.59
14.66
16.03
12.91
11.7
10.11
8.99
7.983
8.437
12.81

0.541
0.549
0.56
0.63
0.69
0.904
1.053
1.657
0.548
0.585
0.619
0.732
0.874
1.195
1.274
1.804

L∞

0.648
0.659
0.667
0.706
0.748
0.921
0.976
0.998
0.666
0.686
0.71
0.857
0.934
0.997
1
1

L∞

0.068
0.075
0.081
0.117
0.147
0.255
0.351
0.661
0.074
0.099
0.121
0.192
0.272
0.542
0.593
0.779

100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

Table 8: Comparison of different adversarial attacks on MNIST. ASR means attack success rate (%). N.A. means “not available”
due to zero ASR.

Attack
C&W (L2)
FGM-L1
FGM-L2
FGM-L∞
I-FGM-L1
I-FGM-L2
I-FGM-L∞
EAD (EN rule)
EAD (L1 rule)

ASR L1
100
99.9
99.4
99.9
100
100
100
100
100

13.93
26.56
25.84
83.82
18.34
17.6
49.8
9.808
7.153

Best case
L2
1.377
2.29
2.245
4.002
1.605
1.543
2.45
1.427
1.639

L∞
0.379
0.577
0.574
0.193
0.403
0.387
0.147
0.452
0.593

Average case

Worst case

ASR L1
100
39
34.6
42.5
100
100
100
100
100

22.46
53.5
39.15
127.2
32.94
30.32
71.39
17.4
14.11

L2
1.972
4.186
3.284
6.09
2.606
2.41
3.472
2.001
2.211

L∞
0.514
0.782
0.747
0.296
0.591
0.561
0.227
0.594
0.768

ASR L1
99.9
0
0
0
100
99.8
99.9
100
100

32.3
N.A.
N.A.
N.A.
53.02
47.8
95.48
25.52
22.05

L2
2.639
N.A.
N.A.
N.A.
3.979
3.597
4.604
2.582
2.747

L∞
0.663
N.A.
N.A.
N.A.
0.807
0.771
0.348
0.748
0.934

Table 9: Comparison of different adversarial attacks on CIFAR10. ASR means attack success rate (%).

Attack
C&W (L2)
FGM-L1
FGM-L2
FGM-L∞
I-FGM-L1
I-FGM-L2
I-FGM-L∞
EAD (EN rule)
EAD (L1 rule)

ASR L1
100
99.5
99.5
100
100
100
100
100
100

7.167
14.76
14.13
32.74
7.906
7.587
17.92
4.014
2.597

Best case
L2
0.209
0.434
0.421
0.595
0.232
0.223
0.35
0.261
0.359

Average case

Worst case

L∞
0.022
0.049
0.05
0.011
0.026
0.025
0.008
0.047
0.103

ASR L1
100
48.8
42.8
52.3
100
100
100
100
100

13.62
51.97
39.5
127.81
17.53
17.12
33.3
8.18
6.066

L2
0.392
1.48
1.157
2.373
0.502
0.489
0.68
0.502
0.613

L∞
0.044
0.152
0.136
0.047
0.055
0.054
0.018
0.097
0.17

ASR L1
99.9
0.7
0.7
0.6
100
100
100
100
100

19.17
157.5
107.1
246.4
29.73
28.94
48.3
12.11
8.986

L2
0.546
4.345
3.115
4.554
0.847
0.823
1.025
0.69
0.871

L∞
0.064
0.415
0.369
0.086
0.095
0.092
0.032
0.147
0.27

Table 10: Comparison of different adversarial attacks on ImageNet. ASR means attack success rate (%). N.A. means “not
available” due to zero ASR.

Method
C&W (L2)
FGM-L1
FGM-L2
FGM-L∞
I-FGM-L1
I-FGM-L2
I-FGM-L∞
EAD (EN rule)
EAD (L1 rule)

Best case
L2
0.511
0.661
2.29
45.76
0.89
0.95
1.163
1.007
1.167

157.3
193.3
752.9
21640
292.2
315.4
504.9
29.56
22.11

ASR L1
100
9
12
19
98
100
100
100
100

Average case

Worst case

L∞
0.018
0.025
0.087
0.115
0.03
0.03
0.004
0.128
0.195

ASR L1
100
1
1
3
77
100
100
100
100

232.2
61
2338
3655
526.4
774.1
864.2
69.47
40.9

L2
0.705
0.187
6.823
7.102
1.609
2.358
2.079
1.563
1.598

L∞
0.03
0.007
0.25
0.014
0.054
0.086
0.01
0.238
0.293

ASR L1
100
0
0
0
34
96
100
100
100

330.3
N.A.
N.A.
N.A.
695.5
1326
1408
160.3
100

L2
0.969
N.A.
N.A.
N.A.
2.104
4.064
3.465
2.3
2.391

L∞
0.044
N.A.
N.A.
N.A.
0.078
0.153
0.019
0.351
0.423

(a) EAD (EN rule)

(b) EAD (L1 rule)

(c) I-FGM-L1

(d) I-FGM-L2

(e) I-FGM-L∞

(f) C&W

Figure 6: Visual illustration of adversarial examples crafted by different attack methods on MNIST. For each method, the images
displayed on the diagonal are the original examples. In each row, the off-diagonal images are the corresponding adversarial
examples with columns indexing target labels (from left to right: digits 0 to 9).

(a) EAD (EN rule)

(b) EAD (L1 rule)

(c) I-FGM-L1

(d) I-FGM-L2

(e) I-FGM-L∞

(f) C&W

Figure 7: Visual illustration of adversarial examples crafted by different attack methods on CIFAR10. For each method, the im-
ages displayed on the diagonal are the original examples. In each row, the off-diagonal images are the corresponding adversarial
examples with columns indexing target labels.

Figure 8: Visual illustration of adversarial examples crafted by different attack methods on ImageNet. The original example is
the ostrich image (Figure 1 (a)). Each column represents a targeted class to attack, and each row represents an attack method.
.

Table 11: Comparison of the C&W method and EAD on attacking defensive distillation with different temperature parameter
T on MNIST. ASR means attack success rate (%).

Method

C&W
(L2)

EAD
(EN rule)

EAD
(L1 rule)

Best case
L2
1.31
1.514
1.407
1.409
1.425
1.449
1.506
1.297
1.315
1.493
1.525
1.363
1.566
1.461
1.463
1.481
1.503
1.559
1.353
1.369
1.546
1.575
1.571
1.806
1.674
1.613
1.62
1.645
1.723
1.524
1.554
1.755
1.801

14.02
16.32
16.08
16.23
16.16
16.48
16.94
15.39
15.86
16.91
16.99
9.672
11.79
11.54
11.82
11.58
12.11
12.56
11.1
11.6
12.57
12.72
6.91
8.472
8.305
8.978
8.9
9.319
9.628
8.419
8.698
9.219
9.243

ASR L1
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

T
1
10
20
30
40
50
60
70
80
90
100
1
10
20
30
40
50
60
70
80
90
100
1
10
20
30
40
50
60
70
80
90
100

L∞
0.347
0.393
0.336
0.332
0.355
0.34
0.36
0.297
0.291
0.357
0.365
0.416
0.468
0.404
0.398
0.426
0.408
0.431
0.356
0.347
0.424
0.433
0.562
0.628
0.556
0.51
0.536
0.51
0.546
0.466
0.462
0.557
0.579

Average case

Worst case

ASR L1
100
100
99.9
99.9
100
100
100
99.9
100
99.9
99.9
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

23.27
25.79
26.31
26.09
27.03
26.01
27.44
25.28
26.89
27.74
27.95
17.75
20.5
20.55
20.98
21.6
21.09
21.71
19.92
21.08
22.62
22.74
14
16.16
16.16
16.48
16.74
17.01
17.7
16.01
17.17
18.46
18.44

L2
1.938
2.183
2.111
2.083
2.164
2.111
2.247
1.961
2.062
2.256
2.3
1.954
2.224
2.133
2.134
2.226
2.161
2.256
2.001
2.076
2.312
2.335
2.149
2.428
2.391
2.417
2.475
2.401
2.477
2.193
2.283
2.529
2.535

L∞
0.507
0.539
0.489
0.468
0.501
0.486
0.512
0.453
0.46
0.508
0.518
0.58
0.617
0.566
0.546
0.583
0.56
0.588
0.518
0.519
0.587
0.596
0.758
0.798
0.764
0.749
0.778
0.744
0.752
0.674
0.677
0.744
0.759

ASR L1
99.8
99.9
99.6
99.7
100
99.9
99.7
99.8
99.9
99.8
99.8
100
100
100
100
100
99.9
100
99.9
99.7
99.9
100
100
99.9
100
100
99.9
100
100
100
100
99.9
99.9

32.96
36.18
37.59
38.05
39.06
36.74
38.74
36.58
38.83
39.77
40.11
26.42
29.59
30.97
31.92
32.48
30.52
32.47
29.99
31.91
33.38
33.82
22.41
25.38
25.84
25.72
25.66
24.59
26.38
24.31
25.97
27.9
28.44

L2
2.576
2.882
2.85
2.858
2.955
2.826
2.998
2.694
2.857
3.059
3.11
2.532
2.866
2.809
2.847
2.936
2.806
2.982
2.655
2.805
3.047
3.1
2.721
3.061
3.031
3.124
3.241
3.103
3.287
2.935
3.083
3.315
3.351

L∞
0.683
0.684
0.65
0.629
0.667
0.651
0.668
0.626
0.651
0.66
0.67
0.762
0.768
0.719
0.701
0.739
0.731
0.752
0.691
0.712
0.744
0.754
0.935
0.945
0.93
0.932
0.947
0.939
0.944
0.892
0.904
0.926
0.929

Table 12: Comparison of the C&W method and EAD on attacking defensive distillation with different temperature parameter
T on CIFAR10. ASR means attack success rate (%).

Method

C&W
(L2)

EAD
(EN rule)

EAD
(L1 rule)

Best case
L2
0.188
0.219
0.256
0.254
0.251
0.26
0.262
0.269
0.266
0.278
0.292
0.236
0.286
0.321
0.313
0.309
0.319
0.319
0.33
0.327
0.34
0.358
0.328
0.37
0.435
0.452
0.465
0.479
0.497
0.506
0.5
0.519
0.543

6.414
7.431
8.712
8.688
8.556
8.88
8.935
9.166
9.026
9.466
9.943
3.594
4.072
5.24
5.54
5.623
5.806
6.07
6.026
5.958
6.261
6.499
2.302
2.884
3.445
3.372
3.234
3.402
3.319
3.438
3.418
3.603
3.702

ASR L1
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

T
1
10
20
30
40
50
60
70
80
90
100
1
10
20
30
40
50
60
70
80
90
100
1
10
20
30
40
50
60
70
80
90
100

L∞
0.02
0.024
0.028
0.028
0.028
0.028
0.029
0.03
0.029
0.031
0.032
0.044
0.052
0.056
0.051
0.05
0.05
0.051
0.052
0.052
0.054
0.057
0.098
0.101
0.118
0.128
0.136
0.142
0.151
0.153
0.15
0.157
0.161

Average case

Worst case

ASR L1
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

12.46
15.48
18.7
19.34
18.43
19.56
19.43
19.99
19.93
21.21
21.46
7.471
9.669
11.79
12.33
12.02
13
13.06
13.85
13.74
14.07
15.02
5.595
6.857
8.943
8.802
8.537
8.965
8.647
9.344
9.202
9.654
9.839

L2
0.358
0.445
0.534
0.554
0.528
0.559
0.554
0.571
0.571
0.606
0.614
0.462
0.567
0.662
0.658
0.626
0.671
0.662
0.695
0.697
0.711
0.756
0.556
0.718
0.802
0.796
0.809
0.848
0.863
0.913
0.914
0.96
0.993

L∞
0.04
0.049
0.058
0.06
0.058
0.06
0.06
0.061
0.061
0.065
0.066
0.09
0.104
0.118
0.112
0.105
0.109
0.109
0.111
0.111
0.117
0.123
0.154
0.2
0.201
0.202
0.209
0.22
0.232
0.239
0.242
0.258
0.269

ASR L1
100
99.9
100
99.8
100
99.9
100
100
99.5
100
99.9
99.9
100
100
100
100
100
100
100
99.9
99.9
100
99.9
100
99.9
100
100
100
99.8
100
100
99.8
99.8

17.47
22.36
27.4
27.67
26.89
29.07
28.56
29.6
29.64
31.23
32.54
10.88
14.97
17.72
18.12
18.14
19.46
19.5
20.06
20.28
21.27
22.12
8.41
10.44
13.73
14.31
13.82
14.96
14.46
15.06
15.14
15.82
16.22

L2
0.498
0.635
0.776
0.785
0.761
0.822
0.809
0.838
0.839
0.884
0.921
0.638
0.782
0.932
0.94
0.915
0.979
0.964
0.997
0.999
1.046
1.084
0.776
1.037
1.164
1.124
1.093
1.17
1.174
1.214
1.227
1.287
1.351

L∞
0.058
0.073
0.087
0.089
0.086
0.091
0.091
0.093
0.092
0.099
0.103
0.136
0.154
0.175
0.175
0.167
0.176
0.173
0.177
0.174
0.189
0.192
0.237
0.321
0.322
0.304
0.297
0.317
0.325
0.331
0.337
0.361
0.379

Table 13: Comparison of attack transferability from the undefended network to the defensively distilled network (T = 100) on
MNIST with varying transferability parameter κ. ASR means attack success rate (%). N.A. means not “not available” due to
zero ASR. There is no κ parameter for I-FGM.

κ
Method
I-FGM-L1
None
I-FGM-L2
None
I-FGM-L∞ None

Best case
L2
1.604
1.537
2.311
1.103
1.491
2.033
2.491
2.665
2.719
2.829
3.012
3.255
3.553
3.892
4.284
4.703
1.197
1.547
1.916
2.263
2.519
2.694
2.838
3.009
3.248
3.526
3.843
4.193
4.562
1.431
1.807
2.154
2.481
2.718
2.897
3.023
3.186
3.406
3.665
3.957
4.293
4.66

18.39
17.77
46.38
11.13
15.58
21.94
27.65
29.71
30.12
31.17
33.27
36.13
39.86
44.2
49.37
54.97
8.373
11.45
15.36
19.18
21.98
23.92
25.52
27.42
30.23
33.61
37.59
42.01
46.7
6.392
8.914
12.16
15.39
17.73
19.71
21.1
23
25.86
29.4
33.71
38.09
42.7

ASR L1
12.2
9.8
14.7
5.4
16.6
42.2
74.2
92.9
98.7
99.8
100
100
100
100
100
100
6
18.2
39.5
69.2
89.5
98.3
99.9
100
100
100
100
100
100
6
19
40.6
70.5
90
98.6
99.8
100
100
100
100
100
100

L∞
0.418
0.395
0.145
0.338
0.424
0.525
0.603
0.639
0.664
0.69
0.727
0.772
0.818
0.868
0.907
0.937
0.426
0.515
0.59
0.651
0.692
0.724
0.748
0.778
0.814
0.857
0.899
0.934
0.961
0.628
0.728
0.773
0.809
0.83
0.851
0.862
0.882
0.904
0.931
0.95
0.971
0.985

Average case

ASR L1
19
1.6
17.25
1.3
48.3
1.7
10.16
1.1
17.35
3.4
21.97
6.5
32.54
22.6
38.34
44.4
45.41
62.9
49.63
78.1
55.56
84.2
61.25
87.4
67.82
85.2
70.87
80.6
76.77
73
82.07
67.9
4.876
0.6
13.07
2.5
16.45
8.4
22.74
19.2
28.36
37
34.14
58
40.2
76.3
45.62
87.9
52.33
95.2
57.75
98
66.22
98.6
70.66
94.4
75.59
90
6.57
0.5
9.717
3.2
13.74
7.5
18.12
19
24.15
39.4
30.33
59.3
37.38
76.9
41.13
89.3
47.54
96.3
55.16
97.6
62.01
98.1
65.79
93.6
72.49
89.6

L2
1.658
1.533
2.44
1.033
1.615
2.001
2.869
3.322
3.837
4.15
4.583
4.98
5.43
5.639
6.034
6.395
0.813
1.691
1.989
2.531
2.99
3.445
3.909
4.324
4.805
5.194
5.758
6.09
6.419
1.565
1.884
2.27
2.689
3.182
3.652
4.191
4.468
4.913
5.399
5.856
6.112
6.572

L∞
0.43
0.408
0.158
0.343
0.46
0.527
0.671
0.745
0.805
0.847
0.886
0.918
0.936
0.953
0.969
0.976
0.307
0.549
0.6
0.697
0.778
0.831
0.884
0.92
0.945
0.965
0.978
0.986
0.992
0.678
0.738
0.8
0.865
0.902
0.933
0.954
0.968
0.979
0.988
0.992
0.995
0.997

Worst case
L2
N.A.
N.A.
N.A.
N.A.
N.A.
N.A.
4.628
4.708
5.946
6.923
8.072
9.09
10.21
10.8
N.A.
N.A.
N.A.
N.A.
N.A.
3.238
3.951
4.65
5.404
6.176
6.981
7.904
8.851
9.487
10.3
N.A.
N.A.
N.A.
3.024
4.173
4.818
5.529
6.256
7.064
7.94
8.845
9.519
10.36

N.A.
N.A.
N.A.
N.A.
N.A.
N.A.
56.93
54.25
71.22
85.93
105.9
125.2
146.9
158.4
N.A.
N.A.
N.A.
N.A.
N.A.
31.18
39.91
49.12
59.9
70.93
83.19
98.51
115.7
127
140.35
N.A.
N.A.
N.A.
23.15
38.22
45.74
55.54
66.76
80.05
96.05
113.6
126.4
141.3

ASR L1
0
0
0
0
0
0
0.4
2.4
10.9
23
30.5
21
7.4
0.5
0
0
0
0
0
0.4
1.8
7.9
23.7
47.4
71.3
86.2
87
44.2
13.3
0
0
0
0.3
1.9
7.9
22.2
46.8
69.9
85.8
85.7
43.8
13

L∞
N.A.
N.A.
N.A.
N.A.
N.A.
N.A.
0.843
0.91
0.972
0.987
0.993
0.995
0.996
0.996
N.A.
N.A.
N.A.
N.A.
N.A.
0.846
0.897
0.973
0.993
0.999
1
1
1
1
1
N.A.
N.A.
N.A.
0.884
0.979
0.997
1
1
1
1
1
1
1

0
5
10
15
20
25
30
35
40
45
50
55
60
0
5
10
15
20
25
30
35
40
45
50
55
60
0
5
10
15
20
25
30
35
40
45
50
55
60

C&W
(L2)

EAD
(EN rule)

EAD
(L1 rule)

Table 14: Comparison of adversarial training using the C&W attack, EAD (L1 rule) and EAD (EN rule) on MNIST. ASR means
attack success rate.

Attack
method

C&W
(L2)

EAD
(L1)

EAD
(EN)

Adversarial
training
100
None
EAD (L1)
100
C&W (L2)
100
EAD + C&W 100
100
None
EAD (L1)
100
C&W (L2)
100
EAD + C&W 100
100
None
100
EAD (EN)
C&W (L2)
100
EAD + C&W 100

Best case

Average case

Worst case

ASR L1

L2

L∞

ASR L1

L2

L∞

ASR L1

L2

13.93
15.98
14.59
16.54
7.153
8.795
7.657
8.936
9.808
11.13
10.38
11.14

1.377
1.704
1.693
1.73
1.639
1.946
1.912
1.975
1.427
1.778
1.724
1.76

0.379
0.492
0.543
0.502
0.593
0.705
0.743
0.711
0.452
0.606
0.611
0.602

100
100
100
100
100
100
100
100
100
100
100
100

22.46
26.11
24.97
27.32
14.11
17.04
15.49
16.83
17.4
19.52
18.99
20.09

1.972
2.468
2.47
2.513
2.211
2.653
2.628
2.66
2.001
2.476
2.453
2.5

0.514
0.643
0.684
0.653
0.768
0.86
0.892
0.87
0.594
0.751
0.759
0.75

99.9
99.7
99.9
99.8
100
99.9
100
99.9
100
99.9
100
100

32.3
36.37
36.4
37.83
22.05
24.94
24.16
25.55
25.52
28.15
27.77
28.91

2.639
3.229
3.28
3.229
2.747
3.266
3.3
3.288
2.582
3.182
3.153
3.193

L∞

0.663
0.794
0.807
0.795
0.934
0.98
0.986
0.979
0.748
0.892
0.891
0.882

