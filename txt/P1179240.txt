Incorporating Discriminator in Sentence Generation: a Gibbs Sampling Method

Jinyue Su, Jiacheng Xu, Xipeng Qiu∗, Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China
{jysu15,jcxu13,xpqiu,xjhuang}@fudan.edu.cn

8
1
0
2
 
b
e
F
 
5
2
 
 
]
L
C
.
s
c
[
 
 
1
v
0
7
9
8
0
.
2
0
8
1
:
v
i
X
r
a

Abstract

Generating plausible and ﬂuent sentence with desired prop-
erties has long been a challenge. Most of the recent works
use recurrent neural networks (RNNs) and their variants to
predict following words given previous sequence and target
label. In this paper, we propose a novel framework to gener-
ate constrained sentences via Gibbs Sampling. The candidate
sentences are revised and updated iteratively, with sampled
new words replacing old ones. Our experiments show the ef-
fectiveness of the proposed method to generate plausible and
diverse sentences.

Introduction
Generating meaningful, coherent and plausible text is an in-
creasingly important topic in NLP (Gatt and Krahmer 2017),
forming the closed loop of interaction between human and
machine. Recent progresses in language modeling, machine
translation (Cho et al. 2014), text summarization (Chopra,
Auli, and Rush 2016), dialogue systems (Li et al. 2016),
etc., have shown the effectiveness of deep learning methods
to understand and generate natural language. Among them,
Recurrent Neural Networks (RNNs) and their variants are
widely used.

However, generating natural language remains a challeng-
ing task. First, underlying semantic structures in language
are complicated and it’s hard for sequence models to con-
struct text with rich structure during generation. Second, in
many cases, we expect the generated text could satisfy cer-
tain properties, such as the polarity of sentiment or the style
of writing. Third, annotated corpora for constrained text are
limited in terms of scale and diversity. In this paper, we
would like to address the challenge of generating short sen-
tences with explicit constraints or desired properties, other
than speciﬁc tasks like translation or summarization.

Most of the previous works on this task rely on a condi-
tional RNN-based language model as the ﬁnal step to gen-
erate words. The explicit constraint information is encoded
into a semantic vector, and a conditional language model de-
codes a sentence from the semantic vector one word per time
step.

On the contrary, we incorporate a discriminator in our
method. With the inspiration of revising sentence iteratively,
we propose a novel sentence generation framework based on
Gibbs Sampling (GS). Our method mainly contains two sep-
arate models: a pure Language Model, i.e. doesn’t aware of
the constraint information, and a Discriminator estimating
the probability of a given sentence satisfying the constraints.
Then by applying Gibbs Sampling, we can replace current
words with new ones iteratively, and the whole sentence is
becoming more and more natural. Since the discriminator
is guiding this process all the time, the sampled sentences
tend to satisfy all the constraints. There is also a Candidate
Generator proposing several most likely words at each it-
eration, thus leave fewer sentences for language model and
discriminator to inspect, which results in a speed-up.

Our method holds following advantages compared to con-

ventional methods:

• Our method directly accesses

to the probability
p(c|w1...n),
i.e. how likely the sentence satisﬁes all
constraints. By checking this probability, our method en-
sures the signiﬁcance of every constraint, thus alleviates
the problem that some safe but meaningless sentences
may be generated. See our experiment results for more
details.

• Since the language model and the discriminator can be
trained separately, we can train the discriminator on la-
beled data, and train the language model both on labeled
and unlabeled data, which enables the framework to work
better in a semi-supervised fashion.

Also, as a side-product, from the nature of Gibbs Sampling,
our method is able to control the length of the generated sen-
tence. And since the two major components are responsible
for ﬂuency and correctness respectively, we can easily ﬁgure
out which part is failing, and then improve it.

To the best of our knowledge, this is the ﬁrst time Gibbs

Sampling is used in generating constrained sentence.

Background

∗Corresponding Author

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

In this section, we will introduce two folds of background
knowledge: gated recurrent neural networks and RNN lan-
guage model.

Gated Recurrent Neural Network
Gated Recurrent Neural Network(GRU) is a variant of re-
current neural network(RNN), which efﬁciently implements
a dynamic gating mechanism to control the ﬂow of infor-
mation, aiming to simplify the propagation path of gradients
and model the context with memory units.

A GRU unit takes an input vector xt and the previous ac-
tivation state ht−1 at any time step, and yields an activation
state ht for current time step. ht is a linear interpolation be-
tween previous state ht−1 and the candidate state ˜ht, where
the ratio is controlled by a balancing vector zt called update
gate. The model is formulated as follow:

ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht
zt = σ(Wzxt + Uzht−1)
˜ht = tanh(W xt + U (rt (cid:12) ht−1))
rt = σ(Wtxt + Urht−1)

(1)
(2)

(3)
(4)

where (cid:12) stands for element-wise multiplication and rt is the
reset gate. The matrices W , U , Wr, Wz, Ur, Uz are trainable
parameters.

Recurrent Language Model
Language models reveal the degree of how much a se-
quence of words is likely to be a realistic sequence of hu-
man language. Formally, let Σ be the vocabulary set, and
w1, w2, ..., wn is a sentence where wi ∈ Σ is a word in the
vocabulary. A language model measures the joint probabil-
ity by decomposing the words one by one.

p(w1...wn) =

p(wi|w<i),

(5)

n
(cid:89)

i=1

where w<i = (w1, · · · , wi−1).

The prevailing language models are usually based on
RNNs (Mikolov et al. 2010; Mikolov and Zweig 2012).
In our model, we adopt GRU (Chung et al. 2014) for
its efﬁciency and effectiveness. The conditional probability
p(wi|w<i) can be modeled by GRU,

Figure 1: This ﬁgure illustrates how our method works, com-
pared to the conventional method with RNN-based language
model. (a) is the diagram of conventional methods: roll
out the language model that is conditioned on the constraints
to generate a sentence from left to right. (b) is the diagram
of our Gibbs Sampling based method. From bottom to
top, “When this ﬁlm came out” is the initial sentence, and
“Worst movie ever made !” is the ﬁnal output sentence. Our
method replace one word with a new word at a time (words
in bolded rounded rectangle). The initial sentence becomes
“heroic this ﬁlm came out” after the ﬁrst modiﬁcation. In
this way, the initial sentence is gradually changed to the ﬁ-
nal sentence.

Most of the previous works of generating constrained text
are essentially based on a conditional language model. The
constraint information is imposed on the network by initial
hidden state or concatenated to the input vector at every time
step.

The conditional probability p(wi|w<i, c) is computed by

p(wi|w<i) = φ(xi, hi−1),

(6)

p(wi|w<i, c) = φ(xi ⊕ a, hi−1),

(9)

where xi is the word-vector of the i-th word, and hi is the
hidden vector at time step i. φ is deﬁned by a single GRU
cell.

Task Description
The task of generating explicitly constrained sentence aims
to generate sentence under the constraints of several spe-
ciﬁc attributes. More speciﬁcally, given k explicit con-
straints c = (c1, · · · , ck), our goal is to generate a sentence
w = w1, · · · , wn which maximize the conditional probabil-
ity p(w|c)

where a is the encoding vector of the constraints c, and ⊕
indicates concatenation operation.

Top part of ﬁgure 1 gives an illustration of the generating

process of conditional RNN.

Generating Constrained Sentence
via Gibbs Sampling
In this section, we introduce our proposed method to gener-
ate sentences with imposed constraints.

Gibbs Sampling

p(w|c) = p(w1, · · · , wi|c)

=

p(wi|w<i, c)

n
(cid:89)

i=1

(7)

(8)

Gibbs sampling is a Markov-Chain-Monte-Carlo (MCMC)
method, and is widely used in probability inference, such
as Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan
2001) etc.

Gibbs sampling aims to approximately sample a set
of random variables from a complicated joint distribu-
tion which is intractable to directly sample from. This
method starts with the set of variables x1...n randomly
initialized. The algorithm goes T turns. In each turn, it-
erate over all variables xi, and sample new xi
from
the full-conditional probability distribution P (xi|x−i) (cid:44)
P (xi|x1, x2...xi−1, xi+1, xi+2, ..., xn). After some turns,
the variables x1...n can be seen as sampled from an approx-
imated distribution of the real joint probability distribution.

Algorithm 1 Gibbs Sampling
Ensure: x1...n randomly initialized

for t = 0 to T do

for i = 1 to n do

sample xi from p(xi|x−i);

end for

end for
return x1...n

Our Method

In this section, we discuss more details of the Gibbs Sam-
pling method in constrained sentence generation. Our pro-
posed method is based on the MCMC method in Berglund et
al. (2015), and incorporates the discriminators of constraints
to generate constrained sentence.

When applied to the constrained sentence generation task,
the random variables x1...n become the words forming the
sentence, i.e. w1...n. 1 That is, the algorithm starts from a
seed sentence, and repeatedly replaces a word in the sen-
tence with another word sampled from the full-conditional
distribution. Figure 1 gives an illustration of this procedure.
More formally, let c1, c2, ...ck be the constrains, and c rep-

resents all the constraints.

In constrained sentence generation task, we want
to
sample sentences from distribution p(w1...n|c). Thus the
full-conditional distribution used in Gibbs Sampling is
p(wi|w−i, c) (cid:44) p(wi|w<i, w>i, c). Use Bayes theorem, we
get

p(wi|w−i, c)
= p(wi|w<i, w>i, c)
p(w1...n, c)
p(w<i, w>i, c)

=

∝ p(w1...n, c)
= p(w<i, w>i) · p(wi|w<i, w>i) · p(c|w1...n)
∝ p(wi|w<i, w>i) · p(c|w1...n).

Eq (15) can be decomposed into two parts dealt with lan-

guage model and discriminator respectively.

1Note that the number of variables, which is also the length of
generated sentence, is invariant during the Gibbs Sampling proce-
dure, which means that we must specify the length at the beginning.

(10)
(11)

(12)

(13)
(14)
(15)

Language Model The ﬁrst part is p(wi|w<i, w>i). And
there are two ways to deal with it.

In a direct fashion, we can simply train a Bidirectional
Recurrent Neural Network here to estimate the probability
p(wi|w<i, w>i). More concretely, a forward RNN takes the
preﬁx as the input and a backward RNN takes the sufﬁx. Let
the ﬁnal states be hf and hb respectively. Then the probabil-
ity is calculated in the following way

p(wi|w<i, w>i) = vT
wi
where the vector vwi is the one-hot vector of the word wi,
and W, b are trainable parameters.

(W[hf ; hb] + b),

(16)

Or, we can apply Bayes theorem again, and get

p(wi|w<i, w>i)
p(w1...n)
P (w<i, w>i)

=

∝ p(w1...n)

=

P (wt|w<t).

n
(cid:89)

t=1

(17)

(18)

(19)

(20)

Then the probability takes the form p(wt|w<t) which is

exactly the same with language model.

In our experiments we use the latter way to estimate the
probability p(wi|w<i, w>i), since the latter model can esti-
mate the probability of the sentence p(w1...n) as well, which
is needed to decide which sentence to output.

Discriminator The second part of Eq (15) is the condi-
tional probability of constraints c given w1...n. For example,
if the constraint is the sentiment of movie review, then this
term is the probability of the sentence w holding positive /
negative sentiment. We can train a classiﬁer to discriminate
the sentiment and estimate the probability.

In our experiments, we use GRU-based discriminators.
We build a forward RNN to take the sentence as input, and
a backward RNN to take the reverse of the sentence. Let hf
and hb be the ﬁnal states of the two RNNs respectively. Then
pass the two vector to a two-layer MLP, and get the proba-
bility of the sentence over all categories of the constraint.

When there are multiple constraints, we can make the in-
dependent assumption that any two constraints are indepen-
dent to each other, which is almost correct in many situa-
tions. Then we have p(c|w1...n) = (cid:81)k
i=1 p(ci|w1...n). Thus
we can train k discriminators independently on the k con-
straints.

We do the transduction above because the task to model
a complicated distribution has been decomposed to model
two much simpler distributions. We can make use of lan-
guage model to estimate the ﬁrst term on the right hand side
in equation(15), and discriminators to estimate the second
term.

By doing this, we can sample many sentences from
the distribution p(w1...n|c), and choose one as the output.
In our experiment, we choose the sentence where ∀i ∈
[1, k], p(ci|w1...n) > threshold with the largest p(w1...n).

Hyper parameters

Value

Hidden-units
Word-vec-size
Constraint-embedding-size
Turns in Gibbs Sampling
Fixed sentence length
Burn-in turns
Threshold
Candidate word number(k)
Sentences sampled in RS
Beam size

200
200
10
100
8
10
0.6
5
800
300

Table 1: Hyper-parameters setting

Model

SST-2

Product

Pure LM
Conditional LM
Candidate Generator

72.83
66.64
46.02

88.01
109.33
64.64

Table 2: The perplexity on test datasets.

The Candidate Generator

Still, there remains two major problems in the Gibbs Sam-
pling based method.

One is that the probability p(c|w1...n) sometimes is not
well-deﬁned on every single choice of w1...n, since there are
many meaningless “sentences” in the |V|n space, and it is
hard to say a meaningless sequence of words has positive or
negative sentiment.

The second problem is the time efﬁciency issue. In most
applications, the size of vocabulary |V| is around 10k. It’s
impossible to calculate the probability p(wi|w<i, w>i) ·
p(c|w1...n) for every possible value of wi, especially the sec-
ond term.

To alleviate these two problems, we propose the candidate
generator. Basically, the candidate generator takes the con-
straints and the preﬁx along with the sufﬁx as inputs, and
output k words as most probable candidates for the i-th po-
sition. By replacing the original i-th word with these words,
we get k candidate sentences. Then, the language model and
the discriminator will go through these sentences, calculate
corresponding probability, and sample one from them.

In this way, every sentence sent to the discriminator will
be more natural, and the number of probability calculation
will be reduced to k in every iteration of inner loop.

This method is still very time consuming after the speed-
up, though. The discriminator needs to look all these k
sentence-with-one-word-replaced over, and estimates the
probability.

In our experiments, for simplicity, the candidate gener-
ator calculates the probability distribution p(wi|w<i, w>i)
using a Bidirectional Recurrent Neural Network, and out-
puts k words with the highest probability.

Model

SST-2

Product

Sentiment discriminator
Domain discriminator

0.738
/

0.784
0.866

Table 3: Classiﬁcation accuracy of discriminators.

Experiments
In this section, we investigate the performance of the pro-
posed method on two review datasets: a movie review
dataset (SST-2) (Socher et al. 2013) and a product review
dataset (Blitzer et al. 2007).

The task is to generate constrained sentence. On movie
review, the sentiment is designated, and on product review,
both the domain and sentiment are constrained.

Dataset
SST-2 is a dataset which contains movie reviews with two
classes of sentiment label : “negative” or “positive”. SST-2
is from the Stanford Sentiment Treebank2.

The Product dataset3 contains reviews from Amazon in
four domains: Books, DVDs, Electronics and Kitchen ap-
pliances. Each review has a sentiment label of “positive” or
“negative”. We split documents in this dataset to sentences
for training language model. The sentiment discriminator
and domain discriminator is still trained on document level.

Baseline
We use the conventional Beam search (BS) method and a
Reject sampling (RS) method for generating constrained
sentence as baselines.

Beam Search is a widely used approximate algorithm to
ﬁnd the sentence with maximum likelihood estimated by the
language model. This algorithm keeps a set of generated pre-
ﬁxes. The size of the kept set is called beam size m. At each
time step, these preﬁxes would be extended by one word,
and the top m new preﬁxes would be kept for the next itera-
tion.

Reject Sampling method works as follows: randomly
sample sentences from language model with no constrain in-
formation for ˆT times, and output the sentence where ∀i ∈
[1, k], p(ci|w1...n) > threshold with the largest p(w1...n).
This method randomly pick words within top-10 words with
most probability at each time step, in order to improve the
performance. This method shares the same discriminator
and language model with the Gibbs Sampling method.

Settings
All Recurrent Neural Networks used in our experiments are
GRU with 200 hidden units. The sentence was fed to the net-
work by replacing words with embeddings. And the inputs
to the conditional language model are concatenated to extra
constraint-embedding vectors at each time step. Words oc-
curs fewer than 10 times in dataset are replaced with UNK.

2http://nlp.stanford.edu/sentiment
3https://www.cs.jhu.edu/˜mdredze/datasets/sentiment/

Sentiment Gibbs Sampling

Yes , the movie is too bad .

Beam Search
It ’s a bad movie .

Reject Sampling
It ’s a bad movie .

negative

One of the worst of the year .

It ’s a bad ﬁlm .

An awful ﬁlm .

The ﬁlm is a little too long .
It ’s not very funny .
This is a movie that is funny . A very funny movie .
It ’s a great kind of movie .

It ’s a very funny movie .

It ’s not a good movie .
A fascinating documentary .
It ’s a good movie .

positive

One of the year ’s best ﬁlms .

One of the year ’s best ﬁlms .

It ’s a great ﬁlm .

Hu et al. (2017)
the acting was also kind of
hit or miss .
the movie is very close to
the show in plot and charac-
ters
i wo n’t watch the movie
his acting was impeccable
this is one of
dance ﬁlms
i hope he ’ll make more
movies in the future

the better

Table 4: The generated sentences for movie review. We can observe that Beam Search method tends to output sentences that
are safe but less meaningful: the domain tag seems to be ignored.

Sentiment Gibbs Sampling

negative
positive

this book is a complete waste of time
this book is a great book to read

negative

positive

after watching this movie i was very dis-
appointed
this is the best movie i’ve ever seen

Beam Search
Books
i was disappointed
highly recommended
DVDs
i was disappointed

highly recommended

Electronics

Reject Sampling

this is the worst book ever
i love this book

i was disappointed

i love this movie

negative
positive

and i would not recommend it to anyone
if you are looking for a good camera

don’t waste your money
highly recommended

it was just ﬁne
it works great

negative
positive

but i will never buy this product again
so i bought this one for my husband

don’t waste your money
i love it

Kitchen Appliances

it does not work well
i would highly recommend
it to anyone

Table 5: The generated sentences for product reviews.

After the preprocessing, the vocabulary size of SST-2 is
about 3700 and Product about 7000.

The beam size of Beam Search is set to 300 in order to
make sure all the methods take about the same amount of
time to generate a sentence. The number of random sampled
sentences in the reject sampling method is set to 800 for the
same reason.

T is the turns executed in Gibbs Sampling, and n is the
ﬁxed sentence length. The seed sentence is modiﬁed from a
segment from training data consisting of 8 words: if n > 8,
use UNK for padding; if n < 8, only use the preﬁx. And the
length of burn-in period is set to 10 turns. Burn-in is a term
used in Gibbs Sampling context, and it means that we don’t
care the samples in ﬁrst several turns.

As a reminder, Pure LM is the language model used in
Gibbs Sampling and Reject Sampling, and Conditional LM
is the conditional language model used in Beam Search.

Table 1 lists the setting of hyper-parameters. Test error of
all components are shown in Table 2 and 3. It is weird that
the test perplexity of conditional language model on Product
dataset is higher than pure language model while on SST-
2 the thing is opposite. This may due to the fact that there
are more kinds of constraints on Product (2 × 4 where 2 for
sentiment and 4 for domain) than on SST-2 (2 for sentiment).

Results
Some illustrations of the generated sentences are shown in
Table 4 and 5. Since Hu et al. (2017) have done experiments
on SST dataset, we also present some of their results here.
We also change the ﬁxed length of sentence, and the gener-
ated results are shown in Table 6.

To make a quantitative evaluation, we use BLEU (Pap-
ineni et al. 2002) to estimate the quality of the generated
sentences.

The BLEU score is a method to automatically evaluate the
quality of sentence generation, which has been shown to be
highly correlated to human judgment when applied to ma-
chine translation. The BLEU score calculates the precision
of n-gram matching between generated sentences and ref-
erences. A brevity penalty term is also used to penalize too
short generated sentences.

We use the BLEU-4 (the geometric mean from 1 gram to 4
grams) in experiments. More concretely, the score of a single
generated sentence is the average BLEU score matching this
sentence to all sentences with the same label from the dataset
(including train, dev, and test split). 80 randomly generated
sentences are used to get the average score of a method. The
number of sentences in each category is equal.

The Beam Search method and Reject Sampling inherently
tend to generate short sentence. This is unequal in the com-

It ’s too bad .
The movie is funny .

I ca n’t recommend it .
It ’s a great movie .

The ﬁlm is just too bad .
It ’s a very good ﬁlm .

One of the worst of the year .
This is one of the best ﬁlms .

Yes , the ﬁlm is a bad movie .
It ’s a great piece of a movie .

This is a movie that is n’t very funny .
This is a good movie in its own way .

It ’s very funny , but it ’s a mess .
It ’s a good movie with a lot of humor .

The ﬁlm is one of the worst movies of the year .
It ’s not very funny , but it ’s worth seeing .

Table 6: The generated sentences with length from 5 to 12
for movie reviews.

method

w/ penalty w/o penalty

RANDOM 4.338
2.060
GS
1.290
BS
0.384
RS

5.888
8.302
6.059
4.029

Table 7: BLEU scores (×10−3) on SST-2 dataset.

parison, since the Gibbs Sampling will deﬁnitely output sen-
tences of length 8, which is longer than most of the sen-
tences generated by the two baselines. To take the length
out of consideration, we also calculate BLEU score without
brevity penalty.

Results are shown in Table 7 and 8. “RANDOM” stands
for the average BLEU score of 80 randomly chosen sen-
tences from the same dataset with the designated labels.

Discriminative Efﬁciency

We further check the “discriminative efﬁciency” of our
method. Here the “discriminative efﬁciency” means the ra-
tio of sentences satisfying the constraints among all sampled
sentences. In other words, the probability estimated by the
discriminator satisfy: ∀i ∈ [1, k], p(ci|w1...n) > threshold.
Since the output is the sentence with the most likelihood
among all valid sentences, the quality of output is highly
related to the ratio of valid sentence samples.

In Table 9, some statistical data of valid sentence ratio is
presented. The data is collected when the two methods gen-
erate the 80 sentences in the last subsection. There are about
57k(after burn-in) and 64k sentences sampled respectively.
RAND is the probability of a randomly sampled sentence
from dataset to have the exact label. As shown in the table,
the sampling process is skewed to the label that was desig-
nated.

method

w/ penalty w/o penalty

RANDOM 2.093
1.476
GS
0.218
BS
0.369
RS

2.854
4.750
4.556
3.323

Table 8: BLEU score (Product) ×10−3

ratio

SST-2

Product

RANDOM 0.5
GS
RS

0.846
0.478

0.125
0.682
0.114

Table 9: Valid Sentence Ratio

Figure 2 shows how the ratio goes during generation pro-
cedure. The curves presented here are sampled with the con-
straint [positive] on movie review and [electronics , positive]
on product review.

Log-likelihood per Word

We also checked the log-likelihood per word of sentences
sampled by the Gibbs Sampling method, and the Reject
Sampling method. As shown in Figure 3 and 4, we can see
that the sentences generated by Gibbs Sampling are consid-
ered to be more realistic by the pure language model, which
is the same in both methods.

The shown ﬁgures 3 and 4 are sampled with the constraint
[positive] on movie review and [electronics , negative] on
product review.

Failure Cases

In our experiments, we observe several kinds of failure.
Some of them are listed below.

• Broken sentence segments. For example, this book is not
written in this book seems to be an unnatural concatena-
tion of two sentence segments. This is due to the failure
of language model.

• Lack of context. For example, but i am very pleased with
this book is not a complete sentence. The word but indi-
cates a turning of sentiment, but there is no context. This
is also a kind of language model failure.

• Failure of discriminator. i would think this is a good prod-

uct is generated with label [Electronics, negative].

Analysis

Our experiments show that the proposed Gibbs Sampling
method achieves a signiﬁcantly better BLEU score with or
without brevity penalty, and can generate sentences with
controllable length.

It’s interesting that Gibbs Sampling achieves a higher
BLEU score than RANDOM when there is no brevity
penalty, which shows the diversity in the dataset. We also

Figure 2: Valid Sentence Ratio

Figure 4: Log-Likelihood per word on product review. Better
sentences lie on the right part of the x-axes.

straint in machine translation is the semantics of the source
language, while the constraint in image caption is the image
content.

Another line of related work focuses on generating sen-

tence from explicit attributes, such as product reviews.

Mou et al. (2015) use two directional RNNs to generate
sentence containing speciﬁc word. Dong et al. (2017) use
network architecture similar to that in NMT: an attribute en-
coder outputs the constraint embedding, and the sequence
decoder with attention to softly align the attributes and gen-
erated words. Hu et al. (2017) use the variational autoen-
coder (VAE) (Kingma and Welling 2013), a regularized ver-
sion of standard autoencoder, and employs the wake-sleep
algorithm to train the whole model in a semi-supervised
fashion. They are also using the dataset SST, though the ex-
periment settings are very different from ours(the constraints
in their works are tense and sentiment). Some of sentences
generated by their method are shown in 4 for comparison.
(Goyal et al. 2017) build a language model capable of gener-
ating emotionally colored conversational text. These meth-
ods feed the constraint information as extra features to the
networks, while our method use discriminators to get con-
trol of the sentence.

And recently, Guu et al. (2017) propose a method that
generates sentences by prototype-then-edit. The pipeline is
very similar to ours, though technically very different.

Conclusion and Future Works
In this paper, we propose a novel Gibbs-Sampling-based
method to generate constrained sentence. The proposed
method doesn’t directly model the distribution p(w1...n|c).
Instead, the method makes use of several separately trained
models, and apply Gibbs Sampling to generate sentence. Ex-
perimental results show that this method can yield diverse
and meaningful sentences.

In the future, we plan to further investigate the relation-
ship between the seed sentence segment and the ﬁnal out-
put sentence. We have observed that they may share some

Figure 3: Log-Likelihood per word on movie review. Better
sentences lie on the right part of the x-axes.

empirically show that Gibbs Sampling is more “discrimina-
tive efﬁcient” than conventional sample method, since the
valid sentence ratio is marginally higher.

And the log-likelihood per word of sampled sentences
are higher in Gibbs Sampling than conventional sampling.
It seems that the language model given contexts are more
precise, and candidate generator works just well.

Related Work

Recently, beneﬁting from the powerful neural
language
model, neural network based text generation achieved great
successes on many NLP tasks, including machine transla-
tion (Cho et al. 2014; Bahdanau, Cho, and Bengio 2014;
Luong, Pham, and Manning 2015), summarization (Chopra,
Auli, and Rush 2016; Miao and Blunsom 2016), and im-
age caption (Vinyals et al. 2015; Xu et al. 2015). These
tasks can be formalized as generating text with the condi-
tional language model. The condition is implicitly modeled
by distributed representation of several constraints. The con-
straints are different in different tasks. For example, the con-

[2017] Guu, K.; Hashimoto, T. B.; Oren, Y.; and Liang, P.
2017. Generating sentences by editing prototypes. arXiv
preprint arXiv:1709.08878.
[2017] Hu, Z.; Yang, Z.; Liang, X.; Salakhutdinov, R.; and
Xing, E. P. 2017. Controllable text generation. arXiv
preprint arXiv:1703.00955.
[2013] Kingma, D. P., and Welling, M. 2013. Auto-Encoding
Variational Bayes. CoRR.
[2016] Li, J.; Galley, M.; Brockett, C.; Gao, J.; and Dolan, B.
2016. A diversity-promoting objective function for neural
conversation models. In Proceedings of NAACL-HLT, 110–
119.
[2015] Luong, M.-T.; Pham, H.; and Manning, C. D. 2015.
Effective approaches to attention-based neural machine
translation. arXiv preprint arXiv:1508.04025.
[2016] Miao, Y., and Blunsom, P. 2016. Language as a latent
variable: Discrete generative models for sentence compres-
sion. In Proceedings of EMNLP 2016, 319–328. ACL.
[2012] Mikolov, T., and Zweig, G. 2012. Context dependent
In 2012 IEEE
recurrent neural network language model.
Spoken Language Technology Workshop (SLT 2012), 234–
239. IEEE.
[2010] Mikolov, T.; Karaﬁ´at, M.; Burget, L.; Cernock´y, J.;
and Khudanpur, S. 2010. Recurrent neural network based
language model. INTERSPEECH.
[2015] Mou, L.; Yan, R.; Li, G.; Zhang, L.; and Jin, Z. 2015.
Backward and forward language modeling for constrained
sentence generation. arXiv preprint arXiv:1512.06612.
[2002] Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J.
2002. Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL 2002, 311–318. ACL.
[2013] Socher, R.; Perelygin, A.; Wu, J. Y.; Chuang, J.; Man-
ning, C. D.; Ng, A. Y.; Potts, C.; et al. 2013. Recursive deep
models for semantic compositionality over a sentiment tree-
bank. In Proceedings of EMNLP 2013, volume 1631, 1642.
Citeseer.
[2015] Vinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D.
2015. Show and tell: A neural image caption generator. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 3156–3164.
[2015] Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A. C.;
Salakhutdinov, R.; Zemel, R. S.; and Bengio, Y. 2015. Show,
attend and tell: Neural image caption generation with visual
attention. In ICML, volume 14, 77–81.

sentence pattern, but the procedure of transferring the seed
sentence to the output sentence still remains mysterious. Be-
sides, we plan to apply the proposed framework to some
sequence-to-sequence task, chatbot for example. Lastly, we
are wondering whether it will be an improvement, to replace
a segment in the sentence at a time, in order to gain more ﬂu-
ent intermediate sentences. Also, this framework can work
in a semi-supervised fashion, which we haven’t thoroughly
examined. We also left this for future work.

Acknowledgement
We would like to thank the anonymous reviewers for their
valuable comments. The research work is supported by the
National Key Research and Development Program of China
(No. 2017YFB1002104), Shanghai Municipal Science and
Technology Commission (No. 17JC1404100), and National
Natural Science Foundation of China (No. 61672162).

References
[2014] Bahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural
machine translation by jointly learning to align and translate.
arXiv preprint arXiv:1409.0473.
[2015] Berglund, M.; Raiko, T.; Honkala, M.; K¨arkk¨ainen,
L.; Vetek, A.; and Karhunen, J. T. 2015. Bidirectional re-
current neural networks as generative models. In Advances
in Neural Information Processing Systems, 856–864.
[2001] Blei, D. M.; Ng, A. Y.; and Jordan, M. I. 2001. Latent
dirichlet allocation. Journal of Machine Learning Research
3:993–1022.
[2007] Blitzer, J.; Dredze, M.; Pereira, F.; et al. 2007. Bi-
ographies, bollywood, boom-boxes and blenders: Domain
adaptation for sentiment classiﬁcation. In ACL, volume 7,
440–447.
[2014] Cho, K.; van Merrienboer, B.; Gulcehre, C.; Bah-
danau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y.
2014. Learning Phrase Representations using RNN En-
coder–Decoder for Statistical Machine Translation. In Pro-
ceedings of EMNLP 2014, 1724–1734. Stroudsburg, PA,
USA: ACL.
[2016] Chopra, S.; Auli, M.; and Rush, A. M. 2016. Ab-
stractive Sentence Summarization with Attentive Recurrent
Neural Networks. In Proceedings of NAACL:HLT 2016, 93–
98. Stroudsburg, PA, USA: ACL.
[2014] Chung, J.; Gulcehre, C.; Cho, K.; and Bengio,
2014. Empirical evaluation of gated recurrent neu-
Y.
arXiv preprint
ral networks on sequence modeling.
arXiv:1412.3555.
[2017] Dong, L.; Huang, S.; Wei, F.; Lapata, M.; and Zhou,
M. 2017. Learning to generate product reviews from at-
tributes. In Proceedings of EACL 2017.
[2017] Gatt, A., and Krahmer, E. 2017. Survey of the State
of the Art in Natural Language Generation: Core tasks, ap-
plications and evaluation. arXiv.org.
[2017] Goyal, P.; Hu, Z.; Liang, X.; Wang, C.; and Xing, E. P.
2017. Nonparametric variational auto-encoders for hierar-
In The IEEE International
chical representation learning.
Conference on Computer Vision (ICCV).

