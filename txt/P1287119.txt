Gated-Attention Readers for Text Comprehension

Bhuwan Dhingra∗

Hanxiao Liu∗

Zhilin Yang

William W. Cohen

Ruslan Salakhutdinov

School of Computer Science
Carnegie Mellon University
{bdhingra,hanxiaol,zhiliny,wcohen,rsalakhu}@cs.cmu.edu

7
1
0
2
 
r
p
A
 
1
2
 
 
]
L
C
.
s
c
[
 
 
3
v
9
4
5
1
0
.
6
0
6
1
:
v
i
X
r
a

Abstract

In this paper we study the problem of an-
swering cloze-style questions over docu-
ments. Our model, the Gated-Attention
(GA) Reader1, integrates a multi-hop ar-
chitecture with a novel attention mecha-
nism, which is based on multiplicative in-
teractions between the query embedding
and the intermediate states of a recurrent
neural network document reader. This
enables the reader to build query-speciﬁc
representations of tokens in the document
for accurate answer selection. The GA
Reader obtains state-of-the-art results on
three benchmarks for this task–the CNN &
Daily Mail news stories and the Who Did
What dataset. The effectiveness of multi-
plicative interaction is demonstrated by an
ablation study, and by comparing to alter-
native compositional operators for imple-
menting the gated-attention.

1

Introduction

A recent trend to measure progress towards ma-
chine reading is to test a system’s ability to an-
swer questions about a document it has to com-
prehend. Towards this end, several large-scale
datasets of cloze-style questions over a context
document have been introduced recently, which
allow the training of supervised machine learning
systems (Hermann et al., 2015; Hill et al., 2016;
Onishi et al., 2016). Such datasets can be eas-
ily constructed automatically and the unambigu-
ous nature of their queries provides an objective
benchmark to measure a system’s performance at
text comprehension.

∗BD and HL contributed equally to this work.
1Source code is available on github:

https://

github.com/bdhingra/ga-reader

Deep learning models have been shown to out-
perform traditional shallow approaches on text
comprehension tasks (Hermann et al., 2015). The
success of many recent models can be attributed
primarily to two factors: (1) Multi-hop architec-
tures (Weston et al., 2015; Sordoni et al., 2016;
Shen et al., 2016), allow a model to scan the doc-
ument and the question iteratively for multiple
(2) Attention mechanisms, (Chen et al.,
passes.
2016; Hermann et al., 2015) borrowed from the
machine translation literature (Bahdanau et al.,
2014), allow the model to focus on appropriate
subparts of the context document. Intuitively, the
multi-hop architecture allows the reader to incre-
mentally reﬁne token representations, and the at-
tention mechanism re-weights different parts in
the document according to their relevance to the
query.

The effectiveness of multi-hop reasoning and
attentions have been explored orthogonally so far
in the literature. In this paper, we focus on com-
bining both in a complementary manner, by de-
signing a novel attention mechanism which gates
the evolving token representations across hops.
More speciﬁcally, unlike existing models where
the query attention is applied either token-wise
(Hermann et al., 2015; Kadlec et al., 2016; Chen
et al., 2016; Hill et al., 2016) or sentence-wise
(Weston et al., 2015; Sukhbaatar et al., 2015) to
allow weighted aggregation, the Gated-Attention
(GA) module proposed in this work allows the
query to directly interact with each dimension of
the token embeddings at the semantic-level, and is
applied layer-wise as information ﬁlters during the
multi-hop representation learning process. Such a
ﬁne-grained attention enables our model to learn
conditional token representations w.r.t. the given
question, leading to accurate answer selections.

We show in our experiments that the proposed
GA reader, despite its relative simplicity, consis-

tently improves over a variety of strong baselines
on three benchmark datasets . Our key contribu-
tion, the GA module, provides a signiﬁcant im-
provement for large datasets. Qualitatively, vi-
sualization of the attentions at intermediate lay-
ers of the GA reader shows that in each layer the
GA reader attends to distinct salient aspects of the
query which help in determining the answer.

2 Related Work

The cloze-style QA task involves tuples of the
form (d, q, a, C), where d is a document (context),
q is a query over the contents of d, in which a
phrase is replaced with a placeholder, and a is the
answer to q, which comes from a set of candidates
C. In this work we consider datasets where each
candidate c ∈ C has at least one token which also
appears in the document. The task can then be
described as: given a document-query pair (d, q),
ﬁnd a ∈ C which answers q. Below we provide an
overview of representative neural network archi-
tectures which have been applied to this problem.
LSTMs with Attention: Several architectures in-
troduced in Hermann et al. (2015) employ LSTM
units to compute a combined document-query rep-
resentation g(d, q), which is used to rank the can-
didate answers. These include the DeepLSTM
Reader which performs a single forward pass
through the concatenated (document, query) pair
to obtain g(d, q); the Attentive Reader which ﬁrst
computes a document vector d(q) by a weighted
aggregation of words according to attentions based
on q, and then combines d(q) and q to obtain
their joint representation g(d(q), q); and the Im-
patient Reader where the document representa-
tion is built incrementally. The architecture of the
Attentive Reader has been simpliﬁed recently in
Stanford Attentive Reader, where shallower re-
current units were used with a bilinear form for the
query-document attention (Chen et al., 2016).

Attention Sum:

The Attention-Sum (AS)
Reader (Kadlec et al., 2016) uses two bi-
directional GRU networks (Cho et al., 2015) to
encode both d and q into vectors. A probability
distribution over the entities in d is obtained by
computing dot products between q and the entity
embeddings and taking a softmax. Then, an ag-
gregation scheme named pointer-sum attention is
further applied to sum the probabilities of the same
entity, so that frequent entities the document will
be favored compared to rare ones. Building on the

AS Reader, the Attention-over-Attention (AoA)
Reader (Cui et al., 2017) introduces a two-way
attention mechanism where the query and the doc-
ument are mutually attentive to each other.

Mulit-hop Architectures: Memory Networks
(MemNets) were proposed in Weston et al.
(2015), where each sentence in the document
is encoded to a memory by aggregating nearby
words. Attention over the memory slots given
the query is used to compute an overall memory
and to renew the query representation over multi-
ple iterations, allowing certain types of reasoning
over the salient facts in the memory and the query.
Neural Semantic Encoders (NSE) (Munkhdalai
& Yu, 2017a) extended MemNets by introducing a
write operation which can evolve the memory over
time during the course of reading. Iterative reason-
ing has been found effective in several more recent
models, including the Iterative Attentive Reader
(Sordoni et al., 2016) and ReasoNet (Shen et al.,
2016). The latter allows dynamic reasoning steps
and is trained with reinforcement learning.

Other related works include Dynamic En-
tity Representation network (DER) (Kobayashi
et al., 2016), which builds dynamic representa-
tions of the candidate answers while reading the
document, and accumulates the information about
an entity by max-pooling; EpiReader (Trischler
et al., 2016) consists of two networks, where one
proposes a small set of candidate answers, and the
other reranks the proposed candidates conditioned
on the query and the context; Bi-Directional
Attention Flow network (BiDAF) (Seo et al.,
2017) adopts a multi-stage hierarchical architec-
ture along with a ﬂow-based attention mechanism;
Bajgar et al. (2016) showed a 10% improvement
on the CBT corpus (Hill et al., 2016) by train-
ing the AS Reader on an augmented training set
of about 14 million examples, making a case for
the community to exploit data abundance. The fo-
cus of this paper, however, is on designing models
which exploit the available data efﬁciently.

3 Gated-Attention Reader

Our proposed GA readers perform multiple hops
over the document (context), similar to the Mem-
ory Networks architecture (Sukhbaatar et al.,
2015). Multi-hop architectures mimic the multi-
step comprehension process of human readers, and
have shown promising results in several recent
models for text comprehension (Sordoni et al.,

2016; Kumar et al., 2016; Shen et al., 2016). The
contextual representations in GA readers, namely
the embeddings of words in the document, are it-
eratively reﬁned across hops until reaching a ﬁ-
nal attention-sum module (Kadlec et al., 2016)
which maps the contextual representations in the
last hop to a probability distribution over candi-
date answers.

The attention mechanism has been introduced
recently to model human focus, leading to signif-
icant improvement in machine translation and im-
age captioning (Bahdanau et al., 2014; Mnih et al.,
In reading comprehension tasks, ideally,
2014).
the semantic meanings carried by the contextual
embeddings should be aware of the query across
hops. As an example, human readers are able to
keep the question in mind during multiple passes
of reading, to successively mask away information
irrelevant to the query. However, existing neural
network readers are restricted to either attend to
tokens (Hermann et al., 2015; Chen et al., 2016)
or entire sentences (Weston et al., 2015), with the
assumption that certain sub-parts of the document
In contrast, we
are more important than others.
propose a ﬁner-grained model which attends to
components of the semantic representation being
built up by the GRU. The new attention mecha-
nism, called gated-attention, is implemented via
multiplicative interactions between the query and
the contextual embeddings, and is applied per hop
to act as ﬁne-grained information ﬁlters during the
multi-step reasoning. The ﬁlters weigh individual
components of the vector representation of each
token in the document separately.

The design of gated-attention layers is moti-
vated by the effectiveness of multiplicative inter-
action among vector-space representations, e.g.,
in various types of recurrent units (Hochreiter &
Schmidhuber, 1997; Wu et al., 2016) and in re-
lational learning (Yang et al., 2014; Kiros et al.,
2014). While other types of compositional opera-
tors are possible, such as concatenation or addition
(Mitchell & Lapata, 2008), we ﬁnd that multipli-
cation has strong empirical performance (section
4.3), where query representations naturally serve
as information ﬁlters across hops.

3.1 Model Details

ouput sequence H = [h1, h2, . . . , hT ] as follows:

rt = σ(Wrxt + Urht−1 + br),
zt = σ(Wzxt + Uzht−1 + bz),
˜ht = tanh(Whxt + Uh(rt (cid:12) ht−1) + bh),
ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht.

where (cid:12) denotes the Hadamard product or the
element-wise multiplication. rt and zt are called
the reset and update gates respectively, and ˜ht
the candidate output. A Bi-directional GRU (Bi-
GRU) processes the sequence in both forward and
backward directions to produce two sequences
[hf
1, hb
T ] and [hb
T ], which are
concatenated at the output

2 , . . . , hf

2, . . . , hb

1 , hf

←→
GRU(X) = [hf

1 (cid:107)hb

T , . . . , hf

T (cid:107)hb
1]

(1)

←→
GRU(X) denotes the full output of the
where
Bi-GRU obtained by concatenating each forward
state hf
T −i+1 at step i given
←→
GRU(X) is a matrix in R2nh×T
the input X. Note
where nh is the number of hidden units in GRU.

i and backward state hb

1 , x(0)

2 , . . . x(0)

Let X (0) = [x(0)

|D|] denote the to-
ken embeddings of the document, which are also
inputs at layer 1 for the document reader below,
and Y = [y1, y2, . . . y|Q|] denote the token embed-
dings of the query. Here |D| and |Q| denote the
document and query lengths respectively.

3.1.1 Multi-Hop Architecture
Fig. 1 illustrates the Gated-Attention (GA) reader.
The model reads the document and the query over
K horizontal layers, where layer k receives the
contextual embeddings X (k−1) of the document
from the previous layer. The document embed-
dings are transformed by taking the full output of
a document Bi-GRU (indicated in blue in Fig. 1):

D(k) =

←→
GRU

(k)
D (X (k−1))

At the same time, a layer-speciﬁc query represen-
tation is computed as the full output of a separate
query Bi-GRU (indicated in green in Figure 1):

Q(k) =

←→
GRU

(k)
Q (Y )

Next, Gated-Attention is applied to D(k) and
Q(k) to compute inputs for the next layer X (k).

(2)

(3)

Several components of the model use a Gated Re-
current Unit (GRU) (Cho et al., 2015) which maps
an input sequence X = [x1, x2, . . . , xT ] to an

X (k) = GA(D(k), Q(k))

(4)

where GA is deﬁned in the following subsection.

Figure 1: Gated-Attention Reader. Dashed lines represent dropout connections.

3.1.2 Gated-Attention Module
For brevity, let us drop the superscript k in this
subsection as we are focusing on a particular layer.
For each token di in D, the GA module forms a
token-speciﬁc representation of the query ˜qi using
soft attention, and then multiplies the query rep-
resentation element-wise with the document token
representation. Speciﬁcally, for i = 1, . . . , |D|:

αi = softmax(Q(cid:62)di)
˜qi = Qαi
xi = di (cid:12) ˜qi

answer is then computed by aggregating the prob-
abilities of all document tokens which appear in c
and renormalizing over the candidates:

Pr(c|d, q) ∝

(8)

(cid:88)

si

i∈I(c,d)

where I(c, d) is the set of positions where a token
in c appears in the document d. This aggregation
operation is the same as the pointer sum attention
applied in the AS Reader (Kadlec et al., 2016).

Finally, the candidate with maximum probabil-

ity is selected as the predicted answer:

(5)

(6)

In equation (6) we use the multiplication operator
to model the interactions between di and ˜qi.
In
the experiments section, we also report results for
other choices of gating functions, including addi-
tion xi = di + ˜qi and concatenation xi = di(cid:107)˜qi.

(cid:96) (cid:107)qb

3.1.3 Answer Prediction
Let q(K)
= qf
T −(cid:96)+1 be an intermediate out-
(cid:96)
put of the ﬁnal layer query Bi-GRU at the loca-
tion (cid:96) of the cloze token in the query, and D(K) =
(K)
←→
D (X (K−1)) be the full output of ﬁnal layer
GRU
document Bi-GRU. To obtain the probability that
a particular token in the document answers the
query, we take an inner-product between these
two, and pass through a softmax layer:

s = softmax((q(K)

)T D(K))

(cid:96)

(7)

where vector s deﬁnes a probability distribution
over the |D| tokens in the document. The proba-
bility of a particular candidate c ∈ C as being the

a∗ = argmaxc∈C Pr(c|d, q).

(9)

During the training phase, model parameters of
GA are updated w.r.t. a cross-entropy loss between
the predicted probabilities and the true answers.

3.1.4 Further Enhancements
Character-level Embeddings: Given a token w
from the document or query, its vector space repre-
sentation is computed as x = L(w)||C(w). L(w)
retrieves the word-embedding for w from a lookup
table L ∈ R|V |×nl, whose rows hold a vector for
each unique token in the vocabulary. We also uti-
lize a character composition model C(w) which
generates an orthographic embedding of the token.
Such embeddings have been previously shown to
be helpful for tasks like Named Entity Recognition
(Yang et al., 2016) and dealing with OOV tokens
at test time (Dhingra et al., 2016). The embedding
C(w) is generated by taking the ﬁnal outputs zf
nc
and zb
nc of a Bi-GRU applied to embeddings from

a lookup table of characters in the token, and ap-
plying a linear transformation:

nc||zb
z = zf
nc
C(w) = W z + b

Question Evidence Common Word Feature (qe-
comm): Li et al. (2016) recently proposed a sim-
ple token level indicator feature which signiﬁ-
cantly boosts reading comprehension performance
in some cases. For each token in the document we
construct a one-hot vector fi ∈ {0, 1}2 indicating
its presence in the query. It can be incorporated
into the GA reader by assigning a feature lookup
table F ∈ RnF ×2 (we use nF = 2), taking the
feature embedding ei = f T
i F and appending it
to the inputs of the last layer document BiGRU
as, x(K)
(cid:107)fi for all i. We conducted several ex-
i
periments both with and without this feature and
observed some interesting trends, which are dis-
cussed below. Henceforth, we refer to this feature
as the qe-comm feature or just feature.

4 Experiments and Results

4.1 Datasets

We evaluate the GA reader on ﬁve large-scale
datasets recently proposed in the literature. The
ﬁrst two, CNN and Daily Mail news stories2 con-
sist of articles from the popular CNN and Daily
Mail websites (Hermann et al., 2015). A query
over each article is formed by removing an en-
tity from the short summary which follows the
article. Further, entities within each article were
anonymized to make the task purely a comprehen-
sion one. N-gram statistics, for instance, com-
puted over the entire corpus are no longer useful
in such an anonymized corpus.

The next two datasets are formed from two dif-
ferent subsets of the Children’s Book Test (CBT)3
(Hill et al., 2016). Documents consist of 20 con-
tiguous sentences from the body of a popular chil-
dren’s book, and queries are formed by deleting a
token from the 21st sentence. We only focus on
subsets where the deleted token is either a com-
mon noun (CN) or named entity (NE) since simple
language models already give human-level perfor-
mance on the other types (cf. (Hill et al., 2016)).

2https://github.com/deepmind/rc-data
3http://www.thespermwhale.com/jaseweston/babi/

CBTest.tgz

The ﬁnal dataset is Who Did What4 (WDW)
(Onishi et al., 2016), constructed from the LDC
English Gigaword newswire corpus. First, article
pairs which appeared around the same time and
with overlapping entities are chosen, and then one
article forms the document and a cloze query is
constructed from the other. Missing tokens are al-
ways person named entities. Questions which are
easily answered by simple baselines are ﬁltered
out, to make the task more challenging. There are
two versions of the training set—a small but fo-
cused “Strict” version and a large but noisy “Re-
laxed” version. We report results on both set-
tings which share the same validation and test sets.
Statistics of all the datasets used in our experi-
ments are summarized in the Appendix (Table 5).

4.2 Performance Comparison

Tables 1 and 3 show a comparison of the perfor-
mance of GA Reader with previously published
results on WDW and CNN, Daily Mail, CBT
datasets respectively. The numbers reported for
GA Reader are for single best models, though
we compare to both ensembles and single models
from prior work. GA Reader-- refers to an earlier
version of the model, unpublished but described
in a preprint, with the following differences—(1)
it does not utilize token-speciﬁc attentions within
the GA module, as described in equation (5), (2)
it does not use a character composition model, (3)
it is initialized with word embeddings pretrained
on the corpus itself rather than GloVe. A detailed
analysis of these differences is studied in the next
section. Here we present 4 variants of the latest
GA Reader, using combinations of whether the
qe-comm feature is used (+feature) or not, and
whether the word lookup table L(w) is updated
during training or ﬁxed to its initial value. Other
hyperparameters are listed in Appendix A.

Interestingly, we observe that feature engineer-
ing leads to signiﬁcant improvements for WDW
and CBT datasets, but not for CNN and Daily Mail
datasets. We note that anonymization of the latter
datasets means that there is already some feature
engineering (it adds hints about whether a token
is an entity), and these are much larger than the
other four. In machine learning it is common to see
the effect of feature engineering diminish with in-
creasing data size. Similarly, ﬁxing the word em-
beddings provides an improvement for the WDW

4https://tticnlp.github.io/who_did_what/

Table 1: Validation/Test accuracy (%) on WDW dataset for both “Strict”
and “Relaxed” settings. Results with “†” are cf previously published works.

Model

Human †

Attentive Reader †
AS Reader †
Stanford AR †
NSE †

GA-- †
GA (update L(w))
GA (ﬁx L(w))
GA (+feature, update L(w))
GA (+feature, ﬁx L(w))

Strict

Relaxed

Val

Test Val

Test

–

–
–
–
66.5

–
67.8
68.3
70.1
71.6

84

53
57
64
66.2

57
67.0
68.0
69.5
71.2

–

–

–
–
–
67.0

–
67.0
69.6
70.9
72.6

55
59
65
66.7

60.0
66.6
69.1
71.0
72.6

Table 2: Top: Performance of different gating
functions. Bottom: Effect of varying the num-
ber of hops K. Results on WDW without using
the qe-comm feature and with ﬁxed L(w).

Gating Function

Accuracy

Val

Test

64.9
64.4
68.3

64.5
63.7
68.0

–
65.6
68.3
68.3

57
65.6
68.0
68.2

Sum
Concatenate
Multiply

K

1 (AS) †
2
3
4

and CBT, but not for CNN and Daily Mail. This
is not surprising given that the latter datasets are
larger and less prone to overﬁtting.

Comparing with prior work, on the WDW
dataset the basic version of the GA Reader out-
performs all previously published models when
trained on the Strict setting. By adding the qe-
comm feature the performance increases by 3.2%
and 3.5% on the Strict and Relaxed settings re-
spectively to set a new state of the art on this
dataset. On the CNN and Daily Mail datasets the
GA Reader leads to an improvement of 3.2% and
4.3% respectively over the best previous single
models. They also outperform previous ensem-
ble models, setting a new state of that art for both
datasets. For CBT-NE, GA Reader with the qe-
comm feature outperforms all previous single and
ensemble models except the AS Reader trained on
the much larger BookTest Corpus (Bajgar et al.,
2016). Lastly, on CBT-CN the GA Reader with
the qe-comm feature outperforms all previously
published single models except the NSE, and AS
Reader trained on a larger corpus. For each of the
4 datasets on which GA achieves the top perfor-
mance, we conducted one-sample proportion tests
to test whether GA is signiﬁcantly better than the
second-best baseline. The p-values are 0.319 for
CNN, <0.00001 for DailyMail, 0.028 for CBT-
NE, and <0.00001 for WDW. In other words,
GA statistically signiﬁcantly outperforms all other
baselines on 3 out of those 4 datasets at the 5%
signiﬁcance level. The results could be even more
signiﬁcant under paired tests, however we did not
have access to the predictions from the baselines.

4.3 GA Reader Analysis

In this section we do an ablation study to see the
effect of Gated Attention. We compare the GA
Reader as described here to a model which is ex-
actly the same in all aspects, except that it passes
document embeddings D(k) in each layer directly
to the inputs of the next layer without using the
GA module. In other words X (k) = D(k) for all
k > 0. This model ends up using only one query
GRU at the output layer for selecting the answer
from the document. We compare these two vari-
ants both with and without the qe-comm feature
on CNN and WDW datasets for three subsets of
the training data - 50%, 75% and 100%. Test set
accuracies for these settings are shown in Figure 2.
On CNN when tested without feature engineering,
we observe that GA provides a signiﬁcant boost
in performance compared to without GA. When
tested with the feature it still gives an improve-
ment, but the improvement is signiﬁcant only with
100% training data. On WDW-Strict, which is a
third of the size of CNN, without the feature we
see an improvement when using GA versus with-
out using GA, which becomes signiﬁcant as the
training set size increases. When tested with the
feature on WDW, for a small data size without GA
does better than with GA, but as the dataset size
increases they become equivalent. We conclude
that GA provides a boost in the absence of feature
engineering, or as the training set size increases.

Next we look at the question of how to gate in-
termediate document reader states from the query,
i.e. what operation to use in equation 6. Table

Table 3: Validation/Test accuracy (%) on CNN, Daily Mail and CBT. Results marked with “†” are cf previously published
works. Results marked with “‡” were obtained by training on a larger training set. Best performance on standard training sets
is in bold, and on larger training sets in italics.

Model

Humans (query) †
Humans (context + query) †

LSTMs (context + query) †
Deep LSTM Reader †
Attentive Reader †
Impatient Reader †
MemNets †
AS Reader †
DER Network †
Stanford AR (relabeling) †
Iterative Attentive Reader †
EpiReader †
AoA Reader †
ReasoNet †
NSE †
BiDAF †

MemNets (ensemble) †
AS Reader (ensemble) †
Stanford AR (relabeling,ensemble) †
Iterative Attentive Reader (ensemble) †
EpiReader (ensemble) †

AS Reader (+BookTest) † ‡
AS Reader (+BookTest,ensemble) † ‡

GA--
GA (update L(w))
GA (ﬁx L(w))
GA (+feature, update L(w))
GA (+feature, ﬁx L(w))

CNN

Daily Mail

CBT-NE

CBT-CN

Val

Test Val

Test Val

Test Val

Test

–
–

–
55.0
61.6
61.8
63.4
68.6
71.3
73.8
72.6
73.4
73.1
72.9
–
76.3

66.2
73.9
77.2
75.2
–

–
–

73.0
77.9
77.9
77.3
76.7

–
–

–
57.0
63.0
63.8
66.8
69.5
72.9
73.6
73.3
74.0
74.4
74.7
–
76.9

69.4
75.4
77.6
76.1
–

–
–

73.8
77.9
77.8
76.9
77.4

–
–

–
63.3
70.5
69.0
–
75.0
–
77.6
–
–
–
77.6
–
80.3

–
78.7
80.2
–
–

–
–

76.7
81.5
80.4
80.7
80.0

–
–

–
62.2
69.0
68.0
–
73.9
–
76.6
–
–
–
76.6
–
79.6

–
77.7
79.2
–
–

–
–

75.7
80.9
79.6
80.0
79.3

–
–

51.2
–
–
–
70.4
73.8
–
–
75.2
75.3
77.8
–
78.2
–

–
76.2
–
76.9
76.6

80.5
82.3

74.9
76.7
77.2
77.2
78.5

52.0
81.6

41.8
–
–
–
66.6
68.6
–
–
68.6
69.7
72.0
–
73.2
–

–
71.0
–
72.0
71.8

76.2
78.4

69.0
70.1
71.4
73.3
74.9

–
–

62.6
–
–
–
64.2
68.8
–
–
72.1
71.5
72.2
–
74.3
–

–
71.1
–
74.1
73.6

83.2
85.7

69.0
69.8
71.6
73.0
74.4

64.4
81.6

56.0
–
–
–
63.0
63.4
–
–
69.2
67.4
69.4
–
71.9
–

–
68.9
–
71.0
70.6

80.8
83.7

63.9
67.3
68.0
69.8
70.7

2 (top) shows the performance on WDW dataset
for three common choices – sum (x = d + q),
concatenate (x = d(cid:107)q) and multiply (x =
d (cid:12) q). Empirically we ﬁnd element-wise multipli-
cation does signiﬁcantly better than the other two,
which justiﬁes our motivation to “ﬁlter” out docu-
ment features which are irrelevant to the query.

that. This is a common trend in machine learn-
ing as model complexity is increased, however we
note that a multi-hop architecture is important to
achieve a high performance for this task, and pro-
vide further evidence for this in the next section.

4.4 Ablation Study for Model Components

At the bottom of Table 2 we show the effect of
varying the number of hops K of the GA Reader
on the ﬁnal performance. We note that for K = 1,
our model is equivalent to the AS Reader with-
out any GA modules. We see a steep and steady
rise in accuracy as the number of hops is increased
from K = 1 to 3, which remains constant beyond

Table 4 shows accuracy on WDW by removing
one component at a time. The steepest reduc-
tion is observed when we replace pretrained GloVe
vectors with those pretrained on the corpus itself.
GloVe vectors were trained on a large corpus of
about 6 billion tokens (Pennington et al., 2014),
and provide an important source of prior knowl-

Figure 2: Performance in accuracy with and without the Gated-Attention module over different training
sizes. p-values for an exact one-sided Mcnemar’s test are given inside the parentheses for each setting.

Table 4: Ablation study on WDW dataset, without using
the qe-comm feature and with ﬁxed L(w). Results marked
with † are cf Onishi et al. (2016).

Model

GA

GA--†

−char
−token-attentions (eq. 5)
−glove, +corpus

Accuracy

Val

Test

68.3
66.9
65.7
64.0

–

68.0
66.9
65.0
62.5

57

edge for the model. Note that the strongest base-
line on WDW, NSE (Munkhdalai & Yu, 2017b),
also uses pretrained GloVe vectors, hence the
comparison is fair in that respect. Next, we ob-
serve a substantial drop when removing token-
speciﬁc attentions over the query in the GA mod-
ule, which allow gating individual tokens in the
document only by parts of the query relevant to
that token rather than the overall query representa-
tion. Finally, removing the character embeddings,
which were only used for WDW and CBT, leads
to a reduction of about 1% in the performance.

4.5 Attention Visualization

To gain an insight into the reading process em-
ployed by the model we analyzed the attention dis-
tributions at intermediate layers of the reader. Fig-
ure 3 shows an example from the validation set of
WDW dataset (several more are in the Appendix).
In each ﬁgure, the left and middle plots visualize
attention over the query (equation 5) for candi-
dates in the document after layers 1 & 2 respec-
tively. The right plot shows attention over candi-

dates in the document of cloze placeholder (XXX)
in the query at the ﬁnal layer. The full document,
query and correct answer are shown at the bottom.
A generic pattern observed in these examples
is that in intermediate layers, candidates in the
document (shown along rows) tend to pick out
salient tokens in the query which provide clues
about the cloze, and in the ﬁnal layer the candi-
date with the highest match with these tokens is
selected as the answer. In Figure 3 there is a high
attention of the correct answer on financial
regulatory standards in the ﬁrst layer, and
on us president in the second layer. The in-
correct answer, in contrast, only attends to one of
these aspects, and hence receives a lower score in
the ﬁnal layer despite the n-gram overlap it has
with the cloze token in the query. Importantly, dif-
ferent layers tend to focus on different tokens in
the query, supporting the hypothesis that the multi-
hop architecture of GA Reader is able to combine
distinct pieces of information to answer the query.

5 Conclusion

We presented the Gated-Attention reader for an-
swering cloze-style questions over documents.
The GA reader features a novel multiplicative gat-
ing mechanism, combined with a multi-hop ar-
chitecture. Our model achieves the state-of-the-
art performance on several large-scale benchmark
datasets with more than 4% improvements over
competitive baselines. Our model design is backed
up by an ablation study showing statistically sig-
niﬁcant improvements of using Gated Attention
as information ﬁlters. We also showed empiri-
cally that multiplicative gating is superior to addi-

Figure 3: Layer-wise attention visualization of GA Reader trained on WDW-Strict. See text for details.

tion and concatenation operations for implement-
ing gated-attentions, though a theoretical justiﬁca-
tion remains part of future research goals. Anal-
ysis of document and query attentions in interme-
diate layers of the reader further reveals that the
model iteratively attends to different aspects of the
query to arrive at the ﬁnal answer. In this paper
we have focused on text comprehension, but we
believe that the Gated-Attention mechanism may
beneﬁt other tasks as well where multiple sources
of information interact.

Acknowledgments

This work was funded by NSF under CCF1414030
and Google Research.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473, 2014.

Ondrej Bajgar, Rudolf Kadlec, and Jan Kleindi-
enst.
Embracing data abundance: Booktest
dataset for reading comprehension. arXiv preprint
arXiv:1610.00956, 2016.

Danqi Chen, Jason Bolton, and Christopher D Man-
ning. A thorough examination of the cnn/daily mail
reading comprehension task. ACL, 2016.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger

Schwenk, and Yoshua Bengio. Learning phrase rep-
resentations using rnn encoder-decoder for statisti-
cal machine translation. ACL, 2015.

Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang,
Ting Liu, and Guoping Hu. Attention-over-attention
neural networks for reading comprehension. ACL,
2017.

Bhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick,
Michael Muehl, and William W Cohen. Tweet2vec:
Character-based distributed representations for so-
cial media. ACL, 2016.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. Teaching machines to
read and comprehend. In Advances in Neural Infor-
mation Processing Systems, pp. 1684–1692, 2015.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. The goldilocks principle: Reading chil-
dren’s books with explicit memory representations.
ICLR, 2016.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-
term memory. Neural computation, 9(8):1735–
1780, 1997.

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan
Kleindienst. Text understanding with the attention
sum reader network. ACL, 2016.

Diederik Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. ICLR, 2015.

Ryan Kiros, Richard Zemel, and Ruslan R Salakhutdi-
nov. A multiplicative model for learning distributed
text-based attribute representations. In Advances in
Neural Information Processing Systems, pp. 2348–
2356, 2014.

Sosuke Kobayashi, Ran Tian, Naoaki Okazaki, and
Kentaro Inui. Dynamic entity representations with
max-pooling improves machine reading. In NAACL-
HLT, 2016.

Ankit Kumar, Ozan Irsoy, Jonathan Su, James Brad-
bury, Robert English, Brian Pierce, Peter Ondruska,
Ishaan Gulrajani, and Richard Socher. Ask me any-
thing: Dynamic memory networks for natural lan-
guage processing. ICML, 2016.

Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying
Cao, Jie Zhou, and Wei Xu. Dataset and neu-
ral recurrent sequence labeling model for open-
domain factoid question answering. arXiv preprint
arXiv:1607.06275, 2016.

Jeff Mitchell and Mirella Lapata. Vector-based mod-
els of semantic composition. In ACL, pp. 236–244,
2008.

Volodymyr Mnih, Nicolas Heess, Alex Graves, et al.
Recurrent models of visual attention. In Advances in
Neural Information Processing Systems, pp. 2204–
2212, 2014.

Tsendsuren Munkhdalai and Hong Yu. Neural seman-

tic encoders. EACL, 2017a.

Tsendsuren Munkhdalai and Hong Yu. Reasoning with
memory augmented neural networks for language
comprehension. ICLR, 2017b.

Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gim-
pel, and David McAllester. Who did what: A large-
scale person-centered cloze dataset. EMNLP, 2016.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
On the difﬁculty of training recurrent neural net-
works. ICML (3), 28:1310–1318, 2013.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pp. 1532–
1543, 2014. URL http://www.aclweb.org/
anthology/D14-1162.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. Bidirectional attention ﬂow
for machine comprehension. ICLR, 2017.

Yelong Shen, Po-Sen Huang, Jianfeng Gao, and
Weizhu Chen. Reasonet: Learning to stop read-
arXiv preprint
ing in machine comprehension.
arXiv:1609.05284, 2016.

Alessandro Sordoni, Phillip Bachman, and Yoshua
Bengio. Iterative alternating neural attention for ma-
arXiv preprint arXiv:1606.02245,
chine reading.
2016.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
End-to-end memory networks. In Advances in Neu-
ral Information Processing Systems, pp. 2431–2439,
2015.

Theano Development Team.

Theano: A Python
framework for fast computation of mathematical ex-
arXiv e-prints, abs/1605.02688, May
pressions.
2016. URL http://arxiv.org/abs/1605.
02688.

Adam Trischler, Zheng Ye, Xingdi Yuan, and Kaheer
Suleman. Natural language comprehension with the
epireader. EMNLP, 2016.

Jason Weston, Sumit Chopra, and Antoine Bordes.

Memory networks. ICLR, 2015.

Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua
Bengio, and Ruslan Salakhutdinov. On multiplica-
tive integration with recurrent neural networks. Ad-
vances in Neural Information Processing Systems,
2016.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. Learning multi-relational seman-
tics using neural-embedding models. NIPS Work-
shop on Learning Semantics, 2014.

Zhilin Yang, Ruslan Salakhutdinov, and William Co-
hen. Multi-task cross-lingual sequence tagging from
scratch. arXiv preprint arXiv:1603.06270, 2016.

A Implementation Details

Our model was implemented using the Theano
(Theano Development Team, 2016) and Lasagne5
Python libraries. We used stochastic gradient de-
scent with ADAM updates for optimization, which
combines classical momentum and adaptive gradi-
ents (Kingma & Ba, 2015). The batch size was 32
and the initial learning rate was 5 × 10−4 which
was halved every epoch after the second epoch.
The same setting is applied to all models and
datasets. We also used gradient clipping with a
threshold of 10 to stabilize GRU training (Pascanu
et al., 2013). We set the number of layers K to be
3 for all experiments. The number of hidden units
for the character GRU was set to 50. The remain-
ing two hyperparameters—size of document and
query GRUs, and dropout rate—were tuned on the
validation set, and their optimal values are shown
in Table 6. In general, the optimal GRU size in-
creases and the dropout rate decreases as the cor-
pus size increases.

The word lookup table was initialized with 100d
GloVe vectors6 (Pennington et al., 2014) and OOV
tokens at test time were assigned unique random
vectors. We empirically observed that initializing
with pre-trained embeddings gives higher perfor-
mance compared to random initialization for all

5https://lasagne.readthedocs.io/en/latest/
6http://nlp.stanford.edu/projects/glove/

Table 5: Dataset statistics.

CNN Daily Mail CBT-NE CBT-CN WDW-Strict WDW-Relaxed

# train
# validation
# test
# vocab
max doc length

380,298
3,924
3,198
118,497
2,000

879,450
64,835
53,182
208,045
2,000

108,719
2,000
2,500
53,063
1,338

120,769
2,000
2,500
53,185
1,338

127,786
10,000
10,000
347,406
3,085

185,978
10,000
10,000
308,602
3,085

Table 6: Hyperparameter settings for each dataset. dim() indicates hidden state size of GRU.

Hyperparameter CNN Daily Mail CBT-NE CBT-CN WDW-Strict WDW-Relaxed

Dropout
←→
GRU∗)

dim(

0.2

256

0.1

256

0.4

128

0.4

128

0.3

128

0.3

128

datasets. Furthermore, for smaller datasets (WDW
and CBT) we found that ﬁxing these embeddings
to their pretrained values led to higher test perfor-
mance, possibly since it avoids overﬁtting. We do
not use the character composition model for CNN
and Daily Mail, since their entities (and hence can-
didate answers) are anonymized to generic tokens.
For other datasets the character lookup table was
randomly initialized with 25d vectors. All other
parameters were initialized to their default values
as speciﬁed in the Lasagne library.

B Attention Plots

Figure 4: Layer-wise attention visualization of GA Reader trained on WDW-Strict. See text for details.

Figure 5: Layer-wise attention visualization of GA Reader trained on WDW-Strict. See text for details.

Figure 6: Layer-wise attention visualization of GA Reader trained on WDW-Strict. See text for details.

Figure 7: Layer-wise attention visualization of GA Reader trained on WDW-Strict. See text for details.

Gated-Attention Readers for Text Comprehension

Bhuwan Dhingra∗

Hanxiao Liu∗

Zhilin Yang

William W. Cohen

Ruslan Salakhutdinov

School of Computer Science
Carnegie Mellon University
{bdhingra,hanxiaol,zhiliny,wcohen,rsalakhu}@cs.cmu.edu

7
1
0
2
 
r
p
A
 
1
2
 
 
]
L
C
.
s
c
[
 
 
3
v
9
4
5
1
0
.
6
0
6
1
:
v
i
X
r
a

Abstract

In this paper we study the problem of an-
swering cloze-style questions over docu-
ments. Our model, the Gated-Attention
(GA) Reader1, integrates a multi-hop ar-
chitecture with a novel attention mecha-
nism, which is based on multiplicative in-
teractions between the query embedding
and the intermediate states of a recurrent
neural network document reader. This
enables the reader to build query-speciﬁc
representations of tokens in the document
for accurate answer selection. The GA
Reader obtains state-of-the-art results on
three benchmarks for this task–the CNN &
Daily Mail news stories and the Who Did
What dataset. The effectiveness of multi-
plicative interaction is demonstrated by an
ablation study, and by comparing to alter-
native compositional operators for imple-
menting the gated-attention.

1

Introduction

A recent trend to measure progress towards ma-
chine reading is to test a system’s ability to an-
swer questions about a document it has to com-
prehend. Towards this end, several large-scale
datasets of cloze-style questions over a context
document have been introduced recently, which
allow the training of supervised machine learning
systems (Hermann et al., 2015; Hill et al., 2016;
Onishi et al., 2016). Such datasets can be eas-
ily constructed automatically and the unambigu-
ous nature of their queries provides an objective
benchmark to measure a system’s performance at
text comprehension.

∗BD and HL contributed equally to this work.
1Source code is available on github:

https://

github.com/bdhingra/ga-reader

Deep learning models have been shown to out-
perform traditional shallow approaches on text
comprehension tasks (Hermann et al., 2015). The
success of many recent models can be attributed
primarily to two factors: (1) Multi-hop architec-
tures (Weston et al., 2015; Sordoni et al., 2016;
Shen et al., 2016), allow a model to scan the doc-
ument and the question iteratively for multiple
(2) Attention mechanisms, (Chen et al.,
passes.
2016; Hermann et al., 2015) borrowed from the
machine translation literature (Bahdanau et al.,
2014), allow the model to focus on appropriate
subparts of the context document. Intuitively, the
multi-hop architecture allows the reader to incre-
mentally reﬁne token representations, and the at-
tention mechanism re-weights different parts in
the document according to their relevance to the
query.

The effectiveness of multi-hop reasoning and
attentions have been explored orthogonally so far
in the literature. In this paper, we focus on com-
bining both in a complementary manner, by de-
signing a novel attention mechanism which gates
the evolving token representations across hops.
More speciﬁcally, unlike existing models where
the query attention is applied either token-wise
(Hermann et al., 2015; Kadlec et al., 2016; Chen
et al., 2016; Hill et al., 2016) or sentence-wise
(Weston et al., 2015; Sukhbaatar et al., 2015) to
allow weighted aggregation, the Gated-Attention
(GA) module proposed in this work allows the
query to directly interact with each dimension of
the token embeddings at the semantic-level, and is
applied layer-wise as information ﬁlters during the
multi-hop representation learning process. Such a
ﬁne-grained attention enables our model to learn
conditional token representations w.r.t. the given
question, leading to accurate answer selections.

We show in our experiments that the proposed
GA reader, despite its relative simplicity, consis-

tently improves over a variety of strong baselines
on three benchmark datasets . Our key contribu-
tion, the GA module, provides a signiﬁcant im-
provement for large datasets. Qualitatively, vi-
sualization of the attentions at intermediate lay-
ers of the GA reader shows that in each layer the
GA reader attends to distinct salient aspects of the
query which help in determining the answer.

2 Related Work

The cloze-style QA task involves tuples of the
form (d, q, a, C), where d is a document (context),
q is a query over the contents of d, in which a
phrase is replaced with a placeholder, and a is the
answer to q, which comes from a set of candidates
C. In this work we consider datasets where each
candidate c ∈ C has at least one token which also
appears in the document. The task can then be
described as: given a document-query pair (d, q),
ﬁnd a ∈ C which answers q. Below we provide an
overview of representative neural network archi-
tectures which have been applied to this problem.
LSTMs with Attention: Several architectures in-
troduced in Hermann et al. (2015) employ LSTM
units to compute a combined document-query rep-
resentation g(d, q), which is used to rank the can-
didate answers. These include the DeepLSTM
Reader which performs a single forward pass
through the concatenated (document, query) pair
to obtain g(d, q); the Attentive Reader which ﬁrst
computes a document vector d(q) by a weighted
aggregation of words according to attentions based
on q, and then combines d(q) and q to obtain
their joint representation g(d(q), q); and the Im-
patient Reader where the document representa-
tion is built incrementally. The architecture of the
Attentive Reader has been simpliﬁed recently in
Stanford Attentive Reader, where shallower re-
current units were used with a bilinear form for the
query-document attention (Chen et al., 2016).

Attention Sum:

The Attention-Sum (AS)
Reader (Kadlec et al., 2016) uses two bi-
directional GRU networks (Cho et al., 2015) to
encode both d and q into vectors. A probability
distribution over the entities in d is obtained by
computing dot products between q and the entity
embeddings and taking a softmax. Then, an ag-
gregation scheme named pointer-sum attention is
further applied to sum the probabilities of the same
entity, so that frequent entities the document will
be favored compared to rare ones. Building on the

AS Reader, the Attention-over-Attention (AoA)
Reader (Cui et al., 2017) introduces a two-way
attention mechanism where the query and the doc-
ument are mutually attentive to each other.

Mulit-hop Architectures: Memory Networks
(MemNets) were proposed in Weston et al.
(2015), where each sentence in the document
is encoded to a memory by aggregating nearby
words. Attention over the memory slots given
the query is used to compute an overall memory
and to renew the query representation over multi-
ple iterations, allowing certain types of reasoning
over the salient facts in the memory and the query.
Neural Semantic Encoders (NSE) (Munkhdalai
& Yu, 2017a) extended MemNets by introducing a
write operation which can evolve the memory over
time during the course of reading. Iterative reason-
ing has been found effective in several more recent
models, including the Iterative Attentive Reader
(Sordoni et al., 2016) and ReasoNet (Shen et al.,
2016). The latter allows dynamic reasoning steps
and is trained with reinforcement learning.

Other related works include Dynamic En-
tity Representation network (DER) (Kobayashi
et al., 2016), which builds dynamic representa-
tions of the candidate answers while reading the
document, and accumulates the information about
an entity by max-pooling; EpiReader (Trischler
et al., 2016) consists of two networks, where one
proposes a small set of candidate answers, and the
other reranks the proposed candidates conditioned
on the query and the context; Bi-Directional
Attention Flow network (BiDAF) (Seo et al.,
2017) adopts a multi-stage hierarchical architec-
ture along with a ﬂow-based attention mechanism;
Bajgar et al. (2016) showed a 10% improvement
on the CBT corpus (Hill et al., 2016) by train-
ing the AS Reader on an augmented training set
of about 14 million examples, making a case for
the community to exploit data abundance. The fo-
cus of this paper, however, is on designing models
which exploit the available data efﬁciently.

3 Gated-Attention Reader

Our proposed GA readers perform multiple hops
over the document (context), similar to the Mem-
ory Networks architecture (Sukhbaatar et al.,
2015). Multi-hop architectures mimic the multi-
step comprehension process of human readers, and
have shown promising results in several recent
models for text comprehension (Sordoni et al.,

2016; Kumar et al., 2016; Shen et al., 2016). The
contextual representations in GA readers, namely
the embeddings of words in the document, are it-
eratively reﬁned across hops until reaching a ﬁ-
nal attention-sum module (Kadlec et al., 2016)
which maps the contextual representations in the
last hop to a probability distribution over candi-
date answers.

The attention mechanism has been introduced
recently to model human focus, leading to signif-
icant improvement in machine translation and im-
age captioning (Bahdanau et al., 2014; Mnih et al.,
In reading comprehension tasks, ideally,
2014).
the semantic meanings carried by the contextual
embeddings should be aware of the query across
hops. As an example, human readers are able to
keep the question in mind during multiple passes
of reading, to successively mask away information
irrelevant to the query. However, existing neural
network readers are restricted to either attend to
tokens (Hermann et al., 2015; Chen et al., 2016)
or entire sentences (Weston et al., 2015), with the
assumption that certain sub-parts of the document
In contrast, we
are more important than others.
propose a ﬁner-grained model which attends to
components of the semantic representation being
built up by the GRU. The new attention mecha-
nism, called gated-attention, is implemented via
multiplicative interactions between the query and
the contextual embeddings, and is applied per hop
to act as ﬁne-grained information ﬁlters during the
multi-step reasoning. The ﬁlters weigh individual
components of the vector representation of each
token in the document separately.

The design of gated-attention layers is moti-
vated by the effectiveness of multiplicative inter-
action among vector-space representations, e.g.,
in various types of recurrent units (Hochreiter &
Schmidhuber, 1997; Wu et al., 2016) and in re-
lational learning (Yang et al., 2014; Kiros et al.,
2014). While other types of compositional opera-
tors are possible, such as concatenation or addition
(Mitchell & Lapata, 2008), we ﬁnd that multipli-
cation has strong empirical performance (section
4.3), where query representations naturally serve
as information ﬁlters across hops.

3.1 Model Details

ouput sequence H = [h1, h2, . . . , hT ] as follows:

rt = σ(Wrxt + Urht−1 + br),
zt = σ(Wzxt + Uzht−1 + bz),
˜ht = tanh(Whxt + Uh(rt (cid:12) ht−1) + bh),
ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht.

where (cid:12) denotes the Hadamard product or the
element-wise multiplication. rt and zt are called
the reset and update gates respectively, and ˜ht
the candidate output. A Bi-directional GRU (Bi-
GRU) processes the sequence in both forward and
backward directions to produce two sequences
[hf
1, hb
T ] and [hb
T ], which are
concatenated at the output

2 , . . . , hf

2, . . . , hb

1 , hf

←→
GRU(X) = [hf

1 (cid:107)hb

T , . . . , hf

T (cid:107)hb
1]

(1)

←→
GRU(X) denotes the full output of the
where
Bi-GRU obtained by concatenating each forward
state hf
T −i+1 at step i given
←→
GRU(X) is a matrix in R2nh×T
the input X. Note
where nh is the number of hidden units in GRU.

i and backward state hb

1 , x(0)

2 , . . . x(0)

Let X (0) = [x(0)

|D|] denote the to-
ken embeddings of the document, which are also
inputs at layer 1 for the document reader below,
and Y = [y1, y2, . . . y|Q|] denote the token embed-
dings of the query. Here |D| and |Q| denote the
document and query lengths respectively.

3.1.1 Multi-Hop Architecture
Fig. 1 illustrates the Gated-Attention (GA) reader.
The model reads the document and the query over
K horizontal layers, where layer k receives the
contextual embeddings X (k−1) of the document
from the previous layer. The document embed-
dings are transformed by taking the full output of
a document Bi-GRU (indicated in blue in Fig. 1):

D(k) =

←→
GRU

(k)
D (X (k−1))

At the same time, a layer-speciﬁc query represen-
tation is computed as the full output of a separate
query Bi-GRU (indicated in green in Figure 1):

Q(k) =

←→
GRU

(k)
Q (Y )

Next, Gated-Attention is applied to D(k) and
Q(k) to compute inputs for the next layer X (k).

(2)

(3)

Several components of the model use a Gated Re-
current Unit (GRU) (Cho et al., 2015) which maps
an input sequence X = [x1, x2, . . . , xT ] to an

X (k) = GA(D(k), Q(k))

(4)

where GA is deﬁned in the following subsection.

Figure 1: Gated-Attention Reader. Dashed lines represent dropout connections.

3.1.2 Gated-Attention Module
For brevity, let us drop the superscript k in this
subsection as we are focusing on a particular layer.
For each token di in D, the GA module forms a
token-speciﬁc representation of the query ˜qi using
soft attention, and then multiplies the query rep-
resentation element-wise with the document token
representation. Speciﬁcally, for i = 1, . . . , |D|:

αi = softmax(Q(cid:62)di)
˜qi = Qαi
xi = di (cid:12) ˜qi

answer is then computed by aggregating the prob-
abilities of all document tokens which appear in c
and renormalizing over the candidates:

Pr(c|d, q) ∝

(8)

(cid:88)

si

i∈I(c,d)

where I(c, d) is the set of positions where a token
in c appears in the document d. This aggregation
operation is the same as the pointer sum attention
applied in the AS Reader (Kadlec et al., 2016).

Finally, the candidate with maximum probabil-

ity is selected as the predicted answer:

(5)

(6)

In equation (6) we use the multiplication operator
to model the interactions between di and ˜qi.
In
the experiments section, we also report results for
other choices of gating functions, including addi-
tion xi = di + ˜qi and concatenation xi = di(cid:107)˜qi.

(cid:96) (cid:107)qb

3.1.3 Answer Prediction
Let q(K)
= qf
T −(cid:96)+1 be an intermediate out-
(cid:96)
put of the ﬁnal layer query Bi-GRU at the loca-
tion (cid:96) of the cloze token in the query, and D(K) =
(K)
←→
D (X (K−1)) be the full output of ﬁnal layer
GRU
document Bi-GRU. To obtain the probability that
a particular token in the document answers the
query, we take an inner-product between these
two, and pass through a softmax layer:

s = softmax((q(K)

)T D(K))

(cid:96)

(7)

where vector s deﬁnes a probability distribution
over the |D| tokens in the document. The proba-
bility of a particular candidate c ∈ C as being the

a∗ = argmaxc∈C Pr(c|d, q).

(9)

During the training phase, model parameters of
GA are updated w.r.t. a cross-entropy loss between
the predicted probabilities and the true answers.

3.1.4 Further Enhancements
Character-level Embeddings: Given a token w
from the document or query, its vector space repre-
sentation is computed as x = L(w)||C(w). L(w)
retrieves the word-embedding for w from a lookup
table L ∈ R|V |×nl, whose rows hold a vector for
each unique token in the vocabulary. We also uti-
lize a character composition model C(w) which
generates an orthographic embedding of the token.
Such embeddings have been previously shown to
be helpful for tasks like Named Entity Recognition
(Yang et al., 2016) and dealing with OOV tokens
at test time (Dhingra et al., 2016). The embedding
C(w) is generated by taking the ﬁnal outputs zf
nc
and zb
nc of a Bi-GRU applied to embeddings from

a lookup table of characters in the token, and ap-
plying a linear transformation:

nc||zb
z = zf
nc
C(w) = W z + b

Question Evidence Common Word Feature (qe-
comm): Li et al. (2016) recently proposed a sim-
ple token level indicator feature which signiﬁ-
cantly boosts reading comprehension performance
in some cases. For each token in the document we
construct a one-hot vector fi ∈ {0, 1}2 indicating
its presence in the query. It can be incorporated
into the GA reader by assigning a feature lookup
table F ∈ RnF ×2 (we use nF = 2), taking the
feature embedding ei = f T
i F and appending it
to the inputs of the last layer document BiGRU
as, x(K)
(cid:107)fi for all i. We conducted several ex-
i
periments both with and without this feature and
observed some interesting trends, which are dis-
cussed below. Henceforth, we refer to this feature
as the qe-comm feature or just feature.

4 Experiments and Results

4.1 Datasets

We evaluate the GA reader on ﬁve large-scale
datasets recently proposed in the literature. The
ﬁrst two, CNN and Daily Mail news stories2 con-
sist of articles from the popular CNN and Daily
Mail websites (Hermann et al., 2015). A query
over each article is formed by removing an en-
tity from the short summary which follows the
article. Further, entities within each article were
anonymized to make the task purely a comprehen-
sion one. N-gram statistics, for instance, com-
puted over the entire corpus are no longer useful
in such an anonymized corpus.

The next two datasets are formed from two dif-
ferent subsets of the Children’s Book Test (CBT)3
(Hill et al., 2016). Documents consist of 20 con-
tiguous sentences from the body of a popular chil-
dren’s book, and queries are formed by deleting a
token from the 21st sentence. We only focus on
subsets where the deleted token is either a com-
mon noun (CN) or named entity (NE) since simple
language models already give human-level perfor-
mance on the other types (cf. (Hill et al., 2016)).

2https://github.com/deepmind/rc-data
3http://www.thespermwhale.com/jaseweston/babi/

CBTest.tgz

The ﬁnal dataset is Who Did What4 (WDW)
(Onishi et al., 2016), constructed from the LDC
English Gigaword newswire corpus. First, article
pairs which appeared around the same time and
with overlapping entities are chosen, and then one
article forms the document and a cloze query is
constructed from the other. Missing tokens are al-
ways person named entities. Questions which are
easily answered by simple baselines are ﬁltered
out, to make the task more challenging. There are
two versions of the training set—a small but fo-
cused “Strict” version and a large but noisy “Re-
laxed” version. We report results on both set-
tings which share the same validation and test sets.
Statistics of all the datasets used in our experi-
ments are summarized in the Appendix (Table 5).

4.2 Performance Comparison

Tables 1 and 3 show a comparison of the perfor-
mance of GA Reader with previously published
results on WDW and CNN, Daily Mail, CBT
datasets respectively. The numbers reported for
GA Reader are for single best models, though
we compare to both ensembles and single models
from prior work. GA Reader-- refers to an earlier
version of the model, unpublished but described
in a preprint, with the following differences—(1)
it does not utilize token-speciﬁc attentions within
the GA module, as described in equation (5), (2)
it does not use a character composition model, (3)
it is initialized with word embeddings pretrained
on the corpus itself rather than GloVe. A detailed
analysis of these differences is studied in the next
section. Here we present 4 variants of the latest
GA Reader, using combinations of whether the
qe-comm feature is used (+feature) or not, and
whether the word lookup table L(w) is updated
during training or ﬁxed to its initial value. Other
hyperparameters are listed in Appendix A.

Interestingly, we observe that feature engineer-
ing leads to signiﬁcant improvements for WDW
and CBT datasets, but not for CNN and Daily Mail
datasets. We note that anonymization of the latter
datasets means that there is already some feature
engineering (it adds hints about whether a token
is an entity), and these are much larger than the
other four. In machine learning it is common to see
the effect of feature engineering diminish with in-
creasing data size. Similarly, ﬁxing the word em-
beddings provides an improvement for the WDW

4https://tticnlp.github.io/who_did_what/

Table 1: Validation/Test accuracy (%) on WDW dataset for both “Strict”
and “Relaxed” settings. Results with “†” are cf previously published works.

Model

Human †

Attentive Reader †
AS Reader †
Stanford AR †
NSE †

GA-- †
GA (update L(w))
GA (ﬁx L(w))
GA (+feature, update L(w))
GA (+feature, ﬁx L(w))

Strict

Relaxed

Val

Test Val

Test

–

–
–
–
66.5

–
67.8
68.3
70.1
71.6

84

53
57
64
66.2

57
67.0
68.0
69.5
71.2

–

–

–
–
–
67.0

–
67.0
69.6
70.9
72.6

55
59
65
66.7

60.0
66.6
69.1
71.0
72.6

Table 2: Top: Performance of different gating
functions. Bottom: Effect of varying the num-
ber of hops K. Results on WDW without using
the qe-comm feature and with ﬁxed L(w).

Gating Function

Accuracy

Val

Test

64.9
64.4
68.3

64.5
63.7
68.0

–
65.6
68.3
68.3

57
65.6
68.0
68.2

Sum
Concatenate
Multiply

K

1 (AS) †
2
3
4

and CBT, but not for CNN and Daily Mail. This
is not surprising given that the latter datasets are
larger and less prone to overﬁtting.

Comparing with prior work, on the WDW
dataset the basic version of the GA Reader out-
performs all previously published models when
trained on the Strict setting. By adding the qe-
comm feature the performance increases by 3.2%
and 3.5% on the Strict and Relaxed settings re-
spectively to set a new state of the art on this
dataset. On the CNN and Daily Mail datasets the
GA Reader leads to an improvement of 3.2% and
4.3% respectively over the best previous single
models. They also outperform previous ensem-
ble models, setting a new state of that art for both
datasets. For CBT-NE, GA Reader with the qe-
comm feature outperforms all previous single and
ensemble models except the AS Reader trained on
the much larger BookTest Corpus (Bajgar et al.,
2016). Lastly, on CBT-CN the GA Reader with
the qe-comm feature outperforms all previously
published single models except the NSE, and AS
Reader trained on a larger corpus. For each of the
4 datasets on which GA achieves the top perfor-
mance, we conducted one-sample proportion tests
to test whether GA is signiﬁcantly better than the
second-best baseline. The p-values are 0.319 for
CNN, <0.00001 for DailyMail, 0.028 for CBT-
NE, and <0.00001 for WDW. In other words,
GA statistically signiﬁcantly outperforms all other
baselines on 3 out of those 4 datasets at the 5%
signiﬁcance level. The results could be even more
signiﬁcant under paired tests, however we did not
have access to the predictions from the baselines.

4.3 GA Reader Analysis

In this section we do an ablation study to see the
effect of Gated Attention. We compare the GA
Reader as described here to a model which is ex-
actly the same in all aspects, except that it passes
document embeddings D(k) in each layer directly
to the inputs of the next layer without using the
GA module. In other words X (k) = D(k) for all
k > 0. This model ends up using only one query
GRU at the output layer for selecting the answer
from the document. We compare these two vari-
ants both with and without the qe-comm feature
on CNN and WDW datasets for three subsets of
the training data - 50%, 75% and 100%. Test set
accuracies for these settings are shown in Figure 2.
On CNN when tested without feature engineering,
we observe that GA provides a signiﬁcant boost
in performance compared to without GA. When
tested with the feature it still gives an improve-
ment, but the improvement is signiﬁcant only with
100% training data. On WDW-Strict, which is a
third of the size of CNN, without the feature we
see an improvement when using GA versus with-
out using GA, which becomes signiﬁcant as the
training set size increases. When tested with the
feature on WDW, for a small data size without GA
does better than with GA, but as the dataset size
increases they become equivalent. We conclude
that GA provides a boost in the absence of feature
engineering, or as the training set size increases.

Next we look at the question of how to gate in-
termediate document reader states from the query,
i.e. what operation to use in equation 6. Table

Table 3: Validation/Test accuracy (%) on CNN, Daily Mail and CBT. Results marked with “†” are cf previously published
works. Results marked with “‡” were obtained by training on a larger training set. Best performance on standard training sets
is in bold, and on larger training sets in italics.

Model

Humans (query) †
Humans (context + query) †

LSTMs (context + query) †
Deep LSTM Reader †
Attentive Reader †
Impatient Reader †
MemNets †
AS Reader †
DER Network †
Stanford AR (relabeling) †
Iterative Attentive Reader †
EpiReader †
AoA Reader †
ReasoNet †
NSE †
BiDAF †

MemNets (ensemble) †
AS Reader (ensemble) †
Stanford AR (relabeling,ensemble) †
Iterative Attentive Reader (ensemble) †
EpiReader (ensemble) †

AS Reader (+BookTest) † ‡
AS Reader (+BookTest,ensemble) † ‡

GA--
GA (update L(w))
GA (ﬁx L(w))
GA (+feature, update L(w))
GA (+feature, ﬁx L(w))

CNN

Daily Mail

CBT-NE

CBT-CN

Val

Test Val

Test Val

Test Val

Test

–
–

–
55.0
61.6
61.8
63.4
68.6
71.3
73.8
72.6
73.4
73.1
72.9
–
76.3

66.2
73.9
77.2
75.2
–

–
–

73.0
77.9
77.9
77.3
76.7

–
–

–
57.0
63.0
63.8
66.8
69.5
72.9
73.6
73.3
74.0
74.4
74.7
–
76.9

69.4
75.4
77.6
76.1
–

–
–

73.8
77.9
77.8
76.9
77.4

–
–

–
63.3
70.5
69.0
–
75.0
–
77.6
–
–
–
77.6
–
80.3

–
78.7
80.2
–
–

–
–

76.7
81.5
80.4
80.7
80.0

–
–

–
62.2
69.0
68.0
–
73.9
–
76.6
–
–
–
76.6
–
79.6

–
77.7
79.2
–
–

–
–

75.7
80.9
79.6
80.0
79.3

–
–

51.2
–
–
–
70.4
73.8
–
–
75.2
75.3
77.8
–
78.2
–

–
76.2
–
76.9
76.6

80.5
82.3

74.9
76.7
77.2
77.2
78.5

52.0
81.6

41.8
–
–
–
66.6
68.6
–
–
68.6
69.7
72.0
–
73.2
–

–
71.0
–
72.0
71.8

76.2
78.4

69.0
70.1
71.4
73.3
74.9

–
–

62.6
–
–
–
64.2
68.8
–
–
72.1
71.5
72.2
–
74.3
–

–
71.1
–
74.1
73.6

83.2
85.7

69.0
69.8
71.6
73.0
74.4

64.4
81.6

56.0
–
–
–
63.0
63.4
–
–
69.2
67.4
69.4
–
71.9
–

–
68.9
–
71.0
70.6

80.8
83.7

63.9
67.3
68.0
69.8
70.7

2 (top) shows the performance on WDW dataset
for three common choices – sum (x = d + q),
concatenate (x = d(cid:107)q) and multiply (x =
d (cid:12) q). Empirically we ﬁnd element-wise multipli-
cation does signiﬁcantly better than the other two,
which justiﬁes our motivation to “ﬁlter” out docu-
ment features which are irrelevant to the query.

that. This is a common trend in machine learn-
ing as model complexity is increased, however we
note that a multi-hop architecture is important to
achieve a high performance for this task, and pro-
vide further evidence for this in the next section.

4.4 Ablation Study for Model Components

At the bottom of Table 2 we show the effect of
varying the number of hops K of the GA Reader
on the ﬁnal performance. We note that for K = 1,
our model is equivalent to the AS Reader with-
out any GA modules. We see a steep and steady
rise in accuracy as the number of hops is increased
from K = 1 to 3, which remains constant beyond

Table 4 shows accuracy on WDW by removing
one component at a time. The steepest reduc-
tion is observed when we replace pretrained GloVe
vectors with those pretrained on the corpus itself.
GloVe vectors were trained on a large corpus of
about 6 billion tokens (Pennington et al., 2014),
and provide an important source of prior knowl-

Figure 2: Performance in accuracy with and without the Gated-Attention module over different training
sizes. p-values for an exact one-sided Mcnemar’s test are given inside the parentheses for each setting.

Table 4: Ablation study on WDW dataset, without using
the qe-comm feature and with ﬁxed L(w). Results marked
with † are cf Onishi et al. (2016).

Model

GA

GA--†

−char
−token-attentions (eq. 5)
−glove, +corpus

Accuracy

Val

Test

68.3
66.9
65.7
64.0

–

68.0
66.9
65.0
62.5

57

edge for the model. Note that the strongest base-
line on WDW, NSE (Munkhdalai & Yu, 2017b),
also uses pretrained GloVe vectors, hence the
comparison is fair in that respect. Next, we ob-
serve a substantial drop when removing token-
speciﬁc attentions over the query in the GA mod-
ule, which allow gating individual tokens in the
document only by parts of the query relevant to
that token rather than the overall query representa-
tion. Finally, removing the character embeddings,
which were only used for WDW and CBT, leads
to a reduction of about 1% in the performance.

4.5 Attention Visualization

To gain an insight into the reading process em-
ployed by the model we analyzed the attention dis-
tributions at intermediate layers of the reader. Fig-
ure 3 shows an example from the validation set of
WDW dataset (several more are in the Appendix).
In each ﬁgure, the left and middle plots visualize
attention over the query (equation 5) for candi-
dates in the document after layers 1 & 2 respec-
tively. The right plot shows attention over candi-

dates in the document of cloze placeholder (XXX)
in the query at the ﬁnal layer. The full document,
query and correct answer are shown at the bottom.
A generic pattern observed in these examples
is that in intermediate layers, candidates in the
document (shown along rows) tend to pick out
salient tokens in the query which provide clues
about the cloze, and in the ﬁnal layer the candi-
date with the highest match with these tokens is
selected as the answer. In Figure 3 there is a high
attention of the correct answer on financial
regulatory standards in the ﬁrst layer, and
on us president in the second layer. The in-
correct answer, in contrast, only attends to one of
these aspects, and hence receives a lower score in
the ﬁnal layer despite the n-gram overlap it has
with the cloze token in the query. Importantly, dif-
ferent layers tend to focus on different tokens in
the query, supporting the hypothesis that the multi-
hop architecture of GA Reader is able to combine
distinct pieces of information to answer the query.

5 Conclusion

We presented the Gated-Attention reader for an-
swering cloze-style questions over documents.
The GA reader features a novel multiplicative gat-
ing mechanism, combined with a multi-hop ar-
chitecture. Our model achieves the state-of-the-
art performance on several large-scale benchmark
datasets with more than 4% improvements over
competitive baselines. Our model design is backed
up by an ablation study showing statistically sig-
niﬁcant improvements of using Gated Attention
as information ﬁlters. We also showed empiri-
cally that multiplicative gating is superior to addi-

Figure 3: Layer-wise attention visualization of GA Reader trained on WDW-Strict. See text for details.

tion and concatenation operations for implement-
ing gated-attentions, though a theoretical justiﬁca-
tion remains part of future research goals. Anal-
ysis of document and query attentions in interme-
diate layers of the reader further reveals that the
model iteratively attends to different aspects of the
query to arrive at the ﬁnal answer. In this paper
we have focused on text comprehension, but we
believe that the Gated-Attention mechanism may
beneﬁt other tasks as well where multiple sources
of information interact.

Acknowledgments

This work was funded by NSF under CCF1414030
and Google Research.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473, 2014.

Ondrej Bajgar, Rudolf Kadlec, and Jan Kleindi-
enst.
Embracing data abundance: Booktest
dataset for reading comprehension. arXiv preprint
arXiv:1610.00956, 2016.

Danqi Chen, Jason Bolton, and Christopher D Man-
ning. A thorough examination of the cnn/daily mail
reading comprehension task. ACL, 2016.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger

Schwenk, and Yoshua Bengio. Learning phrase rep-
resentations using rnn encoder-decoder for statisti-
cal machine translation. ACL, 2015.

Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang,
Ting Liu, and Guoping Hu. Attention-over-attention
neural networks for reading comprehension. ACL,
2017.

Bhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick,
Michael Muehl, and William W Cohen. Tweet2vec:
Character-based distributed representations for so-
cial media. ACL, 2016.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. Teaching machines to
read and comprehend. In Advances in Neural Infor-
mation Processing Systems, pp. 1684–1692, 2015.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. The goldilocks principle: Reading chil-
dren’s books with explicit memory representations.
ICLR, 2016.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-
term memory. Neural computation, 9(8):1735–
1780, 1997.

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan
Kleindienst. Text understanding with the attention
sum reader network. ACL, 2016.

Diederik Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. ICLR, 2015.

Ryan Kiros, Richard Zemel, and Ruslan R Salakhutdi-
nov. A multiplicative model for learning distributed
text-based attribute representations. In Advances in
Neural Information Processing Systems, pp. 2348–
2356, 2014.

Sosuke Kobayashi, Ran Tian, Naoaki Okazaki, and
Kentaro Inui. Dynamic entity representations with
max-pooling improves machine reading. In NAACL-
HLT, 2016.

Ankit Kumar, Ozan Irsoy, Jonathan Su, James Brad-
bury, Robert English, Brian Pierce, Peter Ondruska,
Ishaan Gulrajani, and Richard Socher. Ask me any-
thing: Dynamic memory networks for natural lan-
guage processing. ICML, 2016.

Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying
Cao, Jie Zhou, and Wei Xu. Dataset and neu-
ral recurrent sequence labeling model for open-
domain factoid question answering. arXiv preprint
arXiv:1607.06275, 2016.

Jeff Mitchell and Mirella Lapata. Vector-based mod-
els of semantic composition. In ACL, pp. 236–244,
2008.

Volodymyr Mnih, Nicolas Heess, Alex Graves, et al.
Recurrent models of visual attention. In Advances in
Neural Information Processing Systems, pp. 2204–
2212, 2014.

Tsendsuren Munkhdalai and Hong Yu. Neural seman-

tic encoders. EACL, 2017a.

Tsendsuren Munkhdalai and Hong Yu. Reasoning with
memory augmented neural networks for language
comprehension. ICLR, 2017b.

Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gim-
pel, and David McAllester. Who did what: A large-
scale person-centered cloze dataset. EMNLP, 2016.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
On the difﬁculty of training recurrent neural net-
works. ICML (3), 28:1310–1318, 2013.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pp. 1532–
1543, 2014. URL http://www.aclweb.org/
anthology/D14-1162.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. Bidirectional attention ﬂow
for machine comprehension. ICLR, 2017.

Yelong Shen, Po-Sen Huang, Jianfeng Gao, and
Weizhu Chen. Reasonet: Learning to stop read-
arXiv preprint
ing in machine comprehension.
arXiv:1609.05284, 2016.

Alessandro Sordoni, Phillip Bachman, and Yoshua
Bengio. Iterative alternating neural attention for ma-
arXiv preprint arXiv:1606.02245,
chine reading.
2016.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
End-to-end memory networks. In Advances in Neu-
ral Information Processing Systems, pp. 2431–2439,
2015.

Theano Development Team.

Theano: A Python
framework for fast computation of mathematical ex-
arXiv e-prints, abs/1605.02688, May
pressions.
2016. URL http://arxiv.org/abs/1605.
02688.

Adam Trischler, Zheng Ye, Xingdi Yuan, and Kaheer
Suleman. Natural language comprehension with the
epireader. EMNLP, 2016.

Jason Weston, Sumit Chopra, and Antoine Bordes.

Memory networks. ICLR, 2015.

Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua
Bengio, and Ruslan Salakhutdinov. On multiplica-
tive integration with recurrent neural networks. Ad-
vances in Neural Information Processing Systems,
2016.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. Learning multi-relational seman-
tics using neural-embedding models. NIPS Work-
shop on Learning Semantics, 2014.

Zhilin Yang, Ruslan Salakhutdinov, and William Co-
hen. Multi-task cross-lingual sequence tagging from
scratch. arXiv preprint arXiv:1603.06270, 2016.

A Implementation Details

Our model was implemented using the Theano
(Theano Development Team, 2016) and Lasagne5
Python libraries. We used stochastic gradient de-
scent with ADAM updates for optimization, which
combines classical momentum and adaptive gradi-
ents (Kingma & Ba, 2015). The batch size was 32
and the initial learning rate was 5 × 10−4 which
was halved every epoch after the second epoch.
The same setting is applied to all models and
datasets. We also used gradient clipping with a
threshold of 10 to stabilize GRU training (Pascanu
et al., 2013). We set the number of layers K to be
3 for all experiments. The number of hidden units
for the character GRU was set to 50. The remain-
ing two hyperparameters—size of document and
query GRUs, and dropout rate—were tuned on the
validation set, and their optimal values are shown
in Table 6. In general, the optimal GRU size in-
creases and the dropout rate decreases as the cor-
pus size increases.

The word lookup table was initialized with 100d
GloVe vectors6 (Pennington et al., 2014) and OOV
tokens at test time were assigned unique random
vectors. We empirically observed that initializing
with pre-trained embeddings gives higher perfor-
mance compared to random initialization for all

5https://lasagne.readthedocs.io/en/latest/
6http://nlp.stanford.edu/projects/glove/

Table 5: Dataset statistics.

CNN Daily Mail CBT-NE CBT-CN WDW-Strict WDW-Relaxed

# train
# validation
# test
# vocab
max doc length

380,298
3,924
3,198
118,497
2,000

879,450
64,835
53,182
208,045
2,000

108,719
2,000
2,500
53,063
1,338

120,769
2,000
2,500
53,185
1,338

127,786
10,000
10,000
347,406
3,085

185,978
10,000
10,000
308,602
3,085

Table 6: Hyperparameter settings for each dataset. dim() indicates hidden state size of GRU.

Hyperparameter CNN Daily Mail CBT-NE CBT-CN WDW-Strict WDW-Relaxed

Dropout
←→
GRU∗)

dim(

0.2

256

0.1

256

0.4

128

0.4

128

0.3

128

0.3

128

datasets. Furthermore, for smaller datasets (WDW
and CBT) we found that ﬁxing these embeddings
to their pretrained values led to higher test perfor-
mance, possibly since it avoids overﬁtting. We do
not use the character composition model for CNN
and Daily Mail, since their entities (and hence can-
didate answers) are anonymized to generic tokens.
For other datasets the character lookup table was
randomly initialized with 25d vectors. All other
parameters were initialized to their default values
as speciﬁed in the Lasagne library.

B Attention Plots

Figure 4: Layer-wise attention visualization of GA Reader trained on WDW-Strict. See text for details.

Figure 5: Layer-wise attention visualization of GA Reader trained on WDW-Strict. See text for details.

Figure 6: Layer-wise attention visualization of GA Reader trained on WDW-Strict. See text for details.

Figure 7: Layer-wise attention visualization of GA Reader trained on WDW-Strict. See text for details.

