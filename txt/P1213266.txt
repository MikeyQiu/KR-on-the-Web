8
1
0
2
 
l
u
J
 
4
2
 
 
]

G
L
.
s
c
[
 
 
2
v
3
7
4
1
0
.
2
1
7
1
:
v
i
X
r
a

Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global

Thomas Laurent * 1 James H. von Brecht * 2

Abstract

We consider deep linear networks with arbitrary
convex differentiable loss. We provide a short
and elementary proof of the fact that all local min-
ima are global minima if the hidden layers are ei-
ther 1) at least as wide as the input layer, or 2) at
least as wide as the output layer. This result is the
strongest possible in the following sense: If the
loss is convex and Lipschitz but not differentiable
then deep linear networks can have sub-optimal
local minima.

1. Introduction

Deep linear networks (DLN) are neural networks that have
multiple hidden layers but have no nonlinearities between
layers. That is, for given data points {x(i)}N
i=1 the outputs
of such networks are computed via a series

ˆy(i) = WLWL−1 · · · W1x(i)

of matrix multiplications. Given a target y(i) for the ith
data point and a pairwise loss function ℓ(ˆy(i), y(i)), form-
ing the usual summation

tiable.

layer.

L(W1, . . . , WL) =

ℓ(ˆy(i), y(i))

(1)

1
N

N

i=1
X

then yields the total loss.

Such networks have few direct applications, but they fre-
quently appear as a class of toy models used to understand
the loss surfaces of deep neural networks (Saxe et al., 2014;
Kawaguchi, 2016; Lu & Kawaguchi, 2017; Hardt & Ma,
2017). For example, numerical experiments indicate that
DLNs exhibit some behavior that resembles the behavior of

*Equal contribution

1Department of Mathematics, Loy-
ola Marymount University, Los Angeles, CA 90045, USA
2Department of Mathematics and Statistics, California State Uni-
versity, Long Beach, Long Beach, CA 90840, USA. Correspon-
dence to: Thomas Laurent <tlaurent@lmu.edu>, James H. von
Brecht <james.vonbrecht@csulb.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

deep nonlinear networks during training (Saxe et al., 2014).
Results of this sort provide a small piece of evidence that
DLNs can provide a decent simpliﬁed model of more real-
istic networks with nonlinearities.

From an analytical point-of-view, the simplicity of DLNs
allows for a rigorous, in-depth study of their loss sur-
faces. These models typically employ a convex loss func-
tion ℓ(ˆy, y), and so with one layer (i.e. L = 1) the loss
L(W1) is convex and the resulting optimization problem
(1) has no sub-optimal local minimizers. With multiple lay-
ers (i.e. L ≥ 2) the loss L(W1, . . . , WL) is not longer
convex, and so the question of paramount interest concerns
whether this addition of depth and the subsequent loss of
convexity creates sub-optimal local minimizers.
Indeed,
most analytical treatments of DLNs focus on this question.

We resolve this question in full for arbitrary convex differ-
entiable loss functions. Speciﬁcally, we consider deep lin-
ear networks satisfying the two following hypotheses:

(i) The loss function ˆy 7→ ℓ(y, ˆy) is convex and differen-

(ii) The thinnest layer is either the input layer or the output

Many networks of interest satisfy both of these hypotheses.
The ﬁrst hypothesis (i) holds for nearly all network criteria,
such as the mean squared error loss, the logistic loss or the
cross entropy loss, that appear in applications. In a classiﬁ-
cation scenario, the second hypothesis (ii) holds whenever
each hidden layer has more neurons than the number of
classes. Thus both hypotheses are often satisﬁed when us-
ing a deep linear network (1) to model its nonlinear coun-
terpart. In any such situation we resolve the deep linear
problem in its entirety.
Theorem 1. If hypotheses (i) and (ii) hold then (1) has no
sub-optimal minimizers, i.e. any local minimum is global.

We provide a short, transparent proof of this result. It is
easily accessible to any reader with a basic understanding
of the singular value decomposition, and in particular, it
does not rely on any sophisticated machinery from either
optimization or linear algebra. Moreover, this theorem is
the strongest possible in the following sense —
Theorem 2. There exists a convex, Lipschitz but not dif-
ferentiable function ˆy 7→ ℓ(y, ˆy) for which (1) has sub-

Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global

optimal local minimizers.

as well as the analogous problem

In other words, we have a (perhaps surprising) hard limit
on how far “local equals global” results can reach; differen-
tiability of the loss is essential.

Many prior analytical treatments of DLNs focus on similar
questions. For instance, both (Baldi & Hornik, 1989) and
(Baldi & Lu, 2012) consider “deep” linear networks with
two layers (i.e. L = 2) and a mean squared error loss cri-
terion. They provide a “local equals global” result under
some relatively mild assumptions on the data and targets.
More recently, (Kawaguchi, 2016) proved that deep linear
networks with arbitrary number of layers and with mean
squared error loss do not have sub-optimal local minima
under certain structural assumptions on the data and tar-
gets. The follow-up (Lu & Kawaguchi, 2017) futher sim-
pliﬁes the proof of this result and weakens the structural
assumptions. Speciﬁcally, this result shows that the loss (1)
associated with a deep linear network has no sub-optimal
local minima provided all of assumptions

(i) The loss ℓ(ˆy(i), y(i)) = ky(i) − ˆy(i)k2 is the mean

squared error loss criterion;

(ii) The data matrix X = [x(1), . . . , x(N )] has full row

rank;

rank;

(iii) The target matrix Y = [y(1), . . . , y(N )] has full row

are satisﬁed. Compared to our result, (Lu & Kawaguchi,
2017) therefore allows for the hidden layers of the network
to be thinner than the input and output layers. However, our
result applies to network equipped with any differentiable
convex loss (in fact any differentiable loss L for which ﬁrst-
order optimality implies global optimality) and we do not
require any assumption on the data and targets. Our proof
is also shorter and much more elementary by comparison.

2. Proof of Theorem 1

Theorem 1 follows as a simple consequence of a more gen-
eral theorem concerning real-valued functions that take as
input a product of matrices. That is, we view the deep lin-
ear problem as a speciﬁc instance of the following more
general problem. Let Mm×n denote the space of m × n
real matrices, and let f : MdL×d0 → R denote any differ-
entiable function that takes dL × d0 matrices as input. For
any such function we may then consider both the single-
layer optimization

(P1)

(

Minimize f (A)
over all A in MdL×d0

Minimize f (WLWL−1 · · · W1)
over all L-tuples (W1, . . . , WL)
in Md1×d0 × · · · × MdL×dL−1

(P2) 




that corresponds to a multi-layer deep linear optimization.
In other words, in (P2) we consider the task of optimizing
f over those matrices A ∈ MdL×d0 that can be realized by
an L-fold product

A = WLWL−1 · · · W1

Wℓ ∈ Mdℓ×dℓ−1

(2)

of matrices. We may then ask how the parametrization (2)
of A as a product of matrices affects the minimization of
f, or in other words, whether the problems (P1) and (P2)
have similar structure. At heart, the use of DLNs to model
nonlinear neural networks centers around this question.

Any notion of structural similarity between (P1) and (P2)
should require that their global minima coincide. As a ma-
trix of the form (2) has rank at most min{d0, . . . , dL}, we
must impose the structural requirement

min{d1, . . . , dL−1} ≥ min{dL, d0}

(3)

in order to guarantee that (2) does, in fact, generate the full
space of dL × d0 matrices. Under this assumption we shall
prove the following quite general theorem.
Theorem 3. Assume that f (A) is any differentiable func-
tion and that the structural condition (3) holds. Then at
ˆW1, . . . , ˆWL
any local minimizer
of (P2) the optimality
condition

(cid:0)

(cid:1)

∇f

ˆA

= 0

ˆA := ˆWL ˆWL−1 · · · ˆW1

(cid:0)
is satisﬁed.

(cid:1)

Theorem 1 follows as a simple consequence of this theorem.
The ﬁrst hypothesis (i) of theorem 1 shows that the total
loss (1) takes the form

L(W1, . . . , WL) = f (WL · · · W1)

for f (A) some convex and differentiable function. The
structural hypothesis (3) is equivalent to the second hypoth-
esis (ii) of theorem 1, and so we can directly apply theorem
of L
3 to conclude that a local minimum
corresponds to a critical point ˆA = ˆWL · · · ˆW1 of f (A),
and since f (A) is convex, this critical point is necessarily a
global minimum.

ˆW1, . . . , ˆWL

(cid:0)

(cid:1)

Before turning to the proof of theorem 3 we recall a bit of
notation and provide a calculus lemma. Let

hA, Bifro := Tr(AT B) =

AijBij

and

kAk2

fro := hA, Aifro

i
X

j
X

Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global

denote the Frobenius dot product and the Frobenius norm,
respectively. Also, recall that for a differentiable function
φ : Mm×n 7→ R its gradient ∇φ(A) ∈ Mm×n is the
unique matrix so that the equality

φ(A + H) = φ(A) + h∇φ(A), Hifro + o (kHkfro)

(4)

holds. If F (W1, . . . , WL) := f (WL · · · W1) denotes the
objective of interest in (P2) the following lemma gives the
partial derivatives of F as a function of its arguments.

Lemma 1. The partial derivatives of F are given by

∇W1 F (W1, . . . , WL) = W T
∇Wk F (W1, . . . , WL) = W T
∇WL F (W1, . . . , WL) = ∇f

2,+∇f
A
k+1,+∇f (A)W T
(cid:0)
(cid:1)
W T
L−1,−,

A

,

k−1,−,

where A stands for the full product A := WL · · · W1 and
Wk,+, Wk,− are the truncated products

(cid:0)

(cid:1)

structural assumption that dk ≥ d0, throughout the remain-
der of the proof.

of F and de-
Consider any local minimizer
note by ˆA, ˆWk,+ and ˆWk,− the corresponding full and trun-
(cid:0)
cated products (c.f. (5)). By deﬁnition of a local minimizer
there exists some ε0 > 0 so that

ˆW1, . . . , ˆWL

(cid:1)

F (W1, . . . , WL) ≥ F ( ˆW1, . . . , ˆWL) = f

ˆA

(6)

whenever the family of inequalities

(cid:0)

(cid:1)

kWℓ − ˆWℓkfro ≤ ε0

for all 1 ≤ ℓ ≤ L

all hold. Moreover, lemma 1 yields

(i) 0 = ˆW T
(ii) 0 = ˆW T

(iii) 0 = ∇f

ˆA

,
2,+∇f
ˆA
k+1,+∇f
(cid:1)
(cid:0)
ˆW T
ˆA
L−1,−.
(cid:1)
(cid:0)

ˆW T

k−1,− ∀ 2 ≤ k ≤ L − 1,

(7)

Wk,+ := WL · · · Wk,
Wk,− := Wk · · · W1.

(5)

(cid:0)

(cid:1)

since all partial derivatives must vanish at a local minimum.
If ˆWL−1,− has a trivial kernel, i.e. ker( ˆWL−1,−) = {0},
then the theorem follows easily. The critical point condi-
tion (7) part (iii) implies

Proof. The deﬁnition (4) implies

F (W1, . . . , Wk−1, Wk + H, Wk+1, . . . , WL)
= f

A + Wk+1,+HWk−1,−

= f (A) + h∇f (A), Wk+1,+HWk−1,−ifro + o

kHkfro

.

(cid:1)

(cid:0)

(cid:1)
Using the cyclic property Tr(ABC) = Tr(CAB) of the
trace then gives

(cid:0)

h∇f (A) , Wk+1,+HWk−1,− ifro

= Tr

∇f (A)T Wk+1,+HWk−1,−
Wk−1,−∇f (A)T Wk+1,+H
= Tr
(cid:0)
k+1,+∇f (A)W T
= h W T
(cid:0)

(cid:1)
k−1,− , H ifro
(cid:1)

which, in light of (4), gives the desired formula for ∇Wk F .
The formulas for ∇W1 F and ∇WL F are obtained similarly.

Proof of Theorem 3: To prove theorem 3 it sufﬁces to as-
sume that dL ≥ d0 without loss of generality. This follows
from the simple observation that

ˆWL−1,−∇f

ˆA

T

= 0,

and since ˆWL−1,− has a trivial kernel
∇f

(cid:1)
ˆWL ˆWL−1 · · · ˆW1

= ∇f

ˆA

(cid:0)

= 0 as desired.

this implies

(cid:0)

(cid:1)

(cid:1)

(cid:0)

The remainder of the proof concerns the case that ˆWL−1,−
has a nontrivial kernel. The main idea is to use this non-
trivial kernel to construct a family of inﬁnitesimal perturba-
that leaves the
tions of the local minimizer
overall product ˆWL · · · ˆW1 unchanged. In other words, the
(cid:0)
family of perturbations

ˆW1, . . . , ˆWL

˜W1, . . . , ˜WL

(cid:1)
satisfy

(cid:0)

k ˜Wℓ − ˆWℓkfro ≤ ǫ0/2 ∀ℓ = 1, . . . , L,
˜WL ˜WL−1 · · · ˜W1 = ˆWL ˆWL−1 · · · ˆW1.

(cid:1)

(8)

(9)

Any such perturbation also deﬁnes a local minimizer.

˜W1, . . . , ˜WL
Claim 1. Any tuple of matrices
(8) and (9) is necessarily a local minimizer F .

satisfying

(cid:0)

(cid:1)

Proof. For any matrix Wℓ satisfying kWℓ− ˜Wℓkfro ≤ ε0/2,
inequality (8) implies that

g

A

:= f

AT

kWℓ − ˆWℓkfro ≤ kWℓ − ˜Wℓkfro + k ˜Wℓ − ˆWℓkfro ≤ ε0

(cid:0)

(cid:1)

(cid:0)

(cid:1)

W1, . . . , WL

deﬁnes a differentiable function of d0 × dL matrices for
f (A) any differentiable function of dL × d0 matrices.
As a point
deﬁnes a local minimum of
1 , . . . , W T
W T
if and only if
de-
WLWL−1 · · · W1
f
L
ﬁnes a minimum of g
, the theorem for the
V1 · · · VL−1VL
(cid:0)
(cid:1)
case dL < d0 follows by appealing to its dL ≥ d0 instance.
(cid:1)
It therefore sufﬁces to assume that dL ≥ d0, and by the

(cid:1)

(cid:0)

(cid:0)

(cid:1)

(cid:0)

Equality (9) combined to (6) then leads to

F

W1, . . . , WL

≥ f

ˆA

= f ( ˆWL · · · ˆW1)

(cid:0)

= f ( ˜WL · · · ˜W1) = F
(cid:1)
(cid:1)

(cid:0)

˜W1, . . . , ˜WL

for any Wℓ with kWℓ − ˜Wℓkfro ≤ ε0/2 and so the point
( ˜W1, . . . , ˜WL) deﬁnes a local minimum.

(cid:0)

(cid:1)

Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global

The construction of such perturbations requires a prelimi-
nary observation and then an appeal to the singular value
decomposition. Due to the deﬁnition of ˆWk,− it follows
that ker( ˆWk+1,−) = ker( ˆWk+1 ˆWk,−) ⊇ ker( ˆWk,−), and
so the chain of inclusions

ker( ˆW1,−) ⊆ ker( ˆW2,−) ⊆ · · · ⊆ ker( ˆWL−1,−)

(10)

holds. Since ˆWL−1,− has a nontrivial kernel, the chain of
inclusions (10) implies that there exists k∗ ∈ {1, . . . , L−1}
such that

ker( ˆWk,−) = {0} if k < k∗
ker( ˆWk,−) 6= {0} if k ≥ k∗

(11)

(12)

In other words, ˆWk∗,− is the ﬁrst matrix appearing in (10)
that has a nontrivial kernel.

The structural requirement (3) and the assumption that
dL ≥ d0 imply that dk ≥ d0 for all k, and so the ma-
trix ˆWk,− ∈ Mdk×d0 has more rows than columns. As a
consequence its full singular value decomposition

ˆWk,− = ˆUk ˆΣk ˆV T
k

(13)

has the shape depicted in ﬁgure 1. The matrices ˆUk ∈
Mdk×dk and ˆVk ∈ Md0×d0 are orthogonal, whereas ˆΣk ∈
Mdk×d0 is a diagonal matrix containing the singular values
of ˆWk,− in descending order. From (12) ˆWk,− has a non-
trivial kernel for all k ≥ k∗, and so in particular its least
singular value is zero. In particular, the (d0, d0) entry of
ˆΣk vanishes if k ≥ k∗. Let ˆuk denote the corresponding
0 column of ˆUk, which exists since dk ≥ d0.
dth
Claim 2. Let wk∗+1, . . . , wL denote any collection of vec-
tors and δk∗+1, . . . , δL any collection of scalars satisfying

wk ∈ Rdk ,
0 ≤ δk ≤ ǫ0/2

kwkk2 = 1 and

for all k∗ + 1 ≤ k ≤ L. Then the tuple of matrices
( ˜W1, . . . , ˜WL) deﬁned by

˜Wk :=

ˆWk
ˆWk + δkwk ˆuT

k−1

(

if 1 ≤ k ≤ k∗
else,

satisﬁes (8) and (9).

(14)
(15)

(16)

Proof. Inequality (8) follows from the fact that

k ˜Wk − ˆWkkfro = δkkwk ˆuT
k−1kfro = δkkwkk2kˆuk−1k2
for all k > k∗, together with the fact that ˆuk−1 and wk are
unit vectors and that 0 ≤ δk ≤ ǫ0/2.
To prove (9) let ˜Wk,− = ˜Wk · · · ˜W1 and ˆWk,− =
ˆWk · · · ˆW1 denote the truncated products of the matrices

d0

dth
0 column

dk

=

0

ˆWk,−

ˆΣk

ˆV T
k

ˆuk

ˆUk

Figure1. Full singular value decomposition of ˆWk,− ∈ Mdk×d0 .
If k ≥ k∗ then ˆWk,− does not have full rank and so the (d0, d0)
entry of ˆΣk is 0. The dth

0 column of ˆUk exists since dk ≥ d0.

˜Wk and ˆWk. The equality ˜Wk∗,− = ˆWk∗,− is immediate
from the deﬁnition (16). The equality (9) will then follow
from showing that

˜Wk,− = ˆWk,− for all k∗ ≤ k ≤ L.

(17)

Proceeding by induction, assume that ˜Wk,− = ˆWk,− for a
given k ≥ k∗. Then

˜Wk+1,− = ˜Wk+1 ˜Wk,−

= ˜Wk+1 ˆWk,− (induction hypothesis)

=

ˆWk+1 + δk+1wk+1 ˆuT
k
(cid:17)
ˆWk,−
= ˆWk+1,− + δk+1wk+1uT
k

ˆWk,−

(cid:16)

The second term of the last line vanishes, since

uT
k

ˆWk,− = uT
k

ˆUk ˆΣk ˆV T

k = eT
d0

ˆΣk ˆV T

k = 0

with ed0 ∈ Rdk the dth
0 standard basis vector. The second
equality comes from the fact that the columns of ˆUk are
orthonormal, and the last equality comes from the fact that
0 row of ˆΣk∗ vanishes. Thus (17)
eT
d0Σk∗ = 0 since the dth
holds, and so (9) holds as well.

the perturbation
deﬁned by (16) is a local minimizer of F .

Claims 1 and claim 2 show that
˜W1, . . . , ˜WL
The critical point conditions
(cid:1)
(cid:0)
(i) 0 = ˜W T
2,+∇f
(ii) 0 = ˜W T
k+1,+∇f
(cid:0)
˜W T
˜A

(iii) 0 = ∇f

,
˜A
(cid:1)

˜W T

˜A

(cid:0)
(cid:1)
L−1,−

k−1,− ∀ 2 ≤ k ≤ L − 1,

therefore hold as well for all choices of wk∗+1, . . . , wL
and δk∗+1, . . . , δL satisfying (14) and (15).

(cid:0)

(cid:1)

The proof concludes by appealing to this family of critical
point relations. If k∗ > 1 the transpose of condition (ii)
gives

ˆWk∗−1,−∇f

ˆA

T ˜Wk∗+1,+ = 0

(18)

(cid:0)

(cid:1)

Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global

since the equalities ˜Wk∗−1,− = ˆWk∗−1,− (c.f. (16)) and
˜A = ˜WL · · · ˜W1 = ˆWL · · · ˆW1 = ˆA (c.f. (9)) both hold.
But ker( ˆWk∗−1,−) = {0} by deﬁnition of k∗ (c.f. (11)),
and so

∇f

ˆA

T ˜WL · · · ˜Wk∗+1 = 0.

(19)

(cid:0)

(cid:1)

must hold as well. If k∗ = 1 then (19) follows trivially
from the critical point condition (i). Thus (19) holds for
all choices of wk∗+1, . . . , wL and δk∗+1, . . . , δL satisfying
(14) and (15). First choose δk∗+1 = 0 so that ˜Wk∗+1 =
ˆWk∗+1 and apply (19) to ﬁnd

put it otherwise, if L ≥ 3 the problem (P2) always has
saddle-points even though all local optima are global.

Second, the assumption that f (A) is differentiable is neces-
sary as well. More speciﬁcally, a function of the form

F (W1, . . . , WL) := f (WL · · · W1)

can have sub-optimal local minima if f (A) is convex and
globally Lipschitz but is not differentiable. A simple exam-
ple demonstrates this, and therefore proves theorem 2. For
instance, consider the bi-variate convex function

∇f

ˆA

T ˜WL · · · ˜Wk∗+2 ˆWk∗+1 = 0.

(20)

f (x, y) := |x|+(1−y)+−1,

(y)+ := max{y, 0}, (22)

Then take any δk∗+1 > 0 and substract (20) from (19) to
get

(cid:0)

(cid:1)

which is clearly globally Lipschitz but not differentiable.
The set

1
δk∗+1

∇f

ˆA

T ˜WL · · · ˜Wk∗+2

= ∇f

ˆA

(cid:1)

(cid:0)
T ˜WL · · · ˜Wk∗+2

˜Wk∗+1 − ˆWk∗+1
(cid:16)

(cid:17)

wk∗+1 ˆuT
k∗

= 0

(cid:0)

(cid:1)

for wk∗+1 an arbitrary vector with unit length. Right mul-
tiplying the last equality by ˆuk∗ and using the fact that
(wk∗+1 ˆuT

k∗ ˆuk∗ = wk∗+1 shows

k∗ )ˆuk∗ = wk∗+1 ˆuT

(cid:1)

(cid:0)

∇f

ˆA

T ˜WL · · · ˜Wk∗+2wk∗+1 = 0

(21)

for all choices of wk∗+1 with unit length. Thus (21) implies

(cid:0)

(cid:1)

arg min f := {(x, y) ∈ R2 : x = 0, y ≥ 1}

furnishes its global minimizers while fopt = −1 gives the
optimal value. For this function even a two layer deep lin-
ear problem

F

W1, W2) := f (W2W1) W2 ∈ R2, W1 ∈ R

has sub-optimal local minimizers; the point

(cid:0)

( ˆW1, ˆW2) =

0,

1
0
(cid:20)

(cid:18)

(cid:21)(cid:19)

(23)

∇f

ˆA

T ˜WL · · · ˜Wk∗+2 = 0

provides an example of a sub-optimal solution. The set of
all possible points in R2

for all choices of wk∗+2, . . . , wL and δk∗+2, . . . , δL satis-
fying (14) and (15). The claim

(cid:0)

(cid:1)

N ( ˆW1, ˆW2) :=

∇f

ˆA

= 0

then follows by induction.

(cid:0)

(cid:1)

3. Concluding Remarks

Theorem 3 provides the mathematical basis for our anal-
ysis of deep linear problems. We therefore conclude by
discussing its limits.

First, theorem 3 fails if we refer to critical points rather than
local minimizers. To see this, it sufﬁces to observe that the
critical point conditions for problem (P2),

(i) 0 = ˆW T
(ii) 0 = ˆW T

(iii) 0 = ∇f

,
ˆA
(cid:1)

ˆA

2,+∇f
k+1,+∇f
(cid:0)
ˆW T
ˆA

(cid:0)
(cid:1)
L−1,−

ˆW T

k−1,− ∀ 2 ≤ k ≤ L − 1,

(cid:0)

(cid:1)

where ˆWk,+ := ˆWL · · · ˆWk+1 and ˆWk,− := ˆWk−1 · · · ˆW1,
clearly hold if L ≥ 3 and all of the ˆWℓ vanish. In other
words, the collection of zero matrices always deﬁnes a crit-
need not vanish. To
ical point for (P2) but clearly ∇f

0

(cid:0)

(cid:1)

W2W1 : kW2 − ˆW2k ≤

, kW1 − ˆW1k ≤

1
4

1
4

(cid:27)

(cid:26)

generated by a 1/4-neighborhood of the optimum (23) lies
in the two-sided, truncated cone

N ( ˆW1, ˆW2) ⊂

(x, y) ∈ R2 : |x| ≤

, |y| ≤

|x|

,

1
2

1
2

(cid:27)

(cid:26)

and so if we let x ∈ R denote the ﬁrst component of the
product W2W1 then the inequality

f (W2W1) ≥

|x| ≥ 0 = f ( ˆW2 ˆW1)

1
2

holds on N ( ˆW1, ˆW2) and so ( ˆW1, ˆW2) is a sub-optimal
local minimizer. Moreover, the minimizer ( ˆW1, ˆW2) is a
strict local minimizer in the only sense in which strict opti-
mality can hold for a deep linear problem. Speciﬁcally, the
strict inequality

f (W2W1) > f ( ˆW2 ˆW1)

(24)

holds on N ( ˆW1, ˆW2) unless W2W1 = ˆW2 ˆW1 = 0; in the
latter case (W1, W2) and ( ˆW1, ˆW2) parametrize the same

Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global

Lu, H. and Kawaguchi, K. Depth creates no bad local min-

ima. arXiv preprint arXiv:1702.08580, 2017.

Saxe, A. M., McClelland, J. L., and Ganguli, S. Exact
solutions to the nonlinear dynamics of learning in deep
linear neural networks. In International Conference on
Learning Representations, 2014.

point and so their objectives must coincide. We may iden-
tify the underlying issue easily. The proof of theorem 3
requires a single-valued derivative ∇f ( ˆA) at a local opti-
mum, but with f (x, y) as in (22) its subdifferential

∂f (0) = {(x, y) ∈ R2 : −1 ≤ x ≤ 1, y = 0}

is multi-valued at the sub-optimal local minimum (23). In
other words, if a globally convex function f (A) induces
sub-optimal local minima in the corresponding deep linear
problem then ∇f ( ˆA) cannot exist at any such sub-optimal
solution (assuming the structural condition, of course).

Third, the structural hypothesis

dℓ ≥ min{dL, d0}

for all ℓ ∈ {1, . . . , L}

is necessary for theorem 3 to hold as well.
min{d0, dL} for some ℓ the parametrization

If dℓ <

A = WL · · · W1

cannot recover full rank matrices. Let f (A) denote any
function where ∇f vanishes only at full rank matrices.
Then

∇f

WL · · · W1

6= 0

at all critical points of (P2), and so theorem 3 fails.

(cid:1)

(cid:0)

Finally, if we do not require convexity of f (A) then it is
not true, in general, that local minima of (P2) correspond
to minima of the original problem. The functions

f (x, y) = x2 − y2

F (W1, W2) = f (W2W1)

and the minimizer (23) illustrate this point. While the ori-
gin is clearly a saddle point of the one layer problem, the
argument leading to (24) shows that (23) is a local mini-
mizer for the deep linear problem. So in the absence of ad-
ditional structural assumptions on f (A), we may infer that
a minimizer of the deep linear problem satisﬁes ﬁrst-order
optimality for the original problem, but nothing more.

References

Baldi, P. and Hornik, K. Neural networks and principal
component analysis: Learning from examples without
local minima. Neural networks, 2(1):53–58, 1989.

Baldi, P. and Lu, Z. Complex-valued autoencoders. Neural

Networks, 33:136–147, 2012.

Hardt, M. and Ma, T. Identity matters in deep learning. In
International Conference on Learning Representations,
2017.

Kawaguchi, K. Deep learning without poor local minima.
In Advances in Neural Information Processing Systems,
pp. 586–594, 2016.

