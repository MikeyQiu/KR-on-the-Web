7
1
0
2
 
t
c
O
 
8
 
 
]
L
C
.
s
c
[
 
 
2
v
6
8
6
9
0
.
9
0
7
1
:
v
i
X
r
a

Application of a Hybrid Bi-LSTM-CRF model to
the task of Russian Named Entity Recognition

Anh L. T.1,2, Arkhipov M. Y.1, Burtsev M. S.1

1 Neural Networks and Deep Learning Lab, Moscow Institute of Physics and
Technology, Russia
{burtcev.ms, arkhipov.mu}@mipt.ru
2 Faculty of Information Technology, Vietnam Maritime University, Viet Nam
anhlt@vimaru.edu.vn

Abstract. Named Entity Recognition (NER) is one of the most com-
mon tasks of the natural language processing. The purpose of NER is
to ﬁnd and classify tokens in text documents into predeﬁned categories
called tags, such as person names, quantity expressions, percentage ex-
pressions, names of locations, organizations, as well as expression of time,
currency and others. Although there is a number of approaches have been
proposed for this task in Russian language, it still has a substantial po-
tential for the better solutions. In this work, we studied several deep
neural network models starting from vanilla Bi-directional Long Short
Term Memory (Bi-LSTM) then supplementing it with Conditional Ran-
dom Fields (CRF) as well as highway networks and ﬁnally adding exter-
nal word embeddings. All models were evaluated across three datasets:
Gareev’s dataset, Person-1000 and FactRuEval-2016. We found that ex-
tension of Bi-LSTM model with CRF signiﬁcantly increased the quality
of predictions. Encoding input tokens with external word embeddings
reduced training time and allowed to achieve state of the art for the
Russian NER task.

Keywords: NER · Bi-LSTM · CRF

1

Introduction

There are two main approaches to address the named entity recognition (NER)
problem [1]. The ﬁrst one is based on handcrafted rules, and the other relies on
statistical learning. The rule based methods are primarily focused on engineer-
ing a grammar and syntactic extraction of patterns related to the structure of
language. In this case, a laborious tagging of a large amount of examples is not
required. The downsides of ﬁxed rules are poor ability to generalize and inabil-
ity to learn from examples. As a result, this type of NER systems is costly to
develop and maintain. Learning based systems automatically extract patterns
relevant to the NER task from the training set of examples, so they don’t require
deep language speciﬁc knowledge. This makes possible to apply the same NER
system to diﬀerent languages without signiﬁcant changes in architecture.

NER task can be considered as a sequence labeling problem. At the moment
one of the most common methods to address problems with sequential structure
is Recurrent Neural Networks (RNNs) due their ability to store in memory and
relate to each other diﬀerent parts of a sequence. Thus, RNNs is a natural choice
to deal with the NER problem. Up to now, a series of neural models were sug-
gested for NER. To our knowledge on the moment of writing this article the best
results for a number of languages such as English, German, Dutch and Span-
ish were achieved with a hybrid model combining bi-directional long short-term
memory network with conditional random ﬁelds (Bi-LSTM + CRF) [2]. In our
study we extended the original work by applying Bi-LSTM + CRF model to
NER task in Russian language. We also implemented and experimented with a
series of extensions of the NeuroNER model [3]. NeuroNER is a diﬀerent imple-
mentation of the same Bi-LSTM + CRF model. However, the realizations of the
models might diﬀer in such details as initialization and LSTM cell structure. To
reduce training time and improve results, we used the FastText3 model trained
on Lenta corpus4 to obtain external word embeddings. We studied the following
models:

– Bi-LSTM (char and word);
– Bi-LSTM (char and word) + CRF;
– Bi-LSTM (char and word) + CRF + external word embeddings;
– Default NeuroNER + char level highway network;
– Default NeuroNER + word level highway Bi-LSTM;
– Default NeuroNER + char level highway network + word level highway Bi-

LSTM.

To test all models we used three datasets:

– Gareev’s dataset [4];
– FactRuEval 20165;
– Persons-1000 [20].

Our study shows that Bi-LSTM + CRF + external word embeddings model
achieves state-of-the-art results for Russian NER task.

2 Neuronal NER Models

In this section we brieﬂy outline fundamental concepts of recurrent neural net-
works such as LSTM and Bi-LSTM models. We also describe a hybrid architec-
ture which combines Bi-LSTM with a CRF layer for NER task as well as some
extensions of this baseline architecture.
3 An open-source library for learning text representations and text classiﬁers. URL:

https://fasttext.cc/

4 A Russian public corpus for some tasks of natural

language processing. URL:

https://github.com/yutkin/lenta.ru-news-dataset

5 The dataset for NER and Fact Extraction task given at The International Conference

on Computational Linguistics and Intellectual Technologies - Moscow 2016

2.1 Long Short-Term Memory Recurrent Neural Networks

Recurrent neural networks have been employed to tackle a variety of tasks includ-
ing natural language processing problems due to its ability to use the previous
information from a sequence for calculation of current output. However, it was
found [10] that in spite theoretical possibility to learn a long-term dependency
in practice RNN models don’t perform as expected and suﬀer from gradient
descent issues. For this reason, a special architecture of RNN called Long Short-
Term Memory (LSTM) has been developed to deal with the vanishing gradient
problem [11]. LSTM replaces hidden units in RNN architecture with units called
memory blocks which contain 4 components: input gate, output gate, forget gate
and memory cell. Formulas for these components are listed below:

it = σ(Wixxt + Wihht−1 + bi),
ft = σ(Wf xxt + Wf hht−1 + bf ),
cn = g(Wcxxt + Wchht−1 + bc),
ct = ft ◦ ct−1 + it ◦ cn,
ht = ot ◦ g(ct),
ot = σ(Woxxt + Wohht−1 + bo),

(1)
(2)

(3)

(4)
(5)

(6)

where σ, g denote the sigmoid and tanh functions, respectively; ◦ is an element-
wise product; W terms denotes weight matrices; b are bias vectors; and i, f , o,
c denote input gate, forget gate, output gate and cell activation vectors, respec-
tively.

2.2 Bi-LSTM

Correct recognition of named entity in a sentence depends on the context of the
word. Both preceding and following words matter to predict a tag. Bi-directional
recurrent neuronal networks [12] were designed to encode every element in a
sequence taking into account left and right contexts which makes it one of the
best choices for NER task. Bi-directional model calculation consists of two steps:
(1) the forward layer computes representation of the left context, and (2) the
backward layer computes representation of the right context. Outputs of these
steps are then concatenated to produce a complete representation of an element
of the input sequence. Bi-directional LSTM encoders have been demonstrated to
be useful in many NLP tasks such as machine translation, question answering,
and especially for NER problem.

2.3 CRF model for NER task

Conditional Random Field is a probabilistic model for structured prediction
which has been successfully applied in variety of ﬁelds, such as computer vision,
bioinformatics, natural language processing. CRF can be used independently to
solve NER task ([13], [15]).

The CRF model is trained to predict a vector y = {y0, y1, .., yT } of tags given
a sentence x = {x0, x1, .., xT }. To do this, a conditional probability is computed:

p(y|x) =

eScore(x,y)
′ eScore(x,y

,

′

)

Py

where Score is computed by the formula below [2]:

Score(x, y) =

Ayi,yi+1 +

Pi,yi ,

T

X
i=0

T

X
i=1

(7)

(8)

where Ayi,yi+1 denotes the emission probability which represents the score of
transition from tag i to tag j, Pi,j is transition probability which represents the
score of the jth tag of the word ith.

In the training stage, log probability of correct tag sequence log(p(y|x)) is

maximized.

2.4 Combined Bi-LSTM and CRF model

Russian is a morphologically and grammatically rich language. Thus, we ex-
pected that a combination of CRF model with a Bi-LSTM neural network en-
coding [2] should increase the accuracy of the tagging decisions. The architecture
of the model is presented on the ﬁgure 1 .

In the combined model characters of each word in a sentence are fed into a
Bi-LSTM network in order to capture character-level features of words. Then
these character-level vector representations are concatenated with word embed-
ding vectors and fed into another Bi-LSTM network. This network calculates a
sequence of scores that represent likelihoods of tags for each word in the sen-
tence. To improve accuracy of the prediction a CRF layer is trained to enforce
constraints dependent on the order of tags. For example, in the IOB scheme (I –
Inside, O – Other, B – Begin) tag I never appears at the beginning of a sentence,
or O I B O is an invalid sequence of tags.

Full set of parameters for this model consists of parameters of Bi-LSTM
layers (weight matrices, biases, word embedding matrix) and transition matrix
of CRF layer. All these parameters are tuned during training stage by back
propagation algorithm with stochastic gradient descent. Dropout is applied to
avoid over-ﬁtting and improve the system performance.

2.5 Neuro NER Extensions

NeuroNER is an open-source software package for solving NER tasks. The neural
network architecture of NeuroNER is similar to the architecture proposed in the
previous section.

Inspired by success of character aware networks approach [22] we extended
NeuroNER model with a highway layer on top of the Bi-LSTM character embed-
ding layer. This extension is depicted on ﬁgure 2. Dense layer makes character

Fig. 1. The architecture of Bi-LSTM neural network for solving NER task. Here, xi is a
representation of word in a sequence. It is fed into character and word level embedding
blocks. Then character and word level representations are concatenated into ci. Bi-
LSTM performs conditioning of the concatenated representations on the left and right
contexts. Finally CRF layers provide output tag predictions yi.

embedding network deeper. The carry gate presented by sigmoid layer provides
a possibility to choose between dense and shortcut connections dynamically. A
highway network can be described by the following equation:

y = H(x, WH) · G(x, WG) + x · (1 − G(x, WG))

(9)

where x is the input of the network, H(x, WH) is the processing function,
G(x, WG) is the gating function. The dimensionality of x, y, G(x, WG), and
H(x, WH) must be the same.

Another extension of NeuroNER we implemented is a Bi-LSTM highway
network [23]. The architecture of this network is quite similar to the character-
aware highway network. However, the carry gate is conditioned on the input of
the LSTM cell. The gate provides an ability to dynamically balance between
raw embeddings and context dependent LSTM representation of the input. The
scheme of our implementation of the highway LSTM is depicted in ﬁgure 3.

3 Experiments

3.1 Datasets

Currently, there are a few Russian datasets created for the purpose of developing
and testing NER systems. We trained and evaluated models on the three Russian
datasets:

– Dataset received from Gareev et al. [4] contains 97 documents collected from
ten top cited ”Business” feeds in Yandex ”News” web directory. IOB tagging

Fig. 2. Highway network on top of the character embedding network. The Dense layer
serves for compute higher level representations of the input. The sigmoid layer computes
gate values. This values are used to dynamically balance between high and low level
representations. Block (1-) subtracts input from 1, and block (x) perform multiplication
of the inputs.

Fig. 3. Highway LSTM network. Here sigmoid gate layer is used to dynamically balance
between input and output of the Bi-LSTM layers. The gating applied to the each
direction separately.

scheme is used in this data sets, and entity types are Person, Organization,
Other.

– The FactRuEval 2016 corpus [18] contains news and analytical texts in
Russian. Sources of the dataset are Private Correspondent6 web site and
Wikinews7. Topics of the texts are social and political. Tagging scheme is
IOB.

– Person-1000 [20] is a Russian news corpus with marked up person named
entities. This corpus contains materials from the Russian on line news ser-
vices.

Statistics on these datasets are provided in the table 1.

Table 1. Statistics on datasets

Datasets

Tokens Words and numbers Persons Organizations Locations

FactRuEval 2016 90322
Gareev’s dataset 44326
284221

Persons-1000

73807
35116
224446

2087
486
10600

1181
1317
-

2686
-
-

3.2 External Word Embedding

N ews and Lenta are two external word embeddings we used to initialize lookup
table for the training step.

N ews8 is a Russian word embeddings introduced by Kutuzov, et al. [14].
Corpus for this word embedding is a set of Russian news (from September 2013
until November 2016). Here are more details about news:

– Corpus size: near 5 billion words
– Vocabulary size: 194058
– Frequency threshold: 200
– Algorithm: Continuous Bag of Words
– Vector size: 300

Lenta is a publicly available corpus of unannotated Russian news. This cor-
pus consists of 635000 news from Russian online news resource lenta.ru. The size
of the corpus is around 46 million words. The corpus spans vocabulary of size
376000 words.

To train embeddings on this corpus, we use skip-gram algorithm enriched
with subword information [19]. Parameters of the algorithm were the following:

6 http://www.chaskor.ru/
7 http://ru.wikinews.org
8 Word embeddings, which are available to download from http://rusvectores.org

– Vector size: 100
– Minimal length of char n-gram: 3
– Maximal length of char n-gram: 6
– Frequency threshold: 10

3.3 Results

The purpose of the ﬁrst experiment was to compare tagging accuracy of three
implementations: Bi-LSTM, Bi-LSTM + CRF, Bi-LSTM + CRF + external
word embedding news. To do this, we evaluated these implementations on the
Gareev′s dataset. Parameters of the dataset and hyper-parameters of the models
are listed below:

– Word embedding dimension: 100
– Char embedding dimension: 25
– Dimension of hidden layer: 100 (for each LSTM: forward layer and backward

layer)

– Learning method: SGD, learning rate: 0.005
– Dropout: 0.5
– Number of sentences: 2136 (for training/ validation/ testing: 1282/ 427/ 427)
– Number of words: 25372 (unique words: 7876). 7208 words (account for
91.52% of unique words) was initialized with pre-trained embedding Lenta

– epochs: 100

We used ConllEval9 to calculate metrics of performance.

The result is shown in the Table 2. One can see that adding CRF layer sig-
niﬁcantly improved prediction. Besides that, using external word embeddings
also reduced training time and increased tagging accuracy. Due to absence of
lemmatization in our text processing pipeline news embeddings matched only
about 15% of words in the corpus, embeddings for other words were just ini-
tialized randomly. Therefore, the improvement was not really signiﬁcant and
prediction for Organization type was even lower with news embeddings. To
deal with this problem, in the second experiment we decided to use FastText
trained on Lenta corpus in order to build an external word embedding. After
that, we used this embedding to train on Gareev’s dataset one more time using
the same conﬁguration with the previous experiment.

Table 3 shows the confusion matrix on the test set. We also experimented
on two other datasets: Persons-1000, FactRuEval 2016. The summary of exper-
iments on these datasets are shown in the Table 4.

We compare Bi-LSTM + CRF + Lenta model and other published results as
well as NeuroNER and its extensions on three datasets mentioned in the subsec-
tion 3.1. Results are presented in the table 5. Bi-LSTM + CRF + Lenta model
signiﬁcantly outperforms other approaches on Gareev’s dataset and Persons-
1000. However, the result on FactRuEval 2016 dataset is not as high as we
expected.
9 A Perl script was used to evaluate result of processing CoNLL-2000 shared task:

http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt

Table 2. Tagging results on Gareev’s dataset

Model

Organization
F
R
67.11 78.46 72.34 76.56 72.59 74.52 73.04 74.50 73.76
Bi-LSTM CRF 92.93 86.79 89.76 85.24 81.91 83.54 87.30 83.25 85.22

Overall
R

Person
R

Bi-LSTM

P

P

P

F

F

95.05 90.57 92.75 85.13 81.21 83.12 87.84 83.76 85.75

95.60 94.57 95.08 87.40 81.62 84.41 89.57 84.89 87.17

Bi-LSTM CRF +
news word emb.
Bi-LSTM CRF +
Lenta word emb.

Table 3. The confusion matrix on the test set of Gareev’s dataset

Named Entity Total O I-ORG B-ORG B-PER I-PER Percent
99.467
87.013
84.191
95.652
89.855

7688 7647
308
272
92
69

O
I-ORG
B-ORG
B-PER
I-PER

22
3
229
0
0

19
268
2
1
5

0
0
1
0
62

0
1
2
88
0

36
38
3
2

Table 4. Tagging results of Bi-LSTM + CRF + Lenta word embedding on three
datasets: Gareev’s dataset, FactRuEval 2016, Persons-1000

Datasets

Validation set
F
R
P
FactRuEval 2016 84.39 81.11 82.72 83.88 80.40 82.10
Gareev’s dataset 90.99 86.94 88.92 89.57 84.89 87.17
98.97 98.20 98.58 99.43 99.09 99.26

Test set
R

Persons-1000

P

F

Table 5. Performance of diﬀerent models across datasets

Models

Gareev et al. [4]
Malykh et al. [9]
Troﬁmov [5]
Rubaylo et al. [21]
Sysoev et al.[8]
Ivanitsky et al. [7]
Mozharova et al. [6]

Gareev’s dataset
F
R
P
84.10 67.98 75.05
59.65 65.70 62.49
-
-
-
-
-

-
-
-
-
-

-
-
-
-
-

P
-
-

Persons-1000
F
R
-
-
-
-
97.26 93.92 95.57
-
-
-
-

-
-
-
97.21

-
-
-
-

FactRuEval 2016
F
R
P
-
-
-
-
-
-
-
-
-
77.70 78.50 78.13
88.19 64.75 74.67
87.88
-
-
-

-
-

88.19 82.73 85.37 96.38 96.83 96.60 80.49 79.23 79.86

85.75 88.40 87.06 96.56 97.11 96.83 80.59 80.72 80.66

84.35 81.96 83.14 96.49 97.19 96.84 81.09 79.31 80.19

83.33 85.05 84.18 96.74 96.83 96.78 79.13 78.76 78.95

89.57 84.89 87.17 99.43 99.09 99.26 83.88 80.84 82.10

NeuroNER
NeuroNER +
Highway char
NeuroNER +
Highway LSTM
NeuroNER +
Highway char +
Highway LSTM

Bi-LSTM + CRF +
Lenta

4 Discussion

Traditional approaches to Russian NER heavily relied on hand-crafted rules and
external resources. Thus regular expressions and dictionaries were used in [5]
to solve the task. The next step was application of statistical learning methods
such as conditional random ﬁelds (CRF) and support vector machines (SVM)
for entity classiﬁcation. CRF on top of linguistic features considered as a base-
line in the study of [4]. Mozharova and Loukachevitch [6] proposed two-stage
CRF algorithm. Here, an input for the CRF of the ﬁrst stage was a set of hand-
crafted linguistic features. Then on the second stage the same input features
were combined with a global statistics calculated on the ﬁrst stage and fed into
CRF. Ivanitskiy et al. [7] applied SVM classiﬁer to the distributed representa-
tions of words and phrases. These representations were obtained by extensive
unsupervised pre-training on diﬀerent news corpora. Simultaneous use of dictio-
nary based features and distributed word representations was presented in [8].
Dictionary features were retrieved from Wikidata and word representations were
pre-trained on Wikipedia. Then these features were used for classiﬁcation with
SVM.

At the moment deep learning methods are seen as the most promising choice
for NER. Malykh and Ozerin [9] proposed character aware deep LSTM network
for solving Russian NER task. A distinctive feature of this work is coupling of
language modeling task with named entity classiﬁcation.

In our study we applied current state of the art neural network based model
for English NER to known Russian NER datasets. The model consists of three
main components such as bi-directional LSTM, CRF and external word embed-
dings. Our experiments demonstrated that Bi-LSTM alone was slightly worse
than CRF based model of [4]. Addition of CRF as a next processing step on
top of Bi-LSTM layer signiﬁcantly improves model’s performance and allow to
outperform the model presented in [4]. The diﬀerence of Bi-LSTM + CRF model
from the model presented in [4] is trainable feature representations. Combined
training of Bi-LSTM network on the levels of words and characters gave better
results then manual feature engineering in [4].

Distributed word representations are becoming a standard tool in the ﬁeld of
natural language processing. Such representations are able to capture semantic
features of words and signiﬁcantly improve results for diﬀerent tasks. When we
encoded words with news or Lenta embeddings results were consistently better
for all three datasets. Up to now, the prediction accuracy of the Bi-LSTM + CRF
+ Lenta model outperforms published models on Gareev’s dataset and Persons-
1000. However, the results of both Bi-LSTM + CRF + Lenta and NeuroNER
models on the FactRuEval dataset were better then results reported in [8] and
[21] but not as good as SVM based model reported in [7].

In spite the fact that both models we tested have the same structure, per-
formance of NeuroNER [3] is a bit lower than Bi-LSTM+CRF model [2]. This
issue can be explained by diﬀerent strategies for initialization of parameters.

Our extension of the baseline model with a highway network for character
embedding provides moderate performance growth in nearly all cases. Implemen-
tation of the Bi-LSTM highway network for tokens resulted in a slight increase
of performance for Persons-100 and FactRuEval 2016 datasets and a decrease
of performance for Gareev’s dataset. Simultaneous extension of the NeuroNER
with character and token Bi-LSTM highway networks results in the drop of
performance in the most of the cases.

We think that results of LSTM highway network can be improved by diﬀerent
bias initialization and deeper architectures. In the current work the highway gate
bias was initialized with 0 vector. However, bias could be initialized to some neg-
ative value. This initialization will force the network to prefer processed path
to the raw path. Furthermore, stacking highway LSTM layers might improve
results allowing a network dynamically adjust complexity of the processing. Al-
ternatively, character embedding network can be built using convolutional neural
networks (CNN) instead of LSTM. A number of authors [22,24] reported promis-
ing results with a character level CNN. Another promising extension of presented
architecture is an attention mechanism [25]. For NER task this mechanism can
be used to selectively attend to the diﬀerent parts of the context for each word
giving additional information for the tagging decision.

5 Conclusions

Named Entity Recognition is an important stage in information extraction tasks.
Today, neural network methods for solving NER task in English demonstrate the
highest potential. For Russian language there are still a few papers describing
application of neural networks to NER. We studied a series of neural models
starting from vanilla bi-directional LSTM then supplementing it with conditional
random ﬁelds, highway networks and ﬁnally adding external word embeddings.
For the ﬁrst time in the literature evaluation of models were performed across
three Russian NER datasets. Our results demonstrated that (1) basic Bi-LSTM
model is not suﬃcient to outperform existing state of the art NER solutions,
(2) addition of CRF layer to the Bi-LSTM model signiﬁcantly increases it’s
quality, (3) pre-processing the word level input of the model with external word
embeddings allowed to improve performance further and achieve state-of-the-art
for the Russian NER.

6 Acknowledgments

The statement of author contributions. AL conducted initial literature review,
selected a baseline (Bi-LSTM + CRF) model, prepared datasets and run experi-
ments under supervision of MB. AM implemented and studied extensions of the
NeuroNER model. AL drafted the ﬁrst version of the paper. AM added a review
of works related to the Russian NER and materials related to the NeuroNER
modiﬁcations. MB, AL and AM edited and extended the manuscript.

This work was supported by National Technology Initiative and PAO Sber-

bank project ID 0000000007417F630002.

References

1. Maithilee L. Patawar, M. A. Potey: Approaches to Named Entity Recognition: A
Survey. International Journal of Innovative Research in Computer and Communi-
cation Engineering, Vol 3. Issue 12, 12201 – 12208 (2015).

2. Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami,
Chris Dyer: Neural Architectures for Named Entity Recognition. ArXiv preprint
arXiv: 1603.01360 (2016).

3. Dernoncourt F., Lee, J. Y., Szolovits P.: NeuroNER: an easy-to-use pro-
gram for named-entity recognition based on neural networks. ArXiv preprint
arXiv:1705.05487 (2017).

4. Rinat Gareev, Maksim Tkachenko, Valery Solovyev, Andrey Simanovsky, Vladimir
Ivanov: Introducing Baselines for Russian Named Entity Recognition. Computa-
tional Linguistics and Intelligent Text Processing, 329 – 342 (2013).

5. Troﬁmov, I.V.: Person name recognition in news articles based on the persons-
1000/1111-F collections. In: 16th All-Russian Scientiﬁc Conference Digital Li-
braries: Advanced Methods and Technologies, Digital Collections, RCDL 2014,
pp. 217 – 221 (2014).

6. Mozharova V., Loukachevitch N.: Two-stage approach in Russian named entity
recognition. In Intelligence, Social Media and Web (ISMW FRUCT), 2016 Inter-
national FRUCT Conference, 1 – 6 (2016).

7. Ivanitskiy Roman, Alexander Shipilo, Liubov Kovriguina: Russian Named Entities
Recognition and Classiﬁcation Using Distributed Word and Phrase Representa-
tions. In SIMBig, 150 – 156. (2016).

8. Sysoev A. A., Andrianov I. A.: Named Entity Recognition in Russian: the Power

of Wiki-Based Approach. dialog-21.ru

9. Valentin Malykh, Alexey Ozerin: Reproducing Russian NER Baseline Quality with-
out Additional Data. In proceedings of the 3rd International Workshop on Concept
Discovery in Unstructured Data, Moscow, Russia, 54 – 59 (2016).

10. Yoshua Bengio, Patrice Simard, Paolo Frasconi: Learning long-term dependencies
with gradient descent is diﬃcult. IEEE Transactions on Neural Networks, Volume
5, Issue 2, 157 – 166 (1994).

11. Sepp Hochreiter, Jrgen Schmidhuber: Long Short-Term Memory. MIT Press, Vol.

9, No. 8, 1735-1780 (1997).

12. Schuster, Mike, and Kuldip K. Paliwal: Bidirectional recurrent neural networks.

IEEE Transactions on Signal Processing 45.11, 2673 – 2681 (1997).

13. Wenliang Chen, Yujie Zhang, Hitoshi Isahara: Chinese Named Entity Recognition
with Conditional Random Fields. In Proceedings of the Fifth SIGHAN Workshop
on Chinese Language Processing, 118 – 121 (2006).

14. Kutuzov A., Kuzmenko E.: WebVectors: A Toolkit for Building Web Interfaces
for Vector Semantic Models. In: Ignatov D. et al. (eds) Analysis of Images, Social
Networks and Texts. AIST 2016. Communications in Computer and Information
Science, vol 661. Springer, Cham, 155 – 161 (2016).

15. Asif Ekbal, Rejwanul Haque, Sivaji Bandyopadhyay: Named Entity Recognition in
Bengali: A Conditional Random Field Approach. IJCNLP conference, 589 – 594
(2008).

16. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
N. Gomez, Lukasz Kaiser, Illia Polosukhin: Attention is all you need. ArXiv
preprint arXiv: 1706.03762 (2017)

17. T. Mikolov, M. Karaﬁat, L. Burget, J. Cernocky, S. Khudanpur. 2010: Recurrent

neural network based language model. Interspeech, 1045 – 1048 (2010).

18. Starostin A. S., Bocharov V. V., Alexeeva S. V., Bodrova A., Chuchunkov A.
S., Dzhumaev S. S., Nikolaeva M. A.. FactRuEval 2016: Evaluation of Named
Entity Recognition and Fact Extraction Systems for Russian. In Computational
Linguistics and Intellectual Technologies. Proceedings of the Annual International
Conference Dialogue(2016). No. 15, 702 – 720 (2016).

19. Bojanowski Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov: Enriching
word vectors with subword information. ArXiv preprint arXiv:1607.04606 (2016).
20. Vlasova N.A., Suleymanova E.A., Troﬁmov I.V: Report on Russian corpus for
personal name retrieval. In proceedings of computational and cognitive linguistics
TEL’2014, Kazan, Russia, 36 – 40 (2014)

21. Rubaylo A. V., Kosenko M. Y.: Software utilities for natural language information
retrievial. Almanac of modern science and education, Volume 12 (114), 87 – 92.
(2016)

22. Kim, Y., Jernite, Y., Sontag, D., Rush, A. M.: Character-Aware Neural Language
Models. Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence,
2741 – 2749. (2016)

23. Pundak Golan, and Tara N. Sainath: Highway-LSTM and Recurrent Highway Net-

works for Speech Recognition. Proc. Interspeech 2017, ISCA (2017)

24. Tran P. N., Ta V. D., Truong Q. T., Duong Q. V., Nguyen T. T., Phan X. H.:
Named Entity Recognition for Vietnamese Spoken Texts and Its Application in
Smart Mobile Voice Interaction. In Asian Conference on Intelligent Information
and Database Systems. Springer Berlin Heidelberg. 170 – 180. (2016)

25. Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. ”Neural machine trans-
lation by jointly learning to align and translate.” arXiv preprint arXiv:1409.0473
(2014).

