7
1
0
2
 
t
c
O
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
6
6
0
7
0
.
5
0
6
1
:
v
i
X
r
a

Unifying Gaussian Process Approximations

A Unifying Framework for Gaussian Process Pseudo-Point
Approximations using Power Expectation Propagation

Thang D. Bui

Josiah Yan

Richard E. Turner
Computational and Biological Learning Lab, Department of Engineering
University of Cambridge, Trumpington Street, Cambridge, CB2 1PZ, UK

tdb40@cam.ac.uk

josiah.yan@gmail.com

ret26@cam.ac.uk

Abstract

Gaussian processes (GPs) are ﬂexible distributions over functions that enable high-
level assumptions about unknown functions to be encoded in a parsimonious, ﬂexible and
general way. Although elegant, the application of GPs is limited by computational and
analytical intractabilities that arise when data are suﬃciently numerous or when employ-
ing non-Gaussian models. Consequently, a wealth of GP approximation schemes have been
developed over the last 15 years to address these key limitations. Many of these schemes
employ a small set of pseudo data points to summarise the actual data. In this paper we
develop a new pseudo-point approximation framework using Power Expectation Propaga-
tion (Power EP) that uniﬁes a large number of these pseudo-point approximations. Unlike
much of the previous venerable work in this area, the new framework is built on standard
methods for approximate inference (variational free-energy, EP and Power EP methods)
rather than employing approximations to the probabilistic generative model itself. In this
way all of approximation is performed at ‘inference time’ rather than at ‘modelling time’
resolving awkward philosophical and empirical questions that trouble previous approaches.
Crucially, we demonstrate that the new framework includes new pseudo-point approxima-
tion methods that outperform current approaches on regression and classiﬁcation tasks.

1. Introduction

Gaussian Processes (GPs) are powerful nonparametric distributions over continuous func-
tions that are routinely deployed in probabilistic modelling for applications including regres-
sion and classiﬁcation (Rasmussen and Williams, 2005), representation learning (Lawrence,
2005), state space modelling (Wang et al., 2005), active learning (Houlsby et al., 2011),
reinforcement learning (Deisenroth, 2010), black-box optimisation (Snoek et al., 2012), and
numerical methods (Mahsereci and Hennig, 2015). GPs have many elegant theoretical
properties, but their use in probabilistic modelling is greatly hindered by analytic and
computational intractabilities. A large research eﬀort has been directed at this fundamen-
tal problem resulting in the development of a plethora of sparse approximation methods
that can sidestep these intractabilities (Csat´o, 2002; Csat´o and Opper, 2002; Schwaighofer
and Tresp, 2002; Seeger et al., 2003; Qui˜nonero-Candela and Rasmussen, 2005; Snelson
and Ghahramani, 2006; Snelson, 2007; Naish-Guzman and Holden, 2007; Titsias, 2009;
Figueiras-Vidal and L´azaro-Gredilla, 2009; ´Alvarez et al., 2010; Qi et al., 2010; Bui and

1

Bui, Yan and Turner

Turner, 2014; Frigola et al., 2014; McHutchon, 2014; Hensman et al., 2015; Hern´andez-
Lobato and Hern´andez-Lobato, 2016; Matthews et al., 2016)

This paper develops a general sparse approximate inference framework based upon Power
Expectation Propagation (PEP) (Minka, 2004) that uniﬁes many of these approximations,
extends them signiﬁcantly, and provides improvements in practical settings. In this way, the
paper provides a complementary perspective to the seminal review of Qui˜nonero-Candela
and Rasmussen (2005) viewing sparse approximations through the lens of approximate
inference, rather than approximate generative models.

The paper begins by reviewing several frameworks for sparse approximation focussing
on the GP regression and classiﬁcation setting (section 2). It then lays out the new unifying
framework and the relationship to existing techniques (section 3). Readers whose focus is
to understand the new framework might want to move directly to this section. Finally, a
thorough experimental evaluation is presented in section 4.

2. Pseudo-point Approximations for GP Regression and Classiﬁcation

This section provides a concise introduction to GP regression and classiﬁcation and then
reviews several pseudo-point based sparse approximation schemes for these models. For
simplicity, we ﬁrst consider a supervised learning setting in which the training set com-
prises N D-dimensional input and scalar output pairs {xn, yn}N
n=1 and the goal is to
produce probabilistic predictions for the outputs corresponding to novel inputs. A non-
linear function, f (x), can be used to parameterise the probabilistic mapping between in-
puts and outputs, p(yn|f, xn, θ). Typical choices for the probabilistic mapping are Gaus-
y) for the regression setting (yn ∈ R) and Bernoulli
sian p(yn|f, xn, θ) = N (yn; f (xn), σ2
p(yn|f, xn, θ) = B(yn; Φ(f (xn))) with a sigmoidal link function Φ(f ) for the binary classi-
ﬁcation setting (yn ∈ {0, 1}). Whilst it is possible to specify the non-linear function f via
an explicit parametric form, a more ﬂexible and elegant approach employs a GP prior over
the functions directly, p(f |θ) = GP(f ; 0, kθ(·, ·)), here assumed without loss of generality to
have a zero mean-function and a covariance function kθ(x, x(cid:48)). This class of probabilistic
models has a joint distribution

p(f, y|θ) = p(f |θ)

p(yn|f (xn), θ)

(1)

where we have collected the observations into the vector y and suppressed the inputs on
the left hand side to lighten the notation.

This model class contains two potential sources of intractability. First, the possibly non-
linear likelihood function can introduce analytic intractabilities that require approximation.
Second, the GP prior entails an O(N 3) complexity that is computationally intractable for
many practical problems. These two types of intractability can be handled by combining
standard approximate inference methods with pseudo-point approximations that summarise
the full Gaussian process via M pseudo data points leading to an O(N M 2) cost. The main
approaches of this sort can be characterised in terms of two parallel frameworks that are
described in the following sections.

N
(cid:89)

n=1

2

Unifying Gaussian Process Approximations

2.1 Sparse GP Approximation via Approximate Generative Models

The ﬁrst framework begins by constructing a new generative model that is similar to the
original, so that inference in the new model might be expected to produce similar results,
but which has a special structure that supports eﬃcient computation. Typically this ap-
proach involves approximating the Gaussian process prior as it is the origin of the cubic
cost. If there are analytic intractabilities in the approximate model, as will be the case in
e.g. classiﬁcation or state-space models, then these will require approximate inference to be
performed in the approximate model.

The seminal review by Qui˜nonero-Candela and Rasmussen (Qui˜nonero-Candela and
Rasmussen, 2005) reinterprets a family of approximations in terms of this unifying frame-
work. The GP prior is approximated by identifying a small set of M ≤ N pseudo-points
u, here assumed to be disjoint from the training function values f so that f = {u, f , f
=u,f }.
The GP prior is then decomposed using the product rule

p(f |θ) = p(u|θ)p(f |u, θ)p(f

=u,f |f , u, θ).

(2)

1
uuu, Dﬀ ) where Dﬀ = Kﬀ − Qﬀ and Qﬀ = KfuK−

Of central interest is the relationship between the pseudo-points and the training function
1
uuKuf .
values p(f |u, θ) = N (f ; KfuK−
Here we have introduced matrices corresponding to the covariance function’s evaluation
at the pseudo-input locations {zm}M
m=1, so that [Kuu]mm(cid:48) = kθ(zm, zm(cid:48)) and similarly
for the covariance between the pseudo-input and data locations [Kuf ]mn = kθ(zm, xn).
Importantly, this term saddles learning with a cubic complexity cost. Computationally
eﬃcient approximations can be constructed by simplifying these dependencies between the
pseudo-points and the data function values q(f |u, θ) ≈ p(f |u, θ). In order to beneﬁt from
these eﬃciencies at prediction time as well, a second approximation is made whereby the
pseudo-points form a bottleneck between the data function values and test function values
=u,f |f , u, θ). Together, the two approximations result in an approximate
p(f
prior process,

=u,f |u, θ) ≈ p(f

q(f |θ) = p(u|θ)q(f |u, θ)p(f

=u,f |u, θ).

We can now compactly summarise a number of previous approaches to GP approximation
as special cases of the choice

q(f |u, θ) =

1
N (fb; Kfb,uK−

uuu, αDfb,fb)

B
(cid:89)

b=1

where b indexes B disjoint blocks of data-function values. The Deterministic Training
Conditional (DTC) approximation uses α → 0; the Fully Independent Training Condi-
tional (FITC) approximation uses α = 1 and B = N ; the Partially Independent Training
Conditional (PITC) approximation uses α = 1 (Qui˜nonero-Candela and Rasmussen, 2005;
Schwaighofer and Tresp, 2002).

In a moment we will consider inference in the modiﬁed models, before doing so we note
that it is possible to construct more ﬂexible modiﬁed prior processes using the inter-domain
approach that places the pseudo-points in a diﬀerent domain from the data, deﬁned by
a linear integral transform g(z) = (cid:82) w(z, z(cid:48))f (z(cid:48))dz(cid:48). Here the window w(z, z(cid:48)) might be

3

(3)

(4)

Bui, Yan and Turner

a Gaussian blur or a wavelet transform. The pseudo-points are now placed in the new
domain g = {u, g
=u} where they induce a potentially more ﬂexible Gaussian process in the
old domain f through the linear transform (see Figueiras-Vidal and L´azaro-Gredilla, 2009,
for FITC). The expressions in this section still hold, but the covariance matrices involving
pseudo-points are modiﬁed to take account of the transform,

(cid:90)

(cid:90)

[Kuu]mm(cid:48) =

w(zm, z)kθ(z, z(cid:48))w(z(cid:48), zm(cid:48))dzdz(cid:48),

[Kuf ]mn =

w(zm, z)kθ(z, xn)dz.

(5)

Having speciﬁed modiﬁed prior processes, these can be combined with the original like-
lihood function to produce a new generative model. In the case of point-wise likelihoods we
have

q(y, f |θ) = q(f |θ)

p(yn|f (xn), θ).

(6)

N
(cid:89)

n=1

Inference and learning can now be performed using the modiﬁed model using standard
techniques. Due to the form of the new prior process, the computational complexity is
O(N M 2) (for testing, N becomes the number of test data points, assuming dependencies
between the test-points are not computed).1 For example, in the case of regression, the
posterior distribution over function values f (necessary for inference and prediction) has a
simple analytic form

|
b=1) + σ2

q(f |y, θ) = GP(f ; µf

y, Σf

y), µf

y = Qf f K−

ﬀ y, Σf

1

1
ﬀ Qf f
y = Kf f − Qf f K−

(7)

|

|

|

where Kﬀ = Qﬀ + blkdiag({αbDfbfb}B
yI and blkdiag builds a block-diagonal matrix
from its inputs. One way of understanding the origin of the computational gains is that
the new generative model corresponds to a form of factor analysis in which the M pseudo-
points determine the N function values at the observed data (as well as at potential test
locations) via a linear Gaussian relationship. This results in low rank (sparse) structure in
Kﬀ that can be exploited through the matrix inversion and determinant lemmas. In the
case of regression, the new model’s marginal likelihood also has an analytic form that allows
the hyper-parameters, θ, to be learned via optimisation

log q(y|θ) = −

log(2π) −

log |Kﬀ | −

N
2

1
2

1
y(cid:124)K−
ﬀ y.

1
2

(8)

The approximate generative model framework has attractive properties. The cost of
inference, learning, and prediction has been reduced from O(N 3) to O(N M 2) and in many
cases accuracy can be maintained with a relatively small number of pseudo-points. The
pseudo-point input locations can be optimised by maximising the new model’s marginal
likelihood (Snelson and Ghahramani, 2006). When M = N and the pseudo-points and
observed data inputs coincide, then FITC and PITC are exact which appears reassuring.
However, the framework is philosophically challenging as the elegant separation of model
and (approximate) inference has been lost. Are we allowed in an online inference setting,

1. It is assumed that the maximum size of the blocks is not greater than the number of pseudo-points

dim(fb) ≤ M .

4

Unifying Gaussian Process Approximations

for example, to add new pseudo-points as more data are acquired and the complexity of the
underlying function is revealed? This seems sensible, but eﬀectively changes the modelling
assumptions as more data are seen. Devout Bayesians might then demand that we perform
model averaging for coherence. Similarly, if the pseudo-input locations are optimised, the
principled non-parametric model has suddenly acquired M D parameters and with them
all of the concomitant issues of parametric models including overﬁtting and optimisation
diﬃculties (Bauer et al., 2016). As the pseudo-inputs are considered part of the model, the
Bayesians might then suggest that we place priors over the pseudo-inputs and perform full
blown probabilistic inference over them.

These awkward questions arise because the generative modelling interpretation of pseudo-
data entangles the assumptions made about the data with the approximations required
to perform inference. Instead, the modelling assumptions (which encapsulate prior under-
standing of the data) should remain decoupled from inferential assumptions (which leverage
structure in the posterior for tractability). In this way pseudo-data should be introduced
when we seek to perform computationally eﬃcient approximate inference, leaving the mod-
elling assumptions unchanged as we reﬁne and improve approximate inference. Indeed, even
under the generative modelling perspective, for analytically intractable likelihood functions
an additional approximate inference step is required, begging the question; why not handle
computational and analytic intractabilities together at inference time?

2.2 Sparse GP Approximation via Approximate Inference: VFE

The approximate generative model framework for constructing sparse approximations is
philosophically troubling. In addition, learning pseudo-point input locations via optimisa-
tion of the model likelihood can perform poorly e.g. for DTC it is prone to overﬁtting even
for M (cid:28) N (Titsias, 2009). This motivates a more direct approach that commits to the
true generative model and performs all of the necessary approximation at inference time.

Perhaps the most well known approach in this vein is Titsias’s beautiful sparse varia-
tional free energy (VFE) method (Titsias, 2009). The original presentation of this work
employs ﬁnite variable sets and an augmentation trick that arguably obscures its full ele-
gance. Here instead we follow the presentation in Matthews et al. (2016) and lower bound
the marginal likelihood using a distribution q(f ) over the entire inﬁnite dimensional func-
tion,

log p(y|θ) = log

p(y, f |θ)df ≥

q(f ) log

(cid:90)

(cid:90)

p(y, f |θ)
q(f )

(cid:20)

df = E

q(f )

log

(cid:21)

p(y, f |θ)
q(f )

= F(q, θ).

The VFE bound can be written as the diﬀerence between the model log-marginal likelihood
and the KL divergence between the variational distribution and the true posterior F(q, θ) =
log p(y|θ)−KL(q(f )||p(f |y, θ)). The bound is therefore saturated when q(f ) = p(f |y, θ), but
=u}, and an approxi-
this is intractable. Instead, pseudo-points are made explicit, f = {u, f
mate posterior distribution used of the following form q(f ) = q(u, f
=u|u, θ)q(u).
Under this approximation, the set of variables f
=u do not experience the data directly, but
rather only through the pseudo-points, as can be seen by comparison to the true poste-
rior p(f |y, θ) = p(f
=u|y, u, θ)p(u|y, θ). Importantly, the form of the approximate posterior
causes a cancellation of the prior conditional term, which gives rise to a bound with O(N M 2)

=u|θ) = p(f

5

Bui, Yan and Turner

complexity,

F(q, θ) = E

q(f

θ)

|

E

q(f

|

(cid:88)

=

n

(cid:20)

log

=u|u, θ)p(u|θ)

p(y|f, θ)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
p(f
(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
=u|u, θ)q(u)
p(f
θ) [log p(yn|fn, θ)] − KL(q(u)||p(u|θ)).

(cid:21)

For regression with Gaussian observation noise, the calculus of variations can be used to
ﬁnd the optimal approximate posterior Gaussian process over pseudo-data qopt(f |θ) =
p(f

=u|u, θ)qopt(u) which has the form

qopt(f |θ) = GP(f ; µf

y, Σf

y), µf

y = Qf f ˜K−

1
ﬀ y, Σf

y = Kf f − Qf f ˜K−
1
ﬀ Qf f

(9)

|

|

|

|

where ˜Kﬀ = Qﬀ + σ2
yI. This process is identical to that recovered when performing ex-
act inference under the DTC approximate regression generative model (Titsias, 2009) (see
equation 7 as α → 0). In fact DTC was originally derived using a related KL argument
(Csat´o, 2002; Seeger et al., 2003). The optimised free-energy is

F(qopt, θ) = −

log(2π) −

log | ˜Kﬀ | −

N
2

1
2

1
2

y(cid:124) ˜K−

1
ﬀ y −

1
2σ2
y

trace(Kﬀ − Qﬀ ).

(10)

Notice that the free-energy has an additional trace term as compared to the marginal
likelihood obtained from the DTC generative model approach (see equation 8 as α → 0).
The trace term is proportional to the sum of the variances of the training function values
given the pseudo-points, p(f |u), it thereby encourages pseudo-input locations that explain
the observed data well. This term acts as a regulariser that prevents overﬁtting which
plagues the generative model formulation of DTC.

The VFE approach can be extended to non-linear models including classiﬁcation (Hens-
man et al., 2015), latent variable models (Titsias and Lawrence, 2010) and state space
models (Frigola et al., 2014; McHutchon, 2014) by restricting q(u) to be Gaussian and
optimising its parameters. Indeed, this uncollapsed form of the bound can be beneﬁcial
in the context of regression too as it is amenable to stochastic optimisation (Hensman
et al., 2013). Additional approximation is sometimes required to compute any remaining
intractable non-linear integrals, but these are often low-dimensional. For example, when the
likelihood depends on only one latent function value, as is typically the case for regression
and classiﬁcation, the bound requires only 1D integrals E
q(fn) [log p(yn|fn, θ)] that can be
evaluated using quadrature (Hensman et al., 2015), for example.

The VFE approach can also be extended to employ inter-domain variables ( ´Alvarez et al.,
2010; Tobar et al., 2015; Matthews et al., 2016). The approach considers the augmented
generative model p(f, g|θ) where to remind the reader the auxiliary process is deﬁned by a
linear integral transformation, g(z) = (cid:82) w(z, z(cid:48))f (z(cid:48))dz(cid:48). Variational inference is now per-
formed over both latent processes q(f, g) = q(f, u, g
=u|u, θ)q(u). Here the
pseudo-data have been placed into the auxiliary process with the idea being that they can
induce richer dependencies in the original domain that model the true posterior more accu-
rately. In fact, if the linear integral transformation is parameterised then the transformation
can be learned so that it approximates the posterior more accurately.

=u|θ) = p(f, g

6

Unifying Gaussian Process Approximations

A key concept underpinning the VFE framework is that the pseudo-input locations (and
the parameters of the inter-domain transformation, if employed) are purely parameters of
the approximate posterior, hence the name ‘variational parameters’. This distinction is im-
portant as it means, for example, that we are free to add pseudo-data as more structure is
revealed the underlying function without altering the modelling assumptions (e.g. see Bui
et al. (2017) for an example in online inference). Moreover, since the pseudo-input locations
are variational parameters, placing priors over them is unnecessary in this framework. Un-
like the model parameters, optimisation of variational parameters is automatically protected
from overﬁtting as the optimisation is minimising the KL divergence between the approx-
imate posterior and the true posterior. Indeed, although the DTC posterior is recovered
in the regression setting, as we have seen the free-energy is not equal to the log-marginal
likelihood of the DTC generative model, containing an additional term that substantially
improves the quality of the optimised pseudo-point input locations.

The fact that the form of the DTC approximation can be recovered from a direct ap-
proximate inference approach and that this new perspective leads to superior pseudo-input
optimisation, raises the question; can this also be done for FITC and PITC?

2.3 Sparse GP Approximation via Approximate Inference: EP

Expectation Propagation (EP) is a deterministic inference method (Minka, 2001) that is
known to outperform VFE methods in GP classiﬁcation when unsparsiﬁed fully-factored ap-
proximations q(f ) = (cid:81)
n qn(fn) are used (Nickisch and Rasmussen, 2008). Motivated by this
observation, EP has been combined with the approximate generative modelling approach
to handle non-linear likelihoods (Naish-Guzman and Holden, 2007; Hern´andez-Lobato and
Hern´andez-Lobato, 2016). This begs the question: can the sparsiﬁcation and the non-linear
approximation be handled in a single EP inference stage, as for VFE? Astonishingly Csat´o
and Opper not only developed such a method in 2002 (Csat´o and Opper, 2002), predat-
ing much of the work mentioned above, they showed that it is equivalent to applying the
FITC approximation and running EP if further approximation is required. In our view,
this is a central result, but it appears to have been largely overlooked by the ﬁeld. Snelson
was made aware of it when writing his thesis (Snelson, 2007), brieﬂy acknowledging Csat´o
and Opper’s contribution. Qi et al. (2010) extended Csat´o and Opper’s work to utilise
inter-domain pseudo-points and they additionally recognised that the EP energy function
at convergence is equal to the FITC log-marginal likelihood approximation. Interestingly,
no additional term arises as it does when the VFE approach generalised the DTC generative
model approach. We are unaware of other work in this vein.

It is hard to be known for certain why these important results are not widely known,
but a contributing factor is that the exposition in these papers is largely at Marr’s algo-
rithmic level (Dawson, 1998), and does not focus on the computational level making them
challenging to understand. Moreover, Csat´o and Opper’s paper was written before EP was
formulated in a general way and the presentation, therefore, does not follow what has be-
come the standard approach. In fact, as the focus was online inference, Assumed Density
Filtering (Kushner and Budhiraja, 2000; Ito and Xiong, 2000) was employed rather than
full-blown EP. One of the main contributions of this paper is to provide a clear compu-
tational exposition including an explicit form of the approximating distribution and full

7

Bui, Yan and Turner

details about each step of the EP procedure. In addition, to bringing clarity we make the
following novel contributions:

• We show that a generalisation of EP called Power EP can subsume the EP and
VFE approaches (and therefore FITC and DTC) into a single uniﬁed framework.
More precisely, the ﬁxed points of Power EP yield the FITC and VFE posterior
distribution under diﬀerent limits and the Power EP marginal likelihood estimate
(the negative ‘Power EP energy’) recovers the FITC marginal likelihood and the VFE
too. Critically the connection to the VFE method leans on the new interpretation of
Titsias’s approach (Matthews et al., 2016) outlined in the previous section that directly
employs the approximate posterior over function values (rather than augmenting the
model with pseudo-points). The connection therefore also requires a formulation of
Power EP that involves KL divergence minimisation between stochastic processes.

• We show how versions of PEP that are intermediate between the existing VFE and EP
approaches can be derived, as well as mixed approaches that treat some data variation-
ally and others using EP. We also show how PITC emerges from the same framework
and how to incorporate inter-domain transforms. For regression with Gaussian obser-
vation noise, we obtain analytical expressions for the ﬁxed points of Power EP in a
general case that includes all of these extensions as well as the form of the Power EP
marginal likelihood estimate at convergence that is useful for hyper-parameter and
pseudo-input optimisation.

• We consider (Gaussian) regression and probit classiﬁcation as canonical models on
which to test the new framework and demonstrate through exhaustive testing that
versions of PEP intermediate between VFE and EP perform substantially better on
average. The experiments also shed light on situations where VFE is to be preferred
to EP and vice versa which is an important open area of research.

Many of the new theoretical contributions described above are summarised in ﬁg. 1

along with their relationship to previous work.

3. A New Unifying View using Power Expectation Propagation

In this section, we provide a new unifying view of sparse approximation using Power Ex-
pectation Propagation (PEP or Power EP) (Minka, 2004). We review Power EP, describe
how to apply it for sparse GP regression and classiﬁcation, and then discuss its relationship
to existing methods.

3.1 The Joint-Distribution View of Approximate Inference and Learning

One way of understanding the goal of distributional inference approximations, including the
VFE method, EP and Power EP, is that they return an approximation of a tractable form
to the model joint-distribution evaluated on the observed data. In the case of GP regression
and classiﬁcation, this means q∗(f |θ) ≈ p(f, y|θ) where ∗ is used to denote an unnor-
malised process. Why is the model joint-distribution a sensible object of approximation?
The joint distribution can be decomposed into the product of the posterior distribution

8

Unifying Gaussian Process Approximations

Figure 1: A uniﬁed view of pseudo-point GP approximations applied to A) regression,
and B) classiﬁcation. Every point in the algorithm polygons corresponds to a form of GP
approximation. Previous algorithms correspond to labelled vertices. The new Power EP
framework encompasses the three polygons, including their interior.

and the marginal likelihood, p(f, y|θ) = p∗(f |y, θ) = p(f |y, θ)p(y|θ), the two inferential
objects of interest. A tractable approximation to the joint can therefore be similarly de-
composed q∗(f |θ) = Zq(f |θ) into a normalised component that approximates the posterior
q(f |θ) ≈ p(f |y, θ) and the normalisation constant which approximates the marginal likeli-
hood Z ≈ p(y|θ). In other words, the approximation of the joint simultaneously returns
approximations to the posterior and marginal likelihood. In the current context tractability
of the approximating family means that it is analytically integrable and that this integra-
tion can be performed with an appropriate computational complexity. We consider the
approximating family comprising unnormalised GPs, q∗(f |θ) = ZGP(f ; mf , Vﬀ (cid:48)).

The VFE approach can be reformulated in the new context using the un-normalised KL
divergence (Zhu and Rohwer, 1997) to measure the similarity between the approximation
and the joint distribution

KL(q∗(f |θ)||p(f, y|θ)) =

q∗(f ) log

df +

(p(f, y|θ) − q∗(f )) df.

(11)

(cid:90)

q∗(f )
p(f, y|θ)

(cid:90)

The un-normalised KL divergence generalises the KL divergence to accommodate un-normalised
It is always non-negative and collapses back to the standard form when its
densities.
arguments are normalised. Minimising the un-normalised KL with respect to q∗(f |θ) =
ZVFEq(f ) encourages the approximation to match both the posterior and marginal-likelihood,
and it yields analytic solutions

qopt(f ) = argmin

KL(q(f )||p(f |y, θ)), and Zopt

VFE = exp(F(qopt(f ), θ)).

(12)

q(f )

∈Q

9

Bui, Yan and Turner

That is, the standard variational free-energy approximation to the posterior and marginal
likelihood is recovered. One of the pedagogical advantages of framing VFE in this way is that
approximation of the posterior and marginal likelihood are committed to upfront, in con-
trast to the traditional derivation which begins by targeting approximation of the marginal
likelihood, but shows that approximation of the posterior emerges as an essential part of
this scheme (see section 2.2). A disadvantage is that optimisation of hyper-parameters must
logically proceed by optimising the marginal likelihood approximation, Zopt
VFE, and at ﬁrst
sight therefore appears to necessitate diﬀerent objective functions for q∗(f |θ) and θ (unlike
the standard view which uses a single objective from the beginning). However, it is easy
to show that maximising the single objective p(y|θ) − KL(q∗(f |θ)||p(f, y|θ)) directly for
both q∗(f |θ) and θ is equivalent and that this also recovers the standard VFE method (see
appendix A).

3.2 The Approximating Distribution Employed by Power EP

Power EP also approximates the joint-distribution employing an approximating family
whose form mirrors that of the target,

p∗(f |y, θ) = p(f |y, θ)p(y|θ) = p(f |θ)

p(yn|f, θ) ≈ p(f |θ)

tn(u) = q∗(f |θ).

(13)

(cid:89)

n

(cid:89)

n

Here, the approximation retains the exact prior, but each likelihood term in the exact
posterior, p(yn|fn, θ), is approximated by a simple factor tn(u) that is assumed Gaussian.
These simple factors will be iteratively reﬁned by the PEP algorithm such that they will
capture the eﬀect that each true likelihood has on the posterior.

Before describing the details of the PEP algorithm, it is illuminating to consider an
alternative interpretation of the approximation. Together, the approximate likelihood
functions specify an un-normalised Gaussian over the pseudo-points that can be written
(cid:81)
n tn(u) = N (˜y; ˜Wu, ˜Σ) (assuming that the product of these factors is normalisable which

may not be the case for heavy tailed likelihoods, for example).

The approximate posterior above can therefore be thought of as the (exact) GP pos-
terior resulting from a surrogate regression problem with surrogate observations ˜y that
are generated from linear combinations of the pseudo-points and additive surrogate noise
˜y = ˜Wu + ˜Σ1/2(cid:15). We note that the pseudo-points u live on the latent function (or an
inter-domain transformation thereof) and the surrogate observations ˜y will not generally
lie on the latent function. The surrorate observations and the pseudo-points are therefore
analogous to the data y and the function values f in a normal Gaussian Process regression
problem, respectively. To make the paper more speciﬁc on this point, we have deﬁned pa-
rameters forthe surrogate regression problem explicitly in appendix H. The PEP algorithm
will implicitly iteratively reﬁne {˜y, ˜W, ˜Σ} such that exact inference in the simple surrogate
regression model returns a posterior and marginal likelihood estimate that is ‘close’ to that
returned by performing exact inference in the intractable complex model (see ﬁg. 2).

3.3 The EP Algorithm

One method for updating the approximate likelihood factors tn(u) is to minimise the unnor-
malised KL Divergence between the joint distribution and each of the distributions formed

10

Unifying Gaussian Process Approximations

Figure 2: Perspectives on the approximating family. The true joint distribution over the
unknown function f and the N data points y (top left) comprises the GP prior and an
intractable likelihood function. This is approximated by a surrogate regression model with
a joint distribution over the function f and M surrogate data points ˜y (top right). The
surrogate regression model employs the same GP prior, but uses a Gaussian likelihood
function p(˜y|u, ˜W, ˜Σ) = N (˜y; ˜Wu, ˜Σ). The intractable true posterior (bottom left) is
approximated by reﬁning the surrogate data ˜y their input locations z and the parameters
of the surrogate model ˜W and ˜Σ.

by replacing one of the likelihoods by the corresponding approximating factor (Li et al.,
2015),

(cid:20)

KL

p(f, y|θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

p(f, y|θ)tn(u)
p(yn|fn, θ)

(cid:21)

argmax
tn(u)

= argmax

tn(u)

n(f )tn(u)]. (14)
n(f )p(yn|fn, θ)||p∗
KL[p∗
\
\

n(f ) = p(f, y|θ)/p(yn|fn, θ) which makes
Here we have introduced the leave-one-out joint p∗
\
clear that the minimisation will cause the approximate factors to approximate the like-
lihoods in the context of the leave-one-out joint. Unfortunately, such an update is still
intractable. Instead, EP approximates this idealised procedure by replacing the exact leave-
one-out joint on both sides of the KL by the approximate leave-one-out joint (called the
n(f ) = q∗(f )/tn(u). Not only does this improve tractability, but it also
n(f ) ≈ q∗
cavity) p∗
\
\
means that the new procedure eﬀectively reﬁnes the approximating distribution directly at
each stage, rather than setting the component parts in isolation,

KL([q∗
\

n(f )p(yn|fn, θ)||q∗(f )].
n(f )tn(u)] = KL([q∗
n(f )p(yn|fn, θ)||q∗
\
\

(15)

However, the updates for the approximating factors are now coupled and so the updates
must now be iterated, unlike in the idealised procedure. In this way, EP iteratively reﬁnes

11

Bui, Yan and Turner

the approximate factors or surrogate likelihoods so that the GP posterior of the surro-
gate regression task best approximates the posterior of the original regression/classiﬁcation
problem.

3.4 The Power EP Algorithm

Power EP is, algorithmically, a mild generalisation of the EP algorithm that instead removes
(or includes) a fraction α of the approximate (or true) likelihood functions in the following
steps:

1. Deletion: compute the cavity distribution by removing a fraction of one approximate

n(f |θ) ∝ q∗(f |θ)/tα
factor, q∗
\

n(u).

2. Projection: ﬁrst, compute the tilted distribution by incorporating a corresponding
n(f |θ)pα(yn|fn). Second, project
fraction of the true likelihood into the cavity, ˜p(f ) = q∗
\
the tilted distribution onto the approximate posterior using the KL divergence for un-
normalised densities,

q∗(f |θ) ← argmin

KL(˜p(f )||q∗(f |θ)).

(16)

q∗(f

θ)

|

∈Q

Here Q is the set of allowed q∗(f |θ) deﬁned by eq. (13).

3. Update: compute a new fraction of the approximate factor by dividing the new approx-
imate posterior by the cavity, tα
n(f |θ), and incorporate this fraction
n,new(u) = q∗(f |θ)/q∗
\
back in to obtain the updated factor, tn(u) = t1
α
n,old(u)tα
−

n,new(u).

The above steps are iteratively repeated for each factor that needs to be approximated.
Notice that the procedure only involves one likelihood factor to be handled at a time. In
the case of analytically intractable likelihood functions, this often requires only low dimen-
sional integrals to be computed. In other words, PEP has transformed a high dimensional
intractable integral that is hard to approximate into a set of low dimensional intractable
integrals that are simpler to approximate. The procedure is not, in general guaranteed to
converge but we did not observe any convergence issues in our experiments. Furthermore, it
can be shown to be numerically stable when the factors are log-concave (as in GP regression
and classiﬁcation without pseudo-data) (Seeger, 2008).

If Power EP converges, the fractional updates are equivalent to running the original EP
procedure, but replacing the KL minimisation with an alpha-divergence minimisation (Zhu
and Rohwer, 1995; Minka, 2005),

Dα[p∗(f )||q∗(f )] =

(cid:2)αp∗(f ) + (1 − α)q∗(f ) − p∗(f )αq∗(f )1

−

α(cid:3) df.

(17)

(cid:90)

1
α(1 − α)

When α = 1, the alpha-divergence is the inclusive KL divergence D1[p∗(f )||q∗(f )] =
KL[p∗(f )||q∗(f )] recovering EP as expected from the PEP algorithm. As α → 0 the exclu-
0[p∗(f )||q∗(f )] = KL[q∗(f )||p∗(f )], and since minimising
sive KL divergence is recovered, D
a set of local exclusive KL divergences is equivalent to minimising a single global exclusive
KL divergence (Minka, 2005), the Power EP solution is the minimum of a variational free-
energy (see appendix B for more details). In the current case, we will now show explicitly
that these cases of Power EP recover FITC and Titsias’s VFE solution respectively.

→

12

Unifying Gaussian Process Approximations

3.5 General Results for Gaussian Process Power EP

This section describes the Power EP steps in ﬁner detail showing the complexity is O(N M 2)
and laying the ground work for the equivalence relationships. The appendix F includes a
full derivation.

We start by deﬁning the approximate factors to be in natural parameter form, mak-
ing it simple to combine and delete them, tn(u) = ˜N (u; zn, T1,n, T2,n) = zn exp(u(cid:124)T1,n −
2 u(cid:124)T2,nu). We consider full rank T2,n, but will show that the optimal form is rank 1.
1
The parameterisation means the approximate posterior over the pseudo-points has natural
parameters T1,u = (cid:80)
uu + (cid:80)
n T2,n inducing an approximate posterior,
q∗(f |θ) = ZPEPGP(f ; mf , Vﬀ (cid:48)). Here and in what follows, the dependence on the hyperpa-
rameters θ will be suppressed to lighten the notation. The mean and covariance functions
of the approximate posterior are

1
n T1,n and T2,u = K−

mf = KfuK−

1
uuT−

1
2,uT1,u;

Vﬀ (cid:48) = Kﬀ (cid:48) − Qﬀ (cid:48) + KfuK−

1
1
1
2,uK−
uuT−

uuKuf (cid:48).

(18)

n(f ) ∝ q∗(f )/tα
n(u), has a similar form to the
Deletion: The cavity for data point n, q∗
\
n
1,u = T1,u −αT1,n
posterior, but the natural parameters are modiﬁed by the deletion step, T\

and T\

n
2,u = T2,u − αT2,n, yielding the following mean and covariance functions

n
1
uuT\
f = KfuK−
m\

n
n,
1
2,u T\
1,u;
−

n
ﬀ (cid:48) = Kﬀ (cid:48) − Qﬀ (cid:48) + KfuK−
V \

1
uuT\

n,
1
2,u K−
−

1

uuKuf (cid:48).

(19)

Projection: The central step in Power EP is the projection. Obtaining the new approx-
imate un-normalised posterior q∗(f ) by minimising KL(˜p(f )||q∗(f )) would na¨ıvely appear
intractable. Fortunately,

=u|u)q∗(u),
Remark 1 Due to the structure of the approximate posterior, q∗(f ) = p(f
the objective, KL(˜p(f )||q∗(f )) is minimised when E
q∗(u)[φ(u)], where φ(u) =
{u, uu(cid:124)} are the suﬃcient statistics, that is when the moments at the pseudo-inputs are
matched.

˜p(f )[φ(u)] = E

This is the central result from which computational savings are derived. Furthermore, this
moment matching condition would appear to necessitate computation of a set of integrals
to ﬁnd the zeroth, ﬁrst and second moments. However, the technique known as ‘diﬀerenti-
ation under the integral sign’2 provides a useful shortcut that only requires one integral to
compute the log-normaliser of the tilted distribution, log ˜Zn = log E
\n(f )[pα(yn|fn)], before
q∗
diﬀerentiating w.r.t. the cavity mean to give

mu = m\

n
n
u + V\
ufn

d log ˜Zn
n
dm\
fn

;

Vu = V\

n
n
u + V\
ufn

d2 log ˜Zn
n
fn )2
d(m\

n
V\
fnu.

(20)

Update: Having computed the new approximate posterior, the approximate factor tn,new(u) =
q∗(f )/q∗
n(f ) can be straightforwardly obtained, resulting in,
\

1
T1,n,new = V−

n
u mu − (V\
u )−

1m\

n
u , T2,n,new = V−

n
u − (V\
u )−

1

1, zα

n = ˜ZneG

\n
∗ (u))

(q

(q∗(u)),

−G

2. In this case, the dominated convergence theorem can be used to justify the interchange of integration

and diﬀerentiation (see e.g. Brown, 1986).

13

Bui, Yan and Turner

where we have deﬁned the log-normaliser as the functional G( ˜N (u; z, T1, T2)) =
log (cid:82) ˜N (u; z, T1, T2)du. Remarkably, these results and eq. (20) reveals that T2,n,new is
a rank-1 matrix. As such, the minimal and simplest way to parameterise the approxi-
mate factor is tn(u) = znN (KfnuK−
uuu; gn, vn), where gn and vn are scalars, resulting in a
signiﬁcant memory saving and O(N M 2) cost.

1

In addition to providing the approximate posterior after convergence, Power EP also
provides an approximate log-marginal likelihood for model selection and hyper-parameter
optimisation,

log ZPEP = log

p(f )

tn(u)df = G(q∗(u)) − G(p∗(u)) +

log zn.

(21)

(cid:90)

(cid:89)

n

(cid:88)

n

Armed with these general results, we now consider the implications for Gaussian Process
regression.

3.6 Gaussian Regression case

When the model contains Gaussian likelihood functions, closed-form expressions for the
Power EP approximate factors at convergence can be obtained and hence the approximate
posterior:

tn(u) = N (KfnuK−

uuu; yn, αDfnfn + σ2
1

1
y), q(u) = N (u; Kuf K−

ﬀ y, Kuu − Kuf K−

ﬀ Kfu)

1

where Kﬀ = Qﬀ + αdiag(Dﬀ ) + σ2
yI and Dﬀ = Kﬀ − Qﬀ as deﬁned in section 2. These
analytic expressions can be rigorously proven to be the stable ﬁxed point of the Power EP
procedure using remark 1. Brieﬂy, assuming the factors take the form above, the natural
n(u) become,
parameters of the cavity q∗
\

n
1,u = T1,u − αγnynKfnuK−
T\

n
1
2,u = T2,u − αγnK−
uu, T\

1

uuKufnKfnuK−

1
uu,

(22)

n = αDfnfn + σ2
1
y. The subtracted quantities in the equations above are exactly
where γ−
the contribution the likelihood factor makes to the cavity distribution (see remark 1) so
n(u) (cid:82) p(fn|u)pα(yn|fn)dfn ∝ q∗(u). Therefore, the posterior
(cid:82) q∗
approximation remains unchanged after an update and the form for the factors above is the
ﬁxed point. Moreover, the approximate log-marginal likelihood is also analytically tractable,

n(f )pα(yn|fn)df
\

=u = q∗
\

log ZPEP = −

log(2π) −

log |Kﬀ | −

N
2

1
2

y(cid:124)K−

1
ﬀ y −

1
2

1 − α
2α

(cid:88)

n

log (cid:0)1 + αDfnfn/σ2

(cid:1) .

y

We now look at special cases and the correspondence to the methods discussed in section 2.

Remark 2 When α = 1 [EP], the Power EP posterior becomes the FITC posterior in
eq. (7) and the Power EP approximate marginal likelihood becomes the FITC marginal like-
lihood in eq. (8). In other words, the FITC approximation for GP regression is, surprisingly,
equivalent to running an EP algorithm for sparse GP posterior approximation to conver-
gence.

14

Unifying Gaussian Process Approximations

Remark 3 As α → 0 the approximate posterior and approximate marginal likelihood are
identical to that of the VFE approach in eqs. (9) and (10) (Titsias, 2009). This result uses
1 log(1 + x) = 1. So FITC and Titsias’s VFE approach employ the
the limit:
same form of pseudo-point approximation, but reﬁne it in diﬀerent ways.

limx

0 x−

→

Remark 4 For ﬁxed hyper-parameters, a single pass of Power EP is suﬃcient for conver-
gence in the regression case.

3.7 Extensions: Structured, Inter-domain and Multi-power Power EP

Approximations

The framework can now be generalised in three orthogonal directions:

1. enable structured approximations to be handled that retain more dependencies in the

spirit of PITC (see section 2.1)

of the approximate posterior

2. incorporate inter-domain pseudo-points thereby adding further ﬂexibility to the form

3. employ diﬀerent powers α for each factor (thereby enabling e.g. VFE updates to be

used for some data points and EP for others).

Given the groundwork above, these three extensions are straightforward. In order to handle
structured approximations, we take inspiration from PITC and partition the data into
B disjoint blocks yb = {yn}n
b (see section 2.1). Each PEP factor update will then
approximate an entire block which will contain a set of data points, rather than just a
single one. This is related to a form of EP approximation that has recently been used to
distribute Monte Carlo algorithms across many machines (Gelman et al., 2014; Xu et al.,
2014).

∈B

In order to handle inter-domain variables, we deﬁne a new domain via a linear transform
g(x) = (cid:82) dx(cid:48)W (x, x(cid:48))f (x(cid:48)) which now contains the pseudo-points g = {g
=u, u}. Choices for
W (x, x(cid:48)) include Gaussians or wavelets. These two extensions mean that the approximation
becomes,

p(f, g)

p(yb|f ) ≈ p(f, g)

tb(u) = q∗(f ).

(23)

(cid:89)

b

(cid:89)

b

Power EP is then performed using private powers αb for each data block, which is the third
generalisation mentioned above. Analytic solutions are again available (covariance matrices
now incorporate the inter-domain transform)

tb(u) = N (KfbuK−

uuu; yb, αbDfbfb + σ2
1

yI),

q(u) = N (u; Kuf K−

1

1
ﬀ y, Kuu − Kuf K−

ﬀ Kfu)

where Kﬀ = Qﬀ + blkdiag({αbDfbfb}B
yI and blkdiag builds a block-diagonal matrix
from its inputs. The approximate log-marginal likelihood can also be obtained in closed-
form,

b=1) + σ2

log ZPEP = −

log(2π) −

log |Kﬀ | −

N
2

1
2

y(cid:124)K−

1
ﬀ y +

1
2

(cid:88)

b

1 − αb
2αb

log (cid:0)I + αbDfbfb/σ2

y

(cid:1) .

15

Bui, Yan and Turner

Remark 5 When αb = 1 and W (x, x(cid:48)) = δ(x − x(cid:48)) the structured Power EP posterior
becomes the PITC posterior and the Power EP approximate marginal likelihood becomes
the PITC marginal likelihood. Additionally, when B = N we recover FITC as discussed in
section 3.6.

Remark 6 When αb → 0 and W (x, x(cid:48)) = δ(x − x(cid:48)) the structured Power EP posterior and
approximate marginal likelihood becomes identical to the VFE approach (Titsias, 2009).
This is a result of the equivalence of local and global exclusive KL divergence minimisation.
See appendix B for more details and ﬁg. 1 for more relationships.

3.8 Classiﬁcation

For classiﬁcation, the non-Gaussian likelihood prevents an analytic solution. As such, the
iterative Power EP procedure is required to obtain the approximate posterior. The pro-
jection step requires computation of the log-normaliser of the tilted distribution, log ˜Zn =
log E
\n(f )[pα(yn|f )] = log E
\n(fn)[Φα(ynfn)]. For general α, this quantity is not available in
q∗
q∗
closed form3. However, it involves a one-dimensional expectation of a non-linear function of
a normally-distributed random variable and, therefore, can be approximated using numeri-
cal methods, e.g. Gauss-Hermite quadrature. This procedure gives an approximation to the
expectation, resulting in an approximate update for the posterior mean and covariance. The
approximate log-marginal likelihood can also be obtained and used for hyper-parameter op-
timisation. As α → 0, it becomes the variational free-energy used in (Hensman et al., 2015)
which employs quadrature for the same purpose. These relationships are shown in ﬁg. 1
which also shows that inter-domain transformations and structured approximations have
not yet been fully explored in the classiﬁcation setting. In our view, the inter-domain gen-
eralisation would be a sensible one to pursue and it is mathematically and algorithmically
straightforward. The structured approximation variant is more complicated as it requires
multiple non-linear likelihoods to be handled at each step of EP. This will require further
approximation such as using Monte Carlo methods (Gelman et al., 2014; Xu et al., 2014).
In addition, when α = 1, M = N and the pseudo-points are at the training inputs, the
standard EP algorithm for GP classiﬁcation is recovered (Rasmussen and Williams, 2005,
sec. 3.6).

Since the proposed Power EP approach is general, an extension to other likelihood
functions is as simple as for VFE methods (Dezfouli and Bonilla, 2015). For example, the
multinomial probit likelihood can be handled in the same way as the binary case, where the
log-normaliser of the tilted distribution can be computed using a C-dimensional Gaussian
quadrature [C is the number of classes] (Seeger and Jordan, 2004) or nested EP (Riihim¨aki
et al., 2013).

3.9 Complexity

The computational complexity of all the regression and classiﬁcation methods described
in this section is O(N M 2) for training, and O(M 2) per test point for prediction. The
training cost can be further reduced to O(M 3), in a similar vein to the uncollapsed VFE

3. except for special cases, e.g. when α = 1 and Φ(x) is the probit inverse link function, Φ(x) =

(cid:82) x
−∞ N (a; 0, 1)da.

16

Unifying Gaussian Process Approximations

approach (Hensman et al., 2013, 2015), by employing stochastic updates of the poste-
rior and stochastic optimisation of the hyper-parameters using minibatches of data points
In particular, the Power EP update
(Hern´andez-Lobato and Hern´andez-Lobato, 2016).
steps in section 3.2 are repeated for only a small subset of training points and for only a
small number of iterations. The approximate log-marginal likelihood in eq. (21) is then
computed using this minibatch and optimised as if the Power EP procedure has converged.
This approach results in a computationally eﬃcient training scheme, at the cost of return-
ing noisy hyper-parameter gradients. In practice, we ﬁnd that the noise can be handled
using stochastic optimisers such as Adam (Kingma and Ba, 2015). In summary, given these
advances the general PEP framework is as scalable as variational inference.

4. Experiments

The general framework described above lays out a large space of potential inference algo-
rithms suggesting many exciting directions for innovation. The experiments considered in
the paper will investigate only one aspect of this space; how do algorithms that are interme-
diate between VFE (α = 0) and EP/FITC (α = 1) perform? Speciﬁcally, we will investigate
how the performance of the inference scheme varies as a function of α and whether this de-
pends on; the type of problem (classiﬁcation or regression); the dataset (synthetic datasets,
8 real world regression datasets and 6 classiﬁcation datasets); the performance metric (we
compare metrics that require point-estimates to those that are uncertainty sensitive). An
important by-product of the experiments is that they provide a comprehensive comparison
between the VFE and EP approaches which has been an important area of debate in its
own right.

The results presented below are compact summaries of a large number of experiments full
details of which are included in the appendix I (along with additional experiments). Python
and Matlab implementations are available at http://github.com/thangbui/sparseGP_
powerEP.

4.1 Regression on Synthetic Datasets

In the ﬁrst experiment, we investigate the performance of the proposed Power EP method
on toy regression datasets where ground truth is known. We vary α (from 0 VFE to 1
EP/FITC) and the number of pseudo-points (from 5 to 500). We use thirty datasets, each
comprising 1000 data points with ﬁve input dimensions and one output dimension, that
were drawn from a GP with an Automatic Relevance Determination squared exponential
kernel. A 50:50 train/test split was used. The hyper-parameters and pseudo-inputs were
found by optimising the PEP energy using L-BFGS with a maximum of 2000 function
evaluations. The performances are compared using two metrics: standardised mean squared
error (SMSE) and standardised mean log loss (SMLL) as described in (Rasmussen and
Williams, 2005, page 23). The approximate negative log-marginal likelihood (NLML) for
each experiment is also computed. The mean performance using Power EP with diﬀerent α
values and full GP regression is shown in ﬁg. 3. The results demonstrate that as M increases,
the SMLL and SMSE of the sparse methods approach that of full GP. Power EP with α = 0.8
or α = 1 (EP) overestimates the log-marginal likelihood when intermediate numbers of
pseudo-points are used, but the overestimation is markedly less when M = N = 500.

17

Bui, Yan and Turner

Importantly, however, an intermediate value of α in the range 0.5-0.8 seems to be best for
prediction on average, outperforming both EP and VFE.

Figure 3: The performance of various α values averaged over 30 trials. See text for more
details

4.2 Regression on Real-world Datasets

The experiment above was replicated on 8 UCI regression datasets, each with 20 train/test
splits. We varied α between 0 and 1, and M was varied between 5 and 200. Full details
of the experiments along with extensive additional analysis is presented in the appendices.
Here we concentrate on several key aspects. First we consider pairwise comparisons between
VFE (α → 0), Power EP with α = 0.5 and EP/FITC (α = 1) on both the SMSE and SMLL
evaluation metrics. Power EP with α = 0.5 was chosen because it is the mid-point between
VFE and EP and because settings around this value empirically performed the best on
average across all datasets, splits, numbers of inducing points, and evaluation metrics.

In ﬁg. 4A we plot (for each dataset, each split and each setting of M ) the evaluation
scores obtained using one inference algorithm (e.g. PEP α = 0.5) against the score obtained
using another (e.g. VFE α = 0). In this way, points falling below the identity line indicate
experiments where the method on the y-axis outperformed the method on the x-axis. These
results have been collapsed by forming histograms of the diﬀerence in the performance of
the two algorithms, such that mass to the right of zero indicates the method on the y-axis
outperformed that on the x-axis. The proportion of mass on each side of the histogram,
also indicated on the plots, shows in what fraction of experiments one method returns a
more accurate result than the other. This is a useful summary statistic, linearly related to

18

Unifying Gaussian Process Approximations

the average rank, that we will use to unpack the results. The average rank is insensitive to
the magnitude of the performance diﬀerences and readers might worry that this might give
an overly favourable view of a method that performs the best frequently, but only by a tiny
margin, and when it fails it does so catastrophically. However, the histograms indicate that
the methods that win most frequently tend also to ‘win big’ and ‘lose small’, although EP
is a possible exception to this trend (see the outliers below the identity line on the bottom
right-hand plot).

A clear pattern emerges from these plots. First PEP α = 0.5 is the best performing
approach on the SMSE metric, outperforming VFE 67% of the time and EP 78% of the
time. VFE is better than EP on the SMSE metric 64% of the time. Second, EP performs
the best on the SMLL metric, outperforming VFE 93% of the time and PEP α = 0.5 71%
of the time. PEP α = 0.5 outperforms VFE in terms of the SMLL metric 93% of the time.
These pairwise rank comparisons have been extended to other values of α in ﬁg. 5A.
Here, each row of the ﬁgure compares one approximation with all others. Horizontal bars
indicate that the methods have equal average rank. Upward sloping bars indicate the
method shown on that row has lower average rank (better performance), and downward
sloping bars indicate higher average rank (worse performance). The plots show that PEP
α = 0.5 outperforms all other methods on the SMSE metric, except for PEP α = 0.6 which
is marginally better. EP is outperformed by all other methods, and VFE only outperforms
EP on this metric. On the other hand, EP is the clear winner on the SMLL metric, with
performance monotonically decreasing with α so that VFE is the worst.

The same pattern of results is seen when we simultaneously compare all of the methods,
rather than considering sets of pairwise comparisons. The average rank plots shown in
ﬁg. 4B were produced by sorting the performances of the 8 diﬀerent approximating methods
for each dataset, split, and number of pseudo-points M and assigning a rank. These ranks
are then averaged over all datasets and their splits, and settings of M . PEP α = 0.5 is the
best for the SMSE metric, and the two worst methods are EP and VFE. PEP α = 0.8 is the
best for the SMLL metric, with EP and PEP α = 0.6 not far behind (when EP performs
poorly it can do so with a large magnitude, explaining the discrepancy with the pairwise
ranks).

There is some variability between individual datasets, but the same general trends are
clear: For MSE α = 0.5 is better than VFE on 6/8 datasets and EP on 8/8 datasets, whilst
VFE is better than EP on 3 datasets (the diﬀerence on the others being small). For NLL
EP is better than α = 0.5 on 5/8 datasets and VFE on 7/8 datasets, whilst α = 0.5 is better
than VFE on 8/8 datasets. Performance tends to increase for all methods as a function
of the number of pseudo-points M. The interaction between the choice of M and the best
performing inference method is often complex and variable across datasets making it hard
to give precise advice about selecting α in an M dependent way.

In summary, we make the following recommendations based on these results for GP
regression problems. For a MSE loss, we recommend using α = 0.5. For a NLL we recom-
mend using EP. It is possible that more ﬁne grained recommendations are possible based
upon details of the dataset and the computational resources available for processing, but
further work will be needed to establish this.

19

2
0

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 4: Pair-wise comparisons between Power EP with α = 0.5, EP (α = 1) and VFE (α → 0), evaluated on several regression
datasets and various settings of M . Each coloured point is the result for one split. Points that are below the diagonal line
illustrate the method on the y-axis is better than the method on the x-axis. The inset diagrams show the histograms of the
diﬀerence between methods (x-value − y-value), and the counts of negative and positive diﬀerences. Note that this indicates
pairwise ranking of the two methods. Positive diﬀerences mean the y-axis method is better than the x-axis method and vice
versa. For example, the middle, bottom plot shows EP is on average better than VFE.

Unifying Gaussian Process Approximations

Figure 5: Average ranking of various α values in the regression experiment, lower is better.
Top plots show the pairwise comparisons. Red circles denote rows being better than the
corresponding columns, and blue circles mean vice versa. Bottom plots show the ranks of
all methods when being compared together. Intermediate α values (not EP or VFE) are
best on average.

4.3 Binary Classiﬁcation

We also evaluated the Power EP method on 6 UCI classiﬁcation datasets, each has 20
train/test splits. The details of the datasets are included in appendix I.3. The datasets are
all roughly balanced, and the most imbalanced is pima with 500 positive and 267 negative

21

Bui, Yan and Turner

data points. Again α was varied between 0 and 1, and M was varied between 10 and 100.
We adopt the experimental protocol discussed in section 3.9, including: (i) not waiting for
Power EP to converge before making hyper-parameter updates, (ii) using minibatches of
data points for each Power EP sweep, (iii) parallel factor updates. The Adam optimiser was
used with default hyper-parameters to handle the noisy gradients produced by these ap-
proximations (Kingma and Ba, 2015). We also implemented the VFE approach of Hensman
et al. (2015) and include this in the comparison to the PEP methods. The VFE approach
should be theoretically identical to PEP with small α, however, we note that the results
can be slightly diﬀerent due to diﬀerences in the implementation – optimisation for VFE
vs. the iterative PEP procedure and we also note that each step of PEP only gets to see a
tiny fraction of each data point when α is small which can slow the learning speed. Similar
to the regression experiment, we compare the methods using the pairwise ranking plots on
the test error and negative log-likelihood (NLL) evaluation metrics.

In ﬁg. 6, we plot (for each dataset, each split and each setting of M ) the evaluation scores
using one inference algorithm against the score obtained using another [see section 4.2 for a
detailed explanation of the plots]. In contrast to the regression results in section 4.2, there
are no clear-cut winners among the methods. The test error results show that PEP α = 0.5
is marginally better than VFE and EP, while VFE edges EP out in this metric. Similarly,
all methods perform comparably on the NLL scale, except with PEP α = 0.5 outperforming
EP by a narrow magin (65% of the time vs. 35%)

We repeat the pairwise comparison above to all methods and show the results in ﬁg. 7.
The plots show that there is no conlusive winner on the test error metric, and VFE, PEP
α = 0.4 and PEP α = 0.5 have a slight edge over other α values on the NLL metric. Notably,
methods corresponding to bigger α values, such as PEP α = 0.8 and EP, are outperformed
by all other methods. Similar to the regression experiment, we observe the same pattern
of results when all methods are simultaneously compared, as shown in ﬁg. 7. However, the
large errorbars suggest the diﬀerence between the methods is small in both metrics.

There is some variability between individual datasets, but the general trends are clear
and consistent with the pattern noted above. For test error, PEP α = 0.5 is better than
VFE on 1/6 dataset and is better than EP on 3/6 datasets (the diﬀerences on the other
datasets are small). VFE outperforms EP on 2/6 datasets, while EP beats VFE on only
1/6 datasets. For NLL, PEP α = 0.5 only clearly outperforms VFE on 1/6 dataset, but
is worse compared to VFE on 1 dataset (the other 4 datasets have no clear winner). PEP
α = 0.5 is better than EP on 5/6 datasets and EP is better on the remaining dataset). EP
is only better than VFE on 2/6 datasets, and is outperformed by VFE on the other 4/6
datasets. The ﬁnding that PEP and VFE are slightly better than EP on the NLL metric
is surprising as we expected EP perform the best on the uncertainty sensitive metric (just
as was discovered in the regression case). The full results are included in the appendices
(see ﬁgs 25, 26 and 27). Similar to the regression case, we observe that as M increases, the
performance tends to be better for all methods and the diﬀerences between the methods
tend to become smaller, but we have not found evidence for systematic sensitivity to the
nature of the approximation.

In summary, we make the following recommendations based on these results for GP
classiﬁcation problems. For a raw test error loss and for NLL, we recommend using α = 0.5
(or α = 0.4). It is possible that more ﬁne grained recommendations are possible based upon

22

Unifying Gaussian Process Approximations

details of the dataset and the computational resources available for processing, but further
work will be needed to establish this.

23

2
4

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 6: Pair-wise comparisons between Power EP with α = 0.5, EP (α = 1) and VFE (α → 0), evaluated on several classiﬁcation
datasets and various settings of M . Each coloured point is the result for one split. Points that are below the diagonal line illustrate
the method on the y-axis is better than the method on the x-axis. The inset diagrams show the histograms of the diﬀerence
between methods (x-value − y-value), and the counts of negative and positive diﬀerences. Note that this indicates pairwise
ranking of the two methods. Positive diﬀerences means the y-axis method is better than the x-axis method and vice versa.

Unifying Gaussian Process Approximations

Figure 7: Average ranking of various α values in the classiﬁcation experiment, lower is
better. Top plots show the pairwise comparisons. Red circles denote rows being better
than the corresponding columns, and blue circles mean vice versa. Bottom plots show the
ranks of all methods when being compared together.
Intermediate α values (not EP or
VFE) are best on average.

5. Discussion

It is diﬃcult to identify precisely where the best approximation methods derive their advan-
tages, but here we will speculate. Since the negative variational free-energy is a lower-bound
on the log-marginal likelihood it has the enviable theoretical guarantee that pseudo-input

25

Bui, Yan and Turner

optimisation is always guaranteed to improve the estimate of the log marginal likelihood
and the posterior (as measured by the inclusive KL). The negative EP energy, in contrast,
is not generally a lower bound which can mean that pseudo-input optimisation drives the
solution to the point where the EP energy over-estimates the log marginal likelihood the
most, rather than to the point where the marginal likelihood and/or posterior estimate
is best. For this reason, we believe that variational methods are likely to be better than
EP if the goal is to derive accurate marginal likelihood estimates, or accurate predictive
distributions, for ﬁxed hyper-parameter settings. For hyper-parameter optimisation, things
are less clear cut since variational methods are biased away from the maximal marginal
likelihood, towards hyper-parameter settings for which the posterior approximation is ac-
curate. Often this bias is severe and also creates local-optima (Turner and Sahani, 2011).
So, although EP will generally also be biased away from the maximal marginal likelihood
and potentially towards areas of over-estimation, it can still outperform variational meth-
ods. Superposed onto these factors, is a general trend for variational methods to minimise
MSE / classiﬁcation error-rate and EP methods to minimise negative log-likelihood, due
to the form of their respective energies (the variational free-energy includes the average
training MSE in the regression case, for example). Intermediate methods will blend the
strengths and weaknesses of the two extremes. It is interesting that values of α around a
half are arguably the best performing on average. Similar empirical conclusions have been
made elsewhere Minka (2005); Hern´andez-Lobato et al. (2016); Depeweg et al. (2016). In
this case, the alpha-divergence interpretation of Power EP shows that it is minimising the
Hellinger distance whose square root is a valid distance metric. Further experimental and
theoretical work is required to clarify these issues.

The results presented above employed (approximate) type-II maximum likelihood ﬁtting
of the hyper-parameters. This estimation method is known in some circumstances to overﬁt
the data. It is therefore conceivable therefore that pseudo-point approximations, which have
a tendency to encourage under-ﬁtting due to their limited representational capacity, could
be beneﬁcial due to them mitigating overﬁtting. We do not believe that this is a strong eﬀect
in the experiments above. For example, in the synthetic data experiments the NLML, SMSE
and SMLL obtained from ﬁtting the unapproximated GP were similar to those obtained
using the GP from which the data were generated, indicating that overﬁtting is not a strong
eﬀect (see ﬁg. 9 in the appendix). It is true that EP and α = 0.8 over-estimates the marginal
likelihood in the synthetic data experiments, but this is a distinct eﬀect from over-ﬁtting
which would, for example, result in overconﬁdent predictions on the test dataset. The SMSE
and SMLL on the training and test sets, for example, are similar which is indicative of a
well-ﬁt model. It would be interesting to explore distributional hyper-parameter estimates
(see e.g. Piironen and Vehtari, 2017) that employ these pseudo-point approximations.

One of the features of the approximate generative models introduced in section 2.1 for
regression, is that they contain input-dependent noise, unlike the original model. Many
datasets contain noise of this sort and so approximate models like FITC and PITC, or
models in which the observation noise is explicitly modelled are arguably more appropri-
ate than the original unapproximated regression model (Snelson, 2007; Saul et al., 2016).
Motivated by this train of reasoning, Titsias (2009) applied the variational free-energy ap-
proximation to the FITC generative model an approach that was later generalised by Hoang
et al. (2016) to encompass a more general class of input dependent noise, including Markov

26

Unifying Gaussian Process Approximations

structure (Low et al., 2015). Here the insight is that the resulting variational lower bound
separates over data points (Hensman et al., 2013) and is, therefore, amenable to stochastic
optimisation using minibatches unlike the marginal likelihood. In a sense, these approaches
unify the approximate generative modelling approach, including the FITC and PITC vari-
ants, with the variational free-energy methods. Indeed, one approach is to posit the desired
form of the optimal variational posterior, and to work backwards from this to construct
the generative model implied (Hoang et al., 2016). However, these approaches are quite
diﬀerent from the one described in this paper where FITC and PITC are shown to emerge
in the context of approximating the original unapproximated GP regression model using
Power EP. Indeed, if the goal really is to model input dependent noise, it is not at all clear
that generative models like FITC are the most sensible. For example, FITC uses a single
set of hyper-parameters to describe the variation of the underlying function and the input
dependent noise.

6. Conclusion

This paper provided a new unifying framework for GP pseudo-point approximations based
on Power EP that subsumes many previous approaches including FITC, PITC, DTC, Tit-
sias’s VFE method, Qi et al’s EP method, and inter-domain variants. It provided a clean
computational perspective on the seminal work of Csat´o and Opper that related FITC to
EP, before extending their analysis signiﬁcantly to include a closed form Power EP marginal
likelihood approximation for regression, connections to PITC, and further results on clas-
siﬁcation and GPSSMs. The new framework was used to devise new algorithms for GP
regression and GP classiﬁcation. Extensive experiments indicate that intermediate values
of Power EP with the power parameter set to α = 0.5 often outperform the state-of-the-
art EP and VFE approaches. The new framework suggests many interesting directions
for future work in this area that we have not explored, for example, extensions to online
inference, combinations with special structured matrices (e.g. circulant and Kronecker struc-
ture), Bayesian hyper-parameter learning, and applications to richer models. The current
work has only scratched the surface, but we believe that the new framework will form a
useful theoretical foundation for the next generation of GP approximation schemes.

Acknowledgments

The authors would like to thank Prof. Carl Edward Rasmussen, Nilesh Tripuraneni,
Matthias Bauer, James Hensman, and Hugh Salimbeni for insightful comments and dis-
cussion. TDB thanks Google for funding his European Doctoral Fellowship. RET thanks
EPSRC grants EP/G050821/1, EP/L000776/1 and EP/M026957/1.

Appendix A. A uniﬁed objective for un-normalised KL variational

free-energy methods

Here we show that performing variational inference by optimising the un-normalised KL
naturally leads to a single objective for both the approximation to the joint distribution,
q∗(f |θ) and the hyper-parameters θ.

27

Bui, Yan and Turner

The un-normalised KL is given by

(cid:90)

KL(q∗(f |θ)||p(f, y|θ)) =

q∗(f |θ)
p(f, y|θ)
This is intractable as it includes the marginal likelihood p(y|θ) = (cid:82) p(f, y|θ)df . However,
since we are interested in minimising this objective with respect to q∗(f |θ) we can ignore
the intractable term,

(p(f, y|θ) − q∗(f |θ)) df.

q∗(f |θ) log

df +

(24)

(cid:90)

argmin
q∗(f
θ)

KL(q∗(f |θ)||p(f, y|θ)) = argmax
θ)

q∗(f

|

(cid:0)p(y|θ) − KL(q∗(f |θ)||p(f, y|θ))(cid:1)

(cid:18)(cid:90)

= argmax
q∗(f
θ)

|

|

q∗(f |θ) log

df +

q∗(f |θ)df

.

(cid:90)

p(f, y|θ)
q∗(f |θ)

(25)

(cid:19)

(26)

In other words, we have turned the unnormalised KL into a tractable lower-bound of the
marginal likelihood G(q∗(f |θ), θ) = p(y|θ)−KL(q∗(f |θ)||p(f, y|θ)). The structure of this new
lower-bound can be understood by decomposing the approximation to the joint distribution
into a normalised posterior approximation q(f |θ) and an approximation to the marginal
likelihood, ZVFE, that is q∗(f |θ) = ZVFEq(f |θ).

G(ZVFEq(f |θ), θ) = ZVFE

1 − log ZVFE +

q(f |θ) log

(27)

(cid:18)

(cid:90)

(cid:19)

p(f, y|θ)
q(f |θ)

df

We can see that optimising the lower-bound with respect to θ is equivalent to optimising the
standard variational free-energy F(q(f |θ), θ) = (cid:82) q(f |θ) log p(f,y
θ)
θ) df . Moreover, optimising
|
q(f
|
for ZVFE recovers Zopt

VFE = exp(F(q(f |θ), θ)). Substituting this back into the bound

G(Zopt

VFEq(f |θ), θ) = Zopt

VFE = exp(F(q(f |θ), θ)).

(28)

In other words, the new collapsed bound is just the exponential of the original variational
free-energy and optimising the collapsed bound for θ is equivalent to optimising the approx-
imation to the marginal likelihood.

Appendix B. Global and local inclusive KL minimisations

In this section, we will show that optimising a single global inclusive KL-divergence,
KL(q||p),
is equivalent to optimising a sum of a set of local inclusive KL-divergence,
KL(q||˜p), where p, q and ˜p are the exact posterior, the approximate posterior and the tilted
distribution accordingly. Without loss of generality, we assume that p(θ) = (cid:81)
n fn(θ) ≈
(cid:81)
n tn(θ) = q(θ), that is the exact posterior is a product of factors, {fn(θ)}n, each of which
is approximated by an approximate factor tn(θ). Substituting these distributions into the
global KL-divergence gives,

KL(q(θ)||p(θ)) =

dθq(θ) log

(cid:90)

(cid:90)

=

dθq(θ) log

q(θ)
p(θ)
(cid:81)
(cid:81)

n tn(θ)
n fn(θ)

28

Unifying Gaussian Process Approximations

(cid:21)

=n ti(θ)
=n ti(θ)

i

i

(cid:20) (cid:81)
(cid:81)

(cid:81)

(cid:81)

n

(cid:81)

(cid:81)

n
i ti(θ)]

n tn(θ)
n fn(θ)
n[(cid:81)
(cid:81)
n[fn(θ) (cid:81)
(cid:81)
[fn(θ) (cid:81)

=n ti(θ)]

i
i ti(θ)

=n ti(θ)

i

=

dθq(θ) log

(cid:90)

(cid:90)

=

dθq(θ) log

(cid:81)

(cid:90)

(cid:88)

=

=

n
(cid:88)

n

dθq(θ) log

KL(q(θ)||˜pn(θ)),

which means running the EP procedure, where we use KL(q(θ)||˜pn(θ)) in place of
KL(˜pn(θ)||q(θ)), is equivalent to the VFE approach which optimises a single global KL-
divergence, KL(q(θ)||p(θ)).

Appendix C. Some relevant linear algebra and function expansion

identities

The Woodbury matrix identity or Woodbury formula is:

(A + U CV )−

1 = A−

1 − A−

1U (C−

1 + V A−

1U )−

1V A−

1.

(30)

In general, C need not be invertible, we can use the Binomial inverse theorem,

(A + U CV )−

1 = A−

1 − A−

1U C(C + CV A−

1U C)−

1CV A−

1.

(31)

When C is an identity matrix and U and V are vectors, the Woodbury identity can be

shortened and become the Sherman-Morrison formula,

(A + uv

(cid:124)

)−

1 = A−

1 −

1uv(cid:124)A−
1
A−
1 + v(cid:124)A−
1u

.

Another useful identity is the matrix determinant lemma,

det(A + uv

) = (1 + v

(cid:124)

(cid:124)

A−

1u)det(A).

The above theorem can be extend for matrices U and V ,

det(A + U V

) = det(I + V

(cid:124)

(cid:124)

A−

1U )det(A).

We also make use of the following Maclaurin series,

exp(x) = 1 + x +

+

+ · · ·

and log(1 + x) = x −

+

+ · · · .

x3
3!

x2
2!

x3
3

x2
2

29

(29)

(32)

(33)

(34)

(35)

(36)

Bui, Yan and Turner

Appendix D. KL minimisation between Gaussian processes and moment

matching

The diﬃcult step of Power-EP is the projection step, that is how to ﬁnd the posterior
approximation q(f ) that minimises the KL divergence, KL(˜p(f )||q(f )), where ˜p(f ) is the
tilted distribution. We have chosen the form of the approximate posterior

q(f ) = p(f

=u|u)q(u) = p(f

=u|u)

exp(θ

(cid:124)
uφ(u))

,

Z(θu)

where Z(θu) = (cid:82) exp(θ
minimisation objective as follows,

(cid:124)
uφ(u))du to ensure normalisation. We can then write the KL

FKL = KL(˜p(f )||q(f ))
˜p(f )
q(f )

˜p(f ) log

=

(cid:90)

df

= (cid:104)log ˜p(f )(cid:105)˜p(f ) − (cid:104)log p(f

=u|u)(cid:105)˜p(f ) − θ

(cid:124)
u(cid:104)φ(u)(cid:105)˜p(f ) + log Z(θu).

=u|u) is the prior conditional distribution, the only free parameter that controls
Since p(f
our posterior approximation is θu. As such, to ﬁnd θu that minimises FKL, we ﬁnd the
gradient of FKL w.r.t θu and set it to zero,

0 =

dFKL
dθu

= −(cid:104)φ(u)(cid:105)˜p(f ) +

d log Z(θu)
dθu

= −(cid:104)φ(u)(cid:105)˜p(f ) + (cid:104)φ(u)(cid:105)q(u),

therefore, (cid:104)φ(u)(cid:105)˜p(f ) = (cid:104)φ(u)(cid:105)q(u). That is, though we are trying to perform the KL min-
imisation between two Gaussian processes, due to the special form of the posterior approx-
imation, it is suﬃcient to only match the moments at the inducing points u.4

Appendix E. Shortcuts to the moment matching equations

The most crucial step in Power-EP is the moment matching step as discussed above. This
step can be done analytically for the Gaussian case, as the mean and covariance of the
approximate posterior can be linked to the cavity distribution as follows,

mu = m\

n
n
u + V\
uf

,

d log Ztilted,n
n
dm\
f
d2 log Ztilted,n
dm\
f

n,2

n
n
u + V\
Vu = V\
uf

n
V\
f u,

where Ztilted,n is the normaliser of the tilted distribution,

(cid:90)

Ztilted,n =

q\

n(f )p(yn|f )df

4. We can show that this condition gives the minimum of FKL by computing the second derivative.

30

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

Unifying Gaussian Process Approximations

(cid:90)

(cid:90)

=

=

q\

n(f )p(yn|fn)df

q\

n(fn)p(yn|fn)dfn.

n
n
u + V\
mu = m\
ufn

,

d log Ztilted,n
n
dm\
fn
d2 log Ztilted,n
dm\
fn

n,2

n
n
u + V\
Vu = V\
ufn

n
V\
fnu.

In words, Ztilted,n only depends on the marginal distribution of the cavity process, q\
simplifying the moment matching equations above,

n(fn),

n
n
1
u K−
ufn = V\
We can rewrite the cross-covariance V\

uuKufn. We also note that, m\

n
fn =

n
1
uum\
KfnuK−
u , resulting in,

d log Ztilted,n
n
dm\
u

=

d log Ztilted,n
n
dm\
fn

1

K−

uuKufn,

d log Ztilted,n
n
dV\
u

= K−

uuKufn

1

d2 log Ztilted,n
dm\
fn

n,2

1
KfnuK−
uu.

Substituting these results back in eqs. 48 and 49, we obtain

n
u + V\
mu = m\
u

n

,

d log Ztilted,n
n
dm\
u
d2 log Ztilted,n
dm\
u

n,2

n
n
u + V\
Vu = V\
u

n
V\
u .

Therefore, using eqs. 48 and 49, or eqs. 52 and 53 are equivalent in our approximation

settings.

Appendix F. Full derivation of the Power-EP procedure

We provide the full derivation of the Power-EP procedure in this section. We follow the
derivation in (Qi et al., 2010) closely, but provide a clearer exposition and details how to
get to each step used in the implementation, and how to handle powered/fractional deletion
and update in Power-EP.

F.1 Optimal factor parameterisation

We start by deﬁning the approximate factors to be in natural parameter form as this makes
it simple to combine and delete them, tn(u) = ˜N (u; zn, T1,n, T2,n) = zn exp(u(cid:124)T1,n −
2 u(cid:124)T2,nu). We initially consider full rank T2,n, but will show that the optimal form is rank
1
1.

31

Bui, Yan and Turner

The next goal is to relate these parameters to the approximate GP posterior. The
n T1,n
n T2,n. This induces an approximate GP posterior with mean and

approximate posterior over the pseudo-outputs has natural parameters T1,u = (cid:80)
and T2,u = K−
covariance function,

uu + (cid:80)

1

1
1
2,uT1,u = Kfuγ
mf = KfuK−
uuT−
1
1
2,uK−
uuT−
Vﬀ (cid:48) = Kﬀ (cid:48) − Qﬀ (cid:48) + KfuK−

1

uuKuf (cid:48) = Kﬀ (cid:48) − KfuβKuf (cid:48).

(54)

(55)

where γ and β are likelihood-dependent terms we wish to store and update using PEP; γ
and β fully specify the approximate posterior.

Deletion step: The cavity for data point n, q\

n(u), has a similar form to
n
1,u = T1,u − αT1,n
the posterior, but the natural parameters are modiﬁed by the deletion, T\

n(f ) ∝ q∗(f )/tα

n
2,u = T2,u − αT2,n, yielding a new mean and covariance function
and T\

(56)

nKuf (cid:48).

uuKuf (cid:48) = Kﬀ (cid:48) − Kfuβ\

n
n,
1
n
1,u = Kfuγ\
2,u T\
−

n
1
uuT\
f = KfuK−
m\
n
1
1
uuT\n, −12,uK−
ﬀ (cid:48) = Kﬀ (cid:48) − Qﬀ (cid:48) + KfuK−
V \
Projection step: The central step in Power EP is the projection step. Obtaining the new
approximate unormalised posterior q∗(f ) such that KL(˜p(f )||q∗(f )) is minimised would
na¨ıvely appear intractable. Fortunately, as shown in the previous section, because of the
=u|u)q(u), the objective, KL(˜p(f )||q∗(f ))
structure of the approximate posterior, q(f ) = p(f
is minimised when E
q(u)[φ(u)], where φ(u) are the suﬃcient statistics, that is
when the moments at the pseudo-inputs are matched. This is the central result from which
computational savings are derived. Furthermore, this moment matching condition would
appear to necessitate computation of a set of integrals to ﬁnd the zeroth, ﬁrst and second
moments. Using results from the previous section simpliﬁes and provides the following
shortcuts,

˜p(f )[φ(u)] = E

(57)

n
n
u + V\
mu = m\
ufn

n
n
Vu = V\
u + V\
ufn

n
V\
fnu.

d log ˜Zn
n
dm\
fn
d2 log ˜Zn
n
fn )2
d(m\

where log ˜Zn = log E

q\n(f )[pα(yn|fn)] is the log-normaliser of the tilted distribution.

Update step: Having computed the new approximate posterior, the fractional approx-

imate factor tn,new(u) = q∗(f )/q\

n(f ) can be straightforwardly obtained, resulting in,

T1,n,new = V−

1

n
1
n,
u m\
u mu − V\
−
u
1

n,

1

u − V\
T2,n,new = V−
u
n = ˜Zn exp(G
zα
\n
∗ (u)
q

−

− Gq∗(u)),
(u;z,T1,T2) = (cid:82) ˜N (u; z, T1, T2)du. Let d1 = d log ˜Zn

where G ˜
N

eq. (30) and eq. (59), we have,

and d2 = d2 log ˜Zn

)2 . Using

d(m

\n
fn

dm

\n
fn

1
V−

u − V\
u

n,

1

−

= −V\

n,
1
u V\
−

n
ufn

n
1
fnuV\
2 + V\
d−

n
n,
1
u V\
−
ufn

n
fnuV\
V\
u

n,

1

−

(63)

(cid:105)−
1

(cid:104)

32

(58)

(59)

(60)

(61)

(62)

Unifying Gaussian Process Approximations

n
1
Let vn = α(−d−
fnuV\
2 − V\
eq. (61) gives

n
n,
1
ufn), and wn = V\
u V\
−

1
n,
u V\
−

n
ufn. Combining eq. (63) and

1
T2,n,new = wnαv−

n w(cid:124)

n

At convergence, we have tn(u)α = tn,new(u), hence T2,n = wnv−
1
n w
optimally a rank-1 matrix. Note that,

(cid:124)
n. In words, T2,n is

wn = V\

n
n,
1
u V\
−
ufn

= (Kuu − Kuuβ\

1
= K−
1
= K−

uu(I − Kuuβ\
uuKufn.

nKuu)−
n)−

1(Kufn − Kuuβ\
n)Kufn

1(I − Kuuβ\

nKufn)

Using eq. (58) an eq. (64) gives,

1
V−

1

n,

n w(cid:124)
n
n
u mu = (V\
u + V\
n)(m\
ufnd1)
u
n w(cid:124)
n,
1
n
n
n,
1
1
u V\
u + V\
nm\
u + wnαv−
u m\
−
−

+ wnαv−

= V\

−

1

n
1
ufnd1 + wnαv−

n w(cid:124)

n
nV\
ufnd1

Substituting this result into eq. (60),

T1,n,new = V−

n
n,
1
u m\
−
u

1

u mu − V\
n w(cid:124)
1
nm\
(cid:16)
w(cid:124)

1

= wnαv−

= wnαv−
n

n
u + V\

n w(cid:124)
n
n,
1
ufnd1 + wnαv−
u V\
−
(cid:17)
u + d1vn/α + w(cid:124)
n
nV\
.
ufnd1

n
nm\

1

n
nV\
ufnd1

Let T1,n,new = wnαv−

n gn, we obtain,

1

gn = −

+ Kfnuγ\

n.

d1
d2

At convergence, T1,n = wnv−
and T2,n at convergence,

1

n gn. Re-writing the form of the approximate factor using T1,n

tn(u) = ˜N (u; zn, T1,n, T2,n)
1
= zn exp(u(cid:124)T1,n −
2

= zn exp(u(cid:124)wnv−

1

n gn −

u(cid:124)wnv−

n w(cid:124)

1

nu)

u(cid:124)T2,nu)
1
2

As a result, the minimal and simplest way to parameterise the approximate factor is tn(u) =
˜znN (w
uuu; gn, vn), where gn and vn are scalars, resulting in a
signiﬁcant memory saving compared to the parameterisation using T1,n and T2,n.

(cid:124)
1
nu; gn, vn) = ˜znN (KfnuK−

33

(64)

(65)

(66)

(67)

(68)

(69)

(70)

(71)

(72)

(73)

(74)

(75)

(76)

(77)

Note that:

and

F.2 Projection

We now recall the update equations in the projection step (eqns. 58 and 59):

Bui, Yan and Turner

n
n
u + V\
mu = m\
ufnd1,
n
n
n
ufnd2V\
u + V\
Vu = V\
fnu.

mu = Kuuγ,
Vu = Kuu − KuuβKuu,

n
u = Kuuγ\
m\
n
u = Kuu − Kuuβ\
V\

n,

nKuu.

γ = K−

= K−

= γ\

β = K−

1
uumu
n
n
1
uu(m\
u + V\
ufnd1)
n
1
n + K−
uuV\
ufnd1, and
1
1
uu(Kuu − Vu)K−
uu

= K−

= β\

1

n
n
n
1
fnu)K−
ufnd2V\
u − V\
uu(Kuu − V\
uu
n
n
1
n − K−
1
fnuK−
ufnd2V\
uuV\
uu

(78)

(79)

(80)

(81)

(82)

(83)

(84)

(85)

(86)

(87)

(88)

(89)

Using these results, we can convert the update for the mean and covariance, mu and Vu,
into an update for γ and β,

F.3 Deletion step

Finally, we present how deletion might be accomplished. One direct approach to this step
is to divide out the cavity from the cavity, that is,

q\

n(f ) ∝

q(f )
tα
n(u)

p(f

=

=u|u)q(u)
tα
n(u)

= p(f

=u|u)q\

n(u).

(90)

Instead, we use an alternative using the KL minimisation as used in (Qi et al., 2010), by
realising that doing this will result in an identical outcome as the direct approach since
the factor and distributions are Gaussian. Furthermore, we can re-use results from the
projection and inclusion steps, by simply swapping the quantities and negating the site
approximation variance. In particular, we present projection and deletion side-by-side, to
facilitate the comparison,

Projection:

q(f ) ≈ q\

n(f )p(yn|fn)

(91)

34

Unifying Gaussian Process Approximations

Deletion:

q\

n(f ) ∝ q(f )

1
tα
n(u)

The projection step minimises the KL between the LHS and RHS while moment match-
n(f ), and thus

ing, to get q(f ). We would like to do the same for the deletion step to ﬁnd q\
reuse the same moment matching results for γ and β with some modiﬁcations.

Our task will be to reuse Equations 86 and 89, the moment matching equations in γ
and β. We have two diﬀerences to account for. Firstly, we need to change any uses of
the parameters of the cavity distribution to the parameters of the approximate posterior,
n
n to β. This is the equivalent of re-deriving the entire
V\
ufn to Vufn, γ\
projection operation while swapping the symbols (and quantities) for the cavity and the
full distribution. Secondly, the derivatives d1 and d2 are diﬀerent here, as

n to γ and β\

Now, we note

log ˜Zn = log

q(f )

1
tα
n(u)

df

1
tn(u)

∝

∝

N α(w

(cid:124)
nu; gn, vn)

1

(cid:16)

exp

− α

1
n (w

2 v−

1

= exp

(cid:18) 1
2
∝ N (w(cid:124)
nu; gn, −vn/α)

n (w(cid:124)

αv−

(cid:124)

nu − gn)2(cid:17)
(cid:19)

nu − gn)2

Then we obtain the derivatives of log ˜Zn

= − (cid:2)Kfn,uK−

u,uKu,fn − Kfn,uβKu,fn − vn/α(cid:3)−

1

1

˜d2 =

˜d1 =

d2 log ˜Zn
dm2
fn
d log ˜Zn
dmfn

= (Kfn,uγ − gn) ˜d2

Putting the above results together, we obtain,

γ\

β\

1

n = γ + K−
1
n = β − K−

uuVufn
uuVufn

˜d1, and
˜d2VfnuK−
1
uu

(cid:90)

1

35

F.4 Summary of the PEP procedure

We summarise here the key steps and equations that we have obtained, that are used in
the implementation:

(92)

(93)

(94)

(95)

(96)

(97)

(98)

(99)

(100)

(101)

Bui, Yan and Turner

1. Initialise the parameters: {gn = 0}N

n=1, {vn = ∞}N

n=1, γ = 0M

1 and β = 0M

×

M

×

2. Loop through all data points until convergence:

(a) Deletion step: ﬁnd γ\

n
n and β\

(b) Projection step: ﬁnd γ and β

(c) Update step: ﬁnd gn,new and vn,new

γ\

β\

1
n = γ + K−
n = β − K−

uuVufn
uuVufn

1

˜d1, and
˜d2VfnuK−
1
uu

γ = γ\

β = β\

n
1
n + K−
uuV\
ufnd1,
n
1
n − K−
ufnd2V\
uuV\

n
1
fnuK−
uu

gn,new = −

d1
d2
n
1
fnuV\
2 − V\
vn,new = −d−

+ Kfnuγ\

n,

n
n,
1
u V\
−
ufn

and parameters for the full factor,

1
vn ← (v−
gn ← vn(gn,newv−

n,new + (1 − α)v−
1

1
1
n )−
n,new + (1 − α)gnv−

1
n )

Appendix G. Power-EP energy for sparse GP regression and

classiﬁcation

The Power-EP procedure gives an approximate marginal likelihood, which is the negative
Power-EP energy, as follows,

F = G(q

(u)) − G(p
∗

∗

(u)) +

1
α

(cid:88)

n

(cid:104)
n
log Ztilted,n + G(q\
∗

(u)) − G(q

(cid:105)
(u))

∗

where G(q

(u)) is the log-normaliser of the approximate posterior, that is,

∗

G(q

(u)) = log

p(f

=u|u) exp(θ

(cid:124)
uφ(u))df

=udu

∗

= log

exp(θ

(cid:124)
uφ(u))du

=

log(2π) +

log |V| +

m(cid:124)V−

1m,

1
2

(cid:90)

(cid:90)

M
2

1
2

1
2

1
2

36

where m and V are the mean and covariance of the posterior distribution over u, respec-
tively. Similarly,

n
G(q\
∗

M
2

(u)) =

log(2π) +

log |Vcav,n| +

m(cid:124)

1
cav,nV−

cav,nmcav,n,

(114)

(102)

(103)

(104)

(105)

(106)

(107)

(108)

(109)

(110)

(111)

(112)

(113)

(115)

(116)

(117)

(118)

(119)

(120)

(121)

(122)

(123)

(124)

Unifying Gaussian Process Approximations

and G(p
∗

(u)) =

M
2

log(2π) +

log |Kuu|.

1
2

Finally, log Ztilted,n is the log-normalising constant of the tilted distribution,

log Ztilted = log

qcav(f )pα(yn|f )df

= log

p(f

=u|u)qcav(u)pα(yn|f )df

=udu

= log

p(fn|u)qcav(u)pα(yn|fn)dfndu

(cid:90)

(cid:90)

(cid:90)

Next, we can write down the form of the natural parameters of the approximate posterior

and the cavity distribution, based on the approximate factor’s parameters, as follows,

V−

1 = K−

1
uu +

(cid:88)

(cid:124)
wiτiw
i

V−

1m =

i
wiτi ˜yi

(cid:88)

i

1

V−
cav,n = V−
1mcav,n = V−

1 − αwnτnw(cid:124)
n
1m − αwnτngn

Vcav,n−

Vcav,n = V +

(cid:124)
Vwnατnw
nV
(cid:124)
nατnVwn
1 − w

.

1
Note that τi := v−
i

. Using eq. (32) and eq. (121) gives,

Using eq. (33) and eq. (121) gives,

log det(Vcav,n) = log det(V) − log(1 − w(cid:124)

nατnVwn).

Subsituting eq. (123) and eq. (124) back to eq. (114) results in,

n
G(q\
∗

(u)) =

log(2π) +

1
2
log(1 − w(cid:124)

log det(V) +

nατnVwn) +

1
m(cid:124)V−
1m
2
(cid:124)
m(cid:124)wnατnw
nm
(cid:124)
1 − w
nατnVwn

1
2

M
2
1
2
1
2

−

+

gnατnw(cid:124)

nVcav,nwnατngn − gnατnw(cid:124)

nVcav,nV−

1m

(125)

We now plug the above result back into the approximate marginal likelihood, yeilding,

F =

log |V| +

m(cid:124)V−

1m −

log |Kuu| +

1
2

(cid:20)

(cid:88)

+

−

1
2α

log(1 − w(cid:124)

nατnVwn) +

(cid:88)

1
α

log Ztilted,n

n
(cid:124)
m(cid:124)wnτnw
nm
(cid:124)
nατnVwn
1 − w

1
2

(cid:21)

1
2

n

+

1
2

gnτnw(cid:124)

nVcav,nwnατngn − gnτnw(cid:124)

nVcav,nV−

1m

(126)

1
2

37

Bui, Yan and Turner

1
2

1
2

G.1 Regression

We have shown in the previous section that the ﬁxed point solution of the Power-EP it-
n = dn =
erations can be obtained analytically for the regression case, gn = yn and τ −
uuKufn) + σ2
1
α(Kfnfn − KfnuK−
y. Crucially, we can obtain a closed form expression for
log Ztilted,n,

1

log Ztilted,n = −

log(2πσ2

y) +

log(σ2

y) −

log(αvn + σ2

y) −

1
2

1
2

(yn − µn)2
vn + σ2
y/α

(127)

α
2

σ2
y
where µn = w
α + w
−
therefore simplify the approximate marginal likelihood F further,

1m − wnατnyn) and vn =

(cid:124)
nmcav = w

(cid:124)
nVcav(V−

dn

(cid:124)
nVcavwn. We can

F =

log |V| +

m(cid:124)V−

1m −

log |Kuu| +

= −

log(2π) −

log |D + Qﬀ | −

yT (D + Qﬀ )−

1y −

(cid:20)

(cid:88)

n

−

1
2

log(2πσ2

y) +

log σ2

y −

log dn −

1
2α

1
2α

(cid:21)

y2
n
2dn

1 − α
2α

(cid:88)

n

log(

),

dn
σ2
y

(128)

1
2

N
2

1
2

1
2

1

where Qﬀ = KfuK−

uuKuf and D is a diagonal matrix, Dnn = dn.

When α = 1, the approximate marginal likelihood takes the same form as the FITC

1
2

1
2

marginal likelihood,

F = −

1
2
uuKufn + σ2
1
where Dnn = dn = Kfnfn − KfnuK−
y.
When α tends to 0, we have,

log(2π) −

N
2

1
2

log |D + Qﬀ | −

yT (D + Qﬀ )−

1y

(129)

1 − α
2α

(cid:88)

n

lim
0
α
→

log(

) =

dn
σ2
y

1
2

(cid:88)

lim
0
α
→

n

log(1 + α gn
σ2
y

)

α

=

(cid:80)

n hn
2σ2
y

,

(130)

1
where hn = Kfnfn − KfnuK−

uuKufn. Therefore,

F = −

log(2π) −

log |σ2

yI + Qﬀ | −

yT (σ2

yI + Qﬀ )−

1y −

(131)

1
2

(cid:80)

n hn
2σ2
y

,

N
2

which is the variational lower bound of Titsias (Titsias, 2009).

G.2 Classiﬁcation

In contrast to the regression case, the approximate marginal likelihood for classiﬁcation
cannot be simpliﬁed due to the non-Gaussian likelihood. Speciﬁcally, log Ztilted,n is not
analytically tractable, except when α = 1 and the classiﬁcation link function is the Gaus-
sian CDF. However, this quantity can be evaluated numerically, using sampling or Gauss-
Hermite quadrature, since it only involves a one-dimensional integral.

We now consider the case when α tends to 0 and verify that in such case the approxi-
mate marginal likelihood becomes the variational lower bound. We ﬁrst ﬁnd the limits of
individual terms in eq. (126):

−

1
2α

lim
0
α
→

log(1 − w(cid:124)

nατnVwn) =

w(cid:124)

nτnVwn

1
2

(132)

38

Unifying Gaussian Process Approximations

m(cid:124)wnτnw(cid:124)

nm

1
2
gnτnw(cid:124)

1
2

(cid:124)
m(cid:124)wnτnw
nm
(cid:124)
nατnVwn
1 − w

nVcav,nwnατngn

=

1
2

= 0

(cid:12)
(cid:12)
(cid:12)
(cid:12)α=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)α=0
(cid:12)
(cid:12)
1m
(cid:12)
(cid:12)α=0

−gnτnw(cid:124)

nVcav,nV−

= −gnτnw(cid:124)

nm.

We turn our attention to log Ztilted,n. First, we expand pα(yn|fn) using eq. (35):
pα(yn|fn) = exp(α log p(yn|fn))

= 1 + α log p(yn|fn) + ξ(α2).

Substituting this result back into log Ztilted/α gives,
(cid:90)

log Ztilted =

log

p(fn|u)qcav(u)pα(yn|fn)dfndu

1
α

(cid:90)

(cid:20)

log

p(fn|u)qcav(u)[1 + α log p(yn|fn) + ξ(α2)]dfndu

log

1 + α

(cid:90)

(cid:21)
p(fn|u)qcav(u) log p(yn|fn)dfndu + α2ξ(1)

(cid:20)

(cid:90)

α

(cid:21)
p(fn|u)qcav(u) log p(yn|fn)dfndu + α2ξ(1)

p(fn|u)qcav(u) log p(yn|fn)dfndu + αξ(1).

(133)

(134)

(135)

(136)

(137)

(138)

(139)

(140)

(141)

(142)

Therefore,

1
2

+

1
2

+

1
2

1
α

lim
0
α
→

(cid:90)

log Ztilted =

p(fn|u)q(u) log p(yn|fn)dfndu.

(143)

Putting these results into eq. (126), we obtain,

F =

log |V| +

m(cid:124)V−

1m −

(cid:88)

n

1
2

w(cid:124)

nτnVwn +

1
2

log |Kuu|

1
2
m(cid:124)wnτnw(cid:124)

nm − gnτnw(cid:124)

nm +

(cid:90)

p(fn|u)q(u) log p(yn|fn)dfndu

=

log |V| +

m(cid:124)V−

log |Kuu| +

m(cid:124)

(V−

1 − K−

1

uu)m − m(cid:124)V−

1m

(cid:88)

n

1
2

w(cid:124)

nτnVwn +

p(fn|u)q(u) log p(yn|fn)dfndu

=

log |V| −

m(cid:124)K−
1

uum −

log |Kuu| +

w(cid:124)

nτnVwn +

p(fn|u)q(u) log p(yn|fn)dfndu.

(cid:90)

(cid:88)

n

(144)

1m −
(cid:90)

1
2

1
2

We now write down the evidence lower bound of the global variational approach of

Titsias (Titsias, 2009), as applied to the classiﬁcation case (Hensman et al., 2015),

FVFE = −KL(q(u)||p(u)) +

p(fn|u)q(u) log p(yn|fn)dfndu

(145)

1
2

(cid:88)

n

1
2

(cid:90)

(cid:88)

n

39

1
α
1
α
1
α
1
α
(cid:90)

=

=

=

=

1
2

1
2

1
2

Bui, Yan and Turner

where

−KL(q(u)||p(u)) = −

trace(K−

uuV) −

1

m(cid:124)K−
1

uum +

log |Kuu| +

log |V|

1
2
1
2

1
2

1
2
(cid:88)

n

−

1
2
m(cid:124)K−

M
2
1
2

= −

trace([V−

1 −

wnτnwn]V) −

1

uum +

M
2

1
2

log |Kuu| +

log |V|

1
2

=

trace(

wnτnwnV) −

log |Kuu| +

log |V|. (146)

(cid:88)

n

m(cid:124)K−
1

uum −

1
2

1
2

1
2

−

1
2

Therefore, FVFE is identical to the limit of the approximate marginal likelihood provided
by power-EP as shown in eq. (144).

Appendix H. The surrogate regression viewpoint

It was written in the main text that it is instructive to view the approximation using
pseudo-points as forming a surrogate exact Gaussian process regression problem such that
the posterior and the marginal likelihood of this surrogate problem are close to that of the
original intractable regression/classiﬁcation problem. This approximation view is useful and
could potentially be used for other intractable probabilistic model, despite that we have not
used this view in the practical implementation of the algorithms/PEP procedure discussed
in this paper. In this section, we detail the surrogate model and how the parameters of
this model can be tuned to match the approximate posterior and approximate marginal
likelihood.

We consider the exact GP regression problem with M surrogate observations ˜y that are
formed by linear combininations the pseudo-outputs and additive surrogate Gaussian noise,
˜y = ˜Wu + ˜Σ1/2(cid:15). The exact posterior and log marginal likelihood can be obtained for this
model as follows,

1(u; ˜W ˜Σ−

log(2π) −

log p(˜y) = −

˜p(u|y) = N −
M
2
1
˜y(cid:124) ˜Σ−
2

−

1
1 ˜y, K−
1
2
1
˜y(cid:124) ˜Σ−
2

uu + ˜W(cid:124) ˜Σ−

1 ˜W)
uu + ˜W(cid:124) ˜Σ−

1

(log |K−

1 ˜W| + log |K−

uu| + log | ˜Σ|)
1

1 ˜y −

1 ˜W(K−
1

uu + ˜W(cid:124) ˜Σ−

1 ˜W)−

1 ˜W(cid:124) ˜Σ−

1 ˜y,

(148)

where we have used the matrix inversion lemma and the matrix determinant lemma in the
1 denotes the Gaussian distribution with natural parameters.
equations above, and that N −
The aim is to show that we can use the above quantities is to match a given approximate
1) and an approximate marginal likelihood F, that is,
posterior q(u) = N −
˜p(u|y) = q(u) and log p(˜y) = F. Substituting the above results into the constraints leading
to the following simpliﬁed constraints:

1(u; S−

1m, S−

where c is a constant. Assume that R is invertible, we can simpliﬁed the above results
further,

1 ˜y = m
1 ˜W = R = K−
1

˜W ˜Σ−
˜W(cid:124) ˜Σ−
1 ˜y + log | ˜Σ| = c,

˜y(cid:124) ˜Σ−

1
uu − S−

˜Σ−

1/2 ˜y = R−

1/2m

40

(147)

(149)

(150)

(151)

(152)

Unifying Gaussian Process Approximations

˜Σ−

1/2 ˜W = R(cid:124)/2
log | ˜Σ| = d,

(153)

(154)

where d is a constant. We can choose ˜Σ, e.g. a diagonal matrix, that satisﬁes the third
equality above. Given ˜Σ, obtaining ˜y and ˜W from the ﬁrst two equalities is trivial.

Appendix I. Extra experimental results

I.1 Comparison between various α values on a toy regression problem

41

4
2

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 8: Results on a toy regression problem: Negative log-marginal likelihood, mean squared error and mean log-loss on the
test set for full Gaussian process regression on synthetic datasets with true hyper-parameters and hyper-parameters obtained
by type-2 ML. Each dot is one trial, i.e. one synthetic dataset. The results demonstrate that type-2 maximum likelihood on
hyper-parameters works well, despite being a little conﬁdent on the log-marginal likelihood on the train set.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

4
3

Figure 9: Results on a toy regression problem with 500 training points: Mean squared error and log-likelihood on train and
test sets on synthetic datasets with hyper-parameters obtained by type-2 ML. In this example, the test error is higher than the
training error, as measured by the mean squared error, because the test points and training points are relatively far apart, making
the prediction task on the training set easier (interpolation) than on the test set (extrapolation). This is consistent with the
results with more training points, shown in ﬁg. 10.

4
4

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 10: Results on a toy regression problem with 1000 training points: Mean squared error and log-likelihood on train and
test sets on synthetic datasets with hyper-parameters obtained by type-2 ML. See ﬁg. 9 for a discussion.

4
5

Figure 11: Results on a toy regression problem: Standardsised mean log-loss on the test set for various values of α and various
number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgure shows the mean for various
α, and the results using GP regression.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

4
6

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 12: Results on a toy regression problem: Standardsised mean squared error on the test set for various values of α and
various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgure shows the mean for
various α, and the results using GP regression.

4
7

Figure 13: Results on a toy regression problem: The negative log marginal likelihood of the training set after training for various
values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgure shows
the mean for various α, and the results using GP regression. Power EP with α close to 1 over-estimates the marginal-likelihood.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

I.2 Real-world regression

We include the details of the regression datasets in table 1 and several comparisons of α
values in ﬁgs. 17 to 22.

Bui, Yan and Turner

Dataset N train/test D

boston
concrete
energy
kin8nm
naval
yacht
power
red wine

455/51
927/103
691/77
7373/819
10741/1193
277/31
8611/957
1439/160

14
9
9
9
18
7
5
12

Table 1: Regression datasets

48

4
9

Figure 14: A comparison between Power-EP with α = 0.5 and VFE on several regression datasets, on two metrics SMSE (top
two rows) and SMLL (bottom two rows). The scatter plots show the performance of Power-EP (α = 0.5) vs VFE. Each point is
one split and points with lighter colours are runs with big M. Points that stay below the diagonal line show α = 0.5 is better than
VFE. The plots right underneat the scatter plots show the histogram of the diﬀerence between methods. Red means α = 0.5 is
better than VFE.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

5
0

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 15: A comparison between EP and VFE on several regression datasets, on two metrics SMSE (top two rows) and SMLL
(bottom two rows). See ﬁg. 14 for more details about the plots.

5
1

Figure 16: A comparison between Power-EP with α = 0.5 and EP on several regression datasets, on two metrics SMSE (top two
rows) and SMLL (bottom two rows). See ﬁg. 14 for more details about the plots.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

5
2

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 17: Results on real-world regression problems: Negative training log-marginal likelihood for diﬀerent datasets, various
values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgures
show the mean for various α for comparison. Lower is better [however, lower could mean overestimation].

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

5
3

Figure 18: Results on real-world regression problems: Negative training log-marginal likelihood for diﬀerent datasets, various
values of α and various number of pseudo-points M , averaged over 20 splits. Lower is better [however, lower could mean
overestimation].

5
4

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 19: Results on real-world regression problems: Standardised mean squared error on the test set for diﬀerent datasets,
various values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost
ﬁgures show the mean for various α for comparison. Lower is better.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

5
5

Figure 20: Results on real-world regression problems: Standardised mean squared error on the test set for diﬀerent datasets,
various values of α and various number of pseudo-points M , averaged over 20 splits. Lower is better.

5
6

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 21: Results on real-world regression problems: Standardised mean log loss on the test set for diﬀerent datasets, various
values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgures
show the mean for various α for comparison. Lower is better.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

5
7

Figure 22: Results on real-world regression problems: Standardised mean log loss on the test set for diﬀerent datasets, various
values of α and various number of pseudo-points M , averaged over 20 splits. Lower is better.

Bui, Yan and Turner

I.3 Real-world classiﬁcation

It was demonstrated in (Hern´andez-Lobato and Hern´andez-Lobato, 2016; Hensman et al.,
2015) that, once optimised, the pseudo points tend to concentrate around the decision
boundary for VFE, and spread out to cover the data region in EP. Figure 23 illustrates the
same eﬀect as α goes from close to 0 (VFE) to 1 (EP).

Figure 23: The locations of pseudo data points vary with α.. Best viewed in colour.

We include the details of the classiﬁcation datasets in table 2 and several comparisons

of α values in ﬁgs. 27 to 30.

58

Unifying Gaussian Process Approximations

Dataset

N train/test D N positive/negative

australian
breast
crabs
iono
pima
sonar

621/69
614/68
180/20
315/35
690/77
186/21

15
11
7
35
9
61

222/468
239/443
100/100
126/224
500/267
111/96

Table 2: Classiﬁcation datasets

59

6
0

Figure 24: A comparison between Power-EP with α = 0.5 and VFE on several classiﬁcation datasets, on two metrics: classiﬁcation
error (top two rows) and NLL (bottom two rows). See ﬁg. 14 for more details about the plots.

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

6
1

Figure 25: A comparison between EP and VFE on several classiﬁcation datasets, on two metrics: classiﬁcation error (top two
rows) and NLL (bottom two rows). See ﬁg. 14 for more details about the plots.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

6
2

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 26: A comparison between Power-EP with α = 0.5 and EP on several classiﬁcation datasets, on two metrics: classiﬁcation
error (top two rows) and NLL (bottom two rows). See ﬁg. 14 for more details about the plots.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

6
3

Figure 27: Results on real-world classiﬁcation problems: Classiﬁcation error rate on the test set for diﬀerent datasets, various
values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgures
show the mean for various α for comparison. Lower is better.

6
4

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 28: Results on real-world classiﬁcation problems: Classiﬁcation error rate on the test set for diﬀerent datasets, various
values of α and various number of pseudo-points M , averaged over 20 splits. Lower is better.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

6
5

Figure 29: Results on real-world classiﬁcation problems: Average negative log-likelihood on the test set for diﬀerent datasets,
various values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost
ﬁgures show the mean for various α for comparison. Lower is better.

6
6

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 30: Results on real-world classiﬁcation problems: Average negative log-likelihood on the test set for diﬀerent datasets,
various values of α and various number of pseudo-points M , averaged over 20 splits. Lower is better.

Unifying Gaussian Process Approximations

I.4 Binary classiﬁcation on even/odd MNIST digits

Figure 31: The test error and log-likelihood of the MNIST binary classiﬁcation task
(M=100).

Figure 32: The test error and log-likelihood of the MNIST binary classiﬁcation task
(M=200).

I.5 When M = N and α = 1, do we recover EP for GPC (Rasmussen and

Williams, 2005, sec. 3.6)?

The key diﬀerence between the EP method in this manuscript when M = N and the pseudo-
inputs and the training inputs are identical, and the standard EP method as described by
(Rasmussen and Williams, 2005, sec. 3.6) is the factor representation. While Rasmussen and
Williams (2005) used a one dimensional un-normalised Gaussian distribution that touches
only one function value fn to approximate each exact factor, the approximate factor used
in the EP scheme described in the main text touches all M pseudo-points, hence all N
function values when the pseudo-inputs are placed at the training inputs. However, in
practice both methods give virtually identical results. Figure 33 shows the approximate log
marginal likelihood and the negative test log-likelihood, given by running the EP procedure

67

Bui, Yan and Turner

described in the main text on the ionosphere dataset. We note that these results are
similar to that of the standard EP method (see Kuss and Rasmussen, 2005).

Figure 33: EP energy on the train set [TOP] and the average negative log-likelihood on the
test set[BOTTOM] when M = N .

68

Unifying Gaussian Process Approximations

References

Mauricio A. ´Alvarez, David Luengo, Michalis K. Titsias, and Neil D. Lawrence. Eﬃcient
multioutput Gaussian processes through variational inducing kernels. In 13th Interna-
tional Conference on Artiﬁcial Intelligence and Statistics, pages 25–32, 2010.

Matthias Bauer, Mark van der Wilk, and Carl E. Rasmussen. Understanding probabilistic
sparse Gaussian process approximations. In Advances in Neural Information Processing
Systems 29, pages 1525–1533, 2016.

Lawrence D. Brown. Fundamentals of Statistical Exponential Families with Applications in
Statistical Decision Theory. Institute of Mathematical Statistics, Hayward, CA, 1986.

Thang D. Bui and Richard E. Turner. Tree-structured Gaussian process approximations.

In Advances in Neural Information Processing Systems 27, pages 2213–2221, 2014.

Thang D. Bui, Cuong V. Nguyen, and Richard E. Turner. Streaming sparse Gaussian
process approximations. In Advances in Neural Information Processing Systems 30, 2017.

Lehel Csat´o. Gaussian Processes — Iterative Sparse Approximations. PhD thesis, Aston

University, 2002.

14(3):641–669, 2002.

Lehel Csat´o and Manfred Opper. Sparse online Gaussian processes. Neural Computation,

Michael R.W. Dawson. Understanding cognitive science. Blackwell Publishing, 1998.

Marc P. Deisenroth. Eﬃcient Reinforcement Learning using Gaussian Processes. PhD

thesis, Karlsruhe Institute of Technology, Karlsruhe, Germany, 2010.

Stefan Depeweg, Jos´e Miguel Hern´andez-Lobato, Finale Doshi-Velez, and Steﬀen Udluft.
Learning and policy search in stochastic dynamical systems with Bayesian neural net-
works. In 4th International Conference on Learning Representations, 2016.

Amir Dezfouli and Edwin V. Bonilla. Scalable inference for Gaussian process models with
black-box likelihoods. In Advances in Neural Information Processing Systems 28, pages
1414–1422, 2015.

Anibal Figueiras-Vidal and Miguel L´azaro-Gredilla. Inter-domain Gaussian processes for
sparse inference using inducing features. In Advances in Neural Information Processing
Systems 22, pages 1087–1095, 2009.

Roger Frigola, Yutian Chen, and Carl E. Rasmussen. Variational Gaussian process state-
space models. In Advances in Neural Information Processing Systems 27, pages 3680–
3688, 2014.

Andrew Gelman, Aki Vehtari, Pasi Jyl¨anki, Christian Robert, Nicolas Chopin, and John P
Cunningham. Expectation propagation as a way of life. arXiv preprint arXiv:1412.4869,
2014.

69

Bui, Yan and Turner

James Hensman, Nicolo Fusi, and Neil D. Lawrence. Gaussian processes for big data. In

29th Conference on Uncertainty in Artiﬁcial Intellegence, pages 282–290, 2013.

James Hensman, Alexander G. D. G. Matthews, and Zoubin Ghahramani. Scalable vari-
In 18th International Conference on Artiﬁcial

ational Gaussian process classiﬁcation.
Intelligence and Statistics, pages 351–360, 2015.

Daniel Hern´andez-Lobato and Jos´e Miguel Hern´andez-Lobato. Scalable Gaussian process
classiﬁcation via expectation propagation. In 19th International Conference on Artiﬁcial
Intelligence and Statistics, pages 168–176, 2016.

Jos´e Miguel Hern´andez-Lobato, Yingzhen Li, Mark Rowland, Daniel Hern´andez-Lobato,
In 33rd
Thang D Bui, and Richard E Turner. Black-box α-divergence minimization.
International Conference on International Conference on Machine Learning, pages 1511–
1520, 2016.

Trong Nghia Hoang, Quang Minh Hoang, and Bryan Kian Hsiang Low. A distributed
variational inference framework for unifying parallel sparse Gaussian process regression
models. In 33rd International Conference on Machine Learning, pages 382–391, 2016.

Neil Houlsby, Ferenc Husz´ar, Zoubin Ghahramani, and M´at´e Lengyel. Bayesian active
learning for classiﬁcation and preference learning. arXiv preprint arXiv:1112.5745, 2011.

Kazufumi Ito and Kaiqi Xiong. Gaussian ﬁlters for nonlinear ﬁltering problems.

IEEE

Transactions on Automatic Control, 45(5):910–927, 2000.

Diederik P. Kingma and Jimmy Ba. Adam: a method for stochastic optimization. In 3rd

International Conference on Learning Representations, 2015.

H. J. Kushner and A. S. Budhiraja. A nonlinear ﬁltering algorithm based on an approxi-
mation of the conditional distribution. IEEE Transactions on Automatic Control, 45(3):
580–585, Mar 2000.

Malte Kuss and Carl E. Rasmussen. Assessing approximate inference for binary Gaussian
process classiﬁcation. The Journal of Machine Learning Research, 6:1679–1704, 2005.

Neil D. Lawrence. Probabilistic non-linear principal component analysis with Gaussian
process latent variable models. The Journal of Machine Learning Research, 6:1783–1816,
2005.

Yingzhen Li, Jos´e Miguel Hern´andez-Lobato, and Richard E. Turner. Stochastic expectation
propagation. In Advances in Neural Information Processing Systems 29, pages 2323–2331,
2015.

Kian Hsiang Low, Jiangbo Yu, Jie Chen, and Patrick Jaillet. Parallel Gaussian process
regression for big data: Low-rank representation meets Markov approximation. In 29th
AAAI Conference on Artiﬁcial Intelligence, pages 2821–2827, 2015.

Maren Mahsereci and Philipp Hennig. Probabilistic line searches for stochastic optimization.

In Advances in Neural Information Processing Systems 28, pages 181–189, 2015.

70

Unifying Gaussian Process Approximations

Alexander G. D. G. Matthews, James Hensman, Richard E Turner, and Zoubin Ghahra-
mani. On sparse variational methods and the Kullback-Leibler divergence between
In 19th International Conference on Artiﬁcial Intelligence and
stochastic processes.
Statistics, pages 231–239, 2016.

Andrew McHutchon. Nonlinear modelling and control using Gaussian processes. PhD thesis,

University of Cambridge, 2014.

Thomas P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis,

Massachusetts Institute of Technology, 2001.

Thomas P. Minka. Power EP. Technical report, Microsoft Research Cambridge, 2004.

Thomas P. Minka. Divergence measures and message passing. Technical report, Microsoft

Research Cambridge, 2005.

Andrew Naish-Guzman and Sean B. Holden. The generalized FITC approximation.
Advances in Neural Information Processing Systems 20, pages 1057–1064, 2007.

In

Hannes Nickisch and Carl E. Rasmussen. Approximations for binary Gaussian process

classiﬁcation. The Journal of Machine Learning Research, 9(Oct):2035–2078, 2008.

Juho Piironen and Aki Vehtari. Comparison of Bayesian predictive methods for model

selection. Statistics and Computing, 27(3):711–735, 2017.

Yuan Qi, Ahmed H. Abdel-Gawad, and Thomas P. Minka. Sparse-posterior Gaussian pro-
cesses for general likelihoods. In 26th Conference on Uncertainty in Artiﬁcial Intelligence,
pages 450–457, 2010.

Joaquin Qui˜nonero-Candela and Carl E. Rasmussen. A unifying view of sparse approximate
Gaussian process regression. The Journal of Machine Learning Research, 6:1939–1959,
2005.

Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learn-

ing. The MIT Press, 2005.

Jaakko Riihim¨aki, Pasi Jyl¨anki, and Aki Vehtari. Nested expectation propagation for Gaus-
sian process classiﬁcation with a multinomial probit likelihood. The Journal of Machine
Learning Research, 14(Jan):75–109, 2013.

Alan D. Saul, James Hensman, Aki Vehtari, and Neil D. Lawrence. Chained Gaussian
processes. In 19th International Conference on Artiﬁcial Intelligence and Statistics, pages
1431–1440, 2016.

Anton Schwaighofer and Volker Tresp. Transductive and inductive methods for approximate
Gaussian process regression. In Advances in Neural Information Processing Systems 15,
pages 953–960, 2002.

Matthias Seeger. Bayesian inference and optimal design for the sparse linear model. The

Journal of Machine Learning Research, 9(Apr):759–813, 2008.

71

Bui, Yan and Turner

Matthias Seeger and Michael I. Jordan. Sparse Gaussian process classiﬁcation with multiple
classes. Technical report, Department of Statistics, University of Berkeley, CA, 2004.

Matthias Seeger, Christopher Williams, and Neil D. Lawrence. Fast forward selection to
speed up sparse Gaussian process regression. In 9th International Conference on Artiﬁcial
Intelligence and Statistics, 2003.

Edward Snelson. Flexible and eﬃcient Gaussian process models for machine learning. PhD

thesis, University College London, 2007.

Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs.

In Advances in Neural Information Processing Systems 19, pages 1257–1264, 2006.

Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian optimization of
machine learning algorithms. In Advances in Neural Information Processing Systems 25,
pages 2951–2959, 2012.

Michalis K. Titsias. Variational learning of inducing variables in sparse Gaussian processes.
In 12th International Conference on Artiﬁcial Intelligence and Statistics, pages 567–574,
2009.

Michalis K. Titsias and Neil D. Lawrence. Bayesian Gaussian process latent variable model.
In 13th International Conference on Artiﬁcial Intelligence and Statistics, pages 844–851,
2010.

Felipe Tobar, Thang D. Bui, and Richard E. Turner. Learning stationary time series us-
ing Gaussian processes with nonparametric kernels. In Advances in Neural Information
Processing Systems 29, pages 3501–3509, 2015.

Richard E. Turner and Maneesh Sahani. Two problems with variational expectation max-
imisation for time-series models.
In D. Barber, T. Cemgil, and S. Chiappa, editors,
Bayesian Time series models, chapter 5, pages 109–130. Cambridge University Press,
2011.

Jack M. Wang, David J. Fleet, and Aaron Hertzmann. Gaussian process dynamical models.

In Advances in Neural Information Processing Systems 18, pages 1441–1448, 2005.

Minjie Xu, Balaji Lakshminarayanan, Yee Whye Teh, Jun Zhu, and Bo Zhang. Distributed
In Advances in Neural Information

Bayesian posterior sampling via moment sharing.
Processing Systems 27, pages 3356–3364, 2014.

Huaiyu Zhu and Richard Rohwer. Information geometric measurements of generalisation.

Technical report, Aston University, 1995.

Huaiyu Zhu and Richard Rohwer. Measurements of generalisation based on information

geometry. In Mathematics of Neural Networks, pages 394–398. 1997.

72

7
1
0
2
 
t
c
O
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
6
6
0
7
0
.
5
0
6
1
:
v
i
X
r
a

Unifying Gaussian Process Approximations

A Unifying Framework for Gaussian Process Pseudo-Point
Approximations using Power Expectation Propagation

Thang D. Bui

Josiah Yan

Richard E. Turner
Computational and Biological Learning Lab, Department of Engineering
University of Cambridge, Trumpington Street, Cambridge, CB2 1PZ, UK

tdb40@cam.ac.uk

josiah.yan@gmail.com

ret26@cam.ac.uk

Abstract

Gaussian processes (GPs) are ﬂexible distributions over functions that enable high-
level assumptions about unknown functions to be encoded in a parsimonious, ﬂexible and
general way. Although elegant, the application of GPs is limited by computational and
analytical intractabilities that arise when data are suﬃciently numerous or when employ-
ing non-Gaussian models. Consequently, a wealth of GP approximation schemes have been
developed over the last 15 years to address these key limitations. Many of these schemes
employ a small set of pseudo data points to summarise the actual data. In this paper we
develop a new pseudo-point approximation framework using Power Expectation Propaga-
tion (Power EP) that uniﬁes a large number of these pseudo-point approximations. Unlike
much of the previous venerable work in this area, the new framework is built on standard
methods for approximate inference (variational free-energy, EP and Power EP methods)
rather than employing approximations to the probabilistic generative model itself. In this
way all of approximation is performed at ‘inference time’ rather than at ‘modelling time’
resolving awkward philosophical and empirical questions that trouble previous approaches.
Crucially, we demonstrate that the new framework includes new pseudo-point approxima-
tion methods that outperform current approaches on regression and classiﬁcation tasks.

1. Introduction

Gaussian Processes (GPs) are powerful nonparametric distributions over continuous func-
tions that are routinely deployed in probabilistic modelling for applications including regres-
sion and classiﬁcation (Rasmussen and Williams, 2005), representation learning (Lawrence,
2005), state space modelling (Wang et al., 2005), active learning (Houlsby et al., 2011),
reinforcement learning (Deisenroth, 2010), black-box optimisation (Snoek et al., 2012), and
numerical methods (Mahsereci and Hennig, 2015). GPs have many elegant theoretical
properties, but their use in probabilistic modelling is greatly hindered by analytic and
computational intractabilities. A large research eﬀort has been directed at this fundamen-
tal problem resulting in the development of a plethora of sparse approximation methods
that can sidestep these intractabilities (Csat´o, 2002; Csat´o and Opper, 2002; Schwaighofer
and Tresp, 2002; Seeger et al., 2003; Qui˜nonero-Candela and Rasmussen, 2005; Snelson
and Ghahramani, 2006; Snelson, 2007; Naish-Guzman and Holden, 2007; Titsias, 2009;
Figueiras-Vidal and L´azaro-Gredilla, 2009; ´Alvarez et al., 2010; Qi et al., 2010; Bui and

1

Bui, Yan and Turner

Turner, 2014; Frigola et al., 2014; McHutchon, 2014; Hensman et al., 2015; Hern´andez-
Lobato and Hern´andez-Lobato, 2016; Matthews et al., 2016)

This paper develops a general sparse approximate inference framework based upon Power
Expectation Propagation (PEP) (Minka, 2004) that uniﬁes many of these approximations,
extends them signiﬁcantly, and provides improvements in practical settings. In this way, the
paper provides a complementary perspective to the seminal review of Qui˜nonero-Candela
and Rasmussen (2005) viewing sparse approximations through the lens of approximate
inference, rather than approximate generative models.

The paper begins by reviewing several frameworks for sparse approximation focussing
on the GP regression and classiﬁcation setting (section 2). It then lays out the new unifying
framework and the relationship to existing techniques (section 3). Readers whose focus is
to understand the new framework might want to move directly to this section. Finally, a
thorough experimental evaluation is presented in section 4.

2. Pseudo-point Approximations for GP Regression and Classiﬁcation

This section provides a concise introduction to GP regression and classiﬁcation and then
reviews several pseudo-point based sparse approximation schemes for these models. For
simplicity, we ﬁrst consider a supervised learning setting in which the training set com-
prises N D-dimensional input and scalar output pairs {xn, yn}N
n=1 and the goal is to
produce probabilistic predictions for the outputs corresponding to novel inputs. A non-
linear function, f (x), can be used to parameterise the probabilistic mapping between in-
puts and outputs, p(yn|f, xn, θ). Typical choices for the probabilistic mapping are Gaus-
y) for the regression setting (yn ∈ R) and Bernoulli
sian p(yn|f, xn, θ) = N (yn; f (xn), σ2
p(yn|f, xn, θ) = B(yn; Φ(f (xn))) with a sigmoidal link function Φ(f ) for the binary classi-
ﬁcation setting (yn ∈ {0, 1}). Whilst it is possible to specify the non-linear function f via
an explicit parametric form, a more ﬂexible and elegant approach employs a GP prior over
the functions directly, p(f |θ) = GP(f ; 0, kθ(·, ·)), here assumed without loss of generality to
have a zero mean-function and a covariance function kθ(x, x(cid:48)). This class of probabilistic
models has a joint distribution

p(f, y|θ) = p(f |θ)

p(yn|f (xn), θ)

(1)

where we have collected the observations into the vector y and suppressed the inputs on
the left hand side to lighten the notation.

This model class contains two potential sources of intractability. First, the possibly non-
linear likelihood function can introduce analytic intractabilities that require approximation.
Second, the GP prior entails an O(N 3) complexity that is computationally intractable for
many practical problems. These two types of intractability can be handled by combining
standard approximate inference methods with pseudo-point approximations that summarise
the full Gaussian process via M pseudo data points leading to an O(N M 2) cost. The main
approaches of this sort can be characterised in terms of two parallel frameworks that are
described in the following sections.

N
(cid:89)

n=1

2

Unifying Gaussian Process Approximations

2.1 Sparse GP Approximation via Approximate Generative Models

The ﬁrst framework begins by constructing a new generative model that is similar to the
original, so that inference in the new model might be expected to produce similar results,
but which has a special structure that supports eﬃcient computation. Typically this ap-
proach involves approximating the Gaussian process prior as it is the origin of the cubic
cost. If there are analytic intractabilities in the approximate model, as will be the case in
e.g. classiﬁcation or state-space models, then these will require approximate inference to be
performed in the approximate model.

The seminal review by Qui˜nonero-Candela and Rasmussen (Qui˜nonero-Candela and
Rasmussen, 2005) reinterprets a family of approximations in terms of this unifying frame-
work. The GP prior is approximated by identifying a small set of M ≤ N pseudo-points
u, here assumed to be disjoint from the training function values f so that f = {u, f , f
=u,f }.
The GP prior is then decomposed using the product rule

p(f |θ) = p(u|θ)p(f |u, θ)p(f

=u,f |f , u, θ).

(2)

1
uuu, Dﬀ ) where Dﬀ = Kﬀ − Qﬀ and Qﬀ = KfuK−

Of central interest is the relationship between the pseudo-points and the training function
1
uuKuf .
values p(f |u, θ) = N (f ; KfuK−
Here we have introduced matrices corresponding to the covariance function’s evaluation
at the pseudo-input locations {zm}M
m=1, so that [Kuu]mm(cid:48) = kθ(zm, zm(cid:48)) and similarly
for the covariance between the pseudo-input and data locations [Kuf ]mn = kθ(zm, xn).
Importantly, this term saddles learning with a cubic complexity cost. Computationally
eﬃcient approximations can be constructed by simplifying these dependencies between the
pseudo-points and the data function values q(f |u, θ) ≈ p(f |u, θ). In order to beneﬁt from
these eﬃciencies at prediction time as well, a second approximation is made whereby the
pseudo-points form a bottleneck between the data function values and test function values
=u,f |f , u, θ). Together, the two approximations result in an approximate
p(f
prior process,

=u,f |u, θ) ≈ p(f

q(f |θ) = p(u|θ)q(f |u, θ)p(f

=u,f |u, θ).

We can now compactly summarise a number of previous approaches to GP approximation
as special cases of the choice

q(f |u, θ) =

1
N (fb; Kfb,uK−

uuu, αDfb,fb)

B
(cid:89)

b=1

where b indexes B disjoint blocks of data-function values. The Deterministic Training
Conditional (DTC) approximation uses α → 0; the Fully Independent Training Condi-
tional (FITC) approximation uses α = 1 and B = N ; the Partially Independent Training
Conditional (PITC) approximation uses α = 1 (Qui˜nonero-Candela and Rasmussen, 2005;
Schwaighofer and Tresp, 2002).

In a moment we will consider inference in the modiﬁed models, before doing so we note
that it is possible to construct more ﬂexible modiﬁed prior processes using the inter-domain
approach that places the pseudo-points in a diﬀerent domain from the data, deﬁned by
a linear integral transform g(z) = (cid:82) w(z, z(cid:48))f (z(cid:48))dz(cid:48). Here the window w(z, z(cid:48)) might be

3

(3)

(4)

Bui, Yan and Turner

a Gaussian blur or a wavelet transform. The pseudo-points are now placed in the new
domain g = {u, g
=u} where they induce a potentially more ﬂexible Gaussian process in the
old domain f through the linear transform (see Figueiras-Vidal and L´azaro-Gredilla, 2009,
for FITC). The expressions in this section still hold, but the covariance matrices involving
pseudo-points are modiﬁed to take account of the transform,

(cid:90)

(cid:90)

[Kuu]mm(cid:48) =

w(zm, z)kθ(z, z(cid:48))w(z(cid:48), zm(cid:48))dzdz(cid:48),

[Kuf ]mn =

w(zm, z)kθ(z, xn)dz.

(5)

Having speciﬁed modiﬁed prior processes, these can be combined with the original like-
lihood function to produce a new generative model. In the case of point-wise likelihoods we
have

q(y, f |θ) = q(f |θ)

p(yn|f (xn), θ).

(6)

N
(cid:89)

n=1

Inference and learning can now be performed using the modiﬁed model using standard
techniques. Due to the form of the new prior process, the computational complexity is
O(N M 2) (for testing, N becomes the number of test data points, assuming dependencies
between the test-points are not computed).1 For example, in the case of regression, the
posterior distribution over function values f (necessary for inference and prediction) has a
simple analytic form

|
b=1) + σ2

q(f |y, θ) = GP(f ; µf

y, Σf

y), µf

y = Qf f K−

ﬀ y, Σf

1

1
ﬀ Qf f
y = Kf f − Qf f K−

(7)

|

|

|

where Kﬀ = Qﬀ + blkdiag({αbDfbfb}B
yI and blkdiag builds a block-diagonal matrix
from its inputs. One way of understanding the origin of the computational gains is that
the new generative model corresponds to a form of factor analysis in which the M pseudo-
points determine the N function values at the observed data (as well as at potential test
locations) via a linear Gaussian relationship. This results in low rank (sparse) structure in
Kﬀ that can be exploited through the matrix inversion and determinant lemmas. In the
case of regression, the new model’s marginal likelihood also has an analytic form that allows
the hyper-parameters, θ, to be learned via optimisation

log q(y|θ) = −

log(2π) −

log |Kﬀ | −

N
2

1
2

1
y(cid:124)K−
ﬀ y.

1
2

(8)

The approximate generative model framework has attractive properties. The cost of
inference, learning, and prediction has been reduced from O(N 3) to O(N M 2) and in many
cases accuracy can be maintained with a relatively small number of pseudo-points. The
pseudo-point input locations can be optimised by maximising the new model’s marginal
likelihood (Snelson and Ghahramani, 2006). When M = N and the pseudo-points and
observed data inputs coincide, then FITC and PITC are exact which appears reassuring.
However, the framework is philosophically challenging as the elegant separation of model
and (approximate) inference has been lost. Are we allowed in an online inference setting,

1. It is assumed that the maximum size of the blocks is not greater than the number of pseudo-points

dim(fb) ≤ M .

4

Unifying Gaussian Process Approximations

for example, to add new pseudo-points as more data are acquired and the complexity of the
underlying function is revealed? This seems sensible, but eﬀectively changes the modelling
assumptions as more data are seen. Devout Bayesians might then demand that we perform
model averaging for coherence. Similarly, if the pseudo-input locations are optimised, the
principled non-parametric model has suddenly acquired M D parameters and with them
all of the concomitant issues of parametric models including overﬁtting and optimisation
diﬃculties (Bauer et al., 2016). As the pseudo-inputs are considered part of the model, the
Bayesians might then suggest that we place priors over the pseudo-inputs and perform full
blown probabilistic inference over them.

These awkward questions arise because the generative modelling interpretation of pseudo-
data entangles the assumptions made about the data with the approximations required
to perform inference. Instead, the modelling assumptions (which encapsulate prior under-
standing of the data) should remain decoupled from inferential assumptions (which leverage
structure in the posterior for tractability). In this way pseudo-data should be introduced
when we seek to perform computationally eﬃcient approximate inference, leaving the mod-
elling assumptions unchanged as we reﬁne and improve approximate inference. Indeed, even
under the generative modelling perspective, for analytically intractable likelihood functions
an additional approximate inference step is required, begging the question; why not handle
computational and analytic intractabilities together at inference time?

2.2 Sparse GP Approximation via Approximate Inference: VFE

The approximate generative model framework for constructing sparse approximations is
philosophically troubling. In addition, learning pseudo-point input locations via optimisa-
tion of the model likelihood can perform poorly e.g. for DTC it is prone to overﬁtting even
for M (cid:28) N (Titsias, 2009). This motivates a more direct approach that commits to the
true generative model and performs all of the necessary approximation at inference time.

Perhaps the most well known approach in this vein is Titsias’s beautiful sparse varia-
tional free energy (VFE) method (Titsias, 2009). The original presentation of this work
employs ﬁnite variable sets and an augmentation trick that arguably obscures its full ele-
gance. Here instead we follow the presentation in Matthews et al. (2016) and lower bound
the marginal likelihood using a distribution q(f ) over the entire inﬁnite dimensional func-
tion,

log p(y|θ) = log

p(y, f |θ)df ≥

q(f ) log

(cid:90)

(cid:90)

p(y, f |θ)
q(f )

(cid:20)

df = E

q(f )

log

(cid:21)

p(y, f |θ)
q(f )

= F(q, θ).

The VFE bound can be written as the diﬀerence between the model log-marginal likelihood
and the KL divergence between the variational distribution and the true posterior F(q, θ) =
log p(y|θ)−KL(q(f )||p(f |y, θ)). The bound is therefore saturated when q(f ) = p(f |y, θ), but
=u}, and an approxi-
this is intractable. Instead, pseudo-points are made explicit, f = {u, f
mate posterior distribution used of the following form q(f ) = q(u, f
=u|u, θ)q(u).
Under this approximation, the set of variables f
=u do not experience the data directly, but
rather only through the pseudo-points, as can be seen by comparison to the true poste-
rior p(f |y, θ) = p(f
=u|y, u, θ)p(u|y, θ). Importantly, the form of the approximate posterior
causes a cancellation of the prior conditional term, which gives rise to a bound with O(N M 2)

=u|θ) = p(f

5

Bui, Yan and Turner

complexity,

F(q, θ) = E

q(f

θ)

|

E

q(f

|

(cid:88)

=

n

(cid:20)

log

=u|u, θ)p(u|θ)

p(y|f, θ)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
p(f
(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
=u|u, θ)q(u)
p(f
θ) [log p(yn|fn, θ)] − KL(q(u)||p(u|θ)).

(cid:21)

For regression with Gaussian observation noise, the calculus of variations can be used to
ﬁnd the optimal approximate posterior Gaussian process over pseudo-data qopt(f |θ) =
p(f

=u|u, θ)qopt(u) which has the form

qopt(f |θ) = GP(f ; µf

y, Σf

y), µf

y = Qf f ˜K−

1
ﬀ y, Σf

y = Kf f − Qf f ˜K−
1
ﬀ Qf f

(9)

|

|

|

|

where ˜Kﬀ = Qﬀ + σ2
yI. This process is identical to that recovered when performing ex-
act inference under the DTC approximate regression generative model (Titsias, 2009) (see
equation 7 as α → 0). In fact DTC was originally derived using a related KL argument
(Csat´o, 2002; Seeger et al., 2003). The optimised free-energy is

F(qopt, θ) = −

log(2π) −

log | ˜Kﬀ | −

N
2

1
2

1
2

y(cid:124) ˜K−

1
ﬀ y −

1
2σ2
y

trace(Kﬀ − Qﬀ ).

(10)

Notice that the free-energy has an additional trace term as compared to the marginal
likelihood obtained from the DTC generative model approach (see equation 8 as α → 0).
The trace term is proportional to the sum of the variances of the training function values
given the pseudo-points, p(f |u), it thereby encourages pseudo-input locations that explain
the observed data well. This term acts as a regulariser that prevents overﬁtting which
plagues the generative model formulation of DTC.

The VFE approach can be extended to non-linear models including classiﬁcation (Hens-
man et al., 2015), latent variable models (Titsias and Lawrence, 2010) and state space
models (Frigola et al., 2014; McHutchon, 2014) by restricting q(u) to be Gaussian and
optimising its parameters. Indeed, this uncollapsed form of the bound can be beneﬁcial
in the context of regression too as it is amenable to stochastic optimisation (Hensman
et al., 2013). Additional approximation is sometimes required to compute any remaining
intractable non-linear integrals, but these are often low-dimensional. For example, when the
likelihood depends on only one latent function value, as is typically the case for regression
and classiﬁcation, the bound requires only 1D integrals E
q(fn) [log p(yn|fn, θ)] that can be
evaluated using quadrature (Hensman et al., 2015), for example.

The VFE approach can also be extended to employ inter-domain variables ( ´Alvarez et al.,
2010; Tobar et al., 2015; Matthews et al., 2016). The approach considers the augmented
generative model p(f, g|θ) where to remind the reader the auxiliary process is deﬁned by a
linear integral transformation, g(z) = (cid:82) w(z, z(cid:48))f (z(cid:48))dz(cid:48). Variational inference is now per-
formed over both latent processes q(f, g) = q(f, u, g
=u|u, θ)q(u). Here the
pseudo-data have been placed into the auxiliary process with the idea being that they can
induce richer dependencies in the original domain that model the true posterior more accu-
rately. In fact, if the linear integral transformation is parameterised then the transformation
can be learned so that it approximates the posterior more accurately.

=u|θ) = p(f, g

6

Unifying Gaussian Process Approximations

A key concept underpinning the VFE framework is that the pseudo-input locations (and
the parameters of the inter-domain transformation, if employed) are purely parameters of
the approximate posterior, hence the name ‘variational parameters’. This distinction is im-
portant as it means, for example, that we are free to add pseudo-data as more structure is
revealed the underlying function without altering the modelling assumptions (e.g. see Bui
et al. (2017) for an example in online inference). Moreover, since the pseudo-input locations
are variational parameters, placing priors over them is unnecessary in this framework. Un-
like the model parameters, optimisation of variational parameters is automatically protected
from overﬁtting as the optimisation is minimising the KL divergence between the approx-
imate posterior and the true posterior. Indeed, although the DTC posterior is recovered
in the regression setting, as we have seen the free-energy is not equal to the log-marginal
likelihood of the DTC generative model, containing an additional term that substantially
improves the quality of the optimised pseudo-point input locations.

The fact that the form of the DTC approximation can be recovered from a direct ap-
proximate inference approach and that this new perspective leads to superior pseudo-input
optimisation, raises the question; can this also be done for FITC and PITC?

2.3 Sparse GP Approximation via Approximate Inference: EP

Expectation Propagation (EP) is a deterministic inference method (Minka, 2001) that is
known to outperform VFE methods in GP classiﬁcation when unsparsiﬁed fully-factored ap-
proximations q(f ) = (cid:81)
n qn(fn) are used (Nickisch and Rasmussen, 2008). Motivated by this
observation, EP has been combined with the approximate generative modelling approach
to handle non-linear likelihoods (Naish-Guzman and Holden, 2007; Hern´andez-Lobato and
Hern´andez-Lobato, 2016). This begs the question: can the sparsiﬁcation and the non-linear
approximation be handled in a single EP inference stage, as for VFE? Astonishingly Csat´o
and Opper not only developed such a method in 2002 (Csat´o and Opper, 2002), predat-
ing much of the work mentioned above, they showed that it is equivalent to applying the
FITC approximation and running EP if further approximation is required. In our view,
this is a central result, but it appears to have been largely overlooked by the ﬁeld. Snelson
was made aware of it when writing his thesis (Snelson, 2007), brieﬂy acknowledging Csat´o
and Opper’s contribution. Qi et al. (2010) extended Csat´o and Opper’s work to utilise
inter-domain pseudo-points and they additionally recognised that the EP energy function
at convergence is equal to the FITC log-marginal likelihood approximation. Interestingly,
no additional term arises as it does when the VFE approach generalised the DTC generative
model approach. We are unaware of other work in this vein.

It is hard to be known for certain why these important results are not widely known,
but a contributing factor is that the exposition in these papers is largely at Marr’s algo-
rithmic level (Dawson, 1998), and does not focus on the computational level making them
challenging to understand. Moreover, Csat´o and Opper’s paper was written before EP was
formulated in a general way and the presentation, therefore, does not follow what has be-
come the standard approach. In fact, as the focus was online inference, Assumed Density
Filtering (Kushner and Budhiraja, 2000; Ito and Xiong, 2000) was employed rather than
full-blown EP. One of the main contributions of this paper is to provide a clear compu-
tational exposition including an explicit form of the approximating distribution and full

7

Bui, Yan and Turner

details about each step of the EP procedure. In addition, to bringing clarity we make the
following novel contributions:

• We show that a generalisation of EP called Power EP can subsume the EP and
VFE approaches (and therefore FITC and DTC) into a single uniﬁed framework.
More precisely, the ﬁxed points of Power EP yield the FITC and VFE posterior
distribution under diﬀerent limits and the Power EP marginal likelihood estimate
(the negative ‘Power EP energy’) recovers the FITC marginal likelihood and the VFE
too. Critically the connection to the VFE method leans on the new interpretation of
Titsias’s approach (Matthews et al., 2016) outlined in the previous section that directly
employs the approximate posterior over function values (rather than augmenting the
model with pseudo-points). The connection therefore also requires a formulation of
Power EP that involves KL divergence minimisation between stochastic processes.

• We show how versions of PEP that are intermediate between the existing VFE and EP
approaches can be derived, as well as mixed approaches that treat some data variation-
ally and others using EP. We also show how PITC emerges from the same framework
and how to incorporate inter-domain transforms. For regression with Gaussian obser-
vation noise, we obtain analytical expressions for the ﬁxed points of Power EP in a
general case that includes all of these extensions as well as the form of the Power EP
marginal likelihood estimate at convergence that is useful for hyper-parameter and
pseudo-input optimisation.

• We consider (Gaussian) regression and probit classiﬁcation as canonical models on
which to test the new framework and demonstrate through exhaustive testing that
versions of PEP intermediate between VFE and EP perform substantially better on
average. The experiments also shed light on situations where VFE is to be preferred
to EP and vice versa which is an important open area of research.

Many of the new theoretical contributions described above are summarised in ﬁg. 1

along with their relationship to previous work.

3. A New Unifying View using Power Expectation Propagation

In this section, we provide a new unifying view of sparse approximation using Power Ex-
pectation Propagation (PEP or Power EP) (Minka, 2004). We review Power EP, describe
how to apply it for sparse GP regression and classiﬁcation, and then discuss its relationship
to existing methods.

3.1 The Joint-Distribution View of Approximate Inference and Learning

One way of understanding the goal of distributional inference approximations, including the
VFE method, EP and Power EP, is that they return an approximation of a tractable form
to the model joint-distribution evaluated on the observed data. In the case of GP regression
and classiﬁcation, this means q∗(f |θ) ≈ p(f, y|θ) where ∗ is used to denote an unnor-
malised process. Why is the model joint-distribution a sensible object of approximation?
The joint distribution can be decomposed into the product of the posterior distribution

8

Unifying Gaussian Process Approximations

Figure 1: A uniﬁed view of pseudo-point GP approximations applied to A) regression,
and B) classiﬁcation. Every point in the algorithm polygons corresponds to a form of GP
approximation. Previous algorithms correspond to labelled vertices. The new Power EP
framework encompasses the three polygons, including their interior.

and the marginal likelihood, p(f, y|θ) = p∗(f |y, θ) = p(f |y, θ)p(y|θ), the two inferential
objects of interest. A tractable approximation to the joint can therefore be similarly de-
composed q∗(f |θ) = Zq(f |θ) into a normalised component that approximates the posterior
q(f |θ) ≈ p(f |y, θ) and the normalisation constant which approximates the marginal likeli-
hood Z ≈ p(y|θ). In other words, the approximation of the joint simultaneously returns
approximations to the posterior and marginal likelihood. In the current context tractability
of the approximating family means that it is analytically integrable and that this integra-
tion can be performed with an appropriate computational complexity. We consider the
approximating family comprising unnormalised GPs, q∗(f |θ) = ZGP(f ; mf , Vﬀ (cid:48)).

The VFE approach can be reformulated in the new context using the un-normalised KL
divergence (Zhu and Rohwer, 1997) to measure the similarity between the approximation
and the joint distribution

KL(q∗(f |θ)||p(f, y|θ)) =

q∗(f ) log

df +

(p(f, y|θ) − q∗(f )) df.

(11)

(cid:90)

q∗(f )
p(f, y|θ)

(cid:90)

The un-normalised KL divergence generalises the KL divergence to accommodate un-normalised
It is always non-negative and collapses back to the standard form when its
densities.
arguments are normalised. Minimising the un-normalised KL with respect to q∗(f |θ) =
ZVFEq(f ) encourages the approximation to match both the posterior and marginal-likelihood,
and it yields analytic solutions

qopt(f ) = argmin

KL(q(f )||p(f |y, θ)), and Zopt

VFE = exp(F(qopt(f ), θ)).

(12)

q(f )

∈Q

9

Bui, Yan and Turner

That is, the standard variational free-energy approximation to the posterior and marginal
likelihood is recovered. One of the pedagogical advantages of framing VFE in this way is that
approximation of the posterior and marginal likelihood are committed to upfront, in con-
trast to the traditional derivation which begins by targeting approximation of the marginal
likelihood, but shows that approximation of the posterior emerges as an essential part of
this scheme (see section 2.2). A disadvantage is that optimisation of hyper-parameters must
logically proceed by optimising the marginal likelihood approximation, Zopt
VFE, and at ﬁrst
sight therefore appears to necessitate diﬀerent objective functions for q∗(f |θ) and θ (unlike
the standard view which uses a single objective from the beginning). However, it is easy
to show that maximising the single objective p(y|θ) − KL(q∗(f |θ)||p(f, y|θ)) directly for
both q∗(f |θ) and θ is equivalent and that this also recovers the standard VFE method (see
appendix A).

3.2 The Approximating Distribution Employed by Power EP

Power EP also approximates the joint-distribution employing an approximating family
whose form mirrors that of the target,

p∗(f |y, θ) = p(f |y, θ)p(y|θ) = p(f |θ)

p(yn|f, θ) ≈ p(f |θ)

tn(u) = q∗(f |θ).

(13)

(cid:89)

n

(cid:89)

n

Here, the approximation retains the exact prior, but each likelihood term in the exact
posterior, p(yn|fn, θ), is approximated by a simple factor tn(u) that is assumed Gaussian.
These simple factors will be iteratively reﬁned by the PEP algorithm such that they will
capture the eﬀect that each true likelihood has on the posterior.

Before describing the details of the PEP algorithm, it is illuminating to consider an
alternative interpretation of the approximation. Together, the approximate likelihood
functions specify an un-normalised Gaussian over the pseudo-points that can be written
(cid:81)
n tn(u) = N (˜y; ˜Wu, ˜Σ) (assuming that the product of these factors is normalisable which

may not be the case for heavy tailed likelihoods, for example).

The approximate posterior above can therefore be thought of as the (exact) GP pos-
terior resulting from a surrogate regression problem with surrogate observations ˜y that
are generated from linear combinations of the pseudo-points and additive surrogate noise
˜y = ˜Wu + ˜Σ1/2(cid:15). We note that the pseudo-points u live on the latent function (or an
inter-domain transformation thereof) and the surrogate observations ˜y will not generally
lie on the latent function. The surrorate observations and the pseudo-points are therefore
analogous to the data y and the function values f in a normal Gaussian Process regression
problem, respectively. To make the paper more speciﬁc on this point, we have deﬁned pa-
rameters forthe surrogate regression problem explicitly in appendix H. The PEP algorithm
will implicitly iteratively reﬁne {˜y, ˜W, ˜Σ} such that exact inference in the simple surrogate
regression model returns a posterior and marginal likelihood estimate that is ‘close’ to that
returned by performing exact inference in the intractable complex model (see ﬁg. 2).

3.3 The EP Algorithm

One method for updating the approximate likelihood factors tn(u) is to minimise the unnor-
malised KL Divergence between the joint distribution and each of the distributions formed

10

Unifying Gaussian Process Approximations

Figure 2: Perspectives on the approximating family. The true joint distribution over the
unknown function f and the N data points y (top left) comprises the GP prior and an
intractable likelihood function. This is approximated by a surrogate regression model with
a joint distribution over the function f and M surrogate data points ˜y (top right). The
surrogate regression model employs the same GP prior, but uses a Gaussian likelihood
function p(˜y|u, ˜W, ˜Σ) = N (˜y; ˜Wu, ˜Σ). The intractable true posterior (bottom left) is
approximated by reﬁning the surrogate data ˜y their input locations z and the parameters
of the surrogate model ˜W and ˜Σ.

by replacing one of the likelihoods by the corresponding approximating factor (Li et al.,
2015),

(cid:20)

KL

p(f, y|θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

p(f, y|θ)tn(u)
p(yn|fn, θ)

(cid:21)

argmax
tn(u)

= argmax

tn(u)

n(f )tn(u)]. (14)
n(f )p(yn|fn, θ)||p∗
KL[p∗
\
\

n(f ) = p(f, y|θ)/p(yn|fn, θ) which makes
Here we have introduced the leave-one-out joint p∗
\
clear that the minimisation will cause the approximate factors to approximate the like-
lihoods in the context of the leave-one-out joint. Unfortunately, such an update is still
intractable. Instead, EP approximates this idealised procedure by replacing the exact leave-
one-out joint on both sides of the KL by the approximate leave-one-out joint (called the
n(f ) = q∗(f )/tn(u). Not only does this improve tractability, but it also
n(f ) ≈ q∗
cavity) p∗
\
\
means that the new procedure eﬀectively reﬁnes the approximating distribution directly at
each stage, rather than setting the component parts in isolation,

KL([q∗
\

n(f )p(yn|fn, θ)||q∗(f )].
n(f )tn(u)] = KL([q∗
n(f )p(yn|fn, θ)||q∗
\
\

(15)

However, the updates for the approximating factors are now coupled and so the updates
must now be iterated, unlike in the idealised procedure. In this way, EP iteratively reﬁnes

11

Bui, Yan and Turner

the approximate factors or surrogate likelihoods so that the GP posterior of the surro-
gate regression task best approximates the posterior of the original regression/classiﬁcation
problem.

3.4 The Power EP Algorithm

Power EP is, algorithmically, a mild generalisation of the EP algorithm that instead removes
(or includes) a fraction α of the approximate (or true) likelihood functions in the following
steps:

1. Deletion: compute the cavity distribution by removing a fraction of one approximate

n(f |θ) ∝ q∗(f |θ)/tα
factor, q∗
\

n(u).

2. Projection: ﬁrst, compute the tilted distribution by incorporating a corresponding
n(f |θ)pα(yn|fn). Second, project
fraction of the true likelihood into the cavity, ˜p(f ) = q∗
\
the tilted distribution onto the approximate posterior using the KL divergence for un-
normalised densities,

q∗(f |θ) ← argmin

KL(˜p(f )||q∗(f |θ)).

(16)

q∗(f

θ)

|

∈Q

Here Q is the set of allowed q∗(f |θ) deﬁned by eq. (13).

3. Update: compute a new fraction of the approximate factor by dividing the new approx-
imate posterior by the cavity, tα
n(f |θ), and incorporate this fraction
n,new(u) = q∗(f |θ)/q∗
\
back in to obtain the updated factor, tn(u) = t1
α
n,old(u)tα
−

n,new(u).

The above steps are iteratively repeated for each factor that needs to be approximated.
Notice that the procedure only involves one likelihood factor to be handled at a time. In
the case of analytically intractable likelihood functions, this often requires only low dimen-
sional integrals to be computed. In other words, PEP has transformed a high dimensional
intractable integral that is hard to approximate into a set of low dimensional intractable
integrals that are simpler to approximate. The procedure is not, in general guaranteed to
converge but we did not observe any convergence issues in our experiments. Furthermore, it
can be shown to be numerically stable when the factors are log-concave (as in GP regression
and classiﬁcation without pseudo-data) (Seeger, 2008).

If Power EP converges, the fractional updates are equivalent to running the original EP
procedure, but replacing the KL minimisation with an alpha-divergence minimisation (Zhu
and Rohwer, 1995; Minka, 2005),

Dα[p∗(f )||q∗(f )] =

(cid:2)αp∗(f ) + (1 − α)q∗(f ) − p∗(f )αq∗(f )1

−

α(cid:3) df.

(17)

(cid:90)

1
α(1 − α)

When α = 1, the alpha-divergence is the inclusive KL divergence D1[p∗(f )||q∗(f )] =
KL[p∗(f )||q∗(f )] recovering EP as expected from the PEP algorithm. As α → 0 the exclu-
0[p∗(f )||q∗(f )] = KL[q∗(f )||p∗(f )], and since minimising
sive KL divergence is recovered, D
a set of local exclusive KL divergences is equivalent to minimising a single global exclusive
KL divergence (Minka, 2005), the Power EP solution is the minimum of a variational free-
energy (see appendix B for more details). In the current case, we will now show explicitly
that these cases of Power EP recover FITC and Titsias’s VFE solution respectively.

→

12

Unifying Gaussian Process Approximations

3.5 General Results for Gaussian Process Power EP

This section describes the Power EP steps in ﬁner detail showing the complexity is O(N M 2)
and laying the ground work for the equivalence relationships. The appendix F includes a
full derivation.

We start by deﬁning the approximate factors to be in natural parameter form, mak-
ing it simple to combine and delete them, tn(u) = ˜N (u; zn, T1,n, T2,n) = zn exp(u(cid:124)T1,n −
2 u(cid:124)T2,nu). We consider full rank T2,n, but will show that the optimal form is rank 1.
1
The parameterisation means the approximate posterior over the pseudo-points has natural
parameters T1,u = (cid:80)
uu + (cid:80)
n T2,n inducing an approximate posterior,
q∗(f |θ) = ZPEPGP(f ; mf , Vﬀ (cid:48)). Here and in what follows, the dependence on the hyperpa-
rameters θ will be suppressed to lighten the notation. The mean and covariance functions
of the approximate posterior are

1
n T1,n and T2,u = K−

mf = KfuK−

1
uuT−

1
2,uT1,u;

Vﬀ (cid:48) = Kﬀ (cid:48) − Qﬀ (cid:48) + KfuK−

1
1
1
2,uK−
uuT−

uuKuf (cid:48).

(18)

n(f ) ∝ q∗(f )/tα
n(u), has a similar form to the
Deletion: The cavity for data point n, q∗
\
n
1,u = T1,u −αT1,n
posterior, but the natural parameters are modiﬁed by the deletion step, T\

and T\

n
2,u = T2,u − αT2,n, yielding the following mean and covariance functions

n
1
uuT\
f = KfuK−
m\

n
n,
1
2,u T\
1,u;
−

n
ﬀ (cid:48) = Kﬀ (cid:48) − Qﬀ (cid:48) + KfuK−
V \

1
uuT\

n,
1
2,u K−
−

1

uuKuf (cid:48).

(19)

Projection: The central step in Power EP is the projection. Obtaining the new approx-
imate un-normalised posterior q∗(f ) by minimising KL(˜p(f )||q∗(f )) would na¨ıvely appear
intractable. Fortunately,

=u|u)q∗(u),
Remark 1 Due to the structure of the approximate posterior, q∗(f ) = p(f
the objective, KL(˜p(f )||q∗(f )) is minimised when E
q∗(u)[φ(u)], where φ(u) =
{u, uu(cid:124)} are the suﬃcient statistics, that is when the moments at the pseudo-inputs are
matched.

˜p(f )[φ(u)] = E

This is the central result from which computational savings are derived. Furthermore, this
moment matching condition would appear to necessitate computation of a set of integrals
to ﬁnd the zeroth, ﬁrst and second moments. However, the technique known as ‘diﬀerenti-
ation under the integral sign’2 provides a useful shortcut that only requires one integral to
compute the log-normaliser of the tilted distribution, log ˜Zn = log E
\n(f )[pα(yn|fn)], before
q∗
diﬀerentiating w.r.t. the cavity mean to give

mu = m\

n
n
u + V\
ufn

d log ˜Zn
n
dm\
fn

;

Vu = V\

n
n
u + V\
ufn

d2 log ˜Zn
n
fn )2
d(m\

n
V\
fnu.

(20)

Update: Having computed the new approximate posterior, the approximate factor tn,new(u) =
q∗(f )/q∗
n(f ) can be straightforwardly obtained, resulting in,
\

1
T1,n,new = V−

n
u mu − (V\
u )−

1m\

n
u , T2,n,new = V−

n
u − (V\
u )−

1

1, zα

n = ˜ZneG

\n
∗ (u))

(q

(q∗(u)),

−G

2. In this case, the dominated convergence theorem can be used to justify the interchange of integration

and diﬀerentiation (see e.g. Brown, 1986).

13

Bui, Yan and Turner

where we have deﬁned the log-normaliser as the functional G( ˜N (u; z, T1, T2)) =
log (cid:82) ˜N (u; z, T1, T2)du. Remarkably, these results and eq. (20) reveals that T2,n,new is
a rank-1 matrix. As such, the minimal and simplest way to parameterise the approxi-
mate factor is tn(u) = znN (KfnuK−
uuu; gn, vn), where gn and vn are scalars, resulting in a
signiﬁcant memory saving and O(N M 2) cost.

1

In addition to providing the approximate posterior after convergence, Power EP also
provides an approximate log-marginal likelihood for model selection and hyper-parameter
optimisation,

log ZPEP = log

p(f )

tn(u)df = G(q∗(u)) − G(p∗(u)) +

log zn.

(21)

(cid:90)

(cid:89)

n

(cid:88)

n

Armed with these general results, we now consider the implications for Gaussian Process
regression.

3.6 Gaussian Regression case

When the model contains Gaussian likelihood functions, closed-form expressions for the
Power EP approximate factors at convergence can be obtained and hence the approximate
posterior:

tn(u) = N (KfnuK−

uuu; yn, αDfnfn + σ2
1

1
y), q(u) = N (u; Kuf K−

ﬀ y, Kuu − Kuf K−

ﬀ Kfu)

1

where Kﬀ = Qﬀ + αdiag(Dﬀ ) + σ2
yI and Dﬀ = Kﬀ − Qﬀ as deﬁned in section 2. These
analytic expressions can be rigorously proven to be the stable ﬁxed point of the Power EP
procedure using remark 1. Brieﬂy, assuming the factors take the form above, the natural
n(u) become,
parameters of the cavity q∗
\

n
1,u = T1,u − αγnynKfnuK−
T\

n
1
2,u = T2,u − αγnK−
uu, T\

1

uuKufnKfnuK−

1
uu,

(22)

n = αDfnfn + σ2
1
y. The subtracted quantities in the equations above are exactly
where γ−
the contribution the likelihood factor makes to the cavity distribution (see remark 1) so
n(u) (cid:82) p(fn|u)pα(yn|fn)dfn ∝ q∗(u). Therefore, the posterior
(cid:82) q∗
approximation remains unchanged after an update and the form for the factors above is the
ﬁxed point. Moreover, the approximate log-marginal likelihood is also analytically tractable,

n(f )pα(yn|fn)df
\

=u = q∗
\

log ZPEP = −

log(2π) −

log |Kﬀ | −

N
2

1
2

y(cid:124)K−

1
ﬀ y −

1
2

1 − α
2α

(cid:88)

n

log (cid:0)1 + αDfnfn/σ2

(cid:1) .

y

We now look at special cases and the correspondence to the methods discussed in section 2.

Remark 2 When α = 1 [EP], the Power EP posterior becomes the FITC posterior in
eq. (7) and the Power EP approximate marginal likelihood becomes the FITC marginal like-
lihood in eq. (8). In other words, the FITC approximation for GP regression is, surprisingly,
equivalent to running an EP algorithm for sparse GP posterior approximation to conver-
gence.

14

Unifying Gaussian Process Approximations

Remark 3 As α → 0 the approximate posterior and approximate marginal likelihood are
identical to that of the VFE approach in eqs. (9) and (10) (Titsias, 2009). This result uses
1 log(1 + x) = 1. So FITC and Titsias’s VFE approach employ the
the limit:
same form of pseudo-point approximation, but reﬁne it in diﬀerent ways.

limx

0 x−

→

Remark 4 For ﬁxed hyper-parameters, a single pass of Power EP is suﬃcient for conver-
gence in the regression case.

3.7 Extensions: Structured, Inter-domain and Multi-power Power EP

Approximations

The framework can now be generalised in three orthogonal directions:

1. enable structured approximations to be handled that retain more dependencies in the

spirit of PITC (see section 2.1)

of the approximate posterior

2. incorporate inter-domain pseudo-points thereby adding further ﬂexibility to the form

3. employ diﬀerent powers α for each factor (thereby enabling e.g. VFE updates to be

used for some data points and EP for others).

Given the groundwork above, these three extensions are straightforward. In order to handle
structured approximations, we take inspiration from PITC and partition the data into
B disjoint blocks yb = {yn}n
b (see section 2.1). Each PEP factor update will then
approximate an entire block which will contain a set of data points, rather than just a
single one. This is related to a form of EP approximation that has recently been used to
distribute Monte Carlo algorithms across many machines (Gelman et al., 2014; Xu et al.,
2014).

∈B

In order to handle inter-domain variables, we deﬁne a new domain via a linear transform
g(x) = (cid:82) dx(cid:48)W (x, x(cid:48))f (x(cid:48)) which now contains the pseudo-points g = {g
=u, u}. Choices for
W (x, x(cid:48)) include Gaussians or wavelets. These two extensions mean that the approximation
becomes,

p(f, g)

p(yb|f ) ≈ p(f, g)

tb(u) = q∗(f ).

(23)

(cid:89)

b

(cid:89)

b

Power EP is then performed using private powers αb for each data block, which is the third
generalisation mentioned above. Analytic solutions are again available (covariance matrices
now incorporate the inter-domain transform)

tb(u) = N (KfbuK−

uuu; yb, αbDfbfb + σ2
1

yI),

q(u) = N (u; Kuf K−

1

1
ﬀ y, Kuu − Kuf K−

ﬀ Kfu)

where Kﬀ = Qﬀ + blkdiag({αbDfbfb}B
yI and blkdiag builds a block-diagonal matrix
from its inputs. The approximate log-marginal likelihood can also be obtained in closed-
form,

b=1) + σ2

log ZPEP = −

log(2π) −

log |Kﬀ | −

N
2

1
2

y(cid:124)K−

1
ﬀ y +

1
2

(cid:88)

b

1 − αb
2αb

log (cid:0)I + αbDfbfb/σ2

y

(cid:1) .

15

Bui, Yan and Turner

Remark 5 When αb = 1 and W (x, x(cid:48)) = δ(x − x(cid:48)) the structured Power EP posterior
becomes the PITC posterior and the Power EP approximate marginal likelihood becomes
the PITC marginal likelihood. Additionally, when B = N we recover FITC as discussed in
section 3.6.

Remark 6 When αb → 0 and W (x, x(cid:48)) = δ(x − x(cid:48)) the structured Power EP posterior and
approximate marginal likelihood becomes identical to the VFE approach (Titsias, 2009).
This is a result of the equivalence of local and global exclusive KL divergence minimisation.
See appendix B for more details and ﬁg. 1 for more relationships.

3.8 Classiﬁcation

For classiﬁcation, the non-Gaussian likelihood prevents an analytic solution. As such, the
iterative Power EP procedure is required to obtain the approximate posterior. The pro-
jection step requires computation of the log-normaliser of the tilted distribution, log ˜Zn =
log E
\n(f )[pα(yn|f )] = log E
\n(fn)[Φα(ynfn)]. For general α, this quantity is not available in
q∗
q∗
closed form3. However, it involves a one-dimensional expectation of a non-linear function of
a normally-distributed random variable and, therefore, can be approximated using numeri-
cal methods, e.g. Gauss-Hermite quadrature. This procedure gives an approximation to the
expectation, resulting in an approximate update for the posterior mean and covariance. The
approximate log-marginal likelihood can also be obtained and used for hyper-parameter op-
timisation. As α → 0, it becomes the variational free-energy used in (Hensman et al., 2015)
which employs quadrature for the same purpose. These relationships are shown in ﬁg. 1
which also shows that inter-domain transformations and structured approximations have
not yet been fully explored in the classiﬁcation setting. In our view, the inter-domain gen-
eralisation would be a sensible one to pursue and it is mathematically and algorithmically
straightforward. The structured approximation variant is more complicated as it requires
multiple non-linear likelihoods to be handled at each step of EP. This will require further
approximation such as using Monte Carlo methods (Gelman et al., 2014; Xu et al., 2014).
In addition, when α = 1, M = N and the pseudo-points are at the training inputs, the
standard EP algorithm for GP classiﬁcation is recovered (Rasmussen and Williams, 2005,
sec. 3.6).

Since the proposed Power EP approach is general, an extension to other likelihood
functions is as simple as for VFE methods (Dezfouli and Bonilla, 2015). For example, the
multinomial probit likelihood can be handled in the same way as the binary case, where the
log-normaliser of the tilted distribution can be computed using a C-dimensional Gaussian
quadrature [C is the number of classes] (Seeger and Jordan, 2004) or nested EP (Riihim¨aki
et al., 2013).

3.9 Complexity

The computational complexity of all the regression and classiﬁcation methods described
in this section is O(N M 2) for training, and O(M 2) per test point for prediction. The
training cost can be further reduced to O(M 3), in a similar vein to the uncollapsed VFE

3. except for special cases, e.g. when α = 1 and Φ(x) is the probit inverse link function, Φ(x) =

(cid:82) x
−∞ N (a; 0, 1)da.

16

Unifying Gaussian Process Approximations

approach (Hensman et al., 2013, 2015), by employing stochastic updates of the poste-
rior and stochastic optimisation of the hyper-parameters using minibatches of data points
In particular, the Power EP update
(Hern´andez-Lobato and Hern´andez-Lobato, 2016).
steps in section 3.2 are repeated for only a small subset of training points and for only a
small number of iterations. The approximate log-marginal likelihood in eq. (21) is then
computed using this minibatch and optimised as if the Power EP procedure has converged.
This approach results in a computationally eﬃcient training scheme, at the cost of return-
ing noisy hyper-parameter gradients. In practice, we ﬁnd that the noise can be handled
using stochastic optimisers such as Adam (Kingma and Ba, 2015). In summary, given these
advances the general PEP framework is as scalable as variational inference.

4. Experiments

The general framework described above lays out a large space of potential inference algo-
rithms suggesting many exciting directions for innovation. The experiments considered in
the paper will investigate only one aspect of this space; how do algorithms that are interme-
diate between VFE (α = 0) and EP/FITC (α = 1) perform? Speciﬁcally, we will investigate
how the performance of the inference scheme varies as a function of α and whether this de-
pends on; the type of problem (classiﬁcation or regression); the dataset (synthetic datasets,
8 real world regression datasets and 6 classiﬁcation datasets); the performance metric (we
compare metrics that require point-estimates to those that are uncertainty sensitive). An
important by-product of the experiments is that they provide a comprehensive comparison
between the VFE and EP approaches which has been an important area of debate in its
own right.

The results presented below are compact summaries of a large number of experiments full
details of which are included in the appendix I (along with additional experiments). Python
and Matlab implementations are available at http://github.com/thangbui/sparseGP_
powerEP.

4.1 Regression on Synthetic Datasets

In the ﬁrst experiment, we investigate the performance of the proposed Power EP method
on toy regression datasets where ground truth is known. We vary α (from 0 VFE to 1
EP/FITC) and the number of pseudo-points (from 5 to 500). We use thirty datasets, each
comprising 1000 data points with ﬁve input dimensions and one output dimension, that
were drawn from a GP with an Automatic Relevance Determination squared exponential
kernel. A 50:50 train/test split was used. The hyper-parameters and pseudo-inputs were
found by optimising the PEP energy using L-BFGS with a maximum of 2000 function
evaluations. The performances are compared using two metrics: standardised mean squared
error (SMSE) and standardised mean log loss (SMLL) as described in (Rasmussen and
Williams, 2005, page 23). The approximate negative log-marginal likelihood (NLML) for
each experiment is also computed. The mean performance using Power EP with diﬀerent α
values and full GP regression is shown in ﬁg. 3. The results demonstrate that as M increases,
the SMLL and SMSE of the sparse methods approach that of full GP. Power EP with α = 0.8
or α = 1 (EP) overestimates the log-marginal likelihood when intermediate numbers of
pseudo-points are used, but the overestimation is markedly less when M = N = 500.

17

Bui, Yan and Turner

Importantly, however, an intermediate value of α in the range 0.5-0.8 seems to be best for
prediction on average, outperforming both EP and VFE.

Figure 3: The performance of various α values averaged over 30 trials. See text for more
details

4.2 Regression on Real-world Datasets

The experiment above was replicated on 8 UCI regression datasets, each with 20 train/test
splits. We varied α between 0 and 1, and M was varied between 5 and 200. Full details
of the experiments along with extensive additional analysis is presented in the appendices.
Here we concentrate on several key aspects. First we consider pairwise comparisons between
VFE (α → 0), Power EP with α = 0.5 and EP/FITC (α = 1) on both the SMSE and SMLL
evaluation metrics. Power EP with α = 0.5 was chosen because it is the mid-point between
VFE and EP and because settings around this value empirically performed the best on
average across all datasets, splits, numbers of inducing points, and evaluation metrics.

In ﬁg. 4A we plot (for each dataset, each split and each setting of M ) the evaluation
scores obtained using one inference algorithm (e.g. PEP α = 0.5) against the score obtained
using another (e.g. VFE α = 0). In this way, points falling below the identity line indicate
experiments where the method on the y-axis outperformed the method on the x-axis. These
results have been collapsed by forming histograms of the diﬀerence in the performance of
the two algorithms, such that mass to the right of zero indicates the method on the y-axis
outperformed that on the x-axis. The proportion of mass on each side of the histogram,
also indicated on the plots, shows in what fraction of experiments one method returns a
more accurate result than the other. This is a useful summary statistic, linearly related to

18

Unifying Gaussian Process Approximations

the average rank, that we will use to unpack the results. The average rank is insensitive to
the magnitude of the performance diﬀerences and readers might worry that this might give
an overly favourable view of a method that performs the best frequently, but only by a tiny
margin, and when it fails it does so catastrophically. However, the histograms indicate that
the methods that win most frequently tend also to ‘win big’ and ‘lose small’, although EP
is a possible exception to this trend (see the outliers below the identity line on the bottom
right-hand plot).

A clear pattern emerges from these plots. First PEP α = 0.5 is the best performing
approach on the SMSE metric, outperforming VFE 67% of the time and EP 78% of the
time. VFE is better than EP on the SMSE metric 64% of the time. Second, EP performs
the best on the SMLL metric, outperforming VFE 93% of the time and PEP α = 0.5 71%
of the time. PEP α = 0.5 outperforms VFE in terms of the SMLL metric 93% of the time.
These pairwise rank comparisons have been extended to other values of α in ﬁg. 5A.
Here, each row of the ﬁgure compares one approximation with all others. Horizontal bars
indicate that the methods have equal average rank. Upward sloping bars indicate the
method shown on that row has lower average rank (better performance), and downward
sloping bars indicate higher average rank (worse performance). The plots show that PEP
α = 0.5 outperforms all other methods on the SMSE metric, except for PEP α = 0.6 which
is marginally better. EP is outperformed by all other methods, and VFE only outperforms
EP on this metric. On the other hand, EP is the clear winner on the SMLL metric, with
performance monotonically decreasing with α so that VFE is the worst.

The same pattern of results is seen when we simultaneously compare all of the methods,
rather than considering sets of pairwise comparisons. The average rank plots shown in
ﬁg. 4B were produced by sorting the performances of the 8 diﬀerent approximating methods
for each dataset, split, and number of pseudo-points M and assigning a rank. These ranks
are then averaged over all datasets and their splits, and settings of M . PEP α = 0.5 is the
best for the SMSE metric, and the two worst methods are EP and VFE. PEP α = 0.8 is the
best for the SMLL metric, with EP and PEP α = 0.6 not far behind (when EP performs
poorly it can do so with a large magnitude, explaining the discrepancy with the pairwise
ranks).

There is some variability between individual datasets, but the same general trends are
clear: For MSE α = 0.5 is better than VFE on 6/8 datasets and EP on 8/8 datasets, whilst
VFE is better than EP on 3 datasets (the diﬀerence on the others being small). For NLL
EP is better than α = 0.5 on 5/8 datasets and VFE on 7/8 datasets, whilst α = 0.5 is better
than VFE on 8/8 datasets. Performance tends to increase for all methods as a function
of the number of pseudo-points M. The interaction between the choice of M and the best
performing inference method is often complex and variable across datasets making it hard
to give precise advice about selecting α in an M dependent way.

In summary, we make the following recommendations based on these results for GP
regression problems. For a MSE loss, we recommend using α = 0.5. For a NLL we recom-
mend using EP. It is possible that more ﬁne grained recommendations are possible based
upon details of the dataset and the computational resources available for processing, but
further work will be needed to establish this.

19

2
0

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 4: Pair-wise comparisons between Power EP with α = 0.5, EP (α = 1) and VFE (α → 0), evaluated on several regression
datasets and various settings of M . Each coloured point is the result for one split. Points that are below the diagonal line
illustrate the method on the y-axis is better than the method on the x-axis. The inset diagrams show the histograms of the
diﬀerence between methods (x-value − y-value), and the counts of negative and positive diﬀerences. Note that this indicates
pairwise ranking of the two methods. Positive diﬀerences mean the y-axis method is better than the x-axis method and vice
versa. For example, the middle, bottom plot shows EP is on average better than VFE.

Unifying Gaussian Process Approximations

Figure 5: Average ranking of various α values in the regression experiment, lower is better.
Top plots show the pairwise comparisons. Red circles denote rows being better than the
corresponding columns, and blue circles mean vice versa. Bottom plots show the ranks of
all methods when being compared together. Intermediate α values (not EP or VFE) are
best on average.

4.3 Binary Classiﬁcation

We also evaluated the Power EP method on 6 UCI classiﬁcation datasets, each has 20
train/test splits. The details of the datasets are included in appendix I.3. The datasets are
all roughly balanced, and the most imbalanced is pima with 500 positive and 267 negative

21

Bui, Yan and Turner

data points. Again α was varied between 0 and 1, and M was varied between 10 and 100.
We adopt the experimental protocol discussed in section 3.9, including: (i) not waiting for
Power EP to converge before making hyper-parameter updates, (ii) using minibatches of
data points for each Power EP sweep, (iii) parallel factor updates. The Adam optimiser was
used with default hyper-parameters to handle the noisy gradients produced by these ap-
proximations (Kingma and Ba, 2015). We also implemented the VFE approach of Hensman
et al. (2015) and include this in the comparison to the PEP methods. The VFE approach
should be theoretically identical to PEP with small α, however, we note that the results
can be slightly diﬀerent due to diﬀerences in the implementation – optimisation for VFE
vs. the iterative PEP procedure and we also note that each step of PEP only gets to see a
tiny fraction of each data point when α is small which can slow the learning speed. Similar
to the regression experiment, we compare the methods using the pairwise ranking plots on
the test error and negative log-likelihood (NLL) evaluation metrics.

In ﬁg. 6, we plot (for each dataset, each split and each setting of M ) the evaluation scores
using one inference algorithm against the score obtained using another [see section 4.2 for a
detailed explanation of the plots]. In contrast to the regression results in section 4.2, there
are no clear-cut winners among the methods. The test error results show that PEP α = 0.5
is marginally better than VFE and EP, while VFE edges EP out in this metric. Similarly,
all methods perform comparably on the NLL scale, except with PEP α = 0.5 outperforming
EP by a narrow magin (65% of the time vs. 35%)

We repeat the pairwise comparison above to all methods and show the results in ﬁg. 7.
The plots show that there is no conlusive winner on the test error metric, and VFE, PEP
α = 0.4 and PEP α = 0.5 have a slight edge over other α values on the NLL metric. Notably,
methods corresponding to bigger α values, such as PEP α = 0.8 and EP, are outperformed
by all other methods. Similar to the regression experiment, we observe the same pattern
of results when all methods are simultaneously compared, as shown in ﬁg. 7. However, the
large errorbars suggest the diﬀerence between the methods is small in both metrics.

There is some variability between individual datasets, but the general trends are clear
and consistent with the pattern noted above. For test error, PEP α = 0.5 is better than
VFE on 1/6 dataset and is better than EP on 3/6 datasets (the diﬀerences on the other
datasets are small). VFE outperforms EP on 2/6 datasets, while EP beats VFE on only
1/6 datasets. For NLL, PEP α = 0.5 only clearly outperforms VFE on 1/6 dataset, but
is worse compared to VFE on 1 dataset (the other 4 datasets have no clear winner). PEP
α = 0.5 is better than EP on 5/6 datasets and EP is better on the remaining dataset). EP
is only better than VFE on 2/6 datasets, and is outperformed by VFE on the other 4/6
datasets. The ﬁnding that PEP and VFE are slightly better than EP on the NLL metric
is surprising as we expected EP perform the best on the uncertainty sensitive metric (just
as was discovered in the regression case). The full results are included in the appendices
(see ﬁgs 25, 26 and 27). Similar to the regression case, we observe that as M increases, the
performance tends to be better for all methods and the diﬀerences between the methods
tend to become smaller, but we have not found evidence for systematic sensitivity to the
nature of the approximation.

In summary, we make the following recommendations based on these results for GP
classiﬁcation problems. For a raw test error loss and for NLL, we recommend using α = 0.5
(or α = 0.4). It is possible that more ﬁne grained recommendations are possible based upon

22

Unifying Gaussian Process Approximations

details of the dataset and the computational resources available for processing, but further
work will be needed to establish this.

23

2
4

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 6: Pair-wise comparisons between Power EP with α = 0.5, EP (α = 1) and VFE (α → 0), evaluated on several classiﬁcation
datasets and various settings of M . Each coloured point is the result for one split. Points that are below the diagonal line illustrate
the method on the y-axis is better than the method on the x-axis. The inset diagrams show the histograms of the diﬀerence
between methods (x-value − y-value), and the counts of negative and positive diﬀerences. Note that this indicates pairwise
ranking of the two methods. Positive diﬀerences means the y-axis method is better than the x-axis method and vice versa.

Unifying Gaussian Process Approximations

Figure 7: Average ranking of various α values in the classiﬁcation experiment, lower is
better. Top plots show the pairwise comparisons. Red circles denote rows being better
than the corresponding columns, and blue circles mean vice versa. Bottom plots show the
ranks of all methods when being compared together.
Intermediate α values (not EP or
VFE) are best on average.

5. Discussion

It is diﬃcult to identify precisely where the best approximation methods derive their advan-
tages, but here we will speculate. Since the negative variational free-energy is a lower-bound
on the log-marginal likelihood it has the enviable theoretical guarantee that pseudo-input

25

Bui, Yan and Turner

optimisation is always guaranteed to improve the estimate of the log marginal likelihood
and the posterior (as measured by the inclusive KL). The negative EP energy, in contrast,
is not generally a lower bound which can mean that pseudo-input optimisation drives the
solution to the point where the EP energy over-estimates the log marginal likelihood the
most, rather than to the point where the marginal likelihood and/or posterior estimate
is best. For this reason, we believe that variational methods are likely to be better than
EP if the goal is to derive accurate marginal likelihood estimates, or accurate predictive
distributions, for ﬁxed hyper-parameter settings. For hyper-parameter optimisation, things
are less clear cut since variational methods are biased away from the maximal marginal
likelihood, towards hyper-parameter settings for which the posterior approximation is ac-
curate. Often this bias is severe and also creates local-optima (Turner and Sahani, 2011).
So, although EP will generally also be biased away from the maximal marginal likelihood
and potentially towards areas of over-estimation, it can still outperform variational meth-
ods. Superposed onto these factors, is a general trend for variational methods to minimise
MSE / classiﬁcation error-rate and EP methods to minimise negative log-likelihood, due
to the form of their respective energies (the variational free-energy includes the average
training MSE in the regression case, for example). Intermediate methods will blend the
strengths and weaknesses of the two extremes. It is interesting that values of α around a
half are arguably the best performing on average. Similar empirical conclusions have been
made elsewhere Minka (2005); Hern´andez-Lobato et al. (2016); Depeweg et al. (2016). In
this case, the alpha-divergence interpretation of Power EP shows that it is minimising the
Hellinger distance whose square root is a valid distance metric. Further experimental and
theoretical work is required to clarify these issues.

The results presented above employed (approximate) type-II maximum likelihood ﬁtting
of the hyper-parameters. This estimation method is known in some circumstances to overﬁt
the data. It is therefore conceivable therefore that pseudo-point approximations, which have
a tendency to encourage under-ﬁtting due to their limited representational capacity, could
be beneﬁcial due to them mitigating overﬁtting. We do not believe that this is a strong eﬀect
in the experiments above. For example, in the synthetic data experiments the NLML, SMSE
and SMLL obtained from ﬁtting the unapproximated GP were similar to those obtained
using the GP from which the data were generated, indicating that overﬁtting is not a strong
eﬀect (see ﬁg. 9 in the appendix). It is true that EP and α = 0.8 over-estimates the marginal
likelihood in the synthetic data experiments, but this is a distinct eﬀect from over-ﬁtting
which would, for example, result in overconﬁdent predictions on the test dataset. The SMSE
and SMLL on the training and test sets, for example, are similar which is indicative of a
well-ﬁt model. It would be interesting to explore distributional hyper-parameter estimates
(see e.g. Piironen and Vehtari, 2017) that employ these pseudo-point approximations.

One of the features of the approximate generative models introduced in section 2.1 for
regression, is that they contain input-dependent noise, unlike the original model. Many
datasets contain noise of this sort and so approximate models like FITC and PITC, or
models in which the observation noise is explicitly modelled are arguably more appropri-
ate than the original unapproximated regression model (Snelson, 2007; Saul et al., 2016).
Motivated by this train of reasoning, Titsias (2009) applied the variational free-energy ap-
proximation to the FITC generative model an approach that was later generalised by Hoang
et al. (2016) to encompass a more general class of input dependent noise, including Markov

26

Unifying Gaussian Process Approximations

structure (Low et al., 2015). Here the insight is that the resulting variational lower bound
separates over data points (Hensman et al., 2013) and is, therefore, amenable to stochastic
optimisation using minibatches unlike the marginal likelihood. In a sense, these approaches
unify the approximate generative modelling approach, including the FITC and PITC vari-
ants, with the variational free-energy methods. Indeed, one approach is to posit the desired
form of the optimal variational posterior, and to work backwards from this to construct
the generative model implied (Hoang et al., 2016). However, these approaches are quite
diﬀerent from the one described in this paper where FITC and PITC are shown to emerge
in the context of approximating the original unapproximated GP regression model using
Power EP. Indeed, if the goal really is to model input dependent noise, it is not at all clear
that generative models like FITC are the most sensible. For example, FITC uses a single
set of hyper-parameters to describe the variation of the underlying function and the input
dependent noise.

6. Conclusion

This paper provided a new unifying framework for GP pseudo-point approximations based
on Power EP that subsumes many previous approaches including FITC, PITC, DTC, Tit-
sias’s VFE method, Qi et al’s EP method, and inter-domain variants. It provided a clean
computational perspective on the seminal work of Csat´o and Opper that related FITC to
EP, before extending their analysis signiﬁcantly to include a closed form Power EP marginal
likelihood approximation for regression, connections to PITC, and further results on clas-
siﬁcation and GPSSMs. The new framework was used to devise new algorithms for GP
regression and GP classiﬁcation. Extensive experiments indicate that intermediate values
of Power EP with the power parameter set to α = 0.5 often outperform the state-of-the-
art EP and VFE approaches. The new framework suggests many interesting directions
for future work in this area that we have not explored, for example, extensions to online
inference, combinations with special structured matrices (e.g. circulant and Kronecker struc-
ture), Bayesian hyper-parameter learning, and applications to richer models. The current
work has only scratched the surface, but we believe that the new framework will form a
useful theoretical foundation for the next generation of GP approximation schemes.

Acknowledgments

The authors would like to thank Prof. Carl Edward Rasmussen, Nilesh Tripuraneni,
Matthias Bauer, James Hensman, and Hugh Salimbeni for insightful comments and dis-
cussion. TDB thanks Google for funding his European Doctoral Fellowship. RET thanks
EPSRC grants EP/G050821/1, EP/L000776/1 and EP/M026957/1.

Appendix A. A uniﬁed objective for un-normalised KL variational

free-energy methods

Here we show that performing variational inference by optimising the un-normalised KL
naturally leads to a single objective for both the approximation to the joint distribution,
q∗(f |θ) and the hyper-parameters θ.

27

Bui, Yan and Turner

The un-normalised KL is given by

(cid:90)

KL(q∗(f |θ)||p(f, y|θ)) =

q∗(f |θ)
p(f, y|θ)
This is intractable as it includes the marginal likelihood p(y|θ) = (cid:82) p(f, y|θ)df . However,
since we are interested in minimising this objective with respect to q∗(f |θ) we can ignore
the intractable term,

(p(f, y|θ) − q∗(f |θ)) df.

q∗(f |θ) log

df +

(24)

(cid:90)

argmin
q∗(f
θ)

KL(q∗(f |θ)||p(f, y|θ)) = argmax
θ)

q∗(f

|

(cid:0)p(y|θ) − KL(q∗(f |θ)||p(f, y|θ))(cid:1)

(cid:18)(cid:90)

= argmax
q∗(f
θ)

|

|

q∗(f |θ) log

df +

q∗(f |θ)df

.

(cid:90)

p(f, y|θ)
q∗(f |θ)

(25)

(cid:19)

(26)

In other words, we have turned the unnormalised KL into a tractable lower-bound of the
marginal likelihood G(q∗(f |θ), θ) = p(y|θ)−KL(q∗(f |θ)||p(f, y|θ)). The structure of this new
lower-bound can be understood by decomposing the approximation to the joint distribution
into a normalised posterior approximation q(f |θ) and an approximation to the marginal
likelihood, ZVFE, that is q∗(f |θ) = ZVFEq(f |θ).

G(ZVFEq(f |θ), θ) = ZVFE

1 − log ZVFE +

q(f |θ) log

(27)

(cid:18)

(cid:90)

(cid:19)

p(f, y|θ)
q(f |θ)

df

We can see that optimising the lower-bound with respect to θ is equivalent to optimising the
standard variational free-energy F(q(f |θ), θ) = (cid:82) q(f |θ) log p(f,y
θ)
θ) df . Moreover, optimising
|
q(f
|
for ZVFE recovers Zopt

VFE = exp(F(q(f |θ), θ)). Substituting this back into the bound

G(Zopt

VFEq(f |θ), θ) = Zopt

VFE = exp(F(q(f |θ), θ)).

(28)

In other words, the new collapsed bound is just the exponential of the original variational
free-energy and optimising the collapsed bound for θ is equivalent to optimising the approx-
imation to the marginal likelihood.

Appendix B. Global and local inclusive KL minimisations

In this section, we will show that optimising a single global inclusive KL-divergence,
KL(q||p),
is equivalent to optimising a sum of a set of local inclusive KL-divergence,
KL(q||˜p), where p, q and ˜p are the exact posterior, the approximate posterior and the tilted
distribution accordingly. Without loss of generality, we assume that p(θ) = (cid:81)
n fn(θ) ≈
(cid:81)
n tn(θ) = q(θ), that is the exact posterior is a product of factors, {fn(θ)}n, each of which
is approximated by an approximate factor tn(θ). Substituting these distributions into the
global KL-divergence gives,

KL(q(θ)||p(θ)) =

dθq(θ) log

(cid:90)

(cid:90)

=

dθq(θ) log

q(θ)
p(θ)
(cid:81)
(cid:81)

n tn(θ)
n fn(θ)

28

Unifying Gaussian Process Approximations

(cid:21)

=n ti(θ)
=n ti(θ)

i

i

(cid:20) (cid:81)
(cid:81)

(cid:81)

(cid:81)

n

(cid:81)

(cid:81)

n
i ti(θ)]

n tn(θ)
n fn(θ)
n[(cid:81)
(cid:81)
n[fn(θ) (cid:81)
(cid:81)
[fn(θ) (cid:81)

=n ti(θ)]

i
i ti(θ)

=n ti(θ)

i

=

dθq(θ) log

(cid:90)

(cid:90)

=

dθq(θ) log

(cid:81)

(cid:90)

(cid:88)

=

=

n
(cid:88)

n

dθq(θ) log

KL(q(θ)||˜pn(θ)),

which means running the EP procedure, where we use KL(q(θ)||˜pn(θ)) in place of
KL(˜pn(θ)||q(θ)), is equivalent to the VFE approach which optimises a single global KL-
divergence, KL(q(θ)||p(θ)).

Appendix C. Some relevant linear algebra and function expansion

identities

The Woodbury matrix identity or Woodbury formula is:

(A + U CV )−

1 = A−

1 − A−

1U (C−

1 + V A−

1U )−

1V A−

1.

(30)

In general, C need not be invertible, we can use the Binomial inverse theorem,

(A + U CV )−

1 = A−

1 − A−

1U C(C + CV A−

1U C)−

1CV A−

1.

(31)

When C is an identity matrix and U and V are vectors, the Woodbury identity can be

shortened and become the Sherman-Morrison formula,

(A + uv

(cid:124)

)−

1 = A−

1 −

1uv(cid:124)A−
1
A−
1 + v(cid:124)A−
1u

.

Another useful identity is the matrix determinant lemma,

det(A + uv

) = (1 + v

(cid:124)

(cid:124)

A−

1u)det(A).

The above theorem can be extend for matrices U and V ,

det(A + U V

) = det(I + V

(cid:124)

(cid:124)

A−

1U )det(A).

We also make use of the following Maclaurin series,

exp(x) = 1 + x +

+

+ · · ·

and log(1 + x) = x −

+

+ · · · .

x3
3!

x2
2!

x3
3

x2
2

29

(29)

(32)

(33)

(34)

(35)

(36)

Bui, Yan and Turner

Appendix D. KL minimisation between Gaussian processes and moment

matching

The diﬃcult step of Power-EP is the projection step, that is how to ﬁnd the posterior
approximation q(f ) that minimises the KL divergence, KL(˜p(f )||q(f )), where ˜p(f ) is the
tilted distribution. We have chosen the form of the approximate posterior

q(f ) = p(f

=u|u)q(u) = p(f

=u|u)

exp(θ

(cid:124)
uφ(u))

,

Z(θu)

where Z(θu) = (cid:82) exp(θ
minimisation objective as follows,

(cid:124)
uφ(u))du to ensure normalisation. We can then write the KL

FKL = KL(˜p(f )||q(f ))
˜p(f )
q(f )

˜p(f ) log

=

(cid:90)

df

= (cid:104)log ˜p(f )(cid:105)˜p(f ) − (cid:104)log p(f

=u|u)(cid:105)˜p(f ) − θ

(cid:124)
u(cid:104)φ(u)(cid:105)˜p(f ) + log Z(θu).

=u|u) is the prior conditional distribution, the only free parameter that controls
Since p(f
our posterior approximation is θu. As such, to ﬁnd θu that minimises FKL, we ﬁnd the
gradient of FKL w.r.t θu and set it to zero,

0 =

dFKL
dθu

= −(cid:104)φ(u)(cid:105)˜p(f ) +

d log Z(θu)
dθu

= −(cid:104)φ(u)(cid:105)˜p(f ) + (cid:104)φ(u)(cid:105)q(u),

therefore, (cid:104)φ(u)(cid:105)˜p(f ) = (cid:104)φ(u)(cid:105)q(u). That is, though we are trying to perform the KL min-
imisation between two Gaussian processes, due to the special form of the posterior approx-
imation, it is suﬃcient to only match the moments at the inducing points u.4

Appendix E. Shortcuts to the moment matching equations

The most crucial step in Power-EP is the moment matching step as discussed above. This
step can be done analytically for the Gaussian case, as the mean and covariance of the
approximate posterior can be linked to the cavity distribution as follows,

mu = m\

n
n
u + V\
uf

,

d log Ztilted,n
n
dm\
f
d2 log Ztilted,n
dm\
f

n,2

n
n
u + V\
Vu = V\
uf

n
V\
f u,

where Ztilted,n is the normaliser of the tilted distribution,

(cid:90)

Ztilted,n =

q\

n(f )p(yn|f )df

4. We can show that this condition gives the minimum of FKL by computing the second derivative.

30

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

Unifying Gaussian Process Approximations

(cid:90)

(cid:90)

=

=

q\

n(f )p(yn|fn)df

q\

n(fn)p(yn|fn)dfn.

n
n
u + V\
mu = m\
ufn

,

d log Ztilted,n
n
dm\
fn
d2 log Ztilted,n
dm\
fn

n,2

n
n
u + V\
Vu = V\
ufn

n
V\
fnu.

In words, Ztilted,n only depends on the marginal distribution of the cavity process, q\
simplifying the moment matching equations above,

n(fn),

n
n
1
u K−
ufn = V\
We can rewrite the cross-covariance V\

uuKufn. We also note that, m\

n
fn =

n
1
uum\
KfnuK−
u , resulting in,

d log Ztilted,n
n
dm\
u

=

d log Ztilted,n
n
dm\
fn

1

K−

uuKufn,

d log Ztilted,n
n
dV\
u

= K−

uuKufn

1

d2 log Ztilted,n
dm\
fn

n,2

1
KfnuK−
uu.

Substituting these results back in eqs. 48 and 49, we obtain

n
u + V\
mu = m\
u

n

,

d log Ztilted,n
n
dm\
u
d2 log Ztilted,n
dm\
u

n,2

n
n
u + V\
Vu = V\
u

n
V\
u .

Therefore, using eqs. 48 and 49, or eqs. 52 and 53 are equivalent in our approximation

settings.

Appendix F. Full derivation of the Power-EP procedure

We provide the full derivation of the Power-EP procedure in this section. We follow the
derivation in (Qi et al., 2010) closely, but provide a clearer exposition and details how to
get to each step used in the implementation, and how to handle powered/fractional deletion
and update in Power-EP.

F.1 Optimal factor parameterisation

We start by deﬁning the approximate factors to be in natural parameter form as this makes
it simple to combine and delete them, tn(u) = ˜N (u; zn, T1,n, T2,n) = zn exp(u(cid:124)T1,n −
2 u(cid:124)T2,nu). We initially consider full rank T2,n, but will show that the optimal form is rank
1
1.

31

Bui, Yan and Turner

The next goal is to relate these parameters to the approximate GP posterior. The
n T1,n
n T2,n. This induces an approximate GP posterior with mean and

approximate posterior over the pseudo-outputs has natural parameters T1,u = (cid:80)
and T2,u = K−
covariance function,

uu + (cid:80)

1

1
1
2,uT1,u = Kfuγ
mf = KfuK−
uuT−
1
1
2,uK−
uuT−
Vﬀ (cid:48) = Kﬀ (cid:48) − Qﬀ (cid:48) + KfuK−

1

uuKuf (cid:48) = Kﬀ (cid:48) − KfuβKuf (cid:48).

(54)

(55)

where γ and β are likelihood-dependent terms we wish to store and update using PEP; γ
and β fully specify the approximate posterior.

Deletion step: The cavity for data point n, q\

n(u), has a similar form to
n
1,u = T1,u − αT1,n
the posterior, but the natural parameters are modiﬁed by the deletion, T\

n(f ) ∝ q∗(f )/tα

n
2,u = T2,u − αT2,n, yielding a new mean and covariance function
and T\

(56)

nKuf (cid:48).

uuKuf (cid:48) = Kﬀ (cid:48) − Kfuβ\

n
n,
1
n
1,u = Kfuγ\
2,u T\
−

n
1
uuT\
f = KfuK−
m\
n
1
1
uuT\n, −12,uK−
ﬀ (cid:48) = Kﬀ (cid:48) − Qﬀ (cid:48) + KfuK−
V \
Projection step: The central step in Power EP is the projection step. Obtaining the new
approximate unormalised posterior q∗(f ) such that KL(˜p(f )||q∗(f )) is minimised would
na¨ıvely appear intractable. Fortunately, as shown in the previous section, because of the
=u|u)q(u), the objective, KL(˜p(f )||q∗(f ))
structure of the approximate posterior, q(f ) = p(f
is minimised when E
q(u)[φ(u)], where φ(u) are the suﬃcient statistics, that is
when the moments at the pseudo-inputs are matched. This is the central result from which
computational savings are derived. Furthermore, this moment matching condition would
appear to necessitate computation of a set of integrals to ﬁnd the zeroth, ﬁrst and second
moments. Using results from the previous section simpliﬁes and provides the following
shortcuts,

˜p(f )[φ(u)] = E

(57)

n
n
u + V\
mu = m\
ufn

n
n
Vu = V\
u + V\
ufn

n
V\
fnu.

d log ˜Zn
n
dm\
fn
d2 log ˜Zn
n
fn )2
d(m\

where log ˜Zn = log E

q\n(f )[pα(yn|fn)] is the log-normaliser of the tilted distribution.

Update step: Having computed the new approximate posterior, the fractional approx-

imate factor tn,new(u) = q∗(f )/q\

n(f ) can be straightforwardly obtained, resulting in,

T1,n,new = V−

1

n
1
n,
u m\
u mu − V\
−
u
1

n,

1

u − V\
T2,n,new = V−
u
n = ˜Zn exp(G
zα
\n
∗ (u)
q

−

− Gq∗(u)),
(u;z,T1,T2) = (cid:82) ˜N (u; z, T1, T2)du. Let d1 = d log ˜Zn

where G ˜
N

eq. (30) and eq. (59), we have,

and d2 = d2 log ˜Zn

)2 . Using

d(m

\n
fn

dm

\n
fn

1
V−

u − V\
u

n,

1

−

= −V\

n,
1
u V\
−

n
ufn

n
1
fnuV\
2 + V\
d−

n
n,
1
u V\
−
ufn

n
fnuV\
V\
u

n,

1

−

(63)

(cid:105)−
1

(cid:104)

32

(58)

(59)

(60)

(61)

(62)

Unifying Gaussian Process Approximations

n
1
Let vn = α(−d−
fnuV\
2 − V\
eq. (61) gives

n
n,
1
ufn), and wn = V\
u V\
−

1
n,
u V\
−

n
ufn. Combining eq. (63) and

1
T2,n,new = wnαv−

n w(cid:124)

n

At convergence, we have tn(u)α = tn,new(u), hence T2,n = wnv−
1
n w
optimally a rank-1 matrix. Note that,

(cid:124)
n. In words, T2,n is

wn = V\

n
n,
1
u V\
−
ufn

= (Kuu − Kuuβ\

1
= K−
1
= K−

uu(I − Kuuβ\
uuKufn.

nKuu)−
n)−

1(Kufn − Kuuβ\
n)Kufn

1(I − Kuuβ\

nKufn)

Using eq. (58) an eq. (64) gives,

1
V−

1

n,

n w(cid:124)
n
n
u mu = (V\
u + V\
n)(m\
ufnd1)
u
n w(cid:124)
n,
1
n
n
n,
1
1
u V\
u + V\
nm\
u + wnαv−
u m\
−
−

+ wnαv−

= V\

−

1

n
1
ufnd1 + wnαv−

n w(cid:124)

n
nV\
ufnd1

Substituting this result into eq. (60),

T1,n,new = V−

n
n,
1
u m\
−
u

1

u mu − V\
n w(cid:124)
1
nm\
(cid:16)
w(cid:124)

1

= wnαv−

= wnαv−
n

n
u + V\

n w(cid:124)
n
n,
1
ufnd1 + wnαv−
u V\
−
(cid:17)
u + d1vn/α + w(cid:124)
n
nV\
.
ufnd1

n
nm\

1

n
nV\
ufnd1

Let T1,n,new = wnαv−

n gn, we obtain,

1

gn = −

+ Kfnuγ\

n.

d1
d2

At convergence, T1,n = wnv−
and T2,n at convergence,

1

n gn. Re-writing the form of the approximate factor using T1,n

tn(u) = ˜N (u; zn, T1,n, T2,n)
1
= zn exp(u(cid:124)T1,n −
2

= zn exp(u(cid:124)wnv−

1

n gn −

u(cid:124)wnv−

n w(cid:124)

1

nu)

u(cid:124)T2,nu)
1
2

As a result, the minimal and simplest way to parameterise the approximate factor is tn(u) =
˜znN (w
uuu; gn, vn), where gn and vn are scalars, resulting in a
signiﬁcant memory saving compared to the parameterisation using T1,n and T2,n.

(cid:124)
1
nu; gn, vn) = ˜znN (KfnuK−

33

(64)

(65)

(66)

(67)

(68)

(69)

(70)

(71)

(72)

(73)

(74)

(75)

(76)

(77)

Note that:

and

F.2 Projection

We now recall the update equations in the projection step (eqns. 58 and 59):

Bui, Yan and Turner

n
n
u + V\
mu = m\
ufnd1,
n
n
n
ufnd2V\
u + V\
Vu = V\
fnu.

mu = Kuuγ,
Vu = Kuu − KuuβKuu,

n
u = Kuuγ\
m\
n
u = Kuu − Kuuβ\
V\

n,

nKuu.

γ = K−

= K−

= γ\

β = K−

1
uumu
n
n
1
uu(m\
u + V\
ufnd1)
n
1
n + K−
uuV\
ufnd1, and
1
1
uu(Kuu − Vu)K−
uu

= K−

= β\

1

n
n
n
1
fnu)K−
ufnd2V\
u − V\
uu(Kuu − V\
uu
n
n
1
n − K−
1
fnuK−
ufnd2V\
uuV\
uu

(78)

(79)

(80)

(81)

(82)

(83)

(84)

(85)

(86)

(87)

(88)

(89)

Using these results, we can convert the update for the mean and covariance, mu and Vu,
into an update for γ and β,

F.3 Deletion step

Finally, we present how deletion might be accomplished. One direct approach to this step
is to divide out the cavity from the cavity, that is,

q\

n(f ) ∝

q(f )
tα
n(u)

p(f

=

=u|u)q(u)
tα
n(u)

= p(f

=u|u)q\

n(u).

(90)

Instead, we use an alternative using the KL minimisation as used in (Qi et al., 2010), by
realising that doing this will result in an identical outcome as the direct approach since
the factor and distributions are Gaussian. Furthermore, we can re-use results from the
projection and inclusion steps, by simply swapping the quantities and negating the site
approximation variance. In particular, we present projection and deletion side-by-side, to
facilitate the comparison,

Projection:

q(f ) ≈ q\

n(f )p(yn|fn)

(91)

34

Unifying Gaussian Process Approximations

Deletion:

q\

n(f ) ∝ q(f )

1
tα
n(u)

The projection step minimises the KL between the LHS and RHS while moment match-
n(f ), and thus

ing, to get q(f ). We would like to do the same for the deletion step to ﬁnd q\
reuse the same moment matching results for γ and β with some modiﬁcations.

Our task will be to reuse Equations 86 and 89, the moment matching equations in γ
and β. We have two diﬀerences to account for. Firstly, we need to change any uses of
the parameters of the cavity distribution to the parameters of the approximate posterior,
n
n to β. This is the equivalent of re-deriving the entire
V\
ufn to Vufn, γ\
projection operation while swapping the symbols (and quantities) for the cavity and the
full distribution. Secondly, the derivatives d1 and d2 are diﬀerent here, as

n to γ and β\

Now, we note

log ˜Zn = log

q(f )

1
tα
n(u)

df

1
tn(u)

∝

∝

N α(w

(cid:124)
nu; gn, vn)

1

(cid:16)

exp

− α

1
n (w

2 v−

1

= exp

(cid:18) 1
2
∝ N (w(cid:124)
nu; gn, −vn/α)

n (w(cid:124)

αv−

(cid:124)

nu − gn)2(cid:17)
(cid:19)

nu − gn)2

Then we obtain the derivatives of log ˜Zn

= − (cid:2)Kfn,uK−

u,uKu,fn − Kfn,uβKu,fn − vn/α(cid:3)−

1

1

˜d2 =

˜d1 =

d2 log ˜Zn
dm2
fn
d log ˜Zn
dmfn

= (Kfn,uγ − gn) ˜d2

Putting the above results together, we obtain,

γ\

β\

1

n = γ + K−
1
n = β − K−

uuVufn
uuVufn

˜d1, and
˜d2VfnuK−
1
uu

(cid:90)

1

35

F.4 Summary of the PEP procedure

We summarise here the key steps and equations that we have obtained, that are used in
the implementation:

(92)

(93)

(94)

(95)

(96)

(97)

(98)

(99)

(100)

(101)

Bui, Yan and Turner

1. Initialise the parameters: {gn = 0}N

n=1, {vn = ∞}N

n=1, γ = 0M

1 and β = 0M

×

M

×

2. Loop through all data points until convergence:

(a) Deletion step: ﬁnd γ\

n
n and β\

(b) Projection step: ﬁnd γ and β

(c) Update step: ﬁnd gn,new and vn,new

γ\

β\

1
n = γ + K−
n = β − K−

uuVufn
uuVufn

1

˜d1, and
˜d2VfnuK−
1
uu

γ = γ\

β = β\

n
1
n + K−
uuV\
ufnd1,
n
1
n − K−
ufnd2V\
uuV\

n
1
fnuK−
uu

gn,new = −

d1
d2
n
1
fnuV\
2 − V\
vn,new = −d−

+ Kfnuγ\

n,

n
n,
1
u V\
−
ufn

and parameters for the full factor,

1
vn ← (v−
gn ← vn(gn,newv−

n,new + (1 − α)v−
1

1
1
n )−
n,new + (1 − α)gnv−

1
n )

Appendix G. Power-EP energy for sparse GP regression and

classiﬁcation

The Power-EP procedure gives an approximate marginal likelihood, which is the negative
Power-EP energy, as follows,

F = G(q

(u)) − G(p
∗

∗

(u)) +

1
α

(cid:88)

n

(cid:104)
n
log Ztilted,n + G(q\
∗

(u)) − G(q

(cid:105)
(u))

∗

where G(q

(u)) is the log-normaliser of the approximate posterior, that is,

∗

G(q

(u)) = log

p(f

=u|u) exp(θ

(cid:124)
uφ(u))df

=udu

∗

= log

exp(θ

(cid:124)
uφ(u))du

=

log(2π) +

log |V| +

m(cid:124)V−

1m,

1
2

(cid:90)

(cid:90)

M
2

1
2

1
2

1
2

36

where m and V are the mean and covariance of the posterior distribution over u, respec-
tively. Similarly,

n
G(q\
∗

M
2

(u)) =

log(2π) +

log |Vcav,n| +

m(cid:124)

1
cav,nV−

cav,nmcav,n,

(114)

(102)

(103)

(104)

(105)

(106)

(107)

(108)

(109)

(110)

(111)

(112)

(113)

(115)

(116)

(117)

(118)

(119)

(120)

(121)

(122)

(123)

(124)

Unifying Gaussian Process Approximations

and G(p
∗

(u)) =

M
2

log(2π) +

log |Kuu|.

1
2

Finally, log Ztilted,n is the log-normalising constant of the tilted distribution,

log Ztilted = log

qcav(f )pα(yn|f )df

= log

p(f

=u|u)qcav(u)pα(yn|f )df

=udu

= log

p(fn|u)qcav(u)pα(yn|fn)dfndu

(cid:90)

(cid:90)

(cid:90)

Next, we can write down the form of the natural parameters of the approximate posterior

and the cavity distribution, based on the approximate factor’s parameters, as follows,

V−

1 = K−

1
uu +

(cid:88)

(cid:124)
wiτiw
i

V−

1m =

i
wiτi ˜yi

(cid:88)

i

1

V−
cav,n = V−
1mcav,n = V−

1 − αwnτnw(cid:124)
n
1m − αwnτngn

Vcav,n−

Vcav,n = V +

(cid:124)
Vwnατnw
nV
(cid:124)
nατnVwn
1 − w

.

1
Note that τi := v−
i

. Using eq. (32) and eq. (121) gives,

Using eq. (33) and eq. (121) gives,

log det(Vcav,n) = log det(V) − log(1 − w(cid:124)

nατnVwn).

Subsituting eq. (123) and eq. (124) back to eq. (114) results in,

n
G(q\
∗

(u)) =

log(2π) +

1
2
log(1 − w(cid:124)

log det(V) +

nατnVwn) +

1
m(cid:124)V−
1m
2
(cid:124)
m(cid:124)wnατnw
nm
(cid:124)
1 − w
nατnVwn

1
2

M
2
1
2
1
2

−

+

gnατnw(cid:124)

nVcav,nwnατngn − gnατnw(cid:124)

nVcav,nV−

1m

(125)

We now plug the above result back into the approximate marginal likelihood, yeilding,

F =

log |V| +

m(cid:124)V−

1m −

log |Kuu| +

1
2

(cid:20)

(cid:88)

+

−

1
2α

log(1 − w(cid:124)

nατnVwn) +

(cid:88)

1
α

log Ztilted,n

n
(cid:124)
m(cid:124)wnτnw
nm
(cid:124)
nατnVwn
1 − w

1
2

(cid:21)

1
2

n

+

1
2

gnτnw(cid:124)

nVcav,nwnατngn − gnτnw(cid:124)

nVcav,nV−

1m

(126)

1
2

37

Bui, Yan and Turner

1
2

1
2

G.1 Regression

We have shown in the previous section that the ﬁxed point solution of the Power-EP it-
n = dn =
erations can be obtained analytically for the regression case, gn = yn and τ −
uuKufn) + σ2
1
α(Kfnfn − KfnuK−
y. Crucially, we can obtain a closed form expression for
log Ztilted,n,

1

log Ztilted,n = −

log(2πσ2

y) +

log(σ2

y) −

log(αvn + σ2

y) −

1
2

1
2

(yn − µn)2
vn + σ2
y/α

(127)

α
2

σ2
y
where µn = w
α + w
−
therefore simplify the approximate marginal likelihood F further,

1m − wnατnyn) and vn =

(cid:124)
nmcav = w

(cid:124)
nVcav(V−

dn

(cid:124)
nVcavwn. We can

F =

log |V| +

m(cid:124)V−

1m −

log |Kuu| +

= −

log(2π) −

log |D + Qﬀ | −

yT (D + Qﬀ )−

1y −

(cid:20)

(cid:88)

n

−

1
2

log(2πσ2

y) +

log σ2

y −

log dn −

1
2α

1
2α

(cid:21)

y2
n
2dn

1 − α
2α

(cid:88)

n

log(

),

dn
σ2
y

(128)

1
2

N
2

1
2

1
2

1

where Qﬀ = KfuK−

uuKuf and D is a diagonal matrix, Dnn = dn.

When α = 1, the approximate marginal likelihood takes the same form as the FITC

1
2

1
2

marginal likelihood,

F = −

1
2
uuKufn + σ2
1
where Dnn = dn = Kfnfn − KfnuK−
y.
When α tends to 0, we have,

log(2π) −

N
2

1
2

log |D + Qﬀ | −

yT (D + Qﬀ )−

1y

(129)

1 − α
2α

(cid:88)

n

lim
0
α
→

log(

) =

dn
σ2
y

1
2

(cid:88)

lim
0
α
→

n

log(1 + α gn
σ2
y

)

α

=

(cid:80)

n hn
2σ2
y

,

(130)

1
where hn = Kfnfn − KfnuK−

uuKufn. Therefore,

F = −

log(2π) −

log |σ2

yI + Qﬀ | −

yT (σ2

yI + Qﬀ )−

1y −

(131)

1
2

(cid:80)

n hn
2σ2
y

,

N
2

which is the variational lower bound of Titsias (Titsias, 2009).

G.2 Classiﬁcation

In contrast to the regression case, the approximate marginal likelihood for classiﬁcation
cannot be simpliﬁed due to the non-Gaussian likelihood. Speciﬁcally, log Ztilted,n is not
analytically tractable, except when α = 1 and the classiﬁcation link function is the Gaus-
sian CDF. However, this quantity can be evaluated numerically, using sampling or Gauss-
Hermite quadrature, since it only involves a one-dimensional integral.

We now consider the case when α tends to 0 and verify that in such case the approxi-
mate marginal likelihood becomes the variational lower bound. We ﬁrst ﬁnd the limits of
individual terms in eq. (126):

−

1
2α

lim
0
α
→

log(1 − w(cid:124)

nατnVwn) =

w(cid:124)

nτnVwn

1
2

(132)

38

Unifying Gaussian Process Approximations

m(cid:124)wnτnw(cid:124)

nm

1
2
gnτnw(cid:124)

1
2

(cid:124)
m(cid:124)wnτnw
nm
(cid:124)
nατnVwn
1 − w

nVcav,nwnατngn

=

1
2

= 0

(cid:12)
(cid:12)
(cid:12)
(cid:12)α=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)α=0
(cid:12)
(cid:12)
1m
(cid:12)
(cid:12)α=0

−gnτnw(cid:124)

nVcav,nV−

= −gnτnw(cid:124)

nm.

We turn our attention to log Ztilted,n. First, we expand pα(yn|fn) using eq. (35):
pα(yn|fn) = exp(α log p(yn|fn))

= 1 + α log p(yn|fn) + ξ(α2).

Substituting this result back into log Ztilted/α gives,
(cid:90)

log Ztilted =

log

p(fn|u)qcav(u)pα(yn|fn)dfndu

1
α

(cid:90)

(cid:20)

log

p(fn|u)qcav(u)[1 + α log p(yn|fn) + ξ(α2)]dfndu

log

1 + α

(cid:90)

(cid:21)
p(fn|u)qcav(u) log p(yn|fn)dfndu + α2ξ(1)

(cid:20)

(cid:90)

α

(cid:21)
p(fn|u)qcav(u) log p(yn|fn)dfndu + α2ξ(1)

p(fn|u)qcav(u) log p(yn|fn)dfndu + αξ(1).

(133)

(134)

(135)

(136)

(137)

(138)

(139)

(140)

(141)

(142)

Therefore,

1
2

+

1
2

+

1
2

1
α

lim
0
α
→

(cid:90)

log Ztilted =

p(fn|u)q(u) log p(yn|fn)dfndu.

(143)

Putting these results into eq. (126), we obtain,

F =

log |V| +

m(cid:124)V−

1m −

(cid:88)

n

1
2

w(cid:124)

nτnVwn +

1
2

log |Kuu|

1
2
m(cid:124)wnτnw(cid:124)

nm − gnτnw(cid:124)

nm +

(cid:90)

p(fn|u)q(u) log p(yn|fn)dfndu

=

log |V| +

m(cid:124)V−

log |Kuu| +

m(cid:124)

(V−

1 − K−

1

uu)m − m(cid:124)V−

1m

(cid:88)

n

1
2

w(cid:124)

nτnVwn +

p(fn|u)q(u) log p(yn|fn)dfndu

=

log |V| −

m(cid:124)K−
1

uum −

log |Kuu| +

w(cid:124)

nτnVwn +

p(fn|u)q(u) log p(yn|fn)dfndu.

(cid:90)

(cid:88)

n

(144)

1m −
(cid:90)

1
2

1
2

We now write down the evidence lower bound of the global variational approach of

Titsias (Titsias, 2009), as applied to the classiﬁcation case (Hensman et al., 2015),

FVFE = −KL(q(u)||p(u)) +

p(fn|u)q(u) log p(yn|fn)dfndu

(145)

1
2

(cid:88)

n

1
2

(cid:90)

(cid:88)

n

39

1
α
1
α
1
α
1
α
(cid:90)

=

=

=

=

1
2

1
2

1
2

Bui, Yan and Turner

where

−KL(q(u)||p(u)) = −

trace(K−

uuV) −

1

m(cid:124)K−
1

uum +

log |Kuu| +

log |V|

1
2
1
2

1
2

1
2
(cid:88)

n

−

1
2
m(cid:124)K−

M
2
1
2

= −

trace([V−

1 −

wnτnwn]V) −

1

uum +

M
2

1
2

log |Kuu| +

log |V|

1
2

=

trace(

wnτnwnV) −

log |Kuu| +

log |V|. (146)

(cid:88)

n

m(cid:124)K−
1

uum −

1
2

1
2

1
2

−

1
2

Therefore, FVFE is identical to the limit of the approximate marginal likelihood provided
by power-EP as shown in eq. (144).

Appendix H. The surrogate regression viewpoint

It was written in the main text that it is instructive to view the approximation using
pseudo-points as forming a surrogate exact Gaussian process regression problem such that
the posterior and the marginal likelihood of this surrogate problem are close to that of the
original intractable regression/classiﬁcation problem. This approximation view is useful and
could potentially be used for other intractable probabilistic model, despite that we have not
used this view in the practical implementation of the algorithms/PEP procedure discussed
in this paper. In this section, we detail the surrogate model and how the parameters of
this model can be tuned to match the approximate posterior and approximate marginal
likelihood.

We consider the exact GP regression problem with M surrogate observations ˜y that are
formed by linear combininations the pseudo-outputs and additive surrogate Gaussian noise,
˜y = ˜Wu + ˜Σ1/2(cid:15). The exact posterior and log marginal likelihood can be obtained for this
model as follows,

1(u; ˜W ˜Σ−

log(2π) −

log p(˜y) = −

˜p(u|y) = N −
M
2
1
˜y(cid:124) ˜Σ−
2

−

1
1 ˜y, K−
1
2
1
˜y(cid:124) ˜Σ−
2

uu + ˜W(cid:124) ˜Σ−

1 ˜W)
uu + ˜W(cid:124) ˜Σ−

1

(log |K−

1 ˜W| + log |K−

uu| + log | ˜Σ|)
1

1 ˜y −

1 ˜W(K−
1

uu + ˜W(cid:124) ˜Σ−

1 ˜W)−

1 ˜W(cid:124) ˜Σ−

1 ˜y,

(148)

where we have used the matrix inversion lemma and the matrix determinant lemma in the
1 denotes the Gaussian distribution with natural parameters.
equations above, and that N −
The aim is to show that we can use the above quantities is to match a given approximate
1) and an approximate marginal likelihood F, that is,
posterior q(u) = N −
˜p(u|y) = q(u) and log p(˜y) = F. Substituting the above results into the constraints leading
to the following simpliﬁed constraints:

1(u; S−

1m, S−

where c is a constant. Assume that R is invertible, we can simpliﬁed the above results
further,

1 ˜y = m
1 ˜W = R = K−
1

˜W ˜Σ−
˜W(cid:124) ˜Σ−
1 ˜y + log | ˜Σ| = c,

˜y(cid:124) ˜Σ−

1
uu − S−

˜Σ−

1/2 ˜y = R−

1/2m

40

(147)

(149)

(150)

(151)

(152)

Unifying Gaussian Process Approximations

˜Σ−

1/2 ˜W = R(cid:124)/2
log | ˜Σ| = d,

(153)

(154)

where d is a constant. We can choose ˜Σ, e.g. a diagonal matrix, that satisﬁes the third
equality above. Given ˜Σ, obtaining ˜y and ˜W from the ﬁrst two equalities is trivial.

Appendix I. Extra experimental results

I.1 Comparison between various α values on a toy regression problem

41

4
2

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 8: Results on a toy regression problem: Negative log-marginal likelihood, mean squared error and mean log-loss on the
test set for full Gaussian process regression on synthetic datasets with true hyper-parameters and hyper-parameters obtained
by type-2 ML. Each dot is one trial, i.e. one synthetic dataset. The results demonstrate that type-2 maximum likelihood on
hyper-parameters works well, despite being a little conﬁdent on the log-marginal likelihood on the train set.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

4
3

Figure 9: Results on a toy regression problem with 500 training points: Mean squared error and log-likelihood on train and
test sets on synthetic datasets with hyper-parameters obtained by type-2 ML. In this example, the test error is higher than the
training error, as measured by the mean squared error, because the test points and training points are relatively far apart, making
the prediction task on the training set easier (interpolation) than on the test set (extrapolation). This is consistent with the
results with more training points, shown in ﬁg. 10.

4
4

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 10: Results on a toy regression problem with 1000 training points: Mean squared error and log-likelihood on train and
test sets on synthetic datasets with hyper-parameters obtained by type-2 ML. See ﬁg. 9 for a discussion.

4
5

Figure 11: Results on a toy regression problem: Standardsised mean log-loss on the test set for various values of α and various
number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgure shows the mean for various
α, and the results using GP regression.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

4
6

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 12: Results on a toy regression problem: Standardsised mean squared error on the test set for various values of α and
various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgure shows the mean for
various α, and the results using GP regression.

4
7

Figure 13: Results on a toy regression problem: The negative log marginal likelihood of the training set after training for various
values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgure shows
the mean for various α, and the results using GP regression. Power EP with α close to 1 over-estimates the marginal-likelihood.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

I.2 Real-world regression

We include the details of the regression datasets in table 1 and several comparisons of α
values in ﬁgs. 17 to 22.

Bui, Yan and Turner

Dataset N train/test D

boston
concrete
energy
kin8nm
naval
yacht
power
red wine

455/51
927/103
691/77
7373/819
10741/1193
277/31
8611/957
1439/160

14
9
9
9
18
7
5
12

Table 1: Regression datasets

48

4
9

Figure 14: A comparison between Power-EP with α = 0.5 and VFE on several regression datasets, on two metrics SMSE (top
two rows) and SMLL (bottom two rows). The scatter plots show the performance of Power-EP (α = 0.5) vs VFE. Each point is
one split and points with lighter colours are runs with big M. Points that stay below the diagonal line show α = 0.5 is better than
VFE. The plots right underneat the scatter plots show the histogram of the diﬀerence between methods. Red means α = 0.5 is
better than VFE.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

5
0

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 15: A comparison between EP and VFE on several regression datasets, on two metrics SMSE (top two rows) and SMLL
(bottom two rows). See ﬁg. 14 for more details about the plots.

5
1

Figure 16: A comparison between Power-EP with α = 0.5 and EP on several regression datasets, on two metrics SMSE (top two
rows) and SMLL (bottom two rows). See ﬁg. 14 for more details about the plots.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

5
2

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 17: Results on real-world regression problems: Negative training log-marginal likelihood for diﬀerent datasets, various
values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgures
show the mean for various α for comparison. Lower is better [however, lower could mean overestimation].

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

5
3

Figure 18: Results on real-world regression problems: Negative training log-marginal likelihood for diﬀerent datasets, various
values of α and various number of pseudo-points M , averaged over 20 splits. Lower is better [however, lower could mean
overestimation].

5
4

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 19: Results on real-world regression problems: Standardised mean squared error on the test set for diﬀerent datasets,
various values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost
ﬁgures show the mean for various α for comparison. Lower is better.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

5
5

Figure 20: Results on real-world regression problems: Standardised mean squared error on the test set for diﬀerent datasets,
various values of α and various number of pseudo-points M , averaged over 20 splits. Lower is better.

5
6

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 21: Results on real-world regression problems: Standardised mean log loss on the test set for diﬀerent datasets, various
values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgures
show the mean for various α for comparison. Lower is better.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

5
7

Figure 22: Results on real-world regression problems: Standardised mean log loss on the test set for diﬀerent datasets, various
values of α and various number of pseudo-points M , averaged over 20 splits. Lower is better.

Bui, Yan and Turner

I.3 Real-world classiﬁcation

It was demonstrated in (Hern´andez-Lobato and Hern´andez-Lobato, 2016; Hensman et al.,
2015) that, once optimised, the pseudo points tend to concentrate around the decision
boundary for VFE, and spread out to cover the data region in EP. Figure 23 illustrates the
same eﬀect as α goes from close to 0 (VFE) to 1 (EP).

Figure 23: The locations of pseudo data points vary with α.. Best viewed in colour.

We include the details of the classiﬁcation datasets in table 2 and several comparisons

of α values in ﬁgs. 27 to 30.

58

Unifying Gaussian Process Approximations

Dataset

N train/test D N positive/negative

australian
breast
crabs
iono
pima
sonar

621/69
614/68
180/20
315/35
690/77
186/21

15
11
7
35
9
61

222/468
239/443
100/100
126/224
500/267
111/96

Table 2: Classiﬁcation datasets

59

6
0

Figure 24: A comparison between Power-EP with α = 0.5 and VFE on several classiﬁcation datasets, on two metrics: classiﬁcation
error (top two rows) and NLL (bottom two rows). See ﬁg. 14 for more details about the plots.

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

6
1

Figure 25: A comparison between EP and VFE on several classiﬁcation datasets, on two metrics: classiﬁcation error (top two
rows) and NLL (bottom two rows). See ﬁg. 14 for more details about the plots.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

6
2

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 26: A comparison between Power-EP with α = 0.5 and EP on several classiﬁcation datasets, on two metrics: classiﬁcation
error (top two rows) and NLL (bottom two rows). See ﬁg. 14 for more details about the plots.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

6
3

Figure 27: Results on real-world classiﬁcation problems: Classiﬁcation error rate on the test set for diﬀerent datasets, various
values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgures
show the mean for various α for comparison. Lower is better.

6
4

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 28: Results on real-world classiﬁcation problems: Classiﬁcation error rate on the test set for diﬀerent datasets, various
values of α and various number of pseudo-points M , averaged over 20 splits. Lower is better.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

6
5

Figure 29: Results on real-world classiﬁcation problems: Average negative log-likelihood on the test set for diﬀerent datasets,
various values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost
ﬁgures show the mean for various α for comparison. Lower is better.

6
6

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 30: Results on real-world classiﬁcation problems: Average negative log-likelihood on the test set for diﬀerent datasets,
various values of α and various number of pseudo-points M , averaged over 20 splits. Lower is better.

Unifying Gaussian Process Approximations

I.4 Binary classiﬁcation on even/odd MNIST digits

Figure 31: The test error and log-likelihood of the MNIST binary classiﬁcation task
(M=100).

Figure 32: The test error and log-likelihood of the MNIST binary classiﬁcation task
(M=200).

I.5 When M = N and α = 1, do we recover EP for GPC (Rasmussen and

Williams, 2005, sec. 3.6)?

The key diﬀerence between the EP method in this manuscript when M = N and the pseudo-
inputs and the training inputs are identical, and the standard EP method as described by
(Rasmussen and Williams, 2005, sec. 3.6) is the factor representation. While Rasmussen and
Williams (2005) used a one dimensional un-normalised Gaussian distribution that touches
only one function value fn to approximate each exact factor, the approximate factor used
in the EP scheme described in the main text touches all M pseudo-points, hence all N
function values when the pseudo-inputs are placed at the training inputs. However, in
practice both methods give virtually identical results. Figure 33 shows the approximate log
marginal likelihood and the negative test log-likelihood, given by running the EP procedure

67

Bui, Yan and Turner

described in the main text on the ionosphere dataset. We note that these results are
similar to that of the standard EP method (see Kuss and Rasmussen, 2005).

Figure 33: EP energy on the train set [TOP] and the average negative log-likelihood on the
test set[BOTTOM] when M = N .

68

Unifying Gaussian Process Approximations

References

Mauricio A. ´Alvarez, David Luengo, Michalis K. Titsias, and Neil D. Lawrence. Eﬃcient
multioutput Gaussian processes through variational inducing kernels. In 13th Interna-
tional Conference on Artiﬁcial Intelligence and Statistics, pages 25–32, 2010.

Matthias Bauer, Mark van der Wilk, and Carl E. Rasmussen. Understanding probabilistic
sparse Gaussian process approximations. In Advances in Neural Information Processing
Systems 29, pages 1525–1533, 2016.

Lawrence D. Brown. Fundamentals of Statistical Exponential Families with Applications in
Statistical Decision Theory. Institute of Mathematical Statistics, Hayward, CA, 1986.

Thang D. Bui and Richard E. Turner. Tree-structured Gaussian process approximations.

In Advances in Neural Information Processing Systems 27, pages 2213–2221, 2014.

Thang D. Bui, Cuong V. Nguyen, and Richard E. Turner. Streaming sparse Gaussian
process approximations. In Advances in Neural Information Processing Systems 30, 2017.

Lehel Csat´o. Gaussian Processes — Iterative Sparse Approximations. PhD thesis, Aston

University, 2002.

14(3):641–669, 2002.

Lehel Csat´o and Manfred Opper. Sparse online Gaussian processes. Neural Computation,

Michael R.W. Dawson. Understanding cognitive science. Blackwell Publishing, 1998.

Marc P. Deisenroth. Eﬃcient Reinforcement Learning using Gaussian Processes. PhD

thesis, Karlsruhe Institute of Technology, Karlsruhe, Germany, 2010.

Stefan Depeweg, Jos´e Miguel Hern´andez-Lobato, Finale Doshi-Velez, and Steﬀen Udluft.
Learning and policy search in stochastic dynamical systems with Bayesian neural net-
works. In 4th International Conference on Learning Representations, 2016.

Amir Dezfouli and Edwin V. Bonilla. Scalable inference for Gaussian process models with
black-box likelihoods. In Advances in Neural Information Processing Systems 28, pages
1414–1422, 2015.

Anibal Figueiras-Vidal and Miguel L´azaro-Gredilla. Inter-domain Gaussian processes for
sparse inference using inducing features. In Advances in Neural Information Processing
Systems 22, pages 1087–1095, 2009.

Roger Frigola, Yutian Chen, and Carl E. Rasmussen. Variational Gaussian process state-
space models. In Advances in Neural Information Processing Systems 27, pages 3680–
3688, 2014.

Andrew Gelman, Aki Vehtari, Pasi Jyl¨anki, Christian Robert, Nicolas Chopin, and John P
Cunningham. Expectation propagation as a way of life. arXiv preprint arXiv:1412.4869,
2014.

69

Bui, Yan and Turner

James Hensman, Nicolo Fusi, and Neil D. Lawrence. Gaussian processes for big data. In

29th Conference on Uncertainty in Artiﬁcial Intellegence, pages 282–290, 2013.

James Hensman, Alexander G. D. G. Matthews, and Zoubin Ghahramani. Scalable vari-
In 18th International Conference on Artiﬁcial

ational Gaussian process classiﬁcation.
Intelligence and Statistics, pages 351–360, 2015.

Daniel Hern´andez-Lobato and Jos´e Miguel Hern´andez-Lobato. Scalable Gaussian process
classiﬁcation via expectation propagation. In 19th International Conference on Artiﬁcial
Intelligence and Statistics, pages 168–176, 2016.

Jos´e Miguel Hern´andez-Lobato, Yingzhen Li, Mark Rowland, Daniel Hern´andez-Lobato,
In 33rd
Thang D Bui, and Richard E Turner. Black-box α-divergence minimization.
International Conference on International Conference on Machine Learning, pages 1511–
1520, 2016.

Trong Nghia Hoang, Quang Minh Hoang, and Bryan Kian Hsiang Low. A distributed
variational inference framework for unifying parallel sparse Gaussian process regression
models. In 33rd International Conference on Machine Learning, pages 382–391, 2016.

Neil Houlsby, Ferenc Husz´ar, Zoubin Ghahramani, and M´at´e Lengyel. Bayesian active
learning for classiﬁcation and preference learning. arXiv preprint arXiv:1112.5745, 2011.

Kazufumi Ito and Kaiqi Xiong. Gaussian ﬁlters for nonlinear ﬁltering problems.

IEEE

Transactions on Automatic Control, 45(5):910–927, 2000.

Diederik P. Kingma and Jimmy Ba. Adam: a method for stochastic optimization. In 3rd

International Conference on Learning Representations, 2015.

H. J. Kushner and A. S. Budhiraja. A nonlinear ﬁltering algorithm based on an approxi-
mation of the conditional distribution. IEEE Transactions on Automatic Control, 45(3):
580–585, Mar 2000.

Malte Kuss and Carl E. Rasmussen. Assessing approximate inference for binary Gaussian
process classiﬁcation. The Journal of Machine Learning Research, 6:1679–1704, 2005.

Neil D. Lawrence. Probabilistic non-linear principal component analysis with Gaussian
process latent variable models. The Journal of Machine Learning Research, 6:1783–1816,
2005.

Yingzhen Li, Jos´e Miguel Hern´andez-Lobato, and Richard E. Turner. Stochastic expectation
propagation. In Advances in Neural Information Processing Systems 29, pages 2323–2331,
2015.

Kian Hsiang Low, Jiangbo Yu, Jie Chen, and Patrick Jaillet. Parallel Gaussian process
regression for big data: Low-rank representation meets Markov approximation. In 29th
AAAI Conference on Artiﬁcial Intelligence, pages 2821–2827, 2015.

Maren Mahsereci and Philipp Hennig. Probabilistic line searches for stochastic optimization.

In Advances in Neural Information Processing Systems 28, pages 181–189, 2015.

70

Unifying Gaussian Process Approximations

Alexander G. D. G. Matthews, James Hensman, Richard E Turner, and Zoubin Ghahra-
mani. On sparse variational methods and the Kullback-Leibler divergence between
In 19th International Conference on Artiﬁcial Intelligence and
stochastic processes.
Statistics, pages 231–239, 2016.

Andrew McHutchon. Nonlinear modelling and control using Gaussian processes. PhD thesis,

University of Cambridge, 2014.

Thomas P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis,

Massachusetts Institute of Technology, 2001.

Thomas P. Minka. Power EP. Technical report, Microsoft Research Cambridge, 2004.

Thomas P. Minka. Divergence measures and message passing. Technical report, Microsoft

Research Cambridge, 2005.

Andrew Naish-Guzman and Sean B. Holden. The generalized FITC approximation.
Advances in Neural Information Processing Systems 20, pages 1057–1064, 2007.

In

Hannes Nickisch and Carl E. Rasmussen. Approximations for binary Gaussian process

classiﬁcation. The Journal of Machine Learning Research, 9(Oct):2035–2078, 2008.

Juho Piironen and Aki Vehtari. Comparison of Bayesian predictive methods for model

selection. Statistics and Computing, 27(3):711–735, 2017.

Yuan Qi, Ahmed H. Abdel-Gawad, and Thomas P. Minka. Sparse-posterior Gaussian pro-
cesses for general likelihoods. In 26th Conference on Uncertainty in Artiﬁcial Intelligence,
pages 450–457, 2010.

Joaquin Qui˜nonero-Candela and Carl E. Rasmussen. A unifying view of sparse approximate
Gaussian process regression. The Journal of Machine Learning Research, 6:1939–1959,
2005.

Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learn-

ing. The MIT Press, 2005.

Jaakko Riihim¨aki, Pasi Jyl¨anki, and Aki Vehtari. Nested expectation propagation for Gaus-
sian process classiﬁcation with a multinomial probit likelihood. The Journal of Machine
Learning Research, 14(Jan):75–109, 2013.

Alan D. Saul, James Hensman, Aki Vehtari, and Neil D. Lawrence. Chained Gaussian
processes. In 19th International Conference on Artiﬁcial Intelligence and Statistics, pages
1431–1440, 2016.

Anton Schwaighofer and Volker Tresp. Transductive and inductive methods for approximate
Gaussian process regression. In Advances in Neural Information Processing Systems 15,
pages 953–960, 2002.

Matthias Seeger. Bayesian inference and optimal design for the sparse linear model. The

Journal of Machine Learning Research, 9(Apr):759–813, 2008.

71

Bui, Yan and Turner

Matthias Seeger and Michael I. Jordan. Sparse Gaussian process classiﬁcation with multiple
classes. Technical report, Department of Statistics, University of Berkeley, CA, 2004.

Matthias Seeger, Christopher Williams, and Neil D. Lawrence. Fast forward selection to
speed up sparse Gaussian process regression. In 9th International Conference on Artiﬁcial
Intelligence and Statistics, 2003.

Edward Snelson. Flexible and eﬃcient Gaussian process models for machine learning. PhD

thesis, University College London, 2007.

Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs.

In Advances in Neural Information Processing Systems 19, pages 1257–1264, 2006.

Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian optimization of
machine learning algorithms. In Advances in Neural Information Processing Systems 25,
pages 2951–2959, 2012.

Michalis K. Titsias. Variational learning of inducing variables in sparse Gaussian processes.
In 12th International Conference on Artiﬁcial Intelligence and Statistics, pages 567–574,
2009.

Michalis K. Titsias and Neil D. Lawrence. Bayesian Gaussian process latent variable model.
In 13th International Conference on Artiﬁcial Intelligence and Statistics, pages 844–851,
2010.

Felipe Tobar, Thang D. Bui, and Richard E. Turner. Learning stationary time series us-
ing Gaussian processes with nonparametric kernels. In Advances in Neural Information
Processing Systems 29, pages 3501–3509, 2015.

Richard E. Turner and Maneesh Sahani. Two problems with variational expectation max-
imisation for time-series models.
In D. Barber, T. Cemgil, and S. Chiappa, editors,
Bayesian Time series models, chapter 5, pages 109–130. Cambridge University Press,
2011.

Jack M. Wang, David J. Fleet, and Aaron Hertzmann. Gaussian process dynamical models.

In Advances in Neural Information Processing Systems 18, pages 1441–1448, 2005.

Minjie Xu, Balaji Lakshminarayanan, Yee Whye Teh, Jun Zhu, and Bo Zhang. Distributed
In Advances in Neural Information

Bayesian posterior sampling via moment sharing.
Processing Systems 27, pages 3356–3364, 2014.

Huaiyu Zhu and Richard Rohwer. Information geometric measurements of generalisation.

Technical report, Aston University, 1995.

Huaiyu Zhu and Richard Rohwer. Measurements of generalisation based on information

geometry. In Mathematics of Neural Networks, pages 394–398. 1997.

72

7
1
0
2
 
t
c
O
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
6
6
0
7
0
.
5
0
6
1
:
v
i
X
r
a

Unifying Gaussian Process Approximations

A Unifying Framework for Gaussian Process Pseudo-Point
Approximations using Power Expectation Propagation

Thang D. Bui

Josiah Yan

Richard E. Turner
Computational and Biological Learning Lab, Department of Engineering
University of Cambridge, Trumpington Street, Cambridge, CB2 1PZ, UK

tdb40@cam.ac.uk

josiah.yan@gmail.com

ret26@cam.ac.uk

Abstract

Gaussian processes (GPs) are ﬂexible distributions over functions that enable high-
level assumptions about unknown functions to be encoded in a parsimonious, ﬂexible and
general way. Although elegant, the application of GPs is limited by computational and
analytical intractabilities that arise when data are suﬃciently numerous or when employ-
ing non-Gaussian models. Consequently, a wealth of GP approximation schemes have been
developed over the last 15 years to address these key limitations. Many of these schemes
employ a small set of pseudo data points to summarise the actual data. In this paper we
develop a new pseudo-point approximation framework using Power Expectation Propaga-
tion (Power EP) that uniﬁes a large number of these pseudo-point approximations. Unlike
much of the previous venerable work in this area, the new framework is built on standard
methods for approximate inference (variational free-energy, EP and Power EP methods)
rather than employing approximations to the probabilistic generative model itself. In this
way all of approximation is performed at ‘inference time’ rather than at ‘modelling time’
resolving awkward philosophical and empirical questions that trouble previous approaches.
Crucially, we demonstrate that the new framework includes new pseudo-point approxima-
tion methods that outperform current approaches on regression and classiﬁcation tasks.

1. Introduction

Gaussian Processes (GPs) are powerful nonparametric distributions over continuous func-
tions that are routinely deployed in probabilistic modelling for applications including regres-
sion and classiﬁcation (Rasmussen and Williams, 2005), representation learning (Lawrence,
2005), state space modelling (Wang et al., 2005), active learning (Houlsby et al., 2011),
reinforcement learning (Deisenroth, 2010), black-box optimisation (Snoek et al., 2012), and
numerical methods (Mahsereci and Hennig, 2015). GPs have many elegant theoretical
properties, but their use in probabilistic modelling is greatly hindered by analytic and
computational intractabilities. A large research eﬀort has been directed at this fundamen-
tal problem resulting in the development of a plethora of sparse approximation methods
that can sidestep these intractabilities (Csat´o, 2002; Csat´o and Opper, 2002; Schwaighofer
and Tresp, 2002; Seeger et al., 2003; Qui˜nonero-Candela and Rasmussen, 2005; Snelson
and Ghahramani, 2006; Snelson, 2007; Naish-Guzman and Holden, 2007; Titsias, 2009;
Figueiras-Vidal and L´azaro-Gredilla, 2009; ´Alvarez et al., 2010; Qi et al., 2010; Bui and

1

Bui, Yan and Turner

Turner, 2014; Frigola et al., 2014; McHutchon, 2014; Hensman et al., 2015; Hern´andez-
Lobato and Hern´andez-Lobato, 2016; Matthews et al., 2016)

This paper develops a general sparse approximate inference framework based upon Power
Expectation Propagation (PEP) (Minka, 2004) that uniﬁes many of these approximations,
extends them signiﬁcantly, and provides improvements in practical settings. In this way, the
paper provides a complementary perspective to the seminal review of Qui˜nonero-Candela
and Rasmussen (2005) viewing sparse approximations through the lens of approximate
inference, rather than approximate generative models.

The paper begins by reviewing several frameworks for sparse approximation focussing
on the GP regression and classiﬁcation setting (section 2). It then lays out the new unifying
framework and the relationship to existing techniques (section 3). Readers whose focus is
to understand the new framework might want to move directly to this section. Finally, a
thorough experimental evaluation is presented in section 4.

2. Pseudo-point Approximations for GP Regression and Classiﬁcation

This section provides a concise introduction to GP regression and classiﬁcation and then
reviews several pseudo-point based sparse approximation schemes for these models. For
simplicity, we ﬁrst consider a supervised learning setting in which the training set com-
prises N D-dimensional input and scalar output pairs {xn, yn}N
n=1 and the goal is to
produce probabilistic predictions for the outputs corresponding to novel inputs. A non-
linear function, f (x), can be used to parameterise the probabilistic mapping between in-
puts and outputs, p(yn|f, xn, θ). Typical choices for the probabilistic mapping are Gaus-
y) for the regression setting (yn ∈ R) and Bernoulli
sian p(yn|f, xn, θ) = N (yn; f (xn), σ2
p(yn|f, xn, θ) = B(yn; Φ(f (xn))) with a sigmoidal link function Φ(f ) for the binary classi-
ﬁcation setting (yn ∈ {0, 1}). Whilst it is possible to specify the non-linear function f via
an explicit parametric form, a more ﬂexible and elegant approach employs a GP prior over
the functions directly, p(f |θ) = GP(f ; 0, kθ(·, ·)), here assumed without loss of generality to
have a zero mean-function and a covariance function kθ(x, x(cid:48)). This class of probabilistic
models has a joint distribution

p(f, y|θ) = p(f |θ)

p(yn|f (xn), θ)

(1)

where we have collected the observations into the vector y and suppressed the inputs on
the left hand side to lighten the notation.

This model class contains two potential sources of intractability. First, the possibly non-
linear likelihood function can introduce analytic intractabilities that require approximation.
Second, the GP prior entails an O(N 3) complexity that is computationally intractable for
many practical problems. These two types of intractability can be handled by combining
standard approximate inference methods with pseudo-point approximations that summarise
the full Gaussian process via M pseudo data points leading to an O(N M 2) cost. The main
approaches of this sort can be characterised in terms of two parallel frameworks that are
described in the following sections.

N
(cid:89)

n=1

2

Unifying Gaussian Process Approximations

2.1 Sparse GP Approximation via Approximate Generative Models

The ﬁrst framework begins by constructing a new generative model that is similar to the
original, so that inference in the new model might be expected to produce similar results,
but which has a special structure that supports eﬃcient computation. Typically this ap-
proach involves approximating the Gaussian process prior as it is the origin of the cubic
cost. If there are analytic intractabilities in the approximate model, as will be the case in
e.g. classiﬁcation or state-space models, then these will require approximate inference to be
performed in the approximate model.

The seminal review by Qui˜nonero-Candela and Rasmussen (Qui˜nonero-Candela and
Rasmussen, 2005) reinterprets a family of approximations in terms of this unifying frame-
work. The GP prior is approximated by identifying a small set of M ≤ N pseudo-points
u, here assumed to be disjoint from the training function values f so that f = {u, f , f
=u,f }.
The GP prior is then decomposed using the product rule

p(f |θ) = p(u|θ)p(f |u, θ)p(f

=u,f |f , u, θ).

(2)

1
uuu, Dﬀ ) where Dﬀ = Kﬀ − Qﬀ and Qﬀ = KfuK−

Of central interest is the relationship between the pseudo-points and the training function
1
uuKuf .
values p(f |u, θ) = N (f ; KfuK−
Here we have introduced matrices corresponding to the covariance function’s evaluation
at the pseudo-input locations {zm}M
m=1, so that [Kuu]mm(cid:48) = kθ(zm, zm(cid:48)) and similarly
for the covariance between the pseudo-input and data locations [Kuf ]mn = kθ(zm, xn).
Importantly, this term saddles learning with a cubic complexity cost. Computationally
eﬃcient approximations can be constructed by simplifying these dependencies between the
pseudo-points and the data function values q(f |u, θ) ≈ p(f |u, θ). In order to beneﬁt from
these eﬃciencies at prediction time as well, a second approximation is made whereby the
pseudo-points form a bottleneck between the data function values and test function values
=u,f |f , u, θ). Together, the two approximations result in an approximate
p(f
prior process,

=u,f |u, θ) ≈ p(f

q(f |θ) = p(u|θ)q(f |u, θ)p(f

=u,f |u, θ).

We can now compactly summarise a number of previous approaches to GP approximation
as special cases of the choice

q(f |u, θ) =

1
N (fb; Kfb,uK−

uuu, αDfb,fb)

B
(cid:89)

b=1

where b indexes B disjoint blocks of data-function values. The Deterministic Training
Conditional (DTC) approximation uses α → 0; the Fully Independent Training Condi-
tional (FITC) approximation uses α = 1 and B = N ; the Partially Independent Training
Conditional (PITC) approximation uses α = 1 (Qui˜nonero-Candela and Rasmussen, 2005;
Schwaighofer and Tresp, 2002).

In a moment we will consider inference in the modiﬁed models, before doing so we note
that it is possible to construct more ﬂexible modiﬁed prior processes using the inter-domain
approach that places the pseudo-points in a diﬀerent domain from the data, deﬁned by
a linear integral transform g(z) = (cid:82) w(z, z(cid:48))f (z(cid:48))dz(cid:48). Here the window w(z, z(cid:48)) might be

3

(3)

(4)

Bui, Yan and Turner

a Gaussian blur or a wavelet transform. The pseudo-points are now placed in the new
domain g = {u, g
=u} where they induce a potentially more ﬂexible Gaussian process in the
old domain f through the linear transform (see Figueiras-Vidal and L´azaro-Gredilla, 2009,
for FITC). The expressions in this section still hold, but the covariance matrices involving
pseudo-points are modiﬁed to take account of the transform,

(cid:90)

(cid:90)

[Kuu]mm(cid:48) =

w(zm, z)kθ(z, z(cid:48))w(z(cid:48), zm(cid:48))dzdz(cid:48),

[Kuf ]mn =

w(zm, z)kθ(z, xn)dz.

(5)

Having speciﬁed modiﬁed prior processes, these can be combined with the original like-
lihood function to produce a new generative model. In the case of point-wise likelihoods we
have

q(y, f |θ) = q(f |θ)

p(yn|f (xn), θ).

(6)

N
(cid:89)

n=1

Inference and learning can now be performed using the modiﬁed model using standard
techniques. Due to the form of the new prior process, the computational complexity is
O(N M 2) (for testing, N becomes the number of test data points, assuming dependencies
between the test-points are not computed).1 For example, in the case of regression, the
posterior distribution over function values f (necessary for inference and prediction) has a
simple analytic form

|
b=1) + σ2

q(f |y, θ) = GP(f ; µf

y, Σf

y), µf

y = Qf f K−

ﬀ y, Σf

1

1
ﬀ Qf f
y = Kf f − Qf f K−

(7)

|

|

|

where Kﬀ = Qﬀ + blkdiag({αbDfbfb}B
yI and blkdiag builds a block-diagonal matrix
from its inputs. One way of understanding the origin of the computational gains is that
the new generative model corresponds to a form of factor analysis in which the M pseudo-
points determine the N function values at the observed data (as well as at potential test
locations) via a linear Gaussian relationship. This results in low rank (sparse) structure in
Kﬀ that can be exploited through the matrix inversion and determinant lemmas. In the
case of regression, the new model’s marginal likelihood also has an analytic form that allows
the hyper-parameters, θ, to be learned via optimisation

log q(y|θ) = −

log(2π) −

log |Kﬀ | −

N
2

1
2

1
y(cid:124)K−
ﬀ y.

1
2

(8)

The approximate generative model framework has attractive properties. The cost of
inference, learning, and prediction has been reduced from O(N 3) to O(N M 2) and in many
cases accuracy can be maintained with a relatively small number of pseudo-points. The
pseudo-point input locations can be optimised by maximising the new model’s marginal
likelihood (Snelson and Ghahramani, 2006). When M = N and the pseudo-points and
observed data inputs coincide, then FITC and PITC are exact which appears reassuring.
However, the framework is philosophically challenging as the elegant separation of model
and (approximate) inference has been lost. Are we allowed in an online inference setting,

1. It is assumed that the maximum size of the blocks is not greater than the number of pseudo-points

dim(fb) ≤ M .

4

Unifying Gaussian Process Approximations

for example, to add new pseudo-points as more data are acquired and the complexity of the
underlying function is revealed? This seems sensible, but eﬀectively changes the modelling
assumptions as more data are seen. Devout Bayesians might then demand that we perform
model averaging for coherence. Similarly, if the pseudo-input locations are optimised, the
principled non-parametric model has suddenly acquired M D parameters and with them
all of the concomitant issues of parametric models including overﬁtting and optimisation
diﬃculties (Bauer et al., 2016). As the pseudo-inputs are considered part of the model, the
Bayesians might then suggest that we place priors over the pseudo-inputs and perform full
blown probabilistic inference over them.

These awkward questions arise because the generative modelling interpretation of pseudo-
data entangles the assumptions made about the data with the approximations required
to perform inference. Instead, the modelling assumptions (which encapsulate prior under-
standing of the data) should remain decoupled from inferential assumptions (which leverage
structure in the posterior for tractability). In this way pseudo-data should be introduced
when we seek to perform computationally eﬃcient approximate inference, leaving the mod-
elling assumptions unchanged as we reﬁne and improve approximate inference. Indeed, even
under the generative modelling perspective, for analytically intractable likelihood functions
an additional approximate inference step is required, begging the question; why not handle
computational and analytic intractabilities together at inference time?

2.2 Sparse GP Approximation via Approximate Inference: VFE

The approximate generative model framework for constructing sparse approximations is
philosophically troubling. In addition, learning pseudo-point input locations via optimisa-
tion of the model likelihood can perform poorly e.g. for DTC it is prone to overﬁtting even
for M (cid:28) N (Titsias, 2009). This motivates a more direct approach that commits to the
true generative model and performs all of the necessary approximation at inference time.

Perhaps the most well known approach in this vein is Titsias’s beautiful sparse varia-
tional free energy (VFE) method (Titsias, 2009). The original presentation of this work
employs ﬁnite variable sets and an augmentation trick that arguably obscures its full ele-
gance. Here instead we follow the presentation in Matthews et al. (2016) and lower bound
the marginal likelihood using a distribution q(f ) over the entire inﬁnite dimensional func-
tion,

log p(y|θ) = log

p(y, f |θ)df ≥

q(f ) log

(cid:90)

(cid:90)

p(y, f |θ)
q(f )

(cid:20)

df = E

q(f )

log

(cid:21)

p(y, f |θ)
q(f )

= F(q, θ).

The VFE bound can be written as the diﬀerence between the model log-marginal likelihood
and the KL divergence between the variational distribution and the true posterior F(q, θ) =
log p(y|θ)−KL(q(f )||p(f |y, θ)). The bound is therefore saturated when q(f ) = p(f |y, θ), but
=u}, and an approxi-
this is intractable. Instead, pseudo-points are made explicit, f = {u, f
mate posterior distribution used of the following form q(f ) = q(u, f
=u|u, θ)q(u).
Under this approximation, the set of variables f
=u do not experience the data directly, but
rather only through the pseudo-points, as can be seen by comparison to the true poste-
rior p(f |y, θ) = p(f
=u|y, u, θ)p(u|y, θ). Importantly, the form of the approximate posterior
causes a cancellation of the prior conditional term, which gives rise to a bound with O(N M 2)

=u|θ) = p(f

5

Bui, Yan and Turner

complexity,

F(q, θ) = E

q(f

θ)

|

E

q(f

|

(cid:88)

=

n

(cid:20)

log

=u|u, θ)p(u|θ)

p(y|f, θ)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
p(f
(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
=u|u, θ)q(u)
p(f
θ) [log p(yn|fn, θ)] − KL(q(u)||p(u|θ)).

(cid:21)

For regression with Gaussian observation noise, the calculus of variations can be used to
ﬁnd the optimal approximate posterior Gaussian process over pseudo-data qopt(f |θ) =
p(f

=u|u, θ)qopt(u) which has the form

qopt(f |θ) = GP(f ; µf

y, Σf

y), µf

y = Qf f ˜K−

1
ﬀ y, Σf

y = Kf f − Qf f ˜K−
1
ﬀ Qf f

(9)

|

|

|

|

where ˜Kﬀ = Qﬀ + σ2
yI. This process is identical to that recovered when performing ex-
act inference under the DTC approximate regression generative model (Titsias, 2009) (see
equation 7 as α → 0). In fact DTC was originally derived using a related KL argument
(Csat´o, 2002; Seeger et al., 2003). The optimised free-energy is

F(qopt, θ) = −

log(2π) −

log | ˜Kﬀ | −

N
2

1
2

1
2

y(cid:124) ˜K−

1
ﬀ y −

1
2σ2
y

trace(Kﬀ − Qﬀ ).

(10)

Notice that the free-energy has an additional trace term as compared to the marginal
likelihood obtained from the DTC generative model approach (see equation 8 as α → 0).
The trace term is proportional to the sum of the variances of the training function values
given the pseudo-points, p(f |u), it thereby encourages pseudo-input locations that explain
the observed data well. This term acts as a regulariser that prevents overﬁtting which
plagues the generative model formulation of DTC.

The VFE approach can be extended to non-linear models including classiﬁcation (Hens-
man et al., 2015), latent variable models (Titsias and Lawrence, 2010) and state space
models (Frigola et al., 2014; McHutchon, 2014) by restricting q(u) to be Gaussian and
optimising its parameters. Indeed, this uncollapsed form of the bound can be beneﬁcial
in the context of regression too as it is amenable to stochastic optimisation (Hensman
et al., 2013). Additional approximation is sometimes required to compute any remaining
intractable non-linear integrals, but these are often low-dimensional. For example, when the
likelihood depends on only one latent function value, as is typically the case for regression
and classiﬁcation, the bound requires only 1D integrals E
q(fn) [log p(yn|fn, θ)] that can be
evaluated using quadrature (Hensman et al., 2015), for example.

The VFE approach can also be extended to employ inter-domain variables ( ´Alvarez et al.,
2010; Tobar et al., 2015; Matthews et al., 2016). The approach considers the augmented
generative model p(f, g|θ) where to remind the reader the auxiliary process is deﬁned by a
linear integral transformation, g(z) = (cid:82) w(z, z(cid:48))f (z(cid:48))dz(cid:48). Variational inference is now per-
formed over both latent processes q(f, g) = q(f, u, g
=u|u, θ)q(u). Here the
pseudo-data have been placed into the auxiliary process with the idea being that they can
induce richer dependencies in the original domain that model the true posterior more accu-
rately. In fact, if the linear integral transformation is parameterised then the transformation
can be learned so that it approximates the posterior more accurately.

=u|θ) = p(f, g

6

Unifying Gaussian Process Approximations

A key concept underpinning the VFE framework is that the pseudo-input locations (and
the parameters of the inter-domain transformation, if employed) are purely parameters of
the approximate posterior, hence the name ‘variational parameters’. This distinction is im-
portant as it means, for example, that we are free to add pseudo-data as more structure is
revealed the underlying function without altering the modelling assumptions (e.g. see Bui
et al. (2017) for an example in online inference). Moreover, since the pseudo-input locations
are variational parameters, placing priors over them is unnecessary in this framework. Un-
like the model parameters, optimisation of variational parameters is automatically protected
from overﬁtting as the optimisation is minimising the KL divergence between the approx-
imate posterior and the true posterior. Indeed, although the DTC posterior is recovered
in the regression setting, as we have seen the free-energy is not equal to the log-marginal
likelihood of the DTC generative model, containing an additional term that substantially
improves the quality of the optimised pseudo-point input locations.

The fact that the form of the DTC approximation can be recovered from a direct ap-
proximate inference approach and that this new perspective leads to superior pseudo-input
optimisation, raises the question; can this also be done for FITC and PITC?

2.3 Sparse GP Approximation via Approximate Inference: EP

Expectation Propagation (EP) is a deterministic inference method (Minka, 2001) that is
known to outperform VFE methods in GP classiﬁcation when unsparsiﬁed fully-factored ap-
proximations q(f ) = (cid:81)
n qn(fn) are used (Nickisch and Rasmussen, 2008). Motivated by this
observation, EP has been combined with the approximate generative modelling approach
to handle non-linear likelihoods (Naish-Guzman and Holden, 2007; Hern´andez-Lobato and
Hern´andez-Lobato, 2016). This begs the question: can the sparsiﬁcation and the non-linear
approximation be handled in a single EP inference stage, as for VFE? Astonishingly Csat´o
and Opper not only developed such a method in 2002 (Csat´o and Opper, 2002), predat-
ing much of the work mentioned above, they showed that it is equivalent to applying the
FITC approximation and running EP if further approximation is required. In our view,
this is a central result, but it appears to have been largely overlooked by the ﬁeld. Snelson
was made aware of it when writing his thesis (Snelson, 2007), brieﬂy acknowledging Csat´o
and Opper’s contribution. Qi et al. (2010) extended Csat´o and Opper’s work to utilise
inter-domain pseudo-points and they additionally recognised that the EP energy function
at convergence is equal to the FITC log-marginal likelihood approximation. Interestingly,
no additional term arises as it does when the VFE approach generalised the DTC generative
model approach. We are unaware of other work in this vein.

It is hard to be known for certain why these important results are not widely known,
but a contributing factor is that the exposition in these papers is largely at Marr’s algo-
rithmic level (Dawson, 1998), and does not focus on the computational level making them
challenging to understand. Moreover, Csat´o and Opper’s paper was written before EP was
formulated in a general way and the presentation, therefore, does not follow what has be-
come the standard approach. In fact, as the focus was online inference, Assumed Density
Filtering (Kushner and Budhiraja, 2000; Ito and Xiong, 2000) was employed rather than
full-blown EP. One of the main contributions of this paper is to provide a clear compu-
tational exposition including an explicit form of the approximating distribution and full

7

Bui, Yan and Turner

details about each step of the EP procedure. In addition, to bringing clarity we make the
following novel contributions:

• We show that a generalisation of EP called Power EP can subsume the EP and
VFE approaches (and therefore FITC and DTC) into a single uniﬁed framework.
More precisely, the ﬁxed points of Power EP yield the FITC and VFE posterior
distribution under diﬀerent limits and the Power EP marginal likelihood estimate
(the negative ‘Power EP energy’) recovers the FITC marginal likelihood and the VFE
too. Critically the connection to the VFE method leans on the new interpretation of
Titsias’s approach (Matthews et al., 2016) outlined in the previous section that directly
employs the approximate posterior over function values (rather than augmenting the
model with pseudo-points). The connection therefore also requires a formulation of
Power EP that involves KL divergence minimisation between stochastic processes.

• We show how versions of PEP that are intermediate between the existing VFE and EP
approaches can be derived, as well as mixed approaches that treat some data variation-
ally and others using EP. We also show how PITC emerges from the same framework
and how to incorporate inter-domain transforms. For regression with Gaussian obser-
vation noise, we obtain analytical expressions for the ﬁxed points of Power EP in a
general case that includes all of these extensions as well as the form of the Power EP
marginal likelihood estimate at convergence that is useful for hyper-parameter and
pseudo-input optimisation.

• We consider (Gaussian) regression and probit classiﬁcation as canonical models on
which to test the new framework and demonstrate through exhaustive testing that
versions of PEP intermediate between VFE and EP perform substantially better on
average. The experiments also shed light on situations where VFE is to be preferred
to EP and vice versa which is an important open area of research.

Many of the new theoretical contributions described above are summarised in ﬁg. 1

along with their relationship to previous work.

3. A New Unifying View using Power Expectation Propagation

In this section, we provide a new unifying view of sparse approximation using Power Ex-
pectation Propagation (PEP or Power EP) (Minka, 2004). We review Power EP, describe
how to apply it for sparse GP regression and classiﬁcation, and then discuss its relationship
to existing methods.

3.1 The Joint-Distribution View of Approximate Inference and Learning

One way of understanding the goal of distributional inference approximations, including the
VFE method, EP and Power EP, is that they return an approximation of a tractable form
to the model joint-distribution evaluated on the observed data. In the case of GP regression
and classiﬁcation, this means q∗(f |θ) ≈ p(f, y|θ) where ∗ is used to denote an unnor-
malised process. Why is the model joint-distribution a sensible object of approximation?
The joint distribution can be decomposed into the product of the posterior distribution

8

Unifying Gaussian Process Approximations

Figure 1: A uniﬁed view of pseudo-point GP approximations applied to A) regression,
and B) classiﬁcation. Every point in the algorithm polygons corresponds to a form of GP
approximation. Previous algorithms correspond to labelled vertices. The new Power EP
framework encompasses the three polygons, including their interior.

and the marginal likelihood, p(f, y|θ) = p∗(f |y, θ) = p(f |y, θ)p(y|θ), the two inferential
objects of interest. A tractable approximation to the joint can therefore be similarly de-
composed q∗(f |θ) = Zq(f |θ) into a normalised component that approximates the posterior
q(f |θ) ≈ p(f |y, θ) and the normalisation constant which approximates the marginal likeli-
hood Z ≈ p(y|θ). In other words, the approximation of the joint simultaneously returns
approximations to the posterior and marginal likelihood. In the current context tractability
of the approximating family means that it is analytically integrable and that this integra-
tion can be performed with an appropriate computational complexity. We consider the
approximating family comprising unnormalised GPs, q∗(f |θ) = ZGP(f ; mf , Vﬀ (cid:48)).

The VFE approach can be reformulated in the new context using the un-normalised KL
divergence (Zhu and Rohwer, 1997) to measure the similarity between the approximation
and the joint distribution

KL(q∗(f |θ)||p(f, y|θ)) =

q∗(f ) log

df +

(p(f, y|θ) − q∗(f )) df.

(11)

(cid:90)

q∗(f )
p(f, y|θ)

(cid:90)

The un-normalised KL divergence generalises the KL divergence to accommodate un-normalised
It is always non-negative and collapses back to the standard form when its
densities.
arguments are normalised. Minimising the un-normalised KL with respect to q∗(f |θ) =
ZVFEq(f ) encourages the approximation to match both the posterior and marginal-likelihood,
and it yields analytic solutions

qopt(f ) = argmin

KL(q(f )||p(f |y, θ)), and Zopt

VFE = exp(F(qopt(f ), θ)).

(12)

q(f )

∈Q

9

Bui, Yan and Turner

That is, the standard variational free-energy approximation to the posterior and marginal
likelihood is recovered. One of the pedagogical advantages of framing VFE in this way is that
approximation of the posterior and marginal likelihood are committed to upfront, in con-
trast to the traditional derivation which begins by targeting approximation of the marginal
likelihood, but shows that approximation of the posterior emerges as an essential part of
this scheme (see section 2.2). A disadvantage is that optimisation of hyper-parameters must
logically proceed by optimising the marginal likelihood approximation, Zopt
VFE, and at ﬁrst
sight therefore appears to necessitate diﬀerent objective functions for q∗(f |θ) and θ (unlike
the standard view which uses a single objective from the beginning). However, it is easy
to show that maximising the single objective p(y|θ) − KL(q∗(f |θ)||p(f, y|θ)) directly for
both q∗(f |θ) and θ is equivalent and that this also recovers the standard VFE method (see
appendix A).

3.2 The Approximating Distribution Employed by Power EP

Power EP also approximates the joint-distribution employing an approximating family
whose form mirrors that of the target,

p∗(f |y, θ) = p(f |y, θ)p(y|θ) = p(f |θ)

p(yn|f, θ) ≈ p(f |θ)

tn(u) = q∗(f |θ).

(13)

(cid:89)

n

(cid:89)

n

Here, the approximation retains the exact prior, but each likelihood term in the exact
posterior, p(yn|fn, θ), is approximated by a simple factor tn(u) that is assumed Gaussian.
These simple factors will be iteratively reﬁned by the PEP algorithm such that they will
capture the eﬀect that each true likelihood has on the posterior.

Before describing the details of the PEP algorithm, it is illuminating to consider an
alternative interpretation of the approximation. Together, the approximate likelihood
functions specify an un-normalised Gaussian over the pseudo-points that can be written
(cid:81)
n tn(u) = N (˜y; ˜Wu, ˜Σ) (assuming that the product of these factors is normalisable which

may not be the case for heavy tailed likelihoods, for example).

The approximate posterior above can therefore be thought of as the (exact) GP pos-
terior resulting from a surrogate regression problem with surrogate observations ˜y that
are generated from linear combinations of the pseudo-points and additive surrogate noise
˜y = ˜Wu + ˜Σ1/2(cid:15). We note that the pseudo-points u live on the latent function (or an
inter-domain transformation thereof) and the surrogate observations ˜y will not generally
lie on the latent function. The surrorate observations and the pseudo-points are therefore
analogous to the data y and the function values f in a normal Gaussian Process regression
problem, respectively. To make the paper more speciﬁc on this point, we have deﬁned pa-
rameters forthe surrogate regression problem explicitly in appendix H. The PEP algorithm
will implicitly iteratively reﬁne {˜y, ˜W, ˜Σ} such that exact inference in the simple surrogate
regression model returns a posterior and marginal likelihood estimate that is ‘close’ to that
returned by performing exact inference in the intractable complex model (see ﬁg. 2).

3.3 The EP Algorithm

One method for updating the approximate likelihood factors tn(u) is to minimise the unnor-
malised KL Divergence between the joint distribution and each of the distributions formed

10

Unifying Gaussian Process Approximations

Figure 2: Perspectives on the approximating family. The true joint distribution over the
unknown function f and the N data points y (top left) comprises the GP prior and an
intractable likelihood function. This is approximated by a surrogate regression model with
a joint distribution over the function f and M surrogate data points ˜y (top right). The
surrogate regression model employs the same GP prior, but uses a Gaussian likelihood
function p(˜y|u, ˜W, ˜Σ) = N (˜y; ˜Wu, ˜Σ). The intractable true posterior (bottom left) is
approximated by reﬁning the surrogate data ˜y their input locations z and the parameters
of the surrogate model ˜W and ˜Σ.

by replacing one of the likelihoods by the corresponding approximating factor (Li et al.,
2015),

(cid:20)

KL

p(f, y|θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

p(f, y|θ)tn(u)
p(yn|fn, θ)

(cid:21)

argmax
tn(u)

= argmax

tn(u)

n(f )tn(u)]. (14)
n(f )p(yn|fn, θ)||p∗
KL[p∗
\
\

n(f ) = p(f, y|θ)/p(yn|fn, θ) which makes
Here we have introduced the leave-one-out joint p∗
\
clear that the minimisation will cause the approximate factors to approximate the like-
lihoods in the context of the leave-one-out joint. Unfortunately, such an update is still
intractable. Instead, EP approximates this idealised procedure by replacing the exact leave-
one-out joint on both sides of the KL by the approximate leave-one-out joint (called the
n(f ) = q∗(f )/tn(u). Not only does this improve tractability, but it also
n(f ) ≈ q∗
cavity) p∗
\
\
means that the new procedure eﬀectively reﬁnes the approximating distribution directly at
each stage, rather than setting the component parts in isolation,

KL([q∗
\

n(f )p(yn|fn, θ)||q∗(f )].
n(f )tn(u)] = KL([q∗
n(f )p(yn|fn, θ)||q∗
\
\

(15)

However, the updates for the approximating factors are now coupled and so the updates
must now be iterated, unlike in the idealised procedure. In this way, EP iteratively reﬁnes

11

Bui, Yan and Turner

the approximate factors or surrogate likelihoods so that the GP posterior of the surro-
gate regression task best approximates the posterior of the original regression/classiﬁcation
problem.

3.4 The Power EP Algorithm

Power EP is, algorithmically, a mild generalisation of the EP algorithm that instead removes
(or includes) a fraction α of the approximate (or true) likelihood functions in the following
steps:

1. Deletion: compute the cavity distribution by removing a fraction of one approximate

n(f |θ) ∝ q∗(f |θ)/tα
factor, q∗
\

n(u).

2. Projection: ﬁrst, compute the tilted distribution by incorporating a corresponding
n(f |θ)pα(yn|fn). Second, project
fraction of the true likelihood into the cavity, ˜p(f ) = q∗
\
the tilted distribution onto the approximate posterior using the KL divergence for un-
normalised densities,

q∗(f |θ) ← argmin

KL(˜p(f )||q∗(f |θ)).

(16)

q∗(f

θ)

|

∈Q

Here Q is the set of allowed q∗(f |θ) deﬁned by eq. (13).

3. Update: compute a new fraction of the approximate factor by dividing the new approx-
imate posterior by the cavity, tα
n(f |θ), and incorporate this fraction
n,new(u) = q∗(f |θ)/q∗
\
back in to obtain the updated factor, tn(u) = t1
α
n,old(u)tα
−

n,new(u).

The above steps are iteratively repeated for each factor that needs to be approximated.
Notice that the procedure only involves one likelihood factor to be handled at a time. In
the case of analytically intractable likelihood functions, this often requires only low dimen-
sional integrals to be computed. In other words, PEP has transformed a high dimensional
intractable integral that is hard to approximate into a set of low dimensional intractable
integrals that are simpler to approximate. The procedure is not, in general guaranteed to
converge but we did not observe any convergence issues in our experiments. Furthermore, it
can be shown to be numerically stable when the factors are log-concave (as in GP regression
and classiﬁcation without pseudo-data) (Seeger, 2008).

If Power EP converges, the fractional updates are equivalent to running the original EP
procedure, but replacing the KL minimisation with an alpha-divergence minimisation (Zhu
and Rohwer, 1995; Minka, 2005),

Dα[p∗(f )||q∗(f )] =

(cid:2)αp∗(f ) + (1 − α)q∗(f ) − p∗(f )αq∗(f )1

−

α(cid:3) df.

(17)

(cid:90)

1
α(1 − α)

When α = 1, the alpha-divergence is the inclusive KL divergence D1[p∗(f )||q∗(f )] =
KL[p∗(f )||q∗(f )] recovering EP as expected from the PEP algorithm. As α → 0 the exclu-
0[p∗(f )||q∗(f )] = KL[q∗(f )||p∗(f )], and since minimising
sive KL divergence is recovered, D
a set of local exclusive KL divergences is equivalent to minimising a single global exclusive
KL divergence (Minka, 2005), the Power EP solution is the minimum of a variational free-
energy (see appendix B for more details). In the current case, we will now show explicitly
that these cases of Power EP recover FITC and Titsias’s VFE solution respectively.

→

12

Unifying Gaussian Process Approximations

3.5 General Results for Gaussian Process Power EP

This section describes the Power EP steps in ﬁner detail showing the complexity is O(N M 2)
and laying the ground work for the equivalence relationships. The appendix F includes a
full derivation.

We start by deﬁning the approximate factors to be in natural parameter form, mak-
ing it simple to combine and delete them, tn(u) = ˜N (u; zn, T1,n, T2,n) = zn exp(u(cid:124)T1,n −
2 u(cid:124)T2,nu). We consider full rank T2,n, but will show that the optimal form is rank 1.
1
The parameterisation means the approximate posterior over the pseudo-points has natural
parameters T1,u = (cid:80)
uu + (cid:80)
n T2,n inducing an approximate posterior,
q∗(f |θ) = ZPEPGP(f ; mf , Vﬀ (cid:48)). Here and in what follows, the dependence on the hyperpa-
rameters θ will be suppressed to lighten the notation. The mean and covariance functions
of the approximate posterior are

1
n T1,n and T2,u = K−

mf = KfuK−

1
uuT−

1
2,uT1,u;

Vﬀ (cid:48) = Kﬀ (cid:48) − Qﬀ (cid:48) + KfuK−

1
1
1
2,uK−
uuT−

uuKuf (cid:48).

(18)

n(f ) ∝ q∗(f )/tα
n(u), has a similar form to the
Deletion: The cavity for data point n, q∗
\
n
1,u = T1,u −αT1,n
posterior, but the natural parameters are modiﬁed by the deletion step, T\

and T\

n
2,u = T2,u − αT2,n, yielding the following mean and covariance functions

n
1
uuT\
f = KfuK−
m\

n
n,
1
2,u T\
1,u;
−

n
ﬀ (cid:48) = Kﬀ (cid:48) − Qﬀ (cid:48) + KfuK−
V \

1
uuT\

n,
1
2,u K−
−

1

uuKuf (cid:48).

(19)

Projection: The central step in Power EP is the projection. Obtaining the new approx-
imate un-normalised posterior q∗(f ) by minimising KL(˜p(f )||q∗(f )) would na¨ıvely appear
intractable. Fortunately,

=u|u)q∗(u),
Remark 1 Due to the structure of the approximate posterior, q∗(f ) = p(f
the objective, KL(˜p(f )||q∗(f )) is minimised when E
q∗(u)[φ(u)], where φ(u) =
{u, uu(cid:124)} are the suﬃcient statistics, that is when the moments at the pseudo-inputs are
matched.

˜p(f )[φ(u)] = E

This is the central result from which computational savings are derived. Furthermore, this
moment matching condition would appear to necessitate computation of a set of integrals
to ﬁnd the zeroth, ﬁrst and second moments. However, the technique known as ‘diﬀerenti-
ation under the integral sign’2 provides a useful shortcut that only requires one integral to
compute the log-normaliser of the tilted distribution, log ˜Zn = log E
\n(f )[pα(yn|fn)], before
q∗
diﬀerentiating w.r.t. the cavity mean to give

mu = m\

n
n
u + V\
ufn

d log ˜Zn
n
dm\
fn

;

Vu = V\

n
n
u + V\
ufn

d2 log ˜Zn
n
fn )2
d(m\

n
V\
fnu.

(20)

Update: Having computed the new approximate posterior, the approximate factor tn,new(u) =
q∗(f )/q∗
n(f ) can be straightforwardly obtained, resulting in,
\

1
T1,n,new = V−

n
u mu − (V\
u )−

1m\

n
u , T2,n,new = V−

n
u − (V\
u )−

1

1, zα

n = ˜ZneG

\n
∗ (u))

(q

(q∗(u)),

−G

2. In this case, the dominated convergence theorem can be used to justify the interchange of integration

and diﬀerentiation (see e.g. Brown, 1986).

13

Bui, Yan and Turner

where we have deﬁned the log-normaliser as the functional G( ˜N (u; z, T1, T2)) =
log (cid:82) ˜N (u; z, T1, T2)du. Remarkably, these results and eq. (20) reveals that T2,n,new is
a rank-1 matrix. As such, the minimal and simplest way to parameterise the approxi-
mate factor is tn(u) = znN (KfnuK−
uuu; gn, vn), where gn and vn are scalars, resulting in a
signiﬁcant memory saving and O(N M 2) cost.

1

In addition to providing the approximate posterior after convergence, Power EP also
provides an approximate log-marginal likelihood for model selection and hyper-parameter
optimisation,

log ZPEP = log

p(f )

tn(u)df = G(q∗(u)) − G(p∗(u)) +

log zn.

(21)

(cid:90)

(cid:89)

n

(cid:88)

n

Armed with these general results, we now consider the implications for Gaussian Process
regression.

3.6 Gaussian Regression case

When the model contains Gaussian likelihood functions, closed-form expressions for the
Power EP approximate factors at convergence can be obtained and hence the approximate
posterior:

tn(u) = N (KfnuK−

uuu; yn, αDfnfn + σ2
1

1
y), q(u) = N (u; Kuf K−

ﬀ y, Kuu − Kuf K−

ﬀ Kfu)

1

where Kﬀ = Qﬀ + αdiag(Dﬀ ) + σ2
yI and Dﬀ = Kﬀ − Qﬀ as deﬁned in section 2. These
analytic expressions can be rigorously proven to be the stable ﬁxed point of the Power EP
procedure using remark 1. Brieﬂy, assuming the factors take the form above, the natural
n(u) become,
parameters of the cavity q∗
\

n
1,u = T1,u − αγnynKfnuK−
T\

n
1
2,u = T2,u − αγnK−
uu, T\

1

uuKufnKfnuK−

1
uu,

(22)

n = αDfnfn + σ2
1
y. The subtracted quantities in the equations above are exactly
where γ−
the contribution the likelihood factor makes to the cavity distribution (see remark 1) so
n(u) (cid:82) p(fn|u)pα(yn|fn)dfn ∝ q∗(u). Therefore, the posterior
(cid:82) q∗
approximation remains unchanged after an update and the form for the factors above is the
ﬁxed point. Moreover, the approximate log-marginal likelihood is also analytically tractable,

n(f )pα(yn|fn)df
\

=u = q∗
\

log ZPEP = −

log(2π) −

log |Kﬀ | −

N
2

1
2

y(cid:124)K−

1
ﬀ y −

1
2

1 − α
2α

(cid:88)

n

log (cid:0)1 + αDfnfn/σ2

(cid:1) .

y

We now look at special cases and the correspondence to the methods discussed in section 2.

Remark 2 When α = 1 [EP], the Power EP posterior becomes the FITC posterior in
eq. (7) and the Power EP approximate marginal likelihood becomes the FITC marginal like-
lihood in eq. (8). In other words, the FITC approximation for GP regression is, surprisingly,
equivalent to running an EP algorithm for sparse GP posterior approximation to conver-
gence.

14

Unifying Gaussian Process Approximations

Remark 3 As α → 0 the approximate posterior and approximate marginal likelihood are
identical to that of the VFE approach in eqs. (9) and (10) (Titsias, 2009). This result uses
1 log(1 + x) = 1. So FITC and Titsias’s VFE approach employ the
the limit:
same form of pseudo-point approximation, but reﬁne it in diﬀerent ways.

limx

0 x−

→

Remark 4 For ﬁxed hyper-parameters, a single pass of Power EP is suﬃcient for conver-
gence in the regression case.

3.7 Extensions: Structured, Inter-domain and Multi-power Power EP

Approximations

The framework can now be generalised in three orthogonal directions:

1. enable structured approximations to be handled that retain more dependencies in the

spirit of PITC (see section 2.1)

of the approximate posterior

2. incorporate inter-domain pseudo-points thereby adding further ﬂexibility to the form

3. employ diﬀerent powers α for each factor (thereby enabling e.g. VFE updates to be

used for some data points and EP for others).

Given the groundwork above, these three extensions are straightforward. In order to handle
structured approximations, we take inspiration from PITC and partition the data into
B disjoint blocks yb = {yn}n
b (see section 2.1). Each PEP factor update will then
approximate an entire block which will contain a set of data points, rather than just a
single one. This is related to a form of EP approximation that has recently been used to
distribute Monte Carlo algorithms across many machines (Gelman et al., 2014; Xu et al.,
2014).

∈B

In order to handle inter-domain variables, we deﬁne a new domain via a linear transform
g(x) = (cid:82) dx(cid:48)W (x, x(cid:48))f (x(cid:48)) which now contains the pseudo-points g = {g
=u, u}. Choices for
W (x, x(cid:48)) include Gaussians or wavelets. These two extensions mean that the approximation
becomes,

p(f, g)

p(yb|f ) ≈ p(f, g)

tb(u) = q∗(f ).

(23)

(cid:89)

b

(cid:89)

b

Power EP is then performed using private powers αb for each data block, which is the third
generalisation mentioned above. Analytic solutions are again available (covariance matrices
now incorporate the inter-domain transform)

tb(u) = N (KfbuK−

uuu; yb, αbDfbfb + σ2
1

yI),

q(u) = N (u; Kuf K−

1

1
ﬀ y, Kuu − Kuf K−

ﬀ Kfu)

where Kﬀ = Qﬀ + blkdiag({αbDfbfb}B
yI and blkdiag builds a block-diagonal matrix
from its inputs. The approximate log-marginal likelihood can also be obtained in closed-
form,

b=1) + σ2

log ZPEP = −

log(2π) −

log |Kﬀ | −

N
2

1
2

y(cid:124)K−

1
ﬀ y +

1
2

(cid:88)

b

1 − αb
2αb

log (cid:0)I + αbDfbfb/σ2

y

(cid:1) .

15

Bui, Yan and Turner

Remark 5 When αb = 1 and W (x, x(cid:48)) = δ(x − x(cid:48)) the structured Power EP posterior
becomes the PITC posterior and the Power EP approximate marginal likelihood becomes
the PITC marginal likelihood. Additionally, when B = N we recover FITC as discussed in
section 3.6.

Remark 6 When αb → 0 and W (x, x(cid:48)) = δ(x − x(cid:48)) the structured Power EP posterior and
approximate marginal likelihood becomes identical to the VFE approach (Titsias, 2009).
This is a result of the equivalence of local and global exclusive KL divergence minimisation.
See appendix B for more details and ﬁg. 1 for more relationships.

3.8 Classiﬁcation

For classiﬁcation, the non-Gaussian likelihood prevents an analytic solution. As such, the
iterative Power EP procedure is required to obtain the approximate posterior. The pro-
jection step requires computation of the log-normaliser of the tilted distribution, log ˜Zn =
log E
\n(f )[pα(yn|f )] = log E
\n(fn)[Φα(ynfn)]. For general α, this quantity is not available in
q∗
q∗
closed form3. However, it involves a one-dimensional expectation of a non-linear function of
a normally-distributed random variable and, therefore, can be approximated using numeri-
cal methods, e.g. Gauss-Hermite quadrature. This procedure gives an approximation to the
expectation, resulting in an approximate update for the posterior mean and covariance. The
approximate log-marginal likelihood can also be obtained and used for hyper-parameter op-
timisation. As α → 0, it becomes the variational free-energy used in (Hensman et al., 2015)
which employs quadrature for the same purpose. These relationships are shown in ﬁg. 1
which also shows that inter-domain transformations and structured approximations have
not yet been fully explored in the classiﬁcation setting. In our view, the inter-domain gen-
eralisation would be a sensible one to pursue and it is mathematically and algorithmically
straightforward. The structured approximation variant is more complicated as it requires
multiple non-linear likelihoods to be handled at each step of EP. This will require further
approximation such as using Monte Carlo methods (Gelman et al., 2014; Xu et al., 2014).
In addition, when α = 1, M = N and the pseudo-points are at the training inputs, the
standard EP algorithm for GP classiﬁcation is recovered (Rasmussen and Williams, 2005,
sec. 3.6).

Since the proposed Power EP approach is general, an extension to other likelihood
functions is as simple as for VFE methods (Dezfouli and Bonilla, 2015). For example, the
multinomial probit likelihood can be handled in the same way as the binary case, where the
log-normaliser of the tilted distribution can be computed using a C-dimensional Gaussian
quadrature [C is the number of classes] (Seeger and Jordan, 2004) or nested EP (Riihim¨aki
et al., 2013).

3.9 Complexity

The computational complexity of all the regression and classiﬁcation methods described
in this section is O(N M 2) for training, and O(M 2) per test point for prediction. The
training cost can be further reduced to O(M 3), in a similar vein to the uncollapsed VFE

3. except for special cases, e.g. when α = 1 and Φ(x) is the probit inverse link function, Φ(x) =

(cid:82) x
−∞ N (a; 0, 1)da.

16

Unifying Gaussian Process Approximations

approach (Hensman et al., 2013, 2015), by employing stochastic updates of the poste-
rior and stochastic optimisation of the hyper-parameters using minibatches of data points
In particular, the Power EP update
(Hern´andez-Lobato and Hern´andez-Lobato, 2016).
steps in section 3.2 are repeated for only a small subset of training points and for only a
small number of iterations. The approximate log-marginal likelihood in eq. (21) is then
computed using this minibatch and optimised as if the Power EP procedure has converged.
This approach results in a computationally eﬃcient training scheme, at the cost of return-
ing noisy hyper-parameter gradients. In practice, we ﬁnd that the noise can be handled
using stochastic optimisers such as Adam (Kingma and Ba, 2015). In summary, given these
advances the general PEP framework is as scalable as variational inference.

4. Experiments

The general framework described above lays out a large space of potential inference algo-
rithms suggesting many exciting directions for innovation. The experiments considered in
the paper will investigate only one aspect of this space; how do algorithms that are interme-
diate between VFE (α = 0) and EP/FITC (α = 1) perform? Speciﬁcally, we will investigate
how the performance of the inference scheme varies as a function of α and whether this de-
pends on; the type of problem (classiﬁcation or regression); the dataset (synthetic datasets,
8 real world regression datasets and 6 classiﬁcation datasets); the performance metric (we
compare metrics that require point-estimates to those that are uncertainty sensitive). An
important by-product of the experiments is that they provide a comprehensive comparison
between the VFE and EP approaches which has been an important area of debate in its
own right.

The results presented below are compact summaries of a large number of experiments full
details of which are included in the appendix I (along with additional experiments). Python
and Matlab implementations are available at http://github.com/thangbui/sparseGP_
powerEP.

4.1 Regression on Synthetic Datasets

In the ﬁrst experiment, we investigate the performance of the proposed Power EP method
on toy regression datasets where ground truth is known. We vary α (from 0 VFE to 1
EP/FITC) and the number of pseudo-points (from 5 to 500). We use thirty datasets, each
comprising 1000 data points with ﬁve input dimensions and one output dimension, that
were drawn from a GP with an Automatic Relevance Determination squared exponential
kernel. A 50:50 train/test split was used. The hyper-parameters and pseudo-inputs were
found by optimising the PEP energy using L-BFGS with a maximum of 2000 function
evaluations. The performances are compared using two metrics: standardised mean squared
error (SMSE) and standardised mean log loss (SMLL) as described in (Rasmussen and
Williams, 2005, page 23). The approximate negative log-marginal likelihood (NLML) for
each experiment is also computed. The mean performance using Power EP with diﬀerent α
values and full GP regression is shown in ﬁg. 3. The results demonstrate that as M increases,
the SMLL and SMSE of the sparse methods approach that of full GP. Power EP with α = 0.8
or α = 1 (EP) overestimates the log-marginal likelihood when intermediate numbers of
pseudo-points are used, but the overestimation is markedly less when M = N = 500.

17

Bui, Yan and Turner

Importantly, however, an intermediate value of α in the range 0.5-0.8 seems to be best for
prediction on average, outperforming both EP and VFE.

Figure 3: The performance of various α values averaged over 30 trials. See text for more
details

4.2 Regression on Real-world Datasets

The experiment above was replicated on 8 UCI regression datasets, each with 20 train/test
splits. We varied α between 0 and 1, and M was varied between 5 and 200. Full details
of the experiments along with extensive additional analysis is presented in the appendices.
Here we concentrate on several key aspects. First we consider pairwise comparisons between
VFE (α → 0), Power EP with α = 0.5 and EP/FITC (α = 1) on both the SMSE and SMLL
evaluation metrics. Power EP with α = 0.5 was chosen because it is the mid-point between
VFE and EP and because settings around this value empirically performed the best on
average across all datasets, splits, numbers of inducing points, and evaluation metrics.

In ﬁg. 4A we plot (for each dataset, each split and each setting of M ) the evaluation
scores obtained using one inference algorithm (e.g. PEP α = 0.5) against the score obtained
using another (e.g. VFE α = 0). In this way, points falling below the identity line indicate
experiments where the method on the y-axis outperformed the method on the x-axis. These
results have been collapsed by forming histograms of the diﬀerence in the performance of
the two algorithms, such that mass to the right of zero indicates the method on the y-axis
outperformed that on the x-axis. The proportion of mass on each side of the histogram,
also indicated on the plots, shows in what fraction of experiments one method returns a
more accurate result than the other. This is a useful summary statistic, linearly related to

18

Unifying Gaussian Process Approximations

the average rank, that we will use to unpack the results. The average rank is insensitive to
the magnitude of the performance diﬀerences and readers might worry that this might give
an overly favourable view of a method that performs the best frequently, but only by a tiny
margin, and when it fails it does so catastrophically. However, the histograms indicate that
the methods that win most frequently tend also to ‘win big’ and ‘lose small’, although EP
is a possible exception to this trend (see the outliers below the identity line on the bottom
right-hand plot).

A clear pattern emerges from these plots. First PEP α = 0.5 is the best performing
approach on the SMSE metric, outperforming VFE 67% of the time and EP 78% of the
time. VFE is better than EP on the SMSE metric 64% of the time. Second, EP performs
the best on the SMLL metric, outperforming VFE 93% of the time and PEP α = 0.5 71%
of the time. PEP α = 0.5 outperforms VFE in terms of the SMLL metric 93% of the time.
These pairwise rank comparisons have been extended to other values of α in ﬁg. 5A.
Here, each row of the ﬁgure compares one approximation with all others. Horizontal bars
indicate that the methods have equal average rank. Upward sloping bars indicate the
method shown on that row has lower average rank (better performance), and downward
sloping bars indicate higher average rank (worse performance). The plots show that PEP
α = 0.5 outperforms all other methods on the SMSE metric, except for PEP α = 0.6 which
is marginally better. EP is outperformed by all other methods, and VFE only outperforms
EP on this metric. On the other hand, EP is the clear winner on the SMLL metric, with
performance monotonically decreasing with α so that VFE is the worst.

The same pattern of results is seen when we simultaneously compare all of the methods,
rather than considering sets of pairwise comparisons. The average rank plots shown in
ﬁg. 4B were produced by sorting the performances of the 8 diﬀerent approximating methods
for each dataset, split, and number of pseudo-points M and assigning a rank. These ranks
are then averaged over all datasets and their splits, and settings of M . PEP α = 0.5 is the
best for the SMSE metric, and the two worst methods are EP and VFE. PEP α = 0.8 is the
best for the SMLL metric, with EP and PEP α = 0.6 not far behind (when EP performs
poorly it can do so with a large magnitude, explaining the discrepancy with the pairwise
ranks).

There is some variability between individual datasets, but the same general trends are
clear: For MSE α = 0.5 is better than VFE on 6/8 datasets and EP on 8/8 datasets, whilst
VFE is better than EP on 3 datasets (the diﬀerence on the others being small). For NLL
EP is better than α = 0.5 on 5/8 datasets and VFE on 7/8 datasets, whilst α = 0.5 is better
than VFE on 8/8 datasets. Performance tends to increase for all methods as a function
of the number of pseudo-points M. The interaction between the choice of M and the best
performing inference method is often complex and variable across datasets making it hard
to give precise advice about selecting α in an M dependent way.

In summary, we make the following recommendations based on these results for GP
regression problems. For a MSE loss, we recommend using α = 0.5. For a NLL we recom-
mend using EP. It is possible that more ﬁne grained recommendations are possible based
upon details of the dataset and the computational resources available for processing, but
further work will be needed to establish this.

19

2
0

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 4: Pair-wise comparisons between Power EP with α = 0.5, EP (α = 1) and VFE (α → 0), evaluated on several regression
datasets and various settings of M . Each coloured point is the result for one split. Points that are below the diagonal line
illustrate the method on the y-axis is better than the method on the x-axis. The inset diagrams show the histograms of the
diﬀerence between methods (x-value − y-value), and the counts of negative and positive diﬀerences. Note that this indicates
pairwise ranking of the two methods. Positive diﬀerences mean the y-axis method is better than the x-axis method and vice
versa. For example, the middle, bottom plot shows EP is on average better than VFE.

Unifying Gaussian Process Approximations

Figure 5: Average ranking of various α values in the regression experiment, lower is better.
Top plots show the pairwise comparisons. Red circles denote rows being better than the
corresponding columns, and blue circles mean vice versa. Bottom plots show the ranks of
all methods when being compared together. Intermediate α values (not EP or VFE) are
best on average.

4.3 Binary Classiﬁcation

We also evaluated the Power EP method on 6 UCI classiﬁcation datasets, each has 20
train/test splits. The details of the datasets are included in appendix I.3. The datasets are
all roughly balanced, and the most imbalanced is pima with 500 positive and 267 negative

21

Bui, Yan and Turner

data points. Again α was varied between 0 and 1, and M was varied between 10 and 100.
We adopt the experimental protocol discussed in section 3.9, including: (i) not waiting for
Power EP to converge before making hyper-parameter updates, (ii) using minibatches of
data points for each Power EP sweep, (iii) parallel factor updates. The Adam optimiser was
used with default hyper-parameters to handle the noisy gradients produced by these ap-
proximations (Kingma and Ba, 2015). We also implemented the VFE approach of Hensman
et al. (2015) and include this in the comparison to the PEP methods. The VFE approach
should be theoretically identical to PEP with small α, however, we note that the results
can be slightly diﬀerent due to diﬀerences in the implementation – optimisation for VFE
vs. the iterative PEP procedure and we also note that each step of PEP only gets to see a
tiny fraction of each data point when α is small which can slow the learning speed. Similar
to the regression experiment, we compare the methods using the pairwise ranking plots on
the test error and negative log-likelihood (NLL) evaluation metrics.

In ﬁg. 6, we plot (for each dataset, each split and each setting of M ) the evaluation scores
using one inference algorithm against the score obtained using another [see section 4.2 for a
detailed explanation of the plots]. In contrast to the regression results in section 4.2, there
are no clear-cut winners among the methods. The test error results show that PEP α = 0.5
is marginally better than VFE and EP, while VFE edges EP out in this metric. Similarly,
all methods perform comparably on the NLL scale, except with PEP α = 0.5 outperforming
EP by a narrow magin (65% of the time vs. 35%)

We repeat the pairwise comparison above to all methods and show the results in ﬁg. 7.
The plots show that there is no conlusive winner on the test error metric, and VFE, PEP
α = 0.4 and PEP α = 0.5 have a slight edge over other α values on the NLL metric. Notably,
methods corresponding to bigger α values, such as PEP α = 0.8 and EP, are outperformed
by all other methods. Similar to the regression experiment, we observe the same pattern
of results when all methods are simultaneously compared, as shown in ﬁg. 7. However, the
large errorbars suggest the diﬀerence between the methods is small in both metrics.

There is some variability between individual datasets, but the general trends are clear
and consistent with the pattern noted above. For test error, PEP α = 0.5 is better than
VFE on 1/6 dataset and is better than EP on 3/6 datasets (the diﬀerences on the other
datasets are small). VFE outperforms EP on 2/6 datasets, while EP beats VFE on only
1/6 datasets. For NLL, PEP α = 0.5 only clearly outperforms VFE on 1/6 dataset, but
is worse compared to VFE on 1 dataset (the other 4 datasets have no clear winner). PEP
α = 0.5 is better than EP on 5/6 datasets and EP is better on the remaining dataset). EP
is only better than VFE on 2/6 datasets, and is outperformed by VFE on the other 4/6
datasets. The ﬁnding that PEP and VFE are slightly better than EP on the NLL metric
is surprising as we expected EP perform the best on the uncertainty sensitive metric (just
as was discovered in the regression case). The full results are included in the appendices
(see ﬁgs 25, 26 and 27). Similar to the regression case, we observe that as M increases, the
performance tends to be better for all methods and the diﬀerences between the methods
tend to become smaller, but we have not found evidence for systematic sensitivity to the
nature of the approximation.

In summary, we make the following recommendations based on these results for GP
classiﬁcation problems. For a raw test error loss and for NLL, we recommend using α = 0.5
(or α = 0.4). It is possible that more ﬁne grained recommendations are possible based upon

22

Unifying Gaussian Process Approximations

details of the dataset and the computational resources available for processing, but further
work will be needed to establish this.

23

2
4

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 6: Pair-wise comparisons between Power EP with α = 0.5, EP (α = 1) and VFE (α → 0), evaluated on several classiﬁcation
datasets and various settings of M . Each coloured point is the result for one split. Points that are below the diagonal line illustrate
the method on the y-axis is better than the method on the x-axis. The inset diagrams show the histograms of the diﬀerence
between methods (x-value − y-value), and the counts of negative and positive diﬀerences. Note that this indicates pairwise
ranking of the two methods. Positive diﬀerences means the y-axis method is better than the x-axis method and vice versa.

Unifying Gaussian Process Approximations

Figure 7: Average ranking of various α values in the classiﬁcation experiment, lower is
better. Top plots show the pairwise comparisons. Red circles denote rows being better
than the corresponding columns, and blue circles mean vice versa. Bottom plots show the
ranks of all methods when being compared together.
Intermediate α values (not EP or
VFE) are best on average.

5. Discussion

It is diﬃcult to identify precisely where the best approximation methods derive their advan-
tages, but here we will speculate. Since the negative variational free-energy is a lower-bound
on the log-marginal likelihood it has the enviable theoretical guarantee that pseudo-input

25

Bui, Yan and Turner

optimisation is always guaranteed to improve the estimate of the log marginal likelihood
and the posterior (as measured by the inclusive KL). The negative EP energy, in contrast,
is not generally a lower bound which can mean that pseudo-input optimisation drives the
solution to the point where the EP energy over-estimates the log marginal likelihood the
most, rather than to the point where the marginal likelihood and/or posterior estimate
is best. For this reason, we believe that variational methods are likely to be better than
EP if the goal is to derive accurate marginal likelihood estimates, or accurate predictive
distributions, for ﬁxed hyper-parameter settings. For hyper-parameter optimisation, things
are less clear cut since variational methods are biased away from the maximal marginal
likelihood, towards hyper-parameter settings for which the posterior approximation is ac-
curate. Often this bias is severe and also creates local-optima (Turner and Sahani, 2011).
So, although EP will generally also be biased away from the maximal marginal likelihood
and potentially towards areas of over-estimation, it can still outperform variational meth-
ods. Superposed onto these factors, is a general trend for variational methods to minimise
MSE / classiﬁcation error-rate and EP methods to minimise negative log-likelihood, due
to the form of their respective energies (the variational free-energy includes the average
training MSE in the regression case, for example). Intermediate methods will blend the
strengths and weaknesses of the two extremes. It is interesting that values of α around a
half are arguably the best performing on average. Similar empirical conclusions have been
made elsewhere Minka (2005); Hern´andez-Lobato et al. (2016); Depeweg et al. (2016). In
this case, the alpha-divergence interpretation of Power EP shows that it is minimising the
Hellinger distance whose square root is a valid distance metric. Further experimental and
theoretical work is required to clarify these issues.

The results presented above employed (approximate) type-II maximum likelihood ﬁtting
of the hyper-parameters. This estimation method is known in some circumstances to overﬁt
the data. It is therefore conceivable therefore that pseudo-point approximations, which have
a tendency to encourage under-ﬁtting due to their limited representational capacity, could
be beneﬁcial due to them mitigating overﬁtting. We do not believe that this is a strong eﬀect
in the experiments above. For example, in the synthetic data experiments the NLML, SMSE
and SMLL obtained from ﬁtting the unapproximated GP were similar to those obtained
using the GP from which the data were generated, indicating that overﬁtting is not a strong
eﬀect (see ﬁg. 9 in the appendix). It is true that EP and α = 0.8 over-estimates the marginal
likelihood in the synthetic data experiments, but this is a distinct eﬀect from over-ﬁtting
which would, for example, result in overconﬁdent predictions on the test dataset. The SMSE
and SMLL on the training and test sets, for example, are similar which is indicative of a
well-ﬁt model. It would be interesting to explore distributional hyper-parameter estimates
(see e.g. Piironen and Vehtari, 2017) that employ these pseudo-point approximations.

One of the features of the approximate generative models introduced in section 2.1 for
regression, is that they contain input-dependent noise, unlike the original model. Many
datasets contain noise of this sort and so approximate models like FITC and PITC, or
models in which the observation noise is explicitly modelled are arguably more appropri-
ate than the original unapproximated regression model (Snelson, 2007; Saul et al., 2016).
Motivated by this train of reasoning, Titsias (2009) applied the variational free-energy ap-
proximation to the FITC generative model an approach that was later generalised by Hoang
et al. (2016) to encompass a more general class of input dependent noise, including Markov

26

Unifying Gaussian Process Approximations

structure (Low et al., 2015). Here the insight is that the resulting variational lower bound
separates over data points (Hensman et al., 2013) and is, therefore, amenable to stochastic
optimisation using minibatches unlike the marginal likelihood. In a sense, these approaches
unify the approximate generative modelling approach, including the FITC and PITC vari-
ants, with the variational free-energy methods. Indeed, one approach is to posit the desired
form of the optimal variational posterior, and to work backwards from this to construct
the generative model implied (Hoang et al., 2016). However, these approaches are quite
diﬀerent from the one described in this paper where FITC and PITC are shown to emerge
in the context of approximating the original unapproximated GP regression model using
Power EP. Indeed, if the goal really is to model input dependent noise, it is not at all clear
that generative models like FITC are the most sensible. For example, FITC uses a single
set of hyper-parameters to describe the variation of the underlying function and the input
dependent noise.

6. Conclusion

This paper provided a new unifying framework for GP pseudo-point approximations based
on Power EP that subsumes many previous approaches including FITC, PITC, DTC, Tit-
sias’s VFE method, Qi et al’s EP method, and inter-domain variants. It provided a clean
computational perspective on the seminal work of Csat´o and Opper that related FITC to
EP, before extending their analysis signiﬁcantly to include a closed form Power EP marginal
likelihood approximation for regression, connections to PITC, and further results on clas-
siﬁcation and GPSSMs. The new framework was used to devise new algorithms for GP
regression and GP classiﬁcation. Extensive experiments indicate that intermediate values
of Power EP with the power parameter set to α = 0.5 often outperform the state-of-the-
art EP and VFE approaches. The new framework suggests many interesting directions
for future work in this area that we have not explored, for example, extensions to online
inference, combinations with special structured matrices (e.g. circulant and Kronecker struc-
ture), Bayesian hyper-parameter learning, and applications to richer models. The current
work has only scratched the surface, but we believe that the new framework will form a
useful theoretical foundation for the next generation of GP approximation schemes.

Acknowledgments

The authors would like to thank Prof. Carl Edward Rasmussen, Nilesh Tripuraneni,
Matthias Bauer, James Hensman, and Hugh Salimbeni for insightful comments and dis-
cussion. TDB thanks Google for funding his European Doctoral Fellowship. RET thanks
EPSRC grants EP/G050821/1, EP/L000776/1 and EP/M026957/1.

Appendix A. A uniﬁed objective for un-normalised KL variational

free-energy methods

Here we show that performing variational inference by optimising the un-normalised KL
naturally leads to a single objective for both the approximation to the joint distribution,
q∗(f |θ) and the hyper-parameters θ.

27

Bui, Yan and Turner

The un-normalised KL is given by

(cid:90)

KL(q∗(f |θ)||p(f, y|θ)) =

q∗(f |θ)
p(f, y|θ)
This is intractable as it includes the marginal likelihood p(y|θ) = (cid:82) p(f, y|θ)df . However,
since we are interested in minimising this objective with respect to q∗(f |θ) we can ignore
the intractable term,

(p(f, y|θ) − q∗(f |θ)) df.

q∗(f |θ) log

df +

(24)

(cid:90)

argmin
q∗(f
θ)

KL(q∗(f |θ)||p(f, y|θ)) = argmax
θ)

q∗(f

|

(cid:0)p(y|θ) − KL(q∗(f |θ)||p(f, y|θ))(cid:1)

(cid:18)(cid:90)

= argmax
q∗(f
θ)

|

|

q∗(f |θ) log

df +

q∗(f |θ)df

.

(cid:90)

p(f, y|θ)
q∗(f |θ)

(25)

(cid:19)

(26)

In other words, we have turned the unnormalised KL into a tractable lower-bound of the
marginal likelihood G(q∗(f |θ), θ) = p(y|θ)−KL(q∗(f |θ)||p(f, y|θ)). The structure of this new
lower-bound can be understood by decomposing the approximation to the joint distribution
into a normalised posterior approximation q(f |θ) and an approximation to the marginal
likelihood, ZVFE, that is q∗(f |θ) = ZVFEq(f |θ).

G(ZVFEq(f |θ), θ) = ZVFE

1 − log ZVFE +

q(f |θ) log

(27)

(cid:18)

(cid:90)

(cid:19)

p(f, y|θ)
q(f |θ)

df

We can see that optimising the lower-bound with respect to θ is equivalent to optimising the
standard variational free-energy F(q(f |θ), θ) = (cid:82) q(f |θ) log p(f,y
θ)
θ) df . Moreover, optimising
|
q(f
|
for ZVFE recovers Zopt

VFE = exp(F(q(f |θ), θ)). Substituting this back into the bound

G(Zopt

VFEq(f |θ), θ) = Zopt

VFE = exp(F(q(f |θ), θ)).

(28)

In other words, the new collapsed bound is just the exponential of the original variational
free-energy and optimising the collapsed bound for θ is equivalent to optimising the approx-
imation to the marginal likelihood.

Appendix B. Global and local inclusive KL minimisations

In this section, we will show that optimising a single global inclusive KL-divergence,
KL(q||p),
is equivalent to optimising a sum of a set of local inclusive KL-divergence,
KL(q||˜p), where p, q and ˜p are the exact posterior, the approximate posterior and the tilted
distribution accordingly. Without loss of generality, we assume that p(θ) = (cid:81)
n fn(θ) ≈
(cid:81)
n tn(θ) = q(θ), that is the exact posterior is a product of factors, {fn(θ)}n, each of which
is approximated by an approximate factor tn(θ). Substituting these distributions into the
global KL-divergence gives,

KL(q(θ)||p(θ)) =

dθq(θ) log

(cid:90)

(cid:90)

=

dθq(θ) log

q(θ)
p(θ)
(cid:81)
(cid:81)

n tn(θ)
n fn(θ)

28

Unifying Gaussian Process Approximations

(cid:21)

=n ti(θ)
=n ti(θ)

i

i

(cid:20) (cid:81)
(cid:81)

(cid:81)

(cid:81)

n

(cid:81)

(cid:81)

n
i ti(θ)]

n tn(θ)
n fn(θ)
n[(cid:81)
(cid:81)
n[fn(θ) (cid:81)
(cid:81)
[fn(θ) (cid:81)

=n ti(θ)]

i
i ti(θ)

=n ti(θ)

i

=

dθq(θ) log

(cid:90)

(cid:90)

=

dθq(θ) log

(cid:81)

(cid:90)

(cid:88)

=

=

n
(cid:88)

n

dθq(θ) log

KL(q(θ)||˜pn(θ)),

which means running the EP procedure, where we use KL(q(θ)||˜pn(θ)) in place of
KL(˜pn(θ)||q(θ)), is equivalent to the VFE approach which optimises a single global KL-
divergence, KL(q(θ)||p(θ)).

Appendix C. Some relevant linear algebra and function expansion

identities

The Woodbury matrix identity or Woodbury formula is:

(A + U CV )−

1 = A−

1 − A−

1U (C−

1 + V A−

1U )−

1V A−

1.

(30)

In general, C need not be invertible, we can use the Binomial inverse theorem,

(A + U CV )−

1 = A−

1 − A−

1U C(C + CV A−

1U C)−

1CV A−

1.

(31)

When C is an identity matrix and U and V are vectors, the Woodbury identity can be

shortened and become the Sherman-Morrison formula,

(A + uv

(cid:124)

)−

1 = A−

1 −

1uv(cid:124)A−
1
A−
1 + v(cid:124)A−
1u

.

Another useful identity is the matrix determinant lemma,

det(A + uv

) = (1 + v

(cid:124)

(cid:124)

A−

1u)det(A).

The above theorem can be extend for matrices U and V ,

det(A + U V

) = det(I + V

(cid:124)

(cid:124)

A−

1U )det(A).

We also make use of the following Maclaurin series,

exp(x) = 1 + x +

+

+ · · ·

and log(1 + x) = x −

+

+ · · · .

x3
3!

x2
2!

x3
3

x2
2

29

(29)

(32)

(33)

(34)

(35)

(36)

Bui, Yan and Turner

Appendix D. KL minimisation between Gaussian processes and moment

matching

The diﬃcult step of Power-EP is the projection step, that is how to ﬁnd the posterior
approximation q(f ) that minimises the KL divergence, KL(˜p(f )||q(f )), where ˜p(f ) is the
tilted distribution. We have chosen the form of the approximate posterior

q(f ) = p(f

=u|u)q(u) = p(f

=u|u)

exp(θ

(cid:124)
uφ(u))

,

Z(θu)

where Z(θu) = (cid:82) exp(θ
minimisation objective as follows,

(cid:124)
uφ(u))du to ensure normalisation. We can then write the KL

FKL = KL(˜p(f )||q(f ))
˜p(f )
q(f )

˜p(f ) log

=

(cid:90)

df

= (cid:104)log ˜p(f )(cid:105)˜p(f ) − (cid:104)log p(f

=u|u)(cid:105)˜p(f ) − θ

(cid:124)
u(cid:104)φ(u)(cid:105)˜p(f ) + log Z(θu).

=u|u) is the prior conditional distribution, the only free parameter that controls
Since p(f
our posterior approximation is θu. As such, to ﬁnd θu that minimises FKL, we ﬁnd the
gradient of FKL w.r.t θu and set it to zero,

0 =

dFKL
dθu

= −(cid:104)φ(u)(cid:105)˜p(f ) +

d log Z(θu)
dθu

= −(cid:104)φ(u)(cid:105)˜p(f ) + (cid:104)φ(u)(cid:105)q(u),

therefore, (cid:104)φ(u)(cid:105)˜p(f ) = (cid:104)φ(u)(cid:105)q(u). That is, though we are trying to perform the KL min-
imisation between two Gaussian processes, due to the special form of the posterior approx-
imation, it is suﬃcient to only match the moments at the inducing points u.4

Appendix E. Shortcuts to the moment matching equations

The most crucial step in Power-EP is the moment matching step as discussed above. This
step can be done analytically for the Gaussian case, as the mean and covariance of the
approximate posterior can be linked to the cavity distribution as follows,

mu = m\

n
n
u + V\
uf

,

d log Ztilted,n
n
dm\
f
d2 log Ztilted,n
dm\
f

n,2

n
n
u + V\
Vu = V\
uf

n
V\
f u,

where Ztilted,n is the normaliser of the tilted distribution,

(cid:90)

Ztilted,n =

q\

n(f )p(yn|f )df

4. We can show that this condition gives the minimum of FKL by computing the second derivative.

30

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

Unifying Gaussian Process Approximations

(cid:90)

(cid:90)

=

=

q\

n(f )p(yn|fn)df

q\

n(fn)p(yn|fn)dfn.

n
n
u + V\
mu = m\
ufn

,

d log Ztilted,n
n
dm\
fn
d2 log Ztilted,n
dm\
fn

n,2

n
n
u + V\
Vu = V\
ufn

n
V\
fnu.

In words, Ztilted,n only depends on the marginal distribution of the cavity process, q\
simplifying the moment matching equations above,

n(fn),

n
n
1
u K−
ufn = V\
We can rewrite the cross-covariance V\

uuKufn. We also note that, m\

n
fn =

n
1
uum\
KfnuK−
u , resulting in,

d log Ztilted,n
n
dm\
u

=

d log Ztilted,n
n
dm\
fn

1

K−

uuKufn,

d log Ztilted,n
n
dV\
u

= K−

uuKufn

1

d2 log Ztilted,n
dm\
fn

n,2

1
KfnuK−
uu.

Substituting these results back in eqs. 48 and 49, we obtain

n
u + V\
mu = m\
u

n

,

d log Ztilted,n
n
dm\
u
d2 log Ztilted,n
dm\
u

n,2

n
n
u + V\
Vu = V\
u

n
V\
u .

Therefore, using eqs. 48 and 49, or eqs. 52 and 53 are equivalent in our approximation

settings.

Appendix F. Full derivation of the Power-EP procedure

We provide the full derivation of the Power-EP procedure in this section. We follow the
derivation in (Qi et al., 2010) closely, but provide a clearer exposition and details how to
get to each step used in the implementation, and how to handle powered/fractional deletion
and update in Power-EP.

F.1 Optimal factor parameterisation

We start by deﬁning the approximate factors to be in natural parameter form as this makes
it simple to combine and delete them, tn(u) = ˜N (u; zn, T1,n, T2,n) = zn exp(u(cid:124)T1,n −
2 u(cid:124)T2,nu). We initially consider full rank T2,n, but will show that the optimal form is rank
1
1.

31

Bui, Yan and Turner

The next goal is to relate these parameters to the approximate GP posterior. The
n T1,n
n T2,n. This induces an approximate GP posterior with mean and

approximate posterior over the pseudo-outputs has natural parameters T1,u = (cid:80)
and T2,u = K−
covariance function,

uu + (cid:80)

1

1
1
2,uT1,u = Kfuγ
mf = KfuK−
uuT−
1
1
2,uK−
uuT−
Vﬀ (cid:48) = Kﬀ (cid:48) − Qﬀ (cid:48) + KfuK−

1

uuKuf (cid:48) = Kﬀ (cid:48) − KfuβKuf (cid:48).

(54)

(55)

where γ and β are likelihood-dependent terms we wish to store and update using PEP; γ
and β fully specify the approximate posterior.

Deletion step: The cavity for data point n, q\

n(u), has a similar form to
n
1,u = T1,u − αT1,n
the posterior, but the natural parameters are modiﬁed by the deletion, T\

n(f ) ∝ q∗(f )/tα

n
2,u = T2,u − αT2,n, yielding a new mean and covariance function
and T\

(56)

nKuf (cid:48).

uuKuf (cid:48) = Kﬀ (cid:48) − Kfuβ\

n
n,
1
n
1,u = Kfuγ\
2,u T\
−

n
1
uuT\
f = KfuK−
m\
n
1
1
uuT\n, −12,uK−
ﬀ (cid:48) = Kﬀ (cid:48) − Qﬀ (cid:48) + KfuK−
V \
Projection step: The central step in Power EP is the projection step. Obtaining the new
approximate unormalised posterior q∗(f ) such that KL(˜p(f )||q∗(f )) is minimised would
na¨ıvely appear intractable. Fortunately, as shown in the previous section, because of the
=u|u)q(u), the objective, KL(˜p(f )||q∗(f ))
structure of the approximate posterior, q(f ) = p(f
is minimised when E
q(u)[φ(u)], where φ(u) are the suﬃcient statistics, that is
when the moments at the pseudo-inputs are matched. This is the central result from which
computational savings are derived. Furthermore, this moment matching condition would
appear to necessitate computation of a set of integrals to ﬁnd the zeroth, ﬁrst and second
moments. Using results from the previous section simpliﬁes and provides the following
shortcuts,

˜p(f )[φ(u)] = E

(57)

n
n
u + V\
mu = m\
ufn

n
n
Vu = V\
u + V\
ufn

n
V\
fnu.

d log ˜Zn
n
dm\
fn
d2 log ˜Zn
n
fn )2
d(m\

where log ˜Zn = log E

q\n(f )[pα(yn|fn)] is the log-normaliser of the tilted distribution.

Update step: Having computed the new approximate posterior, the fractional approx-

imate factor tn,new(u) = q∗(f )/q\

n(f ) can be straightforwardly obtained, resulting in,

T1,n,new = V−

1

n
1
n,
u m\
u mu − V\
−
u
1

n,

1

u − V\
T2,n,new = V−
u
n = ˜Zn exp(G
zα
\n
∗ (u)
q

−

− Gq∗(u)),
(u;z,T1,T2) = (cid:82) ˜N (u; z, T1, T2)du. Let d1 = d log ˜Zn

where G ˜
N

eq. (30) and eq. (59), we have,

and d2 = d2 log ˜Zn

)2 . Using

d(m

\n
fn

dm

\n
fn

1
V−

u − V\
u

n,

1

−

= −V\

n,
1
u V\
−

n
ufn

n
1
fnuV\
2 + V\
d−

n
n,
1
u V\
−
ufn

n
fnuV\
V\
u

n,

1

−

(63)

(cid:105)−
1

(cid:104)

32

(58)

(59)

(60)

(61)

(62)

Unifying Gaussian Process Approximations

n
1
Let vn = α(−d−
fnuV\
2 − V\
eq. (61) gives

n
n,
1
ufn), and wn = V\
u V\
−

1
n,
u V\
−

n
ufn. Combining eq. (63) and

1
T2,n,new = wnαv−

n w(cid:124)

n

At convergence, we have tn(u)α = tn,new(u), hence T2,n = wnv−
1
n w
optimally a rank-1 matrix. Note that,

(cid:124)
n. In words, T2,n is

wn = V\

n
n,
1
u V\
−
ufn

= (Kuu − Kuuβ\

1
= K−
1
= K−

uu(I − Kuuβ\
uuKufn.

nKuu)−
n)−

1(Kufn − Kuuβ\
n)Kufn

1(I − Kuuβ\

nKufn)

Using eq. (58) an eq. (64) gives,

1
V−

1

n,

n w(cid:124)
n
n
u mu = (V\
u + V\
n)(m\
ufnd1)
u
n w(cid:124)
n,
1
n
n
n,
1
1
u V\
u + V\
nm\
u + wnαv−
u m\
−
−

+ wnαv−

= V\

−

1

n
1
ufnd1 + wnαv−

n w(cid:124)

n
nV\
ufnd1

Substituting this result into eq. (60),

T1,n,new = V−

n
n,
1
u m\
−
u

1

u mu − V\
n w(cid:124)
1
nm\
(cid:16)
w(cid:124)

1

= wnαv−

= wnαv−
n

n
u + V\

n w(cid:124)
n
n,
1
ufnd1 + wnαv−
u V\
−
(cid:17)
u + d1vn/α + w(cid:124)
n
nV\
.
ufnd1

n
nm\

1

n
nV\
ufnd1

Let T1,n,new = wnαv−

n gn, we obtain,

1

gn = −

+ Kfnuγ\

n.

d1
d2

At convergence, T1,n = wnv−
and T2,n at convergence,

1

n gn. Re-writing the form of the approximate factor using T1,n

tn(u) = ˜N (u; zn, T1,n, T2,n)
1
= zn exp(u(cid:124)T1,n −
2

= zn exp(u(cid:124)wnv−

1

n gn −

u(cid:124)wnv−

n w(cid:124)

1

nu)

u(cid:124)T2,nu)
1
2

As a result, the minimal and simplest way to parameterise the approximate factor is tn(u) =
˜znN (w
uuu; gn, vn), where gn and vn are scalars, resulting in a
signiﬁcant memory saving compared to the parameterisation using T1,n and T2,n.

(cid:124)
1
nu; gn, vn) = ˜znN (KfnuK−

33

(64)

(65)

(66)

(67)

(68)

(69)

(70)

(71)

(72)

(73)

(74)

(75)

(76)

(77)

Note that:

and

F.2 Projection

We now recall the update equations in the projection step (eqns. 58 and 59):

Bui, Yan and Turner

n
n
u + V\
mu = m\
ufnd1,
n
n
n
ufnd2V\
u + V\
Vu = V\
fnu.

mu = Kuuγ,
Vu = Kuu − KuuβKuu,

n
u = Kuuγ\
m\
n
u = Kuu − Kuuβ\
V\

n,

nKuu.

γ = K−

= K−

= γ\

β = K−

1
uumu
n
n
1
uu(m\
u + V\
ufnd1)
n
1
n + K−
uuV\
ufnd1, and
1
1
uu(Kuu − Vu)K−
uu

= K−

= β\

1

n
n
n
1
fnu)K−
ufnd2V\
u − V\
uu(Kuu − V\
uu
n
n
1
n − K−
1
fnuK−
ufnd2V\
uuV\
uu

(78)

(79)

(80)

(81)

(82)

(83)

(84)

(85)

(86)

(87)

(88)

(89)

Using these results, we can convert the update for the mean and covariance, mu and Vu,
into an update for γ and β,

F.3 Deletion step

Finally, we present how deletion might be accomplished. One direct approach to this step
is to divide out the cavity from the cavity, that is,

q\

n(f ) ∝

q(f )
tα
n(u)

p(f

=

=u|u)q(u)
tα
n(u)

= p(f

=u|u)q\

n(u).

(90)

Instead, we use an alternative using the KL minimisation as used in (Qi et al., 2010), by
realising that doing this will result in an identical outcome as the direct approach since
the factor and distributions are Gaussian. Furthermore, we can re-use results from the
projection and inclusion steps, by simply swapping the quantities and negating the site
approximation variance. In particular, we present projection and deletion side-by-side, to
facilitate the comparison,

Projection:

q(f ) ≈ q\

n(f )p(yn|fn)

(91)

34

Unifying Gaussian Process Approximations

Deletion:

q\

n(f ) ∝ q(f )

1
tα
n(u)

The projection step minimises the KL between the LHS and RHS while moment match-
n(f ), and thus

ing, to get q(f ). We would like to do the same for the deletion step to ﬁnd q\
reuse the same moment matching results for γ and β with some modiﬁcations.

Our task will be to reuse Equations 86 and 89, the moment matching equations in γ
and β. We have two diﬀerences to account for. Firstly, we need to change any uses of
the parameters of the cavity distribution to the parameters of the approximate posterior,
n
n to β. This is the equivalent of re-deriving the entire
V\
ufn to Vufn, γ\
projection operation while swapping the symbols (and quantities) for the cavity and the
full distribution. Secondly, the derivatives d1 and d2 are diﬀerent here, as

n to γ and β\

Now, we note

log ˜Zn = log

q(f )

1
tα
n(u)

df

1
tn(u)

∝

∝

N α(w

(cid:124)
nu; gn, vn)

1

(cid:16)

exp

− α

1
n (w

2 v−

1

= exp

(cid:18) 1
2
∝ N (w(cid:124)
nu; gn, −vn/α)

n (w(cid:124)

αv−

(cid:124)

nu − gn)2(cid:17)
(cid:19)

nu − gn)2

Then we obtain the derivatives of log ˜Zn

= − (cid:2)Kfn,uK−

u,uKu,fn − Kfn,uβKu,fn − vn/α(cid:3)−

1

1

˜d2 =

˜d1 =

d2 log ˜Zn
dm2
fn
d log ˜Zn
dmfn

= (Kfn,uγ − gn) ˜d2

Putting the above results together, we obtain,

γ\

β\

1

n = γ + K−
1
n = β − K−

uuVufn
uuVufn

˜d1, and
˜d2VfnuK−
1
uu

(cid:90)

1

35

F.4 Summary of the PEP procedure

We summarise here the key steps and equations that we have obtained, that are used in
the implementation:

(92)

(93)

(94)

(95)

(96)

(97)

(98)

(99)

(100)

(101)

Bui, Yan and Turner

1. Initialise the parameters: {gn = 0}N

n=1, {vn = ∞}N

n=1, γ = 0M

1 and β = 0M

×

M

×

2. Loop through all data points until convergence:

(a) Deletion step: ﬁnd γ\

n
n and β\

(b) Projection step: ﬁnd γ and β

(c) Update step: ﬁnd gn,new and vn,new

γ\

β\

1
n = γ + K−
n = β − K−

uuVufn
uuVufn

1

˜d1, and
˜d2VfnuK−
1
uu

γ = γ\

β = β\

n
1
n + K−
uuV\
ufnd1,
n
1
n − K−
ufnd2V\
uuV\

n
1
fnuK−
uu

gn,new = −

d1
d2
n
1
fnuV\
2 − V\
vn,new = −d−

+ Kfnuγ\

n,

n
n,
1
u V\
−
ufn

and parameters for the full factor,

1
vn ← (v−
gn ← vn(gn,newv−

n,new + (1 − α)v−
1

1
1
n )−
n,new + (1 − α)gnv−

1
n )

Appendix G. Power-EP energy for sparse GP regression and

classiﬁcation

The Power-EP procedure gives an approximate marginal likelihood, which is the negative
Power-EP energy, as follows,

F = G(q

(u)) − G(p
∗

∗

(u)) +

1
α

(cid:88)

n

(cid:104)
n
log Ztilted,n + G(q\
∗

(u)) − G(q

(cid:105)
(u))

∗

where G(q

(u)) is the log-normaliser of the approximate posterior, that is,

∗

G(q

(u)) = log

p(f

=u|u) exp(θ

(cid:124)
uφ(u))df

=udu

∗

= log

exp(θ

(cid:124)
uφ(u))du

=

log(2π) +

log |V| +

m(cid:124)V−

1m,

1
2

(cid:90)

(cid:90)

M
2

1
2

1
2

1
2

36

where m and V are the mean and covariance of the posterior distribution over u, respec-
tively. Similarly,

n
G(q\
∗

M
2

(u)) =

log(2π) +

log |Vcav,n| +

m(cid:124)

1
cav,nV−

cav,nmcav,n,

(114)

(102)

(103)

(104)

(105)

(106)

(107)

(108)

(109)

(110)

(111)

(112)

(113)

(115)

(116)

(117)

(118)

(119)

(120)

(121)

(122)

(123)

(124)

Unifying Gaussian Process Approximations

and G(p
∗

(u)) =

M
2

log(2π) +

log |Kuu|.

1
2

Finally, log Ztilted,n is the log-normalising constant of the tilted distribution,

log Ztilted = log

qcav(f )pα(yn|f )df

= log

p(f

=u|u)qcav(u)pα(yn|f )df

=udu

= log

p(fn|u)qcav(u)pα(yn|fn)dfndu

(cid:90)

(cid:90)

(cid:90)

Next, we can write down the form of the natural parameters of the approximate posterior

and the cavity distribution, based on the approximate factor’s parameters, as follows,

V−

1 = K−

1
uu +

(cid:88)

(cid:124)
wiτiw
i

V−

1m =

i
wiτi ˜yi

(cid:88)

i

1

V−
cav,n = V−
1mcav,n = V−

1 − αwnτnw(cid:124)
n
1m − αwnτngn

Vcav,n−

Vcav,n = V +

(cid:124)
Vwnατnw
nV
(cid:124)
nατnVwn
1 − w

.

1
Note that τi := v−
i

. Using eq. (32) and eq. (121) gives,

Using eq. (33) and eq. (121) gives,

log det(Vcav,n) = log det(V) − log(1 − w(cid:124)

nατnVwn).

Subsituting eq. (123) and eq. (124) back to eq. (114) results in,

n
G(q\
∗

(u)) =

log(2π) +

1
2
log(1 − w(cid:124)

log det(V) +

nατnVwn) +

1
m(cid:124)V−
1m
2
(cid:124)
m(cid:124)wnατnw
nm
(cid:124)
1 − w
nατnVwn

1
2

M
2
1
2
1
2

−

+

gnατnw(cid:124)

nVcav,nwnατngn − gnατnw(cid:124)

nVcav,nV−

1m

(125)

We now plug the above result back into the approximate marginal likelihood, yeilding,

F =

log |V| +

m(cid:124)V−

1m −

log |Kuu| +

1
2

(cid:20)

(cid:88)

+

−

1
2α

log(1 − w(cid:124)

nατnVwn) +

(cid:88)

1
α

log Ztilted,n

n
(cid:124)
m(cid:124)wnτnw
nm
(cid:124)
nατnVwn
1 − w

1
2

(cid:21)

1
2

n

+

1
2

gnτnw(cid:124)

nVcav,nwnατngn − gnτnw(cid:124)

nVcav,nV−

1m

(126)

1
2

37

Bui, Yan and Turner

1
2

1
2

G.1 Regression

We have shown in the previous section that the ﬁxed point solution of the Power-EP it-
n = dn =
erations can be obtained analytically for the regression case, gn = yn and τ −
uuKufn) + σ2
1
α(Kfnfn − KfnuK−
y. Crucially, we can obtain a closed form expression for
log Ztilted,n,

1

log Ztilted,n = −

log(2πσ2

y) +

log(σ2

y) −

log(αvn + σ2

y) −

1
2

1
2

(yn − µn)2
vn + σ2
y/α

(127)

α
2

σ2
y
where µn = w
α + w
−
therefore simplify the approximate marginal likelihood F further,

1m − wnατnyn) and vn =

(cid:124)
nmcav = w

(cid:124)
nVcav(V−

dn

(cid:124)
nVcavwn. We can

F =

log |V| +

m(cid:124)V−

1m −

log |Kuu| +

= −

log(2π) −

log |D + Qﬀ | −

yT (D + Qﬀ )−

1y −

(cid:20)

(cid:88)

n

−

1
2

log(2πσ2

y) +

log σ2

y −

log dn −

1
2α

1
2α

(cid:21)

y2
n
2dn

1 − α
2α

(cid:88)

n

log(

),

dn
σ2
y

(128)

1
2

N
2

1
2

1
2

1

where Qﬀ = KfuK−

uuKuf and D is a diagonal matrix, Dnn = dn.

When α = 1, the approximate marginal likelihood takes the same form as the FITC

1
2

1
2

marginal likelihood,

F = −

1
2
uuKufn + σ2
1
where Dnn = dn = Kfnfn − KfnuK−
y.
When α tends to 0, we have,

log(2π) −

N
2

1
2

log |D + Qﬀ | −

yT (D + Qﬀ )−

1y

(129)

1 − α
2α

(cid:88)

n

lim
0
α
→

log(

) =

dn
σ2
y

1
2

(cid:88)

lim
0
α
→

n

log(1 + α gn
σ2
y

)

α

=

(cid:80)

n hn
2σ2
y

,

(130)

1
where hn = Kfnfn − KfnuK−

uuKufn. Therefore,

F = −

log(2π) −

log |σ2

yI + Qﬀ | −

yT (σ2

yI + Qﬀ )−

1y −

(131)

1
2

(cid:80)

n hn
2σ2
y

,

N
2

which is the variational lower bound of Titsias (Titsias, 2009).

G.2 Classiﬁcation

In contrast to the regression case, the approximate marginal likelihood for classiﬁcation
cannot be simpliﬁed due to the non-Gaussian likelihood. Speciﬁcally, log Ztilted,n is not
analytically tractable, except when α = 1 and the classiﬁcation link function is the Gaus-
sian CDF. However, this quantity can be evaluated numerically, using sampling or Gauss-
Hermite quadrature, since it only involves a one-dimensional integral.

We now consider the case when α tends to 0 and verify that in such case the approxi-
mate marginal likelihood becomes the variational lower bound. We ﬁrst ﬁnd the limits of
individual terms in eq. (126):

−

1
2α

lim
0
α
→

log(1 − w(cid:124)

nατnVwn) =

w(cid:124)

nτnVwn

1
2

(132)

38

Unifying Gaussian Process Approximations

m(cid:124)wnτnw(cid:124)

nm

1
2
gnτnw(cid:124)

1
2

(cid:124)
m(cid:124)wnτnw
nm
(cid:124)
nατnVwn
1 − w

nVcav,nwnατngn

=

1
2

= 0

(cid:12)
(cid:12)
(cid:12)
(cid:12)α=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)α=0
(cid:12)
(cid:12)
1m
(cid:12)
(cid:12)α=0

−gnτnw(cid:124)

nVcav,nV−

= −gnτnw(cid:124)

nm.

We turn our attention to log Ztilted,n. First, we expand pα(yn|fn) using eq. (35):
pα(yn|fn) = exp(α log p(yn|fn))

= 1 + α log p(yn|fn) + ξ(α2).

Substituting this result back into log Ztilted/α gives,
(cid:90)

log Ztilted =

log

p(fn|u)qcav(u)pα(yn|fn)dfndu

1
α

(cid:90)

(cid:20)

log

p(fn|u)qcav(u)[1 + α log p(yn|fn) + ξ(α2)]dfndu

log

1 + α

(cid:90)

(cid:21)
p(fn|u)qcav(u) log p(yn|fn)dfndu + α2ξ(1)

(cid:20)

(cid:90)

α

(cid:21)
p(fn|u)qcav(u) log p(yn|fn)dfndu + α2ξ(1)

p(fn|u)qcav(u) log p(yn|fn)dfndu + αξ(1).

(133)

(134)

(135)

(136)

(137)

(138)

(139)

(140)

(141)

(142)

Therefore,

1
2

+

1
2

+

1
2

1
α

lim
0
α
→

(cid:90)

log Ztilted =

p(fn|u)q(u) log p(yn|fn)dfndu.

(143)

Putting these results into eq. (126), we obtain,

F =

log |V| +

m(cid:124)V−

1m −

(cid:88)

n

1
2

w(cid:124)

nτnVwn +

1
2

log |Kuu|

1
2
m(cid:124)wnτnw(cid:124)

nm − gnτnw(cid:124)

nm +

(cid:90)

p(fn|u)q(u) log p(yn|fn)dfndu

=

log |V| +

m(cid:124)V−

log |Kuu| +

m(cid:124)

(V−

1 − K−

1

uu)m − m(cid:124)V−

1m

(cid:88)

n

1
2

w(cid:124)

nτnVwn +

p(fn|u)q(u) log p(yn|fn)dfndu

=

log |V| −

m(cid:124)K−
1

uum −

log |Kuu| +

w(cid:124)

nτnVwn +

p(fn|u)q(u) log p(yn|fn)dfndu.

(cid:90)

(cid:88)

n

(144)

1m −
(cid:90)

1
2

1
2

We now write down the evidence lower bound of the global variational approach of

Titsias (Titsias, 2009), as applied to the classiﬁcation case (Hensman et al., 2015),

FVFE = −KL(q(u)||p(u)) +

p(fn|u)q(u) log p(yn|fn)dfndu

(145)

1
2

(cid:88)

n

1
2

(cid:90)

(cid:88)

n

39

1
α
1
α
1
α
1
α
(cid:90)

=

=

=

=

1
2

1
2

1
2

Bui, Yan and Turner

where

−KL(q(u)||p(u)) = −

trace(K−

uuV) −

1

m(cid:124)K−
1

uum +

log |Kuu| +

log |V|

1
2
1
2

1
2

1
2
(cid:88)

n

−

1
2
m(cid:124)K−

M
2
1
2

= −

trace([V−

1 −

wnτnwn]V) −

1

uum +

M
2

1
2

log |Kuu| +

log |V|

1
2

=

trace(

wnτnwnV) −

log |Kuu| +

log |V|. (146)

(cid:88)

n

m(cid:124)K−
1

uum −

1
2

1
2

1
2

−

1
2

Therefore, FVFE is identical to the limit of the approximate marginal likelihood provided
by power-EP as shown in eq. (144).

Appendix H. The surrogate regression viewpoint

It was written in the main text that it is instructive to view the approximation using
pseudo-points as forming a surrogate exact Gaussian process regression problem such that
the posterior and the marginal likelihood of this surrogate problem are close to that of the
original intractable regression/classiﬁcation problem. This approximation view is useful and
could potentially be used for other intractable probabilistic model, despite that we have not
used this view in the practical implementation of the algorithms/PEP procedure discussed
in this paper. In this section, we detail the surrogate model and how the parameters of
this model can be tuned to match the approximate posterior and approximate marginal
likelihood.

We consider the exact GP regression problem with M surrogate observations ˜y that are
formed by linear combininations the pseudo-outputs and additive surrogate Gaussian noise,
˜y = ˜Wu + ˜Σ1/2(cid:15). The exact posterior and log marginal likelihood can be obtained for this
model as follows,

1(u; ˜W ˜Σ−

log(2π) −

log p(˜y) = −

˜p(u|y) = N −
M
2
1
˜y(cid:124) ˜Σ−
2

−

1
1 ˜y, K−
1
2
1
˜y(cid:124) ˜Σ−
2

uu + ˜W(cid:124) ˜Σ−

1 ˜W)
uu + ˜W(cid:124) ˜Σ−

1

(log |K−

1 ˜W| + log |K−

uu| + log | ˜Σ|)
1

1 ˜y −

1 ˜W(K−
1

uu + ˜W(cid:124) ˜Σ−

1 ˜W)−

1 ˜W(cid:124) ˜Σ−

1 ˜y,

(148)

where we have used the matrix inversion lemma and the matrix determinant lemma in the
1 denotes the Gaussian distribution with natural parameters.
equations above, and that N −
The aim is to show that we can use the above quantities is to match a given approximate
1) and an approximate marginal likelihood F, that is,
posterior q(u) = N −
˜p(u|y) = q(u) and log p(˜y) = F. Substituting the above results into the constraints leading
to the following simpliﬁed constraints:

1(u; S−

1m, S−

where c is a constant. Assume that R is invertible, we can simpliﬁed the above results
further,

1 ˜y = m
1 ˜W = R = K−
1

˜W ˜Σ−
˜W(cid:124) ˜Σ−
1 ˜y + log | ˜Σ| = c,

˜y(cid:124) ˜Σ−

1
uu − S−

˜Σ−

1/2 ˜y = R−

1/2m

40

(147)

(149)

(150)

(151)

(152)

Unifying Gaussian Process Approximations

˜Σ−

1/2 ˜W = R(cid:124)/2
log | ˜Σ| = d,

(153)

(154)

where d is a constant. We can choose ˜Σ, e.g. a diagonal matrix, that satisﬁes the third
equality above. Given ˜Σ, obtaining ˜y and ˜W from the ﬁrst two equalities is trivial.

Appendix I. Extra experimental results

I.1 Comparison between various α values on a toy regression problem

41

4
2

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 8: Results on a toy regression problem: Negative log-marginal likelihood, mean squared error and mean log-loss on the
test set for full Gaussian process regression on synthetic datasets with true hyper-parameters and hyper-parameters obtained
by type-2 ML. Each dot is one trial, i.e. one synthetic dataset. The results demonstrate that type-2 maximum likelihood on
hyper-parameters works well, despite being a little conﬁdent on the log-marginal likelihood on the train set.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

4
3

Figure 9: Results on a toy regression problem with 500 training points: Mean squared error and log-likelihood on train and
test sets on synthetic datasets with hyper-parameters obtained by type-2 ML. In this example, the test error is higher than the
training error, as measured by the mean squared error, because the test points and training points are relatively far apart, making
the prediction task on the training set easier (interpolation) than on the test set (extrapolation). This is consistent with the
results with more training points, shown in ﬁg. 10.

4
4

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 10: Results on a toy regression problem with 1000 training points: Mean squared error and log-likelihood on train and
test sets on synthetic datasets with hyper-parameters obtained by type-2 ML. See ﬁg. 9 for a discussion.

4
5

Figure 11: Results on a toy regression problem: Standardsised mean log-loss on the test set for various values of α and various
number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgure shows the mean for various
α, and the results using GP regression.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

4
6

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 12: Results on a toy regression problem: Standardsised mean squared error on the test set for various values of α and
various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgure shows the mean for
various α, and the results using GP regression.

4
7

Figure 13: Results on a toy regression problem: The negative log marginal likelihood of the training set after training for various
values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgure shows
the mean for various α, and the results using GP regression. Power EP with α close to 1 over-estimates the marginal-likelihood.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

I.2 Real-world regression

We include the details of the regression datasets in table 1 and several comparisons of α
values in ﬁgs. 17 to 22.

Bui, Yan and Turner

Dataset N train/test D

boston
concrete
energy
kin8nm
naval
yacht
power
red wine

455/51
927/103
691/77
7373/819
10741/1193
277/31
8611/957
1439/160

14
9
9
9
18
7
5
12

Table 1: Regression datasets

48

4
9

Figure 14: A comparison between Power-EP with α = 0.5 and VFE on several regression datasets, on two metrics SMSE (top
two rows) and SMLL (bottom two rows). The scatter plots show the performance of Power-EP (α = 0.5) vs VFE. Each point is
one split and points with lighter colours are runs with big M. Points that stay below the diagonal line show α = 0.5 is better than
VFE. The plots right underneat the scatter plots show the histogram of the diﬀerence between methods. Red means α = 0.5 is
better than VFE.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

5
0

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 15: A comparison between EP and VFE on several regression datasets, on two metrics SMSE (top two rows) and SMLL
(bottom two rows). See ﬁg. 14 for more details about the plots.

5
1

Figure 16: A comparison between Power-EP with α = 0.5 and EP on several regression datasets, on two metrics SMSE (top two
rows) and SMLL (bottom two rows). See ﬁg. 14 for more details about the plots.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

5
2

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 17: Results on real-world regression problems: Negative training log-marginal likelihood for diﬀerent datasets, various
values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgures
show the mean for various α for comparison. Lower is better [however, lower could mean overestimation].

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

5
3

Figure 18: Results on real-world regression problems: Negative training log-marginal likelihood for diﬀerent datasets, various
values of α and various number of pseudo-points M , averaged over 20 splits. Lower is better [however, lower could mean
overestimation].

5
4

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 19: Results on real-world regression problems: Standardised mean squared error on the test set for diﬀerent datasets,
various values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost
ﬁgures show the mean for various α for comparison. Lower is better.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

5
5

Figure 20: Results on real-world regression problems: Standardised mean squared error on the test set for diﬀerent datasets,
various values of α and various number of pseudo-points M , averaged over 20 splits. Lower is better.

5
6

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 21: Results on real-world regression problems: Standardised mean log loss on the test set for diﬀerent datasets, various
values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgures
show the mean for various α for comparison. Lower is better.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

5
7

Figure 22: Results on real-world regression problems: Standardised mean log loss on the test set for diﬀerent datasets, various
values of α and various number of pseudo-points M , averaged over 20 splits. Lower is better.

Bui, Yan and Turner

I.3 Real-world classiﬁcation

It was demonstrated in (Hern´andez-Lobato and Hern´andez-Lobato, 2016; Hensman et al.,
2015) that, once optimised, the pseudo points tend to concentrate around the decision
boundary for VFE, and spread out to cover the data region in EP. Figure 23 illustrates the
same eﬀect as α goes from close to 0 (VFE) to 1 (EP).

Figure 23: The locations of pseudo data points vary with α.. Best viewed in colour.

We include the details of the classiﬁcation datasets in table 2 and several comparisons

of α values in ﬁgs. 27 to 30.

58

Unifying Gaussian Process Approximations

Dataset

N train/test D N positive/negative

australian
breast
crabs
iono
pima
sonar

621/69
614/68
180/20
315/35
690/77
186/21

15
11
7
35
9
61

222/468
239/443
100/100
126/224
500/267
111/96

Table 2: Classiﬁcation datasets

59

6
0

Figure 24: A comparison between Power-EP with α = 0.5 and VFE on several classiﬁcation datasets, on two metrics: classiﬁcation
error (top two rows) and NLL (bottom two rows). See ﬁg. 14 for more details about the plots.

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

6
1

Figure 25: A comparison between EP and VFE on several classiﬁcation datasets, on two metrics: classiﬁcation error (top two
rows) and NLL (bottom two rows). See ﬁg. 14 for more details about the plots.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

6
2

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 26: A comparison between Power-EP with α = 0.5 and EP on several classiﬁcation datasets, on two metrics: classiﬁcation
error (top two rows) and NLL (bottom two rows). See ﬁg. 14 for more details about the plots.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

6
3

Figure 27: Results on real-world classiﬁcation problems: Classiﬁcation error rate on the test set for diﬀerent datasets, various
values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost ﬁgures
show the mean for various α for comparison. Lower is better.

6
4

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 28: Results on real-world classiﬁcation problems: Classiﬁcation error rate on the test set for diﬀerent datasets, various
values of α and various number of pseudo-points M , averaged over 20 splits. Lower is better.

i

U
n
i
f
y
n
g
G
a
u
s
s
i
a
n

P
r
o
c
e
s
s
A
p
p
r
o
x
m
a
t
i
o
n
s

i

6
5

Figure 29: Results on real-world classiﬁcation problems: Average negative log-likelihood on the test set for diﬀerent datasets,
various values of α and various number of pseudo-points M . Each trace is for one split, bold line is the mean. The rightmost
ﬁgures show the mean for various α for comparison. Lower is better.

6
6

B
u
i
,

Y
a
n

a
n
d
T
u
r
n
e
r

Figure 30: Results on real-world classiﬁcation problems: Average negative log-likelihood on the test set for diﬀerent datasets,
various values of α and various number of pseudo-points M , averaged over 20 splits. Lower is better.

Unifying Gaussian Process Approximations

I.4 Binary classiﬁcation on even/odd MNIST digits

Figure 31: The test error and log-likelihood of the MNIST binary classiﬁcation task
(M=100).

Figure 32: The test error and log-likelihood of the MNIST binary classiﬁcation task
(M=200).

I.5 When M = N and α = 1, do we recover EP for GPC (Rasmussen and

Williams, 2005, sec. 3.6)?

The key diﬀerence between the EP method in this manuscript when M = N and the pseudo-
inputs and the training inputs are identical, and the standard EP method as described by
(Rasmussen and Williams, 2005, sec. 3.6) is the factor representation. While Rasmussen and
Williams (2005) used a one dimensional un-normalised Gaussian distribution that touches
only one function value fn to approximate each exact factor, the approximate factor used
in the EP scheme described in the main text touches all M pseudo-points, hence all N
function values when the pseudo-inputs are placed at the training inputs. However, in
practice both methods give virtually identical results. Figure 33 shows the approximate log
marginal likelihood and the negative test log-likelihood, given by running the EP procedure

67

Bui, Yan and Turner

described in the main text on the ionosphere dataset. We note that these results are
similar to that of the standard EP method (see Kuss and Rasmussen, 2005).

Figure 33: EP energy on the train set [TOP] and the average negative log-likelihood on the
test set[BOTTOM] when M = N .

68

Unifying Gaussian Process Approximations

References

Mauricio A. ´Alvarez, David Luengo, Michalis K. Titsias, and Neil D. Lawrence. Eﬃcient
multioutput Gaussian processes through variational inducing kernels. In 13th Interna-
tional Conference on Artiﬁcial Intelligence and Statistics, pages 25–32, 2010.

Matthias Bauer, Mark van der Wilk, and Carl E. Rasmussen. Understanding probabilistic
sparse Gaussian process approximations. In Advances in Neural Information Processing
Systems 29, pages 1525–1533, 2016.

Lawrence D. Brown. Fundamentals of Statistical Exponential Families with Applications in
Statistical Decision Theory. Institute of Mathematical Statistics, Hayward, CA, 1986.

Thang D. Bui and Richard E. Turner. Tree-structured Gaussian process approximations.

In Advances in Neural Information Processing Systems 27, pages 2213–2221, 2014.

Thang D. Bui, Cuong V. Nguyen, and Richard E. Turner. Streaming sparse Gaussian
process approximations. In Advances in Neural Information Processing Systems 30, 2017.

Lehel Csat´o. Gaussian Processes — Iterative Sparse Approximations. PhD thesis, Aston

University, 2002.

14(3):641–669, 2002.

Lehel Csat´o and Manfred Opper. Sparse online Gaussian processes. Neural Computation,

Michael R.W. Dawson. Understanding cognitive science. Blackwell Publishing, 1998.

Marc P. Deisenroth. Eﬃcient Reinforcement Learning using Gaussian Processes. PhD

thesis, Karlsruhe Institute of Technology, Karlsruhe, Germany, 2010.

Stefan Depeweg, Jos´e Miguel Hern´andez-Lobato, Finale Doshi-Velez, and Steﬀen Udluft.
Learning and policy search in stochastic dynamical systems with Bayesian neural net-
works. In 4th International Conference on Learning Representations, 2016.

Amir Dezfouli and Edwin V. Bonilla. Scalable inference for Gaussian process models with
black-box likelihoods. In Advances in Neural Information Processing Systems 28, pages
1414–1422, 2015.

Anibal Figueiras-Vidal and Miguel L´azaro-Gredilla. Inter-domain Gaussian processes for
sparse inference using inducing features. In Advances in Neural Information Processing
Systems 22, pages 1087–1095, 2009.

Roger Frigola, Yutian Chen, and Carl E. Rasmussen. Variational Gaussian process state-
space models. In Advances in Neural Information Processing Systems 27, pages 3680–
3688, 2014.

Andrew Gelman, Aki Vehtari, Pasi Jyl¨anki, Christian Robert, Nicolas Chopin, and John P
Cunningham. Expectation propagation as a way of life. arXiv preprint arXiv:1412.4869,
2014.

69

Bui, Yan and Turner

James Hensman, Nicolo Fusi, and Neil D. Lawrence. Gaussian processes for big data. In

29th Conference on Uncertainty in Artiﬁcial Intellegence, pages 282–290, 2013.

James Hensman, Alexander G. D. G. Matthews, and Zoubin Ghahramani. Scalable vari-
In 18th International Conference on Artiﬁcial

ational Gaussian process classiﬁcation.
Intelligence and Statistics, pages 351–360, 2015.

Daniel Hern´andez-Lobato and Jos´e Miguel Hern´andez-Lobato. Scalable Gaussian process
classiﬁcation via expectation propagation. In 19th International Conference on Artiﬁcial
Intelligence and Statistics, pages 168–176, 2016.

Jos´e Miguel Hern´andez-Lobato, Yingzhen Li, Mark Rowland, Daniel Hern´andez-Lobato,
In 33rd
Thang D Bui, and Richard E Turner. Black-box α-divergence minimization.
International Conference on International Conference on Machine Learning, pages 1511–
1520, 2016.

Trong Nghia Hoang, Quang Minh Hoang, and Bryan Kian Hsiang Low. A distributed
variational inference framework for unifying parallel sparse Gaussian process regression
models. In 33rd International Conference on Machine Learning, pages 382–391, 2016.

Neil Houlsby, Ferenc Husz´ar, Zoubin Ghahramani, and M´at´e Lengyel. Bayesian active
learning for classiﬁcation and preference learning. arXiv preprint arXiv:1112.5745, 2011.

Kazufumi Ito and Kaiqi Xiong. Gaussian ﬁlters for nonlinear ﬁltering problems.

IEEE

Transactions on Automatic Control, 45(5):910–927, 2000.

Diederik P. Kingma and Jimmy Ba. Adam: a method for stochastic optimization. In 3rd

International Conference on Learning Representations, 2015.

H. J. Kushner and A. S. Budhiraja. A nonlinear ﬁltering algorithm based on an approxi-
mation of the conditional distribution. IEEE Transactions on Automatic Control, 45(3):
580–585, Mar 2000.

Malte Kuss and Carl E. Rasmussen. Assessing approximate inference for binary Gaussian
process classiﬁcation. The Journal of Machine Learning Research, 6:1679–1704, 2005.

Neil D. Lawrence. Probabilistic non-linear principal component analysis with Gaussian
process latent variable models. The Journal of Machine Learning Research, 6:1783–1816,
2005.

Yingzhen Li, Jos´e Miguel Hern´andez-Lobato, and Richard E. Turner. Stochastic expectation
propagation. In Advances in Neural Information Processing Systems 29, pages 2323–2331,
2015.

Kian Hsiang Low, Jiangbo Yu, Jie Chen, and Patrick Jaillet. Parallel Gaussian process
regression for big data: Low-rank representation meets Markov approximation. In 29th
AAAI Conference on Artiﬁcial Intelligence, pages 2821–2827, 2015.

Maren Mahsereci and Philipp Hennig. Probabilistic line searches for stochastic optimization.

In Advances in Neural Information Processing Systems 28, pages 181–189, 2015.

70

Unifying Gaussian Process Approximations

Alexander G. D. G. Matthews, James Hensman, Richard E Turner, and Zoubin Ghahra-
mani. On sparse variational methods and the Kullback-Leibler divergence between
In 19th International Conference on Artiﬁcial Intelligence and
stochastic processes.
Statistics, pages 231–239, 2016.

Andrew McHutchon. Nonlinear modelling and control using Gaussian processes. PhD thesis,

University of Cambridge, 2014.

Thomas P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis,

Massachusetts Institute of Technology, 2001.

Thomas P. Minka. Power EP. Technical report, Microsoft Research Cambridge, 2004.

Thomas P. Minka. Divergence measures and message passing. Technical report, Microsoft

Research Cambridge, 2005.

Andrew Naish-Guzman and Sean B. Holden. The generalized FITC approximation.
Advances in Neural Information Processing Systems 20, pages 1057–1064, 2007.

In

Hannes Nickisch and Carl E. Rasmussen. Approximations for binary Gaussian process

classiﬁcation. The Journal of Machine Learning Research, 9(Oct):2035–2078, 2008.

Juho Piironen and Aki Vehtari. Comparison of Bayesian predictive methods for model

selection. Statistics and Computing, 27(3):711–735, 2017.

Yuan Qi, Ahmed H. Abdel-Gawad, and Thomas P. Minka. Sparse-posterior Gaussian pro-
cesses for general likelihoods. In 26th Conference on Uncertainty in Artiﬁcial Intelligence,
pages 450–457, 2010.

Joaquin Qui˜nonero-Candela and Carl E. Rasmussen. A unifying view of sparse approximate
Gaussian process regression. The Journal of Machine Learning Research, 6:1939–1959,
2005.

Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learn-

ing. The MIT Press, 2005.

Jaakko Riihim¨aki, Pasi Jyl¨anki, and Aki Vehtari. Nested expectation propagation for Gaus-
sian process classiﬁcation with a multinomial probit likelihood. The Journal of Machine
Learning Research, 14(Jan):75–109, 2013.

Alan D. Saul, James Hensman, Aki Vehtari, and Neil D. Lawrence. Chained Gaussian
processes. In 19th International Conference on Artiﬁcial Intelligence and Statistics, pages
1431–1440, 2016.

Anton Schwaighofer and Volker Tresp. Transductive and inductive methods for approximate
Gaussian process regression. In Advances in Neural Information Processing Systems 15,
pages 953–960, 2002.

Matthias Seeger. Bayesian inference and optimal design for the sparse linear model. The

Journal of Machine Learning Research, 9(Apr):759–813, 2008.

71

Bui, Yan and Turner

Matthias Seeger and Michael I. Jordan. Sparse Gaussian process classiﬁcation with multiple
classes. Technical report, Department of Statistics, University of Berkeley, CA, 2004.

Matthias Seeger, Christopher Williams, and Neil D. Lawrence. Fast forward selection to
speed up sparse Gaussian process regression. In 9th International Conference on Artiﬁcial
Intelligence and Statistics, 2003.

Edward Snelson. Flexible and eﬃcient Gaussian process models for machine learning. PhD

thesis, University College London, 2007.

Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs.

In Advances in Neural Information Processing Systems 19, pages 1257–1264, 2006.

Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian optimization of
machine learning algorithms. In Advances in Neural Information Processing Systems 25,
pages 2951–2959, 2012.

Michalis K. Titsias. Variational learning of inducing variables in sparse Gaussian processes.
In 12th International Conference on Artiﬁcial Intelligence and Statistics, pages 567–574,
2009.

Michalis K. Titsias and Neil D. Lawrence. Bayesian Gaussian process latent variable model.
In 13th International Conference on Artiﬁcial Intelligence and Statistics, pages 844–851,
2010.

Felipe Tobar, Thang D. Bui, and Richard E. Turner. Learning stationary time series us-
ing Gaussian processes with nonparametric kernels. In Advances in Neural Information
Processing Systems 29, pages 3501–3509, 2015.

Richard E. Turner and Maneesh Sahani. Two problems with variational expectation max-
imisation for time-series models.
In D. Barber, T. Cemgil, and S. Chiappa, editors,
Bayesian Time series models, chapter 5, pages 109–130. Cambridge University Press,
2011.

Jack M. Wang, David J. Fleet, and Aaron Hertzmann. Gaussian process dynamical models.

In Advances in Neural Information Processing Systems 18, pages 1441–1448, 2005.

Minjie Xu, Balaji Lakshminarayanan, Yee Whye Teh, Jun Zhu, and Bo Zhang. Distributed
In Advances in Neural Information

Bayesian posterior sampling via moment sharing.
Processing Systems 27, pages 3356–3364, 2014.

Huaiyu Zhu and Richard Rohwer. Information geometric measurements of generalisation.

Technical report, Aston University, 1995.

Huaiyu Zhu and Richard Rohwer. Measurements of generalisation based on information

geometry. In Mathematics of Neural Networks, pages 394–398. 1997.

72

