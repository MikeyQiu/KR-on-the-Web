Conservative Bandits

Yifan Wu
ywu12@ualberta.ca

Roshan Shariﬀ
rshariff@ualberta.ca

Tor Lattimore
tlattimo@ualberta.ca

Csaba Szepesv´ari
szepesva@ualberta.ca

Department of Computing Science, University of Alberta

6
1
0
2
 
b
e
F
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
2
8
2
4
0
.
2
0
6
1
:
v
i
X
r
a

Abstract

We study a novel multi-armed bandit problem
that models the challenge faced by a company
wishing to explore new strategies to maximize
revenue whilst simultaneously maintaining their
revenue above a ﬁxed baseline, uniformly over
time. While previous work addressed the prob-
lem under the weaker requirement of maintain-
ing the revenue constraint only at a given ﬁxed
time in the future, the algorithms previously pro-
posed are unsuitable due to their design under
the more stringent constraints. We consider both
the stochastic and the adversarial settings, where
we propose, natural, yet novel strategies and an-
alyze the price for maintaining the constraints.
Amongst other things, we prove both high proba-
bility and expectation bounds on the regret, while
we also consider both the problem of maintain-
ing the constraints with high probability or ex-
pectation. For the adversarial setting the price of
maintaining the constraint appears to be higher,
at least for the algorithm considered. A lower
bound is given showing that the algorithm for the
stochastic setting is almost optimal. Empirical
results obtained in synthetic environments com-
plement our theoretical ﬁndings.

1. Introduction

The manager of Zonlex, a ﬁctional company, has just
learned about bandit algorithms and is very excited about
the opportunity to use this advanced technology to max-
imize Zonlex’s revenue by optimizing the content on the
landing page of the company’s website. Every click on the
content of their website pays a small reward; thanks to the
high traﬃc that Zonlex’s website enjoys, this translates into
a decent revenue stream. Currently, Zonlex chooses the
website’s contents using a strategy designed over the years
by its best engineers, but the manager suspects that some

1

alternative strategies could potentially extract signiﬁcantly
more revenue. The manager is willing to explore bandit
algorithms to identify the winning strategy. The manager’s
problem is that Zonlex cannot aﬀord to lose more than 10%
of its current revenue during its day-to-day operations and
at any given point in time, as Zonlex needs a lot of cash
to support its operations. The manager is aware that stan-
dard bandit algorithms experiment “wildly”, at least ini-
tially, and as such may initially lose too much revenue and
jeopardize the company’s stable operations. As a result, the
manager is afraid of deploying cutting-edge bandit meth-
ods, but notes that this just seems to be a chicken-and-egg
problem: a learning algorithm cannot explore due to the
potential high loss, whereas it must explore to be good in
the long run.

The problem described in the previous paragraph is ubiq-
uitous. It is present, for example, when attempting to learn
better human-computer interaction strategies, say in dia-
logue systems or educational games. In these cases a de-
signer may feel that experimenting with sub-par interaction
strategies could cause more harm than good (e.g., Rieser
and Lemon, 2008; Liu et al., 2014). Similarly, optimizing
a production process in a factory via learning (and experi-
mentation) has much potential (e.g., Gabel and Riedmiller,
2011), but deviating too much from established “best prac-
tices” will often be considered too dangerous. For exam-
ples from other domains see the survey paper of Garc´ıa and
Fern´andez (2015).

Staying with Zonlex, the manager also knows that the stan-
dard practice in today’s internet companies is to employ
A/B testing on an appropriately small percentage of the
traﬃc for some period of time (e.g., 10% in the case of
Zonlex). The manager even thinks that perhaps a best-arm
identiﬁcation strategy from the bandit literature, such as
the recent lil’UCB method of Jamieson et al. (2014), could
be more suitable. While this is appealing, identifying the
best possible option may need too much time even with a
good learning algorithm (e.g., this happens when the dif-
ference in payoﬀ between the best and second best strate-

Conservative Bandits

gies is small). One can of course stop earlier, but then the
potential for improvement is wasted: when to stop then
becomes a delicate question on its own. As Zonlex only
plans for the next ﬁve years anyway, they could adopt the
more principled yet quite simple approach of ﬁrst using
their default favorite strategy until enough payoﬀ is col-
lected, so that in the time remaining of the ﬁve years the
return-constraint is guaranteed to hold regardless of the fu-
ture payoﬀs. While this is a solution, the manager suspects
that other approaches may exist. One such potential ap-
proach is to discourage a given bandit algorithm from ex-
ploring the alternative options, while in some way encour-
aging its willingness to use the default option. In fact, this
approach has been studied recently by Lattimore (2015a)
(in a slightly more general setting than ours). However,
the algorithm of Lattimore (2015a) cannot be guaranteed to
maintain the return constraint uniformly in time. It is thus
unsuitable for the conservative manager of Zonlex; a mod-
iﬁcation of the algorithm could possibly meet this stronger
requirement, but it appears that this will substantially in-
crease the worst-case regret.

In this paper we ask whether better approaches than the
above naive one exist in the context of multi-armed ban-
dits, and whether the existing approaches can achieve the
best possible regret given the uniform constraint on the to-
tal return. In particular, our contributions are as follows:
(i) Starting from multi-armed bandits, we ﬁrst formulate
what we call the family of “conservative bandit problems”.
As expected in these problems, the goal is to design learn-
ing algorithms that minimize regret under the additional
constraint that at any given point in time, the total reward
(return) must stay above a ﬁxed percentage of the return of
a ﬁxed default arm, i.e., the return constraint must hold uni-
formly in time. The variants diﬀer in terms of how stringent
the constraint is (i.e., should the constraint hold in expecta-
tion, or with high probability?), whether the bandit prob-
lem is stochastic or adversarial, and whether the default
arm’s payoﬀ is known before learning starts. (ii) We an-
alyze the naive build-budget-then-learn strategy described
above (which we call BudgetFirst) and design a signiﬁ-
cantly better alternative for stochastic bandits that switches
between using the default arm and learning (using a ver-
sion of UCB, a simple yet eﬀective bandit learning algo-
rithm: Agrawal, 1995; Katehakis and Robbins, 1995; Auer
et al., 2002) in a “smoother” fashion. (iii) We prove that the
new algorithm, which we call Conservative UCB, meets
the uniform return constraint (in various senses), while it
can achieve signiﬁcantly less regret than BudgetFirst. In
particular, while BudgetFirst is shown to pay a multiplica-
tive penalty in the regret for maintaining the return con-
straint, Conservative UCB only pays an additive penalty.
We provide both high probability and expectation bounds,
consider both high probability and expectation constraints

on the return, and also consider the case when the pay-
oﬀ of the default arm is initially unknown. (iv) We also
prove a lower bound on the best regret given the constraint
and as a result show that the additive penalty is unavoid-
able; thus Conservative UCB achieves the optimal regret in
a worst-case sense. While Unbalanced MOSS of Lattimore
(2015a), when specialized to our setting, also achieves the
optimal regret (as follows from the analysis of Lattimore,
2015a), as mentioned earlier it does not maintain the con-
straint uniformly in time (it will explore too much at the
beginning of time); it also relies heavily on the knowledge
of the mean payoﬀ of the default strategy. (v) We also con-
sider the adversarial setting where we design an algorithm
similar to Conservative UCB: the algorithm uses an under-
lying “base” adversarial bandit strategy when it ﬁnds that
the return so far is suﬃciently higher than the minimum re-
quired return. We prove that the resulting method indeed
maintains the return constraint uniformly in time and we
also prove a high-probability bound on its regret. We ﬁnd,
however, that the additive penalty in this case is higher than
in the stochastic case. Here, the Exp3-γ algorithm of Latti-
more (2015a) is an alternative, but again, this algorithm is
not able to maintain the return constraint uniformly in time.
(vi) The theoretical analysis is complemented by synthetic
experiments on simple bandit problems whose purpose is
to validate that the newly designed algorithm is reasonable
and to show that the algorithms’ behave as dictated by the
theory developed. We also compare our method to Unbal-
anced MOSS to provide a perspective to see how much is
lost due to maintaining the return constraint uniformly over
time. We also identify future work. In particular, we expect
our paper to inspire further works in related, more com-
plex online learning problems, such as contextual bandits,
or even reinforcement learning.

1.1. Previous Work

Our constraint is equivalent to a constraint on the regret
to a default strategy, or in the language of prediction-with-
expert-advice, or bandit literature, regret to a default action.
In the full information, mostly studied in the adversarial
setting, much work has been devoted to understanding the
price of such constraints (Hutter and Poland, 2005; Even-
Dar et al., 2008; Koolen, 2013; Sani et al., 2014, e.g.,). In
particular, Koolen (2013) studies the Pareto frontier of re-
gret vectors (which contains the non-dominated worst-case
regret vectors of all algorithms). The main lesson of these
works is that in the full information setting even a constant
regret to a ﬁxed default action can be maintained with es-
sentially no increase in the regret to the best action. The sit-
uation quickly deteriorates in the bandit setting as shown by
Lattimore (2015a). This is perhaps unsurprising given that,
as opposed to the full information setting, in the bandit set-
ting one needs to actively explore to get improved estimates

2

Conservative Bandits

of the actions’ payoﬀs. As mentioned earlier, Lattimore de-
scribes two learning algorithms relevant to our setting: In
the stochastic setting we consider, Unbalanced MOSS (and
its relative, Unbalanced UCB) are able to achieve a con-
stant regret penalty while maintaining the return constraint
while Exp3-γ achieves a much better regret as compared to
our strategy for the adversarial setting. However, neither of
these algorithms maintain the return constraint uniformly
in time. Neither will the constraint hold with high probabil-
ity. While Unbalanced UCB achieves problem-dependent
bounds, it has the same issues as Unbalanced MOSS with
maintaining the return constraint. Also, all these strategies
rely heavily on knowing the payoﬀ of the default action.

More broadly, the issue of staying safe while exploring
has long been recognized in reinforcement learning (RL).
Garc´ıa and Fern´andez (2015) provides a comprehensive
survey of the relevant literature. Lack of space prevents us
from including much of this review. However, the short
summary is that while the issue has been considered to
be important, no previous approach addresses the problem
from a theoretical angle. Also, while it has been recog-
nized that adding constraints on the return is one way to en-
sure safety, as far as we know, maintaining the constraints
during learning (as opposed to imposing them as a way of
restricting the set of feasible policies) has not been con-
sidered in this literature. Our work, while it considers a
much simpler setting, suggest a novel approach to address
the safe exploration problem in RL.

Another line of work considers safe exploration in the re-
lated context of optimization (Sui et al., 2015). However,
the techniques and the problem setting (e.g., objective) in
this work is substantially diﬀerent from ours.

2. Conservative Multi-Armed Bandits

The multi-armed bandit problem is a sequential decision-
making task in which a learning agent repeatedly chooses
an action (called an arm) and receives a reward correspond-
ing to that action. We assume there are K + 1 arms and
denote the arm chosen by the agent in round t ∈ {1, 2, . . .}
by It ∈ {0, . . . , K}. There is a reward Xt,i associated with
each arm i at each round t and the agent receives the re-
ward corresponding to its chosen arm, Xt,It . The agent does
not observe the other rewards Xt, j ( j (cid:44) It).

The learning performance of an agent over a time horizon
n is usually measured by its regret, which is the diﬀerence
between its reward and what it could have achieved by con-
sistently choosing the single best arm in hindsight:

Rn = max
i∈{0,...,K}

Xt,i − Xt,It .

n(cid:88)

t=1

An agent is failing to learn unless its regret grows sub-

(1)

3

linearly: Rn ∈ o(n); good agents achieve Rn ∈ O(
even Rn ∈ O(log n).
We also use the notation Ti(n) = (cid:80)n
(cid:49){It = i} for the num-
t=1
ber of times the agent chooses arm i in the ﬁrst n time steps.

n) or

√

2.1. Conservative Exploration

Let arm 0 correspond to the conservative default action
with the other arms 1, . . . , K being the alternatives to be
explored. We want to be able to choose some α > 0 and
constrain the learner to earn at least a 1 − α fraction of the
reward from simply playing arm 0:

t(cid:88)

s=1

t(cid:88)

s=1

Xs,Is ≥ (1 − α)

Xs,0

for all t ∈ {1, . . . , n}.

(2)

For the introductory example above α = 0.1, which corre-
sponds to losing at most 10% of the revenue compared to
the default website. It should be clear that small values of α
force the learner to be highly conservative, whereas larger
α correspond to a weaker constraint.

We introduce a quantity Zn, called the budget, which quan-
tiﬁes how close the constraint (2) is to being violated:

Zt =

Xs,Is − (1 − α)Xs,0;

(3)

t(cid:88)

s=1

the constraint is satisﬁed if and only if Zt ≥ 0 for all t ∈
{1, . . . , n}. Note that the constraints must hold uniformly in
time.

Our objective is to design algorithms that minimize the re-
gret (1) while simultaneously satisfying the constraint (2).
In the following sections, we will consider two variants of
multi-armed bandits: the stochastic setting in Section 3 and
the adversarial setting in Section 4. In each case we will
design algorithms that satisfy diﬀerent versions of the con-
straint and give regret guarantees.

One may wonder: what if we only care about Zn ≥ 0 in-
stead of Zt ≥ 0 for all t. Although our algorithms are de-
signed for satisfying the anytime constraint on Zt our lower
bound, which is based on Zn ≥ 0 only, shows that in the
stochastic setting we cannot improve the regret guarantee
even if we only want to satisfy the overall constraint Zn ≥ 0.

3. The Stochastic Setting

In the stochastic multi-armed bandit setting each arm i and
round t has a stochastic reward Xt,i = µi + ηt,i, where
µi ∈ [0, 1] is the expected reward of arm i and the ηt,i are
independent random noise variables that we assume have
1-subgaussian distributions. We denote the expected re-
ward of the optimal arm by µ∗ = maxi µi and the gap be-
tween it and the expected reward of the ith arm by ∆i =
µ∗ − µi.

Cumulative reward

Conservative Bandits

0t

D efa ult a ctio n : µ

Safe action

(cid:101)Zt > 0

(cid:101)Zt < 0

Unsafe action

d efa ult a ctio n

−

1

(

t :

a i n

(cid:101)Zt−1
Budget

0 t

µ

)

α

F ollo w in g

r

t

s

n

C o

t − 1

t

Rounds

Figure 1. Choosing the default arm increases the budget. Then it
is safe to explore a non-default arm if it cannot violate the con-
straint (i.e. make the budget negative).

The regret Rn is now a random variable. We can bound it in
expectation, of course, but we are often more interested in
high-probability bounds on the weaker notion of pseudo-
regret:

(cid:101)Rn = nµ∗ −

=

µIt

Ti(n)∆i,

(4)

n(cid:88)

t=1

K(cid:88)

i=0

in which the noise in the arms’ rewards is ignored and the
randomness arises from the agent’s choice of arm. The re-
gret Rn and the pseudo-regret (cid:101)Rn are equal in expectation.
High-probability bounds for the latter, however, can cap-
ture the risk of exploration without being dominated by the
variance in the arms’ rewards.
We use the notation ˆµi(n) = 1
(cid:49){It = i} Xt,i for the
Ti(n)
empirical mean of the rewards from arm i observed by the
If Ti(n) = 0 then we deﬁne
agent in the ﬁrst n rounds.
ˆµi(n) = 0. The algorithms for the stochastic setting will
estimate the µi by ˆµi and will construct and act based on
high-probability conﬁdence intervals for the estimates.

(cid:80)n

t=1

3.1. The Budget Constraint

Just as we substituted regret with pseudo-regret, in the
stochastic setting we will use the following form of the con-
straint (2):

The default arm is always safe to play because it increases
the budget by µ0−(1−α)µ0 = αµ0. The budget will decrease
for arms i with µi < (1 − α)µ0; the constraint (cid:101)Zn ≥ 0 is then
in danger of being violated (Fig. 1).

In the following sections we will construct algorithms that
satisfy pseudo-regret bounds and the budget constraint (5)
with high probability 1 − δ (where δ > 0 is a tunable pa-
rameter). In Section 3.4 we will see how these algorithms
can be adapted to satisfy the constraint in expectation and
with bounds on their expected regret.

For simplicity, we will initially assume that the algorithms
know µ0, the expected reward of the default arm. This is
reasonable in situations where the default action has been
used for a long time and is well-characterized. Even so, in
Section 3.5 we will see that having to learn an unknown µ0
is not a great hindrance.

3.2. BudgetFirst — A Naive Algorithm

Before presenting the new algorithm it is worth remarking
on the most obvious naive attempt, which we call the Bud-
getFirst algorithm. A straightforward modiﬁcation of UCB
leads to an algorithm that accepts a conﬁdence parameter
δ ∈ (0, 1) and suﬀers regret at most


(cid:115)

(cid:101)Rn = O

Kn log

= Rworst .

(7)



(cid:32)

log(n)
δ

(cid:33)


Of course this algorithm alone will not satisfy the con-
straint (5), but that can be enforced by naively modifying
the algorithm to deterministically choose It = 0 for the ﬁrst
t0 rounds where

(∀ t0 ≤ t ≤ n)

tµ0 − Rworst ≥ (1 − α)tµ0 .

Subsequently the algorithm plays the high probability ver-
sion of UCB and the regret guarantee (7) ensures the con-
straint (5) is satisﬁed with high probability. Solving the
equation above leads to t0 = ˜O(Rworst/αµ0), and since the
regret while choosing the default arm may be O(1) the
worst-case regret guarantee of this approach is
(cid:33)


log(n)
δ

(cid:101)Rn = Ω

Kn log

1
µ0α



(cid:115)



(cid:32)

.

This is signiﬁcantly worse than the more sophisticated al-
gorithm that is our main contribution and for which the
price of satisfying (5) is only an additive term rather than a
large multiplicative factor.

t(cid:88)

s=1

the budget then becomes

µIs ≥ (1 − α)µ0t

for all t ∈ {1, . . . , n};

(5)

3.3. Conservative UCB

(cid:101)Zt =

µIs − (1 − α)tµ0 .

(6)

t(cid:88)

s=1

A better strategy is to play the default arm only until the
budget (6) is large enough to start exploring other arms with
a low risk of violating the constraint. It is safe to keep ex-
ploring as long as the budget remains large, whereas if it

4

Conservative Bandits

A simple choice is

decreases too much then it must be replenished by play-
In other words, we intersperse the
ing the default arm.
exploration of a standard bandit algorithm with occasional
budget-building phases when required. We show that accu-
mulating a budget does not severely curtail exploration and
thus gives small regret.

Conservative UCB (Algorithm 1) is based on UCB with
the novel twist of maintaining a positive budget. In each
round, UCB calculates upper conﬁdence bounds for each
arm; let Jt be the arm that maximizes this calculated con-
ﬁdence bound. Before playing this arm (as UCB would)
our algorithm decides whether doing so risks the budget
becoming negative. Of course, it does not know the ac-
tual budget (cid:101)Zt because the µi (i (cid:44) 0) are unknown; instead,
it calculates a lower conﬁdence bound ξt based on conﬁ-
dence intervals for the µi. More precisely, it calculates a
lower conﬁdence bound for what the budget would be if it
played arm Jt. If this lower bound is positive then the con-
straint will not be violated as long as the conﬁdence bounds
hold. If so, the the algorithm chooses It = Jt just as UCB
would; otherwise it acts conservatively by choosing It = 0.

1: Input: K, µ0, δ, ψδ(·)
2: for t ∈ 1, 2, . . . do

(cid:46) Compute conﬁdence intervals. . .

θ0(t), λ0(t) ← µ0
for i ∈ 1, . . . , K do

(cid:112)

ψδ(Ti(t − 1))/Ti(t − 1)

∆i(t) ←
θi(t) ← ˆµi(t − 1) + ∆i(t)
λi(t) ← max {0, ˆµi(t − 1) − ∆i(t)}

(cid:46) . . . for known µ0,
(cid:46) . . . for other arms,

end for
Jt ← arg maxi θi(t)
(cid:46) Compute budget and. . .

(cid:46) . . . and ﬁnd UCB arm.

s=1 λIs (t) + λJt (t) − (1 − α)tµ0

(cid:46) . . . choose UCB arm if safe,

(cid:46) . . . default arm otherwise.

ξt ← (cid:80)t−1
if ξt ≥ 0 then
It ← Jt

else

It ← 0

end if

3:
4:
5:
6:
7:
8:
9:

10:
11:
12:
13:
14:
15:

16: end for

Algorithm 1: Conservative UCB

Remark 1 (Choosing ψδ). The conﬁdence intervals in Al-
gorithm 1 are constructed using the function ψδ. Let F be
the event that for all rounds t ∈ {1, 2, . . .} and every action
i ∈ [K], the conﬁdence intervals are valid:

| ˆµi(t) − µi| ≤

(cid:115)

ψδ(Ti(t))
Ti(t)

.

Our goal is to choose ψδ(·) such that
P {F} ≥ 1 − δ .

(8)

5

ψδ(s) = 2 log(K s3/δ),
for which (8) holds by Hoeﬀding’s inequality and union
bounds. The following choice achieve better performance
in practice:

ψδ(s) = log max{3, log ζ} + log(2e2ζ)
+ ζ(1 + log(ζ))
(ζ − 1) log(ζ)

log log(1 + s),

(9)

where ζ = K/δ; it can be seen to achieve (8) by more care-
ful analysis motivated by Garivier (2013),

Some remarks on Algorithm 1

• µ0 is known, so the upper and lower conﬁdence
bounds can both be set to µ0 (line 3). See Section 3.5
for a modiﬁcation that learns an unknown µ0.

• The max in the deﬁnition of the lower conﬁdence
bound λi(t) (line 7) is because we have assumed µi ≥ 0
and so the lower conﬁdence bound should never be
less than 0.

• ξt (line 10) is a lower conﬁdence bound on the bud-
get (6) if action Jt is chosen. More precisely, it is a
lower conﬁdence bound on

(cid:101)Zt =

µIs

+ µJt − (1 − α)tµ0.

t−1(cid:88)

s=1

• If the default arm is also the UCB arm (Jt = 0) and the
conﬁdence intervals all contain the true values, then
µ∗ = µ0 and the algorithm will choose action 0 for all
subsequent rounds, incurring no regret.

The following theorem guarantees that Conservative UCB
satisﬁes the constraint while giving a high-probability up-
per bound on its regret.
Theorem 2. In any stochastic environment where the arms
have expected rewards µi ∈ [0, 1] with 1-subgaussian
noise, Algorithm 1 satisﬁes the following with probability
at least 1 − δ and for every time horizon n:

t(cid:88)

s=1

µIs ≥ (1 − α)µ0t

for all t ∈ {1, . . . , n},

(5)

(cid:101)Rn ≤

(cid:32)

(cid:88)

(cid:33)

+ ∆i

4L
∆i

+ 2(K + 1)∆0
αµ0

i>0:∆i>0

K(cid:88)

∆0
max{∆i, ∆0 − ∆i}

+ 6L
αµ0
i=1
(cid:33)
nKL + KL
,
αµ0
when ψδ is chosen in accordance with Remark 1 and where
L = ψδ(n).

(cid:101)Rn ∈ O

(10)

(11)

(cid:32) √

,

√

Standard unconstrained UCB algorithms achieve a regret
nKL); Theorem 2 tells us that the penalty our
of order O(
algorithm pays to satisfy the constraint is an extra additive
regret of order O(KL/αµ0).

Remark 3. We take a moment to understand how the re-
gret of the algorithm behaves if α is polynomial in 1/n.
Clearly if α ∈ O(1/n) then we have a constant exploration
budget and the problem is trivially hard. In the slightly less
extreme case when α is as small as n−a for some 0 < a < 1,
the extra regret penalty is still not negligible: satisfying the
constraint costs us O(na) more regret in the worst case.

We would argue that
regret
penalty (10) is more informative than the worst case of
O(na); our regret increases by

the problem-dependent

6L
αµ0

K(cid:88)

i=1

∆0
max{∆i, ∆0 − ∆i}

.

Intuitively, even if α is very small, we can still explore as
long as the default arm is close-to-optimal (i.e. ∆0 is small)
and most other arms are clearly sub-optimal (i.e. the ∆i are
large). Then the sub-optimal arms are quickly discarded
and even the budget-building phases accrue little regret: the
regret penalty remains quite small. More precisely, if ∆0 ≈
n−b0 and mini>0:∆i>0 ∆i ≈ n−b, then the regret penalty is

na+min{0,b−b0}(cid:17)
small ∆0 and large ∆i means b − b0 < 0, giving a smaller
penalty than the worst case of O(na).

O

(cid:16)

;

Remark 4. Curious readers may be wondering if It = 0
is the only conservative choice when the arm proposed by
UCB risks violating the constraint. A natural alternative
would be to use the lower conﬁdence bound λi(t) by choos-
ing

It =





Jt ,
arg maxi λi(t) , otherwise .

if ξt ≥ 0 ;

It is easy to see that if F does not occur, then choos-
ing arg maxi λi(t) increases the budget at least as much as
choosing action 0 while incurring less regret and so this
algorithm is preferable to Algorithm 1 in practice. Theo-
retically speaking, however, it is possible to show that the
improvement is by at most a constant factor so our analysis
of the simpler algorithm suﬃces. The proof of this claim is
somewhat tedious so instead we provide two intuitions:

1. The upper bound approximately matches the lower
bound in the minimax regime, so any improvement
must be relatively small in the minimax sense.

2. Imagine we run the unmodiﬁed Algorithm 1 and let t
be the ﬁrst round when It (cid:44) Jt and where there exists
an i > 0 with λi(t) ≥ µ0. If F does not hold, then the

Conservative Bandits

actions chosen by UCB satisfy

Ti(t) ∈ Ω


min





L
∆2
i



T j(t)



 ,

, max
j

which means that arms are being played in approxi-
mately the same frequency until they are proving sub-
optimal (for a similar proof, see Lattimore, 2015b).
From this it follows that once λIt (t) ≥ µ0 for some
i it will not be long before either λ j(t + s) ≥ µ0 or
T j(t + s) ≥ 4L/∆2
i and in both cases the algorithm will
cease playing conservatively. Thus it takes at most a
constant proportion more time before the naive algo-
rithm is exclusively choosing the arm chosen by UCB.

Next we discuss how small modiﬁcations to Algorithm 1
allow it to handle some variants of the problem while guar-
anteeing the same order of regret.

3.4. Considering the Expected Regret and Budget

One may care about the performance of the algorithm in
expectation rather than with high probability, i.e. we want
an upper bound on E (cid:104)

and the constraint (5) becomes

(cid:105)

(cid:101)Rn

E(cid:104) t(cid:88)

(cid:105)

µIs

s=1

≥ (1 − α)µ0t,

for all t ∈ {1, . . . , n}.

(13)

We argued in Remark 3 that if α ∈ O(1/n) then the problem
is trivially hard; let us assume therefore that α ≥ c/n for
some c > 1. By running Algorithm 1 with δ = 1/n and
α(cid:48) = (α − δ)/(1 − δ) we can achieve (13) and a regret bound
with the same order as in Theorem 2.

(12)

To show (13) we have
E(cid:104) t(cid:88)

(cid:105)

µIs

s=1

(cid:105)

≥ P {F} E(cid:104) t(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12) F
≥ (1 − δ)(1 − α(cid:48))µ0t = (1 − α)µ0t .

µIs

s=1

In the upper bound of E [Rn], we have

E [Rn] ≤ E [Rn|F] + δn = E [Rn|F] + 1 .
E [Rn|F] can be upper bounded by Theorem 2 with two
changes: (i) L becomes O(log nK) after replacing δ with
1/n, and (ii) α becomes α(cid:48). Since α(cid:48)/α ≥ 1 − 1/c we get
essentially the same order of regret bound as in Theorem 2.

3.5. Learning an Unknown µ0

Two modiﬁcations to Algorithm 1 allow it to handle the
case when µ0 is unknown. First, just as we do for the non-
default arms, we need to set θ0(t) and λ0(t) based on con-
ﬁdence intervals. Second, the lower bound on the budget

6

Conservative Bandits

needs to be set as

=

ξ(cid:48)
t

Ti(t − 1)λi(t) + λJt (t)

K(cid:88)

i=1

+ (T0(t − 1) − (1 − α)t)θ0(t) .

(14)

Theorem 5. Algorithm 1, modiﬁed as above to work with-
out knowing µ0 but otherwise the same conditions as Theo-
rem 2, satisﬁes with probability 1 − δ and for all time hori-
zons n the constraint (5) and the regret bound
(cid:33)

(cid:32)

(cid:101)Rn ≤

(cid:88)

i:∆i>0

4L
∆i

+ ∆i

+ 2(K + 1)∆0
αµ0

+ 7L
αµ0

K(cid:88)

i=1

∆0
max{∆i, ∆0 − ∆i}

.

(15)

Theorem 5 shows that we get the same order of regret for
unknown µ0. The proof is very similar to the one for Theo-
rem 2 and is also left for the appendix.

4. The Adversarial Setting

Unlike the stochastic case, in the adversarial multi-armed
bandit setting we do not make any assumptions about how
the rewards are generated. Instead, we analyze a learner’s
worst-case performance over all possible sequences of re-
wards (Xt,i). In eﬀect, we are treating the environment as an
adversary that has intimate knowledge of the learner’s strat-
egy and will devise a sequence of rewards that maximizes
regret. To preserve some hope of succeeding, however, the
learner is allowed to behave randomly: in each round it can
randomize its choice of arm It using a distribution it con-
structs; the adversary cannot inﬂuence nor predict the result
of this random choice.

Our goal is, as before, to satisfy the constraint (2) while
bounding the regret (1) with high probability (the random-
ness comes from the learner’s actions). We assume that
the default arm has a ﬁxed reward: Xt,0 = µ0 ∈ [0, 1]
for all t; the other arms’ rewards are generated adversari-
ally in [0, 1]. The constraint to be satisﬁed then becomes
(cid:80)t

s=1 Xs,Is ≥ (1 − α)µ0t for all t.

Safe-playing strategy: We take any standard any-time
high probability algorithm for adversarial bandits and adapt
it to play as usual when it is safe to do so, i.e. when
Zt ≥ (cid:80)t−1
s=1 Xs,Is − (1 − α)µ0t ≥ 0. Otherwise it should play
It = 0. To demonstrate a regret bound, we only require that
the bandit algorithm satisfy the following requirement.
Deﬁnition 6. An algorithm A is ˆRδ
t -admissible ( ˆRδ
linear) if for any δ, in the adversarial setting it satisﬁes
∀t ∈ {1, 2, . . .}, Rt ≤ ˆRδ
t

≥ 1 − δ.

t sub-

P (cid:110)

(cid:111)

Note that this performance requirement is stronger than the

typical high probability bound but is nevertheless achiev-
able. For example, Neu (2015) states the following for
the any-time version of their algorithm: given any time
horizon n and conﬁdence level δ, P (cid:110)
≥ 1 − δ
n(δ)
for some sub-linear ˆR(cid:48)
t(δ). If we let ˆRδ
t(δ/2t2) then
t
(cid:111)
P (cid:110)
2t2 holds for any ﬁxed t. Since the al-
gorithm does not require n and δ as input, a union bound
shows it to be ˆRδ

Rn ≤ ˆR(cid:48)
= ˆR(cid:48)

Rt ≤ ˆRδ
t

≥ 1 − δ

t -admissible.

(cid:111)

Having satisﬁed ourselves that there are indeed algorithms
that meet our requirements, we can prove a regret guarantee
for our safe-playing strategy.

ˆRδ
t -admissible algorithm A, when
Theorem 7. Any
adapted with our safe-playing strategy, satisﬁes the con-
straint (2) and has a regret bound of Rn ≤ t0 + ˆRδ
n with
+ µ0}.
probability at least 1 − δ where t0 = max{t | αµ0t ≤ ˆRδ
t

Corollary 8. The any-time high probability algorithm of
Neu (2015) adapted with our safe-playing strategy gives
ˆRδ
t

Kt log K log(4t2/δ) and

= 7

(cid:112)

Rn ≤ 7

(cid:112)

Kn log K log(4n2/δ) + 49K log K

log2 4n2
δ

α2µ2
0

with probability at least 1 − δ.

(cid:18)

(cid:19)

O

KL2
(αµ0)2

Corollary 8 shows that a strategy similar to that of Algo-
rithm 1 also works for the adversarial setting. However,
we pay a higher regret penalty to satisfy the constraint:
(cid:16) KL
we had in the stochastic
αµ0
setting. Whether this is because (i) our algorithm is sub-
-optimal, (ii) the analysis is not tight, or (iii) there is some
intrinsic hardness in the non-stochastic setting is still not
clear and remains an interesting open problem.

rather than the O

(cid:17)

5. Lower Bound on the Regret

We now present a worst-case lower bound where α, µ0 and
n are ﬁxed, but the mean rewards are free to change. For
any vector µ ∈ [0, 1]K, we will write Eµ to denote expecta-
tions under the environment where all arms have normally-
distributed unit-variance rewards and means µi (i.e., the
ﬁxed value µ0 is the mean reward of arm 0 and the compo-
nents of µ are the mean rewards of the other arms). We as-
sume normally distributed noise for simplicity: Other sub-
gaussian distributions work identically as long as the sub-
gaussian parameter can be kept ﬁxed independently of the
mean rewards.

Theorem 9. Suppose for any µi ∈ [0, 1] (i > 0) and µ0
satisfying

min{µ0, 1 − µ0} ≥ max
(cid:80)n

(cid:110)
e + 1/2
1/2
an algorithm satisﬁes Eµ
t=1 Xt,It ≥ (1−α)µ0n. Then there
is some µ ∈ [0, 1]K such that its expected regret satisﬁes

K/n,

(cid:111) (cid:112)

α,

(cid:112)

√

7

EµRn ≥ B where

B = max





K
(16e + 8)αµ0

,

√

√

Kn
16e + 8





.

(16)

B0 =

√

nK
nK + K
αµ0

Bi = BK =

√

nK + K
αµ0

.

Conservative Bandits

parameters.

(cid:80)n

Theorem 9 shows that our algorithm for the stochastic set-
ting is near-optimal (up to a logarithmic factor L) in the
worst case. A problem-dependent lower bound for the
stochastic setting would be interesting but is left for fu-
ture work. Also note that in the lower bound we only use
Eµ
t=1 Xt ≥ (1 − α)nµ0 for the last round n, which means
that the regret guarantee cannot be improved if we only care
about the last-round budget instead of the anytime budget.
In practice, however, enforcing the constraint in all rounds
will generally lead to signiﬁcantly worse results because
the algorithm cannot explore early on. This is demonstrated
empirically in Section 6, where we ﬁnd that the Unbalanced
MOSS algorithm performs very well in terms of the ex-
pected regret, but does not satisfy the constraint in early
rounds.

Remark 10. The theorem above almost follows from the
lower bound given by Lattimore (2015a), but in that paper
µ0 is unknown, while here it may be known. This makes
our result strictly stronger, as the lower bound is the same
up to constant factors.

6. Experiments

We evaluate the performance of Conservative UCB com-
pared to UCB and Unbalanced MOSS Lattimore (2015a)
using simulated data in two regimes. In the ﬁrst we ﬁx the
horizon and sweep over α ∈ [0, 1] to show the degrada-
tion of the average regret of Conservative UCB relative to
UCB as the constraint becomes harsher (α close to zero).
In the second regime we ﬁx α = 0.1 and plot the long-term
average regret, showing that Conservative UCB is eventu-
ally nearly as good as UCB, despite the constraint. Each
data point is an average of N ≈ 4000 i.i.d. samples, which
makes error bars too small to see. All code and data will
be made available in any ﬁnal version. Results are shown
for both versions of Conservative UCB: The ﬁrst knows
the mean µ0 of the default arm while the second does not
and must act more conservatively while learning this value.
As predicted by the theory, the diﬀerence in performance
between these two versions of the algorithm is relatively
small, but note that even when α = 1 the algorithm that
knows µ0 is performing better because this knowledge is
useful in the unconstrained setting. This is also true of the
BudgetFirst algorithm, which is unconstrained when α = 1
and exploits its knowledge of µ0 to eliminate the default
arm. This algorithm is so conservative that even when α
is nearly zero it must ﬁrst build a signiﬁcant budget. We
tuned the Unbalanced MOSS algorithm with the following

The quantity Bi determines the regret of the algorithm with
respect to arm i up to constant factors, and must be chosen
to lie inside the Pareto frontier given by Lattimore (2015a).
It should be emphasised that Unbalanced MOSS does not
constraint the return except for the last round, and has no
high-probability guarantees. This freedom allows it to ex-
plore early, which gives it a signiﬁcant advantage over the
highly constrained Conservative UCB. Furthermore, it also
requires B0, . . . , BK as inputs, which means that µ0 must be
known in advance. The mean rewards in both experiments
are µ0 = 0.5, µ1 = 0.6, µ2 = µ3 = µ4 = 0.4, which means
that the default arm is slightly sub-optimal.

UCB
Conservative UCB
Conservative UCB (unknown µ0)
BudgetFirst
Unbalanced MOSS

Figure 2. Average regret for varying α and n = 104 and δ = 1/n

0.5
α

1

0.1

0.05

n

/

t
e
r
g
e
R
d
e
t
c
e
p
x
E

0

0

0.1

0.05

n
/

t
e
r
g
e
R
d
e
t
c
e
p
x
E

0
2,000

Figure 3. Average regret as n varies with α = 0.1 and δ = 1/n

50,000

n

100,000

7. Conclusion

We introduced a new family of multi-armed bandit frame-
works motivated by the requirement of exploring conser-
vatively to maintain revenue. We also demonstrated var-
ious strategies that act eﬀectively while maintaining such
constraints. We expect that similar strategies generalize
to other settings, like contextual bandits and reinforcement
learning. We want to emphasize that this is just the be-
ginning of a line of research that has many potential appli-
cations. We hope that others will join us in improving the
current results, closing open problems, and generalizing the
model so it is more widely applicable.

8

Conservative Bandits

G. Neu. Explore no more: Improved high-probability re-
In Advances
gret bounds for non-stochastic bandits.
in Neural Information Processing Systems, pages 3150–
3158, 2015.

V. Rieser and O. Lemon. Learning eﬀective multimodal
dialogue strategies from Wizard-of-Oz data: Bootstrap-
ping and evaluation. In ACL-08: HLT, pages 638–646,
2008.

A. Sani, G. Neu, and A. Lazaric. Exploiting easy data in
online optimization. In Advances in Neural Information
Processing Systems, pages 810–818, 2014.

Y. Sui, A. Gotovos, J. Burdick, and A. Krause. Safe explo-
ration for optimization with Gaussian processes. In Pro-
ceedings of the 32nd International Conference on Ma-
chine Learning (ICML-15), pages 997–1005, 2015.

References

R. Agrawal.

Sample mean based index policies with
o(log n) regret for the multi-armed bandit problem. Ad-
vances in Applied Probability, pages 1054–1078, 1995.

P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time anal-
ysis of the multiarmed bandit problem. Machine Learn-
ing, 47:235–256, 2002.

E. Even-Dar, M. Kearns, Y. Mansour, and J. Wortman. Re-
gret to the best vs. regret to the average. Machine Learn-
ing, 72(1-2):21–37, 2008.

T. Gabel and M. Riedmiller. Distributed policy search rein-
forcement learning for job-shop scheduling tasks. Inter-
national Journal of Production Research, 50(1):41–61,
2011.

J. Garc´ıa and F. Fern´andez. A comprehensive survey on
safe reinforcement learning. Journal of Machine Learn-
ing Research, 16:1437–1480, 2015.

A. Garivier.

Informational conﬁdence bounds for self-
normalized averages and applications. arXiv preprint
arXiv:1309.3376, 2013.

M. Hutter and J. Poland. Adaptive online prediction by fol-
lowing the perturbed leader. Journal of Machine Learn-
ing Research, 6:639–660, 2005.

K. Jamieson, M. Malloy, R. Nowak, and S. Bubeck.
lil’UCB: An optimal exploration algorithm for multi-
armed bandits. In COLT-2014, pages 423—439, 2014.

M. N. Katehakis and H. Robbins.

Sequential choice
from several populations. Proceedings of the National
Academy of Sciences of the United States of America, 92
(19):8584, 1995.

E. Kaufmann, A. Garivier, and O. Capp´e. On the com-
plexity of best arm identiﬁcation in multi-armed bandit
models. Journal of Machine Learning Research, 2015.
To appear.

W. M. Koolen. The Pareto regret frontier. In Advances in
Neural Information Processing Systems, pages 863–871,
2013.

T. Lattimore. The Pareto regret frontier for bandits.

In
Advances in Neural Information Processing Systems,
2015a. To appear.

T. Lattimore. Optimally conﬁdent UCB : Improved regret
for ﬁnite-armed bandits. Technical report, 2015b. URL
http://arxiv.org/abs/1507.07880.

Y.-E. Liu, T. Mandel, E. Brunskill, and Z. Popovi´c. To-
wards automatic experimentation of educational knowl-
edge. In SIGCHI Conference on Human Factors in Com-
puting Systems (CHI 2014), pages 3349–3358. ACM
Press, 2014.

9

Appendix

A. Proof of Theorem 2

Theorem 2. In any stochastic environment where the arms
have expected rewards µi ∈ [0, 1] with 1-subgaussian
noise, Algorithm 1 satisﬁes the following with probability
at least 1 − δ and for every time horizon n:

t(cid:88)

s=1

µIs ≥ (1 − α)µ0t

for all t ∈ {1, . . . , n},

(5)

(cid:101)Rn ≤

(cid:32)

(cid:88)

(cid:33)

+ ∆i

4L
∆i

+ 2(K + 1)∆0
αµ0

i>0:∆i>0

K(cid:88)

∆0
max{∆i, ∆0 − ∆i}

+ 6L
αµ0
i=1
(cid:33)
nKL + KL
,
αµ0
when ψδ is chosen in accordance with Remark 1 and where
L = ψδ(n).

(cid:101)Rn ∈ O

(10)

(11)

(cid:32) √

,

Proof. By Remark 1, with probability P {F} ≥ 1 − δ the
conﬁdence intervals are valid for all t and all arms i ∈
{1, . . . , K}:

| ˆµi(t − 1) − µi| ≤

ψδ(Ti(t − 1))/Ti(t − 1)

(cid:112)

(cid:112)

≤

L/Ti(t − 1);

we will henceforth assume that this is the case (i.e. that F
holds). By the deﬁnition of the conﬁdence intervals and by
the construction of Algorithm 1 we immediately satisfy the
constraint

n(cid:88)

t=1

µIt ≥ (1 − α)nµ0

for all n.

We now bound the regret. Let i > 0 be the index of a
sub-optimal arm and suppose It = i. Since the conﬁdence
intervals are valid,

µ∗ ≤ θi(t) ≤ ˆµi(t − 1) + (cid:112)
≤ µi + 2

(cid:112)

L/Ti(t − 1) ,

L/Ti(t − 1)

which implies that arm i has not been chosen too often; in
particular we obtain

Ti(n) ≤ Ti(n − 1) + 1 ≤

+ 1.

(17)

and the regret satisﬁes

K(cid:88)

(cid:101)Rn =

Ti(n)∆i ≤

(cid:32)

(cid:88)

4L
∆i

+ ∆i

+ T0(n)∆0.

i=0

i>0:∆i>0
If ∆0 = 0 then the theorem holds trivially; we therefore
assume that ∆0 > 0 and ﬁnd an upper bound for T0(n).

4L
∆2
i

(cid:33)

10

Let τ = max{t ≤ n | It = 0} be the last round in which the
default arm is played. Since F holds and θ0(t) = µ0 < µ∗ <
maxi θi(t), it follows that Jt = 0 is never the UCB choice;
the default arm was only played because ξτ < 0:

K(cid:88)

i=0

Ti(τ − 1)λi(τ) + λJτ (τ) − (1 − α)µ0τ < 0

(18)

By dropping λJτ(τ), replacing τ with (cid:80)K
rearranging the terms in (18), we get

i=0 Ti(τ − 1) + 1, and

< (1 − α)µ0 +

Ti(τ − 1) ((1 − α)µ0 − λi(τ))

αT0(τ − 1)µ0

K(cid:88)

i=1

≤ (1 − α)µ0
K(cid:88)

+

Ti(τ − 1)

i=1
K(cid:88)

≤ 1 +

S i .


(1 − α)µ0 − µi +

(cid:114)

L
Ti(τ − 1)




(19)

(20)

(21)

(22)

i=1
where ai = (1 − α)µ0 − µi and

S i = Ti(τ − 1) ·

= aiTi(τ − 1) + (cid:112)

(cid:16)

(1 − α)µ0 − µi + (cid:112)
LTi(τ − 1)

L/Ti(τ − 1)

(cid:17)

is a bound on the decrease in ξt in the ﬁrst τ − 1 rounds due
to choosing arm i. We will now bound S i for each i > 0.
The ﬁrst case is ai ≥ 0, i.e. ∆i ≥ ∆0 + αµ0. Then (17) gives
Ti(τ − 1) ≤ 4L/∆2
i

+ 1 and we get
4Lai
+ 2L
∆i
∆2
i

+ 2 ≤

S i ≤

6L
∆i

+ 2 .

The other case is ai < 0, i.e. ∆i < ∆0 + αµ0. Then

(cid:112)

S i ≤

LTi(τ − 1) ≤

2L
∆i

+ 1,

and by using ax2 + bx ≤ −b2/4a for a < 0 we have

S i ≤ −

L
4ai

=

L
4(∆0 + αµ0 − ∆i)

.

Summarizing (20) to (22) gives
6L
max{∆i, ∆0 − ∆i}

S i ≤

+ 2 .

Continuing from (19), we get
T0(n) = T0(τ − 1) + 1
2K + 2
αµ0

+ 1
αµ0

≤

K(cid:88)

i=1

6L
max{∆i, ∆0 − ∆i}

.

Conservative Bandits

We can now upper bound the regret by
+ 2(K + 1)∆0
αµ0

4L
∆i

(cid:101)Rn ≤

+ ∆i

(cid:88)

(cid:33)

(cid:32)

i>0:∆i>0

+ 6L
αµ0

K(cid:88)

i=1

∆0
max{∆i, ∆0 − ∆i}

.

(10)

We will now show (11). To bound the regret due to the
non-default arms, Jensen’s inequality gives


(cid:88)



i>0

Ti(n)∆i


2



≤ m2 (cid:88)

i>0

Ti(n)
m

∆2
i ,

where m ≤ n is the number of times non-default arms
were chosen. Combining this with ∆2
i ≤ 4L/Ti(n) for sub-
optimal arms from (17) gives
√

√

Ti(n)∆i ≤ 2

mKL ∈ O(

nKL).

(cid:88)

i>0

To bound the regret due to the default arm, observe that
max{∆i, ∆0 − ∆i} ≥ ∆0/2 and thus T0(n)∆0 ∈ O(KL/αµ0).
(cid:3)
Combining these two bounds gives (11).

B. Proof of Theorem 5

Theorem 5. Algorithm 1, modiﬁed as above to work with-
out knowing µ0 but otherwise the same conditions as Theo-
rem 2, satisﬁes with probability 1 − δ and for all time hori-
zons n the constraint (5) and the regret bound
(cid:33)

(cid:32)

(cid:101)Rn ≤

(cid:88)

i:∆i>0

4L
∆i

+ ∆i

+ 2(K + 1)∆0
αµ0

+ 7L
αµ0

K(cid:88)

i=1

∆0
max{∆i, ∆0 − ∆i}

.

(15)

Proof. We proceed very similarly to the proof of Theo-
rem 2 in Appendix A. As we did there, we assume that
F holds: the conﬁdence intervals are valid for all rounds
and all arms (including the default), which happens with
probability P {F} ≥ 1 − δ.

To show that the modiﬁed algorithm satisﬁes the con-
straint (5), we write the budget (6) as

(cid:101)Zt =

Ti(t − 1)µi + µJt

+ (T0(t − 1) − (1 − α)t)µ0

when the UCB arm Jt is chosen and show that it is indeed
lower-bounded by

=

ξ(cid:48)
t

Ti(t − 1)λi(t) + λJt (t)

K(cid:88)

i=1

K(cid:88)

i=1

in (14) is then negative and θ0(t) ≥ µ0. On the other hand,
if T0(t − 1) ≥ (1 − α)t then the constraint is still satisﬁed:

t(cid:88)

s=1

µIs ≥ T0(t − 1)µ0 ≥ (1 − α)µ0t.

We now upper-bound the regret. As in the earlier proof, we
can show that for any arm i > 0 with ∆i > 0 we have Ti(n) ≤
+ 1. If this also holds for i = 0 or if ∆0 = 0 then
4L/∆2
i
(cid:101)Rn ≤ (cid:80)
i:∆>0(4L/∆i + ∆i) and the theorem holds trivially.
From now on we only consider the case when ∆0 > 0 and
T0(n) > 4L/∆2
+ 1. As before, we will proceed to upper-
0
bound T0(n).
Let τ be the last round in which Iτ = 0. We can ignore
the possibility that Jτ = 0, since then the above bound on
Ti(n) would apply even to the default arm, contradicting
our assumption above. Thus we can assume that the default
arm was played because ξ(cid:48)
τ < 0:

K(cid:88)

i=1

Ti(τ − 1)λi(τ) + λJτ(τ)

+ (cid:0)T0(τ − 1) − (1 − α)τ(cid:1) θ0(τ) < 0 ,

in which we drop λJτ(τ), replace τ with (cid:80)K
and rearrange the terms to get

i=0 Ti(τ − 1) + 1,

αT0(τ − 1)θ0(τ) < (1 − α)θ0(τ)
K(cid:88)

+

Ti(τ − 1)(cid:0)(1 − α)θ0(τ) − λi(τ)(cid:1) .

(23)

i=1

We lower-bound the left-hand side of (23) using θ0(τ) ≥ µ0,
whereas we upper-bound the right-hand side using
(cid:114)

θ0(τ) ≤ µ0 +

L
T0(τ − 1)

≤ µ0 +

∆0
2

,

which comes from T0(τ − 1) ≥ 4L/∆2
0. Combining these
in (23) with the lower conﬁdence bound λi(τ) ≥ µi −
√

L/Ti(τ − 1) gives

(cid:32)
αµ0T0(τ − 1) < (1 − α)

µ0 +

(cid:33)

∆0
2

+

K(cid:88)

i=1

(cid:32)
Ti(τ − 1)

(cid:32)
(1 − α)

µ0 +

(cid:33)

∆0
2

− µi +

(cid:114)

(cid:33)

L
Ti(τ − 1)

(cid:32)
= (1 − α)

µ0 +

∆0
2

(cid:33)

+

K(cid:88)

S i

i=1

≤ 1 +

S i ,

K(cid:88)

i=1

(24)

(14)
This is apparent if T0(t − 1) < (1 − α)t, since the last term

+ (T0(t − 1) − (1 − α)t)θ0(t) .

where ai = (1 − α)(µ0 + ∆0/2) − µi and

S i = aiTi(τ − 1) + (cid:112)

LTi(τ − 1)

11

Conservative Bandits

is a bound on the decrease in ξ(cid:48)
to choosing arm i. We will now bound S i for each i > 0.

t in the ﬁrst τ − 1 rounds due

where B(t) = (cid:80)t
which the algorithm plays safe.

(cid:49)(cid:8)Z(cid:48)

s=1

s ≥ 0(cid:9). Let τ be the last round in

Analogously to the previous proof, we get the bounds

µ0B(τ − 1)

S i ≤

S i ≤

6L
∆i
2L
∆i

+ 2, when ai ≥ 0 ;

+ 1 ,

otherwise;

and in the latter case, using ax2 + bx ≤ −b2/4a gives

S i ≤ −

L
4ai

=

L
4(cid:0)(1 + α)∆0/2 + αµ0 − ∆i

(cid:1) .

Summarizing (25) to (27) gives
6L
max(cid:8)∆i, 24(cid:0)(1 + α)∆0/2 + αµ0 − ∆i

S i ≤

(cid:1)(cid:9) + 2

≤

7L
max{∆i, ∆0 − ∆i}

+ 2 .

Continuing with (24), if T0(n) > 4L
∆2
0

+ 1, we get

T0(n) = T0(τ − 1) + 1
2K + 2
αµ0

+ 1
αµ0

≤

K(cid:88)

i=1

7L
max{∆i, ∆0 − ∆i}

.

We can now upper bound the regret by
+ 2(K + 1)∆0
αµ0

4L
∆i

(cid:101)Rn ≤

+ ∆i

(cid:88)

(cid:33)

(cid:32)

i:∆i>0

(25)

(26)

(27)

≤ max
i

τ−1(cid:88)

s=1

(cid:49)(cid:8)Z(cid:48)

s ≥ 0(cid:9) Xs,i

≤ ˆRδ

B(τ−1)

+

(cid:49)(cid:8)Z(cid:48)

s ≥ 0(cid:9) Xs,Is

τ−1(cid:88)

s=1
τ−1(cid:88)

s=1

= ˆRδ

B(τ−1)

+

Xs,Is − µ0(τ − 1 − B(τ − 1))

≤ ˆRδ

+ (1 − α)µ0τ − µ0(τ − 1 − B(τ − 1)) ,

B(τ−1)
which indicates αµ0τ ≤ ˆRδ
τ
that Rn ≤ t0 + ˆRδ
n.

+ µ0 and thus τ ≤ t0. It follows
(cid:3)

D. Proof of Theorem 9

Theorem 9. Suppose for any µi ∈ [0, 1] (i > 0) and µ0
satisfying

min{µ0, 1 − µ0} ≥ max
(cid:80)n

(cid:110)
e + 1/2
1/2
an algorithm satisﬁes Eµ
t=1 Xt,It ≥ (1−α)µ0n. Then there
is some µ ∈ [0, 1]K such that its expected regret satisﬁes
EµRn ≥ B where

K/n,

(cid:111) (cid:112)

α,

(cid:112)

√

B = max





K
(16e + 8)αµ0

,

√

√

Kn
16e + 8





.

(16)

+ 7L
αµ0

K(cid:88)

i=1

∆0
max{∆i, ∆0 − ∆i}

.

(15) (cid:3)

C. Proof of Theorem 7

ˆRδ
t -admissible algorithm A, when
Theorem 7. Any
adapted with our safe-playing strategy, satisﬁes the con-
straint (2) and has a regret bound of Rn ≤ t0 + ˆRδ
n with
+ µ0}.
probability at least 1 − δ where t0 = max{t | αµ0t ≤ ˆRδ
t

Proof of Theorem 7. It is clear from the description of
the safe-playing strategy that it is indeed safe:
the con-
straint (2) is always satisﬁed.

The algorithm plays safe when the following quantity,
which is a lower bound on the budget Zt, is negative:

Z(cid:48)
t

= Zt − Xt,It

=

Xs,Is − (1 − α)µ0t

t−1(cid:88)

s=1
To upper bound the regret, consider only the rounds in
which our safe-playing strategy does not interfere with
playing A’s choice of arm. Then with probability 1 − δ,

max
i∈{0,...,K}

t(cid:88)

s=1

(cid:49)(cid:8)Z(cid:48)

s ≥ 0(cid:9) (Xs,i − Xs,Is ) ≤ ˆRδ

B(t)

Proof of Theorem 9. Pick any algorithm. We want to show
that the algorithm’s regret on some environment is at least
as large as B. If EµRn > B for some µ ∈ [0, 1]K, there is
nothing to be proven. Hence, without loss of generality, we
can assume that the algorithm is consistent in the sense that
EµRn ≤ B for all µ ∈ [0, 1]K.
For some ∆ > 0, deﬁne environment µ ∈ RK such that
µi = µ0 − ∆ for all i ∈ [K]. For now, assume that µ0 and
∆ are such that µi ≥ 0; we will get back to this condition
later. Also deﬁne environment µ(i) for each i = 1, . . . , K by

µ(i)
j

=





µ0 + ∆,
for j = i ;
µ0 − ∆, otherwise.

In this proof, we use Ti = Ti(n) to denote the number of
times arm i was chosen in the ﬁrst n rounds. We distinguish
two cases, based on how large the exploration budget is.

Case 1: α ≥

√

µ0

√

K
(16e + 8)n
√

.

In this case, B =
and we use ∆ = (4e + 2)B/n. For
each i ∈ [K] deﬁne event Ai = {Ti ≤ 2B/∆}. First we prove

Kn
16e+8

√

12

Conservative Bandits

that Pµ(Ai) ≥ 1/2:

Pµ{Ti ≤ 2B/∆} = 1 − Pµ{Ti > 2B/∆}

Next, we show that Pµ(i)(Ai) < 1/4e:
Pµ(i){Ti ≤ 2αµ0n/∆}

= Pµ(i){n − Ti ≥ n − 2αµ0n/∆}

≤

≤

Eµ(i)[n − Ti]
n − 2αµ0n/∆
B
∆n − 2αµ0n

=

K
(4e + 2)K − (32e + 16)α2µ2
0n
As in the other case, we have Eµ[Ti] > 1/4∆2 for each
i ∈ [K]. Therefore

1
4e

<

.

Eµ[Rn] = ∆

Eµ[Ti] >

= αµ0n,

(cid:88)

i∈[K]

K
4∆

which contradicts the fact that Eµ[Rn] ≤ αµ0n. So there
does not exist an algorithm whose worst-case regret is
smaller than B.

To summarize, we proved that

EµRn ≥

√






√

Kn
16e + 8
K
(16e + 8)αµ0

,

, when α ≥

µ0
otherwise,

√

√

K
(16e + 8)n

ﬁnishing the proof.

(cid:3)

Next we prove that Pµ(i)(Ai) ≤ 1/4e:

Pµ(i){Ti ≤ 2B/∆} = Pµ(i){n − Ti ≥ n − 2B/∆}

≥ 1 −

≥ 1 −

∆Eµ[Ti]
2B
Eµ[Rn]
2B

≥

1
2

.

≤

≤

Eµ(i)[n − Ti]
n − 2B/∆

B
∆n − 2B

= 1
4e

.

Note that µ and µ(i) diﬀer only in the ith component:
µi = µ0 − ∆ whereas µ(i)
= µ0 + ∆. Then the KL diver-
i
gence between the reward distributions of the ith arms is
KL(µi, µ(i)
i ) = (2∆)2/2 = 2∆2. Deﬁne the binary relative
entropy to be

d(x, y) = x log

+ (1 − x) log

x
y

1 − x
1 − y

;

it satisﬁes d(x, y) ≥ (1/2) log(1/4y) for x ∈ [1/2, 1] and
y ∈ (0, 1). By a standard change of measure argument (see,
e.g., Kaufmann et al., 2015, Lemma 1) we get that
i ) ≥ d(Pµ(Ai), Pµ(i)(Ai))
= 1
2

Eµ[Ti] · KL(µi; µ(i)

1
4(1/4e)

log

1
2

≥

and so Eµ[Ti] ≥ 1/4∆2 for each i ∈ [K]. Hence

Eµ[Rn] = ∆

Eµ[Ti] ≥

=

√

= B .

K
4∆

√

Kn
16e + 8

(cid:88)

i∈[K]
√

.

K
(16e + 8)n
K
(16e+8)αµ0

Case 2: α <

√

µ0
In this case, B =
and we use ∆ = K/4αµ0n. For
each i deﬁne the event Ai = {Ti ≤ 2αµ0n/∆}. First we prove
that Pµ(Ai) ≥ 1/2:

Pµ{Ti ≤ 2αµ0n/∆} = 1 − Pµ{Ti > 2αµ0n/∆}

≥ 1 −

≥ 1 −

∆Eµ[Ti]
2αµ0n
Eµ[Rn]
2αµ0n

≥

1
2

,

where we use the fact that

Eµ[Rn] = nµ0 − Eµ

(cid:104) n(cid:88)

(cid:105)

Xt,It

t=1
≤ nµ0 − (1 − α)µ0n = αµ0n.

13

