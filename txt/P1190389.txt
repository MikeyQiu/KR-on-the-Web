A Framework for Visually Realistic Multi-robot Simulation in
Natural Environment

Ori Ganoni
Department of Computer Science
University of Canterbury
Christchurch, New Zealand
ori.ganoni@pg.canterbury.ac.nz

Ramakrishnan Mukundan
Department of Computer Science
University of Canterbury
Christchurch, New Zealand
mukundan@canterbury.ac.nz

ABSTRACT
This paper presents a generalized framework for the simulation of multiple robots and drones in highly realistic
models of natural environments. The proposed simulation architecture uses the Unreal Engine4 for generating both
optical and depth sensor outputs from any position and orientation within the environment and provides several key
domain speciﬁc simulation capabilities. Various components and functionalities of the system have been discussed
in detail. The simulation engine also allows users to test and validate a wide range of computer vision algorithms
involving different drone conﬁgurations under many types of environmental effects such as wind gusts. The paper
demonstrates the effectiveness of the system by giving experimental results for a test scenario where one drone
tracks the simulated motion of another in a complex natural environment.

Keywords
Robot simulation, Drone simulation, Natural environment models, Natural feature tracking, Unreal Engine 4
(UE4).

1

INTRODUCTION

Graphical models of realistic natural environments are
extensively used in games, notably simulation games
and those that use immersive environments. These vir-
tual environments provide a high degree of interactive
experience and realism in simulations. Modern game
engines provide tools for prototyping realistic, complex
and detailed virtual environments. Recently, this capa-
bility of game engines has been harnessed to the advan-
tage of computer vision community to develop frame-
works that can be used in scientiﬁc applications where
vision based algorithms for detection, tracking and nav-
igation could be effectively tested and evaluated with
various types of sensor inputs and environmental condi-
tions. This paper focuses on the development of a com-
prehensive standalone framework for multi-robot simu-
lation (speciﬁcally, multi-drone simulation) in complex
natural environments, and proposes suitable conﬁgura-
tions of tools, simulation architectures and also looks at
key performance issues.

Permission to make digital or hard copies of all or part of
this work for personal or classroom use is granted without
fee provided that copies are not made or distributed for proﬁt
or commercial advantage and that copies bear this notice and
the full citation on the ﬁrst page. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires
prior speciﬁc permission and/or a fee.

Several robot simulation engines exist which simulate
different robots and vehicles e.g. multicopters, rovers,
ﬁxed wing UAV, etc. Each engine has its advantages.
The engines use large simulation environments consist-
ing of models, sceneries, etc. generated by other sim-
ulation packages and frameworks. Following are some
examples of such engines with dependencies on other
simulation packages:

• Standalone robot simulation engines using a ﬂight
simulator for models, sceneries and functions for vi-
sualization and simulation. Examples of such pack-
ages are: (i) ArduPilot
[1] which communicates
with Xplane [12] and Flightgear [4] (ii) PX4 [8]
communicates with jmavsim [7]. Flight simulators
are usually much larger projects than robot simula-
tion projects. They are more focused on user experi-
ence and interaction, but they also have visualization
and dynamic simulation capabilities which are use-
ful characteristics for drone projects.

• Standalone simulation packages that use physics en-
gines, graphical interfaces and simulation capabili-
ties provided by other simulation tools: for example
PX4 [8] with Gazebo [5]. Robot simulation en-
vironments are dedicated simulation environments.
They are focused on giving proper tools for mod-
eling and simulating robots but are less focused on
visualization.

• Stand alone robot simulation environment:

those
environments include the robots and ﬂying vehi-

7
1
0
2
 
g
u
A
 
6
 
 
]

O
R
.
s
c
[
 
 
1
v
8
3
9
1
0
.
8
0
7
1
:
v
i
X
r
a

cle models. An example of that kind of environ-
ment is: Modular Open Robots Simulation Engine
(MORSE). Those environments are suitable for test-
ing and evaluating ideas, but they don’t have roots
in real robot projects speciﬁcally in drone projects.

depth sensor

30 fps.

• Game engine stand alone environment:

the robot
is simulated inside a game engine. For example a
benchmark for tracking based on UE4 [22]. Sim-
ilarly to robot simulation environment, the drones
inside game engines don’t have roots in real drone
projects. Additionally drone simulated in game en-
gines don’t share the dynamics of real drones. For
example, they don’t have to deal with wind gusts and
vibrations.

In this paper, we propose a novel conﬁguration that use
game engines for the simulation environment, the pri-
mary motivation being the enhanced capabilities of a
game engine such as UE4 in providing highly realistic
environments and various modes of visualization. One
of the primary advantages of this type of a conﬁguration
is that a game engine such as UE4 can provide realtime
videos of camera output based on the position and at-
titude information of the robot. This paper also gives
an overview of the DroneSimLab [3] developed by us,
which has constantly evolved with the analysis of vari-
ous requirements and concepts related to the simulation
architecture presented later in this work. The design
and implementation aspects of the key components of
this simulation engine have been presented in detail.

This paper is organized as follows: in section 2 we give
an overview of the DroneSimLab project. In section 3
we describe previous related work with game engines.
Section 4 gives detailed information about the frame-
work simulation architecture and design goals. Section
5 focuses on modiﬁcations needed to be made to meet
the simulation design goals. Section 6 describes ex-
perimental results and performance. Section 7 provide
information on future research directions as well as ref-
erences to online demos of this research.

2 THE DRONESIMLAB PROJECT

We developed DroneSimLab as an opensource project
to foster collaborative development of drone simulation
packages that use the power and capabilities of the UE4
as discussed in the previous sections. Some of the main
functionalities which the current implementation pro-
vides are:

• Multi-robot - can handle more than one robot and

create visual interaction.

• Software In The Loop (SITL) driven - can simulate

two drone models: ArduPilot and PX4

• Based on Game Engine - Uses UE4 as an optical and

• Realtime - depends on the hardware but can run at

• Natural environments - can simulate trees, wind
grass, etc. (comes with Game Engines assets).

3 RELATED WORKS

Some image processing and image-based algorithms
have already been integrated into game engine/image
simulation environments, although using game engines
dedicated for games (like UE4, CryEngine, and Unity)
in simulations is much less common. We believe the
reason for that is due that they are more focused on
game experience as opposed to any real-world scien-
tiﬁc applications involving simulations with associated
mathematical and physical models and computer vision
algorithms.

One example of such an approach is the autonomous
landing of a Vertical Takeoff and Landing (VTOL) Un-
manned Aerial Vehicle (UAV) on a moving platform us-
ing image based visual servoing [25], where they used
gazebo simulation simultaneous localization and map-
ping. Simultaneous Localization and Mapping (SLAM)
[13] is also tested and developed for indoor scenarios
using gazebo simulation [5] [20]. Some environments
are combined to create a more powerful engine. For
example, MORSE [17] combined with BGE (blender
game engine) [16] and JSBsim (an open source ﬂight
dynamics model)

Lately, game engines have been increasingly used for
simulations. Successful attempts have been made to
evaluate the stability of structure using UE4, creating
photo-realistic scenes of stacks of blocks and applying
deep learning methods [19]. A series of towers made
from wooden cubes were created in a simulated envi-
ronment using UE4 [18]. Some of the towers were sta-
ble structures, and some collapsed when the dynamic
simulation was run. A network was trained to detect
the outcome of the experiments. Testing the network
on real environments achieved equal performance com-
pared to human subjects in predicting whether the tower
will fall. The most important aspect of this research is
the fact that they could train the network on 180,000
scenarios which seems not feasible in a real life envi-
ronment.

A more recent work connected UE4 with OpenCV [15],
It extends the
the project is called UnrealCV [23].
UE4 with a set of commands to interact with the vir-
tual world. Another work [22] proposed a new aerial
video dataset and benchmark for low altitude UAV tar-
get tracking, as well as a photorealistic UAV simula-
tor that can be coupled with tracking methods. Skinner

[27] proposed a high-ﬁdelity simulation for evaluating
robotic vision performance for repeating robotic vision
experiments under identical conditions. Similarly, we
are providing a sandbox for high-ﬁdelity simulations
not only for algorithms but also for full SITL simula-
tions.

Recently Microsoft released AirSim [26], an open
source simulator based on Unreal Engine for au-
tonomous vehicles from Microsoft AI & Research
which has a similar architecture as our proposed
architecture. In their released implementation they are
using their physics engine and control libraries.

SIT L1

Vehicle1

Container1

SIT L2

Vehicle2

Container2

...

SIT LN

VehicleN

Containern

6

D

O

F

V

i

d

e

o

Plugin

Game Engine

ContainerUE4

Figure 1: Simulation architecture

4 SIMULATION ARCHITECTURE
In this paper, we propose a simulation architecture de-
signed to meet the following primary goals: (i) ability
to generate realtime camera outputs for any arbitrary
position and orientation in a natural environment, (ii)
ability to integrate software and hardware in the loop
simulations (iii) ability to combine multiple simulations
and (iv) ability to reproduce results. These aspects are
elaborated below.

4.1 Domain Speciﬁc Simulation Engine
We focused on three simulation engines for the frame-
work.

• The game engine provides video, depth data and ad-
ditional visual environmental effects like wind and
dust.

• The physical model engine, usually supplied by the

robot development framework.

• Supplimental simulation objects like communica-
tion channels models , computation power restric-
tions and additional simulation ﬁlters (for example a
lens distortion ﬁlter).

Engines can create an environment for a single robot.
For instance in the case of SITL, the simulation engine
interacts with only one vehicle and produces sensory
information for only one robot. On the other hand, the
game engine can provide visual information for mul-
tiple robots as described in Figure 1, this is especially
necessary if a visual interaction exists, for example; one
robot can block the ﬁeld of view for another robot, and
this aspect should be implemented in the simulation.
By embedding SITL into our framework we ensure that
the simulation is highly correlated with the real robot
architecture (since it is used for the robot development).
Hardware In The Loop (HIL) can be later used for fur-
ther validation.

4.2 Simulated sensor architecture
We identiﬁed three types of simulated sensors that can
be used.

• Single domain sensor - lives only in one engine.
For example a simulated RGB camera from UR4
[18]. Another example is the gyro sensor, which is
simulated only in the SITL software e.g. gazebo,
jmavsim, jsbsim etc.

• Multi domain sensor - lives in more then one engine.
For example such a sensor can be seen in Figure 2.
In this example the simulated distance sensor gets
information in from various sources like an external
Digital Elevation Model (DEM).

• Complex sensor - lives in both the physical domain
and in the simulated domain. An example of such
sensor is a camera in front of a screen. The display
provides the visual information and the camera is
used just as in the real system, enabling monitoring
real system performance and hardware issues. This
concept is an extension of the HIL mode which com-
bines hardware testing and software testing.

UE4
Collision
Detection

UE4 Depth
Map

Data
Elevation
Map

Ground
Truth
Output

Sensor
Input
Fusion

Noise
Model

System
Output

Figure 2: Example of a multi-domain distance sensor,
The simulated sensor accepts inputs from different en-
gine resources and produces ground truth information
and noisy output.

4.3 Containers as vehicles
We used a container approach to combine simulations
not originally intended to work alongside each other.
The container approach enables us to use different op-
erating systems and libraries on the same machine. We
can also control the network conﬁguration in each con-
tainer. This approach is different from other frame-
works like AirSim [26] by maintaining the original
ﬁrmware developed by the Robot Simulation Frame-
work. Our architecture can utilize the beneﬁts of new
features and continuing development of those environ-
ments.

4.4 Reproducibility
The ability to reproduce results under differing or con-
stant conditions is vital in system development as well
as in research, and becomes more and more difﬁcult
as the complexity of the system increases. To real-
ize this concept, we are using several existing software
tools. We are using the Docker engine to manage sys-
tem conﬁguration, and Git version control to handle the
software development. Since the experiments are in a
simulation, other reproducibility aspects such as high-
ﬁdelity are built-in. In section 6, we demonstrate test-
ing of algorithms in complex natural environments by
controlling simulation parameters. In this simulation,
we can see that outdoor natural environment can be
problematic for testing visual algorithms since we don’t
have full control of the environment. It seems that true
reproducibility in an outdoor natural environment may
be achieved only in simulation [27].

4.5 Build system & conﬁguration man-

agement

The simulation environment uses these software tools:

• Version Control - All ﬁles of this project are man-
aged by Git version control under GitHub servers
[6]. The only exceptions are the UE4 projects which
are managed locally due to the large ﬁle sizes. The
UE4 [18] source code is still managed by git in dedi-
cated GitHub repository. For the purpose of sending
realtime ground truth position, changes have been
made both to ArduPilot Project and to PX4 and are
managed in separate forks. Those changes are not
compatible with the design and purpose of the orig-
inal projects. Changes that were compatible (e.g. a
turbulence model) were returned to the community
as pull requests and then pulled back into our local
fork.

• PX4 Fork [8] (Drone Project)

• UE4PyServer plugin [10]

5 ENGINE MODIFICATIONS
5.1 UE4 Plugin
Game engines are not dedicated research tools, obvi-
ously, but conveniently for our usage scenario they sup-
ply mechanisms like plugins to extend the capabilities
of the engine. The plugin we used for the UE4 [18] is
called UE4PyServer [10] Plugin and was developed for
the purpose of this research. The main concepts behind
the plugin development were:

• Realtime: For this simulation, we took advantage of
the realtime capabilities of the game engine. Real-
time simulation (RT) is important when you want to
run many tests in a short period. RT simulation is
also necessary when human interaction is involved
because users expect realtime or near realtime be-
havior. To maintain RT behavior, the UE4 plugin
was developed with minimal processioning on the
UE4 side. The primary purpose of the plugin is
to communicate with other parts of the simulation.
e.g. receiving 6 DOF information and sending video
data.

• Multi-Robot support - UE4 enables capture of the
viewable screen to a ﬁle or a buffer, but this pro-
vides us with only one camera feed. To allow multi-
ple cameras in the simulation, we used rendering-
to-texture technique with object ScreenCapture2D
[9]. The method is used in the game engine usually
to render surfaces like security cameras, billboards,
mirrors, etc. We used it to simulate a camera robot
and depth sensors using the depth map provided by
the ScreenCapture2D Object.

• Synchronization - We wanted the sampling to be
synchronized for all the visual objects in the sim-
ulation. It is an important concept and might be crit-
ical for some applications, for example, simulating
stereo camera. For this purpose, we used coroutines
which are a light version of synchronized pseudo-
threads.

5.2 Building realistic environment inside
game engine for computer vision
There are some special considerations when building
virtual environments in game engines for computer vi-
sion purposes.

• Containers - Created with Docker engine.

• ArduPilot Fork [1] (Drone Project)

• ROS - Supporting ﬁrmware for the PX4 project.

• Level of Details (LOD) - in game engines using
multiple LOD [21, Chapter 3] in order to maintain
graphics performance especially frame rate. This
may create unnatural textures changes which can be

Figure 3: Open world Overview - these are the assets
used to build a forest environment

destructive for computer vision algorithms. For the
purpose of this research, we can control the envi-
ronment and the simulation, and we can use that to
create a scene with only one LOD.

• Repeating patterns - In Figure 3 we can see the
meshes used to build the realistic scene. To reduce
the effect of repeating patterns, each element is po-
sitioned in a different orientation and slightly differ-
ent scaling. Also, the elements are positioned with
some overlap with other objects which reduces the
repeating effect.

• Culling adjustments - the area rendered in the scene
also known as frustum should be large enough for
all the objects in the scene to be rendered, so we
will avoid popping effects due to movements of the
cameras or the objects themselves.

• Dynamic shadows adjustments - moving objects in
the scene like trees and robots should always cast
dynamic shadows to imitate real scenarios.

5.3 SITL

The SITL engine needs to send 6 DOF information at a
high rate to the game engine (at least 30 fps) to maintain
realtime constraints. For that purpose, some modiﬁca-
tions are needed to the engine, so the SITL engine will
send ground truth information directly to the game en-
gine, and also to logging mechanisms for later analysis
as described in Figure 1.

6 EXPERIMENTAL RESULTS AND

PERFORMANCE ANALYSIS

6.1 Plugin tests in natural environment

Running visual algorithms in a natural environment can
be very challenging. Relative to artiﬁcial environments,
natural scenes can by highly dynamic due to atmo-
spheric conditions such as wind, and usually will not
have distinct characteristics like straight lines, circles

Figure 4: Scene architecture - In the UE4 Editor, we
placed a camera at an initial position, in front of a tree.
We moved the camera diagonally away from the tree,
and then returned to the same point. Camera maneu-
vers which starts and ends in the same position are ideal
for tracking tests. Ideally, the tracked points should get
back to the same original coordinates.

corners, etc. Using UE4PyServer [10] (which was de-
veloped as part of the simulation framework) and UE4
[18] we developed a tracking simulation (live video can
be found here [11]) to demonstrate the uniqueness of
natural environment. The simulation is based on the
Lucas-Kanade Optical Flow tracker implemented in the
OpenCV library [14] which we use it to track an or-
dered grid of points (no feature extraction). The ma-
neuver is a simple camera facing forward and moving
diagonally back and then return to the original position
as described in Figure 4. Ideally, we would expect that
the tracked points will return to the same coordinates
when the simulation cycles back to the starting frame.
Since this is a complex 3D scene, not all the points will
return to the same location due to the loss of tracking,
but in Figure 5 we can see that running the experiment
twice produces similar results. Similar but not exact,
since there is still some randomness in the scene due
to movement of leaves that might cause slight differ-
ences. In Figure 6 we conducted two experiments with
the same setup, but in the second test, we add the wind
to the scene by adding to the UE4 a Wind Direction
Force object. We can see that the results are now very
different. We repeated the experiment under various
conditions and calculated the following MSE grade to
quantify the tracking quality:

G =

1
N ∑

P∈Tp

|Pe − Ps|2

(1)

where: G is the tracking error, Tp is the tracked points,
Ps an Pe are the start and end coordinates of the tracked

points and N is the number of tracked points. Summa-
rized results are in the following table:

low-alt

high-alt

low-wind
96.36 (98.54%)
81.15 (98.54%)
87.69 (98.54%)
55.72 (97.56%)
53.82 (98.21%)
48.94 (97.40%)

high-wind
219.99 (97.40%)
225.48 (98.05%)
234.41 (97.89%)
243.59 (98.54%)
757.61 (97.73%)
382.78 (98.38%)

Table 1: Tracking error (MSE) values in pixels2. The
numbers in brackets give the percentage of correctly
tracked points.

In Table 1, we can see the behavior of the tracking al-
gorithm under different environmental conditions. As
expected in high altitude (near the tree tops) with the
combination of strong wind will be the most challeng-
ing scenario. As seen in the ﬁrst column, the track-
ing error under low wind conditions is larger at low
altitudes compared to high altitudes due to the pres-
ence of a higher density of objects such as leaves and
branches that occlude the camera view at low altitudes.
On the other hand, when the wind speed increases, the
trend is reversed because leaves and branches tend to
move more than the objects closer to the ground like
tree trunk rocks, etc.

We developed a tool for proﬁling and monitoring the
framework in addition to the existing tools in the UE4
editor. This is a high-level proﬁling tool that gives us
the summary of the system utilization. An example of a
test case is presented in Figures 4 and 7. ﬁg 4 explains
the scene architecture and Figure 7 the corresponding
proﬁling graph. The peaks in the GPU utilization are
due to camera IO-intensive movements as would be ex-
pected. In this example, the GPU peaks in the graphs
correspond to camera movement. When the camera is
moving, we can see that the GPU is fully utilized (it
reaches 100%) resulting in reduced framerate and when
the frame rate is reduced the CPU was less occupied
because it was processing the images at a lower frame
rate.

6.2 DroneSimLab tests
We created a setup in DroneSimLab for an experiment
of one drone tracking another drone. The drones are
Ardupilot drones simulated using their internal SITL
engine. We simulated the wind in the UE4 as well as in
the SITL engine including wind gusts. The two drones
ﬂy into the forest and then return to the original posi-
tion [2]. One of the drones is using HSV tracker [24]
to track the other drone (Figure 9). We repeated this
experiment four times and the results are presented in
Figure 8.

In all four scenarios, we can observe the loss of tracking
capabilities when the drones enter the forest (black dots

(a)

(b)
Figure 5: Simulation outputs (a) and (b) showing repro-
ducability of results under similar environmental condi-
tions.

between frames 300 to 500 in all four scenarios). When
the drones enter the woods, the shades from the forest
canopy affects the color and brightness components of
the drone as can be seen in this demo video [2]. The
threshold for tracking is not updated dynamically to
demonstrate this behavior. Other interesting phenom-
ena is the high frequencies observed in the graph. These
high frequencies result from the continuous maneuver-
ing and changes in the 3D orientation of the drone to
compensate for the high wind forces, which in turn re-
sults in variations in the estimation of the drones center
position. In the last two experiments, we can see espe-
cially large amplitude in the beginning and at the end
of the experiments as a result of the takeoff and landing
process which provided different angles of viewing of
the drone body led to a different estimation of the drone
center.

The above results have clearly demonstrated the useful-
ness of a game engine in not only producing realistic
natural environments and their camera outputs, but also
providing the ability to add and modify realistic envi-
ronmental effects such as changes in wind parameters
and illumination conditions. These features allow us to

(a)

Figure 8: Tracking results - Four consecutive tracking
experiments results. The black dots represents tracking
failures. The X axis is frame number and the Y axis is
pixel position. Green and red lines are the X,Y pixel
coordinates respectively.

(b)
Figure 6: Simulation outputs showing the variations in
results under differing environmental conditions. The
output in (a) was generated without simulated wind,
while in (b), wind was added to the simulation. We
can observe degradation in the tracking quality of the
grid features.

Figure 7: Proﬁling scene - high level proﬁling dur-
ing scene playback created with nvidia-smi and Python
psutil.

Figure 9: Tracking Experiment - A drone following an-
other drone in DroneSimLab.

generate ground truth data for various test conditions
and to evaluate machine vision algorithms.

7 CONCLUSIONS
The creation of visually realistic environments is a very
powerful tool for computer vision research as can be
seen in section 6 and this corresponding video demo
[11]. The DroneSimLab project [3] aims to be a tool
which adds game engines capabilities to the current ex-
isting robot simulation environments. The current work
mainly focuses on UE4, but adding another game en-
gine may increase the dimensionality of modiﬁable pa-
rameters in our systems. For instance, training deep
learning algorithms on multiple worlds each created by
a different game engine may more accurately general-
ize to the real world domain. This paper has presented
a new framework for simulating multi-robot (speciﬁ-
cally, multi-drone) motion in such environments, where
environmental effects can be easily incorporated, and
complex computer vision tasks evaluated. The simula-
tion architecture along with the key functionalities of
the simulation engine have been discussed in detail.

ing ros and gazebo. In International Conference
on Simulation, Modeling, and Programming for
Autonomous Robots, pages 400–411. Springer,
2012.

[21] Thomas Mooney. Unreal Development Kit Game

Design Cookbook. Packt Publishing Ltd, 2012.
[22] Matthias Mueller, Neil Smith, and Bernard

Ghanem. A benchmark and simulator for uav
tracking. In European Conference on Computer
Vision, pages 445–461. Springer, 2016.

[23] Weichao Qiu and Alan Yuille. Unrealcv: Con-
necting computer vision to unreal engine. arXiv
preprint arXiv:1609.01326, 2016.

[24] Adrian Rosebrock. Ball tracking with opencv.
http://www.pyimagesearch.com/2015/09/14/ball-
tracking-with-opencv/.

[25] Pedro Serra, Rita Cunha, Tarek Hamel, David
Cabecinhas, and Carlos Silvestre. Landing on
a moving target using image-based visual servo
control. In 53rd IEEE Conference on Decision
and Control, pages 2179–2184. IEEE, 2014.
[26] Shital Shah, Debadeepta Dey, Chris Lovett, and
Ashish Kapoor. Aerial Informatics and Robotics
platform. Technical Report MSR-TR-2017-9, Mi-
crosoft Research, 2017.

[27] John Skinner, Sourav Garg, Niko Sünderhauf, Pe-

ter Corke, Ben Upcroft, and Michael Milford.
High-ﬁdelity simulation for evaluating robotic
In Intelligent Robots and
vision performance.
Systems (IROS), 2016 IEEE/RSJ International
Conference on, pages 2737–2744. IEEE, 2016.

8 REFERENCES

[1] Ardupilot

fork.
https://github.com/orig74/ardupilot.
[2] Drone tracking drone in dronesimlab.

https://youtu.be/Mj9xZECG40Q.

[3] Dronesimlab.

https://github.com/orig74/DroneSimLab.

[4] ﬂightgear. http://www.ﬂightgear.org.
[5] gazebo. http://gazebosim.org.
[6] Github. https://github.com.
[7]
[8] Px4

jmavsim. https://pixhawk.org/dev/hil/jmavsim.
fork.

ﬁrmware

https://github.com/orig74/Firmware.

[9] Scene

capture

2d.

https://docs.unrealengine.com/latest/INT/Resources
/ContentExamples/Reﬂections/1_7.

[10] Ue4pyserver.

https://github.com/orig74/UE4PyServer.
[11] Unreal engine 4 with python & opencv.

https://youtu.be/q8kAooRaf7g.
[12] X-plane. http://www.x-plane.com/.
[13] Ilya Afanasyev, Artur Sagitov, and Evgeni Magid.

Ros-based slam for a gazebo-simulated mobile
robot in image-based 3d model of indoor environ-
ment. In International Conference on Advanced
Concepts for Intelligent Vision Systems, pages
273–283. Springer, 2015.

[14] Jean-Yves Bouguet. Pyramidal implementation of
the afﬁne lucas kanade feature tracker description
of the algorithm. Intel Corporation, 5(1-10):4,
2001.

[15] G. Bradski. The opencv library. Dr. Dobb’s Jour-

nal of Software Tools, 2000.

[16] Arnaud Degroote, Pierrick Koch, and Simon

Lacroix. Integrating Realistic Simulation Engines
In Workshop
within the MORSE Framework.
on Rapid and Repeatable Robot Simulation (R4
SIM), at Robotics: Science and Systems, Roma,
Italy, July 2015.

[17] Gilberto Echeverria, Nicolas Lassabe, Arnaud

Degroote, and Séverin Lemaignan. Modular open
robots simulation engine: Morse.
and Automation (ICRA), 2011 IEEE International
Conference on, pages 46–51. IEEE, 2011.

In Robotics

[18] Epic Games.

Unreal engine 4.

http://www.unrealengine.com.

[19] Adam Lerer, Sam Gross, and Rob Fergus. Learn-
ing physical intuition of block towers by example.
CoRR, abs/1603.01312, 2016.

[20] Johannes Meyer, Alexander Sendobry, Stefan

Kohlbrecher, Uwe Klingauf, and Oskar Von Stryk.
Comprehensive simulation of quadrotor uavs us-

