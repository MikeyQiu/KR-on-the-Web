Generalized Low Rank Models

Madeleine Udell, Corinne Horn, Reza Zadeh, and Stephen Boyd

May 6, 2015. (Original version posted September 2014.)

Abstract

Principal components analysis (PCA) is a well-known technique for approximating
a tabular data set by a low rank matrix. Here, we extend the idea of PCA to handle
arbitrary data sets consisting of numerical, Boolean, categorical, ordinal, and other
data types. This framework encompasses many well known techniques in data analysis,
such as nonnegative matrix factorization, matrix completion, sparse and robust PCA,
k-means, k-SVD, and maximum margin matrix factorization. The method handles
heterogeneous data sets, and leads to coherent schemes for compressing, denoising,
and imputing missing entries across all data types simultaneously.
It also admits a
number of interesting interpretations of the low rank factors, which allow clustering of
examples or of features. We propose several parallel algorithms for ﬁtting generalized
low rank models, and describe implementations and numerical results.

This manuscript is a draft. Comments sent to udell@stanford.edu are welcome.

5
1
0
2
 
y
a
M
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
2
4
3
0
.
0
1
4
1
:
v
i
X
r
a

1

Contents

1 Introduction

1.1 Previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 PCA and quadratically regularized PCA

2.1 PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Quadratically regularized PCA . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Missing data and matrix completion . . . . . . . . . . . . . . . . . . . . . . .
Interpretations and applications . . . . . . . . . . . . . . . . . . . . . . . . .
2.5
2.6 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Generalized regularization

3.1 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Generalized loss functions

4.1 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Examples
4.3 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Loss functions for abstract data types

5.1 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Missing data and data imputation . . . . . . . . . . . . . . . . . . . . . . . .
5.4
Interpretations and applications . . . . . . . . . . . . . . . . . . . . . . . . .
5.5 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.6 Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Multi-dimensional loss functions

6.1 Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.2 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.3 Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7 Fitting low rank models

7.1 Alternating minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.2 Early stopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.3 Quadratic objectives
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.4 Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.5
Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.6 Global optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

4
5
7

8
8
9
9
12
13
15

15
16
17
21

22
22
22
25

26
26
27
29
30
32
32

38
39
42
42

42
44
45
48
48
49
52

8 Choosing low rank models

8.1 Regularization paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8.2 Choosing model parameters
. . . . . . . . . . . . . . . . . . . . . . . . . . .
8.3 On-line optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9 Implementations

9.1 Python implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.2 Julia implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.3 Spark implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A Quadratically regularized PCA

A.1 Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.2 Fixed points of alternating minimization . . . . . . . . . . . . . . . . . . . .

56
56
58
61

62
63
64
67

70
70
71

3

1

Introduction

In applications of machine learning and data mining, one frequently encounters large collec-
tions of high dimensional data organized into a table. Each row in the table represents an
example, and each column a feature or attribute. These tables may have columns of diﬀerent
(sometimes, non-numeric) types, and often have many missing entries.

For example, in medicine, the table might record patient attributes or lab tests: each row
of the table lists test or survey results for a particular patient, and each column corresponds
to a distinct test or survey question. The values in the table might be numerical (3.14),
Boolean (yes, no), ordinal (never, sometimes, always), or categorical (A, B, O). Tests not
administered or questions left blank result in missing entries in the data set. Other examples
abound:
in ﬁnance, the table might record known characteristics of companies or asset
classes; in social science settings, it might record survey responses; in marketing, it might
record known customer characteristics and purchase history.

Exploratory data analysis can be diﬃcult in this setting. To better understand a complex
data set, one would like to be able to visualize archetypical examples, to cluster examples,
to ﬁnd correlated features, to ﬁll in (impute) missing entries, and to remove (or simply
identify) spurious, anomalous, or noisy data points. This paper introduces a templated
method to enable these analyses even on large data sets with heterogeneous values and
with many missing entries. Our approach will be to embed both the rows (examples) and
columns (features) of the table into the same low dimensional vector space. These low
dimensional vectors can then be plotted, clustered, and used to impute missing entries or
identify anomalous ones.

If the data set consists only of numerical (real-valued) data, then a simple and well-
known technique to ﬁnd this embedding is Principal Components Analysis (PCA). PCA
ﬁnds a low rank matrix that minimizes the approximation error, in the least-squares sense,
to the original data set. A factorization of this low rank matrix embeds the original high
dimensional features into a low dimensional space. Extensions of PCA can handle missing
data values, and can be used to impute missing entries.

Here, we extend PCA to approximate an arbitrary data set by replacing the least-squares
error used in PCA with a loss function that is appropriate for the given data type. Another
extension beyond PCA is to add regularization on the low dimensional factors to impose or
encourage some structure, such as sparsity or nonnegativity, in the low dimensional factors.
In this paper we use the term generalized low rank model (GLRM) to refer to the problem
of approximating a data set as a product of two low dimensional factors by minimizing
an objective function. The objective will consist of a loss function on the approximation
error together with regularization of the low dimensional factors. With these extensions of
PCA, the resulting low rank representation of the data set still produces a low dimensional
embedding of the data set, as in PCA.

Many of the low rank modeling problems we must solve will be familiar. We recover an
optimization formulation of nonnegative matrix factorization, matrix completion, sparse and
robust PCA, k-means, k-SVD, and maximum margin matrix factorization, to name just a
few.

4

These low rank approximation problems are not convex, and in general cannot be solved
globally and eﬃciently. There are a few exceptional problems that are known to have con-
vex relaxations which are tight under certain conditions, and hence are eﬃciently (globally)
solvable under these conditions. However, all of these approximation problems can be heuris-
tically (locally) solved by methods that alternate between updating the two factors in the low
rank approximation. Each step involves either a convex problem, or a nonconvex problem
that is simple enough that we can solve it exactly. While these alternating methods need
not ﬁnd the globally best low rank approximation, they are often very useful and eﬀective
for the original data analysis problem.

1.1 Previous work

Uniﬁed views of matrix factorization. We are certainly not the ﬁrst to note that
matrix factorization algorithms may be viewed in a uniﬁed framework, parametrized by a
small number of modeling decisions. The ﬁrst instance we ﬁnd in the literature of this
uniﬁed view appeared in a paper by Collins, Dasgupta, and Schapire, [CDS01], extending
PCA to use loss functions derived from any probabilistic model in the exponential family.
Gordon’s Generalized2 Linear2 models [Gor02] extended the framework to loss functions
derived from the generalized Bregman divergence of any convex function, which includes
models such as Independent Components Analysis (ICA). Srebro’s 2004 PhD thesis [Sre04]
extended the framework to other loss functions, including hinge loss and KL-divergence loss,
and to other regularizers, including the nuclear norm and max-norm. Similarly, Chapter 8 in
Tropp’s 2004 PhD thesis [Tro04] explored a number of new regularizers, presenting a range
of clustering problems as matrix factorization problems with constraints, and anticipated
the k-SVD algorithm [AEB06]. Singh and Gordon [SG08] oﬀered a complete view of the
state of the literature on matrix factorization in Table 1 of their 2008 paper, and noted that
by changing the loss function and regularizer, one may recover algorithms including PCA,
weighted PCA, k-means, k-medians, (cid:96)1 SVD, probabilistic latent semantic indexing (pLSI),
nonnegative matrix factorization with (cid:96)2 or KL-divergence loss, exponential family PCA,
and MMMF. Witten et al. introduced the statistics community to sparsity-inducing matrix
factorization in a 2009 paper on penalized matrix decomposition, with applications to sparse
PCA and canonical correlation analysis [WTH09]. Recently, Markovsky’s monograph on low
rank approximation [Mar12] reviewed some of this literature, with a focus on applications
in system, control, and signal processing. The GLRMs discussed in this paper include all of
these models, and many more.

Heterogeneous data. Many authors have proposed the use of low rank models as a
tool for integrating heterogeneous data. The earliest example of this approach is canonical
correlation analysis, developed by Hotelling [Hot36] in 1936 to understand the relations
between two sets of variates in terms of the eigenvectors of their covariance matrix. This
approach was extended by Witten et al. [WTH09] to encourage structured (e.g., sparse)
In the 1970s, De Leeuw et al. proposed the use of low rank models to ﬁt data
factors.

5

measured in nominal, ordinal and cardinal levels [DLYT76]. More recently, Goldberg et
al. [GRX+10] used a low rank model to perform transduction (i.e., multi-label learning)
in the presence of missing data by ﬁtting a low rank model to the features and the labels
simultaneously. Low rank models have also been used to embed image, text and video data
into a common low dimensional space [GD14], and have recently come into vogue in the
natural language processing community as a means to embed words and documents into a
low dimensional vector space [MCCD13, MSC+13, PSM14, SM14].

Algorithms.
In general, it can be computationally hard to ﬁnd the global optimum of a
generalized low rank model. For example, it is NP-hard to compute an exact solution to k-
means [DFK+04], nonnegative matrix factorization [Vav09], and weighted PCA and matrix
completion [GG11], all of which are special cases of low rank models.

However, there are many (eﬃcient) ways to go about ﬁtting a low rank model, by which
we mean ﬁnding a good model with a small objective value. The resulting model may or may
not be the global solution of the low rank optimization problem. We distinguish a model ﬁt
in this way from the solution to an optimization problem, which always refers to the global
solution.

The matrix factorization literature presents a wide variety of methods to ﬁt low rank
models in a variety of special cases. For example, there are variants on alternating min-
imization (with alternating least squares as a special case) [DLYT76, YDLT76, TYDL77,
DL84, DLM09], alternating Newton methods [Gor02, SG08], (stochastic or incremental)
gradient descent [KO09, LRS+10, NRRW11, RRWN11, BRRT12, YYH+13, RR13], conju-
gate gradients [RS05, SJ03], expectation minimization (EM) (or “soft-impute”) methods
[TB99, SJ03, MHT10, HMLZ14], multiplicative updates [LS99], and convex relaxations to
semideﬁnite programs [SRJ04, FHB04, RFP10, FM13].

Generally, expectation minimization, which proceeds by iteratively imputing missing en-
tries in the matrix and solving the fully observed problem, has been found to underperform
relative to other methods [SG08]. However, when used in conjunction with computational
tricks exploiting a particular problem structure, such as Gram matrix caching, these methods
can still work extremely well [HMLZ14].

Semideﬁnite programming becomes computationally intractable for very large (or even
just large) scale problems [RS05]. However, a theoretical analysis of optimality conditions for
rank-constrainted semideﬁnite programs [BM03c] has led to a few algorithms for semideﬁnite
programming based on matrix factorization [BM03b, ABEV09, JBAS10] which guarantee
global optimality and converge quickly if the global solution to the problem is exactly low
rank. Fast approximation algorithms for rank-constrained semideﬁnite programs have also
been developed [SSGS11].

Recently, there has been a resurgence of interest in methods based on alternating min-
imization, as numerous authors have shown that alternating minimization (suitably initial-
ized, and under a few technical assumptions) provably converges to the global minimum
for a range of problems including matrix completion [Kes12, JNS13, Har13], robust PCA
[NNS+14], and dictionary learning [AAJN13].

6

Gradient descent methods are often preferred for extremely large scale problems since
these methods parallelize naturally in both shared memory and distributed memory archi-
tectures. See [RR13, YYH+13] and references therein for some recent innovative approaches
to speeding up stochastic gradient descent for matrix factorization by eliminating locking
and reducing interprocess communication.

Contributions. The present paper diﬀers from previous work in a number of ways. We
are consistently concerned with the meaning of applying these diﬀerent loss functions and
regularizers to approximate a data set. The generality of our view allows us to introduce a
number of loss functions and regularizers that have not previously been considered. More-
over, our perspective enables us to extend these ideas to arbitrary data sets, rather than just
matrices of real numbers.

A number of new considerations emerge when considering the problem so broadly. First,
we must face the problem of comparing approximation errors across data of diﬀerent types.
For example, we must choose a scaling to trade oﬀ the loss due to a misclassiﬁcation of a
categorical value with an error of 0.1 (say) in predicting a real value.

Second, we require algorithms that can handle the full gamut of losses and regulariz-
ers, which may be smooth or nonsmooth, ﬁnite or inﬁnite valued, with arbitrary domain.
This work is the ﬁrst to consider these problems in such generality, and therefore also the
ﬁrst to wrestle with the algorithmic consequences. Below, we give a number of algorithms
appropriate for this setting, including many that have not been previously proposed in the
literature. Our algorithms are all based on alternating minimization and variations on al-
ternating minimization that are more suitable for large scale data and can take advantage
of parallel computing resources.

Finally, we present some new results on some old problems. For example, in Appendix A,
we derive a formula for the solution to quadratically regularized PCA, and show that quadrat-
ically regularized PCA has no local nonglobal minima; and in §7.6 we show how to certify
(in some special cases) that a model is a global solution of a GLRM.

1.2 Organization

The organization of this paper is as follows. In §2 we ﬁrst recall some properties of PCA
and its common variations to familiarize the reader with our notation. We then generalize
the regularization on the low dimensional factors in §3, and the loss function on the ap-
proximation error in §4. Returning to the setting of heterogeneous data, we extend these
dimensionality reduction techniques to abstract data types in §5 and to multi-dimensional
loss functions in §6. Finally, we address algorithms for ﬁtting GLRMs in §7, discuss a few
practical considerations in choosing a GLRM for a particular problem in §8, and describe
some implementations of the algorithms that we have developed in §9.

7

2 PCA and quadratically regularized PCA

In this section, we let A ∈ Rm×n be a data matrix consisting of m examples
Data matrix.
each with n numerical features. Thus Aij ∈ R is the value of the jth feature in the ith
example, the ith row of A is the vector of n feature values for the ith example, and the jth
column of A is the vector of the jth feature across our set of m examples.

It is common to represent other data types in a numerical matrix using certain canonical
encoding tricks. For example, Boolean data is often encoded as 1 (for true) and -1 (for
false), ordinal data is often encoded using consecutive integers to represent the consecutive
levels of the variable, and categorical data is often encoded by creating a column for each
possible value of the categorical variable, and representing the data using a 1 in the column
corresponding to the observed value, and -1 or 0 in all other columns. We will see more
systematic and principled ways to deal with these data types, and others, in §4–6. For now,
we assume the entries in the data matrix consist of real numbers.

2.1 PCA

solving

Principal components analysis (PCA) is one of the oldest and most widely used tools in data
analysis [Pea01, Hot33, Jol86]. We review some of its well-known properties here in order to
set notation and as a warm-up to the variants presented later.

PCA seeks the best rank-k approximation to the matrix A in the least-squares sense, by

minimize
subject to Rank(Z) ≤ k,
with variable Z ∈ Rm×n. Here, (cid:107) · (cid:107)F is the Frobenius norm of a matrix, i.e., the square root
of the sum of the squares of the entries.

(cid:107)A − Z(cid:107)2
F

(1)

The rank constraint can be encoded implicitly by expressing Z in factored form as Z =

XY , with X ∈ Rm×k, Y ∈ Rk×n. Then the PCA problem can be expressed as

minimize (cid:107)A − XY (cid:107)2
F

(2)

with variables X ∈ Rm×k and Y ∈ Rk×n. (The factorization of Z is of course not unique.)
Deﬁne xi ∈ R1×n to be the ith row of X, and yj ∈ Rm to be the jth column of Y . Thus
xiyj = (XY )ij ∈ R denotes a dot or inner product. (We will use this notation throughout
the paper.) Using this deﬁnition, we can rewrite the objective in problem (2) as
n
(cid:88)

m
(cid:88)

(Aij − xiyj)2.

i=1

j=1

We will give several interpretations of the low rank factorization (X, Y ) solving (2) in
§2.5. But for now, we note that (2) can interpreted as a method for compressing the n
features in the original data set to k < n new features. The row vector xi is associated with
example i; we can think of it as a feature vector for the example using the compressed set
of k < n features. The column vector yj is associated with the original feature j; it can be
interpreted as mapping the k new features onto the original feature j.

8

2.2 Quadratically regularized PCA

We can add quadratic regularization on X and Y to the objective. The quadratically regu-
larized PCA problem is

minimize (cid:80)m

(cid:80)n

j=1(Aij − xiyj)2 + γ (cid:80)m

i=1 (cid:107)xi(cid:107)2

2 + γ (cid:80)n

j=1 (cid:107)yj(cid:107)2
2,

i=1

with variables X ∈ Rm×k and Y ∈ Rk×n, and regularization parameter γ ≥ 0. Problem (3)
can be written more concisely in matrix form as

minimize (cid:107)A − XY (cid:107)2

F + γ(cid:107)X(cid:107)2

F + γ(cid:107)Y (cid:107)2
F .

When γ = 0, the problem reduces to the PCA problem (2).

2.3 Solution methods

Singular value decomposition.
It is well known that a solution to (2) can be obtained
by truncating the singular value decomposition (SVD) of A [EY36]. The (compact) SVD
of A is given by A = U ΣV T , where U ∈ Rm×r and V ∈ Rn×r have orthonormal columns,
and Σ = diag(σ1, . . . , σr) ∈ Rr×r, with σ1 ≥ · · · ≥ σr > 0 and r = Rank(A). The columns
of U = [u1 · · · ur] and V = [v1 · · · vr] are called the left and right singular vectors of A,
respectively, and σ1, . . . , σr are called the singular values of A.

Using the orthogonal invariance of the Frobenius norm, we can rewrite the objective in

problem (1) as

(cid:107)A − XY (cid:107)2

F = (cid:107)Σ − U T XY V (cid:107)2
F .

That is, we would like to ﬁnd a matrix U T XY V of rank no more than k approximating the
diagonal matrix Σ. It is easy to see that there is no better rank k approximation for Σ than
Σk = diag(σ1, . . . , σk, 0, . . . , 0) ∈ Rr×r. Here we have truncated the SVD to keep only the
top k singular values. We can achieve this approximation by choosing U T XY V = Σk, or
(using the orthogonality of U and V ) XY = U ΣkV T . For example, deﬁne

and let

Uk = [u1 · · · uk],

Vk = [v1 · · · vk],

X = UkΣ1/2
k ,

Y = Σ1/2

k V T
k .

The solution to (3) is clearly not unique: if X, Y is a solution, then so is XG, G−1Y for any
invertible matrix G ∈ Rk×k. When σk > σk+1, all solutions to the PCA problem have this
form. In particular, letting G = tI and taking t → ∞, we see that the solution set of the
PCA problem is unbounded.

It is less well known that a solution to the quadratically regularized PCA problem can
be obtained in the same way. (Proofs for the statements below can be found in Appendix
A.) Deﬁne Uk and Vk as above, and let ˜Σk = diag((σ1 − γ)+, . . . , (σk − γ)+), where (a)+ =
max(a, 0). Here we have both truncated the SVD to keep only the top k singular values, and

(3)

(4)

(5)

(6)

9

performed soft-thresholding on the singular values to reduce their values by γ. A solution to
the quadratically regularized PCA problem (3) is then given by

X = Uk

˜Σ1/2
k ,

Y = ˜Σ1/2

k V T
k .

(7)

For γ = 0, the solution reduces to the familiar solution to PCA (2) obtained by truncating
the SVD to the top k singular values.

The set of solutions to problem (3) is signiﬁcantly smaller than that of problem (2),
although solutions are still not unique: if X, Y is a solution, then so is XT , T −1Y for any
orthogonal matrix T ∈ Rk×k. When σk > σk+1, all solutions to (3) have this form.
In
particular, adding quadratic regularization results in a solution set that is bounded.

The quadratically regularized PCA problem (3) (including the PCA problem as a special
case) is the only problem we will encounter for which an analytical solution exists. The
analytical tractability of PCA explains its popularity as a technique for data analysis in
the era before computers were machines. For example, in his 1933 paper on PCA [Hot33],
Hotelling computes the solution to his problem using power iteration to ﬁnd the eigenvalue
decomposition of the matrix AT A = V Σ2V T , and records in the appendix to his paper the
itermediate results at each of the (three) iterations required for the method to converge.

Alternating minimization. Here we mention a second method for solving (3), which
extends more readily to the extensions of PCA that we discuss below. The alternating
minimization algorithm simply alternates between minimizing the objective over the variable
X, holding Y ﬁxed, and then minimizing over Y , holding X ﬁxed. With an initial guess for
the factors Y 0, we repeat the iteration
(cid:32) m
(cid:88)

n
(cid:88)

m
(cid:88)

(cid:33)

X l = argmin

(Aij − xiyl−1

)2 + γ

j

(cid:107)xi(cid:107)2
2

Y l = argmin

(Aij − xl

iyj)ij)2 + γ

(cid:107)yj(cid:107)2
2

X

Y

i=1
(cid:32) m
(cid:88)

j=1

n
(cid:88)

i=1

j=1

(cid:33)

i=1

n
(cid:88)

j=1

for l = 1, . . . until a stopping condition is satisﬁed. (If X and Y are full rank, or γ > 0, the
minimizers above are unique; when they are not, we can take any minimizer.) The objective
function is nonincreasing at each iteration, and therefore bounded. This implies, for γ > 0,
that the iterates X l and Y l are bounded.

This algorithm does not always work. In particular, it has stationary points that are not
solutions of problem (3). In particular, if the rows of Y l lie in a subspace spanned by a subset
of the (right) singular vectors of A, then the columns of X l+1 will lie in a subspace spanned
by the corresponding left singular vectors of A, and vice versa. Thus, if the algorithm is
initialized with Y 0 orthogonal to any of the top k (right) singular vectors, then the algorithm
(implemented in exact arithmetic) will not converge to the global solution to the problem.
But all stable stationary points of the iteration are solutions (see Appendix A). So as
a practical matter, the alternating minimization method always works, i.e., the objective
converges to the optimal value.

10

Parallelizing alternating minimization. Alternating minimization parallelizes easily
over examples and features. The problem of minimizing over X splits into m independent
minimization problems. We can solve the simple quadratic problems

minimize (cid:80)n

j=1(Aij − xiyj)2 + γ(cid:107)xi(cid:107)2
2

with variable xi, in parallel, for i = 1, . . . , m. Similarly, the problem of minimizing over Y
splits into n independent quadratic problems,

minimize (cid:80)m

i=1(Aij − xiyj)2 + γ(cid:107)yj(cid:107)2
2

with variable yj, which can be solved in parallel for j = 1, . . . , n.

(8)

(9)

Caching factorizations. We can speed up the solution of the quadratic problems using
a simple factorization caching technique.

For ease of exposition, we assume here that X and Y have full rank k. The updates (8)

and (9) can be expressed as

X = AY T (Y Y T + γI)−1,

Y = (X T X + γI)−1X T A.

We show below how to eﬃciently compute X = AY T (Y Y T + γI)−1; the Y update admits a
similar speedup using the same ideas. We assume here that k is modest, say, not more than
a few hundred or a few thousand. (Typical values used in applications are often far smaller,
on the order of tens.) The dimensions m and n, however, can be very large.

First compute the Gram matrix G = Y Y T using an outer product expansion

This sum can be computed on-line by streaming over the index j, or in parallel, split over
the index j. This property allows us to scale up to extremely large problems even if we
cannot store the entire matrix Y in memory. The computation of the Gram matrix requires
2k2n ﬂoating point operations (ﬂops), but is trivially parallelizable: with r workers, we can
expect a speedup on the order of r. We next add the diagonal matrix γI to G in k ﬂops,
and form the Cholesky factorization of G + γI in k3/3 ﬂops and cache the factorization.

In parallel over the rows of A, we compute D = AY T (2kn ﬂops per row), and use the
factorization of G + γI to compute D(G + γI)−1 with two triangular solves (2k2 ﬂops per
row). These computations are also trivially parallelizable: with r workers, we can expect a
speedup on the order of r.

Hence the total time required for each update with r workers scales as O( k2(m+n)+kmn

).

For k small compared to m and n, the time is dominated by the computation of AY T .

r

G =

yjyT
j .

n
(cid:88)

j=1

11

2.4 Missing data and matrix completion

Suppose we observe only entries Aij for (i, j) ∈ Ω ⊂ {1, . . . , m} × {1, . . . , n} from the matrix
A, so the other entries are unknown. Then to ﬁnd a low rank matrix that ﬁts the data well,
we solve the problem

minimize (cid:80)

(i,j)∈Ω(Aij − xiyj)2 + γ(cid:107)X(cid:107)2

F + γ(cid:107)Y (cid:107)2
F ,

(10)

with variables X and Y , with γ > 0. A solution of this problem gives an estimate ˆAij = xiyj
for the value of those entries (i, j) (cid:54)∈ Ω that were not observed. In some applications, this
data imputation (i.e., guessing entries of a matrix that are not known) is the main point.
There are two very diﬀerent regimes in which solving the problem (10) may be useful.

Imputing missing entries to borrow strength. Consider a matrix A in which very few
entries are missing. The typical approach in data analysis is to simply remove any rows with
missing entries from the matrix and exclude them from subsequent analysis. If instead we
solve the problem above without removing these aﬀected rows, we “borrow strength” from
the entries that are not missing to improve our global understanding of the data matrix
A. In this regime we are imputing the (few) missing entries of A, using the examples that
ordinarily we would discard.

Low rank matrix completion. Now consider a matrix A in which most entries are
missing, i.e., we only observe relatively few of the mn elements of A, so that by discarding
every example with a missing feature or every feature with a missing example, we would
discard the entire matrix. Then the solution to (10) becomes even more interesting: we are
guessing all the entries of a (presumed low rank) matrix, given just a few of them. It is a
surprising fact that this is possible: typical results from the matrix completion literature show
that one can recover an unknown m×n matrix A of low rank r from just about nr log2 n noisy
samples Ω with an error that is proportional to the noise level [CR08, CT10, RFP10, CP09],
so long as the matrix A satisﬁes a certain incoherence condition and the samples Ω are chosen
uniformly at random. These works use an estimator that minimizes a nuclear norm penalty
along with a data ﬁtting term to encourage low rank structure in the solution.

The argument in §7.6 shows that problem (10) is equivalent to the rank-constrained

nuclear-norm regularized convex problem

minimize (cid:80)
subject to Rank(Z) ≤ k,

(i,j)∈Ω(Aij − Zij)2 + 2γ(cid:107)Z(cid:107)∗

where the nuclear norm (cid:107)Z(cid:107)∗ (also known as the trace norm) is deﬁned to be the sum of the
singular values of Z. Thus, the solutions to problem (10) correspond exactly to the solutions
of these proposed estimators so long as the rank k of the model is chosen to be larger than
the true rank r of the matrix A. Nuclear norm regularization is often used to encourage
solutions of rank less than k, and has applications ranging from graph embedding to linear
system identiﬁcation [FHB04, LV09, MF10, Smi12, Osn14].

12

Low rank matrix completion problems arise in applications like predicting customer rat-
ings or customer (potential) purchases. Here the matrix consists of the ratings or numbers
of purchases that m customers give (or make) for each of n products. The vast majority of
the entries in this matrix are missing, since a customer will rate (or purchase) only a small
fraction of the total number of products available. In this application, imputing a missing
entry of the matrix as xiyj, for (i, j) (cid:54)∈ Ω, is guessing what rating a customer would give a
product, if she were to rate it. This can used as the basis for a recommendation system, or
a marketing plan.

Alternating minimization. When Ω (cid:54)= {1, . . . , m} × {1, . . . , n}, the problem (10) has no
known analytical solution, but it is still easy to ﬁt a model using alternating minimization.
Algorithms based on alternating minimization have been shown to converge quickly (even
geometrically [JNS13]) to a global solution satisfying a recovery guarantee when the initial
values of X and Y are chosen carefully [KMO09, KMO10, KM10, JNS13, Har13, GAGG13].
On the other hand, all of these analytical results rely on using a fresh batch of samples
Ω for each iteration of alternating minimization; none uses the quadratic regularizer above
that corresponds to the nuclear norm penalized estimator; and interestingly, Hardt [Har13]
notes that none achieves the same sample complexity guarantees found in the convex ma-
trix completion literature which, unlike the alternating minimization guarantees, match the
information theoretic lower bound [CT10] up to logarithmic factors. For these reasons, it
is plausible to expect that in practice using alternating minimization to solve problem (10)
might yield a better solution than the “alternating minimization” algorithms presented in
the literature on matrix completion when suitably initialized (for example, using the method
proposed below in §7.5). However, in general the method should be considered a heuristic.

2.5 Interpretations and applications

The recovered matrices X and Y in the quadratically regularized PCA problems (3) and
(10) admit a number of interesting interpretations. We introduce some of these interpreta-
tions now; the terminology we use here will recur throughout the paper. Of course these
interpretations are related to each other, and not distinct.

Feature compression. Quadratically regularized PCA (3) can be interpreted as a method
for compressing the n features in the original data set to k < n new features. The row vector
xi is associated with example i; we can think of it as a feature vector for the example using
the compressed set of k < n features. The column vector yj is associated with the original
feature j; it can be interpreted as the mapping from the original feature j into the k new
features.

Low-dimensional geometric embedding. We can think of each yj as associating feature
j with a point in a low (k-) dimensional space. Similarly, each xi associates example i with
a point in the low dimensional space. We can use these low dimensional vectors to judge

13

which features (or examples) are similar. For example, we can run a clustering algorithm on
the low dimensional vectors yj (or xi) to ﬁnd groups of similar features (or examples).

Archetypes. We can think of each row of Y as an archetype which captures the behavior
of one of k idealized and maximally informative examples. These archetypes might also
be called proﬁles, factors, or atoms. Every example i = 1, . . . , m is then represented (ap-
proximately) as a linear combination of these archetypes, with the row vector xi giving the
coeﬃcients. The coeﬃcient xil gives the resemblance or loading of example i to the lth
archetype.

Archetypical representations. We call xi the representation of example i in terms of the
archetypes. The rows of X give an embedding of the examples into Rk, where each coordinate
axis corresponds to a diﬀerent archetype.
If the archetypes are simple to understand or
interpret, then the representation of an example can provide better intuition about that
example.

The examples can be clustered according to their representations in order to determine
a group of similar examples.
Indeed, one might choose to apply any machine learning
algorithm to the representations xi rather than to the initial data matrix: in contrast to the
initial data, which may consist of high dimensional vectors with noisy or missing entries, the
representations xi will be low dimensional, less noisy, and complete.

Feature representations. The columns of Y embed the features into Rk. Here, we
think of the columns of X as archetypical features, and represent each feature j as a linear
combination of the archetypical features. Just as with the examples, we might choose to
apply any machine learning algorithm to the feature representations. For example, we might
ﬁnd clusters of similar features that represent redundant measurements.

Latent variables. Each row of X represents an example by a vector in Rk. The matrix
Y maps these representations back into Rm. We might think of X as discovering the latent
variables that best explain the observed data. If the approximation error (cid:80)
(i,j)∈Ω(Aij −xiyj)2
is small, then we view these latent variables as providing a good explanation or summary of
the full data set.

Probabilistic intepretation. We can give a probabilistic interpretation of X and Y ,
building on the probabilistic model of PCA developed by Tipping and Bishop [TB99]. We
suppose that the matrices ¯X and ¯Y have entries which are generated by taking independent
samples from a normal distribution with mean 0 and variance γ−1 for γ > 0. The entries in
the matrix ¯X ¯Y are observed with noise ηij ∈ R,

where the noise η in the (i, j)th entry is sampled independently from a standard normal
distribution. We observe each entry (i, j) ∈ Ω. Then to ﬁnd the maximum a posteriori

Aij = ( ¯X ¯Y )ij + ηij,

14

(MAP) estimator (X, Y ) of ( ¯X, ¯Y ), we solve
(cid:1) exp (cid:0)− γ

maximize exp (cid:0)− γ

2 (cid:107) ¯X(cid:107)2

F

which is equivalent, by taking logs, to (3).

2 (cid:107) ¯Y (cid:107)2

F

(cid:1) (cid:81)

(i,j)∈Ω exp (−(Aij − xiyj)2) ,

This interpretation explains the recommendation we gave above for imputing missing
observations (i, j) (cid:54)∈ Ω. We simply use the MAP estimator xiyj to estimate the missing
entry ( ¯X ¯Y )ij. Similarly, we can interpret (XY )ij for (i, j) ∈ Ω as a denoised version of the
observation Aij.

Auto-encoder. The matrix X encodes the data; the matrix Y decodes it back into the
full space. We can view PCA as providing the best linear auto-encoder for the data; among
all (bi-linear) low rank encodings (X) and decodings (Y ) of the data, PCA minimizes the
squared reconstruction error.

Compression. We impose an information bottleneck [TPB00] on the data by using a low
rank auto-encoder to ﬁt the data. PCA ﬁnds X and Y to maximize the information trans-
mitted through this k-dimensional information bottleneck. We can interpret the solution
as a compressed representation of the data, and use it to eﬃciently store or transmit the
information present in the original data.

2.6 Oﬀsets and scaling

For good practical performance of a generalized low rank model, it is critical to ensure that
model assumptions match the data. We saw above in §2.5 that quadratically regularized
PCA corresponds to a model in which features are observed with N (0, 1) errors. If instead
each column j of XY is observed with N (µj, σ2
j ) errors, our model is no longer unbiased,
and may ﬁt very poorly, particularly if some of the column means µj are large.

For this reason it is standard practice to standardize the data before appplying PCA or
quadratically regularized PCA: the column means are subtracted from each column, and the
columns are normalized by their variances. (This can be done approximately; there is no
need to get the scaling and oﬀset exactly right.) Formally, deﬁne nj = |{i : (i, j) ∈ Ω}|, and
let

µj =

1
nj

(cid:88)

Aij,

(i,j)∈Ω

σ2
j =

1
nj − 1

(cid:88)

(i,j)∈Ω

(Aij − µj)2

estimate the mean and variance of each column of the data matrix. PCA or quadratically
regularized PCA is then applied to the matrix whose (i, j) entry is (Aij − µj)/σj.

3 Generalized regularization

It is easy to see how to extend PCA to allow arbitrary regularization on the rows of X and
columns of Y . We form the regularized PCA problem
(i,j)∈Ω(Aij − xiyj)2 + (cid:80)m

i=1 ri(xi) + (cid:80)n

minimize (cid:80)

j=1 ˜rj(yj),

(11)

15

with variables xi and yj, with given regularizers ri : Rk → R ∪ {∞} and ˜rj : Rk → R ∪ {∞}
for i = 1, . . . , n and j = 1, . . . , m. Regularized PCA (11) reduces to quadratically regularized
PCA (3) when ri = γ(cid:107) · (cid:107)2

2. We do not restrict the regularizers to be convex.

2, ˜rj = γ(cid:107) · (cid:107)2

The objective in problem (11) can be expressed compactly in matrix notation as

(cid:107)A − XY (cid:107)2
i=1 r(xi) and ˜r(Y ) = (cid:80)n

F + r(X) + ˜r(Y ),

where r(X) = (cid:80)n
separable across the rows of X, and the columns of Y , respectively.

j=1 ˜r(yj). The regularization functions r and ˜r are

Inﬁnite values of ri and ˜rj are used to enforce constraints on the values of X and Y . For

example, the regularizer

ri(x) =

(cid:26) 0

x ≥ 0
∞ otherwise,

the indicator function of the nonnegative orthant, imposes the constraint that xi be nonneg-
ative.

Solutions to (11) need not be unique, depending on the choice of regularizers. If X and
Y are a solution, then so are XT and T −1Y , where T is any nonsingular matrix that satisﬁes
r(U T ) = r(U ) for all U and ˜r(T −1V ) = r(V ) for all V .

By varying our choice of regularizers r and ˜r, we are able to represent a wide range of
known models, as well as many new ones. We will discuss a number of choices for regularizers
below, but turn now to methods for solving the regularized PCA problem (11).

3.1 Solution methods

In general, there is no analytical solution for (11). The problem is not convex, even when r
and ˜r are convex. However, when r and ˜r are convex, the problem is bi-convex: it is convex
in X when Y is ﬁxed, and convex in Y when X is ﬁxed.

Alternating minimization. There is no reason to believe that alternating minimization
will always converge to the global minimum of the regularized PCA problem (11). Indeed,
we will see many cases below in which the problem is known to have many local minima.
However, alternating minimization can still be applied in this setting, and it still parallelizes
over the rows of X and columns of Y . To minimize over X, we solve, in parallel,

with variable xi, for i = 1, . . . , m. Similarly, to minimize over Y , we solve, in parallel,

minimize (cid:80)

j:(i,j)∈Ω(Aij − xiyj)2 + ri(xi)

minimize (cid:80)

i:(i,j)∈Ω(Aij − xiyj)2 + ˜rj(yj)

(12)

(13)

with variable yj, for j = 1, . . . , n.

When the regularizers are convex, these problems are convex. When the regularizers
are not convex, there are still many cases in which we can ﬁnd analytical solutions to the
nonconvex subproblems (12) and (13), as we will see below. A number of concrete algorithms,
in which these subproblems are solved explicitly, are given in §7.

16

Caching factorizations. Often, the X and Y updates (12) and (13) reduce to convex
quadratic programs. For example, this is the case for nonnegative matrix factorization,
sparse PCA, and quadratic mixtures (which we deﬁne and discuss below in §3.2). The same
factorization caching of the Gram matrix that was described above in the case of PCA can
be used here to speed up the solution of these updates. Variations on this idea are described
in detail in §7.3.

3.2 Examples

Here and throughout the paper, we present a set of examples chosen for pedagogical clarity,
In all of the examples below, γ > 0 is a parameter that controls
not for completeness.
the strength of the regularization, and we drop the subscripts from r (or ˜r) to lighten the
notation. Of course, it is possible to mix and match these regularizers, i.e., to choose diﬀerent
ri for diﬀerent i, and choose diﬀerent ˜rj for diﬀerent j.

Nonnegative matrix factorization (NNMF). Consider the regularized PCA problem
(11) with r = I+ and ˜r = I+, where I+ is the indicator function of the nonnegative reals.
(Here, and throughout the paper, we deﬁne the indicator function of a set C, to be 0 when
its argument is in C and ∞ otherwise.) Then problem (11) is NNMF: a solution gives the
matrix best approximating A that has a nonnegative factorization (i.e., a factorization into
elementwise nonnegative matrices) [LS99]. It is NP-hard to solve NNMF problems exactly
[Vav09]. However, these problems have a rich analytical structure which can sometimes
be exploited [Gil11, BRRT12, DS14], and a wide range of uses in practice [LS99, SBPP06,
BBL+07, Vir07, KP07, FBD09]. Hence a number of specialized algorithms and codes for
ﬁtting NNMF models are available [LS01, Lin07, KP08a, KP08b, BDKP14, KHP14, KP11].
We can also replace the nonnegativity constraint with any interval constraint. For ex-
ample, r and ˜r can be 0 if all entries of X and Y , respectively, are between 0 and 1, and
inﬁnite otherwise.

Sparse PCA.
If very few of the coeﬃcients of X and Y are nonzero, it can be easier to
interpret the archetypes and representations. We can understand each archetype using only
a small number of features, and can understand each example as a combination of only a
small number of archetypes. To get a sparse version of PCA, we use a sparsifying penalty
as the regularization. Many variants on this basic idea have been proposed, together with a
wide variety of algorithms [dEGJL04, ZHT06, SH08, Mac09, WTH09, RTA12, VCLR13].

For example, we could enforce that no entry Aij depend on more than s columns of X

or of Y by setting r to be the indicator function of a s-sparse vector, i.e.,

and deﬁning ˜r(y) similarly, where card(x) denotes the cardinality (number of nonzero en-
tries) in the vector x. The updates (12) and (13) are not convex using this regularizer, but

r(x) =

(cid:26) 0

card(x) ≤ s

∞ otherwise,

17

one can ﬁnd approximate solutions using a pursuit algorithm (see, e.g., [CDS98, TG07]), or
exact solutions (for small s) using the branch and bound method [LW66, BM03a].

As a simple example, consider s = 1. Here we insist that each xi have at most one
nonzero entry, which means that each example is a multiple of one of the rows of Y . The
X-update is easy to carry out, by evaluating the best quadratic ﬁt of xi with each of the k
rows of Y . This reduces to choosing the row of Y that has the smallest angle to the ith row
of A.

The s-sparse regularization can be relaxed to a convex, but still sparsifying, regularization
using r(x) = (cid:107)x(cid:107)1, ˜r(y) = (cid:107)y(cid:107)1 [ZHT06]. In this case, the X-update reduces to solving a
(small) (cid:96)1-regularized least-squares problem.

Orthogonal nonnegative matrix factorization. One well known property of PCA is
that the principal components obtained (i.e., the columns of X and rows of Y ) can be chosen
to be orthogonal, so X T X and Y Y T are both diagonal. We can impose the same condition
on a nonnegative matrix factorization. Due to nonnegativity of the matrix, two columns
of X cannot be orthogonal if they both have a nonzero in the same row. Conversely, if X
has only one nonzero per row, then its columns are mutually orthogonal. So an orthogonal
nonnegative matrix factorization is identical to to a nonnegativity condition in addition to
the 1-sparse condition described above. Orthogonal nonnegative matrix factorization can be
achieved by using the regularizer

(cid:26) 0

r(x) =

card(x) = 1,

x ≥ 0

∞ otherwise,

and letting ˜r(y) be the indicator of the nonnegative orthant, as in NNMF.

Geometrically, we can interpret this problem as modeling the data A as a union of rays.
Each row of Y , interpreted as a point in Rn, deﬁnes a ray from the origin passing through
that point. Orthogonal nonnegative matrix factorization models each row of X as a point
along one of these rays.

Some authors [DLPP06] have also considered how to obtain a bi-orthogonal nonnegative
matrix factorization, in which both X and Y T have orthogonal columns. By the same
argument as above, we see this is equivalent to requiring both X and Y T to have only one
positive entry per row, with the other entries equal to 0.

Max-norm matrix factorization. We take r = ˜r = φ with

This penalty enforces that

φ(x) =

(cid:26) 0

(cid:107)x(cid:107)2
2 ≤ µ
∞ otherwise.

(cid:107)X(cid:107)2

2,∞ ≤ µ,

(cid:107)Y T (cid:107)2

2,∞ ≤ µ,

18

where the (2, ∞) norm of a matrix X with rows xi is deﬁned as maxi (cid:107)xi(cid:107)2. This is equivalent
to requiring the max-norm (sometimes called the γ2-norm) of Z = XY , which is deﬁned as

(cid:107)Z(cid:107)max = inf{(cid:107)X(cid:107)2,∞(cid:107)Y T (cid:107)2,∞ : XY = Z},

to be bounded by µ. This penalty has been proposed by [LRS+10] as a heuristic for low rank
matrix completion, which can perform better than Frobenius norm regularization when the
low rank factors are known to have bounded entries.

Quadratic clustering. Consider (11) with ˜r = 0. Let r be the indicator function of a
selection, i.e.,

(cid:26) 0

r(x) =

∞ otherwise,

x = el for some l ∈ {1, . . . , k}

where el is the l-th standard basis vector. Thus xi encodes the cluster (one of k) to which
the data vector (Ai1, . . . , Aim) is assigned.

Alternating minimization on this problem reproduces the well-known k-means algorithm
(also known as Lloyd’s algorithm) [Llo82]. The y update (13) is a least squares problem with
the simple solution

Ylj =

(cid:80)

i:(i,j)∈Ω AijXil
(cid:80)
i:(i,j)∈Ω Xil

,

i.e., each row of Y is updated to be the mean of the rows of A assigned to that archetype.
The x update (12) is not a convex problem, but is easily solved. The solution is given
by assigning xi to the closest archetype (often called a cluster centroid in the context of
j=1(Aij − Ylj)2(cid:17)
k-means): xi = el(cid:63) for l(cid:63) = argminl

(cid:16)(cid:80)n

.

Quadratic mixtures. We can also implement partial assignment of data vectors to clus-
ters. Take ˜r = 0, and let r be the indicator function of the set of probability vectors,
i.e.,

r(x) =

(cid:26) 0 (cid:80)k

l=1 xl = 1,

xl ≥ 0

∞ otherwise.

Subspace clustering. PCA approximates a data set by a single low dimensional subspace.
We may also be interested in approximating a data set as a union of low dimensional sub-
spaces. This problem is known as subspace clustering (see [Vid10] and references therein).
Subspace clustering may also be thought of as generalizing quadratic clustering to assign
each data vector to a low dimensional subspace rather than to a single cluster centroid.

To frame subspace clustering as a regularized PCA problem (11), partition the columns
of X into k blocks. Then let r be the indicator function of block sparsity (i.e., r(x) = 0 if
only one block of x has nonzero entries, and otherwise r(x) = ∞).

It is easy to perform alternating minimization on this objective function. This method
is sometimes called the k-planes algorithm [Vid10, Tse00, AM04], which alternates over

19

assigning examples to subspaces, and ﬁtting the subspaces to the examples. Once again, the
X update (12) is not a convex problem, but can be easily solved. Each block of the columns
of X deﬁnes a subspace spanned by the corresponding rows of Y . We compute the distance
from example i (the ith row of A) to each subspace (by solving a least squares problem),
and assign example i to the subspace that minimizes the least squares error by setting xi to
be the solution to the corresponding least squares problem.

Many other algorithms for this problem have also been proposed, such as the k-SVD
[Tro04, AEB06] and sparse subspace clustering [EV09], some with provable guarantees on
the quality of the recovered solution [SC12].

Supervised learning. Sometimes we want to understand the variation that a certain set
of features can explain, and the variance that remains unexplainable. To this end, one
natural strategy would be to regress the labels in the dataset on the features; to subtract
the predicted values from the data; and to use PCA to understand the remaining variance.
This procedure gives the same answer as the solution to a single regularized PCA problem.
Here we present the case in which the features we wish to use in the regression are present
in the data as the ﬁrst column of A. To construct the regularizers, we make sure the ﬁrst
column of A appears as a feature in the supervised learning problem by setting

ri(x) =

(cid:26) r0(x2, . . . , xk+1) x1 = Ai1
otherwise,

∞

where r0 = 0 can be chosen as in any regularized PCA model. The regularization on the
ﬁrst row of Y is the regularization used in the supervised regression, and the regularization
on the other rows will be that used in regularized PCA.

Thus we see that regularized PCA can naturally combine supervised and unsupervised

learning into a single problem.

Feature selection. We can use regularized PCA to perform feature selection. Consider
(11) with r(x) = (cid:107)x(cid:107)2
2 and ˜r(y) = (cid:107)y(cid:107)2. (Notice that we are not using (cid:107)y(cid:107)2
2.) The regularizer
˜r encourages the matrix ˜Y to be column-sparse, so many columns are all zero. If ˜yj = 0,
it means that feature j was uninformative, in the sense that its values do not help much in
predicting any feature in the matrix A (including feature j itself). In this case we say that
feature j was not selected. For this approach to make sense, it is important that the columns
of the matrix A should have mean zero. Alternatively, one can use the de-biasing regularizers
r(cid:48) and ˜r(cid:48) introduced in §3.3 along with the feature selection regularizer introduced here.

Dictionary learning. Dictionary learning (also sometimes called sparse coding) has be-
come a popular method to design concise representations for very high dimensional data
[OF97, LBRN06, MBPS09, MPS+09]. These representations have been shown to perform
well when used as features in subsequent (supervised) machine learning tasks [RBL+07].
In dictionary learning, each row of A is modeled as a linear combination of dictionary
atoms, represented by rows of Y . The total size of the dictionary used is often very large

20

(k (cid:29) max(m, n)), but each example is represented using a very small number of atoms. To
ﬁt the model, one solves the regularized PCA problem (11) with r(x) = (cid:107)x(cid:107)1, to induce spar-
sity in the number of atoms used to represent any given example, and with ˜r(y) = (cid:107)y(cid:107)2
2 or
˜r(y) = I+(c − (cid:107)y(cid:107)2) for some c > 0 ∈ R, in order to ensure the problem is well posed. (Note
that our notation transposes the usual notation in the literature on dictionary learning.)

Mix and match.
It is possible to combine these regularizers to obtain a factorization with
any combination of the above properties. As an example, one may require that both X and
Y be simultaneously sparse and nonnegative by choosing

r(x) = (cid:107)x(cid:107)1 + I+(x) = 1T x + I+(x),

and similarly for ˜r(y). Similarly, [KP07] show how to obtain a nonnegative matrix factor-
ization in which one factor is sparse by using r(x) = (cid:107)x(cid:107)2
2 + I+(y);
they go on to use this factorization as a clustering technique.

1 + I+(x) and ˜r(y) = (cid:107)y(cid:107)2

3.3 Oﬀsets and scaling

In our discussion of the quadratically regularized PCA problem (3), we saw that it can often
be quite important to standardize the data before applying PCA. Conversely, in regularized
PCA problems such as nonnegative matrix factorization, it makes no sense to standardize
the data, since subtracting column means introduces negative entries into the matrix.

A ﬂexible approach is to allow an oﬀset in the model: we solve

minimize (cid:80)

(i,j)∈Ω(Aij − xiyj − µj)2 + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj),

(14)

with variables xi, yj, and µj. Here, µj takes the role of the column mean, and in fact will
be equal to the column mean in the trivial case k = 0.

An oﬀset may be included in the standard form regularized PCA problem (11) by aug-
menting the problem slightly. Suppose we are given an instance of the problem (11), i.e.,
we are given k, r, and ˜r. We can ﬁt an oﬀset term µj by letting k(cid:48) = k + 1 and modifying
the regularizers. Extend the regularization r : Rk → R and ˜r : Rk → R to new regularizers
r(cid:48) : Rk+1 → R and ˜r(cid:48) : Rk+1 → R which enforce that the ﬁrst column of X is constant and
the ﬁrst row of Y is not penalized. Using this scheme, the ﬁrst row of the optimal Y will be
equal to the optimal µ in (14).

Explicitly, let

(cid:26) r(x2, . . . , xk+1) x1 = 1

r(cid:48)(x) =

∞

otherwise,

and ˜r(cid:48)(y) = ˜r(y2, . . . , yk+1). (Here, we identify r(x) = r(x1, . . . , xk) to explicitly show the
dependence on each coordinate of the vector x, and similarly for ˜r.)
It is also possible to introduce row oﬀsets in the same way.

21

4 Generalized loss functions

We may also generalize the loss function in PCA to form a generalized low rank model,

minimize (cid:80)

(i,j)∈Ω Lij(xiyj, Aij) + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj),

(15)

where Lij : R × R → R+ are given loss functions for i = 1, . . . , m and j = 1, . . . , n. Problem
(15) reduces to PCA with generalized regularization when Lij(u, a) = (a − u)2. However,
the loss function Lij can now depend on the data Aij in a more complex way.

4.1 Solution methods

As before, problem (15) is not convex, even when Lij, ri and ˜rj are convex; but if all these
functions are convex, then the problem is bi-convex.

Alternating minimization. Alternating minimization can still be used to ﬁnd a local
minimum, and it is still often possible to use factorization caching to speed up the solution
of the subproblems that arise in alternating minimization. We defer a discussion of how to
solve these subproblems explicitly to §7.

Stochastic proximal gradient method. For use with extremely large scale problems,
we discuss fast variants of the basic alternating minimization algorithm in §7. For example,
we present an alternating directions stochastic proximal gradient method. This algorithm
accesses the functions Lij, ri, and ˜rj only through a subgradient or proximal interface,
allowing it to generalize trivially to nearly any loss function and regularizer. We defer a
more detailed discussion of this method to §7.

4.2 Examples

Weighted PCA. A simple modiﬁcation of the PCA objective is to weight the importance
of ﬁtting each element in the matrix A. In the generalized low rank model, we let Lij(u−a) =
wij(a − u)2, where wij is a weight, and take r = ˜r = 0. Unlike PCA, the weighted PCA
problem has no known analytical solution [SJ03]. In fact, it is NP-hard to ﬁnd an exact
solution to weighted PCA [GG11], although it is not known whether it is always possible to
ﬁnd approximate solutions of moderate accuracy eﬃciently.

Robust PCA. Despite its widespread use, PCA is very sensitive to outliers. Many authors
have proposed a robust version of PCA obtained by replacing least-squares loss with (cid:96)1 loss,
which is less sensitive to large outliers [CLMW11, WGR+09, XCS12]. They propose to solve
the problem

minimize
(cid:107)S(cid:107)1 + (cid:107)Z(cid:107)∗
subject to S + Z = A.

(16)

22

The authors interpret Z as a robust version of the principal components of the data matrix
A, and S as the sparse, possibly large noise corrupting the observations.

We can frame robust PCA as a GLRM in the following way. If Lij(u, a) = |a − u|, and

r(x) = γ

2 (cid:107)x(cid:107)2

2, ˜r(y) = γ

2 (cid:107)y(cid:107)2

2, then (15) becomes
minimize (cid:107)A − XY (cid:107)1 + γ

2 (cid:107)X(cid:107)2

F + γ

2 (cid:107)Y (cid:107)2
F .

Using the arguments in §7.6, we can rewrite the problem by introducing a new variable
Z = XY as

This results in a rank-constrained version of the estimator proposed in the literature on
robust PCA [WGR+09, CLMW11, XCS12]:

minimize
subject to Rank(Z) ≤ k.

(cid:107)A − Z(cid:107)1 + γ(cid:107)Z(cid:107)∗

minimize
subject to S + Z = A

(cid:107)S(cid:107)1 + γ(cid:107)Z(cid:107)∗

Rank(Z) ≤ k,

where we have introduced the new variable S = A − Z.

Huber PCA. The Huber function is deﬁned as
(cid:26) (1/2)x2

huber(x) =

|x| − (1/2)

|x| ≤ 1
|x| > 1.

Using Huber loss,

L(u, a) = huber(u − a),

in place of (cid:96)1 loss also yields an estimator robust to occasionaly large outliers [Hub81]. The
Huber function is less sensitive to small errors |u − a| than the (cid:96)1 norm, but becomes linear
in the error for large errors. This choice of loss function results in a generalized low rank
model formulation that is robust both to large outliers and to small Gaussian perturbations
in the data.

Previously, the problem of Gaussian noise in robust PCA has been treated by decompos-
ing the matrix A = L + S + N into a low rank matrix L, a sparse matrix S, and a matrix
with small Gaussian entries N by minimizing the loss

(cid:107)L(cid:107)∗ + (cid:107)S(cid:107)1 + (1/2)(cid:107)N (cid:107)2
F

over all decompositions A = L + S + N of A [XCS12].

In fact, this formulation is equivalent to Huber PCA with quadratic regularization on
the factors X and Y . The argument showing this is very similar to the one we made above
for robust PCA. The only added ingredient is the observation that

huber(x) = inf{|s| + (1/2)n2 : x = n + s}.

In other words, the Huber function is the inﬁmal convolution of the negative log likelihood
of a gaussian random variable and a laplacian random variable: it represents the most likely
assignment of (additive) blame for the error x to a gaussian error n and a laplacian error s.

23

Robust regularized PCA. We can design robust versions of all the regularized PCA
problems above by the same transformation we used to design robust PCA. Simply replace
the quadratic loss function with an (cid:96)1 or Huber loss function. For example, k-mediods
[KR09, PJ09] is obtained by using (cid:96)1 loss in place of quadratic loss in the quadratic clustering
problem. Similarly, robust subspace clustering [SEC13] can be obtained by using an (cid:96)1 or
Huber penalty in the subspace clustering problem.

Quantile PCA. For some applications, it can be much worse to overestimate the entries
of A than to underestimate them, or vice versa. One can capture this asymmetry by using
the loss function

L(u, a) = α(a − u)+ + (1 − α)(u − a)+

and choosing α ∈ (0, 1) appropriately. This loss function is sometimes called a scalene loss,
and can be interpreted as performing quantile regression, e.g., ﬁtting the 20th percentile
[KB78, Koe05].

Fractional PCA. For other applications, we may be interested in ﬁnding an approxima-
tion of the matrix A whose entries are close to the original matrix on a relative, rather than
an absolute, scale. Here, we assume the entries Aij are all positive. The loss function

L(u, a) = max

(cid:18) a − u
u

,

u − a
a

(cid:19)

can capture this objective. A model (X, Y ) with objective value less than 0.10mn gives a
low rank matrix XY that is on average within 10% of the original matrix.

Logarithmic PCA. Logarithmic loss functions may also useful for ﬁnding an approxima-
tion of A that is close on a relative, rather than absolute, scale. Once again, we assume all
entries of A are positive. Deﬁne the logarithmic loss

This loss is not convex, but has the nice property that it ﬁts the geometric mean of the data:

To see this, note that we are solving a least squares problem in log space. At the solution,
log(u) will be the mean of log(ai), i.e.,

L(u, a) = log2(u/a).

argmin
u

(cid:88)

i

L(u, ai) = (

ai)1/n.

(cid:89)

i

log(u) = 1/n

log(ai) = log

(cid:88)

i

(cid:33)

ai)1/n

.

(cid:32)
(

(cid:89)

i

24

It is easy to formulate a version of PCA corresponding to
Exponential family PCA.
any loss in the exponential family. Here we give some interesting loss functions generated
by exponential families when all the entries Aij are positive. (See [CDS01] for a general
treatment of exponential family PCA.) One popular loss function in the exponential family
is the KL-divergence loss,

which corresponds to a Poisson generative model [CDS01].

Another interesting loss function is the Itakura-Saito (IS) loss,

L(u, a) = a log

− a + u,

(cid:17)

(cid:16) a
u

L(u, a) = log

− 1 +

(cid:17)

(cid:16) a
u

a
u

,

which has the property that it is scale invariant, so scaling a and u by the same factor
produces the same loss [SF14]. The IS loss corresponds to Tweedie distributions (i.e., distri-
butions for which the variance is some power of the mean) [Twe84]. This makes it interesting
in applications, such as audio processing, where fractional errors in recovery are perceived.

The β-divergence,

L(u, a) =

aβ
β(β − 1)

+

−

uβ
β

auβ−1
β − 1

,

generalizes both of these losses. With β = 2, we recover quadratic loss; in the limit as β → 1,
we recover the KL-divergence loss; and in the limit as β → 0, we recover the IS loss [SF14].

4.3 Oﬀsets and scaling

In §2.6, we saw how to use standardization to rescale the data in order to compensate for
unequal scaling in diﬀerent features. In general, standardization destroys sparsity in the data
by subtracting the (column) means (which are in general non-zero) from each element of the
data matrix A. It is possible to instead rescale the loss functions in order to compensate for
unequal scaling. Scaling the loss functions instead has the advantage that no arithmetic is
performed directly on the data A, so sparsity in A is preserved.

A savvy user may be able to select loss functions Lij that are scaled to reﬂect the
importance of ﬁtting diﬀerent columns. However, it is useful to have a default automatic
scaling for times when no savvy user can be found. The scaling proposed here generalizes
the idea of standardization to a setting with heterogeneous loss functions.

Given initial loss functions Lij, which we assume are nonnegative, for each feature j let

µj = argmin

Lij(µ, Aij),

(cid:88)

µ

i:(i,j)∈Ω

σ2
j =

1
nj − 1

(cid:88)

i:(i,j)∈Ω

Lij(µj, Aij).

It is easy to see that µj generalizes the mean of column j, while σ2
j generalizes the column
variance. For example, when Lij(u, a) = (u − a)2 for every i = 1, . . . , m, j = 1, . . . , n, µj is
the mean and σ2
j is the sample variance of the jth column of A. When Lij(u, a) = |u − a| for
every i = 1, . . . , m, j = 1, . . . , n, µj is the median of the jth column of A, and σ2
j is the sum

25

of the absolute values of the deviations of the entries of the jth column from the median
value.

To ﬁt a standardized GLRM, we rescale the loss functions by σ2

j and solve

minimize (cid:80)

(i,j)∈Ω Lij(Aij, xiyj + µj)/σ2

j + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj).

(17)

Note that this problem can be recast in the standard form for a generalized low rank model
(15). For the oﬀset, we may use the same trick described in §3.3 to encode the oﬀset in the
regularization; and for the scaling, we simply replace the original loss function Lij by Lij/σ2
j .

5 Loss functions for abstract data types

We began our study of generalized low rank modeling by considering the best way to ap-
proximate a matrix by another matrix of lower rank. In this section, we apply the same
procedure to approximate a data table that may not consist of real numbers, by choosing a
loss function that respects the data type.

We now consider A to be a table consisting of m examples (i.e., rows, samples) and n
features (i.e., columns, attributes), with each entry Aij drawn from a feature set Fj. The
feature set Fj may be discrete or continuous. So far, we have only considered numerical data
(Fj = R for j = 1, . . . , n), but now Fj can represent more abstract data types. For example,
entries of A can take on Boolean values (Fj = {T, F }), integral values (Fj = 1, 2, 3, . . .),
ordinal values (Fj = {very much, a little, not at all}), or consist of a tuple of these types
(Fj = {(a, b) : a ∈ R}).

We are given a loss function Lij : R × Fj → R. The loss Lij(u, a) describes the approxi-
mation error incurred when we represent a feature value a ∈ Fj by the number u ∈ R. We
give a number of examples of these loss functions below.

We now formulate a generalized low rank model on the database A as

minimize (cid:80)

(i,j)∈Ω Lij(xiyj, Aij) + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj),

(18)

with variables X ∈ Rn×k and Y ∈ Rk×m, and with loss Lij as above and regularizers
ri(xi) : R1×k → R and ˜rj(yj) : Rk×1 → R (as before). When the domain of each loss
function is R × R, we recover the generalized low rank model on a matrix (15).

5.1 Solution methods

As before, this problem is not convex, but it is bi-convex if ri, and ˜rj are convex, and Lij is
convex in its ﬁrst argument. The problem is also separable across samples i = 1, . . . , m and
features j = 1, . . . , m. These properties makes it easy to perform alternating minimization
on this objective. Once again, we defer a discussion of how to solve these subproblems
explicitly to §7.

26

+
)
u
a
−
1
(

4

3

2

1

0

a = −1

a = 1

a = −1

a = 1

−3 −2 −1

1

2

3

−3 −2 −1

1

2

3

0
u

0
u

Figure 1: Hinge loss.

Figure 2: Logistic loss.

5.2 Examples

Boolean PCA. Suppose Aij ∈ {−1, 1}m×n, and we wish to approximate this Boolean
matrix. For example, we might suppose that the entries of A are generated as noisy, 1-
bit observations from an underlying low rank matrix XY . Surprisingly, it is possible to
accurately estimate the underlying matrix with only a few observations |Ω| from the matrix
by solving problem (18) (under a few mild technical conditions) with an appropriate loss
function [DPBW12].

We may take the loss to be

L(u, a) = (1 − au)+,

which is the hinge loss (see Figure 1), and solve the problem (18) with or without regulariza-
tion. When the regularization is sum of squares (r(x) = λ(cid:107)x(cid:107)2
2), ﬁxing X and
minimizing over yj is equivalent to training a support vector machine (SVM) on a data set
consisting of m examples with features xi and labels Aij. Hence alternating minimization
for the problem (15) reduces to repeatedly training an SVM. This model has been previously
considered under the name Maximum Margin Matrix Factorization (MMMF) [SRJ04, RS05].

2, ˜r(y) = λ(cid:107)y(cid:107)2

Logistic PCA. Again supposing Aij ∈ {−1, 1}m×n, we can also use a logistic loss to
measure the approximation quality. Let

L(u, a) = log(1 + exp(−au))

(see Figure 2). With this loss, ﬁxing X and minimizing over yj is equivalent to using logistic
regression to predict the labels Aij. This model has been previously considered under the
name logistic PCA [SSU03].

)
)
u
a
(
p
x
e
+
1
(
g
o
l

3

2

1

0

27

Figure 3: Ordinal hinge loss.

Poisson PCA. Now suppose the data Aij are nonnegative integers. We can use any loss
function that might be used in a regression framework to predict integral data to construct
a generalized low rank model for Poisson PCA. For example, we can take

L(u, a) = exp(u) − au + a log a − a.

This is the exponential family loss corresponding to Poisson data. (It diﬀers from the KL-
divergence loss from §4.2 only in that u has been replaced by exp(u), which allows u to take
negative values.)

Ordinal PCA. Suppose the data Aij records the levels of some ordinal variable, encoded
as {1, 2, . . . , d}. We wish to penalize the entries of the low rank matrix XY which deviate
by many levels from the encoded ordinal value. A convex version of this penalty is given by
the ordinal hinge loss,

L(u, a) =

(1 − u + a(cid:48))+ +

(1 + u − a(cid:48))+,

(19)

a−1
(cid:88)

a(cid:48)=1

d
(cid:88)

a(cid:48)=a+1

which generalizes the hinge loss to ordinal data (see Figure 3).

This loss function may be useful for encoding Likert-scale data indicating degrees of

agreement with a question [Lik32]. For example, we might have

Fj = {strongly disagree, disagree, neither agree nor disagree, agree, strongly agree}.

We can encode these levels as the integers 1, . . . , 5 and use the above loss to ﬁt a model to
ordinal data.

This approach assumes that every increment of error is equally bad: for example, that
approximating “agree” by “strongly disagree” is just as bad as aproximating “neither agree
nor disagree” by “agree”. In §6.1 we introduce a more ﬂexible ordinal loss function that can
learn a more ﬂexible relationship between ordinal labels. For example, it could determine
that the diﬀerence between “agree” and “strongly disagree” is smaller than the diﬀerence
between “neither agree nor disagree” and “agree”.

28

Interval PCA. Suppose that the data Aij ∈ R2 are tuples denoting the endpoints of an
interval, and we wish to ﬁnd a low rank matrix whose entries lie inside these intervals. We
can capture this objective using, for example, the deadzone-linear loss

L(u, a) = max((a1 − u)+, (u − a2)+).

5.3 Missing data and data imputation

We can use the solution (X, Y ) to a low rank model to impute values corresponding to
missing data (i, j) (cid:54)∈ Ω. This process is sometimes also called inference. Above, we saw that
for quadratically regularized PCA, the MAP estimator for the missing entry Aij is equal to
xiyj. This is still true for many of the loss functions above, such as the Huber function or (cid:96)1
loss, for which it makes sense for the data to take on any real value.

However, to approximate abstract data types we must consider a more nuanced view.
While we can still think of the solution (X, Y ) to the generalized low rank model (15) in
Boolean PCA as approximating the Boolean matrix A, the solution is not a Boolean matrix.
Instead we say that we have encoded the original Boolean matrix as a real-valued low rank
matrix XY , or that we have embedded the original Boolean matrix into the space of real-
valued matrices.

To ﬁll in missing entries in the original matrix A, we compute the value ˆAij that minimizes

the loss for xiyj:

ˆAij = argmin

Lij(xiyj, a).

a

This implicitly constrains ˆAij to lie in the domain Fj of Lij. When Lij : R × R → R, as is
the case for the losses in §4 above (including (cid:96)2, (cid:96)1, and Huber loss), then ˆAij = xiyj. But
when the data is of an abstract type, the minimum argmina Lij(u, a) will not in general be
equal to u.

For example, when the data is Boolean, Lij : {0, 1} × R → R, we compute the Boolean

matrix ˆA implied by our low rank model by solving

for MMMF, or

ˆAij = argmin

(a(XY )ij − 1)+

a∈{0,1}

ˆAij = argmin

a∈{0,1}

log(1 + exp(−a(XY )ij))

for logistic PCA. These problems both have the simple solution

ˆAij = sign(xiyj).

When Fj is ﬁnite, inference partitions the real numbers into regions

Ra = {x ∈ R : Lij(u, x) = min

Lij(u, a)}

a

corresponding to diﬀerent values a ∈ Fj. When Lij is convex, these regions are intervals.

29

We can use the estimate ˆAij even when (i, j) ∈ Ω was observed. If the original obser-
vations have been corrupted by noise, we can view ˆAij as a denoised version of the original
data. This is an unusual kind of denoising: both the noisy (Aij) and denoised ( ˆAij) versions
of the data lie in the abstract space Fj.

5.4 Interpretations and applications

We have already discussed some interpretations of X and Y in the PCA setting. Now we
reconsider those interpretations in the context of approximating these abstract data types.

Archetypes. As before, we can think of each row of Y as an archetype which captures the
behavior of an idealized example. However, the rows of Y are real numbers. To represent
each archetype l = 1, . . . , k in the abstract space as Yl with (Yl)j ∈ Fj, we solve

(Yl)j = argmin

Lj(ylj, a).

a∈Fj

(Here we assume that the loss Lij = Lj is independent of the example i.)

Archetypical representations. As before, we call xi the representation of example i in
terms of the archetypes. The rows of X give an embedding of the examples into Rk, where
each coordinate axis corresponds to a diﬀerent archetype. If the archetypes are simple to
understand or interpret, then the representation of an example can provide better intuition
about that example.

In contrast to the initial data, which may consist of arbitrarily complex data types, the
representations xi will be low dimensional vectors, and can easily be plotted, clustered, or
used in nearly any kind of machine learning algorithm. Using the generalized low rank model,
we have converted an abstract feature space into a vector space.

Feature representations. The columns of Y embed the features into Rk. Here we think
of the columns of X as archetypical features, and represent each feature j as a linear com-
bination of the archetypical features. Just as with the examples, we might choose to apply
any machine learning algorithm to the feature representations.

This procedure allows us to compare non-numeric features using their representation in
Rl. For example, if the features F are Likert variables giving the extent to which respondents
on a questionnaire agree with statements 1, . . . , n, we might be able to say that questions i
and j are similar if (cid:107)yi − yj(cid:107) is small; or that question i is a more polarizing form of question
j if yi = αyj, with α > 1.

Even more interesting, it allows us to compare features of diﬀerent types. We could say

that the real-valued feature i is similar to Likert-valued question j if (cid:107)yi − yj(cid:107) is small.

30

Latent variables. Each row of X represents an example by a vector in Rk. The matrix Y
maps these representations back into the original feature space (now nonlinearly) as described
in the discussion on data imputation in §5.3. We might think of X as discovering the latent
variables that best explain the observed data, with the added beneﬁt that these latent
variables lie in the vector space Rk.
(i,j)∈Ω Lij(xiyj, Aij) is
small, then we view these latent variables as providing a good explanation or summary of
the full data set.

If the approximation error (cid:80)

Probabilistic intepretation. We can give a probabilistic interpretation of X and Y ,
generalizing the hierarchical Bayesian model presented by Fithian and Mazumder in [FM13].
We suppose that the matrices ¯X and ¯Y are generated according to a probability distribution
with probability proportional to exp(−r( ¯X)) and exp(−˜r( ¯Y )), respectively. Our observations
A of the entries in the matrix ¯Z = ¯X ¯Y are given by

where the random variable ψij(u) takes value a with probability proportional to

Aij = ψij(( ¯X ¯Y )ij),

exp (−Lij(u, a)) .

We observe each entry (i, j) ∈ Ω. Then to ﬁnd the maximum a posteriori (MAP) estimator
(X, Y ) of ( ¯X, ¯Y ), we solve

maximize exp

(cid:16)

− (cid:80)

(cid:17)
(i,j)∈Ω Lij(xiyj, Aij)

exp(−r(X)) exp(−˜r(Y )),

which is equivalent, by taking logs, to problem (18).

This interpretation gives us a simple way to interpret our procedure for imputing missing

observations (i, j) (cid:54)∈ Ω. We are simply computing the MAP estimator ˆAij.

Auto-encoder. The matrix X encodes the data; the matrix Y decodes it back into the
full space. We can view (18) as providing the best linear auto-encoder for the data. Among
all linear encodings (X) and decodings (Y ) of the data, the abstract generalized low rank
model (18) minimizes the reconstruction error measured according to the loss functions Lij.

Compression. We impose an information bottleneck by using a low rank auto-encoder
to ﬁt the data. The bottleneck is imposed by both the dimensionality reduction and the
regularization, giving both soft and hard constraints on the information content allowed.
The solution (X, Y ) to problem (18) maximizes the information transmitted through this
k-dimensional bottleneck, measured according to the loss functions Lij. This X and Y give
a compressed and real-valued representation that may be used to more eﬃciently store or
transmit the information present in the data.

31

5.5 Oﬀsets and scaling

Just as in the previous section, better practical performance can often be achieved by allowing
an oﬀset in the model as described in §3.3, and automatic scaling of loss functions as described
in §4.3. As we noted in §4.3, scaling the loss functions (instead of standardizing the data)
has the advantage that no arithmetic is performed directly on the data A. When the data A
consists of abstract types, it is quite important that no arithmetic is performed on the data,
so that we need not take the average of, say, “very much” and “a little”, or subtract it from
“not at all”.

5.6 Numerical examples

In this section we give results of some small experiments illustrating the use of diﬀerent loss
functions adapted to abstract data types, and comparing their performance to quadratically
regularized PCA. To ﬁt these GLRMs, we use alternating minimization and solve the sub-
problems with subgradient descent. This approach is explained more fully in §7. Running
the alternating subgradient method multiple times on the same GLRM from diﬀerent initial
conditions yields diﬀerent models, all with very similar (but not identical) objective values.

Boolean PCA. For this experiment, we generate Boolean data A ∈ {−1, +1}n×m as

A = sign (cid:0)X trueY true(cid:1),
where X true ∈ Rn×ktrue and Y true ∈ Rktrue×m have independent, standard normal entries. We
consider a problem instance with m = 50, n = 50, and ktrue = k = 10.

We ﬁt two GLRMs to this data to compare their performance. Boolean PCA uses hinge
loss L(u, a) = max (1 − au, 0) and quadratic regularization r(u) = ˜r(u) = .1(cid:107)u(cid:107)2
2, and pro-
duces the model (X bool, Y bool). Quadratically regularized PCA uses squared loss L(u, a) =
(u − a)2 and the same quadratic regularization, and produces the model (X real, Y real).

Figure 4 shows the results of ﬁtting Boolean PCA to this data. The ﬁrst column shows
the original ground-truth data A; the second shows the imputed data given the model, ˆAbool,
generated by rounding the entries of X boolY bool to the closest number in 0, 1 (as explained in
§5.3); the third shows the error A− ˆAbool. Figure 4 shows the results of running quadratically
regularized PCA on the same data, and shows A, ˆAreal, and A − ˆAreal.

As expected, Boolean PCA performs substantially better than quadratically regularized
PCA on this data set. On average over 100 draws from the ground truth data distribution,
the misclassiﬁcation error (percentage of misclassiﬁed entries)

(cid:15)(X, Y ; A) =

#{(i, j) | Aij (cid:54)= sign (XY )ij}
mn

is much lower using hinge loss ((cid:15)(X bool, Y bool; A) = 0.0016) than squared loss ((cid:15)(X real, Y real; A) =
0.0051). The average RMS errors

RMS(X, Y ; A) =

(Aij − (XY )ij)2

(cid:33)1/2

(cid:32)

1
mn

m
(cid:88)

n
(cid:88)

i=1

j=1

32

using hinge loss (RMS(X bool, Y bool; A) = 0.0816) and squared loss (RMS(X real, Y real; A) =
0.159) also indicate an advantage for Boolean PCA.

Figure 4: Boolean PCA on Boolean data.

Figure 5: Quadratically regularized PCA on Boolean data.

Censored PCA.
In this example, we consider the performance of Boolean PCA when
only a subset of positive entries in the Boolean matrix A ∈ {−1, 1}m×n have been observed,
i.e., the data has been censored. For example, a retailer might know only a subset of the
products each customer purchased; or a medical clinic might know only a subset of the
diseases a patient has contracted, or of the drugs the patient has taken. Imputation can be
used in this setting to (attempt to) distinguish true negatives Aij = −1 from unobserved
positives Aij = +1, (i, j) (cid:54)∈ Ω.

We generate a low rank matrix B = XY ∈ [0, 1]m×n with X ∈ Rm×k, Y ∈ Rk×n, where
the entries of X and Y are drawn from a uniform distribution on [0, 1], m = n = 300 and
k = 3. Our data matrix A is chosen by letting Aij = 1 with probability proportional to
Bij, and −1 otherwise; the constant of proportionality is chosen so that half of the entries
in A are positive. We ﬁt a rank 5 GLRM to an observation set Ω consisting of 10% of the
positive entries in the matrix, drawn uniformly at random, using hinge loss and quadratic

33

regularization. That is, we ﬁt the low rank model

minimize (cid:80)

(i,j)∈Ω max(1 − xiyjAij, 0) + γ (cid:80)m

i=1 (cid:107)xi(cid:107)2

2 + γ (cid:80)n

j=1 (cid:107)yj(cid:107)2
2

and vary the regularization parameter γ.

normalized training error,

We consider three error metrics to measure the performance of the ﬁtted model (X, Y ):

normalized test error,

1
|Ω|

(cid:88)

(i,j)∈Ω

1
|ΩC|

(cid:88)

(i,j)∈ΩC

max(1 − Aijxiyj, 0),

max(1 − Aijxiyj, 0),

and precision at 10 (p@10), which is computed as the fraction of the top ten predicted
values not in the observation set, {xiyj : (i, j) ∈ ΩC}, for which Aij = 1. (Here, ΩC =
{1, . . . , m} × {1, . . . , n} \ Ω.) Precision at 10 measures the usefulness of the model:
if we
predicted that the top 10 unseen elements (i, j) had values +1, how many would we get
right?

Figure 6 shows the regularization path as γ ranges from 0 to 40, averaged over 50 sam-
ples from the distribution generating the data. Here, we see that while the training error
decreases as γ decreases, the test error reaches a minimum around γ = 5. Interestingly, the
precision at 10 improves as the regularization increases; since precision at 10 is computed
using only relative rather than absolute values of the model, it is insensitive to the shrinkage
of the parameters introduced by the regularization. The grey line shows the probability of
identifying a positive entry by guessing randomly; precision at 10, which exceeds 80% when
γ (cid:38) 30, is signiﬁcantly higher. This performance is particularly impressive given that the
observations Ω are generated by sampling from rather than rounding the auxiliary matrix
B.

Mixed data types.
In this experiment, we ﬁt a GLRM to a data table with numerical,
Boolean, and ordinal columns generated as follows. Let N1, N2, and N3 partition the column
indices 1, . . . , n. Choose X true ∈ Rm×ktrue, Y true ∈ Rktrue×n to have independent, standard
normal entries. Assign entries of A as follows:

Aij =






xiyj
sign (xiyj)
round(3xiyj + 1)

j ∈ N1
j ∈ N2
j ∈ N3,

where the function round maps a to the nearest integer in the set {1, . . . , 7}. Thus, N1
corresponds to real-valued data; N2 corresponds to Boolean data; and N3 corresponds to
ordinal data. We consider a problem instance in which m = 100, n1 = 40, n2 = 30, n3 = 30,
and ktrue = k = 10.

34

train error
test error

p@10

r
o
r
r
e

d
e
z
i
l
a
m
r
o
n

8

6

4

2

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1
+

f
o

y
t
i
l
i
b
a
b
o
r
p

0
0

5
5

10
10

15
15

20
20
regularization parameter
regularization parameter

25
25

30
30

35
35

40
40

Figure 6: Error metrics for Boolean GLRM on censored data. The grey line shows
the probability of identifying a positive entry by guessing randomly.

35

We ﬁt a heterogeneous loss GLRM to this data with loss function

Lij(u, a) =






Lreal(u, a)
Lbool(u, a)
Lord(u, a)

j ∈ N1
j ∈ N2
j ∈ N3,

where Lreal(u, a) = (u − a)2, Lbool(u, a) = (1 − au)+, and Lord(u, a) is deﬁned in (19), and
with quadratic regularization r(u) = ˜r(u) = .1(cid:107)u(cid:107)2
2. We ﬁt the GLRM to produce the model
(X mix, Y mix). For comparison, we also ﬁt quadratically regularized PCA to the same data,
using Lij(u, a) = (u − a)2 for all j and quadratic regularization r(u) = ˜r(u) = .1(cid:107)u(cid:107)2
2, to
produce the model (X real, Y real).

Figure 7 shows the results of ﬁtting the heterogeneous loss GLRM to the data. The ﬁrst
column shows the original ground-truth data A; the second shows the imputed data given
the model, ˆAmix, generated by rounding the entries of X mixY mix to the closest number in
0, 1 (as explained in §5.3); the third shows the error A − ˆAmix. Figure 8 corresponds to
quadratically regularized PCA, and shows A, ˆAreal, and A − ˆAreal.

To evaluate error for Boolean and ordinal data, we use the misclassiﬁcation error (cid:15) deﬁned
above. For notational convenience, we let YNl (ANl) denote Y (A) restricted to the columns
Nl in order to pick out real-valued columns (l = 1), Boolean columns (l = 2), and ordinal
columns (l = 3).

Table 1 compare the average error (diﬀerence between imputed entries and ground truth)
over 100 draws from the ground truth distribution for models using heterogeneous loss (X mix,
Y mix) and quadratically regularized loss (X real, Y real). Columns are labeled by error metric.
We use misclassiﬁcation error (cid:15) for Boolean and ordinal data and MSE for numerical data.

X mix, Y mix
X real , Y real

MSE(X, YN1; AN1)
0.0224
0.0076

(cid:15)(X, YN2; AN2)
0.0074
0.0213

(cid:15)(X, YN3; AN3)
0.0531
0.0618

Table 1: Average error for numerical, Boolean, and ordinal features using GLRM
with heterogenous loss and quadratically regularized loss.

Missing data. Here, we explore the eﬀect of missing entries on the accuracy of the re-
covered model. We generate data A as detailed above, but then censor one large block of
entries in the table (constituting 3.75% of numerical, 50% of Boolean, and 50% of ordinal
data), removing them from the observed set Ω.

Figure 9 shows the results of ﬁtting the heterogeneous loss GLRM described above on
the censored data. The ﬁrst column shows the original ground-truth data A; the second
shows the block of data that has been removed from the observation set Ω; the third shows
the imputed data given the model, ˆAmix, generated by rounding the entries of X mixY mix to
the closest number in {0, 1} (as explained in §5.3); the fourth shows the error A − ˆAmix.

36

Figure 7: Heterogeneous loss GLRM on mixed data.

Figure 8: Quadratically regularized PCA on mixed data.

37

Figure 9: Heterogeneous loss GLRM on missing data.

Figure 10: Quadratically regularized PCA on missing data.

Figure 10 corresponds to running quadratically regularized PCA on the same data, and
shows A, ˆAreal, and A − ˆAreal. While quadradically regularized PCA and the heterogeneous
loss GLRM performed similarly when no data was missing, the heterogeneous loss GLRM
performs much better than quadradically regularized PCA when a large block of data is
censored.

We compare the average error (diﬀerence between imputed entries and ground truth) over
100 draws from the ground truth distribution in Table 2. As above, we use misclassiﬁcation
error (cid:15) for Boolean and ordinal data and MSE for numerical data.

X mix, Y mix
X real , Y real

MSE(X, YN1; AN1)
0.392
0.561

(cid:15)(X, YN2; AN2)
0.2968
0.4029

(cid:15)(X, YN3; AN3)
0.3396
0.9418

Table 2: Average error over imputed data using a GLRM with heterogenous loss
and regularized quadratic loss.

6 Multi-dimensional loss functions

In this section, we generalize the procedure to allow the loss functions to depend on blocks
of the matrix XY , which allows us to represent abstract data types more naturally. For
example, we can now represent categorical values , permutations, distributions, and rankings.

38

We are given a loss function Lij : R1×dj ×Fj → R, where dj is the embedding dimension of
feature j, and d = (cid:80)
j dj is the total dimension of the embedded features. The loss Lij(u, a)
describes the approximation error incurred when we represent a feature value a ∈ Fj by the
vector u ∈ Rdj .

Let xi ∈ R1×k be the ith row of X (as before), and let Yj ∈ Rk×dj be the jth block
matrix of Y so the columns of Yj correspond to the columns of embedded feature j. We now
formulate a multi-dimensional generalized low rank model on the database A,

minimize (cid:80)

(i,j)∈Ω Lij(xiYj, Aij) + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(Yj),

(20)

with variables X ∈ Rn×k and Y ∈ Rk×d, and with loss Lij as above and regularizers
ri(xi) : R1×k → R (as before) and ˜rj(Yj) : Rk×dj → R. Note that the ﬁrst argument of Lij
is a row vector with dj entries, and the ﬁrst argument of rj is a matrix with dj columns.
When every entry Aij is real-valued (i.e., dj = 1), then we recover the generalized low rank
model (15) seen in the previous section.

6.1 Examples

Categorical PCA. Suppose that a ∈ F is a categorical variable, taking on one of d values
or labels. Identify the labels with the integers {1, . . . , d}. In (20), set

L(u, a) = (1 − ua)+ +

(1 + ua(cid:48))+,

(cid:88)

a(cid:48)∈F , a(cid:48)(cid:54)=a

and use the quadratic regularizer ri = γ(cid:107) · (cid:107)2

2, ˜r = γ(cid:107) · (cid:107)2
2.

Fixing X and optimizing over Y is equivalent to training one SVM per label to separate
that label from all the others: the jth column of Y gives the weight vector corresponding
to the jth SVM. (This is sometimes called one-vs-all multiclass classiﬁcation [RK04].) Op-
timizing over X identiﬁes the low-dimensional feature vectors for each example that allow
these SVMs to most accurately predict the labels.

The diﬀerence between categorical PCA and Boolean PCA is in how missing labels are
imputed. To impute a label for entry (i, j) with feature vector xi according to the procedure
described above in 5.3, we project the representation Yj onto the line spanned by xi to form
u = xiYj. Given u, the imputed label is simply argmaxl ul. This model has the interesting
property that if column l(cid:48) of Yj lies in the interior of the convex hull of the columns of Yj,
then ul(cid:48) will lie in the interior of the interval [minl ul, maxl ul] [BV04]. Hence the model will
never impute label l(cid:48) for any example.

We need not restrict ourselves to the loss function given above. In fact, any loss func-
tion that can be used to train a classiﬁer for categorical variables (also called a multi-class
classiﬁer) can be used to ﬁt a categorical PCA model, so long as the loss function depends
only on the inner products between the parameters of the model and the features corre-
sponding to each example. The loss function becomes the loss function L used in (20); the
optimal parameters of the model give the optimal matrix Y , while the implied features will
populate the optimal matrix X. For example, it is possible to use loss functions derived

39

from error-correcting output codes [DB95]; the Directed Acyclic Graph SVM [PCST99]; the
Crammer-Singer multi-class loss [CS02]; or the multi-category SVM [LLW04].

Of these loss functions, only the one-vs-all loss is separable across the classes a ∈ F.
(By separable, we mean that the objective value can be written as a sum over the classes.)
Hence ﬁtting a categorical features with any other loss functions is not the same as ﬁtting d
Boolean features. For example, in the Crammer-Singer loss

L(u, a) = (1 − ua + max

u(cid:48)
a)+,

a(cid:48)∈F , a(cid:48)(cid:54)=a

the classes are combined according to their maximum, rather than their sum. While one-vs-
all classiﬁcation performs about as well as more sophisticated loss functions on small data
sets [RK04], these more sophisticated nonseparable loss tend to perform much better as the
number of classes (and examples) increases [GBW14].

Some interesting nonconvex loss functions have also been suggested for this problem. For

example, consider a generalization of Hamming distance to this setting,

L(u, a) = δua,1 +

δua(cid:48) ,0,

(cid:88)

a(cid:48)(cid:54)=a

where δα,β = 0 if α = β and 1 otherwise.
In this case, alternating minimization with
regularization that enforces a clustered structure in the low rank model (see the discussion
of quadratic clustering in §3.2) reproduces the k-modes algorithm [HN99].

Ordinal PCA. We saw in §5 one way to ﬁt a GLRM to ordinal data. Here, we use a
larger embedding dimension for ordinal features. The multi-dimensional embedding will be
particularly useful when the best mapping of the ordinal variable onto a linear scale is not
uniform; e.g., if level 1 of the ordinal variable is much more similar to level 2 than level 2
is to level 3. Using a larger embedding dimension allows us to infer the relations between
the levels from the data itself. Here we again identify the labels a ∈ F with the integers
{1, . . . , d}.

One approach we can use for (multi-dimensional) ordinal PCA is to solve (20) with the

loss function

L(u, a) =

(1 − Ia>a(cid:48)ua(cid:48))+,

(21)

and with quadratic regularization. Fixing X and optimizing over Y is equivalent to training
an SVM to separate labels a ≤ l from a > l for each l ∈ F. This approach produces
a set of hyperplanes (given by the columns of Y ) separating each level l from the next.
The hyperplanes need not be parallel to each other. Fixing Y and optimizing over X ﬁnds
the low dimensional features vector for each example that places the example between the
appropriate hyperplanes.
(See Figure 11 for an illustration of an optimal ﬁt of this loss
function, with k = 2, to a simple synthetic data set.)

d−1
(cid:88)

a(cid:48)=1

40

Figure 11: Multi-dimensional ordinal loss.

Permutation PCA. Suppose that a is a permutation of the numbers 1, . . . , d. Deﬁne the
permutation loss

L(u, a) =

(1 − uai + uai+1)+.

d−1
(cid:88)

i=1

This loss is zero if uai > uai+1 + 1 for i = 1, . . . , d − 1, and increases linearly when these
inequalities are violated. Deﬁne sort(u) to return a permutation ˆa of the indices 1, . . . , d so
that uˆai ≥ uˆai+1 for i = 1, . . . , d−1. It is easy to check that argmina L(u, a) = sort(u). Hence
using the permutation loss function in generalized PCA (20) ﬁnds a low rank approximation
of a given table of permutations.

Ranking PCA. Many variants on the permutation PCA problem are possible. For ex-
ample, in ranking PCA, we interpret the permutation as a ranking of the choices 1, . . . , d,
and penalize deviations of many levels more strongly than deviations of only one level by
choosing the loss

L(u, a) =

(1 − uai + uaj )+.

d−1
(cid:88)

d
(cid:88)

i=1

j=i+1

From here, it is easy to generalize to a setting in which the rankings are only partially
observed. Suppose that we observe pairwise comparisons a ⊆ {1, . . . , d} × {1, . . . , d}, where
(i, j) ∈ a means that choice i was ranked above choice j. Then a loss function penalizing

41

devations from these observed rankings is

L(u, a) =

(1 − uai + uaj )+.

(cid:88)

(i,j)∈a

Many other modiﬁcations to ranking loss functions have been proposed in the literature
that interpolate between the the two ﬁrst loss functions proposed above, or which priori-
tize correctly predicting the top ranked choices. These losses include the area under the
curve loss [Ste07], ordered weighted average of pairwise classiﬁcation losses [UBG09], the
weighted approximate-rank pairwise loss [WBU10], the k-order statistic loss [WYW13], and
the accuracy at the top loss [BCMR12].

6.2 Oﬀsets and scaling

Just as in the previous section, better practical performance can often be achieved by allowing
an oﬀset in the model as described in §3.3, and scaling loss functions as described in §4.3.

6.3 Numerical examples

We ﬁt a low rank model to the 2013 American Community Survey (ACS) to illustrate how
to ﬁt a low rank model to heterogeneous data.

The ACS is a survey administered to 1% of the population of the United States each
year to gather their responses to a variety of demographic and economic questions. Our
data sample consists of m = 3132796 responses gathered from residents of the US, excluding
Puerto Rico, in the year 2013, on the 23 questions listed in Table 3.

We ﬁt a rank 10 model to this data using Huber loss for real valued data, hinge loss for
Boolean data, ordinal hinge loss for ordinal data, one-vs-all categorical loss for categorical
data, and regularization parameter γ = .1. We allow an oﬀset in the model and scale the
loss functions and regularization as described in §4.3.

In Table 4, we select a few features j from the model, along with their associated vec-
tors yj, and ﬁnd the two features most similar to them by ﬁnding the two features j(cid:48) which
minimize cos(yj, yj(cid:48)). The model automatically groups states which intuitively share de-
for example, three wealthy states adjoining (but excluding) a major
mographic features:
metropolitan area — Virginia, Maryland, and Connecticut — are grouped together. The
low rank structure also identiﬁes the results (high water prices) of the prolonged drought
aﬄicting California, and corroborates the intuition that work leads only to more work: hours
worked per week, weeks worked per year, and education level are highly correlated.

7 Fitting low rank models

In this section, we discuss a number of algorithms that may be used to ﬁt generalized low
rank models. As noted earlier, it can be computationally hard to ﬁnd the global optimum
of a generalized low rank model. For example, it is NP-hard to compute an exact solution

42

Description
household type
state

commercial use
house on ≥ 10 acres
household income
monthly electricity bill

Variable
HHTYPE
STATEICP
OWNERSHP own home
COMMUSE
ACREHOUS
HHINCOME
COSTELEC
COSTWATR monthly water bill
monthly gas bill
COSTGAS
FOODSTMP on food stamps
HCOVANY
SCHOOL
EDUC
GRADEATT highest grade level attained
EMPSTAT
LABFORCE
CLASSWKR class of worker
WKSWORK2 weeks worked per year
UHRSWORK usual hours worked per week real
looking for work
LOOKING
migration status
MIGRATE1

have health insurance
currently in school
highest level of education

Type
categorical
categorical
Boolean
Boolean
Boolean
real
real
real
real
Boolean
Boolean
Boolean
ordinal
ordinal
categorical
Boolean
Boolean
ordinal

employment status
in labor force

Boolean
categorical

Table 3: ACS variables.

Most similar features
Montana, North Dakota
Illinois, cost of water
Oregon, Idaho
Indiana, Michigan

Feature
Alaska
California
Colorado
Ohio
Pennsylvania Massachusetts, New Jersey
Virginia
Hours worked weeks worked, education

Maryland, Connecticut

Table 4: Most similar features in demography space.

43

to k-means [DFK+04], nonnegative matrix factorization [Vav09], and weighted PCA and
matrix completion [GG11] all of which are special cases of low rank models.

In §7.1, we will examine a number of local optimization methods based on alternating
minimization. Algorithms implementing lazy variants of alternating minimization, such as
the alternating gradient, proximal gradient, or stochastic gradient algorithms, are faster
per iteration than alternating minimization, although they may require more iterations for
convergence. In numerical experiments, we notice that lazy variants often converge to points
with a lower objective value: it seems that these lazy variants are less likely to be trapped
at a saddle point than is alternating minimization. §7.4 explores the convergence of these
algorithms in practice.

We then consider a few special cases in which we can show that alternating minimization
converges to the global optimum in some sense: for example, we will see convergence with
high probability, approximately, and in retrospect. §7.5 discusses a few strategies for initial-
izing these local optimization methods, with provable guarantees in special cases. §7.6 shows
that for problems with convex loss functions and quadratic regularization, it is sometimes
possible to certify global optimality of the resulting model.

7.1 Alternating minimization

We showed earlier how to use alternating minimization to ﬁnd an (approximate) solution
to a generalized low rank model. Algorithm (1) shows how to explicitly extend alternating
minimization to a generalized low rank model (15) with observations Ω.

Algorithm 1

given X 0, Y 0
for k = 1, 2, . . . do

for i = 1, . . . , M do
(cid:16)(cid:80)
xk
i = argminx
end for
for j = 1, . . . , N do
(cid:16)(cid:80)
yk
j = argminy
end for

end for

j:(i,j)∈Ω Lij(xyk−1

j

, Aij) + r(x)

(cid:17)

i:(i,j)∈Ω Lij(xk

i y, Aij) + ˜r(y)

(cid:17)

Parallelization. Alternating minimization parallelizes naturally over examples and fea-
tures.
In Algorithm 1, the loops over i = 1, . . . , N and over j = 1, . . . , M may both be
executed in parallel.

44

7.2 Early stopping

It is not very useful to spend a lot of eﬀort optimizing over X before we have a good estimate
for Y . If an iterative algorithm is used to compute the minimum over X, it may make sense to
stop the optimization over X early before going on to update Y . In general, we may consider
replacing the minimization over x and y above by any update rule that moves towards the
minimum. This templated algorithm is presented as Algorithm 2. Empirically, we ﬁnd that
this approach often ﬁnds a better local minimum than performing a full optimization over
each factor in every iteration, in addition to saving computational eﬀort on each iteration.

Algorithm 2

given X 0, Y 0
for t = 1, 2, . . . do

, Y t−1, A)

for i = 1, . . . , m do

i = updateL,r(xt−1
xt
end for
for j = 1, . . . , n do

i

j = updateL,˜r(y(t−1)T
yt
end for

j

, X (t)T , AT )

end for

We describe below a number of diﬀerent update rules updateL,r by writing the X update.
The Y update can be implemented similarly. (In fact, it can be implemented by substituting
˜r for r, switching the roles of X and Y , and transposing all matrix arguments.) All of the
approaches outlined below can still be executed in parallel over examples (for the X update)
and features (for the Y update).

Gradient method. For example, we might take just one gradient step on the objective.
This method can be used as long as L, r, and ˜r do not take inﬁnite values. (If any of these
functions f is not diﬀerentiable, replace ∇f below by any subgradient of f [BL10, BXM03].)

We implement updateL,r as follows. Let

(cid:88)

g =

j:(i,j)∈Ω

∇Lij(xiyj, Aij)yj + ∇r(xi).

Then set

i = xt−1
xt

i − αtg,

for some step size αt. For example, a common step size rule is αt = 1/t, which guarantees
convergence to the globally optimal X if Y is ﬁxed [BL10, BXM03].

Proximal gradient method.
If a function takes on the value ∞, it need not have a
subgradient at that point, which limits the gradient update to cases where the regularizer

45

and loss are (ﬁnite) real-valued. When the regularizer (but not the loss) takes on inﬁnite
values (say, to represent a hard constraint), we can use a proximal gradient method instead.

The proximal operator of a function f [PB13] is

proxf (z) = argmin

(f (x) +

(cid:107)x − z(cid:107)2

2).

x

1
2

If f is the indicator function of a set C, the proximal operator of f is just (Euclidean)
projection onto C.

A proximal gradient update updateL,r is implemented as follows. Let

Then set

(cid:88)

g =

j:(i,j)∈Ω

∇Lij(xt−1

i

yt−1
j

, Aij)yt−1

.

j

i = proxαtr(xt−1
xt

i − αtg),

for some step size αt. The step size rule αt = 1/t guarantees convergence to the globally
optimal X if Y is ﬁxed, while using a ﬁxed, but suﬃciently small, step size α guarantees
convergence to a small O(α) neighborhood around the optimum [Ber11]. The technical
condition required on the step size is that αt < 1/L, where L is the Lipshitz constant of
the gradient of the objective function. Bolte et al. have shown that the iterates xt
i and
yt
j produced by the proximal gradient update rule (which they call proximal alternating
linearized minimization, or PALM) globally converge to a critical point of the objective
function under very mild conditions on L, r, and ˜r [BST13].

Prox-prox method. Letting ft(X) = (cid:80)
(prox-prox) update

(i,j)∈Ω Lij(xiyt

j, Aij), deﬁne the proximal-proximal

X t+1 = proxαtr(proxαtft(X t)).

The prox-prox update is simply a proximal gradient step on the objective when f is

replaced by the Moreau envelope of f ,

Mf (X) = inf
X (cid:48)

(cid:0)f (X (cid:48)) + (cid:107)X − X (cid:48)(cid:107)2

(cid:1) .

F

(See [PB13] for details.) The Moreau envelope has the same minimizers as the original
objective. Thus, just as the proximal gradient method repeatedly applied to X converges to
global minimum of the objective if Y is ﬁxed, the prox-prox method repeatedly applied to
X also converges to global minimum of the objective if Y is ﬁxed under the same conditions
on the step size αt. for any constant stepsize α ≤ (cid:107)G(cid:107)2
2. (Here, (cid:107)G(cid:107)2 = sup(cid:107)x(cid:107)2≤1 (cid:107)Gx(cid:107)2 is
the operator norm of G.)

This update can also be seen as a single iteration of ADMM when the dual variable
in ADMM is initialized to 0; see [BPC+11].
In the case of quadratic objectives, we will
see below that the prox-prox update can be applied very eﬃciently, making iterated prox-
prox, or ADMM, eﬀective means of computing the solution to the subproblems arising in
alternating minimization.

46

In numerical experiments, we ﬁnd that using a slightly more
Choosing a step size.
nuanced rule allowing diﬀerent step sizes for diﬀerent rows and columns can allow fast
progress towards convergence while ensuring that the value of the objective never increases.
The safeguards on step sizes we propose are quite important in practice: without these
checks, we observe divergence when the initial step sizes are chosen too large.

Motivated by the convergence proof in [Ber11], for each row, we seek a step size on
the order of 1/(cid:107)gi(cid:107)2, where gi is the gradient of the objective function with respect to xi.
We start by choosing an initial step size scale αi for each row of the same order as the
average gradient of the loss functions for that row. In the numerical experiments reported
here, we choose αi = 1 for i = 1, . . . , m. Since gi grows with the number of observations
ni = |{j : (i, j) ∈ Ω}| in row i, we achieve the desired scaling by setting α0
i = αi/ni. We
take a gradient step on each row xi using the step size αi. Our procedure for choosing α0
j is
the same.

We then check whether the objective value for the row,

(cid:88)

j:(i,j)∈Ω

Lj(xiyj, Aij) + γ(cid:107)xi(cid:107)2
2,

has increased or decreased. If it has increased, then we trust our ﬁrst order approximation
to the objective function less far, and reduce the step size; if it has decreased, we gain
conﬁdence, and increase the step size.
In the numerical experiments reported below, we
decrease the step size by 30% when the objective increases, and increase the step size by 5%
when the objective decreases. This check stabilizes the algorithm and prevents divergence
even when the initial scale has been chosen poorly.

We then do the same with respect to each column yj: we take a gradient step, check if

the objective value for the column has increased or decreased, and adjust the step size.

The time per iteration is thus O(k(m + n + |Ω|)): computing the gradient of the ith loss
function with respect to xi takes time O(kni); computing the proximal operator of the square
loss takes time O(k); summing these over all the rows i = 1, . . . , m gives time O(k(m + |Ω|));
and adding the same costs for the column updates gives time O(k(m + n + |Ω|)). The checks
on the objective value take time O(k) per observation (to compute the inner product xiyj
and value of the loss function for each observation) and time O(1) per row and column to
compute the value of the regularizer. Hence the total time per iteration is O(k(m + n + |Ω|)).
By partitioning the job of updating diﬀerent rows and diﬀerent columns onto diﬀerent

processors, we can achieve an iteration time of O(k(m + n + |Ω|)/p) using p processors.

Stochastic gradients.
Instead of computing the full gradient of L with respect to xi
above, we can replace the gradient g in either the gradient or proximal gradient method by
any stochastic gradient g, which is a vector that satisﬁes

E g =

(cid:88)

j:(i,j)∈Ω

∇Lij(xiyj, Aij)yj.

47

A stochastic gradient can be computed by sampling j uniformly at random from among
observed features of i, and setting g = |{j : (i, j) ∈ Ω}|∇Lij(xiyj, Aij)yj. More samples from
{j : (i, j) ∈ Ω} can be used to compute a less noisy stochastic gradient.

7.3 Quadratic objectives

Here we describe how to eﬃciently implement the prox-prox update rule for quadratic ob-
jectives and arbitrary regularizers, extending the factorization caching technique introduced
in §2.3. We assume here that the objective is given by

(cid:107)A − XY (cid:107)2

F + r(X) + ˜r(Y ).

We will concentrate here on the X update; as always, the Y update is exactly analogous.

As in the case of quadratic regularization, we ﬁrst form the Gram matrix G = Y Y T .

Then the proximal gradient update is fast to evaluate:

proxαkr(X − αk(XG − 2AY T )).

But we can take advantage of the ease of inverting the Gram matrix G to design a
faster algorithm using the prox-prox update. For quadratic objectives with Gram matrix
G = Y T Y , the prox-prox update takes the simple form

proxαkr((G +

I)−1(AY T +

X)).

1
αk

1
αk

As in §2.3, we can compute (G + 1
αk
ization of (G + 1
αk
updating Y , since most of the computational eﬀort is in forming G and AY T .
For example, in the case of nonnegative least squares, this update is just

X) in parallel by ﬁrst caching the factor-
I)−1. Hence it is advantageous to repeat this update many times before

I)−1(AY T + 1
αk

Π+((G +

I)−1(AY T +

X)),

1
αk

1
αk

where Π+ projects its argument onto the nonnegative orthant.

7.4 Convergence

Alternating minimization need not converge to the same model (or the same objective value)
when initialized at diﬀerent starting points. Through examples, we explore this idea here.
These examples are ﬁt using the serial Julia implementation (presented in §9) of the alter-
nating proximal gradient updates method.

48

Global convergence for quadratically regularized PCA. Figure 12 shows the con-
vergence of the alternating proximal gradient update method on a quadratically regularized
PCA problem with randomly generated, fully observed data A = X trueY true, where entries
of X true and Y true are drawn from a standard normal distribution. We pick ﬁve diﬀerent
random initializations of X and Y with standard normal entries to generate ﬁve diﬀerent
convergence trajectories. Quadratically regularized PCA is a simple problem with an ana-
lytical solution (see §2), and with no local minima (see Appendix A). Hence it should come
as no surprise that the trajectories all converge to the same, globally optimal value.

Local convergence for nonnegative matrix factorization. Figure 13 shows conver-
gence of the same algorithm on a nonnegative matrix factorization model, with data gener-
ated in the same way as in Figure 12. (Note that A has some negative entries, so the minimal
objective value is strictly greater than zero.) Here, we plot the convergence of the objective
value, rather than the suboptimality, since we cannot provably compute the global minimum
of the objective function. We see that the algorithm converges to a diﬀerent optimal value
(and point) depending on the initialization of X and Y . Three trajectories converge to the
same optimal value (though one does so much faster than the others), one to a value that is
somewhat better, and one to a value that is substantially worse.

7.5 Initialization

Alternating minimization need not converge to the same solution (or the same objective
value) when initialized at diﬀerent starting points. Above, we saw that alternating mini-
mization can converge to models with optimal values that diﬀer signiﬁcantly.

Here, we discuss two approaches to initialization that result in provably good solutions,
for special cases of the generalized low rank problem. We then discuss how to apply these
initialization schemes to more general models.

SVD. A literature that is by now extensive shows that the SVD provides a provably good
initialization for the quadratic matrix completion problem (10) [KMO09, KMO10, KM10,
JNS13, Har13, GAGG13]. Algorithms based on alternating minimization have been shown
to converge quickly (even geometrically [JNS13]) to a global solution satisfying a recovery
guarantee when the initial values of X and Y are chosen carefully; see §2.4 for more details.
Here, we extend the SVD initialization previously proposed for matrix completion to one
that works well for all PCA-like problems: problems with convex loss functions that have
been scaled as in §4.3; with data A that consists of real values, Booleans, categoricals, and
ordinals; and with quadratic (or no) regularization.

But we will need a matrix on which to perform the SVD. What matrix corresponds to our
data table? Here, we give a simple proposal for how to construct such a matrix, motivated
by [KMO10, JNS13, Cha14]. Our key insight is that the SVD is the solution to our problem
when the entries in the table have mean zero and variance one (and all the loss functions are

49

y
t
i
l
a
m

i
t
p
o
b
u
s

e
v
i
t
c
e
j
b
o

105

104

103

102

101

100

0

1

2

3

time (s)

Figure 12: Convergence of alternating proximal gradient updates on quadratically
regularized PCA for n = m = 200, k = 2.

quadratic). Our initialization will construct a matrix with mean zero and variance one from
the data table, take its SVD, and invert the construction to produce the correct initialization.
Our ﬁrst step is to expand the categorical columns taking on d values into d Boolean
columns, and to re-interpret ordinal and Boolean columns as numbers. The scaling we
propose below is insensitive to the values of the numbers in the expansion of the Booleans: for
example, using (false, true)= (0, 1) or (false, true)= (−1, 1) produces the same initialization.
The scaling is sensitive to the diﬀerences between ordinal values: while encoding (never,
sometimes, always) as (1, 2, 3) or as (−5, 0, 5) will make no diﬀerence, encoding these ordinals
as (0, 1, 10) will result in a diﬀerent initialization.

Now we assume that the rows of the data table are independent and identically dis-
tributed, so they each have equal means and variances. Our mission is to standardize the

50

·104

e
u
l
a
v

e
v
i
t
c
e
j
b
o

1.8

1.7

1.6

1.5

1.4

1.3

0

1

2

3

time (s)

Figure 13: Convergence of alternating proximal gradient updates on NNMF for
n = m = 200, k = 2.

columns. The observed entries in column j have mean µj and variance σ2
j ,

µj = argmin

µ

σ2
j =

1
nj − 1

(cid:88)

i:(i,j)∈Ω
(cid:88)

i:(i,j)∈Ω

Lj(µ, Aij)

Lj(µj, Aij),

so the matrix whose (i, j)th entry is (Aij − µj)/σj for (i, j) ∈ Ω has columns whose observed
entries have mean 0 and variance 1.

Each missing entry can be safely replaced with 0 in the scaled version of the data without
changing the column mean. But the column variance will decrease to mj/m. If instead we
deﬁne

˜Aij =

(cid:26) m
σj mj
0

(Aij − µj)

(i, j) ∈ Ω
otherwise,

then the column will have mean 0 and variance 1.

51

Take the SVD U ΣV T of ˜A, and let ˜U ∈ Rm×k, ˜Σ ∈ Rk×k, and ˜V ∈ Rn×k denote these
matrices truncated to the top k singular vectors and values. We initialize X = ˜U ˜Σ1/2, and
Y = ˜Σ1/2 ˜V T diag(σ). The oﬀset row in the model is initialized with the means, i.e., the kth
column of X is ﬁlled with 1’s, and the kth row of Y is ﬁlled with the means, so Ykj = µj.

Finally, note that we need not compute the full SVD of ˜A, but instead can simply compute
the top k singular triples. For example, the randomized top k SVD algorithm proposed in
[HMT11] computes the top k singular triples of ˜A in time linear in |Ω|, m, and n (and
quadratic in k).

Figure 14 compares the convergence of this SVD-based initialization with random ini-
tialization on a low rank model for census data described in detail in §6.3. We initialize the
algorithm at six diﬀerent points: from ﬁve diﬀerent random normal initializations (entries
of X 0 and Y 0 drawn iid from N (0, 1)), and from the SVD of ˜A. The SVD initialization
produces a better initial value for the objective function, and also allows the algorithm to
converge to a substantially lower ﬁnal objective value than can be found from any of the ﬁve
random starting points. This behaviour indicates that the “good” local minimum discovered
by the SVD initialization is located in a basin of attraction that has low probability with
respect to the measure induced by random normal initialization.

k-means++. The k-means++ algorithm is an initialization scheme designed for quadratic
clustering problems [AV07]. It consists of choosing an initial cluster centroid at random from
the points, and then choosing the remaining k − 1 centroids from the points x that have
not yet been chosen with probability proportional to D(x)2, where D(x) is the minimum
distance of x to any previously chosen centroid.

Quadratic clustering is known to be NP-hard, even with only two clusters (k = 2)
[DFK+04]. However, k-means++ followed by alternating minimization gives a solution with
expected approximation ratio within O(log k) of the optimal value [AV07]. (Here, the expec-
tation is over the randomization in the initialization algorithm.) In contrast, an arbitrary
initialization of the cluster centers for k-means can result in a solution whose value is arbi-
trarily worse than the true optimum.

A similar idea can be used for other low rank models. If the model rewards a solution
that is spread out, as is the case in quadratic clustering or subspace clustering, it may be
better to initialize the algorithm by choosing elements with probability proportional to a
distance measure, as in k-means++.
In the k-means++ procedure, one can use the loss
function L(u) as the distance metric D.

7.6 Global optimality

All generalized low rank models are non-convex, but some are more non-convex than others.
In particular, for some problems, the only important source of non-convexity is the low rank
constraint. For these problems, it is sometimes possible to certify global optimality of a
model by considering an equivalent rank-constrained convex problem.

The arguments in this section are similar to ones found in [RFP10], in which Recht et al.
propose using a factored (nonconvex) formulation of the (convex) nuclear norm regularized

52

random
random
random
random
random
SVD

·105

e
u
l
a
v

e
v
i
t
c
e
j
b
o

9

8

7

6

5

4

3

2

0

5

10

15

30

35

40

45

50

20

25
iteration

Figure 14: Convergence from ﬁve diﬀerent random initializations, and from the
SVD initialization.

53

estimator in order to eﬃciently solve the large-scale SDP arising in a matrix completion
problem. However, the algorithm in [RFP10] relies on a subroutine for ﬁnding a local
minimum of an augmented Lagrangian which has the same biconvex form as problem (10).
Finding a local minimum of this problem (rather than a saddle point) may be hard. In this
section, we avoid the issue of ﬁnding a local minimum of the nonconvex problem; we consider
instead whether it is possible to verify global optimality when presented with some putative
solution.

The factored problem is equivalent to the rank constrained problem. Consider
the factored problem

minimize L(XY ) + γ

2 (cid:107)X(cid:107)2

F + γ

2 (cid:107)Y (cid:107)2
F ,

with variables X ∈ Rm×k, Y ∈ Rk×n, where L : Rm×n → R is any convex loss function.
Compare this to the rank-constrained problem

minimize L(Z) + γ(cid:107)Z(cid:107)∗
subject to Rank(Z) ≤ k.

with variable Z ∈ Rm×n. Here, we use (cid:107) · (cid:107)∗ to denote the nuclear norm, the sum of the
singular values of a matrix.

Theorem 1. (X (cid:63), Y (cid:63)) is a solution to the factored problem 22 if and only if Z (cid:63) = X (cid:63)Y (cid:63) is
a solution to the rank-constrained problem 23, and (cid:107)X (cid:63)(cid:107)2

F = (cid:107)Y (cid:63)(cid:107)2

F = 1

2(cid:107)Z (cid:63)(cid:107)∗.

We will need the following lemmas to understand the relation between the rank-constrained

problem and the factored problem.

Lemma 1. Let XY = U ΣV T be the SVD of XY , where Σ = diag(σ). Then

(22)

(23)

(24)

Proof. We may derive this fact as follows:

(cid:107)σ(cid:107)1 ≤

(||X||2

F + ||Y ||2

F ).

1
2

(cid:107)σ(cid:107)1 = tr(U T XY V )

≤ (cid:107)U T X(cid:107)F (cid:107)Y V (cid:107)F
≤ (cid:107)X(cid:107)F (cid:107)Y (cid:107)F

≤

(||X||2

F + ||Y ||2

F ),

1
2

where the ﬁrst inequality above uses the Cauchy-Schwartz inequality, the second relies on the
orthogonal invariance of the Frobenius norm, and the third follows from the basic inequality
ab ≤ 1

2(a2 + b2) for any real numbers a and b.

54

Lemma 2. For any matrix Z, (cid:107)Z(cid:107)∗ = inf XY =Z

1

2(||X||2

F + ||Y ||2

F ).

Proof. Writing Z = U DV T and recalling the deﬁnition of the nuclear norm (cid:107)Z(cid:107)∗ = (cid:107)σ(cid:107)1,
we see that Lemma 1 implies

(cid:107)Z(cid:107)∗ ≤ inf

XY =Z

(||X||2

F + ||Y ||2

F ).

1
2

But taking X = U Σ1/2 and Y = Σ1/2V T , we have

1
2

1
2

(||X||2

F + ||Y ||2

F ) =

((cid:107)Σ1/2(cid:107)2

F + (cid:107)Σ1/2(cid:107)2

F ) = (cid:107)σ(cid:107)1,

(using once again the orthogonal invariance of the Frobenius norm), so the bound is satisﬁed
with equality.

Note that the inﬁmum is achieved by X = U Σ1/2T and Y = T T Σ1/2V T for any orthonor-

mal matrix T .

Theorem 1 follows as a corollary, since L(Z) = L(XY ) so long as Z = XY .

The rank constrained problem is sometimes equivalent to an unconstrained prob-
lem. Note that problem (23) is still a hard problem to solve:
it is a rank-constrained
semideﬁnite program. On the other hand, the same problem without the rank constraint is
convex and tractable (though not easy to solve at scale). In particular, it is possible to write
down an optimality condition for the problem

minimize L(Z) + γ(cid:107)Z(cid:107)∗

(25)

that certiﬁes that a matrix Z is globally optimal. This problem is a relaxation of prob-
lem (23), and so has an optimal value that is at least as small. Furthermore, if any solution
to problem (25) has rank no more than k, then it is feasible for problem (23), so the op-
timal values of problem (25) and problem (23) must be the same. Hence any solution of
problem (25) with rank no more than k also solves problem 23.

Recall that the matrix Z is a solution the problem U if and only if

0 ∈ ∂(L(Z) + γ(cid:107)Z(cid:107)∗),

where ∂f (Z) is the subgradient of the function f at Z. The subgradient is a set-valued
function.

The subgradient of the nuclear norm at a matrix Z = U ΣV T is any matrix of the form
U V T + W where U T W = 0, W V = 0, and (cid:107)W (cid:107)2 ≤ 1. Equivalently, deﬁne the set-valued
function sign on scalar arguments x as

sign(x) =






{1}
x > 0
[ − 1, 1] x = 0
x < 0,
{−1}

,

55

and deﬁne (sign(x))i = sign(xi) for vectors x ∈ Rn. Then we can write the subgradient of
the nuclear norm at Z as

∂(cid:107)Z(cid:107)∗ = U diag(sign(σ))V T ,
where now we use the full SVD of Z with U ∈ Rm×min(m,n), V ∈ Rn×min(m,n), and σ ∈
Rmin(m,n).

Hence Z = U ΣV T is a solution to problem (25) if and only if

0 ∈ ∂L(Z) + γ(U V T + W ),

or more simply, if

(cid:107)(1/γ)G + U V T (cid:107)2 ≤ 1

(26)

for some G ∈ ∂L(Z). In particular, if a matrix Z with rank no more than k satisﬁes (26),
then Z also solves the rank-constrained problem (23).

This result allows us to (sometimes) certify global optimality of a particular model.
Given a model (X, Y ), we compute the SVD of the product XY = U ΣV T and an element
G ∈ ∂L(Z). If (cid:107)(1/γ)G + U V T (cid:107)2 ≤ 1, then (X, Y ) is globally optimal. (If the objective is
diﬀerentiable then we simply pick G = ∇L(Z); otherwise some choices of G ∈ ∂L(Z) may
produce invalid certiﬁcates even if (X, Y ) is globally optimal.)

8 Choosing low rank models

8.1 Regularization paths

Suppose that we wish to understand the entire regularization path for a GLRM; that is, we
would like to know the solution (X(γ), Y (γ)) to the problem

minimize (cid:80)

(i,j)∈Ω Lij(xiyj, Aij) + γ (cid:80)m

i=1 ri(xi) + γ (cid:80)n

j=1 ˜rj(yj)

as a function of γ. Frequently, the regularization path may be computed almost as quickly
as the solution for a single value of γ. We can achieve this by initially ﬁtting the model
with a very high value for γ, which is often a very easy problem. (For example, when r
and ˜r are norms, the solution is (X, Y ) = (0, 0) for suﬃciently large γ.) Then we may
ﬁt models corresponding to smaller and smaller values of γ by initializing the alternating
minimization algorithm from our previous solution. This procedure is sometimes called a
homotopy method.

For example, Figure 15 shows the regularization path for quadratically regularized Huber
PCA on a synthetic data set. We generate a dataset A = XY +S with X ∈ Rm×k, Y ∈ Rk×n,
and S ∈ Rm×n, with m = n = 300 and k = 3. The entries of X and Y are drawn from a
standard normal distribution, while the entries of the sparse noise matrix S are drawn from
a uniform distribution on [0, 1] with probability 0.05, and are 0 otherwise. We ﬁt a rank 5
GLRM to an observation set Ω consisting of 10% of the entries in the matrix, drawn uniformly

56

at random from {1, . . . , i} × {1, . . . , j}, using Huber loss and quadratic regularization, and
vary the regularization parameter. That is, we ﬁt the model

minimize (cid:80)

(i,j)∈Ω huber(xiyj, Aij) + γ (cid:80)m

i=1 (cid:107)xi(cid:107)2

2 + γ (cid:80)n

j=1 (cid:107)yj(cid:107)2
2

and vary the regularization parameter γ. The ﬁgure plots both the normalized training error,

and the normalized test error,

1
|Ω|

(cid:88)

(i,j)∈Ω

huber(xiyj, Aij),

1
nm − |Ω|

(cid:88)

(i,j)(cid:54)∈Ω

huber(xiyj, Aij),

of the ﬁtted model (X, Y ), for γ ranging from 0 to 3. Here, we see that while the training error
decreases and γ decreases, the test error reaches a minimum around γ = .5. Interestingly,
it takes only three times longer (about 3 seconds) to generate the entire regularization path
than it does to ﬁt the model for a single value of the regularization parameter (about 1
second).

test error
train error

0.35

0.3

0.25

0.2

0.15

0.1

r
o
r
r
e

d
e
z
i
l
a
m
r
o
n

0

0.5

1

2

2.5

3

Figure 15: Regularization path.

1.5
γ

57

8.2 Choosing model parameters

To form a generalized low rank model, one needs to specify the loss functions Lj, regularizers
r and ˜r, and a rank k. The loss function should usually be chosen by a domain expert to
reﬂect the intuitive notion of what it means to “ﬁt the data well”. On the other hand, the
regularizers and rank are often chosen based on statistical considerations, so that the model
generalizes well to unseen (missing) data.

There are three major considerations to balance in choosing the regularization and rank
In the following discussion, we suppose that the regularizers r = γr0 and

of the model.
˜r = γ˜r0 have been chosen up to a scaling γ.

Compression. A low rank model (X, Y ) with rank k and no sparsity represents the data
table A with only (m + n)k numbers, achieving a compression ratio of (m + n)k/(mn). If the
factors X or Y are sparse, then we have used fewer than (m + n)k numbers to represent the
data A, achieving a higher compression ratio. We may want to pick parameters of the model
(k and γ) in order to achieve a good error (cid:80)
(i,j)∈Ω Lj(Aij − xiyj) for a given compression
ratio. For each possible combination of model parameters, we can ﬁt a low rank model with
those parameters, observing both the error and the compression ratio. We can then choose
the best model parameters (highest compression rate) achieving the error we require, or the
best model parameters (lowest error rate) achieving the compression we require.

More formally, one can construct an information criterion for low rank models by analogy
with the Aikake Information Criterion (AIC) or the Bayesian Information Criterion (BIC).
For use in the AIC, the number of degrees of freedom in a low rank model can be computed
as the diﬀerence between the number of nonzeros in the model and the dimensionality of the
symmetry group of the problem. For example, if the model (X, Y ) is dense, and the regu-
larizer is invariant under orthogonal transformations (e.g., r(x) = (cid:107)x(cid:107)2
2), then the number
of degrees of freedom is (m + n)k − k2 [TB99]. Minka [Min01] proposes a method based on
the BIC to automatically choose the dimensionality in PCA, and observes that it performs
better than cross validation in identifying the true rank of the model when the number of
observations is small (m, n (cid:46) 100).

Denoising. Suppose we observe every entry in a true data matrix contaminated by noise,
e.g., Aij = Atrue
ij + (cid:15)ij, with (cid:15)ij some random variable. We may wish to choose model
parameters to identify the truth and remove the noise: we would like to ﬁnd k and γ to
minimize (cid:80)

(i,j)∈Ω Lj(Atrue

ij − xiyj).

A number of commonly used rules-of-thumb have been proposed in the case of PCA to
distinguish the signal (the true rank k of the data) from the noise, some of which can be
generalized to other low rank models. These include using scree plots, often known as the
“elbow method” [Cat66]; the eigenvalue method; Horn’s parallel analysis [Hor65, Din09];
and other related methods [ZV86, PM03]. A recent, more sophisticated method adapts the
idea of dropout training [SHK+14] to regularize low-rank matrix estimation [JW14].

Some of these methods can easily be adapted to the GLRM context. The “elbow method”
increases k until the objective value decreases less than linearly; the eigenvalue method

58

increases k until the objective value decreases by less than some threshold; Horn’s parallel
analysis increases k until the objective value compares unfavorably to one generated by
ﬁtting a model to data drawn from a synthetic noise distribution.

Cross validation is also simple to apply, and is discussed further below as a means of
predicting missing entries. However, applying cross validation to the denoising problem is
somewhat tricky, since leaving out too few entries results in overﬁtting to the noise, while
leaving out too many results in underﬁtting to the signal. The optimal number of entries to
leave out may depend on the aspect ratio of the data, as well as on the type of noise present
in the data [Per09], and is not well understood except in the case of Gaussian noise [OP09].
We explore the problem of choosing a holdout size numerically below.

Predicting missing entries. Suppose we observe some entries in the matrix and wish
to predict the others. A GLRM with a higher rank will always be able to ﬁt the (noisy)
data better than one of lower rank. However, a model with many parameters may also
overﬁt to the noise. Similarly, a GLRM with no regularization (γ = 0) will always produce
a model with a lower empirical loss (cid:80)
(i,j)∈Ω Lj(xiyj, Aij). Hence, we cannot pick a rank k or
regularization γ simply by considering the objective value obtained by ﬁtting the low rank
model.

But by resampling from the data, we can simulate the performance of the model on out
of sample (missing) data to identify GLRMs that neither over nor underﬁt. Here, we discuss
a few methods for choosing model parameters by cross-validation; that is, by resampling
from the data to evaluate the model’s performance. Cross validation is commonly used in
regression models to choose parameters such as the regularization parameter γ, as in Figure
15. In GLRMs, cross validation can also be used to choose the rank k. Indeed, using a lower
rank k can be considered another form of model regularization.

We can distinguish between three sources of noise or variability in the data, which give

rise to three diﬀerent resampling procedures.

• The rows or columns of the data are chosen at random, i.e., drawn iid from some

population. In this case it makes sense to resample the rows or columns.

• The rows or columns may be ﬁxed, but the indices of the observed entries in the matrix
are chosen at random. In this case, it makes sense to resample from the observed entries
in the matrix.

• The indices of the observed entries are ﬁxed, but the values are observed with some
measurement error. In this case, it makes sense to resample the errors in the model.

Each of these leads to a diﬀerent reasonable kind of resampling scheme. The ﬁrst two
give rise to resampling schemes based on cross validation (i.e., resampling the rows, columns,
or individual entries of the matrix) which we discuss further below. The third gives rise to
resampling schemes based on the bootstrap or jackknife procedures, which resample from
the errors or residuals after ﬁtting the model. A number of methods using the third kind

59

of resampling have been proposed in order to perform inference (i.e., generate conﬁdence
intervals) for PCA; see Josse et al. [JWH14] and references therein.

As an example, let’s explore the eﬀect of varying |Ω|/mn, γ, and k. We generate random
, Y ∈ Rktrue×n, and S ∈ Rm×n, with m = n = 300 and
data as follows. Let X ∈ Rm×ktrue
ktrue = 3,. Draw the entries of X and Y from a standard normal distribution, and draw the
entries of the sparse outlier matrix S are drawn from a uniform distribution on [0, 3] with
probability 0.05, and are 0 otherwise. Form A = XY + S. Select an observation set Ω by
picking entries in the matrix uniformly at random from {1, . . . , n} × {1, . . . , m}. We ﬁt a
rank k GLRM with Huber loss and quadratic regularization γ(cid:107) · (cid:107)2
2, varying |Ω|/mn, γ, and
k, and compute the test error. We average our results over 5 draws from the distribution
generating the data.

In Figure 16, we see that the true rank k = 3 performs best on cross-validated error for
any number of observations |Ω|. (Here, we show performance for γ = 0. The plot for other
values of the regularization parameter is qualitatively the same.) Interestingly, it is easiest to
identify the true rank with a small number of observations: higher numbers of observations
make it more diﬃcult to overﬁt to the data even when allowing higher ranks.

|Ω|/mn=0.1
|Ω|/mn=0.3
|Ω|/mn=0.5
|Ω|/mn=0.7
|Ω|/mn=0.9

r
o
r
r
e

t
s
e
t

d
e
z
i
l
a
m
r
o
n

1

0.8

0.6

0.4

0.2

0

1

2

4

5

Figure 16: Test error as a function of k, for γ = 0.

In Figure 17, we consider the interdependence of our choice of γ and k. Regularization is
most important when few matrix elements have been observed: the curve for each k is nearly
ﬂat when more than about 10% of the entries have been observed, so we show here a plot

3
k

60

for |Ω| = .1mn. Here, we see that the true rank k = 3 performs best on cross-validated error
for any value of the regularization parameter. Ranks that are too high (k > 3) beneﬁt from
increased regularization γ, whereas higher regularization hurts the performance of models
with k lower than the true rank. That is, regularizing the rank (small k) can substitute for
explicit regularization of the factors (large γ).

k=1
k=2
k=3
k=4
k=5

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

r
o
r
r
e

t
s
e
t

d
e
z
i
l
a
m
r
o
n

0

1

2

3

4

5

γ

Figure 17: Test error as a function of γ when 10% of entries are observed.

Finally, in Figure 18 we consider how the ﬁt of the model depends on the number of
observations. If we correctly guess the rank k = 3, we ﬁnd that the ﬁt is insensitive to the
number of observations. If our rank is either too high or too low, the ﬁt improves with more
observations.

8.3 On-line optimization

Suppose that new examples or features are being added to our data set continuously, and we
wish to perform on-line optimization, which means that we should have a good estimate at
any time for the representations of those examples xi or features yj which we have seen. This
model is equivalent to adding new rows or columns to the data table A as the algorithm
continues.
In this setting, alternating minimization performs quite well, and has a very
natural interpretation. Given an estimate for Y , when a new example is observed in row i,

61

k=1
k=2
k=3
k=4
k=5

r
o
r
r
e

t
s
e
t

d
e
z
i
l
a
m
r
o
n

1

0.8

0.6

0.4

0.2

0

0.1

0.3

0.7

0.9

0.5
|Ω|/mn

Figure 18: Test error as a function of observations |Ω|/mn, for γ = 0.

we may solve

minimize (cid:80)

j:(i,j)∈Ω Lij(Aij, xyj) + r(x)

with variable x to compute a representation for row i. This computation is exactly the same
as one step of alternating minimization. Here, we are ﬁnding the best feature representation
for the new example in terms of the (already well understood) archetypes Y . If the number
of other examples previously seen is large, the addition of a single new example should not
change the optimal Y by very much; hence if (X, Y ) was previously the global minimum of
(15), this estimate of the feature representation for the new example will be very close to its
optimal representation (i.e., the one that minimizes problem (15)). A similar interpretation
holds when new columns are added to A.

9

Implementations

The authors have developed and released three open source codes for modelling and ﬁtting
generalized low rank models: a basic serial implementation written in Python, a serial and
shared-memory parallel implementation written in Julia, and a distributed implementation
written in Scala using the Spark framework. The Julia and Spark implementations use the

62

alternating proximal gradient method described in §7 to ﬁt GLRMs, while the Python imple-
mentation uses alternating minimization and a cvxpy [DCB14] backend for each subproblem.
In this section we brieﬂy discuss these implementations, and report some timing results. For
a full description and up-to-date information about available functionality, we encourage the
reader to consult the on-line documentation for each of these packages.

There are also many implementations available for ﬁtting special cases of GLRMs. For
example, an implementation capable of ﬁtting any GLRM for which the subproblems in an
alternating minimization method are quadratic programs was recently developed in Spark
by Debasish Das [DD14].

9.1 Python implementation

GLRM.py is a Python implementation for ﬁtting GLRMs that can be found, together with
documentation, at

https://github.com/cehorn/glrm.

We encourage the interested reader to consult the on-line documentation for the most up-
to-date functionality and a collection of examples.

Usage. The user initializes a GLRM by specifying

• the data table A (A), stored as a Python list of 2-D arrays, where each 2-D array in A

contains all data associated with a particular loss function,

• the list of loss functions L (Lj, j = 1, . . . , n), that correspond to the data as speciﬁed

by A,

• regularizers regX (r) and regY (˜r),

• the rank k (k),

• an optional list missing_list with the same length as A so that each entry of missing_list

is a list of missing entries corresponding to the data from A, and

• an optional convergence object converge that characterizes the stopping criterion for

the alternating minimization procedure.

The following example illustrates how to use GLRM.py to ﬁt a GLRM with Boolean
(A_bool) and numerical (A_real) data, with quadratic regularization and a few missing
entries.

from glrm import GLRM
from glrm.loss import QuadraticLoss, HingeLoss
from glrm.reg import QuadraticReg

# import the model
# import losses
# import regularizer

63

A = [A_bool, A_real]
L = [Hinge_Loss, QuadraticLoss]
regX, regY = QuadraticReg(0.1), QuadraticReg(0.1)
missing_list = [[], [(0,0), (0,1)]]

# data stored as a list
# loss function as a list
# penalty weight is 0.1
# indexed by submatrix

model = GLRM(A, L, regX, regY, k, missing_list)
model.fit()

# initialize GLRM
# fit GLRM

The fit() method automatically adds an oﬀset to the GLRM and scales the loss functions
as described in §4.3.

GLRM.py ﬁts GLRMS by alternating minimization. The code instantiates cvxpy problems
[DCB14] corresponding to the X- and Y -update steps, then iterates by alternately solving
each problem until convergence criteria are met.

The following loss functions and regularizers are supported by GLRM.py:

• quadratic loss QuadraticLoss,

• Huber loss HuberLoss,

• hinge loss HingeLoss,

• ordinal loss OrdinalLoss,

• no regularization ZeroReg,

• (cid:96)1 regularization LinearReg,

• quadratic regularization QuadraticReg, and

• nonnegative constraint NonnegativeReg.

Users may implement their own loss functions (regularizers) using the abstract class Loss
(Reg).

9.2 Julia implementation

LowRankModels is a code written in Julia [BKSE12] for modelling and ﬁtting GLRMs. The
implementation is available on-line at

https://github.com/madeleineudell/LowRankModels.jl.

We discuss some aspects of the usage and features of the code here. For a full description
and up-to-date information about available functionality, we encourage the reader to consult
the on-line documentation.

64

Usage. To form a GLRM using LowRankModels, the user speciﬁes

• the data A (A), which can be any array or array-like data structure (e.g., a Julia

DataFrame);

• the observed entries obs (Ω), a list of tuples of the indices of the observed entries in
the matrix, which may be omitted if all the entries in the matrix have been observed;

• the list of loss functions losses (Lj, j = 1, . . . , n), one for each column of A;

• the regularizers rx (r) and ry (˜r); and

• the rank k (k).

For example, the following code forms and ﬁts a k-means model with k = 5 on the matrix
A ∈ Rm×n.

losses = fill(quadratic(),n)
rx = unitonesparse()
ry = zeroreg()
glrm = GLRM(A,losses,rx,ry,k) # form GLRM
X,Y,ch = fit!(glrm)

# fit GLRM

# quadratic loss
# x is 1-sparse unit vector
# y is not regularized

LowRankModels uses the proximal gradient method described in §7.2 to ﬁt GLRMs. The
optimal model is returned in the factors X and Y, while ch gives the convergence history. The
exclamation mark suﬃx follows the convention in Julia denoting that the function mutates
at least one of its arguments. In this case, it caches the best ﬁt X and Y as glrm.X and
glrm.Y [CE14].

Losses and regularizers must be of type Loss and Regularizer, respectively, and may

be chosen from a list of supported losses and regularizers, which include

• quadratic loss quadratic,

• hinge loss hinge,

• (cid:96)1 loss l1,

• Huber loss huber,

• ordinal hinge loss ordinal_hinge,

• quadratic regularization quadreg,

• no regularization zeroreg,

• nonnegative constraint nonnegative, and

• 1-sparse constraint onesparse.

• unit 1-sparse constraint unitonesparse.

Users may also implement their own losses and regularizers.

65

Shared memory parallelism. LowRankModels takes advantage of Julia’s SharedArray
data structure to implement a ﬁtting procedure that takes advantage of shared memory par-
allelism. While Julia does not yet support threading, SharedArrays in Julia allow separate
processes on the same computer to access the same block of memory. To ﬁt a model using
multiple processes, LowRankModels loads the data A and the initial model X and Y into
shared memory, broadcasts other problem data (e.g., the losses and regularizers) to each
process, and assigns to each process a partition of the rows of X and columns of Y . At every
iteration, each process updates its rows of X, its columns of Y , and computes its portion of
the objective function, synchronizing after each of these steps to ensure that e.g.the X up-
date is completed before the Y update begins; then the master process checks a convergence
criterion and adjusts the step length.

Automatic modeling. LowRankModels is capable of adding oﬀsets to a GLRM, and of
automatically scaling the loss functions, as described in §4.3. It can also automatically detect
the types of diﬀerent columns of a data frame and select an appropriate loss. Using these
features, LowRankModels implements a method

glrm(dataframe, k)

that forms a rank k model on a data frame, automatically selecting loss functions and
regularization that suit the data well, and ignoring any missing (NA) element in the data
frame. This GLRM can then be ﬁt with the function fit!.

Example. As an example, we ﬁt a GLRM to the Motivational States Questionnaire (MSQ)
data set [RA98]. This data set measures 3896 subjects on 92 aspects of mood and personality
type, as well as recording the time of day the data were collected. The data include real-
valued, Boolean, and ordinal measurements, and approximately 6% of the measurements are
missing (NA).

The following code loads the MSQ data set and encodes it in two dimensions:

using RDatasets
using LowRankModels
# pick a data set
df = RDatasets.dataset("psych","msq")
# encode it!
X,Y,labels,ch = fit(glrm(df,2))

Figure 19 uses the rows of Y as a coordinate system to plot some of the features of the
data set. Here we see the automatic embedding separates positive from negative emotions
along the y axis. This embedding is notable for being interpretable despite having been
generated completely automatically. Of course, better embeddings may be obtained by a
more careful choice of loss functions, regularizers, scaling, and embedding dimension k.

66

Figure 19: An automatic embedding of the MSQ [RA98] data set into two dimen-
sions.

9.3 Spark implementation

SparkGLRM is a code written in Scala, built on the Spark cluster programming framework
[ZCF+10], for modelling and ﬁtting GLRMs. The implementation is available on-line at

http://git.io/glrmspark.

In SparkGLRM, the data matrix A is split entry-wise across many machines, just
Design.
as in [HMLZ14]. The model (X, Y ) is replicated and stored in memory on every machine.

67

Thus the total computation time required to ﬁt the model is proportional to the number
of nonzeros divided by the number of cores, with the restriction that the model should ﬁt
in memory. (The authors leave to future work an extension to models that do not ﬁt in
memory, e.g., by using a parameter server [SSZ14].) Where possible, hardware acceleration
(via breeze and BLAS) is used for local linear algebraic operations.

At every iteration, the current model is broadcast to all machines, so there is only one
copy of the model on each machine. This particularly important in machines with many
cores, because it avoids duplicating the model those machines. Each core on a machine will
process a partition of the input matrix, using the local copy of the model.

Usage. The user provides loss functions Lij(u, a) indexed by i = 0, . . . , m − 1 and j =
0, . . . , n − 1, so a diﬀerent loss function can be deﬁned for each column, or even for each
entry. Each loss function is deﬁned by its gradient (or a subgradient). The method signature
is

loss grad(i: Int, j: Int, u: Double, a: Double)

whose implementation can be customized by particular i and j. As an example, the following
line implements squared error loss (L(u, a) = 1/2(u − a)2) for all entries:

Similarly, the user provides functions implementing the proximal operator of the regu-
larizers r and ˜r, which take a dense vector and perform the appropriate proximal operation.

Experiments. We ran experiments on several large matrices. For size comparison, a very
popular matrix in the recommender systems community is the Netﬂix Prize Matrix, which
has 17770 rows, 480189 columns, and 100480507 nonzeros. Below we report results on several
larger matrices, up to 10 times larger. The matrices are generated by ﬁxing the dimensions
and number of nonzeros per row, then uniformly sampling the locations for the nonzeros,
and ﬁnally ﬁlling in those locations with a uniform random number in [0, 1].

We report iteration times using an Amazon EC2 cluster with 10 slaves and one master,
of instance type “c3.4xlarge”. Each machine has 16 CPU cores and 30 GB of RAM. We
ran SparkGLRM to ﬁt two GLRMs on matrices of varying sizes. Table 5 gives results for
quadratically regularized PCA (i.e., quadratic loss and quadratic regularization) with k = 5.
To illustrate the capability to write and ﬁt custom loss functions, we also ﬁt a GLRM using
a loss function that depends on the parity of i + j:

Lij(u, a) =

(cid:26) |u − a|
(u − a)2

i + j is even
i + j is odd,

with r(x) = (cid:107)x(cid:107)1 and ˜r(y) = (cid:107)y(cid:107)2
2, setting k = 10. (This loss function was chosen merely to
illustrate the generality of the implementation. Usually losses will be the same for each row
in the same column.) The results for this custom GLRM are given in Table 6.

u - a

68

Matrix size # nonzeros Time per iteration (s)

106 × 106
106 × 106
107 × 107

106 × 106
106 × 106
107 × 107

106
109
109

106
109
109

Table 5: SparkGLRM for quadratically regularized PCA, k = 5.

Matrix size # nonzeros Time per iteration (s)

7
11
227

9
13
294

Table 6: SparkGLRM for custom GLRM, k = 10.

The table gives the time per iteration. The number of iterations required for convergence
depends on the size of the ambient dimension. On the matrices with the dimensions shown in
Tables 5 and 6, convergence typically requires about 100 iterations, but we note that useful
GLRMs often emerge after only a few tens of iterations.

Acknowledgements

The authors are grateful to Chris De Sa, Yash Deshpande, Nicolas Gillis, Maya Gupta,
Trevor Hastie, Irene Kaplow, Lester Mackey, Andrea Montanari, Art Owen, Haesun Park,
David Price, Chris R´e, Ben Recht, Yoram Singer, Nati Srebro, Ashok Srivastava, Peter
Stoica, Sze-chuan Suen, Stephen Taylor, Joel Tropp, Ben Van Roy, and Stefan Wager for
a number of illuminating discussions and comments on early drafts of this paper, and to
Debasish Das and Matei Zaharia for their insights into creating a successful Spark imple-
mentation. This work was developed with support from the National Science Foundation
Graduate Research Fellowship program (under Grant No. DGE-1147470), the Gabilan Stan-
ford Graduate Fellowship, the Gerald J. Lieberman Fellowship, and the DARPA X-DATA
program.

69

A Quadratically regularized PCA

In this appendix we describe some properties of the quadratically regularized PCA prob-
lem (3),

minimize (cid:107)A − XY (cid:107)2

F + γ(cid:107)X(cid:107)2

F + γ(cid:107)Y (cid:107)2
F .

(27)

In the sequel, we let U ΣV T = A be the SVD of A and let r be the rank of A. We assume for
convenience that all the nonzero singular values σ1 > σ2 > · · · > σr > 0 of A are distinct.

A.1 Solution

Problem (3) is the only problem we will encounter that has an analytical solution. A solution
is given by

X = ˜U ˜Σ1/2,

Y = ˜Σ1/2 ˜V T ,

(28)

where ˜U and ˜V are deﬁned as in (5), and ˜Σ = diag((σ1 − γ)+, . . . , (σk − γ)+).

To prove this, let’s consider the optimality conditions of (3). The optimality conditions

are

−(A − XY )Y T + γX = 0,

−(A − XY )T X + γY T = 0.

Multiplying the ﬁrst optimality condition on the left by X T and the second on the left by
Y and rearranging, we ﬁnd

X T (A − XY )Y T = γX T X,

Y (A − XY )T X = γY Y T ,

which shows, by taking a transpose, that X T X = Y Y T at any stationary point.

We may rewrite the optimality conditions together as

(cid:20)−γI

A
AT −γI

(cid:21) (cid:20) X
Y T

(cid:21)

(cid:21)

(cid:21) (cid:20) X
Y T

=

=

=

(cid:20)

0
XY
(XY )T
0
(cid:21)
(cid:20) X(Y Y T )
Y T (X T X)
(cid:20) X
Y T

(X T X),

(cid:21)

where we have used the fact that X T X = Y Y T .

Now we see that (X, Y T ) lies in an invariant subspace of the matrix

(cid:20)−γI

A
AT −γI

(cid:21)

. Recall

that V is an invariant subspace of a matrix A if AV = V M for some matrix M . If Rank(M ) ≤
Rank(A), we know that the eigenvalues of M are eigenvalues of A, and that the corresponding
eigenvectors lie in the span of V .

Thus the eigenvalues of X T X must be eigenvalues of

, and (X, Y T ) must

span the corresponding eigenspace. More concretely, notice that

is (symmetric,

(cid:21)

(cid:20)−γI

A
AT −γI
(cid:20)−γI

(cid:21)

A
AT −γI

70

and therefore) diagonalizable, with eigenvalues −γ ± σi. The larger eigenvalues −γ + σi
correspond to the eigenvectors (ui, vi), and the smaller ones −γ − σi to (ui, −vi).

Now, X T X is positive semideﬁnite, so the eigenvalues shared by X T X and

(cid:20)−γI

A
AT −γI

(cid:21)

√

must be positive. Hence there is some set |Ω| ≤ k with σi ≥ γ for i ∈ Ω such that X has
−γ + σi for i ∈ Ω. (Recall that X T X = Y Y T , so Y has the same
have singular values
singular values as X.) Then (X, Y T ) spans the subspace generated by the vectors (ui, vi for
i ∈ Ω. We say the stationary point (X, Y ) has active subspace Ω. It is easy to verify that
XY = (cid:80)

i∈Ω ui(σi − γ)vT
i .

Each active subspace gives rise to an orbit of stationary points. If (X, Y ) is a stationary

point, then (XT, T −1Y ) is also a stationary point so long as

−(A − XY )Y T T −T + γXT = 0,

−(A − XY )T XT + γY T T −T = 0,

which is always true if T −T = T , i.e., T is orthogonal. This shows that the set of stationary
points is invariant under orthogonal transformations.

To simplify what follows, we choose a representative element for each orbit. Represent

any stationary point with active subspace Ω by

X = UΩ(ΣΩ − γI)1/2,

Y = (ΣΩ − γI)1/2V T
Ω ,

where by UΩ we denote the submatrix of U with columns indexed by Ω, and similarly for
(cid:0)k(cid:48)(γ)
(cid:1)
Σ and V . At any value of γ, let k(cid:48)(γ) = max{i : σi ≥ γ}. Then we have (cid:80)k
i
(representative) stationary points, one for each choice of Ω The number of (representative)
stationary points is decreasing in γ; when γ > σ1, the only stationary point is X = 0, Y = 0.
These stationary points can have quite diﬀerent values. If (X, Y ) has active subspace Ω,

i=0

then

||A − XY ||2

F + γ(||X||2

F + ||Y ||2

F ) =

σ2
i +

(cid:0)γ2 + 2γ|σi − γ|(cid:1) .

(cid:88)

i /∈Ω

(cid:88)

i∈Ω

From this form, it is clear that we should choose Ω to include the top singular values i =
1, . . . , k(cid:48)(γ). Choosing any other subset Ω will result in a higher (worse) objective value:
that is, the other stationary points are not global minima.

A.2 Fixed points of alternating minimization

Theorem 2. The quadratically regularized PCA problem (3) has only one local minimum,
which is the global minimum.

Our proof is similar to that of [BH89], who proved a related theorem for the case of

PCA (2).
Proof. We showed above that every stationary point of (3) has the form XY = (cid:80)
i∈Ω uidivT
i ,
with Ω ⊆ {1, . . . , k(cid:48)}, |Ω| ≤ k, and di = σi − γ. We use the representative element from each
√
divT
stationary orbit described above, so each column of X is ui
i
for some i ∈ Ω. The columns of X are orthogonal, as are the rows of Y .

di and each row of Y is

√

71

If a stationary point is not the global minimum, then σj > σi for some i ∈ Ω, j (cid:54)∈ Ω.
Below, we show we can always ﬁnd a descent direction if this condition holds, thus showing
that the only local minimum is the global minimum.

Assume we are at a stationary point with σj > σi for some i ∈ Ω, j (cid:54)∈ Ω. We will ﬁnd a
j . Form ˜X by replacing the column of
i by

descent direction by perturbing XY in direction ujvT
X containing ui
√

di, and ˜Y by replacing the row of Y containing

di by (ui + (cid:15)uj)

divT

√

√

√

di(vi + (cid:15)vj)T . Now the regularization term increases slightly:

γ((cid:107) ˜X(cid:107)2

F + (cid:107) ˜Y (cid:107)2

F ) − γ((cid:107)X(cid:107)2

F + (cid:107)Y (cid:107)2

F ) =

(2γti(cid:48)) + 2γdi(1 + (cid:15)2) −

2γti(cid:48)

(cid:88)

i(cid:48)∈Ω

(cid:88)

i(cid:48)∈Ω,i(cid:48)(cid:54)=i
= 2γdi(cid:15)2.

Meanwhile, the approximation error decreases:

(cid:107)A − ˜X ˜Y (cid:107)2

F − (cid:107)A − XY (cid:107)2

F = (cid:107)uiσivT

i + ujσjvT

j − (ui + (cid:15)uj)di(vi + (cid:15)vj)T (cid:107)2

= (cid:107)ui(σi − di)vT

i + uj(σj − (cid:15)2di)vT

j − (cid:15)uidivT

F − (σi − di)2 − σ2
j
i (cid:107)2
F

j − (cid:15)ujdivT

−(σi − di)2 − σ2
j
(cid:13)
(cid:20)σi − di
−(cid:15)di
(cid:13)
(cid:13)
σj − (cid:15)2di
−(cid:15)di
(cid:13)

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
F

=

= (σi − di)2 + (σj − (cid:15)2di)2 + 2(cid:15)2d2
i + 2(cid:15)2d2
= −2σj(cid:15)2di + (cid:15)4d2
i
= 2(cid:15)2di(di − σj) + (cid:15)4d2
i ,

− (σi − di)2 − σ2
j

i − (σi − di)2 − σ2
j

where we have used the rotational invariance of the Frobenius norm to arrive at the third
equality above. Hence the net change in the objective value in going from (X, Y ) to ( ˜X, ˜Y )
is

2γdi(cid:15)2 + 2(cid:15)2di(di − σj) + (cid:15)4d2

i = 2(cid:15)2di(γ + di − σj) + (cid:15)4d2
i
= 2(cid:15)2di(σi − σj) + (cid:15)4d2
i ,

which is negative for small (cid:15). Hence we have found a descent direction, showing that any
stationary point with σj > σi for some i ∈ Ω, j (cid:54)∈ Ω is not a local minimum.

72

References

[AAJN13] A. Agarwal, A. Anandkumar, P. Jain, and P. Netrapalli. Learning sparsely
used overcomplete dictionaries via alternating minimization. arXiv preprint
arXiv:1310.7991, 2013.

[ABEV09] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. A new approach to collab-
orative ﬁltering: Operator estimation with spectral regularization. The Journal
of Machine Learning Research, 10:803–826, 2009.

[AEB06] M. Aharon, M. Elad, and A. Bruckstein. k-SVD: An algorithm for designing
overcomplete dictionaries for sparse representation. IEEE Transactions on Signal
Processing, 54(11):4311–4322, 2006.

[AM04]

[AV07]

P. K. Agarwal and N. H. Mustafa. k-means projective clustering. In Proceed-
ings of the 23rd ACM SIGMOD-SIGACT-SIGART Symposium on Principles of
Database Systems, pages 155–165. ACM, 2004.

D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding.
In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete
Algorithms, pages 1027–1035. Society for Industrial and Applied Mathematics,
2007.

[BBL+07] M. Berry, M. Browne, A. Langville, V. Pauca, and R. Plemmons. Algorithms and
applications for approximate nonnegative matrix factorization. Computational
Statistics & Data Analysis, 52(1):155–173, 2007.

[BCMR12] S. Boyd, C. Cortes, M. Mohri, and A. Radovanovic. Accuracy at the top. In

Advances in Neural Information Processing Systems, pages 962–970, 2012.

[BDKP14] R. Boyd, B. Drake, D. Kuang, and H. Park. Smallk is a C++/Python high-
performance software library for nonnegative matrix factorization (NMF) and
hierarchical and ﬂat clustering using the NMF; current version 1.2.0. http:
//smallk.github.io/, June 2014.

[Ber11]

[BH89]

D. P. Bertsekas. Incremental gradient, subgradient, and proximal methods for
convex optimization: A survey. Optimization for Machine Learning, 2010:1–38,
2011.

P. Baldi and K. Hornik. Neural networks and principal component analysis:
Learning from examples without local minima. Neural Networks, 2(1):53–58,
1989.

[BKSE12] J. Bezanson, S. Karpinski, V. B. Shah, and A. Edelman. Julia: A fast dynamic

language for technical computing. arXiv preprint arXiv:1209.5145, 2012.

73

[BL10]

J. Borwein and A. Lewis. Convex analysis and nonlinear optimization: theory
and examples, volume 3. Springer Science & Business Media, 2010.

[BM03a]

S. Boyd and J. Mattingley. Branch and bound methods. Lecture notes for
EE364b, Stanford University, 2003.

[BM03b]

S. Burer and R. Monteiro. A nonlinear programming algorithm for solving
semideﬁnite programs via low-rank factorization. Mathematical Programming,
95(2):329–357, 2003.

[BM03c]

S. Burer and R. D. C. Monteiro. Local minima and convergence in low-rank
semideﬁnite programming. Mathematical Programming, 103:2005, 2003.

[BPC+11] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimiza-
tion and statistical learning via the alternating direction method of multipliers.
Foundations and Trends in Machine Learning, 3(1):1–122, 2011.

[BRRT12] V. Bittorf, B. Recht, C. R´e, and J. A. Tropp. Factoring nonnegative matri-
ces with linear programs. Advances in Neural Information Processing Systems,
25:1223–1231, 2012.

[BST13]

J. Bolte, S. Sabach, and M. Teboulle. Proximal alternating linearized minimiza-
tion for nonconvex and nonsmooth problems. Mathematical Programming, pages
1–36, 2013.

[BV04]

S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University
Press, 2004.

[BXM03]

S. Boyd, L. Xiao, and A. Mutapcic. Subgradient methods. Lecture notes for
EE364b, Stanford University, 2003.

[Cat66]

Raymond B Cattell. The scree test for the number of factors. Multivariate
behavioral research, 1(2):245–276, 1966.

[CDS98]

S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit.
SIAM Journal on Scientiﬁc Computing, 20(1):33–61, 1998.

[CDS01] M. Collins, S. Dasgupta, and R. Schapire. A generalization of principal com-
ponent analysis to the exponential family. In Advances in Neural Information
Processing Systems, volume 13, page 23, 2001.

[CE14]

[Cha14]

J. Chen and A. Edelman. Parallel preﬁx polymorphism permits parallelization,
presentation & proof. arXiv preprint arXiv:1410.6449, 2014.

S. Chatterjee. Matrix estimation by universal singular value thresholding. The
Annals of Statistics, 43(1):177–214, 2014.

74

[CLMW11] E. Cand`es, X. Li, Y. Ma, and J. Wright. Robust principal component analysis?

Journal of the ACM (JACM), 58(3):11, 2011.

[CP09]

[CR08]

[CS02]

[CT10]

[DB95]

E. Cand`es and Y. Plan. Matrix completion with noise. CoRR, abs/0903.3131,
2009.

E. Cand`es and B. Recht. Exact matrix completion via convex optimization.
CoRR, abs/0805.4471, 2008.

K. Crammer and Y. Singer. On the algorithmic implementation of multiclass
kernel-based vector machines. The Journal of Machine Learning Research, 2:265–
292, 2002.

E. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix
completion. IEEE Transactions on Information Theory, 56(5):2053–2080, 2010.

T. Dietterich and G. Bakiri. Solving multiclass learning problems via error-
correcting output codes. CoRR, cs.AI/9501101, 1995.

[DCB14]

S. Diamond, E. Chu, and S. Boyd. CVXPY: A Python-embedded modeling
language for convex optimization, version 0.2. http://cvxpy.org/, May 2014.

[DD14]

D. Das and S. Das. Quadratic programing solver for non-negative matrix factor-
ization with spark. In Spark Summit 2014, 2014.

[dEGJL04] A. d’Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. Lanckriet. A direct
In Advances in

formulation for sparse PCA using semideﬁnite programming.
Neural Information Processing Systems, volume 16, pages 41–48, 2004.

[DFK+04] P. Drineas, A. Frieze, R. Kannan, S. Vempala, and V. Vinay. Clustering large
graphs via the singular value decomposition. Machine Learning, 56(1-3):9–33,
2004.

[Din09]

[DL84]

A. Dinno. Implementing Horn’s parallel analysis for principal component analysis
and factor analysis. Stata Journal, 9(2):291, 2009.

J. De Leeuw. The Giﬁ system of nonlinear multivariate analysis. Data analysis
and informatics, 3:415–424, 1984.

[DLM09]

J. De Leeuw and P. Mair. Giﬁ methods for optimal scaling in R: The package
homals. Journal of Statistical Software, pages 1–30, 2009.

[DLPP06] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal nonnegative matrix t-
factorizations for clustering. In Proceedings of the 12th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data Mining, pages 126–135.
ACM, 2006.

75

[DLYT76] J. De Leeuw, F. Young, and Y. Takane. Additive structure in qualitative data:
An alternating least squares method with optimal scaling features. Psychome-
trika, 41(4):471–503, 1976.

[DPBW12] M. Davenport, Y. Plan, E. Berg, and M. Wootters. 1-bit matrix completion.

arXiv preprint arXiv:1209.3672, 2012.

[DS14]

[EV09]

[EY36]

[FBD09]

A. Damle and Y. Sun. Random projections for non-negative matrix factorization.
arXiv preprint arXiv:1405.4275, 2014.

E. Elhamifar and R. Vidal. Sparse subspace clustering. In IEEE Conference on
Computer Vision and Pattern Recognition, 2009, pages 2790–2797. IEEE, 2009.

C. Eckart and G. Young. The approximation of one matrix by another of lower
rank. Psychometrika, 1(3):211–218, 1936.

C. F´evotte, N. Bertin, and J. Durrieu. Nonnegative matrix factorization with
the Itakura-Saito divergence: With application to music analysis. Neural Com-
putation, 21(3):793–830, 2009.

[FHB04] M. Fazel, H. Hindi, and S. Boyd. Rank minimization and applications in sys-
tem theory. In Proceedings of the 2004 American Control Conference (ACC),
volume 4, pages 3273–3278. IEEE, 2004.

[FM13]

W. Fithian and R. Mazumder. Scalable convex methods for ﬂexible low-rank
matrix modeling. arXiv preprint arXiv:1308.4211, 2013.

[GAGG13] S. Gunasekar, A. Acharya, N. Gaur, and J. Ghosh. Noisy matrix completion
using alternating minimization. In Machine Learning and Knowledge Discovery
in Databases, pages 194–209. Springer, 2013.

[GBW14] M. Gupta, S. Bengio, and J. Weston. Training highly multiclass classiﬁers. The

Journal of Machine Learning Research, 15(1):1461–1492, 2014.

[GD14]

A. Gress and I. Davidson. A ﬂexible framework for projecting heterogeneous
data. In Proceedings of the 23rd ACM International Conference on Conference
on Information and Knowledge Management, CIKM ’14, pages 1169–1178, New
York, NY, USA, 2014. ACM.

[GG11]

N. Gillis and F. Glineur. Low-rank matrix approximation with weights or
missing data is NP-hard. SIAM Journal on Matrix Analysis and Applications,
32(4):1149–1165, 2011.

[Gil11]

N. Gillis. Nonnegative matrix factorization: Complexity, algorithms and appli-
cations. PhD thesis, UCL, 2011.

76

[Gor02]

G. J. Gordon. Generalized2 linear2 models. In Advances in Neural Information
Processing Systems, pages 577–584, 2002.

[GRX+10] A. Goldberg, B. Recht, J. Xu, R. Nowak, and X. Zhu. Transduction with matrix
In Advances in Neural Information

completion: Three birds with one stone.
Processing Systems, pages 757–765, 2010.

[Har13]

M. Hardt. On the provable convergence of alternating minimization for matrix
completion. arXiv preprint arXiv:1312.0925, 2013.

[HMLZ14] T. Hastie, R. Mazumder, J. Lee, and R. Zadeh. Matrix completion and low-rank

svd via fast alternating least squares. arXiv, 2014.

[HMT11] N. Halko, P.-G. Martinsson, and J. Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions.
SIAM Review, 53(2):217–288, 2011.

[HN99]

[Hor65]

[Hot33]

[Hot36]

Z. Huang and M. Ng. A fuzzy k-modes algorithm for clustering categorical data.
IEEE Transactions on Fuzzy Systems, 7(4):446–452, 1999.

J. Horn. A rationale and test for the number of factors in factor analysis. Psy-
chometrika, 30(2):179–185, 1965.

H. Hotelling. Analysis of a complex of statistical variables into principal compo-
nents. Journal of Educational Psychology, 24(6):417, 1933.

H. Hotelling. Relations between two sets of variates. Biometrika, 28(3-4):321–
377, 1936.

[Hub81]

P. Huber. Robust Statistics. Wiley, New York, 1981.

[JBAS10] M. Journ´ee, F. Bach, P. Absil, and R. Sepulchre. Low-rank optimization on
the cone of positive semideﬁnite matrices. SIAM Journal on Optimization,
20(5):2327–2351, 2010.

[JNS13]

P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank matrix completion using
alternating minimization. In Proceedings of the 45th annual ACM Symposium
on the Theory of Computing, pages 665–674. ACM, 2013.

[Jol86]

I. Jolliﬀe. Principal component analysis. Springer, 1986.

[JW14]

J. Josse and S. Wager. Stable autoencoding: A ﬂexible framework for regularized
low-rank matrix estimation. arXiv preprint arXiv:1410.8275, 2014.

[JWH14]

J. Josse, S. Wager, and F. Husson. Conﬁdence areas for ﬁxed-eﬀects pca. arXiv
preprint arXiv:1407.7614, 2014.

77

[KB78]

[Kes12]

[KHP14]

[KM10]

R. Koenker and J. G. Bassett. Regression quantiles. Econometrica: Journal of
the Econometric Society, pages 33–50, 1978.

R. Keshavan. Eﬃcient algorithms for collaborative ﬁltering. PhD thesis, Stanford
University, 2012.

J. Kim, Y. He, and H. Park. Algorithms for nonnegative matrix and tensor
factorizations: A uniﬁed view based on block coordinate descent framework.
Journal of Global Optimization, 58(2):285–319, 2014.

R. Keshavan and A. Montanari. Regularization for matrix completion.
In
2010 IEEE International Symposium on Information Theory Proceedings (ISIT),
pages 1503–1507. IEEE, 2010.

[KMO09] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries.

In Advances in Neural Information Processing Systems, pages 952–960, 2009.

[KMO10] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries.
IEEE Transactions on Information Theory, 56(6):2980–2998, 2010.

[KO09]

R. Keshavan and S. Oh. A gradient descent algorithm on the Grassman manifold
for matrix completion. arXiv preprint arXiv:0910.5260, 2009.

[Koe05]

R. Koenker. Quantile regression. Cambridge University Press, 2005.

[KP07]

[KP08a]

[KP08b]

[KP11]

H. Kim and H. Park. Sparse non-negative matrix factorizations via alternating
non-negativity-constrained least squares for microarray data analysis. Bioinfor-
matics, 23(12):1495–1502, 2007.

H. Kim and H. Park. Nonnegative matrix factorization based on alternating
nonnegativity constrained least squares and active set method. SIAM Journal
on Matrix Analysis and Applications, 30(2):713–730, 2008.

J. Kim and H. Park. Toward faster nonnegative matrix factorization: A new
algorithm and comparisons. In Eighth IEEE International Conference on Data
Mining, pages 353–362. IEEE, 2008.

J. Kim and H. Park. Fast nonnegative matrix factorization: An active-set-like
method and comparisons. SIAM Journal on Scientiﬁc Computing, 33(6):3261–
3281, 2011.

[KR09]

L. Kaufman and P. J. Rousseeuw. Finding groups in data: an introduction to
cluster analysis, volume 344. John Wiley & Sons, 2009.

[LBRN06] H. Lee, A. Battle, R. Raina, and A. Ng. Eﬃcient sparse coding algorithms. In

Advances in Neural Information Processing Systems, pages 801–808, 2006.

78

[Lik32]

[Lin07]

[Llo82]

R. Likert. A technique for the measurement of attitudes. Archives of Psychology,
1932.

C. Lin. Projected gradient methods for nonnegative matrix factorization. Neural
Computation, 19(10):2756–2779, 2007.

S. Lloyd. Least squares quantization in PCM. IEEE Transactions on Information
Theory, 28(2):129–137, 1982.

[LLW04] Y. Lee, Y. Lin, and G. Wahba. Multicategory support vector machines: Theory
and application to the classiﬁcation of microarray data and satellite radiance
data. Journal of the American Statistical Association, 99(465):67–81, 2004.

[LRS+10]

J. Lee, B. Recht, R. Salakhutdinov, N. Srebro, and J. Tropp. Practical large-scale
optimization for max-norm regularization. In Advances in Neural Information
Processing Systems, pages 1297–1305, 2010.

[LS99]

[LS01]

[LV09]

[LW66]

[Mac09]

D. Lee and H. Seung. Learning the parts of objects by non-negative matrix
factorization. Nature, 401(6755):788–791, 1999.

D. Lee and H. Seung. Algorithms for non-negative matrix factorization.
Advances in Neural Information Processing Systems, pages 556–562, 2001.

In

Z. Liu and L. Vandenberghe. Interior-point method for nuclear norm approxima-
tion with application to system identiﬁcation. SIAM Journal on Matrix Analysis
and Applications, 31(3):1235–1256, 2009.

E. Lawler and D. Wood. Branch-and-bound methods: A survey. Operations
Research, 14(4):699–719, 1966.

L. Mackey. Deﬂation methods for sparse PCA. In D. Koller, D. Schuurmans,
Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing
Systems, 2009.

[Mar12]

I. Markovsky. Low Rank Approximation: Algorithms, Implementation, Applica-
tions. Communications and Control Engineering. Springer, 2012.

[MBPS09] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary learning for sparse
coding. In Proceedings of the 26th Annual International Conference on Machine
Learning, pages 689–696. ACM, 2009.

[MCCD13] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Eﬃcient estimation of word

representations in vector space. arXiv preprint arXiv:1301.3781, 2013.

[MF10]

K. Mohan and M. Fazel. Reweighted nuclear norm minimization with application
to system identiﬁcation. In Proceedings of the 2010 American Control Conference
(ACC), pages 2953–2959. IEEE, 2010.

79

[MHT10] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms
for learning large incomplete matrices. The Journal of Machine Learning Re-
search, 11:2287–2322, 2010.

[Min01]

In T.K. Leen, T.G.
T. Minka. Automatic choice of dimensionality for pca.
Dietterich, and V. Tresp, editors, Advances in Neural Information Processing
Systems, pages 598–604. MIT Press, 2001.

[MPS+09] J. Mairal, J. Ponce, G. Sapiro, A. Zisserman, and F. Bach. Supervised dictionary
learning. In Advances in Neural Information Processing Systems, pages 1033–
1040, 2009.

[MSC+13] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed rep-
resentations of words and phrases and their compositionality. In Advances in
Neural Information Processing Systems, pages 3111–3119, 2013.

[NNS+14] P. Netrapalli, U. Niranjan, S. Sanghavi, A. Anandkumar, and P. Jain. Provable
non-convex robust PCA. In Advances in Neural Information Processing Systems,
pages 1107–1115, 2014.

[NRRW11] F. Niu, B. Recht, C. R´e, and S. Wright. Hogwild!: A lock-free approach to
In Advances in Neural Information

parallelizing stochastic gradient descent.
Processing Systems, 2011.

[OF97]

[OP09]

[Osn14]

[PB13]

B. Olshausen and D. Field. Sparse coding with an overcomplete basis set: A
strategy employed by V1? Vision Research, 37(23):3311–3325, 1997.

A. Owen and P. Perry. Bi-cross-validation of the svd and the nonnegative matrix
factorization. The Annals of Applied Statistics, pages 564–594, 2009.

S. Osnaga. Low Rank Representations of Matrices using Nuclear Norm Heuris-
tics. PhD thesis, Colorado State University, 2014.

N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in Opti-
mization, 1(3):123–231, 2013.

[PCST99] J. Platt, N. Cristianini, and J. Shawe-Taylor. Large margin DAGs for multiclass
In Advances in Neural Information Processing Systems, pages

classiﬁcation.
547–553, 1999.

[Pea01]

K. Pearson. On lines and planes of closest ﬁt to systems of points in space. The
London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science,
2(11):559–572, 1901.

[Per09]

Cross-validation for unsupervised learning.

arXiv preprint

P. Perry.
arXiv:0909.3052, 2009.

80

[PJ09]

[PM03]

[PSM14]

[RA98]

H.-S. Park and C.-H. Jun. A simple and fast algorithm for k-medoids clustering.
Expert Systems with Applications, 36(2, Part 2):3336 – 3341, 2009.

K. Preacher and R. MacCallum. Repairing Tom Swift’s electric factor analysis
machine. Understanding Statistics: Statistical Issues in Psychology, Education,
and the Social Sciences, 2(1):13–43, 2003.

J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for word repre-
sentation. Proceedings of the Empiricial Methods in Natural Language Processing
(EMNLP 2014), 12, 2014.

W. Revelle and K. Anderson. Personality, motivation and cognitive performance:
Final report to the army research institute on contract MDA 903-93-K-0008.
Technical report, 1998.

[RBL+07] R. Raina, A. Battle, H. Lee, B. Packer, and A. Ng. Self-taught learning: Transfer
learning from unlabeled data. In Proceedings of the 24th International Conference
on Machine Learning, pages 759–766. ACM, 2007.

[RFP10]

B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization. SIAM Review, 52(3):471–501,
August 2010.

R. Rifkin and A. Klautau. In defense of one-vs-all classiﬁcation. The Journal of
Machine Learning Research, 5:101–141, 2004.

B. Recht and C. R´e. Parallel stochastic gradient algorithms for large-scale matrix
completion. Mathematical Programming Computation, 5(2):201–226, 2013.

[RRWN11] B. Recht, C. R´e, S. Wright, and F. Niu. Hogwild: A lock-free approach to
In Advances in Neural Information

parallelizing stochastic gradient descent.
Processing Systems, pages 693–701, 2011.

J. Rennie and N. Srebro. Fast maximum margin matrix factorization for col-
laborative prediction. In Proceedings of the 22nd International Conference on
Machine Learning, pages 713–719. ACM, 2005.

P. Richt´arik, M. Tak´aˇc, and S. Ahipa¸sao˘glu. Alternating maximization: Unifying
framework for 8 sparse PCA formulations and eﬃcient parallel codes. arXiv
preprint arXiv:1212.4137, 2012.

[SBPP06] F. Shahnaz, M. W. Berry, V. P. Pauca, and R. J. Plemmons. Document clustering
using nonnegative matrix factorization. Information Processing & Management,
42(2):373–386, 2006.

[SC12]

M. Soltanolkotabi and E. Candes. A geometric analysis of subspace clustering
with outliers. The Annals of Statistics, 40(4):2195–2238, 2012.

[RK04]

[RR13]

[RS05]

[RTA12]

81

[SF14]

[SG08]

[SH08]

[SJ03]

[SM14]

[Smi12]

[Sre04]

[SRJ04]

[SSU03]

[SSZ14]

[SEC13] M. Soltanolkotabi, E. Elhamifar, and E. Candes. Robust subspace clustering.

arXiv preprint arXiv:1301.2603, 2013.

D. L. Sun and C. F´evotte. Alternating direction method of multipliers for non-
negative matrix factorization with the beta-divergence. In IEEE International
Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2014.

A. Singh and G. Gordon. A uniﬁed view of matrix factorization models. In Ma-
chine Learning and Knowledge Discovery in Databases, pages 358–373. Springer,
2008.

H. Shen and J. Huang. Sparse principal component analysis via regularized low
rank matrix approximation. Journal of Multivariate Analysis, 99(6):1015–1034,
2008.

[SHK+14] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov.
Dropout: A simple way to prevent neural networks from overﬁtting. The Journal
of Machine Learning Research, 15(1):1929–1958, 2014.

N. Srebro and T. Jaakkola. Weighted low-rank approximations. In ICML, vol-
ume 3, pages 720–727, 2003.

V. Srikumar and C. Manning. Learning distributed representations for structured
output prediction. In Advances in Neural Information Processing Systems, pages
3266–3274, 2014.

R. Smith. Nuclear norm minimization methods for frequency domain subspace
identiﬁcation. In Proceedings of the 2010 American Control Conference (ACC),
pages 2689–2694. IEEE, 2012.

N. Srebro. Learning with Matrix Factorizations. PhD thesis, Massachusetts
Institute of Technology, 2004.

N. Srebro, J. Rennie, and T. Jaakkola. Maximum-margin matrix factorization.
In Advances in Neural Information Processing Systems, volume 17, pages 1329–
1336, 2004.

[SSGS11]

S. Shalev-Shwartz, A. Gonen, and O. Shamir. Large-scale convex minimization
with a low-rank constraint. arXiv preprint arXiv:1106.1622, 2011.

A. Schein, L. Saul, and L. Ungar. A generalized linear model for principal
component analysis of binary data. In Proceedings of the Ninth International
Workshop on Artiﬁcial Intelligence and Statistics, volume 38, page 46, 2003.

S. Schelter, V. Satuluri, and R. Zadeh. Factorbird — a parameter server ap-
proach to distributed matrix factorization. NIPS 2014 Workshop on Distributed
Machine Learning and Matrix Computations, 2014.

82

[Ste07]

[TB99]

[TG07]

[Tro04]

[Tse00]

H. Steck. Hinge rank loss and the area under the ROC curve. In J. N. Kok,
J. Koronacki, R. L. Mantaras, S. Matwin, D. Mladeniˇc, and A. Skowron, editors,
Machine Learning: ECML 2007, volume 4701 of Lecture Notes in Computer
Science, pages 347–358. Springer Berlin Heidelberg, 2007.

M. Tipping and C. Bishop. Probabilistic principal component analysis. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–
622, 1999.

J. Tropp and A. Gilbert. Signal recovery from random measurements via orthog-
onal matching pursuit. IEEE Transactions on Information Theory, 53(12):4655–
4666, 2007.

[TPB00]

N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.

J. Tropp. Topics in Sparse Approximation. PhD thesis, The University of Texas
at Austin, 2004.

P. Tseng. Nearest q-ﬂat to m points. Journal of Optimization Theory and
Applications, 105(1):249–252, 2000.

[Twe84] M. Tweedie. An index which distinguishes between some important exponen-
tial families. In Statistics: Applications and New Directions. Proceedings of the
Indian Statistical Institute Golden Jubilee International Conference, pages 579–
604, 1984.

[TYDL77] Y. Takane, F. Young, and J. De Leeuw. Nonmetric individual diﬀerences mul-
tidimensional scaling: an alternating least squares method with optimal scaling
features. Psychometrika, 42(1):7–67, 1977.

[UBG09] N. Usunier, D. Buﬀoni, and P. Gallinari. Ranking with ordered weighted pairwise
In Proceedings of the 26th annual International Conference on

classiﬁcation.
Machine Learning, pages 1057–1064. ACM, 2009.

[Vav09]

S. Vavasis. On the complexity of nonnegative matrix factorization. SIAM Journal
on Optimization, 20(3):1364–1377, 2009.

[VCLR13] V. Vu, J. Cho, J. Lei, and K. Rohe. Fantope projection and selection: A near-
optimal convex relaxation of sparse PCA. In C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information
Processing Systems 26, pages 2670–2678. Curran Associates, Inc., 2013.

[Vid10]

R. Vidal. A tutorial on subspace clustering. IEEE Signal Processing Magazine,
28(2):52–68, 2010.

83

[Vir07]

T. Virtanen. Monaural sound source separation by nonnegative matrix factor-
ization with temporal continuity and sparseness criteria. IEEE Transactions on
Audio, Speech, and Language Processing, 15(3):1066–1074, 2007.

[WBU10]

J. Weston, S. Bengio, and N. Usunier. Large scale image annotation: Learning to
rank with joint word-image embeddings. Machine Learning, 81(1):21–35, 2010.

[WGR+09] J. Wright, A. Ganesh, S. Rao, Y. Peng, and Y. Ma. Robust principal component
analysis: Exact recovery of corrupted low-rank matrices by convex optimization.
In Advances in Neural Information Processing Systems, volume 3, 2009.

[WTH09] D. Witten, R. Tibshirani, and T. Hastie. A penalized matrix decomposition, with
applications to sparse principal components and canonical correlation analysis.
Biostatistics, page kxp008, 2009.

[WYW13] J. Weston, H. Yee, and R. J. Weiss. Learning to rank recommendations with
the k-order statistic loss. In Proceedings of the 7th ACM Conference on Recom-
mender Systems, RecSys ’13, pages 245–248, New York, NY, USA, 2013. ACM.

[XCS12]

H. Xu, C. Caramanis, and S. Sanghavi. Robust PCA via outlier pursuit. IEEE
Transactions on Information Theory, 58(5):3047–3064, 2012.

[YDLT76] F. Young, J. De Leeuw, and Y. Takane. Regression with qualitative and quan-
titative variables: An alternating least squares method with optimal scaling
features. Psychometrika, 41(4):505–529, 1976.

[YYH+13] H. Yun, H.-F. Yu, C.-J. Hsieh, S. V. N. Vishwanathan, and I. Dhillon. NO-
MAD: Non-locking, stOchastic Multi-machine algorithm for Asynchronous and
Decentralized matrix completion. arXiv preprint arXiv:1312.0193, 2013.

[ZCF+10] M. Zaharia, M. Chowdhury, M. Franklin, S. Shenker, and I. Stoica. Spark: Clus-
ter computing with working sets. In Proceedings of the 2nd USENIX conference
on hot topics in cloud computing, page 10, 2010.

[ZHT06]

H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis.
Journal of Computational and Graphical Statistics, 15(2):265–286, 2006.

[ZV86]

W. Zwick and W. Velicer. Comparison of ﬁve rules for determining the number
of components to retain. Psychological bulletin, 99(3):432, 1986.

84

Generalized Low Rank Models

Madeleine Udell, Corinne Horn, Reza Zadeh, and Stephen Boyd

May 6, 2015. (Original version posted September 2014.)

Abstract

Principal components analysis (PCA) is a well-known technique for approximating
a tabular data set by a low rank matrix. Here, we extend the idea of PCA to handle
arbitrary data sets consisting of numerical, Boolean, categorical, ordinal, and other
data types. This framework encompasses many well known techniques in data analysis,
such as nonnegative matrix factorization, matrix completion, sparse and robust PCA,
k-means, k-SVD, and maximum margin matrix factorization. The method handles
heterogeneous data sets, and leads to coherent schemes for compressing, denoising,
and imputing missing entries across all data types simultaneously.
It also admits a
number of interesting interpretations of the low rank factors, which allow clustering of
examples or of features. We propose several parallel algorithms for ﬁtting generalized
low rank models, and describe implementations and numerical results.

This manuscript is a draft. Comments sent to udell@stanford.edu are welcome.

5
1
0
2
 
y
a
M
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
2
4
3
0
.
0
1
4
1
:
v
i
X
r
a

1

Contents

1 Introduction

1.1 Previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 PCA and quadratically regularized PCA

2.1 PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Quadratically regularized PCA . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Missing data and matrix completion . . . . . . . . . . . . . . . . . . . . . . .
Interpretations and applications . . . . . . . . . . . . . . . . . . . . . . . . .
2.5
2.6 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Generalized regularization

3.1 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Generalized loss functions

4.1 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Examples
4.3 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Loss functions for abstract data types

5.1 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Missing data and data imputation . . . . . . . . . . . . . . . . . . . . . . . .
5.4
Interpretations and applications . . . . . . . . . . . . . . . . . . . . . . . . .
5.5 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.6 Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Multi-dimensional loss functions

6.1 Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.2 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.3 Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7 Fitting low rank models

7.1 Alternating minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.2 Early stopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.3 Quadratic objectives
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.4 Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.5
Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.6 Global optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

4
5
7

8
8
9
9
12
13
15

15
16
17
21

22
22
22
25

26
26
27
29
30
32
32

38
39
42
42

42
44
45
48
48
49
52

8 Choosing low rank models

8.1 Regularization paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8.2 Choosing model parameters
. . . . . . . . . . . . . . . . . . . . . . . . . . .
8.3 On-line optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9 Implementations

9.1 Python implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.2 Julia implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.3 Spark implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A Quadratically regularized PCA

A.1 Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.2 Fixed points of alternating minimization . . . . . . . . . . . . . . . . . . . .

56
56
58
61

62
63
64
67

70
70
71

3

1

Introduction

In applications of machine learning and data mining, one frequently encounters large collec-
tions of high dimensional data organized into a table. Each row in the table represents an
example, and each column a feature or attribute. These tables may have columns of diﬀerent
(sometimes, non-numeric) types, and often have many missing entries.

For example, in medicine, the table might record patient attributes or lab tests: each row
of the table lists test or survey results for a particular patient, and each column corresponds
to a distinct test or survey question. The values in the table might be numerical (3.14),
Boolean (yes, no), ordinal (never, sometimes, always), or categorical (A, B, O). Tests not
administered or questions left blank result in missing entries in the data set. Other examples
abound:
in ﬁnance, the table might record known characteristics of companies or asset
classes; in social science settings, it might record survey responses; in marketing, it might
record known customer characteristics and purchase history.

Exploratory data analysis can be diﬃcult in this setting. To better understand a complex
data set, one would like to be able to visualize archetypical examples, to cluster examples,
to ﬁnd correlated features, to ﬁll in (impute) missing entries, and to remove (or simply
identify) spurious, anomalous, or noisy data points. This paper introduces a templated
method to enable these analyses even on large data sets with heterogeneous values and
with many missing entries. Our approach will be to embed both the rows (examples) and
columns (features) of the table into the same low dimensional vector space. These low
dimensional vectors can then be plotted, clustered, and used to impute missing entries or
identify anomalous ones.

If the data set consists only of numerical (real-valued) data, then a simple and well-
known technique to ﬁnd this embedding is Principal Components Analysis (PCA). PCA
ﬁnds a low rank matrix that minimizes the approximation error, in the least-squares sense,
to the original data set. A factorization of this low rank matrix embeds the original high
dimensional features into a low dimensional space. Extensions of PCA can handle missing
data values, and can be used to impute missing entries.

Here, we extend PCA to approximate an arbitrary data set by replacing the least-squares
error used in PCA with a loss function that is appropriate for the given data type. Another
extension beyond PCA is to add regularization on the low dimensional factors to impose or
encourage some structure, such as sparsity or nonnegativity, in the low dimensional factors.
In this paper we use the term generalized low rank model (GLRM) to refer to the problem
of approximating a data set as a product of two low dimensional factors by minimizing
an objective function. The objective will consist of a loss function on the approximation
error together with regularization of the low dimensional factors. With these extensions of
PCA, the resulting low rank representation of the data set still produces a low dimensional
embedding of the data set, as in PCA.

Many of the low rank modeling problems we must solve will be familiar. We recover an
optimization formulation of nonnegative matrix factorization, matrix completion, sparse and
robust PCA, k-means, k-SVD, and maximum margin matrix factorization, to name just a
few.

4

These low rank approximation problems are not convex, and in general cannot be solved
globally and eﬃciently. There are a few exceptional problems that are known to have con-
vex relaxations which are tight under certain conditions, and hence are eﬃciently (globally)
solvable under these conditions. However, all of these approximation problems can be heuris-
tically (locally) solved by methods that alternate between updating the two factors in the low
rank approximation. Each step involves either a convex problem, or a nonconvex problem
that is simple enough that we can solve it exactly. While these alternating methods need
not ﬁnd the globally best low rank approximation, they are often very useful and eﬀective
for the original data analysis problem.

1.1 Previous work

Uniﬁed views of matrix factorization. We are certainly not the ﬁrst to note that
matrix factorization algorithms may be viewed in a uniﬁed framework, parametrized by a
small number of modeling decisions. The ﬁrst instance we ﬁnd in the literature of this
uniﬁed view appeared in a paper by Collins, Dasgupta, and Schapire, [CDS01], extending
PCA to use loss functions derived from any probabilistic model in the exponential family.
Gordon’s Generalized2 Linear2 models [Gor02] extended the framework to loss functions
derived from the generalized Bregman divergence of any convex function, which includes
models such as Independent Components Analysis (ICA). Srebro’s 2004 PhD thesis [Sre04]
extended the framework to other loss functions, including hinge loss and KL-divergence loss,
and to other regularizers, including the nuclear norm and max-norm. Similarly, Chapter 8 in
Tropp’s 2004 PhD thesis [Tro04] explored a number of new regularizers, presenting a range
of clustering problems as matrix factorization problems with constraints, and anticipated
the k-SVD algorithm [AEB06]. Singh and Gordon [SG08] oﬀered a complete view of the
state of the literature on matrix factorization in Table 1 of their 2008 paper, and noted that
by changing the loss function and regularizer, one may recover algorithms including PCA,
weighted PCA, k-means, k-medians, (cid:96)1 SVD, probabilistic latent semantic indexing (pLSI),
nonnegative matrix factorization with (cid:96)2 or KL-divergence loss, exponential family PCA,
and MMMF. Witten et al. introduced the statistics community to sparsity-inducing matrix
factorization in a 2009 paper on penalized matrix decomposition, with applications to sparse
PCA and canonical correlation analysis [WTH09]. Recently, Markovsky’s monograph on low
rank approximation [Mar12] reviewed some of this literature, with a focus on applications
in system, control, and signal processing. The GLRMs discussed in this paper include all of
these models, and many more.

Heterogeneous data. Many authors have proposed the use of low rank models as a
tool for integrating heterogeneous data. The earliest example of this approach is canonical
correlation analysis, developed by Hotelling [Hot36] in 1936 to understand the relations
between two sets of variates in terms of the eigenvectors of their covariance matrix. This
approach was extended by Witten et al. [WTH09] to encourage structured (e.g., sparse)
In the 1970s, De Leeuw et al. proposed the use of low rank models to ﬁt data
factors.

5

measured in nominal, ordinal and cardinal levels [DLYT76]. More recently, Goldberg et
al. [GRX+10] used a low rank model to perform transduction (i.e., multi-label learning)
in the presence of missing data by ﬁtting a low rank model to the features and the labels
simultaneously. Low rank models have also been used to embed image, text and video data
into a common low dimensional space [GD14], and have recently come into vogue in the
natural language processing community as a means to embed words and documents into a
low dimensional vector space [MCCD13, MSC+13, PSM14, SM14].

Algorithms.
In general, it can be computationally hard to ﬁnd the global optimum of a
generalized low rank model. For example, it is NP-hard to compute an exact solution to k-
means [DFK+04], nonnegative matrix factorization [Vav09], and weighted PCA and matrix
completion [GG11], all of which are special cases of low rank models.

However, there are many (eﬃcient) ways to go about ﬁtting a low rank model, by which
we mean ﬁnding a good model with a small objective value. The resulting model may or may
not be the global solution of the low rank optimization problem. We distinguish a model ﬁt
in this way from the solution to an optimization problem, which always refers to the global
solution.

The matrix factorization literature presents a wide variety of methods to ﬁt low rank
models in a variety of special cases. For example, there are variants on alternating min-
imization (with alternating least squares as a special case) [DLYT76, YDLT76, TYDL77,
DL84, DLM09], alternating Newton methods [Gor02, SG08], (stochastic or incremental)
gradient descent [KO09, LRS+10, NRRW11, RRWN11, BRRT12, YYH+13, RR13], conju-
gate gradients [RS05, SJ03], expectation minimization (EM) (or “soft-impute”) methods
[TB99, SJ03, MHT10, HMLZ14], multiplicative updates [LS99], and convex relaxations to
semideﬁnite programs [SRJ04, FHB04, RFP10, FM13].

Generally, expectation minimization, which proceeds by iteratively imputing missing en-
tries in the matrix and solving the fully observed problem, has been found to underperform
relative to other methods [SG08]. However, when used in conjunction with computational
tricks exploiting a particular problem structure, such as Gram matrix caching, these methods
can still work extremely well [HMLZ14].

Semideﬁnite programming becomes computationally intractable for very large (or even
just large) scale problems [RS05]. However, a theoretical analysis of optimality conditions for
rank-constrainted semideﬁnite programs [BM03c] has led to a few algorithms for semideﬁnite
programming based on matrix factorization [BM03b, ABEV09, JBAS10] which guarantee
global optimality and converge quickly if the global solution to the problem is exactly low
rank. Fast approximation algorithms for rank-constrained semideﬁnite programs have also
been developed [SSGS11].

Recently, there has been a resurgence of interest in methods based on alternating min-
imization, as numerous authors have shown that alternating minimization (suitably initial-
ized, and under a few technical assumptions) provably converges to the global minimum
for a range of problems including matrix completion [Kes12, JNS13, Har13], robust PCA
[NNS+14], and dictionary learning [AAJN13].

6

Gradient descent methods are often preferred for extremely large scale problems since
these methods parallelize naturally in both shared memory and distributed memory archi-
tectures. See [RR13, YYH+13] and references therein for some recent innovative approaches
to speeding up stochastic gradient descent for matrix factorization by eliminating locking
and reducing interprocess communication.

Contributions. The present paper diﬀers from previous work in a number of ways. We
are consistently concerned with the meaning of applying these diﬀerent loss functions and
regularizers to approximate a data set. The generality of our view allows us to introduce a
number of loss functions and regularizers that have not previously been considered. More-
over, our perspective enables us to extend these ideas to arbitrary data sets, rather than just
matrices of real numbers.

A number of new considerations emerge when considering the problem so broadly. First,
we must face the problem of comparing approximation errors across data of diﬀerent types.
For example, we must choose a scaling to trade oﬀ the loss due to a misclassiﬁcation of a
categorical value with an error of 0.1 (say) in predicting a real value.

Second, we require algorithms that can handle the full gamut of losses and regulariz-
ers, which may be smooth or nonsmooth, ﬁnite or inﬁnite valued, with arbitrary domain.
This work is the ﬁrst to consider these problems in such generality, and therefore also the
ﬁrst to wrestle with the algorithmic consequences. Below, we give a number of algorithms
appropriate for this setting, including many that have not been previously proposed in the
literature. Our algorithms are all based on alternating minimization and variations on al-
ternating minimization that are more suitable for large scale data and can take advantage
of parallel computing resources.

Finally, we present some new results on some old problems. For example, in Appendix A,
we derive a formula for the solution to quadratically regularized PCA, and show that quadrat-
ically regularized PCA has no local nonglobal minima; and in §7.6 we show how to certify
(in some special cases) that a model is a global solution of a GLRM.

1.2 Organization

The organization of this paper is as follows. In §2 we ﬁrst recall some properties of PCA
and its common variations to familiarize the reader with our notation. We then generalize
the regularization on the low dimensional factors in §3, and the loss function on the ap-
proximation error in §4. Returning to the setting of heterogeneous data, we extend these
dimensionality reduction techniques to abstract data types in §5 and to multi-dimensional
loss functions in §6. Finally, we address algorithms for ﬁtting GLRMs in §7, discuss a few
practical considerations in choosing a GLRM for a particular problem in §8, and describe
some implementations of the algorithms that we have developed in §9.

7

2 PCA and quadratically regularized PCA

In this section, we let A ∈ Rm×n be a data matrix consisting of m examples
Data matrix.
each with n numerical features. Thus Aij ∈ R is the value of the jth feature in the ith
example, the ith row of A is the vector of n feature values for the ith example, and the jth
column of A is the vector of the jth feature across our set of m examples.

It is common to represent other data types in a numerical matrix using certain canonical
encoding tricks. For example, Boolean data is often encoded as 1 (for true) and -1 (for
false), ordinal data is often encoded using consecutive integers to represent the consecutive
levels of the variable, and categorical data is often encoded by creating a column for each
possible value of the categorical variable, and representing the data using a 1 in the column
corresponding to the observed value, and -1 or 0 in all other columns. We will see more
systematic and principled ways to deal with these data types, and others, in §4–6. For now,
we assume the entries in the data matrix consist of real numbers.

2.1 PCA

solving

Principal components analysis (PCA) is one of the oldest and most widely used tools in data
analysis [Pea01, Hot33, Jol86]. We review some of its well-known properties here in order to
set notation and as a warm-up to the variants presented later.

PCA seeks the best rank-k approximation to the matrix A in the least-squares sense, by

minimize
subject to Rank(Z) ≤ k,
with variable Z ∈ Rm×n. Here, (cid:107) · (cid:107)F is the Frobenius norm of a matrix, i.e., the square root
of the sum of the squares of the entries.

(cid:107)A − Z(cid:107)2
F

(1)

The rank constraint can be encoded implicitly by expressing Z in factored form as Z =

XY , with X ∈ Rm×k, Y ∈ Rk×n. Then the PCA problem can be expressed as

minimize (cid:107)A − XY (cid:107)2
F

(2)

with variables X ∈ Rm×k and Y ∈ Rk×n. (The factorization of Z is of course not unique.)
Deﬁne xi ∈ R1×n to be the ith row of X, and yj ∈ Rm to be the jth column of Y . Thus
xiyj = (XY )ij ∈ R denotes a dot or inner product. (We will use this notation throughout
the paper.) Using this deﬁnition, we can rewrite the objective in problem (2) as
n
(cid:88)

m
(cid:88)

(Aij − xiyj)2.

i=1

j=1

We will give several interpretations of the low rank factorization (X, Y ) solving (2) in
§2.5. But for now, we note that (2) can interpreted as a method for compressing the n
features in the original data set to k < n new features. The row vector xi is associated with
example i; we can think of it as a feature vector for the example using the compressed set
of k < n features. The column vector yj is associated with the original feature j; it can be
interpreted as mapping the k new features onto the original feature j.

8

2.2 Quadratically regularized PCA

We can add quadratic regularization on X and Y to the objective. The quadratically regu-
larized PCA problem is

minimize (cid:80)m

(cid:80)n

j=1(Aij − xiyj)2 + γ (cid:80)m

i=1 (cid:107)xi(cid:107)2

2 + γ (cid:80)n

j=1 (cid:107)yj(cid:107)2
2,

i=1

with variables X ∈ Rm×k and Y ∈ Rk×n, and regularization parameter γ ≥ 0. Problem (3)
can be written more concisely in matrix form as

minimize (cid:107)A − XY (cid:107)2

F + γ(cid:107)X(cid:107)2

F + γ(cid:107)Y (cid:107)2
F .

When γ = 0, the problem reduces to the PCA problem (2).

2.3 Solution methods

Singular value decomposition.
It is well known that a solution to (2) can be obtained
by truncating the singular value decomposition (SVD) of A [EY36]. The (compact) SVD
of A is given by A = U ΣV T , where U ∈ Rm×r and V ∈ Rn×r have orthonormal columns,
and Σ = diag(σ1, . . . , σr) ∈ Rr×r, with σ1 ≥ · · · ≥ σr > 0 and r = Rank(A). The columns
of U = [u1 · · · ur] and V = [v1 · · · vr] are called the left and right singular vectors of A,
respectively, and σ1, . . . , σr are called the singular values of A.

Using the orthogonal invariance of the Frobenius norm, we can rewrite the objective in

problem (1) as

(cid:107)A − XY (cid:107)2

F = (cid:107)Σ − U T XY V (cid:107)2
F .

That is, we would like to ﬁnd a matrix U T XY V of rank no more than k approximating the
diagonal matrix Σ. It is easy to see that there is no better rank k approximation for Σ than
Σk = diag(σ1, . . . , σk, 0, . . . , 0) ∈ Rr×r. Here we have truncated the SVD to keep only the
top k singular values. We can achieve this approximation by choosing U T XY V = Σk, or
(using the orthogonality of U and V ) XY = U ΣkV T . For example, deﬁne

and let

Uk = [u1 · · · uk],

Vk = [v1 · · · vk],

X = UkΣ1/2
k ,

Y = Σ1/2

k V T
k .

The solution to (3) is clearly not unique: if X, Y is a solution, then so is XG, G−1Y for any
invertible matrix G ∈ Rk×k. When σk > σk+1, all solutions to the PCA problem have this
form. In particular, letting G = tI and taking t → ∞, we see that the solution set of the
PCA problem is unbounded.

It is less well known that a solution to the quadratically regularized PCA problem can
be obtained in the same way. (Proofs for the statements below can be found in Appendix
A.) Deﬁne Uk and Vk as above, and let ˜Σk = diag((σ1 − γ)+, . . . , (σk − γ)+), where (a)+ =
max(a, 0). Here we have both truncated the SVD to keep only the top k singular values, and

(3)

(4)

(5)

(6)

9

performed soft-thresholding on the singular values to reduce their values by γ. A solution to
the quadratically regularized PCA problem (3) is then given by

X = Uk

˜Σ1/2
k ,

Y = ˜Σ1/2

k V T
k .

(7)

For γ = 0, the solution reduces to the familiar solution to PCA (2) obtained by truncating
the SVD to the top k singular values.

The set of solutions to problem (3) is signiﬁcantly smaller than that of problem (2),
although solutions are still not unique: if X, Y is a solution, then so is XT , T −1Y for any
orthogonal matrix T ∈ Rk×k. When σk > σk+1, all solutions to (3) have this form.
In
particular, adding quadratic regularization results in a solution set that is bounded.

The quadratically regularized PCA problem (3) (including the PCA problem as a special
case) is the only problem we will encounter for which an analytical solution exists. The
analytical tractability of PCA explains its popularity as a technique for data analysis in
the era before computers were machines. For example, in his 1933 paper on PCA [Hot33],
Hotelling computes the solution to his problem using power iteration to ﬁnd the eigenvalue
decomposition of the matrix AT A = V Σ2V T , and records in the appendix to his paper the
itermediate results at each of the (three) iterations required for the method to converge.

Alternating minimization. Here we mention a second method for solving (3), which
extends more readily to the extensions of PCA that we discuss below. The alternating
minimization algorithm simply alternates between minimizing the objective over the variable
X, holding Y ﬁxed, and then minimizing over Y , holding X ﬁxed. With an initial guess for
the factors Y 0, we repeat the iteration
(cid:32) m
(cid:88)

n
(cid:88)

m
(cid:88)

(cid:33)

X l = argmin

(Aij − xiyl−1

)2 + γ

j

(cid:107)xi(cid:107)2
2

Y l = argmin

(Aij − xl

iyj)ij)2 + γ

(cid:107)yj(cid:107)2
2

X

Y

i=1
(cid:32) m
(cid:88)

j=1

n
(cid:88)

i=1

j=1

(cid:33)

i=1

n
(cid:88)

j=1

for l = 1, . . . until a stopping condition is satisﬁed. (If X and Y are full rank, or γ > 0, the
minimizers above are unique; when they are not, we can take any minimizer.) The objective
function is nonincreasing at each iteration, and therefore bounded. This implies, for γ > 0,
that the iterates X l and Y l are bounded.

This algorithm does not always work. In particular, it has stationary points that are not
solutions of problem (3). In particular, if the rows of Y l lie in a subspace spanned by a subset
of the (right) singular vectors of A, then the columns of X l+1 will lie in a subspace spanned
by the corresponding left singular vectors of A, and vice versa. Thus, if the algorithm is
initialized with Y 0 orthogonal to any of the top k (right) singular vectors, then the algorithm
(implemented in exact arithmetic) will not converge to the global solution to the problem.
But all stable stationary points of the iteration are solutions (see Appendix A). So as
a practical matter, the alternating minimization method always works, i.e., the objective
converges to the optimal value.

10

Parallelizing alternating minimization. Alternating minimization parallelizes easily
over examples and features. The problem of minimizing over X splits into m independent
minimization problems. We can solve the simple quadratic problems

minimize (cid:80)n

j=1(Aij − xiyj)2 + γ(cid:107)xi(cid:107)2
2

with variable xi, in parallel, for i = 1, . . . , m. Similarly, the problem of minimizing over Y
splits into n independent quadratic problems,

minimize (cid:80)m

i=1(Aij − xiyj)2 + γ(cid:107)yj(cid:107)2
2

with variable yj, which can be solved in parallel for j = 1, . . . , n.

(8)

(9)

Caching factorizations. We can speed up the solution of the quadratic problems using
a simple factorization caching technique.

For ease of exposition, we assume here that X and Y have full rank k. The updates (8)

and (9) can be expressed as

X = AY T (Y Y T + γI)−1,

Y = (X T X + γI)−1X T A.

We show below how to eﬃciently compute X = AY T (Y Y T + γI)−1; the Y update admits a
similar speedup using the same ideas. We assume here that k is modest, say, not more than
a few hundred or a few thousand. (Typical values used in applications are often far smaller,
on the order of tens.) The dimensions m and n, however, can be very large.

First compute the Gram matrix G = Y Y T using an outer product expansion

This sum can be computed on-line by streaming over the index j, or in parallel, split over
the index j. This property allows us to scale up to extremely large problems even if we
cannot store the entire matrix Y in memory. The computation of the Gram matrix requires
2k2n ﬂoating point operations (ﬂops), but is trivially parallelizable: with r workers, we can
expect a speedup on the order of r. We next add the diagonal matrix γI to G in k ﬂops,
and form the Cholesky factorization of G + γI in k3/3 ﬂops and cache the factorization.

In parallel over the rows of A, we compute D = AY T (2kn ﬂops per row), and use the
factorization of G + γI to compute D(G + γI)−1 with two triangular solves (2k2 ﬂops per
row). These computations are also trivially parallelizable: with r workers, we can expect a
speedup on the order of r.

Hence the total time required for each update with r workers scales as O( k2(m+n)+kmn

).

For k small compared to m and n, the time is dominated by the computation of AY T .

r

G =

yjyT
j .

n
(cid:88)

j=1

11

2.4 Missing data and matrix completion

Suppose we observe only entries Aij for (i, j) ∈ Ω ⊂ {1, . . . , m} × {1, . . . , n} from the matrix
A, so the other entries are unknown. Then to ﬁnd a low rank matrix that ﬁts the data well,
we solve the problem

minimize (cid:80)

(i,j)∈Ω(Aij − xiyj)2 + γ(cid:107)X(cid:107)2

F + γ(cid:107)Y (cid:107)2
F ,

(10)

with variables X and Y , with γ > 0. A solution of this problem gives an estimate ˆAij = xiyj
for the value of those entries (i, j) (cid:54)∈ Ω that were not observed. In some applications, this
data imputation (i.e., guessing entries of a matrix that are not known) is the main point.
There are two very diﬀerent regimes in which solving the problem (10) may be useful.

Imputing missing entries to borrow strength. Consider a matrix A in which very few
entries are missing. The typical approach in data analysis is to simply remove any rows with
missing entries from the matrix and exclude them from subsequent analysis. If instead we
solve the problem above without removing these aﬀected rows, we “borrow strength” from
the entries that are not missing to improve our global understanding of the data matrix
A. In this regime we are imputing the (few) missing entries of A, using the examples that
ordinarily we would discard.

Low rank matrix completion. Now consider a matrix A in which most entries are
missing, i.e., we only observe relatively few of the mn elements of A, so that by discarding
every example with a missing feature or every feature with a missing example, we would
discard the entire matrix. Then the solution to (10) becomes even more interesting: we are
guessing all the entries of a (presumed low rank) matrix, given just a few of them. It is a
surprising fact that this is possible: typical results from the matrix completion literature show
that one can recover an unknown m×n matrix A of low rank r from just about nr log2 n noisy
samples Ω with an error that is proportional to the noise level [CR08, CT10, RFP10, CP09],
so long as the matrix A satisﬁes a certain incoherence condition and the samples Ω are chosen
uniformly at random. These works use an estimator that minimizes a nuclear norm penalty
along with a data ﬁtting term to encourage low rank structure in the solution.

The argument in §7.6 shows that problem (10) is equivalent to the rank-constrained

nuclear-norm regularized convex problem

minimize (cid:80)
subject to Rank(Z) ≤ k,

(i,j)∈Ω(Aij − Zij)2 + 2γ(cid:107)Z(cid:107)∗

where the nuclear norm (cid:107)Z(cid:107)∗ (also known as the trace norm) is deﬁned to be the sum of the
singular values of Z. Thus, the solutions to problem (10) correspond exactly to the solutions
of these proposed estimators so long as the rank k of the model is chosen to be larger than
the true rank r of the matrix A. Nuclear norm regularization is often used to encourage
solutions of rank less than k, and has applications ranging from graph embedding to linear
system identiﬁcation [FHB04, LV09, MF10, Smi12, Osn14].

12

Low rank matrix completion problems arise in applications like predicting customer rat-
ings or customer (potential) purchases. Here the matrix consists of the ratings or numbers
of purchases that m customers give (or make) for each of n products. The vast majority of
the entries in this matrix are missing, since a customer will rate (or purchase) only a small
fraction of the total number of products available. In this application, imputing a missing
entry of the matrix as xiyj, for (i, j) (cid:54)∈ Ω, is guessing what rating a customer would give a
product, if she were to rate it. This can used as the basis for a recommendation system, or
a marketing plan.

Alternating minimization. When Ω (cid:54)= {1, . . . , m} × {1, . . . , n}, the problem (10) has no
known analytical solution, but it is still easy to ﬁt a model using alternating minimization.
Algorithms based on alternating minimization have been shown to converge quickly (even
geometrically [JNS13]) to a global solution satisfying a recovery guarantee when the initial
values of X and Y are chosen carefully [KMO09, KMO10, KM10, JNS13, Har13, GAGG13].
On the other hand, all of these analytical results rely on using a fresh batch of samples
Ω for each iteration of alternating minimization; none uses the quadratic regularizer above
that corresponds to the nuclear norm penalized estimator; and interestingly, Hardt [Har13]
notes that none achieves the same sample complexity guarantees found in the convex ma-
trix completion literature which, unlike the alternating minimization guarantees, match the
information theoretic lower bound [CT10] up to logarithmic factors. For these reasons, it
is plausible to expect that in practice using alternating minimization to solve problem (10)
might yield a better solution than the “alternating minimization” algorithms presented in
the literature on matrix completion when suitably initialized (for example, using the method
proposed below in §7.5). However, in general the method should be considered a heuristic.

2.5 Interpretations and applications

The recovered matrices X and Y in the quadratically regularized PCA problems (3) and
(10) admit a number of interesting interpretations. We introduce some of these interpreta-
tions now; the terminology we use here will recur throughout the paper. Of course these
interpretations are related to each other, and not distinct.

Feature compression. Quadratically regularized PCA (3) can be interpreted as a method
for compressing the n features in the original data set to k < n new features. The row vector
xi is associated with example i; we can think of it as a feature vector for the example using
the compressed set of k < n features. The column vector yj is associated with the original
feature j; it can be interpreted as the mapping from the original feature j into the k new
features.

Low-dimensional geometric embedding. We can think of each yj as associating feature
j with a point in a low (k-) dimensional space. Similarly, each xi associates example i with
a point in the low dimensional space. We can use these low dimensional vectors to judge

13

which features (or examples) are similar. For example, we can run a clustering algorithm on
the low dimensional vectors yj (or xi) to ﬁnd groups of similar features (or examples).

Archetypes. We can think of each row of Y as an archetype which captures the behavior
of one of k idealized and maximally informative examples. These archetypes might also
be called proﬁles, factors, or atoms. Every example i = 1, . . . , m is then represented (ap-
proximately) as a linear combination of these archetypes, with the row vector xi giving the
coeﬃcients. The coeﬃcient xil gives the resemblance or loading of example i to the lth
archetype.

Archetypical representations. We call xi the representation of example i in terms of the
archetypes. The rows of X give an embedding of the examples into Rk, where each coordinate
axis corresponds to a diﬀerent archetype.
If the archetypes are simple to understand or
interpret, then the representation of an example can provide better intuition about that
example.

The examples can be clustered according to their representations in order to determine
a group of similar examples.
Indeed, one might choose to apply any machine learning
algorithm to the representations xi rather than to the initial data matrix: in contrast to the
initial data, which may consist of high dimensional vectors with noisy or missing entries, the
representations xi will be low dimensional, less noisy, and complete.

Feature representations. The columns of Y embed the features into Rk. Here, we
think of the columns of X as archetypical features, and represent each feature j as a linear
combination of the archetypical features. Just as with the examples, we might choose to
apply any machine learning algorithm to the feature representations. For example, we might
ﬁnd clusters of similar features that represent redundant measurements.

Latent variables. Each row of X represents an example by a vector in Rk. The matrix
Y maps these representations back into Rm. We might think of X as discovering the latent
variables that best explain the observed data. If the approximation error (cid:80)
(i,j)∈Ω(Aij −xiyj)2
is small, then we view these latent variables as providing a good explanation or summary of
the full data set.

Probabilistic intepretation. We can give a probabilistic interpretation of X and Y ,
building on the probabilistic model of PCA developed by Tipping and Bishop [TB99]. We
suppose that the matrices ¯X and ¯Y have entries which are generated by taking independent
samples from a normal distribution with mean 0 and variance γ−1 for γ > 0. The entries in
the matrix ¯X ¯Y are observed with noise ηij ∈ R,

where the noise η in the (i, j)th entry is sampled independently from a standard normal
distribution. We observe each entry (i, j) ∈ Ω. Then to ﬁnd the maximum a posteriori

Aij = ( ¯X ¯Y )ij + ηij,

14

(MAP) estimator (X, Y ) of ( ¯X, ¯Y ), we solve
(cid:1) exp (cid:0)− γ

maximize exp (cid:0)− γ

2 (cid:107) ¯X(cid:107)2

F

which is equivalent, by taking logs, to (3).

2 (cid:107) ¯Y (cid:107)2

F

(cid:1) (cid:81)

(i,j)∈Ω exp (−(Aij − xiyj)2) ,

This interpretation explains the recommendation we gave above for imputing missing
observations (i, j) (cid:54)∈ Ω. We simply use the MAP estimator xiyj to estimate the missing
entry ( ¯X ¯Y )ij. Similarly, we can interpret (XY )ij for (i, j) ∈ Ω as a denoised version of the
observation Aij.

Auto-encoder. The matrix X encodes the data; the matrix Y decodes it back into the
full space. We can view PCA as providing the best linear auto-encoder for the data; among
all (bi-linear) low rank encodings (X) and decodings (Y ) of the data, PCA minimizes the
squared reconstruction error.

Compression. We impose an information bottleneck [TPB00] on the data by using a low
rank auto-encoder to ﬁt the data. PCA ﬁnds X and Y to maximize the information trans-
mitted through this k-dimensional information bottleneck. We can interpret the solution
as a compressed representation of the data, and use it to eﬃciently store or transmit the
information present in the original data.

2.6 Oﬀsets and scaling

For good practical performance of a generalized low rank model, it is critical to ensure that
model assumptions match the data. We saw above in §2.5 that quadratically regularized
PCA corresponds to a model in which features are observed with N (0, 1) errors. If instead
each column j of XY is observed with N (µj, σ2
j ) errors, our model is no longer unbiased,
and may ﬁt very poorly, particularly if some of the column means µj are large.

For this reason it is standard practice to standardize the data before appplying PCA or
quadratically regularized PCA: the column means are subtracted from each column, and the
columns are normalized by their variances. (This can be done approximately; there is no
need to get the scaling and oﬀset exactly right.) Formally, deﬁne nj = |{i : (i, j) ∈ Ω}|, and
let

µj =

1
nj

(cid:88)

Aij,

(i,j)∈Ω

σ2
j =

1
nj − 1

(cid:88)

(i,j)∈Ω

(Aij − µj)2

estimate the mean and variance of each column of the data matrix. PCA or quadratically
regularized PCA is then applied to the matrix whose (i, j) entry is (Aij − µj)/σj.

3 Generalized regularization

It is easy to see how to extend PCA to allow arbitrary regularization on the rows of X and
columns of Y . We form the regularized PCA problem
(i,j)∈Ω(Aij − xiyj)2 + (cid:80)m

i=1 ri(xi) + (cid:80)n

minimize (cid:80)

j=1 ˜rj(yj),

(11)

15

with variables xi and yj, with given regularizers ri : Rk → R ∪ {∞} and ˜rj : Rk → R ∪ {∞}
for i = 1, . . . , n and j = 1, . . . , m. Regularized PCA (11) reduces to quadratically regularized
PCA (3) when ri = γ(cid:107) · (cid:107)2

2. We do not restrict the regularizers to be convex.

2, ˜rj = γ(cid:107) · (cid:107)2

The objective in problem (11) can be expressed compactly in matrix notation as

(cid:107)A − XY (cid:107)2
i=1 r(xi) and ˜r(Y ) = (cid:80)n

F + r(X) + ˜r(Y ),

where r(X) = (cid:80)n
separable across the rows of X, and the columns of Y , respectively.

j=1 ˜r(yj). The regularization functions r and ˜r are

Inﬁnite values of ri and ˜rj are used to enforce constraints on the values of X and Y . For

example, the regularizer

ri(x) =

(cid:26) 0

x ≥ 0
∞ otherwise,

the indicator function of the nonnegative orthant, imposes the constraint that xi be nonneg-
ative.

Solutions to (11) need not be unique, depending on the choice of regularizers. If X and
Y are a solution, then so are XT and T −1Y , where T is any nonsingular matrix that satisﬁes
r(U T ) = r(U ) for all U and ˜r(T −1V ) = r(V ) for all V .

By varying our choice of regularizers r and ˜r, we are able to represent a wide range of
known models, as well as many new ones. We will discuss a number of choices for regularizers
below, but turn now to methods for solving the regularized PCA problem (11).

3.1 Solution methods

In general, there is no analytical solution for (11). The problem is not convex, even when r
and ˜r are convex. However, when r and ˜r are convex, the problem is bi-convex: it is convex
in X when Y is ﬁxed, and convex in Y when X is ﬁxed.

Alternating minimization. There is no reason to believe that alternating minimization
will always converge to the global minimum of the regularized PCA problem (11). Indeed,
we will see many cases below in which the problem is known to have many local minima.
However, alternating minimization can still be applied in this setting, and it still parallelizes
over the rows of X and columns of Y . To minimize over X, we solve, in parallel,

with variable xi, for i = 1, . . . , m. Similarly, to minimize over Y , we solve, in parallel,

minimize (cid:80)

j:(i,j)∈Ω(Aij − xiyj)2 + ri(xi)

minimize (cid:80)

i:(i,j)∈Ω(Aij − xiyj)2 + ˜rj(yj)

(12)

(13)

with variable yj, for j = 1, . . . , n.

When the regularizers are convex, these problems are convex. When the regularizers
are not convex, there are still many cases in which we can ﬁnd analytical solutions to the
nonconvex subproblems (12) and (13), as we will see below. A number of concrete algorithms,
in which these subproblems are solved explicitly, are given in §7.

16

Caching factorizations. Often, the X and Y updates (12) and (13) reduce to convex
quadratic programs. For example, this is the case for nonnegative matrix factorization,
sparse PCA, and quadratic mixtures (which we deﬁne and discuss below in §3.2). The same
factorization caching of the Gram matrix that was described above in the case of PCA can
be used here to speed up the solution of these updates. Variations on this idea are described
in detail in §7.3.

3.2 Examples

Here and throughout the paper, we present a set of examples chosen for pedagogical clarity,
In all of the examples below, γ > 0 is a parameter that controls
not for completeness.
the strength of the regularization, and we drop the subscripts from r (or ˜r) to lighten the
notation. Of course, it is possible to mix and match these regularizers, i.e., to choose diﬀerent
ri for diﬀerent i, and choose diﬀerent ˜rj for diﬀerent j.

Nonnegative matrix factorization (NNMF). Consider the regularized PCA problem
(11) with r = I+ and ˜r = I+, where I+ is the indicator function of the nonnegative reals.
(Here, and throughout the paper, we deﬁne the indicator function of a set C, to be 0 when
its argument is in C and ∞ otherwise.) Then problem (11) is NNMF: a solution gives the
matrix best approximating A that has a nonnegative factorization (i.e., a factorization into
elementwise nonnegative matrices) [LS99]. It is NP-hard to solve NNMF problems exactly
[Vav09]. However, these problems have a rich analytical structure which can sometimes
be exploited [Gil11, BRRT12, DS14], and a wide range of uses in practice [LS99, SBPP06,
BBL+07, Vir07, KP07, FBD09]. Hence a number of specialized algorithms and codes for
ﬁtting NNMF models are available [LS01, Lin07, KP08a, KP08b, BDKP14, KHP14, KP11].
We can also replace the nonnegativity constraint with any interval constraint. For ex-
ample, r and ˜r can be 0 if all entries of X and Y , respectively, are between 0 and 1, and
inﬁnite otherwise.

Sparse PCA.
If very few of the coeﬃcients of X and Y are nonzero, it can be easier to
interpret the archetypes and representations. We can understand each archetype using only
a small number of features, and can understand each example as a combination of only a
small number of archetypes. To get a sparse version of PCA, we use a sparsifying penalty
as the regularization. Many variants on this basic idea have been proposed, together with a
wide variety of algorithms [dEGJL04, ZHT06, SH08, Mac09, WTH09, RTA12, VCLR13].

For example, we could enforce that no entry Aij depend on more than s columns of X

or of Y by setting r to be the indicator function of a s-sparse vector, i.e.,

and deﬁning ˜r(y) similarly, where card(x) denotes the cardinality (number of nonzero en-
tries) in the vector x. The updates (12) and (13) are not convex using this regularizer, but

r(x) =

(cid:26) 0

card(x) ≤ s

∞ otherwise,

17

one can ﬁnd approximate solutions using a pursuit algorithm (see, e.g., [CDS98, TG07]), or
exact solutions (for small s) using the branch and bound method [LW66, BM03a].

As a simple example, consider s = 1. Here we insist that each xi have at most one
nonzero entry, which means that each example is a multiple of one of the rows of Y . The
X-update is easy to carry out, by evaluating the best quadratic ﬁt of xi with each of the k
rows of Y . This reduces to choosing the row of Y that has the smallest angle to the ith row
of A.

The s-sparse regularization can be relaxed to a convex, but still sparsifying, regularization
using r(x) = (cid:107)x(cid:107)1, ˜r(y) = (cid:107)y(cid:107)1 [ZHT06]. In this case, the X-update reduces to solving a
(small) (cid:96)1-regularized least-squares problem.

Orthogonal nonnegative matrix factorization. One well known property of PCA is
that the principal components obtained (i.e., the columns of X and rows of Y ) can be chosen
to be orthogonal, so X T X and Y Y T are both diagonal. We can impose the same condition
on a nonnegative matrix factorization. Due to nonnegativity of the matrix, two columns
of X cannot be orthogonal if they both have a nonzero in the same row. Conversely, if X
has only one nonzero per row, then its columns are mutually orthogonal. So an orthogonal
nonnegative matrix factorization is identical to to a nonnegativity condition in addition to
the 1-sparse condition described above. Orthogonal nonnegative matrix factorization can be
achieved by using the regularizer

(cid:26) 0

r(x) =

card(x) = 1,

x ≥ 0

∞ otherwise,

and letting ˜r(y) be the indicator of the nonnegative orthant, as in NNMF.

Geometrically, we can interpret this problem as modeling the data A as a union of rays.
Each row of Y , interpreted as a point in Rn, deﬁnes a ray from the origin passing through
that point. Orthogonal nonnegative matrix factorization models each row of X as a point
along one of these rays.

Some authors [DLPP06] have also considered how to obtain a bi-orthogonal nonnegative
matrix factorization, in which both X and Y T have orthogonal columns. By the same
argument as above, we see this is equivalent to requiring both X and Y T to have only one
positive entry per row, with the other entries equal to 0.

Max-norm matrix factorization. We take r = ˜r = φ with

This penalty enforces that

φ(x) =

(cid:26) 0

(cid:107)x(cid:107)2
2 ≤ µ
∞ otherwise.

(cid:107)X(cid:107)2

2,∞ ≤ µ,

(cid:107)Y T (cid:107)2

2,∞ ≤ µ,

18

where the (2, ∞) norm of a matrix X with rows xi is deﬁned as maxi (cid:107)xi(cid:107)2. This is equivalent
to requiring the max-norm (sometimes called the γ2-norm) of Z = XY , which is deﬁned as

(cid:107)Z(cid:107)max = inf{(cid:107)X(cid:107)2,∞(cid:107)Y T (cid:107)2,∞ : XY = Z},

to be bounded by µ. This penalty has been proposed by [LRS+10] as a heuristic for low rank
matrix completion, which can perform better than Frobenius norm regularization when the
low rank factors are known to have bounded entries.

Quadratic clustering. Consider (11) with ˜r = 0. Let r be the indicator function of a
selection, i.e.,

(cid:26) 0

r(x) =

∞ otherwise,

x = el for some l ∈ {1, . . . , k}

where el is the l-th standard basis vector. Thus xi encodes the cluster (one of k) to which
the data vector (Ai1, . . . , Aim) is assigned.

Alternating minimization on this problem reproduces the well-known k-means algorithm
(also known as Lloyd’s algorithm) [Llo82]. The y update (13) is a least squares problem with
the simple solution

Ylj =

(cid:80)

i:(i,j)∈Ω AijXil
(cid:80)
i:(i,j)∈Ω Xil

,

i.e., each row of Y is updated to be the mean of the rows of A assigned to that archetype.
The x update (12) is not a convex problem, but is easily solved. The solution is given
by assigning xi to the closest archetype (often called a cluster centroid in the context of
j=1(Aij − Ylj)2(cid:17)
k-means): xi = el(cid:63) for l(cid:63) = argminl

(cid:16)(cid:80)n

.

Quadratic mixtures. We can also implement partial assignment of data vectors to clus-
ters. Take ˜r = 0, and let r be the indicator function of the set of probability vectors,
i.e.,

r(x) =

(cid:26) 0 (cid:80)k

l=1 xl = 1,

xl ≥ 0

∞ otherwise.

Subspace clustering. PCA approximates a data set by a single low dimensional subspace.
We may also be interested in approximating a data set as a union of low dimensional sub-
spaces. This problem is known as subspace clustering (see [Vid10] and references therein).
Subspace clustering may also be thought of as generalizing quadratic clustering to assign
each data vector to a low dimensional subspace rather than to a single cluster centroid.

To frame subspace clustering as a regularized PCA problem (11), partition the columns
of X into k blocks. Then let r be the indicator function of block sparsity (i.e., r(x) = 0 if
only one block of x has nonzero entries, and otherwise r(x) = ∞).

It is easy to perform alternating minimization on this objective function. This method
is sometimes called the k-planes algorithm [Vid10, Tse00, AM04], which alternates over

19

assigning examples to subspaces, and ﬁtting the subspaces to the examples. Once again, the
X update (12) is not a convex problem, but can be easily solved. Each block of the columns
of X deﬁnes a subspace spanned by the corresponding rows of Y . We compute the distance
from example i (the ith row of A) to each subspace (by solving a least squares problem),
and assign example i to the subspace that minimizes the least squares error by setting xi to
be the solution to the corresponding least squares problem.

Many other algorithms for this problem have also been proposed, such as the k-SVD
[Tro04, AEB06] and sparse subspace clustering [EV09], some with provable guarantees on
the quality of the recovered solution [SC12].

Supervised learning. Sometimes we want to understand the variation that a certain set
of features can explain, and the variance that remains unexplainable. To this end, one
natural strategy would be to regress the labels in the dataset on the features; to subtract
the predicted values from the data; and to use PCA to understand the remaining variance.
This procedure gives the same answer as the solution to a single regularized PCA problem.
Here we present the case in which the features we wish to use in the regression are present
in the data as the ﬁrst column of A. To construct the regularizers, we make sure the ﬁrst
column of A appears as a feature in the supervised learning problem by setting

ri(x) =

(cid:26) r0(x2, . . . , xk+1) x1 = Ai1
otherwise,

∞

where r0 = 0 can be chosen as in any regularized PCA model. The regularization on the
ﬁrst row of Y is the regularization used in the supervised regression, and the regularization
on the other rows will be that used in regularized PCA.

Thus we see that regularized PCA can naturally combine supervised and unsupervised

learning into a single problem.

Feature selection. We can use regularized PCA to perform feature selection. Consider
(11) with r(x) = (cid:107)x(cid:107)2
2 and ˜r(y) = (cid:107)y(cid:107)2. (Notice that we are not using (cid:107)y(cid:107)2
2.) The regularizer
˜r encourages the matrix ˜Y to be column-sparse, so many columns are all zero. If ˜yj = 0,
it means that feature j was uninformative, in the sense that its values do not help much in
predicting any feature in the matrix A (including feature j itself). In this case we say that
feature j was not selected. For this approach to make sense, it is important that the columns
of the matrix A should have mean zero. Alternatively, one can use the de-biasing regularizers
r(cid:48) and ˜r(cid:48) introduced in §3.3 along with the feature selection regularizer introduced here.

Dictionary learning. Dictionary learning (also sometimes called sparse coding) has be-
come a popular method to design concise representations for very high dimensional data
[OF97, LBRN06, MBPS09, MPS+09]. These representations have been shown to perform
well when used as features in subsequent (supervised) machine learning tasks [RBL+07].
In dictionary learning, each row of A is modeled as a linear combination of dictionary
atoms, represented by rows of Y . The total size of the dictionary used is often very large

20

(k (cid:29) max(m, n)), but each example is represented using a very small number of atoms. To
ﬁt the model, one solves the regularized PCA problem (11) with r(x) = (cid:107)x(cid:107)1, to induce spar-
sity in the number of atoms used to represent any given example, and with ˜r(y) = (cid:107)y(cid:107)2
2 or
˜r(y) = I+(c − (cid:107)y(cid:107)2) for some c > 0 ∈ R, in order to ensure the problem is well posed. (Note
that our notation transposes the usual notation in the literature on dictionary learning.)

Mix and match.
It is possible to combine these regularizers to obtain a factorization with
any combination of the above properties. As an example, one may require that both X and
Y be simultaneously sparse and nonnegative by choosing

r(x) = (cid:107)x(cid:107)1 + I+(x) = 1T x + I+(x),

and similarly for ˜r(y). Similarly, [KP07] show how to obtain a nonnegative matrix factor-
ization in which one factor is sparse by using r(x) = (cid:107)x(cid:107)2
2 + I+(y);
they go on to use this factorization as a clustering technique.

1 + I+(x) and ˜r(y) = (cid:107)y(cid:107)2

3.3 Oﬀsets and scaling

In our discussion of the quadratically regularized PCA problem (3), we saw that it can often
be quite important to standardize the data before applying PCA. Conversely, in regularized
PCA problems such as nonnegative matrix factorization, it makes no sense to standardize
the data, since subtracting column means introduces negative entries into the matrix.

A ﬂexible approach is to allow an oﬀset in the model: we solve

minimize (cid:80)

(i,j)∈Ω(Aij − xiyj − µj)2 + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj),

(14)

with variables xi, yj, and µj. Here, µj takes the role of the column mean, and in fact will
be equal to the column mean in the trivial case k = 0.

An oﬀset may be included in the standard form regularized PCA problem (11) by aug-
menting the problem slightly. Suppose we are given an instance of the problem (11), i.e.,
we are given k, r, and ˜r. We can ﬁt an oﬀset term µj by letting k(cid:48) = k + 1 and modifying
the regularizers. Extend the regularization r : Rk → R and ˜r : Rk → R to new regularizers
r(cid:48) : Rk+1 → R and ˜r(cid:48) : Rk+1 → R which enforce that the ﬁrst column of X is constant and
the ﬁrst row of Y is not penalized. Using this scheme, the ﬁrst row of the optimal Y will be
equal to the optimal µ in (14).

Explicitly, let

(cid:26) r(x2, . . . , xk+1) x1 = 1

r(cid:48)(x) =

∞

otherwise,

and ˜r(cid:48)(y) = ˜r(y2, . . . , yk+1). (Here, we identify r(x) = r(x1, . . . , xk) to explicitly show the
dependence on each coordinate of the vector x, and similarly for ˜r.)
It is also possible to introduce row oﬀsets in the same way.

21

4 Generalized loss functions

We may also generalize the loss function in PCA to form a generalized low rank model,

minimize (cid:80)

(i,j)∈Ω Lij(xiyj, Aij) + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj),

(15)

where Lij : R × R → R+ are given loss functions for i = 1, . . . , m and j = 1, . . . , n. Problem
(15) reduces to PCA with generalized regularization when Lij(u, a) = (a − u)2. However,
the loss function Lij can now depend on the data Aij in a more complex way.

4.1 Solution methods

As before, problem (15) is not convex, even when Lij, ri and ˜rj are convex; but if all these
functions are convex, then the problem is bi-convex.

Alternating minimization. Alternating minimization can still be used to ﬁnd a local
minimum, and it is still often possible to use factorization caching to speed up the solution
of the subproblems that arise in alternating minimization. We defer a discussion of how to
solve these subproblems explicitly to §7.

Stochastic proximal gradient method. For use with extremely large scale problems,
we discuss fast variants of the basic alternating minimization algorithm in §7. For example,
we present an alternating directions stochastic proximal gradient method. This algorithm
accesses the functions Lij, ri, and ˜rj only through a subgradient or proximal interface,
allowing it to generalize trivially to nearly any loss function and regularizer. We defer a
more detailed discussion of this method to §7.

4.2 Examples

Weighted PCA. A simple modiﬁcation of the PCA objective is to weight the importance
of ﬁtting each element in the matrix A. In the generalized low rank model, we let Lij(u−a) =
wij(a − u)2, where wij is a weight, and take r = ˜r = 0. Unlike PCA, the weighted PCA
problem has no known analytical solution [SJ03]. In fact, it is NP-hard to ﬁnd an exact
solution to weighted PCA [GG11], although it is not known whether it is always possible to
ﬁnd approximate solutions of moderate accuracy eﬃciently.

Robust PCA. Despite its widespread use, PCA is very sensitive to outliers. Many authors
have proposed a robust version of PCA obtained by replacing least-squares loss with (cid:96)1 loss,
which is less sensitive to large outliers [CLMW11, WGR+09, XCS12]. They propose to solve
the problem

minimize
(cid:107)S(cid:107)1 + (cid:107)Z(cid:107)∗
subject to S + Z = A.

(16)

22

The authors interpret Z as a robust version of the principal components of the data matrix
A, and S as the sparse, possibly large noise corrupting the observations.

We can frame robust PCA as a GLRM in the following way. If Lij(u, a) = |a − u|, and

r(x) = γ

2 (cid:107)x(cid:107)2

2, ˜r(y) = γ

2 (cid:107)y(cid:107)2

2, then (15) becomes
minimize (cid:107)A − XY (cid:107)1 + γ

2 (cid:107)X(cid:107)2

F + γ

2 (cid:107)Y (cid:107)2
F .

Using the arguments in §7.6, we can rewrite the problem by introducing a new variable
Z = XY as

This results in a rank-constrained version of the estimator proposed in the literature on
robust PCA [WGR+09, CLMW11, XCS12]:

minimize
subject to Rank(Z) ≤ k.

(cid:107)A − Z(cid:107)1 + γ(cid:107)Z(cid:107)∗

minimize
subject to S + Z = A

(cid:107)S(cid:107)1 + γ(cid:107)Z(cid:107)∗

Rank(Z) ≤ k,

where we have introduced the new variable S = A − Z.

Huber PCA. The Huber function is deﬁned as
(cid:26) (1/2)x2

huber(x) =

|x| − (1/2)

|x| ≤ 1
|x| > 1.

Using Huber loss,

L(u, a) = huber(u − a),

in place of (cid:96)1 loss also yields an estimator robust to occasionaly large outliers [Hub81]. The
Huber function is less sensitive to small errors |u − a| than the (cid:96)1 norm, but becomes linear
in the error for large errors. This choice of loss function results in a generalized low rank
model formulation that is robust both to large outliers and to small Gaussian perturbations
in the data.

Previously, the problem of Gaussian noise in robust PCA has been treated by decompos-
ing the matrix A = L + S + N into a low rank matrix L, a sparse matrix S, and a matrix
with small Gaussian entries N by minimizing the loss

(cid:107)L(cid:107)∗ + (cid:107)S(cid:107)1 + (1/2)(cid:107)N (cid:107)2
F

over all decompositions A = L + S + N of A [XCS12].

In fact, this formulation is equivalent to Huber PCA with quadratic regularization on
the factors X and Y . The argument showing this is very similar to the one we made above
for robust PCA. The only added ingredient is the observation that

huber(x) = inf{|s| + (1/2)n2 : x = n + s}.

In other words, the Huber function is the inﬁmal convolution of the negative log likelihood
of a gaussian random variable and a laplacian random variable: it represents the most likely
assignment of (additive) blame for the error x to a gaussian error n and a laplacian error s.

23

Robust regularized PCA. We can design robust versions of all the regularized PCA
problems above by the same transformation we used to design robust PCA. Simply replace
the quadratic loss function with an (cid:96)1 or Huber loss function. For example, k-mediods
[KR09, PJ09] is obtained by using (cid:96)1 loss in place of quadratic loss in the quadratic clustering
problem. Similarly, robust subspace clustering [SEC13] can be obtained by using an (cid:96)1 or
Huber penalty in the subspace clustering problem.

Quantile PCA. For some applications, it can be much worse to overestimate the entries
of A than to underestimate them, or vice versa. One can capture this asymmetry by using
the loss function

L(u, a) = α(a − u)+ + (1 − α)(u − a)+

and choosing α ∈ (0, 1) appropriately. This loss function is sometimes called a scalene loss,
and can be interpreted as performing quantile regression, e.g., ﬁtting the 20th percentile
[KB78, Koe05].

Fractional PCA. For other applications, we may be interested in ﬁnding an approxima-
tion of the matrix A whose entries are close to the original matrix on a relative, rather than
an absolute, scale. Here, we assume the entries Aij are all positive. The loss function

L(u, a) = max

(cid:18) a − u
u

,

u − a
a

(cid:19)

can capture this objective. A model (X, Y ) with objective value less than 0.10mn gives a
low rank matrix XY that is on average within 10% of the original matrix.

Logarithmic PCA. Logarithmic loss functions may also useful for ﬁnding an approxima-
tion of A that is close on a relative, rather than absolute, scale. Once again, we assume all
entries of A are positive. Deﬁne the logarithmic loss

This loss is not convex, but has the nice property that it ﬁts the geometric mean of the data:

To see this, note that we are solving a least squares problem in log space. At the solution,
log(u) will be the mean of log(ai), i.e.,

L(u, a) = log2(u/a).

argmin
u

(cid:88)

i

L(u, ai) = (

ai)1/n.

(cid:89)

i

log(u) = 1/n

log(ai) = log

(cid:88)

i

(cid:33)

ai)1/n

.

(cid:32)
(

(cid:89)

i

24

It is easy to formulate a version of PCA corresponding to
Exponential family PCA.
any loss in the exponential family. Here we give some interesting loss functions generated
by exponential families when all the entries Aij are positive. (See [CDS01] for a general
treatment of exponential family PCA.) One popular loss function in the exponential family
is the KL-divergence loss,

which corresponds to a Poisson generative model [CDS01].

Another interesting loss function is the Itakura-Saito (IS) loss,

L(u, a) = a log

− a + u,

(cid:17)

(cid:16) a
u

L(u, a) = log

− 1 +

(cid:17)

(cid:16) a
u

a
u

,

which has the property that it is scale invariant, so scaling a and u by the same factor
produces the same loss [SF14]. The IS loss corresponds to Tweedie distributions (i.e., distri-
butions for which the variance is some power of the mean) [Twe84]. This makes it interesting
in applications, such as audio processing, where fractional errors in recovery are perceived.

The β-divergence,

L(u, a) =

aβ
β(β − 1)

+

−

uβ
β

auβ−1
β − 1

,

generalizes both of these losses. With β = 2, we recover quadratic loss; in the limit as β → 1,
we recover the KL-divergence loss; and in the limit as β → 0, we recover the IS loss [SF14].

4.3 Oﬀsets and scaling

In §2.6, we saw how to use standardization to rescale the data in order to compensate for
unequal scaling in diﬀerent features. In general, standardization destroys sparsity in the data
by subtracting the (column) means (which are in general non-zero) from each element of the
data matrix A. It is possible to instead rescale the loss functions in order to compensate for
unequal scaling. Scaling the loss functions instead has the advantage that no arithmetic is
performed directly on the data A, so sparsity in A is preserved.

A savvy user may be able to select loss functions Lij that are scaled to reﬂect the
importance of ﬁtting diﬀerent columns. However, it is useful to have a default automatic
scaling for times when no savvy user can be found. The scaling proposed here generalizes
the idea of standardization to a setting with heterogeneous loss functions.

Given initial loss functions Lij, which we assume are nonnegative, for each feature j let

µj = argmin

Lij(µ, Aij),

(cid:88)

µ

i:(i,j)∈Ω

σ2
j =

1
nj − 1

(cid:88)

i:(i,j)∈Ω

Lij(µj, Aij).

It is easy to see that µj generalizes the mean of column j, while σ2
j generalizes the column
variance. For example, when Lij(u, a) = (u − a)2 for every i = 1, . . . , m, j = 1, . . . , n, µj is
the mean and σ2
j is the sample variance of the jth column of A. When Lij(u, a) = |u − a| for
every i = 1, . . . , m, j = 1, . . . , n, µj is the median of the jth column of A, and σ2
j is the sum

25

of the absolute values of the deviations of the entries of the jth column from the median
value.

To ﬁt a standardized GLRM, we rescale the loss functions by σ2

j and solve

minimize (cid:80)

(i,j)∈Ω Lij(Aij, xiyj + µj)/σ2

j + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj).

(17)

Note that this problem can be recast in the standard form for a generalized low rank model
(15). For the oﬀset, we may use the same trick described in §3.3 to encode the oﬀset in the
regularization; and for the scaling, we simply replace the original loss function Lij by Lij/σ2
j .

5 Loss functions for abstract data types

We began our study of generalized low rank modeling by considering the best way to ap-
proximate a matrix by another matrix of lower rank. In this section, we apply the same
procedure to approximate a data table that may not consist of real numbers, by choosing a
loss function that respects the data type.

We now consider A to be a table consisting of m examples (i.e., rows, samples) and n
features (i.e., columns, attributes), with each entry Aij drawn from a feature set Fj. The
feature set Fj may be discrete or continuous. So far, we have only considered numerical data
(Fj = R for j = 1, . . . , n), but now Fj can represent more abstract data types. For example,
entries of A can take on Boolean values (Fj = {T, F }), integral values (Fj = 1, 2, 3, . . .),
ordinal values (Fj = {very much, a little, not at all}), or consist of a tuple of these types
(Fj = {(a, b) : a ∈ R}).

We are given a loss function Lij : R × Fj → R. The loss Lij(u, a) describes the approxi-
mation error incurred when we represent a feature value a ∈ Fj by the number u ∈ R. We
give a number of examples of these loss functions below.

We now formulate a generalized low rank model on the database A as

minimize (cid:80)

(i,j)∈Ω Lij(xiyj, Aij) + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj),

(18)

with variables X ∈ Rn×k and Y ∈ Rk×m, and with loss Lij as above and regularizers
ri(xi) : R1×k → R and ˜rj(yj) : Rk×1 → R (as before). When the domain of each loss
function is R × R, we recover the generalized low rank model on a matrix (15).

5.1 Solution methods

As before, this problem is not convex, but it is bi-convex if ri, and ˜rj are convex, and Lij is
convex in its ﬁrst argument. The problem is also separable across samples i = 1, . . . , m and
features j = 1, . . . , m. These properties makes it easy to perform alternating minimization
on this objective. Once again, we defer a discussion of how to solve these subproblems
explicitly to §7.

26

+
)
u
a
−
1
(

4

3

2

1

0

a = −1

a = 1

a = −1

a = 1

−3 −2 −1

1

2

3

−3 −2 −1

1

2

3

0
u

0
u

Figure 1: Hinge loss.

Figure 2: Logistic loss.

5.2 Examples

Boolean PCA. Suppose Aij ∈ {−1, 1}m×n, and we wish to approximate this Boolean
matrix. For example, we might suppose that the entries of A are generated as noisy, 1-
bit observations from an underlying low rank matrix XY . Surprisingly, it is possible to
accurately estimate the underlying matrix with only a few observations |Ω| from the matrix
by solving problem (18) (under a few mild technical conditions) with an appropriate loss
function [DPBW12].

We may take the loss to be

L(u, a) = (1 − au)+,

which is the hinge loss (see Figure 1), and solve the problem (18) with or without regulariza-
tion. When the regularization is sum of squares (r(x) = λ(cid:107)x(cid:107)2
2), ﬁxing X and
minimizing over yj is equivalent to training a support vector machine (SVM) on a data set
consisting of m examples with features xi and labels Aij. Hence alternating minimization
for the problem (15) reduces to repeatedly training an SVM. This model has been previously
considered under the name Maximum Margin Matrix Factorization (MMMF) [SRJ04, RS05].

2, ˜r(y) = λ(cid:107)y(cid:107)2

Logistic PCA. Again supposing Aij ∈ {−1, 1}m×n, we can also use a logistic loss to
measure the approximation quality. Let

L(u, a) = log(1 + exp(−au))

(see Figure 2). With this loss, ﬁxing X and minimizing over yj is equivalent to using logistic
regression to predict the labels Aij. This model has been previously considered under the
name logistic PCA [SSU03].

)
)
u
a
(
p
x
e
+
1
(
g
o
l

3

2

1

0

27

Figure 3: Ordinal hinge loss.

Poisson PCA. Now suppose the data Aij are nonnegative integers. We can use any loss
function that might be used in a regression framework to predict integral data to construct
a generalized low rank model for Poisson PCA. For example, we can take

L(u, a) = exp(u) − au + a log a − a.

This is the exponential family loss corresponding to Poisson data. (It diﬀers from the KL-
divergence loss from §4.2 only in that u has been replaced by exp(u), which allows u to take
negative values.)

Ordinal PCA. Suppose the data Aij records the levels of some ordinal variable, encoded
as {1, 2, . . . , d}. We wish to penalize the entries of the low rank matrix XY which deviate
by many levels from the encoded ordinal value. A convex version of this penalty is given by
the ordinal hinge loss,

L(u, a) =

(1 − u + a(cid:48))+ +

(1 + u − a(cid:48))+,

(19)

a−1
(cid:88)

a(cid:48)=1

d
(cid:88)

a(cid:48)=a+1

which generalizes the hinge loss to ordinal data (see Figure 3).

This loss function may be useful for encoding Likert-scale data indicating degrees of

agreement with a question [Lik32]. For example, we might have

Fj = {strongly disagree, disagree, neither agree nor disagree, agree, strongly agree}.

We can encode these levels as the integers 1, . . . , 5 and use the above loss to ﬁt a model to
ordinal data.

This approach assumes that every increment of error is equally bad: for example, that
approximating “agree” by “strongly disagree” is just as bad as aproximating “neither agree
nor disagree” by “agree”. In §6.1 we introduce a more ﬂexible ordinal loss function that can
learn a more ﬂexible relationship between ordinal labels. For example, it could determine
that the diﬀerence between “agree” and “strongly disagree” is smaller than the diﬀerence
between “neither agree nor disagree” and “agree”.

28

Interval PCA. Suppose that the data Aij ∈ R2 are tuples denoting the endpoints of an
interval, and we wish to ﬁnd a low rank matrix whose entries lie inside these intervals. We
can capture this objective using, for example, the deadzone-linear loss

L(u, a) = max((a1 − u)+, (u − a2)+).

5.3 Missing data and data imputation

We can use the solution (X, Y ) to a low rank model to impute values corresponding to
missing data (i, j) (cid:54)∈ Ω. This process is sometimes also called inference. Above, we saw that
for quadratically regularized PCA, the MAP estimator for the missing entry Aij is equal to
xiyj. This is still true for many of the loss functions above, such as the Huber function or (cid:96)1
loss, for which it makes sense for the data to take on any real value.

However, to approximate abstract data types we must consider a more nuanced view.
While we can still think of the solution (X, Y ) to the generalized low rank model (15) in
Boolean PCA as approximating the Boolean matrix A, the solution is not a Boolean matrix.
Instead we say that we have encoded the original Boolean matrix as a real-valued low rank
matrix XY , or that we have embedded the original Boolean matrix into the space of real-
valued matrices.

To ﬁll in missing entries in the original matrix A, we compute the value ˆAij that minimizes

the loss for xiyj:

ˆAij = argmin

Lij(xiyj, a).

a

This implicitly constrains ˆAij to lie in the domain Fj of Lij. When Lij : R × R → R, as is
the case for the losses in §4 above (including (cid:96)2, (cid:96)1, and Huber loss), then ˆAij = xiyj. But
when the data is of an abstract type, the minimum argmina Lij(u, a) will not in general be
equal to u.

For example, when the data is Boolean, Lij : {0, 1} × R → R, we compute the Boolean

matrix ˆA implied by our low rank model by solving

for MMMF, or

ˆAij = argmin

(a(XY )ij − 1)+

a∈{0,1}

ˆAij = argmin

a∈{0,1}

log(1 + exp(−a(XY )ij))

for logistic PCA. These problems both have the simple solution

ˆAij = sign(xiyj).

When Fj is ﬁnite, inference partitions the real numbers into regions

Ra = {x ∈ R : Lij(u, x) = min

Lij(u, a)}

a

corresponding to diﬀerent values a ∈ Fj. When Lij is convex, these regions are intervals.

29

We can use the estimate ˆAij even when (i, j) ∈ Ω was observed. If the original obser-
vations have been corrupted by noise, we can view ˆAij as a denoised version of the original
data. This is an unusual kind of denoising: both the noisy (Aij) and denoised ( ˆAij) versions
of the data lie in the abstract space Fj.

5.4 Interpretations and applications

We have already discussed some interpretations of X and Y in the PCA setting. Now we
reconsider those interpretations in the context of approximating these abstract data types.

Archetypes. As before, we can think of each row of Y as an archetype which captures the
behavior of an idealized example. However, the rows of Y are real numbers. To represent
each archetype l = 1, . . . , k in the abstract space as Yl with (Yl)j ∈ Fj, we solve

(Yl)j = argmin

Lj(ylj, a).

a∈Fj

(Here we assume that the loss Lij = Lj is independent of the example i.)

Archetypical representations. As before, we call xi the representation of example i in
terms of the archetypes. The rows of X give an embedding of the examples into Rk, where
each coordinate axis corresponds to a diﬀerent archetype. If the archetypes are simple to
understand or interpret, then the representation of an example can provide better intuition
about that example.

In contrast to the initial data, which may consist of arbitrarily complex data types, the
representations xi will be low dimensional vectors, and can easily be plotted, clustered, or
used in nearly any kind of machine learning algorithm. Using the generalized low rank model,
we have converted an abstract feature space into a vector space.

Feature representations. The columns of Y embed the features into Rk. Here we think
of the columns of X as archetypical features, and represent each feature j as a linear com-
bination of the archetypical features. Just as with the examples, we might choose to apply
any machine learning algorithm to the feature representations.

This procedure allows us to compare non-numeric features using their representation in
Rl. For example, if the features F are Likert variables giving the extent to which respondents
on a questionnaire agree with statements 1, . . . , n, we might be able to say that questions i
and j are similar if (cid:107)yi − yj(cid:107) is small; or that question i is a more polarizing form of question
j if yi = αyj, with α > 1.

Even more interesting, it allows us to compare features of diﬀerent types. We could say

that the real-valued feature i is similar to Likert-valued question j if (cid:107)yi − yj(cid:107) is small.

30

Latent variables. Each row of X represents an example by a vector in Rk. The matrix Y
maps these representations back into the original feature space (now nonlinearly) as described
in the discussion on data imputation in §5.3. We might think of X as discovering the latent
variables that best explain the observed data, with the added beneﬁt that these latent
variables lie in the vector space Rk.
(i,j)∈Ω Lij(xiyj, Aij) is
small, then we view these latent variables as providing a good explanation or summary of
the full data set.

If the approximation error (cid:80)

Probabilistic intepretation. We can give a probabilistic interpretation of X and Y ,
generalizing the hierarchical Bayesian model presented by Fithian and Mazumder in [FM13].
We suppose that the matrices ¯X and ¯Y are generated according to a probability distribution
with probability proportional to exp(−r( ¯X)) and exp(−˜r( ¯Y )), respectively. Our observations
A of the entries in the matrix ¯Z = ¯X ¯Y are given by

where the random variable ψij(u) takes value a with probability proportional to

Aij = ψij(( ¯X ¯Y )ij),

exp (−Lij(u, a)) .

We observe each entry (i, j) ∈ Ω. Then to ﬁnd the maximum a posteriori (MAP) estimator
(X, Y ) of ( ¯X, ¯Y ), we solve

maximize exp

(cid:16)

− (cid:80)

(cid:17)
(i,j)∈Ω Lij(xiyj, Aij)

exp(−r(X)) exp(−˜r(Y )),

which is equivalent, by taking logs, to problem (18).

This interpretation gives us a simple way to interpret our procedure for imputing missing

observations (i, j) (cid:54)∈ Ω. We are simply computing the MAP estimator ˆAij.

Auto-encoder. The matrix X encodes the data; the matrix Y decodes it back into the
full space. We can view (18) as providing the best linear auto-encoder for the data. Among
all linear encodings (X) and decodings (Y ) of the data, the abstract generalized low rank
model (18) minimizes the reconstruction error measured according to the loss functions Lij.

Compression. We impose an information bottleneck by using a low rank auto-encoder
to ﬁt the data. The bottleneck is imposed by both the dimensionality reduction and the
regularization, giving both soft and hard constraints on the information content allowed.
The solution (X, Y ) to problem (18) maximizes the information transmitted through this
k-dimensional bottleneck, measured according to the loss functions Lij. This X and Y give
a compressed and real-valued representation that may be used to more eﬃciently store or
transmit the information present in the data.

31

5.5 Oﬀsets and scaling

Just as in the previous section, better practical performance can often be achieved by allowing
an oﬀset in the model as described in §3.3, and automatic scaling of loss functions as described
in §4.3. As we noted in §4.3, scaling the loss functions (instead of standardizing the data)
has the advantage that no arithmetic is performed directly on the data A. When the data A
consists of abstract types, it is quite important that no arithmetic is performed on the data,
so that we need not take the average of, say, “very much” and “a little”, or subtract it from
“not at all”.

5.6 Numerical examples

In this section we give results of some small experiments illustrating the use of diﬀerent loss
functions adapted to abstract data types, and comparing their performance to quadratically
regularized PCA. To ﬁt these GLRMs, we use alternating minimization and solve the sub-
problems with subgradient descent. This approach is explained more fully in §7. Running
the alternating subgradient method multiple times on the same GLRM from diﬀerent initial
conditions yields diﬀerent models, all with very similar (but not identical) objective values.

Boolean PCA. For this experiment, we generate Boolean data A ∈ {−1, +1}n×m as

A = sign (cid:0)X trueY true(cid:1),
where X true ∈ Rn×ktrue and Y true ∈ Rktrue×m have independent, standard normal entries. We
consider a problem instance with m = 50, n = 50, and ktrue = k = 10.

We ﬁt two GLRMs to this data to compare their performance. Boolean PCA uses hinge
loss L(u, a) = max (1 − au, 0) and quadratic regularization r(u) = ˜r(u) = .1(cid:107)u(cid:107)2
2, and pro-
duces the model (X bool, Y bool). Quadratically regularized PCA uses squared loss L(u, a) =
(u − a)2 and the same quadratic regularization, and produces the model (X real, Y real).

Figure 4 shows the results of ﬁtting Boolean PCA to this data. The ﬁrst column shows
the original ground-truth data A; the second shows the imputed data given the model, ˆAbool,
generated by rounding the entries of X boolY bool to the closest number in 0, 1 (as explained in
§5.3); the third shows the error A− ˆAbool. Figure 4 shows the results of running quadratically
regularized PCA on the same data, and shows A, ˆAreal, and A − ˆAreal.

As expected, Boolean PCA performs substantially better than quadratically regularized
PCA on this data set. On average over 100 draws from the ground truth data distribution,
the misclassiﬁcation error (percentage of misclassiﬁed entries)

(cid:15)(X, Y ; A) =

#{(i, j) | Aij (cid:54)= sign (XY )ij}
mn

is much lower using hinge loss ((cid:15)(X bool, Y bool; A) = 0.0016) than squared loss ((cid:15)(X real, Y real; A) =
0.0051). The average RMS errors

RMS(X, Y ; A) =

(Aij − (XY )ij)2

(cid:33)1/2

(cid:32)

1
mn

m
(cid:88)

n
(cid:88)

i=1

j=1

32

using hinge loss (RMS(X bool, Y bool; A) = 0.0816) and squared loss (RMS(X real, Y real; A) =
0.159) also indicate an advantage for Boolean PCA.

Figure 4: Boolean PCA on Boolean data.

Figure 5: Quadratically regularized PCA on Boolean data.

Censored PCA.
In this example, we consider the performance of Boolean PCA when
only a subset of positive entries in the Boolean matrix A ∈ {−1, 1}m×n have been observed,
i.e., the data has been censored. For example, a retailer might know only a subset of the
products each customer purchased; or a medical clinic might know only a subset of the
diseases a patient has contracted, or of the drugs the patient has taken. Imputation can be
used in this setting to (attempt to) distinguish true negatives Aij = −1 from unobserved
positives Aij = +1, (i, j) (cid:54)∈ Ω.

We generate a low rank matrix B = XY ∈ [0, 1]m×n with X ∈ Rm×k, Y ∈ Rk×n, where
the entries of X and Y are drawn from a uniform distribution on [0, 1], m = n = 300 and
k = 3. Our data matrix A is chosen by letting Aij = 1 with probability proportional to
Bij, and −1 otherwise; the constant of proportionality is chosen so that half of the entries
in A are positive. We ﬁt a rank 5 GLRM to an observation set Ω consisting of 10% of the
positive entries in the matrix, drawn uniformly at random, using hinge loss and quadratic

33

regularization. That is, we ﬁt the low rank model

minimize (cid:80)

(i,j)∈Ω max(1 − xiyjAij, 0) + γ (cid:80)m

i=1 (cid:107)xi(cid:107)2

2 + γ (cid:80)n

j=1 (cid:107)yj(cid:107)2
2

and vary the regularization parameter γ.

normalized training error,

We consider three error metrics to measure the performance of the ﬁtted model (X, Y ):

normalized test error,

1
|Ω|

(cid:88)

(i,j)∈Ω

1
|ΩC|

(cid:88)

(i,j)∈ΩC

max(1 − Aijxiyj, 0),

max(1 − Aijxiyj, 0),

and precision at 10 (p@10), which is computed as the fraction of the top ten predicted
values not in the observation set, {xiyj : (i, j) ∈ ΩC}, for which Aij = 1. (Here, ΩC =
{1, . . . , m} × {1, . . . , n} \ Ω.) Precision at 10 measures the usefulness of the model:
if we
predicted that the top 10 unseen elements (i, j) had values +1, how many would we get
right?

Figure 6 shows the regularization path as γ ranges from 0 to 40, averaged over 50 sam-
ples from the distribution generating the data. Here, we see that while the training error
decreases as γ decreases, the test error reaches a minimum around γ = 5. Interestingly, the
precision at 10 improves as the regularization increases; since precision at 10 is computed
using only relative rather than absolute values of the model, it is insensitive to the shrinkage
of the parameters introduced by the regularization. The grey line shows the probability of
identifying a positive entry by guessing randomly; precision at 10, which exceeds 80% when
γ (cid:38) 30, is signiﬁcantly higher. This performance is particularly impressive given that the
observations Ω are generated by sampling from rather than rounding the auxiliary matrix
B.

Mixed data types.
In this experiment, we ﬁt a GLRM to a data table with numerical,
Boolean, and ordinal columns generated as follows. Let N1, N2, and N3 partition the column
indices 1, . . . , n. Choose X true ∈ Rm×ktrue, Y true ∈ Rktrue×n to have independent, standard
normal entries. Assign entries of A as follows:

Aij =






xiyj
sign (xiyj)
round(3xiyj + 1)

j ∈ N1
j ∈ N2
j ∈ N3,

where the function round maps a to the nearest integer in the set {1, . . . , 7}. Thus, N1
corresponds to real-valued data; N2 corresponds to Boolean data; and N3 corresponds to
ordinal data. We consider a problem instance in which m = 100, n1 = 40, n2 = 30, n3 = 30,
and ktrue = k = 10.

34

train error
test error

p@10

r
o
r
r
e

d
e
z
i
l
a
m
r
o
n

8

6

4

2

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1
+

f
o

y
t
i
l
i
b
a
b
o
r
p

0
0

5
5

10
10

15
15

20
20
regularization parameter
regularization parameter

25
25

30
30

35
35

40
40

Figure 6: Error metrics for Boolean GLRM on censored data. The grey line shows
the probability of identifying a positive entry by guessing randomly.

35

We ﬁt a heterogeneous loss GLRM to this data with loss function

Lij(u, a) =






Lreal(u, a)
Lbool(u, a)
Lord(u, a)

j ∈ N1
j ∈ N2
j ∈ N3,

where Lreal(u, a) = (u − a)2, Lbool(u, a) = (1 − au)+, and Lord(u, a) is deﬁned in (19), and
with quadratic regularization r(u) = ˜r(u) = .1(cid:107)u(cid:107)2
2. We ﬁt the GLRM to produce the model
(X mix, Y mix). For comparison, we also ﬁt quadratically regularized PCA to the same data,
using Lij(u, a) = (u − a)2 for all j and quadratic regularization r(u) = ˜r(u) = .1(cid:107)u(cid:107)2
2, to
produce the model (X real, Y real).

Figure 7 shows the results of ﬁtting the heterogeneous loss GLRM to the data. The ﬁrst
column shows the original ground-truth data A; the second shows the imputed data given
the model, ˆAmix, generated by rounding the entries of X mixY mix to the closest number in
0, 1 (as explained in §5.3); the third shows the error A − ˆAmix. Figure 8 corresponds to
quadratically regularized PCA, and shows A, ˆAreal, and A − ˆAreal.

To evaluate error for Boolean and ordinal data, we use the misclassiﬁcation error (cid:15) deﬁned
above. For notational convenience, we let YNl (ANl) denote Y (A) restricted to the columns
Nl in order to pick out real-valued columns (l = 1), Boolean columns (l = 2), and ordinal
columns (l = 3).

Table 1 compare the average error (diﬀerence between imputed entries and ground truth)
over 100 draws from the ground truth distribution for models using heterogeneous loss (X mix,
Y mix) and quadratically regularized loss (X real, Y real). Columns are labeled by error metric.
We use misclassiﬁcation error (cid:15) for Boolean and ordinal data and MSE for numerical data.

X mix, Y mix
X real , Y real

MSE(X, YN1; AN1)
0.0224
0.0076

(cid:15)(X, YN2; AN2)
0.0074
0.0213

(cid:15)(X, YN3; AN3)
0.0531
0.0618

Table 1: Average error for numerical, Boolean, and ordinal features using GLRM
with heterogenous loss and quadratically regularized loss.

Missing data. Here, we explore the eﬀect of missing entries on the accuracy of the re-
covered model. We generate data A as detailed above, but then censor one large block of
entries in the table (constituting 3.75% of numerical, 50% of Boolean, and 50% of ordinal
data), removing them from the observed set Ω.

Figure 9 shows the results of ﬁtting the heterogeneous loss GLRM described above on
the censored data. The ﬁrst column shows the original ground-truth data A; the second
shows the block of data that has been removed from the observation set Ω; the third shows
the imputed data given the model, ˆAmix, generated by rounding the entries of X mixY mix to
the closest number in {0, 1} (as explained in §5.3); the fourth shows the error A − ˆAmix.

36

Figure 7: Heterogeneous loss GLRM on mixed data.

Figure 8: Quadratically regularized PCA on mixed data.

37

Figure 9: Heterogeneous loss GLRM on missing data.

Figure 10: Quadratically regularized PCA on missing data.

Figure 10 corresponds to running quadratically regularized PCA on the same data, and
shows A, ˆAreal, and A − ˆAreal. While quadradically regularized PCA and the heterogeneous
loss GLRM performed similarly when no data was missing, the heterogeneous loss GLRM
performs much better than quadradically regularized PCA when a large block of data is
censored.

We compare the average error (diﬀerence between imputed entries and ground truth) over
100 draws from the ground truth distribution in Table 2. As above, we use misclassiﬁcation
error (cid:15) for Boolean and ordinal data and MSE for numerical data.

X mix, Y mix
X real , Y real

MSE(X, YN1; AN1)
0.392
0.561

(cid:15)(X, YN2; AN2)
0.2968
0.4029

(cid:15)(X, YN3; AN3)
0.3396
0.9418

Table 2: Average error over imputed data using a GLRM with heterogenous loss
and regularized quadratic loss.

6 Multi-dimensional loss functions

In this section, we generalize the procedure to allow the loss functions to depend on blocks
of the matrix XY , which allows us to represent abstract data types more naturally. For
example, we can now represent categorical values , permutations, distributions, and rankings.

38

We are given a loss function Lij : R1×dj ×Fj → R, where dj is the embedding dimension of
feature j, and d = (cid:80)
j dj is the total dimension of the embedded features. The loss Lij(u, a)
describes the approximation error incurred when we represent a feature value a ∈ Fj by the
vector u ∈ Rdj .

Let xi ∈ R1×k be the ith row of X (as before), and let Yj ∈ Rk×dj be the jth block
matrix of Y so the columns of Yj correspond to the columns of embedded feature j. We now
formulate a multi-dimensional generalized low rank model on the database A,

minimize (cid:80)

(i,j)∈Ω Lij(xiYj, Aij) + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(Yj),

(20)

with variables X ∈ Rn×k and Y ∈ Rk×d, and with loss Lij as above and regularizers
ri(xi) : R1×k → R (as before) and ˜rj(Yj) : Rk×dj → R. Note that the ﬁrst argument of Lij
is a row vector with dj entries, and the ﬁrst argument of rj is a matrix with dj columns.
When every entry Aij is real-valued (i.e., dj = 1), then we recover the generalized low rank
model (15) seen in the previous section.

6.1 Examples

Categorical PCA. Suppose that a ∈ F is a categorical variable, taking on one of d values
or labels. Identify the labels with the integers {1, . . . , d}. In (20), set

L(u, a) = (1 − ua)+ +

(1 + ua(cid:48))+,

(cid:88)

a(cid:48)∈F , a(cid:48)(cid:54)=a

and use the quadratic regularizer ri = γ(cid:107) · (cid:107)2

2, ˜r = γ(cid:107) · (cid:107)2
2.

Fixing X and optimizing over Y is equivalent to training one SVM per label to separate
that label from all the others: the jth column of Y gives the weight vector corresponding
to the jth SVM. (This is sometimes called one-vs-all multiclass classiﬁcation [RK04].) Op-
timizing over X identiﬁes the low-dimensional feature vectors for each example that allow
these SVMs to most accurately predict the labels.

The diﬀerence between categorical PCA and Boolean PCA is in how missing labels are
imputed. To impute a label for entry (i, j) with feature vector xi according to the procedure
described above in 5.3, we project the representation Yj onto the line spanned by xi to form
u = xiYj. Given u, the imputed label is simply argmaxl ul. This model has the interesting
property that if column l(cid:48) of Yj lies in the interior of the convex hull of the columns of Yj,
then ul(cid:48) will lie in the interior of the interval [minl ul, maxl ul] [BV04]. Hence the model will
never impute label l(cid:48) for any example.

We need not restrict ourselves to the loss function given above. In fact, any loss func-
tion that can be used to train a classiﬁer for categorical variables (also called a multi-class
classiﬁer) can be used to ﬁt a categorical PCA model, so long as the loss function depends
only on the inner products between the parameters of the model and the features corre-
sponding to each example. The loss function becomes the loss function L used in (20); the
optimal parameters of the model give the optimal matrix Y , while the implied features will
populate the optimal matrix X. For example, it is possible to use loss functions derived

39

from error-correcting output codes [DB95]; the Directed Acyclic Graph SVM [PCST99]; the
Crammer-Singer multi-class loss [CS02]; or the multi-category SVM [LLW04].

Of these loss functions, only the one-vs-all loss is separable across the classes a ∈ F.
(By separable, we mean that the objective value can be written as a sum over the classes.)
Hence ﬁtting a categorical features with any other loss functions is not the same as ﬁtting d
Boolean features. For example, in the Crammer-Singer loss

L(u, a) = (1 − ua + max

u(cid:48)
a)+,

a(cid:48)∈F , a(cid:48)(cid:54)=a

the classes are combined according to their maximum, rather than their sum. While one-vs-
all classiﬁcation performs about as well as more sophisticated loss functions on small data
sets [RK04], these more sophisticated nonseparable loss tend to perform much better as the
number of classes (and examples) increases [GBW14].

Some interesting nonconvex loss functions have also been suggested for this problem. For

example, consider a generalization of Hamming distance to this setting,

L(u, a) = δua,1 +

δua(cid:48) ,0,

(cid:88)

a(cid:48)(cid:54)=a

where δα,β = 0 if α = β and 1 otherwise.
In this case, alternating minimization with
regularization that enforces a clustered structure in the low rank model (see the discussion
of quadratic clustering in §3.2) reproduces the k-modes algorithm [HN99].

Ordinal PCA. We saw in §5 one way to ﬁt a GLRM to ordinal data. Here, we use a
larger embedding dimension for ordinal features. The multi-dimensional embedding will be
particularly useful when the best mapping of the ordinal variable onto a linear scale is not
uniform; e.g., if level 1 of the ordinal variable is much more similar to level 2 than level 2
is to level 3. Using a larger embedding dimension allows us to infer the relations between
the levels from the data itself. Here we again identify the labels a ∈ F with the integers
{1, . . . , d}.

One approach we can use for (multi-dimensional) ordinal PCA is to solve (20) with the

loss function

L(u, a) =

(1 − Ia>a(cid:48)ua(cid:48))+,

(21)

and with quadratic regularization. Fixing X and optimizing over Y is equivalent to training
an SVM to separate labels a ≤ l from a > l for each l ∈ F. This approach produces
a set of hyperplanes (given by the columns of Y ) separating each level l from the next.
The hyperplanes need not be parallel to each other. Fixing Y and optimizing over X ﬁnds
the low dimensional features vector for each example that places the example between the
appropriate hyperplanes.
(See Figure 11 for an illustration of an optimal ﬁt of this loss
function, with k = 2, to a simple synthetic data set.)

d−1
(cid:88)

a(cid:48)=1

40

Figure 11: Multi-dimensional ordinal loss.

Permutation PCA. Suppose that a is a permutation of the numbers 1, . . . , d. Deﬁne the
permutation loss

L(u, a) =

(1 − uai + uai+1)+.

d−1
(cid:88)

i=1

This loss is zero if uai > uai+1 + 1 for i = 1, . . . , d − 1, and increases linearly when these
inequalities are violated. Deﬁne sort(u) to return a permutation ˆa of the indices 1, . . . , d so
that uˆai ≥ uˆai+1 for i = 1, . . . , d−1. It is easy to check that argmina L(u, a) = sort(u). Hence
using the permutation loss function in generalized PCA (20) ﬁnds a low rank approximation
of a given table of permutations.

Ranking PCA. Many variants on the permutation PCA problem are possible. For ex-
ample, in ranking PCA, we interpret the permutation as a ranking of the choices 1, . . . , d,
and penalize deviations of many levels more strongly than deviations of only one level by
choosing the loss

L(u, a) =

(1 − uai + uaj )+.

d−1
(cid:88)

d
(cid:88)

i=1

j=i+1

From here, it is easy to generalize to a setting in which the rankings are only partially
observed. Suppose that we observe pairwise comparisons a ⊆ {1, . . . , d} × {1, . . . , d}, where
(i, j) ∈ a means that choice i was ranked above choice j. Then a loss function penalizing

41

devations from these observed rankings is

L(u, a) =

(1 − uai + uaj )+.

(cid:88)

(i,j)∈a

Many other modiﬁcations to ranking loss functions have been proposed in the literature
that interpolate between the the two ﬁrst loss functions proposed above, or which priori-
tize correctly predicting the top ranked choices. These losses include the area under the
curve loss [Ste07], ordered weighted average of pairwise classiﬁcation losses [UBG09], the
weighted approximate-rank pairwise loss [WBU10], the k-order statistic loss [WYW13], and
the accuracy at the top loss [BCMR12].

6.2 Oﬀsets and scaling

Just as in the previous section, better practical performance can often be achieved by allowing
an oﬀset in the model as described in §3.3, and scaling loss functions as described in §4.3.

6.3 Numerical examples

We ﬁt a low rank model to the 2013 American Community Survey (ACS) to illustrate how
to ﬁt a low rank model to heterogeneous data.

The ACS is a survey administered to 1% of the population of the United States each
year to gather their responses to a variety of demographic and economic questions. Our
data sample consists of m = 3132796 responses gathered from residents of the US, excluding
Puerto Rico, in the year 2013, on the 23 questions listed in Table 3.

We ﬁt a rank 10 model to this data using Huber loss for real valued data, hinge loss for
Boolean data, ordinal hinge loss for ordinal data, one-vs-all categorical loss for categorical
data, and regularization parameter γ = .1. We allow an oﬀset in the model and scale the
loss functions and regularization as described in §4.3.

In Table 4, we select a few features j from the model, along with their associated vec-
tors yj, and ﬁnd the two features most similar to them by ﬁnding the two features j(cid:48) which
minimize cos(yj, yj(cid:48)). The model automatically groups states which intuitively share de-
for example, three wealthy states adjoining (but excluding) a major
mographic features:
metropolitan area — Virginia, Maryland, and Connecticut — are grouped together. The
low rank structure also identiﬁes the results (high water prices) of the prolonged drought
aﬄicting California, and corroborates the intuition that work leads only to more work: hours
worked per week, weeks worked per year, and education level are highly correlated.

7 Fitting low rank models

In this section, we discuss a number of algorithms that may be used to ﬁt generalized low
rank models. As noted earlier, it can be computationally hard to ﬁnd the global optimum
of a generalized low rank model. For example, it is NP-hard to compute an exact solution

42

Description
household type
state

commercial use
house on ≥ 10 acres
household income
monthly electricity bill

Variable
HHTYPE
STATEICP
OWNERSHP own home
COMMUSE
ACREHOUS
HHINCOME
COSTELEC
COSTWATR monthly water bill
monthly gas bill
COSTGAS
FOODSTMP on food stamps
HCOVANY
SCHOOL
EDUC
GRADEATT highest grade level attained
EMPSTAT
LABFORCE
CLASSWKR class of worker
WKSWORK2 weeks worked per year
UHRSWORK usual hours worked per week real
looking for work
LOOKING
migration status
MIGRATE1

have health insurance
currently in school
highest level of education

Type
categorical
categorical
Boolean
Boolean
Boolean
real
real
real
real
Boolean
Boolean
Boolean
ordinal
ordinal
categorical
Boolean
Boolean
ordinal

employment status
in labor force

Boolean
categorical

Table 3: ACS variables.

Most similar features
Montana, North Dakota
Illinois, cost of water
Oregon, Idaho
Indiana, Michigan

Feature
Alaska
California
Colorado
Ohio
Pennsylvania Massachusetts, New Jersey
Virginia
Hours worked weeks worked, education

Maryland, Connecticut

Table 4: Most similar features in demography space.

43

to k-means [DFK+04], nonnegative matrix factorization [Vav09], and weighted PCA and
matrix completion [GG11] all of which are special cases of low rank models.

In §7.1, we will examine a number of local optimization methods based on alternating
minimization. Algorithms implementing lazy variants of alternating minimization, such as
the alternating gradient, proximal gradient, or stochastic gradient algorithms, are faster
per iteration than alternating minimization, although they may require more iterations for
convergence. In numerical experiments, we notice that lazy variants often converge to points
with a lower objective value: it seems that these lazy variants are less likely to be trapped
at a saddle point than is alternating minimization. §7.4 explores the convergence of these
algorithms in practice.

We then consider a few special cases in which we can show that alternating minimization
converges to the global optimum in some sense: for example, we will see convergence with
high probability, approximately, and in retrospect. §7.5 discusses a few strategies for initial-
izing these local optimization methods, with provable guarantees in special cases. §7.6 shows
that for problems with convex loss functions and quadratic regularization, it is sometimes
possible to certify global optimality of the resulting model.

7.1 Alternating minimization

We showed earlier how to use alternating minimization to ﬁnd an (approximate) solution
to a generalized low rank model. Algorithm (1) shows how to explicitly extend alternating
minimization to a generalized low rank model (15) with observations Ω.

Algorithm 1

given X 0, Y 0
for k = 1, 2, . . . do

for i = 1, . . . , M do
(cid:16)(cid:80)
xk
i = argminx
end for
for j = 1, . . . , N do
(cid:16)(cid:80)
yk
j = argminy
end for

end for

j:(i,j)∈Ω Lij(xyk−1

j

, Aij) + r(x)

(cid:17)

i:(i,j)∈Ω Lij(xk

i y, Aij) + ˜r(y)

(cid:17)

Parallelization. Alternating minimization parallelizes naturally over examples and fea-
tures.
In Algorithm 1, the loops over i = 1, . . . , N and over j = 1, . . . , M may both be
executed in parallel.

44

7.2 Early stopping

It is not very useful to spend a lot of eﬀort optimizing over X before we have a good estimate
for Y . If an iterative algorithm is used to compute the minimum over X, it may make sense to
stop the optimization over X early before going on to update Y . In general, we may consider
replacing the minimization over x and y above by any update rule that moves towards the
minimum. This templated algorithm is presented as Algorithm 2. Empirically, we ﬁnd that
this approach often ﬁnds a better local minimum than performing a full optimization over
each factor in every iteration, in addition to saving computational eﬀort on each iteration.

Algorithm 2

given X 0, Y 0
for t = 1, 2, . . . do

, Y t−1, A)

for i = 1, . . . , m do

i = updateL,r(xt−1
xt
end for
for j = 1, . . . , n do

i

j = updateL,˜r(y(t−1)T
yt
end for

j

, X (t)T , AT )

end for

We describe below a number of diﬀerent update rules updateL,r by writing the X update.
The Y update can be implemented similarly. (In fact, it can be implemented by substituting
˜r for r, switching the roles of X and Y , and transposing all matrix arguments.) All of the
approaches outlined below can still be executed in parallel over examples (for the X update)
and features (for the Y update).

Gradient method. For example, we might take just one gradient step on the objective.
This method can be used as long as L, r, and ˜r do not take inﬁnite values. (If any of these
functions f is not diﬀerentiable, replace ∇f below by any subgradient of f [BL10, BXM03].)

We implement updateL,r as follows. Let

(cid:88)

g =

j:(i,j)∈Ω

∇Lij(xiyj, Aij)yj + ∇r(xi).

Then set

i = xt−1
xt

i − αtg,

for some step size αt. For example, a common step size rule is αt = 1/t, which guarantees
convergence to the globally optimal X if Y is ﬁxed [BL10, BXM03].

Proximal gradient method.
If a function takes on the value ∞, it need not have a
subgradient at that point, which limits the gradient update to cases where the regularizer

45

and loss are (ﬁnite) real-valued. When the regularizer (but not the loss) takes on inﬁnite
values (say, to represent a hard constraint), we can use a proximal gradient method instead.

The proximal operator of a function f [PB13] is

proxf (z) = argmin

(f (x) +

(cid:107)x − z(cid:107)2

2).

x

1
2

If f is the indicator function of a set C, the proximal operator of f is just (Euclidean)
projection onto C.

A proximal gradient update updateL,r is implemented as follows. Let

Then set

(cid:88)

g =

j:(i,j)∈Ω

∇Lij(xt−1

i

yt−1
j

, Aij)yt−1

.

j

i = proxαtr(xt−1
xt

i − αtg),

for some step size αt. The step size rule αt = 1/t guarantees convergence to the globally
optimal X if Y is ﬁxed, while using a ﬁxed, but suﬃciently small, step size α guarantees
convergence to a small O(α) neighborhood around the optimum [Ber11]. The technical
condition required on the step size is that αt < 1/L, where L is the Lipshitz constant of
the gradient of the objective function. Bolte et al. have shown that the iterates xt
i and
yt
j produced by the proximal gradient update rule (which they call proximal alternating
linearized minimization, or PALM) globally converge to a critical point of the objective
function under very mild conditions on L, r, and ˜r [BST13].

Prox-prox method. Letting ft(X) = (cid:80)
(prox-prox) update

(i,j)∈Ω Lij(xiyt

j, Aij), deﬁne the proximal-proximal

X t+1 = proxαtr(proxαtft(X t)).

The prox-prox update is simply a proximal gradient step on the objective when f is

replaced by the Moreau envelope of f ,

Mf (X) = inf
X (cid:48)

(cid:0)f (X (cid:48)) + (cid:107)X − X (cid:48)(cid:107)2

(cid:1) .

F

(See [PB13] for details.) The Moreau envelope has the same minimizers as the original
objective. Thus, just as the proximal gradient method repeatedly applied to X converges to
global minimum of the objective if Y is ﬁxed, the prox-prox method repeatedly applied to
X also converges to global minimum of the objective if Y is ﬁxed under the same conditions
on the step size αt. for any constant stepsize α ≤ (cid:107)G(cid:107)2
2. (Here, (cid:107)G(cid:107)2 = sup(cid:107)x(cid:107)2≤1 (cid:107)Gx(cid:107)2 is
the operator norm of G.)

This update can also be seen as a single iteration of ADMM when the dual variable
in ADMM is initialized to 0; see [BPC+11].
In the case of quadratic objectives, we will
see below that the prox-prox update can be applied very eﬃciently, making iterated prox-
prox, or ADMM, eﬀective means of computing the solution to the subproblems arising in
alternating minimization.

46

In numerical experiments, we ﬁnd that using a slightly more
Choosing a step size.
nuanced rule allowing diﬀerent step sizes for diﬀerent rows and columns can allow fast
progress towards convergence while ensuring that the value of the objective never increases.
The safeguards on step sizes we propose are quite important in practice: without these
checks, we observe divergence when the initial step sizes are chosen too large.

Motivated by the convergence proof in [Ber11], for each row, we seek a step size on
the order of 1/(cid:107)gi(cid:107)2, where gi is the gradient of the objective function with respect to xi.
We start by choosing an initial step size scale αi for each row of the same order as the
average gradient of the loss functions for that row. In the numerical experiments reported
here, we choose αi = 1 for i = 1, . . . , m. Since gi grows with the number of observations
ni = |{j : (i, j) ∈ Ω}| in row i, we achieve the desired scaling by setting α0
i = αi/ni. We
take a gradient step on each row xi using the step size αi. Our procedure for choosing α0
j is
the same.

We then check whether the objective value for the row,

(cid:88)

j:(i,j)∈Ω

Lj(xiyj, Aij) + γ(cid:107)xi(cid:107)2
2,

has increased or decreased. If it has increased, then we trust our ﬁrst order approximation
to the objective function less far, and reduce the step size; if it has decreased, we gain
conﬁdence, and increase the step size.
In the numerical experiments reported below, we
decrease the step size by 30% when the objective increases, and increase the step size by 5%
when the objective decreases. This check stabilizes the algorithm and prevents divergence
even when the initial scale has been chosen poorly.

We then do the same with respect to each column yj: we take a gradient step, check if

the objective value for the column has increased or decreased, and adjust the step size.

The time per iteration is thus O(k(m + n + |Ω|)): computing the gradient of the ith loss
function with respect to xi takes time O(kni); computing the proximal operator of the square
loss takes time O(k); summing these over all the rows i = 1, . . . , m gives time O(k(m + |Ω|));
and adding the same costs for the column updates gives time O(k(m + n + |Ω|)). The checks
on the objective value take time O(k) per observation (to compute the inner product xiyj
and value of the loss function for each observation) and time O(1) per row and column to
compute the value of the regularizer. Hence the total time per iteration is O(k(m + n + |Ω|)).
By partitioning the job of updating diﬀerent rows and diﬀerent columns onto diﬀerent

processors, we can achieve an iteration time of O(k(m + n + |Ω|)/p) using p processors.

Stochastic gradients.
Instead of computing the full gradient of L with respect to xi
above, we can replace the gradient g in either the gradient or proximal gradient method by
any stochastic gradient g, which is a vector that satisﬁes

E g =

(cid:88)

j:(i,j)∈Ω

∇Lij(xiyj, Aij)yj.

47

A stochastic gradient can be computed by sampling j uniformly at random from among
observed features of i, and setting g = |{j : (i, j) ∈ Ω}|∇Lij(xiyj, Aij)yj. More samples from
{j : (i, j) ∈ Ω} can be used to compute a less noisy stochastic gradient.

7.3 Quadratic objectives

Here we describe how to eﬃciently implement the prox-prox update rule for quadratic ob-
jectives and arbitrary regularizers, extending the factorization caching technique introduced
in §2.3. We assume here that the objective is given by

(cid:107)A − XY (cid:107)2

F + r(X) + ˜r(Y ).

We will concentrate here on the X update; as always, the Y update is exactly analogous.

As in the case of quadratic regularization, we ﬁrst form the Gram matrix G = Y Y T .

Then the proximal gradient update is fast to evaluate:

proxαkr(X − αk(XG − 2AY T )).

But we can take advantage of the ease of inverting the Gram matrix G to design a
faster algorithm using the prox-prox update. For quadratic objectives with Gram matrix
G = Y T Y , the prox-prox update takes the simple form

proxαkr((G +

I)−1(AY T +

X)).

1
αk

1
αk

As in §2.3, we can compute (G + 1
αk
ization of (G + 1
αk
updating Y , since most of the computational eﬀort is in forming G and AY T .
For example, in the case of nonnegative least squares, this update is just

X) in parallel by ﬁrst caching the factor-
I)−1. Hence it is advantageous to repeat this update many times before

I)−1(AY T + 1
αk

Π+((G +

I)−1(AY T +

X)),

1
αk

1
αk

where Π+ projects its argument onto the nonnegative orthant.

7.4 Convergence

Alternating minimization need not converge to the same model (or the same objective value)
when initialized at diﬀerent starting points. Through examples, we explore this idea here.
These examples are ﬁt using the serial Julia implementation (presented in §9) of the alter-
nating proximal gradient updates method.

48

Global convergence for quadratically regularized PCA. Figure 12 shows the con-
vergence of the alternating proximal gradient update method on a quadratically regularized
PCA problem with randomly generated, fully observed data A = X trueY true, where entries
of X true and Y true are drawn from a standard normal distribution. We pick ﬁve diﬀerent
random initializations of X and Y with standard normal entries to generate ﬁve diﬀerent
convergence trajectories. Quadratically regularized PCA is a simple problem with an ana-
lytical solution (see §2), and with no local minima (see Appendix A). Hence it should come
as no surprise that the trajectories all converge to the same, globally optimal value.

Local convergence for nonnegative matrix factorization. Figure 13 shows conver-
gence of the same algorithm on a nonnegative matrix factorization model, with data gener-
ated in the same way as in Figure 12. (Note that A has some negative entries, so the minimal
objective value is strictly greater than zero.) Here, we plot the convergence of the objective
value, rather than the suboptimality, since we cannot provably compute the global minimum
of the objective function. We see that the algorithm converges to a diﬀerent optimal value
(and point) depending on the initialization of X and Y . Three trajectories converge to the
same optimal value (though one does so much faster than the others), one to a value that is
somewhat better, and one to a value that is substantially worse.

7.5 Initialization

Alternating minimization need not converge to the same solution (or the same objective
value) when initialized at diﬀerent starting points. Above, we saw that alternating mini-
mization can converge to models with optimal values that diﬀer signiﬁcantly.

Here, we discuss two approaches to initialization that result in provably good solutions,
for special cases of the generalized low rank problem. We then discuss how to apply these
initialization schemes to more general models.

SVD. A literature that is by now extensive shows that the SVD provides a provably good
initialization for the quadratic matrix completion problem (10) [KMO09, KMO10, KM10,
JNS13, Har13, GAGG13]. Algorithms based on alternating minimization have been shown
to converge quickly (even geometrically [JNS13]) to a global solution satisfying a recovery
guarantee when the initial values of X and Y are chosen carefully; see §2.4 for more details.
Here, we extend the SVD initialization previously proposed for matrix completion to one
that works well for all PCA-like problems: problems with convex loss functions that have
been scaled as in §4.3; with data A that consists of real values, Booleans, categoricals, and
ordinals; and with quadratic (or no) regularization.

But we will need a matrix on which to perform the SVD. What matrix corresponds to our
data table? Here, we give a simple proposal for how to construct such a matrix, motivated
by [KMO10, JNS13, Cha14]. Our key insight is that the SVD is the solution to our problem
when the entries in the table have mean zero and variance one (and all the loss functions are

49

y
t
i
l
a
m

i
t
p
o
b
u
s

e
v
i
t
c
e
j
b
o

105

104

103

102

101

100

0

1

2

3

time (s)

Figure 12: Convergence of alternating proximal gradient updates on quadratically
regularized PCA for n = m = 200, k = 2.

quadratic). Our initialization will construct a matrix with mean zero and variance one from
the data table, take its SVD, and invert the construction to produce the correct initialization.
Our ﬁrst step is to expand the categorical columns taking on d values into d Boolean
columns, and to re-interpret ordinal and Boolean columns as numbers. The scaling we
propose below is insensitive to the values of the numbers in the expansion of the Booleans: for
example, using (false, true)= (0, 1) or (false, true)= (−1, 1) produces the same initialization.
The scaling is sensitive to the diﬀerences between ordinal values: while encoding (never,
sometimes, always) as (1, 2, 3) or as (−5, 0, 5) will make no diﬀerence, encoding these ordinals
as (0, 1, 10) will result in a diﬀerent initialization.

Now we assume that the rows of the data table are independent and identically dis-
tributed, so they each have equal means and variances. Our mission is to standardize the

50

·104

e
u
l
a
v

e
v
i
t
c
e
j
b
o

1.8

1.7

1.6

1.5

1.4

1.3

0

1

2

3

time (s)

Figure 13: Convergence of alternating proximal gradient updates on NNMF for
n = m = 200, k = 2.

columns. The observed entries in column j have mean µj and variance σ2
j ,

µj = argmin

µ

σ2
j =

1
nj − 1

(cid:88)

i:(i,j)∈Ω
(cid:88)

i:(i,j)∈Ω

Lj(µ, Aij)

Lj(µj, Aij),

so the matrix whose (i, j)th entry is (Aij − µj)/σj for (i, j) ∈ Ω has columns whose observed
entries have mean 0 and variance 1.

Each missing entry can be safely replaced with 0 in the scaled version of the data without
changing the column mean. But the column variance will decrease to mj/m. If instead we
deﬁne

˜Aij =

(cid:26) m
σj mj
0

(Aij − µj)

(i, j) ∈ Ω
otherwise,

then the column will have mean 0 and variance 1.

51

Take the SVD U ΣV T of ˜A, and let ˜U ∈ Rm×k, ˜Σ ∈ Rk×k, and ˜V ∈ Rn×k denote these
matrices truncated to the top k singular vectors and values. We initialize X = ˜U ˜Σ1/2, and
Y = ˜Σ1/2 ˜V T diag(σ). The oﬀset row in the model is initialized with the means, i.e., the kth
column of X is ﬁlled with 1’s, and the kth row of Y is ﬁlled with the means, so Ykj = µj.

Finally, note that we need not compute the full SVD of ˜A, but instead can simply compute
the top k singular triples. For example, the randomized top k SVD algorithm proposed in
[HMT11] computes the top k singular triples of ˜A in time linear in |Ω|, m, and n (and
quadratic in k).

Figure 14 compares the convergence of this SVD-based initialization with random ini-
tialization on a low rank model for census data described in detail in §6.3. We initialize the
algorithm at six diﬀerent points: from ﬁve diﬀerent random normal initializations (entries
of X 0 and Y 0 drawn iid from N (0, 1)), and from the SVD of ˜A. The SVD initialization
produces a better initial value for the objective function, and also allows the algorithm to
converge to a substantially lower ﬁnal objective value than can be found from any of the ﬁve
random starting points. This behaviour indicates that the “good” local minimum discovered
by the SVD initialization is located in a basin of attraction that has low probability with
respect to the measure induced by random normal initialization.

k-means++. The k-means++ algorithm is an initialization scheme designed for quadratic
clustering problems [AV07]. It consists of choosing an initial cluster centroid at random from
the points, and then choosing the remaining k − 1 centroids from the points x that have
not yet been chosen with probability proportional to D(x)2, where D(x) is the minimum
distance of x to any previously chosen centroid.

Quadratic clustering is known to be NP-hard, even with only two clusters (k = 2)
[DFK+04]. However, k-means++ followed by alternating minimization gives a solution with
expected approximation ratio within O(log k) of the optimal value [AV07]. (Here, the expec-
tation is over the randomization in the initialization algorithm.) In contrast, an arbitrary
initialization of the cluster centers for k-means can result in a solution whose value is arbi-
trarily worse than the true optimum.

A similar idea can be used for other low rank models. If the model rewards a solution
that is spread out, as is the case in quadratic clustering or subspace clustering, it may be
better to initialize the algorithm by choosing elements with probability proportional to a
distance measure, as in k-means++.
In the k-means++ procedure, one can use the loss
function L(u) as the distance metric D.

7.6 Global optimality

All generalized low rank models are non-convex, but some are more non-convex than others.
In particular, for some problems, the only important source of non-convexity is the low rank
constraint. For these problems, it is sometimes possible to certify global optimality of a
model by considering an equivalent rank-constrained convex problem.

The arguments in this section are similar to ones found in [RFP10], in which Recht et al.
propose using a factored (nonconvex) formulation of the (convex) nuclear norm regularized

52

random
random
random
random
random
SVD

·105

e
u
l
a
v

e
v
i
t
c
e
j
b
o

9

8

7

6

5

4

3

2

0

5

10

15

30

35

40

45

50

20

25
iteration

Figure 14: Convergence from ﬁve diﬀerent random initializations, and from the
SVD initialization.

53

estimator in order to eﬃciently solve the large-scale SDP arising in a matrix completion
problem. However, the algorithm in [RFP10] relies on a subroutine for ﬁnding a local
minimum of an augmented Lagrangian which has the same biconvex form as problem (10).
Finding a local minimum of this problem (rather than a saddle point) may be hard. In this
section, we avoid the issue of ﬁnding a local minimum of the nonconvex problem; we consider
instead whether it is possible to verify global optimality when presented with some putative
solution.

The factored problem is equivalent to the rank constrained problem. Consider
the factored problem

minimize L(XY ) + γ

2 (cid:107)X(cid:107)2

F + γ

2 (cid:107)Y (cid:107)2
F ,

with variables X ∈ Rm×k, Y ∈ Rk×n, where L : Rm×n → R is any convex loss function.
Compare this to the rank-constrained problem

minimize L(Z) + γ(cid:107)Z(cid:107)∗
subject to Rank(Z) ≤ k.

with variable Z ∈ Rm×n. Here, we use (cid:107) · (cid:107)∗ to denote the nuclear norm, the sum of the
singular values of a matrix.

Theorem 1. (X (cid:63), Y (cid:63)) is a solution to the factored problem 22 if and only if Z (cid:63) = X (cid:63)Y (cid:63) is
a solution to the rank-constrained problem 23, and (cid:107)X (cid:63)(cid:107)2

F = (cid:107)Y (cid:63)(cid:107)2

F = 1

2(cid:107)Z (cid:63)(cid:107)∗.

We will need the following lemmas to understand the relation between the rank-constrained

problem and the factored problem.

Lemma 1. Let XY = U ΣV T be the SVD of XY , where Σ = diag(σ). Then

(22)

(23)

(24)

Proof. We may derive this fact as follows:

(cid:107)σ(cid:107)1 ≤

(||X||2

F + ||Y ||2

F ).

1
2

(cid:107)σ(cid:107)1 = tr(U T XY V )

≤ (cid:107)U T X(cid:107)F (cid:107)Y V (cid:107)F
≤ (cid:107)X(cid:107)F (cid:107)Y (cid:107)F

≤

(||X||2

F + ||Y ||2

F ),

1
2

where the ﬁrst inequality above uses the Cauchy-Schwartz inequality, the second relies on the
orthogonal invariance of the Frobenius norm, and the third follows from the basic inequality
ab ≤ 1

2(a2 + b2) for any real numbers a and b.

54

Lemma 2. For any matrix Z, (cid:107)Z(cid:107)∗ = inf XY =Z

1

2(||X||2

F + ||Y ||2

F ).

Proof. Writing Z = U DV T and recalling the deﬁnition of the nuclear norm (cid:107)Z(cid:107)∗ = (cid:107)σ(cid:107)1,
we see that Lemma 1 implies

(cid:107)Z(cid:107)∗ ≤ inf

XY =Z

(||X||2

F + ||Y ||2

F ).

1
2

But taking X = U Σ1/2 and Y = Σ1/2V T , we have

1
2

1
2

(||X||2

F + ||Y ||2

F ) =

((cid:107)Σ1/2(cid:107)2

F + (cid:107)Σ1/2(cid:107)2

F ) = (cid:107)σ(cid:107)1,

(using once again the orthogonal invariance of the Frobenius norm), so the bound is satisﬁed
with equality.

Note that the inﬁmum is achieved by X = U Σ1/2T and Y = T T Σ1/2V T for any orthonor-

mal matrix T .

Theorem 1 follows as a corollary, since L(Z) = L(XY ) so long as Z = XY .

The rank constrained problem is sometimes equivalent to an unconstrained prob-
lem. Note that problem (23) is still a hard problem to solve:
it is a rank-constrained
semideﬁnite program. On the other hand, the same problem without the rank constraint is
convex and tractable (though not easy to solve at scale). In particular, it is possible to write
down an optimality condition for the problem

minimize L(Z) + γ(cid:107)Z(cid:107)∗

(25)

that certiﬁes that a matrix Z is globally optimal. This problem is a relaxation of prob-
lem (23), and so has an optimal value that is at least as small. Furthermore, if any solution
to problem (25) has rank no more than k, then it is feasible for problem (23), so the op-
timal values of problem (25) and problem (23) must be the same. Hence any solution of
problem (25) with rank no more than k also solves problem 23.

Recall that the matrix Z is a solution the problem U if and only if

0 ∈ ∂(L(Z) + γ(cid:107)Z(cid:107)∗),

where ∂f (Z) is the subgradient of the function f at Z. The subgradient is a set-valued
function.

The subgradient of the nuclear norm at a matrix Z = U ΣV T is any matrix of the form
U V T + W where U T W = 0, W V = 0, and (cid:107)W (cid:107)2 ≤ 1. Equivalently, deﬁne the set-valued
function sign on scalar arguments x as

sign(x) =






{1}
x > 0
[ − 1, 1] x = 0
x < 0,
{−1}

,

55

and deﬁne (sign(x))i = sign(xi) for vectors x ∈ Rn. Then we can write the subgradient of
the nuclear norm at Z as

∂(cid:107)Z(cid:107)∗ = U diag(sign(σ))V T ,
where now we use the full SVD of Z with U ∈ Rm×min(m,n), V ∈ Rn×min(m,n), and σ ∈
Rmin(m,n).

Hence Z = U ΣV T is a solution to problem (25) if and only if

0 ∈ ∂L(Z) + γ(U V T + W ),

or more simply, if

(cid:107)(1/γ)G + U V T (cid:107)2 ≤ 1

(26)

for some G ∈ ∂L(Z). In particular, if a matrix Z with rank no more than k satisﬁes (26),
then Z also solves the rank-constrained problem (23).

This result allows us to (sometimes) certify global optimality of a particular model.
Given a model (X, Y ), we compute the SVD of the product XY = U ΣV T and an element
G ∈ ∂L(Z). If (cid:107)(1/γ)G + U V T (cid:107)2 ≤ 1, then (X, Y ) is globally optimal. (If the objective is
diﬀerentiable then we simply pick G = ∇L(Z); otherwise some choices of G ∈ ∂L(Z) may
produce invalid certiﬁcates even if (X, Y ) is globally optimal.)

8 Choosing low rank models

8.1 Regularization paths

Suppose that we wish to understand the entire regularization path for a GLRM; that is, we
would like to know the solution (X(γ), Y (γ)) to the problem

minimize (cid:80)

(i,j)∈Ω Lij(xiyj, Aij) + γ (cid:80)m

i=1 ri(xi) + γ (cid:80)n

j=1 ˜rj(yj)

as a function of γ. Frequently, the regularization path may be computed almost as quickly
as the solution for a single value of γ. We can achieve this by initially ﬁtting the model
with a very high value for γ, which is often a very easy problem. (For example, when r
and ˜r are norms, the solution is (X, Y ) = (0, 0) for suﬃciently large γ.) Then we may
ﬁt models corresponding to smaller and smaller values of γ by initializing the alternating
minimization algorithm from our previous solution. This procedure is sometimes called a
homotopy method.

For example, Figure 15 shows the regularization path for quadratically regularized Huber
PCA on a synthetic data set. We generate a dataset A = XY +S with X ∈ Rm×k, Y ∈ Rk×n,
and S ∈ Rm×n, with m = n = 300 and k = 3. The entries of X and Y are drawn from a
standard normal distribution, while the entries of the sparse noise matrix S are drawn from
a uniform distribution on [0, 1] with probability 0.05, and are 0 otherwise. We ﬁt a rank 5
GLRM to an observation set Ω consisting of 10% of the entries in the matrix, drawn uniformly

56

at random from {1, . . . , i} × {1, . . . , j}, using Huber loss and quadratic regularization, and
vary the regularization parameter. That is, we ﬁt the model

minimize (cid:80)

(i,j)∈Ω huber(xiyj, Aij) + γ (cid:80)m

i=1 (cid:107)xi(cid:107)2

2 + γ (cid:80)n

j=1 (cid:107)yj(cid:107)2
2

and vary the regularization parameter γ. The ﬁgure plots both the normalized training error,

and the normalized test error,

1
|Ω|

(cid:88)

(i,j)∈Ω

huber(xiyj, Aij),

1
nm − |Ω|

(cid:88)

(i,j)(cid:54)∈Ω

huber(xiyj, Aij),

of the ﬁtted model (X, Y ), for γ ranging from 0 to 3. Here, we see that while the training error
decreases and γ decreases, the test error reaches a minimum around γ = .5. Interestingly,
it takes only three times longer (about 3 seconds) to generate the entire regularization path
than it does to ﬁt the model for a single value of the regularization parameter (about 1
second).

test error
train error

0.35

0.3

0.25

0.2

0.15

0.1

r
o
r
r
e

d
e
z
i
l
a
m
r
o
n

0

0.5

1

2

2.5

3

Figure 15: Regularization path.

1.5
γ

57

8.2 Choosing model parameters

To form a generalized low rank model, one needs to specify the loss functions Lj, regularizers
r and ˜r, and a rank k. The loss function should usually be chosen by a domain expert to
reﬂect the intuitive notion of what it means to “ﬁt the data well”. On the other hand, the
regularizers and rank are often chosen based on statistical considerations, so that the model
generalizes well to unseen (missing) data.

There are three major considerations to balance in choosing the regularization and rank
In the following discussion, we suppose that the regularizers r = γr0 and

of the model.
˜r = γ˜r0 have been chosen up to a scaling γ.

Compression. A low rank model (X, Y ) with rank k and no sparsity represents the data
table A with only (m + n)k numbers, achieving a compression ratio of (m + n)k/(mn). If the
factors X or Y are sparse, then we have used fewer than (m + n)k numbers to represent the
data A, achieving a higher compression ratio. We may want to pick parameters of the model
(k and γ) in order to achieve a good error (cid:80)
(i,j)∈Ω Lj(Aij − xiyj) for a given compression
ratio. For each possible combination of model parameters, we can ﬁt a low rank model with
those parameters, observing both the error and the compression ratio. We can then choose
the best model parameters (highest compression rate) achieving the error we require, or the
best model parameters (lowest error rate) achieving the compression we require.

More formally, one can construct an information criterion for low rank models by analogy
with the Aikake Information Criterion (AIC) or the Bayesian Information Criterion (BIC).
For use in the AIC, the number of degrees of freedom in a low rank model can be computed
as the diﬀerence between the number of nonzeros in the model and the dimensionality of the
symmetry group of the problem. For example, if the model (X, Y ) is dense, and the regu-
larizer is invariant under orthogonal transformations (e.g., r(x) = (cid:107)x(cid:107)2
2), then the number
of degrees of freedom is (m + n)k − k2 [TB99]. Minka [Min01] proposes a method based on
the BIC to automatically choose the dimensionality in PCA, and observes that it performs
better than cross validation in identifying the true rank of the model when the number of
observations is small (m, n (cid:46) 100).

Denoising. Suppose we observe every entry in a true data matrix contaminated by noise,
e.g., Aij = Atrue
ij + (cid:15)ij, with (cid:15)ij some random variable. We may wish to choose model
parameters to identify the truth and remove the noise: we would like to ﬁnd k and γ to
minimize (cid:80)

(i,j)∈Ω Lj(Atrue

ij − xiyj).

A number of commonly used rules-of-thumb have been proposed in the case of PCA to
distinguish the signal (the true rank k of the data) from the noise, some of which can be
generalized to other low rank models. These include using scree plots, often known as the
“elbow method” [Cat66]; the eigenvalue method; Horn’s parallel analysis [Hor65, Din09];
and other related methods [ZV86, PM03]. A recent, more sophisticated method adapts the
idea of dropout training [SHK+14] to regularize low-rank matrix estimation [JW14].

Some of these methods can easily be adapted to the GLRM context. The “elbow method”
increases k until the objective value decreases less than linearly; the eigenvalue method

58

increases k until the objective value decreases by less than some threshold; Horn’s parallel
analysis increases k until the objective value compares unfavorably to one generated by
ﬁtting a model to data drawn from a synthetic noise distribution.

Cross validation is also simple to apply, and is discussed further below as a means of
predicting missing entries. However, applying cross validation to the denoising problem is
somewhat tricky, since leaving out too few entries results in overﬁtting to the noise, while
leaving out too many results in underﬁtting to the signal. The optimal number of entries to
leave out may depend on the aspect ratio of the data, as well as on the type of noise present
in the data [Per09], and is not well understood except in the case of Gaussian noise [OP09].
We explore the problem of choosing a holdout size numerically below.

Predicting missing entries. Suppose we observe some entries in the matrix and wish
to predict the others. A GLRM with a higher rank will always be able to ﬁt the (noisy)
data better than one of lower rank. However, a model with many parameters may also
overﬁt to the noise. Similarly, a GLRM with no regularization (γ = 0) will always produce
a model with a lower empirical loss (cid:80)
(i,j)∈Ω Lj(xiyj, Aij). Hence, we cannot pick a rank k or
regularization γ simply by considering the objective value obtained by ﬁtting the low rank
model.

But by resampling from the data, we can simulate the performance of the model on out
of sample (missing) data to identify GLRMs that neither over nor underﬁt. Here, we discuss
a few methods for choosing model parameters by cross-validation; that is, by resampling
from the data to evaluate the model’s performance. Cross validation is commonly used in
regression models to choose parameters such as the regularization parameter γ, as in Figure
15. In GLRMs, cross validation can also be used to choose the rank k. Indeed, using a lower
rank k can be considered another form of model regularization.

We can distinguish between three sources of noise or variability in the data, which give

rise to three diﬀerent resampling procedures.

• The rows or columns of the data are chosen at random, i.e., drawn iid from some

population. In this case it makes sense to resample the rows or columns.

• The rows or columns may be ﬁxed, but the indices of the observed entries in the matrix
are chosen at random. In this case, it makes sense to resample from the observed entries
in the matrix.

• The indices of the observed entries are ﬁxed, but the values are observed with some
measurement error. In this case, it makes sense to resample the errors in the model.

Each of these leads to a diﬀerent reasonable kind of resampling scheme. The ﬁrst two
give rise to resampling schemes based on cross validation (i.e., resampling the rows, columns,
or individual entries of the matrix) which we discuss further below. The third gives rise to
resampling schemes based on the bootstrap or jackknife procedures, which resample from
the errors or residuals after ﬁtting the model. A number of methods using the third kind

59

of resampling have been proposed in order to perform inference (i.e., generate conﬁdence
intervals) for PCA; see Josse et al. [JWH14] and references therein.

As an example, let’s explore the eﬀect of varying |Ω|/mn, γ, and k. We generate random
, Y ∈ Rktrue×n, and S ∈ Rm×n, with m = n = 300 and
data as follows. Let X ∈ Rm×ktrue
ktrue = 3,. Draw the entries of X and Y from a standard normal distribution, and draw the
entries of the sparse outlier matrix S are drawn from a uniform distribution on [0, 3] with
probability 0.05, and are 0 otherwise. Form A = XY + S. Select an observation set Ω by
picking entries in the matrix uniformly at random from {1, . . . , n} × {1, . . . , m}. We ﬁt a
rank k GLRM with Huber loss and quadratic regularization γ(cid:107) · (cid:107)2
2, varying |Ω|/mn, γ, and
k, and compute the test error. We average our results over 5 draws from the distribution
generating the data.

In Figure 16, we see that the true rank k = 3 performs best on cross-validated error for
any number of observations |Ω|. (Here, we show performance for γ = 0. The plot for other
values of the regularization parameter is qualitatively the same.) Interestingly, it is easiest to
identify the true rank with a small number of observations: higher numbers of observations
make it more diﬃcult to overﬁt to the data even when allowing higher ranks.

|Ω|/mn=0.1
|Ω|/mn=0.3
|Ω|/mn=0.5
|Ω|/mn=0.7
|Ω|/mn=0.9

r
o
r
r
e

t
s
e
t

d
e
z
i
l
a
m
r
o
n

1

0.8

0.6

0.4

0.2

0

1

2

4

5

Figure 16: Test error as a function of k, for γ = 0.

In Figure 17, we consider the interdependence of our choice of γ and k. Regularization is
most important when few matrix elements have been observed: the curve for each k is nearly
ﬂat when more than about 10% of the entries have been observed, so we show here a plot

3
k

60

for |Ω| = .1mn. Here, we see that the true rank k = 3 performs best on cross-validated error
for any value of the regularization parameter. Ranks that are too high (k > 3) beneﬁt from
increased regularization γ, whereas higher regularization hurts the performance of models
with k lower than the true rank. That is, regularizing the rank (small k) can substitute for
explicit regularization of the factors (large γ).

k=1
k=2
k=3
k=4
k=5

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

r
o
r
r
e

t
s
e
t

d
e
z
i
l
a
m
r
o
n

0

1

2

3

4

5

γ

Figure 17: Test error as a function of γ when 10% of entries are observed.

Finally, in Figure 18 we consider how the ﬁt of the model depends on the number of
observations. If we correctly guess the rank k = 3, we ﬁnd that the ﬁt is insensitive to the
number of observations. If our rank is either too high or too low, the ﬁt improves with more
observations.

8.3 On-line optimization

Suppose that new examples or features are being added to our data set continuously, and we
wish to perform on-line optimization, which means that we should have a good estimate at
any time for the representations of those examples xi or features yj which we have seen. This
model is equivalent to adding new rows or columns to the data table A as the algorithm
continues.
In this setting, alternating minimization performs quite well, and has a very
natural interpretation. Given an estimate for Y , when a new example is observed in row i,

61

k=1
k=2
k=3
k=4
k=5

r
o
r
r
e

t
s
e
t

d
e
z
i
l
a
m
r
o
n

1

0.8

0.6

0.4

0.2

0

0.1

0.3

0.7

0.9

0.5
|Ω|/mn

Figure 18: Test error as a function of observations |Ω|/mn, for γ = 0.

we may solve

minimize (cid:80)

j:(i,j)∈Ω Lij(Aij, xyj) + r(x)

with variable x to compute a representation for row i. This computation is exactly the same
as one step of alternating minimization. Here, we are ﬁnding the best feature representation
for the new example in terms of the (already well understood) archetypes Y . If the number
of other examples previously seen is large, the addition of a single new example should not
change the optimal Y by very much; hence if (X, Y ) was previously the global minimum of
(15), this estimate of the feature representation for the new example will be very close to its
optimal representation (i.e., the one that minimizes problem (15)). A similar interpretation
holds when new columns are added to A.

9

Implementations

The authors have developed and released three open source codes for modelling and ﬁtting
generalized low rank models: a basic serial implementation written in Python, a serial and
shared-memory parallel implementation written in Julia, and a distributed implementation
written in Scala using the Spark framework. The Julia and Spark implementations use the

62

alternating proximal gradient method described in §7 to ﬁt GLRMs, while the Python imple-
mentation uses alternating minimization and a cvxpy [DCB14] backend for each subproblem.
In this section we brieﬂy discuss these implementations, and report some timing results. For
a full description and up-to-date information about available functionality, we encourage the
reader to consult the on-line documentation for each of these packages.

There are also many implementations available for ﬁtting special cases of GLRMs. For
example, an implementation capable of ﬁtting any GLRM for which the subproblems in an
alternating minimization method are quadratic programs was recently developed in Spark
by Debasish Das [DD14].

9.1 Python implementation

GLRM.py is a Python implementation for ﬁtting GLRMs that can be found, together with
documentation, at

https://github.com/cehorn/glrm.

We encourage the interested reader to consult the on-line documentation for the most up-
to-date functionality and a collection of examples.

Usage. The user initializes a GLRM by specifying

• the data table A (A), stored as a Python list of 2-D arrays, where each 2-D array in A

contains all data associated with a particular loss function,

• the list of loss functions L (Lj, j = 1, . . . , n), that correspond to the data as speciﬁed

by A,

• regularizers regX (r) and regY (˜r),

• the rank k (k),

• an optional list missing_list with the same length as A so that each entry of missing_list

is a list of missing entries corresponding to the data from A, and

• an optional convergence object converge that characterizes the stopping criterion for

the alternating minimization procedure.

The following example illustrates how to use GLRM.py to ﬁt a GLRM with Boolean
(A_bool) and numerical (A_real) data, with quadratic regularization and a few missing
entries.

from glrm import GLRM
from glrm.loss import QuadraticLoss, HingeLoss
from glrm.reg import QuadraticReg

# import the model
# import losses
# import regularizer

63

A = [A_bool, A_real]
L = [Hinge_Loss, QuadraticLoss]
regX, regY = QuadraticReg(0.1), QuadraticReg(0.1)
missing_list = [[], [(0,0), (0,1)]]

# data stored as a list
# loss function as a list
# penalty weight is 0.1
# indexed by submatrix

model = GLRM(A, L, regX, regY, k, missing_list)
model.fit()

# initialize GLRM
# fit GLRM

The fit() method automatically adds an oﬀset to the GLRM and scales the loss functions
as described in §4.3.

GLRM.py ﬁts GLRMS by alternating minimization. The code instantiates cvxpy problems
[DCB14] corresponding to the X- and Y -update steps, then iterates by alternately solving
each problem until convergence criteria are met.

The following loss functions and regularizers are supported by GLRM.py:

• quadratic loss QuadraticLoss,

• Huber loss HuberLoss,

• hinge loss HingeLoss,

• ordinal loss OrdinalLoss,

• no regularization ZeroReg,

• (cid:96)1 regularization LinearReg,

• quadratic regularization QuadraticReg, and

• nonnegative constraint NonnegativeReg.

Users may implement their own loss functions (regularizers) using the abstract class Loss
(Reg).

9.2 Julia implementation

LowRankModels is a code written in Julia [BKSE12] for modelling and ﬁtting GLRMs. The
implementation is available on-line at

https://github.com/madeleineudell/LowRankModels.jl.

We discuss some aspects of the usage and features of the code here. For a full description
and up-to-date information about available functionality, we encourage the reader to consult
the on-line documentation.

64

Usage. To form a GLRM using LowRankModels, the user speciﬁes

• the data A (A), which can be any array or array-like data structure (e.g., a Julia

DataFrame);

• the observed entries obs (Ω), a list of tuples of the indices of the observed entries in
the matrix, which may be omitted if all the entries in the matrix have been observed;

• the list of loss functions losses (Lj, j = 1, . . . , n), one for each column of A;

• the regularizers rx (r) and ry (˜r); and

• the rank k (k).

For example, the following code forms and ﬁts a k-means model with k = 5 on the matrix
A ∈ Rm×n.

losses = fill(quadratic(),n)
rx = unitonesparse()
ry = zeroreg()
glrm = GLRM(A,losses,rx,ry,k) # form GLRM
X,Y,ch = fit!(glrm)

# fit GLRM

# quadratic loss
# x is 1-sparse unit vector
# y is not regularized

LowRankModels uses the proximal gradient method described in §7.2 to ﬁt GLRMs. The
optimal model is returned in the factors X and Y, while ch gives the convergence history. The
exclamation mark suﬃx follows the convention in Julia denoting that the function mutates
at least one of its arguments. In this case, it caches the best ﬁt X and Y as glrm.X and
glrm.Y [CE14].

Losses and regularizers must be of type Loss and Regularizer, respectively, and may

be chosen from a list of supported losses and regularizers, which include

• quadratic loss quadratic,

• hinge loss hinge,

• (cid:96)1 loss l1,

• Huber loss huber,

• ordinal hinge loss ordinal_hinge,

• quadratic regularization quadreg,

• no regularization zeroreg,

• nonnegative constraint nonnegative, and

• 1-sparse constraint onesparse.

• unit 1-sparse constraint unitonesparse.

Users may also implement their own losses and regularizers.

65

Shared memory parallelism. LowRankModels takes advantage of Julia’s SharedArray
data structure to implement a ﬁtting procedure that takes advantage of shared memory par-
allelism. While Julia does not yet support threading, SharedArrays in Julia allow separate
processes on the same computer to access the same block of memory. To ﬁt a model using
multiple processes, LowRankModels loads the data A and the initial model X and Y into
shared memory, broadcasts other problem data (e.g., the losses and regularizers) to each
process, and assigns to each process a partition of the rows of X and columns of Y . At every
iteration, each process updates its rows of X, its columns of Y , and computes its portion of
the objective function, synchronizing after each of these steps to ensure that e.g.the X up-
date is completed before the Y update begins; then the master process checks a convergence
criterion and adjusts the step length.

Automatic modeling. LowRankModels is capable of adding oﬀsets to a GLRM, and of
automatically scaling the loss functions, as described in §4.3. It can also automatically detect
the types of diﬀerent columns of a data frame and select an appropriate loss. Using these
features, LowRankModels implements a method

glrm(dataframe, k)

that forms a rank k model on a data frame, automatically selecting loss functions and
regularization that suit the data well, and ignoring any missing (NA) element in the data
frame. This GLRM can then be ﬁt with the function fit!.

Example. As an example, we ﬁt a GLRM to the Motivational States Questionnaire (MSQ)
data set [RA98]. This data set measures 3896 subjects on 92 aspects of mood and personality
type, as well as recording the time of day the data were collected. The data include real-
valued, Boolean, and ordinal measurements, and approximately 6% of the measurements are
missing (NA).

The following code loads the MSQ data set and encodes it in two dimensions:

using RDatasets
using LowRankModels
# pick a data set
df = RDatasets.dataset("psych","msq")
# encode it!
X,Y,labels,ch = fit(glrm(df,2))

Figure 19 uses the rows of Y as a coordinate system to plot some of the features of the
data set. Here we see the automatic embedding separates positive from negative emotions
along the y axis. This embedding is notable for being interpretable despite having been
generated completely automatically. Of course, better embeddings may be obtained by a
more careful choice of loss functions, regularizers, scaling, and embedding dimension k.

66

Figure 19: An automatic embedding of the MSQ [RA98] data set into two dimen-
sions.

9.3 Spark implementation

SparkGLRM is a code written in Scala, built on the Spark cluster programming framework
[ZCF+10], for modelling and ﬁtting GLRMs. The implementation is available on-line at

http://git.io/glrmspark.

In SparkGLRM, the data matrix A is split entry-wise across many machines, just
Design.
as in [HMLZ14]. The model (X, Y ) is replicated and stored in memory on every machine.

67

Thus the total computation time required to ﬁt the model is proportional to the number
of nonzeros divided by the number of cores, with the restriction that the model should ﬁt
in memory. (The authors leave to future work an extension to models that do not ﬁt in
memory, e.g., by using a parameter server [SSZ14].) Where possible, hardware acceleration
(via breeze and BLAS) is used for local linear algebraic operations.

At every iteration, the current model is broadcast to all machines, so there is only one
copy of the model on each machine. This particularly important in machines with many
cores, because it avoids duplicating the model those machines. Each core on a machine will
process a partition of the input matrix, using the local copy of the model.

Usage. The user provides loss functions Lij(u, a) indexed by i = 0, . . . , m − 1 and j =
0, . . . , n − 1, so a diﬀerent loss function can be deﬁned for each column, or even for each
entry. Each loss function is deﬁned by its gradient (or a subgradient). The method signature
is

loss grad(i: Int, j: Int, u: Double, a: Double)

whose implementation can be customized by particular i and j. As an example, the following
line implements squared error loss (L(u, a) = 1/2(u − a)2) for all entries:

Similarly, the user provides functions implementing the proximal operator of the regu-
larizers r and ˜r, which take a dense vector and perform the appropriate proximal operation.

Experiments. We ran experiments on several large matrices. For size comparison, a very
popular matrix in the recommender systems community is the Netﬂix Prize Matrix, which
has 17770 rows, 480189 columns, and 100480507 nonzeros. Below we report results on several
larger matrices, up to 10 times larger. The matrices are generated by ﬁxing the dimensions
and number of nonzeros per row, then uniformly sampling the locations for the nonzeros,
and ﬁnally ﬁlling in those locations with a uniform random number in [0, 1].

We report iteration times using an Amazon EC2 cluster with 10 slaves and one master,
of instance type “c3.4xlarge”. Each machine has 16 CPU cores and 30 GB of RAM. We
ran SparkGLRM to ﬁt two GLRMs on matrices of varying sizes. Table 5 gives results for
quadratically regularized PCA (i.e., quadratic loss and quadratic regularization) with k = 5.
To illustrate the capability to write and ﬁt custom loss functions, we also ﬁt a GLRM using
a loss function that depends on the parity of i + j:

Lij(u, a) =

(cid:26) |u − a|
(u − a)2

i + j is even
i + j is odd,

with r(x) = (cid:107)x(cid:107)1 and ˜r(y) = (cid:107)y(cid:107)2
2, setting k = 10. (This loss function was chosen merely to
illustrate the generality of the implementation. Usually losses will be the same for each row
in the same column.) The results for this custom GLRM are given in Table 6.

u - a

68

Matrix size # nonzeros Time per iteration (s)

106 × 106
106 × 106
107 × 107

106 × 106
106 × 106
107 × 107

106
109
109

106
109
109

Table 5: SparkGLRM for quadratically regularized PCA, k = 5.

Matrix size # nonzeros Time per iteration (s)

7
11
227

9
13
294

Table 6: SparkGLRM for custom GLRM, k = 10.

The table gives the time per iteration. The number of iterations required for convergence
depends on the size of the ambient dimension. On the matrices with the dimensions shown in
Tables 5 and 6, convergence typically requires about 100 iterations, but we note that useful
GLRMs often emerge after only a few tens of iterations.

Acknowledgements

The authors are grateful to Chris De Sa, Yash Deshpande, Nicolas Gillis, Maya Gupta,
Trevor Hastie, Irene Kaplow, Lester Mackey, Andrea Montanari, Art Owen, Haesun Park,
David Price, Chris R´e, Ben Recht, Yoram Singer, Nati Srebro, Ashok Srivastava, Peter
Stoica, Sze-chuan Suen, Stephen Taylor, Joel Tropp, Ben Van Roy, and Stefan Wager for
a number of illuminating discussions and comments on early drafts of this paper, and to
Debasish Das and Matei Zaharia for their insights into creating a successful Spark imple-
mentation. This work was developed with support from the National Science Foundation
Graduate Research Fellowship program (under Grant No. DGE-1147470), the Gabilan Stan-
ford Graduate Fellowship, the Gerald J. Lieberman Fellowship, and the DARPA X-DATA
program.

69

A Quadratically regularized PCA

In this appendix we describe some properties of the quadratically regularized PCA prob-
lem (3),

minimize (cid:107)A − XY (cid:107)2

F + γ(cid:107)X(cid:107)2

F + γ(cid:107)Y (cid:107)2
F .

(27)

In the sequel, we let U ΣV T = A be the SVD of A and let r be the rank of A. We assume for
convenience that all the nonzero singular values σ1 > σ2 > · · · > σr > 0 of A are distinct.

A.1 Solution

Problem (3) is the only problem we will encounter that has an analytical solution. A solution
is given by

X = ˜U ˜Σ1/2,

Y = ˜Σ1/2 ˜V T ,

(28)

where ˜U and ˜V are deﬁned as in (5), and ˜Σ = diag((σ1 − γ)+, . . . , (σk − γ)+).

To prove this, let’s consider the optimality conditions of (3). The optimality conditions

are

−(A − XY )Y T + γX = 0,

−(A − XY )T X + γY T = 0.

Multiplying the ﬁrst optimality condition on the left by X T and the second on the left by
Y and rearranging, we ﬁnd

X T (A − XY )Y T = γX T X,

Y (A − XY )T X = γY Y T ,

which shows, by taking a transpose, that X T X = Y Y T at any stationary point.

We may rewrite the optimality conditions together as

(cid:20)−γI

A
AT −γI

(cid:21) (cid:20) X
Y T

(cid:21)

(cid:21)

(cid:21) (cid:20) X
Y T

=

=

=

(cid:20)

0
XY
(XY )T
0
(cid:21)
(cid:20) X(Y Y T )
Y T (X T X)
(cid:20) X
Y T

(X T X),

(cid:21)

where we have used the fact that X T X = Y Y T .

Now we see that (X, Y T ) lies in an invariant subspace of the matrix

(cid:20)−γI

A
AT −γI

(cid:21)

. Recall

that V is an invariant subspace of a matrix A if AV = V M for some matrix M . If Rank(M ) ≤
Rank(A), we know that the eigenvalues of M are eigenvalues of A, and that the corresponding
eigenvectors lie in the span of V .

Thus the eigenvalues of X T X must be eigenvalues of

, and (X, Y T ) must

span the corresponding eigenspace. More concretely, notice that

is (symmetric,

(cid:21)

(cid:20)−γI

A
AT −γI
(cid:20)−γI

(cid:21)

A
AT −γI

70

and therefore) diagonalizable, with eigenvalues −γ ± σi. The larger eigenvalues −γ + σi
correspond to the eigenvectors (ui, vi), and the smaller ones −γ − σi to (ui, −vi).

Now, X T X is positive semideﬁnite, so the eigenvalues shared by X T X and

(cid:20)−γI

A
AT −γI

(cid:21)

√

must be positive. Hence there is some set |Ω| ≤ k with σi ≥ γ for i ∈ Ω such that X has
−γ + σi for i ∈ Ω. (Recall that X T X = Y Y T , so Y has the same
have singular values
singular values as X.) Then (X, Y T ) spans the subspace generated by the vectors (ui, vi for
i ∈ Ω. We say the stationary point (X, Y ) has active subspace Ω. It is easy to verify that
XY = (cid:80)

i∈Ω ui(σi − γ)vT
i .

Each active subspace gives rise to an orbit of stationary points. If (X, Y ) is a stationary

point, then (XT, T −1Y ) is also a stationary point so long as

−(A − XY )Y T T −T + γXT = 0,

−(A − XY )T XT + γY T T −T = 0,

which is always true if T −T = T , i.e., T is orthogonal. This shows that the set of stationary
points is invariant under orthogonal transformations.

To simplify what follows, we choose a representative element for each orbit. Represent

any stationary point with active subspace Ω by

X = UΩ(ΣΩ − γI)1/2,

Y = (ΣΩ − γI)1/2V T
Ω ,

where by UΩ we denote the submatrix of U with columns indexed by Ω, and similarly for
(cid:0)k(cid:48)(γ)
(cid:1)
Σ and V . At any value of γ, let k(cid:48)(γ) = max{i : σi ≥ γ}. Then we have (cid:80)k
i
(representative) stationary points, one for each choice of Ω The number of (representative)
stationary points is decreasing in γ; when γ > σ1, the only stationary point is X = 0, Y = 0.
These stationary points can have quite diﬀerent values. If (X, Y ) has active subspace Ω,

i=0

then

||A − XY ||2

F + γ(||X||2

F + ||Y ||2

F ) =

σ2
i +

(cid:0)γ2 + 2γ|σi − γ|(cid:1) .

(cid:88)

i /∈Ω

(cid:88)

i∈Ω

From this form, it is clear that we should choose Ω to include the top singular values i =
1, . . . , k(cid:48)(γ). Choosing any other subset Ω will result in a higher (worse) objective value:
that is, the other stationary points are not global minima.

A.2 Fixed points of alternating minimization

Theorem 2. The quadratically regularized PCA problem (3) has only one local minimum,
which is the global minimum.

Our proof is similar to that of [BH89], who proved a related theorem for the case of

PCA (2).
Proof. We showed above that every stationary point of (3) has the form XY = (cid:80)
i∈Ω uidivT
i ,
with Ω ⊆ {1, . . . , k(cid:48)}, |Ω| ≤ k, and di = σi − γ. We use the representative element from each
√
divT
stationary orbit described above, so each column of X is ui
i
for some i ∈ Ω. The columns of X are orthogonal, as are the rows of Y .

di and each row of Y is

√

71

If a stationary point is not the global minimum, then σj > σi for some i ∈ Ω, j (cid:54)∈ Ω.
Below, we show we can always ﬁnd a descent direction if this condition holds, thus showing
that the only local minimum is the global minimum.

Assume we are at a stationary point with σj > σi for some i ∈ Ω, j (cid:54)∈ Ω. We will ﬁnd a
j . Form ˜X by replacing the column of
i by

descent direction by perturbing XY in direction ujvT
X containing ui
√

di, and ˜Y by replacing the row of Y containing

di by (ui + (cid:15)uj)

divT

√

√

√

di(vi + (cid:15)vj)T . Now the regularization term increases slightly:

γ((cid:107) ˜X(cid:107)2

F + (cid:107) ˜Y (cid:107)2

F ) − γ((cid:107)X(cid:107)2

F + (cid:107)Y (cid:107)2

F ) =

(2γti(cid:48)) + 2γdi(1 + (cid:15)2) −

2γti(cid:48)

(cid:88)

i(cid:48)∈Ω

(cid:88)

i(cid:48)∈Ω,i(cid:48)(cid:54)=i
= 2γdi(cid:15)2.

Meanwhile, the approximation error decreases:

(cid:107)A − ˜X ˜Y (cid:107)2

F − (cid:107)A − XY (cid:107)2

F = (cid:107)uiσivT

i + ujσjvT

j − (ui + (cid:15)uj)di(vi + (cid:15)vj)T (cid:107)2

= (cid:107)ui(σi − di)vT

i + uj(σj − (cid:15)2di)vT

j − (cid:15)uidivT

F − (σi − di)2 − σ2
j
i (cid:107)2
F

j − (cid:15)ujdivT

−(σi − di)2 − σ2
j
(cid:13)
(cid:20)σi − di
−(cid:15)di
(cid:13)
(cid:13)
σj − (cid:15)2di
−(cid:15)di
(cid:13)

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
F

=

= (σi − di)2 + (σj − (cid:15)2di)2 + 2(cid:15)2d2
i + 2(cid:15)2d2
= −2σj(cid:15)2di + (cid:15)4d2
i
= 2(cid:15)2di(di − σj) + (cid:15)4d2
i ,

− (σi − di)2 − σ2
j

i − (σi − di)2 − σ2
j

where we have used the rotational invariance of the Frobenius norm to arrive at the third
equality above. Hence the net change in the objective value in going from (X, Y ) to ( ˜X, ˜Y )
is

2γdi(cid:15)2 + 2(cid:15)2di(di − σj) + (cid:15)4d2

i = 2(cid:15)2di(γ + di − σj) + (cid:15)4d2
i
= 2(cid:15)2di(σi − σj) + (cid:15)4d2
i ,

which is negative for small (cid:15). Hence we have found a descent direction, showing that any
stationary point with σj > σi for some i ∈ Ω, j (cid:54)∈ Ω is not a local minimum.

72

References

[AAJN13] A. Agarwal, A. Anandkumar, P. Jain, and P. Netrapalli. Learning sparsely
used overcomplete dictionaries via alternating minimization. arXiv preprint
arXiv:1310.7991, 2013.

[ABEV09] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. A new approach to collab-
orative ﬁltering: Operator estimation with spectral regularization. The Journal
of Machine Learning Research, 10:803–826, 2009.

[AEB06] M. Aharon, M. Elad, and A. Bruckstein. k-SVD: An algorithm for designing
overcomplete dictionaries for sparse representation. IEEE Transactions on Signal
Processing, 54(11):4311–4322, 2006.

[AM04]

[AV07]

P. K. Agarwal and N. H. Mustafa. k-means projective clustering. In Proceed-
ings of the 23rd ACM SIGMOD-SIGACT-SIGART Symposium on Principles of
Database Systems, pages 155–165. ACM, 2004.

D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding.
In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete
Algorithms, pages 1027–1035. Society for Industrial and Applied Mathematics,
2007.

[BBL+07] M. Berry, M. Browne, A. Langville, V. Pauca, and R. Plemmons. Algorithms and
applications for approximate nonnegative matrix factorization. Computational
Statistics & Data Analysis, 52(1):155–173, 2007.

[BCMR12] S. Boyd, C. Cortes, M. Mohri, and A. Radovanovic. Accuracy at the top. In

Advances in Neural Information Processing Systems, pages 962–970, 2012.

[BDKP14] R. Boyd, B. Drake, D. Kuang, and H. Park. Smallk is a C++/Python high-
performance software library for nonnegative matrix factorization (NMF) and
hierarchical and ﬂat clustering using the NMF; current version 1.2.0. http:
//smallk.github.io/, June 2014.

[Ber11]

[BH89]

D. P. Bertsekas. Incremental gradient, subgradient, and proximal methods for
convex optimization: A survey. Optimization for Machine Learning, 2010:1–38,
2011.

P. Baldi and K. Hornik. Neural networks and principal component analysis:
Learning from examples without local minima. Neural Networks, 2(1):53–58,
1989.

[BKSE12] J. Bezanson, S. Karpinski, V. B. Shah, and A. Edelman. Julia: A fast dynamic

language for technical computing. arXiv preprint arXiv:1209.5145, 2012.

73

[BL10]

J. Borwein and A. Lewis. Convex analysis and nonlinear optimization: theory
and examples, volume 3. Springer Science & Business Media, 2010.

[BM03a]

S. Boyd and J. Mattingley. Branch and bound methods. Lecture notes for
EE364b, Stanford University, 2003.

[BM03b]

S. Burer and R. Monteiro. A nonlinear programming algorithm for solving
semideﬁnite programs via low-rank factorization. Mathematical Programming,
95(2):329–357, 2003.

[BM03c]

S. Burer and R. D. C. Monteiro. Local minima and convergence in low-rank
semideﬁnite programming. Mathematical Programming, 103:2005, 2003.

[BPC+11] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimiza-
tion and statistical learning via the alternating direction method of multipliers.
Foundations and Trends in Machine Learning, 3(1):1–122, 2011.

[BRRT12] V. Bittorf, B. Recht, C. R´e, and J. A. Tropp. Factoring nonnegative matri-
ces with linear programs. Advances in Neural Information Processing Systems,
25:1223–1231, 2012.

[BST13]

J. Bolte, S. Sabach, and M. Teboulle. Proximal alternating linearized minimiza-
tion for nonconvex and nonsmooth problems. Mathematical Programming, pages
1–36, 2013.

[BV04]

S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University
Press, 2004.

[BXM03]

S. Boyd, L. Xiao, and A. Mutapcic. Subgradient methods. Lecture notes for
EE364b, Stanford University, 2003.

[Cat66]

Raymond B Cattell. The scree test for the number of factors. Multivariate
behavioral research, 1(2):245–276, 1966.

[CDS98]

S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit.
SIAM Journal on Scientiﬁc Computing, 20(1):33–61, 1998.

[CDS01] M. Collins, S. Dasgupta, and R. Schapire. A generalization of principal com-
ponent analysis to the exponential family. In Advances in Neural Information
Processing Systems, volume 13, page 23, 2001.

[CE14]

[Cha14]

J. Chen and A. Edelman. Parallel preﬁx polymorphism permits parallelization,
presentation & proof. arXiv preprint arXiv:1410.6449, 2014.

S. Chatterjee. Matrix estimation by universal singular value thresholding. The
Annals of Statistics, 43(1):177–214, 2014.

74

[CLMW11] E. Cand`es, X. Li, Y. Ma, and J. Wright. Robust principal component analysis?

Journal of the ACM (JACM), 58(3):11, 2011.

[CP09]

[CR08]

[CS02]

[CT10]

[DB95]

E. Cand`es and Y. Plan. Matrix completion with noise. CoRR, abs/0903.3131,
2009.

E. Cand`es and B. Recht. Exact matrix completion via convex optimization.
CoRR, abs/0805.4471, 2008.

K. Crammer and Y. Singer. On the algorithmic implementation of multiclass
kernel-based vector machines. The Journal of Machine Learning Research, 2:265–
292, 2002.

E. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix
completion. IEEE Transactions on Information Theory, 56(5):2053–2080, 2010.

T. Dietterich and G. Bakiri. Solving multiclass learning problems via error-
correcting output codes. CoRR, cs.AI/9501101, 1995.

[DCB14]

S. Diamond, E. Chu, and S. Boyd. CVXPY: A Python-embedded modeling
language for convex optimization, version 0.2. http://cvxpy.org/, May 2014.

[DD14]

D. Das and S. Das. Quadratic programing solver for non-negative matrix factor-
ization with spark. In Spark Summit 2014, 2014.

[dEGJL04] A. d’Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. Lanckriet. A direct
In Advances in

formulation for sparse PCA using semideﬁnite programming.
Neural Information Processing Systems, volume 16, pages 41–48, 2004.

[DFK+04] P. Drineas, A. Frieze, R. Kannan, S. Vempala, and V. Vinay. Clustering large
graphs via the singular value decomposition. Machine Learning, 56(1-3):9–33,
2004.

[Din09]

[DL84]

A. Dinno. Implementing Horn’s parallel analysis for principal component analysis
and factor analysis. Stata Journal, 9(2):291, 2009.

J. De Leeuw. The Giﬁ system of nonlinear multivariate analysis. Data analysis
and informatics, 3:415–424, 1984.

[DLM09]

J. De Leeuw and P. Mair. Giﬁ methods for optimal scaling in R: The package
homals. Journal of Statistical Software, pages 1–30, 2009.

[DLPP06] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal nonnegative matrix t-
factorizations for clustering. In Proceedings of the 12th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data Mining, pages 126–135.
ACM, 2006.

75

[DLYT76] J. De Leeuw, F. Young, and Y. Takane. Additive structure in qualitative data:
An alternating least squares method with optimal scaling features. Psychome-
trika, 41(4):471–503, 1976.

[DPBW12] M. Davenport, Y. Plan, E. Berg, and M. Wootters. 1-bit matrix completion.

arXiv preprint arXiv:1209.3672, 2012.

[DS14]

[EV09]

[EY36]

[FBD09]

A. Damle and Y. Sun. Random projections for non-negative matrix factorization.
arXiv preprint arXiv:1405.4275, 2014.

E. Elhamifar and R. Vidal. Sparse subspace clustering. In IEEE Conference on
Computer Vision and Pattern Recognition, 2009, pages 2790–2797. IEEE, 2009.

C. Eckart and G. Young. The approximation of one matrix by another of lower
rank. Psychometrika, 1(3):211–218, 1936.

C. F´evotte, N. Bertin, and J. Durrieu. Nonnegative matrix factorization with
the Itakura-Saito divergence: With application to music analysis. Neural Com-
putation, 21(3):793–830, 2009.

[FHB04] M. Fazel, H. Hindi, and S. Boyd. Rank minimization and applications in sys-
tem theory. In Proceedings of the 2004 American Control Conference (ACC),
volume 4, pages 3273–3278. IEEE, 2004.

[FM13]

W. Fithian and R. Mazumder. Scalable convex methods for ﬂexible low-rank
matrix modeling. arXiv preprint arXiv:1308.4211, 2013.

[GAGG13] S. Gunasekar, A. Acharya, N. Gaur, and J. Ghosh. Noisy matrix completion
using alternating minimization. In Machine Learning and Knowledge Discovery
in Databases, pages 194–209. Springer, 2013.

[GBW14] M. Gupta, S. Bengio, and J. Weston. Training highly multiclass classiﬁers. The

Journal of Machine Learning Research, 15(1):1461–1492, 2014.

[GD14]

A. Gress and I. Davidson. A ﬂexible framework for projecting heterogeneous
data. In Proceedings of the 23rd ACM International Conference on Conference
on Information and Knowledge Management, CIKM ’14, pages 1169–1178, New
York, NY, USA, 2014. ACM.

[GG11]

N. Gillis and F. Glineur. Low-rank matrix approximation with weights or
missing data is NP-hard. SIAM Journal on Matrix Analysis and Applications,
32(4):1149–1165, 2011.

[Gil11]

N. Gillis. Nonnegative matrix factorization: Complexity, algorithms and appli-
cations. PhD thesis, UCL, 2011.

76

[Gor02]

G. J. Gordon. Generalized2 linear2 models. In Advances in Neural Information
Processing Systems, pages 577–584, 2002.

[GRX+10] A. Goldberg, B. Recht, J. Xu, R. Nowak, and X. Zhu. Transduction with matrix
In Advances in Neural Information

completion: Three birds with one stone.
Processing Systems, pages 757–765, 2010.

[Har13]

M. Hardt. On the provable convergence of alternating minimization for matrix
completion. arXiv preprint arXiv:1312.0925, 2013.

[HMLZ14] T. Hastie, R. Mazumder, J. Lee, and R. Zadeh. Matrix completion and low-rank

svd via fast alternating least squares. arXiv, 2014.

[HMT11] N. Halko, P.-G. Martinsson, and J. Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions.
SIAM Review, 53(2):217–288, 2011.

[HN99]

[Hor65]

[Hot33]

[Hot36]

Z. Huang and M. Ng. A fuzzy k-modes algorithm for clustering categorical data.
IEEE Transactions on Fuzzy Systems, 7(4):446–452, 1999.

J. Horn. A rationale and test for the number of factors in factor analysis. Psy-
chometrika, 30(2):179–185, 1965.

H. Hotelling. Analysis of a complex of statistical variables into principal compo-
nents. Journal of Educational Psychology, 24(6):417, 1933.

H. Hotelling. Relations between two sets of variates. Biometrika, 28(3-4):321–
377, 1936.

[Hub81]

P. Huber. Robust Statistics. Wiley, New York, 1981.

[JBAS10] M. Journ´ee, F. Bach, P. Absil, and R. Sepulchre. Low-rank optimization on
the cone of positive semideﬁnite matrices. SIAM Journal on Optimization,
20(5):2327–2351, 2010.

[JNS13]

P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank matrix completion using
alternating minimization. In Proceedings of the 45th annual ACM Symposium
on the Theory of Computing, pages 665–674. ACM, 2013.

[Jol86]

I. Jolliﬀe. Principal component analysis. Springer, 1986.

[JW14]

J. Josse and S. Wager. Stable autoencoding: A ﬂexible framework for regularized
low-rank matrix estimation. arXiv preprint arXiv:1410.8275, 2014.

[JWH14]

J. Josse, S. Wager, and F. Husson. Conﬁdence areas for ﬁxed-eﬀects pca. arXiv
preprint arXiv:1407.7614, 2014.

77

[KB78]

[Kes12]

[KHP14]

[KM10]

R. Koenker and J. G. Bassett. Regression quantiles. Econometrica: Journal of
the Econometric Society, pages 33–50, 1978.

R. Keshavan. Eﬃcient algorithms for collaborative ﬁltering. PhD thesis, Stanford
University, 2012.

J. Kim, Y. He, and H. Park. Algorithms for nonnegative matrix and tensor
factorizations: A uniﬁed view based on block coordinate descent framework.
Journal of Global Optimization, 58(2):285–319, 2014.

R. Keshavan and A. Montanari. Regularization for matrix completion.
In
2010 IEEE International Symposium on Information Theory Proceedings (ISIT),
pages 1503–1507. IEEE, 2010.

[KMO09] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries.

In Advances in Neural Information Processing Systems, pages 952–960, 2009.

[KMO10] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries.
IEEE Transactions on Information Theory, 56(6):2980–2998, 2010.

[KO09]

R. Keshavan and S. Oh. A gradient descent algorithm on the Grassman manifold
for matrix completion. arXiv preprint arXiv:0910.5260, 2009.

[Koe05]

R. Koenker. Quantile regression. Cambridge University Press, 2005.

[KP07]

[KP08a]

[KP08b]

[KP11]

H. Kim and H. Park. Sparse non-negative matrix factorizations via alternating
non-negativity-constrained least squares for microarray data analysis. Bioinfor-
matics, 23(12):1495–1502, 2007.

H. Kim and H. Park. Nonnegative matrix factorization based on alternating
nonnegativity constrained least squares and active set method. SIAM Journal
on Matrix Analysis and Applications, 30(2):713–730, 2008.

J. Kim and H. Park. Toward faster nonnegative matrix factorization: A new
algorithm and comparisons. In Eighth IEEE International Conference on Data
Mining, pages 353–362. IEEE, 2008.

J. Kim and H. Park. Fast nonnegative matrix factorization: An active-set-like
method and comparisons. SIAM Journal on Scientiﬁc Computing, 33(6):3261–
3281, 2011.

[KR09]

L. Kaufman and P. J. Rousseeuw. Finding groups in data: an introduction to
cluster analysis, volume 344. John Wiley & Sons, 2009.

[LBRN06] H. Lee, A. Battle, R. Raina, and A. Ng. Eﬃcient sparse coding algorithms. In

Advances in Neural Information Processing Systems, pages 801–808, 2006.

78

[Lik32]

[Lin07]

[Llo82]

R. Likert. A technique for the measurement of attitudes. Archives of Psychology,
1932.

C. Lin. Projected gradient methods for nonnegative matrix factorization. Neural
Computation, 19(10):2756–2779, 2007.

S. Lloyd. Least squares quantization in PCM. IEEE Transactions on Information
Theory, 28(2):129–137, 1982.

[LLW04] Y. Lee, Y. Lin, and G. Wahba. Multicategory support vector machines: Theory
and application to the classiﬁcation of microarray data and satellite radiance
data. Journal of the American Statistical Association, 99(465):67–81, 2004.

[LRS+10]

J. Lee, B. Recht, R. Salakhutdinov, N. Srebro, and J. Tropp. Practical large-scale
optimization for max-norm regularization. In Advances in Neural Information
Processing Systems, pages 1297–1305, 2010.

[LS99]

[LS01]

[LV09]

[LW66]

[Mac09]

D. Lee and H. Seung. Learning the parts of objects by non-negative matrix
factorization. Nature, 401(6755):788–791, 1999.

D. Lee and H. Seung. Algorithms for non-negative matrix factorization.
Advances in Neural Information Processing Systems, pages 556–562, 2001.

In

Z. Liu and L. Vandenberghe. Interior-point method for nuclear norm approxima-
tion with application to system identiﬁcation. SIAM Journal on Matrix Analysis
and Applications, 31(3):1235–1256, 2009.

E. Lawler and D. Wood. Branch-and-bound methods: A survey. Operations
Research, 14(4):699–719, 1966.

L. Mackey. Deﬂation methods for sparse PCA. In D. Koller, D. Schuurmans,
Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing
Systems, 2009.

[Mar12]

I. Markovsky. Low Rank Approximation: Algorithms, Implementation, Applica-
tions. Communications and Control Engineering. Springer, 2012.

[MBPS09] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary learning for sparse
coding. In Proceedings of the 26th Annual International Conference on Machine
Learning, pages 689–696. ACM, 2009.

[MCCD13] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Eﬃcient estimation of word

representations in vector space. arXiv preprint arXiv:1301.3781, 2013.

[MF10]

K. Mohan and M. Fazel. Reweighted nuclear norm minimization with application
to system identiﬁcation. In Proceedings of the 2010 American Control Conference
(ACC), pages 2953–2959. IEEE, 2010.

79

[MHT10] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms
for learning large incomplete matrices. The Journal of Machine Learning Re-
search, 11:2287–2322, 2010.

[Min01]

In T.K. Leen, T.G.
T. Minka. Automatic choice of dimensionality for pca.
Dietterich, and V. Tresp, editors, Advances in Neural Information Processing
Systems, pages 598–604. MIT Press, 2001.

[MPS+09] J. Mairal, J. Ponce, G. Sapiro, A. Zisserman, and F. Bach. Supervised dictionary
learning. In Advances in Neural Information Processing Systems, pages 1033–
1040, 2009.

[MSC+13] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed rep-
resentations of words and phrases and their compositionality. In Advances in
Neural Information Processing Systems, pages 3111–3119, 2013.

[NNS+14] P. Netrapalli, U. Niranjan, S. Sanghavi, A. Anandkumar, and P. Jain. Provable
non-convex robust PCA. In Advances in Neural Information Processing Systems,
pages 1107–1115, 2014.

[NRRW11] F. Niu, B. Recht, C. R´e, and S. Wright. Hogwild!: A lock-free approach to
In Advances in Neural Information

parallelizing stochastic gradient descent.
Processing Systems, 2011.

[OF97]

[OP09]

[Osn14]

[PB13]

B. Olshausen and D. Field. Sparse coding with an overcomplete basis set: A
strategy employed by V1? Vision Research, 37(23):3311–3325, 1997.

A. Owen and P. Perry. Bi-cross-validation of the svd and the nonnegative matrix
factorization. The Annals of Applied Statistics, pages 564–594, 2009.

S. Osnaga. Low Rank Representations of Matrices using Nuclear Norm Heuris-
tics. PhD thesis, Colorado State University, 2014.

N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in Opti-
mization, 1(3):123–231, 2013.

[PCST99] J. Platt, N. Cristianini, and J. Shawe-Taylor. Large margin DAGs for multiclass
In Advances in Neural Information Processing Systems, pages

classiﬁcation.
547–553, 1999.

[Pea01]

K. Pearson. On lines and planes of closest ﬁt to systems of points in space. The
London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science,
2(11):559–572, 1901.

[Per09]

Cross-validation for unsupervised learning.

arXiv preprint

P. Perry.
arXiv:0909.3052, 2009.

80

[PJ09]

[PM03]

[PSM14]

[RA98]

H.-S. Park and C.-H. Jun. A simple and fast algorithm for k-medoids clustering.
Expert Systems with Applications, 36(2, Part 2):3336 – 3341, 2009.

K. Preacher and R. MacCallum. Repairing Tom Swift’s electric factor analysis
machine. Understanding Statistics: Statistical Issues in Psychology, Education,
and the Social Sciences, 2(1):13–43, 2003.

J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for word repre-
sentation. Proceedings of the Empiricial Methods in Natural Language Processing
(EMNLP 2014), 12, 2014.

W. Revelle and K. Anderson. Personality, motivation and cognitive performance:
Final report to the army research institute on contract MDA 903-93-K-0008.
Technical report, 1998.

[RBL+07] R. Raina, A. Battle, H. Lee, B. Packer, and A. Ng. Self-taught learning: Transfer
learning from unlabeled data. In Proceedings of the 24th International Conference
on Machine Learning, pages 759–766. ACM, 2007.

[RFP10]

B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization. SIAM Review, 52(3):471–501,
August 2010.

R. Rifkin and A. Klautau. In defense of one-vs-all classiﬁcation. The Journal of
Machine Learning Research, 5:101–141, 2004.

B. Recht and C. R´e. Parallel stochastic gradient algorithms for large-scale matrix
completion. Mathematical Programming Computation, 5(2):201–226, 2013.

[RRWN11] B. Recht, C. R´e, S. Wright, and F. Niu. Hogwild: A lock-free approach to
In Advances in Neural Information

parallelizing stochastic gradient descent.
Processing Systems, pages 693–701, 2011.

J. Rennie and N. Srebro. Fast maximum margin matrix factorization for col-
laborative prediction. In Proceedings of the 22nd International Conference on
Machine Learning, pages 713–719. ACM, 2005.

P. Richt´arik, M. Tak´aˇc, and S. Ahipa¸sao˘glu. Alternating maximization: Unifying
framework for 8 sparse PCA formulations and eﬃcient parallel codes. arXiv
preprint arXiv:1212.4137, 2012.

[SBPP06] F. Shahnaz, M. W. Berry, V. P. Pauca, and R. J. Plemmons. Document clustering
using nonnegative matrix factorization. Information Processing & Management,
42(2):373–386, 2006.

[SC12]

M. Soltanolkotabi and E. Candes. A geometric analysis of subspace clustering
with outliers. The Annals of Statistics, 40(4):2195–2238, 2012.

[RK04]

[RR13]

[RS05]

[RTA12]

81

[SF14]

[SG08]

[SH08]

[SJ03]

[SM14]

[Smi12]

[Sre04]

[SRJ04]

[SSU03]

[SSZ14]

[SEC13] M. Soltanolkotabi, E. Elhamifar, and E. Candes. Robust subspace clustering.

arXiv preprint arXiv:1301.2603, 2013.

D. L. Sun and C. F´evotte. Alternating direction method of multipliers for non-
negative matrix factorization with the beta-divergence. In IEEE International
Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2014.

A. Singh and G. Gordon. A uniﬁed view of matrix factorization models. In Ma-
chine Learning and Knowledge Discovery in Databases, pages 358–373. Springer,
2008.

H. Shen and J. Huang. Sparse principal component analysis via regularized low
rank matrix approximation. Journal of Multivariate Analysis, 99(6):1015–1034,
2008.

[SHK+14] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov.
Dropout: A simple way to prevent neural networks from overﬁtting. The Journal
of Machine Learning Research, 15(1):1929–1958, 2014.

N. Srebro and T. Jaakkola. Weighted low-rank approximations. In ICML, vol-
ume 3, pages 720–727, 2003.

V. Srikumar and C. Manning. Learning distributed representations for structured
output prediction. In Advances in Neural Information Processing Systems, pages
3266–3274, 2014.

R. Smith. Nuclear norm minimization methods for frequency domain subspace
identiﬁcation. In Proceedings of the 2010 American Control Conference (ACC),
pages 2689–2694. IEEE, 2012.

N. Srebro. Learning with Matrix Factorizations. PhD thesis, Massachusetts
Institute of Technology, 2004.

N. Srebro, J. Rennie, and T. Jaakkola. Maximum-margin matrix factorization.
In Advances in Neural Information Processing Systems, volume 17, pages 1329–
1336, 2004.

[SSGS11]

S. Shalev-Shwartz, A. Gonen, and O. Shamir. Large-scale convex minimization
with a low-rank constraint. arXiv preprint arXiv:1106.1622, 2011.

A. Schein, L. Saul, and L. Ungar. A generalized linear model for principal
component analysis of binary data. In Proceedings of the Ninth International
Workshop on Artiﬁcial Intelligence and Statistics, volume 38, page 46, 2003.

S. Schelter, V. Satuluri, and R. Zadeh. Factorbird — a parameter server ap-
proach to distributed matrix factorization. NIPS 2014 Workshop on Distributed
Machine Learning and Matrix Computations, 2014.

82

[Ste07]

[TB99]

[TG07]

[Tro04]

[Tse00]

H. Steck. Hinge rank loss and the area under the ROC curve. In J. N. Kok,
J. Koronacki, R. L. Mantaras, S. Matwin, D. Mladeniˇc, and A. Skowron, editors,
Machine Learning: ECML 2007, volume 4701 of Lecture Notes in Computer
Science, pages 347–358. Springer Berlin Heidelberg, 2007.

M. Tipping and C. Bishop. Probabilistic principal component analysis. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–
622, 1999.

J. Tropp and A. Gilbert. Signal recovery from random measurements via orthog-
onal matching pursuit. IEEE Transactions on Information Theory, 53(12):4655–
4666, 2007.

[TPB00]

N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.

J. Tropp. Topics in Sparse Approximation. PhD thesis, The University of Texas
at Austin, 2004.

P. Tseng. Nearest q-ﬂat to m points. Journal of Optimization Theory and
Applications, 105(1):249–252, 2000.

[Twe84] M. Tweedie. An index which distinguishes between some important exponen-
tial families. In Statistics: Applications and New Directions. Proceedings of the
Indian Statistical Institute Golden Jubilee International Conference, pages 579–
604, 1984.

[TYDL77] Y. Takane, F. Young, and J. De Leeuw. Nonmetric individual diﬀerences mul-
tidimensional scaling: an alternating least squares method with optimal scaling
features. Psychometrika, 42(1):7–67, 1977.

[UBG09] N. Usunier, D. Buﬀoni, and P. Gallinari. Ranking with ordered weighted pairwise
In Proceedings of the 26th annual International Conference on

classiﬁcation.
Machine Learning, pages 1057–1064. ACM, 2009.

[Vav09]

S. Vavasis. On the complexity of nonnegative matrix factorization. SIAM Journal
on Optimization, 20(3):1364–1377, 2009.

[VCLR13] V. Vu, J. Cho, J. Lei, and K. Rohe. Fantope projection and selection: A near-
optimal convex relaxation of sparse PCA. In C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information
Processing Systems 26, pages 2670–2678. Curran Associates, Inc., 2013.

[Vid10]

R. Vidal. A tutorial on subspace clustering. IEEE Signal Processing Magazine,
28(2):52–68, 2010.

83

[Vir07]

T. Virtanen. Monaural sound source separation by nonnegative matrix factor-
ization with temporal continuity and sparseness criteria. IEEE Transactions on
Audio, Speech, and Language Processing, 15(3):1066–1074, 2007.

[WBU10]

J. Weston, S. Bengio, and N. Usunier. Large scale image annotation: Learning to
rank with joint word-image embeddings. Machine Learning, 81(1):21–35, 2010.

[WGR+09] J. Wright, A. Ganesh, S. Rao, Y. Peng, and Y. Ma. Robust principal component
analysis: Exact recovery of corrupted low-rank matrices by convex optimization.
In Advances in Neural Information Processing Systems, volume 3, 2009.

[WTH09] D. Witten, R. Tibshirani, and T. Hastie. A penalized matrix decomposition, with
applications to sparse principal components and canonical correlation analysis.
Biostatistics, page kxp008, 2009.

[WYW13] J. Weston, H. Yee, and R. J. Weiss. Learning to rank recommendations with
the k-order statistic loss. In Proceedings of the 7th ACM Conference on Recom-
mender Systems, RecSys ’13, pages 245–248, New York, NY, USA, 2013. ACM.

[XCS12]

H. Xu, C. Caramanis, and S. Sanghavi. Robust PCA via outlier pursuit. IEEE
Transactions on Information Theory, 58(5):3047–3064, 2012.

[YDLT76] F. Young, J. De Leeuw, and Y. Takane. Regression with qualitative and quan-
titative variables: An alternating least squares method with optimal scaling
features. Psychometrika, 41(4):505–529, 1976.

[YYH+13] H. Yun, H.-F. Yu, C.-J. Hsieh, S. V. N. Vishwanathan, and I. Dhillon. NO-
MAD: Non-locking, stOchastic Multi-machine algorithm for Asynchronous and
Decentralized matrix completion. arXiv preprint arXiv:1312.0193, 2013.

[ZCF+10] M. Zaharia, M. Chowdhury, M. Franklin, S. Shenker, and I. Stoica. Spark: Clus-
ter computing with working sets. In Proceedings of the 2nd USENIX conference
on hot topics in cloud computing, page 10, 2010.

[ZHT06]

H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis.
Journal of Computational and Graphical Statistics, 15(2):265–286, 2006.

[ZV86]

W. Zwick and W. Velicer. Comparison of ﬁve rules for determining the number
of components to retain. Psychological bulletin, 99(3):432, 1986.

84

Generalized Low Rank Models

Madeleine Udell, Corinne Horn, Reza Zadeh, and Stephen Boyd

May 6, 2015. (Original version posted September 2014.)

Abstract

Principal components analysis (PCA) is a well-known technique for approximating
a tabular data set by a low rank matrix. Here, we extend the idea of PCA to handle
arbitrary data sets consisting of numerical, Boolean, categorical, ordinal, and other
data types. This framework encompasses many well known techniques in data analysis,
such as nonnegative matrix factorization, matrix completion, sparse and robust PCA,
k-means, k-SVD, and maximum margin matrix factorization. The method handles
heterogeneous data sets, and leads to coherent schemes for compressing, denoising,
and imputing missing entries across all data types simultaneously.
It also admits a
number of interesting interpretations of the low rank factors, which allow clustering of
examples or of features. We propose several parallel algorithms for ﬁtting generalized
low rank models, and describe implementations and numerical results.

This manuscript is a draft. Comments sent to udell@stanford.edu are welcome.

5
1
0
2
 
y
a
M
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
2
4
3
0
.
0
1
4
1
:
v
i
X
r
a

1

Contents

1 Introduction

1.1 Previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 PCA and quadratically regularized PCA

2.1 PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Quadratically regularized PCA . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Missing data and matrix completion . . . . . . . . . . . . . . . . . . . . . . .
Interpretations and applications . . . . . . . . . . . . . . . . . . . . . . . . .
2.5
2.6 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Generalized regularization

3.1 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Generalized loss functions

4.1 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Examples
4.3 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Loss functions for abstract data types

5.1 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Missing data and data imputation . . . . . . . . . . . . . . . . . . . . . . . .
5.4
Interpretations and applications . . . . . . . . . . . . . . . . . . . . . . . . .
5.5 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.6 Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Multi-dimensional loss functions

6.1 Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.2 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.3 Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7 Fitting low rank models

7.1 Alternating minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.2 Early stopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.3 Quadratic objectives
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.4 Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.5
Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.6 Global optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

4
5
7

8
8
9
9
12
13
15

15
16
17
21

22
22
22
25

26
26
27
29
30
32
32

38
39
42
42

42
44
45
48
48
49
52

8 Choosing low rank models

8.1 Regularization paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8.2 Choosing model parameters
. . . . . . . . . . . . . . . . . . . . . . . . . . .
8.3 On-line optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9 Implementations

9.1 Python implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.2 Julia implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.3 Spark implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A Quadratically regularized PCA

A.1 Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.2 Fixed points of alternating minimization . . . . . . . . . . . . . . . . . . . .

56
56
58
61

62
63
64
67

70
70
71

3

1

Introduction

In applications of machine learning and data mining, one frequently encounters large collec-
tions of high dimensional data organized into a table. Each row in the table represents an
example, and each column a feature or attribute. These tables may have columns of diﬀerent
(sometimes, non-numeric) types, and often have many missing entries.

For example, in medicine, the table might record patient attributes or lab tests: each row
of the table lists test or survey results for a particular patient, and each column corresponds
to a distinct test or survey question. The values in the table might be numerical (3.14),
Boolean (yes, no), ordinal (never, sometimes, always), or categorical (A, B, O). Tests not
administered or questions left blank result in missing entries in the data set. Other examples
abound:
in ﬁnance, the table might record known characteristics of companies or asset
classes; in social science settings, it might record survey responses; in marketing, it might
record known customer characteristics and purchase history.

Exploratory data analysis can be diﬃcult in this setting. To better understand a complex
data set, one would like to be able to visualize archetypical examples, to cluster examples,
to ﬁnd correlated features, to ﬁll in (impute) missing entries, and to remove (or simply
identify) spurious, anomalous, or noisy data points. This paper introduces a templated
method to enable these analyses even on large data sets with heterogeneous values and
with many missing entries. Our approach will be to embed both the rows (examples) and
columns (features) of the table into the same low dimensional vector space. These low
dimensional vectors can then be plotted, clustered, and used to impute missing entries or
identify anomalous ones.

If the data set consists only of numerical (real-valued) data, then a simple and well-
known technique to ﬁnd this embedding is Principal Components Analysis (PCA). PCA
ﬁnds a low rank matrix that minimizes the approximation error, in the least-squares sense,
to the original data set. A factorization of this low rank matrix embeds the original high
dimensional features into a low dimensional space. Extensions of PCA can handle missing
data values, and can be used to impute missing entries.

Here, we extend PCA to approximate an arbitrary data set by replacing the least-squares
error used in PCA with a loss function that is appropriate for the given data type. Another
extension beyond PCA is to add regularization on the low dimensional factors to impose or
encourage some structure, such as sparsity or nonnegativity, in the low dimensional factors.
In this paper we use the term generalized low rank model (GLRM) to refer to the problem
of approximating a data set as a product of two low dimensional factors by minimizing
an objective function. The objective will consist of a loss function on the approximation
error together with regularization of the low dimensional factors. With these extensions of
PCA, the resulting low rank representation of the data set still produces a low dimensional
embedding of the data set, as in PCA.

Many of the low rank modeling problems we must solve will be familiar. We recover an
optimization formulation of nonnegative matrix factorization, matrix completion, sparse and
robust PCA, k-means, k-SVD, and maximum margin matrix factorization, to name just a
few.

4

These low rank approximation problems are not convex, and in general cannot be solved
globally and eﬃciently. There are a few exceptional problems that are known to have con-
vex relaxations which are tight under certain conditions, and hence are eﬃciently (globally)
solvable under these conditions. However, all of these approximation problems can be heuris-
tically (locally) solved by methods that alternate between updating the two factors in the low
rank approximation. Each step involves either a convex problem, or a nonconvex problem
that is simple enough that we can solve it exactly. While these alternating methods need
not ﬁnd the globally best low rank approximation, they are often very useful and eﬀective
for the original data analysis problem.

1.1 Previous work

Uniﬁed views of matrix factorization. We are certainly not the ﬁrst to note that
matrix factorization algorithms may be viewed in a uniﬁed framework, parametrized by a
small number of modeling decisions. The ﬁrst instance we ﬁnd in the literature of this
uniﬁed view appeared in a paper by Collins, Dasgupta, and Schapire, [CDS01], extending
PCA to use loss functions derived from any probabilistic model in the exponential family.
Gordon’s Generalized2 Linear2 models [Gor02] extended the framework to loss functions
derived from the generalized Bregman divergence of any convex function, which includes
models such as Independent Components Analysis (ICA). Srebro’s 2004 PhD thesis [Sre04]
extended the framework to other loss functions, including hinge loss and KL-divergence loss,
and to other regularizers, including the nuclear norm and max-norm. Similarly, Chapter 8 in
Tropp’s 2004 PhD thesis [Tro04] explored a number of new regularizers, presenting a range
of clustering problems as matrix factorization problems with constraints, and anticipated
the k-SVD algorithm [AEB06]. Singh and Gordon [SG08] oﬀered a complete view of the
state of the literature on matrix factorization in Table 1 of their 2008 paper, and noted that
by changing the loss function and regularizer, one may recover algorithms including PCA,
weighted PCA, k-means, k-medians, (cid:96)1 SVD, probabilistic latent semantic indexing (pLSI),
nonnegative matrix factorization with (cid:96)2 or KL-divergence loss, exponential family PCA,
and MMMF. Witten et al. introduced the statistics community to sparsity-inducing matrix
factorization in a 2009 paper on penalized matrix decomposition, with applications to sparse
PCA and canonical correlation analysis [WTH09]. Recently, Markovsky’s monograph on low
rank approximation [Mar12] reviewed some of this literature, with a focus on applications
in system, control, and signal processing. The GLRMs discussed in this paper include all of
these models, and many more.

Heterogeneous data. Many authors have proposed the use of low rank models as a
tool for integrating heterogeneous data. The earliest example of this approach is canonical
correlation analysis, developed by Hotelling [Hot36] in 1936 to understand the relations
between two sets of variates in terms of the eigenvectors of their covariance matrix. This
approach was extended by Witten et al. [WTH09] to encourage structured (e.g., sparse)
In the 1970s, De Leeuw et al. proposed the use of low rank models to ﬁt data
factors.

5

measured in nominal, ordinal and cardinal levels [DLYT76]. More recently, Goldberg et
al. [GRX+10] used a low rank model to perform transduction (i.e., multi-label learning)
in the presence of missing data by ﬁtting a low rank model to the features and the labels
simultaneously. Low rank models have also been used to embed image, text and video data
into a common low dimensional space [GD14], and have recently come into vogue in the
natural language processing community as a means to embed words and documents into a
low dimensional vector space [MCCD13, MSC+13, PSM14, SM14].

Algorithms.
In general, it can be computationally hard to ﬁnd the global optimum of a
generalized low rank model. For example, it is NP-hard to compute an exact solution to k-
means [DFK+04], nonnegative matrix factorization [Vav09], and weighted PCA and matrix
completion [GG11], all of which are special cases of low rank models.

However, there are many (eﬃcient) ways to go about ﬁtting a low rank model, by which
we mean ﬁnding a good model with a small objective value. The resulting model may or may
not be the global solution of the low rank optimization problem. We distinguish a model ﬁt
in this way from the solution to an optimization problem, which always refers to the global
solution.

The matrix factorization literature presents a wide variety of methods to ﬁt low rank
models in a variety of special cases. For example, there are variants on alternating min-
imization (with alternating least squares as a special case) [DLYT76, YDLT76, TYDL77,
DL84, DLM09], alternating Newton methods [Gor02, SG08], (stochastic or incremental)
gradient descent [KO09, LRS+10, NRRW11, RRWN11, BRRT12, YYH+13, RR13], conju-
gate gradients [RS05, SJ03], expectation minimization (EM) (or “soft-impute”) methods
[TB99, SJ03, MHT10, HMLZ14], multiplicative updates [LS99], and convex relaxations to
semideﬁnite programs [SRJ04, FHB04, RFP10, FM13].

Generally, expectation minimization, which proceeds by iteratively imputing missing en-
tries in the matrix and solving the fully observed problem, has been found to underperform
relative to other methods [SG08]. However, when used in conjunction with computational
tricks exploiting a particular problem structure, such as Gram matrix caching, these methods
can still work extremely well [HMLZ14].

Semideﬁnite programming becomes computationally intractable for very large (or even
just large) scale problems [RS05]. However, a theoretical analysis of optimality conditions for
rank-constrainted semideﬁnite programs [BM03c] has led to a few algorithms for semideﬁnite
programming based on matrix factorization [BM03b, ABEV09, JBAS10] which guarantee
global optimality and converge quickly if the global solution to the problem is exactly low
rank. Fast approximation algorithms for rank-constrained semideﬁnite programs have also
been developed [SSGS11].

Recently, there has been a resurgence of interest in methods based on alternating min-
imization, as numerous authors have shown that alternating minimization (suitably initial-
ized, and under a few technical assumptions) provably converges to the global minimum
for a range of problems including matrix completion [Kes12, JNS13, Har13], robust PCA
[NNS+14], and dictionary learning [AAJN13].

6

Gradient descent methods are often preferred for extremely large scale problems since
these methods parallelize naturally in both shared memory and distributed memory archi-
tectures. See [RR13, YYH+13] and references therein for some recent innovative approaches
to speeding up stochastic gradient descent for matrix factorization by eliminating locking
and reducing interprocess communication.

Contributions. The present paper diﬀers from previous work in a number of ways. We
are consistently concerned with the meaning of applying these diﬀerent loss functions and
regularizers to approximate a data set. The generality of our view allows us to introduce a
number of loss functions and regularizers that have not previously been considered. More-
over, our perspective enables us to extend these ideas to arbitrary data sets, rather than just
matrices of real numbers.

A number of new considerations emerge when considering the problem so broadly. First,
we must face the problem of comparing approximation errors across data of diﬀerent types.
For example, we must choose a scaling to trade oﬀ the loss due to a misclassiﬁcation of a
categorical value with an error of 0.1 (say) in predicting a real value.

Second, we require algorithms that can handle the full gamut of losses and regulariz-
ers, which may be smooth or nonsmooth, ﬁnite or inﬁnite valued, with arbitrary domain.
This work is the ﬁrst to consider these problems in such generality, and therefore also the
ﬁrst to wrestle with the algorithmic consequences. Below, we give a number of algorithms
appropriate for this setting, including many that have not been previously proposed in the
literature. Our algorithms are all based on alternating minimization and variations on al-
ternating minimization that are more suitable for large scale data and can take advantage
of parallel computing resources.

Finally, we present some new results on some old problems. For example, in Appendix A,
we derive a formula for the solution to quadratically regularized PCA, and show that quadrat-
ically regularized PCA has no local nonglobal minima; and in §7.6 we show how to certify
(in some special cases) that a model is a global solution of a GLRM.

1.2 Organization

The organization of this paper is as follows. In §2 we ﬁrst recall some properties of PCA
and its common variations to familiarize the reader with our notation. We then generalize
the regularization on the low dimensional factors in §3, and the loss function on the ap-
proximation error in §4. Returning to the setting of heterogeneous data, we extend these
dimensionality reduction techniques to abstract data types in §5 and to multi-dimensional
loss functions in §6. Finally, we address algorithms for ﬁtting GLRMs in §7, discuss a few
practical considerations in choosing a GLRM for a particular problem in §8, and describe
some implementations of the algorithms that we have developed in §9.

7

2 PCA and quadratically regularized PCA

In this section, we let A ∈ Rm×n be a data matrix consisting of m examples
Data matrix.
each with n numerical features. Thus Aij ∈ R is the value of the jth feature in the ith
example, the ith row of A is the vector of n feature values for the ith example, and the jth
column of A is the vector of the jth feature across our set of m examples.

It is common to represent other data types in a numerical matrix using certain canonical
encoding tricks. For example, Boolean data is often encoded as 1 (for true) and -1 (for
false), ordinal data is often encoded using consecutive integers to represent the consecutive
levels of the variable, and categorical data is often encoded by creating a column for each
possible value of the categorical variable, and representing the data using a 1 in the column
corresponding to the observed value, and -1 or 0 in all other columns. We will see more
systematic and principled ways to deal with these data types, and others, in §4–6. For now,
we assume the entries in the data matrix consist of real numbers.

2.1 PCA

solving

Principal components analysis (PCA) is one of the oldest and most widely used tools in data
analysis [Pea01, Hot33, Jol86]. We review some of its well-known properties here in order to
set notation and as a warm-up to the variants presented later.

PCA seeks the best rank-k approximation to the matrix A in the least-squares sense, by

minimize
subject to Rank(Z) ≤ k,
with variable Z ∈ Rm×n. Here, (cid:107) · (cid:107)F is the Frobenius norm of a matrix, i.e., the square root
of the sum of the squares of the entries.

(cid:107)A − Z(cid:107)2
F

(1)

The rank constraint can be encoded implicitly by expressing Z in factored form as Z =

XY , with X ∈ Rm×k, Y ∈ Rk×n. Then the PCA problem can be expressed as

minimize (cid:107)A − XY (cid:107)2
F

(2)

with variables X ∈ Rm×k and Y ∈ Rk×n. (The factorization of Z is of course not unique.)
Deﬁne xi ∈ R1×n to be the ith row of X, and yj ∈ Rm to be the jth column of Y . Thus
xiyj = (XY )ij ∈ R denotes a dot or inner product. (We will use this notation throughout
the paper.) Using this deﬁnition, we can rewrite the objective in problem (2) as
n
(cid:88)

m
(cid:88)

(Aij − xiyj)2.

i=1

j=1

We will give several interpretations of the low rank factorization (X, Y ) solving (2) in
§2.5. But for now, we note that (2) can interpreted as a method for compressing the n
features in the original data set to k < n new features. The row vector xi is associated with
example i; we can think of it as a feature vector for the example using the compressed set
of k < n features. The column vector yj is associated with the original feature j; it can be
interpreted as mapping the k new features onto the original feature j.

8

2.2 Quadratically regularized PCA

We can add quadratic regularization on X and Y to the objective. The quadratically regu-
larized PCA problem is

minimize (cid:80)m

(cid:80)n

j=1(Aij − xiyj)2 + γ (cid:80)m

i=1 (cid:107)xi(cid:107)2

2 + γ (cid:80)n

j=1 (cid:107)yj(cid:107)2
2,

i=1

with variables X ∈ Rm×k and Y ∈ Rk×n, and regularization parameter γ ≥ 0. Problem (3)
can be written more concisely in matrix form as

minimize (cid:107)A − XY (cid:107)2

F + γ(cid:107)X(cid:107)2

F + γ(cid:107)Y (cid:107)2
F .

When γ = 0, the problem reduces to the PCA problem (2).

2.3 Solution methods

Singular value decomposition.
It is well known that a solution to (2) can be obtained
by truncating the singular value decomposition (SVD) of A [EY36]. The (compact) SVD
of A is given by A = U ΣV T , where U ∈ Rm×r and V ∈ Rn×r have orthonormal columns,
and Σ = diag(σ1, . . . , σr) ∈ Rr×r, with σ1 ≥ · · · ≥ σr > 0 and r = Rank(A). The columns
of U = [u1 · · · ur] and V = [v1 · · · vr] are called the left and right singular vectors of A,
respectively, and σ1, . . . , σr are called the singular values of A.

Using the orthogonal invariance of the Frobenius norm, we can rewrite the objective in

problem (1) as

(cid:107)A − XY (cid:107)2

F = (cid:107)Σ − U T XY V (cid:107)2
F .

That is, we would like to ﬁnd a matrix U T XY V of rank no more than k approximating the
diagonal matrix Σ. It is easy to see that there is no better rank k approximation for Σ than
Σk = diag(σ1, . . . , σk, 0, . . . , 0) ∈ Rr×r. Here we have truncated the SVD to keep only the
top k singular values. We can achieve this approximation by choosing U T XY V = Σk, or
(using the orthogonality of U and V ) XY = U ΣkV T . For example, deﬁne

and let

Uk = [u1 · · · uk],

Vk = [v1 · · · vk],

X = UkΣ1/2
k ,

Y = Σ1/2

k V T
k .

The solution to (3) is clearly not unique: if X, Y is a solution, then so is XG, G−1Y for any
invertible matrix G ∈ Rk×k. When σk > σk+1, all solutions to the PCA problem have this
form. In particular, letting G = tI and taking t → ∞, we see that the solution set of the
PCA problem is unbounded.

It is less well known that a solution to the quadratically regularized PCA problem can
be obtained in the same way. (Proofs for the statements below can be found in Appendix
A.) Deﬁne Uk and Vk as above, and let ˜Σk = diag((σ1 − γ)+, . . . , (σk − γ)+), where (a)+ =
max(a, 0). Here we have both truncated the SVD to keep only the top k singular values, and

(3)

(4)

(5)

(6)

9

performed soft-thresholding on the singular values to reduce their values by γ. A solution to
the quadratically regularized PCA problem (3) is then given by

X = Uk

˜Σ1/2
k ,

Y = ˜Σ1/2

k V T
k .

(7)

For γ = 0, the solution reduces to the familiar solution to PCA (2) obtained by truncating
the SVD to the top k singular values.

The set of solutions to problem (3) is signiﬁcantly smaller than that of problem (2),
although solutions are still not unique: if X, Y is a solution, then so is XT , T −1Y for any
orthogonal matrix T ∈ Rk×k. When σk > σk+1, all solutions to (3) have this form.
In
particular, adding quadratic regularization results in a solution set that is bounded.

The quadratically regularized PCA problem (3) (including the PCA problem as a special
case) is the only problem we will encounter for which an analytical solution exists. The
analytical tractability of PCA explains its popularity as a technique for data analysis in
the era before computers were machines. For example, in his 1933 paper on PCA [Hot33],
Hotelling computes the solution to his problem using power iteration to ﬁnd the eigenvalue
decomposition of the matrix AT A = V Σ2V T , and records in the appendix to his paper the
itermediate results at each of the (three) iterations required for the method to converge.

Alternating minimization. Here we mention a second method for solving (3), which
extends more readily to the extensions of PCA that we discuss below. The alternating
minimization algorithm simply alternates between minimizing the objective over the variable
X, holding Y ﬁxed, and then minimizing over Y , holding X ﬁxed. With an initial guess for
the factors Y 0, we repeat the iteration
(cid:32) m
(cid:88)

n
(cid:88)

m
(cid:88)

(cid:33)

X l = argmin

(Aij − xiyl−1

)2 + γ

j

(cid:107)xi(cid:107)2
2

Y l = argmin

(Aij − xl

iyj)ij)2 + γ

(cid:107)yj(cid:107)2
2

X

Y

i=1
(cid:32) m
(cid:88)

j=1

n
(cid:88)

i=1

j=1

(cid:33)

i=1

n
(cid:88)

j=1

for l = 1, . . . until a stopping condition is satisﬁed. (If X and Y are full rank, or γ > 0, the
minimizers above are unique; when they are not, we can take any minimizer.) The objective
function is nonincreasing at each iteration, and therefore bounded. This implies, for γ > 0,
that the iterates X l and Y l are bounded.

This algorithm does not always work. In particular, it has stationary points that are not
solutions of problem (3). In particular, if the rows of Y l lie in a subspace spanned by a subset
of the (right) singular vectors of A, then the columns of X l+1 will lie in a subspace spanned
by the corresponding left singular vectors of A, and vice versa. Thus, if the algorithm is
initialized with Y 0 orthogonal to any of the top k (right) singular vectors, then the algorithm
(implemented in exact arithmetic) will not converge to the global solution to the problem.
But all stable stationary points of the iteration are solutions (see Appendix A). So as
a practical matter, the alternating minimization method always works, i.e., the objective
converges to the optimal value.

10

Parallelizing alternating minimization. Alternating minimization parallelizes easily
over examples and features. The problem of minimizing over X splits into m independent
minimization problems. We can solve the simple quadratic problems

minimize (cid:80)n

j=1(Aij − xiyj)2 + γ(cid:107)xi(cid:107)2
2

with variable xi, in parallel, for i = 1, . . . , m. Similarly, the problem of minimizing over Y
splits into n independent quadratic problems,

minimize (cid:80)m

i=1(Aij − xiyj)2 + γ(cid:107)yj(cid:107)2
2

with variable yj, which can be solved in parallel for j = 1, . . . , n.

(8)

(9)

Caching factorizations. We can speed up the solution of the quadratic problems using
a simple factorization caching technique.

For ease of exposition, we assume here that X and Y have full rank k. The updates (8)

and (9) can be expressed as

X = AY T (Y Y T + γI)−1,

Y = (X T X + γI)−1X T A.

We show below how to eﬃciently compute X = AY T (Y Y T + γI)−1; the Y update admits a
similar speedup using the same ideas. We assume here that k is modest, say, not more than
a few hundred or a few thousand. (Typical values used in applications are often far smaller,
on the order of tens.) The dimensions m and n, however, can be very large.

First compute the Gram matrix G = Y Y T using an outer product expansion

This sum can be computed on-line by streaming over the index j, or in parallel, split over
the index j. This property allows us to scale up to extremely large problems even if we
cannot store the entire matrix Y in memory. The computation of the Gram matrix requires
2k2n ﬂoating point operations (ﬂops), but is trivially parallelizable: with r workers, we can
expect a speedup on the order of r. We next add the diagonal matrix γI to G in k ﬂops,
and form the Cholesky factorization of G + γI in k3/3 ﬂops and cache the factorization.

In parallel over the rows of A, we compute D = AY T (2kn ﬂops per row), and use the
factorization of G + γI to compute D(G + γI)−1 with two triangular solves (2k2 ﬂops per
row). These computations are also trivially parallelizable: with r workers, we can expect a
speedup on the order of r.

Hence the total time required for each update with r workers scales as O( k2(m+n)+kmn

).

For k small compared to m and n, the time is dominated by the computation of AY T .

r

G =

yjyT
j .

n
(cid:88)

j=1

11

2.4 Missing data and matrix completion

Suppose we observe only entries Aij for (i, j) ∈ Ω ⊂ {1, . . . , m} × {1, . . . , n} from the matrix
A, so the other entries are unknown. Then to ﬁnd a low rank matrix that ﬁts the data well,
we solve the problem

minimize (cid:80)

(i,j)∈Ω(Aij − xiyj)2 + γ(cid:107)X(cid:107)2

F + γ(cid:107)Y (cid:107)2
F ,

(10)

with variables X and Y , with γ > 0. A solution of this problem gives an estimate ˆAij = xiyj
for the value of those entries (i, j) (cid:54)∈ Ω that were not observed. In some applications, this
data imputation (i.e., guessing entries of a matrix that are not known) is the main point.
There are two very diﬀerent regimes in which solving the problem (10) may be useful.

Imputing missing entries to borrow strength. Consider a matrix A in which very few
entries are missing. The typical approach in data analysis is to simply remove any rows with
missing entries from the matrix and exclude them from subsequent analysis. If instead we
solve the problem above without removing these aﬀected rows, we “borrow strength” from
the entries that are not missing to improve our global understanding of the data matrix
A. In this regime we are imputing the (few) missing entries of A, using the examples that
ordinarily we would discard.

Low rank matrix completion. Now consider a matrix A in which most entries are
missing, i.e., we only observe relatively few of the mn elements of A, so that by discarding
every example with a missing feature or every feature with a missing example, we would
discard the entire matrix. Then the solution to (10) becomes even more interesting: we are
guessing all the entries of a (presumed low rank) matrix, given just a few of them. It is a
surprising fact that this is possible: typical results from the matrix completion literature show
that one can recover an unknown m×n matrix A of low rank r from just about nr log2 n noisy
samples Ω with an error that is proportional to the noise level [CR08, CT10, RFP10, CP09],
so long as the matrix A satisﬁes a certain incoherence condition and the samples Ω are chosen
uniformly at random. These works use an estimator that minimizes a nuclear norm penalty
along with a data ﬁtting term to encourage low rank structure in the solution.

The argument in §7.6 shows that problem (10) is equivalent to the rank-constrained

nuclear-norm regularized convex problem

minimize (cid:80)
subject to Rank(Z) ≤ k,

(i,j)∈Ω(Aij − Zij)2 + 2γ(cid:107)Z(cid:107)∗

where the nuclear norm (cid:107)Z(cid:107)∗ (also known as the trace norm) is deﬁned to be the sum of the
singular values of Z. Thus, the solutions to problem (10) correspond exactly to the solutions
of these proposed estimators so long as the rank k of the model is chosen to be larger than
the true rank r of the matrix A. Nuclear norm regularization is often used to encourage
solutions of rank less than k, and has applications ranging from graph embedding to linear
system identiﬁcation [FHB04, LV09, MF10, Smi12, Osn14].

12

Low rank matrix completion problems arise in applications like predicting customer rat-
ings or customer (potential) purchases. Here the matrix consists of the ratings or numbers
of purchases that m customers give (or make) for each of n products. The vast majority of
the entries in this matrix are missing, since a customer will rate (or purchase) only a small
fraction of the total number of products available. In this application, imputing a missing
entry of the matrix as xiyj, for (i, j) (cid:54)∈ Ω, is guessing what rating a customer would give a
product, if she were to rate it. This can used as the basis for a recommendation system, or
a marketing plan.

Alternating minimization. When Ω (cid:54)= {1, . . . , m} × {1, . . . , n}, the problem (10) has no
known analytical solution, but it is still easy to ﬁt a model using alternating minimization.
Algorithms based on alternating minimization have been shown to converge quickly (even
geometrically [JNS13]) to a global solution satisfying a recovery guarantee when the initial
values of X and Y are chosen carefully [KMO09, KMO10, KM10, JNS13, Har13, GAGG13].
On the other hand, all of these analytical results rely on using a fresh batch of samples
Ω for each iteration of alternating minimization; none uses the quadratic regularizer above
that corresponds to the nuclear norm penalized estimator; and interestingly, Hardt [Har13]
notes that none achieves the same sample complexity guarantees found in the convex ma-
trix completion literature which, unlike the alternating minimization guarantees, match the
information theoretic lower bound [CT10] up to logarithmic factors. For these reasons, it
is plausible to expect that in practice using alternating minimization to solve problem (10)
might yield a better solution than the “alternating minimization” algorithms presented in
the literature on matrix completion when suitably initialized (for example, using the method
proposed below in §7.5). However, in general the method should be considered a heuristic.

2.5 Interpretations and applications

The recovered matrices X and Y in the quadratically regularized PCA problems (3) and
(10) admit a number of interesting interpretations. We introduce some of these interpreta-
tions now; the terminology we use here will recur throughout the paper. Of course these
interpretations are related to each other, and not distinct.

Feature compression. Quadratically regularized PCA (3) can be interpreted as a method
for compressing the n features in the original data set to k < n new features. The row vector
xi is associated with example i; we can think of it as a feature vector for the example using
the compressed set of k < n features. The column vector yj is associated with the original
feature j; it can be interpreted as the mapping from the original feature j into the k new
features.

Low-dimensional geometric embedding. We can think of each yj as associating feature
j with a point in a low (k-) dimensional space. Similarly, each xi associates example i with
a point in the low dimensional space. We can use these low dimensional vectors to judge

13

which features (or examples) are similar. For example, we can run a clustering algorithm on
the low dimensional vectors yj (or xi) to ﬁnd groups of similar features (or examples).

Archetypes. We can think of each row of Y as an archetype which captures the behavior
of one of k idealized and maximally informative examples. These archetypes might also
be called proﬁles, factors, or atoms. Every example i = 1, . . . , m is then represented (ap-
proximately) as a linear combination of these archetypes, with the row vector xi giving the
coeﬃcients. The coeﬃcient xil gives the resemblance or loading of example i to the lth
archetype.

Archetypical representations. We call xi the representation of example i in terms of the
archetypes. The rows of X give an embedding of the examples into Rk, where each coordinate
axis corresponds to a diﬀerent archetype.
If the archetypes are simple to understand or
interpret, then the representation of an example can provide better intuition about that
example.

The examples can be clustered according to their representations in order to determine
a group of similar examples.
Indeed, one might choose to apply any machine learning
algorithm to the representations xi rather than to the initial data matrix: in contrast to the
initial data, which may consist of high dimensional vectors with noisy or missing entries, the
representations xi will be low dimensional, less noisy, and complete.

Feature representations. The columns of Y embed the features into Rk. Here, we
think of the columns of X as archetypical features, and represent each feature j as a linear
combination of the archetypical features. Just as with the examples, we might choose to
apply any machine learning algorithm to the feature representations. For example, we might
ﬁnd clusters of similar features that represent redundant measurements.

Latent variables. Each row of X represents an example by a vector in Rk. The matrix
Y maps these representations back into Rm. We might think of X as discovering the latent
variables that best explain the observed data. If the approximation error (cid:80)
(i,j)∈Ω(Aij −xiyj)2
is small, then we view these latent variables as providing a good explanation or summary of
the full data set.

Probabilistic intepretation. We can give a probabilistic interpretation of X and Y ,
building on the probabilistic model of PCA developed by Tipping and Bishop [TB99]. We
suppose that the matrices ¯X and ¯Y have entries which are generated by taking independent
samples from a normal distribution with mean 0 and variance γ−1 for γ > 0. The entries in
the matrix ¯X ¯Y are observed with noise ηij ∈ R,

where the noise η in the (i, j)th entry is sampled independently from a standard normal
distribution. We observe each entry (i, j) ∈ Ω. Then to ﬁnd the maximum a posteriori

Aij = ( ¯X ¯Y )ij + ηij,

14

(MAP) estimator (X, Y ) of ( ¯X, ¯Y ), we solve
(cid:1) exp (cid:0)− γ

maximize exp (cid:0)− γ

2 (cid:107) ¯X(cid:107)2

F

which is equivalent, by taking logs, to (3).

2 (cid:107) ¯Y (cid:107)2

F

(cid:1) (cid:81)

(i,j)∈Ω exp (−(Aij − xiyj)2) ,

This interpretation explains the recommendation we gave above for imputing missing
observations (i, j) (cid:54)∈ Ω. We simply use the MAP estimator xiyj to estimate the missing
entry ( ¯X ¯Y )ij. Similarly, we can interpret (XY )ij for (i, j) ∈ Ω as a denoised version of the
observation Aij.

Auto-encoder. The matrix X encodes the data; the matrix Y decodes it back into the
full space. We can view PCA as providing the best linear auto-encoder for the data; among
all (bi-linear) low rank encodings (X) and decodings (Y ) of the data, PCA minimizes the
squared reconstruction error.

Compression. We impose an information bottleneck [TPB00] on the data by using a low
rank auto-encoder to ﬁt the data. PCA ﬁnds X and Y to maximize the information trans-
mitted through this k-dimensional information bottleneck. We can interpret the solution
as a compressed representation of the data, and use it to eﬃciently store or transmit the
information present in the original data.

2.6 Oﬀsets and scaling

For good practical performance of a generalized low rank model, it is critical to ensure that
model assumptions match the data. We saw above in §2.5 that quadratically regularized
PCA corresponds to a model in which features are observed with N (0, 1) errors. If instead
each column j of XY is observed with N (µj, σ2
j ) errors, our model is no longer unbiased,
and may ﬁt very poorly, particularly if some of the column means µj are large.

For this reason it is standard practice to standardize the data before appplying PCA or
quadratically regularized PCA: the column means are subtracted from each column, and the
columns are normalized by their variances. (This can be done approximately; there is no
need to get the scaling and oﬀset exactly right.) Formally, deﬁne nj = |{i : (i, j) ∈ Ω}|, and
let

µj =

1
nj

(cid:88)

Aij,

(i,j)∈Ω

σ2
j =

1
nj − 1

(cid:88)

(i,j)∈Ω

(Aij − µj)2

estimate the mean and variance of each column of the data matrix. PCA or quadratically
regularized PCA is then applied to the matrix whose (i, j) entry is (Aij − µj)/σj.

3 Generalized regularization

It is easy to see how to extend PCA to allow arbitrary regularization on the rows of X and
columns of Y . We form the regularized PCA problem
(i,j)∈Ω(Aij − xiyj)2 + (cid:80)m

i=1 ri(xi) + (cid:80)n

minimize (cid:80)

j=1 ˜rj(yj),

(11)

15

with variables xi and yj, with given regularizers ri : Rk → R ∪ {∞} and ˜rj : Rk → R ∪ {∞}
for i = 1, . . . , n and j = 1, . . . , m. Regularized PCA (11) reduces to quadratically regularized
PCA (3) when ri = γ(cid:107) · (cid:107)2

2. We do not restrict the regularizers to be convex.

2, ˜rj = γ(cid:107) · (cid:107)2

The objective in problem (11) can be expressed compactly in matrix notation as

(cid:107)A − XY (cid:107)2
i=1 r(xi) and ˜r(Y ) = (cid:80)n

F + r(X) + ˜r(Y ),

where r(X) = (cid:80)n
separable across the rows of X, and the columns of Y , respectively.

j=1 ˜r(yj). The regularization functions r and ˜r are

Inﬁnite values of ri and ˜rj are used to enforce constraints on the values of X and Y . For

example, the regularizer

ri(x) =

(cid:26) 0

x ≥ 0
∞ otherwise,

the indicator function of the nonnegative orthant, imposes the constraint that xi be nonneg-
ative.

Solutions to (11) need not be unique, depending on the choice of regularizers. If X and
Y are a solution, then so are XT and T −1Y , where T is any nonsingular matrix that satisﬁes
r(U T ) = r(U ) for all U and ˜r(T −1V ) = r(V ) for all V .

By varying our choice of regularizers r and ˜r, we are able to represent a wide range of
known models, as well as many new ones. We will discuss a number of choices for regularizers
below, but turn now to methods for solving the regularized PCA problem (11).

3.1 Solution methods

In general, there is no analytical solution for (11). The problem is not convex, even when r
and ˜r are convex. However, when r and ˜r are convex, the problem is bi-convex: it is convex
in X when Y is ﬁxed, and convex in Y when X is ﬁxed.

Alternating minimization. There is no reason to believe that alternating minimization
will always converge to the global minimum of the regularized PCA problem (11). Indeed,
we will see many cases below in which the problem is known to have many local minima.
However, alternating minimization can still be applied in this setting, and it still parallelizes
over the rows of X and columns of Y . To minimize over X, we solve, in parallel,

with variable xi, for i = 1, . . . , m. Similarly, to minimize over Y , we solve, in parallel,

minimize (cid:80)

j:(i,j)∈Ω(Aij − xiyj)2 + ri(xi)

minimize (cid:80)

i:(i,j)∈Ω(Aij − xiyj)2 + ˜rj(yj)

(12)

(13)

with variable yj, for j = 1, . . . , n.

When the regularizers are convex, these problems are convex. When the regularizers
are not convex, there are still many cases in which we can ﬁnd analytical solutions to the
nonconvex subproblems (12) and (13), as we will see below. A number of concrete algorithms,
in which these subproblems are solved explicitly, are given in §7.

16

Caching factorizations. Often, the X and Y updates (12) and (13) reduce to convex
quadratic programs. For example, this is the case for nonnegative matrix factorization,
sparse PCA, and quadratic mixtures (which we deﬁne and discuss below in §3.2). The same
factorization caching of the Gram matrix that was described above in the case of PCA can
be used here to speed up the solution of these updates. Variations on this idea are described
in detail in §7.3.

3.2 Examples

Here and throughout the paper, we present a set of examples chosen for pedagogical clarity,
In all of the examples below, γ > 0 is a parameter that controls
not for completeness.
the strength of the regularization, and we drop the subscripts from r (or ˜r) to lighten the
notation. Of course, it is possible to mix and match these regularizers, i.e., to choose diﬀerent
ri for diﬀerent i, and choose diﬀerent ˜rj for diﬀerent j.

Nonnegative matrix factorization (NNMF). Consider the regularized PCA problem
(11) with r = I+ and ˜r = I+, where I+ is the indicator function of the nonnegative reals.
(Here, and throughout the paper, we deﬁne the indicator function of a set C, to be 0 when
its argument is in C and ∞ otherwise.) Then problem (11) is NNMF: a solution gives the
matrix best approximating A that has a nonnegative factorization (i.e., a factorization into
elementwise nonnegative matrices) [LS99]. It is NP-hard to solve NNMF problems exactly
[Vav09]. However, these problems have a rich analytical structure which can sometimes
be exploited [Gil11, BRRT12, DS14], and a wide range of uses in practice [LS99, SBPP06,
BBL+07, Vir07, KP07, FBD09]. Hence a number of specialized algorithms and codes for
ﬁtting NNMF models are available [LS01, Lin07, KP08a, KP08b, BDKP14, KHP14, KP11].
We can also replace the nonnegativity constraint with any interval constraint. For ex-
ample, r and ˜r can be 0 if all entries of X and Y , respectively, are between 0 and 1, and
inﬁnite otherwise.

Sparse PCA.
If very few of the coeﬃcients of X and Y are nonzero, it can be easier to
interpret the archetypes and representations. We can understand each archetype using only
a small number of features, and can understand each example as a combination of only a
small number of archetypes. To get a sparse version of PCA, we use a sparsifying penalty
as the regularization. Many variants on this basic idea have been proposed, together with a
wide variety of algorithms [dEGJL04, ZHT06, SH08, Mac09, WTH09, RTA12, VCLR13].

For example, we could enforce that no entry Aij depend on more than s columns of X

or of Y by setting r to be the indicator function of a s-sparse vector, i.e.,

and deﬁning ˜r(y) similarly, where card(x) denotes the cardinality (number of nonzero en-
tries) in the vector x. The updates (12) and (13) are not convex using this regularizer, but

r(x) =

(cid:26) 0

card(x) ≤ s

∞ otherwise,

17

one can ﬁnd approximate solutions using a pursuit algorithm (see, e.g., [CDS98, TG07]), or
exact solutions (for small s) using the branch and bound method [LW66, BM03a].

As a simple example, consider s = 1. Here we insist that each xi have at most one
nonzero entry, which means that each example is a multiple of one of the rows of Y . The
X-update is easy to carry out, by evaluating the best quadratic ﬁt of xi with each of the k
rows of Y . This reduces to choosing the row of Y that has the smallest angle to the ith row
of A.

The s-sparse regularization can be relaxed to a convex, but still sparsifying, regularization
using r(x) = (cid:107)x(cid:107)1, ˜r(y) = (cid:107)y(cid:107)1 [ZHT06]. In this case, the X-update reduces to solving a
(small) (cid:96)1-regularized least-squares problem.

Orthogonal nonnegative matrix factorization. One well known property of PCA is
that the principal components obtained (i.e., the columns of X and rows of Y ) can be chosen
to be orthogonal, so X T X and Y Y T are both diagonal. We can impose the same condition
on a nonnegative matrix factorization. Due to nonnegativity of the matrix, two columns
of X cannot be orthogonal if they both have a nonzero in the same row. Conversely, if X
has only one nonzero per row, then its columns are mutually orthogonal. So an orthogonal
nonnegative matrix factorization is identical to to a nonnegativity condition in addition to
the 1-sparse condition described above. Orthogonal nonnegative matrix factorization can be
achieved by using the regularizer

(cid:26) 0

r(x) =

card(x) = 1,

x ≥ 0

∞ otherwise,

and letting ˜r(y) be the indicator of the nonnegative orthant, as in NNMF.

Geometrically, we can interpret this problem as modeling the data A as a union of rays.
Each row of Y , interpreted as a point in Rn, deﬁnes a ray from the origin passing through
that point. Orthogonal nonnegative matrix factorization models each row of X as a point
along one of these rays.

Some authors [DLPP06] have also considered how to obtain a bi-orthogonal nonnegative
matrix factorization, in which both X and Y T have orthogonal columns. By the same
argument as above, we see this is equivalent to requiring both X and Y T to have only one
positive entry per row, with the other entries equal to 0.

Max-norm matrix factorization. We take r = ˜r = φ with

This penalty enforces that

φ(x) =

(cid:26) 0

(cid:107)x(cid:107)2
2 ≤ µ
∞ otherwise.

(cid:107)X(cid:107)2

2,∞ ≤ µ,

(cid:107)Y T (cid:107)2

2,∞ ≤ µ,

18

where the (2, ∞) norm of a matrix X with rows xi is deﬁned as maxi (cid:107)xi(cid:107)2. This is equivalent
to requiring the max-norm (sometimes called the γ2-norm) of Z = XY , which is deﬁned as

(cid:107)Z(cid:107)max = inf{(cid:107)X(cid:107)2,∞(cid:107)Y T (cid:107)2,∞ : XY = Z},

to be bounded by µ. This penalty has been proposed by [LRS+10] as a heuristic for low rank
matrix completion, which can perform better than Frobenius norm regularization when the
low rank factors are known to have bounded entries.

Quadratic clustering. Consider (11) with ˜r = 0. Let r be the indicator function of a
selection, i.e.,

(cid:26) 0

r(x) =

∞ otherwise,

x = el for some l ∈ {1, . . . , k}

where el is the l-th standard basis vector. Thus xi encodes the cluster (one of k) to which
the data vector (Ai1, . . . , Aim) is assigned.

Alternating minimization on this problem reproduces the well-known k-means algorithm
(also known as Lloyd’s algorithm) [Llo82]. The y update (13) is a least squares problem with
the simple solution

Ylj =

(cid:80)

i:(i,j)∈Ω AijXil
(cid:80)
i:(i,j)∈Ω Xil

,

i.e., each row of Y is updated to be the mean of the rows of A assigned to that archetype.
The x update (12) is not a convex problem, but is easily solved. The solution is given
by assigning xi to the closest archetype (often called a cluster centroid in the context of
j=1(Aij − Ylj)2(cid:17)
k-means): xi = el(cid:63) for l(cid:63) = argminl

(cid:16)(cid:80)n

.

Quadratic mixtures. We can also implement partial assignment of data vectors to clus-
ters. Take ˜r = 0, and let r be the indicator function of the set of probability vectors,
i.e.,

r(x) =

(cid:26) 0 (cid:80)k

l=1 xl = 1,

xl ≥ 0

∞ otherwise.

Subspace clustering. PCA approximates a data set by a single low dimensional subspace.
We may also be interested in approximating a data set as a union of low dimensional sub-
spaces. This problem is known as subspace clustering (see [Vid10] and references therein).
Subspace clustering may also be thought of as generalizing quadratic clustering to assign
each data vector to a low dimensional subspace rather than to a single cluster centroid.

To frame subspace clustering as a regularized PCA problem (11), partition the columns
of X into k blocks. Then let r be the indicator function of block sparsity (i.e., r(x) = 0 if
only one block of x has nonzero entries, and otherwise r(x) = ∞).

It is easy to perform alternating minimization on this objective function. This method
is sometimes called the k-planes algorithm [Vid10, Tse00, AM04], which alternates over

19

assigning examples to subspaces, and ﬁtting the subspaces to the examples. Once again, the
X update (12) is not a convex problem, but can be easily solved. Each block of the columns
of X deﬁnes a subspace spanned by the corresponding rows of Y . We compute the distance
from example i (the ith row of A) to each subspace (by solving a least squares problem),
and assign example i to the subspace that minimizes the least squares error by setting xi to
be the solution to the corresponding least squares problem.

Many other algorithms for this problem have also been proposed, such as the k-SVD
[Tro04, AEB06] and sparse subspace clustering [EV09], some with provable guarantees on
the quality of the recovered solution [SC12].

Supervised learning. Sometimes we want to understand the variation that a certain set
of features can explain, and the variance that remains unexplainable. To this end, one
natural strategy would be to regress the labels in the dataset on the features; to subtract
the predicted values from the data; and to use PCA to understand the remaining variance.
This procedure gives the same answer as the solution to a single regularized PCA problem.
Here we present the case in which the features we wish to use in the regression are present
in the data as the ﬁrst column of A. To construct the regularizers, we make sure the ﬁrst
column of A appears as a feature in the supervised learning problem by setting

ri(x) =

(cid:26) r0(x2, . . . , xk+1) x1 = Ai1
otherwise,

∞

where r0 = 0 can be chosen as in any regularized PCA model. The regularization on the
ﬁrst row of Y is the regularization used in the supervised regression, and the regularization
on the other rows will be that used in regularized PCA.

Thus we see that regularized PCA can naturally combine supervised and unsupervised

learning into a single problem.

Feature selection. We can use regularized PCA to perform feature selection. Consider
(11) with r(x) = (cid:107)x(cid:107)2
2 and ˜r(y) = (cid:107)y(cid:107)2. (Notice that we are not using (cid:107)y(cid:107)2
2.) The regularizer
˜r encourages the matrix ˜Y to be column-sparse, so many columns are all zero. If ˜yj = 0,
it means that feature j was uninformative, in the sense that its values do not help much in
predicting any feature in the matrix A (including feature j itself). In this case we say that
feature j was not selected. For this approach to make sense, it is important that the columns
of the matrix A should have mean zero. Alternatively, one can use the de-biasing regularizers
r(cid:48) and ˜r(cid:48) introduced in §3.3 along with the feature selection regularizer introduced here.

Dictionary learning. Dictionary learning (also sometimes called sparse coding) has be-
come a popular method to design concise representations for very high dimensional data
[OF97, LBRN06, MBPS09, MPS+09]. These representations have been shown to perform
well when used as features in subsequent (supervised) machine learning tasks [RBL+07].
In dictionary learning, each row of A is modeled as a linear combination of dictionary
atoms, represented by rows of Y . The total size of the dictionary used is often very large

20

(k (cid:29) max(m, n)), but each example is represented using a very small number of atoms. To
ﬁt the model, one solves the regularized PCA problem (11) with r(x) = (cid:107)x(cid:107)1, to induce spar-
sity in the number of atoms used to represent any given example, and with ˜r(y) = (cid:107)y(cid:107)2
2 or
˜r(y) = I+(c − (cid:107)y(cid:107)2) for some c > 0 ∈ R, in order to ensure the problem is well posed. (Note
that our notation transposes the usual notation in the literature on dictionary learning.)

Mix and match.
It is possible to combine these regularizers to obtain a factorization with
any combination of the above properties. As an example, one may require that both X and
Y be simultaneously sparse and nonnegative by choosing

r(x) = (cid:107)x(cid:107)1 + I+(x) = 1T x + I+(x),

and similarly for ˜r(y). Similarly, [KP07] show how to obtain a nonnegative matrix factor-
ization in which one factor is sparse by using r(x) = (cid:107)x(cid:107)2
2 + I+(y);
they go on to use this factorization as a clustering technique.

1 + I+(x) and ˜r(y) = (cid:107)y(cid:107)2

3.3 Oﬀsets and scaling

In our discussion of the quadratically regularized PCA problem (3), we saw that it can often
be quite important to standardize the data before applying PCA. Conversely, in regularized
PCA problems such as nonnegative matrix factorization, it makes no sense to standardize
the data, since subtracting column means introduces negative entries into the matrix.

A ﬂexible approach is to allow an oﬀset in the model: we solve

minimize (cid:80)

(i,j)∈Ω(Aij − xiyj − µj)2 + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj),

(14)

with variables xi, yj, and µj. Here, µj takes the role of the column mean, and in fact will
be equal to the column mean in the trivial case k = 0.

An oﬀset may be included in the standard form regularized PCA problem (11) by aug-
menting the problem slightly. Suppose we are given an instance of the problem (11), i.e.,
we are given k, r, and ˜r. We can ﬁt an oﬀset term µj by letting k(cid:48) = k + 1 and modifying
the regularizers. Extend the regularization r : Rk → R and ˜r : Rk → R to new regularizers
r(cid:48) : Rk+1 → R and ˜r(cid:48) : Rk+1 → R which enforce that the ﬁrst column of X is constant and
the ﬁrst row of Y is not penalized. Using this scheme, the ﬁrst row of the optimal Y will be
equal to the optimal µ in (14).

Explicitly, let

(cid:26) r(x2, . . . , xk+1) x1 = 1

r(cid:48)(x) =

∞

otherwise,

and ˜r(cid:48)(y) = ˜r(y2, . . . , yk+1). (Here, we identify r(x) = r(x1, . . . , xk) to explicitly show the
dependence on each coordinate of the vector x, and similarly for ˜r.)
It is also possible to introduce row oﬀsets in the same way.

21

4 Generalized loss functions

We may also generalize the loss function in PCA to form a generalized low rank model,

minimize (cid:80)

(i,j)∈Ω Lij(xiyj, Aij) + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj),

(15)

where Lij : R × R → R+ are given loss functions for i = 1, . . . , m and j = 1, . . . , n. Problem
(15) reduces to PCA with generalized regularization when Lij(u, a) = (a − u)2. However,
the loss function Lij can now depend on the data Aij in a more complex way.

4.1 Solution methods

As before, problem (15) is not convex, even when Lij, ri and ˜rj are convex; but if all these
functions are convex, then the problem is bi-convex.

Alternating minimization. Alternating minimization can still be used to ﬁnd a local
minimum, and it is still often possible to use factorization caching to speed up the solution
of the subproblems that arise in alternating minimization. We defer a discussion of how to
solve these subproblems explicitly to §7.

Stochastic proximal gradient method. For use with extremely large scale problems,
we discuss fast variants of the basic alternating minimization algorithm in §7. For example,
we present an alternating directions stochastic proximal gradient method. This algorithm
accesses the functions Lij, ri, and ˜rj only through a subgradient or proximal interface,
allowing it to generalize trivially to nearly any loss function and regularizer. We defer a
more detailed discussion of this method to §7.

4.2 Examples

Weighted PCA. A simple modiﬁcation of the PCA objective is to weight the importance
of ﬁtting each element in the matrix A. In the generalized low rank model, we let Lij(u−a) =
wij(a − u)2, where wij is a weight, and take r = ˜r = 0. Unlike PCA, the weighted PCA
problem has no known analytical solution [SJ03]. In fact, it is NP-hard to ﬁnd an exact
solution to weighted PCA [GG11], although it is not known whether it is always possible to
ﬁnd approximate solutions of moderate accuracy eﬃciently.

Robust PCA. Despite its widespread use, PCA is very sensitive to outliers. Many authors
have proposed a robust version of PCA obtained by replacing least-squares loss with (cid:96)1 loss,
which is less sensitive to large outliers [CLMW11, WGR+09, XCS12]. They propose to solve
the problem

minimize
(cid:107)S(cid:107)1 + (cid:107)Z(cid:107)∗
subject to S + Z = A.

(16)

22

The authors interpret Z as a robust version of the principal components of the data matrix
A, and S as the sparse, possibly large noise corrupting the observations.

We can frame robust PCA as a GLRM in the following way. If Lij(u, a) = |a − u|, and

r(x) = γ

2 (cid:107)x(cid:107)2

2, ˜r(y) = γ

2 (cid:107)y(cid:107)2

2, then (15) becomes
minimize (cid:107)A − XY (cid:107)1 + γ

2 (cid:107)X(cid:107)2

F + γ

2 (cid:107)Y (cid:107)2
F .

Using the arguments in §7.6, we can rewrite the problem by introducing a new variable
Z = XY as

This results in a rank-constrained version of the estimator proposed in the literature on
robust PCA [WGR+09, CLMW11, XCS12]:

minimize
subject to Rank(Z) ≤ k.

(cid:107)A − Z(cid:107)1 + γ(cid:107)Z(cid:107)∗

minimize
subject to S + Z = A

(cid:107)S(cid:107)1 + γ(cid:107)Z(cid:107)∗

Rank(Z) ≤ k,

where we have introduced the new variable S = A − Z.

Huber PCA. The Huber function is deﬁned as
(cid:26) (1/2)x2

huber(x) =

|x| − (1/2)

|x| ≤ 1
|x| > 1.

Using Huber loss,

L(u, a) = huber(u − a),

in place of (cid:96)1 loss also yields an estimator robust to occasionaly large outliers [Hub81]. The
Huber function is less sensitive to small errors |u − a| than the (cid:96)1 norm, but becomes linear
in the error for large errors. This choice of loss function results in a generalized low rank
model formulation that is robust both to large outliers and to small Gaussian perturbations
in the data.

Previously, the problem of Gaussian noise in robust PCA has been treated by decompos-
ing the matrix A = L + S + N into a low rank matrix L, a sparse matrix S, and a matrix
with small Gaussian entries N by minimizing the loss

(cid:107)L(cid:107)∗ + (cid:107)S(cid:107)1 + (1/2)(cid:107)N (cid:107)2
F

over all decompositions A = L + S + N of A [XCS12].

In fact, this formulation is equivalent to Huber PCA with quadratic regularization on
the factors X and Y . The argument showing this is very similar to the one we made above
for robust PCA. The only added ingredient is the observation that

huber(x) = inf{|s| + (1/2)n2 : x = n + s}.

In other words, the Huber function is the inﬁmal convolution of the negative log likelihood
of a gaussian random variable and a laplacian random variable: it represents the most likely
assignment of (additive) blame for the error x to a gaussian error n and a laplacian error s.

23

Robust regularized PCA. We can design robust versions of all the regularized PCA
problems above by the same transformation we used to design robust PCA. Simply replace
the quadratic loss function with an (cid:96)1 or Huber loss function. For example, k-mediods
[KR09, PJ09] is obtained by using (cid:96)1 loss in place of quadratic loss in the quadratic clustering
problem. Similarly, robust subspace clustering [SEC13] can be obtained by using an (cid:96)1 or
Huber penalty in the subspace clustering problem.

Quantile PCA. For some applications, it can be much worse to overestimate the entries
of A than to underestimate them, or vice versa. One can capture this asymmetry by using
the loss function

L(u, a) = α(a − u)+ + (1 − α)(u − a)+

and choosing α ∈ (0, 1) appropriately. This loss function is sometimes called a scalene loss,
and can be interpreted as performing quantile regression, e.g., ﬁtting the 20th percentile
[KB78, Koe05].

Fractional PCA. For other applications, we may be interested in ﬁnding an approxima-
tion of the matrix A whose entries are close to the original matrix on a relative, rather than
an absolute, scale. Here, we assume the entries Aij are all positive. The loss function

L(u, a) = max

(cid:18) a − u
u

,

u − a
a

(cid:19)

can capture this objective. A model (X, Y ) with objective value less than 0.10mn gives a
low rank matrix XY that is on average within 10% of the original matrix.

Logarithmic PCA. Logarithmic loss functions may also useful for ﬁnding an approxima-
tion of A that is close on a relative, rather than absolute, scale. Once again, we assume all
entries of A are positive. Deﬁne the logarithmic loss

This loss is not convex, but has the nice property that it ﬁts the geometric mean of the data:

To see this, note that we are solving a least squares problem in log space. At the solution,
log(u) will be the mean of log(ai), i.e.,

L(u, a) = log2(u/a).

argmin
u

(cid:88)

i

L(u, ai) = (

ai)1/n.

(cid:89)

i

log(u) = 1/n

log(ai) = log

(cid:88)

i

(cid:33)

ai)1/n

.

(cid:32)
(

(cid:89)

i

24

It is easy to formulate a version of PCA corresponding to
Exponential family PCA.
any loss in the exponential family. Here we give some interesting loss functions generated
by exponential families when all the entries Aij are positive. (See [CDS01] for a general
treatment of exponential family PCA.) One popular loss function in the exponential family
is the KL-divergence loss,

which corresponds to a Poisson generative model [CDS01].

Another interesting loss function is the Itakura-Saito (IS) loss,

L(u, a) = a log

− a + u,

(cid:17)

(cid:16) a
u

L(u, a) = log

− 1 +

(cid:17)

(cid:16) a
u

a
u

,

which has the property that it is scale invariant, so scaling a and u by the same factor
produces the same loss [SF14]. The IS loss corresponds to Tweedie distributions (i.e., distri-
butions for which the variance is some power of the mean) [Twe84]. This makes it interesting
in applications, such as audio processing, where fractional errors in recovery are perceived.

The β-divergence,

L(u, a) =

aβ
β(β − 1)

+

−

uβ
β

auβ−1
β − 1

,

generalizes both of these losses. With β = 2, we recover quadratic loss; in the limit as β → 1,
we recover the KL-divergence loss; and in the limit as β → 0, we recover the IS loss [SF14].

4.3 Oﬀsets and scaling

In §2.6, we saw how to use standardization to rescale the data in order to compensate for
unequal scaling in diﬀerent features. In general, standardization destroys sparsity in the data
by subtracting the (column) means (which are in general non-zero) from each element of the
data matrix A. It is possible to instead rescale the loss functions in order to compensate for
unequal scaling. Scaling the loss functions instead has the advantage that no arithmetic is
performed directly on the data A, so sparsity in A is preserved.

A savvy user may be able to select loss functions Lij that are scaled to reﬂect the
importance of ﬁtting diﬀerent columns. However, it is useful to have a default automatic
scaling for times when no savvy user can be found. The scaling proposed here generalizes
the idea of standardization to a setting with heterogeneous loss functions.

Given initial loss functions Lij, which we assume are nonnegative, for each feature j let

µj = argmin

Lij(µ, Aij),

(cid:88)

µ

i:(i,j)∈Ω

σ2
j =

1
nj − 1

(cid:88)

i:(i,j)∈Ω

Lij(µj, Aij).

It is easy to see that µj generalizes the mean of column j, while σ2
j generalizes the column
variance. For example, when Lij(u, a) = (u − a)2 for every i = 1, . . . , m, j = 1, . . . , n, µj is
the mean and σ2
j is the sample variance of the jth column of A. When Lij(u, a) = |u − a| for
every i = 1, . . . , m, j = 1, . . . , n, µj is the median of the jth column of A, and σ2
j is the sum

25

of the absolute values of the deviations of the entries of the jth column from the median
value.

To ﬁt a standardized GLRM, we rescale the loss functions by σ2

j and solve

minimize (cid:80)

(i,j)∈Ω Lij(Aij, xiyj + µj)/σ2

j + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj).

(17)

Note that this problem can be recast in the standard form for a generalized low rank model
(15). For the oﬀset, we may use the same trick described in §3.3 to encode the oﬀset in the
regularization; and for the scaling, we simply replace the original loss function Lij by Lij/σ2
j .

5 Loss functions for abstract data types

We began our study of generalized low rank modeling by considering the best way to ap-
proximate a matrix by another matrix of lower rank. In this section, we apply the same
procedure to approximate a data table that may not consist of real numbers, by choosing a
loss function that respects the data type.

We now consider A to be a table consisting of m examples (i.e., rows, samples) and n
features (i.e., columns, attributes), with each entry Aij drawn from a feature set Fj. The
feature set Fj may be discrete or continuous. So far, we have only considered numerical data
(Fj = R for j = 1, . . . , n), but now Fj can represent more abstract data types. For example,
entries of A can take on Boolean values (Fj = {T, F }), integral values (Fj = 1, 2, 3, . . .),
ordinal values (Fj = {very much, a little, not at all}), or consist of a tuple of these types
(Fj = {(a, b) : a ∈ R}).

We are given a loss function Lij : R × Fj → R. The loss Lij(u, a) describes the approxi-
mation error incurred when we represent a feature value a ∈ Fj by the number u ∈ R. We
give a number of examples of these loss functions below.

We now formulate a generalized low rank model on the database A as

minimize (cid:80)

(i,j)∈Ω Lij(xiyj, Aij) + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj),

(18)

with variables X ∈ Rn×k and Y ∈ Rk×m, and with loss Lij as above and regularizers
ri(xi) : R1×k → R and ˜rj(yj) : Rk×1 → R (as before). When the domain of each loss
function is R × R, we recover the generalized low rank model on a matrix (15).

5.1 Solution methods

As before, this problem is not convex, but it is bi-convex if ri, and ˜rj are convex, and Lij is
convex in its ﬁrst argument. The problem is also separable across samples i = 1, . . . , m and
features j = 1, . . . , m. These properties makes it easy to perform alternating minimization
on this objective. Once again, we defer a discussion of how to solve these subproblems
explicitly to §7.

26

+
)
u
a
−
1
(

4

3

2

1

0

a = −1

a = 1

a = −1

a = 1

−3 −2 −1

1

2

3

−3 −2 −1

1

2

3

0
u

0
u

Figure 1: Hinge loss.

Figure 2: Logistic loss.

5.2 Examples

Boolean PCA. Suppose Aij ∈ {−1, 1}m×n, and we wish to approximate this Boolean
matrix. For example, we might suppose that the entries of A are generated as noisy, 1-
bit observations from an underlying low rank matrix XY . Surprisingly, it is possible to
accurately estimate the underlying matrix with only a few observations |Ω| from the matrix
by solving problem (18) (under a few mild technical conditions) with an appropriate loss
function [DPBW12].

We may take the loss to be

L(u, a) = (1 − au)+,

which is the hinge loss (see Figure 1), and solve the problem (18) with or without regulariza-
tion. When the regularization is sum of squares (r(x) = λ(cid:107)x(cid:107)2
2), ﬁxing X and
minimizing over yj is equivalent to training a support vector machine (SVM) on a data set
consisting of m examples with features xi and labels Aij. Hence alternating minimization
for the problem (15) reduces to repeatedly training an SVM. This model has been previously
considered under the name Maximum Margin Matrix Factorization (MMMF) [SRJ04, RS05].

2, ˜r(y) = λ(cid:107)y(cid:107)2

Logistic PCA. Again supposing Aij ∈ {−1, 1}m×n, we can also use a logistic loss to
measure the approximation quality. Let

L(u, a) = log(1 + exp(−au))

(see Figure 2). With this loss, ﬁxing X and minimizing over yj is equivalent to using logistic
regression to predict the labels Aij. This model has been previously considered under the
name logistic PCA [SSU03].

)
)
u
a
(
p
x
e
+
1
(
g
o
l

3

2

1

0

27

Figure 3: Ordinal hinge loss.

Poisson PCA. Now suppose the data Aij are nonnegative integers. We can use any loss
function that might be used in a regression framework to predict integral data to construct
a generalized low rank model for Poisson PCA. For example, we can take

L(u, a) = exp(u) − au + a log a − a.

This is the exponential family loss corresponding to Poisson data. (It diﬀers from the KL-
divergence loss from §4.2 only in that u has been replaced by exp(u), which allows u to take
negative values.)

Ordinal PCA. Suppose the data Aij records the levels of some ordinal variable, encoded
as {1, 2, . . . , d}. We wish to penalize the entries of the low rank matrix XY which deviate
by many levels from the encoded ordinal value. A convex version of this penalty is given by
the ordinal hinge loss,

L(u, a) =

(1 − u + a(cid:48))+ +

(1 + u − a(cid:48))+,

(19)

a−1
(cid:88)

a(cid:48)=1

d
(cid:88)

a(cid:48)=a+1

which generalizes the hinge loss to ordinal data (see Figure 3).

This loss function may be useful for encoding Likert-scale data indicating degrees of

agreement with a question [Lik32]. For example, we might have

Fj = {strongly disagree, disagree, neither agree nor disagree, agree, strongly agree}.

We can encode these levels as the integers 1, . . . , 5 and use the above loss to ﬁt a model to
ordinal data.

This approach assumes that every increment of error is equally bad: for example, that
approximating “agree” by “strongly disagree” is just as bad as aproximating “neither agree
nor disagree” by “agree”. In §6.1 we introduce a more ﬂexible ordinal loss function that can
learn a more ﬂexible relationship between ordinal labels. For example, it could determine
that the diﬀerence between “agree” and “strongly disagree” is smaller than the diﬀerence
between “neither agree nor disagree” and “agree”.

28

Interval PCA. Suppose that the data Aij ∈ R2 are tuples denoting the endpoints of an
interval, and we wish to ﬁnd a low rank matrix whose entries lie inside these intervals. We
can capture this objective using, for example, the deadzone-linear loss

L(u, a) = max((a1 − u)+, (u − a2)+).

5.3 Missing data and data imputation

We can use the solution (X, Y ) to a low rank model to impute values corresponding to
missing data (i, j) (cid:54)∈ Ω. This process is sometimes also called inference. Above, we saw that
for quadratically regularized PCA, the MAP estimator for the missing entry Aij is equal to
xiyj. This is still true for many of the loss functions above, such as the Huber function or (cid:96)1
loss, for which it makes sense for the data to take on any real value.

However, to approximate abstract data types we must consider a more nuanced view.
While we can still think of the solution (X, Y ) to the generalized low rank model (15) in
Boolean PCA as approximating the Boolean matrix A, the solution is not a Boolean matrix.
Instead we say that we have encoded the original Boolean matrix as a real-valued low rank
matrix XY , or that we have embedded the original Boolean matrix into the space of real-
valued matrices.

To ﬁll in missing entries in the original matrix A, we compute the value ˆAij that minimizes

the loss for xiyj:

ˆAij = argmin

Lij(xiyj, a).

a

This implicitly constrains ˆAij to lie in the domain Fj of Lij. When Lij : R × R → R, as is
the case for the losses in §4 above (including (cid:96)2, (cid:96)1, and Huber loss), then ˆAij = xiyj. But
when the data is of an abstract type, the minimum argmina Lij(u, a) will not in general be
equal to u.

For example, when the data is Boolean, Lij : {0, 1} × R → R, we compute the Boolean

matrix ˆA implied by our low rank model by solving

for MMMF, or

ˆAij = argmin

(a(XY )ij − 1)+

a∈{0,1}

ˆAij = argmin

a∈{0,1}

log(1 + exp(−a(XY )ij))

for logistic PCA. These problems both have the simple solution

ˆAij = sign(xiyj).

When Fj is ﬁnite, inference partitions the real numbers into regions

Ra = {x ∈ R : Lij(u, x) = min

Lij(u, a)}

a

corresponding to diﬀerent values a ∈ Fj. When Lij is convex, these regions are intervals.

29

We can use the estimate ˆAij even when (i, j) ∈ Ω was observed. If the original obser-
vations have been corrupted by noise, we can view ˆAij as a denoised version of the original
data. This is an unusual kind of denoising: both the noisy (Aij) and denoised ( ˆAij) versions
of the data lie in the abstract space Fj.

5.4 Interpretations and applications

We have already discussed some interpretations of X and Y in the PCA setting. Now we
reconsider those interpretations in the context of approximating these abstract data types.

Archetypes. As before, we can think of each row of Y as an archetype which captures the
behavior of an idealized example. However, the rows of Y are real numbers. To represent
each archetype l = 1, . . . , k in the abstract space as Yl with (Yl)j ∈ Fj, we solve

(Yl)j = argmin

Lj(ylj, a).

a∈Fj

(Here we assume that the loss Lij = Lj is independent of the example i.)

Archetypical representations. As before, we call xi the representation of example i in
terms of the archetypes. The rows of X give an embedding of the examples into Rk, where
each coordinate axis corresponds to a diﬀerent archetype. If the archetypes are simple to
understand or interpret, then the representation of an example can provide better intuition
about that example.

In contrast to the initial data, which may consist of arbitrarily complex data types, the
representations xi will be low dimensional vectors, and can easily be plotted, clustered, or
used in nearly any kind of machine learning algorithm. Using the generalized low rank model,
we have converted an abstract feature space into a vector space.

Feature representations. The columns of Y embed the features into Rk. Here we think
of the columns of X as archetypical features, and represent each feature j as a linear com-
bination of the archetypical features. Just as with the examples, we might choose to apply
any machine learning algorithm to the feature representations.

This procedure allows us to compare non-numeric features using their representation in
Rl. For example, if the features F are Likert variables giving the extent to which respondents
on a questionnaire agree with statements 1, . . . , n, we might be able to say that questions i
and j are similar if (cid:107)yi − yj(cid:107) is small; or that question i is a more polarizing form of question
j if yi = αyj, with α > 1.

Even more interesting, it allows us to compare features of diﬀerent types. We could say

that the real-valued feature i is similar to Likert-valued question j if (cid:107)yi − yj(cid:107) is small.

30

Latent variables. Each row of X represents an example by a vector in Rk. The matrix Y
maps these representations back into the original feature space (now nonlinearly) as described
in the discussion on data imputation in §5.3. We might think of X as discovering the latent
variables that best explain the observed data, with the added beneﬁt that these latent
variables lie in the vector space Rk.
(i,j)∈Ω Lij(xiyj, Aij) is
small, then we view these latent variables as providing a good explanation or summary of
the full data set.

If the approximation error (cid:80)

Probabilistic intepretation. We can give a probabilistic interpretation of X and Y ,
generalizing the hierarchical Bayesian model presented by Fithian and Mazumder in [FM13].
We suppose that the matrices ¯X and ¯Y are generated according to a probability distribution
with probability proportional to exp(−r( ¯X)) and exp(−˜r( ¯Y )), respectively. Our observations
A of the entries in the matrix ¯Z = ¯X ¯Y are given by

where the random variable ψij(u) takes value a with probability proportional to

Aij = ψij(( ¯X ¯Y )ij),

exp (−Lij(u, a)) .

We observe each entry (i, j) ∈ Ω. Then to ﬁnd the maximum a posteriori (MAP) estimator
(X, Y ) of ( ¯X, ¯Y ), we solve

maximize exp

(cid:16)

− (cid:80)

(cid:17)
(i,j)∈Ω Lij(xiyj, Aij)

exp(−r(X)) exp(−˜r(Y )),

which is equivalent, by taking logs, to problem (18).

This interpretation gives us a simple way to interpret our procedure for imputing missing

observations (i, j) (cid:54)∈ Ω. We are simply computing the MAP estimator ˆAij.

Auto-encoder. The matrix X encodes the data; the matrix Y decodes it back into the
full space. We can view (18) as providing the best linear auto-encoder for the data. Among
all linear encodings (X) and decodings (Y ) of the data, the abstract generalized low rank
model (18) minimizes the reconstruction error measured according to the loss functions Lij.

Compression. We impose an information bottleneck by using a low rank auto-encoder
to ﬁt the data. The bottleneck is imposed by both the dimensionality reduction and the
regularization, giving both soft and hard constraints on the information content allowed.
The solution (X, Y ) to problem (18) maximizes the information transmitted through this
k-dimensional bottleneck, measured according to the loss functions Lij. This X and Y give
a compressed and real-valued representation that may be used to more eﬃciently store or
transmit the information present in the data.

31

5.5 Oﬀsets and scaling

Just as in the previous section, better practical performance can often be achieved by allowing
an oﬀset in the model as described in §3.3, and automatic scaling of loss functions as described
in §4.3. As we noted in §4.3, scaling the loss functions (instead of standardizing the data)
has the advantage that no arithmetic is performed directly on the data A. When the data A
consists of abstract types, it is quite important that no arithmetic is performed on the data,
so that we need not take the average of, say, “very much” and “a little”, or subtract it from
“not at all”.

5.6 Numerical examples

In this section we give results of some small experiments illustrating the use of diﬀerent loss
functions adapted to abstract data types, and comparing their performance to quadratically
regularized PCA. To ﬁt these GLRMs, we use alternating minimization and solve the sub-
problems with subgradient descent. This approach is explained more fully in §7. Running
the alternating subgradient method multiple times on the same GLRM from diﬀerent initial
conditions yields diﬀerent models, all with very similar (but not identical) objective values.

Boolean PCA. For this experiment, we generate Boolean data A ∈ {−1, +1}n×m as

A = sign (cid:0)X trueY true(cid:1),
where X true ∈ Rn×ktrue and Y true ∈ Rktrue×m have independent, standard normal entries. We
consider a problem instance with m = 50, n = 50, and ktrue = k = 10.

We ﬁt two GLRMs to this data to compare their performance. Boolean PCA uses hinge
loss L(u, a) = max (1 − au, 0) and quadratic regularization r(u) = ˜r(u) = .1(cid:107)u(cid:107)2
2, and pro-
duces the model (X bool, Y bool). Quadratically regularized PCA uses squared loss L(u, a) =
(u − a)2 and the same quadratic regularization, and produces the model (X real, Y real).

Figure 4 shows the results of ﬁtting Boolean PCA to this data. The ﬁrst column shows
the original ground-truth data A; the second shows the imputed data given the model, ˆAbool,
generated by rounding the entries of X boolY bool to the closest number in 0, 1 (as explained in
§5.3); the third shows the error A− ˆAbool. Figure 4 shows the results of running quadratically
regularized PCA on the same data, and shows A, ˆAreal, and A − ˆAreal.

As expected, Boolean PCA performs substantially better than quadratically regularized
PCA on this data set. On average over 100 draws from the ground truth data distribution,
the misclassiﬁcation error (percentage of misclassiﬁed entries)

(cid:15)(X, Y ; A) =

#{(i, j) | Aij (cid:54)= sign (XY )ij}
mn

is much lower using hinge loss ((cid:15)(X bool, Y bool; A) = 0.0016) than squared loss ((cid:15)(X real, Y real; A) =
0.0051). The average RMS errors

RMS(X, Y ; A) =

(Aij − (XY )ij)2

(cid:33)1/2

(cid:32)

1
mn

m
(cid:88)

n
(cid:88)

i=1

j=1

32

using hinge loss (RMS(X bool, Y bool; A) = 0.0816) and squared loss (RMS(X real, Y real; A) =
0.159) also indicate an advantage for Boolean PCA.

Figure 4: Boolean PCA on Boolean data.

Figure 5: Quadratically regularized PCA on Boolean data.

Censored PCA.
In this example, we consider the performance of Boolean PCA when
only a subset of positive entries in the Boolean matrix A ∈ {−1, 1}m×n have been observed,
i.e., the data has been censored. For example, a retailer might know only a subset of the
products each customer purchased; or a medical clinic might know only a subset of the
diseases a patient has contracted, or of the drugs the patient has taken. Imputation can be
used in this setting to (attempt to) distinguish true negatives Aij = −1 from unobserved
positives Aij = +1, (i, j) (cid:54)∈ Ω.

We generate a low rank matrix B = XY ∈ [0, 1]m×n with X ∈ Rm×k, Y ∈ Rk×n, where
the entries of X and Y are drawn from a uniform distribution on [0, 1], m = n = 300 and
k = 3. Our data matrix A is chosen by letting Aij = 1 with probability proportional to
Bij, and −1 otherwise; the constant of proportionality is chosen so that half of the entries
in A are positive. We ﬁt a rank 5 GLRM to an observation set Ω consisting of 10% of the
positive entries in the matrix, drawn uniformly at random, using hinge loss and quadratic

33

regularization. That is, we ﬁt the low rank model

minimize (cid:80)

(i,j)∈Ω max(1 − xiyjAij, 0) + γ (cid:80)m

i=1 (cid:107)xi(cid:107)2

2 + γ (cid:80)n

j=1 (cid:107)yj(cid:107)2
2

and vary the regularization parameter γ.

normalized training error,

We consider three error metrics to measure the performance of the ﬁtted model (X, Y ):

normalized test error,

1
|Ω|

(cid:88)

(i,j)∈Ω

1
|ΩC|

(cid:88)

(i,j)∈ΩC

max(1 − Aijxiyj, 0),

max(1 − Aijxiyj, 0),

and precision at 10 (p@10), which is computed as the fraction of the top ten predicted
values not in the observation set, {xiyj : (i, j) ∈ ΩC}, for which Aij = 1. (Here, ΩC =
{1, . . . , m} × {1, . . . , n} \ Ω.) Precision at 10 measures the usefulness of the model:
if we
predicted that the top 10 unseen elements (i, j) had values +1, how many would we get
right?

Figure 6 shows the regularization path as γ ranges from 0 to 40, averaged over 50 sam-
ples from the distribution generating the data. Here, we see that while the training error
decreases as γ decreases, the test error reaches a minimum around γ = 5. Interestingly, the
precision at 10 improves as the regularization increases; since precision at 10 is computed
using only relative rather than absolute values of the model, it is insensitive to the shrinkage
of the parameters introduced by the regularization. The grey line shows the probability of
identifying a positive entry by guessing randomly; precision at 10, which exceeds 80% when
γ (cid:38) 30, is signiﬁcantly higher. This performance is particularly impressive given that the
observations Ω are generated by sampling from rather than rounding the auxiliary matrix
B.

Mixed data types.
In this experiment, we ﬁt a GLRM to a data table with numerical,
Boolean, and ordinal columns generated as follows. Let N1, N2, and N3 partition the column
indices 1, . . . , n. Choose X true ∈ Rm×ktrue, Y true ∈ Rktrue×n to have independent, standard
normal entries. Assign entries of A as follows:

Aij =






xiyj
sign (xiyj)
round(3xiyj + 1)

j ∈ N1
j ∈ N2
j ∈ N3,

where the function round maps a to the nearest integer in the set {1, . . . , 7}. Thus, N1
corresponds to real-valued data; N2 corresponds to Boolean data; and N3 corresponds to
ordinal data. We consider a problem instance in which m = 100, n1 = 40, n2 = 30, n3 = 30,
and ktrue = k = 10.

34

train error
test error

p@10

r
o
r
r
e

d
e
z
i
l
a
m
r
o
n

8

6

4

2

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1
+

f
o

y
t
i
l
i
b
a
b
o
r
p

0
0

5
5

10
10

15
15

20
20
regularization parameter
regularization parameter

25
25

30
30

35
35

40
40

Figure 6: Error metrics for Boolean GLRM on censored data. The grey line shows
the probability of identifying a positive entry by guessing randomly.

35

We ﬁt a heterogeneous loss GLRM to this data with loss function

Lij(u, a) =






Lreal(u, a)
Lbool(u, a)
Lord(u, a)

j ∈ N1
j ∈ N2
j ∈ N3,

where Lreal(u, a) = (u − a)2, Lbool(u, a) = (1 − au)+, and Lord(u, a) is deﬁned in (19), and
with quadratic regularization r(u) = ˜r(u) = .1(cid:107)u(cid:107)2
2. We ﬁt the GLRM to produce the model
(X mix, Y mix). For comparison, we also ﬁt quadratically regularized PCA to the same data,
using Lij(u, a) = (u − a)2 for all j and quadratic regularization r(u) = ˜r(u) = .1(cid:107)u(cid:107)2
2, to
produce the model (X real, Y real).

Figure 7 shows the results of ﬁtting the heterogeneous loss GLRM to the data. The ﬁrst
column shows the original ground-truth data A; the second shows the imputed data given
the model, ˆAmix, generated by rounding the entries of X mixY mix to the closest number in
0, 1 (as explained in §5.3); the third shows the error A − ˆAmix. Figure 8 corresponds to
quadratically regularized PCA, and shows A, ˆAreal, and A − ˆAreal.

To evaluate error for Boolean and ordinal data, we use the misclassiﬁcation error (cid:15) deﬁned
above. For notational convenience, we let YNl (ANl) denote Y (A) restricted to the columns
Nl in order to pick out real-valued columns (l = 1), Boolean columns (l = 2), and ordinal
columns (l = 3).

Table 1 compare the average error (diﬀerence between imputed entries and ground truth)
over 100 draws from the ground truth distribution for models using heterogeneous loss (X mix,
Y mix) and quadratically regularized loss (X real, Y real). Columns are labeled by error metric.
We use misclassiﬁcation error (cid:15) for Boolean and ordinal data and MSE for numerical data.

X mix, Y mix
X real , Y real

MSE(X, YN1; AN1)
0.0224
0.0076

(cid:15)(X, YN2; AN2)
0.0074
0.0213

(cid:15)(X, YN3; AN3)
0.0531
0.0618

Table 1: Average error for numerical, Boolean, and ordinal features using GLRM
with heterogenous loss and quadratically regularized loss.

Missing data. Here, we explore the eﬀect of missing entries on the accuracy of the re-
covered model. We generate data A as detailed above, but then censor one large block of
entries in the table (constituting 3.75% of numerical, 50% of Boolean, and 50% of ordinal
data), removing them from the observed set Ω.

Figure 9 shows the results of ﬁtting the heterogeneous loss GLRM described above on
the censored data. The ﬁrst column shows the original ground-truth data A; the second
shows the block of data that has been removed from the observation set Ω; the third shows
the imputed data given the model, ˆAmix, generated by rounding the entries of X mixY mix to
the closest number in {0, 1} (as explained in §5.3); the fourth shows the error A − ˆAmix.

36

Figure 7: Heterogeneous loss GLRM on mixed data.

Figure 8: Quadratically regularized PCA on mixed data.

37

Figure 9: Heterogeneous loss GLRM on missing data.

Figure 10: Quadratically regularized PCA on missing data.

Figure 10 corresponds to running quadratically regularized PCA on the same data, and
shows A, ˆAreal, and A − ˆAreal. While quadradically regularized PCA and the heterogeneous
loss GLRM performed similarly when no data was missing, the heterogeneous loss GLRM
performs much better than quadradically regularized PCA when a large block of data is
censored.

We compare the average error (diﬀerence between imputed entries and ground truth) over
100 draws from the ground truth distribution in Table 2. As above, we use misclassiﬁcation
error (cid:15) for Boolean and ordinal data and MSE for numerical data.

X mix, Y mix
X real , Y real

MSE(X, YN1; AN1)
0.392
0.561

(cid:15)(X, YN2; AN2)
0.2968
0.4029

(cid:15)(X, YN3; AN3)
0.3396
0.9418

Table 2: Average error over imputed data using a GLRM with heterogenous loss
and regularized quadratic loss.

6 Multi-dimensional loss functions

In this section, we generalize the procedure to allow the loss functions to depend on blocks
of the matrix XY , which allows us to represent abstract data types more naturally. For
example, we can now represent categorical values , permutations, distributions, and rankings.

38

We are given a loss function Lij : R1×dj ×Fj → R, where dj is the embedding dimension of
feature j, and d = (cid:80)
j dj is the total dimension of the embedded features. The loss Lij(u, a)
describes the approximation error incurred when we represent a feature value a ∈ Fj by the
vector u ∈ Rdj .

Let xi ∈ R1×k be the ith row of X (as before), and let Yj ∈ Rk×dj be the jth block
matrix of Y so the columns of Yj correspond to the columns of embedded feature j. We now
formulate a multi-dimensional generalized low rank model on the database A,

minimize (cid:80)

(i,j)∈Ω Lij(xiYj, Aij) + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(Yj),

(20)

with variables X ∈ Rn×k and Y ∈ Rk×d, and with loss Lij as above and regularizers
ri(xi) : R1×k → R (as before) and ˜rj(Yj) : Rk×dj → R. Note that the ﬁrst argument of Lij
is a row vector with dj entries, and the ﬁrst argument of rj is a matrix with dj columns.
When every entry Aij is real-valued (i.e., dj = 1), then we recover the generalized low rank
model (15) seen in the previous section.

6.1 Examples

Categorical PCA. Suppose that a ∈ F is a categorical variable, taking on one of d values
or labels. Identify the labels with the integers {1, . . . , d}. In (20), set

L(u, a) = (1 − ua)+ +

(1 + ua(cid:48))+,

(cid:88)

a(cid:48)∈F , a(cid:48)(cid:54)=a

and use the quadratic regularizer ri = γ(cid:107) · (cid:107)2

2, ˜r = γ(cid:107) · (cid:107)2
2.

Fixing X and optimizing over Y is equivalent to training one SVM per label to separate
that label from all the others: the jth column of Y gives the weight vector corresponding
to the jth SVM. (This is sometimes called one-vs-all multiclass classiﬁcation [RK04].) Op-
timizing over X identiﬁes the low-dimensional feature vectors for each example that allow
these SVMs to most accurately predict the labels.

The diﬀerence between categorical PCA and Boolean PCA is in how missing labels are
imputed. To impute a label for entry (i, j) with feature vector xi according to the procedure
described above in 5.3, we project the representation Yj onto the line spanned by xi to form
u = xiYj. Given u, the imputed label is simply argmaxl ul. This model has the interesting
property that if column l(cid:48) of Yj lies in the interior of the convex hull of the columns of Yj,
then ul(cid:48) will lie in the interior of the interval [minl ul, maxl ul] [BV04]. Hence the model will
never impute label l(cid:48) for any example.

We need not restrict ourselves to the loss function given above. In fact, any loss func-
tion that can be used to train a classiﬁer for categorical variables (also called a multi-class
classiﬁer) can be used to ﬁt a categorical PCA model, so long as the loss function depends
only on the inner products between the parameters of the model and the features corre-
sponding to each example. The loss function becomes the loss function L used in (20); the
optimal parameters of the model give the optimal matrix Y , while the implied features will
populate the optimal matrix X. For example, it is possible to use loss functions derived

39

from error-correcting output codes [DB95]; the Directed Acyclic Graph SVM [PCST99]; the
Crammer-Singer multi-class loss [CS02]; or the multi-category SVM [LLW04].

Of these loss functions, only the one-vs-all loss is separable across the classes a ∈ F.
(By separable, we mean that the objective value can be written as a sum over the classes.)
Hence ﬁtting a categorical features with any other loss functions is not the same as ﬁtting d
Boolean features. For example, in the Crammer-Singer loss

L(u, a) = (1 − ua + max

u(cid:48)
a)+,

a(cid:48)∈F , a(cid:48)(cid:54)=a

the classes are combined according to their maximum, rather than their sum. While one-vs-
all classiﬁcation performs about as well as more sophisticated loss functions on small data
sets [RK04], these more sophisticated nonseparable loss tend to perform much better as the
number of classes (and examples) increases [GBW14].

Some interesting nonconvex loss functions have also been suggested for this problem. For

example, consider a generalization of Hamming distance to this setting,

L(u, a) = δua,1 +

δua(cid:48) ,0,

(cid:88)

a(cid:48)(cid:54)=a

where δα,β = 0 if α = β and 1 otherwise.
In this case, alternating minimization with
regularization that enforces a clustered structure in the low rank model (see the discussion
of quadratic clustering in §3.2) reproduces the k-modes algorithm [HN99].

Ordinal PCA. We saw in §5 one way to ﬁt a GLRM to ordinal data. Here, we use a
larger embedding dimension for ordinal features. The multi-dimensional embedding will be
particularly useful when the best mapping of the ordinal variable onto a linear scale is not
uniform; e.g., if level 1 of the ordinal variable is much more similar to level 2 than level 2
is to level 3. Using a larger embedding dimension allows us to infer the relations between
the levels from the data itself. Here we again identify the labels a ∈ F with the integers
{1, . . . , d}.

One approach we can use for (multi-dimensional) ordinal PCA is to solve (20) with the

loss function

L(u, a) =

(1 − Ia>a(cid:48)ua(cid:48))+,

(21)

and with quadratic regularization. Fixing X and optimizing over Y is equivalent to training
an SVM to separate labels a ≤ l from a > l for each l ∈ F. This approach produces
a set of hyperplanes (given by the columns of Y ) separating each level l from the next.
The hyperplanes need not be parallel to each other. Fixing Y and optimizing over X ﬁnds
the low dimensional features vector for each example that places the example between the
appropriate hyperplanes.
(See Figure 11 for an illustration of an optimal ﬁt of this loss
function, with k = 2, to a simple synthetic data set.)

d−1
(cid:88)

a(cid:48)=1

40

Figure 11: Multi-dimensional ordinal loss.

Permutation PCA. Suppose that a is a permutation of the numbers 1, . . . , d. Deﬁne the
permutation loss

L(u, a) =

(1 − uai + uai+1)+.

d−1
(cid:88)

i=1

This loss is zero if uai > uai+1 + 1 for i = 1, . . . , d − 1, and increases linearly when these
inequalities are violated. Deﬁne sort(u) to return a permutation ˆa of the indices 1, . . . , d so
that uˆai ≥ uˆai+1 for i = 1, . . . , d−1. It is easy to check that argmina L(u, a) = sort(u). Hence
using the permutation loss function in generalized PCA (20) ﬁnds a low rank approximation
of a given table of permutations.

Ranking PCA. Many variants on the permutation PCA problem are possible. For ex-
ample, in ranking PCA, we interpret the permutation as a ranking of the choices 1, . . . , d,
and penalize deviations of many levels more strongly than deviations of only one level by
choosing the loss

L(u, a) =

(1 − uai + uaj )+.

d−1
(cid:88)

d
(cid:88)

i=1

j=i+1

From here, it is easy to generalize to a setting in which the rankings are only partially
observed. Suppose that we observe pairwise comparisons a ⊆ {1, . . . , d} × {1, . . . , d}, where
(i, j) ∈ a means that choice i was ranked above choice j. Then a loss function penalizing

41

devations from these observed rankings is

L(u, a) =

(1 − uai + uaj )+.

(cid:88)

(i,j)∈a

Many other modiﬁcations to ranking loss functions have been proposed in the literature
that interpolate between the the two ﬁrst loss functions proposed above, or which priori-
tize correctly predicting the top ranked choices. These losses include the area under the
curve loss [Ste07], ordered weighted average of pairwise classiﬁcation losses [UBG09], the
weighted approximate-rank pairwise loss [WBU10], the k-order statistic loss [WYW13], and
the accuracy at the top loss [BCMR12].

6.2 Oﬀsets and scaling

Just as in the previous section, better practical performance can often be achieved by allowing
an oﬀset in the model as described in §3.3, and scaling loss functions as described in §4.3.

6.3 Numerical examples

We ﬁt a low rank model to the 2013 American Community Survey (ACS) to illustrate how
to ﬁt a low rank model to heterogeneous data.

The ACS is a survey administered to 1% of the population of the United States each
year to gather their responses to a variety of demographic and economic questions. Our
data sample consists of m = 3132796 responses gathered from residents of the US, excluding
Puerto Rico, in the year 2013, on the 23 questions listed in Table 3.

We ﬁt a rank 10 model to this data using Huber loss for real valued data, hinge loss for
Boolean data, ordinal hinge loss for ordinal data, one-vs-all categorical loss for categorical
data, and regularization parameter γ = .1. We allow an oﬀset in the model and scale the
loss functions and regularization as described in §4.3.

In Table 4, we select a few features j from the model, along with their associated vec-
tors yj, and ﬁnd the two features most similar to them by ﬁnding the two features j(cid:48) which
minimize cos(yj, yj(cid:48)). The model automatically groups states which intuitively share de-
for example, three wealthy states adjoining (but excluding) a major
mographic features:
metropolitan area — Virginia, Maryland, and Connecticut — are grouped together. The
low rank structure also identiﬁes the results (high water prices) of the prolonged drought
aﬄicting California, and corroborates the intuition that work leads only to more work: hours
worked per week, weeks worked per year, and education level are highly correlated.

7 Fitting low rank models

In this section, we discuss a number of algorithms that may be used to ﬁt generalized low
rank models. As noted earlier, it can be computationally hard to ﬁnd the global optimum
of a generalized low rank model. For example, it is NP-hard to compute an exact solution

42

Description
household type
state

commercial use
house on ≥ 10 acres
household income
monthly electricity bill

Variable
HHTYPE
STATEICP
OWNERSHP own home
COMMUSE
ACREHOUS
HHINCOME
COSTELEC
COSTWATR monthly water bill
monthly gas bill
COSTGAS
FOODSTMP on food stamps
HCOVANY
SCHOOL
EDUC
GRADEATT highest grade level attained
EMPSTAT
LABFORCE
CLASSWKR class of worker
WKSWORK2 weeks worked per year
UHRSWORK usual hours worked per week real
looking for work
LOOKING
migration status
MIGRATE1

have health insurance
currently in school
highest level of education

Type
categorical
categorical
Boolean
Boolean
Boolean
real
real
real
real
Boolean
Boolean
Boolean
ordinal
ordinal
categorical
Boolean
Boolean
ordinal

employment status
in labor force

Boolean
categorical

Table 3: ACS variables.

Most similar features
Montana, North Dakota
Illinois, cost of water
Oregon, Idaho
Indiana, Michigan

Feature
Alaska
California
Colorado
Ohio
Pennsylvania Massachusetts, New Jersey
Virginia
Hours worked weeks worked, education

Maryland, Connecticut

Table 4: Most similar features in demography space.

43

to k-means [DFK+04], nonnegative matrix factorization [Vav09], and weighted PCA and
matrix completion [GG11] all of which are special cases of low rank models.

In §7.1, we will examine a number of local optimization methods based on alternating
minimization. Algorithms implementing lazy variants of alternating minimization, such as
the alternating gradient, proximal gradient, or stochastic gradient algorithms, are faster
per iteration than alternating minimization, although they may require more iterations for
convergence. In numerical experiments, we notice that lazy variants often converge to points
with a lower objective value: it seems that these lazy variants are less likely to be trapped
at a saddle point than is alternating minimization. §7.4 explores the convergence of these
algorithms in practice.

We then consider a few special cases in which we can show that alternating minimization
converges to the global optimum in some sense: for example, we will see convergence with
high probability, approximately, and in retrospect. §7.5 discusses a few strategies for initial-
izing these local optimization methods, with provable guarantees in special cases. §7.6 shows
that for problems with convex loss functions and quadratic regularization, it is sometimes
possible to certify global optimality of the resulting model.

7.1 Alternating minimization

We showed earlier how to use alternating minimization to ﬁnd an (approximate) solution
to a generalized low rank model. Algorithm (1) shows how to explicitly extend alternating
minimization to a generalized low rank model (15) with observations Ω.

Algorithm 1

given X 0, Y 0
for k = 1, 2, . . . do

for i = 1, . . . , M do
(cid:16)(cid:80)
xk
i = argminx
end for
for j = 1, . . . , N do
(cid:16)(cid:80)
yk
j = argminy
end for

end for

j:(i,j)∈Ω Lij(xyk−1

j

, Aij) + r(x)

(cid:17)

i:(i,j)∈Ω Lij(xk

i y, Aij) + ˜r(y)

(cid:17)

Parallelization. Alternating minimization parallelizes naturally over examples and fea-
tures.
In Algorithm 1, the loops over i = 1, . . . , N and over j = 1, . . . , M may both be
executed in parallel.

44

7.2 Early stopping

It is not very useful to spend a lot of eﬀort optimizing over X before we have a good estimate
for Y . If an iterative algorithm is used to compute the minimum over X, it may make sense to
stop the optimization over X early before going on to update Y . In general, we may consider
replacing the minimization over x and y above by any update rule that moves towards the
minimum. This templated algorithm is presented as Algorithm 2. Empirically, we ﬁnd that
this approach often ﬁnds a better local minimum than performing a full optimization over
each factor in every iteration, in addition to saving computational eﬀort on each iteration.

Algorithm 2

given X 0, Y 0
for t = 1, 2, . . . do

, Y t−1, A)

for i = 1, . . . , m do

i = updateL,r(xt−1
xt
end for
for j = 1, . . . , n do

i

j = updateL,˜r(y(t−1)T
yt
end for

j

, X (t)T , AT )

end for

We describe below a number of diﬀerent update rules updateL,r by writing the X update.
The Y update can be implemented similarly. (In fact, it can be implemented by substituting
˜r for r, switching the roles of X and Y , and transposing all matrix arguments.) All of the
approaches outlined below can still be executed in parallel over examples (for the X update)
and features (for the Y update).

Gradient method. For example, we might take just one gradient step on the objective.
This method can be used as long as L, r, and ˜r do not take inﬁnite values. (If any of these
functions f is not diﬀerentiable, replace ∇f below by any subgradient of f [BL10, BXM03].)

We implement updateL,r as follows. Let

(cid:88)

g =

j:(i,j)∈Ω

∇Lij(xiyj, Aij)yj + ∇r(xi).

Then set

i = xt−1
xt

i − αtg,

for some step size αt. For example, a common step size rule is αt = 1/t, which guarantees
convergence to the globally optimal X if Y is ﬁxed [BL10, BXM03].

Proximal gradient method.
If a function takes on the value ∞, it need not have a
subgradient at that point, which limits the gradient update to cases where the regularizer

45

and loss are (ﬁnite) real-valued. When the regularizer (but not the loss) takes on inﬁnite
values (say, to represent a hard constraint), we can use a proximal gradient method instead.

The proximal operator of a function f [PB13] is

proxf (z) = argmin

(f (x) +

(cid:107)x − z(cid:107)2

2).

x

1
2

If f is the indicator function of a set C, the proximal operator of f is just (Euclidean)
projection onto C.

A proximal gradient update updateL,r is implemented as follows. Let

Then set

(cid:88)

g =

j:(i,j)∈Ω

∇Lij(xt−1

i

yt−1
j

, Aij)yt−1

.

j

i = proxαtr(xt−1
xt

i − αtg),

for some step size αt. The step size rule αt = 1/t guarantees convergence to the globally
optimal X if Y is ﬁxed, while using a ﬁxed, but suﬃciently small, step size α guarantees
convergence to a small O(α) neighborhood around the optimum [Ber11]. The technical
condition required on the step size is that αt < 1/L, where L is the Lipshitz constant of
the gradient of the objective function. Bolte et al. have shown that the iterates xt
i and
yt
j produced by the proximal gradient update rule (which they call proximal alternating
linearized minimization, or PALM) globally converge to a critical point of the objective
function under very mild conditions on L, r, and ˜r [BST13].

Prox-prox method. Letting ft(X) = (cid:80)
(prox-prox) update

(i,j)∈Ω Lij(xiyt

j, Aij), deﬁne the proximal-proximal

X t+1 = proxαtr(proxαtft(X t)).

The prox-prox update is simply a proximal gradient step on the objective when f is

replaced by the Moreau envelope of f ,

Mf (X) = inf
X (cid:48)

(cid:0)f (X (cid:48)) + (cid:107)X − X (cid:48)(cid:107)2

(cid:1) .

F

(See [PB13] for details.) The Moreau envelope has the same minimizers as the original
objective. Thus, just as the proximal gradient method repeatedly applied to X converges to
global minimum of the objective if Y is ﬁxed, the prox-prox method repeatedly applied to
X also converges to global minimum of the objective if Y is ﬁxed under the same conditions
on the step size αt. for any constant stepsize α ≤ (cid:107)G(cid:107)2
2. (Here, (cid:107)G(cid:107)2 = sup(cid:107)x(cid:107)2≤1 (cid:107)Gx(cid:107)2 is
the operator norm of G.)

This update can also be seen as a single iteration of ADMM when the dual variable
in ADMM is initialized to 0; see [BPC+11].
In the case of quadratic objectives, we will
see below that the prox-prox update can be applied very eﬃciently, making iterated prox-
prox, or ADMM, eﬀective means of computing the solution to the subproblems arising in
alternating minimization.

46

In numerical experiments, we ﬁnd that using a slightly more
Choosing a step size.
nuanced rule allowing diﬀerent step sizes for diﬀerent rows and columns can allow fast
progress towards convergence while ensuring that the value of the objective never increases.
The safeguards on step sizes we propose are quite important in practice: without these
checks, we observe divergence when the initial step sizes are chosen too large.

Motivated by the convergence proof in [Ber11], for each row, we seek a step size on
the order of 1/(cid:107)gi(cid:107)2, where gi is the gradient of the objective function with respect to xi.
We start by choosing an initial step size scale αi for each row of the same order as the
average gradient of the loss functions for that row. In the numerical experiments reported
here, we choose αi = 1 for i = 1, . . . , m. Since gi grows with the number of observations
ni = |{j : (i, j) ∈ Ω}| in row i, we achieve the desired scaling by setting α0
i = αi/ni. We
take a gradient step on each row xi using the step size αi. Our procedure for choosing α0
j is
the same.

We then check whether the objective value for the row,

(cid:88)

j:(i,j)∈Ω

Lj(xiyj, Aij) + γ(cid:107)xi(cid:107)2
2,

has increased or decreased. If it has increased, then we trust our ﬁrst order approximation
to the objective function less far, and reduce the step size; if it has decreased, we gain
conﬁdence, and increase the step size.
In the numerical experiments reported below, we
decrease the step size by 30% when the objective increases, and increase the step size by 5%
when the objective decreases. This check stabilizes the algorithm and prevents divergence
even when the initial scale has been chosen poorly.

We then do the same with respect to each column yj: we take a gradient step, check if

the objective value for the column has increased or decreased, and adjust the step size.

The time per iteration is thus O(k(m + n + |Ω|)): computing the gradient of the ith loss
function with respect to xi takes time O(kni); computing the proximal operator of the square
loss takes time O(k); summing these over all the rows i = 1, . . . , m gives time O(k(m + |Ω|));
and adding the same costs for the column updates gives time O(k(m + n + |Ω|)). The checks
on the objective value take time O(k) per observation (to compute the inner product xiyj
and value of the loss function for each observation) and time O(1) per row and column to
compute the value of the regularizer. Hence the total time per iteration is O(k(m + n + |Ω|)).
By partitioning the job of updating diﬀerent rows and diﬀerent columns onto diﬀerent

processors, we can achieve an iteration time of O(k(m + n + |Ω|)/p) using p processors.

Stochastic gradients.
Instead of computing the full gradient of L with respect to xi
above, we can replace the gradient g in either the gradient or proximal gradient method by
any stochastic gradient g, which is a vector that satisﬁes

E g =

(cid:88)

j:(i,j)∈Ω

∇Lij(xiyj, Aij)yj.

47

A stochastic gradient can be computed by sampling j uniformly at random from among
observed features of i, and setting g = |{j : (i, j) ∈ Ω}|∇Lij(xiyj, Aij)yj. More samples from
{j : (i, j) ∈ Ω} can be used to compute a less noisy stochastic gradient.

7.3 Quadratic objectives

Here we describe how to eﬃciently implement the prox-prox update rule for quadratic ob-
jectives and arbitrary regularizers, extending the factorization caching technique introduced
in §2.3. We assume here that the objective is given by

(cid:107)A − XY (cid:107)2

F + r(X) + ˜r(Y ).

We will concentrate here on the X update; as always, the Y update is exactly analogous.

As in the case of quadratic regularization, we ﬁrst form the Gram matrix G = Y Y T .

Then the proximal gradient update is fast to evaluate:

proxαkr(X − αk(XG − 2AY T )).

But we can take advantage of the ease of inverting the Gram matrix G to design a
faster algorithm using the prox-prox update. For quadratic objectives with Gram matrix
G = Y T Y , the prox-prox update takes the simple form

proxαkr((G +

I)−1(AY T +

X)).

1
αk

1
αk

As in §2.3, we can compute (G + 1
αk
ization of (G + 1
αk
updating Y , since most of the computational eﬀort is in forming G and AY T .
For example, in the case of nonnegative least squares, this update is just

X) in parallel by ﬁrst caching the factor-
I)−1. Hence it is advantageous to repeat this update many times before

I)−1(AY T + 1
αk

Π+((G +

I)−1(AY T +

X)),

1
αk

1
αk

where Π+ projects its argument onto the nonnegative orthant.

7.4 Convergence

Alternating minimization need not converge to the same model (or the same objective value)
when initialized at diﬀerent starting points. Through examples, we explore this idea here.
These examples are ﬁt using the serial Julia implementation (presented in §9) of the alter-
nating proximal gradient updates method.

48

Global convergence for quadratically regularized PCA. Figure 12 shows the con-
vergence of the alternating proximal gradient update method on a quadratically regularized
PCA problem with randomly generated, fully observed data A = X trueY true, where entries
of X true and Y true are drawn from a standard normal distribution. We pick ﬁve diﬀerent
random initializations of X and Y with standard normal entries to generate ﬁve diﬀerent
convergence trajectories. Quadratically regularized PCA is a simple problem with an ana-
lytical solution (see §2), and with no local minima (see Appendix A). Hence it should come
as no surprise that the trajectories all converge to the same, globally optimal value.

Local convergence for nonnegative matrix factorization. Figure 13 shows conver-
gence of the same algorithm on a nonnegative matrix factorization model, with data gener-
ated in the same way as in Figure 12. (Note that A has some negative entries, so the minimal
objective value is strictly greater than zero.) Here, we plot the convergence of the objective
value, rather than the suboptimality, since we cannot provably compute the global minimum
of the objective function. We see that the algorithm converges to a diﬀerent optimal value
(and point) depending on the initialization of X and Y . Three trajectories converge to the
same optimal value (though one does so much faster than the others), one to a value that is
somewhat better, and one to a value that is substantially worse.

7.5 Initialization

Alternating minimization need not converge to the same solution (or the same objective
value) when initialized at diﬀerent starting points. Above, we saw that alternating mini-
mization can converge to models with optimal values that diﬀer signiﬁcantly.

Here, we discuss two approaches to initialization that result in provably good solutions,
for special cases of the generalized low rank problem. We then discuss how to apply these
initialization schemes to more general models.

SVD. A literature that is by now extensive shows that the SVD provides a provably good
initialization for the quadratic matrix completion problem (10) [KMO09, KMO10, KM10,
JNS13, Har13, GAGG13]. Algorithms based on alternating minimization have been shown
to converge quickly (even geometrically [JNS13]) to a global solution satisfying a recovery
guarantee when the initial values of X and Y are chosen carefully; see §2.4 for more details.
Here, we extend the SVD initialization previously proposed for matrix completion to one
that works well for all PCA-like problems: problems with convex loss functions that have
been scaled as in §4.3; with data A that consists of real values, Booleans, categoricals, and
ordinals; and with quadratic (or no) regularization.

But we will need a matrix on which to perform the SVD. What matrix corresponds to our
data table? Here, we give a simple proposal for how to construct such a matrix, motivated
by [KMO10, JNS13, Cha14]. Our key insight is that the SVD is the solution to our problem
when the entries in the table have mean zero and variance one (and all the loss functions are

49

y
t
i
l
a
m

i
t
p
o
b
u
s

e
v
i
t
c
e
j
b
o

105

104

103

102

101

100

0

1

2

3

time (s)

Figure 12: Convergence of alternating proximal gradient updates on quadratically
regularized PCA for n = m = 200, k = 2.

quadratic). Our initialization will construct a matrix with mean zero and variance one from
the data table, take its SVD, and invert the construction to produce the correct initialization.
Our ﬁrst step is to expand the categorical columns taking on d values into d Boolean
columns, and to re-interpret ordinal and Boolean columns as numbers. The scaling we
propose below is insensitive to the values of the numbers in the expansion of the Booleans: for
example, using (false, true)= (0, 1) or (false, true)= (−1, 1) produces the same initialization.
The scaling is sensitive to the diﬀerences between ordinal values: while encoding (never,
sometimes, always) as (1, 2, 3) or as (−5, 0, 5) will make no diﬀerence, encoding these ordinals
as (0, 1, 10) will result in a diﬀerent initialization.

Now we assume that the rows of the data table are independent and identically dis-
tributed, so they each have equal means and variances. Our mission is to standardize the

50

·104

e
u
l
a
v

e
v
i
t
c
e
j
b
o

1.8

1.7

1.6

1.5

1.4

1.3

0

1

2

3

time (s)

Figure 13: Convergence of alternating proximal gradient updates on NNMF for
n = m = 200, k = 2.

columns. The observed entries in column j have mean µj and variance σ2
j ,

µj = argmin

µ

σ2
j =

1
nj − 1

(cid:88)

i:(i,j)∈Ω
(cid:88)

i:(i,j)∈Ω

Lj(µ, Aij)

Lj(µj, Aij),

so the matrix whose (i, j)th entry is (Aij − µj)/σj for (i, j) ∈ Ω has columns whose observed
entries have mean 0 and variance 1.

Each missing entry can be safely replaced with 0 in the scaled version of the data without
changing the column mean. But the column variance will decrease to mj/m. If instead we
deﬁne

˜Aij =

(cid:26) m
σj mj
0

(Aij − µj)

(i, j) ∈ Ω
otherwise,

then the column will have mean 0 and variance 1.

51

Take the SVD U ΣV T of ˜A, and let ˜U ∈ Rm×k, ˜Σ ∈ Rk×k, and ˜V ∈ Rn×k denote these
matrices truncated to the top k singular vectors and values. We initialize X = ˜U ˜Σ1/2, and
Y = ˜Σ1/2 ˜V T diag(σ). The oﬀset row in the model is initialized with the means, i.e., the kth
column of X is ﬁlled with 1’s, and the kth row of Y is ﬁlled with the means, so Ykj = µj.

Finally, note that we need not compute the full SVD of ˜A, but instead can simply compute
the top k singular triples. For example, the randomized top k SVD algorithm proposed in
[HMT11] computes the top k singular triples of ˜A in time linear in |Ω|, m, and n (and
quadratic in k).

Figure 14 compares the convergence of this SVD-based initialization with random ini-
tialization on a low rank model for census data described in detail in §6.3. We initialize the
algorithm at six diﬀerent points: from ﬁve diﬀerent random normal initializations (entries
of X 0 and Y 0 drawn iid from N (0, 1)), and from the SVD of ˜A. The SVD initialization
produces a better initial value for the objective function, and also allows the algorithm to
converge to a substantially lower ﬁnal objective value than can be found from any of the ﬁve
random starting points. This behaviour indicates that the “good” local minimum discovered
by the SVD initialization is located in a basin of attraction that has low probability with
respect to the measure induced by random normal initialization.

k-means++. The k-means++ algorithm is an initialization scheme designed for quadratic
clustering problems [AV07]. It consists of choosing an initial cluster centroid at random from
the points, and then choosing the remaining k − 1 centroids from the points x that have
not yet been chosen with probability proportional to D(x)2, where D(x) is the minimum
distance of x to any previously chosen centroid.

Quadratic clustering is known to be NP-hard, even with only two clusters (k = 2)
[DFK+04]. However, k-means++ followed by alternating minimization gives a solution with
expected approximation ratio within O(log k) of the optimal value [AV07]. (Here, the expec-
tation is over the randomization in the initialization algorithm.) In contrast, an arbitrary
initialization of the cluster centers for k-means can result in a solution whose value is arbi-
trarily worse than the true optimum.

A similar idea can be used for other low rank models. If the model rewards a solution
that is spread out, as is the case in quadratic clustering or subspace clustering, it may be
better to initialize the algorithm by choosing elements with probability proportional to a
distance measure, as in k-means++.
In the k-means++ procedure, one can use the loss
function L(u) as the distance metric D.

7.6 Global optimality

All generalized low rank models are non-convex, but some are more non-convex than others.
In particular, for some problems, the only important source of non-convexity is the low rank
constraint. For these problems, it is sometimes possible to certify global optimality of a
model by considering an equivalent rank-constrained convex problem.

The arguments in this section are similar to ones found in [RFP10], in which Recht et al.
propose using a factored (nonconvex) formulation of the (convex) nuclear norm regularized

52

random
random
random
random
random
SVD

·105

e
u
l
a
v

e
v
i
t
c
e
j
b
o

9

8

7

6

5

4

3

2

0

5

10

15

30

35

40

45

50

20

25
iteration

Figure 14: Convergence from ﬁve diﬀerent random initializations, and from the
SVD initialization.

53

estimator in order to eﬃciently solve the large-scale SDP arising in a matrix completion
problem. However, the algorithm in [RFP10] relies on a subroutine for ﬁnding a local
minimum of an augmented Lagrangian which has the same biconvex form as problem (10).
Finding a local minimum of this problem (rather than a saddle point) may be hard. In this
section, we avoid the issue of ﬁnding a local minimum of the nonconvex problem; we consider
instead whether it is possible to verify global optimality when presented with some putative
solution.

The factored problem is equivalent to the rank constrained problem. Consider
the factored problem

minimize L(XY ) + γ

2 (cid:107)X(cid:107)2

F + γ

2 (cid:107)Y (cid:107)2
F ,

with variables X ∈ Rm×k, Y ∈ Rk×n, where L : Rm×n → R is any convex loss function.
Compare this to the rank-constrained problem

minimize L(Z) + γ(cid:107)Z(cid:107)∗
subject to Rank(Z) ≤ k.

with variable Z ∈ Rm×n. Here, we use (cid:107) · (cid:107)∗ to denote the nuclear norm, the sum of the
singular values of a matrix.

Theorem 1. (X (cid:63), Y (cid:63)) is a solution to the factored problem 22 if and only if Z (cid:63) = X (cid:63)Y (cid:63) is
a solution to the rank-constrained problem 23, and (cid:107)X (cid:63)(cid:107)2

F = (cid:107)Y (cid:63)(cid:107)2

F = 1

2(cid:107)Z (cid:63)(cid:107)∗.

We will need the following lemmas to understand the relation between the rank-constrained

problem and the factored problem.

Lemma 1. Let XY = U ΣV T be the SVD of XY , where Σ = diag(σ). Then

(22)

(23)

(24)

Proof. We may derive this fact as follows:

(cid:107)σ(cid:107)1 ≤

(||X||2

F + ||Y ||2

F ).

1
2

(cid:107)σ(cid:107)1 = tr(U T XY V )

≤ (cid:107)U T X(cid:107)F (cid:107)Y V (cid:107)F
≤ (cid:107)X(cid:107)F (cid:107)Y (cid:107)F

≤

(||X||2

F + ||Y ||2

F ),

1
2

where the ﬁrst inequality above uses the Cauchy-Schwartz inequality, the second relies on the
orthogonal invariance of the Frobenius norm, and the third follows from the basic inequality
ab ≤ 1

2(a2 + b2) for any real numbers a and b.

54

Lemma 2. For any matrix Z, (cid:107)Z(cid:107)∗ = inf XY =Z

1

2(||X||2

F + ||Y ||2

F ).

Proof. Writing Z = U DV T and recalling the deﬁnition of the nuclear norm (cid:107)Z(cid:107)∗ = (cid:107)σ(cid:107)1,
we see that Lemma 1 implies

(cid:107)Z(cid:107)∗ ≤ inf

XY =Z

(||X||2

F + ||Y ||2

F ).

1
2

But taking X = U Σ1/2 and Y = Σ1/2V T , we have

1
2

1
2

(||X||2

F + ||Y ||2

F ) =

((cid:107)Σ1/2(cid:107)2

F + (cid:107)Σ1/2(cid:107)2

F ) = (cid:107)σ(cid:107)1,

(using once again the orthogonal invariance of the Frobenius norm), so the bound is satisﬁed
with equality.

Note that the inﬁmum is achieved by X = U Σ1/2T and Y = T T Σ1/2V T for any orthonor-

mal matrix T .

Theorem 1 follows as a corollary, since L(Z) = L(XY ) so long as Z = XY .

The rank constrained problem is sometimes equivalent to an unconstrained prob-
lem. Note that problem (23) is still a hard problem to solve:
it is a rank-constrained
semideﬁnite program. On the other hand, the same problem without the rank constraint is
convex and tractable (though not easy to solve at scale). In particular, it is possible to write
down an optimality condition for the problem

minimize L(Z) + γ(cid:107)Z(cid:107)∗

(25)

that certiﬁes that a matrix Z is globally optimal. This problem is a relaxation of prob-
lem (23), and so has an optimal value that is at least as small. Furthermore, if any solution
to problem (25) has rank no more than k, then it is feasible for problem (23), so the op-
timal values of problem (25) and problem (23) must be the same. Hence any solution of
problem (25) with rank no more than k also solves problem 23.

Recall that the matrix Z is a solution the problem U if and only if

0 ∈ ∂(L(Z) + γ(cid:107)Z(cid:107)∗),

where ∂f (Z) is the subgradient of the function f at Z. The subgradient is a set-valued
function.

The subgradient of the nuclear norm at a matrix Z = U ΣV T is any matrix of the form
U V T + W where U T W = 0, W V = 0, and (cid:107)W (cid:107)2 ≤ 1. Equivalently, deﬁne the set-valued
function sign on scalar arguments x as

sign(x) =






{1}
x > 0
[ − 1, 1] x = 0
x < 0,
{−1}

,

55

and deﬁne (sign(x))i = sign(xi) for vectors x ∈ Rn. Then we can write the subgradient of
the nuclear norm at Z as

∂(cid:107)Z(cid:107)∗ = U diag(sign(σ))V T ,
where now we use the full SVD of Z with U ∈ Rm×min(m,n), V ∈ Rn×min(m,n), and σ ∈
Rmin(m,n).

Hence Z = U ΣV T is a solution to problem (25) if and only if

0 ∈ ∂L(Z) + γ(U V T + W ),

or more simply, if

(cid:107)(1/γ)G + U V T (cid:107)2 ≤ 1

(26)

for some G ∈ ∂L(Z). In particular, if a matrix Z with rank no more than k satisﬁes (26),
then Z also solves the rank-constrained problem (23).

This result allows us to (sometimes) certify global optimality of a particular model.
Given a model (X, Y ), we compute the SVD of the product XY = U ΣV T and an element
G ∈ ∂L(Z). If (cid:107)(1/γ)G + U V T (cid:107)2 ≤ 1, then (X, Y ) is globally optimal. (If the objective is
diﬀerentiable then we simply pick G = ∇L(Z); otherwise some choices of G ∈ ∂L(Z) may
produce invalid certiﬁcates even if (X, Y ) is globally optimal.)

8 Choosing low rank models

8.1 Regularization paths

Suppose that we wish to understand the entire regularization path for a GLRM; that is, we
would like to know the solution (X(γ), Y (γ)) to the problem

minimize (cid:80)

(i,j)∈Ω Lij(xiyj, Aij) + γ (cid:80)m

i=1 ri(xi) + γ (cid:80)n

j=1 ˜rj(yj)

as a function of γ. Frequently, the regularization path may be computed almost as quickly
as the solution for a single value of γ. We can achieve this by initially ﬁtting the model
with a very high value for γ, which is often a very easy problem. (For example, when r
and ˜r are norms, the solution is (X, Y ) = (0, 0) for suﬃciently large γ.) Then we may
ﬁt models corresponding to smaller and smaller values of γ by initializing the alternating
minimization algorithm from our previous solution. This procedure is sometimes called a
homotopy method.

For example, Figure 15 shows the regularization path for quadratically regularized Huber
PCA on a synthetic data set. We generate a dataset A = XY +S with X ∈ Rm×k, Y ∈ Rk×n,
and S ∈ Rm×n, with m = n = 300 and k = 3. The entries of X and Y are drawn from a
standard normal distribution, while the entries of the sparse noise matrix S are drawn from
a uniform distribution on [0, 1] with probability 0.05, and are 0 otherwise. We ﬁt a rank 5
GLRM to an observation set Ω consisting of 10% of the entries in the matrix, drawn uniformly

56

at random from {1, . . . , i} × {1, . . . , j}, using Huber loss and quadratic regularization, and
vary the regularization parameter. That is, we ﬁt the model

minimize (cid:80)

(i,j)∈Ω huber(xiyj, Aij) + γ (cid:80)m

i=1 (cid:107)xi(cid:107)2

2 + γ (cid:80)n

j=1 (cid:107)yj(cid:107)2
2

and vary the regularization parameter γ. The ﬁgure plots both the normalized training error,

and the normalized test error,

1
|Ω|

(cid:88)

(i,j)∈Ω

huber(xiyj, Aij),

1
nm − |Ω|

(cid:88)

(i,j)(cid:54)∈Ω

huber(xiyj, Aij),

of the ﬁtted model (X, Y ), for γ ranging from 0 to 3. Here, we see that while the training error
decreases and γ decreases, the test error reaches a minimum around γ = .5. Interestingly,
it takes only three times longer (about 3 seconds) to generate the entire regularization path
than it does to ﬁt the model for a single value of the regularization parameter (about 1
second).

test error
train error

0.35

0.3

0.25

0.2

0.15

0.1

r
o
r
r
e

d
e
z
i
l
a
m
r
o
n

0

0.5

1

2

2.5

3

Figure 15: Regularization path.

1.5
γ

57

8.2 Choosing model parameters

To form a generalized low rank model, one needs to specify the loss functions Lj, regularizers
r and ˜r, and a rank k. The loss function should usually be chosen by a domain expert to
reﬂect the intuitive notion of what it means to “ﬁt the data well”. On the other hand, the
regularizers and rank are often chosen based on statistical considerations, so that the model
generalizes well to unseen (missing) data.

There are three major considerations to balance in choosing the regularization and rank
In the following discussion, we suppose that the regularizers r = γr0 and

of the model.
˜r = γ˜r0 have been chosen up to a scaling γ.

Compression. A low rank model (X, Y ) with rank k and no sparsity represents the data
table A with only (m + n)k numbers, achieving a compression ratio of (m + n)k/(mn). If the
factors X or Y are sparse, then we have used fewer than (m + n)k numbers to represent the
data A, achieving a higher compression ratio. We may want to pick parameters of the model
(k and γ) in order to achieve a good error (cid:80)
(i,j)∈Ω Lj(Aij − xiyj) for a given compression
ratio. For each possible combination of model parameters, we can ﬁt a low rank model with
those parameters, observing both the error and the compression ratio. We can then choose
the best model parameters (highest compression rate) achieving the error we require, or the
best model parameters (lowest error rate) achieving the compression we require.

More formally, one can construct an information criterion for low rank models by analogy
with the Aikake Information Criterion (AIC) or the Bayesian Information Criterion (BIC).
For use in the AIC, the number of degrees of freedom in a low rank model can be computed
as the diﬀerence between the number of nonzeros in the model and the dimensionality of the
symmetry group of the problem. For example, if the model (X, Y ) is dense, and the regu-
larizer is invariant under orthogonal transformations (e.g., r(x) = (cid:107)x(cid:107)2
2), then the number
of degrees of freedom is (m + n)k − k2 [TB99]. Minka [Min01] proposes a method based on
the BIC to automatically choose the dimensionality in PCA, and observes that it performs
better than cross validation in identifying the true rank of the model when the number of
observations is small (m, n (cid:46) 100).

Denoising. Suppose we observe every entry in a true data matrix contaminated by noise,
e.g., Aij = Atrue
ij + (cid:15)ij, with (cid:15)ij some random variable. We may wish to choose model
parameters to identify the truth and remove the noise: we would like to ﬁnd k and γ to
minimize (cid:80)

(i,j)∈Ω Lj(Atrue

ij − xiyj).

A number of commonly used rules-of-thumb have been proposed in the case of PCA to
distinguish the signal (the true rank k of the data) from the noise, some of which can be
generalized to other low rank models. These include using scree plots, often known as the
“elbow method” [Cat66]; the eigenvalue method; Horn’s parallel analysis [Hor65, Din09];
and other related methods [ZV86, PM03]. A recent, more sophisticated method adapts the
idea of dropout training [SHK+14] to regularize low-rank matrix estimation [JW14].

Some of these methods can easily be adapted to the GLRM context. The “elbow method”
increases k until the objective value decreases less than linearly; the eigenvalue method

58

increases k until the objective value decreases by less than some threshold; Horn’s parallel
analysis increases k until the objective value compares unfavorably to one generated by
ﬁtting a model to data drawn from a synthetic noise distribution.

Cross validation is also simple to apply, and is discussed further below as a means of
predicting missing entries. However, applying cross validation to the denoising problem is
somewhat tricky, since leaving out too few entries results in overﬁtting to the noise, while
leaving out too many results in underﬁtting to the signal. The optimal number of entries to
leave out may depend on the aspect ratio of the data, as well as on the type of noise present
in the data [Per09], and is not well understood except in the case of Gaussian noise [OP09].
We explore the problem of choosing a holdout size numerically below.

Predicting missing entries. Suppose we observe some entries in the matrix and wish
to predict the others. A GLRM with a higher rank will always be able to ﬁt the (noisy)
data better than one of lower rank. However, a model with many parameters may also
overﬁt to the noise. Similarly, a GLRM with no regularization (γ = 0) will always produce
a model with a lower empirical loss (cid:80)
(i,j)∈Ω Lj(xiyj, Aij). Hence, we cannot pick a rank k or
regularization γ simply by considering the objective value obtained by ﬁtting the low rank
model.

But by resampling from the data, we can simulate the performance of the model on out
of sample (missing) data to identify GLRMs that neither over nor underﬁt. Here, we discuss
a few methods for choosing model parameters by cross-validation; that is, by resampling
from the data to evaluate the model’s performance. Cross validation is commonly used in
regression models to choose parameters such as the regularization parameter γ, as in Figure
15. In GLRMs, cross validation can also be used to choose the rank k. Indeed, using a lower
rank k can be considered another form of model regularization.

We can distinguish between three sources of noise or variability in the data, which give

rise to three diﬀerent resampling procedures.

• The rows or columns of the data are chosen at random, i.e., drawn iid from some

population. In this case it makes sense to resample the rows or columns.

• The rows or columns may be ﬁxed, but the indices of the observed entries in the matrix
are chosen at random. In this case, it makes sense to resample from the observed entries
in the matrix.

• The indices of the observed entries are ﬁxed, but the values are observed with some
measurement error. In this case, it makes sense to resample the errors in the model.

Each of these leads to a diﬀerent reasonable kind of resampling scheme. The ﬁrst two
give rise to resampling schemes based on cross validation (i.e., resampling the rows, columns,
or individual entries of the matrix) which we discuss further below. The third gives rise to
resampling schemes based on the bootstrap or jackknife procedures, which resample from
the errors or residuals after ﬁtting the model. A number of methods using the third kind

59

of resampling have been proposed in order to perform inference (i.e., generate conﬁdence
intervals) for PCA; see Josse et al. [JWH14] and references therein.

As an example, let’s explore the eﬀect of varying |Ω|/mn, γ, and k. We generate random
, Y ∈ Rktrue×n, and S ∈ Rm×n, with m = n = 300 and
data as follows. Let X ∈ Rm×ktrue
ktrue = 3,. Draw the entries of X and Y from a standard normal distribution, and draw the
entries of the sparse outlier matrix S are drawn from a uniform distribution on [0, 3] with
probability 0.05, and are 0 otherwise. Form A = XY + S. Select an observation set Ω by
picking entries in the matrix uniformly at random from {1, . . . , n} × {1, . . . , m}. We ﬁt a
rank k GLRM with Huber loss and quadratic regularization γ(cid:107) · (cid:107)2
2, varying |Ω|/mn, γ, and
k, and compute the test error. We average our results over 5 draws from the distribution
generating the data.

In Figure 16, we see that the true rank k = 3 performs best on cross-validated error for
any number of observations |Ω|. (Here, we show performance for γ = 0. The plot for other
values of the regularization parameter is qualitatively the same.) Interestingly, it is easiest to
identify the true rank with a small number of observations: higher numbers of observations
make it more diﬃcult to overﬁt to the data even when allowing higher ranks.

|Ω|/mn=0.1
|Ω|/mn=0.3
|Ω|/mn=0.5
|Ω|/mn=0.7
|Ω|/mn=0.9

r
o
r
r
e

t
s
e
t

d
e
z
i
l
a
m
r
o
n

1

0.8

0.6

0.4

0.2

0

1

2

4

5

Figure 16: Test error as a function of k, for γ = 0.

In Figure 17, we consider the interdependence of our choice of γ and k. Regularization is
most important when few matrix elements have been observed: the curve for each k is nearly
ﬂat when more than about 10% of the entries have been observed, so we show here a plot

3
k

60

for |Ω| = .1mn. Here, we see that the true rank k = 3 performs best on cross-validated error
for any value of the regularization parameter. Ranks that are too high (k > 3) beneﬁt from
increased regularization γ, whereas higher regularization hurts the performance of models
with k lower than the true rank. That is, regularizing the rank (small k) can substitute for
explicit regularization of the factors (large γ).

k=1
k=2
k=3
k=4
k=5

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

r
o
r
r
e

t
s
e
t

d
e
z
i
l
a
m
r
o
n

0

1

2

3

4

5

γ

Figure 17: Test error as a function of γ when 10% of entries are observed.

Finally, in Figure 18 we consider how the ﬁt of the model depends on the number of
observations. If we correctly guess the rank k = 3, we ﬁnd that the ﬁt is insensitive to the
number of observations. If our rank is either too high or too low, the ﬁt improves with more
observations.

8.3 On-line optimization

Suppose that new examples or features are being added to our data set continuously, and we
wish to perform on-line optimization, which means that we should have a good estimate at
any time for the representations of those examples xi or features yj which we have seen. This
model is equivalent to adding new rows or columns to the data table A as the algorithm
continues.
In this setting, alternating minimization performs quite well, and has a very
natural interpretation. Given an estimate for Y , when a new example is observed in row i,

61

k=1
k=2
k=3
k=4
k=5

r
o
r
r
e

t
s
e
t

d
e
z
i
l
a
m
r
o
n

1

0.8

0.6

0.4

0.2

0

0.1

0.3

0.7

0.9

0.5
|Ω|/mn

Figure 18: Test error as a function of observations |Ω|/mn, for γ = 0.

we may solve

minimize (cid:80)

j:(i,j)∈Ω Lij(Aij, xyj) + r(x)

with variable x to compute a representation for row i. This computation is exactly the same
as one step of alternating minimization. Here, we are ﬁnding the best feature representation
for the new example in terms of the (already well understood) archetypes Y . If the number
of other examples previously seen is large, the addition of a single new example should not
change the optimal Y by very much; hence if (X, Y ) was previously the global minimum of
(15), this estimate of the feature representation for the new example will be very close to its
optimal representation (i.e., the one that minimizes problem (15)). A similar interpretation
holds when new columns are added to A.

9

Implementations

The authors have developed and released three open source codes for modelling and ﬁtting
generalized low rank models: a basic serial implementation written in Python, a serial and
shared-memory parallel implementation written in Julia, and a distributed implementation
written in Scala using the Spark framework. The Julia and Spark implementations use the

62

alternating proximal gradient method described in §7 to ﬁt GLRMs, while the Python imple-
mentation uses alternating minimization and a cvxpy [DCB14] backend for each subproblem.
In this section we brieﬂy discuss these implementations, and report some timing results. For
a full description and up-to-date information about available functionality, we encourage the
reader to consult the on-line documentation for each of these packages.

There are also many implementations available for ﬁtting special cases of GLRMs. For
example, an implementation capable of ﬁtting any GLRM for which the subproblems in an
alternating minimization method are quadratic programs was recently developed in Spark
by Debasish Das [DD14].

9.1 Python implementation

GLRM.py is a Python implementation for ﬁtting GLRMs that can be found, together with
documentation, at

https://github.com/cehorn/glrm.

We encourage the interested reader to consult the on-line documentation for the most up-
to-date functionality and a collection of examples.

Usage. The user initializes a GLRM by specifying

• the data table A (A), stored as a Python list of 2-D arrays, where each 2-D array in A

contains all data associated with a particular loss function,

• the list of loss functions L (Lj, j = 1, . . . , n), that correspond to the data as speciﬁed

by A,

• regularizers regX (r) and regY (˜r),

• the rank k (k),

• an optional list missing_list with the same length as A so that each entry of missing_list

is a list of missing entries corresponding to the data from A, and

• an optional convergence object converge that characterizes the stopping criterion for

the alternating minimization procedure.

The following example illustrates how to use GLRM.py to ﬁt a GLRM with Boolean
(A_bool) and numerical (A_real) data, with quadratic regularization and a few missing
entries.

from glrm import GLRM
from glrm.loss import QuadraticLoss, HingeLoss
from glrm.reg import QuadraticReg

# import the model
# import losses
# import regularizer

63

A = [A_bool, A_real]
L = [Hinge_Loss, QuadraticLoss]
regX, regY = QuadraticReg(0.1), QuadraticReg(0.1)
missing_list = [[], [(0,0), (0,1)]]

# data stored as a list
# loss function as a list
# penalty weight is 0.1
# indexed by submatrix

model = GLRM(A, L, regX, regY, k, missing_list)
model.fit()

# initialize GLRM
# fit GLRM

The fit() method automatically adds an oﬀset to the GLRM and scales the loss functions
as described in §4.3.

GLRM.py ﬁts GLRMS by alternating minimization. The code instantiates cvxpy problems
[DCB14] corresponding to the X- and Y -update steps, then iterates by alternately solving
each problem until convergence criteria are met.

The following loss functions and regularizers are supported by GLRM.py:

• quadratic loss QuadraticLoss,

• Huber loss HuberLoss,

• hinge loss HingeLoss,

• ordinal loss OrdinalLoss,

• no regularization ZeroReg,

• (cid:96)1 regularization LinearReg,

• quadratic regularization QuadraticReg, and

• nonnegative constraint NonnegativeReg.

Users may implement their own loss functions (regularizers) using the abstract class Loss
(Reg).

9.2 Julia implementation

LowRankModels is a code written in Julia [BKSE12] for modelling and ﬁtting GLRMs. The
implementation is available on-line at

https://github.com/madeleineudell/LowRankModels.jl.

We discuss some aspects of the usage and features of the code here. For a full description
and up-to-date information about available functionality, we encourage the reader to consult
the on-line documentation.

64

Usage. To form a GLRM using LowRankModels, the user speciﬁes

• the data A (A), which can be any array or array-like data structure (e.g., a Julia

DataFrame);

• the observed entries obs (Ω), a list of tuples of the indices of the observed entries in
the matrix, which may be omitted if all the entries in the matrix have been observed;

• the list of loss functions losses (Lj, j = 1, . . . , n), one for each column of A;

• the regularizers rx (r) and ry (˜r); and

• the rank k (k).

For example, the following code forms and ﬁts a k-means model with k = 5 on the matrix
A ∈ Rm×n.

losses = fill(quadratic(),n)
rx = unitonesparse()
ry = zeroreg()
glrm = GLRM(A,losses,rx,ry,k) # form GLRM
X,Y,ch = fit!(glrm)

# fit GLRM

# quadratic loss
# x is 1-sparse unit vector
# y is not regularized

LowRankModels uses the proximal gradient method described in §7.2 to ﬁt GLRMs. The
optimal model is returned in the factors X and Y, while ch gives the convergence history. The
exclamation mark suﬃx follows the convention in Julia denoting that the function mutates
at least one of its arguments. In this case, it caches the best ﬁt X and Y as glrm.X and
glrm.Y [CE14].

Losses and regularizers must be of type Loss and Regularizer, respectively, and may

be chosen from a list of supported losses and regularizers, which include

• quadratic loss quadratic,

• hinge loss hinge,

• (cid:96)1 loss l1,

• Huber loss huber,

• ordinal hinge loss ordinal_hinge,

• quadratic regularization quadreg,

• no regularization zeroreg,

• nonnegative constraint nonnegative, and

• 1-sparse constraint onesparse.

• unit 1-sparse constraint unitonesparse.

Users may also implement their own losses and regularizers.

65

Shared memory parallelism. LowRankModels takes advantage of Julia’s SharedArray
data structure to implement a ﬁtting procedure that takes advantage of shared memory par-
allelism. While Julia does not yet support threading, SharedArrays in Julia allow separate
processes on the same computer to access the same block of memory. To ﬁt a model using
multiple processes, LowRankModels loads the data A and the initial model X and Y into
shared memory, broadcasts other problem data (e.g., the losses and regularizers) to each
process, and assigns to each process a partition of the rows of X and columns of Y . At every
iteration, each process updates its rows of X, its columns of Y , and computes its portion of
the objective function, synchronizing after each of these steps to ensure that e.g.the X up-
date is completed before the Y update begins; then the master process checks a convergence
criterion and adjusts the step length.

Automatic modeling. LowRankModels is capable of adding oﬀsets to a GLRM, and of
automatically scaling the loss functions, as described in §4.3. It can also automatically detect
the types of diﬀerent columns of a data frame and select an appropriate loss. Using these
features, LowRankModels implements a method

glrm(dataframe, k)

that forms a rank k model on a data frame, automatically selecting loss functions and
regularization that suit the data well, and ignoring any missing (NA) element in the data
frame. This GLRM can then be ﬁt with the function fit!.

Example. As an example, we ﬁt a GLRM to the Motivational States Questionnaire (MSQ)
data set [RA98]. This data set measures 3896 subjects on 92 aspects of mood and personality
type, as well as recording the time of day the data were collected. The data include real-
valued, Boolean, and ordinal measurements, and approximately 6% of the measurements are
missing (NA).

The following code loads the MSQ data set and encodes it in two dimensions:

using RDatasets
using LowRankModels
# pick a data set
df = RDatasets.dataset("psych","msq")
# encode it!
X,Y,labels,ch = fit(glrm(df,2))

Figure 19 uses the rows of Y as a coordinate system to plot some of the features of the
data set. Here we see the automatic embedding separates positive from negative emotions
along the y axis. This embedding is notable for being interpretable despite having been
generated completely automatically. Of course, better embeddings may be obtained by a
more careful choice of loss functions, regularizers, scaling, and embedding dimension k.

66

Figure 19: An automatic embedding of the MSQ [RA98] data set into two dimen-
sions.

9.3 Spark implementation

SparkGLRM is a code written in Scala, built on the Spark cluster programming framework
[ZCF+10], for modelling and ﬁtting GLRMs. The implementation is available on-line at

http://git.io/glrmspark.

In SparkGLRM, the data matrix A is split entry-wise across many machines, just
Design.
as in [HMLZ14]. The model (X, Y ) is replicated and stored in memory on every machine.

67

Thus the total computation time required to ﬁt the model is proportional to the number
of nonzeros divided by the number of cores, with the restriction that the model should ﬁt
in memory. (The authors leave to future work an extension to models that do not ﬁt in
memory, e.g., by using a parameter server [SSZ14].) Where possible, hardware acceleration
(via breeze and BLAS) is used for local linear algebraic operations.

At every iteration, the current model is broadcast to all machines, so there is only one
copy of the model on each machine. This particularly important in machines with many
cores, because it avoids duplicating the model those machines. Each core on a machine will
process a partition of the input matrix, using the local copy of the model.

Usage. The user provides loss functions Lij(u, a) indexed by i = 0, . . . , m − 1 and j =
0, . . . , n − 1, so a diﬀerent loss function can be deﬁned for each column, or even for each
entry. Each loss function is deﬁned by its gradient (or a subgradient). The method signature
is

loss grad(i: Int, j: Int, u: Double, a: Double)

whose implementation can be customized by particular i and j. As an example, the following
line implements squared error loss (L(u, a) = 1/2(u − a)2) for all entries:

Similarly, the user provides functions implementing the proximal operator of the regu-
larizers r and ˜r, which take a dense vector and perform the appropriate proximal operation.

Experiments. We ran experiments on several large matrices. For size comparison, a very
popular matrix in the recommender systems community is the Netﬂix Prize Matrix, which
has 17770 rows, 480189 columns, and 100480507 nonzeros. Below we report results on several
larger matrices, up to 10 times larger. The matrices are generated by ﬁxing the dimensions
and number of nonzeros per row, then uniformly sampling the locations for the nonzeros,
and ﬁnally ﬁlling in those locations with a uniform random number in [0, 1].

We report iteration times using an Amazon EC2 cluster with 10 slaves and one master,
of instance type “c3.4xlarge”. Each machine has 16 CPU cores and 30 GB of RAM. We
ran SparkGLRM to ﬁt two GLRMs on matrices of varying sizes. Table 5 gives results for
quadratically regularized PCA (i.e., quadratic loss and quadratic regularization) with k = 5.
To illustrate the capability to write and ﬁt custom loss functions, we also ﬁt a GLRM using
a loss function that depends on the parity of i + j:

Lij(u, a) =

(cid:26) |u − a|
(u − a)2

i + j is even
i + j is odd,

with r(x) = (cid:107)x(cid:107)1 and ˜r(y) = (cid:107)y(cid:107)2
2, setting k = 10. (This loss function was chosen merely to
illustrate the generality of the implementation. Usually losses will be the same for each row
in the same column.) The results for this custom GLRM are given in Table 6.

u - a

68

Matrix size # nonzeros Time per iteration (s)

106 × 106
106 × 106
107 × 107

106 × 106
106 × 106
107 × 107

106
109
109

106
109
109

Table 5: SparkGLRM for quadratically regularized PCA, k = 5.

Matrix size # nonzeros Time per iteration (s)

7
11
227

9
13
294

Table 6: SparkGLRM for custom GLRM, k = 10.

The table gives the time per iteration. The number of iterations required for convergence
depends on the size of the ambient dimension. On the matrices with the dimensions shown in
Tables 5 and 6, convergence typically requires about 100 iterations, but we note that useful
GLRMs often emerge after only a few tens of iterations.

Acknowledgements

The authors are grateful to Chris De Sa, Yash Deshpande, Nicolas Gillis, Maya Gupta,
Trevor Hastie, Irene Kaplow, Lester Mackey, Andrea Montanari, Art Owen, Haesun Park,
David Price, Chris R´e, Ben Recht, Yoram Singer, Nati Srebro, Ashok Srivastava, Peter
Stoica, Sze-chuan Suen, Stephen Taylor, Joel Tropp, Ben Van Roy, and Stefan Wager for
a number of illuminating discussions and comments on early drafts of this paper, and to
Debasish Das and Matei Zaharia for their insights into creating a successful Spark imple-
mentation. This work was developed with support from the National Science Foundation
Graduate Research Fellowship program (under Grant No. DGE-1147470), the Gabilan Stan-
ford Graduate Fellowship, the Gerald J. Lieberman Fellowship, and the DARPA X-DATA
program.

69

A Quadratically regularized PCA

In this appendix we describe some properties of the quadratically regularized PCA prob-
lem (3),

minimize (cid:107)A − XY (cid:107)2

F + γ(cid:107)X(cid:107)2

F + γ(cid:107)Y (cid:107)2
F .

(27)

In the sequel, we let U ΣV T = A be the SVD of A and let r be the rank of A. We assume for
convenience that all the nonzero singular values σ1 > σ2 > · · · > σr > 0 of A are distinct.

A.1 Solution

Problem (3) is the only problem we will encounter that has an analytical solution. A solution
is given by

X = ˜U ˜Σ1/2,

Y = ˜Σ1/2 ˜V T ,

(28)

where ˜U and ˜V are deﬁned as in (5), and ˜Σ = diag((σ1 − γ)+, . . . , (σk − γ)+).

To prove this, let’s consider the optimality conditions of (3). The optimality conditions

are

−(A − XY )Y T + γX = 0,

−(A − XY )T X + γY T = 0.

Multiplying the ﬁrst optimality condition on the left by X T and the second on the left by
Y and rearranging, we ﬁnd

X T (A − XY )Y T = γX T X,

Y (A − XY )T X = γY Y T ,

which shows, by taking a transpose, that X T X = Y Y T at any stationary point.

We may rewrite the optimality conditions together as

(cid:20)−γI

A
AT −γI

(cid:21) (cid:20) X
Y T

(cid:21)

(cid:21)

(cid:21) (cid:20) X
Y T

=

=

=

(cid:20)

0
XY
(XY )T
0
(cid:21)
(cid:20) X(Y Y T )
Y T (X T X)
(cid:20) X
Y T

(X T X),

(cid:21)

where we have used the fact that X T X = Y Y T .

Now we see that (X, Y T ) lies in an invariant subspace of the matrix

(cid:20)−γI

A
AT −γI

(cid:21)

. Recall

that V is an invariant subspace of a matrix A if AV = V M for some matrix M . If Rank(M ) ≤
Rank(A), we know that the eigenvalues of M are eigenvalues of A, and that the corresponding
eigenvectors lie in the span of V .

Thus the eigenvalues of X T X must be eigenvalues of

, and (X, Y T ) must

span the corresponding eigenspace. More concretely, notice that

is (symmetric,

(cid:21)

(cid:20)−γI

A
AT −γI
(cid:20)−γI

(cid:21)

A
AT −γI

70

and therefore) diagonalizable, with eigenvalues −γ ± σi. The larger eigenvalues −γ + σi
correspond to the eigenvectors (ui, vi), and the smaller ones −γ − σi to (ui, −vi).

Now, X T X is positive semideﬁnite, so the eigenvalues shared by X T X and

(cid:20)−γI

A
AT −γI

(cid:21)

√

must be positive. Hence there is some set |Ω| ≤ k with σi ≥ γ for i ∈ Ω such that X has
−γ + σi for i ∈ Ω. (Recall that X T X = Y Y T , so Y has the same
have singular values
singular values as X.) Then (X, Y T ) spans the subspace generated by the vectors (ui, vi for
i ∈ Ω. We say the stationary point (X, Y ) has active subspace Ω. It is easy to verify that
XY = (cid:80)

i∈Ω ui(σi − γ)vT
i .

Each active subspace gives rise to an orbit of stationary points. If (X, Y ) is a stationary

point, then (XT, T −1Y ) is also a stationary point so long as

−(A − XY )Y T T −T + γXT = 0,

−(A − XY )T XT + γY T T −T = 0,

which is always true if T −T = T , i.e., T is orthogonal. This shows that the set of stationary
points is invariant under orthogonal transformations.

To simplify what follows, we choose a representative element for each orbit. Represent

any stationary point with active subspace Ω by

X = UΩ(ΣΩ − γI)1/2,

Y = (ΣΩ − γI)1/2V T
Ω ,

where by UΩ we denote the submatrix of U with columns indexed by Ω, and similarly for
(cid:0)k(cid:48)(γ)
(cid:1)
Σ and V . At any value of γ, let k(cid:48)(γ) = max{i : σi ≥ γ}. Then we have (cid:80)k
i
(representative) stationary points, one for each choice of Ω The number of (representative)
stationary points is decreasing in γ; when γ > σ1, the only stationary point is X = 0, Y = 0.
These stationary points can have quite diﬀerent values. If (X, Y ) has active subspace Ω,

i=0

then

||A − XY ||2

F + γ(||X||2

F + ||Y ||2

F ) =

σ2
i +

(cid:0)γ2 + 2γ|σi − γ|(cid:1) .

(cid:88)

i /∈Ω

(cid:88)

i∈Ω

From this form, it is clear that we should choose Ω to include the top singular values i =
1, . . . , k(cid:48)(γ). Choosing any other subset Ω will result in a higher (worse) objective value:
that is, the other stationary points are not global minima.

A.2 Fixed points of alternating minimization

Theorem 2. The quadratically regularized PCA problem (3) has only one local minimum,
which is the global minimum.

Our proof is similar to that of [BH89], who proved a related theorem for the case of

PCA (2).
Proof. We showed above that every stationary point of (3) has the form XY = (cid:80)
i∈Ω uidivT
i ,
with Ω ⊆ {1, . . . , k(cid:48)}, |Ω| ≤ k, and di = σi − γ. We use the representative element from each
√
divT
stationary orbit described above, so each column of X is ui
i
for some i ∈ Ω. The columns of X are orthogonal, as are the rows of Y .

di and each row of Y is

√

71

If a stationary point is not the global minimum, then σj > σi for some i ∈ Ω, j (cid:54)∈ Ω.
Below, we show we can always ﬁnd a descent direction if this condition holds, thus showing
that the only local minimum is the global minimum.

Assume we are at a stationary point with σj > σi for some i ∈ Ω, j (cid:54)∈ Ω. We will ﬁnd a
j . Form ˜X by replacing the column of
i by

descent direction by perturbing XY in direction ujvT
X containing ui
√

di, and ˜Y by replacing the row of Y containing

di by (ui + (cid:15)uj)

divT

√

√

√

di(vi + (cid:15)vj)T . Now the regularization term increases slightly:

γ((cid:107) ˜X(cid:107)2

F + (cid:107) ˜Y (cid:107)2

F ) − γ((cid:107)X(cid:107)2

F + (cid:107)Y (cid:107)2

F ) =

(2γti(cid:48)) + 2γdi(1 + (cid:15)2) −

2γti(cid:48)

(cid:88)

i(cid:48)∈Ω

(cid:88)

i(cid:48)∈Ω,i(cid:48)(cid:54)=i
= 2γdi(cid:15)2.

Meanwhile, the approximation error decreases:

(cid:107)A − ˜X ˜Y (cid:107)2

F − (cid:107)A − XY (cid:107)2

F = (cid:107)uiσivT

i + ujσjvT

j − (ui + (cid:15)uj)di(vi + (cid:15)vj)T (cid:107)2

= (cid:107)ui(σi − di)vT

i + uj(σj − (cid:15)2di)vT

j − (cid:15)uidivT

F − (σi − di)2 − σ2
j
i (cid:107)2
F

j − (cid:15)ujdivT

−(σi − di)2 − σ2
j
(cid:13)
(cid:20)σi − di
−(cid:15)di
(cid:13)
(cid:13)
σj − (cid:15)2di
−(cid:15)di
(cid:13)

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
F

=

= (σi − di)2 + (σj − (cid:15)2di)2 + 2(cid:15)2d2
i + 2(cid:15)2d2
= −2σj(cid:15)2di + (cid:15)4d2
i
= 2(cid:15)2di(di − σj) + (cid:15)4d2
i ,

− (σi − di)2 − σ2
j

i − (σi − di)2 − σ2
j

where we have used the rotational invariance of the Frobenius norm to arrive at the third
equality above. Hence the net change in the objective value in going from (X, Y ) to ( ˜X, ˜Y )
is

2γdi(cid:15)2 + 2(cid:15)2di(di − σj) + (cid:15)4d2

i = 2(cid:15)2di(γ + di − σj) + (cid:15)4d2
i
= 2(cid:15)2di(σi − σj) + (cid:15)4d2
i ,

which is negative for small (cid:15). Hence we have found a descent direction, showing that any
stationary point with σj > σi for some i ∈ Ω, j (cid:54)∈ Ω is not a local minimum.

72

References

[AAJN13] A. Agarwal, A. Anandkumar, P. Jain, and P. Netrapalli. Learning sparsely
used overcomplete dictionaries via alternating minimization. arXiv preprint
arXiv:1310.7991, 2013.

[ABEV09] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. A new approach to collab-
orative ﬁltering: Operator estimation with spectral regularization. The Journal
of Machine Learning Research, 10:803–826, 2009.

[AEB06] M. Aharon, M. Elad, and A. Bruckstein. k-SVD: An algorithm for designing
overcomplete dictionaries for sparse representation. IEEE Transactions on Signal
Processing, 54(11):4311–4322, 2006.

[AM04]

[AV07]

P. K. Agarwal and N. H. Mustafa. k-means projective clustering. In Proceed-
ings of the 23rd ACM SIGMOD-SIGACT-SIGART Symposium on Principles of
Database Systems, pages 155–165. ACM, 2004.

D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding.
In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete
Algorithms, pages 1027–1035. Society for Industrial and Applied Mathematics,
2007.

[BBL+07] M. Berry, M. Browne, A. Langville, V. Pauca, and R. Plemmons. Algorithms and
applications for approximate nonnegative matrix factorization. Computational
Statistics & Data Analysis, 52(1):155–173, 2007.

[BCMR12] S. Boyd, C. Cortes, M. Mohri, and A. Radovanovic. Accuracy at the top. In

Advances in Neural Information Processing Systems, pages 962–970, 2012.

[BDKP14] R. Boyd, B. Drake, D. Kuang, and H. Park. Smallk is a C++/Python high-
performance software library for nonnegative matrix factorization (NMF) and
hierarchical and ﬂat clustering using the NMF; current version 1.2.0. http:
//smallk.github.io/, June 2014.

[Ber11]

[BH89]

D. P. Bertsekas. Incremental gradient, subgradient, and proximal methods for
convex optimization: A survey. Optimization for Machine Learning, 2010:1–38,
2011.

P. Baldi and K. Hornik. Neural networks and principal component analysis:
Learning from examples without local minima. Neural Networks, 2(1):53–58,
1989.

[BKSE12] J. Bezanson, S. Karpinski, V. B. Shah, and A. Edelman. Julia: A fast dynamic

language for technical computing. arXiv preprint arXiv:1209.5145, 2012.

73

[BL10]

J. Borwein and A. Lewis. Convex analysis and nonlinear optimization: theory
and examples, volume 3. Springer Science & Business Media, 2010.

[BM03a]

S. Boyd and J. Mattingley. Branch and bound methods. Lecture notes for
EE364b, Stanford University, 2003.

[BM03b]

S. Burer and R. Monteiro. A nonlinear programming algorithm for solving
semideﬁnite programs via low-rank factorization. Mathematical Programming,
95(2):329–357, 2003.

[BM03c]

S. Burer and R. D. C. Monteiro. Local minima and convergence in low-rank
semideﬁnite programming. Mathematical Programming, 103:2005, 2003.

[BPC+11] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimiza-
tion and statistical learning via the alternating direction method of multipliers.
Foundations and Trends in Machine Learning, 3(1):1–122, 2011.

[BRRT12] V. Bittorf, B. Recht, C. R´e, and J. A. Tropp. Factoring nonnegative matri-
ces with linear programs. Advances in Neural Information Processing Systems,
25:1223–1231, 2012.

[BST13]

J. Bolte, S. Sabach, and M. Teboulle. Proximal alternating linearized minimiza-
tion for nonconvex and nonsmooth problems. Mathematical Programming, pages
1–36, 2013.

[BV04]

S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University
Press, 2004.

[BXM03]

S. Boyd, L. Xiao, and A. Mutapcic. Subgradient methods. Lecture notes for
EE364b, Stanford University, 2003.

[Cat66]

Raymond B Cattell. The scree test for the number of factors. Multivariate
behavioral research, 1(2):245–276, 1966.

[CDS98]

S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit.
SIAM Journal on Scientiﬁc Computing, 20(1):33–61, 1998.

[CDS01] M. Collins, S. Dasgupta, and R. Schapire. A generalization of principal com-
ponent analysis to the exponential family. In Advances in Neural Information
Processing Systems, volume 13, page 23, 2001.

[CE14]

[Cha14]

J. Chen and A. Edelman. Parallel preﬁx polymorphism permits parallelization,
presentation & proof. arXiv preprint arXiv:1410.6449, 2014.

S. Chatterjee. Matrix estimation by universal singular value thresholding. The
Annals of Statistics, 43(1):177–214, 2014.

74

[CLMW11] E. Cand`es, X. Li, Y. Ma, and J. Wright. Robust principal component analysis?

Journal of the ACM (JACM), 58(3):11, 2011.

[CP09]

[CR08]

[CS02]

[CT10]

[DB95]

E. Cand`es and Y. Plan. Matrix completion with noise. CoRR, abs/0903.3131,
2009.

E. Cand`es and B. Recht. Exact matrix completion via convex optimization.
CoRR, abs/0805.4471, 2008.

K. Crammer and Y. Singer. On the algorithmic implementation of multiclass
kernel-based vector machines. The Journal of Machine Learning Research, 2:265–
292, 2002.

E. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix
completion. IEEE Transactions on Information Theory, 56(5):2053–2080, 2010.

T. Dietterich and G. Bakiri. Solving multiclass learning problems via error-
correcting output codes. CoRR, cs.AI/9501101, 1995.

[DCB14]

S. Diamond, E. Chu, and S. Boyd. CVXPY: A Python-embedded modeling
language for convex optimization, version 0.2. http://cvxpy.org/, May 2014.

[DD14]

D. Das and S. Das. Quadratic programing solver for non-negative matrix factor-
ization with spark. In Spark Summit 2014, 2014.

[dEGJL04] A. d’Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. Lanckriet. A direct
In Advances in

formulation for sparse PCA using semideﬁnite programming.
Neural Information Processing Systems, volume 16, pages 41–48, 2004.

[DFK+04] P. Drineas, A. Frieze, R. Kannan, S. Vempala, and V. Vinay. Clustering large
graphs via the singular value decomposition. Machine Learning, 56(1-3):9–33,
2004.

[Din09]

[DL84]

A. Dinno. Implementing Horn’s parallel analysis for principal component analysis
and factor analysis. Stata Journal, 9(2):291, 2009.

J. De Leeuw. The Giﬁ system of nonlinear multivariate analysis. Data analysis
and informatics, 3:415–424, 1984.

[DLM09]

J. De Leeuw and P. Mair. Giﬁ methods for optimal scaling in R: The package
homals. Journal of Statistical Software, pages 1–30, 2009.

[DLPP06] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal nonnegative matrix t-
factorizations for clustering. In Proceedings of the 12th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data Mining, pages 126–135.
ACM, 2006.

75

[DLYT76] J. De Leeuw, F. Young, and Y. Takane. Additive structure in qualitative data:
An alternating least squares method with optimal scaling features. Psychome-
trika, 41(4):471–503, 1976.

[DPBW12] M. Davenport, Y. Plan, E. Berg, and M. Wootters. 1-bit matrix completion.

arXiv preprint arXiv:1209.3672, 2012.

[DS14]

[EV09]

[EY36]

[FBD09]

A. Damle and Y. Sun. Random projections for non-negative matrix factorization.
arXiv preprint arXiv:1405.4275, 2014.

E. Elhamifar and R. Vidal. Sparse subspace clustering. In IEEE Conference on
Computer Vision and Pattern Recognition, 2009, pages 2790–2797. IEEE, 2009.

C. Eckart and G. Young. The approximation of one matrix by another of lower
rank. Psychometrika, 1(3):211–218, 1936.

C. F´evotte, N. Bertin, and J. Durrieu. Nonnegative matrix factorization with
the Itakura-Saito divergence: With application to music analysis. Neural Com-
putation, 21(3):793–830, 2009.

[FHB04] M. Fazel, H. Hindi, and S. Boyd. Rank minimization and applications in sys-
tem theory. In Proceedings of the 2004 American Control Conference (ACC),
volume 4, pages 3273–3278. IEEE, 2004.

[FM13]

W. Fithian and R. Mazumder. Scalable convex methods for ﬂexible low-rank
matrix modeling. arXiv preprint arXiv:1308.4211, 2013.

[GAGG13] S. Gunasekar, A. Acharya, N. Gaur, and J. Ghosh. Noisy matrix completion
using alternating minimization. In Machine Learning and Knowledge Discovery
in Databases, pages 194–209. Springer, 2013.

[GBW14] M. Gupta, S. Bengio, and J. Weston. Training highly multiclass classiﬁers. The

Journal of Machine Learning Research, 15(1):1461–1492, 2014.

[GD14]

A. Gress and I. Davidson. A ﬂexible framework for projecting heterogeneous
data. In Proceedings of the 23rd ACM International Conference on Conference
on Information and Knowledge Management, CIKM ’14, pages 1169–1178, New
York, NY, USA, 2014. ACM.

[GG11]

N. Gillis and F. Glineur. Low-rank matrix approximation with weights or
missing data is NP-hard. SIAM Journal on Matrix Analysis and Applications,
32(4):1149–1165, 2011.

[Gil11]

N. Gillis. Nonnegative matrix factorization: Complexity, algorithms and appli-
cations. PhD thesis, UCL, 2011.

76

[Gor02]

G. J. Gordon. Generalized2 linear2 models. In Advances in Neural Information
Processing Systems, pages 577–584, 2002.

[GRX+10] A. Goldberg, B. Recht, J. Xu, R. Nowak, and X. Zhu. Transduction with matrix
In Advances in Neural Information

completion: Three birds with one stone.
Processing Systems, pages 757–765, 2010.

[Har13]

M. Hardt. On the provable convergence of alternating minimization for matrix
completion. arXiv preprint arXiv:1312.0925, 2013.

[HMLZ14] T. Hastie, R. Mazumder, J. Lee, and R. Zadeh. Matrix completion and low-rank

svd via fast alternating least squares. arXiv, 2014.

[HMT11] N. Halko, P.-G. Martinsson, and J. Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions.
SIAM Review, 53(2):217–288, 2011.

[HN99]

[Hor65]

[Hot33]

[Hot36]

Z. Huang and M. Ng. A fuzzy k-modes algorithm for clustering categorical data.
IEEE Transactions on Fuzzy Systems, 7(4):446–452, 1999.

J. Horn. A rationale and test for the number of factors in factor analysis. Psy-
chometrika, 30(2):179–185, 1965.

H. Hotelling. Analysis of a complex of statistical variables into principal compo-
nents. Journal of Educational Psychology, 24(6):417, 1933.

H. Hotelling. Relations between two sets of variates. Biometrika, 28(3-4):321–
377, 1936.

[Hub81]

P. Huber. Robust Statistics. Wiley, New York, 1981.

[JBAS10] M. Journ´ee, F. Bach, P. Absil, and R. Sepulchre. Low-rank optimization on
the cone of positive semideﬁnite matrices. SIAM Journal on Optimization,
20(5):2327–2351, 2010.

[JNS13]

P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank matrix completion using
alternating minimization. In Proceedings of the 45th annual ACM Symposium
on the Theory of Computing, pages 665–674. ACM, 2013.

[Jol86]

I. Jolliﬀe. Principal component analysis. Springer, 1986.

[JW14]

J. Josse and S. Wager. Stable autoencoding: A ﬂexible framework for regularized
low-rank matrix estimation. arXiv preprint arXiv:1410.8275, 2014.

[JWH14]

J. Josse, S. Wager, and F. Husson. Conﬁdence areas for ﬁxed-eﬀects pca. arXiv
preprint arXiv:1407.7614, 2014.

77

[KB78]

[Kes12]

[KHP14]

[KM10]

R. Koenker and J. G. Bassett. Regression quantiles. Econometrica: Journal of
the Econometric Society, pages 33–50, 1978.

R. Keshavan. Eﬃcient algorithms for collaborative ﬁltering. PhD thesis, Stanford
University, 2012.

J. Kim, Y. He, and H. Park. Algorithms for nonnegative matrix and tensor
factorizations: A uniﬁed view based on block coordinate descent framework.
Journal of Global Optimization, 58(2):285–319, 2014.

R. Keshavan and A. Montanari. Regularization for matrix completion.
In
2010 IEEE International Symposium on Information Theory Proceedings (ISIT),
pages 1503–1507. IEEE, 2010.

[KMO09] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries.

In Advances in Neural Information Processing Systems, pages 952–960, 2009.

[KMO10] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries.
IEEE Transactions on Information Theory, 56(6):2980–2998, 2010.

[KO09]

R. Keshavan and S. Oh. A gradient descent algorithm on the Grassman manifold
for matrix completion. arXiv preprint arXiv:0910.5260, 2009.

[Koe05]

R. Koenker. Quantile regression. Cambridge University Press, 2005.

[KP07]

[KP08a]

[KP08b]

[KP11]

H. Kim and H. Park. Sparse non-negative matrix factorizations via alternating
non-negativity-constrained least squares for microarray data analysis. Bioinfor-
matics, 23(12):1495–1502, 2007.

H. Kim and H. Park. Nonnegative matrix factorization based on alternating
nonnegativity constrained least squares and active set method. SIAM Journal
on Matrix Analysis and Applications, 30(2):713–730, 2008.

J. Kim and H. Park. Toward faster nonnegative matrix factorization: A new
algorithm and comparisons. In Eighth IEEE International Conference on Data
Mining, pages 353–362. IEEE, 2008.

J. Kim and H. Park. Fast nonnegative matrix factorization: An active-set-like
method and comparisons. SIAM Journal on Scientiﬁc Computing, 33(6):3261–
3281, 2011.

[KR09]

L. Kaufman and P. J. Rousseeuw. Finding groups in data: an introduction to
cluster analysis, volume 344. John Wiley & Sons, 2009.

[LBRN06] H. Lee, A. Battle, R. Raina, and A. Ng. Eﬃcient sparse coding algorithms. In

Advances in Neural Information Processing Systems, pages 801–808, 2006.

78

[Lik32]

[Lin07]

[Llo82]

R. Likert. A technique for the measurement of attitudes. Archives of Psychology,
1932.

C. Lin. Projected gradient methods for nonnegative matrix factorization. Neural
Computation, 19(10):2756–2779, 2007.

S. Lloyd. Least squares quantization in PCM. IEEE Transactions on Information
Theory, 28(2):129–137, 1982.

[LLW04] Y. Lee, Y. Lin, and G. Wahba. Multicategory support vector machines: Theory
and application to the classiﬁcation of microarray data and satellite radiance
data. Journal of the American Statistical Association, 99(465):67–81, 2004.

[LRS+10]

J. Lee, B. Recht, R. Salakhutdinov, N. Srebro, and J. Tropp. Practical large-scale
optimization for max-norm regularization. In Advances in Neural Information
Processing Systems, pages 1297–1305, 2010.

[LS99]

[LS01]

[LV09]

[LW66]

[Mac09]

D. Lee and H. Seung. Learning the parts of objects by non-negative matrix
factorization. Nature, 401(6755):788–791, 1999.

D. Lee and H. Seung. Algorithms for non-negative matrix factorization.
Advances in Neural Information Processing Systems, pages 556–562, 2001.

In

Z. Liu and L. Vandenberghe. Interior-point method for nuclear norm approxima-
tion with application to system identiﬁcation. SIAM Journal on Matrix Analysis
and Applications, 31(3):1235–1256, 2009.

E. Lawler and D. Wood. Branch-and-bound methods: A survey. Operations
Research, 14(4):699–719, 1966.

L. Mackey. Deﬂation methods for sparse PCA. In D. Koller, D. Schuurmans,
Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing
Systems, 2009.

[Mar12]

I. Markovsky. Low Rank Approximation: Algorithms, Implementation, Applica-
tions. Communications and Control Engineering. Springer, 2012.

[MBPS09] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary learning for sparse
coding. In Proceedings of the 26th Annual International Conference on Machine
Learning, pages 689–696. ACM, 2009.

[MCCD13] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Eﬃcient estimation of word

representations in vector space. arXiv preprint arXiv:1301.3781, 2013.

[MF10]

K. Mohan and M. Fazel. Reweighted nuclear norm minimization with application
to system identiﬁcation. In Proceedings of the 2010 American Control Conference
(ACC), pages 2953–2959. IEEE, 2010.

79

[MHT10] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms
for learning large incomplete matrices. The Journal of Machine Learning Re-
search, 11:2287–2322, 2010.

[Min01]

In T.K. Leen, T.G.
T. Minka. Automatic choice of dimensionality for pca.
Dietterich, and V. Tresp, editors, Advances in Neural Information Processing
Systems, pages 598–604. MIT Press, 2001.

[MPS+09] J. Mairal, J. Ponce, G. Sapiro, A. Zisserman, and F. Bach. Supervised dictionary
learning. In Advances in Neural Information Processing Systems, pages 1033–
1040, 2009.

[MSC+13] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed rep-
resentations of words and phrases and their compositionality. In Advances in
Neural Information Processing Systems, pages 3111–3119, 2013.

[NNS+14] P. Netrapalli, U. Niranjan, S. Sanghavi, A. Anandkumar, and P. Jain. Provable
non-convex robust PCA. In Advances in Neural Information Processing Systems,
pages 1107–1115, 2014.

[NRRW11] F. Niu, B. Recht, C. R´e, and S. Wright. Hogwild!: A lock-free approach to
In Advances in Neural Information

parallelizing stochastic gradient descent.
Processing Systems, 2011.

[OF97]

[OP09]

[Osn14]

[PB13]

B. Olshausen and D. Field. Sparse coding with an overcomplete basis set: A
strategy employed by V1? Vision Research, 37(23):3311–3325, 1997.

A. Owen and P. Perry. Bi-cross-validation of the svd and the nonnegative matrix
factorization. The Annals of Applied Statistics, pages 564–594, 2009.

S. Osnaga. Low Rank Representations of Matrices using Nuclear Norm Heuris-
tics. PhD thesis, Colorado State University, 2014.

N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in Opti-
mization, 1(3):123–231, 2013.

[PCST99] J. Platt, N. Cristianini, and J. Shawe-Taylor. Large margin DAGs for multiclass
In Advances in Neural Information Processing Systems, pages

classiﬁcation.
547–553, 1999.

[Pea01]

K. Pearson. On lines and planes of closest ﬁt to systems of points in space. The
London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science,
2(11):559–572, 1901.

[Per09]

Cross-validation for unsupervised learning.

arXiv preprint

P. Perry.
arXiv:0909.3052, 2009.

80

[PJ09]

[PM03]

[PSM14]

[RA98]

H.-S. Park and C.-H. Jun. A simple and fast algorithm for k-medoids clustering.
Expert Systems with Applications, 36(2, Part 2):3336 – 3341, 2009.

K. Preacher and R. MacCallum. Repairing Tom Swift’s electric factor analysis
machine. Understanding Statistics: Statistical Issues in Psychology, Education,
and the Social Sciences, 2(1):13–43, 2003.

J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for word repre-
sentation. Proceedings of the Empiricial Methods in Natural Language Processing
(EMNLP 2014), 12, 2014.

W. Revelle and K. Anderson. Personality, motivation and cognitive performance:
Final report to the army research institute on contract MDA 903-93-K-0008.
Technical report, 1998.

[RBL+07] R. Raina, A. Battle, H. Lee, B. Packer, and A. Ng. Self-taught learning: Transfer
learning from unlabeled data. In Proceedings of the 24th International Conference
on Machine Learning, pages 759–766. ACM, 2007.

[RFP10]

B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization. SIAM Review, 52(3):471–501,
August 2010.

R. Rifkin and A. Klautau. In defense of one-vs-all classiﬁcation. The Journal of
Machine Learning Research, 5:101–141, 2004.

B. Recht and C. R´e. Parallel stochastic gradient algorithms for large-scale matrix
completion. Mathematical Programming Computation, 5(2):201–226, 2013.

[RRWN11] B. Recht, C. R´e, S. Wright, and F. Niu. Hogwild: A lock-free approach to
In Advances in Neural Information

parallelizing stochastic gradient descent.
Processing Systems, pages 693–701, 2011.

J. Rennie and N. Srebro. Fast maximum margin matrix factorization for col-
laborative prediction. In Proceedings of the 22nd International Conference on
Machine Learning, pages 713–719. ACM, 2005.

P. Richt´arik, M. Tak´aˇc, and S. Ahipa¸sao˘glu. Alternating maximization: Unifying
framework for 8 sparse PCA formulations and eﬃcient parallel codes. arXiv
preprint arXiv:1212.4137, 2012.

[SBPP06] F. Shahnaz, M. W. Berry, V. P. Pauca, and R. J. Plemmons. Document clustering
using nonnegative matrix factorization. Information Processing & Management,
42(2):373–386, 2006.

[SC12]

M. Soltanolkotabi and E. Candes. A geometric analysis of subspace clustering
with outliers. The Annals of Statistics, 40(4):2195–2238, 2012.

[RK04]

[RR13]

[RS05]

[RTA12]

81

[SF14]

[SG08]

[SH08]

[SJ03]

[SM14]

[Smi12]

[Sre04]

[SRJ04]

[SSU03]

[SSZ14]

[SEC13] M. Soltanolkotabi, E. Elhamifar, and E. Candes. Robust subspace clustering.

arXiv preprint arXiv:1301.2603, 2013.

D. L. Sun and C. F´evotte. Alternating direction method of multipliers for non-
negative matrix factorization with the beta-divergence. In IEEE International
Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2014.

A. Singh and G. Gordon. A uniﬁed view of matrix factorization models. In Ma-
chine Learning and Knowledge Discovery in Databases, pages 358–373. Springer,
2008.

H. Shen and J. Huang. Sparse principal component analysis via regularized low
rank matrix approximation. Journal of Multivariate Analysis, 99(6):1015–1034,
2008.

[SHK+14] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov.
Dropout: A simple way to prevent neural networks from overﬁtting. The Journal
of Machine Learning Research, 15(1):1929–1958, 2014.

N. Srebro and T. Jaakkola. Weighted low-rank approximations. In ICML, vol-
ume 3, pages 720–727, 2003.

V. Srikumar and C. Manning. Learning distributed representations for structured
output prediction. In Advances in Neural Information Processing Systems, pages
3266–3274, 2014.

R. Smith. Nuclear norm minimization methods for frequency domain subspace
identiﬁcation. In Proceedings of the 2010 American Control Conference (ACC),
pages 2689–2694. IEEE, 2012.

N. Srebro. Learning with Matrix Factorizations. PhD thesis, Massachusetts
Institute of Technology, 2004.

N. Srebro, J. Rennie, and T. Jaakkola. Maximum-margin matrix factorization.
In Advances in Neural Information Processing Systems, volume 17, pages 1329–
1336, 2004.

[SSGS11]

S. Shalev-Shwartz, A. Gonen, and O. Shamir. Large-scale convex minimization
with a low-rank constraint. arXiv preprint arXiv:1106.1622, 2011.

A. Schein, L. Saul, and L. Ungar. A generalized linear model for principal
component analysis of binary data. In Proceedings of the Ninth International
Workshop on Artiﬁcial Intelligence and Statistics, volume 38, page 46, 2003.

S. Schelter, V. Satuluri, and R. Zadeh. Factorbird — a parameter server ap-
proach to distributed matrix factorization. NIPS 2014 Workshop on Distributed
Machine Learning and Matrix Computations, 2014.

82

[Ste07]

[TB99]

[TG07]

[Tro04]

[Tse00]

H. Steck. Hinge rank loss and the area under the ROC curve. In J. N. Kok,
J. Koronacki, R. L. Mantaras, S. Matwin, D. Mladeniˇc, and A. Skowron, editors,
Machine Learning: ECML 2007, volume 4701 of Lecture Notes in Computer
Science, pages 347–358. Springer Berlin Heidelberg, 2007.

M. Tipping and C. Bishop. Probabilistic principal component analysis. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–
622, 1999.

J. Tropp and A. Gilbert. Signal recovery from random measurements via orthog-
onal matching pursuit. IEEE Transactions on Information Theory, 53(12):4655–
4666, 2007.

[TPB00]

N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.

J. Tropp. Topics in Sparse Approximation. PhD thesis, The University of Texas
at Austin, 2004.

P. Tseng. Nearest q-ﬂat to m points. Journal of Optimization Theory and
Applications, 105(1):249–252, 2000.

[Twe84] M. Tweedie. An index which distinguishes between some important exponen-
tial families. In Statistics: Applications and New Directions. Proceedings of the
Indian Statistical Institute Golden Jubilee International Conference, pages 579–
604, 1984.

[TYDL77] Y. Takane, F. Young, and J. De Leeuw. Nonmetric individual diﬀerences mul-
tidimensional scaling: an alternating least squares method with optimal scaling
features. Psychometrika, 42(1):7–67, 1977.

[UBG09] N. Usunier, D. Buﬀoni, and P. Gallinari. Ranking with ordered weighted pairwise
In Proceedings of the 26th annual International Conference on

classiﬁcation.
Machine Learning, pages 1057–1064. ACM, 2009.

[Vav09]

S. Vavasis. On the complexity of nonnegative matrix factorization. SIAM Journal
on Optimization, 20(3):1364–1377, 2009.

[VCLR13] V. Vu, J. Cho, J. Lei, and K. Rohe. Fantope projection and selection: A near-
optimal convex relaxation of sparse PCA. In C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information
Processing Systems 26, pages 2670–2678. Curran Associates, Inc., 2013.

[Vid10]

R. Vidal. A tutorial on subspace clustering. IEEE Signal Processing Magazine,
28(2):52–68, 2010.

83

[Vir07]

T. Virtanen. Monaural sound source separation by nonnegative matrix factor-
ization with temporal continuity and sparseness criteria. IEEE Transactions on
Audio, Speech, and Language Processing, 15(3):1066–1074, 2007.

[WBU10]

J. Weston, S. Bengio, and N. Usunier. Large scale image annotation: Learning to
rank with joint word-image embeddings. Machine Learning, 81(1):21–35, 2010.

[WGR+09] J. Wright, A. Ganesh, S. Rao, Y. Peng, and Y. Ma. Robust principal component
analysis: Exact recovery of corrupted low-rank matrices by convex optimization.
In Advances in Neural Information Processing Systems, volume 3, 2009.

[WTH09] D. Witten, R. Tibshirani, and T. Hastie. A penalized matrix decomposition, with
applications to sparse principal components and canonical correlation analysis.
Biostatistics, page kxp008, 2009.

[WYW13] J. Weston, H. Yee, and R. J. Weiss. Learning to rank recommendations with
the k-order statistic loss. In Proceedings of the 7th ACM Conference on Recom-
mender Systems, RecSys ’13, pages 245–248, New York, NY, USA, 2013. ACM.

[XCS12]

H. Xu, C. Caramanis, and S. Sanghavi. Robust PCA via outlier pursuit. IEEE
Transactions on Information Theory, 58(5):3047–3064, 2012.

[YDLT76] F. Young, J. De Leeuw, and Y. Takane. Regression with qualitative and quan-
titative variables: An alternating least squares method with optimal scaling
features. Psychometrika, 41(4):505–529, 1976.

[YYH+13] H. Yun, H.-F. Yu, C.-J. Hsieh, S. V. N. Vishwanathan, and I. Dhillon. NO-
MAD: Non-locking, stOchastic Multi-machine algorithm for Asynchronous and
Decentralized matrix completion. arXiv preprint arXiv:1312.0193, 2013.

[ZCF+10] M. Zaharia, M. Chowdhury, M. Franklin, S. Shenker, and I. Stoica. Spark: Clus-
ter computing with working sets. In Proceedings of the 2nd USENIX conference
on hot topics in cloud computing, page 10, 2010.

[ZHT06]

H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis.
Journal of Computational and Graphical Statistics, 15(2):265–286, 2006.

[ZV86]

W. Zwick and W. Velicer. Comparison of ﬁve rules for determining the number
of components to retain. Psychological bulletin, 99(3):432, 1986.

84

Generalized Low Rank Models

Madeleine Udell, Corinne Horn, Reza Zadeh, and Stephen Boyd

May 6, 2015. (Original version posted September 2014.)

Abstract

Principal components analysis (PCA) is a well-known technique for approximating
a tabular data set by a low rank matrix. Here, we extend the idea of PCA to handle
arbitrary data sets consisting of numerical, Boolean, categorical, ordinal, and other
data types. This framework encompasses many well known techniques in data analysis,
such as nonnegative matrix factorization, matrix completion, sparse and robust PCA,
k-means, k-SVD, and maximum margin matrix factorization. The method handles
heterogeneous data sets, and leads to coherent schemes for compressing, denoising,
and imputing missing entries across all data types simultaneously.
It also admits a
number of interesting interpretations of the low rank factors, which allow clustering of
examples or of features. We propose several parallel algorithms for ﬁtting generalized
low rank models, and describe implementations and numerical results.

This manuscript is a draft. Comments sent to udell@stanford.edu are welcome.

5
1
0
2
 
y
a
M
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
2
4
3
0
.
0
1
4
1
:
v
i
X
r
a

1

Contents

1 Introduction

1.1 Previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 PCA and quadratically regularized PCA

2.1 PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Quadratically regularized PCA . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Missing data and matrix completion . . . . . . . . . . . . . . . . . . . . . . .
Interpretations and applications . . . . . . . . . . . . . . . . . . . . . . . . .
2.5
2.6 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Generalized regularization

3.1 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Generalized loss functions

4.1 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Examples
4.3 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Loss functions for abstract data types

5.1 Solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Missing data and data imputation . . . . . . . . . . . . . . . . . . . . . . . .
5.4
Interpretations and applications . . . . . . . . . . . . . . . . . . . . . . . . .
5.5 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.6 Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Multi-dimensional loss functions

6.1 Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.2 Oﬀsets and scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.3 Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7 Fitting low rank models

7.1 Alternating minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.2 Early stopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.3 Quadratic objectives
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.4 Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.5
Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.6 Global optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

4
5
7

8
8
9
9
12
13
15

15
16
17
21

22
22
22
25

26
26
27
29
30
32
32

38
39
42
42

42
44
45
48
48
49
52

8 Choosing low rank models

8.1 Regularization paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8.2 Choosing model parameters
. . . . . . . . . . . . . . . . . . . . . . . . . . .
8.3 On-line optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9 Implementations

9.1 Python implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.2 Julia implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.3 Spark implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A Quadratically regularized PCA

A.1 Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.2 Fixed points of alternating minimization . . . . . . . . . . . . . . . . . . . .

56
56
58
61

62
63
64
67

70
70
71

3

1

Introduction

In applications of machine learning and data mining, one frequently encounters large collec-
tions of high dimensional data organized into a table. Each row in the table represents an
example, and each column a feature or attribute. These tables may have columns of diﬀerent
(sometimes, non-numeric) types, and often have many missing entries.

For example, in medicine, the table might record patient attributes or lab tests: each row
of the table lists test or survey results for a particular patient, and each column corresponds
to a distinct test or survey question. The values in the table might be numerical (3.14),
Boolean (yes, no), ordinal (never, sometimes, always), or categorical (A, B, O). Tests not
administered or questions left blank result in missing entries in the data set. Other examples
abound:
in ﬁnance, the table might record known characteristics of companies or asset
classes; in social science settings, it might record survey responses; in marketing, it might
record known customer characteristics and purchase history.

Exploratory data analysis can be diﬃcult in this setting. To better understand a complex
data set, one would like to be able to visualize archetypical examples, to cluster examples,
to ﬁnd correlated features, to ﬁll in (impute) missing entries, and to remove (or simply
identify) spurious, anomalous, or noisy data points. This paper introduces a templated
method to enable these analyses even on large data sets with heterogeneous values and
with many missing entries. Our approach will be to embed both the rows (examples) and
columns (features) of the table into the same low dimensional vector space. These low
dimensional vectors can then be plotted, clustered, and used to impute missing entries or
identify anomalous ones.

If the data set consists only of numerical (real-valued) data, then a simple and well-
known technique to ﬁnd this embedding is Principal Components Analysis (PCA). PCA
ﬁnds a low rank matrix that minimizes the approximation error, in the least-squares sense,
to the original data set. A factorization of this low rank matrix embeds the original high
dimensional features into a low dimensional space. Extensions of PCA can handle missing
data values, and can be used to impute missing entries.

Here, we extend PCA to approximate an arbitrary data set by replacing the least-squares
error used in PCA with a loss function that is appropriate for the given data type. Another
extension beyond PCA is to add regularization on the low dimensional factors to impose or
encourage some structure, such as sparsity or nonnegativity, in the low dimensional factors.
In this paper we use the term generalized low rank model (GLRM) to refer to the problem
of approximating a data set as a product of two low dimensional factors by minimizing
an objective function. The objective will consist of a loss function on the approximation
error together with regularization of the low dimensional factors. With these extensions of
PCA, the resulting low rank representation of the data set still produces a low dimensional
embedding of the data set, as in PCA.

Many of the low rank modeling problems we must solve will be familiar. We recover an
optimization formulation of nonnegative matrix factorization, matrix completion, sparse and
robust PCA, k-means, k-SVD, and maximum margin matrix factorization, to name just a
few.

4

These low rank approximation problems are not convex, and in general cannot be solved
globally and eﬃciently. There are a few exceptional problems that are known to have con-
vex relaxations which are tight under certain conditions, and hence are eﬃciently (globally)
solvable under these conditions. However, all of these approximation problems can be heuris-
tically (locally) solved by methods that alternate between updating the two factors in the low
rank approximation. Each step involves either a convex problem, or a nonconvex problem
that is simple enough that we can solve it exactly. While these alternating methods need
not ﬁnd the globally best low rank approximation, they are often very useful and eﬀective
for the original data analysis problem.

1.1 Previous work

Uniﬁed views of matrix factorization. We are certainly not the ﬁrst to note that
matrix factorization algorithms may be viewed in a uniﬁed framework, parametrized by a
small number of modeling decisions. The ﬁrst instance we ﬁnd in the literature of this
uniﬁed view appeared in a paper by Collins, Dasgupta, and Schapire, [CDS01], extending
PCA to use loss functions derived from any probabilistic model in the exponential family.
Gordon’s Generalized2 Linear2 models [Gor02] extended the framework to loss functions
derived from the generalized Bregman divergence of any convex function, which includes
models such as Independent Components Analysis (ICA). Srebro’s 2004 PhD thesis [Sre04]
extended the framework to other loss functions, including hinge loss and KL-divergence loss,
and to other regularizers, including the nuclear norm and max-norm. Similarly, Chapter 8 in
Tropp’s 2004 PhD thesis [Tro04] explored a number of new regularizers, presenting a range
of clustering problems as matrix factorization problems with constraints, and anticipated
the k-SVD algorithm [AEB06]. Singh and Gordon [SG08] oﬀered a complete view of the
state of the literature on matrix factorization in Table 1 of their 2008 paper, and noted that
by changing the loss function and regularizer, one may recover algorithms including PCA,
weighted PCA, k-means, k-medians, (cid:96)1 SVD, probabilistic latent semantic indexing (pLSI),
nonnegative matrix factorization with (cid:96)2 or KL-divergence loss, exponential family PCA,
and MMMF. Witten et al. introduced the statistics community to sparsity-inducing matrix
factorization in a 2009 paper on penalized matrix decomposition, with applications to sparse
PCA and canonical correlation analysis [WTH09]. Recently, Markovsky’s monograph on low
rank approximation [Mar12] reviewed some of this literature, with a focus on applications
in system, control, and signal processing. The GLRMs discussed in this paper include all of
these models, and many more.

Heterogeneous data. Many authors have proposed the use of low rank models as a
tool for integrating heterogeneous data. The earliest example of this approach is canonical
correlation analysis, developed by Hotelling [Hot36] in 1936 to understand the relations
between two sets of variates in terms of the eigenvectors of their covariance matrix. This
approach was extended by Witten et al. [WTH09] to encourage structured (e.g., sparse)
In the 1970s, De Leeuw et al. proposed the use of low rank models to ﬁt data
factors.

5

measured in nominal, ordinal and cardinal levels [DLYT76]. More recently, Goldberg et
al. [GRX+10] used a low rank model to perform transduction (i.e., multi-label learning)
in the presence of missing data by ﬁtting a low rank model to the features and the labels
simultaneously. Low rank models have also been used to embed image, text and video data
into a common low dimensional space [GD14], and have recently come into vogue in the
natural language processing community as a means to embed words and documents into a
low dimensional vector space [MCCD13, MSC+13, PSM14, SM14].

Algorithms.
In general, it can be computationally hard to ﬁnd the global optimum of a
generalized low rank model. For example, it is NP-hard to compute an exact solution to k-
means [DFK+04], nonnegative matrix factorization [Vav09], and weighted PCA and matrix
completion [GG11], all of which are special cases of low rank models.

However, there are many (eﬃcient) ways to go about ﬁtting a low rank model, by which
we mean ﬁnding a good model with a small objective value. The resulting model may or may
not be the global solution of the low rank optimization problem. We distinguish a model ﬁt
in this way from the solution to an optimization problem, which always refers to the global
solution.

The matrix factorization literature presents a wide variety of methods to ﬁt low rank
models in a variety of special cases. For example, there are variants on alternating min-
imization (with alternating least squares as a special case) [DLYT76, YDLT76, TYDL77,
DL84, DLM09], alternating Newton methods [Gor02, SG08], (stochastic or incremental)
gradient descent [KO09, LRS+10, NRRW11, RRWN11, BRRT12, YYH+13, RR13], conju-
gate gradients [RS05, SJ03], expectation minimization (EM) (or “soft-impute”) methods
[TB99, SJ03, MHT10, HMLZ14], multiplicative updates [LS99], and convex relaxations to
semideﬁnite programs [SRJ04, FHB04, RFP10, FM13].

Generally, expectation minimization, which proceeds by iteratively imputing missing en-
tries in the matrix and solving the fully observed problem, has been found to underperform
relative to other methods [SG08]. However, when used in conjunction with computational
tricks exploiting a particular problem structure, such as Gram matrix caching, these methods
can still work extremely well [HMLZ14].

Semideﬁnite programming becomes computationally intractable for very large (or even
just large) scale problems [RS05]. However, a theoretical analysis of optimality conditions for
rank-constrainted semideﬁnite programs [BM03c] has led to a few algorithms for semideﬁnite
programming based on matrix factorization [BM03b, ABEV09, JBAS10] which guarantee
global optimality and converge quickly if the global solution to the problem is exactly low
rank. Fast approximation algorithms for rank-constrained semideﬁnite programs have also
been developed [SSGS11].

Recently, there has been a resurgence of interest in methods based on alternating min-
imization, as numerous authors have shown that alternating minimization (suitably initial-
ized, and under a few technical assumptions) provably converges to the global minimum
for a range of problems including matrix completion [Kes12, JNS13, Har13], robust PCA
[NNS+14], and dictionary learning [AAJN13].

6

Gradient descent methods are often preferred for extremely large scale problems since
these methods parallelize naturally in both shared memory and distributed memory archi-
tectures. See [RR13, YYH+13] and references therein for some recent innovative approaches
to speeding up stochastic gradient descent for matrix factorization by eliminating locking
and reducing interprocess communication.

Contributions. The present paper diﬀers from previous work in a number of ways. We
are consistently concerned with the meaning of applying these diﬀerent loss functions and
regularizers to approximate a data set. The generality of our view allows us to introduce a
number of loss functions and regularizers that have not previously been considered. More-
over, our perspective enables us to extend these ideas to arbitrary data sets, rather than just
matrices of real numbers.

A number of new considerations emerge when considering the problem so broadly. First,
we must face the problem of comparing approximation errors across data of diﬀerent types.
For example, we must choose a scaling to trade oﬀ the loss due to a misclassiﬁcation of a
categorical value with an error of 0.1 (say) in predicting a real value.

Second, we require algorithms that can handle the full gamut of losses and regulariz-
ers, which may be smooth or nonsmooth, ﬁnite or inﬁnite valued, with arbitrary domain.
This work is the ﬁrst to consider these problems in such generality, and therefore also the
ﬁrst to wrestle with the algorithmic consequences. Below, we give a number of algorithms
appropriate for this setting, including many that have not been previously proposed in the
literature. Our algorithms are all based on alternating minimization and variations on al-
ternating minimization that are more suitable for large scale data and can take advantage
of parallel computing resources.

Finally, we present some new results on some old problems. For example, in Appendix A,
we derive a formula for the solution to quadratically regularized PCA, and show that quadrat-
ically regularized PCA has no local nonglobal minima; and in §7.6 we show how to certify
(in some special cases) that a model is a global solution of a GLRM.

1.2 Organization

The organization of this paper is as follows. In §2 we ﬁrst recall some properties of PCA
and its common variations to familiarize the reader with our notation. We then generalize
the regularization on the low dimensional factors in §3, and the loss function on the ap-
proximation error in §4. Returning to the setting of heterogeneous data, we extend these
dimensionality reduction techniques to abstract data types in §5 and to multi-dimensional
loss functions in §6. Finally, we address algorithms for ﬁtting GLRMs in §7, discuss a few
practical considerations in choosing a GLRM for a particular problem in §8, and describe
some implementations of the algorithms that we have developed in §9.

7

2 PCA and quadratically regularized PCA

In this section, we let A ∈ Rm×n be a data matrix consisting of m examples
Data matrix.
each with n numerical features. Thus Aij ∈ R is the value of the jth feature in the ith
example, the ith row of A is the vector of n feature values for the ith example, and the jth
column of A is the vector of the jth feature across our set of m examples.

It is common to represent other data types in a numerical matrix using certain canonical
encoding tricks. For example, Boolean data is often encoded as 1 (for true) and -1 (for
false), ordinal data is often encoded using consecutive integers to represent the consecutive
levels of the variable, and categorical data is often encoded by creating a column for each
possible value of the categorical variable, and representing the data using a 1 in the column
corresponding to the observed value, and -1 or 0 in all other columns. We will see more
systematic and principled ways to deal with these data types, and others, in §4–6. For now,
we assume the entries in the data matrix consist of real numbers.

2.1 PCA

solving

Principal components analysis (PCA) is one of the oldest and most widely used tools in data
analysis [Pea01, Hot33, Jol86]. We review some of its well-known properties here in order to
set notation and as a warm-up to the variants presented later.

PCA seeks the best rank-k approximation to the matrix A in the least-squares sense, by

minimize
subject to Rank(Z) ≤ k,
with variable Z ∈ Rm×n. Here, (cid:107) · (cid:107)F is the Frobenius norm of a matrix, i.e., the square root
of the sum of the squares of the entries.

(cid:107)A − Z(cid:107)2
F

(1)

The rank constraint can be encoded implicitly by expressing Z in factored form as Z =

XY , with X ∈ Rm×k, Y ∈ Rk×n. Then the PCA problem can be expressed as

minimize (cid:107)A − XY (cid:107)2
F

(2)

with variables X ∈ Rm×k and Y ∈ Rk×n. (The factorization of Z is of course not unique.)
Deﬁne xi ∈ R1×n to be the ith row of X, and yj ∈ Rm to be the jth column of Y . Thus
xiyj = (XY )ij ∈ R denotes a dot or inner product. (We will use this notation throughout
the paper.) Using this deﬁnition, we can rewrite the objective in problem (2) as
n
(cid:88)

m
(cid:88)

(Aij − xiyj)2.

i=1

j=1

We will give several interpretations of the low rank factorization (X, Y ) solving (2) in
§2.5. But for now, we note that (2) can interpreted as a method for compressing the n
features in the original data set to k < n new features. The row vector xi is associated with
example i; we can think of it as a feature vector for the example using the compressed set
of k < n features. The column vector yj is associated with the original feature j; it can be
interpreted as mapping the k new features onto the original feature j.

8

2.2 Quadratically regularized PCA

We can add quadratic regularization on X and Y to the objective. The quadratically regu-
larized PCA problem is

minimize (cid:80)m

(cid:80)n

j=1(Aij − xiyj)2 + γ (cid:80)m

i=1 (cid:107)xi(cid:107)2

2 + γ (cid:80)n

j=1 (cid:107)yj(cid:107)2
2,

i=1

with variables X ∈ Rm×k and Y ∈ Rk×n, and regularization parameter γ ≥ 0. Problem (3)
can be written more concisely in matrix form as

minimize (cid:107)A − XY (cid:107)2

F + γ(cid:107)X(cid:107)2

F + γ(cid:107)Y (cid:107)2
F .

When γ = 0, the problem reduces to the PCA problem (2).

2.3 Solution methods

Singular value decomposition.
It is well known that a solution to (2) can be obtained
by truncating the singular value decomposition (SVD) of A [EY36]. The (compact) SVD
of A is given by A = U ΣV T , where U ∈ Rm×r and V ∈ Rn×r have orthonormal columns,
and Σ = diag(σ1, . . . , σr) ∈ Rr×r, with σ1 ≥ · · · ≥ σr > 0 and r = Rank(A). The columns
of U = [u1 · · · ur] and V = [v1 · · · vr] are called the left and right singular vectors of A,
respectively, and σ1, . . . , σr are called the singular values of A.

Using the orthogonal invariance of the Frobenius norm, we can rewrite the objective in

problem (1) as

(cid:107)A − XY (cid:107)2

F = (cid:107)Σ − U T XY V (cid:107)2
F .

That is, we would like to ﬁnd a matrix U T XY V of rank no more than k approximating the
diagonal matrix Σ. It is easy to see that there is no better rank k approximation for Σ than
Σk = diag(σ1, . . . , σk, 0, . . . , 0) ∈ Rr×r. Here we have truncated the SVD to keep only the
top k singular values. We can achieve this approximation by choosing U T XY V = Σk, or
(using the orthogonality of U and V ) XY = U ΣkV T . For example, deﬁne

and let

Uk = [u1 · · · uk],

Vk = [v1 · · · vk],

X = UkΣ1/2
k ,

Y = Σ1/2

k V T
k .

The solution to (3) is clearly not unique: if X, Y is a solution, then so is XG, G−1Y for any
invertible matrix G ∈ Rk×k. When σk > σk+1, all solutions to the PCA problem have this
form. In particular, letting G = tI and taking t → ∞, we see that the solution set of the
PCA problem is unbounded.

It is less well known that a solution to the quadratically regularized PCA problem can
be obtained in the same way. (Proofs for the statements below can be found in Appendix
A.) Deﬁne Uk and Vk as above, and let ˜Σk = diag((σ1 − γ)+, . . . , (σk − γ)+), where (a)+ =
max(a, 0). Here we have both truncated the SVD to keep only the top k singular values, and

(3)

(4)

(5)

(6)

9

performed soft-thresholding on the singular values to reduce their values by γ. A solution to
the quadratically regularized PCA problem (3) is then given by

X = Uk

˜Σ1/2
k ,

Y = ˜Σ1/2

k V T
k .

(7)

For γ = 0, the solution reduces to the familiar solution to PCA (2) obtained by truncating
the SVD to the top k singular values.

The set of solutions to problem (3) is signiﬁcantly smaller than that of problem (2),
although solutions are still not unique: if X, Y is a solution, then so is XT , T −1Y for any
orthogonal matrix T ∈ Rk×k. When σk > σk+1, all solutions to (3) have this form.
In
particular, adding quadratic regularization results in a solution set that is bounded.

The quadratically regularized PCA problem (3) (including the PCA problem as a special
case) is the only problem we will encounter for which an analytical solution exists. The
analytical tractability of PCA explains its popularity as a technique for data analysis in
the era before computers were machines. For example, in his 1933 paper on PCA [Hot33],
Hotelling computes the solution to his problem using power iteration to ﬁnd the eigenvalue
decomposition of the matrix AT A = V Σ2V T , and records in the appendix to his paper the
itermediate results at each of the (three) iterations required for the method to converge.

Alternating minimization. Here we mention a second method for solving (3), which
extends more readily to the extensions of PCA that we discuss below. The alternating
minimization algorithm simply alternates between minimizing the objective over the variable
X, holding Y ﬁxed, and then minimizing over Y , holding X ﬁxed. With an initial guess for
the factors Y 0, we repeat the iteration
(cid:32) m
(cid:88)

m
(cid:88)

n
(cid:88)

(cid:33)

X l = argmin

(Aij − xiyl−1

)2 + γ

j

(cid:107)xi(cid:107)2
2

Y l = argmin

(Aij − xl

iyj)ij)2 + γ

(cid:107)yj(cid:107)2
2

X

Y

i=1
(cid:32) m
(cid:88)

j=1

n
(cid:88)

i=1

j=1

(cid:33)

i=1

n
(cid:88)

j=1

for l = 1, . . . until a stopping condition is satisﬁed. (If X and Y are full rank, or γ > 0, the
minimizers above are unique; when they are not, we can take any minimizer.) The objective
function is nonincreasing at each iteration, and therefore bounded. This implies, for γ > 0,
that the iterates X l and Y l are bounded.

This algorithm does not always work. In particular, it has stationary points that are not
solutions of problem (3). In particular, if the rows of Y l lie in a subspace spanned by a subset
of the (right) singular vectors of A, then the columns of X l+1 will lie in a subspace spanned
by the corresponding left singular vectors of A, and vice versa. Thus, if the algorithm is
initialized with Y 0 orthogonal to any of the top k (right) singular vectors, then the algorithm
(implemented in exact arithmetic) will not converge to the global solution to the problem.
But all stable stationary points of the iteration are solutions (see Appendix A). So as
a practical matter, the alternating minimization method always works, i.e., the objective
converges to the optimal value.

10

Parallelizing alternating minimization. Alternating minimization parallelizes easily
over examples and features. The problem of minimizing over X splits into m independent
minimization problems. We can solve the simple quadratic problems

minimize (cid:80)n

j=1(Aij − xiyj)2 + γ(cid:107)xi(cid:107)2
2

with variable xi, in parallel, for i = 1, . . . , m. Similarly, the problem of minimizing over Y
splits into n independent quadratic problems,

minimize (cid:80)m

i=1(Aij − xiyj)2 + γ(cid:107)yj(cid:107)2
2

with variable yj, which can be solved in parallel for j = 1, . . . , n.

(8)

(9)

Caching factorizations. We can speed up the solution of the quadratic problems using
a simple factorization caching technique.

For ease of exposition, we assume here that X and Y have full rank k. The updates (8)

and (9) can be expressed as

X = AY T (Y Y T + γI)−1,

Y = (X T X + γI)−1X T A.

We show below how to eﬃciently compute X = AY T (Y Y T + γI)−1; the Y update admits a
similar speedup using the same ideas. We assume here that k is modest, say, not more than
a few hundred or a few thousand. (Typical values used in applications are often far smaller,
on the order of tens.) The dimensions m and n, however, can be very large.

First compute the Gram matrix G = Y Y T using an outer product expansion

This sum can be computed on-line by streaming over the index j, or in parallel, split over
the index j. This property allows us to scale up to extremely large problems even if we
cannot store the entire matrix Y in memory. The computation of the Gram matrix requires
2k2n ﬂoating point operations (ﬂops), but is trivially parallelizable: with r workers, we can
expect a speedup on the order of r. We next add the diagonal matrix γI to G in k ﬂops,
and form the Cholesky factorization of G + γI in k3/3 ﬂops and cache the factorization.

In parallel over the rows of A, we compute D = AY T (2kn ﬂops per row), and use the
factorization of G + γI to compute D(G + γI)−1 with two triangular solves (2k2 ﬂops per
row). These computations are also trivially parallelizable: with r workers, we can expect a
speedup on the order of r.

Hence the total time required for each update with r workers scales as O( k2(m+n)+kmn

).

For k small compared to m and n, the time is dominated by the computation of AY T .

r

G =

yjyT
j .

n
(cid:88)

j=1

11

2.4 Missing data and matrix completion

Suppose we observe only entries Aij for (i, j) ∈ Ω ⊂ {1, . . . , m} × {1, . . . , n} from the matrix
A, so the other entries are unknown. Then to ﬁnd a low rank matrix that ﬁts the data well,
we solve the problem

minimize (cid:80)

(i,j)∈Ω(Aij − xiyj)2 + γ(cid:107)X(cid:107)2

F + γ(cid:107)Y (cid:107)2
F ,

(10)

with variables X and Y , with γ > 0. A solution of this problem gives an estimate ˆAij = xiyj
for the value of those entries (i, j) (cid:54)∈ Ω that were not observed. In some applications, this
data imputation (i.e., guessing entries of a matrix that are not known) is the main point.
There are two very diﬀerent regimes in which solving the problem (10) may be useful.

Imputing missing entries to borrow strength. Consider a matrix A in which very few
entries are missing. The typical approach in data analysis is to simply remove any rows with
missing entries from the matrix and exclude them from subsequent analysis. If instead we
solve the problem above without removing these aﬀected rows, we “borrow strength” from
the entries that are not missing to improve our global understanding of the data matrix
A. In this regime we are imputing the (few) missing entries of A, using the examples that
ordinarily we would discard.

Low rank matrix completion. Now consider a matrix A in which most entries are
missing, i.e., we only observe relatively few of the mn elements of A, so that by discarding
every example with a missing feature or every feature with a missing example, we would
discard the entire matrix. Then the solution to (10) becomes even more interesting: we are
guessing all the entries of a (presumed low rank) matrix, given just a few of them. It is a
surprising fact that this is possible: typical results from the matrix completion literature show
that one can recover an unknown m×n matrix A of low rank r from just about nr log2 n noisy
samples Ω with an error that is proportional to the noise level [CR08, CT10, RFP10, CP09],
so long as the matrix A satisﬁes a certain incoherence condition and the samples Ω are chosen
uniformly at random. These works use an estimator that minimizes a nuclear norm penalty
along with a data ﬁtting term to encourage low rank structure in the solution.

The argument in §7.6 shows that problem (10) is equivalent to the rank-constrained

nuclear-norm regularized convex problem

minimize (cid:80)
subject to Rank(Z) ≤ k,

(i,j)∈Ω(Aij − Zij)2 + 2γ(cid:107)Z(cid:107)∗

where the nuclear norm (cid:107)Z(cid:107)∗ (also known as the trace norm) is deﬁned to be the sum of the
singular values of Z. Thus, the solutions to problem (10) correspond exactly to the solutions
of these proposed estimators so long as the rank k of the model is chosen to be larger than
the true rank r of the matrix A. Nuclear norm regularization is often used to encourage
solutions of rank less than k, and has applications ranging from graph embedding to linear
system identiﬁcation [FHB04, LV09, MF10, Smi12, Osn14].

12

Low rank matrix completion problems arise in applications like predicting customer rat-
ings or customer (potential) purchases. Here the matrix consists of the ratings or numbers
of purchases that m customers give (or make) for each of n products. The vast majority of
the entries in this matrix are missing, since a customer will rate (or purchase) only a small
fraction of the total number of products available. In this application, imputing a missing
entry of the matrix as xiyj, for (i, j) (cid:54)∈ Ω, is guessing what rating a customer would give a
product, if she were to rate it. This can used as the basis for a recommendation system, or
a marketing plan.

Alternating minimization. When Ω (cid:54)= {1, . . . , m} × {1, . . . , n}, the problem (10) has no
known analytical solution, but it is still easy to ﬁt a model using alternating minimization.
Algorithms based on alternating minimization have been shown to converge quickly (even
geometrically [JNS13]) to a global solution satisfying a recovery guarantee when the initial
values of X and Y are chosen carefully [KMO09, KMO10, KM10, JNS13, Har13, GAGG13].
On the other hand, all of these analytical results rely on using a fresh batch of samples
Ω for each iteration of alternating minimization; none uses the quadratic regularizer above
that corresponds to the nuclear norm penalized estimator; and interestingly, Hardt [Har13]
notes that none achieves the same sample complexity guarantees found in the convex ma-
trix completion literature which, unlike the alternating minimization guarantees, match the
information theoretic lower bound [CT10] up to logarithmic factors. For these reasons, it
is plausible to expect that in practice using alternating minimization to solve problem (10)
might yield a better solution than the “alternating minimization” algorithms presented in
the literature on matrix completion when suitably initialized (for example, using the method
proposed below in §7.5). However, in general the method should be considered a heuristic.

2.5 Interpretations and applications

The recovered matrices X and Y in the quadratically regularized PCA problems (3) and
(10) admit a number of interesting interpretations. We introduce some of these interpreta-
tions now; the terminology we use here will recur throughout the paper. Of course these
interpretations are related to each other, and not distinct.

Feature compression. Quadratically regularized PCA (3) can be interpreted as a method
for compressing the n features in the original data set to k < n new features. The row vector
xi is associated with example i; we can think of it as a feature vector for the example using
the compressed set of k < n features. The column vector yj is associated with the original
feature j; it can be interpreted as the mapping from the original feature j into the k new
features.

Low-dimensional geometric embedding. We can think of each yj as associating feature
j with a point in a low (k-) dimensional space. Similarly, each xi associates example i with
a point in the low dimensional space. We can use these low dimensional vectors to judge

13

which features (or examples) are similar. For example, we can run a clustering algorithm on
the low dimensional vectors yj (or xi) to ﬁnd groups of similar features (or examples).

Archetypes. We can think of each row of Y as an archetype which captures the behavior
of one of k idealized and maximally informative examples. These archetypes might also
be called proﬁles, factors, or atoms. Every example i = 1, . . . , m is then represented (ap-
proximately) as a linear combination of these archetypes, with the row vector xi giving the
coeﬃcients. The coeﬃcient xil gives the resemblance or loading of example i to the lth
archetype.

Archetypical representations. We call xi the representation of example i in terms of the
archetypes. The rows of X give an embedding of the examples into Rk, where each coordinate
axis corresponds to a diﬀerent archetype.
If the archetypes are simple to understand or
interpret, then the representation of an example can provide better intuition about that
example.

The examples can be clustered according to their representations in order to determine
a group of similar examples.
Indeed, one might choose to apply any machine learning
algorithm to the representations xi rather than to the initial data matrix: in contrast to the
initial data, which may consist of high dimensional vectors with noisy or missing entries, the
representations xi will be low dimensional, less noisy, and complete.

Feature representations. The columns of Y embed the features into Rk. Here, we
think of the columns of X as archetypical features, and represent each feature j as a linear
combination of the archetypical features. Just as with the examples, we might choose to
apply any machine learning algorithm to the feature representations. For example, we might
ﬁnd clusters of similar features that represent redundant measurements.

Latent variables. Each row of X represents an example by a vector in Rk. The matrix
Y maps these representations back into Rm. We might think of X as discovering the latent
variables that best explain the observed data. If the approximation error (cid:80)
(i,j)∈Ω(Aij −xiyj)2
is small, then we view these latent variables as providing a good explanation or summary of
the full data set.

Probabilistic intepretation. We can give a probabilistic interpretation of X and Y ,
building on the probabilistic model of PCA developed by Tipping and Bishop [TB99]. We
suppose that the matrices ¯X and ¯Y have entries which are generated by taking independent
samples from a normal distribution with mean 0 and variance γ−1 for γ > 0. The entries in
the matrix ¯X ¯Y are observed with noise ηij ∈ R,

where the noise η in the (i, j)th entry is sampled independently from a standard normal
distribution. We observe each entry (i, j) ∈ Ω. Then to ﬁnd the maximum a posteriori

Aij = ( ¯X ¯Y )ij + ηij,

14

(MAP) estimator (X, Y ) of ( ¯X, ¯Y ), we solve
(cid:1) exp (cid:0)− γ

maximize exp (cid:0)− γ

2 (cid:107) ¯X(cid:107)2

F

which is equivalent, by taking logs, to (3).

2 (cid:107) ¯Y (cid:107)2

F

(cid:1) (cid:81)

(i,j)∈Ω exp (−(Aij − xiyj)2) ,

This interpretation explains the recommendation we gave above for imputing missing
observations (i, j) (cid:54)∈ Ω. We simply use the MAP estimator xiyj to estimate the missing
entry ( ¯X ¯Y )ij. Similarly, we can interpret (XY )ij for (i, j) ∈ Ω as a denoised version of the
observation Aij.

Auto-encoder. The matrix X encodes the data; the matrix Y decodes it back into the
full space. We can view PCA as providing the best linear auto-encoder for the data; among
all (bi-linear) low rank encodings (X) and decodings (Y ) of the data, PCA minimizes the
squared reconstruction error.

Compression. We impose an information bottleneck [TPB00] on the data by using a low
rank auto-encoder to ﬁt the data. PCA ﬁnds X and Y to maximize the information trans-
mitted through this k-dimensional information bottleneck. We can interpret the solution
as a compressed representation of the data, and use it to eﬃciently store or transmit the
information present in the original data.

2.6 Oﬀsets and scaling

For good practical performance of a generalized low rank model, it is critical to ensure that
model assumptions match the data. We saw above in §2.5 that quadratically regularized
PCA corresponds to a model in which features are observed with N (0, 1) errors. If instead
each column j of XY is observed with N (µj, σ2
j ) errors, our model is no longer unbiased,
and may ﬁt very poorly, particularly if some of the column means µj are large.

For this reason it is standard practice to standardize the data before appplying PCA or
quadratically regularized PCA: the column means are subtracted from each column, and the
columns are normalized by their variances. (This can be done approximately; there is no
need to get the scaling and oﬀset exactly right.) Formally, deﬁne nj = |{i : (i, j) ∈ Ω}|, and
let

µj =

1
nj

(cid:88)

Aij,

(i,j)∈Ω

σ2
j =

1
nj − 1

(cid:88)

(i,j)∈Ω

(Aij − µj)2

estimate the mean and variance of each column of the data matrix. PCA or quadratically
regularized PCA is then applied to the matrix whose (i, j) entry is (Aij − µj)/σj.

3 Generalized regularization

It is easy to see how to extend PCA to allow arbitrary regularization on the rows of X and
columns of Y . We form the regularized PCA problem
(i,j)∈Ω(Aij − xiyj)2 + (cid:80)m

i=1 ri(xi) + (cid:80)n

minimize (cid:80)

j=1 ˜rj(yj),

(11)

15

with variables xi and yj, with given regularizers ri : Rk → R ∪ {∞} and ˜rj : Rk → R ∪ {∞}
for i = 1, . . . , n and j = 1, . . . , m. Regularized PCA (11) reduces to quadratically regularized
PCA (3) when ri = γ(cid:107) · (cid:107)2

2. We do not restrict the regularizers to be convex.

2, ˜rj = γ(cid:107) · (cid:107)2

The objective in problem (11) can be expressed compactly in matrix notation as

(cid:107)A − XY (cid:107)2
i=1 r(xi) and ˜r(Y ) = (cid:80)n

F + r(X) + ˜r(Y ),

where r(X) = (cid:80)n
separable across the rows of X, and the columns of Y , respectively.

j=1 ˜r(yj). The regularization functions r and ˜r are

Inﬁnite values of ri and ˜rj are used to enforce constraints on the values of X and Y . For

example, the regularizer

ri(x) =

(cid:26) 0

x ≥ 0
∞ otherwise,

the indicator function of the nonnegative orthant, imposes the constraint that xi be nonneg-
ative.

Solutions to (11) need not be unique, depending on the choice of regularizers. If X and
Y are a solution, then so are XT and T −1Y , where T is any nonsingular matrix that satisﬁes
r(U T ) = r(U ) for all U and ˜r(T −1V ) = r(V ) for all V .

By varying our choice of regularizers r and ˜r, we are able to represent a wide range of
known models, as well as many new ones. We will discuss a number of choices for regularizers
below, but turn now to methods for solving the regularized PCA problem (11).

3.1 Solution methods

In general, there is no analytical solution for (11). The problem is not convex, even when r
and ˜r are convex. However, when r and ˜r are convex, the problem is bi-convex: it is convex
in X when Y is ﬁxed, and convex in Y when X is ﬁxed.

Alternating minimization. There is no reason to believe that alternating minimization
will always converge to the global minimum of the regularized PCA problem (11). Indeed,
we will see many cases below in which the problem is known to have many local minima.
However, alternating minimization can still be applied in this setting, and it still parallelizes
over the rows of X and columns of Y . To minimize over X, we solve, in parallel,

with variable xi, for i = 1, . . . , m. Similarly, to minimize over Y , we solve, in parallel,

minimize (cid:80)

j:(i,j)∈Ω(Aij − xiyj)2 + ri(xi)

minimize (cid:80)

i:(i,j)∈Ω(Aij − xiyj)2 + ˜rj(yj)

(12)

(13)

with variable yj, for j = 1, . . . , n.

When the regularizers are convex, these problems are convex. When the regularizers
are not convex, there are still many cases in which we can ﬁnd analytical solutions to the
nonconvex subproblems (12) and (13), as we will see below. A number of concrete algorithms,
in which these subproblems are solved explicitly, are given in §7.

16

Caching factorizations. Often, the X and Y updates (12) and (13) reduce to convex
quadratic programs. For example, this is the case for nonnegative matrix factorization,
sparse PCA, and quadratic mixtures (which we deﬁne and discuss below in §3.2). The same
factorization caching of the Gram matrix that was described above in the case of PCA can
be used here to speed up the solution of these updates. Variations on this idea are described
in detail in §7.3.

3.2 Examples

Here and throughout the paper, we present a set of examples chosen for pedagogical clarity,
In all of the examples below, γ > 0 is a parameter that controls
not for completeness.
the strength of the regularization, and we drop the subscripts from r (or ˜r) to lighten the
notation. Of course, it is possible to mix and match these regularizers, i.e., to choose diﬀerent
ri for diﬀerent i, and choose diﬀerent ˜rj for diﬀerent j.

Nonnegative matrix factorization (NNMF). Consider the regularized PCA problem
(11) with r = I+ and ˜r = I+, where I+ is the indicator function of the nonnegative reals.
(Here, and throughout the paper, we deﬁne the indicator function of a set C, to be 0 when
its argument is in C and ∞ otherwise.) Then problem (11) is NNMF: a solution gives the
matrix best approximating A that has a nonnegative factorization (i.e., a factorization into
elementwise nonnegative matrices) [LS99]. It is NP-hard to solve NNMF problems exactly
[Vav09]. However, these problems have a rich analytical structure which can sometimes
be exploited [Gil11, BRRT12, DS14], and a wide range of uses in practice [LS99, SBPP06,
BBL+07, Vir07, KP07, FBD09]. Hence a number of specialized algorithms and codes for
ﬁtting NNMF models are available [LS01, Lin07, KP08a, KP08b, BDKP14, KHP14, KP11].
We can also replace the nonnegativity constraint with any interval constraint. For ex-
ample, r and ˜r can be 0 if all entries of X and Y , respectively, are between 0 and 1, and
inﬁnite otherwise.

Sparse PCA.
If very few of the coeﬃcients of X and Y are nonzero, it can be easier to
interpret the archetypes and representations. We can understand each archetype using only
a small number of features, and can understand each example as a combination of only a
small number of archetypes. To get a sparse version of PCA, we use a sparsifying penalty
as the regularization. Many variants on this basic idea have been proposed, together with a
wide variety of algorithms [dEGJL04, ZHT06, SH08, Mac09, WTH09, RTA12, VCLR13].

For example, we could enforce that no entry Aij depend on more than s columns of X

or of Y by setting r to be the indicator function of a s-sparse vector, i.e.,

and deﬁning ˜r(y) similarly, where card(x) denotes the cardinality (number of nonzero en-
tries) in the vector x. The updates (12) and (13) are not convex using this regularizer, but

r(x) =

(cid:26) 0

card(x) ≤ s

∞ otherwise,

17

one can ﬁnd approximate solutions using a pursuit algorithm (see, e.g., [CDS98, TG07]), or
exact solutions (for small s) using the branch and bound method [LW66, BM03a].

As a simple example, consider s = 1. Here we insist that each xi have at most one
nonzero entry, which means that each example is a multiple of one of the rows of Y . The
X-update is easy to carry out, by evaluating the best quadratic ﬁt of xi with each of the k
rows of Y . This reduces to choosing the row of Y that has the smallest angle to the ith row
of A.

The s-sparse regularization can be relaxed to a convex, but still sparsifying, regularization
using r(x) = (cid:107)x(cid:107)1, ˜r(y) = (cid:107)y(cid:107)1 [ZHT06]. In this case, the X-update reduces to solving a
(small) (cid:96)1-regularized least-squares problem.

Orthogonal nonnegative matrix factorization. One well known property of PCA is
that the principal components obtained (i.e., the columns of X and rows of Y ) can be chosen
to be orthogonal, so X T X and Y Y T are both diagonal. We can impose the same condition
on a nonnegative matrix factorization. Due to nonnegativity of the matrix, two columns
of X cannot be orthogonal if they both have a nonzero in the same row. Conversely, if X
has only one nonzero per row, then its columns are mutually orthogonal. So an orthogonal
nonnegative matrix factorization is identical to to a nonnegativity condition in addition to
the 1-sparse condition described above. Orthogonal nonnegative matrix factorization can be
achieved by using the regularizer

(cid:26) 0

r(x) =

card(x) = 1,

x ≥ 0

∞ otherwise,

and letting ˜r(y) be the indicator of the nonnegative orthant, as in NNMF.

Geometrically, we can interpret this problem as modeling the data A as a union of rays.
Each row of Y , interpreted as a point in Rn, deﬁnes a ray from the origin passing through
that point. Orthogonal nonnegative matrix factorization models each row of X as a point
along one of these rays.

Some authors [DLPP06] have also considered how to obtain a bi-orthogonal nonnegative
matrix factorization, in which both X and Y T have orthogonal columns. By the same
argument as above, we see this is equivalent to requiring both X and Y T to have only one
positive entry per row, with the other entries equal to 0.

Max-norm matrix factorization. We take r = ˜r = φ with

This penalty enforces that

φ(x) =

(cid:26) 0

(cid:107)x(cid:107)2
2 ≤ µ
∞ otherwise.

(cid:107)X(cid:107)2

2,∞ ≤ µ,

(cid:107)Y T (cid:107)2

2,∞ ≤ µ,

18

where the (2, ∞) norm of a matrix X with rows xi is deﬁned as maxi (cid:107)xi(cid:107)2. This is equivalent
to requiring the max-norm (sometimes called the γ2-norm) of Z = XY , which is deﬁned as

(cid:107)Z(cid:107)max = inf{(cid:107)X(cid:107)2,∞(cid:107)Y T (cid:107)2,∞ : XY = Z},

to be bounded by µ. This penalty has been proposed by [LRS+10] as a heuristic for low rank
matrix completion, which can perform better than Frobenius norm regularization when the
low rank factors are known to have bounded entries.

Quadratic clustering. Consider (11) with ˜r = 0. Let r be the indicator function of a
selection, i.e.,

(cid:26) 0

r(x) =

∞ otherwise,

x = el for some l ∈ {1, . . . , k}

where el is the l-th standard basis vector. Thus xi encodes the cluster (one of k) to which
the data vector (Ai1, . . . , Aim) is assigned.

Alternating minimization on this problem reproduces the well-known k-means algorithm
(also known as Lloyd’s algorithm) [Llo82]. The y update (13) is a least squares problem with
the simple solution

Ylj =

(cid:80)

i:(i,j)∈Ω AijXil
(cid:80)
i:(i,j)∈Ω Xil

,

i.e., each row of Y is updated to be the mean of the rows of A assigned to that archetype.
The x update (12) is not a convex problem, but is easily solved. The solution is given
by assigning xi to the closest archetype (often called a cluster centroid in the context of
j=1(Aij − Ylj)2(cid:17)
k-means): xi = el(cid:63) for l(cid:63) = argminl

(cid:16)(cid:80)n

.

Quadratic mixtures. We can also implement partial assignment of data vectors to clus-
ters. Take ˜r = 0, and let r be the indicator function of the set of probability vectors,
i.e.,

r(x) =

(cid:26) 0 (cid:80)k

l=1 xl = 1,

xl ≥ 0

∞ otherwise.

Subspace clustering. PCA approximates a data set by a single low dimensional subspace.
We may also be interested in approximating a data set as a union of low dimensional sub-
spaces. This problem is known as subspace clustering (see [Vid10] and references therein).
Subspace clustering may also be thought of as generalizing quadratic clustering to assign
each data vector to a low dimensional subspace rather than to a single cluster centroid.

To frame subspace clustering as a regularized PCA problem (11), partition the columns
of X into k blocks. Then let r be the indicator function of block sparsity (i.e., r(x) = 0 if
only one block of x has nonzero entries, and otherwise r(x) = ∞).

It is easy to perform alternating minimization on this objective function. This method
is sometimes called the k-planes algorithm [Vid10, Tse00, AM04], which alternates over

19

assigning examples to subspaces, and ﬁtting the subspaces to the examples. Once again, the
X update (12) is not a convex problem, but can be easily solved. Each block of the columns
of X deﬁnes a subspace spanned by the corresponding rows of Y . We compute the distance
from example i (the ith row of A) to each subspace (by solving a least squares problem),
and assign example i to the subspace that minimizes the least squares error by setting xi to
be the solution to the corresponding least squares problem.

Many other algorithms for this problem have also been proposed, such as the k-SVD
[Tro04, AEB06] and sparse subspace clustering [EV09], some with provable guarantees on
the quality of the recovered solution [SC12].

Supervised learning. Sometimes we want to understand the variation that a certain set
of features can explain, and the variance that remains unexplainable. To this end, one
natural strategy would be to regress the labels in the dataset on the features; to subtract
the predicted values from the data; and to use PCA to understand the remaining variance.
This procedure gives the same answer as the solution to a single regularized PCA problem.
Here we present the case in which the features we wish to use in the regression are present
in the data as the ﬁrst column of A. To construct the regularizers, we make sure the ﬁrst
column of A appears as a feature in the supervised learning problem by setting

ri(x) =

(cid:26) r0(x2, . . . , xk+1) x1 = Ai1
otherwise,

∞

where r0 = 0 can be chosen as in any regularized PCA model. The regularization on the
ﬁrst row of Y is the regularization used in the supervised regression, and the regularization
on the other rows will be that used in regularized PCA.

Thus we see that regularized PCA can naturally combine supervised and unsupervised

learning into a single problem.

Feature selection. We can use regularized PCA to perform feature selection. Consider
(11) with r(x) = (cid:107)x(cid:107)2
2 and ˜r(y) = (cid:107)y(cid:107)2. (Notice that we are not using (cid:107)y(cid:107)2
2.) The regularizer
˜r encourages the matrix ˜Y to be column-sparse, so many columns are all zero. If ˜yj = 0,
it means that feature j was uninformative, in the sense that its values do not help much in
predicting any feature in the matrix A (including feature j itself). In this case we say that
feature j was not selected. For this approach to make sense, it is important that the columns
of the matrix A should have mean zero. Alternatively, one can use the de-biasing regularizers
r(cid:48) and ˜r(cid:48) introduced in §3.3 along with the feature selection regularizer introduced here.

Dictionary learning. Dictionary learning (also sometimes called sparse coding) has be-
come a popular method to design concise representations for very high dimensional data
[OF97, LBRN06, MBPS09, MPS+09]. These representations have been shown to perform
well when used as features in subsequent (supervised) machine learning tasks [RBL+07].
In dictionary learning, each row of A is modeled as a linear combination of dictionary
atoms, represented by rows of Y . The total size of the dictionary used is often very large

20

(k (cid:29) max(m, n)), but each example is represented using a very small number of atoms. To
ﬁt the model, one solves the regularized PCA problem (11) with r(x) = (cid:107)x(cid:107)1, to induce spar-
sity in the number of atoms used to represent any given example, and with ˜r(y) = (cid:107)y(cid:107)2
2 or
˜r(y) = I+(c − (cid:107)y(cid:107)2) for some c > 0 ∈ R, in order to ensure the problem is well posed. (Note
that our notation transposes the usual notation in the literature on dictionary learning.)

Mix and match.
It is possible to combine these regularizers to obtain a factorization with
any combination of the above properties. As an example, one may require that both X and
Y be simultaneously sparse and nonnegative by choosing

r(x) = (cid:107)x(cid:107)1 + I+(x) = 1T x + I+(x),

and similarly for ˜r(y). Similarly, [KP07] show how to obtain a nonnegative matrix factor-
ization in which one factor is sparse by using r(x) = (cid:107)x(cid:107)2
2 + I+(y);
they go on to use this factorization as a clustering technique.

1 + I+(x) and ˜r(y) = (cid:107)y(cid:107)2

3.3 Oﬀsets and scaling

In our discussion of the quadratically regularized PCA problem (3), we saw that it can often
be quite important to standardize the data before applying PCA. Conversely, in regularized
PCA problems such as nonnegative matrix factorization, it makes no sense to standardize
the data, since subtracting column means introduces negative entries into the matrix.

A ﬂexible approach is to allow an oﬀset in the model: we solve

minimize (cid:80)

(i,j)∈Ω(Aij − xiyj − µj)2 + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj),

(14)

with variables xi, yj, and µj. Here, µj takes the role of the column mean, and in fact will
be equal to the column mean in the trivial case k = 0.

An oﬀset may be included in the standard form regularized PCA problem (11) by aug-
menting the problem slightly. Suppose we are given an instance of the problem (11), i.e.,
we are given k, r, and ˜r. We can ﬁt an oﬀset term µj by letting k(cid:48) = k + 1 and modifying
the regularizers. Extend the regularization r : Rk → R and ˜r : Rk → R to new regularizers
r(cid:48) : Rk+1 → R and ˜r(cid:48) : Rk+1 → R which enforce that the ﬁrst column of X is constant and
the ﬁrst row of Y is not penalized. Using this scheme, the ﬁrst row of the optimal Y will be
equal to the optimal µ in (14).

Explicitly, let

(cid:26) r(x2, . . . , xk+1) x1 = 1

r(cid:48)(x) =

∞

otherwise,

and ˜r(cid:48)(y) = ˜r(y2, . . . , yk+1). (Here, we identify r(x) = r(x1, . . . , xk) to explicitly show the
dependence on each coordinate of the vector x, and similarly for ˜r.)
It is also possible to introduce row oﬀsets in the same way.

21

4 Generalized loss functions

We may also generalize the loss function in PCA to form a generalized low rank model,

minimize (cid:80)

(i,j)∈Ω Lij(xiyj, Aij) + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj),

(15)

where Lij : R × R → R+ are given loss functions for i = 1, . . . , m and j = 1, . . . , n. Problem
(15) reduces to PCA with generalized regularization when Lij(u, a) = (a − u)2. However,
the loss function Lij can now depend on the data Aij in a more complex way.

4.1 Solution methods

As before, problem (15) is not convex, even when Lij, ri and ˜rj are convex; but if all these
functions are convex, then the problem is bi-convex.

Alternating minimization. Alternating minimization can still be used to ﬁnd a local
minimum, and it is still often possible to use factorization caching to speed up the solution
of the subproblems that arise in alternating minimization. We defer a discussion of how to
solve these subproblems explicitly to §7.

Stochastic proximal gradient method. For use with extremely large scale problems,
we discuss fast variants of the basic alternating minimization algorithm in §7. For example,
we present an alternating directions stochastic proximal gradient method. This algorithm
accesses the functions Lij, ri, and ˜rj only through a subgradient or proximal interface,
allowing it to generalize trivially to nearly any loss function and regularizer. We defer a
more detailed discussion of this method to §7.

4.2 Examples

Weighted PCA. A simple modiﬁcation of the PCA objective is to weight the importance
of ﬁtting each element in the matrix A. In the generalized low rank model, we let Lij(u−a) =
wij(a − u)2, where wij is a weight, and take r = ˜r = 0. Unlike PCA, the weighted PCA
problem has no known analytical solution [SJ03]. In fact, it is NP-hard to ﬁnd an exact
solution to weighted PCA [GG11], although it is not known whether it is always possible to
ﬁnd approximate solutions of moderate accuracy eﬃciently.

Robust PCA. Despite its widespread use, PCA is very sensitive to outliers. Many authors
have proposed a robust version of PCA obtained by replacing least-squares loss with (cid:96)1 loss,
which is less sensitive to large outliers [CLMW11, WGR+09, XCS12]. They propose to solve
the problem

minimize
(cid:107)S(cid:107)1 + (cid:107)Z(cid:107)∗
subject to S + Z = A.

(16)

22

The authors interpret Z as a robust version of the principal components of the data matrix
A, and S as the sparse, possibly large noise corrupting the observations.

We can frame robust PCA as a GLRM in the following way. If Lij(u, a) = |a − u|, and

r(x) = γ

2 (cid:107)x(cid:107)2

2, ˜r(y) = γ

2 (cid:107)y(cid:107)2

2, then (15) becomes
minimize (cid:107)A − XY (cid:107)1 + γ

2 (cid:107)X(cid:107)2

F + γ

2 (cid:107)Y (cid:107)2
F .

Using the arguments in §7.6, we can rewrite the problem by introducing a new variable
Z = XY as

This results in a rank-constrained version of the estimator proposed in the literature on
robust PCA [WGR+09, CLMW11, XCS12]:

minimize
subject to Rank(Z) ≤ k.

(cid:107)A − Z(cid:107)1 + γ(cid:107)Z(cid:107)∗

minimize
subject to S + Z = A

(cid:107)S(cid:107)1 + γ(cid:107)Z(cid:107)∗

Rank(Z) ≤ k,

where we have introduced the new variable S = A − Z.

Huber PCA. The Huber function is deﬁned as
(cid:26) (1/2)x2

huber(x) =

|x| − (1/2)

|x| ≤ 1
|x| > 1.

Using Huber loss,

L(u, a) = huber(u − a),

in place of (cid:96)1 loss also yields an estimator robust to occasionaly large outliers [Hub81]. The
Huber function is less sensitive to small errors |u − a| than the (cid:96)1 norm, but becomes linear
in the error for large errors. This choice of loss function results in a generalized low rank
model formulation that is robust both to large outliers and to small Gaussian perturbations
in the data.

Previously, the problem of Gaussian noise in robust PCA has been treated by decompos-
ing the matrix A = L + S + N into a low rank matrix L, a sparse matrix S, and a matrix
with small Gaussian entries N by minimizing the loss

(cid:107)L(cid:107)∗ + (cid:107)S(cid:107)1 + (1/2)(cid:107)N (cid:107)2
F

over all decompositions A = L + S + N of A [XCS12].

In fact, this formulation is equivalent to Huber PCA with quadratic regularization on
the factors X and Y . The argument showing this is very similar to the one we made above
for robust PCA. The only added ingredient is the observation that

huber(x) = inf{|s| + (1/2)n2 : x = n + s}.

In other words, the Huber function is the inﬁmal convolution of the negative log likelihood
of a gaussian random variable and a laplacian random variable: it represents the most likely
assignment of (additive) blame for the error x to a gaussian error n and a laplacian error s.

23

Robust regularized PCA. We can design robust versions of all the regularized PCA
problems above by the same transformation we used to design robust PCA. Simply replace
the quadratic loss function with an (cid:96)1 or Huber loss function. For example, k-mediods
[KR09, PJ09] is obtained by using (cid:96)1 loss in place of quadratic loss in the quadratic clustering
problem. Similarly, robust subspace clustering [SEC13] can be obtained by using an (cid:96)1 or
Huber penalty in the subspace clustering problem.

Quantile PCA. For some applications, it can be much worse to overestimate the entries
of A than to underestimate them, or vice versa. One can capture this asymmetry by using
the loss function

L(u, a) = α(a − u)+ + (1 − α)(u − a)+

and choosing α ∈ (0, 1) appropriately. This loss function is sometimes called a scalene loss,
and can be interpreted as performing quantile regression, e.g., ﬁtting the 20th percentile
[KB78, Koe05].

Fractional PCA. For other applications, we may be interested in ﬁnding an approxima-
tion of the matrix A whose entries are close to the original matrix on a relative, rather than
an absolute, scale. Here, we assume the entries Aij are all positive. The loss function

L(u, a) = max

(cid:18) a − u
u

,

u − a
a

(cid:19)

can capture this objective. A model (X, Y ) with objective value less than 0.10mn gives a
low rank matrix XY that is on average within 10% of the original matrix.

Logarithmic PCA. Logarithmic loss functions may also useful for ﬁnding an approxima-
tion of A that is close on a relative, rather than absolute, scale. Once again, we assume all
entries of A are positive. Deﬁne the logarithmic loss

This loss is not convex, but has the nice property that it ﬁts the geometric mean of the data:

To see this, note that we are solving a least squares problem in log space. At the solution,
log(u) will be the mean of log(ai), i.e.,

L(u, a) = log2(u/a).

argmin
u

(cid:88)

i

L(u, ai) = (

ai)1/n.

(cid:89)

i

log(u) = 1/n

log(ai) = log

(cid:88)

i

(cid:33)

ai)1/n

.

(cid:32)
(

(cid:89)

i

24

It is easy to formulate a version of PCA corresponding to
Exponential family PCA.
any loss in the exponential family. Here we give some interesting loss functions generated
by exponential families when all the entries Aij are positive. (See [CDS01] for a general
treatment of exponential family PCA.) One popular loss function in the exponential family
is the KL-divergence loss,

which corresponds to a Poisson generative model [CDS01].

Another interesting loss function is the Itakura-Saito (IS) loss,

L(u, a) = a log

− a + u,

(cid:17)

(cid:16) a
u

L(u, a) = log

− 1 +

(cid:17)

(cid:16) a
u

a
u

,

which has the property that it is scale invariant, so scaling a and u by the same factor
produces the same loss [SF14]. The IS loss corresponds to Tweedie distributions (i.e., distri-
butions for which the variance is some power of the mean) [Twe84]. This makes it interesting
in applications, such as audio processing, where fractional errors in recovery are perceived.

The β-divergence,

L(u, a) =

aβ
β(β − 1)

+

−

uβ
β

auβ−1
β − 1

,

generalizes both of these losses. With β = 2, we recover quadratic loss; in the limit as β → 1,
we recover the KL-divergence loss; and in the limit as β → 0, we recover the IS loss [SF14].

4.3 Oﬀsets and scaling

In §2.6, we saw how to use standardization to rescale the data in order to compensate for
unequal scaling in diﬀerent features. In general, standardization destroys sparsity in the data
by subtracting the (column) means (which are in general non-zero) from each element of the
data matrix A. It is possible to instead rescale the loss functions in order to compensate for
unequal scaling. Scaling the loss functions instead has the advantage that no arithmetic is
performed directly on the data A, so sparsity in A is preserved.

A savvy user may be able to select loss functions Lij that are scaled to reﬂect the
importance of ﬁtting diﬀerent columns. However, it is useful to have a default automatic
scaling for times when no savvy user can be found. The scaling proposed here generalizes
the idea of standardization to a setting with heterogeneous loss functions.

Given initial loss functions Lij, which we assume are nonnegative, for each feature j let

µj = argmin

Lij(µ, Aij),

(cid:88)

µ

i:(i,j)∈Ω

σ2
j =

1
nj − 1

(cid:88)

i:(i,j)∈Ω

Lij(µj, Aij).

It is easy to see that µj generalizes the mean of column j, while σ2
j generalizes the column
variance. For example, when Lij(u, a) = (u − a)2 for every i = 1, . . . , m, j = 1, . . . , n, µj is
the mean and σ2
j is the sample variance of the jth column of A. When Lij(u, a) = |u − a| for
every i = 1, . . . , m, j = 1, . . . , n, µj is the median of the jth column of A, and σ2
j is the sum

25

of the absolute values of the deviations of the entries of the jth column from the median
value.

To ﬁt a standardized GLRM, we rescale the loss functions by σ2

j and solve

minimize (cid:80)

(i,j)∈Ω Lij(Aij, xiyj + µj)/σ2

j + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj).

(17)

Note that this problem can be recast in the standard form for a generalized low rank model
(15). For the oﬀset, we may use the same trick described in §3.3 to encode the oﬀset in the
regularization; and for the scaling, we simply replace the original loss function Lij by Lij/σ2
j .

5 Loss functions for abstract data types

We began our study of generalized low rank modeling by considering the best way to ap-
proximate a matrix by another matrix of lower rank. In this section, we apply the same
procedure to approximate a data table that may not consist of real numbers, by choosing a
loss function that respects the data type.

We now consider A to be a table consisting of m examples (i.e., rows, samples) and n
features (i.e., columns, attributes), with each entry Aij drawn from a feature set Fj. The
feature set Fj may be discrete or continuous. So far, we have only considered numerical data
(Fj = R for j = 1, . . . , n), but now Fj can represent more abstract data types. For example,
entries of A can take on Boolean values (Fj = {T, F }), integral values (Fj = 1, 2, 3, . . .),
ordinal values (Fj = {very much, a little, not at all}), or consist of a tuple of these types
(Fj = {(a, b) : a ∈ R}).

We are given a loss function Lij : R × Fj → R. The loss Lij(u, a) describes the approxi-
mation error incurred when we represent a feature value a ∈ Fj by the number u ∈ R. We
give a number of examples of these loss functions below.

We now formulate a generalized low rank model on the database A as

minimize (cid:80)

(i,j)∈Ω Lij(xiyj, Aij) + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(yj),

(18)

with variables X ∈ Rn×k and Y ∈ Rk×m, and with loss Lij as above and regularizers
ri(xi) : R1×k → R and ˜rj(yj) : Rk×1 → R (as before). When the domain of each loss
function is R × R, we recover the generalized low rank model on a matrix (15).

5.1 Solution methods

As before, this problem is not convex, but it is bi-convex if ri, and ˜rj are convex, and Lij is
convex in its ﬁrst argument. The problem is also separable across samples i = 1, . . . , m and
features j = 1, . . . , m. These properties makes it easy to perform alternating minimization
on this objective. Once again, we defer a discussion of how to solve these subproblems
explicitly to §7.

26

+
)
u
a
−
1
(

4

3

2

1

0

a = −1

a = 1

a = −1

a = 1

−3 −2 −1

1

2

3

−3 −2 −1

1

2

3

0
u

0
u

Figure 1: Hinge loss.

Figure 2: Logistic loss.

5.2 Examples

Boolean PCA. Suppose Aij ∈ {−1, 1}m×n, and we wish to approximate this Boolean
matrix. For example, we might suppose that the entries of A are generated as noisy, 1-
bit observations from an underlying low rank matrix XY . Surprisingly, it is possible to
accurately estimate the underlying matrix with only a few observations |Ω| from the matrix
by solving problem (18) (under a few mild technical conditions) with an appropriate loss
function [DPBW12].

We may take the loss to be

L(u, a) = (1 − au)+,

which is the hinge loss (see Figure 1), and solve the problem (18) with or without regulariza-
tion. When the regularization is sum of squares (r(x) = λ(cid:107)x(cid:107)2
2), ﬁxing X and
minimizing over yj is equivalent to training a support vector machine (SVM) on a data set
consisting of m examples with features xi and labels Aij. Hence alternating minimization
for the problem (15) reduces to repeatedly training an SVM. This model has been previously
considered under the name Maximum Margin Matrix Factorization (MMMF) [SRJ04, RS05].

2, ˜r(y) = λ(cid:107)y(cid:107)2

Logistic PCA. Again supposing Aij ∈ {−1, 1}m×n, we can also use a logistic loss to
measure the approximation quality. Let

L(u, a) = log(1 + exp(−au))

(see Figure 2). With this loss, ﬁxing X and minimizing over yj is equivalent to using logistic
regression to predict the labels Aij. This model has been previously considered under the
name logistic PCA [SSU03].

)
)
u
a
(
p
x
e
+
1
(
g
o
l

3

2

1

0

27

Figure 3: Ordinal hinge loss.

Poisson PCA. Now suppose the data Aij are nonnegative integers. We can use any loss
function that might be used in a regression framework to predict integral data to construct
a generalized low rank model for Poisson PCA. For example, we can take

L(u, a) = exp(u) − au + a log a − a.

This is the exponential family loss corresponding to Poisson data. (It diﬀers from the KL-
divergence loss from §4.2 only in that u has been replaced by exp(u), which allows u to take
negative values.)

Ordinal PCA. Suppose the data Aij records the levels of some ordinal variable, encoded
as {1, 2, . . . , d}. We wish to penalize the entries of the low rank matrix XY which deviate
by many levels from the encoded ordinal value. A convex version of this penalty is given by
the ordinal hinge loss,

L(u, a) =

(1 − u + a(cid:48))+ +

(1 + u − a(cid:48))+,

(19)

a−1
(cid:88)

a(cid:48)=1

d
(cid:88)

a(cid:48)=a+1

which generalizes the hinge loss to ordinal data (see Figure 3).

This loss function may be useful for encoding Likert-scale data indicating degrees of

agreement with a question [Lik32]. For example, we might have

Fj = {strongly disagree, disagree, neither agree nor disagree, agree, strongly agree}.

We can encode these levels as the integers 1, . . . , 5 and use the above loss to ﬁt a model to
ordinal data.

This approach assumes that every increment of error is equally bad: for example, that
approximating “agree” by “strongly disagree” is just as bad as aproximating “neither agree
nor disagree” by “agree”. In §6.1 we introduce a more ﬂexible ordinal loss function that can
learn a more ﬂexible relationship between ordinal labels. For example, it could determine
that the diﬀerence between “agree” and “strongly disagree” is smaller than the diﬀerence
between “neither agree nor disagree” and “agree”.

28

Interval PCA. Suppose that the data Aij ∈ R2 are tuples denoting the endpoints of an
interval, and we wish to ﬁnd a low rank matrix whose entries lie inside these intervals. We
can capture this objective using, for example, the deadzone-linear loss

L(u, a) = max((a1 − u)+, (u − a2)+).

5.3 Missing data and data imputation

We can use the solution (X, Y ) to a low rank model to impute values corresponding to
missing data (i, j) (cid:54)∈ Ω. This process is sometimes also called inference. Above, we saw that
for quadratically regularized PCA, the MAP estimator for the missing entry Aij is equal to
xiyj. This is still true for many of the loss functions above, such as the Huber function or (cid:96)1
loss, for which it makes sense for the data to take on any real value.

However, to approximate abstract data types we must consider a more nuanced view.
While we can still think of the solution (X, Y ) to the generalized low rank model (15) in
Boolean PCA as approximating the Boolean matrix A, the solution is not a Boolean matrix.
Instead we say that we have encoded the original Boolean matrix as a real-valued low rank
matrix XY , or that we have embedded the original Boolean matrix into the space of real-
valued matrices.

To ﬁll in missing entries in the original matrix A, we compute the value ˆAij that minimizes

the loss for xiyj:

ˆAij = argmin

Lij(xiyj, a).

a

This implicitly constrains ˆAij to lie in the domain Fj of Lij. When Lij : R × R → R, as is
the case for the losses in §4 above (including (cid:96)2, (cid:96)1, and Huber loss), then ˆAij = xiyj. But
when the data is of an abstract type, the minimum argmina Lij(u, a) will not in general be
equal to u.

For example, when the data is Boolean, Lij : {0, 1} × R → R, we compute the Boolean

matrix ˆA implied by our low rank model by solving

for MMMF, or

ˆAij = argmin

(a(XY )ij − 1)+

a∈{0,1}

ˆAij = argmin

a∈{0,1}

log(1 + exp(−a(XY )ij))

for logistic PCA. These problems both have the simple solution

ˆAij = sign(xiyj).

When Fj is ﬁnite, inference partitions the real numbers into regions

Ra = {x ∈ R : Lij(u, x) = min

Lij(u, a)}

a

corresponding to diﬀerent values a ∈ Fj. When Lij is convex, these regions are intervals.

29

We can use the estimate ˆAij even when (i, j) ∈ Ω was observed. If the original obser-
vations have been corrupted by noise, we can view ˆAij as a denoised version of the original
data. This is an unusual kind of denoising: both the noisy (Aij) and denoised ( ˆAij) versions
of the data lie in the abstract space Fj.

5.4 Interpretations and applications

We have already discussed some interpretations of X and Y in the PCA setting. Now we
reconsider those interpretations in the context of approximating these abstract data types.

Archetypes. As before, we can think of each row of Y as an archetype which captures the
behavior of an idealized example. However, the rows of Y are real numbers. To represent
each archetype l = 1, . . . , k in the abstract space as Yl with (Yl)j ∈ Fj, we solve

(Yl)j = argmin

Lj(ylj, a).

a∈Fj

(Here we assume that the loss Lij = Lj is independent of the example i.)

Archetypical representations. As before, we call xi the representation of example i in
terms of the archetypes. The rows of X give an embedding of the examples into Rk, where
each coordinate axis corresponds to a diﬀerent archetype. If the archetypes are simple to
understand or interpret, then the representation of an example can provide better intuition
about that example.

In contrast to the initial data, which may consist of arbitrarily complex data types, the
representations xi will be low dimensional vectors, and can easily be plotted, clustered, or
used in nearly any kind of machine learning algorithm. Using the generalized low rank model,
we have converted an abstract feature space into a vector space.

Feature representations. The columns of Y embed the features into Rk. Here we think
of the columns of X as archetypical features, and represent each feature j as a linear com-
bination of the archetypical features. Just as with the examples, we might choose to apply
any machine learning algorithm to the feature representations.

This procedure allows us to compare non-numeric features using their representation in
Rl. For example, if the features F are Likert variables giving the extent to which respondents
on a questionnaire agree with statements 1, . . . , n, we might be able to say that questions i
and j are similar if (cid:107)yi − yj(cid:107) is small; or that question i is a more polarizing form of question
j if yi = αyj, with α > 1.

Even more interesting, it allows us to compare features of diﬀerent types. We could say

that the real-valued feature i is similar to Likert-valued question j if (cid:107)yi − yj(cid:107) is small.

30

Latent variables. Each row of X represents an example by a vector in Rk. The matrix Y
maps these representations back into the original feature space (now nonlinearly) as described
in the discussion on data imputation in §5.3. We might think of X as discovering the latent
variables that best explain the observed data, with the added beneﬁt that these latent
variables lie in the vector space Rk.
(i,j)∈Ω Lij(xiyj, Aij) is
small, then we view these latent variables as providing a good explanation or summary of
the full data set.

If the approximation error (cid:80)

Probabilistic intepretation. We can give a probabilistic interpretation of X and Y ,
generalizing the hierarchical Bayesian model presented by Fithian and Mazumder in [FM13].
We suppose that the matrices ¯X and ¯Y are generated according to a probability distribution
with probability proportional to exp(−r( ¯X)) and exp(−˜r( ¯Y )), respectively. Our observations
A of the entries in the matrix ¯Z = ¯X ¯Y are given by

where the random variable ψij(u) takes value a with probability proportional to

Aij = ψij(( ¯X ¯Y )ij),

exp (−Lij(u, a)) .

We observe each entry (i, j) ∈ Ω. Then to ﬁnd the maximum a posteriori (MAP) estimator
(X, Y ) of ( ¯X, ¯Y ), we solve

maximize exp

(cid:16)

− (cid:80)

(cid:17)
(i,j)∈Ω Lij(xiyj, Aij)

exp(−r(X)) exp(−˜r(Y )),

which is equivalent, by taking logs, to problem (18).

This interpretation gives us a simple way to interpret our procedure for imputing missing

observations (i, j) (cid:54)∈ Ω. We are simply computing the MAP estimator ˆAij.

Auto-encoder. The matrix X encodes the data; the matrix Y decodes it back into the
full space. We can view (18) as providing the best linear auto-encoder for the data. Among
all linear encodings (X) and decodings (Y ) of the data, the abstract generalized low rank
model (18) minimizes the reconstruction error measured according to the loss functions Lij.

Compression. We impose an information bottleneck by using a low rank auto-encoder
to ﬁt the data. The bottleneck is imposed by both the dimensionality reduction and the
regularization, giving both soft and hard constraints on the information content allowed.
The solution (X, Y ) to problem (18) maximizes the information transmitted through this
k-dimensional bottleneck, measured according to the loss functions Lij. This X and Y give
a compressed and real-valued representation that may be used to more eﬃciently store or
transmit the information present in the data.

31

5.5 Oﬀsets and scaling

Just as in the previous section, better practical performance can often be achieved by allowing
an oﬀset in the model as described in §3.3, and automatic scaling of loss functions as described
in §4.3. As we noted in §4.3, scaling the loss functions (instead of standardizing the data)
has the advantage that no arithmetic is performed directly on the data A. When the data A
consists of abstract types, it is quite important that no arithmetic is performed on the data,
so that we need not take the average of, say, “very much” and “a little”, or subtract it from
“not at all”.

5.6 Numerical examples

In this section we give results of some small experiments illustrating the use of diﬀerent loss
functions adapted to abstract data types, and comparing their performance to quadratically
regularized PCA. To ﬁt these GLRMs, we use alternating minimization and solve the sub-
problems with subgradient descent. This approach is explained more fully in §7. Running
the alternating subgradient method multiple times on the same GLRM from diﬀerent initial
conditions yields diﬀerent models, all with very similar (but not identical) objective values.

Boolean PCA. For this experiment, we generate Boolean data A ∈ {−1, +1}n×m as

A = sign (cid:0)X trueY true(cid:1),
where X true ∈ Rn×ktrue and Y true ∈ Rktrue×m have independent, standard normal entries. We
consider a problem instance with m = 50, n = 50, and ktrue = k = 10.

We ﬁt two GLRMs to this data to compare their performance. Boolean PCA uses hinge
loss L(u, a) = max (1 − au, 0) and quadratic regularization r(u) = ˜r(u) = .1(cid:107)u(cid:107)2
2, and pro-
duces the model (X bool, Y bool). Quadratically regularized PCA uses squared loss L(u, a) =
(u − a)2 and the same quadratic regularization, and produces the model (X real, Y real).

Figure 4 shows the results of ﬁtting Boolean PCA to this data. The ﬁrst column shows
the original ground-truth data A; the second shows the imputed data given the model, ˆAbool,
generated by rounding the entries of X boolY bool to the closest number in 0, 1 (as explained in
§5.3); the third shows the error A− ˆAbool. Figure 4 shows the results of running quadratically
regularized PCA on the same data, and shows A, ˆAreal, and A − ˆAreal.

As expected, Boolean PCA performs substantially better than quadratically regularized
PCA on this data set. On average over 100 draws from the ground truth data distribution,
the misclassiﬁcation error (percentage of misclassiﬁed entries)

(cid:15)(X, Y ; A) =

#{(i, j) | Aij (cid:54)= sign (XY )ij}
mn

is much lower using hinge loss ((cid:15)(X bool, Y bool; A) = 0.0016) than squared loss ((cid:15)(X real, Y real; A) =
0.0051). The average RMS errors

RMS(X, Y ; A) =

(Aij − (XY )ij)2

(cid:33)1/2

(cid:32)

1
mn

m
(cid:88)

n
(cid:88)

i=1

j=1

32

using hinge loss (RMS(X bool, Y bool; A) = 0.0816) and squared loss (RMS(X real, Y real; A) =
0.159) also indicate an advantage for Boolean PCA.

Figure 4: Boolean PCA on Boolean data.

Figure 5: Quadratically regularized PCA on Boolean data.

Censored PCA.
In this example, we consider the performance of Boolean PCA when
only a subset of positive entries in the Boolean matrix A ∈ {−1, 1}m×n have been observed,
i.e., the data has been censored. For example, a retailer might know only a subset of the
products each customer purchased; or a medical clinic might know only a subset of the
diseases a patient has contracted, or of the drugs the patient has taken. Imputation can be
used in this setting to (attempt to) distinguish true negatives Aij = −1 from unobserved
positives Aij = +1, (i, j) (cid:54)∈ Ω.

We generate a low rank matrix B = XY ∈ [0, 1]m×n with X ∈ Rm×k, Y ∈ Rk×n, where
the entries of X and Y are drawn from a uniform distribution on [0, 1], m = n = 300 and
k = 3. Our data matrix A is chosen by letting Aij = 1 with probability proportional to
Bij, and −1 otherwise; the constant of proportionality is chosen so that half of the entries
in A are positive. We ﬁt a rank 5 GLRM to an observation set Ω consisting of 10% of the
positive entries in the matrix, drawn uniformly at random, using hinge loss and quadratic

33

regularization. That is, we ﬁt the low rank model

minimize (cid:80)

(i,j)∈Ω max(1 − xiyjAij, 0) + γ (cid:80)m

i=1 (cid:107)xi(cid:107)2

2 + γ (cid:80)n

j=1 (cid:107)yj(cid:107)2
2

and vary the regularization parameter γ.

normalized training error,

We consider three error metrics to measure the performance of the ﬁtted model (X, Y ):

normalized test error,

1
|Ω|

(cid:88)

(i,j)∈Ω

1
|ΩC|

(cid:88)

(i,j)∈ΩC

max(1 − Aijxiyj, 0),

max(1 − Aijxiyj, 0),

and precision at 10 (p@10), which is computed as the fraction of the top ten predicted
values not in the observation set, {xiyj : (i, j) ∈ ΩC}, for which Aij = 1. (Here, ΩC =
{1, . . . , m} × {1, . . . , n} \ Ω.) Precision at 10 measures the usefulness of the model:
if we
predicted that the top 10 unseen elements (i, j) had values +1, how many would we get
right?

Figure 6 shows the regularization path as γ ranges from 0 to 40, averaged over 50 sam-
ples from the distribution generating the data. Here, we see that while the training error
decreases as γ decreases, the test error reaches a minimum around γ = 5. Interestingly, the
precision at 10 improves as the regularization increases; since precision at 10 is computed
using only relative rather than absolute values of the model, it is insensitive to the shrinkage
of the parameters introduced by the regularization. The grey line shows the probability of
identifying a positive entry by guessing randomly; precision at 10, which exceeds 80% when
γ (cid:38) 30, is signiﬁcantly higher. This performance is particularly impressive given that the
observations Ω are generated by sampling from rather than rounding the auxiliary matrix
B.

Mixed data types.
In this experiment, we ﬁt a GLRM to a data table with numerical,
Boolean, and ordinal columns generated as follows. Let N1, N2, and N3 partition the column
indices 1, . . . , n. Choose X true ∈ Rm×ktrue, Y true ∈ Rktrue×n to have independent, standard
normal entries. Assign entries of A as follows:

Aij =






xiyj
sign (xiyj)
round(3xiyj + 1)

j ∈ N1
j ∈ N2
j ∈ N3,

where the function round maps a to the nearest integer in the set {1, . . . , 7}. Thus, N1
corresponds to real-valued data; N2 corresponds to Boolean data; and N3 corresponds to
ordinal data. We consider a problem instance in which m = 100, n1 = 40, n2 = 30, n3 = 30,
and ktrue = k = 10.

34

train error
test error

p@10

r
o
r
r
e

d
e
z
i
l
a
m
r
o
n

8

6

4

2

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1
+

f
o

y
t
i
l
i
b
a
b
o
r
p

0
0

5
5

10
10

15
15

20
20
regularization parameter
regularization parameter

25
25

30
30

35
35

40
40

Figure 6: Error metrics for Boolean GLRM on censored data. The grey line shows
the probability of identifying a positive entry by guessing randomly.

35

We ﬁt a heterogeneous loss GLRM to this data with loss function

Lij(u, a) =






Lreal(u, a)
Lbool(u, a)
Lord(u, a)

j ∈ N1
j ∈ N2
j ∈ N3,

where Lreal(u, a) = (u − a)2, Lbool(u, a) = (1 − au)+, and Lord(u, a) is deﬁned in (19), and
with quadratic regularization r(u) = ˜r(u) = .1(cid:107)u(cid:107)2
2. We ﬁt the GLRM to produce the model
(X mix, Y mix). For comparison, we also ﬁt quadratically regularized PCA to the same data,
using Lij(u, a) = (u − a)2 for all j and quadratic regularization r(u) = ˜r(u) = .1(cid:107)u(cid:107)2
2, to
produce the model (X real, Y real).

Figure 7 shows the results of ﬁtting the heterogeneous loss GLRM to the data. The ﬁrst
column shows the original ground-truth data A; the second shows the imputed data given
the model, ˆAmix, generated by rounding the entries of X mixY mix to the closest number in
0, 1 (as explained in §5.3); the third shows the error A − ˆAmix. Figure 8 corresponds to
quadratically regularized PCA, and shows A, ˆAreal, and A − ˆAreal.

To evaluate error for Boolean and ordinal data, we use the misclassiﬁcation error (cid:15) deﬁned
above. For notational convenience, we let YNl (ANl) denote Y (A) restricted to the columns
Nl in order to pick out real-valued columns (l = 1), Boolean columns (l = 2), and ordinal
columns (l = 3).

Table 1 compare the average error (diﬀerence between imputed entries and ground truth)
over 100 draws from the ground truth distribution for models using heterogeneous loss (X mix,
Y mix) and quadratically regularized loss (X real, Y real). Columns are labeled by error metric.
We use misclassiﬁcation error (cid:15) for Boolean and ordinal data and MSE for numerical data.

X mix, Y mix
X real , Y real

MSE(X, YN1; AN1)
0.0224
0.0076

(cid:15)(X, YN2; AN2)
0.0074
0.0213

(cid:15)(X, YN3; AN3)
0.0531
0.0618

Table 1: Average error for numerical, Boolean, and ordinal features using GLRM
with heterogenous loss and quadratically regularized loss.

Missing data. Here, we explore the eﬀect of missing entries on the accuracy of the re-
covered model. We generate data A as detailed above, but then censor one large block of
entries in the table (constituting 3.75% of numerical, 50% of Boolean, and 50% of ordinal
data), removing them from the observed set Ω.

Figure 9 shows the results of ﬁtting the heterogeneous loss GLRM described above on
the censored data. The ﬁrst column shows the original ground-truth data A; the second
shows the block of data that has been removed from the observation set Ω; the third shows
the imputed data given the model, ˆAmix, generated by rounding the entries of X mixY mix to
the closest number in {0, 1} (as explained in §5.3); the fourth shows the error A − ˆAmix.

36

Figure 7: Heterogeneous loss GLRM on mixed data.

Figure 8: Quadratically regularized PCA on mixed data.

37

Figure 9: Heterogeneous loss GLRM on missing data.

Figure 10: Quadratically regularized PCA on missing data.

Figure 10 corresponds to running quadratically regularized PCA on the same data, and
shows A, ˆAreal, and A − ˆAreal. While quadradically regularized PCA and the heterogeneous
loss GLRM performed similarly when no data was missing, the heterogeneous loss GLRM
performs much better than quadradically regularized PCA when a large block of data is
censored.

We compare the average error (diﬀerence between imputed entries and ground truth) over
100 draws from the ground truth distribution in Table 2. As above, we use misclassiﬁcation
error (cid:15) for Boolean and ordinal data and MSE for numerical data.

X mix, Y mix
X real , Y real

MSE(X, YN1; AN1)
0.392
0.561

(cid:15)(X, YN2; AN2)
0.2968
0.4029

(cid:15)(X, YN3; AN3)
0.3396
0.9418

Table 2: Average error over imputed data using a GLRM with heterogenous loss
and regularized quadratic loss.

6 Multi-dimensional loss functions

In this section, we generalize the procedure to allow the loss functions to depend on blocks
of the matrix XY , which allows us to represent abstract data types more naturally. For
example, we can now represent categorical values , permutations, distributions, and rankings.

38

We are given a loss function Lij : R1×dj ×Fj → R, where dj is the embedding dimension of
feature j, and d = (cid:80)
j dj is the total dimension of the embedded features. The loss Lij(u, a)
describes the approximation error incurred when we represent a feature value a ∈ Fj by the
vector u ∈ Rdj .

Let xi ∈ R1×k be the ith row of X (as before), and let Yj ∈ Rk×dj be the jth block
matrix of Y so the columns of Yj correspond to the columns of embedded feature j. We now
formulate a multi-dimensional generalized low rank model on the database A,

minimize (cid:80)

(i,j)∈Ω Lij(xiYj, Aij) + (cid:80)m

i=1 ri(xi) + (cid:80)n

j=1 ˜rj(Yj),

(20)

with variables X ∈ Rn×k and Y ∈ Rk×d, and with loss Lij as above and regularizers
ri(xi) : R1×k → R (as before) and ˜rj(Yj) : Rk×dj → R. Note that the ﬁrst argument of Lij
is a row vector with dj entries, and the ﬁrst argument of rj is a matrix with dj columns.
When every entry Aij is real-valued (i.e., dj = 1), then we recover the generalized low rank
model (15) seen in the previous section.

6.1 Examples

Categorical PCA. Suppose that a ∈ F is a categorical variable, taking on one of d values
or labels. Identify the labels with the integers {1, . . . , d}. In (20), set

L(u, a) = (1 − ua)+ +

(1 + ua(cid:48))+,

(cid:88)

a(cid:48)∈F , a(cid:48)(cid:54)=a

and use the quadratic regularizer ri = γ(cid:107) · (cid:107)2

2, ˜r = γ(cid:107) · (cid:107)2
2.

Fixing X and optimizing over Y is equivalent to training one SVM per label to separate
that label from all the others: the jth column of Y gives the weight vector corresponding
to the jth SVM. (This is sometimes called one-vs-all multiclass classiﬁcation [RK04].) Op-
timizing over X identiﬁes the low-dimensional feature vectors for each example that allow
these SVMs to most accurately predict the labels.

The diﬀerence between categorical PCA and Boolean PCA is in how missing labels are
imputed. To impute a label for entry (i, j) with feature vector xi according to the procedure
described above in 5.3, we project the representation Yj onto the line spanned by xi to form
u = xiYj. Given u, the imputed label is simply argmaxl ul. This model has the interesting
property that if column l(cid:48) of Yj lies in the interior of the convex hull of the columns of Yj,
then ul(cid:48) will lie in the interior of the interval [minl ul, maxl ul] [BV04]. Hence the model will
never impute label l(cid:48) for any example.

We need not restrict ourselves to the loss function given above. In fact, any loss func-
tion that can be used to train a classiﬁer for categorical variables (also called a multi-class
classiﬁer) can be used to ﬁt a categorical PCA model, so long as the loss function depends
only on the inner products between the parameters of the model and the features corre-
sponding to each example. The loss function becomes the loss function L used in (20); the
optimal parameters of the model give the optimal matrix Y , while the implied features will
populate the optimal matrix X. For example, it is possible to use loss functions derived

39

from error-correcting output codes [DB95]; the Directed Acyclic Graph SVM [PCST99]; the
Crammer-Singer multi-class loss [CS02]; or the multi-category SVM [LLW04].

Of these loss functions, only the one-vs-all loss is separable across the classes a ∈ F.
(By separable, we mean that the objective value can be written as a sum over the classes.)
Hence ﬁtting a categorical features with any other loss functions is not the same as ﬁtting d
Boolean features. For example, in the Crammer-Singer loss

L(u, a) = (1 − ua + max

u(cid:48)
a)+,

a(cid:48)∈F , a(cid:48)(cid:54)=a

the classes are combined according to their maximum, rather than their sum. While one-vs-
all classiﬁcation performs about as well as more sophisticated loss functions on small data
sets [RK04], these more sophisticated nonseparable loss tend to perform much better as the
number of classes (and examples) increases [GBW14].

Some interesting nonconvex loss functions have also been suggested for this problem. For

example, consider a generalization of Hamming distance to this setting,

L(u, a) = δua,1 +

δua(cid:48) ,0,

(cid:88)

a(cid:48)(cid:54)=a

where δα,β = 0 if α = β and 1 otherwise.
In this case, alternating minimization with
regularization that enforces a clustered structure in the low rank model (see the discussion
of quadratic clustering in §3.2) reproduces the k-modes algorithm [HN99].

Ordinal PCA. We saw in §5 one way to ﬁt a GLRM to ordinal data. Here, we use a
larger embedding dimension for ordinal features. The multi-dimensional embedding will be
particularly useful when the best mapping of the ordinal variable onto a linear scale is not
uniform; e.g., if level 1 of the ordinal variable is much more similar to level 2 than level 2
is to level 3. Using a larger embedding dimension allows us to infer the relations between
the levels from the data itself. Here we again identify the labels a ∈ F with the integers
{1, . . . , d}.

One approach we can use for (multi-dimensional) ordinal PCA is to solve (20) with the

loss function

L(u, a) =

(1 − Ia>a(cid:48)ua(cid:48))+,

(21)

and with quadratic regularization. Fixing X and optimizing over Y is equivalent to training
an SVM to separate labels a ≤ l from a > l for each l ∈ F. This approach produces
a set of hyperplanes (given by the columns of Y ) separating each level l from the next.
The hyperplanes need not be parallel to each other. Fixing Y and optimizing over X ﬁnds
the low dimensional features vector for each example that places the example between the
appropriate hyperplanes.
(See Figure 11 for an illustration of an optimal ﬁt of this loss
function, with k = 2, to a simple synthetic data set.)

d−1
(cid:88)

a(cid:48)=1

40

Figure 11: Multi-dimensional ordinal loss.

Permutation PCA. Suppose that a is a permutation of the numbers 1, . . . , d. Deﬁne the
permutation loss

L(u, a) =

(1 − uai + uai+1)+.

d−1
(cid:88)

i=1

This loss is zero if uai > uai+1 + 1 for i = 1, . . . , d − 1, and increases linearly when these
inequalities are violated. Deﬁne sort(u) to return a permutation ˆa of the indices 1, . . . , d so
that uˆai ≥ uˆai+1 for i = 1, . . . , d−1. It is easy to check that argmina L(u, a) = sort(u). Hence
using the permutation loss function in generalized PCA (20) ﬁnds a low rank approximation
of a given table of permutations.

Ranking PCA. Many variants on the permutation PCA problem are possible. For ex-
ample, in ranking PCA, we interpret the permutation as a ranking of the choices 1, . . . , d,
and penalize deviations of many levels more strongly than deviations of only one level by
choosing the loss

L(u, a) =

(1 − uai + uaj )+.

d−1
(cid:88)

d
(cid:88)

i=1

j=i+1

From here, it is easy to generalize to a setting in which the rankings are only partially
observed. Suppose that we observe pairwise comparisons a ⊆ {1, . . . , d} × {1, . . . , d}, where
(i, j) ∈ a means that choice i was ranked above choice j. Then a loss function penalizing

41

devations from these observed rankings is

L(u, a) =

(1 − uai + uaj )+.

(cid:88)

(i,j)∈a

Many other modiﬁcations to ranking loss functions have been proposed in the literature
that interpolate between the the two ﬁrst loss functions proposed above, or which priori-
tize correctly predicting the top ranked choices. These losses include the area under the
curve loss [Ste07], ordered weighted average of pairwise classiﬁcation losses [UBG09], the
weighted approximate-rank pairwise loss [WBU10], the k-order statistic loss [WYW13], and
the accuracy at the top loss [BCMR12].

6.2 Oﬀsets and scaling

Just as in the previous section, better practical performance can often be achieved by allowing
an oﬀset in the model as described in §3.3, and scaling loss functions as described in §4.3.

6.3 Numerical examples

We ﬁt a low rank model to the 2013 American Community Survey (ACS) to illustrate how
to ﬁt a low rank model to heterogeneous data.

The ACS is a survey administered to 1% of the population of the United States each
year to gather their responses to a variety of demographic and economic questions. Our
data sample consists of m = 3132796 responses gathered from residents of the US, excluding
Puerto Rico, in the year 2013, on the 23 questions listed in Table 3.

We ﬁt a rank 10 model to this data using Huber loss for real valued data, hinge loss for
Boolean data, ordinal hinge loss for ordinal data, one-vs-all categorical loss for categorical
data, and regularization parameter γ = .1. We allow an oﬀset in the model and scale the
loss functions and regularization as described in §4.3.

In Table 4, we select a few features j from the model, along with their associated vec-
tors yj, and ﬁnd the two features most similar to them by ﬁnding the two features j(cid:48) which
minimize cos(yj, yj(cid:48)). The model automatically groups states which intuitively share de-
for example, three wealthy states adjoining (but excluding) a major
mographic features:
metropolitan area — Virginia, Maryland, and Connecticut — are grouped together. The
low rank structure also identiﬁes the results (high water prices) of the prolonged drought
aﬄicting California, and corroborates the intuition that work leads only to more work: hours
worked per week, weeks worked per year, and education level are highly correlated.

7 Fitting low rank models

In this section, we discuss a number of algorithms that may be used to ﬁt generalized low
rank models. As noted earlier, it can be computationally hard to ﬁnd the global optimum
of a generalized low rank model. For example, it is NP-hard to compute an exact solution

42

Description
household type
state

commercial use
house on ≥ 10 acres
household income
monthly electricity bill

Variable
HHTYPE
STATEICP
OWNERSHP own home
COMMUSE
ACREHOUS
HHINCOME
COSTELEC
COSTWATR monthly water bill
monthly gas bill
COSTGAS
FOODSTMP on food stamps
HCOVANY
SCHOOL
EDUC
GRADEATT highest grade level attained
EMPSTAT
LABFORCE
CLASSWKR class of worker
WKSWORK2 weeks worked per year
UHRSWORK usual hours worked per week real
looking for work
LOOKING
migration status
MIGRATE1

have health insurance
currently in school
highest level of education

Type
categorical
categorical
Boolean
Boolean
Boolean
real
real
real
real
Boolean
Boolean
Boolean
ordinal
ordinal
categorical
Boolean
Boolean
ordinal

employment status
in labor force

Boolean
categorical

Table 3: ACS variables.

Most similar features
Montana, North Dakota
Illinois, cost of water
Oregon, Idaho
Indiana, Michigan

Feature
Alaska
California
Colorado
Ohio
Pennsylvania Massachusetts, New Jersey
Virginia
Hours worked weeks worked, education

Maryland, Connecticut

Table 4: Most similar features in demography space.

43

to k-means [DFK+04], nonnegative matrix factorization [Vav09], and weighted PCA and
matrix completion [GG11] all of which are special cases of low rank models.

In §7.1, we will examine a number of local optimization methods based on alternating
minimization. Algorithms implementing lazy variants of alternating minimization, such as
the alternating gradient, proximal gradient, or stochastic gradient algorithms, are faster
per iteration than alternating minimization, although they may require more iterations for
convergence. In numerical experiments, we notice that lazy variants often converge to points
with a lower objective value: it seems that these lazy variants are less likely to be trapped
at a saddle point than is alternating minimization. §7.4 explores the convergence of these
algorithms in practice.

We then consider a few special cases in which we can show that alternating minimization
converges to the global optimum in some sense: for example, we will see convergence with
high probability, approximately, and in retrospect. §7.5 discusses a few strategies for initial-
izing these local optimization methods, with provable guarantees in special cases. §7.6 shows
that for problems with convex loss functions and quadratic regularization, it is sometimes
possible to certify global optimality of the resulting model.

7.1 Alternating minimization

We showed earlier how to use alternating minimization to ﬁnd an (approximate) solution
to a generalized low rank model. Algorithm (1) shows how to explicitly extend alternating
minimization to a generalized low rank model (15) with observations Ω.

Algorithm 1

given X 0, Y 0
for k = 1, 2, . . . do

for i = 1, . . . , M do
(cid:16)(cid:80)
xk
i = argminx
end for
for j = 1, . . . , N do
(cid:16)(cid:80)
yk
j = argminy
end for

end for

j:(i,j)∈Ω Lij(xyk−1

j

, Aij) + r(x)

(cid:17)

i:(i,j)∈Ω Lij(xk

i y, Aij) + ˜r(y)

(cid:17)

Parallelization. Alternating minimization parallelizes naturally over examples and fea-
tures.
In Algorithm 1, the loops over i = 1, . . . , N and over j = 1, . . . , M may both be
executed in parallel.

44

7.2 Early stopping

It is not very useful to spend a lot of eﬀort optimizing over X before we have a good estimate
for Y . If an iterative algorithm is used to compute the minimum over X, it may make sense to
stop the optimization over X early before going on to update Y . In general, we may consider
replacing the minimization over x and y above by any update rule that moves towards the
minimum. This templated algorithm is presented as Algorithm 2. Empirically, we ﬁnd that
this approach often ﬁnds a better local minimum than performing a full optimization over
each factor in every iteration, in addition to saving computational eﬀort on each iteration.

Algorithm 2

given X 0, Y 0
for t = 1, 2, . . . do

, Y t−1, A)

for i = 1, . . . , m do

i = updateL,r(xt−1
xt
end for
for j = 1, . . . , n do

i

j = updateL,˜r(y(t−1)T
yt
end for

j

, X (t)T , AT )

end for

We describe below a number of diﬀerent update rules updateL,r by writing the X update.
The Y update can be implemented similarly. (In fact, it can be implemented by substituting
˜r for r, switching the roles of X and Y , and transposing all matrix arguments.) All of the
approaches outlined below can still be executed in parallel over examples (for the X update)
and features (for the Y update).

Gradient method. For example, we might take just one gradient step on the objective.
This method can be used as long as L, r, and ˜r do not take inﬁnite values. (If any of these
functions f is not diﬀerentiable, replace ∇f below by any subgradient of f [BL10, BXM03].)

We implement updateL,r as follows. Let

(cid:88)

g =

j:(i,j)∈Ω

∇Lij(xiyj, Aij)yj + ∇r(xi).

Then set

i = xt−1
xt

i − αtg,

for some step size αt. For example, a common step size rule is αt = 1/t, which guarantees
convergence to the globally optimal X if Y is ﬁxed [BL10, BXM03].

Proximal gradient method.
If a function takes on the value ∞, it need not have a
subgradient at that point, which limits the gradient update to cases where the regularizer

45

and loss are (ﬁnite) real-valued. When the regularizer (but not the loss) takes on inﬁnite
values (say, to represent a hard constraint), we can use a proximal gradient method instead.

The proximal operator of a function f [PB13] is

proxf (z) = argmin

(f (x) +

(cid:107)x − z(cid:107)2

2).

x

1
2

If f is the indicator function of a set C, the proximal operator of f is just (Euclidean)
projection onto C.

A proximal gradient update updateL,r is implemented as follows. Let

Then set

(cid:88)

g =

j:(i,j)∈Ω

∇Lij(xt−1

i

yt−1
j

, Aij)yt−1

.

j

i = proxαtr(xt−1
xt

i − αtg),

for some step size αt. The step size rule αt = 1/t guarantees convergence to the globally
optimal X if Y is ﬁxed, while using a ﬁxed, but suﬃciently small, step size α guarantees
convergence to a small O(α) neighborhood around the optimum [Ber11]. The technical
condition required on the step size is that αt < 1/L, where L is the Lipshitz constant of
the gradient of the objective function. Bolte et al. have shown that the iterates xt
i and
yt
j produced by the proximal gradient update rule (which they call proximal alternating
linearized minimization, or PALM) globally converge to a critical point of the objective
function under very mild conditions on L, r, and ˜r [BST13].

Prox-prox method. Letting ft(X) = (cid:80)
(prox-prox) update

(i,j)∈Ω Lij(xiyt

j, Aij), deﬁne the proximal-proximal

X t+1 = proxαtr(proxαtft(X t)).

The prox-prox update is simply a proximal gradient step on the objective when f is

replaced by the Moreau envelope of f ,

Mf (X) = inf
X (cid:48)

(cid:0)f (X (cid:48)) + (cid:107)X − X (cid:48)(cid:107)2

(cid:1) .

F

(See [PB13] for details.) The Moreau envelope has the same minimizers as the original
objective. Thus, just as the proximal gradient method repeatedly applied to X converges to
global minimum of the objective if Y is ﬁxed, the prox-prox method repeatedly applied to
X also converges to global minimum of the objective if Y is ﬁxed under the same conditions
on the step size αt. for any constant stepsize α ≤ (cid:107)G(cid:107)2
2. (Here, (cid:107)G(cid:107)2 = sup(cid:107)x(cid:107)2≤1 (cid:107)Gx(cid:107)2 is
the operator norm of G.)

This update can also be seen as a single iteration of ADMM when the dual variable
in ADMM is initialized to 0; see [BPC+11].
In the case of quadratic objectives, we will
see below that the prox-prox update can be applied very eﬃciently, making iterated prox-
prox, or ADMM, eﬀective means of computing the solution to the subproblems arising in
alternating minimization.

46

In numerical experiments, we ﬁnd that using a slightly more
Choosing a step size.
nuanced rule allowing diﬀerent step sizes for diﬀerent rows and columns can allow fast
progress towards convergence while ensuring that the value of the objective never increases.
The safeguards on step sizes we propose are quite important in practice: without these
checks, we observe divergence when the initial step sizes are chosen too large.

Motivated by the convergence proof in [Ber11], for each row, we seek a step size on
the order of 1/(cid:107)gi(cid:107)2, where gi is the gradient of the objective function with respect to xi.
We start by choosing an initial step size scale αi for each row of the same order as the
average gradient of the loss functions for that row. In the numerical experiments reported
here, we choose αi = 1 for i = 1, . . . , m. Since gi grows with the number of observations
ni = |{j : (i, j) ∈ Ω}| in row i, we achieve the desired scaling by setting α0
i = αi/ni. We
take a gradient step on each row xi using the step size αi. Our procedure for choosing α0
j is
the same.

We then check whether the objective value for the row,

(cid:88)

j:(i,j)∈Ω

Lj(xiyj, Aij) + γ(cid:107)xi(cid:107)2
2,

has increased or decreased. If it has increased, then we trust our ﬁrst order approximation
to the objective function less far, and reduce the step size; if it has decreased, we gain
conﬁdence, and increase the step size.
In the numerical experiments reported below, we
decrease the step size by 30% when the objective increases, and increase the step size by 5%
when the objective decreases. This check stabilizes the algorithm and prevents divergence
even when the initial scale has been chosen poorly.

We then do the same with respect to each column yj: we take a gradient step, check if

the objective value for the column has increased or decreased, and adjust the step size.

The time per iteration is thus O(k(m + n + |Ω|)): computing the gradient of the ith loss
function with respect to xi takes time O(kni); computing the proximal operator of the square
loss takes time O(k); summing these over all the rows i = 1, . . . , m gives time O(k(m + |Ω|));
and adding the same costs for the column updates gives time O(k(m + n + |Ω|)). The checks
on the objective value take time O(k) per observation (to compute the inner product xiyj
and value of the loss function for each observation) and time O(1) per row and column to
compute the value of the regularizer. Hence the total time per iteration is O(k(m + n + |Ω|)).
By partitioning the job of updating diﬀerent rows and diﬀerent columns onto diﬀerent

processors, we can achieve an iteration time of O(k(m + n + |Ω|)/p) using p processors.

Stochastic gradients.
Instead of computing the full gradient of L with respect to xi
above, we can replace the gradient g in either the gradient or proximal gradient method by
any stochastic gradient g, which is a vector that satisﬁes

E g =

(cid:88)

j:(i,j)∈Ω

∇Lij(xiyj, Aij)yj.

47

A stochastic gradient can be computed by sampling j uniformly at random from among
observed features of i, and setting g = |{j : (i, j) ∈ Ω}|∇Lij(xiyj, Aij)yj. More samples from
{j : (i, j) ∈ Ω} can be used to compute a less noisy stochastic gradient.

7.3 Quadratic objectives

Here we describe how to eﬃciently implement the prox-prox update rule for quadratic ob-
jectives and arbitrary regularizers, extending the factorization caching technique introduced
in §2.3. We assume here that the objective is given by

(cid:107)A − XY (cid:107)2

F + r(X) + ˜r(Y ).

We will concentrate here on the X update; as always, the Y update is exactly analogous.

As in the case of quadratic regularization, we ﬁrst form the Gram matrix G = Y Y T .

Then the proximal gradient update is fast to evaluate:

proxαkr(X − αk(XG − 2AY T )).

But we can take advantage of the ease of inverting the Gram matrix G to design a
faster algorithm using the prox-prox update. For quadratic objectives with Gram matrix
G = Y T Y , the prox-prox update takes the simple form

proxαkr((G +

I)−1(AY T +

X)).

1
αk

1
αk

As in §2.3, we can compute (G + 1
αk
ization of (G + 1
αk
updating Y , since most of the computational eﬀort is in forming G and AY T .
For example, in the case of nonnegative least squares, this update is just

X) in parallel by ﬁrst caching the factor-
I)−1. Hence it is advantageous to repeat this update many times before

I)−1(AY T + 1
αk

Π+((G +

I)−1(AY T +

X)),

1
αk

1
αk

where Π+ projects its argument onto the nonnegative orthant.

7.4 Convergence

Alternating minimization need not converge to the same model (or the same objective value)
when initialized at diﬀerent starting points. Through examples, we explore this idea here.
These examples are ﬁt using the serial Julia implementation (presented in §9) of the alter-
nating proximal gradient updates method.

48

Global convergence for quadratically regularized PCA. Figure 12 shows the con-
vergence of the alternating proximal gradient update method on a quadratically regularized
PCA problem with randomly generated, fully observed data A = X trueY true, where entries
of X true and Y true are drawn from a standard normal distribution. We pick ﬁve diﬀerent
random initializations of X and Y with standard normal entries to generate ﬁve diﬀerent
convergence trajectories. Quadratically regularized PCA is a simple problem with an ana-
lytical solution (see §2), and with no local minima (see Appendix A). Hence it should come
as no surprise that the trajectories all converge to the same, globally optimal value.

Local convergence for nonnegative matrix factorization. Figure 13 shows conver-
gence of the same algorithm on a nonnegative matrix factorization model, with data gener-
ated in the same way as in Figure 12. (Note that A has some negative entries, so the minimal
objective value is strictly greater than zero.) Here, we plot the convergence of the objective
value, rather than the suboptimality, since we cannot provably compute the global minimum
of the objective function. We see that the algorithm converges to a diﬀerent optimal value
(and point) depending on the initialization of X and Y . Three trajectories converge to the
same optimal value (though one does so much faster than the others), one to a value that is
somewhat better, and one to a value that is substantially worse.

7.5 Initialization

Alternating minimization need not converge to the same solution (or the same objective
value) when initialized at diﬀerent starting points. Above, we saw that alternating mini-
mization can converge to models with optimal values that diﬀer signiﬁcantly.

Here, we discuss two approaches to initialization that result in provably good solutions,
for special cases of the generalized low rank problem. We then discuss how to apply these
initialization schemes to more general models.

SVD. A literature that is by now extensive shows that the SVD provides a provably good
initialization for the quadratic matrix completion problem (10) [KMO09, KMO10, KM10,
JNS13, Har13, GAGG13]. Algorithms based on alternating minimization have been shown
to converge quickly (even geometrically [JNS13]) to a global solution satisfying a recovery
guarantee when the initial values of X and Y are chosen carefully; see §2.4 for more details.
Here, we extend the SVD initialization previously proposed for matrix completion to one
that works well for all PCA-like problems: problems with convex loss functions that have
been scaled as in §4.3; with data A that consists of real values, Booleans, categoricals, and
ordinals; and with quadratic (or no) regularization.

But we will need a matrix on which to perform the SVD. What matrix corresponds to our
data table? Here, we give a simple proposal for how to construct such a matrix, motivated
by [KMO10, JNS13, Cha14]. Our key insight is that the SVD is the solution to our problem
when the entries in the table have mean zero and variance one (and all the loss functions are

49

y
t
i
l
a
m

i
t
p
o
b
u
s

e
v
i
t
c
e
j
b
o

105

104

103

102

101

100

0

1

2

3

time (s)

Figure 12: Convergence of alternating proximal gradient updates on quadratically
regularized PCA for n = m = 200, k = 2.

quadratic). Our initialization will construct a matrix with mean zero and variance one from
the data table, take its SVD, and invert the construction to produce the correct initialization.
Our ﬁrst step is to expand the categorical columns taking on d values into d Boolean
columns, and to re-interpret ordinal and Boolean columns as numbers. The scaling we
propose below is insensitive to the values of the numbers in the expansion of the Booleans: for
example, using (false, true)= (0, 1) or (false, true)= (−1, 1) produces the same initialization.
The scaling is sensitive to the diﬀerences between ordinal values: while encoding (never,
sometimes, always) as (1, 2, 3) or as (−5, 0, 5) will make no diﬀerence, encoding these ordinals
as (0, 1, 10) will result in a diﬀerent initialization.

Now we assume that the rows of the data table are independent and identically dis-
tributed, so they each have equal means and variances. Our mission is to standardize the

50

·104

e
u
l
a
v

e
v
i
t
c
e
j
b
o

1.8

1.7

1.6

1.5

1.4

1.3

0

1

2

3

time (s)

Figure 13: Convergence of alternating proximal gradient updates on NNMF for
n = m = 200, k = 2.

columns. The observed entries in column j have mean µj and variance σ2
j ,

µj = argmin

µ

σ2
j =

1
nj − 1

(cid:88)

i:(i,j)∈Ω
(cid:88)

i:(i,j)∈Ω

Lj(µ, Aij)

Lj(µj, Aij),

so the matrix whose (i, j)th entry is (Aij − µj)/σj for (i, j) ∈ Ω has columns whose observed
entries have mean 0 and variance 1.

Each missing entry can be safely replaced with 0 in the scaled version of the data without
changing the column mean. But the column variance will decrease to mj/m. If instead we
deﬁne

˜Aij =

(cid:26) m
σj mj
0

(Aij − µj)

(i, j) ∈ Ω
otherwise,

then the column will have mean 0 and variance 1.

51

Take the SVD U ΣV T of ˜A, and let ˜U ∈ Rm×k, ˜Σ ∈ Rk×k, and ˜V ∈ Rn×k denote these
matrices truncated to the top k singular vectors and values. We initialize X = ˜U ˜Σ1/2, and
Y = ˜Σ1/2 ˜V T diag(σ). The oﬀset row in the model is initialized with the means, i.e., the kth
column of X is ﬁlled with 1’s, and the kth row of Y is ﬁlled with the means, so Ykj = µj.

Finally, note that we need not compute the full SVD of ˜A, but instead can simply compute
the top k singular triples. For example, the randomized top k SVD algorithm proposed in
[HMT11] computes the top k singular triples of ˜A in time linear in |Ω|, m, and n (and
quadratic in k).

Figure 14 compares the convergence of this SVD-based initialization with random ini-
tialization on a low rank model for census data described in detail in §6.3. We initialize the
algorithm at six diﬀerent points: from ﬁve diﬀerent random normal initializations (entries
of X 0 and Y 0 drawn iid from N (0, 1)), and from the SVD of ˜A. The SVD initialization
produces a better initial value for the objective function, and also allows the algorithm to
converge to a substantially lower ﬁnal objective value than can be found from any of the ﬁve
random starting points. This behaviour indicates that the “good” local minimum discovered
by the SVD initialization is located in a basin of attraction that has low probability with
respect to the measure induced by random normal initialization.

k-means++. The k-means++ algorithm is an initialization scheme designed for quadratic
clustering problems [AV07]. It consists of choosing an initial cluster centroid at random from
the points, and then choosing the remaining k − 1 centroids from the points x that have
not yet been chosen with probability proportional to D(x)2, where D(x) is the minimum
distance of x to any previously chosen centroid.

Quadratic clustering is known to be NP-hard, even with only two clusters (k = 2)
[DFK+04]. However, k-means++ followed by alternating minimization gives a solution with
expected approximation ratio within O(log k) of the optimal value [AV07]. (Here, the expec-
tation is over the randomization in the initialization algorithm.) In contrast, an arbitrary
initialization of the cluster centers for k-means can result in a solution whose value is arbi-
trarily worse than the true optimum.

A similar idea can be used for other low rank models. If the model rewards a solution
that is spread out, as is the case in quadratic clustering or subspace clustering, it may be
better to initialize the algorithm by choosing elements with probability proportional to a
distance measure, as in k-means++.
In the k-means++ procedure, one can use the loss
function L(u) as the distance metric D.

7.6 Global optimality

All generalized low rank models are non-convex, but some are more non-convex than others.
In particular, for some problems, the only important source of non-convexity is the low rank
constraint. For these problems, it is sometimes possible to certify global optimality of a
model by considering an equivalent rank-constrained convex problem.

The arguments in this section are similar to ones found in [RFP10], in which Recht et al.
propose using a factored (nonconvex) formulation of the (convex) nuclear norm regularized

52

random
random
random
random
random
SVD

·105

e
u
l
a
v

e
v
i
t
c
e
j
b
o

9

8

7

6

5

4

3

2

0

5

10

15

30

35

40

45

50

20

25
iteration

Figure 14: Convergence from ﬁve diﬀerent random initializations, and from the
SVD initialization.

53

estimator in order to eﬃciently solve the large-scale SDP arising in a matrix completion
problem. However, the algorithm in [RFP10] relies on a subroutine for ﬁnding a local
minimum of an augmented Lagrangian which has the same biconvex form as problem (10).
Finding a local minimum of this problem (rather than a saddle point) may be hard. In this
section, we avoid the issue of ﬁnding a local minimum of the nonconvex problem; we consider
instead whether it is possible to verify global optimality when presented with some putative
solution.

The factored problem is equivalent to the rank constrained problem. Consider
the factored problem

minimize L(XY ) + γ

2 (cid:107)X(cid:107)2

F + γ

2 (cid:107)Y (cid:107)2
F ,

with variables X ∈ Rm×k, Y ∈ Rk×n, where L : Rm×n → R is any convex loss function.
Compare this to the rank-constrained problem

minimize L(Z) + γ(cid:107)Z(cid:107)∗
subject to Rank(Z) ≤ k.

with variable Z ∈ Rm×n. Here, we use (cid:107) · (cid:107)∗ to denote the nuclear norm, the sum of the
singular values of a matrix.

Theorem 1. (X (cid:63), Y (cid:63)) is a solution to the factored problem 22 if and only if Z (cid:63) = X (cid:63)Y (cid:63) is
a solution to the rank-constrained problem 23, and (cid:107)X (cid:63)(cid:107)2

F = (cid:107)Y (cid:63)(cid:107)2

F = 1

2(cid:107)Z (cid:63)(cid:107)∗.

We will need the following lemmas to understand the relation between the rank-constrained

problem and the factored problem.

Lemma 1. Let XY = U ΣV T be the SVD of XY , where Σ = diag(σ). Then

(22)

(23)

(24)

Proof. We may derive this fact as follows:

(cid:107)σ(cid:107)1 ≤

(||X||2

F + ||Y ||2

F ).

1
2

(cid:107)σ(cid:107)1 = tr(U T XY V )

≤ (cid:107)U T X(cid:107)F (cid:107)Y V (cid:107)F
≤ (cid:107)X(cid:107)F (cid:107)Y (cid:107)F

≤

(||X||2

F + ||Y ||2

F ),

1
2

where the ﬁrst inequality above uses the Cauchy-Schwartz inequality, the second relies on the
orthogonal invariance of the Frobenius norm, and the third follows from the basic inequality
ab ≤ 1

2(a2 + b2) for any real numbers a and b.

54

Lemma 2. For any matrix Z, (cid:107)Z(cid:107)∗ = inf XY =Z

1

2(||X||2

F + ||Y ||2

F ).

Proof. Writing Z = U DV T and recalling the deﬁnition of the nuclear norm (cid:107)Z(cid:107)∗ = (cid:107)σ(cid:107)1,
we see that Lemma 1 implies

(cid:107)Z(cid:107)∗ ≤ inf

XY =Z

(||X||2

F + ||Y ||2

F ).

1
2

But taking X = U Σ1/2 and Y = Σ1/2V T , we have

1
2

1
2

(||X||2

F + ||Y ||2

F ) =

((cid:107)Σ1/2(cid:107)2

F + (cid:107)Σ1/2(cid:107)2

F ) = (cid:107)σ(cid:107)1,

(using once again the orthogonal invariance of the Frobenius norm), so the bound is satisﬁed
with equality.

Note that the inﬁmum is achieved by X = U Σ1/2T and Y = T T Σ1/2V T for any orthonor-

mal matrix T .

Theorem 1 follows as a corollary, since L(Z) = L(XY ) so long as Z = XY .

The rank constrained problem is sometimes equivalent to an unconstrained prob-
lem. Note that problem (23) is still a hard problem to solve:
it is a rank-constrained
semideﬁnite program. On the other hand, the same problem without the rank constraint is
convex and tractable (though not easy to solve at scale). In particular, it is possible to write
down an optimality condition for the problem

minimize L(Z) + γ(cid:107)Z(cid:107)∗

(25)

that certiﬁes that a matrix Z is globally optimal. This problem is a relaxation of prob-
lem (23), and so has an optimal value that is at least as small. Furthermore, if any solution
to problem (25) has rank no more than k, then it is feasible for problem (23), so the op-
timal values of problem (25) and problem (23) must be the same. Hence any solution of
problem (25) with rank no more than k also solves problem 23.

Recall that the matrix Z is a solution the problem U if and only if

0 ∈ ∂(L(Z) + γ(cid:107)Z(cid:107)∗),

where ∂f (Z) is the subgradient of the function f at Z. The subgradient is a set-valued
function.

The subgradient of the nuclear norm at a matrix Z = U ΣV T is any matrix of the form
U V T + W where U T W = 0, W V = 0, and (cid:107)W (cid:107)2 ≤ 1. Equivalently, deﬁne the set-valued
function sign on scalar arguments x as

sign(x) =






{1}
x > 0
[ − 1, 1] x = 0
x < 0,
{−1}

,

55

and deﬁne (sign(x))i = sign(xi) for vectors x ∈ Rn. Then we can write the subgradient of
the nuclear norm at Z as

∂(cid:107)Z(cid:107)∗ = U diag(sign(σ))V T ,
where now we use the full SVD of Z with U ∈ Rm×min(m,n), V ∈ Rn×min(m,n), and σ ∈
Rmin(m,n).

Hence Z = U ΣV T is a solution to problem (25) if and only if

0 ∈ ∂L(Z) + γ(U V T + W ),

or more simply, if

(cid:107)(1/γ)G + U V T (cid:107)2 ≤ 1

(26)

for some G ∈ ∂L(Z). In particular, if a matrix Z with rank no more than k satisﬁes (26),
then Z also solves the rank-constrained problem (23).

This result allows us to (sometimes) certify global optimality of a particular model.
Given a model (X, Y ), we compute the SVD of the product XY = U ΣV T and an element
G ∈ ∂L(Z). If (cid:107)(1/γ)G + U V T (cid:107)2 ≤ 1, then (X, Y ) is globally optimal. (If the objective is
diﬀerentiable then we simply pick G = ∇L(Z); otherwise some choices of G ∈ ∂L(Z) may
produce invalid certiﬁcates even if (X, Y ) is globally optimal.)

8 Choosing low rank models

8.1 Regularization paths

Suppose that we wish to understand the entire regularization path for a GLRM; that is, we
would like to know the solution (X(γ), Y (γ)) to the problem

minimize (cid:80)

(i,j)∈Ω Lij(xiyj, Aij) + γ (cid:80)m

i=1 ri(xi) + γ (cid:80)n

j=1 ˜rj(yj)

as a function of γ. Frequently, the regularization path may be computed almost as quickly
as the solution for a single value of γ. We can achieve this by initially ﬁtting the model
with a very high value for γ, which is often a very easy problem. (For example, when r
and ˜r are norms, the solution is (X, Y ) = (0, 0) for suﬃciently large γ.) Then we may
ﬁt models corresponding to smaller and smaller values of γ by initializing the alternating
minimization algorithm from our previous solution. This procedure is sometimes called a
homotopy method.

For example, Figure 15 shows the regularization path for quadratically regularized Huber
PCA on a synthetic data set. We generate a dataset A = XY +S with X ∈ Rm×k, Y ∈ Rk×n,
and S ∈ Rm×n, with m = n = 300 and k = 3. The entries of X and Y are drawn from a
standard normal distribution, while the entries of the sparse noise matrix S are drawn from
a uniform distribution on [0, 1] with probability 0.05, and are 0 otherwise. We ﬁt a rank 5
GLRM to an observation set Ω consisting of 10% of the entries in the matrix, drawn uniformly

56

at random from {1, . . . , i} × {1, . . . , j}, using Huber loss and quadratic regularization, and
vary the regularization parameter. That is, we ﬁt the model

minimize (cid:80)

(i,j)∈Ω huber(xiyj, Aij) + γ (cid:80)m

i=1 (cid:107)xi(cid:107)2

2 + γ (cid:80)n

j=1 (cid:107)yj(cid:107)2
2

and vary the regularization parameter γ. The ﬁgure plots both the normalized training error,

and the normalized test error,

1
|Ω|

(cid:88)

(i,j)∈Ω

huber(xiyj, Aij),

1
nm − |Ω|

(cid:88)

(i,j)(cid:54)∈Ω

huber(xiyj, Aij),

of the ﬁtted model (X, Y ), for γ ranging from 0 to 3. Here, we see that while the training error
decreases and γ decreases, the test error reaches a minimum around γ = .5. Interestingly,
it takes only three times longer (about 3 seconds) to generate the entire regularization path
than it does to ﬁt the model for a single value of the regularization parameter (about 1
second).

test error
train error

0.35

0.3

0.25

0.2

0.15

0.1

r
o
r
r
e

d
e
z
i
l
a
m
r
o
n

0

0.5

1

2

2.5

3

Figure 15: Regularization path.

1.5
γ

57

8.2 Choosing model parameters

To form a generalized low rank model, one needs to specify the loss functions Lj, regularizers
r and ˜r, and a rank k. The loss function should usually be chosen by a domain expert to
reﬂect the intuitive notion of what it means to “ﬁt the data well”. On the other hand, the
regularizers and rank are often chosen based on statistical considerations, so that the model
generalizes well to unseen (missing) data.

There are three major considerations to balance in choosing the regularization and rank
In the following discussion, we suppose that the regularizers r = γr0 and

of the model.
˜r = γ˜r0 have been chosen up to a scaling γ.

Compression. A low rank model (X, Y ) with rank k and no sparsity represents the data
table A with only (m + n)k numbers, achieving a compression ratio of (m + n)k/(mn). If the
factors X or Y are sparse, then we have used fewer than (m + n)k numbers to represent the
data A, achieving a higher compression ratio. We may want to pick parameters of the model
(k and γ) in order to achieve a good error (cid:80)
(i,j)∈Ω Lj(Aij − xiyj) for a given compression
ratio. For each possible combination of model parameters, we can ﬁt a low rank model with
those parameters, observing both the error and the compression ratio. We can then choose
the best model parameters (highest compression rate) achieving the error we require, or the
best model parameters (lowest error rate) achieving the compression we require.

More formally, one can construct an information criterion for low rank models by analogy
with the Aikake Information Criterion (AIC) or the Bayesian Information Criterion (BIC).
For use in the AIC, the number of degrees of freedom in a low rank model can be computed
as the diﬀerence between the number of nonzeros in the model and the dimensionality of the
symmetry group of the problem. For example, if the model (X, Y ) is dense, and the regu-
larizer is invariant under orthogonal transformations (e.g., r(x) = (cid:107)x(cid:107)2
2), then the number
of degrees of freedom is (m + n)k − k2 [TB99]. Minka [Min01] proposes a method based on
the BIC to automatically choose the dimensionality in PCA, and observes that it performs
better than cross validation in identifying the true rank of the model when the number of
observations is small (m, n (cid:46) 100).

Denoising. Suppose we observe every entry in a true data matrix contaminated by noise,
e.g., Aij = Atrue
ij + (cid:15)ij, with (cid:15)ij some random variable. We may wish to choose model
parameters to identify the truth and remove the noise: we would like to ﬁnd k and γ to
minimize (cid:80)

(i,j)∈Ω Lj(Atrue

ij − xiyj).

A number of commonly used rules-of-thumb have been proposed in the case of PCA to
distinguish the signal (the true rank k of the data) from the noise, some of which can be
generalized to other low rank models. These include using scree plots, often known as the
“elbow method” [Cat66]; the eigenvalue method; Horn’s parallel analysis [Hor65, Din09];
and other related methods [ZV86, PM03]. A recent, more sophisticated method adapts the
idea of dropout training [SHK+14] to regularize low-rank matrix estimation [JW14].

Some of these methods can easily be adapted to the GLRM context. The “elbow method”
increases k until the objective value decreases less than linearly; the eigenvalue method

58

increases k until the objective value decreases by less than some threshold; Horn’s parallel
analysis increases k until the objective value compares unfavorably to one generated by
ﬁtting a model to data drawn from a synthetic noise distribution.

Cross validation is also simple to apply, and is discussed further below as a means of
predicting missing entries. However, applying cross validation to the denoising problem is
somewhat tricky, since leaving out too few entries results in overﬁtting to the noise, while
leaving out too many results in underﬁtting to the signal. The optimal number of entries to
leave out may depend on the aspect ratio of the data, as well as on the type of noise present
in the data [Per09], and is not well understood except in the case of Gaussian noise [OP09].
We explore the problem of choosing a holdout size numerically below.

Predicting missing entries. Suppose we observe some entries in the matrix and wish
to predict the others. A GLRM with a higher rank will always be able to ﬁt the (noisy)
data better than one of lower rank. However, a model with many parameters may also
overﬁt to the noise. Similarly, a GLRM with no regularization (γ = 0) will always produce
a model with a lower empirical loss (cid:80)
(i,j)∈Ω Lj(xiyj, Aij). Hence, we cannot pick a rank k or
regularization γ simply by considering the objective value obtained by ﬁtting the low rank
model.

But by resampling from the data, we can simulate the performance of the model on out
of sample (missing) data to identify GLRMs that neither over nor underﬁt. Here, we discuss
a few methods for choosing model parameters by cross-validation; that is, by resampling
from the data to evaluate the model’s performance. Cross validation is commonly used in
regression models to choose parameters such as the regularization parameter γ, as in Figure
15. In GLRMs, cross validation can also be used to choose the rank k. Indeed, using a lower
rank k can be considered another form of model regularization.

We can distinguish between three sources of noise or variability in the data, which give

rise to three diﬀerent resampling procedures.

• The rows or columns of the data are chosen at random, i.e., drawn iid from some

population. In this case it makes sense to resample the rows or columns.

• The rows or columns may be ﬁxed, but the indices of the observed entries in the matrix
are chosen at random. In this case, it makes sense to resample from the observed entries
in the matrix.

• The indices of the observed entries are ﬁxed, but the values are observed with some
measurement error. In this case, it makes sense to resample the errors in the model.

Each of these leads to a diﬀerent reasonable kind of resampling scheme. The ﬁrst two
give rise to resampling schemes based on cross validation (i.e., resampling the rows, columns,
or individual entries of the matrix) which we discuss further below. The third gives rise to
resampling schemes based on the bootstrap or jackknife procedures, which resample from
the errors or residuals after ﬁtting the model. A number of methods using the third kind

59

of resampling have been proposed in order to perform inference (i.e., generate conﬁdence
intervals) for PCA; see Josse et al. [JWH14] and references therein.

As an example, let’s explore the eﬀect of varying |Ω|/mn, γ, and k. We generate random
, Y ∈ Rktrue×n, and S ∈ Rm×n, with m = n = 300 and
data as follows. Let X ∈ Rm×ktrue
ktrue = 3,. Draw the entries of X and Y from a standard normal distribution, and draw the
entries of the sparse outlier matrix S are drawn from a uniform distribution on [0, 3] with
probability 0.05, and are 0 otherwise. Form A = XY + S. Select an observation set Ω by
picking entries in the matrix uniformly at random from {1, . . . , n} × {1, . . . , m}. We ﬁt a
rank k GLRM with Huber loss and quadratic regularization γ(cid:107) · (cid:107)2
2, varying |Ω|/mn, γ, and
k, and compute the test error. We average our results over 5 draws from the distribution
generating the data.

In Figure 16, we see that the true rank k = 3 performs best on cross-validated error for
any number of observations |Ω|. (Here, we show performance for γ = 0. The plot for other
values of the regularization parameter is qualitatively the same.) Interestingly, it is easiest to
identify the true rank with a small number of observations: higher numbers of observations
make it more diﬃcult to overﬁt to the data even when allowing higher ranks.

|Ω|/mn=0.1
|Ω|/mn=0.3
|Ω|/mn=0.5
|Ω|/mn=0.7
|Ω|/mn=0.9

r
o
r
r
e

t
s
e
t

d
e
z
i
l
a
m
r
o
n

1

0.8

0.6

0.4

0.2

0

1

2

4

5

Figure 16: Test error as a function of k, for γ = 0.

In Figure 17, we consider the interdependence of our choice of γ and k. Regularization is
most important when few matrix elements have been observed: the curve for each k is nearly
ﬂat when more than about 10% of the entries have been observed, so we show here a plot

3
k

60

for |Ω| = .1mn. Here, we see that the true rank k = 3 performs best on cross-validated error
for any value of the regularization parameter. Ranks that are too high (k > 3) beneﬁt from
increased regularization γ, whereas higher regularization hurts the performance of models
with k lower than the true rank. That is, regularizing the rank (small k) can substitute for
explicit regularization of the factors (large γ).

k=1
k=2
k=3
k=4
k=5

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

r
o
r
r
e

t
s
e
t

d
e
z
i
l
a
m
r
o
n

0

1

2

3

4

5

γ

Figure 17: Test error as a function of γ when 10% of entries are observed.

Finally, in Figure 18 we consider how the ﬁt of the model depends on the number of
observations. If we correctly guess the rank k = 3, we ﬁnd that the ﬁt is insensitive to the
number of observations. If our rank is either too high or too low, the ﬁt improves with more
observations.

8.3 On-line optimization

Suppose that new examples or features are being added to our data set continuously, and we
wish to perform on-line optimization, which means that we should have a good estimate at
any time for the representations of those examples xi or features yj which we have seen. This
model is equivalent to adding new rows or columns to the data table A as the algorithm
continues.
In this setting, alternating minimization performs quite well, and has a very
natural interpretation. Given an estimate for Y , when a new example is observed in row i,

61

k=1
k=2
k=3
k=4
k=5

r
o
r
r
e

t
s
e
t

d
e
z
i
l
a
m
r
o
n

1

0.8

0.6

0.4

0.2

0

0.1

0.3

0.7

0.9

0.5
|Ω|/mn

Figure 18: Test error as a function of observations |Ω|/mn, for γ = 0.

we may solve

minimize (cid:80)

j:(i,j)∈Ω Lij(Aij, xyj) + r(x)

with variable x to compute a representation for row i. This computation is exactly the same
as one step of alternating minimization. Here, we are ﬁnding the best feature representation
for the new example in terms of the (already well understood) archetypes Y . If the number
of other examples previously seen is large, the addition of a single new example should not
change the optimal Y by very much; hence if (X, Y ) was previously the global minimum of
(15), this estimate of the feature representation for the new example will be very close to its
optimal representation (i.e., the one that minimizes problem (15)). A similar interpretation
holds when new columns are added to A.

9

Implementations

The authors have developed and released three open source codes for modelling and ﬁtting
generalized low rank models: a basic serial implementation written in Python, a serial and
shared-memory parallel implementation written in Julia, and a distributed implementation
written in Scala using the Spark framework. The Julia and Spark implementations use the

62

alternating proximal gradient method described in §7 to ﬁt GLRMs, while the Python imple-
mentation uses alternating minimization and a cvxpy [DCB14] backend for each subproblem.
In this section we brieﬂy discuss these implementations, and report some timing results. For
a full description and up-to-date information about available functionality, we encourage the
reader to consult the on-line documentation for each of these packages.

There are also many implementations available for ﬁtting special cases of GLRMs. For
example, an implementation capable of ﬁtting any GLRM for which the subproblems in an
alternating minimization method are quadratic programs was recently developed in Spark
by Debasish Das [DD14].

9.1 Python implementation

GLRM.py is a Python implementation for ﬁtting GLRMs that can be found, together with
documentation, at

https://github.com/cehorn/glrm.

We encourage the interested reader to consult the on-line documentation for the most up-
to-date functionality and a collection of examples.

Usage. The user initializes a GLRM by specifying

• the data table A (A), stored as a Python list of 2-D arrays, where each 2-D array in A

contains all data associated with a particular loss function,

• the list of loss functions L (Lj, j = 1, . . . , n), that correspond to the data as speciﬁed

by A,

• regularizers regX (r) and regY (˜r),

• the rank k (k),

• an optional list missing_list with the same length as A so that each entry of missing_list

is a list of missing entries corresponding to the data from A, and

• an optional convergence object converge that characterizes the stopping criterion for

the alternating minimization procedure.

The following example illustrates how to use GLRM.py to ﬁt a GLRM with Boolean
(A_bool) and numerical (A_real) data, with quadratic regularization and a few missing
entries.

from glrm import GLRM
from glrm.loss import QuadraticLoss, HingeLoss
from glrm.reg import QuadraticReg

# import the model
# import losses
# import regularizer

63

A = [A_bool, A_real]
L = [Hinge_Loss, QuadraticLoss]
regX, regY = QuadraticReg(0.1), QuadraticReg(0.1)
missing_list = [[], [(0,0), (0,1)]]

# data stored as a list
# loss function as a list
# penalty weight is 0.1
# indexed by submatrix

model = GLRM(A, L, regX, regY, k, missing_list)
model.fit()

# initialize GLRM
# fit GLRM

The fit() method automatically adds an oﬀset to the GLRM and scales the loss functions
as described in §4.3.

GLRM.py ﬁts GLRMS by alternating minimization. The code instantiates cvxpy problems
[DCB14] corresponding to the X- and Y -update steps, then iterates by alternately solving
each problem until convergence criteria are met.

The following loss functions and regularizers are supported by GLRM.py:

• quadratic loss QuadraticLoss,

• Huber loss HuberLoss,

• hinge loss HingeLoss,

• ordinal loss OrdinalLoss,

• no regularization ZeroReg,

• (cid:96)1 regularization LinearReg,

• quadratic regularization QuadraticReg, and

• nonnegative constraint NonnegativeReg.

Users may implement their own loss functions (regularizers) using the abstract class Loss
(Reg).

9.2 Julia implementation

LowRankModels is a code written in Julia [BKSE12] for modelling and ﬁtting GLRMs. The
implementation is available on-line at

https://github.com/madeleineudell/LowRankModels.jl.

We discuss some aspects of the usage and features of the code here. For a full description
and up-to-date information about available functionality, we encourage the reader to consult
the on-line documentation.

64

Usage. To form a GLRM using LowRankModels, the user speciﬁes

• the data A (A), which can be any array or array-like data structure (e.g., a Julia

DataFrame);

• the observed entries obs (Ω), a list of tuples of the indices of the observed entries in
the matrix, which may be omitted if all the entries in the matrix have been observed;

• the list of loss functions losses (Lj, j = 1, . . . , n), one for each column of A;

• the regularizers rx (r) and ry (˜r); and

• the rank k (k).

For example, the following code forms and ﬁts a k-means model with k = 5 on the matrix
A ∈ Rm×n.

losses = fill(quadratic(),n)
rx = unitonesparse()
ry = zeroreg()
glrm = GLRM(A,losses,rx,ry,k) # form GLRM
X,Y,ch = fit!(glrm)

# fit GLRM

# quadratic loss
# x is 1-sparse unit vector
# y is not regularized

LowRankModels uses the proximal gradient method described in §7.2 to ﬁt GLRMs. The
optimal model is returned in the factors X and Y, while ch gives the convergence history. The
exclamation mark suﬃx follows the convention in Julia denoting that the function mutates
at least one of its arguments. In this case, it caches the best ﬁt X and Y as glrm.X and
glrm.Y [CE14].

Losses and regularizers must be of type Loss and Regularizer, respectively, and may

be chosen from a list of supported losses and regularizers, which include

• quadratic loss quadratic,

• hinge loss hinge,

• (cid:96)1 loss l1,

• Huber loss huber,

• ordinal hinge loss ordinal_hinge,

• quadratic regularization quadreg,

• no regularization zeroreg,

• nonnegative constraint nonnegative, and

• 1-sparse constraint onesparse.

• unit 1-sparse constraint unitonesparse.

Users may also implement their own losses and regularizers.

65

Shared memory parallelism. LowRankModels takes advantage of Julia’s SharedArray
data structure to implement a ﬁtting procedure that takes advantage of shared memory par-
allelism. While Julia does not yet support threading, SharedArrays in Julia allow separate
processes on the same computer to access the same block of memory. To ﬁt a model using
multiple processes, LowRankModels loads the data A and the initial model X and Y into
shared memory, broadcasts other problem data (e.g., the losses and regularizers) to each
process, and assigns to each process a partition of the rows of X and columns of Y . At every
iteration, each process updates its rows of X, its columns of Y , and computes its portion of
the objective function, synchronizing after each of these steps to ensure that e.g.the X up-
date is completed before the Y update begins; then the master process checks a convergence
criterion and adjusts the step length.

Automatic modeling. LowRankModels is capable of adding oﬀsets to a GLRM, and of
automatically scaling the loss functions, as described in §4.3. It can also automatically detect
the types of diﬀerent columns of a data frame and select an appropriate loss. Using these
features, LowRankModels implements a method

glrm(dataframe, k)

that forms a rank k model on a data frame, automatically selecting loss functions and
regularization that suit the data well, and ignoring any missing (NA) element in the data
frame. This GLRM can then be ﬁt with the function fit!.

Example. As an example, we ﬁt a GLRM to the Motivational States Questionnaire (MSQ)
data set [RA98]. This data set measures 3896 subjects on 92 aspects of mood and personality
type, as well as recording the time of day the data were collected. The data include real-
valued, Boolean, and ordinal measurements, and approximately 6% of the measurements are
missing (NA).

The following code loads the MSQ data set and encodes it in two dimensions:

using RDatasets
using LowRankModels
# pick a data set
df = RDatasets.dataset("psych","msq")
# encode it!
X,Y,labels,ch = fit(glrm(df,2))

Figure 19 uses the rows of Y as a coordinate system to plot some of the features of the
data set. Here we see the automatic embedding separates positive from negative emotions
along the y axis. This embedding is notable for being interpretable despite having been
generated completely automatically. Of course, better embeddings may be obtained by a
more careful choice of loss functions, regularizers, scaling, and embedding dimension k.

66

Figure 19: An automatic embedding of the MSQ [RA98] data set into two dimen-
sions.

9.3 Spark implementation

SparkGLRM is a code written in Scala, built on the Spark cluster programming framework
[ZCF+10], for modelling and ﬁtting GLRMs. The implementation is available on-line at

http://git.io/glrmspark.

In SparkGLRM, the data matrix A is split entry-wise across many machines, just
Design.
as in [HMLZ14]. The model (X, Y ) is replicated and stored in memory on every machine.

67

Thus the total computation time required to ﬁt the model is proportional to the number
of nonzeros divided by the number of cores, with the restriction that the model should ﬁt
in memory. (The authors leave to future work an extension to models that do not ﬁt in
memory, e.g., by using a parameter server [SSZ14].) Where possible, hardware acceleration
(via breeze and BLAS) is used for local linear algebraic operations.

At every iteration, the current model is broadcast to all machines, so there is only one
copy of the model on each machine. This particularly important in machines with many
cores, because it avoids duplicating the model those machines. Each core on a machine will
process a partition of the input matrix, using the local copy of the model.

Usage. The user provides loss functions Lij(u, a) indexed by i = 0, . . . , m − 1 and j =
0, . . . , n − 1, so a diﬀerent loss function can be deﬁned for each column, or even for each
entry. Each loss function is deﬁned by its gradient (or a subgradient). The method signature
is

loss grad(i: Int, j: Int, u: Double, a: Double)

whose implementation can be customized by particular i and j. As an example, the following
line implements squared error loss (L(u, a) = 1/2(u − a)2) for all entries:

Similarly, the user provides functions implementing the proximal operator of the regu-
larizers r and ˜r, which take a dense vector and perform the appropriate proximal operation.

Experiments. We ran experiments on several large matrices. For size comparison, a very
popular matrix in the recommender systems community is the Netﬂix Prize Matrix, which
has 17770 rows, 480189 columns, and 100480507 nonzeros. Below we report results on several
larger matrices, up to 10 times larger. The matrices are generated by ﬁxing the dimensions
and number of nonzeros per row, then uniformly sampling the locations for the nonzeros,
and ﬁnally ﬁlling in those locations with a uniform random number in [0, 1].

We report iteration times using an Amazon EC2 cluster with 10 slaves and one master,
of instance type “c3.4xlarge”. Each machine has 16 CPU cores and 30 GB of RAM. We
ran SparkGLRM to ﬁt two GLRMs on matrices of varying sizes. Table 5 gives results for
quadratically regularized PCA (i.e., quadratic loss and quadratic regularization) with k = 5.
To illustrate the capability to write and ﬁt custom loss functions, we also ﬁt a GLRM using
a loss function that depends on the parity of i + j:

Lij(u, a) =

(cid:26) |u − a|
(u − a)2

i + j is even
i + j is odd,

with r(x) = (cid:107)x(cid:107)1 and ˜r(y) = (cid:107)y(cid:107)2
2, setting k = 10. (This loss function was chosen merely to
illustrate the generality of the implementation. Usually losses will be the same for each row
in the same column.) The results for this custom GLRM are given in Table 6.

u - a

68

Matrix size # nonzeros Time per iteration (s)

106 × 106
106 × 106
107 × 107

106 × 106
106 × 106
107 × 107

106
109
109

106
109
109

Table 5: SparkGLRM for quadratically regularized PCA, k = 5.

Matrix size # nonzeros Time per iteration (s)

7
11
227

9
13
294

Table 6: SparkGLRM for custom GLRM, k = 10.

The table gives the time per iteration. The number of iterations required for convergence
depends on the size of the ambient dimension. On the matrices with the dimensions shown in
Tables 5 and 6, convergence typically requires about 100 iterations, but we note that useful
GLRMs often emerge after only a few tens of iterations.

Acknowledgements

The authors are grateful to Chris De Sa, Yash Deshpande, Nicolas Gillis, Maya Gupta,
Trevor Hastie, Irene Kaplow, Lester Mackey, Andrea Montanari, Art Owen, Haesun Park,
David Price, Chris R´e, Ben Recht, Yoram Singer, Nati Srebro, Ashok Srivastava, Peter
Stoica, Sze-chuan Suen, Stephen Taylor, Joel Tropp, Ben Van Roy, and Stefan Wager for
a number of illuminating discussions and comments on early drafts of this paper, and to
Debasish Das and Matei Zaharia for their insights into creating a successful Spark imple-
mentation. This work was developed with support from the National Science Foundation
Graduate Research Fellowship program (under Grant No. DGE-1147470), the Gabilan Stan-
ford Graduate Fellowship, the Gerald J. Lieberman Fellowship, and the DARPA X-DATA
program.

69

A Quadratically regularized PCA

In this appendix we describe some properties of the quadratically regularized PCA prob-
lem (3),

minimize (cid:107)A − XY (cid:107)2

F + γ(cid:107)X(cid:107)2

F + γ(cid:107)Y (cid:107)2
F .

(27)

In the sequel, we let U ΣV T = A be the SVD of A and let r be the rank of A. We assume for
convenience that all the nonzero singular values σ1 > σ2 > · · · > σr > 0 of A are distinct.

A.1 Solution

Problem (3) is the only problem we will encounter that has an analytical solution. A solution
is given by

X = ˜U ˜Σ1/2,

Y = ˜Σ1/2 ˜V T ,

(28)

where ˜U and ˜V are deﬁned as in (5), and ˜Σ = diag((σ1 − γ)+, . . . , (σk − γ)+).

To prove this, let’s consider the optimality conditions of (3). The optimality conditions

are

−(A − XY )Y T + γX = 0,

−(A − XY )T X + γY T = 0.

Multiplying the ﬁrst optimality condition on the left by X T and the second on the left by
Y and rearranging, we ﬁnd

X T (A − XY )Y T = γX T X,

Y (A − XY )T X = γY Y T ,

which shows, by taking a transpose, that X T X = Y Y T at any stationary point.

We may rewrite the optimality conditions together as

(cid:20)−γI

A
AT −γI

(cid:21) (cid:20) X
Y T

(cid:21)

(cid:21)

(cid:21) (cid:20) X
Y T

=

=

=

(cid:20)

0
XY
(XY )T
0
(cid:21)
(cid:20) X(Y Y T )
Y T (X T X)
(cid:20) X
Y T

(X T X),

(cid:21)

where we have used the fact that X T X = Y Y T .

Now we see that (X, Y T ) lies in an invariant subspace of the matrix

(cid:20)−γI

A
AT −γI

(cid:21)

. Recall

that V is an invariant subspace of a matrix A if AV = V M for some matrix M . If Rank(M ) ≤
Rank(A), we know that the eigenvalues of M are eigenvalues of A, and that the corresponding
eigenvectors lie in the span of V .

Thus the eigenvalues of X T X must be eigenvalues of

, and (X, Y T ) must

span the corresponding eigenspace. More concretely, notice that

is (symmetric,

(cid:21)

(cid:20)−γI

A
AT −γI
(cid:20)−γI

(cid:21)

A
AT −γI

70

and therefore) diagonalizable, with eigenvalues −γ ± σi. The larger eigenvalues −γ + σi
correspond to the eigenvectors (ui, vi), and the smaller ones −γ − σi to (ui, −vi).

Now, X T X is positive semideﬁnite, so the eigenvalues shared by X T X and

(cid:20)−γI

A
AT −γI

(cid:21)

√

must be positive. Hence there is some set |Ω| ≤ k with σi ≥ γ for i ∈ Ω such that X has
−γ + σi for i ∈ Ω. (Recall that X T X = Y Y T , so Y has the same
have singular values
singular values as X.) Then (X, Y T ) spans the subspace generated by the vectors (ui, vi for
i ∈ Ω. We say the stationary point (X, Y ) has active subspace Ω. It is easy to verify that
XY = (cid:80)

i∈Ω ui(σi − γ)vT
i .

Each active subspace gives rise to an orbit of stationary points. If (X, Y ) is a stationary

point, then (XT, T −1Y ) is also a stationary point so long as

−(A − XY )Y T T −T + γXT = 0,

−(A − XY )T XT + γY T T −T = 0,

which is always true if T −T = T , i.e., T is orthogonal. This shows that the set of stationary
points is invariant under orthogonal transformations.

To simplify what follows, we choose a representative element for each orbit. Represent

any stationary point with active subspace Ω by

X = UΩ(ΣΩ − γI)1/2,

Y = (ΣΩ − γI)1/2V T
Ω ,

where by UΩ we denote the submatrix of U with columns indexed by Ω, and similarly for
(cid:0)k(cid:48)(γ)
(cid:1)
Σ and V . At any value of γ, let k(cid:48)(γ) = max{i : σi ≥ γ}. Then we have (cid:80)k
i
(representative) stationary points, one for each choice of Ω The number of (representative)
stationary points is decreasing in γ; when γ > σ1, the only stationary point is X = 0, Y = 0.
These stationary points can have quite diﬀerent values. If (X, Y ) has active subspace Ω,

i=0

then

||A − XY ||2

F + γ(||X||2

F + ||Y ||2

F ) =

σ2
i +

(cid:0)γ2 + 2γ|σi − γ|(cid:1) .

(cid:88)

i /∈Ω

(cid:88)

i∈Ω

From this form, it is clear that we should choose Ω to include the top singular values i =
1, . . . , k(cid:48)(γ). Choosing any other subset Ω will result in a higher (worse) objective value:
that is, the other stationary points are not global minima.

A.2 Fixed points of alternating minimization

Theorem 2. The quadratically regularized PCA problem (3) has only one local minimum,
which is the global minimum.

Our proof is similar to that of [BH89], who proved a related theorem for the case of

PCA (2).
Proof. We showed above that every stationary point of (3) has the form XY = (cid:80)
i∈Ω uidivT
i ,
with Ω ⊆ {1, . . . , k(cid:48)}, |Ω| ≤ k, and di = σi − γ. We use the representative element from each
√
divT
stationary orbit described above, so each column of X is ui
i
for some i ∈ Ω. The columns of X are orthogonal, as are the rows of Y .

di and each row of Y is

√

71

If a stationary point is not the global minimum, then σj > σi for some i ∈ Ω, j (cid:54)∈ Ω.
Below, we show we can always ﬁnd a descent direction if this condition holds, thus showing
that the only local minimum is the global minimum.

Assume we are at a stationary point with σj > σi for some i ∈ Ω, j (cid:54)∈ Ω. We will ﬁnd a
j . Form ˜X by replacing the column of
i by

descent direction by perturbing XY in direction ujvT
X containing ui
√

di, and ˜Y by replacing the row of Y containing

di by (ui + (cid:15)uj)

divT

√

√

√

di(vi + (cid:15)vj)T . Now the regularization term increases slightly:

γ((cid:107) ˜X(cid:107)2

F + (cid:107) ˜Y (cid:107)2

F ) − γ((cid:107)X(cid:107)2

F + (cid:107)Y (cid:107)2

F ) =

(2γti(cid:48)) + 2γdi(1 + (cid:15)2) −

2γti(cid:48)

(cid:88)

i(cid:48)∈Ω

(cid:88)

i(cid:48)∈Ω,i(cid:48)(cid:54)=i
= 2γdi(cid:15)2.

Meanwhile, the approximation error decreases:

(cid:107)A − ˜X ˜Y (cid:107)2

F − (cid:107)A − XY (cid:107)2

F = (cid:107)uiσivT

i + ujσjvT

j − (ui + (cid:15)uj)di(vi + (cid:15)vj)T (cid:107)2

= (cid:107)ui(σi − di)vT

i + uj(σj − (cid:15)2di)vT

j − (cid:15)uidivT

F − (σi − di)2 − σ2
j
i (cid:107)2
F

j − (cid:15)ujdivT

−(σi − di)2 − σ2
j
(cid:13)
(cid:20)σi − di
−(cid:15)di
(cid:13)
(cid:13)
σj − (cid:15)2di
−(cid:15)di
(cid:13)

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
F

=

= (σi − di)2 + (σj − (cid:15)2di)2 + 2(cid:15)2d2
i + 2(cid:15)2d2
= −2σj(cid:15)2di + (cid:15)4d2
i
= 2(cid:15)2di(di − σj) + (cid:15)4d2
i ,

− (σi − di)2 − σ2
j

i − (σi − di)2 − σ2
j

where we have used the rotational invariance of the Frobenius norm to arrive at the third
equality above. Hence the net change in the objective value in going from (X, Y ) to ( ˜X, ˜Y )
is

2γdi(cid:15)2 + 2(cid:15)2di(di − σj) + (cid:15)4d2

i = 2(cid:15)2di(γ + di − σj) + (cid:15)4d2
i
= 2(cid:15)2di(σi − σj) + (cid:15)4d2
i ,

which is negative for small (cid:15). Hence we have found a descent direction, showing that any
stationary point with σj > σi for some i ∈ Ω, j (cid:54)∈ Ω is not a local minimum.

72

References

[AAJN13] A. Agarwal, A. Anandkumar, P. Jain, and P. Netrapalli. Learning sparsely
used overcomplete dictionaries via alternating minimization. arXiv preprint
arXiv:1310.7991, 2013.

[ABEV09] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. A new approach to collab-
orative ﬁltering: Operator estimation with spectral regularization. The Journal
of Machine Learning Research, 10:803–826, 2009.

[AEB06] M. Aharon, M. Elad, and A. Bruckstein. k-SVD: An algorithm for designing
overcomplete dictionaries for sparse representation. IEEE Transactions on Signal
Processing, 54(11):4311–4322, 2006.

[AM04]

[AV07]

P. K. Agarwal and N. H. Mustafa. k-means projective clustering. In Proceed-
ings of the 23rd ACM SIGMOD-SIGACT-SIGART Symposium on Principles of
Database Systems, pages 155–165. ACM, 2004.

D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding.
In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete
Algorithms, pages 1027–1035. Society for Industrial and Applied Mathematics,
2007.

[BBL+07] M. Berry, M. Browne, A. Langville, V. Pauca, and R. Plemmons. Algorithms and
applications for approximate nonnegative matrix factorization. Computational
Statistics & Data Analysis, 52(1):155–173, 2007.

[BCMR12] S. Boyd, C. Cortes, M. Mohri, and A. Radovanovic. Accuracy at the top. In

Advances in Neural Information Processing Systems, pages 962–970, 2012.

[BDKP14] R. Boyd, B. Drake, D. Kuang, and H. Park. Smallk is a C++/Python high-
performance software library for nonnegative matrix factorization (NMF) and
hierarchical and ﬂat clustering using the NMF; current version 1.2.0. http:
//smallk.github.io/, June 2014.

[Ber11]

[BH89]

D. P. Bertsekas. Incremental gradient, subgradient, and proximal methods for
convex optimization: A survey. Optimization for Machine Learning, 2010:1–38,
2011.

P. Baldi and K. Hornik. Neural networks and principal component analysis:
Learning from examples without local minima. Neural Networks, 2(1):53–58,
1989.

[BKSE12] J. Bezanson, S. Karpinski, V. B. Shah, and A. Edelman. Julia: A fast dynamic

language for technical computing. arXiv preprint arXiv:1209.5145, 2012.

73

[BL10]

J. Borwein and A. Lewis. Convex analysis and nonlinear optimization: theory
and examples, volume 3. Springer Science & Business Media, 2010.

[BM03a]

S. Boyd and J. Mattingley. Branch and bound methods. Lecture notes for
EE364b, Stanford University, 2003.

[BM03b]

S. Burer and R. Monteiro. A nonlinear programming algorithm for solving
semideﬁnite programs via low-rank factorization. Mathematical Programming,
95(2):329–357, 2003.

[BM03c]

S. Burer and R. D. C. Monteiro. Local minima and convergence in low-rank
semideﬁnite programming. Mathematical Programming, 103:2005, 2003.

[BPC+11] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimiza-
tion and statistical learning via the alternating direction method of multipliers.
Foundations and Trends in Machine Learning, 3(1):1–122, 2011.

[BRRT12] V. Bittorf, B. Recht, C. R´e, and J. A. Tropp. Factoring nonnegative matri-
ces with linear programs. Advances in Neural Information Processing Systems,
25:1223–1231, 2012.

[BST13]

J. Bolte, S. Sabach, and M. Teboulle. Proximal alternating linearized minimiza-
tion for nonconvex and nonsmooth problems. Mathematical Programming, pages
1–36, 2013.

[BV04]

S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University
Press, 2004.

[BXM03]

S. Boyd, L. Xiao, and A. Mutapcic. Subgradient methods. Lecture notes for
EE364b, Stanford University, 2003.

[Cat66]

Raymond B Cattell. The scree test for the number of factors. Multivariate
behavioral research, 1(2):245–276, 1966.

[CDS98]

S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit.
SIAM Journal on Scientiﬁc Computing, 20(1):33–61, 1998.

[CDS01] M. Collins, S. Dasgupta, and R. Schapire. A generalization of principal com-
ponent analysis to the exponential family. In Advances in Neural Information
Processing Systems, volume 13, page 23, 2001.

[CE14]

[Cha14]

J. Chen and A. Edelman. Parallel preﬁx polymorphism permits parallelization,
presentation & proof. arXiv preprint arXiv:1410.6449, 2014.

S. Chatterjee. Matrix estimation by universal singular value thresholding. The
Annals of Statistics, 43(1):177–214, 2014.

74

[CLMW11] E. Cand`es, X. Li, Y. Ma, and J. Wright. Robust principal component analysis?

Journal of the ACM (JACM), 58(3):11, 2011.

[CP09]

[CR08]

[CS02]

[CT10]

[DB95]

E. Cand`es and Y. Plan. Matrix completion with noise. CoRR, abs/0903.3131,
2009.

E. Cand`es and B. Recht. Exact matrix completion via convex optimization.
CoRR, abs/0805.4471, 2008.

K. Crammer and Y. Singer. On the algorithmic implementation of multiclass
kernel-based vector machines. The Journal of Machine Learning Research, 2:265–
292, 2002.

E. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix
completion. IEEE Transactions on Information Theory, 56(5):2053–2080, 2010.

T. Dietterich and G. Bakiri. Solving multiclass learning problems via error-
correcting output codes. CoRR, cs.AI/9501101, 1995.

[DCB14]

S. Diamond, E. Chu, and S. Boyd. CVXPY: A Python-embedded modeling
language for convex optimization, version 0.2. http://cvxpy.org/, May 2014.

[DD14]

D. Das and S. Das. Quadratic programing solver for non-negative matrix factor-
ization with spark. In Spark Summit 2014, 2014.

[dEGJL04] A. d’Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. Lanckriet. A direct
In Advances in

formulation for sparse PCA using semideﬁnite programming.
Neural Information Processing Systems, volume 16, pages 41–48, 2004.

[DFK+04] P. Drineas, A. Frieze, R. Kannan, S. Vempala, and V. Vinay. Clustering large
graphs via the singular value decomposition. Machine Learning, 56(1-3):9–33,
2004.

[Din09]

[DL84]

A. Dinno. Implementing Horn’s parallel analysis for principal component analysis
and factor analysis. Stata Journal, 9(2):291, 2009.

J. De Leeuw. The Giﬁ system of nonlinear multivariate analysis. Data analysis
and informatics, 3:415–424, 1984.

[DLM09]

J. De Leeuw and P. Mair. Giﬁ methods for optimal scaling in R: The package
homals. Journal of Statistical Software, pages 1–30, 2009.

[DLPP06] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal nonnegative matrix t-
factorizations for clustering. In Proceedings of the 12th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data Mining, pages 126–135.
ACM, 2006.

75

[DLYT76] J. De Leeuw, F. Young, and Y. Takane. Additive structure in qualitative data:
An alternating least squares method with optimal scaling features. Psychome-
trika, 41(4):471–503, 1976.

[DPBW12] M. Davenport, Y. Plan, E. Berg, and M. Wootters. 1-bit matrix completion.

arXiv preprint arXiv:1209.3672, 2012.

[DS14]

[EV09]

[EY36]

[FBD09]

A. Damle and Y. Sun. Random projections for non-negative matrix factorization.
arXiv preprint arXiv:1405.4275, 2014.

E. Elhamifar and R. Vidal. Sparse subspace clustering. In IEEE Conference on
Computer Vision and Pattern Recognition, 2009, pages 2790–2797. IEEE, 2009.

C. Eckart and G. Young. The approximation of one matrix by another of lower
rank. Psychometrika, 1(3):211–218, 1936.

C. F´evotte, N. Bertin, and J. Durrieu. Nonnegative matrix factorization with
the Itakura-Saito divergence: With application to music analysis. Neural Com-
putation, 21(3):793–830, 2009.

[FHB04] M. Fazel, H. Hindi, and S. Boyd. Rank minimization and applications in sys-
tem theory. In Proceedings of the 2004 American Control Conference (ACC),
volume 4, pages 3273–3278. IEEE, 2004.

[FM13]

W. Fithian and R. Mazumder. Scalable convex methods for ﬂexible low-rank
matrix modeling. arXiv preprint arXiv:1308.4211, 2013.

[GAGG13] S. Gunasekar, A. Acharya, N. Gaur, and J. Ghosh. Noisy matrix completion
using alternating minimization. In Machine Learning and Knowledge Discovery
in Databases, pages 194–209. Springer, 2013.

[GBW14] M. Gupta, S. Bengio, and J. Weston. Training highly multiclass classiﬁers. The

Journal of Machine Learning Research, 15(1):1461–1492, 2014.

[GD14]

A. Gress and I. Davidson. A ﬂexible framework for projecting heterogeneous
data. In Proceedings of the 23rd ACM International Conference on Conference
on Information and Knowledge Management, CIKM ’14, pages 1169–1178, New
York, NY, USA, 2014. ACM.

[GG11]

N. Gillis and F. Glineur. Low-rank matrix approximation with weights or
missing data is NP-hard. SIAM Journal on Matrix Analysis and Applications,
32(4):1149–1165, 2011.

[Gil11]

N. Gillis. Nonnegative matrix factorization: Complexity, algorithms and appli-
cations. PhD thesis, UCL, 2011.

76

[Gor02]

G. J. Gordon. Generalized2 linear2 models. In Advances in Neural Information
Processing Systems, pages 577–584, 2002.

[GRX+10] A. Goldberg, B. Recht, J. Xu, R. Nowak, and X. Zhu. Transduction with matrix
In Advances in Neural Information

completion: Three birds with one stone.
Processing Systems, pages 757–765, 2010.

[Har13]

M. Hardt. On the provable convergence of alternating minimization for matrix
completion. arXiv preprint arXiv:1312.0925, 2013.

[HMLZ14] T. Hastie, R. Mazumder, J. Lee, and R. Zadeh. Matrix completion and low-rank

svd via fast alternating least squares. arXiv, 2014.

[HMT11] N. Halko, P.-G. Martinsson, and J. Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions.
SIAM Review, 53(2):217–288, 2011.

[HN99]

[Hor65]

[Hot33]

[Hot36]

Z. Huang and M. Ng. A fuzzy k-modes algorithm for clustering categorical data.
IEEE Transactions on Fuzzy Systems, 7(4):446–452, 1999.

J. Horn. A rationale and test for the number of factors in factor analysis. Psy-
chometrika, 30(2):179–185, 1965.

H. Hotelling. Analysis of a complex of statistical variables into principal compo-
nents. Journal of Educational Psychology, 24(6):417, 1933.

H. Hotelling. Relations between two sets of variates. Biometrika, 28(3-4):321–
377, 1936.

[Hub81]

P. Huber. Robust Statistics. Wiley, New York, 1981.

[JBAS10] M. Journ´ee, F. Bach, P. Absil, and R. Sepulchre. Low-rank optimization on
the cone of positive semideﬁnite matrices. SIAM Journal on Optimization,
20(5):2327–2351, 2010.

[JNS13]

P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank matrix completion using
alternating minimization. In Proceedings of the 45th annual ACM Symposium
on the Theory of Computing, pages 665–674. ACM, 2013.

[Jol86]

I. Jolliﬀe. Principal component analysis. Springer, 1986.

[JW14]

J. Josse and S. Wager. Stable autoencoding: A ﬂexible framework for regularized
low-rank matrix estimation. arXiv preprint arXiv:1410.8275, 2014.

[JWH14]

J. Josse, S. Wager, and F. Husson. Conﬁdence areas for ﬁxed-eﬀects pca. arXiv
preprint arXiv:1407.7614, 2014.

77

[KB78]

[Kes12]

[KHP14]

[KM10]

R. Koenker and J. G. Bassett. Regression quantiles. Econometrica: Journal of
the Econometric Society, pages 33–50, 1978.

R. Keshavan. Eﬃcient algorithms for collaborative ﬁltering. PhD thesis, Stanford
University, 2012.

J. Kim, Y. He, and H. Park. Algorithms for nonnegative matrix and tensor
factorizations: A uniﬁed view based on block coordinate descent framework.
Journal of Global Optimization, 58(2):285–319, 2014.

R. Keshavan and A. Montanari. Regularization for matrix completion.
In
2010 IEEE International Symposium on Information Theory Proceedings (ISIT),
pages 1503–1507. IEEE, 2010.

[KMO09] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries.

In Advances in Neural Information Processing Systems, pages 952–960, 2009.

[KMO10] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries.
IEEE Transactions on Information Theory, 56(6):2980–2998, 2010.

[KO09]

R. Keshavan and S. Oh. A gradient descent algorithm on the Grassman manifold
for matrix completion. arXiv preprint arXiv:0910.5260, 2009.

[Koe05]

R. Koenker. Quantile regression. Cambridge University Press, 2005.

[KP07]

[KP08a]

[KP08b]

[KP11]

H. Kim and H. Park. Sparse non-negative matrix factorizations via alternating
non-negativity-constrained least squares for microarray data analysis. Bioinfor-
matics, 23(12):1495–1502, 2007.

H. Kim and H. Park. Nonnegative matrix factorization based on alternating
nonnegativity constrained least squares and active set method. SIAM Journal
on Matrix Analysis and Applications, 30(2):713–730, 2008.

J. Kim and H. Park. Toward faster nonnegative matrix factorization: A new
algorithm and comparisons. In Eighth IEEE International Conference on Data
Mining, pages 353–362. IEEE, 2008.

J. Kim and H. Park. Fast nonnegative matrix factorization: An active-set-like
method and comparisons. SIAM Journal on Scientiﬁc Computing, 33(6):3261–
3281, 2011.

[KR09]

L. Kaufman and P. J. Rousseeuw. Finding groups in data: an introduction to
cluster analysis, volume 344. John Wiley & Sons, 2009.

[LBRN06] H. Lee, A. Battle, R. Raina, and A. Ng. Eﬃcient sparse coding algorithms. In

Advances in Neural Information Processing Systems, pages 801–808, 2006.

78

[Lik32]

[Lin07]

[Llo82]

R. Likert. A technique for the measurement of attitudes. Archives of Psychology,
1932.

C. Lin. Projected gradient methods for nonnegative matrix factorization. Neural
Computation, 19(10):2756–2779, 2007.

S. Lloyd. Least squares quantization in PCM. IEEE Transactions on Information
Theory, 28(2):129–137, 1982.

[LLW04] Y. Lee, Y. Lin, and G. Wahba. Multicategory support vector machines: Theory
and application to the classiﬁcation of microarray data and satellite radiance
data. Journal of the American Statistical Association, 99(465):67–81, 2004.

[LRS+10]

J. Lee, B. Recht, R. Salakhutdinov, N. Srebro, and J. Tropp. Practical large-scale
optimization for max-norm regularization. In Advances in Neural Information
Processing Systems, pages 1297–1305, 2010.

[LS99]

[LS01]

[LV09]

[LW66]

[Mac09]

D. Lee and H. Seung. Learning the parts of objects by non-negative matrix
factorization. Nature, 401(6755):788–791, 1999.

D. Lee and H. Seung. Algorithms for non-negative matrix factorization.
Advances in Neural Information Processing Systems, pages 556–562, 2001.

In

Z. Liu and L. Vandenberghe. Interior-point method for nuclear norm approxima-
tion with application to system identiﬁcation. SIAM Journal on Matrix Analysis
and Applications, 31(3):1235–1256, 2009.

E. Lawler and D. Wood. Branch-and-bound methods: A survey. Operations
Research, 14(4):699–719, 1966.

L. Mackey. Deﬂation methods for sparse PCA. In D. Koller, D. Schuurmans,
Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing
Systems, 2009.

[Mar12]

I. Markovsky. Low Rank Approximation: Algorithms, Implementation, Applica-
tions. Communications and Control Engineering. Springer, 2012.

[MBPS09] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary learning for sparse
coding. In Proceedings of the 26th Annual International Conference on Machine
Learning, pages 689–696. ACM, 2009.

[MCCD13] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Eﬃcient estimation of word

representations in vector space. arXiv preprint arXiv:1301.3781, 2013.

[MF10]

K. Mohan and M. Fazel. Reweighted nuclear norm minimization with application
to system identiﬁcation. In Proceedings of the 2010 American Control Conference
(ACC), pages 2953–2959. IEEE, 2010.

79

[MHT10] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms
for learning large incomplete matrices. The Journal of Machine Learning Re-
search, 11:2287–2322, 2010.

[Min01]

In T.K. Leen, T.G.
T. Minka. Automatic choice of dimensionality for pca.
Dietterich, and V. Tresp, editors, Advances in Neural Information Processing
Systems, pages 598–604. MIT Press, 2001.

[MPS+09] J. Mairal, J. Ponce, G. Sapiro, A. Zisserman, and F. Bach. Supervised dictionary
learning. In Advances in Neural Information Processing Systems, pages 1033–
1040, 2009.

[MSC+13] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed rep-
resentations of words and phrases and their compositionality. In Advances in
Neural Information Processing Systems, pages 3111–3119, 2013.

[NNS+14] P. Netrapalli, U. Niranjan, S. Sanghavi, A. Anandkumar, and P. Jain. Provable
non-convex robust PCA. In Advances in Neural Information Processing Systems,
pages 1107–1115, 2014.

[NRRW11] F. Niu, B. Recht, C. R´e, and S. Wright. Hogwild!: A lock-free approach to
In Advances in Neural Information

parallelizing stochastic gradient descent.
Processing Systems, 2011.

[OF97]

[OP09]

[Osn14]

[PB13]

B. Olshausen and D. Field. Sparse coding with an overcomplete basis set: A
strategy employed by V1? Vision Research, 37(23):3311–3325, 1997.

A. Owen and P. Perry. Bi-cross-validation of the svd and the nonnegative matrix
factorization. The Annals of Applied Statistics, pages 564–594, 2009.

S. Osnaga. Low Rank Representations of Matrices using Nuclear Norm Heuris-
tics. PhD thesis, Colorado State University, 2014.

N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in Opti-
mization, 1(3):123–231, 2013.

[PCST99] J. Platt, N. Cristianini, and J. Shawe-Taylor. Large margin DAGs for multiclass
In Advances in Neural Information Processing Systems, pages

classiﬁcation.
547–553, 1999.

[Pea01]

K. Pearson. On lines and planes of closest ﬁt to systems of points in space. The
London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science,
2(11):559–572, 1901.

[Per09]

Cross-validation for unsupervised learning.

arXiv preprint

P. Perry.
arXiv:0909.3052, 2009.

80

[PJ09]

[PM03]

[PSM14]

[RA98]

H.-S. Park and C.-H. Jun. A simple and fast algorithm for k-medoids clustering.
Expert Systems with Applications, 36(2, Part 2):3336 – 3341, 2009.

K. Preacher and R. MacCallum. Repairing Tom Swift’s electric factor analysis
machine. Understanding Statistics: Statistical Issues in Psychology, Education,
and the Social Sciences, 2(1):13–43, 2003.

J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for word repre-
sentation. Proceedings of the Empiricial Methods in Natural Language Processing
(EMNLP 2014), 12, 2014.

W. Revelle and K. Anderson. Personality, motivation and cognitive performance:
Final report to the army research institute on contract MDA 903-93-K-0008.
Technical report, 1998.

[RBL+07] R. Raina, A. Battle, H. Lee, B. Packer, and A. Ng. Self-taught learning: Transfer
learning from unlabeled data. In Proceedings of the 24th International Conference
on Machine Learning, pages 759–766. ACM, 2007.

[RFP10]

B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization. SIAM Review, 52(3):471–501,
August 2010.

R. Rifkin and A. Klautau. In defense of one-vs-all classiﬁcation. The Journal of
Machine Learning Research, 5:101–141, 2004.

B. Recht and C. R´e. Parallel stochastic gradient algorithms for large-scale matrix
completion. Mathematical Programming Computation, 5(2):201–226, 2013.

[RRWN11] B. Recht, C. R´e, S. Wright, and F. Niu. Hogwild: A lock-free approach to
In Advances in Neural Information

parallelizing stochastic gradient descent.
Processing Systems, pages 693–701, 2011.

J. Rennie and N. Srebro. Fast maximum margin matrix factorization for col-
laborative prediction. In Proceedings of the 22nd International Conference on
Machine Learning, pages 713–719. ACM, 2005.

P. Richt´arik, M. Tak´aˇc, and S. Ahipa¸sao˘glu. Alternating maximization: Unifying
framework for 8 sparse PCA formulations and eﬃcient parallel codes. arXiv
preprint arXiv:1212.4137, 2012.

[SBPP06] F. Shahnaz, M. W. Berry, V. P. Pauca, and R. J. Plemmons. Document clustering
using nonnegative matrix factorization. Information Processing & Management,
42(2):373–386, 2006.

[SC12]

M. Soltanolkotabi and E. Candes. A geometric analysis of subspace clustering
with outliers. The Annals of Statistics, 40(4):2195–2238, 2012.

[RK04]

[RR13]

[RS05]

[RTA12]

81

[SF14]

[SG08]

[SH08]

[SJ03]

[SM14]

[Smi12]

[Sre04]

[SRJ04]

[SSU03]

[SSZ14]

[SEC13] M. Soltanolkotabi, E. Elhamifar, and E. Candes. Robust subspace clustering.

arXiv preprint arXiv:1301.2603, 2013.

D. L. Sun and C. F´evotte. Alternating direction method of multipliers for non-
negative matrix factorization with the beta-divergence. In IEEE International
Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2014.

A. Singh and G. Gordon. A uniﬁed view of matrix factorization models. In Ma-
chine Learning and Knowledge Discovery in Databases, pages 358–373. Springer,
2008.

H. Shen and J. Huang. Sparse principal component analysis via regularized low
rank matrix approximation. Journal of Multivariate Analysis, 99(6):1015–1034,
2008.

[SHK+14] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov.
Dropout: A simple way to prevent neural networks from overﬁtting. The Journal
of Machine Learning Research, 15(1):1929–1958, 2014.

N. Srebro and T. Jaakkola. Weighted low-rank approximations. In ICML, vol-
ume 3, pages 720–727, 2003.

V. Srikumar and C. Manning. Learning distributed representations for structured
output prediction. In Advances in Neural Information Processing Systems, pages
3266–3274, 2014.

R. Smith. Nuclear norm minimization methods for frequency domain subspace
identiﬁcation. In Proceedings of the 2010 American Control Conference (ACC),
pages 2689–2694. IEEE, 2012.

N. Srebro. Learning with Matrix Factorizations. PhD thesis, Massachusetts
Institute of Technology, 2004.

N. Srebro, J. Rennie, and T. Jaakkola. Maximum-margin matrix factorization.
In Advances in Neural Information Processing Systems, volume 17, pages 1329–
1336, 2004.

[SSGS11]

S. Shalev-Shwartz, A. Gonen, and O. Shamir. Large-scale convex minimization
with a low-rank constraint. arXiv preprint arXiv:1106.1622, 2011.

A. Schein, L. Saul, and L. Ungar. A generalized linear model for principal
component analysis of binary data. In Proceedings of the Ninth International
Workshop on Artiﬁcial Intelligence and Statistics, volume 38, page 46, 2003.

S. Schelter, V. Satuluri, and R. Zadeh. Factorbird — a parameter server ap-
proach to distributed matrix factorization. NIPS 2014 Workshop on Distributed
Machine Learning and Matrix Computations, 2014.

82

[Ste07]

[TB99]

[TG07]

[Tro04]

[Tse00]

H. Steck. Hinge rank loss and the area under the ROC curve. In J. N. Kok,
J. Koronacki, R. L. Mantaras, S. Matwin, D. Mladeniˇc, and A. Skowron, editors,
Machine Learning: ECML 2007, volume 4701 of Lecture Notes in Computer
Science, pages 347–358. Springer Berlin Heidelberg, 2007.

M. Tipping and C. Bishop. Probabilistic principal component analysis. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–
622, 1999.

J. Tropp and A. Gilbert. Signal recovery from random measurements via orthog-
onal matching pursuit. IEEE Transactions on Information Theory, 53(12):4655–
4666, 2007.

[TPB00]

N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.

J. Tropp. Topics in Sparse Approximation. PhD thesis, The University of Texas
at Austin, 2004.

P. Tseng. Nearest q-ﬂat to m points. Journal of Optimization Theory and
Applications, 105(1):249–252, 2000.

[Twe84] M. Tweedie. An index which distinguishes between some important exponen-
tial families. In Statistics: Applications and New Directions. Proceedings of the
Indian Statistical Institute Golden Jubilee International Conference, pages 579–
604, 1984.

[TYDL77] Y. Takane, F. Young, and J. De Leeuw. Nonmetric individual diﬀerences mul-
tidimensional scaling: an alternating least squares method with optimal scaling
features. Psychometrika, 42(1):7–67, 1977.

[UBG09] N. Usunier, D. Buﬀoni, and P. Gallinari. Ranking with ordered weighted pairwise
In Proceedings of the 26th annual International Conference on

classiﬁcation.
Machine Learning, pages 1057–1064. ACM, 2009.

[Vav09]

S. Vavasis. On the complexity of nonnegative matrix factorization. SIAM Journal
on Optimization, 20(3):1364–1377, 2009.

[VCLR13] V. Vu, J. Cho, J. Lei, and K. Rohe. Fantope projection and selection: A near-
optimal convex relaxation of sparse PCA. In C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information
Processing Systems 26, pages 2670–2678. Curran Associates, Inc., 2013.

[Vid10]

R. Vidal. A tutorial on subspace clustering. IEEE Signal Processing Magazine,
28(2):52–68, 2010.

83

[Vir07]

T. Virtanen. Monaural sound source separation by nonnegative matrix factor-
ization with temporal continuity and sparseness criteria. IEEE Transactions on
Audio, Speech, and Language Processing, 15(3):1066–1074, 2007.

[WBU10]

J. Weston, S. Bengio, and N. Usunier. Large scale image annotation: Learning to
rank with joint word-image embeddings. Machine Learning, 81(1):21–35, 2010.

[WGR+09] J. Wright, A. Ganesh, S. Rao, Y. Peng, and Y. Ma. Robust principal component
analysis: Exact recovery of corrupted low-rank matrices by convex optimization.
In Advances in Neural Information Processing Systems, volume 3, 2009.

[WTH09] D. Witten, R. Tibshirani, and T. Hastie. A penalized matrix decomposition, with
applications to sparse principal components and canonical correlation analysis.
Biostatistics, page kxp008, 2009.

[WYW13] J. Weston, H. Yee, and R. J. Weiss. Learning to rank recommendations with
the k-order statistic loss. In Proceedings of the 7th ACM Conference on Recom-
mender Systems, RecSys ’13, pages 245–248, New York, NY, USA, 2013. ACM.

[XCS12]

H. Xu, C. Caramanis, and S. Sanghavi. Robust PCA via outlier pursuit. IEEE
Transactions on Information Theory, 58(5):3047–3064, 2012.

[YDLT76] F. Young, J. De Leeuw, and Y. Takane. Regression with qualitative and quan-
titative variables: An alternating least squares method with optimal scaling
features. Psychometrika, 41(4):505–529, 1976.

[YYH+13] H. Yun, H.-F. Yu, C.-J. Hsieh, S. V. N. Vishwanathan, and I. Dhillon. NO-
MAD: Non-locking, stOchastic Multi-machine algorithm for Asynchronous and
Decentralized matrix completion. arXiv preprint arXiv:1312.0193, 2013.

[ZCF+10] M. Zaharia, M. Chowdhury, M. Franklin, S. Shenker, and I. Stoica. Spark: Clus-
ter computing with working sets. In Proceedings of the 2nd USENIX conference
on hot topics in cloud computing, page 10, 2010.

[ZHT06]

H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis.
Journal of Computational and Graphical Statistics, 15(2):265–286, 2006.

[ZV86]

W. Zwick and W. Velicer. Comparison of ﬁve rules for determining the number
of components to retain. Psychological bulletin, 99(3):432, 1986.

84

