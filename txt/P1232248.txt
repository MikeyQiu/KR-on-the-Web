8
1
0
2
 
b
e
F
 
1
2
 
 
]

G
L
.
s
c
[
 
 
1
v
5
9
5
7
0
.
2
0
8
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

SMOOTH LOSS FUNCTIONS FOR DEEP TOP-K
CLASSIFICATION

Leonard Berrada1, Andrew Zisserman1 and M. Pawan Kumar1,2
1Department of Engineering Science
University of Oxford
2Alan Turing Institute
{lberrada,az,pawan}@robots.ox.ac.uk

ABSTRACT

The top-k error is a common measure of performance in machine learning and
computer vision. In practice, top-k classiﬁcation is typically performed with deep
neural networks trained with the cross-entropy loss. Theoretical results indeed
suggest that cross-entropy is an optimal learning objective for such a task in the limit
of inﬁnite data. In the context of limited and noisy data however, the use of a loss
function that is speciﬁcally designed for top-k classiﬁcation can bring signiﬁcant
improvements. Our empirical evidence suggests that the loss function must be
smooth and have non-sparse gradients in order to work well with deep neural
networks. Consequently, we introduce a family of smoothed loss functions that are
suited to top-k optimization via deep learning. The widely used cross-entropy is a
special case of our family. Evaluating our smooth loss functions is computationally
challenging: a na¨ıve algorithm would require O((cid:0)n
(cid:1)) operations, where n is the
number of classes. Thanks to a connection to polynomial algebra and a divide-
and-conquer approach, we provide an algorithm with a time complexity of O(kn).
Furthermore, we present a novel approximation to obtain fast and stable algorithms
on GPUs with single ﬂoating point precision. We compare the performance of the
cross-entropy loss and our margin-based losses in various regimes of noise and
data size, for the predominant use case of k = 5. Our investigation reveals that our
loss is more robust to noise and overﬁtting than cross-entropy.

k

1

INTRODUCTION

In machine learning many classiﬁcation tasks present inherent label confusion. The confusion
can originate from a variety of factors, such as incorrect labeling, incomplete annotation, or some
fundamental ambiguities that obfuscate the ground truth label even to a human expert. For example,
consider the images from the ImageNet data set (Russakovsky et al., 2015) in Figure 1, which
illustrate the aforementioned factors. To mitigate these issues, one may require the model to predict
the k most likely labels, where k is typically very small compared to the total number of labels. Then
the prediction is considered incorrect if all of its k labels differ from the ground truth, and correct
otherwise. This is commonly referred to as the top-k error. Learning such models is a longstanding
task in machine learning, and many loss functions for top-k error have been suggested in the literature.

In the context of correctly labeled large data, deep neural networks trained with cross-entropy have
shown exemplary capacity to accurately approximate the data distribution. An illustration of this
phenomenon is the performance attained by deep convolutional neural networks on the ImageNet
challenge. Speciﬁcally, state-of-the-art models trained with cross-entropy yield remarkable success on
the top-5 error, although cross-entropy is not tailored for top-5 error minimization. This phenomenon
can be explained by the fact that cross-entropy is top-k calibrated for any k (Lapin et al., 2016), an
asymptotic property which is veriﬁed in practice in the large data setting. However, in cases where
only a limited amount of data is available, learning large models with cross-entropy can be prone to
over-ﬁtting on incomplete or noisy labels.

To alleviate the deﬁciency of cross-entropy, we present a new family of top-k classiﬁcation loss
functions for deep neural networks. Taking inspiration from multi-class SVMs, our loss creates a

1

Published as a conference paper at ICLR 2018

Figure 1: Examples of images with label confusion, from the validation set of ImageNet. The top-left
image is incorrectly labeled as “red panda”, instead of “giant panda”. The bottom-left image is
labeled as “strawberry”, although the categories “apple”, “banana” and “pineapple” would be
other valid labels. The center image is labeled as “indigo bunting”, which is only valid for the lower
bird of the image. The right-most image is labeled as a cocktail shaker, yet could arguably be a
part of a music instrument (for example with label “cornet, horn, trumpet, trump”). Such examples
motivate the need to predict more than a single label per image.

margin between the correct top-k predictions and the incorrect ones. Our empirical results show that
traditional top-k loss functions do not perform well in combination with deep neural networks. We
believe that the reason for this is the lack of smoothness and the sparsity of the derivatives that are
used in backpropagation. In order to overcome this difﬁculty, we smooth the loss with a temperature
parameter. The evaluation of the smooth function and its gradient is challenging, as smoothing
increases the na¨ıve time complexity from O(n) to O((cid:0)n
(cid:1)). With a connection to polynomial algebra
and a divide-and-conquer method, we present an algorithm with O(kn) time complexity and training
time comparable to cross-entropy in practice. We provide insights for numerical stability of the
forward pass. To deal with instabilities of the backward pass, we derive a novel approximation. Our
investigation reveals that our top-k loss outperforms cross-entropy in the presence of noisy labels
or in the absence of large amounts of data. We further conﬁrm that the difference of performance
reduces with large correctly labeled data, which is consistent with known theoretical results.

k

2 RELATED WORK

Top-k Loss Functions. The majority of the work on top-k loss functions has been applied to
shallow models: Lapin et al. (2016) suggest a convex surrogate on the top-k loss; Fan et al. (2017)
select the k largest individual losses in order to be robust to data outliers; Chang et al. (2017)
formulate a truncated re-weighted top-k loss as a difference-of-convex objective and optimize it with
the Concave-Convex Procedure (Yuille & Rangarajan, 2002); and Yan et al. (2017) propose to use a
combination of top-k classiﬁers and to fuse their outputs.

Closest to our work is the extensive review of top-k loss functions for computer vision by Lapin et al.
(2017). The authors conduct a study of a number of top-k loss functions derived from cross-entropy
and hinge losses. Interestingly, they prove that for any k, cross-entropy is top-k calibrated, which
is a necessary condition for the classiﬁer to be consistent with regard to the theoretically optimal
top-k risk. In other words, cross-entropy satisﬁes an essential property to perform the optimal top-k
classiﬁcation decision for any k in the limit of inﬁnite data. This may explain why cross-entropy
performs well on top-5 error on large scale data sets. While thorough, the experiments are conducted
on linear models, or pre-trained deep networks that are ﬁne-tuned. For a more complete analysis,
we wish to design loss functions that allow for the training of deep neural networks from a random
initialization.

Smoothing. Smoothing is a helpful technique in optimization (Beck & Teboulle, 2012). In work
closely related to ours, Lee & Mangasarian (2001) show that smoothing a binary SVM with a
temperature parameter improves the theoretical convergence speed of their algorithm. Schwing et al.
(2012) use a temperature parameter to smooth latent variables for structured prediction. Lapin et al.
(2017) apply Moreau-Yosida regularization to smooth their top-k surrogate losses.

2

Published as a conference paper at ICLR 2018

Smoothing has also been applied in the context of deep neural networks. In particular, Zheng et al.
(2015) and Clevert et al. (2016) both suggest modifying the non-smooth ReLU activation to improve
the training. Gulcehre et al. (2017) suggest to introduce “mollifyers” to smooth the objective function
by gradually increasing the difﬁculty of the optimization problem. Chaudhari et al. (2017) add a local
entropy term to the loss to promote solutions with high local entropy. These smoothing techniques are
used to speed up the optimization or improve generalization. In this work, we show that smoothing is
necessary for the neural network to perform well in combination with our loss function. We hope that
this insight can also help the design of losses for tasks other than top-k error minimization.

3 TOP-K SVM

3.1 BACKGROUND: MULTI-CLASS SVM

In order to build an intuition about top-k losses, we start with the simple case of k = 1, namely
multi-class classiﬁcation, where the output space is deﬁned as Y = {1, ..., n}. We suppose that a
vector of scores per label s ∈ Rn, and a ground truth label y ∈ Y are both given. The vector s is
the output of the model we wish to learn, for example a linear model or a deep neural network. The
notation 1 will refer to the indicator function over Boolean statements (1 if true, 0 if false).

Prediction. The prediction is given by any index with maximal score:

P (s) ∈ argmax s.

Loss. The classiﬁcation loss incurs a binary penalty by comparing the prediction to the ground
truth label. Plugging in equation (1), this can also be written in terms of scores s as follows:

Λ(s, y) (cid:44) 1(y (cid:54)= P (s)) = 1(max
j∈Y

sj > sy).

Surrogate. The loss in equation (2) is not amenable to optimization, as it is not even continuous in
s. To overcome this difﬁculty, a typical approach in machine learning is to resort to a surrogate loss
that provides a continuous upper bound on Λ. Crammer & Singer (2001) suggest the following upper
bound on the loss, known as the multi-class SVM loss:

l(s, y) = max

{sj + 1} − sy, 0

.

(cid:27)

(cid:26)

max
j∈Y\{y}

(1)

(2)

(3)

In other words, the surrogate loss is zero if the ground truth score is higher than all other scores by a
margin of at least one. Otherwise it incurs a penalty which is linear in the difference between the
score of the ground truth and the highest score over all other classes.

Rescaling. Note that the value of 1 as a margin is an arbitrary choice, and can be changed to α for
any α > 0. This simply entails that we consider the cost Λ of a misclassiﬁcation to be α instead of
1. Moreover, we show in Proposition 8 of Appendix D.2 how the choices of α and of the quadratic
regularization hyper-parameter are interchangeable.

3.2 TOP-K CLASSIFICATION

We now generalize the above framework to top-k classiﬁcation, where k ∈ {1, ..., n − 1}. We use
the following notation: for p ∈ {1, ..., n}, s[p] refers to the p-th largest element of s, and s\p to the
vector (s1, ..., sp−1, sp+1, ..., sn) ∈ Rn−1 (that is, the vector s with the p-th element omitted). The
term Y (k) denotes the set of k-tuples with k distinct elements of Y. Note that we use a bold font for a
tuple ¯y ∈ Y (k) in order to distinguish it from a single label ¯y ∈ Y.
Prediction. Given the scores s ∈ Rn, the top-k prediction consists of any set of labels correspond-
ing to the k largest scores:

(cid:110)

Pk(s) ∈

¯y ∈ Y (k) : ∀ i ∈ {1, .., k}, s¯yi ≥ s[k]

(cid:111)

.

(4)

3

Published as a conference paper at ICLR 2018

Loss. The loss depends on whether y is part of the top-k prediction, which is equivalent to comparing
the k-largest score with the ground truth score:

Again, such a binary loss is not suitable for optimization. Thus we introduce a surrogate loss.

Λk(s, y) (cid:44) 1(y /∈ Pk(s)) = 1(s[k] > sy).

Surrogate. As pointed out in Lapin et al. (2015), there is a natural extension of the previous
multi-class case:

lk(s, y) (cid:44) max

(cid:110)(cid:0)s\y + 1(cid:1)

(cid:111)
[k] − sy, 0

.

This loss creates a margin between the ground truth and the k-th largest score, irrespectively of the
values of the (k − 1)-largest scores. Note that we retrieve the formulation of Crammer & Singer
(2001) for k = 1.

Difﬁculty of the Optimization. The surrogate loss lk of equation (6) suffers from two disadvan-
tages that make it difﬁcult to optimize: (i) it is not a smooth function of s – it is continuous but not
differentiable – and (ii) its weak derivatives have at most two non-zero elements. Indeed at most two
elements of s are retained by the (·)[k] and max operators in equation (6). All others are discarded
and thus get zero derivatives. When lk is coupled with a deep neural network, the model typically
yields poor performance, even on the training set. Similar difﬁculties to optimizing a piecewise
linear loss have also been reported by Li et al. (2017) in the context of multi-label classiﬁcation. We
illustrate this in the next section.

We postulate that the difﬁculty of the optimization explains why there has been little work exploring
the use of SVM losses in deep learning (even in the case k = 1), and that this work may help remedy
it. We propose a smoothing that alleviates both issues (i) and (ii), and we present experimental
evidence that the smooth surrogate loss offers better performance in practice.

3.3 SMOOTH SURROGATE LOSS

Reformulation. We introduce the following notation: given a label ¯y ∈ Y, Y (k)
is the subset of
tuples from Y (k) that include ¯y as one of their elements. For ¯y ∈ Y (k) and y ∈ Y, we further deﬁne
∆k(¯y, y) (cid:44) 1(y /∈ ¯y). Then, by adding and subtracting the k − 1 largest scores of s\y as well as sy,
we obtain:

¯y

lk(s, y) = max

[k] − sy, 0

(cid:110)(cid:0)s\y + 1(cid:1)



= max
¯y∈Y (k)



∆k(¯y, y) +

(cid:88)

j∈¯y

sj

− max
¯y∈Y (k)
y




(cid:88)



j∈¯y






sj

.

(cid:111)

,





We give a more detailed proof of this in Appendix A.1. Since the margin can be rescaled without loss
of generality, we rewrite lk as:

lk(s, y) = max
¯y∈Y (k)

∆k(¯y, y) +






1
k

(cid:88)

j∈¯y

sj











1
k






sj

.

(cid:88)

j∈¯y

− max
¯y∈Y (k)
y

Smoothing.
parameter τ > 0:

In the form of equation (8), the loss function can be smoothed with a temperature

Lk,τ (s, y) = τ log

exp

∆k(¯y, y) +

− τ log

(cid:20) (cid:88)

¯y∈Y (k)

(cid:16)

(cid:18) 1
τ

(cid:17)(cid:19)(cid:21)

1
k

(cid:88)

j∈¯y

sj

(cid:20) (cid:88)

¯y∈Y (k)

y

exp

(cid:16) 1
kτ

(cid:88)

j∈¯y

(cid:17)(cid:21)
.

sj

Note that we have changed the notation to use Lk,τ to refer to the smooth loss. In what follows,
we ﬁrst outline the properties of Lk,τ and its relationship with cross-entropy. Then we show the
empirical advantage of Lk,τ over its non-smooth counter-part lk.

(5)

(6)

(7)

(8)

(9)

4

Published as a conference paper at ICLR 2018

Properties of the Smooth Loss. The smooth loss Lk,τ has a few interesting properties. First,
for any τ > 0, Lk,τ is inﬁnitely differentiable and has non-sparse gradients. Second, under mild
conditions, when τ → 0+, the non-maximal terms become negligible, therefore the summations
collapse to maximizations and Lk,τ → lk in a pointwise sense (Proposition 2 in Appendix A.2).
Third, Lk,τ is an upper bound on lk if and only if k = 1 (Proposition 3 in Appendix A.3), but Lk,τ is,
up to a scaling factor, an upper bound on Λk (Proposition 4 in Appendix A.4). This makes it a valid
surrogate loss for the minimization of Λk.

Relationship with Cross-Entropy. We have previously seen that the margin can be rescaled by
a factor of α > 0. In particular, if we scale ∆ by α → 0+ and choose a temperature τ = 1, it can
be seen that L1,1 becomes exactly the cross-entropy loss for classiﬁcation. In that sense, Lk,τ is
a generalization of the cross-entropy loss to: (i) different values of k ≥ 1, (ii) different values of
temperature and (iii) higher margins with the scaling α of ∆. For simplicity purposes, we will keep
α = 1 in this work.

Experimental Validation.
In order to show how smoothing helps the training, we train a DenseNet
40-12 on CIFAR-100 from Huang et al. (2017) with the same hyper-parameters and learning rate
schedule. The only difference with Huang et al. (2017) is that we replace the cross-entropy loss with
L5,τ for different values of τ . We plot the top-5 training error in Figure 2a (for each curve, the value
of τ is held constant during training):

(a) Top-5 training error for different values of τ . The
dashed line y = 0.95 represents the base error for ran-
dom predictions. The successive drops in the curves
correspond to the decreases of the learning rate at
epochs 150 and 225.

(b) Proportion of non (numerically) zero elements in
the loss derivatives for different values of τ . These
values are obtained with the initial random weights of
the neural network, and are averaged over the training
set.

Figure 2: Inﬂuence of the temperature τ on the learning of a DenseNet 40-12 on CIFAR-100. We
conﬁrm that smoothing helps the training of a neural network in Figure 2a, where a large enough
value of τ greatly helps the performance on the training set. In Figure 2b, we observe that such
high temperatures yield gradients that are not sparse. In other words, with a high temperature, the
gradient is informative about a greater number of labels, which helps the training of the model.

We remark that the network exhibits good accuracy when τ is high enough (0.01 or larger). For τ
too small, the model fails to converge to a good critical point. When τ is positive but small, the
function is smooth but the gradients are numerically sparse (see Figure 2b), which suggests that the
smoothness property is not sufﬁcient and that non-sparsity is a key factor here.

5

Published as a conference paper at ICLR 2018

4 COMPUTATIONAL CHALLENGES AND EFFICIENT ALGORITHMS

4.1 CHALLENGE

y , which have a cardinality of (cid:0)n

Experimental evidence suggests that it is beneﬁcial to use Lk,τ rather than lk to train a neural network.
However, at ﬁrst glance, Lk,τ may appear prohibitively expensive to compute. Speciﬁcally, there are
(cid:1) respectively. For instance
summations over Y (k) and Y (k)
for ImageNet, we have k = 5 and n = 1, 000, which amounts to (cid:0)n
(cid:1) (cid:39) 8.1012 terms to compute and
sum over for each single sample, thereby making the approach practically infeasible. This is in stark
contrast with lk, for which the most expensive operation is to compute the k-th largest score of an
array of size n, which can be done in O(n). To overcome this computational challenge, we will now
reframe the problem and reveal its exploitable structure.
For a vector e ∈ Rn and i ∈ {1, .., n}, we deﬁne σi(e) as the sum of all products of i distinct
elements of e. Explicitly, σi(e) can be written as σi(e) = (cid:80)
1≤j1<...<ji≤n ej1 ...eji. The terms σi
are known as the elementary symmetric polynomials. We further deﬁne σ0(e) = 1 for convenience.

(cid:1) and (cid:0) n

k−1

k

k

We now re-write Lk,τ using the elementary symmetric polynomials, which appear naturally when
separating the terms that contain the ground truth from the ones that do not:

Lk,τ (s, y) = τ log

exp (∆k(¯y, y)/τ )

exp(sj/kτ )

(cid:89)

(cid:21)

(cid:20) (cid:88)

¯y∈Y (k)

− τ log

(cid:20) (cid:88)

(cid:89)

¯y∈Y (k)

y

j∈¯y

(cid:20) (cid:88)

(cid:89)

¯y∈Y (k)

y

j∈¯y

j∈¯y
(cid:21)
,
exp(sj/kτ )

= τ log

exp(sj/kτ ) + exp (1/τ )

(cid:88)

(cid:89)

(cid:21)
exp(sj/kτ )

¯y∈Y (k)\Y (k)

y

j∈¯y

− τ log

(cid:20) (cid:88)

(cid:89)

(cid:21)
,
exp(sj/kτ )

¯y∈Y (k)

y

j∈¯y

(cid:20)

(cid:20)

− τ log

exp(sy/kτ )σk−1(exp(s\y/kτ ))

(cid:21)
.

= τ log

exp(sy/kτ )σk−1(exp(s\y/kτ )) + exp (1/τ ) σk(exp(s\y/kτ ))

(10)

(cid:21)

Note that the application of exp to vectors is meant in an element-wise fashion. The last equality of
equation (10) reveals that the challenge is to efﬁciently compute σk−1 and σk, and their derivatives
for the optimization.

While there are existing algorithms to evaluate the elementary symmetric polynomials, they have
been designed for computations on CPU with double ﬂoating point precision. For the most recent
work, see Jiang et al. (2016). To efﬁciently train deep neural networks with Lk,τ , we need algorithms
that are numerically stable with single ﬂoating point precision and that exploit GPU parallelization.
In the next sections, we design algorithms that meet these requirements. The ﬁnal performance is
compared to the standard alternative algorithm in Appendix B.3.

4.2 FORWARD COMPUTATION

We consider the general problem of efﬁciently computing (σk−1, σk). Our goal is to compute σk(e),
where e ∈ Rn and k (cid:28) n. Since this algorithm will be applied to e = exp(s\y/kτ ) (see equation
1, n
(10)), we can safely assume ei (cid:54)= 0 for all i ∈
(cid:75)
(cid:74)

The main insight of our approach is the connection of σi(e) to the polynomial:

.

Indeed, if we expand P to α0 + α1X + ... + αnX n, Vieta’s formula gives the relationship:

P (cid:44) (X + e1)(X + e2)...(X + en).

∀i ∈

0, n
(cid:75)

(cid:74)

, αi = σn−i(e).

6

(11)

(12)

Published as a conference paper at ICLR 2018

Therefore, it sufﬁces to compute the coefﬁcients αn−k to obtain the value of σk(e). To compute the
expansion of P , we can use a divide-and-conquer approach with polynomial multiplications when
merging two branches of the recursion.

This method computes all (σi)1≤i≤n instead of the only (σi)k−1≤i≤k that we require. Since we do
not need σi(e) for i > k, we can avoid computations of all coefﬁcients for a degree higher than n − k.
However, typically k (cid:28) n. For example, in ImageNet, we have k = 5 and n = 1, 000, therefore we
have to compute coefﬁcients up to a degree 995 instead of 1,000, which is a negligible improvement.

To turn k (cid:28) n to our advantage, we notice that σi(e) = σn(e)σn−i(1/e). Moreover, σn(e) =

can be computed in O(n). Therefore we introduce the polynomial:

Q (cid:44) σn(e)(X +

)(X +

)...(X +

1
e1

1
e2

1
en

).

Then if we expand Q to β0 + β1X + ... + βnX n, we obtain with Vieta’s formula again:

∀i ∈

,
0, n
(cid:75)
(cid:74)

βi = σn(e)σn−i(1/e) = σi(e).

Subsequently, in order to compute σk(e), we only require the k ﬁrst coefﬁcients of Q, which is
very efﬁcient when k is small in comparison with n. This results in a time complexity of O(kn)
(Proposition 5 in Appendix B.1). Moreover, there are only O(log(n)) levels of recursion, and since
every level can have its operations parallelized, the resulting algorithm scales very well with n when
implemented on a GPU (see Appendix B.3.2 for practical runtimes).

The algorithm is described in Algorithm 1: step 2 initializes the polynomials for the divide and
conquer method. While the polynomial has not been fully expanded, steps 5-6 merge branches
by performing the polynomial multiplications (which can be done in parallel). Step 10 adjusts
the coefﬁcients using equation (14). We point out that we could obtain an algorithm with a time
complexity of O(n log(k)2) if we were using Fast Fourier Transform for polynomial multiplications
in steps 5-6. Since we are interested in the case where k is small (typically 5), such an improvement
is negligible.

n
(cid:81)
i=1

ei

(13)

(14)

+)n, k ∈ N∗

Algorithm 1 Forward Pass
Require: e ∈ (R∗
1: t ← 0
2: P (t)
3: p ← n
4: while p > 1 do
P (t+1)
5:
1

i ← (1, 1/ei) for i ∈

← P (t)

1 ∗ P (t)

2

...

P (t+1)
(p−1)//2 ← P (t)
t ← t + 1
p ← (p − 1)//2

6:
7:
8:
9: end while
10: P (t+1) ← P (t) ×

n
(cid:81)
i=1

ei

11: return P (t+1)

p−1 ∗ P (t)

p

1, n
(cid:75)
(cid:74)

(cid:46) Initialize n polynomials to X + 1
ei

(encoded by coefﬁcients)
(cid:46) Number of polynomials
(cid:46) Merge branches with polynomial multiplications
(cid:46) Polynomial multiplication up to degree k

(cid:46) Polynomial multiplication up to degree k

(cid:46) Update number of polynomials

(cid:46) Recover σi(e) = σn−i(1/e)σn(e)

Obtaining numerical stability in single ﬂoating point precision requires special attention: the use of
exponentials with an arbitrarily small temperature parameter is fundamentally unstable. In Appendix
B.2.1, we describe how operating in the log-space and using the log-sum-exp trick alleviates this
issue. The stability of the resulting algorithm is empirically veriﬁed in Appendix B.3.3.

4.3 BACKWARD COMPUTATION

A side effect of using Algorithm 1 is that a large number of buffers are allocated for automatic
differentiation: for each addition in log-space, we apply log and exp operations, each of which needs
to store values for the backward pass. This results in a signiﬁcant amount of time spent on memory
allocations, which become the time bottleneck. To avoid this, we exploit the structure of the problem

7

Published as a conference paper at ICLR 2018

and design a backward algorithm that relies on the results of the forward pass. By avoiding the
memory allocations and considerably reducing the number of operations, the backward pass is then
sped up by one to two orders of magnitude and becomes negligible in comparison to the forward pass.
We describe our efﬁcient backward pass in more details below.

First, we introduce the notation for derivatives:

For i ∈

, 1 ≤ j ≤ k,
1, n
(cid:75)
(cid:74)

δj,i (cid:44) ∂σj(e)
∂ei

.

We now observe that:

δj,i = σj−1(e\i).
(16)
In other words, equation (16) states that δj,i, the derivative of σj(e) with respect to ei, is the sum of
product of all (j − 1)-tuples that do not include ei. One way of obtaining σj−1(e\i) is to compute
a forward pass for e\i, which we would need to do for every i ∈
. To avoid such expensive
computations, we remark that σj(e) can be split into two terms: the ones that contain ei (which can
expressed as eiσj−1(e\i)) and the ones that do not (which are equal to σj(e\i) by deﬁnition). This
gives the following relationship:

1, n
(cid:75)

(cid:74)

Simplifying equation (17) using equation (16), we obtain the following recursive relationship:

σj(e\i) = σj(e) − eiσj−1(e\i).

δj,i = σj−1(e) − eiδj−1,i.

Since the (σj(e))1≤i≤k have been computed during the forward pass, we can initialize the induction
with δ1,i = 1 and iteratively compute the derivatives δj,i for j ≥ 2 with equation (18). This is
summarized in Algorithm 2.

(15)

(17)

(18)

Algorithm 2 Backward Pass
Require: e, (σj(e))1≤j≤k, k ∈ N∗
1: δ1,i = 1 for i ∈
2: for j ∈
3:
4: end for

1, n
(cid:74)
(cid:75)
do
δj,i = σj−1(e) − eiδj−1,i for i ∈

1, k
(cid:74)

(cid:75)

1, n
(cid:75)
(cid:74)

(cid:46) (σj(e))1≤j≤k have been computed in the forward pass

Algorithm 2 is subject to numerical instabilities (Observation 1 in Appendix B.2.2). In order to
avoid these, one solution is to use equation (16) for each unstable element, which requires numerous
forward passes. To avoid this inefﬁciency, we provide a novel approximation in Appendix B.2.2: the
computation can be stabilized by an approximation with signiﬁcantly smaller overhead.

5 EXPERIMENTS

Theoretical results suggest that Cross-Entropy (CE) is an optimal classiﬁer in the limit of inﬁnite
data, by accurately approximating the data distribution. In practice, the presence of label noise makes
the data distribution more complex to estimate when only a ﬁnite number of samples is available. For
these reasons, we explore the behavior of CE and Lk,τ when varying the amount of label noise and
the training data size. For the former, we introduce label noise in the CIFAR-100 data set (Krizhevsky,
2009) in a manner that would not perturb the top-5 error of a perfect classiﬁer. For the latter, we vary
the training data size on subsets of the ImageNet data set (Russakovsky et al., 2015).

In all the following experiments, the temperature parameter is ﬁxed throughout training. This choice
is discussed in Appendix D.1. The algorithms are implemented in Pytorch (Paszke et al., 2017) and
are publicly available at https://github.com/oval-group/smooth-topk. Experiments
on CIFAR-100 and ImageNet are performed on respectively one and two Nvidia Titan Xp cards.

5.1 CIFAR-100 WITH NOISE

In this experiment, we investigate the impact of label noise on CE and L5,1. The CIFAR-
Data set.
100 data set contains 60,000 RGB images, with 50,000 samples for training-validation and 10,000 for
testing. There are 20 “coarse” classes, each consisting of 5 “ﬁne” labels. For example, the coarse

8

Published as a conference paper at ICLR 2018

class “people” is made up of the ﬁve ﬁne labels “baby”, “boy”, “girl”, “man” and “woman”. In this
set of experiments, the images are centered and normalized channel-wise before they are fed to the
network. We use the standard data augmentation technique with random horizontal ﬂips and random
crops of size 32 × 32 on the images padded with 4 pixels on each side.

We introduce noise in the labels as follows: with probability p, each ﬁne label is replaced by a
ﬁne label from the same coarse class. This new label is chosen at random and may be identical to
the original label. Note that all instances generated by data augmentation from a single image are
assigned the same label. The case p = 0 corresponds to the original data set without noise, and p = 1
to the case where the label is completely random (within the ﬁne labels of the coarse class). With this
method, a perfect top-5 classiﬁer would still be able to achieve 100 % accuracy by systematically
predicting the ﬁve ﬁne labels of the unperturbed coarse label.

Methods. To evaluate our loss functions, we use the architecture DenseNet 40-40 from Huang
et al. (2017), and we use the same hyper-parameters and learning rate schedule as in Huang et al.
(2017). The temperature parameter is ﬁxed to one. When the level of noise becomes non-negligible,
we empirically ﬁnd that CE suffers from over-ﬁtting and signiﬁcantly beneﬁts from early stopping –
which our loss does not need. Therefore we help the baseline and hold out a validation set of 5,000
images, on which we monitor the accuracy across epochs. Then we use the model with the best top-5
validation accuracy and report its performance on the test set. Results are averaged over three runs
with different random seeds.

Table 1: Testing performance on CIFAR-100 with different levels of label noise. With noisy labels,
L5,1 consistently outperforms CE on both top-5 and top-1 accuracies, with improvements increasingly
signiﬁcant with the level of noise. For reference, a model making random predictions would obtain
1% top-1 accuracy and 5% top-5 accuracy.

Noise Top-1 Accuracy (%) Top-5 Accuracy (%)
Level
0.0
0.2
0.4
0.6
0.8
1.0

L5,1
94.29
90.59
87.39
83.86
79.32
72.93

L5,1
69.33
71.30
70.02
67.97
55.85
15.28

CE
94.34
87.89
83.04
79.59
74.80
67.70

CE
76.68
68.20
61.18
52.50
35.53
14.06

Results. As seen in Table 1, L5,1 outperforms CE on the top-5 testing accuracy when the labels
are noisy, with an improvement of over 5% in the case p = 1. When there is no noise in the labels,
CE provides better top-1 performance, as expected. It also obtains a better top-5 accuracy, although
by a very small margin. Interestingly, L5,1 outperforms CE on the top-1 error when there is noise,
although L5,1 is not a surrogate for the top-1 error. For example when p = 0.8, L5,1 still yields an
accuracy of 55.85%, as compared to 35.53% for CE. This suggests that when the provided label is
only informative about top-5 predictions (because of noise or ambiguity), it is preferable to use L5,1.

5.2

IMAGENET

Data set. As shown in Figure 1, the ImageNet data set presents different forms of ambiguity and
noise in the labels. It also has a large number of training samples, which allows us to explore different
regimes up to the large-scale setting. Out of the 1.28 million training samples, we use subsets of
various sizes and always hold out a balanced validation set of 50,000 images. We then report results
on the 50,000 images of the ofﬁcial validation set, which we use as our test set. Images are resized so
that their smaller dimension is 256, and they are centered and normalized channel-wise. At training
time, we take random crops of 224 × 224 and randomly ﬂip the images horizontally. At test time, we
use the standard ten-crop procedure (Krizhevsky et al., 2012).

We report results for the following subset sizes of the data: 64k images (5%), 128k images (10%),
320k images (25%), 640k images (50%) and ﬁnally the whole data set (1.28M − 50k = 1.23M
images for training). Each strict subset has all 1,000 classes and a balanced number of images per
class. The largest subset has the same slight unbalance as the full ImageNet data set.

9

Published as a conference paper at ICLR 2018

Methods.
In all the following experiments, we train a ResNet-18 (He et al., 2016), adapting the
protocol of the ImageNet experiment in Huang et al. (2017). In more details, we optimize the model
with Stochastic Gradient Descent with a batch-size of 256, for a total of 120 epochs. We use a
Nesterov momentum of 0.9. The temperature is set to 0.1 for the SVM loss (we discuss the choice
of the temperature parameter in Appendix D.1). The learning rate is divided by ten at epochs 30,
60 and 90, and is set to an initial value of 0.1 for CE and 1 for L5,0.1. The quadratic regularization
hyper-parameter is set to 0.0001 for CE. For L5,0.1, it is set to 0.000025 to preserve a similar relative
weighting of the loss and the regularizer. For both methods, training on the whole data set takes about
a day and a half (it is only 10% longer with L5,0.1 than with CE). As in the previous experiments,
the validation top-5 accuracy is monitored at every epoch, and we use the model with best top-5
validation accuracy to report its test error.

Probabilities for Multiple Crops. Using multiple crops requires a probability distribution over
labels for each crop. Then this probability is averaged over the crops to compute the ﬁnal prediction.
The standard method is to use a softmax activation over the scores. We believe that such an approach
is only grounded to make top-1 predictions. The probability of a label ¯y being part of the top-5
prediction should be marginalized over all combinations of 5 labels that include ¯y as one of their
elements. This can be directly computed with our algorithms to evaluate σk and its derivative. We
refer the reader to Appendix C for details. All the reported results of top-5 error with multiple crops
are computed with this method. This provides a systematic boost of at least 0.2% for all loss functions.
In fact, it is more beneﬁcial to the CE baseline, by up to 1% in the small data setting.

Table 2: Top-5 accuracy (%) on ImageNet using training sets of various sizes. Results are reported
on the ofﬁcial validation set, which we use as our test set.

% Data Set Number

CE

L5,0.1

100%
50%
25%
10%
5%

of Images
1.23M
640k
320k
128k
64k

90.67
87.57
82.62
71.06
58.31

90.61
87.87
83.38
73.10
60.44

Results. The results of Table 2 conﬁrm that L5,0.1 offers better top-5 error than CE when the
amount of training data is restricted. As the data set size increases, the difference of performance
becomes very small, and CE outperforms L5,0.1 by an insigniﬁcant amount in the full data setting.

6 CONCLUSION

This work has introduced a new family of loss functions for the direct minimization of the top-k error
(that is, without the need for ﬁne-tuning). We have empirically shown that non-sparsity is essential
for loss functions to work well with deep neural networks. Thanks to a connection to polynomial
algebra and a novel approximation, we have presented efﬁcient algorithms to compute the smooth
loss and its gradient. The experimental results have demonstrated that our smooth top-5 loss function
is more robust to noise and overﬁtting than cross-entropy when the amount of training data is limited.

We have argued that smoothing the surrogate loss function helps the training of deep neural networks.
This insight is not speciﬁc to top-k classiﬁcation, and we hope that it will help the design of other
surrogate loss functions. In particular, structured prediction problems could beneﬁt from smoothed
SVM losses. How to efﬁciently compute such smooth functions could open interesting research
problems.

ACKNOWLEDGMENTS

This work was supported by the EPSRC grants AIMS CDT EP/L015987/1, Seebibyte EP/M013774/1,
EP/P020658/1 and TU/B/000048, and by Yougov. Many thanks to A. Desmaison and R. Bunel for
the helpful discussions.

10

Published as a conference paper at ICLR 2018

REFERENCES

Amir Beck and Marc Teboulle. Smoothing and ﬁrst order methods: A uniﬁed framework. SIAM

Journal on Optimization, 2012.

Xiaojun Chang, Yao-Liang Yu, and Yi Yang. Robust top-k multiclass SVM for visual category

recognition. International Conference on Knowledge Discovery and Data Mining, 2017.

Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-SGD: Biasing
gradient descent into wide valleys. International Conference on Learning Representations, 2017.

Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (ELUs). International Conference on Learning Representations,
2016.

Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-based

vector machines. Journal of Machine Learning Research, 2001.

Yanbo Fan, Siwei Lyu, Yiming Ying, and Bao-Gang Hu. Learning with average top-k loss. Neural

Information Processing Systems, 2017.

Caglar Gulcehre, Marcin Moczulski, Francesco Visin, and Yoshua Bengio. Mollifying networks.

International Conference on Learning Representations, 2017.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image

recognition. Conference on Computer Vision and Pattern Recognition, 2016.

Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected

convolutional networks. Conference on Computer Vision and Pattern Recognition, 2017.

Hao Jiang, Stef Graillat, Roberto Barrio, and Canqun Yang. Accurate, validated and fast evaluation of
elementary symmetric functions and its application. Applied Mathematics and Computation, 2016.

Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-

tional neural networks. Neural Information Processing Systems, 2012.

Maksim Lapin, Matthias Hein, and Bernt Schiele. Top-k multiclass SVM. Neural Information

Processing Systems, 2015.

Maksim Lapin, Matthias Hein, and Bernt Schiele. Loss functions for top-k error: Analysis and

insights. Conference on Computer Vision and Pattern Recognition, 2016.

Maksim Lapin, Matthias Hein, and Bernt Schiele. Analysis and optimization of loss functions
for multiclass, top-k, and multilabel classiﬁcation. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2017.

Yuh-Jye Lee and Olvi L Mangasarian. SSVM: A smooth support vector machine for classiﬁcation.

Computational optimization and Applications, 2001.

Yuncheng Li, Yale Song, and Jiebo Luo. Improving pairwise ranking for multi-label image classiﬁca-

tion. Conference on Computer Vision and Pattern Recognition, 2017.

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. NIPS Autodiff Workshop, 2017.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 2015.

Alexander G. Schwing, Tamir Hazan, Marc Pollefeys, and Raquel Urtasun. Efﬁcient structured
prediction with latent variables for general graphical models. International Conference on Machine
Learning, 2012.

11

Published as a conference paper at ICLR 2018

Caixia Yan, Minnan Luo, Huan Liu, Zhihui Li, and Qinghua Zheng. Top-k multi-class svm using

multiple features. Information Sciences, 2017.

Alan L. Yuille and Anand Rangarajan. The concave-convex procedure (CCCP). Neural Information

Processing Systems, 2002.

Hao Zheng, Zhanlei Yang, Wenju Liu, Jizhong Liang, and Yanpeng Li. Improving deep neural

networks using softplus units. International Joint Conference on Neural Networks, 2015.

12

Published as a conference paper at ICLR 2018

APPENDIX

A Surrogate Losses: Properties

A.1 Reformulation .

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.2 Point-wise Convergence

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.3 Bound on Non-Smooth Function . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.4 Bound on Prediction Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B Algorithms: Properties & Performance

B.1 Time Complexity .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.2 Numerical Stability .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.2.1 Forward Pass .

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.2.2 Backward Pass .

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

.

.

B.3 A Performance Comparison with the Summation Algorithm . . . . . . . . . . . .

B.3.1 Summation Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.3.2 Speed .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

B.3.3 Stability .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

.

.

.

.

.

.

.

.

.

.

C Top-k Prediction: Marginalization with the Elementary Symmetric Polynomials

D Hyper-Parameters & Experimental Details

D.1 The Temperature Parameter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

D.1.1 Optimization and Learning . . . . . . . . . . . . . . . . . . . . . . . . . .

D.1.2 Illustration on CIFAR-100 . . . . . . . . . . . . . . . . . . . . . . . . . .

D.1.3 To Anneal or Not To Anneal

. . . . . . . . . . . . . . . . . . . . . . . . .

D.1.4 Practical Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . .

D.2 The Margin .

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

D.2.1 Relationship with Squared Norm Regularization . . . . . . . . . . . . . .

D.2.2 Experiment on ImageNet . . . . . . . . . . . . . . . . . . . . . . . . . . .

D.3 Supplementary Details

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

14

14

15

16

18

18

19

19

19

20

20

22

23

23

23

23

23

23

24

24

25

25

13

Published as a conference paper at ICLR 2018

A SURROGATE LOSSES: PROPERTIES

In this section, we ﬁx n the number of classes. We let τ > 0 and k ∈ {1, ..., n − 1}. All following
results are derived with a loss lk deﬁned as in equation (8):

lk(s, y) (cid:44) max

s\y + 1

−

sy, 0

.

(cid:40)(cid:18) 1
k

(cid:41)

(cid:19)

[k]

1
k

A.1 REFORMULATION

Proposition 1. We can equivalently re-write lk as:

lk(s, y) = max
¯y∈Y (k)

∆k(¯y, y) +

1
k

(cid:88)

j∈¯y

sj











1
k






sj

.

(cid:88)

j∈¯y

− max
¯y∈Y (k)
y

Proof.

lk(s, y) = max

s\y + 1

−

sy, 0

,

= max

s\y + 1

−

sy, 0

+



s[j] +

sy

 −



s[j] +

sy

 ,

(cid:40)(cid:18) 1
k
(cid:40)(cid:18) 1
k

(cid:18) 1
k















1
k

1
k

(cid:41)

(cid:41)



1
k

k−1
(cid:88)

j=1

1
k

k−1
(cid:88)

j=1

1
k

k−1
(cid:88)

j=1






1 +

(cid:88)

sj

1
k

j∈¯y



sj



1
k

(cid:88)

j∈¯y

, max
¯y∈Y (k)
y



− max
¯y∈Y (k)
y



1
k





1
k

k−1
(cid:88)

j=1






1
k



1
k

k−1
(cid:88)

j=1



1
k



1
k






1
k











(cid:88)

j∈¯y

sj






1
k






sj

,

(cid:88)

j∈¯y

− max
¯y∈Y (k)
y






sj

.

1
k

(cid:88)

j∈¯y

= max

max
¯y∈Y (k)\Y (k)

y

= max
¯y∈Y (k)

∆k(¯y, y) +

= max

s\y + 1

+

s[j],

s[j] +

sy

−



s[j] +

sy

 ,






(cid:19)

(cid:19)

[k]

[k]

[k]

(cid:19)






(19)

(20)

(21)

A.2 POINT-WISE CONVERGENCE

Lemma 1. Let n ≥ 2 and e ∈ Rn. Assume that the largest element of e is greater than its second

largest element: e[1] > e[2]. Then lim
τ →0
τ >0

τ log

exp(ei/τ )

= e[1].

(cid:19)

(cid:18) n
(cid:80)
i=1

Proof. For simplicity of notation, and without loss of generality, we suppose that the elements of e
are sorted in descending order. Then for i ∈ {2, ..n}, we have ei − e1 ≤ e2 − e1 < 0 by assumption,
and thus ∀ i ∈ {2, ..n}, lim
τ →0
τ >0

exp((ei − e1)/τ ) = 0. Therefore:

n
(cid:88)

i=1

lim
τ →0
τ >0

And thus:

exp((ei − e1)/τ ) =

exp((ei − e1)/τ ) = 1.

(22)

lim
τ →0
τ >0

(cid:32) n
(cid:88)

i=1

(cid:33)

τ log

exp((ei − e1)/τ )

= 0.

(23)

n
(cid:88)

i=1

lim
τ →0
τ >0

14

Published as a conference paper at ICLR 2018

The result follows by noting that:

τ log

exp(ei/τ )

= e1 + τ log

exp((ei − e1)/τ )

.

(24)

(cid:33)

(cid:33)

(cid:32) n
(cid:88)

i=1

(cid:32) n
(cid:88)

i=1

Proposition 2. Assume that s[k−1] > s[k] and that s[k] > s[k+1] or
lim
τ →0
τ >0

Lk,τ (s, y) = lk(s, y).

1
k

sy > 1 +

s[k]. Then

1
k

Proof. From s[k] > s[k+1] or

sy > 1 +

1
k

1
k

strict maximum. Similarly, from s[k−1] > s[k], we have that max
¯y∈Y (k)

y

Since Lk,τ can be written as:

s[k], one can see that max
¯y∈Y (k)
(cid:110) 1
k

(cid:111)

(cid:80)
j∈¯y

sj

(cid:110)

∆k(¯y, y) +

1
k

(cid:80)
j∈¯y

(cid:111)

sj

is a

is a strict maximum.

Lk,τ (s, y) = τ log

exp

∆k(¯y, y) +

(cid:20) (cid:88)

(cid:18)(cid:16)

(cid:19)(cid:21)

(cid:88)

(cid:17)

sj

/τ

1
k

− τ log

exp

¯y∈Y (k)
(cid:20) (cid:88)

¯y∈Y (k)

y

j∈¯y
(cid:19)(cid:21)

,

(cid:17)

sj

/τ

(cid:18)(cid:16) 1
k

(cid:88)

j∈¯y

the result follows by two applications of Lemma 1.

A.3 BOUND ON NON-SMOOTH FUNCTION

Proposition 3. Lk,τ is an upper bound on lk if and only if k = 1.

Proof. Suppose k = 1. Let s ∈ Rn and y ∈ Y. We introduce y∗ = argmax

{∆1(¯y, y) + s¯y}. Then

¯y∈Y

we have:

l1(s, y) = ∆1(y∗, y) + sy∗ − sy,

= τ log(exp((∆1(y∗, y) + sy∗ )/τ ) − τ log exp(sy/τ ),

≤ τ log(

exp((∆1(¯y, y) + s¯y)/τ ) − τ log exp(sy/τ ) = L1,τ (s, y).

(cid:88)

¯y∈Y

Now suppose k ≥ 2. We construct an example (s, y) such that Lk,τ (s, y) < lk(s, y). For simplicity,
we set y = 1. Then let s1 = α, si = β for i ∈ {2, ..., k + 1} and si = −∞ for i ∈ {k + 2, ..., n}.
The variables α and β are our degrees of freedom to construct the example. Assuming inﬁnite values
simpliﬁes the analysis, and by continuity of Lk,τ and lk, the proof will hold for real values sufﬁciently
small. We further assume that 1 + 1

k (β − α) > 0. Then can write lk(s, y) as:

Exploiting the fact that exp(si/τ ) = 0 for i ≥ k + 2, we have:

lk(s, y) = 1 +

(β − α).

1
k

And:

(cid:88)

(cid:89)

¯y∈Y (k)\Y (k)

y

j∈¯y

exp((1 + sj)/kτ ) = exp

(cid:18) 1 + β
τ

(cid:19)

,

(cid:88)

exp

¯y∈Y (k)

y

(cid:18)(cid:16) 1
k

(cid:88)

j∈¯y

(cid:19)

(cid:17)

sj

/τ

= k exp

(cid:18) α + (k − 1)β
kτ

(cid:19)

.

15

(25)

(26)

(27)

(28)

(29)

(cid:19)

+ exp

(cid:19)(cid:19)

(cid:18) 1 + β
τ

(cid:18)

− τ log

k exp

(cid:18) α + (k − 1)β
kτ

(cid:19)(cid:19)

,

Published as a conference paper at ICLR 2018

This allows us to write Lk,τ as:

Lk,τ (s, y) = τ log

k exp

= τ log

1 +

= τ log

1 +

(cid:18)





(cid:18)



 ,

(cid:17)

(cid:18) α + (k − 1)β
kτ
(cid:16) 1+β
τ
(cid:16) α+(k−1)β
kτ

exp

(cid:17)

k exp

k exp

exp (cid:0) 1

(cid:1)
τ
(cid:16) α−β
kτ
(cid:18) 1
τ

1
k



 ,

(cid:17)

1
k

= τ log

1 +

exp

(1 +

(β − α))

.

(cid:19)(cid:19)

We introduce x = 1 + 1

k (β − α). Then we have:

Lk,τ (s, y) = τ log

1 +

exp

(cid:18)

1
k

(cid:17)(cid:19)

,

(cid:16) x
τ

And:

For any value x > 0, we can ﬁnd (α, β) ∈ R2 such that x = 1 + 1
are veriﬁed. Consequently, we only have to prove that there exists x > 0 such that:

lk(s, y) = x.

(32)
k (β − α) and that all our hypotheses

∆(x) (cid:44) τ log

1 +

exp

− x < 0.

(cid:18)

1
k

(cid:17)(cid:19)

(cid:16) x
τ

We show that

∆(x) < 0, which will conclude the proof by continuity of ∆.

lim
x→∞

∆(x) = τ log

1 +

− x,

(cid:18)

(cid:18)

(cid:18)

(cid:17)(cid:19)

(cid:17)(cid:19)

(cid:16) x
τ
(cid:16) x
τ

exp

1
k
1
k
(cid:18) −x
τ

exp

(cid:19)

+

(cid:19)

1
k

−−−−→
x→∞

x
τ

1
k

= τ log

exp

τ log(

) < 0

since k ≥ 2.

= τ log

1 +

− τ log(exp(

)),

(34)

A.4 BOUND ON PREDICTION LOSS
Lemma 2. Let (p, q) ∈ N2 such that p ≤ q − 1 and q ≥ 1. Then (cid:0)q

(cid:1) ≤ q(cid:0) q

(cid:1).

p+1

p

Proof.

This is a monotonically increasing function of p ≤ q − 1, therefore it is upper bounded by its maximal
value at p = q − 1:

(cid:1)

(cid:0)q
p
(cid:0) q
p+1

(cid:1) =

(q − p − 1)!(p + 1)!
(q − p)!p!

,

=

(p + 1)
q − p

.

(cid:1)

(cid:0)q
p
(cid:0) q
p+1

(cid:1) =

(p + 1)
q − p

≤ q.

16

(30)

(31)

(33)

(35)

(36)

Published as a conference paper at ICLR 2018

Lemma 3. Assume that y /∈ Pk(s). Then we have:



exp



(cid:88)

j∈¯y

sj
kτ



 ≤

1
k

(cid:88)

¯y∈Y (k)

y

(cid:88)

¯y∈Y (k)\Y (k)

y



exp





 .

(cid:88)

j∈¯y

sj
kτ

0, k − 1
(cid:74)

Proof. Let j ∈
. We introduce the random variable Uj, whose probability distribution is
uniform over the set Uj (cid:44) {¯y ∈ Y (k)
: ¯y ∩ Pk(s) = j}. Then Vj is the random variable such that
Vj|Uj replaces y from Uj with a value drawn uniformly from Pk(s). We denote by Vj the set of
values taken by Vj with non-zero probability. Since Vj replaces the ground truth score by one of the
values of Pk(s), it can be seen that:

(cid:75)

y

Vj = {¯y ∈ Y (k)\Y (k)

y

: ¯y ∩ Pk(s) = j + 1}.

Furthermore, we introduce the scoring function f : ¯y ∈ Y (k) (cid:55)→ exp( 1
kτ

sj). Since Pk(s) is the

(cid:80)
j∈¯y

set of the k largest scores and y /∈ Pk(s), we have that:

Therefore we also have that:

This ﬁnally gives us:

f (Vj|Uj) ≥ f (Uj)

with probability 1.

EVj |Uj f (Vj) ≥ f (Uj)

with probability 1.

Making the (uniform) probabilities explicit, we obtain:

EUj

EVj |Uj f (Vj) ≥ EUj f (Uj),
EVj f (Vj) ≥ EUj f (Uj).

f (v) ≥

f (u),

1
|Vj|

|Uj|
|Vj|

(cid:88)

v∈Vj

(cid:88)

v∈Vj

(cid:88)

u∈Uj

1
|Uj|

(cid:88)

u∈Uj

f (v) ≥

f (u).

To derive the set cardinalities, we rewrite Uj and Vj as:

Uj = {¯y ∈ Y (k)
Vj = {¯y ∈ Y (k)\Y (k)

y

y

Therefore we have that:

: ¯y ∩ Pk(s) = j} = {y} × Pk(s)(j) × (Y\({y} ∪ Pk(s))(k−j−1),

: ¯y ∩ Pk(s) = j + 1} = Pk(s)(j+1) × (Y\({y} ∪ Pk(s))(k−j−1).

And:

Therefore:

|Uj| =

(cid:12)
(cid:12){y} × Pk(s)(j) × (Y\({y} ∪ Pk(s))(k−j−1)(cid:12)
(cid:12)
(cid:12)
(cid:12) ,
(cid:18)k
j

(cid:19)(cid:18)n − k − 1
k − j − 1

(cid:19)

,

=

|Vj| =

(cid:12)
(cid:12)Pk(s)(j+1) × (Y\({y} ∪ Pk(s))(k−j−1)(cid:12)
(cid:12)
(cid:12)
(cid:12) ,
(cid:18) k

(cid:19)

(cid:19)(cid:18)n − k − 1
k − j − 1

.

j + 1

=

|Uj|
|Vj|

=

(cid:0)k
j
(cid:0) k
j+1

(cid:1)
(cid:1)(cid:0)n−k−1
k−j−1
(cid:1)(cid:0)n−k−1
k−j−1

(cid:1)

(cid:0)k
j
(cid:0) k
j+1

(cid:1) =

(cid:1) ≤ k

by Lemma 2.

Combining with equation (42), we obtain:
(cid:88)

k

f (v) ≥

f (u).

v∈Vj

(cid:88)

u∈Uj

17

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

Published as a conference paper at ICLR 2018

We sum over j ∈

, which yields:

0, k − 1
(cid:75)
(cid:74)

Finally, we note that {Uj}0≤j≤k−1 and {Vj}0≤j≤k−1 are respective partitions of Y (k)
which gives us the ﬁnal result:

y

and Y (k)\Y (k)
y ,

k−1
(cid:88)

(cid:88)

k

j=0

v∈Vj

f (v) ≥

f (u).

k−1
(cid:88)

(cid:88)

j=0

u∈Uj

k

(cid:88)

f (v) ≥

f (u).

(cid:88)

v∈Y (k)\Y (k)

y

u∈Y (k)

y

Proposition 4. Lk,τ is, up to a scaling factor, an upper bound on the prediction loss Λk:

Lk,τ (s, y) ≥ (1 − τ log(k))Λk(s, y).

Proof. Suppose that Λk(s, y) = 0. Then the inequality is trivial because Lk,τ (s, y) ≥ 0. We now
assume that Λk(s, y) = 1. Then there exist at least k higher scores than sy. To simplify indexing,
we introduce Z (k)
and Tk the set of k labels corresponding to the k-largest scores. By
y
assumption, y /∈ Tk since y is misclassiﬁed. We then write:

y = Y (k)\Y (k)

exp (∆(¯y, y)/τ )

uj = exp (1/τ )

(cid:88)

(cid:89)

(cid:88)

(cid:89)

uj +

uj.

(51)

(cid:88)

¯y∈Y (k)

(cid:89)

j∈¯y

¯y∈Z (k)

y

j∈¯y

¯y∈Y (k)

y

j∈¯y

Thanks to Lemma 3, we have:

Injecting this back into (51):

(cid:88)

¯y∈Y (k)

(cid:20)

And back to the original loss:

(cid:88)

(cid:89)

uj ≥

1
k

(cid:88)

(cid:89)

uj.

¯y∈Z (k)

y

j∈¯y

¯y∈Y (k)

y

j∈¯y

exp (∆(¯y, y)/τ )

uj ≥ (1 +

exp (1/τ ))

(cid:89)

j∈¯y

Lk,τ (s, y) ≥ τ log

(1 +

exp (1/τ ))

uj

− τ log

1
k

1
k

(cid:88)

(cid:89)

¯y∈Y (k)

y

j∈¯y

1
k

= τ log(1 +

exp (1/τ )) ≥ τ log(

exp (1/τ )) = τ log(

) + 1 = 1 − τ log(k).

1
k

(cid:21)

(cid:88)

(cid:89)

uj,

¯y∈Y (k)

y

j∈¯y

(cid:20) (cid:88)

(cid:21)
,

uj

(cid:89)

j∈¯y

y

¯y∈Y (k)
1
k

(48)

(49)

(50)

(52)

(53)

(54)

B ALGORITHMS: PROPERTIES & PERFORMANCE

B.1 TIME COMPLEXITY

Lemma 4. Let P and Q be two polynomials of degree p and q. The time complexity of obtaining the
ﬁrst r coefﬁcients of P Q is O(min{r, p} min{r, q}).

Proof. The multiplication of two polynomials can be written as the convolution of their coefﬁcients,
which can be truncated at degree r for each polynomial.

Proposition 5. The time complexity of Algorithm 1 is O(kn).

18

Published as a conference paper at ICLR 2018

Proof. Let N = log2(n), or equivalently n = 2N . With the divide-and-conquer algorithm, the
complexity of computing the k ﬁrst coefﬁcients of P can be written as:

n
2
Indeed we decompose P = Q1Q2, with each Qi of degree n/2, and for these we compute their k
ﬁrst coefﬁcients in T ( n
2 ). Then given the k ﬁrst coefﬁcients of Q1 and Q2, the k ﬁrst coefﬁcients of
P are computed in O(min{k, n}2) by Lemma 4. Then we can write:

) + min{k, n}2.

T (k, n) = 2T (k,

(55)

T (k, n) = 2T (cid:0)k,

(cid:1) + min{k, n}2,

2T (cid:0)k,

(cid:1) = 4T (cid:0)k,

(cid:1) + 2 min

n
2

(cid:110)

k,

(cid:111)2

,

n
2

n
2
n
4

2N −1T (cid:0)k,

n
2N −1

...
(cid:1) = 2N T (k, 1)
(cid:123)(cid:122)
(cid:125)
(cid:124)
2N O(1)=O(n)

+2N −1 min

(cid:110)

k,

n
2N −1

(cid:111)2

.

By summing these terms, we obtain T (k, n) = 2N T (k, 1) +

N −1
(cid:80)
j=0

(cid:110)

2j min

k,

(cid:111)2

n
2j

. Let n0 ∈ N

such that

2n0+1 < k ≤

n

N −1
(cid:88)

j=0

n
2n0

(cid:110)

k,

. In loose notation, we have k

= O(1). Then we can write:

2n0
n

2j min

(cid:111)2

=

n
2j

2j min

(cid:110)

k,

n
2j

(cid:111)2

+

N −1
(cid:88)

j=n0+1

2j min

(cid:110)

k,

(cid:111)2

,

n
2j

n0(cid:88)

j=0

n0(cid:88)

j=0

=

2jk2 +

N −1
(cid:88)

j=n0+1

2j (cid:16) n
2j

(cid:17)2

,

= (2n0+1 − 1)k2 + n2(2−n0−1 − 2−N ),
= O(kn).

T (k, n) = 2N T (k, 1) +

2j min

N −1
(cid:88)

j=0

(cid:110)

k,

(cid:111)2

,

n
2j

= O(n) + O(kn),
= O(kn).

Thus ﬁnally:

(56)

(57)

(58)

B.2 NUMERICAL STABILITY

B.2.1 FORWARD PASS

In order to ensure numerical stability of the computation, we maintain all computations in the log
space: for a multiplication exp(x1) exp(x2), we actually compute and store x1 + x2; for an addition
exp(x1) + exp(x2) we use the “log-sum-exp” trick: we compute m = max{x1, x2}, and store
m + log(exp(x1 − m) + exp(x2 − m)), which guarantees stability of the result. These two operations
sufﬁce to describe the forward pass.

B.2.2 BACKWARD PASS

Observation 1. The backward recursion of Algorithm 2 is unstable when ei (cid:29) 1 and ei (cid:29) max
p(cid:54)=i

{ep}.

Sketch of Proof. To see that, assume that when we compute (

ep) − ei, we make a numerical error

in the order of (cid:15) (e.g (cid:15) (cid:39) 10−5 for single ﬂoating point precision). With the numerical errors, we

n
(cid:80)
p=1

19

(59)

(60)

(61)

(62)

Published as a conference paper at ICLR 2018

obtain approximate ˆδ as follows:

ˆδ1,i = 1,

...

ˆδ2,i = σ1(e) − ei

ˆδ1,i =

ep − ei = δ2,i + O((cid:15)),

n
(cid:88)

p=1

ˆδ3,i = σ2(e) − ei

ˆδ2,i = σ2(e) − ei(δ2,i + O((cid:15))) = δ3,i + O(ei(cid:15))),

ˆδk,i = σk−1(e) − ei

ˆδk−1,i = ... = δk,i + O(ek−1

(cid:15))).

i

Since ei (cid:29) 1, we quickly obtain unstable results.

Deﬁnition 1. For p ∈ {0, ..., n − k}, we deﬁne the p-th order approximation to the gradient as:

˜δ(p)
k,i

(cid:44)

p
(cid:88)

j=0

(−1)j σk+j(e)

.

ej
i

(cid:12)
(cid:12)δk,i − ˜δ(p)
(cid:12)

k,i

(cid:12)
(cid:12)
(cid:12) =

σk+p(e\i)
ep+1
i

.

δj,i = σj−1(e) − eiδj−1,i.

δj−1,i =

(σj−1(e) − δj,i) .

1
ei

Proposition 6. If we approximate the gradient by its p-th order approximation as deﬁned in equation
(60), the absolute error is:

Proof. We remind equation (18), which gives a recursive relationship for the gradients:

This can be re-written as:

We write σk+p(e\i) = δk+p+1,i, and the result follows by repeated applications of equation (62) for
j ∈ {k + 1, k + 2, ..., k + p + 1}.

Intuition. We have seen in Observation 1 that the recursion tends to be unstable for δj,i when ei
is among the largest elements. When that is the case, the ratio σk+p(e\i)
decreases quickly with p.
This has two consequences: (i) the sum of equation (60) is stable to compute because the summands
have different orders of magnitude and (ii) the error becomes small. Unfortunately, it is difﬁcult to
upper-bound the error of equation (61) by a quantity that is both measurable at runtime (without
expensive computations) and small enough to be informative. Therefore the approximation error
is not controlled at runtime. In practice, we detect the instability of δk,i: numerical issues arise
if subtracted terms have a very small relative difference. For those unstable elements we use the
p-th order approximation (to choose the value of p, a good rule of thumb is p (cid:39) 0.2k). We have
empirically found out that this heuristic works well in practice. Note that this changes the complexity
of the forward pass to O((k + p)n) since we need p additional coefﬁcients during the backward. If
p (cid:39) 0.2k, this increases the running time of the forward pass by 20%, which is a moderate impact.

ep+1
i

B.3 A PERFORMANCE COMPARISON WITH THE SUMMATION ALGORITHM

B.3.1 SUMMATION ALGORITHM

The Summation Algorithm (SA) is an alternative to the Divide-and-Conquer (DC) algorithm for the
evaluation of the elementary symmetric polynomials. It is described for instance in (Jiang et al.,
2016). The algorithm can be summarized as follows:

Implementation. Note that the inner loop can be parallelized, but the outer one is essentially
sequential. In our implementation for speed comparisons, the inner loop is parallelized and a buffer
is pre-allocated for the σj,i.

20

Published as a conference paper at ICLR 2018

Algorithm 3 Summation Algorithm
Require: e ∈ Rn, k ∈ N∗
1: σ0,i ← 1 for 1 ≤ i ≤ n
2: σj,i ← 0 for i < j
3: σ1,1 ← e1
4: for i ∈
5:
6:
7:
8:
9:
10: end for
11: return σk,n

2, n
do
(cid:75)
(cid:74)
m ← max{1, i + k − n}
M ← min{i, k}
for i ∈

m, M
(cid:74)

end for

do
σj,i ← σj,i−1 + eiσj−1,i−1

(cid:75)

B.3.2 SPEED

(cid:46) σj,i = σj(e1, . . . , ei)
(cid:46) Do not deﬁne values for i < j (meaningless)
(cid:46) Initialize recursion

We compare the execution time of the DC and SA algorithms on a GPU (Nvidia Titan Xp). We use
the following parameters: k = 5, a batch size of 256 and a varying value of n. The following timings
are given in seconds, and are computed as the average of 50 runs. In Table 3, we compare the speed
of Summation and DC for the evaluation of the forward pass. In Table 4, we compare the speed of the
evaluation of the backward pass using Automatic Differentiation (AD) and our Custom Algorithm
(CA) (see Algorithm 2).

Table 3: Execution time (s) of the forward pass. The Divide and Conquer (DC) algorithm offers
nearly logarithmic scaling with n in practice, thanks to its parallelization. In contrast, the runtime of
the Summation Algorithm (SA) scales linearly with n.

n
100
SA 0.006
DC 0.011

1,000
0.062
0.018

10,000
0.627
0.024

100,000
6.258
0.146

We remind that both algorithms have a time complexity of O(kn). SA provides little parallelization
(the parallelizable inner loop is small for k (cid:28) n), which is reﬂected in the runtimes. On the other hand,
DC is a recursive algorithm with O(log(n)) levels of recursion, and all operations are parallelized at
each level of the recursion. This allows DC to have near-logarithmic effective scaling with n, at least
in the range {100 − 10, 000}.

Table 4: Execution time (s) of the backward pass. Our Custom Backward (CB) is faster than Automatic
Differentiation (AD).

n
DC (AD)
DC (CB)

100
0.093
0.007

1,000
0.139
0.006

10,000
0.194
0.020

100,000
0.287
0.171

These runtimes demonstrate the advantage of using Algorithm 2 instead of automatic differentiation.
In particular, we see that in the use case of ImageNet (n = 1, 000), the backward computation
changes from being 8x slower than the forward pass to being 3x faster.

B.3.3 STABILITY

We now investigate the numerical stability of the algorithms. Here we only analyze the numerical
stability, and not the precision of the algorithm. We point out that compensation algorithms are
useful to improve the precision of SA but not its stability. Therefore they are not considered in this
discussion.

Jiang et al. (2016) mention that SA is a stable algorithm, under the assumption that no overﬂow or
underﬂow is encountered. However this assumption is not veriﬁed in our use case, as we demonstrate
below. We consider that the algorithm is stable if no overﬂow occurs in the algorithm (underﬂows

21

Published as a conference paper at ICLR 2018

are not an issue for our use cases). We stress out that numerical stability is critical for our machine
learning context: if an overﬂow occurs, the weights of the learning model inevitably diverge to inﬁnite
values.

To test numerical stability in a representative setting of our use cases, we take a random mini-batch
of 128 images from the ImageNet data set and forward it through a pre-trained ResNet-18 to obtain a
vector of scores per sample. Then we use the scores as an input to the SA and DC algorithms, for
various values of the temperature parameter τ . We compare the algorithms with single (S) and double
(D) ﬂoating point precision.

Table 5: Stability on forward pass. A setting is considered stable if no overﬂow has occurred.

101
τ
(cid:88)
SA (S)
(cid:88)
SA (D)
(cid:88)
DC log (S)
DC log (D) (cid:88)

100
(cid:88)
(cid:88)
(cid:88)
(cid:88)

10−1
(cid:55)
(cid:88)
(cid:88)
(cid:88)

10−2
(cid:55)
(cid:55)
(cid:88)
(cid:88)

10−3
(cid:55)
(cid:55)
(cid:88)
(cid:88)

10−4
(cid:55)
(cid:55)
(cid:88)
(cid:88)

By operating in the log-space, DC is signiﬁcantly more stable than SA. In this experimental setting,
DC log is stable in single ﬂoating point precision until τ = 10−36.

C TOP-K PREDICTION: MARGINALIZATION WITH THE ELEMENTARY

SYMMETRIC POLYNOMIALS

We consider the probability of label i being part of the ﬁnal top-k prediction. To that end, we
marginalize over all k-tuples that contain i as one of their element. Then the probability of selecting
label i for the top-k prediction can be written as:

Proposition 7. The unnormalized probability can be computed as:

Proof.

p(k)
i ∝

(cid:88)

¯y∈Y (k)
i

exp(

sj).

(cid:88)

j∈¯y

p(k)
i ∝

d log σi(exp(s))
dsi

.

p(k)
i ∝ exp(si)σk−1(exp(s\i)),

= exp(si)

dσi(exp(s))
d exp(si)

,

=

dσi(exp(s))
dsi

.

Finally we can rescale the unnormalized probability p(k)
independent of i. We obtain:

i

by σk(exp(s)) since the latter quantity is

(k) ∝

ˆpi

1
σk(exp(s))

dσi(exp(s))
dsi

=

d log σi(exp(s))
dsi

.

NB. We prefer to use

rather than

for stability reasons. Once the

d log σi(exp(s))
dsi

dσi(exp(s))
dsi

unnormalized probabilities are computed, they can be normalized by simply dividing by their sum.

22

(63)

(64)

(65)

(66)

Published as a conference paper at ICLR 2018

D HYPER-PARAMETERS & EXPERIMENTAL DETAILS

D.1 THE TEMPERATURE PARAMETER

In this section, we discuss the choice of the temperature parameter. Note that such insights are not
necessarily conﬁned to a top-k minimization: we believe that these ideas generalize to any loss that is
smoothed with a temperature parameter.

D.1.1 OPTIMIZATION AND LEARNING

When the temperature τ has a low value, propositions 3 and 4 suggest that Lk,τ is a sound learning ob-
jective. However, as shown in Figure 2a, optimization is difﬁcult and can fail in practice. Conversely,
optimization with a high value of the temperature is easy, but uninformative about the learning: then
Lk,τ is not representative of the task loss we wish to learn.

In other words, there is a trade-off between the ease of the optimization and the quality of the
surrogate loss in terms of learning. Therefore, it makes sense to use a low temperature that still
permits satisfactory optimization.

D.1.2

ILLUSTRATION ON CIFAR-100

In Figure 2a, we have provided the plots of the training objective to illustrate the speed of convergence.
In Table 6, we give the training and validation accuracies to show the inﬂuence of the temperature:

Table 6: Inﬂuence of the temperature parameter on the training accuracy and testing accuracy.

Temperature Training Accuracy (%) Testing Accuracy (%)
0
10−3
10−2
10−1
100
101
102

10.01
17.40
98.95
99.73
99.78
99.62
99.42

10.38
18.19
91.35
91.70
91.52
90.92
90.46

D.1.3 TO ANNEAL OR NOT TO ANNEAL

The choice of temperature parameter can affect the scale of the loss function. In order to preserve
a sensible trade-off between regularizer and loss, it is important to adjust the regularization hyper-
parameter(s) accordingly (the value of the quadratic regularization for instance). Similarly, the energy
landscape may vary signiﬁcantly for a different value of the temperature, and the learning rate may
need to be adapted too.

Continuation methods usually rely on an annealing scheme to gradually improve the quality of the
approximation. For this work, we have found that such an approach required heavy engineering and
did not provide substantial improvement in our experiments. Indeed, we have mentioned that other
hyper-parameters depend on the temperature, thus these need to be adapted dynamically too. This
requires sensitive heuristics. Furthermore, we empirically ﬁnd that setting the temperature to an
appropriate ﬁxed value yields the same performance as careful ﬁne-tuning of a pre-trained network
with temperature annealing.

D.1.4 PRACTICAL METHODOLOGY

We summarize the methodology that reﬂects the previous insights and that we have found to work
well during our experimental investigation. First, the temperature hyper-parameter is set to a low
ﬁxed value that allows for the model to learn on the training data set. Then other hyper-parameters,
such as quadratic regularization and learning rate are adapted as usual by cross-validation on the
validation set. We believe that the optimal value of the temperature is mostly independent of the
architecture of the neural network, but is greatly inﬂuenced by the values of k and n (see how these
impact the number of summands involved in Lk,τ , and therefore its scale).

23

Published as a conference paper at ICLR 2018

D.2 THE MARGIN

D.2.1 RELATIONSHIP WITH SQUARED NORM REGULARIZATION

In this subsection, we establish the relationship between hyper-parameters of the margin and of the
regularization with a squared norm. Typically the regularizing norm is the Frobenius norm in deep
learning, but the following results will follow for any norm (cid:107) · (cid:107). Although we prove the result for our
top-k loss, we also point out that these results easily generalize to any linear latent structural SVM.

First, we make explicit the role of α in lk with an overload of notation:
(cid:111)
[k] − sy, 0

(cid:110)(cid:0)s\y + α1(cid:1)

lk(s, y, α) = max

,

(67)

where α is a non-negative real number. Now consider the problem of learning a linear top-k SVM on
a dataset (xi, yi)1≤i≤N ∈ (cid:0)Rd × {1, ..., n}(cid:1)N
. We (hyper-)parameterize this problem by λ and α:

(Pλ,α) :

min
w∈Rd×n

λ
2

(cid:107)w(cid:107)2 +

1
N

N
(cid:88)

i=1

lk(wT xi, yi, α).

(68)

Deﬁnition 2. Let λ1, λ2, α1, α2 ≥ 0. We say that (Pλ1,α1 ) and (Pλ2,α2) are equivalent if there exists
γ > 0, ν ∈ R such that:

w ∈ Rd×n is a solution of (Pλ1,α1 ) ⇐⇒ (γw + ν) is a solution of (Pλ2,α2 )

(69)

Justiﬁcation. This deﬁnition makes sense because for γ > 0, ν ∈ R, (γw + ν) has the same
decision boundary as w. In other words, equivalent problems yield equivalent classiﬁers.
Proposition 8. Let λ, α ≥ 0.

1. If α > 0 and λ > 0, then problem (Pλ,α) is equivalent to problems (Pαλ,1) and (P1,αλ).

2. If α = 0 or λ = 0, then problem (Pλ,α) is equivalent to problem (P0,0).

Proof. Let w ∈ Rd×n. We introduce a constant β > 0. Then we can successively write:

w is a solution to (Pλ,α)

⇐⇒ w is a solution to min

(cid:107)w(cid:107)2 +

lk(wT xi, yi, α),

⇐⇒ w is a solution to min

(cid:107)w(cid:107)2 +

max

wT

\yxi + α1

− wT

y xi, 0

,

⇐⇒ w is a solution to min

(cid:107)w(cid:107)2 +

max

wT

\yxi +

−

wT

y xi, 0

,

λ
2

λ
2

λ
2β

w∈Rd×n

w∈Rd×n

w∈Rd×n

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

i=1

1
N

N
(cid:88)

i=1

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

i=1

(cid:26)(cid:16)

(cid:40)(cid:18) 1
β
(cid:40)(cid:18) 1
β

(cid:40)(cid:18)

(cid:27)

1
β

1
β

(cid:17)

[k]

(cid:19)

α
β

1

[k]

(cid:19)

α
β

1

[k]

(cid:19)

α
β

1

[k]

(cid:41)

(cid:41)

(cid:41)

(70)

max

wT

\yxi +

−

wT

y xi, 0

,

max

wT

\yxi +

− wT

y xi, 0

,

⇐⇒ w is a solution to min

βλ
2

(cid:107)

1
β

w(cid:107)2 +

w∈Rd×n

⇐⇒

w is a solution to min

βλ
2

(cid:107)w(cid:107)2 +

w∈Rd×n

⇐⇒

w is a solution to (Pβλ, α

).

β

1
β

1
β

This holds for any β > 0.
If α > 0 and λ > 0, we show equivalence with (Pαλ,1) by setting β to α and with (P1,αλ) by setting
β to 1
λ .
If α = 0, then α

β = 0 for any β > 0 and we can choose β as small as needed to make βλ arbitrarily

24

Published as a conference paper at ICLR 2018

small.
If λ = 0, βλ = 0 for any β > 0 and we can choose β as large as needed to make α

β arbitrarily small.

Note that we do not need any hypothesis on the norm (cid:107) · (cid:107), the result makes only use of the positive
homogeneity property.

Consequence On Deep Networks. Proposition 8 shows that for a deep network trained with lk,
one can ﬁx the value of α to 1, and treat the quadratic regularization of the last fully connected layer
as an independent hyper-parameter. By doing this rather than tuning α, the loss keeps the same scale
which may make it easier to ﬁnd an appropriate learning rate.

When using the smooth loss, there is no direct equivalent to Proposition 8 because the log-sum-
exp function is not positively homogeneous. However one can consider that with a low enough
temperature, the above insight can still be used in practice.

D.2.2 EXPERIMENT ON IMAGENET

In this section, we provide experiments to qualitatively assess the importance of the margin by running
experiments with a margin of either 0 or 1. The following results are obtained on our validation set,
and do not make use of multiple crops.

Top-1 Error. As we have mentioned before, the case (k, τ, α) = (1, 1, 0) corresponds exactly to
Cross-Entropy. We compare this case against the same loss with a margin of 1: (k, τ, α) = (1, 1, 1).
We obtain the following results:

Table 7: Inﬂuence of the margin parameter on top-1 performance.

Top-5 Error. We now compare (k, τ, α) = (5, 0.1, 0) and (k, τ, α) = (5, 0.1, 1):

Margin Top-5 Accuracy (%)

Margin Top-1 Accuracy (%)

0
1

0
1

71.03
71.15

89.12
89.45

Table 8: Inﬂuence of the margin parameter on top-5 performance.

D.3 SUPPLEMENTARY DETAILS

In the main paper, we report the average of the scores on CIFAR-100 for clarity purposes. Here, we
also detail the standard deviation of the scores for completeness.

Table 9: Testing performance on CIFAR-100 with different levels of label noise. We indicate the mean
and standard deviation (in parenthesis) for each score.

Noise
Level
0.0
0.2
0.4
0.6
0.8
1.0

Top-1 Accuracy (%)
L5,1
CE
76.68 (0.38)
69.33 (0.27)
71.30 (0.79)
68.20 (0.50)
70.02 (0.40)
61.18 (0.97)
67.97 (0.51)
52.50 (0.27)
55.85 (0.80)
35.53 (0.79)
15.28 (0.39)
14.06 (0.13)

Top-5 Accuracy (%)
L5,1
CE
94.34 (0.09)
94.29 (0.10)
90.59 (0.08)
87.89 (0.08)
87.39 (0.23)
83.04 (0.38)
83.86 (0.39)
79.59 (0.36)
79.32 (0.25)
74.80 (0.15)
72.93 (0.25)
67.70 (0.16)

25

