SCDV : Sparse Composite Document Vectors using soft clustering over
distributional representations

Dheeraj Mekala*

IIT Kanpur

Vivek Gupta*
Microsoft Research

Bhargavi Paranjape
Microsoft Research

Harish Karnick
IIT Kanpur

dheerajm@iitk.ac.in {t-vigu,t-bhpara}@microsoft.com hk@iitk.ac.in

7
1
0
2
 
y
a
M
 
2
1
 
 
]
L
C
.
s
c
[
 
 
3
v
8
7
7
6
0
.
2
1
6
1
:
v
i
X
r
a

Abstract

We present a feature vector

forma-
tion technique for documents - Sparse
Composite Document Vector (SCDV) -
which overcomes several shortcomings of
the current distributional paragraph vec-
tor representations that are widely used for
In SCDV, word em-
text representation.
beddings are clustered to capture multiple
semantic contexts in which words occur.
They are then chained together to form
document topic-vectors that can express
complex, multi-topic documents. Through
extensive experiments on multi-class and
multi-label classiﬁcation tasks, we outper-
form the previous state-of-the-art method,
NTSG (Liu et al., 2015a). We also show
that SCDV embeddings perform well on
heterogeneous tasks like Topic Coherence,
context-sensitive Learning and Informa-
tion Retrieval. Moreover, we achieve sig-
niﬁcant reduction in training and predic-
tion times compared to other representa-
tion methods. SCDV achieves best of both
worlds - better performance with lower
time and space complexity.

1

Introduction

Distributed word embeddings represent words as
dense, low-dimensional and real-valued vectors
that can capture their semantic and syntactic prop-
erties. These embeddings are used abundantly
by machine learning algorithms in tasks such as
text classiﬁcation and clustering. Traditional bag-
of-word models that represent words as indices
into a vocabulary don’t account for word ordering
and long-distance semantic relations. Represen-
tations based on neural network language models

*Represents equal contribution

(Mikolov et al., 2013b) can overcome these ﬂaws
and further reduce the dimensionality of the vec-
tors. However, there is a need to extend word em-
beddings to entire paragraphs and documents for
tasks such as document and short-text classiﬁca-
tion.

Representing entire documents in a dense, low-
dimensional space is a challenge. A simple
weighted average of the word embeddings in a
large chunk of text ignores word ordering, while
a parse tree based combination of embeddings
(Socher et al., 2013) can only extend to sentences.
(Le and Mikolov, 2014) trains word and para-
graph vectors to predict context but shares word-
embeddings across paragraphs. However, words
can have different semantic meanings in differ-
ent contexts. Hence, vectors of two documents
that contain the same word in two distinct senses
need to account for this distinction for an ac-
curate semantic representation of the documents.
(Wang Ling, 2015), (Liu et al., 2015a) map word
embeddings to a latent topic space to capture dif-
ferent senses in which words occur. However, they
represent complex documents in the same space
as words, reducing their expressive power. These
methods are also computationally intensive.

In this work, we propose the Sparse Compos-
ite Document Vector(SCDV) representation learn-
ing technique to address these challenges and cre-
ate efﬁcient, accurate and robust semantic repre-
sentations of large texts for document classiﬁca-
tion tasks. SCDV combines syntax and semantics
learnt by word embedding models together with a
latent topic model that can handle different senses
of words, thus enhancing the expressive power of
document vectors. The topic space is learnt efﬁ-
ciently using a soft clustering technique over em-
beddings and the ﬁnal document vectors are made
sparse for reduced time and space complexity in
tasks that consume these vectors.

The remaining part of the paper is organized as
follows. Section 2 discusses related work in docu-
ment representations. Section 3 introduces and ex-
plains SCDV in detail. This is followed by exten-
sive and rigorous experiments together with anal-
ysis in section 4 and 5 respectively.

2 Related Work

(Le and Mikolov, 2014) proposed two models
for distributional representation of a document,
namely, Distributed Memory Model Paragraph
Vectors (PV-DM) and Distributed BoWs para-
graph vectors (PV-DBoW). In PV-DM, the model
is learned to predict the next context word us-
In PV-DBoW,
ing word and paragraph vectors.
the paragraph vector is directly learned to predict
randomly sampled context words. In both mod-
els, word vectors are shared across paragraphs.
While word vectors capture semantics across dif-
ferent paragraphs of the text, documents vectors
are learned over context words generated from the
same paragraph and potentially capture only lo-
cal semantics (Pranjal Singh, 2015). Moreover, a
paragraph vector is embedded in the same space as
word vectors though it can contain multiple top-
ics and words with multiple senses. As a result,
doc2vec (Le and Mikolov, 2014) doesn’t perform
well on Information Retrieval as described in (Ai
et al., 2016a) and (Roy et al., 2016). Consequently,
we expect a paragraph vector to be embedded in a
higher dimensional space.

A paragraph vector also assumes all words con-
tribute equally, both quantitatively (weight) and
qualitatively (meaning). They ignore the impor-
tance and distinctiveness of a word across all doc-
uments (Pranjal Singh, 2015). Mukerjee et al.
(Pranjal Singh, 2015) proposed idf-weighted av-
eraging of word vectors to form document vec-
tors. This method tries to address the above prob-
lem. However, it assumes that all words within
a document belong to the same semantic topic.
Intuitively, a paragraph often has words originat-
ing from several semantically different topics. In
fact, Latent Dirichlet Allocation (Blei et al., 2003)
models a document as a distribution of multiple
topics.

These shortcomings are addressed in three
novel composite document representations called
Topical word embedding (TWE-1,TWE-2 and
TWE-3) by (Liu et al., 2015a). TWE-1 learns word
and topic embeddings by considering each topic as

a pseudo word and builds the topical word embed-
ding for each word-topic assignment. Here, the
interaction between a word and the topic to which
it is assigned is not considered. TWE-2 learns a
topical word embedding for each word-topic as-
signment directly, by considering each word- topic
pair as a pseudo word. Here, the interaction be-
tween a word and its assigned topic is considered
but the vocabulary of pseudo-words blows up. For
each word and each topic, TWE-3 builds distinct
embeddings for the topic and word and concate-
nates them for each word-topic assignment. Here,
the word embeddings are inﬂuenced by the corre-
sponding topic embeddings, making words in the
same topic less discriminative.

(Liu et al., 2015a) proposed an architecture
called Neural tensor skip-gram model (NTSG-1,
NTSG-2, NTSG-3, NTSG-4),
that learns multi-
prototype word embeddings and uses a tensor
layer to model the interaction of words and top-
ics to capture different senses. N T SG outper-
forms other embedding methods like T W E −1 on
the 20 newsgroup data-set by modeling context-
sensitive embeddings in addition to topical-word
embeddings. LT SG (Law et al., 2017) builds on
N T SG by jointly learning the latent topic space
and context-sensitive word embeddings. All three,
T W E, N T SG and LT SG use LDA and suf-
fer from computational issues like large training
time, prediction time and storage space. They
also embed document vectors in the same space
as terms. Other works that harness topic modeling
like W T M (Fu et al., 2016), w2v−LDA (Nguyen
et al., 2015), T V + M eanW V (Li et al., 2016a),
LT SG (Law et al., 2017), Gaussian − LDA
(Das et al., 2015), T opic2V ec (Niu et al., 2015),
(Moody, 2016) and M vT M (Li et al., 2016b) also
suffer from similar issues.

(Vivek Gupta, 2016) proposed a method to form
a composite document vector using word embed-
dings and tf-idf values, called the Bag of Words
Vector (BoWV). In BoW V , each document is rep-
resented by a vector of dimension D = K ∗d+K,
where K is the number of clusters and d is the
dimension of the word embeddings. The core
idea behind BoW V is that semantically different
words belong to different topics and their word
vectors should not be averaged. Further, BoW V
computes inverse cluster frequency of each clus-
ter (icf) by averaging the idf values of its mem-
ber terms to capture the importance of words in

the corpus. However, BoW V does hard clustering
using K-means algorithm, assigning each word to
only one cluster or semantic topic but a word can
belong to multiple topics. For example, the word
apple belongs to topic food as a fruit, and belongs
to topic Information Technology as an IT company.
Moreover, BoW V is a non-sparse, high dimen-
sional continuous vector and suffers from compu-
tational problems like large training time, predic-
tion time and storage requirements.

3 Sparse Composite Document Vectors

In this section, we present the proposed Sparse
Composite Document Vector (SCDV) representa-
tion as a novel document vector learning algo-
rithm. The feature formation algorithm can be di-
vided into three steps.

3.1 Word Vector Clustering

We begin by learning d dimensional word vec-
tor representations for every word in the vocab-
ulary V using the skip-gram algorithm with neg-
ative sampling (SGNS) (Mikolov et al., 2013a).
We then cluster these word embeddings using
the Gaussian Mixture Models(GMM) (Reynolds,
2015) soft clustering technique. The number of
clusters, K, to be formed is a parameter of the
SCDV model. By inducing soft clusters, we en-
sure that each word belongs to every cluster with
some probability P (ck|wi).

p(ck = 1) = πk

p(ck = 1|w) =

πkN (w|µk, Σk)
j=1πjN (w|µj, Σj)

ΣK

3.2 Document Topic-vector Formation

For each word wi, we create K different word-
cluster vectors of d dimensions ( (cid:126)wcvik) by weigh-
ing the word’s embedding with its probability dis-
tribution in the kth cluster, P (ck|wi). We then
concatenate all K word-cluster vectors ( (cid:126)wcvik)
into a K×d dimensional embedding and weigh it
with inverse document frequency of wi to form a
word-topics vector ( (cid:126)wtvi). Finally, for all words
appearing in document Dn, we sum their word-
topic vectors (cid:126)wtvi to obtain the document vector
(cid:126)dvDn.

(cid:126)wcvik = (cid:126)wvi × P (ck|wi)

Algorithm 1: Sparse Composite Document
Vector
Data: Documents Dn, n = 1 . . . N
Result: Document vectors

(cid:126)SCDVDn, n = 1

. . . N

1 Obtain word vector ( (cid:126)wvi), for each word wi;
2 Calculate idf values, idf (wi), i = 1..|V | ;
/* |V | is vocabulary size */

3 Cluster word vectors (cid:126)wv using GMM

clustering into K clusters;

4 Obtain soft assignment P (ck|wi) for word wi

and cluster ck;
/* Loop 5-10 can be
pre-computed

5 for each word wi in vocabulary V do
6

for each cluster ck do

(cid:126)wcvik = (cid:126)wvi × P (ck|wi);

end
(cid:126)wtvi = idf (wi) × (cid:76)K
(cid:126)wcvik ;
/* (cid:76) is concatenation

k=1

10 end
11 for n ∈ (1..N ) do

*/

*/

Initialize document vector
for word wi in Dn do
(cid:126)dvDn += (cid:126)wtvi;

(cid:126)dvDn = (cid:126)0;

end
(cid:126)SCDVDn = make-sparse( (cid:126)dvDn);
/* as mentioned in sec 3

*/

7

8

9

12

13

14

15

16

17 end

(cid:126)wtvi = idf (wi) ×

(cid:126)wcvik

K
(cid:77)

k=1

where, (cid:76) is concatenation

3.3 Sparse Document Vectors

After normalizing the vector, we observed that
(cid:126)dvDn are very close to zero. Fig-
most values in
ure 3 veriﬁes this observation. We utilize this fact
to make the document vector (cid:126)dvDn sparse by zero-
ing attribute values whose absolute value is close
to a threshold (speciﬁed as a parameter), which re-
sults in the Sparse Composite Document Vector
(cid:126)SCDVDn.

In particular, let p be percentage sparsity thresh-
old parameter, ai the value of the ith attribute of
the non-Sparse Composite Document Vector and
n represent the nth document in the training set:

4 Experiments

We perform multiple experiments to show the ef-
fectiveness of SCDV representations for multi-
class and multi-label text classiﬁcation. For all ex-
periments and baselines, we use Intel(R) Xeon(R)
CPU E5-2670 v2 @ 2.50GHz, 40 working cores,
128GB RAM machine with Linux Ubuntu 14.4.
However, we utilize multiple cores only during
Word2Vec training and when we run the one-vs-
rest classiﬁer for Reuters.

4.1 Baselines

We consider the following baselines: Bag-of-
Words (BoW) model (Harris, 1954), Bag of Word
Vector (BoWV) (Vivek Gupta, 2016) model, para-
graph vector models (Le and Mikolov, 2014),
Topical word embeddings (TWE-1) (Liu et al.,
2015b), Neural Tensor Skip-Gram Model (NTSG-
1 to NTSG-3) (Liu et al., 2015a), tf-idf weighted
average word-vector model (Pranjal Singh, 2015)
and weighted Bog of Concepts (weight-BoC)
(Kim et al., 2015), where we build topic-document
vectors by counting the member words in each
topic.

We use the best parameter settings as reported in
all our baselines to generate their results. We use
200 dimensions for tf-idf weighted word-vector
model, 400 for paragraph vector model, 80 top-
ics and 400 dimensional vectors for TWE, NTSG,
LTSG and 60 topics and 200 dimensional word
vectors for BOWV. We also compare our results
with reported results of other topic modeling based
document embedding methods like W T M (Fu
et al., 2016), w2v − LDA (Nguyen et al., 2015),
LDA (Liu and EDU, 2014), T V + M eanW V
(Li et al., 2016a), LT SG (Law et al., 2017),
Gaussian − LDA (Das et al., 2015), T opic2V ec
(Niu et al., 2015), (Moody, 2016) and M vT M (Li
et al., 2016b). Implementation of SCDV and re-
lated experiments is available here 1.

4.2 Text Classiﬁcation

We run multi-class experiments on 20NewsGroup
dataset 2 and multi-label classiﬁcation experi-
ments on Reuters-21578 dataset 3. We use
the script4 for preprocessing the Reuters-21578
dataset. We use LinearSVM for multi-class classi-

Figure 1: Word-topics vector formation.

Figure 2: Sparse Composite Document Vector for-
mation.

Figure 3: Distribution of attribute feature vector
values.

ai =

(cid:40)

ai
0

if |ai| ≥ p
otherwise

100 ∗ t

t =

|amin| + |amax|
2

amin = avgn(mini(ai))

amax = avgn(maxi(ai))

Flowcharts depicting the formation of word-
topics vector and Sparse Composite Document
Vectors are shown in ﬁgure 1 and ﬁgure 2 respec-
tively. Algorithm 1 describes SCDV in detail.

1https://github.com/dheeraj7596/SCDV
2http://qwone.com/∼jason/20Newsgroups/
3www.daviddlewis.com/resources/testcollections/reuters21578/
4 https://gist.github.com/herrfz/7967781

ﬁcation and Logistic regression with OneVsRest
setting for multi-label classiﬁcation in baselines
and SCDV.

For SCDV, we set

the dimension of word-
embeddings to 200, sparsity threshold parameter
to 4% and the number of mixture components in
GMM to 60. All mixture components share the
same spherical co-variance matrix. We learn word
vector embedding using Skip-Gram with Negative
Sampling (SGNS) of 10 and minimum word fre-
quency as 20. We use 5-fold cross-validation on
F1 score to tune parameter C of SVM.

4.2.1 Multi-class classiﬁcation

We evaluate classiﬁer performance using standard
metrics like accuracy, macro-averaging precision,
recall and F-measure. Table 1 shows a compari-
son with the current state-of-art (NTSG) document
representations on the 20Newsgroup dataset. We
observe that SCDV outperforms all other current
models by fair margins. We also present the class-
wise precision and recall for 20Newsgroup on an
almost balanced dataset with SVM over Bag of
Words model and the SCDV embeddings in Table
2 and observe that SCDV improves consistently
over all classes.

Table 1: Performance on multi-class classiﬁcation
(Values in red show best performance, the SCDV
algorithm of this paper)

Model
SCDV
NTSG-1
NTSG-2
BoWV
NTSG-3
LTSG
WTM
w2v-LDA

Acc Prec Rec F-mes
84.6
84.6
81.2
82.6
82.4
82.5
80.9
81.6
81.1
81.9
81.8
82.8
80.0
80.9
76.9
77.7
71.6
TV+MeanWV 72.2
71.6
72.2
80.6
81.5
80.5
81.3
70.0
72.2
81.7
81.9
79.0
79.7
71.4
71.8
74.3
75.4
71.5
72.4

MvTM
TWE-1
lda2Vec
lda
weight-AvgVec
BoW
weight-BOC
PV-DBoW
PV-DM

84.5
81.9
82.8
81.1
81.7
81.8
80.3
77.2
71.5
71.5
80.6
80.4
70.7
81.9
79.0
71.8
74.3
71.5

84.6
82.5
83.7
81.1
83.0
82.4
80.3
77.4
71.8
71.8
81.2
81.4
70.8
81.7
79.5
71.3
74.9
72.1

Table 2: Class-level results on the balanced
20newsgroup dataset.

Class Name
alt.atheism
comp.graphics
comp.os.ms-windows.misc
comp.sys.ibm.pc.hardware
comp.sys.mac.hardware
comp.windows.x
misc.forsale
rec.autos
rec.motorcycles
rec.sport.baseball
rec.sport.hockey
sci.crypt
sci.electronics
sci.med
sci.space
soc.religion.christian
talk.politics.guns
talk.politics.mideast
talk.politics.misc
talk.religion.misc

BoW

SCDV

Pre. Rec. Pre. Rec.
79.5
67.8
77.4
67.1
77.2
77.1
73.5
62.8
85.5
77.4
78.6
83.2
85.9
81.3
90.6
80.7
95.7
92.3
94.7
89.8
99.2
93.3
94.7
92.2
74.9
70.9
88.4
79.3
93.8
90.2
92.3
77.3
90.6
71.7
95.4
91.7
59.7
71.7
57.2
63.2

80.2
75.3
78.6
75.6
83.4
87.6
81.4
91.2
95.4
93.2
96.3
92.5
74.6
91.3
88.5
83.3
72.7
96.2
80.9
73.5

72.1
73.5
66.5
72.4
78.2
73.2
88.2
82.8
87.9
89.2
93.7
86.1
73.3
81.3
88.3
87.9
85.7
76.9
56.5
55.4

4.2.2 Multi-label classiﬁcation

We evaluate multi-label classiﬁcation perfor-
mance using Precision@K, nDCG@k (Bhatia
et al., 2015), Coverage error, Label ranking av-
erage precision score (LRAPS)5 and F1-score.
All measures are extensively used for the multi-
label classiﬁcation task. However, F1-score is
an appropriate metric for multi-label classiﬁca-
tion as it considers label biases when train-test
splits are random. Table 3 show evaluation results
for multi-label text classiﬁcation on the Reuters-
21578 dataset.

4.2.3 Effect of Hyper-Parameters

SCDV has three parameters: the number of clus-
ters, word vector dimension and sparsity threshold
parameter. We vary one parameter by keeping the
other two constant. Performance on varying all
three parameters in shown in Figure 4. We ob-
serve that performance improves as we increase
the number of clusters and saturates at 60. The

5Section 3.3.3.2 of

scikit−learn.org/stable/modules/model evaluation.html

Table 3: Performance on various metrics for multi-label classiﬁcation for Reuters(Values in red show
best performance, the SCDV algorithm of this paper)

LRAPS F1-Score

Model

SCDV
BoWV
TWE-1
PV-DM
PV-DBoW
AvgVec
tﬁdf AvgVec

Prec@1
nDCG@1
94.20
92.90
90.91
87.54
88.78
89.09
89.33

Prec
@5
36.98
36.14
35.49
33.24
34.51
34.73
35.04

nDCG
@5
49.55
48.55
47.54
44.21
46.42
46.48
46.83

Coverage
Error
6.48
8.16
8.16
13.15
11.28
9.67
9.42

93.30
91.46
91.46
86.21
87.43
87.28
87.90

81.75
79.16
79.16
70.24
73.68
71.91
71.97

performance improves until a word vector dimen-
sion of 300 after which it saturates. Similarly,
we observe that the performance improves as we
increase p till 4 after which it declines. At 4%
thresholding, we reduce the storage space by 80%
compared to the dense vectors.

4.3 Topic Coherence

We evaluate the topics generated by GMM cluster-
ing on 20NewsGroup for quantitative and qualita-
tive analysis. Instead of using perplexity (Chang
et al., 2011), which doesn’t correlate with seman-
tic coherence and human judgment of individ-
ual topics, we used the popular topic coherence
(Mimno et al., 2011), (Arora et al., 2013), (Liu
and EDU, 2014) measure. A higher topic coher-
ence score indicates a more coherent topic.

We used Bayes rule to compute the P (wk|ci)
for a given topic ci and given word wj and com-
pute the score of the top 10 words for each topic.

P (wk|ci) =

P (ci|wk)P (wk)
P (ci)

where,

LDA and -92.23 of LTSG. Thus, SCDV creates
more coherent topics than both LDA and LTSG.

Table 4 shows top 10 words of 3 topics from
GM M clustering, LDA model and LT SG model
on 20NewsGroup and SCDV shows higher topic
coherence. Words are ranked based on their prob-
ability distribution in each topic. Our results also
support the qualitative results of (Randhawa et al.,
2016) paper, where k-means was used over word
vectors ﬁnd topics.

4.4 Context-Sensitive Learning

In order to demonstrate the effects of soft clus-
tering (GMM) during SCDV formation, we se-
lect some words (wj) with multiple senses from
20Newsgroup and their soft cluster assignments
to ﬁnd the dominant clusters. We also select top
scoring words (wk) from each cluster (ci) to rep-
resent the meaning of that cluster. Table 5 shows
polysemic words and their dominant clusters with
assignment probabilities. This indicates that using
soft clustering to learn word vectors helps com-
bine multiple senses into a single embedding vec-
tor.

P (ci) =

P (ci|wk)P (wk)

4.5

Information Retrieval

K
(cid:88)

i=1

P (wk) =

#(wk)
i=1 #(wi)

(cid:80)V

Here, #(wk) denotes the number of times word
wk appears in the corpus and V represents vocab-
ulary size.

We calculated the topic coherence score for all
topics for SCDV , LDA and LT SG (Law et al.,
2017). Averaging the score of all 80 topics, GMM
clustering scores -85.23 compared to -108.72 of

(Ai et al., 2016b) used (Mikolov et al., 2013b)’s
paragraph vectors to enhance the basic language
model based retrieval model.
The language
model(LM) probabilities are estimated from the
corpus and smoothed using a Dirichlet prior (Zhai
and Lafferty, 2004).
In (Ai et al., 2016b), this
language model is then interpolated with the para-
graph vector (PV) language model as follows.

P (w|d) = (1 − λ)PLM (w|d) + λPP V (w|d)

Figure 4: Effect of varying number of clusters (left), varying word vector dimension (center) and varying
sparsity parameter (right) on performance for 20NewsGroup with SCDV

Figure 5: Visualization of paragraph vectors(left) and SCDV(right) using t-SNE

where,

PP V (w|d) =

exp( (cid:126)w. (cid:126)d)
i=1 exp( (cid:126)wi.(cid:126)d)

(cid:80)V

and the score for document d and query string Q is
given by

score(q, d) =

P (w)P (w|d)

(cid:88)

w∈Q

language model.

where P (w) is obtained from the unigram query
model and score(q, d) is used to rank documents.
(Ai et al., 2016b) do not directly make use of
paragraph vectors for the retrieval task, but im-
prove the document
To di-
rectly make use of paragraph vectors and make
computations more tractable, we directly inter-
polate the language model query-document score
score(q, d) with the similarity score between the
normalized query and document vectors to gener-
ate scoreP V (q, d), which is then used to rank doc-
uments.

scoreP V (q, d) = (1 − λ)score(q, d) + λ(cid:126)q.(cid:126)d

Directly evaluating the document similarity score
with the query paragraph vector rather than col-
lecting similarity scores for individual words in
the query helps avoid confusion amongst distinct
query topics and makes the interpolation operation
faster. In Table 6, we report Mean Average Pre-
cision(MAP) values for four datasets, Associated
Press 88-89 (topics 51-200), Wall Street Journal
(topics 51-200), San Jose Mercury (topics 51-150)
and Disks 4 & 5 (topics 301-450) in the TREC
collection. We learn λ on a held out set of topics.
We observe consistent improvement in MAP for
all datasets. We marginally improve the MAP re-
ported by (Ai et al., 2016b) on the Robust04 task.
In addition, we also report the improvements in
MAP score when Model based relevance feedback
(Zhai and Lafferty, 2001) is applied over the ini-
tially retrieved results from both models. Again,
we notice a consistent improvement in MAP.

Table 4: Top words of some topics from GMM and LDA on 20NewsGroup for K = 80. Higher score
represent better coherent topics.

GMM
ﬁle
bit
image
ﬁles
color
format
images
jpeg
gif

Topic Image
LTSG
image
jpeg
gif
format
ﬁle
ﬁles
convert
color
formats
program images
-75.66
-67.16

LDA
image
ﬁle
color
gif
jpeg
ﬁle
format
bit
images
quality
-88.79

GMM
heath
study
medical
drug
test
drugs
studies
disease
education
age
-66.91

Topic Health
LTSG
stimulation
diseases
disease
toxin
toxic
newsletter
staff
volume
heaths
aids
-96.98

LDA
doctor
disease
coupons
treatment
pain
medical
day
microorganism
medicine
body
-100.39

GMM
ftp
mail
internet
phone
email
send
opinions
fax
address
box
-77.47

Topic Mail
LTSG
anonymous
faq
send
ftp
mailing
server
mail
alt
archive
email
-78.23

LDA
list
mail
information
internet
send
posting
email
group
news
anonymous
-95.47

Table 5: Words with multiple senses assigned to
multiple clusters with signiﬁcant probabilities

Word
subject:1
subject:2
interest:1
interest:2
break:1
break:2
break:3
unit:1
unit:2

Cluster Words
physics, chemistry, math, science
mail, letter, email, gmail
information, enthusiasm, question
bank, market, ﬁnance, investment
vacation, holiday, trip, spring
encryption, cipher, security, privacy
if, elseif, endif, loop, continue
calculation, distance, mass, length
electronics, KWH, digital, signal

P(ci|wj)
0.27
0.72
0.65
0.32
0.52
0.22
0.23
0.25
0.69

5 Analysis and Discussion

SCDV overcomes several challenges encountered
while training document vectors, which we had
mentioned above.

1. Clustering word-embeddings to discover top-
ics improves performance of classiﬁcation as
Figure 4 (left) indicates, while also gener-
ating coherent clusters of words (Table 4).
Figure 5 shows that clustering gives more
discriminative representations of documents
than paragraph vectors do since it uses K ×
d dimensions while paragraph vectors embed
documents and words in the same space. This
enables SCDV to represent complex docu-
ments.
Fuzzy clustering allows words to
belong to multiple topics, thereby recogniz-
ing polysemic words, as Table 5 indicates.
Thus it mimics the word-context interaction
in NTSG and LTSG.

2. Semantically different words are assigned to
different topics. Moreover, a single docu-
ment can contain words from multiple differ-
ent topics. Instead of a weighted averaging of

word embeddings to form document vectors,
as most of the previous work does, concate-
nating word embeddings for each topic (clus-
ter) avoids merging of semantically different
topics.

3. It is well-known that in higher dimensions,
structural regularizers such as sparsity help
overcome the curse of dimensionality (Wain-
wright, 2014).Figure 3 demonstrates this,
since majority of the features are close to
zero. Sparsity also enables linear SVM to
scale to large dimensions. On 20News-
Groups, BoWV model takes up 1.1 GB while
SCDV takes up only 236MB( 80% decrease).
Since GMM assigns a non-zero probability to
every topic in the word embedding, noise can
accumulate when document vectors are cre-
ated and tip the scales in favor of an unrelated
topic. Sparsity helps to reduce this by zeroing
out very small values of probability.

4. SCDV uses Gaussian Mixture Model (GMM)
while T W E, N T SG and LT SG use LDA
for ﬁnding semantic topics respectively.
GMM time complexity is O(V N T 2) while
that of LDA is O(V 2N T ). Here, V = Vo-
cabulary size, N = number of documents
Since num-
and T = number of topics.
ber of topics T < vocabulary size V, GMM
is faster. Empirically, compared to T W E,
SCDV reduces document vector formation,
training and prediction time signiﬁcantly. Ta-
ble 7 shows training and prediction times for
BoWV, SCDV and TWE models.

Table 6: Mean average precision (MAP) for IR on four IR datasets

Dataset
AP
SJM
WSJ
Robust04

LM LM+SCDV MB
0.2856
0.2105
0.2705
0.2684

0.2742
0.2052
0.2618
0.2516

0.3283
0.2341
0.3027
0.2819

MB + SCDV
0.3395
0.2409
0.3126
0.2933

Table 7: Time Comparison (20NewsGroup) (Val-
ues in red show least time, the SCDV algorithm of
this paper)

Time (sec)
DocVec Formation
Total Training
Total Prediction

BoWV TWE-1 SCDV
700
1250
740
1320
120
780

160
200
25

6 Conclusion

In this paper, we propose a document feature for-
mation technique for topic-based document rep-
resentation. SCDV outperforms state-of-the-art
models in multi-class and multi-label classiﬁca-
tion tasks. SCDV introduces sparsity in document
vectors to handle high dimensionality. Table 7 in-
dicates that SCDV shows considerable improve-
ments in feature formation, training and prediction
times for the 20NewsGroups dataset. We show
that fuzzy GMM clustering on word-vectors lead
to more coherent topic than LDA and can also be
used to detect Polysemic words. SCDV embed-
dings also provide a robust estimation of the query
and document language models, thus improving
the MAP of language model based retrieval sys-
tems. In conclusion, SCDV is simple, efﬁcient and
creates a more accurate semantic representation of
documents.

Acknowledgments

The authors wants to thank Nagarajan Natarajan
(Post-Doc, Microsoft Research, India), Praneeth
Netrapalli (Researcher, Microsoft Research, In-
dia), Raghavendra Udupa (Researcher, Microsoft
Research, India), Prateek Jain (Researcher, Mi-
crosoft Research, India) for encouraging and valu-
able feedback .

References

Qingyao Ai, Liu Yang, Jiafeng Guo, and W Bruce
Croft. 2016a. Analysis of the paragraph vector
model for information retrieval. In Proceedings of
the 2016 ACM on International Conference on the
Theory of Information Retrieval. ACM, pages 133–
142.

Qingyao Ai, Liu Yang, Jiafeng Guo, and W Bruce
Croft. 2016b. Improving language estimation with
the paragraph vector model for ad-hoc retrieval. In
Proceedings of the 39th International ACM SIGIR
conference on Research and Development in Infor-
mation Retrieval. ACM, pages 869–872.

Sanjeev Arora, Rong Ge, Yonatan Halpern, David M
Mimno, Ankur Moitra, David Sontag, Yichen Wu,
and Michael Zhu. 2013. A practical algorithm for
topic modeling with provable guarantees. In ICML
(2). pages 280–288.

Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik
Varma, and Prateek Jain. 2015. Sparse local embed-
dings for extreme multi-label classiﬁcation. In Ad-
vances in Neural Information Processing Systems.
pages 730–738.

David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet allocation.
Journal of Machine Learning Research 3:2003.

Jonathan Chang, Jordan L Boyd-Graber, Sean Gerrish,
Chong Wang, and David M Blei. 2011. Reading tea
leaves: How humans interpret topic models. pages
262–272.

Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015.
Gaussian lda for topic models with word embed-
dings. In ACL (1). pages 795–804.

Xianghua Fu, Ting Wang, Jing Li, Chong Yu, and
Wangwang Liu. 2016. Improving distributed word
representation and topic model by word-topic mix-
ture model. In Proceedings of The 8th Asian Con-
ference on Machine Learning. pages 190–205.

Zellig Harris. 1954. Distributional structure. Word

10:146–162.

Han Kyul Kim, Hyunjoong Kim, and Sungzoon Cho.
Bag-of-concepts: Comprehending docu-
2015.
ment representation through clustering words in dis-
tributed representation. SNU Data Mining Center
12.

Jarvan Law, Hankz Hankui Zhuo, Junhua He, and Erhu
Rong. 2017. Ltsg: Latent topical skip-gram for mu-
tually learning topic model and vector representa-
tions. arXiv preprint arXiv:1702.07117 .

Quoc V Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML.
volume 14, pages 1188–1196.

Shaohua Li, Tat-Seng Chua, Jun Zhu, and Chunyan
Miao. 2016a. Generative topic embedding: a con-
In Proceed-
tinuous representation of documents.
ings of The 54th Annual Meeting of the Association
for Computational Linguistics (ACL).

Ximing Li, Jinjin Chi, Changchun Li, Jihong Ouyang,
and Bo Fu. 2016b. Integrating topic modeling with
word embeddings by mixtures of vmfs. Proceedings
of COLING 2016, the 26th International Confer-
ence on Computational Linguistics: Technical Pa-
pers pages 151–160.

Bing Liu and UIC EDU. 2014. Topic modeling using
topics from many domains, lifelong learning and big
data .

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2015a.
Learning context-sensitive word embeddings with
In IJCAI. pages
neural tensor skip-gram model.
1284–1290.

Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong
In AAAI.

Sun. 2015b. Topical word embeddings.
pages 2418–2424.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems. pages 3111–3119.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In HLT-NAACL. pages 746–
751.

David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 262–272.

Christopher E Moody. 2016. Mixing dirichlet topic
models and word embeddings to make lda2vec.
arXiv preprint arXiv:1605.02019 .

Dat Quoc Nguyen, Richard Billingsley, Lan Du, and
Mark Johnson. 2015. Improving topic models with
latent feature word representations. Transactions
of the Association for Computational Linguistics
3:299–313.

Liqiang Niu, Xinyu Dai, Jianbing Zhang, and Jiajun
Chen. 2015. Topic2vec: learning distributed repre-
sentations of topics. In Asian Language Processing

(IALP), 2015 International Conference on. IEEE,
pages 193–196.

Amitabha Mukerjee Pranjal Singh. 2015. Words are
not equal: Graded weighting model for building
composite document vectors. In Proceedings of the
twelfth International Conference on Natural Lan-
guage Processing (ICON-2015). BSP Books Pvt.
Ltd.

Ramandeep S Randhawa, Parag Jain, and Gagan
Topic modeling using dis-
arXiv preprint

Madan. 2016.
tributed word embeddings.
arXiv:1603.04747 .

Douglas Reynolds. 2015. Gaussian mixture models.

Encyclopedia of biometrics pages 827–832.

Dwaipayan Roy, Debasis Ganguly, Mandar Mitra, and
Gareth JF Jones. 2016. Representing documents and
queries as sets of word embedded vectors for infor-
mation retrieval. arXiv preprint arXiv:1606.07869
.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the conference on empirical
methods in natural language processing (EMNLP).
Citeseer, volume 1631, page 1642.

Harish Karnick Ashendra Bansal Pradhuman Jhala
Product classiﬁcation in e-
Vivek Gupta. 2016.
In Pro-
commerce using distributional semantics.
ceedings of COLING 2016, the 26th International
Conference on Computational Linguistics: Techni-
cal Papers.

Martin J Wainwright. 2014. Structured regularizers for
high-dimensional problems: Statistical and compu-
tational issues. Annual Review of Statistics and Its
Application 1:233–253.

Chris Dyer Wang Ling. 2015. Two/too simple adap-
In Pro-
tations of wordvec for syntax problems.
ceedings of the 50th Annual Meeting of the North
American Association for Computational Linguis-
tics. North American Association for Computational
Linguistics.

Chengxiang Zhai and John Lafferty. 2001. Model-
based feedback in the language modeling approach
to information retrieval. In Proceedings of the tenth
international conference on Information and knowl-
edge management. ACM, pages 403–410.

Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to
information retrieval. ACM Transactions on Infor-
mation Systems (TOIS) 22(2):179–214.

SCDV : Sparse Composite Document Vectors using soft clustering over
distributional representations

Dheeraj Mekala*

IIT Kanpur

Vivek Gupta*
Microsoft Research

Bhargavi Paranjape
Microsoft Research

Harish Karnick
IIT Kanpur

dheerajm@iitk.ac.in {t-vigu,t-bhpara}@microsoft.com hk@iitk.ac.in

7
1
0
2
 
y
a
M
 
2
1
 
 
]
L
C
.
s
c
[
 
 
3
v
8
7
7
6
0
.
2
1
6
1
:
v
i
X
r
a

Abstract

We present a feature vector

forma-
tion technique for documents - Sparse
Composite Document Vector (SCDV) -
which overcomes several shortcomings of
the current distributional paragraph vec-
tor representations that are widely used for
In SCDV, word em-
text representation.
beddings are clustered to capture multiple
semantic contexts in which words occur.
They are then chained together to form
document topic-vectors that can express
complex, multi-topic documents. Through
extensive experiments on multi-class and
multi-label classiﬁcation tasks, we outper-
form the previous state-of-the-art method,
NTSG (Liu et al., 2015a). We also show
that SCDV embeddings perform well on
heterogeneous tasks like Topic Coherence,
context-sensitive Learning and Informa-
tion Retrieval. Moreover, we achieve sig-
niﬁcant reduction in training and predic-
tion times compared to other representa-
tion methods. SCDV achieves best of both
worlds - better performance with lower
time and space complexity.

1

Introduction

Distributed word embeddings represent words as
dense, low-dimensional and real-valued vectors
that can capture their semantic and syntactic prop-
erties. These embeddings are used abundantly
by machine learning algorithms in tasks such as
text classiﬁcation and clustering. Traditional bag-
of-word models that represent words as indices
into a vocabulary don’t account for word ordering
and long-distance semantic relations. Represen-
tations based on neural network language models

*Represents equal contribution

(Mikolov et al., 2013b) can overcome these ﬂaws
and further reduce the dimensionality of the vec-
tors. However, there is a need to extend word em-
beddings to entire paragraphs and documents for
tasks such as document and short-text classiﬁca-
tion.

Representing entire documents in a dense, low-
dimensional space is a challenge. A simple
weighted average of the word embeddings in a
large chunk of text ignores word ordering, while
a parse tree based combination of embeddings
(Socher et al., 2013) can only extend to sentences.
(Le and Mikolov, 2014) trains word and para-
graph vectors to predict context but shares word-
embeddings across paragraphs. However, words
can have different semantic meanings in differ-
ent contexts. Hence, vectors of two documents
that contain the same word in two distinct senses
need to account for this distinction for an ac-
curate semantic representation of the documents.
(Wang Ling, 2015), (Liu et al., 2015a) map word
embeddings to a latent topic space to capture dif-
ferent senses in which words occur. However, they
represent complex documents in the same space
as words, reducing their expressive power. These
methods are also computationally intensive.

In this work, we propose the Sparse Compos-
ite Document Vector(SCDV) representation learn-
ing technique to address these challenges and cre-
ate efﬁcient, accurate and robust semantic repre-
sentations of large texts for document classiﬁca-
tion tasks. SCDV combines syntax and semantics
learnt by word embedding models together with a
latent topic model that can handle different senses
of words, thus enhancing the expressive power of
document vectors. The topic space is learnt efﬁ-
ciently using a soft clustering technique over em-
beddings and the ﬁnal document vectors are made
sparse for reduced time and space complexity in
tasks that consume these vectors.

The remaining part of the paper is organized as
follows. Section 2 discusses related work in docu-
ment representations. Section 3 introduces and ex-
plains SCDV in detail. This is followed by exten-
sive and rigorous experiments together with anal-
ysis in section 4 and 5 respectively.

2 Related Work

(Le and Mikolov, 2014) proposed two models
for distributional representation of a document,
namely, Distributed Memory Model Paragraph
Vectors (PV-DM) and Distributed BoWs para-
graph vectors (PV-DBoW). In PV-DM, the model
is learned to predict the next context word us-
In PV-DBoW,
ing word and paragraph vectors.
the paragraph vector is directly learned to predict
randomly sampled context words. In both mod-
els, word vectors are shared across paragraphs.
While word vectors capture semantics across dif-
ferent paragraphs of the text, documents vectors
are learned over context words generated from the
same paragraph and potentially capture only lo-
cal semantics (Pranjal Singh, 2015). Moreover, a
paragraph vector is embedded in the same space as
word vectors though it can contain multiple top-
ics and words with multiple senses. As a result,
doc2vec (Le and Mikolov, 2014) doesn’t perform
well on Information Retrieval as described in (Ai
et al., 2016a) and (Roy et al., 2016). Consequently,
we expect a paragraph vector to be embedded in a
higher dimensional space.

A paragraph vector also assumes all words con-
tribute equally, both quantitatively (weight) and
qualitatively (meaning). They ignore the impor-
tance and distinctiveness of a word across all doc-
uments (Pranjal Singh, 2015). Mukerjee et al.
(Pranjal Singh, 2015) proposed idf-weighted av-
eraging of word vectors to form document vec-
tors. This method tries to address the above prob-
lem. However, it assumes that all words within
a document belong to the same semantic topic.
Intuitively, a paragraph often has words originat-
ing from several semantically different topics. In
fact, Latent Dirichlet Allocation (Blei et al., 2003)
models a document as a distribution of multiple
topics.

These shortcomings are addressed in three
novel composite document representations called
Topical word embedding (TWE-1,TWE-2 and
TWE-3) by (Liu et al., 2015a). TWE-1 learns word
and topic embeddings by considering each topic as

a pseudo word and builds the topical word embed-
ding for each word-topic assignment. Here, the
interaction between a word and the topic to which
it is assigned is not considered. TWE-2 learns a
topical word embedding for each word-topic as-
signment directly, by considering each word- topic
pair as a pseudo word. Here, the interaction be-
tween a word and its assigned topic is considered
but the vocabulary of pseudo-words blows up. For
each word and each topic, TWE-3 builds distinct
embeddings for the topic and word and concate-
nates them for each word-topic assignment. Here,
the word embeddings are inﬂuenced by the corre-
sponding topic embeddings, making words in the
same topic less discriminative.

(Liu et al., 2015a) proposed an architecture
called Neural tensor skip-gram model (NTSG-1,
NTSG-2, NTSG-3, NTSG-4),
that learns multi-
prototype word embeddings and uses a tensor
layer to model the interaction of words and top-
ics to capture different senses. N T SG outper-
forms other embedding methods like T W E −1 on
the 20 newsgroup data-set by modeling context-
sensitive embeddings in addition to topical-word
embeddings. LT SG (Law et al., 2017) builds on
N T SG by jointly learning the latent topic space
and context-sensitive word embeddings. All three,
T W E, N T SG and LT SG use LDA and suf-
fer from computational issues like large training
time, prediction time and storage space. They
also embed document vectors in the same space
as terms. Other works that harness topic modeling
like W T M (Fu et al., 2016), w2v−LDA (Nguyen
et al., 2015), T V + M eanW V (Li et al., 2016a),
LT SG (Law et al., 2017), Gaussian − LDA
(Das et al., 2015), T opic2V ec (Niu et al., 2015),
(Moody, 2016) and M vT M (Li et al., 2016b) also
suffer from similar issues.

(Vivek Gupta, 2016) proposed a method to form
a composite document vector using word embed-
dings and tf-idf values, called the Bag of Words
Vector (BoWV). In BoW V , each document is rep-
resented by a vector of dimension D = K ∗d+K,
where K is the number of clusters and d is the
dimension of the word embeddings. The core
idea behind BoW V is that semantically different
words belong to different topics and their word
vectors should not be averaged. Further, BoW V
computes inverse cluster frequency of each clus-
ter (icf) by averaging the idf values of its mem-
ber terms to capture the importance of words in

the corpus. However, BoW V does hard clustering
using K-means algorithm, assigning each word to
only one cluster or semantic topic but a word can
belong to multiple topics. For example, the word
apple belongs to topic food as a fruit, and belongs
to topic Information Technology as an IT company.
Moreover, BoW V is a non-sparse, high dimen-
sional continuous vector and suffers from compu-
tational problems like large training time, predic-
tion time and storage requirements.

3 Sparse Composite Document Vectors

In this section, we present the proposed Sparse
Composite Document Vector (SCDV) representa-
tion as a novel document vector learning algo-
rithm. The feature formation algorithm can be di-
vided into three steps.

3.1 Word Vector Clustering

We begin by learning d dimensional word vec-
tor representations for every word in the vocab-
ulary V using the skip-gram algorithm with neg-
ative sampling (SGNS) (Mikolov et al., 2013a).
We then cluster these word embeddings using
the Gaussian Mixture Models(GMM) (Reynolds,
2015) soft clustering technique. The number of
clusters, K, to be formed is a parameter of the
SCDV model. By inducing soft clusters, we en-
sure that each word belongs to every cluster with
some probability P (ck|wi).

p(ck = 1) = πk

p(ck = 1|w) =

πkN (w|µk, Σk)
j=1πjN (w|µj, Σj)

ΣK

3.2 Document Topic-vector Formation

For each word wi, we create K different word-
cluster vectors of d dimensions ( (cid:126)wcvik) by weigh-
ing the word’s embedding with its probability dis-
tribution in the kth cluster, P (ck|wi). We then
concatenate all K word-cluster vectors ( (cid:126)wcvik)
into a K×d dimensional embedding and weigh it
with inverse document frequency of wi to form a
word-topics vector ( (cid:126)wtvi). Finally, for all words
appearing in document Dn, we sum their word-
topic vectors (cid:126)wtvi to obtain the document vector
(cid:126)dvDn.

(cid:126)wcvik = (cid:126)wvi × P (ck|wi)

Algorithm 1: Sparse Composite Document
Vector
Data: Documents Dn, n = 1 . . . N
Result: Document vectors

(cid:126)SCDVDn, n = 1

. . . N

1 Obtain word vector ( (cid:126)wvi), for each word wi;
2 Calculate idf values, idf (wi), i = 1..|V | ;
/* |V | is vocabulary size */

3 Cluster word vectors (cid:126)wv using GMM

clustering into K clusters;

4 Obtain soft assignment P (ck|wi) for word wi

and cluster ck;
/* Loop 5-10 can be
pre-computed

5 for each word wi in vocabulary V do
6

for each cluster ck do

(cid:126)wcvik = (cid:126)wvi × P (ck|wi);

end
(cid:126)wtvi = idf (wi) × (cid:76)K
(cid:126)wcvik ;
/* (cid:76) is concatenation

k=1

10 end
11 for n ∈ (1..N ) do

*/

*/

Initialize document vector
for word wi in Dn do
(cid:126)dvDn += (cid:126)wtvi;

(cid:126)dvDn = (cid:126)0;

end
(cid:126)SCDVDn = make-sparse( (cid:126)dvDn);
/* as mentioned in sec 3

*/

7

8

9

12

13

14

15

16

17 end

(cid:126)wtvi = idf (wi) ×

(cid:126)wcvik

K
(cid:77)

k=1

where, (cid:76) is concatenation

3.3 Sparse Document Vectors

After normalizing the vector, we observed that
(cid:126)dvDn are very close to zero. Fig-
most values in
ure 3 veriﬁes this observation. We utilize this fact
to make the document vector (cid:126)dvDn sparse by zero-
ing attribute values whose absolute value is close
to a threshold (speciﬁed as a parameter), which re-
sults in the Sparse Composite Document Vector
(cid:126)SCDVDn.

In particular, let p be percentage sparsity thresh-
old parameter, ai the value of the ith attribute of
the non-Sparse Composite Document Vector and
n represent the nth document in the training set:

4 Experiments

We perform multiple experiments to show the ef-
fectiveness of SCDV representations for multi-
class and multi-label text classiﬁcation. For all ex-
periments and baselines, we use Intel(R) Xeon(R)
CPU E5-2670 v2 @ 2.50GHz, 40 working cores,
128GB RAM machine with Linux Ubuntu 14.4.
However, we utilize multiple cores only during
Word2Vec training and when we run the one-vs-
rest classiﬁer for Reuters.

4.1 Baselines

We consider the following baselines: Bag-of-
Words (BoW) model (Harris, 1954), Bag of Word
Vector (BoWV) (Vivek Gupta, 2016) model, para-
graph vector models (Le and Mikolov, 2014),
Topical word embeddings (TWE-1) (Liu et al.,
2015b), Neural Tensor Skip-Gram Model (NTSG-
1 to NTSG-3) (Liu et al., 2015a), tf-idf weighted
average word-vector model (Pranjal Singh, 2015)
and weighted Bog of Concepts (weight-BoC)
(Kim et al., 2015), where we build topic-document
vectors by counting the member words in each
topic.

We use the best parameter settings as reported in
all our baselines to generate their results. We use
200 dimensions for tf-idf weighted word-vector
model, 400 for paragraph vector model, 80 top-
ics and 400 dimensional vectors for TWE, NTSG,
LTSG and 60 topics and 200 dimensional word
vectors for BOWV. We also compare our results
with reported results of other topic modeling based
document embedding methods like W T M (Fu
et al., 2016), w2v − LDA (Nguyen et al., 2015),
LDA (Liu and EDU, 2014), T V + M eanW V
(Li et al., 2016a), LT SG (Law et al., 2017),
Gaussian − LDA (Das et al., 2015), T opic2V ec
(Niu et al., 2015), (Moody, 2016) and M vT M (Li
et al., 2016b). Implementation of SCDV and re-
lated experiments is available here 1.

4.2 Text Classiﬁcation

We run multi-class experiments on 20NewsGroup
dataset 2 and multi-label classiﬁcation experi-
ments on Reuters-21578 dataset 3. We use
the script4 for preprocessing the Reuters-21578
dataset. We use LinearSVM for multi-class classi-

Figure 1: Word-topics vector formation.

Figure 2: Sparse Composite Document Vector for-
mation.

Figure 3: Distribution of attribute feature vector
values.

ai =

(cid:40)

ai
0

if |ai| ≥ p
otherwise

100 ∗ t

t =

|amin| + |amax|
2

amin = avgn(mini(ai))

amax = avgn(maxi(ai))

Flowcharts depicting the formation of word-
topics vector and Sparse Composite Document
Vectors are shown in ﬁgure 1 and ﬁgure 2 respec-
tively. Algorithm 1 describes SCDV in detail.

1https://github.com/dheeraj7596/SCDV
2http://qwone.com/∼jason/20Newsgroups/
3www.daviddlewis.com/resources/testcollections/reuters21578/
4 https://gist.github.com/herrfz/7967781

ﬁcation and Logistic regression with OneVsRest
setting for multi-label classiﬁcation in baselines
and SCDV.

For SCDV, we set

the dimension of word-
embeddings to 200, sparsity threshold parameter
to 4% and the number of mixture components in
GMM to 60. All mixture components share the
same spherical co-variance matrix. We learn word
vector embedding using Skip-Gram with Negative
Sampling (SGNS) of 10 and minimum word fre-
quency as 20. We use 5-fold cross-validation on
F1 score to tune parameter C of SVM.

4.2.1 Multi-class classiﬁcation

We evaluate classiﬁer performance using standard
metrics like accuracy, macro-averaging precision,
recall and F-measure. Table 1 shows a compari-
son with the current state-of-art (NTSG) document
representations on the 20Newsgroup dataset. We
observe that SCDV outperforms all other current
models by fair margins. We also present the class-
wise precision and recall for 20Newsgroup on an
almost balanced dataset with SVM over Bag of
Words model and the SCDV embeddings in Table
2 and observe that SCDV improves consistently
over all classes.

Table 1: Performance on multi-class classiﬁcation
(Values in red show best performance, the SCDV
algorithm of this paper)

Model
SCDV
NTSG-1
NTSG-2
BoWV
NTSG-3
LTSG
WTM
w2v-LDA

Acc Prec Rec F-mes
84.6
84.6
81.2
82.6
82.4
82.5
80.9
81.6
81.1
81.9
81.8
82.8
80.0
80.9
76.9
77.7
71.6
TV+MeanWV 72.2
71.6
72.2
80.6
81.5
80.5
81.3
70.0
72.2
81.7
81.9
79.0
79.7
71.4
71.8
74.3
75.4
71.5
72.4

MvTM
TWE-1
lda2Vec
lda
weight-AvgVec
BoW
weight-BOC
PV-DBoW
PV-DM

84.5
81.9
82.8
81.1
81.7
81.8
80.3
77.2
71.5
71.5
80.6
80.4
70.7
81.9
79.0
71.8
74.3
71.5

84.6
82.5
83.7
81.1
83.0
82.4
80.3
77.4
71.8
71.8
81.2
81.4
70.8
81.7
79.5
71.3
74.9
72.1

Table 2: Class-level results on the balanced
20newsgroup dataset.

Class Name
alt.atheism
comp.graphics
comp.os.ms-windows.misc
comp.sys.ibm.pc.hardware
comp.sys.mac.hardware
comp.windows.x
misc.forsale
rec.autos
rec.motorcycles
rec.sport.baseball
rec.sport.hockey
sci.crypt
sci.electronics
sci.med
sci.space
soc.religion.christian
talk.politics.guns
talk.politics.mideast
talk.politics.misc
talk.religion.misc

BoW

SCDV

Pre. Rec. Pre. Rec.
79.5
67.8
77.4
67.1
77.2
77.1
73.5
62.8
85.5
77.4
78.6
83.2
85.9
81.3
90.6
80.7
95.7
92.3
94.7
89.8
99.2
93.3
94.7
92.2
74.9
70.9
88.4
79.3
93.8
90.2
92.3
77.3
90.6
71.7
95.4
91.7
59.7
71.7
57.2
63.2

80.2
75.3
78.6
75.6
83.4
87.6
81.4
91.2
95.4
93.2
96.3
92.5
74.6
91.3
88.5
83.3
72.7
96.2
80.9
73.5

72.1
73.5
66.5
72.4
78.2
73.2
88.2
82.8
87.9
89.2
93.7
86.1
73.3
81.3
88.3
87.9
85.7
76.9
56.5
55.4

4.2.2 Multi-label classiﬁcation

We evaluate multi-label classiﬁcation perfor-
mance using Precision@K, nDCG@k (Bhatia
et al., 2015), Coverage error, Label ranking av-
erage precision score (LRAPS)5 and F1-score.
All measures are extensively used for the multi-
label classiﬁcation task. However, F1-score is
an appropriate metric for multi-label classiﬁca-
tion as it considers label biases when train-test
splits are random. Table 3 show evaluation results
for multi-label text classiﬁcation on the Reuters-
21578 dataset.

4.2.3 Effect of Hyper-Parameters

SCDV has three parameters: the number of clus-
ters, word vector dimension and sparsity threshold
parameter. We vary one parameter by keeping the
other two constant. Performance on varying all
three parameters in shown in Figure 4. We ob-
serve that performance improves as we increase
the number of clusters and saturates at 60. The

5Section 3.3.3.2 of

scikit−learn.org/stable/modules/model evaluation.html

Table 3: Performance on various metrics for multi-label classiﬁcation for Reuters(Values in red show
best performance, the SCDV algorithm of this paper)

LRAPS F1-Score

Model

SCDV
BoWV
TWE-1
PV-DM
PV-DBoW
AvgVec
tﬁdf AvgVec

Prec@1
nDCG@1
94.20
92.90
90.91
87.54
88.78
89.09
89.33

Prec
@5
36.98
36.14
35.49
33.24
34.51
34.73
35.04

nDCG
@5
49.55
48.55
47.54
44.21
46.42
46.48
46.83

Coverage
Error
6.48
8.16
8.16
13.15
11.28
9.67
9.42

93.30
91.46
91.46
86.21
87.43
87.28
87.90

81.75
79.16
79.16
70.24
73.68
71.91
71.97

performance improves until a word vector dimen-
sion of 300 after which it saturates. Similarly,
we observe that the performance improves as we
increase p till 4 after which it declines. At 4%
thresholding, we reduce the storage space by 80%
compared to the dense vectors.

4.3 Topic Coherence

We evaluate the topics generated by GMM cluster-
ing on 20NewsGroup for quantitative and qualita-
tive analysis. Instead of using perplexity (Chang
et al., 2011), which doesn’t correlate with seman-
tic coherence and human judgment of individ-
ual topics, we used the popular topic coherence
(Mimno et al., 2011), (Arora et al., 2013), (Liu
and EDU, 2014) measure. A higher topic coher-
ence score indicates a more coherent topic.

We used Bayes rule to compute the P (wk|ci)
for a given topic ci and given word wj and com-
pute the score of the top 10 words for each topic.

P (wk|ci) =

P (ci|wk)P (wk)
P (ci)

where,

LDA and -92.23 of LTSG. Thus, SCDV creates
more coherent topics than both LDA and LTSG.

Table 4 shows top 10 words of 3 topics from
GM M clustering, LDA model and LT SG model
on 20NewsGroup and SCDV shows higher topic
coherence. Words are ranked based on their prob-
ability distribution in each topic. Our results also
support the qualitative results of (Randhawa et al.,
2016) paper, where k-means was used over word
vectors ﬁnd topics.

4.4 Context-Sensitive Learning

In order to demonstrate the effects of soft clus-
tering (GMM) during SCDV formation, we se-
lect some words (wj) with multiple senses from
20Newsgroup and their soft cluster assignments
to ﬁnd the dominant clusters. We also select top
scoring words (wk) from each cluster (ci) to rep-
resent the meaning of that cluster. Table 5 shows
polysemic words and their dominant clusters with
assignment probabilities. This indicates that using
soft clustering to learn word vectors helps com-
bine multiple senses into a single embedding vec-
tor.

P (ci) =

P (ci|wk)P (wk)

4.5

Information Retrieval

K
(cid:88)

i=1

P (wk) =

#(wk)
i=1 #(wi)

(cid:80)V

Here, #(wk) denotes the number of times word
wk appears in the corpus and V represents vocab-
ulary size.

We calculated the topic coherence score for all
topics for SCDV , LDA and LT SG (Law et al.,
2017). Averaging the score of all 80 topics, GMM
clustering scores -85.23 compared to -108.72 of

(Ai et al., 2016b) used (Mikolov et al., 2013b)’s
paragraph vectors to enhance the basic language
model based retrieval model.
The language
model(LM) probabilities are estimated from the
corpus and smoothed using a Dirichlet prior (Zhai
and Lafferty, 2004).
In (Ai et al., 2016b), this
language model is then interpolated with the para-
graph vector (PV) language model as follows.

P (w|d) = (1 − λ)PLM (w|d) + λPP V (w|d)

Figure 4: Effect of varying number of clusters (left), varying word vector dimension (center) and varying
sparsity parameter (right) on performance for 20NewsGroup with SCDV

Figure 5: Visualization of paragraph vectors(left) and SCDV(right) using t-SNE

where,

PP V (w|d) =

exp( (cid:126)w. (cid:126)d)
i=1 exp( (cid:126)wi.(cid:126)d)

(cid:80)V

and the score for document d and query string Q is
given by

score(q, d) =

P (w)P (w|d)

(cid:88)

w∈Q

language model.

where P (w) is obtained from the unigram query
model and score(q, d) is used to rank documents.
(Ai et al., 2016b) do not directly make use of
paragraph vectors for the retrieval task, but im-
prove the document
To di-
rectly make use of paragraph vectors and make
computations more tractable, we directly inter-
polate the language model query-document score
score(q, d) with the similarity score between the
normalized query and document vectors to gener-
ate scoreP V (q, d), which is then used to rank doc-
uments.

scoreP V (q, d) = (1 − λ)score(q, d) + λ(cid:126)q.(cid:126)d

Directly evaluating the document similarity score
with the query paragraph vector rather than col-
lecting similarity scores for individual words in
the query helps avoid confusion amongst distinct
query topics and makes the interpolation operation
faster. In Table 6, we report Mean Average Pre-
cision(MAP) values for four datasets, Associated
Press 88-89 (topics 51-200), Wall Street Journal
(topics 51-200), San Jose Mercury (topics 51-150)
and Disks 4 & 5 (topics 301-450) in the TREC
collection. We learn λ on a held out set of topics.
We observe consistent improvement in MAP for
all datasets. We marginally improve the MAP re-
ported by (Ai et al., 2016b) on the Robust04 task.
In addition, we also report the improvements in
MAP score when Model based relevance feedback
(Zhai and Lafferty, 2001) is applied over the ini-
tially retrieved results from both models. Again,
we notice a consistent improvement in MAP.

Table 4: Top words of some topics from GMM and LDA on 20NewsGroup for K = 80. Higher score
represent better coherent topics.

GMM
ﬁle
bit
image
ﬁles
color
format
images
jpeg
gif

Topic Image
LTSG
image
jpeg
gif
format
ﬁle
ﬁles
convert
color
formats
program images
-75.66
-67.16

LDA
image
ﬁle
color
gif
jpeg
ﬁle
format
bit
images
quality
-88.79

GMM
heath
study
medical
drug
test
drugs
studies
disease
education
age
-66.91

Topic Health
LTSG
stimulation
diseases
disease
toxin
toxic
newsletter
staff
volume
heaths
aids
-96.98

LDA
doctor
disease
coupons
treatment
pain
medical
day
microorganism
medicine
body
-100.39

GMM
ftp
mail
internet
phone
email
send
opinions
fax
address
box
-77.47

Topic Mail
LTSG
anonymous
faq
send
ftp
mailing
server
mail
alt
archive
email
-78.23

LDA
list
mail
information
internet
send
posting
email
group
news
anonymous
-95.47

Table 5: Words with multiple senses assigned to
multiple clusters with signiﬁcant probabilities

Word
subject:1
subject:2
interest:1
interest:2
break:1
break:2
break:3
unit:1
unit:2

Cluster Words
physics, chemistry, math, science
mail, letter, email, gmail
information, enthusiasm, question
bank, market, ﬁnance, investment
vacation, holiday, trip, spring
encryption, cipher, security, privacy
if, elseif, endif, loop, continue
calculation, distance, mass, length
electronics, KWH, digital, signal

P(ci|wj)
0.27
0.72
0.65
0.32
0.52
0.22
0.23
0.25
0.69

5 Analysis and Discussion

SCDV overcomes several challenges encountered
while training document vectors, which we had
mentioned above.

1. Clustering word-embeddings to discover top-
ics improves performance of classiﬁcation as
Figure 4 (left) indicates, while also gener-
ating coherent clusters of words (Table 4).
Figure 5 shows that clustering gives more
discriminative representations of documents
than paragraph vectors do since it uses K ×
d dimensions while paragraph vectors embed
documents and words in the same space. This
enables SCDV to represent complex docu-
ments.
Fuzzy clustering allows words to
belong to multiple topics, thereby recogniz-
ing polysemic words, as Table 5 indicates.
Thus it mimics the word-context interaction
in NTSG and LTSG.

2. Semantically different words are assigned to
different topics. Moreover, a single docu-
ment can contain words from multiple differ-
ent topics. Instead of a weighted averaging of

word embeddings to form document vectors,
as most of the previous work does, concate-
nating word embeddings for each topic (clus-
ter) avoids merging of semantically different
topics.

3. It is well-known that in higher dimensions,
structural regularizers such as sparsity help
overcome the curse of dimensionality (Wain-
wright, 2014).Figure 3 demonstrates this,
since majority of the features are close to
zero. Sparsity also enables linear SVM to
scale to large dimensions. On 20News-
Groups, BoWV model takes up 1.1 GB while
SCDV takes up only 236MB( 80% decrease).
Since GMM assigns a non-zero probability to
every topic in the word embedding, noise can
accumulate when document vectors are cre-
ated and tip the scales in favor of an unrelated
topic. Sparsity helps to reduce this by zeroing
out very small values of probability.

4. SCDV uses Gaussian Mixture Model (GMM)
while T W E, N T SG and LT SG use LDA
for ﬁnding semantic topics respectively.
GMM time complexity is O(V N T 2) while
that of LDA is O(V 2N T ). Here, V = Vo-
cabulary size, N = number of documents
Since num-
and T = number of topics.
ber of topics T < vocabulary size V, GMM
is faster. Empirically, compared to T W E,
SCDV reduces document vector formation,
training and prediction time signiﬁcantly. Ta-
ble 7 shows training and prediction times for
BoWV, SCDV and TWE models.

Table 6: Mean average precision (MAP) for IR on four IR datasets

Dataset
AP
SJM
WSJ
Robust04

LM LM+SCDV MB
0.2856
0.2105
0.2705
0.2684

0.2742
0.2052
0.2618
0.2516

0.3283
0.2341
0.3027
0.2819

MB + SCDV
0.3395
0.2409
0.3126
0.2933

Table 7: Time Comparison (20NewsGroup) (Val-
ues in red show least time, the SCDV algorithm of
this paper)

Time (sec)
DocVec Formation
Total Training
Total Prediction

BoWV TWE-1 SCDV
700
1250
740
1320
120
780

160
200
25

6 Conclusion

In this paper, we propose a document feature for-
mation technique for topic-based document rep-
resentation. SCDV outperforms state-of-the-art
models in multi-class and multi-label classiﬁca-
tion tasks. SCDV introduces sparsity in document
vectors to handle high dimensionality. Table 7 in-
dicates that SCDV shows considerable improve-
ments in feature formation, training and prediction
times for the 20NewsGroups dataset. We show
that fuzzy GMM clustering on word-vectors lead
to more coherent topic than LDA and can also be
used to detect Polysemic words. SCDV embed-
dings also provide a robust estimation of the query
and document language models, thus improving
the MAP of language model based retrieval sys-
tems. In conclusion, SCDV is simple, efﬁcient and
creates a more accurate semantic representation of
documents.

Acknowledgments

The authors wants to thank Nagarajan Natarajan
(Post-Doc, Microsoft Research, India), Praneeth
Netrapalli (Researcher, Microsoft Research, In-
dia), Raghavendra Udupa (Researcher, Microsoft
Research, India), Prateek Jain (Researcher, Mi-
crosoft Research, India) for encouraging and valu-
able feedback .

References

Qingyao Ai, Liu Yang, Jiafeng Guo, and W Bruce
Croft. 2016a. Analysis of the paragraph vector
model for information retrieval. In Proceedings of
the 2016 ACM on International Conference on the
Theory of Information Retrieval. ACM, pages 133–
142.

Qingyao Ai, Liu Yang, Jiafeng Guo, and W Bruce
Croft. 2016b. Improving language estimation with
the paragraph vector model for ad-hoc retrieval. In
Proceedings of the 39th International ACM SIGIR
conference on Research and Development in Infor-
mation Retrieval. ACM, pages 869–872.

Sanjeev Arora, Rong Ge, Yonatan Halpern, David M
Mimno, Ankur Moitra, David Sontag, Yichen Wu,
and Michael Zhu. 2013. A practical algorithm for
topic modeling with provable guarantees. In ICML
(2). pages 280–288.

Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik
Varma, and Prateek Jain. 2015. Sparse local embed-
dings for extreme multi-label classiﬁcation. In Ad-
vances in Neural Information Processing Systems.
pages 730–738.

David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet allocation.
Journal of Machine Learning Research 3:2003.

Jonathan Chang, Jordan L Boyd-Graber, Sean Gerrish,
Chong Wang, and David M Blei. 2011. Reading tea
leaves: How humans interpret topic models. pages
262–272.

Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015.
Gaussian lda for topic models with word embed-
dings. In ACL (1). pages 795–804.

Xianghua Fu, Ting Wang, Jing Li, Chong Yu, and
Wangwang Liu. 2016. Improving distributed word
representation and topic model by word-topic mix-
ture model. In Proceedings of The 8th Asian Con-
ference on Machine Learning. pages 190–205.

Zellig Harris. 1954. Distributional structure. Word

10:146–162.

Han Kyul Kim, Hyunjoong Kim, and Sungzoon Cho.
Bag-of-concepts: Comprehending docu-
2015.
ment representation through clustering words in dis-
tributed representation. SNU Data Mining Center
12.

Jarvan Law, Hankz Hankui Zhuo, Junhua He, and Erhu
Rong. 2017. Ltsg: Latent topical skip-gram for mu-
tually learning topic model and vector representa-
tions. arXiv preprint arXiv:1702.07117 .

Quoc V Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML.
volume 14, pages 1188–1196.

Shaohua Li, Tat-Seng Chua, Jun Zhu, and Chunyan
Miao. 2016a. Generative topic embedding: a con-
In Proceed-
tinuous representation of documents.
ings of The 54th Annual Meeting of the Association
for Computational Linguistics (ACL).

Ximing Li, Jinjin Chi, Changchun Li, Jihong Ouyang,
and Bo Fu. 2016b. Integrating topic modeling with
word embeddings by mixtures of vmfs. Proceedings
of COLING 2016, the 26th International Confer-
ence on Computational Linguistics: Technical Pa-
pers pages 151–160.

Bing Liu and UIC EDU. 2014. Topic modeling using
topics from many domains, lifelong learning and big
data .

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2015a.
Learning context-sensitive word embeddings with
In IJCAI. pages
neural tensor skip-gram model.
1284–1290.

Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong
In AAAI.

Sun. 2015b. Topical word embeddings.
pages 2418–2424.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems. pages 3111–3119.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In HLT-NAACL. pages 746–
751.

David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 262–272.

Christopher E Moody. 2016. Mixing dirichlet topic
models and word embeddings to make lda2vec.
arXiv preprint arXiv:1605.02019 .

Dat Quoc Nguyen, Richard Billingsley, Lan Du, and
Mark Johnson. 2015. Improving topic models with
latent feature word representations. Transactions
of the Association for Computational Linguistics
3:299–313.

Liqiang Niu, Xinyu Dai, Jianbing Zhang, and Jiajun
Chen. 2015. Topic2vec: learning distributed repre-
sentations of topics. In Asian Language Processing

(IALP), 2015 International Conference on. IEEE,
pages 193–196.

Amitabha Mukerjee Pranjal Singh. 2015. Words are
not equal: Graded weighting model for building
composite document vectors. In Proceedings of the
twelfth International Conference on Natural Lan-
guage Processing (ICON-2015). BSP Books Pvt.
Ltd.

Ramandeep S Randhawa, Parag Jain, and Gagan
Topic modeling using dis-
arXiv preprint

Madan. 2016.
tributed word embeddings.
arXiv:1603.04747 .

Douglas Reynolds. 2015. Gaussian mixture models.

Encyclopedia of biometrics pages 827–832.

Dwaipayan Roy, Debasis Ganguly, Mandar Mitra, and
Gareth JF Jones. 2016. Representing documents and
queries as sets of word embedded vectors for infor-
mation retrieval. arXiv preprint arXiv:1606.07869
.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the conference on empirical
methods in natural language processing (EMNLP).
Citeseer, volume 1631, page 1642.

Harish Karnick Ashendra Bansal Pradhuman Jhala
Product classiﬁcation in e-
Vivek Gupta. 2016.
In Pro-
commerce using distributional semantics.
ceedings of COLING 2016, the 26th International
Conference on Computational Linguistics: Techni-
cal Papers.

Martin J Wainwright. 2014. Structured regularizers for
high-dimensional problems: Statistical and compu-
tational issues. Annual Review of Statistics and Its
Application 1:233–253.

Chris Dyer Wang Ling. 2015. Two/too simple adap-
In Pro-
tations of wordvec for syntax problems.
ceedings of the 50th Annual Meeting of the North
American Association for Computational Linguis-
tics. North American Association for Computational
Linguistics.

Chengxiang Zhai and John Lafferty. 2001. Model-
based feedback in the language modeling approach
to information retrieval. In Proceedings of the tenth
international conference on Information and knowl-
edge management. ACM, pages 403–410.

Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to
information retrieval. ACM Transactions on Infor-
mation Systems (TOIS) 22(2):179–214.

SCDV : Sparse Composite Document Vectors using soft clustering over
distributional representations

Dheeraj Mekala*

IIT Kanpur

Vivek Gupta*
Microsoft Research

Bhargavi Paranjape
Microsoft Research

Harish Karnick
IIT Kanpur

dheerajm@iitk.ac.in {t-vigu,t-bhpara}@microsoft.com hk@iitk.ac.in

7
1
0
2
 
y
a
M
 
2
1
 
 
]
L
C
.
s
c
[
 
 
3
v
8
7
7
6
0
.
2
1
6
1
:
v
i
X
r
a

Abstract

We present a feature vector

forma-
tion technique for documents - Sparse
Composite Document Vector (SCDV) -
which overcomes several shortcomings of
the current distributional paragraph vec-
tor representations that are widely used for
In SCDV, word em-
text representation.
beddings are clustered to capture multiple
semantic contexts in which words occur.
They are then chained together to form
document topic-vectors that can express
complex, multi-topic documents. Through
extensive experiments on multi-class and
multi-label classiﬁcation tasks, we outper-
form the previous state-of-the-art method,
NTSG (Liu et al., 2015a). We also show
that SCDV embeddings perform well on
heterogeneous tasks like Topic Coherence,
context-sensitive Learning and Informa-
tion Retrieval. Moreover, we achieve sig-
niﬁcant reduction in training and predic-
tion times compared to other representa-
tion methods. SCDV achieves best of both
worlds - better performance with lower
time and space complexity.

1

Introduction

Distributed word embeddings represent words as
dense, low-dimensional and real-valued vectors
that can capture their semantic and syntactic prop-
erties. These embeddings are used abundantly
by machine learning algorithms in tasks such as
text classiﬁcation and clustering. Traditional bag-
of-word models that represent words as indices
into a vocabulary don’t account for word ordering
and long-distance semantic relations. Represen-
tations based on neural network language models

*Represents equal contribution

(Mikolov et al., 2013b) can overcome these ﬂaws
and further reduce the dimensionality of the vec-
tors. However, there is a need to extend word em-
beddings to entire paragraphs and documents for
tasks such as document and short-text classiﬁca-
tion.

Representing entire documents in a dense, low-
dimensional space is a challenge. A simple
weighted average of the word embeddings in a
large chunk of text ignores word ordering, while
a parse tree based combination of embeddings
(Socher et al., 2013) can only extend to sentences.
(Le and Mikolov, 2014) trains word and para-
graph vectors to predict context but shares word-
embeddings across paragraphs. However, words
can have different semantic meanings in differ-
ent contexts. Hence, vectors of two documents
that contain the same word in two distinct senses
need to account for this distinction for an ac-
curate semantic representation of the documents.
(Wang Ling, 2015), (Liu et al., 2015a) map word
embeddings to a latent topic space to capture dif-
ferent senses in which words occur. However, they
represent complex documents in the same space
as words, reducing their expressive power. These
methods are also computationally intensive.

In this work, we propose the Sparse Compos-
ite Document Vector(SCDV) representation learn-
ing technique to address these challenges and cre-
ate efﬁcient, accurate and robust semantic repre-
sentations of large texts for document classiﬁca-
tion tasks. SCDV combines syntax and semantics
learnt by word embedding models together with a
latent topic model that can handle different senses
of words, thus enhancing the expressive power of
document vectors. The topic space is learnt efﬁ-
ciently using a soft clustering technique over em-
beddings and the ﬁnal document vectors are made
sparse for reduced time and space complexity in
tasks that consume these vectors.

The remaining part of the paper is organized as
follows. Section 2 discusses related work in docu-
ment representations. Section 3 introduces and ex-
plains SCDV in detail. This is followed by exten-
sive and rigorous experiments together with anal-
ysis in section 4 and 5 respectively.

2 Related Work

(Le and Mikolov, 2014) proposed two models
for distributional representation of a document,
namely, Distributed Memory Model Paragraph
Vectors (PV-DM) and Distributed BoWs para-
graph vectors (PV-DBoW). In PV-DM, the model
is learned to predict the next context word us-
In PV-DBoW,
ing word and paragraph vectors.
the paragraph vector is directly learned to predict
randomly sampled context words. In both mod-
els, word vectors are shared across paragraphs.
While word vectors capture semantics across dif-
ferent paragraphs of the text, documents vectors
are learned over context words generated from the
same paragraph and potentially capture only lo-
cal semantics (Pranjal Singh, 2015). Moreover, a
paragraph vector is embedded in the same space as
word vectors though it can contain multiple top-
ics and words with multiple senses. As a result,
doc2vec (Le and Mikolov, 2014) doesn’t perform
well on Information Retrieval as described in (Ai
et al., 2016a) and (Roy et al., 2016). Consequently,
we expect a paragraph vector to be embedded in a
higher dimensional space.

A paragraph vector also assumes all words con-
tribute equally, both quantitatively (weight) and
qualitatively (meaning). They ignore the impor-
tance and distinctiveness of a word across all doc-
uments (Pranjal Singh, 2015). Mukerjee et al.
(Pranjal Singh, 2015) proposed idf-weighted av-
eraging of word vectors to form document vec-
tors. This method tries to address the above prob-
lem. However, it assumes that all words within
a document belong to the same semantic topic.
Intuitively, a paragraph often has words originat-
ing from several semantically different topics. In
fact, Latent Dirichlet Allocation (Blei et al., 2003)
models a document as a distribution of multiple
topics.

These shortcomings are addressed in three
novel composite document representations called
Topical word embedding (TWE-1,TWE-2 and
TWE-3) by (Liu et al., 2015a). TWE-1 learns word
and topic embeddings by considering each topic as

a pseudo word and builds the topical word embed-
ding for each word-topic assignment. Here, the
interaction between a word and the topic to which
it is assigned is not considered. TWE-2 learns a
topical word embedding for each word-topic as-
signment directly, by considering each word- topic
pair as a pseudo word. Here, the interaction be-
tween a word and its assigned topic is considered
but the vocabulary of pseudo-words blows up. For
each word and each topic, TWE-3 builds distinct
embeddings for the topic and word and concate-
nates them for each word-topic assignment. Here,
the word embeddings are inﬂuenced by the corre-
sponding topic embeddings, making words in the
same topic less discriminative.

(Liu et al., 2015a) proposed an architecture
called Neural tensor skip-gram model (NTSG-1,
NTSG-2, NTSG-3, NTSG-4),
that learns multi-
prototype word embeddings and uses a tensor
layer to model the interaction of words and top-
ics to capture different senses. N T SG outper-
forms other embedding methods like T W E −1 on
the 20 newsgroup data-set by modeling context-
sensitive embeddings in addition to topical-word
embeddings. LT SG (Law et al., 2017) builds on
N T SG by jointly learning the latent topic space
and context-sensitive word embeddings. All three,
T W E, N T SG and LT SG use LDA and suf-
fer from computational issues like large training
time, prediction time and storage space. They
also embed document vectors in the same space
as terms. Other works that harness topic modeling
like W T M (Fu et al., 2016), w2v−LDA (Nguyen
et al., 2015), T V + M eanW V (Li et al., 2016a),
LT SG (Law et al., 2017), Gaussian − LDA
(Das et al., 2015), T opic2V ec (Niu et al., 2015),
(Moody, 2016) and M vT M (Li et al., 2016b) also
suffer from similar issues.

(Vivek Gupta, 2016) proposed a method to form
a composite document vector using word embed-
dings and tf-idf values, called the Bag of Words
Vector (BoWV). In BoW V , each document is rep-
resented by a vector of dimension D = K ∗d+K,
where K is the number of clusters and d is the
dimension of the word embeddings. The core
idea behind BoW V is that semantically different
words belong to different topics and their word
vectors should not be averaged. Further, BoW V
computes inverse cluster frequency of each clus-
ter (icf) by averaging the idf values of its mem-
ber terms to capture the importance of words in

the corpus. However, BoW V does hard clustering
using K-means algorithm, assigning each word to
only one cluster or semantic topic but a word can
belong to multiple topics. For example, the word
apple belongs to topic food as a fruit, and belongs
to topic Information Technology as an IT company.
Moreover, BoW V is a non-sparse, high dimen-
sional continuous vector and suffers from compu-
tational problems like large training time, predic-
tion time and storage requirements.

3 Sparse Composite Document Vectors

In this section, we present the proposed Sparse
Composite Document Vector (SCDV) representa-
tion as a novel document vector learning algo-
rithm. The feature formation algorithm can be di-
vided into three steps.

3.1 Word Vector Clustering

We begin by learning d dimensional word vec-
tor representations for every word in the vocab-
ulary V using the skip-gram algorithm with neg-
ative sampling (SGNS) (Mikolov et al., 2013a).
We then cluster these word embeddings using
the Gaussian Mixture Models(GMM) (Reynolds,
2015) soft clustering technique. The number of
clusters, K, to be formed is a parameter of the
SCDV model. By inducing soft clusters, we en-
sure that each word belongs to every cluster with
some probability P (ck|wi).

p(ck = 1) = πk

p(ck = 1|w) =

πkN (w|µk, Σk)
j=1πjN (w|µj, Σj)

ΣK

3.2 Document Topic-vector Formation

For each word wi, we create K different word-
cluster vectors of d dimensions ( (cid:126)wcvik) by weigh-
ing the word’s embedding with its probability dis-
tribution in the kth cluster, P (ck|wi). We then
concatenate all K word-cluster vectors ( (cid:126)wcvik)
into a K×d dimensional embedding and weigh it
with inverse document frequency of wi to form a
word-topics vector ( (cid:126)wtvi). Finally, for all words
appearing in document Dn, we sum their word-
topic vectors (cid:126)wtvi to obtain the document vector
(cid:126)dvDn.

(cid:126)wcvik = (cid:126)wvi × P (ck|wi)

Algorithm 1: Sparse Composite Document
Vector
Data: Documents Dn, n = 1 . . . N
Result: Document vectors

(cid:126)SCDVDn, n = 1

. . . N

1 Obtain word vector ( (cid:126)wvi), for each word wi;
2 Calculate idf values, idf (wi), i = 1..|V | ;
/* |V | is vocabulary size */

3 Cluster word vectors (cid:126)wv using GMM

clustering into K clusters;

4 Obtain soft assignment P (ck|wi) for word wi

and cluster ck;
/* Loop 5-10 can be
pre-computed

5 for each word wi in vocabulary V do
6

for each cluster ck do

(cid:126)wcvik = (cid:126)wvi × P (ck|wi);

end
(cid:126)wtvi = idf (wi) × (cid:76)K
(cid:126)wcvik ;
/* (cid:76) is concatenation

k=1

10 end
11 for n ∈ (1..N ) do

*/

*/

Initialize document vector
for word wi in Dn do
(cid:126)dvDn += (cid:126)wtvi;

(cid:126)dvDn = (cid:126)0;

end
(cid:126)SCDVDn = make-sparse( (cid:126)dvDn);
/* as mentioned in sec 3

*/

7

8

9

12

13

14

15

16

17 end

(cid:126)wtvi = idf (wi) ×

(cid:126)wcvik

K
(cid:77)

k=1

where, (cid:76) is concatenation

3.3 Sparse Document Vectors

After normalizing the vector, we observed that
(cid:126)dvDn are very close to zero. Fig-
most values in
ure 3 veriﬁes this observation. We utilize this fact
to make the document vector (cid:126)dvDn sparse by zero-
ing attribute values whose absolute value is close
to a threshold (speciﬁed as a parameter), which re-
sults in the Sparse Composite Document Vector
(cid:126)SCDVDn.

In particular, let p be percentage sparsity thresh-
old parameter, ai the value of the ith attribute of
the non-Sparse Composite Document Vector and
n represent the nth document in the training set:

4 Experiments

We perform multiple experiments to show the ef-
fectiveness of SCDV representations for multi-
class and multi-label text classiﬁcation. For all ex-
periments and baselines, we use Intel(R) Xeon(R)
CPU E5-2670 v2 @ 2.50GHz, 40 working cores,
128GB RAM machine with Linux Ubuntu 14.4.
However, we utilize multiple cores only during
Word2Vec training and when we run the one-vs-
rest classiﬁer for Reuters.

4.1 Baselines

We consider the following baselines: Bag-of-
Words (BoW) model (Harris, 1954), Bag of Word
Vector (BoWV) (Vivek Gupta, 2016) model, para-
graph vector models (Le and Mikolov, 2014),
Topical word embeddings (TWE-1) (Liu et al.,
2015b), Neural Tensor Skip-Gram Model (NTSG-
1 to NTSG-3) (Liu et al., 2015a), tf-idf weighted
average word-vector model (Pranjal Singh, 2015)
and weighted Bog of Concepts (weight-BoC)
(Kim et al., 2015), where we build topic-document
vectors by counting the member words in each
topic.

We use the best parameter settings as reported in
all our baselines to generate their results. We use
200 dimensions for tf-idf weighted word-vector
model, 400 for paragraph vector model, 80 top-
ics and 400 dimensional vectors for TWE, NTSG,
LTSG and 60 topics and 200 dimensional word
vectors for BOWV. We also compare our results
with reported results of other topic modeling based
document embedding methods like W T M (Fu
et al., 2016), w2v − LDA (Nguyen et al., 2015),
LDA (Liu and EDU, 2014), T V + M eanW V
(Li et al., 2016a), LT SG (Law et al., 2017),
Gaussian − LDA (Das et al., 2015), T opic2V ec
(Niu et al., 2015), (Moody, 2016) and M vT M (Li
et al., 2016b). Implementation of SCDV and re-
lated experiments is available here 1.

4.2 Text Classiﬁcation

We run multi-class experiments on 20NewsGroup
dataset 2 and multi-label classiﬁcation experi-
ments on Reuters-21578 dataset 3. We use
the script4 for preprocessing the Reuters-21578
dataset. We use LinearSVM for multi-class classi-

Figure 1: Word-topics vector formation.

Figure 2: Sparse Composite Document Vector for-
mation.

Figure 3: Distribution of attribute feature vector
values.

ai =

(cid:40)

ai
0

if |ai| ≥ p
otherwise

100 ∗ t

t =

|amin| + |amax|
2

amin = avgn(mini(ai))

amax = avgn(maxi(ai))

Flowcharts depicting the formation of word-
topics vector and Sparse Composite Document
Vectors are shown in ﬁgure 1 and ﬁgure 2 respec-
tively. Algorithm 1 describes SCDV in detail.

1https://github.com/dheeraj7596/SCDV
2http://qwone.com/∼jason/20Newsgroups/
3www.daviddlewis.com/resources/testcollections/reuters21578/
4 https://gist.github.com/herrfz/7967781

ﬁcation and Logistic regression with OneVsRest
setting for multi-label classiﬁcation in baselines
and SCDV.

For SCDV, we set

the dimension of word-
embeddings to 200, sparsity threshold parameter
to 4% and the number of mixture components in
GMM to 60. All mixture components share the
same spherical co-variance matrix. We learn word
vector embedding using Skip-Gram with Negative
Sampling (SGNS) of 10 and minimum word fre-
quency as 20. We use 5-fold cross-validation on
F1 score to tune parameter C of SVM.

4.2.1 Multi-class classiﬁcation

We evaluate classiﬁer performance using standard
metrics like accuracy, macro-averaging precision,
recall and F-measure. Table 1 shows a compari-
son with the current state-of-art (NTSG) document
representations on the 20Newsgroup dataset. We
observe that SCDV outperforms all other current
models by fair margins. We also present the class-
wise precision and recall for 20Newsgroup on an
almost balanced dataset with SVM over Bag of
Words model and the SCDV embeddings in Table
2 and observe that SCDV improves consistently
over all classes.

Table 1: Performance on multi-class classiﬁcation
(Values in red show best performance, the SCDV
algorithm of this paper)

Model
SCDV
NTSG-1
NTSG-2
BoWV
NTSG-3
LTSG
WTM
w2v-LDA

Acc Prec Rec F-mes
84.6
84.6
81.2
82.6
82.4
82.5
80.9
81.6
81.1
81.9
81.8
82.8
80.0
80.9
76.9
77.7
71.6
TV+MeanWV 72.2
71.6
72.2
80.6
81.5
80.5
81.3
70.0
72.2
81.7
81.9
79.0
79.7
71.4
71.8
74.3
75.4
71.5
72.4

MvTM
TWE-1
lda2Vec
lda
weight-AvgVec
BoW
weight-BOC
PV-DBoW
PV-DM

84.6
82.5
83.7
81.1
83.0
82.4
80.3
77.4
71.8
71.8
81.2
81.4
70.8
81.7
79.5
71.3
74.9
72.1

84.5
81.9
82.8
81.1
81.7
81.8
80.3
77.2
71.5
71.5
80.6
80.4
70.7
81.9
79.0
71.8
74.3
71.5

Table 2: Class-level results on the balanced
20newsgroup dataset.

Class Name
alt.atheism
comp.graphics
comp.os.ms-windows.misc
comp.sys.ibm.pc.hardware
comp.sys.mac.hardware
comp.windows.x
misc.forsale
rec.autos
rec.motorcycles
rec.sport.baseball
rec.sport.hockey
sci.crypt
sci.electronics
sci.med
sci.space
soc.religion.christian
talk.politics.guns
talk.politics.mideast
talk.politics.misc
talk.religion.misc

BoW

SCDV

Pre. Rec. Pre. Rec.
79.5
67.8
77.4
67.1
77.2
77.1
73.5
62.8
85.5
77.4
78.6
83.2
85.9
81.3
90.6
80.7
95.7
92.3
94.7
89.8
99.2
93.3
94.7
92.2
74.9
70.9
88.4
79.3
93.8
90.2
92.3
77.3
90.6
71.7
95.4
91.7
59.7
71.7
57.2
63.2

80.2
75.3
78.6
75.6
83.4
87.6
81.4
91.2
95.4
93.2
96.3
92.5
74.6
91.3
88.5
83.3
72.7
96.2
80.9
73.5

72.1
73.5
66.5
72.4
78.2
73.2
88.2
82.8
87.9
89.2
93.7
86.1
73.3
81.3
88.3
87.9
85.7
76.9
56.5
55.4

4.2.2 Multi-label classiﬁcation

We evaluate multi-label classiﬁcation perfor-
mance using Precision@K, nDCG@k (Bhatia
et al., 2015), Coverage error, Label ranking av-
erage precision score (LRAPS)5 and F1-score.
All measures are extensively used for the multi-
label classiﬁcation task. However, F1-score is
an appropriate metric for multi-label classiﬁca-
tion as it considers label biases when train-test
splits are random. Table 3 show evaluation results
for multi-label text classiﬁcation on the Reuters-
21578 dataset.

4.2.3 Effect of Hyper-Parameters

SCDV has three parameters: the number of clus-
ters, word vector dimension and sparsity threshold
parameter. We vary one parameter by keeping the
other two constant. Performance on varying all
three parameters in shown in Figure 4. We ob-
serve that performance improves as we increase
the number of clusters and saturates at 60. The

5Section 3.3.3.2 of

scikit−learn.org/stable/modules/model evaluation.html

Table 3: Performance on various metrics for multi-label classiﬁcation for Reuters(Values in red show
best performance, the SCDV algorithm of this paper)

LRAPS F1-Score

Model

SCDV
BoWV
TWE-1
PV-DM
PV-DBoW
AvgVec
tﬁdf AvgVec

Prec@1
nDCG@1
94.20
92.90
90.91
87.54
88.78
89.09
89.33

Prec
@5
36.98
36.14
35.49
33.24
34.51
34.73
35.04

nDCG
@5
49.55
48.55
47.54
44.21
46.42
46.48
46.83

Coverage
Error
6.48
8.16
8.16
13.15
11.28
9.67
9.42

93.30
91.46
91.46
86.21
87.43
87.28
87.90

81.75
79.16
79.16
70.24
73.68
71.91
71.97

performance improves until a word vector dimen-
sion of 300 after which it saturates. Similarly,
we observe that the performance improves as we
increase p till 4 after which it declines. At 4%
thresholding, we reduce the storage space by 80%
compared to the dense vectors.

4.3 Topic Coherence

We evaluate the topics generated by GMM cluster-
ing on 20NewsGroup for quantitative and qualita-
tive analysis. Instead of using perplexity (Chang
et al., 2011), which doesn’t correlate with seman-
tic coherence and human judgment of individ-
ual topics, we used the popular topic coherence
(Mimno et al., 2011), (Arora et al., 2013), (Liu
and EDU, 2014) measure. A higher topic coher-
ence score indicates a more coherent topic.

We used Bayes rule to compute the P (wk|ci)
for a given topic ci and given word wj and com-
pute the score of the top 10 words for each topic.

P (wk|ci) =

P (ci|wk)P (wk)
P (ci)

where,

LDA and -92.23 of LTSG. Thus, SCDV creates
more coherent topics than both LDA and LTSG.

Table 4 shows top 10 words of 3 topics from
GM M clustering, LDA model and LT SG model
on 20NewsGroup and SCDV shows higher topic
coherence. Words are ranked based on their prob-
ability distribution in each topic. Our results also
support the qualitative results of (Randhawa et al.,
2016) paper, where k-means was used over word
vectors ﬁnd topics.

4.4 Context-Sensitive Learning

In order to demonstrate the effects of soft clus-
tering (GMM) during SCDV formation, we se-
lect some words (wj) with multiple senses from
20Newsgroup and their soft cluster assignments
to ﬁnd the dominant clusters. We also select top
scoring words (wk) from each cluster (ci) to rep-
resent the meaning of that cluster. Table 5 shows
polysemic words and their dominant clusters with
assignment probabilities. This indicates that using
soft clustering to learn word vectors helps com-
bine multiple senses into a single embedding vec-
tor.

P (ci) =

P (ci|wk)P (wk)

4.5

Information Retrieval

K
(cid:88)

i=1

P (wk) =

#(wk)
i=1 #(wi)

(cid:80)V

Here, #(wk) denotes the number of times word
wk appears in the corpus and V represents vocab-
ulary size.

We calculated the topic coherence score for all
topics for SCDV , LDA and LT SG (Law et al.,
2017). Averaging the score of all 80 topics, GMM
clustering scores -85.23 compared to -108.72 of

(Ai et al., 2016b) used (Mikolov et al., 2013b)’s
paragraph vectors to enhance the basic language
model based retrieval model.
The language
model(LM) probabilities are estimated from the
corpus and smoothed using a Dirichlet prior (Zhai
and Lafferty, 2004).
In (Ai et al., 2016b), this
language model is then interpolated with the para-
graph vector (PV) language model as follows.

P (w|d) = (1 − λ)PLM (w|d) + λPP V (w|d)

Figure 4: Effect of varying number of clusters (left), varying word vector dimension (center) and varying
sparsity parameter (right) on performance for 20NewsGroup with SCDV

Figure 5: Visualization of paragraph vectors(left) and SCDV(right) using t-SNE

where,

PP V (w|d) =

exp( (cid:126)w. (cid:126)d)
i=1 exp( (cid:126)wi.(cid:126)d)

(cid:80)V

and the score for document d and query string Q is
given by

score(q, d) =

P (w)P (w|d)

(cid:88)

w∈Q

language model.

where P (w) is obtained from the unigram query
model and score(q, d) is used to rank documents.
(Ai et al., 2016b) do not directly make use of
paragraph vectors for the retrieval task, but im-
prove the document
To di-
rectly make use of paragraph vectors and make
computations more tractable, we directly inter-
polate the language model query-document score
score(q, d) with the similarity score between the
normalized query and document vectors to gener-
ate scoreP V (q, d), which is then used to rank doc-
uments.

scoreP V (q, d) = (1 − λ)score(q, d) + λ(cid:126)q.(cid:126)d

Directly evaluating the document similarity score
with the query paragraph vector rather than col-
lecting similarity scores for individual words in
the query helps avoid confusion amongst distinct
query topics and makes the interpolation operation
faster. In Table 6, we report Mean Average Pre-
cision(MAP) values for four datasets, Associated
Press 88-89 (topics 51-200), Wall Street Journal
(topics 51-200), San Jose Mercury (topics 51-150)
and Disks 4 & 5 (topics 301-450) in the TREC
collection. We learn λ on a held out set of topics.
We observe consistent improvement in MAP for
all datasets. We marginally improve the MAP re-
ported by (Ai et al., 2016b) on the Robust04 task.
In addition, we also report the improvements in
MAP score when Model based relevance feedback
(Zhai and Lafferty, 2001) is applied over the ini-
tially retrieved results from both models. Again,
we notice a consistent improvement in MAP.

Table 4: Top words of some topics from GMM and LDA on 20NewsGroup for K = 80. Higher score
represent better coherent topics.

GMM
ﬁle
bit
image
ﬁles
color
format
images
jpeg
gif

Topic Image
LTSG
image
jpeg
gif
format
ﬁle
ﬁles
convert
color
formats
program images
-75.66
-67.16

LDA
image
ﬁle
color
gif
jpeg
ﬁle
format
bit
images
quality
-88.79

GMM
heath
study
medical
drug
test
drugs
studies
disease
education
age
-66.91

Topic Health
LTSG
stimulation
diseases
disease
toxin
toxic
newsletter
staff
volume
heaths
aids
-96.98

LDA
doctor
disease
coupons
treatment
pain
medical
day
microorganism
medicine
body
-100.39

GMM
ftp
mail
internet
phone
email
send
opinions
fax
address
box
-77.47

Topic Mail
LTSG
anonymous
faq
send
ftp
mailing
server
mail
alt
archive
email
-78.23

LDA
list
mail
information
internet
send
posting
email
group
news
anonymous
-95.47

Table 5: Words with multiple senses assigned to
multiple clusters with signiﬁcant probabilities

Word
subject:1
subject:2
interest:1
interest:2
break:1
break:2
break:3
unit:1
unit:2

Cluster Words
physics, chemistry, math, science
mail, letter, email, gmail
information, enthusiasm, question
bank, market, ﬁnance, investment
vacation, holiday, trip, spring
encryption, cipher, security, privacy
if, elseif, endif, loop, continue
calculation, distance, mass, length
electronics, KWH, digital, signal

P(ci|wj)
0.27
0.72
0.65
0.32
0.52
0.22
0.23
0.25
0.69

5 Analysis and Discussion

SCDV overcomes several challenges encountered
while training document vectors, which we had
mentioned above.

1. Clustering word-embeddings to discover top-
ics improves performance of classiﬁcation as
Figure 4 (left) indicates, while also gener-
ating coherent clusters of words (Table 4).
Figure 5 shows that clustering gives more
discriminative representations of documents
than paragraph vectors do since it uses K ×
d dimensions while paragraph vectors embed
documents and words in the same space. This
enables SCDV to represent complex docu-
ments.
Fuzzy clustering allows words to
belong to multiple topics, thereby recogniz-
ing polysemic words, as Table 5 indicates.
Thus it mimics the word-context interaction
in NTSG and LTSG.

2. Semantically different words are assigned to
different topics. Moreover, a single docu-
ment can contain words from multiple differ-
ent topics. Instead of a weighted averaging of

word embeddings to form document vectors,
as most of the previous work does, concate-
nating word embeddings for each topic (clus-
ter) avoids merging of semantically different
topics.

3. It is well-known that in higher dimensions,
structural regularizers such as sparsity help
overcome the curse of dimensionality (Wain-
wright, 2014).Figure 3 demonstrates this,
since majority of the features are close to
zero. Sparsity also enables linear SVM to
scale to large dimensions. On 20News-
Groups, BoWV model takes up 1.1 GB while
SCDV takes up only 236MB( 80% decrease).
Since GMM assigns a non-zero probability to
every topic in the word embedding, noise can
accumulate when document vectors are cre-
ated and tip the scales in favor of an unrelated
topic. Sparsity helps to reduce this by zeroing
out very small values of probability.

4. SCDV uses Gaussian Mixture Model (GMM)
while T W E, N T SG and LT SG use LDA
for ﬁnding semantic topics respectively.
GMM time complexity is O(V N T 2) while
that of LDA is O(V 2N T ). Here, V = Vo-
cabulary size, N = number of documents
Since num-
and T = number of topics.
ber of topics T < vocabulary size V, GMM
is faster. Empirically, compared to T W E,
SCDV reduces document vector formation,
training and prediction time signiﬁcantly. Ta-
ble 7 shows training and prediction times for
BoWV, SCDV and TWE models.

Table 6: Mean average precision (MAP) for IR on four IR datasets

Dataset
AP
SJM
WSJ
Robust04

LM LM+SCDV MB
0.2856
0.2105
0.2705
0.2684

0.2742
0.2052
0.2618
0.2516

0.3283
0.2341
0.3027
0.2819

MB + SCDV
0.3395
0.2409
0.3126
0.2933

Table 7: Time Comparison (20NewsGroup) (Val-
ues in red show least time, the SCDV algorithm of
this paper)

Time (sec)
DocVec Formation
Total Training
Total Prediction

BoWV TWE-1 SCDV
700
1250
740
1320
120
780

160
200
25

6 Conclusion

In this paper, we propose a document feature for-
mation technique for topic-based document rep-
resentation. SCDV outperforms state-of-the-art
models in multi-class and multi-label classiﬁca-
tion tasks. SCDV introduces sparsity in document
vectors to handle high dimensionality. Table 7 in-
dicates that SCDV shows considerable improve-
ments in feature formation, training and prediction
times for the 20NewsGroups dataset. We show
that fuzzy GMM clustering on word-vectors lead
to more coherent topic than LDA and can also be
used to detect Polysemic words. SCDV embed-
dings also provide a robust estimation of the query
and document language models, thus improving
the MAP of language model based retrieval sys-
tems. In conclusion, SCDV is simple, efﬁcient and
creates a more accurate semantic representation of
documents.

Acknowledgments

The authors wants to thank Nagarajan Natarajan
(Post-Doc, Microsoft Research, India), Praneeth
Netrapalli (Researcher, Microsoft Research, In-
dia), Raghavendra Udupa (Researcher, Microsoft
Research, India), Prateek Jain (Researcher, Mi-
crosoft Research, India) for encouraging and valu-
able feedback .

References

Qingyao Ai, Liu Yang, Jiafeng Guo, and W Bruce
Croft. 2016a. Analysis of the paragraph vector
model for information retrieval. In Proceedings of
the 2016 ACM on International Conference on the
Theory of Information Retrieval. ACM, pages 133–
142.

Qingyao Ai, Liu Yang, Jiafeng Guo, and W Bruce
Croft. 2016b. Improving language estimation with
the paragraph vector model for ad-hoc retrieval. In
Proceedings of the 39th International ACM SIGIR
conference on Research and Development in Infor-
mation Retrieval. ACM, pages 869–872.

Sanjeev Arora, Rong Ge, Yonatan Halpern, David M
Mimno, Ankur Moitra, David Sontag, Yichen Wu,
and Michael Zhu. 2013. A practical algorithm for
topic modeling with provable guarantees. In ICML
(2). pages 280–288.

Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik
Varma, and Prateek Jain. 2015. Sparse local embed-
dings for extreme multi-label classiﬁcation. In Ad-
vances in Neural Information Processing Systems.
pages 730–738.

David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet allocation.
Journal of Machine Learning Research 3:2003.

Jonathan Chang, Jordan L Boyd-Graber, Sean Gerrish,
Chong Wang, and David M Blei. 2011. Reading tea
leaves: How humans interpret topic models. pages
262–272.

Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015.
Gaussian lda for topic models with word embed-
dings. In ACL (1). pages 795–804.

Xianghua Fu, Ting Wang, Jing Li, Chong Yu, and
Wangwang Liu. 2016. Improving distributed word
representation and topic model by word-topic mix-
ture model. In Proceedings of The 8th Asian Con-
ference on Machine Learning. pages 190–205.

Zellig Harris. 1954. Distributional structure. Word

10:146–162.

Han Kyul Kim, Hyunjoong Kim, and Sungzoon Cho.
Bag-of-concepts: Comprehending docu-
2015.
ment representation through clustering words in dis-
tributed representation. SNU Data Mining Center
12.

Jarvan Law, Hankz Hankui Zhuo, Junhua He, and Erhu
Rong. 2017. Ltsg: Latent topical skip-gram for mu-
tually learning topic model and vector representa-
tions. arXiv preprint arXiv:1702.07117 .

Quoc V Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML.
volume 14, pages 1188–1196.

Shaohua Li, Tat-Seng Chua, Jun Zhu, and Chunyan
Miao. 2016a. Generative topic embedding: a con-
In Proceed-
tinuous representation of documents.
ings of The 54th Annual Meeting of the Association
for Computational Linguistics (ACL).

Ximing Li, Jinjin Chi, Changchun Li, Jihong Ouyang,
and Bo Fu. 2016b. Integrating topic modeling with
word embeddings by mixtures of vmfs. Proceedings
of COLING 2016, the 26th International Confer-
ence on Computational Linguistics: Technical Pa-
pers pages 151–160.

Bing Liu and UIC EDU. 2014. Topic modeling using
topics from many domains, lifelong learning and big
data .

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2015a.
Learning context-sensitive word embeddings with
In IJCAI. pages
neural tensor skip-gram model.
1284–1290.

Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong
In AAAI.

Sun. 2015b. Topical word embeddings.
pages 2418–2424.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems. pages 3111–3119.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In HLT-NAACL. pages 746–
751.

David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 262–272.

Christopher E Moody. 2016. Mixing dirichlet topic
models and word embeddings to make lda2vec.
arXiv preprint arXiv:1605.02019 .

Dat Quoc Nguyen, Richard Billingsley, Lan Du, and
Mark Johnson. 2015. Improving topic models with
latent feature word representations. Transactions
of the Association for Computational Linguistics
3:299–313.

Liqiang Niu, Xinyu Dai, Jianbing Zhang, and Jiajun
Chen. 2015. Topic2vec: learning distributed repre-
sentations of topics. In Asian Language Processing

(IALP), 2015 International Conference on. IEEE,
pages 193–196.

Amitabha Mukerjee Pranjal Singh. 2015. Words are
not equal: Graded weighting model for building
composite document vectors. In Proceedings of the
twelfth International Conference on Natural Lan-
guage Processing (ICON-2015). BSP Books Pvt.
Ltd.

Ramandeep S Randhawa, Parag Jain, and Gagan
Topic modeling using dis-
arXiv preprint

Madan. 2016.
tributed word embeddings.
arXiv:1603.04747 .

Douglas Reynolds. 2015. Gaussian mixture models.

Encyclopedia of biometrics pages 827–832.

Dwaipayan Roy, Debasis Ganguly, Mandar Mitra, and
Gareth JF Jones. 2016. Representing documents and
queries as sets of word embedded vectors for infor-
mation retrieval. arXiv preprint arXiv:1606.07869
.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the conference on empirical
methods in natural language processing (EMNLP).
Citeseer, volume 1631, page 1642.

Harish Karnick Ashendra Bansal Pradhuman Jhala
Product classiﬁcation in e-
Vivek Gupta. 2016.
In Pro-
commerce using distributional semantics.
ceedings of COLING 2016, the 26th International
Conference on Computational Linguistics: Techni-
cal Papers.

Martin J Wainwright. 2014. Structured regularizers for
high-dimensional problems: Statistical and compu-
tational issues. Annual Review of Statistics and Its
Application 1:233–253.

Chris Dyer Wang Ling. 2015. Two/too simple adap-
In Pro-
tations of wordvec for syntax problems.
ceedings of the 50th Annual Meeting of the North
American Association for Computational Linguis-
tics. North American Association for Computational
Linguistics.

Chengxiang Zhai and John Lafferty. 2001. Model-
based feedback in the language modeling approach
to information retrieval. In Proceedings of the tenth
international conference on Information and knowl-
edge management. ACM, pages 403–410.

Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to
information retrieval. ACM Transactions on Infor-
mation Systems (TOIS) 22(2):179–214.

SCDV : Sparse Composite Document Vectors using soft clustering over
distributional representations

Dheeraj Mekala*

IIT Kanpur

Vivek Gupta*
Microsoft Research

Bhargavi Paranjape
Microsoft Research

Harish Karnick
IIT Kanpur

dheerajm@iitk.ac.in {t-vigu,t-bhpara}@microsoft.com hk@iitk.ac.in

7
1
0
2
 
y
a
M
 
2
1
 
 
]
L
C
.
s
c
[
 
 
3
v
8
7
7
6
0
.
2
1
6
1
:
v
i
X
r
a

Abstract

We present a feature vector

forma-
tion technique for documents - Sparse
Composite Document Vector (SCDV) -
which overcomes several shortcomings of
the current distributional paragraph vec-
tor representations that are widely used for
In SCDV, word em-
text representation.
beddings are clustered to capture multiple
semantic contexts in which words occur.
They are then chained together to form
document topic-vectors that can express
complex, multi-topic documents. Through
extensive experiments on multi-class and
multi-label classiﬁcation tasks, we outper-
form the previous state-of-the-art method,
NTSG (Liu et al., 2015a). We also show
that SCDV embeddings perform well on
heterogeneous tasks like Topic Coherence,
context-sensitive Learning and Informa-
tion Retrieval. Moreover, we achieve sig-
niﬁcant reduction in training and predic-
tion times compared to other representa-
tion methods. SCDV achieves best of both
worlds - better performance with lower
time and space complexity.

1

Introduction

Distributed word embeddings represent words as
dense, low-dimensional and real-valued vectors
that can capture their semantic and syntactic prop-
erties. These embeddings are used abundantly
by machine learning algorithms in tasks such as
text classiﬁcation and clustering. Traditional bag-
of-word models that represent words as indices
into a vocabulary don’t account for word ordering
and long-distance semantic relations. Represen-
tations based on neural network language models

*Represents equal contribution

(Mikolov et al., 2013b) can overcome these ﬂaws
and further reduce the dimensionality of the vec-
tors. However, there is a need to extend word em-
beddings to entire paragraphs and documents for
tasks such as document and short-text classiﬁca-
tion.

Representing entire documents in a dense, low-
dimensional space is a challenge. A simple
weighted average of the word embeddings in a
large chunk of text ignores word ordering, while
a parse tree based combination of embeddings
(Socher et al., 2013) can only extend to sentences.
(Le and Mikolov, 2014) trains word and para-
graph vectors to predict context but shares word-
embeddings across paragraphs. However, words
can have different semantic meanings in differ-
ent contexts. Hence, vectors of two documents
that contain the same word in two distinct senses
need to account for this distinction for an ac-
curate semantic representation of the documents.
(Wang Ling, 2015), (Liu et al., 2015a) map word
embeddings to a latent topic space to capture dif-
ferent senses in which words occur. However, they
represent complex documents in the same space
as words, reducing their expressive power. These
methods are also computationally intensive.

In this work, we propose the Sparse Compos-
ite Document Vector(SCDV) representation learn-
ing technique to address these challenges and cre-
ate efﬁcient, accurate and robust semantic repre-
sentations of large texts for document classiﬁca-
tion tasks. SCDV combines syntax and semantics
learnt by word embedding models together with a
latent topic model that can handle different senses
of words, thus enhancing the expressive power of
document vectors. The topic space is learnt efﬁ-
ciently using a soft clustering technique over em-
beddings and the ﬁnal document vectors are made
sparse for reduced time and space complexity in
tasks that consume these vectors.

The remaining part of the paper is organized as
follows. Section 2 discusses related work in docu-
ment representations. Section 3 introduces and ex-
plains SCDV in detail. This is followed by exten-
sive and rigorous experiments together with anal-
ysis in section 4 and 5 respectively.

2 Related Work

(Le and Mikolov, 2014) proposed two models
for distributional representation of a document,
namely, Distributed Memory Model Paragraph
Vectors (PV-DM) and Distributed BoWs para-
graph vectors (PV-DBoW). In PV-DM, the model
is learned to predict the next context word us-
In PV-DBoW,
ing word and paragraph vectors.
the paragraph vector is directly learned to predict
randomly sampled context words. In both mod-
els, word vectors are shared across paragraphs.
While word vectors capture semantics across dif-
ferent paragraphs of the text, documents vectors
are learned over context words generated from the
same paragraph and potentially capture only lo-
cal semantics (Pranjal Singh, 2015). Moreover, a
paragraph vector is embedded in the same space as
word vectors though it can contain multiple top-
ics and words with multiple senses. As a result,
doc2vec (Le and Mikolov, 2014) doesn’t perform
well on Information Retrieval as described in (Ai
et al., 2016a) and (Roy et al., 2016). Consequently,
we expect a paragraph vector to be embedded in a
higher dimensional space.

A paragraph vector also assumes all words con-
tribute equally, both quantitatively (weight) and
qualitatively (meaning). They ignore the impor-
tance and distinctiveness of a word across all doc-
uments (Pranjal Singh, 2015). Mukerjee et al.
(Pranjal Singh, 2015) proposed idf-weighted av-
eraging of word vectors to form document vec-
tors. This method tries to address the above prob-
lem. However, it assumes that all words within
a document belong to the same semantic topic.
Intuitively, a paragraph often has words originat-
ing from several semantically different topics. In
fact, Latent Dirichlet Allocation (Blei et al., 2003)
models a document as a distribution of multiple
topics.

These shortcomings are addressed in three
novel composite document representations called
Topical word embedding (TWE-1,TWE-2 and
TWE-3) by (Liu et al., 2015a). TWE-1 learns word
and topic embeddings by considering each topic as

a pseudo word and builds the topical word embed-
ding for each word-topic assignment. Here, the
interaction between a word and the topic to which
it is assigned is not considered. TWE-2 learns a
topical word embedding for each word-topic as-
signment directly, by considering each word- topic
pair as a pseudo word. Here, the interaction be-
tween a word and its assigned topic is considered
but the vocabulary of pseudo-words blows up. For
each word and each topic, TWE-3 builds distinct
embeddings for the topic and word and concate-
nates them for each word-topic assignment. Here,
the word embeddings are inﬂuenced by the corre-
sponding topic embeddings, making words in the
same topic less discriminative.

(Liu et al., 2015a) proposed an architecture
called Neural tensor skip-gram model (NTSG-1,
NTSG-2, NTSG-3, NTSG-4),
that learns multi-
prototype word embeddings and uses a tensor
layer to model the interaction of words and top-
ics to capture different senses. N T SG outper-
forms other embedding methods like T W E −1 on
the 20 newsgroup data-set by modeling context-
sensitive embeddings in addition to topical-word
embeddings. LT SG (Law et al., 2017) builds on
N T SG by jointly learning the latent topic space
and context-sensitive word embeddings. All three,
T W E, N T SG and LT SG use LDA and suf-
fer from computational issues like large training
time, prediction time and storage space. They
also embed document vectors in the same space
as terms. Other works that harness topic modeling
like W T M (Fu et al., 2016), w2v−LDA (Nguyen
et al., 2015), T V + M eanW V (Li et al., 2016a),
LT SG (Law et al., 2017), Gaussian − LDA
(Das et al., 2015), T opic2V ec (Niu et al., 2015),
(Moody, 2016) and M vT M (Li et al., 2016b) also
suffer from similar issues.

(Vivek Gupta, 2016) proposed a method to form
a composite document vector using word embed-
dings and tf-idf values, called the Bag of Words
Vector (BoWV). In BoW V , each document is rep-
resented by a vector of dimension D = K ∗d+K,
where K is the number of clusters and d is the
dimension of the word embeddings. The core
idea behind BoW V is that semantically different
words belong to different topics and their word
vectors should not be averaged. Further, BoW V
computes inverse cluster frequency of each clus-
ter (icf) by averaging the idf values of its mem-
ber terms to capture the importance of words in

the corpus. However, BoW V does hard clustering
using K-means algorithm, assigning each word to
only one cluster or semantic topic but a word can
belong to multiple topics. For example, the word
apple belongs to topic food as a fruit, and belongs
to topic Information Technology as an IT company.
Moreover, BoW V is a non-sparse, high dimen-
sional continuous vector and suffers from compu-
tational problems like large training time, predic-
tion time and storage requirements.

3 Sparse Composite Document Vectors

In this section, we present the proposed Sparse
Composite Document Vector (SCDV) representa-
tion as a novel document vector learning algo-
rithm. The feature formation algorithm can be di-
vided into three steps.

3.1 Word Vector Clustering

We begin by learning d dimensional word vec-
tor representations for every word in the vocab-
ulary V using the skip-gram algorithm with neg-
ative sampling (SGNS) (Mikolov et al., 2013a).
We then cluster these word embeddings using
the Gaussian Mixture Models(GMM) (Reynolds,
2015) soft clustering technique. The number of
clusters, K, to be formed is a parameter of the
SCDV model. By inducing soft clusters, we en-
sure that each word belongs to every cluster with
some probability P (ck|wi).

p(ck = 1) = πk

p(ck = 1|w) =

πkN (w|µk, Σk)
j=1πjN (w|µj, Σj)

ΣK

3.2 Document Topic-vector Formation

For each word wi, we create K different word-
cluster vectors of d dimensions ( (cid:126)wcvik) by weigh-
ing the word’s embedding with its probability dis-
tribution in the kth cluster, P (ck|wi). We then
concatenate all K word-cluster vectors ( (cid:126)wcvik)
into a K×d dimensional embedding and weigh it
with inverse document frequency of wi to form a
word-topics vector ( (cid:126)wtvi). Finally, for all words
appearing in document Dn, we sum their word-
topic vectors (cid:126)wtvi to obtain the document vector
(cid:126)dvDn.

(cid:126)wcvik = (cid:126)wvi × P (ck|wi)

Algorithm 1: Sparse Composite Document
Vector
Data: Documents Dn, n = 1 . . . N
Result: Document vectors

(cid:126)SCDVDn, n = 1

. . . N

1 Obtain word vector ( (cid:126)wvi), for each word wi;
2 Calculate idf values, idf (wi), i = 1..|V | ;
/* |V | is vocabulary size */

3 Cluster word vectors (cid:126)wv using GMM

clustering into K clusters;

4 Obtain soft assignment P (ck|wi) for word wi

and cluster ck;
/* Loop 5-10 can be
pre-computed

5 for each word wi in vocabulary V do
6

for each cluster ck do

(cid:126)wcvik = (cid:126)wvi × P (ck|wi);

end
(cid:126)wtvi = idf (wi) × (cid:76)K
(cid:126)wcvik ;
/* (cid:76) is concatenation

k=1

10 end
11 for n ∈ (1..N ) do

*/

*/

Initialize document vector
for word wi in Dn do
(cid:126)dvDn += (cid:126)wtvi;

(cid:126)dvDn = (cid:126)0;

end
(cid:126)SCDVDn = make-sparse( (cid:126)dvDn);
/* as mentioned in sec 3

*/

7

8

9

12

13

14

15

16

17 end

(cid:126)wtvi = idf (wi) ×

(cid:126)wcvik

K
(cid:77)

k=1

where, (cid:76) is concatenation

3.3 Sparse Document Vectors

After normalizing the vector, we observed that
(cid:126)dvDn are very close to zero. Fig-
most values in
ure 3 veriﬁes this observation. We utilize this fact
to make the document vector (cid:126)dvDn sparse by zero-
ing attribute values whose absolute value is close
to a threshold (speciﬁed as a parameter), which re-
sults in the Sparse Composite Document Vector
(cid:126)SCDVDn.

In particular, let p be percentage sparsity thresh-
old parameter, ai the value of the ith attribute of
the non-Sparse Composite Document Vector and
n represent the nth document in the training set:

4 Experiments

We perform multiple experiments to show the ef-
fectiveness of SCDV representations for multi-
class and multi-label text classiﬁcation. For all ex-
periments and baselines, we use Intel(R) Xeon(R)
CPU E5-2670 v2 @ 2.50GHz, 40 working cores,
128GB RAM machine with Linux Ubuntu 14.4.
However, we utilize multiple cores only during
Word2Vec training and when we run the one-vs-
rest classiﬁer for Reuters.

4.1 Baselines

We consider the following baselines: Bag-of-
Words (BoW) model (Harris, 1954), Bag of Word
Vector (BoWV) (Vivek Gupta, 2016) model, para-
graph vector models (Le and Mikolov, 2014),
Topical word embeddings (TWE-1) (Liu et al.,
2015b), Neural Tensor Skip-Gram Model (NTSG-
1 to NTSG-3) (Liu et al., 2015a), tf-idf weighted
average word-vector model (Pranjal Singh, 2015)
and weighted Bog of Concepts (weight-BoC)
(Kim et al., 2015), where we build topic-document
vectors by counting the member words in each
topic.

We use the best parameter settings as reported in
all our baselines to generate their results. We use
200 dimensions for tf-idf weighted word-vector
model, 400 for paragraph vector model, 80 top-
ics and 400 dimensional vectors for TWE, NTSG,
LTSG and 60 topics and 200 dimensional word
vectors for BOWV. We also compare our results
with reported results of other topic modeling based
document embedding methods like W T M (Fu
et al., 2016), w2v − LDA (Nguyen et al., 2015),
LDA (Liu and EDU, 2014), T V + M eanW V
(Li et al., 2016a), LT SG (Law et al., 2017),
Gaussian − LDA (Das et al., 2015), T opic2V ec
(Niu et al., 2015), (Moody, 2016) and M vT M (Li
et al., 2016b). Implementation of SCDV and re-
lated experiments is available here 1.

4.2 Text Classiﬁcation

We run multi-class experiments on 20NewsGroup
dataset 2 and multi-label classiﬁcation experi-
ments on Reuters-21578 dataset 3. We use
the script4 for preprocessing the Reuters-21578
dataset. We use LinearSVM for multi-class classi-

Figure 1: Word-topics vector formation.

Figure 2: Sparse Composite Document Vector for-
mation.

Figure 3: Distribution of attribute feature vector
values.

ai =

(cid:40)

ai
0

if |ai| ≥ p
otherwise

100 ∗ t

t =

|amin| + |amax|
2

amin = avgn(mini(ai))

amax = avgn(maxi(ai))

Flowcharts depicting the formation of word-
topics vector and Sparse Composite Document
Vectors are shown in ﬁgure 1 and ﬁgure 2 respec-
tively. Algorithm 1 describes SCDV in detail.

1https://github.com/dheeraj7596/SCDV
2http://qwone.com/∼jason/20Newsgroups/
3www.daviddlewis.com/resources/testcollections/reuters21578/
4 https://gist.github.com/herrfz/7967781

ﬁcation and Logistic regression with OneVsRest
setting for multi-label classiﬁcation in baselines
and SCDV.

For SCDV, we set

the dimension of word-
embeddings to 200, sparsity threshold parameter
to 4% and the number of mixture components in
GMM to 60. All mixture components share the
same spherical co-variance matrix. We learn word
vector embedding using Skip-Gram with Negative
Sampling (SGNS) of 10 and minimum word fre-
quency as 20. We use 5-fold cross-validation on
F1 score to tune parameter C of SVM.

4.2.1 Multi-class classiﬁcation

We evaluate classiﬁer performance using standard
metrics like accuracy, macro-averaging precision,
recall and F-measure. Table 1 shows a compari-
son with the current state-of-art (NTSG) document
representations on the 20Newsgroup dataset. We
observe that SCDV outperforms all other current
models by fair margins. We also present the class-
wise precision and recall for 20Newsgroup on an
almost balanced dataset with SVM over Bag of
Words model and the SCDV embeddings in Table
2 and observe that SCDV improves consistently
over all classes.

Table 1: Performance on multi-class classiﬁcation
(Values in red show best performance, the SCDV
algorithm of this paper)

Model
SCDV
NTSG-1
NTSG-2
BoWV
NTSG-3
LTSG
WTM
w2v-LDA

Acc Prec Rec F-mes
84.6
84.6
81.2
82.6
82.4
82.5
80.9
81.6
81.1
81.9
81.8
82.8
80.0
80.9
76.9
77.7
71.6
TV+MeanWV 72.2
71.6
72.2
80.6
81.5
80.5
81.3
70.0
72.2
81.7
81.9
79.0
79.7
71.4
71.8
74.3
75.4
71.5
72.4

MvTM
TWE-1
lda2Vec
lda
weight-AvgVec
BoW
weight-BOC
PV-DBoW
PV-DM

84.5
81.9
82.8
81.1
81.7
81.8
80.3
77.2
71.5
71.5
80.6
80.4
70.7
81.9
79.0
71.8
74.3
71.5

84.6
82.5
83.7
81.1
83.0
82.4
80.3
77.4
71.8
71.8
81.2
81.4
70.8
81.7
79.5
71.3
74.9
72.1

Table 2: Class-level results on the balanced
20newsgroup dataset.

Class Name
alt.atheism
comp.graphics
comp.os.ms-windows.misc
comp.sys.ibm.pc.hardware
comp.sys.mac.hardware
comp.windows.x
misc.forsale
rec.autos
rec.motorcycles
rec.sport.baseball
rec.sport.hockey
sci.crypt
sci.electronics
sci.med
sci.space
soc.religion.christian
talk.politics.guns
talk.politics.mideast
talk.politics.misc
talk.religion.misc

BoW

SCDV

Pre. Rec. Pre. Rec.
79.5
67.8
77.4
67.1
77.2
77.1
73.5
62.8
85.5
77.4
78.6
83.2
85.9
81.3
90.6
80.7
95.7
92.3
94.7
89.8
99.2
93.3
94.7
92.2
74.9
70.9
88.4
79.3
93.8
90.2
92.3
77.3
90.6
71.7
95.4
91.7
59.7
71.7
57.2
63.2

80.2
75.3
78.6
75.6
83.4
87.6
81.4
91.2
95.4
93.2
96.3
92.5
74.6
91.3
88.5
83.3
72.7
96.2
80.9
73.5

72.1
73.5
66.5
72.4
78.2
73.2
88.2
82.8
87.9
89.2
93.7
86.1
73.3
81.3
88.3
87.9
85.7
76.9
56.5
55.4

4.2.2 Multi-label classiﬁcation

We evaluate multi-label classiﬁcation perfor-
mance using Precision@K, nDCG@k (Bhatia
et al., 2015), Coverage error, Label ranking av-
erage precision score (LRAPS)5 and F1-score.
All measures are extensively used for the multi-
label classiﬁcation task. However, F1-score is
an appropriate metric for multi-label classiﬁca-
tion as it considers label biases when train-test
splits are random. Table 3 show evaluation results
for multi-label text classiﬁcation on the Reuters-
21578 dataset.

4.2.3 Effect of Hyper-Parameters

SCDV has three parameters: the number of clus-
ters, word vector dimension and sparsity threshold
parameter. We vary one parameter by keeping the
other two constant. Performance on varying all
three parameters in shown in Figure 4. We ob-
serve that performance improves as we increase
the number of clusters and saturates at 60. The

5Section 3.3.3.2 of

scikit−learn.org/stable/modules/model evaluation.html

Table 3: Performance on various metrics for multi-label classiﬁcation for Reuters(Values in red show
best performance, the SCDV algorithm of this paper)

LRAPS F1-Score

Model

SCDV
BoWV
TWE-1
PV-DM
PV-DBoW
AvgVec
tﬁdf AvgVec

Prec@1
nDCG@1
94.20
92.90
90.91
87.54
88.78
89.09
89.33

Prec
@5
36.98
36.14
35.49
33.24
34.51
34.73
35.04

nDCG
@5
49.55
48.55
47.54
44.21
46.42
46.48
46.83

Coverage
Error
6.48
8.16
8.16
13.15
11.28
9.67
9.42

93.30
91.46
91.46
86.21
87.43
87.28
87.90

81.75
79.16
79.16
70.24
73.68
71.91
71.97

performance improves until a word vector dimen-
sion of 300 after which it saturates. Similarly,
we observe that the performance improves as we
increase p till 4 after which it declines. At 4%
thresholding, we reduce the storage space by 80%
compared to the dense vectors.

4.3 Topic Coherence

We evaluate the topics generated by GMM cluster-
ing on 20NewsGroup for quantitative and qualita-
tive analysis. Instead of using perplexity (Chang
et al., 2011), which doesn’t correlate with seman-
tic coherence and human judgment of individ-
ual topics, we used the popular topic coherence
(Mimno et al., 2011), (Arora et al., 2013), (Liu
and EDU, 2014) measure. A higher topic coher-
ence score indicates a more coherent topic.

We used Bayes rule to compute the P (wk|ci)
for a given topic ci and given word wj and com-
pute the score of the top 10 words for each topic.

P (wk|ci) =

P (ci|wk)P (wk)
P (ci)

where,

LDA and -92.23 of LTSG. Thus, SCDV creates
more coherent topics than both LDA and LTSG.

Table 4 shows top 10 words of 3 topics from
GM M clustering, LDA model and LT SG model
on 20NewsGroup and SCDV shows higher topic
coherence. Words are ranked based on their prob-
ability distribution in each topic. Our results also
support the qualitative results of (Randhawa et al.,
2016) paper, where k-means was used over word
vectors ﬁnd topics.

4.4 Context-Sensitive Learning

In order to demonstrate the effects of soft clus-
tering (GMM) during SCDV formation, we se-
lect some words (wj) with multiple senses from
20Newsgroup and their soft cluster assignments
to ﬁnd the dominant clusters. We also select top
scoring words (wk) from each cluster (ci) to rep-
resent the meaning of that cluster. Table 5 shows
polysemic words and their dominant clusters with
assignment probabilities. This indicates that using
soft clustering to learn word vectors helps com-
bine multiple senses into a single embedding vec-
tor.

P (ci) =

P (ci|wk)P (wk)

4.5

Information Retrieval

K
(cid:88)

i=1

P (wk) =

#(wk)
i=1 #(wi)

(cid:80)V

Here, #(wk) denotes the number of times word
wk appears in the corpus and V represents vocab-
ulary size.

We calculated the topic coherence score for all
topics for SCDV , LDA and LT SG (Law et al.,
2017). Averaging the score of all 80 topics, GMM
clustering scores -85.23 compared to -108.72 of

(Ai et al., 2016b) used (Mikolov et al., 2013b)’s
paragraph vectors to enhance the basic language
model based retrieval model.
The language
model(LM) probabilities are estimated from the
corpus and smoothed using a Dirichlet prior (Zhai
and Lafferty, 2004).
In (Ai et al., 2016b), this
language model is then interpolated with the para-
graph vector (PV) language model as follows.

P (w|d) = (1 − λ)PLM (w|d) + λPP V (w|d)

Figure 4: Effect of varying number of clusters (left), varying word vector dimension (center) and varying
sparsity parameter (right) on performance for 20NewsGroup with SCDV

Figure 5: Visualization of paragraph vectors(left) and SCDV(right) using t-SNE

where,

PP V (w|d) =

exp( (cid:126)w. (cid:126)d)
i=1 exp( (cid:126)wi.(cid:126)d)

(cid:80)V

and the score for document d and query string Q is
given by

score(q, d) =

P (w)P (w|d)

(cid:88)

w∈Q

language model.

where P (w) is obtained from the unigram query
model and score(q, d) is used to rank documents.
(Ai et al., 2016b) do not directly make use of
paragraph vectors for the retrieval task, but im-
prove the document
To di-
rectly make use of paragraph vectors and make
computations more tractable, we directly inter-
polate the language model query-document score
score(q, d) with the similarity score between the
normalized query and document vectors to gener-
ate scoreP V (q, d), which is then used to rank doc-
uments.

scoreP V (q, d) = (1 − λ)score(q, d) + λ(cid:126)q.(cid:126)d

Directly evaluating the document similarity score
with the query paragraph vector rather than col-
lecting similarity scores for individual words in
the query helps avoid confusion amongst distinct
query topics and makes the interpolation operation
faster. In Table 6, we report Mean Average Pre-
cision(MAP) values for four datasets, Associated
Press 88-89 (topics 51-200), Wall Street Journal
(topics 51-200), San Jose Mercury (topics 51-150)
and Disks 4 & 5 (topics 301-450) in the TREC
collection. We learn λ on a held out set of topics.
We observe consistent improvement in MAP for
all datasets. We marginally improve the MAP re-
ported by (Ai et al., 2016b) on the Robust04 task.
In addition, we also report the improvements in
MAP score when Model based relevance feedback
(Zhai and Lafferty, 2001) is applied over the ini-
tially retrieved results from both models. Again,
we notice a consistent improvement in MAP.

Table 4: Top words of some topics from GMM and LDA on 20NewsGroup for K = 80. Higher score
represent better coherent topics.

GMM
ﬁle
bit
image
ﬁles
color
format
images
jpeg
gif

Topic Image
LTSG
image
jpeg
gif
format
ﬁle
ﬁles
convert
color
formats
program images
-75.66
-67.16

LDA
image
ﬁle
color
gif
jpeg
ﬁle
format
bit
images
quality
-88.79

GMM
heath
study
medical
drug
test
drugs
studies
disease
education
age
-66.91

Topic Health
LTSG
stimulation
diseases
disease
toxin
toxic
newsletter
staff
volume
heaths
aids
-96.98

LDA
doctor
disease
coupons
treatment
pain
medical
day
microorganism
medicine
body
-100.39

GMM
ftp
mail
internet
phone
email
send
opinions
fax
address
box
-77.47

Topic Mail
LTSG
anonymous
faq
send
ftp
mailing
server
mail
alt
archive
email
-78.23

LDA
list
mail
information
internet
send
posting
email
group
news
anonymous
-95.47

Table 5: Words with multiple senses assigned to
multiple clusters with signiﬁcant probabilities

Word
subject:1
subject:2
interest:1
interest:2
break:1
break:2
break:3
unit:1
unit:2

Cluster Words
physics, chemistry, math, science
mail, letter, email, gmail
information, enthusiasm, question
bank, market, ﬁnance, investment
vacation, holiday, trip, spring
encryption, cipher, security, privacy
if, elseif, endif, loop, continue
calculation, distance, mass, length
electronics, KWH, digital, signal

P(ci|wj)
0.27
0.72
0.65
0.32
0.52
0.22
0.23
0.25
0.69

5 Analysis and Discussion

SCDV overcomes several challenges encountered
while training document vectors, which we had
mentioned above.

1. Clustering word-embeddings to discover top-
ics improves performance of classiﬁcation as
Figure 4 (left) indicates, while also gener-
ating coherent clusters of words (Table 4).
Figure 5 shows that clustering gives more
discriminative representations of documents
than paragraph vectors do since it uses K ×
d dimensions while paragraph vectors embed
documents and words in the same space. This
enables SCDV to represent complex docu-
ments.
Fuzzy clustering allows words to
belong to multiple topics, thereby recogniz-
ing polysemic words, as Table 5 indicates.
Thus it mimics the word-context interaction
in NTSG and LTSG.

2. Semantically different words are assigned to
different topics. Moreover, a single docu-
ment can contain words from multiple differ-
ent topics. Instead of a weighted averaging of

word embeddings to form document vectors,
as most of the previous work does, concate-
nating word embeddings for each topic (clus-
ter) avoids merging of semantically different
topics.

3. It is well-known that in higher dimensions,
structural regularizers such as sparsity help
overcome the curse of dimensionality (Wain-
wright, 2014).Figure 3 demonstrates this,
since majority of the features are close to
zero. Sparsity also enables linear SVM to
scale to large dimensions. On 20News-
Groups, BoWV model takes up 1.1 GB while
SCDV takes up only 236MB( 80% decrease).
Since GMM assigns a non-zero probability to
every topic in the word embedding, noise can
accumulate when document vectors are cre-
ated and tip the scales in favor of an unrelated
topic. Sparsity helps to reduce this by zeroing
out very small values of probability.

4. SCDV uses Gaussian Mixture Model (GMM)
while T W E, N T SG and LT SG use LDA
for ﬁnding semantic topics respectively.
GMM time complexity is O(V N T 2) while
that of LDA is O(V 2N T ). Here, V = Vo-
cabulary size, N = number of documents
Since num-
and T = number of topics.
ber of topics T < vocabulary size V, GMM
is faster. Empirically, compared to T W E,
SCDV reduces document vector formation,
training and prediction time signiﬁcantly. Ta-
ble 7 shows training and prediction times for
BoWV, SCDV and TWE models.

Table 6: Mean average precision (MAP) for IR on four IR datasets

Dataset
AP
SJM
WSJ
Robust04

LM LM+SCDV MB
0.2856
0.2105
0.2705
0.2684

0.2742
0.2052
0.2618
0.2516

0.3283
0.2341
0.3027
0.2819

MB + SCDV
0.3395
0.2409
0.3126
0.2933

Table 7: Time Comparison (20NewsGroup) (Val-
ues in red show least time, the SCDV algorithm of
this paper)

Time (sec)
DocVec Formation
Total Training
Total Prediction

BoWV TWE-1 SCDV
700
1250
740
1320
120
780

160
200
25

6 Conclusion

In this paper, we propose a document feature for-
mation technique for topic-based document rep-
resentation. SCDV outperforms state-of-the-art
models in multi-class and multi-label classiﬁca-
tion tasks. SCDV introduces sparsity in document
vectors to handle high dimensionality. Table 7 in-
dicates that SCDV shows considerable improve-
ments in feature formation, training and prediction
times for the 20NewsGroups dataset. We show
that fuzzy GMM clustering on word-vectors lead
to more coherent topic than LDA and can also be
used to detect Polysemic words. SCDV embed-
dings also provide a robust estimation of the query
and document language models, thus improving
the MAP of language model based retrieval sys-
tems. In conclusion, SCDV is simple, efﬁcient and
creates a more accurate semantic representation of
documents.

Acknowledgments

The authors wants to thank Nagarajan Natarajan
(Post-Doc, Microsoft Research, India), Praneeth
Netrapalli (Researcher, Microsoft Research, In-
dia), Raghavendra Udupa (Researcher, Microsoft
Research, India), Prateek Jain (Researcher, Mi-
crosoft Research, India) for encouraging and valu-
able feedback .

References

Qingyao Ai, Liu Yang, Jiafeng Guo, and W Bruce
Croft. 2016a. Analysis of the paragraph vector
model for information retrieval. In Proceedings of
the 2016 ACM on International Conference on the
Theory of Information Retrieval. ACM, pages 133–
142.

Qingyao Ai, Liu Yang, Jiafeng Guo, and W Bruce
Croft. 2016b. Improving language estimation with
the paragraph vector model for ad-hoc retrieval. In
Proceedings of the 39th International ACM SIGIR
conference on Research and Development in Infor-
mation Retrieval. ACM, pages 869–872.

Sanjeev Arora, Rong Ge, Yonatan Halpern, David M
Mimno, Ankur Moitra, David Sontag, Yichen Wu,
and Michael Zhu. 2013. A practical algorithm for
topic modeling with provable guarantees. In ICML
(2). pages 280–288.

Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik
Varma, and Prateek Jain. 2015. Sparse local embed-
dings for extreme multi-label classiﬁcation. In Ad-
vances in Neural Information Processing Systems.
pages 730–738.

David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet allocation.
Journal of Machine Learning Research 3:2003.

Jonathan Chang, Jordan L Boyd-Graber, Sean Gerrish,
Chong Wang, and David M Blei. 2011. Reading tea
leaves: How humans interpret topic models. pages
262–272.

Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015.
Gaussian lda for topic models with word embed-
dings. In ACL (1). pages 795–804.

Xianghua Fu, Ting Wang, Jing Li, Chong Yu, and
Wangwang Liu. 2016. Improving distributed word
representation and topic model by word-topic mix-
ture model. In Proceedings of The 8th Asian Con-
ference on Machine Learning. pages 190–205.

Zellig Harris. 1954. Distributional structure. Word

10:146–162.

Han Kyul Kim, Hyunjoong Kim, and Sungzoon Cho.
Bag-of-concepts: Comprehending docu-
2015.
ment representation through clustering words in dis-
tributed representation. SNU Data Mining Center
12.

Jarvan Law, Hankz Hankui Zhuo, Junhua He, and Erhu
Rong. 2017. Ltsg: Latent topical skip-gram for mu-
tually learning topic model and vector representa-
tions. arXiv preprint arXiv:1702.07117 .

Quoc V Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML.
volume 14, pages 1188–1196.

Shaohua Li, Tat-Seng Chua, Jun Zhu, and Chunyan
Miao. 2016a. Generative topic embedding: a con-
In Proceed-
tinuous representation of documents.
ings of The 54th Annual Meeting of the Association
for Computational Linguistics (ACL).

Ximing Li, Jinjin Chi, Changchun Li, Jihong Ouyang,
and Bo Fu. 2016b. Integrating topic modeling with
word embeddings by mixtures of vmfs. Proceedings
of COLING 2016, the 26th International Confer-
ence on Computational Linguistics: Technical Pa-
pers pages 151–160.

Bing Liu and UIC EDU. 2014. Topic modeling using
topics from many domains, lifelong learning and big
data .

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2015a.
Learning context-sensitive word embeddings with
In IJCAI. pages
neural tensor skip-gram model.
1284–1290.

Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong
In AAAI.

Sun. 2015b. Topical word embeddings.
pages 2418–2424.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems. pages 3111–3119.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In HLT-NAACL. pages 746–
751.

David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 262–272.

Christopher E Moody. 2016. Mixing dirichlet topic
models and word embeddings to make lda2vec.
arXiv preprint arXiv:1605.02019 .

Dat Quoc Nguyen, Richard Billingsley, Lan Du, and
Mark Johnson. 2015. Improving topic models with
latent feature word representations. Transactions
of the Association for Computational Linguistics
3:299–313.

Liqiang Niu, Xinyu Dai, Jianbing Zhang, and Jiajun
Chen. 2015. Topic2vec: learning distributed repre-
sentations of topics. In Asian Language Processing

(IALP), 2015 International Conference on. IEEE,
pages 193–196.

Amitabha Mukerjee Pranjal Singh. 2015. Words are
not equal: Graded weighting model for building
composite document vectors. In Proceedings of the
twelfth International Conference on Natural Lan-
guage Processing (ICON-2015). BSP Books Pvt.
Ltd.

Ramandeep S Randhawa, Parag Jain, and Gagan
Topic modeling using dis-
arXiv preprint

Madan. 2016.
tributed word embeddings.
arXiv:1603.04747 .

Douglas Reynolds. 2015. Gaussian mixture models.

Encyclopedia of biometrics pages 827–832.

Dwaipayan Roy, Debasis Ganguly, Mandar Mitra, and
Gareth JF Jones. 2016. Representing documents and
queries as sets of word embedded vectors for infor-
mation retrieval. arXiv preprint arXiv:1606.07869
.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the conference on empirical
methods in natural language processing (EMNLP).
Citeseer, volume 1631, page 1642.

Harish Karnick Ashendra Bansal Pradhuman Jhala
Product classiﬁcation in e-
Vivek Gupta. 2016.
In Pro-
commerce using distributional semantics.
ceedings of COLING 2016, the 26th International
Conference on Computational Linguistics: Techni-
cal Papers.

Martin J Wainwright. 2014. Structured regularizers for
high-dimensional problems: Statistical and compu-
tational issues. Annual Review of Statistics and Its
Application 1:233–253.

Chris Dyer Wang Ling. 2015. Two/too simple adap-
In Pro-
tations of wordvec for syntax problems.
ceedings of the 50th Annual Meeting of the North
American Association for Computational Linguis-
tics. North American Association for Computational
Linguistics.

Chengxiang Zhai and John Lafferty. 2001. Model-
based feedback in the language modeling approach
to information retrieval. In Proceedings of the tenth
international conference on Information and knowl-
edge management. ACM, pages 403–410.

Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to
information retrieval. ACM Transactions on Infor-
mation Systems (TOIS) 22(2):179–214.

