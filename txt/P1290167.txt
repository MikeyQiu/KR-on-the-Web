Domain Adaptive Text Style Transfer

Dianqi Li1, Yizhe Zhang2, Zhe Gan3, Yu Cheng3,
Chris Brockett2, Ming-Ting Sun1, Bill Dolan2
1University of Washington 2Microsoft Research 3Microsoft Dynamics 365 AI Research
{dianqili,mts}@uw.edu
{Yizhe.Zhang,Zhe.Gan,Yu.Cheng,Chris.Brockett,billdol}@microsoft.com

9
1
0
2
 
g
u
A
 
5
2
 
 
]
L
C
.
s
c
[
 
 
1
v
5
9
3
9
0
.
8
0
9
1
:
v
i
X
r
a

Abstract

Text style transfer without parallel data has
achieved some practical success. However,
in the scenario where less data is available,
these methods may yield poor performance.
In this paper, we examine domain adaptation
for text style transfer to leverage massively
available data from other domains. These data
may demonstrate domain shift, which impedes
the beneﬁts of utilizing such data for training.
To address this challenge, we propose simple
yet effective domain adaptive text style trans-
fer models, enabling domain-adaptive infor-
mation exchange. The proposed models pre-
sumably learn from the source domain to: (i)
distinguish stylized information and generic
content information; (ii) maximally preserve
content information; and (iii) adaptively trans-
fer the styles in a domain-aware manner. We
evaluate the proposed models on two style
transfer tasks (sentiment and formality) over
multiple target domains where only limited
non-parallel data is available. Extensive ex-
periments demonstrate the effectiveness of the
proposed model compared to the baselines.

1

Introduction

Text style transfer, which aims to edit an input
sentence with the desired style while preserving
style-irrelevant content, has received increasing at-
tention in recent years. It has been applied suc-
cessfully to stylized image captioning (Gan et al.,
2017), personalized conversational response gen-
eration (Zhang et al., 2018a), formalized writ-
ing (Rao and Tetreault, 2018), offensive to non-
offensive language transfer (dos Santos et al.,
2018), and other stylized text generation tasks
(Akama et al., 2017; Zhang et al., 2019).

Text style transfer has been explored as a
sequence-to-sequence learning task using parallel
datasets (Jhamtani et al., 2017). However, par-
allel datasets are often not available, and hand-

annotating sentences in different styles is expen-
sive. The recent surge of deep generative mod-
els (Kingma and Welling, 2013; Goodfellow et al.,
2014) has spurred progress in text style trans-
fer without parallel data by learning disentangle-
ment (Hu et al., 2017; Shen et al., 2017; Fu et al.,
2018; Li et al., 2018; Prabhumoye et al., 2018).
These methods typically require massive amounts
of data (Subramanian et al., 2018), and may per-
form poorly in limited data scenarios.

A natural solution to the data-scarcity issue is
to resort to massive data from other domains.
However, directly leveraging abundant data from
other domains is problematic due to the discrep-
ancies in data distribution on different domains.
Different domains generally manifest themselves
in domain-speciﬁc lexica. For example, senti-
ment adjectives such as “delicious”, “tasty”, and
“disgusting” in restaurant reviews might be out
of place in movie reviews, where the sentiment
words such as “imaginative”, “hilarious”, and
“dramatic” are more typical. Domain shift (Gret-
ton et al., 2009) is thus apt to result in feature mis-
alignment.

In this work, we take up the problem of domain
adaptation in scenarios where the target domain
data is scarce and misaligned with the distribution
in the source domain. Our goal is to achieve suc-
cessful style transfer into the target domain, with
the help of the source domain, while the trans-
ferred sentences carry relevant characteristics in
the target domain.

We present two ﬁrst-of-their-kind domain adap-
facilitate
tive text style transfer models that
domain-adaptive information exchange between
the source and target domains. These models
effectively learn generic content information and
distinguish domain-speciﬁc information. Generic
content information, primarily captured by mod-
eling a large corpus from the source domain, fa-

cilitates better content preservation on the target
domain. Meanwhile, domain-speciﬁc informa-
tion, implicitly imposed by domain vectors and
domain-speciﬁc style classiﬁers, underpins the
transferred sentences by generating target-speciﬁc
lexical terms.

Our contributions in this paper are threefold:
(i) We explore a challenging domain adapta-
tion problem for text style transfer by leverag-
ing massively-available data from other domains.
(ii) We introduce simple text style transfer mod-
els that preserve content and meanwhile trans-
late text adaptively into target-domain-speciﬁc
(iii) We demonstrate through exten-
terms.
sive experiments the robustness of these meth-
ods for style transfer tasks (sentiment and formal-
ity) on multiple target domains where only lim-
ited non-parallel data is available. Our implemen-
tation is available at https://github.com/
cookielee77/DAST.

2 Related Work

Text Style Transfer. Text style transfer using
neural networks has been widely studied in the
past few years. A common paradigm is to ﬁrst dis-
entangle latent space as content and style features,
and then generate stylistic sentences by tweak-
ing the style-relevant features and passing through
a decoder. Hu et al. (2017); Fu et al. (2018);
Shen et al. (2017); Yang et al. (2018); Gong et al.
(2019); Lin et al. (2017) explored this direction by
assuming the disentanglement can be achieved in
an auto-encoding procedure with a suitable style
regularization, implemented by either adversarial
discriminators or style classiﬁers. Li et al. (2018);
Xu et al. (2018); Zhang et al. (2018c) achieved dis-
entanglement by ﬁltering the stylistic words of in-
put sentences. Recently, Prabhumoye et al. (2018)
has proposed to use back-translation for text style
transfer with a de-noising auto-encoding objec-
tive (Logeswaran et al., 2018; Subramanian et al.,
2018). Our work differs from the above in that
we leverage domain adaptation to deal with lim-
ited target domain data, whereas previous methods
require massive target domain style-labelled sam-
ples.

Domain Adaptation. Domain adaptation has
been studied in various natural language process-
ing tasks, such as sentiment classiﬁcation (Qu
et al., 2019), dialogue system (Wen et al., 2016),
abstractive summarization (Hua and Wang, 2017;

Zhang et al., 2018b), machine translation (Koehn
and Schroeder, 2007; Axelrod et al., 2011; Sen-
nrich et al., 2016b; Michel and Neubig, 2018), etc.
However, no work has been done for exploring do-
main adaptation on text style transfer. To our best
knowledge, we are the ﬁrst to explore the adapta-
tion of text style transfer models for a new domain
with limited non-parallel data available. The task
requires both style transfer and domain-speciﬁc
generation on the target domain. To differenti-
ate different domains, Sennrich et al. (2016a); Chu
et al. (2017) appended domain tokens to the input
sentences. Our model uses learnable domain vec-
tors combining domain-speciﬁc style classiﬁers,
which force the model to learn distinct stylized in-
formation in each domain.

3 Preliminary

We ﬁrst describe a standard text style transfer ap-
proach, which only considers data in the target
domain. We limit our discussion to the scenario
where only non-parallel data is available, since
large amounts of parallel data is typically not fea-
sible.

Given a set of style-labelled sentences T =
{(xi, li)}N
i=1 in the target domain, the goal is to
transfer sentence xi with style li to a sentence (cid:101)xi
with another style (cid:101)li, where (cid:101)li (cid:54)= li. li, (cid:101)li belong
to a set of style labels lT in the target domain:
li, (cid:101)li ∈ lT . Typically, an encoder encodes the
input xi to a semantic representation ci, while a
decoder controls or modiﬁes the stylistic property
and decodes the sentence (cid:101)xi based on ci and the
pre-speciﬁc style (cid:101)li.

Speciﬁcally, we denote an encoder-decoder
model as (E, D). The semantic representation ci
of sentence xi is extracted by the encoder E, i.e.,
ci = E(xi). The decoder D aims to learn a condi-
tional distribution of (cid:101)xi given the semantic repre-
sentation ci and style (cid:101)li:

pD((cid:101)xi|ci, (cid:101)li) =

pD((cid:101)xt

i|(cid:101)x<t

i

, ci, (cid:101)li),

(1)

T
(cid:89)

t=1

i is the tth token of (cid:101)xi, and (cid:101)x<t

where (cid:101)xt
of (cid:101)xi up to the tth token.

i

is the preﬁx

Directly estimating Eqn. (1) is impractical dur-
ing training due to a lack of parallel data (xi, (cid:101)xi).
Alternatively, the original sentence xi should have
high probability under the conditional distribution
pD(xi|ci, li). Thus, an auto-encoding reconstruc-

tion loss could be formulated as:

LT

ae = − E
xi∼T

log pD(xi|ci, li) .

(2)

Note that we assume that the decoder D recovers
xi’s original stylistic property as accurate as pos-
sible when given the style label li. To achieve text
style transfer, the decoder manipulates the style
of generated sentences by replacing li with a de-
sired style (cid:101)li. Speciﬁcally, the generated sentence
(cid:101)xi is sampled from (cid:101)xi ∼ pD((cid:101)xi|ci, (cid:101)li). How-
ever, by directly optimizing Eqn. (2), the encoder-
decoder model tends to ignoring the style labels
and collapses to a reconstruction model, which
might simply copy the input sentence, hence fails
to transfer the style. To force the model to learn
meaningful style properties, Hu et al. (2017, 2018)
apply a style classiﬁer for the style regularization.
The style classiﬁer ensures the encoder-decoder
model to transfer (cid:101)xi with its correct style label (cid:101)li:

LT

style = −

E
(cid:101)xi∼pD((cid:101)xi|ci,(cid:101)li)

log PCT ((cid:101)li|(cid:101)xi) ,

(3)

where CT is the style classiﬁer pretrained on the
target domain. The overall training objective for
text style transfer within the target domain T is
written as:

LT = LT

ae + LT

style .

(4)

4 Domain Adaptive Text Style Transfer

In this section, we present Domain Adaptive Style
Transfer (DAST) models to perform style transfer
on a target domain by borrowing the strength from
a source domain, while maintaining the transfer to
be domain-speciﬁc.

4.1 Problem Deﬁnition

i, l(cid:48)

i)}N (cid:48)

i=1, T = {(xi, li)}N

Suppose we have two sets of style-labelled sen-
tences S = {(x(cid:48)
i=1 in
the source domain S and the target domain T , re-
i denotes the ith source sentence. l(cid:48)
spectively. x(cid:48)
i
denotes the corresponding style label, which be-
i ∈ lS (e.g.,
l(cid:48)
longs to a source style label set:
positive/negative). l(cid:48)
i can be available or unknown.
Likewise, pair (xi, li) represents the sentence and
style label in the target domain, where li ∈ lT .

We consider domain adaptation in two settings:
(i) the source style lS is unknown, e.g., we may
have a large corpus, such as Yahoo! Answers, but
the underlying style for each sample is not avail-
able; (ii) the source styles are available, and are

the same as the target styles, i.e., lT = lS, e.g.,
both IMDB movie reviews and Yelp restaurant re-
views have the same style classes (negative and
positive sentiments).

In both scenarios, we assume that the target do-
main T only has limited non-parallel data. With
the help of source domain data S, the goal is to
transfer (xi, li) to ((cid:101)xi, (cid:101)li) in the target domain.
The transferred sentence (cid:101)xi should simultaneously
hold: (i) the main content with xi, (ii) a different
style (cid:101)li from li, and (iii) domain-speciﬁc charac-
teristics of the target data distribution T .

4.2 DAST with unknown-stylized source data

In this section, we investigate the case that the
source style lS is unknown. We ﬁrst examine a
drawback of limited target data to motivate our
method. With limited target data, Eqn. (4) may
yield an undesirable transferred text, where the
generated text tends to using the most discrim-
inative words that the target style prefers while
ignoring the content. This is because the classi-
ﬁer CT typically requires less data to train, com-
paring with a sequence autoencoder (E, D). The
classiﬁer objective LT
style thus dominates Eqn. (4),
rendering the generator to bias the sentences with
most representative stylized (e.g., positive or nega-
tive) words rather than preserving the contents (see
Table 5 for examples).

We consider alleviating this issue by leverag-
ing massive source domain data to enhance the
content-preserving ability, though the underlying
styles in the source domain are unknown. By
jointly training an auto-encoder on both the source
and target domain data, the learned generic con-
tent information enables the model to yield better
content preservation on the target domain.

To utilize the source data, we consider that lS
only contains a special unknown-style label lu,
separated from the target style lT . We assume
the semantic representation of the source data c(cid:48)
i
is encoded by the encoder, i.e., c(cid:48)
i). The
decoder takes c(cid:48)
i with style lu to generate the sen-
tences on the source domain. The auto-encoding
reconstruction objective of the source domain is:

i = E(x(cid:48)

LS

ae = − E
x(cid:48)
i∼S

log pD(x(cid:48)

i|c(cid:48)

i, lu),

(5)

where the encoder-decoder model (E, D) is
shared in both domains. Therefore, the corre-
sponding objective can be written as:

LDAST-C = LT

ae + LT

style + LS

ae .

(6)

Figure 1: Illustration of the proposed DAST-C (left) and DAST (right) model. DAST-C learns the generic content
ae on massive source domain data with unknown style lu. For DAST, dT , dS and C T , C S
information through LS
denote domain vectors and domain-speciﬁc style classiﬁers, respectively. Better looked in color.

style.
ae and LS

This can be perceived as combining the source do-
main data with the target domain data to train a
better encoder-decoder framework, while target-
speciﬁc style information on the target domain is
learned through LT
Note that LT

ae are conditional on
lT and lu, which
domain-speciﬁc styles labels:
implicitly encourages the model to learn domain-
speciﬁc features. The decoder could thus generate
target sentences adaptively with lT , while achiev-
ing favorable content preservation with the generic
content information modeled by LS
ae. We refer this
model, which is illustrated in Figure 1(left), as Do-
main Adaptive Style Transfer with generic Content
preservation (DAST-C).

4.3 DAST with stylized source data
We further explore the scenario where lS = lT . In
this case, besides the generic content information,
there is much style information from the source
domain that could be leveraged, e.g., generic styl-
ized expressions like “fantastic” and “terrible” for
sentiment transfer can be applied to both restau-
rant and movie reviews. We thus consider to bor-
row the full strength of the source data, by sharing
learned knowledge on both the generic content and
style information.

A straightforward way to achieve this is to train
Eqn. (4) on both domains. However, simply mix-
ing the two domains together will lead to unde-
sirable style transfers, where the transfer is not
domain-speciﬁc. For example, when adapting the
IMDB movie reviews to the Yelp restaurant re-
views, directly sharing the style transfer model
without specifying the domain will inevitably re-
sult in generations like “The pizza is dramatic!”.
To alleviate this problem, we introduce addi-
tional domain vectors, encouraging the model to
perform style transfer in a domain-aware manner.
The proposed DAST model is illustrated in Fig-

ure 1(right). Consider two domain vectors: dS for
the source domain and dT for the target domain,
respectively. We rewrite the auto-encoding loss as:

LS,T

ae = − E
x(cid:48)
i∼S
− E
xi∼T

log pD(x(cid:48)

i|c(cid:48)

i, dS, l(cid:48)
i)

log pD(xi|ci, dT , li) ,

(7)

learned from the model,

where the encoder-decoder model (E, D) is
shared across domains. The domain vectors, dS,
dT ,
implicitly guide
the decoder to generate sentences with domain-
speciﬁc characteristics. Note that li and l(cid:48)
i are
shared, i.e., lT = lS. This enables the model
to learn generic style information from both do-
mains. On the other hand, explicitly learning pre-
cise stylized information within each domain is
crucial to generate domain-speciﬁc styles. Thus,
two domain-speciﬁc style classiﬁers ensure the
model to learn the corresponding styles by condi-
tioning on (dS, (cid:101)l(cid:48)
i) in the source domain or (dT , (cid:101)li)
in the target domain:

LS,T

style = −

i,dS ,(cid:101)l(cid:48)
i)

E
i∼pD((cid:101)x(cid:48)
i|c(cid:48)
(cid:101)x(cid:48)
E
(cid:101)xi∼pD((cid:101)xi|ci,dT ,(cid:101)li)

−

log PCS ((cid:101)l(cid:48)

i|(cid:101)x(cid:48)
i)

log PCT ((cid:101)li|(cid:101)xi) ,

(8)

where (cid:101)x(cid:48)
i, (cid:101)xi are the transferred sentences with pre-
speciﬁc styles (cid:101)l(cid:48)
i, (cid:101)li in the source and target do-
mains, respectively. The domain-speciﬁc style
classiﬁers, CT and CS, are trained separately on
each domain. The signals from classiﬁers en-
courage the model to learn domain-speciﬁc styles
combining with the domain vectors and style la-
bels. The overall training objective of the pro-
posed DAST model is:

LDAST = LS,T

ae + LS,T

style .

(9)

The domain-speciﬁc style classiﬁers enforce the
model to learn domain-speciﬁc style information

conditioning on (dS, (cid:101)l(cid:48)
i) or (dT , (cid:101)li), which in turn
controls the model to generate sentences with
domain-speciﬁc words. The model can thus dis-
tinguish domain-speciﬁc features, and adaptively
transfer the styles in a domain-aware manner.

5 Experiments

sentiment

We evaluate our proposed models on two
(positive-to-negative
transfer
tasks:
and negative-to-positive), and formality transfer
(informal-to-formal). In both tasks, we make com-
parisons with previous approaches over multiple
target domains. All experiments are conducted on
one Nvidia GTX 1080Ti GPU.

5.1 Dataset

A statistics for the source and target corpora used
in the experiments is summarized in Table 1.

Source

Train

IMDB

344k

Sentiment Transfer

Target
YELP
AMAZON
YAHOO

Train Dev
4k
444k
2k
554k
2k
4k

Formality Transfer

Source
GYAFC

Train
103k

Target
ENRON

Train Dev
500

6k

Test
1k
1k
1k

Test
500

Table 1: Statistics of source and target datasets.

Sentiment Transfer. For the source domain, we
use IMDB movie review corpus (Diao et al.,
2014) by following the ﬁltering and preprocessing
pipelines from Shen et al. (2017). This results in
344k training samples with sentiment labels. For
the target domain, both the Yelp restaurant review
dataset and the Amazon product review dataset are
from Li et al. (2018). For the test sets, we evaluate
our methods by using 1k human-transferred sen-
tences, annotated by Li et al. (2018), on both Yelp
and Amazon datasets. In addition to the two stan-
dard sentiment datasets, we manually collected a
Yahoo sentimental question dataset - 7k question
samples with sentiments from Yahoo! Answers
dataset (Zhang et al., 2015). We split the 7k sen-
timental questions into 4k/2k/1k for train/dev/test
sets, respectively. Note that the Yahoo sentiment
dataset only consists of questions, which have
different domain characteristics with the IMDB
dataset. In all the sentiment experiments, we con-
sider both transfer directions (positive-to-negative
and negative-to-positive).

Formality Transfer. We use the Grammarly’s
Yahoo Answers Formality Corpus (GYAFC) (Rao

and Tetreault, 2018) as the source dataset. The
publicly released version of GYAFC only cov-
ers two topics (Entertainment & Music and Fam-
ily & Relationships), where each topic contains
50k paired informal and formal sentences written
by humans. For the target domain, we use En-
ron email conversation dataset1, which covers sev-
eral different ﬁelds like Business, Politics, Daily
Life, etc. We manually labeled 7k non-parallel
sentences written in either the formal or informal
style. We split the Enron dataset into 6k, 500, 500
samples for training, validation and testing, re-
spectively. Both the validation and test set consist
of mere informal sentences, where the correspond-
ing formal references are annotated by us from a
crowd-sourcing platform for evaluation. We only
assess the informal-to-formal transfer direction in
the formality transfer experiment.

5.2 Evaluation

Automatic Metrics. We evaluate the effective-
ness of our DAST models based on three auto-
matic metrics:
(i) Content Preservation. We assess the con-
tent preservation according to n-gram statistics,
by measuring the BLEU scores (Papineni et al.,
2002) between generated sentences and human
references on the target domain, refered as hu-
man BLEU (hBLEU). When no human reference
is available (e.g., Yahoo), we compute the BLEU
scores with respect to the input sentences.
(ii) Style Control. We generate samples from the
model and measure the style accuracy with a style
classiﬁer that is pre-trained on the target domain.
We refer the style accuracy as S-acc.
(iii) Domain Control. To validate whether the
generated sentences hold the characteristics of the
target domain, we adopt a pre-trained domain clas-
siﬁer to measure the percentage of generated sen-
tences that belong to the target domain. We refer
the domain accuracy as D-acc.

All the pre-trained classiﬁers are implemented
by TextCNN (Kim, 2014; Zhang et al., 2017). The
test accuracy of all these classiﬁers used for evalu-
ation are reported in Appendix A.1. Following Xu
et al. (2018), we also evaluate all methods using
a single uniﬁed metric called G-score, which cal-
culates the geometric mean of style accuracy and
hBLEU.

1https://www.cs.cmu.edu/˜./Enron/

Yelp

Amazon

Model (100% target data)

D-acc

S-acc

hBLEU G-score D-acc

S-acc

hBLEU G-score

CrossAlign (Shen et al., 2017)
Delete&Retrieve (Li et al., 2018)
CycleRL (Xu et al., 2018)
SMAE (Zhang et al., 2018c)
ControlGen (Hu et al., 2018)
Finetune
DAST-C (ours)
DAST (ours)

CrossAlign (Shen et al., 2017)
Delete&Retrieve (Li et al., 2018)
CycleRL (Xu et al., 2018)
SMAE (Zhang et al., 2018c)
ControlGen (Hu et al., 2018)
Finetune
DAST-C (ours)
DAST (ours)

-
-
-
-

96.1
93.8
95.8

-
-
-
-
-
98.1
96.9
97.0

85.0
90.6
88.7
85.1
91.5
91.3
91.7
92.3

76.3
82.1
86.6
96.0
98.5
96.7
90.3
92.6

3.7
14.8
12.3
12.1
25.5
25.6
25.7
26.3

4.8
4.1
1.4
1.2
3.7
13.9
17.8
20.1

8.3
17.9
16.4
15.5
27.4
27.8
27.5
28.9

8.5
7.6
5.2
4.8
8.6
18.5
19.3
23.1

-
-
-
-
-
97.4
96.7
96.9

-
-
-
-
-
96.0
94.8
94.6

23.0
50.9
68.7
71.1
79.0
79.2
81.9
83.0

83.2
63.0
79.5
87.2
83.2
89.2
78.2
82.7

34.1
30.3
14.2
12.9
31.1
34.1
35.7
35.9

2.0
6.9
0.7
0.4
1.9
11.3
20.1
21.0

18.0
25.7
15.5
14.9
30.5
34.3
35.0
35.1

5.9
9.3
3.8
3.2
5.8
14.4
21.6
23.1

Model (1% target data)

D-acc

S-acc

hBLEU G-score D-acc

S-acc

hBLEU G-score

Table 2: Automatic evaluation results on Yelp and Amazon test sets. D-acc and S-acc denote domain accuracy and
style accuracy, respectively. G-score is the geometric mean of S-acc and hBLEU.

Human Evaluation. To accurately evaluate the
quality of transferred sentences, we conduct hu-
man evaluations based on the content preserva-
tion, style control and ﬂuency aspects by follow-
ing Mir et al. (2019). Previous works (Subrama-
nian et al., 2018; Gong et al., 2019) ask workers
to evaluate the quality via a numerical score, how-
ever, we found that this empirically leads to high-
variance results. Instead, we pair transferred sen-
tences from two different models, and ask workers
to choose the sentence they prefer when compared
to the input on each evaluation aspect. We pro-
vide a “No Preference” option to choose when the
workers think the qualities of the two sentences
are indistinguishable. Details of the human eval-
uation instruction are included in Appendix A.3.
For each testing, we randomly sample 100 sen-
tences from the corresponding test set and collect
three human responses for each pair on every eval-
uation aspect, resulting in 2700 responses in total.

5.3 Experimental Setup

The encoder E and the decoder D are imple-
mented by one-layer GRU (Cho et al., 2014) with
hidden dimensions 500 and 700, respectively. The
domain-vector dimension is set to 50. The style
labels are represented by learnable vectors with
150 dimensions. The decoder is initialized by a
concatenation of representations of content, style,
and domain vectors.
If domain vectors are not
used, the dimension of style labels is set to 200;

accordingly, the initialization of the decoder is
a concatenation of content and style representa-
tions. TextCNN (Kim, 2014) is employed for
the domain-speciﬁc style classiﬁers pre-trained on
corresponding domains. After pre-training, the pa-
rameters of the classiﬁers are ﬁxed. We use the
hard-sampling trick (Logeswaran et al., 2018) to
back-propagate the loss through discrete tokens
from the classiﬁer to the encoder-decoder model.
During training, we assign each mini-batch the
same amount of source and target data to balance
the training.

text

style

state-of-the-art

We make an extensive comparison with
ﬁve
transfer
al., 2017),
CrossAlign (Shen et
models:
Delete&Retrieve (Li et al., 2018), CycleRL (Xu
et al., 2018), SMAE (Zhang et al., 2018c) and
ControlGen (Hu et al., 2018). We also experiment
a simple and effective domain adaptation baseline
- Finetune, which is trained with Eqn. (4) on the
source domain and then ﬁne-tuned on the target
domain.

5.4 Results

Model Comparisons. To evaluate the effective-
ness of leveraging massive data from other do-
mains, we compare our proposed DAST mod-
els with previously proposed models trained on
the target domain (Table 2). We observe that by
leveraging massive data from the IMDB dataset,
our models achieve better performance against all

Figure 2: Results on Yelp test set in terms of different percentage of target domain data. 0.1% ≈ 400 samples.

Style Control (Yelp 1% data)

Content Preservation (Yelp 1% data)

Fluency (Yelp 1% data)

Our Model

Neutral

Comparison

Our Model

Neutral

Comparison

Our Model

Neutral

Comparison

DAST 56.2% 30.5% 13.3% ControlGen
DAST 40.5% 42.3% 17.2% DAST-C
DAST 17.9% 18.5% 63.6% Human

DAST 47.0% 48.4% 4.6% ControlGen
DAST 22.4% 65.7% 11.9% DAST-C
DAST 17.7% 47.4% 34.9% Human

DAST 47.1% 40.8% 12.0% ControlGen
DAST 29.1% 55.8% 15.1% DAST-C
DAST 10.1% 30.4% 59.5% Human

Style Control (Enron)

Content Preservation (Enron)

Fluency (Enron)

Our Model

Neutral

Comparison

Our Model

Neutral

Comparison

Our Model

Neutral

Comparison

DAST 74.2% 19.8%
DAST 28.4% 50.2% 21.4% DAST-C
DAST 17.6% 30.5% 51.9% Human

6% ControlGen

DAST 80.8% 14.8% 4.4% ControlGen
DAST 26.8% 48.8% 24.4% DAST-C
DAST 15.3% 36.9% 47.8% Human

DAST 73.8% 20.6% 5.6% ControlGen
DAST 26.9% 51.6% 21.5% DAST-C
DAST 11.6% 36.5% 51.9% Human

Table 3: Results of Human Evaluation for style control, content preservation and ﬂuency, showing preferences
(%) for DAST model vis-a-vis baseline or other comparison systems. Evaluation results of the overall transfer
quality are provided in Appendix A.3.

Yahoo Sentiment Transfer

Model
ControlGen
Finetune
DAST-C
DAST

D-acc
-
97.8
90.7
90.8

S-acc BLEU
99.1
98.8
98.8
99.2

9.7
31.4
35.9
39.2

Table 4: Results on Yahoo sentiment transfer task.

baselines on the sentiment transfer tasks in both
the Yelp and Amazon domains.

Notably, when the target domain has limited
data (1%), all baselines trained on the target do-
mian only completely fail on content preservation.
Finetune preserves better content but experiences
the catastrophic forgetting problem (Goodfellow
et al., 2013) to the source domain information. As
a result, the overall style transfer performance is
still nonoptimal. On the contrary, with the help
of the source domain, DAST obtains consider-
able content preservation performance improve-
ment when compared with other baselines. Our
model also attains favorable performance in terms
of style transferring accuracy (S-acc), resulting in
a good overall G-score.
In general, we observe
that DAST-C is able to better preserve content
information, while DAST further improves both
content preservation and style control abilities.

Additionally, both DAST-C and DAST can adapt
to the target domain, evidenced by the high do-
main accuracy (D-acc). The human evaluation re-
sults (Table 3) show a strong preference of DAST
over DAST-C as well as ControlGen in terms of
style control, content preservation and ﬂuency.

Finally, we evaluate our models on Yahoo senti-
ment transfer task. As can be seen in Table 4, both
DAST and DAST-C achieve successful style trans-
fer even if the target data is formed as questions
which have a large discrepancy with the source
IMDB domain. The samples of Yelp and Yahoo
sentiment transfer are shown in Table 5. We also
investigate the effect of different source domain
data, included in Appendix A.2.

Limiting the Target Domain Data. We further
test the limit of our model by using as few target
domain data as possible. Figure 2 shows the quan-
titative results with different percentages of target
domain training data. When the target domain data
is insufﬁcient, especially less than 10%, the con-
tent preservation ability of the baseline (trained
with target data only) has degenerated rapidly de-
spite a relatively high style transfer accuracy. This
is not desirable because a transferred sentence can
easily have the correct style while barely con-

Input
ControlGen
Finetune
DAST-C
DAST
Human

Input
ControlGen
Finetune
DAST-C
DAST

Input
ControlGen
Finetune
DAST-C
DAST
Human

Yelp (positive-to-negative)
the service was great , food delicious , and the value impeccable .
the service was horrible , service , the service and very frustrated .
the service was poor , food ... , and the experience were .
the service was horrible , food horrible , and the slow sparse .
the service was horrible , food bland , and the value lousy .
service was poor and the food expensive and weak tasting .
Yahoo (positive-to-negative)
who is more romantic ? man or woman ?
which is more stupid ? and or why ?
the is more expensive ? man or woman ?
who is more ugly ? man or woman ?
who is more crazy ? man or woman ?
Enron (informal-to-formal)
ya ’ll need to come visit us in austin .
could we need to look on saturday in enpower .
you will need to go in bed with him .
you will need to visit town .
yes , you will need to visit us in austin .
all of you should come visit us in austin .

Yelp (negative-to-positive)
and the pizza was cold , greasy , and generally quite awful .
and the food was delicious, delicious , and freaking tasty , delicious .
and the pizza was professional , friendly , and always have great .
and the pizza was fresh, greasy , and generally quite cool .
and the pizza was tasty , juicy , and deﬁnitely quite amazing .
the pizza was warm , not greasy , and generally tasted great .
Yahoo (negative-to-positive)
why do stupid questions constantly receive intelligent answers ?
men do fantastic questions constantly receive intelligent bound !
why do great questions read more entertaining answers ?
why do important questions constantly receive intelligent answers ?
why do nice questions constantly receive intelligent answers ?
Enron (informal-to-formal)
are n’t you suppose to be teaching some kids or something ?
are you not supposed to be disloyal some kids or something ?
are you not to be able to be some man or something ?
are not you supposed to be teaching some kids or something ?
are you not supposed to be teaching some children or something ?
are you not supposed to be instructing children ?

Table 5: Transferred sentences on Yelp (1% data), Yahoo and Enron datasets, where red denotes successful style
transfers, blue denotes content losses, and orange denotes grammar errors. Better looked in color.

hBLEU G-score

Model
DAST

D-acc
97.0
w/o d-spec attributes 83.9
w/o d-spec classiﬁers 91.4
73.8
D-acc
97.0
98.1
62.8
96.8

w/o both
Setup
IMDB+Yelp
Finetune
IMDB
Yelp

S-acc
92.6
90.9
83.8
80.6
S-acc
92.6
96.7
59.3
98.5

20.1
20.0
19.0
18.7

20.1
13.9
21.4
3.7

23.1
22.7
20.8
19.9

23.1
18.5
12.2
8.6

hBLEU G-score

Enron Formality Transfer

Model
ControlGen
Finetune
DAST-C
DAST

D-acc
-
91.3
87.6
88.4

S-acc
81.2
81.6
89.2
91.6

hBLEU
4.74
14.7
15.5
16.4

Table 7: Results on Enron formality transfer tasks.

Table 6: Ablation study on Yelp (1%) dataset with help
from IMDB dataset. The results are evaluated on Yelp
test set. d-spec is short for domain-speciﬁc.

domain-aware manner, achieving high domain ac-
curacy all the time.

tains any similar content to the input by retriev-
ing sentences with the target style. Finetune im-
proves content preservation but still suffers the
same problem with fewer target data. Note that
DAST-C is not comparable to Finetune as the pre-
vious one does not use the style information in the
source domain.

On the other hand, both DAST models bring
substantial improvements to content preservation,
and can still successfully manipulate the styles, re-
sulting in consistently higher G-scores. This is
presumably because our models adapt the content
information as well as the style information from
the source domain to consistently sustain the style
transfer on the target domain. By learning both
generic and domain-speciﬁc stylized information,
DAST outperforms DAST-C in terms of content
preservation and style control. Even with 0.1%
target domain data (400 samples), DAST could
still attain reasonable text style transfer, whereas
the model trained on the target data completely
generates sentences in nonsense. Meanwhile,
DAST could keep transferring the sentences in a

Ablation Study. To investigate the effect of in-
dividual components and training setup on the
overall performance, we conduct an ablation study
in Table 6. The domain vectors enable the model
to transfer sentences in a domain-aware manner,
and thus give the largest boost on domain accu-
racy. Without domain-speciﬁc style classiﬁers, the
model mixes the style information on both do-
mains, resulting in worse style control and content
preservation. Additionally, simply increasing the
number of training data (i.e., the row “w/o both”)
improves content preserving, while introducing a
data distribution discrepancy between the training
(Yelp+IMDB) and test data (Yelp), as evidenced
by the lower S-acc and D-acc scores.

In terms of the training setup, the source domain
IMDB mostly helps content preservation, while
accurate style information is mainly learned from
the target domain Yelp. Finetune gives higher S-
acc and D-acc and lower hBLEU due to the catas-
trophic forgetting. Our proposed DAST uses the
source domain data more wisely thus gives bal-
anced results on the style and domain control as
well as content preservation.

Non-parallel Style Transfer with Parallel
Source Data. Finally, to verify the versatility
of our proposed models on different scenarios,
we investigate another domain adaptation setting,
where the source domain data (GYAFC) is par-
allel but the target domain data (Enron) is non-
parallel, on the challenging formality transfer task.
Since parallel data is available in the source do-
main, we can simply add a sequence-to-sequence
loss LS
s2s on source domain data in Eqn. (6) and
Eqn. (9) to help the target domain without paral-
lel data. The training objectives can be written as:
LT
s2s,
respectively. Results are summarized in Table 7.
DAST outperforms other methods on both style
control and content preservation while keeping the
transferred sentences with target-speciﬁc charac-
teristics (D-acc). A strong human preference for
DAST can be observed in Table 3 when compared
to the baselines. Qualitative samples are provided
in Table 5.

s2s and LS,T

ae +LS,T

style+LS

style+LS

ae+LS

ae+LT

6 Conclusion

We present two simple yet effective domain adap-
tive text style transfer models that leverage mas-
sively available data from other domains to facil-
itate the transfer task in the target domain. The
proposed models achieve better content preserva-
tion with the generic information learned from
the source domain and simultaneously distinguish
the domain-speciﬁc information, which enables
the models to transfer text in a domain-adaptive
manner. Extensive experiments demonstrate the
robustness and applicability on various scenarios
where the target data is limited.

Acknowledgments

We would like to thank the reviewers for their con-
structive comments. We thank NVIDIA Corpora-
tion for the donation of the GPU used for this re-
search. We also thank Hao Peng, Tianyi Zhou for
their helpful discussions.

References

Reina Akama, Kazuaki Inada, Naoya Inoue, Sosuke
Kobayashi, and Kentaro Inui. 2017. Generating
stylistically consistent dialog responses with trans-
fer learning. In IJCNLP.

Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In EMNLP.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In EMNLP.

Chenhui Chu, Raj Dabre, and Sadao Kurohashi. 2017.
An empirical comparison of simple domain adapta-
tion methods for neural machine translation. arXiv
preprint arXiv:1701.03214.

Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexan-
der J Smola, Jing Jiang, and Chong Wang. 2014.
Jointly modeling aspects, ratings and sentiments for
movie recommendation (jmars). In SIGKDD.

Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan
Zhao, and Rui Yan. 2018. Style transfer in text: Ex-
ploration and evaluation. In AAAI.

Chuang Gan, Zhe Gan, Xiaodong He, Jianfeng Gao,
and Li Deng. 2017. Stylenet: Generating attractive
visual captions with styles. In CVPR.

Hongyu Gong, Suma Bhat, Lingfei Wu, Jinjun Xiong,
and Wen-mei Hwu. 2019. Reinforcement learning
based text style transfer without parallel training cor-
pus. arXiv preprint arXiv:1903.10671.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In NeurIPS.

Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron
Courville, and Yoshua Bengio. 2013. An em-
investigation of catastrophic forgetting in
pirical
arXiv preprint
gradient-based neural networks.
arXiv:1312.6211.

Arthur Gretton, AJ. Smola, Jiayuan Huang, Mar-
cel Schmittfull, Karsten Borgwardt, and Bernhard
Sch¨olkopf. 2009. Covariate shift and local learning
by distribution matching. MIT Press.

Zhiting Hu, Haoran Shi, Zichao Yang, Bowen Tan,
Tiancheng Zhao, Junxian He, Wentao Wang, Lian-
hui Qin, Di Wang, et al. 2018. Texar: A modular-
ized, versatile, and extensible toolkit for text gener-
ation. arXiv preprint arXiv:1809.00794.

Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P Xing. 2017. Toward con-
trolled generation of text. In ICML.

Xinyu Hua and Lu Wang. 2017. A pilot study of do-
main adaptation effect for neural abstractive summa-
In Proceedings of the Workshop on New
rization.
Frontiers in Summarization.

Harsh Jhamtani, Varun Gangal, Eduard Hovy, and Eric
Nyberg. 2017. Shakespearizing modern language
using copy-enriched sequence to sequence models.
In Proceedings of the Workshop on Stylistic Varia-
tion.

Subramanian,

Lample,
Denoyer,
Eric Michael
Marc’Aurelio Ranzato,
and Y-Lan Boureau.
2018. Multiple-attribute text style transfer. arXiv
preprint arXiv:1811.00552.

Guillaume
Ludovic

Smith,

Tsung-Hsien Wen, Milica Gaˇsi´c, Nikola Mrkˇsi´c,
Lina M Rojas-Barahona, Pei-Hao Su, David
Vandyke, and Steve Young. 2016. Multi-domain
neural network language generation for spoken di-
alogue systems. In NAACL.

Jingjing Xu, Sun Xu, Qi Zeng, Xiaodong Zhang, Xu-
ancheng Ren, Houfeng Wang, and Wenjie Li. 2018.
Unpaired sentiment-to-sentiment translation: A cy-
cled reinforcement learning approach. In ACL.

Zichao Yang, Zhiting Hu, Chris Dyer, Eric P Xing, and
Taylor Berg-Kirkpatrick. 2018. Unsupervised text
style transfer using language models as discrimina-
tors. In NeurIPS.

Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
Szlam, Douwe Kiela, and Jason Weston. 2018a.
Personalizing dialogue agents: I have a dog, do you
have pets too? In ACL.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
siﬁcation. In NeurIPS.

Ye Zhang, Nan Ding, and Radu Soricut. 2018b.
Shaped: Shared-private encoder-decoder for text
style adaptation. In NAACL.

Yi Zhang, Jingjing Xu, Pengcheng Yang, and Xu Sun.
2018c. Learning sentiment memories for sentiment
modiﬁcation without parallel data. In EMNLP.

Yizhe Zhang, Xiang Gao, Sungjin Lee, Chris Brockett,
Michel Galley, Jianfeng Gao, and Bill Dolan. 2019.
Consistent dialogue generation with self-supervised
feature learning. arXiv preprint arXiv:1903.05759.

Yizhe Zhang, Dinghan Shen, Guoyin Wang, Zhe Gan,
Ricardo Henao, and Lawrence Carin. 2017. De-
convolutional paragraph representation learning. In
NeurIPS.

Yoon Kim. 2014. Convolutional neural networks for

Sandeep

sentence classiﬁcation. In EMNLP.

Diederik P Kingma and Max Welling. 2013. Auto-
arXiv preprint

encoding variational bayes.
arXiv:1312.6114.

Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine trans-
In Proceedings of the second workshop on
lation.
statistical machine translation.

Juncen Li, Robin Jia, He He, and Percy Liang. 2018.
Delete, retrieve, generate: a simple approach to sen-
timent and style transfer. In NAACL.

Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang,
and Ming-Ting Sun. 2017. Adversarial ranking for
language generation. In NeurIPS.

Lajanugen Logeswaran, Honglak Lee, and Samy Ben-
gio. 2018. Content preserving text generation with
attribute controls. In NeurIPS.

Paul Michel and Graham Neubig. 2018. Extreme adap-
tation for personalized neural machine translation.
In ACL.

Remi Mir, Bjarke Felbo, Nick Obradovich, and Iyad
Rahwan. 2019. Evaluating style transfer for text.
arXiv preprint arXiv:1904.02295.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL.

Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan
Style

Salakhutdinov, and Alan W Black. 2018.
transfer through back-translation. In ACL.

Xiaoye Qu, Zhikang Zou, Yu Cheng, Yang Yang, and
Pan Zhou. 2019. Adversarial category alignment
network for cross-domain sentiment classiﬁcation.
In NAACL.

Sudha Rao and Joel Tetreault. 2018. Dear sir or
madam, may i introduce the gyafc dataset: Corpus,
benchmarks and metrics for formality style transfer.
In NAACL.

Cicero Nogueira dos Santos, Igor Melnyk, and Inkit
Padhi. 2018. Fighting offensive language on social
media with unsupervised text style transfer. In ACL.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Controlling politeness in neural machine
translation via side constraints. In NAACL.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Improving neural machine translation mod-
els with monolingual data. In ACL.

Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2017. Style transfer from non-parallel text
by cross-alignment. In NeurIPS.

for each pair on every evaluation aspect, yielding
2700 responses in total. Each pair of system out-
puts was randomly presented to 7 crowd-sourced
judges, who indicated their preference for style
control, content preservation and ﬂuency using the
form shown in Figure 3. To minimize the impact
of spamming, we employed the top-ranked 30%
of U.S. workers provided by the crowd-sourcing
In order to make the task less abstract,
service.
following Mir et al. (2019), we asked the judges to
evaluate the content preservation quality indepen-
dently of style information. Detailed task descrip-
tions and examples were also provided to guide
the judges. Inter-rater agreement, as measured by
agreement with the most common judgment was
75.9%.

Besides the style control, content preservation
and ﬂuency evaluated in Table 3, we also asked
each worker to provide a judgement of the overall
quality in terms of three aspects as a whole. Re-
sults are summarized in Table 10. It shows that our
DAST model is better in the overall quality com-
pared to the baselines.

Overall Quality (Yelp 1% data)

Our Model

Neutral

Comparison

DAST
DAST
DAST

81.1% 14.0%
31.4% 43.0%
16.9%

4.9%
25.6%
23.9% 59.2%

ControlGen
DAST-C
human

Overall Quality (Enron)

Our Model

Neutral

Comparison

DAST
DAST
DAST

52.7% 35.3%
34.0% 48.4%
12.0%

12.0% ControlGen
17.6%
17.8% 68.0%

DAST-C
human

Table 10: Results of Human Evaluation in terms of
the overall quality on Yelp sentiment transfer and En-
ron formality transfer tasks.

A Supplementary Material

A.1 Evaluation Classiﬁers

We train the style classiﬁer to classify the styles
on the target domain. The domain classiﬁers are
trained to distinguish the samples from different
domains. After training, all classiﬁers are used for
evaluation only. The test accuracy of evaluation
classiﬁers are reported in Table 8.

Style Classiﬁer

Domain Classiﬁer

Dataset Accuracy

Dataset
IMDB & Yelp

Accuracy
94.8%
Yelp
97.1%
Amazon
Yahoo
86.9%
ENRON 87.0% GYAFC & ENRON 89.7%

97.6%
81.0% IMDB & Amazon
IMDB & Yahoo
99.4%

Table 8: Test accuracy of evaluation classiﬁers.

A.2 Source Domain Data

To investigate the effectiveness of the source do-
main data, we evaluate our proposed models on
different source domains that have unknown styles
or the same styles as Yelp. Results are included in
Table 9. It can be seen that the proposed models
can robustly achieve favorable style transfer with
help of different source domain data. Since DAST-
C model mainly learns the generic content infor-
mation by modeling the large corpus on the source
domain, the number of source training data sig-
niﬁcantly affects the performance, especially on
content preservation (BLEU). On the other hand,
DAST also adapts the generic style information,
the source domain with closer sentiment informa-
tion (IMDB) can thus beneﬁt more to the target do-
main (Yelp) comparing to the TripAdvisor dataset.

Model

Source

# samples D-acc S-acc BLEU

DAST-C

IMDB
Yahoo
GYAFC

DAST

IMDB
TripAdvisor

572K
900k
206k

334k
572k

96.9
90.3
93.5

97.0
86.2

90.3
91.3
92.9

92.6
91.4

17.8
19.6
16.1

20.1
18.4

Table 9: Performance on the Yelp (1% data) dataset
with help of different source domain data.

A.3 Human Evaluation

For each human evaluation on Yelp sentiment
transfer and Enron formality transfer tasks, we
randomly sampled 100 sentences from the cor-
responding test set and collected three responses

Figure 3: Questionnaire used to elicit pairwise judgments from crowd-sourced annotators. Candidate responses
were presented in random order.

