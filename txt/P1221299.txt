Exploring Shared Structures and Hierarchies for Multiple NLP Tasks

Junkun Chen∗, Kaiyu Chen∗, Xinchi Chen, Xipeng Qiu†, Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
{jkchen16, kychen15, xinchichen13, xpqiu, xjhuang}@fudan.edu.cn

8
1
0
2
 
g
u
A
 
3
2
 
 
]
I

A
.
s
c
[
 
 
1
v
8
5
6
7
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

Designing shared neural architecture plays an important role
in multi-task learning. The challenge is that ﬁnding an op-
timal sharing scheme relies heavily on the expert knowledge
and is not scalable to a large number of diverse tasks. Inspired
by the promising work of neural architecture search (NAS),
we apply reinforcement learning to automatically ﬁnd possi-
ble shared architecture for multi-task learning. Speciﬁcally,
we use a controller to select from a set of shareable modules
and assemble a task-speciﬁc architecture, and repeat the same
procedure for other tasks. The controller is trained with rein-
forcement learning to maximize the expected accuracies for
all tasks. We conduct extensive experiments on two types of
tasks, text classiﬁcation and sequence labeling, which demon-
strate the beneﬁts of our approach.

Introduction
Multi-task Learning (MTL) is an essential technique to im-
prove the performance of a single task by sharing the ex-
periences of other related tasks. A fundamental aspect of
MTL is designing the shared schemes. This is a challeng-
ing “meta-task” of itself, often requires expert knowledge of
the various NLP tasks at hand.

Figure 1 shows, broadly, the categories of existing MTL

schemes:

1. Hard-sharing: All tasks share a common set of modules
before fanning out to their privates “heads”. This is a basic
shared strategy and an efﬁcient approach to ﬁght overﬁt-
ting (Baxter 1997).

2. Soft-sharing: In this scheme, modules of each task are
shareable and can contribute to different tasks. Conse-
quently, the input to a task’s certain module is a weighted
sum of the outputs from depending modules of all tasks;
the weights can differ across tasks (Misra et al. 2016).
This scheme is much more ﬂexible.

3. Hierarchical-sharing When the tasks are related but be-
long to different stages in a pipeline or general data ﬂow
graph, the sharing scheme should have a hierarchical
structure, where lower level tasks branching off earlier to
feed to their private heads (Søgaard and Goldberg 2016).
This scheme is very suitable for NLP tasks since most of
NLP tasks are related and there is an implicit hierarchy

∗ Equal contribution
† Corresponding Author

(a) Hard

(b) Soft

(c) Hierarchical

(d) This work

Figure 1: Sharing schemes of multi-task learning. Modules
connected by dashed lines in (1d) are identical, selected
from a shared module pool.

among their representations. A well-designed hierarchi-
cal sharing scheme can boost the performance of all tasks
signiﬁcantly (Hashimoto et al. 2017).
Despite their success, there are still two challenging is-

sues.

The ﬁrst issue is that judging relatedness of two tasks is
not easy, and it is even more difﬁcult of when number of
tasks scale. For instance, consider three classiﬁcation tasks
for movie reviews, product reviews, and spam detection. A
preferred option is sharing information between movie re-
views and product reviews classiﬁcation, but not spam de-
tection. However, it is hard to explore all conﬁgurations
when number of tasks become large. An improper sharing
scheme design may cause “negative transfer”, a severe prob-
lem in MTL and transfer learning. Besides, it suffers from a
problem of “architecture engineering”. Once datasets alter,
we need to redesign and explore a new sharing scheme. This
is taxing even for experts.

The second issue is how to design a hierarchical-sharing

Figure 2: The architecture of proposed model. The blue modules are shared between different tasks. The RNN-based controller
consecutively selects Ma1, Ma2, · · · , Malk
for task k. at is the action of step t generated by controller, indicating the index of
selected modules. Finally, the controller generates a Stop action, terminating the selection process. P k denotes to the private
module of task k. ⊕ is a concatenation operation. The reward of the controller is the performance of the task k under the selected
architecture of the shared layers.

scheme for multi-level tasks. For instance, many tasks in
NLP are related but not belonging to the same level, such
as part-of-speech (POS) tagging, named entity recogni-
tion (NER), semantic role labeling (SRL), etc. Thus, the
hierarchical-sharing scheme should be adopted. However, it
is non-trivial to choose the proper sharing structure manu-
ally especially facing a large set of tasks according to the
expert knowledge. It also suffers from the problems of “neg-
ative transfer” and “architecture engineering”.

To address the above two challenging issues for MTL, in
this paper, we learn a controller by reinforcement learning
to automatically design the shared neural architecture for a
group of tasks, inspired by a recent promising work, neural
architecture search (NAS) (Zoph and Le 2016). Speciﬁcally,
there is a pool of sharable modules, where one or multiple
modules can be integrated into an end-to-end task-speciﬁc
network. As shown in Figure 1d, for each task, the con-
troller chooses one or multiple modules from sharable mod-
ule pool, stacks them in proper order, and then integrates
them into a task-speciﬁc network. The controller is trained
to maximize the expected accuracies of its designed archi-
tectures for all the tasks in MTL. Additionally, we also pro-
pose a novel training strategy, namely simultaneous on-line
update strategy, which accelerates the training procedure.

The beneﬁts of our approach could be summarized as fol-

lows.

• Since not all the sharable modules are used in a task-
speciﬁc network, the related tasks can share overlapping
modules, whereas the unrelated tasks keep their private
modules.

• The controller can stack the chosen modules in various
orders. Therefore its designed architecture can handle the

multi-level tasks. One shared module can posit at low-
layer for one high-level task, but at high-layer for another
low-level task, therefore a hierarchical-sharing structure
is implicitly constructed.

• Overall, our approach provides a ﬂexible solution to the
problem of designing shared architecture for MTL and
can handle complicated correlation and dependency re-
lationship among multiple tasks.

Model
Usually, a multi-task framework contains shared layers and
task-speciﬁc layers. The proposed model gives a mechanism
for automatically learning the structure of the shared layers
for each task. The model contains L shared modules: M =
{M1, M2, · · · , ML}, and a RNN-based controller will select
modules for each task. Figure 2 gives an illustration.

the shared layers for

Training Controller with REINFORCE
task k is se-
The structure of
lected by a RNN-based controller (a list of actions ak =
{a1, a2, · · · , alk }), which takes the task embedding of
ET (k) of task k as the input. The chosen layers stacked one
by one form the shared part of the model for the speciﬁc task
k. And performance under the chosen structure gives the re-
ward feedback Rk for the controller. Thus, we can use re-
inforcement learning to train the controller. Concretely, our
objective is to ﬁnd the optimal with policy πφ(ET (k), ak)
by maximizing the expected reward Jk(φ):

Jk(φ) = Eπφ(ET (k),ak)[Rk(ak; θk)].
(1)
Since the Rk(·) is non-differentiable, we need to use a policy
gradient method to optimize φ, thus the derivative of Jk(φ)
is:

for each batch do

for each task k do

Drawn a1, a2, · · · , aN ∼ πφ(ET (k))
for each a ∈ {a1, a2, · · · , aN } do
Update θ ← θ + α∇θRk(a; θ)

Algorithm 1 Training with Simultaneous On-line Update Strategy
1: repeat
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: until θ and φ convergence

end for
Calculate N rewards Rk = {Rk(a1), Rk(a2), · · · , Rk(aN )}
Normalize rewards ¯Rk = softmax(Rk, τ )
Update φ ← φ + α∇φ ¯Rk

end for

end for

// Sample N structures with policy πφ(ET (k))

// Update the parameters of shared modules

// Rk is calculated by Eq (4)
// Eq (5)
// Update the parameters of controller

∇φJk(φ) =

E[∇φ logπφ(ET (k), at)Rk(ak; θk)].

(2)

lk(cid:88)

t=1

In this paper, we use LSTM to model πφ(ET (k), at),
where at indicates a selection among L shared modules M.

Reward Function
The reward is the performance of the predicted architec-
i }Nk
ture on task k. Speciﬁcally, given training set {xk
i , yk
i=1
of task k, we ﬁrstly map the input sentence xk
to ek
i
i
with word embeddings. Assuming that the predicted actions
are ak = {a1, a2, · · · , alk } and the selected modules are
Mak = {Ma1, Ma2 , · · · , Malk
P k
−−→ p(yk
Nk(cid:88)

}, the reward Rk(ak; θ) is:

Mak−−−→ sk

i |xk
i )

ek
i

(3)

i

log p(yk

i |xk

i ).

(4)

Rk(ak; θ) =

1
Nk

i=1

In this paper, modules are Bi-LSTM layers.

Training with Simultaneous On-line Update
Strategy
Our model is composed of two parts. The learning modules
focus on processing textual data for each task given the ar-
chitecture. The controller aims to ﬁnd a better structure for
learning modules to achieve higher performance.

These two parts are trained together with our on-line up-
date strategy demonstrated in Algorithm 1, which means we
simultaneously train these two parts at each batch. It is dif-
ferent from previous work, where the controller updates it-
self only when the multi-task learning procedure completed.
A signiﬁcant advantage of simultaneous training strategy is
its high efﬁciency compared with previous training strategy.
In implementing on-line update strategy, we have three

major components as follows.

Multi-sampling of Architecture Controller As the am-
plitudes and distributions of rewards of each task are prob-
ably distinct, rewards can only be privately used for indi-
vidual tasks, causing a problem of insufﬁcient rewards. To

solve the problem and help the controller distinguish the
performance of different architectures, we sample N times
of rewards for each single training instance. These rewards
are subsequently used for training the controller after reward
normalization.

k , R(2)

Reward Normalization Given N rewards Rk =
{R(1)
k } for task k, ¯Rk could be ﬁnally de-
rived by normalizing these rewards with a parameterized
softmax function (Norouzi et al. 2016):

k , · · · , R(N )

¯Rk =

exp(R(i)
k /τ )
j=1 exp(R(j)

k /τ )

,

(cid:80)N

(5)

where τ is a hyper-parameter that adjusts the variance of
¯Rk. This normalization method maps rewards to [0, 1], will
stabilize the training process.

Experiment
In this section, we evaluate our model on two types of tasks,
text classiﬁcation, and sequence labeling.

Exp-I: Text Classiﬁcation
Firstly, we apply our model to a text classiﬁcation dataset
whose tasks are separated by domains. With this dataset, we
can check if our model can effectively use shared informa-
tion across domains.

Data We use a dataset of 16 tasks. The dataset is a com-
bination of two categories. The ﬁrst category that makes up
14 tasks is product reviews, extracted from the dataset1 con-
structed by (Blitzer et al. 2007). Reviews of each task come
from different Amazon products domain, such as Books,
DVDs, and so on. The second category making up the re-
maining 2 tasks is movie reviews, speciﬁcally IMDB2 con-
ducted by (Maas et al. 2011), and MR3 from rotten tomato
website, conducted by (Pang and Lee 2005).

1https://www.cs.jhu.edu/˜mdredze/datasets/

sentiment/

2https://www.cs.jhu.edu/˜mdredze/datasets/

sentiment/unprocessed.tar.gz

3https://www.cs.cornell.edu/people/pabo/

movie-review-data/

Single-Task

LSTM

Bi-LSTM

Multi-Task
PSP

Tasks

Books
Electronics
DVD
Kitchen
Apparel
Camera
Health
Music
Toys
Video
Baby
Magazines
Software
Sports
IMDB
MR

Avg

80.3
78.5
78.3
81.7
83.0
87.3
84.5
77.5
84.0
80.3
85.7
92.3
84.5
79.3
79.5
73.5

81.9

82.0
80.0
84.0
83.5
85.7
88.3
85.3
78.7
85.7
83.7
85.7
92.5
86.7
81.0
83.7
73.7

83.0

FS

84.5
86.7
86.5
86.0
85.7
86.7
89.3
82.5
88.5
86.7
88.0
91.5
87.0
85.5
82.3
71.0

85.5

SSP

87.0
86.5
86.3
88.7
86.3
88.0
90.0
81.0
87.7
85.5
87.7
93.0
88.3
86.5
85.3
72.7

86.3

84.7
85.5
86.0
88.0
87.5
87.7
89.0
81.3
88.5
84.5
87.3
93.3
89.7
86.7
84.5
72.5

86.1

CS

84.0
85.5
86.5
87.3
87.0
86.3
90.0
84.0
87.5
85.7
88.3
91.0
87.5
87.7
82.5
73.3

85.9

Our

87.3
88.3
86.0
87.5
88.7
90.0
90.3
84.7
89.7
86.0
89.0
90.3
91.0
88.7
84.7
76.3

87.4

Table 1: The results of text classiﬁcation experiment.

The dataset for each task contains 2000 samples, which
partitioned randomly into training data, development data
and testing data with the proportion of 70%, 10%, and 20%
respectively. Each sample has two class labels, either posi-
tive or negative.

Baseline Model We compare our proposed model with
four multi-task models and one single-task model. These
baseline models have the same Bi-LSTM modules with pro-
posed model. They differ only in how we organize these
modules.
• FS-MTL: All tasks use a fully-shared module and a task-

speciﬁc classiﬁer.

• SSP-MTL: This model is followed stack share-private
scheme. There is a shared module that receives embed in-
puts of all tasks and takes its outputs as inputs for task-
speciﬁc modules. The outputs of task-speciﬁc modules
are then fed into the task-speciﬁc classiﬁer.

• PSP-MTL: This model

is followed a parallel share-
private scheme. Concatenate the outputs of a fully-shared
module with the feature extracted from task-speciﬁc mod-
ules as inputs to the task-speciﬁc classiﬁer.

• CS-MTL: Cross Stitch multi-task model proposed by
(Misra et al. 2016). It’s a soft-sharing approach, using
units to model shared representations as a linear combi-
nation.

• Single Task: Standard Bi-LSTM classiﬁer that trains sep-

arately on each task.
The classiﬁer mentioned above is a linear layer taking the

average of the text sequence as inputs.

Hyperparameters The networks are trained with back-
propagation, and the gradient-based optimization is per-
formed using the Adam update rule (Kingma and Ba 2014),
with an early-stop parameter of 12 epochs.

Domains

Train

Dev

Test

Broadcast Conversation (bc) 173,289
29,957 35,947
25,271 26,424
206,902
164,217
15,421 17,874
878,223 147,955 60,756
25,206 25,883
297,049
11,200 10,916
90,403
49,393 52,225
388,851

Broadcast News (bn)
Magazines (mz)
Newswire (nw)
Pivot Corpus (pc)
Telephone Conversation (tc)
Weblogs (wb)

Table 2: Statistics of tokens for each domain in OntoNote
5.0 dataset.

As for the controller, the size of the RNN’s hidden state
is 50. Additionally, an exploration probability of sampling
randomly is set to 0.2, and the scaling factor in reward nor-
malization τ is 1
30 . The N = 20 modules are available, but
the model tends to use less than N = 20 modules. The size
of task embeddings S is 15.

The word embeddings for all of the models are initialized
with the 100d GloVe vectors (6B token version) (Penning-
ton, Socher, and Manning 2014), while the size of hidden
state h in Bi-directional LSTMs is 50. Per-task Fine-tuning
during training with shared parameters ﬁxed is used to im-
prove the performance. The mini-batch size is set to 64.

Result We evaluate all these models, and the classifying
accuracy is demonstrated in Table 1. Compared with the best
baseline model, our model still achieves higher performance
by 1.5 percent. With more observation of the ﬁnal architec-
ture selected by the controller, we ﬁnd out that the controller
groups similar tasks together, showing the ability of cluster-
ing, which will be further discussed in Section 5.

Domain

Task

Bi-LSTM PSP

SSP

CS

Our

Task Bi-LSTM PSP

SSP

CS

Our

bc

bn

mz

nw

wb

tc

pt

Avg

POS
CHUNK

POS
CHUNK

POS
CHUNK

POS
CHUNK

POS
CHUNK
POS
CHUNK

POS
CHUNK

POS
CHUNK

89.56
85.60

92.26
87.23

79.77
83.79

94.35
90.51

89.40
87.62
92.75
88.05

97.26
93.86

90.76
88.09

94.21
89.20

94.67
90.80

91.22
89.85

95.11
92.19

92.69
91.39
93.88
90.41

97.73
95.33

94.22
91.31

94.67
89.81

95.17
91.06

92.80
90.36

94.80
91.00

93.24
91.56
94.63
90.23

97.54
94.45

94.69
91.21

94.17
86.24

92.49
86.02

84.61
81.81

96.23
92.25

90.08
88.00
94.24
87.35

98.81
95.47

92.95
88.16

94.61 NER
91.19
SRL

95.28 NER
91.34
SRL

92.88 NER
91.04
SRL

95.02 NER
92.29
SRL

92.71 NER
92.52
SRL
94.99 NER
91.38
SRL

97.68 NER
95.91
SRL

94.74 NER
92.24
SRL

92.15
97.58

88.46
95.56

89.69
97.75

93.27
96.45

96.20
95.13
95.25
98.32

/
96.38

92.50
96.74

94.26
98.07

93.19
97.94

92.09
97.52

94.30
97.05

96.48
95.17
95.32
98.80

/
96.35

94.27
97.27

94.50
98.11

93.11
97.71

93.22
97.86

93.62
96.73

96.51
96.17
95.37
98.84

/
96.36

94.39
97.40

93.03
97.80

89.70
97.66

91.23
97.96

95.48
96.83

98.17
95.84
97.21
98.81

/
96.20

94.14
97.30

94.50
98.21

93.68
97.35

93.37
98.16

93.94
97.81

96.58
96.36
95.95
99.05

/
97.42

94.67
97.77

Table 3: Resultant accuracy for the sequence labeling dataset, regarding every combination of (domain, problem) as tasks,
accumulating to 7 * 4 - 1 = 27 separate tasks (accurately 27 tasks, since pt has no NER annotation).

Unite

Domains

Separated

Domains

POS

CHUNK

NER

SRL

Avg.

94.23

91.86

94.58

97.12

94.45

94.74

92.24

94.67

97.77

94.85

Table 4: Comparison of results in separated domains and
united domains, respectively.

Exp-II: Sequence labeling
Secondly, we evaluate our model on sequence labeling task.
It challenges the model to discover a shared architecture
with hidden hierarchies to complete tasks that are differ-
ent in semantic levels and signiﬁcantly differ from each
other. Different from Exp-I, to make the model more suit-
able for sequence labeling, we use a conditional random
ﬁeld (CRF)(Lafferty, McCallum, and Pereira 2001) as the
output layer.

Data We use OntoNotes 5.0 dataset4 (Weischedel et al.
2013) for our sequence labeling experiment.
OntoNotes contains several tasks across different domains,
which helps us validate our model’s ability to learn shared

4https://catalog.ldc.upenn.edu/ldc2013t19.

Figure 3: Task embedding for Exp-I (text classiﬁcation),
projected onto two-dimensional space by t-SNE. The full
names of domains are displayed in Table 2.

hierarchies. We select four tasks: part-of-speech tagging
(POS tagging), chunking (Chunk), named entity recognition
(NER), and semantic role labeling(SRL). The statistical de-
tails of the sub-datasets for each domain are shown in Table
2. There are four annotations for each sentence, correspond-
ing to four tasks.

In contrast to Exp-I, where the differences among tasks
are simply domains, tasks in this experiment differ from
each other more signiﬁcantly, and the relationships of tasks

(a) The hierarchical architecture selected by the controller.
Different colors represent different tasks. The lines and
arrows show the chosen information ﬂow of tasks. For ex-
ample, given a sample of Chunking task, the controller se-
quentially chooses M1, M2, and Stop, forming the path
in teal of the picture. The selective modules are originally
disordered and are renamed M1 · · · M4 for more explicit
representation.

(b) Probabilities of module selection at each step of the decision
process. Probabilities are drawn from controller’s module deci-
sion process after softmax function. The ﬁrst four columns in
every histogram represent the probability of choosing four differ-
ent modules. The ﬁfth column means the probability of deciding
to Stop. For chunking, the decision sequence with the biggest
probabilities is M1, M2, Stop, which corresponds with the path
in 4a.

Figure 4: The selected architecture and the corresponding probabilities at every step of the decision process.

are more complicated. For example, solving NER problems
can rarely help solving POS tagging task, while POS tag-
ging may somehow provide low-level information for NER
problems. There seems to be a hidden hierarchical structure
of these tasks, and it challenges our model to ﬁnd the hidden
structure without any prior knowledge.

Result We regard every combination of (domain, prob-
lem) such as (bc, NER) as different tasks, accumulating to 7
* 4 - 1 = 27 tasks. We evaluate our model together with all
baseline models mentioned in Exp-I, with a slight change of
removing the average pooling classiﬁer to every step in the
sequence. Table 3 shows the ﬁnal accuracy of all these tasks.
It is remarkable that our model achieves higher performance
over all baseline models. It shows that our model success-
fully handles the situation of complicated task relationships.
Additionally, we carry out a small experiment of gather-
ing data across domains together, reducing the number of
tasks from 27 to 4. According to experiment results demon-
strated in Table 4, the performance with united domains is
lower than that with separated domains. It shows that our
model is capable of leveraging domain-speciﬁc knowledge.

Analysis
The high performance of our model suggests that the con-
troller selects proper structures in different situations. To get
a more intuitive understanding of how our model works, we
explore and visualize the chosen architecture. Three main

characteristics of our generated structure are as follows.

Task Embedding Clustering
In our text classiﬁcation experiment, the structure generated
by our model is capable of clustering tasks from similar do-
mains together. To illustrate our clustering ability, we use
t-SNE (Maaten and Hinton 2008) to project our task embed-
ding onto two-dimensional space. The results are in Figure
3.

From the illustration, we ﬁnd that the task embedding
shows the ability to cluster similar domains. For exam-
ple, the task embedding of ’IMDB’(movie review) lies near
’video’, ’music’, and ’DVD’. It indicates that our model can
distinguish characteristics of different tasks, and eventually
ﬁnds a proper structure for the problem.

Learned Hierarchical Architecture
As proposed by (Hashimoto et al. 2017), tasks of POS Tag-
ging, Chunking, NER and sentence-level problems form a
hierarchical linguistic structure, to which multi-task models
should conform. To validate if our model learns a hierarchi-
cal architecture, we investigate the selected architecture in
the 4-task experiment of sequence labeling. For each task,
we visualize the module selection sequence of the biggest
probability. The results are shown in Figure 4a.

In the beginning, the controller randomly chooses mod-
ules, trying structures that may contain loop paths or even
self-loops. Nevertheless, our model eventually comes up
with a hierarchical structure, with decision probabilities

(a) 3D view

(b) Vertical view

(c) Front view

(d) Left view

Figure 5: Module selection sequence projected into three-dimensional space. Each point in the space represents a task. The
position of it is determined by its module selection sequence. POS Tagging, Chunking, NER and SRL are respectively denoted
by •, (cid:72), (cid:70), and ×. Points align precisely on discrete positions. Slight shifts are imposed upon points for better visualization.

shown in Figure 4b. It convincingly indicates that our model
has learned to generate a superior hierarchical structure.

Module Selection Clustering
Another characteristic of the selected architecture is that
similar tasks are clustered to share similar module structures.
It can be illustrated if we regard the sequence of chosen
modules as coordinates and project it into high-dimensional
space. Since only three steps are used in consequence, Fig-
ure 5 shows results in three-dimensional space.

There are some interesting patterns in the distributions of
different tasks. Firstly, tasks of the same problem are likely
to be neighboring. Secondly, high-level tasks like SRL are
distributed more sparsely. Additionally, similar domains, for
example, tasks in bc(broadcast conversation) have the same
positions as tasks in bn(broadcast news). All these patterns
found in the distributions show the ability of our model of
clustering similar tasks and generating a reasonable struc-
ture.

Related Work
Multi-Task Learning has achieved superior performance in
many applications (Collobert and Weston 2008; Glorot, Bor-
des, and Bengio 2011; Liu et al. 2015; Liu, Qiu, and Huang
2016; Duong et al. 2015). However, it is always done with
hard or soft sharing of layers in the hope of obtaining a more
generative representation shared between different tasks.

Because of restrictions on shared structure, so that only
when facing the loosely relevant tasks or datasets, the effect
will be good. Søgaard and Goldberg (2016) and Hashimoto
et al. (2017) train their model considering linguistic hier-
archies between related NLP tasks, by making lower level
tasks affect the lower levels of the representation. They show
that given a hierarchy of tasks, the effectiveness of multi-
tasking learning increases, but they are pre-deﬁned by hu-
man knowledge.

Recently, to break through the constraints of the shared
structure, Lu et al. (2017) proposes a bottom-up approach
that dynamically creates branches of networks during train-
ing that promotes grouping of similar tasks. Meyerson and

Miikkulainen (2017) introduced as a method for learning
how to apply layers in different ways at different depths
for different tasks, while simultaneously learning the layers
themselves. However it can only change the order of shared
layers, cannot learn the hierarchies.

Other approaches related to ours include meta-learning
and neural architecture search(NAS), meta-learning tries to
learn a meta-model that generates a model for speciﬁc tasks.
Most of them are using the meta-network to controls the pa-
rameters of the task-speciﬁc networks (Ravi and Larochelle
2017; Chen et al. 2018). The NAS is introduced in (Zoph
and Le 2016; Baker et al. 2016; Pham et al. 2018), where
it’s applied to construct CNN for image classiﬁcation. And
Liu et al. (2017) has introduced a hierarchical genetic repre-
sentation method into it. However, they are all dealing with
single tasks, not the shared architecture search in Multi-Task
Learning. Rosenbaum, Klinger, and Riemer (2017)’s work
is similar with us, but they have made restrictions on the op-
tional position of modules. And they only optimize on the
same type of tasks, like image classiﬁcation, do not consider
the latent hierarchy between tasks that enhance representa-
tion.

Conclusion
In this paper, we introduce a novel framework to explore the
shared structure for MTL with automatic neural architecture
search. Different from the existing MTL models, we use a
controller network to select modules from a shared pool for
individual tasks. Through this way, we can learn an opti-
mal shared structure for tasks to achieve better performance
leverage the implicit relations between them. We evaluate
our model on two types of tasks, have both achieved supe-
rior results. Besides, we have veriﬁed that our model can
learn a reasonable hierarchy between tasks without human
expertise.

References
[2016] Baker, B.; Gupta, O.; Naik, N.; and Raskar, R. 2016.
Designing neural network architectures using reinforcement
learning. arXiv preprint arXiv:1611.02167.

shared hierarchies: Deep multitask learning through soft
layer ordering. arXiv preprint arXiv:1711.00108.
[2016] Misra, I.; Shrivastava, A.; Gupta, A.; and Hebert, M.
2016. Cross-stitch networks for multi-task learning. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 3994–4003.
[2016] Norouzi, M.; Bengio, S.; Jaitly, N.; Schuster, M.; Wu,
Y.; Schuurmans, D.; et al. 2016. Reward augmented max-
In Ad-
imum likelihood for neural structured prediction.
vances In Neural Information Processing Systems, 1723–
1731.
[2005] Pang, B., and Lee, L. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with respect
to rating scales. In Proceedings of the ACL, 115–124.
[2014] Pennington, J.; Socher, R.; and Manning, C. 2014.
Glove: Global vectors for word representation. In Proceed-
ings of the 2014 conference on empirical methods in natural
language processing (EMNLP), 1532–1543.
[2018] Pham, H.; Guan, M. Y.; Zoph, B.; Le, Q. V.; and
Dean, J. 2018. Efﬁcient neural architecture search via pa-
rameter sharing. arXiv preprint arXiv:1802.03268.
[2017] Ravi, S., and Larochelle, H. 2017. Optimization as a
model for few-shot learning.
[2017] Rosenbaum, C.; Klinger, T.; and Riemer, M.
Routing networks: Adaptive selection of non-
2017.
arXiv preprint
linear functions for multi-task learning.
arXiv:1711.01239.
[2016] Søgaard, A., and Goldberg, Y. 2016. Deep multi-
task learning with low level tasks supervised at lower layers.
In Proceedings of the 54th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 2: Short Pa-
pers), volume 2, 231–235.
[2013] Weischedel, R.; Palmer, M.; Marcus, M.; Hovy, E.;
Pradhan, S.; Ramshaw, L.; Xue, N.; Taylor, A.; Kaufman, J.;
Franchini, M.; et al. 2013. Ontonotes release 5.0 ldc2013t19.
Linguistic Data Consortium, Philadelphia, PA.
[2016] Zoph, B., and Le, Q. V.
ture search with reinforcement learning.
arXiv:1611.01578.

2016. Neural architec-
arXiv preprint

[1997] Baxter, J. 1997. A bayesian/information theoretic
model of learning to learn via multiple task sampling. Ma-
chine learning 28(1):7–39.
[2007] Blitzer, J.; Dredze, M.; Pereira, F.; et al. 2007. Bi-
ographies, bollywood, boom-boxes and blenders: Domain
adaptation for sentiment classiﬁcation. In ACL, volume 7,
440–447.
[2018] Chen, J.; Qiu, X.; Liu, P.; and Huang, X. 2018. Meta
multi-task learning for sequence modeling. In Proceeding of
AAAI.
[2008] Collobert, R., and Weston, J. 2008. A uniﬁed ar-
chitecture for natural language processing: Deep neural net-
works with multitask learning. In Proceedings of ICML.
[2015] Duong, L.; Cohn, T.; Bird, S.; and Cook, P. 2015.
Low resource dependency parsing: Cross-lingual parame-
In Proceedings of
ter sharing in a neural network parser.
the 53rd Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 2: Short Papers),
volume 2, 845–850.
[2011] Glorot, X.; Bordes, A.; and Bengio, Y. 2011. Domain
adaptation for large-scale sentiment classiﬁcation: A deep
learning approach. In ICML-11, 513–520.
[2017] Hashimoto, K.; Tsuruoka, Y.; Socher, R.; et al. 2017.
A joint many-task model: Growing a neural network for
In Proceedings of the 2017 Confer-
multiple nlp tasks.
ence on Empirical Methods in Natural Language Process-
ing, 1923–1933.
[2014] Kingma, D. P., and Ba, J. 2014. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980.
[2001] Lafferty, J. D.; McCallum, A.; and Pereira, F. C. N.
2001. Conditional random ﬁelds: Probabilistic models for
segmenting and labeling sequence data. In ICML-2001.
[2015] Liu, X.; Gao, J.; He, X.; Deng, L.; Duh, K.; and Wang,
Y.-Y. 2015. Representation learning using multi-task deep
neural networks for semantic classiﬁcation and information
retrieval. In NAACL.
[2017] Liu, H.; Simonyan, K.; Vinyals, O.; Fernando, C.; and
Kavukcuoglu, K. 2017. Hierarchical representations for efﬁ-
cient architecture search. arXiv preprint arXiv:1711.00436.
[2016] Liu, P.; Qiu, X.; and Huang, X. 2016. Recurrent neu-
ral network for text classiﬁcation with multi-task learning.
In Proceedings of IJCAI, 2873–2879.
[2017] Lu, Y.; Kumar, A.; Zhai, S.; Cheng, Y.; Javidi, T.; and
Feris, R. 2017. Fully-adaptive feature sharing in multi-task
networks with applications in person attribute classiﬁcation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 5334–5343.
[2011] Maas, A. L.; Daly, R. E.; Pham, P. T.; Huang, D.; Ng,
A. Y.; and Potts, C. 2011. Learning word vectors for senti-
ment analysis. In Proceedings of the ACL, 142–150.
[2008] Maaten, L. v. d., and Hinton, G. 2008. Visualiz-
ing data using t-sne. Journal of machine learning research
9(Nov):2579–2605.
[2017] Meyerson, E., and Miikkulainen, R. 2017. Beyond

