DropCluster: A structured dropout for convolutional networks

Liyan Chen 1 Philip Gautier 2 Sergul Aydore 1

0
2
0
2
 
b
e
F
 
7
 
 
]

G
L
.
s
c
[
 
 
1
v
7
9
9
2
0
.
2
0
0
2
:
v
i
X
r
a

Abstract
Dropout as a regularizer in deep neural networks
has been less effective in convolutional layers than
in fully connected layers. This is due to the fact
that dropout drops features randomly. When fea-
tures are spatially correlated as in the case of con-
volutional layers, information about the dropped
pixels can still propagate to the next layers via
neighboring pixels. In order to address this prob-
lem, more structured forms of dropout have been
proposed. A drawback of these methods is that
they do not adapt to the data. In this work, we
introduce a novel structured regularization for
convolutional layers, which we call DropClus-
ter. Our regularizer relies on data-driven structure.
It ﬁnds clusters of correlated features in convo-
lutional layer outputs and drops the clusters ran-
domly at each iteration. The clusters are learned
and updated during model training so that they
adapt both to the data and to the model weights.
Our experiments on the ResNet-50 architecture
demonstrate that our approach achieves better per-
formance than DropBlock or other existing struc-
tured dropout variants. We also demonstrate the
robustness of our approach when the size of train-
ing data is limited and when there is corruption in
the data at test time.

1. Introduction

Convolutional neural networks (CNNs) have become a foun-
dational tool for computer vision problems. Regularization
is needed during CNN training to ensure that the model gen-
eralizes well to unseen data, especially when the training
set is small, or the test set is noisy. Achieving good general-
ization is especially crucial for critical applications such as
medical diagnostics, self-driving cars, or facial recognition.

Dropout is a well-known regularization approach to re-
duce generalization error (Srivastava et al., 2014). Dropout,
which randomly drops units in hidden layers during each

1Stevens Institute of Technology 2Unafﬁliated. Correspondence

to: Sergul Aydore <sergulaydore@gmail.com>.

Work in progress. Copyright 2020 by the author(s).

training iteration, can be interpreted as a way of ensembling
thinned networks at test time. Although dropout has been
successful for fully-connected layers, it has been less ef-
fective for CNNs. Recently, (Ghiasi et al., 2018) argued
that the presence of spatial correlation between features in
convolutional layers prevents the success of dropout.

In this work, we introduce a novel structured regularization
for convolutional layers, called DropCluster, which learns
the structure in feature maps and leverages this structure to
drop features more effectively. Our method learns clusters
in convolutional layer outputs during training. The clusters
are learned after some number of epochs in training and
updated at regular intervals for the rest of training. We drop
features in two ways: ﬁrst, we drop clusters of correlated
features in each training step; second, if a channel does not
demonstrate any clusterable structure, we drop that channel
entirely.

The clustering algorithm we use is recursive nearest ag-
glomeration (ReNA) (Hoyos-Idrobo et al., 2018). ReNA is
well-suited to two-dimensional images because it assumes
some prior graph structure connecting features which are
likely to be correlated, and is appropriate for learning clus-
ters during training due to its fast computation (Aydore et al.,
2019). In our application, the graph structure is given by
adjacent pixels in each feature map. Our experiments on
CIFAR-100 and Tiny ImageNet with ResNet-50 architec-
ture demonstrate that our approach performs better than the
alternatives.

Typically, the performance of machine learning models is
evaluated on a separate test dataset. It is assumed that the
training data is representative of the unseen data. However,
this is not necessarily true in real-world applications. It is
possible to encounter new data from a distribution different
than the distribution of the training data (Liu et al., 2018;
Recht et al., 2018). Even when humans can barely notice
the difference, this mismatch between training and new data
distributions can produce incorrect scientiﬁc conclusions or
let attackers fool machine learning systems by feeding ma-
nipulated input to the models (Hendrycks & Gimpel, 2016).
Therefore, achieving robustness to small perturbations in
the data is essential especially for applications such as spam
ﬁlters, detecting diseases or discovering astronomical phe-
nomena (Hendrycks & Dietterich, 2019; Hendrycks et al.,

DropCluster: A structured dropout for convolutional networks

2018). In this work, without including adversarial training,
we evaluate the robustness of our DropCluster to common
corruptions in the data. We use noise and blur corruptions as
also used by several studies to demonstrate the fragility of
CNNs (Hosseini et al., 2017; Dodge & Karam, 2017). Our
results show that our DropCluster is more robust to these
corruptions compared to other structured regularizers.

convolutional ﬁlters and common CNN architectures, im-
prove generalization. Our algorithm goes a step beyond
architecture-driven regularization with data-driven regular-
ization. We demonstrate the appropriateness of our approach
with cluster analysis, and we demonstrate its effectiveness
with improved performance and robustness on benchmark
datasets.

Fitting deep learning models with many parameters can be
even more challenging when the sample size of training
data is small. This problem can occur in ﬁelds such as as-
tronomy, genomics, chemistry and medical imaging where
data acquisition is expensive (Fan & Li, 2006; Consortium
et al., 2015). Therefore, it is important for a machine learn-
ing model to perform well when trained on data with small
sample size. We provide performance comparisons between
our approach and other approaches to evaluate robustness to
small sample size. Our results show that DropCluster per-
forms better than the others even when the size of training
data is reduced.

2. Related work

In order to address the limitation of dropout in CNNs,
structured variants of dropout such as DropBlock, Spatial-
Dropout and StochasticDepth have been proposed. Among
these, the most relevant work to ours is DropBlock. Drop-
Block randomly drops a small square region of a feature
map to remove certain semantic information. DropBlock
improves training of convolutional layers, where traditional
dropout does not, because selecting contiguous blocks lever-
ages the spatial correlation found in feature maps (Ghiasi
et al., 2018). The intention is that information from dropped
features cannot leak into training via the features’ corre-
lated neighbors. However, the shape of the dropped region
is constant. We provide analysis showing that the shapes
of correlated regions in feature maps tend to vary between
channels. Our algorithm uses these varying shapes directly,
dropping exactly the features which are highly correlated
with each other.

StochasticDepth is another regularization tool for CNNs
(Huang et al., 2016). To address the redundancy between
layers, StochasticDepth randomly removes a subset of lay-
ers during training while maintaining a full depth network at
test time. In Tompson et al. (2015), the authors formulated
another structured dropout method called SpatialDropout
that randomly drops channels of the network. However,
StochasticDepth and SpatialDropout only make use of cor-
relation within layers and within channels, respectively. Fur-
thermore, they are informed by the network architecture,
and not by patterns observed during model training, unlike
our approach.

These approaches, which were speciﬁcally designed for

3. ReNA: Recursive Nearest Agglomeration

In this work, we use ReNA to compute clusters. Although
our method is independent of the choice of clustering algo-
rithm, we choose ReNA because it is fast enough to compute
multiple times during training, and because its graph-based
feature grouping is well suited to two-dimensional (and po-
tentially three-dimensional) images and feature maps, which
have a natural graph structure deﬁned by adjacent pixels. In
this section we provide some details of the ReNA algorithm
as background.

ReNA is initialized by placing every feature (pixel) in its
own cluster, and proceeds by greedily merging pairs of con-
nected pixels whose values are consistently similar across
examples. This process is repeated until reaching a pre-
speciﬁed number of clusters. ReNA’s computation time
is O(nm log(m/k)), where m is the number of pixels in
the feature map, n is the number of examples, and k is the
number of clusters (Hoyos-Idrobo et al., 2018). We hold n
constant as the number of examples in a single mini-batch,
and we hold k constant as a tunable hyperparameter. More
formally, let Φ ∈ Rk×m represent a feature grouping matrix
that represents the k clusters of m features such that

Φ =








α1 · · · α1
0 · · · 0
...
0 · · · 0

0 · · · 0
α2 · · · α2
...
0 · · · 0

· · ·
· · ·
...

0 · · · 0
0 · · · 0
...








(1)

0 · · · 0 αk · · · αk

with an appropriate permutation of the features. The αi
values are chosen so that Φ is an orthonormal matrix. ReNA
obtains this featuring grouping matrix by using neighbor-
hood graphs based on local statistics of the image. The
algorithm takes the data X ∈ Rm×n where n is the number
of samples and the regular square lattice represented by a
binary adjacency matrix G ∈ Rm×m ∈ {0, 1} as graph.

ReNA converges when the desired number of clusters are
learned. However, when the data lack meaningful clusters,
ReNA will fail to converge. We make use of this property
of ReNA to identify channels in the network which are too
noisy to contribute to the learning task. We show empirically
that dropping these channels improves learning, and we
compute cluster tendency statistics on the feature maps to
demonstrate that the lack of structure is not an artifact of our
selected clustering algorithm, but is an important property
of CNN training.

DropCluster: A structured dropout for convolutional networks

4. Clusters in Convolutional Layers

In this section, we motivate our approach by providing evi-
dence that convolutional layer feature maps exhibit cluster
tendency. We demonstrate cluster tendency qualitatively by
visualizing computed clusters, and quantitatively by comput-
ing clustering statistics. We reference the Hopkins statistic
for computing the degree to which clusters exist in data,
and we introduce a novel modiﬁcation to make the statistic
appropriate for spatial data, such as images.

We train a LeNet-5 model on the CIFAR-10 dataset, and a
ResNet-50 model on the CIFAR-100 and Tiny ImageNet
datasets. We compute clusters in each channel given a mini-
batch of outputs from the ﬁrst convolutional layer. It is
important to note that as the size of the convolutional layer
outputs gets smaller, it becomes more challenging to identify
structure in the data. Therefore our algorithm implements
clustering only after the ﬁrst convolutional layer for the
datasets used in this paper. We also note that clusters in
each feature map change rapidly during the early stages of
training. Therefore we do not attempt to learn clusters until
the 50th epoch.

4.1. Qualitative Approach

For visualization of computed clusters, we train LeNet-5 on
CIFAR-10 and ResNet-50 on CIFAR-100. The LeNet-5 ar-
chitecture is useful for analysis because the relatively large
feature maps produced by its ﬁrst convolutional layer con-
tain recognizable features with obvious connections to the
raw input image. Analysis of ResNet-50 demonstrates that
the behavior observed in LeNet-5 persists in more modern
CNN architectures.

For the LeNet-5 architecture trained on CIFAR-10, we com-
pute clusters on the outputs of the ﬁrst convolutional layer
after training the model for 50 epochs. The third row in Fig-
ure 1 shows the clusters at each channel. It can be observed
that channel 3 captures vertical features while channel 5
captures horizontal features. The correlated groups of fea-
tures identiﬁed by clustering are not found in regular square
blocks, which would be adequately regularized by a method
such as DropBlock. Instead, they are irregularly shaped.
Furthermore, we observed during our experiments that the
structure of the clusters varies between training runs, due to
random initialization of the model weights. Both of these
observations point to the need for an adaptive, data-driven
approach to regularizing convolutional layers.

Similarly, for ResNet-50 trained on CIFAR-100, we com-
pute clusters on the outputs of the ﬁrst convolutional layer af-
ter training the model for 50 epochs. This layer includes 64
channels. The output size of each channel is 16×16 which
is much smaller than the LeNet-5 outputs. For each channel,
we compute 15 clusters. Figure 2 shows the clusters found

Figure 1. Clusters of the ﬁrst convolutional layer outputs from a
LeNet-5 model trained on the CIFAR-10 dataset after 50 epochs.
This layer includes 6 channels. The left two images are the original
input samples (horse and bird). The black-and-white images are
the feature maps of these two samples at the ﬁrst convolutional
layer. The third row shows the clusters of these 6 channels.

in these 64 channels. Again, clusters are irregularly shaped,
necessitating an adaptive regularization strategy.

Figure 2. Clusters of the ﬁrst convolutional layer outputs from a
ResNet-50 model trained on the CIFAR-100 dataset.

Across many training runs, we found that training LeNet-5
on CIFAR-10 and ResNet-50 on CIFAR-100 did not re-
sult in any unstructured channels where features were not
clusterable. However, unstructured channels were present
when training ResNet-50 on Tiny ImageNet, where drop-
ping unstructured channels improved performance. These
unstructured channels are clearly visible in Figure 3. In
channels 2, 5, 6, 8, 13, etc., ReNA was not able to converge,
providing qualitative evidence of lack of structure. In the
following section, we investigate this lack of structure more
rigorously.

DropCluster: A structured dropout for convolutional networks

generated artiﬁcial points, and H will be close to 0.5, im-
plying lack of cluster tendency. If, on the other hand, H
is close to 1, then we can conclude that the data exhibits
strong cluster tendency. Values of H near 0 implies the data
are regularly spaced.

The Hopkins statistic is not applicable to image data be-
cause it does not take the ordering of features into account.
Direct application to images would consider only the pixel
intensities, and not their spatial locations. Therefore, we
propose a new cluster tendency statistic Spatial Hopkins
based on the difference of each pixel’s intensity from its
neighbors. When there is spatial structure in the data, then
each pixel’s intensity value will be close to its neighbors’
values. Based on this observation, we sample m pixel loca-
tions (xi, yi) uniformly randomly from xi ∈ {2, · · · , X−1}
and yi ∈ {2, · · · , Y − 1}, i ∈ 1, · · · , m for an image I of
size X × Y . We avoid pixels on the edges of the image for
convenience. Similar to the Hopkins statistic, we choose
m << XY . Then we compute the average L2 distance
between the value of the pixel at this location and its 8
neighbors as:

wi =

(cid:107)I(xi, yi) − I(u, v)(cid:107)2

(3)

where (u, v) ∈ {(xi − 1, yi − 1), (xi − 1, yi), (xi − 1, yi +
1), (xi, yi−1), (xi, yi+1), (xi+1, yi−1), (xi+1, yi), (xi+
1, yi + 1)}. Next, we sample another m pixel locations,
(q1, r1), · · · , (qm, rm) again uniformly randomly in the im-
age, and compute average distance between the intensities
of (xi, yi) and the neighbors of (qi, ri):

zi =

(cid:107)I(qi, ri) − I(u, v)(cid:107)2.

(4)

The set of points (u, v) are the neighbors of (xi, yi) in both
equations. Finally, similar to the Hopkins statistic, we com-
pute our Spatial Hopkins metric for a given image as:

S =

(cid:80)m
i=1 zi
i=1 zi + (cid:80)m

(cid:80)m

i=1 wi

.

(5)

As is the case with the original Hopkins statistic, the Spatial
Hopkins statistic will have values near 0.5 if the data have
no clustered structure. In Figure 4, we report the average
Spatial Hopkins values across a mini-batch for each channel
from the outputs of the ﬁrst convolutional layer of Tiny Ima-
geNet with a ResNet-50 model trained for 50 epochs. It can
be seen that the majority of the channels have Spatial Hop-
kins values close to 1 indicating the presence of structure.
In these channels, we cluster features and drop one cluster at
a time. However, for channels such as 2, 5, 6, 8, 13, etc. the
values are close to 0, indicating a lack of structure. These
correspond to the channels in Figure 3 where ReNA was not

1
8

(cid:88)

u,v

1
8

(cid:88)

u,v

Figure 3. Clusters of the ﬁrst convolutional layer outputs from a
ResNet-50 model trained on the Tiny ImageNet dataset for 50
epochs without any dropout. The channel indices are shown at the
top of each image. The dark blue images represent the channels
where ReNA was not able to compute the desired number of clus-
ters (15 in this case). These channels also had cluster tendency
values close to 0.

4.2. Quantitative Approach: Spatial Clustering

Statistics

In this section, we present quantitative evidence of the pres-
ence of clusters in feature maps, and of the variability of
clusters between channels. We ﬁrst discuss the Hopkins
statistic (Banerjee & Dave, 2004), a measure used in clus-
tering literature to quantify the presence of clusters in data.
Then we propose a novel adaptation of the Hopkins statistic
making it appropriate for image data.

The Hopkins statistic is a cluster tendency measure whose
value is irrespective of any particular clustering algorithm.
Computation of the Hopkins statistic begins by generating
m random points uniformly distributed between the min-
imum and maximum of each feature, and also sampling
m actual data points from the collection of n data points
(m << n). For both sets of points, it measures the distance
to the nearest neighbor in the original dataset. For each
sample i in both sets, let ui be the nearest neighbor distance
from the artiﬁcial points and wi be the nearest neighbor
distance from the actual points. Then the Hopkins statistic
is deﬁned as

H =

(cid:80)m
i=1 ui
i=1 ui + (cid:80)m

(cid:80)m

i=1 wi

.

(2)

When the actual data points are approximately uniformly
distributed, they will have similar distances to the randomly

DropCluster: A structured dropout for convolutional networks

Algorithm 1 Compute Clusters
Input:
A ∈ Rb×t×w×h, output activations of the ﬁrst convolution
layer, where b is the mini-batch size, t is the number of
channels and w×h is the size of feature map at each channel.
n, number of clusters to compute
Output:
T ∈ {0, 1}t×n×w×h, cluster assignments for each pixel in
each channel, where 1 indicates membership.
N, set of indices of unstructured channels
1: for i = 1 to t do
2:
3:
4:
5:
6:
7:
8:
end if
9:
10: end for
11: Return T, N

Train ReNA on channel i: rena.fit(A[:, i, :, :])
Let m be the number of clusters found by ReNA
if m > n then

{Record cluster indices}
Let T = rena.get clusters()

Append i to N

else

5. DropCluster

In this section, we introduce our novel regularizer Drop-
Cluster. Our approach consists of two steps: (i) computing
clusters in feature maps and identifying unstructured chan-
nels which are not clusterable, (ii) dropping random clusters
from the structured channels during training and masking
all unstructured channels both during training and inference.

First, we apply ReNA to extract clusters from the outputs of
the ﬁrst convolutional layer. Since structure in convolutional
layers tends to change rapidly near the beginning of training,
we apply this step after some ﬁxed number of epochs, say s,
in training and update it every s epochs. We found s = 50
to be a suitable choice, and we use it in all experiments in
this paper. We use the publicly available implementation
for ReNA (Idrobo, 2017) with the maximum number of
iterations set to 1000 and the graph structure deﬁned by
adjacency, where each pixel has connections to the four
pixels above, below, left, and right. The clustering steps
are explained in detail in Algorithm 1. Figure 5 illustrates
dropping the unstructured channels.

Second, we drop randomly selected clusters from struc-
tured channels during training only, and mask unstructured
channels during training and inference. Similar to standard
dropout, we do not drop clusters in the structured channels
during inference. But channels which are found to have no
cluster structure stay masked in both training and inference,
similar to channel pruning (He et al., 2017). The details of
how clusters are dropped and channels are masked are given
in Algorithm 2 and also illustrated in Figure 6.

Figure 4. Spatial Hopkins statistic as a measure of cluster tendency
for the ﬁrst convolutional layer outputs from a ResNet-50 model
trained on the Tiny ImageNet dataset for 50 epochs without any
dropout.

able to ﬁnd any clusters. We set these channels’ output to
zero in training and at test time. In Figure 8, we report the
histogram of Spatial Hopkins values for deeper layers where
it can be seen that the values tend toward 0.5 for deeper lay-
ers. We advise practitioners to inspect these histograms to
decide at which layer to implement DropCluster.

The variability in cluster shapes found in feature maps, and
the variability in cluster tendency between structured and
unstructured channels, demonstrate the need for data-driven
regularization in CNN model training. Our algorithm’s
ability to adapt to these sources of variability explains its
improved performance.

Figure 5. Illustration of dropping unstructured channels (a) Input
image mini-batch to a CNN. (b) Output activations of a convolu-
tional layer with several channels. (c) Feature clusters for each
channel generated by ReNA. Channels that do not contain the
desired number of clusters are identiﬁed as unstructured channels
(marked with red X). (d) Inputs to the next layer after dropping
unstructured channels.

DropCluster: A structured dropout for convolutional networks

Algorithm 2 Apply DropCluster
Input:
A ∈ Rb×t×w×h, output activations of the ﬁrst convolution
layer, where b is the mini-batch size, t is the number of
channels and w×h is the size of feature map at each channel.
T ∈ {0, 1}t×n×w×h, cluster assignments for each pixel in
each channel
N, the set of unstructured channel indices
p, the dropout probability
n, the number of clusters
Output:
A, the masked outputs
1: Initialize mask matrix to ones: M = 1t×w×h
2: for i = 1 to t do
if i ∈ N then
3:
4:
5:
6:

Mask the entire channel: M[i, :, :] = 0w×h

else if mode == T raining then

Randomly sample a set of cluster indices L ⊂
{1, · · · n}, |L| = (cid:98)pn(cid:99)
for l in L do

M[i, :, :] ← M × (1 − T[i, l, :, :])

7:
8:
9:
end if
10:
11: end for
12: Repeat the mask along mini-batch dimension: M ←

end for

M.repeat(b, 1, 1, 1)

13: Apply the mask: A ← A × M
14: Normalize

features:

the

count(M )/count ones(M )

15: return A

A ← A ×

6. Experiments

In this section, we empirically evaluate the effectiveness of
our DropCluster for image classiﬁcation tasks. We compare
our approach with dropout, DropBlock, SpatialDropout,
StochasticDepth, and training without any type of dropout.
We show that DropCluster achieves better Top1 (%) accu-
racy than other regularizers on both CIFAR-100 and Tiny
ImageNet, and better Top5 (%) accuracy on CIFAR-100.

The goal of regularization is to improve how the model
generalizes to unseen data. Two important problems in gen-
eralization are small sample sizes available at training time,
and test data that comes from a different distribution than
the training data. In order to study robustness to small sam-
ple size in training and corrupted test data, we supplement
our standard benchmark results with experiments on various
sample sizes and with noise and blur corruptions.

Implementation Details: We use the standard widely
used CNN architecture ResNet-50 (He et al., 2016) in all our
experiments, changing only the types of dropout. We vary
the dropout probability parameter p for all dropout models

Figure 6. Visualization of dropping a single cluster from structured
channels detailed in Algorithm 2. In practice, we drop more than a
single cluster.

for CIFAR-100 from 0.1 to 0.3 with a grid of 0.05 and for
Tiny ImageNet from 0.1 to 0.5 with a grid of 0.1. We use
the momentum SGD algorithm (Sutskever et al., 2013) with
initial learning rate 0.1 and momentum parameter 0.9 for
training. A scheduler is used with a multiplicative factor of
learning rate decay 0.1 applied at epochs 150 and 200. The
weight decay parameter for CIFAR-100 is set to 5e − 4 and
to 1e − 4 for Tiny ImageNet. We repeat each experiment
for 4 different random initializations. We report the average
accuracy of predictions on the test set computed after each
of the last 50 epochs of training, averaged over 4 random
initializations.

The dropout and SpatialDropout regularizers are imple-
mented by inserting the corresponding dropout layer with
a dropout rate p after the convolutional layer in the fourth
group of ResNet blocks in both CIFAR-100 and Tiny Im-
ageNet. Following the publicly available implementation
of DropBlock (Ramos, 2019), we use a block size of 5 × 5
for CIFAR-100 and 7 × 7 for Tiny ImageNet in all experi-
ments with DropBlock. We follow Ghiasi et al. (2018) to
match up the effective dropout rate of DropBlock to the
desired dropout rate p. A DropBlock layer is applied after
the fourth group in Tiny ImageNet and ﬁrst group CIFAR-
100. In (Ghiasi et al., 2018), DropBlock is applied to the
fourth layer only, as well as to the third and fourth layers
for ImageNet. In our experiments, applying it to the fourth
layer only performed best so we report those results. For
the StochasticDepth regularizer, we follow the method from
Zhang et al. (2019) that randomly drops out entire ResNet
blocks at a dropout rate p during training.

We implement DropCluster only after the ﬁrst convolutional

DropCluster: A structured dropout for convolutional networks

layer and before the ﬁrst group of ResNet blocks. The size
of the channels become too small to exhibit any structure
information in the subsequent layers. We do not apply Drop-
Cluster until 50th (s in Section section 5) epoch in training
since the structure information can be highly variable at
the beginning of training. At every 50 epochs in training,
we compute the clusters in structured channels and identify
unstructured channels. We drop the unstructured channels
entirely both in training and inference, and we drop random
clusters from structured channels in training only, selecting
new clusters at each mini-batch. That is, we apply Algo-
rithm 1 at epochs 50, 100, · · ·, and we apply Algorithm 2
for each mini-batch. The number of clusters, n, is set to 15.

Computational details: We use Python 3.6 for imple-
mentation (Oliphant, 2007) using open-source libraries Py-
Torch (Paszke et al., 2019), scikit-learn (Pedregosa et al.,
2011), and NumPy (Walt et al., 2011). Experiments are run
using 8 Nvidia RTX 2080 Ti GPUs with 512 GB memory.
Our implementation will be openly available upon accep-
tance and is provided in the supplementary material.

6.1. CIFAR-100 Image Classiﬁcation

The CIFAR-100 dataset consists of 32 × 32 RGB images
from 100 classes (Krizhevsky et al., 2009). For each class,
there are 600 different images. The standard split contains
50, 000 training images and 10, 000 test images. For each
training sample, we apply horizontal random ﬂipping, ran-
dom rotation, 32 × 32 random crops after padding with 4
pixels on each side and normalization following the com-
mon practice (Huang et al., 2016). For test samples, we only
apply normalization as a pre-processing step.

Model

Top1 (%) Top5 (%)

Dropout
Probability

0
No Dropout
0.2
Dropout
0.15
DropBlock
0.1
DropCluster
SpatialDropout
0.15
StochasticDepth 0.25

60.82
61.00
62.01
63.01
61.04
61.73

83.66
84.32
84.54
85.45
84.63
85.39

Table 1. Average accuracy of predictions on the CIFAR-100 test
set computed after each of the last 50 epochs of training, averaged
over 4 random initializations, when full training set is used. Except
for the model trained without dropout, we report the best result
with best dropout probability for each model.

Standard Settings:
In this experiment, we use the full
training data for training and uncorrupted test data for eval-
uation. We report Top1 (%) and Top5 (%) accuracy results
on test data when the full training set is used for all models

in Table 1. For each regularizer, we report the best result
among the range of dropout probability values. The results
show that DropCluster achieves the highest performance
among all regularizers both for Top1 (%) and Top5 (%)
accuracies.

Small-sample Settings:
In this experiment, we explore
the robustness of DropCluster to small sample sizes on
CIFAR-100. For this purpose, we gradually reduce the size
of the training dataset from 100% to 20% of the the full
training size. Since DropBlock, DropCluster and Stochas-
ticDepth were the top 3 performers when the full training
set is used, we only compare these 3 for the experiments
in the small-sample setting. The results are summarized in
Table 2. It can be seen that DropCluster stays more robust
compared to DropBlock and StochasticDepth as the training
set size decreases.

Model

DropBlock

DropCluster

StochasticDepth

Trainset
size

Top1
(%)

100%
90%
80%
50%
20%

62.01
61.89
60.72
61.39
53.33

Top5
(%)

84.54
85.07
84.63
85.84
81.91

Top1
(%)

63.01
62.31
62.18
61.36
56.03

Top5
(%)

85.45
85.01
85.31
85.69
83.52

Top1
(%)

61.73
56.30
49.48
42.16
29.57

Top5
(%)

85.39
82.11
76.73
70.81
58.21

Table 2. Performance of DropBlock, DropCluster and Stochas-
ticDepth on test data of CIFAR-100 versus the training set size in
%.

6.2. Tiny ImageNet (Tiny200) Classiﬁcation

Tiny ImageNet dataset (CS231N, 2017) is a modiﬁed ver-
sion of the original ImageNet dataset (Deng et al., 2009)
containing 110, 000 images from 200 classes with resolu-
tion 64 × 64. We follow the standard 100, 000/10, 000 train-
ing/validation split. We used horizontal random ﬂip, scale,
random crop and normalization for training images as in
(Szegedy et al., 2015; Huang et al., 2017). During testing,
we only apply a single crop and normalization. Following
the common practice, we report the accuracy results on the
validation set.

Standard Settings: Similar to CIFAR-100, we ﬁrst evalu-
ate the performance of different dropout approaches trained
on the full training dataset. In Table 3, we compare the
performance of each model for the Tiny ImageNet dataset
on uncorrupted validation data. The results show that our
DropCluster performs the best for Top1 (%) accuracy. How-
ever, StochasticDepth performs better than DropCluster for
Top5 (%) accuracy.

DropCluster: A structured dropout for convolutional networks

Model

Top1 (%) Top5 (%)

Dropout
Probability

0
No Dropout
0.4
Dropout
0.1
DropBlock
0.1
DropCluster
SpatialDropout
0.1
StochasticDepth 0.1

64.69
64.93
65.59
66.26
64.85
64.86

83.49
84.49
84.14
84.35
84.13
85.41

Table 3. Average accuracy of predictions on the Tiny ImageNet test
set computed after each of the last 50 epochs of training, averaged
over 4 random initializations, when full training set is used. Except
for the model trained without dropout, we report the best result
with best dropout probability for each model.

Small-sample Settings: Similar to the experiments on
the CIFAR-100 dataset, we explore the robustness of Drop-
Cluster, DropBlock and StochasticDepth to small training
datasets. We summarize the results in Table 4. The results
demonstrate that DropCluster is more robust than Drop-
Block to reduced sample size as measured by Top1 % accu-
racy. StochasticDepth performs better than DropCluster in
Top1 % when the training size is 70% and 50%.

Model

DropBlock

DropCluster

StochasticDepth

Trainset
size

100%
90%
70%
50%
30%
10%

Top1
(%)

65.59
64.49
61.89
56.88
48.42
28.89

Top5
(%)

84.14
84.67
82.70
78.99
72.03
52.10

Top1
(%)

66.26
65.16
62.29
57.92
49.18
29.04

Top5
(%)

85.35
85.03
82.88
79.83
72.65
52.77

Top1
(%)

64.86
63.59
62.52
58.86
48.97
26.29

Top5
(%)

85.41
83.74
83.23
80.38
72.26
49.97

Table 4. Performance of DropBlock, DropCluster and Stochas-
ticDepth on Tiny ImageNet versus the training set size in % .

Corruption Settings:
In order to evaluate the robustness
of DropCluster and other approaches to corruption in test
data, we generated corrupted test datasets using the imple-
mentation provided by Hendrycks & Dietterich (2019). We
use 7 different types of corruption: Gaussian noise, shot
noise, impulse noise, defocus blur, glass blur, motion blur
and zoom blur using various severity levels as described in
(Hendrycks & Dietterich, 2019). Figure 7 shows the effects
of these corruptions on a sample image.

We report the performance of DropBlock, DropCluster and
StochasticDepth under common noise and blur corruptions
with severity level 1 in Table 5. We also show the perfor-
mance as a function of more severity levels in Figures 9 and
10. These results demonstrate that our DropCluster is more
robust to corruption than DropBlock and StochasticDepth.

Figure 7. A sample image with different noise and blur corruptions.

Model

DropBlock DropCluster

StochasticDepth

CORRUPTION Top1

Top5
(%)

Top1
(%)

Top5
(%)

(%)

GAUSSIAN

SHOT

IMPULSE

DEFOCUS

GLASS

MOTION

ZOOM

61.25 82.53
60.57 82.46
58.31 79.97
54.48 76.68
48.87 72.44
56.27 78.85
51.63 74.51

62.57 83.31
61.92 82.79
58.85 80.34
56.29 78.94
51.94 75.23
58.44 79.92
54.07 76.72

Top1
(%)

Top5
(%)

62.08 82.58
61.64 82.00
57.83 79.23
52.40 74.56
47.92 70.60
57.45 78.97
50.18 72.99

Table 5. Performance of DropBlock, DropCluster and Stochas-
ticDepth on validation data of Tiny ImageNet versus common
noise and blur corruptions with severity level 1.

7. Conclusion

In this work, we propose a new data-driven regularizer Drop-
Cluster for CNNs. Our approach is based on learning cor-
related and redundant information in the data by using a
clustering algorithm. DropCluster drops spatially correlated
features in structured and all features in unstructured fea-
ture maps. We also propose a Spatial Hopkins statistic to
evaluate cluster tendency. Our statistic does not depend
on the clustering algorithm or the number of clusters. We
show that convolutional layers exhibit cluster tendency via
our Spatial Hopkins statistic and visual inspection. Our
results on CIFAR-100 and Tiny ImageNet demonstrate that
our approach performs better than the alternatives and is
more robust to small-sample size of training data and cor-
ruption in new data. Our ﬁndings indicate that the structure
in convolutional layers can be leveraged for training CNNs
to achieve better performance and robustness.

References

Aydore, S., Thirion, B., and Varoquaux, G. Feature grouping
as a stochastic regularizer for high-dimensional structured
data. In International Conference on Machine Learning,

DropCluster: A structured dropout for convolutional networks

pp. 385–394, 2019.

Banerjee, A. and Dave, R. N. Validating clusters using the
hopkins statistic. In 2004 IEEE International conference
on fuzzy systems (IEEE Cat. No. 04CH37542), volume 1,
pp. 149–153. IEEE, 2004.

Consortium, . G. P. et al. A global reference for human

genetic variation. Nature, 526(7571):68–74, 2015.

CS231N, S. Tiny ImageNet Visual Recognition Chal-
lenge. https://tiny-imagenet.herokuapp.
com/, 2017. [Online; accessed February 2020].

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
L. Imagenet: A large-scale hierarchical image database.
In 2009 IEEE conference on computer vision and pattern
recognition, pp. 248–255. Ieee, 2009.

Dodge, S. and Karam, L. A study and comparison of human
and deep learning recognition performance under visual
In 2017 26th international conference on
distortions.
computer communication and networks (ICCCN), pp. 1–
7. IEEE, 2017.

Fan, J. and Li, R. Statistical challenges with high dimension-
ality: Feature selection in knowledge discovery. arXiv
preprint math/0602133, 2006.

Ghiasi, G., Lin, T.-Y., and Le, Q. V. Dropblock: A regular-
ization method for convolutional networks. In Advances
in Neural Information Processing Systems, pp. 10727–
10737, 2018.

He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings
in deep residual networks. In European conference on
computer vision, pp. 630–645. Springer, 2016.

He, Y., Zhang, X., and Sun, J. Channel pruning for acceler-
ating very deep neural networks. In Proceedings of the
IEEE International Conference on Computer Vision, pp.
1389–1397, 2017.

Hendrycks, D. and Dietterich, T. Benchmarking neural
network robustness to common corruptions and perturba-
tions. arXiv preprint arXiv:1903.12261, 2019.

Hendrycks, D. and Gimpel, K. Early methods for detecting
adversarial images. arXiv preprint arXiv:1608.00530,
2016.

Hendrycks, D., Mazeika, M., and Dietterich, T. Deep
anomaly detection with outlier exposure. arXiv preprint
arXiv:1812.04606, 2018.

Hosseini, H., Xiao, B., and Poovendran, R. Google’s cloud
In 2017 16th IEEE
vision api is not robust to noise.
International Conference on Machine Learning and Ap-
plications (ICMLA), pp. 101–105. IEEE, 2017.

Hoyos-Idrobo, A., Varoquaux, G., Kahn, J., and Thirion, B.
Recursive nearest agglomeration (rena): fast clustering
for approximation of structured signals. IEEE transac-
tions on pattern analysis and machine intelligence, 41(3):
669–681, 2018.

Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger,
K. Q. Deep networks with stochastic depth. In European
conference on computer vision, pp. 646–661. Springer,
2016.

Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger,
K. Q. Densely connected convolutional networks. In
Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 4700–4708, 2017.

Idrobo, A. H. Recursive nearest agglomeration (ReNA).
https://github.com/ahoyosid/ReNA, 2017.
[Online; accessed February 2020].

Krizhevsky, A., Hinton, G., et al. Learning multiple layers

of features from tiny images. 2009.

Liu, S., Garrepalli, R., Dietterich, T. G., Fern, A., and
Hendrycks, D. Open category detection with pac guaran-
tees. arXiv preprint arXiv:1808.00529, 2018.

Oliphant, T. E. Python for scientiﬁc computing. Computing

in Science & Engineering, 9(3), 2007.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., et al. Pytorch: An imperative style, high-performance
deep learning library. In Advances in Neural Information
Processing Systems, pp. 8024–8035, 2019.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., et al. Scikit-learn: Machine
learning in python. Journal of machine learning research,
12(Oct):2825–2830, 2011.

Ramos, M. V. Dropblock. https://github.com/
[Online; accessed

miguelvr/dropblock, 2019.
February 2020].

Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do
cifar-10 classiﬁers generalize to cifar-10? arXiv preprint
arXiv:1806.00451, 2018.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever,
I., and Salakhutdinov, R. Dropout: A simple way
Jour-
to prevent neural networks from overﬁtting.
nal of Machine Learning Research, 15:1929–1958,
URL http://jmlr.org/papers/v15/
2014.
srivastava14a.html.

DropCluster: A structured dropout for convolutional networks

Sutskever, I., Martens, J., Dahl, G., and Hinton, G. On the
importance of initialization and momentum in deep learn-
ing. In International conference on machine learning, pp.
1139–1147, 2013.

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich,
In Proceedings
A. Going deeper with convolutions.
of the IEEE conference on computer vision and pattern
recognition, pp. 1–9, 2015.

Tompson, J., Goroshin, R., Jain, A., LeCun, Y., and Bre-
gler, C. Efﬁcient object localization using convolutional
networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 648–656,
2015.

Walt, S. v. d., Colbert, S. C., and Varoquaux, G. The numpy
array: a structure for efﬁcient numerical computation.
Computing in Science & Engineering, 13(2):22–30, 2011.

Zhang, Z., Dalca, A. V., and Sabuncu, M. R. Conﬁdence
calibration for convolutional neural networks using struc-
tured dropout. arXiv preprint arXiv:1906.09551, 2019.

DropCluster: A structured dropout for convolutional networks

Figure 8. Histograms of Spatial Hopkins values from different layers. As the layer gets deeper, the center of histograms approach 0.5
indicating less structure.

Figure 9. Top1 % performance of different dropout models on validation data of Tiny ImageNet versus severity levels in corruption.

Figure 10. Top5 % performance of different dropout models on validation data of Tiny ImageNet versus severity levels in corruption.

