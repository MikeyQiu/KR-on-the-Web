7
1
0
2
 
r
a

M
 
8
2
 
 
]

G
L
.
s
c
[
 
 
1
v
1
9
3
9
0
.
3
0
7
1
:
v
i
X
r
a

Fast Optimization of Wildﬁre Suppression Policies with SMAC

Sean McGregor
School of Electrical Engineering and Computer Science, Oregon State University

ARXIV@SEANBMCGREGOR.COM

Rachel Houtman
College of Forestry, Oregon State University

Claire Montgomery
College of Forestry, Oregon State University

Ronald Metoyer
College of Engineering, University of Notre Dame

RACHEL.HOUTMAN@OREGONSTATE.EDU

CLAIRE.MONTGOMERY@OREGONSTATE.EDU

RMETOYER@ND.EDU

Thomas G. Dietterich
School of Electrical Engineering and Computer Science, Oregon State University

TGD@OREGONSTATE.EDU

Abstract

Managers of US National Forests must decide
what policy to apply for dealing with lightning-
caused wildﬁres. Conﬂicts among stakehold-
ers (e.g., timber companies, home owners, and
wildlife biologists) have often led to spirited po-
litical debates and even violent eco-terrorism.
One way to transform these conﬂicts into multi-
stakeholder negotiations is to provide a high-
ﬁdelity simulation environment in which stake-
holders can explore the space of alternative poli-
cies and understand the tradeoffs therein. Such
an environment needs to support fast optimiza-
tion of MDP policies so that users can adjust
reward functions and analyze the resulting op-
timal policies. This paper assesses the suitabil-
ity of SMAC—a black-box empirical function
optimization algorithm—for rapid optimization
of MDP policies. The paper describes ﬁve re-
ward function components and four stakeholder
constituencies. It then introduces a parameterized
class of policies that can be easily understood by
the stakeholders. SMAC is applied to ﬁnd the op-
timal policy in this class for the reward functions
of each of the stakeholder constituencies. The re-
sults conﬁrm that SMAC is able to rapidly ﬁnd
good policies that make sense from the domain
perspective. Because the full-ﬁdelity forest ﬁre
simulator is far too expensive to support inter-
active optimization, SMAC is applied to a sur-
rogate model constructed from a modest num-
ber of runs of the full-ﬁdelity simulator. To check

the quality of the SMAC-optimized policies, the
policies are evaluated on the full-ﬁdelity simula-
tor. The results conﬁrm that the surrogate values
estimates are valid. This is the ﬁrst successful op-
timization of wildﬁre management policies using
a full-ﬁdelity simulation. The same methodology
should be applicable to other contentious natu-
ral resource management problems where high-
ﬁdelity simulation is extremely expensive.

1. Introduction

When lightning ignites a ﬁre in the US National Forest sys-
tem, the forest manager must decide whether to suppress
that ﬁre or allow it to burn itself out. This decision has im-
mediate costs in terms of ﬁre ﬁghting expenses and smoke
pollution and long-term beneﬁts, including increased tim-
ber harvest revenue and reduced severity of future ﬁres.
Different stakeholders place different values on these var-
ious outcomes, and this leads to contentious and difﬁcult
policy debates. In the US Paciﬁc Northwest, a period in the
1990s is referred to as the “Timber Wars” because of the
troubling and occasionally violent conﬂicts that arose be-
tween stakeholder groups over forest management policies
during that period. This is typical of many ecosystem man-
agement problems—the complexity of ecosystem dynam-
ics and the broad array interested parties makes it difﬁcult
to identify politically-feasible policies.

One way that computer science can help is to provide a
high-ﬁdelity simulation environment in which stakeholders
can explore the policy space, experiment with different re-

Fast Optimization of Wildﬁre Suppression Policies with SMAC

ward functions, compute the resulting optimal policies, and
visualize the behavior of the ecosystem when it is managed
according to those policies. This process can elicit missing
aspects of the reward function, and it can help the stake-
holders reach a policy consensus that is informed by the
best available ecosystem models. To create such a simula-
tion environment, we need a tool that meets the following
requirements:

i. Modiﬁability: users should be able to modify the re-
ward function to represent the interests of various
stakeholders.

ii. Automatic Optimization: users should be able to op-
timize policies for the updated reward functions with-
out the involvement of a machine learning researcher.

iii. Visualization: users should be able to explore the be-
havior of the system when it is controlled by the opti-
mized policies.

iv. Interactivity: all these tasks should be performed at

interactive speeds.

Previous work by McGregor et al. (2015) presented an in-
teractive visualization system, MDPvis, that supports re-
quirements i and iii. However, MDPvis does not support
the optimization capability needed for requirement ii. Fur-
thermore, the full-ﬁdelity wildﬁre simulator is very slow,
so even if we had an optimization algorithm for this noisy,
high-dimensional problem, the optimization could not meet
the interactive speeds needed for requirement iv.

This paper and a companion paper (McGregor et al., 2017)
(simultaneously published on arXiv) address requirements
ii and iv. The companion paper shows how to create a sur-
rogate model that is able to accurately mimic the expen-
sive full-ﬁdelity simulator while running orders of magni-
tude faster. This addresses requirement iv. The current pa-
per studies whether the SMAC method for black-box func-
tion optimization (Hutter et al., 2010) can use this surrogate
model to rapidly ﬁnd near-optimal policies.

SMAC is similar to Bayesian methods for black box opti-
mization. However, unlike those methods, SMAC does not
employ a Gaussian process to model the black box func-
tion. Instead, it ﬁts a random forest ensemble (Breiman,
2001). This has three important beneﬁts. First, it does not
impose any smoothness assumption on the black box func-
tion. We will see below that wildﬁre policies are naturally
expressed in the form of decision trees, so they are highly
non-smooth. Second, SMAC does not require the design
of a Gaussian process kernel. This makes it more suit-
able for application by end users such as our policy stake-
holders. Third, the CPU time required for SMAC scales
as O(n log n) where n is the number of evaluations of the

black box function, whereas standard GP methods scale as
O(n3) because they must compute and invert the kernel
matrix.

This paper makes two contributions. First, it shows that
SMAC can rapidly ﬁnd high-scoring policies for a range
of different reward functions that incorporate both short-
term and long-term rewards. Second, it conﬁrms that this is
possible even though SMAC is using an approximate sur-
rogate for the high-ﬁdelity simulator. Taken together, these
contributions mark the ﬁrst successful optimization a wild-
ﬁre suppression policy for a full 100-year planning horizon.

The paper is structured as follows. First, we introduce our
notation and provide an overview of direct policy search
and SMAC. This is followed by a description of the ﬁre
management problem including a review of the differ-
ent components of the reward function and the relative
weight that different stakeholder constituencies place on
these components. We then describe the parameterized pol-
icy representation for wildﬁre management policies. The
results of applying SMAC to optimize these policies are
shown next. Finally, the surrogate estimates of the values
of these policies are checked by running them on the full-
ﬁdelity simulator. The results conﬁrm the accuracy of the
surrogate estimates.

2. Direct Policy Search Methods

We work with the standard ﬁnite horizon discounted MDP
(Bellman, 1957; Puterman, 1994), denoted by the tuple
M = (cid:104)S, A, P, R, P0, h, γ(cid:105). S is a ﬁnite set of states of the
world; A is a ﬁnite set of possible actions that can be taken
in each state; P : S × A × S (cid:55)→ [0, 1] is the conditional
probability of entering state s(cid:48) when action a is executed in
state s; R(s, a) is the ﬁnite reward received after perform-
ing action a in state s; P0 is the distribution over starting
states; h is the horizon; and γ is the discount factor.

Let Π be a class of deterministic policies with an associated
parameter space Θ. Each parameter vector θ ∈ Θ deﬁnes a
policy πθ : S (cid:55)→ A that speciﬁes what action to take in each
state. Let τ = (cid:104)s0, s1, . . . , sh(cid:105) be a trajectory generated
by drawing a state s0 ∼ P0(s0) according to the starting
state distribution and then following policy πθ for h steps.
Let ρ = (cid:104)r0, . . . , rh−1(cid:105) be the corresponding sequence of
rewards. Both τ and ρ are random variables because they
reﬂect the stochasticity of the starting state and the proba-
bility transition function. Let Vθ deﬁne the expected cumu-
lative discounted return of applying policy πθ starting in a
state s0 ∼ P0(s0) and executing it for h steps:

Vθ = Eρ[r0 + γr1 + γ2r2 + · · · + γh−1rh−1]

The goal of direct policy search is to ﬁnd θ∗ that maximizes

Fast Optimization of Wildﬁre Suppression Policies with SMAC

the value of the corresponding policy:

θ∗ = argmax

Vθ.

θ∈Θ

Two prominent forms of policy search are policy gradi-
ent methods and sequential model-based optimization. Pol-
icy gradient methods (Williams, 1992; Sutton et al., 2000;
Deisenroth, 2011; Schulman et al., 2015) estimate the gra-
dient of Vθ with respect to θ and then take steps in param-
eter space to ascend the gradient. This is often challenging
because the estimate is based on Monte Carlo samples of τ
and because gradient search only guarantees to ﬁnd a local
optimum.

Sequential model-based optimization methods (Kushner,
1964; Mockus, 1994; Zilinskas, 1992; Hutter et al., 2010;
Srinivas et al., 2010; Wilson et al., 2014; Wang et al., 2016)
construct a model of Vθ called a Policy Value Model and
denoted P V M (θ). The PVM estimates both the value of
Vθ and a measure of the uncertainty of that value. The most
popular form of PVM is the Gaussian Process, which mod-
els Vθ as the GP mean and the uncertainty as the GP vari-
ance. The basic operation of sequential model-based opti-
mization methods is to select a new point θ at which to im-
prove the PVM, observe the value of Vθ at that point (e.g.,
by simulating a trajectory τ using πθ), and then update the
PVM to reﬂect this new information. In Bayesian Opti-
mization, the PVM is initialized with a prior distribution
over possible policy value functions and then updated after
each observation by applying Bayes rule. The new points θ
are selected by invoking an acquisition function.

SMAC (Hutter et al., 2010) is a sequential model-based op-
timization method in which the PVM is a random forest of
regression trees. The estimated value of Vθ is obtained by
“dropping” θ through each of the regression trees until it
reaches a leaf in each tree and then computing the mean and
the variance of the training data points stored in all of those
leaves. In each iteration, SMAC evaluates Vθ at 10 different
values of θ, adds the observed values to its database R of
(θ, Vθ) pairs, and then rebuilds the random forest.

SMAC chooses 5 of the 10 θ values with the goal of ﬁnd-
ing points that have high “generalized expected improve-
ment”. The (ordinary) expected improvement at point θ is
the expected increase in the maximum value of the PVM
that will be observed when we measure Vθ under the as-
sumption that Vθ has a normal distribution whose mean is
µθ (the current PVM estimate of the mean at θ) and whose
variance is σ2
θ (the PVM estimate of the variance at θ). The
expected improvement at θ can be computed as

EI(θ) := E(cid:2)I(θ)(cid:3) = σθ

(cid:2)z· Φ(z) + φ(z)(cid:3),

(1)

where z := µθ−fmax
, fmax is the largest known value
of the current PVM, Φ denotes the cumulative distribution

σθ

Figure 1. The landscape totals approximately one million pixels,
each of which has 13 state variables that inﬂuence the spread of
wildﬁre on the landscape. (Map is copyright of OpenStreetMap
contributors)

function of the standard normal distribution, and φ denotes
the probability density function of the standard normal dis-
tribution (Jones et al., 1998).

The generalized expected improvement (GEI) is obtained
by computing the expected value of I(θ) raised to the g-th
power. In SMAC, g is set to 2. Hutter et al. (2010) show
that this can be computed as

GEI(θ) = E(cid:2)I 2(θ)(cid:3) = σ2

(cid:2)(z2 +1)· Φ(z)+z· φ(z)(cid:3). (2)

θ

Ideally, SMAC would ﬁnd the value of θ that maximizes
GEI(θ) and then evaluate Vθ at that point. However, this
would require a search in the high-dimensional space of
Θ and it would also tend to focus on a small region of
Θ. Instead, SMAC employs the following heuristic strat-
egy to ﬁnd 10 candidate values of θ. First, it performs a lo-
cal search in the neighborhood of the 10 best known values
of θ in the PVM. This provides 10 candidate values. Next,
it randomly generates another 10,000 candidate θ vectors
from Θ and evaluates the GEI of each of them. Finally, it
chooses the 5 best points from these 10,010 candidates and
5 points sampled at random from the 10,000 random can-
didates, and evaluates Vθ at each of these 10 points. This
procedure mixes “exploration” (the 5 random points) with
“exploitation” (the 5 points with maximum GEI), and it has
been found empirically to work well.

Hutter et al. (2010) prove that the SMAC PVM is a consis-
tent estimator of V and that given an unbounded number of
evaluations of V , it ﬁnds the optimal value θ∗.

3. The Wildﬁre Management Domain

To evaluate our methods, we have selected a portion of
the Deschutes National Forest in Eastern Oregon. This area
was selected because it is being managed with the goal of

Fast Optimization of Wildﬁre Suppression Policies with SMAC

reward function constituencies.

restoring the landscape to the condition it was believed to
be in prior to the arrival of Europeans. Figure 1 shows a
map of the study site. It is comprised of approximately one
million pixels, each described by 13 state variables.

We employ the high-ﬁdelity simulator described in Hout-
man et al. (2013), which combines a simple model of the
spatial distribution of lightning strikes (based on histori-
cal data) with the state-of-the-art Farsite ﬁre spread simu-
lator (Finney, 1998), a ﬁre duration model (Finney et al.,
2009), and the high-resolution FVS forest growth simu-
lator (Dixon, 2002). Weather is simulated by resampling
from the historical weather time series observed at a nearby
weather station.

The MDP advances in a sequence of decision points. Each
decision point corresponds to a lightning-caused ignition,
and the MDP policy must decide between two possible ac-
tions: Suppress (ﬁght the ﬁre) and Let Burn (do nothing).
Hence, |A| = 2. Based on the chosen action and the (simu-
lated) weather, the intensity, spread, and duration of the ﬁre
is determined by the simulator. In order to capture the long-
term impacts of our policy decisions, we employ a planning
horizon of h = 100 years.

Unfortunately, the simulator is very expensive. Simulating
a 100-year trajectory of ﬁres can take up to 7 hours of clock
time. This is obviously too slow for interactive use. We
therefore have adopted a surrogate modeling approach in
which, during the optimization, we replace the full-ﬁdelity
model with a much more efﬁcient approximation. Our sur-
rogate model is based on the Model Free Monte Carlo
(MFMC) method of Fonteneau et al. (2013). MFMC gen-
erates trajectories for a new policy by combining short seg-
ments of trajectories previously computed by the full sim-
ulator for other policies. It relies on a similarity metric to
match segments to each other. Because this metric does not
work well in a high-dimensional state-and-action space, the
companion paper describes an extension of MFMC called
MFMCi (MFMC with independencies) that exploits certain
conditional independencies to reduce the dimensionality of
the similarity metric calculations (McGregor et al., 2017).

Wildﬁres produce a variety of immediate and long term
losses and beneﬁts. For market-based rewards, such as sup-
pression costs and timber revenues, there is a single defen-
sible reward function. Since wildﬁre decisions also affect
many outcomes for which there is no ﬁnancial market, such
as air, water, ecology, and recreation, there are potentially
many different composite reward functions. A beneﬁt of the
random forest method is the ability to change the reward
function and re-optimize without making any assumptions
about the character of the response surface V . In our exper-
imental evaluation, we optimize and validate policies for
four different reward functions as an approximation of dif-
ferent stakeholder interests. Table 1 details each of these

C osts
Tim ber Revenues
Suppression
Ecology

Target
Air Q uality

Recreation

Target

Constituency
Composite
Politics
Home Owners
Timber

(cid:88)
-
-
(cid:88)

(cid:88)
(cid:88)
-
(cid:88)

(cid:88)
(cid:88)
-
-

(cid:88)
(cid:88)
(cid:88)
-

(cid:88)
(cid:88)
(cid:88)
-

Table 1. Components of each reward function. The “politics” con-
stituency approximates a decision maker that is not responsi-
ble for funding ﬁreﬁghting operations. The “home owner” con-
stituency only cares about air quality and recreation. The “timber”
companies only care about how much timber they harvest, and
how much money they spend protecting that timber. The “com-
posite” reward function takes an unweighted sum of all the costs
and revenues produced for the constituencies. Additional reward
functions can be speciﬁed by users interactively within MDPvis.

The reward functions are compositions of ﬁve different re-
ward components. The Suppression component gives the
expenses incurred for suppressing a ﬁre. Fire suppression
expenses increase with ﬁre size and the number of days the
ﬁre burns. Without ﬁre suppression effort, the ﬁre suppres-
sion costs are zero, but the ﬁre generally takes longer to
self-extinguish. Timber harvest values are determined by
the number of board feet harvested from the landscape.
A harvest scheduler included in the simulator determines
the board feet based on existing forest practice regulations.
Generally we would expect timber harvest to increase with
suppression efforts, but complex interactions between the
harvest scheduler and tree properties (size, age, species)
often results in high timber harvests following a ﬁre. Eco-
logical value is a function of the squared deviation from
an ofﬁcially-speciﬁed target distribution of vegetation on
the landscape known as the “restoration target.” Since our
landscape begins in a state that has a recent history of ﬁre
suppression efforts, there is much more vegetation than the
target. A good way to reach the target is to allow wild-
ﬁres to burn, but increased timber harvest can also con-
tribute to this goal. Air Quality is a function of the num-
ber of days a wildﬁre burns. When ﬁres burn, the smoke
results in a large number of home-owner complaints. We
encode this as a negative reward for each smoky day. Fi-
nally, the recreation component penalizes the squared devi-
ation from a second vegetation target distribution—namely,
one preferred by people hiking and camping. This distribu-
tion consists of old, low-density ponderosa pine trees. Fre-
quent, low-intensity ﬁres produce this kind of distribution,
because they burn out the undergrowth while leaving the
ﬁre-resilient ponderosa pine unharmed. If we optimize for
any single reward component, the optimal policy will tend
to be one of the trivial policies “suppress-all” or “letburn-

Fast Optimization of Wildﬁre Suppression Policies with SMAC

all”. When multiple reward components are included, the
optimal policy still tends to either suppress or let burn most
ﬁres by default, but it tries to identify exceptional ﬁres
where the default should be overridden. See Houtman et al.
(2013) for a discussion of this phenomenon.

A natural policy class in this domain takes the form of a bi-
nary decision tree as shown in Figure 2. At each level of the
tree, the variable to split on is ﬁxed in this policy class. With
the exception of the very ﬁrst split at the root, which has a
hard-coded threshold, the splitting thresholds θ1, . . . , θ14
are all adjusted by the policy optimization process. Mov-
ing from the top layer of the tree to the bottom, the tree
splits ﬁrst on whether the ﬁre will be extinguished within
the next 8 days by a “ﬁre-ending weather event” (i.e., sub-
stantial rain or snowfall). The threshold value of 8 is ﬁxed
(based on discussions with land managers and on the pre-
dictive scope of weather forecasts). The next layer splits on
the state of fuel accumulation on the landscape. The fuel
level is compared either to θ1 (left branch, no weather event
predicted within 8 days) or θ2 (right branch; weather pre-
dicted within 8 days). When the fuel level is larger than
the corresponding threshold, the right branch of the tree is
taken. The next layer splits on the intensity of the ﬁre at
the time of the ignition. In this ﬁre model, the ﬁre intensity
is quantiﬁed in terms of the Energy Release Component
(ERC), which is a common composite measure of dryness
in fuels. Finally, the last layer of the tree asks whether the
current date is close to the start or end of the ﬁre season.
Our study region in Eastern Oregon is prone to late spring
and early fall rains, which means ﬁres near the boundary
of the ﬁre season are less likely burn very long. We note
that this policy function is difﬁcult for gradient-based pol-
icy search, because it is not differentiable and exhibits com-
plex responses to parameter changes.

4. Experiments

To train the MFMCi surrogate model, we sampled 360 poli-
cies from a policy class that suppresses wildﬁres based on
the ERC at the time of the ignition and the day of the ig-
nition (see supporting materials for details). Every query
to the MFMCi surrogate generates 30 trajectories, and the
mean cumulative discounted reward is returned as the ob-
served value of Vθ.

We apply SMAC with its default conﬁguration. When
SMAC grows a random regression tree for its PVM, there is
a parameter that speciﬁes the fraction of the parameter di-
mensions (i.e., θ1, . . . , θ14) that should be considered when
splitting a regression tree node. We set this parameter to
5/6. A second parameter determines when to stop splitting,
namely: a random forest node can only be split if it contains
at least 10 examples. Finally, the size of the random forest
is set to 10 trees.

Figure 2. The layers of the decision tree used to select wildﬁres
for suppression. The top layer splits on whether the ﬁre will likely
be extinguished by a storm in the next 8 days regardless of the
suppression decision. The next layers then have 14 parameters for
the number of pixels that are in high fuel (parameters 1 and 2,
(cid:2)0, 1000000(cid:3)), the intensity of the weather conditions at the time
of the ﬁre (3 through 6, (cid:2)0, 100(cid:3)), and a threshold that determines
whether the ﬁre is close to either the start or end of the ﬁre season
(7 through 14, (cid:2)0, 90(cid:3)).

Figure 3 shows the results of applying SMAC to ﬁnd op-
timal policies for the four reward function constituencies.
The left column of plots show the order in which SMAC
explores the policy parameter space. The vertical axis is the
estimated cumulative discounted reward, and the point that
is highest denotes the ﬁnal policy output by SMAC. Blue
points are policy parameter vectors chosen by the GEI ac-
quisition function whereas red points are parameter vectors
chosen by SMAC’s random sampling process. Notice that
in all cases, SMAC rapidly ﬁnds a fairly good policy. The
right column of plots gives us some sense of how the dif-
ferent policies behave. Each plot sorts the evaluated policy
parameter vectors according to the percentage of ﬁres that
each policy suppresses. In 3(b), we see that the highest-
scoring policies allow almost all ﬁres to burn, whereas in
3(f), the highest-scoring policies suppress about 80% of the
ﬁres.

Let us examine these policies in more detail. The optimal
policies for the politics and timber reward constituencies
allow most wildﬁres to burn, but for different reasons. For
the politics constituency, it is the Ecological reward that en-
courages this choice, whereas for the timber constituency, it
is the increased harvest levels that result. The composite re-
ward function produces a very similar optimal policy, pre-
sumably because it contains both the Ecological and Har-
vest reward components. These results indicate that timber
company and political interests largely coincide.

The most interesting case is the home owner constituency
reward function, which seeks to minimize smoke (which

Fast Optimization of Wildﬁre Suppression Policies with SMAC

suggests suppressing all ﬁres) and maximize recreation
value (which suggests allowing ﬁres to burn the understory
occasionally). We can see in 3(f) that the best policy found
by SMAC allows 20% of ﬁres to burn and suppresses the
remaining 80%.

These results agree with our intuition for each reward con-
stituency and provide evidence that SMAC is succeeding
in optimizing these policies. However, the expected dis-
counted rewards in Figure 3 are estimates obtained from
the modiﬁed MFMC surrogate model. To check that these
estimates are valid, we invoked each optimal policy on the
full simulator at least 50 times and measured the cumula-
tive discounted reward under each of the reward functions.
We also evaluated the Suppress All and Let Burn All poli-
cies for comparison. The results are shown in Figure 4.

Each panel of boxplots depicts the range of cumulative re-
turns for each policy on one reward function. For the pol-
icy that SMAC constructed, we also plot a dashed red line
showing the MFMC estimate of the return. In all cases, the
estimates are systematically biased high. This is to be ex-
pected, because any optimization against a noisy function
will tend to ﬁt the noise and, hence, over-estimate the true
value of the function. Nonetheless, the MFMC estimates all
fall within the inter-quartile range of the full simulator es-
timates. This conﬁrms that the MFMC estimates are giving
an accurate picture of the true rewards.

Note that because of the stochasticity of this domain, us-
ing only 50 trajectories from the full simulator is in gen-
eral not sufﬁcient to determine which policy is optimal for
each reward function. The only clear case is for the home
owner reward function where the SMAC-optimized policy
is clearly superior to all of the other policies.

5. Discussion

Previous work on high-ﬁdelity wildﬁre management
(Houtman et al., 2013) has focused only on policy evalu-
ation, in which the full simulator was applied to evaluate a
handful of alternative policies. This paper reports the ﬁrst
successful policy optimization for wildﬁre suppression at
scale. This paper demonstrates that SMAC applied to our
MFMC-based surrogate model is able to ﬁnd high-quality
policies for four different reward functions and do so at
interactive speeds. This combination of optimization efﬁ-
ciency and robust ease of use has the potential to provide a
basis for interactive decision support that can help diverse
groups of stakeholders explore the policy ramiﬁcations of
different reward functions and perhaps reach consensus on
wildﬁre management policies.

Acknowledgment

This material is based upon work supported by the National
Science Foundation under Grant No. 1331932.

References

Bellman, Richard. Dynamic Programming. Princeton Uni-

versity Press, New Jersey, 1957.

Breiman, Leo. Random Forests. Machine learning, 45(1):

5–32, 2001.

Deisenroth, Marc Peter.
Search for Robotics.
in Robotics, 2(2011):1–142, 2011.
8253.
doi: 10.1561/2300000021.
//www.nowpublishers.com/articles/
foundations-and-trends-in-robotics/
ROB-021.

A Survey on Policy
Foundations and Trends
ISSN 1935-
URL http:

Dixon, G. Essential FVS : A User’s Guide to the Forest Veg-
etation Simulator. USDA Forest Service, Fort Collins,
CO, 2002. ISBN 9780903024976.

Finney, Mark, Grenfell, Isaac C, and McHugh, Charles W.
Modeling containment of large wildﬁres using general-
ized linear mixed-model analysis. Forest Science, 55(3):
249–255, 2009.

Finney, Mark A. FARSITE: ﬁre area simulator model de-
velopment and evaluation. USDA Forest Service, Rocky
Mountain Research Station, Missoula, MT, 1998.

Fonteneau, Raphael, Murphy, Susan a, Wehenkel, Louis,
and Ernst, Damien. Batch Mode Reinforcement Learn-
ing based on the Synthesis of Artiﬁcial Trajectories. An-
nals of Operations Research, 208(1):383–416, Sep 2013.
ISSN 0254-5330. doi: 10.1007/s10479-012-1248-5.

Houtman, Rachel M., Montgomery, Claire A., Gagnon,
Aaron R., Calkin, David E., Dietterich, Thomas G., Mc-
Gregor, Sean, and Crowley, Mark. Allowing a Wildﬁre
to Burn: Estimating the Effect on Future Fire Suppres-
sion Costs. International Journal of Wildland Fire, 22
(7):871–882, 2013.

Hutter, Frank, Hoos, Holger H., and Leyton-Brown, Kevin.
Sequential Model-Based Optimization for General Al-
gorithm Conﬁguration (extended version). University
of British Columbia, Department of Computer Science,
ISSN 03029743. doi: 10.1007/
pp. 507–523, 2010.
978-3-642-25566-3 40.

Jones, Donald R, Schonlau, Matthias, and Welch,
William J. Efﬁcient Global Optimization of Expensive
Black-Box Functions. Journal of Global Optimization,
13:455–492, 1998.
ISSN 09255001. doi: 10.1023/a:
1008306431147.

Fast Optimization of Wildﬁre Suppression Policies with SMAC

(a) Composite reward function.

(b) Composite reward function.

(c) Politics reward function.

(d) Politics reward function.

(e) Home owner reward function.

(f) Home owners reward function.

(g) Timber reward function.

(h) Timber reward function.

Figure 3. Discounted reward achieved for each reward function averaged over 30 trajectories. The blue diamonds are samples selected
according to the EI heuristic, and the red diamonds are randomly sampled points.

Fast Optimization of Wildﬁre Suppression Policies with SMAC

(a) Policies for composite reward function.

(b) Policies for politics reward function.

(c) Policies for home owner reward function.

(d) Policies for timber reward function.

Figure 4. Each set of box charts show the performance of various policies for a constituency. The individual box plots show the expected
discounted reward for each of the policies optimized for a constituency, as well as the hard-coded policies of suppress-all and let-burn-all.
The red dashed lines indicate the expected discounted reward estimated by MFMCi.

Fast Optimization of Wildﬁre Suppression Policies with SMAC

Kushner, H. J. A new method of locating the maximum
point of an arbitrary multipeak curve in the presence of
noise. Journal of Basic Engineering, 86:97–106, 1964.

McGregor,

Sean, Buckingham, Hailey, Dietterich,
Thomas G., Houtman, Rachel, Montgomery, Claire, and
Metoyer, Ron. Facilitating Testing and Debugging of
Markov Decision Processes with Interactive Visualiza-
In IEEE Symposium on Visual Languages and
tion.
Human-Centric Computing, Atlanta, 2015.

McGregor, Sean, Houtman, Rachel, Montgomery, Claire,
Metoyer, Ronald, and Dietterich, Thomas G. Factoring
Exogenous State for Model-Free Monte Carlo. 2017.

Mockus, J. Application of bayesian approach to numerical
methods of global and stochastic optimization. Journal
of Global Optimization, 4:347–365, 1994.

Puterman, Martin. Markov Decision Processes: Discrete
Stochastic Dynamic Programming. Wiley-Interscience,
1st edition, 1994.

Schulman, John, Levine, Sergey, Jordan, Michael, and
In
Abbeel, Pieter. Trust Region Policy Optimization.
International Conference on Machine Learning, 2015.
ISBN 0375-9687. doi: 10.1063/1.4927398.

Srinivas, Niranjan, Krause, Andreas, Kakade, Sham M.,
and Seeger, Matthias. Gaussian Process Optimization in
the Bandit Setting: No Regret and Experimental Design.
Proceedings of the 27th International Conference on
Machine Learning (ICML 2010), pp. 1015–1022, 2010.
ISSN 00189448. doi: 10.1109/TIT.2011.2182033.

Sutton, R S, Mcallester, D, Singh, S, and Mansour, Y. Pol-
icy gradient methods for reinforcement learning with
function approximation. Advances in Neural Informa-
tion Processing Systems, 12:1057–1063, 2000.

Wang, Ziyu, Hutter, Frank, Zoghi, Masrour, Matheson,
David, and de Freitas, Nando. Bayesian optimization in
high dimensions via random embeddings. Journal of Ar-
tiﬁcial Intelligence Research, 55:361–387, 2016. ISSN
10450823. doi: 10.1613/jair.4806.

Williams, Ronald J. Simple statistical gradient-following
algorithms for connectionist reinforcement
learning.
Machine Learning, 8:229–256, 1992. ISSN 08856125.
doi: 10.1007/BF00992696.

Wilson, Aaron, Fern, Alan, and Tadepalli, P. Using Tra-
jectory Data to Improve Bayesian Optimization for Re-
inforcement Learning. Journal of Machine Learning Re-
search, 15:253–282, 2014. ISSN 1532-4435.

Zilinskas, A. A review of statistical models for global op-
timization. Journal of Global Optimization, 2:145–153,
1992.

