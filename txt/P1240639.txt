9
1
0
2
 
n
u
J
 
9
1
 
 
]

G
L
.
s
c
[
 
 
8
v
0
2
5
3
0
.
2
0
5
1
:
v
i
X
r
a

RAND-WALK: A latent variable model approach to word
embeddings

Sanjeev Arora

Yuanzhi Li

Yingyu Liang

Tengyu Ma

Andrej Risteski ∗

Abstract

Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse
methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparam-
eters and reweighting methods.

This paper proposes a new generative model, a dynamic version of the log-linear topic model of Mnih
and Hinton (2007). The methodological novelty is to use the prior to compute closed form expressions
for word statistics. This provides a theoretical justiﬁcation for nonlinear models like PMI, word2vec,
and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic
embeddings contain linear algebraic structure that allows solution of word analogies, as shown by Mikolov
et al. (2013a) and many subsequent papers.

Experimental support is provided for the generative model assumptions, the most important of which

is that latent word vectors are fairly uniformly dispersed in space.

1 Introduction

Vector representations of words (word embeddings) try to capture relationships between words as distance or
angle, and have many applications in computational linguistics and machine learning. They are constructed
by various models whose unifying philosophy is that the meaning of a word is deﬁned by “the company it
keeps” (Firth, 1957), namely, co-occurrence statistics. The simplest methods use word vectors that explicitly
represent co-occurrence statistics. Reweighting heuristics are known to improve these methods, as is dimen-
sion reduction (Deerwester et al., 1990). Some reweighting methods are nonlinear, which include taking the
square root of co-occurrence counts (Rohde et al., 2006), or the logarithm, or the related Pointwise Mutual
Information (PMI) (Church and Hanks, 1990). These are collectively referred to as Vector Space Models,
surveyed in (Turney and Pantel, 2010).

Neural network language models (Rumelhart et al., 1986; 1988; Bengio et al., 2006; Collobert and Weston,
2008a) propose another way to construct embeddings: the word vector is simply the neural network’s internal
representation for the word. This method is nonlinear and nonconvex. It was popularized via word2vec, a
family of energy-based models in (Mikolov et al., 2013b;c), followed by a matrix factorization approach called
GloVe (Pennington et al., 2014). The ﬁrst paper also showed how to solve analogies using linear algebra on
word embeddings. Experiments and theory were used to suggest that these newer methods are related to
the older PMI-based models, but with new hyperparameters and/or term reweighting methods (Levy and
Goldberg, 2014b).

But note that even the old PMI method is a bit mysterious. The simplest version considers a symmetric
matrix with each row/column indexed by a word. The entry for (w, w(cid:48)) is PMI(w, w(cid:48)) = log p(w,w(cid:48))
p(w)p(w(cid:48)) , where
p(w, w(cid:48)) is the empirical probability of words w, w(cid:48) appearing within a window of certain size in the corpus,
and p(w) is the marginal probability of w. (More complicated models could use asymmetric matrices with

∗Princeton University, Computer Science Department. {arora,yuanzhil,yingyul,tengyu,risteski}@cs.princeton.edu.
This work was supported in part by NSF grants CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, Simons Investigator
Award, and Simons Collaboration Grant. Tengyu Ma was also supported by Simons Award for Graduate Students in Theoretical
Computer Science and IBM PhD Fellowship.

1

columns corresponding to context words or phrases, and also involve tensorization.) Then word vectors are
obtained by low-rank SVD on this matrix, or a related matrix with term reweightings. In particular, the
PMI matrix is found to be closely approximated by a low rank matrix: there exist word vectors in say 300
dimensions, which is much smaller than the number of words in the dictionary, such that

(cid:104)vw, vw(cid:48)(cid:105) ≈ PMI(w, w(cid:48))

(1.1)

where ≈ should be interpreted loosely.

There appears to be no theoretical explanation for this empirical ﬁnding about the approximate low
rank of the PMI matrix. The current paper addresses this. Speciﬁcally, we propose a probabilistic model
of text generation that augments the log-linear topic model of Mnih and Hinton (2007) with dynamics, in
the form of a random walk over a latent discourse space. The chief methodological contribution is using the
model priors to analytically derive a closed-form expression that directly explains (1.1); see Theorem 2.2
in Section 2. Section 3 builds on this insight to give a rigorous justiﬁcation for models such as word2vec
and GloVe, including the hyperparameter choices for the latter. The insight also leads to a mathematical
explanation for why these word embeddings allow analogies to be solved using linear algebra; see Section 4.
Section 5 shows good empirical ﬁt to this model’s assumtions and predictions, including the surprising one
that word vectors are pretty uniformly distributed (isotropic) in space.1

1.1 Related work

Latent variable probabilistic models of language have been used for word embeddings before, including
Latent Dirichlet Allocation (LDA) and its more complicated variants (see the survey (Blei, 2012)), and some
neurally inspired nonlinear models (Mnih and Hinton, 2007; Maas et al., 2011). In fact, LDA evolved out of
eﬀorts in the 1990s to provide a generative model that “explains” the success of older vector space methods
like Latent Semantic Indexing (Papadimitriou et al., 1998; Hofmann, 1999). However, none of these earlier
generative models has been linked to PMI models.

Levy and Goldberg (2014b) tried to relate word2vec to PMI models. They showed that if there were no
dimension constraint in word2vec, speciﬁcally, the “skip-gram with negative sampling (SGNS)” version of the
model, then its solutions would satisfy (1.1), provided the right hand side were replaced by PMI(w, w(cid:48)) − β
for some scalar β. However, skip-gram is a discriminative model (due to the use of negative sampling), not
generative. Furthermore, their argument only applies to very high-dimensional word embeddings, and thus
does not address low-dimensional embeddings, which have superior quality in applications.

Hashimoto et al. (2016) focuses on issues similar to our paper. They model text generation as a random
walk on words, which are assumed to be embedded as vectors in a geometric space. Given that the last word
produced was w, the probability that the next word is w(cid:48) is assumed to be given by h(|vw − vw(cid:48)|2) for a
suitable function h, and this model leads to an explanation of (1.1). By contrast our random walk involves
a latent discourse vector, which has a clearer semantic interpretation and has proven useful in subsequent
work, e.g. understanding structure of word embeddings for polysemous words Arora et al. (2016). Also our
work clariﬁes some weighting and bias terms in the training objectives of previous methods (Section 3) and
also the phenomenon discussed in the next paragraph.

Researchers have tried to understand why vectors obtained from the highly nonlinear word2vec models
exhibit linear structures (Levy and Goldberg, 2014a; Pennington et al., 2014). Speciﬁcally, for analogies like
“man:woman::king:??,” queen happens to be the word whose vector vqueen is the most similar to the vector
vking − vman + vwoman. This suggests that simple semantic relationships, such as masculine vs feminine
tested in the above example, correspond approximately to a single direction in space, a phenomenon we will
henceforth refer to as relations=lines.

Section 4 surveys earlier attempts to explain this phenomenon and their shortcoming, namely, that they
ignore the large approximation error in relationships like (1.1). This error appears larger than the diﬀerence
between the best solution and the second best (incorrect) solution in analogy solving, so that this error could
in principle lead to a complete failure in analogy solving. In our explanation, the low dimensionality of the

1The code is available at https://github.com/PrincetonML/SemanticVector

2

word vectors plays a key role. This can also be seen as a theoretical explanation of the old observation that
dimension reduction improves the quality of word embeddings for various tasks. The intuitive explanation
often given —that smaller models generalize better—turns out to be fallacious, since the training method
for creating embeddings makes no reference to analogy solving. Thus there is no a priori reason why low-
dimensional model parameters (i.e., lower model capacity) should lead to better performance in analogy
solving, just as there is no reason they are better at some other unrelated task like predicting the weather.

1.2 Beneﬁts of generative approaches

In addition to giving some form of “uniﬁcation” of existing methods, our generative model also brings
more intepretability to word embeddings beyond traditional cosine similarity and even analogy solving. For
example, it led to an understanding of how the diﬀerent senses of a polysemous word (e.g., bank) reside in
linear superposition within the word embedding (Arora et al., 2016). Such insight into embeddings may
prove useful in the numerous settings in NLP and neuroscience where they are used.

Another new explanatory feature of our model is that low dimensionality of word embeddings plays a key
theoretical role —unlike in previous papers where the model is agnostic about the dimension of the embed-
dings, and the superiority of low-dimensional embeddings is an empirical ﬁnding (starting with Deerwester
et al. (1990)). Speciﬁcally, our theoretical analysis makes the key assumption that the set of all word vectors
(which are latent variables of the generative model) are spatially isotropic, which means that they have no
preferred direction in space. Having n vectors be isotropic in d dimensions requires d (cid:28) n. This isotropy is
needed in the calculations (i.e., multidimensional integral) that yield (1.1). It also holds empirically for our
word vectors, as shown in Section 5.

The isotropy of low-dimensional word vectors also plays a key role in our explanation of the rela-
tions=lines phenomenon (Section 4). The isotropy has a “puriﬁcation” eﬀect that mitigates the eﬀect of
the (rather large) approximation error in the PMI models.

2 Generative model and its properties

The model treats corpus generation as a dynamic process, where the t-th word is produced at step t. The
process is driven by the random walk of a discourse vector ct ∈ (cid:60)d. Its coordinates represent what is being
talked about.2 Each word has a (time-invariant) latent vector vw ∈ (cid:60)d that captures its correlations with
the discourse vector. We model this bias with a log-linear word production model:

Pr[w emitted at time t | ct] ∝ exp((cid:104)ct, vw(cid:105)).

(2.1)

The discourse vector ct does a slow random walk (meaning that ct+1 is obtained from ct by adding a small
random displacement vector), so that nearby words are generated under similar discourses. We are interested
in the probabilities that word pairs co-occur near each other, so occasional big jumps in the random walk
are allowed because they have negligible eﬀect on these probabilities.

A similar log-linear model appears in Mnih and Hinton (2007) but without the random walk. The linear
chain CRF of Collobert and Weston (2008b) is more general. The dynamic topic model of Blei and Laﬀerty
(2006) utilizes topic dynamics, but with a linear word production model. Belanger and Kakade (2015) have
proposed a dynamic model for text using Kalman Filters, where the sequence of words is generated from
Gaussian linear dynamical systems, rather than the log-linear model in our case.

The novelty here over such past works is a theoretical analysis in the method-of-moments tradition (Hsu
et al., 2012; Cohen et al., 2012). Assuming a prior on the random walk we analytically integrate out the
hidden random variables and compute a simple closed form expression that approximately connects the
model parameters to the observable joint probabilities (see Theorem 2.2). This is reminiscent of analysis of
similar random walk models in ﬁnance (Black and Scholes, 1973).

2This is a diﬀerent interpretation of the term “discourse” compared to some other settings in computational linguistics.

3

Model details. Let n denote the number of words and d denote the dimension of the discourse space,
where 1 ≤ d ≤ n. Inspecting (2.1) suggests word vectors need to have varying lengths, to ﬁt the empirical
ﬁnding that word probabilities satisfy a power law. Furthermore, we will assume that in the bulk, the word
vectors are distributed uniformly in space, earlier referred to as isotropy. This can be quantiﬁed as a prior
in the Bayesian tradition. More precisely, the ensemble of word vectors consists of i.i.d draws generated by
v = s · ˆv, where ˆv is from the spherical Gaussian distribution, and s is a scalar random variable. We assume
s is a random scalar with expectation τ = Θ(1) and s is always upper bounded by κ, which is another
constant. Here τ governs the expected magnitude of (cid:104)v, ct(cid:105), and it is particularly important to choose it to
be Θ(1) so that the distribution Pr[w|ct] ∝ exp((cid:104)vw, ct(cid:105)) is interesting.3 Moreover, the dynamic range of
word probabilities will roughly equal exp(κ2), so one should think of κ as an absolute constant like 5. These
details about s are important for realistic modeling but not too important in our analysis. (Furthermore,
readers uncomfortable with this simplistic Bayesian prior should look at Section 2.2 below.)

Finally, we clarify the nature of the random walk. We assume that the stationary distribution of the
random walk is uniform over the unit sphere, denoted by C. The transition kernel of the random walk can
d in (cid:96)2 norm.4
be in any form so long as at each step the movement of the discourse vector is at most (cid:15)2/
This is still fast enough to let the walk mix quickly in the space.

√

The following lemma (whose proof appears in the appendix) is central to the analysis.

It says that
under the Bayesian prior, the partition function Zc = (cid:80)
w exp((cid:104)vw, c(cid:105)), which is the implied normalization
in equation (2.1), is close to some constant Z for most of the discourses c. This can be seen as a plausible
theoretical explanation of a phenomenon called self-normalization in log-linear models: ignoring the partition
function or treating it as a constant (which greatly simpliﬁes training) is known to often give good results.
This has also been studied in (Andreas and Klein, 2014).

Lemma 2.1 (Concentration of partition functions). If the word vectors satisfy the Bayesian prior described
in the model details, then

Pr
c∼C

[(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z] ≥ 1 − δ,

(2.2)

for (cid:15)z = (cid:101)O(1/

n), and δ = exp(−Ω(log2 n)).

√

The concentration of the partition functions then leads to our main theorem (the proof is in the appendix).
The theorem gives simple closed form approximations for p(w), the probability of word w in the corpus, and
p(w, w(cid:48)), the probability that two words w, w(cid:48) occur next to each other. The theorem states the result for
the window size q = 2, but the same analysis works for pairs that appear in a small window, say of size 10,
as stated in Corollary 2.3. Recall that PMI(w, w(cid:48)) = log[p(w, w(cid:48))/(p(w)p(w(cid:48)))].

Theorem 2.2. Suppose the word vectors satisfy the inequality (2.2), and window size q = 2. Then,

for (cid:15) = O((cid:15)z) + (cid:101)O(1/d) + O((cid:15)2). Jointly these imply:

log p(w, w(cid:48)) =

− 2 log Z ± (cid:15),

(cid:107)vw + vw(cid:48)(cid:107)2
2
2d
(cid:107)vw(cid:107)2
2
2d

log p(w) =

− log Z ± (cid:15).

PMI (w, w(cid:48)) =

(cid:104)vw, vw(cid:48)(cid:105)
d

± O((cid:15)).

(2.3)

(2.4)

(2.5)

Remarks 1. Since the word vectors have (cid:96)2 norm of the order of
(cid:107)vw + vw(cid:48)(cid:107)2

d, for two typical word vectors vw, vw(cid:48),
2 is of the order of Θ(d). Therefore the noise level (cid:15) is very small compared to the leading

3A larger τ will make Pr[w|ct] too peaked and a smaller one will make it too uniform.
4 More precisely, the proof extends to any symmetric product stationary distribution C with sub-Gaussian coordinate

satisfying Ec

(cid:2)(cid:107)c(cid:107)2(cid:3) = 1, and the steps are such that for all ct, Ep(ct+1|ct)[exp(κ

d(cid:107)ct+1 − ct(cid:107))] ≤ 1 + (cid:15)2 for some small (cid:15)2.

√

√

4

2d (cid:107)vw + vw(cid:48)(cid:107)2

term 1
empirically we also ﬁnd higher error here.

2. For PMI however, the noise level O((cid:15)) could be comparable to the leading term, and

Remarks 2. Variants of the expression for joint probability in (2.3) had been hypothesized based upon
empirical evidence in Mikolov et al. (2013b) and also Globerson et al. (2007), and Maron et al. (2010) .

Remarks 3. Theorem 2.2 directly leads to the extension to a general window size q as follows:

Corollary 2.3. Let pq(w, w(cid:48)) be the co-occurrence probability in windows of size q, and PMIq(w, w(cid:48)) be the
corresponding PMI value. Then

log pq(w, w(cid:48)) =

− 2 log Z + γ ± (cid:15),

PMIq (w, w(cid:48)) =

+ γ ± O((cid:15)).

(cid:107)vw + vw(cid:48)(cid:107)2
2
2d

(cid:104)vw, vw(cid:48)(cid:105)
d

where γ = log

(cid:16) q(q−1)
2

(cid:17)

.

It is quite easy to see that Theorem 2.2 implies the Corollary 2.3, as when the window size is q the
(cid:1) positions within the window, and the joint probability of w, w(cid:48) is
pair w, w(cid:48) could appear in any of (cid:0)q
roughly the same for any positions because the discourse vector changes slowly.
(Of course, the error
term gets worse as we consider larger window sizes, although for any constant size, the statement of the
theorem is correct.) This is also consistent with the shift β for ﬁtting PMI in (Levy and Goldberg, 2014b),
which showed that without dimension constraints, the solution to skip-gram with negative sampling satisﬁes
PMI (w, w(cid:48)) − β = (cid:104)vw, vw(cid:48)(cid:105) for a constant β that is related to the negative sampling in the optimization.
Our result justiﬁes via a generative model why this should be satisﬁed even for low dimensional word vectors.

2

2.1 Proof sketches

Here we provide the proof sketches, while the complete proof can be found in the appendix.

Proof sketch of Theorem 2.2 Let w and w(cid:48) be two arbitrary words. Let c and c(cid:48) denote two consecutive
context vectors, where c ∼ C and c(cid:48)|c is deﬁned by the Markov kernel p(c(cid:48) | c).

We start by using the law of total expectation, integrating out the hidden variables c and c(cid:48):

An expectation like (2.6) would normally be diﬃcult to analyze because of the partition functions.
However, we can assume the inequality (2.2), that is, the partition function typically does not vary much for
most of context vectors c. Let F be the event that both c and c(cid:48) are within (1 ± (cid:15)z)Z. Then by (2.2) and the
union bound, event F happens with probability at least 1 − 2 exp(−Ω(log2 n)). We will split the right-hand
side (RHS) of (2.6) into the parts according to whether F happens or not.

p(w, w(cid:48)) = E
c,c(cid:48)
= E
c,c(cid:48)

[Pr[w, w(cid:48)|c, c(cid:48)]]

[p(w|c)p(w(cid:48)|c(cid:48))]

= E
c,c(cid:48)

(cid:20) exp((cid:104)vw, c(cid:105))
Zc

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

(cid:21)

RHS of (2.6) = E
c,c(cid:48)
(cid:124)

(cid:20) exp((cid:104)vw, c(cid:105))
Zc

(cid:20) exp((cid:104)vw, c(cid:105))
Zc

+ E
c,c(cid:48)
(cid:124)

(cid:123)(cid:122)
T2

5

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

(cid:123)(cid:122)
T1
exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

1F

1 ¯F

(cid:21)

(cid:125)

(cid:21)

(cid:125)

(2.6)

(2.7)

where ¯F denotes the complement of event F and 1F and 1 ¯F denote indicator functions for F and ¯F ,
respectively. When F happens, we can replace Zc by Z with a 1 ± (cid:15)z factor loss: The ﬁrst term of the RHS
of (2.7) equals to

T1 =

1 ± O((cid:15)z)
Z 2

E
c,c(cid:48)

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ]

(2.8)

On the other hand, we can use E[1 ¯F ] = Pr[ ¯F ] ≤ exp(−Ω(log2 n)) to show that the second term of RHS

of (2.7) is negligible,

|T2| = exp(−Ω(log1.8 n)) .
This claim must be handled somewhat carefully since the RHS does not depend on d at all. Brieﬂy, the
d = o(log2 n)), any word vector vw and
reason this holds is as follows:
d)), and since E[1 ¯F ] = exp(−Ω(log2 n)), the
discourse c satisﬁes that exp((cid:104)vw, c(cid:105)) ≤ exp((cid:107)vw(cid:107)) = exp(O(
d = Ω(log2 n)), we can use concentration inequalities
claim follows directly; In the regime when d is large (
to show that except with a small probability exp(−Ω(d)) = exp(−Ω(log2 n)), a uniform sample from the
sphere behaves equivalently to sampling all of the coordinates from a standard Gaussian distribution with
mean 0 and variance 1

d , in which case the claim is not too diﬃcult to show using Gaussian tail bounds.

in the regime when d is small (

Therefore it suﬃces to only consider (2.8). Our model assumptions state that c and c(cid:48) cannot be too

(2.9)

√

√

√

diﬀerent. We leverage that by rewriting (2.8) a little, and get that it equals

(cid:20)
exp((cid:104)vw, c(cid:105)) E
c(cid:48)|c

(cid:21)
[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

T1 =

=

1 ± O((cid:15)z)
Z 2
1 ± O((cid:15)z)
Z 2

E
c

E
c

[exp((cid:104)vw, c(cid:105))A(c)]

(2.10)

[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]. We claim that A(c) = (1 ± O((cid:15)2)) exp((cid:104)vw(cid:48), c(cid:105)). Doing some algebraic

where A(c) := E
c(cid:48)|c

manipulations,

Furthermore, by our model assumptions, (cid:107)c − c(cid:48)(cid:107) ≤ (cid:15)2/

d. So

√

A(c) = exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48) − c(cid:105))] .

(cid:104)vw, c − c(cid:48)(cid:105) ≤ (cid:107)vw(cid:107)(cid:107)c − c(cid:48)(cid:107) = O((cid:15)2)

and thus A(c) = (1 ± O((cid:15)2)) exp((cid:104)vw(cid:48), c(cid:105)). Plugging the simpliﬁcation of A(c) to (2.10),

T1 =

1 ± O((cid:15)z)
Z 2

E[exp((cid:104)vw + vw(cid:48), c(cid:105))].

Since c has uniform distribution over the sphere, the random variable (cid:104)vw + vw(cid:48), c(cid:105) has distribution pretty
similar to Gaussian distribution N (0, (cid:107)vw + vw(cid:48)(cid:107)2/d), especially when d is relatively large. Observe that
E[exp(X)] has a closed form for Gaussian random variable X ∼ N (0, σ2),

E[exp(X)] =

(cid:90)

1
√
2π

x

σ
= exp(σ2/2) .

exp(−

x2
2σ2 ) exp(x)dx

(2.11)

(2.12)

(2.13)

Bounding the diﬀerence between (cid:104)vw + vw(cid:48), c(cid:105) from Gaussian random variable, we can show that for

(cid:15) = (cid:101)O(1/d),

E[exp((cid:104)vw + vw(cid:48), c(cid:105))] = (1 ± (cid:15)) exp

(cid:18) (cid:107)vw + vw(cid:48)(cid:107)2
2d

(cid:19)

.

Therefore, the series of simpliﬁcation/approximation above (concretely, combining equations (2.6), (2.7), (2.9), (2.11),

and (2.13)) lead to the desired bound on log p(w, w(cid:48)) for the case when the window size q = 2. The bound
on log p(w) can be shown similarly.

6

Proof sketch of Lemma 2.1 Note that for ﬁxed c, when word vectors have Gaussian priors assumed as
in our model, Zc = (cid:80)

w exp((cid:104)vw, c(cid:105)) is a sum of independent random variables.

We ﬁrst claim that using proper concentration of measure tools, it can be shown that the variance of
Zc are relatively small compared to its mean Evw [Zc], and thus Zc concentrates around its mean. Note this
is quite non-trivial: the random variable exp((cid:104)vw, c(cid:105)) is neither bounded nor subgaussian/sub-exponential,
since the tail is approximately inverse poly-logarithmic instead of inverse exponential.
In fact, the same
concentration phenomenon does not happen for w. The occurrence probability of word w is not necessarily
concentrated because the (cid:96)2 norm of vw can vary a lot in our model, which allows the frequency of the words
to have a large dynamic range.

So now it suﬃces to show that Evw [Zc] for diﬀerent c are close to each other. Using the fact that the
word vector directions have a Gaussian distribution, Evw [Zc] turns out to only depend on the norm of c
(which is equal to 1). More precisely,

[Zc] = f ((cid:107)c(cid:107)2

2) = f (1)

E
vw

(2.14)

where f is deﬁned as f (α) = n Es[exp(s2α/2)] and s has the same distribution as the norms of the word
vectors. We sketch the proof of this. In our model, vw = sw · ˆvw, where ˆvw is a Gaussian vector with identity
covariance I. Then

E
vw

[Zc] = n E
vw

[exp((cid:104)vw, c(cid:105))]

(cid:20)

= n E
sw

E
vw|sw

(cid:21)
[exp((cid:104)vw, c(cid:105)) | sw]

where the second line is just an application of the law of total expectation, if we pick the norm of the
(random) vector vw ﬁrst, followed by its direction. Conditioned on sw, (cid:104)vw, c(cid:105) is a Gaussian random variable
with variance (cid:107)c(cid:107)2

w, and therefore using similar calculation as in (2.12), we have

2s2

Hence, Evw [Zc] = n Es[exp(s2(cid:107)c(cid:107)2

2/2)] as needed.

E
vw|sw

[exp((cid:104)vw, c(cid:105)) | sw] = exp(s2(cid:107)c(cid:107)2

2/2) .

Proof of Theorem 4.1 The proof uses the standard analysis of linear regression. Let V = P ΣQT be the
SVD of V and let σ1, . . . , σd be the left singular values of V (the diagonal entries of Σ). For notational ease
we omit the subscripts in ¯ζ and ζ (cid:48) since they are not relevant for this proof. Since V † = QΣ−1P T and thus
¯ζ = V †ζ (cid:48) = QΣ−1P T ζ (cid:48), we have

We claim

Indeed, (cid:80)d
from the ﬁrst assumption. Furthermore, by the second assumption, (cid:107)P T ζ (cid:48)(cid:107)∞ ≤ c2√

i = O(nd), since the average squared norm of a word vector is d. The claim then follows

i=1 σ2

n (cid:107)ζ (cid:48)(cid:107)2, so

(2.15)

(2.16)

(2.17)

Plugging (2.16) and (2.17) into (2.15), we get

(cid:107)¯ζ(cid:107)2 ≤

(cid:114)

(cid:114) 1
c1n

c2
2d
n

(cid:107)ζ (cid:48)(cid:107)2

2 =

(cid:107)ζ (cid:48)(cid:107)2

c2
√

√

d
c1n

as desired. The last statement follows because the norm of the signal, which is d log(νR) originally and is
V †d log(νR) = va − vb after dimension reduction, also gets reduced by a factor of

n.

√

(cid:107)¯ζ(cid:107)2 ≤ σ−1

d (cid:107)P T ζ (cid:48)(cid:107)2.

σ−1
d ≤

(cid:114) 1
c1n

.

(cid:107)P T ζ (cid:48)(cid:107)2

2 ≤

(cid:107)ζ (cid:48)(cid:107)2
2.

c2
2d
n

7

2.2 Weakening the model assumptions

For readers uncomfortable with Bayesian priors, we can replace our assumptions with concrete properties of
word vectors that are empirically veriﬁable (Section 5.1) for our ﬁnal word vectors, and in fact also for word
vectors computed using other recent methods.

The word meanings are assumed to be represented by some “ground truth” vectors, which the experi-
menter is trying to recover. These ground truth vectors are assumed to be spatially isotropic in the bulk,
in the following two speciﬁc ways: (i) For almost all unit vectors c the sum (cid:80)
w exp((cid:104)vw, c(cid:105)) is close to a
constant Z; (ii) Singular values of the matrix of word vectors satisfy properties similar to those of random
matrices, as formalized in the paragraph before Theorem 4.1. Our Bayesian prior on the word vectors hap-
pens to imply that these two conditions hold with high probability. But the conditions may hold even if the
prior doesn’t hold. Furthermore, they are compatible with all sorts of local structure among word vectors
such as existence of clusterings, which would be absent in truly random vectors drawn from our prior.

3 Training objective and relationship to other models

To get a training objective out of Theorem 2.2, we reason as follows. Let Xw,w(cid:48) be the number of times words
w and w(cid:48) co-occur within the same window in the corpus. The probability p(w, w(cid:48)) of such a co-occurrence
at any particular time is given by (2.3). Successive samples from a random walk are not independent.
But if the random walk mixes fairly quickly (the mixing time is related to the logarithm of the vocabulary
size), then the distribution of Xw,w(cid:48)’s is very close to a multinomial distribution Mul( ˜L, {p(w, w(cid:48))}), where
˜L = (cid:80)

w,w(cid:48) Xw,w(cid:48) is the total number of word pairs.

Assuming this approximation, we show below that the maximum likelihood values for the word vectors

correspond to the following optimization,

min
{vw},C

(cid:88)

w,w(cid:48)

(cid:16)

Xw,w(cid:48)

log(Xw,w(cid:48)) − (cid:107)vw +vw(cid:48)(cid:107)2

2 − C

(cid:17)2

As is usual, empirical performance is improved by weighting down very frequent word pairs, possibly
because very frequent words such as “the” do not ﬁt our model. This is done by replacing the weighting
Xw,w(cid:48) by its truncation min{Xw,w(cid:48), Xmax} where Xmax is a constant such as 100. We call this objective with
the truncated weights SN (Squared Norm).

We now give its derivation. Maximizing the likelihood of {Xw,w(cid:48)} is equivalent to maximizing

Denote the logarithm of the ratio between the expected count and the empirical count as

Then with some calculation, we obtain the following where c is independent of the empirical observations
Xw,w(cid:48)’s.

(3.1)

(3.2)

(cid:96) = log

p(w, w(cid:48))Xw,w(cid:48)





(cid:89)

(w,w(cid:48))



 .

∆w,w(cid:48) = log

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

(cid:33)

.

(cid:96) = c +

Xw,w(cid:48)∆w,w(cid:48)

(cid:88)

(w,w(cid:48))

8

On the other hand, using ex ≈ 1 + x + x2/2 when x is small,5 we have

(cid:88)

˜L =

˜Lpw,w(cid:48) =

(cid:88)

Xw,w(cid:48)e∆w,w(cid:48)

(w,w(cid:48))

(w,w(cid:48))

(cid:32)

Xw,w(cid:48)

1 + ∆w,w(cid:48) +

(cid:88)

≈

(w,w(cid:48))

(cid:33)

.

∆2
w,w(cid:48)
2

(cid:88)

(w,w(cid:48))

Xw,w(cid:48)∆w,w(cid:48) ≈ −

Xw,w(cid:48)∆2

w,w(cid:48).

1
2

(cid:88)

(w,w(cid:48))

Note that ˜L = (cid:80)

(w,w(cid:48)) Xw,w(cid:48), so

Plugging this into (3.2) leads to

2(c − (cid:96)) ≈

Xw,w(cid:48)∆2

w,w(cid:48).

(cid:88)

(w,w(cid:48))

(3.3)

So maximizing the likelihood is approximately equivalent to minimizing the right hand side, which (by
examining (3.1)) leads to our objective.

Objective for training with PMI. A similar objective PMI can be obtained from (2.5), by computing
an approximate MLE, using the fact that the error between the empirical and true value of PMI(w, w(cid:48)) is
driven by the smaller term p(w, w(cid:48)), and not the larger terms p(w), p(w(cid:48)).

min
{vw},C

(cid:88)

w,w(cid:48)

Xw,w(cid:48) (PMI(w, w(cid:48)) − (cid:104)vw, vw(cid:48)(cid:105))2

This is of course very analogous to classical VSM methods, with a novel reweighting method.

Fitting to either of the objectives involves solving a version of Weighted SVD which is NP-hard, but

empirically seems solvable in our setting via AdaGrad (Duchi et al., 2011).

Connection to GloVe. Compare SN with the objective used by GloVe (Pennington et al., 2014):

f (Xw,w(cid:48))(log(Xw,w(cid:48)) − (cid:104)vw, vw(cid:48)(cid:105) − sw − sw(cid:48) − C)2

(cid:88)

w,w(cid:48)

with f (Xw,w(cid:48)) = min{X 3/4
w,w(cid:48), 100}. Their weighting methods and the need for bias terms sw, sw(cid:48), C were
derived by trial and error; here they are all predicted and given meanings due to Theorem 2.2, speciﬁcally
sw = (cid:107)vw(cid:107)2.

Connection to word2vec(CBOW). The CBOW model in word2vec posits that the probability of a
word wk+1 as a function of the previous k words w1, w2, . . . , wk:

(cid:16)

p

wk+1

(cid:12)
(cid:12) {wi}k

i=1

(cid:17)

∝ exp((cid:104)vwk+1,

vwi (cid:105)).

1
k

k
(cid:88)

i=1

5This Taylor series approximation has an error of the order of x3, but ignoring it can be theoretically justiﬁed as follows.
is close to 0 and thus ignoring
For a large Xw,w(cid:48) , its value approaches its expectation and thus the corresponding ∆w,w(cid:48)
∆3
w,w(cid:48) is well justiﬁed. The terms where ∆w,w(cid:48) is signiﬁcant correspond to Xw,w(cid:48) ’s that are small. But empirically, Xw,w(cid:48) ’s
obey a power law distribution (see, e.g. Pennington et al. (2014)) using which it can be shown that these terms contribute a
small fraction of the ﬁnal objective (3.3). So we can safely ignore the errors. Full details appear in the ArXiv version of this
paper (Arora et al., 2015).

9

This expression seems mysterious since it depends upon the average word vector for the previous k words.
We show it can be theoretically justiﬁed. Assume a simpliﬁed version of our model, where a small window
of k words is generated as follows: sample c ∼ C, where C is a uniformly random unit vector, then sample
(w1, w2, . . . , wk) ∼ exp((cid:104)(cid:80)k
i=1 vwi , c(cid:105))/Zc. Furthermore, assume Zc = Z for any c.

Lemma 3.1. In the simpliﬁed version of our model, the Maximum-a-Posteriori (MAP) estimate of c given

(w1, w2, . . . , wk) is

(cid:80)k
(cid:107) (cid:80)k

i=1 vwi
i=1 vwi (cid:107)2

.

(cid:80)k
(cid:107) (cid:80)k

i=1 vwi
i=1 vwi (cid:107)2

.

Proof. The c maximizing p (c|w1, w2, . . . , wk) is the maximizer of p(c)p (w1, w2, . . . , wk|c). Since p(c) =
p(c(cid:48)) for any c, c(cid:48), and we have p (w1, w2, . . . , wk|c) = exp((cid:104)(cid:80)
i vwi, c(cid:105))/Z, the maximizer is clearly c =

Thus using the MAP estimate of ct gives essentially the same expression as CBOW apart from the

rescaling, which is often omitted due to computational eﬃciency in empirical works.

4 Explaining relations=lines

As mentioned, word analogies like “a:b::c:??” can be solved via a linear algebraic expression:

argmin
d

(cid:107)va − vb − vc + vd(cid:107)2
2 ,

(4.1)

where vectors have been normalized such that (cid:107)vd(cid:107)2 = 1. This suggests that the semantic relationships
being tested in the analogy are characterized by a straight line,6 referred to earlier as relations=lines.

Using our model we will show the following for low-dimensional embeddings: for each such relation R
there is a direction µR in space such that for any word pair a, b satisfying the relation, va − vb is like µR
plus some noise vector. This happens for relations satisfying a certain condition described below. Empirical
results supporting this theory appear in Section 5, where this linear structure is further leveraged to slightly
improve analogy solving.

A side product of our argument will be a mathematical explanation of the empirically well-established su-
periority of low-dimensional word embeddings over high-dimensional ones in this setting (Levy and Goldberg,
2014a). As mentioned earlier, the usual explanation that smaller models generalize better is fallacious.

We ﬁrst sketch what was missing in prior attempts to prove versions of relations=lines from ﬁrst
principles. The basic issue is approximation error: the diﬀerence between the best solution and the 2nd best
solution to (4.1) is typically small, whereas the approximation error in the objective in the low-dimensional
solutions is larger. For instance, if one uses our PMI objective, then the weighted average of the termwise
error in (2.5) is 17%, and the expression in (4.1) above contains six inner products. Thus in principle the
approximation error could lead to a failure of the method and the emergence of linear relationship, but it
does not.

Prior explanations. Pennington et al. (2014) try to propose a model where such linear relationships
should occur by design. They posit that queen is a solution to the analogy “man:woman::king:??” because

p(χ | king)
p(χ | queen)

≈

p(χ | man)
p(χ | woman)

,

(4.2)

where p(χ | king) denotes the conditional probability of seeing word χ in a small window of text around
king. Relationship (4.2) is intuitive since both sides will be ≈ 1 for gender-neutral χ like “walks” or “food”,

6Note that this interpretation has been disputed; e.g., it is argued in Levy and Goldberg (2014a) that (4.1) can be understood
using only the classical connection between inner product and word similarity, using which the objective (4.1) is slightly
improved to a diﬀerent objective called 3COSMUL. However, this “explanation” is still dogged by the issue of large termwise
error pinpointed here, since inner product is only a rough approximation to word similarity. Furthermore, the experiments in
Section 5 clearly support the relations=lines interpretation.

10

will be > 1 when χ is like “he, Henry” and will be < 1 when χ is like “dress, she, Elizabeth.” This was
also observed by Levy and Goldberg (2014a). Given (4.2), they then posit that the correct model describing
word embeddings in terms of word occurrences must be a homomorphism from ((cid:60)d, +) to ((cid:60)+, ×), so vector
diﬀerences map to ratios of probabilities. This leads to the expression

pw,w(cid:48) = (cid:104)vw, vw(cid:48)(cid:105) + bw + bw(cid:48),

and their method is a (weighted) least squares ﬁt for this expression. One shortcoming of this argument
is that the homomorphism assumption assumes the linear relationships instead of explaining them from a
more basic principle. More importantly, the empirical ﬁt to the homomorphism has nontrivial approximation
error, high enough that it does not imply the desired strong linear relationships.
Levy and Goldberg (2014b) show that empirically, skip-gram vectors satisfy

(cid:104)vw, vw(cid:48)(cid:105) ≈ PMI(w, w(cid:48))

(4.3)

up to some shift. They also give an argument suggesting this relationship must be present if the solution
is allowed to be very high-dimensional. Unfortunately, that argument does not extend to low-dimensional
embeddings. Even if it did, the issue of termwise approximation error remains.

Our explanation. The current paper has introduced a generative model to theoretically explain the
emergence of relationship (4.3). However, as noted after Theorem 2.2, the issue of high approximation error
does not go away either in theory or in the empirical ﬁt. We now show that the isotropy of word vectors
(assumed in the theoretical model and veriﬁed empirically) implies that even a weak version of (4.3) is enough
to imply the emergence of the observed linear relationships in low-dimensional embeddings.

This argument will assume the analogy in question involves a relation that obeys Pennington et al.’s
suggestion in (4.2). Namely, for such a relation R there exists function νR(·) depending only upon R such
that for any a, b satisfying R there is a noise function ξa,b,R(·) for which:

p(χ | a)
p(χ | b)

= νR(χ) · ξa,b,R(χ)

For diﬀerent words χ there is huge variation in (4.4), so the multiplicative noise may be large.

Our goal is to show that the low-dimensional word embeddings have the property that there is a vector
µR such that for every pair of words a, b in that relation, va − vb = µR + noise vector, where the noise vector
is small.

Taking logarithms of (4.4) results in:

log

(cid:19)

(cid:18) p(χ | a)
p(χ | b)

= log(νR(χ)) + ζa,b,R(χ)

Theorem 2.2 implies that the left-hand side simpliﬁes to log

d (cid:104)vχ, va − vb(cid:105) + (cid:15)a,b(χ) where
(cid:15) captures the small approximation errors induced by the inexactness of Theorem 2.2. This adds yet more
noise! Denoting by V the n × d matrix whose rows are the vχ vectors, we rewrite (4.5) as:

(cid:16) p(χ|a)
p(χ|b)

(cid:17)

= 1

V (va − vb) = d log(νR) + ζ (cid:48)

a,b,R

where log(νR) in the element-wise log of vector νR and ζ (cid:48)

a,b,R = d(ζa,b,R − (cid:15)a,b,R) is the noise.

In essence, (4.6) shows that va − vb is a solution to a linear regression in d variables and m constraints,
with ζ (cid:48)
a,b,R being the “noise.” The design matrix in the regression is V , the matrix of all word vectors, which
in our model (as well as empirically) satisﬁes an isotropy condition. This makes it random-like, and thus
solving the regression by left-multiplying by V †, the pseudo-inverse of V , ought to “denoise” eﬀectively. We
now show that it does.

Our model assumed the set of all word vectors satisﬁes bulk properties similar to a set of Gaussian vectors.
The next theorem will only need the following weaker properties. (1) The smallest non-zero singular value

(4.4)

(4.5)

(4.6)

11

(a) SN

(b) GloVe

(c) CBOW

(d) skip-gram

Figure 1: The partition function Zc. The ﬁgure shows the histogram of Zc for 1000 random vectors c of
appropriate norm, as deﬁned in the text. The x-axis is normalized by the mean of the values. The values Zc
for diﬀerent c concentrate around the mean, mostly in [0.9, 1.1]. This concentration phenomenon is predicted
by our analysis.

of V is larger than some constant c1 times the quadratic mean of the singular values, namely, (cid:107)V (cid:107)F /
d.
Empirically we ﬁnd c1 ≈ 1/3 holds; see Section 5. (2) The left singular vectors behave like random vectors
with respect to ζ (cid:48)
a,b,R, for some constant c2.
(3) The max norm of a row in V is O(

a,b,R, namely, have inner product at most c2(cid:107)ζ (cid:48)
√

d). The proof is included in the appendix.

n with ζ (cid:48)

a,b,R(cid:107)/

√

√

Theorem 4.1 (Noise reduction). Under the conditions of the previous paragraph, the noise in the dimension-
reduced semantic vector space satisﬁes

(cid:107)¯ζa,b,R(cid:107)2 (cid:46) (cid:107)ζ (cid:48)

a,b,R(cid:107)2

√

d
n

.

As a corollary, the relative error in the dimension-reduced space is a factor of (cid:112)d/n smaller.

5 Experimental veriﬁcation

In this section, we provide experiments empirically supporting our generative model.

Corpus. All word embedding vectors are trained on the English Wikipedia (March 2015 dump).
It is
pre-processed by standard approach (removing non-textual elements, sentence splitting, and tokenization),
leaving about 3 billion tokens. Words that appeared less than 1000 times in the corpus are ignored, resulting
in a vocabulary of 68, 430. The co-occurrence is then computed using windows of 10 tokens to each side of
the focus word.

Training method. Our embedding vectors are trained by optimizing the SN objective using AdaGrad (Duchi
et al., 2011) with initial learning rate of 0.05 and 100 iterations. The PMI objective derived from (2.5) was
also used. SN has average (weighted) term-wise error of 5%, and PMI has 17%. We observed that SN
vectors typically ﬁt the model better and have better performance, which can be explained by larger errors
in PMI, as implied by Theorem 2.2. So, we only report the results for SN.

For comparison, GloVe and two variants of word2vec (skip-gram and CBOW) vectors are trained. GloVe’s
vectors are trained on the same co-occurrence as SN with the default parameter values.7 word2vec vectors
are trained using a window size of 10, with other parameters set to default values.8

7http://nlp.stanford.edu/projects/glove/
8https://code.google.com/p/word2vec/

12

Figure 2: The linear relationship between the squared norms of our word vectors and the logarithms of
the word frequencies. Each dot in the plot corresponds to a word, where x-axis is the natural logarithm of
the word frequency, and y-axis is the squared norm of the word vector. The Pearson correlation coeﬃcient
between the two is 0.75, indicating a signiﬁcant linear relationship, which strongly supports our mathematical
prediction, that is, equation (2.4) of Theorem 2.2.

5.1 Model veriﬁcation

Experiments were run to test our modeling assumptions. First, we tested two counter-intuitive properties:
the concentration of the partition function Zc for diﬀerent discourse vectors c (see Theorem 2.1), and the
random-like behavior of the matrix of word embeddings in terms of its singular values (see Theorem 4.1).
For comparison we also tested these properties for word2vec and GloVe vectors, though they are trained by
diﬀerent objectives. Finally, we tested the linear relation between the squared norms of our word vectors
and the logarithm of the word frequencies, as implied by Theorem 2.2.

Partition function. Our theory predicts the counter-intuitive concentration of the partition function
Zc = (cid:80)
w(cid:48) exp(c(cid:62)vw(cid:48)) for a random discourse vector c (see Lemma 2.1). This is veriﬁed empirically by
picking a uniformly random direction, of norm (cid:107)c(cid:107) = 4/µw, where µw is the average norm of the word
vectors.9 Figure 1(a) shows the histogram of Zc for 1000 such randomly chosen c’s for our vectors. The
values are concentrated, mostly in the range [0.9, 1.1] times the mean. Concentration is also observed for
other types of vectors, especially for GloVe and CBOW.

Isotropy with respect to singular values. Our theoretical explanation of relations=lines assumes
that the matrix of word vectors behaves like a random matrix with respect to the properties of singular
values. In our embeddings, the quadratic mean of the singular values is 34.3, while the minimum non-zero
singular value of our word vectors is 11. Therefore, the ratio between them is a small constant, consistent
with our model. The ratios for GloVe, CBOW, and skip-gram are 1.4, 10.1, and 3.1, respectively, which are
also small constants.

Squared norms v.s. word frequencies. Figure 2 shows a scatter plot for the squared norms of our
vectors and the logarithms of the word frequencies. A linear relationship is observed (Pearson correlation
0.75), thus supporting Theorem 2.2. The correlation is stronger for high frequency words, possibly because
the corresponding terms have higher weights in the training objective.

9Note that our model uses the inner products between the discourse vectors and word vectors, so it is invariant if the
discourse vectors are scaled by s while the word vectors are scaled by 1/s for any s > 0. Therefore, one needs to choose the
norm of c properly. We assume (cid:107)c(cid:107)µw =
d/κ ≈ 4 for a constant κ = 5 so that it gives a reasonable ﬁt to the predicted
dynamic range of word frequencies according to our theory; see model details in Section 2.

√

13

Relations
semantic
syntactic
total
adjective
noun
verb
total

SN GloVe CBOW skip-gram
0.84
0.61
0.71
0.50
0.69
0.48
0.53

0.73
0.68
0.70
0.58
0.58
0.56
0.57

0.79
0.71
0.74
0.58
0.56
0.64
0.62

0.85
0.65
0.73
0.56
0.70
0.53
0.57

G

M

Table 1: The accuracy on two word analogy task testbeds: G (the GOOGLE testbed); M (the MSR testbed).
Performance is close to the state of the art despite using a generative model with provable properties.

This correlation is much weaker for other types of word embeddings. This is possibly because they have
more free parameters (“knobs to turn”), which imbue the embeddings with other properties. This can also
cause the diﬀerence in the concentration of the partition function for the two methods.

5.2 Performance on analogy tasks

We compare the performance of our word vectors on analogy tasks, speciﬁcally the two testbeds GOOGLE
and MSR (Mikolov et al., 2013a;c). The former contains 7874 semantic questions such as “man:woman::king:??”,
and 10167 syntactic ones such as “run:runs::walk :??.” The latter has 8000 syntactic questions for adjectives,
nouns, and verbs.

To solve these tasks, we use linear algebraic queries.10 That is, ﬁrst normalize the vectors to unit norm

and then solve “a:b::c:??” by

argmin
d

(cid:107)va − vb − vc + vd(cid:107)2
2 .

(5.1)

The algorithm succeeds if the best d happens to be correct.

The performance of diﬀerent methods is presented in Table 1. Our vectors achieve performance compara-
ble to the state of art on semantic analogies (similar accuracy as GloVe, better than word2vec). On syntactic
tasks, they achieve accuracy 0.04 lower than GloVe and skip-gram, while CBOW typically outperforms the
others.11 The reason is probably that our model ignores local word order, whereas the other models capture
it to some extent. For example, a word “she” can aﬀect the context by a lot and determine if the next word
is “thinks” rather than “think ”. Incorporating such linguistic features in the model is left for future work.

5.3 Verifying relations=lines

The theory in Section 4 predicts the existence of a direction for a relation, whereas earlier Levy and Goldberg
(2014a) had questioned if this phenomenon is real. The experiment uses the analogy testbed, where each
relation is tested using 20 or more analogies. For each relation, we take the set of vectors vab = va − vb
where the word pair (a, b) satisﬁes the relation. Then calculate the top singular vectors of the matrix formed
by these vab’s, and compute the cosine similarity (i.e., normalized inner product) of individual vab to the
singular vectors. We observed that most (va − vb)’s are correlated with the ﬁrst singular vector, but have
inner products around 0 with the second singular vector. Over all relations, the average projection on the
ﬁrst singular vector is 0.51 (semantic: 0.58; syntactic: 0.46), and the average on the second singular vector
is 0.035. For example, Table 2 shows the mean similarities and standard deviations on the ﬁrst and second
singular vectors for 4 relations. Similar results are also obtained for word embedings by GloVe and word2vec.
Therefore, the ﬁrst singular vector can be taken as the direction associated with this relation, while the other
components are like random noise, in line with our model.

10One can instead use the 3COSMUL in (Levy and Goldberg, 2014a), which increases the accuracy by about 3%. But it is

not linear while our focus here is the linear algebraic structure.

11It was earlier reported that skip-gram outperforms CBOW (Mikolov et al., 2013a; Pennington et al., 2014). This may be

due to the diﬀerent training data sets and hyperparameters used.

14

relation
1st
2nd
relation
1st
2nd

1
0.65 ± 0.07
0.02 ± 0.28
8
0.56 ± 0.09
0.00 ± 0.22

2
0.61 ± 0.09
0.00 ± 0.23
9
0.53 ± 0.08
0.01 ± 0.26

3
0.52 ± 0.08
0.05 ± 0.30
10
0.37 ± 0.11
0.02 ± 0.20

4
0.54 ± 0.18
0.06 ± 0.27
11
0.72 ± 0.10
0.01 ± 0.24

5
0.60 ± 0.21
0.01 ± 0.24
12
0.37 ± 0.14
0.07 ± 0.26

6
0.35 ± 0.17
0.07 ± 0.24
13
0.40 ± 0.19
0.07 ± 0.23

7
0.42 ± 0.16
0.01 ± 0.25
14
0.43 ± 0.14
0.09 ± 0.23

Table 2: The veriﬁcation of relation directions on 2 semantic and 2 syntactic relations in the GOOGLE
testbed. Relations include cap-com: capital-common-countries; cap-wor: capital-world; adj-adv: gram1-
adjective-to-adverb; opp: gram2-opposite. For each relation, take vab = va − vb for pairs (a, b) in the
relation, and then calculate the top singular vectors of the matrix formed by these vab’s. The row with label
“1st”/“2nd” shows the cosine similarities of individual vab to the 1st/2nd singular vector (the mean and
standard deviation).

w/o RD
RD(k = 20)
RD(k = 30)
RD(k = 40)

SN GloVe CBOW skip-gram
0.71
0.74
0.79
0.76

0.70
0.75
0.80
0.77

0.73
0.77
0.80
0.80

0.74
0.79
0.82
0.80

Table 3: The accuracy of the RD algorithm (i.e., the cheater method) on the GOOGLE testbed. The RD
algorithm is described in the text. For comparison, the row “w/o RD” shows the accuracy of the old method
without using RD.

Cheating solver for analogy testbeds. The above linear structure suggests a better (but cheating) way
to solve the analogy task. This uses the fact that the same semantic relationship (e.g., masculine-feminine,
singular-plural) is tested many times in the testbed. If a relation R is represented by a direction µR then the
cheating algorithm can learn this direction (via rank 1 SVD) after seeing a few examples of the relationship.
Then use the following method of solving “a:b::c:??”: look for a word d such that vc − vd has the largest
projection on µR, the relation direction for (a, b). This can boost success rates by about 10%.

The testbed can try to combat such cheating by giving analogy questions in a random order. But the
cheating algorithm can just cluster the presented analogies to learn which of them are in the same relation.
Thus the ﬁnal algorithm, named analogy solver with relation direction (RD), is: take all vectors va − vb for
all the word pairs (a, b) presented among the analogy questions and do k-means clustering on them; for each
(a, b), estimate the relation direction by taking the ﬁrst singular vector of its cluster, and substitute that
for va − vb in (5.1) when solving the analogy. Table 3 shows the performance on GOOGLE with diﬀerent
values of k; e.g. using our SN vectors and k = 30 leads to 0.79 accuracy. Thus future designers of analogy
testbeds should remember not to test the same relationship too many times! This still leaves other ways to
cheat, such as learning the directions for interesting semantic relations from other collections of analogies.

Non-cheating solver for analogy testbeds. Now we show that even if a relationship is tested only
once in the testbed, there is a way to use the above structure. Given “a:b::c:??,” the solver ﬁrst ﬁnds the
top 300 nearest neighbors of a and those of b, and then ﬁnds among these neighbors the top k pairs (a(cid:48), b(cid:48))
so that the cosine similarities between va(cid:48) − vb(cid:48) and va − vb are largest. Finally, the solver uses these pairs
to estimate the relation direction (via rank 1 SVD), and substitute this (corrected) estimate for va − vb in
(5.1) when solving the analogy. This algorithm is named analogy solver with relation direction by nearest
neighbors (RD-nn). Table 4 shows its performance, which consistently improves over the old method by
about 3%.

15

w/o RD-nn
RD-nn (k = 10)
RD-nn (k = 20)
RD-nn (k = 30)

SN GloVe CBOW skip-gram
0.71
0.71
0.72
0.73

0.74
0.77
0.77
0.78

0.70
0.73
0.74
0.74

0.73
0.74
0.75
0.76

Table 4: The accuracy of the RD-nn algorithm on the GOOGLE testbed. The algorithm is described in the
text. For comparison, the row “w/o RD-nn” shows the accuracy of the old method without using RD-nn.

6 Conclusions

A simple generative model has been introduced to explain the classical PMI based word embedding models,
as well as recent variants involving energy-based models and matrix factorization. The model yields an
optimization objective with essentially “no knobs to turn”, yet the embeddings lead to good performance
on analogy tasks, and ﬁt other predictions of our generative model. A model with fewer knobs to turn
should be seen as a better scientiﬁc explanation (Occam’s razor), and certainly makes the embeddings more
interpretable.

The spatial isotropy of word vectors is both an assumption in our model, and also a new empirical
ﬁnding of our paper. We feel it may help with further development of language models. It is important for
explaining the success of solving analogies via low dimensional vectors (relations=lines). It also implies
that semantic relationships among words manifest themselves as special directions among word embeddings
(Section 4), which lead to a cheater algorithm for solving analogy testbeds.

Our model is tailored to capturing semantic similarity, more akin to a log-linear dynamic topic model.
In particular, local word order is unimportant. Designing similar generative models (with provable and
interpretable properties) with linguistic features is left for future work.

Acknowledgements

We thank the editors of TACL for granting a special relaxation of the page limit for our paper. We thank
Yann LeCun, Christopher D. Manning, and Sham Kakade for helpful discussions at various stages of this
work.

This work was supported in part by NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award,
Simons Collaboration Grant, and ONR-N00014-16-1-2329. Tengyu Ma was supported in addition by Simons
Award in Theoretical Computer Science and IBM PhD Fellowship.

References

Jacob Andreas and Dan Klein. When and why are log-linear models self-normalizing? In Proceedings of the
Annual Meeting of the North American Chapter of the Association for Computational Linguistics, 2014.

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model approach

to PMI-based word embeddings. Technical report, ArXiV, 2015. http://arxiv.org/abs/1502.03520.

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski.

braic structure of word senses, with applications to polysemy.
http://arxiv.org/abs/1502.03520.

Linear alge-
Technical report, ArXiV, 2016.

David Belanger and Sham M. Kakade. A linear dynamical system model for text. In Proceedings of the 32nd

International Conference on Machine Learning, 2015.

Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. Neural

probabilistic language models. In Innovations in Machine Learning. 2006.

16

Fischer Black and Myron Scholes. The pricing of options and corporate liabilities. Journal of Political

David M. Blei. Probabilistic topic models. Communication of the Association for Computing Machinery,

Economy, 1973.

2012.

David M. Blei and John D. Laﬀerty. Dynamic topic models.

In Proceedings of the 23rd International

Conference on Machine Learning, 2006.

Kenneth Ward Church and Patrick Hanks. Word association norms, mutual information, and lexicography.

Computational linguistics, 1990.

Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar. Spectral learning of latent-
variable PCFGs. In Proceedings of the 50th Annual Meeting of the Association for Computational Lin-
guistics: Long Papers-Volume 1, 2012.

Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep neural net-
works with multitask learning. In Proceedings of the 25th International Conference on Machine Learning,
2008a.

Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep neural net-
works with multitask learning. In Proceedings of the 25th International Conference on Machine Learning,
2008b.

Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman.
Indexing by latent semantic analysis. Journal of the American Society for Information Science, 1990.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic

optimization. The Journal of Machine Learning Research, 2011.

John Rupert Firth. A synopsis of linguistic theory. 1957.

Amir Globerson, Gal Chechik, Fernando Pereira, and Naftali Tishby. Euclidean embedding of co-occurrence

data. Journal of Machine Learning Research, 2007.

Tatsunori B. Hashimoto, David Alvarez-Melis, and Tommi S. Jaakkola. Word embeddings as metric recovery

in semantic spaces. Transactions of the Association for Computational Linguistics, 2016.

Thomas Hofmann. Probabilistic latent semantic analysis.

In Proceedings of the Fifteenth Conference on

Uncertainty in Artiﬁcial Intelligence, 1999.

Daniel Hsu, Sham M. Kakade, and Tong Zhang. A spectral algorithm for learning hidden markov models.

Journal of Computer and System Sciences, 2012.

Omer Levy and Yoav Goldberg. Linguistic regularities in sparse and explicit word representations.

In

Proceedings of the Eighteenth Conference on Computational Natural Language Learning, 2014a.

Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in

Neural Information Processing Systems, 2014b.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Compu-
tational Linguistics, 2011.

Yariv Maron, Michael Lamar, and Elie Bienenstock. Sphere embedding: An application to part-of-speech

induction. In Advances in Neural Information Processing Systems, 2010.

17

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey Dean. Eﬃcient estimation of word representations in

vector space. Proceedings of the International Conference on Learning Representations, 2013a.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeﬀ Dean. Distributed representations of
words and phrases and their compositionality. In Advances in Neural Information Processing Systems,
2013b.

Tomas Mikolov, Wen-tau Yih, and Geoﬀrey Zweig. Linguistic regularities in continuous space word rep-
In Proceedings of the Conference of the North American Chapter of the Association for

resentations.
Computational Linguistics: Human Language Technologies, 2013c.

Andriy Mnih and Geoﬀrey Hinton. Three new graphical models for statistical language modelling.

In

Proceedings of the 24th International Conference on Machine Learning, 2007.

Christos H. Papadimitriou, Hisao Tamaki, Prabhakar Raghavan, and Santosh Vempala. Latent semantic
indexing: A probabilistic analysis. In Proceedings of the 7th ACM SIGACT-SIGMOD-SIGART Symposium
on Principles of Database Systems, 1998.

Jeﬀrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word represen-

tation. Proceedings of the Empiricial Methods in Natural Language Processing, 2014.

Douglas L. T. Rohde, Laura M. Gonnerman, and David C. Plaut. An improved model of semantic similarity

based on lexical co-occurence. Communication of the Association for Computing Machinery, 2006.

David E. Rumelhart, Geoﬀrey E. Hinton, and James L. McClelland, editors. Parallel Distributed Processing:

Explorations in the Microstructure of Cognition. 1986.

David E. Rumelhart, Geoﬀrey E. Hinton, and Ronald J. Williams. Learning representations by back-

propagating errors. Cognitive modeling, 1988.

Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. Journal

of Artiﬁcial Intelligence Research, 2010.

18

A Proofs of Theorem 1

In this section we prove Theorem 2.2 and Lemma 2.1 (restated below).

Theorem 2.2. Suppose the word vectors satisfy equation (2.2), and window size q = 2. Then,

for (cid:15) = O((cid:15)z) + (cid:101)O(1/d) + O((cid:15)2). Jointly these imply:

log p(w, w(cid:48)) =

− 2 log Z ± (cid:15),

(cid:107)vw + vw(cid:48)(cid:107)2
2
2d
(cid:107)vw(cid:107)2
2
2d

log p(w) =

− log Z ± (cid:15).

PMI (w, w(cid:48)) =

(cid:104)vw, vw(cid:48)(cid:105)
d

± O((cid:15)).

Lemma 2.1. If the word vectors satisfy the bayesian prior v = s · ˆv, where ˆv is from the spherical Gaussian
distribution, and s is a scalar random variable, then with high probability the entire ensemble of word vectors
satisﬁes that

for (cid:15)z = (cid:101)O(1/

n), and δ = exp(−Ω(log2 n)).

√

Pr
c∼C

[(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z] ≥ 1 − δ,

We ﬁrst prove Theorem 2.2 using Lemma 2.1, and Lemma 2.1 will be proved in Section A.1. Please see
Section 2 of the main paper for the intuition of the proof and a cleaner sketch without too many technicalities.

Proof of Theorem 2.2. Let c be the hidden discourse that determines the probability of word w, and c(cid:48) be
the next one that determines w(cid:48). We use p(c(cid:48)|c) to denote the Markov kernel (transition matrix) of the
Markov chain. Let C be the stationary distribution of discourse vector c, and D be the joint distribution of
(c, c(cid:48)). We marginalize over the contexts c, c(cid:48) and then use the independence of w, w(cid:48) conditioned on c, c(cid:48),

(cid:20) exp((cid:104)vw, c(cid:105))
Zc
We ﬁrst get rid of the partition function Zc using Lemma 2.1. As sketched in the main paper, essentially
we will replace Zc by Z in equation (A.5), though a very careful control of the approximation error is
required. Formally, Let F1 be the event that c satisﬁes

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

p(w, w(cid:48)) = E

(A.5)

(c,c(cid:48))∼D

(cid:21)

(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z .

(A.6)

Similarly, let F2 be the even that c(cid:48) satisﬁes (1 − (cid:15)z)Z ≤ Zc(cid:48) ≤ (1 + (cid:15)z)Z, and let F = F1 ∩ F2, and F be its
negation. Moreover, let 1F be the indicator function for the event F. Therefore by Lemma 2.1 and union
bound, we have E[1F ] = Pr[F] ≥ 1 − exp(−Ω(log2 n)).

We ﬁrst decompose the integral (A.5) into the two parts according to whether event F happens,

We bound the ﬁrst quantity on the right hand side using (2.2) and the deﬁnition of F.

p(w, w(cid:48)) = E

(c,c(cid:48))∼D

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

+ E

(c,c(cid:48))∼D

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:20)

(cid:20)

1
ZcZc(cid:48)
1
ZcZc(cid:48)

(cid:21)

(cid:21)

(cid:20)

E
(c,c(cid:48))∼D

1
ZcZc(cid:48)
≤ (1 + (cid:15)z)2 1
Z 2

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:21)

E
(c,c(cid:48))∼D

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ]

19

(A.1)

(A.2)

(A.3)

(A.4)

(A.7)

(A.8)

For the second quantity of the right hand side of (A.7), we have by Cauchy-Schwartz,

1
ZcZc(cid:48)
(cid:20) 1
Z 2
c

(cid:18)

(cid:20)

E
(c,c(cid:48))∼D
(cid:18)

≤

≤

(cid:18)

E
(c,c(cid:48))∼D
(cid:20) 1
Z 2
c

E
c

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:21)(cid:19)2

exp((cid:104)vw, c(cid:105))21F

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

[1F ]

(cid:21)(cid:19) (cid:18)

(cid:21)(cid:19) (cid:18)

E
(c,c(cid:48))∼D
(cid:20) 1
Z 2
c(cid:48)

E
c(cid:48)

(cid:20)(cid:90)

1
Z 2
c(cid:48)

c,c(cid:48)

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))21F

(cid:21)(cid:19)

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))2 E
c|c(cid:48)

[1F ]

(cid:21)(cid:19)

.

(A.9)

Using the fact that Zc ≥ 1, then we have that

(cid:20) 1
Z 2
c

E
c

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

(cid:21)
[1F ]

(cid:20)
exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

(cid:21)
[1F ]

≤ E
c

We can split that expectation as
(cid:20)
exp((cid:104)vw, c(cid:105))21(cid:104)vw,c(cid:105)>0 E
c(cid:48)|c

E
c

The second term of (A.10) is upper bounded by

(cid:21)
[1F ]

(cid:20)
exp((cid:104)vw, c(cid:105))21(cid:104)vw,c(cid:105)<0 E
c(cid:48)|c

(cid:21)
[1F ]

.

+ E
c

(A.10)

We proceed to the ﬁrst term of (A.10) and observe the following property of it:

(cid:20)
exp((cid:104)vw, c(cid:105))21(cid:104)vw,c(cid:105)>0 E
c(cid:48)|c

E
c

(cid:21)

(cid:20)

[1F ]

≤ E
c

exp((cid:104)αvw, c(cid:105))21(cid:104)vw,c(cid:105)>0 E
c(cid:48)|c

[1F ]

≤ E
c

(cid:21)

(cid:20)
exp((cid:104)αvw, c(cid:105))2 E
c(cid:48)|c

(cid:21)

[1F ]

where α > 1. Therefore, it’s suﬃcient to bound

[1F ] ≤ exp(−Ω(log2 n))

E
c,c(cid:48)

(cid:20)

E
c

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

[1F ]

(cid:21)

when (cid:107)vw(cid:107) = Ω(

d).

√

Let’s denote by z the random variable 2 (cid:104)vw, c(cid:105).
Let’s denote r(z) = Ec(cid:48)|z[1F ] which is a function of z between [0, 1]. We wish to upper bound Ec [exp(z)r(z)].

The worst-case r(z) can be quantiﬁed using a continuous version of Abel’s inequality as proven in Lemma A.4,
which gives

[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3)
E
c

(A.11)

where t satisﬁes that Ec[1[t,+∞]] = Pr[z ≥ t] = Ec[r(z)] ≤ exp(−Ω(log2 n)). Then, we claim Pr[z ≥ t] ≤
exp(−Ω(log2 n)) implies that t ≥ Ω(log.9 n).

If c were distributed as N (0, 1

d I), this would be a simple tail bound. However, as c is distributed uniformly
on the sphere, this requires special care, and the claim follows by applying Lemma A.1 instead. Finally,
applying Corollary A.3, we have:

E[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(log1.8 n))
We have the same bound for c(cid:48) as well. Hence, for the second quantity of the right hand side of (A.7),

(A.12)

we have

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:21)

(cid:20)

1
ZcZc(cid:48)

E
(c,c(cid:48))∼D
(cid:18)

≤

E
c

(cid:20) 1
Z 2
c
≤ exp(−Ω(log1.8 n))

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

(cid:21)(cid:19)1/2 (cid:18)

[1F ]

(cid:20) 1
Z 2
c(cid:48)

E
c(cid:48)

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))2 E
c|c(cid:48)

[1F ]

(cid:21)(cid:19)1/2

(A.13)

20

where the ﬁrst inequality follows from Cauchy-Schwartz, and the second from the calculation above. Com-
bining (A.7), (A.8) and (A.13), we obtain

p(w, w(cid:48)) ≤ (1 + (cid:15)z)2 1
Z 2
≤ (1 + (cid:15)z)2 1
Z 2

E
(c,c(cid:48))∼D

E
(c,c(cid:48))∼D

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ] +

1
n2 exp(−Ω(log1.8 n))

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] + δ0

where δ0 = exp(−Ω(log1.8 n))Z 2 ≤ exp(−Ω(log1.8 n)) by the fact that Z ≤ exp(2κ)n = O(n). Note that
κ is treated as an absolute constant throughout the paper. On the other hand, we can lowerbound similarly

p(w, w(cid:48)) ≥ (1 − (cid:15)z)2 1
Z 2
≥ (1 − (cid:15)z)2 1
Z 2
≥ (1 − (cid:15)z)2 1
Z 2

E
(c,c(cid:48))∼D

E
(c,c(cid:48))∼D

E
(c,c(cid:48))∼D

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ]

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] −

1
n2 exp(−Ω(log1.8 n))

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] − δ0

Taking logarithm, the multiplicative error translates to a additive error

log p(w, w(cid:48)) = log

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

(cid:18)

E
(c,c(cid:48))∼D

For the purpose of exploiting the fact that c, c(cid:48) should be close to each other, we further rewrite log p(w, w(cid:48))
by re-organizing the expectations above,

log p(w, w(cid:48)) = log

± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

= log

[exp((cid:104)vw, c(cid:105))A(c)] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

(A.14)

(cid:20)
exp((cid:104)vw, c(cid:105)) E
c(cid:48)|c

(cid:21)
[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

(cid:19)

(cid:18)

(cid:18)

E
c

E
c

A(c) := E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

where the inner integral which is denoted by A(c),

Since (cid:107)vw(cid:107) ≤ κ

d. Therefore we have that (cid:104)vw, c − c(cid:48)(cid:105) ≤ (cid:107)vw(cid:107)(cid:107)c − c(cid:48)(cid:107) ≤ κ

d(cid:107)c − c(cid:48)(cid:107).

√

√

Then we can bound A(c) by

(cid:19)

(cid:19)

A(c) = E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

= exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

≤ exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48) − c(cid:105))]

√

[exp(κ

d(cid:107)c − c(cid:48)(cid:107))]

≤ (1 + (cid:15)2) exp((cid:104)vw(cid:48), c(cid:105))

21

where the last inequality follows from our model assumptions. To derive a lower bound of A(c), observe that

√

[exp(κ

E
c(cid:48)|c

d(cid:107)c − c(cid:48)(cid:107))] + E
c(cid:48)|c

√

[exp(−κ

d(cid:107)c − c(cid:48)(cid:107))] ≥ 2

Therefore, our model assumptions imply that

Hence,

√

[exp(−κ

d(cid:107)c − c(cid:48)(cid:107))] ≥ 1 − (cid:15)2

E
c(cid:48)|c

A(c) = exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

exp((cid:104)vw(cid:48), c(cid:48) − c(cid:105))
√

exp(−κ

d(cid:107)c − c(cid:48)(cid:107))

≥ exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

≥ (1 − (cid:15)2) exp((cid:104)vw(cid:48), c(cid:105))

Therefore, we obtain that A(c) = (1 ± (cid:15)2) exp((cid:104)vw(cid:48), c(cid:105)). Plugging the just obtained estimate of A(c) into

the equation (A.14), we get that

(cid:18)

(cid:18)

(cid:18)

E
c

E
c

E
c

E
c

log p(w, w(cid:48)) = log

[exp((cid:104)vw, c(cid:105))A(c)] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

= log

[(1 ± (cid:15)2) exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:105))] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

(cid:19)

= log

[exp((cid:104)vw + vw(cid:48), c(cid:105))] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z) + log(1 ± (cid:15)2)

(A.15)

Now it suﬃces to compute Ec[exp((cid:104)vw + vw(cid:48), c(cid:105))]. Note that if c had the distribution N (0, 1

d I), which
is very similar to uniform distribution over the sphere, then we could get straightforwardly Ec[exp((cid:104)vw +
vw(cid:48), c(cid:105))] = exp((cid:107)vw + vw(cid:48)(cid:107)2/(2d)). For c having a uniform distribution over the sphere, by Lemma A.5, the
same equality holds approximately,

[exp((cid:104)vw + vw(cid:48), c(cid:105))] = (1 ± (cid:15)3) exp((cid:107)vw + vw(cid:48)(cid:107)2/(2d))

(A.16)

where (cid:15)3 = (cid:101)O(1/d).

Plugging in equation (A.16) into equation (A.15), we have that
log p(w, w(cid:48)) = log (cid:0)(1 ± (cid:15)3) exp((cid:107)vw + vw(cid:48)(cid:107)2/(2d)) ± δ0

(cid:1) − 2 log Z + 2 log(1 ± (cid:15)z) + log(1 ± (cid:15)2)

= (cid:107)vw + vw(cid:48)(cid:107)2/(2d) + O((cid:15)3) + O(δ(cid:48)

0) − 2 log Z ± 2(cid:15)z ± (cid:15)2

where δ(cid:48)

0 = δ0 · (Ec∼C[exp((cid:104)vw + vw(cid:48), c(cid:105))])−1 = exp(−Ω(log1.8 n)). Note that (cid:15)3 = (cid:101)O(1/d), (cid:15)z = (cid:101)O(1/

√

n),

and (cid:15)2 by assumption, therefore we obtain that

log p(w, w(cid:48)) =

(cid:107)vw + vw(cid:48)(cid:107)2 − 2 log Z ± O((cid:15)z) + O((cid:15)2) + (cid:101)O(1/d).

1
2d

The following lemmas are helper lemmas that were used in the proof above. We use Cd to denote the

uniform distribution over the unit sphere in Rd.

Lemma A.1 (Tail bound for spherical distribution). If c ∼ Cd, v ∈ Rd is a vector with (cid:107)v(cid:107) = Ω(
t = ω(1), the random variable z = (cid:104)v, c(cid:105) satisﬁes Pr[z ≥ t] = e−O(t2).

√

d) and

(cid:19)

(cid:19)

22

Proof. If c = (c1, c2, . . . , cd) ∼ Cd, c is in distribution equal to
samples from a univariate Gaussian with mean 0 and variance 1
that v = ((cid:107)v(cid:107), 0, . . . , 0). Let’s introduce the random variable r = (cid:80)d

(cid:16) ˜c1
(cid:107)˜c(cid:107) , ˜c2

(cid:107)˜c(cid:107) , . . . , ˜cd

where the ˜ci are i.i.d.
d . By spherical symmetry, we may assume

(cid:107)˜c(cid:107)

(cid:17)

Pr [(cid:104)v, c(cid:105) ≥ t] = Pr

(cid:107)v|

≥ t

≤ Pr

≥ t | r ≥

Pr

r ≥

+ Pr

≥ t | r ≥

Pr

r ≥

(cid:20)

(cid:21)

˜c1
(cid:107)˜c(cid:107)

(cid:20) (cid:107)v(cid:107)˜c1
(cid:107)˜c(cid:107)

(cid:20)

(cid:21)

1
2

(cid:20)

(cid:21)

1
2

(cid:21)

1
2

it’s suﬃcient to lower bound Pr [r ≤ 100] and Pr

. The former probability is easily

seen to be lower bounded by a constant by a Chernoﬀ bound. Consider the latter one next. It holds that

(cid:104)

(cid:107)v(cid:107) ˜c1

(cid:107)c(cid:107) ≥ t | r ≤ 100

(cid:105)

i=2 ˜c2
(cid:21)

1
2

i . Since

(cid:20) (cid:107)v(cid:107)˜c1
(cid:107)˜c(cid:107)

Pr

(cid:107)v(cid:107)

≥ t | r ≤ 100

= Pr

˜c1 ≥

(cid:21)

(cid:34)

(cid:115)

t2 · r

(cid:107)v(cid:107)2 − t2 | r ≤ 100

(cid:35)

(cid:34)

(cid:115)

≥ Pr

˜c1 ≥

(cid:35)

100t2
(cid:107)v(cid:107)2 − t2

(cid:20)

˜c1
(cid:107)˜c(cid:107)

Denoting ˜t =

(cid:113) 100t2

(cid:107)v(cid:107)2−t2 , by a well-known Gaussian tail bound it follows that

Pr (cid:2)˜c1 ≥ ˜t(cid:3) = e−O(d˜t2)

(cid:32)

1
√
d˜t

(cid:18) 1
√

−

d˜t

(cid:19)3(cid:33)

= e−O(t2)

where the last equality holds since (cid:107)v(cid:107) = Ω(

d) and t = ω(1).

√

√

Lemma A.2. If c ∼ Cd, v ∈ Rd is a vector with (cid:107)v(cid:107) = Θ(
satisﬁes

d) and t = ω(1), the random variable z = (cid:104)v, c(cid:105)

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(t2)) + exp(−Ω(d))

Proof. Similarly as in Lemma A.1, if c = (c1, c2, . . . , cd) ∼ Cd, c is in distribution equal to
where the ˜ci are i.i.d. samples from a univariate Gaussian with mean 0 and variance 1
symmetry, we may assume v = ((cid:107)v(cid:107), 0, . . . , 0). Let’s introduce the random variable r = (cid:80)d
an arbitrary u > 1, some algebraic manipulation shows

(cid:16) ˜c1
(cid:107)˜c(cid:107) , ˜c2

(cid:107)˜c(cid:107) , . . . , ˜cd

(cid:107)˜c(cid:107)

(cid:17)

d . Again, by spherical
i . Then, for

i=2 ˜c2

Pr (cid:2)exp ((cid:104)v, c(cid:105)) 1[t,+∞]((cid:104)v, c(cid:105)) ≥ u(cid:3) = Pr [exp ((cid:104)v, c(cid:105)) ≥ u ∧ (cid:104)v, c(cid:105) ≥ t] =

(cid:20)

(cid:18)

Pr

exp

(cid:107)v(cid:107)

(cid:19)

˜c1
(cid:107)˜c(cid:107)

˜c1
(cid:107)˜c(cid:107)

≥ u ∧ (cid:107)v(cid:107)

≥ u

= Pr

˜c1 = max

(cid:21)

(cid:34)

(cid:32)(cid:115)

(cid:115)

(cid:33)(cid:35)

˜u2r
(cid:107)v(cid:107)2 − ˜u2 ,

t2r
(cid:107)v(cid:107)2 − t2

(A.17)

√

where we denote ˜u = log u. Since ˜c1 is a mean 0 univariate Gaussian with variance 1
have ∀x ∈ R

d , and (cid:107)v(cid:107) = Ω(

d) we

(cid:34)

(cid:115)

Pr

˜c1 ≥

(cid:35)

x2r
(cid:107)v(cid:107)2 − u2

(cid:16)

e−Ω(x2r)(cid:17)

= O

Next, we show that r is lower bounded by a constant with probability 1 − exp(−Ω(d)).
distribution equal to 1

Indeed, r is in
k is a Chi-squared distribution with k degrees of freedom. Standard
d ] ≤ exp(−ξ). Taking ξ = αd for α a constant

concentration bounds (?) imply that ∀ξ ≥ 0, Pr[r − 1 ≤ −2
implies that with probability 1 − exp(−Ω(d)), r ≥ M for some constant M . We can now rewrite

d−1, where χ2

d χ2

(cid:113) ξ

(cid:34)

(cid:115)

Pr

˜c1 ≥

x2r
(cid:107)v(cid:107)2 − x2

(cid:35)

=

23

(cid:34)

(cid:115)

Pr

˜c1 ≥

x2r

(cid:107)v(cid:107)2 − x2 | r ≥ M

(cid:35)

Pr[r ≥ M ] + Pr

˜c1 ≥

(cid:34)

(cid:115)

x2r

(cid:107)v(cid:107)2 − x2 | r ≤ M

(cid:35)

Pr[r ≤ M ]

The ﬁrst term is clearly bounded by e−Ω(x2) and the second by exp(−Ω(d)). Therefore,

(cid:34)

(cid:115)

Pr

˜c1 ≥

(cid:35)

x2r
(cid:107)v(cid:107)2 − x2

= O (cid:0)max (cid:0)exp (cid:0)−Ω (cid:0)x2(cid:1)(cid:1) , exp (−Ω (d))(cid:1)(cid:1)

(A.18)

Putting A.17 and A.18 together, we get that

Pr (cid:2)exp ((cid:104)v, c(cid:105)) 1[t,+∞]((cid:104)v, c(cid:105)) ≥ u(cid:3) = O

(cid:16)

(cid:16)

(cid:16)

(cid:16)

max

exp

−Ω

min

(cid:16)

d, (max (˜u, t))2(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)

(A.19)

(where again, we denote ˜u = log u)

For any random variable X which has non-negative support, it’s easy to check that

E[X] =

Pr[X ≥ x]dx

(cid:90) ∞

0

Hence,

E (cid:2)exp(z)1[t,+∞](z)(cid:3) =

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du =

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du

(cid:90) ∞

0

(cid:90) exp((cid:107)v(cid:107))

0

To bound this integral, we split into the following two cases:

• Case t2 ≥ d: max (˜u, t) ≥ t, so min

(cid:16)

d, (max (˜u, t))2(cid:17)

= d. Hence, A.19 implies

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp((cid:107)v(cid:107)) exp(−Ω(d)) = exp(−Ω(d))

where the last inequality follows since (cid:107)v(cid:107) = O(

d).

√

• Case t2 < d: In the second case, we will split the integral into two portions: u ∈ [0, exp(t)] and

u ∈ [exp(t), exp((cid:107)v(cid:107))].
When u ∈ [0, exp(t)], max (˜u, t) = t, so min(d, (max (˜u, t))2) = t2. Hence,

(cid:90) exp(t)

0

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du ≤ exp(t) exp(−Ω(t2)) = − exp(Ω(t2))

When u ∈ [exp(t), exp((cid:107)v(cid:107))], max (˜u, t) = ˜u. But ˜u ≤ log(exp((cid:107)v(cid:107))) = O(
˜u. Hence,

(cid:90) exp((cid:107)v(cid:107))

exp(t)

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du ≤

exp(−(log(u))2)du

(cid:90) exp((cid:107)v(cid:107))

exp(t)

Making the change of variable ˜u = log(u), the we can rewrite the last integral as

√

d), so min(d, (max (˜u, t))2) =

(cid:90) (cid:107)v(cid:107)

t

exp(−˜u2) exp(˜u)d˜u = O(exp(−t2))

where the last inequality is the usual Gaussian tail bound.

In either case, we get that

(cid:90) exp((cid:107)v(cid:107))

0

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du = exp(−Ω(t2)) + exp(−Ω(d)))

which is what we want.

24

As a corollary to the above lemma, we get the following:

Corollary A.3. If c ∼ Cd, v ∈ Rd is a vector with (cid:107)v(cid:107) = Θ(

d) and t = Ω(log.9 n) then

√

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(log1.8 n))

Proof. We claim the proof is trivial if d = o(log4 n).
exp(O(

d)). Hence,

√

Indeed,

in this case, exp((cid:104)v, c(cid:105)) ≤ exp((cid:107)v(cid:107)) =

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(O(

d)) E[1[t,+∞](z)] = exp(O(

d)) Pr[z ≥ t]

√

Since by Lemma A.1, Pr[z ≥ t] ≤ exp(−Ω(log2 n), we get

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(O(

d) − Ω(log2 n)) = exp(−Ω(log1.8 n))

as we wanted.

So, we may, without loss of generality assume that d = Ω(log4 n). In this case, Lemma A.2 implies

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(− log1.8 n) + exp(−Ω(d))) = exp(− log1.8 n)

where the last inequality holds because d = Ω(log4 n) and t2 = Ω(log.9 n), so we get the claim we wanted.

√

√

Lemma A.4 (Continuous Abel’s Inequality). Let 0 ≤ r(x) ≤ 1 be a function such that such that E[r(x)] = ρ.
Moreover, suppose increasing function u(x) satisﬁes that E[|u(x)|] < ∞. Let t be the real number such that
E[1[t,+∞]] = ρ. Then we have

E[u(x)r(x)] ≤ E[u(x)1[t,+∞]]

(A.20)

z f (x)r(x)dx, and H(z) = (cid:82) ∞

Proof. Let G(z) = (cid:82) ∞
z f (x)1[t,+∞](x)dx. Then we have that G(z) ≤ H(z) for
all z. Indeed, for z ≥ t, this is trivial since r(z) ≤ 1. For z ≤ t, we have H(z) = E[1[t,+∞]] = ρ = E[r(x)] ≥
(cid:82) ∞
z f (x)r(x)dx. Then by integration by parts we have,
(cid:90) ∞

(cid:90) ∞

u(x)f (x)r(x)dx = −

u(x)dG

−∞

−∞

= −u(x)G(x) |∞

−∞ +

G(x)u(cid:48)(x)dx

(cid:90) +∞

−∞

≤

=

(cid:90) +∞

−∞
(cid:90) ∞

−∞

H(x)u(cid:48)(x)dx

u(x)f (x)1[t,+∞](x)dx,

where at the third line we use the fact that u(x)G(x) → 0 as x → ∞ and that u(cid:48)(x) ≥ 0, and at the last line
we integrate by parts again.

Lemma A.5. Let v ∈ Rd be a ﬁxed vector with norm (cid:107)v(cid:107) ≤ κ
variable c with uniform distribution over the sphere, we have that

√

d for absolute constant κ. Then for random

log E[exp((cid:104)v, d(cid:105))] = (cid:107)v(cid:107)2/2d ± (cid:15)

(A.21)

where (cid:15) = (cid:101)O( 1

d ).

25

Proof. Let g ∈ N (0, I), then g/(cid:107)g(cid:107) has the same distribution as c. Let r = (cid:107)v(cid:107). Since c is spherically
symmetric, we could, we can assume without loss of generality that v = (r, 0, . . . , 0). Let x = g1 and
y = (cid:112)g2
d. Therefore x ∈ N (0, 1) and y2 has χ2 distribution with mean d − 1 and variance O(d).
d. Note that the Pr[F] ≥ 1−exp(−Ω(log1.8(d)).

Let F be the event that x ≤ 20 log d and 1.5

2 + · · · + g2

d ≥ y ≥ 0.5

√

√

By Proposition 2, we have that E[exp((cid:104)v, c(cid:105))] = E[exp((cid:104)v, c(cid:105)) | F] · (1 ± Ω(− log1.8 d)).

Conditioned on event F, we have

E[exp((cid:104)v, c(cid:105)) | F] = E

exp(

(cid:35)

rx
(cid:112)x2 + y2

) | F

(cid:34)

(cid:34)

(cid:34)

(cid:20)

= E

exp(

−

rx3
y(cid:112)x2 + y2(y + (cid:112)x2 + y2)

) | F

(cid:35)

= E

exp(

) · exp(

rx3
y(cid:112)x2 + y2(y + (cid:112)x2 + y2)

) | F

(cid:35)

= E

exp(

) | F

· (1 ± O(

(cid:21)

))

log3 d
d
√

√

rx
y

rx
y

rx
y

where we used the fact that r ≤ κ
we have that

√

d. Let E be the event that 1.5

d ≥ y ≥ 0.5

d. By using Proposition 1,

E[exp(rx/y) | F] = E[exp(rx/y) | E] ± exp(−Ω(log2(d))

(A.23)

Then let z = y2/(d − 1) and w = z − 1. Therefore z has χ2 distribution with mean 1 and variance 1/(d − 1),
and w has mean 0 and variance 1/(d − 1).

(A.22)

E

exp(

) | E

= E[E[exp(rx/y) | y] | E] = E[exp(r2/y2) | E]

(cid:20)

(cid:21)

rx
y

= E[exp(r2/(d − 1) · 1/z2) | E]

= E[exp(r2/(d − 1) · (1 +

= exp(r2/(d − 1)) E[exp(1 +

2w + w2
(1 + w)2 )) | E]
2w + w2
(1 + w)2 )) | E]

= exp(r2/(d − 1)) E[1 + 2w ± O(w2) | E]
= exp(r2/(d − 1)2)(1 ± 1/d)

where the second-to-last line uses the fact that conditioned on 1/2 ≥ E, w ≥ −1/2 and therefore the Taylor
expansion approximates the exponential accurately, and the last line uses the fact that | E[w | E]| = O(1/d)
and E[w2 | E] ≤ O(1/d). Combining the series of approximations above completes the proof.

We ﬁnally provide the proofs for a few helper propositions on conditional probabilities for high probability

events used in the lemma above.

Proposition 1. Suppose x ∼ N (0, σ2) with σ = O(1). Then for any event E with Pr[E] = 1−O(−Ω(log2 d)),
we have that E[exp(x)] = E[exp(x) | E] ± exp(−Ω(log2(d)).

Proof. Let’s denote by ¯E the complement of the event E. We will consider the upper and lower bound
separately. Since

E[exp(x)] = E[exp(x) | E] Pr[E] + E[exp(x) | ¯E] Pr[ ¯E]

we have that

E[exp(x)] ≤ E[exp(x) | E] + E[exp(x) | ¯E] Pr[ ¯E]

(A.24)

26

and

E[exp(x)] ≥ E[exp(x) | E](1 − exp(−Ω(log2 d))) ≥ E[exp(x) | E] − E[exp(x) | E] exp(−Ω(log2 d)))

(A.25)

Consider the upper bound (A.24) ﬁrst. To show the statement of the lemma, it suﬃces to bound

E[exp(x) | ¯E] Pr[ ¯E].

Working towards that, notice that

E[exp(x) | ¯E] Pr[ ¯E] = E[exp(x)1 ¯E ] = E[exp(x) E[1 ¯E |x]] = E[exp(x)r(x)]

if we denote r(x) = E[1 ¯E |x]. We wish to upper bound E[exp(x)r(x)]. By Lemma A.4, we have

E[exp(x)r(x)] ≤ E[exp(x)1[t,∞]]

where t is such that E[1[t,∞]] = E[r(x)]. However, since E[r(x)] = Pr[ ¯E] = exp(−Ω(log2 d)), it must be the
case that t = Ω(log d) by the standard Gaussian tail bound, and the assumption that σ = O(1). In turn,
this means

E[exp(x)1[t,∞]] ≤

(cid:90) ∞

1
√
2π

σ

t

exe− x2

σ2 dx =

(cid:90) ∞

1
√
2π

σ

t

e−( x

σ − σ

2 )2+ σ2

4 dx = e

σ2
4

e−(x(cid:48)− σ

2 )2

dx(cid:48)

1
√
2π

(cid:90) +∞

t/σ

where the last equality follows from the change of variables x = σx(cid:48). However,

1
√
2π

(cid:90) +∞

t/σ

e−(x(cid:48)− σ

2 )2

dx(cid:48)

is nothing more than Pr[x(cid:48) > t
1. Bearing in mind that σ = O(1)

σ ], where x(cid:48) is distributed like a univariate gaussian with mean σ

2 and variance

σ2
4

e

1
√
2π

(cid:90) +∞

t/σ

e−(x(cid:48)− σ

2 )2

dx(cid:48) = exp(−Ω(t2)) = exp(−Ω(log2 d))

by the usual Gaussian tail bounds, which proves the lower bound we need.

We proceed to consider the lower bound A.25. To show the statement of the lemma, we will bound

E[exp(x) | E]. Notice trivially that since exp(x) ≥ 0,

E[exp(x) | E] ≤

E[exp(x)]
Pr[E]

Since Pr[E] ≥ 1 − exp(Ω(log2 d)),

1

Pr[E] ≤ 1 + exp(O(log2)). So, it suﬃces to bound E[exp(x)]. However,

E[exp(x)] =

1
√
2π

σ

(cid:90) +∞

t=−∞

exe− x2

σ2 dx =

1
√
2π

σ

(cid:90) +∞

t=−∞

e−( x

σ − σ

2 )2+ σ2

4 dx =

e−(x(cid:48)− σ

2 )2+ σ2

4 dx(cid:48)

1
√
2π

(cid:90) +∞

t=−∞

where the last equality follows from the same change of variables x = σx(cid:48) as before. Since (cid:82) +∞
√

t=−∞ e−(x(cid:48)− σ

2 )2

dx(cid:48) =

2π, we get

(cid:90) +∞

1
√
2π

e−(x(cid:48)− σ

2 )2+ σ2

4 dx(cid:48) = e

σ2
4 = O(1)

t=−∞
Pr[E] , we get that E[exp(x) | E] = O(1). Plugging this back in A.25,

1

Putting together with the estimate of
we get the desired upper bound.

Proposition 2. Suppose c ∼ C and v is an arbitrary vector with (cid:107)v(cid:107) = O(
Pr[E] ≥ 1 − exp(−Ω(log2 d)), we have that E[exp((cid:104)v, c(cid:105))] = E[exp((cid:104)v, c(cid:105)) | E] ± exp(− log1.8 d).

d). Then for any event E with

√

27

Proof of Proposition 2. Let z = (cid:104)v, c(cid:105). We proceed similarly as in the proof of Proposition 1. We have

and

and

E[exp(z)] = E[exp(z) | E] Pr[E] + E[exp(z) | ¯E] Pr[ ¯E]

E[exp(z)] ≤ E[exp(z) | E] + E[exp(z) | ¯E] Pr[ ¯E]

E[exp(z)] ≥ E[exp(z) | E] Pr[E] = E[exp(z) | E] − E[exp(z) | E] exp(−Ω(log2 d))

We again proceed by separating the upper and lower bound.

We ﬁrst consider the upper bound A.26.
Notice that that

E[exp(z) | ¯E] Pr[ ¯E] = E[exp(z)1 ¯E ]

We can split the last expression as

E (cid:2)exp((cid:104)vw, c(cid:105))1(cid:104)vw,c(cid:105)>01E

(cid:3) + E (cid:2)exp((cid:104)vw, c(cid:105))1(cid:104)vw,c(cid:105)<01E

(cid:3) .

The second term is upper bounded by

E[1E ] ≤ exp(−Ω(log2 n))

We proceed to the ﬁrst term of (A.10) and observe the following property of it:

E (cid:2)exp((cid:104)vw, c(cid:105))1(cid:104)vw,c(cid:105)>01E

(cid:3) ≤ E (cid:2)exp((cid:104)αvw, c(cid:105))1(cid:104)vw,c(cid:105)>0 1E

(cid:3) ≤ E [exp((cid:104)αvw, c(cid:105))1E ]

(A.26)

(A.27)

where α > 1. Therefore, it’s suﬃcient to bound

√

when (cid:107)vw(cid:107) = Θ(

d). Let’s denote r(z) = E[1 ¯E |z].

Using Lemma A.4, we have that

E [exp(z)1E ]

[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3)
E
c

(A.28)

where t satisﬁes that Ec[1[t,+∞]] = Pr[z ≥ t] = Ec[r(z)] ≤ exp(−Ω(log2 d)). Then, we claim Pr[z ≥ t] ≤
exp(−Ω(log2 d)) implies that t ≥ Ω(log.9 d).

Indeed, this follows by directly applying Lemma A.1. Afterward, applying Lemma A.2, we have:

E[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(log1.8 d))

(A.29)

which proves the upper bound we want.

We now proceed to the lower bound A.27, which is again similar to the lower bound in the proof of

Proposition 1: we just need to bound E[exp(z) | E]. Same as in Proposition 1, since exp(x) ≥ 0,

E[exp(z) | E] ≤

E[exp(z)]
Pr[E]

Consider the event E (cid:48) : z ≤ t, for t = Θ(log.9 d), which by Lemma A.1 satisﬁes Pr[E (cid:48)] ≥ 1 − exp(−Ω(log2 d)).
By the upper bound we just showed,

E[exp(z)] ≤ E[exp(z) | E (cid:48)] + exp(−Ω(log2 n)) = O(exp(log.9 d))

where the last equality follows since conditioned on E (cid:48), z = O(log.9 d). Finally, this implies
1
Pr[E]

O(exp(log.9 d)) = O(exp(log.9 d))

E[exp(z) | E] ≤

where the last equality follows since Pr[E] ≥ 1 − exp(−Ω(log2 n)). Putting this together with A.27, we get
E[exp(z)] ≥ E[exp(z) | E] Pr[E] = E[exp(z) | E] − E[exp(z) | E] exp(−Ω(log2 d)) ≥
E[exp(z) | E] − O(exp(log.9 d)) exp(−Ω(log2 d)) ≥ E[exp(z) | E] − exp(−Ω(log2 d))

which is what we needed.

28

A.1 Analyzing partition function Zc

In this section, we prove Lemma 2.1. We basically ﬁrst prove that for the means of Zc are all (1 + o(1))-close
to each other, and then prove that Zc is concentrated around its mean. It turns out the concentration part
is non trivial because the random variable of concern, exp((cid:104)vw, c(cid:105)) is not well-behaved in terms of the tail.
Note that exp((cid:104)vw, c(cid:105)) is NOT sub-gaussian for any variance proxy. This essentially disallows us to use an
existing concentration inequality directly. We get around this issue by considering the truncated version of
exp((cid:104)vw, c(cid:105)), which is bounded, and have similar tail properties as the original one, in the regime that we
are concerning.

We bound the mean and variance of Zc ﬁrst in the Lemma below.

Lemma A.6. For any ﬁxed unit vector c ∈ Rd, we have that E[Zc] ≥ n and V[Zc] ≤ O(n).

Proof of Lemma A.6. Recall that by deﬁnition

Zc =

exp((cid:104)vw, c(cid:105)).

(cid:88)

w

We ﬁx context c and view vw’s as random variables throughout this proof. Recall that vw is composed of
vw = sw · ˆvw, where sw is the scaling and ˆvw is from spherical Gaussian with identity covariance Id×d. Let
s be a random variable that has the same distribution as sw.

We lowerbound the mean of Zc as follows:

E[Zc] = n E [exp((cid:104)vw, c(cid:105))] ≥ n E [1 + (cid:104)vw, c(cid:105)] = n

where the last equality holds because of the symmetry of the spherical Gaussian distibution. On the other
hand, to upperbound the mean of Zc, we condition on the scaling sw,

E[Zc] = n E[exp((cid:104)vw, c(cid:105))]

= n E [E [exp((cid:104)vw, c(cid:105)) | sw]]

Note that conditioned on sw, we have that (cid:104)vw, c(cid:105) is a Gaussian random variable with variance σ2 = s2
w.

Therefore,

E [exp((cid:104)vw, c(cid:105)) | sw] =

(cid:90)

σ

x
(cid:90)

1
√
2π
1
√
σ
2π
= exp(σ2/2)

=

x

exp(−

exp(−

x2
2σ2 ) exp(x)dx
(x − σ2)2
2σ2

+ σ2/2)dx

It follows that

E[Zc] = n E[exp(σ2/2)] = n E[exp(s2

w/2)] = n E[exp(s2/2)].

We calculate the variance of Zc as follows:

V[Zc] =

V [exp((cid:104)vw, c(cid:105))] ≤ n E[exp(2(cid:104)vw, c(cid:105))]

(cid:88)

w

= n E [E [exp(2(cid:104)vw, c(cid:105)) | sw]]

By a very similar calculation as above, using the fact that 2(cid:104)vw, c(cid:105) is a Gaussian random variable with
variance 4σ2 = 4s2
w,

E [exp(2(cid:104)vw, c(cid:105)) | sw] = exp(2σ2)

29

Therefore, we have that

V[Zc] ≤ n E [E [exp(2(cid:104)vw, c(cid:105)) | sw]]

= n E (cid:2)exp(2σ2)(cid:3) = n E (cid:2)exp(2s2)(cid:3) ≤ Λn

for Λ = exp(8κ2) a constant, and at the last step we used the facts that s ≤ κ a.s.

Now we are ready to prove Lemma 2.1.

Proof of Lemma 2.1. We ﬁx the choice of c, and the proving the concentration using the randomness of vw’s
ﬁrst. Note that that exp((cid:104)vw, c(cid:105)) is neither sub-Gaussian nor sub-exponential (actually the Orlicz norm of
random variable exp((cid:104)vw, c(cid:105)) is never bounded). This prevents us from applying the usual concentration
inequalities. The proof deals with this issue in a slightly more specialized manner.

Let’s deﬁne Fw be the event that |(cid:104)vw, c(cid:105)| ≤ 1

2 log n. We claim that Pr[Fw] ≥ 1−exp(−Ω(log2 n)). Indeed
note that (cid:104)vw, c(cid:105) | sw has a Gaussian distribution with standard deviation sw(cid:107)c(cid:107) = sw ≤ 2κ a.s. Therefore
by the Gaussianity of (cid:104)vw, c(cid:105) we have that

Pr[|(cid:104)vw, c(cid:105)| ≥ η log n | sw] ≤ 2 exp(−Ω(

log2 n/κ2)) = exp(−Ω(log2 n)),

where Ω(·) hides the dependency on κ which is treated as absolute constants. Taking expectations over sw,
we obtain that

Pr[Fw] = Pr[|(cid:104)vw, c(cid:105)| ≤

log n] ≥ 1 − exp(−Ω(log2 n)).

Note that by deﬁnition, we in particular have that conditioned on Fw, it holds that exp((cid:104)vw, c(cid:105)) ≤

Let the random variable Xw have the same distribution as exp((cid:104)vw, c(cid:105))|Fw . We prove that the random
w Xw concentrates well. By convexity of the exponential function, we have that the mean

variable Z (cid:48)
of Z (cid:48)

c = (cid:80)
c is lowerbounded

√

n.

1
4

1
2

E[Z (cid:48)

c] = n E [exp((cid:104)vw, c(cid:105))|Fw ] ≥ n exp(E [(cid:104)vw, c(cid:105)|Fw ]) = n

and the variance is upperbounded by

V[Z (cid:48)

c] ≤ n E (cid:2)exp((cid:104)vw, c(cid:105))2|Fw

(cid:3)

≤

≤

1
Pr[Fw]
1
Pr[Fw]

E (cid:2)exp((cid:104)vw, c(cid:105))2(cid:3)

Λn ≤ 1.1Λn

where the second line uses the fact that

E (cid:2)exp((cid:104)vw, c(cid:105))2(cid:3)

= Pr[Fw] E (cid:2)exp((cid:104)vw, c(cid:105))2|Fw
≥ Pr[Fw] E (cid:2)exp((cid:104)vw, c(cid:105))2|Fw
√

(cid:3) + Pr[F w] E (cid:2)exp((cid:104)vw, c(cid:105))2|F w
(cid:3) .

(cid:3)

Moreover, by deﬁnition, for any w, |Xw| ≤

n. Therefore by Bernstein’s inequality, we have that

Pr [|Z (cid:48)

c − E[Z (cid:48)

c]| > (cid:15)n] ≤ exp(−

1

2 (cid:15)2n2
√

1.1Λn + 1
3

)

n · (cid:15)n

30

Note that E[Z (cid:48)

c] ≥ n, therefore for (cid:15) (cid:29) log2 n√

n , we have,

Pr [|Z (cid:48)

c − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c]] ≤ Pr [|Z (cid:48)

c − E[Z (cid:48)

c]| > (cid:15)n] ≤ exp(−
√

n}))

1

2 (cid:15)2n2
√

)

n · (cid:15)n

Λn + 1
3

≤ exp(−Ω(min{(cid:15)2n/Λ, (cid:15)
≤ exp(−Ω(log2 n))

Let F = ∪wFw be the union of all Fw. Then by union bound, it holds that Pr[ ¯F] ≤ (cid:80)

n · exp(−Ω(log2 n)) = exp(−Ω(log2 n)). We have that by deﬁnition, Z (cid:48)
Therefore, we have that

w Pr[ ¯Fw] ≤
c has the same distribution as Zc|F .

Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c] | F] ≤ exp(−Ω(log2 n))

(A.30)

and therefore

Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c]] = Pr[F] · Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c] | F] + Pr[ ¯F] Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c] | ¯F]

≤ Pr[|Zc − E[Z (cid:48)
≤ exp(−Ω(log2 n))

c]| > (cid:15) E[Z (cid:48)

c] | F] + Pr[ ¯F]

(A.31)

where at the last line we used the fact that Pr[ ¯F] ≤ exp(−Ω(log2 n)) and equation (A.30).

Let Z = E[Z (cid:48)

c] = E[exp((cid:104)vw, c(cid:105)) | | (cid:104)vw, c(cid:105) | < 1

2 log n] (note that E[Z (cid:48)

c] only depends on the norm of (cid:107)c(cid:107)

which is equal to 1). Therefore we obtain that with high probability over the randomness of vw’s,

(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z

(A.32)

Taking expectation over the randomness of c, we have that

[ (A.32) holds] ≥ 1 − exp(−Ω(log2 n))

Pr
c,vw

Therefore by a standard averaging argument (using Markov inequality), we have

(cid:104)

Pr
vw

Pr
c

[ (A.32) holds] ≥ 1 − exp(−Ω(log2 n))

≥ 1 − exp(−Ω(log2 n))

(cid:105)

For now on we ﬁx a choice of vw’s so that Prc[ (A.32) holds] ≥ 1 − exp(−Ω(log2 n)) is true. Therefore in the
rest of the proof, only c is random variable, and with probability 1 − exp(−Ω(log2 n)) over the randomness
of c, it holds that,

(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z.

(A.33)

B Maximum likelihood estimator for co-occurrence

Let L be the corpus size, and Xw,w(cid:48) the number of times words w, w(cid:48) co-occur within a context of size 10
in the corpus. According to the model, the probability of this event at any particular time is log p(w, w(cid:48)) ∝
(cid:107)vw + vw(cid:48)(cid:107)2
2 . Successive samples from a random walk are not independent of course, but if the random walk
mixes fairly quickly (and the mixing time of our random walk is related to the logarithm of the number
of words) then the set of Xw,w(cid:48)’s over all word pairs is distributed up to a very close approximation as a
multinomial distribution Mul( ˜L, {p(w, w(cid:48))}) where ˜L = (cid:80)
w,w(cid:48) Xw,w(cid:48) is the total number of word pairs in
consideration (roughly 10L).

31

Assuming this approximation, we show below that the maximum likelihood values for the word vectors

correspond to the following optimization,

min
{vw},C

(cid:88)

w,w(cid:48)

(cid:16)

Xw,w(cid:48)

log(Xw,w(cid:48)) − (cid:107)vw +vw(cid:48)(cid:107)2

2 − C

(cid:17)2

(Objective SN)

Now we give the derivation of the objective. According to the multinomial distribution, maximizing the

likelihood of {Xw,w(cid:48)} is equivalent to maximizing

To reason about the likelihood, denote the logarithm of the ratio between the expected count and the

empirical count as

Note that

(cid:96) = log

p(w, w(cid:48))Xw,w(cid:48)

 =

Xw,w(cid:48) log p(w, w(cid:48)).





(cid:89)

(w,w(cid:48))



(cid:88)

(w,w(cid:48))

∆w,w(cid:48) = log

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

(cid:33)

.

(cid:88)

(cid:96) =

Xw,w(cid:48) log p(w, w(cid:48))

(w,w(cid:48))

(cid:88)

(w,w(cid:48))

(cid:88)

(w,w(cid:48))

=

=

Xw,w(cid:48)

log

+ log

(cid:34)

Xw,w(cid:48)
˜L

(cid:33)(cid:35)

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

Xw,w(cid:48) log

Xw,w(cid:48) log

Xw,w(cid:48)
˜L

(cid:88)

+

(w,w(cid:48))

(cid:33)

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

where we let c denote the constant (cid:80)

(w,w(cid:48)) Xw,w(cid:48) log Xw,w(cid:48)
˜L

. Furthermore, we have

(B.1)

(cid:88)

= c +

Xw,w(cid:48)∆w,w(cid:48)

(w,w(cid:48))

(cid:88)

˜L =

˜Lpw,w(cid:48)

Xw,w(cid:48)e∆w,w(cid:48)

=

=

(w,w(cid:48))
(cid:88)

(w,w(cid:48))
(cid:88)

(w,w(cid:48))

Xw,w(cid:48)(1 + ∆w,w(cid:48) + ∆2

w,w(cid:48)/2 + O(|∆w,w(cid:48)|3))

and also ˜L = (cid:80)

(w,w(cid:48)) Xw,w(cid:48). So

Plugging this into (3.2) leads to

Xw,w(cid:48)∆w,w(cid:48) = −

Xw,w(cid:48)∆2

w,w(cid:48)/2 +

Xw,w(cid:48)O(|∆w,w(cid:48)|3)

 .





(cid:88)

(w,w(cid:48))

(cid:88)

(w,w(cid:48))



(cid:88)

(w,w(cid:48))

c − (cid:96) =

Xw,w(cid:48)∆2

w,w(cid:48)/2 +

Xw,w(cid:48)O(|∆w,w(cid:48)|3).

(B.2)

(cid:88)

(w,w(cid:48))

(cid:88)

(w,w(cid:48))

32

When the last term is much smaller than the ﬁrst term on the right hand side, maximizing the likelihood

is approximately equivalent to minimizing the ﬁrst term on the right hand side, which is our objective:

(cid:88)

(w,w(cid:48))

Xw,w(cid:48)∆2

w,w(cid:48) ≈

Xw,w(cid:48)

(cid:107)vw + vw(cid:48)(cid:107)2

2/(2d) − log Xw,w(cid:48) + log ˜L − 2 log Z

(cid:17)2

(cid:16)

(cid:88)

(w,w(cid:48))

where Z is the partition function.

We now argue that the last term is much smaller than the ﬁrst term on the right hand side in (B.2). For
a large Xw,w(cid:48), the ∆w,w(cid:48) is close to 0 and thus the induced approximation error is small. Small Xw,w(cid:48)’s only
contribute a small fraction of the ﬁnal objective (3.3), so we can safely ignore the errors. To see this, note
that the objective (cid:80)
(w,w(cid:48)) Xw,w(cid:48)O(|∆w,w(cid:48)|3) diﬀer by a factor of
|∆w,w(cid:48)| for each Xw,w(cid:48). For large Xw,w(cid:48)’s, |∆w,w(cid:48)| (cid:28) 1, and thus their corresponding errors are much smaller
than the objective. So we only need to consider the Xw,w(cid:48)’s that are small constants. The co-occurrence
counts obey a power law distribution (see, e.g. (Pennington et al., 2014)). That is, if one sorts {Xw,w(cid:48)} in
decreasing order, then the r-th value in the list is roughly

w,w(cid:48) and the error term (cid:80)

(w,w(cid:48)) Xw,w(cid:48)∆2

where k is some constant. Some calculation shows that

x[r] =

k
r5/4

and thus when x is a small constant

˜L ≈ 4k,

(cid:88)

Xw,w(cid:48) ≈ 4k4/5x1/5,

Xw,w(cid:48) ≤x

(cid:80)

Xw,w(cid:48) ≤x Xw,w(cid:48)
˜L

≈

(cid:18) 4x
˜L

(cid:19)1/5

= O

(cid:18) 1

˜L1/5

(cid:19)

.

So there are only a negligible mass of Xw,w(cid:48)’s that are small constants, which vanishes when ˜L increases.
Furthermore, we empirically observe that the relative error of our objective is 5%, which means that the
errors induced by Xw,w(cid:48)’s that are small constants is only a small fraction of the objective. Therefore,
(cid:80)

w,w(cid:48) Xw,w(cid:48)O(|∆w,w(cid:48)|3) is small compared to the objective and can be safely ignored.

33

9
1
0
2
 
n
u
J
 
9
1
 
 
]

G
L
.
s
c
[
 
 
8
v
0
2
5
3
0
.
2
0
5
1
:
v
i
X
r
a

RAND-WALK: A latent variable model approach to word
embeddings

Sanjeev Arora

Yuanzhi Li

Yingyu Liang

Tengyu Ma

Andrej Risteski ∗

Abstract

Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse
methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparam-
eters and reweighting methods.

This paper proposes a new generative model, a dynamic version of the log-linear topic model of Mnih
and Hinton (2007). The methodological novelty is to use the prior to compute closed form expressions
for word statistics. This provides a theoretical justiﬁcation for nonlinear models like PMI, word2vec,
and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic
embeddings contain linear algebraic structure that allows solution of word analogies, as shown by Mikolov
et al. (2013a) and many subsequent papers.

Experimental support is provided for the generative model assumptions, the most important of which

is that latent word vectors are fairly uniformly dispersed in space.

1 Introduction

Vector representations of words (word embeddings) try to capture relationships between words as distance or
angle, and have many applications in computational linguistics and machine learning. They are constructed
by various models whose unifying philosophy is that the meaning of a word is deﬁned by “the company it
keeps” (Firth, 1957), namely, co-occurrence statistics. The simplest methods use word vectors that explicitly
represent co-occurrence statistics. Reweighting heuristics are known to improve these methods, as is dimen-
sion reduction (Deerwester et al., 1990). Some reweighting methods are nonlinear, which include taking the
square root of co-occurrence counts (Rohde et al., 2006), or the logarithm, or the related Pointwise Mutual
Information (PMI) (Church and Hanks, 1990). These are collectively referred to as Vector Space Models,
surveyed in (Turney and Pantel, 2010).

Neural network language models (Rumelhart et al., 1986; 1988; Bengio et al., 2006; Collobert and Weston,
2008a) propose another way to construct embeddings: the word vector is simply the neural network’s internal
representation for the word. This method is nonlinear and nonconvex. It was popularized via word2vec, a
family of energy-based models in (Mikolov et al., 2013b;c), followed by a matrix factorization approach called
GloVe (Pennington et al., 2014). The ﬁrst paper also showed how to solve analogies using linear algebra on
word embeddings. Experiments and theory were used to suggest that these newer methods are related to
the older PMI-based models, but with new hyperparameters and/or term reweighting methods (Levy and
Goldberg, 2014b).

But note that even the old PMI method is a bit mysterious. The simplest version considers a symmetric
matrix with each row/column indexed by a word. The entry for (w, w(cid:48)) is PMI(w, w(cid:48)) = log p(w,w(cid:48))
p(w)p(w(cid:48)) , where
p(w, w(cid:48)) is the empirical probability of words w, w(cid:48) appearing within a window of certain size in the corpus,
and p(w) is the marginal probability of w. (More complicated models could use asymmetric matrices with

∗Princeton University, Computer Science Department. {arora,yuanzhil,yingyul,tengyu,risteski}@cs.princeton.edu.
This work was supported in part by NSF grants CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, Simons Investigator
Award, and Simons Collaboration Grant. Tengyu Ma was also supported by Simons Award for Graduate Students in Theoretical
Computer Science and IBM PhD Fellowship.

1

columns corresponding to context words or phrases, and also involve tensorization.) Then word vectors are
obtained by low-rank SVD on this matrix, or a related matrix with term reweightings. In particular, the
PMI matrix is found to be closely approximated by a low rank matrix: there exist word vectors in say 300
dimensions, which is much smaller than the number of words in the dictionary, such that

(cid:104)vw, vw(cid:48)(cid:105) ≈ PMI(w, w(cid:48))

(1.1)

where ≈ should be interpreted loosely.

There appears to be no theoretical explanation for this empirical ﬁnding about the approximate low
rank of the PMI matrix. The current paper addresses this. Speciﬁcally, we propose a probabilistic model
of text generation that augments the log-linear topic model of Mnih and Hinton (2007) with dynamics, in
the form of a random walk over a latent discourse space. The chief methodological contribution is using the
model priors to analytically derive a closed-form expression that directly explains (1.1); see Theorem 2.2
in Section 2. Section 3 builds on this insight to give a rigorous justiﬁcation for models such as word2vec
and GloVe, including the hyperparameter choices for the latter. The insight also leads to a mathematical
explanation for why these word embeddings allow analogies to be solved using linear algebra; see Section 4.
Section 5 shows good empirical ﬁt to this model’s assumtions and predictions, including the surprising one
that word vectors are pretty uniformly distributed (isotropic) in space.1

1.1 Related work

Latent variable probabilistic models of language have been used for word embeddings before, including
Latent Dirichlet Allocation (LDA) and its more complicated variants (see the survey (Blei, 2012)), and some
neurally inspired nonlinear models (Mnih and Hinton, 2007; Maas et al., 2011). In fact, LDA evolved out of
eﬀorts in the 1990s to provide a generative model that “explains” the success of older vector space methods
like Latent Semantic Indexing (Papadimitriou et al., 1998; Hofmann, 1999). However, none of these earlier
generative models has been linked to PMI models.

Levy and Goldberg (2014b) tried to relate word2vec to PMI models. They showed that if there were no
dimension constraint in word2vec, speciﬁcally, the “skip-gram with negative sampling (SGNS)” version of the
model, then its solutions would satisfy (1.1), provided the right hand side were replaced by PMI(w, w(cid:48)) − β
for some scalar β. However, skip-gram is a discriminative model (due to the use of negative sampling), not
generative. Furthermore, their argument only applies to very high-dimensional word embeddings, and thus
does not address low-dimensional embeddings, which have superior quality in applications.

Hashimoto et al. (2016) focuses on issues similar to our paper. They model text generation as a random
walk on words, which are assumed to be embedded as vectors in a geometric space. Given that the last word
produced was w, the probability that the next word is w(cid:48) is assumed to be given by h(|vw − vw(cid:48)|2) for a
suitable function h, and this model leads to an explanation of (1.1). By contrast our random walk involves
a latent discourse vector, which has a clearer semantic interpretation and has proven useful in subsequent
work, e.g. understanding structure of word embeddings for polysemous words Arora et al. (2016). Also our
work clariﬁes some weighting and bias terms in the training objectives of previous methods (Section 3) and
also the phenomenon discussed in the next paragraph.

Researchers have tried to understand why vectors obtained from the highly nonlinear word2vec models
exhibit linear structures (Levy and Goldberg, 2014a; Pennington et al., 2014). Speciﬁcally, for analogies like
“man:woman::king:??,” queen happens to be the word whose vector vqueen is the most similar to the vector
vking − vman + vwoman. This suggests that simple semantic relationships, such as masculine vs feminine
tested in the above example, correspond approximately to a single direction in space, a phenomenon we will
henceforth refer to as relations=lines.

Section 4 surveys earlier attempts to explain this phenomenon and their shortcoming, namely, that they
ignore the large approximation error in relationships like (1.1). This error appears larger than the diﬀerence
between the best solution and the second best (incorrect) solution in analogy solving, so that this error could
in principle lead to a complete failure in analogy solving. In our explanation, the low dimensionality of the

1The code is available at https://github.com/PrincetonML/SemanticVector

2

word vectors plays a key role. This can also be seen as a theoretical explanation of the old observation that
dimension reduction improves the quality of word embeddings for various tasks. The intuitive explanation
often given —that smaller models generalize better—turns out to be fallacious, since the training method
for creating embeddings makes no reference to analogy solving. Thus there is no a priori reason why low-
dimensional model parameters (i.e., lower model capacity) should lead to better performance in analogy
solving, just as there is no reason they are better at some other unrelated task like predicting the weather.

1.2 Beneﬁts of generative approaches

In addition to giving some form of “uniﬁcation” of existing methods, our generative model also brings
more intepretability to word embeddings beyond traditional cosine similarity and even analogy solving. For
example, it led to an understanding of how the diﬀerent senses of a polysemous word (e.g., bank) reside in
linear superposition within the word embedding (Arora et al., 2016). Such insight into embeddings may
prove useful in the numerous settings in NLP and neuroscience where they are used.

Another new explanatory feature of our model is that low dimensionality of word embeddings plays a key
theoretical role —unlike in previous papers where the model is agnostic about the dimension of the embed-
dings, and the superiority of low-dimensional embeddings is an empirical ﬁnding (starting with Deerwester
et al. (1990)). Speciﬁcally, our theoretical analysis makes the key assumption that the set of all word vectors
(which are latent variables of the generative model) are spatially isotropic, which means that they have no
preferred direction in space. Having n vectors be isotropic in d dimensions requires d (cid:28) n. This isotropy is
needed in the calculations (i.e., multidimensional integral) that yield (1.1). It also holds empirically for our
word vectors, as shown in Section 5.

The isotropy of low-dimensional word vectors also plays a key role in our explanation of the rela-
tions=lines phenomenon (Section 4). The isotropy has a “puriﬁcation” eﬀect that mitigates the eﬀect of
the (rather large) approximation error in the PMI models.

2 Generative model and its properties

The model treats corpus generation as a dynamic process, where the t-th word is produced at step t. The
process is driven by the random walk of a discourse vector ct ∈ (cid:60)d. Its coordinates represent what is being
talked about.2 Each word has a (time-invariant) latent vector vw ∈ (cid:60)d that captures its correlations with
the discourse vector. We model this bias with a log-linear word production model:

Pr[w emitted at time t | ct] ∝ exp((cid:104)ct, vw(cid:105)).

(2.1)

The discourse vector ct does a slow random walk (meaning that ct+1 is obtained from ct by adding a small
random displacement vector), so that nearby words are generated under similar discourses. We are interested
in the probabilities that word pairs co-occur near each other, so occasional big jumps in the random walk
are allowed because they have negligible eﬀect on these probabilities.

A similar log-linear model appears in Mnih and Hinton (2007) but without the random walk. The linear
chain CRF of Collobert and Weston (2008b) is more general. The dynamic topic model of Blei and Laﬀerty
(2006) utilizes topic dynamics, but with a linear word production model. Belanger and Kakade (2015) have
proposed a dynamic model for text using Kalman Filters, where the sequence of words is generated from
Gaussian linear dynamical systems, rather than the log-linear model in our case.

The novelty here over such past works is a theoretical analysis in the method-of-moments tradition (Hsu
et al., 2012; Cohen et al., 2012). Assuming a prior on the random walk we analytically integrate out the
hidden random variables and compute a simple closed form expression that approximately connects the
model parameters to the observable joint probabilities (see Theorem 2.2). This is reminiscent of analysis of
similar random walk models in ﬁnance (Black and Scholes, 1973).

2This is a diﬀerent interpretation of the term “discourse” compared to some other settings in computational linguistics.

3

Model details. Let n denote the number of words and d denote the dimension of the discourse space,
where 1 ≤ d ≤ n. Inspecting (2.1) suggests word vectors need to have varying lengths, to ﬁt the empirical
ﬁnding that word probabilities satisfy a power law. Furthermore, we will assume that in the bulk, the word
vectors are distributed uniformly in space, earlier referred to as isotropy. This can be quantiﬁed as a prior
in the Bayesian tradition. More precisely, the ensemble of word vectors consists of i.i.d draws generated by
v = s · ˆv, where ˆv is from the spherical Gaussian distribution, and s is a scalar random variable. We assume
s is a random scalar with expectation τ = Θ(1) and s is always upper bounded by κ, which is another
constant. Here τ governs the expected magnitude of (cid:104)v, ct(cid:105), and it is particularly important to choose it to
be Θ(1) so that the distribution Pr[w|ct] ∝ exp((cid:104)vw, ct(cid:105)) is interesting.3 Moreover, the dynamic range of
word probabilities will roughly equal exp(κ2), so one should think of κ as an absolute constant like 5. These
details about s are important for realistic modeling but not too important in our analysis. (Furthermore,
readers uncomfortable with this simplistic Bayesian prior should look at Section 2.2 below.)

Finally, we clarify the nature of the random walk. We assume that the stationary distribution of the
random walk is uniform over the unit sphere, denoted by C. The transition kernel of the random walk can
d in (cid:96)2 norm.4
be in any form so long as at each step the movement of the discourse vector is at most (cid:15)2/
This is still fast enough to let the walk mix quickly in the space.

√

The following lemma (whose proof appears in the appendix) is central to the analysis.

It says that
under the Bayesian prior, the partition function Zc = (cid:80)
w exp((cid:104)vw, c(cid:105)), which is the implied normalization
in equation (2.1), is close to some constant Z for most of the discourses c. This can be seen as a plausible
theoretical explanation of a phenomenon called self-normalization in log-linear models: ignoring the partition
function or treating it as a constant (which greatly simpliﬁes training) is known to often give good results.
This has also been studied in (Andreas and Klein, 2014).

Lemma 2.1 (Concentration of partition functions). If the word vectors satisfy the Bayesian prior described
in the model details, then

Pr
c∼C

[(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z] ≥ 1 − δ,

(2.2)

for (cid:15)z = (cid:101)O(1/

n), and δ = exp(−Ω(log2 n)).

√

The concentration of the partition functions then leads to our main theorem (the proof is in the appendix).
The theorem gives simple closed form approximations for p(w), the probability of word w in the corpus, and
p(w, w(cid:48)), the probability that two words w, w(cid:48) occur next to each other. The theorem states the result for
the window size q = 2, but the same analysis works for pairs that appear in a small window, say of size 10,
as stated in Corollary 2.3. Recall that PMI(w, w(cid:48)) = log[p(w, w(cid:48))/(p(w)p(w(cid:48)))].

Theorem 2.2. Suppose the word vectors satisfy the inequality (2.2), and window size q = 2. Then,

for (cid:15) = O((cid:15)z) + (cid:101)O(1/d) + O((cid:15)2). Jointly these imply:

log p(w, w(cid:48)) =

− 2 log Z ± (cid:15),

(cid:107)vw + vw(cid:48)(cid:107)2
2
2d
(cid:107)vw(cid:107)2
2
2d

log p(w) =

− log Z ± (cid:15).

PMI (w, w(cid:48)) =

(cid:104)vw, vw(cid:48)(cid:105)
d

± O((cid:15)).

(2.3)

(2.4)

(2.5)

Remarks 1. Since the word vectors have (cid:96)2 norm of the order of
(cid:107)vw + vw(cid:48)(cid:107)2

d, for two typical word vectors vw, vw(cid:48),
2 is of the order of Θ(d). Therefore the noise level (cid:15) is very small compared to the leading

3A larger τ will make Pr[w|ct] too peaked and a smaller one will make it too uniform.
4 More precisely, the proof extends to any symmetric product stationary distribution C with sub-Gaussian coordinate

satisfying Ec

(cid:2)(cid:107)c(cid:107)2(cid:3) = 1, and the steps are such that for all ct, Ep(ct+1|ct)[exp(κ

d(cid:107)ct+1 − ct(cid:107))] ≤ 1 + (cid:15)2 for some small (cid:15)2.

√

√

4

2d (cid:107)vw + vw(cid:48)(cid:107)2

term 1
empirically we also ﬁnd higher error here.

2. For PMI however, the noise level O((cid:15)) could be comparable to the leading term, and

Remarks 2. Variants of the expression for joint probability in (2.3) had been hypothesized based upon
empirical evidence in Mikolov et al. (2013b) and also Globerson et al. (2007), and Maron et al. (2010) .

Remarks 3. Theorem 2.2 directly leads to the extension to a general window size q as follows:

Corollary 2.3. Let pq(w, w(cid:48)) be the co-occurrence probability in windows of size q, and PMIq(w, w(cid:48)) be the
corresponding PMI value. Then

log pq(w, w(cid:48)) =

− 2 log Z + γ ± (cid:15),

PMIq (w, w(cid:48)) =

+ γ ± O((cid:15)).

(cid:107)vw + vw(cid:48)(cid:107)2
2
2d

(cid:104)vw, vw(cid:48)(cid:105)
d

where γ = log

(cid:16) q(q−1)
2

(cid:17)

.

It is quite easy to see that Theorem 2.2 implies the Corollary 2.3, as when the window size is q the
(cid:1) positions within the window, and the joint probability of w, w(cid:48) is
pair w, w(cid:48) could appear in any of (cid:0)q
roughly the same for any positions because the discourse vector changes slowly.
(Of course, the error
term gets worse as we consider larger window sizes, although for any constant size, the statement of the
theorem is correct.) This is also consistent with the shift β for ﬁtting PMI in (Levy and Goldberg, 2014b),
which showed that without dimension constraints, the solution to skip-gram with negative sampling satisﬁes
PMI (w, w(cid:48)) − β = (cid:104)vw, vw(cid:48)(cid:105) for a constant β that is related to the negative sampling in the optimization.
Our result justiﬁes via a generative model why this should be satisﬁed even for low dimensional word vectors.

2

2.1 Proof sketches

Here we provide the proof sketches, while the complete proof can be found in the appendix.

Proof sketch of Theorem 2.2 Let w and w(cid:48) be two arbitrary words. Let c and c(cid:48) denote two consecutive
context vectors, where c ∼ C and c(cid:48)|c is deﬁned by the Markov kernel p(c(cid:48) | c).

We start by using the law of total expectation, integrating out the hidden variables c and c(cid:48):

An expectation like (2.6) would normally be diﬃcult to analyze because of the partition functions.
However, we can assume the inequality (2.2), that is, the partition function typically does not vary much for
most of context vectors c. Let F be the event that both c and c(cid:48) are within (1 ± (cid:15)z)Z. Then by (2.2) and the
union bound, event F happens with probability at least 1 − 2 exp(−Ω(log2 n)). We will split the right-hand
side (RHS) of (2.6) into the parts according to whether F happens or not.

p(w, w(cid:48)) = E
c,c(cid:48)
= E
c,c(cid:48)

[Pr[w, w(cid:48)|c, c(cid:48)]]

[p(w|c)p(w(cid:48)|c(cid:48))]

= E
c,c(cid:48)

(cid:20) exp((cid:104)vw, c(cid:105))
Zc

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

(cid:21)

RHS of (2.6) = E
c,c(cid:48)
(cid:124)

(cid:20) exp((cid:104)vw, c(cid:105))
Zc

(cid:20) exp((cid:104)vw, c(cid:105))
Zc

+ E
c,c(cid:48)
(cid:124)

(cid:123)(cid:122)
T2

5

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

(cid:123)(cid:122)
T1
exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

1F

1 ¯F

(cid:21)

(cid:125)

(cid:21)

(cid:125)

(2.6)

(2.7)

where ¯F denotes the complement of event F and 1F and 1 ¯F denote indicator functions for F and ¯F ,
respectively. When F happens, we can replace Zc by Z with a 1 ± (cid:15)z factor loss: The ﬁrst term of the RHS
of (2.7) equals to

T1 =

1 ± O((cid:15)z)
Z 2

E
c,c(cid:48)

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ]

(2.8)

On the other hand, we can use E[1 ¯F ] = Pr[ ¯F ] ≤ exp(−Ω(log2 n)) to show that the second term of RHS

of (2.7) is negligible,

|T2| = exp(−Ω(log1.8 n)) .
This claim must be handled somewhat carefully since the RHS does not depend on d at all. Brieﬂy, the
d = o(log2 n)), any word vector vw and
reason this holds is as follows:
d)), and since E[1 ¯F ] = exp(−Ω(log2 n)), the
discourse c satisﬁes that exp((cid:104)vw, c(cid:105)) ≤ exp((cid:107)vw(cid:107)) = exp(O(
d = Ω(log2 n)), we can use concentration inequalities
claim follows directly; In the regime when d is large (
to show that except with a small probability exp(−Ω(d)) = exp(−Ω(log2 n)), a uniform sample from the
sphere behaves equivalently to sampling all of the coordinates from a standard Gaussian distribution with
mean 0 and variance 1

d , in which case the claim is not too diﬃcult to show using Gaussian tail bounds.

in the regime when d is small (

Therefore it suﬃces to only consider (2.8). Our model assumptions state that c and c(cid:48) cannot be too

(2.9)

√

√

√

diﬀerent. We leverage that by rewriting (2.8) a little, and get that it equals

(cid:20)
exp((cid:104)vw, c(cid:105)) E
c(cid:48)|c

(cid:21)
[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

T1 =

=

1 ± O((cid:15)z)
Z 2
1 ± O((cid:15)z)
Z 2

E
c

E
c

[exp((cid:104)vw, c(cid:105))A(c)]

(2.10)

[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]. We claim that A(c) = (1 ± O((cid:15)2)) exp((cid:104)vw(cid:48), c(cid:105)). Doing some algebraic

where A(c) := E
c(cid:48)|c

manipulations,

Furthermore, by our model assumptions, (cid:107)c − c(cid:48)(cid:107) ≤ (cid:15)2/

d. So

√

A(c) = exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48) − c(cid:105))] .

(cid:104)vw, c − c(cid:48)(cid:105) ≤ (cid:107)vw(cid:107)(cid:107)c − c(cid:48)(cid:107) = O((cid:15)2)

and thus A(c) = (1 ± O((cid:15)2)) exp((cid:104)vw(cid:48), c(cid:105)). Plugging the simpliﬁcation of A(c) to (2.10),

T1 =

1 ± O((cid:15)z)
Z 2

E[exp((cid:104)vw + vw(cid:48), c(cid:105))].

Since c has uniform distribution over the sphere, the random variable (cid:104)vw + vw(cid:48), c(cid:105) has distribution pretty
similar to Gaussian distribution N (0, (cid:107)vw + vw(cid:48)(cid:107)2/d), especially when d is relatively large. Observe that
E[exp(X)] has a closed form for Gaussian random variable X ∼ N (0, σ2),

E[exp(X)] =

(cid:90)

1
√
2π

x

σ
= exp(σ2/2) .

exp(−

x2
2σ2 ) exp(x)dx

(2.11)

(2.12)

(2.13)

Bounding the diﬀerence between (cid:104)vw + vw(cid:48), c(cid:105) from Gaussian random variable, we can show that for

(cid:15) = (cid:101)O(1/d),

E[exp((cid:104)vw + vw(cid:48), c(cid:105))] = (1 ± (cid:15)) exp

(cid:18) (cid:107)vw + vw(cid:48)(cid:107)2
2d

(cid:19)

.

Therefore, the series of simpliﬁcation/approximation above (concretely, combining equations (2.6), (2.7), (2.9), (2.11),

and (2.13)) lead to the desired bound on log p(w, w(cid:48)) for the case when the window size q = 2. The bound
on log p(w) can be shown similarly.

6

Proof sketch of Lemma 2.1 Note that for ﬁxed c, when word vectors have Gaussian priors assumed as
in our model, Zc = (cid:80)

w exp((cid:104)vw, c(cid:105)) is a sum of independent random variables.

We ﬁrst claim that using proper concentration of measure tools, it can be shown that the variance of
Zc are relatively small compared to its mean Evw [Zc], and thus Zc concentrates around its mean. Note this
is quite non-trivial: the random variable exp((cid:104)vw, c(cid:105)) is neither bounded nor subgaussian/sub-exponential,
since the tail is approximately inverse poly-logarithmic instead of inverse exponential.
In fact, the same
concentration phenomenon does not happen for w. The occurrence probability of word w is not necessarily
concentrated because the (cid:96)2 norm of vw can vary a lot in our model, which allows the frequency of the words
to have a large dynamic range.

So now it suﬃces to show that Evw [Zc] for diﬀerent c are close to each other. Using the fact that the
word vector directions have a Gaussian distribution, Evw [Zc] turns out to only depend on the norm of c
(which is equal to 1). More precisely,

[Zc] = f ((cid:107)c(cid:107)2

2) = f (1)

E
vw

(2.14)

where f is deﬁned as f (α) = n Es[exp(s2α/2)] and s has the same distribution as the norms of the word
vectors. We sketch the proof of this. In our model, vw = sw · ˆvw, where ˆvw is a Gaussian vector with identity
covariance I. Then

E
vw

[Zc] = n E
vw

[exp((cid:104)vw, c(cid:105))]

(cid:20)

= n E
sw

E
vw|sw

(cid:21)
[exp((cid:104)vw, c(cid:105)) | sw]

where the second line is just an application of the law of total expectation, if we pick the norm of the
(random) vector vw ﬁrst, followed by its direction. Conditioned on sw, (cid:104)vw, c(cid:105) is a Gaussian random variable
with variance (cid:107)c(cid:107)2

w, and therefore using similar calculation as in (2.12), we have

2s2

Hence, Evw [Zc] = n Es[exp(s2(cid:107)c(cid:107)2

2/2)] as needed.

E
vw|sw

[exp((cid:104)vw, c(cid:105)) | sw] = exp(s2(cid:107)c(cid:107)2

2/2) .

Proof of Theorem 4.1 The proof uses the standard analysis of linear regression. Let V = P ΣQT be the
SVD of V and let σ1, . . . , σd be the left singular values of V (the diagonal entries of Σ). For notational ease
we omit the subscripts in ¯ζ and ζ (cid:48) since they are not relevant for this proof. Since V † = QΣ−1P T and thus
¯ζ = V †ζ (cid:48) = QΣ−1P T ζ (cid:48), we have

We claim

Indeed, (cid:80)d
from the ﬁrst assumption. Furthermore, by the second assumption, (cid:107)P T ζ (cid:48)(cid:107)∞ ≤ c2√

i = O(nd), since the average squared norm of a word vector is d. The claim then follows

i=1 σ2

n (cid:107)ζ (cid:48)(cid:107)2, so

(2.15)

(2.16)

(2.17)

Plugging (2.16) and (2.17) into (2.15), we get

(cid:107)¯ζ(cid:107)2 ≤

(cid:114)

(cid:114) 1
c1n

c2
2d
n

(cid:107)ζ (cid:48)(cid:107)2

2 =

(cid:107)ζ (cid:48)(cid:107)2

c2
√

√

d
c1n

as desired. The last statement follows because the norm of the signal, which is d log(νR) originally and is
V †d log(νR) = va − vb after dimension reduction, also gets reduced by a factor of

n.

√

(cid:107)¯ζ(cid:107)2 ≤ σ−1

d (cid:107)P T ζ (cid:48)(cid:107)2.

σ−1
d ≤

(cid:114) 1
c1n

.

(cid:107)P T ζ (cid:48)(cid:107)2

2 ≤

(cid:107)ζ (cid:48)(cid:107)2
2.

c2
2d
n

7

2.2 Weakening the model assumptions

For readers uncomfortable with Bayesian priors, we can replace our assumptions with concrete properties of
word vectors that are empirically veriﬁable (Section 5.1) for our ﬁnal word vectors, and in fact also for word
vectors computed using other recent methods.

The word meanings are assumed to be represented by some “ground truth” vectors, which the experi-
menter is trying to recover. These ground truth vectors are assumed to be spatially isotropic in the bulk,
in the following two speciﬁc ways: (i) For almost all unit vectors c the sum (cid:80)
w exp((cid:104)vw, c(cid:105)) is close to a
constant Z; (ii) Singular values of the matrix of word vectors satisfy properties similar to those of random
matrices, as formalized in the paragraph before Theorem 4.1. Our Bayesian prior on the word vectors hap-
pens to imply that these two conditions hold with high probability. But the conditions may hold even if the
prior doesn’t hold. Furthermore, they are compatible with all sorts of local structure among word vectors
such as existence of clusterings, which would be absent in truly random vectors drawn from our prior.

3 Training objective and relationship to other models

To get a training objective out of Theorem 2.2, we reason as follows. Let Xw,w(cid:48) be the number of times words
w and w(cid:48) co-occur within the same window in the corpus. The probability p(w, w(cid:48)) of such a co-occurrence
at any particular time is given by (2.3). Successive samples from a random walk are not independent.
But if the random walk mixes fairly quickly (the mixing time is related to the logarithm of the vocabulary
size), then the distribution of Xw,w(cid:48)’s is very close to a multinomial distribution Mul( ˜L, {p(w, w(cid:48))}), where
˜L = (cid:80)

w,w(cid:48) Xw,w(cid:48) is the total number of word pairs.

Assuming this approximation, we show below that the maximum likelihood values for the word vectors

correspond to the following optimization,

min
{vw},C

(cid:88)

w,w(cid:48)

(cid:16)

Xw,w(cid:48)

log(Xw,w(cid:48)) − (cid:107)vw +vw(cid:48)(cid:107)2

2 − C

(cid:17)2

As is usual, empirical performance is improved by weighting down very frequent word pairs, possibly
because very frequent words such as “the” do not ﬁt our model. This is done by replacing the weighting
Xw,w(cid:48) by its truncation min{Xw,w(cid:48), Xmax} where Xmax is a constant such as 100. We call this objective with
the truncated weights SN (Squared Norm).

We now give its derivation. Maximizing the likelihood of {Xw,w(cid:48)} is equivalent to maximizing

Denote the logarithm of the ratio between the expected count and the empirical count as

Then with some calculation, we obtain the following where c is independent of the empirical observations
Xw,w(cid:48)’s.

(3.1)

(3.2)

(cid:96) = log

p(w, w(cid:48))Xw,w(cid:48)





(cid:89)

(w,w(cid:48))



 .

∆w,w(cid:48) = log

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

(cid:33)

.

(cid:96) = c +

Xw,w(cid:48)∆w,w(cid:48)

(cid:88)

(w,w(cid:48))

8

On the other hand, using ex ≈ 1 + x + x2/2 when x is small,5 we have

(cid:88)

˜L =

˜Lpw,w(cid:48) =

(cid:88)

Xw,w(cid:48)e∆w,w(cid:48)

(w,w(cid:48))

(w,w(cid:48))

(cid:32)

Xw,w(cid:48)

1 + ∆w,w(cid:48) +

(cid:88)

≈

(w,w(cid:48))

(cid:33)

.

∆2
w,w(cid:48)
2

(cid:88)

(w,w(cid:48))

Xw,w(cid:48)∆w,w(cid:48) ≈ −

Xw,w(cid:48)∆2

w,w(cid:48).

1
2

(cid:88)

(w,w(cid:48))

Note that ˜L = (cid:80)

(w,w(cid:48)) Xw,w(cid:48), so

Plugging this into (3.2) leads to

2(c − (cid:96)) ≈

Xw,w(cid:48)∆2

w,w(cid:48).

(cid:88)

(w,w(cid:48))

(3.3)

So maximizing the likelihood is approximately equivalent to minimizing the right hand side, which (by
examining (3.1)) leads to our objective.

Objective for training with PMI. A similar objective PMI can be obtained from (2.5), by computing
an approximate MLE, using the fact that the error between the empirical and true value of PMI(w, w(cid:48)) is
driven by the smaller term p(w, w(cid:48)), and not the larger terms p(w), p(w(cid:48)).

min
{vw},C

(cid:88)

w,w(cid:48)

Xw,w(cid:48) (PMI(w, w(cid:48)) − (cid:104)vw, vw(cid:48)(cid:105))2

This is of course very analogous to classical VSM methods, with a novel reweighting method.

Fitting to either of the objectives involves solving a version of Weighted SVD which is NP-hard, but

empirically seems solvable in our setting via AdaGrad (Duchi et al., 2011).

Connection to GloVe. Compare SN with the objective used by GloVe (Pennington et al., 2014):

f (Xw,w(cid:48))(log(Xw,w(cid:48)) − (cid:104)vw, vw(cid:48)(cid:105) − sw − sw(cid:48) − C)2

(cid:88)

w,w(cid:48)

with f (Xw,w(cid:48)) = min{X 3/4
w,w(cid:48), 100}. Their weighting methods and the need for bias terms sw, sw(cid:48), C were
derived by trial and error; here they are all predicted and given meanings due to Theorem 2.2, speciﬁcally
sw = (cid:107)vw(cid:107)2.

Connection to word2vec(CBOW). The CBOW model in word2vec posits that the probability of a
word wk+1 as a function of the previous k words w1, w2, . . . , wk:

(cid:16)

p

wk+1

(cid:12)
(cid:12) {wi}k

i=1

(cid:17)

∝ exp((cid:104)vwk+1,

vwi (cid:105)).

1
k

k
(cid:88)

i=1

5This Taylor series approximation has an error of the order of x3, but ignoring it can be theoretically justiﬁed as follows.
is close to 0 and thus ignoring
For a large Xw,w(cid:48) , its value approaches its expectation and thus the corresponding ∆w,w(cid:48)
∆3
w,w(cid:48) is well justiﬁed. The terms where ∆w,w(cid:48) is signiﬁcant correspond to Xw,w(cid:48) ’s that are small. But empirically, Xw,w(cid:48) ’s
obey a power law distribution (see, e.g. Pennington et al. (2014)) using which it can be shown that these terms contribute a
small fraction of the ﬁnal objective (3.3). So we can safely ignore the errors. Full details appear in the ArXiv version of this
paper (Arora et al., 2015).

9

This expression seems mysterious since it depends upon the average word vector for the previous k words.
We show it can be theoretically justiﬁed. Assume a simpliﬁed version of our model, where a small window
of k words is generated as follows: sample c ∼ C, where C is a uniformly random unit vector, then sample
(w1, w2, . . . , wk) ∼ exp((cid:104)(cid:80)k
i=1 vwi , c(cid:105))/Zc. Furthermore, assume Zc = Z for any c.

Lemma 3.1. In the simpliﬁed version of our model, the Maximum-a-Posteriori (MAP) estimate of c given

(w1, w2, . . . , wk) is

(cid:80)k
(cid:107) (cid:80)k

i=1 vwi
i=1 vwi (cid:107)2

.

(cid:80)k
(cid:107) (cid:80)k

i=1 vwi
i=1 vwi (cid:107)2

.

Proof. The c maximizing p (c|w1, w2, . . . , wk) is the maximizer of p(c)p (w1, w2, . . . , wk|c). Since p(c) =
p(c(cid:48)) for any c, c(cid:48), and we have p (w1, w2, . . . , wk|c) = exp((cid:104)(cid:80)
i vwi, c(cid:105))/Z, the maximizer is clearly c =

Thus using the MAP estimate of ct gives essentially the same expression as CBOW apart from the

rescaling, which is often omitted due to computational eﬃciency in empirical works.

4 Explaining relations=lines

As mentioned, word analogies like “a:b::c:??” can be solved via a linear algebraic expression:

argmin
d

(cid:107)va − vb − vc + vd(cid:107)2
2 ,

(4.1)

where vectors have been normalized such that (cid:107)vd(cid:107)2 = 1. This suggests that the semantic relationships
being tested in the analogy are characterized by a straight line,6 referred to earlier as relations=lines.

Using our model we will show the following for low-dimensional embeddings: for each such relation R
there is a direction µR in space such that for any word pair a, b satisfying the relation, va − vb is like µR
plus some noise vector. This happens for relations satisfying a certain condition described below. Empirical
results supporting this theory appear in Section 5, where this linear structure is further leveraged to slightly
improve analogy solving.

A side product of our argument will be a mathematical explanation of the empirically well-established su-
periority of low-dimensional word embeddings over high-dimensional ones in this setting (Levy and Goldberg,
2014a). As mentioned earlier, the usual explanation that smaller models generalize better is fallacious.

We ﬁrst sketch what was missing in prior attempts to prove versions of relations=lines from ﬁrst
principles. The basic issue is approximation error: the diﬀerence between the best solution and the 2nd best
solution to (4.1) is typically small, whereas the approximation error in the objective in the low-dimensional
solutions is larger. For instance, if one uses our PMI objective, then the weighted average of the termwise
error in (2.5) is 17%, and the expression in (4.1) above contains six inner products. Thus in principle the
approximation error could lead to a failure of the method and the emergence of linear relationship, but it
does not.

Prior explanations. Pennington et al. (2014) try to propose a model where such linear relationships
should occur by design. They posit that queen is a solution to the analogy “man:woman::king:??” because

p(χ | king)
p(χ | queen)

≈

p(χ | man)
p(χ | woman)

,

(4.2)

where p(χ | king) denotes the conditional probability of seeing word χ in a small window of text around
king. Relationship (4.2) is intuitive since both sides will be ≈ 1 for gender-neutral χ like “walks” or “food”,

6Note that this interpretation has been disputed; e.g., it is argued in Levy and Goldberg (2014a) that (4.1) can be understood
using only the classical connection between inner product and word similarity, using which the objective (4.1) is slightly
improved to a diﬀerent objective called 3COSMUL. However, this “explanation” is still dogged by the issue of large termwise
error pinpointed here, since inner product is only a rough approximation to word similarity. Furthermore, the experiments in
Section 5 clearly support the relations=lines interpretation.

10

will be > 1 when χ is like “he, Henry” and will be < 1 when χ is like “dress, she, Elizabeth.” This was
also observed by Levy and Goldberg (2014a). Given (4.2), they then posit that the correct model describing
word embeddings in terms of word occurrences must be a homomorphism from ((cid:60)d, +) to ((cid:60)+, ×), so vector
diﬀerences map to ratios of probabilities. This leads to the expression

pw,w(cid:48) = (cid:104)vw, vw(cid:48)(cid:105) + bw + bw(cid:48),

and their method is a (weighted) least squares ﬁt for this expression. One shortcoming of this argument
is that the homomorphism assumption assumes the linear relationships instead of explaining them from a
more basic principle. More importantly, the empirical ﬁt to the homomorphism has nontrivial approximation
error, high enough that it does not imply the desired strong linear relationships.
Levy and Goldberg (2014b) show that empirically, skip-gram vectors satisfy

(cid:104)vw, vw(cid:48)(cid:105) ≈ PMI(w, w(cid:48))

(4.3)

up to some shift. They also give an argument suggesting this relationship must be present if the solution
is allowed to be very high-dimensional. Unfortunately, that argument does not extend to low-dimensional
embeddings. Even if it did, the issue of termwise approximation error remains.

Our explanation. The current paper has introduced a generative model to theoretically explain the
emergence of relationship (4.3). However, as noted after Theorem 2.2, the issue of high approximation error
does not go away either in theory or in the empirical ﬁt. We now show that the isotropy of word vectors
(assumed in the theoretical model and veriﬁed empirically) implies that even a weak version of (4.3) is enough
to imply the emergence of the observed linear relationships in low-dimensional embeddings.

This argument will assume the analogy in question involves a relation that obeys Pennington et al.’s
suggestion in (4.2). Namely, for such a relation R there exists function νR(·) depending only upon R such
that for any a, b satisfying R there is a noise function ξa,b,R(·) for which:

p(χ | a)
p(χ | b)

= νR(χ) · ξa,b,R(χ)

For diﬀerent words χ there is huge variation in (4.4), so the multiplicative noise may be large.

Our goal is to show that the low-dimensional word embeddings have the property that there is a vector
µR such that for every pair of words a, b in that relation, va − vb = µR + noise vector, where the noise vector
is small.

Taking logarithms of (4.4) results in:

log

(cid:19)

(cid:18) p(χ | a)
p(χ | b)

= log(νR(χ)) + ζa,b,R(χ)

Theorem 2.2 implies that the left-hand side simpliﬁes to log

d (cid:104)vχ, va − vb(cid:105) + (cid:15)a,b(χ) where
(cid:15) captures the small approximation errors induced by the inexactness of Theorem 2.2. This adds yet more
noise! Denoting by V the n × d matrix whose rows are the vχ vectors, we rewrite (4.5) as:

(cid:16) p(χ|a)
p(χ|b)

(cid:17)

= 1

V (va − vb) = d log(νR) + ζ (cid:48)

a,b,R

where log(νR) in the element-wise log of vector νR and ζ (cid:48)

a,b,R = d(ζa,b,R − (cid:15)a,b,R) is the noise.

In essence, (4.6) shows that va − vb is a solution to a linear regression in d variables and m constraints,
with ζ (cid:48)
a,b,R being the “noise.” The design matrix in the regression is V , the matrix of all word vectors, which
in our model (as well as empirically) satisﬁes an isotropy condition. This makes it random-like, and thus
solving the regression by left-multiplying by V †, the pseudo-inverse of V , ought to “denoise” eﬀectively. We
now show that it does.

Our model assumed the set of all word vectors satisﬁes bulk properties similar to a set of Gaussian vectors.
The next theorem will only need the following weaker properties. (1) The smallest non-zero singular value

(4.4)

(4.5)

(4.6)

11

(a) SN

(b) GloVe

(c) CBOW

(d) skip-gram

Figure 1: The partition function Zc. The ﬁgure shows the histogram of Zc for 1000 random vectors c of
appropriate norm, as deﬁned in the text. The x-axis is normalized by the mean of the values. The values Zc
for diﬀerent c concentrate around the mean, mostly in [0.9, 1.1]. This concentration phenomenon is predicted
by our analysis.

of V is larger than some constant c1 times the quadratic mean of the singular values, namely, (cid:107)V (cid:107)F /
d.
Empirically we ﬁnd c1 ≈ 1/3 holds; see Section 5. (2) The left singular vectors behave like random vectors
with respect to ζ (cid:48)
a,b,R, for some constant c2.
(3) The max norm of a row in V is O(

a,b,R, namely, have inner product at most c2(cid:107)ζ (cid:48)
√

d). The proof is included in the appendix.

n with ζ (cid:48)

a,b,R(cid:107)/

√

√

Theorem 4.1 (Noise reduction). Under the conditions of the previous paragraph, the noise in the dimension-
reduced semantic vector space satisﬁes

(cid:107)¯ζa,b,R(cid:107)2 (cid:46) (cid:107)ζ (cid:48)

a,b,R(cid:107)2

√

d
n

.

As a corollary, the relative error in the dimension-reduced space is a factor of (cid:112)d/n smaller.

5 Experimental veriﬁcation

In this section, we provide experiments empirically supporting our generative model.

Corpus. All word embedding vectors are trained on the English Wikipedia (March 2015 dump).
It is
pre-processed by standard approach (removing non-textual elements, sentence splitting, and tokenization),
leaving about 3 billion tokens. Words that appeared less than 1000 times in the corpus are ignored, resulting
in a vocabulary of 68, 430. The co-occurrence is then computed using windows of 10 tokens to each side of
the focus word.

Training method. Our embedding vectors are trained by optimizing the SN objective using AdaGrad (Duchi
et al., 2011) with initial learning rate of 0.05 and 100 iterations. The PMI objective derived from (2.5) was
also used. SN has average (weighted) term-wise error of 5%, and PMI has 17%. We observed that SN
vectors typically ﬁt the model better and have better performance, which can be explained by larger errors
in PMI, as implied by Theorem 2.2. So, we only report the results for SN.

For comparison, GloVe and two variants of word2vec (skip-gram and CBOW) vectors are trained. GloVe’s
vectors are trained on the same co-occurrence as SN with the default parameter values.7 word2vec vectors
are trained using a window size of 10, with other parameters set to default values.8

7http://nlp.stanford.edu/projects/glove/
8https://code.google.com/p/word2vec/

12

Figure 2: The linear relationship between the squared norms of our word vectors and the logarithms of
the word frequencies. Each dot in the plot corresponds to a word, where x-axis is the natural logarithm of
the word frequency, and y-axis is the squared norm of the word vector. The Pearson correlation coeﬃcient
between the two is 0.75, indicating a signiﬁcant linear relationship, which strongly supports our mathematical
prediction, that is, equation (2.4) of Theorem 2.2.

5.1 Model veriﬁcation

Experiments were run to test our modeling assumptions. First, we tested two counter-intuitive properties:
the concentration of the partition function Zc for diﬀerent discourse vectors c (see Theorem 2.1), and the
random-like behavior of the matrix of word embeddings in terms of its singular values (see Theorem 4.1).
For comparison we also tested these properties for word2vec and GloVe vectors, though they are trained by
diﬀerent objectives. Finally, we tested the linear relation between the squared norms of our word vectors
and the logarithm of the word frequencies, as implied by Theorem 2.2.

Partition function. Our theory predicts the counter-intuitive concentration of the partition function
Zc = (cid:80)
w(cid:48) exp(c(cid:62)vw(cid:48)) for a random discourse vector c (see Lemma 2.1). This is veriﬁed empirically by
picking a uniformly random direction, of norm (cid:107)c(cid:107) = 4/µw, where µw is the average norm of the word
vectors.9 Figure 1(a) shows the histogram of Zc for 1000 such randomly chosen c’s for our vectors. The
values are concentrated, mostly in the range [0.9, 1.1] times the mean. Concentration is also observed for
other types of vectors, especially for GloVe and CBOW.

Isotropy with respect to singular values. Our theoretical explanation of relations=lines assumes
that the matrix of word vectors behaves like a random matrix with respect to the properties of singular
values. In our embeddings, the quadratic mean of the singular values is 34.3, while the minimum non-zero
singular value of our word vectors is 11. Therefore, the ratio between them is a small constant, consistent
with our model. The ratios for GloVe, CBOW, and skip-gram are 1.4, 10.1, and 3.1, respectively, which are
also small constants.

Squared norms v.s. word frequencies. Figure 2 shows a scatter plot for the squared norms of our
vectors and the logarithms of the word frequencies. A linear relationship is observed (Pearson correlation
0.75), thus supporting Theorem 2.2. The correlation is stronger for high frequency words, possibly because
the corresponding terms have higher weights in the training objective.

9Note that our model uses the inner products between the discourse vectors and word vectors, so it is invariant if the
discourse vectors are scaled by s while the word vectors are scaled by 1/s for any s > 0. Therefore, one needs to choose the
norm of c properly. We assume (cid:107)c(cid:107)µw =
d/κ ≈ 4 for a constant κ = 5 so that it gives a reasonable ﬁt to the predicted
dynamic range of word frequencies according to our theory; see model details in Section 2.

√

13

Relations
semantic
syntactic
total
adjective
noun
verb
total

SN GloVe CBOW skip-gram
0.84
0.61
0.71
0.50
0.69
0.48
0.53

0.73
0.68
0.70
0.58
0.58
0.56
0.57

0.79
0.71
0.74
0.58
0.56
0.64
0.62

0.85
0.65
0.73
0.56
0.70
0.53
0.57

G

M

Table 1: The accuracy on two word analogy task testbeds: G (the GOOGLE testbed); M (the MSR testbed).
Performance is close to the state of the art despite using a generative model with provable properties.

This correlation is much weaker for other types of word embeddings. This is possibly because they have
more free parameters (“knobs to turn”), which imbue the embeddings with other properties. This can also
cause the diﬀerence in the concentration of the partition function for the two methods.

5.2 Performance on analogy tasks

We compare the performance of our word vectors on analogy tasks, speciﬁcally the two testbeds GOOGLE
and MSR (Mikolov et al., 2013a;c). The former contains 7874 semantic questions such as “man:woman::king:??”,
and 10167 syntactic ones such as “run:runs::walk :??.” The latter has 8000 syntactic questions for adjectives,
nouns, and verbs.

To solve these tasks, we use linear algebraic queries.10 That is, ﬁrst normalize the vectors to unit norm

and then solve “a:b::c:??” by

argmin
d

(cid:107)va − vb − vc + vd(cid:107)2
2 .

(5.1)

The algorithm succeeds if the best d happens to be correct.

The performance of diﬀerent methods is presented in Table 1. Our vectors achieve performance compara-
ble to the state of art on semantic analogies (similar accuracy as GloVe, better than word2vec). On syntactic
tasks, they achieve accuracy 0.04 lower than GloVe and skip-gram, while CBOW typically outperforms the
others.11 The reason is probably that our model ignores local word order, whereas the other models capture
it to some extent. For example, a word “she” can aﬀect the context by a lot and determine if the next word
is “thinks” rather than “think ”. Incorporating such linguistic features in the model is left for future work.

5.3 Verifying relations=lines

The theory in Section 4 predicts the existence of a direction for a relation, whereas earlier Levy and Goldberg
(2014a) had questioned if this phenomenon is real. The experiment uses the analogy testbed, where each
relation is tested using 20 or more analogies. For each relation, we take the set of vectors vab = va − vb
where the word pair (a, b) satisﬁes the relation. Then calculate the top singular vectors of the matrix formed
by these vab’s, and compute the cosine similarity (i.e., normalized inner product) of individual vab to the
singular vectors. We observed that most (va − vb)’s are correlated with the ﬁrst singular vector, but have
inner products around 0 with the second singular vector. Over all relations, the average projection on the
ﬁrst singular vector is 0.51 (semantic: 0.58; syntactic: 0.46), and the average on the second singular vector
is 0.035. For example, Table 2 shows the mean similarities and standard deviations on the ﬁrst and second
singular vectors for 4 relations. Similar results are also obtained for word embedings by GloVe and word2vec.
Therefore, the ﬁrst singular vector can be taken as the direction associated with this relation, while the other
components are like random noise, in line with our model.

10One can instead use the 3COSMUL in (Levy and Goldberg, 2014a), which increases the accuracy by about 3%. But it is

not linear while our focus here is the linear algebraic structure.

11It was earlier reported that skip-gram outperforms CBOW (Mikolov et al., 2013a; Pennington et al., 2014). This may be

due to the diﬀerent training data sets and hyperparameters used.

14

relation
1st
2nd
relation
1st
2nd

1
0.65 ± 0.07
0.02 ± 0.28
8
0.56 ± 0.09
0.00 ± 0.22

2
0.61 ± 0.09
0.00 ± 0.23
9
0.53 ± 0.08
0.01 ± 0.26

3
0.52 ± 0.08
0.05 ± 0.30
10
0.37 ± 0.11
0.02 ± 0.20

4
0.54 ± 0.18
0.06 ± 0.27
11
0.72 ± 0.10
0.01 ± 0.24

5
0.60 ± 0.21
0.01 ± 0.24
12
0.37 ± 0.14
0.07 ± 0.26

6
0.35 ± 0.17
0.07 ± 0.24
13
0.40 ± 0.19
0.07 ± 0.23

7
0.42 ± 0.16
0.01 ± 0.25
14
0.43 ± 0.14
0.09 ± 0.23

Table 2: The veriﬁcation of relation directions on 2 semantic and 2 syntactic relations in the GOOGLE
testbed. Relations include cap-com: capital-common-countries; cap-wor: capital-world; adj-adv: gram1-
adjective-to-adverb; opp: gram2-opposite. For each relation, take vab = va − vb for pairs (a, b) in the
relation, and then calculate the top singular vectors of the matrix formed by these vab’s. The row with label
“1st”/“2nd” shows the cosine similarities of individual vab to the 1st/2nd singular vector (the mean and
standard deviation).

w/o RD
RD(k = 20)
RD(k = 30)
RD(k = 40)

SN GloVe CBOW skip-gram
0.71
0.74
0.79
0.76

0.74
0.79
0.82
0.80

0.70
0.75
0.80
0.77

0.73
0.77
0.80
0.80

Table 3: The accuracy of the RD algorithm (i.e., the cheater method) on the GOOGLE testbed. The RD
algorithm is described in the text. For comparison, the row “w/o RD” shows the accuracy of the old method
without using RD.

Cheating solver for analogy testbeds. The above linear structure suggests a better (but cheating) way
to solve the analogy task. This uses the fact that the same semantic relationship (e.g., masculine-feminine,
singular-plural) is tested many times in the testbed. If a relation R is represented by a direction µR then the
cheating algorithm can learn this direction (via rank 1 SVD) after seeing a few examples of the relationship.
Then use the following method of solving “a:b::c:??”: look for a word d such that vc − vd has the largest
projection on µR, the relation direction for (a, b). This can boost success rates by about 10%.

The testbed can try to combat such cheating by giving analogy questions in a random order. But the
cheating algorithm can just cluster the presented analogies to learn which of them are in the same relation.
Thus the ﬁnal algorithm, named analogy solver with relation direction (RD), is: take all vectors va − vb for
all the word pairs (a, b) presented among the analogy questions and do k-means clustering on them; for each
(a, b), estimate the relation direction by taking the ﬁrst singular vector of its cluster, and substitute that
for va − vb in (5.1) when solving the analogy. Table 3 shows the performance on GOOGLE with diﬀerent
values of k; e.g. using our SN vectors and k = 30 leads to 0.79 accuracy. Thus future designers of analogy
testbeds should remember not to test the same relationship too many times! This still leaves other ways to
cheat, such as learning the directions for interesting semantic relations from other collections of analogies.

Non-cheating solver for analogy testbeds. Now we show that even if a relationship is tested only
once in the testbed, there is a way to use the above structure. Given “a:b::c:??,” the solver ﬁrst ﬁnds the
top 300 nearest neighbors of a and those of b, and then ﬁnds among these neighbors the top k pairs (a(cid:48), b(cid:48))
so that the cosine similarities between va(cid:48) − vb(cid:48) and va − vb are largest. Finally, the solver uses these pairs
to estimate the relation direction (via rank 1 SVD), and substitute this (corrected) estimate for va − vb in
(5.1) when solving the analogy. This algorithm is named analogy solver with relation direction by nearest
neighbors (RD-nn). Table 4 shows its performance, which consistently improves over the old method by
about 3%.

15

w/o RD-nn
RD-nn (k = 10)
RD-nn (k = 20)
RD-nn (k = 30)

SN GloVe CBOW skip-gram
0.71
0.71
0.72
0.73

0.74
0.77
0.77
0.78

0.70
0.73
0.74
0.74

0.73
0.74
0.75
0.76

Table 4: The accuracy of the RD-nn algorithm on the GOOGLE testbed. The algorithm is described in the
text. For comparison, the row “w/o RD-nn” shows the accuracy of the old method without using RD-nn.

6 Conclusions

A simple generative model has been introduced to explain the classical PMI based word embedding models,
as well as recent variants involving energy-based models and matrix factorization. The model yields an
optimization objective with essentially “no knobs to turn”, yet the embeddings lead to good performance
on analogy tasks, and ﬁt other predictions of our generative model. A model with fewer knobs to turn
should be seen as a better scientiﬁc explanation (Occam’s razor), and certainly makes the embeddings more
interpretable.

The spatial isotropy of word vectors is both an assumption in our model, and also a new empirical
ﬁnding of our paper. We feel it may help with further development of language models. It is important for
explaining the success of solving analogies via low dimensional vectors (relations=lines). It also implies
that semantic relationships among words manifest themselves as special directions among word embeddings
(Section 4), which lead to a cheater algorithm for solving analogy testbeds.

Our model is tailored to capturing semantic similarity, more akin to a log-linear dynamic topic model.
In particular, local word order is unimportant. Designing similar generative models (with provable and
interpretable properties) with linguistic features is left for future work.

Acknowledgements

We thank the editors of TACL for granting a special relaxation of the page limit for our paper. We thank
Yann LeCun, Christopher D. Manning, and Sham Kakade for helpful discussions at various stages of this
work.

This work was supported in part by NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award,
Simons Collaboration Grant, and ONR-N00014-16-1-2329. Tengyu Ma was supported in addition by Simons
Award in Theoretical Computer Science and IBM PhD Fellowship.

References

Jacob Andreas and Dan Klein. When and why are log-linear models self-normalizing? In Proceedings of the
Annual Meeting of the North American Chapter of the Association for Computational Linguistics, 2014.

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model approach

to PMI-based word embeddings. Technical report, ArXiV, 2015. http://arxiv.org/abs/1502.03520.

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski.

braic structure of word senses, with applications to polysemy.
http://arxiv.org/abs/1502.03520.

Linear alge-
Technical report, ArXiV, 2016.

David Belanger and Sham M. Kakade. A linear dynamical system model for text. In Proceedings of the 32nd

International Conference on Machine Learning, 2015.

Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. Neural

probabilistic language models. In Innovations in Machine Learning. 2006.

16

Fischer Black and Myron Scholes. The pricing of options and corporate liabilities. Journal of Political

David M. Blei. Probabilistic topic models. Communication of the Association for Computing Machinery,

Economy, 1973.

2012.

David M. Blei and John D. Laﬀerty. Dynamic topic models.

In Proceedings of the 23rd International

Conference on Machine Learning, 2006.

Kenneth Ward Church and Patrick Hanks. Word association norms, mutual information, and lexicography.

Computational linguistics, 1990.

Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar. Spectral learning of latent-
variable PCFGs. In Proceedings of the 50th Annual Meeting of the Association for Computational Lin-
guistics: Long Papers-Volume 1, 2012.

Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep neural net-
works with multitask learning. In Proceedings of the 25th International Conference on Machine Learning,
2008a.

Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep neural net-
works with multitask learning. In Proceedings of the 25th International Conference on Machine Learning,
2008b.

Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman.
Indexing by latent semantic analysis. Journal of the American Society for Information Science, 1990.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic

optimization. The Journal of Machine Learning Research, 2011.

John Rupert Firth. A synopsis of linguistic theory. 1957.

Amir Globerson, Gal Chechik, Fernando Pereira, and Naftali Tishby. Euclidean embedding of co-occurrence

data. Journal of Machine Learning Research, 2007.

Tatsunori B. Hashimoto, David Alvarez-Melis, and Tommi S. Jaakkola. Word embeddings as metric recovery

in semantic spaces. Transactions of the Association for Computational Linguistics, 2016.

Thomas Hofmann. Probabilistic latent semantic analysis.

In Proceedings of the Fifteenth Conference on

Uncertainty in Artiﬁcial Intelligence, 1999.

Daniel Hsu, Sham M. Kakade, and Tong Zhang. A spectral algorithm for learning hidden markov models.

Journal of Computer and System Sciences, 2012.

Omer Levy and Yoav Goldberg. Linguistic regularities in sparse and explicit word representations.

In

Proceedings of the Eighteenth Conference on Computational Natural Language Learning, 2014a.

Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in

Neural Information Processing Systems, 2014b.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Compu-
tational Linguistics, 2011.

Yariv Maron, Michael Lamar, and Elie Bienenstock. Sphere embedding: An application to part-of-speech

induction. In Advances in Neural Information Processing Systems, 2010.

17

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey Dean. Eﬃcient estimation of word representations in

vector space. Proceedings of the International Conference on Learning Representations, 2013a.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeﬀ Dean. Distributed representations of
words and phrases and their compositionality. In Advances in Neural Information Processing Systems,
2013b.

Tomas Mikolov, Wen-tau Yih, and Geoﬀrey Zweig. Linguistic regularities in continuous space word rep-
In Proceedings of the Conference of the North American Chapter of the Association for

resentations.
Computational Linguistics: Human Language Technologies, 2013c.

Andriy Mnih and Geoﬀrey Hinton. Three new graphical models for statistical language modelling.

In

Proceedings of the 24th International Conference on Machine Learning, 2007.

Christos H. Papadimitriou, Hisao Tamaki, Prabhakar Raghavan, and Santosh Vempala. Latent semantic
indexing: A probabilistic analysis. In Proceedings of the 7th ACM SIGACT-SIGMOD-SIGART Symposium
on Principles of Database Systems, 1998.

Jeﬀrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word represen-

tation. Proceedings of the Empiricial Methods in Natural Language Processing, 2014.

Douglas L. T. Rohde, Laura M. Gonnerman, and David C. Plaut. An improved model of semantic similarity

based on lexical co-occurence. Communication of the Association for Computing Machinery, 2006.

David E. Rumelhart, Geoﬀrey E. Hinton, and James L. McClelland, editors. Parallel Distributed Processing:

Explorations in the Microstructure of Cognition. 1986.

David E. Rumelhart, Geoﬀrey E. Hinton, and Ronald J. Williams. Learning representations by back-

propagating errors. Cognitive modeling, 1988.

Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. Journal

of Artiﬁcial Intelligence Research, 2010.

18

A Proofs of Theorem 1

In this section we prove Theorem 2.2 and Lemma 2.1 (restated below).

Theorem 2.2. Suppose the word vectors satisfy equation (2.2), and window size q = 2. Then,

for (cid:15) = O((cid:15)z) + (cid:101)O(1/d) + O((cid:15)2). Jointly these imply:

log p(w, w(cid:48)) =

− 2 log Z ± (cid:15),

(cid:107)vw + vw(cid:48)(cid:107)2
2
2d
(cid:107)vw(cid:107)2
2
2d

log p(w) =

− log Z ± (cid:15).

PMI (w, w(cid:48)) =

(cid:104)vw, vw(cid:48)(cid:105)
d

± O((cid:15)).

Lemma 2.1. If the word vectors satisfy the bayesian prior v = s · ˆv, where ˆv is from the spherical Gaussian
distribution, and s is a scalar random variable, then with high probability the entire ensemble of word vectors
satisﬁes that

for (cid:15)z = (cid:101)O(1/

n), and δ = exp(−Ω(log2 n)).

√

Pr
c∼C

[(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z] ≥ 1 − δ,

We ﬁrst prove Theorem 2.2 using Lemma 2.1, and Lemma 2.1 will be proved in Section A.1. Please see
Section 2 of the main paper for the intuition of the proof and a cleaner sketch without too many technicalities.

Proof of Theorem 2.2. Let c be the hidden discourse that determines the probability of word w, and c(cid:48) be
the next one that determines w(cid:48). We use p(c(cid:48)|c) to denote the Markov kernel (transition matrix) of the
Markov chain. Let C be the stationary distribution of discourse vector c, and D be the joint distribution of
(c, c(cid:48)). We marginalize over the contexts c, c(cid:48) and then use the independence of w, w(cid:48) conditioned on c, c(cid:48),

(cid:20) exp((cid:104)vw, c(cid:105))
Zc
We ﬁrst get rid of the partition function Zc using Lemma 2.1. As sketched in the main paper, essentially
we will replace Zc by Z in equation (A.5), though a very careful control of the approximation error is
required. Formally, Let F1 be the event that c satisﬁes

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

p(w, w(cid:48)) = E

(A.5)

(c,c(cid:48))∼D

(cid:21)

(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z .

(A.6)

Similarly, let F2 be the even that c(cid:48) satisﬁes (1 − (cid:15)z)Z ≤ Zc(cid:48) ≤ (1 + (cid:15)z)Z, and let F = F1 ∩ F2, and F be its
negation. Moreover, let 1F be the indicator function for the event F. Therefore by Lemma 2.1 and union
bound, we have E[1F ] = Pr[F] ≥ 1 − exp(−Ω(log2 n)).

We ﬁrst decompose the integral (A.5) into the two parts according to whether event F happens,

We bound the ﬁrst quantity on the right hand side using (2.2) and the deﬁnition of F.

p(w, w(cid:48)) = E

(c,c(cid:48))∼D

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

+ E

(c,c(cid:48))∼D

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:20)

(cid:20)

1
ZcZc(cid:48)
1
ZcZc(cid:48)

(cid:21)

(cid:21)

(cid:20)

E
(c,c(cid:48))∼D

1
ZcZc(cid:48)
≤ (1 + (cid:15)z)2 1
Z 2

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:21)

E
(c,c(cid:48))∼D

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ]

19

(A.1)

(A.2)

(A.3)

(A.4)

(A.7)

(A.8)

For the second quantity of the right hand side of (A.7), we have by Cauchy-Schwartz,

1
ZcZc(cid:48)
(cid:20) 1
Z 2
c

(cid:18)

(cid:20)

E
(c,c(cid:48))∼D
(cid:18)

≤

≤

(cid:18)

E
(c,c(cid:48))∼D
(cid:20) 1
Z 2
c

E
c

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:21)(cid:19)2

exp((cid:104)vw, c(cid:105))21F

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

[1F ]

(cid:21)(cid:19) (cid:18)

(cid:21)(cid:19) (cid:18)

E
(c,c(cid:48))∼D
(cid:20) 1
Z 2
c(cid:48)

E
c(cid:48)

(cid:20)(cid:90)

1
Z 2
c(cid:48)

c,c(cid:48)

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))21F

(cid:21)(cid:19)

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))2 E
c|c(cid:48)

[1F ]

(cid:21)(cid:19)

.

(A.9)

Using the fact that Zc ≥ 1, then we have that

(cid:20) 1
Z 2
c

E
c

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

(cid:21)
[1F ]

(cid:20)
exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

(cid:21)
[1F ]

≤ E
c

We can split that expectation as
(cid:20)
exp((cid:104)vw, c(cid:105))21(cid:104)vw,c(cid:105)>0 E
c(cid:48)|c

E
c

The second term of (A.10) is upper bounded by

(cid:21)
[1F ]

(cid:20)
exp((cid:104)vw, c(cid:105))21(cid:104)vw,c(cid:105)<0 E
c(cid:48)|c

(cid:21)
[1F ]

.

+ E
c

(A.10)

We proceed to the ﬁrst term of (A.10) and observe the following property of it:

(cid:20)
exp((cid:104)vw, c(cid:105))21(cid:104)vw,c(cid:105)>0 E
c(cid:48)|c

E
c

(cid:21)

(cid:20)

[1F ]

≤ E
c

exp((cid:104)αvw, c(cid:105))21(cid:104)vw,c(cid:105)>0 E
c(cid:48)|c

[1F ]

≤ E
c

(cid:21)

(cid:20)
exp((cid:104)αvw, c(cid:105))2 E
c(cid:48)|c

(cid:21)

[1F ]

where α > 1. Therefore, it’s suﬃcient to bound

[1F ] ≤ exp(−Ω(log2 n))

E
c,c(cid:48)

(cid:20)

E
c

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

[1F ]

(cid:21)

when (cid:107)vw(cid:107) = Ω(

d).

√

Let’s denote by z the random variable 2 (cid:104)vw, c(cid:105).
Let’s denote r(z) = Ec(cid:48)|z[1F ] which is a function of z between [0, 1]. We wish to upper bound Ec [exp(z)r(z)].

The worst-case r(z) can be quantiﬁed using a continuous version of Abel’s inequality as proven in Lemma A.4,
which gives

[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3)
E
c

(A.11)

where t satisﬁes that Ec[1[t,+∞]] = Pr[z ≥ t] = Ec[r(z)] ≤ exp(−Ω(log2 n)). Then, we claim Pr[z ≥ t] ≤
exp(−Ω(log2 n)) implies that t ≥ Ω(log.9 n).

If c were distributed as N (0, 1

d I), this would be a simple tail bound. However, as c is distributed uniformly
on the sphere, this requires special care, and the claim follows by applying Lemma A.1 instead. Finally,
applying Corollary A.3, we have:

E[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(log1.8 n))
We have the same bound for c(cid:48) as well. Hence, for the second quantity of the right hand side of (A.7),

(A.12)

we have

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:21)

(cid:20)

1
ZcZc(cid:48)

E
(c,c(cid:48))∼D
(cid:18)

≤

E
c

(cid:20) 1
Z 2
c
≤ exp(−Ω(log1.8 n))

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

(cid:21)(cid:19)1/2 (cid:18)

[1F ]

(cid:20) 1
Z 2
c(cid:48)

E
c(cid:48)

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))2 E
c|c(cid:48)

[1F ]

(cid:21)(cid:19)1/2

(A.13)

20

where the ﬁrst inequality follows from Cauchy-Schwartz, and the second from the calculation above. Com-
bining (A.7), (A.8) and (A.13), we obtain

p(w, w(cid:48)) ≤ (1 + (cid:15)z)2 1
Z 2
≤ (1 + (cid:15)z)2 1
Z 2

E
(c,c(cid:48))∼D

E
(c,c(cid:48))∼D

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ] +

1
n2 exp(−Ω(log1.8 n))

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] + δ0

where δ0 = exp(−Ω(log1.8 n))Z 2 ≤ exp(−Ω(log1.8 n)) by the fact that Z ≤ exp(2κ)n = O(n). Note that
κ is treated as an absolute constant throughout the paper. On the other hand, we can lowerbound similarly

p(w, w(cid:48)) ≥ (1 − (cid:15)z)2 1
Z 2
≥ (1 − (cid:15)z)2 1
Z 2
≥ (1 − (cid:15)z)2 1
Z 2

E
(c,c(cid:48))∼D

E
(c,c(cid:48))∼D

E
(c,c(cid:48))∼D

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ]

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] −

1
n2 exp(−Ω(log1.8 n))

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] − δ0

Taking logarithm, the multiplicative error translates to a additive error

log p(w, w(cid:48)) = log

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

(cid:18)

E
(c,c(cid:48))∼D

For the purpose of exploiting the fact that c, c(cid:48) should be close to each other, we further rewrite log p(w, w(cid:48))
by re-organizing the expectations above,

log p(w, w(cid:48)) = log

± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

= log

[exp((cid:104)vw, c(cid:105))A(c)] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

(A.14)

(cid:20)
exp((cid:104)vw, c(cid:105)) E
c(cid:48)|c

(cid:21)
[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

(cid:19)

(cid:18)

(cid:18)

E
c

E
c

A(c) := E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

where the inner integral which is denoted by A(c),

Since (cid:107)vw(cid:107) ≤ κ

d. Therefore we have that (cid:104)vw, c − c(cid:48)(cid:105) ≤ (cid:107)vw(cid:107)(cid:107)c − c(cid:48)(cid:107) ≤ κ

d(cid:107)c − c(cid:48)(cid:107).

√

√

Then we can bound A(c) by

(cid:19)

(cid:19)

A(c) = E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

= exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

≤ exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48) − c(cid:105))]

√

[exp(κ

d(cid:107)c − c(cid:48)(cid:107))]

≤ (1 + (cid:15)2) exp((cid:104)vw(cid:48), c(cid:105))

21

where the last inequality follows from our model assumptions. To derive a lower bound of A(c), observe that

√

[exp(κ

E
c(cid:48)|c

d(cid:107)c − c(cid:48)(cid:107))] + E
c(cid:48)|c

√

[exp(−κ

d(cid:107)c − c(cid:48)(cid:107))] ≥ 2

Therefore, our model assumptions imply that

Hence,

√

[exp(−κ

d(cid:107)c − c(cid:48)(cid:107))] ≥ 1 − (cid:15)2

E
c(cid:48)|c

A(c) = exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

exp((cid:104)vw(cid:48), c(cid:48) − c(cid:105))
√

exp(−κ

d(cid:107)c − c(cid:48)(cid:107))

≥ exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

≥ (1 − (cid:15)2) exp((cid:104)vw(cid:48), c(cid:105))

Therefore, we obtain that A(c) = (1 ± (cid:15)2) exp((cid:104)vw(cid:48), c(cid:105)). Plugging the just obtained estimate of A(c) into

the equation (A.14), we get that

(cid:18)

(cid:18)

(cid:18)

E
c

E
c

E
c

E
c

log p(w, w(cid:48)) = log

[exp((cid:104)vw, c(cid:105))A(c)] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

= log

[(1 ± (cid:15)2) exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:105))] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

(cid:19)

= log

[exp((cid:104)vw + vw(cid:48), c(cid:105))] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z) + log(1 ± (cid:15)2)

(A.15)

Now it suﬃces to compute Ec[exp((cid:104)vw + vw(cid:48), c(cid:105))]. Note that if c had the distribution N (0, 1

d I), which
is very similar to uniform distribution over the sphere, then we could get straightforwardly Ec[exp((cid:104)vw +
vw(cid:48), c(cid:105))] = exp((cid:107)vw + vw(cid:48)(cid:107)2/(2d)). For c having a uniform distribution over the sphere, by Lemma A.5, the
same equality holds approximately,

[exp((cid:104)vw + vw(cid:48), c(cid:105))] = (1 ± (cid:15)3) exp((cid:107)vw + vw(cid:48)(cid:107)2/(2d))

(A.16)

where (cid:15)3 = (cid:101)O(1/d).

Plugging in equation (A.16) into equation (A.15), we have that
log p(w, w(cid:48)) = log (cid:0)(1 ± (cid:15)3) exp((cid:107)vw + vw(cid:48)(cid:107)2/(2d)) ± δ0

(cid:1) − 2 log Z + 2 log(1 ± (cid:15)z) + log(1 ± (cid:15)2)

= (cid:107)vw + vw(cid:48)(cid:107)2/(2d) + O((cid:15)3) + O(δ(cid:48)

0) − 2 log Z ± 2(cid:15)z ± (cid:15)2

where δ(cid:48)

0 = δ0 · (Ec∼C[exp((cid:104)vw + vw(cid:48), c(cid:105))])−1 = exp(−Ω(log1.8 n)). Note that (cid:15)3 = (cid:101)O(1/d), (cid:15)z = (cid:101)O(1/

√

n),

and (cid:15)2 by assumption, therefore we obtain that

log p(w, w(cid:48)) =

(cid:107)vw + vw(cid:48)(cid:107)2 − 2 log Z ± O((cid:15)z) + O((cid:15)2) + (cid:101)O(1/d).

1
2d

The following lemmas are helper lemmas that were used in the proof above. We use Cd to denote the

uniform distribution over the unit sphere in Rd.

Lemma A.1 (Tail bound for spherical distribution). If c ∼ Cd, v ∈ Rd is a vector with (cid:107)v(cid:107) = Ω(
t = ω(1), the random variable z = (cid:104)v, c(cid:105) satisﬁes Pr[z ≥ t] = e−O(t2).

√

d) and

(cid:19)

(cid:19)

22

Proof. If c = (c1, c2, . . . , cd) ∼ Cd, c is in distribution equal to
samples from a univariate Gaussian with mean 0 and variance 1
that v = ((cid:107)v(cid:107), 0, . . . , 0). Let’s introduce the random variable r = (cid:80)d

(cid:16) ˜c1
(cid:107)˜c(cid:107) , ˜c2

(cid:107)˜c(cid:107) , . . . , ˜cd

where the ˜ci are i.i.d.
d . By spherical symmetry, we may assume

(cid:107)˜c(cid:107)

(cid:17)

Pr [(cid:104)v, c(cid:105) ≥ t] = Pr

(cid:107)v|

≥ t

≤ Pr

≥ t | r ≥

Pr

r ≥

+ Pr

≥ t | r ≥

Pr

r ≥

(cid:20)

(cid:21)

˜c1
(cid:107)˜c(cid:107)

(cid:20) (cid:107)v(cid:107)˜c1
(cid:107)˜c(cid:107)

(cid:20)

(cid:21)

1
2

(cid:20)

(cid:21)

1
2

(cid:21)

1
2

it’s suﬃcient to lower bound Pr [r ≤ 100] and Pr

. The former probability is easily

seen to be lower bounded by a constant by a Chernoﬀ bound. Consider the latter one next. It holds that

(cid:104)

(cid:107)v(cid:107) ˜c1

(cid:107)c(cid:107) ≥ t | r ≤ 100

(cid:105)

i=2 ˜c2
(cid:21)

1
2

i . Since

(cid:20) (cid:107)v(cid:107)˜c1
(cid:107)˜c(cid:107)

Pr

(cid:107)v(cid:107)

≥ t | r ≤ 100

= Pr

˜c1 ≥

(cid:21)

(cid:34)

(cid:115)

t2 · r

(cid:107)v(cid:107)2 − t2 | r ≤ 100

(cid:35)

(cid:34)

(cid:115)

≥ Pr

˜c1 ≥

(cid:35)

100t2
(cid:107)v(cid:107)2 − t2

(cid:20)

˜c1
(cid:107)˜c(cid:107)

Denoting ˜t =

(cid:113) 100t2

(cid:107)v(cid:107)2−t2 , by a well-known Gaussian tail bound it follows that

Pr (cid:2)˜c1 ≥ ˜t(cid:3) = e−O(d˜t2)

(cid:32)

1
√
d˜t

(cid:18) 1
√

−

d˜t

(cid:19)3(cid:33)

= e−O(t2)

where the last equality holds since (cid:107)v(cid:107) = Ω(

d) and t = ω(1).

√

√

Lemma A.2. If c ∼ Cd, v ∈ Rd is a vector with (cid:107)v(cid:107) = Θ(
satisﬁes

d) and t = ω(1), the random variable z = (cid:104)v, c(cid:105)

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(t2)) + exp(−Ω(d))

Proof. Similarly as in Lemma A.1, if c = (c1, c2, . . . , cd) ∼ Cd, c is in distribution equal to
where the ˜ci are i.i.d. samples from a univariate Gaussian with mean 0 and variance 1
symmetry, we may assume v = ((cid:107)v(cid:107), 0, . . . , 0). Let’s introduce the random variable r = (cid:80)d
an arbitrary u > 1, some algebraic manipulation shows

(cid:16) ˜c1
(cid:107)˜c(cid:107) , ˜c2

(cid:107)˜c(cid:107) , . . . , ˜cd

(cid:107)˜c(cid:107)

(cid:17)

d . Again, by spherical
i . Then, for

i=2 ˜c2

Pr (cid:2)exp ((cid:104)v, c(cid:105)) 1[t,+∞]((cid:104)v, c(cid:105)) ≥ u(cid:3) = Pr [exp ((cid:104)v, c(cid:105)) ≥ u ∧ (cid:104)v, c(cid:105) ≥ t] =

(cid:20)

(cid:18)

Pr

exp

(cid:107)v(cid:107)

(cid:19)

˜c1
(cid:107)˜c(cid:107)

˜c1
(cid:107)˜c(cid:107)

≥ u ∧ (cid:107)v(cid:107)

≥ u

= Pr

˜c1 = max

(cid:21)

(cid:34)

(cid:32)(cid:115)

(cid:115)

(cid:33)(cid:35)

˜u2r
(cid:107)v(cid:107)2 − ˜u2 ,

t2r
(cid:107)v(cid:107)2 − t2

(A.17)

√

where we denote ˜u = log u. Since ˜c1 is a mean 0 univariate Gaussian with variance 1
have ∀x ∈ R

d , and (cid:107)v(cid:107) = Ω(

d) we

(cid:34)

(cid:115)

Pr

˜c1 ≥

(cid:35)

x2r
(cid:107)v(cid:107)2 − u2

(cid:16)

e−Ω(x2r)(cid:17)

= O

Next, we show that r is lower bounded by a constant with probability 1 − exp(−Ω(d)).
distribution equal to 1

Indeed, r is in
k is a Chi-squared distribution with k degrees of freedom. Standard
d ] ≤ exp(−ξ). Taking ξ = αd for α a constant

concentration bounds (?) imply that ∀ξ ≥ 0, Pr[r − 1 ≤ −2
implies that with probability 1 − exp(−Ω(d)), r ≥ M for some constant M . We can now rewrite

d−1, where χ2

d χ2

(cid:113) ξ

(cid:34)

(cid:115)

Pr

˜c1 ≥

x2r
(cid:107)v(cid:107)2 − x2

(cid:35)

=

23

(cid:34)

(cid:115)

Pr

˜c1 ≥

x2r

(cid:107)v(cid:107)2 − x2 | r ≥ M

(cid:35)

Pr[r ≥ M ] + Pr

˜c1 ≥

(cid:34)

(cid:115)

x2r

(cid:107)v(cid:107)2 − x2 | r ≤ M

(cid:35)

Pr[r ≤ M ]

The ﬁrst term is clearly bounded by e−Ω(x2) and the second by exp(−Ω(d)). Therefore,

(cid:34)

(cid:115)

Pr

˜c1 ≥

(cid:35)

x2r
(cid:107)v(cid:107)2 − x2

= O (cid:0)max (cid:0)exp (cid:0)−Ω (cid:0)x2(cid:1)(cid:1) , exp (−Ω (d))(cid:1)(cid:1)

(A.18)

Putting A.17 and A.18 together, we get that

Pr (cid:2)exp ((cid:104)v, c(cid:105)) 1[t,+∞]((cid:104)v, c(cid:105)) ≥ u(cid:3) = O

(cid:16)

(cid:16)

(cid:16)

(cid:16)

max

exp

−Ω

min

(cid:16)

d, (max (˜u, t))2(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)

(A.19)

(where again, we denote ˜u = log u)

For any random variable X which has non-negative support, it’s easy to check that

E[X] =

Pr[X ≥ x]dx

(cid:90) ∞

0

Hence,

E (cid:2)exp(z)1[t,+∞](z)(cid:3) =

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du =

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du

(cid:90) ∞

0

(cid:90) exp((cid:107)v(cid:107))

0

To bound this integral, we split into the following two cases:

• Case t2 ≥ d: max (˜u, t) ≥ t, so min

(cid:16)

d, (max (˜u, t))2(cid:17)

= d. Hence, A.19 implies

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp((cid:107)v(cid:107)) exp(−Ω(d)) = exp(−Ω(d))

where the last inequality follows since (cid:107)v(cid:107) = O(

d).

√

• Case t2 < d: In the second case, we will split the integral into two portions: u ∈ [0, exp(t)] and

u ∈ [exp(t), exp((cid:107)v(cid:107))].
When u ∈ [0, exp(t)], max (˜u, t) = t, so min(d, (max (˜u, t))2) = t2. Hence,

(cid:90) exp(t)

0

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du ≤ exp(t) exp(−Ω(t2)) = − exp(Ω(t2))

When u ∈ [exp(t), exp((cid:107)v(cid:107))], max (˜u, t) = ˜u. But ˜u ≤ log(exp((cid:107)v(cid:107))) = O(
˜u. Hence,

(cid:90) exp((cid:107)v(cid:107))

exp(t)

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du ≤

exp(−(log(u))2)du

(cid:90) exp((cid:107)v(cid:107))

exp(t)

Making the change of variable ˜u = log(u), the we can rewrite the last integral as

√

d), so min(d, (max (˜u, t))2) =

(cid:90) (cid:107)v(cid:107)

t

exp(−˜u2) exp(˜u)d˜u = O(exp(−t2))

where the last inequality is the usual Gaussian tail bound.

In either case, we get that

(cid:90) exp((cid:107)v(cid:107))

0

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du = exp(−Ω(t2)) + exp(−Ω(d)))

which is what we want.

24

As a corollary to the above lemma, we get the following:

Corollary A.3. If c ∼ Cd, v ∈ Rd is a vector with (cid:107)v(cid:107) = Θ(

d) and t = Ω(log.9 n) then

√

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(log1.8 n))

Proof. We claim the proof is trivial if d = o(log4 n).
exp(O(

d)). Hence,

√

Indeed,

in this case, exp((cid:104)v, c(cid:105)) ≤ exp((cid:107)v(cid:107)) =

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(O(

d)) E[1[t,+∞](z)] = exp(O(

d)) Pr[z ≥ t]

√

Since by Lemma A.1, Pr[z ≥ t] ≤ exp(−Ω(log2 n), we get

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(O(

d) − Ω(log2 n)) = exp(−Ω(log1.8 n))

as we wanted.

So, we may, without loss of generality assume that d = Ω(log4 n). In this case, Lemma A.2 implies

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(− log1.8 n) + exp(−Ω(d))) = exp(− log1.8 n)

where the last inequality holds because d = Ω(log4 n) and t2 = Ω(log.9 n), so we get the claim we wanted.

√

√

Lemma A.4 (Continuous Abel’s Inequality). Let 0 ≤ r(x) ≤ 1 be a function such that such that E[r(x)] = ρ.
Moreover, suppose increasing function u(x) satisﬁes that E[|u(x)|] < ∞. Let t be the real number such that
E[1[t,+∞]] = ρ. Then we have

E[u(x)r(x)] ≤ E[u(x)1[t,+∞]]

(A.20)

z f (x)r(x)dx, and H(z) = (cid:82) ∞

Proof. Let G(z) = (cid:82) ∞
z f (x)1[t,+∞](x)dx. Then we have that G(z) ≤ H(z) for
all z. Indeed, for z ≥ t, this is trivial since r(z) ≤ 1. For z ≤ t, we have H(z) = E[1[t,+∞]] = ρ = E[r(x)] ≥
(cid:82) ∞
z f (x)r(x)dx. Then by integration by parts we have,
(cid:90) ∞

(cid:90) ∞

u(x)f (x)r(x)dx = −

u(x)dG

−∞

−∞

= −u(x)G(x) |∞

−∞ +

G(x)u(cid:48)(x)dx

(cid:90) +∞

−∞

≤

=

(cid:90) +∞

−∞
(cid:90) ∞

−∞

H(x)u(cid:48)(x)dx

u(x)f (x)1[t,+∞](x)dx,

where at the third line we use the fact that u(x)G(x) → 0 as x → ∞ and that u(cid:48)(x) ≥ 0, and at the last line
we integrate by parts again.

Lemma A.5. Let v ∈ Rd be a ﬁxed vector with norm (cid:107)v(cid:107) ≤ κ
variable c with uniform distribution over the sphere, we have that

√

d for absolute constant κ. Then for random

log E[exp((cid:104)v, d(cid:105))] = (cid:107)v(cid:107)2/2d ± (cid:15)

(A.21)

where (cid:15) = (cid:101)O( 1

d ).

25

Proof. Let g ∈ N (0, I), then g/(cid:107)g(cid:107) has the same distribution as c. Let r = (cid:107)v(cid:107). Since c is spherically
symmetric, we could, we can assume without loss of generality that v = (r, 0, . . . , 0). Let x = g1 and
y = (cid:112)g2
d. Therefore x ∈ N (0, 1) and y2 has χ2 distribution with mean d − 1 and variance O(d).
d. Note that the Pr[F] ≥ 1−exp(−Ω(log1.8(d)).

Let F be the event that x ≤ 20 log d and 1.5

2 + · · · + g2

d ≥ y ≥ 0.5

√

√

By Proposition 2, we have that E[exp((cid:104)v, c(cid:105))] = E[exp((cid:104)v, c(cid:105)) | F] · (1 ± Ω(− log1.8 d)).

Conditioned on event F, we have

E[exp((cid:104)v, c(cid:105)) | F] = E

exp(

(cid:35)

rx
(cid:112)x2 + y2

) | F

(cid:34)

(cid:34)

(cid:34)

(cid:20)

= E

exp(

−

rx3
y(cid:112)x2 + y2(y + (cid:112)x2 + y2)

) | F

(cid:35)

= E

exp(

) · exp(

rx3
y(cid:112)x2 + y2(y + (cid:112)x2 + y2)

) | F

(cid:35)

= E

exp(

) | F

· (1 ± O(

(cid:21)

))

log3 d
d
√

√

rx
y

rx
y

rx
y

where we used the fact that r ≤ κ
we have that

√

d. Let E be the event that 1.5

d ≥ y ≥ 0.5

d. By using Proposition 1,

E[exp(rx/y) | F] = E[exp(rx/y) | E] ± exp(−Ω(log2(d))

(A.23)

Then let z = y2/(d − 1) and w = z − 1. Therefore z has χ2 distribution with mean 1 and variance 1/(d − 1),
and w has mean 0 and variance 1/(d − 1).

(A.22)

E

exp(

) | E

= E[E[exp(rx/y) | y] | E] = E[exp(r2/y2) | E]

(cid:20)

(cid:21)

rx
y

= E[exp(r2/(d − 1) · 1/z2) | E]

= E[exp(r2/(d − 1) · (1 +

= exp(r2/(d − 1)) E[exp(1 +

2w + w2
(1 + w)2 )) | E]
2w + w2
(1 + w)2 )) | E]

= exp(r2/(d − 1)) E[1 + 2w ± O(w2) | E]
= exp(r2/(d − 1)2)(1 ± 1/d)

where the second-to-last line uses the fact that conditioned on 1/2 ≥ E, w ≥ −1/2 and therefore the Taylor
expansion approximates the exponential accurately, and the last line uses the fact that | E[w | E]| = O(1/d)
and E[w2 | E] ≤ O(1/d). Combining the series of approximations above completes the proof.

We ﬁnally provide the proofs for a few helper propositions on conditional probabilities for high probability

events used in the lemma above.

Proposition 1. Suppose x ∼ N (0, σ2) with σ = O(1). Then for any event E with Pr[E] = 1−O(−Ω(log2 d)),
we have that E[exp(x)] = E[exp(x) | E] ± exp(−Ω(log2(d)).

Proof. Let’s denote by ¯E the complement of the event E. We will consider the upper and lower bound
separately. Since

E[exp(x)] = E[exp(x) | E] Pr[E] + E[exp(x) | ¯E] Pr[ ¯E]

we have that

E[exp(x)] ≤ E[exp(x) | E] + E[exp(x) | ¯E] Pr[ ¯E]

(A.24)

26

and

E[exp(x)] ≥ E[exp(x) | E](1 − exp(−Ω(log2 d))) ≥ E[exp(x) | E] − E[exp(x) | E] exp(−Ω(log2 d)))

(A.25)

Consider the upper bound (A.24) ﬁrst. To show the statement of the lemma, it suﬃces to bound

E[exp(x) | ¯E] Pr[ ¯E].

Working towards that, notice that

E[exp(x) | ¯E] Pr[ ¯E] = E[exp(x)1 ¯E ] = E[exp(x) E[1 ¯E |x]] = E[exp(x)r(x)]

if we denote r(x) = E[1 ¯E |x]. We wish to upper bound E[exp(x)r(x)]. By Lemma A.4, we have

E[exp(x)r(x)] ≤ E[exp(x)1[t,∞]]

where t is such that E[1[t,∞]] = E[r(x)]. However, since E[r(x)] = Pr[ ¯E] = exp(−Ω(log2 d)), it must be the
case that t = Ω(log d) by the standard Gaussian tail bound, and the assumption that σ = O(1). In turn,
this means

E[exp(x)1[t,∞]] ≤

(cid:90) ∞

1
√
2π

σ

t

exe− x2

σ2 dx =

(cid:90) ∞

1
√
2π

σ

t

e−( x

σ − σ

2 )2+ σ2

4 dx = e

σ2
4

e−(x(cid:48)− σ

2 )2

dx(cid:48)

1
√
2π

(cid:90) +∞

t/σ

where the last equality follows from the change of variables x = σx(cid:48). However,

1
√
2π

(cid:90) +∞

t/σ

e−(x(cid:48)− σ

2 )2

dx(cid:48)

is nothing more than Pr[x(cid:48) > t
1. Bearing in mind that σ = O(1)

σ ], where x(cid:48) is distributed like a univariate gaussian with mean σ

2 and variance

σ2
4

e

1
√
2π

(cid:90) +∞

t/σ

e−(x(cid:48)− σ

2 )2

dx(cid:48) = exp(−Ω(t2)) = exp(−Ω(log2 d))

by the usual Gaussian tail bounds, which proves the lower bound we need.

We proceed to consider the lower bound A.25. To show the statement of the lemma, we will bound

E[exp(x) | E]. Notice trivially that since exp(x) ≥ 0,

E[exp(x) | E] ≤

E[exp(x)]
Pr[E]

Since Pr[E] ≥ 1 − exp(Ω(log2 d)),

1

Pr[E] ≤ 1 + exp(O(log2)). So, it suﬃces to bound E[exp(x)]. However,

E[exp(x)] =

1
√
2π

σ

(cid:90) +∞

t=−∞

exe− x2

σ2 dx =

1
√
2π

σ

(cid:90) +∞

t=−∞

e−( x

σ − σ

2 )2+ σ2

4 dx =

e−(x(cid:48)− σ

2 )2+ σ2

4 dx(cid:48)

1
√
2π

(cid:90) +∞

t=−∞

where the last equality follows from the same change of variables x = σx(cid:48) as before. Since (cid:82) +∞
√

t=−∞ e−(x(cid:48)− σ

2 )2

dx(cid:48) =

2π, we get

(cid:90) +∞

1
√
2π

e−(x(cid:48)− σ

2 )2+ σ2

4 dx(cid:48) = e

σ2
4 = O(1)

t=−∞
Pr[E] , we get that E[exp(x) | E] = O(1). Plugging this back in A.25,

1

Putting together with the estimate of
we get the desired upper bound.

Proposition 2. Suppose c ∼ C and v is an arbitrary vector with (cid:107)v(cid:107) = O(
Pr[E] ≥ 1 − exp(−Ω(log2 d)), we have that E[exp((cid:104)v, c(cid:105))] = E[exp((cid:104)v, c(cid:105)) | E] ± exp(− log1.8 d).

d). Then for any event E with

√

27

Proof of Proposition 2. Let z = (cid:104)v, c(cid:105). We proceed similarly as in the proof of Proposition 1. We have

and

and

E[exp(z)] = E[exp(z) | E] Pr[E] + E[exp(z) | ¯E] Pr[ ¯E]

E[exp(z)] ≤ E[exp(z) | E] + E[exp(z) | ¯E] Pr[ ¯E]

E[exp(z)] ≥ E[exp(z) | E] Pr[E] = E[exp(z) | E] − E[exp(z) | E] exp(−Ω(log2 d))

We again proceed by separating the upper and lower bound.

We ﬁrst consider the upper bound A.26.
Notice that that

E[exp(z) | ¯E] Pr[ ¯E] = E[exp(z)1 ¯E ]

We can split the last expression as

E (cid:2)exp((cid:104)vw, c(cid:105))1(cid:104)vw,c(cid:105)>01E

(cid:3) + E (cid:2)exp((cid:104)vw, c(cid:105))1(cid:104)vw,c(cid:105)<01E

(cid:3) .

The second term is upper bounded by

E[1E ] ≤ exp(−Ω(log2 n))

We proceed to the ﬁrst term of (A.10) and observe the following property of it:

E (cid:2)exp((cid:104)vw, c(cid:105))1(cid:104)vw,c(cid:105)>01E

(cid:3) ≤ E (cid:2)exp((cid:104)αvw, c(cid:105))1(cid:104)vw,c(cid:105)>0 1E

(cid:3) ≤ E [exp((cid:104)αvw, c(cid:105))1E ]

(A.26)

(A.27)

where α > 1. Therefore, it’s suﬃcient to bound

√

when (cid:107)vw(cid:107) = Θ(

d). Let’s denote r(z) = E[1 ¯E |z].

Using Lemma A.4, we have that

E [exp(z)1E ]

[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3)
E
c

(A.28)

where t satisﬁes that Ec[1[t,+∞]] = Pr[z ≥ t] = Ec[r(z)] ≤ exp(−Ω(log2 d)). Then, we claim Pr[z ≥ t] ≤
exp(−Ω(log2 d)) implies that t ≥ Ω(log.9 d).

Indeed, this follows by directly applying Lemma A.1. Afterward, applying Lemma A.2, we have:

E[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(log1.8 d))

(A.29)

which proves the upper bound we want.

We now proceed to the lower bound A.27, which is again similar to the lower bound in the proof of

Proposition 1: we just need to bound E[exp(z) | E]. Same as in Proposition 1, since exp(x) ≥ 0,

E[exp(z) | E] ≤

E[exp(z)]
Pr[E]

Consider the event E (cid:48) : z ≤ t, for t = Θ(log.9 d), which by Lemma A.1 satisﬁes Pr[E (cid:48)] ≥ 1 − exp(−Ω(log2 d)).
By the upper bound we just showed,

E[exp(z)] ≤ E[exp(z) | E (cid:48)] + exp(−Ω(log2 n)) = O(exp(log.9 d))

where the last equality follows since conditioned on E (cid:48), z = O(log.9 d). Finally, this implies
1
Pr[E]

O(exp(log.9 d)) = O(exp(log.9 d))

E[exp(z) | E] ≤

where the last equality follows since Pr[E] ≥ 1 − exp(−Ω(log2 n)). Putting this together with A.27, we get
E[exp(z)] ≥ E[exp(z) | E] Pr[E] = E[exp(z) | E] − E[exp(z) | E] exp(−Ω(log2 d)) ≥
E[exp(z) | E] − O(exp(log.9 d)) exp(−Ω(log2 d)) ≥ E[exp(z) | E] − exp(−Ω(log2 d))

which is what we needed.

28

A.1 Analyzing partition function Zc

In this section, we prove Lemma 2.1. We basically ﬁrst prove that for the means of Zc are all (1 + o(1))-close
to each other, and then prove that Zc is concentrated around its mean. It turns out the concentration part
is non trivial because the random variable of concern, exp((cid:104)vw, c(cid:105)) is not well-behaved in terms of the tail.
Note that exp((cid:104)vw, c(cid:105)) is NOT sub-gaussian for any variance proxy. This essentially disallows us to use an
existing concentration inequality directly. We get around this issue by considering the truncated version of
exp((cid:104)vw, c(cid:105)), which is bounded, and have similar tail properties as the original one, in the regime that we
are concerning.

We bound the mean and variance of Zc ﬁrst in the Lemma below.

Lemma A.6. For any ﬁxed unit vector c ∈ Rd, we have that E[Zc] ≥ n and V[Zc] ≤ O(n).

Proof of Lemma A.6. Recall that by deﬁnition

Zc =

exp((cid:104)vw, c(cid:105)).

(cid:88)

w

We ﬁx context c and view vw’s as random variables throughout this proof. Recall that vw is composed of
vw = sw · ˆvw, where sw is the scaling and ˆvw is from spherical Gaussian with identity covariance Id×d. Let
s be a random variable that has the same distribution as sw.

We lowerbound the mean of Zc as follows:

E[Zc] = n E [exp((cid:104)vw, c(cid:105))] ≥ n E [1 + (cid:104)vw, c(cid:105)] = n

where the last equality holds because of the symmetry of the spherical Gaussian distibution. On the other
hand, to upperbound the mean of Zc, we condition on the scaling sw,

E[Zc] = n E[exp((cid:104)vw, c(cid:105))]

= n E [E [exp((cid:104)vw, c(cid:105)) | sw]]

Note that conditioned on sw, we have that (cid:104)vw, c(cid:105) is a Gaussian random variable with variance σ2 = s2
w.

Therefore,

E [exp((cid:104)vw, c(cid:105)) | sw] =

(cid:90)

σ

x
(cid:90)

1
√
2π
1
√
σ
2π
= exp(σ2/2)

=

x

exp(−

exp(−

x2
2σ2 ) exp(x)dx
(x − σ2)2
2σ2

+ σ2/2)dx

It follows that

E[Zc] = n E[exp(σ2/2)] = n E[exp(s2

w/2)] = n E[exp(s2/2)].

We calculate the variance of Zc as follows:

V[Zc] =

V [exp((cid:104)vw, c(cid:105))] ≤ n E[exp(2(cid:104)vw, c(cid:105))]

(cid:88)

w

= n E [E [exp(2(cid:104)vw, c(cid:105)) | sw]]

By a very similar calculation as above, using the fact that 2(cid:104)vw, c(cid:105) is a Gaussian random variable with
variance 4σ2 = 4s2
w,

E [exp(2(cid:104)vw, c(cid:105)) | sw] = exp(2σ2)

29

Therefore, we have that

V[Zc] ≤ n E [E [exp(2(cid:104)vw, c(cid:105)) | sw]]

= n E (cid:2)exp(2σ2)(cid:3) = n E (cid:2)exp(2s2)(cid:3) ≤ Λn

for Λ = exp(8κ2) a constant, and at the last step we used the facts that s ≤ κ a.s.

Now we are ready to prove Lemma 2.1.

Proof of Lemma 2.1. We ﬁx the choice of c, and the proving the concentration using the randomness of vw’s
ﬁrst. Note that that exp((cid:104)vw, c(cid:105)) is neither sub-Gaussian nor sub-exponential (actually the Orlicz norm of
random variable exp((cid:104)vw, c(cid:105)) is never bounded). This prevents us from applying the usual concentration
inequalities. The proof deals with this issue in a slightly more specialized manner.

Let’s deﬁne Fw be the event that |(cid:104)vw, c(cid:105)| ≤ 1

2 log n. We claim that Pr[Fw] ≥ 1−exp(−Ω(log2 n)). Indeed
note that (cid:104)vw, c(cid:105) | sw has a Gaussian distribution with standard deviation sw(cid:107)c(cid:107) = sw ≤ 2κ a.s. Therefore
by the Gaussianity of (cid:104)vw, c(cid:105) we have that

Pr[|(cid:104)vw, c(cid:105)| ≥ η log n | sw] ≤ 2 exp(−Ω(

log2 n/κ2)) = exp(−Ω(log2 n)),

where Ω(·) hides the dependency on κ which is treated as absolute constants. Taking expectations over sw,
we obtain that

Pr[Fw] = Pr[|(cid:104)vw, c(cid:105)| ≤

log n] ≥ 1 − exp(−Ω(log2 n)).

Note that by deﬁnition, we in particular have that conditioned on Fw, it holds that exp((cid:104)vw, c(cid:105)) ≤

Let the random variable Xw have the same distribution as exp((cid:104)vw, c(cid:105))|Fw . We prove that the random
w Xw concentrates well. By convexity of the exponential function, we have that the mean

variable Z (cid:48)
of Z (cid:48)

c = (cid:80)
c is lowerbounded

√

n.

1
4

1
2

E[Z (cid:48)

c] = n E [exp((cid:104)vw, c(cid:105))|Fw ] ≥ n exp(E [(cid:104)vw, c(cid:105)|Fw ]) = n

and the variance is upperbounded by

V[Z (cid:48)

c] ≤ n E (cid:2)exp((cid:104)vw, c(cid:105))2|Fw

(cid:3)

≤

≤

1
Pr[Fw]
1
Pr[Fw]

E (cid:2)exp((cid:104)vw, c(cid:105))2(cid:3)

Λn ≤ 1.1Λn

where the second line uses the fact that

E (cid:2)exp((cid:104)vw, c(cid:105))2(cid:3)

= Pr[Fw] E (cid:2)exp((cid:104)vw, c(cid:105))2|Fw
≥ Pr[Fw] E (cid:2)exp((cid:104)vw, c(cid:105))2|Fw
√

(cid:3) + Pr[F w] E (cid:2)exp((cid:104)vw, c(cid:105))2|F w
(cid:3) .

(cid:3)

Moreover, by deﬁnition, for any w, |Xw| ≤

n. Therefore by Bernstein’s inequality, we have that

Pr [|Z (cid:48)

c − E[Z (cid:48)

c]| > (cid:15)n] ≤ exp(−

1

2 (cid:15)2n2
√

1.1Λn + 1
3

)

n · (cid:15)n

30

Note that E[Z (cid:48)

c] ≥ n, therefore for (cid:15) (cid:29) log2 n√

n , we have,

Pr [|Z (cid:48)

c − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c]] ≤ Pr [|Z (cid:48)

c − E[Z (cid:48)

c]| > (cid:15)n] ≤ exp(−
√

n}))

1

2 (cid:15)2n2
√

)

n · (cid:15)n

Λn + 1
3

≤ exp(−Ω(min{(cid:15)2n/Λ, (cid:15)
≤ exp(−Ω(log2 n))

Let F = ∪wFw be the union of all Fw. Then by union bound, it holds that Pr[ ¯F] ≤ (cid:80)

n · exp(−Ω(log2 n)) = exp(−Ω(log2 n)). We have that by deﬁnition, Z (cid:48)
Therefore, we have that

w Pr[ ¯Fw] ≤
c has the same distribution as Zc|F .

Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c] | F] ≤ exp(−Ω(log2 n))

(A.30)

and therefore

Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c]] = Pr[F] · Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c] | F] + Pr[ ¯F] Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c] | ¯F]

≤ Pr[|Zc − E[Z (cid:48)
≤ exp(−Ω(log2 n))

c]| > (cid:15) E[Z (cid:48)

c] | F] + Pr[ ¯F]

(A.31)

where at the last line we used the fact that Pr[ ¯F] ≤ exp(−Ω(log2 n)) and equation (A.30).

Let Z = E[Z (cid:48)

c] = E[exp((cid:104)vw, c(cid:105)) | | (cid:104)vw, c(cid:105) | < 1

2 log n] (note that E[Z (cid:48)

c] only depends on the norm of (cid:107)c(cid:107)

which is equal to 1). Therefore we obtain that with high probability over the randomness of vw’s,

(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z

(A.32)

Taking expectation over the randomness of c, we have that

[ (A.32) holds] ≥ 1 − exp(−Ω(log2 n))

Pr
c,vw

Therefore by a standard averaging argument (using Markov inequality), we have

(cid:104)

Pr
vw

Pr
c

[ (A.32) holds] ≥ 1 − exp(−Ω(log2 n))

≥ 1 − exp(−Ω(log2 n))

(cid:105)

For now on we ﬁx a choice of vw’s so that Prc[ (A.32) holds] ≥ 1 − exp(−Ω(log2 n)) is true. Therefore in the
rest of the proof, only c is random variable, and with probability 1 − exp(−Ω(log2 n)) over the randomness
of c, it holds that,

(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z.

(A.33)

B Maximum likelihood estimator for co-occurrence

Let L be the corpus size, and Xw,w(cid:48) the number of times words w, w(cid:48) co-occur within a context of size 10
in the corpus. According to the model, the probability of this event at any particular time is log p(w, w(cid:48)) ∝
(cid:107)vw + vw(cid:48)(cid:107)2
2 . Successive samples from a random walk are not independent of course, but if the random walk
mixes fairly quickly (and the mixing time of our random walk is related to the logarithm of the number
of words) then the set of Xw,w(cid:48)’s over all word pairs is distributed up to a very close approximation as a
multinomial distribution Mul( ˜L, {p(w, w(cid:48))}) where ˜L = (cid:80)
w,w(cid:48) Xw,w(cid:48) is the total number of word pairs in
consideration (roughly 10L).

31

Assuming this approximation, we show below that the maximum likelihood values for the word vectors

correspond to the following optimization,

min
{vw},C

(cid:88)

w,w(cid:48)

(cid:16)

Xw,w(cid:48)

log(Xw,w(cid:48)) − (cid:107)vw +vw(cid:48)(cid:107)2

2 − C

(cid:17)2

(Objective SN)

Now we give the derivation of the objective. According to the multinomial distribution, maximizing the

likelihood of {Xw,w(cid:48)} is equivalent to maximizing

To reason about the likelihood, denote the logarithm of the ratio between the expected count and the

empirical count as

Note that

(cid:96) = log

p(w, w(cid:48))Xw,w(cid:48)

 =

Xw,w(cid:48) log p(w, w(cid:48)).





(cid:89)

(w,w(cid:48))



(cid:88)

(w,w(cid:48))

∆w,w(cid:48) = log

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

(cid:33)

.

(cid:88)

(cid:96) =

Xw,w(cid:48) log p(w, w(cid:48))

(w,w(cid:48))

(cid:88)

(w,w(cid:48))

(cid:88)

(w,w(cid:48))

=

=

Xw,w(cid:48)

log

+ log

(cid:34)

Xw,w(cid:48)
˜L

(cid:33)(cid:35)

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

Xw,w(cid:48) log

Xw,w(cid:48) log

Xw,w(cid:48)
˜L

(cid:88)

+

(w,w(cid:48))

(cid:33)

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

where we let c denote the constant (cid:80)

(w,w(cid:48)) Xw,w(cid:48) log Xw,w(cid:48)
˜L

. Furthermore, we have

(B.1)

(cid:88)

= c +

Xw,w(cid:48)∆w,w(cid:48)

(w,w(cid:48))

(cid:88)

˜L =

˜Lpw,w(cid:48)

Xw,w(cid:48)e∆w,w(cid:48)

=

=

(w,w(cid:48))
(cid:88)

(w,w(cid:48))
(cid:88)

(w,w(cid:48))

Xw,w(cid:48)(1 + ∆w,w(cid:48) + ∆2

w,w(cid:48)/2 + O(|∆w,w(cid:48)|3))

and also ˜L = (cid:80)

(w,w(cid:48)) Xw,w(cid:48). So

Plugging this into (3.2) leads to

Xw,w(cid:48)∆w,w(cid:48) = −

Xw,w(cid:48)∆2

w,w(cid:48)/2 +

Xw,w(cid:48)O(|∆w,w(cid:48)|3)

 .





(cid:88)

(w,w(cid:48))

(cid:88)

(w,w(cid:48))



(cid:88)

(w,w(cid:48))

c − (cid:96) =

Xw,w(cid:48)∆2

w,w(cid:48)/2 +

Xw,w(cid:48)O(|∆w,w(cid:48)|3).

(B.2)

(cid:88)

(w,w(cid:48))

(cid:88)

(w,w(cid:48))

32

When the last term is much smaller than the ﬁrst term on the right hand side, maximizing the likelihood

is approximately equivalent to minimizing the ﬁrst term on the right hand side, which is our objective:

(cid:88)

(w,w(cid:48))

Xw,w(cid:48)∆2

w,w(cid:48) ≈

Xw,w(cid:48)

(cid:107)vw + vw(cid:48)(cid:107)2

2/(2d) − log Xw,w(cid:48) + log ˜L − 2 log Z

(cid:17)2

(cid:16)

(cid:88)

(w,w(cid:48))

where Z is the partition function.

We now argue that the last term is much smaller than the ﬁrst term on the right hand side in (B.2). For
a large Xw,w(cid:48), the ∆w,w(cid:48) is close to 0 and thus the induced approximation error is small. Small Xw,w(cid:48)’s only
contribute a small fraction of the ﬁnal objective (3.3), so we can safely ignore the errors. To see this, note
that the objective (cid:80)
(w,w(cid:48)) Xw,w(cid:48)O(|∆w,w(cid:48)|3) diﬀer by a factor of
|∆w,w(cid:48)| for each Xw,w(cid:48). For large Xw,w(cid:48)’s, |∆w,w(cid:48)| (cid:28) 1, and thus their corresponding errors are much smaller
than the objective. So we only need to consider the Xw,w(cid:48)’s that are small constants. The co-occurrence
counts obey a power law distribution (see, e.g. (Pennington et al., 2014)). That is, if one sorts {Xw,w(cid:48)} in
decreasing order, then the r-th value in the list is roughly

w,w(cid:48) and the error term (cid:80)

(w,w(cid:48)) Xw,w(cid:48)∆2

where k is some constant. Some calculation shows that

x[r] =

k
r5/4

and thus when x is a small constant

˜L ≈ 4k,

(cid:88)

Xw,w(cid:48) ≈ 4k4/5x1/5,

Xw,w(cid:48) ≤x

(cid:80)

Xw,w(cid:48) ≤x Xw,w(cid:48)
˜L

≈

(cid:18) 4x
˜L

(cid:19)1/5

= O

(cid:18) 1

˜L1/5

(cid:19)

.

So there are only a negligible mass of Xw,w(cid:48)’s that are small constants, which vanishes when ˜L increases.
Furthermore, we empirically observe that the relative error of our objective is 5%, which means that the
errors induced by Xw,w(cid:48)’s that are small constants is only a small fraction of the objective. Therefore,
(cid:80)

w,w(cid:48) Xw,w(cid:48)O(|∆w,w(cid:48)|3) is small compared to the objective and can be safely ignored.

33

9
1
0
2
 
n
u
J
 
9
1
 
 
]

G
L
.
s
c
[
 
 
8
v
0
2
5
3
0
.
2
0
5
1
:
v
i
X
r
a

RAND-WALK: A latent variable model approach to word
embeddings

Sanjeev Arora

Yuanzhi Li

Yingyu Liang

Tengyu Ma

Andrej Risteski ∗

Abstract

Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse
methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparam-
eters and reweighting methods.

This paper proposes a new generative model, a dynamic version of the log-linear topic model of Mnih
and Hinton (2007). The methodological novelty is to use the prior to compute closed form expressions
for word statistics. This provides a theoretical justiﬁcation for nonlinear models like PMI, word2vec,
and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic
embeddings contain linear algebraic structure that allows solution of word analogies, as shown by Mikolov
et al. (2013a) and many subsequent papers.

Experimental support is provided for the generative model assumptions, the most important of which

is that latent word vectors are fairly uniformly dispersed in space.

1 Introduction

Vector representations of words (word embeddings) try to capture relationships between words as distance or
angle, and have many applications in computational linguistics and machine learning. They are constructed
by various models whose unifying philosophy is that the meaning of a word is deﬁned by “the company it
keeps” (Firth, 1957), namely, co-occurrence statistics. The simplest methods use word vectors that explicitly
represent co-occurrence statistics. Reweighting heuristics are known to improve these methods, as is dimen-
sion reduction (Deerwester et al., 1990). Some reweighting methods are nonlinear, which include taking the
square root of co-occurrence counts (Rohde et al., 2006), or the logarithm, or the related Pointwise Mutual
Information (PMI) (Church and Hanks, 1990). These are collectively referred to as Vector Space Models,
surveyed in (Turney and Pantel, 2010).

Neural network language models (Rumelhart et al., 1986; 1988; Bengio et al., 2006; Collobert and Weston,
2008a) propose another way to construct embeddings: the word vector is simply the neural network’s internal
representation for the word. This method is nonlinear and nonconvex. It was popularized via word2vec, a
family of energy-based models in (Mikolov et al., 2013b;c), followed by a matrix factorization approach called
GloVe (Pennington et al., 2014). The ﬁrst paper also showed how to solve analogies using linear algebra on
word embeddings. Experiments and theory were used to suggest that these newer methods are related to
the older PMI-based models, but with new hyperparameters and/or term reweighting methods (Levy and
Goldberg, 2014b).

But note that even the old PMI method is a bit mysterious. The simplest version considers a symmetric
matrix with each row/column indexed by a word. The entry for (w, w(cid:48)) is PMI(w, w(cid:48)) = log p(w,w(cid:48))
p(w)p(w(cid:48)) , where
p(w, w(cid:48)) is the empirical probability of words w, w(cid:48) appearing within a window of certain size in the corpus,
and p(w) is the marginal probability of w. (More complicated models could use asymmetric matrices with

∗Princeton University, Computer Science Department. {arora,yuanzhil,yingyul,tengyu,risteski}@cs.princeton.edu.
This work was supported in part by NSF grants CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, Simons Investigator
Award, and Simons Collaboration Grant. Tengyu Ma was also supported by Simons Award for Graduate Students in Theoretical
Computer Science and IBM PhD Fellowship.

1

columns corresponding to context words or phrases, and also involve tensorization.) Then word vectors are
obtained by low-rank SVD on this matrix, or a related matrix with term reweightings. In particular, the
PMI matrix is found to be closely approximated by a low rank matrix: there exist word vectors in say 300
dimensions, which is much smaller than the number of words in the dictionary, such that

(cid:104)vw, vw(cid:48)(cid:105) ≈ PMI(w, w(cid:48))

(1.1)

where ≈ should be interpreted loosely.

There appears to be no theoretical explanation for this empirical ﬁnding about the approximate low
rank of the PMI matrix. The current paper addresses this. Speciﬁcally, we propose a probabilistic model
of text generation that augments the log-linear topic model of Mnih and Hinton (2007) with dynamics, in
the form of a random walk over a latent discourse space. The chief methodological contribution is using the
model priors to analytically derive a closed-form expression that directly explains (1.1); see Theorem 2.2
in Section 2. Section 3 builds on this insight to give a rigorous justiﬁcation for models such as word2vec
and GloVe, including the hyperparameter choices for the latter. The insight also leads to a mathematical
explanation for why these word embeddings allow analogies to be solved using linear algebra; see Section 4.
Section 5 shows good empirical ﬁt to this model’s assumtions and predictions, including the surprising one
that word vectors are pretty uniformly distributed (isotropic) in space.1

1.1 Related work

Latent variable probabilistic models of language have been used for word embeddings before, including
Latent Dirichlet Allocation (LDA) and its more complicated variants (see the survey (Blei, 2012)), and some
neurally inspired nonlinear models (Mnih and Hinton, 2007; Maas et al., 2011). In fact, LDA evolved out of
eﬀorts in the 1990s to provide a generative model that “explains” the success of older vector space methods
like Latent Semantic Indexing (Papadimitriou et al., 1998; Hofmann, 1999). However, none of these earlier
generative models has been linked to PMI models.

Levy and Goldberg (2014b) tried to relate word2vec to PMI models. They showed that if there were no
dimension constraint in word2vec, speciﬁcally, the “skip-gram with negative sampling (SGNS)” version of the
model, then its solutions would satisfy (1.1), provided the right hand side were replaced by PMI(w, w(cid:48)) − β
for some scalar β. However, skip-gram is a discriminative model (due to the use of negative sampling), not
generative. Furthermore, their argument only applies to very high-dimensional word embeddings, and thus
does not address low-dimensional embeddings, which have superior quality in applications.

Hashimoto et al. (2016) focuses on issues similar to our paper. They model text generation as a random
walk on words, which are assumed to be embedded as vectors in a geometric space. Given that the last word
produced was w, the probability that the next word is w(cid:48) is assumed to be given by h(|vw − vw(cid:48)|2) for a
suitable function h, and this model leads to an explanation of (1.1). By contrast our random walk involves
a latent discourse vector, which has a clearer semantic interpretation and has proven useful in subsequent
work, e.g. understanding structure of word embeddings for polysemous words Arora et al. (2016). Also our
work clariﬁes some weighting and bias terms in the training objectives of previous methods (Section 3) and
also the phenomenon discussed in the next paragraph.

Researchers have tried to understand why vectors obtained from the highly nonlinear word2vec models
exhibit linear structures (Levy and Goldberg, 2014a; Pennington et al., 2014). Speciﬁcally, for analogies like
“man:woman::king:??,” queen happens to be the word whose vector vqueen is the most similar to the vector
vking − vman + vwoman. This suggests that simple semantic relationships, such as masculine vs feminine
tested in the above example, correspond approximately to a single direction in space, a phenomenon we will
henceforth refer to as relations=lines.

Section 4 surveys earlier attempts to explain this phenomenon and their shortcoming, namely, that they
ignore the large approximation error in relationships like (1.1). This error appears larger than the diﬀerence
between the best solution and the second best (incorrect) solution in analogy solving, so that this error could
in principle lead to a complete failure in analogy solving. In our explanation, the low dimensionality of the

1The code is available at https://github.com/PrincetonML/SemanticVector

2

word vectors plays a key role. This can also be seen as a theoretical explanation of the old observation that
dimension reduction improves the quality of word embeddings for various tasks. The intuitive explanation
often given —that smaller models generalize better—turns out to be fallacious, since the training method
for creating embeddings makes no reference to analogy solving. Thus there is no a priori reason why low-
dimensional model parameters (i.e., lower model capacity) should lead to better performance in analogy
solving, just as there is no reason they are better at some other unrelated task like predicting the weather.

1.2 Beneﬁts of generative approaches

In addition to giving some form of “uniﬁcation” of existing methods, our generative model also brings
more intepretability to word embeddings beyond traditional cosine similarity and even analogy solving. For
example, it led to an understanding of how the diﬀerent senses of a polysemous word (e.g., bank) reside in
linear superposition within the word embedding (Arora et al., 2016). Such insight into embeddings may
prove useful in the numerous settings in NLP and neuroscience where they are used.

Another new explanatory feature of our model is that low dimensionality of word embeddings plays a key
theoretical role —unlike in previous papers where the model is agnostic about the dimension of the embed-
dings, and the superiority of low-dimensional embeddings is an empirical ﬁnding (starting with Deerwester
et al. (1990)). Speciﬁcally, our theoretical analysis makes the key assumption that the set of all word vectors
(which are latent variables of the generative model) are spatially isotropic, which means that they have no
preferred direction in space. Having n vectors be isotropic in d dimensions requires d (cid:28) n. This isotropy is
needed in the calculations (i.e., multidimensional integral) that yield (1.1). It also holds empirically for our
word vectors, as shown in Section 5.

The isotropy of low-dimensional word vectors also plays a key role in our explanation of the rela-
tions=lines phenomenon (Section 4). The isotropy has a “puriﬁcation” eﬀect that mitigates the eﬀect of
the (rather large) approximation error in the PMI models.

2 Generative model and its properties

The model treats corpus generation as a dynamic process, where the t-th word is produced at step t. The
process is driven by the random walk of a discourse vector ct ∈ (cid:60)d. Its coordinates represent what is being
talked about.2 Each word has a (time-invariant) latent vector vw ∈ (cid:60)d that captures its correlations with
the discourse vector. We model this bias with a log-linear word production model:

Pr[w emitted at time t | ct] ∝ exp((cid:104)ct, vw(cid:105)).

(2.1)

The discourse vector ct does a slow random walk (meaning that ct+1 is obtained from ct by adding a small
random displacement vector), so that nearby words are generated under similar discourses. We are interested
in the probabilities that word pairs co-occur near each other, so occasional big jumps in the random walk
are allowed because they have negligible eﬀect on these probabilities.

A similar log-linear model appears in Mnih and Hinton (2007) but without the random walk. The linear
chain CRF of Collobert and Weston (2008b) is more general. The dynamic topic model of Blei and Laﬀerty
(2006) utilizes topic dynamics, but with a linear word production model. Belanger and Kakade (2015) have
proposed a dynamic model for text using Kalman Filters, where the sequence of words is generated from
Gaussian linear dynamical systems, rather than the log-linear model in our case.

The novelty here over such past works is a theoretical analysis in the method-of-moments tradition (Hsu
et al., 2012; Cohen et al., 2012). Assuming a prior on the random walk we analytically integrate out the
hidden random variables and compute a simple closed form expression that approximately connects the
model parameters to the observable joint probabilities (see Theorem 2.2). This is reminiscent of analysis of
similar random walk models in ﬁnance (Black and Scholes, 1973).

2This is a diﬀerent interpretation of the term “discourse” compared to some other settings in computational linguistics.

3

Model details. Let n denote the number of words and d denote the dimension of the discourse space,
where 1 ≤ d ≤ n. Inspecting (2.1) suggests word vectors need to have varying lengths, to ﬁt the empirical
ﬁnding that word probabilities satisfy a power law. Furthermore, we will assume that in the bulk, the word
vectors are distributed uniformly in space, earlier referred to as isotropy. This can be quantiﬁed as a prior
in the Bayesian tradition. More precisely, the ensemble of word vectors consists of i.i.d draws generated by
v = s · ˆv, where ˆv is from the spherical Gaussian distribution, and s is a scalar random variable. We assume
s is a random scalar with expectation τ = Θ(1) and s is always upper bounded by κ, which is another
constant. Here τ governs the expected magnitude of (cid:104)v, ct(cid:105), and it is particularly important to choose it to
be Θ(1) so that the distribution Pr[w|ct] ∝ exp((cid:104)vw, ct(cid:105)) is interesting.3 Moreover, the dynamic range of
word probabilities will roughly equal exp(κ2), so one should think of κ as an absolute constant like 5. These
details about s are important for realistic modeling but not too important in our analysis. (Furthermore,
readers uncomfortable with this simplistic Bayesian prior should look at Section 2.2 below.)

Finally, we clarify the nature of the random walk. We assume that the stationary distribution of the
random walk is uniform over the unit sphere, denoted by C. The transition kernel of the random walk can
d in (cid:96)2 norm.4
be in any form so long as at each step the movement of the discourse vector is at most (cid:15)2/
This is still fast enough to let the walk mix quickly in the space.

√

The following lemma (whose proof appears in the appendix) is central to the analysis.

It says that
under the Bayesian prior, the partition function Zc = (cid:80)
w exp((cid:104)vw, c(cid:105)), which is the implied normalization
in equation (2.1), is close to some constant Z for most of the discourses c. This can be seen as a plausible
theoretical explanation of a phenomenon called self-normalization in log-linear models: ignoring the partition
function or treating it as a constant (which greatly simpliﬁes training) is known to often give good results.
This has also been studied in (Andreas and Klein, 2014).

Lemma 2.1 (Concentration of partition functions). If the word vectors satisfy the Bayesian prior described
in the model details, then

Pr
c∼C

[(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z] ≥ 1 − δ,

(2.2)

for (cid:15)z = (cid:101)O(1/

n), and δ = exp(−Ω(log2 n)).

√

The concentration of the partition functions then leads to our main theorem (the proof is in the appendix).
The theorem gives simple closed form approximations for p(w), the probability of word w in the corpus, and
p(w, w(cid:48)), the probability that two words w, w(cid:48) occur next to each other. The theorem states the result for
the window size q = 2, but the same analysis works for pairs that appear in a small window, say of size 10,
as stated in Corollary 2.3. Recall that PMI(w, w(cid:48)) = log[p(w, w(cid:48))/(p(w)p(w(cid:48)))].

Theorem 2.2. Suppose the word vectors satisfy the inequality (2.2), and window size q = 2. Then,

for (cid:15) = O((cid:15)z) + (cid:101)O(1/d) + O((cid:15)2). Jointly these imply:

log p(w, w(cid:48)) =

− 2 log Z ± (cid:15),

(cid:107)vw + vw(cid:48)(cid:107)2
2
2d
(cid:107)vw(cid:107)2
2
2d

log p(w) =

− log Z ± (cid:15).

PMI (w, w(cid:48)) =

(cid:104)vw, vw(cid:48)(cid:105)
d

± O((cid:15)).

(2.3)

(2.4)

(2.5)

Remarks 1. Since the word vectors have (cid:96)2 norm of the order of
(cid:107)vw + vw(cid:48)(cid:107)2

d, for two typical word vectors vw, vw(cid:48),
2 is of the order of Θ(d). Therefore the noise level (cid:15) is very small compared to the leading

3A larger τ will make Pr[w|ct] too peaked and a smaller one will make it too uniform.
4 More precisely, the proof extends to any symmetric product stationary distribution C with sub-Gaussian coordinate

satisfying Ec

(cid:2)(cid:107)c(cid:107)2(cid:3) = 1, and the steps are such that for all ct, Ep(ct+1|ct)[exp(κ

d(cid:107)ct+1 − ct(cid:107))] ≤ 1 + (cid:15)2 for some small (cid:15)2.

√

√

4

2d (cid:107)vw + vw(cid:48)(cid:107)2

term 1
empirically we also ﬁnd higher error here.

2. For PMI however, the noise level O((cid:15)) could be comparable to the leading term, and

Remarks 2. Variants of the expression for joint probability in (2.3) had been hypothesized based upon
empirical evidence in Mikolov et al. (2013b) and also Globerson et al. (2007), and Maron et al. (2010) .

Remarks 3. Theorem 2.2 directly leads to the extension to a general window size q as follows:

Corollary 2.3. Let pq(w, w(cid:48)) be the co-occurrence probability in windows of size q, and PMIq(w, w(cid:48)) be the
corresponding PMI value. Then

log pq(w, w(cid:48)) =

− 2 log Z + γ ± (cid:15),

PMIq (w, w(cid:48)) =

+ γ ± O((cid:15)).

(cid:107)vw + vw(cid:48)(cid:107)2
2
2d

(cid:104)vw, vw(cid:48)(cid:105)
d

where γ = log

(cid:16) q(q−1)
2

(cid:17)

.

It is quite easy to see that Theorem 2.2 implies the Corollary 2.3, as when the window size is q the
(cid:1) positions within the window, and the joint probability of w, w(cid:48) is
pair w, w(cid:48) could appear in any of (cid:0)q
roughly the same for any positions because the discourse vector changes slowly.
(Of course, the error
term gets worse as we consider larger window sizes, although for any constant size, the statement of the
theorem is correct.) This is also consistent with the shift β for ﬁtting PMI in (Levy and Goldberg, 2014b),
which showed that without dimension constraints, the solution to skip-gram with negative sampling satisﬁes
PMI (w, w(cid:48)) − β = (cid:104)vw, vw(cid:48)(cid:105) for a constant β that is related to the negative sampling in the optimization.
Our result justiﬁes via a generative model why this should be satisﬁed even for low dimensional word vectors.

2

2.1 Proof sketches

Here we provide the proof sketches, while the complete proof can be found in the appendix.

Proof sketch of Theorem 2.2 Let w and w(cid:48) be two arbitrary words. Let c and c(cid:48) denote two consecutive
context vectors, where c ∼ C and c(cid:48)|c is deﬁned by the Markov kernel p(c(cid:48) | c).

We start by using the law of total expectation, integrating out the hidden variables c and c(cid:48):

An expectation like (2.6) would normally be diﬃcult to analyze because of the partition functions.
However, we can assume the inequality (2.2), that is, the partition function typically does not vary much for
most of context vectors c. Let F be the event that both c and c(cid:48) are within (1 ± (cid:15)z)Z. Then by (2.2) and the
union bound, event F happens with probability at least 1 − 2 exp(−Ω(log2 n)). We will split the right-hand
side (RHS) of (2.6) into the parts according to whether F happens or not.

p(w, w(cid:48)) = E
c,c(cid:48)
= E
c,c(cid:48)

[Pr[w, w(cid:48)|c, c(cid:48)]]

[p(w|c)p(w(cid:48)|c(cid:48))]

= E
c,c(cid:48)

(cid:20) exp((cid:104)vw, c(cid:105))
Zc

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

(cid:21)

RHS of (2.6) = E
c,c(cid:48)
(cid:124)

(cid:20) exp((cid:104)vw, c(cid:105))
Zc

(cid:20) exp((cid:104)vw, c(cid:105))
Zc

+ E
c,c(cid:48)
(cid:124)

(cid:123)(cid:122)
T2

5

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

(cid:123)(cid:122)
T1
exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

1F

1 ¯F

(cid:21)

(cid:125)

(cid:21)

(cid:125)

(2.6)

(2.7)

where ¯F denotes the complement of event F and 1F and 1 ¯F denote indicator functions for F and ¯F ,
respectively. When F happens, we can replace Zc by Z with a 1 ± (cid:15)z factor loss: The ﬁrst term of the RHS
of (2.7) equals to

T1 =

1 ± O((cid:15)z)
Z 2

E
c,c(cid:48)

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ]

(2.8)

On the other hand, we can use E[1 ¯F ] = Pr[ ¯F ] ≤ exp(−Ω(log2 n)) to show that the second term of RHS

of (2.7) is negligible,

|T2| = exp(−Ω(log1.8 n)) .
This claim must be handled somewhat carefully since the RHS does not depend on d at all. Brieﬂy, the
d = o(log2 n)), any word vector vw and
reason this holds is as follows:
d)), and since E[1 ¯F ] = exp(−Ω(log2 n)), the
discourse c satisﬁes that exp((cid:104)vw, c(cid:105)) ≤ exp((cid:107)vw(cid:107)) = exp(O(
d = Ω(log2 n)), we can use concentration inequalities
claim follows directly; In the regime when d is large (
to show that except with a small probability exp(−Ω(d)) = exp(−Ω(log2 n)), a uniform sample from the
sphere behaves equivalently to sampling all of the coordinates from a standard Gaussian distribution with
mean 0 and variance 1

d , in which case the claim is not too diﬃcult to show using Gaussian tail bounds.

in the regime when d is small (

Therefore it suﬃces to only consider (2.8). Our model assumptions state that c and c(cid:48) cannot be too

(2.9)

√

√

√

diﬀerent. We leverage that by rewriting (2.8) a little, and get that it equals

(cid:20)
exp((cid:104)vw, c(cid:105)) E
c(cid:48)|c

(cid:21)
[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

T1 =

=

1 ± O((cid:15)z)
Z 2
1 ± O((cid:15)z)
Z 2

E
c

E
c

[exp((cid:104)vw, c(cid:105))A(c)]

(2.10)

[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]. We claim that A(c) = (1 ± O((cid:15)2)) exp((cid:104)vw(cid:48), c(cid:105)). Doing some algebraic

where A(c) := E
c(cid:48)|c

manipulations,

Furthermore, by our model assumptions, (cid:107)c − c(cid:48)(cid:107) ≤ (cid:15)2/

d. So

√

A(c) = exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48) − c(cid:105))] .

(cid:104)vw, c − c(cid:48)(cid:105) ≤ (cid:107)vw(cid:107)(cid:107)c − c(cid:48)(cid:107) = O((cid:15)2)

and thus A(c) = (1 ± O((cid:15)2)) exp((cid:104)vw(cid:48), c(cid:105)). Plugging the simpliﬁcation of A(c) to (2.10),

T1 =

1 ± O((cid:15)z)
Z 2

E[exp((cid:104)vw + vw(cid:48), c(cid:105))].

Since c has uniform distribution over the sphere, the random variable (cid:104)vw + vw(cid:48), c(cid:105) has distribution pretty
similar to Gaussian distribution N (0, (cid:107)vw + vw(cid:48)(cid:107)2/d), especially when d is relatively large. Observe that
E[exp(X)] has a closed form for Gaussian random variable X ∼ N (0, σ2),

E[exp(X)] =

(cid:90)

1
√
2π

x

σ
= exp(σ2/2) .

exp(−

x2
2σ2 ) exp(x)dx

(2.11)

(2.12)

(2.13)

Bounding the diﬀerence between (cid:104)vw + vw(cid:48), c(cid:105) from Gaussian random variable, we can show that for

(cid:15) = (cid:101)O(1/d),

E[exp((cid:104)vw + vw(cid:48), c(cid:105))] = (1 ± (cid:15)) exp

(cid:18) (cid:107)vw + vw(cid:48)(cid:107)2
2d

(cid:19)

.

Therefore, the series of simpliﬁcation/approximation above (concretely, combining equations (2.6), (2.7), (2.9), (2.11),

and (2.13)) lead to the desired bound on log p(w, w(cid:48)) for the case when the window size q = 2. The bound
on log p(w) can be shown similarly.

6

Proof sketch of Lemma 2.1 Note that for ﬁxed c, when word vectors have Gaussian priors assumed as
in our model, Zc = (cid:80)

w exp((cid:104)vw, c(cid:105)) is a sum of independent random variables.

We ﬁrst claim that using proper concentration of measure tools, it can be shown that the variance of
Zc are relatively small compared to its mean Evw [Zc], and thus Zc concentrates around its mean. Note this
is quite non-trivial: the random variable exp((cid:104)vw, c(cid:105)) is neither bounded nor subgaussian/sub-exponential,
since the tail is approximately inverse poly-logarithmic instead of inverse exponential.
In fact, the same
concentration phenomenon does not happen for w. The occurrence probability of word w is not necessarily
concentrated because the (cid:96)2 norm of vw can vary a lot in our model, which allows the frequency of the words
to have a large dynamic range.

So now it suﬃces to show that Evw [Zc] for diﬀerent c are close to each other. Using the fact that the
word vector directions have a Gaussian distribution, Evw [Zc] turns out to only depend on the norm of c
(which is equal to 1). More precisely,

[Zc] = f ((cid:107)c(cid:107)2

2) = f (1)

E
vw

(2.14)

where f is deﬁned as f (α) = n Es[exp(s2α/2)] and s has the same distribution as the norms of the word
vectors. We sketch the proof of this. In our model, vw = sw · ˆvw, where ˆvw is a Gaussian vector with identity
covariance I. Then

E
vw

[Zc] = n E
vw

[exp((cid:104)vw, c(cid:105))]

(cid:20)

= n E
sw

E
vw|sw

(cid:21)
[exp((cid:104)vw, c(cid:105)) | sw]

where the second line is just an application of the law of total expectation, if we pick the norm of the
(random) vector vw ﬁrst, followed by its direction. Conditioned on sw, (cid:104)vw, c(cid:105) is a Gaussian random variable
with variance (cid:107)c(cid:107)2

w, and therefore using similar calculation as in (2.12), we have

2s2

Hence, Evw [Zc] = n Es[exp(s2(cid:107)c(cid:107)2

2/2)] as needed.

E
vw|sw

[exp((cid:104)vw, c(cid:105)) | sw] = exp(s2(cid:107)c(cid:107)2

2/2) .

Proof of Theorem 4.1 The proof uses the standard analysis of linear regression. Let V = P ΣQT be the
SVD of V and let σ1, . . . , σd be the left singular values of V (the diagonal entries of Σ). For notational ease
we omit the subscripts in ¯ζ and ζ (cid:48) since they are not relevant for this proof. Since V † = QΣ−1P T and thus
¯ζ = V †ζ (cid:48) = QΣ−1P T ζ (cid:48), we have

We claim

Indeed, (cid:80)d
from the ﬁrst assumption. Furthermore, by the second assumption, (cid:107)P T ζ (cid:48)(cid:107)∞ ≤ c2√

i = O(nd), since the average squared norm of a word vector is d. The claim then follows

i=1 σ2

n (cid:107)ζ (cid:48)(cid:107)2, so

(2.15)

(2.16)

(2.17)

Plugging (2.16) and (2.17) into (2.15), we get

(cid:107)¯ζ(cid:107)2 ≤

(cid:114)

(cid:114) 1
c1n

c2
2d
n

(cid:107)ζ (cid:48)(cid:107)2

2 =

(cid:107)ζ (cid:48)(cid:107)2

c2
√

√

d
c1n

as desired. The last statement follows because the norm of the signal, which is d log(νR) originally and is
V †d log(νR) = va − vb after dimension reduction, also gets reduced by a factor of

n.

√

(cid:107)¯ζ(cid:107)2 ≤ σ−1

d (cid:107)P T ζ (cid:48)(cid:107)2.

σ−1
d ≤

(cid:114) 1
c1n

.

(cid:107)P T ζ (cid:48)(cid:107)2

2 ≤

(cid:107)ζ (cid:48)(cid:107)2
2.

c2
2d
n

7

2.2 Weakening the model assumptions

For readers uncomfortable with Bayesian priors, we can replace our assumptions with concrete properties of
word vectors that are empirically veriﬁable (Section 5.1) for our ﬁnal word vectors, and in fact also for word
vectors computed using other recent methods.

The word meanings are assumed to be represented by some “ground truth” vectors, which the experi-
menter is trying to recover. These ground truth vectors are assumed to be spatially isotropic in the bulk,
in the following two speciﬁc ways: (i) For almost all unit vectors c the sum (cid:80)
w exp((cid:104)vw, c(cid:105)) is close to a
constant Z; (ii) Singular values of the matrix of word vectors satisfy properties similar to those of random
matrices, as formalized in the paragraph before Theorem 4.1. Our Bayesian prior on the word vectors hap-
pens to imply that these two conditions hold with high probability. But the conditions may hold even if the
prior doesn’t hold. Furthermore, they are compatible with all sorts of local structure among word vectors
such as existence of clusterings, which would be absent in truly random vectors drawn from our prior.

3 Training objective and relationship to other models

To get a training objective out of Theorem 2.2, we reason as follows. Let Xw,w(cid:48) be the number of times words
w and w(cid:48) co-occur within the same window in the corpus. The probability p(w, w(cid:48)) of such a co-occurrence
at any particular time is given by (2.3). Successive samples from a random walk are not independent.
But if the random walk mixes fairly quickly (the mixing time is related to the logarithm of the vocabulary
size), then the distribution of Xw,w(cid:48)’s is very close to a multinomial distribution Mul( ˜L, {p(w, w(cid:48))}), where
˜L = (cid:80)

w,w(cid:48) Xw,w(cid:48) is the total number of word pairs.

Assuming this approximation, we show below that the maximum likelihood values for the word vectors

correspond to the following optimization,

min
{vw},C

(cid:88)

w,w(cid:48)

(cid:16)

Xw,w(cid:48)

log(Xw,w(cid:48)) − (cid:107)vw +vw(cid:48)(cid:107)2

2 − C

(cid:17)2

As is usual, empirical performance is improved by weighting down very frequent word pairs, possibly
because very frequent words such as “the” do not ﬁt our model. This is done by replacing the weighting
Xw,w(cid:48) by its truncation min{Xw,w(cid:48), Xmax} where Xmax is a constant such as 100. We call this objective with
the truncated weights SN (Squared Norm).

We now give its derivation. Maximizing the likelihood of {Xw,w(cid:48)} is equivalent to maximizing

Denote the logarithm of the ratio between the expected count and the empirical count as

Then with some calculation, we obtain the following where c is independent of the empirical observations
Xw,w(cid:48)’s.

(3.1)

(3.2)

(cid:96) = log

p(w, w(cid:48))Xw,w(cid:48)





(cid:89)

(w,w(cid:48))



 .

∆w,w(cid:48) = log

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

(cid:33)

.

(cid:96) = c +

Xw,w(cid:48)∆w,w(cid:48)

(cid:88)

(w,w(cid:48))

8

On the other hand, using ex ≈ 1 + x + x2/2 when x is small,5 we have

(cid:88)

˜L =

˜Lpw,w(cid:48) =

(cid:88)

Xw,w(cid:48)e∆w,w(cid:48)

(w,w(cid:48))

(w,w(cid:48))

(cid:32)

Xw,w(cid:48)

1 + ∆w,w(cid:48) +

(cid:88)

≈

(w,w(cid:48))

(cid:33)

.

∆2
w,w(cid:48)
2

(cid:88)

(w,w(cid:48))

Xw,w(cid:48)∆w,w(cid:48) ≈ −

Xw,w(cid:48)∆2

w,w(cid:48).

1
2

(cid:88)

(w,w(cid:48))

Note that ˜L = (cid:80)

(w,w(cid:48)) Xw,w(cid:48), so

Plugging this into (3.2) leads to

2(c − (cid:96)) ≈

Xw,w(cid:48)∆2

w,w(cid:48).

(cid:88)

(w,w(cid:48))

(3.3)

So maximizing the likelihood is approximately equivalent to minimizing the right hand side, which (by
examining (3.1)) leads to our objective.

Objective for training with PMI. A similar objective PMI can be obtained from (2.5), by computing
an approximate MLE, using the fact that the error between the empirical and true value of PMI(w, w(cid:48)) is
driven by the smaller term p(w, w(cid:48)), and not the larger terms p(w), p(w(cid:48)).

min
{vw},C

(cid:88)

w,w(cid:48)

Xw,w(cid:48) (PMI(w, w(cid:48)) − (cid:104)vw, vw(cid:48)(cid:105))2

This is of course very analogous to classical VSM methods, with a novel reweighting method.

Fitting to either of the objectives involves solving a version of Weighted SVD which is NP-hard, but

empirically seems solvable in our setting via AdaGrad (Duchi et al., 2011).

Connection to GloVe. Compare SN with the objective used by GloVe (Pennington et al., 2014):

f (Xw,w(cid:48))(log(Xw,w(cid:48)) − (cid:104)vw, vw(cid:48)(cid:105) − sw − sw(cid:48) − C)2

(cid:88)

w,w(cid:48)

with f (Xw,w(cid:48)) = min{X 3/4
w,w(cid:48), 100}. Their weighting methods and the need for bias terms sw, sw(cid:48), C were
derived by trial and error; here they are all predicted and given meanings due to Theorem 2.2, speciﬁcally
sw = (cid:107)vw(cid:107)2.

Connection to word2vec(CBOW). The CBOW model in word2vec posits that the probability of a
word wk+1 as a function of the previous k words w1, w2, . . . , wk:

(cid:16)

p

wk+1

(cid:12)
(cid:12) {wi}k

i=1

(cid:17)

∝ exp((cid:104)vwk+1,

vwi (cid:105)).

1
k

k
(cid:88)

i=1

5This Taylor series approximation has an error of the order of x3, but ignoring it can be theoretically justiﬁed as follows.
is close to 0 and thus ignoring
For a large Xw,w(cid:48) , its value approaches its expectation and thus the corresponding ∆w,w(cid:48)
∆3
w,w(cid:48) is well justiﬁed. The terms where ∆w,w(cid:48) is signiﬁcant correspond to Xw,w(cid:48) ’s that are small. But empirically, Xw,w(cid:48) ’s
obey a power law distribution (see, e.g. Pennington et al. (2014)) using which it can be shown that these terms contribute a
small fraction of the ﬁnal objective (3.3). So we can safely ignore the errors. Full details appear in the ArXiv version of this
paper (Arora et al., 2015).

9

This expression seems mysterious since it depends upon the average word vector for the previous k words.
We show it can be theoretically justiﬁed. Assume a simpliﬁed version of our model, where a small window
of k words is generated as follows: sample c ∼ C, where C is a uniformly random unit vector, then sample
(w1, w2, . . . , wk) ∼ exp((cid:104)(cid:80)k
i=1 vwi , c(cid:105))/Zc. Furthermore, assume Zc = Z for any c.

Lemma 3.1. In the simpliﬁed version of our model, the Maximum-a-Posteriori (MAP) estimate of c given

(w1, w2, . . . , wk) is

(cid:80)k
(cid:107) (cid:80)k

i=1 vwi
i=1 vwi (cid:107)2

.

(cid:80)k
(cid:107) (cid:80)k

i=1 vwi
i=1 vwi (cid:107)2

.

Proof. The c maximizing p (c|w1, w2, . . . , wk) is the maximizer of p(c)p (w1, w2, . . . , wk|c). Since p(c) =
p(c(cid:48)) for any c, c(cid:48), and we have p (w1, w2, . . . , wk|c) = exp((cid:104)(cid:80)
i vwi, c(cid:105))/Z, the maximizer is clearly c =

Thus using the MAP estimate of ct gives essentially the same expression as CBOW apart from the

rescaling, which is often omitted due to computational eﬃciency in empirical works.

4 Explaining relations=lines

As mentioned, word analogies like “a:b::c:??” can be solved via a linear algebraic expression:

argmin
d

(cid:107)va − vb − vc + vd(cid:107)2
2 ,

(4.1)

where vectors have been normalized such that (cid:107)vd(cid:107)2 = 1. This suggests that the semantic relationships
being tested in the analogy are characterized by a straight line,6 referred to earlier as relations=lines.

Using our model we will show the following for low-dimensional embeddings: for each such relation R
there is a direction µR in space such that for any word pair a, b satisfying the relation, va − vb is like µR
plus some noise vector. This happens for relations satisfying a certain condition described below. Empirical
results supporting this theory appear in Section 5, where this linear structure is further leveraged to slightly
improve analogy solving.

A side product of our argument will be a mathematical explanation of the empirically well-established su-
periority of low-dimensional word embeddings over high-dimensional ones in this setting (Levy and Goldberg,
2014a). As mentioned earlier, the usual explanation that smaller models generalize better is fallacious.

We ﬁrst sketch what was missing in prior attempts to prove versions of relations=lines from ﬁrst
principles. The basic issue is approximation error: the diﬀerence between the best solution and the 2nd best
solution to (4.1) is typically small, whereas the approximation error in the objective in the low-dimensional
solutions is larger. For instance, if one uses our PMI objective, then the weighted average of the termwise
error in (2.5) is 17%, and the expression in (4.1) above contains six inner products. Thus in principle the
approximation error could lead to a failure of the method and the emergence of linear relationship, but it
does not.

Prior explanations. Pennington et al. (2014) try to propose a model where such linear relationships
should occur by design. They posit that queen is a solution to the analogy “man:woman::king:??” because

p(χ | king)
p(χ | queen)

≈

p(χ | man)
p(χ | woman)

,

(4.2)

where p(χ | king) denotes the conditional probability of seeing word χ in a small window of text around
king. Relationship (4.2) is intuitive since both sides will be ≈ 1 for gender-neutral χ like “walks” or “food”,

6Note that this interpretation has been disputed; e.g., it is argued in Levy and Goldberg (2014a) that (4.1) can be understood
using only the classical connection between inner product and word similarity, using which the objective (4.1) is slightly
improved to a diﬀerent objective called 3COSMUL. However, this “explanation” is still dogged by the issue of large termwise
error pinpointed here, since inner product is only a rough approximation to word similarity. Furthermore, the experiments in
Section 5 clearly support the relations=lines interpretation.

10

will be > 1 when χ is like “he, Henry” and will be < 1 when χ is like “dress, she, Elizabeth.” This was
also observed by Levy and Goldberg (2014a). Given (4.2), they then posit that the correct model describing
word embeddings in terms of word occurrences must be a homomorphism from ((cid:60)d, +) to ((cid:60)+, ×), so vector
diﬀerences map to ratios of probabilities. This leads to the expression

pw,w(cid:48) = (cid:104)vw, vw(cid:48)(cid:105) + bw + bw(cid:48),

and their method is a (weighted) least squares ﬁt for this expression. One shortcoming of this argument
is that the homomorphism assumption assumes the linear relationships instead of explaining them from a
more basic principle. More importantly, the empirical ﬁt to the homomorphism has nontrivial approximation
error, high enough that it does not imply the desired strong linear relationships.
Levy and Goldberg (2014b) show that empirically, skip-gram vectors satisfy

(cid:104)vw, vw(cid:48)(cid:105) ≈ PMI(w, w(cid:48))

(4.3)

up to some shift. They also give an argument suggesting this relationship must be present if the solution
is allowed to be very high-dimensional. Unfortunately, that argument does not extend to low-dimensional
embeddings. Even if it did, the issue of termwise approximation error remains.

Our explanation. The current paper has introduced a generative model to theoretically explain the
emergence of relationship (4.3). However, as noted after Theorem 2.2, the issue of high approximation error
does not go away either in theory or in the empirical ﬁt. We now show that the isotropy of word vectors
(assumed in the theoretical model and veriﬁed empirically) implies that even a weak version of (4.3) is enough
to imply the emergence of the observed linear relationships in low-dimensional embeddings.

This argument will assume the analogy in question involves a relation that obeys Pennington et al.’s
suggestion in (4.2). Namely, for such a relation R there exists function νR(·) depending only upon R such
that for any a, b satisfying R there is a noise function ξa,b,R(·) for which:

p(χ | a)
p(χ | b)

= νR(χ) · ξa,b,R(χ)

For diﬀerent words χ there is huge variation in (4.4), so the multiplicative noise may be large.

Our goal is to show that the low-dimensional word embeddings have the property that there is a vector
µR such that for every pair of words a, b in that relation, va − vb = µR + noise vector, where the noise vector
is small.

Taking logarithms of (4.4) results in:

log

(cid:19)

(cid:18) p(χ | a)
p(χ | b)

= log(νR(χ)) + ζa,b,R(χ)

Theorem 2.2 implies that the left-hand side simpliﬁes to log

d (cid:104)vχ, va − vb(cid:105) + (cid:15)a,b(χ) where
(cid:15) captures the small approximation errors induced by the inexactness of Theorem 2.2. This adds yet more
noise! Denoting by V the n × d matrix whose rows are the vχ vectors, we rewrite (4.5) as:

(cid:16) p(χ|a)
p(χ|b)

(cid:17)

= 1

V (va − vb) = d log(νR) + ζ (cid:48)

a,b,R

where log(νR) in the element-wise log of vector νR and ζ (cid:48)

a,b,R = d(ζa,b,R − (cid:15)a,b,R) is the noise.

In essence, (4.6) shows that va − vb is a solution to a linear regression in d variables and m constraints,
with ζ (cid:48)
a,b,R being the “noise.” The design matrix in the regression is V , the matrix of all word vectors, which
in our model (as well as empirically) satisﬁes an isotropy condition. This makes it random-like, and thus
solving the regression by left-multiplying by V †, the pseudo-inverse of V , ought to “denoise” eﬀectively. We
now show that it does.

Our model assumed the set of all word vectors satisﬁes bulk properties similar to a set of Gaussian vectors.
The next theorem will only need the following weaker properties. (1) The smallest non-zero singular value

(4.4)

(4.5)

(4.6)

11

(a) SN

(b) GloVe

(c) CBOW

(d) skip-gram

Figure 1: The partition function Zc. The ﬁgure shows the histogram of Zc for 1000 random vectors c of
appropriate norm, as deﬁned in the text. The x-axis is normalized by the mean of the values. The values Zc
for diﬀerent c concentrate around the mean, mostly in [0.9, 1.1]. This concentration phenomenon is predicted
by our analysis.

of V is larger than some constant c1 times the quadratic mean of the singular values, namely, (cid:107)V (cid:107)F /
d.
Empirically we ﬁnd c1 ≈ 1/3 holds; see Section 5. (2) The left singular vectors behave like random vectors
with respect to ζ (cid:48)
a,b,R, for some constant c2.
(3) The max norm of a row in V is O(

a,b,R, namely, have inner product at most c2(cid:107)ζ (cid:48)
√

d). The proof is included in the appendix.

n with ζ (cid:48)

a,b,R(cid:107)/

√

√

Theorem 4.1 (Noise reduction). Under the conditions of the previous paragraph, the noise in the dimension-
reduced semantic vector space satisﬁes

(cid:107)¯ζa,b,R(cid:107)2 (cid:46) (cid:107)ζ (cid:48)

a,b,R(cid:107)2

√

d
n

.

As a corollary, the relative error in the dimension-reduced space is a factor of (cid:112)d/n smaller.

5 Experimental veriﬁcation

In this section, we provide experiments empirically supporting our generative model.

Corpus. All word embedding vectors are trained on the English Wikipedia (March 2015 dump).
It is
pre-processed by standard approach (removing non-textual elements, sentence splitting, and tokenization),
leaving about 3 billion tokens. Words that appeared less than 1000 times in the corpus are ignored, resulting
in a vocabulary of 68, 430. The co-occurrence is then computed using windows of 10 tokens to each side of
the focus word.

Training method. Our embedding vectors are trained by optimizing the SN objective using AdaGrad (Duchi
et al., 2011) with initial learning rate of 0.05 and 100 iterations. The PMI objective derived from (2.5) was
also used. SN has average (weighted) term-wise error of 5%, and PMI has 17%. We observed that SN
vectors typically ﬁt the model better and have better performance, which can be explained by larger errors
in PMI, as implied by Theorem 2.2. So, we only report the results for SN.

For comparison, GloVe and two variants of word2vec (skip-gram and CBOW) vectors are trained. GloVe’s
vectors are trained on the same co-occurrence as SN with the default parameter values.7 word2vec vectors
are trained using a window size of 10, with other parameters set to default values.8

7http://nlp.stanford.edu/projects/glove/
8https://code.google.com/p/word2vec/

12

Figure 2: The linear relationship between the squared norms of our word vectors and the logarithms of
the word frequencies. Each dot in the plot corresponds to a word, where x-axis is the natural logarithm of
the word frequency, and y-axis is the squared norm of the word vector. The Pearson correlation coeﬃcient
between the two is 0.75, indicating a signiﬁcant linear relationship, which strongly supports our mathematical
prediction, that is, equation (2.4) of Theorem 2.2.

5.1 Model veriﬁcation

Experiments were run to test our modeling assumptions. First, we tested two counter-intuitive properties:
the concentration of the partition function Zc for diﬀerent discourse vectors c (see Theorem 2.1), and the
random-like behavior of the matrix of word embeddings in terms of its singular values (see Theorem 4.1).
For comparison we also tested these properties for word2vec and GloVe vectors, though they are trained by
diﬀerent objectives. Finally, we tested the linear relation between the squared norms of our word vectors
and the logarithm of the word frequencies, as implied by Theorem 2.2.

Partition function. Our theory predicts the counter-intuitive concentration of the partition function
Zc = (cid:80)
w(cid:48) exp(c(cid:62)vw(cid:48)) for a random discourse vector c (see Lemma 2.1). This is veriﬁed empirically by
picking a uniformly random direction, of norm (cid:107)c(cid:107) = 4/µw, where µw is the average norm of the word
vectors.9 Figure 1(a) shows the histogram of Zc for 1000 such randomly chosen c’s for our vectors. The
values are concentrated, mostly in the range [0.9, 1.1] times the mean. Concentration is also observed for
other types of vectors, especially for GloVe and CBOW.

Isotropy with respect to singular values. Our theoretical explanation of relations=lines assumes
that the matrix of word vectors behaves like a random matrix with respect to the properties of singular
values. In our embeddings, the quadratic mean of the singular values is 34.3, while the minimum non-zero
singular value of our word vectors is 11. Therefore, the ratio between them is a small constant, consistent
with our model. The ratios for GloVe, CBOW, and skip-gram are 1.4, 10.1, and 3.1, respectively, which are
also small constants.

Squared norms v.s. word frequencies. Figure 2 shows a scatter plot for the squared norms of our
vectors and the logarithms of the word frequencies. A linear relationship is observed (Pearson correlation
0.75), thus supporting Theorem 2.2. The correlation is stronger for high frequency words, possibly because
the corresponding terms have higher weights in the training objective.

9Note that our model uses the inner products between the discourse vectors and word vectors, so it is invariant if the
discourse vectors are scaled by s while the word vectors are scaled by 1/s for any s > 0. Therefore, one needs to choose the
norm of c properly. We assume (cid:107)c(cid:107)µw =
d/κ ≈ 4 for a constant κ = 5 so that it gives a reasonable ﬁt to the predicted
dynamic range of word frequencies according to our theory; see model details in Section 2.

√

13

Relations
semantic
syntactic
total
adjective
noun
verb
total

SN GloVe CBOW skip-gram
0.84
0.61
0.71
0.50
0.69
0.48
0.53

0.73
0.68
0.70
0.58
0.58
0.56
0.57

0.79
0.71
0.74
0.58
0.56
0.64
0.62

0.85
0.65
0.73
0.56
0.70
0.53
0.57

G

M

Table 1: The accuracy on two word analogy task testbeds: G (the GOOGLE testbed); M (the MSR testbed).
Performance is close to the state of the art despite using a generative model with provable properties.

This correlation is much weaker for other types of word embeddings. This is possibly because they have
more free parameters (“knobs to turn”), which imbue the embeddings with other properties. This can also
cause the diﬀerence in the concentration of the partition function for the two methods.

5.2 Performance on analogy tasks

We compare the performance of our word vectors on analogy tasks, speciﬁcally the two testbeds GOOGLE
and MSR (Mikolov et al., 2013a;c). The former contains 7874 semantic questions such as “man:woman::king:??”,
and 10167 syntactic ones such as “run:runs::walk :??.” The latter has 8000 syntactic questions for adjectives,
nouns, and verbs.

To solve these tasks, we use linear algebraic queries.10 That is, ﬁrst normalize the vectors to unit norm

and then solve “a:b::c:??” by

argmin
d

(cid:107)va − vb − vc + vd(cid:107)2
2 .

(5.1)

The algorithm succeeds if the best d happens to be correct.

The performance of diﬀerent methods is presented in Table 1. Our vectors achieve performance compara-
ble to the state of art on semantic analogies (similar accuracy as GloVe, better than word2vec). On syntactic
tasks, they achieve accuracy 0.04 lower than GloVe and skip-gram, while CBOW typically outperforms the
others.11 The reason is probably that our model ignores local word order, whereas the other models capture
it to some extent. For example, a word “she” can aﬀect the context by a lot and determine if the next word
is “thinks” rather than “think ”. Incorporating such linguistic features in the model is left for future work.

5.3 Verifying relations=lines

The theory in Section 4 predicts the existence of a direction for a relation, whereas earlier Levy and Goldberg
(2014a) had questioned if this phenomenon is real. The experiment uses the analogy testbed, where each
relation is tested using 20 or more analogies. For each relation, we take the set of vectors vab = va − vb
where the word pair (a, b) satisﬁes the relation. Then calculate the top singular vectors of the matrix formed
by these vab’s, and compute the cosine similarity (i.e., normalized inner product) of individual vab to the
singular vectors. We observed that most (va − vb)’s are correlated with the ﬁrst singular vector, but have
inner products around 0 with the second singular vector. Over all relations, the average projection on the
ﬁrst singular vector is 0.51 (semantic: 0.58; syntactic: 0.46), and the average on the second singular vector
is 0.035. For example, Table 2 shows the mean similarities and standard deviations on the ﬁrst and second
singular vectors for 4 relations. Similar results are also obtained for word embedings by GloVe and word2vec.
Therefore, the ﬁrst singular vector can be taken as the direction associated with this relation, while the other
components are like random noise, in line with our model.

10One can instead use the 3COSMUL in (Levy and Goldberg, 2014a), which increases the accuracy by about 3%. But it is

not linear while our focus here is the linear algebraic structure.

11It was earlier reported that skip-gram outperforms CBOW (Mikolov et al., 2013a; Pennington et al., 2014). This may be

due to the diﬀerent training data sets and hyperparameters used.

14

relation
1st
2nd
relation
1st
2nd

1
0.65 ± 0.07
0.02 ± 0.28
8
0.56 ± 0.09
0.00 ± 0.22

2
0.61 ± 0.09
0.00 ± 0.23
9
0.53 ± 0.08
0.01 ± 0.26

3
0.52 ± 0.08
0.05 ± 0.30
10
0.37 ± 0.11
0.02 ± 0.20

4
0.54 ± 0.18
0.06 ± 0.27
11
0.72 ± 0.10
0.01 ± 0.24

5
0.60 ± 0.21
0.01 ± 0.24
12
0.37 ± 0.14
0.07 ± 0.26

6
0.35 ± 0.17
0.07 ± 0.24
13
0.40 ± 0.19
0.07 ± 0.23

7
0.42 ± 0.16
0.01 ± 0.25
14
0.43 ± 0.14
0.09 ± 0.23

Table 2: The veriﬁcation of relation directions on 2 semantic and 2 syntactic relations in the GOOGLE
testbed. Relations include cap-com: capital-common-countries; cap-wor: capital-world; adj-adv: gram1-
adjective-to-adverb; opp: gram2-opposite. For each relation, take vab = va − vb for pairs (a, b) in the
relation, and then calculate the top singular vectors of the matrix formed by these vab’s. The row with label
“1st”/“2nd” shows the cosine similarities of individual vab to the 1st/2nd singular vector (the mean and
standard deviation).

w/o RD
RD(k = 20)
RD(k = 30)
RD(k = 40)

SN GloVe CBOW skip-gram
0.71
0.74
0.79
0.76

0.70
0.75
0.80
0.77

0.73
0.77
0.80
0.80

0.74
0.79
0.82
0.80

Table 3: The accuracy of the RD algorithm (i.e., the cheater method) on the GOOGLE testbed. The RD
algorithm is described in the text. For comparison, the row “w/o RD” shows the accuracy of the old method
without using RD.

Cheating solver for analogy testbeds. The above linear structure suggests a better (but cheating) way
to solve the analogy task. This uses the fact that the same semantic relationship (e.g., masculine-feminine,
singular-plural) is tested many times in the testbed. If a relation R is represented by a direction µR then the
cheating algorithm can learn this direction (via rank 1 SVD) after seeing a few examples of the relationship.
Then use the following method of solving “a:b::c:??”: look for a word d such that vc − vd has the largest
projection on µR, the relation direction for (a, b). This can boost success rates by about 10%.

The testbed can try to combat such cheating by giving analogy questions in a random order. But the
cheating algorithm can just cluster the presented analogies to learn which of them are in the same relation.
Thus the ﬁnal algorithm, named analogy solver with relation direction (RD), is: take all vectors va − vb for
all the word pairs (a, b) presented among the analogy questions and do k-means clustering on them; for each
(a, b), estimate the relation direction by taking the ﬁrst singular vector of its cluster, and substitute that
for va − vb in (5.1) when solving the analogy. Table 3 shows the performance on GOOGLE with diﬀerent
values of k; e.g. using our SN vectors and k = 30 leads to 0.79 accuracy. Thus future designers of analogy
testbeds should remember not to test the same relationship too many times! This still leaves other ways to
cheat, such as learning the directions for interesting semantic relations from other collections of analogies.

Non-cheating solver for analogy testbeds. Now we show that even if a relationship is tested only
once in the testbed, there is a way to use the above structure. Given “a:b::c:??,” the solver ﬁrst ﬁnds the
top 300 nearest neighbors of a and those of b, and then ﬁnds among these neighbors the top k pairs (a(cid:48), b(cid:48))
so that the cosine similarities between va(cid:48) − vb(cid:48) and va − vb are largest. Finally, the solver uses these pairs
to estimate the relation direction (via rank 1 SVD), and substitute this (corrected) estimate for va − vb in
(5.1) when solving the analogy. This algorithm is named analogy solver with relation direction by nearest
neighbors (RD-nn). Table 4 shows its performance, which consistently improves over the old method by
about 3%.

15

w/o RD-nn
RD-nn (k = 10)
RD-nn (k = 20)
RD-nn (k = 30)

SN GloVe CBOW skip-gram
0.71
0.71
0.72
0.73

0.74
0.77
0.77
0.78

0.70
0.73
0.74
0.74

0.73
0.74
0.75
0.76

Table 4: The accuracy of the RD-nn algorithm on the GOOGLE testbed. The algorithm is described in the
text. For comparison, the row “w/o RD-nn” shows the accuracy of the old method without using RD-nn.

6 Conclusions

A simple generative model has been introduced to explain the classical PMI based word embedding models,
as well as recent variants involving energy-based models and matrix factorization. The model yields an
optimization objective with essentially “no knobs to turn”, yet the embeddings lead to good performance
on analogy tasks, and ﬁt other predictions of our generative model. A model with fewer knobs to turn
should be seen as a better scientiﬁc explanation (Occam’s razor), and certainly makes the embeddings more
interpretable.

The spatial isotropy of word vectors is both an assumption in our model, and also a new empirical
ﬁnding of our paper. We feel it may help with further development of language models. It is important for
explaining the success of solving analogies via low dimensional vectors (relations=lines). It also implies
that semantic relationships among words manifest themselves as special directions among word embeddings
(Section 4), which lead to a cheater algorithm for solving analogy testbeds.

Our model is tailored to capturing semantic similarity, more akin to a log-linear dynamic topic model.
In particular, local word order is unimportant. Designing similar generative models (with provable and
interpretable properties) with linguistic features is left for future work.

Acknowledgements

We thank the editors of TACL for granting a special relaxation of the page limit for our paper. We thank
Yann LeCun, Christopher D. Manning, and Sham Kakade for helpful discussions at various stages of this
work.

This work was supported in part by NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award,
Simons Collaboration Grant, and ONR-N00014-16-1-2329. Tengyu Ma was supported in addition by Simons
Award in Theoretical Computer Science and IBM PhD Fellowship.

References

Jacob Andreas and Dan Klein. When and why are log-linear models self-normalizing? In Proceedings of the
Annual Meeting of the North American Chapter of the Association for Computational Linguistics, 2014.

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model approach

to PMI-based word embeddings. Technical report, ArXiV, 2015. http://arxiv.org/abs/1502.03520.

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski.

braic structure of word senses, with applications to polysemy.
http://arxiv.org/abs/1502.03520.

Linear alge-
Technical report, ArXiV, 2016.

David Belanger and Sham M. Kakade. A linear dynamical system model for text. In Proceedings of the 32nd

International Conference on Machine Learning, 2015.

Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. Neural

probabilistic language models. In Innovations in Machine Learning. 2006.

16

Fischer Black and Myron Scholes. The pricing of options and corporate liabilities. Journal of Political

David M. Blei. Probabilistic topic models. Communication of the Association for Computing Machinery,

Economy, 1973.

2012.

David M. Blei and John D. Laﬀerty. Dynamic topic models.

In Proceedings of the 23rd International

Conference on Machine Learning, 2006.

Kenneth Ward Church and Patrick Hanks. Word association norms, mutual information, and lexicography.

Computational linguistics, 1990.

Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar. Spectral learning of latent-
variable PCFGs. In Proceedings of the 50th Annual Meeting of the Association for Computational Lin-
guistics: Long Papers-Volume 1, 2012.

Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep neural net-
works with multitask learning. In Proceedings of the 25th International Conference on Machine Learning,
2008a.

Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep neural net-
works with multitask learning. In Proceedings of the 25th International Conference on Machine Learning,
2008b.

Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman.
Indexing by latent semantic analysis. Journal of the American Society for Information Science, 1990.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic

optimization. The Journal of Machine Learning Research, 2011.

John Rupert Firth. A synopsis of linguistic theory. 1957.

Amir Globerson, Gal Chechik, Fernando Pereira, and Naftali Tishby. Euclidean embedding of co-occurrence

data. Journal of Machine Learning Research, 2007.

Tatsunori B. Hashimoto, David Alvarez-Melis, and Tommi S. Jaakkola. Word embeddings as metric recovery

in semantic spaces. Transactions of the Association for Computational Linguistics, 2016.

Thomas Hofmann. Probabilistic latent semantic analysis.

In Proceedings of the Fifteenth Conference on

Uncertainty in Artiﬁcial Intelligence, 1999.

Daniel Hsu, Sham M. Kakade, and Tong Zhang. A spectral algorithm for learning hidden markov models.

Journal of Computer and System Sciences, 2012.

Omer Levy and Yoav Goldberg. Linguistic regularities in sparse and explicit word representations.

In

Proceedings of the Eighteenth Conference on Computational Natural Language Learning, 2014a.

Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in

Neural Information Processing Systems, 2014b.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Compu-
tational Linguistics, 2011.

Yariv Maron, Michael Lamar, and Elie Bienenstock. Sphere embedding: An application to part-of-speech

induction. In Advances in Neural Information Processing Systems, 2010.

17

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey Dean. Eﬃcient estimation of word representations in

vector space. Proceedings of the International Conference on Learning Representations, 2013a.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeﬀ Dean. Distributed representations of
words and phrases and their compositionality. In Advances in Neural Information Processing Systems,
2013b.

Tomas Mikolov, Wen-tau Yih, and Geoﬀrey Zweig. Linguistic regularities in continuous space word rep-
In Proceedings of the Conference of the North American Chapter of the Association for

resentations.
Computational Linguistics: Human Language Technologies, 2013c.

Andriy Mnih and Geoﬀrey Hinton. Three new graphical models for statistical language modelling.

In

Proceedings of the 24th International Conference on Machine Learning, 2007.

Christos H. Papadimitriou, Hisao Tamaki, Prabhakar Raghavan, and Santosh Vempala. Latent semantic
indexing: A probabilistic analysis. In Proceedings of the 7th ACM SIGACT-SIGMOD-SIGART Symposium
on Principles of Database Systems, 1998.

Jeﬀrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word represen-

tation. Proceedings of the Empiricial Methods in Natural Language Processing, 2014.

Douglas L. T. Rohde, Laura M. Gonnerman, and David C. Plaut. An improved model of semantic similarity

based on lexical co-occurence. Communication of the Association for Computing Machinery, 2006.

David E. Rumelhart, Geoﬀrey E. Hinton, and James L. McClelland, editors. Parallel Distributed Processing:

Explorations in the Microstructure of Cognition. 1986.

David E. Rumelhart, Geoﬀrey E. Hinton, and Ronald J. Williams. Learning representations by back-

propagating errors. Cognitive modeling, 1988.

Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. Journal

of Artiﬁcial Intelligence Research, 2010.

18

A Proofs of Theorem 1

In this section we prove Theorem 2.2 and Lemma 2.1 (restated below).

Theorem 2.2. Suppose the word vectors satisfy equation (2.2), and window size q = 2. Then,

for (cid:15) = O((cid:15)z) + (cid:101)O(1/d) + O((cid:15)2). Jointly these imply:

log p(w, w(cid:48)) =

− 2 log Z ± (cid:15),

(cid:107)vw + vw(cid:48)(cid:107)2
2
2d
(cid:107)vw(cid:107)2
2
2d

log p(w) =

− log Z ± (cid:15).

PMI (w, w(cid:48)) =

(cid:104)vw, vw(cid:48)(cid:105)
d

± O((cid:15)).

Lemma 2.1. If the word vectors satisfy the bayesian prior v = s · ˆv, where ˆv is from the spherical Gaussian
distribution, and s is a scalar random variable, then with high probability the entire ensemble of word vectors
satisﬁes that

for (cid:15)z = (cid:101)O(1/

n), and δ = exp(−Ω(log2 n)).

√

Pr
c∼C

[(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z] ≥ 1 − δ,

We ﬁrst prove Theorem 2.2 using Lemma 2.1, and Lemma 2.1 will be proved in Section A.1. Please see
Section 2 of the main paper for the intuition of the proof and a cleaner sketch without too many technicalities.

Proof of Theorem 2.2. Let c be the hidden discourse that determines the probability of word w, and c(cid:48) be
the next one that determines w(cid:48). We use p(c(cid:48)|c) to denote the Markov kernel (transition matrix) of the
Markov chain. Let C be the stationary distribution of discourse vector c, and D be the joint distribution of
(c, c(cid:48)). We marginalize over the contexts c, c(cid:48) and then use the independence of w, w(cid:48) conditioned on c, c(cid:48),

(cid:20) exp((cid:104)vw, c(cid:105))
Zc
We ﬁrst get rid of the partition function Zc using Lemma 2.1. As sketched in the main paper, essentially
we will replace Zc by Z in equation (A.5), though a very careful control of the approximation error is
required. Formally, Let F1 be the event that c satisﬁes

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

p(w, w(cid:48)) = E

(A.5)

(c,c(cid:48))∼D

(cid:21)

(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z .

(A.6)

Similarly, let F2 be the even that c(cid:48) satisﬁes (1 − (cid:15)z)Z ≤ Zc(cid:48) ≤ (1 + (cid:15)z)Z, and let F = F1 ∩ F2, and F be its
negation. Moreover, let 1F be the indicator function for the event F. Therefore by Lemma 2.1 and union
bound, we have E[1F ] = Pr[F] ≥ 1 − exp(−Ω(log2 n)).

We ﬁrst decompose the integral (A.5) into the two parts according to whether event F happens,

We bound the ﬁrst quantity on the right hand side using (2.2) and the deﬁnition of F.

p(w, w(cid:48)) = E

(c,c(cid:48))∼D

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

+ E

(c,c(cid:48))∼D

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:20)

(cid:20)

1
ZcZc(cid:48)
1
ZcZc(cid:48)

(cid:21)

(cid:21)

(cid:20)

E
(c,c(cid:48))∼D

1
ZcZc(cid:48)
≤ (1 + (cid:15)z)2 1
Z 2

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:21)

E
(c,c(cid:48))∼D

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ]

19

(A.1)

(A.2)

(A.3)

(A.4)

(A.7)

(A.8)

For the second quantity of the right hand side of (A.7), we have by Cauchy-Schwartz,

1
ZcZc(cid:48)
(cid:20) 1
Z 2
c

(cid:18)

(cid:20)

E
(c,c(cid:48))∼D
(cid:18)

≤

≤

(cid:18)

E
(c,c(cid:48))∼D
(cid:20) 1
Z 2
c

E
c

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:21)(cid:19)2

exp((cid:104)vw, c(cid:105))21F

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

[1F ]

(cid:21)(cid:19) (cid:18)

(cid:21)(cid:19) (cid:18)

E
(c,c(cid:48))∼D
(cid:20) 1
Z 2
c(cid:48)

E
c(cid:48)

(cid:20)(cid:90)

1
Z 2
c(cid:48)

c,c(cid:48)

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))21F

(cid:21)(cid:19)

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))2 E
c|c(cid:48)

[1F ]

(cid:21)(cid:19)

.

(A.9)

Using the fact that Zc ≥ 1, then we have that

(cid:20) 1
Z 2
c

E
c

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

(cid:21)
[1F ]

(cid:20)
exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

(cid:21)
[1F ]

≤ E
c

We can split that expectation as
(cid:20)
exp((cid:104)vw, c(cid:105))21(cid:104)vw,c(cid:105)>0 E
c(cid:48)|c

E
c

The second term of (A.10) is upper bounded by

(cid:21)
[1F ]

(cid:20)
exp((cid:104)vw, c(cid:105))21(cid:104)vw,c(cid:105)<0 E
c(cid:48)|c

(cid:21)
[1F ]

.

+ E
c

(A.10)

We proceed to the ﬁrst term of (A.10) and observe the following property of it:

(cid:20)
exp((cid:104)vw, c(cid:105))21(cid:104)vw,c(cid:105)>0 E
c(cid:48)|c

E
c

(cid:21)

(cid:20)

[1F ]

≤ E
c

exp((cid:104)αvw, c(cid:105))21(cid:104)vw,c(cid:105)>0 E
c(cid:48)|c

[1F ]

≤ E
c

(cid:21)

(cid:20)
exp((cid:104)αvw, c(cid:105))2 E
c(cid:48)|c

(cid:21)

[1F ]

where α > 1. Therefore, it’s suﬃcient to bound

[1F ] ≤ exp(−Ω(log2 n))

E
c,c(cid:48)

(cid:20)

E
c

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

[1F ]

(cid:21)

when (cid:107)vw(cid:107) = Ω(

d).

√

Let’s denote by z the random variable 2 (cid:104)vw, c(cid:105).
Let’s denote r(z) = Ec(cid:48)|z[1F ] which is a function of z between [0, 1]. We wish to upper bound Ec [exp(z)r(z)].

The worst-case r(z) can be quantiﬁed using a continuous version of Abel’s inequality as proven in Lemma A.4,
which gives

[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3)
E
c

(A.11)

where t satisﬁes that Ec[1[t,+∞]] = Pr[z ≥ t] = Ec[r(z)] ≤ exp(−Ω(log2 n)). Then, we claim Pr[z ≥ t] ≤
exp(−Ω(log2 n)) implies that t ≥ Ω(log.9 n).

If c were distributed as N (0, 1

d I), this would be a simple tail bound. However, as c is distributed uniformly
on the sphere, this requires special care, and the claim follows by applying Lemma A.1 instead. Finally,
applying Corollary A.3, we have:

E[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(log1.8 n))
We have the same bound for c(cid:48) as well. Hence, for the second quantity of the right hand side of (A.7),

(A.12)

we have

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:21)

(cid:20)

1
ZcZc(cid:48)

E
(c,c(cid:48))∼D
(cid:18)

≤

E
c

(cid:20) 1
Z 2
c
≤ exp(−Ω(log1.8 n))

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

(cid:21)(cid:19)1/2 (cid:18)

[1F ]

(cid:20) 1
Z 2
c(cid:48)

E
c(cid:48)

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))2 E
c|c(cid:48)

[1F ]

(cid:21)(cid:19)1/2

(A.13)

20

where the ﬁrst inequality follows from Cauchy-Schwartz, and the second from the calculation above. Com-
bining (A.7), (A.8) and (A.13), we obtain

p(w, w(cid:48)) ≤ (1 + (cid:15)z)2 1
Z 2
≤ (1 + (cid:15)z)2 1
Z 2

E
(c,c(cid:48))∼D

E
(c,c(cid:48))∼D

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ] +

1
n2 exp(−Ω(log1.8 n))

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] + δ0

where δ0 = exp(−Ω(log1.8 n))Z 2 ≤ exp(−Ω(log1.8 n)) by the fact that Z ≤ exp(2κ)n = O(n). Note that
κ is treated as an absolute constant throughout the paper. On the other hand, we can lowerbound similarly

p(w, w(cid:48)) ≥ (1 − (cid:15)z)2 1
Z 2
≥ (1 − (cid:15)z)2 1
Z 2
≥ (1 − (cid:15)z)2 1
Z 2

E
(c,c(cid:48))∼D

E
(c,c(cid:48))∼D

E
(c,c(cid:48))∼D

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ]

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] −

1
n2 exp(−Ω(log1.8 n))

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] − δ0

Taking logarithm, the multiplicative error translates to a additive error

log p(w, w(cid:48)) = log

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

(cid:18)

E
(c,c(cid:48))∼D

For the purpose of exploiting the fact that c, c(cid:48) should be close to each other, we further rewrite log p(w, w(cid:48))
by re-organizing the expectations above,

log p(w, w(cid:48)) = log

± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

= log

[exp((cid:104)vw, c(cid:105))A(c)] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

(A.14)

(cid:20)
exp((cid:104)vw, c(cid:105)) E
c(cid:48)|c

(cid:21)
[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

(cid:19)

(cid:18)

(cid:18)

E
c

E
c

A(c) := E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

where the inner integral which is denoted by A(c),

Since (cid:107)vw(cid:107) ≤ κ

d. Therefore we have that (cid:104)vw, c − c(cid:48)(cid:105) ≤ (cid:107)vw(cid:107)(cid:107)c − c(cid:48)(cid:107) ≤ κ

d(cid:107)c − c(cid:48)(cid:107).

√

√

Then we can bound A(c) by

(cid:19)

(cid:19)

A(c) = E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

= exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

≤ exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48) − c(cid:105))]

√

[exp(κ

d(cid:107)c − c(cid:48)(cid:107))]

≤ (1 + (cid:15)2) exp((cid:104)vw(cid:48), c(cid:105))

21

where the last inequality follows from our model assumptions. To derive a lower bound of A(c), observe that

√

[exp(κ

E
c(cid:48)|c

d(cid:107)c − c(cid:48)(cid:107))] + E
c(cid:48)|c

√

[exp(−κ

d(cid:107)c − c(cid:48)(cid:107))] ≥ 2

Therefore, our model assumptions imply that

Hence,

√

[exp(−κ

d(cid:107)c − c(cid:48)(cid:107))] ≥ 1 − (cid:15)2

E
c(cid:48)|c

A(c) = exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

exp((cid:104)vw(cid:48), c(cid:48) − c(cid:105))
√

exp(−κ

d(cid:107)c − c(cid:48)(cid:107))

≥ exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

≥ (1 − (cid:15)2) exp((cid:104)vw(cid:48), c(cid:105))

Therefore, we obtain that A(c) = (1 ± (cid:15)2) exp((cid:104)vw(cid:48), c(cid:105)). Plugging the just obtained estimate of A(c) into

the equation (A.14), we get that

(cid:18)

(cid:18)

(cid:18)

E
c

E
c

E
c

E
c

log p(w, w(cid:48)) = log

[exp((cid:104)vw, c(cid:105))A(c)] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

= log

[(1 ± (cid:15)2) exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:105))] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

(cid:19)

= log

[exp((cid:104)vw + vw(cid:48), c(cid:105))] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z) + log(1 ± (cid:15)2)

(A.15)

Now it suﬃces to compute Ec[exp((cid:104)vw + vw(cid:48), c(cid:105))]. Note that if c had the distribution N (0, 1

d I), which
is very similar to uniform distribution over the sphere, then we could get straightforwardly Ec[exp((cid:104)vw +
vw(cid:48), c(cid:105))] = exp((cid:107)vw + vw(cid:48)(cid:107)2/(2d)). For c having a uniform distribution over the sphere, by Lemma A.5, the
same equality holds approximately,

[exp((cid:104)vw + vw(cid:48), c(cid:105))] = (1 ± (cid:15)3) exp((cid:107)vw + vw(cid:48)(cid:107)2/(2d))

(A.16)

where (cid:15)3 = (cid:101)O(1/d).

Plugging in equation (A.16) into equation (A.15), we have that
log p(w, w(cid:48)) = log (cid:0)(1 ± (cid:15)3) exp((cid:107)vw + vw(cid:48)(cid:107)2/(2d)) ± δ0

(cid:1) − 2 log Z + 2 log(1 ± (cid:15)z) + log(1 ± (cid:15)2)

= (cid:107)vw + vw(cid:48)(cid:107)2/(2d) + O((cid:15)3) + O(δ(cid:48)

0) − 2 log Z ± 2(cid:15)z ± (cid:15)2

where δ(cid:48)

0 = δ0 · (Ec∼C[exp((cid:104)vw + vw(cid:48), c(cid:105))])−1 = exp(−Ω(log1.8 n)). Note that (cid:15)3 = (cid:101)O(1/d), (cid:15)z = (cid:101)O(1/

√

n),

and (cid:15)2 by assumption, therefore we obtain that

log p(w, w(cid:48)) =

(cid:107)vw + vw(cid:48)(cid:107)2 − 2 log Z ± O((cid:15)z) + O((cid:15)2) + (cid:101)O(1/d).

1
2d

The following lemmas are helper lemmas that were used in the proof above. We use Cd to denote the

uniform distribution over the unit sphere in Rd.

Lemma A.1 (Tail bound for spherical distribution). If c ∼ Cd, v ∈ Rd is a vector with (cid:107)v(cid:107) = Ω(
t = ω(1), the random variable z = (cid:104)v, c(cid:105) satisﬁes Pr[z ≥ t] = e−O(t2).

√

d) and

(cid:19)

(cid:19)

22

Proof. If c = (c1, c2, . . . , cd) ∼ Cd, c is in distribution equal to
samples from a univariate Gaussian with mean 0 and variance 1
that v = ((cid:107)v(cid:107), 0, . . . , 0). Let’s introduce the random variable r = (cid:80)d

(cid:16) ˜c1
(cid:107)˜c(cid:107) , ˜c2

(cid:107)˜c(cid:107) , . . . , ˜cd

where the ˜ci are i.i.d.
d . By spherical symmetry, we may assume

(cid:107)˜c(cid:107)

(cid:17)

Pr [(cid:104)v, c(cid:105) ≥ t] = Pr

(cid:107)v|

≥ t

≤ Pr

≥ t | r ≥

Pr

r ≥

+ Pr

≥ t | r ≥

Pr

r ≥

(cid:20)

(cid:21)

˜c1
(cid:107)˜c(cid:107)

(cid:20) (cid:107)v(cid:107)˜c1
(cid:107)˜c(cid:107)

(cid:20)

(cid:21)

1
2

(cid:20)

(cid:21)

1
2

(cid:21)

1
2

it’s suﬃcient to lower bound Pr [r ≤ 100] and Pr

. The former probability is easily

seen to be lower bounded by a constant by a Chernoﬀ bound. Consider the latter one next. It holds that

(cid:104)

(cid:107)v(cid:107) ˜c1

(cid:107)c(cid:107) ≥ t | r ≤ 100

(cid:105)

i=2 ˜c2
(cid:21)

1
2

i . Since

(cid:20) (cid:107)v(cid:107)˜c1
(cid:107)˜c(cid:107)

Pr

(cid:107)v(cid:107)

≥ t | r ≤ 100

= Pr

˜c1 ≥

(cid:21)

(cid:34)

(cid:115)

t2 · r

(cid:107)v(cid:107)2 − t2 | r ≤ 100

(cid:35)

(cid:34)

(cid:115)

≥ Pr

˜c1 ≥

(cid:35)

100t2
(cid:107)v(cid:107)2 − t2

(cid:20)

˜c1
(cid:107)˜c(cid:107)

Denoting ˜t =

(cid:113) 100t2

(cid:107)v(cid:107)2−t2 , by a well-known Gaussian tail bound it follows that

Pr (cid:2)˜c1 ≥ ˜t(cid:3) = e−O(d˜t2)

(cid:32)

1
√
d˜t

(cid:18) 1
√

−

d˜t

(cid:19)3(cid:33)

= e−O(t2)

where the last equality holds since (cid:107)v(cid:107) = Ω(

d) and t = ω(1).

√

√

Lemma A.2. If c ∼ Cd, v ∈ Rd is a vector with (cid:107)v(cid:107) = Θ(
satisﬁes

d) and t = ω(1), the random variable z = (cid:104)v, c(cid:105)

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(t2)) + exp(−Ω(d))

Proof. Similarly as in Lemma A.1, if c = (c1, c2, . . . , cd) ∼ Cd, c is in distribution equal to
where the ˜ci are i.i.d. samples from a univariate Gaussian with mean 0 and variance 1
symmetry, we may assume v = ((cid:107)v(cid:107), 0, . . . , 0). Let’s introduce the random variable r = (cid:80)d
an arbitrary u > 1, some algebraic manipulation shows

(cid:16) ˜c1
(cid:107)˜c(cid:107) , ˜c2

(cid:107)˜c(cid:107) , . . . , ˜cd

(cid:107)˜c(cid:107)

(cid:17)

d . Again, by spherical
i . Then, for

i=2 ˜c2

Pr (cid:2)exp ((cid:104)v, c(cid:105)) 1[t,+∞]((cid:104)v, c(cid:105)) ≥ u(cid:3) = Pr [exp ((cid:104)v, c(cid:105)) ≥ u ∧ (cid:104)v, c(cid:105) ≥ t] =

(cid:20)

(cid:18)

Pr

exp

(cid:107)v(cid:107)

(cid:19)

˜c1
(cid:107)˜c(cid:107)

˜c1
(cid:107)˜c(cid:107)

≥ u ∧ (cid:107)v(cid:107)

≥ u

= Pr

˜c1 = max

(cid:21)

(cid:34)

(cid:32)(cid:115)

(cid:115)

(cid:33)(cid:35)

˜u2r
(cid:107)v(cid:107)2 − ˜u2 ,

t2r
(cid:107)v(cid:107)2 − t2

(A.17)

√

where we denote ˜u = log u. Since ˜c1 is a mean 0 univariate Gaussian with variance 1
have ∀x ∈ R

d , and (cid:107)v(cid:107) = Ω(

d) we

(cid:34)

(cid:115)

Pr

˜c1 ≥

(cid:35)

x2r
(cid:107)v(cid:107)2 − u2

(cid:16)

e−Ω(x2r)(cid:17)

= O

Next, we show that r is lower bounded by a constant with probability 1 − exp(−Ω(d)).
distribution equal to 1

Indeed, r is in
k is a Chi-squared distribution with k degrees of freedom. Standard
d ] ≤ exp(−ξ). Taking ξ = αd for α a constant

concentration bounds (?) imply that ∀ξ ≥ 0, Pr[r − 1 ≤ −2
implies that with probability 1 − exp(−Ω(d)), r ≥ M for some constant M . We can now rewrite

d−1, where χ2

d χ2

(cid:113) ξ

(cid:34)

(cid:115)

Pr

˜c1 ≥

x2r
(cid:107)v(cid:107)2 − x2

(cid:35)

=

23

(cid:34)

(cid:115)

Pr

˜c1 ≥

x2r

(cid:107)v(cid:107)2 − x2 | r ≥ M

(cid:35)

Pr[r ≥ M ] + Pr

˜c1 ≥

(cid:34)

(cid:115)

x2r

(cid:107)v(cid:107)2 − x2 | r ≤ M

(cid:35)

Pr[r ≤ M ]

The ﬁrst term is clearly bounded by e−Ω(x2) and the second by exp(−Ω(d)). Therefore,

(cid:34)

(cid:115)

Pr

˜c1 ≥

(cid:35)

x2r
(cid:107)v(cid:107)2 − x2

= O (cid:0)max (cid:0)exp (cid:0)−Ω (cid:0)x2(cid:1)(cid:1) , exp (−Ω (d))(cid:1)(cid:1)

(A.18)

Putting A.17 and A.18 together, we get that

Pr (cid:2)exp ((cid:104)v, c(cid:105)) 1[t,+∞]((cid:104)v, c(cid:105)) ≥ u(cid:3) = O

(cid:16)

(cid:16)

(cid:16)

(cid:16)

max

exp

−Ω

min

(cid:16)

d, (max (˜u, t))2(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)

(A.19)

(where again, we denote ˜u = log u)

For any random variable X which has non-negative support, it’s easy to check that

E[X] =

Pr[X ≥ x]dx

(cid:90) ∞

0

Hence,

E (cid:2)exp(z)1[t,+∞](z)(cid:3) =

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du =

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du

(cid:90) ∞

0

(cid:90) exp((cid:107)v(cid:107))

0

To bound this integral, we split into the following two cases:

• Case t2 ≥ d: max (˜u, t) ≥ t, so min

(cid:16)

d, (max (˜u, t))2(cid:17)

= d. Hence, A.19 implies

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp((cid:107)v(cid:107)) exp(−Ω(d)) = exp(−Ω(d))

where the last inequality follows since (cid:107)v(cid:107) = O(

d).

√

• Case t2 < d: In the second case, we will split the integral into two portions: u ∈ [0, exp(t)] and

u ∈ [exp(t), exp((cid:107)v(cid:107))].
When u ∈ [0, exp(t)], max (˜u, t) = t, so min(d, (max (˜u, t))2) = t2. Hence,

(cid:90) exp(t)

0

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du ≤ exp(t) exp(−Ω(t2)) = − exp(Ω(t2))

When u ∈ [exp(t), exp((cid:107)v(cid:107))], max (˜u, t) = ˜u. But ˜u ≤ log(exp((cid:107)v(cid:107))) = O(
˜u. Hence,

(cid:90) exp((cid:107)v(cid:107))

exp(t)

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du ≤

exp(−(log(u))2)du

(cid:90) exp((cid:107)v(cid:107))

exp(t)

Making the change of variable ˜u = log(u), the we can rewrite the last integral as

√

d), so min(d, (max (˜u, t))2) =

(cid:90) (cid:107)v(cid:107)

t

exp(−˜u2) exp(˜u)d˜u = O(exp(−t2))

where the last inequality is the usual Gaussian tail bound.

In either case, we get that

(cid:90) exp((cid:107)v(cid:107))

0

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du = exp(−Ω(t2)) + exp(−Ω(d)))

which is what we want.

24

As a corollary to the above lemma, we get the following:

Corollary A.3. If c ∼ Cd, v ∈ Rd is a vector with (cid:107)v(cid:107) = Θ(

d) and t = Ω(log.9 n) then

√

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(log1.8 n))

Proof. We claim the proof is trivial if d = o(log4 n).
exp(O(

d)). Hence,

√

Indeed,

in this case, exp((cid:104)v, c(cid:105)) ≤ exp((cid:107)v(cid:107)) =

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(O(

d)) E[1[t,+∞](z)] = exp(O(

d)) Pr[z ≥ t]

√

Since by Lemma A.1, Pr[z ≥ t] ≤ exp(−Ω(log2 n), we get

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(O(

d) − Ω(log2 n)) = exp(−Ω(log1.8 n))

as we wanted.

So, we may, without loss of generality assume that d = Ω(log4 n). In this case, Lemma A.2 implies

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(− log1.8 n) + exp(−Ω(d))) = exp(− log1.8 n)

where the last inequality holds because d = Ω(log4 n) and t2 = Ω(log.9 n), so we get the claim we wanted.

√

√

Lemma A.4 (Continuous Abel’s Inequality). Let 0 ≤ r(x) ≤ 1 be a function such that such that E[r(x)] = ρ.
Moreover, suppose increasing function u(x) satisﬁes that E[|u(x)|] < ∞. Let t be the real number such that
E[1[t,+∞]] = ρ. Then we have

E[u(x)r(x)] ≤ E[u(x)1[t,+∞]]

(A.20)

z f (x)r(x)dx, and H(z) = (cid:82) ∞

Proof. Let G(z) = (cid:82) ∞
z f (x)1[t,+∞](x)dx. Then we have that G(z) ≤ H(z) for
all z. Indeed, for z ≥ t, this is trivial since r(z) ≤ 1. For z ≤ t, we have H(z) = E[1[t,+∞]] = ρ = E[r(x)] ≥
(cid:82) ∞
z f (x)r(x)dx. Then by integration by parts we have,
(cid:90) ∞

(cid:90) ∞

u(x)f (x)r(x)dx = −

u(x)dG

−∞

−∞

= −u(x)G(x) |∞

−∞ +

G(x)u(cid:48)(x)dx

(cid:90) +∞

−∞

≤

=

(cid:90) +∞

−∞
(cid:90) ∞

−∞

H(x)u(cid:48)(x)dx

u(x)f (x)1[t,+∞](x)dx,

where at the third line we use the fact that u(x)G(x) → 0 as x → ∞ and that u(cid:48)(x) ≥ 0, and at the last line
we integrate by parts again.

Lemma A.5. Let v ∈ Rd be a ﬁxed vector with norm (cid:107)v(cid:107) ≤ κ
variable c with uniform distribution over the sphere, we have that

√

d for absolute constant κ. Then for random

log E[exp((cid:104)v, d(cid:105))] = (cid:107)v(cid:107)2/2d ± (cid:15)

(A.21)

where (cid:15) = (cid:101)O( 1

d ).

25

Proof. Let g ∈ N (0, I), then g/(cid:107)g(cid:107) has the same distribution as c. Let r = (cid:107)v(cid:107). Since c is spherically
symmetric, we could, we can assume without loss of generality that v = (r, 0, . . . , 0). Let x = g1 and
y = (cid:112)g2
d. Therefore x ∈ N (0, 1) and y2 has χ2 distribution with mean d − 1 and variance O(d).
d. Note that the Pr[F] ≥ 1−exp(−Ω(log1.8(d)).

Let F be the event that x ≤ 20 log d and 1.5

2 + · · · + g2

d ≥ y ≥ 0.5

√

√

By Proposition 2, we have that E[exp((cid:104)v, c(cid:105))] = E[exp((cid:104)v, c(cid:105)) | F] · (1 ± Ω(− log1.8 d)).

Conditioned on event F, we have

E[exp((cid:104)v, c(cid:105)) | F] = E

exp(

(cid:35)

rx
(cid:112)x2 + y2

) | F

(cid:34)

(cid:34)

(cid:34)

(cid:20)

= E

exp(

−

rx3
y(cid:112)x2 + y2(y + (cid:112)x2 + y2)

) | F

(cid:35)

= E

exp(

) · exp(

rx3
y(cid:112)x2 + y2(y + (cid:112)x2 + y2)

) | F

(cid:35)

= E

exp(

) | F

· (1 ± O(

(cid:21)

))

log3 d
d
√

√

rx
y

rx
y

rx
y

where we used the fact that r ≤ κ
we have that

√

d. Let E be the event that 1.5

d ≥ y ≥ 0.5

d. By using Proposition 1,

E[exp(rx/y) | F] = E[exp(rx/y) | E] ± exp(−Ω(log2(d))

(A.23)

Then let z = y2/(d − 1) and w = z − 1. Therefore z has χ2 distribution with mean 1 and variance 1/(d − 1),
and w has mean 0 and variance 1/(d − 1).

(A.22)

E

exp(

) | E

= E[E[exp(rx/y) | y] | E] = E[exp(r2/y2) | E]

(cid:20)

(cid:21)

rx
y

= E[exp(r2/(d − 1) · 1/z2) | E]

= E[exp(r2/(d − 1) · (1 +

= exp(r2/(d − 1)) E[exp(1 +

2w + w2
(1 + w)2 )) | E]
2w + w2
(1 + w)2 )) | E]

= exp(r2/(d − 1)) E[1 + 2w ± O(w2) | E]
= exp(r2/(d − 1)2)(1 ± 1/d)

where the second-to-last line uses the fact that conditioned on 1/2 ≥ E, w ≥ −1/2 and therefore the Taylor
expansion approximates the exponential accurately, and the last line uses the fact that | E[w | E]| = O(1/d)
and E[w2 | E] ≤ O(1/d). Combining the series of approximations above completes the proof.

We ﬁnally provide the proofs for a few helper propositions on conditional probabilities for high probability

events used in the lemma above.

Proposition 1. Suppose x ∼ N (0, σ2) with σ = O(1). Then for any event E with Pr[E] = 1−O(−Ω(log2 d)),
we have that E[exp(x)] = E[exp(x) | E] ± exp(−Ω(log2(d)).

Proof. Let’s denote by ¯E the complement of the event E. We will consider the upper and lower bound
separately. Since

E[exp(x)] = E[exp(x) | E] Pr[E] + E[exp(x) | ¯E] Pr[ ¯E]

we have that

E[exp(x)] ≤ E[exp(x) | E] + E[exp(x) | ¯E] Pr[ ¯E]

(A.24)

26

and

E[exp(x)] ≥ E[exp(x) | E](1 − exp(−Ω(log2 d))) ≥ E[exp(x) | E] − E[exp(x) | E] exp(−Ω(log2 d)))

(A.25)

Consider the upper bound (A.24) ﬁrst. To show the statement of the lemma, it suﬃces to bound

E[exp(x) | ¯E] Pr[ ¯E].

Working towards that, notice that

E[exp(x) | ¯E] Pr[ ¯E] = E[exp(x)1 ¯E ] = E[exp(x) E[1 ¯E |x]] = E[exp(x)r(x)]

if we denote r(x) = E[1 ¯E |x]. We wish to upper bound E[exp(x)r(x)]. By Lemma A.4, we have

E[exp(x)r(x)] ≤ E[exp(x)1[t,∞]]

where t is such that E[1[t,∞]] = E[r(x)]. However, since E[r(x)] = Pr[ ¯E] = exp(−Ω(log2 d)), it must be the
case that t = Ω(log d) by the standard Gaussian tail bound, and the assumption that σ = O(1). In turn,
this means

E[exp(x)1[t,∞]] ≤

(cid:90) ∞

1
√
2π

σ

t

exe− x2

σ2 dx =

(cid:90) ∞

1
√
2π

σ

t

e−( x

σ − σ

2 )2+ σ2

4 dx = e

σ2
4

e−(x(cid:48)− σ

2 )2

dx(cid:48)

1
√
2π

(cid:90) +∞

t/σ

where the last equality follows from the change of variables x = σx(cid:48). However,

1
√
2π

(cid:90) +∞

t/σ

e−(x(cid:48)− σ

2 )2

dx(cid:48)

is nothing more than Pr[x(cid:48) > t
1. Bearing in mind that σ = O(1)

σ ], where x(cid:48) is distributed like a univariate gaussian with mean σ

2 and variance

σ2
4

e

1
√
2π

(cid:90) +∞

t/σ

e−(x(cid:48)− σ

2 )2

dx(cid:48) = exp(−Ω(t2)) = exp(−Ω(log2 d))

by the usual Gaussian tail bounds, which proves the lower bound we need.

We proceed to consider the lower bound A.25. To show the statement of the lemma, we will bound

E[exp(x) | E]. Notice trivially that since exp(x) ≥ 0,

E[exp(x) | E] ≤

E[exp(x)]
Pr[E]

Since Pr[E] ≥ 1 − exp(Ω(log2 d)),

1

Pr[E] ≤ 1 + exp(O(log2)). So, it suﬃces to bound E[exp(x)]. However,

E[exp(x)] =

1
√
2π

σ

(cid:90) +∞

t=−∞

exe− x2

σ2 dx =

1
√
2π

σ

(cid:90) +∞

t=−∞

e−( x

σ − σ

2 )2+ σ2

4 dx =

e−(x(cid:48)− σ

2 )2+ σ2

4 dx(cid:48)

1
√
2π

(cid:90) +∞

t=−∞

where the last equality follows from the same change of variables x = σx(cid:48) as before. Since (cid:82) +∞
√

t=−∞ e−(x(cid:48)− σ

2 )2

dx(cid:48) =

2π, we get

(cid:90) +∞

1
√
2π

e−(x(cid:48)− σ

2 )2+ σ2

4 dx(cid:48) = e

σ2
4 = O(1)

t=−∞
Pr[E] , we get that E[exp(x) | E] = O(1). Plugging this back in A.25,

1

Putting together with the estimate of
we get the desired upper bound.

Proposition 2. Suppose c ∼ C and v is an arbitrary vector with (cid:107)v(cid:107) = O(
Pr[E] ≥ 1 − exp(−Ω(log2 d)), we have that E[exp((cid:104)v, c(cid:105))] = E[exp((cid:104)v, c(cid:105)) | E] ± exp(− log1.8 d).

d). Then for any event E with

√

27

Proof of Proposition 2. Let z = (cid:104)v, c(cid:105). We proceed similarly as in the proof of Proposition 1. We have

and

and

E[exp(z)] = E[exp(z) | E] Pr[E] + E[exp(z) | ¯E] Pr[ ¯E]

E[exp(z)] ≤ E[exp(z) | E] + E[exp(z) | ¯E] Pr[ ¯E]

E[exp(z)] ≥ E[exp(z) | E] Pr[E] = E[exp(z) | E] − E[exp(z) | E] exp(−Ω(log2 d))

We again proceed by separating the upper and lower bound.

We ﬁrst consider the upper bound A.26.
Notice that that

E[exp(z) | ¯E] Pr[ ¯E] = E[exp(z)1 ¯E ]

We can split the last expression as

E (cid:2)exp((cid:104)vw, c(cid:105))1(cid:104)vw,c(cid:105)>01E

(cid:3) + E (cid:2)exp((cid:104)vw, c(cid:105))1(cid:104)vw,c(cid:105)<01E

(cid:3) .

The second term is upper bounded by

E[1E ] ≤ exp(−Ω(log2 n))

We proceed to the ﬁrst term of (A.10) and observe the following property of it:

E (cid:2)exp((cid:104)vw, c(cid:105))1(cid:104)vw,c(cid:105)>01E

(cid:3) ≤ E (cid:2)exp((cid:104)αvw, c(cid:105))1(cid:104)vw,c(cid:105)>0 1E

(cid:3) ≤ E [exp((cid:104)αvw, c(cid:105))1E ]

(A.26)

(A.27)

where α > 1. Therefore, it’s suﬃcient to bound

√

when (cid:107)vw(cid:107) = Θ(

d). Let’s denote r(z) = E[1 ¯E |z].

Using Lemma A.4, we have that

E [exp(z)1E ]

[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3)
E
c

(A.28)

where t satisﬁes that Ec[1[t,+∞]] = Pr[z ≥ t] = Ec[r(z)] ≤ exp(−Ω(log2 d)). Then, we claim Pr[z ≥ t] ≤
exp(−Ω(log2 d)) implies that t ≥ Ω(log.9 d).

Indeed, this follows by directly applying Lemma A.1. Afterward, applying Lemma A.2, we have:

E[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(log1.8 d))

(A.29)

which proves the upper bound we want.

We now proceed to the lower bound A.27, which is again similar to the lower bound in the proof of

Proposition 1: we just need to bound E[exp(z) | E]. Same as in Proposition 1, since exp(x) ≥ 0,

E[exp(z) | E] ≤

E[exp(z)]
Pr[E]

Consider the event E (cid:48) : z ≤ t, for t = Θ(log.9 d), which by Lemma A.1 satisﬁes Pr[E (cid:48)] ≥ 1 − exp(−Ω(log2 d)).
By the upper bound we just showed,

E[exp(z)] ≤ E[exp(z) | E (cid:48)] + exp(−Ω(log2 n)) = O(exp(log.9 d))

where the last equality follows since conditioned on E (cid:48), z = O(log.9 d). Finally, this implies
1
Pr[E]

O(exp(log.9 d)) = O(exp(log.9 d))

E[exp(z) | E] ≤

where the last equality follows since Pr[E] ≥ 1 − exp(−Ω(log2 n)). Putting this together with A.27, we get
E[exp(z)] ≥ E[exp(z) | E] Pr[E] = E[exp(z) | E] − E[exp(z) | E] exp(−Ω(log2 d)) ≥
E[exp(z) | E] − O(exp(log.9 d)) exp(−Ω(log2 d)) ≥ E[exp(z) | E] − exp(−Ω(log2 d))

which is what we needed.

28

A.1 Analyzing partition function Zc

In this section, we prove Lemma 2.1. We basically ﬁrst prove that for the means of Zc are all (1 + o(1))-close
to each other, and then prove that Zc is concentrated around its mean. It turns out the concentration part
is non trivial because the random variable of concern, exp((cid:104)vw, c(cid:105)) is not well-behaved in terms of the tail.
Note that exp((cid:104)vw, c(cid:105)) is NOT sub-gaussian for any variance proxy. This essentially disallows us to use an
existing concentration inequality directly. We get around this issue by considering the truncated version of
exp((cid:104)vw, c(cid:105)), which is bounded, and have similar tail properties as the original one, in the regime that we
are concerning.

We bound the mean and variance of Zc ﬁrst in the Lemma below.

Lemma A.6. For any ﬁxed unit vector c ∈ Rd, we have that E[Zc] ≥ n and V[Zc] ≤ O(n).

Proof of Lemma A.6. Recall that by deﬁnition

Zc =

exp((cid:104)vw, c(cid:105)).

(cid:88)

w

We ﬁx context c and view vw’s as random variables throughout this proof. Recall that vw is composed of
vw = sw · ˆvw, where sw is the scaling and ˆvw is from spherical Gaussian with identity covariance Id×d. Let
s be a random variable that has the same distribution as sw.

We lowerbound the mean of Zc as follows:

E[Zc] = n E [exp((cid:104)vw, c(cid:105))] ≥ n E [1 + (cid:104)vw, c(cid:105)] = n

where the last equality holds because of the symmetry of the spherical Gaussian distibution. On the other
hand, to upperbound the mean of Zc, we condition on the scaling sw,

E[Zc] = n E[exp((cid:104)vw, c(cid:105))]

= n E [E [exp((cid:104)vw, c(cid:105)) | sw]]

Note that conditioned on sw, we have that (cid:104)vw, c(cid:105) is a Gaussian random variable with variance σ2 = s2
w.

Therefore,

E [exp((cid:104)vw, c(cid:105)) | sw] =

(cid:90)

σ

x
(cid:90)

1
√
2π
1
√
σ
2π
= exp(σ2/2)

=

x

exp(−

exp(−

x2
2σ2 ) exp(x)dx
(x − σ2)2
2σ2

+ σ2/2)dx

It follows that

E[Zc] = n E[exp(σ2/2)] = n E[exp(s2

w/2)] = n E[exp(s2/2)].

We calculate the variance of Zc as follows:

V[Zc] =

V [exp((cid:104)vw, c(cid:105))] ≤ n E[exp(2(cid:104)vw, c(cid:105))]

(cid:88)

w

= n E [E [exp(2(cid:104)vw, c(cid:105)) | sw]]

By a very similar calculation as above, using the fact that 2(cid:104)vw, c(cid:105) is a Gaussian random variable with
variance 4σ2 = 4s2
w,

E [exp(2(cid:104)vw, c(cid:105)) | sw] = exp(2σ2)

29

Therefore, we have that

V[Zc] ≤ n E [E [exp(2(cid:104)vw, c(cid:105)) | sw]]

= n E (cid:2)exp(2σ2)(cid:3) = n E (cid:2)exp(2s2)(cid:3) ≤ Λn

for Λ = exp(8κ2) a constant, and at the last step we used the facts that s ≤ κ a.s.

Now we are ready to prove Lemma 2.1.

Proof of Lemma 2.1. We ﬁx the choice of c, and the proving the concentration using the randomness of vw’s
ﬁrst. Note that that exp((cid:104)vw, c(cid:105)) is neither sub-Gaussian nor sub-exponential (actually the Orlicz norm of
random variable exp((cid:104)vw, c(cid:105)) is never bounded). This prevents us from applying the usual concentration
inequalities. The proof deals with this issue in a slightly more specialized manner.

Let’s deﬁne Fw be the event that |(cid:104)vw, c(cid:105)| ≤ 1

2 log n. We claim that Pr[Fw] ≥ 1−exp(−Ω(log2 n)). Indeed
note that (cid:104)vw, c(cid:105) | sw has a Gaussian distribution with standard deviation sw(cid:107)c(cid:107) = sw ≤ 2κ a.s. Therefore
by the Gaussianity of (cid:104)vw, c(cid:105) we have that

Pr[|(cid:104)vw, c(cid:105)| ≥ η log n | sw] ≤ 2 exp(−Ω(

log2 n/κ2)) = exp(−Ω(log2 n)),

where Ω(·) hides the dependency on κ which is treated as absolute constants. Taking expectations over sw,
we obtain that

Pr[Fw] = Pr[|(cid:104)vw, c(cid:105)| ≤

log n] ≥ 1 − exp(−Ω(log2 n)).

Note that by deﬁnition, we in particular have that conditioned on Fw, it holds that exp((cid:104)vw, c(cid:105)) ≤

Let the random variable Xw have the same distribution as exp((cid:104)vw, c(cid:105))|Fw . We prove that the random
w Xw concentrates well. By convexity of the exponential function, we have that the mean

variable Z (cid:48)
of Z (cid:48)

c = (cid:80)
c is lowerbounded

√

n.

1
4

1
2

E[Z (cid:48)

c] = n E [exp((cid:104)vw, c(cid:105))|Fw ] ≥ n exp(E [(cid:104)vw, c(cid:105)|Fw ]) = n

and the variance is upperbounded by

V[Z (cid:48)

c] ≤ n E (cid:2)exp((cid:104)vw, c(cid:105))2|Fw

(cid:3)

≤

≤

1
Pr[Fw]
1
Pr[Fw]

E (cid:2)exp((cid:104)vw, c(cid:105))2(cid:3)

Λn ≤ 1.1Λn

where the second line uses the fact that

E (cid:2)exp((cid:104)vw, c(cid:105))2(cid:3)

= Pr[Fw] E (cid:2)exp((cid:104)vw, c(cid:105))2|Fw
≥ Pr[Fw] E (cid:2)exp((cid:104)vw, c(cid:105))2|Fw
√

(cid:3) + Pr[F w] E (cid:2)exp((cid:104)vw, c(cid:105))2|F w
(cid:3) .

(cid:3)

Moreover, by deﬁnition, for any w, |Xw| ≤

n. Therefore by Bernstein’s inequality, we have that

Pr [|Z (cid:48)

c − E[Z (cid:48)

c]| > (cid:15)n] ≤ exp(−

1

2 (cid:15)2n2
√

1.1Λn + 1
3

)

n · (cid:15)n

30

Note that E[Z (cid:48)

c] ≥ n, therefore for (cid:15) (cid:29) log2 n√

n , we have,

Pr [|Z (cid:48)

c − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c]] ≤ Pr [|Z (cid:48)

c − E[Z (cid:48)

c]| > (cid:15)n] ≤ exp(−
√

n}))

1

2 (cid:15)2n2
√

)

n · (cid:15)n

Λn + 1
3

≤ exp(−Ω(min{(cid:15)2n/Λ, (cid:15)
≤ exp(−Ω(log2 n))

Let F = ∪wFw be the union of all Fw. Then by union bound, it holds that Pr[ ¯F] ≤ (cid:80)

n · exp(−Ω(log2 n)) = exp(−Ω(log2 n)). We have that by deﬁnition, Z (cid:48)
Therefore, we have that

w Pr[ ¯Fw] ≤
c has the same distribution as Zc|F .

Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c] | F] ≤ exp(−Ω(log2 n))

(A.30)

and therefore

Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c]] = Pr[F] · Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c] | F] + Pr[ ¯F] Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c] | ¯F]

≤ Pr[|Zc − E[Z (cid:48)
≤ exp(−Ω(log2 n))

c]| > (cid:15) E[Z (cid:48)

c] | F] + Pr[ ¯F]

(A.31)

where at the last line we used the fact that Pr[ ¯F] ≤ exp(−Ω(log2 n)) and equation (A.30).

Let Z = E[Z (cid:48)

c] = E[exp((cid:104)vw, c(cid:105)) | | (cid:104)vw, c(cid:105) | < 1

2 log n] (note that E[Z (cid:48)

c] only depends on the norm of (cid:107)c(cid:107)

which is equal to 1). Therefore we obtain that with high probability over the randomness of vw’s,

(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z

(A.32)

Taking expectation over the randomness of c, we have that

[ (A.32) holds] ≥ 1 − exp(−Ω(log2 n))

Pr
c,vw

Therefore by a standard averaging argument (using Markov inequality), we have

(cid:104)

Pr
vw

Pr
c

[ (A.32) holds] ≥ 1 − exp(−Ω(log2 n))

≥ 1 − exp(−Ω(log2 n))

(cid:105)

For now on we ﬁx a choice of vw’s so that Prc[ (A.32) holds] ≥ 1 − exp(−Ω(log2 n)) is true. Therefore in the
rest of the proof, only c is random variable, and with probability 1 − exp(−Ω(log2 n)) over the randomness
of c, it holds that,

(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z.

(A.33)

B Maximum likelihood estimator for co-occurrence

Let L be the corpus size, and Xw,w(cid:48) the number of times words w, w(cid:48) co-occur within a context of size 10
in the corpus. According to the model, the probability of this event at any particular time is log p(w, w(cid:48)) ∝
(cid:107)vw + vw(cid:48)(cid:107)2
2 . Successive samples from a random walk are not independent of course, but if the random walk
mixes fairly quickly (and the mixing time of our random walk is related to the logarithm of the number
of words) then the set of Xw,w(cid:48)’s over all word pairs is distributed up to a very close approximation as a
multinomial distribution Mul( ˜L, {p(w, w(cid:48))}) where ˜L = (cid:80)
w,w(cid:48) Xw,w(cid:48) is the total number of word pairs in
consideration (roughly 10L).

31

Assuming this approximation, we show below that the maximum likelihood values for the word vectors

correspond to the following optimization,

min
{vw},C

(cid:88)

w,w(cid:48)

(cid:16)

Xw,w(cid:48)

log(Xw,w(cid:48)) − (cid:107)vw +vw(cid:48)(cid:107)2

2 − C

(cid:17)2

(Objective SN)

Now we give the derivation of the objective. According to the multinomial distribution, maximizing the

likelihood of {Xw,w(cid:48)} is equivalent to maximizing

To reason about the likelihood, denote the logarithm of the ratio between the expected count and the

empirical count as

Note that

(cid:96) = log

p(w, w(cid:48))Xw,w(cid:48)

 =

Xw,w(cid:48) log p(w, w(cid:48)).





(cid:89)

(w,w(cid:48))



(cid:88)

(w,w(cid:48))

∆w,w(cid:48) = log

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

(cid:33)

.

(cid:88)

(cid:96) =

Xw,w(cid:48) log p(w, w(cid:48))

(w,w(cid:48))

(cid:88)

(w,w(cid:48))

(cid:88)

(w,w(cid:48))

=

=

Xw,w(cid:48)

log

+ log

(cid:34)

Xw,w(cid:48)
˜L

(cid:33)(cid:35)

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

Xw,w(cid:48) log

Xw,w(cid:48) log

Xw,w(cid:48)
˜L

(cid:88)

+

(w,w(cid:48))

(cid:33)

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

where we let c denote the constant (cid:80)

(w,w(cid:48)) Xw,w(cid:48) log Xw,w(cid:48)
˜L

. Furthermore, we have

(B.1)

(cid:88)

= c +

Xw,w(cid:48)∆w,w(cid:48)

(w,w(cid:48))

(cid:88)

˜L =

˜Lpw,w(cid:48)

Xw,w(cid:48)e∆w,w(cid:48)

=

=

(w,w(cid:48))
(cid:88)

(w,w(cid:48))
(cid:88)

(w,w(cid:48))

Xw,w(cid:48)(1 + ∆w,w(cid:48) + ∆2

w,w(cid:48)/2 + O(|∆w,w(cid:48)|3))

and also ˜L = (cid:80)

(w,w(cid:48)) Xw,w(cid:48). So

Plugging this into (3.2) leads to

Xw,w(cid:48)∆w,w(cid:48) = −

Xw,w(cid:48)∆2

w,w(cid:48)/2 +

Xw,w(cid:48)O(|∆w,w(cid:48)|3)

 .





(cid:88)

(w,w(cid:48))

(cid:88)

(w,w(cid:48))



(cid:88)

(w,w(cid:48))

c − (cid:96) =

Xw,w(cid:48)∆2

w,w(cid:48)/2 +

Xw,w(cid:48)O(|∆w,w(cid:48)|3).

(B.2)

(cid:88)

(w,w(cid:48))

(cid:88)

(w,w(cid:48))

32

When the last term is much smaller than the ﬁrst term on the right hand side, maximizing the likelihood

is approximately equivalent to minimizing the ﬁrst term on the right hand side, which is our objective:

(cid:88)

(w,w(cid:48))

Xw,w(cid:48)∆2

w,w(cid:48) ≈

Xw,w(cid:48)

(cid:107)vw + vw(cid:48)(cid:107)2

2/(2d) − log Xw,w(cid:48) + log ˜L − 2 log Z

(cid:17)2

(cid:16)

(cid:88)

(w,w(cid:48))

where Z is the partition function.

We now argue that the last term is much smaller than the ﬁrst term on the right hand side in (B.2). For
a large Xw,w(cid:48), the ∆w,w(cid:48) is close to 0 and thus the induced approximation error is small. Small Xw,w(cid:48)’s only
contribute a small fraction of the ﬁnal objective (3.3), so we can safely ignore the errors. To see this, note
that the objective (cid:80)
(w,w(cid:48)) Xw,w(cid:48)O(|∆w,w(cid:48)|3) diﬀer by a factor of
|∆w,w(cid:48)| for each Xw,w(cid:48). For large Xw,w(cid:48)’s, |∆w,w(cid:48)| (cid:28) 1, and thus their corresponding errors are much smaller
than the objective. So we only need to consider the Xw,w(cid:48)’s that are small constants. The co-occurrence
counts obey a power law distribution (see, e.g. (Pennington et al., 2014)). That is, if one sorts {Xw,w(cid:48)} in
decreasing order, then the r-th value in the list is roughly

w,w(cid:48) and the error term (cid:80)

(w,w(cid:48)) Xw,w(cid:48)∆2

where k is some constant. Some calculation shows that

x[r] =

k
r5/4

and thus when x is a small constant

˜L ≈ 4k,

(cid:88)

Xw,w(cid:48) ≈ 4k4/5x1/5,

Xw,w(cid:48) ≤x

(cid:80)

Xw,w(cid:48) ≤x Xw,w(cid:48)
˜L

≈

(cid:18) 4x
˜L

(cid:19)1/5

= O

(cid:18) 1

˜L1/5

(cid:19)

.

So there are only a negligible mass of Xw,w(cid:48)’s that are small constants, which vanishes when ˜L increases.
Furthermore, we empirically observe that the relative error of our objective is 5%, which means that the
errors induced by Xw,w(cid:48)’s that are small constants is only a small fraction of the objective. Therefore,
(cid:80)

w,w(cid:48) Xw,w(cid:48)O(|∆w,w(cid:48)|3) is small compared to the objective and can be safely ignored.

33

9
1
0
2
 
n
u
J
 
9
1
 
 
]

G
L
.
s
c
[
 
 
8
v
0
2
5
3
0
.
2
0
5
1
:
v
i
X
r
a

RAND-WALK: A latent variable model approach to word
embeddings

Sanjeev Arora

Yuanzhi Li

Yingyu Liang

Tengyu Ma

Andrej Risteski ∗

Abstract

Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse
methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparam-
eters and reweighting methods.

This paper proposes a new generative model, a dynamic version of the log-linear topic model of Mnih
and Hinton (2007). The methodological novelty is to use the prior to compute closed form expressions
for word statistics. This provides a theoretical justiﬁcation for nonlinear models like PMI, word2vec,
and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic
embeddings contain linear algebraic structure that allows solution of word analogies, as shown by Mikolov
et al. (2013a) and many subsequent papers.

Experimental support is provided for the generative model assumptions, the most important of which

is that latent word vectors are fairly uniformly dispersed in space.

1 Introduction

Vector representations of words (word embeddings) try to capture relationships between words as distance or
angle, and have many applications in computational linguistics and machine learning. They are constructed
by various models whose unifying philosophy is that the meaning of a word is deﬁned by “the company it
keeps” (Firth, 1957), namely, co-occurrence statistics. The simplest methods use word vectors that explicitly
represent co-occurrence statistics. Reweighting heuristics are known to improve these methods, as is dimen-
sion reduction (Deerwester et al., 1990). Some reweighting methods are nonlinear, which include taking the
square root of co-occurrence counts (Rohde et al., 2006), or the logarithm, or the related Pointwise Mutual
Information (PMI) (Church and Hanks, 1990). These are collectively referred to as Vector Space Models,
surveyed in (Turney and Pantel, 2010).

Neural network language models (Rumelhart et al., 1986; 1988; Bengio et al., 2006; Collobert and Weston,
2008a) propose another way to construct embeddings: the word vector is simply the neural network’s internal
representation for the word. This method is nonlinear and nonconvex. It was popularized via word2vec, a
family of energy-based models in (Mikolov et al., 2013b;c), followed by a matrix factorization approach called
GloVe (Pennington et al., 2014). The ﬁrst paper also showed how to solve analogies using linear algebra on
word embeddings. Experiments and theory were used to suggest that these newer methods are related to
the older PMI-based models, but with new hyperparameters and/or term reweighting methods (Levy and
Goldberg, 2014b).

But note that even the old PMI method is a bit mysterious. The simplest version considers a symmetric
matrix with each row/column indexed by a word. The entry for (w, w(cid:48)) is PMI(w, w(cid:48)) = log p(w,w(cid:48))
p(w)p(w(cid:48)) , where
p(w, w(cid:48)) is the empirical probability of words w, w(cid:48) appearing within a window of certain size in the corpus,
and p(w) is the marginal probability of w. (More complicated models could use asymmetric matrices with

∗Princeton University, Computer Science Department. {arora,yuanzhil,yingyul,tengyu,risteski}@cs.princeton.edu.
This work was supported in part by NSF grants CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, Simons Investigator
Award, and Simons Collaboration Grant. Tengyu Ma was also supported by Simons Award for Graduate Students in Theoretical
Computer Science and IBM PhD Fellowship.

1

columns corresponding to context words or phrases, and also involve tensorization.) Then word vectors are
obtained by low-rank SVD on this matrix, or a related matrix with term reweightings. In particular, the
PMI matrix is found to be closely approximated by a low rank matrix: there exist word vectors in say 300
dimensions, which is much smaller than the number of words in the dictionary, such that

(cid:104)vw, vw(cid:48)(cid:105) ≈ PMI(w, w(cid:48))

(1.1)

where ≈ should be interpreted loosely.

There appears to be no theoretical explanation for this empirical ﬁnding about the approximate low
rank of the PMI matrix. The current paper addresses this. Speciﬁcally, we propose a probabilistic model
of text generation that augments the log-linear topic model of Mnih and Hinton (2007) with dynamics, in
the form of a random walk over a latent discourse space. The chief methodological contribution is using the
model priors to analytically derive a closed-form expression that directly explains (1.1); see Theorem 2.2
in Section 2. Section 3 builds on this insight to give a rigorous justiﬁcation for models such as word2vec
and GloVe, including the hyperparameter choices for the latter. The insight also leads to a mathematical
explanation for why these word embeddings allow analogies to be solved using linear algebra; see Section 4.
Section 5 shows good empirical ﬁt to this model’s assumtions and predictions, including the surprising one
that word vectors are pretty uniformly distributed (isotropic) in space.1

1.1 Related work

Latent variable probabilistic models of language have been used for word embeddings before, including
Latent Dirichlet Allocation (LDA) and its more complicated variants (see the survey (Blei, 2012)), and some
neurally inspired nonlinear models (Mnih and Hinton, 2007; Maas et al., 2011). In fact, LDA evolved out of
eﬀorts in the 1990s to provide a generative model that “explains” the success of older vector space methods
like Latent Semantic Indexing (Papadimitriou et al., 1998; Hofmann, 1999). However, none of these earlier
generative models has been linked to PMI models.

Levy and Goldberg (2014b) tried to relate word2vec to PMI models. They showed that if there were no
dimension constraint in word2vec, speciﬁcally, the “skip-gram with negative sampling (SGNS)” version of the
model, then its solutions would satisfy (1.1), provided the right hand side were replaced by PMI(w, w(cid:48)) − β
for some scalar β. However, skip-gram is a discriminative model (due to the use of negative sampling), not
generative. Furthermore, their argument only applies to very high-dimensional word embeddings, and thus
does not address low-dimensional embeddings, which have superior quality in applications.

Hashimoto et al. (2016) focuses on issues similar to our paper. They model text generation as a random
walk on words, which are assumed to be embedded as vectors in a geometric space. Given that the last word
produced was w, the probability that the next word is w(cid:48) is assumed to be given by h(|vw − vw(cid:48)|2) for a
suitable function h, and this model leads to an explanation of (1.1). By contrast our random walk involves
a latent discourse vector, which has a clearer semantic interpretation and has proven useful in subsequent
work, e.g. understanding structure of word embeddings for polysemous words Arora et al. (2016). Also our
work clariﬁes some weighting and bias terms in the training objectives of previous methods (Section 3) and
also the phenomenon discussed in the next paragraph.

Researchers have tried to understand why vectors obtained from the highly nonlinear word2vec models
exhibit linear structures (Levy and Goldberg, 2014a; Pennington et al., 2014). Speciﬁcally, for analogies like
“man:woman::king:??,” queen happens to be the word whose vector vqueen is the most similar to the vector
vking − vman + vwoman. This suggests that simple semantic relationships, such as masculine vs feminine
tested in the above example, correspond approximately to a single direction in space, a phenomenon we will
henceforth refer to as relations=lines.

Section 4 surveys earlier attempts to explain this phenomenon and their shortcoming, namely, that they
ignore the large approximation error in relationships like (1.1). This error appears larger than the diﬀerence
between the best solution and the second best (incorrect) solution in analogy solving, so that this error could
in principle lead to a complete failure in analogy solving. In our explanation, the low dimensionality of the

1The code is available at https://github.com/PrincetonML/SemanticVector

2

word vectors plays a key role. This can also be seen as a theoretical explanation of the old observation that
dimension reduction improves the quality of word embeddings for various tasks. The intuitive explanation
often given —that smaller models generalize better—turns out to be fallacious, since the training method
for creating embeddings makes no reference to analogy solving. Thus there is no a priori reason why low-
dimensional model parameters (i.e., lower model capacity) should lead to better performance in analogy
solving, just as there is no reason they are better at some other unrelated task like predicting the weather.

1.2 Beneﬁts of generative approaches

In addition to giving some form of “uniﬁcation” of existing methods, our generative model also brings
more intepretability to word embeddings beyond traditional cosine similarity and even analogy solving. For
example, it led to an understanding of how the diﬀerent senses of a polysemous word (e.g., bank) reside in
linear superposition within the word embedding (Arora et al., 2016). Such insight into embeddings may
prove useful in the numerous settings in NLP and neuroscience where they are used.

Another new explanatory feature of our model is that low dimensionality of word embeddings plays a key
theoretical role —unlike in previous papers where the model is agnostic about the dimension of the embed-
dings, and the superiority of low-dimensional embeddings is an empirical ﬁnding (starting with Deerwester
et al. (1990)). Speciﬁcally, our theoretical analysis makes the key assumption that the set of all word vectors
(which are latent variables of the generative model) are spatially isotropic, which means that they have no
preferred direction in space. Having n vectors be isotropic in d dimensions requires d (cid:28) n. This isotropy is
needed in the calculations (i.e., multidimensional integral) that yield (1.1). It also holds empirically for our
word vectors, as shown in Section 5.

The isotropy of low-dimensional word vectors also plays a key role in our explanation of the rela-
tions=lines phenomenon (Section 4). The isotropy has a “puriﬁcation” eﬀect that mitigates the eﬀect of
the (rather large) approximation error in the PMI models.

2 Generative model and its properties

The model treats corpus generation as a dynamic process, where the t-th word is produced at step t. The
process is driven by the random walk of a discourse vector ct ∈ (cid:60)d. Its coordinates represent what is being
talked about.2 Each word has a (time-invariant) latent vector vw ∈ (cid:60)d that captures its correlations with
the discourse vector. We model this bias with a log-linear word production model:

Pr[w emitted at time t | ct] ∝ exp((cid:104)ct, vw(cid:105)).

(2.1)

The discourse vector ct does a slow random walk (meaning that ct+1 is obtained from ct by adding a small
random displacement vector), so that nearby words are generated under similar discourses. We are interested
in the probabilities that word pairs co-occur near each other, so occasional big jumps in the random walk
are allowed because they have negligible eﬀect on these probabilities.

A similar log-linear model appears in Mnih and Hinton (2007) but without the random walk. The linear
chain CRF of Collobert and Weston (2008b) is more general. The dynamic topic model of Blei and Laﬀerty
(2006) utilizes topic dynamics, but with a linear word production model. Belanger and Kakade (2015) have
proposed a dynamic model for text using Kalman Filters, where the sequence of words is generated from
Gaussian linear dynamical systems, rather than the log-linear model in our case.

The novelty here over such past works is a theoretical analysis in the method-of-moments tradition (Hsu
et al., 2012; Cohen et al., 2012). Assuming a prior on the random walk we analytically integrate out the
hidden random variables and compute a simple closed form expression that approximately connects the
model parameters to the observable joint probabilities (see Theorem 2.2). This is reminiscent of analysis of
similar random walk models in ﬁnance (Black and Scholes, 1973).

2This is a diﬀerent interpretation of the term “discourse” compared to some other settings in computational linguistics.

3

Model details. Let n denote the number of words and d denote the dimension of the discourse space,
where 1 ≤ d ≤ n. Inspecting (2.1) suggests word vectors need to have varying lengths, to ﬁt the empirical
ﬁnding that word probabilities satisfy a power law. Furthermore, we will assume that in the bulk, the word
vectors are distributed uniformly in space, earlier referred to as isotropy. This can be quantiﬁed as a prior
in the Bayesian tradition. More precisely, the ensemble of word vectors consists of i.i.d draws generated by
v = s · ˆv, where ˆv is from the spherical Gaussian distribution, and s is a scalar random variable. We assume
s is a random scalar with expectation τ = Θ(1) and s is always upper bounded by κ, which is another
constant. Here τ governs the expected magnitude of (cid:104)v, ct(cid:105), and it is particularly important to choose it to
be Θ(1) so that the distribution Pr[w|ct] ∝ exp((cid:104)vw, ct(cid:105)) is interesting.3 Moreover, the dynamic range of
word probabilities will roughly equal exp(κ2), so one should think of κ as an absolute constant like 5. These
details about s are important for realistic modeling but not too important in our analysis. (Furthermore,
readers uncomfortable with this simplistic Bayesian prior should look at Section 2.2 below.)

Finally, we clarify the nature of the random walk. We assume that the stationary distribution of the
random walk is uniform over the unit sphere, denoted by C. The transition kernel of the random walk can
d in (cid:96)2 norm.4
be in any form so long as at each step the movement of the discourse vector is at most (cid:15)2/
This is still fast enough to let the walk mix quickly in the space.

√

The following lemma (whose proof appears in the appendix) is central to the analysis.

It says that
under the Bayesian prior, the partition function Zc = (cid:80)
w exp((cid:104)vw, c(cid:105)), which is the implied normalization
in equation (2.1), is close to some constant Z for most of the discourses c. This can be seen as a plausible
theoretical explanation of a phenomenon called self-normalization in log-linear models: ignoring the partition
function or treating it as a constant (which greatly simpliﬁes training) is known to often give good results.
This has also been studied in (Andreas and Klein, 2014).

Lemma 2.1 (Concentration of partition functions). If the word vectors satisfy the Bayesian prior described
in the model details, then

Pr
c∼C

[(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z] ≥ 1 − δ,

(2.2)

for (cid:15)z = (cid:101)O(1/

n), and δ = exp(−Ω(log2 n)).

√

The concentration of the partition functions then leads to our main theorem (the proof is in the appendix).
The theorem gives simple closed form approximations for p(w), the probability of word w in the corpus, and
p(w, w(cid:48)), the probability that two words w, w(cid:48) occur next to each other. The theorem states the result for
the window size q = 2, but the same analysis works for pairs that appear in a small window, say of size 10,
as stated in Corollary 2.3. Recall that PMI(w, w(cid:48)) = log[p(w, w(cid:48))/(p(w)p(w(cid:48)))].

Theorem 2.2. Suppose the word vectors satisfy the inequality (2.2), and window size q = 2. Then,

for (cid:15) = O((cid:15)z) + (cid:101)O(1/d) + O((cid:15)2). Jointly these imply:

log p(w, w(cid:48)) =

− 2 log Z ± (cid:15),

(cid:107)vw + vw(cid:48)(cid:107)2
2
2d
(cid:107)vw(cid:107)2
2
2d

log p(w) =

− log Z ± (cid:15).

PMI (w, w(cid:48)) =

(cid:104)vw, vw(cid:48)(cid:105)
d

± O((cid:15)).

(2.3)

(2.4)

(2.5)

Remarks 1. Since the word vectors have (cid:96)2 norm of the order of
(cid:107)vw + vw(cid:48)(cid:107)2

d, for two typical word vectors vw, vw(cid:48),
2 is of the order of Θ(d). Therefore the noise level (cid:15) is very small compared to the leading

3A larger τ will make Pr[w|ct] too peaked and a smaller one will make it too uniform.
4 More precisely, the proof extends to any symmetric product stationary distribution C with sub-Gaussian coordinate

satisfying Ec

(cid:2)(cid:107)c(cid:107)2(cid:3) = 1, and the steps are such that for all ct, Ep(ct+1|ct)[exp(κ

d(cid:107)ct+1 − ct(cid:107))] ≤ 1 + (cid:15)2 for some small (cid:15)2.

√

√

4

2d (cid:107)vw + vw(cid:48)(cid:107)2

term 1
empirically we also ﬁnd higher error here.

2. For PMI however, the noise level O((cid:15)) could be comparable to the leading term, and

Remarks 2. Variants of the expression for joint probability in (2.3) had been hypothesized based upon
empirical evidence in Mikolov et al. (2013b) and also Globerson et al. (2007), and Maron et al. (2010) .

Remarks 3. Theorem 2.2 directly leads to the extension to a general window size q as follows:

Corollary 2.3. Let pq(w, w(cid:48)) be the co-occurrence probability in windows of size q, and PMIq(w, w(cid:48)) be the
corresponding PMI value. Then

log pq(w, w(cid:48)) =

− 2 log Z + γ ± (cid:15),

PMIq (w, w(cid:48)) =

+ γ ± O((cid:15)).

(cid:107)vw + vw(cid:48)(cid:107)2
2
2d

(cid:104)vw, vw(cid:48)(cid:105)
d

where γ = log

(cid:16) q(q−1)
2

(cid:17)

.

It is quite easy to see that Theorem 2.2 implies the Corollary 2.3, as when the window size is q the
(cid:1) positions within the window, and the joint probability of w, w(cid:48) is
pair w, w(cid:48) could appear in any of (cid:0)q
roughly the same for any positions because the discourse vector changes slowly.
(Of course, the error
term gets worse as we consider larger window sizes, although for any constant size, the statement of the
theorem is correct.) This is also consistent with the shift β for ﬁtting PMI in (Levy and Goldberg, 2014b),
which showed that without dimension constraints, the solution to skip-gram with negative sampling satisﬁes
PMI (w, w(cid:48)) − β = (cid:104)vw, vw(cid:48)(cid:105) for a constant β that is related to the negative sampling in the optimization.
Our result justiﬁes via a generative model why this should be satisﬁed even for low dimensional word vectors.

2

2.1 Proof sketches

Here we provide the proof sketches, while the complete proof can be found in the appendix.

Proof sketch of Theorem 2.2 Let w and w(cid:48) be two arbitrary words. Let c and c(cid:48) denote two consecutive
context vectors, where c ∼ C and c(cid:48)|c is deﬁned by the Markov kernel p(c(cid:48) | c).

We start by using the law of total expectation, integrating out the hidden variables c and c(cid:48):

An expectation like (2.6) would normally be diﬃcult to analyze because of the partition functions.
However, we can assume the inequality (2.2), that is, the partition function typically does not vary much for
most of context vectors c. Let F be the event that both c and c(cid:48) are within (1 ± (cid:15)z)Z. Then by (2.2) and the
union bound, event F happens with probability at least 1 − 2 exp(−Ω(log2 n)). We will split the right-hand
side (RHS) of (2.6) into the parts according to whether F happens or not.

p(w, w(cid:48)) = E
c,c(cid:48)
= E
c,c(cid:48)

[Pr[w, w(cid:48)|c, c(cid:48)]]

[p(w|c)p(w(cid:48)|c(cid:48))]

= E
c,c(cid:48)

(cid:20) exp((cid:104)vw, c(cid:105))
Zc

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

(cid:21)

RHS of (2.6) = E
c,c(cid:48)
(cid:124)

(cid:20) exp((cid:104)vw, c(cid:105))
Zc

(cid:20) exp((cid:104)vw, c(cid:105))
Zc

+ E
c,c(cid:48)
(cid:124)

(cid:123)(cid:122)
T2

5

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

(cid:123)(cid:122)
T1
exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

1F

1 ¯F

(cid:21)

(cid:125)

(cid:21)

(cid:125)

(2.6)

(2.7)

where ¯F denotes the complement of event F and 1F and 1 ¯F denote indicator functions for F and ¯F ,
respectively. When F happens, we can replace Zc by Z with a 1 ± (cid:15)z factor loss: The ﬁrst term of the RHS
of (2.7) equals to

T1 =

1 ± O((cid:15)z)
Z 2

E
c,c(cid:48)

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ]

(2.8)

On the other hand, we can use E[1 ¯F ] = Pr[ ¯F ] ≤ exp(−Ω(log2 n)) to show that the second term of RHS

of (2.7) is negligible,

|T2| = exp(−Ω(log1.8 n)) .
This claim must be handled somewhat carefully since the RHS does not depend on d at all. Brieﬂy, the
d = o(log2 n)), any word vector vw and
reason this holds is as follows:
d)), and since E[1 ¯F ] = exp(−Ω(log2 n)), the
discourse c satisﬁes that exp((cid:104)vw, c(cid:105)) ≤ exp((cid:107)vw(cid:107)) = exp(O(
d = Ω(log2 n)), we can use concentration inequalities
claim follows directly; In the regime when d is large (
to show that except with a small probability exp(−Ω(d)) = exp(−Ω(log2 n)), a uniform sample from the
sphere behaves equivalently to sampling all of the coordinates from a standard Gaussian distribution with
mean 0 and variance 1

d , in which case the claim is not too diﬃcult to show using Gaussian tail bounds.

in the regime when d is small (

Therefore it suﬃces to only consider (2.8). Our model assumptions state that c and c(cid:48) cannot be too

(2.9)

√

√

√

diﬀerent. We leverage that by rewriting (2.8) a little, and get that it equals

(cid:20)
exp((cid:104)vw, c(cid:105)) E
c(cid:48)|c

(cid:21)
[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

T1 =

=

1 ± O((cid:15)z)
Z 2
1 ± O((cid:15)z)
Z 2

E
c

E
c

[exp((cid:104)vw, c(cid:105))A(c)]

(2.10)

[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]. We claim that A(c) = (1 ± O((cid:15)2)) exp((cid:104)vw(cid:48), c(cid:105)). Doing some algebraic

where A(c) := E
c(cid:48)|c

manipulations,

Furthermore, by our model assumptions, (cid:107)c − c(cid:48)(cid:107) ≤ (cid:15)2/

d. So

√

A(c) = exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48) − c(cid:105))] .

(cid:104)vw, c − c(cid:48)(cid:105) ≤ (cid:107)vw(cid:107)(cid:107)c − c(cid:48)(cid:107) = O((cid:15)2)

and thus A(c) = (1 ± O((cid:15)2)) exp((cid:104)vw(cid:48), c(cid:105)). Plugging the simpliﬁcation of A(c) to (2.10),

T1 =

1 ± O((cid:15)z)
Z 2

E[exp((cid:104)vw + vw(cid:48), c(cid:105))].

Since c has uniform distribution over the sphere, the random variable (cid:104)vw + vw(cid:48), c(cid:105) has distribution pretty
similar to Gaussian distribution N (0, (cid:107)vw + vw(cid:48)(cid:107)2/d), especially when d is relatively large. Observe that
E[exp(X)] has a closed form for Gaussian random variable X ∼ N (0, σ2),

E[exp(X)] =

(cid:90)

1
√
2π

x

σ
= exp(σ2/2) .

exp(−

x2
2σ2 ) exp(x)dx

(2.11)

(2.12)

(2.13)

Bounding the diﬀerence between (cid:104)vw + vw(cid:48), c(cid:105) from Gaussian random variable, we can show that for

(cid:15) = (cid:101)O(1/d),

E[exp((cid:104)vw + vw(cid:48), c(cid:105))] = (1 ± (cid:15)) exp

(cid:18) (cid:107)vw + vw(cid:48)(cid:107)2
2d

(cid:19)

.

Therefore, the series of simpliﬁcation/approximation above (concretely, combining equations (2.6), (2.7), (2.9), (2.11),

and (2.13)) lead to the desired bound on log p(w, w(cid:48)) for the case when the window size q = 2. The bound
on log p(w) can be shown similarly.

6

Proof sketch of Lemma 2.1 Note that for ﬁxed c, when word vectors have Gaussian priors assumed as
in our model, Zc = (cid:80)

w exp((cid:104)vw, c(cid:105)) is a sum of independent random variables.

We ﬁrst claim that using proper concentration of measure tools, it can be shown that the variance of
Zc are relatively small compared to its mean Evw [Zc], and thus Zc concentrates around its mean. Note this
is quite non-trivial: the random variable exp((cid:104)vw, c(cid:105)) is neither bounded nor subgaussian/sub-exponential,
since the tail is approximately inverse poly-logarithmic instead of inverse exponential.
In fact, the same
concentration phenomenon does not happen for w. The occurrence probability of word w is not necessarily
concentrated because the (cid:96)2 norm of vw can vary a lot in our model, which allows the frequency of the words
to have a large dynamic range.

So now it suﬃces to show that Evw [Zc] for diﬀerent c are close to each other. Using the fact that the
word vector directions have a Gaussian distribution, Evw [Zc] turns out to only depend on the norm of c
(which is equal to 1). More precisely,

[Zc] = f ((cid:107)c(cid:107)2

2) = f (1)

E
vw

(2.14)

where f is deﬁned as f (α) = n Es[exp(s2α/2)] and s has the same distribution as the norms of the word
vectors. We sketch the proof of this. In our model, vw = sw · ˆvw, where ˆvw is a Gaussian vector with identity
covariance I. Then

E
vw

[Zc] = n E
vw

[exp((cid:104)vw, c(cid:105))]

(cid:20)

= n E
sw

E
vw|sw

(cid:21)
[exp((cid:104)vw, c(cid:105)) | sw]

where the second line is just an application of the law of total expectation, if we pick the norm of the
(random) vector vw ﬁrst, followed by its direction. Conditioned on sw, (cid:104)vw, c(cid:105) is a Gaussian random variable
with variance (cid:107)c(cid:107)2

w, and therefore using similar calculation as in (2.12), we have

2s2

Hence, Evw [Zc] = n Es[exp(s2(cid:107)c(cid:107)2

2/2)] as needed.

E
vw|sw

[exp((cid:104)vw, c(cid:105)) | sw] = exp(s2(cid:107)c(cid:107)2

2/2) .

Proof of Theorem 4.1 The proof uses the standard analysis of linear regression. Let V = P ΣQT be the
SVD of V and let σ1, . . . , σd be the left singular values of V (the diagonal entries of Σ). For notational ease
we omit the subscripts in ¯ζ and ζ (cid:48) since they are not relevant for this proof. Since V † = QΣ−1P T and thus
¯ζ = V †ζ (cid:48) = QΣ−1P T ζ (cid:48), we have

We claim

Indeed, (cid:80)d
from the ﬁrst assumption. Furthermore, by the second assumption, (cid:107)P T ζ (cid:48)(cid:107)∞ ≤ c2√

i = O(nd), since the average squared norm of a word vector is d. The claim then follows

i=1 σ2

n (cid:107)ζ (cid:48)(cid:107)2, so

(2.15)

(2.16)

(2.17)

Plugging (2.16) and (2.17) into (2.15), we get

(cid:107)¯ζ(cid:107)2 ≤

(cid:114)

(cid:114) 1
c1n

c2
2d
n

(cid:107)ζ (cid:48)(cid:107)2

2 =

(cid:107)ζ (cid:48)(cid:107)2

c2
√

√

d
c1n

as desired. The last statement follows because the norm of the signal, which is d log(νR) originally and is
V †d log(νR) = va − vb after dimension reduction, also gets reduced by a factor of

n.

√

(cid:107)¯ζ(cid:107)2 ≤ σ−1

d (cid:107)P T ζ (cid:48)(cid:107)2.

σ−1
d ≤

(cid:114) 1
c1n

.

(cid:107)P T ζ (cid:48)(cid:107)2

2 ≤

(cid:107)ζ (cid:48)(cid:107)2
2.

c2
2d
n

7

2.2 Weakening the model assumptions

For readers uncomfortable with Bayesian priors, we can replace our assumptions with concrete properties of
word vectors that are empirically veriﬁable (Section 5.1) for our ﬁnal word vectors, and in fact also for word
vectors computed using other recent methods.

The word meanings are assumed to be represented by some “ground truth” vectors, which the experi-
menter is trying to recover. These ground truth vectors are assumed to be spatially isotropic in the bulk,
in the following two speciﬁc ways: (i) For almost all unit vectors c the sum (cid:80)
w exp((cid:104)vw, c(cid:105)) is close to a
constant Z; (ii) Singular values of the matrix of word vectors satisfy properties similar to those of random
matrices, as formalized in the paragraph before Theorem 4.1. Our Bayesian prior on the word vectors hap-
pens to imply that these two conditions hold with high probability. But the conditions may hold even if the
prior doesn’t hold. Furthermore, they are compatible with all sorts of local structure among word vectors
such as existence of clusterings, which would be absent in truly random vectors drawn from our prior.

3 Training objective and relationship to other models

To get a training objective out of Theorem 2.2, we reason as follows. Let Xw,w(cid:48) be the number of times words
w and w(cid:48) co-occur within the same window in the corpus. The probability p(w, w(cid:48)) of such a co-occurrence
at any particular time is given by (2.3). Successive samples from a random walk are not independent.
But if the random walk mixes fairly quickly (the mixing time is related to the logarithm of the vocabulary
size), then the distribution of Xw,w(cid:48)’s is very close to a multinomial distribution Mul( ˜L, {p(w, w(cid:48))}), where
˜L = (cid:80)

w,w(cid:48) Xw,w(cid:48) is the total number of word pairs.

Assuming this approximation, we show below that the maximum likelihood values for the word vectors

correspond to the following optimization,

min
{vw},C

(cid:88)

w,w(cid:48)

(cid:16)

Xw,w(cid:48)

log(Xw,w(cid:48)) − (cid:107)vw +vw(cid:48)(cid:107)2

2 − C

(cid:17)2

As is usual, empirical performance is improved by weighting down very frequent word pairs, possibly
because very frequent words such as “the” do not ﬁt our model. This is done by replacing the weighting
Xw,w(cid:48) by its truncation min{Xw,w(cid:48), Xmax} where Xmax is a constant such as 100. We call this objective with
the truncated weights SN (Squared Norm).

We now give its derivation. Maximizing the likelihood of {Xw,w(cid:48)} is equivalent to maximizing

Denote the logarithm of the ratio between the expected count and the empirical count as

Then with some calculation, we obtain the following where c is independent of the empirical observations
Xw,w(cid:48)’s.

(3.1)

(3.2)

(cid:96) = log

p(w, w(cid:48))Xw,w(cid:48)





(cid:89)

(w,w(cid:48))



 .

∆w,w(cid:48) = log

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

(cid:33)

.

(cid:96) = c +

Xw,w(cid:48)∆w,w(cid:48)

(cid:88)

(w,w(cid:48))

8

On the other hand, using ex ≈ 1 + x + x2/2 when x is small,5 we have

(cid:88)

˜L =

˜Lpw,w(cid:48) =

(cid:88)

Xw,w(cid:48)e∆w,w(cid:48)

(w,w(cid:48))

(w,w(cid:48))

(cid:32)

Xw,w(cid:48)

1 + ∆w,w(cid:48) +

(cid:88)

≈

(w,w(cid:48))

(cid:33)

.

∆2
w,w(cid:48)
2

(cid:88)

(w,w(cid:48))

Xw,w(cid:48)∆w,w(cid:48) ≈ −

Xw,w(cid:48)∆2

w,w(cid:48).

1
2

(cid:88)

(w,w(cid:48))

Note that ˜L = (cid:80)

(w,w(cid:48)) Xw,w(cid:48), so

Plugging this into (3.2) leads to

2(c − (cid:96)) ≈

Xw,w(cid:48)∆2

w,w(cid:48).

(cid:88)

(w,w(cid:48))

(3.3)

So maximizing the likelihood is approximately equivalent to minimizing the right hand side, which (by
examining (3.1)) leads to our objective.

Objective for training with PMI. A similar objective PMI can be obtained from (2.5), by computing
an approximate MLE, using the fact that the error between the empirical and true value of PMI(w, w(cid:48)) is
driven by the smaller term p(w, w(cid:48)), and not the larger terms p(w), p(w(cid:48)).

min
{vw},C

(cid:88)

w,w(cid:48)

Xw,w(cid:48) (PMI(w, w(cid:48)) − (cid:104)vw, vw(cid:48)(cid:105))2

This is of course very analogous to classical VSM methods, with a novel reweighting method.

Fitting to either of the objectives involves solving a version of Weighted SVD which is NP-hard, but

empirically seems solvable in our setting via AdaGrad (Duchi et al., 2011).

Connection to GloVe. Compare SN with the objective used by GloVe (Pennington et al., 2014):

f (Xw,w(cid:48))(log(Xw,w(cid:48)) − (cid:104)vw, vw(cid:48)(cid:105) − sw − sw(cid:48) − C)2

(cid:88)

w,w(cid:48)

with f (Xw,w(cid:48)) = min{X 3/4
w,w(cid:48), 100}. Their weighting methods and the need for bias terms sw, sw(cid:48), C were
derived by trial and error; here they are all predicted and given meanings due to Theorem 2.2, speciﬁcally
sw = (cid:107)vw(cid:107)2.

Connection to word2vec(CBOW). The CBOW model in word2vec posits that the probability of a
word wk+1 as a function of the previous k words w1, w2, . . . , wk:

(cid:16)

p

wk+1

(cid:12)
(cid:12) {wi}k

i=1

(cid:17)

∝ exp((cid:104)vwk+1,

vwi (cid:105)).

1
k

k
(cid:88)

i=1

5This Taylor series approximation has an error of the order of x3, but ignoring it can be theoretically justiﬁed as follows.
is close to 0 and thus ignoring
For a large Xw,w(cid:48) , its value approaches its expectation and thus the corresponding ∆w,w(cid:48)
∆3
w,w(cid:48) is well justiﬁed. The terms where ∆w,w(cid:48) is signiﬁcant correspond to Xw,w(cid:48) ’s that are small. But empirically, Xw,w(cid:48) ’s
obey a power law distribution (see, e.g. Pennington et al. (2014)) using which it can be shown that these terms contribute a
small fraction of the ﬁnal objective (3.3). So we can safely ignore the errors. Full details appear in the ArXiv version of this
paper (Arora et al., 2015).

9

This expression seems mysterious since it depends upon the average word vector for the previous k words.
We show it can be theoretically justiﬁed. Assume a simpliﬁed version of our model, where a small window
of k words is generated as follows: sample c ∼ C, where C is a uniformly random unit vector, then sample
(w1, w2, . . . , wk) ∼ exp((cid:104)(cid:80)k
i=1 vwi , c(cid:105))/Zc. Furthermore, assume Zc = Z for any c.

Lemma 3.1. In the simpliﬁed version of our model, the Maximum-a-Posteriori (MAP) estimate of c given

(w1, w2, . . . , wk) is

(cid:80)k
(cid:107) (cid:80)k

i=1 vwi
i=1 vwi (cid:107)2

.

(cid:80)k
(cid:107) (cid:80)k

i=1 vwi
i=1 vwi (cid:107)2

.

Proof. The c maximizing p (c|w1, w2, . . . , wk) is the maximizer of p(c)p (w1, w2, . . . , wk|c). Since p(c) =
p(c(cid:48)) for any c, c(cid:48), and we have p (w1, w2, . . . , wk|c) = exp((cid:104)(cid:80)
i vwi, c(cid:105))/Z, the maximizer is clearly c =

Thus using the MAP estimate of ct gives essentially the same expression as CBOW apart from the

rescaling, which is often omitted due to computational eﬃciency in empirical works.

4 Explaining relations=lines

As mentioned, word analogies like “a:b::c:??” can be solved via a linear algebraic expression:

argmin
d

(cid:107)va − vb − vc + vd(cid:107)2
2 ,

(4.1)

where vectors have been normalized such that (cid:107)vd(cid:107)2 = 1. This suggests that the semantic relationships
being tested in the analogy are characterized by a straight line,6 referred to earlier as relations=lines.

Using our model we will show the following for low-dimensional embeddings: for each such relation R
there is a direction µR in space such that for any word pair a, b satisfying the relation, va − vb is like µR
plus some noise vector. This happens for relations satisfying a certain condition described below. Empirical
results supporting this theory appear in Section 5, where this linear structure is further leveraged to slightly
improve analogy solving.

A side product of our argument will be a mathematical explanation of the empirically well-established su-
periority of low-dimensional word embeddings over high-dimensional ones in this setting (Levy and Goldberg,
2014a). As mentioned earlier, the usual explanation that smaller models generalize better is fallacious.

We ﬁrst sketch what was missing in prior attempts to prove versions of relations=lines from ﬁrst
principles. The basic issue is approximation error: the diﬀerence between the best solution and the 2nd best
solution to (4.1) is typically small, whereas the approximation error in the objective in the low-dimensional
solutions is larger. For instance, if one uses our PMI objective, then the weighted average of the termwise
error in (2.5) is 17%, and the expression in (4.1) above contains six inner products. Thus in principle the
approximation error could lead to a failure of the method and the emergence of linear relationship, but it
does not.

Prior explanations. Pennington et al. (2014) try to propose a model where such linear relationships
should occur by design. They posit that queen is a solution to the analogy “man:woman::king:??” because

p(χ | king)
p(χ | queen)

≈

p(χ | man)
p(χ | woman)

,

(4.2)

where p(χ | king) denotes the conditional probability of seeing word χ in a small window of text around
king. Relationship (4.2) is intuitive since both sides will be ≈ 1 for gender-neutral χ like “walks” or “food”,

6Note that this interpretation has been disputed; e.g., it is argued in Levy and Goldberg (2014a) that (4.1) can be understood
using only the classical connection between inner product and word similarity, using which the objective (4.1) is slightly
improved to a diﬀerent objective called 3COSMUL. However, this “explanation” is still dogged by the issue of large termwise
error pinpointed here, since inner product is only a rough approximation to word similarity. Furthermore, the experiments in
Section 5 clearly support the relations=lines interpretation.

10

will be > 1 when χ is like “he, Henry” and will be < 1 when χ is like “dress, she, Elizabeth.” This was
also observed by Levy and Goldberg (2014a). Given (4.2), they then posit that the correct model describing
word embeddings in terms of word occurrences must be a homomorphism from ((cid:60)d, +) to ((cid:60)+, ×), so vector
diﬀerences map to ratios of probabilities. This leads to the expression

pw,w(cid:48) = (cid:104)vw, vw(cid:48)(cid:105) + bw + bw(cid:48),

and their method is a (weighted) least squares ﬁt for this expression. One shortcoming of this argument
is that the homomorphism assumption assumes the linear relationships instead of explaining them from a
more basic principle. More importantly, the empirical ﬁt to the homomorphism has nontrivial approximation
error, high enough that it does not imply the desired strong linear relationships.
Levy and Goldberg (2014b) show that empirically, skip-gram vectors satisfy

(cid:104)vw, vw(cid:48)(cid:105) ≈ PMI(w, w(cid:48))

(4.3)

up to some shift. They also give an argument suggesting this relationship must be present if the solution
is allowed to be very high-dimensional. Unfortunately, that argument does not extend to low-dimensional
embeddings. Even if it did, the issue of termwise approximation error remains.

Our explanation. The current paper has introduced a generative model to theoretically explain the
emergence of relationship (4.3). However, as noted after Theorem 2.2, the issue of high approximation error
does not go away either in theory or in the empirical ﬁt. We now show that the isotropy of word vectors
(assumed in the theoretical model and veriﬁed empirically) implies that even a weak version of (4.3) is enough
to imply the emergence of the observed linear relationships in low-dimensional embeddings.

This argument will assume the analogy in question involves a relation that obeys Pennington et al.’s
suggestion in (4.2). Namely, for such a relation R there exists function νR(·) depending only upon R such
that for any a, b satisfying R there is a noise function ξa,b,R(·) for which:

p(χ | a)
p(χ | b)

= νR(χ) · ξa,b,R(χ)

For diﬀerent words χ there is huge variation in (4.4), so the multiplicative noise may be large.

Our goal is to show that the low-dimensional word embeddings have the property that there is a vector
µR such that for every pair of words a, b in that relation, va − vb = µR + noise vector, where the noise vector
is small.

Taking logarithms of (4.4) results in:

log

(cid:19)

(cid:18) p(χ | a)
p(χ | b)

= log(νR(χ)) + ζa,b,R(χ)

Theorem 2.2 implies that the left-hand side simpliﬁes to log

d (cid:104)vχ, va − vb(cid:105) + (cid:15)a,b(χ) where
(cid:15) captures the small approximation errors induced by the inexactness of Theorem 2.2. This adds yet more
noise! Denoting by V the n × d matrix whose rows are the vχ vectors, we rewrite (4.5) as:

(cid:16) p(χ|a)
p(χ|b)

(cid:17)

= 1

V (va − vb) = d log(νR) + ζ (cid:48)

a,b,R

where log(νR) in the element-wise log of vector νR and ζ (cid:48)

a,b,R = d(ζa,b,R − (cid:15)a,b,R) is the noise.

In essence, (4.6) shows that va − vb is a solution to a linear regression in d variables and m constraints,
with ζ (cid:48)
a,b,R being the “noise.” The design matrix in the regression is V , the matrix of all word vectors, which
in our model (as well as empirically) satisﬁes an isotropy condition. This makes it random-like, and thus
solving the regression by left-multiplying by V †, the pseudo-inverse of V , ought to “denoise” eﬀectively. We
now show that it does.

Our model assumed the set of all word vectors satisﬁes bulk properties similar to a set of Gaussian vectors.
The next theorem will only need the following weaker properties. (1) The smallest non-zero singular value

(4.4)

(4.5)

(4.6)

11

(a) SN

(b) GloVe

(c) CBOW

(d) skip-gram

Figure 1: The partition function Zc. The ﬁgure shows the histogram of Zc for 1000 random vectors c of
appropriate norm, as deﬁned in the text. The x-axis is normalized by the mean of the values. The values Zc
for diﬀerent c concentrate around the mean, mostly in [0.9, 1.1]. This concentration phenomenon is predicted
by our analysis.

of V is larger than some constant c1 times the quadratic mean of the singular values, namely, (cid:107)V (cid:107)F /
d.
Empirically we ﬁnd c1 ≈ 1/3 holds; see Section 5. (2) The left singular vectors behave like random vectors
with respect to ζ (cid:48)
a,b,R, for some constant c2.
(3) The max norm of a row in V is O(

a,b,R, namely, have inner product at most c2(cid:107)ζ (cid:48)
√

d). The proof is included in the appendix.

n with ζ (cid:48)

a,b,R(cid:107)/

√

√

Theorem 4.1 (Noise reduction). Under the conditions of the previous paragraph, the noise in the dimension-
reduced semantic vector space satisﬁes

(cid:107)¯ζa,b,R(cid:107)2 (cid:46) (cid:107)ζ (cid:48)

a,b,R(cid:107)2

√

d
n

.

As a corollary, the relative error in the dimension-reduced space is a factor of (cid:112)d/n smaller.

5 Experimental veriﬁcation

In this section, we provide experiments empirically supporting our generative model.

Corpus. All word embedding vectors are trained on the English Wikipedia (March 2015 dump).
It is
pre-processed by standard approach (removing non-textual elements, sentence splitting, and tokenization),
leaving about 3 billion tokens. Words that appeared less than 1000 times in the corpus are ignored, resulting
in a vocabulary of 68, 430. The co-occurrence is then computed using windows of 10 tokens to each side of
the focus word.

Training method. Our embedding vectors are trained by optimizing the SN objective using AdaGrad (Duchi
et al., 2011) with initial learning rate of 0.05 and 100 iterations. The PMI objective derived from (2.5) was
also used. SN has average (weighted) term-wise error of 5%, and PMI has 17%. We observed that SN
vectors typically ﬁt the model better and have better performance, which can be explained by larger errors
in PMI, as implied by Theorem 2.2. So, we only report the results for SN.

For comparison, GloVe and two variants of word2vec (skip-gram and CBOW) vectors are trained. GloVe’s
vectors are trained on the same co-occurrence as SN with the default parameter values.7 word2vec vectors
are trained using a window size of 10, with other parameters set to default values.8

7http://nlp.stanford.edu/projects/glove/
8https://code.google.com/p/word2vec/

12

Figure 2: The linear relationship between the squared norms of our word vectors and the logarithms of
the word frequencies. Each dot in the plot corresponds to a word, where x-axis is the natural logarithm of
the word frequency, and y-axis is the squared norm of the word vector. The Pearson correlation coeﬃcient
between the two is 0.75, indicating a signiﬁcant linear relationship, which strongly supports our mathematical
prediction, that is, equation (2.4) of Theorem 2.2.

5.1 Model veriﬁcation

Experiments were run to test our modeling assumptions. First, we tested two counter-intuitive properties:
the concentration of the partition function Zc for diﬀerent discourse vectors c (see Theorem 2.1), and the
random-like behavior of the matrix of word embeddings in terms of its singular values (see Theorem 4.1).
For comparison we also tested these properties for word2vec and GloVe vectors, though they are trained by
diﬀerent objectives. Finally, we tested the linear relation between the squared norms of our word vectors
and the logarithm of the word frequencies, as implied by Theorem 2.2.

Partition function. Our theory predicts the counter-intuitive concentration of the partition function
Zc = (cid:80)
w(cid:48) exp(c(cid:62)vw(cid:48)) for a random discourse vector c (see Lemma 2.1). This is veriﬁed empirically by
picking a uniformly random direction, of norm (cid:107)c(cid:107) = 4/µw, where µw is the average norm of the word
vectors.9 Figure 1(a) shows the histogram of Zc for 1000 such randomly chosen c’s for our vectors. The
values are concentrated, mostly in the range [0.9, 1.1] times the mean. Concentration is also observed for
other types of vectors, especially for GloVe and CBOW.

Isotropy with respect to singular values. Our theoretical explanation of relations=lines assumes
that the matrix of word vectors behaves like a random matrix with respect to the properties of singular
values. In our embeddings, the quadratic mean of the singular values is 34.3, while the minimum non-zero
singular value of our word vectors is 11. Therefore, the ratio between them is a small constant, consistent
with our model. The ratios for GloVe, CBOW, and skip-gram are 1.4, 10.1, and 3.1, respectively, which are
also small constants.

Squared norms v.s. word frequencies. Figure 2 shows a scatter plot for the squared norms of our
vectors and the logarithms of the word frequencies. A linear relationship is observed (Pearson correlation
0.75), thus supporting Theorem 2.2. The correlation is stronger for high frequency words, possibly because
the corresponding terms have higher weights in the training objective.

9Note that our model uses the inner products between the discourse vectors and word vectors, so it is invariant if the
discourse vectors are scaled by s while the word vectors are scaled by 1/s for any s > 0. Therefore, one needs to choose the
norm of c properly. We assume (cid:107)c(cid:107)µw =
d/κ ≈ 4 for a constant κ = 5 so that it gives a reasonable ﬁt to the predicted
dynamic range of word frequencies according to our theory; see model details in Section 2.

√

13

Relations
semantic
syntactic
total
adjective
noun
verb
total

SN GloVe CBOW skip-gram
0.84
0.61
0.71
0.50
0.69
0.48
0.53

0.73
0.68
0.70
0.58
0.58
0.56
0.57

0.79
0.71
0.74
0.58
0.56
0.64
0.62

0.85
0.65
0.73
0.56
0.70
0.53
0.57

G

M

Table 1: The accuracy on two word analogy task testbeds: G (the GOOGLE testbed); M (the MSR testbed).
Performance is close to the state of the art despite using a generative model with provable properties.

This correlation is much weaker for other types of word embeddings. This is possibly because they have
more free parameters (“knobs to turn”), which imbue the embeddings with other properties. This can also
cause the diﬀerence in the concentration of the partition function for the two methods.

5.2 Performance on analogy tasks

We compare the performance of our word vectors on analogy tasks, speciﬁcally the two testbeds GOOGLE
and MSR (Mikolov et al., 2013a;c). The former contains 7874 semantic questions such as “man:woman::king:??”,
and 10167 syntactic ones such as “run:runs::walk :??.” The latter has 8000 syntactic questions for adjectives,
nouns, and verbs.

To solve these tasks, we use linear algebraic queries.10 That is, ﬁrst normalize the vectors to unit norm

and then solve “a:b::c:??” by

argmin
d

(cid:107)va − vb − vc + vd(cid:107)2
2 .

(5.1)

The algorithm succeeds if the best d happens to be correct.

The performance of diﬀerent methods is presented in Table 1. Our vectors achieve performance compara-
ble to the state of art on semantic analogies (similar accuracy as GloVe, better than word2vec). On syntactic
tasks, they achieve accuracy 0.04 lower than GloVe and skip-gram, while CBOW typically outperforms the
others.11 The reason is probably that our model ignores local word order, whereas the other models capture
it to some extent. For example, a word “she” can aﬀect the context by a lot and determine if the next word
is “thinks” rather than “think ”. Incorporating such linguistic features in the model is left for future work.

5.3 Verifying relations=lines

The theory in Section 4 predicts the existence of a direction for a relation, whereas earlier Levy and Goldberg
(2014a) had questioned if this phenomenon is real. The experiment uses the analogy testbed, where each
relation is tested using 20 or more analogies. For each relation, we take the set of vectors vab = va − vb
where the word pair (a, b) satisﬁes the relation. Then calculate the top singular vectors of the matrix formed
by these vab’s, and compute the cosine similarity (i.e., normalized inner product) of individual vab to the
singular vectors. We observed that most (va − vb)’s are correlated with the ﬁrst singular vector, but have
inner products around 0 with the second singular vector. Over all relations, the average projection on the
ﬁrst singular vector is 0.51 (semantic: 0.58; syntactic: 0.46), and the average on the second singular vector
is 0.035. For example, Table 2 shows the mean similarities and standard deviations on the ﬁrst and second
singular vectors for 4 relations. Similar results are also obtained for word embedings by GloVe and word2vec.
Therefore, the ﬁrst singular vector can be taken as the direction associated with this relation, while the other
components are like random noise, in line with our model.

10One can instead use the 3COSMUL in (Levy and Goldberg, 2014a), which increases the accuracy by about 3%. But it is

not linear while our focus here is the linear algebraic structure.

11It was earlier reported that skip-gram outperforms CBOW (Mikolov et al., 2013a; Pennington et al., 2014). This may be

due to the diﬀerent training data sets and hyperparameters used.

14

relation
1st
2nd
relation
1st
2nd

1
0.65 ± 0.07
0.02 ± 0.28
8
0.56 ± 0.09
0.00 ± 0.22

2
0.61 ± 0.09
0.00 ± 0.23
9
0.53 ± 0.08
0.01 ± 0.26

3
0.52 ± 0.08
0.05 ± 0.30
10
0.37 ± 0.11
0.02 ± 0.20

4
0.54 ± 0.18
0.06 ± 0.27
11
0.72 ± 0.10
0.01 ± 0.24

5
0.60 ± 0.21
0.01 ± 0.24
12
0.37 ± 0.14
0.07 ± 0.26

6
0.35 ± 0.17
0.07 ± 0.24
13
0.40 ± 0.19
0.07 ± 0.23

7
0.42 ± 0.16
0.01 ± 0.25
14
0.43 ± 0.14
0.09 ± 0.23

Table 2: The veriﬁcation of relation directions on 2 semantic and 2 syntactic relations in the GOOGLE
testbed. Relations include cap-com: capital-common-countries; cap-wor: capital-world; adj-adv: gram1-
adjective-to-adverb; opp: gram2-opposite. For each relation, take vab = va − vb for pairs (a, b) in the
relation, and then calculate the top singular vectors of the matrix formed by these vab’s. The row with label
“1st”/“2nd” shows the cosine similarities of individual vab to the 1st/2nd singular vector (the mean and
standard deviation).

w/o RD
RD(k = 20)
RD(k = 30)
RD(k = 40)

SN GloVe CBOW skip-gram
0.71
0.74
0.79
0.76

0.70
0.75
0.80
0.77

0.73
0.77
0.80
0.80

0.74
0.79
0.82
0.80

Table 3: The accuracy of the RD algorithm (i.e., the cheater method) on the GOOGLE testbed. The RD
algorithm is described in the text. For comparison, the row “w/o RD” shows the accuracy of the old method
without using RD.

Cheating solver for analogy testbeds. The above linear structure suggests a better (but cheating) way
to solve the analogy task. This uses the fact that the same semantic relationship (e.g., masculine-feminine,
singular-plural) is tested many times in the testbed. If a relation R is represented by a direction µR then the
cheating algorithm can learn this direction (via rank 1 SVD) after seeing a few examples of the relationship.
Then use the following method of solving “a:b::c:??”: look for a word d such that vc − vd has the largest
projection on µR, the relation direction for (a, b). This can boost success rates by about 10%.

The testbed can try to combat such cheating by giving analogy questions in a random order. But the
cheating algorithm can just cluster the presented analogies to learn which of them are in the same relation.
Thus the ﬁnal algorithm, named analogy solver with relation direction (RD), is: take all vectors va − vb for
all the word pairs (a, b) presented among the analogy questions and do k-means clustering on them; for each
(a, b), estimate the relation direction by taking the ﬁrst singular vector of its cluster, and substitute that
for va − vb in (5.1) when solving the analogy. Table 3 shows the performance on GOOGLE with diﬀerent
values of k; e.g. using our SN vectors and k = 30 leads to 0.79 accuracy. Thus future designers of analogy
testbeds should remember not to test the same relationship too many times! This still leaves other ways to
cheat, such as learning the directions for interesting semantic relations from other collections of analogies.

Non-cheating solver for analogy testbeds. Now we show that even if a relationship is tested only
once in the testbed, there is a way to use the above structure. Given “a:b::c:??,” the solver ﬁrst ﬁnds the
top 300 nearest neighbors of a and those of b, and then ﬁnds among these neighbors the top k pairs (a(cid:48), b(cid:48))
so that the cosine similarities between va(cid:48) − vb(cid:48) and va − vb are largest. Finally, the solver uses these pairs
to estimate the relation direction (via rank 1 SVD), and substitute this (corrected) estimate for va − vb in
(5.1) when solving the analogy. This algorithm is named analogy solver with relation direction by nearest
neighbors (RD-nn). Table 4 shows its performance, which consistently improves over the old method by
about 3%.

15

w/o RD-nn
RD-nn (k = 10)
RD-nn (k = 20)
RD-nn (k = 30)

SN GloVe CBOW skip-gram
0.71
0.71
0.72
0.73

0.70
0.73
0.74
0.74

0.74
0.77
0.77
0.78

0.73
0.74
0.75
0.76

Table 4: The accuracy of the RD-nn algorithm on the GOOGLE testbed. The algorithm is described in the
text. For comparison, the row “w/o RD-nn” shows the accuracy of the old method without using RD-nn.

6 Conclusions

A simple generative model has been introduced to explain the classical PMI based word embedding models,
as well as recent variants involving energy-based models and matrix factorization. The model yields an
optimization objective with essentially “no knobs to turn”, yet the embeddings lead to good performance
on analogy tasks, and ﬁt other predictions of our generative model. A model with fewer knobs to turn
should be seen as a better scientiﬁc explanation (Occam’s razor), and certainly makes the embeddings more
interpretable.

The spatial isotropy of word vectors is both an assumption in our model, and also a new empirical
ﬁnding of our paper. We feel it may help with further development of language models. It is important for
explaining the success of solving analogies via low dimensional vectors (relations=lines). It also implies
that semantic relationships among words manifest themselves as special directions among word embeddings
(Section 4), which lead to a cheater algorithm for solving analogy testbeds.

Our model is tailored to capturing semantic similarity, more akin to a log-linear dynamic topic model.
In particular, local word order is unimportant. Designing similar generative models (with provable and
interpretable properties) with linguistic features is left for future work.

Acknowledgements

We thank the editors of TACL for granting a special relaxation of the page limit for our paper. We thank
Yann LeCun, Christopher D. Manning, and Sham Kakade for helpful discussions at various stages of this
work.

This work was supported in part by NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award,
Simons Collaboration Grant, and ONR-N00014-16-1-2329. Tengyu Ma was supported in addition by Simons
Award in Theoretical Computer Science and IBM PhD Fellowship.

References

Jacob Andreas and Dan Klein. When and why are log-linear models self-normalizing? In Proceedings of the
Annual Meeting of the North American Chapter of the Association for Computational Linguistics, 2014.

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model approach

to PMI-based word embeddings. Technical report, ArXiV, 2015. http://arxiv.org/abs/1502.03520.

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski.

braic structure of word senses, with applications to polysemy.
http://arxiv.org/abs/1502.03520.

Linear alge-
Technical report, ArXiV, 2016.

David Belanger and Sham M. Kakade. A linear dynamical system model for text. In Proceedings of the 32nd

International Conference on Machine Learning, 2015.

Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. Neural

probabilistic language models. In Innovations in Machine Learning. 2006.

16

Fischer Black and Myron Scholes. The pricing of options and corporate liabilities. Journal of Political

David M. Blei. Probabilistic topic models. Communication of the Association for Computing Machinery,

Economy, 1973.

2012.

David M. Blei and John D. Laﬀerty. Dynamic topic models.

In Proceedings of the 23rd International

Conference on Machine Learning, 2006.

Kenneth Ward Church and Patrick Hanks. Word association norms, mutual information, and lexicography.

Computational linguistics, 1990.

Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar. Spectral learning of latent-
variable PCFGs. In Proceedings of the 50th Annual Meeting of the Association for Computational Lin-
guistics: Long Papers-Volume 1, 2012.

Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep neural net-
works with multitask learning. In Proceedings of the 25th International Conference on Machine Learning,
2008a.

Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep neural net-
works with multitask learning. In Proceedings of the 25th International Conference on Machine Learning,
2008b.

Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman.
Indexing by latent semantic analysis. Journal of the American Society for Information Science, 1990.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic

optimization. The Journal of Machine Learning Research, 2011.

John Rupert Firth. A synopsis of linguistic theory. 1957.

Amir Globerson, Gal Chechik, Fernando Pereira, and Naftali Tishby. Euclidean embedding of co-occurrence

data. Journal of Machine Learning Research, 2007.

Tatsunori B. Hashimoto, David Alvarez-Melis, and Tommi S. Jaakkola. Word embeddings as metric recovery

in semantic spaces. Transactions of the Association for Computational Linguistics, 2016.

Thomas Hofmann. Probabilistic latent semantic analysis.

In Proceedings of the Fifteenth Conference on

Uncertainty in Artiﬁcial Intelligence, 1999.

Daniel Hsu, Sham M. Kakade, and Tong Zhang. A spectral algorithm for learning hidden markov models.

Journal of Computer and System Sciences, 2012.

Omer Levy and Yoav Goldberg. Linguistic regularities in sparse and explicit word representations.

In

Proceedings of the Eighteenth Conference on Computational Natural Language Learning, 2014a.

Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in

Neural Information Processing Systems, 2014b.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Compu-
tational Linguistics, 2011.

Yariv Maron, Michael Lamar, and Elie Bienenstock. Sphere embedding: An application to part-of-speech

induction. In Advances in Neural Information Processing Systems, 2010.

17

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey Dean. Eﬃcient estimation of word representations in

vector space. Proceedings of the International Conference on Learning Representations, 2013a.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeﬀ Dean. Distributed representations of
words and phrases and their compositionality. In Advances in Neural Information Processing Systems,
2013b.

Tomas Mikolov, Wen-tau Yih, and Geoﬀrey Zweig. Linguistic regularities in continuous space word rep-
In Proceedings of the Conference of the North American Chapter of the Association for

resentations.
Computational Linguistics: Human Language Technologies, 2013c.

Andriy Mnih and Geoﬀrey Hinton. Three new graphical models for statistical language modelling.

In

Proceedings of the 24th International Conference on Machine Learning, 2007.

Christos H. Papadimitriou, Hisao Tamaki, Prabhakar Raghavan, and Santosh Vempala. Latent semantic
indexing: A probabilistic analysis. In Proceedings of the 7th ACM SIGACT-SIGMOD-SIGART Symposium
on Principles of Database Systems, 1998.

Jeﬀrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word represen-

tation. Proceedings of the Empiricial Methods in Natural Language Processing, 2014.

Douglas L. T. Rohde, Laura M. Gonnerman, and David C. Plaut. An improved model of semantic similarity

based on lexical co-occurence. Communication of the Association for Computing Machinery, 2006.

David E. Rumelhart, Geoﬀrey E. Hinton, and James L. McClelland, editors. Parallel Distributed Processing:

Explorations in the Microstructure of Cognition. 1986.

David E. Rumelhart, Geoﬀrey E. Hinton, and Ronald J. Williams. Learning representations by back-

propagating errors. Cognitive modeling, 1988.

Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. Journal

of Artiﬁcial Intelligence Research, 2010.

18

A Proofs of Theorem 1

In this section we prove Theorem 2.2 and Lemma 2.1 (restated below).

Theorem 2.2. Suppose the word vectors satisfy equation (2.2), and window size q = 2. Then,

for (cid:15) = O((cid:15)z) + (cid:101)O(1/d) + O((cid:15)2). Jointly these imply:

log p(w, w(cid:48)) =

− 2 log Z ± (cid:15),

(cid:107)vw + vw(cid:48)(cid:107)2
2
2d
(cid:107)vw(cid:107)2
2
2d

log p(w) =

− log Z ± (cid:15).

PMI (w, w(cid:48)) =

(cid:104)vw, vw(cid:48)(cid:105)
d

± O((cid:15)).

Lemma 2.1. If the word vectors satisfy the bayesian prior v = s · ˆv, where ˆv is from the spherical Gaussian
distribution, and s is a scalar random variable, then with high probability the entire ensemble of word vectors
satisﬁes that

for (cid:15)z = (cid:101)O(1/

n), and δ = exp(−Ω(log2 n)).

√

Pr
c∼C

[(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z] ≥ 1 − δ,

We ﬁrst prove Theorem 2.2 using Lemma 2.1, and Lemma 2.1 will be proved in Section A.1. Please see
Section 2 of the main paper for the intuition of the proof and a cleaner sketch without too many technicalities.

Proof of Theorem 2.2. Let c be the hidden discourse that determines the probability of word w, and c(cid:48) be
the next one that determines w(cid:48). We use p(c(cid:48)|c) to denote the Markov kernel (transition matrix) of the
Markov chain. Let C be the stationary distribution of discourse vector c, and D be the joint distribution of
(c, c(cid:48)). We marginalize over the contexts c, c(cid:48) and then use the independence of w, w(cid:48) conditioned on c, c(cid:48),

(cid:20) exp((cid:104)vw, c(cid:105))
Zc
We ﬁrst get rid of the partition function Zc using Lemma 2.1. As sketched in the main paper, essentially
we will replace Zc by Z in equation (A.5), though a very careful control of the approximation error is
required. Formally, Let F1 be the event that c satisﬁes

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))
Zc(cid:48)

p(w, w(cid:48)) = E

(A.5)

(c,c(cid:48))∼D

(cid:21)

(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z .

(A.6)

Similarly, let F2 be the even that c(cid:48) satisﬁes (1 − (cid:15)z)Z ≤ Zc(cid:48) ≤ (1 + (cid:15)z)Z, and let F = F1 ∩ F2, and F be its
negation. Moreover, let 1F be the indicator function for the event F. Therefore by Lemma 2.1 and union
bound, we have E[1F ] = Pr[F] ≥ 1 − exp(−Ω(log2 n)).

We ﬁrst decompose the integral (A.5) into the two parts according to whether event F happens,

We bound the ﬁrst quantity on the right hand side using (2.2) and the deﬁnition of F.

p(w, w(cid:48)) = E

(c,c(cid:48))∼D

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

+ E

(c,c(cid:48))∼D

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:20)

(cid:20)

1
ZcZc(cid:48)
1
ZcZc(cid:48)

(cid:21)

(cid:21)

(cid:20)

E
(c,c(cid:48))∼D

1
ZcZc(cid:48)
≤ (1 + (cid:15)z)2 1
Z 2

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:21)

E
(c,c(cid:48))∼D

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ]

19

(A.1)

(A.2)

(A.3)

(A.4)

(A.7)

(A.8)

For the second quantity of the right hand side of (A.7), we have by Cauchy-Schwartz,

1
ZcZc(cid:48)
(cid:20) 1
Z 2
c

(cid:18)

(cid:20)

E
(c,c(cid:48))∼D
(cid:18)

≤

≤

(cid:18)

E
(c,c(cid:48))∼D
(cid:20) 1
Z 2
c

E
c

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:21)(cid:19)2

exp((cid:104)vw, c(cid:105))21F

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

[1F ]

(cid:21)(cid:19) (cid:18)

(cid:21)(cid:19) (cid:18)

E
(c,c(cid:48))∼D
(cid:20) 1
Z 2
c(cid:48)

E
c(cid:48)

(cid:20)(cid:90)

1
Z 2
c(cid:48)

c,c(cid:48)

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))21F

(cid:21)(cid:19)

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))2 E
c|c(cid:48)

[1F ]

(cid:21)(cid:19)

.

(A.9)

Using the fact that Zc ≥ 1, then we have that

(cid:20) 1
Z 2
c

E
c

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

(cid:21)
[1F ]

(cid:20)
exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

(cid:21)
[1F ]

≤ E
c

We can split that expectation as
(cid:20)
exp((cid:104)vw, c(cid:105))21(cid:104)vw,c(cid:105)>0 E
c(cid:48)|c

E
c

The second term of (A.10) is upper bounded by

(cid:21)
[1F ]

(cid:20)
exp((cid:104)vw, c(cid:105))21(cid:104)vw,c(cid:105)<0 E
c(cid:48)|c

(cid:21)
[1F ]

.

+ E
c

(A.10)

We proceed to the ﬁrst term of (A.10) and observe the following property of it:

(cid:20)
exp((cid:104)vw, c(cid:105))21(cid:104)vw,c(cid:105)>0 E
c(cid:48)|c

E
c

(cid:21)

(cid:20)

[1F ]

≤ E
c

exp((cid:104)αvw, c(cid:105))21(cid:104)vw,c(cid:105)>0 E
c(cid:48)|c

[1F ]

≤ E
c

(cid:21)

(cid:20)
exp((cid:104)αvw, c(cid:105))2 E
c(cid:48)|c

(cid:21)

[1F ]

where α > 1. Therefore, it’s suﬃcient to bound

[1F ] ≤ exp(−Ω(log2 n))

E
c,c(cid:48)

(cid:20)

E
c

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

[1F ]

(cid:21)

when (cid:107)vw(cid:107) = Ω(

d).

√

Let’s denote by z the random variable 2 (cid:104)vw, c(cid:105).
Let’s denote r(z) = Ec(cid:48)|z[1F ] which is a function of z between [0, 1]. We wish to upper bound Ec [exp(z)r(z)].

The worst-case r(z) can be quantiﬁed using a continuous version of Abel’s inequality as proven in Lemma A.4,
which gives

[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3)
E
c

(A.11)

where t satisﬁes that Ec[1[t,+∞]] = Pr[z ≥ t] = Ec[r(z)] ≤ exp(−Ω(log2 n)). Then, we claim Pr[z ≥ t] ≤
exp(−Ω(log2 n)) implies that t ≥ Ω(log.9 n).

If c were distributed as N (0, 1

d I), this would be a simple tail bound. However, as c is distributed uniformly
on the sphere, this requires special care, and the claim follows by applying Lemma A.1 instead. Finally,
applying Corollary A.3, we have:

E[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(log1.8 n))
We have the same bound for c(cid:48) as well. Hence, for the second quantity of the right hand side of (A.7),

(A.12)

we have

exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F

(cid:21)

(cid:20)

1
ZcZc(cid:48)

E
(c,c(cid:48))∼D
(cid:18)

≤

E
c

(cid:20) 1
Z 2
c
≤ exp(−Ω(log1.8 n))

exp((cid:104)vw, c(cid:105))2 E
c(cid:48)|c

(cid:21)(cid:19)1/2 (cid:18)

[1F ]

(cid:20) 1
Z 2
c(cid:48)

E
c(cid:48)

exp((cid:104)vw(cid:48), c(cid:48)(cid:105))2 E
c|c(cid:48)

[1F ]

(cid:21)(cid:19)1/2

(A.13)

20

where the ﬁrst inequality follows from Cauchy-Schwartz, and the second from the calculation above. Com-
bining (A.7), (A.8) and (A.13), we obtain

p(w, w(cid:48)) ≤ (1 + (cid:15)z)2 1
Z 2
≤ (1 + (cid:15)z)2 1
Z 2

E
(c,c(cid:48))∼D

E
(c,c(cid:48))∼D

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ] +

1
n2 exp(−Ω(log1.8 n))

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] + δ0

where δ0 = exp(−Ω(log1.8 n))Z 2 ≤ exp(−Ω(log1.8 n)) by the fact that Z ≤ exp(2κ)n = O(n). Note that
κ is treated as an absolute constant throughout the paper. On the other hand, we can lowerbound similarly

p(w, w(cid:48)) ≥ (1 − (cid:15)z)2 1
Z 2
≥ (1 − (cid:15)z)2 1
Z 2
≥ (1 − (cid:15)z)2 1
Z 2

E
(c,c(cid:48))∼D

E
(c,c(cid:48))∼D

E
(c,c(cid:48))∼D

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))1F ]

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] −

1
n2 exp(−Ω(log1.8 n))

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] − δ0

Taking logarithm, the multiplicative error translates to a additive error

log p(w, w(cid:48)) = log

[exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:48)(cid:105))] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

(cid:18)

E
(c,c(cid:48))∼D

For the purpose of exploiting the fact that c, c(cid:48) should be close to each other, we further rewrite log p(w, w(cid:48))
by re-organizing the expectations above,

log p(w, w(cid:48)) = log

± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

= log

[exp((cid:104)vw, c(cid:105))A(c)] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

(A.14)

(cid:20)
exp((cid:104)vw, c(cid:105)) E
c(cid:48)|c

(cid:21)
[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

(cid:19)

(cid:18)

(cid:18)

E
c

E
c

A(c) := E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

where the inner integral which is denoted by A(c),

Since (cid:107)vw(cid:107) ≤ κ

d. Therefore we have that (cid:104)vw, c − c(cid:48)(cid:105) ≤ (cid:107)vw(cid:107)(cid:107)c − c(cid:48)(cid:107) ≤ κ

d(cid:107)c − c(cid:48)(cid:107).

√

√

Then we can bound A(c) by

(cid:19)

(cid:19)

A(c) = E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48)(cid:105))]

= exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

≤ exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

[exp((cid:104)vw(cid:48), c(cid:48) − c(cid:105))]

√

[exp(κ

d(cid:107)c − c(cid:48)(cid:107))]

≤ (1 + (cid:15)2) exp((cid:104)vw(cid:48), c(cid:105))

21

where the last inequality follows from our model assumptions. To derive a lower bound of A(c), observe that

√

[exp(κ

E
c(cid:48)|c

d(cid:107)c − c(cid:48)(cid:107))] + E
c(cid:48)|c

√

[exp(−κ

d(cid:107)c − c(cid:48)(cid:107))] ≥ 2

Therefore, our model assumptions imply that

Hence,

√

[exp(−κ

d(cid:107)c − c(cid:48)(cid:107))] ≥ 1 − (cid:15)2

E
c(cid:48)|c

A(c) = exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

exp((cid:104)vw(cid:48), c(cid:48) − c(cid:105))
√

exp(−κ

d(cid:107)c − c(cid:48)(cid:107))

≥ exp((cid:104)vw(cid:48), c(cid:105)) E
c(cid:48)|c

≥ (1 − (cid:15)2) exp((cid:104)vw(cid:48), c(cid:105))

Therefore, we obtain that A(c) = (1 ± (cid:15)2) exp((cid:104)vw(cid:48), c(cid:105)). Plugging the just obtained estimate of A(c) into

the equation (A.14), we get that

(cid:18)

(cid:18)

(cid:18)

E
c

E
c

E
c

E
c

log p(w, w(cid:48)) = log

[exp((cid:104)vw, c(cid:105))A(c)] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

= log

[(1 ± (cid:15)2) exp((cid:104)vw, c(cid:105)) exp((cid:104)vw(cid:48), c(cid:105))] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z)

(cid:19)

= log

[exp((cid:104)vw + vw(cid:48), c(cid:105))] ± δ0

− 2 log Z + 2 log(1 ± (cid:15)z) + log(1 ± (cid:15)2)

(A.15)

Now it suﬃces to compute Ec[exp((cid:104)vw + vw(cid:48), c(cid:105))]. Note that if c had the distribution N (0, 1

d I), which
is very similar to uniform distribution over the sphere, then we could get straightforwardly Ec[exp((cid:104)vw +
vw(cid:48), c(cid:105))] = exp((cid:107)vw + vw(cid:48)(cid:107)2/(2d)). For c having a uniform distribution over the sphere, by Lemma A.5, the
same equality holds approximately,

[exp((cid:104)vw + vw(cid:48), c(cid:105))] = (1 ± (cid:15)3) exp((cid:107)vw + vw(cid:48)(cid:107)2/(2d))

(A.16)

where (cid:15)3 = (cid:101)O(1/d).

Plugging in equation (A.16) into equation (A.15), we have that
log p(w, w(cid:48)) = log (cid:0)(1 ± (cid:15)3) exp((cid:107)vw + vw(cid:48)(cid:107)2/(2d)) ± δ0

(cid:1) − 2 log Z + 2 log(1 ± (cid:15)z) + log(1 ± (cid:15)2)

= (cid:107)vw + vw(cid:48)(cid:107)2/(2d) + O((cid:15)3) + O(δ(cid:48)

0) − 2 log Z ± 2(cid:15)z ± (cid:15)2

where δ(cid:48)

0 = δ0 · (Ec∼C[exp((cid:104)vw + vw(cid:48), c(cid:105))])−1 = exp(−Ω(log1.8 n)). Note that (cid:15)3 = (cid:101)O(1/d), (cid:15)z = (cid:101)O(1/

√

n),

and (cid:15)2 by assumption, therefore we obtain that

log p(w, w(cid:48)) =

(cid:107)vw + vw(cid:48)(cid:107)2 − 2 log Z ± O((cid:15)z) + O((cid:15)2) + (cid:101)O(1/d).

1
2d

The following lemmas are helper lemmas that were used in the proof above. We use Cd to denote the

uniform distribution over the unit sphere in Rd.

Lemma A.1 (Tail bound for spherical distribution). If c ∼ Cd, v ∈ Rd is a vector with (cid:107)v(cid:107) = Ω(
t = ω(1), the random variable z = (cid:104)v, c(cid:105) satisﬁes Pr[z ≥ t] = e−O(t2).

√

d) and

(cid:19)

(cid:19)

22

Proof. If c = (c1, c2, . . . , cd) ∼ Cd, c is in distribution equal to
samples from a univariate Gaussian with mean 0 and variance 1
that v = ((cid:107)v(cid:107), 0, . . . , 0). Let’s introduce the random variable r = (cid:80)d

(cid:16) ˜c1
(cid:107)˜c(cid:107) , ˜c2

(cid:107)˜c(cid:107) , . . . , ˜cd

where the ˜ci are i.i.d.
d . By spherical symmetry, we may assume

(cid:107)˜c(cid:107)

(cid:17)

Pr [(cid:104)v, c(cid:105) ≥ t] = Pr

(cid:107)v|

≥ t

≤ Pr

≥ t | r ≥

Pr

r ≥

+ Pr

≥ t | r ≥

Pr

r ≥

(cid:20)

(cid:21)

˜c1
(cid:107)˜c(cid:107)

(cid:20) (cid:107)v(cid:107)˜c1
(cid:107)˜c(cid:107)

(cid:20)

(cid:21)

1
2

(cid:20)

(cid:21)

1
2

(cid:21)

1
2

it’s suﬃcient to lower bound Pr [r ≤ 100] and Pr

. The former probability is easily

seen to be lower bounded by a constant by a Chernoﬀ bound. Consider the latter one next. It holds that

(cid:104)

(cid:107)v(cid:107) ˜c1

(cid:107)c(cid:107) ≥ t | r ≤ 100

(cid:105)

i=2 ˜c2
(cid:21)

1
2

i . Since

(cid:20) (cid:107)v(cid:107)˜c1
(cid:107)˜c(cid:107)

Pr

(cid:107)v(cid:107)

≥ t | r ≤ 100

= Pr

˜c1 ≥

(cid:21)

(cid:34)

(cid:115)

t2 · r

(cid:107)v(cid:107)2 − t2 | r ≤ 100

(cid:35)

(cid:34)

(cid:115)

≥ Pr

˜c1 ≥

(cid:35)

100t2
(cid:107)v(cid:107)2 − t2

(cid:20)

˜c1
(cid:107)˜c(cid:107)

Denoting ˜t =

(cid:113) 100t2

(cid:107)v(cid:107)2−t2 , by a well-known Gaussian tail bound it follows that

Pr (cid:2)˜c1 ≥ ˜t(cid:3) = e−O(d˜t2)

(cid:32)

1
√
d˜t

(cid:18) 1
√

−

d˜t

(cid:19)3(cid:33)

= e−O(t2)

where the last equality holds since (cid:107)v(cid:107) = Ω(

d) and t = ω(1).

√

√

Lemma A.2. If c ∼ Cd, v ∈ Rd is a vector with (cid:107)v(cid:107) = Θ(
satisﬁes

d) and t = ω(1), the random variable z = (cid:104)v, c(cid:105)

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(t2)) + exp(−Ω(d))

Proof. Similarly as in Lemma A.1, if c = (c1, c2, . . . , cd) ∼ Cd, c is in distribution equal to
where the ˜ci are i.i.d. samples from a univariate Gaussian with mean 0 and variance 1
symmetry, we may assume v = ((cid:107)v(cid:107), 0, . . . , 0). Let’s introduce the random variable r = (cid:80)d
an arbitrary u > 1, some algebraic manipulation shows

(cid:16) ˜c1
(cid:107)˜c(cid:107) , ˜c2

(cid:107)˜c(cid:107) , . . . , ˜cd

(cid:107)˜c(cid:107)

(cid:17)

d . Again, by spherical
i . Then, for

i=2 ˜c2

Pr (cid:2)exp ((cid:104)v, c(cid:105)) 1[t,+∞]((cid:104)v, c(cid:105)) ≥ u(cid:3) = Pr [exp ((cid:104)v, c(cid:105)) ≥ u ∧ (cid:104)v, c(cid:105) ≥ t] =

(cid:20)

(cid:18)

Pr

exp

(cid:107)v(cid:107)

(cid:19)

˜c1
(cid:107)˜c(cid:107)

˜c1
(cid:107)˜c(cid:107)

≥ u ∧ (cid:107)v(cid:107)

≥ u

= Pr

˜c1 = max

(cid:21)

(cid:34)

(cid:32)(cid:115)

(cid:115)

(cid:33)(cid:35)

˜u2r
(cid:107)v(cid:107)2 − ˜u2 ,

t2r
(cid:107)v(cid:107)2 − t2

(A.17)

√

where we denote ˜u = log u. Since ˜c1 is a mean 0 univariate Gaussian with variance 1
have ∀x ∈ R

d , and (cid:107)v(cid:107) = Ω(

d) we

(cid:34)

(cid:115)

Pr

˜c1 ≥

(cid:35)

x2r
(cid:107)v(cid:107)2 − u2

(cid:16)

e−Ω(x2r)(cid:17)

= O

Next, we show that r is lower bounded by a constant with probability 1 − exp(−Ω(d)).
distribution equal to 1

Indeed, r is in
k is a Chi-squared distribution with k degrees of freedom. Standard
d ] ≤ exp(−ξ). Taking ξ = αd for α a constant

concentration bounds (?) imply that ∀ξ ≥ 0, Pr[r − 1 ≤ −2
implies that with probability 1 − exp(−Ω(d)), r ≥ M for some constant M . We can now rewrite

d−1, where χ2

d χ2

(cid:113) ξ

(cid:34)

(cid:115)

Pr

˜c1 ≥

x2r
(cid:107)v(cid:107)2 − x2

(cid:35)

=

23

(cid:34)

(cid:115)

Pr

˜c1 ≥

x2r

(cid:107)v(cid:107)2 − x2 | r ≥ M

(cid:35)

Pr[r ≥ M ] + Pr

˜c1 ≥

(cid:34)

(cid:115)

x2r

(cid:107)v(cid:107)2 − x2 | r ≤ M

(cid:35)

Pr[r ≤ M ]

The ﬁrst term is clearly bounded by e−Ω(x2) and the second by exp(−Ω(d)). Therefore,

(cid:34)

(cid:115)

Pr

˜c1 ≥

(cid:35)

x2r
(cid:107)v(cid:107)2 − x2

= O (cid:0)max (cid:0)exp (cid:0)−Ω (cid:0)x2(cid:1)(cid:1) , exp (−Ω (d))(cid:1)(cid:1)

(A.18)

Putting A.17 and A.18 together, we get that

Pr (cid:2)exp ((cid:104)v, c(cid:105)) 1[t,+∞]((cid:104)v, c(cid:105)) ≥ u(cid:3) = O

(cid:16)

(cid:16)

(cid:16)

(cid:16)

max

exp

−Ω

min

(cid:16)

d, (max (˜u, t))2(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)

(A.19)

(where again, we denote ˜u = log u)

For any random variable X which has non-negative support, it’s easy to check that

E[X] =

Pr[X ≥ x]dx

(cid:90) ∞

0

Hence,

E (cid:2)exp(z)1[t,+∞](z)(cid:3) =

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du =

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du

(cid:90) ∞

0

(cid:90) exp((cid:107)v(cid:107))

0

To bound this integral, we split into the following two cases:

• Case t2 ≥ d: max (˜u, t) ≥ t, so min

(cid:16)

d, (max (˜u, t))2(cid:17)

= d. Hence, A.19 implies

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp((cid:107)v(cid:107)) exp(−Ω(d)) = exp(−Ω(d))

where the last inequality follows since (cid:107)v(cid:107) = O(

d).

√

• Case t2 < d: In the second case, we will split the integral into two portions: u ∈ [0, exp(t)] and

u ∈ [exp(t), exp((cid:107)v(cid:107))].
When u ∈ [0, exp(t)], max (˜u, t) = t, so min(d, (max (˜u, t))2) = t2. Hence,

(cid:90) exp(t)

0

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du ≤ exp(t) exp(−Ω(t2)) = − exp(Ω(t2))

When u ∈ [exp(t), exp((cid:107)v(cid:107))], max (˜u, t) = ˜u. But ˜u ≤ log(exp((cid:107)v(cid:107))) = O(
˜u. Hence,

(cid:90) exp((cid:107)v(cid:107))

exp(t)

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du ≤

exp(−(log(u))2)du

(cid:90) exp((cid:107)v(cid:107))

exp(t)

Making the change of variable ˜u = log(u), the we can rewrite the last integral as

√

d), so min(d, (max (˜u, t))2) =

(cid:90) (cid:107)v(cid:107)

t

exp(−˜u2) exp(˜u)d˜u = O(exp(−t2))

where the last inequality is the usual Gaussian tail bound.

In either case, we get that

(cid:90) exp((cid:107)v(cid:107))

0

Pr (cid:2)exp(z)1[t,+∞](z) ≥ u(cid:3) du = exp(−Ω(t2)) + exp(−Ω(d)))

which is what we want.

24

As a corollary to the above lemma, we get the following:

Corollary A.3. If c ∼ Cd, v ∈ Rd is a vector with (cid:107)v(cid:107) = Θ(

d) and t = Ω(log.9 n) then

√

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(log1.8 n))

Proof. We claim the proof is trivial if d = o(log4 n).
exp(O(

d)). Hence,

√

Indeed,

in this case, exp((cid:104)v, c(cid:105)) ≤ exp((cid:107)v(cid:107)) =

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(O(

d)) E[1[t,+∞](z)] = exp(O(

d)) Pr[z ≥ t]

√

Since by Lemma A.1, Pr[z ≥ t] ≤ exp(−Ω(log2 n), we get

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(O(

d) − Ω(log2 n)) = exp(−Ω(log1.8 n))

as we wanted.

So, we may, without loss of generality assume that d = Ω(log4 n). In this case, Lemma A.2 implies

E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(− log1.8 n) + exp(−Ω(d))) = exp(− log1.8 n)

where the last inequality holds because d = Ω(log4 n) and t2 = Ω(log.9 n), so we get the claim we wanted.

√

√

Lemma A.4 (Continuous Abel’s Inequality). Let 0 ≤ r(x) ≤ 1 be a function such that such that E[r(x)] = ρ.
Moreover, suppose increasing function u(x) satisﬁes that E[|u(x)|] < ∞. Let t be the real number such that
E[1[t,+∞]] = ρ. Then we have

E[u(x)r(x)] ≤ E[u(x)1[t,+∞]]

(A.20)

z f (x)r(x)dx, and H(z) = (cid:82) ∞

Proof. Let G(z) = (cid:82) ∞
z f (x)1[t,+∞](x)dx. Then we have that G(z) ≤ H(z) for
all z. Indeed, for z ≥ t, this is trivial since r(z) ≤ 1. For z ≤ t, we have H(z) = E[1[t,+∞]] = ρ = E[r(x)] ≥
(cid:82) ∞
z f (x)r(x)dx. Then by integration by parts we have,
(cid:90) ∞

(cid:90) ∞

u(x)f (x)r(x)dx = −

u(x)dG

−∞

−∞

= −u(x)G(x) |∞

−∞ +

G(x)u(cid:48)(x)dx

(cid:90) +∞

−∞

≤

=

(cid:90) +∞

−∞
(cid:90) ∞

−∞

H(x)u(cid:48)(x)dx

u(x)f (x)1[t,+∞](x)dx,

where at the third line we use the fact that u(x)G(x) → 0 as x → ∞ and that u(cid:48)(x) ≥ 0, and at the last line
we integrate by parts again.

Lemma A.5. Let v ∈ Rd be a ﬁxed vector with norm (cid:107)v(cid:107) ≤ κ
variable c with uniform distribution over the sphere, we have that

√

d for absolute constant κ. Then for random

log E[exp((cid:104)v, d(cid:105))] = (cid:107)v(cid:107)2/2d ± (cid:15)

(A.21)

where (cid:15) = (cid:101)O( 1

d ).

25

Proof. Let g ∈ N (0, I), then g/(cid:107)g(cid:107) has the same distribution as c. Let r = (cid:107)v(cid:107). Since c is spherically
symmetric, we could, we can assume without loss of generality that v = (r, 0, . . . , 0). Let x = g1 and
y = (cid:112)g2
d. Therefore x ∈ N (0, 1) and y2 has χ2 distribution with mean d − 1 and variance O(d).
d. Note that the Pr[F] ≥ 1−exp(−Ω(log1.8(d)).

Let F be the event that x ≤ 20 log d and 1.5

2 + · · · + g2

d ≥ y ≥ 0.5

√

√

By Proposition 2, we have that E[exp((cid:104)v, c(cid:105))] = E[exp((cid:104)v, c(cid:105)) | F] · (1 ± Ω(− log1.8 d)).

Conditioned on event F, we have

E[exp((cid:104)v, c(cid:105)) | F] = E

exp(

(cid:35)

rx
(cid:112)x2 + y2

) | F

(cid:34)

(cid:34)

(cid:34)

(cid:20)

= E

exp(

−

rx3
y(cid:112)x2 + y2(y + (cid:112)x2 + y2)

) | F

(cid:35)

= E

exp(

) · exp(

rx3
y(cid:112)x2 + y2(y + (cid:112)x2 + y2)

) | F

(cid:35)

= E

exp(

) | F

· (1 ± O(

(cid:21)

))

log3 d
d
√

√

rx
y

rx
y

rx
y

where we used the fact that r ≤ κ
we have that

√

d. Let E be the event that 1.5

d ≥ y ≥ 0.5

d. By using Proposition 1,

E[exp(rx/y) | F] = E[exp(rx/y) | E] ± exp(−Ω(log2(d))

(A.23)

Then let z = y2/(d − 1) and w = z − 1. Therefore z has χ2 distribution with mean 1 and variance 1/(d − 1),
and w has mean 0 and variance 1/(d − 1).

(A.22)

E

exp(

) | E

= E[E[exp(rx/y) | y] | E] = E[exp(r2/y2) | E]

(cid:20)

(cid:21)

rx
y

= E[exp(r2/(d − 1) · 1/z2) | E]

= E[exp(r2/(d − 1) · (1 +

= exp(r2/(d − 1)) E[exp(1 +

2w + w2
(1 + w)2 )) | E]
2w + w2
(1 + w)2 )) | E]

= exp(r2/(d − 1)) E[1 + 2w ± O(w2) | E]
= exp(r2/(d − 1)2)(1 ± 1/d)

where the second-to-last line uses the fact that conditioned on 1/2 ≥ E, w ≥ −1/2 and therefore the Taylor
expansion approximates the exponential accurately, and the last line uses the fact that | E[w | E]| = O(1/d)
and E[w2 | E] ≤ O(1/d). Combining the series of approximations above completes the proof.

We ﬁnally provide the proofs for a few helper propositions on conditional probabilities for high probability

events used in the lemma above.

Proposition 1. Suppose x ∼ N (0, σ2) with σ = O(1). Then for any event E with Pr[E] = 1−O(−Ω(log2 d)),
we have that E[exp(x)] = E[exp(x) | E] ± exp(−Ω(log2(d)).

Proof. Let’s denote by ¯E the complement of the event E. We will consider the upper and lower bound
separately. Since

E[exp(x)] = E[exp(x) | E] Pr[E] + E[exp(x) | ¯E] Pr[ ¯E]

we have that

E[exp(x)] ≤ E[exp(x) | E] + E[exp(x) | ¯E] Pr[ ¯E]

(A.24)

26

and

E[exp(x)] ≥ E[exp(x) | E](1 − exp(−Ω(log2 d))) ≥ E[exp(x) | E] − E[exp(x) | E] exp(−Ω(log2 d)))

(A.25)

Consider the upper bound (A.24) ﬁrst. To show the statement of the lemma, it suﬃces to bound

E[exp(x) | ¯E] Pr[ ¯E].

Working towards that, notice that

E[exp(x) | ¯E] Pr[ ¯E] = E[exp(x)1 ¯E ] = E[exp(x) E[1 ¯E |x]] = E[exp(x)r(x)]

if we denote r(x) = E[1 ¯E |x]. We wish to upper bound E[exp(x)r(x)]. By Lemma A.4, we have

E[exp(x)r(x)] ≤ E[exp(x)1[t,∞]]

where t is such that E[1[t,∞]] = E[r(x)]. However, since E[r(x)] = Pr[ ¯E] = exp(−Ω(log2 d)), it must be the
case that t = Ω(log d) by the standard Gaussian tail bound, and the assumption that σ = O(1). In turn,
this means

E[exp(x)1[t,∞]] ≤

(cid:90) ∞

1
√
2π

σ

t

exe− x2

σ2 dx =

(cid:90) ∞

1
√
2π

σ

t

e−( x

σ − σ

2 )2+ σ2

4 dx = e

σ2
4

e−(x(cid:48)− σ

2 )2

dx(cid:48)

1
√
2π

(cid:90) +∞

t/σ

where the last equality follows from the change of variables x = σx(cid:48). However,

1
√
2π

(cid:90) +∞

t/σ

e−(x(cid:48)− σ

2 )2

dx(cid:48)

is nothing more than Pr[x(cid:48) > t
1. Bearing in mind that σ = O(1)

σ ], where x(cid:48) is distributed like a univariate gaussian with mean σ

2 and variance

σ2
4

e

1
√
2π

(cid:90) +∞

t/σ

e−(x(cid:48)− σ

2 )2

dx(cid:48) = exp(−Ω(t2)) = exp(−Ω(log2 d))

by the usual Gaussian tail bounds, which proves the lower bound we need.

We proceed to consider the lower bound A.25. To show the statement of the lemma, we will bound

E[exp(x) | E]. Notice trivially that since exp(x) ≥ 0,

E[exp(x) | E] ≤

E[exp(x)]
Pr[E]

Since Pr[E] ≥ 1 − exp(Ω(log2 d)),

1

Pr[E] ≤ 1 + exp(O(log2)). So, it suﬃces to bound E[exp(x)]. However,

E[exp(x)] =

1
√
2π

σ

(cid:90) +∞

t=−∞

exe− x2

σ2 dx =

1
√
2π

σ

(cid:90) +∞

t=−∞

e−( x

σ − σ

2 )2+ σ2

4 dx =

e−(x(cid:48)− σ

2 )2+ σ2

4 dx(cid:48)

1
√
2π

(cid:90) +∞

t=−∞

where the last equality follows from the same change of variables x = σx(cid:48) as before. Since (cid:82) +∞
√

t=−∞ e−(x(cid:48)− σ

2 )2

dx(cid:48) =

2π, we get

(cid:90) +∞

1
√
2π

e−(x(cid:48)− σ

2 )2+ σ2

4 dx(cid:48) = e

σ2
4 = O(1)

t=−∞
Pr[E] , we get that E[exp(x) | E] = O(1). Plugging this back in A.25,

1

Putting together with the estimate of
we get the desired upper bound.

Proposition 2. Suppose c ∼ C and v is an arbitrary vector with (cid:107)v(cid:107) = O(
Pr[E] ≥ 1 − exp(−Ω(log2 d)), we have that E[exp((cid:104)v, c(cid:105))] = E[exp((cid:104)v, c(cid:105)) | E] ± exp(− log1.8 d).

d). Then for any event E with

√

27

Proof of Proposition 2. Let z = (cid:104)v, c(cid:105). We proceed similarly as in the proof of Proposition 1. We have

and

and

E[exp(z)] = E[exp(z) | E] Pr[E] + E[exp(z) | ¯E] Pr[ ¯E]

E[exp(z)] ≤ E[exp(z) | E] + E[exp(z) | ¯E] Pr[ ¯E]

E[exp(z)] ≥ E[exp(z) | E] Pr[E] = E[exp(z) | E] − E[exp(z) | E] exp(−Ω(log2 d))

We again proceed by separating the upper and lower bound.

We ﬁrst consider the upper bound A.26.
Notice that that

E[exp(z) | ¯E] Pr[ ¯E] = E[exp(z)1 ¯E ]

We can split the last expression as

E (cid:2)exp((cid:104)vw, c(cid:105))1(cid:104)vw,c(cid:105)>01E

(cid:3) + E (cid:2)exp((cid:104)vw, c(cid:105))1(cid:104)vw,c(cid:105)<01E

(cid:3) .

The second term is upper bounded by

E[1E ] ≤ exp(−Ω(log2 n))

We proceed to the ﬁrst term of (A.10) and observe the following property of it:

E (cid:2)exp((cid:104)vw, c(cid:105))1(cid:104)vw,c(cid:105)>01E

(cid:3) ≤ E (cid:2)exp((cid:104)αvw, c(cid:105))1(cid:104)vw,c(cid:105)>0 1E

(cid:3) ≤ E [exp((cid:104)αvw, c(cid:105))1E ]

(A.26)

(A.27)

where α > 1. Therefore, it’s suﬃcient to bound

√

when (cid:107)vw(cid:107) = Θ(

d). Let’s denote r(z) = E[1 ¯E |z].

Using Lemma A.4, we have that

E [exp(z)1E ]

[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3)
E
c

(A.28)

where t satisﬁes that Ec[1[t,+∞]] = Pr[z ≥ t] = Ec[r(z)] ≤ exp(−Ω(log2 d)). Then, we claim Pr[z ≥ t] ≤
exp(−Ω(log2 d)) implies that t ≥ Ω(log.9 d).

Indeed, this follows by directly applying Lemma A.1. Afterward, applying Lemma A.2, we have:

E[exp(z)r(z)] ≤ E (cid:2)exp(z)1[t,+∞](z)(cid:3) = exp(−Ω(log1.8 d))

(A.29)

which proves the upper bound we want.

We now proceed to the lower bound A.27, which is again similar to the lower bound in the proof of

Proposition 1: we just need to bound E[exp(z) | E]. Same as in Proposition 1, since exp(x) ≥ 0,

E[exp(z) | E] ≤

E[exp(z)]
Pr[E]

Consider the event E (cid:48) : z ≤ t, for t = Θ(log.9 d), which by Lemma A.1 satisﬁes Pr[E (cid:48)] ≥ 1 − exp(−Ω(log2 d)).
By the upper bound we just showed,

E[exp(z)] ≤ E[exp(z) | E (cid:48)] + exp(−Ω(log2 n)) = O(exp(log.9 d))

where the last equality follows since conditioned on E (cid:48), z = O(log.9 d). Finally, this implies
1
Pr[E]

O(exp(log.9 d)) = O(exp(log.9 d))

E[exp(z) | E] ≤

where the last equality follows since Pr[E] ≥ 1 − exp(−Ω(log2 n)). Putting this together with A.27, we get
E[exp(z)] ≥ E[exp(z) | E] Pr[E] = E[exp(z) | E] − E[exp(z) | E] exp(−Ω(log2 d)) ≥
E[exp(z) | E] − O(exp(log.9 d)) exp(−Ω(log2 d)) ≥ E[exp(z) | E] − exp(−Ω(log2 d))

which is what we needed.

28

A.1 Analyzing partition function Zc

In this section, we prove Lemma 2.1. We basically ﬁrst prove that for the means of Zc are all (1 + o(1))-close
to each other, and then prove that Zc is concentrated around its mean. It turns out the concentration part
is non trivial because the random variable of concern, exp((cid:104)vw, c(cid:105)) is not well-behaved in terms of the tail.
Note that exp((cid:104)vw, c(cid:105)) is NOT sub-gaussian for any variance proxy. This essentially disallows us to use an
existing concentration inequality directly. We get around this issue by considering the truncated version of
exp((cid:104)vw, c(cid:105)), which is bounded, and have similar tail properties as the original one, in the regime that we
are concerning.

We bound the mean and variance of Zc ﬁrst in the Lemma below.

Lemma A.6. For any ﬁxed unit vector c ∈ Rd, we have that E[Zc] ≥ n and V[Zc] ≤ O(n).

Proof of Lemma A.6. Recall that by deﬁnition

Zc =

exp((cid:104)vw, c(cid:105)).

(cid:88)

w

We ﬁx context c and view vw’s as random variables throughout this proof. Recall that vw is composed of
vw = sw · ˆvw, where sw is the scaling and ˆvw is from spherical Gaussian with identity covariance Id×d. Let
s be a random variable that has the same distribution as sw.

We lowerbound the mean of Zc as follows:

E[Zc] = n E [exp((cid:104)vw, c(cid:105))] ≥ n E [1 + (cid:104)vw, c(cid:105)] = n

where the last equality holds because of the symmetry of the spherical Gaussian distibution. On the other
hand, to upperbound the mean of Zc, we condition on the scaling sw,

E[Zc] = n E[exp((cid:104)vw, c(cid:105))]

= n E [E [exp((cid:104)vw, c(cid:105)) | sw]]

Note that conditioned on sw, we have that (cid:104)vw, c(cid:105) is a Gaussian random variable with variance σ2 = s2
w.

Therefore,

E [exp((cid:104)vw, c(cid:105)) | sw] =

(cid:90)

σ

x
(cid:90)

1
√
2π
1
√
σ
2π
= exp(σ2/2)

=

x

exp(−

exp(−

x2
2σ2 ) exp(x)dx
(x − σ2)2
2σ2

+ σ2/2)dx

It follows that

E[Zc] = n E[exp(σ2/2)] = n E[exp(s2

w/2)] = n E[exp(s2/2)].

We calculate the variance of Zc as follows:

V[Zc] =

V [exp((cid:104)vw, c(cid:105))] ≤ n E[exp(2(cid:104)vw, c(cid:105))]

(cid:88)

w

= n E [E [exp(2(cid:104)vw, c(cid:105)) | sw]]

By a very similar calculation as above, using the fact that 2(cid:104)vw, c(cid:105) is a Gaussian random variable with
variance 4σ2 = 4s2
w,

E [exp(2(cid:104)vw, c(cid:105)) | sw] = exp(2σ2)

29

Therefore, we have that

V[Zc] ≤ n E [E [exp(2(cid:104)vw, c(cid:105)) | sw]]

= n E (cid:2)exp(2σ2)(cid:3) = n E (cid:2)exp(2s2)(cid:3) ≤ Λn

for Λ = exp(8κ2) a constant, and at the last step we used the facts that s ≤ κ a.s.

Now we are ready to prove Lemma 2.1.

Proof of Lemma 2.1. We ﬁx the choice of c, and the proving the concentration using the randomness of vw’s
ﬁrst. Note that that exp((cid:104)vw, c(cid:105)) is neither sub-Gaussian nor sub-exponential (actually the Orlicz norm of
random variable exp((cid:104)vw, c(cid:105)) is never bounded). This prevents us from applying the usual concentration
inequalities. The proof deals with this issue in a slightly more specialized manner.

Let’s deﬁne Fw be the event that |(cid:104)vw, c(cid:105)| ≤ 1

2 log n. We claim that Pr[Fw] ≥ 1−exp(−Ω(log2 n)). Indeed
note that (cid:104)vw, c(cid:105) | sw has a Gaussian distribution with standard deviation sw(cid:107)c(cid:107) = sw ≤ 2κ a.s. Therefore
by the Gaussianity of (cid:104)vw, c(cid:105) we have that

Pr[|(cid:104)vw, c(cid:105)| ≥ η log n | sw] ≤ 2 exp(−Ω(

log2 n/κ2)) = exp(−Ω(log2 n)),

where Ω(·) hides the dependency on κ which is treated as absolute constants. Taking expectations over sw,
we obtain that

Pr[Fw] = Pr[|(cid:104)vw, c(cid:105)| ≤

log n] ≥ 1 − exp(−Ω(log2 n)).

Note that by deﬁnition, we in particular have that conditioned on Fw, it holds that exp((cid:104)vw, c(cid:105)) ≤

Let the random variable Xw have the same distribution as exp((cid:104)vw, c(cid:105))|Fw . We prove that the random
w Xw concentrates well. By convexity of the exponential function, we have that the mean

variable Z (cid:48)
of Z (cid:48)

c = (cid:80)
c is lowerbounded

√

n.

1
4

1
2

E[Z (cid:48)

c] = n E [exp((cid:104)vw, c(cid:105))|Fw ] ≥ n exp(E [(cid:104)vw, c(cid:105)|Fw ]) = n

and the variance is upperbounded by

V[Z (cid:48)

c] ≤ n E (cid:2)exp((cid:104)vw, c(cid:105))2|Fw

(cid:3)

≤

≤

1
Pr[Fw]
1
Pr[Fw]

E (cid:2)exp((cid:104)vw, c(cid:105))2(cid:3)

Λn ≤ 1.1Λn

where the second line uses the fact that

E (cid:2)exp((cid:104)vw, c(cid:105))2(cid:3)

= Pr[Fw] E (cid:2)exp((cid:104)vw, c(cid:105))2|Fw
≥ Pr[Fw] E (cid:2)exp((cid:104)vw, c(cid:105))2|Fw
√

(cid:3) + Pr[F w] E (cid:2)exp((cid:104)vw, c(cid:105))2|F w
(cid:3) .

(cid:3)

Moreover, by deﬁnition, for any w, |Xw| ≤

n. Therefore by Bernstein’s inequality, we have that

Pr [|Z (cid:48)

c − E[Z (cid:48)

c]| > (cid:15)n] ≤ exp(−

1

2 (cid:15)2n2
√

1.1Λn + 1
3

)

n · (cid:15)n

30

Note that E[Z (cid:48)

c] ≥ n, therefore for (cid:15) (cid:29) log2 n√

n , we have,

Pr [|Z (cid:48)

c − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c]] ≤ Pr [|Z (cid:48)

c − E[Z (cid:48)

c]| > (cid:15)n] ≤ exp(−
√

n}))

1

2 (cid:15)2n2
√

)

n · (cid:15)n

Λn + 1
3

≤ exp(−Ω(min{(cid:15)2n/Λ, (cid:15)
≤ exp(−Ω(log2 n))

Let F = ∪wFw be the union of all Fw. Then by union bound, it holds that Pr[ ¯F] ≤ (cid:80)

n · exp(−Ω(log2 n)) = exp(−Ω(log2 n)). We have that by deﬁnition, Z (cid:48)
Therefore, we have that

w Pr[ ¯Fw] ≤
c has the same distribution as Zc|F .

Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c] | F] ≤ exp(−Ω(log2 n))

(A.30)

and therefore

Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c]] = Pr[F] · Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c] | F] + Pr[ ¯F] Pr[|Zc − E[Z (cid:48)

c]| > (cid:15) E[Z (cid:48)

c] | ¯F]

≤ Pr[|Zc − E[Z (cid:48)
≤ exp(−Ω(log2 n))

c]| > (cid:15) E[Z (cid:48)

c] | F] + Pr[ ¯F]

(A.31)

where at the last line we used the fact that Pr[ ¯F] ≤ exp(−Ω(log2 n)) and equation (A.30).

Let Z = E[Z (cid:48)

c] = E[exp((cid:104)vw, c(cid:105)) | | (cid:104)vw, c(cid:105) | < 1

2 log n] (note that E[Z (cid:48)

c] only depends on the norm of (cid:107)c(cid:107)

which is equal to 1). Therefore we obtain that with high probability over the randomness of vw’s,

(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z

(A.32)

Taking expectation over the randomness of c, we have that

[ (A.32) holds] ≥ 1 − exp(−Ω(log2 n))

Pr
c,vw

Therefore by a standard averaging argument (using Markov inequality), we have

(cid:104)

Pr
vw

Pr
c

[ (A.32) holds] ≥ 1 − exp(−Ω(log2 n))

≥ 1 − exp(−Ω(log2 n))

(cid:105)

For now on we ﬁx a choice of vw’s so that Prc[ (A.32) holds] ≥ 1 − exp(−Ω(log2 n)) is true. Therefore in the
rest of the proof, only c is random variable, and with probability 1 − exp(−Ω(log2 n)) over the randomness
of c, it holds that,

(1 − (cid:15)z)Z ≤ Zc ≤ (1 + (cid:15)z)Z.

(A.33)

B Maximum likelihood estimator for co-occurrence

Let L be the corpus size, and Xw,w(cid:48) the number of times words w, w(cid:48) co-occur within a context of size 10
in the corpus. According to the model, the probability of this event at any particular time is log p(w, w(cid:48)) ∝
(cid:107)vw + vw(cid:48)(cid:107)2
2 . Successive samples from a random walk are not independent of course, but if the random walk
mixes fairly quickly (and the mixing time of our random walk is related to the logarithm of the number
of words) then the set of Xw,w(cid:48)’s over all word pairs is distributed up to a very close approximation as a
multinomial distribution Mul( ˜L, {p(w, w(cid:48))}) where ˜L = (cid:80)
w,w(cid:48) Xw,w(cid:48) is the total number of word pairs in
consideration (roughly 10L).

31

Assuming this approximation, we show below that the maximum likelihood values for the word vectors

correspond to the following optimization,

min
{vw},C

(cid:88)

w,w(cid:48)

(cid:16)

Xw,w(cid:48)

log(Xw,w(cid:48)) − (cid:107)vw +vw(cid:48)(cid:107)2

2 − C

(cid:17)2

(Objective SN)

Now we give the derivation of the objective. According to the multinomial distribution, maximizing the

likelihood of {Xw,w(cid:48)} is equivalent to maximizing

To reason about the likelihood, denote the logarithm of the ratio between the expected count and the

empirical count as

Note that

(cid:96) = log

p(w, w(cid:48))Xw,w(cid:48)

 =

Xw,w(cid:48) log p(w, w(cid:48)).





(cid:89)

(w,w(cid:48))



(cid:88)

(w,w(cid:48))

∆w,w(cid:48) = log

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

(cid:33)

.

(cid:88)

(cid:96) =

Xw,w(cid:48) log p(w, w(cid:48))

(w,w(cid:48))

(cid:88)

(w,w(cid:48))

(cid:88)

(w,w(cid:48))

=

=

Xw,w(cid:48)

log

+ log

(cid:34)

Xw,w(cid:48)
˜L

(cid:33)(cid:35)

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

Xw,w(cid:48) log

Xw,w(cid:48) log

Xw,w(cid:48)
˜L

(cid:88)

+

(w,w(cid:48))

(cid:33)

(cid:32) ˜Lp(w, w(cid:48))
Xw,w(cid:48)

where we let c denote the constant (cid:80)

(w,w(cid:48)) Xw,w(cid:48) log Xw,w(cid:48)
˜L

. Furthermore, we have

(B.1)

(cid:88)

= c +

Xw,w(cid:48)∆w,w(cid:48)

(w,w(cid:48))

(cid:88)

˜L =

˜Lpw,w(cid:48)

Xw,w(cid:48)e∆w,w(cid:48)

=

=

(w,w(cid:48))
(cid:88)

(w,w(cid:48))
(cid:88)

(w,w(cid:48))

Xw,w(cid:48)(1 + ∆w,w(cid:48) + ∆2

w,w(cid:48)/2 + O(|∆w,w(cid:48)|3))

and also ˜L = (cid:80)

(w,w(cid:48)) Xw,w(cid:48). So

Plugging this into (3.2) leads to

Xw,w(cid:48)∆w,w(cid:48) = −

Xw,w(cid:48)∆2

w,w(cid:48)/2 +

Xw,w(cid:48)O(|∆w,w(cid:48)|3)

 .





(cid:88)

(w,w(cid:48))

(cid:88)

(w,w(cid:48))



(cid:88)

(w,w(cid:48))

c − (cid:96) =

Xw,w(cid:48)∆2

w,w(cid:48)/2 +

Xw,w(cid:48)O(|∆w,w(cid:48)|3).

(B.2)

(cid:88)

(w,w(cid:48))

(cid:88)

(w,w(cid:48))

32

When the last term is much smaller than the ﬁrst term on the right hand side, maximizing the likelihood

is approximately equivalent to minimizing the ﬁrst term on the right hand side, which is our objective:

(cid:88)

(w,w(cid:48))

Xw,w(cid:48)∆2

w,w(cid:48) ≈

Xw,w(cid:48)

(cid:107)vw + vw(cid:48)(cid:107)2

2/(2d) − log Xw,w(cid:48) + log ˜L − 2 log Z

(cid:17)2

(cid:16)

(cid:88)

(w,w(cid:48))

where Z is the partition function.

We now argue that the last term is much smaller than the ﬁrst term on the right hand side in (B.2). For
a large Xw,w(cid:48), the ∆w,w(cid:48) is close to 0 and thus the induced approximation error is small. Small Xw,w(cid:48)’s only
contribute a small fraction of the ﬁnal objective (3.3), so we can safely ignore the errors. To see this, note
that the objective (cid:80)
(w,w(cid:48)) Xw,w(cid:48)O(|∆w,w(cid:48)|3) diﬀer by a factor of
|∆w,w(cid:48)| for each Xw,w(cid:48). For large Xw,w(cid:48)’s, |∆w,w(cid:48)| (cid:28) 1, and thus their corresponding errors are much smaller
than the objective. So we only need to consider the Xw,w(cid:48)’s that are small constants. The co-occurrence
counts obey a power law distribution (see, e.g. (Pennington et al., 2014)). That is, if one sorts {Xw,w(cid:48)} in
decreasing order, then the r-th value in the list is roughly

w,w(cid:48) and the error term (cid:80)

(w,w(cid:48)) Xw,w(cid:48)∆2

where k is some constant. Some calculation shows that

x[r] =

k
r5/4

and thus when x is a small constant

˜L ≈ 4k,

(cid:88)

Xw,w(cid:48) ≈ 4k4/5x1/5,

Xw,w(cid:48) ≤x

(cid:80)

Xw,w(cid:48) ≤x Xw,w(cid:48)
˜L

≈

(cid:18) 4x
˜L

(cid:19)1/5

= O

(cid:18) 1

˜L1/5

(cid:19)

.

So there are only a negligible mass of Xw,w(cid:48)’s that are small constants, which vanishes when ˜L increases.
Furthermore, we empirically observe that the relative error of our objective is 5%, which means that the
errors induced by Xw,w(cid:48)’s that are small constants is only a small fraction of the objective. Therefore,
(cid:80)

w,w(cid:48) Xw,w(cid:48)O(|∆w,w(cid:48)|3) is small compared to the objective and can be safely ignored.

33

