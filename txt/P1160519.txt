3
1
0
2
 
n
u
J
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
5
8
1
1
.
6
0
3
1
:
v
i
X
r
a

Multiclass Total Variation Clustering

Xavier Bresson∗, Thomas Laurent†, David Uminsky‡and James H. von Brecht§

Abstract

Ideas from the image processing literature have recently motivated a new set of
clustering algorithms that rely on the concept of total variation. While these algorithms
perform well for bi-partitioning tasks, their recursive extensions yield unimpressive
results for multiclass clustering tasks. This paper presents a general framework for
multiclass total variation clustering that does not rely on recursion. The results greatly
outperform previous total variation algorithms and compare well with state-of-the-art
NMF approaches.

1 Introduction

Many clustering models rely on the minimization of an energy over possible partitions of
the data set. These discrete optimizations usually pose NP-hard problems, however. A
natural resolution of this issue involves relaxing the discrete minimization space into a
continuous one to obtain an easier minimization procedure. Many current algorithms, such
as spectral clustering methods or non-negative matrix factorization (NMF) methods, follow
this relaxation approach.

A fundamental problem arises when using this approach, however; in general the solution
of the relaxed continuous problem and that of the discrete NP-hard problem can diﬀer
substantially. In other words, the relaxation is too loose. A tight relaxation, on the other
hand, has a solution that closely matches the solution of the original discrete NP-hard
problem. Ideas from the image processing literature have recently motivated a new set of
algorithms [17, 18, 11, 12, 4, 15, 3, 2, 13, 10] that can obtain tighter relaxations than those
used by NMF and spectral clustering. These new algorithms all rely on the concept of total
variation. Total variation techniques promote the formation of sharp indicator functions
in the continuous relaxation. These functions equal one on a subset of the graph, zero
elsewhere and exhibit a non-smooth jump between these two regions. In contrast to the
relaxations employed by spectral clustering and NMF, total variation techniques therefore
lead to quasi-discrete solutions that closely resemble the discrete solution of the original
NP-hard problem. They provide a promising set of clustering tools for precisely this reason.

∗Department of Computer Science, City University of Hong Kong, Hong Kong (xbresson@cityu.edu.hk).
†Department
of California Riverside, Riverside CA 92521

of Mathematics, University

(laurent@math.ucr.edu)

‡Department

(duminsky@usfca.edu)

(jub@math.ucla.edu)

of Mathematics, University

of

San Francisco,

San Francisco, CA 94117

§Department of Mathematics, University of California Los Angeles, Los Angeles CA 90095

1

Previous total variation algorithms obtain excellent results for two class partitioning
problems [18, 11, 12, 3] . Until now, total variation techniques have relied upon a re-
cursive bi-partitioning procedure to handle more than two classes. Unfortunately, these
recursive extensions have yet to produce state-of-the art results. This paper presents a
general framework for multiclass total variation clustering that does not rely on a recursive
procedure. Speciﬁcally, we introduce a new discrete multiclass clustering model, its cor-
responding continuous relaxation and a new algorithm for optimizing the relaxation. Our
approach also easily adapts to handle either unsupervised or transductive clustering tasks.
The results signiﬁcantly outperform previous total variation algorithms and compare well
against state-of-the-art approaches [19, 20, 1]. We name our approach Multiclass Total
Variation clustering (MTV-clustering).

2 The Multiclass Balanced-Cut Model

Given a weighted graph G = (V, W ) we let V = {x1, . . . , xN } denote the vertex set and
W := {wi,j}1≤i,j≤N denote the non-negative, symmetric similarity matrix. Each entry
wij of W encodes the similarity, or lack thereof, between a pair of vertices. The classical
balanced-cut (or, Cheeger cut) [7, 8] asks for a partition of V = A ∪ Ac into two disjoint
sets that minimizes the set energy

Bal(A) :=

Cut(A, Ac)
min{|A|, |Ac|}

=

(cid:80)

xi∈A,xj ∈Ac wij
min{|A|, |Ac|}

.

(1)

A simple rationale motivates this model: clusters should exhibit similarity between data
points, which is reﬂected by small values of Cut(A, Ac), and also form an approximately
equal sized partition of the vertex set. Note that min{|A|, |Ac|} attains its maximum when
|A| = |Ac| = N/2, so that for a given value of Cut(A, Ac) the minimum occurs when A and
Ac have approximately equal size.

We generalize this model to the multiclass setting by pursuing the same rationale. For

a given number of classes R we formulate our generalized balanced-cut problem as

Minimize

R
(cid:88)

r=1

Cut(Ar, Ac
r)
min{λ|Ar|, |Ac
r|}






(P)

over all disjoint partitions Ar ∩ As = ∅, A1 ∪ · · · ∪ AR = V of the vertex set.

In this model the parameter λ controls the sizes of the sets Ar in the partition. Previous
work [4] has used λ = 1 to obtain a multiclass energy by a straightforward sum of the two-
class balanced-cut terms (1). While this follows the usual practice, it erroneously attempts
to enforce that each set in the partition occupy half of the total number of vertices in the
graph. We instead select the parameter λ to ensure that each of the classes approximately
occupy the appropriate fraction 1/R of the total number of vertices. As the maximum of
min{λ|Ar|, |Ac
r| = N − |Ar|, we see that λ = R − 1 is the proper
choice.

r|} occurs when λ|Ar| = |Ac

This general framework also easily incorporates a priori known information, such as a
set of labels for transductive learning.
If Lr ⊂ V denotes a set of data points that are
a priori known to belong to class r then we simply enforce Lr ⊂ Ar in the deﬁnition of
an allowable partition of the vertex set. In other words, any allowable disjoint partition
Ar ∩ As = ∅, A1 ∪ · · · ∪ AR = V must also respect the given set of labels.

2

3 Total Variation and a Tight Continuous Relaxation

We derive our continuous optimization by relaxing the set energy (P) to the continuous
energy

Here F := [f1, . . . , fR] ∈ MN ×R([0, 1]) denotes the N × R matrix that contains in its
columns the relaxed optimization variables associated to the R clusters. A few deﬁnitions
will help clarify the meaning of this formula. The total variation (cid:107)f (cid:107)T V of a vertex function
f : V → R is deﬁned by:

E(F ) =

R
(cid:88)

r=1

(cid:107)fr(cid:107)T V

(cid:107)fr − medλ(fr)(cid:107) 1,λ

.

(cid:107)f (cid:107)T V =

wij|f (xi) − f (xj)|.

n
(cid:88)

i=1

(2)

(3)

Alternatively, if we view a vertex function f as a vector (f (x1), . . . , f (xN ))t ∈ RN then we
can write

(4)
(cid:107)f (cid:107)T V := (cid:107)Kf (cid:107)1.
Here K ∈ MM ×N (R) denotes the gradient matrix of a graph with M edges and N vertices.
Each row of K corresponds to an edge and each column corresponds to a vertex. For any
edge (i, j) in the graph the corresponding row in the matrix K has an entry wij in the
column corresponding to the ith vertex, an entry −wij in the column corresponding to the
jth vertex and zeros otherwise.

To make sense of the remainder of (2) we must introduce the asymmetric (cid:96)1-norm, which

(cid:107)f (cid:107)1,λ =

|f (xi)|λ

where

|t|λ =

(5)

(cid:40)

λt
−t

if t ≥ 0
if t < 0.

n
(cid:88)

i=1

Finally we deﬁne the λ-median (or quantile), denoted medλ(f ), as:

medλ(f ) = the (k + 1)st

largest value in the range of f , where k = (cid:98)N/(λ + 1)(cid:99).

(6)

These deﬁnitions, as well as the relaxation (2) itself, were motivated by the following theo-
rem. Its proof, found in the Appendix, relies only on using the three preceeding deﬁnitions
and some simple algebra.

Theorem 1. If f = 1A is the indicator function of a subset A ⊂ V then

(cid:107)f (cid:107)T V

(cid:107)f − medλ(f )(cid:107) 1,λ

=

2 Cut(A, Ac)
min {λ|A|, |Ac|}

.

The preceding theorem allows us to restate the original set optimization problem (P) in

the equivalent discrete form

Minimize

R
(cid:88)

r=1

(cid:107)fr(cid:107)T V

(cid:107)fr − medλ(fr)(cid:107) 1,λ






(P’)

over non-zero functions f1, . . . , fR : V → {0, 1} such that f1 + . . . + fR = 1V .

Indeed, since the non-zero functions fr can take only two values, zero or one, they must
deﬁne indicator functions of some nonempty set. The simplex constraint f1 + . . . + fR = 1V

3

then guarantees that the sets Ar := {xi ∈ V : fr(xi) = 1} form a partition of the vertex set.
We obtain the relaxed version (P-rlx) of (P’) in the usual manner by allowing fr ∈ [0, 1] to
have a continuous range. This yields

Minimize

R
(cid:88)

r=1

(cid:107)fr(cid:107)T V

(cid:107)fr − medλ(fr)(cid:107) 1,λ






(P-rlx)

over functions f1, . . . , fR : V → [0, 1] such that f1 + . . . + fR = 1V .

The following two points form the foundation on which total variation clustering relies:
1 — As the next subsection details, the total variation terms give rise to quasi-indicator
functions. That is, the relaxed solutions [f1, . . . , fR] of (P-rlx) mostly take values near zero
or one and exhibit a sharp, non-smooth transition between these two regions. Since these
quasi-indicator functions essentially take values in the discrete set {0, 1} rather than the
continuous interval [0, 1], solving (P-rlx) is almost equivalent to solving either (P) or (P’).
In other words, (P-rlx) is a tight relaxation of (P).

2 — Both functions f (cid:55)→ (cid:107)f (cid:107)T V and f (cid:55)→ (cid:107)f − medλ(f )(cid:107)1,λ are convex. The simplex
constraint in (P-rlx) is also convex. Therefore solving (P-rlx) amounts to minimizing a sum
of ratios of convex functions with convex constraints. As the next section details, this fact
allows us to use machinery from convex analysis to develop an eﬃcient, novel algorithm for
such problems.

3.1 The Role of Total Variation in the Formation of Quasi-Indicator

Functions

To elucidate the precise role that the total variation itself plays in the formation of quasi-
indicator functions, it proves useful to consider a version of (P-rlx) that uses a spectral
relaxation in place of the total variation:

Minimize

R
(cid:88)

r=1

(cid:107)fr(cid:107)Lap

(cid:107)fr − medλ(fr)(cid:107) 1,λ






(P-rlx2)

over functions f1, . . . , fR : V → [0, 1] such that f1 + . . . + fR = 1V

Lap = (cid:80)n

Here (cid:107)f (cid:107)2
i=1 wij|f (xi) − f (xj)|2 denotes the spectral relaxation of Cut(A, Ac); it
equals (cid:104)f, Lf (cid:105) if L denotes the unnormalized graph Laplacian matrix. Thus problem (P-
rlx2) relates to spectral clustering (and therefore NMF [9]) with a positivity constraint.
Note that the only diﬀerence between (P-rlx2) and (P-rlx) is that the exponent 2 appears
in (cid:107) · (cid:107)Lap while the exponent 1 appears in the total variation. This simple diﬀerence
of exponent has an important consequence for the tightness of the relaxations. Figure 1
presents a simple example that illuminates this diﬀerence. If we bi-partition the depicted
graph, i.e. a line with 20 vertices and edge weights wij = 1, then the optimal cut lies
between vertex 10 and vertex 11 since this gives a perfectly balanced cut. Figure 1(a)
shows the vertex function f1 generated by (P-rlx) while ﬁgure 1(b) shows the one generated
by (P-rlx2). Observe that the solution of the total variation model coincides with the
indicator function of the desired cut whereas the the spectral model prefers its smoothed
version. Note that both functions in ﬁgure 1a) and 1b) have exactly the same total variation
(cid:107)f (cid:107)T V = |f (x1) − f (x2)| + · · · + |f (x19) − f (x20)| = f (x1) − f (x20) = 1 since both functions
are monotonic. The total variation model will therefore prefer the sharp indicator function

4

since it diﬀers more from its λ-median than the smooth indicator function.
Indeed, the
denominator (cid:107)fr − medλ(fr)(cid:107)1,λ is larger for the sharp indicator function than for the
smooth one. A diﬀerent scenario occurs when we replace the exponent one in (cid:107) · (cid:107)T V by an
exponent two, however. As (cid:107)f (cid:107)2
Lap = |f (x1) − f (x2)|2 + · · · + |f (x19) − f (x20)|2 and t2 < t
when t < 1 it follows that(cid:107)f (cid:107)Lap is much smaller for the smooth function than for the sharp
one. Thus the spectral model will prefer the smooth indicator function despite the fact that
it diﬀers less from its λ-median. We therefore recognize the total variation as the driving
force behind the formation of sharp indicator functions.

(a)

(b)

Figure 1: Top: The graph used for both relaxations. Bottom left: the solution given by
the total variation relaxation. Bottom right: the solution given by the spectral relaxation.
Position along the x-axis = vertex number, height along the y-axis = value of the vertex
function.

This heuristic explanation on a simple, two-class example generalizes to the multiclass
case and to real data sets (see ﬁgure 2). In simple terms, quasi-indicator functions arise due
to the fact that the total variation of a sharp indicator function equals the total variation of a
smoothed version of the same indicator function. The denominator (cid:107)fr − medλ(fr)(cid:107)1,λ then
measures the deviation of these functions from their λ-median. A sharp indicator function
deviates more from its median than does its smoothed version since most of its values
concentrate around zero and one. The energy is therefore much smaller for a sharp indicator
function than for a smooth indicator function, and consequently the total variation clustering
energy always prefers sharp indicator functions to smooth ones. For bi-partitioning problems
this fact is well-known. Several previous works have proven that the relaxation is exact in
the two-class case; that is, the total variation solution coincides with the solution of the
original NP-hard problem [8, 18, 3, 5].

Figure 2 illustrates the result of the diﬀerence between total variation and NMF relax-
ations on the data set OPTDIGITS, which contains 5620 images of handwritten numerical
digits. Figure 2(a) shows the quasi-indicator function f4 obtained by our MTV algorithm
while 2(b) shows the function f4 obtained from the NMF algorithm of [1]. We extract the
portion of each function corresponding to the digits four and nine, then sort and plot the
result. The MTV relaxation leads a sharp transition between the fours and the nines while
the NMF relaxation leads to a smooth transition.

5

Figure 2: Left: Solution f4 from our MTV algorithm plotted over the fours and nines.
Right: Solution f4 from LSD [1] plotted over the fours and nines.

3.2 Transductive Framework

From a modeling point-of-view, the presence of transductive labels poses no additional
diﬃculty. In addition to the simplex constraint

(cid:40)

F ∈ Σ :=

F ∈ MN ×R([0, 1]) : fr(xi) ≥ 0,

fr(xi) = 1

(7)

R
(cid:88)

r=1

required for unsupervised clustering we also impose the set of labels as a hard constraint.
If L1, . . . , LR denote the R vertex subsets representing the labeled points, so that xi ∈ Lr
means xi belongs to class r, then we may enforce these labels by restricting F to lie in the
subset

F ∈ Λ := {F ∈ MN ×R([0, 1]) : ∀r, (f1(xi), . . . , fR(xi)) = er ∀ xi ∈ Lr } .

(8)

Here er denotes the row vector containing a one in the rth location and zeros elsewhere.
Our model for transductive classiﬁcation then aims to solve the problem

Minimize

R
(cid:88)

r=1

(cid:107)fr(cid:107)T V

(cid:107)fr − medλ(fr)(cid:107) 1,λ

over matrices F ∈ Σ ∩ Λ.

(P-trans)

Note that Σ ∩ Λ also deﬁnes a convex set, so this minimization remains a sum of ratios of
convex functions subject to a convex constraint. Transductive classiﬁcation therefore poses
no additional algorithmic diﬃculty, either. In particular, we may use the proximal splitting
algorithm detailed in the next section for both unsupervised and transductive classiﬁcation
tasks.

(cid:41)

(cid:41)

4 Proximal Splitting Algorithm

This section details our proximal splitting algorithm for ﬁnding local minimizers of a sum
of ratios of convex functions subject to a convex constraint. We start by showing in the ﬁrst
subsection that the functions

T (f ) := (cid:107)f (cid:107)T V

and B(f ) := (cid:107)f − medλ(f )1(cid:107)1,λ

(9)

involved in (P-rlx) or (P-trans) are indeed convex. We also give an explicit formula for a
subdiﬀerential of B since our proximal splitting algorithm requires this in explicit form. We
then summarize a few properties of proximal operators before presenting the algorithm.

6

4.1 Convexity, Subgradients and Proximal Operators
Recall that we may view each function f : V → R as a vector in RN with f (xi) as the ith
component of the vector. We may then view T and B as functions from RN to R. The next
theorem states that both B and T deﬁne convex functions on RN and furnishes an element
v ∈ ∂B(f ) by means of an easily computable formula. The formula for the subdiﬀerential
generalizes a related result for the symmetric case [11] to the asymmetric setting.

Theorem 2. The functions B and T are convex. Moreover, given f ∈ RN the vector
v ∈ RN deﬁned by

v(xi) =




λ
n−−λn+
n0



−1

if f (xi) > medλ(f )
if f (xi) = medλ(f )
if f (xi) < medλ(f )

where






n0 = |{xi ∈ V : f (xi) = medλ(f )}|
n− = |{xi ∈ V : f (xi) < medλ(f )}|
n+ = |{xi ∈ V : f (xi) > medλ(f )}|

belongs to ∂B(f ).

In the above theorem ∂B(f ) denotes the subdiﬀerential of B at f and v ∈ ∂B(f )
denotes a subgradient. The proof of Theroem 2 can be found in the Appendix. Given a
convex function A : RN → R, the proximal operator of A is deﬁned by

proxA(g) := argmin
f ∈RN

A(f ) +

||f − g||2
2.

1
2

If we let δC denote the barrier function of the convex set C,

δC(f ) :=

(cid:40)
0
if f ∈ C
+∞ if f /∈ C,

then we easily see that proxδC is simply the least-squares projection on C:

proxδC (f ) = projC(f ) := argmin

||f − g||2
2.

1
2

g∈C

In this manner the proximal operator deﬁnes a mapping from RN to RN that generalizes
the least-squares projection onto a convex set.

4.2 The Algorithm

We can rewrite the problem (P-rlx) or (P-trans) as

Minimize

δC(F ) +

E(fr)

over all matrices F = [f1, . . . , fr] ∈ MN ×R

(13)

R
(cid:88)

r=1

where E(fr) = T (fr)/B(fr) denotes the energy of the quasi-indicator function of the rth
cluster. The set C = Σ or C = Σ ∩ Λ is the convex subset of MN ×R that encodes the
simplex constraint (7) or the simplex constraint with labels. The corresponding function
δC(F ), deﬁned in (11), is the barrier function of the desired set. Beginning from an initial
iterate F 0 ∈ C we propose the following proximal splitting algorithm:

F k+1 := proxT k+δC (F k + ∂Bk(F k)).

(14)

7

(10)

(11)

(12)

Here T k(F ) and Bk(F ) denote the convex functions

T k(F ) :=

ck
r T (fr)

Bk(F ) :=

dk
r B(fr),

R
(cid:88)

r=1

R
(cid:88)

r=1

the constants (ck

r , dk

r ) are computed using the previous iterate

ck
r =

∆k
B(f k
r )

and

dk
r =

∆kE(f k
r )
B(f k
r )

r , dk
and ∆k denotes the timestep for the current iteration. This choice of the constants (ck
r )
yields Bk(F k) = T k(F k), and this fundamental property allows us to derive the energy
descent estimate:

Theorem 3 (Estimate of the energy descent). Each of the F k belongs to C, and if Bk
then

r (cid:54)= 0

R
(cid:88)

Bk+1
r
Bk
r

(cid:0)Ek

r − Ek+1

r

(cid:1) ≥

(cid:107)F k − F k+1(cid:107)2
∆k

(15)

where Bk

r , Ek

r=1
r stand for B(f k

r ), E(f k

r ).

Inequality (18) states that the energies of the quasi-indicator functions (as a weighted
sum) decrease at every step of the algorithm. It also gives a lower bound for how much these
/Bk
energies decrease. As the algorithm progress and the iterates stabilize the ratio Bk+1
r
converges to 1, in which case the sum, rather than a weighted sum, of the individual cluster
energies decreases. The proof of Theorem 3 can be found in the Appendix.

r

Our proximal splitting algorithm (14) requires two steps. The ﬁrst step requires com-
puting Gk = F k + ∂Bk(F k), and this is straightforward since theorem 5 provides the subd-
iﬀerential of B, and therefore of Bk, through an explicit formula. The second step requires
computing proxT k+δC (Gk), which seems daunting at a ﬁrst glance. Fortunately, minimiza-
tion problems of this form play an important role in the image processing literature. Recent
years have therefore produced several fast and accurate algorithms for computing the proxi-
mal operator of the total variation. As T k + δC consists of a weighted sum of total variation
terms subject to a convex constraint, we can readily adapt these algorithms to compute the
second step of our algorithm eﬃciently. In this work we use the primal-dual algorithm of
[6] with acceleration. This relies on a proper uniformly convex formulation of the proximal
minimization, which we detail completely in the Appendix.

The primal-dual algorithm we use to compute proxT k+δC (Gk) produces a sequence of
approximate solutions by means of an iterative procedure. A stopping criterion is therefore
needed to indicate when the current iterate approximates the actual solution proxT k+δC (Gk)
suﬃciently. Ideally, we would like to terminate F k+1 ≈ proxT k+δC (Gk) in such a manner
so that the energy descent property (18) still holds and F k+1 always satisﬁes the required
constraints. In theory we cannot guarantee that the energy estimate holds for an inexact
solution. We may note, however, that a slightly weaker version of the energy estimate (18)

R
(cid:88)

r=1

Bk+1
r
Bk
r

(cid:0)Ek

r − Ek+1

r

(cid:1) ≥ (1 − (cid:15))

(cid:107)F k − F k+1(cid:107)2
F
∆k

(16)

holds after a ﬁnite number of iterations of the inner minimization. Moreover, this weaker ver-
sion still guarantees that the energies of the quasi-indicator functions decrease as a weighted

8

sum in exactly the same manner as before. In this way we can terminate the inner loop
adaptively: we solve F k+1 ≈ proxT k+δC (Gk) less precisely when F k+1 lies far from a mini-
mum and more precisely as the sequence {F k} progresses. Overall this leads to a substantial
increase in eﬃciency of the full algorithm.

Our implementation of the proximal splitting algorithm also guarantees that F k+1 always
satisﬁes the required constraints. We accomplish this task by implementing the primal-dual
algorithm in such a way that each inner iteration always satisﬁes the constraints. This
requires computing the projection projC(F ) exactly at each inner iteration. The overall
algorithm remains eﬃcient provided we can compute this projection quickly. When C = Σ
the algorithm [14] performs the required projection in at most R steps. When C = Σ ∩ Λ
the computational eﬀort actually decreases, since in this case the projection consists of a
simplex projection on the unlabeled points and straightforward assignment on the labeled
points.

We may now summarize the full algorithm, including the proximal operator computation.
In practice we ﬁnd the choices ∆k = max{Bk
R} and any small (cid:15) work well, so we
present the algorithm with these choices. Recall the matrix K in (4) denotes the gradient
matrix of the graph.

1 , . . . , Bk

Algorithm 1 Proximal Splitting Algorithm

Input: F ∈ C, P = 0, L = ||K||2, τ = L−1, (cid:15) = 10−3
while loop not converged do

(cid:105)

¯F = F

DB = diag

(cid:104) E(f1)
B(f1) , . . . , E(fR)

σ = ∆2
0(τ ∆2L2)−1
(cid:104) ∆
B(f1) , . . . , ∆

//Perform outer step Gk = F k + ∂Bk(F k)
∆ = maxr B(fr) ∆0 = minr B(fr)
DE = diag
B(fR)
V = ∆[∂B(f1), . . . , ∂B(fR)]DE (using theorem 5)
G = F + V
//Perform F k+1 ≈ proxT k+δC (Gk) until energy estimate holds
while (16) fails do
˜P = P + σK ¯F DB
˜F = F − τ K tP DB
θ = 1/
end while

P = ˜P / max{| ˜P |, 1} (both operations entriwise)
F = ( ˜F + τ G)/(1 + τ )
σ = σ/θ

F = projC(F )
¯F = (1 + θ)F − θFold

τ = θτ

1 + 2τ

B(fR)

√

(cid:105)

end while

Fold = F

5 Numerical Experiments

We now demonstrate the MTV algorithm for unsupervised and transductive clustering tasks.
We selected six standard, large-scale data sets as a basis of comparison. We obtained the
ﬁrst data set (4MOONS) and its similarity matrix from [4] and the remaining ﬁve data sets
and matrices (WEBKB4, OPTDIGITS, PENDIGITS, 20NEWS, MNIST) from [19]. The
4MOONS data set contains 4K points while the remaining ﬁve contain 4.2K, 5.6K, 11K,
20K and 70K points, respectively.

Our ﬁrst set of experiments compares our MTV algorithm against other unsupervised
approaches. We compare against two previous total variation algorithms [11, 3], which rely
on recursive bi-partitioning, and two top NMF algorithms [1, 19]. We use the normalized
Cheeger cut versions of [11] and [3] with default parameters. We used the code available

9

from [19] to test each NMF algorithm. All non-recursive algorithms (LSD [1], NMFR [19],
MTV) received two types of initial data: (a) the deterministic data used in [19]; (b) a
random procedure leveraging normalized-cut [16]. Procedure (b) ﬁrst selects one data point
uniformly at random from each computed NCut cluster, then sets fr equal to one at the data
point drawn from the rth cluster and zero otherwise. We then propagate this initial stage
by replacing each fr with (I + L)−1fr where L denotes the unnormalized graph Laplacian.
Finally, to aid the NMF algorithms, we add a small constant 0.2 to the result (each performed
better than without adding this constant). For MTV we use (a) and 30 random trials of
(b) then report the cluster purity of the solution with the lowest discrete energy (P). We
then use each NMF with exactly the same initial conditions and report simply the highest
purity achieved over all 31 runs. This biases the results in favor of the NMF algorithms.
Due to the non-convex nature of these algorithms, the random initialization gave the best
results and signiﬁcantly improved on previously reported results of LSD in particular. We
allowed each non-recursive algorithm 10000 iterations using initial condition (a) while each
trial of (b) performed 2000 iterations. The following table reports the results. Our next

Alg/Data
NCC-TV [3]
1SPEC [11]
LSD [1]
NMFR [19]
MTV

88.75
73.92
99.40
77.80
99.53

4MOONS WEBKB4 OPTDIGITS PENDIGITS

20NEWS MNIST

51.76
39.68
54.50
64.32
59.15

95.91
88.65
97.94
97.92
98.29

73.25
82.42
88.44
91.21
89.06

23.20
11.49
41.25
63.93
39.40

88.80
88.17
95.67
96.99
97.60

set of experiments demonstrate our algorithm in a transductive setting. For each data
set we randomly sample either one label per class or a percentage of labels per class from
the ground truth. We then run ten trials of initial condition (b) (propagating all labels
instead of one) and report the purity of the lowest energy solution as before along with the
average computational time (for simple MATLAB code running on a standard desktop) of
the ten runs. We terminate the algorithm once the relative change in energy falls below
10−4 between outer steps of algorithm 1. The table below reports the results. Note that
for well-constructed graphs (such as MNIST), our algorithm performs remarkably well with
only one label per class.

Labels
4MOONS WEBKB4 OPTDIGITS PENDIGITS
1
99.55/ 3.0s
99.55/ 3.1s
1%
2.5% 99.55/ 1.9s
99.53/ 1.2s
5%
99.55/ 0.8s
10%

56.58/ 1.8s
58.75/ 2.0s
57.01/ 1.7s
58.34/ 1.3s
62.01/ 1.2s

89.17/ 14s
93.73/ 9s
95.83/ 7s
97.98/ 5s
98.22/ 4s

98.29/ 7s
98.29/ 4s
98.35/ 3s
98.38/ 2s
98.45/ 2s

20NEWS
50.07/ 52s
61.70/ 54s
67.61/ 42s
70.51/ 32s
73.97/ 25s

MNIST
97.53/ 98s
97.59/ 54s
97.72/ 39s
97.79/ 31s
98.05/ 25s

Our non-recursive MTV algorithm vastly outperforms the two previous recursive total
variation approaches and also comparse well with state-of-the-art NMF approaches. Each
of MTV, LSD and NMFR perform well on manifold data sets such as MNIST but NMFR
tends to perform best on noisy, non-manifold data sets. This results from the fact that
NMFR uses a costly graph smoothing technique while our algorithm and LSD do not. We
plan to incorporate such improvements into the total variation framework in future work.
Lastly, we found procedure (b) can help overcome the lack of convexity inherent in many
clustering approaches. We plan to pursue a more principled and eﬃcient initialization along

10

these lines in the future as well. Overall, our total variation framework therefore presents
a promising alternative to NMF methods due to its strong mathematical foundation and
tight relaxation.

6 Appendix

6.1 Proofs of Theorems

Theorem 4. If f = 1A is the indicator function of a subset A ⊂ V then

(cid:107)f (cid:107)T V

(cid:107)f − medλ(f )(cid:107) 1,λ

=

2 Cut(A, Ac)
min {λ|A|, |Ac|}

.

Proof. The fact that (cid:107)f (cid:107)T V = 2 Cut(A, Ac) follows directly from the deﬁnition of the total
variation. Indeed, a straightforward computation shows

(cid:107)f (cid:107)T V =

wij|1 − f (xj)| +

(cid:88)

N
(cid:88)

xi∈A

j=1

(cid:88)

N
(cid:88)

xi∈Ac

j=1

wij|f (xj)| =

(cid:88)

(cid:88)

(cid:88)

(cid:88)

wij +

wij.

xi∈A

xj ∈Ac

xi∈Ac

xj ∈A

Thus (cid:107)f (cid:107)T V = 2 Cut(A, Ac) as W is symmetric. Let B(f ) := (cid:107)f − medλ(f )(cid:107)1,λ. To
show that B(f ) = min {λ|A|, |Ac|}, suppose ﬁrst that λ|A| ≤ |Ac|. This inequality implies
λ|A| ≤ N − |A|, or equivalently that |A| ≤ N/(1 + λ). Thus |A| ≤ k := (cid:98)N/(1 + λ)(cid:99),
and since f = 1A for |A| ≤ k it follows immediately that the (k + 1)st largest entry in the
range of f equals zero. Thus medλ(f ) = 0 by deﬁnition. A direct computation then yields
that B(f ) = (cid:80)
i∈V |f (xi)|λ = λ|A|. In the converse case, the fact that |Ac| < λ|A| implies
|A| > N/(1 + λ) ≥ k. Thus |A| ≥ k + 1 and medλ(f ) = 1. Direct computation then shows
that B(f ) = (cid:80)

i∈V |f (xi) − 1|λ = |Ac| as claimed.

Lemma 1. Let h ∈ RN and suppose v ∈ RN satisﬁes

v(xi) ∈






λ
[−1, λ]
−1

if h(xi) > 0
if h(xi) = 0
if h(xi) < 0.

(17)

Then v ∈ ∂(cid:107)h(cid:107)1,λ.
Proof. Note that |h(xi)|λ = v(xi)h(xi) for each xi, so that for arbitrary g ∈ RN and each
xi the inequality

|g(xi)|λ − |h(xi)|λ ≥ v(xi) (g(xi) − h(xi))

holds. Summing both sides over all xi ∈ V then gives the claim.

Theorem 5. The functions B and T are convex. Moreover, given f ∈ RN the vector
v ∈ RN deﬁned by

v(xi) =




λ
n−−λn+
n0



−1

if f (xi) > medλ(f )
if f (xi) = medλ(f )
if f (xi) < medλ(f )

where






n0 = |{xi ∈ V : f (xi) = medλ(f )}|
n− = |{xi ∈ V : f (xi) < medλ(f )}|
n+ = |{xi ∈ V : f (xi) > medλ(f )}|

belongs to ∂B(f ).

11

Proof. The convexity of T (f ) follows directly from its deﬁnition and a straightforward com-
putation using the deﬁnition of convexity. Due to the continuity B(f ), to show convexity it
suﬃces to establish the existence of a subdiﬀerential at every point.

To this end note that medλ(f ) ∈ range(f ), so that in particular n0 ≥ 1 by deﬁnition. Let
1 ≤ k := (cid:98)N/(1 + λ)(cid:99) < N denote that entry of f so that f (xk) = medλ(f ). By deﬁnition
of medλ(f ) there exist at most k elements of f larger than medλ(f ), so that n+ ≤ k ≤
N/(1 + λ). As N = n− + n0 + n+ this implies λn+−n−
≤ 1. Similarly there exist at most
N − (k + 1) elements of f smaller than medλ(f ), so that n− ≤ N − (k + 1) ≤ N − N/(1 + λ).
The fact that N = n− + n0 + n+ then implies n−−λn+
≤ λ. Combining this with the
previous inequality yields −1 ≤ n−−λn+

≤ λ.

n0

n0

Put h := f − medλ(f )1, and note that the vector v deﬁned above satisﬁes v ∈ ∂(cid:107)h(cid:107)1,λ

n0

by the preceeding lemma. Thus for any g ∈ RN it holds that

||g − medλ(g)1||1,λ − ||f − medλ(f )1||1,λ ≥ (cid:104)v, g − f + (medλ(f ) − medλ(g))1(cid:105)

by deﬁnition of the subdiﬀerential. Note also that (cid:104)v, 1(cid:105) = 0, so that in fact

B(g) − B(f ) = ||g − medλ(g)1||1,λ − ||f − medλ(f )1||1,λ ≥ (cid:104)v, g − f (cid:105)
for g ∈ RN arbitrary. Thus v ∈ ∂B(f ) by deﬁnition of the subdiﬀerential. In particular
∂B(f ) is always non-empty, so B(f ) is convex.

Theorem 6 (Estimate of the energy descent). Each of the F k belongs to C, and if Bk
then

r (cid:54)= 0

where Bk

r , Ek

r=1
r stand for B(f k

R
(cid:88)

Bk+1
r
Bk
r
r ), E(f k

r ).

(cid:0)Ek

r − Ek+1

r

(cid:1) ≥

(cid:107)F k − F k+1(cid:107)2
∆k

Proof. Let V k ∈ ∂Bk(F k). Then by deﬁnition of the subdiﬀerential it follows that

Bk(F k+1) ≥ Bk(F k) + (cid:104)F k+1 − F k, V k(cid:105).

As F k+1 = proxT k+δC (F k +V k) the deﬁnition of the proximal operator implies that F k+1 ∈
C and that also

F k + V k − F k+1 ∈ ∂(T k + δC)(F k+1).
The deﬁnition of the subdiﬀerential and the fact that δC(F k) = δC(F k+1) = 0 then combine
to imply

T k(F k) ≥ T k(F k+1) + (cid:104)F k − F k+1, F k + V k − F k+1(cid:105)

= T k(F k+1) + (cid:107)F k − F k+1(cid:107)2 + (cid:104)F k − F k+1, V k(cid:105)

(20)

(18)

(19)

Adding (19) and (20) yields

T k(F k) + Bk(F k+1) ≥ T k(F k+1) + Bk(F k) + (cid:107)F k − F k+1(cid:107)2,

or equivalently that Bk(F k+1) ≥ T k(F k+1) + (cid:107)F k − F k+1(cid:107)2 since Bk(F k) = T k(F k) by
construction. Expanding this last inequality shows

R
(cid:88)

r=1

∆k
Bk
r

(cid:0)Ek

r Bk+1

r − T k+1

r

(cid:1) ≥ (cid:107)F k − F k+1(cid:107)2,

which yields the claim after by Bk+1

in each term of the summation.

r

12

6.2 Primal-Dual Formulation

Consider the minimization

We may write this as the saddle-point problem

F k+1 := proxT k+δC (Gk).

min
u∈RN R

max
p∈RM R

(cid:104)p, Ku(cid:105) + G(u) − F ∗(p).

Here the vector u = (f1, . . . , fR)t is a “vectorized” version of F and the matrix K denotes
the block diagonal matrix

where K is the gradient matrix of the graph. We deﬁne the convex function G(u) as

K := blkdiag

K, . . . ,

(cid:18) ∆k
Bk
1

(cid:19)

K

∆k
Bk
R

G(u) :=

||fr − gk

r ||2 + δC(u),

1
2

R
(cid:88)

r=1

where δC denotes the barrier function of the convex set C (either the simplex or simplex
with labels) as before. The convex function F ∗(p) denotes the barrier function of the l∞
unit ball, so that

F ∗(p) =

(cid:40)
0
+∞ otherwise.

if

|pi| ≤ 1 ∀ 1 ≤ i ≤ M R

Note also that G(u) is uniformly convex, in that if v ∈ ∂G(u) denotes any subdiﬀerential
then for any u(cid:48) ∈ RN R the inequality

G(u(cid:48)) − G(u) ≥ (cid:104)v, u(cid:48) − u(cid:105) +

||u − u(cid:48)||2

1
2

holds. We may therefore apply algorithm 2 of [6] with γ = 1 with to solve the saddle-point
problem. This algorithm consists in the iterations

pn+1 = proxσnF ∗ (pn + σnK¯un)
un+1 = proxτ nG(un − τ nKtpn+1)

√

θn =

1
1 + 2τ n
¯un+1 = un+1 + θn(un+1 − un)

τ n+1 = θnτ n σn+1 = σn/θn

and converges provided the inequality σ0 ≤ (τ 0||K||2
may compute the inner proximal operators analytically to ﬁnd

2)−1 holds for the initial timesteps. We

(proxσnF ∗ (z))i = zi/ max{1, |zi|} ∀ 1 ≤ i ≤ M R,

and by completing the square that

proxτ nG(z) = projC

(cid:18) z + τ ng
1 + τ n

(cid:19)

,

where g = (gk
follows by re-writing these computations in matrix form.

R)t denotes Gk in vectorized form. The inner loop of algorithm 1 then

1 , . . . , gk

13

References

[1] Raman Arora, M Gupta, Amol Kapila, and Maryam Fazel. Clustering by left-stochastic
matrix factorization. In International Conference on Machine Learning (ICML), pages
761–768, 2011.

[2] A. Bertozzi and A. Flenner. Diﬀuse Interface Models on Graphs for Classiﬁcation of
High Dimensional Data. Multiscale Modeling and Simulation, 10(3):1090–1118, 2012.

[3] X. Bresson, T. Laurent, D. Uminsky, and J. von Brecht. Convergence and energy
In Advances in Neural Information Processing

landscape for cheeger cut clustering.
Systems (NIPS), pages 1394–1402, 2012.

[4] X. Bresson, X.-C. Tai, T.F. Chan, and A. Szlam. Multi-Class Transductive Learning
based on (cid:96)1 Relaxations of Cheeger Cut and Mumford-Shah-Potts Model. UCLA CAM
Report, 2012.

[5] T. B¨uhler and M. Hein. Spectral Clustering Based on the Graph p-Laplacian.

In

International Conference on Machine Learning (ICML), pages 81–88, 2009.

[6] A. Chambolle and T. Pock. A First-Order Primal-Dual Algorithm for Convex Problems
with Applications to Imaging. Journal of Mathematical Imaging and Vision, 40(1):120–
145, 2011.

[7] J. Cheeger. A Lower Bound for the Smallest Eigenvalue of the Laplacian. Problems in

Analysis, pages 195–199, 1970.

[8] F. R. K. Chung. Spectral Graph Theory, volume 92 of CBMS Regional Conference Series
in Mathematics. Published for the Conference Board of the Mathematical Sciences,
Washington, DC, 1997.

[9] Chris Ding, Xiaofeng He, and Horst D Simon. On the equivalence of nonnegative matrix
In Proc. SIAM Data Mining Conf, number 4,

factorization and spectral clustering.
pages 606–610, 2005.

[10] C. Garcia-Cardona, E. Merkurjev, A. L. Bertozzi, A. Flenner, and A. G. Percus. Fast

multiclass segmentation using diﬀuse interface methods on graphs. Submitted, 2013.

[11] M. Hein and T. B¨uhler. An Inverse Power Method for Nonlinear Eigenproblems with
Applications in 1-Spectral Clustering and Sparse PCA. In Advances in Neural Infor-
mation Processing Systems (NIPS), pages 847–855, 2010.

[12] M. Hein and S. Setzer. Beyond Spectral Clustering - Tight Relaxations of Balanced
Graph Cuts. In Advances in Neural Information Processing Systems (NIPS), 2011.

[13] E. Merkurjev, T. Kostic, and A. Bertozzi. An mbo scheme on graphs for segmentation

and image processing. UCLA CAM Report 12-46, 2012.

[14] C. Michelot. A Finite Algorithm for Finding the Projection of a Point onto the Canon-
ical Simplex of Rn. Journal of Optimization Theory and Applications, 50(1):195–200,
1986.

[15] S. Rangapuram and M. Hein. Constrained 1-Spectral Clustering.

In International
conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 1143–1151, 2012.

14

[16] J. Shi and J. Malik. Normalized Cuts and Image Segmentation. IEEE Transactions on

Pattern Analysis and Machine Intelligence (PAMI), 22(8):888–905, 2000.

[17] A. Szlam and X. Bresson. A total variation-based graph clustering algorithm for cheeger

ratio cuts. UCLA CAM Report 09-68, 2009.

[18] A. Szlam and X. Bresson. Total variation and cheeger cuts. In International Conference

on Machine Learning (ICML), pages 1039–1046, 2010.

[19] Zhirong Yang, Tele Hao, Onur Dikmen, Xi Chen, and Erkki Oja. Clustering by nonneg-
ative matrix factorization using graph random walk. In Advances in Neural Information
Processing Systems (NIPS), pages 1088–1096, 2012.

[20] Zhirong Yang and Erkki Oja. Clustering by low-rank doubly stochastic matrix decom-

position. In International Conference on Machine Learning (ICML), 2012.

15

