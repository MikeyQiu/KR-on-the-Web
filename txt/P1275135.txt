Dependency-based Convolutional Neural Networks
for Sentence Embedding∗

Mingbo Ma†
Liang Huang† ‡
†Graduate Center & Queens College
City University of New York

Bing Xiang‡

Bowen Zhou‡

‡IBM Watson Group
T. J. Watson Research Center, IBM

mma2,lhuang
}

{

gc.cuny.edu

@us.ibm.com
lhuang,bingxia,zhou
}

{

Abstract

In sentence modeling and classiﬁcation,
convolutional neural network approaches
have recently achieved state-of-the-art re-
sults, but all such efforts process word vec-
tors sequentially and neglect long-distance
dependencies. To combine deep learn-
ing with linguistic structures, we pro-
pose a dependency-based convolution ap-
proach, making use of tree-based n-grams
rather than surface ones, thus utlizing non-
local interactions between words. Our
model improves sequential baselines on all
four sentiment and question classiﬁcation
tasks, and achieves the highest published
accuracy on TREC.

Introduction

1
Convolutional neural networks (CNNs), originally
invented in computer vision (LeCun et al., 1995),
has recently attracted much attention in natural
language processing (NLP) on problems such as
sequence labeling (Collobert et al., 2011), seman-
tic parsing (Yih et al., 2014), and search query
retrieval (Shen et al., 2014). In particular, recent
work on CNN-based sentence modeling (Kalch-
brenner et al., 2014; Kim, 2014) has achieved ex-
cellent, often state-of-the-art, results on various
classiﬁcation tasks such as sentiment, subjectivity,
and question-type classiﬁcation. However, despite
their celebrated success, there remains a major
limitation from the linguistics perspective: CNNs,
being invented on pixel matrices in image process-
ing, only consider sequential n-grams that are con-
secutive on the surface string and neglect long-
distance dependencies, while the latter play an im-
portant role in many linguistic phenomena such as
negation, subordination, and wh-extraction, all of
which might dully affect the sentiment, subjectiv-
ity, or other categorization of the sentence.

∗ This work was done at both IBM and CUNY, and was supported in
part by DARPA FA8750-13-2-0041 (DEFT), and NSF IIS-1449278. We thank
Yoon Kim for sharing his code, and James Cross and Kai Zhao for discussions.

Indeed, in the sentiment analysis literature, re-
searchers have incorporated long-distance infor-
mation from syntactic parse trees, but the results
are somewhat inconsistent: some reported small
improvements (Gamon, 2004; Matsumoto et al.,
2005), while some otherwise (Dave et al., 2003;
Kudo and Matsumoto, 2004). As a result, syn-
tactic features have yet to become popular in the
sentiment analysis community. We suspect one
of the reasons for this is data sparsity (according
to our experiments, tree n-grams are signiﬁcantly
sparser than surface n-grams), but this problem
has largely been alleviated by the recent advances
in word embedding. Can we combine the advan-
tages of both worlds?

So we propose a very simple dependency-based
convolutional neural networks (DCNNs). Our
model is similar to Kim (2014), but while his se-
quential CNNs put a word in its sequential con-
text, ours considers a word and its parent, grand-
parent, great-grand-parent, and siblings on the de-
pendency tree. This way we incorporate long-
distance information that are otherwise unavail-
able on the surface string.

Experiments on three

classiﬁcation tasks
demonstrate the superior performance of our
DCNNs over the baseline sequential CNNs.
In
particular, our accuracy on the TREC dataset
outperforms all previously published results
including those with heavy
in the literature,
hand-engineered features.

Independently of this work, Mou et al. (2015,
unpublished) reported related efforts; see Sec. 3.3.

2 Dependency-based Convolution

The original CNN, ﬁrst proposed by LeCun et
al. (1995), applies convolution kernels on a se-
ries of continuous areas of given images, and was
adapted to NLP by Collobert et al. (2011). Fol-
lowing Kim (2014), one dimensional convolution
operates the convolution kernel in sequential order
Rd represents the d di-
in Equation 1, where xi ∈
mensional word representation for the i-th word in

5
1
0
2
 
g
u
A
 
3
 
 
]
L
C
.
s
c
[
 
 
2
v
9
3
8
1
0
.
7
0
5
1
:
v
i
X
r
a

Figure 1: Dependency tree of an example sentence from the Movie Reviews dataset.

the sentence, and
Therefore
from the i-th word to the (i + j)-th word:

is the concatenation operator.
xi,j refers to concatenated word vector

⊕

(1)

xi+j

(cid:101)
xi,j = xi ⊕

xi+1 ⊕ · · · ⊕
Sequential word concatenation

(cid:101)

xi,j works as
n-gram models which feeds local information into
convolution operations. However, this setting can
not capture long-distance relationships unless we
enlarge the window indeﬁnitely which would in-
evitably cause the data sparsity problem.

(cid:101)

In order to capture the long-distance dependen-
cies we propose the dependency-based convolu-
tion model (DCNN). Figure 1 illustrates an exam-
ple from the Movie Reviews (MR) dataset (Pang
and Lee, 2005). The sentiment of this sentence
is obviously positive, but this is quite difﬁcult for
sequential CNNs because many n-gram windows
would include the highly negative word “short-
comings”, and the distance between “Despite” and
“shortcomings” is quite long. DCNN, however,
could capture the tree-based bigram “Despite –
shortcomings”, thus ﬂipping the sentiment, and
the tree-based trigram “ROOT – moving – sto-
ries”, which is highly positive.

2.1 Convolution on Ancestor Paths
We deﬁne our concatenation based on the depen-
dency tree for a given modiﬁer xi:
xp(i) ⊕ · · · ⊕
where function pk(i) returns the i-th word’s k-th
ancestor index, which is recursively deﬁned as:

xi,k = xi ⊕

xpk

(2)

1(i)

−

pk(i) =

1(i))

−

p(pk
i
(cid:40)

if k > 0
if k = 0

(3)

Figure 2 (left) illustrates ancestor paths patterns
with various orders. We always start the convo-
lution with xi and concatenate with its ancestors.
If the root node is reached, we add “ROOT” as
dummy ancestors (vertical padding).

For a given tree-based concatenated word se-
quence xi,k, the convolution operation applies a
d to xi,k with a bias term b de-
ﬁlter w
×
∈
scribed in equation 4:

Rk

ci = f (w

xi,k + b)

·

(4)

where f is a non-linear activation function such as
rectiﬁed linear unit (ReLu) or sigmoid function.
The ﬁlter w is applied to each word in the sen-
tence, generating the feature map c

Rl:

∈

c = [c1, c2,

, cl]

· · ·

(5)

where l is the length of the sentence.

2.2 Max-Over-Tree Pooling and Dropout

The ﬁlters convolve with different word concate-
nation in Eq. 4 can be regarded as pattern detec-
tion: only the most similar pattern between the
words and the ﬁlter could return the maximum ac-
tivation. In sequential CNNs, max-over-time pool-
ing (Collobert et al., 2011; Kim, 2014) operates
over the feature map to get the maximum acti-
vation ˆc = max c representing the entire feature
map. Our DCNNs also pool the maximum activa-
tion from feature map to detect the strongest ac-
tivation over the whole tree (i.e., over the whole
sentence). Since the tree no longer deﬁnes a se-
quential “time” direction, we refer to our pooling
as “max-over-tree” pooling.

In order to capture enough variations, we ran-
domly initialize the set of ﬁlters to detect different
structure patterns. Each ﬁlter’s height is the num-
ber of words considered and the width is always
equal to the dimensionality d of word representa-
tion. Each ﬁlter will be represented by only one
feature after max-over-tree pooling. After a series
of convolution with different ﬁlter with different
heights, multiple features carry different structural
information become the ﬁnal representation of the
input sentence. Then, this sentence representation
is passed to a fully connected soft-max layer and
outputs a distribution over different labels.

Neural networks often suffer from overtrain-
ing. Following Kim (2014), we employ random
dropout on penultimate layer (Hinton et al., 2014).
in order to prevent co-adaptation of hidden units.
In our experiments, we set our drop out rate as 0.5
and learning rate as 0.95 by default. Following
Kim (2014), training is done through stochastic
gradient descent over shufﬂed mini-batches with
the Adadelta update rule (Zeiler, 2012).

Figure 2: Convolution patterns on trees. Word concatenation always starts with m, while h, g, and g2
denote parent, grand parent, and great-grand parent, etc., and “ ” denotes words excluded in convolution.

2.3 Convolution on Siblings
Ancestor paths alone is not enough to capture
many linguistic phenomena such as conjunction.
Inspired by higher-order dependency parsing (Mc-
Donald and Pereira, 2006; Koo and Collins, 2010),
we also incorporate siblings for a given word in
various ways. See Figure 2 (right) for details.

2.4 Combined Model
Powerful as it is, structural information still does
not fully cover sequential information. Also, pars-
ing errors (which are common especially for in-
formal text such as online reviews) directly affect
DCNN performance while sequential n-grams are
always correctly observed. To best exploit both in-
formation, we want to combine both models. The
easiest way of combination is to concatenate these
representations together, then feed into fully con-
nected soft-max neural networks. In these cases,
combine with different feature from different type
of sources could stabilize the performance. The
ﬁnal sentence representation is thus:

ˆc = [ˆc(1)

a , ..., ˆc(Na)
ancestors

a

; ˆc(1)

s , ..., ˆc(Ns)
siblings

s

; ˆc(1), ..., ˆc(N )

]

sequential

(cid:124)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

where Na, Ns, and N are the number of ancestor,
sibling, and sequential ﬁlters. In practice, we use
100 ﬁlters for each template in Figure 2 . The fully
combined representation is 1,100-dimensional by
contrast to 300-dimensional for sequential CNN.

3 Experiments

Table 1 summarizes results in the context of other
high-performing efforts in the literature. We use
three benchmark datasets in two categories: senti-
ment analysis on both Movie Review (MR) (Pang
and Lee, 2005) and Stanford Sentiment Treebank
(SST-1) (Socher et al., 2013) datasets, and ques-
tion classiﬁcation on TREC (Li and Roth, 2002).

For all datasets, we ﬁrst obtain the dependency
parse tree from Stanford parser (Manning et al.,
2014).1 Different window size for different choice
of convolution are shown in Figure 2. For the
dataset without a development set (MR), we ran-
domly choose 10% of the training data to indicate
early stopping. In order to have a fare compari-
son with baseline CNN, we also use 3 to 5 as our
window size. Most of our results are generated by
GPU due to its efﬁciency, however CPU could po-
tentially get better results.2 Our implementation,
on top of Kim (2014)’s code,3 will be released.4

3.1 Sentiment Analysis

Both sentiment analysis datasets (MR and SST-
1) are based on movie reviews. The differences
between them are mainly in the different num-
bers of categories and whether the standard split
is given. There are 10,662 sentences in the MR
dataset. Each instance is labeled positive or neg-
ative, and in most cases contains one sentence.
Since no standard data split is given, following the
literature we use 10 fold cross validation to include
every sentence in training and testing at least once.
Concatenating with sibling and sequential infor-
mation obviously improves DCNNs, and the ﬁnal
model outperforms the baseline sequential CNNs
by 0.4, and ties with Zhu et al. (2015).

Different from MR,

the Stanford Sentiment
Treebank (SST-1) annotates ﬁner-grained labels,
very positive, positive, neutral, negative and very
negative, on an extension of the MR dataset. There
are 11,855 sentences with standard split. Our
model achieves an accuracy of 49.5 which is sec-
ond only to Irsoy and Cardie (2014).

1

The phrase-structure trees in SST-1 are actually automatically parsed,

and thus can not be used as gold-standard trees.

2
GPU only supports float32 while CPU supports float64.
3https://github.comw/yoonkim/CNN_sentence
4https://github.com/cosmmb/DCNN

Category

This work

CNNs

Recursive NNs

Recurrent NNs
Other deep learning
Hand-coded rules

Model
DCNNs: ancestor
DCNNs: ancestor+sibling
DCNNs: ancestor+sibling+sequential
CNNs-non-static (Kim, 2014) – baseline
CNNs-multichannel (Kim, 2014)
Deep CNNs (Kalchbrenner et al., 2014)
Recursive Autoencoder (Socher et al., 2011)
Recursive Neural Tensor (Socher et al., 2013)
Deep Recursive NNs (Irsoy and Cardie, 2014)
LSTM on tree (Zhu et al., 2015)
Paragraph-Vec (Le and Mikolov, 2014)
SVMS (Silva et al., 2011)

MR
80.4†
81.7†
81.9
81.5
81.1
-
77.7
-
-
81.9
-
-

SST-1 TREC TREC-2
47.7†
48.3†
49.5
48.0
47.4
48.5
43.2
45.7
49.8
48.0
48.7

88.4†
89.0†
88.8†
86.4∗
86.0∗
-
-
-
-
-
-
90.8

95.4†
95.6†
95.4†
93.6
92.2
93.0
-
-
-
-
-
95.0

Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.
TREC-2 is TREC with ﬁne grained labels. †Results generated by GPU (all others generated by CPU).
∗Results generated from Kim (2014)’s implementation.

3.2 Question Classiﬁcation
In the TREC dataset, the entire dataset of 5,952
sentences are classiﬁed into the following 6 cate-
gories: abbreviation, entity, description, location
and numeric. In this experiment, DCNNs easily
outperform any other methods even with ancestor
convolution only. DCNNs with sibling achieve the
best performance in the published literature. DC-
NNs combined with sibling and sequential infor-
mation might suffer from overﬁtting on the train-
ing data based on our observation. One thing
to note here is that our best result even exceeds
SVMS (Silva et al., 2011) with 60 hand-coded
rules.

The TREC dataset also provides subcategories
such as numeric:temperature, numeric:distance,
and entity:vehicle. To make our task more real-
istic and challenging, we also test the proposed
model with respect to the 50 subcategories. There
are obvious improvements over sequential CNNs
from the last column of Table 1. Like ours, Silva
et al. (2011) is a tree-based system but it uses
constituency trees compared to ours dependency
trees. They report a higher ﬁne-grained accuracy
of 90.8 but their parser is trained only on the Ques-
tionBank (Judge et al., 2006) while we used the
standard Stanford parser trained on both the Penn
Treebank and QuestionBank. Moreover, as men-
tioned above, their approach is rule-based while
ours is automatically learned.

3.3 Discussions and Examples
Compared with sentiment analysis, the advantage
of our proposed model is obviously more substan-
tial on the TREC dataset. Based on our error anal-
ysis, we conclude that this is mainly due to the

Figure 3: Examples from TREC (a–c), SST-1 (d)
and TREC with ﬁne-grained label (e–f) that are
misclassiﬁed by the baseline CNN but correctly
labeled by our DCNN. For example, (a) should be
entity but is labeled location by CNN.

Figure 4: Examples from TREC datasets that are
misclassiﬁed by DCNN but correctly labeled by
baseline CNN. For example, (a) should be numer-
ical but is labeled entity by DCNN.

difference of the parse tree quality between the
two tasks.
In sentiment analysis, the dataset is
collected from the Rotten Tomatoes website which
includes many irregular usage of language. Some
of the sentences even come from languages other
than English. The errors in parse trees inevitably
affect the classiﬁcation accuracy. However, the
parser works substantially better on the TREC
dataset since all questions are in formal written
English, and the training set for Stanford parser5
already includes the QuestionBank (Judge et al.,
2006) which includes 2,000 TREC sentences.

Figure 3 visualizes examples where CNN errs
while DCNN does not. For example, CNN la-
bels (a) as location due to “Hawaii” and “state”,
while the long-distance backbone “What – ﬂower”
is clearly asking for an entity. Similarly, in (d),
DCNN captures the obviously negative tree-based
trigram “Nothing – worth – emailing”. Note that
our model also works with non-projective depen-
dency trees such as the one in (b). The last two ex-
amples in Figure 3 visualize cases where DCNN
outperforms the baseline CNNs in ﬁne-grained
TREC. In example (e), the word “temperature” is
at second from the top and is root of a 8 word span
“the ... earth”. When we use a window of size 5
for tree convolution, every words in that span get
convolved with “temperature” and this should be
the reason why DCNN get correct.

Figure 4 showcases examples where baseline
CNNs get better results than DCNNs. Example
(a) is misclassiﬁed as entity by DCNN due to pars-
ing/tagging error (the Stanford parser performs its

5

http://nlp.stanford.edu/software/parser-faq.shtml

Figure 5: Examples from TREC datasets that are
misclassiﬁed by both DCNN and baseline CNN.
For example, (a) should be numerical but is la-
beled entity by DCNN and description by CNN.

own part-of-speech tagging). The word “ﬂy” at
the end of the sentence should be a verb instead of
noun, and “hummingbirds ﬂy” should be a relative
clause modifying “speed”.

There are some sentences that are misclassiﬁed
by both the baseline CNN and DCNN. Figure 5
shows three such examples. Example (a) is not
classiﬁed as numerical by both methods due to the
ambiguous meaning of the word “point” which is
difﬁcult to capture by word embedding. This word
can mean location, opinion, etc. Apparently, the
numerical aspect is not captured by word embed-
ding. Example (c) might be an annotation error.

Shortly before submitting to ACL 2015 we
learned Mou et al. (2015, unpublished) have inde-
pendently reported concurrent and related efforts.
Their constituency model, based on their unpub-
lished work in programming languages (Mou et
al., 2014),6 performs convolution on pretrained re-
cursive node representations rather than word em-
beddings, thus baring little, if any, resemblance to
our dependency-based model. Their dependency
model is related, but always includes a node and
all its children (resembling Iyyer et al. (2014)),
which is a variant of our sibling model and always
ﬂat. By contrast, our ancestor model looks at the
vertical path from any word to its ancestors, being
linguistically motivated (Shen et al., 2008).

4 Conclusions
We have presented a very simple dependency-
based convolution framework which outperforms
sequential CNN baselines on modeling sentences.

6

Both their 2014 and 2015 reports proposed (independently of each other
and independently of our work) the term “tree-based convolution” (TBCNN).

References

R. Collobert,

J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12.

Kushal Dave, Steve Lawrence, and David M Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classiﬁcation of product reviews.
In
Proceedings of World Wide Web.

Michael Gamon. 2004. Sentiment classiﬁcation on
customer feedback data: noisy data, large feature
vectors, and the role of linguistic analysis. In Pro-
ceedings of COLING.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
Improving neural networks by
dinov.
feature detectors.
preventing co-adaptation of
Journal of Machine Learning Research, 15.

2014.

Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Advances in Neural Information Processing Sys-
tems, pages 2096–2104.

Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,
Richard Socher, and Hal Daum´e III. 2014. A neural
network for factoid question answering over para-
graphs. In Proceedings of EMNLP.

John Judge, Aoife Cahill, and Josef van Genabith.
2006. Questionbank: Creating a corpus of parse-
annotated questions. In Proceedings of COLING.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of EMNLP.

Terry Koo and Michael Collins. 2010. Efﬁcient third-
order dependency parsers. In Proceedings of ACL.

Taku Kudo and Yuji Matsumoto. 2004. A boosting
algorithm for classiﬁcation of semi-structured text.
In Proceedings of EMNLP.

Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
ceedings of ICML.

Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes,
J. Denker, H. Drucker,
I. Guyon, U. Mller,
E. Sckinger, P. Simard, and V. Vapnik. 1995. Com-
parison of learning algorithms for handwritten digit
recognition. In Int’l Conf. on Artiﬁcial Neural Nets.

Xin Li and Dan Roth. 2002. Learning question classi-

ﬁers. In Proceedings of COLING.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
In Proceedings of ACL:
guage processing toolkit.
Demonstrations, pages 55–60.

Shotaro Matsumoto, Hiroya Takamura, and Manabu
2005. Sentiment classiﬁcation using
In

Okumura.
word sub-sequences and dependency sub-trees.
Proceedings of PA-KDD.

Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL.

Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang.
2014. TBCNN: A tree-based convolutional neu-
ral network for programming language processing.
Unpublished manuscript: http://arxiv.org/
abs/1409.5718.

Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and
Zhi Jin.
2015. Discriminative neural sentence
modeling by tree-based convolution. Unpublished
manuscript: http://arxiv.org/abs/1504.
01106v5. Version 5 dated June 2, 2015; Version 1
(“Tree-based Convolution: A New Architecture for
Sentence Modeling”) dated Apr 5, 2015.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of ACL,
pages 115–124.

Libin Shen, Lucas Champollion, and Aravind K Joshi.
2008. LTAG-spinal and the treebank. Language Re-
sources and Evaluation, 42(1):1–19.

Yelong Shen, Xiaodong he, Jianfeng Gao, Li Deng, and
Gregoire Mesnil. 2014. Learning semantic repre-
sentations using convolutional neural networks for
web search. In Proceedings of WWW.

J. Silva, L. Coheur, A. C. Mendes, and Andreas
Wichert. 2011. From symbolic to sub-symbolic in-
formation in question classiﬁcation. Artiﬁcial Intel-
ligence Review, 35.

Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
EMNLP 2011.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP 2013.

Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation question
answering. In Proceedings of ACL.

Mattgew Zeiler. 2012. Adadelta: An adaptive learning
rate method. Unpublished manuscript: http://
arxiv.org/abs/1212.5701.

Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long short-term memory over tree structures.
In Proceedings of ICML.

Dependency-based Convolutional Neural Networks
for Sentence Embedding∗

Mingbo Ma†
Liang Huang† ‡
†Graduate Center & Queens College
City University of New York

Bing Xiang‡

Bowen Zhou‡

‡IBM Watson Group
T. J. Watson Research Center, IBM

mma2,lhuang
}

{

gc.cuny.edu

@us.ibm.com
lhuang,bingxia,zhou
}

{

Abstract

In sentence modeling and classiﬁcation,
convolutional neural network approaches
have recently achieved state-of-the-art re-
sults, but all such efforts process word vec-
tors sequentially and neglect long-distance
dependencies. To combine deep learn-
ing with linguistic structures, we pro-
pose a dependency-based convolution ap-
proach, making use of tree-based n-grams
rather than surface ones, thus utlizing non-
local interactions between words. Our
model improves sequential baselines on all
four sentiment and question classiﬁcation
tasks, and achieves the highest published
accuracy on TREC.

Introduction

1
Convolutional neural networks (CNNs), originally
invented in computer vision (LeCun et al., 1995),
has recently attracted much attention in natural
language processing (NLP) on problems such as
sequence labeling (Collobert et al., 2011), seman-
tic parsing (Yih et al., 2014), and search query
retrieval (Shen et al., 2014). In particular, recent
work on CNN-based sentence modeling (Kalch-
brenner et al., 2014; Kim, 2014) has achieved ex-
cellent, often state-of-the-art, results on various
classiﬁcation tasks such as sentiment, subjectivity,
and question-type classiﬁcation. However, despite
their celebrated success, there remains a major
limitation from the linguistics perspective: CNNs,
being invented on pixel matrices in image process-
ing, only consider sequential n-grams that are con-
secutive on the surface string and neglect long-
distance dependencies, while the latter play an im-
portant role in many linguistic phenomena such as
negation, subordination, and wh-extraction, all of
which might dully affect the sentiment, subjectiv-
ity, or other categorization of the sentence.

∗ This work was done at both IBM and CUNY, and was supported in
part by DARPA FA8750-13-2-0041 (DEFT), and NSF IIS-1449278. We thank
Yoon Kim for sharing his code, and James Cross and Kai Zhao for discussions.

Indeed, in the sentiment analysis literature, re-
searchers have incorporated long-distance infor-
mation from syntactic parse trees, but the results
are somewhat inconsistent: some reported small
improvements (Gamon, 2004; Matsumoto et al.,
2005), while some otherwise (Dave et al., 2003;
Kudo and Matsumoto, 2004). As a result, syn-
tactic features have yet to become popular in the
sentiment analysis community. We suspect one
of the reasons for this is data sparsity (according
to our experiments, tree n-grams are signiﬁcantly
sparser than surface n-grams), but this problem
has largely been alleviated by the recent advances
in word embedding. Can we combine the advan-
tages of both worlds?

So we propose a very simple dependency-based
convolutional neural networks (DCNNs). Our
model is similar to Kim (2014), but while his se-
quential CNNs put a word in its sequential con-
text, ours considers a word and its parent, grand-
parent, great-grand-parent, and siblings on the de-
pendency tree. This way we incorporate long-
distance information that are otherwise unavail-
able on the surface string.

Experiments on three

classiﬁcation tasks
demonstrate the superior performance of our
DCNNs over the baseline sequential CNNs.
In
particular, our accuracy on the TREC dataset
outperforms all previously published results
including those with heavy
in the literature,
hand-engineered features.

Independently of this work, Mou et al. (2015,
unpublished) reported related efforts; see Sec. 3.3.

2 Dependency-based Convolution

The original CNN, ﬁrst proposed by LeCun et
al. (1995), applies convolution kernels on a se-
ries of continuous areas of given images, and was
adapted to NLP by Collobert et al. (2011). Fol-
lowing Kim (2014), one dimensional convolution
operates the convolution kernel in sequential order
Rd represents the d di-
in Equation 1, where xi ∈
mensional word representation for the i-th word in

5
1
0
2
 
g
u
A
 
3
 
 
]
L
C
.
s
c
[
 
 
2
v
9
3
8
1
0
.
7
0
5
1
:
v
i
X
r
a

Figure 1: Dependency tree of an example sentence from the Movie Reviews dataset.

the sentence, and
Therefore
from the i-th word to the (i + j)-th word:

is the concatenation operator.
xi,j refers to concatenated word vector

⊕

(1)

xi+j

(cid:101)
xi,j = xi ⊕

xi+1 ⊕ · · · ⊕
Sequential word concatenation

(cid:101)

xi,j works as
n-gram models which feeds local information into
convolution operations. However, this setting can
not capture long-distance relationships unless we
enlarge the window indeﬁnitely which would in-
evitably cause the data sparsity problem.

(cid:101)

In order to capture the long-distance dependen-
cies we propose the dependency-based convolu-
tion model (DCNN). Figure 1 illustrates an exam-
ple from the Movie Reviews (MR) dataset (Pang
and Lee, 2005). The sentiment of this sentence
is obviously positive, but this is quite difﬁcult for
sequential CNNs because many n-gram windows
would include the highly negative word “short-
comings”, and the distance between “Despite” and
“shortcomings” is quite long. DCNN, however,
could capture the tree-based bigram “Despite –
shortcomings”, thus ﬂipping the sentiment, and
the tree-based trigram “ROOT – moving – sto-
ries”, which is highly positive.

2.1 Convolution on Ancestor Paths
We deﬁne our concatenation based on the depen-
dency tree for a given modiﬁer xi:
xp(i) ⊕ · · · ⊕
where function pk(i) returns the i-th word’s k-th
ancestor index, which is recursively deﬁned as:

xi,k = xi ⊕

xpk

(2)

1(i)

−

pk(i) =

1(i))

−

p(pk
i
(cid:40)

if k > 0
if k = 0

(3)

Figure 2 (left) illustrates ancestor paths patterns
with various orders. We always start the convo-
lution with xi and concatenate with its ancestors.
If the root node is reached, we add “ROOT” as
dummy ancestors (vertical padding).

For a given tree-based concatenated word se-
quence xi,k, the convolution operation applies a
d to xi,k with a bias term b de-
ﬁlter w
×
∈
scribed in equation 4:

Rk

ci = f (w

xi,k + b)

·

(4)

where f is a non-linear activation function such as
rectiﬁed linear unit (ReLu) or sigmoid function.
The ﬁlter w is applied to each word in the sen-
tence, generating the feature map c

Rl:

∈

c = [c1, c2,

, cl]

· · ·

(5)

where l is the length of the sentence.

2.2 Max-Over-Tree Pooling and Dropout

The ﬁlters convolve with different word concate-
nation in Eq. 4 can be regarded as pattern detec-
tion: only the most similar pattern between the
words and the ﬁlter could return the maximum ac-
tivation. In sequential CNNs, max-over-time pool-
ing (Collobert et al., 2011; Kim, 2014) operates
over the feature map to get the maximum acti-
vation ˆc = max c representing the entire feature
map. Our DCNNs also pool the maximum activa-
tion from feature map to detect the strongest ac-
tivation over the whole tree (i.e., over the whole
sentence). Since the tree no longer deﬁnes a se-
quential “time” direction, we refer to our pooling
as “max-over-tree” pooling.

In order to capture enough variations, we ran-
domly initialize the set of ﬁlters to detect different
structure patterns. Each ﬁlter’s height is the num-
ber of words considered and the width is always
equal to the dimensionality d of word representa-
tion. Each ﬁlter will be represented by only one
feature after max-over-tree pooling. After a series
of convolution with different ﬁlter with different
heights, multiple features carry different structural
information become the ﬁnal representation of the
input sentence. Then, this sentence representation
is passed to a fully connected soft-max layer and
outputs a distribution over different labels.

Neural networks often suffer from overtrain-
ing. Following Kim (2014), we employ random
dropout on penultimate layer (Hinton et al., 2014).
in order to prevent co-adaptation of hidden units.
In our experiments, we set our drop out rate as 0.5
and learning rate as 0.95 by default. Following
Kim (2014), training is done through stochastic
gradient descent over shufﬂed mini-batches with
the Adadelta update rule (Zeiler, 2012).

Figure 2: Convolution patterns on trees. Word concatenation always starts with m, while h, g, and g2
denote parent, grand parent, and great-grand parent, etc., and “ ” denotes words excluded in convolution.

2.3 Convolution on Siblings
Ancestor paths alone is not enough to capture
many linguistic phenomena such as conjunction.
Inspired by higher-order dependency parsing (Mc-
Donald and Pereira, 2006; Koo and Collins, 2010),
we also incorporate siblings for a given word in
various ways. See Figure 2 (right) for details.

2.4 Combined Model
Powerful as it is, structural information still does
not fully cover sequential information. Also, pars-
ing errors (which are common especially for in-
formal text such as online reviews) directly affect
DCNN performance while sequential n-grams are
always correctly observed. To best exploit both in-
formation, we want to combine both models. The
easiest way of combination is to concatenate these
representations together, then feed into fully con-
nected soft-max neural networks. In these cases,
combine with different feature from different type
of sources could stabilize the performance. The
ﬁnal sentence representation is thus:

ˆc = [ˆc(1)

a , ..., ˆc(Na)
ancestors

a

; ˆc(1)

s , ..., ˆc(Ns)
siblings

s

; ˆc(1), ..., ˆc(N )

]

sequential

(cid:124)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

where Na, Ns, and N are the number of ancestor,
sibling, and sequential ﬁlters. In practice, we use
100 ﬁlters for each template in Figure 2 . The fully
combined representation is 1,100-dimensional by
contrast to 300-dimensional for sequential CNN.

3 Experiments

Table 1 summarizes results in the context of other
high-performing efforts in the literature. We use
three benchmark datasets in two categories: senti-
ment analysis on both Movie Review (MR) (Pang
and Lee, 2005) and Stanford Sentiment Treebank
(SST-1) (Socher et al., 2013) datasets, and ques-
tion classiﬁcation on TREC (Li and Roth, 2002).

For all datasets, we ﬁrst obtain the dependency
parse tree from Stanford parser (Manning et al.,
2014).1 Different window size for different choice
of convolution are shown in Figure 2. For the
dataset without a development set (MR), we ran-
domly choose 10% of the training data to indicate
early stopping. In order to have a fare compari-
son with baseline CNN, we also use 3 to 5 as our
window size. Most of our results are generated by
GPU due to its efﬁciency, however CPU could po-
tentially get better results.2 Our implementation,
on top of Kim (2014)’s code,3 will be released.4

3.1 Sentiment Analysis

Both sentiment analysis datasets (MR and SST-
1) are based on movie reviews. The differences
between them are mainly in the different num-
bers of categories and whether the standard split
is given. There are 10,662 sentences in the MR
dataset. Each instance is labeled positive or neg-
ative, and in most cases contains one sentence.
Since no standard data split is given, following the
literature we use 10 fold cross validation to include
every sentence in training and testing at least once.
Concatenating with sibling and sequential infor-
mation obviously improves DCNNs, and the ﬁnal
model outperforms the baseline sequential CNNs
by 0.4, and ties with Zhu et al. (2015).

Different from MR,

the Stanford Sentiment
Treebank (SST-1) annotates ﬁner-grained labels,
very positive, positive, neutral, negative and very
negative, on an extension of the MR dataset. There
are 11,855 sentences with standard split. Our
model achieves an accuracy of 49.5 which is sec-
ond only to Irsoy and Cardie (2014).

1

The phrase-structure trees in SST-1 are actually automatically parsed,

and thus can not be used as gold-standard trees.

2
GPU only supports float32 while CPU supports float64.
3https://github.comw/yoonkim/CNN_sentence
4https://github.com/cosmmb/DCNN

Category

This work

CNNs

Recursive NNs

Recurrent NNs
Other deep learning
Hand-coded rules

Model
DCNNs: ancestor
DCNNs: ancestor+sibling
DCNNs: ancestor+sibling+sequential
CNNs-non-static (Kim, 2014) – baseline
CNNs-multichannel (Kim, 2014)
Deep CNNs (Kalchbrenner et al., 2014)
Recursive Autoencoder (Socher et al., 2011)
Recursive Neural Tensor (Socher et al., 2013)
Deep Recursive NNs (Irsoy and Cardie, 2014)
LSTM on tree (Zhu et al., 2015)
Paragraph-Vec (Le and Mikolov, 2014)
SVMS (Silva et al., 2011)

MR
80.4†
81.7†
81.9
81.5
81.1
-
77.7
-
-
81.9
-
-

SST-1 TREC TREC-2
47.7†
48.3†
49.5
48.0
47.4
48.5
43.2
45.7
49.8
48.0
48.7

88.4†
89.0†
88.8†
86.4∗
86.0∗
-
-
-
-
-
-
90.8

95.4†
95.6†
95.4†
93.6
92.2
93.0
-
-
-
-
-
95.0

Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.
TREC-2 is TREC with ﬁne grained labels. †Results generated by GPU (all others generated by CPU).
∗Results generated from Kim (2014)’s implementation.

3.2 Question Classiﬁcation
In the TREC dataset, the entire dataset of 5,952
sentences are classiﬁed into the following 6 cate-
gories: abbreviation, entity, description, location
and numeric. In this experiment, DCNNs easily
outperform any other methods even with ancestor
convolution only. DCNNs with sibling achieve the
best performance in the published literature. DC-
NNs combined with sibling and sequential infor-
mation might suffer from overﬁtting on the train-
ing data based on our observation. One thing
to note here is that our best result even exceeds
SVMS (Silva et al., 2011) with 60 hand-coded
rules.

The TREC dataset also provides subcategories
such as numeric:temperature, numeric:distance,
and entity:vehicle. To make our task more real-
istic and challenging, we also test the proposed
model with respect to the 50 subcategories. There
are obvious improvements over sequential CNNs
from the last column of Table 1. Like ours, Silva
et al. (2011) is a tree-based system but it uses
constituency trees compared to ours dependency
trees. They report a higher ﬁne-grained accuracy
of 90.8 but their parser is trained only on the Ques-
tionBank (Judge et al., 2006) while we used the
standard Stanford parser trained on both the Penn
Treebank and QuestionBank. Moreover, as men-
tioned above, their approach is rule-based while
ours is automatically learned.

3.3 Discussions and Examples
Compared with sentiment analysis, the advantage
of our proposed model is obviously more substan-
tial on the TREC dataset. Based on our error anal-
ysis, we conclude that this is mainly due to the

Figure 3: Examples from TREC (a–c), SST-1 (d)
and TREC with ﬁne-grained label (e–f) that are
misclassiﬁed by the baseline CNN but correctly
labeled by our DCNN. For example, (a) should be
entity but is labeled location by CNN.

Figure 4: Examples from TREC datasets that are
misclassiﬁed by DCNN but correctly labeled by
baseline CNN. For example, (a) should be numer-
ical but is labeled entity by DCNN.

difference of the parse tree quality between the
two tasks.
In sentiment analysis, the dataset is
collected from the Rotten Tomatoes website which
includes many irregular usage of language. Some
of the sentences even come from languages other
than English. The errors in parse trees inevitably
affect the classiﬁcation accuracy. However, the
parser works substantially better on the TREC
dataset since all questions are in formal written
English, and the training set for Stanford parser5
already includes the QuestionBank (Judge et al.,
2006) which includes 2,000 TREC sentences.

Figure 3 visualizes examples where CNN errs
while DCNN does not. For example, CNN la-
bels (a) as location due to “Hawaii” and “state”,
while the long-distance backbone “What – ﬂower”
is clearly asking for an entity. Similarly, in (d),
DCNN captures the obviously negative tree-based
trigram “Nothing – worth – emailing”. Note that
our model also works with non-projective depen-
dency trees such as the one in (b). The last two ex-
amples in Figure 3 visualize cases where DCNN
outperforms the baseline CNNs in ﬁne-grained
TREC. In example (e), the word “temperature” is
at second from the top and is root of a 8 word span
“the ... earth”. When we use a window of size 5
for tree convolution, every words in that span get
convolved with “temperature” and this should be
the reason why DCNN get correct.

Figure 4 showcases examples where baseline
CNNs get better results than DCNNs. Example
(a) is misclassiﬁed as entity by DCNN due to pars-
ing/tagging error (the Stanford parser performs its

5

http://nlp.stanford.edu/software/parser-faq.shtml

Figure 5: Examples from TREC datasets that are
misclassiﬁed by both DCNN and baseline CNN.
For example, (a) should be numerical but is la-
beled entity by DCNN and description by CNN.

own part-of-speech tagging). The word “ﬂy” at
the end of the sentence should be a verb instead of
noun, and “hummingbirds ﬂy” should be a relative
clause modifying “speed”.

There are some sentences that are misclassiﬁed
by both the baseline CNN and DCNN. Figure 5
shows three such examples. Example (a) is not
classiﬁed as numerical by both methods due to the
ambiguous meaning of the word “point” which is
difﬁcult to capture by word embedding. This word
can mean location, opinion, etc. Apparently, the
numerical aspect is not captured by word embed-
ding. Example (c) might be an annotation error.

Shortly before submitting to ACL 2015 we
learned Mou et al. (2015, unpublished) have inde-
pendently reported concurrent and related efforts.
Their constituency model, based on their unpub-
lished work in programming languages (Mou et
al., 2014),6 performs convolution on pretrained re-
cursive node representations rather than word em-
beddings, thus baring little, if any, resemblance to
our dependency-based model. Their dependency
model is related, but always includes a node and
all its children (resembling Iyyer et al. (2014)),
which is a variant of our sibling model and always
ﬂat. By contrast, our ancestor model looks at the
vertical path from any word to its ancestors, being
linguistically motivated (Shen et al., 2008).

4 Conclusions
We have presented a very simple dependency-
based convolution framework which outperforms
sequential CNN baselines on modeling sentences.

6

Both their 2014 and 2015 reports proposed (independently of each other
and independently of our work) the term “tree-based convolution” (TBCNN).

References

R. Collobert,

J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12.

Kushal Dave, Steve Lawrence, and David M Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classiﬁcation of product reviews.
In
Proceedings of World Wide Web.

Michael Gamon. 2004. Sentiment classiﬁcation on
customer feedback data: noisy data, large feature
vectors, and the role of linguistic analysis. In Pro-
ceedings of COLING.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
Improving neural networks by
dinov.
feature detectors.
preventing co-adaptation of
Journal of Machine Learning Research, 15.

2014.

Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Advances in Neural Information Processing Sys-
tems, pages 2096–2104.

Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,
Richard Socher, and Hal Daum´e III. 2014. A neural
network for factoid question answering over para-
graphs. In Proceedings of EMNLP.

John Judge, Aoife Cahill, and Josef van Genabith.
2006. Questionbank: Creating a corpus of parse-
annotated questions. In Proceedings of COLING.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of EMNLP.

Terry Koo and Michael Collins. 2010. Efﬁcient third-
order dependency parsers. In Proceedings of ACL.

Taku Kudo and Yuji Matsumoto. 2004. A boosting
algorithm for classiﬁcation of semi-structured text.
In Proceedings of EMNLP.

Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
ceedings of ICML.

Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes,
J. Denker, H. Drucker,
I. Guyon, U. Mller,
E. Sckinger, P. Simard, and V. Vapnik. 1995. Com-
parison of learning algorithms for handwritten digit
recognition. In Int’l Conf. on Artiﬁcial Neural Nets.

Xin Li and Dan Roth. 2002. Learning question classi-

ﬁers. In Proceedings of COLING.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
In Proceedings of ACL:
guage processing toolkit.
Demonstrations, pages 55–60.

Shotaro Matsumoto, Hiroya Takamura, and Manabu
2005. Sentiment classiﬁcation using
In

Okumura.
word sub-sequences and dependency sub-trees.
Proceedings of PA-KDD.

Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL.

Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang.
2014. TBCNN: A tree-based convolutional neu-
ral network for programming language processing.
Unpublished manuscript: http://arxiv.org/
abs/1409.5718.

Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and
Zhi Jin.
2015. Discriminative neural sentence
modeling by tree-based convolution. Unpublished
manuscript: http://arxiv.org/abs/1504.
01106v5. Version 5 dated June 2, 2015; Version 1
(“Tree-based Convolution: A New Architecture for
Sentence Modeling”) dated Apr 5, 2015.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of ACL,
pages 115–124.

Libin Shen, Lucas Champollion, and Aravind K Joshi.
2008. LTAG-spinal and the treebank. Language Re-
sources and Evaluation, 42(1):1–19.

Yelong Shen, Xiaodong he, Jianfeng Gao, Li Deng, and
Gregoire Mesnil. 2014. Learning semantic repre-
sentations using convolutional neural networks for
web search. In Proceedings of WWW.

J. Silva, L. Coheur, A. C. Mendes, and Andreas
Wichert. 2011. From symbolic to sub-symbolic in-
formation in question classiﬁcation. Artiﬁcial Intel-
ligence Review, 35.

Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
EMNLP 2011.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP 2013.

Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation question
answering. In Proceedings of ACL.

Mattgew Zeiler. 2012. Adadelta: An adaptive learning
rate method. Unpublished manuscript: http://
arxiv.org/abs/1212.5701.

Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long short-term memory over tree structures.
In Proceedings of ICML.

Dependency-based Convolutional Neural Networks
for Sentence Embedding∗

Mingbo Ma†
Liang Huang† ‡
†Graduate Center & Queens College
City University of New York

Bing Xiang‡

Bowen Zhou‡

‡IBM Watson Group
T. J. Watson Research Center, IBM

mma2,lhuang
}

{

gc.cuny.edu

@us.ibm.com
lhuang,bingxia,zhou
}

{

Abstract

In sentence modeling and classiﬁcation,
convolutional neural network approaches
have recently achieved state-of-the-art re-
sults, but all such efforts process word vec-
tors sequentially and neglect long-distance
dependencies. To combine deep learn-
ing with linguistic structures, we pro-
pose a dependency-based convolution ap-
proach, making use of tree-based n-grams
rather than surface ones, thus utlizing non-
local interactions between words. Our
model improves sequential baselines on all
four sentiment and question classiﬁcation
tasks, and achieves the highest published
accuracy on TREC.

Introduction

1
Convolutional neural networks (CNNs), originally
invented in computer vision (LeCun et al., 1995),
has recently attracted much attention in natural
language processing (NLP) on problems such as
sequence labeling (Collobert et al., 2011), seman-
tic parsing (Yih et al., 2014), and search query
retrieval (Shen et al., 2014). In particular, recent
work on CNN-based sentence modeling (Kalch-
brenner et al., 2014; Kim, 2014) has achieved ex-
cellent, often state-of-the-art, results on various
classiﬁcation tasks such as sentiment, subjectivity,
and question-type classiﬁcation. However, despite
their celebrated success, there remains a major
limitation from the linguistics perspective: CNNs,
being invented on pixel matrices in image process-
ing, only consider sequential n-grams that are con-
secutive on the surface string and neglect long-
distance dependencies, while the latter play an im-
portant role in many linguistic phenomena such as
negation, subordination, and wh-extraction, all of
which might dully affect the sentiment, subjectiv-
ity, or other categorization of the sentence.

∗ This work was done at both IBM and CUNY, and was supported in
part by DARPA FA8750-13-2-0041 (DEFT), and NSF IIS-1449278. We thank
Yoon Kim for sharing his code, and James Cross and Kai Zhao for discussions.

Indeed, in the sentiment analysis literature, re-
searchers have incorporated long-distance infor-
mation from syntactic parse trees, but the results
are somewhat inconsistent: some reported small
improvements (Gamon, 2004; Matsumoto et al.,
2005), while some otherwise (Dave et al., 2003;
Kudo and Matsumoto, 2004). As a result, syn-
tactic features have yet to become popular in the
sentiment analysis community. We suspect one
of the reasons for this is data sparsity (according
to our experiments, tree n-grams are signiﬁcantly
sparser than surface n-grams), but this problem
has largely been alleviated by the recent advances
in word embedding. Can we combine the advan-
tages of both worlds?

So we propose a very simple dependency-based
convolutional neural networks (DCNNs). Our
model is similar to Kim (2014), but while his se-
quential CNNs put a word in its sequential con-
text, ours considers a word and its parent, grand-
parent, great-grand-parent, and siblings on the de-
pendency tree. This way we incorporate long-
distance information that are otherwise unavail-
able on the surface string.

Experiments on three

classiﬁcation tasks
demonstrate the superior performance of our
DCNNs over the baseline sequential CNNs.
In
particular, our accuracy on the TREC dataset
outperforms all previously published results
including those with heavy
in the literature,
hand-engineered features.

Independently of this work, Mou et al. (2015,
unpublished) reported related efforts; see Sec. 3.3.

2 Dependency-based Convolution

The original CNN, ﬁrst proposed by LeCun et
al. (1995), applies convolution kernels on a se-
ries of continuous areas of given images, and was
adapted to NLP by Collobert et al. (2011). Fol-
lowing Kim (2014), one dimensional convolution
operates the convolution kernel in sequential order
Rd represents the d di-
in Equation 1, where xi ∈
mensional word representation for the i-th word in

5
1
0
2
 
g
u
A
 
3
 
 
]
L
C
.
s
c
[
 
 
2
v
9
3
8
1
0
.
7
0
5
1
:
v
i
X
r
a

Figure 1: Dependency tree of an example sentence from the Movie Reviews dataset.

the sentence, and
Therefore
from the i-th word to the (i + j)-th word:

is the concatenation operator.
xi,j refers to concatenated word vector

⊕

(1)

xi+j

(cid:101)
xi,j = xi ⊕

xi+1 ⊕ · · · ⊕
Sequential word concatenation

(cid:101)

xi,j works as
n-gram models which feeds local information into
convolution operations. However, this setting can
not capture long-distance relationships unless we
enlarge the window indeﬁnitely which would in-
evitably cause the data sparsity problem.

(cid:101)

In order to capture the long-distance dependen-
cies we propose the dependency-based convolu-
tion model (DCNN). Figure 1 illustrates an exam-
ple from the Movie Reviews (MR) dataset (Pang
and Lee, 2005). The sentiment of this sentence
is obviously positive, but this is quite difﬁcult for
sequential CNNs because many n-gram windows
would include the highly negative word “short-
comings”, and the distance between “Despite” and
“shortcomings” is quite long. DCNN, however,
could capture the tree-based bigram “Despite –
shortcomings”, thus ﬂipping the sentiment, and
the tree-based trigram “ROOT – moving – sto-
ries”, which is highly positive.

2.1 Convolution on Ancestor Paths
We deﬁne our concatenation based on the depen-
dency tree for a given modiﬁer xi:
xp(i) ⊕ · · · ⊕
where function pk(i) returns the i-th word’s k-th
ancestor index, which is recursively deﬁned as:

xi,k = xi ⊕

xpk

(2)

1(i)

−

pk(i) =

1(i))

−

p(pk
i
(cid:40)

if k > 0
if k = 0

(3)

Figure 2 (left) illustrates ancestor paths patterns
with various orders. We always start the convo-
lution with xi and concatenate with its ancestors.
If the root node is reached, we add “ROOT” as
dummy ancestors (vertical padding).

For a given tree-based concatenated word se-
quence xi,k, the convolution operation applies a
d to xi,k with a bias term b de-
ﬁlter w
×
∈
scribed in equation 4:

Rk

ci = f (w

xi,k + b)

·

(4)

where f is a non-linear activation function such as
rectiﬁed linear unit (ReLu) or sigmoid function.
The ﬁlter w is applied to each word in the sen-
tence, generating the feature map c

Rl:

∈

c = [c1, c2,

, cl]

· · ·

(5)

where l is the length of the sentence.

2.2 Max-Over-Tree Pooling and Dropout

The ﬁlters convolve with different word concate-
nation in Eq. 4 can be regarded as pattern detec-
tion: only the most similar pattern between the
words and the ﬁlter could return the maximum ac-
tivation. In sequential CNNs, max-over-time pool-
ing (Collobert et al., 2011; Kim, 2014) operates
over the feature map to get the maximum acti-
vation ˆc = max c representing the entire feature
map. Our DCNNs also pool the maximum activa-
tion from feature map to detect the strongest ac-
tivation over the whole tree (i.e., over the whole
sentence). Since the tree no longer deﬁnes a se-
quential “time” direction, we refer to our pooling
as “max-over-tree” pooling.

In order to capture enough variations, we ran-
domly initialize the set of ﬁlters to detect different
structure patterns. Each ﬁlter’s height is the num-
ber of words considered and the width is always
equal to the dimensionality d of word representa-
tion. Each ﬁlter will be represented by only one
feature after max-over-tree pooling. After a series
of convolution with different ﬁlter with different
heights, multiple features carry different structural
information become the ﬁnal representation of the
input sentence. Then, this sentence representation
is passed to a fully connected soft-max layer and
outputs a distribution over different labels.

Neural networks often suffer from overtrain-
ing. Following Kim (2014), we employ random
dropout on penultimate layer (Hinton et al., 2014).
in order to prevent co-adaptation of hidden units.
In our experiments, we set our drop out rate as 0.5
and learning rate as 0.95 by default. Following
Kim (2014), training is done through stochastic
gradient descent over shufﬂed mini-batches with
the Adadelta update rule (Zeiler, 2012).

Figure 2: Convolution patterns on trees. Word concatenation always starts with m, while h, g, and g2
denote parent, grand parent, and great-grand parent, etc., and “ ” denotes words excluded in convolution.

2.3 Convolution on Siblings
Ancestor paths alone is not enough to capture
many linguistic phenomena such as conjunction.
Inspired by higher-order dependency parsing (Mc-
Donald and Pereira, 2006; Koo and Collins, 2010),
we also incorporate siblings for a given word in
various ways. See Figure 2 (right) for details.

2.4 Combined Model
Powerful as it is, structural information still does
not fully cover sequential information. Also, pars-
ing errors (which are common especially for in-
formal text such as online reviews) directly affect
DCNN performance while sequential n-grams are
always correctly observed. To best exploit both in-
formation, we want to combine both models. The
easiest way of combination is to concatenate these
representations together, then feed into fully con-
nected soft-max neural networks. In these cases,
combine with different feature from different type
of sources could stabilize the performance. The
ﬁnal sentence representation is thus:

ˆc = [ˆc(1)

a , ..., ˆc(Na)
ancestors

a

; ˆc(1)

s , ..., ˆc(Ns)
siblings

s

; ˆc(1), ..., ˆc(N )

]

sequential

(cid:124)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

where Na, Ns, and N are the number of ancestor,
sibling, and sequential ﬁlters. In practice, we use
100 ﬁlters for each template in Figure 2 . The fully
combined representation is 1,100-dimensional by
contrast to 300-dimensional for sequential CNN.

3 Experiments

Table 1 summarizes results in the context of other
high-performing efforts in the literature. We use
three benchmark datasets in two categories: senti-
ment analysis on both Movie Review (MR) (Pang
and Lee, 2005) and Stanford Sentiment Treebank
(SST-1) (Socher et al., 2013) datasets, and ques-
tion classiﬁcation on TREC (Li and Roth, 2002).

For all datasets, we ﬁrst obtain the dependency
parse tree from Stanford parser (Manning et al.,
2014).1 Different window size for different choice
of convolution are shown in Figure 2. For the
dataset without a development set (MR), we ran-
domly choose 10% of the training data to indicate
early stopping. In order to have a fare compari-
son with baseline CNN, we also use 3 to 5 as our
window size. Most of our results are generated by
GPU due to its efﬁciency, however CPU could po-
tentially get better results.2 Our implementation,
on top of Kim (2014)’s code,3 will be released.4

3.1 Sentiment Analysis

Both sentiment analysis datasets (MR and SST-
1) are based on movie reviews. The differences
between them are mainly in the different num-
bers of categories and whether the standard split
is given. There are 10,662 sentences in the MR
dataset. Each instance is labeled positive or neg-
ative, and in most cases contains one sentence.
Since no standard data split is given, following the
literature we use 10 fold cross validation to include
every sentence in training and testing at least once.
Concatenating with sibling and sequential infor-
mation obviously improves DCNNs, and the ﬁnal
model outperforms the baseline sequential CNNs
by 0.4, and ties with Zhu et al. (2015).

Different from MR,

the Stanford Sentiment
Treebank (SST-1) annotates ﬁner-grained labels,
very positive, positive, neutral, negative and very
negative, on an extension of the MR dataset. There
are 11,855 sentences with standard split. Our
model achieves an accuracy of 49.5 which is sec-
ond only to Irsoy and Cardie (2014).

1

The phrase-structure trees in SST-1 are actually automatically parsed,

and thus can not be used as gold-standard trees.

2
GPU only supports float32 while CPU supports float64.
3https://github.comw/yoonkim/CNN_sentence
4https://github.com/cosmmb/DCNN

Category

This work

CNNs

Recursive NNs

Recurrent NNs
Other deep learning
Hand-coded rules

Model
DCNNs: ancestor
DCNNs: ancestor+sibling
DCNNs: ancestor+sibling+sequential
CNNs-non-static (Kim, 2014) – baseline
CNNs-multichannel (Kim, 2014)
Deep CNNs (Kalchbrenner et al., 2014)
Recursive Autoencoder (Socher et al., 2011)
Recursive Neural Tensor (Socher et al., 2013)
Deep Recursive NNs (Irsoy and Cardie, 2014)
LSTM on tree (Zhu et al., 2015)
Paragraph-Vec (Le and Mikolov, 2014)
SVMS (Silva et al., 2011)

MR
80.4†
81.7†
81.9
81.5
81.1
-
77.7
-
-
81.9
-
-

SST-1 TREC TREC-2
47.7†
48.3†
49.5
48.0
47.4
48.5
43.2
45.7
49.8
48.0
48.7

88.4†
89.0†
88.8†
86.4∗
86.0∗
-
-
-
-
-
-
90.8

95.4†
95.6†
95.4†
93.6
92.2
93.0
-
-
-
-
-
95.0

Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.
TREC-2 is TREC with ﬁne grained labels. †Results generated by GPU (all others generated by CPU).
∗Results generated from Kim (2014)’s implementation.

3.2 Question Classiﬁcation
In the TREC dataset, the entire dataset of 5,952
sentences are classiﬁed into the following 6 cate-
gories: abbreviation, entity, description, location
and numeric. In this experiment, DCNNs easily
outperform any other methods even with ancestor
convolution only. DCNNs with sibling achieve the
best performance in the published literature. DC-
NNs combined with sibling and sequential infor-
mation might suffer from overﬁtting on the train-
ing data based on our observation. One thing
to note here is that our best result even exceeds
SVMS (Silva et al., 2011) with 60 hand-coded
rules.

The TREC dataset also provides subcategories
such as numeric:temperature, numeric:distance,
and entity:vehicle. To make our task more real-
istic and challenging, we also test the proposed
model with respect to the 50 subcategories. There
are obvious improvements over sequential CNNs
from the last column of Table 1. Like ours, Silva
et al. (2011) is a tree-based system but it uses
constituency trees compared to ours dependency
trees. They report a higher ﬁne-grained accuracy
of 90.8 but their parser is trained only on the Ques-
tionBank (Judge et al., 2006) while we used the
standard Stanford parser trained on both the Penn
Treebank and QuestionBank. Moreover, as men-
tioned above, their approach is rule-based while
ours is automatically learned.

3.3 Discussions and Examples
Compared with sentiment analysis, the advantage
of our proposed model is obviously more substan-
tial on the TREC dataset. Based on our error anal-
ysis, we conclude that this is mainly due to the

Figure 3: Examples from TREC (a–c), SST-1 (d)
and TREC with ﬁne-grained label (e–f) that are
misclassiﬁed by the baseline CNN but correctly
labeled by our DCNN. For example, (a) should be
entity but is labeled location by CNN.

Figure 4: Examples from TREC datasets that are
misclassiﬁed by DCNN but correctly labeled by
baseline CNN. For example, (a) should be numer-
ical but is labeled entity by DCNN.

difference of the parse tree quality between the
two tasks.
In sentiment analysis, the dataset is
collected from the Rotten Tomatoes website which
includes many irregular usage of language. Some
of the sentences even come from languages other
than English. The errors in parse trees inevitably
affect the classiﬁcation accuracy. However, the
parser works substantially better on the TREC
dataset since all questions are in formal written
English, and the training set for Stanford parser5
already includes the QuestionBank (Judge et al.,
2006) which includes 2,000 TREC sentences.

Figure 3 visualizes examples where CNN errs
while DCNN does not. For example, CNN la-
bels (a) as location due to “Hawaii” and “state”,
while the long-distance backbone “What – ﬂower”
is clearly asking for an entity. Similarly, in (d),
DCNN captures the obviously negative tree-based
trigram “Nothing – worth – emailing”. Note that
our model also works with non-projective depen-
dency trees such as the one in (b). The last two ex-
amples in Figure 3 visualize cases where DCNN
outperforms the baseline CNNs in ﬁne-grained
TREC. In example (e), the word “temperature” is
at second from the top and is root of a 8 word span
“the ... earth”. When we use a window of size 5
for tree convolution, every words in that span get
convolved with “temperature” and this should be
the reason why DCNN get correct.

Figure 4 showcases examples where baseline
CNNs get better results than DCNNs. Example
(a) is misclassiﬁed as entity by DCNN due to pars-
ing/tagging error (the Stanford parser performs its

5

http://nlp.stanford.edu/software/parser-faq.shtml

Figure 5: Examples from TREC datasets that are
misclassiﬁed by both DCNN and baseline CNN.
For example, (a) should be numerical but is la-
beled entity by DCNN and description by CNN.

own part-of-speech tagging). The word “ﬂy” at
the end of the sentence should be a verb instead of
noun, and “hummingbirds ﬂy” should be a relative
clause modifying “speed”.

There are some sentences that are misclassiﬁed
by both the baseline CNN and DCNN. Figure 5
shows three such examples. Example (a) is not
classiﬁed as numerical by both methods due to the
ambiguous meaning of the word “point” which is
difﬁcult to capture by word embedding. This word
can mean location, opinion, etc. Apparently, the
numerical aspect is not captured by word embed-
ding. Example (c) might be an annotation error.

Shortly before submitting to ACL 2015 we
learned Mou et al. (2015, unpublished) have inde-
pendently reported concurrent and related efforts.
Their constituency model, based on their unpub-
lished work in programming languages (Mou et
al., 2014),6 performs convolution on pretrained re-
cursive node representations rather than word em-
beddings, thus baring little, if any, resemblance to
our dependency-based model. Their dependency
model is related, but always includes a node and
all its children (resembling Iyyer et al. (2014)),
which is a variant of our sibling model and always
ﬂat. By contrast, our ancestor model looks at the
vertical path from any word to its ancestors, being
linguistically motivated (Shen et al., 2008).

4 Conclusions
We have presented a very simple dependency-
based convolution framework which outperforms
sequential CNN baselines on modeling sentences.

6

Both their 2014 and 2015 reports proposed (independently of each other
and independently of our work) the term “tree-based convolution” (TBCNN).

References

R. Collobert,

J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12.

Kushal Dave, Steve Lawrence, and David M Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classiﬁcation of product reviews.
In
Proceedings of World Wide Web.

Michael Gamon. 2004. Sentiment classiﬁcation on
customer feedback data: noisy data, large feature
vectors, and the role of linguistic analysis. In Pro-
ceedings of COLING.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
Improving neural networks by
dinov.
feature detectors.
preventing co-adaptation of
Journal of Machine Learning Research, 15.

2014.

Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Advances in Neural Information Processing Sys-
tems, pages 2096–2104.

Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,
Richard Socher, and Hal Daum´e III. 2014. A neural
network for factoid question answering over para-
graphs. In Proceedings of EMNLP.

John Judge, Aoife Cahill, and Josef van Genabith.
2006. Questionbank: Creating a corpus of parse-
annotated questions. In Proceedings of COLING.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of EMNLP.

Terry Koo and Michael Collins. 2010. Efﬁcient third-
order dependency parsers. In Proceedings of ACL.

Taku Kudo and Yuji Matsumoto. 2004. A boosting
algorithm for classiﬁcation of semi-structured text.
In Proceedings of EMNLP.

Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
ceedings of ICML.

Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes,
J. Denker, H. Drucker,
I. Guyon, U. Mller,
E. Sckinger, P. Simard, and V. Vapnik. 1995. Com-
parison of learning algorithms for handwritten digit
recognition. In Int’l Conf. on Artiﬁcial Neural Nets.

Xin Li and Dan Roth. 2002. Learning question classi-

ﬁers. In Proceedings of COLING.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
In Proceedings of ACL:
guage processing toolkit.
Demonstrations, pages 55–60.

Shotaro Matsumoto, Hiroya Takamura, and Manabu
2005. Sentiment classiﬁcation using
In

Okumura.
word sub-sequences and dependency sub-trees.
Proceedings of PA-KDD.

Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL.

Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang.
2014. TBCNN: A tree-based convolutional neu-
ral network for programming language processing.
Unpublished manuscript: http://arxiv.org/
abs/1409.5718.

Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and
Zhi Jin.
2015. Discriminative neural sentence
modeling by tree-based convolution. Unpublished
manuscript: http://arxiv.org/abs/1504.
01106v5. Version 5 dated June 2, 2015; Version 1
(“Tree-based Convolution: A New Architecture for
Sentence Modeling”) dated Apr 5, 2015.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of ACL,
pages 115–124.

Libin Shen, Lucas Champollion, and Aravind K Joshi.
2008. LTAG-spinal and the treebank. Language Re-
sources and Evaluation, 42(1):1–19.

Yelong Shen, Xiaodong he, Jianfeng Gao, Li Deng, and
Gregoire Mesnil. 2014. Learning semantic repre-
sentations using convolutional neural networks for
web search. In Proceedings of WWW.

J. Silva, L. Coheur, A. C. Mendes, and Andreas
Wichert. 2011. From symbolic to sub-symbolic in-
formation in question classiﬁcation. Artiﬁcial Intel-
ligence Review, 35.

Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
EMNLP 2011.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP 2013.

Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation question
answering. In Proceedings of ACL.

Mattgew Zeiler. 2012. Adadelta: An adaptive learning
rate method. Unpublished manuscript: http://
arxiv.org/abs/1212.5701.

Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long short-term memory over tree structures.
In Proceedings of ICML.

Dependency-based Convolutional Neural Networks
for Sentence Embedding∗

Mingbo Ma†
Liang Huang† ‡
†Graduate Center & Queens College
City University of New York

Bing Xiang‡

Bowen Zhou‡

‡IBM Watson Group
T. J. Watson Research Center, IBM

mma2,lhuang
}

{

gc.cuny.edu

@us.ibm.com
lhuang,bingxia,zhou
}

{

Abstract

In sentence modeling and classiﬁcation,
convolutional neural network approaches
have recently achieved state-of-the-art re-
sults, but all such efforts process word vec-
tors sequentially and neglect long-distance
dependencies. To combine deep learn-
ing with linguistic structures, we pro-
pose a dependency-based convolution ap-
proach, making use of tree-based n-grams
rather than surface ones, thus utlizing non-
local interactions between words. Our
model improves sequential baselines on all
four sentiment and question classiﬁcation
tasks, and achieves the highest published
accuracy on TREC.

Introduction

1
Convolutional neural networks (CNNs), originally
invented in computer vision (LeCun et al., 1995),
has recently attracted much attention in natural
language processing (NLP) on problems such as
sequence labeling (Collobert et al., 2011), seman-
tic parsing (Yih et al., 2014), and search query
retrieval (Shen et al., 2014). In particular, recent
work on CNN-based sentence modeling (Kalch-
brenner et al., 2014; Kim, 2014) has achieved ex-
cellent, often state-of-the-art, results on various
classiﬁcation tasks such as sentiment, subjectivity,
and question-type classiﬁcation. However, despite
their celebrated success, there remains a major
limitation from the linguistics perspective: CNNs,
being invented on pixel matrices in image process-
ing, only consider sequential n-grams that are con-
secutive on the surface string and neglect long-
distance dependencies, while the latter play an im-
portant role in many linguistic phenomena such as
negation, subordination, and wh-extraction, all of
which might dully affect the sentiment, subjectiv-
ity, or other categorization of the sentence.

∗ This work was done at both IBM and CUNY, and was supported in
part by DARPA FA8750-13-2-0041 (DEFT), and NSF IIS-1449278. We thank
Yoon Kim for sharing his code, and James Cross and Kai Zhao for discussions.

Indeed, in the sentiment analysis literature, re-
searchers have incorporated long-distance infor-
mation from syntactic parse trees, but the results
are somewhat inconsistent: some reported small
improvements (Gamon, 2004; Matsumoto et al.,
2005), while some otherwise (Dave et al., 2003;
Kudo and Matsumoto, 2004). As a result, syn-
tactic features have yet to become popular in the
sentiment analysis community. We suspect one
of the reasons for this is data sparsity (according
to our experiments, tree n-grams are signiﬁcantly
sparser than surface n-grams), but this problem
has largely been alleviated by the recent advances
in word embedding. Can we combine the advan-
tages of both worlds?

So we propose a very simple dependency-based
convolutional neural networks (DCNNs). Our
model is similar to Kim (2014), but while his se-
quential CNNs put a word in its sequential con-
text, ours considers a word and its parent, grand-
parent, great-grand-parent, and siblings on the de-
pendency tree. This way we incorporate long-
distance information that are otherwise unavail-
able on the surface string.

Experiments on three

classiﬁcation tasks
demonstrate the superior performance of our
DCNNs over the baseline sequential CNNs.
In
particular, our accuracy on the TREC dataset
outperforms all previously published results
including those with heavy
in the literature,
hand-engineered features.

Independently of this work, Mou et al. (2015,
unpublished) reported related efforts; see Sec. 3.3.

2 Dependency-based Convolution

The original CNN, ﬁrst proposed by LeCun et
al. (1995), applies convolution kernels on a se-
ries of continuous areas of given images, and was
adapted to NLP by Collobert et al. (2011). Fol-
lowing Kim (2014), one dimensional convolution
operates the convolution kernel in sequential order
Rd represents the d di-
in Equation 1, where xi ∈
mensional word representation for the i-th word in

5
1
0
2
 
g
u
A
 
3
 
 
]
L
C
.
s
c
[
 
 
2
v
9
3
8
1
0
.
7
0
5
1
:
v
i
X
r
a

Figure 1: Dependency tree of an example sentence from the Movie Reviews dataset.

the sentence, and
Therefore
from the i-th word to the (i + j)-th word:

is the concatenation operator.
xi,j refers to concatenated word vector

⊕

(1)

xi+j

(cid:101)
xi,j = xi ⊕

xi+1 ⊕ · · · ⊕
Sequential word concatenation

(cid:101)

xi,j works as
n-gram models which feeds local information into
convolution operations. However, this setting can
not capture long-distance relationships unless we
enlarge the window indeﬁnitely which would in-
evitably cause the data sparsity problem.

(cid:101)

In order to capture the long-distance dependen-
cies we propose the dependency-based convolu-
tion model (DCNN). Figure 1 illustrates an exam-
ple from the Movie Reviews (MR) dataset (Pang
and Lee, 2005). The sentiment of this sentence
is obviously positive, but this is quite difﬁcult for
sequential CNNs because many n-gram windows
would include the highly negative word “short-
comings”, and the distance between “Despite” and
“shortcomings” is quite long. DCNN, however,
could capture the tree-based bigram “Despite –
shortcomings”, thus ﬂipping the sentiment, and
the tree-based trigram “ROOT – moving – sto-
ries”, which is highly positive.

2.1 Convolution on Ancestor Paths
We deﬁne our concatenation based on the depen-
dency tree for a given modiﬁer xi:
xp(i) ⊕ · · · ⊕
where function pk(i) returns the i-th word’s k-th
ancestor index, which is recursively deﬁned as:

xi,k = xi ⊕

xpk

(2)

1(i)

−

pk(i) =

1(i))

−

p(pk
i
(cid:40)

if k > 0
if k = 0

(3)

Figure 2 (left) illustrates ancestor paths patterns
with various orders. We always start the convo-
lution with xi and concatenate with its ancestors.
If the root node is reached, we add “ROOT” as
dummy ancestors (vertical padding).

For a given tree-based concatenated word se-
quence xi,k, the convolution operation applies a
d to xi,k with a bias term b de-
ﬁlter w
×
∈
scribed in equation 4:

Rk

ci = f (w

xi,k + b)

·

(4)

where f is a non-linear activation function such as
rectiﬁed linear unit (ReLu) or sigmoid function.
The ﬁlter w is applied to each word in the sen-
tence, generating the feature map c

Rl:

∈

c = [c1, c2,

, cl]

· · ·

(5)

where l is the length of the sentence.

2.2 Max-Over-Tree Pooling and Dropout

The ﬁlters convolve with different word concate-
nation in Eq. 4 can be regarded as pattern detec-
tion: only the most similar pattern between the
words and the ﬁlter could return the maximum ac-
tivation. In sequential CNNs, max-over-time pool-
ing (Collobert et al., 2011; Kim, 2014) operates
over the feature map to get the maximum acti-
vation ˆc = max c representing the entire feature
map. Our DCNNs also pool the maximum activa-
tion from feature map to detect the strongest ac-
tivation over the whole tree (i.e., over the whole
sentence). Since the tree no longer deﬁnes a se-
quential “time” direction, we refer to our pooling
as “max-over-tree” pooling.

In order to capture enough variations, we ran-
domly initialize the set of ﬁlters to detect different
structure patterns. Each ﬁlter’s height is the num-
ber of words considered and the width is always
equal to the dimensionality d of word representa-
tion. Each ﬁlter will be represented by only one
feature after max-over-tree pooling. After a series
of convolution with different ﬁlter with different
heights, multiple features carry different structural
information become the ﬁnal representation of the
input sentence. Then, this sentence representation
is passed to a fully connected soft-max layer and
outputs a distribution over different labels.

Neural networks often suffer from overtrain-
ing. Following Kim (2014), we employ random
dropout on penultimate layer (Hinton et al., 2014).
in order to prevent co-adaptation of hidden units.
In our experiments, we set our drop out rate as 0.5
and learning rate as 0.95 by default. Following
Kim (2014), training is done through stochastic
gradient descent over shufﬂed mini-batches with
the Adadelta update rule (Zeiler, 2012).

Figure 2: Convolution patterns on trees. Word concatenation always starts with m, while h, g, and g2
denote parent, grand parent, and great-grand parent, etc., and “ ” denotes words excluded in convolution.

2.3 Convolution on Siblings
Ancestor paths alone is not enough to capture
many linguistic phenomena such as conjunction.
Inspired by higher-order dependency parsing (Mc-
Donald and Pereira, 2006; Koo and Collins, 2010),
we also incorporate siblings for a given word in
various ways. See Figure 2 (right) for details.

2.4 Combined Model
Powerful as it is, structural information still does
not fully cover sequential information. Also, pars-
ing errors (which are common especially for in-
formal text such as online reviews) directly affect
DCNN performance while sequential n-grams are
always correctly observed. To best exploit both in-
formation, we want to combine both models. The
easiest way of combination is to concatenate these
representations together, then feed into fully con-
nected soft-max neural networks. In these cases,
combine with different feature from different type
of sources could stabilize the performance. The
ﬁnal sentence representation is thus:

ˆc = [ˆc(1)

a , ..., ˆc(Na)
ancestors

a

; ˆc(1)

s , ..., ˆc(Ns)
siblings

s

; ˆc(1), ..., ˆc(N )

]

sequential

(cid:124)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

where Na, Ns, and N are the number of ancestor,
sibling, and sequential ﬁlters. In practice, we use
100 ﬁlters for each template in Figure 2 . The fully
combined representation is 1,100-dimensional by
contrast to 300-dimensional for sequential CNN.

3 Experiments

Table 1 summarizes results in the context of other
high-performing efforts in the literature. We use
three benchmark datasets in two categories: senti-
ment analysis on both Movie Review (MR) (Pang
and Lee, 2005) and Stanford Sentiment Treebank
(SST-1) (Socher et al., 2013) datasets, and ques-
tion classiﬁcation on TREC (Li and Roth, 2002).

For all datasets, we ﬁrst obtain the dependency
parse tree from Stanford parser (Manning et al.,
2014).1 Different window size for different choice
of convolution are shown in Figure 2. For the
dataset without a development set (MR), we ran-
domly choose 10% of the training data to indicate
early stopping. In order to have a fare compari-
son with baseline CNN, we also use 3 to 5 as our
window size. Most of our results are generated by
GPU due to its efﬁciency, however CPU could po-
tentially get better results.2 Our implementation,
on top of Kim (2014)’s code,3 will be released.4

3.1 Sentiment Analysis

Both sentiment analysis datasets (MR and SST-
1) are based on movie reviews. The differences
between them are mainly in the different num-
bers of categories and whether the standard split
is given. There are 10,662 sentences in the MR
dataset. Each instance is labeled positive or neg-
ative, and in most cases contains one sentence.
Since no standard data split is given, following the
literature we use 10 fold cross validation to include
every sentence in training and testing at least once.
Concatenating with sibling and sequential infor-
mation obviously improves DCNNs, and the ﬁnal
model outperforms the baseline sequential CNNs
by 0.4, and ties with Zhu et al. (2015).

Different from MR,

the Stanford Sentiment
Treebank (SST-1) annotates ﬁner-grained labels,
very positive, positive, neutral, negative and very
negative, on an extension of the MR dataset. There
are 11,855 sentences with standard split. Our
model achieves an accuracy of 49.5 which is sec-
ond only to Irsoy and Cardie (2014).

1

The phrase-structure trees in SST-1 are actually automatically parsed,

and thus can not be used as gold-standard trees.

2
GPU only supports float32 while CPU supports float64.
3https://github.comw/yoonkim/CNN_sentence
4https://github.com/cosmmb/DCNN

Category

This work

CNNs

Recursive NNs

Recurrent NNs
Other deep learning
Hand-coded rules

Model
DCNNs: ancestor
DCNNs: ancestor+sibling
DCNNs: ancestor+sibling+sequential
CNNs-non-static (Kim, 2014) – baseline
CNNs-multichannel (Kim, 2014)
Deep CNNs (Kalchbrenner et al., 2014)
Recursive Autoencoder (Socher et al., 2011)
Recursive Neural Tensor (Socher et al., 2013)
Deep Recursive NNs (Irsoy and Cardie, 2014)
LSTM on tree (Zhu et al., 2015)
Paragraph-Vec (Le and Mikolov, 2014)
SVMS (Silva et al., 2011)

MR
80.4†
81.7†
81.9
81.5
81.1
-
77.7
-
-
81.9
-
-

SST-1 TREC TREC-2
47.7†
48.3†
49.5
48.0
47.4
48.5
43.2
45.7
49.8
48.0
48.7

88.4†
89.0†
88.8†
86.4∗
86.0∗
-
-
-
-
-
-
90.8

95.4†
95.6†
95.4†
93.6
92.2
93.0
-
-
-
-
-
95.0

Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.
TREC-2 is TREC with ﬁne grained labels. †Results generated by GPU (all others generated by CPU).
∗Results generated from Kim (2014)’s implementation.

3.2 Question Classiﬁcation
In the TREC dataset, the entire dataset of 5,952
sentences are classiﬁed into the following 6 cate-
gories: abbreviation, entity, description, location
and numeric. In this experiment, DCNNs easily
outperform any other methods even with ancestor
convolution only. DCNNs with sibling achieve the
best performance in the published literature. DC-
NNs combined with sibling and sequential infor-
mation might suffer from overﬁtting on the train-
ing data based on our observation. One thing
to note here is that our best result even exceeds
SVMS (Silva et al., 2011) with 60 hand-coded
rules.

The TREC dataset also provides subcategories
such as numeric:temperature, numeric:distance,
and entity:vehicle. To make our task more real-
istic and challenging, we also test the proposed
model with respect to the 50 subcategories. There
are obvious improvements over sequential CNNs
from the last column of Table 1. Like ours, Silva
et al. (2011) is a tree-based system but it uses
constituency trees compared to ours dependency
trees. They report a higher ﬁne-grained accuracy
of 90.8 but their parser is trained only on the Ques-
tionBank (Judge et al., 2006) while we used the
standard Stanford parser trained on both the Penn
Treebank and QuestionBank. Moreover, as men-
tioned above, their approach is rule-based while
ours is automatically learned.

3.3 Discussions and Examples
Compared with sentiment analysis, the advantage
of our proposed model is obviously more substan-
tial on the TREC dataset. Based on our error anal-
ysis, we conclude that this is mainly due to the

Figure 3: Examples from TREC (a–c), SST-1 (d)
and TREC with ﬁne-grained label (e–f) that are
misclassiﬁed by the baseline CNN but correctly
labeled by our DCNN. For example, (a) should be
entity but is labeled location by CNN.

Figure 4: Examples from TREC datasets that are
misclassiﬁed by DCNN but correctly labeled by
baseline CNN. For example, (a) should be numer-
ical but is labeled entity by DCNN.

difference of the parse tree quality between the
two tasks.
In sentiment analysis, the dataset is
collected from the Rotten Tomatoes website which
includes many irregular usage of language. Some
of the sentences even come from languages other
than English. The errors in parse trees inevitably
affect the classiﬁcation accuracy. However, the
parser works substantially better on the TREC
dataset since all questions are in formal written
English, and the training set for Stanford parser5
already includes the QuestionBank (Judge et al.,
2006) which includes 2,000 TREC sentences.

Figure 3 visualizes examples where CNN errs
while DCNN does not. For example, CNN la-
bels (a) as location due to “Hawaii” and “state”,
while the long-distance backbone “What – ﬂower”
is clearly asking for an entity. Similarly, in (d),
DCNN captures the obviously negative tree-based
trigram “Nothing – worth – emailing”. Note that
our model also works with non-projective depen-
dency trees such as the one in (b). The last two ex-
amples in Figure 3 visualize cases where DCNN
outperforms the baseline CNNs in ﬁne-grained
TREC. In example (e), the word “temperature” is
at second from the top and is root of a 8 word span
“the ... earth”. When we use a window of size 5
for tree convolution, every words in that span get
convolved with “temperature” and this should be
the reason why DCNN get correct.

Figure 4 showcases examples where baseline
CNNs get better results than DCNNs. Example
(a) is misclassiﬁed as entity by DCNN due to pars-
ing/tagging error (the Stanford parser performs its

5

http://nlp.stanford.edu/software/parser-faq.shtml

Figure 5: Examples from TREC datasets that are
misclassiﬁed by both DCNN and baseline CNN.
For example, (a) should be numerical but is la-
beled entity by DCNN and description by CNN.

own part-of-speech tagging). The word “ﬂy” at
the end of the sentence should be a verb instead of
noun, and “hummingbirds ﬂy” should be a relative
clause modifying “speed”.

There are some sentences that are misclassiﬁed
by both the baseline CNN and DCNN. Figure 5
shows three such examples. Example (a) is not
classiﬁed as numerical by both methods due to the
ambiguous meaning of the word “point” which is
difﬁcult to capture by word embedding. This word
can mean location, opinion, etc. Apparently, the
numerical aspect is not captured by word embed-
ding. Example (c) might be an annotation error.

Shortly before submitting to ACL 2015 we
learned Mou et al. (2015, unpublished) have inde-
pendently reported concurrent and related efforts.
Their constituency model, based on their unpub-
lished work in programming languages (Mou et
al., 2014),6 performs convolution on pretrained re-
cursive node representations rather than word em-
beddings, thus baring little, if any, resemblance to
our dependency-based model. Their dependency
model is related, but always includes a node and
all its children (resembling Iyyer et al. (2014)),
which is a variant of our sibling model and always
ﬂat. By contrast, our ancestor model looks at the
vertical path from any word to its ancestors, being
linguistically motivated (Shen et al., 2008).

4 Conclusions
We have presented a very simple dependency-
based convolution framework which outperforms
sequential CNN baselines on modeling sentences.

6

Both their 2014 and 2015 reports proposed (independently of each other
and independently of our work) the term “tree-based convolution” (TBCNN).

References

R. Collobert,

J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12.

Kushal Dave, Steve Lawrence, and David M Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classiﬁcation of product reviews.
In
Proceedings of World Wide Web.

Michael Gamon. 2004. Sentiment classiﬁcation on
customer feedback data: noisy data, large feature
vectors, and the role of linguistic analysis. In Pro-
ceedings of COLING.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
Improving neural networks by
dinov.
feature detectors.
preventing co-adaptation of
Journal of Machine Learning Research, 15.

2014.

Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Advances in Neural Information Processing Sys-
tems, pages 2096–2104.

Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,
Richard Socher, and Hal Daum´e III. 2014. A neural
network for factoid question answering over para-
graphs. In Proceedings of EMNLP.

John Judge, Aoife Cahill, and Josef van Genabith.
2006. Questionbank: Creating a corpus of parse-
annotated questions. In Proceedings of COLING.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of EMNLP.

Terry Koo and Michael Collins. 2010. Efﬁcient third-
order dependency parsers. In Proceedings of ACL.

Taku Kudo and Yuji Matsumoto. 2004. A boosting
algorithm for classiﬁcation of semi-structured text.
In Proceedings of EMNLP.

Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
ceedings of ICML.

Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes,
J. Denker, H. Drucker,
I. Guyon, U. Mller,
E. Sckinger, P. Simard, and V. Vapnik. 1995. Com-
parison of learning algorithms for handwritten digit
recognition. In Int’l Conf. on Artiﬁcial Neural Nets.

Xin Li and Dan Roth. 2002. Learning question classi-

ﬁers. In Proceedings of COLING.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
In Proceedings of ACL:
guage processing toolkit.
Demonstrations, pages 55–60.

Shotaro Matsumoto, Hiroya Takamura, and Manabu
2005. Sentiment classiﬁcation using
In

Okumura.
word sub-sequences and dependency sub-trees.
Proceedings of PA-KDD.

Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL.

Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang.
2014. TBCNN: A tree-based convolutional neu-
ral network for programming language processing.
Unpublished manuscript: http://arxiv.org/
abs/1409.5718.

Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and
Zhi Jin.
2015. Discriminative neural sentence
modeling by tree-based convolution. Unpublished
manuscript: http://arxiv.org/abs/1504.
01106v5. Version 5 dated June 2, 2015; Version 1
(“Tree-based Convolution: A New Architecture for
Sentence Modeling”) dated Apr 5, 2015.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of ACL,
pages 115–124.

Libin Shen, Lucas Champollion, and Aravind K Joshi.
2008. LTAG-spinal and the treebank. Language Re-
sources and Evaluation, 42(1):1–19.

Yelong Shen, Xiaodong he, Jianfeng Gao, Li Deng, and
Gregoire Mesnil. 2014. Learning semantic repre-
sentations using convolutional neural networks for
web search. In Proceedings of WWW.

J. Silva, L. Coheur, A. C. Mendes, and Andreas
Wichert. 2011. From symbolic to sub-symbolic in-
formation in question classiﬁcation. Artiﬁcial Intel-
ligence Review, 35.

Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
EMNLP 2011.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP 2013.

Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation question
answering. In Proceedings of ACL.

Mattgew Zeiler. 2012. Adadelta: An adaptive learning
rate method. Unpublished manuscript: http://
arxiv.org/abs/1212.5701.

Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long short-term memory over tree structures.
In Proceedings of ICML.

