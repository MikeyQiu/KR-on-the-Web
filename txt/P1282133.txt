Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Heinrich Jiang 1 Jennifer Jang 2 Samory Kpotufe 3

8
1
0
2
 
y
a
M
 
1
2
 
 
]

G
L
.
s
c
[
 
 
1
v
9
0
9
7
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

We provide initial seedings to the Quick Shift clus-
tering algorithm, which approximate the locally
high-density regions of the data. Such seedings
act as more stable and expressive cluster-cores
than the singleton modes found by Quick Shift.
We establish statistical consistency guarantees for
this modiﬁcation. We then show strong clustering
performance on real datasets as well as promising
applications to image segmentation.

1. Introduction

Quick Shift (Vedaldi & Soatto, 2008) is a mode-seeking
based clustering algorithm that has a growing popularity in
computer vision. It proceeds by repeatedly moving each
sample to its closest sample point that has higher empirical
density if one exists within a τ -radius ball, otherwise we
stop. Thus each path ends at a point which can be viewed
as a local mode of the empirical density. Then, points that
end up at the same mode are assigned to the same cluster.
The most popular choice of empirical density function is
the Kernel Density Estimator (KDE) with Gaussian Kernel.
The algorithm also appears in Rodriguez & Laio (2014).

Quick Shift was designed as a faster alternative to the well-
known Mean Shift algorithm (Cheng, 1995; Comaniciu &
Meer, 2002). Mean Shift is equivalent to performing a
gradient ascent of the KDE starting at each sample until
convergence (Arias-Castro et al., 2016). Samples that cor-
respond to the same points of convergence are in the same
cluster and the points of convergence are taken to be the
estimates of the modes. Thus, both procedures hill-climb
to the local modes of the empirical density function and
cluster based on these modes. The key differences are that
Quick Shift restricts the steps to sample points (and thus is
a sample-based version of Mean Shift) and has the extra τ
parameter which allows it to merge close segments together.

1Google Research, Mountain View, CA 2Uber Inc, San Fran-
cisco, CA 3Princeton University, Princeton, NJ. Correspondence
to: Heinrich Jiang <heinrich.jiang@gmail.com>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

One of the drawbacks of these two procedures, as well as
many mode-seeking based clustering algorithms, is that the
point-modes of the density functions are often poor rep-
resentations of the clusters. This will happen when the
high-density regions within a cluster are of arbitrary shape
and have some variations causing the underlying density
function to have possibly many apparent, but not so salient
modes. In this case, such procedures asymptotically recover
all of the modes separately, leading to over-segmentation.
To combat this effect, practitioners often increase the kernel
bandwidth, which makes the density estimate more smooth.
However, this can cause the density estimate to deviate too
far from the original density we are intending to cluster
based on.1 Thus, practitioners may not wish to identify
the clusters based on the point-modes of the density func-
tion, but rather identify them based on locally high density
regions of the dataset (See Figure 1).2

We propose modeling these locally high-density regions as
cluster-cores (to be precisely deﬁned later), which can be of
arbitrary shape, size, and density level, and are thus better
suited at capturing the possibly complex topological prop-
erties of clusters that can arise in practice. In other words,
these cluster-cores are better at expressing the clusters and
are more stable as they are less sensitive to the small ﬂuctu-
ations that can arise in the empirical density function. We
parameterize the cluster-core by β where 0 < β < 1, which
determines how much the density is allowed to vary within
the cluster-core. We estimate them from a ﬁnite sample us-
ing a minor modiﬁcation of the MCores algorithm of Jiang
& Kpotufe (2017).

We introduce Quickshift++, which ﬁrst estimates these
cluster-cores, and then runs the Quick Shift based hill-
climbing procedure on each remaining sample until it
reaches a cluster-core.
Samples that end up in the
same cluster-core are assigned to the same cluster; thus,

1KDE with Gaussian kernel and bandwidth h approximates the
underlying density convolved with a Gaussian with mean 0 and
covariance h2I. Thus, the higher h is, the more the KDE deviates
from the original density.

2Over-segmentation is also dealt with in Quick Shift via the
τ parameter, but a threshold for the distance between two modes
which should be clustered together is hard to determine in practice.
Moreover, there may not even be a good setting of τ which works
everywhere in the input space.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

the cluster-cores can be seen as representing the high-
conﬁdence regions within each cluster. We utilize the k-NN
density estimator as our empirical density.

Despite the simplicity of our approach, we show that Quick-
shift++ considerably outperforms the popular density-based
clustering algorithms, while being efﬁcient. Another desir-
able property of Quickshift++ is that it is simple to tune its
two hyperparameters β and k.3 We show that a few settings
of β turn out to work for a wide range of applications and
that the procedure is stable in choices of k.

We then give a novel statistical consistency analysis for
Quickshift++ which provides guarantees that points within
a cluster-core’s attraction regions (to be described later)
are correctly assigned. We also show promising results on
image segmentation, which further validates the desirability
of using cluster-cores on real-data applications.

Figure 1. It can often be the case that the locally high-density
regions are of arbitrary shape and ﬂuctuations within them lead to
many apparent modes. Left: Mode-seeking clustering procedures
often lead to over-segmentation. Right: It may be more desirable
to use cluster-cores (shaded), which allows ﬂuctuations within
arbitrarily-shaped regions of locally high density.

2. Related Works and Contributions

We show that Quickshift++ is a new and powerful addition
to the family of clustering procedures known as density-
based clustering, which most notably includes DBSCAN
(Ester et al., 1996) and Mean Shift (Cheng, 1995). Such
procedures operate on the estimated density function based
on a ﬁnite sample to recover structures in the density func-
tion that ultimately correspond to the clusters. There are
several advantages of density-based clustering over classical
objective-based procedures such as k-means and spectral
clustering. Density-based procedures can automatically
detect the number of clusters, while objective-based pro-
cedures typically require this as an input. Density-based
clustering algorithms also make little assumptions on the
shapes of the clusters as well as their relative positions.

Density-based clustering procedures can roughly be classi-
ﬁed into two categories: hill-climbing based approaches
(discussed earlier, which includes both Mean Shift and

3The τ parameter from Quick Shift is unnecessary here because
we climb until we reach a cluster-core as our stopping condition.

Quick Shift) and density-level set based approaches. We
now discuss the latter approach, which takes the connected
components of the density-level set deﬁned by {x : f (x) ≥
λ} for some density level λ as the clusters. This statistical
notion of clustering traces back to Hartigan (1975). Since
then, there has been extensive work done, e.g. Tsybakov
et al. (1997); Cadre (2006); Rigollet et al. (2009); Singh et al.
(2009); Chaudhuri & Dasgupta (2010); Rinaldo & Wasser-
man (2010); Kpotufe & von Luxburg (2011); Balakrishnan
et al. (2013); Chaudhuri et al. (2014); Chen et al. (2017).
More recently, Sriperumbudur & Steinwart (2012); Jiang
(2017a) show that the popular DBSCAN algorithm turns
out to converge to these clusters. However, one of the main
drawbacks of this approach is that the density-level λ is
ﬁxed and thus such methods perform poorly when the clus-
ters are at different density-levels. Moreover, the question
of how to choose λ remains (e.g. Steinwart (2011)).

Jiang & Kpotufe (2017) provide an alternative notion of
clusters, called modal-sets, which are regions of ﬂat den-
sity which are local maximas of the density. They can be
of arbitrary shape, dimension, or density. They provide a
procedure, MCores, which estimates these with consistency
guarantees. Our notion of cluster-core is similar to modal-
sets, but the density within a cluster-core is allowed to vary
by a substantial amount in order to capture such variations
seen in real data as a the ﬂat density of modal-sets may be
too restrictive in practice. It turns out that a small modiﬁ-
cation of MCores allows us to estimate these cluster-cores.
Thus Quickshift++ has the advantage over DBSCAN in that
clusters can be at any density level and that furthermore, the
density levels are chosen adaptively.

Mcores however consists of an over-simplistic ﬁnal cluster-
ing: it simply assigns each point to its closest modal-set,
while in practice, clusters tend not to follow the geome-
try induced by the Euclidean metric. Quickshift++ on the
other hand clusters the remaining points by a hill-climbing
method which we show is far better in practice.

Thus, Quickshift++ combines the strengths of both density-
based clustering approaches while avoiding many of their
weaknesses. In addition to the general advantages of density-
based clustering algorithms shared by both approaches, it
is also able to both (1) recover clusters at varying density
levels and (2) not suffer from the over-segmentation issue
described in Figure 1. To our knowledge, no other procedure
has been shown to have this property.

For our theoretical analysis, we give guarantees about Quick-
shift++’s ability to recover the clusters based on attraction
regions deﬁned by the gradient ﬂows. Wasserman et al.
(2014); Arias-Castro et al. (2016) showed that Mean Shift’s
iterates approximate the gradient ﬂows. Some progress
has been made in understanding Quick Shift (Jiang, 2017b;
Verdinelli & Wasserman, 2018). There are also related lines

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

of work in mode clustering e.g. (Li et al., 2007; Chac´on,
2012; Genovese et al., 2016; Chen et al., 2016). In this
paper, we show that Quickshift++ recovers the interior of its
attraction region, thus adding to our statistical understanding
of hill-climbing based clustering procedures.

a ﬁxed additive ﬂuctuation. It uses the levels of the mutual
k-NN graph of the sample points, deﬁned below.
Deﬁnition 3. Let G(λ) denote the λ-level of the mutual
k-NN graph with vertices {x ∈ X[n] : fk(x) ≥ λ} and an
edge between x and x(cid:48) iff ||x − x(cid:48)|| ≤ min{rk(x), rk(x(cid:48))}.

3. Algorithm

3.1. Basic Deﬁnitions

Let X[n] = {x1, ..., xn} be n i.i.d. samples drawn from an
unknown density f , deﬁned over the Lebesgue measure on
Rd. Suppose that f has compact support X .

Our procedure will operate on the k-NN density estimator:

Deﬁnition 1. Let rk(x) := inf{r > 0 : |B(x, r) ∩ X[n]| ≥
k}, i.e., the distance from x to its k-th nearest neighbor.
Deﬁne the k-NN density estimator as

fk(x) :=

k
n · vd · rk(x)d ,

where vd is the volume of a unit ball in Rd.

3.2. Cluster-Cores

We deﬁne the cluster core with respect to ﬁxed ﬂuctuation
parameter β as follows.

Deﬁnition 2. Let 0 < β < 1. Closed and connected set
M ⊂ X is a cluster-core if M is a connected component
(CC) of {x ∈ X : f (x) ≥ (1 − β) · maxx(cid:48)∈M f (x(cid:48))}.

Note that when β → 0, then the cluster-cores become the
modes or local-maximas of f . When β → 1, then the
cluster-core becomes the entire support X . We next give a
very basic fact about cluster-cores, that they do not overlap.

Lemma 1. Suppose that M1, M2 are distinct cluster-cores
of f . Then M1 ∩ M2 = ∅.

Proof. Suppose otherwise. We have that M1 and M2 are
CCs of {x ∈ X : f (x) ≥ λ1} and {x ∈ X : f (x) ≥ λ2},
respectively for some λ1, λ2. Clearly, if λ1 = λ2, then it
follows that M1 = M2. Then, without loss of generality, let
λ1 < λ2. Then since the CCs of {x ∈ X : f (x) ≥ λ2} are
nested in the CCs of {x ∈ X : f (x) ≥ λ1}, then it follows
that M2 ⊆ M1. Then, λ2 = (1 − β) supx∈M2 f (x) ≤ (1 −
β) supx∈M1 f (x) = λ1, a contradiction. As desired.

Algorithm 1 is a simple modiﬁcation of MCores by Jiang &
Kpotufe (2017). The difference is that we use a multiplica-
tive ﬂuctuation parameter β, while Jiang & Kpotufe (2017)
uses an additive one. The latter requires knowledge of the
scale of the density function, which is difﬁcult to determine
in practice. Moreover, the multiplicative ﬂuctuation adapts
to clusters at varying density levels more reasonably than

It is known that G(λ) approximates the CCs of the λ-level
sets of the true density, deﬁned as {x ∈ X : f (x) ≥ λ}
see e.g. (Chaudhuri & Dasgupta, 2010). Moreover, it can
be seen that the CCs of G(λ) forms a hierarchical nesting
structure as λ decreases.

Algorithm 1 proceeds by performing a top-down sweep of
the levels of the mutual k-NN graph, G(λ). As λ decreases,
it is clear that more nodes appear and that connectivity
increases. In other words, as we scan top-down, the CCs of
G(λ) become larger, some CCs can merge, or new CCs can
appear. When a new CC appears at level λ, then intuitively,
it should correspond to a local maxima of f , which appears
at a density level approximately λ. This follows from the
fact that the CCs of G(λ) approximates the CCs of {x ∈
X : f (x) ≥ λ}. Thus, the idea is that when a new CC
appears in G(λ), then we can take the corresponding CC in
G(λ − βλ) (which is the density level (1 − β) times that of
the highest point in the CC) to estimate the cluster-core.

Algorithm 1 MCores (estimating cluster-cores)

Parameters k, β
Initialize (cid:99)M := ∅.
Sort the xi’s in decreasing order of fk values (i.e.
fk(xi) ≥ fk(xi+1)).
for i = 1 to n do

Deﬁne λ := fk(xi).
Let A be the CC of G(λ − βλ) containing xi.
if A is disjoint from all cluster-cores in (cid:99)M then

Add A to (cid:99)M.

end if
end for
return (cid:99)M.

Algorithm 2 Quickshift++

Let (cid:99)M be the cluster-cores obtained by running Algo-
rithm 1.
Initialize directed graph G with vertices {x1, ..., xn} and
no edges.
for i = 1 to n do

If xi is not in any cluster-core, then add to G an edge
from xi to its closest sample x ∈ X[n] such that
fk(x) > fk(xi).

end for
For each cluster-core M ∈ (cid:99)M, let (cid:98)CM be the points
x ∈ X[n] such that the directed path in G starting at x
ends in M .
return { (cid:98)CM : M ∈ (cid:99)M}.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

3.3. Quickshift++

Quickshift++ (Algorithm 2) proceeds by ﬁrst running Algo-
rithm 1 to obtain the cluster-cores, and then moving each
sample point to its nearest neighbor that has higher k-NN
density until it reaches some cluster-core. All samples that
end up in the same cluster-core after the hill-climbing are
assigned to the same cluster. Note that since the highest
empirical density sample point is contained in a cluster-core,
it follows that each sample point not in a cluster-core will
eventually be assigned to a unique cluster-core. Thus, Quick-
shift++ provides a clustering assignment of every sample
point.
Remark 1. Although it seems a similar procedure could
have been constructed by using Mean Shift in place of Quick
Shift, Mean Shift could have convergence outside of the
estimated cluster-cores, while Quick Shift guarantees that
each sample outside of a cluster-core get assigned to some
cluster-core.

3.4. Implementation

The implementation details for the MCores modiﬁcation
can be inferred from Jiang & Kpotufe (2017). This step
runs in O(nk · α(n)) where α is the Inverse Ackermann
function (Cormen, 2009), in addition to the time it takes to
compute the k-NN sets for the n sample points. To cluster
the remaining points, for each sample not in a cluster-core,
we must ﬁnd its nearest sample of higher k-NN density. Al-
though this is worst-case O(n) time for each sample point,
fortunately we see that in practice (as long as k is not too
small) for the vast majority of cases, the nearest sample
with higher density is within the k-nearest neighbor set
so it only takes O(k) in most cases. It is an open prob-
lem whether there the nearest sample with higher empir-
ical density is often in its k-NN set. Code release is at
https://github.com/google/quickshift.

4. Theoretical Analysis

For the theoretical analysis, we make ﬁrst the following
regularity assumption, that the density is continuously dif-
ferentiable and lower bounded on X .
Assumption 1. f is continuously differentiable on X and
there exists λ0 > 0 such that inf x∈X f (x) ≥ λ0.

Let M1, ..., MC be the cluster-cores of f . Then we can
deﬁne the following notion of attraction region for each
cluster-core based on the gradient ascent curve or ﬂow. This
is similar to notions of attraction regions for some previous
analyses of mode-based clustering, such as Wasserman et al.
(2014); Arias-Castro et al. (2016), where the intuition is
that attraction regions are deﬁned based by following the
direction of the gradient of the underlying density. In our
situation, instead of an attraction region deﬁned as all points

which ﬂow towards a particular point-mode, the attraction
region is deﬁned around a cluster-core.
Deﬁnition 4 (Attraction Regions). Let path πx : R → Rd
satisfy πx(0) = x, π(cid:48)
x(t) = ∇f (πx(t)). For cluster-core
Mi, its attraction region Ai is the set of points x ∈ X that
satisfy limt→∞ πx(t) ∈ Mi.

It is clear that these attraction regions are well-deﬁned. The
ﬂow path is well-deﬁned since the density is differentiable
and since each cluster-core is deﬁned as a CC of a level
set, the density must decay around its boundaries. In other
words, once an ascent path reaches a cluster-core, it cannot
leave the cluster-core.

However, it is in general not the case that the space can be
partitioned into attraction regions. For example, if a ﬂow
reaches a saddle point, it will get stuck there and thus any
point whose ﬂow ends up at a saddle point will not belong to
any attraction region. In this paper, we only give guarantees
about the clustering of points which are in an attraction
region.

The next regularity assumption we make is that the cluster-
cores are on the interior of the attraction region (to avoid
situations such as when the cluster-cores intersect with the
boundary of the input space).
Assumption 2. There exists R0 > 0 such that Mi +
B(0, R0) ⊆ Ai for i = 1, ..., C, where M + B(0, r) de-
notes {x : inf y∈M ||x − y|| ≤ r}.
Deﬁnition 5 (Level Set). The λ level set of f is deﬁned as
Lf (λ) := {x ∈ X : f (x) ≥ λ}.

The next assumption says that the level sets are continuous
w.r.t. the level in the following sense where we denote the
(cid:15)-interior of A as A(cid:9)(cid:15) := {x ∈ A, inf y∈∂A ||x − y|| ≥ (cid:15)}
(∂A is the boundary of A):
Assumption 3 (Uniform Continuity of Level Sets). For
each (cid:15) > 0, there exists δ > 0 such that for 0 < λ ≤ λ(cid:48) ≤
||f ||∞ with |λ − λ(cid:48)| < δ, then Lf (λ)(cid:9)(cid:15) ⊆ Lf (λ(cid:48)).

This ensures that there are no approximately ﬂat areas in
which the procedure may get stuck at. The assumption is
borrowed from (Jiang, 2017b). Finally, we need the fol-
lowing regularity condition which ensures that level sets
away from cluster-cores do not get arbitrarily thin. This is
adapted from standard analyses of level-set estimation (e.g.
Assumption B of Singh et al. (2009)).
Assumption 4. Let µ denote the Lebesgue measure on Rd.
For any r > 0, there exists σ > 0 such that the following
holds for any connected component A of any level-set of f
which is not contained in Mi for any i: µ(B(x, r) ∩ A) ≥ σ
for all x ∈ A.

For our consistency results, we prove that Quickshift++ can
cluster the sample points in the (R, ρ)-interior of an attrac-

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

tion region (deﬁned below) for each cluster-core properly
where R, ρ > 0 are ﬁxed and can be chosen arbitrarily
small.
Deﬁnition 6 ((R, ρ)-interior of Attraction Regions). Deﬁne
the (R, ρ)-interior of Ai, denoted as A(R,ρ)
, as the set of
points x0 ∈ Ai such that each path P from x0 to any point
y ∈ ∂Ai satisﬁes the following.

i

sup
x∈P

inf
x(cid:48)∈B(x,R)

f (x(cid:48)) ≥ sup

f (x(cid:48)) + ρ.

x(cid:48)∈B(y,R)

In other words, points in the interior satisfy the property
that any path leaving its attraction region must sufﬁciently
decrease in density at some point. This decrease threshold
is parameterized by R and ρ.

Figure 2. Illustration of interior of attraction region in 1-dimension.
The pink and green shaded regions indicated the two attraction
regions. The striped parts show the corresponding interiors of the
attraction regions.

We ﬁrst give a guarantee on the ﬁrst step of MCores recov-
ers, that the cluster-cores are reasonably recovered. The
proof follows from the analysis of Jiang & Kpotufe (2017)
by replacing modal-sets with cluster-cores, and the results
match up to constant factors. The proof is omitted here.

Theorem 1. [Adapted from Theorem 3, 4 of Jiang &
Kpotufe (2017)] Suppose that Assumptions 1, 3, and 4 hold.
Let 0 < β < 1, (cid:15), δ > 0 and suppose that k ≡ k(n) is
chosen such that log2 n/k → 0 and n4/(4+d)/k → 0. Let
M1, ..., MC be the cluster-cores of f . Then for n sufﬁciently
large depending on f , δ, (cid:15), and β, with probability at least
1 − δ, MCores returns C cluster-core estimates (cid:99)M1, ..., (cid:100)MC
such that Mi ∩ X[n] ⊆ (cid:99)Mi ⊆ Mi + B(0, (cid:15)) for i ∈ 1, ..., C.
Remark 2. The original result from Jiang & Kpotufe (2017)
is about (cid:15)-approximate modal-set which are deﬁned as level-
sets whose density has range (cid:15). Our notion of cluster-core
is similar, but the range is a β-proportion of the highest
density level within the level-set. Using a proportion is
more interpretable and thus more useful, as the scale of the
density function is difﬁcult to determine in practice.

We now state the main result, which says that as long as the
cluster-cores are sufﬁciently well estimated (up to a certain
Hausdorff error) by MCores (via previous theorem), then
Quickshift++ will correctly cluster the (R, ρ)-interiors of
the attraction regions with high probability.

Theorem 2. Suppose that Assumptions 1, 2, 3, and 4 hold.
Let 0 < R < R0 and ρ, δ > 0. Suppose that k ≡ k(n)
is chosen such that log2 n/k → 0 and n4/(4+d)/k → 0.
Suppose that (cid:99)M1, ..., (cid:99)MC are the cluster-cores returned by
Algorithm 1 and satisfy Mi ∩X[n] ⊆ (cid:99)Mi ⊆ Mi +B(0, R/4)
for i = 1, ..., C. Then for n sufﬁciently large depending
on f , ρ, δ and R, the following holds with probably at
least 1 − 2δ uniformly in x ∈ A(R,ρ)
∩ X[n] and i ∈ [C]:
Quickshift++ clusters x to the cluster corresponding to Mi.

i

4.1. Proof of Theorem 2

We require the following uniform bound on k-NN density
estimator, which follows from Dasgupta & Kpotufe (2014).

Lemma 2. Let δ > 0. Suppose that f is Lipschitz continu-
ous with compact support X (e.g. there exists L such that
|f (x)−f (x(cid:48))| ≤ L|x−x(cid:48)| for all x, x(cid:48) ∈ X ) and f satsiﬁes
Assumption 1. Then exists constant C depending on f such
that the following holds if n ≥ C 2
δ,n with probability at least
1 − δ.

|fk(x) − f (x)| ≤ C

sup
x∈X

(cid:32)

Cδ,n√
k

+

(cid:18) k
n

(cid:19)1/d(cid:33)

.

where Cδ,n := 16 log(2/δ)

d log n.

√

We next need the following uniform concentration bound
on balls intersected with level-sets, which says that if such
a set has large enough probability mass, then it will contain
a sample point with high probability.
Lemma 3. Let E := {B(x, r) ∩ Lf (λ) : x ∈ Rd, r >
0, λ > 0}. Then the following holds with probability at
least 1 − δ uniformly for all E ∈ E

F(E) ≥ Cδ,n

⇒ E ∩ Xn (cid:54)= ∅.

√

d log n
n

Proof. The indicator functions 1[B(x, f ) ∩ Lf (λ)] for x ∈
Rd, λ > 0 have VC-dimension d + 1. This is because the
balls over Rd have VC-dimension d + 1 and the level-sets
Lf (λ) has VC-dimension 1 and thus their intersection has
VC-dimension d + 1 (Van Der Vaart & Wellner, 2009). The
result follows by applying Theorem 15 of Chaudhuri &
Dasgupta (2010).

In other words, with high probability, MCores estimates
each cluster-core bijectively and that for each cluster-core,
MCores’ estimate contains all of the sample points and that
the estimate does not over-estimate by much.

Proof of Theorem 2. Suppose that x0 ∈ A(R,ρ)
∩ X[n] and
Quickshift++ gives directed path x0 → x1 → · · · → xL
where x1, ..., xL−1 are outside of cluster-cores and xL is in
a cluster-core but xL (cid:54)∈ Ai.

i

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

We ﬁrst show that ||xi − xi+1|| ≤ R/2 for i = 0, ..., L − 1.
By Assumption 3 and 4, we have that there exists τ > 0
and σ > 0 such that the following holds uniformly for
i = 0, ..., L − 1:

(cid:18)

(cid:18)

(cid:19)

(cid:19)

µ

B(xi, R/2) ∩ Lf (f (xi) + τ )

≥ σ.

Hence, since the density is uniformly lower bounded by λ0,
we have

F

B(xi, R/2) ∩ Lf (f (xi) + τ )

≥ σλ0.

√

Then by Lemma 3, for n sufﬁicently large such that σλ0 >
d log n
, then with probability at least 1 − δ there exists
Cδ,n
n
sample point x(cid:48)
i in B(xi, R/2) ∩ Lf (f (xi) + τ ) for i =
0, ..., L − 1.

Next, choose n sufﬁciently large such that by Lemma 2, we
have with probability at least 1 − δ that

|fk(x) − f (x)| ≤ min{τ, ρ}/3.

sup
x∈X

Thus, we have

fk(x(cid:48)

i) ≥ f (x(cid:48)

i) − τ /3 ≥ f (xi) + 2τ /3

≥ fk(xi) + τ /3 > fk(xi).

i|| ≤ R/2 and x(cid:48)

i ∈ X[n], it follows that

Moreover ||xi − x(cid:48)
||xi − xi+1|| ≤ R/2 for i = 0, ..., L − 1.
Let π : [0, 1] → Rd be the piecewise linear path deﬁned by
π(j/L) = xj for j = 0, ..., L. Let t2 = min{t ∈ [0, 1] :
π(t) ∈ ∂Ai}. Then, by deﬁnition of A(R,ρ)
, there exists
0 ≤ t1 < t2 such that x := π(t1) and y := π(t2) satisﬁes
y ∈ ∂Ai and

i

inf
x(cid:48)∈B(x,R)

f (x(cid:48)) ≥ sup

f (x(cid:48)) + ρ.

x(cid:48)∈B(y,R)

Thus, there exists indices p, q ∈ {0, ..., L − 1} such that
p ≤ q, |xp − x| ≤ R, and |xq − y| ≤ R. Thus, we have
f (xp) ≥ f (xq) + ρ, but fk(xp) ≤ fk(xq). However, we
have

fk(xp) ≥ f (xp) − ρ/3 ≥ f (xq) + 2ρ/3
≥ fk(xq) + ρ/3 > fk(xq),

a contradiction, as desired.

5. Simulations

Figure 3 provides simple veriﬁcation that Quickshift++
provides reasonable clusterings in a wide variety of situ-
ations where other density-based procedures are known to
fail. For instance, in the two rings dataset (ﬁrst row), we

Figure 3. Comparison against other clustering algorithms on toy
datasets, adapted from scikit-learn cluster demo. Quickshift++
settings were ﬁxed at k = 20, β = 0.7 for all the datasets, while
the other algorithms were tuned to obtain a reasonable number of
clusters.

see that Mean Shift and Quick Shift suffer from the over-
segmentation issue coupled with the oversized bandwidth
which causes them to recover clusters that have points from
both the rings even though the rings are separated. In the
three Gaussians dataset (third row), we see that DBSCAN
fails because the three clusters are of different density levels
and thus no matter which density-level we set, DBSCAN
will not be able to recover the three clusters.

6. Image Segmentation

In order to apply clustering to image segmentation, we use
the following standard approach (see e.g. Felzenszwalb
& Huttenlocher (2004)): we transform each pixel into a
5-dimensional vector where two coordinates correspond to
the location of the pixel and three correspond to each of
the RGB color channels. Then segmentation is done by
clustering this 5-dimensional dataset.

We observed that for Quickshift++, setting β = 0.9 is rea-
sonable across a wide range of images, β was ﬁxed to this
value for segmentation here. We compare Quickshift++ to
Quick Shift, as the latter is often used for segmentation.
Quick Shift often over-segments in some areas and under-
segments in other areas under any hyperparameter setting
and we showed the settings which provided a reasonable
trade-off. On the other hand Quickshift++ gives us reason-

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

able segmentations in many cases and can capture segments
that may be problematic for other procedures.

As shown in the ﬁgures, it moreover has the interesting prop-
erty of being able to recover segments of widely varying
shapes and sizes in the same image, which suggests that
modelling the dense regions of the segments as cluster-cores
instead of point-modes may be useful as we compare to
Quick Shift. Although this is only qualitative, it further sug-
gests that Quickshift++ is a versatile algorithm and begins
to show its potential application in many more areas.

Figure 6. Assorted fruit in a metal bowl. For Quick Shift, band-
width was set to 8 and for Quickshift++, k = 100 and β = 0.9.
Quickshift++ is able to segment most of the fruits in the bowl,
while recovering the details of the bowl as well as the structures in
the background.

7. Clustering Experiments

We ran Quickshift++ against other clustering algorithms
on the various real datasets and scored against the ground-
truth using the adjusted rand index and the adjusted mutual
information scores.

Dataset
(A) seeds
(B) phonemes
(C) iris
(D) banknote
(E) images
(F) letters
(G) MNIST
(H) page blocks
(I) glass

n
210
4509
150
1372
210
20000
1000
5473
214

d
7
258
4
4
19
16
784
10
19

Clusters
4
5
3
2
7
26
10
5
7

Figure 8. Summary of datasets used, including dataset size (n),
number of features (d) and number of clusters.

Datasets Used: Summary of the datasets can be found in
Figure 8. Seeds, glass, and iris are standard UCI datasets
(Lichman, 2013) used for clustering. Banknote is another
UCI dataset which involves identifying whether a banknote
is forged or not, based on various statistics of an image of
the banknote. Page Blocks is a UCI dataset which involves
determining the type of a portion of a page (e.g. text, image,
etc) based on various statistics of an image of the portion.
Phonemes (Friedman et al., 2001) is a dataset which involves
the log periodograms of spoken phonemes. Images is a UCI
dataset called Statlog, based on features extracted from
various images, and letters is the UCI letter recognition
dataset. We also used a small subset of MNIST (LeCun
et al., 2010) for our experiments.

Figure 4. Figure skater Yuzuru Hanyu performs at the 2018 Winter
Olympics. Quick Shift was set with bandwidth 10 and Quick-
shift++ was set with k = 300 and β = 0.9. We see that when
compared to Quick Shift, Quickshift++ is able to recover the varia-
tions in the background more accurately, including correctly seg-
menting most of the letters on the wall, while still recovering the
structure of Hanyu’s costume accurately.

Figure 5. Yuzuru Hanyu at the 2017 Rostelecom Cup. Quick Shift
was set with bandwidth 15 and Quickshift++ was set with k =
50 and β = 0.9. Quickshift++ can recover the homogeneous
background as a whole, and reasonably separates Hanyu’s light-
colored costume from the background.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Figure 7. For each algorithm, we show clustering performance as a function of its respective hyperparameter setting. The blue line is adj.
mutual information and the red line is adj. rand index. Notice that for Quickshift++, we show a wide range of k (relative to n), while for
the popular procedures, their respective parameters had to be carefully tuned to ﬁnd the region where the scores are non-trivial.

B

C

KMns DScn MCrs MSft QSft
.7327
.6715
.6872
.6360
.7165
.7361
.7149
.7479
.7261
.6203
.5836
.7265
.3318
.5145
.3397
.3857
.5008
.4077
.5814
.5364
.1793
.1284
.4940
.4217
.2503
.2584
.2911
.3483
.3251
.0925
.1363
.0397
.2929
.2647
.4195
.3523

.7319
.6769
.5974
.5700
.7028
.6106
.2434
.2351
.3497
.4656
.1287
.3027
.2281
.1958
.0028
.0526
.2790
.3858

.4473
.4429
.4458
.5731
.5898
.5865
.5584
.4594
.3313
.5264
.0705
.4422
.1070
.2164
.1962
.1179
.2844
.3542

A .7092
.6738
.7432
.7574
.7294
.7418
D .2893
.2690
.4177
.5497
.1384
.3741
G .3320
.4629
H .0830
.0524
.2770
.3865

E

F

I

QS++
.7261
.7085
.7530
.7870
.7399
.7424
.6152
.4866
.5359
.6456
.1766
.5001
.3606
.4806
.4727
.2192
.2849
.4250

Figure 9. For each dataset, the ﬁrst row is the adjusted rand in-
dex scores and the second row is the adjusted mutual informa-
tion scores. Bolded are highest and second highest scores. For
MCores and Quickshift++, we used a single β = 0.3 for each
dataset with the exception of for banknote where β = 0.7. Then
the procedures were tuned in their respective essential hyperpa-
rameter: k-means (KMns) number of clusters, DBSCAN (DScn)
epsilon, MCores (MCrs) k from k-NN, mean shift (MSft) band-
width, quick shift (QSft) bandwidth, Quickshift++ (QS++) k.

We evaluate performance under the Adjusted Mutual Infor-
mation and Rand Index scores (Vinh et al., 2010) which are
metrics to compare clusterings. Not only do we show that
Quickshift++ considerably outperforms the popular density-
based clustering procedures under optimal tuning (Figure 9),
but that it is also robust in its hyperparameter k (Figure 7),
all while ﬁxing β = 0.3 for all but one of the datasets. Such
robustness to its tuning parameters is highly desirable since
optimal tuning is usually not available in practice.

8. Conclusion

We presented Quickshift++, a new density-based clustering
procedure that ﬁrst estimates the cluster-cores of the density,
which are locally high-density regions. Then remaining
points are assigned to its appropriate cluster-core using a
hill-climbing procedure based on Quick Shift. Such cluster-
cores turn out to be more stable and expressive representa-
tions of the possibly complex clusters than point-modes. As
a result, Quickshift++ enjoys the advantages of the popular
density-based clustering algorithms while avoiding many
of their respective weaknesses. We then gave guarantees
for cluster recovery. Finally, we showed that the algorithm
has strong and robust performance on real datasets and has
promising applications to image segmentation.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Acknowledgements

We thank the anonymous reviewers for their helpful feed-
back.

References

Arias-Castro, Ery, Mason, David, and Pelletier, Bruno. On
the estimation of the gradient lines of a density and the
consistency of the mean-shift algorithm. Journal of Ma-
chine Learning Research, 17(43):1–28, 2016.

Balakrishnan, Sivaraman, Narayanan, Srivatsan, Rinaldo,
Alessandro, Singh, Aarti, and Wasserman, Larry. Cluster
trees on manifolds. In Advances in Neural Information
Processing Systems, pp. 2679–2687, 2013.

Cadre, Benoıt. Kernel estimation of density level sets. Jour-
nal of multivariate analysis, 97(4):999–1023, 2006.

Chac´on, Jos´e E. Clusters and water ﬂows: a novel approach
to modal clustering through morse theory. arXiv preprint
arXiv:1212.1384, 2012.

Chaudhuri, Kamalika and Dasgupta, Sanjoy. Rates of con-
In Advances in Neural

vergence for the cluster tree.
Information Processing Systems, pp. 343–351, 2010.

Chaudhuri, Kamalika, Dasgupta, Sanjoy, Kpotufe, Samory,
and von Luxburg, Ulrike. Consistent procedures for clus-
ter tree estimation and pruning. IEEE Transactions on
Information Theory, 60(12):7900–7912, 2014.

Chen, Yen-Chi, Genovese, Christopher R, Wasserman,
Larry, et al. A comprehensive approach to mode clus-
tering. Electronic Journal of Statistics, 10(1):210–241,
2016.

Chen, Yen-Chi, Genovese, Christopher R, and Wasserman,
Larry. Density level sets: Asymptotics, inference, and
visualization. Journal of the American Statistical Associ-
ation, pp. 1–13, 2017.

Cheng, Yizong. Mean shift, mode seeking, and cluster-
ing. IEEE transactions on pattern analysis and machine
intelligence, 17(8):790–799, 1995.

Comaniciu, Dorin and Meer, Peter. Mean shift: A robust
approach toward feature space analysis. IEEE Transac-
tions on pattern analysis and machine intelligence, 24(5):
603–619, 2002.

Cormen, Thomas H. Introduction to algorithms. MIT press,

2009.

Ester, Martin, Kriegel, Hans-Peter, Sander, J¨org, Xu, Xi-
aowei, et al. A density-based algorithm for discovering
clusters in large spatial databases with noise. In Kdd,
volume 96, pp. 226–231, 1996.

Felzenszwalb, Pedro F and Huttenlocher, Daniel P. Efﬁcient
graph-based image segmentation. International journal
of computer vision, 59(2):167–181, 2004.

Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
The elements of statistical learning, volume 1. Springer
series in statistics New York, 2001.

Genovese, Christopher R, Perone-Paciﬁco, Marco,
Verdinelli, Isabella, and Wasserman, Larry. Non-
Journal of
parametric inference for density modes.
the Royal Statistical Society: Series B (Statistical
Methodology), 78(1):99–126, 2016.

Hartigan, John A. Clustering algorithms, volume 209. Wiley

New York, 1975.

Jiang, Heinrich. Density level set estimation on manifolds
with dbscan. arXiv preprint arXiv:1703.03503, 2017a.

Jiang, Heinrich. On the consistency of quick shift. In Neural

Information Processing Systems (NIPS), 2017b.

Jiang, Heinrich and Kpotufe, Samory. Modal-set estimation
with an application to clustering. In Artiﬁcial Intelligence
and Statistics, pp. 1197–1206, 2017.

Kpotufe, Samory and von Luxburg, Ulrike. Pruning nearest
neighbor cluster trees. arXiv preprint arXiv:1105.0540,
2011.

LeCun, Yann, Cortes, Corinna, and Burges, CJ. Mnist hand-
written digit database. AT&T Labs [Online]. Available:
http://yann. lecun. com/exdb/mnist, 2, 2010.

Li, Jia, Ray, Surajit, and Lindsay, Bruce G. A nonparametric
statistical approach to clustering via mode identiﬁcation.
Journal of Machine Learning Research, 8(Aug):1687–
1723, 2007.

Lichman, M. UCI machine learning repository, 2013. URL

http://archive.ics.uci.edu/ml.

Rigollet, Philippe, Vert, R´egis, et al. Optimal rates for
plug-in estimators of density level sets. Bernoulli, 15(4):
1154–1178, 2009.

Rinaldo, Alessandro and Wasserman, Larry. Generalized
density clustering. The Annals of Statistics, pp. 2678–
2722, 2010.

Dasgupta, Sanjoy and Kpotufe, Samory. Optimal rates for
k-nn density and mode estimation. In Advances in Neural
Information Processing Systems, pp. 2555–2563, 2014.

Rodriguez, Alex and Laio, Alessandro. Clustering by fast
search and ﬁnd of density peaks. Science, 344(6191):
1492–1496, 2014.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Singh, Aarti, Scott, Clayton, Nowak, Robert, et al. Adaptive
hausdorff estimation of density level sets. The Annals of
Statistics, 37(5B):2760–2782, 2009.

Sriperumbudur, Bharath and Steinwart, Ingo. Consistency
and rates for clustering with dbscan. In Artiﬁcial Intelli-
gence and Statistics, pp. 1090–1098, 2012.

Steinwart, Ingo. Adaptive density level set clustering. In
Proceedings of the 24th Annual Conference on Learning
Theory, pp. 703–738, 2011.

Tsybakov, Alexandre B et al. On nonparametric estimation
of density level sets. The Annals of Statistics, 25(3):
948–969, 1997.

Van Der Vaart, Aad and Wellner, Jon A. A note on bounds
for vc dimensions. Institute of Mathematical Statistics
collections, 5:103, 2009.

Vedaldi, Andrea and Soatto, Stefano. Quick shift and kernel
methods for mode seeking. Computer vision–ECCV 2008,
pp. 705–718, 2008.

Verdinelli,

Isabella and Wasserman, Larry.

sis of a mode clustering diagram.
arXiv:1805.04187, 2018.

Analy-
arXiv preprint

Vinh, Nguyen Xuan, Epps, Julien, and Bailey, James. In-
formation theoretic measures for clusterings compari-
son: Variants, properties, normalization and correction
for chance. Journal of Machine Learning Research, 11
(Oct):2837–2854, 2010.

Wasserman, Larry, Azizyan, Martin, and Singh, Aarti. Fea-
ture selection for high-dimensional clustering. arXiv
preprint arXiv:1406.2240, 2014.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Heinrich Jiang 1 Jennifer Jang 2 Samory Kpotufe 3

8
1
0
2
 
y
a
M
 
1
2
 
 
]

G
L
.
s
c
[
 
 
1
v
9
0
9
7
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

We provide initial seedings to the Quick Shift clus-
tering algorithm, which approximate the locally
high-density regions of the data. Such seedings
act as more stable and expressive cluster-cores
than the singleton modes found by Quick Shift.
We establish statistical consistency guarantees for
this modiﬁcation. We then show strong clustering
performance on real datasets as well as promising
applications to image segmentation.

1. Introduction

Quick Shift (Vedaldi & Soatto, 2008) is a mode-seeking
based clustering algorithm that has a growing popularity in
computer vision. It proceeds by repeatedly moving each
sample to its closest sample point that has higher empirical
density if one exists within a τ -radius ball, otherwise we
stop. Thus each path ends at a point which can be viewed
as a local mode of the empirical density. Then, points that
end up at the same mode are assigned to the same cluster.
The most popular choice of empirical density function is
the Kernel Density Estimator (KDE) with Gaussian Kernel.
The algorithm also appears in Rodriguez & Laio (2014).

Quick Shift was designed as a faster alternative to the well-
known Mean Shift algorithm (Cheng, 1995; Comaniciu &
Meer, 2002). Mean Shift is equivalent to performing a
gradient ascent of the KDE starting at each sample until
convergence (Arias-Castro et al., 2016). Samples that cor-
respond to the same points of convergence are in the same
cluster and the points of convergence are taken to be the
estimates of the modes. Thus, both procedures hill-climb
to the local modes of the empirical density function and
cluster based on these modes. The key differences are that
Quick Shift restricts the steps to sample points (and thus is
a sample-based version of Mean Shift) and has the extra τ
parameter which allows it to merge close segments together.

1Google Research, Mountain View, CA 2Uber Inc, San Fran-
cisco, CA 3Princeton University, Princeton, NJ. Correspondence
to: Heinrich Jiang <heinrich.jiang@gmail.com>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

One of the drawbacks of these two procedures, as well as
many mode-seeking based clustering algorithms, is that the
point-modes of the density functions are often poor rep-
resentations of the clusters. This will happen when the
high-density regions within a cluster are of arbitrary shape
and have some variations causing the underlying density
function to have possibly many apparent, but not so salient
modes. In this case, such procedures asymptotically recover
all of the modes separately, leading to over-segmentation.
To combat this effect, practitioners often increase the kernel
bandwidth, which makes the density estimate more smooth.
However, this can cause the density estimate to deviate too
far from the original density we are intending to cluster
based on.1 Thus, practitioners may not wish to identify
the clusters based on the point-modes of the density func-
tion, but rather identify them based on locally high density
regions of the dataset (See Figure 1).2

We propose modeling these locally high-density regions as
cluster-cores (to be precisely deﬁned later), which can be of
arbitrary shape, size, and density level, and are thus better
suited at capturing the possibly complex topological prop-
erties of clusters that can arise in practice. In other words,
these cluster-cores are better at expressing the clusters and
are more stable as they are less sensitive to the small ﬂuctu-
ations that can arise in the empirical density function. We
parameterize the cluster-core by β where 0 < β < 1, which
determines how much the density is allowed to vary within
the cluster-core. We estimate them from a ﬁnite sample us-
ing a minor modiﬁcation of the MCores algorithm of Jiang
& Kpotufe (2017).

We introduce Quickshift++, which ﬁrst estimates these
cluster-cores, and then runs the Quick Shift based hill-
climbing procedure on each remaining sample until it
reaches a cluster-core.
Samples that end up in the
same cluster-core are assigned to the same cluster; thus,

1KDE with Gaussian kernel and bandwidth h approximates the
underlying density convolved with a Gaussian with mean 0 and
covariance h2I. Thus, the higher h is, the more the KDE deviates
from the original density.

2Over-segmentation is also dealt with in Quick Shift via the
τ parameter, but a threshold for the distance between two modes
which should be clustered together is hard to determine in practice.
Moreover, there may not even be a good setting of τ which works
everywhere in the input space.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

the cluster-cores can be seen as representing the high-
conﬁdence regions within each cluster. We utilize the k-NN
density estimator as our empirical density.

Despite the simplicity of our approach, we show that Quick-
shift++ considerably outperforms the popular density-based
clustering algorithms, while being efﬁcient. Another desir-
able property of Quickshift++ is that it is simple to tune its
two hyperparameters β and k.3 We show that a few settings
of β turn out to work for a wide range of applications and
that the procedure is stable in choices of k.

We then give a novel statistical consistency analysis for
Quickshift++ which provides guarantees that points within
a cluster-core’s attraction regions (to be described later)
are correctly assigned. We also show promising results on
image segmentation, which further validates the desirability
of using cluster-cores on real-data applications.

Figure 1. It can often be the case that the locally high-density
regions are of arbitrary shape and ﬂuctuations within them lead to
many apparent modes. Left: Mode-seeking clustering procedures
often lead to over-segmentation. Right: It may be more desirable
to use cluster-cores (shaded), which allows ﬂuctuations within
arbitrarily-shaped regions of locally high density.

2. Related Works and Contributions

We show that Quickshift++ is a new and powerful addition
to the family of clustering procedures known as density-
based clustering, which most notably includes DBSCAN
(Ester et al., 1996) and Mean Shift (Cheng, 1995). Such
procedures operate on the estimated density function based
on a ﬁnite sample to recover structures in the density func-
tion that ultimately correspond to the clusters. There are
several advantages of density-based clustering over classical
objective-based procedures such as k-means and spectral
clustering. Density-based procedures can automatically
detect the number of clusters, while objective-based pro-
cedures typically require this as an input. Density-based
clustering algorithms also make little assumptions on the
shapes of the clusters as well as their relative positions.

Density-based clustering procedures can roughly be classi-
ﬁed into two categories: hill-climbing based approaches
(discussed earlier, which includes both Mean Shift and

3The τ parameter from Quick Shift is unnecessary here because
we climb until we reach a cluster-core as our stopping condition.

Quick Shift) and density-level set based approaches. We
now discuss the latter approach, which takes the connected
components of the density-level set deﬁned by {x : f (x) ≥
λ} for some density level λ as the clusters. This statistical
notion of clustering traces back to Hartigan (1975). Since
then, there has been extensive work done, e.g. Tsybakov
et al. (1997); Cadre (2006); Rigollet et al. (2009); Singh et al.
(2009); Chaudhuri & Dasgupta (2010); Rinaldo & Wasser-
man (2010); Kpotufe & von Luxburg (2011); Balakrishnan
et al. (2013); Chaudhuri et al. (2014); Chen et al. (2017).
More recently, Sriperumbudur & Steinwart (2012); Jiang
(2017a) show that the popular DBSCAN algorithm turns
out to converge to these clusters. However, one of the main
drawbacks of this approach is that the density-level λ is
ﬁxed and thus such methods perform poorly when the clus-
ters are at different density-levels. Moreover, the question
of how to choose λ remains (e.g. Steinwart (2011)).

Jiang & Kpotufe (2017) provide an alternative notion of
clusters, called modal-sets, which are regions of ﬂat den-
sity which are local maximas of the density. They can be
of arbitrary shape, dimension, or density. They provide a
procedure, MCores, which estimates these with consistency
guarantees. Our notion of cluster-core is similar to modal-
sets, but the density within a cluster-core is allowed to vary
by a substantial amount in order to capture such variations
seen in real data as a the ﬂat density of modal-sets may be
too restrictive in practice. It turns out that a small modiﬁ-
cation of MCores allows us to estimate these cluster-cores.
Thus Quickshift++ has the advantage over DBSCAN in that
clusters can be at any density level and that furthermore, the
density levels are chosen adaptively.

Mcores however consists of an over-simplistic ﬁnal cluster-
ing: it simply assigns each point to its closest modal-set,
while in practice, clusters tend not to follow the geome-
try induced by the Euclidean metric. Quickshift++ on the
other hand clusters the remaining points by a hill-climbing
method which we show is far better in practice.

Thus, Quickshift++ combines the strengths of both density-
based clustering approaches while avoiding many of their
weaknesses. In addition to the general advantages of density-
based clustering algorithms shared by both approaches, it
is also able to both (1) recover clusters at varying density
levels and (2) not suffer from the over-segmentation issue
described in Figure 1. To our knowledge, no other procedure
has been shown to have this property.

For our theoretical analysis, we give guarantees about Quick-
shift++’s ability to recover the clusters based on attraction
regions deﬁned by the gradient ﬂows. Wasserman et al.
(2014); Arias-Castro et al. (2016) showed that Mean Shift’s
iterates approximate the gradient ﬂows. Some progress
has been made in understanding Quick Shift (Jiang, 2017b;
Verdinelli & Wasserman, 2018). There are also related lines

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

of work in mode clustering e.g. (Li et al., 2007; Chac´on,
2012; Genovese et al., 2016; Chen et al., 2016). In this
paper, we show that Quickshift++ recovers the interior of its
attraction region, thus adding to our statistical understanding
of hill-climbing based clustering procedures.

a ﬁxed additive ﬂuctuation. It uses the levels of the mutual
k-NN graph of the sample points, deﬁned below.
Deﬁnition 3. Let G(λ) denote the λ-level of the mutual
k-NN graph with vertices {x ∈ X[n] : fk(x) ≥ λ} and an
edge between x and x(cid:48) iff ||x − x(cid:48)|| ≤ min{rk(x), rk(x(cid:48))}.

3. Algorithm

3.1. Basic Deﬁnitions

Let X[n] = {x1, ..., xn} be n i.i.d. samples drawn from an
unknown density f , deﬁned over the Lebesgue measure on
Rd. Suppose that f has compact support X .

Our procedure will operate on the k-NN density estimator:

Deﬁnition 1. Let rk(x) := inf{r > 0 : |B(x, r) ∩ X[n]| ≥
k}, i.e., the distance from x to its k-th nearest neighbor.
Deﬁne the k-NN density estimator as

fk(x) :=

k
n · vd · rk(x)d ,

where vd is the volume of a unit ball in Rd.

3.2. Cluster-Cores

We deﬁne the cluster core with respect to ﬁxed ﬂuctuation
parameter β as follows.

Deﬁnition 2. Let 0 < β < 1. Closed and connected set
M ⊂ X is a cluster-core if M is a connected component
(CC) of {x ∈ X : f (x) ≥ (1 − β) · maxx(cid:48)∈M f (x(cid:48))}.

Note that when β → 0, then the cluster-cores become the
modes or local-maximas of f . When β → 1, then the
cluster-core becomes the entire support X . We next give a
very basic fact about cluster-cores, that they do not overlap.

Lemma 1. Suppose that M1, M2 are distinct cluster-cores
of f . Then M1 ∩ M2 = ∅.

Proof. Suppose otherwise. We have that M1 and M2 are
CCs of {x ∈ X : f (x) ≥ λ1} and {x ∈ X : f (x) ≥ λ2},
respectively for some λ1, λ2. Clearly, if λ1 = λ2, then it
follows that M1 = M2. Then, without loss of generality, let
λ1 < λ2. Then since the CCs of {x ∈ X : f (x) ≥ λ2} are
nested in the CCs of {x ∈ X : f (x) ≥ λ1}, then it follows
that M2 ⊆ M1. Then, λ2 = (1 − β) supx∈M2 f (x) ≤ (1 −
β) supx∈M1 f (x) = λ1, a contradiction. As desired.

Algorithm 1 is a simple modiﬁcation of MCores by Jiang &
Kpotufe (2017). The difference is that we use a multiplica-
tive ﬂuctuation parameter β, while Jiang & Kpotufe (2017)
uses an additive one. The latter requires knowledge of the
scale of the density function, which is difﬁcult to determine
in practice. Moreover, the multiplicative ﬂuctuation adapts
to clusters at varying density levels more reasonably than

It is known that G(λ) approximates the CCs of the λ-level
sets of the true density, deﬁned as {x ∈ X : f (x) ≥ λ}
see e.g. (Chaudhuri & Dasgupta, 2010). Moreover, it can
be seen that the CCs of G(λ) forms a hierarchical nesting
structure as λ decreases.

Algorithm 1 proceeds by performing a top-down sweep of
the levels of the mutual k-NN graph, G(λ). As λ decreases,
it is clear that more nodes appear and that connectivity
increases. In other words, as we scan top-down, the CCs of
G(λ) become larger, some CCs can merge, or new CCs can
appear. When a new CC appears at level λ, then intuitively,
it should correspond to a local maxima of f , which appears
at a density level approximately λ. This follows from the
fact that the CCs of G(λ) approximates the CCs of {x ∈
X : f (x) ≥ λ}. Thus, the idea is that when a new CC
appears in G(λ), then we can take the corresponding CC in
G(λ − βλ) (which is the density level (1 − β) times that of
the highest point in the CC) to estimate the cluster-core.

Algorithm 1 MCores (estimating cluster-cores)

Parameters k, β
Initialize (cid:99)M := ∅.
Sort the xi’s in decreasing order of fk values (i.e.
fk(xi) ≥ fk(xi+1)).
for i = 1 to n do

Deﬁne λ := fk(xi).
Let A be the CC of G(λ − βλ) containing xi.
if A is disjoint from all cluster-cores in (cid:99)M then

Add A to (cid:99)M.

end if
end for
return (cid:99)M.

Algorithm 2 Quickshift++

Let (cid:99)M be the cluster-cores obtained by running Algo-
rithm 1.
Initialize directed graph G with vertices {x1, ..., xn} and
no edges.
for i = 1 to n do

If xi is not in any cluster-core, then add to G an edge
from xi to its closest sample x ∈ X[n] such that
fk(x) > fk(xi).

end for
For each cluster-core M ∈ (cid:99)M, let (cid:98)CM be the points
x ∈ X[n] such that the directed path in G starting at x
ends in M .
return { (cid:98)CM : M ∈ (cid:99)M}.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

3.3. Quickshift++

Quickshift++ (Algorithm 2) proceeds by ﬁrst running Algo-
rithm 1 to obtain the cluster-cores, and then moving each
sample point to its nearest neighbor that has higher k-NN
density until it reaches some cluster-core. All samples that
end up in the same cluster-core after the hill-climbing are
assigned to the same cluster. Note that since the highest
empirical density sample point is contained in a cluster-core,
it follows that each sample point not in a cluster-core will
eventually be assigned to a unique cluster-core. Thus, Quick-
shift++ provides a clustering assignment of every sample
point.
Remark 1. Although it seems a similar procedure could
have been constructed by using Mean Shift in place of Quick
Shift, Mean Shift could have convergence outside of the
estimated cluster-cores, while Quick Shift guarantees that
each sample outside of a cluster-core get assigned to some
cluster-core.

3.4. Implementation

The implementation details for the MCores modiﬁcation
can be inferred from Jiang & Kpotufe (2017). This step
runs in O(nk · α(n)) where α is the Inverse Ackermann
function (Cormen, 2009), in addition to the time it takes to
compute the k-NN sets for the n sample points. To cluster
the remaining points, for each sample not in a cluster-core,
we must ﬁnd its nearest sample of higher k-NN density. Al-
though this is worst-case O(n) time for each sample point,
fortunately we see that in practice (as long as k is not too
small) for the vast majority of cases, the nearest sample
with higher density is within the k-nearest neighbor set
so it only takes O(k) in most cases. It is an open prob-
lem whether there the nearest sample with higher empir-
ical density is often in its k-NN set. Code release is at
https://github.com/google/quickshift.

4. Theoretical Analysis

For the theoretical analysis, we make ﬁrst the following
regularity assumption, that the density is continuously dif-
ferentiable and lower bounded on X .
Assumption 1. f is continuously differentiable on X and
there exists λ0 > 0 such that inf x∈X f (x) ≥ λ0.

Let M1, ..., MC be the cluster-cores of f . Then we can
deﬁne the following notion of attraction region for each
cluster-core based on the gradient ascent curve or ﬂow. This
is similar to notions of attraction regions for some previous
analyses of mode-based clustering, such as Wasserman et al.
(2014); Arias-Castro et al. (2016), where the intuition is
that attraction regions are deﬁned based by following the
direction of the gradient of the underlying density. In our
situation, instead of an attraction region deﬁned as all points

which ﬂow towards a particular point-mode, the attraction
region is deﬁned around a cluster-core.
Deﬁnition 4 (Attraction Regions). Let path πx : R → Rd
satisfy πx(0) = x, π(cid:48)
x(t) = ∇f (πx(t)). For cluster-core
Mi, its attraction region Ai is the set of points x ∈ X that
satisfy limt→∞ πx(t) ∈ Mi.

It is clear that these attraction regions are well-deﬁned. The
ﬂow path is well-deﬁned since the density is differentiable
and since each cluster-core is deﬁned as a CC of a level
set, the density must decay around its boundaries. In other
words, once an ascent path reaches a cluster-core, it cannot
leave the cluster-core.

However, it is in general not the case that the space can be
partitioned into attraction regions. For example, if a ﬂow
reaches a saddle point, it will get stuck there and thus any
point whose ﬂow ends up at a saddle point will not belong to
any attraction region. In this paper, we only give guarantees
about the clustering of points which are in an attraction
region.

The next regularity assumption we make is that the cluster-
cores are on the interior of the attraction region (to avoid
situations such as when the cluster-cores intersect with the
boundary of the input space).
Assumption 2. There exists R0 > 0 such that Mi +
B(0, R0) ⊆ Ai for i = 1, ..., C, where M + B(0, r) de-
notes {x : inf y∈M ||x − y|| ≤ r}.
Deﬁnition 5 (Level Set). The λ level set of f is deﬁned as
Lf (λ) := {x ∈ X : f (x) ≥ λ}.

The next assumption says that the level sets are continuous
w.r.t. the level in the following sense where we denote the
(cid:15)-interior of A as A(cid:9)(cid:15) := {x ∈ A, inf y∈∂A ||x − y|| ≥ (cid:15)}
(∂A is the boundary of A):
Assumption 3 (Uniform Continuity of Level Sets). For
each (cid:15) > 0, there exists δ > 0 such that for 0 < λ ≤ λ(cid:48) ≤
||f ||∞ with |λ − λ(cid:48)| < δ, then Lf (λ)(cid:9)(cid:15) ⊆ Lf (λ(cid:48)).

This ensures that there are no approximately ﬂat areas in
which the procedure may get stuck at. The assumption is
borrowed from (Jiang, 2017b). Finally, we need the fol-
lowing regularity condition which ensures that level sets
away from cluster-cores do not get arbitrarily thin. This is
adapted from standard analyses of level-set estimation (e.g.
Assumption B of Singh et al. (2009)).
Assumption 4. Let µ denote the Lebesgue measure on Rd.
For any r > 0, there exists σ > 0 such that the following
holds for any connected component A of any level-set of f
which is not contained in Mi for any i: µ(B(x, r) ∩ A) ≥ σ
for all x ∈ A.

For our consistency results, we prove that Quickshift++ can
cluster the sample points in the (R, ρ)-interior of an attrac-

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

tion region (deﬁned below) for each cluster-core properly
where R, ρ > 0 are ﬁxed and can be chosen arbitrarily
small.
Deﬁnition 6 ((R, ρ)-interior of Attraction Regions). Deﬁne
the (R, ρ)-interior of Ai, denoted as A(R,ρ)
, as the set of
points x0 ∈ Ai such that each path P from x0 to any point
y ∈ ∂Ai satisﬁes the following.

i

sup
x∈P

inf
x(cid:48)∈B(x,R)

f (x(cid:48)) ≥ sup

f (x(cid:48)) + ρ.

x(cid:48)∈B(y,R)

In other words, points in the interior satisfy the property
that any path leaving its attraction region must sufﬁciently
decrease in density at some point. This decrease threshold
is parameterized by R and ρ.

Figure 2. Illustration of interior of attraction region in 1-dimension.
The pink and green shaded regions indicated the two attraction
regions. The striped parts show the corresponding interiors of the
attraction regions.

We ﬁrst give a guarantee on the ﬁrst step of MCores recov-
ers, that the cluster-cores are reasonably recovered. The
proof follows from the analysis of Jiang & Kpotufe (2017)
by replacing modal-sets with cluster-cores, and the results
match up to constant factors. The proof is omitted here.

Theorem 1. [Adapted from Theorem 3, 4 of Jiang &
Kpotufe (2017)] Suppose that Assumptions 1, 3, and 4 hold.
Let 0 < β < 1, (cid:15), δ > 0 and suppose that k ≡ k(n) is
chosen such that log2 n/k → 0 and n4/(4+d)/k → 0. Let
M1, ..., MC be the cluster-cores of f . Then for n sufﬁciently
large depending on f , δ, (cid:15), and β, with probability at least
1 − δ, MCores returns C cluster-core estimates (cid:99)M1, ..., (cid:100)MC
such that Mi ∩ X[n] ⊆ (cid:99)Mi ⊆ Mi + B(0, (cid:15)) for i ∈ 1, ..., C.
Remark 2. The original result from Jiang & Kpotufe (2017)
is about (cid:15)-approximate modal-set which are deﬁned as level-
sets whose density has range (cid:15). Our notion of cluster-core
is similar, but the range is a β-proportion of the highest
density level within the level-set. Using a proportion is
more interpretable and thus more useful, as the scale of the
density function is difﬁcult to determine in practice.

We now state the main result, which says that as long as the
cluster-cores are sufﬁciently well estimated (up to a certain
Hausdorff error) by MCores (via previous theorem), then
Quickshift++ will correctly cluster the (R, ρ)-interiors of
the attraction regions with high probability.

Theorem 2. Suppose that Assumptions 1, 2, 3, and 4 hold.
Let 0 < R < R0 and ρ, δ > 0. Suppose that k ≡ k(n)
is chosen such that log2 n/k → 0 and n4/(4+d)/k → 0.
Suppose that (cid:99)M1, ..., (cid:99)MC are the cluster-cores returned by
Algorithm 1 and satisfy Mi ∩X[n] ⊆ (cid:99)Mi ⊆ Mi +B(0, R/4)
for i = 1, ..., C. Then for n sufﬁciently large depending
on f , ρ, δ and R, the following holds with probably at
least 1 − 2δ uniformly in x ∈ A(R,ρ)
∩ X[n] and i ∈ [C]:
Quickshift++ clusters x to the cluster corresponding to Mi.

i

4.1. Proof of Theorem 2

We require the following uniform bound on k-NN density
estimator, which follows from Dasgupta & Kpotufe (2014).

Lemma 2. Let δ > 0. Suppose that f is Lipschitz continu-
ous with compact support X (e.g. there exists L such that
|f (x)−f (x(cid:48))| ≤ L|x−x(cid:48)| for all x, x(cid:48) ∈ X ) and f satsiﬁes
Assumption 1. Then exists constant C depending on f such
that the following holds if n ≥ C 2
δ,n with probability at least
1 − δ.

|fk(x) − f (x)| ≤ C

sup
x∈X

(cid:32)

Cδ,n√
k

+

(cid:18) k
n

(cid:19)1/d(cid:33)

.

where Cδ,n := 16 log(2/δ)

d log n.

√

We next need the following uniform concentration bound
on balls intersected with level-sets, which says that if such
a set has large enough probability mass, then it will contain
a sample point with high probability.
Lemma 3. Let E := {B(x, r) ∩ Lf (λ) : x ∈ Rd, r >
0, λ > 0}. Then the following holds with probability at
least 1 − δ uniformly for all E ∈ E

F(E) ≥ Cδ,n

⇒ E ∩ Xn (cid:54)= ∅.

√

d log n
n

Proof. The indicator functions 1[B(x, f ) ∩ Lf (λ)] for x ∈
Rd, λ > 0 have VC-dimension d + 1. This is because the
balls over Rd have VC-dimension d + 1 and the level-sets
Lf (λ) has VC-dimension 1 and thus their intersection has
VC-dimension d + 1 (Van Der Vaart & Wellner, 2009). The
result follows by applying Theorem 15 of Chaudhuri &
Dasgupta (2010).

In other words, with high probability, MCores estimates
each cluster-core bijectively and that for each cluster-core,
MCores’ estimate contains all of the sample points and that
the estimate does not over-estimate by much.

Proof of Theorem 2. Suppose that x0 ∈ A(R,ρ)
∩ X[n] and
Quickshift++ gives directed path x0 → x1 → · · · → xL
where x1, ..., xL−1 are outside of cluster-cores and xL is in
a cluster-core but xL (cid:54)∈ Ai.

i

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

We ﬁrst show that ||xi − xi+1|| ≤ R/2 for i = 0, ..., L − 1.
By Assumption 3 and 4, we have that there exists τ > 0
and σ > 0 such that the following holds uniformly for
i = 0, ..., L − 1:

(cid:18)

(cid:18)

(cid:19)

(cid:19)

µ

B(xi, R/2) ∩ Lf (f (xi) + τ )

≥ σ.

Hence, since the density is uniformly lower bounded by λ0,
we have

F

B(xi, R/2) ∩ Lf (f (xi) + τ )

≥ σλ0.

√

Then by Lemma 3, for n sufﬁicently large such that σλ0 >
d log n
, then with probability at least 1 − δ there exists
Cδ,n
n
sample point x(cid:48)
i in B(xi, R/2) ∩ Lf (f (xi) + τ ) for i =
0, ..., L − 1.

Next, choose n sufﬁciently large such that by Lemma 2, we
have with probability at least 1 − δ that

|fk(x) − f (x)| ≤ min{τ, ρ}/3.

sup
x∈X

Thus, we have

fk(x(cid:48)

i) ≥ f (x(cid:48)

i) − τ /3 ≥ f (xi) + 2τ /3

≥ fk(xi) + τ /3 > fk(xi).

i|| ≤ R/2 and x(cid:48)

i ∈ X[n], it follows that

Moreover ||xi − x(cid:48)
||xi − xi+1|| ≤ R/2 for i = 0, ..., L − 1.
Let π : [0, 1] → Rd be the piecewise linear path deﬁned by
π(j/L) = xj for j = 0, ..., L. Let t2 = min{t ∈ [0, 1] :
π(t) ∈ ∂Ai}. Then, by deﬁnition of A(R,ρ)
, there exists
0 ≤ t1 < t2 such that x := π(t1) and y := π(t2) satisﬁes
y ∈ ∂Ai and

i

inf
x(cid:48)∈B(x,R)

f (x(cid:48)) ≥ sup

f (x(cid:48)) + ρ.

x(cid:48)∈B(y,R)

Thus, there exists indices p, q ∈ {0, ..., L − 1} such that
p ≤ q, |xp − x| ≤ R, and |xq − y| ≤ R. Thus, we have
f (xp) ≥ f (xq) + ρ, but fk(xp) ≤ fk(xq). However, we
have

fk(xp) ≥ f (xp) − ρ/3 ≥ f (xq) + 2ρ/3
≥ fk(xq) + ρ/3 > fk(xq),

a contradiction, as desired.

5. Simulations

Figure 3 provides simple veriﬁcation that Quickshift++
provides reasonable clusterings in a wide variety of situ-
ations where other density-based procedures are known to
fail. For instance, in the two rings dataset (ﬁrst row), we

Figure 3. Comparison against other clustering algorithms on toy
datasets, adapted from scikit-learn cluster demo. Quickshift++
settings were ﬁxed at k = 20, β = 0.7 for all the datasets, while
the other algorithms were tuned to obtain a reasonable number of
clusters.

see that Mean Shift and Quick Shift suffer from the over-
segmentation issue coupled with the oversized bandwidth
which causes them to recover clusters that have points from
both the rings even though the rings are separated. In the
three Gaussians dataset (third row), we see that DBSCAN
fails because the three clusters are of different density levels
and thus no matter which density-level we set, DBSCAN
will not be able to recover the three clusters.

6. Image Segmentation

In order to apply clustering to image segmentation, we use
the following standard approach (see e.g. Felzenszwalb
& Huttenlocher (2004)): we transform each pixel into a
5-dimensional vector where two coordinates correspond to
the location of the pixel and three correspond to each of
the RGB color channels. Then segmentation is done by
clustering this 5-dimensional dataset.

We observed that for Quickshift++, setting β = 0.9 is rea-
sonable across a wide range of images, β was ﬁxed to this
value for segmentation here. We compare Quickshift++ to
Quick Shift, as the latter is often used for segmentation.
Quick Shift often over-segments in some areas and under-
segments in other areas under any hyperparameter setting
and we showed the settings which provided a reasonable
trade-off. On the other hand Quickshift++ gives us reason-

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

able segmentations in many cases and can capture segments
that may be problematic for other procedures.

As shown in the ﬁgures, it moreover has the interesting prop-
erty of being able to recover segments of widely varying
shapes and sizes in the same image, which suggests that
modelling the dense regions of the segments as cluster-cores
instead of point-modes may be useful as we compare to
Quick Shift. Although this is only qualitative, it further sug-
gests that Quickshift++ is a versatile algorithm and begins
to show its potential application in many more areas.

Figure 6. Assorted fruit in a metal bowl. For Quick Shift, band-
width was set to 8 and for Quickshift++, k = 100 and β = 0.9.
Quickshift++ is able to segment most of the fruits in the bowl,
while recovering the details of the bowl as well as the structures in
the background.

7. Clustering Experiments

We ran Quickshift++ against other clustering algorithms
on the various real datasets and scored against the ground-
truth using the adjusted rand index and the adjusted mutual
information scores.

Dataset
(A) seeds
(B) phonemes
(C) iris
(D) banknote
(E) images
(F) letters
(G) MNIST
(H) page blocks
(I) glass

n
210
4509
150
1372
210
20000
1000
5473
214

d
7
258
4
4
19
16
784
10
19

Clusters
4
5
3
2
7
26
10
5
7

Figure 8. Summary of datasets used, including dataset size (n),
number of features (d) and number of clusters.

Datasets Used: Summary of the datasets can be found in
Figure 8. Seeds, glass, and iris are standard UCI datasets
(Lichman, 2013) used for clustering. Banknote is another
UCI dataset which involves identifying whether a banknote
is forged or not, based on various statistics of an image of
the banknote. Page Blocks is a UCI dataset which involves
determining the type of a portion of a page (e.g. text, image,
etc) based on various statistics of an image of the portion.
Phonemes (Friedman et al., 2001) is a dataset which involves
the log periodograms of spoken phonemes. Images is a UCI
dataset called Statlog, based on features extracted from
various images, and letters is the UCI letter recognition
dataset. We also used a small subset of MNIST (LeCun
et al., 2010) for our experiments.

Figure 4. Figure skater Yuzuru Hanyu performs at the 2018 Winter
Olympics. Quick Shift was set with bandwidth 10 and Quick-
shift++ was set with k = 300 and β = 0.9. We see that when
compared to Quick Shift, Quickshift++ is able to recover the varia-
tions in the background more accurately, including correctly seg-
menting most of the letters on the wall, while still recovering the
structure of Hanyu’s costume accurately.

Figure 5. Yuzuru Hanyu at the 2017 Rostelecom Cup. Quick Shift
was set with bandwidth 15 and Quickshift++ was set with k =
50 and β = 0.9. Quickshift++ can recover the homogeneous
background as a whole, and reasonably separates Hanyu’s light-
colored costume from the background.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Figure 7. For each algorithm, we show clustering performance as a function of its respective hyperparameter setting. The blue line is adj.
mutual information and the red line is adj. rand index. Notice that for Quickshift++, we show a wide range of k (relative to n), while for
the popular procedures, their respective parameters had to be carefully tuned to ﬁnd the region where the scores are non-trivial.

B

C

KMns DScn MCrs MSft QSft
.7327
.6715
.6872
.6360
.7165
.7361
.7149
.7479
.7261
.6203
.5836
.7265
.3318
.5145
.3397
.3857
.5008
.4077
.5814
.5364
.1793
.1284
.4940
.4217
.2503
.2584
.2911
.3483
.3251
.0925
.1363
.0397
.2929
.2647
.4195
.3523

.7319
.6769
.5974
.5700
.7028
.6106
.2434
.2351
.3497
.4656
.1287
.3027
.2281
.1958
.0028
.0526
.2790
.3858

.4473
.4429
.4458
.5731
.5898
.5865
.5584
.4594
.3313
.5264
.0705
.4422
.1070
.2164
.1962
.1179
.2844
.3542

A .7092
.6738
.7432
.7574
.7294
.7418
D .2893
.2690
.4177
.5497
.1384
.3741
G .3320
.4629
H .0830
.0524
.2770
.3865

E

F

I

QS++
.7261
.7085
.7530
.7870
.7399
.7424
.6152
.4866
.5359
.6456
.1766
.5001
.3606
.4806
.4727
.2192
.2849
.4250

Figure 9. For each dataset, the ﬁrst row is the adjusted rand in-
dex scores and the second row is the adjusted mutual informa-
tion scores. Bolded are highest and second highest scores. For
MCores and Quickshift++, we used a single β = 0.3 for each
dataset with the exception of for banknote where β = 0.7. Then
the procedures were tuned in their respective essential hyperpa-
rameter: k-means (KMns) number of clusters, DBSCAN (DScn)
epsilon, MCores (MCrs) k from k-NN, mean shift (MSft) band-
width, quick shift (QSft) bandwidth, Quickshift++ (QS++) k.

We evaluate performance under the Adjusted Mutual Infor-
mation and Rand Index scores (Vinh et al., 2010) which are
metrics to compare clusterings. Not only do we show that
Quickshift++ considerably outperforms the popular density-
based clustering procedures under optimal tuning (Figure 9),
but that it is also robust in its hyperparameter k (Figure 7),
all while ﬁxing β = 0.3 for all but one of the datasets. Such
robustness to its tuning parameters is highly desirable since
optimal tuning is usually not available in practice.

8. Conclusion

We presented Quickshift++, a new density-based clustering
procedure that ﬁrst estimates the cluster-cores of the density,
which are locally high-density regions. Then remaining
points are assigned to its appropriate cluster-core using a
hill-climbing procedure based on Quick Shift. Such cluster-
cores turn out to be more stable and expressive representa-
tions of the possibly complex clusters than point-modes. As
a result, Quickshift++ enjoys the advantages of the popular
density-based clustering algorithms while avoiding many
of their respective weaknesses. We then gave guarantees
for cluster recovery. Finally, we showed that the algorithm
has strong and robust performance on real datasets and has
promising applications to image segmentation.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Acknowledgements

We thank the anonymous reviewers for their helpful feed-
back.

References

Arias-Castro, Ery, Mason, David, and Pelletier, Bruno. On
the estimation of the gradient lines of a density and the
consistency of the mean-shift algorithm. Journal of Ma-
chine Learning Research, 17(43):1–28, 2016.

Balakrishnan, Sivaraman, Narayanan, Srivatsan, Rinaldo,
Alessandro, Singh, Aarti, and Wasserman, Larry. Cluster
trees on manifolds. In Advances in Neural Information
Processing Systems, pp. 2679–2687, 2013.

Cadre, Benoıt. Kernel estimation of density level sets. Jour-
nal of multivariate analysis, 97(4):999–1023, 2006.

Chac´on, Jos´e E. Clusters and water ﬂows: a novel approach
to modal clustering through morse theory. arXiv preprint
arXiv:1212.1384, 2012.

Chaudhuri, Kamalika and Dasgupta, Sanjoy. Rates of con-
In Advances in Neural

vergence for the cluster tree.
Information Processing Systems, pp. 343–351, 2010.

Chaudhuri, Kamalika, Dasgupta, Sanjoy, Kpotufe, Samory,
and von Luxburg, Ulrike. Consistent procedures for clus-
ter tree estimation and pruning. IEEE Transactions on
Information Theory, 60(12):7900–7912, 2014.

Chen, Yen-Chi, Genovese, Christopher R, Wasserman,
Larry, et al. A comprehensive approach to mode clus-
tering. Electronic Journal of Statistics, 10(1):210–241,
2016.

Chen, Yen-Chi, Genovese, Christopher R, and Wasserman,
Larry. Density level sets: Asymptotics, inference, and
visualization. Journal of the American Statistical Associ-
ation, pp. 1–13, 2017.

Cheng, Yizong. Mean shift, mode seeking, and cluster-
ing. IEEE transactions on pattern analysis and machine
intelligence, 17(8):790–799, 1995.

Comaniciu, Dorin and Meer, Peter. Mean shift: A robust
approach toward feature space analysis. IEEE Transac-
tions on pattern analysis and machine intelligence, 24(5):
603–619, 2002.

Cormen, Thomas H. Introduction to algorithms. MIT press,

2009.

Ester, Martin, Kriegel, Hans-Peter, Sander, J¨org, Xu, Xi-
aowei, et al. A density-based algorithm for discovering
clusters in large spatial databases with noise. In Kdd,
volume 96, pp. 226–231, 1996.

Felzenszwalb, Pedro F and Huttenlocher, Daniel P. Efﬁcient
graph-based image segmentation. International journal
of computer vision, 59(2):167–181, 2004.

Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
The elements of statistical learning, volume 1. Springer
series in statistics New York, 2001.

Genovese, Christopher R, Perone-Paciﬁco, Marco,
Verdinelli, Isabella, and Wasserman, Larry. Non-
Journal of
parametric inference for density modes.
the Royal Statistical Society: Series B (Statistical
Methodology), 78(1):99–126, 2016.

Hartigan, John A. Clustering algorithms, volume 209. Wiley

New York, 1975.

Jiang, Heinrich. Density level set estimation on manifolds
with dbscan. arXiv preprint arXiv:1703.03503, 2017a.

Jiang, Heinrich. On the consistency of quick shift. In Neural

Information Processing Systems (NIPS), 2017b.

Jiang, Heinrich and Kpotufe, Samory. Modal-set estimation
with an application to clustering. In Artiﬁcial Intelligence
and Statistics, pp. 1197–1206, 2017.

Kpotufe, Samory and von Luxburg, Ulrike. Pruning nearest
neighbor cluster trees. arXiv preprint arXiv:1105.0540,
2011.

LeCun, Yann, Cortes, Corinna, and Burges, CJ. Mnist hand-
written digit database. AT&T Labs [Online]. Available:
http://yann. lecun. com/exdb/mnist, 2, 2010.

Li, Jia, Ray, Surajit, and Lindsay, Bruce G. A nonparametric
statistical approach to clustering via mode identiﬁcation.
Journal of Machine Learning Research, 8(Aug):1687–
1723, 2007.

Lichman, M. UCI machine learning repository, 2013. URL

http://archive.ics.uci.edu/ml.

Rigollet, Philippe, Vert, R´egis, et al. Optimal rates for
plug-in estimators of density level sets. Bernoulli, 15(4):
1154–1178, 2009.

Rinaldo, Alessandro and Wasserman, Larry. Generalized
density clustering. The Annals of Statistics, pp. 2678–
2722, 2010.

Dasgupta, Sanjoy and Kpotufe, Samory. Optimal rates for
k-nn density and mode estimation. In Advances in Neural
Information Processing Systems, pp. 2555–2563, 2014.

Rodriguez, Alex and Laio, Alessandro. Clustering by fast
search and ﬁnd of density peaks. Science, 344(6191):
1492–1496, 2014.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Singh, Aarti, Scott, Clayton, Nowak, Robert, et al. Adaptive
hausdorff estimation of density level sets. The Annals of
Statistics, 37(5B):2760–2782, 2009.

Sriperumbudur, Bharath and Steinwart, Ingo. Consistency
and rates for clustering with dbscan. In Artiﬁcial Intelli-
gence and Statistics, pp. 1090–1098, 2012.

Steinwart, Ingo. Adaptive density level set clustering. In
Proceedings of the 24th Annual Conference on Learning
Theory, pp. 703–738, 2011.

Tsybakov, Alexandre B et al. On nonparametric estimation
of density level sets. The Annals of Statistics, 25(3):
948–969, 1997.

Van Der Vaart, Aad and Wellner, Jon A. A note on bounds
for vc dimensions. Institute of Mathematical Statistics
collections, 5:103, 2009.

Vedaldi, Andrea and Soatto, Stefano. Quick shift and kernel
methods for mode seeking. Computer vision–ECCV 2008,
pp. 705–718, 2008.

Verdinelli,

Isabella and Wasserman, Larry.

sis of a mode clustering diagram.
arXiv:1805.04187, 2018.

Analy-
arXiv preprint

Vinh, Nguyen Xuan, Epps, Julien, and Bailey, James. In-
formation theoretic measures for clusterings compari-
son: Variants, properties, normalization and correction
for chance. Journal of Machine Learning Research, 11
(Oct):2837–2854, 2010.

Wasserman, Larry, Azizyan, Martin, and Singh, Aarti. Fea-
ture selection for high-dimensional clustering. arXiv
preprint arXiv:1406.2240, 2014.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Heinrich Jiang 1 Jennifer Jang 2 Samory Kpotufe 3

8
1
0
2
 
y
a
M
 
1
2
 
 
]

G
L
.
s
c
[
 
 
1
v
9
0
9
7
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

We provide initial seedings to the Quick Shift clus-
tering algorithm, which approximate the locally
high-density regions of the data. Such seedings
act as more stable and expressive cluster-cores
than the singleton modes found by Quick Shift.
We establish statistical consistency guarantees for
this modiﬁcation. We then show strong clustering
performance on real datasets as well as promising
applications to image segmentation.

1. Introduction

Quick Shift (Vedaldi & Soatto, 2008) is a mode-seeking
based clustering algorithm that has a growing popularity in
computer vision. It proceeds by repeatedly moving each
sample to its closest sample point that has higher empirical
density if one exists within a τ -radius ball, otherwise we
stop. Thus each path ends at a point which can be viewed
as a local mode of the empirical density. Then, points that
end up at the same mode are assigned to the same cluster.
The most popular choice of empirical density function is
the Kernel Density Estimator (KDE) with Gaussian Kernel.
The algorithm also appears in Rodriguez & Laio (2014).

Quick Shift was designed as a faster alternative to the well-
known Mean Shift algorithm (Cheng, 1995; Comaniciu &
Meer, 2002). Mean Shift is equivalent to performing a
gradient ascent of the KDE starting at each sample until
convergence (Arias-Castro et al., 2016). Samples that cor-
respond to the same points of convergence are in the same
cluster and the points of convergence are taken to be the
estimates of the modes. Thus, both procedures hill-climb
to the local modes of the empirical density function and
cluster based on these modes. The key differences are that
Quick Shift restricts the steps to sample points (and thus is
a sample-based version of Mean Shift) and has the extra τ
parameter which allows it to merge close segments together.

1Google Research, Mountain View, CA 2Uber Inc, San Fran-
cisco, CA 3Princeton University, Princeton, NJ. Correspondence
to: Heinrich Jiang <heinrich.jiang@gmail.com>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

One of the drawbacks of these two procedures, as well as
many mode-seeking based clustering algorithms, is that the
point-modes of the density functions are often poor rep-
resentations of the clusters. This will happen when the
high-density regions within a cluster are of arbitrary shape
and have some variations causing the underlying density
function to have possibly many apparent, but not so salient
modes. In this case, such procedures asymptotically recover
all of the modes separately, leading to over-segmentation.
To combat this effect, practitioners often increase the kernel
bandwidth, which makes the density estimate more smooth.
However, this can cause the density estimate to deviate too
far from the original density we are intending to cluster
based on.1 Thus, practitioners may not wish to identify
the clusters based on the point-modes of the density func-
tion, but rather identify them based on locally high density
regions of the dataset (See Figure 1).2

We propose modeling these locally high-density regions as
cluster-cores (to be precisely deﬁned later), which can be of
arbitrary shape, size, and density level, and are thus better
suited at capturing the possibly complex topological prop-
erties of clusters that can arise in practice. In other words,
these cluster-cores are better at expressing the clusters and
are more stable as they are less sensitive to the small ﬂuctu-
ations that can arise in the empirical density function. We
parameterize the cluster-core by β where 0 < β < 1, which
determines how much the density is allowed to vary within
the cluster-core. We estimate them from a ﬁnite sample us-
ing a minor modiﬁcation of the MCores algorithm of Jiang
& Kpotufe (2017).

We introduce Quickshift++, which ﬁrst estimates these
cluster-cores, and then runs the Quick Shift based hill-
climbing procedure on each remaining sample until it
reaches a cluster-core.
Samples that end up in the
same cluster-core are assigned to the same cluster; thus,

1KDE with Gaussian kernel and bandwidth h approximates the
underlying density convolved with a Gaussian with mean 0 and
covariance h2I. Thus, the higher h is, the more the KDE deviates
from the original density.

2Over-segmentation is also dealt with in Quick Shift via the
τ parameter, but a threshold for the distance between two modes
which should be clustered together is hard to determine in practice.
Moreover, there may not even be a good setting of τ which works
everywhere in the input space.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

the cluster-cores can be seen as representing the high-
conﬁdence regions within each cluster. We utilize the k-NN
density estimator as our empirical density.

Despite the simplicity of our approach, we show that Quick-
shift++ considerably outperforms the popular density-based
clustering algorithms, while being efﬁcient. Another desir-
able property of Quickshift++ is that it is simple to tune its
two hyperparameters β and k.3 We show that a few settings
of β turn out to work for a wide range of applications and
that the procedure is stable in choices of k.

We then give a novel statistical consistency analysis for
Quickshift++ which provides guarantees that points within
a cluster-core’s attraction regions (to be described later)
are correctly assigned. We also show promising results on
image segmentation, which further validates the desirability
of using cluster-cores on real-data applications.

Figure 1. It can often be the case that the locally high-density
regions are of arbitrary shape and ﬂuctuations within them lead to
many apparent modes. Left: Mode-seeking clustering procedures
often lead to over-segmentation. Right: It may be more desirable
to use cluster-cores (shaded), which allows ﬂuctuations within
arbitrarily-shaped regions of locally high density.

2. Related Works and Contributions

We show that Quickshift++ is a new and powerful addition
to the family of clustering procedures known as density-
based clustering, which most notably includes DBSCAN
(Ester et al., 1996) and Mean Shift (Cheng, 1995). Such
procedures operate on the estimated density function based
on a ﬁnite sample to recover structures in the density func-
tion that ultimately correspond to the clusters. There are
several advantages of density-based clustering over classical
objective-based procedures such as k-means and spectral
clustering. Density-based procedures can automatically
detect the number of clusters, while objective-based pro-
cedures typically require this as an input. Density-based
clustering algorithms also make little assumptions on the
shapes of the clusters as well as their relative positions.

Density-based clustering procedures can roughly be classi-
ﬁed into two categories: hill-climbing based approaches
(discussed earlier, which includes both Mean Shift and

3The τ parameter from Quick Shift is unnecessary here because
we climb until we reach a cluster-core as our stopping condition.

Quick Shift) and density-level set based approaches. We
now discuss the latter approach, which takes the connected
components of the density-level set deﬁned by {x : f (x) ≥
λ} for some density level λ as the clusters. This statistical
notion of clustering traces back to Hartigan (1975). Since
then, there has been extensive work done, e.g. Tsybakov
et al. (1997); Cadre (2006); Rigollet et al. (2009); Singh et al.
(2009); Chaudhuri & Dasgupta (2010); Rinaldo & Wasser-
man (2010); Kpotufe & von Luxburg (2011); Balakrishnan
et al. (2013); Chaudhuri et al. (2014); Chen et al. (2017).
More recently, Sriperumbudur & Steinwart (2012); Jiang
(2017a) show that the popular DBSCAN algorithm turns
out to converge to these clusters. However, one of the main
drawbacks of this approach is that the density-level λ is
ﬁxed and thus such methods perform poorly when the clus-
ters are at different density-levels. Moreover, the question
of how to choose λ remains (e.g. Steinwart (2011)).

Jiang & Kpotufe (2017) provide an alternative notion of
clusters, called modal-sets, which are regions of ﬂat den-
sity which are local maximas of the density. They can be
of arbitrary shape, dimension, or density. They provide a
procedure, MCores, which estimates these with consistency
guarantees. Our notion of cluster-core is similar to modal-
sets, but the density within a cluster-core is allowed to vary
by a substantial amount in order to capture such variations
seen in real data as a the ﬂat density of modal-sets may be
too restrictive in practice. It turns out that a small modiﬁ-
cation of MCores allows us to estimate these cluster-cores.
Thus Quickshift++ has the advantage over DBSCAN in that
clusters can be at any density level and that furthermore, the
density levels are chosen adaptively.

Mcores however consists of an over-simplistic ﬁnal cluster-
ing: it simply assigns each point to its closest modal-set,
while in practice, clusters tend not to follow the geome-
try induced by the Euclidean metric. Quickshift++ on the
other hand clusters the remaining points by a hill-climbing
method which we show is far better in practice.

Thus, Quickshift++ combines the strengths of both density-
based clustering approaches while avoiding many of their
weaknesses. In addition to the general advantages of density-
based clustering algorithms shared by both approaches, it
is also able to both (1) recover clusters at varying density
levels and (2) not suffer from the over-segmentation issue
described in Figure 1. To our knowledge, no other procedure
has been shown to have this property.

For our theoretical analysis, we give guarantees about Quick-
shift++’s ability to recover the clusters based on attraction
regions deﬁned by the gradient ﬂows. Wasserman et al.
(2014); Arias-Castro et al. (2016) showed that Mean Shift’s
iterates approximate the gradient ﬂows. Some progress
has been made in understanding Quick Shift (Jiang, 2017b;
Verdinelli & Wasserman, 2018). There are also related lines

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

of work in mode clustering e.g. (Li et al., 2007; Chac´on,
2012; Genovese et al., 2016; Chen et al., 2016). In this
paper, we show that Quickshift++ recovers the interior of its
attraction region, thus adding to our statistical understanding
of hill-climbing based clustering procedures.

a ﬁxed additive ﬂuctuation. It uses the levels of the mutual
k-NN graph of the sample points, deﬁned below.
Deﬁnition 3. Let G(λ) denote the λ-level of the mutual
k-NN graph with vertices {x ∈ X[n] : fk(x) ≥ λ} and an
edge between x and x(cid:48) iff ||x − x(cid:48)|| ≤ min{rk(x), rk(x(cid:48))}.

3. Algorithm

3.1. Basic Deﬁnitions

Let X[n] = {x1, ..., xn} be n i.i.d. samples drawn from an
unknown density f , deﬁned over the Lebesgue measure on
Rd. Suppose that f has compact support X .

Our procedure will operate on the k-NN density estimator:

Deﬁnition 1. Let rk(x) := inf{r > 0 : |B(x, r) ∩ X[n]| ≥
k}, i.e., the distance from x to its k-th nearest neighbor.
Deﬁne the k-NN density estimator as

fk(x) :=

k
n · vd · rk(x)d ,

where vd is the volume of a unit ball in Rd.

3.2. Cluster-Cores

We deﬁne the cluster core with respect to ﬁxed ﬂuctuation
parameter β as follows.

Deﬁnition 2. Let 0 < β < 1. Closed and connected set
M ⊂ X is a cluster-core if M is a connected component
(CC) of {x ∈ X : f (x) ≥ (1 − β) · maxx(cid:48)∈M f (x(cid:48))}.

Note that when β → 0, then the cluster-cores become the
modes or local-maximas of f . When β → 1, then the
cluster-core becomes the entire support X . We next give a
very basic fact about cluster-cores, that they do not overlap.

Lemma 1. Suppose that M1, M2 are distinct cluster-cores
of f . Then M1 ∩ M2 = ∅.

Proof. Suppose otherwise. We have that M1 and M2 are
CCs of {x ∈ X : f (x) ≥ λ1} and {x ∈ X : f (x) ≥ λ2},
respectively for some λ1, λ2. Clearly, if λ1 = λ2, then it
follows that M1 = M2. Then, without loss of generality, let
λ1 < λ2. Then since the CCs of {x ∈ X : f (x) ≥ λ2} are
nested in the CCs of {x ∈ X : f (x) ≥ λ1}, then it follows
that M2 ⊆ M1. Then, λ2 = (1 − β) supx∈M2 f (x) ≤ (1 −
β) supx∈M1 f (x) = λ1, a contradiction. As desired.

Algorithm 1 is a simple modiﬁcation of MCores by Jiang &
Kpotufe (2017). The difference is that we use a multiplica-
tive ﬂuctuation parameter β, while Jiang & Kpotufe (2017)
uses an additive one. The latter requires knowledge of the
scale of the density function, which is difﬁcult to determine
in practice. Moreover, the multiplicative ﬂuctuation adapts
to clusters at varying density levels more reasonably than

It is known that G(λ) approximates the CCs of the λ-level
sets of the true density, deﬁned as {x ∈ X : f (x) ≥ λ}
see e.g. (Chaudhuri & Dasgupta, 2010). Moreover, it can
be seen that the CCs of G(λ) forms a hierarchical nesting
structure as λ decreases.

Algorithm 1 proceeds by performing a top-down sweep of
the levels of the mutual k-NN graph, G(λ). As λ decreases,
it is clear that more nodes appear and that connectivity
increases. In other words, as we scan top-down, the CCs of
G(λ) become larger, some CCs can merge, or new CCs can
appear. When a new CC appears at level λ, then intuitively,
it should correspond to a local maxima of f , which appears
at a density level approximately λ. This follows from the
fact that the CCs of G(λ) approximates the CCs of {x ∈
X : f (x) ≥ λ}. Thus, the idea is that when a new CC
appears in G(λ), then we can take the corresponding CC in
G(λ − βλ) (which is the density level (1 − β) times that of
the highest point in the CC) to estimate the cluster-core.

Algorithm 1 MCores (estimating cluster-cores)

Parameters k, β
Initialize (cid:99)M := ∅.
Sort the xi’s in decreasing order of fk values (i.e.
fk(xi) ≥ fk(xi+1)).
for i = 1 to n do

Deﬁne λ := fk(xi).
Let A be the CC of G(λ − βλ) containing xi.
if A is disjoint from all cluster-cores in (cid:99)M then

Add A to (cid:99)M.

end if
end for
return (cid:99)M.

Algorithm 2 Quickshift++

Let (cid:99)M be the cluster-cores obtained by running Algo-
rithm 1.
Initialize directed graph G with vertices {x1, ..., xn} and
no edges.
for i = 1 to n do

If xi is not in any cluster-core, then add to G an edge
from xi to its closest sample x ∈ X[n] such that
fk(x) > fk(xi).

end for
For each cluster-core M ∈ (cid:99)M, let (cid:98)CM be the points
x ∈ X[n] such that the directed path in G starting at x
ends in M .
return { (cid:98)CM : M ∈ (cid:99)M}.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

3.3. Quickshift++

Quickshift++ (Algorithm 2) proceeds by ﬁrst running Algo-
rithm 1 to obtain the cluster-cores, and then moving each
sample point to its nearest neighbor that has higher k-NN
density until it reaches some cluster-core. All samples that
end up in the same cluster-core after the hill-climbing are
assigned to the same cluster. Note that since the highest
empirical density sample point is contained in a cluster-core,
it follows that each sample point not in a cluster-core will
eventually be assigned to a unique cluster-core. Thus, Quick-
shift++ provides a clustering assignment of every sample
point.
Remark 1. Although it seems a similar procedure could
have been constructed by using Mean Shift in place of Quick
Shift, Mean Shift could have convergence outside of the
estimated cluster-cores, while Quick Shift guarantees that
each sample outside of a cluster-core get assigned to some
cluster-core.

3.4. Implementation

The implementation details for the MCores modiﬁcation
can be inferred from Jiang & Kpotufe (2017). This step
runs in O(nk · α(n)) where α is the Inverse Ackermann
function (Cormen, 2009), in addition to the time it takes to
compute the k-NN sets for the n sample points. To cluster
the remaining points, for each sample not in a cluster-core,
we must ﬁnd its nearest sample of higher k-NN density. Al-
though this is worst-case O(n) time for each sample point,
fortunately we see that in practice (as long as k is not too
small) for the vast majority of cases, the nearest sample
with higher density is within the k-nearest neighbor set
so it only takes O(k) in most cases. It is an open prob-
lem whether there the nearest sample with higher empir-
ical density is often in its k-NN set. Code release is at
https://github.com/google/quickshift.

4. Theoretical Analysis

For the theoretical analysis, we make ﬁrst the following
regularity assumption, that the density is continuously dif-
ferentiable and lower bounded on X .
Assumption 1. f is continuously differentiable on X and
there exists λ0 > 0 such that inf x∈X f (x) ≥ λ0.

Let M1, ..., MC be the cluster-cores of f . Then we can
deﬁne the following notion of attraction region for each
cluster-core based on the gradient ascent curve or ﬂow. This
is similar to notions of attraction regions for some previous
analyses of mode-based clustering, such as Wasserman et al.
(2014); Arias-Castro et al. (2016), where the intuition is
that attraction regions are deﬁned based by following the
direction of the gradient of the underlying density. In our
situation, instead of an attraction region deﬁned as all points

which ﬂow towards a particular point-mode, the attraction
region is deﬁned around a cluster-core.
Deﬁnition 4 (Attraction Regions). Let path πx : R → Rd
satisfy πx(0) = x, π(cid:48)
x(t) = ∇f (πx(t)). For cluster-core
Mi, its attraction region Ai is the set of points x ∈ X that
satisfy limt→∞ πx(t) ∈ Mi.

It is clear that these attraction regions are well-deﬁned. The
ﬂow path is well-deﬁned since the density is differentiable
and since each cluster-core is deﬁned as a CC of a level
set, the density must decay around its boundaries. In other
words, once an ascent path reaches a cluster-core, it cannot
leave the cluster-core.

However, it is in general not the case that the space can be
partitioned into attraction regions. For example, if a ﬂow
reaches a saddle point, it will get stuck there and thus any
point whose ﬂow ends up at a saddle point will not belong to
any attraction region. In this paper, we only give guarantees
about the clustering of points which are in an attraction
region.

The next regularity assumption we make is that the cluster-
cores are on the interior of the attraction region (to avoid
situations such as when the cluster-cores intersect with the
boundary of the input space).
Assumption 2. There exists R0 > 0 such that Mi +
B(0, R0) ⊆ Ai for i = 1, ..., C, where M + B(0, r) de-
notes {x : inf y∈M ||x − y|| ≤ r}.
Deﬁnition 5 (Level Set). The λ level set of f is deﬁned as
Lf (λ) := {x ∈ X : f (x) ≥ λ}.

The next assumption says that the level sets are continuous
w.r.t. the level in the following sense where we denote the
(cid:15)-interior of A as A(cid:9)(cid:15) := {x ∈ A, inf y∈∂A ||x − y|| ≥ (cid:15)}
(∂A is the boundary of A):
Assumption 3 (Uniform Continuity of Level Sets). For
each (cid:15) > 0, there exists δ > 0 such that for 0 < λ ≤ λ(cid:48) ≤
||f ||∞ with |λ − λ(cid:48)| < δ, then Lf (λ)(cid:9)(cid:15) ⊆ Lf (λ(cid:48)).

This ensures that there are no approximately ﬂat areas in
which the procedure may get stuck at. The assumption is
borrowed from (Jiang, 2017b). Finally, we need the fol-
lowing regularity condition which ensures that level sets
away from cluster-cores do not get arbitrarily thin. This is
adapted from standard analyses of level-set estimation (e.g.
Assumption B of Singh et al. (2009)).
Assumption 4. Let µ denote the Lebesgue measure on Rd.
For any r > 0, there exists σ > 0 such that the following
holds for any connected component A of any level-set of f
which is not contained in Mi for any i: µ(B(x, r) ∩ A) ≥ σ
for all x ∈ A.

For our consistency results, we prove that Quickshift++ can
cluster the sample points in the (R, ρ)-interior of an attrac-

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

tion region (deﬁned below) for each cluster-core properly
where R, ρ > 0 are ﬁxed and can be chosen arbitrarily
small.
Deﬁnition 6 ((R, ρ)-interior of Attraction Regions). Deﬁne
the (R, ρ)-interior of Ai, denoted as A(R,ρ)
, as the set of
points x0 ∈ Ai such that each path P from x0 to any point
y ∈ ∂Ai satisﬁes the following.

i

sup
x∈P

inf
x(cid:48)∈B(x,R)

f (x(cid:48)) ≥ sup

f (x(cid:48)) + ρ.

x(cid:48)∈B(y,R)

In other words, points in the interior satisfy the property
that any path leaving its attraction region must sufﬁciently
decrease in density at some point. This decrease threshold
is parameterized by R and ρ.

Figure 2. Illustration of interior of attraction region in 1-dimension.
The pink and green shaded regions indicated the two attraction
regions. The striped parts show the corresponding interiors of the
attraction regions.

We ﬁrst give a guarantee on the ﬁrst step of MCores recov-
ers, that the cluster-cores are reasonably recovered. The
proof follows from the analysis of Jiang & Kpotufe (2017)
by replacing modal-sets with cluster-cores, and the results
match up to constant factors. The proof is omitted here.

Theorem 1. [Adapted from Theorem 3, 4 of Jiang &
Kpotufe (2017)] Suppose that Assumptions 1, 3, and 4 hold.
Let 0 < β < 1, (cid:15), δ > 0 and suppose that k ≡ k(n) is
chosen such that log2 n/k → 0 and n4/(4+d)/k → 0. Let
M1, ..., MC be the cluster-cores of f . Then for n sufﬁciently
large depending on f , δ, (cid:15), and β, with probability at least
1 − δ, MCores returns C cluster-core estimates (cid:99)M1, ..., (cid:100)MC
such that Mi ∩ X[n] ⊆ (cid:99)Mi ⊆ Mi + B(0, (cid:15)) for i ∈ 1, ..., C.
Remark 2. The original result from Jiang & Kpotufe (2017)
is about (cid:15)-approximate modal-set which are deﬁned as level-
sets whose density has range (cid:15). Our notion of cluster-core
is similar, but the range is a β-proportion of the highest
density level within the level-set. Using a proportion is
more interpretable and thus more useful, as the scale of the
density function is difﬁcult to determine in practice.

We now state the main result, which says that as long as the
cluster-cores are sufﬁciently well estimated (up to a certain
Hausdorff error) by MCores (via previous theorem), then
Quickshift++ will correctly cluster the (R, ρ)-interiors of
the attraction regions with high probability.

Theorem 2. Suppose that Assumptions 1, 2, 3, and 4 hold.
Let 0 < R < R0 and ρ, δ > 0. Suppose that k ≡ k(n)
is chosen such that log2 n/k → 0 and n4/(4+d)/k → 0.
Suppose that (cid:99)M1, ..., (cid:99)MC are the cluster-cores returned by
Algorithm 1 and satisfy Mi ∩X[n] ⊆ (cid:99)Mi ⊆ Mi +B(0, R/4)
for i = 1, ..., C. Then for n sufﬁciently large depending
on f , ρ, δ and R, the following holds with probably at
least 1 − 2δ uniformly in x ∈ A(R,ρ)
∩ X[n] and i ∈ [C]:
Quickshift++ clusters x to the cluster corresponding to Mi.

i

4.1. Proof of Theorem 2

We require the following uniform bound on k-NN density
estimator, which follows from Dasgupta & Kpotufe (2014).

Lemma 2. Let δ > 0. Suppose that f is Lipschitz continu-
ous with compact support X (e.g. there exists L such that
|f (x)−f (x(cid:48))| ≤ L|x−x(cid:48)| for all x, x(cid:48) ∈ X ) and f satsiﬁes
Assumption 1. Then exists constant C depending on f such
that the following holds if n ≥ C 2
δ,n with probability at least
1 − δ.

|fk(x) − f (x)| ≤ C

sup
x∈X

(cid:32)

Cδ,n√
k

+

(cid:18) k
n

(cid:19)1/d(cid:33)

.

where Cδ,n := 16 log(2/δ)

d log n.

√

We next need the following uniform concentration bound
on balls intersected with level-sets, which says that if such
a set has large enough probability mass, then it will contain
a sample point with high probability.
Lemma 3. Let E := {B(x, r) ∩ Lf (λ) : x ∈ Rd, r >
0, λ > 0}. Then the following holds with probability at
least 1 − δ uniformly for all E ∈ E

F(E) ≥ Cδ,n

⇒ E ∩ Xn (cid:54)= ∅.

√

d log n
n

Proof. The indicator functions 1[B(x, f ) ∩ Lf (λ)] for x ∈
Rd, λ > 0 have VC-dimension d + 1. This is because the
balls over Rd have VC-dimension d + 1 and the level-sets
Lf (λ) has VC-dimension 1 and thus their intersection has
VC-dimension d + 1 (Van Der Vaart & Wellner, 2009). The
result follows by applying Theorem 15 of Chaudhuri &
Dasgupta (2010).

In other words, with high probability, MCores estimates
each cluster-core bijectively and that for each cluster-core,
MCores’ estimate contains all of the sample points and that
the estimate does not over-estimate by much.

Proof of Theorem 2. Suppose that x0 ∈ A(R,ρ)
∩ X[n] and
Quickshift++ gives directed path x0 → x1 → · · · → xL
where x1, ..., xL−1 are outside of cluster-cores and xL is in
a cluster-core but xL (cid:54)∈ Ai.

i

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

We ﬁrst show that ||xi − xi+1|| ≤ R/2 for i = 0, ..., L − 1.
By Assumption 3 and 4, we have that there exists τ > 0
and σ > 0 such that the following holds uniformly for
i = 0, ..., L − 1:

(cid:18)

(cid:18)

(cid:19)

(cid:19)

µ

B(xi, R/2) ∩ Lf (f (xi) + τ )

≥ σ.

Hence, since the density is uniformly lower bounded by λ0,
we have

F

B(xi, R/2) ∩ Lf (f (xi) + τ )

≥ σλ0.

√

Then by Lemma 3, for n sufﬁicently large such that σλ0 >
d log n
, then with probability at least 1 − δ there exists
Cδ,n
n
sample point x(cid:48)
i in B(xi, R/2) ∩ Lf (f (xi) + τ ) for i =
0, ..., L − 1.

Next, choose n sufﬁciently large such that by Lemma 2, we
have with probability at least 1 − δ that

|fk(x) − f (x)| ≤ min{τ, ρ}/3.

sup
x∈X

Thus, we have

fk(x(cid:48)

i) ≥ f (x(cid:48)

i) − τ /3 ≥ f (xi) + 2τ /3

≥ fk(xi) + τ /3 > fk(xi).

i|| ≤ R/2 and x(cid:48)

i ∈ X[n], it follows that

Moreover ||xi − x(cid:48)
||xi − xi+1|| ≤ R/2 for i = 0, ..., L − 1.
Let π : [0, 1] → Rd be the piecewise linear path deﬁned by
π(j/L) = xj for j = 0, ..., L. Let t2 = min{t ∈ [0, 1] :
π(t) ∈ ∂Ai}. Then, by deﬁnition of A(R,ρ)
, there exists
0 ≤ t1 < t2 such that x := π(t1) and y := π(t2) satisﬁes
y ∈ ∂Ai and

i

inf
x(cid:48)∈B(x,R)

f (x(cid:48)) ≥ sup

f (x(cid:48)) + ρ.

x(cid:48)∈B(y,R)

Thus, there exists indices p, q ∈ {0, ..., L − 1} such that
p ≤ q, |xp − x| ≤ R, and |xq − y| ≤ R. Thus, we have
f (xp) ≥ f (xq) + ρ, but fk(xp) ≤ fk(xq). However, we
have

fk(xp) ≥ f (xp) − ρ/3 ≥ f (xq) + 2ρ/3
≥ fk(xq) + ρ/3 > fk(xq),

a contradiction, as desired.

5. Simulations

Figure 3 provides simple veriﬁcation that Quickshift++
provides reasonable clusterings in a wide variety of situ-
ations where other density-based procedures are known to
fail. For instance, in the two rings dataset (ﬁrst row), we

Figure 3. Comparison against other clustering algorithms on toy
datasets, adapted from scikit-learn cluster demo. Quickshift++
settings were ﬁxed at k = 20, β = 0.7 for all the datasets, while
the other algorithms were tuned to obtain a reasonable number of
clusters.

see that Mean Shift and Quick Shift suffer from the over-
segmentation issue coupled with the oversized bandwidth
which causes them to recover clusters that have points from
both the rings even though the rings are separated. In the
three Gaussians dataset (third row), we see that DBSCAN
fails because the three clusters are of different density levels
and thus no matter which density-level we set, DBSCAN
will not be able to recover the three clusters.

6. Image Segmentation

In order to apply clustering to image segmentation, we use
the following standard approach (see e.g. Felzenszwalb
& Huttenlocher (2004)): we transform each pixel into a
5-dimensional vector where two coordinates correspond to
the location of the pixel and three correspond to each of
the RGB color channels. Then segmentation is done by
clustering this 5-dimensional dataset.

We observed that for Quickshift++, setting β = 0.9 is rea-
sonable across a wide range of images, β was ﬁxed to this
value for segmentation here. We compare Quickshift++ to
Quick Shift, as the latter is often used for segmentation.
Quick Shift often over-segments in some areas and under-
segments in other areas under any hyperparameter setting
and we showed the settings which provided a reasonable
trade-off. On the other hand Quickshift++ gives us reason-

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

able segmentations in many cases and can capture segments
that may be problematic for other procedures.

As shown in the ﬁgures, it moreover has the interesting prop-
erty of being able to recover segments of widely varying
shapes and sizes in the same image, which suggests that
modelling the dense regions of the segments as cluster-cores
instead of point-modes may be useful as we compare to
Quick Shift. Although this is only qualitative, it further sug-
gests that Quickshift++ is a versatile algorithm and begins
to show its potential application in many more areas.

Figure 6. Assorted fruit in a metal bowl. For Quick Shift, band-
width was set to 8 and for Quickshift++, k = 100 and β = 0.9.
Quickshift++ is able to segment most of the fruits in the bowl,
while recovering the details of the bowl as well as the structures in
the background.

7. Clustering Experiments

We ran Quickshift++ against other clustering algorithms
on the various real datasets and scored against the ground-
truth using the adjusted rand index and the adjusted mutual
information scores.

Dataset
(A) seeds
(B) phonemes
(C) iris
(D) banknote
(E) images
(F) letters
(G) MNIST
(H) page blocks
(I) glass

n
210
4509
150
1372
210
20000
1000
5473
214

d
7
258
4
4
19
16
784
10
19

Clusters
4
5
3
2
7
26
10
5
7

Figure 8. Summary of datasets used, including dataset size (n),
number of features (d) and number of clusters.

Datasets Used: Summary of the datasets can be found in
Figure 8. Seeds, glass, and iris are standard UCI datasets
(Lichman, 2013) used for clustering. Banknote is another
UCI dataset which involves identifying whether a banknote
is forged or not, based on various statistics of an image of
the banknote. Page Blocks is a UCI dataset which involves
determining the type of a portion of a page (e.g. text, image,
etc) based on various statistics of an image of the portion.
Phonemes (Friedman et al., 2001) is a dataset which involves
the log periodograms of spoken phonemes. Images is a UCI
dataset called Statlog, based on features extracted from
various images, and letters is the UCI letter recognition
dataset. We also used a small subset of MNIST (LeCun
et al., 2010) for our experiments.

Figure 4. Figure skater Yuzuru Hanyu performs at the 2018 Winter
Olympics. Quick Shift was set with bandwidth 10 and Quick-
shift++ was set with k = 300 and β = 0.9. We see that when
compared to Quick Shift, Quickshift++ is able to recover the varia-
tions in the background more accurately, including correctly seg-
menting most of the letters on the wall, while still recovering the
structure of Hanyu’s costume accurately.

Figure 5. Yuzuru Hanyu at the 2017 Rostelecom Cup. Quick Shift
was set with bandwidth 15 and Quickshift++ was set with k =
50 and β = 0.9. Quickshift++ can recover the homogeneous
background as a whole, and reasonably separates Hanyu’s light-
colored costume from the background.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Figure 7. For each algorithm, we show clustering performance as a function of its respective hyperparameter setting. The blue line is adj.
mutual information and the red line is adj. rand index. Notice that for Quickshift++, we show a wide range of k (relative to n), while for
the popular procedures, their respective parameters had to be carefully tuned to ﬁnd the region where the scores are non-trivial.

B

C

KMns DScn MCrs MSft QSft
.7327
.6715
.6872
.6360
.7165
.7361
.7149
.7479
.7261
.6203
.5836
.7265
.3318
.5145
.3397
.3857
.5008
.4077
.5814
.5364
.1793
.1284
.4940
.4217
.2503
.2584
.2911
.3483
.3251
.0925
.1363
.0397
.2929
.2647
.4195
.3523

.7319
.6769
.5974
.5700
.7028
.6106
.2434
.2351
.3497
.4656
.1287
.3027
.2281
.1958
.0028
.0526
.2790
.3858

.4473
.4429
.4458
.5731
.5898
.5865
.5584
.4594
.3313
.5264
.0705
.4422
.1070
.2164
.1962
.1179
.2844
.3542

A .7092
.6738
.7432
.7574
.7294
.7418
D .2893
.2690
.4177
.5497
.1384
.3741
G .3320
.4629
H .0830
.0524
.2770
.3865

E

F

I

QS++
.7261
.7085
.7530
.7870
.7399
.7424
.6152
.4866
.5359
.6456
.1766
.5001
.3606
.4806
.4727
.2192
.2849
.4250

Figure 9. For each dataset, the ﬁrst row is the adjusted rand in-
dex scores and the second row is the adjusted mutual informa-
tion scores. Bolded are highest and second highest scores. For
MCores and Quickshift++, we used a single β = 0.3 for each
dataset with the exception of for banknote where β = 0.7. Then
the procedures were tuned in their respective essential hyperpa-
rameter: k-means (KMns) number of clusters, DBSCAN (DScn)
epsilon, MCores (MCrs) k from k-NN, mean shift (MSft) band-
width, quick shift (QSft) bandwidth, Quickshift++ (QS++) k.

We evaluate performance under the Adjusted Mutual Infor-
mation and Rand Index scores (Vinh et al., 2010) which are
metrics to compare clusterings. Not only do we show that
Quickshift++ considerably outperforms the popular density-
based clustering procedures under optimal tuning (Figure 9),
but that it is also robust in its hyperparameter k (Figure 7),
all while ﬁxing β = 0.3 for all but one of the datasets. Such
robustness to its tuning parameters is highly desirable since
optimal tuning is usually not available in practice.

8. Conclusion

We presented Quickshift++, a new density-based clustering
procedure that ﬁrst estimates the cluster-cores of the density,
which are locally high-density regions. Then remaining
points are assigned to its appropriate cluster-core using a
hill-climbing procedure based on Quick Shift. Such cluster-
cores turn out to be more stable and expressive representa-
tions of the possibly complex clusters than point-modes. As
a result, Quickshift++ enjoys the advantages of the popular
density-based clustering algorithms while avoiding many
of their respective weaknesses. We then gave guarantees
for cluster recovery. Finally, we showed that the algorithm
has strong and robust performance on real datasets and has
promising applications to image segmentation.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Acknowledgements

We thank the anonymous reviewers for their helpful feed-
back.

References

Arias-Castro, Ery, Mason, David, and Pelletier, Bruno. On
the estimation of the gradient lines of a density and the
consistency of the mean-shift algorithm. Journal of Ma-
chine Learning Research, 17(43):1–28, 2016.

Balakrishnan, Sivaraman, Narayanan, Srivatsan, Rinaldo,
Alessandro, Singh, Aarti, and Wasserman, Larry. Cluster
trees on manifolds. In Advances in Neural Information
Processing Systems, pp. 2679–2687, 2013.

Cadre, Benoıt. Kernel estimation of density level sets. Jour-
nal of multivariate analysis, 97(4):999–1023, 2006.

Chac´on, Jos´e E. Clusters and water ﬂows: a novel approach
to modal clustering through morse theory. arXiv preprint
arXiv:1212.1384, 2012.

Chaudhuri, Kamalika and Dasgupta, Sanjoy. Rates of con-
In Advances in Neural

vergence for the cluster tree.
Information Processing Systems, pp. 343–351, 2010.

Chaudhuri, Kamalika, Dasgupta, Sanjoy, Kpotufe, Samory,
and von Luxburg, Ulrike. Consistent procedures for clus-
ter tree estimation and pruning. IEEE Transactions on
Information Theory, 60(12):7900–7912, 2014.

Chen, Yen-Chi, Genovese, Christopher R, Wasserman,
Larry, et al. A comprehensive approach to mode clus-
tering. Electronic Journal of Statistics, 10(1):210–241,
2016.

Chen, Yen-Chi, Genovese, Christopher R, and Wasserman,
Larry. Density level sets: Asymptotics, inference, and
visualization. Journal of the American Statistical Associ-
ation, pp. 1–13, 2017.

Cheng, Yizong. Mean shift, mode seeking, and cluster-
ing. IEEE transactions on pattern analysis and machine
intelligence, 17(8):790–799, 1995.

Comaniciu, Dorin and Meer, Peter. Mean shift: A robust
approach toward feature space analysis. IEEE Transac-
tions on pattern analysis and machine intelligence, 24(5):
603–619, 2002.

Cormen, Thomas H. Introduction to algorithms. MIT press,

2009.

Ester, Martin, Kriegel, Hans-Peter, Sander, J¨org, Xu, Xi-
aowei, et al. A density-based algorithm for discovering
clusters in large spatial databases with noise. In Kdd,
volume 96, pp. 226–231, 1996.

Felzenszwalb, Pedro F and Huttenlocher, Daniel P. Efﬁcient
graph-based image segmentation. International journal
of computer vision, 59(2):167–181, 2004.

Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
The elements of statistical learning, volume 1. Springer
series in statistics New York, 2001.

Genovese, Christopher R, Perone-Paciﬁco, Marco,
Verdinelli, Isabella, and Wasserman, Larry. Non-
Journal of
parametric inference for density modes.
the Royal Statistical Society: Series B (Statistical
Methodology), 78(1):99–126, 2016.

Hartigan, John A. Clustering algorithms, volume 209. Wiley

New York, 1975.

Jiang, Heinrich. Density level set estimation on manifolds
with dbscan. arXiv preprint arXiv:1703.03503, 2017a.

Jiang, Heinrich. On the consistency of quick shift. In Neural

Information Processing Systems (NIPS), 2017b.

Jiang, Heinrich and Kpotufe, Samory. Modal-set estimation
with an application to clustering. In Artiﬁcial Intelligence
and Statistics, pp. 1197–1206, 2017.

Kpotufe, Samory and von Luxburg, Ulrike. Pruning nearest
neighbor cluster trees. arXiv preprint arXiv:1105.0540,
2011.

LeCun, Yann, Cortes, Corinna, and Burges, CJ. Mnist hand-
written digit database. AT&T Labs [Online]. Available:
http://yann. lecun. com/exdb/mnist, 2, 2010.

Li, Jia, Ray, Surajit, and Lindsay, Bruce G. A nonparametric
statistical approach to clustering via mode identiﬁcation.
Journal of Machine Learning Research, 8(Aug):1687–
1723, 2007.

Lichman, M. UCI machine learning repository, 2013. URL

http://archive.ics.uci.edu/ml.

Rigollet, Philippe, Vert, R´egis, et al. Optimal rates for
plug-in estimators of density level sets. Bernoulli, 15(4):
1154–1178, 2009.

Rinaldo, Alessandro and Wasserman, Larry. Generalized
density clustering. The Annals of Statistics, pp. 2678–
2722, 2010.

Dasgupta, Sanjoy and Kpotufe, Samory. Optimal rates for
k-nn density and mode estimation. In Advances in Neural
Information Processing Systems, pp. 2555–2563, 2014.

Rodriguez, Alex and Laio, Alessandro. Clustering by fast
search and ﬁnd of density peaks. Science, 344(6191):
1492–1496, 2014.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Singh, Aarti, Scott, Clayton, Nowak, Robert, et al. Adaptive
hausdorff estimation of density level sets. The Annals of
Statistics, 37(5B):2760–2782, 2009.

Sriperumbudur, Bharath and Steinwart, Ingo. Consistency
and rates for clustering with dbscan. In Artiﬁcial Intelli-
gence and Statistics, pp. 1090–1098, 2012.

Steinwart, Ingo. Adaptive density level set clustering. In
Proceedings of the 24th Annual Conference on Learning
Theory, pp. 703–738, 2011.

Tsybakov, Alexandre B et al. On nonparametric estimation
of density level sets. The Annals of Statistics, 25(3):
948–969, 1997.

Van Der Vaart, Aad and Wellner, Jon A. A note on bounds
for vc dimensions. Institute of Mathematical Statistics
collections, 5:103, 2009.

Vedaldi, Andrea and Soatto, Stefano. Quick shift and kernel
methods for mode seeking. Computer vision–ECCV 2008,
pp. 705–718, 2008.

Verdinelli,

Isabella and Wasserman, Larry.

sis of a mode clustering diagram.
arXiv:1805.04187, 2018.

Analy-
arXiv preprint

Vinh, Nguyen Xuan, Epps, Julien, and Bailey, James. In-
formation theoretic measures for clusterings compari-
son: Variants, properties, normalization and correction
for chance. Journal of Machine Learning Research, 11
(Oct):2837–2854, 2010.

Wasserman, Larry, Azizyan, Martin, and Singh, Aarti. Fea-
ture selection for high-dimensional clustering. arXiv
preprint arXiv:1406.2240, 2014.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Heinrich Jiang 1 Jennifer Jang 2 Samory Kpotufe 3

8
1
0
2
 
y
a
M
 
1
2
 
 
]

G
L
.
s
c
[
 
 
1
v
9
0
9
7
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

We provide initial seedings to the Quick Shift clus-
tering algorithm, which approximate the locally
high-density regions of the data. Such seedings
act as more stable and expressive cluster-cores
than the singleton modes found by Quick Shift.
We establish statistical consistency guarantees for
this modiﬁcation. We then show strong clustering
performance on real datasets as well as promising
applications to image segmentation.

1. Introduction

Quick Shift (Vedaldi & Soatto, 2008) is a mode-seeking
based clustering algorithm that has a growing popularity in
computer vision. It proceeds by repeatedly moving each
sample to its closest sample point that has higher empirical
density if one exists within a τ -radius ball, otherwise we
stop. Thus each path ends at a point which can be viewed
as a local mode of the empirical density. Then, points that
end up at the same mode are assigned to the same cluster.
The most popular choice of empirical density function is
the Kernel Density Estimator (KDE) with Gaussian Kernel.
The algorithm also appears in Rodriguez & Laio (2014).

Quick Shift was designed as a faster alternative to the well-
known Mean Shift algorithm (Cheng, 1995; Comaniciu &
Meer, 2002). Mean Shift is equivalent to performing a
gradient ascent of the KDE starting at each sample until
convergence (Arias-Castro et al., 2016). Samples that cor-
respond to the same points of convergence are in the same
cluster and the points of convergence are taken to be the
estimates of the modes. Thus, both procedures hill-climb
to the local modes of the empirical density function and
cluster based on these modes. The key differences are that
Quick Shift restricts the steps to sample points (and thus is
a sample-based version of Mean Shift) and has the extra τ
parameter which allows it to merge close segments together.

1Google Research, Mountain View, CA 2Uber Inc, San Fran-
cisco, CA 3Princeton University, Princeton, NJ. Correspondence
to: Heinrich Jiang <heinrich.jiang@gmail.com>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

One of the drawbacks of these two procedures, as well as
many mode-seeking based clustering algorithms, is that the
point-modes of the density functions are often poor rep-
resentations of the clusters. This will happen when the
high-density regions within a cluster are of arbitrary shape
and have some variations causing the underlying density
function to have possibly many apparent, but not so salient
modes. In this case, such procedures asymptotically recover
all of the modes separately, leading to over-segmentation.
To combat this effect, practitioners often increase the kernel
bandwidth, which makes the density estimate more smooth.
However, this can cause the density estimate to deviate too
far from the original density we are intending to cluster
based on.1 Thus, practitioners may not wish to identify
the clusters based on the point-modes of the density func-
tion, but rather identify them based on locally high density
regions of the dataset (See Figure 1).2

We propose modeling these locally high-density regions as
cluster-cores (to be precisely deﬁned later), which can be of
arbitrary shape, size, and density level, and are thus better
suited at capturing the possibly complex topological prop-
erties of clusters that can arise in practice. In other words,
these cluster-cores are better at expressing the clusters and
are more stable as they are less sensitive to the small ﬂuctu-
ations that can arise in the empirical density function. We
parameterize the cluster-core by β where 0 < β < 1, which
determines how much the density is allowed to vary within
the cluster-core. We estimate them from a ﬁnite sample us-
ing a minor modiﬁcation of the MCores algorithm of Jiang
& Kpotufe (2017).

We introduce Quickshift++, which ﬁrst estimates these
cluster-cores, and then runs the Quick Shift based hill-
climbing procedure on each remaining sample until it
reaches a cluster-core.
Samples that end up in the
same cluster-core are assigned to the same cluster; thus,

1KDE with Gaussian kernel and bandwidth h approximates the
underlying density convolved with a Gaussian with mean 0 and
covariance h2I. Thus, the higher h is, the more the KDE deviates
from the original density.

2Over-segmentation is also dealt with in Quick Shift via the
τ parameter, but a threshold for the distance between two modes
which should be clustered together is hard to determine in practice.
Moreover, there may not even be a good setting of τ which works
everywhere in the input space.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

the cluster-cores can be seen as representing the high-
conﬁdence regions within each cluster. We utilize the k-NN
density estimator as our empirical density.

Despite the simplicity of our approach, we show that Quick-
shift++ considerably outperforms the popular density-based
clustering algorithms, while being efﬁcient. Another desir-
able property of Quickshift++ is that it is simple to tune its
two hyperparameters β and k.3 We show that a few settings
of β turn out to work for a wide range of applications and
that the procedure is stable in choices of k.

We then give a novel statistical consistency analysis for
Quickshift++ which provides guarantees that points within
a cluster-core’s attraction regions (to be described later)
are correctly assigned. We also show promising results on
image segmentation, which further validates the desirability
of using cluster-cores on real-data applications.

Figure 1. It can often be the case that the locally high-density
regions are of arbitrary shape and ﬂuctuations within them lead to
many apparent modes. Left: Mode-seeking clustering procedures
often lead to over-segmentation. Right: It may be more desirable
to use cluster-cores (shaded), which allows ﬂuctuations within
arbitrarily-shaped regions of locally high density.

2. Related Works and Contributions

We show that Quickshift++ is a new and powerful addition
to the family of clustering procedures known as density-
based clustering, which most notably includes DBSCAN
(Ester et al., 1996) and Mean Shift (Cheng, 1995). Such
procedures operate on the estimated density function based
on a ﬁnite sample to recover structures in the density func-
tion that ultimately correspond to the clusters. There are
several advantages of density-based clustering over classical
objective-based procedures such as k-means and spectral
clustering. Density-based procedures can automatically
detect the number of clusters, while objective-based pro-
cedures typically require this as an input. Density-based
clustering algorithms also make little assumptions on the
shapes of the clusters as well as their relative positions.

Density-based clustering procedures can roughly be classi-
ﬁed into two categories: hill-climbing based approaches
(discussed earlier, which includes both Mean Shift and

3The τ parameter from Quick Shift is unnecessary here because
we climb until we reach a cluster-core as our stopping condition.

Quick Shift) and density-level set based approaches. We
now discuss the latter approach, which takes the connected
components of the density-level set deﬁned by {x : f (x) ≥
λ} for some density level λ as the clusters. This statistical
notion of clustering traces back to Hartigan (1975). Since
then, there has been extensive work done, e.g. Tsybakov
et al. (1997); Cadre (2006); Rigollet et al. (2009); Singh et al.
(2009); Chaudhuri & Dasgupta (2010); Rinaldo & Wasser-
man (2010); Kpotufe & von Luxburg (2011); Balakrishnan
et al. (2013); Chaudhuri et al. (2014); Chen et al. (2017).
More recently, Sriperumbudur & Steinwart (2012); Jiang
(2017a) show that the popular DBSCAN algorithm turns
out to converge to these clusters. However, one of the main
drawbacks of this approach is that the density-level λ is
ﬁxed and thus such methods perform poorly when the clus-
ters are at different density-levels. Moreover, the question
of how to choose λ remains (e.g. Steinwart (2011)).

Jiang & Kpotufe (2017) provide an alternative notion of
clusters, called modal-sets, which are regions of ﬂat den-
sity which are local maximas of the density. They can be
of arbitrary shape, dimension, or density. They provide a
procedure, MCores, which estimates these with consistency
guarantees. Our notion of cluster-core is similar to modal-
sets, but the density within a cluster-core is allowed to vary
by a substantial amount in order to capture such variations
seen in real data as a the ﬂat density of modal-sets may be
too restrictive in practice. It turns out that a small modiﬁ-
cation of MCores allows us to estimate these cluster-cores.
Thus Quickshift++ has the advantage over DBSCAN in that
clusters can be at any density level and that furthermore, the
density levels are chosen adaptively.

Mcores however consists of an over-simplistic ﬁnal cluster-
ing: it simply assigns each point to its closest modal-set,
while in practice, clusters tend not to follow the geome-
try induced by the Euclidean metric. Quickshift++ on the
other hand clusters the remaining points by a hill-climbing
method which we show is far better in practice.

Thus, Quickshift++ combines the strengths of both density-
based clustering approaches while avoiding many of their
weaknesses. In addition to the general advantages of density-
based clustering algorithms shared by both approaches, it
is also able to both (1) recover clusters at varying density
levels and (2) not suffer from the over-segmentation issue
described in Figure 1. To our knowledge, no other procedure
has been shown to have this property.

For our theoretical analysis, we give guarantees about Quick-
shift++’s ability to recover the clusters based on attraction
regions deﬁned by the gradient ﬂows. Wasserman et al.
(2014); Arias-Castro et al. (2016) showed that Mean Shift’s
iterates approximate the gradient ﬂows. Some progress
has been made in understanding Quick Shift (Jiang, 2017b;
Verdinelli & Wasserman, 2018). There are also related lines

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

of work in mode clustering e.g. (Li et al., 2007; Chac´on,
2012; Genovese et al., 2016; Chen et al., 2016). In this
paper, we show that Quickshift++ recovers the interior of its
attraction region, thus adding to our statistical understanding
of hill-climbing based clustering procedures.

a ﬁxed additive ﬂuctuation. It uses the levels of the mutual
k-NN graph of the sample points, deﬁned below.
Deﬁnition 3. Let G(λ) denote the λ-level of the mutual
k-NN graph with vertices {x ∈ X[n] : fk(x) ≥ λ} and an
edge between x and x(cid:48) iff ||x − x(cid:48)|| ≤ min{rk(x), rk(x(cid:48))}.

3. Algorithm

3.1. Basic Deﬁnitions

Let X[n] = {x1, ..., xn} be n i.i.d. samples drawn from an
unknown density f , deﬁned over the Lebesgue measure on
Rd. Suppose that f has compact support X .

Our procedure will operate on the k-NN density estimator:

Deﬁnition 1. Let rk(x) := inf{r > 0 : |B(x, r) ∩ X[n]| ≥
k}, i.e., the distance from x to its k-th nearest neighbor.
Deﬁne the k-NN density estimator as

fk(x) :=

k
n · vd · rk(x)d ,

where vd is the volume of a unit ball in Rd.

3.2. Cluster-Cores

We deﬁne the cluster core with respect to ﬁxed ﬂuctuation
parameter β as follows.

Deﬁnition 2. Let 0 < β < 1. Closed and connected set
M ⊂ X is a cluster-core if M is a connected component
(CC) of {x ∈ X : f (x) ≥ (1 − β) · maxx(cid:48)∈M f (x(cid:48))}.

Note that when β → 0, then the cluster-cores become the
modes or local-maximas of f . When β → 1, then the
cluster-core becomes the entire support X . We next give a
very basic fact about cluster-cores, that they do not overlap.

Lemma 1. Suppose that M1, M2 are distinct cluster-cores
of f . Then M1 ∩ M2 = ∅.

Proof. Suppose otherwise. We have that M1 and M2 are
CCs of {x ∈ X : f (x) ≥ λ1} and {x ∈ X : f (x) ≥ λ2},
respectively for some λ1, λ2. Clearly, if λ1 = λ2, then it
follows that M1 = M2. Then, without loss of generality, let
λ1 < λ2. Then since the CCs of {x ∈ X : f (x) ≥ λ2} are
nested in the CCs of {x ∈ X : f (x) ≥ λ1}, then it follows
that M2 ⊆ M1. Then, λ2 = (1 − β) supx∈M2 f (x) ≤ (1 −
β) supx∈M1 f (x) = λ1, a contradiction. As desired.

Algorithm 1 is a simple modiﬁcation of MCores by Jiang &
Kpotufe (2017). The difference is that we use a multiplica-
tive ﬂuctuation parameter β, while Jiang & Kpotufe (2017)
uses an additive one. The latter requires knowledge of the
scale of the density function, which is difﬁcult to determine
in practice. Moreover, the multiplicative ﬂuctuation adapts
to clusters at varying density levels more reasonably than

It is known that G(λ) approximates the CCs of the λ-level
sets of the true density, deﬁned as {x ∈ X : f (x) ≥ λ}
see e.g. (Chaudhuri & Dasgupta, 2010). Moreover, it can
be seen that the CCs of G(λ) forms a hierarchical nesting
structure as λ decreases.

Algorithm 1 proceeds by performing a top-down sweep of
the levels of the mutual k-NN graph, G(λ). As λ decreases,
it is clear that more nodes appear and that connectivity
increases. In other words, as we scan top-down, the CCs of
G(λ) become larger, some CCs can merge, or new CCs can
appear. When a new CC appears at level λ, then intuitively,
it should correspond to a local maxima of f , which appears
at a density level approximately λ. This follows from the
fact that the CCs of G(λ) approximates the CCs of {x ∈
X : f (x) ≥ λ}. Thus, the idea is that when a new CC
appears in G(λ), then we can take the corresponding CC in
G(λ − βλ) (which is the density level (1 − β) times that of
the highest point in the CC) to estimate the cluster-core.

Algorithm 1 MCores (estimating cluster-cores)

Parameters k, β
Initialize (cid:99)M := ∅.
Sort the xi’s in decreasing order of fk values (i.e.
fk(xi) ≥ fk(xi+1)).
for i = 1 to n do

Deﬁne λ := fk(xi).
Let A be the CC of G(λ − βλ) containing xi.
if A is disjoint from all cluster-cores in (cid:99)M then

Add A to (cid:99)M.

end if
end for
return (cid:99)M.

Algorithm 2 Quickshift++

Let (cid:99)M be the cluster-cores obtained by running Algo-
rithm 1.
Initialize directed graph G with vertices {x1, ..., xn} and
no edges.
for i = 1 to n do

If xi is not in any cluster-core, then add to G an edge
from xi to its closest sample x ∈ X[n] such that
fk(x) > fk(xi).

end for
For each cluster-core M ∈ (cid:99)M, let (cid:98)CM be the points
x ∈ X[n] such that the directed path in G starting at x
ends in M .
return { (cid:98)CM : M ∈ (cid:99)M}.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

3.3. Quickshift++

Quickshift++ (Algorithm 2) proceeds by ﬁrst running Algo-
rithm 1 to obtain the cluster-cores, and then moving each
sample point to its nearest neighbor that has higher k-NN
density until it reaches some cluster-core. All samples that
end up in the same cluster-core after the hill-climbing are
assigned to the same cluster. Note that since the highest
empirical density sample point is contained in a cluster-core,
it follows that each sample point not in a cluster-core will
eventually be assigned to a unique cluster-core. Thus, Quick-
shift++ provides a clustering assignment of every sample
point.
Remark 1. Although it seems a similar procedure could
have been constructed by using Mean Shift in place of Quick
Shift, Mean Shift could have convergence outside of the
estimated cluster-cores, while Quick Shift guarantees that
each sample outside of a cluster-core get assigned to some
cluster-core.

3.4. Implementation

The implementation details for the MCores modiﬁcation
can be inferred from Jiang & Kpotufe (2017). This step
runs in O(nk · α(n)) where α is the Inverse Ackermann
function (Cormen, 2009), in addition to the time it takes to
compute the k-NN sets for the n sample points. To cluster
the remaining points, for each sample not in a cluster-core,
we must ﬁnd its nearest sample of higher k-NN density. Al-
though this is worst-case O(n) time for each sample point,
fortunately we see that in practice (as long as k is not too
small) for the vast majority of cases, the nearest sample
with higher density is within the k-nearest neighbor set
so it only takes O(k) in most cases. It is an open prob-
lem whether there the nearest sample with higher empir-
ical density is often in its k-NN set. Code release is at
https://github.com/google/quickshift.

4. Theoretical Analysis

For the theoretical analysis, we make ﬁrst the following
regularity assumption, that the density is continuously dif-
ferentiable and lower bounded on X .
Assumption 1. f is continuously differentiable on X and
there exists λ0 > 0 such that inf x∈X f (x) ≥ λ0.

Let M1, ..., MC be the cluster-cores of f . Then we can
deﬁne the following notion of attraction region for each
cluster-core based on the gradient ascent curve or ﬂow. This
is similar to notions of attraction regions for some previous
analyses of mode-based clustering, such as Wasserman et al.
(2014); Arias-Castro et al. (2016), where the intuition is
that attraction regions are deﬁned based by following the
direction of the gradient of the underlying density. In our
situation, instead of an attraction region deﬁned as all points

which ﬂow towards a particular point-mode, the attraction
region is deﬁned around a cluster-core.
Deﬁnition 4 (Attraction Regions). Let path πx : R → Rd
satisfy πx(0) = x, π(cid:48)
x(t) = ∇f (πx(t)). For cluster-core
Mi, its attraction region Ai is the set of points x ∈ X that
satisfy limt→∞ πx(t) ∈ Mi.

It is clear that these attraction regions are well-deﬁned. The
ﬂow path is well-deﬁned since the density is differentiable
and since each cluster-core is deﬁned as a CC of a level
set, the density must decay around its boundaries. In other
words, once an ascent path reaches a cluster-core, it cannot
leave the cluster-core.

However, it is in general not the case that the space can be
partitioned into attraction regions. For example, if a ﬂow
reaches a saddle point, it will get stuck there and thus any
point whose ﬂow ends up at a saddle point will not belong to
any attraction region. In this paper, we only give guarantees
about the clustering of points which are in an attraction
region.

The next regularity assumption we make is that the cluster-
cores are on the interior of the attraction region (to avoid
situations such as when the cluster-cores intersect with the
boundary of the input space).
Assumption 2. There exists R0 > 0 such that Mi +
B(0, R0) ⊆ Ai for i = 1, ..., C, where M + B(0, r) de-
notes {x : inf y∈M ||x − y|| ≤ r}.
Deﬁnition 5 (Level Set). The λ level set of f is deﬁned as
Lf (λ) := {x ∈ X : f (x) ≥ λ}.

The next assumption says that the level sets are continuous
w.r.t. the level in the following sense where we denote the
(cid:15)-interior of A as A(cid:9)(cid:15) := {x ∈ A, inf y∈∂A ||x − y|| ≥ (cid:15)}
(∂A is the boundary of A):
Assumption 3 (Uniform Continuity of Level Sets). For
each (cid:15) > 0, there exists δ > 0 such that for 0 < λ ≤ λ(cid:48) ≤
||f ||∞ with |λ − λ(cid:48)| < δ, then Lf (λ)(cid:9)(cid:15) ⊆ Lf (λ(cid:48)).

This ensures that there are no approximately ﬂat areas in
which the procedure may get stuck at. The assumption is
borrowed from (Jiang, 2017b). Finally, we need the fol-
lowing regularity condition which ensures that level sets
away from cluster-cores do not get arbitrarily thin. This is
adapted from standard analyses of level-set estimation (e.g.
Assumption B of Singh et al. (2009)).
Assumption 4. Let µ denote the Lebesgue measure on Rd.
For any r > 0, there exists σ > 0 such that the following
holds for any connected component A of any level-set of f
which is not contained in Mi for any i: µ(B(x, r) ∩ A) ≥ σ
for all x ∈ A.

For our consistency results, we prove that Quickshift++ can
cluster the sample points in the (R, ρ)-interior of an attrac-

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

tion region (deﬁned below) for each cluster-core properly
where R, ρ > 0 are ﬁxed and can be chosen arbitrarily
small.
Deﬁnition 6 ((R, ρ)-interior of Attraction Regions). Deﬁne
the (R, ρ)-interior of Ai, denoted as A(R,ρ)
, as the set of
points x0 ∈ Ai such that each path P from x0 to any point
y ∈ ∂Ai satisﬁes the following.

i

sup
x∈P

inf
x(cid:48)∈B(x,R)

f (x(cid:48)) ≥ sup

f (x(cid:48)) + ρ.

x(cid:48)∈B(y,R)

In other words, points in the interior satisfy the property
that any path leaving its attraction region must sufﬁciently
decrease in density at some point. This decrease threshold
is parameterized by R and ρ.

Figure 2. Illustration of interior of attraction region in 1-dimension.
The pink and green shaded regions indicated the two attraction
regions. The striped parts show the corresponding interiors of the
attraction regions.

We ﬁrst give a guarantee on the ﬁrst step of MCores recov-
ers, that the cluster-cores are reasonably recovered. The
proof follows from the analysis of Jiang & Kpotufe (2017)
by replacing modal-sets with cluster-cores, and the results
match up to constant factors. The proof is omitted here.

Theorem 1. [Adapted from Theorem 3, 4 of Jiang &
Kpotufe (2017)] Suppose that Assumptions 1, 3, and 4 hold.
Let 0 < β < 1, (cid:15), δ > 0 and suppose that k ≡ k(n) is
chosen such that log2 n/k → 0 and n4/(4+d)/k → 0. Let
M1, ..., MC be the cluster-cores of f . Then for n sufﬁciently
large depending on f , δ, (cid:15), and β, with probability at least
1 − δ, MCores returns C cluster-core estimates (cid:99)M1, ..., (cid:100)MC
such that Mi ∩ X[n] ⊆ (cid:99)Mi ⊆ Mi + B(0, (cid:15)) for i ∈ 1, ..., C.
Remark 2. The original result from Jiang & Kpotufe (2017)
is about (cid:15)-approximate modal-set which are deﬁned as level-
sets whose density has range (cid:15). Our notion of cluster-core
is similar, but the range is a β-proportion of the highest
density level within the level-set. Using a proportion is
more interpretable and thus more useful, as the scale of the
density function is difﬁcult to determine in practice.

We now state the main result, which says that as long as the
cluster-cores are sufﬁciently well estimated (up to a certain
Hausdorff error) by MCores (via previous theorem), then
Quickshift++ will correctly cluster the (R, ρ)-interiors of
the attraction regions with high probability.

Theorem 2. Suppose that Assumptions 1, 2, 3, and 4 hold.
Let 0 < R < R0 and ρ, δ > 0. Suppose that k ≡ k(n)
is chosen such that log2 n/k → 0 and n4/(4+d)/k → 0.
Suppose that (cid:99)M1, ..., (cid:99)MC are the cluster-cores returned by
Algorithm 1 and satisfy Mi ∩X[n] ⊆ (cid:99)Mi ⊆ Mi +B(0, R/4)
for i = 1, ..., C. Then for n sufﬁciently large depending
on f , ρ, δ and R, the following holds with probably at
least 1 − 2δ uniformly in x ∈ A(R,ρ)
∩ X[n] and i ∈ [C]:
Quickshift++ clusters x to the cluster corresponding to Mi.

i

4.1. Proof of Theorem 2

We require the following uniform bound on k-NN density
estimator, which follows from Dasgupta & Kpotufe (2014).

Lemma 2. Let δ > 0. Suppose that f is Lipschitz continu-
ous with compact support X (e.g. there exists L such that
|f (x)−f (x(cid:48))| ≤ L|x−x(cid:48)| for all x, x(cid:48) ∈ X ) and f satsiﬁes
Assumption 1. Then exists constant C depending on f such
that the following holds if n ≥ C 2
δ,n with probability at least
1 − δ.

|fk(x) − f (x)| ≤ C

sup
x∈X

(cid:32)

Cδ,n√
k

+

(cid:18) k
n

(cid:19)1/d(cid:33)

.

where Cδ,n := 16 log(2/δ)

d log n.

√

We next need the following uniform concentration bound
on balls intersected with level-sets, which says that if such
a set has large enough probability mass, then it will contain
a sample point with high probability.
Lemma 3. Let E := {B(x, r) ∩ Lf (λ) : x ∈ Rd, r >
0, λ > 0}. Then the following holds with probability at
least 1 − δ uniformly for all E ∈ E

F(E) ≥ Cδ,n

⇒ E ∩ Xn (cid:54)= ∅.

√

d log n
n

Proof. The indicator functions 1[B(x, f ) ∩ Lf (λ)] for x ∈
Rd, λ > 0 have VC-dimension d + 1. This is because the
balls over Rd have VC-dimension d + 1 and the level-sets
Lf (λ) has VC-dimension 1 and thus their intersection has
VC-dimension d + 1 (Van Der Vaart & Wellner, 2009). The
result follows by applying Theorem 15 of Chaudhuri &
Dasgupta (2010).

In other words, with high probability, MCores estimates
each cluster-core bijectively and that for each cluster-core,
MCores’ estimate contains all of the sample points and that
the estimate does not over-estimate by much.

Proof of Theorem 2. Suppose that x0 ∈ A(R,ρ)
∩ X[n] and
Quickshift++ gives directed path x0 → x1 → · · · → xL
where x1, ..., xL−1 are outside of cluster-cores and xL is in
a cluster-core but xL (cid:54)∈ Ai.

i

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

We ﬁrst show that ||xi − xi+1|| ≤ R/2 for i = 0, ..., L − 1.
By Assumption 3 and 4, we have that there exists τ > 0
and σ > 0 such that the following holds uniformly for
i = 0, ..., L − 1:

(cid:18)

(cid:18)

(cid:19)

(cid:19)

µ

B(xi, R/2) ∩ Lf (f (xi) + τ )

≥ σ.

Hence, since the density is uniformly lower bounded by λ0,
we have

F

B(xi, R/2) ∩ Lf (f (xi) + τ )

≥ σλ0.

√

Then by Lemma 3, for n sufﬁicently large such that σλ0 >
d log n
, then with probability at least 1 − δ there exists
Cδ,n
n
sample point x(cid:48)
i in B(xi, R/2) ∩ Lf (f (xi) + τ ) for i =
0, ..., L − 1.

Next, choose n sufﬁciently large such that by Lemma 2, we
have with probability at least 1 − δ that

|fk(x) − f (x)| ≤ min{τ, ρ}/3.

sup
x∈X

Thus, we have

fk(x(cid:48)

i) ≥ f (x(cid:48)

i) − τ /3 ≥ f (xi) + 2τ /3

≥ fk(xi) + τ /3 > fk(xi).

i|| ≤ R/2 and x(cid:48)

i ∈ X[n], it follows that

Moreover ||xi − x(cid:48)
||xi − xi+1|| ≤ R/2 for i = 0, ..., L − 1.
Let π : [0, 1] → Rd be the piecewise linear path deﬁned by
π(j/L) = xj for j = 0, ..., L. Let t2 = min{t ∈ [0, 1] :
π(t) ∈ ∂Ai}. Then, by deﬁnition of A(R,ρ)
, there exists
0 ≤ t1 < t2 such that x := π(t1) and y := π(t2) satisﬁes
y ∈ ∂Ai and

i

inf
x(cid:48)∈B(x,R)

f (x(cid:48)) ≥ sup

f (x(cid:48)) + ρ.

x(cid:48)∈B(y,R)

Thus, there exists indices p, q ∈ {0, ..., L − 1} such that
p ≤ q, |xp − x| ≤ R, and |xq − y| ≤ R. Thus, we have
f (xp) ≥ f (xq) + ρ, but fk(xp) ≤ fk(xq). However, we
have

fk(xp) ≥ f (xp) − ρ/3 ≥ f (xq) + 2ρ/3
≥ fk(xq) + ρ/3 > fk(xq),

a contradiction, as desired.

5. Simulations

Figure 3 provides simple veriﬁcation that Quickshift++
provides reasonable clusterings in a wide variety of situ-
ations where other density-based procedures are known to
fail. For instance, in the two rings dataset (ﬁrst row), we

Figure 3. Comparison against other clustering algorithms on toy
datasets, adapted from scikit-learn cluster demo. Quickshift++
settings were ﬁxed at k = 20, β = 0.7 for all the datasets, while
the other algorithms were tuned to obtain a reasonable number of
clusters.

see that Mean Shift and Quick Shift suffer from the over-
segmentation issue coupled with the oversized bandwidth
which causes them to recover clusters that have points from
both the rings even though the rings are separated. In the
three Gaussians dataset (third row), we see that DBSCAN
fails because the three clusters are of different density levels
and thus no matter which density-level we set, DBSCAN
will not be able to recover the three clusters.

6. Image Segmentation

In order to apply clustering to image segmentation, we use
the following standard approach (see e.g. Felzenszwalb
& Huttenlocher (2004)): we transform each pixel into a
5-dimensional vector where two coordinates correspond to
the location of the pixel and three correspond to each of
the RGB color channels. Then segmentation is done by
clustering this 5-dimensional dataset.

We observed that for Quickshift++, setting β = 0.9 is rea-
sonable across a wide range of images, β was ﬁxed to this
value for segmentation here. We compare Quickshift++ to
Quick Shift, as the latter is often used for segmentation.
Quick Shift often over-segments in some areas and under-
segments in other areas under any hyperparameter setting
and we showed the settings which provided a reasonable
trade-off. On the other hand Quickshift++ gives us reason-

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

able segmentations in many cases and can capture segments
that may be problematic for other procedures.

As shown in the ﬁgures, it moreover has the interesting prop-
erty of being able to recover segments of widely varying
shapes and sizes in the same image, which suggests that
modelling the dense regions of the segments as cluster-cores
instead of point-modes may be useful as we compare to
Quick Shift. Although this is only qualitative, it further sug-
gests that Quickshift++ is a versatile algorithm and begins
to show its potential application in many more areas.

Figure 6. Assorted fruit in a metal bowl. For Quick Shift, band-
width was set to 8 and for Quickshift++, k = 100 and β = 0.9.
Quickshift++ is able to segment most of the fruits in the bowl,
while recovering the details of the bowl as well as the structures in
the background.

7. Clustering Experiments

We ran Quickshift++ against other clustering algorithms
on the various real datasets and scored against the ground-
truth using the adjusted rand index and the adjusted mutual
information scores.

Dataset
(A) seeds
(B) phonemes
(C) iris
(D) banknote
(E) images
(F) letters
(G) MNIST
(H) page blocks
(I) glass

n
210
4509
150
1372
210
20000
1000
5473
214

d
7
258
4
4
19
16
784
10
19

Clusters
4
5
3
2
7
26
10
5
7

Figure 8. Summary of datasets used, including dataset size (n),
number of features (d) and number of clusters.

Datasets Used: Summary of the datasets can be found in
Figure 8. Seeds, glass, and iris are standard UCI datasets
(Lichman, 2013) used for clustering. Banknote is another
UCI dataset which involves identifying whether a banknote
is forged or not, based on various statistics of an image of
the banknote. Page Blocks is a UCI dataset which involves
determining the type of a portion of a page (e.g. text, image,
etc) based on various statistics of an image of the portion.
Phonemes (Friedman et al., 2001) is a dataset which involves
the log periodograms of spoken phonemes. Images is a UCI
dataset called Statlog, based on features extracted from
various images, and letters is the UCI letter recognition
dataset. We also used a small subset of MNIST (LeCun
et al., 2010) for our experiments.

Figure 4. Figure skater Yuzuru Hanyu performs at the 2018 Winter
Olympics. Quick Shift was set with bandwidth 10 and Quick-
shift++ was set with k = 300 and β = 0.9. We see that when
compared to Quick Shift, Quickshift++ is able to recover the varia-
tions in the background more accurately, including correctly seg-
menting most of the letters on the wall, while still recovering the
structure of Hanyu’s costume accurately.

Figure 5. Yuzuru Hanyu at the 2017 Rostelecom Cup. Quick Shift
was set with bandwidth 15 and Quickshift++ was set with k =
50 and β = 0.9. Quickshift++ can recover the homogeneous
background as a whole, and reasonably separates Hanyu’s light-
colored costume from the background.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Figure 7. For each algorithm, we show clustering performance as a function of its respective hyperparameter setting. The blue line is adj.
mutual information and the red line is adj. rand index. Notice that for Quickshift++, we show a wide range of k (relative to n), while for
the popular procedures, their respective parameters had to be carefully tuned to ﬁnd the region where the scores are non-trivial.

C

B

KMns DScn MCrs MSft QSft
.7327
.6715
.6872
.6360
.7165
.7361
.7149
.7479
.7261
.6203
.5836
.7265
.3318
.5145
.3397
.3857
.5008
.4077
.5814
.5364
.1793
.1284
.4940
.4217
.2503
.2584
.2911
.3483
.3251
.0925
.1363
.0397
.2929
.2647
.4195
.3523

.7319
.6769
.5974
.5700
.7028
.6106
.2434
.2351
.3497
.4656
.1287
.3027
.2281
.1958
.0028
.0526
.2790
.3858

.4473
.4429
.4458
.5731
.5898
.5865
.5584
.4594
.3313
.5264
.0705
.4422
.1070
.2164
.1962
.1179
.2844
.3542

A .7092
.6738
.7432
.7574
.7294
.7418
D .2893
.2690
.4177
.5497
.1384
.3741
G .3320
.4629
H .0830
.0524
.2770
.3865

E

F

I

QS++
.7261
.7085
.7530
.7870
.7399
.7424
.6152
.4866
.5359
.6456
.1766
.5001
.3606
.4806
.4727
.2192
.2849
.4250

Figure 9. For each dataset, the ﬁrst row is the adjusted rand in-
dex scores and the second row is the adjusted mutual informa-
tion scores. Bolded are highest and second highest scores. For
MCores and Quickshift++, we used a single β = 0.3 for each
dataset with the exception of for banknote where β = 0.7. Then
the procedures were tuned in their respective essential hyperpa-
rameter: k-means (KMns) number of clusters, DBSCAN (DScn)
epsilon, MCores (MCrs) k from k-NN, mean shift (MSft) band-
width, quick shift (QSft) bandwidth, Quickshift++ (QS++) k.

We evaluate performance under the Adjusted Mutual Infor-
mation and Rand Index scores (Vinh et al., 2010) which are
metrics to compare clusterings. Not only do we show that
Quickshift++ considerably outperforms the popular density-
based clustering procedures under optimal tuning (Figure 9),
but that it is also robust in its hyperparameter k (Figure 7),
all while ﬁxing β = 0.3 for all but one of the datasets. Such
robustness to its tuning parameters is highly desirable since
optimal tuning is usually not available in practice.

8. Conclusion

We presented Quickshift++, a new density-based clustering
procedure that ﬁrst estimates the cluster-cores of the density,
which are locally high-density regions. Then remaining
points are assigned to its appropriate cluster-core using a
hill-climbing procedure based on Quick Shift. Such cluster-
cores turn out to be more stable and expressive representa-
tions of the possibly complex clusters than point-modes. As
a result, Quickshift++ enjoys the advantages of the popular
density-based clustering algorithms while avoiding many
of their respective weaknesses. We then gave guarantees
for cluster recovery. Finally, we showed that the algorithm
has strong and robust performance on real datasets and has
promising applications to image segmentation.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Acknowledgements

We thank the anonymous reviewers for their helpful feed-
back.

References

Arias-Castro, Ery, Mason, David, and Pelletier, Bruno. On
the estimation of the gradient lines of a density and the
consistency of the mean-shift algorithm. Journal of Ma-
chine Learning Research, 17(43):1–28, 2016.

Balakrishnan, Sivaraman, Narayanan, Srivatsan, Rinaldo,
Alessandro, Singh, Aarti, and Wasserman, Larry. Cluster
trees on manifolds. In Advances in Neural Information
Processing Systems, pp. 2679–2687, 2013.

Cadre, Benoıt. Kernel estimation of density level sets. Jour-
nal of multivariate analysis, 97(4):999–1023, 2006.

Chac´on, Jos´e E. Clusters and water ﬂows: a novel approach
to modal clustering through morse theory. arXiv preprint
arXiv:1212.1384, 2012.

Chaudhuri, Kamalika and Dasgupta, Sanjoy. Rates of con-
In Advances in Neural

vergence for the cluster tree.
Information Processing Systems, pp. 343–351, 2010.

Chaudhuri, Kamalika, Dasgupta, Sanjoy, Kpotufe, Samory,
and von Luxburg, Ulrike. Consistent procedures for clus-
ter tree estimation and pruning. IEEE Transactions on
Information Theory, 60(12):7900–7912, 2014.

Chen, Yen-Chi, Genovese, Christopher R, Wasserman,
Larry, et al. A comprehensive approach to mode clus-
tering. Electronic Journal of Statistics, 10(1):210–241,
2016.

Chen, Yen-Chi, Genovese, Christopher R, and Wasserman,
Larry. Density level sets: Asymptotics, inference, and
visualization. Journal of the American Statistical Associ-
ation, pp. 1–13, 2017.

Cheng, Yizong. Mean shift, mode seeking, and cluster-
ing. IEEE transactions on pattern analysis and machine
intelligence, 17(8):790–799, 1995.

Comaniciu, Dorin and Meer, Peter. Mean shift: A robust
approach toward feature space analysis. IEEE Transac-
tions on pattern analysis and machine intelligence, 24(5):
603–619, 2002.

Cormen, Thomas H. Introduction to algorithms. MIT press,

2009.

Ester, Martin, Kriegel, Hans-Peter, Sander, J¨org, Xu, Xi-
aowei, et al. A density-based algorithm for discovering
clusters in large spatial databases with noise. In Kdd,
volume 96, pp. 226–231, 1996.

Felzenszwalb, Pedro F and Huttenlocher, Daniel P. Efﬁcient
graph-based image segmentation. International journal
of computer vision, 59(2):167–181, 2004.

Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
The elements of statistical learning, volume 1. Springer
series in statistics New York, 2001.

Genovese, Christopher R, Perone-Paciﬁco, Marco,
Verdinelli, Isabella, and Wasserman, Larry. Non-
Journal of
parametric inference for density modes.
the Royal Statistical Society: Series B (Statistical
Methodology), 78(1):99–126, 2016.

Hartigan, John A. Clustering algorithms, volume 209. Wiley

New York, 1975.

Jiang, Heinrich. Density level set estimation on manifolds
with dbscan. arXiv preprint arXiv:1703.03503, 2017a.

Jiang, Heinrich. On the consistency of quick shift. In Neural

Information Processing Systems (NIPS), 2017b.

Jiang, Heinrich and Kpotufe, Samory. Modal-set estimation
with an application to clustering. In Artiﬁcial Intelligence
and Statistics, pp. 1197–1206, 2017.

Kpotufe, Samory and von Luxburg, Ulrike. Pruning nearest
neighbor cluster trees. arXiv preprint arXiv:1105.0540,
2011.

LeCun, Yann, Cortes, Corinna, and Burges, CJ. Mnist hand-
written digit database. AT&T Labs [Online]. Available:
http://yann. lecun. com/exdb/mnist, 2, 2010.

Li, Jia, Ray, Surajit, and Lindsay, Bruce G. A nonparametric
statistical approach to clustering via mode identiﬁcation.
Journal of Machine Learning Research, 8(Aug):1687–
1723, 2007.

Lichman, M. UCI machine learning repository, 2013. URL

http://archive.ics.uci.edu/ml.

Rigollet, Philippe, Vert, R´egis, et al. Optimal rates for
plug-in estimators of density level sets. Bernoulli, 15(4):
1154–1178, 2009.

Rinaldo, Alessandro and Wasserman, Larry. Generalized
density clustering. The Annals of Statistics, pp. 2678–
2722, 2010.

Dasgupta, Sanjoy and Kpotufe, Samory. Optimal rates for
k-nn density and mode estimation. In Advances in Neural
Information Processing Systems, pp. 2555–2563, 2014.

Rodriguez, Alex and Laio, Alessandro. Clustering by fast
search and ﬁnd of density peaks. Science, 344(6191):
1492–1496, 2014.

Quickshift++: Provably Good Initializations for Sample-Based Mean Shift

Singh, Aarti, Scott, Clayton, Nowak, Robert, et al. Adaptive
hausdorff estimation of density level sets. The Annals of
Statistics, 37(5B):2760–2782, 2009.

Sriperumbudur, Bharath and Steinwart, Ingo. Consistency
and rates for clustering with dbscan. In Artiﬁcial Intelli-
gence and Statistics, pp. 1090–1098, 2012.

Steinwart, Ingo. Adaptive density level set clustering. In
Proceedings of the 24th Annual Conference on Learning
Theory, pp. 703–738, 2011.

Tsybakov, Alexandre B et al. On nonparametric estimation
of density level sets. The Annals of Statistics, 25(3):
948–969, 1997.

Van Der Vaart, Aad and Wellner, Jon A. A note on bounds
for vc dimensions. Institute of Mathematical Statistics
collections, 5:103, 2009.

Vedaldi, Andrea and Soatto, Stefano. Quick shift and kernel
methods for mode seeking. Computer vision–ECCV 2008,
pp. 705–718, 2008.

Verdinelli,

Isabella and Wasserman, Larry.

sis of a mode clustering diagram.
arXiv:1805.04187, 2018.

Analy-
arXiv preprint

Vinh, Nguyen Xuan, Epps, Julien, and Bailey, James. In-
formation theoretic measures for clusterings compari-
son: Variants, properties, normalization and correction
for chance. Journal of Machine Learning Research, 11
(Oct):2837–2854, 2010.

Wasserman, Larry, Azizyan, Martin, and Singh, Aarti. Fea-
ture selection for high-dimensional clustering. arXiv
preprint arXiv:1406.2240, 2014.

