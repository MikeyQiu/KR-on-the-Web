0
2
0
2
 
b
e
F
 
1
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
1
8
0
0
.
6
0
9
1
:
v
i
X
r
a

Bayesian Evidential Deep Learning with PAC Regularization

Manuel Haußmann1

Sebastian Gerwinn2

Melih Kandemir2

1HCI/IWR, Heidelberg University, Germany
2Bosch Center for Artiﬁcial Intelligence, Renningen, Germany

Abstract

We propose a novel method for closed-form pre-
dictive distribution modeling with neural nets.
In quantifying prediction uncertainty, we build
on Evidential Deep Learning (EDL), which has
been impactful as being both simple to imple-
ment and giving closed-form access to predic-
tive uncertainty. We employ EDL to model
aleatoric uncertainty and extend it to account
also for epistemic uncertainty by converting it
to a Bayesian Neural Net (BNN). While ex-
tending its uncertainty quantiﬁcation capabili-
ties, we maintain its analytically accessible pre-
dictive distribution model by performing pro-
gressive moment matching for the ﬁrst time
for approximate weight marginalization. The
eventual model introduces a prohibitively large
number of hyperparameters for stable training.
We overcome this drawback by deriving a vac-
uous PAC bound that comprises the marginal
likelihood of the predictor and a complexity
penalty. We observe on regression, classiﬁca-
tion, and out-of-domain detection benchmarks
that our method improves model ﬁt and uncer-
tainty quantiﬁcation.

1

INTRODUCTION

As the interest of the machine learning community in
data-efﬁcient and uncertainty-aware predictors increases,
research on Bayesian Neural Networks (BNNs) (MacKay,
1995; Neal, 1995) gains gradual prominence. Differently
from deterministic neural nets, BNNs have stochastic-
ity on their synaptic weights. Thanks to their stacked
structure, they propagate prediction uncertainty through
the hidden layers. Hence, they can characterize complex
uncertainty structures. Exact inference of such a highly

nonlinear system is not only analytically intractable but
also extremely hard to approximate with high precision.
Consequently, research on BNNs thus far focused greatly
on improving approximate inference techniques in terms
of precision and computational cost (Hernández-Lobato
and Adams, 2015; Kingma et al., 2015; Louizos and
Welling, 2017). All these prior attempts take the pos-
terior inference of global parameters as given and develop
their approximation based on it.

A newly emerging alternative approach is direct predictive
distribution modeling. This approach proposes devising a
highly expressive predictive distribution with several free
parameters. These parameters are then ﬁt to data via max-
imum likelihood estimation. This way, observations are
used to train directly the end product of interest: the pre-
dictive distribution, bypassing the need for the intractable
posterior inference step. Some existing methods model
the predictive distribution via a stochastic process param-
eterized as a neural net (Garnelo et al., 2018a), while
others introduce local priors on the likelihood activations,
integrate them out and train the hyperparameters of the re-
sultant marginal (Sensoy et al., 2018; Malinin and Gales,
2018).

This paper builds on the state of the art of the latter ap-
proach, Evidential Deep Learning (EDL) (Sensoy et al.,
2018), due to its technical simplicity and observed ef-
fectiveness on data. EDL places a Dirichlet prior on the
class assignment probabilities of a classiﬁer and parame-
terizes the Dirichlet strengths of this prior with a neural
net. While demonstrating substantial improvements in
out-of-domain (OOD) detection and adversarial robust-
ness, EDL is not capable of decomposing epistemic and
aleatoric uncertainties.

We propose an efﬁcient and effective method that extends
EDL to BNNs, equipping it with more advanced uncer-
tainty quantiﬁcation and decomposition capabilities. We
assume independent local weight random variables con-
trolling the BNN for each input/target pair of data points,

µ,σ2

→

µ , σ 2 →

⇒

→

→

→

→

⇒

µ , σ 2 →

µ,σ2

→

⇒

Dir(λ|α)

Cat(y|λ)

(cid:114)

+

KL(cid:0)Dir(λ|α)||Dir(λ|1)(cid:1)+log(B/δ)
N

Figure 1: The proposed model for Bayesian Evidential Deep Learning. The orange arrows on the Bayesian Neural
Network (BNN) indicate the progressive ﬂow of the moments during the weight integration phase. The output layer
of the BNN determines the Dirichlet strengths of the prior on the class probability masses. We train this model using
empirical Bayes supported by a complexity penalty term derived from learning-theoretic ﬁrst principles.

which share common hyperparameters. We marginal-
ize these data-point speciﬁc weights of our network and
perform training via empirical Bayes on the prior hy-
perparameters. This analytically intractable marginaliza-
tion is approximated using the Central Limit Theorem
(CLT). Differently from earlier weight marginalization
approaches that assign global weight distributions on in-
ﬁnitely many neurons that recover a Gaussian Process
(GP) (Neal, 1995; Lee et al., 2018; Garragia-Alonso et al.,
2019), our formulation maintains ﬁnitely many hidden
units per layer and assigns them individual weight distri-
butions. Due to per data-point treatment of the weight
marginalization the BNN scales training linearly with the
data size. Adopting empirical Bayes training on this sim-
pliﬁed setup, our method avoids the explicit approxima-
tion of a highly nonlinear and intractable weight posterior,
yet can improve the quality of uncertainty estimations.

The advantages of such a BNN with data-point speciﬁc
marginalization comes at the expense of a major draw-
back. As the number of hyperparameters in a weight-
marginalized BNN grows proportionally to the number of
synaptic connections and hence maximizing the marginal
likelihood w.r.t. such a large number of hyperparameters
is prone to overﬁtting (Bauer et al., 2016). Since the
weight variables are marginalized out and their hyperpa-
rameters of the weight prior are set via optimization, the
model can no longer incorporate regularizing knowledge
other than the parametric form of the prior distribution
(e.g. normal with mean and variance as free parameters).
We address this drawback by deriving a provably vacu-
ous Probably Approximately Correct (PAC) (McAllester,
1999, 2003) bound that contains the marginal likelihood
as its empirical risk term. Minimization of this PAC bound
automatically balances the ﬁt to the data and deviation

from a prior regularizing hypothesis.

We compare our method on various standard regression,
classiﬁcation, and out-of-domain detection benchmarks
against state-of-the-art approximate posterior inference
based BNN training approaches. We observe that our
method provides competitive prediction accuracy and bet-
ter uncertainty estimation scores than those baselines.

2 BAYESIAN EVIDENTIAL DEEP

LEARNING (BEDL)

2.1 EVIDENTIAL DEEP LEARNING

Classiﬁcation with cross-entropy loss in deep learning can
be interpreted as a categorical likelihood parameterized
by a neural net. Evidential Deep Learning nets (EDLs)
(Sensoy et al., 2018) generalize this setup by parameter-
izing a prior by a neural net f (·; w) with deterministic
weights w, instead of the likelihood, formally

λn|xn ∼ p(λn|xn),
yn|λn ∼ p(yn|λn), ∀n.

This way, the model explicitly accounts for the distribu-
tional uncertainty which may arise due to a mismatch
between the train and test data distributions (Quionero-
Candela et al., 2009). For classiﬁcation, a natural choice
for the prior p(λn|w, xn) on the categorical likelihood is
a Dirichlet distribution, and the ﬁnal model becomes

(cid:1),
λn|xn ∼ Dir(cid:0)λn|αn
(cid:1),
yn|λn ∼ Cat(cid:0)yn|λn

∀n,

where Sensoy et al. (2018) use αn = f (xn; w) + 1.

(1)

(2)

(3)

(4)

To train an EDL, the loss they minimize is the expected
sum of squares between yn and λn with an additional
regularizing Kullback Leibler (KL) divergence on the λn.
This loss is (up to constant scaling factors) equivalent to a
Variational Inference approach maximizing the evidence
lower bound. Assuming a normal likelihood p(yn|λn)
with precision β, a prior p(λn) = Dir(cid:0)λn|(1, ..., 1)(cid:1), and
a variational posterior q(λn) = Dir(cid:0)λn|αn), we have

LEDL(w) = −

Eq(λn) [log p(yn|λn)]

N
(cid:88)

n=1
+ KL (q(λn) (cid:107) p(λn))
N
(cid:88)

Eq(λn)

(cid:2)||yn − λn||2

2

(cid:3)

β
2

=

n=1

+ KL (q(λn) (cid:107) p(λn)) ,

(5)

(6)

where both the expectation as well as the KL terms are
analytically tractable for the chosen distributions. How-
ever, modeling the probabilities of the weights has been
avoided due to the high computational cost.

2.2 BAYESIAN LOCAL NEURAL NETS

Given a data set D = {(xn, yn)N
n=1} consisting of N
pairs of input xn and target yn, parameterizing the likeli-
hood by a BNN f (·; w) with random variables w as the
weights results in the following probabilistic model

wn ∼ pφ(wn),
yn|xn, wn ∼ p(yn|xn, wn),

∀n

(7)

(8)

one would

p(yn|xn, wn)
precision β,

where p(yn|xn, wn) is some likelihood, e.g.
for
=
regression
have
N (cid:0)yn|f (xn; wn), β−1(cid:1), with
and
pφ(wn) is some prior over local weights with shared
hyperparameters φ. Differently from a canonical BNN
where all data points share the same global weight latent
variable (Blundell et al., 2015; Gal and Ghahramani,
2015; Kingma et al., 2015; Louizos and Welling, 2017),
here the mapping between each input-output pair is
determined by a separate random variable wn, giving a
unique mapping constrained by sharing a common set
of prior hyperparameters φ, which is required for the
empirical Bayes based objective we introduce below.
As this modiﬁcation implies a collection of only local
latent variables, we refer to the resultant probabilistic
model as a Bayesian Local Neural Net (BLNN). This
model consists of two sources of uncertainty. First, the
model (epistemic) uncertainty captured by the prior over
the parameters, i.e. pφ(wn), which accounts for the
mismatch between the model and the true functional
mapping from xn to yn. Second, the irreducible data
(aleatoric) uncertainty given by p(yn|xn, wn) stemming

from irreducible measurement noise. The mainstream
BNN methods are characterized speciﬁcally to model
these two sources of uncertainty (Hernández-Lobato and
Adams, 2015; Kendall and Gal, 2017; Wu et al., 2019).

2.3 BLNN TRAINING WITH EMPIRICAL

BAYES AND PREDICTION

A well-established alternative to Bayesian model training
with posterior inference is learning by model selection.
This approach suggests marginalizing out all latent vari-
ables, comparing the marginal likelihoods of all possible
hypotheses, and choosing the one providing the highest
response (Kass and Raftery, 1995). Choosing the hyperpa-
rameter value set that maximizes the marginal likelihood
serves as training, hence avoids the posterior inference
step on latent variables. Referred to as empirical Bayes
(Efron, 2012), this technique is fundamental for ﬁtting
non-parametric models such as Gaussian Processes. The
optimization objective for empirical Bayes on a BLNN
is: arg maxφ log pφ(y|X). Introducing an independent
wn for each pair (xn, yn) leads to a marginal likelihood
formulation corresponding to a sum of N independent
marginal likelihoods, giving

log pφ(y|X)

(cid:90) N
(cid:89)

n=1
(cid:90)

=

=

N
(cid:88)

n=1

N
(cid:88)

n=1

= log

p(yn|xn, wn)pφ(wn)dw1 · · · dwN

log

p(yn|xn, wn)pφ(wn)dwn

log pφ(yn|xn)

(9)

which is amenable to using mini-batches for further scal-
ability. The density function of the marginal likelihood
of a training data point is identical to the posterior pre-
dictive density for new test data x∗. Hence, an analytic
approximation developed for training is directly applica-
ble to test time. The independence assumption across data
points has various beneﬁts. Provided that the integrals
in (9) are analytically tractable, the computational com-
plexity scales linearly with the training set size. More
importantly, directly optimizing the marginal likelihood
avoids any complexity for estimating a full posterior over
weights, including complex covariance structures and
multi-modal distributions.

2.4 ANALYTIC MARGINALIZATION OF
LOCAL WEIGHTS WITH MOMENT
MATCHING

Marginalizing out the local weights wn in (9) is an in-
tractable problem due to the highly nonlinear neural net

appearing in the likelihood p(yn|xn, wn). However, we
can marginalize the weights approximately by recursive
moment matching resorting to the Central Limit Theo-
rem (CLT). This technique has previously been used in
BNNs for other purposes, such as expectation propaga-
tion (Hernández-Lobato and Adams, 2015; Ghosh et al.,
2016), fast dropout (Wang and Manning, 2013), and vari-
ational inference (Wu et al., 2019). We employ the same
technique for marginalizing out the weights of the BLNN.
For a single data point and the l-th hidden fully-connected
layer1 consisting of K units with an arbitrary activation
function a(·), the post-activation layer output is given as
hl = a(f l), where f l = Wlhl−1. The j-th pre-activation
output f l
, which
allows us to assume it to be normal distributed via the
CLT due to the independence of the individual wl
jk and
hl−1
terms. The mean and the variance of this random
k
variable can be computed as

j is a sum of K terms f l

j = (cid:80)

jkhl−1

k wl

k

E (cid:2)f l

(cid:3) =

j

E (cid:2)wl

jk

(cid:3) E (cid:2)hl−1

(cid:3) ,

k

var (cid:2)f l

(cid:3) =

j

E (cid:2)(wl

jk)2(cid:3) var (cid:2)hl−1

k

(cid:3)

K
(cid:88)

k=1

K
(cid:88)

k=1

+ var (cid:2)wl

(cid:3) (cid:0)E (cid:2)hl−1

(cid:3) (cid:1)2

,

jk

k

(10)

(11)

k

) = max(0, hl−1

where we drop any potential covariance structure be-
tween the outputs of a layer. The mean and the vari-
ance of the weights are readily available via the distri-
butions pφ(wn), leaving only the ﬁrst two moments of
hl−1
undetermined. For common activations such as the
k
ReLU, a(hl−1
), which we will rely on
k
in this work, closed-form solutions to these moments are
tractable (Frey and Hinton, 1999) given the moments of
the pre-activations of the previous layer f l−1 (see Ap-
pendix B for details). This gives a recursive scheme termi-
nating at the input layer, since we have f 1
hjxk.
As xk is a constant, its ﬁrst moment is itself and the sec-
ond is zero. Consequently,

j = (cid:80)

k w1

E (cid:2)f 1

(cid:3) =

j

E[w1

jk]xk and

var (cid:2)f 1

(cid:3) =

j

var[wl

jk]x2
k,

K
(cid:88)

k=1

K
(cid:88)

k=1

(12)

(13)

completing the full recipe of how all weights of a BNN
can be recursively integrated out from bottom to top, sub-
ject to a tight approximation. Scenarios with stochastic
input x ∼ p(x) typically entail controllable assumptions
on p(x). The equations above remain intact after adding

1Convolutional layers follow analogously.

an expectation operator around xk and x2
k, readily avail-
able for any explicitly deﬁned p(x). Contrarily to the case
in GPs, stochastic inputs can be trivially adapted into this
framework, greatly simplifying the math for uncertainty-
sensitive setups, such as PILCO (Deisenroth et al., 2015).
For a net with L layers, the outcome of recursive moment
matching is a distribution over the ﬁnal latent hidden layer
f L
n ∼ N (f L
n), simplifying the highly nonlinear
integrals in (9) to

n |mn, s2

(cid:90)

p(yn|xn, wn)p(wn)dwn

(cid:90)

≈

p(yn|f L

n )N (f L

n |mn, s2

n)df L
n ,

n are the mean and variance of f L

where mn, s2
n . This
leaves us in a much simpler situation as we can choose a
suitable distribution family for yn.

Extensions. Adaptations of CLT-based recursive mo-
ment matching to many other activation types and skip
connections are feasible without further approximations.
Max pooling can also be incorporated using approxima-
tions, but have also been shown to be replaceable alto-
gether by strided convolutions without a performance
loss (Springenberg et al., 2015). Deeper networks tend to
require normalization procedures, which are not directly
amenable to this moment matching. However tractable
moments can also be computed for activation functions
such as the ELU (Clevert et al., 2015) as we show in the
appendix alleviating this constraint. The variance com-
putations in (11) and the ReLU speciﬁc post-activation
do not model any potential covariance structure between
the pre-/post-activations units of a layer. While this is in
principle feasible, e.g. along the lines of Wu et al. (2019),
it leads to an explosion in the required computational cost
and memory, hindering the applicability of the approach
to deeper nets. Hence, we stick to a diagonal covariance
structure throughout, as Wu et al. (2019) have also shown
only little test set performance beneﬁt of modeling it.

2.5 BAYESIAN EVIDENTIAL DEEP LEARNING

The original formulation for EDL builds on deterministic
neural nets, hence assumes a point estimate on the model
weights (consciously ignoring model uncertainty) and a
sum-of-squares loss term. We improve this framework by
assigning a local prior on the EDL weights pφ(wn),2 and
instead consider the optimization of the marginal likeli-
hood, which amounts to employing a BLNN as a prior on
the likelihood and marginalizing over all wn as well as

2Throughout

this work we will assume the weights
to be normal distributed and such that we have
wn ∼ pφ(wn) = N (cid:0)wn|µ, diag(σ2)(cid:1), i.e. φ = (µ, σ2).

λn. We name the eventual model that combines BLNN
with EDL Bayesian Evidential Deep Learning (BEDL).
By virtue of the localized weights, the marginal likelihood
of BEDL factorizes across data points, bringing additive
data point speciﬁc marginal log-likelihoods maintaining
the central source of its scalability, formally

log pφ(y|X) = log

p(yn|λn)p(λn|wn, xn)

(cid:90) (cid:89)

n

· pφ(wn)dλ1 . . . dλN dw1 . . . dwN
(cid:88)

p(yn|λn)p(λn|wn, xn)

log

(cid:90)

=

· pφ(wn)dλndwn
(cid:88)

log pφ(yn|xn).

=

n

n

The marginalization of λn on the last step can be per-
formed analytically under conjugacy or can be efﬁciently
approximated by Taylor expansion or Monte Carlo sam-
pling, while one can employ the moment matching to
marginalize the weights wn.

For the C-class classiﬁcation task with one-hot encoded
targets categorically distributed yn ∈ {0, 1}C and Dirich-
let distributed λn, this gives us for the n-th term in (14),

(cid:90) (cid:18)(cid:90)

log

p(yn|λn)p(λn|αn)dλn

N (f L

n |mn, s2

n)df L
n

(cid:19)

= log E

N (f L

n |mn,s2

n)

(cid:34) C
(cid:89)

c=1

(cid:18) αnc
αn0

(cid:19)ync(cid:35)

,

(15)

the

use

parametrization αn
n ), and αn0 = (cid:80)

=
where we
(αn1, ..., αnC) = exp(f L
c αnc.
Since the computational bottleneck on marginalizing the
weights through the neural net layers is circumvented
by the analytical CLT-based moment matching, the ﬁnal
expectation can be efﬁciently approximated by sampling.

Uncertainty Decomposition. Following Depeweg
et al. (2018) one can use the law of total variance to
decompose the predictive variance of ﬁnal marginal for
one data point (x, y) as follows

var [y|x] = varw [E [y|w, x]] + Ew [var [y|w, x]] ,

where the ﬁrst term, varw [E [y|w, x]], focuses on the
contribution to this predictive uncertainty by the variance
over the network weights, i.e. the epistemic uncertainty,
while the second, Ew [var [y|w, x]], represents the re-
maining variance in the likelihood for the average weights.
After having marginalized over λ, the mean and variance
E [y|w, x], var [y|w, x], are analytically tractable with

E [yc|w, x] =

and var [yc|w, x] =

αc
α0

(cid:16)

αc
α0

1 −

(cid:17)

.

αc
α0

The EDL formulation allows for an analytic computation
of the predictive variance, however as it considers only
deterministic weights, it gets for learned parameters ˆw

var [y|x] = var [y| ˆw, x] ,

i.e. only a measure of the aleatoric uncertainty, lacking
the epistemic. Our extension allows for the decomposi-
tion of the predictive uncertainty maintaining analytical
tractability of the approximation to a great extent

var [y|x] = varw [E [y|w, x]] + Ew [var [y|w, x]]
y|f L, x

+ E

(cid:104)
E

var

(cid:105)(cid:105)

(cid:104)

(cid:104)

(cid:104)

≈ varf L

f L

y|f L, x

(cid:105)(cid:105)

,

(14)

where the ﬁnal variance and expectation can be efﬁciently
approximated with samples as discussed above.

3 A VACUOUS PAC BOUND TO

REGULARIZE BEDL

Training the objective in (14) is effective for ﬁtting a
predictor on data. It also naturally provides a learned
loss attenuation mechanism. However, it lacks a key
advantage of the Bayesian modeling paradigm. As the
hyperparameters of the weight priors are employed for
model ﬁtting, they no longer contribute to training as
complexity penalizers. It is well-known from the GP liter-
ature that marginal likelihood-based training is prone to
overﬁtting for models with a large number of hyperparam-
eters (Bauer et al., 2016).3 We address this shortcoming
by complementing the marginal likelihood objective of
(14) with a penalty term derived from learning-theoretic
ﬁrst principles. We tailor the eventual loss only for ro-
bust model training and keep it maximally generic across
learning setups. This comes at the expense of arriving at
a generalization bound that makes a theoretically trivial
statement, yet brings signiﬁcant improvements to training
quality as illustrated in our experiments.

PAC bounds have been commonly used for likelihood-
free and loss-driven learning settings. A rare exception
by Germain et al. (2016) proves the theoretical equiv-
alence of a particular sort of PAC bound to variational
inference. Similarly, we keep the notion of a likelihood
in our risk deﬁnition, but differently, we correspond our
bound to the marginal likelihood. Given a predictor h
chosen from a hypothesis class H as a mapping from x
to y, we deﬁne the true and the empirical risks as

R(h) = −Ex,y∼∆

(cid:2)p(cid:0)y|h(x)(cid:1)(cid:3)

and

3An obvious direct objection is to note that one could just go
one level higher in the hierarchy, introducing hyperpriors over
the parameters φ. We derive in Appendix A an approach of how
one would go ahead to do this, but preliminary experiments have
shown it to perform a lot worse than the PAC-based approach.

RD(h) = −

p(cid:0)yn|h(xn)(cid:1),

1
N

N
(cid:88)

n=1

for the data set D drawn from an arbitrary and unknown
data distribution ∆. The risks R(h) and RD(h) are
bounded below by − max p(y|h(x)) and above by zero.
Although this setting relaxes the common assumption that
bounds risk to the [0, 1] interval, it is substantially simpler
than the one suggested in (Germain et al., 2016), which
deﬁnes R(h) = Ex,y∼∆ [log p(y|h(x))] ∈ (−∞, +∞).
This unboundedness brings severe technical complica-
tions, which are no longer relevant for our approach.
Denoting by Q the distributions learnable over H and
by P some regularizing distribution on H, according to
Theorem 2.14 in (Germain et al., 2009) we have for any
δ ∈ (0, 1] and any convex function d(·, ·) the bound below

(cid:40)

(cid:16)

d

Pr

Eh∼Q [RD(h)] , Eh∼Q [R(h)]

(cid:17)

≤

Looseness of the Bound.
It should ﬁnally be noted that
while we use the PAC theory to derive and motivate the
ﬁnal objective, it should no longer be used in its PAC
interpretation, as the approximative steps result in a loose
bound that is always trivially fulﬁlled. Its justiﬁcation lies
primarily in its regularizing function.

4 GENERALIZATION TO

REGRESSION

The EDL formulation was introduced by Sensoy et al.
(2018) only for the case of classiﬁcations. However, the
main motivation of the approach can also be extended
to the case of regression as follows. We place a normal
likelihood over the targets, treating the λn as the mean and
another normal as the distribution over λn parameterizing
both mean and variance with a BNN, giving

KL (Q (cid:107) P ) + log(B/δ)
N

(cid:41)

≥ 1 − δ,

(16)

λn|wn, xn ∼ N

λn

(cid:16)

(cid:12)
(cid:12)f1(xn; wn), exp (cid:0)f2(xn; wn)(cid:1)
(cid:12)

(cid:33)

wn ∼ pφ(wn)

(cid:2)Eh∼P

(cid:2)exp (cid:0)N d(RD(h), R(h))(cid:1)(cid:3)(cid:3).
where B = ED∼∆
Using a quadratic distance measure d(x, y) = (x − y)2
and suitably bounding B by exploiting the boundedness
of the likelihood and in turn the risk, we get as an upper
bound on the expected true risk

log pφ(yn|xn)

−

1
N

N
(cid:88)

n=1

(cid:114)

+

KL (Q (cid:107) P ) − log δ
N

+

log max(B)
N

,

which is the objective we use to train BEDL that contains
the marginal likelihood in the ﬁrst term and a regularizer
in the second (see Appendix C for a detailed derivation
of the bound). The additional term resembles the KL
term in the EDL loss, and gives a theoretically-grounded
mechanism to incorporate regularization.

Computation of the Bound for Classiﬁcation. For a
C-class classiﬁcation we can compute the ﬁrst term in
(17) as discussed above ((15)). In the regularization term
as we use (similar to EDL) P = Dir(cid:0)λ|(1, . . . , 1)(cid:1), i.e.
the assumption that each class is equally likely, as the reg-
ularizing distribution. Given Q = Dir(λ|α), KL (Q (cid:107) P )
is analytically tractable, as is the last term where we use
the upper bound log max(B) ≤ N (see Appendix C).

4The theorem assumes risks to be deﬁned within the [0, 1]
interval in the original paper, however, it is valid for any bounded
risk. Furthermore, our risk deﬁnitions can trivially be squashed
into [0, 1] up to a constant.

yn|λn ∼ N (cid:0)yn|λn, β−1(cid:1) ,

with some ﬁxed observation precision β. For the n-th
sample in (14) with f L
n2), mn = (mn1, mn2),
n = (s2
and s2
n2) (the moment matching mean and
variance), the log marginal likelihood is then given as

n = (f L

n1, f L

n1, s2

log p(yn|xn) =
(cid:16)

log N

yn

(cid:12)
(cid:12)mn1, β−1 + s2
(cid:12)

(17)

n1 + exp(mn2 + s2

n2/2)

(cid:17)

.

n )df L

The approximation is computed via another moment
matching step approximating the result of the inner inte-
gral p(λn) = (cid:82) p(λn|f L
n )p(f L
n with a normal distri-
bution (See Appendix B.3 for the derivation) while the
ﬁnal equality follows directly from standard results on
normal distributions. Contrary to the case of classiﬁcation
where the ﬁnal step requires samples, for regression we
stay completely sampling-free. We bound max B in (17)
by exploiting that β is ﬁxed prior to training. Conse-
quently, we obtain log max(B)
2π as a bound (see Ap-
pendix C), which, as for the classiﬁcation case, gives only
a trivial performance guarantee (exceeding the maximum
possible risk) but provides a justiﬁed training scheme.

≤ β

N

5 RELATED WORK

CLT-based moment matching of local weight real-
izations. The objective for variational inference on
BNNs (Kingma et al., 2015; Wu et al., 2019; Gal and
Ghahramani, 2016), optimizing a global variational pos-
terior q(w), consists of a computationally intractable

Table 1: Regression. Average test log-likelihood ± standard error over 20 random train/test splits. N/d give the number
of data points in the complete data set and the number of input feature. The sparse GP results are cited from (Bui et al.,
2016) and VarOut relies on our own implementation.

N/d

Sparse GP

MC Dropout
VarOut
PBP
DVI

boston
506/13

concrete
1030/8

energy
768/8

kin8nm
8192/8

naval
11934/16

power
9568/4

protein
45730/9

wine
1599/11

−2.22 ± 0.07

−2.85 ± 0.02

−1.29 ± 0.01

1.31 ± 0.01

4.86 ± 0.04

−2.66 ± 0.01

−2.95 ± 0.05

−0.67 ± 0.01

−2.46 ± 0.25
−2.63 ± 0.02
−2.57 ± 0.09
−2.41 ± 0.02

−3.04 ± 0.09
−3.15 ± 0.02
−3.16 ± 0.02
−3.06 ± 0.01

−1.99 ± 0.09
−3.29 ± 0.00
−2.04 ± 0.02
−1.01 ± 0.06

0.95 ± 0.03
1.09 ± 0.01
0.90 ± 0.01
1.13 ± 0.00

3.80 ± 0.05
5.50 ± 0.03
3.73 ± 0.01
6.29 ± 0.04

−2.89 ± 0.01
−2.82 ± 0.01
−2.84 ± 0.01
−2.80 ± 0.00

−2.80 ± 0.05
−0.93 ± 0.06
−2.90 ± 0.01 −0.88 ± 0.02
−0.97 ± 0.01
−2.97 ± 0.00
−0.90 ± 0.01
−2.84 ± 0.01

BEDL-Hyper (Ours)
BEDL (Ours)
BEDL+Reg (Ours)

−2.59 ± 0.02
−3.30 ± 0.01
−2.57 ± 0.04
−2.45 ± 0.08
−0.87 ± 0.10
−3.09 ± 0.06
−2.43 ± 0.06 −3.02 ± 0.02 −0.73 ± 0.04

0.44 ± 0.00
1.12 ± 0.01
1.15 ± 0.01

−3.00 ± 0.00
−2.98 ± 0.01
3.69 ± 0.00
5.76 ± 0.07
−2.82 ± 0.01
−2.80 ± 0.01
5.60 ± 0.11 −2.79 ± 0.01 −2.77 ± 0.01

−1.00 ± 0.01
−0.93 ± 0.01
−0.90 ± 0.01

Eq(w) [log p(y|x, w)] that decomposes across data points.
Fast dropout (Wang and Manning, 2013) approximates
these terms via local reparameterization with moment
matching. The same local reparameterization has been
later combined with a KL term to perform mean-ﬁeld
variational inference via MC sampling (Kingma et al.,
2015) or moment matching (Wu et al., 2019). Our BLNN
formulation is akin to an amortized VI approach learn-
ing a single global set of posterior parameters φ for the
variational posterior approximation qφ(w). We use the
same trick to marginalize the local weights, which keeps
the machinery intact until the top-most step where the
order of the log(·) and E [·] operations is swapped. This
small change, however, has a large impact on the quality
of uncertainty estimations.

Wide neural nets as GPs. The equivalence of a GP to
a weight-marginalized BNN with a single inﬁnitely wide
hidden layer has been discovered long ago (Neal, 1995)
using the multivariate version of CLT. This result has
later been generalized to multiple dense layers (Lee et al.,
2018; Matthews et al., 2018), as well as to convolutional
layers (Garragia-Alonso et al., 2019). The asymptotic
treatment of the neuron count makes this approach exact
at the expense of a lack of neuron-speciﬁc parameteriza-
tion. The eventual GP has few hyperparameters to train,
however, a prohibitively expensive covariance matrix to
calculate. We employ the same training method on a
middle ground where the hyperparameter count is double
as many as a deterministic net and the cross-covariances
across data points are not explicitly modeled.

Predictive density modeling. Neural Processes (Gar-
nelo et al., 2018a,b) follow a GP inspired approach of
learning the predictive density using neural networks as
part of the mapping from input to output space relying on
an incorporation of the input data as context points for
the predictive distribution of a test point. Earlier work on
prior networks (Malinin and Gales, 2018) parameterizes
a prior to a classiﬁcation-speciﬁc likelihood with deter-

ministic neural nets, hence, discards model uncertainty.
Additionally, they require samples from another domain
to learn the distributional awareness. BEDL reformulates
prior networks independently from the output structure,
extends them to support also model uncertainty, and in-
troduces a principled scheme for their training.

6 EXPERIMENTS

We evaluate BEDL and its PAC-regularized version
BEDL+Reg on a diverse selection of regression and classi-
ﬁcation tasks. Complete details on the training procedure
can be found in Appendix D.5

Regression. We evaluate the regression performance
of BEDL+Reg and the baselines on eight standard UCI
benchmark data sets. Adopting the experiment protocol
introduced in (Hernández-Lobato and Adams, 2015), we
use 20 random train-test set splits comprising 90% and
10% of the samples, respectively. The nets consist of a
single hidden layer with 50 units and ReLU nonlineari-
ties.6 The hypothesis class in this task is over the regu-
larization parameters P := p(λ) = (cid:81)
n N (λn|0, α−1),
with precision α, while Q is given as Q := p(λ) =
(cid:82) p(λn|f n)p(f n)df n. We compare BEDL+Reg
(cid:81)
n
against the state of the art in BNN inference methods that
do not require sampling across neural net weights, Proba-
bilistic Back-Propagation (PBP) (Hernández-Lobato and
Adams, 2015) and Deterministic Variational Inference
(DVI) (Wu et al., 2019), which use the CLT-based moment
matching for expectation propagation and VI, respec-
tively. For completeness, we also compare against the two
most common sampling-based alternatives, Variational
Dropout (VarOut) (Kingma et al., 2015; Molchanov et al.,
2017) and MC Dropout (Gal and Ghahramani, 2016). In

5We provide an implementation of the model under

github.com/manuelhaussmann/bedl.

6Except for the larger protein, for which we use 100

hidden units.

Table 2: Classiﬁcation and OOD Detection. Test error and the area under curve of the empirical CDF (ECDF-AUC)
of the predictive entropies on two pairs of datasets. Smaller values are better for both metrics.

MNIST
(In Domain)

Fashion-MNIST
(Out-of-Domain)

CIFAR 1-5
(In Domain)

CIFAR 6-10
(Out-of-Domain)

Reference

Test Error (%)

ECDF-AUC

Test Error(%)

ECDF-AUC

MC Dropout
VarOut
DVI
EDL

BEDL
BEDL+Reg

(Gal and Ghahramani, 2016)
(Kingma et al., 2015)
(Wu et al., 2019)
(Sensoy et al., 2018)

Ours
Ours

1.12
1.47
0.72
1.08

0.81
0.66

0.429
1.381
1.318
0.132

1.512
0.055

18.36
33.94
23.32
20.34

24.38
20.02

0.946
0.673
1.251
0.451

1.253
0.083

the results summarized in Table 1, BEDL+Reg outper-
forms all baselines in the majority of the data sets and is
competitive in the others.

The PAC regularization improves over BEDL in all data
sets except one. We also report results for a sparse GP
with 50 inducing points, which approximates a BNN
of one inﬁnitely wide hidden layer (Neal, 1995). As
expected, the GP sets a theoretical upper bound on
BEDL+Reg as well as the baselines for one hidden
layer architectures. Lastly, we compare our tediously
derived PAC regularizer to straightforward Maximum-A-
Posteriori estimation on the BEDL hyperpriors (BEDL-
Hyper) (see Appendix A for details), which deteriorates
performance on all UCI data sets.

Classiﬁcation and out-of-domain detection. We eval-
uate classiﬁcation and out-of-domain (OOD) sample de-
tection performance of BDN+Reg on image classiﬁca-
tion with deep architectures, adhering to the protocol re-
peatedly used in prior work (Louizos and Welling, 2017;
Sensoy et al., 2018). We train LeNet-5 networks on the
MNIST train split, evaluate their classiﬁcation accuracy
on the MNIST test split as the in-domain task, and mea-
sure their uncertainty on the Fashion-MNIST7 data set
as the out-of-domain task. We expect from a perfect
model to predict true classes with high accuracy on the
in-domain task and always predict a uniform probabil-
ity mass on the out-of-domain task, i.e. the area under
curve of the empirical CDF (ECDF-AUC) of its predic-
tive distribution entropy is zero. We perform the same
experiment on CIFAR10 using the ﬁrst ﬁve classes for the
in-domain task and treating the rest as out-of-domain. We
use P := Dir(cid:0)λ|(1, . . . , 1)(cid:1) as the regularization prior
on the class assignment parameters, which has the uni-

7Due to the license status of the not-MNIST data conﬂicting
with the afﬁliation of the authors, we have to change the setup
of earlier work, e.g. (Lakshminarayanan et al., 2017; Louizos
and Welling, 2017; Sensoy et al., 2018), using instead Fashion-
MNIST as the closest substitute.

form probability mass on its mean, encouraging an OOD
alarm in the absence of contrary evidence. In Table 2, we
compare BEDL+Reg against EDL (Sensoy et al., 2018),
the state of the art in neural net based uncertainty quan-
tiﬁcation, also the non-Bayesian and heuristically trained
counterpart of BEDL+Reg. We consider EDL also as a
special case of Prior Networks (Malinin and Gales, 2018)
that does not need to rely on OOD data during training
time, commensurate for our training assumptions. We
evaluate MC Dropout, VarOut, and DVI as baselines in
this setup. BEDL+Reg improves the state of the art in all
four metrics except the CIFAR10 in-domain task, where
it ranks second after the prediction time weight sampling-
based (hence less scalable) MC Dropout. Remarkably,
BEDL+Reg detects the OOD samples with signiﬁcantly
better calibrated ECDF-AUC scores than EDL.

Comparison to GP variants. We evaluate the impact
of local weight realization on prediction performance by
comparing BEDL+Reg to GPs with kernels derived from
BNNs with global weight realizations (Garragia-Alonso
et al., 2019; Lee et al., 2018; Neal, 1995) on MNIST and
CIFAR10 data sets. It is technically not possible to per-
form this evaluation in a fully commensurate setup, as
these baselines assume inﬁnitely many neurons per layer
and do not have weight-speciﬁc degrees of freedom. Fur-
thermore, Garragia-Alonso et al. (2019) perform neural
architecture search and Lee et al. (2018) use only part of
the CIFAR10 training set reporting that the rest does not
ﬁt into the memory of a powerful workstation. We nev-
ertheless view the performance scores reported in these
papers as practical upper bounds and provide qualitative
comparison. For the choice of neural net depth, we take
NNGP (Lee et al., 2018) as a reference and devise a con-
trarily thin two-layer convolutional BEDL+Reg network.
The results and the architectural details are summarized
in Table 3. BEDL and BEDL+Reg can reach lower error
rates using signiﬁcantly less computational resources.

Table 3: Comparison to GP Variants. Test error in %
on two image classiﬁcation tasks. BEDL reaches lower
error rate than previously proposed neural net based GP
constructions by two convolutional layers with 96 ﬁlters
of size 5 × 5 and stride 2. BEDL converges in 50 epochs,
amounting to circa 30 minutes of training time on a single
GPU. The GP alternatives have been reported to have
signiﬁcantly larger time and memory requirements. The
GP results are cited from (Garragia-Alonso et al., 2019)

MNIST CIFAR10

NNGP
Convolutional GP
ConvNet GP
Residual CNN GP
ResNet GP

BEDL (Ours)
BEDL+Reg (Ours)

1.21
1.17
1.03
0.96
0.84

0.91
0.63

44.3
35.4
-
-
-

34.20
32.47

Computational cost. Table 4 summarizes the compu-
tational cost analysis of the considered approaches. MC
Dropout and VarOut can quantify uncertainty only by
taking samples across weights, which increases the pre-
diction cost linearly to the sample count. DVI and
BEDL+Reg perform the forward pass during both training
and prediction time via analytical moment matching at
double and triple costs, respectively. Both methods have
sampling costs for intractable likelihoods.8 BEDL+Reg
may also have another additive per-data-point sampling
cost for calculating intractable functional mapping reg-
ularizers. Favorably, both of these overheads are only
additive to the forward pass cost, i.e. sampling time is
independent of the neural net depth, hence they do not set
a computational bottleneck. The training and prediction
cost of BEDL+Reg is three times EDL which builds on
deterministic neural nets. However, it provides substantial
improvements in both prediction accuracy and uncertainty
quantiﬁcation.

7 CONCLUSION

In this paper, we present a method for performing
Bayesian inference within the framework of evidential
deep learning. Employing empirical Bayesian methods
for inference and combining it with PAC-bounds for reg-
ularization, we achieve higher accuracy and better pre-
dictive uncertainty estimates while maintaining scalable

8Even this sampling step could be avoided by a suitable
Taylor approximation, see e.g. Appendix B.4/B.5 in (Wu et al.,
2019). As the added approximation error was more detrimental
to model performance than a cheap MC approach in preliminary
experiments, we stay with the latter for both.

Table 4: Computational Cost. Per data point computa-
tional cost analysis in FLOPs. F: Forward pass cost of
a deterministic neural net. W: Number of weights in the
net. L: Analytical calculation cost for the exact or approx-
imate likelihood or the loss term. S: Number of samples
taken for approximation. R: The cost of the regularization
term per unit (weight or data point).

Training per iteration

Prediction
O(cid:0)(F + L)S(cid:1)
O(cid:0)2(F + L)S + R(W/N )(cid:1) O(cid:0)2(F + L)S(cid:1)
O(cid:0)2F + SL + R(W/N )(cid:1)
O(cid:0)F + L(cid:1)
O(cid:0)3F + SL(cid:1)

MC Dropout O(cid:0)(F + L)S(cid:1)
VarOut
DVI
EDL
BEDL
BEDL+Reg O(cid:0)3F + S(L + R)(cid:1)

O(cid:0)2F + SL(cid:1)
O(cid:0)F + L(cid:1)
O(cid:0)3F + SL(cid:1)
O(cid:0)3F + SL(cid:1)

inference. Exact inference in a fully Bayesian model
such as a GP (c.f. Table 1) or Hamiltonian Monte Carlo
inference for BNNs (Bui et al., 2016) are known to pro-
vide better error rates and test log-likelihood scores, yet
their computational demand does not scale well to large
networks and data-sets. Our method, on the other hand,
shows strong indicators for improvement in uncertainty
quantiﬁcation and predictive performance when compared
to other BNN approximate inference schemes with rea-
sonable computational requirements. These beneﬁts of
the BEDL+Reg framework might especially be fruitful
in setups such as model-based deep reinforcement learn-
ing, active learning, and data synthesis, where uncertainty
quantiﬁcation is a vital ingredient of the predictor.

References

M. Bauer, M. v.d. Wilk, and C.E. Rasmussen. Understand-
ing Probabilistic Sparse Gaussian Process Approxima-
tions. In NIPS, 2016.

C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wies-
tra. Weight Uncertainty in Neural Networks. In ICML,
2015.

T. Bui, D. Hernández-Lobato, J. M. Hernandez-Lobato,
Y. Li, and R. Turner. Deep Gaussian Processes for Re-
gression using Approximate Expectation Propagation.
In ICML, 2016.

O. Catoni. PAC-Bayesian Supervised Classiﬁcation: The
Thermodynamics of Statistical Learning. IMS Lecture
Notes Monograph Series, 56, 2007.

D. Clevert, T. Unterthiner, and S. Hochreiter. Fast and
accurate deep network learning by exponential linear
units (elus). arXiv preprint arXiv:1511.07289, 2015.

M.P. Deisenroth, D. Fox, and C.E. Rasmussen. Gaussian
Processes for Data-Efﬁcient Learning in Robotics and
Control. IEEE Trans. Pattern Analysis and Machine
Intelligence, 17(2):402–423, 2015.

S. Depeweg, J.-Miguel Hernandez-Lobato, F. Doshi-
Velez, and S. Udluft. Decomposition of uncertainty in
Bayesian deep learning for efﬁcient and risk-sensitive
learning. In ICML, 2018.

G. Dziugaite and D.M. Roy. Computing Nonvacuous
Generalization Bounds for Deep (Stochastic) Neural
Networks with Many More Parameters than Training
Data. In UAI, 2017.

B. Efron. Large-scale Inference: Empirical Bayes Meth-
ods for Estimation, Testing, and Prediction, volume 1.
Cambridge University Press, 2012.

B.J. Frey and G.E. Hinton. Variational Learning in Non-
linear Gaussian Belief Networks. Neural Computation,
11(1):193–213, 1999.

Y. Gal and Y. Ghahramani. Dropout as a Bayesian Ap-
proximation: Representing Model Uncertainty in Deep
Learning. In ICML, 2016.

Y. Gal and Z. Ghahramani. Bayesian Convolutional Neu-
ral Networks with Bernoulli Approximate Variational
Inference. arXiv preprint arXiv:1506.02158, 2015.

M. Garnelo, D. Rosenbaum, C. J Maddison, T. Ramalho,
D. Saxton, M. Shanahan, Y. W. Teh, D. J Rezende,
and SM Eslami. Conditional neural processes. arXiv
preprint arXiv:1807.01613, 2018a.

M. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J
Rezende, SM Eslami, and Y. W. Teh. Neural processes.
arXiv preprint arXiv:1807.01622, 2018b.

A. Garragia-Alonso, C.E. Rasmussen, and L. Aitchinson.
Deep Convolutional Networks as Shallow Gaussian
Processe. In ICLR, 2019.

P. Germain, A. Lacasse, F. Laviolette, and M. Marchand.
PAC-Bayesian Learning of Linear Classiﬁers. In ICML,
2009.

P. Germain, F. Bach, A. Lacoste, and S. Lacoste-Julien.
PAC-Bayesian Theory Meets Bayesian Inference. In
NIPS, 2016.

S. Ghosh, J. Yedidia, and F.M. Delle Fave. Assumed
Density Filtering Methods for Scalable Learning of
Bayesian Neural Networks. In AAAI, 2016.

J. M. Hernández-Lobato and R. Adams. Probabilistic
Backpropagation for Scalable Learning of Bayesian
Neural Networks. In ICML, 2015.

R.E. Kass and A.E. Raftery. Bayes Factors. Journal of

the American Statistical Association, 1995.

A. Kendall and Y. Gal. What Uncertainties Do We Need
in Bayesian Deep Learning for Computer Vision? In
NIPS, 2017.

D.P. Kingma, T. Salimans, and M. Welling. Variational
Dropout and The Local Reparameterization Trick. In
NIPS, 2015.

B. Lakshminarayanan, A. Pritzel, and C/ Blundell. Simple
and Scalable Predictive Uncertainty Estimation using
Deep Ensembles. In NIPS, 2017.

J. Lee, Y. Bahri, R. Novak, S. Schoenholz, J. Penning-
ton, and J. Sohl-Dickstein. Deep Neural Networks as
Gaussian Processe. In ICLR, 2018.

C. Louizos and M. Welling. Multiplicative Normalizing
Flows for Variational Bayesian Neural Networks. In
ICML, 2017.

D.J. MacKay. Probable Networks and Plausible Predic-
tions – A Review of Practical Bayesian Methods for
Supervised Neural Networks. Network: Computation
in Neural Systems, 6(3):469–505, 1995.

A. Malinin and M. Gales. Predictive uncertainty estima-

tion via prior networks. In NeurIPS, 2018.

A. G. de G. Matthews, M. Rowland, J. Hron, R. E. Turner,
and Z. Ghahramani. Gaussian process behaviour in
wide deep neural networks. In ICLR, 2018.

D. McAllester. PAC-Bayesian Model Averaging.

In

COLT, 1999.

D. McAllester. PAC-Bayesian Stochastic Model Selection.

Machine Learning, 51:5–21, 2003.

D. Molchanov, A. Ashukha, and D. Vetrov. Variational
Dropout Sparsiﬁes Deep Neural Networks. In ICML,
2017.

R. Neal. Bayesian Learning for Neural Networks. PhD

Thesis, 1995.

J. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and
N. D. Lawrence. Dataset Shift in Machine Learning.
The MIT Press, 2009.

M. Seeger. PAC-Bayesian Generalisation Error Bounds
for Gaussian Process Classiﬁcation. Journal of Ma-
chine Learning Research, 3:233–269, 2002.

M. Sensoy, L. Kaplan, and M. Kandemir. Evidential
Deep Learning to Quantify Classiﬁcation Uncertainty.
In NeurIPS, 2018.

J.T. Springenberg, A. Dosovitskiy, T. Brox, and M. Ried-
miller. Striving for Simplicity: The All Convolutional
Net. In ICLR, 2015.

S.I. Wang and C.D. Manning. Fast Dropout Training. In

ICML, 2013.

A. Wu, S. Nowozin, E. Meeds, R.E. Turner, J.M.
Hernández-Lobato, and A.L. Gaunt. Deterministic
Variational Inference for Bayesian Neural Networks.
In ICLR, 2019.

APPENDIX

A BEDL WITH HYPERPRIORS

Instead of relying on the PAC-bound based could try to
incorporate further hyper-priors on φ. This would hope-
fully reintroduce the missing regularization BEDL faces.
The hierarchical model then has the following structure:

φ ∼ p(φ),

w|φ ∼

pφ(wn),

λ|w, x ∼

p(λn|wn, xn),

y|λ ∼

p(yn|λn).

(cid:89)

n
(cid:89)

n
(cid:89)

n

The marginal to be optimized over is then given as

log p(y, φ|X) =

p(yn|λn)p(λn|wn, xn)

(cid:90)

(cid:88)

log

n

· p(wn|φ)dλndwn + log p(φ).

The ﬁrst term is our regular marginal likelihood, while
the second serves as as a regularizer as an optimization
scheme aims to choose φ such that the marginal likelihood
is high, but also that the prior density is large. The form
of this hyperprior will vary depending on the problem at
hand, but if we consider e.g. the i-th weight of the BNN
wi

n to follow a normal distribution, we have

wi

n|φi ∼ N (wi

n|µi, σ2

i ), where φi = (µi, σ2

i ).

An obvious choice for the prior p(φi) is then given as

p(φi) = p(µi)p(σ2
i )
= N (µ|0, α−1
0 )InvGam(σ2|a0, b0).

The regression results summarized in Table 1 however
show that this approach tends to perform worse than both
the PAC regularized BEDL as well as the unregularized
BEDL.

B FURTHER DETAILS ON THE BLNN

DERIVATION

|µ, σ2) for

k

k

k

|µ,σ2)

(cid:3) = E

E (cid:2)hl−1

∼ N (f l−1

k) where f l−1
hl−1
k = max(0, f l
some mean and variance they can be computed as
(cid:2)max(0, f l−1
)(cid:3)
N (f l−1
k
(cid:17)
(cid:16) µ
(cid:17)
(cid:16) µ
= µΦ
σ
σ
(cid:3) = var (cid:2)max(0, f l−1
)(cid:3)
k
(cid:17)
(cid:16) µ
σ
− (cid:0)E (cid:2)hl−1

= (µ2 + σ2)Φ
(cid:17)

var (cid:2)hl−1

+ µσφ

+ σφ

(cid:3) (cid:1)2

k

k

,

,

k

(cid:16) µ
σ

where Φ(·) and φ(·) are the cdf and pdf of the standard
normal distribution respectively.

B.2 FIRST TWO MOMENTS OF THE ELU

ACTIVATION

The following derivations are an adaptation of the ReLU
results to ELU in order to scale to deeper networks. We
are again interested in the ﬁrst two moments for the ELU
deﬁned as

(cid:40)

g(x) =

x,
x > 0
α(exp(x) − 1) x < 0

.

(18)

With f (x) = max(0, x), i.e. the ReLU activation, we
have for the expectation of g(·), that

E [g(x)] =

α(exp(x)−1)N (x|µ, σ2)dx+E [f (x)] .

(cid:90) 0

−∞

The ﬁrst term can be split into

(cid:90) 0

α

−∞

exp(x)N (x|µ, σ2)dx − α

N (x|µ, σ2).

(cid:90) 0

−∞

We get for these to terms that the ﬁrst is equal to

(cid:90) 0

α

−∞

exp(x)N (x|µ, σ2)dx

= α exp(µ + σ2/2)Φ

−

(cid:18)

(cid:19)

µ + σ2
σ

and the second gives

(cid:90) 0

α

−∞

N (x|µ, σ2) = αΦ

(cid:16)

−

(cid:17)

µ
σ

(cid:16)

= α

1 − Φ

(cid:17)(cid:17)

.

(cid:16) µ
σ

Combining all of this we end up with

B.1 FIRST TWO MOMENTS OF THE ReLU

ACTIVATION

E [g(x)] = α

exp(µ + σ2/2)Φ

−

(cid:18)

(cid:18)

µ + σ2
σ

(cid:19)

(cid:16)

− Φ

−

(cid:17)(cid:19)

µ
σ

Mean and variance of a normally distributed variable
transformed by the ReLU activation function are ana-
lytical tractable (Frey and Hinton, 1999). Following
the notation from the main paper, we have that for

+ E [f (x)] .

For the variance we have the general form

var [g(x)] = E (cid:2)g(x)2(cid:3) − E [g(x)]2

(cid:90) 0

α2

−∞
(cid:90) − µ

σ

−∞

(cid:18)

in which only the second moment is missing. We have
that

E (cid:2)g(x)2(cid:3) =

α2(exp(x) − 1)2N (x|µ, σ2)dx

(cid:90) 0

−∞

+ E (cid:2)f (x)2(cid:3) .

The ﬁrst term expands into the following monstrosity

(exp(2x) − 2 exp(x) + 1)N (x|µ, σ2)dx

= α2

(cid:0) exp(2µ + 2σy) − 2 exp(µ + σy) + 1(cid:1)

φ(y)dy

= α2

exp(2µ + 2σ2)Φ

−2 exp(µ + σ2/2)Φ

(cid:18)

(cid:19)

−

(cid:18)

µ + 2σ2
σ
µ + σ2
σ

−

(cid:19)

(cid:16)

+ Φ

−

(cid:17)(cid:19)

.

µ
σ

These two moments ﬁnally can be used as replacements
for the ReLU moments in the main paper for deeper net-
works.

B.3 DERIVATION OF THE

MARGINALIZATION FOR REGRESSION

We approximate the marginal distribution

(cid:90)

p(λn) =

N (cid:0)λn|f L

n1, exp(f L

n2)(cid:1)N (f L

n |mn, s2

n)df L
n

with a normal distribution by a further moment match-
ing step. Dropping the indices n and L for notational
simplicity, the mean of the right hand side is given as

Ep(λ) [λ] =

λp(λ)dλ

(cid:90)

(cid:90)

(cid:90)

=

=

λN (λ|f1, exp(f2))N (f |m, s2)df dλ

f1N (f |m, s2)df = m1.

For the variance term we rely on the law of total variance
and have

varp(λ) [λ] = Ep(f )

(cid:2)varp(λ|f ) [λ](cid:3)
(cid:2)Ep(λ|f ) [λ](cid:3)
= Ep(f ) [exp(f2)] + varp(f ) [f1]

+ varp(f )

(cid:90)

=

exp(f2)N (f2|m2, σ2

2)df2 + s2
1

= exp(m2 + s2

2/2) + s2
1,

where the last integral is given as the mean of a log-normal
random variable. Altogether we end up with the desired

p(λn) ≈ N

(cid:16)

(cid:12)
(cid:12)mn1, s2

λn

n1 + exp(mn2 + s2

n2/2)

(cid:17)

.

This then allows us to compute the log marginal likelihood

log p(yn|xn) = log

N (yn|λn, β−1)

(cid:90)

n1, exp(f L

n2)(cid:1)N (f L

n |mn, s2

n)df L
n

dλn

(cid:19)

(cid:18)(cid:90)

·

N (cid:0)λn|f L
(cid:90)

≈ log

N (yn|λn, β−1)

(cid:16)

· N

= log N

λn
(cid:16)

(cid:12)
(cid:12)mn1, s2
(cid:12)
(cid:12)mn1, β−1 + s2
n1

yn

n1 + exp(mn2 + s2

n2/2)

dλn

(cid:17)

+ exp(mn2 + s2

n2/2)

(cid:17)

.

C DERIVATION OF THE PAC-BOUND

This section gives a more detailed derivation of the indi-
vidual results stated in the main paper. As stated there,
given a predictor h chosen from a hypothesis class H as a
mapping from x to y, we deﬁne the true and the empirical
risks as

R(h) = −Ex,y∼∆

(cid:2)p(cid:0)y|h(x)(cid:1)(cid:3) ,

RD(h) = −

p(cid:0)yn|h(xn)(cid:1)

1
N

N
(cid:88)

n=1

(19)

(20)

for the data set D drawn from an arbitrary and unknown
data distribution ∆. R(h) and RD(h) are bounded below
by − max p(y|h(x)) and above by zero.

Theorem 2.1 in (Germain et al., 2009) gives us that for
any δ ∈ (0, 1] and any convex function d(·, ·)

(cid:40)

(cid:16)

d

Pr

Eh∼Q [RD(h)] , Eh∼Q [R(h)]

(cid:17)

≤

KL (Q (cid:107) P ) + log(B/δ)
N

(cid:41)
)

≥ 1 − δ,

(21)

(cid:2)Eh∼P

(cid:2)exp (cid:0)N d(RD(h), R(h))(cid:1)(cid:3)(cid:3).
where B = ED∼∆
The PAC framework necessitates a convex and non-
negative distance measure for risk evaluations. Common
practice is to rescale the risk into the unit interval, deﬁne
the KL divergence as the distance measure, and upper
bound its intractable inverse (Germain et al., 2016) using
Pinsker’s inequality (Catoni, 2007; Dziugaite and Roy,
2017). We follow an alternative path. As our risk is
bounded but not restricted to the unit interval, we choose
our distance measure as d(r, r(cid:48)) = (r − r(cid:48))2 and avoid
the Pinsker’s inequality step.

Adapting the standard KL inversion trick (Seeger,
2002) to the Euclidean distance, we can simply deﬁne
d−1(x, ε) = max{x(cid:48) : (x − x(cid:48))2 = ε} = x +
ε for

√

some ε ≥ 0. We apply this function to both sides of the
inequality and get

d−1(cid:16)

Eh∼Q [RD(h)] ,
d(cid:0)Eh∼Q [RD(h)] , Eh∼Q [R(h)] (cid:1)(cid:17)

≤ d−1(cid:16)

Eh∼Q [RD(h)] ,

(cid:0)KL(Q||P ) + log(B/δ)(cid:1)/N

(cid:17)

,

where d(·, ·) ≥ 0 and KL(Q||P ) ≥ 0 by deﬁnition and
because δ ∈ [0, 1] and ed(·,·) ≥ 0 we have log(B/δ) =
− log δ + log B ≥ 0 . Since

Eh∼Q [R(h)] ≤

Eh∼Q [R(h)] + d

Eh∼Q [RD(h)] , Eh∼Q [R(h)]

(cid:16)

(cid:17)

directly follows from d−1(·, ·), we bound the true risk as

(cid:40)

Pr

Eh∼Q [R(h)] ≤ Eh∼Q [RD(h)]

(cid:114)

+

KL(Q||P ) + log(B/δ)
N

(cid:41)

≥ 1 − δ.

This outcome has a similar structure to application of
Pinsker’s inequality to a setup with risk deﬁned on the
unit interval, but without such a restriction. Hence, the
implied upper bound is no longer trivial. To arrive at the
ﬁnal bound we have to further approximate each of the
two terms of the bound.

For the ﬁrst term, we have that

Eh∼Q [RD(h)] = −

Eh∼Q

(cid:2)p(cid:0)yn|h(xn)(cid:1)(cid:3)

1
N

1
N

1
N

N
(cid:88)

n=1

N
(cid:88)

n=1

N
(cid:88)

n=1

≤ −

log Eh∼Q

(cid:2)p(cid:0)yn|h(xn)(cid:1)(cid:3)

= −

log p(yn|xn),

where the inequality uses that − log(u) > −u ∀u ∈
(0, ∞) and the equality follows by the marginalisation
techniques discussed in the main paper.

To get a tractable second term we process B further. Ex-
ploiting the fact that

Ex∼p(x) [max f (x)] ≤ max f (x)

(22)

for any p(·) and f (·), we can drop the the expectation
term and get

B = ED∼∆

Eh∼P

(cid:104)

eN (RD(h)−R(h))2 (cid:105)(cid:105)
(cid:104)

≤ max eN (RD(h)−R(h))2

.

(23)

For a multiclass classiﬁcation, the likelihood is bounded
into the interval [0, 1] such that with max(RD(h) −
R(h))2 = (0 − 1)2 = 1 we have that

B ≤ eN ⇒ log B ≤ N.

(24)

For regression, with the likelihood N (y|h(x), β−1), we
have that RD(h) and R(h) are bounded from above by
0 and from below by −N (cid:0)y = µ|µ, β−1(cid:1), i.e. by the
density at the mode of a normal distribution with precision
β−1. Hence,

B ≤ eN(cid:0)0−N (µ,β−1)(cid:1)2

⇒ log B ≤ N

(25)

β
2π

.

Combining these relaxations we get the objectives de-
scribed in the main paper.

D EXPERIMENTAL DETAILS AND
FURTHER EXPERIMENTS

This section contains experiments on synthetic toy data
as well as details on the hyperparameters of the experi-
ments performed in the main paper. See github.com/
manuelhaussmann/bedl for a pytorch implementa-
tion to reproduce the reported results.

D.1 REGRESSION

The neural net used consists of a single hidden layer of 50
units for all data sets except protein, which gets 100.
The results for all of the baselines except for Variational
Dropout (VarOut) are quoted from the results reported
by the respective papers who introduced them, while the
results on the sparse GP are reported via (Bui et al., 2016).
For VarOut we rely on our own implementation as there
are no ofﬁcial results. BEDL, BEDL+Reg, and VarOut
all share the same initialization scheme for the mean and
variance parameters for each weight following the initial-
ization of (Louizos and Welling, 2017), i.e. He-Normal
for the means and N (−9, 0.001) for the log variances.
VarOut gets a Normal prior with a precision of α = 1.0,
and all three get an observation precision of β = 100,
to encourage them to learn as much of the predictive un-
certainty instead of relying on a ﬁxed hyper-parameter.
Note that we keep these values ﬁx and data set indepen-
dent, different to many of the baselines who set them to
data set speciﬁc values given cross-validations on separate
validation subsets.

Each model is trained with the Adam optimizer with de-
fault parameters for 100 epochs with a learning rate of
10−3, with varying minibatch sizes depending on the data
set size.

D.2 CLASSIFICATION AND OUT-OF-DOMAIN

DETECTION

The network for this task follows the common LeNet5
architecture with the following modiﬁcations. Instead
of max-pooling layers after the two convolutional layers,
the convolutional layers themselves use a larger stride to
mimic the behavior. And for the more complex CIFAR
data set the number of channels in the two convolutional
layers is increased from the default 20,50 to 192 each,
while the number of hidden units for the fully connected
layer is increased from 500 to 1000 for that data set fol-
lowing (Gal and Ghahramani, 2015).

Since there are no OOD results on the BNN baselines we
compare against, we rely on our own reimplementations
of them, ensuring that they each share the same initial-
ization schemes as in the regression setup. For DVI we
implement the diagonal version and use a sampling-based
approximation on the intractable softmax. Each model
gets access to ﬁve samples whenever it needs to conduct
an MC sampling approximation. All models get trained
via the Adam optimizer with the default hyperparameters
and a learning rate of 10−3. For EDL we rely on the
public implementation the authors (Sensoy et al., 2018)
provide and use their hyperparameters to learn the model.

D.3 GP VARIANTS COMPARISON

The results for the baselines are taken from the respective
original papers. The nets for BEDL and BEDL+Reg
consist of two convolutional layers with 96 ﬁlters of size
5 × 5 and a stride of 5. They are trained until convergence
(50 epochs) using Adam with the default hyperparameters
and a learning rate of 10−3.

