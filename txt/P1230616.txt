9
1
0
2
 
t
c
O
 
3
1
 
 
]

G
L
.
s
c
[
 
 
2
v
1
7
5
6
0
.
8
0
9
1
:
v
i
X
r
a

Under review

POLYGAN: HIGH-ORDER POLYNOMIAL GENERATORS

Grigorios Chrysos1, Stylianos Moschoglou1, Yannis Panagakis1,2, Stefanos Zafeiriou1
Imperial College London1
Middlesex University2
{g.chrysos, s.moschoglou, i.panagakis, s.zafeiriou}@imperial.ac.uk

ABSTRACT

Generative Adversarial Networks (GANs) have become the gold standard when
it comes to learning generative models for high-dimensional distributions. Since
their advent, numerous variations of GANs have been introduced in the literature,
primarily focusing on utilization of novel loss functions, optimization/regularization
strategies and network architectures. In this paper, we turn our attention to the
generator and investigate the use of high-order polynomials as an alternative class
of universal function approximators. Concretely, we propose PolyGAN, where we
model the data generator by means of a high-order polynomial whose unknown
parameters are naturally represented by high-order tensors. We introduce two
tensor decompositions that signiﬁcantly reduce the number of parameters and show
how they can be efﬁciently implemented by hierarchical neural networks that only
employ linear/convolutional blocks. We exhibit for the ﬁrst time that by using our
approach a GAN generator can approximate the data distribution without using any
activation functions. Thorough experimental evaluation on both synthetic and real
data (images and 3D point clouds) demonstrates the merits of PolyGAN against
the state of the art.

1

INTRODUCTION

Generative Adversarial Networks (GANs) are currently one of the most popular lines of research in
machine learning. Research on GANs mainly revolves around: (a) how to achieve faster and/or more
accurate convergence (e.g., by studying different loss functions (Nowozin et al., 2016; Arjovsky &
Bottou, 2017; Mao et al., 2017) or regularization schemes (Odena et al., 2018; Miyato et al., 2018;
Gulrajani et al., 2017)), and (b) how to design different hierarchical neural networks architectures
composed of linear and non-linear operators that can effectively model high-dimensional distributions
(e.g., by progressively training large networks (Karras et al., 2018) or by utilizing deep ResNet type
of networks as generators (Brock et al., 2019)).

Even though hierarchical deep networks are efﬁcient universal approximators for the class of con-
tinuous compositional functions (Mhaskar et al., 2016), the non-linear activation functions pose
difﬁculties in their theoretical analysis, understanding, and interpretation. For instance, as illustrated
in Arora et al. (2019), element-wise non-linearities pose a challenge on proving convergence, espe-
cially in an adversarial learning setting (Ji & Liang, 2018). Consequently, several methods, e.g., Saxe
et al. (2014); Hardt & Ma (2017); Laurent & Brecht (2018); Lampinen & Ganguli (2019), focus only
on linear models (with respect to the weights) in order to be able to rigorously analyze the neural
network dynamics, the residual design principle, local extrema and generalization error, respectively.
Moreover, as stated in the recent in-depth comparison of many different GAN training schemes
(Lucic et al., 2018), the improvements may mainly arise from a higher computational budget and
tuning and not from fundamental architectural choices.

In this paper, we depart from the choice of hierarchical neural networks that involve activation
functions and investigate for the ﬁrst time in the literature of GANs the use of high-order polynomials
as an alternative class of universal function approximators for data generator functions. This choice
is motivated by the strong evidence provided by the Stone–Weierstrass theorem (Stone, 1948), which
states that every continuous function deﬁned on a closed interval can be uniformly approximated as
closely as desired by a polynomial function. Hence, we propose to model the vector-valued generator

1

Under review

function Gpzq : Rd Ñ Ro by a high-order multivariate polynomial of the latent vector z, whose
unknown parameters are naturally represented by high-order tensors.

However, the number of parameters required to accommodate all higher-order correlations of the
latent vector explodes with the desired order of the polynomial and the dimension of the latent vector.
To alleviate this issue and at the same time capture interactions of parameters across different orders
of approximation in a hierarchical manner, we cast polynomial parameters estimation as a coupled
tensor factorization (Papalexakis et al., 2016; Sidiropoulos et al., 2017) that jointly factorizes all the
polynomial parameters tensors. To this end, we introduce two speciﬁcally tailored coupled canonical
polyadic (CP)-type of decompositions with shared factors. The proposed coupled decompositions of
the parameters tensors result into two different hierarchical structures (i.e., architectures of neural
network decoders) that do not involve any activation function, providing an intuitive way of generating
samples with an increasing level of detail. This is pictorially shown in Figure 1. The result of the
proposed PolyGAN using a fourth-order polynomial approximator is shown in Figure 1 (a), while
Figure 1 (b) shows the corresponding generation when removing the fourth-order power from the
generator.

Our contributions are summarized as follows:

• We model the data generator with a high-order polynomial. Core to our approach is to
cast polynomial parameters estimation as a coupled tensor factorization with shared factors.
To this end, we develop two coupled tensor decompositions and demonstrate how those
two derivations result in different neural network architectures involving only linear (e.g.,
convolution) units. This approach reveals links between high-order polynomials, coupled
tensor decompositions and network architectures.

• We experimentally verify that the resulting networks can learn to approximate functions

with analytic expressions.

• We show how the proposed networks can be used with linear blocks, i.e., without utilizing

activation functions, to synthesize high-order intricate signals, such as images.

• We demonstrate that by incorporating activation functions to the derived polynomial-based
architectures, PolyGAN improves upon three different GAN architectures, namely DC-
GAN (Radford et al., 2015), SNGAN (Miyato et al., 2018) and SAGAN (Zhang et al.,
2019).

(a)

(b)

Figure 1: Generated samples by an instance of the proposed PolyGAN. (a) Generated samples using
a fourth-order polynomial and (b) the corresponding generated samples when removing the terms
that correspond to the fourth-order. As evidenced, by extending the polynomial terms, PolyGAN
generates samples with an increasing level of detail.

2 METHOD

In this Section, we investigate the use of a polynomial expansion as a function approximator for the
data generator in the context of GANs. To begin with, we introduce the notation in Section 2.1. In
Section 2.2, we introduce two different polynomials models along with speciﬁcally tailored coupled
tensor factorizations for the efﬁcient estimation of their parameters.

2.1 PRELIMINARIES AND NOTATION

Matrices (vectors) are denoted by uppercase (lowercase) boldface letters e.g., X, (x). Tensors are
denoted by calligraphic letters, e.g., X . The order of a tensor is the number of indices needed to

2

Under review

ś

.
“ xi1,i2,...,iM .

address its elements. Consequently, each element of an M th-order tensor X is addressed by M
indices, i.e., pX qi1,i2,...,iM
The mode-m unfolding of a tensor X P RI1ˆI2ˆ¨¨¨ˆIM maps X to a matrix Xpmq P RImˆ ¯Im with
¯Im “
Ik such that the tensor element xi1,i2,...,iM is mapped to the matrix element xim,j
In. The mode-m vector product of X with a
where j “ 1 `
vector u P RIm, denoted by X ˆn u P RI1ˆI2ˆ¨¨¨ˆIn´1ˆIn`1ˆ¨¨¨ˆIN , results in a tensor of order
M ´ 1:

pik ´ 1qJk with Jk “

k´1
n“1
n‰m

M
k“1
k‰m

M
k“1
k‰m

ś

ř

Imÿ

im“1

pX ˆm uqi1,...,im´1,im`1,...,iM “

xi1,i2,...,iM uim .

(1)

ś

m

“ X

m“1 ˆmupmq.

Furthermore, we denote X ˆ1 up1q ˆ2 up2q ˆ3 ¨ ¨ ¨ ˆM upM q .
The Khatri-Rao product (i.e., column-wise Kronecker product) of matrices A P RIˆN and B P
RJˆN is denoted by A d B and yields a matrix of dimensions pIJq ˆ N . The Hadamard product of
A P RIˆN and B P RIˆN is deﬁned as A ˚ B and is equal to Api,jqBpi,jq for the pi, jq element.
The CP decomposition (Kolda & Bader, 2009; Sidiropoulos et al., 2017) factorizes a tensor into
a sum of component rank-one tensors. An M th-order tensor X P RI1ˆI2ˆ¨¨¨ˆIM has rank-1,
when it is decomposed as the outer product of M vectors tupmq P RImuM
m“1. That is, X “
up1q ˝ up2q ˝ ¨ ¨ ¨ ˝ upM q .
m“1xpmq, where ˝ denotes for the vector outer product. Consequently,
the rank-R CP decomposition of an M th-order tensor X is written as:

“ (cid:13)M

X

.
“ rrUr1s, Ur2s, . . . , UrM sss “

up1q

r ˝ up2q

r ˝ ¨ ¨ ¨ ˝ upM q

r

,

Rÿ

r“1

(
M
where the factor matrices
m“1 collect the vectors from
the rank-one components. By considering the mode-1 unfolding of X , the CP decomposition can be
written in matrix form as (Kolda & Bader, 2009):

(cid:32)
Urms “ rupmq

R s P RImˆR

, ¨ ¨ ¨ , upmq

, upmq
2

1

ˆ

X p1q “ Ur1s

UrM s d UrM ´1s d ¨ ¨ ¨ d Ur2s

˙

T

.
“ Ur1s

ˆ

2ä

˙

T

Urms

m“M

(2)

(3)

More details on tensors and multilinear operators can be found in Kolda & Bader (2009); Sidiropoulos
et al. (2017).

2.2 HIGH-ORDER POLYNOMIAL GENERATORS

GANs typically consist of two deep networks, namely a generator G and a discriminator D. G is
a decoder (i.e., a function approximator of the sampler of the target distribution) which receives as
input a random noise vector z P Rd and outputs a sample x “ Gpzq P Ro. D receives as input both
Gpzq and real samples and tries to differentiate the fake and the real samples. During training, both
G and D compete against each other till they reach an “equilibrium” (Goodfellow et al., 2014). In
practice, both the generator and the discriminator are modeled as deep neural networks, involving
composition of linear and non-linear operators (Radford et al., 2015).

Table 1: Nomenclature

Symbol

n, N
k
z
C, β
Arns, Srns, Brns
d, ˚

Dimension(s)
N
N
Rd
Roˆk, Ro

Deﬁnition

Polynomial term order, total approximation order.
Rank of the decompositions.
Input to the polynomial approximator, i.e., generator.
Parameters in both decompositions.

Rdˆk, Rkˆk, Rωˆk Matrix parameters in the hierarchical decomposition.

-

Khatri-Rao product, Hadamard product.

3

Under review

In this paper, we focus on the generator. Instead of modeling the generator as a composition of linear
and non-linear functions, we assume that each generated pixel xi “ pGpzqqi may be expanded as a
N th order polynomial1 in z. That is,

Nź

T

i

z ` zT W r2s

xi “ pGpzqqi “ βi ` wr1s

i z ` W r3s
i ˆ1 z ˆ2 z ˆ3 z ` ¨ ¨ ¨ ` W rN s
(cid:32)
(
N
W rns
where the scalar βi, and the set of tensors
n“1 are the parameters of the
i
polynomial expansion associated to each output of the generator, e.g., pixel. Clearly, when n “ 1,
the weights are d-dimensional vectors; when n “ 2, the weights, i.e., W r2s
, form a d ˆ d matrix. For
higher orders of approximation, i.e., when n ě 3, the weights are nth order tensors.

ˆnz, (4)

n
m“1 ˆmd

P R

n“1

ś

i

i

By stacking the parameters for all pixels, we deﬁne the parameters β
(cid:32)
W rns P Roˆ

.
“ rβ1, β2, . . . , βosT P Ro and
(
N
n“1. Consequently, the vector-valued generator function is expressed as:
˙

n
m“1 ˆmd

ˆ

ś

Gpzq “

W rns

ˆjz

` β

Nÿ

n“1

n`1ź

j“2

(5)

Intuitively, (5) is an expansion which allows the N th order interactions between the elements of
the noise latent vector z. Furthermore, (5) resembles the functional form of a truncated Maclaurin
expansion of vector-valued functions. In the case of a Maclaurin expansion, W rns represent the nth
order partial derivatives of a known function. However, in our case the generator function is unknown
and hence all the parameters need to be estimated from training samples.
The number of the unknown parameters in (5) is pdN `1 ´ 1q o
d´1 , which grows exponentially with
the order of the approximation. Consequently, the model of (5) is prone to overﬁtting and its training
is computationally demanding.

A natural approach to reduce the number of parameters is to assume that the weights exhibit re-
dundancy and hence the parameter tensors are of low-rank. To this end, several low-rank tensor
decompositions can be employed (Kolda & Bader, 2009; Sidiropoulos et al., 2017). For instance, let
the parameter tensors W rns admit a CP decompostion (Kolda & Bader, 2009) of mutilinear rank-k,
namely, tW rns “ rrUrns,1, Urns,2, . . . , Urns,pn`1qssuN
n“1, with Urns,1 P Roˆk, and Urns,m P Rdˆk,
for m “ 2, . . . , n ` 1. Then, (5) is expressed as

Gpzq “

rrUrns,1, Urns,2, . . . , Urns,pn`1qss

ˆjz

` β,

(6)

ˆ

Nÿ

n“1

˙

n`1ź

j“2

which has signiﬁcantly less parameters than (5), especially when k ! d. However, a set of different
factor matrices for each level of approximation are required in equation 6, and hence the hierarchical
nature of images is not taken into account. To promote compositional structures and capture inter-
actions among parameters in different orders of approximation we introduce next two coupled CP
decompositions with shared factors.

Model 1: Coupled CP decomposition:

Instead of factorizing each parameters tensor individually we propose to jointly factorize all the
parameter tensors using a coupled CP decomposition with a speciﬁc pattern of factor sharing. To
illustrate the factorization, we assume a third order approximation (N “ 3), however in the appendix
a generalization to N -th order approximation is provided. Let us assume that the parameters tensors
admit the following coupled CP decomposition with the factors corresponding to lower-order levels
of approximation being shared across all parameters tensors. That is:

• Let W r1s “ CU T

r1s, be the parameters for ﬁrst level of approximation.
• Let assume W r2s being a superposition of of two weights tensors, namely W r2s “ W r2s

1:2 `
i:j denoting parameters associated with the second order interactions across

1:3, with W r2s

W r2s

1With an N th order polynomial we can approximate any smooth function (Stone, 1948).

4

Under review

Figure 2: Schematic illustration of the Coupled CP decomposition (for third order approximation).
Symbol ˚ refers to the Hadamard product.

the i-th and j-th order of approximation. By enforcing the CP decomposition of the above
tensors to share the factor with tensors corresponding to lower-order of approximation we
obtain in matrix form: W r2s

p1q “ CpUr3s d Ur1sqT ` CpUr2s d Ur1sqT .

• Similarly, we enforce the third-order parameters tensor to admit the following CP decompo-
sition (in matrix form) W r3s
p1q “ CpUr3s d Ur2s d Ur1sqT . Note that all but the Ur3s factor
matrices are shared in the factorization of tensors capturing polynomial parameters for the
ﬁrst and second order of approximation.

The parameters are C P Roˆk, Urms P Rdˆk for m “ 1, 2, 3. Then, (6) for N “ 3 is written as:

Gpzq “ β ` CU T

r1sz ` C

Ur3s d Ur1s

pz d zq ` C

´

¯

T

´

¯

T

pz d zq`

´

Ur2s d Ur1s
¯
T

C

Ur3s d Ur2s d Ur1s

pz d z d zq

(7)

The third order approximation of (7) can be implemented as a neural network with the structure of
Figure 2 (proved in section B, Claim 1 of the appendix). It is worth noting that the structure of the
proposed network allows for incremental network growth.

Model 2: Coupled nested CP decomposition: Instead of explicitly separating the interactions
between layers, we can utilize a joint hierarchical decomposition on the polynomial parameters. Let
(
N
n“1, which act as scaling factors for each
us ﬁrst introduce learnable hyper-parameters
parameter tensor. Therefore, we modify (5) to:

(cid:32)
brns P Rω

Gpzq “

W rns ˆ2 brns

ˆjz

` β,

(8)

ˆ

Nÿ

n“1

˙

n`2ź

j“3

(cid:32)
W rns P Roˆωˆ

n
m“1 ˆmd
with
approximation (N “ 3). That is,

ś

(
N
n“1. For illustration purposes, we consider a third order function

Gpzq “ β ` W r1s ˆ2 br1s ˆ3 z ` W r2s ˆ2 br2s ˆ3 z ˆ4 z ` W r3s ˆ2 br3s ˆ3 z ˆ4 z ˆ5 z (9)

To estimate its parameters we jointly factorize all parameters tensors by employing nested CP
detecomposion with parameter sharing as follows (in matrix form)

• First order parameters : W r1s

p1q “ CpAr3s d Br3sqT .

• Second order parametes: W r2s

Ar3s d

Ar2s d Br2s

Sr3s

"

p1q “ C
"

„´

„ˆ

¯

*

T

.

¯

!´

)˙

*

T

• Third order parameters: W r3s

p1q “ C

Ar3s d

Ar2s d

Ar1s d Br1s

Sr2s

Sr3s

5

Under review

with C P Roˆk, Arns P Rdˆk, Srns P Rkˆk, Brns P Rωˆk for n “ 1, . . . , N . Altogether, (9) is
written as:

Gpzq “ β ` CpAr3s d Br3sqT pz d br3sq ` C

Ar3s d

z d z d br2s

"

„ˆ

"

!´

„´

¯

*

T ´

Ar2s d Br2s
)˙

¯

Sr3s
*

T ´

C

Ar3s d

Ar2s d

Ar1s d Br1s

Sr2s

Sr3s

z d z d z d br1s

¯

`

¯

(10)

As we prove in the appendix (section B, Claim 3), (10) can be implemented in a hierarchical manner
with a three-layer neural network as shown in Figure 3.

Comparison between the two models: Both models are based on the polynomial expansion, how-
ever there are few differences between those. The Coupled CP decomposition has a simpler expression,
however the Coupled nested CP decomposition relates to standard architectures using hierarchical
composition that has recently yielded promising results in GANs (see Section 3). In the remainder
of the paper, we use the Coupled nested CP decomposition by default; in Section G, we include an
experimental comparison of the two models. The experimental comparison demonstrates that neither
model outperforms the other in all datasets; they perform similarly.

Figure 3: Schematic illustration of the Coupled nested CP decomposition (for third order approxima-
tion). Symbol ˚ refers to the Hadamard product.

3 RELATED WORK

The literature on GANs is vast; we focus only on the works most closely related to ours. The
interested reader can ﬁnd further information in a recent survey (Creswell et al., 2018).

Berthelot et al. (2017) use skip connections to concatenate the noise z in deeper layers in the
generator. The recent BigGAN (Brock et al., 2019) performs a hierarchical composition through skip
connections from the noise z to multiple resolutions of the generator. In their implementation, they
split z into one chunk per resolution and concatenate each chunk (of z) to the respective resolution.

Despite the propagation of the noise z to successive layers, the aforementioned works have substantial
differences from ours. We introduce a well-motivated and mathematically elaborate method to achieve
a more precise approximation with a polynomial expansion. In contrast to the previously mentioned
works, we also do not concatenate the noise with the feature representations, but rather perform
multiplication of the noise with the feature representations, which we mathematically justify.

The work that is most closely related to ours is the recently proposed StyleGAN (Karras et al., 2019),
which is an improvement over the Progressive Growing of GANs (ProGAN) (Karras et al., 2018). As
ProGAN, StyleGAN is a highly-engineered network that achieves compelling results on synthesized
2D images. In order to provide an explanation on the improvements of StyleGAN over ProGAN, the

6

Under review

authors adopt arguments from the style transfer literature (Huang & Belongie, 2017). Nevertheless,
the idea of style transfer proposes to use features from images for conditional image translation, which
is very different to unsupervised samples (image) generation. We believe that these improvements
can be better explained under the light of our proposed polynomial function approximation. That is,
as we show in Figure 1, the Hadamard products build a hierachical decomposition with increasing
level of detail (rather than different styles). In addition, the improvements in StyleGAN (Karras et al.,
2019) are demonstrated by using a well-tuned model. In this paper we showcase that without any
complicated engineering process the polynomial generation can be applied into several architectures
(or any other type of decoders) and consistently improves the performance.

4 EXPERIMENTS

A sequence of experiments in both synthetic data (2D and 3D data manifolds) and higher-dimensional
signals are conducted to assess the empirical performance of the proposed polynomial expansion.
The ﬁrst experiments are conducted on a 2D manifolds that are analytically known (Section 4.1).
Further experiments on three 3D manifolds are deferred to the appendix (Section D). In Section 4.2,
the polynomial expansion is used for synthesizing digits. Experiments on images beyond digits
are conducted in Section E; more speciﬁcally, we experiment with images of faces and natural
scenes. The experiments with such images demonstrate how polynomial expansion can be used for
learning highly complex distributions by using a single activation function in the generator. Lastly,
we augment our polynomial-based generator with non-linearities and show that this generator is at
least as powerful as contemporary architectures.

Apart from the polynomial-based generators, we implemented two variations that are considered
baselines: (a) ‘Concat’: we replace the Hadamard operator with concatenation (used frequently in
recent methods, such as in Brock et al. (2019)), (b) ‘Orig’: the Hadamard products are ditched, while
use br1s Ð z, i.e., there is a composition of linear layers that transform the noise z.

4.1 SYNTHETIC EXPERIMENT ON 2D MANIFOLD

Sinusoidal: We assess the polynomial-based generator on a sinusoidal function in the bounded
domain r0, 2πs. Only linear blocks, i.e., no activation functions, are used in the generator. That
is, all the element-wise non-linearities (such as ReLU’s, tanh) are ditched. The distribution we
want to match is a sin x signal. The input to the generator is z P R and the output is rx, sin xs
with x P r0, 2πs. We assume a 12th order approximation where each Sris, Aris is a fully-connected
layer and Bris is an identity matrix. Each fully-connected layer has width 15. In Figure 4, 2, 000
random samples are synthesized. We indeed verify that in low-dimensional distributions, such as the
univariate sinusoidal, PolyGAN indeed approximates the data distribution quite accurately without
using any non-linear activation functions.

(a) GT

(b) Orig

(c) Concat

(d) PolyGAN

Figure 4: Synthesized data for learning the rx, sin xs signal. No activation functions are used in
the generators. From left to right: (a) the data distribution, (b) ‘Orig’, (c) ‘Concat’, (d) PolyGAN.
Notably, neither ‘Orig’ nor ‘Concat’ can learn to approximate different Taylor terms.

7

Under review

(a) GT

(b) Orig

(c) Concat

(d) PolyGAN

Figure 5: Synthesized data for MNIST with a single activation in the generator. From left to right: (a)
The ground-truth signals, (b) ‘Orig’, (c) ‘Concat’, (d) PolyGAN.

4.2 DIGIT GENERATION WITH LINEAR BLOCKS

The linear generator of the previous section is extended to greyscale images, in which an analytic
expression of the ground-truth distribution remains elusive. To our knowledge, there has not been a
generation of greyscale images based on polynomial expansion in the past.

We capitalize on the expressivity of the recent resnet-based generator (Miyato et al., 2018; Brock et al.,
2019), to devise a new polynomial generator Gpzq : R128 Ñ R32x32. We consider a fourth-order
approximation (as derived in (5)) where Bris is the identity matrix, Sris is a residual block with
two convolutions for i “ 1, . . . , 4. We emphasize that the residual block as well as all layers are
linear, i.e., there are no activation functions. We only add a tanh in the output of the generator
for normalization purposes. The discriminator and the optimization procedure are the same as in
SNGAN; the only difference is that we run one discriminator step per generator step (ndis “ 1). Note
that the ‘Orig’ resnet-based generator resembles the generator of Miyato et al. (2018) in this case.

We perform digit generation (trained on MNIST (LeCun et al., 1998)). In Figure 5, random samples
are visualized for the three compared methods. Note that the two baselines have practically collapsed
into a single number each, whereas PolyGAN does synthesize plausible digits.

To further assist the generation process, we utilize the labels and train a conditional GAN. That is,
the class labels are used for conditional batch normalization. As illustrated in Figure 6, the samples
synthesized are improved over the unsupervised setting. ‘Orig’ and ‘Concat’ still suffer from severe
mode collapse, while PolyGAN synthesizes digits that have different thickness (e.g. 9), style (e.g. 2)
and rotation (e.g. 1).

(a) GT

(b) Orig

(c) Concat

(d) PolyGAN

Figure 6: Conditional digit generation. Note that both ‘Orig’ and ‘Concat’ suffer from severe mode
collapse (details in section 4.2). On the contrary, PolyGAN synthesizes digits that have different
thickness (e.g. 9), style (e.g. 2) and rotation (e.g. 1).

8

Under review

5 CONCLUSION

We express data generation as a polynomial expansion task. We model the high-order polynomi-
als with tensorial factors. We introduce two tailored coupled decompositions and show how the
polynomial parameters can be implemented by hierarchical neural networks, e.g. as generators
in a GAN setting. We exhibit how such polynomial-based generators can be used to synthesize
images by utilizing only linear blocks. In addition, we empirically demonstrate that our polynomial
expansion can be used with non-linear activation functions to improve the performance of standard
state-of-the-art architectures. Finally, it is worth mentioning that our approach reveals links between
high-order polynomials, coupled tensor decompositions and network architectures.

REFERENCES

Martin Arjovsky and Léon Bottou. Towards principled methods for training generative adversarial

networks. In International Conference on Learning Representations (ICLR), 2017.

Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. In International Conference on Learning Representations
(ICLR), 2019.

David Berthelot, Thomas Schumm, and Luke Metz. Began: Boundary equilibrium generative

adversarial networks. arXiv preprint arXiv:1703.10717, 2017.

Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity natural

image synthesis. In International Conference on Learning Representations (ICLR), 2019.

Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A
Bharath. Generative adversarial networks: An overview. IEEE Signal Processing Magazine, 35(1):
53–65, 2018.

Athinodoros S Georghiades, Peter N Belhumeur, and David J Kriegman. From few to many:
Illumination cone models for face recognition under variable lighting and pose. IEEE Transactions
on Pattern Analysis and Machine Intelligence (T-PAMI), (6):643–660, 2001.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems (NIPS), 2014.

Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of wasserstein gans. In Advances in neural information processing systems
(NIPS), pp. 5767–5777, 2017.

Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In International Conference on

Learning Representations (ICLR), 2017.

Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural
information processing systems (NIPS), pp. 6626–6637, 2017.

Emiel Hoogeboom, Rianne van den Berg, and Max Welling. Emerging convolutions for generative

normalizing ﬂows. In International Conference on Machine Learning (ICML), 2019.

Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance nor-
malization. In IEEE Proceedings of International Conference on Computer Vision (ICCV), pp.
1501–1510, 2017.

Kaiyi Ji and Yingbin Liang. Minimax estimation of neural net distance. In Advances in neural

information processing systems (NIPS), pp. 3845–3854, 2018.

Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for
improved quality, stability, and variation. In International Conference on Learning Representations
(ICLR), 2018.

9

Under review

Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In IEEE Proceedings of International Conference on Computer Vision and
Pattern Recognition (CVPR), 2019.

Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):

455–500, 2009.

Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The cifar-10 dataset. online: http://www. cs.

toronto. edu/kriz/cifar. html, 55, 2014.

Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer
learning in deep linear networks. In International Conference on Learning Representations (ICLR),
2019.

Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are

global. In International Conference on Machine Learning (ICML), 2018.

Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to

document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created
equal? a large-scale study. In Advances in neural information processing systems (NIPS), pp.
700–709, 2018.

Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least
squares generative adversarial networks. In IEEE Proceedings of International Conference on
Computer Vision (ICCV), pp. 2813–2821. IEEE, 2017.

Hrushikesh Mhaskar, Qianli Liao, and Tomaso Poggio. Learning functions: when is deep better than

shallow. arXiv preprint arXiv:1603.00988, 2016.

Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International Conference on Learning Representations (ICLR),
2018.

Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Advances in neural information processing systems
(NIPS), pp. 271–279, 2016.

Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B Brown, Christopher Olah, Colin
Raffel, and Ian Goodfellow. Is generator conditioning causally related to gan performance? In
International Conference on Machine Learning (ICML), 2018.

Evangelos E Papalexakis, Tom M Mitchell, Nicholas D Sidiropoulos, Christos Faloutsos, Partha Pra-
tim Talukdar, and Brian Murphy. Turbo-smt: Parallel coupled sparse matrix-tensor factorizations
and applications. Statistical Analysis and Data Mining: The ASA Data Science Journal, 9(4):
269–290, 2016.

Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep

convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015.

Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems
(NIPS), pp. 2234–2242, 2016.

Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks. In International Conference on Learning
Representations (ICLR), 2014.

10

Under review

N. D. Sidiropoulos, L. De Lathauwer, X. Fu, K. Huang, E. E. Papalexakis, and C. Faloutsos.
Tensor decomposition for signal processing and machine learning. IEEE Transactions on Signal
Processing, 65(13):3551–3582, 2017.

Nicholas D Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang, Evangelos E Papalexakis,
and Christos Faloutsos. Tensor decomposition for signal processing and machine learning. IEEE
Transactions on Signal Processing, 65(13):3551–3582, 2017.

Marshall H Stone. The generalized weierstrass approximation theorem. Mathematics Magazine, 21

(5):237–254, 1948.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In IEEE
Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pp.
1–9, 2015.

Lucas Theis, Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative

models. In International Conference on Learning Representations (ICLR), 2016.

Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In
IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 7794–7803, 2018.

Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative

adversarial networks. In International Conference on Machine Learning (ICML), 2019.

11

Under review

Input
Output

:Noise z P Rd, N P N
:x P Ro
1 % global transformation(s) of z.
2 v = Linear(z)
3 % ﬁrst layer.
4 κ “ pUr1sqT v
5 for n=2:N do
6

7

8 end
9 x “ β ` Cκ.

A INTRODUCTION

The appendix is organized as:

Algorithm 1: PolyGAN (model 1).

Algorithm 2: PolyGAN (model 2).

Input
Output

:Noise z P Rd, N P N
:x P Ro
1 % global transformation(s) of z.
2 v = Linear(z)
3 % ﬁrst layer.
´

¯

´

¯

pBr1sqT br1s

˚

pAr1sqT v

4 κ “
5 for n=2:N do
6

7 end
8 x “ β ` Cκ.

% Perform the Hadamard product for the nth
layer.
¯
κ “ κ `

´
pUrnsqT v

˚ κ

% Multiply with the current layer weight Srns
and perform the Hadamard product.
Srnsκ ` pBrnsqT brns
κ “

pArnsqT v

´

´

¯

˚

¯

Table 2: The pseudocode for the two models for N th order polynomial approximation.

• Section B provides the Lemmas and their proofs required for our derivations.
• Section C generalizes the Coupled CP decomposition for N th order expansion.
• Section D extends the experiments to 3D manifolds.
• In Section E, additional experiments on image generation with linear blocks are conducted.
• Comparisons with popular GAN architectures are conducted in Section F. Speciﬁcally, we
utilize three popular generator architectures and devise their polynomial equivalent and
perform comparisons on image generation. We also conduct an ablation study indicating how
standard engineering techniques affect the image generation of the polynomial generator.
• In Section G, a comparison between the two proposed decompositions is conducted on data

distributions from the previous Sections.

B PROOFS

For a set of matrices tXm P RImˆN uN

m“1 the Khatri-Rao product is denoted by:
Mä

X1 d X2 d ¨ ¨ ¨ d XM

Xm

(11)

.
“

m“1

In this section, we prove the following identity connecting the sets of matrices tAνRIν ˆKuN
tBνRIν ˆLuN

ν“1 and

ν“1:

Nä

p
ν“1

Nä

ν“1

AνqT ¨ p

Bνq “ pAT

1 ¨ B1q ˚ pAT

2 ¨ B2q ˚ . . . ˚ pAT

N ¨ BN q

(12)

To demonstrate the simple case with two matrices, we prove ﬁrst the special case with N “ 2.
Lemma 1. It holds that

pA1 d A2qT ¨ pB1 d B2q “ pAT

1 ¨ B1q ˚ pAT

2 ¨ B2q

(13)

Proof. Initially, both sides of the equation have dimensions of K ˆ L, i.e., they match. The pi, jq
element of the matrix product of pAT

1 ¨ B1q is

12

Under review

I1ÿ

k1“1

A1,pk1,iqB1,pk1,jq

Then the pi, jq element of the right hand side (rhs) of (13) is:
I1ÿ

I2ÿ

Erhs “ p

A1,pk1,iqB1,pk1,jqq ¨ p

A2,pk2,iqB2,pk2,jqq “

k2“1

k1“1

I1ÿ

I2ÿ

k1“1

k2“1

pA1,pk1,iqA2,pk2,iqqpBp1,k1,jqB2,pk2,jqq

From the deﬁnition of Khatri-Rao, it is straightforward to obtain the pρ, iq element with ρ “
pk1 ´ 1qI2 ` k2, (i.e. ρ P r1, I1I2s) of A1 d A2 as A1,pk1,iqA2,pk2,iq. Similarly, the pρ, jq element
of B1 d B2 is B1,pk1,jqB2,pk2,jq.

The respective pi, jq element of the left hand side (lhs) of (13) is:

I1I2ÿ

ρ“1

I1ÿ

I2ÿ

k1“1

k2“1

Elhs “

A1,pk1,iqA2,pk2,iqB1,pk1,jqB2,pk2,jq “

A1,pk1,iqA2,pk2,iqB1,pk1,jqB2,pk2,jq “ Erhs

In the last equation, we replace the sum in ρ (ρ P r1, I1I2s) with the equivalent sums in k1, k2.

In a similar manner, we generalize the identity to the case of N ą 2 terms below.
Lemma 2. It holds that

Nä

p
ν“1

Nä

ν“1

AνqT ¨ p

Bνq “ pAT

1 ¨ B1q ˚ pAT

2 ¨ B2q ˚ . . . ˚ pAT

N ¨ BN q

(17)

Proof. The rhs includes the Hadamard products of the matrices AT
(AT

ν ¨ Bν. Each matrix multiplication
ν ¨ Bν) results in a matrix of K ˆ L dimensions. Thus, the rhs is a matrix of K ˆ L dimensions.
The lhs is a matrix multiplication of two Khatri-Rao products. The ﬁrst Khatri-Rao product has
ν Iνq ˆ L. Altogether, the lhs has K ˆ L dimensions.
dimensions K ˆ p

ν Iνq, while the second p
Similarly to the previous Lemma, the pi, jq element of the rhs is:

ś

ś

Erhs “ p

A1,pk1,iqB1,pk1,jqq ¨ p

A2,pk2,iqB2,pk2,jqq . . . p

AN,pkN ,iqBN,pkN ,jqq “

INÿ

kN “1

I1ÿ

k1“1
I1ÿ

I2ÿ

INÿ

k1“1

k2“1

kN “1

I2ÿ

k2“1

. . .

pA1,pk1,iqA2,pk2,iq . . . AN,pkN ,iqqpB1,pk1,jqB2,pk2,jq . . . BN,pkN ,jqq

To proceed with the lhs, it is straightforward to derive that

Nä

ν“1

p

Aνq “ A1,ps1,iqA2,ps2,iq . . . AN,psN ,iq

where s1 “ i and sν is a recursive function of the sν´1.

However, the recursive deﬁnition of sν is summed in the multiplication and we obtain:

Elhs “

. . .

pA1,pk1,iqA2,pk2,iq . . . AN,pkN ,iqqpB1,pk1,jqB2,pk2,jq . . . BN,pkN ,jqq “ Erhs

I1ÿ

I2ÿ

INÿ

k1“1

k2“1

kN “1

(14)

(15)

(16)

(18)

(19)

(20)

13

Under review

B.1 PROOFS FOR MODEL 1

Below, we prove that (7) (main paper) is equivalent to the three-layer neural network as shown in
Figure 2.

´

¯

´

¯

Claim 1. Let ω “

U T

r2sz

˚

U T

r1sz

` U T

r1sz.

Then, the form of (7) is equal to:

"´

¯

*

Gpzq “ β ` C

U T

r3sz

˚ ω ` ω

Proof. Applying Lemma 2 on (7), we obtain:
´

"

Gpzq “ β ` C

U T

r1sz `

U T

r3sz

¯

´

¯

´

¯

´

¯

˚
´

U T

r1sz
¯

`
´

U T

r2sz
¯

˚
´

U T

r1sz
¯*

U T
r3sz
´
ı

˚

U T
r2sz
´
¯

˚

r1sz

U T
¯

`

“
*

"´

¯

”´

¯

´

¯

β ` C

U T

r3sz

˚

U T

r2sz

˚

U T

r1sz

` U T

r1sz

`

U T

r2sz

˚

U T

r1sz

` U T

r1sz

The last equation is the same as (21).

B.2 PROOFS FOR MODEL 2

In Claim 2 and Claim 3, we prove that (10) (main paper) is equivalent to the three-layer neural
network as shown in Figure 3.

ˆ

˙

"

„ˆ

˙

ˆ

˙*

ω “

pAr2sqT z

˚

pBr2sqT br2s ` pSr2sqT

pAr1sqT z

˚

pBr1sqT br1s

(23)

Claim 2. Let

It holds that
"

”´

¯

ı*

T ´

¯

´

¯
T

´

¯

ω “

Ar2s d

Ar1s d Br1s

Sr2s

z d z d br1s

`

Ar2s d Br2s

z d br2s

(24)

Proof. We will prove the equivalence starting from (23) and transform it into (24). From (23):

ω “

¯

´
pAr2sqT z
´

˚

´
pBr2sqT br2s
´

¯
T

¯

´

`

Ar2s d Br2s

z d br2s

"

¯

„ˆ

˙

ˆ

˙*

˚

pAr2sqT z
´
¯
pAr2sqT z

`

pSr2sqT
¯

˚

"„ˆ

pAr1sqT z

˚
˙

pBr1sqT br1s
T ´



“
¯*

Ar1s d Br1s

Sr2s

z d br1s

where in the last equation, we have applied Lemma 1. Applying the Lemma once more in the last
term of (25), we obtain (24).

Claim 3. Let

"ˆ

˙

„

*

λ “ β ` C

pAr3sqT z

˚

pBr3sqT br3s ` pSr3sqT ω

with ω as in Claim 2. Then, it holds for Gpzq of (10) that Gpzq “ λ.

14

(21)

(22)

(25)

(26)

Under review

Proof. Transforming (26) into (10):

"ˆ

˙

ˆ

˙

ˆ

˙

ˆ

˙*

λ “ β ` C

pAr3sqT z
"ˆ

˚

pBr3sqT br3s
ˆ
˙

T

`
˙

pAr3sqT z
ˆ

˚
˙

pSr3sqT ω
ˆ

“
˙*

(27)

β ` C

Ar3s d Br3s

z d br3s

`

pAr3sqT z

˚

pSr3sqT ω

To simplify the notation, we deﬁne M1 “
´

´

¯

Ar2sd
¯

Then, ω “ M T
1

z d z d br1s

` M T
2

z d br2s

. The last term of (27) becomes:

"

”´

¯

ı*

´

¯

Ar1sdBr1s

Sr2s

and M2 “

Ar2sdBr2s

.

ˆ

˙

ˆ

˙

ˆ

˙

„ˆ

˙

T ´

pAr3sqT z

˚

pSr3sqT ω

“

pAr3sqT z

˚

M1Sr3s
ˆ

z d z d br1s
¯
T ´

˙

M2Sr3s
˙

ˆ

T ´

z d br2s

(28)

¯

`

“

¯

„

ˆ

˙

T ´

¯

„

Ar3s d

M1Sr3s

z d z d z d br1s

`

Ar3s d

M2Sr3s

z d z d br2s

Replacing (28) into (27), we obtain (10).

Note that the λ in Claim 3 is the equation behind Figure 3. By proving the claim, we have illus-
trated how the polynomial generator can be transformed into a network architecture for third-order
approximation.

C DERIVATIONS

In this Section, we will show how the Coupled CP decomposition generalizes to the N th order
approximation. It sufﬁces to ﬁnd the decomposition that converts the N th order polynomial into a
network structure (see Alg. 1).
As done in Section 2.2, we capture the nth order interactions by decomposing the parameter tensor
W rns (with 2 ď n ď N ) as:

Nÿ

j1´1ÿ

jn´2´1ÿ

W rns “

. . .

W rns

1:jn´1:...:j1

loooooooooooomoooooooooooon
jn´1“2
j1“n

j2“n´1

pn´1qsums

The term W rns
1:jn´1:...:j1
approximation becomes:

denotes the interactions across the layers 1, jn´1, . . . , j1. The N th order

Gpzq “ β `

"ˆ

Nÿ

Nÿ

j1´1ÿ

jn´2´1ÿ

´

˙ˆ

¯

nä

˙*

n“1

j1“n

j2“n´1

jn´1“2

. . .

W rns

1:jn´1:...:j1

z

(30)

p1q

m“1

By considering the mode-1 unfoding of Coupled CP decomposition (like in Section 2.2), we obtain:

"

Nÿ

Nÿ

j1´1ÿ

jn´2´1ÿ

Gpzq “ β ` C

. . .

"

Nÿ

Nÿ

n“1
j1´1ÿ

j1“n

j2“n´1
jn´2´1ÿ

ˆ´

jn´1“2
¯

´
Urj1s d . . . d Urjn´1s d Ur1s

¯
T

´ nä

¯*

z

“

´

¯

´

¯˙*

m“1

β ` C

. . .

U T

rj1sz

˚ . . . ˚

U T

rjn´1sz

˚

U T

r1sz

“ β ` CxN

n“1

j1“n

j2“n´1

jn´1“2

(29)

(31)

15

Under review

where we use xN as an abbreviation of the sums. In the last equation, we have used Lemma 2
(Section B).
Claim 4. The N th order approximation of (30) can be implemented with a neural network as
described in Alg. 1.

Proof. We will use induction to prove the Claim. For N “ 2, it trivially holds, while the proof for
N “ 3 is provided in Claim 1. Suppose it holds for N th order approximation; we prove below that it
holds for N ` 1th order approximation.
Let us denote the approximation of (30) as GN pzq. The pN ` 1qth order approximation from (30) is:

"ˆ

N `1ÿ

N `1ÿ

j1´1ÿ

jn´2´1ÿ

´

GN `1pzq “ β `

. . .

W rns

1:jn´1:...:j1

n“1

j1“n

"ˆ

j2“n´1
j1´1ÿ

Nÿ

Nÿ

jn´1“2

jn´2´1ÿ

´

β `

. . .

W rns

1:jn´1:...:j1

z

`

(32)

Nÿ

jn´2´1ÿ

´

. . .

W rns

j2“n´1

jn´1“2

n“1

j1“n

j2“n´1
¯

jn´1“2
¯*

´ nä

´

1:jn´1:...:j2:pN `1q

p1q

m“1

z

`

W rN `1s

1:2:...:pN `1q

˙ˆ

¯

nä

˙*

z

“

p1q

¯

m“1

˙ˆ

nä

˙

p1q

¯

m“1
´ N `1ä

p1q

m“1

¯

z

In the last equation, the ﬁrst term in the sums is xN ; for the rest two terms we apply Lemma 2:

GN `1pzq “ β ` CxN ` C

U T

rN `1sz

˚

U T

rN sz

˚ . . . ˚

U T

r2sz

˚

U T

"´

¯

´

¯

´

¯

´

"

Nÿ

Nÿ

jn´2´1ÿ

ˆ´

C

. . .

n“1

j2“n´1

jn´1“2

¯

´

¯

´

¯

´

U T

rN `1sz

˚

U T

rj2sz

˚ . . . ˚

"´

¯

„´

¯

U T

rjn´1sz
¯

´

˚

´

¯*

`

r1sz
¯˙*

¯

*

U T

r1sz

“

(33)

β ` CxN ` C

U T

rN `1sz

˚

U T

rN sz

˚ . . . ˚

U T

r2sz

˚

U T

r1sz

` λ

where

Nÿ

Nÿ

jn´2´1ÿ

ˆ´

¯

´

¯

´

¯˙

U T

rj2sz

˚ . . . ˚

U T

rjn´1sz

˚

U T

r1sz

(34)

λ “

. . .

n“1

j2“n´1

jn´1“2

The term λ is equal to the κ “ pn ´ 1qth order of (31), while there is only a single term for n “ N .
Therefore, (33) is transformed into:

GN `1pzq “ β ` CxN ` C

!´

¯

)

U T

rN `1sz

˚ xN

(35)

which is exactly the form described by Alg. 1. This concludes the induction proof.

D EXPERIMENTS ON SURFACES

Astroid: We implement a superellipse with parametric expression rα cos3 t, α sin3 ts for t P r´α, αs.
This has a more complex distribution and four sharp edges. The random samples are visualized in
Figure 7. PolyGAN models the data distribution accurately in contrast to the two baselines.

We conduct three experiments in which the data distribution is analytically derived. The experiments
are:

16

Under review

(a) GT

(b) Orig

(c) Concat

(d) PolyGAN

Figure 7: Synthesized data for learning the ‘astroid’ signal. No activation functions are used in the
generators.

Sin3D: The data manifold is an extension over the 2D manifold of the sinusoidal experiment
(Section 4.1). The function we want to learn is Gpzq : R2 Ñ R3 with the data manifold described by
the vector rx, y, sinp10 ˚

x2 ` y2qs for x, y P r´0.5, 0.5s.

a

In Figure 8, 20, 000 samples are sampled from the generators and visualized. PolyGAN captures the
data distribution, while ‘Orig’ and ‘Concat’ fail.

(a) GT

(b) Orig

(c) Concat

(d) PolyGAN

Figure 8: Experiment on 3D synthetic data. From left to right: (a) the data distribution, (b) ‘Orig’, (c)
‘Concat’, (d) PolyGAN. As expected, the ‘Orig’ and the ‘Concat’ cannot capture the data distribution.

Swiss roll: The three dimensional vector rt¨sin t, y, t¨cos ts`0.05¨s for t, y P r0, 1s and s „ N p0, 1q
forms the data manifold2. In Figure 9, 20, 000 samples are visualized.

(a) GT

(b) Orig

(c) Concat

(d) PolyGAN

Figure 9: Experiment on 3D synthetic data (‘swiss roll’). From left to right: (a) the data distribution,
(b) ‘Orig’, (c) ‘Concat’, (d) PolyGAN.

Gabriel’s Horn: The three dimensional vector rx, α ¨ cos t
x s for t P r0, 160πs and x P r1, 4s
forms the data manifold. The dependence on both sinusoidal and the function 1
x makes this curve
challenging for a polynomial expansion. In Figure 10, the synthesized samples are plotted. PolyGAN
learns how to generate samples on the manifold despite the fraction in the parametric form.

x , α ¨ sin t

E IMAGE GENERATION WITH LINEAR BLOCKS

Apart from digit generation (Section 4.2), we conduct two experiments on image generation of
face and natural scenes. Since both distributions are harder than the digits one, we extend the

2This is a standard synthetic distribution in popular machine learning frameworks such as scikit-learn.

17

Under review

(a) GT

(b) Orig

(c) Concat

(d) PolyGAN

Figure 10: Synthesized data on ‘Gabriel’s Horn’. From left to right: (a) the data distribution, (b)
‘Orig’, (c) ‘Concat’, (d) PolyGAN.

approximation followed on Section 4.2 by one order, i.e., we assume a ﬁfth-order approximation. We
emphasize that each block is a residual block with no activation functions.

Faces: In the experiment with faces, we utilize as the training samples the YaleB (Georghiades et al.,
2001) dataset. The dataset includes greyscale images of faces under extreme illuminations. We
rescale all of the images into 64 ˆ 64 for our analysis.

Random samples are illustrated in Figure 11. Our method generates diverse images and captures the
case of illuminating either half part of the face, while ‘Orig’ and ‘Concat’ generate images that have
a dark side only on the left and right side, respectively. The difference becomes profound in the ﬁner
details of the face (please zoom in), where both baselines fail to synthesize realistic semantic parts of
the face.

(a) GT

(b) Orig

(c) Concat

(d) PolyGAN

Figure 11: Image generation on faces (YaleB (Georghiades et al., 2001)) for a generator with linear
blocks and a single activation function only on the output (i.e., tan h). Notice that our method can
illuminate either the left or right part of the face, in contrast to ‘Orig’ (and ‘Concat’) which generate
images that have a dark side only on the left (respectively right) side. In addition, both ‘Orig’ and
‘Concat’ fail to capture the ﬁne details of the facial structure (please zoom in for the details).

Natural scenes: We further evaluate the generation of natural images, speciﬁcally by training on
CIFAR10 (Krizhevsky et al., 2014). CIFAR10 includes 50, 000 training images of 32 ˆ 32 ˆ 3
resolution.

In Table 3, we evaluate the standard metrics of Inception Score (IS) and Frechet Inception Distance
(FID) (see more details for the metrics in section F). Our model outperforms both ‘Orig’ and ‘Concat’
by a considerable margin. In Figure 12, some random synthesized samples are presented.

Table 3: IS/FID scores on CIFAR10 (Krizhevsky et al., 2014) with linear blocks.

conditional SNGAN with linear blocks on CIFAR10

Model
Orig
Concat

IS (Ò)
4.47 ˘ 0.21
4.23 ˘ 0.37
PolyGAN 6.43 ˘ 0.11

FID (Ó)
156.67 ˘ 12.29
188.08 ˘ 17.00
53.50 ˘ 2.71

18

Under review

(a) GT

(b) Orig

(c) Concat

(d) PolyGAN

Figure 12: Conditional image generation on CIFAR10 for a generator with linear blocks and a single
activation function. Our approach generates more realistic samples in comparison to the compared
methods, where severe mode collapse also takes place.

F IMAGE GENERATION WITH ACTIVATION FUNCTIONS

To demonstrate the ﬂexibility of the PolyGAN, we utilize three different popular generators. The
three acrhitectures chosen are DCGAN (Radford et al., 2015), SNGAN (Miyato et al., 2018), and
SAGAN (Zhang et al., 2019). Each original generator is converted into a polynomial expansion,
while we use the non-linearities to boost the performance of the polynomial generator. The hyper-
parameters are kept the same as the corresponding baseline. Algorithms 3 and 4 succinctly present
the key differences of our approach compared to the traditional one (in the case of SNGAN, similarly
for other architectures).

In addition to the baseline, we implement the most closely related alternative to our framework,
namely instead of using the Hadamard operator as in Figure 3, we concatenate the noise with the
feature representations at that block. The latter approach is frequently used in the literature (Berthelot
et al., 2017; Brock et al., 2019) (referred as “Concat” in the paper). The number of the trainable
parameters of the generators are reported in Table 13. Our method has only a minimal increase of the
parameters, while the concatenation increases the number of parameters substantially.

To reduce the variance often observed during GAN training (Lucic et al., 2018; Odena et al., 2018),
each reported score is averaged over 10 runs utilizing different seeds. The metrics we utilize are
Inception Score (IS) (Salimans et al., 2016) and Frechet Inception Distance (FID) (Heusel et al.,
2017).

Below, we perform an ablation study on Section F.2, and then present the experiments on unsupervised
(Section F.3) and conditional image generation (Section F.4) respectively.

Datasets: We use CIFAR10 (Krizhevsky et al., 2014) and Imagenet (Russakovsky et al., 2015) as the
two most widely used baselines for GANs:

• CIFAR10 (Krizhevsky et al., 2014) includes 60, 000 images of 32 ˆ 32 resolution. We use

50, 000 images for training and the rest for testing.

• Imagenet (Russakovsky et al., 2015) is a large scale dataset that includes over one million
training images and 50, 000 validation images. We reshape the images to 128 ˆ 128
resolution.

Baseline architectures: The architectures employed are:

• DCGAN (Radford et al., 2015), as implemented in https://github.com/pytorch/

examples/tree/master/dcgan. This is a widely used baseline.

• SNGAN (Miyato et al., 2018),

implemented in https://github.com/
pfnet-research/sngan_projection. SNGAN is a strong performing GAN that
introduced a spectral normalization in the discriminator.

as

• SAGAN (Zhang et al., 2019), as implemented in https://github.com/voletiv/
self-attention-GAN-pytorch. This is a recent network architecture that utilizes

19

Under review

the notion of self-attention (Wang et al., 2018) in a GAN setting, achieving impressive
results on Imagenet (Russakovsky et al., 2015).

The default hyper-parameters are left unchanged. The aforementioned codes are used for reporting
the results of both the baseline and our method to avoid any discrepancies, e.g. different frameworks
resulting in unfair comparisons. The source code will be released to enable the reproduction of our
results.

Evaluation metrics: The popular Inception Score (IS) (Salimans et al., 2016) and Frechet Inception
Distance (FID) (Heusel et al., 2017) are used for the quantitative evaluation. Both scores extract
feature representations from a pretrained classiﬁer (in practice the Inception network (Szegedy et al.,
2015)). Despite their shortcomings, IS and FID are widely used (Lucic et al., 2018; Creswell et al.,
2018), since alternative metrics fail for generative models (Theis et al., 2016).

The Inception Score is deﬁned as

exp pExPPθ rKLpppy|xq}ppyqsq

(36)

where x is a generated sample and ppy|xq is the conditional distribution for labels y. The distribution
M
ppyq over the labels is approximated by 1
n“1 ppy|xnq for xn generated samples. Following the
M
methods in the literature (Miyato et al., 2018), we compute the inception score for M “ 5, 000
generated samples per run (10 splits for each run).

ř

The Frechet Inception Distance (FID) utilizes feature representations from a pretrained net-
work (Szegedy et al., 2015) and assumes that the distributions of these representations are Gaussian.
Denoting the representations of real images as N pµr, Crq and the generated (fake) as N pµf , Cf q,
FID is:

}µr ´ µf }2

`
2 ` trace

`
Cr ` Cf ´ 2

CrCf

˘

1{2

˘

(37)

In the experiments, we use M “ 10, 000 to compute the mean and covariance of the real images and
M “ 10, 000 synthesized samples for µf , Cr.

For both scores the original tensorﬂow inception network weights are used; the routines of tensor-
ﬂow.contrib.gan.eval are called for the metric evaluation.

F.1

IMPLEMENTATION DETAILS

We experimentally deﬁne that a (series of) afﬁne transformation(s) on the input noise z are beneﬁcial
before using the transformed z for the Hadamard products.3 These afﬁne transformations are
henceforth mentioned as global transformations on z.

The implementation details for each network are the following:

• DCGAN: We use a global transformation followed by a RELU non-linearity. WThe rest

details remain the same as the baseline model.

• SNGAN: Similarly to DCGAN, we use a global transformation with a RELU non-linearity.

We consider each residual block as one order of approximation and compute the Hadamard product
after each block (see algorithm 4).

F.2 ABLATION STUDY ON CIFAR10 WITH NON-LINEAR GENERATORS

We conduct an ablation study based on SNGAN architecture (or our variant of SNGAN-poly),
since most recent methods are based on similar generators Zhang et al. (2019); Brock et al. (2019).
Unless explicitly mentioned otherwise, the SNGAN is trained on CIFAR10 for unsupervised image
generation.

3A similar transformation is performed in other GAN architectures, such as in Karras et al. (2019).

20

Under review

Algorithm 3: Original SNGAN generator.
:Noise z P R128, φ “ RELU
:x P R32ˆ32ˆ3

Input
Output

1 ;
2 ;
3 % fully-connected layer for reshaping.
4 h = φ(Linear(z)) % dims out: 4 ˆ 4 ˆ 256.;
5 ;
6 ;
7 ;
8 for n=1:3 do
9

% resnet blocks.
h = resblock(h)
% dims out: p4 ¨ 2nq ˆ p4 ¨ 2nq ˆ 256.;
;
;
;

14
15 end
16 x = tanh(Conv(h)) % dims out: 32 ˆ 32 ˆ 3.

10

11

12

13

Algorithm 4: Modiﬁed SNGAN-poly.

Input
Output

:Noise z P R128, φ “ RELU
:x P R32ˆ32ˆ3

1 % global transformation of z.
2 v = φ(Linear(z))
3 % fully-connected layer for reshaping.
4 h = φ(Linear(v)) % dims out: 4 ˆ 4 ˆ 256.
5 % perform a hadamard product here.
6 v0 “ pAr0sqT ¨ v
7 h = h ˚ v0 % dims out: 4 ˆ 4 ˆ 256.
8 for n=1:3 do
9

% resnet blocks.
h = resblock(h)
% dims out: p4 ¨ 2iq ˆ p4 ¨ 2iq ˆ 256.
% reshape v for hadamard product.
vn “ pArnsqT ¨ v
h = h ˚ vn

14
15 end
16 x = tanh(Conv(h)) % dims out: 32 ˆ 32 ˆ 3.

10

11

12

13

Table 4: The algorithm on the left describes the SNGAN generator. The algorithm on the right
preserves the resnet blocks of the SNGAN generator, but converts it into a polynomial (named
SNGAN-poly). The different lines are emphasized with blue color.

Global transformation validation: We add a global transformation on z, i.e. a fully-connected
layer and use the transformed noise as input to the generator. In the ﬁrst experiment, we evaluate
whether to add a non-linear activation to the global transformation. The two alternatives are: i)
with linear global transformation (‘Ours-linear-global’), i.e. no non-linearity, and ii) with global
transformation followed by a RELU non-linearity (‘Ours-RELU-global’).

Table 5: Global transformation validation on SNGAN. The ﬁrst two results assess the addition of a
non-linear activation function after the global transformation. The last two rows compare the addition
of a global transformation on the original generator.

SNGAN on CIFAR10

Model
Ours-linear-global
Ours-RELU-global
Original
Original-RELU-global

IS (Ò)
8.23 ˘ 0.10
8.30 ˘ 0.09
8.06 ˘ 0.10
7.98 ˘ 0.27

FID (Ó)
18.85 ˘ 0.59
17.65 ˘ 0.76
19.06 ˘ 0.50
37.61 ˘ 7.16

The ﬁrst two results in Table 5 demonstrate that both metrics marginally improve when using a
non-linear activation function. We add this global transformation with RELU on the original SNGAN.
The results are reported in the last two rows of Table 5 (where the original is mentioned as ‘Orig’,
while the alternative of adding a global transformation as ‘Original-RELU-global’).

Split z into chunks: The recent BigGAN of (Brock et al., 2019) performs hierarchical synthesis
of images by splitting the latent vector z into one chunk per resolution (block). Each chunk is then
concatenated into the respective resolution.

We scrutinize this splitting against our method; we split the noise z into pk ` 1q non-overlapping
chunks of equal size for performing k injections. The injection with splitting is mentioned as ‘Inject-
split’ below. Our splitting deteriorates the scores on the task as reported in Table 6. It is possible that
more elaborate splitting techniques, such as those in Brock et al. (2019) are beneﬁcial.

Normalization before Hadamard product: In Karras et al. (2019) they normalize the transformed
noise through ADAIN, while in Karras et al. (2018) they similarly perform a feature vector normal-
ization.

21

Under review

Table 6: Ablation experiment on splitting the noise z into non-overlapping chunks for the injection.

SNGAN on CIFAR10

Model
Original
Inject-split
Ours-RELU-global

IS (Ò)
8.06 ˘ 0.10
7.75 ˘ 0.12
8.30 ˘ 0.09

FID (Ó)
19.06 ˘ 0.50
22.08 ˘ 0.98
17.65 ˘ 0.76

We scrutinize a feature normalization on the baseline of ‘Ours-RELU-global’. For each layer i we
divide the Arisz vector with its standard deviation. The variant with global transformation followed
by RELU and normalization before the Hadamard product is called ‘Ours-norm’. The results in
Table 7 illustrate that normalization improves the metrics.

Table 7: Ablation experiment on normalizing the Arisz vector before the Hadamard product.

SNGAN on CIFAR10

Model
Ours-RELU-global
Ours-norm

IS (Ò)
8.30 ˘ 0.09
8.37 ˘ 0.11

FID (Ó)
17.65 ˘ 0.76
17.14 ˘ 0.58

Skip the Hadamard product: Motivated by the skip connection of our Coupled CP decomposition,
we add a skip connection to each Hadamard product. For instance, we modify the term
˚
´
pBr1sqT br1s

´
pBr1sqT br1s

´
pAr1sqT z

` pBr1sqT br1s.

pAr1sqT z

into

¯

¯

´

¯

¯

˚

In Table 8, we use ‘Ours-RELU-global’ as baseline against the model with the skip connection
(‘Ours-skip’).

Table 8: Ablation experiment on adding a skip connection to each Hadamard product.

SNGAN on CIFAR10

Model
Ours-RELU-global
Ours-skip

IS (Ò)
8.30 ˘ 0.09
8.43 ˘ 0.11

FID (Ó)
17.65 ˘ 0.76
21.54 ˘ 1.59

Since we use SNGAN both for unsupervised/conditional image generation, we verify the afore-
mentioned results in the conditional setting, i.e. when the class information is also provided to the
generator and the discriminator.

Normalization before Hadamard product: Similarly to the experiment above, for each layer i we
divide the Arisz vector with its standard deviation. The quantitative results in Table 9 improve the IS
score, but the FID deteriorates.

Skip the Hadamard product: Similarly to the aforementioned unsupervised case, we assess the
performance if we add a skip connection in the Hadamard. In Table 10, the quantitative results
comparing the baseline and the skip case are presented.

F.3 UNSUPERVISED IMAGE GENERATION

In this experiment, we study the image generation problem without any labels or class information for
the images. The architectures of DCGAN and resnet-based SNGAN are used for image generation
in CIFAR10 (Krizhevsky et al., 2014). Table 11 summarizes the results of the IS/FID scores of the
compared methods. In all of the experiments, PolyGAN outperforms the compared methods.

F.4 CONDITIONAL IMAGE GENERATION

Frequently class information is available. We can utilize the labels, e.g. use conditional batch normal-
ization or class embeddings, to synthesize images conditioned on a class. We train two networks,

22

Under review

Table 9: Ablation experiment (conditional GAN setting) on normalizing the Arisz vector before the
Hadamard product.

Table 10: Ablation experiment (conditional GAN setting) on adding a skip connection to each
Hadamard product.

conditional SNGAN on CIFAR10

Model
Ours-RELU-global
Ours-norm

IS (Ò)
8.66 ˘ 0.14
8.76 ˘ 0.11

FID (Ó)
13.52 ˘ 0.60
15.40 ˘ 1.29

conditional SNGAN on CIFAR10

Model
Ours-RELU-global
Ours-skip

IS (Ò)
8.66 ˘ 0.14
8.77 ˘ 0.10

FID (Ó)
13.52 ˘ 0.60
13.62 ˘ 0.69

i.e., SNGAN (Miyato et al., 2018) in CIFAR10 (Krizhevsky et al., 2014) and SAGAN (Zhang et al.,
2019) in Imagenet (Russakovsky et al., 2015). SAGAN uses self-attention blocks (Wang et al., 2018)
to improve the resnet-based generator.

Despite our best efforts to show that our method is both architecture and database agnostic, the recent
methods are run for hundreds of thousands or even million iterations till “convergence”. In SAGAN
the authors report that for each training multiple GPUs need to be utilized for weeks to reach the
ﬁnal reported Inception Score. We report the metrics for networks that are run with batch size 64
(i.e., four times less than the original 256) to ﬁt in a single 16GB NVIDIA V100 GPU. Following the
current practice in ML, due to the lack of computational budget (Hoogeboom et al., 2019), we run
SAGAN for 400, 000 iterations (see Figure 3 of the original paper for the IS during training)4. Each
such experiment takes roughly 6 days to train. The FID/IS scores of our approach compared against
the baseline method can be found in Table 12. In both cases, our proposed method yields a higher
Inception Score and a lower FID.

G EXPERIMENTAL MODEL COMPARISON

An experimental comparison of the two models described in Section 2 is conducted below. Unless
explicitly mentioned otherwise, the networks used below do not include any non-linear activation
functions, they are polynomial expansions with linear blocks. We use the following four experiments:

Sinusoidal on 2D: The data distribution is described by rx, sinpxqs with x P r0, 2πs (see Section 4.1
for further details). We assume 8th order approximation for Coupled CP decomposition and 12th
order for Coupled nested CP decomposition. Both have width 15 units. The comparison between the
two models in Figure 13 demonstrates that they can both capture the data manifold. Impressively, the
Coupled CP decomposition does not synthesize a single point that is outside of the manifold.

Astroid: The data distribution is described on Section D. The samples comparing the two models are
visualized in Figure 15.

Sin3D: The data distribution is described on Section D. In Figure 15 the samples from the two models
are illustrated.

Swiss roll: The data distribution is described on Section D. In Figure 16 the samples from the two
models are illustrated.

Digit generation: We conduct an experiment on images to verify that both architectures can learn
higher-dimensional distributions. We select the digit images as described in Section 4.2. In this case,
Coupled CP decomposition is implemented as follows: each Uris is a series of linear convolutions
with stride 2 for i “ 1, . . . , 4, while C is a linear residual block. We emphasize that in both models

4Given the batch size difference, our training corresponds to roughly the 100, 000 steps of the authors’

reported results.

23

Under review

Table 11: IS/FID scores on CIFAR10 (Krizhevsky et al., 2014) utilizing DCGAN (Radford et al.,
2015) and SNGAN (Miyato et al., 2018) architectures for unsupervised image generation. Each
network is run for 10 times and the mean and standard deviation are reported. In both cases, inserting
block-wise noise injections to the generator (i.e., converting to our proposed PolyGAN) results in an
improved score. Higher IS / lower FID score indicate better performance.

Table 12: Quantitative results on conditional image generation. We implement both SNGAN trained
on CIFAR10 and SAGAN trained on Imagenet (for 400, 000 iterations). Each network is run for 10
times and the mean and variance are reported.

all the activation functions are removed and there is a single tanh in the output of the generator for
normalization purposes.

DCGAN
IS (Ò)
6.25 ˘ 0.06
6.03 ˘ 0.06
PolyGAN 6.61 ˘ 0.05

Model
Orig
Concat

SNGAN
IS (Ò)
8.06 ˘ 0.10
8.28 ˘ 0.16
PolyGAN 8.30 ˘ 0.09

Model
Orig
Concat

FID (Ó)
47.29 ˘ 2.06
49.35 ˘ 2.17
42.86 ˘ 1.02

FID (Ó)
19.06 ˘ 0.50
20.77 ˘ 2.91
17.65 ˘ 0.76

SNGAN (CIFAR10)

Model
Orig

IS (Ò)
8.30 ˘ 0.11
PolyGAN 8.66 ˘ 0.14

FID (Ó)
14.70 ˘ 0.97
13.52 ˘ 0.60

SAGAN (Imagenet)

Model
Orig

IS (Ò)
13.81 ˘ 0.21
PolyGAN 14.60 ˘ 0.15

FID (Ó)
138.20 ˘ 8.71
84.37 ˘ 6.37

24

Under review

Table 13: Number of parameters for the generators of each approach and on various databases. As
can be seen, our method only marginally increases the parameters while substantially improving the
performance. On the other hand, “Concat” signiﬁcantly increases the parameters without analogous
increase in the performance.

DCGAN (CIFAR10)
Params
Model
3, 573, 440
Orig
6, 416, 448
Concat
PolyGAN 3, 663, 936

SNGAN (CIFAR10)
Params
Model
4, 276, 739
Orig
6, 383, 875
Concat
PolyGAN 4, 408, 835

SAGAN (Imagenet)
Params
Model
42, 079, 300
Orig
PolyGAN 42, 351, 748

(a) GT

(b) Coupled CP decomposition (c) Coupled nested CP decomposition

Figure 13: Synthesized data for learning the rx, sinpxqs signal. No activation functions are used
in the generators. From left to right: (a) the data distribution, (b) Coupled CP decomposition, (c)
Coupled nested CP decomposition.

(a) GT

(b) Coupled CP decomposition (c) Coupled nested CP decomposition

Figure 14: Synthesized data for learning the Astroid. Both models generate only plausible examples.

(a) GT

(b) Coupled CP decomposition (c) Coupled nested CP decomposi-
tion

Figure 15: Experiment on 3D synthetic data. From left to right: (a) the data distribution, (b) Coupled
CP decomposition, (c) Coupled nested CP decomposition.

25

Under review

(a) GT

(b) Coupled CP decomposition (c) Coupled nested CP decomposi-
tion

Figure 16: Experiment on 3D synthetic data (‘swiss roll’). From left to right: (a) the data distribution,
(b) Coupled CP decomposition, (c) Coupled nested CP decomposition. Note that Coupled CP
decompositiongenerates some noisy samples in contrast to Coupled nested CP decomposition.

(a) GT

(b) Coupled CP decomposition (c) Coupled nested CP decomposi-
tion

Figure 17: Comparison of the two decompositions on digit generation.

26

