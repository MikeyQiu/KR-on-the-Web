Distributional Semantics Meets Construction Grammar.
Towards a Uniﬁed Usage-Based Model of Grammar and Meaning

Giulia Rambelli
University of Pisa
giulia.rambelli@phd.unipi.it

Emmanuele Chersoni
The Hong Kong Polytechnic University
emmanuelechersoni@gmail.com

Philippe Blache
Aix-Marseille University
blache@lpl-aix.fr

Chu-Ren Huang
The Hong Kong Polytechnic University
churen.huang@polyu.edu.hk

Alessandro Lenci
University of Pisa
alessandro.lenci@unipi.it

Abstract

In this paper, we propose a new type of seman-
tic representation of Construction Grammar
that combines constructions with the vector
representations used in Distributional Seman-
tics. We introduce a new framework, Distribu-
tional Construction Grammar, where grammar
and meaning are systematically modeled from
language use, and ﬁnally, we discuss the kind
of contributions that distributional models can
provide to CxG representation from a linguis-
tic and cognitive perspective.

1

Introduction

In the last decades, usage-based models of lan-
guage have captured the attention of linguistics
and cognitive science (Tommasello, 2003; Bybee,
2010). The different approaches covered by this
label are based on the assumptions that linguistic
knowledge is embodied in mental processing and
representations that are sensitive to context and
statistical probabilities (Boyland, 2009), and that
language structures at all levels, from morphology
to syntax, emerge out of facts of actual language
usage (Bybee, 2010).

A usage-based framework that turned out to be
extremely inﬂuential is Construction Grammar
(CxG) (Hoffman and Trousdale, 2013), a family
of theories sharing the fundamental idea that lan-
guage is a collection of form-meaning pairings
called constructions (henceforth Cxs) (Fillmore,
1988; Goldberg, 2006). Cxs differ for their degree
of schematicity, ranging from morphemes (e.g.,
pre-, -ing), to complex words (e.g., daredevil) to
ﬁlled or partially-ﬁlled idioms (e.g., give the devil
his dues or Jog (someones) memory) to more ab-
stract patterns like the ditransitive Cxs [Subj V

Obj1 Obj2]).
It is worth stressing that, even
if the concept of construction is based on the idea
that linguistic properties actually emerge from lan-
guage use, CxG theories have typically preferred
to model the semantic content of constructions in
terms of hand-made, formal representations like
those of Frame Semantics (Baker et al., 1998).
This leaves open the issue of how semantic repre-
sentations can be learned from empirical evidence,
and how do they relate to the usage-based nature of
Cxs. In fact, for a usage-based model of grammar
based on a strong syntax-semantics parallelism, it
would be desirable to be grounded on a framework
allowing to learn the semantic content of Cxs from
language use.

In this perspective, a promising solution for
representing constructional semantics is given by
an approach to meaning representations that has
gained a rising interest in both computational lin-
guistics and cognitive science, namely Distribu-
tional Semantics (henceforth DS). DS is a usage-
based model of word meaning, based on the well-
established assumption that the statistical distribu-
tion of linguistic items in context plays a key role
in characterizing their semantic behaviour (Distri-
butional Hypothesis (Harris, 1954)). More pre-
cisely, Distributional Semantic Models (DSMs)
represent the lexicon in terms of vector spaces,
where a lexical target is described in terms of a
vector (also known as embedding) built by identi-
fying in a corpus its syntactic and lexical contexts
(Lenci, 2018). Lately, neural models to learn dis-
tributional vectors have gained massive popular-
ity: these algorithms build low-dimensional vec-
tor representations by learning to optimally predict
the contexts of the target words (Mikolov et al.,
2013). On the negative side, DS lacks a clear

connection with usage-based theoretical frame-
works. To the best of our knowledge, existing
attempts of linking DS with models of grammar
have rather targeted formal theories like Montague
Grammar and Categorial Grammar (Baroni et al.,
2014; Grefenstette and Sadrzadeh, 2015).

To sum up, both CxG and DS share the assump-
tion that
linguistic structures naturally emerge
from language usage, and that a representation
of both form and meaning of any linguistic item
can be modeled through its distributional statistics,
and more generally, with the quantitative informa-
tion derived from corpus data. However, these two
models still live in parallel worlds. On the one
hand, CxG is a model of grammar in search for
a consistent usage-based model of meaning, and,
conversely, DS is a computational framework to
build semantic representations in search for an em-
pirically adequate theory of grammar.

As we illustrate in Section 2, occasional en-
counters between DS and CxG have already hap-
pened, but we believe that new fruitful advances
could come from the exploitation of the mu-
tual synergies between CxG and DS, and by let-
ting these two worlds ﬁnally meet and interact
in a more systematic way. Following this direc-
tion of research, we introduce a new representa-
tion framework called Distributional Construc-
tion Grammar, which aims at bringing together
these two theoretical paradigms. Our goal is to
integrate distributional information into construc-
tions by completing their semantic structures with
distributional vectors extracted from large textual
corpora, as samples of language usage.

These pages are structured as follows: after
reviewing existing literature on CxG and related
computational studies,
in Section 3 we outline
the key characteristics of our theoretical proposal,
while Section 4 provides a general discussion
about what contributions DSMs can provide to
CxG representation from a linguistic and cognitive
perspective. Although this is essentially a theoret-
ical contribution, we outline ongoing work focus-
ing on its computational implementation and em-
pirical validation. We conclude by reporting future
perspectives of research.

2 Related Work

Despite the popularity of the constructional ap-
proach in corpus linguistics (Gries and Stefanow-
itsch, 2004), computational semantics research

has never formulated a systematic proposal for de-
riving representations of constructional meaning
from corpus data. Previous literature has mostly
focused either on the automatic identiﬁcation of
constructions on the basis of their formal features,
or on modeling the meaning of a speciﬁc CxG.

For the former approach, we should mention
the works of Dunn (2017, 2019) that aim at au-
tomatically inducing a set of grammatical units
(Cxs) from a large corpus. On the one hand,
Dunn’s contributions provide a method for extract-
ing Cxs from corpora, but on the other hand they
are mainly concerned with the formal side of the
constructions, and especially with the problem of
how syntactic constraints are learned. Some sort
of semantic representation is included, in the form
of semantic cluster of word embeddings to which
the word forms appearing in the constructions are
assigned. However, these works do not present
any evaluation of the construction representations
in terms of semantic tasks.

Another line of research has focused in using
constructions for building computational models
of language acquisition. Alishahi and Stevenson
(2008) propose a model for the representation, ac-
quisition and use of verb argument structure by
formulating constructions as probabilistic associ-
ations between syntactic and semantic properties
of verbs and their arguments. This probabilistic
association emerges over time through a Bayesian
acquisition process in which similar verb usages
are detected and grouped together to form general
constructions, based on their syntactic and seman-
tic properties. Despite the success of this model,
the semantic representation of argument structure
is still symbolic and each semantic category of in-
put constructions are manually compiled, in con-
trast with the usage-based nature of constructions.

Other studies used DSMs to model construc-
tional meaning, by focusing on a speciﬁc type of
Cx rather than on the entire grammar. For exam-
ple, Levshina and Heylen (2014) build a vector
space to study Dutch causative constructions with
doen (‘do’) and laten (‘let’). They compute several
vector spaces with different context types, both for
the nouns that ﬁll the Causer and Causee slot and
for the verbs that ﬁll the Effected Predicate slot.
Then, they cluster these nouns and verbs at differ-
ent levels of granularity and test which classiﬁca-
tion better predicts the use of laten and doen.

A recent trend in diachronic linguistics investi-

gates linguistic change as a sequence of gradual
changes in distributional patterns of usage (By-
bee, 2010). For instance, Perek (2016) investi-
gates the productivity of the V the hell out of NP
construction (e.g., You scared the hell out of me)
from 1930 to 2009. On one side, he clusters the
vectors of verbs occurring in this construction to
pin point the preferred semantic domains of the
Cx in its diachronic evolution. Secondly, he com-
putes the density of the semantic space of the con-
struction around a given word in a certain period
to be predictive of that word joining the construc-
tion in the subsequent period. A similar approach
is applied to study changes in the productivity of
the Way-construction over the period 1830-2009
(Perek, 2018). Perek’s analysis also proves that
distributional similarity and neighbourhood den-
sity in the vector space can be predictive of the
usage of a construction with a new lexical item.
Other works have followed this approach, demon-
strating the validity of DSMs to model the seman-
tic change of constructions in diachrony. Amato
and Lenci (2017) examine the Italian Gerundival
Periphrases stare (to stay) andare (to go), venire
(to come) followed by a gerund. As in previous
works, they uses DSMs to i) identify similarities
and differences among Cxs clustering the vectors
of verbs occurring in each Cx, and ii) investigate
the changes undergone by the semantic space of
the verbs occurring in the Cxs throughout a very
long period (from 1550 to 2009).

Lebani and Lenci (2017) present an unsuper-
vised distributional semantic representation of ar-
gument constructions. Following the assumption
that constructional meanings for argument Cxs
arise from the meaning of high frequency verbs
that co-occur with them (Goldberg, 1999; Casen-
hiser and Goldberg, 2005; Barak and Goldberg,
2017), they compute distributional vectors for CxS
as the centroids of the vectors of their typical
verbs, and use them to model the psycholinguis-
tic data about construction priming in Johnson and
Goldberg (2013). This representation of construc-
tion meaning has also been applied to study va-
lency coercion by Busso et al. (2018).

Following a parallel research line on probing
tasks for distributed vectors, Kann et al. (2019) in-
vestigate whether word and sentence embeddings
encode the grammatical distinctions necessary for
inferring the idiosyncratic frame-selectional prop-
erties of verbs. Their ﬁndings show that, at least

for some alternations, verb embeddings encode
sufﬁcient information for distinguishing between
acceptable and unacceptable combinations.

3 Distributional CxG Framework

We introduce a new framework aimed at integrat-
ing the computational representation derived from
distributional methods into the explicit formaliza-
tion of Construction Grammars, called Distribu-
tional Construction Grammar (DisCxG).
DisCxG is based on three components:

• Constructions: stored pairings of form and
function, including morphemes, words, id-
ioms, partially lexically ﬁlled and fully gen-
eral linguistic patterns (Goldberg, 2003);

• Frames: schematic semantic knowledge de-
scribing scenes and situations in terms of
their semantic roles;

• Events:

semantic information concerning
particular event instances with their speciﬁc
participants. The introduction of this compo-
nent, which is a novelty with respect to tra-
ditional CxG frameworks, has been inspired
by cognitive models such as the General-
ized Event Knowledge (McRae and Matsuki,
2009) and the Words-as-Cues hypothesis (El-
man, 2014).

The peculiarity of DisCxG is that we distinguish
two layers of semantic representation, referring to
two different and yet complementary aspects of
semantic knowledge. Speciﬁcally, frames deﬁne
a prototypical semantic representation based on
the different semantic roles (the frame elements)
deﬁning argument structures, while events provide
a specialization of the frame by taking into ac-
count information about speciﬁc participants and
relations between them. Crucially, we assume
that both these layers have a DS representation in
terms of distributional vectors learned from cor-
pus co-occurrences.

Following the central tenet of CxGs, according
to which linguistic information is encoded in sim-
ilar way for lexical items as well as for more ab-
stract Cxs (e.g., covariational-conditional Cx, di-
transitive Cx etc.), the three components of Dis-
CxG are modeled using the same type of formal
representation with recursive feature-structures,
which is inspired by Sign-Based Construction
Grammar (SBCG) (Sag, 2012; Michaelis, 2013).

3.1 Constructions

In DisCxG, a construction is represented by form
and semantic features. The following list presents
the set of main features of Cxs adapting the for-
malization in SBCG:

• The FORM feature contains the basic formal
characteristics of constructions.
It includes
the (i) PHONological/SURFACE form, (ii)
the (morpho)syntactic features (SYN),
i.e
part-of-speech (TYPE), CASE (nominal, ac-
cusative), the set of elements subcategorized
(VAL), and (iii) PROPERTIES representing
explicitly the syntactic relations among the
elements of the Cx.

• The ARGument-STructure implements the
interface between syntactic and semantic
roles. The arguments are in order of their ac-
cessibility hierarchy (subj ≺ d-obj ≺ obl...),
encoding the syntactic role. Each argument
speciﬁes the case, related to the grammatical
function, and links to the thematic role.1

• The SEMantic feature speciﬁes the properties

of Cx’s meaning (Section 3.2).

Unlike SGBG or other CxG theories, we in-
clude inside FORM a new feature called PROP-
ERTIES, borrowed from Property Grammars
(Blache, 2005). Properties encode syntactic infor-
mation about the components of a Cx, and they
play an important role in its recognition. How-
ever, the discussion of this linguistic aspect is not
presented here, as the focus of this paper is on the
semantic side of constructions. 2

As said above, a Cx can describe linguis-
tic objects of various levels of complexity and
schematicity: words, phrases, fully lexicalized
idiomatic patterns, partially lexicalized schemas,
etc. Thus, the attribute-value matrix can be applied
to lexical entries, as the verb read in Figure 1, as
well as to abstract constructions that do not involve
lexical material. Figure 2 depicts the ditransitive
Cx. The semantic particularity of this construction
is that whatever the lexicalization of the verb, this

1SGCG distinguishes between valence and argument
structure: the ARG-ST encodes overt and covert arguments,
including extracted (non-local) and unexpressed elements,
while VAL in the form description represents only realized
elements. When no covert arguments occur, these features
are identical.

2For more details on the Propery Grammar framework,

see Blache (2016).

construction always involve a possession interpre-
tation (more precisely the transfer of something to
somebody), represented in the TRANSFER frame.
Differently from standard SBCG formalization
of Cxs, we add the distributional feature DS-
VECTOR into the semantic layer in order to in-
tegrate lexical distributional representations. The
semantic structure of a lexical item can be asso-
ciated with its distributional vector (e.g., the em-
bedding of read), but we can also include a distri-
butional representation of abstract syntactic con-
structions following the approach of Lebani and
Lenci (2017) we have illustrated in Section 2.

3.2 Frames

A frame is a schematic representation of an
event or scenario together with the participating
actors/objects/locations and their (semantic) role
(Fillmore, 1982). For instance, the sentences

1.

(a) Mary bought a car from John (for

5000$).

(b) John sold a car to Mary (for 5000$).

activate the same COMMERCIAL TRANSACTION
frame, consisting of a SELLER (John), a BUYER
(Mary), a GOOD which is sold (car), and the
MONEY used in the transaction (5000$ ).

Semantic frames are the standard meaning rep-
resentation in CxG, which represent them as sym-
bolic structures. The source of this informa-
tion is typically FrameNet (Ruppenhofer et al.,
2016), a lexical database of English containing
more than 1,200 semantic frames linked to more
than 200,000 manually annotated sentences. The
not negligible problem of FrameNet is that entries
must be created by expert lexicographers. This has
lead to a widely recognized coverage problem in
its lexical units (Baker, 2012).

In DisCxG, semantic frames are still repre-
sented as structures, but the value of semantic
roles consists of distributional vectors. As for the
COMMERCIAL TRANSACTION frame in Figure 3,
each frame element has associated a speciﬁc em-
bedding. It is worth noting that in this ﬁrst version
of the DisCxG model, frame representations are
still based on predeﬁned lists of semantic roles, as
deﬁned in FrameNet (e.g., BUYER, SELLER, etc.).
However, some works have recently attempted to
automatically infer frames (and their roles) from
distributional information3. Woodsend and Lap-

3Lately, SemEval 2019 proposed a task on unsupervised































(cid:68)

read

(cid:69)

read-lxm-cx


FORM

SURFACE


(cid:34)

SYN

CAT















VAL (cid:104)(cid:105)
(cid:69)

(cid:68)

ARG-ST

NPi, NPj

V
TYPE
VFORM ﬁn

FRAMES

SEM












(cid:42)




read-fr
READER i
j

TEXT



(cid:43)











DS-VECTOR

−−→
read












(cid:35)
































































ditransitive-cx

(cid:104)

SYN

CAT

FORM

PROPERTIES

(cid:105)

1 V


(cid:110)

(cid:110)




LIN

ADJ








(cid:68)

2 ≺ 1 ≺ 3 ≺ 4
1 (cid:76) 3 ; 3 (cid:76) 4
(cid:69)

ARG-ST

2 NPx[subj], 3 NPy[obl], 4 NPz[obj]
















(cid:42)










transfer-fr

AGENT

RECIPIENT


(cid:43)


x




y





z


−−−−−−−−→

ditransitive

THEME

FRAMES

SEM

DS-VECTOR



(cid:111)









(cid:111)































Figure 1: Description of read verb

Figure 2: Description of ditransitive Cx

ata (2015) use distributional representations to in-
duce embeddings for predicates and their argu-
ments. Ustalov et al. (2018) propose a different
methodology for unsupervised semantic frame in-
duction. They build embeddings as the concate-
nations of subject-verb-object triples and identify
frames as clustered triples. Of course, a limit of
this approach is that it only uses subject and object
arguments, while frames are generally associated
with a wider variety of roles. Lebani and Lenci
(2018) instead provide a distributional representa-
tion of verb-speciﬁc semantic roles as clusters of
features automatically induced from corpora.

In this paper, we assume that at least some as-
pects of semantic roles can be derived from com-
bining (e.g., with summation) the distributional
vectors of their most prototypical ﬁllers, follow-
ing an approach widely explored in DS (Baroni
and Lenci, 2010; Erk et al., 2010; Sayeed et al.,
−−−→
buyer
2016; Santus et al., 2017). For instance, the
role in the COMMERCIAL TRANSACTION frame
can be taken as a vector encoding the properties
of the typical nouns ﬁlling this role. We are aware
that this solution is just an approximation of the
content of frames elements. How to satisfactorily
characterize semantic frames and roles using DS
is in fact still an open research question.

3.3 Events

Neurocognitive research has brought extensive ev-
idence that stored world knowledge plays a key
role in online language production and compre-

lexical semantic frame induction (http://alt.qcri.
org/semeval2019/index.php?id=tasks)



















commercial-transaction-fr




BUYER

SELLER


−−−→

buyer


−−−→


seller

(cid:43)



−−−→


goods

GOODS



MONEY −−−−→money







−−→
shop

PLACE



(cid:42)








SEM

Figure 3: The COMMERCIAL TRANSACTION frame
containing the distributional representation of the se-
mantic roles

hension. An important aspect of such knowl-
edge consists of the events and situations that we
experience under different modalities, including
the linguistic input. McRae and Matsuki (2009)
call it Generalized Event Knowledge (GEK), be-
cause it contains information about prototypical
event structures. Language comprehension has
been characterized as a largely predictive pro-
cess (Kuperberg and Jaeger, 2015). Predictions
are memory-based, and experiences about events
and their participants are used to generate ex-
pectations about the upcoming linguistic input,
thereby minimizing the processing effort (Elman,
2014; McRae and Matsuki, 2009). For instance,
argument combinations that are more ‘coherent’
with the event scenarios activated by the previous
words are read faster in self-paced reading tasks
and elicited smaller N400 amplitudes in ERP ex-
periments (Paczynski and Kuperberg, 2012).

In DisCxG, events have a crucial role:

they

bridge the gap between the concrete instantiation
of a Cx in context and its conceptualized meaning
(conveyed from frames). For example, let’s con-
sider the verb read. We know that this verb sub-
categorizes for two noun phrases (form) and in-
volves a generic READING frame in which there
is someone who reads (READER) and something
that is read (TEXT). This frame only provides an
abstract, context-independent representation of the
verb meaning, and the two roles can be generally
deﬁned as clusters of properties derived from sin-
gular subjects and objects of read. However, the
semantic representation comprehenders build dur-
ing sentence processing is inﬂuenced by the spe-
ciﬁc ﬁllers that instantiate the frame elements. If
the input is A student reads.., the fact that the word
student appears as the subject of the verb activates
a speciﬁc scenario, together with a series of ex-
pectations about the prototypicality of other lexi-
cal items. Consequently, the object of the previous
sentence is more likely to be book rather than mag-
azine (Chersoni et al., 2019). Accordingly, in Dis-
CxG events are considered as functions that spe-
cialize the semantic meaning encoded in frames.
The word student specializes the READING frame
into a speciﬁc event, triggering expectations about
the most likely participants of the other roles: the
READER is encoded as a lexical unit vector, and
the distributional restriction applied to the TEXT
is represented by a subset of possible objects or-
dered by their degree of typicality in the event.
Figure 4 gives a simple example of the specializa-
tion brought out by event knowledge.












student-read-event
SPECIALIZE reading-fr

(cid:42)


SEM

READER

TEXT

−−−−−→
student
(cid:110)−−→

book, −−−→paper..








(cid:43)





(cid:111)


Figure 4: Student-read event as the specialization of
READING frame

In a similar way, events can instantiate an ab-
stract construction dynamically, according to the
context. The different lexicalization of the AGENT
and the RECIPIENT in the ditransitive construction
causes a different selection of the THEME. For
example, the fact that the sentence fragment The
teacher gives students ... could be completed as in
(2) expresses a distributional restriction that can be
encoded as an event capturing the co-occurrences

SPECIALIZE ditransitive-cx
(cid:69)

(cid:68)

ARG-ST

NPx, NPy, NPz






























SEM

FRAMES




(cid:42)







transfer-fr
AGENT x

−−−−−→
teacher]

[

RECIPIENT y
[

−−−−−−→
students]

THEME z(cid:104)−−−−−−→

exercise,−−−−−−−→

homework..(cid:105)

















(cid:43)




















Figure 5: Ditransitive event specialization

teacher/student/exercises (Figure 5).

2. The teacher gives students ... → The teacher

gives students exercises

Any lexical item activate a portion of event
knowledge (Elman, 2014): in fact, if verbs evoke
events, nouns evoke entities that participate into
events. Thus, events and entities are themselves
interlinked: there is not a speciﬁc feature EVENT
in the description of the lexical entry teacher, but
events are activated by the lexical entry, generating
a network of expectations about upcoming words
in the sentence (McRae and Matsuki, 2009).

Given this assumption, Chersoni et al. (2019)
represent event knowledge in terms of a Distri-
butional Event Graph (DEG) automatically built
from parsed corpora.
In this graph, nodes are
embeddings and edges are labeled with syntac-
tic relations and weighted using statistic associa-
tion measures (Figure 6). Each event is a a path
in DEG. Thus, given a lexical cue w, it is pos-
sible to identify the events it activates (together
with the strength of its activation, deﬁned as a
function of the graph weights) and generate ex-
pectations about incoming inputs on both paradig-
matic and syntagmatic axes. With this graph-
based approach, Chersoni et al. (2019) model sen-
tence comprehension as the dynamic and incre-
mental creation of a semantic representation inte-
grated into a semantically coherent structure con-
tributing to the sentence interpretation.

We propose to include in our framework the
information encoded in DEG. Each lexical entry
contains a pointer to its corresponding node in
the graph. Therefore, the frame specialization we
have described above corresponds to an event en-
coded with a speciﬁc path in the DEG. Event in-
formation represents a way to unify the schematic
descriptions contained in the grammar with the

world knowledge and contextual information pro-
gressively activated by lexical items and integrated
during language processing.

4 Some Other Arguments in Favor of a

Distributional CxG

As we said in Section 2, few works have tried to
use distributional semantic representations of con-
structions and existing studied have focused more
on applying DS to a particular construction type,
instead of providing a general model to represent
the semantic content of Cxs. We argue that DSMs
could give an important contribution in design-
ing representations of constructional meaning. In
what follows, we brieﬂy discuss some speciﬁc is-
sues related to Construction Grammars that could
be addressed by combining them with Distribu-
tional Semantics.

Measuring similarity among constructions
and frames The dominant approaches
like
frame semantics and traditional CxGs tend to
represent entities and their relations in a formal
(hand-made) way. A potential limitation of these
methods is that it is hard to assess the similarity
between frames or constructions, while one advan-
tage of distributional vectors is that one can easily
compute the degree of similarity between linguis-
tic items represented in a vector space. For ex-
ample, Busso et al. (2018) built a semantic space
for several Italian argument constructions and then
computed the similarity of their vectors, observ-
ing that some Cxs have similar distributional be-
haviour like Caused-Motion and Dative.

As for frames, there has been some work on
using distributional similarity between vectors for
their unsupervised induction (Ustalov et al., 2018),
for comparing frames across languages (Sikos and
Pad´o, 2018), and even for the automatic identiﬁ-
cation of the semantic relations holding between
them (Botschen et al., 2017).

Identifying idiomatic meaning Many stud-
ies in theoretical, descriptive and experimental lin-
guistics have recently questioned the fregean prin-
ciple of compositionality, which assumes that the
meaning of an expression is the result of the incre-
mental composition of its sub-constituents. There
is a large number of linguistic phenomena whose
meaning is accessed directly from the whole lin-
guistic structure: this is typically the case with id-
ioms or multi-word expressions, where the ﬁgura-

tive meaning cannot be decomposed. In computa-
tional semantics, a large literature has been aim-
ing at modeling idiomaticity using DSMs. Senaldi
et al. (2016) carried out an idiom type identiﬁ-
cation task representing Italian V-NP and V-PP
Cxs as vectors. They observed that the vectors
of VN and AN idioms are less similar to the vec-
tors of lexical variants of these expressions with
respect to the vectors of compositional construc-
tions.
(Cordeiro et al., 2019) realized a frame-
work for predict compound compositionality us-
ing DSMs, evaluating to what extent they cap-
ture idiomaticity compared to human judgments.
Results revealed a high agreement between the
models and human predictions, suggesting that
they are able to incorporate information about id-
iomaticity.

In future works, it would be interesting to see if
DSMs-based approaches can be used in combina-
tion with methods for the identiﬁcation of the for-
mal features of constructions (Dunn, 2017, 2019),
in order to tackle the task of compositionality pre-
diction simultaneously with syntactic and seman-
tic features.

Modeling sentence comprehension A trend
in computational semantics regards the application
of DSMs to sentence processing (Mitchell et al.,
2010; Lenci, 2011; Sayeed et al., 2015; Johns and
Jones, 2015, i.a.).

Chersoni et al. (2016, 2017) propose a Distribu-
tional Model of sentence comprehension inspired
by the general principles of the Memory, Uni-
ﬁcation and Control framework (Hagoort, 2013,
2015). The memory component includes events
in GEK with feature structures containing infor-
mation directly extracted from parsed sentences
in corpora: attributes are syntactic dependencies,
while values are distributional vectors of depen-
dent lexemes. Then, they model semantic com-
position as an event construction and update func-
tion F, whose aim is to build a coherent semantic
representation by integrating the GEK cued by the
linguistic elements.

The framework has been applied to the logical
metonymy phenomenon (e.g, The student begins
the book), using the semantic complexity func-
tion to model the processing costs of metonymic
sentences, which was shown to be higher com-
pared to non-coercion sentences (McElree et al.,
2001; Traxler et al., 2002). Evaluation against
psycholinguistic datasets proves the linguistic and

Figure 6: An extract of DEG showing several instances of events (Chersoni et al., 2019)

psycholinguistic validity of using embeddings to
represent events and including them in incremen-
tal model of sentence comprehension.

Evaluations based on experimental evidence
DSMs have proved to be very useful in model-
ing human performance in psycholinguistic tasks
(Mandera et al., 2017). This is an important ﬁnd-
ing, since it allows to test the predictions of Con-
struction Grammar theories against data derived
from behavioral experiments.

To cite an example from the DS literature, the
models proposed by Lebani and Lenci (2017)
replicated the priming effect of the lexical decision
task by Johnson and Goldberg (2013), where the
participants were asked to judge whether a given
verb was a real word or not, after being exposed to
an argument structure construction in the form of
a Jabberwocky sentence. The authors of the study
created distributional representations of construc-
tions as combinations of the vectors of their typical
verbs, and measured their cosine similarity with
the verbs of the original experiment, showing that
their model can accurately reproduce the results
reported by Johnson and Goldberg (2013).

5 Conclusion

In this paper, we investigated the potential con-
tribution of DSMs to the semantic representation
of constructions, and we presented a theoretical
proposal bringing together vector spaces and con-
structions into a unique framework.
It is worth
highlighting our main contributions:

• We built a uniﬁed representation of gram-
mar and meaning based on the assumption

that language structure and properties emerge
from language use.

• We integrated information about events to
build a semantic representation of an input as
an incremental and predictive process.

Converging different layers of meaning represen-
tation into a unique framework is not a trivial prob-
lem, and in our future work we will need to ﬁnd
optimal ways to balance these two components:
semantic vectors derived from corpus data on the
one hand, and a possibly accurate formalization of
the internal structure of the constructions on the
other hand. In this contribution, we hoped to show
that merging the two frameworks would be worth
the efforts, as they share many theoretical assump-
tions and complement themselves on the basis of
their respective strengths.

Our future goal is the automatic building and in-
clusion of a distributional representation of frames
and event in DisCxG; our aim is to exploit the ﬁ-
nal formalism to build for the ﬁrst time a Distribu-
tional Construction Treebank. Moreover, we are
planning to apply this framework in a predictive
model of language comprehension, deﬁning how
a Cx is activated by the combination of syntactic,
lexical and distributional cues occurring in Dis-
CxG. We believe this framework could be a start-
ing point for applications in NLP such as Knowl-
edge representation and reasoning, Natural Lan-
guage Understanding and Generation, but also a
potential term of comparison for psycholinguistic
models of human language comprehension.

References

Afra Alishahi and Suzanne Stevenson. 2008. A Com-
putational Model of Early Argument Structure Ac-
quisition. Cognitive Science, 32(5):789–834.

Irene Amato and Alessandro Lenci. 2017. Story of a
Construction: Statistical and Distributional Analysis
of the Development of the Italian Gerundival Con-
struction. In G. Marotta and F. S. Lievers, editors,
Strutture Linguistiche e Dati Empirici in Diacronia
e Sincronia, pages 135–158. Pisa University Press.

Collin F Baker. 2012. FrameNet, Current Collabora-
tions and Future Goals. Language, Resources and
Evaluation, 46(2):269–286.

Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
In Pro-
1998. The Berkeley FrameNet project.
ceedings of COLING-ACL, pages 86–90, Montreal,
Canada. ACL.

Libby Barak and Adele Goldberg. 2017. Modeling the
Partial Productivity of Constructions. In Proceeed-
ings of the 2017 AAAI Spring Symposium Series on
Computational Construction Grammar and Natural
Language Understanding, pages 131–138.

Marco Baroni, Raffaela Bernardi, and Roberto Zam-
parelli. 2014. Frege in Space: A Program of Com-
positional Distributional Semantics. Linguistic Is-
sues in Language Technology, 9(6):5–110.

Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A General Framework for
Corpus-Based Semantics. Computational Linguis-
tics, 36(4):673–721.

Philippe Blache. 2005. Property Grammars: A Fully
Constraint-Based Theory. In Skadhauge P. R. Chris-
tiansen, H. and J. Villadsen, editors, Constraint
Solving and Language Processing. CSLP 2004, vol-
ume 3438 of LNAI, pages 1–16. Springer, Berlin,
Heidelberg.

Philippe Blache. 2016. Representing Syntax by Means
of Properties: A Formal Framework for Descrip-
tive Approaches. Journal of Language Modelling,
4(2):183–224.

Teresa Botschen, Hatem Mousselly Sergieh, and Iryna
Gurevych. 2017. Prediction of Frame-to-Frame Re-
lations in the FrameNet Hierarchy with Frame Em-
beddings. In Proceedings of the ACL Workshop on
Representation Learning for NLP, pages 146–156.

Joyce Tang Boyland. 2009. Usage-Based Models of
In D. Eddington, editor, Experimental
Language.
and Quantitative Linguistics, pages 351–419. Lin-
com, Munich.

Lucia Busso, Ludovica Pannitto, and Alessandro
Lenci. 2018. Modelling Italian Construction Flex-
ibility with Distributional Semantics: Are Construc-
In Proceedings of the Fifth Italian
tions Enough?
Conference on Computational Linguistics (CLiC-it
2018), pages 68–74.

J. Bybee. 2010.

Language, Usage and Cognition.

Cambridge University Press.

Devin Casenhiser and Adele E Goldberg. 2005. Fast
Mapping Between a Phrasal Form and Meaning.
Developmental science, 8(6):500–508.

Emmanuele Chersoni, Philippe Blache, and Alessan-
dro Lenci. 2016. Towards a Distributional Model
In Proceedings of the
of Semantic Complexity.
COLING Workshop on Computational Linguistics
for Linguistic Complexity (CL4LC), pages 12–22.

Emmanuele Chersoni, Alessandro Lenci, and Philippe
Blache. 2017. Logical Metonymy in a Distributional
Model of Sentence Comprehension. In Proceedings
of *SEM, pages 168–177.

Emmanuele Chersoni, Enrico Santus, Ludovica Pan-
nitto, Alessandro Lenci, Philippe Blache, and Chu-
Ren Huang. 2019. A Structured Distributional
Model of Sentence Meaning and Processing. Jour-
nal of Natural Language Engineering. To Appear.

Silvio Cordeiro, Aline Villavicencio, Marco Idiart, and
Carlos Ramisch. 2019. Unsupervised Composition-
ality Prediction of Nominal Compounds. Computa-
tional Linguistics, 45:1–57.

Jonathan Dunn. 2017. Computational Learning of
Construction Grammars. Language and Cognition,
9(2):254–292.

Jonathan Dunn. 2019. Frequency vs. Association for
Constraint Selection in Usage-Based Construction
Grammar. In Proceedings of the NAACL Workshop
on Cognitive Modeling and Computational Linguis-
tics.

Jeffrey L. Elman. 2014. Systematicity in the Lexicon:
On Having Your Cake and Eating It Too.
In Paco
Calvo and John Symons, editors, The Architecture
of Cognition, pages 115–145. The MIT Press.

Katrin Erk, Sebastian Pad´o, and Ulrike Pad´o. 2010. A
Flexible, Corpus-Driven Model of Regular and In-
verse Selectional Preferences. Computational Lin-
guistics, 36(4):723–763.

Charles J. Fillmore. 1982. Frame Semantics. Linguis-

tics in the Morning Calm, pages 111–37.

Charles J Fillmore. 1988. The Mechanisms of Con-
struction Grammar. Annual Meeting of the Berkeley
Linguistics Society, 14:35–55.

Adele E. Goldberg. 1999. The Emergence of the Se-
In
mantics of Argument Structure Constructions.
B. MacWhinney, editor, The Emergence of Lan-
guage, pages 197–212. Lawrence Erlbaum Publica-
tions, Hillsdale, NJ.

Adele E. Goldberg. 2003. Constructions: A New The-
oretical Approach to Language. Trends in Cognitive
Sciences, 7(5):219–224.

Adele E Goldberg. 2006. Constructions at Work: The
Nature of Generalization in Language. Oxford Uni-
versity Press on Demand.

Edward Grefenstette and Mehrnoosh Sadrzadeh. 2015.
Concrete Models and Empirical Evaluations for the
Categorical Compositional Distributional Model of
Meaning. Computational Linguistics, 41(1):71–
118.

Stefan Th. Gries and Anatol Stefanowitsch. 2004. Ex-
tending Collostructional Analysis. A Corpus-Based
Perspective on ’Alternations’. International Journal
of Corpus Linguistics, 9(1):97–129.

Peter Hagoort. 2013. MUC (Memory, Uniﬁcation,
Control) and Beyond. Frontiers in Psychology, 4:1–
13.

Peter Hagoort. 2015. MUC (Memory, Uniﬁcation,
Control): A Model on the Neurobiology of Lan-
guage Beyond Single Word Processing. In Neuro-
biology of language, pages 339–347. Elsevier.

Zellig S. Harris. 1954. Distributional Structure. Word,

10:146–62.

Thomas Hoffman and Graeme Trousdale, editors.
2013. The Oxford Handbook of Construction Gram-
mar. Oxford University Press, Oxford.

Brendan T Johns and Michael N Jones. 2015. Gener-
ating Structure from Experience: A Retrieval-Based
Model of Language Processing. Canadian Journal
of Experimental Psychology, 69(3):233–251.

Matt A. Johnson and Adele E. Goldberg. 2013. Ev-
idence for Automatic Accessing of Constructional
Jabberwocky Sentences Prime Associ-
Meaning:
Language and Cognitive Processes,
ated Verbs.
28(10):1439–1452.

Katharina Kann, Alex Warstadt, Adina Williams, and
Samuel R Bowman. 2019. Verb Argument Structure
Alternations in Word and Sentence Embeddings. In
Proceedings of the Society for Computation in Lin-
guistics (SCiL) 2019, pages 287–297.

Gina R. Kuperberg and T. Florian Jaeger. 2015. What
Do We Mean by Prediction in Language Compre-
Language Cognition & Neuroscience,
hension?
3798.

Gianluca E Lebani and Alessandro Lenci. 2017. Mod-
elling the Meaning of Argument Constructions with
In Proceedings of the
Distributional Semantics.
AAAI 2017 Spring Symposium on Computational
Construction Grammar and Natural Language Un-
derstanding, pages 197–204.

Gianluca E. Lebani and Alessandro Lenci. 2018.
A Distributional Model of Verb-Speciﬁc Semantic
In Thierry Poibeau and Aline
Roles Inferences.
Villavicencio, editors, Language, Cognition, and
Computational Models, pages 118–158. Cambridge
University Press, Cambridge.

Alessandro Lenci. 2011. Composing and Updating
Verb Argument Expectations: A Distributional Se-
mantic Model. In Proceedings of the ACL Workshop
on Cognitive Modeling and Computational Linguis-
tics, pages 58–66.

Alessandro Lenci. 2018. Distributional Models of
Annual Review of Linguistics,

Word Meaning.
4(1):151–171.

Natalia Levshina and Kris Heylen. 2014. A Radi-
cally Data-Driven Construction Grammar: Exper-
iments with Dutch Causative Constructions.
In
R. Boogaart, T. Colleman, and Rutten G., edi-
tors, Extending the Scope of Construction Grammar,
pages 17–46. Mouton de Gruyter, Berlin.

Paweł Mandera, Emmanuel Keuleers, and Marc Brys-
baert. 2017. Explaining Human Performance in
Psycholinguistic Tasks with Models of Semantic
Similarity Based on Prediction and Counting: A Re-
view and Empirical Validation. Journal of Memory
and Language, 92:57–78.

Brian McElree, Matthew J Traxler, Martin J Pickering,
Rachel E Seely, and Ray Jackendoff. 2001. Reading
Time Evidence for Enriched Composition. Cogni-
tion, 78(1):B17–B25.

Ken McRae and Kazunaga Matsuki. 2009. People Use
their Knowledge of Common Events to Understand
Language, and Do So as Quickly as Possible. Lan-
guage and Linguistics Compass, 3(6):1417–1429.

Laura A Michaelis. 2013. Sign-Based Construction
Grammar. In The Oxford Handbook of Construction
Grammar, pages 133–152, Oxford. Oxford Univer-
sity Press.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed Represen-
tations of Words and Phrases and their Composition-
ality. In Advances in Neural Information Processing
Systems (NIPS 2013), pages 3111–3119.

Jeff Mitchell, Mirella Lapata, Vera Demberg, and
Frank Keller. 2010. Syntactic and Semantic Factors
in Processing Difﬁculty: An Integrated Measure. In
Proceedings of ACL, pages 196–206, Uppsala, Swe-
den. ACL.

Martin Paczynski and Gina R. Kuperberg. 2012. Mul-
tiple Inﬂuences of Semantic Memory on Sentence
Processing: Distinct Effects of Semantic Relat-
edness on Violations of Real-World Event/State
Knowledge and Animacy Selection Restrictions. J
Memory and Language, 67(4).

Florent Perek. 2016. Using Distributional Semantics to
Study Syntactic Productivity in Diachrony: A Case
Study. Linguistics, 54(1):149–188.

Florent Perek. 2018. Recent Change in the Produc-
tivity and Schematicity of the Way-Construction: A
Distributional Semantic Analysis. Corpus Linguis-
tics and Linguistic Theory, 14(1):65–97.

Josef Ruppenhofer, Michael Ellsworth, Myriam
Schwarzer-Petruck, Christopher R Johnson, and Jan
Scheffczyk. 2016. FrameNet II: Extended Theory
and Practice.

Ivan A Sag. 2012. Sign-Based Construction Grammar:
An Informal Synopsis. In Sign-based construction
grammar, volume 193, pages 69–202. CSLI: CSLI
Publications.

Enrico Santus, Emmanuele Chersoni, Alessandro
Lenci, and Philippe Blache. 2017. Measuring The-
matic Fit with Distributional Feature Overlap.
In
Proceedings of EMNLP, pages 648–658.

Asad Sayeed, Stefan Fischer, and Vera Demberg. 2015.
Vector-Space Calculation of Semantic Surprisal for
In Pro-
Predicting Word Pronunciation Duration.
ceedings of ACL-IJCNLP, pages 763–773, Beijing,
China. ACL.

Asad Sayeed, Clayton Greenberg, and Vera Demberg.
2016. Thematic Fit Evaluation: An Aspect of Se-
In Proceedings of the ACL
lectional Preferences.
Workshop on Evaluating Vector-Space Representa-
tions for NLP, pages 99–105.

Marco Silvio Giuseppe Senaldi, Gianluca E Lebani,
and Alessandro Lenci. 2016. Lexical Variability and
Investigating Idiomaticity with
Compositionality:
Distributional Semantic Models. In Proceedings of
the ACL Workshop on Multiword Expressions, pages
21–31.

Jennifer Sikos and Sebastian Pad´o. 2018. Using Em-
beddings to Compare FrameNet Frames Across Lan-
guages. In Proceedings of the COLING Workshop
on Linguistic Resources for Natural Language Pro-
cessing, pages 91–101.

Michael Tommasello. 2003. Constructing a Language:
A Usage-Based Theory of Language Acquisition.
Harvard University Press, Cambridge, MA.

Matthew J Traxler, Martin J Pickering, and Brian
McElree. 2002. Coercion in Sentence Process-
ing: Evidence from Eye-Movements and Self-
Paced Reading. Journal of Memory and Language,
47(4):530–547.

Dmitry Ustalov, Alexander Panchenko, Andrei Kutu-
zov, Chris Biemann, and Simone Paolo Ponzetto.
2018. Unsupervised Semantic Frame Induction Us-
ing Triclustering. In Proceedings of ACL, pages 55–
62.

Kristian Woodsend and Mirella Lapata. 2015. Dis-
tributed Representations for Unsupervised Semantic
In Proceedings of EMNLP, pages
Role Labeling.
2482–2491.

